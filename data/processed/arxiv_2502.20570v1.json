{
  "id": "arxiv_2502.20570v1",
  "text": "An Integrated Deep Learning Framework Leveraging NASNet and Vision \nTransformer with MixProcessing for Accurate and Precise Diagnosis of Lung \nDiseases \nSajjad Saleem1 \nDepartment of Information and Technology, Washington University of Science and \nTechnology, Alexandria, VA 22314, USA. \nssaleem.student@wust.edu \n \nMuhammad Imran Sharif 2âˆ— \nDepartment of Computer Science, Kansas State University, Manhattan, KS 66506, USA. \nimransharif@ksu.edu \n \nCorresponding Author: \nMuhammad Imran Sharif \nimransharif@ksu.edu \nAbstract \nThe lungs are the essential organs of respiration, and this system is significant in the carbon dioxide \nand exchange between oxygen that occurs in human life. However, several lung diseases, which \ninclude pneumonia, tuberculosis, COVID-19, and lung cancer, are serious healthiness challenges \nand demand early and precise diagnostics. The methodological study has proposed a new deep \nlearning framework called NASNet-ViT, which effectively incorporates the convolution capability \nof NASNet with the global attention mechanism capability of Vision Transformer ViT. The \nproposed model will classify the lung conditions into five classes: Lung cancer, COVID-19, \npneumonia, TB, and normal. A sophisticated multi-faceted preprocessing strategy called \nMixProcessing has been used to improve diagnostic accuracy. This preprocessing combines \nwavelet transform, adaptive histogram equalization, and morphological filtering techniques. The \nNASNet-ViT model performs at state of the art, achieving an accuracy of 98.9%, sensitivity of \n0.99, an F1-score of 0.989, and specificity of 0.987, outperforming other state of the art \narchitectures such as MixNet-LD, D-ResNet, MobileNet, and ResNet50. The model's efficiency is \nfurther emphasized by its compact size, 25.6 MB, and a low computational time of 12.4 seconds, \nhence suitable for real-time, clinically constrained environments. These results reflect the high-\nquality capability of NASNet-ViT in extracting meaningful features and recognizing various types \nof lung diseases with very high accuracy. This work contributes to medical image analysis by \nproviding a robust and scalable solution for diagnostics in lung diseases. \nKeywords: Radiology; Lung cancer; Pneumonia; COVID-19; Tuberculosis; NASNet-ViT; Vision \nTransformer (ViT); deep learning (DL); MixProcessing; Feature extraction; Medical image \nanalysis \n\n\n1. Introduction \nDeep learning algorithms have revolutionized the face of medical image analysis and are very \npromising for detecting and classifying lung diseases. This introduction draws on key research \narticles that explain the applications of the use of DL models to classify and diagnose different \nlung conditions, including Lung cancer, COVID-19, pneumonia, TB, and normal. One of the major \ncontributions in this area came from Wang et al. [1] when they made the ChestX-ray8 dataset \navailable, which is crucial for developing various DL models needed for chest X-ray image \nanalysis. The dataset provided a basis for the diagnosis of pneumonia and other lung anomalies, \nwith standards supporting the weakly supervised annotation and localization of common thoracic \ndiseases. In parallel, CNNs have shown impressive performance in diagnosing lung diseases. \nMulti-scale dense networks were introduced by Shen et al. [2], which leverage deep CNN \narchitectures to enhance image classification. This has been consistently effective in the \nhierarchical feature extraction for accurately identifying lung diseases. COVID-19 underlined the \nneed for rapid and efficient diagnostic tools. Li et al. [3] proposed a different AI system for \nCOVID-19 identification from community-obtained pneumonia in chest CT images. By using DL \nmethods, this method helps find COVID-19-positive cases and their discrimination, helping better \nmanage and control the disease. TB is another major lung disease and has also been one of the \nmajor points of extensive study regarding DL methods. Lakhani and Sundaram [4] suggested a \ndeep learning model for automatically classifying chest radiographs for pulmonary tuberculosis. \nThe proposed network accurately detected the presence of TB-related abnormalities and, therefore, \nwas useful for effective screening and diagnosis. Their network also demonstrated great potential \nin identifying and characterizing various pattern variations related to interstitial lung diseases for \nfurther appropriate identification and medication. The use of DL for COVID-19 detection has \ndrawn a lot of attention lately. Apostolopoulos and Mpesiana [5] detected COVID-19 from X-ray \npictures by utilizing CNNs and transfer learning. Their approach indicated that DL could be helpful \nin the early detection of patients who suffer from COVID-19 by supporting the medical diagnosis \nmade by radiologists and healthcare professionals and helpful for healthcare responders during the \npandemic. Anthimopoulos et al. [6] proposed a deep CNN for classifying lung patterns limited to \ninterstitial lung disorders. It was a significant turning point in the categorization of lung diseases. \nSimilarly, Jin et al. [7] designed an AI-driven system for COVID-19 diagnosis and evaluated its \nperformance using deep learning techniques to identify COVID-19 cases from imaging data \nprecisely. Rajpurkar et al. [8] proposed CheXNeXt, a DL model, for medical scalability and \nefficiency. It performed at par with practicing radiologists in detecting a wide range of pathologies \non chest X-rays, showing how DL can assist radiologists in furthering diagnostic precision. Salama \net al.  [9] propose a model for COVID-19 detection from chest CT images, where machine learning \nis combined with deep learning to underscore early and precise identification of the disease to \nsupport the reduction of patient mortality. It extracts features from 10 different deep CNN \narchitectures and selects optimal layers for feature extraction. Finally, it is classified using five \nmachine-learning classifiers. Experimental results reveal that the model has higher precision, \npeaking at 99.39%, thus outperforming state-of-the-art techniques and bolstering the model's \n\n\nprospects for COVID-19 diagnostics reliability. Lopes et al. [10] targeted tuberculosis \ninvestigating using a DL method to identify TB-related abnormalities in chest radiograms. Their \nresults emphasized how well the model could flag those with a high probability of TB, thereby \nenabling intervention and treatment to be provided promptly. Besides infectious diseases, lung \ncancer diagnosis has also been extensively investigated using DL models. Wang et al. [11] \ndeveloped a deep learning-based algorithm that could detect pulmonary tuberculosis in chest X-\nrays taken in the setting of the emergency department. They adopted EfficientNetV2 and applied \nthe semi-supervised learning approach to improve the diagnostic performance further. Excellent \nperformance was demonstrated, especially in posterior-anterior views, by yielding an AUC of \n0.878, providing huge potential for fast and reliable tuberculosis screening in high-demand settings\n. Kermany et al. [12] created an image-based deep learning algorithm for identifying lung cancer \nand other curable disorders. Indeed, this model showed DL's ability to identify lung cancer \neffectively and widened its applications in diagnosing other conditions that affect the lungs. These \nstudies have provided valuable insights into developing DL models for detecting Lung cancer, \nCOVID-19, pneumonia, TB, and normal. They indicate that DL can identify and classify lung \ndiseases without failure, hence improving patient outcomes through optimizing health care \ndelivery. Our contribution to this work is an effort to further advance the frontier by constructing \na general DL model for recognizing and classifying lung diseases, as depicted in Figure 1. The \nvarious datasets analyzed and state-of-the-art methodologies included in our study aimed to \nenhance accuracy, speed, and reliability for the detection of lung diseases that would eventually \ncontribute to improved patient treatment and care. \n \nFigure 1. Visual Representation of Various Lung Diseases. \n\n\n1.1.Research Inspiration \nChallenges persist despite progress in several methods to diagnosing lung illnesses from pictures \nrepresenting normal conditions, COVID-19, bacterial pneumonia, viral pneumonia, tuberculosis, \nand lung cancer. It has remained difficult to define the lung features from pictures of normal, \nCOVID-19, bacterial pneumonia, viral pneumonia, tuberculosis, and lung cancer using advanced \ntechnologies during and after image acquisition Due to difficulties in locating and extracting lesion \ncharacteristics linked with lung disorders. Qualified medical annotation of public datasets on \ndamage variables due to normal COVID-19, bacterial pneumonia, viral pneumonia, tuberculosis, \nand lung cancer is limited. Thus, the computerized diagnosis of symptoms of specific disorders \nwith accuracy is quite difficult for the respective systems. Thus, the primary objective in this regard \nhas been two-fold. It aims to Establish a detailed data set for the classification of lung cancer, or \nPak-Lungs, as well as normal, COVID-19, bacterial, viral, and TB pneumonia. The study's goal \nwas to design a complete multi-layered DL architecture that can independently interpret pictures \nrelevant to lung disorders, with a particular emphasis on the setting of lung-related illnesses. This \nwork presented the deep learning-based NASNet-ViT model, which combined the strengths of \nNASNet and Vision Transformer architectures into a form that could realize effective and scalable \ndisease detection. Improved by EnviroSpect, an innovative model utilized for feature isolation, the \nmodel will benefit diseased management by ensuring timely and accurate support. Future \nintegration with IoT technology allows for real-time monitoring that may one day help farmers \nreduce crop losses and promote sustainable agricultural practices. \n1.2. Research Contribution \nWe suggest a new deep-learning model in this context to address the challenge of detecting \ndifferent types of lung diseases. Next are the descriptions of the main contributions of the NASNet-\nViT system: \nâ— A new hybrid deep learning network architecture was created by fusing NASNet and \nVision Transformer, which are specially tailored for classification in lung diseases. This \nunique integration takes advantage of NASNet's convolutional capabilities with the Global \nattention mechanisms of ViT for precise feature extraction and higher classification \nperformance. \nâ— The following study represents the MixProcessing technique, a novel multi-faceted \npreprocessing framework incorporating wavelet transform decomposition, contrast-limited \nadaptive histogram equalization, Fourier-based filtering, and morphological processing to \nenhance image clarity and diagnostic accuracy. \nâ— The investigation focuses on computational efficiency and scalability, making NASNet-\nViT suitable for real-time healthcare applications. Compact model size (25.6 MB) and \nrelatively low computation time (12.4 seconds) allow easy deployment in resource-\nconstrained settings, hence wide accessibility within clinical environments. \n\n\nâ— It utilizes transfer learning and local and global feature extraction mechanisms to tune into \nlung-specific abnormalities with high robustness and accuracy on multiple categories of \nlung diseases. \n1.3. Research Paper Organization \nIn this paper, Section 2 presents the literature review of papers related to the research topic; Section \n3 describes the planned structural design of the approach. Section 4 illustrates the results of the \nexperiments; Section 5 compares our results with state-of-the-art studies on the subject; in Section \n6, a deep discussion of the research findings is performed, and the results of this study are \npresented. \n2. Related Work \nLung diseases, such as pneumonia, tuberculosis, COVID-19, and lung cancer, are some of the \nsignificant global health burdens that result in high morbidity and mortality rates [13]. Their \ndiagnosis can be accurately and early made to offer appropriate treatment and improve patient \noutcomes. Recently, deep learning models have shown remarkable performance in medical image \nanalysis for the automated detection and classification of lung diseases [14]. The following section \ndiscusses the DL methods for TB, COVID-19, lung cancer and pneumonia. Based on the concept \nof transfer learning using models Examples include VGG-16, ResNet-50, and InceptionV3 on \nclinically collected lung images, encouraging performance has been reported in the literature. \nAmong these, pneumonia is identified as a serious symptom of COVID-19, and the relevant \ntransfer learning studies suggest the viral etiology of both diseases is the same. It has also been \nestablished that models trained to detect pneumonia can just as well detect COVID-19. There have \nbeen applications of Haralick features to improve feature extraction, while statistical analysis has \nbeen made to support specific aspects of COVID-19 identification. Transfer learning delivers \nstatistically meaningful results improvements over traditional classification methods [15]. \nLung cancer significantly contributes to the mortality rate, and early detection is an essential \nrequirement to increase the survival rates of patients. In one such study, An MLP classifier \nsurpassed other classifiers in terms of accuracy, scoring 88.55%. Early diagnosis of lung cancer \nincreased the probability of survival from 14% to 49%. Even though CT is generally more \nconsistent than X-ray imaging, a comprehensive identification often involves multiple imagery \nmodes. To address this, a deep neural network was developed for lung cancer detection in CT \nscans. Other researchs have proposed a flexible DenseNet-based boosting technique to classify the \nlung images as either standard or malignant, with an achieved testing accuracy of 90% on the \ndataset of 201 lung images, 85% used for training, and 15% used for testing. Employing the LIDC \ndatabase, CT pictures for benign and malignant lung nodes were assessed by CNN, DNN, and \nsparse auto-encoder deep neural networks were used to identify lung cancer with an accuracy of \n84.15%, sensitivity of 83.96%, and specificity of 84.32%. Among them, CNN showed the \nmaximum accuracy. Machine learning combined with image processing has also shown great \npotential to improve lung cancer diagnosis [16,17,18,19]. The others utilized an artificial neural \n\n\nnetwork, ensemble classifier, SVM, and KNN for COVID-19 versus pneumonia classification, \nwhile a robust DL architecture was based on RNN with LSTM in finding lung diseases [20]. \nFurthermore, the best performance by an ensemble model for combining three deep learning \nfeature extractors, such as InceptionResNet_V2, ResNet50, and MobileNet_V2, achieved a \nmaximum F1-score of 94.84% in classification [21]. \nSeveral related studies have developed automated COVID-19 detection systems based on CT \nimages. Some COVID-19 neural network approaches were used for volumetric chest CT images \nto extract informative graphical features, and their results outperformed the previous methods. Five \npre-trained CNNs have been transferred to classify COVID-19 pneumonia from CXR images. \nThese are Inception-ResNetV2, ResNet152, ResNet50, InceptionV3, and ResNet101, out of which \nResNet50 yielded the highest accuracy in the classification among the preferred models. Collected \nfrom two locations in China, CT scans of 101 pneumonia cases, 88 COVID-19 cases, and 86 \nhealthy ones were used in comparing performances between models in this study [22]. \nFurthermore, COVID-19 patients were effectively diagnosed using a DL-based diagnostic method \nfor the Details Relation Extraction Neural Network on CT scans. This achieved a recall of 0.93, \nan AUC of 0.99, and an accuracy of 0.96, thus showing great potential in diagnosing COVID-19 \nand automatic critical change detection. The other approach presented the development of a \nmodified MobileNet and ResNet architecture to classify COVID-19 CXR images. The \nmethodology of that approach dynamically combined the features from different layers to mitigate \nthe problem of gradient vanishing, yielding better results with accuracy values of 99.3% and 99.6% \non CT and CXR images, respectively [23]. Another used kernel principal component analysis for \nfeature reduction extracted from pre-trained EfficientNet models, followed by a feature fusion \ntechnique. The approach of stacked ensemble meta-classifier utilized a two-stage process wherein \nthe first stage made predictions using an SVM and random forest classifiers, combining the \npredictions into the second stage. A logistic regression classifier classified X-ray and CT data as \nCOVID and non-COVID cases. This model outperformed previous pre-trained CNN-based \nmodels and thus could be a promising tool for clinicians in point-of-care diagnostics [24]. A hybrid \ndeep learning-machine learning model was proposed for COVID-19 detection using CT images \nby extracting features from 10 CNN architectures and by classifying extracted features using five \ndifferent machine learning classifiers. This dataset contained 2,481 CT images divided into \nCOVID-19 and non-COVID-19. The maximum accuracy in experimental results was 99.39%. \nAlso, the best-performing layer for each CNN network was identified and fused with machine \nlearning classifiers. It was concluded that this technique was more effective and robust in \nclassifying COVID-19 compared to state-of-the-art models [9]. It has proposed a hierarchical \nmulti-modal approach for COVID-19 classification, fusing CXR images and tabular medical data. \nOvercoming limitations in binary classification methods and single-feature modality, the proposed \nmodel employed ResNet and VGG-based CNN models with GANs and achieved a high macro-\naverage F1-score of 95.9% and an F1-score of 87.5% specifically for COVID-19 detection in an \nimbalanced dataset. This has substantially enhanced the diagnostic performance by exploiting the \n\n\nhierarchical structures inherent in pneumonia classification while incorporating various data \nsources to support radiological assessments [25]. \nFurther research presented a DCNN model for TB detection using the CXR dataset from the \nNational Library of Medicine and Shenzhen No. 3 Hospital. A DCNN was independently trained \non two datasets and achieved AUC values of 0.9845 and 0.8502. In contrast, the AUC value for \nthe supervised DCNN model for the CXR dataset was comparatively poor, 0.7054. The resulting \nDCNN model detected 36.51% of aberrant radiographs associated to tuberculosis on the CXR \ndataset [26]. Another approach was assessing TB severity and risk using ResNet and depth-ResNet \nmodels. Depth-ResNet and ResNet-50 reached 92.7% and 67.15% accuracy, respectively. Severity \nscores were converted into probabilities: 0.9, 0.7, 0.5, 0.3, and 0.2, based on high severity levels \ncorresponding to the high scores (1-3) and low severity levels for the rest of the scores (4-5). \nAverage accuracies for these methods were 75.88% and 85.29%, respectively [27]. The recent \nstudy proposed an ensemble of three well-known architectures: AlexNet, GoogleNet, and ResNet. \nAgain, using the same pooled dataset of publicly available datasets, a newly developed \ntuberculosis classifier was developed from scratch, demonstrating 88% accuracy with an AUC of \n0.93, higher than most of the existing algorithms [28]. In a recent study, the authors proposed a \ndeep learning-based algorithm to detect pulmonary tuberculosis (PTB) in chest X-ray images \nspecifically for emergency departments. This was a retrospective series based on 3,498 chest X-\nrays of NTUH and external public datasets such as NIH ChestX-ray14, Montgomery County, and \nShenzhen databases. The proposed algorithm with the backbone of an EfficientNetV2 architecture \nshowed an AUC value of 0.878 for the detection of PTB on the NTUH test set, particularly with \noutstandingly high accuracy in posterior-anterior views of 0.940. Hence, This model can show \nperfect external generalization by considering semi-supervised learning and image preprocessing \ntechniques and may promise early PTB detection in emergency settings for better-isolating patients \nand treatment outcomes [29]. \nThis study further utilized deep learning to enhance image quality, reduce pattern overlap, and \nhighlight individual ridge features, which potentially improved authentication systems based on \ndistinctive features of individuals [30]. In the study of lung disorder classification, VGG-16 and \nDenseNet-169 were used on X-ray images to detect pneumonia, tuberculosis, COVID-19, and \ntypical cases, where DenseNet-169 produced 91% accuracy. In particular, these models are useful \nin resource-constrained areas for early diagnosis and improvement of results for patients, \ncontributing to the international fight against lung illnesses. Additional clinical validation may be \nrequired in health care [31]. Table 1 provides a summary of research on the identification and \nclassification of chest diseases. \n \n \n \n\n\nTable 1. Summary and Evaluation of Recent Research. \nRef No \nMethod \nDisease \nDataset \n[15] \nVGG-16 \nCOVID-19 \nCXR + CT \n[16] \nInceptionV3 \nCOVID-19 \nCXR + CT \n[17] \nVGG-19 + ResNet-50 COVID-19 \nCXR + CT \n[18] \nDRE-Net \nCOVID-19 \nCXR + CT \n[16] \nFPSO-CNN \nLungs Cancer \nCT \n[17] \nMulti-layer \nPerceptron (MLP) \nLungs Disease Cancer CT \n[18] \nCNN \nLungs Cancer \nCT \n[20] \nXception \nNetwork \npre-trained \nweights \non ImageNet \nLungs \nDisease \nPneumonia \nCXR and CT \n[21] \nRNN-LSTM \nPneumonia \nCXR + CT \n[26] \nDCNN \nTuberculosis \nCXR, CT \n[27] \nDepth-ResNet, \nEnsemble (AlexNet) \nTuberculosis \nCXR, CT \n[28] \nGoogleNet, \nand \nResNet) \nTB \nCXR and CT \n[31] \nVGG-16 \nand \nDenseNet-169 for the \ncategorization of lung \nillnesses based on X-\nray images \nNormal, pneumonia, \nCOVID-19, \nand \ntuberculosis \nCXR \n \n3. Material and Methods \nThis paper presents a novel framework, NASNet-ViT, an ensemble model that combines the \narchitectures of NASNet and Vision Transformer architectures. The work is developing a \nNASNet-ViT framework for classifying lung disease images such as standard, Lung cancer, \nCOVID-19, pneumonia, TB, and normal. This model leverages NASNet's convolution capabilities \nwith the attention mechanisms in ViT to enhance feature extraction and classification. Transfer \nlearning is utilized to fine-tune the model for lung-specific abnormalities by combining the power \nof dense blocks with the attention-focused structure of ViT to capture essential features. Figure 2 \nshows the different processes involved in the approach in a step-by-step manner. The extracted \nfeatures through NASNet and ViT are combined using a feature transform layer, which fuses \ncharacteristics through element-wise multiplication. The classification results are finally improved \nusing a Multi-Layer Perceptron classifier, which offers a robust yet flexible solution for accurate \ndisease categorization. \n\n\n \nFigure 2. NASNet-ViT System Structured Flow Diagram for Identification of Lung Diseases. \n \n \n \n3.1.  Data Procurement and Preprocessing \nThe 13,313-photo Pak-Lungs dataset was used to train and estimate the NASNet-ViT model. \nImages were acquired through personal sources from a number of reputable ophthalmic clinics in \nPakistan. Patients' and doctors' consent and willingness to share data were acquired. No clinical \ndata was to be disclosed, and the parties' mutual agreement permitted the release of anonymised \ndata. Because of these circumstances, patient data was kept confidential yet available for research. \nPak-Lungs and other well-known internet sources served as the foundation for the dataset and \npreprocessing [51]. On Kaggle, data was generated by merging data from several sources. It \nincludes several chest X-ray pictures linked to lung conditions, such as TB, COVID-19, \npneumonia, and normal lung pictures. To create the training dataset, a certified pulmonologist \nmanually segregated the images of lung disease from the normal dataset. The pulmonologist \ndetermines \nthe \nlung-related \ntraits \nand \nestablishes \nthe \nnorm. \nMixProcessing enhances X-ray image clarity and structural integrity through a wavelet transform \ndecomposition in combination with contrast-limited adaptive histogram equalization, Fourier-\nbased bandpass filtering, adaptive nonlinear filtering, and morphological processing. Wavelet \ntransform decomposition offers hierarchical detail enhancement through the decomposition of the \nimage into approximation and detail components that highlight critical features at different scales. \nCLAHE enhances local contrast by adaptively equalizing histogram values of small regions in the \nimage, thereby highlighting subtle details that form the basis for diagnosis in medical practice. \nFourier-based bandpass filtering refines the representation of textures by selecting frequency bands \nof interest and highlights spatial patterns containing relevant structure information. Adaptive \n\n\nnonlinear filtering- a bilateral filter-smoothes out intensity variations without affecting the \nsharpness of the boundaries to reduce the influence of noise. Finally, morphological processing \nhelps to excerpt critical structures through binary thresholding and morphological closing; this fills \ngaps and removes artifacts to provide a cleaner representation of the anatomical features. Applying \nthese techniques results in high-contrast, noise-reduced images showing essential medical details \nin the proposed framework. Such details help improve diagnostic accuracy and interpretability in \nmedical imaging applications. This overall inclusion of MixProcessing highlights ongoing efforts \nto improve deep learning models' transparency and dependability, increasing their usability and \nreliability for different applications. This is shown in Figure 3. \nFigure 1 presents 13,313 lung images that have been carefully examined. The three datasets used \nin composing the training and testing fundus sets are itemized in Table 2 and Table 3, with a \ndifferent dimension setting for each. All images used in the experiment were reduced to 700Ã—600 \npixels and then processed according to the process required for creating binary labels. The dataset \nconsisted of 13,313 photos, of which 3993 were used for the system evaluation phase. In order to \nguarantee that fairness was taken into account, the dataset was first pre-converted into several \nclasses to balance the total number of photographs in the dataset both during and after the sickness. \nBefore the images were put into an algorithm created especially for the NASNet-ViT model, they \nwere pre-processed by scaling them to 700 by 600 pixels. In order to lessen the variance among \nthe data points, the photographs were also normalized. The NASNet-ViT system is also trained \nand evaluated using data from Pak-Lungs and internet sources [51]. The original resolution of each \nphotograph was 1125 x 1264 pixels. \n \nFigure 3. This picture shows the pre-processing outcomes after the MixProcessing method. \n\n\n \nTable 3. Lung illness dataset for the NASNet-ViT model. \nRef \nDatasets \nNormal COVID-19 \nPneumonia Tuberculosi\ns \nTotal \n[32] \nLung diseases \n(4 types) \n1342 \n462 \n3872 \n660 \n6336 \n[34] \nPak-Lungs \n1500 \n1500 \n1500 \n1500 \n6000 \n \n \n2842 \n1962 \n5372 \n2160 \n12,336 \n \nTable 4. Lung cancer dataset for the NASNet-ViT architecture. \nRef \nDataset \nNormal \nCancer \nTotal \n[33] \nChest CT-Scan \n154 \n473 \n627 \n[34] \nPak-Lungs \n175 \n175 \n350 \n \n \n329 \n648 \n977 \n \nTo simplify and standardize the dataset, the photos were reduced to the more common 700x600 \npixel size using information from three sources. Additionally, seasoned pulmonologists \ncontributed to the creation of this dataset by contributing data on lung and non-lung diseases for \nthe assessment of ground truth. Figure 3: In the image, MixProcessing was used for image pre-\nprocessing to clarify features of the image and remove interference. Applying MixProcessing on \nX-ray images helped us identify central regions and determine their importance linked to detecting \nthe presence of pneumonia disease. MixProcessing helps us identify the distinguishing \ncharacteristics that affected CNN's X-ray-based pneumonia diagnostic forecasts. Adenocarcinoma, \nbig cell carcinoma, squamous cell carcinoma, and normal cells are among the chest malignancies \nthat are represented in the Chest CT-Scan dataset. After that, the data were separated into sets for \ntraining, testing, and validation and placed in a single \"Data\" folder. The documentation supplied \nmakes no mention of the precise location of the source photos. Rather, the dataset was produced \nby combining information from many sources, and the images are in PNG or JPG format. Although \nthe dataset is built on publicly accessible data on Kaggle, the details provided about the dataset do \nnot identify the precise sources for each image. \n3.2. NASNet-ViT Architecture \nIn this work, the authors have proposed the NASNet-ViT framework for classifying lung diseases; \nthis effectively integrates the convolutional capabilities provided by NASNet with the global \nattention mechanisms available in the so-called Vision Transformer (ViT) to perform accurate \nclassification of images into Lung cancer, COVID-19, TB, and normal pneumonia classes of lung \ndiseases. By combining the strengths of both architectures, NASNet-ViT effectively models both \nlocal patterns and global dependencies, improving feature extraction for better classification \nperformance. \n\n\nThe input to the NASNet-ViT framework is a chest X-ray or CT scan image ğ‘‹âˆˆğ‘…ğ»Ã—ğ‘ŠÃ—3, where \nğ» and ğ‘Š represent the image dimensions (224 Ã— 224 pixels), and 3 corresponds to the RGB \nchannels. The preprocessing pipeline ensures that the input images are processed for optimal \nperformance. All input images are resized to 224 Ã— 224 pixels to uniform their dimensions. Further, \nthe normalization of pixel values is done by: \nğ‘‹ğ‘ğ‘œğ‘Ÿğ‘š= \nğ‘‹âˆ’ Âµ\nğœ                                              (1) \n \nwhere ğœ‡ = [ 0.485 , 0.456 , 0.406 ] Î¼=[0.485,0.456,0.406] and ğœ = [ 0.229 , 0.224 , 0.225 ] \nÏƒ=[0.229,0.224,0.225] are the mean and standard deviation of the ImageNet dataset. Data \naugmentation techniques, including rotation, flipping, scaling, and brightness adjustment, are \nemployed to increase data variability and robustness. \nThe NASNet module is used as the backbone for local feature extraction. NASNet, optimized by \nNeural Architecture Search, efficiently applies convolutional operations to extract fine-grained \nfeatures in depth. The input image is fed through a stem block that extracts the initial feature using \ndepthwise separable convolutions. These retain the spatial dimensions and capture the local \nfeatures. Further, these reduce the spatial dimensions by half and increase the depth of the feature \nin order to capture hierarchical patterns. The juxtaposition of Normal and Reduction Cells \nproduces a feature map at a high resolution to capture local patterns. The Global Average Pooling \nLayer follows this to generate a feature vector: \nğ¹ğ‘ğ´ğ‘†ğ‘ğ‘’ğ‘¡= ğºğ‘™ğ‘œğ‘ğ‘ğ‘™ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™(ğ‘(ğ¿))                    (2) \n \nwhere ğ¿ is the total number of NASNet layers. The ViT module uses transformer-based self-\nattention mechanisms to model long-range dependencies on top of NASNet. First, the normalized \ninput image is divided into 16 Ã— 16 16Ã—16 non-overlapping patches. Each patch is then flattened \nand linearly mapped into a higher dimensionality space: \nğ‘ƒğ‘–= (ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(ğ¹ğ‘™ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘›(ğ‘‹ğ‘–)), ğ‘–= 1, â€¦ , ğ‘         (3) \nwhere ğ‘ is the total number of patches. Positional encodings are added to retain spatial information \nacross patches. The transformer encoder takes this patch embedding, augmented with positional \nencoding, through multiple layers, where each layer is composed of multi-head self-attention \nmechanisms along with feed-forward networks to grasp global relations. The ViT module outputs \na feature vector ğ¹ğ‘‰ğ‘–ğ‘‡ representing the global dependencies across the image. \nThese outputs of NASNet and ViT are fused through a feature transform layer to leverage their \ncomplementary strengths. Unlike concatenation, element-wise multiplication can also be used for \nfusion to emphasize shared attributes between the local and global features: \n\n\nğ¹ğ‘’ğ‘›ğ‘ ğ‘’ğ‘šğ‘ğ‘™ğ‘’= ğ¹ğ‘ğ´ğ‘†ğ‘ğ‘’ğ‘¡ Ê˜ ğ¹ğ‘‰ğ‘–ğ‘‡   (4) \nThis fusion strategy will ensure that the detailed local features and the global contextual patterns \ncontribute equally to the final classification task. The fused feature vector then goes through the \nclassifier, which consists of a Multi-Layer Perceptron with dense layers that involve activation \nfunctions and dropout regularization. The model will produce the final predictions by assigning \nthe input image to one of five classes: regular, pneumonia, TB, COVID-19, or lung cancer. The \nframework of NASNet-ViT combines the localized feature extraction of NASNet with the global \npattern recognition capability of ViT. Therefore, This model would elicit intricate details and high-\nlevel relationships in lung disease images. By leveraging transfer learning, pretraining on large \ndatasets allows the model to adapt efficiently for lung-specific abnormalities. \nMoreover, the feature fusion strategy and MLP classifier further improve the overall classification \naccuracy of the framework, making it robust and scalable for real-world applications. The \nNASNet-ViT framework proposed herein has enormous potential for deployment in healthcare \nsystems, especially for automatically diagnosing and screening lung diseases. Its power for \ncorrectly classifying different lung conditions makes it very useful in supporting clinicians, mainly \nwhere access to expert radiologists may be limited due to resource constraints. The systematic \nprocess through which this proposed NASNet-ViT framework applies the classification of lung \ndiseases is outlined in detail via a step-by-step algorithm. \n \nFigure 4. The suggested structure for the enhanced NASNet_ViT model. \n \n \n \n\n\nAlgorithm: The NASNet-ViT Framework for Classifying Lung Diseases. \nStep \nExplanation \nInput / Output \n1 \nInput Image and Preprocessing: Load chest X-ray or \nCT scan images, resize them 224 Ã— 224 pixels, and \nnormalize pixel values using mean and standard deviation \nvalues. \nInput: Raw lung images (ğ»Ã—\nğ‘ŠÃ— 3) \nOutput: Preprocessed images \nresized to 224 Ã—  224 Ã—  3, \nnormalized. \n2 \nData Augmentation: Increase dataset variability by \napplying random flipping, rotation, scaling, and \nbrightness adjustments. \nInput: Preprocessed images \nOutput: Augmented image \ndataset with diverse \ntransformations. \n3 \nFeature Extraction (NASNet): Input the preprocessed \nand augmented images into NASNet for local feature \nextraction through Normal and Reduction Cells. Global \nAverage Pooling produces a feature vector. \nInput: Preprocessed and \naugmented images \nOutput: Local feature vector \n(ğ¹ğ‘ğ´ğ‘†ğ‘ğ‘’ğ‘¡). \n4 \nPatch Embedding (ViT): Split the preprocessed image \ninto 16 Ã—  16  non-overlapping patches, flatten them, and \napply linear projection to create patch embeddings. \nInput: Preprocessed images \nOutput: Patch embeddings \n(ğ‘ƒğ‘– (ğ‘ Ã— ğ‘‘)), where (ğ‘) is the \nnumber of patches, (ğ‘‘) is the \nembedding dimension. \n5 \nPositional Encoding (ViT): Add positional embeddings \nto each patch embedding to preserve spatial relationships. \nInput: Patch embeddings (ğ‘ƒğ‘– ) \nOutput: Position-aware \nembeddings (ğ‘ƒğ‘’ğ‘›ğ‘). \n6 \nTransformer Encoder (ViT): Pass the position-aware \nembeddings through self-attention and feed-forward \nlayers to model global dependencies between patches. \nInput: Position-aware \nembeddings (ğ‘ƒğ‘’ğ‘›ğ‘). \nOutput: Global feature vector \n(ğ¹ğ‘‰ğ‘–ğ‘‡). \n7 \nFeature Fusion: Combine the local features from \nNASNet (ğ¹ğ‘ğ´ğ‘†ğ‘ğ‘’ğ‘¡) and global features from ViT ((ğ¹ğ‘‰ğ‘–ğ‘‡) \nusing element-wise multiplication. \nInput: Feature vectors \n(ğ¹ğ‘ğ´ğ‘†ğ‘ğ‘’ğ‘¡) and ((ğ¹ğ‘‰ğ‘–ğ‘‡) \nOutput: Aggregated feature \nvector (ğ¹ğ‘’ğ‘›ğ‘ ğ‘’ğ‘šğ‘ğ‘™ğ‘’). \n8 \nClassification (MLP): Pass the aggregated feature vector \n(ğ¹ğ‘’ğ‘›ğ‘ ğ‘’ğ‘šğ‘ğ‘™ğ‘’) through a Multi-Layer Perceptron (MLP) \nclassifier to predict the disease class. \nInput: Aggregated feature \nvector (ğ¹ğ‘’ğ‘›ğ‘ ğ‘’ğ‘šğ‘ğ‘™ğ‘’) \nOutput: Predicted class label \n(normal, pneumonia, TB, \nCOVID-19, or lung cancer). \n9 \nTraining: Train the NASNet-ViT framework using \nsupervised learning on the training dataset with cross-\nentropy loss and validate its performance using a \nvalidation set. \nInput: Training dataset, class \nlabels, NASNet-ViT model \nOutput: Trained NASNet-ViT \nmodel. \n\n\n10 \nEvaluation: Use performance indicators like accuracy, \nprecision, recall, F1-score, and confusion matrix to assess \nthe trained model on the test dataset. \nInput: Test dataset, ground \ntruth labels, predicted labels \nOutput: Performance metrics \nincluding F1-score, accuracy, \nand precision. \n \n3.3.Multi-Layer Perceptron (MLP) \nA lightweight Multi-Layer Perceptron classifier, a neural network-based component, serves as the \nfinal decision-making layer for the NASNet-ViT framework. In contrast, the whole framework is \ndesigned to perform lung disease classification. In particular, its crucial role is to turn the fused \nfeature representation F_ensemble, which combines the local features extracted by NASNet and \nthe global features modeled by ViT into accurate predictions of the categories of lung diseases. \nThe MLP is a multi-layer network structure that includes an input layer, one or more hidden layers, \nand one output layer, all of which further enhance the abilities of the model to learn complex \npatterns. The input to the MLP is the aggregated feature vector F_ensemble, which is high-\ndimensional, encapsulating fine-grained local details and long-range dependencies in the input \nlung image. This feature vector is taken by the MLP and passed through one or more hidden layers. \nA given hidden layer performs a linear transformation by calculating a weighted sum over the \ninputs, combined with biases, and usually relies on some nonlinear activation function, such as \nReLU. Such layers are essential in learning nonlinear relationships among the features and \nincreasing their discriminative power. Also, techniques like dropout may be used in the hidden \nlayers to avoid overfitting and ensure better generalization. \nThe last layer in the MLP is the output layer, which is five neurons for the five lung disease \ncategories: normal, pneumonia, tuberculosis, COVID-19, and lung cancer. This layer applies the \nsoftmax activation function to the output logits, converting them into a probability distribution \nover classes. It is adequate for multi-class classification tasks because the softmax function ensures \nthat the sum of the probability across all classes equals one. The class can be considered to belong \nto the class with a higher probability. The MLP classifier is designed to handle the high complexity \nand diversity of the fused feature vector. This enables the model to capture hierarchical \nrelationships in the data, reinforcing more relevant patterns useful for classification. The strengths \nof the NASNet and ViT representations in the MLP ensure intense and exacting predictions. \nFurther, taking completely connected layers means this classifier can work with various input \ndimensions and be suitable for supervised learning. MLP in the NASNet-ViT framework bridged \nthe gap between feature extraction and the overall classification task, making the framework \neffective for diagnosing lung diseases. \n \n \n\n\n4. Results \nThis work uses a dataset of 13,313 normal and diseased lung high-resolution images to train the \nNASNet-ViT model to classify them. These were gathered from authentic sources locally from \nvarious Pakistani hospitals and other web-based valid repositories. All photos were scaled to \n224Ã—224 pixels for easier feature extraction and classification tasks. The NASNet-ViT model was \ntrained using transfer learning for 100 epochs. The best model was at epoch 35 with an F1-score \nof 0.96. The performance of the suggested model has been statistically analyzed in terms of ACC, \nSE, and SP and compared to the state-of-the-art system. The NASNet-ViT system has been \nimplemented on an Intel Core i7 high-end machine with eight cores, 32 GB RAM, and a single \nNVIDIA GeForce GTX 1660 GPU with 6 GB VRAM. Development and training were performed \non the Windows 11 Professional 64-bit operating system. The computation setup was big enough \nfor efficient processing to achieve the best performance of the NASNet-ViT framework both \nduring training and testing. \n4.1. Experiment 1 \nA NASNet-ViT system was estimated to be executed by running an experiment for VGG16, \nVGG19, ResNet-50, Xception, InceptionV3, DenseNet, MobileNet, and EfficientNet DL models \nin this research. Notably, all of these deep learning models were trained using the same amount of \nepochs. In every case, two identical deep neural networks were trained after determining the top \nnetwork based on validation accuracy. Table 8: Comparing results of NASNet-ViT system against \nsensitivity, specificity, accuracy, and area under the curve for VGG16, VGG19, ResNet-50, \nXception, InceptionV3, DenseNet, MobileNet, and EfficientNet models. The results in the process \nindicate that the performance of the NASNet-ViT system is better than that of other DL methods, \nhence validating its presentation. Figure 6 describes the comparison among diverse deep learning \nmodels with NASNet-ViT. \n\n\n \nFigure 5. Comparison of NASNet-ViT and several DL models. \n4.2.  Experiment \nIn this investigation, we will use a dataset known as the \"lung disease dataset (four types),\" \nretrieved from a trusted online source [32], to test the effectiveness of the proposed NASNet-ViT \ntechnique. First, we used acceptable datasets to compare the model's performance on training and \nvalidation sets and evaluate the loss function. Figures 6 and 7 clearly depict the accuracy of the \nNASNet-ViT model's training and validation on this dataset, respectively. The results demonstrate \nhow effectively our model works in training and validation scenarios.  \n \nFigure 6. The accuracy and loss of the suggested model's training and validation. \n\n\n \n \nFigure 7. Lung disease dataset confusion matrix (four kinds). \n4.3.Experiment 3 \nIn this work, we used the Pak-Lungs dataset to estimate the performance of our suggested NASNet-\nViT method. First, the Pak-Lungs dataset was used to evaluate the representation and the loss \nfunction on the training and validation sets. Figure 8 shows the confusion matrix for the NASNet-\nViT model's training and validation using the Pak-Lungs dataset. Figure 9 shows the NASNet-\nViT model's training and validation accuracy using the Pak-Lungs dataset. Our model does well \non tasks involving both training and validation. \n \nFigure 8. Illustration of the proposed model's training and validation accuracy and loss using \nPak-Lungs. \n\n\n \nFigure 9. Pak-Lung's dataset's confusion matrix. \n4.4. Experiment 4 \nIn this study, the Chest CT-Scan image dataset is used to assess the effectiveness of our proposed \nNASNet-ViT approach [33]. The model's performance on training and validation datasets was first \ncompared, and the loss function was assessed on the appropriate datasets. Figures 10 and 11, \nrespectively, illustrate the NASNet-ViT model's training and validation accuracy for this dataset. \nOur model demonstrated exceptional performance in both training and validation scenarios, \naccording to the findings. Our accuracy on the dataset's training and validation sets was exceptional \n[33]. \n \nFigure 10. Illustration of the proposed model's training and validation accuracy and loss using \nCT-Scan. \n\n\n \nFigure 11. Confusion matrix for CT-Scan dataset. \n4.5. State of the art comparison \nResNet50 [22], MobileNet [23], D-Resne [35], and MixNet-LD [34] are among the various designs \nthat are available in the literature and are contrasted with MixNet-LD in Figure 12. The COVID-\n19, pneumonia, and lung tuberculosis normal classes were categorized using these. According to \nthe table, ResNet50 obtained specificity, sensitivity, F1-score, recall, and accuracy values of 0.77, \n0.81, 82, and 82.10, in that order. Although the next designs outperform them entirely, their results \nare nonetheless noteworthy. MobileNet reports gains in all parameters, including accuracy of \n84.55, recall of 0.85, F1-score of 84, sensitivity of 0.82, and specificity of 0.83. With sensitivity, \nspecificity, F1-score, recall, and accuracy of 0.84, 0.85, 87, 0.86, and 85.20, respectively, the D-\nResnet findings show even more improvement. The paper's suggested NASNet-ViT model, on the \nother hand, is exceptional and superior, achieving nearly flawless results across the board. The \naccuracy of the model is an impressive 0.99, with sensitivity and recall of 0.99, specificity of 0.985, \nand F1-score of 0.988. The data given indicates that NASNet-ViT is one of the best instruments \nin the industry due to its impressive outperformance, which highlights its advanced capabilities \nand efficiency for properly categorizing various lung illnesses. \n\n\n \nFigure 12. State-of-the-art performance comparison of NASNet-ViT against other architectures \nfor different classes: normal, COVID-19, pneumonia, and tuberculosis. \nTable 5 and Figure 12 present the computation performance in deep learning models like NASNet-\nViT, MixNet-LD, D-ResNet, MobileNet, and ResNet50 to classify lung disease classes. Their \neffectiveness and efficiency have been judged based on performance metrics like accuracy, \nsensitivity, specificity, F1 score, recall, computation time, and model size. Among them, NASNet-\nViT is the most superior model in this analysis, with an accuracy of 98.9%, which MixNet-LD \nclosely matches at 99.0%. However, NASNet-ViT performed even better than MixNet-LD and the \nrest in sensitivity, 0.99; specificity, 0.985; and F1-score, 0.988, indicating that it can identify true \npositives while keeping false positives low. Most of the high values of precision and balance in \nthe classification metrics justify the robustness of NASNet-ViT in handling challenging lung \ndisease cases, including those with difficult classifications like pneumonia and tuberculosis. \nBesides excellent classification performance, NASNet-ViT is outstanding concerning \ncomputational efficiency: the lowest computational time is 12.4 seconds, an essential aspect of \nreal-time healthcare applications. Besides that, it has a compact model size of 25.6 MB, making it \nvery resource-efficient; thus, it can be deployed on any device with limited hardware capability, \nsuch as mobile or edge devices. This is in contrast to other models, such as ResNet50, which, \nthough acceptable in accuracy, suffers from much more computational overheads and memory. \nWith better classification metrics, much lower computational cost, and reduced model size, as in \nTable 5, NASNet-ViT remains the best tool in the field. Mainly, NASNet-ViT is of great value for \na real-world medical application due to its high accuracy and recall at a minimum resource \nutilization cost, which requires speed, reliability, and scalability. The above analysis underlines \nthe advanced architecture and optimization of NASNet-ViT while setting a benchmark for further \nresearch on the classification of lung diseases. \n \n \n\n\nTable 5. A computational analysis table compares the models, with NASNet-ViT emerging as \nthe best model across various metrics. \nModel \nAccuracy \n(%) \nSensitivity \nSpecificity \nF1-\nScore \nRecall \nComputational \nTime (s) \nModel \nSize \n(MB) \nNASNet-\nViT \n98.9 \n0.99 \n0.985 \n0.988 \n0.99 \n12.4 \n25.6 \nMixNet-LD 99.0 \n0.99 \n0.98 \n0.98 \n0.99 \n14.7 \n30.2 \nD-ResNet \n85.2 \n0.84 \n0.85 \n0.87 \n0.86 \n18.3 \n50.1 \nMobileNet \n84.5 \n0.82 \n0.83 \n0.84 \n0.85 \n20.1 \n48.3 \nResNet50 \n82.1 \n0.77 \n0.81 \n0.82 \n0.81 \n22.5 \n60.5 \n \n \nFigure 13. Displaying computational time (in seconds) and model size (in MB) for the models. \n \n5. Discussion \nLung diseases are a critical area of concern in global healthcare due to their high morbidity and \nmortality rates. The lungs, essential organs in the respiratory system, facilitate gas exchange, \nensuring oxygen reaches the bloodstream while expelling carbon dioxide. However, a range of \nlung diseases pneumonia, tuberculosis, COVID-19, and lung cancer, among others pose significant \nhealth challenges. Pneumonia is an inflammation of the alveoli caused by a bacterial, viral, or \nfungal infection; it can fill the air sacs with fluid, creating symptoms such as fever, cough, and \nbreathing difficulties. Tuberculosis is a bacterial disease caused by Mycobacterium tuberculosis \n\n\nand is one of the most contagious diseases in low- and middle-income countries; its control \ndepends on early diagnosis. COVID-19 is a viral infection from SARS-CoV-2 that has underscored \nthe need for rapid and accurate diagnostic tools in its globally fragmented outbreak. Lung cancer \nis one of the highest burdens of cancer-related deaths worldwide, management of which requires \nearly diagnosis for improved survival rates. Other chronic conditions, such as COPD and asthma, \nhave a continuous need for monitoring and treatment. Though of widely differing pathology, a \ncommon demand these diseases make is for timely and accurate diagnosis to ensure treatment. \nThe discussed paper was related to the challenges of diagnosing lung diseases by proposing a new \nhybrid deep learning model, namely NASNet-ViT. It merges the convolution strengths of NASNet \nwith the global attention mechanisms of Vision Transformer (ViT) for robustness in lung condition \nclassification. The proposed framework classifies lung images into normal, pneumonia, \ntuberculosis, COVID-19, and lung cancer. Due to the implementation of state-of-the-art \npreprocessing techniques and the newest machine learning architectures, its performance metrics \nare superior and position it among the leaders in medical diagnostics. This study uniquely uses a \nhybrid architecture of NASNet and ViT. With its convolutional operations, NASNet efficiently \nextracts local features, but the ViT models bear the strategic spatial dependencies in an image due \nto global attention mechanisms. In this way, the NASNet-ViT framework combines convolutional \nand transformer models' best properties to overcome their shortcomings for superior feature \nextraction from intricate medical images. Another strong point of this study is the preprocessing \napproach, MixProcessing. It follows the decomposition of wavelet transform, CLAHE, and \nmorphological filtering to enhance the image's clarity and emphasize the critical structure. This \noptimizes the quality of the input data and ensures a high diagnostic accuracy by the model, even \nfor quite challenging datasets. \nThe model runs very efficiently and can give an accuracy of 98.9%, a sensitivity of 0.99, and a \nspecificity of 0.985, outperforming the results of current state-of-the-art models such as ResNet50, \nMobileNet, and MixNet-LD, among others that further illustrate the efficiency of the approach. \nEfficiency and scalability in this work are added features. Accordingly, with a compact model size \nof 25.6 MB and a computational time of only 12.4 s, NASNet-ViT would be suitable for real-time \napplications even in resource-constrained clinical settings. The efficiency here does not take a back \nseat to accuracy; hence, this model represents one feasible deployment option for regions devoid \nof advanced health facilities or experienced radiologists. Besides, transfer learning ensures that the \nframework developed will be adaptable to different datasets for more real-world applications. \nDespite this, the study has some limitations. Validation on diverse, multi-regional datasets would \nstrengthen its robustness and applicability. Further, the model requires high-performance \ncomputational resources, like GPUs, which can be challenging in a highly resource-limited \nenvironment. Other challenges include the interpretability issues of the NASNet-ViT model. Like \nmany deep learning frameworks, NASNet-ViT acts as a black-box system that may not be accepted \nin a clinical setting where explainability is crucial for trust and reliability. \n\n\nThe authors have identified some potential promising future directions for this work. It can \nintegrate the NASNet-ViT model with IoT devices for real-time monitoring and diagnostics over \nremote or underserved areas. The scale-up model may also extend its capabilities to include \nmultimodal data to expand the scope of diagnosis. Additionally, explaining AI mechanisms can be \nbuilt to improve clinician trust further and speed up the integration of the model into healthcare \nworkflows. Coupled with global validation, these developments could make NASNet-ViT a game-\nchanging tool in lung disease diagnostics. The main contribution of this paper is the proposal of a \nnew framework for medical imaging called NASNet-ViT. Overcoming the hurdles in classifying \nlung diseases and using state-of-the-art technologies has empowered the study to show that AI-\nbased interventions will improve diagnostic accuracy and efficiency. Though further validation \nand improvements are necessary, the model NASNet-ViT has set a very high bar for further \nstudies, thus opening perspectives toward more reliable and accessible healthcare solutions \nworldwide. \n6. Conclusion \nThe current study introduces NASNet-ViT, a hybrid deep learning model for reliably classifying \nlung disorders, such as lung cancer, COVID-19, TB, and normal pneumonia. The suggested model \nincorporates NASNet's convolutional feature extraction capabilities with the global attention \nmechanism capabilities of ViT to effectively handle the complexities in medical image analysis. \nAdvanced pre-processing techniques, such as MixProcessing, enhanced the model's complex \nmedical image processing ability. The proposed NASNet-ViT model produced remarkable metrics \nof 98.9% accuracy, sensitivity of 0.99, and specificity of 0.985, hence outperforming the current \nstate-of-the-art architectures such as ResNet50, MobileNet, and MixNet-LD. It yields a highly \nefficient and scalable model with a size of 25.6 MB and computational time of 12.4 seconds, hence \ndeployable in real-time in resource-constrained clinical environments. The study further points out \nthat more validation on diverse multi-regional datasets must be done to have better \ngeneralizability. Future work could be directed toward integrating explainable AI techniques in \norder to build more trust among clinicians or exploring multimodal data to increase diagnosis \ncapability. NASNet-ViT represents the state-of-the-art in lung disease diagnosis that is \nsimultaneously robust, efficient, and scalable and closes the gap from advanced AI models to \npractical healthcare applications. It provides the starting point for further research into new ways \nof carrying out medical image analyses, benefiting both patient outcomes and clinical support \nworldwide. \nData Availability Statement: The data presented in this study are available in Kaggle at \nhttps://www.kaggle.com/datasets/omkarmanohardalvi/lungs-disease-dataset-4-types (accessed on \n28 \nJuly \n2023) \nand \nChest \nCT-Scan \nImages \nDataset. \nAvailable \nonline: \nwww.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images (accessed on 30 August 2023). \n\n\nConflicts of Interest: The authors of this work state that they have no conflicts of interest with \nrelation to its publication. \nEthics approval and consent to participate: This study utilized publicly available datasets that \nare fully anonymized. Since no human subjects were directly involved, institutional review board \n(IRB) approval and informed consent were not required. \nConsent for publication: Not applicable.  \nCompeting interests: The authors declare no competing interests. \nReferences \n1. Wang, X.; Peng, Y.; Lu, L.; Lu, Z.; Bagheri, M.; Summers, R.M. ChestX-ray8: Hospital-\nscale chest X-ray database and benchmarks on weakly-supervised classification and \nlocalization of common thorax diseases. In Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition, Honolulu, HI, USA, 21â€“26 July 2017; pp. \n2097â€“2106. \n2. Shen, W.; Zhou, M.; Yang, F.; Yang, C.; Tian, J.; Wang, K. Multi-scale dense networks \nfor resource-efficient image classification. Pattern Recognit. 2019, 85, 54â€“66. \n3. Li, L.; Qin, L.; Xu, Z.; Yin, Y.; Wang, X.; Kong, B.; Bai, J.; Lu, Y.; Fang, Z.; Song, Q.; et \nal. Artificial intelligence distinguishes COVID-19 from community acquired pneumonia \non chest CT. Radiology 2020, 296, E65â€“E71.  \n4. Lakhani, P.; Sundaram, B. DL at chest radiography: Automated classification of pulmonary \ntuberculosis by using convolutional neural networks. Radiology 2017, 284, 574â€“582. \n5. Apostolopoulos, I.D.; Mpesiana, T.A. Covid-19: Automatic detection from X-ray images \nutilizing transfer learning with convolutional neural networks. Phys. Eng. Sci. \nMed. 2020, 43, 635â€“640.  \n6. Anthimopoulos, M.; Christodoulidis, S.; Ebner, L.; Christe, A.; Mougiakakou, S.; Ebner, \nL. Lung pattern classification for interstitial lung diseases using a deep convolutional \nneural network. IEEE Trans. Med. Imaging 2016, 35, 1207â€“1216.  \n7. Jin, C.; Chen, W.; Cao, Y.; Xu, Z.; Tan, Z.; Zhang, H.; Deng, L.; Zheng, C.; Zhou, J.; Shi, \nH.; et al. Development and evaluation of an AI system for COVID-19 diagnosis. Nat. \nCommun. 2020, 11, 5088.  \n8. Rajpurkar, P.; Irvin, J.; Ball, R.L.; Zhu, K.; Yang, B.; Mehta, H.; Duan, T.; Ding, D.; Bagul, \nA.; Langlotz, C.P.; et al. DL for chest radiograph diagnosis: A retrospective comparison of \nthe CheXNeXt algorithm to practicing radiologists. PLoS Med. 2017, 15, e1002686. \n9. Salama, G.M., Mohamed, A. & Abd-Ellah, M.K. COVID-19 classification based on a deep \nlearning and machine learning fusion technique using chest CT images. Neural Comput & \nApplic 36, 5347â€“5365 (2024). https://doi.org/10.1007/s00521-023-09346-7.    \n\n\n10. Lopes, A.J.; Capone, D.; Mogami, R.; Godoy, I.; Gazzotti, M.R.; Roscani, M.G.; Jansen, \nJ.M. Assessment of a DL system for tuberculosis screening on chest radiographs in high-\nrisk populations. JAMA Netw. Open 2020, 3, e2023249. \n11.  Wang, CH., Chang, W., Lee, MR. et al. Deep Learningâ€“based Diagnosis of Pulmonary \nTuberculosis on Chest X-ray in the Emergency Department: A Retrospective Study. J Digit \nImaging. Inform. med. 37, 589â€“600 (2024). https://doi.org/10.1007/s10278-023-00952-4. \n12. Kermany, D.S.; Goldbaum, M.; Cai, W.; Valentim, C.C.; Liang, H.; Baxter, S.L.; \nMcKeown, A.; Yang, G.; Wu, X.; Yan, F.; et al. Identifying medical diagnoses and \ntreatable diseases by image-based DL. Cell 2018, 172, 1122â€“1131.  \n13. Wang, G. DL for lung cancer classification: A comprehensive review. IEEE Rev. Biomed. \nEng. 2021, 14, 134â€“149.  \n14. Rajpurkar, P.; Irvin, J.; Zhu, K.; Yang, B.; Mehta, H.; Duan, T.; Ding, D.; Bagul, A.; \nLanglotz, C.; Shpanskaya, K.; et al. CheXNet: Radiologist-level pneumonia detection on \nchest X-rays with DL. arXiv 2017, arXiv:1711.05225. \n15. Perumal, V.; Narayanan, V.; Rajasekar, S.J.S. Detection of COVID-19 using CXR and CT \nimages using Transfer Learning and Haralick features. Appl. Intell. 2021, 51, 341â€“358. \n16. Lakshmanaprabu, S.K.; Mohanty, S.N.; Shankar, K.; Arunkumar, N.; Ramirez, G. Optimal \nDL model for classification of lung cancer on CT images. Future Gener. Comput. \nSyst. 2019, 92, 374â€“382.  \n17. Song, Q.Z.; Zhao, L.; Luo, X.K.; Dou, X.C. Using DL for Classification of Lung Nodules \non Computed Tomography Images. J. Healthc. Eng. 2017, 2017, 8314740.  \n18. Singh, G.A.P.; Gupta, P.K. Performance analysis of various machine learning-based \napproaches for detection and classification of lung cancer in humans. Neural Comput. \nAppl. 2019, 31, 6863â€“6877. \n19. Kalaivani, N.; Manimaran, N.; Sophia, S.; Devi, D.D. DL Based Lung Cancer Detection \nand Classification. IOP Conf. Ser. Mater. Sci. Eng. 2020, 994, 7731â€“7776.  \n20. Luja, J.E.; Moreno-Ibarra, M.A.; Villuendas-Rey, Y.; YÃ¡Ã±ez-MÃ¡rquez, C. Fast COVID-19 \nand pneumonia classification using chest X-ray images. Mathematics 2020, 8, 1423. \n21. Goyal, S.; Singh, R. Detection and classification of lung diseases for pneumonia and \nCovid-19 \nusing \nmachine \nand \nDL \ntechniques. J. \nAmbient \nIntell. \nHumaniz. \nComput. 2021, 14, 3239â€“3259.  \n22. Narin, A.; Kaya, C.; Pamuk, Z. Automatic detection of coronavirus disease (COVID-19) \nusing X-ray images and deep convolutional neural networks. Pattern Anal. Appl. 2021, 24, \n1207â€“1220.  \n23. Jia, G.; Lam, H.K.; Xu, Y. Classification of COVID-19 chest X-ray and CT images using \na type of dynamic CNN modification method. Comput. Biol. Med. 2020, 134, 104425. \n24. Ravi, V.; Narasimhan, H.; Chakraborty, C.; Pham, T.D. DL-based meta-classifier approach \nfor COVID-19 classification using CT scan and chest X-ray images. Multimed. \nSyst. 2021, 28, 1401â€“1415. \n25. Althenayan, Albatoul S., Shada A. AlSalamah, Sherin Aly, Thamer Nouh, Bassam \nMahboub, Laila Salameh, Metab Alkubeyyer, and Abdulrahman Mirza. 2024. \"COVID-\n\n\n19 Hierarchical Classification Using a Deep Learning Multi-Modal\" Sensors 24, no. 8: \n2641. https://doi.org/10.3390/s24082641.  \n26. Sathitratanacheewin, S.; Sunanta, P.; Pongpirul, K. Deep learning for automated \nclassification of tuberculosis-related chest X-ray: Dataset distribution shift limits \ndiagnostic performance generalizability. Heliyon 2020, 6, e04614. \n27. Gao, X.W.; James-Reynolds, C.; Currie, E. Analysis of tuberculosis severity levels from \nCT \npulmonary \nimages \nbased \non \nenhanced \nresidual \nDL \narchitecture. Neurocomputing 2020, 392, 233â€“244. \n28. Hooda, R.; Mittal, A.; Sofat, S. Automated TB classification using ensemble of deep \narchitectures. Multimed. Tools Appl. 2019, 78, 31515â€“31532. \n29. Wang, CH., Chang, W., Lee, MR. et al. Deep Learningâ€“based Diagnosis of Pulmonary \nTuberculosis on Chest X-ray in the Emergency Department: A Retrospective Study. J Digit \nImaging. Inform. med. 37, 589â€“600 (2024). https://doi.org/10.1007/s10278-023-00952-4.  \n30. Saponara, S.; Elhanashi, A.; Gagliardi, A. Reconstruct fingerprint images using deep \nlearning and sparse autoencoder algorithms. Proc. SPIE Real Time Image Process. Deep \nLearn. 2021, 11736, 1173603. \n31. Kartik, N.; Deshpande, A.; Guntuka, R.; Patil, A. Analysing X-ray Images to Detect Lung \nDiseases \nUsing \nDenseNet-169 \nTechnique. \nAvailable \nonline: https://ssrn.com/abstract=4111864 (accessed on 8 April 2022). \n32. Lungs \nDisease \nDataset \n(4 \nTypes). \nAvailable \nonline: \nhttps://www.kaggle.com/datasets/omkarmanohardalvi/lungs-disease-dataset-4-types \n(accessed on 28 July 2023). \n33. Chest \nCT-Scan \nImages \nDataset. \nAvailable \nonline: \nwww.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images (accessed on 30 August \n2023). \n34. Ahoor, Ayesha, Fahim Arif, Muhammad Zaheer Sajid, Imran Qureshi, Fakhar Abbas, \nSohail Jabbar, and Qaisar Abbas. 2023. \"MixNet-LD: An Automated Classification System \nfor Multiple Lung Diseases Using Modified MixNet Model\" Diagnostics 13, no. 20: 3195. \nhttps://doi.org/10.3390/diagnostics13203195. \n35. Kassania, S.H.; Kassanib, P.H.; Wesolowskic, M.J.; Schneidera, K.A.; Detersa, R. \nAutomatic Detection of Coronavirus Disease (COVID-19) in X-ray and CT Images: A \nMachine Learning Based Approach. Biocybern. Biomed. Eng. 2021, 41, 867â€“879. \n \n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20570v1.pdf",
    "total_pages": 27,
    "title": "An Integrated Deep Learning Framework Leveraging NASNet and Vision Transformer with MixProcessing for Accurate and Precise Diagnosis of Lung Diseases",
    "authors": [
      "Sajjad Saleem",
      "Muhammad Imran Sharif"
    ],
    "abstract": "The lungs are the essential organs of respiration, and this system is\nsignificant in the carbon dioxide and exchange between oxygen that occurs in\nhuman life. However, several lung diseases, which include pneumonia,\ntuberculosis, COVID-19, and lung cancer, are serious healthiness challenges and\ndemand early and precise diagnostics. The methodological study has proposed a\nnew deep learning framework called NASNet-ViT, which effectively incorporates\nthe convolution capability of NASNet with the global attention mechanism\ncapability of Vision Transformer ViT. The proposed model will classify the lung\nconditions into five classes: Lung cancer, COVID-19, pneumonia, TB, and normal.\nA sophisticated multi-faceted preprocessing strategy called MixProcessing has\nbeen used to improve diagnostic accuracy. This preprocessing combines wavelet\ntransform, adaptive histogram equalization, and morphological filtering\ntechniques. The NASNet-ViT model performs at state of the art, achieving an\naccuracy of 98.9%, sensitivity of 0.99, an F1-score of 0.989, and specificity\nof 0.987, outperforming other state of the art architectures such as MixNet-LD,\nD-ResNet, MobileNet, and ResNet50. The model's efficiency is further emphasized\nby its compact size, 25.6 MB, and a low computational time of 12.4 seconds,\nhence suitable for real-time, clinically constrained environments. These\nresults reflect the high-quality capability of NASNet-ViT in extracting\nmeaningful features and recognizing various types of lung diseases with very\nhigh accuracy. This work contributes to medical image analysis by providing a\nrobust and scalable solution for diagnostics in lung diseases.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}