{
  "id": "arxiv_2502.21009v1",
  "text": "Position: Solve Layerwise Linear Models First to Understand Neural Dynamical\nPhenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)\nYoonsoo Nam * 1 Seok Hyeong Lee * 2 Cl´ementine Domin´e 3 Yea Chan Park 4 Charles London 5 Wonyl Choi 6\nNiclas G¨oring 1 Seungjai Lee 7\nAbstract\nIn physics, complex systems are often simplified\ninto minimal, solvable models that retain only the\ncore principles. In machine learning, layerwise\nlinear models (e.g., linear neural networks) act\nas simplified representations of neural network\ndynamics. These models follow the dynamical\nfeedback principle, which describes how layers\nmutually govern and amplify each other’s evolu-\ntion. This principle extends beyond the simplified\nmodels, successfully explaining a wide range of\ndynamical phenomena in deep neural networks,\nincluding neural collapse, emergence, lazy and\nrich regimes, and grokking. In this position pa-\nper, we call for the use of layerwise linear models\nretaining the core principles of neural dynami-\ncal phenomena to accelerate the science of deep\nlearning.\n1. Introduction\nPhysicists often build intuition about complex natural phe-\nnomena by abstracting away intricate details—for instance,\nmodeling a cow as a sphere (Kaiser, 2014), linearizing pen-\ndulum motion (Huygens, 1966), or using simplified frame-\nworks like Hopfield networks to explore associative memory\nand network dynamics (Hopfield, 2007). These abstractions\nenable tractable analysis while preserving the core princi-\n*Equal contribution 1Department of Theoretical Physics, Uni-\nversity of Oxford, Oxfordshire, United Kingdom 2Center for Quan-\ntum Structures in Modules and Spaces, Seoul National University,\nSeoul, South Korea 3Gatsby Computational Neuroscience Unit,\nUniversity College London, London, United Kingdom 4Center\nfor AI and Natural Science, Korea Institute For Advanced Study,\nSeoul, South Korea 5Department of Computer Science, University\nof Oxford, Oxfordshire, United Kingdom 6Department of Com-\nputer Science, Boston University, Massachusetts, United States\nof America 7Department of Mathematics, Incheon National Uni-\nversity, Incheon, South Korea. Correspondence to: Seungjai Lee\n<seungjai.lee@inu.ac.kr>.\nPreliminary work. Under review by the International Conference\non Machine Learning (ICML). Do not distribute.\nples of the system. A good model not only allows analytical\nsolutions but also captures the underlying principle, often\nproviding insights and extending their utility beyond their\nformal limits.\nDeep neural networks (DNNs) are complex dynamical sys-\ntems, with non-linear activations (e.g., ReLU) posing ma-\njor challenges in analysis. Without non-linear activations,\nDNNs become layerwise linear models (e.g., linear neural\nnetworks), often underappreciated as simple product of ma-\ntrices for their lack of expressivity. However, the dynamics\nof layerwise linear models are non-linear, allowing them\nto explain key DNN phenomena such as emergence (Nam\net al., 2024a) observed in large language models (Brown\net al., 2020), neural collapse (Mixon et al., 2020) observed\nin image classification tasks (Papyan et al., 2020), lazy/rich\nregimes (Domin´e et al., 2024) observed in extreme limits\n(Jacot et al., 2018; Chizat et al., 2019), and grokking (Kunin\net al., 2024) observed in algorithmic tasks (Power et al.,\n2022), and more as dynamical phenomena of layerwise\nstructure.\nContrary to most non-linear models, the dynamics of layer-\nwise linear models, under suitable assumptions, are exactly\nsolvable (for example, (Baldi & Hornik, 1989; Saxe et al.,\n2014; Braun et al., 2022)). These solutions allow mathe-\nmatical interpretations that extend beyond the limits of the\nmodels. In this paper, we propose the dynamical feedback\nprinciple (Section 3) as a unified principle for understanding\nlayerwise linear models, and review how they accurately\nrepresent DNN dynamics to argue our position:\nPosition: We call for new studies to prioritize the dy-\nnamics of layerwise structure as the key feature over the\nnon-linear activations, until its limits are fully explored.\nLayerwise linear models help identify whether the dynamics\nof layerwise structure drives a phenomenon, demystifying\nperceptions of the complex roles of datasets, logic, and non-\nlinear activations. Their solvability enables a theoretical\napproach, while their simplicity makes them accessible to a\nbroader audience, accelerating the science of deep learning.\n1\narXiv:2502.21009v1  [stat.ML]  28 Feb 2025\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nAmplifying (reinforcing) dynamics among layers (Section 3)\nGreedy Dynamics\nSigmoidal Growth\nSingle Layer Dynamics\nLazy Regime\nEmergence\nNeural Collapse\nLazy/Rich Regimes,\nGrokking\nGrokking\nKey Principle:\nDynamical Feedback\nSolvable Model Dynamics\n(without non-linearity)\nEmpirical phenomena of\nDNN\nSection 4\nSection 5\nSection 6\nSection 7\nCondition\nSmall Initialization,\nDiagonal Network\nSmall Initialization,\nWhitened Input\nImbalanced\nInitialization\nWeight Norm   \nTarget Scale\nFigure 1. Paper outline For a systematic presentation of various works on layerwise linear models, we begin the section by building\nintuition on how the key principle (green) behaves under a condition (yellow). We then formalize this intuition as a key property (blue)\nusing a solvable layerwise linear model. Finally, we discuss how this property from the layerwise linear model extends to describe an\nempirical phenomenon in DNNs (red).\n1.1. Contributions\n• We propose dynamical feedback principle as a novel\nunifying principle for understanding seemingly uncor-\nrelated DNN phenomena (neural collapse, emergence,\nlazy/rich regimes, and grokking) as consequences of\ndynamics under layerwise structure.\n• We review solvable layerwise linear models and dis-\ncuss their significance through the principle and intu-\nitive derivations.\n1.2. Paper outline\nFigure 1 illustrates the paper outline. In Section 3, we\nintroduce the dynamical feedback principle, which unifies\nthe explanation of seemingly unrelated DNN phenomena.\nIn Sections 4 to 7, we first demonstrate how the principle ap-\nplies under specific conditions, then formalize the intuition\nusing a solvable layerwise linear model, prioritizing intuitive\nderivations over rigor found in the references. Finally, we\nexplore how the principle extends beyond layerwise linear\nmodels to explain phenomena in practical DNNs.\nIn the discussion, we argue that solvable layerwise linear\nmodels effectively capture the dynamics of DNN through\nthe feedback principle, making them essential for advanc-\ning DNN theory, hence our position. We conclude with\npromising research directions. For a glossary of notations\nand terms (emphasized in italic), see Appendix A.\n2. Setup\nThroughout the paper, unless explicitly stated, we assume\nthe input features x ∈Rd to be d dimensional independent\nzero-mean Gaussian random variable. We only consider gra-\ndient flow (GF) and assume that the training dataset follows\na certain probability distribution for analytic simplicity.\nTarget function We assume a realizable target function f ∗\nwith target scales Si ∈R for i = 1, . . . , d:\nf ∗(x) =\nd\nX\ni=1\nxiSi.\n(1)\nLoss We consider the mean square error (MSE) loss\nL := 1\n2E\nh\n(f ∗(x) −f(x))2i\n.\n(2)\nModels We define layerwise linear models as 2-layer mod-\nels whose outputs are multilinear with respect to any layers\nof parameters. Examples include the variants of linear neu-\nral networks, illustrated in Figure 2:\nf (diag)\na,b\n(x) =\nd\nX\ni=1\nxiaibi,\na, b ∈Rd,\n(3)\nf (lnn)\nW1,W2(x) = xT W1W2,\nW1 ∈Rd×p, W2 ∈Rp×c.\n(4)\nA linear model with no hidden layer is used as a base model\nfor comparison:\nf (lin)\nθ\n(x) = xT θ,\nθ ∈Rd.\n(5)\nLike linear models, layerwise linear models do not require\nlinearity with the input features xi, though we use them for\ndemonstrative purposes. See Appendix B for details.\n2\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\n...\n...\n...\n...\n(a) Diagonal linear neural network\n(b) Linear neural network\nFigure 2. Layerwise linear models. Layerwise linear models (Ap-\npendix B) include, but are not limited to, (a): diagonal linear\nneural networks (Equation (3)) and (b): linear neural networks\n(Equation (4)). The layerwise structure leads to distinct dynamics\ncompared to linear models (Equation (5)).\n3. The dynamical feedback principle\nDynamical feedback principle describes how the magnitude\nof one layer mutually scales the rate of change in other layers\n(Figure 1). The dynamical feedback principle arises from\nthe gradient descent dynamics under layerwise structure.\nFor example, the GF equation of diagonal linear neural\nnetwork is\ndai\ndt = −biE[x2\ni ](aibi−Si),\ndbi\ndt = −aiE[x2\ni ](aibi−Si).\n(6)\nThe product of parameters (i.e., layerwise structure) creates\nan amplifying dynamics: the size of ai governs the rate\nof change for bi, and vice versa, yielding a dynamical\nfeedback. To highlight its significance, we compare the\ngradient equation to that of the linear model:\ndθi\ndt = −E[x2\ni ](θi −Si).\n(7)\nIn linear models, parameters evolve depending only on their\ndistances to respective target scales (θi −Si), lacking the\nfeedback with other parameters. The following sections\nfocus on exploring the consequences of feedback.\nConservation of magnitude difference\nBecause of the\ncommutativity of products in Equation (3), we identify a\nconserved quantity in Equation (6) as\nai\nd\ndtai −bi\nd\ndtbi = 0\n⇒\na2\ni −b2\ni = Ci,\n(8)\nwhere Ci is determined at initialization. We can extend the\nargument for linear neural networks to show that W2W T\n2 −\nW T\n1 W1 is conserved (Appendix C).\n4. From amplifying dynamics to emergence\nIn Equation (6), when ai ≈bi, the dynamical feedback\nexhibits rich-get-richer (poor-get-poorer) property: a larger\n(smaller) |ai| results in a faster (slower) evolution of bi,\nwhich amplifies (attenuates) in positive (negative) feedback.\nIn this section, we formalize this property as sigmoidal\ngrowth (delayed saturation) of aibi in diagonal linear neural\nnetworks, which explains the emergence – or sudden change\nin task or subtask performance – observed in large language\nmodels (Ganguli et al., 2022; Srivastava et al., 2022; Wei\net al., 2022; Chen et al., 2023).\n4.1. Sigmoidal dynamics and stage-like training\nWe consider dynamics of diagonal linear neural networks\n(Equation (6)) with small and equal initialization; ai(0) =\nbi(0) and 0 < ai(0) ≪1. The dynamics decouples into d\nindependent modes, where each mode follows a sigmoidal\nsaturation:\nai(t)bi(t)/Si =\n1\n1 +\n\u0010\nSi\nai(0)bi(0) −1\n\u0011\ne−2SiE[x2\ni ]t .\n(9)\nThis contrasts to the exponential saturation of modes in the\nlinear model with θi(0) = 0 (Equation (7)):\nθi(t)/Si = 1 −e−E[x2\ni ]t.\n(10)\nSee Appendix C for the derivation of both models. For a\ngeneral derivation when ai ̸= bi, see Appendix A of Saxe\net al. (2014).\nFigure 3 demonstrates the difference between the dynamics\nof two models. While a mode in linear models saturates\nimmediately after initialization (Figure 3(a)), that of diag-\nonal linear neural network saturates after a delay. The\nsmall value of aibi delays the saturation as small ai down-\nscales the gradient of bi and vice versa (feedback principle).\nAfter sufficient evolution, the feedback principle abruptly\nsaturates aibi (Figure 3(b)).\n0\n50\n100\nt\n0.0\n0.5\n1.0\nθi\n(a) Linear\n0\n50\n100\nt\n0.0\n0.5\n1.0\naibi\n(b) Layerwise\nE[x2\n1] = 0.5\nE[x2\n2] = 0.2\nE[x2\n3] = 0.1\nFigure 3. Dynamics of the linear model and the diagonal linear\nneural network. The colored lines show the saturation curves of\nmodes with different variances for (a): the linear model (Equa-\ntion (10)) and (b): the diagonal linear neural network (Equa-\ntion (9)) with Si = 1. For the linear model, all θi’s saturate\nfrom t = 0 only differing in the saturation speed. For the layer-\nwise model, aibi’s show delayed saturations depending on E[x2\ni ],\nlearning the modes in sequences.\nStage-like training\nIn Figure 3(b), the first mode (blue)\nsaturates fully while the next mode remains negligible, con-\ntrasting with the concurrent saturation of modes in linear\n3\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nmodels (Figure 3(a)). The sequential learning of modes\nor the stage-like training occurs in layerwise models when\nsaturation time scales (1/(SiE[x2\ni ])) differ sufficiently; the\nsigmoidal dynamics (Equation (9)) delays slower modes\nlong enough for faster modes to fully saturate.\n4.2. Application 1: Saddle-to-saddle learning\nSaddle-to-saddle learning refers to sudden drops and\nplateaus of the loss during training (Jacot et al., 2022; Pesme\n& Flammarion, 2023; Atanasov et al., 2024). We can de-\nscribe the DNN phenomenon with the stage-like training\nof diagonal linear neural network with varying SiE[x2\ni ]. A\ndrop maps to the abrupt saturation of a feature, while a\nplateau to the delay before the saturation of the next feature.\n4.3. Application 2: Emergence\nEmergence is an empirical observation that larger languages\nmodels suddenly gain performance when using more data or\nparameters (Wei et al., 2022).1 Emergence has been consid-\nered challenging to explain through “analysis of gradient-\nbased training” (Arora & Goyal, 2023), and studies have\nbeen focused on defining basic skills and how they cumulate\nto a sudden improvement in more complex abilities (Arora\n& Goyal, 2023; Chen et al., 2023; Yu et al., 2023; Okawa\net al., 2024).\nDiagonal linear neural network with prebuilt skills\nIn\nNam et al. (2024a), the authors modeled emergence as a\ndynamical outcome of sigmoidal growth. They studied the\nmultitask parity problem, consisting of parity tasks (skills)\nwhose frequencies follow a power-law distribution. They\nused diagonal linear neural network with the input features\n(xi in Equation (3)) replaced by the skill functions gk(x):\nf(x) =\nX\nk\nakbkgk(x),\nf ∗(x) = S\nX\nk\ngk(x). (11)\nThe skill functions gk(x) are prebuilt features that map the\ninputs to a scalar by calculating the spare parity function.\nIn their setup, the model learns the skills through sigmoidal\ndynamics (Equation (9)), prioritizing the more frequent\nskills (with larger E[g2\nk(x)]) first.\nThe power-law distribution in skill frequency and layer-\nwise structure drives stage-like training, prioritizing limited\nresources for learning only the most frequent skills. In Fig-\nure 4, the model (Equation (11)) accurately captures the\nemergent dynamics in a 2-layer neural network (with ReLU\nactivation) for time, data, and the number of parameters.\n1Emergence differs from grokking in that it does not account\nfor the gap between achieving prefect training and test accuracies.\n103\n104\nT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSkill strength  Rk/S\n(a) Time emergence\n103\n104\nD\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSkill strength  Rk/S\n(b) Data emergence\n100\n101\nN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSkill strength  Rk/S\n(c) Parameter emergence\nk = 1\nk = 2\nk = 3\nk = 4\nk = 5\nk = 1\nk = 2\nk = 3\nk = 4\nk = 5\nFigure 4. Predicting emergence with layerwise linear model\n(Figure 1 from (Nam et al., 2024a)). The skill strength Rk =\nE[f(x)gk(x)]/E[(gk(x))2] measures the linear correlation be-\ntween the kth skill function and the learned function or how well\nthe skill is learned. Each color represents a different skill. The\nsolid lines show the theoretical layerwise model (Equation (11))\ncalibrated on the first skill, while the dashed lines represent the\nempirical results of a 2-layer neural network trained on the multi-\ntask sparse parity problem.\n4.4. Application 3: Scaling laws\nScaling laws refer to how the performance of large language\nmodels scales as a power law with additional resources\n(e.g., compute, data, parameters) (Hestness et al., 2017;\nKaplan et al., 2020). Michaud et al. (2023) proposed that\nscaling laws arise from the emergent learning of numerous\nsubtasks or skills whose frequencies follow a power law.\nConsequently, Nam et al. (2024a) predicted the scaling laws\nobserved in 2-layer neural networks trained on multitask\nsparse parity problem with Equation (11).\nIntuition\nThe key question is why the layerwise linear\nmodel (Equation (11)) well approximates the neural network\nin Figure 4. The former has prebuilt skills and decoupled\ndynamics while the latter lacks such information of skills or\nany decoupling. The layerwise structure and the power-law\ndistribution of skill frequencies induce stage-like training\nin neural networks, where skills are learned on different time\nscales. This effectively decouples feature learning, enabling\nthe model to distinguish features and justifying the use of\nprebuilt skill functions in Equation (11). See Appendix D\nor Nam et al. (2024a) for a detailed discussion.\n5. From greedy (low-rank) dynamics toward\nsalient features to neural collapse\nIn Section 4, we demonstrated that sigmoidal saturation\n(Equation (9)) in diagonal linear neural networks priori-\ntizes learning features that are more correlated (with larger\nSiE[x2\ni ] = E[xiyi]). In this section, by building on on Saxe\net al. (2014), we show how the preference toward correlated\nfeatures leads to a low-rank bias (greedy dynamics) in lin-\near neural networks. We then draw on Mixon et al. (2020) to\nexplain neural collapse — the collapse of last-layer features\ninto a low-rank structure —observed in vision classification\ntasks (Papyan et al., 2020).\n4\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\n5.1. Greedy dynamics\nThe dynamics of linear neural networks is challenging to\nsolve in general. Assuming small initialization and whitened\ninput (E[xxT ] = I), Saxe et al. (2014) and following works\n(Ji & Telgarsky, 2018; Arora et al., 2018a; Lampinen &\nGanguli, 2018; Gidel et al., 2019; Tarmoun et al., 2021)\nsolved the exact dynamics of linear neural networks. The\ndynamics decouples into c independent modes similar to\nthat of diagonal linear neural network (Appendix C):\nα2\ni (t)\nρi\n=\n1\n1 +\n\u0010\nρi\nα2\ni (0) −1\n\u0011\ne−2ρit ,\n(12)\nwhere αi := uT\ni W1W2vi, and ui, ρi, vi are the ith left sin-\ngular vector, singular value, and the right singular vector of\nthe correlation matrix E\n\u0002\nxf ∗(x)T \u0003\n= UPV , respectively.\nNote the similarity between Equation (9) and Equation (12),\nwhere aibi and Si in Equation (9) are replaced by α2\ni and ρi\nin Equation (12), respectively. Linear neural networks auto-\nmatically identify the linear combination of input features\nmost correlated (largest ρi) with the target (salient features).\nAs discussed in stage-like training, modes are learned se-\nquentially based on the order of ρi or their saliency. This\nbias toward salient features introduces a preference for min-\nimum rank, as only target-relevant c (the dimension of the\noutput) modes are trained, even though W1 can reach a\nrank of min(d, p). We will refer to the tendency toward\nlearning the most target-relevant features first as the greedy\ndynamics.\n5.2. Application: Neural collapse\nNeural collapse (NC) (Papyan et al., 2020) describes the phe-\nnomenon where last-layer features form a simplex equian-\ngular tight frame (ETF) – orthogonal vectors projected at\nthe complement of the global mean. For instance, last layer\nfeature vectors of ResNet18 trained on CIFAR10 converge\nto 10 orthogonal vectors (9-simplex ETF), corresponding to\nthe number of classes (Papyan et al., 2020). See Appendix E\nor (Papyan et al., 2020) for the formal definition.\nThe NC gained attention as the last layer feature vectors\noccupied only c dimensions out of p dimensional feature\nspace, where c is the number of classes and p is the width\nof the last layer. Mixon et al. 2020 and similar works (Fang\net al., 2021; Han et al., 2022) studied the dynamics of NC\nthrough the unconstrained feature model (UFM). The UFM\ntrains a product of two matrices where one matrix represents\nthe features and the other matrix represents the last layer\nweights, a special case of linear neural networks. See also\nrelevant works on matrix factorization (Arora et al., 2019;\nLi et al., 2020).\nClass 1\nClass 2\nClass 3\nGlobal\nMean\nFigure 5. Illustration of neural collapse. In NC, the last layer\nfeature vectors (the post-activation of the penultimate layer), illus-\ntrated as colored dots, cluster by their class mean vector, illustrated\nas colored arrow, and form a simplex ETF structure (orthogonal\nvectors projected at the compliment of the global mean vector).\nInformal derivation of NC\nWe show a non-rigorous\nderivation of NC and discuss why feature vectors of lin-\near neural network XW1, where X ∈Rn×d is the input\nfeature matrix for n training datapoins, collapse into a rank\nc matrix. For rigorous theorems, see (Mixon et al., 2020;\nFang et al., 2021). First, we assume sufficiently small ini-\ntialization such that the conserved quantity (Equation (8)) is\nW1(0)T W1(0) −W2(0)W2(0)T ≈0. After training,\nXW1(∞)W2(∞) = Y,\n(13)\nwhere Y ∈Rn×c is the label matrix with one-hot vector\nas the labels, and Wi(∞) the trained matrices. Since Y is\na rank c matrix, W2(∞) is also rank c. The small initial-\nization condition W T\n1 W1 −W2W T\n2 ≈0 ensures that W1\nand W2 have the same rank, resulting in the feature matrix\nXW1 having rank c instead of min(n, p). The matrix XW1\ncollapses into c orthogonal directions, with each direction\ntrivially mapping to a class to satisfy Equation (13).\nIntuition\nLoosely speaking, W T\n1 W1 −W2W T\n2\n≈0 is\nanalogous to the small initialization in Saxe et al. (2014).2\nFrom the dynamics perspective, the greedy dynamics of\nlayerwise models (Equation (12)) sends gradients toward\nonly the relevant c directions (modes), effectively limiting\nthe rank of the feature matrix (XW1).\nPractical DNNs are often initialized with small weights and\nare expressive enough to fit the training set without using\nall layers. Because of the layerwise structure, we can expect\nthe greedy dynamics toward minimal rank to approximately\nhold even with non-linear activations, resulting in NC.\n6. From layer imbalance to lazy/rizy regimes\nSo far, we assumed that layers have similar magnitudes\n(ai ≈bi), where the feedback principle creates positive or\n2Large initialization can achieve W T\n1 W1 −W2W T\n2 ≈0 and\nsimilar dynamics; see (Braun et al., 2022; Domin´e et al., 2024).\n5\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nnegative feedback on their change. However, for heavily\nimbalanced layers (e.g., ai ≫1 ≫bi), the heavier layer\nreceives negligible gradients, remaining nearly fixed and\ndisrupting nonlinear reinforcement between layers. In this\nsection, we formalize the role of layer imbalance in linear\nneural networks and review Kunin et al. (2024) to show how\nlayer imbalance can control lazy/rich regimes (Chizat et al.,\n2019; Domin´e et al., 2024) - a linear/non-linear dynamics -\nand grokking (Power et al., 2022) - a delayed generalization\n- in neural networks.\n6.1. λ-balanced assumption\nDeriving explicit dynamics for linear neural networks with\ngeneral initialization is challenging. However, we can for-\nmalize the intuition of layer imbalance as a solvable model\nunder the following condition (Fukumizu, 2000; Arora et al.,\n2018b; Braun et al., 2022; Domin´e et al., 2024; Kunin et al.,\n2024; Varre et al., 2024):\nW2W T\n2 −W T\n1 W1 = λI,\n(14)\nwhere I is an identity matrix. Recent analytical models\n(Domin´e et al., 2024; Kunin et al., 2024; Tu et al., 2024;\nXu & Ziyin, 2024) solved the exact dynamics for a square\n(d = c) linear neural network satisfying Equation (14) and\nconfirmed the intuition on layer imbalance. They showed\nthat smaller |λ| (balanced layers) leads to nonlinear dy-\nnamics similar to greedy dynamics even with large weight\ninitialization, while larger |λ| (imbalanced layers) results\nin linear dynamics where only the lighter layer is trained\n(Figure 6(a)). For details and their relationship to the archi-\ntecture, see Appendix F.\n6.2. Application 1: Target learning a layer with layer\nimbalance\nLazy regimes (Chizat et al., 2019; Woodworth et al., 2020;\nAzulay et al., 2021) refer to when neural networks shows a\nlinear dynamics (Equation (7)) while rich regime refers to\na sufficient deviation from it (e.g., non-linear dynamics of\nEquation (6)).\nIn 2-layer linear neural networks, learning only W1 or W2\nthrough layer imbalance results in linear dynamics (lazy).\nIn 2-layer neural networks, the non-linear activations (e.g.,\nReLU) breaks the symmetry between layers: updating only\nthe last layer results in linear dynamics, whereas updating\nonly the first layer can facilitate feature learning. Kunin et\nal. (2024) demonstrated that the upstream initialization —\nheavier later layers — promotes the learning in the earlier\nlayers of convolutional neural networks and improves their\ninterpretability and feature learning.\n6.3. Application 2: Controlling grokking with layer\nimbalance\nGrokking (Power et al., 2022) refers to a delayed generaliza-\ntion. Grokking was considered as an artifact of algorithmic\ndataset, but recent studies (Kumar et al., 2024; Lyu et al.,\n2024; Kunin et al., 2024) showed that grokking is a tran-\nsition from a lazy overfitting regime to a rich generalizing\nregime. See Appendix G for details. Kunin et al. (2024)\nused upstream initialization to focus learning on earlier\nlayers, reducing the generalization delay in a transformer\ntrained on a modular arithmetic task. See Appendix F or\nKunin et al. (2024) for details.\nIntuition\nThe dynamical feedback principle, valid even\nwith non-linear activations, enables layer imbalance to con-\ntrol which layer is trained significantly. Thus, the layer\nimbalance can control lazy/rich regimes and grokking.\n0\n10\n20\nWeight-Target-Ratio\n9\n0\n-9\nLayer Imbalance \n4\n3\n2\n1\nKernel Distance (Log Scale)\n(a) Layer Imbalance\n0\n25\n50\nWeight-Target-Ratio\n5\n0\n-5\nLayer Imbalance (\n1\n2)\n5\n4\n3\n2\n1\nKernel Distance (Log Scale)\n(b) Weight-to-target Scale\nFigure 6. Two ways to get lazy dynamics. Figures show the\nchanges in NTK (measure of non-linear dynamics) of linear neural\nnetwork trained on randomized regression. (a): In Section 6,\nwhen Equation (14) holds, layer imbalance governs the transition\nbetween rich (red) and lazy (blue) dynamics. (b): In Section 7,\nwhen Equation (14) does not hold, larger weight-to-target ratio\npushes the dynamics toward lazy regime, although layer imbalance\nstill has an effect. For details of the experiment, see Appendix F.\n7. From weight-to-target ratio to grokking\nAs discussed in Section 6, sufficient layer imbalance can\nmitigate feedback effects (Figure 6(a)). At the same time,\nEquation (6) suggests that mitigation can also arise when\nai(0)bi(0) closely matches Si, allowing dynamics to com-\nplete before feedback takes effect (Figure 6(b)). In this\nsection, we use linear neural networks to formalize this in-\ntuition through the weight-to-target ratio that can explain\ntwo lazy regime methods — NTK initialization (Jacot et al.,\n2020) and target downscaling (Chizat et al., 2019) — within\na single framework. We then empirically show that reducing\nthe weight-to-target ratio eliminates the delay in grokking,\n6\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\naccelerating generalization.\n(a) Rich: Greedy Dynamics\n(b) Lazy: Target Downscaling\n(c) Lazy: NTK Initialization\ntrain\naligned\nfeature\nanti-aligned\nfeature\nanti-aligned\nfeature\naligned\nfeature\naligned\nfeature\nanti-aligned\nfeature\naligned\nfeature\nanti-aligned\nfeature\ntrain\ntrain\nFigure 7. Illustration of rich vs. lazy dynamics. The colored\narrows represent features (e.g., aibixi in Equation (15)), the dotted\narrows represent their gradients, and the black dashed arrow repre-\nsents the learned function in function space. (a): With smaller ini-\ntialization and larger S, amplifying dynamics reinforce the growth\nof the aligned feature (larger parameters lead to faster growth)\nwhile mitigating the reduction of the anti-aligned feature (smaller\nparameters lead to slower decay), resulting in the aligned feature\nto dominate. (b): With a smaller target scale (or larger initializa-\ntion), minimal adjustments per feature are sufficient to fit the target\n(lazy).(c): Under the NTK initialization with numerous features,\ngradients toward the target are distributed across the features, re-\nsulting in negligible movement for each feature to fit the target\n(lazy). See Appendix B for details of the illustration.\n7.1. Toy model for NTK initialization and target\ndownscaling\nWe assume a 2-layer p-wide linear neural network with\nscalar input x and scalar output (d = c = 1):\nf(x) = x\nZ\np\nX\ni=1\naibi,\nf ∗(x) = xS,\n(15)\nwhere x ∈R, S > 0, and Z ∈R is a normalization\nor output rescaling constant. In Appendix H, we solve\nthe exact dynamics of Equation (15) for any initialization\n(e.g., Gaussian distribution). For simplicity, we demonstrate\na special case where ai(0) and bi(0) are initialized from\n{−1, 1} with equal probabilities.\nThe dynamics can be decomposed into two modes: the\nanti-aligned mode I−= {i : ai(0)bi(0)S < 0}, and\nthe aligned mode I+ = {i : ai(0)bi(0)S > 0}, where\ni ∈{1, · · · , p}. This gives\nX\ni∈I±\nda2\ni\ndt = ∓1\nZ (θ −S)\nX\ni∈I±\na2\ni ,\n(16)\nwhere we used the conserved quantity a2\ni −b2\ni = 0 and\nexpressed θ := Z−1 Pp\ni=1 aibi so that f(x) = xθ. Let-\nting ∆θ := Z−1 \u0010P\ni∈I+ aibi −P\nj∈I−ajbj\n\u0011\n, the above\nequations can be rearranged into\ndθ\ndt = −∆θ\nZ (θ −S),\n(17)\nd(∆θ)\ndt\n= −θ\nZ (θ −S).\n(18)\nIf the right hand side of Equation (18) is 0 or ∆θ is constant,\nEquation (17) reduces to linear dynamics (Equation (7)).\nThus, variations in ∆θ reflect the differences in the learn-\ning speed of aligned and anti-aligned modes. In the rich\n(greedy) setup, the aligned mode greedily saturate before the\nanti-aligned mode (Figure 7(a)), while in the lazy (NTK ini-\ntialization and target downscale) setup, both modes evolve\ninfinitesimally.\nTarget downscaling\nTarget downscaling assumes Z = 1,\nS →0, and θ(0) = 0 (or p even). Then d∆θ\ndt\n≈0 as\nboth θ and S are close to 0. In this case, all features barely\nneed to move to fit the target (Figure 7(b)), avoiding greedy\ndynamics where a few features dominate. For more rigorous\narguments, see Chizat et al. (2019).\nNTK initialization\nThe NTK limit requires p →∞with\nZ = √p. Then ∆θ/Z = O(1) is fixed with d∆θ\ndt\n≈0 as\nboth θ and S are of O(1) as p →∞. Intuitively, ∆θ is\nfixed because the dynamics spread the required movement\nθ −S = O(1) among p features, resulting in minimal\nchange for individual features. To be specific, each feature\nmoves O(p−1/2), and the collective change of p features\nsums to O(1) change in θ. See Figure 7(c) for illustration\nand Luo et al. (2021) for more rigorous arguments.\nGeneral initialization\nIn Appendix H, we solve the exact\ndynamics of Equation (15) on general initialization of a and\nb. The analogue of ∆θ or the deviation among the features\nare captured in the following quantity\nγ+ = S +\np\nΣ2\n0 −S2\n0 + S2\nΣ0 + S0\n,\n(19)\nwhere\nS0 =\np\nX\ni=1\nai(0)bi(0)\nZ\n,\nΣ0 =\np\nX\ni=1\nai(0)2 + bi(0)2\n2Z\n. (20)\n7\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nThe constant γ+, loosely speaking, determines how ai and\nbi scale with their initial values (Theorem H.1). A large\nγ+ indicates more significant difference in contributions\nbetween more and less aligned features, while γ+ ≈1\nimplies negligible changes in ai and bi, resulting in lazy\ndynamics. See Appendix H for further discussion.\nNote that the ratio between S (target scale) and Σ0 (norm\nat initialization) determines γ+ (Remark H.3). The small\ninitialization in Sections 4 and 5 maps to small Σ0/S, while\nthe NTK initialization (by infinite width) and target down-\nscaling (by small S) map to large Σ0/S.\n7.2. Application: Controlling grokking with\nweight-to-target ratio\nAs discussed in Section 6, grokking is a transition from the\nlazy (overfitting) regime to the rich (generalizing) regime.\nInstead of waiting for the rich regime through overtraining,\nwe can set the model in the rich regime from the initial-\nization. Kumar et al. (2024) empirically verified that the\nmethods to escape the lazy regime (decreasing Σ0/S) in our\ntoy model also eliminate the delay in grokking in various\nsettings (Figure 8).\nSeveral methods decrease Σ0/S, which coincides with the\ntransition out of the lazy regime: We can increase S by\nupscaling the target or analogously downscaling the input\nfeatures (as S is the ratio between y and x). Alternatively,\nwe can decrease Σ0 by downscaling initialization (Jeffares\net al., 2024) or by downscaling the output of the function\nby increasing Z. See Appendix G or Kumar et al. (2024)\nfor empirical verification and a summary of methods that\nremove grokking.\nIntuition\nFor greedy dynamics to emerge, the target must\nbe sufficiently distant from initial output function for better\nand worse features (modes) to form a gap. Reducing the\nweight-to-target ratio widens this gap, promoting greedy\ndynamics that generalize. Based on this understanding,\nwe can interpret grokking as a special case of a poorly\ninitialized setup in the lazy regime.\n8. Discussion\nWe demonstrated how the feedback principle unifies seem-\ningly unrelated phenomena as consequences of dynamics of\nthe layerwise structure. In Sections 4 and 5, we explained\nemergence and Neural Collapse through the amplifying dy-\nnamics from feedback principle. In Sections 6 and 7, we\nlinked the lazy regime and grokking to the lack of amplifi-\ncation. Throughout, layerwise linear models with specific\nconditions served as the formal bridge between phenomenon\nand intuition.\nWhile our investigation mainly focused on layerwise lin-\nFigure 8. Removing grokking by decreasing weight-to-target\nratio (Figure. 6 of (Kumar et al., 2024)). The output rescaling\nconstant α is proportional to Z−1 (and Σ0/S) in our formalism.\nThe authors show that downscaling α to escape the lazy regime\nremoves the delay in grokking for (a): 2-layer MLP on MNIST\nand (b): one-layer transformer on modular addition problem.\near models and four phenomena, similar models have also\nbeen applied to understand other phenomena, including var-\nious architectures (Pinson et al., 2023; Heij et al., 2007;\nAhn et al., 2023; Katharopoulos et al., 2020), self super-\nvised learning (Simon et al., 2023), modularization (Jarvis\net al., 2024a; Zhang et al., 2024a), specialization (Jarvis\net al., 2024b), fine-tunning (Lippl & Lindsey, 2023), gener-\nalization (Advani et al., 2020), and neural representation in\nneuroscience (Saxe et al., 2019; Richards et al., 2019; Nelli\net al., 2023; Flesch et al., 2022; Farrell et al., 2023; Lippl\net al., 2024).\n8.1. Alternative view: Essential non-linear activations\nWhile layerwise linear models provide valuable insights, we\nacknowledge their limitations. These models are not used\nin practice, as DNNs lose expressiveness and performance\nwithout non-linearities. Some phenomena may require non-\nlinear activations for explanation, making layerwise linear\nmodels too simple. However, given their success in ex-\nplaining various phenomena — such as grokking which\nwas initially thought to be linked to complicated nature of\nalgorithmic datasets (Power et al., 2022) — the limits of\nlayerwise linear models have yet to be fully explored.\n9. Conclusion\nThe success of feedback principle in describing seemingly\nunrelated phenomena highlights that layerwise structure,\neven without non-linear activations, is a defining charac-\nteristic of DNNs. The simplicity and solvability of this\napproach in determining whether DNN phenomena result\nfrom dynamics make the approach highly appealing.\nThus, in this position paper, we argue that layerwise linear\nmodels must be solved first to understand a phenomena in\nDNNs.\n8\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\n9.1. Call to action: Research directions\nThe role of depth\nAs explored in (Kunin et al., 2024),\ninitialization can be controlled to concentrate feature learn-\ning in a few layers or distribute it across many. The former\neffectively reduces the depth, while the latter fully leverages\nit. When do these approaches differ in generalization, and\nwhy? If performances are similar, what is the true role of\ndepth — purely dynamical or something more fundamental?\nSee for example, (Lyu & Zhu, 2023; Wang & Jacot, 2023).\nCan we incorporate non-linearity as perturbation?\nPhysicists often extend simple and solvable systems with\nperturbative methods to describe more complex systems\n(Sulejmanpasic & ¨Unsal, 2018). Since ReLU is partially lin-\near, can we similarly integrate non-linearity into layerwise\nlinear models as a perturbation? Several studies have laid\nthe groundwork in this direction, highlighting the need for\ncontinued exploration (Kunin et al., 2024; Saxe et al., 2022;\nZhang et al., 2024b).\nFor additional research directions, see Appendix I.\n10. Acknowledgement\nWe thank Nayara Fonseca de S´a and SeWook Oh for\nhelpful discussions. This research was funded in whole\nor in part by the Wellcome Trust [216386/Z/19/Z]. For\nthe purpose of Open Access, the author has applied a\nCC BY public copyright licence to any Author Accepted\nManuscript version arising from this submission. SL was\nsupported by the National Research Foundation of Korea\n(NRF) grants No.2020R1A5A1016126 and No.RS–2024-\n00462910. CD was supported by the Gatsby Charitable\nFoundation (GAT3755). YP was supported by a KIAS Indi-\nvidual Grant [AP090301] via the Center for AI and Natural\nSciences at Korea Institute for Advanced Study. CL was sup-\nported by the Engineering and Physical Sciences Research\nCouncil grant EP/W524311/1.\n9\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nA. Impact Statement\nThis position paper advocates for research directions based\non a dynamic principle and intuitively explains phenom-\nena of DNN. Therefore, we assess no societal impact that\nrequires attention.\nReferences\nAdvani, M. S., Saxe, A. M., and Sompolinsky, H. High-\ndimensional dynamics of generalization error in neural\nnetworks. Neural Networks, 132:428–446, 2020.\nAhn, K., Cheng, X., Song, M., Yun, C., Jadbabaie, A.,\nand Sra, S. Linear attention is (maybe) all you need\n(to understand transformer optimization). arXiv preprint\narXiv:2310.01082, 2023.\nArora, S. and Goyal, A. A theory for emergence of complex\nskills in language models. arXiv preprint:2307.15936,\n2023.\nArora, S., Cohen, N., Golowich, N., and Hu, W. A conver-\ngence analysis of gradient descent for deep linear neural\nnetworks. arXiv preprint arXiv:1810.02281, 2018a.\nArora, S., Ge, R., Neyshabur, B., and Zhang, Y. Stronger\ngeneralization bounds for deep nets via a compression\napproach. In Proceedings of the 35th International Con-\nference on Machine Learning, volume 80 of Proceed-\nings of Machine Learning Research, pp. 254–263. PMLR,\n10–15 Jul 2018b. URL http://proceedings.mlr.\npress/v80/arora18b.html.\nArora, S., Cohen, N., Hu, W., and Luo, Y. Implicit regular-\nization in deep matrix factorization. Advances in Neural\nInformation Processing Systems, 32, 2019.\nAtanasov, A., Meterez, A., Simon, J. B., and Pehlevan,\nC. The optimization landscape of sgd across the fea-\nture learning strength. arXiv preprint arXiv:2410.04642,\n2024.\nAzulay, S., Moroshko, E., Nacson, M. S., Woodworth, B. E.,\nSrebro, N., Globerson, A., and Soudry, D. On the im-\nplicit bias of initialization shape: Beyond infinitesimal\nmirror descent. In International Conference on Machine\nLearning, pp. 468–477. PMLR, 2021.\nBachmann, G., Anagnostidis, S., and Hofmann, T. Scal-\ning mlps: A tale of inductive bias. Advances in Neural\nInformation Processing Systems, 36, 2024.\nBahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. Ex-\nplaining neural scaling laws. arXiv preprint:2102.06701,\n2021.\nBaldi, P. and Hornik, K. Neural networks and principal\ncomponent analysis: Learning from examples without\nlocal minima. Neural networks, 2(1):53–58, 1989.\nBordelon, B., Canatar, A., and Pehlevan, C.\nSpectrum\ndependent learning curves in kernel regression and wide\nneural networks. In International Conference on Machine\nLearning, pp. 1024–1034. PMLR, 2020.\nBordelon, B., Atanasov, A., and Pehlevan, C. A dynamical\nmodel of neural scaling laws. arXiv preprint:2402.01092,\n2024.\nBraun, L., Domin´e, C., Fitzgerald, J., and Saxe, A. Exact\nlearning dynamics of deep linear networks with prior\nknowledge. Advances in Neural Information Processing\nSystems, 35:6615–6629, 2022.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877–1901, 2020.\nCabannes, V., Dohmatob, E., and Bietti, A. Scaling laws for\nassociative memories. arXiv preprint arXiv:2310.02984,\n2023.\nChen, M., Roberts, N., Bhatia, K., Wang, J., Zhang, C., Sala,\nF., and R´e, C. Skill-it! a data-driven skills framework for\nunderstanding and training language models. Advances\nin Neural Information Processing Systems, 36, 2023.\nChizat, L., Oyallon, E., and Bach, F. On lazy training in\ndifferentiable programming. Advances in neural informa-\ntion processing systems, 32, 2019.\nCui, H., Loureiro, B., Krzakala, F., and Zdeborov´a, L. Gen-\neralization error rates in kernel regression: The crossover\nfrom the noiseless to noisy regime. Advances in Neural\nInformation Processing Systems, 34:10131–10143, 2021.\nDomin´e, C. C., Anguita, N., Proca, A. M., Braun, L., Kunin,\nD., Mediano, P. A., and Saxe, A. M. From lazy to rich:\nExact learning dynamics in deep linear networks. arXiv\npreprint arXiv:2409.14623, 2024.\nFang, C., He, H., Long, Q., and Su, W. J. Layer-peeled\nmodel: Toward understanding well-trained deep neural\nnetworks. arXiv preprint arXiv:2101.12699, 4, 2021.\nFarrell, M., Recanatesi, S., and Shea-Brown, E. From lazy to\nrich to exclusive task representations in neural networks\nand neural codes. Current opinion in neurobiology, 83:\n102780, 2023.\nFlesch, T., Juechems, K., Dumbalska, T., Saxe, A., and\nSummerfield, C. Orthogonal representations for robust\n10\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\ncontext-dependent task performance in brains and neural\nnetworks. Neuron, 110(7):1258–1270, 2022.\nFort, S., Dziugaite, G. K., Paul, M., Kharaghani, S., Roy,\nD. M., and Ganguli, S.\nDeep learning versus kernel\nlearning: an empirical study of loss landscape geome-\ntry and the time evolution of the neural tangent kernel.\nAdvances in Neural Information Processing Systems, 33:\n5850–5861, 2020.\nFukumizu, K. Statistical active learning in multilayer per-\nceptrons. IEEE Transactions on Neural Networks, 11(1):\n17–26, 2000.\nFuruta, H., Minegishi, G., Iwasawa, Y., and Matsuo, Y.\nTowards empirical interpretation of internal circuits and\nproperties in grokked transformers on modular polynomi-\nals. Transactions on Machine Learning Research, 2024.\nGanguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y.,\nChen, A., Conerly, T., Dassarma, N., Drain, D., Elhage,\nN., et al. Predictability and surprise in large generative\nmodels. In Proceedings of the 2022 ACM Conference\non Fairness, Accountability, and Transparency, pp. 1747–\n1764, 2022.\nGidel, G., Bach, F., and Lacoste-Julien, S. Implicit regu-\nlarization of discrete gradient dynamics in linear neural\nnetworks. Advances in Neural Information Processing\nSystems, 32, 2019.\nGordon, M. A., Duh, K., and Kaplan, J. Data and parameter\nscaling laws for neural machine translation. In Moens,\nM.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Pro-\nceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pp. 5915–5922, On-\nline and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.emnlp-main.478. URL https://\naclanthology.org/2021.emnlp-main.478.\nGromov, A. Grokking modular arithmetic. arXiv preprint\narXiv:2301.02679, 2023.\nHan, X., Papyan, V., and Donoho, D. L. Neural collapse un-\nder MSE loss: Proximity to and dynamics on the central\npath. In International Conference on Learning Represen-\ntations, 2022. URL https://openreview.net/\nforum?id=w1UbdvWH_R3.\nHe, T., Doshi, D., Das, A., and Gromov, A. Learning to\ngrok: Emergence of in-context learning and skill com-\nposition in modular arithmetic tasks.\narXiv preprint\narXiv:2406.02550, 2024.\nHeij, C., Ran, A. C., and Van Schagen, F. Introduction to\nmathematical systems theory. Springer, 2007.\nHenighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C.,\nJackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S.,\net al. Scaling laws for autoregressive generative modeling.\narXiv preprint:2010.14701, 2020.\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\nKianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y.\nDeep learning scaling is predictable, empirically. arXiv\npreprint:1712.00409, 2017.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal large\nlanguage models. arXiv preprint:2203.15556, 2022.\nHopfield, J. J. Hopfield network. Scholarpedia, 2(5):1977,\n2007.\nHumayun, A. I., Balestriero, R., and Baraniuk, R. Deep\nnetworks always grok and here is why.\nIn Forty-\nfirst International Conference on Machine Learning,\n2024. URL https://openreview.net/forum?\nid=zMue490KMr.\nHutter,\nM.\nLearning\ncurve\ntheory.\narXiv\npreprint:2102.04074, 2021.\nHuygens, C. 1673 horologium oscillatorium, sive, de motu\npendulorum ad horologia aptato demonstrationes geomet-\nricae. Paris: Apud F. Muguet, 1966.\nJacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel:\nConvergence and generalization in neural networks. In\nAdvances in neural information processing systems, pp.\n8571–8580, 2018.\nJacot, A., Simsek, B., Spadaro, F., Hongler, C., and Gabriel,\nF. Kernel alignment risk estimator: Risk prediction from\ntraining data. Advances in Neural Information Processing\nSystems, 33:15568–15578, 2020.\nJacot, A., Ged, F., S¸ims¸ek, B., Hongler, C., and Gabriel,\nF.\nSaddle-to-saddle dynamics in deep linear net-\nworks: Small initialization training, symmetry, and spar-\nsity, 2022. URL https://arxiv.org/abs/2106.\n15933.\nJarvis, D., Klein, R., Rosman, B., and Saxe, A. M. On\nthe specialization of neural modules.\narXiv preprint\narXiv:2409.14981, 2024a.\nJarvis, D., Lee, S., Domin´e, C. C. J., Saxe, A. M., and\nMannelli, S. S. A theory of initialisation’s impact on spe-\ncialisation. In NeurIPS 2024 Workshop on Mathematics\nof Modern Machine Learning, 2024b.\nJeffares, A., Curth, A., and van der Schaar, M. Deep learn-\ning through a telescoping lens: A simple model provides\n11\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nempirical insights on grokking, gradient boosting & be-\nyond. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024. URL https:\n//openreview.net/forum?id=NhucGZtikE.\nJi, Z. and Telgarsky, M. Gradient descent aligns the layers of\ndeep linear networks. arXiv preprint arXiv:1810.02032,\n2018.\nKaiser, D. The sacred, spherical cows of physics. Nautilus\nQuaterly, 2014.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint:2001.08361, 2020.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In International conference on ma-\nchine learning, pp. 5156–5165. PMLR, 2020.\nKumar, T., Bordelon, B., Gershman, S. J., and Pehlevan,\nC. Grokking as the transition from lazy to rich train-\ning dynamics.\nIn The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=vt5mnLVIVo.\nKunin, D., Raventos, A., Domin´e, C. C. J., Chen, F., Klindt,\nD., Saxe, A. M., and Ganguli, S. Get rich quick: ex-\nact solutions reveal how unbalanced initializations pro-\nmote rapid feature learning. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems,\n2024. URL https://openreview.net/forum?\nid=eNM94i7R3A.\nLampinen, A. K. and Ganguli, S. An analytic theory of\ngeneralization dynamics and transfer learning in deep\nlinear networks. arXiv preprint arXiv:1809.10374, 2018.\nLee, J., Kang, B. G., Kim, K., and Lee, K. M. Grokfast:\nAccelerated grokking by amplifying slow gradients. arXiv\npreprint arXiv:2405.20233, 2024.\nLevi, N. I., Beck, A., and Bar-Sinai, Y. Grokking in lin-\near estimators – a solvable model that groks without\nunderstanding.\nIn The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=GH2LYb9XV0.\nLi, Z., Luo, Y., and Lyu, K. Towards resolving the implicit\nbias of gradient descent for matrix factorization: Greedy\nlow-rank learning.\narXiv preprint arXiv:2012.09839,\n2020.\nLippl, S. and Lindsey, J. W. Inductive biases of multi-task\nlearning and finetuning: multiple regimes of feature reuse.\narXiv preprint arXiv:2310.02396, 2023.\nLippl, S., Kay, K., Jensen, G., Ferrera, V. P., and Abbott, L.\nA mathematical theory of relational generalization in tran-\nsitive inference. Proceedings of the National Academy of\nSciences, 121(28):e2314511121, 2024.\nLiu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark,\nM., and Williams, M. Towards understanding grokking:\nAn effective theory of representation learning. Advances\nin Neural Information Processing Systems, 35:34651–\n34663, 2022a.\nLiu, Z., Michaud, E. J., and Tegmark, M.\nOmnigrok:\nGrokking beyond algorithmic data.\nIn The Eleventh\nInternational Conference on Learning Representations,\n2022b.\nLuo, T., Xu, Z.-Q. J., Ma, Z., and Zhang, Y. Phase dia-\ngram for two-layer relu neural networks at infinite-width\nlimit. Journal of Machine Learning Research, 22(71):\n1–47, 2021.\nLyu, B. and Zhu, Z. Implicit bias of (stochastic) gradient\ndescent for rank-1 linear neural network. In Oh, A., Nau-\nmann, T., Globerson, A., Saenko, K., Hardt, M., and\nLevine, S. (eds.), Advances in Neural Information Pro-\ncessing Systems, volume 36, pp. 58166–58201. Curran\nAssociates, Inc., 2023.\nLyu, K., Jin, J., Li, Z., Du, S. S., Lee, J. D., and Hu,\nW.\nDichotomy of early and late phase implicit bi-\nases can provably induce grokking.\nIn The Twelfth\nInternational Conference on Learning Representations,\n2024. URL https://openreview.net/forum?\nid=XsHqr9dEGH.\nMaloney, A., Roberts, D. A., and Sully, J. A solvable model\nof neural scaling laws. arXiv preprint:2210.16859, 2022.\nMarcotte, S., Gribonval, R., and Peyr´e, G. Abide by the law\nand follow the flow: Conservation laws for gradient flows.\nAdvances in neural information processing systems, 36,\n2024.\nMichaud, E., Liu, Z., Girit, U., and Tegmark, M. The\nquantization model of neural scaling. Advances in Neural\nInformation Processing Systems, 36, 2023.\nMixon, D. G., Parshall, H., and Pi, J. Neural collapse with\nunconstrained features. arXiv preprint arXiv:2011.11619,\n2020.\nMohamadi, M. A., Li, Z., Wu, L., and Sutherland, D. J. Why\ndo you grok? a theoretical analysis on grokking modular\naddition. In Forty-first International Conference on Ma-\nchine Learning, 2024. URL https://openreview.\nnet/forum?id=ad5I6No9G1.\n12\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nNam, Y., Fonseca, N., Lee, S. H., Mingard, C., and Louis,\nA. A. An exactly solvable model for emergence and\nscaling laws in the multitask sparse parity problem.\nIn The Thirty-eighth Annual Conference on Neural In-\nformation Processing Systems, 2024a. URL https:\n//openreview.net/forum?id=cuWsR25bbI.\nNam, Y., Mingard, C., Lee, S. H., Hayou, S., and Louis,\nA. Visualising feature learning in deep neural networks\nby diagonalizing the forward feature map, 2024b. URL\nhttps://arxiv.org/abs/2410.04264.\nNanda, N., Chan, L., Lieberum, T., Smith, J., and Stein-\nhardt, J. Progress measures for grokking via mechanistic\ninterpretability. In The Eleventh International Confer-\nence on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=9XFSbDPmdW.\nNelli, S., Braun, L., Dumbalska, T., Saxe, A., and Sum-\nmerfield, C. Neural knowledge assembly in humans and\nneural networks. Neuron, 111(9):1504–1516, 2023.\nOkawa, M., Lubana, E. S., Dick, R., and Tanaka, H. Com-\npositional abilities emerge multiplicatively: Exploring\ndiffusion models on a synthetic task. Advances in Neural\nInformation Processing Systems, 36, 2024.\nPapyan, V., Han, X., and Donoho, D. L. Prevalence of\nneural collapse during the terminal phase of deep learn-\ning training. Proceedings of the National Academy of\nSciences, 117(40):24652–24663, 2020.\nPesme, S. and Flammarion, N.\nSaddle-to-saddle dy-\nnamics in diagonal linear networks. In Thirty-seventh\nConference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?\nid=iuqCXg1Gng.\nPinson, H., Lenaerts, J., and Ginis, V. Linear cnns discover\nthe statistical structure of the dataset using only the most\ndominant frequencies. In International Conference on\nMachine Learning, pp. 27876–27906. PMLR, 2023.\nPower, A., Burda, Y., Edwards, H., Babuschkin, I., and\nMisra, V. Grokking: Generalization beyond overfitting\non small algorithmic datasets. arXiv:2201.02177, 2022.\nRichards, B. A., Lillicrap, T. P., Beaudoin, P., Bengio, Y.,\nBogacz, R., Christensen, A., Clopath, C., Costa, R. P.,\nde Berker, A., Ganguli, S., et al. A deep learning frame-\nwork for neuroscience.\nNature neuroscience, 22(11):\n1761–1770, 2019.\nRosenfeld, J. S., Rosenfeld, A., Belinkov, Y., and Shavit,\nN. A constructive prediction of the generalization error\nacross scales. arXiv preprint: 1909.12673, 2019.\nRubin, N., Seroussi, I., and Ringel, Z. Grokking as a first or-\nder phase transition in two layer networks. In The Twelfth\nInternational Conference on Learning Representations,\n2024. URL https://openreview.net/forum?\nid=3ROGsTX3IR.\nSaxe, A., Sodhani, S., and Lewallen, S. J. The neural race\nreduction: Dynamics of abstraction in gated networks.\nIn International Conference on Machine Learning, pp.\n19287–19309. PMLR, 2022.\nSaxe, A. M., McClelland, J. L., and Ganguli, S. Exact\nsolutions to the nonlinear dynamics of learning in deep\nlinear neural networks. Proceedings of the International\nConference on Learning Representations 2014, 2014.\narXiv:1312.6120.\nSaxe, A. M., McClelland, J. L., and Ganguli, S. A mathe-\nmatical theory of semantic development in deep neural\nnetworks. Proceedings of the National Academy of Sci-\nences, 116(23):11537–11546, 2019.\nSchaeffer, R., Miranda, B., and Koyejo, S. Are emergent\nabilities of large language models a mirage? Advances in\nNeural Information Processing Systems, 36, 2023.\nSimon, J. B., Knutins, M., Ziyin, L., Geisz, D., Fetter-\nman, A. J., and Albrecht, J. On the stepwise nature of\nself-supervised learning. In International Conference on\nMachine Learning, pp. 31852–31876. PMLR, 2023.\nSpigler, S., Geiger, M., and Wyart, M. Asymptotic learning\ncurves of kernel methods: empirical data versus teacher–\nstudent paradigm. Journal of Statistical Mechanics: The-\nory and Experiment, 2020(12):124001, 2020.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language\nmodels. arXiv preprint:2206.04615, 2022.\nSulejmanpasic, T. and\n¨Unsal, M.\nAspects of per-\nturbation theory in quantum mechanics:\nThe ben-\nderwu mathematica ® package.\nComputer Physics\nCommunications, 228:273–289, 2018.\nISSN 0010-\n4655.\ndoi:\nhttps://doi.org/10.1016/j.cpc.2017.11.\n018. URL https://www.sciencedirect.com/\nscience/article/pii/S0010465518300420.\nTarmoun, S., Franca, G., Haeffele, B. D., and Vidal, R.\nUnderstanding the dynamics of gradient flow in overpa-\nrameterized linear models. In International Conference\non Machine Learning, pp. 10153–10161. PMLR, 2021.\nThilak, V., Littwin, E., Zhai, S., Saremi, O., Paiss, R., and\nSusskind, J. M. The slingshot mechanism: An empirical\nstudy of adaptive optimizers and the\\emph {Grokking\n13\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nPhenomenon}. In Has it Trained Yet? NeurIPS 2022\nWorkshop, 2022.\nTu, Z., Aranguri, S., and Jacot, A. Mixed dynamics in linear\nnetworks: Unifying the lazy and active regimes. arXiv\npreprint arXiv:2405.17580, 2024.\nVarre, A. V., Vladarean, M.-L., Pillaud-Vivien, L., and\nFlammarion, N. On the spectral bias of two-layer linear\nnetworks. Advances in Neural Information Processing\nSystems, 36, 2024.\nWang, B., Yue, X., Su, Y., and Sun, H. Grokking of im-\nplicit reasoning in transformers: A mechanistic journey\nto the edge of generalization. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems,\n2024. URL https://openreview.net/forum?\nid=D4QgSWxiOb.\nWang, Z. and Jacot, A.\nImplicit bias of sgd in l {2}-\nregularized linear dnns: One-way jumps from high to\nlow rank. arXiv preprint arXiv:2305.16038, 2023.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language models.\narXiv preprint: 2206.07682, 2022.\nWoodworth, B., Gunasekar, S., Lee, J. D., Moroshko, E.,\nSavarese, P., Golan, I., Soudry, D., and Srebro, N. Ker-\nnel and rich regimes in overparametrized models. In\nConference on Learning Theory, pp. 3635–3673. PMLR,\n2020.\nXu, Y. and Ziyin, L. When does feature learning happen?\nperspective from an analytically solvable model. arXiv\npreprint arXiv:2401.07085, 2024.\nYu, D., Kaur, S., Gupta, A., Brown-Cohen, J., Goyal, A.,\nand Arora, S. Skill-mix: A flexible and expandable family\nof evaluations for ai models. arXiv preprint:2310.17567,\n2023.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\nvision transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 12104–12113, 2022.\nZhang, Y., Latham, P. E., and Saxe, A. M.\nUnder-\nstanding unimodal bias in multimodal deep linear net-\nworks. In Forty-first International Conference on Ma-\nchine Learning, 2024a. URL https://openreview.\nnet/forum?id=CTEMHDSwIj.\nZhang, Y., Saxe, A., and Latham, P. E. When are bias-\nfree relu networks like linear networks? arXiv preprint\narXiv:2406.12615, 2024b.\nZheng, X., Daruwalla, K., Benjamin, A. S., and Klindt,\nD. Delays in generalization match delayed changes in\nrepresentational geometry. In UniReps: 2nd Edition of the\nWorkshop on Unifying Representations in Neural Models,\n2024. URL https://openreview.net/forum?\nid=1ae108kHk2.\nZhong, Z., Liu, Z., Tegmark, M., and Andreas, J. The clock\nand the pizza: Two stories in mechanistic explanation\nof neural networks. Advances in Neural Information\nProcessing Systems, 36, 2024.\nZhu, X., Fu, Y., Zhou, B., and Lin, Z. Critical data size\nof language models from a grokking perspective. arXiv\npreprint arXiv:2401.10463, 2024.\nZhu, Z., Ding, T., Zhou, J., Li, X., You, C., Sulam, J., and\nQu, Q. A geometric analysis of neural collapse with\nunconstrained features. Advances in Neural Information\nProcessing Systems, 34:29820–29834, 2021.\n14\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nA. Glossary\nName\nSymbol\n/Abbr.\nDefinition\nCross-ref\nAmplifying dynamics\nThe rich-get-richer dynamics of layerwise linear models\nstemming from the dynamical feedback principle.\nSection 4\nDimension of input features\nd\nDimension of hidden layer\np\nDimension of output\nc\nDynamical feedback princi-\nple\nThe principle that magnitude of one layer governs the\nevolution of the other layers.\nSection 3\nEmergence\nA phenomenon where DNN suddenly gains performance\non a subtask or skill.\nAppendix D\nGradient flow\nGF\nA continuous variant of gradient descent algorithm\nGreedy dynamics\nThe dynamics of low-rank bias.\nSection 5\nGrokking\nA delayed generalization.\nAppendix G\nInput features\nx\nx ∈Rd. We notate the element as xi and the input feature\nspace with X.\nSection 2\nLayerwise linear models\nTwo layer models in which the output is multilinear with\nrespect to each layer of parameters.\nAppendix B\nLazy regime\nRegime where DNN’s evolution follows a linear dynamics\nSection 6\nLinear dynamics\nThe dynamics of linear model\nEquation (7)\nLinear model\nOne layer model in which the output is linear with respect\nto parameters.\nAppendix B\nLow-rank bias\nAn inductive bias of linear neural networks toward low-\nrank representation.\nSection 5\nMode\nA distinct degree of freedom the dynamical system decou-\npled into.\nAppendix B\nMultilayer perceptron\nMLP\nAlso known as fully connected network (FCN)\nNeural collapse\nNC\nThe collapse of training set’s last layer features vectors\ninto a low-rank structure.\nAppendix E\nNormalization constant\nZ\nThe constant that rescales the output of the model.\nEquation (15)\nNeural tangent kernel\nNTK\nkernel with the gradient operator as the feature map.\nParameters (diagonal linear\nneural network)\na, b\na, b ∈Rd ai, bi are used to indicate the ith entry. In\nSection 7, the notation a, b ∈Rp was duplicately used for\nthe parameters of linear neural networks for demonstrative\npurposes.\nEquation (3)\nParameters (linear neural net-\nwork)\nW1, W2\nW1 ∈Rd×p, W2 ∈Rp×c. W (1)\nij , W (2)\nij\nare used to indi-\ncate the element of matrix.\nEquation (4)\nRich regime\nRegime where DNN’s evolution follows a non-linear dy-\nnamics\nSection 6\nSigmoidal growth\nThe evolution amplifying dynamics when a ≈b.\nEquation (9)\nSkill\ngk(x)\nA subtask of the problem.\nSection 4\nStage-like training\nThe dynamics when modes are learned in sequences with-\nout overlap in their saturation.\nFigure 3(b)\nTarget (scale)\nSi or S\nThe multiplicative constant to the feature in the target\nfunction.\nEquation (1)\nWeight-to-target ratio\nΣ0/S\nThe ratio between the normalized (by Z) norm of initial\nweights (Equation (20)) and the target scale S.\nSection 7\n15\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nB. Definitions\nIn this section, we provide more detailed definitions of the following terms: linear models, layerwise linear models, modes,\nfeatures, function space. We additionally explain Figure 7. We assume scalar output (c = 1) and focus on models with\nf : X →R where X is the input feature space.\nB.1. Linear models\nLinear models are models in which the output is completely linear in parameters. The definition allows non-linear\nrelationship between the input features and the output. The general form of a linear model is\nf(x) =\nd\nX\ni=1\nϕi(x)θi,\n(21)\nwhere ϕi : X →R is a feature map that can be a non-linear function.\nB.2. Layerwise linear models\nLayerwise linear models are 2-layer models in which the output is multilinear with respect to each layer of parameters. The\ndefinition also allows non-linear relationship between the input features and the output. The general form of a layerwise\nlinear model is\nf(x) =\nX\n(i,j)∈V\np\nX\nj=1\nϕi(x)aijbj,\n(22)\nwhere ϕ : X →R is a map from the input data space X to the reals, and V is an architecture dependent set of connections\nbetween ϕi(x) and hidden neurons. Note that the presence of ϕ in place of xi does not change the dynamics, as long as\nrandom variable ϕi(x) satisfy the assumptions in the main text, such as E[ϕi(x)ϕj(x)] = 0 for i ̸= j.\nB.3. Modes\nFor a given dynamics, we define modes as distinct degrees of freedom into which the dynamical system decouples.\nB.4. Features\nWe return to the special case of ϕi(x) = xi to maintain greater consistency with the main text. For the last layer feature\nvectors used in Section 5, see Appendix E instead.\nWe define the kth feature as the post-activation of the kth neuron of the last layer multiplied by the respective last layer\nparameter. For a linear model, the kth feature is\nhk := xkθk,\nk ∈{1, 2, · · · , d}.\n(23)\nFor a linear neural network, the kth feature is\nhk :=\nd\nX\ni=1\nxiW (1)\nik bk,\nk ∈{1, 2, · · · , p}.\n(24)\nNote that by the definition, the sum of all features equals the output function:\nf =\nX\nk\nhk.\n(25)\nB.5. Function space and the explanation of Figure. 7\nWe define the function space as the vector space spanned by x1, · · · , xd: that is, we only consider linear functionals in\nvariables x1, · · · , xd among all functionals on X. Because x is a d dimensional random variable with full ranked covariance\nmatrix, x1, · · · , xd are linearly independent and thus form the basis of function space.\n16\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\n(a) Rich: Greedy Dynamics\n(b) Lazy: Target Downscaling\n(c) Lazy: NTK Initialization\ntrain\naligned\nfeature\nanti-aligned\nfeature\nanti-aligned\nfeature\naligned\nfeature\naligned\nfeature\nanti-aligned\nfeature\naligned\nfeature\nanti-aligned\nfeature\ntrain\ntrain\nFigure 9. Figure 7 in the main text.\nGiven our orthogonality condition of features (E[xixj] = 0 for i ̸= j), we can use the input feature as the (unnormalized)\nbasis, and express the target function f ∗(x) = Pd\ni xiSi as a vector in this function space with coordinate [S1, S2, · · · , Sd].\nLikewise, the features can be expressed as a vector in the function space, allowing visualization.\nIn Figure 9(a), we have two axis because d = 2 ([x1, x2]), and two features because p = 2 (Equation (24)). The blue feature\nis aligned (E[hT f ∗] > 0) and the red feature is anti-aligned (E[hT f ∗] < 0) to the target, analogous to the aligned and\nanti-aligned features in the d = 1 setup of the main text (Equation (15)). The sum of features equals the current output\nfunction (Equation (25)) or the black dashed line. The training ends when the output function equals the target function\n(black dot).\nIn Figure 9(c), we observe p ≫1 features (colored arrows) sum to a smaller output function (dashed black line). A collection\nof infinitesimal changes in these features can sum up to an O(1) change in expressing the target function.\n17\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nC. Derivation for completeness\nThis section includes simple derivations for completeness. For the original derivations in a more general setup, we refer to\nthe references mentioned in the main text.\nC.1. Dynamics of linear model\nThe gradient flow equation is\ndθi\ndt = −∂L\n∂θi\n,\n(26)\nwhere L is the MSE loss (Equation (2)). Then\ndθi\ndt = −E\n\u0014 ∂\n∂θi\n1\n2(f(x) −f ∗(x))2\n\u0015\n(27)\n= −E\n\nxi\n\n\np\nX\nj=1\n(θj −Sj)xj\n\n\n\n\n(28)\n= −E\n\u0002\nx2\ni\n\u0003\n(θi −Si).\n(29)\nIn the second line, we used the definition of the linear model (Equation (5)) and the target function (Equation (1)). In the last\nline, we used our assumptions that the input features are orthogonal (E[xixj] = 0 if i ̸= j) and that we have an infinite\ntraining set (the expectation is over the true distribution).\nSolving Equation (29), we obtain\nlog(Si −θi(t)) −log(Si −θ(0)) = −E\n\u0002\nx2\ni\n\u0003\nt.\n(30)\nUsing the zero initialization condition of θ(0) = 0, we obtain Equation (10):\nθi/Si = 1 −e−E[x2\ni ]t.\n(31)\nC.2. Dynamics of diagonal linear neural network\nStarting with the gradient flow equation in diagonal linear neural networks,\ndai\ndt = −∂L\n∂ai\n,\ndbi\ndt = −∂L\n∂bi\n.\n(32)\nAnalogous to Equation (29), we can use the orthogonality condition (E[xixj] = 0 if i ̸= j) to get\ndai\ndt = −E\n\u0014 ∂\n∂ai\n1\n2(f(x) −f ∗(x))2\n\u0015\n(33)\n= −E\n\nbixi\n\n\np\nX\nj=1\n(ajbj −Sj)xj\n\n\n\n\n(34)\n= −biE\n\u0002\nx2\ni\n\u0003\n(aibi −Si).\n(35)\nWe can analogously obtain dbi\ndt , and the evolution of aibi is\nd(aibi)\ndt\n= dai\ndt bi + ai\ndbi\ndt\n(36)\n= −E\n\u0002\nx2\ni\n\u0003\n(b2\ni + a2\ni )(aibi −S)\n(37)\n= −2E\n\u0002\nx2\ni\n\u0003\naibi(aibi −S),\n(38)\n18\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nwhere we used Equation (35) (and its equivalent for bi) in the second line and the condition ai = bi in the last line. Assuming\nai(0)bi(0) < S, we can solve the differential equation to obtain\nai(t)bi(t) =\nSi\n1 +\n\u0010\nSi\nai(0)bi(0) −1\n\u0011\ne−2SiE[x2\ni ]t .\n(39)\nFor a general derivation when ai ̸= bi, see Appendix A of Saxe et al. (2014).\nC.3. Derivation of the magnitude difference conservation in linear neural network\nFor the sake of readability, we restate the dimensions of inputs, weights, and outputs (labels) for linear neural networks:\nx ∈Rd×1,\ny ∈Rc×1,\nW1 ∈Rd×p,\nW2 ∈Rp×c.\n(40)\nFor a linear neural network, the gradient flow equation is\ndW1\ndt\n= −∂L\n∂W1\n,\ndW2\ndt\n= −∂L\n∂W2\n.\n(41)\nThe loss in the summand notation is\nL = 1\n2E\n\nX\nk\n\nX\ni,j\nxiw(1)\nij w(2)\njk −yk\n\n\n\nX\ni′,j′\nxi′w(1)\ni′j′w(2)\nj′k −yk\n\n\n\n,\n(42)\nwhere w(1)\nij and w(2)\njk are elements of matrix W1 and W2, respectively. The derivative for the first matrix W1 is\n∂L\n∂w(1)\nij\n= E\n\nX\nk\n\u0010\nxiw(2)\njk\n\u0011\n\nX\ni′,j′\nxi′w(1)\ni′j′w(2)\nj′k −yk\n\n\n\n\n(43)\n= E\n\nxi\n\nX\ni′,j′,k\nxi′w(1)\ni′j′w(2)\nj′kw(2)\njk −ykw(2)\njk\n\n\n\n\n(44)\nand this can be expressed in the matrix form as\n∂L\n∂W1\n= E\n\u0002\nx(xT W1W2 −yT )W T\n2\n\u0003\n.\n(45)\nLikewise for W2,\n∂L\n∂w(2)\njk\n= E\n\n\n X\ni\nxiw(1)\nij\n! \nX\ni′,j′\nxi′w(1)\ni′j′w(2)\nj′k −yk\n\n\n\n,\n(46)\nwhich in the matrix form is\n∂L\n∂W2\n= E\n\u0002\nW T\n1 x(xT W1W2 −yT )\n\u0003\n.\n(47)\nUsing Equations (41), (45) and (47), we obtain\nW T\n1\ndW1\ndt\n= dW2\ndt W T\n2\n⇒\nd\ndt(W T\n1 W1 −W2W T\n2 ) = 0.\n(48)\nC.4. Derivation of Equation (12)\nHere, we show that a special initialization of W1 and W2 leads to Equation (12) and argue that a sufficiently small\ninitialization makes the assumption plausible. For a more rigorous arguments, see (Saxe et al., 2014; Mixon et al., 2020; Ji\n19\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\n& Telgarsky, 2018; Arora et al., 2018a; Lampinen & Ganguli, 2018; Gidel et al., 2019; Tarmoun et al., 2021; Braun et al.,\n2022; Domin´e et al., 2024), especially more recent works for formal handling of initialization.\nLet the singular value decomposition of the correlation matrix be\nU T PV = E\n\u0002\nxyT \u0003\n.\n(49)\nWe assume whitened input E[xxT ] = I, W1(0) = U T AR, and W2(0) = RT AV . The diagonal matrix A is of rank c and\ncomprises sufficiently small singular values, while R ∈Rc×c is an orthogonal matrix. These assumptions on W1 and W2\nimmediately shows that layers are balanced:\nW T\n1 W1 −W2W T\n2 = 0.\n(50)\nDenote\nf\nW1 = UW1RT ,\nf\nW2 = RW2V T .\n(51)\nThen from the gradient dynamics\ndW1\ndt\n= −E[xxT ]W1W2W T\n2 + E[xyT ]W T\n2 = −W1W2W T\n2 + U T PV W T\n2 ,\n(52)\nwe have\ndf\nW1\ndt\n= d(UW1RT )\ndt\n= −(UW1RT )(RW2V T )(V W T\n2 RT ) + (UU T )P(V W T\n2 RT )\n(53)\n= −f\nW1f\nW2f\nW T\n2 + P f\nW T\n2 .\n(54)\nSimilarly, for f\nW2,\ndf\nW2\ndt\n= −f\nW T\n1 f\nW1f\nW2 + f\nW T\n1 P.\n(55)\nAs both f\nW1 and f\nW2 starts with diagonal matrix f\nW1(0) = f\nW2(0) = A, and all matrices in Equation (54) and Equation (55)\nare diagonal, f\nW1 and f\nW2 are always diagonal. Also from\nf\nW T\n1 f\nW1 −f\nW2f\nW T\n2 = R(W T\n1 W1 −W2W T\n2 )RT = 0,\n(56)\nwe have f\nW1(t) = f\nW2(t) for any t. Denote by αi the ith diagonal entry of f\nW1. We have either from Equation (54) or\nEquation (55),\ndαi\ndt = −α3\ni + ρiαi,\n(57)\nwhere ρi is the ith diagonal entry of P or the ith singular value of the correlation matrix. Solving this equation gives\ndα2\ni\ndt = −2α2\ni (α2\ni −ρi)\n⇒\nα2\ni (t)\nρi\n=\n1\n1 +\n\u0010\nρi\nα2\ni (0) −1\n\u0011\ne−2ρit ,\n(58)\nand the dynamics of W1W2 is given as\nW1W2 = U T A(t)2V,\nA(t)2 =\n\n\n\n\nα1(t)2\nα2(t)2\n...\n\n\n\n.\n(59)\nDiscussion on the assumption\nThe assumption specifies a particular form for the matrix, but with sufficiently small\ninitialization, all matrices approximately meet the conditions.\n20\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nD. Emergence and scaling laws\nIn this section, we review the emergence and (neural) scaling laws in the literature and discuss in detail how sigmoidal\ndynamics explain emergence, particularly in relation to data and parameters.\nD.1. Scaling law\nNeural scaling laws refer to the empirical observation that the performance of DNNs follows a power-law relationship with\nthe number of parameters or the amount of data (Hestness et al., 2017; Kaplan et al., 2020). This phenomenon has been\nobserved across various architectures and datasets (Rosenfeld et al., 2019; Henighan et al., 2020; Gordon et al., 2021; Zhai\net al., 2022; Hoffmann et al., 2022; Cabannes et al., 2023; Bachmann et al., 2024).\nTo explain the data scaling law, Hutter (2021) developed a discrete feature model where the frequency of features follows a\npower-law, leading to a power-law relationship between performance and the number of datapoints. Similar assumptions\nhave been used in recent works, often using linear models such as kernels, that a power-law in input distribution can explain\nscaling laws (Spigler et al., 2020; Bordelon et al., 2020; Cui et al., 2021; Bahri et al., 2021; Maloney et al., 2022; Bordelon\net al., 2024).\nD.2. Emergence\nIn machine learning, emergence refers to a sudden improvement in DNN’s performance when a key factor (such as the size\nof the training set or the number of parameters) increases (Ganguli et al., 2022; Srivastava et al., 2022; Wei et al., 2022).\nSuch observations are often made in large language models (Brown et al., 2020), where LLMs demonstrate certain abilities,\nsuch as logical reasoning, once the model size exceeds a specific threshold. For example, Wei et al. (2022) quoted “An\nability is emergent if it is not present in smaller models but is present in larger models.”\nAlthough a concrete definition of emergence in machine learning remains elusive and has been criticized for its sensitivity\nto measurement (Schaeffer et al., 2023), many theoretical studies have explored its causes. Emergence has been deemed\nchallenging to explain through “mathematical analysis of gradient-based training” (Arora & Goyal, 2023) and is often\nviewed as a collection of basic skills that interact to suddenly produce a complex ability (Arora & Goyal, 2023; Chen et al.,\n2023; Yu et al., 2023; Okawa et al., 2024).\nD.3. Scaling laws from emergence\nSimilar to the approach of Hutter (2021), Michaud et al. (2023) proposed a quanta model and the multitask sparse parity\ndataset to link emergence (a sudden gain in ability) to the scaling laws. The quanta model posits that a skill is either learned\nor not, depending on the available resources (time, data, and parameters). The multitask sparse parity problem consists\nof skills whose frequencies follow a power law. By leveraging the power-law distribution of the skill frequencies, they\ndemonstrated that the sum of quantized learning, in the large limit, leads to scaling laws observed in a 2-layer MLP trained\non the multitask sparse parity problem.\nD.4. Emergence from dynamics\nAs discussed in Section 4, authors of Nam et al. (2024a) proposed that emergence arises from dynamics. They used a\nlayerwise linear model with prebuilt skill functions (Equation (11)). The learning of a skill corresponds to the learning of a\nmode, and the sigmoidal dynamics naturally explain the time emergence (Figure 4(a)). However, the connections to data and\nparameter emergence (Figure 4(b,c)) were less explained in the main text.\nIn their model, they assumed that only the first N most frequent skill functions (with the largest E[gi(x)2]) were available to\nthe model, and that the skill functions were mutually exclusive:\nf(x; a, b) =\nN\nX\ni=1\naibigi(x),\ngi(x)gj(x) = 0 if i ̸= j.\n(60)\nThe two conditions state that a model will learn the kth skill if and only if 1) a datapoint from the kth skill exists in the\ntraining set and 2) the skill function exists in the model (N ≥k), creating emergent (abrupt) property of the model on data\nand parameters.\n21\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nThey argued that the emergent property carries over to the 2-layer MLP (and NNs) because the greedy dynamics of the\nlayerwise structure prioritizes only the most frequent skills. With limited parameters, the greedy dynamics will, in a\nstage-like training fashion, use all available neurons to express the most frequent (correlated) skill, creating a sharp threshold\nbetween learned and not-learned skills.\nFor insufficient data, neurons that help express more frequent skills will learn significantly faster compared to those involved\nwith less frequent skills, similar to stage-like training. The faster-learning skills experience decoupled dynamics from the\nother skills, resulting in higher individual skill performance. The dynamics of less frequent skills mix together, leading to\npoorer skill performance. See Nam et al. (2024a) for further discussion.\n22\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nE. Neural collapse\nIn this section, we restate the four NC conditions given in Papyan et al. (2020), and discuss their significance.\nE.1. Setup\nWe decompose a neural network, a map from input feature space X to output Rc, into two components: 1) a map from the\ninput features to the last layer features Φ : X →Rp and 2) the last layer (a map from RP to Rc). Then, we can express the\nneural network as\nfk(x) =\np\nX\ni=1\nΦi(x)Wik + bk,\n(61)\nwhere W and b ∈Rp are the weight and bias terms of the last layer. We use size n training set that is partitioned into c sets\n[A1, A2, · · · , Ac] of the same size (balanced classification), where each Ak consists of all training datapoints with true label\nk. The kth class mean vector is\nµk := c\nn\nX\nx∈Ak\nΦ(x),\n(62)\nwhere n is the number of training set. The global mean vector is\nµg := 1\nc\nc\nX\nk=1\nµk.\n(63)\nThe feature within-class covariance ΣW ∈Rp×p is\nΣW := 1\nn\nc\nX\nk=1\nX\nx∈Ak\n(Φ(x) −µk)(Φ(x) −µk)T ,\n(64)\nwhich shows the deviation of last layer feature vectors away from the corresponding class mean vectors.\nE.2. NC conditions\nThe four NC conditions are the following:\n1. Within-class variance tends to 0\nΣW →0.\n(65)\n2. Convergence to simplex ETF\n(µj −µg)T (µk −µg)\n∥(µj −µg)∥2∥(µk −µg)∥2\n→cδjk −1\nc −1 .\n(66)\n3. Convergence to self duality\nwk\n∥wk∥2\n−\nµk −µg\n∥(µk −µg)∥2\n→0.\n(67)\n4. Simplification to nearest class center\narg max\nk\n p\nX\ni=1\nΦi(x)Wik + bk\n!\n→arg min\nk ∥Φ(x) −µk∥2.\n(68)\nCondition 1 states that all feature vectors collapse around their class mean vector. Condition 2 states that the class mean\nvectors form a simplex ETF structure (Figure 5). Condition 3 states that the last layer found the minimum-norm solution for\nthe last layer feature vectors satisfying conditions 1 and 2. The last condition states that the model assesses classification\nbased on the distance between the last layer feature vectors and the class mean vectors.\n23\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nF. Imbalanced layer initialization and data-adaptiveness\nWe extend the discussion on the topic developed in Section 6. Note that we summarize here information mainly taken from\nDomine et al. (2024) and Kunin et al. (2024).\nF.1. The balanced condition\nLet us formally reintroduce Equation (14):\nDefinition F.1 (Definition of λ-balanced Property ((Saxe et al., 2014; Marcotte et al., 2024))). The weights W1 and W2 are\nsaid to be λ-balanced if and only if there exists a Balanced Coefficient λ ∈R such that:\nW T\n2 W2 −W1W T\n1 = λI.\nWhen λ = 0, this property is referred to as Zero-Balanced, and it satisfies the condition:\nW T\n2 W2 −W1W T\n1 = 0,\nThis condition is of interest because it remains conserved for any initial value λ, as detailed in (Marcotte et al., 2024; Domin´e\net al., 2024). It becomes particularly relevant in the continual learning, reversal learning, and transfer learning settings.\nThis quantity can be interpreted as the relative scale across layers. A straightforward intuition for the relative scale, as\ncompared to the absolute scale (the norm W2W1 when target scale is fixed to 1), can be gained by considering the scalar\ncase where the input, output and hidden dimension are equal to 1. In this simple scenario, it is easy to ensure that w2\n1 = w2\n2\nsatisfies λ = 0 while allowing for different absolute scales. For example, w1 = w2 = 0.001 or w1 = w2 = 5. In such cases,\nthe absolute scale is clearly decoupled from the relative scale. However, in more complex settings, the interaction between\nrelative and absolute scales becomes non-trivial. As we will discussed below, their role in shaping and amplifying dynamics\nis intricate and highly dependent on the underlying network architecture.\nIntuition\nEven with non-linearity, one layer’s weight controls the other’s rate of change (Section 3). This principle enables\ntargeted training of a layer, allowing tuning between lazy and rich (and thus grokking) dynamics. This intuition can be\nformalized as\nTheorem F.2. Under the assumptions of whitened inputs (1), λ-balanced weights (2), and no bottleneck and with a task-\naligned initialization, as defined in (Saxe et al., 2014), the network function is given by the expression W2W1(t) = ˜UP(t) ˜V T\nwhere P(t) ∈Rc×c is a diagonal matrix of singular values with elements ρα(t) that evolve according to the equation,\nρα(t) = ρα(0) + γα(t; λ) (˜ρα −ρα(0)) ,\n(69)\nwhere ˜ρα is the α singular value of the correlation matrix and γα(t; λ) is a λ-dependent monotonic transition function for\neach singular value that increases from γα(0; λ) = 0 to limt→∞γα(t; λ) = 1 defined explicitly in Appendix of (Domin´e\net al., 2024). We find that under different limits of λ, the transition function converges pointwise to the sigmoidal (λ →0)\nand exponential (λ →±∞) transition functions,\nlim\nλ→0 γα(t; λ) =\ne2˜sα t\nτ −1\ne2˜sα t\nτ −1 +\n˜sα\nsα(0)\n,\nlim\nλ→±∞γα(t; λ) = 1 −e−|λ| t\nτ .\n(70)\nThe proof for Theorem F.2 can be found in (Domin´e et al., 2024). As λ increases, the dynamics of the network resembles\nthose of a linear model, transitioning into the lazy regime. On the other end, as lambda goes to zero the networks learns the\nmost salient features first, which can be beneficial for generalization (Lampinen & Ganguli, 2018).\nF.2. Controlling the dynamics with layer imbalance\nWe conducted an experiment to explore the relationship between relative weight scale, absolute weight scale, and the\nnetwork’s learning regime in a general setting as shown in Figure 6. The absolute scale of the weights in Figure 6(a) is\ndefined as the norm of W2W1. Random initial weights with specified relative and absolute scales were generated, and the\nnetwork was trained on a random input-output task. During training, we calculated the logarithmic kernel distance of the\n24\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nNTK from initialization and the logarithmic loss. The kernel distance is calculated as: S(t) = 1 −\n⟨K0,Kt⟩\n∥K0∥F ∥Kt∥F . as defined\nin Fort et al. (2020). These values were visualized as heat maps for λ ∈[−9, 9] and relative scales in (0, 20]. The regression\ntask parameters were set with (σ =\n√\n3). The task has batch size N = 10. The network has with a learning rate of η = 0.01.\nThe lambda-balanced network are initialized with E[xyT ] = I of a random regression task. Figure 6(a) shows that a square\n(d = c) linear neural network satisfying Equation (14) only show amplifying dynamics when the weights are balanced with\nsmall |λ|. Larger imbalances lead to linear dynamics in the training of a single layer. However, as the relationship becomes\nmore complex.(Domin´e et al., 2024) demonstrates that both relative and absolute scales significantly impact the learning\nregime of the network across different architectural types.\nNote that here, we examine how much the NTK has shifted by the end of training, without focusing on the dynamics of the\nNTK throughout the training process. As discribed above, lazy training is associated with exponential dynamics, while rich\ntraining is linked to sigmoidal dynamics. As highlighted in Braun et al. (2022), the dynamical transition in the structure of\ninternal representations can be decoupled. This result demonstrates that rich, task-specific representations can emerge at any\nscale when λ = 0, with the dynamics transitioning from sigmoidal to exponential as the scale increases.\nF.3. Controlling grokking with layer imbalance\nAs discussed above, larger weight imbalance (in any direction) leads the transition from the rich to the lazy regime in square\nlinear neural networks (Figure 6). For neural networks with non-linear activations, the non-linear activations break the\nsymmetry between layers, potentially initiating feature learning for λ →∞(Kunin et al., 2024). Here, we explore how the\narchitecture can break the symmetry and lead to grokking — a transition from the lazy to rich regime — in linear neural\nnetworks, followed by its application to networks with non-linear activations.\nGrokking in linear neural networks\nWe introduce two networks: funnel networks that narrow from input to output,\nand anti-funnel networks that expand from input to output. In Figure 10, we observe that funnel networks enters the lazy\nregime as λ →−∞(downstream initialization), while anti-funnel networks do so as λ →∞(upstream initialization),\nwhich are consistent with Figure 6. However, the networks in opposite limits (λ →∞for Figure 10(a) and λ →−∞for\nFigure 10(b)) show significant change in the NTK after training (rich regime). The networks were initially at the lazy regime,\nbut showed a transition to a delayed rich regime (grokking).\nIntuitively, the gradient from lighter layer negligibly changes the heavier layer (lazy), but the wider input (output) of funnel\n(anti-funnel) networks allows the lighter layer to aggregate sufficient gradients on the heavier layer, driving a meaningful\nchange into a rich regime.\n(a) Layer Imbalance in Funnel Network\n(b) Layer Imbalance in Anti-Funnel Network\nFigure 10. The impact of the architecture. Linear neural networks that satisfy Equation (14) are trained on randomized regression.\nFigures show the changes in NTK as a function of weight-to-target ratio and layer imbalances for (a): funnel networks and (b): in\nanti-funnel networks.\nGrokking in non-linear neural networks\nThe study of linear networks does not always directly translate into similar\nphenomenology in non-linear networks, but these networks successfully highlight the relevant parameters that control\n25\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\ndifferent learning regimes: the relative scale. Notably, networks initialized with upstream scaling can exhibit a delayed\nentry into the rich learning phase. For example, Kunin et al. (2024) demonstrated that decreasing the variance of token\nembeddings in a single-layer transformer (accounting for less than 6% of all parameters) significantly reduces the time\nrequired to achieve grokking. The method can be viewed as targeted training for the earliest layer that undergoes the most\nnon-linear activations.\nF.4. Additional experimental details for Figure. 6\nCode\nThe codes used to plot Figure 6 are based on the original codes of Domine et al. (2024) with only minor changes.\nThey are available at https://anonymous.4open.science/r/linear_first-0FCA.\nFigure. 6(b)\nThe setup is identical to that of Figure 6(a) except a few changes. We used the linear neural network\nwith d = 20, p = 20, c = 2. We randomly initialized W2, W1 (which do not satisfying the balanced condition) using\nsymmetrized initialization (Chizat et al., 2019) to set the initial function equal to the zero function. The layer imbalance was\nimplemented using the standard deviations σ1, σ2 used to initialize the weights. To change the weight-to-target ratio, we\nused target downscaling in the range [0.1, 50].\n26\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nG. Grokking\nIn this section, we present a brief history and related works on grokking.\nPower et al. (2022) first identified grokking, a phenomenon characterized by delayed generalization in algorithmic tasks.\nExtending beyond algorithmic datasets, Liu et al. (2022b) showed that grokking can also occur in general machine learning\ntasks, including image classification, sentiment analysis, and molecular property prediction. Humayun et al. (2024) further\ndemonstrated that grokking is a more widespread phenomenon by showing its presence in adversarial robustness for image\nclassification tasks. Wang et al. (2024) discovered that transformers can grok in implicit reasoning tasks, while Zhu et al.\n(2024) studied the impact of dataset size on grokking in text classification tasks.\nG.1. Approaches to understand grokking\nLiu et al. (2022b) claimed that large weight initialization induces grokking. Thilak et al. (2022) proposed the slingshot\nmechanism to explain grokking, attributing it to anomalies in the adaptive optimization process, particularly in the absence\nof weight decay. Zheng et al. (2024) argued that sudden generalization is closely related to changes in representational\ngeometry. Levi et al. (2024) addressed grokking in the lazy regime with Gaussian inputs by analyzing gradient-flow training\ndynamics. Based on this analysis, they demonstrate that grokking time (gap between overfitting ang generalization) depends\non weight initialization, input and output dimensions, training sample size, and regularization. However, due to strong\nassumptions, their analysis is limited to the lazy regime, resulting in an insufficient explanation of other aspects of grokking\nsuch as the lazy-to-rich transition.\nSeveral approaches have been proposed to interpret grokking in the modular arithmetic task, where grokking was originally\nreported. Nanda et al. (2023) and Zhong et al. (2024) used mechanical interpretability to examine case studies of grokking\nin modular addition. Gromov (2023) provided a solution for grokking in a two-layer MLP on modular addition. He et al.\n(2024) analyzed grokking in linear modular functions. Mohamadi et al. (2024) demonstrated that a two-layer network can\ngeneralize with fewer training data on the modular addition task. Rubin et al.(2024) showed that grokking is a first-order\nphase transition in both teacher-student setup and modular arithmetic problem.\nG.2. Acceleration of grokking\nSeveral methods have been proposed to accelerate grokking and escape the lazy regime. Liu et al. (2022a) argued that\ngrokking can be mitigated through appropriate hyperparameter tuning, such as adjusting the learning rate or weight decay.\nGromov (2023) also suggested that regularization techniques, including weight decay and the momentum of the optimizer,\ncan reduce the training time required for grokking.\nKumar et al. (2024) asserted that grokking occurs when the network is initialized in the lazy regime, particularly\nwhen it has large width, large initialization, small label scaling, or large output scaling. Lee et al. (2024) proposed\namplifying the gradients of low-frequency components to accelerate the speed of generalization. Furuta et al. (2024)\nsuggested pre-training networks on similar tasks that share transferable features, which can then be applied to downstream\ntarget tasks to speed up grokking.\nG.3. Empirical confirmation on MLP\nCode\nThe code for Figure 11 is available at https://anonymous.4open.science/r/linear_first-0FCA.\nTo confirm that our analysis of the scalar input neural network extends to neural networks, we trained depth 4, width 512\nMLPs with tanh activation on 1000 datapoints of flattened (vectorized) MNIST dataset. We used MSE loss with the class\nlabel as one-hot vector multiplied by the target scale. By default, the weights of the layers were multiplied by factor of\n5 compared to LeCun initialization. For training, Adam with learning ratio of 10−3 and weight decay of 10−4 was used.\nBatch size was 128 and the model was trained for 2000 epochs. For input downscaling, the MNIST data vectors were\nmultiplied by the input scaling factor. The output scale was multiplied to the output of the function (MLP). We summarize\nthe different configurations used in Figure 11:\n27\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nName\nSub figure\nWeight init ratio\nTarget scale\nInput scale\nOutput scale\nDefault\n(a)\n5\n3\n1\n1\nWeight downscaling\n(b)\n1\n3\n1\n1\nTarget upscaling\n(c)\n5\n30\n1\n1\nInput downscaling\n(d)\n5\n3\n0.01\n1\nOutput downscaling\n(e)\n5\n3\n1\n0.1\n100\n101\n102\n103\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\ntrain\ntest\n(a) Default\n100\n101\n102\n103\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\ntrain\ntest\n(b) Weight Downscaling\n100\n101\n102\n103\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\ntrain\ntest\n(c) Target Upscaling\n100\n101\n102\n103\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\ntrain\ntest\n(d) Input Downscaling\n100\n101\n102\n103\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\ntrain\ntest\n(e) Output Downscaling\nFigure 11. Grokking in MLP. We apply four techniques to remove (accelerate) grokking for a 4-layer MLP trained on MNIST.(a): Basic\ngrokking scenario. (b-e): The methods discussed in Section 7 that reduce the weight-to-target ratio. See Appendix G.3 for the setup.\n28\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nH. Explicit solution for the 2-layer linear neural network with scalar input and scalar output\nWe provide a general solution for the 2-layer layerwise model described in Equation (15). To repeat the setting, our model\nand target functions are single variable functions\nf(x) = x\nZ\n p\nX\ni=1\naibi\n!\n,\nf ∗(x) = xS,\n(71)\nwhere we can set our loss function as\nL = 1\n2\n \n1\nZ\np\nX\ni=1\naibi −S\n!2\n.\n(72)\nTheorem H.1. The solution for the gradient descent dynamics for the loss function (Equation (72)) is given as\nai(t) = ai(0) + bi(0)\n2\nγ(t)1/2 + ai(0) −bi(0)\n2\nγ(t)−1/2,\n(73)\nbi(t) = ai(0) + bi(0)\n2\nγ(t)1/2 −ai(0) −bi(0)\n2\nγ(t)−1/2,\n(74)\nwhere\nγ(t) =\nΣ0 −S0 + S +\np\nΣ2\n0 −S2\n0 + S2 +\n\u0010\n−Σ0 + S0 −S +\np\nΣ2\n0 −S2\n0 + S2\n\u0011\nexp\n\u0012\n−\n4√\nΣ2\n0−S2\n0+S2\nZ\nt\n\u0013\nΣ0 + S0 −S +\np\nΣ2\n0 −S2\n0 + S2 +\n\u0010\n−Σ0 −S0 + S +\np\nΣ2\n0 −S2\n0 + S2\n\u0011\nexp\n\u0012\n−\n4√\nΣ2\n0−S2\n0+S2\nZ\nt\n\u0013\n(75)\nand\nS0 = 1\nZ\np\nX\nj=1\naj(0)bj(0),\nΣ0 = 1\nZ\np\nX\nj=1\naj(0)2 + bj(0)2\n2\n.\n(76)\nIn particular, we have\nγ+ := lim\nt→∞γ(t) = γ(∞) = S +\np\nΣ2\n0 −S2\n0 + S2\nΣ0 + S0\n.\n(77)\nThe function γ is always monotone from γ(0) = 1 to γ(∞): if S > S0 then γ is increasing, and if S < S0 then γ is\ndecreasing.\nProof. Since our gradient descent dynamics is given as\ndai\ndt = −∂L\n∂ai\n= −bi\n\nZ−2\nm\nX\nj=1\najbj −Z−1S\n\n,\ndbi\ndt = −∂L\n∂bi\n= −ai\n\nZ−2\nm\nX\nj=1\najbj −Z−1S\n\n,\n(78)\none can observe\nd\ndt(ai + bi) = −(ai + bi)\n\nZ−2\np\nX\nj=1\najbj −Z−1S\n\n,\nd\ndt(ai −bi) = (ai −bi)\n\nZ−2\np\nX\nj=1\najbj −Z−1S\n\n.\n(79)\nSo if we define γ as\nγ(t) = exp\n\n−2\nZ t\ns=0\n\nZ−2\np\nX\nj=1\naj(s)bj(s) −Z−1S\n\nds\n\n,\n(80)\nthen we have\nd\ndt (γ) = −2γ\n\nZ−2\np\nX\nj=1\najbj −Z−1S\n\n.\n(81)\n29\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nHence\nd\ndt\n\u0010\nγ−1/2(ai + bi)\n\u0011\n= γ−1/2\n\nZ−2\np\nX\nj=1\najbj −Z−1S\n\n(ai + bi) −γ1/2\n\nZ−2\np\nX\nj=1\najbj −Z−1S\n\n(ai + bi) = 0,\n(82)\nd\ndt\n\u0010\nγ1/2(ai −bi)\n\u0011\n= −γ1/2\n\nZ−2\np\nX\nj=1\najbj −Z−1S\n\n(ai −bi) + γ1/2\n\nZ−2\np\nX\nj=1\najbj −Z−1S\n\n(ai −bi) = 0.\n(83)\nFrom those and γ(0) = 0 we have\nai(t) + bi(t) = (γ(t))1/2(ai(0) + bi(0)),\nai(t) −bi(t) = (γ(t))−1/2(ai(0) −bi(0)),\n(84)\nwhich leads to Equation (73) and Equation (74). We next work on dynamics of γ. We start from\n1\nγ\ndγ\ndt = d\ndt(log γ) = −2Z−2\nm\nX\nj=1\najbj + 2Z−1S.\n(85)\nWe can express\nZ−2\np\nX\nj=1\najbj = Z−2\np\nX\nj=1\n\u0012(aj + bj)2\n4\n−(aj −bj)2\n4\n\u0013\n(86)\n= Z−2\np\nX\nj=1\n(aj(0) + bj(0))2\n4\nγ −Z−2\np\nX\nj=1\n(aj(0) + bj(0))2\n4\nγ−1\n(87)\n= Z−2\np\nX\nj=1\naj(0)bj(0)γ + γ−1\n2\n+ Z−2\np\nX\nj=1\naj(0)2 + bj(0)2\n2\nγ −γ−1\n2\n.\n(88)\nSo if we denote\nS0 = 1\nZ\np\nX\nj=1\naj(0)bj(0),\nΣ0 = 1\nZ\np\nX\nj=1\naj(0)2 + bj(0)2\n2\n,\n(89)\nthen our dynamics for γ becomes\ndγ\ndt = −2Z−1γ\n\u0012\nS0\nγ −γ−1\n2\n+ Σ0\nγ + γ−1\n2\n−S\n\u0013\n= −2Z−1 \u0000(Σ0 + S0)γ2 −2Sγ −(Σ0 −S0)\n\u0001\n.\n(90)\nOne can observe\nΣ0 = 1\nZ\np\nX\nj=1\naj(0)2 + bj(0)2\n2\n≥1\nZ\np\nX\nj=1\n|aj(0)bj(0)| ≥|S0|,\n(91)\nso we can always factor the quadratic in γ as\n(Σ0 + S0)γ2 −2Sγ −(Σ0 −S0) = (Σ0 + S0)(γ −γ−)(γ −γ+)\n(92)\nwhere\n(γ−, γ+) =\n \nS −\np\nΣ2\n0 −S2\n0 + S2\nΣ0 + S0\n, S +\np\nΣ2\n0 −S2\n0 + S2\nΣ0 + S0\n!\n⇒\nγ−< 0 < γ+.\n(93)\nWe continue using partial fraction decomposition as\n1\n(γ −γ−)(γ −γ+)\ndγ\ndt =\n1\nγ+ −γ−\n\u0012\n1\nγ −γ+\n−\n1\nγ −γ−\n\u0013 dγ\ndt = −2Z−1(Σ0 + S0)\n(94)\n30\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nand\nd\ndt log\n\f\f\f\f\nγ −γ+\nγ −γ−\n\f\f\f\f = −2Z−1(Σ0 + S0)(γ+ −γ−) = −4Z−1\nq\nΣ2\n0 −S2\n0 + S2.\n(95)\nBoth γ −γ+ and γ −γ−do not change signs, so the following holds regardless of the sign of γ −γ+:\nγ −γ+\nγ −γ−\n= γ(0) −γ+\nγ(0) −γ−\nexp\n \n−4\np\nΣ2\n0 −S2\n0 + S2\nZ\nt\n!\n.\n(96)\nAs γ(0) = 1, this is solved as\nγ(t) =\nΣ0 −S0 + S +\np\nΣ2\n0 −S2\n0 + S2 +\n\u0010\n−Σ0 + S0 −S +\np\nΣ2\n0 −S2\n0 + S2\n\u0011\nexp\n\u0012\n−\n4√\nΣ2\n0−S2\n0+S2\nZ\nt\n\u0013\nΣ0 + S0 −S +\np\nΣ2\n0 −S2\n0 + S2 +\n\u0010\n−Σ0 −S0 + S +\np\nΣ2\n0 −S2\n0 + S2\n\u0011\nexp\n\u0012\n−\n4√\nΣ2\n0−S2\n0+S2\nZ\nt\n\u0013.\n(97)\nThe limit of t →∞in Equation (96) gives\nγ(t)\nt→∞\n−−−→γ+ = S +\np\nΣ2\n0 −S2\n0 + S2\nΣ0 + S0\n.\n(98)\nFrom Equation (96), we can also see γ ∈(γ−, γ+), and γ increases if and only if 1 −γ+ < 0. It is straightforward to check\nthat if S > S0 (or S < S0) then γ+ > 1 (and γ+ < 1 respectively).\nRemark H.2. If S > S0, then the graph of γ(t) becomes part of some sigmoid function whose image is (γ−, γ+) and center\nis at height γ−+γ+\n2\n=\nS\n∆0+Σ0 . As the graph of γ starts from height 1 = γ(0), one can show that γ(t) is concave for all t ≥0\nif and only if S < ∆0 + Σ0, and otherwise it contains an inflection point. If S < S0, the graph takes a shape of (dilation\nand translation of) (1 −e−t)−1, so after the point γ(0) = 1 it is convex and monotone decreasing.\nRemark H.3. Since ai(0)2 + bi(0)2 ≥2ai(0)bi(0), we have Σ0 ≥0 and Σ0 ≥S0. Therefore,\nγ+ = γ(∞) = S +\np\nΣ2\n0 −S2\n0 + S2\nΣ0 + S0\n≥2S\n2Σ0\n= S\nΣ0\n=\n\u0012Σ0\nS\n\u0013−1\n,\nHence, if Σ0/S is small, we have large γ+ (rich regime).\n31\n\n\nPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena\nI. Research directions\nWe share additional research directions for layerwise linear models.\nI.1. Connection between generalization and greedy dynamics\nGreedy dynamics or the rich regime often correlate with generalization (Lampinen & Ganguli, 2018; Xu & Ziyin, 2024).\nGeneralizing models in vision tasks exhibit NC (Papyan et al., 2020). Models that grok achieves a generalizing solution\nwhen they are in the rich regime (Kumar et al., 2024).\nHowever, greedy dynamics alone cannot fully predict generalization. ResNet18 can show NC on CIFAR10 dataset with\npartially and fully randomized labels (Zhu et al., 2021; Nam et al., 2024b). Transition into a rich regime in grokking\nmay degrade the performance (Lyu et al., 2024). The imperfect correlation between greedy dynamics and generalization\nhighlights the need for a deeper understanding of what these dynamics optimize.\nI.2. Role of batch normalization\nBatch normalization has an interesting connection to the setup of layerwise linear models. For a given layer, it Gaussianizes\nthe features and introduces a learnable parameter that scales the input. The Gaussianized inputs align with the whitening\nassumption in linear neural networks, while the scaling parameter increases similarity to diagonal linear neural networks.\nFurther research in this direction may provide a dynamical understanding of batch normalization.\nI.3. Role of the bias terms and skip connections\nIn this paper, we did not explicitly mention bias terms, which follow different dynamics from weights in deep layerwise\nlinear models. Unlike weights, biases are not connected to the input features and exhibit distinct multilinearity, breaking\nlayer symmetry. For example, the bias term of the last layer follows an exponential saturation (linear dynamics).\nSkip connections, like bias terms, introduce varying degrees of multilinearity across different layers. A deeper dynamical\nunderstanding of bias terms and skip connections may clarify the roles of layers at different depths.\nI.4. Encouraging greedy dynamics\nOne of the goals of understanding grokking is to uncover techniques for transitioning models into generalizing ones. In this\npaper and previous studies, we have gained insight into the rich regime and ways to enhance it. In fact, extreme rich regimes\nhave shown improved performance on certain tasks (Atanasov et al., 2024). Can this knowledge be extended to promote rich\nregimes in various scenarios, such as during training?\n32\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21009v1.pdf",
    "total_pages": 32,
    "title": "Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)",
    "authors": [
      "Yoonsoo Nam",
      "Seok Hyeong Lee",
      "Clementine Domine",
      "Yea Chan Park",
      "Charles London",
      "Wonyl Choi",
      "Niclas Goring",
      "Seungjai Lee"
    ],
    "abstract": "In physics, complex systems are often simplified into minimal, solvable\nmodels that retain only the core principles. In machine learning, layerwise\nlinear models (e.g., linear neural networks) act as simplified representations\nof neural network dynamics. These models follow the dynamical feedback\nprinciple, which describes how layers mutually govern and amplify each other's\nevolution. This principle extends beyond the simplified models, successfully\nexplaining a wide range of dynamical phenomena in deep neural networks,\nincluding neural collapse, emergence, lazy and rich regimes, and grokking. In\nthis position paper, we call for the use of layerwise linear models retaining\nthe core principles of neural dynamical phenomena to accelerate the science of\ndeep learning.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}