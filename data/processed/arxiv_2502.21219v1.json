{
  "id": "arxiv_2502.21219v1",
  "text": "Brickify: Enabling Expressive Design Intent Specification\nthrough Direct Manipulation on Design Tokens\nXinyu Shi\nSchool of Computer Science\nUniversity of Waterloo\nWaterloo, ON, Canada\nxinyu.shi@uwaterloo.ca\nYinghou Wang\nGraduate School of Design\nHarvard University\nCambridge, MA, United States\nyinghouwang@gsd.harvard.edu\nRyan Rossi\nAdobe Research\nSan Jose, CA, United States\nryrossi@adobe.com\nJian Zhao\nSchool of Computer Science\nUniversity of Waterloo\nWaterloo, ON, Canada\njianzhao@uwaterloo.ca\nReifying elements into design tokens\nDirect manipulation on tokens\nAlternative token constructions\nb\na\nc\nsunset\ncolor / style / textual tokens \nsubject tokens\nGlobal\nLocal\nbeautiful park\nstanding, \nfacing left\ncolorful\nFigure 1: Brickify introduces a visual-centric interaction paradigm to specify design intent for controllable image generation:\n(a) users start with reference images and identify design elements (subjects, styles, colors, and concepts) which can be reified\ninto interactive, reusable design tokens; (b) then directly manipulate on these tokens to build a visual lexicon to express how to\nconstruct these elements as a whole; (c) explore alternative compositions by reusing tokens and refining the visual lexicon.\nABSTRACT\nExpressing design intent using natural language prompts requires\ndesigners to verbalize the ambiguous visual details concisely, which\ncan be challenging or even impossible. To address this, we introduce\nBrickify, a visual-centric interaction paradigm — expressing de-\nsign intent through direct manipulation on design tokens. Brickify\nextracts visual elements (e.g., subject, style, and color) from refer-\nence images and converts them into interactive and reusable design\ntokens that can be directly manipulated (e.g., resize, group, link,\netc.) to form the visual lexicon. The lexicon reflects users’ intent for\nboth what visual elements are desired and how to construct them\ninto a whole. We developed Brickify to demonstrate how AI models\ncan interpret and execute the visual lexicon through an end-to-end\npipeline. In a user study, experienced designers found Brickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author’s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in CHI Conference\non Human Factors in Computing Systems (CHI ’25), April 26-May 1, 2025, Yokohama,\nJapan, https://doi.org/10.1145/3706598.3714087.\nmore efficient and intuitive than text-based prompts, allowing them\nto describe visual details, explore alternatives, and refine complex\ndesigns with greater ease and control.\nCCS CONCEPTS\n• Human-centered computing →Graphical user interfaces; In-\nteractive systems and tools; • Applied computing →Arts and\nhumanities.\nKEYWORDS\nDesign Intent Expression, Interaction Techniques, Direct Manipu-\nlation, Interactive Design Token\nACM Reference Format:\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao. 2025. Brickify: En-\nabling Expressive Design Intent Specification through Direct Manipulation\non Design Tokens. In CHI Conference on Human Factors in Computing Sys-\ntems (CHI ’25), April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY,\nUSA, 20 pages. https://doi.org/10.1145/3706598.3714087\narXiv:2502.21219v1  [cs.HC]  28 Feb 2025\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\n1\nINTRODUCTION\nDesign is about the choices of visual elements (e.g., subject, style,\ncolor) and the construction of them towards an intended effect [25];\nthe decisions involved in this process are the art of design, embraced\nwith a designer’s creativity [36]. However, current text-to-image\ngeneration tools (e.g., DALL-E, Midjourney) shift design decision-\nmaking from designers to models, which are designed to create\nfor users rather than work with designers [75, 83, 98]. Despite their\nability to produce aesthetically appealing images for casual use,\nwithout designer’s thoughtful planning, they lack the capability to\ncreate meaningful and professional design solutions that effectively\nconvey intended visual messages [13, 17, 78]. One key barrier to\nkeeping designers in-the-loop is communication [83, 98] — genera-\ntive AI tools are designed to receive instructions textually, while\ndesigners often prefer to think and communicate visually [87, 96].\nConsider the design case of crafting a Halloween poster: the\ndesigner collects some reference images (Figure 2a-2f), thinks about\nwhat visual elements to use, and after a while, forms a rough idea\n(Figure 2g) about how to compose them into a whole, and wants to\nwork with generative AI tools for quick prototyping. However, the\nfollowing three challenges arise.\nFirst, clearly describing what visual elements to use in natural\nlanguage is challenging due to inconsistent naming standards [39,\n64]. For example, colors in Figure 2a might be described as “light\nred, greenish teal, navy blue, with greyish black” by some designers,\nand simply as “orange, green, blue, and black” by others. These\nsubtle differences can lead to significant variations in hues and\nshades. Uploading reference images for model to “see” [17, 68]\ncan help when the desired element dominates the image as in\nFigure 2d. However, in complex images as Figure 2c, specifying an\nexact element is harder. Designers might describe it as “the abstract\nshape with curves in the middle right”, but models often struggle\nwith such ambiguous descriptions regarding the shape and position.\nFurther, precisely verbalizing how to construct those visual ele-\nments into a whole is hard. Deciding on relative scale and proximity\nis to relate the isolated elements with each other as interacting\nparts [25, 96]. However, both scale and proximity are continuous\nvalues, while language is often too discrete for fine-grained instruc-\ntions. For example, describing Figure 2h as “five pumpkins in front of\na large building, with a moon above” lacks precise size and position\ndetails, making it hard for the model to interpret their nuanced\nspatial relationship. The designer might finally obtain a reason-\nable version after multiple rounds of conversations with the tool,\ne.g., “make the building a little bit larger”, but this process is often\ntime-consuming and tedious without guaranteed outcomes.\nLastly, the choices of elemental construction are infinite; how-\never, prompting with texts limits the flexibility to reuse those visual\nelements to explore alternative constructions. Designers need to\ncopy-and-paste an entire paragraph from previous prompts, then\nmodify certain parts to change relationships or replace visual el-\nements. Each iteration of exploration requires users to manually\ntrack where to change and where to keep among lengthy texts,\nwhich is tedious and error-prone. It is because existing generative\nAI tools treat each prompt as an independent request, without a\nFigure 2: Design example of a Halloween party poster, show-\ning (a) the color palette, (b-f) reference images with high-\nlighted elements, and (g) the envisioned poster in designer’s\nmind. (h) illustrates the spatial relationships between pump-\nkins, building, and moon. We have obtained the designer’s\nconsent to include this design in the paper.\nmechanism to selectively separate persistent information (e.g., vi-\nsual characteristics) from single-use prompts, making it inefficient\nto share information across multi-turn interactions.\nTo better align generative AI tools with designers’ visual think-\ning process, we need a visual-centric interaction paradigm with\nversatile expressive power to facilitate graphic design that builds\nupon visual assets. We propose Brickify — specifying design intent\nvia interactive design tokens that clearly carry the information of\nwhat primitive design elements to use (e.g., subject, color, style). De-\nsign tokens can be directly manipulated (e.g., drag-and-drop, resize,\nmove, group, and link) to allow designers to precisely plan how the\nvisual elements are constructed. The resultant construction — visual\nlexicon — can be translated into control signals for AI to faithfully\ngenerate the desired outcome, e.g., spatial layouts, relative scales,\nand the effect radius (e.g., applying a color to a specific subject or\nthe entire image). Since design tokens can be persistent throughout\nthe design process, users can efficiently reuse and recombine ele-\nments, avoiding the need to start from scratch for each iteration. We\nimplemented and iteratively refined this interaction paradigm of\nBrickify in an interactive system named Brickify (Brickify refers\nto the paradigm while Brickify denotes the system).\nWe evaluated the interaction paradigm Brickify and its imple-\nmentation in Brickify through an in-lab user study with 12 ex-\nperienced designers. In a replication task simulating a scenario\nwhere designers have a clear intent, we compared Brickify with\ntextual-centric prompting, finding that participants expressed de-\nsign intent more precisely with less cognitive effort using Brickify\nand performed refinements faster, especially in complex designs.\nIn an open-ended task with a simulated exploratory design sce-\nnario, designers found Brickify provided a controllable, expressive,\nand engaging design experience. Our findings offer insights for\nfuture research on designing interaction mediums for human-AI\nco-creation in broader visual design contexts.\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\n2\nRELATED WORK\nAI-assisted graphic design involves forming, specifying, and real-\nizing design intentions. We review how mood boards help form\nintentions, explore interactive techniques for specifying them, and\nsummarize personalized image generation approaches to translate\nintentions into design outcomes.\n2.1\nForming Intent from Reference Imagery\nA common approach to develop design ideas is using references. De-\nsigners often start with exploring and organizing inspirations, then\nidentify key design elements and strategically compose them [30].\nMood board usage. To support divergent thinking, designers\noften start with creating the mood board [26], a collection of im-\nages, shapes, colors, and other visual stimuli, as an aid to conduct\nvisual research — framing, aligning, paradoxing, abstracting, and\ndirecting the design [58]. Mood boards are intentionally ambigu-\nous, allowing different interpretations and serving as a tool for\nexploring creative possibilities [30]. How designers organize mood\nboards reflects their intended use, with studies exploring image\narrangement in digital drawing and other fields [28, 43, 59]. Recent\nworks [51–53] enhance mood boards with AI for semantic cluster-\ning, image recommendations, and material arrangement. Designers\noften decompose images into sub-elements [25] and then integrate\nthem into cohesive designs [34]. Tools like MoodCubes [45] and\nMetaMap [47] help break down elements and enhance mood boards.\nCurrent generative AI tools allow users to upload reference images\nbut lack flexibility in arranging or specifying sub-elements like on\na mood board. Users cannot easily select which parts to use. Our\napproach integrates mood boards, enabling users to specify what\nsub-elements to use by converting them into design tokens.\nRecombination of recognized elements. After identifying\nthe elements of interest on the mood board, researchers have cre-\nated a variety of tools to support the elements recombination. Visi-\nBlends [16] and VisiFit [15] enable the blending of two semantic\nobjects. Building on this, PopBlends [93] explores strategies for\nmerging two conceptual keywords into pop culture images using\nlarge language models. Tools like 3DALL-E [57] and CreativeCon-\nnect [17] propose workflows for generating 3D designs and graphic\nsketches by suggesting keywords from design references and then\nrefining them into detailed text prompts. Beyond keywords, Artin-\nter [18] and GANCollage [92] enhance designer-client communica-\ntion and style mixing by combining keywords and example imagery.\nExisting tools typically emphasize recombination outcomes under\npredefined rules for task-specific usage, such as blending by shape\ncontours or mixing by styles. However, users have limited control\nover how to combine. This paper addresses this issue by enabling\nusers to expressively construct the relationships among elements\nthrough direct manipulation of reified interactive tokens.\n2.2\nSpecifying Intent by Interactive Strategies\nThe challenge of specifying design intent solely with natural lan-\nguage is well recognized [13, 80, 85, 98]. Subramonyam et al. [83]\ntheorize how users translate their goals into clear intentions, high-\nlighting the instruction gap where generative models are highly\nsensitive to language precision but human language tolerates vari-\nants in expression to communicate similar meaning. This challenge\nis evident across studies in various design domains, where casual\nusers [60], graphic designers [55], manufacturing designers [35],\nand game professionals [90] struggle to articulate visual intent\nthrough the tedious process of prompt engineering. This friction\nof translating visual design into a verbal medium can be under-\nstood through the lens of design methodology literature [23, 86, 95].\nRecent efforts to facilitate the intent expression fall into three cate-\ngories: 1) decomposing the lengthy prompt into modular ones; 2)\naugmenting the text prompt with other modalities; and 3) resolving\nambiguities in text prompts through direct manipulation [79].\nModular prompting. Managing lengthy prompts is challeng-\ning, and many research has explored to modularize them into man-\nageable pieces. For instance, AI-chains [100] enables chaining in-\ndividual prompts for end-to-end execution. In the similar spirit,\nChainForge [4] supports comparing prompt variations between\nmodels. In visual design, Keyframer [88] uses “decomposed prompt-\ning” for step-by-step animation design. Similarly, Spellburst [3] and\nComfyUI [22] employ node-based interfaces to modularize creative\ncoding and image generation, integrating diverse control signals.\nAlthough these tools aid in flexibly writing and modifying prompts\nand allow viewing intermediate results, they still require users to\nspecify their visual intentions in text, which does not fully address\nthe precision of intent specification.\nMultimodal prompting. Most recent commercialized tools\n(e.g., DALL·E3 [24], Adobe Firefly [31], MidJourney [62], Flux AI\n[2]) allow users upload images as global content and/or style refer-\nences; however, users cannot add local annotations on the image to\nfurther illustrate their fine-grained intentions. Kaiber Superstudio\n[84] supports character-consistent generation by allowing users to\nspecify a single local subject but does not support multiple subjects.\nKrea.ai [54] enables users to train on their own assets but requires\nmultiple instances for each subject. Research efforts have also\ninvestigated different strategies for multimodal prompting. For ex-\nample, DesignPrompt [68] allows users to input texts, images, and\ncolors, then help translate them into a final textual prompt. How-\never, translating visuals into text often leads to information loss,\nmaking it difficult to capture precise visual identities. PromptCharm\n[94] enables users to examine which part of the generated image\ncorresponds to which part of the text prompt. Sarukkai et al. [74]\nintroduce the coarse-to-fine sketch guided image generation. Al-\nthough these tools are helpful in clarifying what visual elements\nto use to some degree, they fail to help users express how multiple\nvisual elements relate with each other because they typically treat\neach input modality in isolation.\nPrompting through direct manipulation. Some tools employ\nvisual metaphors to help users specify their intentions through\ndirectly manipulating on visual objects rather than text prompts.\nFor instance, TaleBrush [20] uses sketch lines to indicate narra-\ntive transitions, and PromptPaint [19] offers a paint-like interface\nfor semantic prompt interpolations. Gestures are utilized to repre-\nsent editing intentions, for example, drawing masks to inpaint [6],\nadding colored strokes to recolorize [103], and dragging points to\nedit pose or facial expressions [67]. While intuitive, these methods\nare task-specific, requiring users to re-learn interactions for each\nuse case. Recent work extends this direction by aiming to integrate\ngraphical user interfaces (GUI) with natural language interfaces\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\n(NLI). DirectGPT [61] allows users to drag and drop graphical el-\nements onto prompts, but these elements act as isolated symbols,\nlosing their spatial relationships. DynaVis [89] combines NLIs with\ndynamically generated GUI widgets for visualization authoring,\nbut requires users to specify the edit intention in text. Both Direct-\nGPT and DynaVis address the ambiguity of continuous numerical\nexpressions in textual prompts, yet they still force users to think\ntextually and work primarily on texts.\n2.3\nTranslating Intent into Design Outcomes\nthrough Computer Vision Techniques\nText-to-image generation is an active research topic since diffusion\nmodels emerging [42, 82], there are many amazing work has made\nthe performance of text-to-image generation has reached a height\nthat never been reached, such as DALL-E [11, 69, 70], GLIDE [65],\nStable Diffusion [71], Imagen [73], etc. Research has also focused\non improving controllability in image generation and editing.\nSubject-driven and style-specific personalized generation.\nThe task of personalization, introduced by Cohen et al. [21], aims to\nincorporate user-provided concepts absent from the training data\ninto generated results. Early methods like Textual-Inversion [32]\nand DreamBooth [72] learn a single subject from several user images,\nwhile Custom Diffusion [56] and SVDiff [38] extend this to multiple\nsubjects for recombination. Later, ELITE [97] and E4T-diffusion [33]\nmake it possible to learn a single subject from one image but re-\nquire the subject to be visually dominant. Break-A-Scene [5] allows\nlearning multiple subjects from a single image with loose segmenta-\ntion masks. Personalization also applies to styles: StyleDrop [81]\nfine-tunes models for style customization, while Style-Align [41]\nensures style consistency without fine-tuning.\nSpatial-aware controllable image generation and editing.\nTo guide the large-scale pre-trained diffusion models to generate\nimages following the spatially-localized conditions (e.g., depth map,\nsegmentation, pose, etc.), Controlnet [102] embeds task-specific\nnetworks, while T2I-Adapter [63] offers a lighter adapter-based\nsolution, and Composer [44] provides more flexible control with a\nlarger model. For local image editing, initial methods [7, 8, 40] rely\non precise text descriptions. Imagic [48] allows description-free\nediting but requires time-intensive fine-tuning, whereas Blended\nLatent Diffusion [6] enables faster editing without fine-tuning.\nThe advancements in the computer vision field are powerful\nand hold great potential. However, without intuitive interactions\nthat allow users to precisely express their underlying intent, users\ncannot fully benefit from these capabilities. Our work focuses on\ninnovating the user interaction, using these off-the-shelf methods\nas the technical foundation to realize our proposed visual-centric\ninteraction paradigm.\n3\nITERATIVE USER-CENTERED DESIGN\nIn collaboration with designers for about nine months, we em-\nployed an iterative design approach to define the visual-centric\ninteraction paradigm, Brickify, and develop the Brickify system.\nThe design process consists of four stages (noted as S1-4), with\ndifferent participants involved in each stage. In this section, we\nintroduce the participants and procedures for S1, S2, and S3, and\ndiscuss the findings and derived design goals from S1 (Section 3.1).\nTo provide a holistic view, we briefly highlight the key design deci-\nsions made from S2 (Section 3.2) and S3 (Section 3.3), with further\ndetails provided in Section 4.\nS1: Problem understanding (2 months) — interviews with six\ndesigners to identify challenges in using generative AI tools.\nS2: Early Prototyping (4 months) — weekly co-design sessions\nwith an expert designer to design the interaction paradigm\nand develop the early working prototype;\nS3: Prototype Iteration (2 months) — informal testing involved\nsix designers to collect feedback and iteratively refine the\ndesign;\nS4: System Evaluation (1 month) — a user study with two\ntasks compares the visual-centric interaction paradigm of\nBrickify with the textual-centric one and examines how\nusers interact with Brickify, which will be described in Sec-\ntion 7 and Section 8.\n3.1\nProblem Understanding: Interview Study\nWe conducted semi-structured interviews with six design experts\n(E1-6), with the aim to understand: (1) how designers approach\nprompting the generative AI models to craft graphic designs; and\n(2) what challenges they have encountered.\n3.1.1\nParticipants and Procedure. Participants were recruited via\nemail lists and social media, screened through a pre-test survey on\ndesign experience and familiarity with text-to-image generation\ntools. All had over two years of experience in graphic design, famil-\niar with and regularly used the text-to-image generation tools in\ntheir work. Participants provided consent and were compensated\nwith $20 for a 45-minute study session. We asked each participant\nto share at least one recent design project involving using text-to-\nimage AI tools to reflect on how they use them.\n3.1.2\nIdentified Challenges. We summarize the following chal-\nlenges designers encountered when using generative AI tools.\nC1: Failure to convey designers’ attended elements to AI.\nParticipants consistently began their design projects with visual\nresearch, using mood boards to collect inspirational images on a\ncanvas in Figma (5/6) or Photoshop (1/6). They emphasized “it’s\nfor understanding how pieces can fit together in my head.” -E3, align-\ning with prior studies [43, 52, 58]. Some participants (E1, E2, E4)\ngrouped references spatially by element type, while others (E3,\nE5, E6) used annotations to mark elements. E5 explained, “I’m not\nlooking at the whole image, just the parts that matter to my design.”\nThis selective focus, known as active vision [29, 37], is central to\nvisual thinking [96]. While designers use active vision to pinpoint\nspecific details such as a particular texture, a color theme, or the\ncomposition of shapes, AI models often lack the ability to recognize\nor prioritize these details in the same way. As E4 explained, “The\nAI seems to understand the image globally, but I need it to work with\nspecific parts.” Designers have to communicate the visual elements\nthey are focusing on with AI through natural language, a medium\nthat “super hard for describing fine visual nuances.” -E1\nC2: Difficulty in verbalizing element relationships. Partici-\npants emphasized the complexity of composing visual elements in\na design, as E2 described, “like constructing a house, you must place\neach brick properly.” E3 explained, “It’s not just about having the\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\npieces; how to balance their weight is also important — some are more\nimportant, while others just for decorating.” Designers must create\nfocal points, balance hierarchy, and manage spatial placements,\nbut articulating these relationships in words is difficult, especially\nwhen elements are intertwined. As E6 noted, “planning that (how\nto compose them) in my head is hard enough, translating (the en-\ntire mental image) into a sentence feels much more difficult, I often\nsketch them down then describe.” This challenge often forces de-\nsigners to simplify their ideas. As E2 mentioned, “I tend to only\nask for simple structures like centering a certain one (element)”, but\nsuch compromise often results in outputs that “lack the balance and\nspatial nuance we have been taught and always pursue in design.”\nC3: Inefficiencies in iterative refinements. Designers face\nsignificant challenges in refining visuals through current text-to-\nimage tools, as the conversations with AI is linear and lacks the\nmechanism to share key visual information in multi-turn dialogues.\nE4 pointed out, “each change feels like starting over. I need a way to\ngo back to tweak some [elements], but I don’t want to touch certain\nones I already feel good about.” As E2 explained, “I have to copy and\npaste descriptions into every prompt, just to keep that part, but even so,\nit often get changed.” Without a way to selectively preserve certain\nvisual information and design decisions, designers are forced to\nmanage these iterations manually, making the design refinements\ninefficient and the creative flow disrupted.\n3.1.3\nDesign Goals. To tackle the identified challenges, we articu-\nlate the following key design goals to drive the design of Brickify.\nDG1: Support externalization of selective focus on primi-\ntive elements. Designers often focus on specific elements in refer-\nence images, but current systems require uploading entire images\nand verbally explaining their focus (C1). This creates a gap between\nwhat designers attend to and what the system processes. To bridge\nthis gap, designers should be allowed to externalize their selective\nfocus of elements into visual representations. This might involve\nallowing easy annotation, grouping, and flexible organization of\nthese elements. By making these elements tangible, we aim to en-\nable designers to interact with them directly, facilitating both their\ncognitive process and communication with AI at the element level.\nDG2: Enable spatial management and visual communi-\ncation of element relationships. Designers view elements as\ninterdependent in a design, but articulating relationships such as\nscale, hierarchy, and spatial proximity is challenging (C2). As such,\nthe user interaction should provide a flexible 2D workspace where\ndesigners can visually arrange and manipulate elements, defining\nrelationships intuitively, and reducing reliance on verbal descrip-\ntions. The goal is to establish a shared visual structure that allows\ndesigners to clearly define the composition while enabling AI to\naccurately interpret and understand it, improving communication\nof complex elemental compositions.\nDG3: Facilitate element reuse and iterative refinement. De-\nsigners struggle with the linear nature of current conversational\ntext-to-image tools, which lack mechanisms for selectively pre-\nserving or refining elements across iterations (C3). To address this,\ndesigners should be able to easily reuse individual elements and\npartial configurations from previous versions to reduce repetitive\nmanual work. With such a reusing mechanism, designers could\nexplore different design variations more efficiently.\n3.2\nEarly Prototyping: Co-designing with a\nDesigner\n3.2.1\nProcedure. In the early stages of our project, we engaged in a\nfour-month co-design process with an expert designer, who has over\neight years of graphic design experience. We held weekly 30-minute\ndesign meetings. During this phase, we collaboratively created\nearly low-fidelity mock-ups using sketches and iteratively built\nnon-functional prototypes in Figma. This collaboration focused\non defining core components of Brickify and basic features in\nBrickify. Given her commitment, we include her as a coauthor.\n3.2.2\nDesign Outcomes. We defined two key aspects of the Brick-\nify interaction paradigm: 1) reifying [10] design elements into\ndesign tokens; and 2) enabling direct manipulation [79] on tokens,\nconstructing the visual lexicon, to specify relationships. We iden-\ntified two types of design tokens: visual and textual. Among the\nvisual tokens, we included three core elements: subject, style, and\ncolor. We also defined five essential manipulation capabilities: drag-\nand-drop, move, resize, group, and link tokens. The interface oper-\nationalizing this paradigm was structured into three panels: 1) a\nmood board panel for organizing reference images and creating\ntokens, 2) a token manipulation panel for building relationships,\nand 3) a history panel to track versions. These design decisions led\nto the development of an initial working prototype. Details will be\ndescribed in Section 4.\n3.3\nPrototype Iteration: Involving Another Six\nDesigners\n3.3.1\nParticipants and Procedure. To refine the initial design of\nBrickify and the early prototype, we recruited six additional de-\nsigners with over one year of graphic design experience, each hav-\ning used at least one text-to-image generation tool more than five\ntimes in the past three months. We began by walking them through\nthe initial prototype and explaining how to interact with the system.\nUsing our prepared reference images, we asked them to explore\nthe system and generate multiple designs. Designers used a think-\naloud method to inform us when they encountered difficulties or\ndesired alternative functionality. At the end of the session, partic-\nipants provided feedback and discussed their overall experience\nand suggestions for improvement. The entire study lasted about 45\nminutes, and participants were compensated with the equivalent\nof $20 CAD.\n3.3.2\nFeedback and Design Refinements. Designers identified sev-\neral inadequacies in the current Brickify design and suggested im-\nmediate feedback. Based on their suggestions, we strengthened the\nvisual association between design tokens and their original imagery\nto improve clarity. Additionally, we introduced a cross-referencing\nfeature to allow for more effective descriptions of relationships\nbetween subject tokens. Designers also expressed the need to ac-\ncommodate both concrete instructions and abstract imagination, so\nwe added an imaginative token to the interaction vocabulary. These\nrefinements helped make the Brickify more expressive. Details\nwill be described in Section 4.\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\n4\nBRICKIFY: A VISUAL-CENTRIC\nINTERACTION PARADIGM\nIn this section, we explain the design decisions and rationale behind\nBrickify, a visual-centric interaction paradigm that enables users to\nexpress design intent through direct manipulation on design tokens.\n4.1\nDesign Tokens: Specifying What Elements\nto Use\nWe introduce design tokens as the externalizations of designers’\nattended design elements (DG1), reifying [10] the abstract visual\ninformation into concrete first-class graphical objects that can be\ndirectly manipulated and reused.\n4.1.1\nToken types: Being polymorphic to ensure expressiveness and\nextensibility. A key design insight in Brickify is that all types of\ndesign elements and intentions should be regarded as tokens. The goal\nis not to support a complete set of all possible design elements but\nto build an extensible paradigm with the affordance to accommodate\ndifferent types. Such polymorphism [10] is essential for maintaining\na simple interface with consistent interaction logic. During the\nearly prototyping (S2), the designer expressed the desire for precise\ncontrol over style, colors, and subject identity. Later in the prototype\niteration stage (S3), participants added that they also appreciated\nthe model’s hallucinations for certain details. For instance, one\nparticipant noted, “I will leave it to the model to decide how an\nexact ‘joyful’ facial expression looks like.” Thus, we categorize design\ntokens into three types: visual, textual, and imaginative (Fig. 3).\nVisual token carries the visual information such as the\nsubject, style, and color, reified from reference images.\nTextual token complements visual tokens by conveying\ninformation that is easier to express through language, such\nas adjectives for emotions or verbs for gestures.\nImaginative token mediates the initiative between design-\ners and models, indicating where the model should intervene\nand how much imagination is needed.\n4.1.2\nToken appearances: Balancing fidelity with re-envisioning po-\ntential. The design tokens can be regarded as a visual abstraction\ndepicting the elements graphically. When designing their appear-\nance, we balanced between fidelity and re-envisioning potential.\nTokens need to be visually distinct, allowing users to easily identify\nwhat element they represent while retaining enough abstraction\nfor designers to re-imagine them in new contexts.\nIn early co-design sessions (S2), we used geometric shapes to\nrepresent different elements, e.g., rectangles for subjects, circles for\ncolors, and filled rectangles with different colors to distinguish be-\ntween subjects. Hovering over a token would highlight the original\nsource in the reference image. However, in the later prototype iter-\nation stage (S3), we found that as the number of subjects grew and\ndesign complexity increased, participants struggled to track which\ntoken represented which subject, frequently switching between the\nmood board and token manipulation panels to confirm identities.\nTo address this, we refined the subject tokens by attaching a small\ncropped image of the subject to the token’s corner. For style tokens,\nwe represented them by transferring the style to a standard image.\nsunset\nfacing left with a happy smile.\nplayfulness\nadventure\nSubject Token (persistent)\nVisual Token \nStyle Token (persistent)\nColor Token (persistent)\nReified from reference images (persistent)\nCreated by users (temporary)\n(temporary)\n(discrete size: small | medium | large)\n(continuous size)\n(no size)\nTextual Token \nImaginative Token\nFigure 3: The definition of design tokens in Brickify: visual,\ntextual, and imaginative tokens. Each type of tokens has\ntheir own appearances and life-cycles.\n4.1.3\nToken life-cycles: Offering both persistent ones for reuse and\ntemporary ones to avoid overwhelm. Towards facilitating element\nreuse and iterative refinement (DG3), we make a distinction be-\ntween persistent and temporary tokens. Persistent tokens are used\nfor core content elements that are repeatedly referenced throughout\nthe design process, while temporary tokens represent contextual\ndetails or single-use modifications.\nDesigners typically plan a design by considering content, con-\nstruction, and context. For instance, “two girls (content) next to each\nother in the center of the image (construction) are dancing surrounded\nby flowers (context)”. Designers often explore alternative design\npossibilities by altering the construction or context while keeping\nthe same visual elements. To support this process, content-related\ntokens are persistent for reuse across different design variations,\nwhile context-related tokens remain temporary to avoid clutter. We\nimplemented this distinction by separating token creation into two\npanels: tokens created in the mood board panel are persistent, with\neach use being a copy of the original, ensuring the flexibility of\nreuse. In contrast, tokens in the manipulation panel are temporary\nand deleted when the panel is cleared to prevent interface clutter.\n4.2\nDirect Manipulation on Tokens: Expressing\nHow to Construct Elements\nDirect manipulation [79] has long been integral to designers’ work-\nflows, especially for rapid prototyping and visual planning. In\nBrickify, users express how they want to construct elements through\ndirect manipulations on design tokens (DG2).\n4.2.1\nIntuitive actions to reflect intentions. The mapping between\nintention and action should be intuitive, allowing users to engage\nthrough technical reasoning [66] rather than procedural learning. In\ncollaboration with designers, we defined the following actions to\nreflect design intent, constructing the visual lexicon by manipulating\non tokens, as shown in Fig. 4.\nDrag-and-drop. Users create persistent tokens (subject,\nstyle, color, concept) from reference images in the mood\nboard panel. To use these tokens, they can drag their copies\nand drop them onto the token manipulation panel, where\neach token can be reused multiple times without limits.\nMove. Users can freely move subject tokens to define the\nspatial relationships between subjects. While other tokens\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\ndriving\nstanding\na\nb\nd\nc\nFigure 4: Demonstration of exploring different compositions through direct manipulation on design tokens. (a)–(d) show how\nadjusting sizes and positions of the owl and car tokens changes their relationships in the outcomes.\n(e.g., color, style, textual) can also be moved, the movement\nof these tokens is purely for organizational purposes without\nencoding spatial relationships, as their function is to modify\nor describe attributes of the subject.\nResize. Resizing tokens adjust their scale. Color tokens\nindicate proportional weights (e.g., primary vs. secondary\ncolors), while style tokens work similarly. Resizing subject\ntokens specifies their sizes in the output, and resizing imag-\ninative tokens controls the extent of AI-imagined details.\nTextual tokens cannot be resized.\nGroup. Grouping tokens helps manage multiple elements\neasily. For example, users can group 3-5 colors into a color\ntheme or apply several colors to a single subject.\nLink. Design elements are often interconnected. For exam-\nple, a color token can be linked to a specific subject, applying\nonly to that subject, or left unlinked to apply globally. Links\nspecify the relationships between tokens, such as binding\ncolors or textual descriptions to subjects.\nCross-reference. Subject tokens often reference one an-\nother to specify certain relationships. For example, in the\nphrase “an owl is driving the car”, describing the owl’s be-\nhavior using a textual token requires referencing the car’s\ntoken. To cross-reference, users can assign a name to a token\nto refer and tag the name in a textual token.\n4.2.2\nFlexible action reuse. In addition to reusing tokens, designers\nshould be able to reuse their previous actions to explore alternative\ndesign paths (DG3). Since actions are reflected in the construction\nof design tokens, the visual lexicon, we support action reuse by\nrecording each lexicon created. Designers can refine their work\nbased on this visual lexicon rather than redoing previous actions,\nenabling more efficient iteration and exploration.\n4.2.3\nIntermediate action outcomes. During prototype refinement\n(S3), designers expressed the need for immediate feedback on their\nactions to assess how design tokens respond and evaluate the results.\nHowever, the current generative models have noticeable inference\ntimes and high computational costs, making instant feedback for\nevery action impractical. To address this, we introduced feedback at\na higher granularity. Since the execution process follows a sequence\n— first composing the layout, then aligning the style, and finally\napplying colors — we provide intermediate results at each step.\nAs model inference times improve, we envision the possibility of\nreal-time feedback for more responsive interaction.\n5\nUSAGE SCENARIO\nBefore diving into the design process and the implementation in\ndetail, we walk readers through an example usage scenario to ex-\npress design intent through Brickify. Imagine Stella, a designer,\nworking on a children’s storybook about the adventures of an owl.\nThe story follows the owl as he travels in different landscapes with\nhis trusty car and friend.\nCreating design tokens. Stella begins by gathering inspirational\nimages to define the look of the characters and the feel of the\nscenes and importing them to the Mood Board Panel (Fig. 5A). She\ndraws bounding boxes around the owl, car, and tree to create their\nsubject tokens. She then creates color and style tokens with the\ncorresponding tools shown in (Fig. 5a). To set the thematic tone,\nshe adds a textual token for “playfulness”.\nManipulating design tokens. She drags and drops (Fig. 5(b1)) the\ncreated tokens to the Token Manipulation Panel (Fig. 5B) to build\nher story (Fig. 5(b2–b6)). She imagines the owl parking under a tree,\nwaiting for his friend. She resizes and positions the owl, car, and\ntree tokens to define their spatial relationships, then links textual\ntokens to specify the owl standing behind the car and facing left.\nFor the tree, which she imagines as “colorful” but undefined, she\nlinks an imaginative token to let the AI decide it. Stella then groups\nthe color tokens for a cohesive theme and adds a style token. For\nthe background, she creates a textual token of “beautiful park”.\nReusing design tokens. In the next scene, where the owl’s friend\njoins, Stella reuses parts of the previous visual lexicon, making slight\nadjustments and dragging and dropping another subject token of\nowl from the Mood Board Panel as his friend. By reusing design\ntokens, she streamlines her workflow and avoids redundant work.\nAll generated results and their visual lexicons are organized in the\nHistory Panel (Fig. 5C).\n6\nBRICKIFY SYSTEM IMPLEMENTATION\nIn this section, we explain how Brickify extracts primitive design\nelements from reference images to create design tokens (Section 6.1)\nand transforms the tokens together with users’ actions on tokens\ninto control signals for models to process (Section 6.2).\n6.1\nDesign Token Creation\n6.1.1\nSubject token. Users create a subject token by drawing a\nbounding box around the desired subject using the subject tool.\nTo ensure that generative models accurately capture the visual\ndetails specified by users, we employ the SAM [49] model to extract\nsegmentation maps and then use the Break-A-Scene [5] approach\nto fine-tune the Stable Diffusion (v2.1) model. This process\nlearns the subject’s visual identity and binds each subject to a\nspecific token within the model for later use. To accommodate\nmultiple reference images, we concatenate them into one image\nbefore fine-tuning because Break-A-Scene can only learn subjects\nwithin one single image.\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\nplayfulness\ndrag-and-drop\nbeautiful park\nstanding behind \n,\nfacing left\n#car\ncolorful\nmove\nresize\ngroup\ncross-reference\nlink\nb2\nb1\nb3\nb4\nb5\nb6\na\nc\n#car\nMood Board Panel\nA\nC\nB\nHistory Panel\nToken Manipulation Panel\nGenerate\nFigure 5: User interface of Brickify, consisting of three panels: (A) Mood Board Panel for arranging reference images and creating\npersistent design tokens (subject, color, style, concept), which can be drag-and-dropped (b1) into (B) Token Manipulation Panel\nfor direct manipulation (b2 – b6). Clicking the Generate button (c), generated results are organized in (C) History Panel.\n6.1.2\nColor token. Users can extract color tokens both automati-\ncally and manually. Using the color tool and clicking on an image\nautomatically extracts five dominant colors as color tokens based\non K-Means clustering. If the extracted colors do not meet the user’s\nneeds, they can click on them to manually change their colors with\nthe color picker. If they want to create more color tokens based on\none image, they can click again on the image to create a circle with\na random color, allowing users to manually select a color with an\neyedropper tool to create a color token.\n6.1.3\nStyle token. Users indicate their desire to use an image’s style\nby clicking on it with the style tool. We leverage Style-Align [41]\nto transfer the image’s style to a standard balloon image, which is\nthen cropped to the marker shape, creating a style token.\n6.1.4\nConcept token. Concept tokens capture the high-level spirit\nor emotional feeling of an image in textual format. When users click\non an image with the concept tool, GPT-4o describes the feeling\nand atmosphere of an image, summarizing it into five keywords.\n6.2\nVisual Lexicon Execution\nTo ensure a smooth iterative design experience, we selected the\napproaches for the visual lexicon execution that do not require\ntraining or fine-tuning on diffusion models. It should be noted that\nthe field of computer vision evolves rapidly and methods could be\nreplaced as better solutions emerge, our goal is to provide a feasible\ntechnical pipeline for executing the visual lexicon users create. The\nexecution consists of four primary steps: handling layout, style,\nglobal colors, and local colors (Fig.6). This order is deliberately de-\nsigned to prevent visual effects from being overridden. For example,\nhandling the style inevitably changes the colors to some degree, so\ncolor adjustments must come afterward. Similarly, global colors are\nhandled before local colors.\n6.2.1\nExtend keywords description. Users often use only keywords\nin textual tokens, but diffusion models perform better with complete\nsentences as prompts. We thus extend textual tokens into sentences\nusing GPT-4o. The size of the imagination token determines the\nlevel of detail added, with three levels: small, medium, and large. If\nno imagination tokens are used, only factual information is stated\nwithout any added imagination.\n6.2.2\nCompose the layout. To compose subjects into the desired\nlayout, we use BoxDiff [101], which constrains image genera-\ntion with spatial control guidance. For the foreground, it takes\nthe subject token placements as bounding boxes and related tex-\ntual keywords describing each bounding box as input, triggering\nthe special tokens in the pre-trained diffusion model fine-tuned\nby Break-A-Scene to generate subjects according to the specified\nlayout. It also handles the background generation with the given\ntext description.\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\n(b) Compose the layout\n(a) Learn visual identity\nReferences\nOutcome\n(d) Apply global colors\n(c) Align the style\n(e) Calibrate local colors\nS2\nS1\nS2\nBreak-A-Scene\nS1\nBoxDiff\nStyle-Align\nHistoGAN\nLocal Color Adjust\nS1\nS2\nFigure 6: The technical pipeline of Brickify interprets and executes the visual lexicon step-by-step, using off-the-shelf methods.\n6.2.3\nAlign the global style. By default, the image composed in the\nlayout at the previous step is in a realistic, natural style. To align it\nwith the user-specified style token, we use the Style-Align [41]\nmethod. Style-Align performs shared self-attention with the ref-\nerence style image at each diffusion denoising step to achieve style\nalignment during the image reconstruction process.\n6.2.4\nApply global colors. To apply the global color palette to the\nimage, we employ the HistoGan [1] to recolorize images based on\nthe given palette. Specifically, we use the pre-trained checkpoint of\nUniversal model-0. This method is based on generative adversar-\nial networks (GANs) and optimizes the recolored image to match\nthe color proportion distribution by projecting color histogram\nfeatures into the model’s latent space.\n6.2.5\nCalibrate local colors. Lastly, we handle the color tokens\nattached to local subjects. Blended Latent Diffusion [6] is lever-\naged to perform local color modifications on the image. This method\ntargets the local editing of generic images, where the desired edits\nare confined to a user-provided mask without touching the rest.\n7\nUSER STUDY DESIGN\nWe conducted an in-lab user study to evaluate the effectiveness of\nBrickify in intent expression and users’ experience when interact-\ning with the system of Brickify. The study involved two tasks with\nthe same set of participants.\n7.1\nParticipants\nWe recruited 12 experienced designers via social media and mailing\nlists. All participants hold a formal design degree and have more\nthan 3 years of experience in graphic design. Participants rated their\nfrequency of using generative AI tools for text-to-image generation\non a 5-point Likert scale (1 = “never” to 5 = “very often”), with an\naverage rating of 3.33. Their detailed demographic information is\nlisted in Appendix 1. Participants took part in the study remotely.\nAll study sessions were audio and video recorded. The entire study\nlasted about 75 - 100 minutes, and participants were compensated\nwith the equivalent of $30 CAD. The study was approved by the\nuniversity’s ethics review board.\n7.2\nStudy 1: Interaction Paradigm Comparison\nStudy 1 uses a replication task to simulate a scenario where design-\ners have a well-developed idea in mind. The goal is to answer the\nresearch question (RQ1): How does the visual-centric interaction\nparadigm of Brickify compare to the textual-centric paradigm in\nterms of clarity, mental effort, and time investment for expressing\ndesign intent?\nEasy-v1\nEasy-v2\nHard-v1\nHard-v2\nReference Images\nSTUDY 1\nTarget Images\nFigure 7: Reference and target images for Study 1. For each\ncondition (Easy and Hard), users work with two versions:\none created from scratch and a second tweaked version where\nusers adjust their original description to match the modified\ntarget image.\n7.2.1\nExperimental Design. We use a 2 × 2 within-subject design\nwith two primary factors: Techniqe (Brickify or Baseline) ×\nDifficulty (Easy or Hard). The Brickify condition, as described\nin Section 4, is our proposed interaction paradigm, accessed through\nthe Brickify interface shown in Figure 5, with the Generate button\ndisabled. In the Baseline condition, participants describe their de-\nsign intentions by typing textual prompts in a Google Doc, with\nthe ability to refer to reference images by their provided names.\nWe avoided using existing commercial text-to-image interfaces like\nDall-E or MidJourney as baselines due to their differing interaction\ndesigns — Dall-E allows uploading an unlimited number of images\nbut lacks a clear mechanism to reference them while MidJourney\nuses command-style prompts and requires images to be in URL for-\nmat. To ensure fairness for participants familiar with different tools,\nwe thus provided this general textual-centric prompting method\nreflecting common generative model interactions as a baseline.\nThe Difficulty levels (Easy and Hard) are determined based\non the number of visual elements and the complexity of their com-\npositions. To establish these conditions, an expert designer selected\nelements from a set of reference images and created initial versions\nfor both difficulty levels. For the Easy condition, the design involved\nfewer elements and simpler compositions, while the Hard condi-\ntion included more elements with more intricate arrangements. The\ndesigner then further refined the compositions to create a second\nversion for each condition, as shown in Fig. 7.\nParticipants were asked to express their design intent for the\ntarget images across the two Difficulty levels (Easy and Hard)\nusing two interaction paradigms: Brickify (visual-centric) and\nBaseline (textual-centric). For each Difficulty level, participants\nfirst described the design of a target image (Easy-v1 or Hard-v1),\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\nthen refined their expression to produce a modified version (Easy-\nv2 or Hard-v2), simulating a real-world design refinement process.\n7.2.2\nMeasurements. To assess whether user expressions suffi-\nciently described the target image, we adopted a human-evaluation\napproach by recruiting three external raters to assess all partici-\npants’ expressions under both Techniqes. Raters self-reported\nfrequent use of text-to-image generation tools (𝑀= 4.33,𝑆𝐷= 0.58;\nscale: 1 = “never” to 5 = “very often”) and demonstrated being knowl-\nedgeable in prompt engineering (𝑀= 3.67,𝑆𝐷= 0.58; scale: 1 =\n“no experience” to 5 = “expert”). The three raters performed the\nevaluation independently on five 7-point Likert Items (i.e., element\ncoverage, size clarity, position clarity, style clarity, and color clarity)\nfollowed the predefined rubric (see Appendix A.3). The expres-\nsions were order-randomized for each rater. The rating process\ntook around 2 hours and the raters received $50 CAD for their time.\nWe chose not to use AI models to directly execute expressions\nand generate final outcomes to compare due to the following rea-\nsons. First, there is no off-the-shelf techniques built on top of Stable\nDiffusion 2.1 (the same base model in our technique) that resemble\nBaseline — taking multiple references as input and leveraging their\nsubjects and styles — to make it comparable. Second, as in this study\nwe focus on the expressivity aspect, uncertainty and complexity in\nthe process of execution for AI models may introduce compound-\ning factors unrelated to the quality of user expressions. Therefore,\nwe instead rely on human raters, whose evaluations could more\neffectively reflect the quality of the expressions from the message\nreceiver perspective.\nTo this end, the measurements for Study 1 included 1) partici-\npants’ responses to five questions evaluating intent expression, 2)\nexternal ratings for participants’ expression in different conditions,\n3) task completion times for both the initial and refined design\nversions, 4) participants’ preferences between the two Techniqes,\nand 5) self-reported cognitive load during the tasks.\n7.3\nStudy 2: Brickify Exploration\nStudy 2 is an open-ended task, without comparison with other\nsystems, designed to explore the research question (RQ2): How\ndoes Brickify influence users’ creative exploration when they start\nwithout a clear intent?\nFigure 8: Reference images used in Study 2.\n7.3.1\nTask design. In this task, participants assumed the role of\njunior graphic designers tasked with creating a graphic series for a\nchildren’s storybook about the adventures of an owl. The senior\ndesigner provided four reference images (Figure 8) to define the\nvisual characteristics. Participants were asked to create three im-\nages depicting scenes where the owl, with or without his friend\nand car, embarks on an adventure. The task required maintaining\nvisual consistency across all images. There was no time limit, and\nparticipants worked until they felt their designs were complete.\n7.4\nProcedure\nAfter signing the consent form, participants were given an overview\nof the study procedure, duration, and data collection details. The\nstudies were conducted remotely via Zoom, with participants ac-\ncessing Brickify through the web browser. A brief training session,\nincluding a toy example, was provided to demonstrate the use of\nBrickify and explain the text-based prompting in the Baseline\ncondition. Participants could familiarize themselves with the tools\nbefore starting the tasks.\nParticipants began with Study 1, completing both Easy and Hard\ntasks using Brickify and the Baseline, with no time limits. The\nsequence of the four trials (Baseline-Easy, Baseline-Hard, Brick-\nify-Easy, Brickify-Hard) was randomly assigned across partici-\npants, as a full counter-balancing was not feasible. After each trial,\nparticipants filled out questionnaires to rate their intent expression\nexperience. Upon completing Study 1, they filled out the question-\nnaire on their preference and cognitive load on a 7-point Likert\nScale. Next, participants proceeded to Study 2. After completing\nthis task, they rated their experience with Brickify using a post-\nstudy questionnaire for Creativity Support Index (CSI) [14]. A\nsemi-structured interview was conducted to gather feedback on\nparticipants’ experiences with Brickify. Participants were encour-\naged to share general comments on any aspect of the study and\nthen prompted on specific aspects, including interface usability\nfrustrations, challenging intention expression cases, prior difficul-\nties with text-centric GenAI tools, and whether similar issues arose\nwith Brickify. They were also asked about the system’s impact on\ntheir approach to solve design problems, exploration of design op-\ntions, and suggestions for improvement. Observations noted by the\nexperimenter during the session were also discussed.\n7.5\nData Analysis\nTo analyze the qualitative feedback, we analyzed interviews us-\ning thematic analysis, employing both inductive and deductive\napproaches. Two researchers collaboratively analyzed and open-\ncoded the transcribed interviewees’ responses, employing affinity\ndiagramming to sort the initial codes onto cards. Then, they dis-\ncussed and reconciled any discrepancies in the coding process to\nensure a consistent and accurate representation of participants’\nperspectives. Through iterative discussions and the organization\nof these codes, we identified a number of recurring patterns and\nthemes within the interview data.\n8\nUSER STUDY RESULTS\n8.1\nSelf-Rated Design Intent Expression\nWe conducted the non-parametric Aligned Rank Transform (ART)\nANOVA [99] statistical analysis on the ordinal Likert-Scale subjec-\ntive ratings for Study 1 to understand the influence of Techniqe\nand Difficulty on users’ self-reported design intent expression\nexperience (Q1-Q5). Results show that there is a significant main\neffect of Techniqe on users’ self-reported design intent expres-\nsion experience across all five questions, while Difficulty had\nsignificant effects for Q2, Q3, Q4, and Q5 but not for Q1. The in-\nteraction effect between Techniqe × Difficulty was significant\nfor Q3 only. The post-hoc multi-factor contrast tests following the\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\n0\n5\n10\nBaseline (Easy)\nQ1: Describing the visual identity of the elements to use was easy\nQ2: Specifying the relationship among visual elements was easy\nQ3: Reusing prompts to explore design alternatives was easy\nQ4: Conveying design intent required low mental demand\nQ5: I am confident that I have expressed my design intent clearly 1\n3\n3\n1\n1\n5\n2\n2\n2\n1\n2\n2\n3\n4\n1\n5\n2\n2\n2\n1\n3\n3\n2\n5\n2\n0\n5\n10\nBrickify (Easy)\n1\n1\n1\n2\n2\n1\n2\n3\n4\n1\n3\n1\n7\n3\n7\n5\n7\n4\n3\n2\n2\n1\n0\n1\n2\n3\n4\n5\nMean Difference & 95% CI\n0\n5\n10\nBaseline (Hard)\nQ1: Describing the visual identity of the elements to use was easy\nQ2: Specifying the relationship among visual elements was easy\nQ3: Reusing prompts to explore design alternatives was easy\nQ4: Conveying design intent required low mental demand\nQ5: I am confident that I have expressed my design intent clearly\n4\n3\n1\n3\n3\n2\n1\n3\n1\n1\n3\n5\n2\n2\n2\n1\n2\n3\n3\n2\n2\n1\n2\n2\n1\n1\n1\n1\n1\n1\n0\n5\n10\nBrickify (Hard)\n1\n1\n2\n1\n1\n1\n1\n1\n2\n2\n4\n3\n2\n5\n4\n4\n5\n6\n5\n4\n3\n1\n1\n2\n1\n0\n1\n2\n3\n4\n5\nMean Difference & 95% CI\nFigure 9: Participants’ response for Study 1 when rating the 7-point statements for Baseline and Brickify interaction paradigm\nunder Easy and Hard conditions. Dots are the mean differences of Brickify compared to Baseline.\n1\n2\n3\n4\n5\n6\n7\nBaseline-Easy\nBrickify-Easy\nBaseline-Hard\nBrickify-Hard\nElement Coverage\n1\n2\n3\n4\n5\n6\n7\nClarity - Size\n1\n2\n3\n4\n5\n6\n7\nClarity - Position\n1\n2\n3\n4\n5\n6\n7\nClarity - Color\n1\n2\n3\n4\n5\n6\n7\nClarity - Style\nFigure 10: External rating for the quality of expressions that participants produced in Baseline and Brickify regarding the\nelement coverage, the clarity of size, position, style, and color.\nART-C [27] procedure were conducted to identify the exact differ-\nences. The results show that participants’ ratings on Q2, Q4, Q5\nwere significantly higher for Brickify than the Baseline in the Easy\ntask. For Hard task, participants’ ratings for Brickify on all five\nquestions (Q1-Q5) were significantly higher than the Baseline.\nThe analysis, along with the detailed scores in Fig. 9, demon-\nstrates that Brickify is more effective than the text-centric in-\nteraction paradigm in supporting users’ design intent expression,\nparticularly in higher difficulty tasks. Brickify simplifies describing\nvisual identity, specifying relationships, reusing prompts for alter-\nnatives, reduces cognitive load, increases expression confidence,\nand enhances the overall design expression experience.\n8.2\nExternal-Rated Expression Quality\nFigure 10 shows the external ratings for participants’ expressions\nacross five items — element coverage, clarity of size, position, color,\nand style (the rubric is shown in Appendix A.3). The reliability\nof the ratings was measured using a two-way random Intraclass\nCorrelation Coefficient (ICC). The ICC values for each item ranged\nfrom 0.688 to 0.930, with an average of 0.817, indicating accept-\nable reliability. We use the average score from the three raters for\neach expression for further statistical analysis. Across different\nDifficulty levels, participants in both the Baseline and Brick-\nify conditions successfully covered most elements in the target\nimages. While Brickify received slightly higher scores for element\ncoverage, the difference was not statistically significant. For the\nperceived clarity, the two-way ANOVAs indicated that the choice\nof Techniqe (i.e., Baseline vs. Brickify) is the primary factor\nsignificantly influencing the clarity of size (𝐹1,92 = 892.15, 𝑝< .001),\nposition (𝐹1,92 = 501.02, 𝑝< .001), color (𝐹1,92 = 58.14, 𝑝< .001),\nand style (𝐹1,92 = 51.80, 𝑝< .001) regardless of task difficulty. These\nresults suggest Brickify provides a more effective approach for\nreducing the ambiguity in intent expression.\n8.3\nTask Completion Time\nIn Study 1, on average, participants took longer to complete the\ninitial version in the Brickify condition compared to the Base-\nline condition (𝑡Baseline_Easy = 324𝑠vs. 𝑡Brickify_Easy = 474𝑠,\n𝑡Baseline_Hard = 444𝑠vs. 𝑡Brickify_Hard = 465𝑠), detailed data is\nshown in Figure 11. However, the differences were not statistically\nsignificant. An ANOVA analysis of the model Initial Comple-\ntion Time ∼Techniqe × Difficulty revealed no significant\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\n200\n400\n600\n800\n1000\nBaseline-Easy\nBrickify-Easy\nBaseline-Hard\nBrickify-Hard\nInitial Completion Time (s)\n0\n100\n200\n300\n400\n500\n600\nRefinement Time (s)\nFigure 11: Users’ Initial Completion Time (left) and the Refinement Time (right) with Baseline and Brickify interaction\nparadigm under Easy and Hard conditions in Study 1. Black dots are means, bars are 95%CIs.\nPreference: Which interaction paradigm would you prefer to use in the future\nPerformance: Which one made you feel more successful in accomplishing the task\nMental Demand: Which one was more mentally demanding to communicate\nHurry: Which one made you feel hurries or rushed during the task\nEffort: For which one did you work harder to accomplish your level of performance\nFrustration: Which one made you feel more disappointed, discouraged, and stressed\n2\n1\n2\n6\n1\n4\n2\n2\n1\n3\n1\n3\n1\n2\n3\n1\n3\n1\n2\n6\n3\n2\n1\n2\n1\n4\n6\n2\n2\n2\nFigure 12: Participants self-reported preference and cognitive load that directly compare Baseline and Brickify.\nmain effects of Techniqe (𝐹1,44 = 2.93, 𝑝> .05) or Difficulty\n(𝐹1,44 = 1.44, 𝑝> .05), and no significant interaction between Tech-\nniqe and Difficulty (𝐹1,44 = 1.32, 𝑝> .05). This result aligns with\nour expectations, as Brickify inherently involves many operations\n(e.g., creating, dragging, and constructing tokens) that participants\nhave to perform to express their intent from scratch. The break-\ndown of Initial Completion Time shows that participants spent\napproximately one-third (𝑡Easy = 139𝑠, 𝑡Hard = 165𝑠) of the total\ntime in creating tokens and dragging them to the manipulation\npanel. The remaining time was dedicated to manipulating these\ntokens to construct the lexicon.\nWhen entering the refinement stage, participants, on average,\ntook less time in Brickify condition to refine their initial expres-\nsion to achieve the modified version than Baseline condition\n(𝑡Baseline_Easy = 121𝑠vs. 𝑡Brickify_Easy = 99𝑠, 𝑡Baseline_Hard =\n309𝑠vs. 𝑡Brickify_Hard = 181𝑠). The ANOVA in the model Refine-\nment Time ∼Techniqe × Difficulty shows significant main\neffects of Techniqe (𝐹1,44 = 5.81, 𝑝< .05) and Difficulty\n(𝐹1,44 = 18.03, 𝑝< .001), but no significant Techniqe × Dif-\nficulty interaction (𝐹1,44 = 2.97, 𝑝> .05) on Refinement Time.\nPost-hoc Tukey HSD tests further show that users spent signifi-\ncantly less Refinement Time in Brickify on average by 128 sec-\nonds (𝑝= .027) in Hard condition, while no significant difference\nfound for Easy condition. This result can be attributed to the fact\nthat participants did not create new tokens in Brickify during\nthe refinement stage; instead, they focused solely on manipulating\nexisting tokens. The faster Refinement Time highlights Brickify’s\nstrength in enabling quicker and more efficient modifications once\nthe initial design intent is established.\n8.4\nUser Preference and Cognitive Load\nAfter completing Study 1, participants were asked to rate their\npreference between Brickify and the Baseline condition, as well as\ntheir cognitive load for each condition (Fig. 12). Participants (11/12)\nshowed a clear preference for Brickify over the Baseline. On a\n7-point Likert scale (1 = strongly prefer Baseline, 7 = strongly prefer\nBrickify), the mean preference rating of 6.0 was significantly above\nthe neutral midpoint of 4. This indicates a strong preference for\nBrickify when participants had a clear design intent to express.\nRegarding cognitive load, most participants (11/12) felt they\nwere more successful using Brickify. Additionally, 7 out of 12\nparticipants reported making less effort, and 8 out of 12 felt less\nfrustrated, suggesting that Brickify simplifies the design intention\nexpression and reduces users’ frustration. However, only half of the\nparticipants felt reduced mental demand and hurry compared to the\nBaseline. As users engage more deeply in articulating their design\nintent, they may invest more time and mental effort in decisions\nsuch as token placements. Overall, Brickify enhances the design\nintent expression clarity and reduces frustration but still requires a\ncertain level of mental effort and time commitment to fully engage\nwith design token manipulations.\n8.5\nUser Behavior\n8.5.1\nToken Usage. Figure 13 illustrates the number of participants\nusing each token type across studies. Subject tokens were consis-\ntently adopted by all participants across all conditions, while color,\nstyle, and textual tokens were used by most participants (more than\n8). There was a slightly reduced usage (but still more than half of\nthe participants) in Study 2 for these tokens, because they focused\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nStudy 1 (Easy)\nStudy 1 (Hard)\nStudy 2\n0\n2\n4\n6\n8\n10\n12\nNumber of Participants\nToken Usage Across Studies\nToken Type\nSubject\nColor\nStyle\nTextual\nImaginative\nFigure 13: Token usage in Brickify across studies, indicating\nthe number of participants have used each token type.\nmore on shaping the narrative by manipulating subject tokens. No-\ntably, no participants used imaginative tokens in Study 1, which\nmay be because of its well-defined scenario with clear targets relied\nentirely on reference images. During Study 2, where the design\nscenario was more exploratory, five participants employed imag-\ninative tokens for (1) enriching background descriptions (P2, P4,\nP10), (2) modifying subject tokens to adjust visual identity and/or\nactions, e.g., differentiating the owl’s friend from the owl (P11), and\n(3) expanding text descriptions for global styles (P4, P9).\n8.5.2\nUser Strategy. Across both studies, we observed several strate-\ngies shared across participants when interacting with Brickify to\nconstruct the visual lexicon. All participants adopted an on-demand\napproach for token creation, using tokens as needed rather than\ncreating all tokens upfront. This led to frequent navigation between\nthe mood board panel and the token manipulation panel. When\nconstructing the visual lexicon, 9 out of 12 participants prioritized\ncreating and positioning subjects first, followed by adding local\ncolors (if any), and then applying global styles and/or colors. The\nremaining participants began by considering global styles and col-\nors before adding subjects and their local colors. We also observed\nthat most participants (N = 10) started with the background or\nunderlying layers and worked progressively toward the foreground.\nAs P2 explained in the interview, this approach likely stems from\ntheir unconscious habit of working with layer-based logic in tools\nlike Adobe Photoshop and Illustrator: “I always build layer by layer,\nback to front in Illustrator and it feels natural to follow that order.”\n8.6\nSelf-reported Creativity Support Index\nWe utilize the Creativity Support Index (CSI) to measure the de-\ngree of creativity support for Brickify in the Study 2. Since Study\n2 does not include a baseline for comparison, we present this self-\nreported rating as a reference point to better understand users’\nexperiences, rather than drawing definitive conclusions. Partici-\npants rated six creativity support factors on a scale from 1 (strongly\ndisagree) to 7 (strongly agree) shown in Fig. 14: expressiveness,\nresults-worth-effort, exploration, enjoyment, immersion, and col-\nlaboration. Overall, Brickify shows strong support for creativity,\neffectively supports idea exploration, and is generally enjoyable\nto use. Although most participants rated positively, one partici-\npant strongly disagreed with the immersion factor, which may\nreflect interface limitations, as noted in an interview where the user\nFigure 14: Self-reported Creativity Support Index for Brickify\nafter Study 2.\nsuggested frequently used tools to create visual tokens should be\ndirectly available in the toolbar rather than in a list.\nThe factor of results worth effort reflects how effectively the\nmodel executes users’ intent. Half of the participants rated it pos-\nitively, while the rest were less satisfied. This disparity arised be-\ncause, while our pipeline for executing visual lexicons (Section 6.2)\nis feasible for simpler cases, its limitations become apparent in\ncomplex scenarios, such as handling multiple subjects or intricate\nrelationships, leaving significant room for improvement. We show-\ncase some participant-created story using Brickify, including plant-\ning a tree (Figure 15), sharing an apple (Figure 16), and hosting a\nmusic party (Figure 17), where they crafted three-scene narratives\neffectively. However, failure cases (Figure 18) reveal that the model\noften omits subjects when there are more than three and/or when\nsubjects overlap significantly (e.g., being “inside”). Additionally, the\nmodel sometimes fails to match specified sizes and occasionally\nproduces patchy images. In response, participants typically regener-\nated outputs with different seeds, which sometimes worked. If not,\nthey reduced the complexity, such as removing some subjects or\ntweaking the layout, to gradually adapt themselves to the model’s\ncapacity limits. We anticipate that as the base model (currently\nStable Diffusion 2.1) continues to grow in size and evolve in ar-\nchitecture, its performance in executing the visual lexicon would\nimprove, thus mitigating this problem.\n8.7\nObservations and Participants’ Feedback\n8.7.1\nDecomposing a reference image into elements is more effective\nthan using it as a whole. Participants consistently valued decom-\nposing reference images into individual elements rather than using\nthem as a whole, aligning with their design process. P3 noted, “Be-\ning able to break down an image into parts lets me mix and match\nelements in a way that fits my vision, rather than being constrained by\nthe original composition”. Compared to their previous experiences\nwith tools that only allow for remixing entire images, participants\nfound the ability to recombine multiple decomposed elements and\nspecify their relationships particularly valuable. P7 expressed, “With\nother tools, I’d have to try different images multiple times to get some-\nthing close to what I want. Here, I can just pull out the pieces I need\nand arrange them how I like”.\n8.7.2\nBrickify enhances the sense of control but requires a tar-\nget in mind. Participants unanimously agreed that Brickify offers\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\n1\n2\n3\nFigure 15: P3 generated results with Brickify, illustrating a story where (1) two owls plant a tree together, (2) nurture it with\ncare, and (3) later, one owl drives by and happily witnesses the tree’s growth into tall and strong.\n1\n2\n3\nFigure 16: P6 generated results with Brickify. The user intends to describe (1) an owl discovers some apples in a park, picks one,\nand (2) brings it home to share with a friend, and (3) his friend puts the apple on the head, sharing a happy moment together.\n1\n2\n3\nFigure 17: P10 generated results with Brickify, depicting a story where (1) the owl and his friend discover and move a guitar\nfrom a warehouse, (2) joyfully sing and play the guitar on a car, and (3) invite another friend to join them for a lively music\nparty.\na\nb\nd\nc\nFigure 18: Failure cases: (a) when there are multiple subjects, the model often omits some of them (from P1); (b) when two\nsubjects overlap significantly, the model struggles and incorrectly interprets the interaction as being “inside” (from P5); (c) the\nsize of generated subjects sometimes may not align with the user-specified one; and (d) the model occasionally fails to produce\na cohesive image, instead generating patchy outputs with visible edges (from P11).\na greater sense of control compared to using natural language\nprompts. They found it particularly useful for defining and manip-\nulating the relationships between elements. P4 noted, “it (Brickify)\nmakes it so much easier to specify how different parts of my design\ninteract. I feel like I have more direct control over the outcome”. How-\never, participants also highlighted that this increased control comes\nwith a prerequisite: having a clear idea or target in mind. When\nthey were unsure of what they wanted to create, they found that\nnatural language prompts offered a quicker and more flexible start-\ning point. As P9 explained, “If I don’t have a clear idea, it’s easier\nto just throw in some random words and see what the AI generates,\nit’s a good way to get inspired”. These feedback implies a trade-off\nbetween control and exploration. Natural language allows for broad\nexploration and can spark new ideas even from vague or random\ninputs, while Brickify excels in depth and precision when users\nhave a rough direction or specific visual properties in mind. As P1\nput it, “Once I know the general look I’m going for, it (Brickify) lets\nme really arrive there”.\n8.7.3\nDirect manipulation on design tokens helps reduce the efforts\nof refinements. Brickify allows for more precise and enjoyable\nfine-tuning. Participants felt useful to directly manipulate elements\nwithout losing the core identity of their design. “[With Brickify,] I\ncan make the adjustments I want without compromising the overall\nlook, I actually very enjoy this refinement process”, shared by P5.\nThe persistence of design tokens throughout the creative process\nwas particularly valued. Participants liked that once a design token\nwas created, it could be reused throughout a session, streamlining\nthe workflow and ensuring consistency across different iterations.\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\n“I love that I can just drag and drop a copy of a token for reuse,”\nP8 shared, “It keeps my design consistent without having to start\nfrom scratch every time”. In contrast, participants mentioned the\nchallenges they faced with other tools when trying to make minor\nrefinements. In many cases, they found that the visual identity of\ntheir work would start to drift with each new text-based prompt,\nleading to frustration. “I’ve tried making refinements in other tools,\nbut often the visuals change too much, even when I just want minor\ntweaks. It’s so frustrating that I usually just switch to Photoshop to\nfix it myself”, P12 explained.\n8.7.4\nBrickify do not require a rigid structure thus enable a flexible\nthinking flow. Several (P1, P2, P4, P7, P8, P9, P11) participants men-\ntioned they benefited from freely constructing their visual lexicon\nwithout worrying about ordering and format constraints. They felt\nBrickify can enable them to think non-linearly and creatively. P7\nnoted, “with text prompts, I was advised to follow a template for\neffective prompting, though I don’t know if it really matters: start\nwith the overall context, list subjects, describe the view with terms\nlike headshots or close-ups, and then add magic words for styling. It\nfeels like I’m being forced to think in a certain way, which is not how\ndesign should work, it is supposed to be messy. You think about this,\nthen that, then come back to adjust one accordingly, the elements are\ninfluencing each other”. P11 expressed frustrations that “different\ntools often require different prompt formats, making it (prompting\nwith texts) more complicated”. In contrast, Brickify does not assume\na procedural process to construct the visual lexicon, users are free\nto start from any design aspects and seamlessly navigate between\nthem. Overall, the flexibility offered by Brickify was seen as a\nsignificant advantage, enabling users to engage in a more dynamic\nand less restrictive creative thinking process.\n8.7.5\nClarity in intent expression help users appropriately under-\nstand AI accountability. Users often face challenges in correctly\nattributing AI failures due to the lack of transparency in how AI\ninterprets their inputs. This can lead to confusion, where users\nmistakenly believe that failures are due to unclear intent rather\nthan AI’s limitations. As P5 mentioned, “Sometimes, I’m not sure\nif the problem is with how I’m phrasing things”. This uncertainty\noften results in users repeatedly refining their inputs, leading to\nunnecessary back-and-forth. However, with Brickify, users felt\nmore confident in the clarity of their intent expression, such confi-\ndence making users can appropriately attribute failures to AI. P12\ncommented that “[With Brickify,] I know if something goes wrong,\nit’s probably the AI, not me”.\n9\nDISCUSSION\nWe reflect on the design of Brickify and discuss the lessons we\nlearned and the implications for future research.\n9.1\nDesign Implications\n9.1.1\nIntegrating texts into visual-centric paradigm versus embed-\nding visuals into texutal-centric paradigm. Brickify addresses two\nkey ambiguities in textual prompts: what elements to use and how\nto construct them for an intended effect. Prior work, like Direct-\nGPT [61], also augments textual prompts by embedding visual\nsymbols to clarify object references, focusing on the what aspect.\nHowever, these two approaches differ fundamentally in their ex-\npressiveness of the how aspect due to their structural nature. Visual-\ncentric paradigms like Brickify operate in a two-dimensional (2D)\nspace, enabling spatial manipulation and richer exploration of re-\nlationships between elements, with text serving as a complement.\nIn contrast, text-centric paradigms with embedded visuals, such as\nDirectGPT, function within a linear, one-dimensional (1D) space,\nwhere the narrative sequence is preserved, and visuals enhance\nobject references or enrich textual information. We argue that the\neffectiveness of either paradigm is likely task-dependent. For visual\ntasks, the spatial properties of a 2D approach can provide more\nintuitive and efficient interactions, aiding in design, spatial rea-\nsoning, and layout organization. However, this may disrupt the\nnarrative flow, making it less suitable for tasks that require step-by-\nstep instructions such as programming. Conversely, a text-centric\napproach with visual enhancements may be better suited for tasks\nrequiring logical reasoning and narrative coherence. This distinc-\ntion raises important questions for future research: when should\none paradigm be chosen over the other? Or is there potential for\nblending the spatial advantages of visuals with the narrative flow\nof text to enable a unified paradigm for various tasks?\n9.1.2\nDynamic distributed agency between user and AI: letting users\nto specify when, where, and how much. Brickify is designed to re-\nduce the ambiguity in design intent expression and thus improve\nusers’ sense of control when working with AI. We recognize that\nusers’ required control varied between individuals with different\nskill levels and varied at different stages in design process. When\nusers seek to leverage AI’s creativity, they often choose to leave\nambiguity in the visual lexicon for the AI to refine. Conversely,\nwhen they have a clear vision, they specify their intentions with\ndetailed visual tokens and intricate spatial manipulations, letting AI\nto execute their vision with precision. Thus, the agency distribution\nbetween user and AI is dynamically changing, also discussed by\nSatyanarayan et al. [75]. In Brickify, users can actively and explic-\nitly configure when, where, and how much control they wish to shift\nto AI through the use of the imagination token. For instance, during\nthe Study 2, P2 provided a brief description “a beautiful park” as\nbackground, and assign a large imagination token, signaling the AI\nto elaborate. Participants in our study valued this flexibility and\ncontrol. In contrast, most current text-to-image tools, like DaLLE,\nMidJourney, and Adobe Firefly, automatically expand and refine\nusers’ whole prompts without asking users if they want, leading to\nunintended results. This informs the importance of providing an\nexplicit way for users to actively delegate control to AI — managing\nwhen, where, and how much — rather than assuming a fixed agency\ndistribution pre-defined by the system.\n9.1.3\nTowards bi-directional visual lexicon construction: enabling\nboth users and AI to be constructors. Reflecting the current design\nof Brickify, design tokens serve as the communication medium\nbetween users and AI, while the constructed visual lexicon acts as a\nvisual abstraction of the generated image. However, Brickify only\nallows users to construct the visual lexicon, with AI solely acting\nas the receiver to execute it. This workflow presents a challenge:\nas mentioned by some participants, in the early ideation stage\nwhen users may not have a clear vision, they would prefer natural\nlanguage as a quick starting point. But how can users complete the\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\niterative design process without an initial visual lexicon? What if\nthey wish to refine an AI-generated image instead? The current\nsystem of Brickify does not fully close this interaction loop. One\npotential solution is to enable AI to construct the visual lexicon as\nwell. Given an image, AI could automatically extract design tokens\nand compose a corresponding visual lexicon for users to manipulate.\nThis approach is similar to the prior work on abstraction-driven\ncolor manipulation for image [76] and motion graphic videos [77],\nwhere the system creates color abstractions for manipulation. This\ninforms that visual abstractions can be constructed bi-directionally,\nwith both users and AI acting as constructors. However, this also\nraises the questions warrant more in-depth exploration in future\nwork: how to decide the granularity of the AI-generated visual\nlexicon; what elements should be reified into tokens; and what\nrelationships should be reflected?\n9.2\nDesign Opportunities in Brickify\n9.2.1\nDiversifying design token types and sources. Currently, Brick-\nify supports limited visual token types including subject, style, and\ncolor. However, there are more visual elements essential for con-\nstructing a successful design, such as camera angle, depth, texture,\nand material. Brickify can naturally incorporate them by adding\ncorresponding token types. For example, with a camera token, users\ncan specify the camera angle by positioning it in the visual lexicon,\nwhich is otherwise hard to express with texts. It is also possible to\nautomate the token creation process by decomposing an image into\nlow-level design elements [91]. It is worth noting that Brickify\ncurrently only supports reference image as the source of visual\ntokens. While starting from references is common, it is certainly\nnot the only approach that designers use. Designers could draw\ninspirations from other sources such as their own memory, using\nsketches to externalize and articulate their intent [46, 50]. Expand-\ning Brickify to incorporate sketch as a source of design tokens is\na promising way to accommodate such scenarios.\n9.2.2\nCustomizing and Re-configuring design tokens by users. While\nBrickify can expand the supported design token types, it is imprac-\ntical to preset every possible type. A valuable future direction is to\nallow users to make their own tokens. Users could define tokens\nby describing their functions in natural language and providing ex-\namples. The system would then dynamically generate and support\nthese tokens. Furthermore, preset tokens may not always match\nuser’s intended usage. It would be more flexible to allow users\nto appropriate the pre-designed tokens for their specific needs —\nreconfiguring their represented meanings. For example, while cur-\nrent color tokens only convey proportional information through\nsize, we observed P3 using their position to indicate specific ar-\neas on a subject to colorize. Lastly, it is important to constrain\nsuch customization and reconfiguration to remain interpretable by\ngenerative AI models to ensure effective interaction.\n9.2.3\nAffording richer manipulations on design tokens. Brickify\ncurrently supports the manipulations of resizing, positioning, group-\ning, and linking. However, during the user study, participants at-\ntempted additional manipulations beyond those provided. For ex-\nample, in the first task, many participants (7/12) tried to rotate the\ntoken to indicate the pumpkin’s position. Similarly, some (3/12)\nwanted to move subject tokens forward or backward to specify\nspatial relationships. These observations reveal the potential for\nour interaction paradigm to support more intricate user intentions.\nBeyond spatial relationships, other behaviors like blending could\nalso be supported. While richer manipulations can enhance user\nexperience, they also introduce complexity. Ideally, users should\nrely on technical reasoning — intuitively understanding how ma-\nnipulations affect outcomes — rather than procedure learning —\nmemorizing steps to achieve desired effects [9]. Future work will\ninvestigate what manipulations are both desired and natural for\nusers to express their intentions effectively.\n9.2.4\nPropagating the modifications of the design token. A limita-\ntion of the current version of Brickify is that only textual tokens\nare editable, while visual tokens, once created, cannot be re-linked\nto another visual element. For example, if a user creates a subject\ntoken of an owl and uses it in multiple designs, changing this owl to\na rabbit requires reconstructing everything from scratch. To address\nthis, a possible improvement is to allow design token modification\nwith automatic propagation to all instances. Since we designed\nthe visual tokens to be persistent and every time users construct a\nvisual lexicon, they drag a copy of the original token, this creates\na natural link between the original and its copies. Leveraging this\nlink, once users modify the original token, such as re-attaching it\nto a new subject in another image, we can propagate this modifica-\ntion to all its copies in related visual lexicons. Such a propagation\nmechanism can allow designers to quickly compare different visual\ncandidates and streamline their workflow.\n9.2.5\nBeyond static graphic design: extending Brickify to video,\n3D scene, and other co-creation tasks with AI. While Brickify is\ninitially designed for static graphic design, its visual-centric inter-\naction paradigm holds significant potential for broader applications\nin general AI-assisted visual design tasks. In video creation, for\ninstance, Brickify could adapt the visual lexicon to a timeline-\nbased structure, where individual lexicons construct each scene.\nSimilarly, in 3D scene modeling, where spatial relationships are\nmore complex, Brickify could extend 2D design tokens into 3D\ntokens and extend the 2D manipulations to 3D operations. While\ntoken design and manipulations may be domain-specific, the fun-\ndamental interaction logic of using direct manipulation on tokens\nto construct elemental relationships remains coherent and consis-\ntent across different design domains. We believe Brickify opens\nup the possibility to offer designers a unifying design language to\ncommunicate with AI across the broad creative landscape.\n9.3\nLimitations\n9.3.1\nBrickify might fail in describing unseen visuals beyond recom-\nbination. Each modality has special strengths and weaknesses in\nits ability to communicate particular concepts [12]. While Brickify\nexcels at referring to elements and describing spatial relationships,\nits reliance on existing visuals might lead to design fixation [46],\nwhere designers may unconsciously adhere to what has already\nbeen known or available. In contrast, natural language could de-\nscribe unseen visuals that go beyond recombination (e.g., “a cute\nsock with a human-like face”) or ideas that might seem unreason-\nable (e.g., “time is melting”). Therefore, our proposed visual-centric\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\ninteraction paradigm is not intended to replace the text-centric ap-\nproach. Instead, it is important for designers to strategically choose\nthe most suitable modality based on the design context and the\nlevel of originality they seek to achieve.\n9.3.2\nVisual lexicon extraction could be improved. Our implemen-\ntation of visual lexicon execution relies heavily on off-the-shelf\ncomputer vision techniques. Despite that we selected the state-of-\nthe-art ones at the time of developing Brickify, these techniques\nhave inherent limitations that impact our system’s capacity. Cur-\nrently, Brickify supports only 4-6 subject tokens. This constraint\narises because we use the Break-A-Scene [5] approach to preserve\neach subject’s visual identity. However, our experiments show that\nwhen there are more than six subjects, the performance in main-\ntaining visual identity drops significantly. The scope of this work\nis not to improve the performances of computer vision models, but\nwe do hope this work informs the importance of computer vision\nresearch to push the progress forward — training the model to be\naware and preserve visual details rather than solely taking natural\nlanguage as input.\n9.3.3\nInference and computation costs could hinder user experience.\nDue to the high computational costs and inference time of diffusion\nmodels, we cannot support on-the-fly inference and immediate feed-\nback. Our visual lexicon execution pipeline sequentially handles\ndifferent design aspects (layout, style, and color), each requiring\na 50-step diffusion model inference (around 30 seconds). As a re-\nsult, users must click a button to generate and wait for the results,\ninterrupting their design experience to some degree. We envision\nthat as inference time and computation costs decrease, users will\nno longer need to click the generation button after constructing\nthe visual lexicon. Instead, they will receive immediate feedback\nwhile manipulating the tokens. This will enable users to instantly\nsee the effects of their actions, providing a smoother co-creation\nexperience with generative models.\n9.3.4\nStudy results might not be generalizable for design novices.\nParticipants in our study are experienced designers, all with a mini-\nmum of 3 years of design experience. These participants are trained\nto approach design problems visually, and Brickify was specifically\ndesigned to align with this visual-centric mental model. As a result,\nthe study’s findings may not generalize to novice designers or ca-\nsual users who lack this level of expertise. It is uncertain whether\nnovice users could adapt to this visual-centric paradigm and fully\nleverage the fine-grained control.\n10\nCONCLUSION\nIn this paper, we introduce Brickify, a visual-centric interaction\nparadigm that allows users to express design intent more effectively.\nBrickify reifies primitive design elements from reference images\ninto interactive, reusable design tokens, enabling users to specify\nwhat elements to use and how to construct them towards the desired\neffect. We implement Brickify to exemplify how state-of-the-art\nAI models can execute users’ intent expressed through Brickify.\nIn a user study, experienced designers found it easier to describe\nvisual details and relationships with fewer mental demands through\nBrickify. They efficiently explored design alternatives by reusing\ntokens and performed refinements more quickly, particularly for\ncomplex designs. Designers preferred Brickify over textual-centric\nprompting approach, valuing the sense of control it provided when\nthey had design ideas in mind. Moving forward, we plan to extend\nBrickify to include more element types and operations, broadening\nits expressive capabilities. The design implications derived from this\nwork shed light on future research to design effective interaction\nmediums for human-AI co-creation.\nACKNOWLEDGMENTS\nWe sincerely thank Xueguang Ma, Ryan Yen, and anonymous re-\nviewers for their insightful suggestions that have led to a great\nimprovement of this work. We also extend our appreciation to An-\nnie Sun and Hakeerat Singh Mayall for their kind help on a few\nearly-stage prototypes. We would also like to thank all our partici-\npants for their time and valuable input. This work is supported in\npart by the Discovery Grant (RGPIN-2020-03966) from the NSERC\n(Natural Sciences and Engineering Research Council of Canada)\nand a gift fund from Adobe Systems Inc.\nWe acknowledge that much of our work takes place on the tradi-\ntional territory of the Neutral, Anishinaabeg, and Haudenosaunee\npeoples. Our main campus is located on the Haldimand Tract, the\nland granted to the Six Nations that includes six miles on each side\nof the Grand River.\nREFERENCES\n[1] Mahmoud Afifi, Marcus A Brubaker, and Michael S Brown. 2021. Histogan:\nControlling colors of gan-generated and real images via color histograms. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition.\n7941–7950.\n[2] Flux AI. 2024. https://flux-ai.io/ Accessed: November 22, 2024.\n[3] Tyler Angert, Miroslav Suzara, Jenny Han, Christopher Pondoc, and Hariharan\nSubramonyam. 2023. Spellburst: A Node-based Interface for Exploratory Cre-\native Coding with Natural Language Prompts. In Proceedings of the 36th Annual\nACM Symposium on User Interface Software and Technology. 1–22.\n[4] Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, and\nElena L Glassman. 2024. ChainForge: A Visual Toolkit for Prompt Engineering\nand LLM Hypothesis Testing. In Proceedings of the CHI Conference on Human\nFactors in Computing Systems. 1–18.\n[5] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischin-\nski. 2023. Break-a-scene: Extracting multiple concepts from a single image. In\nSIGGRAPH Asia 2023 Conference Papers. 1–12.\n[6] Omri Avrahami, Ohad Fried, and Dani Lischinski. 2023. Blended latent diffusion.\nACM Transactions on Graphics (TOG) 42, 4 (2023), 1–11.\n[7] Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for\ntext-driven editing of natural images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 18208–18218.\n[8] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel.\n2022. Text2live: Text-driven layered image and video editing. In European\nconference on computer vision. Springer, 707–723.\n[9] Michel Beaudouin-Lafon, Susanne Bødker, and Wendy E Mackay. 2021. Genera-\ntive theories of interaction. ACM Transactions on Computer-Human Interaction\n(TOCHI) 28, 6 (2021), 1–54.\n[10] Michel Beaudouin-Lafon and Wendy E Mackay. 2000. Reification, polymorphism\nand reuse: three principles for designing visual interfaces. In Proceedings of the\nworking conference on Advanced visual interfaces. 102–109.\n[11] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long\nOuyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. 2023. Improving im-\nage generation with better captions. Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf 2, 3 (2023), 8.\n[12] Bill Buxton. 1989. The “Natural” language of interaction: A perspective on\nnon-verbal dialogues. INFOR: Information Systems and Operational Research 27,\n2 (1989), 221–229.\n[13] Xiang’Anthony’ Chen, Jeff Burke, Ruofei Du, Matthew K Hong, Jennifer Jacobs,\nPhilippe Laban, Dingzeyu Li, Nanyun Peng, Karl DD Willis, Chien-Sheng Wu,\net al. 2023. Next steps for human-centered generative ai: A technical perspective.\narXiv preprint arXiv:2306.15774 (2023).\n[14] Erin Cherry and Celine Latulipe. 2014. Quantifying the creativity support\nof digital tools through the creativity support index. ACM Transactions on\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\nComputer-Human Interaction (TOCHI) 21, 4 (2014), 1–25.\n[15] Lydia B Chilton, Ecenaz Jen Ozmen, Sam H Ross, and Vivian Liu. 2021. VisiFit:\nStructuring iterative improvement for novice designers. In Proceedings of the\n2021 CHI Conference on Human Factors in Computing Systems. 1–14.\n[16] Lydia B Chilton, Savvas Petridis, and Maneesh Agrawala. 2019. Visiblends: A\nflexible workflow for visual blends. In Proceedings of the 2019 CHI Conference on\nHuman Factors in Computing Systems. 1–14.\n[17] DaEun Choi, Sumin Hong, Jeongeon Park, John Joon Young Chung, and Juho\nKim. 2024. CreativeConnect: Supporting Reference Recombination for Graphic\nDesign Ideation with Generative AI. In Proceedings of the CHI Conference on\nHuman Factors in Computing Systems. 1–25.\n[18] John Joon Young Chung and Eytan Adar. 2023. Artinter: AI-powered Boundary\nObjects for Commissioning Visual Arts. In Proceedings of the 2023 ACM Designing\nInteractive Systems Conference. 1997–2018.\n[19] John Joon Young Chung and Eytan Adar. 2023. PromptPaint: Steering Text-\nto-Image Generation Through Paint Medium-like Interactions. In Proceedings\nof the 36th Annual ACM Symposium on User Interface Software and Technology.\n1–17.\n[20] John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan\nAdar, and Minsuk Chang. 2022. TaleBrush: Sketching stories with generative\npretrained language models. In Proceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems. 1–19.\n[21] Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and Yuval Atzmon. 2022.\n“This is my unicorn, Fluffy”: Personalizing frozen vision-language representa-\ntions. In European conference on computer vision. Springer, 558–577.\n[22] ComfyUI. 2024. https://www.comfy.org/en/ Accessed: November 22, 2024.\n[23] Nigel Cross. 1990. The nature and nurture of design ability. Design studies 11, 3\n(1990), 127–140.\n[24] OpenAI DALL·E3. 2024. https://openai.com/index/dall-e-3/ Accessed: Novem-\nber 22, 2024.\n[25] Donis A Dondis. 1974. A primer of visual literacy. Mit Press.\n[26] Claudia Eckert and Martin Stacey. 2000. Sources of inspiration: a language of\ndesign. Design studies 21, 5 (2000), 523–538.\n[27] Lisa A Elkin, Matthew Kay, James J Higgins, and Jacob O Wobbrock. 2021. An\naligned rank transform procedure for multifactor contrast tests. In The 34th\nannual ACM symposium on user interface software and technology. 754–768.\n[28] Nada Endrissat, Gazi Islam, and Claus Noppeney. 2016. Visual organizing:\nBalancing coordination and creative freedom via mood boards. Journal of\nBusiness Research 69, 7 (2016), 2353–2362.\n[29] John M Findlay and Iain D Gilchrist. 2003. Active vision: The psychology of\nlooking and seeing. Number 37. Oxford University Press.\n[30] Ronald A Finke, Thomas B Ward, and Steven M Smith. 1996. Creative cognition:\nTheory, research, and applications. MIT press.\n[31] Adobe Firefly. 2024. https://www.adobe.com/ca/products/firefly.html Accessed:\nNovember 22, 2024.\n[32] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano,\nGal Chechik, and Daniel Cohen-or. 2022. An Image is Worth One Word: Per-\nsonalizing Text-to-Image Generation using Textual Inversion. In The Eleventh\nInternational Conference on Learning Representations.\n[33] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. 2023. Encoder-based domain tuning for fast personalization\nof text-to-image models. ACM Transactions on Graphics (TOG) 42, 4 (2023),\n1–13.\n[34] Steve Garner and Deana McDonagh-Philp. 2001. Problem interpretation and\nresolution via visual stimuli: the use of ‘mood boards’ in design education.\nJournal of Art & Design Education 20, 1 (2001), 57–64.\n[35] Frederic Gmeiner, Humphrey Yang, Lining Yao, Kenneth Holstein, and Nikolas\nMartelaro. 2023. Exploring challenges and opportunities to support designers in\nlearning to co-create with AI-based manufacturing design tools. In Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems. 1–20.\n[36] Gabriela Goldschmidt and Dan Tatsa. 2005. How good are good ideas? Correlates\nof design creativity. Design studies 26, 6 (2005), 593–611.\n[37] Charles Goodwin. 2015. Professional vision. In Aufmerksamkeit: Geschichte-\ntheorie-empirie. Springer, 387–425.\n[38] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and\nFeng Yang. 2023. Svdiff: Compact parameter space for diffusion fine-tuning.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision.\n7323–7334.\n[39] Eleanor R Heider. 1972. Universals in color naming and memory. Journal of\nexperimental psychology 93, 1 (1972), 10.\n[40] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-or. 2022. Prompt-to-Prompt Image Editing with Cross-Attention Control.\nIn The Eleventh International Conference on Learning Representations.\n[41] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. 2024. Style\naligned image generation via shared attention. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 4775–4785.\n[42] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion proba-\nbilistic models. Advances in neural information processing systems 33 (2020),\n6840–6851.\n[43] Josh Holinaty, Alec Jacobson, and Fanny Chevalier. 2021. Supporting refer-\nence imagery for digital drawing. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 2434–2442.\n[44] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou.\n2023. Composer: creative and controllable image synthesis with composable con-\nditions. In Proceedings of the 40th International Conference on Machine Learning.\n13753–13773.\n[45] Alexander Ivanov, David Ledo, Tovi Grossman, George Fitzmaurice, and Fraser\nAnderson. 2022. MoodCubes: Immersive spaces for collecting, discovering and\nenvisioning inspiration materials. In Proceedings of the 2022 ACM Designing\nInteractive Systems Conference. 189–203.\n[46] Ben Jonson. 2005. Design ideation: the conceptual sketch in the digital age.\nDesign studies 26, 6 (2005), 613–624.\n[47] Youwen Kang, Zhida Sun, Sitong Wang, Zeyu Huang, Ziming Wu, and Xi-\naojuan Ma. 2021. MetaMap: Supporting visual metaphor ideation through\nmulti-dimensional example-based exploration. In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems. 1–15.\n[48] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel,\nInbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image editing\nwith diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 6007–6017.\n[49] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\n2023. Segment anything. In Proceedings of the IEEE/CVF International Conference\non Computer Vision. 4015–4026.\n[50] David Kirsh. 2010. Thinking with external representations. AI & society 25\n(2010), 441–454.\n[51] Janin Koch, Andrés Lucero, Lena Hegemann, and Antti Oulasvirta. 2019. May\nAI? Design ideation with cooperative contextual bandits. In Proceedings of the\n2019 CHI Conference on Human Factors in Computing Systems. 1–12.\n[52] Janin Koch, Nicolas Taffin, Michel Beaudouin-Lafon, Markku Laine, Andrés\nLucero, and Wendy E Mackay. 2020. Imagesense: An intelligent collaborative\nideation tool to support diverse human-computer partnerships. Proceedings of\nthe ACM on human-computer interaction 4, CSCW1 (2020), 1–27.\n[53] Janin Koch, Nicolas Taffin, Andrés Lucero, and Wendy E Mackay. 2020. Se-\nmanticCollage: enriching digital mood board design with semantic labels. In\nProceedings of the 2020 ACM Designing Interactive Systems Conference. Associa-\ntion for Computing Machinery, New York, NY, USA, 407–418.\n[54] Krea.ai. 2024. https://www.krea.ai/home Accessed: November 22, 2024.\n[55] Chinmay Kulkarni, Stefania Druga, Minsuk Chang, Alex Fiannaca, Carrie Cai,\nand Michael Terry. 2023. A word is worth a thousand pictures: Prompts as ai\ndesign material. arXiv preprint arXiv:2303.12647 (2023).\n[56] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan\nZhu. 2023. Multi-Concept Customization of Text-to-Image Diffusion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n[57] Vivian Liu, Jo Vermeulen, George Fitzmaurice, and Justin Matejka. 2023. 3DALL-\nE: Integrating text-to-image AI in 3D design workflows. In Proceedings of the\n2023 ACM designing interactive systems conference. 1955–1977.\n[58] Andrés Lucero. 2012.\nFraming, aligning, paradoxing, abstracting, and di-\nrecting: how design mood boards work. In Proceedings of the Designing In-\nteractive Systems Conference (Newcastle Upon Tyne, United Kingdom) (DIS\n’12). Association for Computing Machinery, New York, NY, USA, 438–447.\nhttps://doi.org/10.1145/2317956.2318021\n[59] Andrés Lucero and JBOS Martens. 2005. Mood Boards: Industrial designers’\nperception of using mixed reality. In Proc. SIGCHI. NL Conference. 13–16.\n[60] Atefeh Mahdavi Goloujeh, Anne Sullivan, and Brian Magerko. 2024. Is It AI or\nIs It Me? Understanding Users’ Prompt Journey with Text-to-Image Generative\nAI Tools. In Proceedings of the CHI Conference on Human Factors in Computing\nSystems. 1–13.\n[61] Damien Masson, Sylvain Malacria, Géry Casiez, and Daniel Vogel. 2024. Di-\nrectgpt: A direct manipulation interface to interact with large language models.\nIn Proceedings of the CHI Conference on Human Factors in Computing Systems.\n1–16.\n[62] MidJourney. 2024. https://www.midjourney.com/home Accessed: November\n22, 2024.\n[63] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi,\nand Ying Shan. 2024. T2i-adapter: Learning adapters to dig out more controllable\nability for text-to-image diffusion models. In Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 38. 4296–4304.\n[64] Gregory Murphy. 2004. The big book of concepts. MIT press.\n[65] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam,\nPamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: To-\nwards Photorealistic Image Generation and Editing with Text-Guided Diffusion\nModels. In International Conference on Machine Learning. PMLR, 16784–16804.\n[66] François Osiurak, Christophe Jarry, Philippe Allain, Ghislaine Aubin, Frédérique\nEtcharry-Bouyx, Isabelle Richard, Isabelle Bernard, and Didier Le Gall. 2009.\nUnusual use of objects after unilateral brain damage. The technical reasoning\n\n\nBrickify\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nmodel. Cortex 45, 6 (2009), 769–783.\n[67] Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka,\nand Christian Theobalt. 2023. Drag your gan: Interactive point-based manipu-\nlation on the generative image manifold. In ACM SIGGRAPH 2023 Conference\nProceedings. 1–11.\n[68] Xiaohan Peng, Janin Koch, and Wendy E Mackay. 2024. DesignPrompt: Using\nMultimodal Interaction for Design Exploration with Generative AI. In Proceed-\nings of the 2024 ACM Designing Interactive Systems Conference. 804–818.\n[69] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n2022. Hierarchical text-conditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125 1, 2 (2022), 3.\n[70] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad-\nford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation.\nIn International conference on machine learning. Pmlr, 8821–8831.\n[71] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition.\n10684–10695.\n[72] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\nKfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models\nfor subject-driven generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 22500–22510.\n[73] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L\nDenton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,\nTim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with\ndeep language understanding. Advances in neural information processing systems\n35 (2022), 36479–36494.\n[74] Vishnu Sarukkai, Lu Yuan, Mia Tang, Maneesh Agrawala, and Kayvon Fatahalian.\n2024. Block and Detail: Scaffolding Sketch-to-Image Generation. In Proceedings\nof the 37th Annual ACM Symposium on User Interface Software and Technology\n(Pittsburgh, PA, USA) (UIST ’24). Association for Computing Machinery, New\nYork, NY, USA, Article 33, 13 pages. https://doi.org/10.1145/3654777.3676444\n[75] Arvind Satyanarayan and Graham M. Jones. 2024. Intelligence as Agency:\nEvaluating the Capacity of Generative AI to Empower or Constrain Human\nAction.\nAn MIT Exploration of Generative AI (mar 27 2024).\nhttps://mit-\ngenai.pubpub.org/pub/94y6e0f8.\n[76] Xinyu Shi, Mingyu Liu, Ziqi Zhou, Ali Neshati, Ryan Rossi, and Jian Zhao.\n2024. Exploring interactive color palettes for abstraction-driven exploratory\nimage colorization. In Proceedings of the CHI Conference on Human Factors in\nComputing Systems. 1–16.\n[77] Xinyu Shi, Yinghou Wang, Yun Wang, and Jian Zhao. 2024. Piet: Facilitating\nColor Authoring for Motion Graphics Video. In Proceedings of the CHI Conference\non Human Factors in Computing Systems. 1–17.\n[78] Yang Shi, Tian Gao, Xiaohan Jiao, and Nan Cao. 2023. Understanding design\ncollaboration between designers and artificial intelligence: a systematic litera-\nture review. Proceedings of the ACM on Human-Computer Interaction 7, CSCW2\n(2023), 1–35.\n[79] Ben Shneiderman. 1983. Direct manipulation: A step beyond programming\nlanguages. Computer 16, 08 (1983), 57–69.\n[80] Auste Simkute, Lev Tankelevitch, Viktor Kewenig, Ava Elizabeth Scott, Abigail\nSellen, and Sean Rintel. 2024. Ironies of Generative AI: Understanding and\nMitigating Productivity Loss in Human-AI Interaction. International Journal of\nHuman–Computer Interaction (2024), 1–22.\n[81] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen\nChang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. 2023. StyleDrop:\nText-to-Image Generation in Any Style. In 37th Conference on Neural Infor-\nmation Processing Systems (NeurIPS). Neural Information Processing Systems\nFoundation.\n[82] Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradi-\nents of the data distribution. Advances in neural information processing systems\n32 (2019).\n[83] Hari Subramonyam, Roy Pea, Christopher Pondoc, Maneesh Agrawala, and\nColleen Seifert. 2024. Bridging the Gulf of Envisioning: Cognitive Challenges\nin Prompt Based Interactions with LLMs. In Proceedings of the CHI Conference\non Human Factors in Computing Systems. 1–19.\n[84] Kaiber Superstudio. 2024. https://www.kaiber.ai/product/ Accessed: November\n22, 2024.\n[85] Lev Tankelevitch, Viktor Kewenig, Auste Simkute, Ava Elizabeth Scott, Advait\nSarkar, Abigail Sellen, and Sean Rintel. 2024. The metacognitive demands and\nopportunities of generative AI. In Proceedings of the CHI Conference on Human\nFactors in Computing Systems. 1–24.\n[86] Anne Tomes, Caroline Oates, and Peter Armstrong. 1998.\nTalking design:\nnegotiating the verbal–visual translation. Design Studies 19, 2 (1998), 127–142.\n[87] Katja Tschimmel. 2012. Design Thinking as an effective Toolkit for Innovation.\nIn ISPIM Conference Proceedings. The International Society for Professional\nInnovation Management (ISPIM), 1.\n[88] Tiffany Tseng, Ruijia Cheng, and Jeffrey Nichols. 2024. Keyframer: Empowering\nAnimation Design using Large Language Models. https://arxiv.org/abs/2402.\n06071\n[89] Priyan Vaithilingam, Elena L Glassman, Jeevana Priya Inala, and Chenglong\nWang. 2024. DynaVis: Dynamically Synthesized UI Widgets for Visualization\nEditing. In Proceedings of the CHI Conference on Human Factors in Computing\nSystems. 1–17.\n[90] Veera Vimpari, Annakaisa Kultima, Perttu Hämäläinen, and Christian Guckels-\nberger. 2023. “An Adapt-or-Die Type of Situation”: Perception, Adoption, and\nUse of Text-to-Image-Generation AI by Game Industry Professionals. Proceed-\nings of the ACM on Human-Computer Interaction 7, CHI PLAY (2023), 131–164.\n[91] Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. 2023. Concept\ndecomposition for visual exploration and inspiration. ACM Transactions on\nGraphics (TOG) 42, 6 (2023), 1–13.\n[92] Qian Wan and Zhicong Lu. 2023. GANCollage: A GAN-Driven Digital Mood\nBoard to Facilitate Ideation in Creativity Support. In Proceedings of the 2023\nACM Designing Interactive Systems Conference. 136–146.\n[93] Sitong Wang, Savvas Petridis, Taeahn Kwon, Xiaojuan Ma, and Lydia B Chilton.\n2023. PopBlends: Strategies for conceptual blending with large language models.\nIn Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems.\n1–19.\n[94] Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, and Tianyi Zhang. 2024.\nPromptCharm: Text-to-Image Generation through Multi-modal Prompting and\nRefinement. In Proceedings of the CHI Conference on Human Factors in Computing\nSystems. 1–21.\n[95] Anthony Ward. 1984. Design cosmologies and brain research. Design Studies 5,\n4 (1984), 229–238.\n[96] Colin Ware. 2010. Visual thinking for design. Elsevier.\n[97] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng\nZuo. 2023. Elite: Encoding visual concepts into textual embeddings for cus-\ntomized text-to-image generation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 15943–15953.\n[98] Justin D Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel Miles, and\nWerner Geyer. 2024. Design Principles for Generative AI Applications. In\nProceedings of the CHI Conference on Human Factors in Computing Systems.\n1–22.\n[99] Jacob O Wobbrock, Leah Findlater, Darren Gergle, and James J Higgins. 2011.\nThe aligned rank transform for nonparametric factorial analyses using only\nanova procedures. In Proceedings of the SIGCHI conference on human factors in\ncomputing systems. 143–146.\n[100] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transpar-\nent and controllable human-ai interaction by chaining large language model\nprompts. In Proceedings of the 2022 CHI conference on human factors in computing\nsystems. 1–22.\n[101] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng\nZheng, and Mike Zheng Shou. 2023. Boxdiff: Text-to-image synthesis with\ntraining-free box-constrained diffusion. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision. 7452–7461.\n[102] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional\ncontrol to text-to-image diffusion models. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision. 3836–3847.\n[103] Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe\nYu, and Alexei A Efros. 2017. Real-time user-guided image colorization with\nlearned deep priors. ACM Transactions on Graphics (TOG) 36, 4 (2017), 1–11.\n\n\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan\nXinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao\nTable 1: The table records participants’ demographic information, including gender, age, occupation, and experiences of graphic\ndesign in years (Design Exp.), self-rated frequency of Generative AI usage (GenAI Freq.), and GenAI tools they frequently use.\nID\nGender\nAge\nOccupation\nDesign Exp.\nGenAI Freq.\nGenAI Tools\nP1\nF\n25\nDesign New Grad\n5\n3\nMidjourney\nP2\nM\n27\nDesign Researcher\n5\n5\nDall-E\nP3\nF\nNot Reveal\nDesigner\n4\n3\nMidjourney\nP4\nM\n28\nVisual Designer\n5\n3\nMidjourney\nP5\nF\n25\n3D Artist\n5\n5\nDall-E, Midjourney\nP6\nF\n26\nExhibition Designer\n4\n3\nMidJourney\nP7\nM\n27\nGraphic Designer\n5\n2\nMidJourney\nP8\nF\n26\nUX Designer\n5\n4\nMidJourney\nP9\nM\n25\nTechnical Aritist\n3\n3\nMidJourney\nP10\nM\n25\nArchitect\n3\n2\nDall-E\nP11\nM\n25\nGraphic Designer\n5\n4\nMidJourney\nP12\nM\n30\nVisual Designer\n5\n3\nMidJourney\nTable 2: Rating rubric for the quality of participants’ expressions in terms of the element coverage, clarity of size, position,\nstyle, and color. Raters were rated on a 7-point Likert Scale where 1 means very low and 7 means very high.\nItem\nScore\nCriteria\nElement Coverage\n7\nAll key elements from the target image are included and accurately represented.\n4\nThree elements are missing.\n1\nSix or more elements are missing.\nClarity of Size\n7\nThe relative size of all elements is clearly and accurately described.\n4\nThe relative size of around half elements is described, but some ambiguity exists.\n1\nThe size of elements is highly unclear or not described.\nClarity of Position\n7\nThe position of all elements is clearly and accurately described relative to each other.\n4\nThe position of around half elements is described, but some spatial relationships\nare ambiguous.\n1\nThe position of elements is highly unclear or not described.\nClarity of Style\n7\nThe global style is clearly and accurately described.\n1) Referred to the style of the first and/or the second reference images; or\n2) style descriptors such as “minimal/simplicity/geometric/abstract” or similar ones.\n4\nThe global style is described, but it is not apparent how it relates to the target image.\n1\nThe style of elements is unclear or not described.\nClarity of Color\n7\nThe color of all elements is clearly and accurately described.\n4\nThe color of about half elements is described and close to the target colors,\nbut the rest are ambiguous or missing.\n1\nThe color of elements mostly described far away from the target image or not described.\nA\nAPPENDIX\nA.1\nImplementation Details\nThe front-end user interface of Brickify was built using React.js\nas the primary framework. Most of the design token management\nfunctionalities, such as the creation, deletion, and manipulation,\nwere implemented using D3.js. The rest of the interface components,\nsuch as the buttons and icons, were taken from the Material UI\nlibrary and customized to fit the needs of the application. The server-\nside rendering for API calls is handled by fastAPI. The back-end\nmodel fine-tuning and inferences for visual lexicon execution are\nwritten in Python and performed on an 80G A100 GPU.\nA.2\nParticipants’ Demographic Information\nTable 1 describes the detailed demographic information of partici-\npants in our user study.\nA.3\nRating Rubric for Expressions in Study 1\nTable 2 lists the detailed rating rubric for the external scorers in\nStudy 1 to rate participants’ expressed intention.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21219v1.pdf",
    "total_pages": 20,
    "title": "Brickify: Enabling Expressive Design Intent Specification through Direct Manipulation on Design Tokens",
    "authors": [
      "Xinyu Shi",
      "Yinghou Wang",
      "Ryan Rossi",
      "Jian Zhao"
    ],
    "abstract": "Expressing design intent using natural language prompts requires designers to\nverbalize the ambiguous visual details concisely, which can be challenging or\neven impossible. To address this, we introduce Brickify, a visual-centric\ninteraction paradigm -- expressing design intent through direct manipulation on\ndesign tokens. Brickify extracts visual elements (e.g., subject, style, and\ncolor) from reference images and converts them into interactive and reusable\ndesign tokens that can be directly manipulated (e.g., resize, group, link,\netc.) to form the visual lexicon. The lexicon reflects users' intent for both\nwhat visual elements are desired and how to construct them into a whole. We\ndeveloped Brickify to demonstrate how AI models can interpret and execute the\nvisual lexicon through an end-to-end pipeline. In a user study, experienced\ndesigners found Brickify more efficient and intuitive than text-based prompts,\nallowing them to describe visual details, explore alternatives, and refine\ncomplex designs with greater ease and control.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}