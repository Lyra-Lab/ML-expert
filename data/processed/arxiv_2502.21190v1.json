{
  "id": "arxiv_2502.21190v1",
  "text": "Geodesic Slice Sampler for Multimodal Distributions with Strong Curvature\nBernardo Williams †,1\nHanlin Yu1\nHoang Phuc Hau Luu1\nGeorgios Arvanitidis2\nArto Klami1\n1Department of Coputer Science, University of Helsinki, Finland\n2Cognitive Systems, DTU Compute, Technical University of Denmark\n†bernardo.williamsmoreno@helsinki.fi\nAbstract\nTraditional Markov Chain Monte Carlo sampling\nmethods often struggle with sharp curvatures, in-\ntricate geometries, and multimodal distributions.\nSlice sampling can resolve local exploration inef-\nficiency issues and Riemannian geometries help\nwith sharp curvatures. Recent extensions enable\nslice sampling on Riemannian manifolds, but they\nare restricted to cases where geodesics are avail-\nable in closed form. We propose a method that\ngeneralizes Hit-and-Run slice sampling to more\ngeneral geometries tailored to the target distribu-\ntion, by approximating geodesics as solutions to\ndifferential equations. Our approach enables explo-\nration of regions with strong curvature and rapid\ntransitions between modes in multimodal distribu-\ntions. We demonstrate the advantages of the ap-\nproach over challenging sampling problems.\n1\nINTRODUCTION\nSampling from a differentiable unnormalized log-density\ndefined on a Euclidean space is a core problem in machine\nlearning and statistics. While gradient-based Markov Chain\nMonte Carlo (MCMC) methods have proven effective in\nmany scenarios, they often face significant challenges when\nthe target distribution exhibits complex geometry (sharp\ncurvature) or multimodal behavior. The two core challenges\nare largely addressed with complementary techniques, with\nlittle work in algorithms that excel for targets that are both\nmultimodal and complex in shape.\nComplex shapes and sharp curvatures are often addressed\nby using a suitably chosen Riemannian geometry within\nthe sampling algorithms [Girolami and Calderhead, 2011].\nInstead of operating in a Euclidean space and metric, the\nsamplers carry out the necessary operations using a met-\nric that adapts to the curvature of the parameter space. In\npractice, the methods follow flows induced by the metric,\nin most cases by numerical integration, and consequently\nthe methods are sometimes called geodesic methods as in\nour title. Various practical metrics and sampling algorithms\nhave been shown to improve sampling of targets with strong\ncurvature [Girolami and Calderhead, 2011, Byrne and Giro-\nlami, 2013, Lan et al., 2015, Hartmann et al., 2022, Yu et al.,\n2024, Williams et al., 2024], albeit always with increased\ncomputational cost.\nMultimodality, in turn, is most commonly addressed by tem-\npering or diffusion techniques [Earl and Deem, 2005, Chen\net al., 2024]. These methods use tempered – smoothed –\nversion of the target to improve exploration over multiple\nmodes, intuitively changing the problem itself so that the\nmodes are connected with areas of sufficient probability.\nAt high degree of tempering these methods can efficiently\nexplore the different modes, but low tempering is needed for\naccurate sampling within the modes, necessitating adaptive\nor parallel sampling with different degrees of tempering.\nThe efficiency of parallel tempering depends on the swap\nacceptance rate between adjacent temperatures, which can\ndecrease in high dimensions if the temperature schedule\nis not well-tuned [Woodard et al., 2009]. Diffusion-based\napproaches, in turn, require careful choice of the noise sched-\nule to balance exploration and accuracy [Song and Ermon,\n2019, Chen et al., 2024]. Unlike tempering, diffusion meth-\nods can achieve smooth transitions between modes without\nexplicitly maintaining a set of parallel chains, but the accep-\ntance rate of noisy samples can be low [Chen et al., 2024].\nEven though the two approaches are efficient in addressing\nthe two challenges separately, there is very little work on\nsamplers designed for the general setup where both diffi-\nculties may arise simultaneously. One could consider e.g.\nparallel tempering in a Riemannian metric – see Byrne and\nGirolami [2013] for a rare example in this intersection – but\nideally we would like to address both aspects using a com-\nmon mechanism. This work explores one such approach,\ndeveloping a Riemannian sampler capable of efficiently ex-\nploring multiple modalities, without any tempering for the\narXiv:2502.21190v1  [cs.LG]  28 Feb 2025\n\n\ntarget distribution. Instead, we seek to improve mode ex-\nploration by change of the metric, in the spirit of the early\nwork by Lan et al. [2014] that developed a specific metric\nsolely for this purpose. Their metric, however, requires ex-\nplicit identification and tracking of the modes and is more\nlike a conceptual demonstration, and we are not aware of\nany other works aiming for efficient multi-modal samplers\nsolely by the change of the metric.\nOur starting point is the idealized slice sampler, with com-\nputable level sets. The performance of such an idealized\nsampler is theoretically independent of the sampling prob-\nlem, as pointed out by Durmus et al. [2023]: “This means\nthat the performance of the idealized slice sampler is igno-\nrant of the introduction of, e.g., multimodality, local modes,\nor anisotropy as long as the volume of the level sets is\nnot modified.” From a practical perspective, we build on\nthe (Euclidean) Hit-and-Run slice sampler by Bélisle et al.\n[1993], which at each iteration selects a random direction\nand then samples from the resulting one-dimensional distri-\nbution formed by the intersection of the line and the slice.\nIn effect, it transforms multi-dimensional sampling into se-\nquential one-dimensional sampling tasks, but the overall\nsampler may be inefficient. Especially in higher dimensions\nthe intersection with the slice can be small for almost all\ndirections Murray et al. [2010].\nBoth Habeck et al. [2023] and Durmus et al. [2023] recently\nconsidered generalizations of the Hit-and-Run sampler for\nRiemannian manifolds, replacing the lines with geodesics.\nWe build on the general algorithmic framework introduced\nby Durmus et al. [2023] and adapt it to the task of sam-\npling from a distribution with a complex geometry. Specif-\nically, we begin by embedding the (Euclidean) sampling\nspace into a higher-dimensional space that incorporates the\ntarget distribution’s geometric information, such as Fisher\ninformation or Monge embedding [Hartmann et al., 2022].\nThis transforms the problem into sampling from a particular\nRiemannian manifold where the target distribution corre-\nsponds to the Hausdorff density (see Section 3). Note that\neven though we leverage components proposed by Durmus\net al. [2023], our task is fundamentally more difficult. Their\nstarting point was sampling of a density on a known mani-\nfold (e.g. a sphere) where the geodesics are known exactly,\nwhereas the complexity of our embedding manifold requires\nus to approximate the geodesics using numerical integrators.\nIn this work, we propose a geodesic slice sampler applicable\nfor arbitrary Riemannian metrics, and discuss the choice of\nthe metric. In particular, we introduce two new computa-\ntionally efficient metrics that both improve sampling over\nmultimodal targets by, in a sense, pulling the modes closer\nto each other; see Figure 1 illustrating this effect within the\nslice sampler, as a function of a parameter λ controlling how\nmuch the metric warps the space. In addition, we introduce\na meta-sampler similar to Tjelmeland and Hegstad [2001]\nthat combines the proposed method with separate sampler\nFigure 1: Illustration of the step-out procedure in Metric-\nagnostic Geodesic Slice Sampler. The lines drawn with\ndifferent colors represent the Hausdorff density p(t) :=\npH(ˆγ(x,v)(t)) (Eq. (1)) considering the inverse Generative\nmetric for different values of λ (Eq. (4)). The step-out pro-\ncedure chooses s ∼Unif(0, p(0)) and sets randomly an\ninterval of length r −ℓat t = 0 with left and right points\nℓand r. While p(r) > p(s) it expands the right side of the\ninterval as r = r + w and for the left side while p(l) > p(s)\nit does ℓ= ℓ−w. This expands the length of the initial\ninterval. As λ →0 the space shrinks due the properties of\nthe metric, making it easier for the step-out procedure to\njump to the distant mode.\nfor improved exploration of local modes.\nWe empirically demonstrate improved sampling over Eu-\nclidean methods for complex targets, and highlight im-\nproved mixing over multiple modes in high dimensional-\ncases when compared against parallel tempering [Swendsen\nand Wang, 1986, Łatuszy´nski et al., 2025] and diffusive\nGibbs sampler by Chen et al. [2024] designed for address-\ning multimodality. Similar to previous Riemannian methods,\nthe algorithm shows good exploration and mixing but has\nslower iterations, because of computation of the geodesics.\n2\nBACKGROUND: SLICE SAMPLING\nThe classic work of Neal [2003] introduces Slice Sampling\nas a method for generating samples by uniformly sampling\nfrom the RD+1 manifold defined by the graph of the prob-\nability density. Let p(x) be an unnormalized continuous\ntarget density that satisfies\nR\np(x) dx < ∞. Suppose that\ndirect sampling from p(x) is not feasible. We consider den-\nsities where x ∈RD with respect to the Lebesgue measure.\nIdealized slice sampling defines a uniform distribution over\nthe volume under the graph of p(x) and generates samples\n\n\nthrough the following two steps:\n1. Sample s ∼Unif(0, p(x)).\n2. Sample x ∼Unif(L(s)).\nwhere the slice is given by L(s) := {x | p(x) > s}. For\nspecial cases, such as log-concave or rotationally invari-\nant densities, the slice sampler has theoretical performance\nguarantees [Natarovskii et al., 2021]. However, for more\ncomplex distributions, drawing uniform samples from L(s)\nis often impractical [Rudolf and Ullrich, 2018].\nTo address this, the step-out and shrinkage procedures are\nused. Below, we provide an informal explanation of these\nprocedures. The full algorithm is detailed in the appendix\n(Algorithms 3 and 4). Both procedures were first introduced\nby Neal [2003], but we adopt a modified version of the\nshrinkage step as proposed by Durmus et al. [2023]. Both\nversions are equally valid. For a moment, assume a uni-\nvariate density p(x) and a current position x ∈R. The\nprocedures are as follows:\nThe Step-Out Procedure\nThe step-out procedure, illus-\ntrated in Figure 1, takes two parameters: the width w ∈R\nand maximum steps m ∈N. Given the slice L(s), the goal\nis to expand an interval around the current point x. Consider\nthe auxiliary function γx(t) = x + t.\nThe initial left ℓand right r points are set at a random\ndistance w apart. This is done by sampling u ∼Unif(0, w)\nand setting ℓ= −u and r = ℓ+ w. To ensure that at\nmost m + 1 expansion steps are performed (combined for\nboth directions), a random integer ι ∼Unif({1, . . . , m}) is\nsampled. The right limit is expanded up to ι times, and the\nleft limit up to m + 1 −ι times.\nThe expansion proceeds as follows: The right limit r is\nexpanded by adding w until p(γx(r + w)) < s, meaning\nγx(r + w) /∈L(s). The left limit ℓis expanded by subtract-\ning w until p(γx(ℓ−w)) < s, meaning γx(ℓ−w) /∈L(s).\nThe procedure returns the updated interval (ℓ, r). We denote\nit by Step-outw,m(s, γx).\nThe Shrinkage Procedure\nThe shrinkage procedure se-\nlects a sample from the interval (ℓ, r) by gradually reducing\nits size until a point is found within L(s) ∩(ℓ, r).\nThe interval J = (ℓ, r) is treated as a circular domain,\nmeaning that if we move past r, we continue from ℓ. The\nprocedure starts by sampling two points y and z uniformly\nwithin (ℓ, r). If neither γx(y) or γx(z) fall inside L(s), the\ninterval is shrunk as follows:\n• Form the interval (y ∧z, y ∨z). Update the circular\nregion by\nJ =\n(\nJ ∩(y ∧z, y ∨z),\nif 0 ∈J,\nJ \\ (y ∧z, y ∨z),\nif 0 /∈J.\nFigure 2: The Hit-and-Run procedure on the Funnel distri-\nbution with base point (⋆). The green circle is SD−1, and\nthe red arrow is a norm one velocity. On the left the space is\nEuclidean and the straight line scapes the distribution while\nthe right for the Monge metric α = 1 the geodesic follows\ncurves towards the narrow area.\n• Set y = z and update z ∼Unif(J).\n• This process repeats, each time reducing the size of the\ninterval, until γx(z) ∈L(s).\nWe denote the procedure Shrinkl,r(s, γx). One complete\nstep of the slice sampler is:\n1. Sample s ∼Unif(0, p(x))\n2. Obtain ℓ, r = Step-outw,m(s, γx)\n3. Sample t∗= Shrinkℓ,r(s, γx).\n4. Set x = γx(t∗).\nHit-and-Run\nOne way to extend slice sampling to mul-\ntivariate distributions is to combine it with Hit-and-Run\nsampling, here presented following Bélisle et al. [1993].\nLet SD−1(x) = {v ∈RD : ∥v∥2 = 1}, and v ∼\nUnif(SD−1(x)). An iteration of the whole sampler is:\n• Obtain v ∼Unif(SD−1(x)).\n• Obtain a sample from the density evaluated along the\ngeodesic t 7→p(x +t v)/\nR\np(x +t v)dt.\nFigure 3 illustrates a step of the Hit-and-Run When directly\nsampling a value t from the density along geodesics t 7→\np(x +t v)/\nR\np(x +t v)dt is not feasible we can use slice\nsampling on t 7→p(x +t v), since it is an unnormalized\nunivariate distribution. Define γ(x,v)(t) = x +t v. The step-\nout procedure outputs ℓ, r = Step-outw,m(s, γ(x,v)) and the\nshrinkage procedure will return t∗= Shrinkℓ,r(s, γ(x,v)).\nThe new sample is x = γ(x,v)(t∗). This is called Hit-and-\nRun slice sampling or hybrid Slice sampling [Łatuszy´nski\nand Rudolf, 2014]. This method extends slice sampling to\nprobability distributions defined over RD.\n3\nMETHOD\nOur main contribution is a geodesic slice sampler that can\naccommodate arbitrary metrics. It extends the Hit-and-Run\nslice sampler described above for non-Euclidean geome-\ntries, similar to the recent works of Durmus et al. [2023] and\n\n\nHabeck et al. [2023], but instead of leveraging closed-form\nanalytic geodesics of predefined manifolds we induce met-\nrics using characteristics of the target density itself to guide\nthe sampling. Now the geodesics need to be approximated\nby numerical integrators. This section explains the sampler\nand a meta-sampler that combines the core method with\nseparate local sampler for improved efficiency for general\nmetrics, always using G(x) to denote the metric tensor. We\nwill discuss specific metrics in Section 4.\nStraight-line Hit-and-Run sampling can be inefficient be-\ncause proposals often move away from high-probability re-\ngions [Murray et al., 2010]. To resolve this, we perform slice\nsampling along geodesic curves that can accommodate the\ngeometry of the target distribution. This improves efficiency\nwhen the target distribution is highly curved, or multimodal\ndistributions; see Section 4.2. We are interested in sampling\nproblems defined in Rd but allow using different plug-in\nmetrics (preferably using the target density information) to\nenhance exploration. This can be cast as sampling from a\ndistribution defined on a Riemannian manifold where the\nalgorithm in Durmus et al. [2023] can be employed. Alter-\nnatively, it can be seen as Hit-and-Run where straight lines\nare replaced by curves that better wrap around the level sets\nof the target density (given the metrics are good enough).\nHowever, since the metric is quite general, geodesics can\nnot be obtained in closed-forms and we need to use numer-\nical integrators. To correctly sample along geodesics, we\nneed three key components: Adjusting for the correct den-\nsity on the manifold, properly sampling directions using the\nRiemannian metric, and solving the geodesic equations.\nHausdorff Density:\nTo ensure we sample from the correct\ndistribution on the manifold with metric G(x), we must\naccount for the change in measure from the Euclidean space\nto the manifold. The correct density is the Hausdorff density\npH(x) =\np(x)\np\ndet G(x)\n.\n(1)\nThe denominator adjusts for local volume distortion intro-\nduced by the metric G(x), ensuring that the volume over\nthe manifold is preserved and hence maintaining proper\nsampling behavior. See Appendix A.5 for further details.\nSampling from the Riemannian Unit Ball:\nInstead of\nsampling a random direction in Euclidean space, we must\nnow sample from the unit geodesic ball under the Rieman-\nnian metric, where we can directly use the method proposed\nby Durmus et al. [2023]. Given a position x, a velocity\nv is sampled as follows: First draw v ∼N(0, G−1(x)),\nand then normalize it to obtain a unit-length vector in the\nRiemannian metric with:\nv ←\nv\n∥v ∥g\n,\nwhere\n∥v ∥g =\nq\nv⊤G(x) v.\nThis ensures that the direction is uniformly distributed on\nthe unit sphere under the metric G(x). See Appendix A for\nadditional implementation details.\nApproximating Geodesic Curves\nGiven a sampled ve-\nlocity v, we need to follow the geodesic curve starting at x\nin direction v. In general, the geodesic equation\n˙xk = vk,\n˙vk = −∥v∥2\nΓk,\nfor k = 1, . . . , D.\n(2)\nwhere Γk\nij = 1\n2gkm(∂igmj +∂jgim−∂mgij), does not have\na closed-form solution for arbitrary G(x). See more detail\nin appendix A.3. Instead, we numerically approximate the\nexponential map γ(x,v)(t) by solving these differential equa-\ntions with an ordinary differential equation (ODE) solver,\ndenoted as ˆγ(x,v)(t). The choice of the metric determines\nthe shape of geodesic trajectories, allowing the sampler to\nadapt to different target distributions; see Section 4.\nAlgorithm 1 explains the full Metric-agnostic Geodesic\nSlice Sampler (MAGSS). After sampling a velocity v from\nthe unit Riemannian sphere, slice sampling is performed on\nthe Hausdorff density evaluated along the numerical solu-\ntion of the geodesic trajectory. The step-out and shrinkage\nprocedures then determine the final sample.\nAlgorithm 1 Metric-agnostic Geodesic Slice Sampler\nInput: Initial position x[0], metric tensor G(x), and param-\neters m ∈N, w ≥0.\nOutput: N samples x[n].\n1: for n ←0, . . . , N −1 do\n2:\nSample s ∼Unif(0, pH(x[n]))\n3:\nSample velocity v[n] ∼Unif(SD−1\ng\n(x[n]))\n4:\nCompute (ℓ, r) = Step-outw,m(s, ˆγ(x[n],v[n]))\n5:\nSample time t∗= Shrinkℓ,r(s, ˆγ(x[n],v[n]))\n6:\nx[n+1] = ˆγ(x[n],v[n])(t∗)\n7: end for\n3.1\nMETA SAMPLER AND MULTIMODALITY\nThe sampler as described above is valid as such, but we also\nintroduce a simple extension that can further improve sam-\npling for multi-modal targets with complex local structure.\nFollowing Tjelmeland and Hegstad [2001], Łatuszy´nski et al.\n[2025], we create a meta-sampler that alternates between us-\ning MAGSS for global moves and an arbitrary local MCMC\nfor sampling within each mode. To generate one sample,\nwe first run K-steps of MAGSS followed by L-steps of any\nlocal MCMC sampler. We refer to this combined strategy\nas Meta-MAGSS, detailed in Algorithm 2 (in Appendix).\nThe main motivation for this hybrid strategy is to leverage\ngradient-based algorithms for fast exploration of the mode,\nto utilize their efficient mixing and fast per-iteration com-\nputation when they are sufficiently good for the local target.\nWe could in principle use any sampler for the local part,\n\n\nincluding Riemannian samplers, but we in practice use stan-\ndard Euclidean Metropolis-adjusted Langevin Algorithms\n(MALA) [Roberts and Tweedie, 1996] in our experiments.\n4\nMETRICS\nThe sampler is general, applicable for arbitrary metric and\nonly requiring G(x) to be positive definite and vary contin-\nuously. By selecting an appropriate metric we can influence\nhow the geodesics explore the space, controlling the overall\nsampling behavior. There is no single metric that is optimal\nfor all targets, and the metrics proposed in the literature are\nmotivated by complementary argumentation, with notable\nemphasis in computational efficiency.\nNext we discuss the metric choice. The literature has exclu-\nsively focused on metrics that improve local exploration for\ncomplex target distributions, with several practical solutions\nthat we re-cap in Section 4.1. We then turn our attention on\nhow to improve exploration of multiple modes, presenting\nnovel metrics specifically designed for this in Section 4.2.\n4.1\nFOR ADAPTING TO LOCAL CURVATURE\nThe Fisher metric\nThe Fisher Information Metric (FIM)\nis defined as the covariance of the score function, and was\npredominantly used in the early Riemannian methods [Giro-\nlami and Calderhead, 2011] due to its close connection to\nestimation theory. A general form of the metric is\nGF (x) = Ey | x\n\u0002\n∇x log p(y | x)∇x log p(y | x)⊤\u0003\n,\nbut the specific form depends on the underlying problem,\ndue to integration over the conditional density. Furthermore,\nit requires direct matrix inversion for computing G−1\nF (x)\nthat is required during geodesic computations (Eq. 2), with\ncomplexity of O(D3). This makes the metric impractical\nand inefficient for high-dimensional problems.\nThe Monge Metric\nThe computational cost of solving the\ngeodesic equations (Eq. 2) is primarily determined by the\ninversion of the metric tensor, and consequently metrics with\nclosed-form inverse offer significant savings. The Monge\nmetric by Hartmann et al. [2022] naturally arises from the\ngeometry of the graph of log-density function when viewed\nas a submanifold embedded in RD+1. Let α2 ≥0 and\nλ ≥0. The Monge metric and its inverse are given by\nGM(x) = ID +α2∇ℓ∇ℓ⊤,\nG−1\nM (x) = ID −\nα2\n1 + α2∥∇ℓ∥2 ∇ℓ∇ℓ⊤.\n(3)\nAs α2 →0, the metric reduces to the Euclidean metric ID.\nThe determinant required for computing the Hausdorff den-\nsity (Equation (1)) is det GM(x) = 1+α2∥∇ℓ∥2. Figure 3\nFigure 3: Exponential map for Riemannian balls of increas-\ning radius on the Funnel distribution for the Monge metric\nwith α = 1 on the left panel. On the right, the plot is anal-\nogous but considering the Generative metric with λ = 0.1\nand p0 = 0.1. Each color represents a bigger radius from\nthe base point (⋆). Both metrics achieve the desired goal,\nshortening the distances to the points along the narrow fun-\nnel that would be difficult to reach in a Euclidean geometry.\nillustrates the exponential map of geodesic balls with in-\ncreasing radius under the Monge metric. This metric adapts\nto the geometry of the target distribution, expanding regions\nbased on the local structure of the density.\nThe Generative Metric\nAnother efficient metric, the Gen-\nerative metric that is proportional to the target density func-\ntion, was recently proposed by [Kim et al., 2024]. One of\nits advantages is that computing the Christoffel symbols Γk\nij\nonly requires first-order derivatives of the density, whereas\nthe Monge metric (Equation 3) introduces second-order\nterms. For p0 > 0 and λ ≥0, the Generative metric and its\ninverse are:\nGg(x) =\n\u0012p(x) + λ\np0 + λ\n\u00132\nID,\n(4)\nG−1\ng (x) =\n\u0012 p0 + λ\np(x) + λ\n\u00132\nID.\n(5)\nAs λ →∞, the metric reduces to the Euclidean metric.\nThe determinant is det Gg(x) =\n\u0010\np(x)+λ\np0+λ\n\u00112D\n. Figure 1\nillustrates the effect of λ on the Hausdorff density along\ngeodesics t 7→pH\n\u0000ˆγ(x,v)(t)\n\u0001\n, and Figure 3 again shows\nhow the metric transforms the space.\n4.2\nFOR BRIDGING THE MODES\nThe above metrics adapt for the local curvature and have\nbeen designed to improve sampling of, for instance, narrow\nfunnels by re-defining the proximity (see Figure 3). For\nassisting exploration of multimodal targets we need differ-\nent kinds of metrics: Now we would want a metric that\nmakes modes that are far away in the original Euclidean\nsense appear closer (see Figure 1). With the exception of\nthe construction of Lan et al. [2014], which we will dis-\ncuss more closely in Section 6, we are not aware of any\nprevious metrics designed for this. Next, we introduce two\n\n\ncomputationally efficient metrics, with fast inverses and\ndeterminants, for assisting multi-modal sampling.\nWe start by reminding that any matrix G(x) defines a valid\nRiemannian metric as long as it is positive definite for every\nx ∈M and varies continuously on M. Since the inverse of\na positive definite matrix is also positive definite, we observe\nthat it is possible to use any of the previously formulated\nG−1(x) for defining a metric. Concretely, we propose:\nThe Inverse Monge Metric\nWe use\nGIM(x) = ID −\nα2\n1 + α2∥∇ℓ∥2 ∇ℓ∇ℓ⊤.\nas the metric tensor, with the inverse G−1\nIM(x) = GM(x)\ngiven by the previously introduced metric tensor of the stan-\ndard Monge metric (Eq. (3)). The determinant of this metric\nis 1/ det(GM), and hence it retains the computational effi-\nciency of the original Monge metric.\nThe Inverse Generative Metric\nSimilarly, we define\nGIg(x) =\n\u0012 p0 + λ\np(x) + λ\n\u00132\nID,\nand obtain the inverse G−1\nIg (x) = Gg(x) as the metric\ntensor of the standard Generative metric (Eq. (4)) and the\ndeterminant as 1/ det(Gg). Again, the computational effi-\nciency of the original Generative metric is retained.\nNext, we explain how the inverse Monge and inverse Gener-\native achieve the desired mode-bridging ability. Geodesic\ncurves maintain a constant velocity norm in the Riemannian\nsense by construction. Let γ(x0,v0)(t) = xt be a geodesic\ncurve with velocity ˙γ(x0,v0)(t) = vt, starting from x0 with\ninitial velocity v0. If the geodesic moves toward a low-\nprobability region where p(xt) →0, then we the “mode\nbridging” behavior occurs if the Euclidean velocity norms\nsatisfy ∥v0∥2 ≫∥vt∥2. This means that as t increases in\nlow-density regions, geodesics traverse these areas quickly,\nbringing distant modes closer. Figure 1 illustrates this ef-\nfect for the inverse Generative metric. And the following\nobservation collects this idea which we discuss further in\nAppendix A.6:\nObservation 1. Let p(x) be a smooth density function. Let\n(xt, vt) be the geodesic flow with initial conditions (x0, v0)\nwith respect to the inverse Generative/Monge metric. Then,\nfor t such that p(xt) →0, the Euclidean velocity norm\n∥vt∥2 gets small, implying the low value density regions\nare covered faster as function of time t.\n5\nEXPERIMENTS\nWe evaluate MAGSS for targets with sharp curvature (Sec-\ntion 5.1), multiple modes (Section 5.2), or both (Section 5.3),\nalways considering different choices of the metric. We also\nempirically quantify the effect of the numerical integrator 1.\nEvaluation\nWe use targets with known reference sam-\nples, which allows measuring the accuracy using the 1-\nWasserstein (earth mover’s) distance with the samples pro-\nvided by the algorithm [Flamary et al., 2021]. Besides ac-\ncuracy, we quantify the samplers with the probability of\njumping between the different modes, as the raw ratio of\nconsecutive samples that are within separate modes (defined\nmanually for each problem).\nComparison methods\nTo showcase the effect of the met-\nric we will be running MAGSS also with in Euclidean met-\nric, with G(x) = ID, and we additionally compared against\nparallel tempering and diffusive Gibbs sampling.\nParallel Tempering (PT) runs many Markov Chains in par-\nallel, each of which has p(x)1/τi as targets for different\ntemperatures τi ≥1, where for τ1 = 1 the original target\nis recovered. As τ →∞, the density flattens, facilitating\ntransitions between regions of higher densities that are far\napart from each other. The parallel chains jumps randomly\nbetween each other, thus visiting the modes more often\naccording to a Metropolis-Hastings ratio [Swendsen and\nWang, 1986, Geyer, 1991]. Our implementation of PT fol-\nlows Łatuszy´nski et al. [2025].\nDiffusive Gibbs sampling (DiGS) by Chen et al. [2024]\nis a sampler designed for addressing multi-modality. It ap-\nproaches the sampling task by using an auxiliary variable\nwith a Gibbs scheme. It uses the variance preserving (VP)\n[Song et al., 2021] noise scaling: p(˜x| x) = N(˜x|αt x, σ2\nt ),\nwhere σt =\np\n1 −α2\nt, sampled directly and p(x |˜x) ∝\np(˜x| x)p(x) sampled through a local MCMC sampler. It\nhas an additional Metropolis within Gibbs proposal scheme\nq(x |˜x) = N(x |˜x/αt, (αt/σt)2). VP has the property that\nat when αt →0 then p(˜x| x) = N(˜x|0, ID) and when\nαt →1 then p(˜x| x) = δx.\n5.1\nCOMPLEX UNIMODAL TARGETS\nWe evaluate the methods on three canonical benchmark tar-\ngets (funnel, hybrid Rosenbrock and squiggle) with strong\ncurvature, with the densities given in Appendix C.5. PT is\nleft out here and we only consider the metrics in Section 4.1\nsince the target is unimodal.\nFigure 4 shows that MAGSS with Fisher metric GF (x) is\nclearly superior, but runs out of the limited computational\nbudget already at low dimensions, and the Monge metric\nGM(x) offers notable improvement for Rosenbrock and\nsquiggle targets. DiGS remains on the level of the Euclidean\nMAGSS and the generative metric does not help either.\n1Code at: github.com/williwilliams3/magss\n\n\nFigure 4: Univariate sampling accuracy in various metrics (Wasserstein distance, lower is better) for targets of varying\ndimensionality. The medians over 5 runs are connected with a line. Left: Funnel. Middle: Rosenbrock. Right: Squiggle.\nExperiment specification:\nWe obtain 10, 000 samples us-\ning 10 chains and omit results for runs that did not complete\nin 12 hours. We set α2 = 1 for the inverse Monge metric\nsince this value has been shown to work in 2 dimensions\n[Hartmann et al., 2022]. We select λ = 1, p0 = 1 and\ninverse Generative without further tuning. We use Dopri5\nintegrator with adaptive step-size. We set w = 3 and m = 8.\nDiGS uses a single noise scale α = 1 and step-size 0.1 for\nMALA within the algorithm.\n5.2\nMULTIMODAL WITH SIMPLE MODES\nFor studying mode exploration, we use a target of two D-\ndimensional Gaussian distributions centered at −1D and\n1D with scale σ = 0.1 and weights {0.2, 0.8}. The distance\nbetween the modes (\n√\nD2) grows for higher dimensions,\nmaking transition between the modes more difficult. Now\nwe only consider the new metrics for boosting mixing be-\ntween the modes (Section 4.2).\nTable 1 reports the percentage of jumps between the modes\nand Figure 5 reports the corresponding accuracies. While\nthe comparison methods PT and DiGS explore the modes\nwell in low dimensions, they get completely stuck in one\nmode for D ≥8. MAGSS with the inverse Monge metric\n(α = 0.1) is able to jump between the modes even for higher\ndimensions and is overall the most accurate method. The\nmeta-sampler retains efficient exploration throughout the ex-\nperiment, but fails to reach the correct balance between the\ntwo modes, which is shown as higher Wasserstein distance.\nExperiment specifications:\nFor MALA we find a step-\nsize of 60% acceptance rate for each dimension, since it\nis close to the optimal for Gaussians [Roberts and Rosen-\nthal, 1998]. For MAGSS we try the Euclidean metric\nand the grid: α2 ∈{10−3, 10−2, 10−1, 1, 10} and λ ∈\n{10−4, 10−3, 10−2, 10−1, 1, 10}. We find α2 = 0.1 is al-\nways optimal. For Meta-MAGSS we fix α2 = 0.1 based\non what was observed for MAGSS. We use Dopri5 solver\nwith adaptive step-size. DiGS uses 10 MALA steps per iter-\nTable 1: Mixture of two Gaussians (GIM, with α = 0.1).\njump%\ndimension\nsampler\n2\n4\n8\n16\n32\n64\nMAGSS\n8.96\n5.07 2.28\n0.8\n0.2 0.03\nMeta-MAGSS 18.91 12.33\n7.5 4.29 2.45 1.07\nDiGS\n5.05\n0.02\n0.0\n0.0\n0.0\n0.0\nPT\n12.54\n4.68 0.23\n0.0\n0.0\n0.0\nFigure 5: Accuracy (lower is better) for mixture of Gaus-\nsians. Both MAGSS variants use GIM with α = 0.1.\nations, T = 100 equally spaced times between α1 = 10−4,\nα1000 = 1 −10−4. PT uses N = 100 temperatures in the\nscale τi = b−i/N\nmin\nfor i = 1, .., N where bmin = 10−4.\n5.3\nMULTIMODAL WITH COMPLEX MODES\nTo demonstrate that we can simultaneously handle multi-\nmodality and complex local geometry, we consider a (uni-\nform) mixture of two narrow bivariate distributions, the\nRosenbrock and Squiggle distributions (Figure 6 left; the\nred line is purely for identifying jumps between the modes).\nWe use use the inverse Monge and inverse Generative met-\nrics. However, Figure 6 (right) indicates that PT is the least\naccurate method, requiring substantially more samples for\nmatching the target well. All methods will reach approxi-\nmately the same Wasserstein distance if ran long enough, but\nboth of our variants achieve it in less samples, confirming\n\n\nTable 2: Mixture of narrow distributions.\nsampler\nmetric\njump%\nt(s)\nPT\nNA\n6.18\n2\nDiGS\nNA\n0.27\n2\nMAGSS\nGIg, λ = 1.0\n0.82\n17\nMeta-MAGSS\nGIM, α2 = 10−4\n0.99\n134\nFigure 6: Left: Mixture of narrow distributions, with sam-\nples using Meta-MAGSS. Right: Wasserstein distance as a\nfunction of iterations (samples). For MAGSS we use GIM\nwith α2 = 10−4 and for Meta-MAGSS GIg with λ = 1.\nmore efficient mixing.\nExperiment specifications:\nObtain 10, 000 samples us-\ning 10 chains. We run DiGS with 5 noise steps between\n0.1 and 0.9, and 10 MALA iterations per sample. PT uses\nτ ∈{1.0, 5.62, 31.62, 177.83, 1000} and thinning of 10.\nMAGSS and Meta-MAGSS are tuned using the same grid\nof values as Experiment 5.2, reporting the best based on dis-\ntances. Meta-MAGSS uses 5 sweeps, 10 MALA iterations\nper sample. PT, DiGS and Meta-MAGSS rely on MALA\nwith stepsize 0.001 (roughly 60% acceptance rate). We use\nw = 3 and m = 8 and the Dopri8 integrator with adaptive\nstep-size.\n5.4\nEFFECT OF NUMERICAL INTEGRATOR\nWe use numerical integrators for computing the geodesics\nin Eq. (2). To explore the effect of the integrator, we present\nresults for broad range of integrators for a multimodal bench-\nmark task considered previously by Chen et al. [2024]: The\ntarget is a 40-mode Gaussian mixture model with equal\nweights and each component of variance σ = 0.1 where the\nmeans are distributed uniformly on the square (−40, 40)2.\nWe use seven different integrators for GIM and GIg metrics,\nincluding both adaptive and fixed step-sizes as implemented\nin [Kidger, 2021], and report the results in Figure 7. The\nthree main conclusion are: (a) For metrics that are further\naway from Euclidean (large α or small λ) the integration\ntime for adaptive methods grows dramatically. This is a nat-\nural consequence of operating in a less flat geometry. (b)\nFor good accuracy we typically need to use such a geom-\netry, which means there is inherent compromise between\naccuracy an computation. (c) Simple fixed-step integrators,\nFigure 7: Different numerical integrators for the 40 mode\nGaussian mixture model. The black dotted lines are PT and\nDiGS. Left: Inverse Generative metric with parameter λ.\nRight: Inverse Monge metric with parameter α2.\neven the Euler method, are efficient when they work, but for\nrobustness we recommended adaptive methods. We recog-\nnize dopri5 as a good practical recommendation, but Euler\nis worth trying for the Inverse Generative metric.\nExperiment specifications:\nObtain 10, 000 samples using\n10 chains. PT has τ ∈{1, 5.62, 31.62, 177.83, 1000} and\nthinning of 200. DiGS uses α = 0.1, thinning of 200 and 5\nMALA steps per step. MALA has step size of 0.1. MAGSS\nis run with w = 3, m = 8 for the GIM and GIg and metrics\nwith the parameter grid of Experiment 5.2. We test seven\ndifferent numerical integrators of Equation (2). The fixed\nintegration size is 0.01. Details in Appendix B.\n6\nRELATED WORK\nLan et al. [2014] constructed the Wormhole Hamiltonian\nMonte Carlo where a specific geometry is constructed to\n(only) connect the modes of multi-modal distributions. The\nmodes are first identified along the Markov Chain evolution.\nAfter a new mode identification, it \"stores\" the mode’s loca-\ntion for later use. A jump using the updated mode candidates\nguarantees correct detailed balance equations. While this\nwork served as an inspirational motivation for us, it requires\nnotable additional components. MAGSS does not require\nseparate identification or storage of the modes, but instead\nshrinks the distances by naturally warping the space.\n7\nCONCLUSIONS\nOur aim was to show that multimodality and local curvature\ncan be address by the same set of tools, namely Riemannian\n\n\ngeometry. We provided a concrete Riemannian slice sampler,\nintroduced two new metrics for improving mixing between\nmodes, and showed that we can achieve accuracy and mixing\ncomparable to recent samplers designed specifically for\nmulti-modal targets, by only using a metric for this.\nThe obvious limitation is the computational cost. We form\ngeodesics by numeric integration and even when using\nmetrics with closed-form inverses the per-iteration cost of\nMAGSS is larger than of competing methods. However,\nwe note that we used maximally exact solvers rather than\nseeking for the highest computational efficiency. Now that\nthe principle has been demonstrated, use of more approx-\nimative numerical integrators for speeding up the overall\ncomputation could be studied in future work.\nAcknowledgements\nThe work was supported by the Research Council of Finland\nFlagship programme: Finnish Center for Artificial Intelli-\ngence (FCAI), and additionally by grants: 363317, 345811\nand 348952. The authors wish to acknowledge CSC – IT\nCenter for Science, Finland, for computational resources.\nReferences\nHerbert Amann, Joachim Escher, and Gary Brookfield. Anal-\nysis. Springer, 2005.\nClaude JP Bélisle, H Edwin Romeijn, and Robert L Smith.\nHit-and-run algorithms for generating multivariate dis-\ntributions. Mathematics of Operations Research, 18(2):\n255–266, 1993.\nSimon Byrne and Mark Girolami. Geodesic Monte Carlo on\nembedded manifolds. Scandinavian Journal of Statistics,\n40(4):825–845, 2013.\nWenlin Chen, Mingtian Zhang, Brooks Paige, José Miguel\nHernández-Lobato, and David Barber. Diffusive Gibbs\nsampling. In Forty-first International Conference on Ma-\nchine Learning, 2024.\nAlain Durmus, Samuel Gruffaz, Mareike Hasenpflug, and\nDaniel Rudolf. Geodesic slice sampling on Riemannian\nmanifolds. arXiv preprint arXiv:2312.00417, 2023.\nDavid J Earl and Michael W Deem. Parallel tempering:\nTheory, applications, and new perspectives. Physical\nChemistry Chemical Physics, 7(23):3910–3916, 2005.\nRémi Flamary, Nicolas Courty, Alexandre Gramfort,\nMokhtar Z Alaya, Aurélie Boisbunon, Stanislas Cham-\nbon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras,\nNemo Fournier, et al. Pot: Python optimal transport. Jour-\nnal of Machine Learning Research, 22(78):1–8, 2021.\nCharles J Geyer.\nMarkov chain monte carlo maximum\nlikelihood. Computing science and statistics: proceedings\nof the 23rd symposium on the interface, 1991.\nMark Girolami and Ben Calderhead. Riemann manifold\nLangevin and Hamiltonian Monte Carlo methods. Jour-\nnal of the Royal Statistical Society: Series B (Statistical\nMethodology), 73(2):123–214, 2011.\nMichael Habeck, Mareike Hasenpflug, Shantanu Kodgirwar,\nand Daniel Rudolf. Geodesic slice sampling on the sphere.\narXiv preprint arXiv:2301.08056, 2023.\nMarcelo Hartmann, Mark Girolami, and Arto Klami. La-\ngrangian manifold Monte Carlo on Monge patches. In\nInternational Conference on Artificial Intelligence and\nStatistics, pages 4764–4781. PMLR, 2022.\nPatrick Kidger. On neural differential equations. PhD thesis,\nUniversity of Oxford, 2021.\nBeomsu Kim, Michael Puthawala, Jong Chul Ye, and\nEmanuele Sansone. (deep) generative geodesics. arXiv\npreprint arXiv:2407.11244, 2024.\nShiwei Lan, Jeffrey Streets, and Babak Shahbaba. Worm-\nhole Hamiltonian Monte Carlo. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 28,\n2014.\nShiwei Lan, Vasileios Stathopoulos, Babak Shahbaba, and\nMark Girolami. Markov Chain Monte Carlo from La-\ngrangian Dynamics.\nJournal of Computational and\nGraphical Statistics, 24(2):357–378, 2015.\nKrzysztof Łatuszy´nski and Daniel Rudolf. Convergence of\nhybrid slice sampling via spectral gap. arXiv preprint\narXiv:1409.2709, 2014.\nKrzysztof Łatuszy´nski, Matthew T Moores, and Timothée\nStumpf-Fétizon. MCMC for multi-modal distributions.\narXiv preprint arXiv:2501.05908, 2025.\nJohn M Lee. Introduction to Riemannian manifolds, vol-\nume 2. Springer, 2018.\nIain Murray, Ryan Adams, and David MacKay. Elliptical\nslice sampling. In Proceedings of the thirteenth interna-\ntional conference on artificial intelligence and statistics,\npages 541–548. JMLR Workshop and Conference Pro-\nceedings, 2010.\nViacheslav Natarovskii, Daniel Rudolf, and Björn Sprungk.\nQuantitative spectral gap estimate and Wasserstein con-\ntraction of simple slice sampling. The Annals of Applied\nProbability, 2021.\nRadford M Neal. Slice sampling. The annals of statistics,\n31(3):705–767, 2003.\n\n\nFilippo Pagani, Martin Wiegand, and Saralees Nadarajah.\nAn n-dimensional Rosenbrock distribution for Markov\nchain Monte Carlo testing.\nScandinavian Journal of\nStatistics, 49(2):657–680, 2022.\nGareth O Roberts and Jeffrey S Rosenthal. Optimal scaling\nof discrete approximations to Langevin diffusions. Jour-\nnal of the Royal Statistical Society: Series B (Statistical\nMethodology), 60(1):255–268, 1998.\nGareth O Roberts and Richard L Tweedie. Exponential\nconvergence of Langevin distributions and their discrete\napproximations. Bernoulli, 1996.\nDaniel Rudolf and Mario Ullrich. Comparison of hit-and-\nrun, slice sampler and random walk metropolis. Journal\nof Applied Probability, 55(4):1186–1202, 2018.\nYang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. Advances in\nneural information processing systems, 32, 2019.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole. Score-\nbased generative modeling through stochastic differential\nequations. In International Conference on Learning Rep-\nresentations, 2021.\nRobert H Swendsen and Jian-Sheng Wang. Replica Monte\nCarlo simulation of spin-glasses. Physical review letters,\n57(21):2607, 1986.\nHakon Tjelmeland and Bjorn Kare Hegstad. Mode jumping\nproposals in MCMC. Scandinavian journal of statistics,\n28(1):205–223, 2001.\nBernardo Williams, Hanlin Yu, Marcelo Hartmann, and\nArto Klami. Geometric no-U-Turn samplers: Concepts\nand evaluation.\nIn 12th International Conference on\nProbabilistic Graphical Models (PGM), pages 327–347.\nJournal of Machine Learning Research, 2024.\nDawn B Woodard, Scott C Schmidler, and Mark Huber.\nConditions for rapid mixing of parallel and simulated\ntempering on multimodal distributions. The Annals of\nApplied Probability, 2009.\nHanlin Yu, Marcelo Hartmann, Bernardo Williams Moreno\nSanchez, Mark Girolami, and Arto Klami. Riemannian\nLaplace approximation with the Fisher metric. In Interna-\ntional Conference on Artificial Intelligence and Statistics,\npages 820–828. PMLR, 2024.\n\n\nGeodesic Slice Sampler for Multimodal Distributions with Strong Curvature\n(Supplementary Material)\nBernardo Williams †,1\nHanlin Yu1\nHoang Phuc Hau Luu1\nGeorgios Arvanitidis2\nArto Klami1\n1Department of Coputer Science, University of Helsinki, Finland\n2Cognitive Systems, DTU Compute, Technical University of Denmark\n†bernardo.williamsmoreno@helsinki.fi\nCONTENTS\nA Metric-agnostic Geodesic Slice Sampler\n12\nA.1\nMeta-Metric-agnostic Geodesic Slice Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nA.2\nStep-out and Shrinkage procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nA.3 The Geodesic Equations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nA.4\nSample Uniformly from the Unit Tangent Sphere\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nA.5\nHausdorff measure\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nA.6\nObservation and Proposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nB Additional Experimental Results\n15\nB.1\nUnimodal: Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nB.2\nNumerical Integrators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nC Mathematical Derivations\n16\nC.1\nThe Generative Metric\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nC.2\nThe Monge metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nC.3\nMonge metric: Christoffel symbols derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nC.4\nInverse Monge metric: Christoffel symbols derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.5\nComplex geometry unimodal distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n\n\nA\nMETRIC-AGNOSTIC GEODESIC SLICE SAMPLER\nA.1\nMETA-METRIC-AGNOSTIC GEODESIC SLICE SAMPLER\nAlgorithm 2 is the combination of MAGSS for K-steps followed by a local MCMC sampler for L-steps.\nAlgorithm 2 Meta-MAGSS\nInput: Initial position x[0] and metric components G(x). Parameters m ∈N, w ≥0, K sweeps, L steps of local MCMC\nsampler.\nOutput: N samples x[n].\n1: for n ←1, . . . , N do\n2:\nLet x ←x[n−1]\n3:\nfor k ←1, . . . , K do\n4:\nUpdate x by Approximate Geodesic Slice Sampling starting at x\n5:\nend for\n6:\nfor l ←1, . . . , L do\n7:\nUpdate x by local MCMC sampler starting at x.\n8:\nend for\n9:\nSet x[n] ←x\n10: end for\nA.2\nSTEP-OUT AND SHRINKAGE PROCEDURES\nThe stepping-out and shrinkage procedures can be found in algorithm boxes 3 and 4, the algorithms are taken from Durmus\net al. [2023]. The code implementation of the step-out procedure has vectorized both while loops by evaluation of the log\ndensity on all step-out points at once. The code implementation of the shrinkage procedure has a max number of iteration\nset at 100 of the while loop, which defaults back to the previous point of the chain if exceeded. In the algorithm boxes we\nuse the notation for the exponential map γ(x,v)(t).\nAlgorithm 3 Stepping-out procedure. Call it Step-outw,m(s, γ(x,v))\nInput: point x ∈M, direction v ∈Sd−1\nx\n, level s ∈(0, p(x)), hyperparameters w ∈(0, ∞) and m ∈N\nOutput: two points ℓ, r ∈R such that ℓ< 0 < r\n1: Draw u ∼Unif([0, w]).\n2: Set ℓ:= −u and r := ℓ+ w.\n3: Draw ι ∼Unif({1, . . . , m}).\n4: Set i = 2 and j = 2.\n5: while i ≤ι and pH(γ(x,v)(ℓ)) > s do\n6:\nSet ℓ= ℓ−w.\n7:\nUpdate i = i + 1.\n8: end while\n9: while j ≤m + 1 −ι and pH(γ(x,v)(r)) > s do\n10:\nSet r = r + w.\n11:\nUpdate j = j + 1.\n12: end while\n13: return (ℓ, r)\n\n\nAlgorithm 4 Shrinkage procedure. Call as Shrinkℓ,r(s, γ(x,v))\nInput: point x ∈M, direction v ∈Sd−1\nx\n, level s ∈(0, p(x)) and parameters ℓ< 0 < r\nOutput: point θ ∈L(x, v, s) ∩[ℓ, r]\n1: Draw θh ∼Unif((0, r −l)).\n2: Set θ := θh −1{θh>r}(r −l).\n3: Set θmin := θh.\n4: Set θmax := θh.\n5: while pH(γ(x,v)(θ)) ≤s do\n6:\nif θh ∈[θmin, r −l] then\n7:\nSet θmin = θh.\n8:\nelse\n9:\nSet θmax = θh.\n10:\nend if\n11:\nDraw θh ∼Unif((0, θmax) ∪[θmin, r −l)).\n12:\nSet θ = θh −1{θh>r}(r −l).\n13: end while\n14: return θ.\nA.3\nTHE GEODESIC EQUATIONS\nWe provide only the necessary background on Riemannian geometry.\nA Riemannian metric tensor is the map g(·, ·) : Tx M ×Tx M →R where Tx M is the tangent space. The map must\nsatisfy positive definiteness for every x ∈M. Namely, the components G(x) are such that for all v, u ∈Tx M, then\ng(v, u) = v⊤G(x)u > 0 (this is equivalent to saying that G(x) is positive definite).\nGeodesic equations generalize straight lines to curved manifolds. Let M be a Riemannian manifold with metric tensor G.\nThe tangent space at a point x is denoted by Tx M. A geodesic curve is determined by solving a second-order differential\nequation with initial conditions x0 ∈M and v0 ∈Tx M. The metric components are given by gij = G(x)ij, and their\ninverse by gkm = G−1(x)km. The geodesic equations are:\n˙xk = vk,\n˙vk = −∥v∥2\nΓk,\nfor k = 1, . . . , D.\nThe Christoffel symbols, using Einstein summation convention, are: Γk\nij = 1\n2gkm(∂igmj + ∂jgim −∂mgij).\nA.4\nSAMPLE UNIFORMLY FROM THE UNIT TANGENT SPHERE\nRecall the unit tangent sphere is defined as:\nSD−1\ng\n(x) := {v ∈RD : ∥v∥2\ng = 1}.\nDurmus et al. [2023, Appendix C.4] justify the existence of the uniform distribution distribution over SD−1\ng\n(x). One method\nfor producing samples from the uniform distribution on the unit tangent sphere is:\n1. Sample z ∼N(0, I).\n2. Transform v ←G−1\n2 (x)z, then v is a sample from N(0, G−1(x)).\n3. Compute the Riemannian norm ∥v∥g =\np\nvT G(x)v.\n4. Project to the boundary v ←\nv\n∥v∥g .\nA.5\nHAUSDORFF MEASURE\nThe volume form of a Riemannian Manifold with metric G(x) is defined as V (dx) :=\np\ndet G(x) dx. For the technical\ndetails about the volume form interested readers can consult Proposition 2.41 in Lee [2018]. The volume element gives the\n\n\nnatural measure on the manifold, analogous to the Lebesgue measure in the Euclidean space [Durmus et al., 2023]. The\nHausdorff density is defined as the density which integrates to one with respect to the volume element:\npH(x) =\np(x)\np\ndet G(x)\n.\nA.6\nOBSERVATION AND PROPOSITION\nObservation 1. Let p(x) be a smooth density function. Let (xt, vt) be the geodesic flow with initial conditions (x0, v0)\nwith respect to the inverse Generative/Monge metric. Then, for t such that p(xt) →0, the Euclidean velocity norm ∥vt∥2\ngets small, implying the low value density regions are covered faster as function of time t.\nDenote by GIM(x) the inverse Monge metric and by GIg(x) the inverse generative metric. The metrics are defined as:\nGIg(x) =\n\u0012 p0 + λ\np(x) + λ\n\u00132\nI,\nGIM(x) = I −\nα2\n1 + α2∥∇ℓ(x)∥2 ∇ℓ(x)∇ℓ(x)⊤.\nAnalysis for the Generative Metric\nAssume a geodesic curve starting at (x0, v0) satisfies p(x0) ≥p(xt) for all t ≥0\nand p(xt) →0. Recall that along a geodesic curve, the magnitude of the velocity with respect to the metric remains constant:\n∥vt∥2\nGIg = ∥v0∥2\nGg\n∀t.\nThus, the equality holds:\n∥vt∥2\nGIg = ∥v0∥2\nGIg\n\u0012\np0 + λ\np(x0) + λ\n\u00132\n∥v0∥2 =\n\u0012 p0 + λ\np(xt) + λ\n\u00132\n∥vt∥2\n2\n\u0012 p(xt) + λ\np(x0) + λ\n\u00132\n∥v0∥2 = ∥vt∥2\n2.\nSince p(x0) ≥p(xt), it follows that ∥v0∥2\n2 ≥∥vt∥2\n2.\nAnalysis for the Inverse Monge Metric\nAssume a geodesic curve starting at (x0, v0) satisfies p(x0) ≥p(xt) for all\nt ≥0 and p(xt) →0. Assume x0 is not a critical point, i.e., ∇log p(x0) ̸= 0. Since p(x) ≥0 is smooth with finite integral,\nwe have ∇p(xt) →0. Then:\n∥v0∥2\nGIM = ∥vt∥2\nGIM\n∥v0∥2\n2 −\nα2\n1 + α2∥∇ℓ(x0)∥2 ⟨∇ℓ(x0), v0⟩2 = ∥vt∥2\n2 −\nα2\n1 + α2∥∇ℓ(xt)∥2\n2\n⟨∇ℓ(xt), vt⟩2\n∥v0∥2\n2 −\nα2\n1 + α2∥∇ℓ(x0)∥2 ⟨∇ℓ(x0), v0⟩2 = ∥vt∥2\n2.\nSince\nα2\n1+α2∥∇ℓ(x0)∥2 ⟨∇ℓ(x0), v0⟩2 > 0, it follows that ∥v0∥2\n2 > ∥vt∥2\n2.\nJoint conclusion\nIntuitively, this means that if we are on a position xt along the trajectories we described before where\nthe velocity is of small Euclidean norm ∥vt∥2 then the Riemannian norm of that vector is much higher ∥vt∥G. Then the\nExponential map evaluated at that point and velocity is:\nγ(x,v)(1) = γ(x,v /∥v∥G)(∥v∥G),\n(1)\nthis means that even a velocity vector with small euclidean norm ∥vt∥2, corresponds to a larger a unit ∥v∥G of time along\nthe geodesic equation.\n\n\nFigure 1: The Hausdorff density of a mixture of two Gaussian distributions evaluated along the geodesic, namely t 7→\npH(ˆγ(x,v)(t)) for different values of λ and α2. Left: inverse Monge metric. Right: inverse Generative metric\nNote that for a multimodal distribution in dim ≥2 Observation 4.2 guarantees that low density regions will be transversed\nquickly, but we do not have guarantees any that the geodesics given by the inverse Generative and inverse Monge metrics\nwill connect modes.\nNote that this analysis also holds for the inverse Monge metric when the geodesic curve approaches a local maximum of the\ndistribution, which explains the behavior observed in Figure 1 around the local maximum values when α = 0.1.\nProposition 1. An MCMC sampler targeting the Hausdorff density on a Riemannian manifold M with metric tensor G(x)\nalso targets the correct distribution on the Euclidean space.\nFor a proof of the Proposition in a more general setting, consult 1.5 Proposition of section XII.1 in Amann et al. [2005].\nThe volume element on the manifold is defined as V (dx) =\np\ndet G(x) dx, where G(x) is the Riemannian metric tensor.\nLet X be a random variable on M whose law is pH where pH(x) is the Hausdorff density. Let B ∈B(M) be a Borel set\non the manifold M. The probability of X being in B, under the Hausdorff target density, is given by\nP(X ∈B) =\nZ\nB\npH(x)V (dx).\n(2)\nSubstituting pH(x) =\np(x)\n√\ndet G(x),\nP(X ∈B) =\nZ\nB\np(x)\np\ndet G(x)\np\ndet G(x) dx\n=\nZ\nB\np(x) dx .\n(3)\nThus, the integral of the Hausdorff density with respect to the volume element on the manifold coincides with the integral of\nthe Euclidean density p(x) over the same set B.\nSince the probabilities computed for any B ∈B(M) are identical whether using (2) or (3), the corresponding estimators for\nthe probabilities also coincide. Consequently, an MCMC sampler on the manifold targeting the Hausdorff density pH(x)\ncorrectly targets the Euclidean density p(x) in RD.\nB\nADDITIONAL EXPERIMENTAL RESULTS\nB.1\nUNIMODAL: LOGISTIC REGRESSION\nHere we denote by θ the random variable of interest and by x input data. The Logistic regression model [Girolami and\nCalderhead, 2011]\np(yi | θ, xi) = Bernoulli(yi |s(x⊤\ni θ)),\np(θ) = N(θ |0, α ID),\ni = 1, .., N,\nwhere α = 100 and s(·) is the Sigmoid function. The Fisher Information Metric plus the Hessian is: G(x) =\nX⊤Λ X +α−1 I. Where X is the covariate matrix and Λ is a diagonal matrix with entries Λnn = s(x⊤\ni θ)\n\u00001 −s(x⊤\ni θ)\n\u0001\n.\n\n\nmodel\nmetr\nWass\nmin ESS\navg ESS\navg step-out\navg shrinkage\nt(s)\naus\neuclidean\n[0.74, 0.12]\n[18, 5]\n[228, 12]\n[0.14, 0.0]\n[3.33, 0.01]\n8.2\nfisher\n[0.58, 0.01]\n[177, 9]\n[270, 13]\n[0.95, 0.0]\n[1.0, 0.01]\n3298.6\nger\neuclidean\n[0.49, 0.01]\n[35, 10]\n[119, 9]\n[0.08, 0.0]\n[4.19, 0.02]\n15.8\nmonge\n[0.75, 0.02]\n[80, 5]\n[4673, 113]\n[3.97, 0.03]\n[0.28, 0.01]\n4665.4\nfisher\n[0.49, 0.0]\n[85, 24]\n[169, 5]\n[0.95, 0.0]\n[0.99, 0.01]\n19684.4\nhrt\neuclidean\n[0.63, 0.01]\n[137, 26]\n[254, 16]\n[0.19, 0.01]\n[2.72, 0.02]\n7.3\nmonge\n[0.74, 0.03]\n[394, 55]\n[2979, 266]\n[2.99, 0.04]\n[0.38, 0.02]\n637.4\nfisher\n[0.64, 0.01]\n[232, 14]\n[311, 8]\n[0.92, 0.01]\n[1.01, 0.01]\n1694.8\npim\neuclidean\n[0.21, 0.0]\n[266, 41]\n[445, 45]\n[0.11, 0.0]\n[3.59, 0.03]\n6.8\nmonge\n[0.29, 0.01]\n[515, 72]\n[4656, 269]\n[2.38, 0.03]\n[0.53, 0.02]\n1711.7\nfisher\n[0.21, 0.0]\n[427, 47]\n[547, 30]\n[0.93, 0.0]\n[0.98, 0.01]\n425.4\nrip\neuclidean\n[0.09, 0.01]\n[829, 112]\n[1499, 56]\n[0.24, 0.01]\n[2.46, 0.02]\n4.2\nmonge\n[0.15, 0.02]\n[1042, 211]\n[3123, 685]\n[1.38, 0.02]\n[0.97, 0.01]\n593.9\nfisher\n[0.09, 0.0]\n[1623, 113]\n[1753, 92]\n[0.91, 0.0]\n[0.96, 0.01]\n204.7\nTable 1: Bayesian Logistic Regression, the notation is [mean, std].\nReferences samples are obtained with HMC-NUTS. The Euclidean and Fisher metrics are just as close to the samples, but\nFisher and Monge have higher effective sample size and use less shrinkage iterations. In spite of the fact that on flat methods\ntake significantly longer to run. The notation used is [mean, std] over 5 runs with different seeds.\nB.2\nNUMERICAL INTEGRATORS\nThe numerical solvers we consider are part of the diffrax package [Kidger, 2021]. We consider three groups of solvers.\nSimple solvers (euler, tsit, dopri). Implicit solvers (kv). And reversible solvers (revheun). The solvers have the following\ncharacteristics:\n• euler: The Euler solver can only be used with a fixed step-size.\n• tsit: Tsitouras’ 5/4 method can be used with both fixed and adaptive step-size.\n• dopri5: Dormand-Prince’s 5/4 method can be used with both fixed and adaptive step-size.\n• dopri8: Dormand-Prince’s 8/7 method can be used with both fixed and adaptive step-size.\n• kv3: Kvaerno’s 3/2 method is an implicit solver can be only used with adaptive step-size.\n• kv5: Kvaerno’s 5/4 method is an implicit solver can only be used adaptive step-size.\n• revheun: Reversible Heun method can be used with both fixed and adaptive step-size.\nC\nMATHEMATICAL DERIVATIONS\nC.1\nTHE GENERATIVE METRIC\nThe Generative metric components are:\nG(x) = f(x) I = exp(log f(x)) I,\nwhere the scalar factor is f(x) =\n\u0010\np(x)+λ\np0+λ\n\u00112\n.\nSquare root and inverse square root\nG\n1\n2 (x) = exp\n\b 1\n2 log f(x)\n\t\nI,\nG−1\n2 (x) = exp\n\b\n−1\n2 log f(x)\n\t\nI,\nlog | det G(x)| = D log f(x),\n\n\nwhere f(x) is the scalar factor.\nChristoffel symbols derivation\nGiven the Riemannian metric\nG(x) =\n\u0012p(x) + λ\np0 + λ\n\u00132\nI,\nThe metric tensor entries are\nGij(x) = f(x)δij.\nThe partial derivative of f(x) with respect to xi is\n∂if(x) =\nd\ndxi\n\u0012p(x) + λ\np0 + λ\n\u00132\n= 2\n\u0012p(x) + λ\np0 + λ\n\u0013\n1\np0 + λ\n∂p\n∂xi\n.\nThe Christoffel symbols for this metric are given by\nΓk\nij =\n1\n2f(x) (δjk∂if(x) + δik∂jf(x) −δij∂kf(x))\n= 1\n2 (δjk∂i log f(x) + δik∂j log f(x) −δij∂k log f(x)) .\nIn matrix notation, let ek be the standard basis vectors,\nΓk = 1\n2\n\u0000∇log f(x) e⊤\nk + ek ∇log f(x)⊤−∂k log f(x) I\n\u0001\n.\nThe geodesic equations require computing\n∥v∥2\nΓk = ⟨v, ∇log f⟩vk −1\n2∥v∥2∂k log f.\nThe geodesic equations read\n˙x = v,\n˙v = 1\n2∥v∥2∇log f −⟨v, ∇log f⟩v .\nC.2\nTHE MONGE METRIC\nThe Monge metric and its inverse,\nG(x) = I +α2∇ℓ∇ℓ⊤,\nG−1(x) = I −\nα2\n1 + α2∥∇ℓ∥2 ∇ℓ∇ℓ⊤.\nSquare root and inverse square root\nDefine the quantity Lα := 1 + α2∥∇ℓ∥2, then the values are:\nG1/2(x) = I +\nα2\n1 + √Lα\n∇ℓ(x)∇ℓ(x)⊤,\nG−1/2(x) = I −\nα2\nLα + √Lα\n∇ℓ(x)∇ℓ(x)⊤,\nlog | det G(x)| = log\n\u0010\n1 + α2∥∇ℓ∥2\u0011\n.\nWe now derive the quantities. Hartmann et al. [2022] gave:\nG−1\n2 (x) = I +\n1\n∥∇ℓ∥2\n\u0012\n1\n√Lα\n−1\n\u0013\n∇ℓ∇ℓ⊤\n\n\nLet us find a numerical sable form for G−1\n2 (x). Multiplying by a convenient quantity, gives:\n1\n∥∇ℓ∥2\n\u0012\n1\n√Lα −1\n\u0013\n=\n1\n∥∇ℓ∥2\n\u00121 −√Lα\n√Lα\n\u0013 \u00121 + √Lα\n1 + √Lα\n\u0013\n=\nα2\nLα + √Lα\n.\nThe computation of G\n1\n2 (x). For convenience take y = ∇ℓ(x). The the metric is G(y) = I + yy⊤. Let us assume the\nsquare root is of the form G\n1\n2 (y) = I +λ y y⊤. Let us formulate a quadratic equation for λ:\nG\n1\n2 (y) G\n1\n2 (y) = I + y y⊤\nI + 2λ y y⊤+λ2∥y∥2 y y⊤= I + y y⊤\n0 =\n\u0010\n1 −2λ −λ2∥y∥2\u0011\ny y⊤.\nThe solutions of the quadratic equation are:\nλ = −1 ±\np\n1 + ∥y ∥2\n∥y ∥2\n.\nLet us simplify\n−1+√\n1+∥y ∥2\n∥y ∥2\n, multiply by conjugate\np\n1 + ∥y ∥2 −1\n∥y ∥2\n p\n1 + ∥y ∥2 + 1\np\n1 + ∥y ∥2 + 1\n!\n=\n∥y∥2\n∥y∥2p\n1 + ∥y ∥2 + 1\n.\nSubstitute y = α∇ℓ(x) and we obtain the result:\nG1/2(x) = I +\nα2\np\n1 + α2∥∇ℓ(x)∥2 + 1\n∇ℓ∇ℓ⊤.\nChristoffel symbols\nFor the Monge metric the Christoffel symbols taken from Hartmann et al. [2022] are:\nΓk(x) =\nα2\n1 + α2∥∇ℓ∥2 ∇2ℓ∂kℓ.\nThe geodesic equations read\n˙x = v,\n˙v = −α2\nLα\n∥v∥2\n∇2ℓ∇ℓ.\nThe Christoffel symbols associated to the inverse Monge metric are, the derivation in Section C.4:\nΓk = α2\n2\nh\nLα\n\u0000∇f∇ℓ⊤+ ∇ℓ∇f ⊤+ 2f∇2ℓ\n\u0001\n∂kℓ+ ∇ℓ∇ℓ⊤∂kf + α2 ⟨∇ℓ, ∇f⟩∇ℓ∇ℓ⊤∂kℓ\ni\n.\nThe geodesic equations read:\n˙x = v,\n˙v = −α2\n2\nh\n2Lα\n\u0010\n⟨v, ∇f⟩⟨∇ℓ, v⟩+ f∥v∥2\n∇2ℓ\n\u0011\n∇ℓ+ ⟨∇ℓ, v⟩2 ∇f + α2 ⟨∇ℓ, ∇f⟩⟨∇ℓ, v⟩2 ∇ℓ\ni\n.\nC.3\nMONGE METRIC: CHRISTOFFEL SYMBOLS DERIVATION\nFor completeness let us do a different derivation from Hartmann et al. [2022] of the Christoffel symbols. Take the auxiliary\nfunction f(x) = −1\nLα , where Lα = 1 + α2∥∇ℓ∥2. The metric and inverse components are:\ngij = δij + α2∂iℓ∂jℓ,\ngkm = δkm + α2f(x)∂kℓ∂mℓ.\n\n\nThe derivatives of the metric are\n∂igmj = α2 (∂imℓ∂jℓ+ ∂mℓ∂ijℓ) ,\n∂jgim = α2 (∂ijℓ∂mℓ+ ∂iℓ∂jmℓ) ,\n∂mgij = α2 (∂imℓ∂jℓ+ ∂iℓ∂jmℓ) .\nThe Christoffel symbols read,\nΓk\nij = 1\n2gkm (∂igmj + ∂jgim −∂mgij)\n= α2\n2 gkm (2∂mℓ∂ijℓ)\n= α2 X\nm\n\u0000δkm + α2f(x)∂kℓ∂mℓ\n\u0001\n∂mℓ∂ijℓ\n= α2\n \n∂kℓ∂ijℓ+ α2f(x)\nX\nm\n(∂mℓ)2∂k∂ijℓ\n!\n= α2∂kℓ∂ij\n\u0010\n1 −\nα2∥∇ℓ∥2\n1+α2∥∇ℓ∥2\n\u0011\n=\nα2\n1 + α2∥∇ℓ∥2 ∂kℓ∂ij.\nThus, the Christoffel symbols are Γk\nij = α2\nLα ∂kℓ∂ij. Writing in matrix form Γk of size D ×D with components [Γk]ij = Γk\nij,\nΓk =\nα2\n1 + α2∥∇ℓ∥2 ∇2ℓ∂kℓ.\nLet us compute ∥v∥2\nΓk, which appears in the geodesic equations:\nv⊤Γkv =\nα2\n1 + α2∥∇ℓ∥2 ∥v∥2\n∇2ℓ∂kℓ.\nThe geodesic equations read:\n˙x = v,\n˙v = −α2\nLα\n∥v∥2\n∇2ℓ∇ℓ.\nC.4\nINVERSE MONGE METRIC: CHRISTOFFEL SYMBOLS DERIVATION\nAgain the auxilary function is f(x) = −1\nLα , where Lα = 1 + α2∥∇ℓ∥2, the metric and inverse components are:\ngij = δij + f(x)α2∂iℓ∂jℓ\ngkm = δkm + α2∂kℓ∂mℓ.\nThe derivatives of the metric are (we mark with the same color repeating terms):\n∂igmj = α2 (∂if∂mℓ∂jℓ+ f∂imℓ∂jℓ+ f∂mℓ∂ijℓ) ,\n∂jgim = α2 (∂jf∂iℓ∂mℓ+ f∂ijℓ∂mℓ+ f∂iℓ∂jmℓ) ,\n∂mgij = α2 (∂mf∂iℓ∂jℓ+ f∂miℓ∂jℓ+ f∂iℓ∂mjℓ) .\nLet us compute the Christoffel symbols of the first kind (blue and red terms will cancel out, pink terms add to each other):\nΓkij = 1\n2 (∂igmj + ∂jgim −∂mgij)\n= α2\n2 (∂if∂mℓ∂jℓ+ ∂jf∂iℓ∂mℓ−∂mf∂iℓ∂jℓ+ 2f∂ijℓ∂mℓ) .\n\n\nThe Christoffel symbols of the second kind read:\nΓk\nij = 1\n2gkm (∂igmj + ∂jgim −∂mgij)\n= α2\n2\nX\nm\n\u0000δkm + α2∂kℓ∂mℓ\n\u0001\n(∂if∂mℓ∂jℓ+ ∂jf∂iℓ∂mℓ−∂mf∂iℓ∂jℓ+ 2f∂ijℓ∂mℓ)\n= α2\n2\n\u0012\n∂if∂kℓ∂jℓ+ ∂jf∂iℓ∂kℓ−∂kf∂iℓ∂jℓ+ 2f∂ijℓ∂kℓ\n+ α2∂kℓ\n\u0010\n∂if∥∇ℓ∥2∂jℓ+ ∂jf∂iℓ∥∇ℓ∥2 −⟨∇f, ∇ℓ⟩∂iℓ∂jℓ+ 2f∂ijℓ∥∇ℓ∥2\u0011 \u0013\n= α2\n2\n\u0012\n∂kℓ\n\u0000Lα∂if∂jℓ+ Lα∂iℓ∂jf −α2 ⟨∇f, ∇ℓ⟩∂iℓ∂jℓ+ 2Lαf(x)∂ijℓ\n\u0001\n−∂kf∂iℓ∂jℓ\n\u0013\n.\nThus, the Christoffel symbols are:\nΓk\nij = α2\n2\nh\n∂kℓ\n\u0000Lα (∂if∂jℓ+ ∂iℓ∂jf + 2f(x)∂ijℓ) −α2 ⟨∇f, ∇ℓ⟩∂iℓ∂jℓ\n\u0001\n−∂kf∂iℓ∂jℓ\ni\n.\nWritten in matrix form:\nΓk = α2\n2\nh\n∂kℓ\n\u0000Lα\n\u0000∇f∇ℓ⊤+ ∇ℓ∇f ⊤+ 2f(x)∇2ℓ\n\u0001\n−α2 ⟨∇f, ∇ℓ⟩∇ℓ∇ℓ⊤\u0001\n−∂kf∇ℓ∇ℓ⊤i\n.\nLet us compute ∥v∥2\nΓk which appears in the geodesic equations:\nv⊤Γkv = α2\n2\nh\n∂kℓ\n\u0010\n2Lα\n\u0010\n⟨v, ∇f⟩⟨∇ℓ, v⟩+ f∥v∥2\n∇2ℓ\n\u0011\n−α2 ⟨∇f, ∇ℓ⟩⟨∇ℓ, v⟩2\u0011\n−∂kf ⟨∇ℓ, v⟩2 i\n.\nThen the geodesic equations read:\n˙x = v,\n˙v = −α2\n2\nh \u0010\n2Lα\n\u0010\n⟨v, ∇f⟩⟨∇ℓ, v⟩+ f∥v∥2\n∇2ℓ\n\u0011\n−α2 ⟨∇f, ∇ℓ⟩⟨∇ℓ, v⟩2\u0011\n∇ℓ−⟨∇ℓ, v⟩2 ∇f\ni\n.\nWhere the gradient of f is:\n∇f = 2α2\nL2α\n∇2ℓ∇ℓ.\nC.5\nCOMPLEX GEOMETRY UNIMODAL DISTRIBUTIONS\nThe Funnel distribution\np(x) = N(xD|0, σ2)N(x1:D−1|µ, exDID−1), use the notation x = x(z) and z = z(x), where\nz ∼N(0, I). Choice of parameters σ = 3, µ = 0.\nx =\n\u0014\neσzD/2z1:D−1\nσzD\n\u0015\n,\n∂x\n∂z =\n\u0014\neσzD/2ID−1\nσ\n2 eσzD/2z1:D−1\n0\nσ\n\u0015\n∂z\n∂x =\n\u0014\ne−xD/2ID−1\n−1\n2e−xD/2x1:D−1\n0\n1\nσ.\n\u0015\n.\nThe log determinant of the inverse Jacobian is log det\n\u0000 ∂z\n∂x\n\u0001\n= −(D −1)xD/2 −log σ.\nThe hybrid Rosenbrock distribution\nFor simplicity here we show the two dimensional case, the full distribution can be\nconsulted in Pagani et al. [2022]. The two dimensional density is: p(x) = N(x1|a, 1\n2) N(x2|x2\n1, 1\n2b). Choice of parameters\na = 1, b = 100 and block size of 3 and ⌊D−1\n3 ⌋total blocks.\nx =\n\"\na +\n1\n√\n2z1\n(a +\n1\n√\n2z1)2 +\n1\n√\n2bz2\n#\n,\n∂x\n∂z =\n\"\n1\n√\n2\n0\n√\n2a + z1\n1\n√\n2b\n#\n,\n∂z\n∂x =\n\u0014\n√\n2\n0\n−2\n√\n2bx1\n√\n2b\n\u0015\n.\n\n\nThe Squiggle distribution\nThe density is p(x) = N(x(z)|µ, Σ)| det ∂x\n∂z |, where z ∼N(µ, Σ). Choice of parameters\na = 1.5, µ = 0, Σ = diag(5, 1\n2, .., 1\n2)\nx =\n\u0014\nz1\nz2:D −sin(az1)\n\u0015\n∂x\n∂z =\n\u0014\n1\n0\n−a cos(az1)\nI\n\u0015\n,\n∂z\n∂x =\n\u0014\n1\n0\na cos(ax1)\nI\n\u0015\n.\nThe log determinant of the inverse Jacobian is log det\n\u0000 ∂z\n∂x\n\u0001\n= 0.\nFor these toy problems we can express the Fisher Information follows from the transformation rule for Riemannian metrics:\nG(x) = ∂z\n∂x\n⊤\nΣ−1 ∂z\n∂x.\nAdd Location and Scale parameters for Complex Distributions\nIn Experiment 5.3 we consider the mixture of two\ncomplex distributions. We introduce a location and scale parameter for the components of the mixture. The Funnel, Squiggle\nand Rosenbrock distributions are smooth bijective transformations from a Z ∼N(µ, Σ) to Y = f(X). Let us add a location\nand scale parameters by an additional transformation g(y) = x, where g(y) = Σy I +µy\nZ\nf\n7−−−→Y\ng\n7−−−→X\nChange of variables for g ◦f gives\npX(x) = pZ\n\u0000(g ◦f)−1(x)\n\u0001 \f\f\f\fdet ∂x\n∂z\n\f\f\f\f\n= pZ\n\u0000(f −1 ◦g−1)(x)\n\u0001 \f\f\f\fdet ∂x\n∂y\n\f\f\f\f\n\f\f\f\fdet ∂y\n∂z\n\f\f\f\f\nPlug in g−1(x) = Σ−1/2\ny\n(x −µ), det ∂y\n∂z = det Σ−1/2, and pZ(z) = N(z|µ, Σ) to obtain the expression for the density of\nthe Funnel and Squiggle distributions with a location and scale parameters (µy, Σy):\npX(x) = N\n\u0012\nf −1(Σ1/2(x −µ))\n\f\f\f\fµ, Σ\n\u0013 \f\f\f\fdet ∂x\n∂y\n\f\f\f\f\n\f\f\fdet Σ−1/2\ny\n\f\f\f .\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21190v1.pdf",
    "total_pages": 21,
    "title": "Geodesic Slice Sampler for Multimodal Distributions with Strong Curvature",
    "authors": [
      "Bernardo Williams",
      "Hanlin Yu",
      "Hoang Phuc Hau Luu",
      "Georgios Arvanitidis",
      "Arto Klami"
    ],
    "abstract": "Traditional Markov Chain Monte Carlo sampling methods often struggle with\nsharp curvatures, intricate geometries, and multimodal distributions. Slice\nsampling can resolve local exploration inefficiency issues and Riemannian\ngeometries help with sharp curvatures. Recent extensions enable slice sampling\non Riemannian manifolds, but they are restricted to cases where geodesics are\navailable in closed form. We propose a method that generalizes Hit-and-Run\nslice sampling to more general geometries tailored to the target distribution,\nby approximating geodesics as solutions to differential equations. Our approach\nenables exploration of regions with strong curvature and rapid transitions\nbetween modes in multimodal distributions. We demonstrate the advantages of the\napproach over challenging sampling problems.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}