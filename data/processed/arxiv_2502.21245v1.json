{
  "id": "arxiv_2502.21245v1",
  "text": "TimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nHaoran Zhang * 1 Yong Liu * 1 Yunzhong Qiu * 1 Haixuan Liu 1\nZhongyi Pei 1 Jianmin Wang 1 Mingsheng Long 1\nAbstract\nTime series analysis is crucial in diverse scenar-\nios. Beyond forecasting, considerable real-world\ntasks are categorized into classification, imputa-\ntion, and anomaly detection, underscoring differ-\nent capabilities termed time series understand-\ning in this paper. While GPT-style models have\nbeen positioned as foundation models for time\nseries forecasting, the BERT-style architecture,\nwhich has made significant advances in natural\nlanguage understanding, has not been fully un-\nlocked for time series understanding, possibly at-\ntributed to the undesirable dropout of essential\nelements of BERT. In this paper, inspired by the\nshared multi-granularity structure between multi-\nvariate time series and multisentence documents,\nwe design TimesBERT to learn generic represen-\ntations of time series including temporal patterns\nand variate-centric characteristics. In addition\nto a natural adaptation of masked modeling, we\npropose a parallel task of functional token pre-\ndiction to embody vital multi-granularity struc-\ntures. Our model is pre-trained on 260 billion time\npoints across diverse domains. Leveraging multi-\ngranularity representations, TimesBERT achieves\nstate-of-the-art performance across four typical\ndownstream understanding tasks, outperforming\ntask-specific models and language pre-trained\nbackbones, positioning it as a versatile founda-\ntion model for time series understanding.\n1. Introduction\nTime series analysis is extensively applied across numer-\nous practical applications and has a diverse form of tasks,\namong which time series forecasting has attracted signifi-\ncant attention and research efforts (Oreshkin et al., 2019;\n*Equal contribution\n1 School of Software, BNRist, Tsinghua\nUniversity. Haoran Zhang <zhang-hr24@mails.tsinghua.edu.cn>.\nYong Liu <liuyong21@mails.tsinghua.edu.cn>. Yunzhong Qiu\n<qiuyz24@mails.tsinghua.edu.cn>. Correspondence to: Ming-\nsheng Long <mingsheng@tsinghua.edu.cn>.\nPreliminary work.\nGlobal Pattern\nClassification\n…\n…\nTimesBERT\nArbitrary Multivariate\nTime Series\nMasked Patch Modeling\nFunctional Token Prediction\nDiverse Time Series\nUnderstanding Tasks\n…\n…\nPre-training\nFine-Tuning\nAssociation\nAnomaly Detection\nFigure 1. TimesBERT inherits and extends the pre-training and\nfine-tuning paradigm established by BERT, which learns generaliz-\nable representation through pre-training on large-scale datasets of\narbitrary multivariate time series, and adapts the foundation model\nto diverse tasks of time series understanding.\nWu et al., 2021; Nie et al., 2023; Liu et al., 2024a), be-\ncoming a primary task for evaluating the advances of deep\nlearning methods. However, the remaining tasks have re-\nceived relatively limited focus, resulting in a lack of compre-\nhensive explorations on the model capabilities for practical\ndemands. As shown in Figure 1, for tasks such as time series\nclassification (Franceschi et al., 2019) and anomaly detec-\ntion (Xu et al., 2021), multifaceted patterns in the context,\nsuch as bidirectional temporal dependencies, variate-centric\nrepresentations, and mutual correlations between multiple\nvariates, can outweigh causal dependencies and local vari-\nations emphasized in forecasting. It underscores a generic\nrepresentation learning capability from multi-granularity\nstructures in multivariate time series. We collectively refer\nto this paradigm as time series understanding.\nFoundation models (Radford et al., 2018; Dosovitskiy et al.,\n2020) have advanced significantly in generalization perfor-\nmance, making them a promising solution for data-scarce\nand task-agnostic applications. While prevailing GPT-style\nmodels excel in generative tasks like time series forecasting\n(Das et al., 2023b; Liu et al., 2024d), they lack the ability to\nleverage bidirectional context, causing a critical bottleneck\nfor global understanding. By contrast, BERT (Devlin et al.,\n2018) has exhibited task-versatility in natural language un-\n1\narXiv:2502.21245v1  [cs.LG]  28 Feb 2025\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nNatural Language\nToken\nSentence\nDocument\nMultivariate Time Series\nVariate\nVariates\nTime\nPatch\nPatch\nPatch\nPatch\nMulti-granularity\nStructure\nThe introduction of the new public library has had a\nsignificant impact on the local community. It provides\na quiet and comfortable space for individuals of all\nages to access a wide range of resources. The library\nis well-stocked with books, magazines and supplies,\nwhich meets the needs of residents.\nFigure 2. A multivariate time series is worth a natural language document. We propose to fully repurpose BERT for learning structured\nrepresentations of time series. The representations embodying different granularities can facilitate diverse time series understanding tasks.\nderstanding like sentiment classification and entity identifi-\ncation. In addition to the primary objective of masked lan-\nguage modeling (MLM), BERT facilitates functional tokens\n[CLS] and [SEP] to enable modeling multi-granularity\nstructures from multisentence text documents, in compli-\nance with the auxiliary task of next-sentence prediction\n(NSP) to reason about sentence-wise relationships. More-\nover, BERT has popularized the pre-training and fine-tuning\nparadigm, which can facilitate a broader range of distinct\ndownstream tasks, which has however not been unleashed\nin large-scale pre-trained time series foundation models.\nAs illustrated in Figure 2, time series share surprising struc-\ntural similarities with natural language. In addition to the\nprevalent practice of regarding a patch as a token (Nie et al.,\n2023), we observe that a sufficiently long context of time\nseries contains rich semantics to reveal variate-wise charac-\nteristics of time series, which possesses a similar structural\ncorrespondence with a sentence in natural language. On\nthis basis, we highlight that “a multivariate time series is\nworth a multisentence text document,” allowing us to inherit\nBERT, a general-purpose language representation learning\nframework with remarkable contextual awareness and multi-\ngranularity capabilities for document-like data structures, to\nextract generic representations from heterogeneousness time\nseries. The outcome pre-trained model manifested as a gen-\neral feature extractor of time series, can facilitate a variety\nof understanding tasks. Nevertheless, while BERT adopts a\nfixed number of sentences with a fixed length for NSP, mul-\ntivariate series can vary in both temporal length and variate\nnumber. It is required to implement a unified embedding\non BERT, which is to adhere to large-scale pre-training and\nadaptation to various understanding tasks.\nInspired by the above motivations, we propose TimesBERT,\na pre-trained foundation model for time series understand-\ning. We conduct large-scale pre-training on 260 billion time\npoints collected from multiple domains. We devise a unified\nembedding mechanism and repurpose the functional tokens\nin BERT to align with multivariate time series. In contrast\nto the previous methods that prevalently employed Channel-\nIndependence (Nie et al., 2023), in accordance with our\nestablished framework for interpreting multivariate time\nseries as analogous to documents, we implement the pre-\ntraining of any-variate and any-length time series to handle\nthe discrepancies in variate- and sentence-wise modeling,\nwhich reserves inherently structured representations of time\nseries such as variate correlation. TimesBERT achieves sig-\nnificant improvement on four typical understanding tasks\nand 113 real-world datasets compared with state-of-the-art\ntask-specifc models, exhibiting outstanding transferability.\nOur contributions can be summarized as follows:\n• We rethink the common appeal of representation learn-\ning for time series understanding, and propose to treat\nmultivariate time series as multisentence documents, re-\nvealing the advantages of BERT as a pre-trained model.\n• We develop TimesBERT, which consists of a unified\nstructured embedding and a functional token prediction\ntask toward the multi-granularity structure of multivari-\nate time series, fully aligning BERT to time series.\n• We pre-train our model on large-scale dataset with 260\nbillion time points, which can be adapted with state-of-\nthe-art results on time series classification, imputation,\nanomaly detection, and short-term forecasting tasks.\n2. Related Works\n2.1. Time Series Understanding\nTime series understanding includes a series of tasks that\nrequire structured representations and semantic extraction.\nClassical time series understanding methods such as Dy-\nnamic Time Warping (Berndt & Clifford, 1994) and Iso-\nlation Forests (Bandaragoda et al., 2018) make use of\nstatistical-based representations to identify temporal mo-\ntifs. Subsequent works (Wu et al., 2023; donghao & wang\nxue, 2024) based on CNN backbones preliminarily exhibit\nthe ability of deep learning-based models in time series un-\nderstanding. Prevailing Transformer-based models (Zhou\net al., 2021b; Wu et al., 2021; Nie et al., 2023; Liu et al.,\n2024a) apply attention mechanisms to discover potential cor-\nrelations among different granularities. However, most deep\nlearning models are originally designed for forecasting tasks\nwith insufficient adaptation for time series understanding.\n2\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\n1\n2\nTransformer (Encoder-Only)\nC\n1\n[MASK]\n3\nSentences\n3’\n2’\nTransformer (Encoder-Only)\nVariates\n1\n2\n[DOM]\n1\n3\ng\n3’\nV2\nV3\nMasked Language Modeling\nNext Sentence Prediction\nFunctional Token Prediction\nBERT\nTimesBERT\nToken Embedding\nTime Seires Embedding\nPre-training\nTasks\nBackbone\nEmbedding\nGPT\nTransformer (Decoder-Only)\nToken Embedding\n1\n2\n4\n4\n[CLS]\n[SEP]\n[SEP]\n3\n4\n2’\n3’\n4’\n5’\nNext Token Prediction\n1\n3\n2’\n2’\n1’\n3’\n1’\n2’\n1’\n2’\n3’\n1’\n4’\n4’\n3’\n1’\nV1\n2\nMasked Patch Modeling\n[MASK]\n[MASK]\n[MASK]\n[VAR]\n[VAR]\n[VAR]\nFigure 3. Comparison between GPT (Radford et al., 2018), BERT (Devlin et al., 2018), and TimesBERT on embedding, backbone, and\ntraining objective. In contrast to BERT’s sentence pair formulation, we implement an embedding approach for data with an arbitrary\nnumber of variates and design corresponding functional tokens to accommodate the inherent irregularity of time series variates.\n2.2. BERT-Style Models\nDeveloped for natural language processing, BERT (Devlin\net al., 2018) conducted pioneer work in highlighting the sig-\nnificance of bidirectional information for data comprehen-\nsion and demonstrates the effectiveness of the pre-training\nfine-tuning paradigm. More significantly, BERT introduces\na structured and generic view to analyzing words, sentences,\nand documents, as opposed to simply considering natural\nlanguage as entirely serialized entities.\nWhile subsequent models extended capabilities of BERT for\nnatural language (Liu, 2019; Lan, 2019), BERT-style models\nalso exhibit wide-ranging effectiveness in other data modali-\nties. MAE (He et al., 2022) employs an asymmetric encoder-\ndecoder structure within the framework of masked modeling,\nachieving substantial pre-training improvements in image\nclassification tasks. BEiT (Bao et al., 2021) employs VQ-\nVAE (Van Den Oord et al., 2017) to convert images into\ncorresponding discrete semantic representations. Notably,\nmodels like T5 (Raffel et al., 2020) and GPT-3 (Brown et al.,\n2020) are the counterpart of BERT-style architecture, with\nencoder-decoder T5 unifying tasks into a text-to-text frame-\nwork and decoder-only GPT-3 leveraging massive scale for\ngenerative modeling, pushing the limits of scalable models.\n2.3. Pre-trained Time Series Models\nPre-training methods in the field of time series have achieved\nadvancements in building task-specific and foundation mod-\nels. TST (Zerveas et al., 2021) and PatchTST (Nie et al.,\n2023) employ BERT-style masked pre-training at the point\nlevel and patch level, respectively. SimMTM (Dong et al.,\n2023) attempts to integrate neighbor data comparison with\nmasked point modeling. Free from respectively fine-tuning,\nTimesFM (Das et al., 2023b), Timer (Liu et al., 2024d;c),\nand Chronos (Ansari et al., 2024) exhibit advantages of zero-\nshot forecasting through large-scale pre-training. However,\nthese models primarily focus on forecasting-based tasks,\nlacking task versatility for distinct understanding tasks.\nThere have been several initial explorations on BERT-style\npre-trained models. MOMENT (Goswami et al., 2024)\nutilizes the T5 encoder for pre-training to achieve down-\nstream multi-task capabilities. Moirai (Woo et al., 2024)\nachieves multivariate embedding and employs masked mod-\neling by forecasting the future patches. VisionTS (Chen\net al., 2024) exhibits the robust transferability of vision-\nmasked autoencoders across different modalities. However,\nessential elements for structured representation learning in\nBERT are not fully leveraged. Therefore, we delve into\naligning time series with multisentence documents and next\nsentence prediction tasks, thus innovatively repurposing\nTimesBERT’s pre-training objective to present a versatile\npre-trained model for diverse understanding tasks.\n3. Approach\nTimesBERT employs an encoder-only Transformer to learn\nstructured representations of multivariate time series, which\nis aligned with BERT (Devlin et al., 2018) in both model ar-\nchitecture and objective design as shown in Figure 3. By pre-\ntraining on 260 billion time points from different domains,\nwe present a task-versatile foundation model, which can be\nfine-tuned for various time series understanding tasks.\n3.1. TimesBERT\nTransformers (Vaswani et al., 2017b) are currently the de\nfacto architecture of foundation models, especially for the\nvast use of GPT-style decoder-only Transformers in gener-\native tasks. However, the primary objective of generative\npre-trained models does not include learning bidirectional\n3\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nVariate k\nVariate 2\nVariate 1\nReplaced variate from another domain\nMultivar series\n…\n…\nz01,1\nz02,1\nz01,C\nz02,C\nz0N,C\nz[MASK]\nz[VAR]\nz[DOM]\n…\nz[MASK]\nz[VAR]\nz[VAR]\nTransformer (Encoder-Only)\nzg\n…\n…\n…\nzl1,2\nzl1,2\nzl1,1\nzl1,1\nzl1,C\nzl2,C\nzlN,C\nzl1,1\nzv1\nzv3\nzg\nTask 1: Masked Patch Modeling\nMask-Patch\nHead: Wm\n…\nz1,1\nz1,2\nP(\n): 0.15\nP(\n): 0.65\nP(\n): 0.05\nDomain Classifier:\nWd\n…\nVariate\nDiscriminator : Wv\nTask 2: Functional Token Prediction\nzv1\nzv3\nzg\nReconstructed\nMask Token\nProcessed\nFunctional Token\nFunctional\nToken\nz1,1\nz[DOM]\nz01,2\nz0N,2\nzl1,2\nzv2\nzv2\nzv2\nPE:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nFigure 4. Illustration of the TimesBERT architecture and pre-training objectives. The input multivariate time series is embedded into a\ntoken sequence for the transformer encoder following a unified time series embedding process that includes patching, function token\ninsertion, and flattening. Following the output from the backbone, the reconstructed patches and functional tokens are respectively fed\ninto corresponding pre-training tasks including MPM and FTP, ultimately forming the joint optimization objective.\nrepresentations at different granularities. Thus, we adopt\nthe BERT-style encoder-only architecture as a representa-\ntion learning backbone. To cope with variable lengths in\ntime points and variates, we design a unified time series\nembedding for structured representation learning.\n3.1.1. TIME SERIES EMBEDDING\nGiven multivariate time series X = [x1, x2, ..., xC] of C\nvariates, each variate xi ∈RT is a length-T time series that\nwill be divided into N = ⌈T\nP ⌉patches of patch length P:\npi,c = [X(i−1)P +1,c, . . . , XiP,c],\nz0\ni,c = Winpi,c + PEi+P c,\n(1)\nwhere Win ∈RD×P is a linear layer, and PEi+P c ∈RD\ndenotes absolute position embedding. We adopt a shared\nlearnable embedding z[MASK] in each masked position.\nWe repurpose the functional token [SEP], which is used for\nnext-sentence prediction in BERT, as a learnable embedding\nz[VAR]. Inspired by the functional token [CLS] used for\nclassifying a group of sentences in BERT, we also append a\nlearnable embedding z[DOM] at the beginning as the domain\ntoken. We formulate Z0 as the input of Transformer by\nstacking (N + 1)C + 1 token embeddings:\nZ0 =\n\n\n\n\n\nz[DOM]\nz0\n1:1\nz0\n2:1\n...\nz[MASK]\nz[VAR]\n·\nz0\n1:2\nz[MASK]\n· · ·\nz0\nN,2\nz[VAR]\n·\n...\n...\n...\n...\n...\n·\nz0\n1:C\nz0\n2:C\n· · ·\nz0\nN,C\nz[VAR]\n\n\n\n\n.\n(2)\nWe implement packing (Raffel et al., 2020) to simultane-\nously train on different multivariate time series in one large\ncontext length (512 tokens in TimesBERT). In addition to\naggregating global information, the functional token helps\nseparate different training samples.\nWe adopt an encoder-only Transformer with dimensions\nD and L layers as the backbone of TimesBERT, which\nforwards the flattened token embeddings:\nZl = TrmBlock(Zl−1), l = 1, . . . , L.\n(3)\nAs shown in Figure 4, we extend the 1D format of word\nsequences in BERT to accommodate multivariate time se-\nries with arbitrary variates and time points. In conjunction\nwith the pre-training tasks designed, patterns at the patch\nlevel, variate level, and sample level are aggregated on the\ncorresponding functional tokens, which ultimately forms\nmulti-granularity representation extraction.\n3.2. Pre-training TimesBERT\nWe design two pre-training objectives for structured time\nseries to acquire a generic understanding.\n3.2.1. TASK #1: MASKED PATCH MODELING\nInspired by the masked language modeling task utilized\nin BERT, we employ masked patch modeling (MPM) to\nprovide a pedestal understanding ability for the foundation\nmodel. Given the input token sequence, we adopt a masked\nratio α = 25% for non-functional tokens. To minimize\nthe discrepancy between pre-training and fine-tuning tasks,\n4\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nthese selected masked tokens are then actually replaced by\nz[MASK] with a 90% probability. Here let zi denote recon-\nstructed token at position i, let S = αNC denote the total\nnumber of masked patches, and we denote reconstructed\npatches as {bpi}S\ni=1. We use a linear layer Wout ∈RD×P\nto project tokens to reconstructed patches:\n{bpi}S\ni=1 = {zi}S\ni=1Wout.\n(4)\nGiven the ground truth of masked patches as {pi}S\ni=1, the\nmasked patch modeling objective is formulated as:\nLMPM =\n1\nSP\nS\nX\ni=1\n||pi −bpi||2\n2.\n(5)\nEquation 5 enhances the basic model capability to extract\ntemporal representations from local variations. Neverthe-\nless, ablation studies 4.5 indicate that MPM alone is insuffi-\ncient to provide optimal transfer ability toward downstream\ntasks. For tasks requiring explicit understanding of global\nrepresentations, such as classification and anomaly detec-\ntion, it is necessary to propose a pre-training task that better\naligns with the document-like structure of time series data.\n3.2.2. TASK #2: FUNCTIONAL TOKEN PREDICTION\nDespite the goal of masked patch modeling to model tempo-\nral patterns in a single time series, it struggles to explicitly\nhandle inter-variate relationships and effectively aggregate\nthe overall characteristics of variates. Inspired by next sen-\ntence prediction (NSP) task in BERT, we propose Func-\ntional Token Prediction (FTP) relying on special tokens.\nWe design a variate discrimination task. Given a multivariate\ntime series with C ≥2, we randomly replace one variate\nwith another from another dataset. The task of the model is\nto identify the replaced variate by its own variation patterns.\nHere let zvc denote the output of z[VAR] of n-th variate, and\nwe project zvc with a linear layer WVAR ∈RD×2 to classify\nwhether a variate originated the same as other variates.\nHere let zg denote the output of z[DOM]. Based on the do-\nmain token zg, we propose a domain classification task.\nWith M datasets indexed in pre-training, the backbone pro-\nvides outputs, and zg is fed into a linear layer WDOM ∈\nRD×M to predict the dataset index of the series.\nBased on the aforementioned process, the functional token\nprediction objective can be formulated as follows:\nLFTP = −\nC\nX\nc=1\n2\nX\ni=1\nyc\ni log(zvcWVAR)i −\nM\nX\ni=1\nyd\ni log(zgWDOM)i,\n(6)\nwhere the one-hot vector yc denotes labels marking whether\nvc is the replaced variate, and the one-hot vector yd denotes\nthe index of the dataset.\nFinally, the training objective is represented as follows:\nL = LMPM + LFTP.\n(7)\nOur functional token prediction task treats each variate as\na time series sentence, requiring them to distribute and ag-\ngregate with one another to identify their similarities and\ndifferences in relation to the entire sequence. As functional\ntokens learn representations at varying granularities, they\nenhance task versatility during downstream adaptation, al-\nlowing a series of token embeddings to be employed for\nunderstanding time series patches, variables, and domains.\nFollowing the pre-training phase, the task head is removed,\nwhile the Transformer backbone is adapted for representa-\ntion extraction during fine-tuning. This process effectively\ndecouples the pre-trained backbone from the task design.\n3.3. Pre-Training Data\nWe construct large-scale time series corpora from various\nsources. We adopt the LOTSA dataset (Woo et al., 2024)\nas the main body of the pre-training dataset, taking into\naccount the needs of the basic model for multi-domain and\npattern diversity requirements. Simultaneously, there is a\nnotable discrepancy in data features between understanding-\noriented domains, such as medical (Gow et al., 2023), and\npre-training data used for forecasting, thus to account for\nthe varying temporal dynamics and variate correlations of\ntime series across different tasks, we incorporate the UEA\nArchive (Bagnall et al., 2018) to achieve a balanced data por-\ntrait, forming a large-scale corpus with a total of 260 billion\ntime points. Based on our structure-preserving design for\nmultivariate time series, TimesBERT fully leverages time\nseries native during large-scale pre-training to achieve rapid\nand effective transfer for complex time series tasks.\n3.4. Fine-Tuning TimesBERT\nAnalogous to the fine-tuning methodology employed with\nBERT, we adopt a trainable output layer during the fine-\ntuning phase of TimesBERT to accommodate various down-\nstream datasets. Considering the low er information density,\nwe utilize all tokens to ensure a comprehensive representa-\ntion when migrating to the classification, while tokens at\nthe corresponding positions are directly used as represen-\ntations for imputation and anomaly detection. For diverse\nunderstanding tasks, the fine-tuning paradigm of BERT-\nstyle models empirically exhibits advantages compared to\nthe prevailing zero-shot paradigm of GPT-style models.\n4. Experiments\nIn order to validate the capacity of TimesBERT for typical\nunderstanding scenarios, we conduct experiments in time\nseries classification, imputation, short-term forecasting, and\nanomaly detection tasks. We compare TimesBERT with\n5\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nstate-of-the-art task-specific and general models and exhibit\nthe benefit of pre-training. In addition, we conducted com-\nprehensive ablation studies to evaluate various aspects of\nthe model design and its capabilities. We provide implemen-\ntation details and model configurations in Appendix A.\nClassification\nUEA (Accuracy)\nClassification\nUCR (Accuracy)\nAnomaly Detection\n(F1-Score)\nShort-Term Forecasting\n(SMAPE)\nImputation\n(MSE)\n74.5\n85.6\n88.40\n11.648\n0.0817\n73.6\n80.0\n11.829\n0.1273\n76.8\n86.62\n0.0885\n67.5\n71.0\n82.46\n12.418\n0.2223\n72.5\n78.4\n84.82\n13.022\n0.1483\n84.34\n14.593\nTimesBERT (Ours)\nTimesNet\nModernTCN\nDLinear\nPatchTST\nMOMENT\nFigure 5. Overall Performance of TimesBERT.\n4.1. Classification\nSetups\nTime series classification represents a typical data\nunderstanding task. During the feature extraction process\nof the pre-trained model, capturing temporal patterns ne-\ncessitates that the classifier possesses robust global com-\nprehension capabilities. We utilize two benchmark datasets\nfor time series classification. Specifically, we employ 10\nsubsets from the UEA Archive (Bagnall et al., 2018) and 91\nsubsets from the UCR Archive (Dau et al., 2019), spanning\ndiverse domains such as biology, physics, environmental\nmonitoring, human activity recognition, and finance, among\nothers. These datasets encompass varying variate numbers\nand sequence lengths which exhibit diversity in granularity.\nResults\nFigure 6 illustrates the inference outcomes of our\nmodel on UEA and UCR Archive. Compared to statistical\nmethods, state-of-the-art deep learning models, and classical\nunsupervised representation learning methods, the average\nclassification accuracy of TimesBERT shows consistent per-\nformance enhancements. Notably, given the substantial\nvariations in sequence length, number of variates, change\npatterns, and class counts across existing time series classi-\nfication benchmarks, TimesBERT exhibits comprehensive\nimprovements across over one hundred benchmark datasets.\n4.2. Anomaly Detection\nSetups\nTime series anomaly detection is a widely dis-\ncussed task aimed at discerning anomalous data segments,\nwhich is of great importance in actual time series analy-\nsis. We include five widely-used anomaly detection bench-\nmarks into our experiments, namely: SMD (Su et al., 2019),\nMSL (Hundman et al., 2018), SMAP (Hundman et al.,\n2018), SWaT (Mathur & Tippenhauer, 2016), PSM (Abdu-\nlaal et al., 2021). Our evaluation follows the unsupervised\ntime series anomaly detection logic mentioned in previous\nworks such as TimesNet (Wu et al., 2023), where datasets\nare split into non-overlapping sliding windows, and the re-\nconstruction error is applied as the anomaly criterion.\nResults\nOur experiments result in table 1 shows that\nTimesBERT performs better than previous sota baselines\nsuch as TimesNet (Wu et al., 2023). We highlight that\nTimesBERT’s improvements are consistent across all time\nseries anomaly detection benchmarks, which demonstrates\nthe robust adaptability of pre-trained models to complex\ndownstream understanding tasks and diverse datasets.\n4.3. Imputation\nSetups\nGiven the pervasive occurrence of missing values\nin real-world industrial production scenarios, we evaluate\nthe effectiveness of time series imputation tasks, where\nbidirectional information is significantly important for en-\nhancing the model’s ability to analyze missing segments.\nAdditionally, this task necessitates that the model compre-\nhends and encapsulates the overall features of the series\nthrough high-order representations, revealing the advantages\nof TimesBERT. Since value missing in real scenarios often\noccurs in continuous segments, we employ patch-level im-\nputation for evaluation following Timer (Liu et al., 2024d),\nwhich is more challenging than point-level imputation.\nResults\nWe conduct comprehensive evaluations on se-\nlected six classical benchmarks with four different mask\nratios in order to avoid data leakage caused by pre-trained\ncorpus and compared them with advanced general and foun-\ndation models. As shown in Table 7, TimesBERT achieves a\n7.7% loss reduction compared to the state-of-the-art model\non this task. In addition, we test the benefits of model pre-\ntraining under the data scarcities of {5%, 20%, 100%}. As\nshown in Figure 8, the pre-trained model gives a significant\nimprovement with fewer fine-tuning samples.\n4.4. Short-Term Forecasting\nSetups\nShort-term time series forecasting is extensively\nutilized in domains such as meteorological prediction, mar-\nket analysis, and finance. Unlike long-term forecasting\ntasks, which depend on capturing consistent local change\npatterns within the retrospective window and require robust\nmodel roll-out capabilities, short-term forecasting empha-\nsizes providing trend predictions for the forecast horizon\nbased on the overall characteristics of the series. Conse-\nquently, it is more suitable for models with understanding\ncapabilities. For this task, we employ the M4 dataset (Spy-\n6\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nLSTNet\nLSSL\nRocket\nSCINet\nMICN\nTimesNet\nModernTCN\nDLinear\nLightTS\nMTS-Mixer\nRMLP\nR-Linear\nTS2Vec\nFED.\nFlow.\nCross.\nPatchTST\niTransformer\nMOMENT\nTimesBERT\nAverage Accuracy (%)\n71.8\n70.9\n72.5\n71.7\n70.0\n73.6\n74.2\n67.5\n70.470.9\n70.0\n70.6\n69.6\n70.7\n73.073.2\n72.5\n70.5\n72.2\n74.5\n(a) UEA Archive Results\nDTW\nTS2Vec\nT-Loss\nTNC\nTS-TCC\nTST\nCNN\nEncoder\nFCN\nResNet\nMCDNN\nTimesNet\nModernTCN\nt-LeNet\nTWIESN\nMLP\nDLinear\nPatchTST\nMOMENT\nGPT4TS\nTimesBERT\nAverage Accuracy (%)\n76.4\n85.2\n83.4\n79.379.3\n65.9\n75.274.3\n80.9\n82.5\n70.2\n80.0\n76.8\n34.8\n72.7\n75.1\n71.0\n78.4\n85.3\n56.7\n85.6\n(b) UCR Archive Results\nRNN\nCNN\nMLP\nTransformer\nRepresentation\nStatistics Method\nOurs\nFigure 6. Averaged results of classification task on UEA and UCR Archive. We illustrate an extensive range of analytical baselines, among\nwhich TimesBERT achieved state-of-the-art performance in two classification benchmarks. The results of previous methods are directly\ntaken down from previous literature for fair comparison. See Table 5 and Table 6 for full results.\nTable 1. Anomaly detection results. We calculate the F1-score (as %) for each dataset. *. means the *former. A higher value of the\nF1-score indicates a better performance. See Table 12 for full results.\nMODEL\nTIMESBERT\nMTCN\nTIMESNET\nETS.\nFED.\nLIGHTTS\nDLINEAR\nNS.\nAUTO.\nPYRA.\nANOMALY.\nINFORMER\nREFORMER\nLOGTRANS\nTRANS.\n(OURS)\n(2024)\n(2023)\n(2022)\n(2022)\n(2022)\n(2023A)\n(2022)\n(2021)\n(2021)\n(2021)\n(2021A)\n(2020)\n(2019)\n(2017A)\nSMD\n86.04\n85.81\n85.81\n83.13 85.08\n82.53\n77.10\n84.72 85.11 83.04\n85.49\n81.65\n75.32\n76.21\n79.56\nMSL\n88.07\n84.92\n85.15\n85.03 78.57\n78.95\n84.88\n77.50 79.05 84.86\n83.31\n84.06\n84.40\n79.57\n78.68\nSMAP\n75.69\n71.26\n71.52\n69.50 70.76\n69.21\n69.26\n71.09 71.12 71.09\n71.18\n69.92\n70.40\n69.97\n69.70\nSWAT\n93.95\n93.86\n91.74\n84.91 93.19\n93.33\n87.52\n79.88 92.74 91.78\n83.10\n81.43\n82.80\n80.52\n80.37\nPSM\n98.27\n97.23\n97.47\n91.76 97.23\n97.15\n93.55\n97.29 93.29 82.08\n79.40\n77.10\n73.61\n76.74\n76.07\nAVG. F1\n88.40\n86.62\n86.34\n82.87 84.97\n84.23\n82.46\n82.08 84.26 82.57\n80.50\n78.83\n77.31\n76.60\n76.88\n0.08\n0.12\n0.16\n0.20\nAvg. Inference Time (ms)\nDLinear\nTimer\nMOMENT\nPatchTST\nTimesNet\nModernTCN\nTimesBERT\nOurs\nCNN\nMLP\nTransformer\nFigure 7. Averaged results of imputation tasks. We randomly mask\n{12.5%, 25%, 37.5%, 50%} patches. The results are averaged\nfrom 4 mask ratios. See Table 7 and Table 10 for full results.\nros Makridakis, 2018) as a benchmark. We adhere to the\nevaluation established by TimesNet (Wu et al., 2023).\nResults\nWe evaluate well-acknowledged forecasting\nmodels,\nincluding iTransformer (Liu et al., 2024a),\nPatchTST (Nie et al., 2023) and TimesNet (Wu et al., 2023),\nby three widely accepted metrics on the M4 dataset. As\nshown in Table 2, TimesBERT outperforms previous mod-\nels consistently on all average SMAPE and OWA.\n4.5. Model Analysis\nPre-training Tasks\nDuring the pre-training phase, we\nemploy two distinct pre-training tasks and concurrently\noptimize three task-specific heads, among which there is\nETTh1\nETTh2\nETTm1\nETTm2\nECL\nWeather\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\nReduced MSE by Pre-training\n10%\n29%\n23%\n22%\n30%\n6%\n40%\n52%\n73%\n53%\n50%\n12%\n38%\n47%\n77%\n59%\n46%\n37%\n100% Samples\n20% Samples\n5% Samples\nFigure 8. Pre-training benefit of TimesBERT on the downstream\nimputation task with 100%, 20% and 5% available samples. Each\ndataset is imputed with four mask ratios and we calculate the\naverage reduced imputation error in MSE relative to training from\nscratch. See Table 7, Table 8 and Table 9 for full results.\nexplicit complementary relation. We conduct an ablation\nstudy on the tasks and compare in detail the performance\nof pre-trained TimesBERT with and without the functional\ntoken prediction task before transferring to time series clas-\nsification tasks. On this basis, we examine the impact of\nfunctional token selection during the fine-tuning stage.\nAs shown in Table 3, on average, all our pre-training tasks\nand functional tokens while fine-tuning yield positive im-\nprovements. Notably, the inclusion of functional tokens\nprovides a significant boost to classification performance,\nconfirming their ability to aggregate relevant representation.\n7\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nTable 2. Average short-term forecasting results on the M4 (Spyros Makridakis, 2018). Full results are provided in Table 11.\nMETHOD TIMESBERT MTCN TIMESNET\nITRANS. KOOPA\nNHITS DLINEAR PATCHTST\nMICN\nTIDE\nMOMENT NBEATS\n(OURS)\n(2023)\n(2024)\n(2024A) (2023)\n(2023)\n(2023B)\n(2023)\n(2022) (2023A)\n(2024)\n(2019)\nAVERAGE\nSMAPE\n11.648\n11.698\n11.829\n12.684 11.863 11.960\n12.418\n13.022\n13.023 13.950\n14.593\n11.910\nMASE\n1.560\n1.556\n1.585\n1.764\n1.595\n1.606\n1.656\n1.814\n1.836\n1.940\n2.161\n1.613\nOWA\n0.837\n0.838\n0.851\n0.929\n0.858\n0.861\n0.891\n0.954\n0.960\n1.020\n1.103\n0.862\nTable 3. Ablation of pre-training tasks. D in the “pre-train” row\nmeans using domain classification, and V means using variate\ndiscrimination; In the “fine-tuning” row, D means z[DOM] is used\nfor the task head while fine-tuning, and V means z[VAR] is used.\nPRE-TRAIN\nD+V\nD\nNONE\nFINE-TUNE\nD+V\nD\nNONE\nD\nNONE\nEC\n34.60\n34.22\n33.46\n30.04\n32.32\nFD\n68.67\n68.67\n69.44\n68.93\n69.55\nHW\n36.59\n36.59\n36.12\n35.76\n35.18\nHB\n78.54\n78.54\n77.56\n78.05\n78.54\nJV\n97.57\n97.57\n98.38\n98.38\n98.38\nSRS1\n93.17\n93.17\n93.52\n91.47\n90.78\nSRS2\n58.33\n58.33\n57.22\n59.44\n57.22\nSWJ\n99.41\n99.41\n99.50\n99.27\n99.32\nSW\n95.00\n95.00\n95.31\n94.38\n93.44\nAVG.\n73.54\n73.50\n73.39\n72.86\n72.75\nInitialization\nRecent studies on large language models\nand vision models for time series (Zhou et al., 2023; Liu\net al., 2024b; Chen et al., 2024) have demonstrated the ad-\nvantages of leveraging pre-trained models on other modali-\nties for time series modeling to some extent, while we posit\nthat time series exhibit more complex intrinsic variation\npatterns. Therefore, in this ablation study, we attempt to\ndirectly initialize TimesBERT using a pre-trained BERT\nmodel and compare its performance with that of a normal\npre-trained TimesBERT on classification tasks.\nFigure 9 illustrates the inclusion of different initialization\nmethods alongside a random initialization as a control group,\nwhich highlights the fundamental difference among linguis-\ntic manifolds, image semantic space, and time series, while\nalso demonstrating the significant improvements achieved\nby our pre-trained models. These findings underscore the\nimportance of native time-series pre-training.\nMultivariate Modeling\nTime series understanding tasks\ntypically require robust multivariate modeling capabilities,\nin which capturing multivariate correlations is often essen-\ntial for accurately representing the overall data features.\nNevertheless, existing foundation model designs often em-\nploy Channel Independence (CI) to avoid interference from\nvariate modeling (Liu et al., 2024d; Ansari et al., 2024). We\nconduct ablation studies on classification and imputation\ntasks utilizing datasets with explicit multivariate features.\nFigure 10 indicates that in both tasks, compared to employ-\n40\n45\n50\n55\n60\n65\n70\n75\nAverage Accuracy (%)\nMAE\nBERT\nRandom\nTimesBERT\nInitialization\n48.2\n67.8\n72.8\n74.6\nClassification\nMAE\nBERT\nRandom\nTimesBERT\nFigure 9. We initialize TimesBERT for downstream tasks using\nthree methods: (1) pre-trained TimesBERT model, (2) random\ninitialization, (3) A Vision Transformer (Dosovitskiy et al., 2020)\npre-trained with Masked Autoencoder (He et al., 2022), and (4) a\nBERT (Devlin et al., 2018) model pre-trained on language.\n73.0\n73.5\n74.0\n74.5\n75.0\nAverage Accuracy (%)\n74.6\n74.1\nClassification\nETTh1\nETTh2\nETTm1\nETTm2\nWeather\n0.0\n0.1\n0.2\n0.3\nMSE\n0.117\n0.094\n0.074\n0.052\n0.059\n0.270\n0.156\n0.186\n0.093\n0.082\nImputation\nTimesBERT\nTimesBERT (CI)\nFigure 10. Ablation of TimesBERT’s multivariate capabilities on\nclassification and imputation tasks. CI indicates that the model\nuses the Channel Independence strategy (Nie et al., 2023).\ning CI to mitigate the interference from variate relationships,\nTimesBERT consistently leverages the variate correlations\nto achieve benefits while supporting multivariate inputs.\n5. Conclusion and Future Work\nIn this paper, we highlight that multivariate time series\nand multisentence text documents exhibit a similar multi-\ngranularity structure. Inspired by BERT which facilitates\nstructured representation learning for agnostic downstream\ntasks, we leverage BERT-style architectures for generic time\nseries understanding, which is achieved by repurposing\nmasked modeling and functional token prediction for ar-\nbitrary multivariate time series. By large-scale pre-training\non 260 billion time points across different domains, Times-\nBERT surpasses state-of-the-art models across four typical\nunderstanding tasks, which validates the exceptional gener-\nalization capabilities that BERT can also offer in the realm\nof time series analysis. We will explore the adaption of func-\ntional tokens and delve into domain-universal pre-training\nfor more time series understanding tasks.\n8\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nReferences\nAbdulaal, A., Liu, Z., and Lancewicki, T. Practical ap-\nproach to asynchronous multivariate time series anomaly\ndetection and localization. KDD, 2021.\nAnsari, A. F., Stella, L., Turkmen, C., Zhang, X., Mercado,\nP., Shen, H., Shchur, O., Rangapuram, S. S., Arango, S. P.,\nKapoor, S., et al. Chronos: Learning the language of time\nseries. arXiv preprint arXiv:2403.07815, 2024.\nBagnall, A. J., Dau, H. A., Lines, J., Flynn, M., Large,\nJ., Bostrom, A. G., Southam, P., and Keogh, E. J. The\nuea multivariate time series classification archive, 2018.\narXiv preprint arXiv:1811.00075, 2018.\nBandaragoda, T. R., Ting, K. M., Albrecht, D., Liu, F. T.,\nZhu, Y., and Wells, J. R. Isolation-based anomaly detec-\ntion using nearest-neighbor ensembles. Computational\nIntelligence, 34(4):968–998, 2018.\nBao, H., Dong, L., Piao, S., and Wei, F.\nBeit: Bert\npre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021.\nBerndt, D. J. and Clifford, J. Using dynamic time warping\nto find patterns in time series. In KDD Workshop, 1994.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877–1901, 2020.\nChallu, C., Olivares, K. G., Oreshkin, B. N., Ramirez, F. G.,\nCanseco, M. M., and Dubrawski, A. Nhits: Neural hi-\nerarchical interpolation for time series forecasting. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 37, pp. 6989–6997, 2023.\nChen, M., Shen, L., Li, Z., Wang, X. J., Sun, J., and\nLiu, C. Visionts: Visual masked autoencoders are free-\nlunch zero-shot time series forecasters. arXiv preprint\narXiv:2408.17253, 2024.\nChen, T. and Guestrin, C. Xgboost: A scalable tree boosting\nsystem. KDD, 2016.\nDas, A., Kong, W., Leach, A., Sen, R., and Yu, R. Long-\nterm forecasting with tide: Time-series dense encoder.\narXiv preprint arXiv:2304.08424, 2023a.\nDas, A., Kong, W., Sen, R., and Zhou, Y.\nA decoder-\nonly foundation model for time-series forecasting. arXiv\npreprint arXiv:2310.10688, 2023b.\nDau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu,\nY., Gharghabi, S., Ratanamahatana, C. A., and Keogh,\nE. The ucr time series archive. IEEE/CAA Journal of\nAutomatica Sinica, 6(6):1293–1305, 2019.\nDempster, A., Petitjean, F., and Webb, G. I. Rocket: excep-\ntionally fast and accurate time series classification using\nrandom convolutional kernels. Data Min. Knowl. Discov.,\n2020.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., and\nLong, M.\nSimmtm:\nA simple pre-training frame-\nwork for masked time-series modeling. arXiv preprint\narXiv:2302.00861, 2023.\ndonghao, L. and wang xue. ModernTCN: A modern pure\nconvolution structure for general time series analysis.\nIn The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.\nnet/forum?id=vpJMJerXHU.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\nFranceschi, J.-Y., Dieuleveut, A., and Jaggi, M. Unsuper-\nvised scalable representation learning for multivariate\ntime series. In NeurIPS, 2019.\nGoswami, M., Szafer, K., Choudhry, A., Cai, Y., Li, S., and\nDubrawski, A. Moment: A family of open time-series\nfoundation models. arXiv preprint arXiv:2402.03885,\n2024.\nGow, B., Pollard, T., Nathanson, L. A., Johnson, A.,\nMoody, B., Fernandes, C., Greenbaum, N., Berkowitz,\nS., Moukheiber, D., Eslami, P., et al.\nMimic-iv-ecg-\ndiagnostic electrocardiogram matched subset.\nType:\ndataset, 2023.\nGu, A., Goel, K., and R´e, C. Efficiently modeling long\nsequences with structured state spaces. In ICLR, 2022.\nHe, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Girshick,\nR. Masked autoencoders are scalable vision learners. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 16000–16009, 2022.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural Comput., 1997.\nHundman, K., Constantinou, V., Laporte, C., Colwell, I.,\nand S¨oderstr¨om, T. Detecting spacecraft anomalies using\nlstms and nonparametric dynamic thresholding. KDD,\n2018.\n9\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer: The\nefficient transformer. In ICLR, 2020.\nLai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling\nlong-and short-term temporal patterns with deep neural\nnetworks. In SIGIR, 2018.\nLan, Z. Albert: A lite bert for self-supervised learning of lan-\nguage representations. arXiv preprint arXiv:1909.11942,\n2019.\nLi, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X.,\nand Yan, X. Enhancing the locality and breaking the mem-\nory bottleneck of transformer on time series forecasting.\nIn NeurIPS, 2019.\nLiu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dust-\ndar, S. Pyraformer: Low-complexity pyramidal attention\nfor long-range time series modeling and forecasting. In\nICLR, 2021.\nLiu, Y. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 364, 2019.\nLiu, Y., Wu, H., Wang, J., and Long, M. Non-stationary\ntransformers: Rethinking the stationarity in time series\nforecasting. In NeurIPS, 2022.\nLiu, Y., Li, C., Wang, J., and Long, M. Koopa: Learning non-\nstationary time series dynamics with koopman predictors.\narXiv preprint arXiv:2305.18803, 2023.\nLiu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L.,\nand Long, M. itransformer: Inverted transformers are\neffective for time series forecasting.\narXiv preprint\narXiv:2310.06625, 2024a.\nLiu, Y., Qin, G., Huang, X., Wang, J., and Long, M. Auto-\ntimes: Autoregressive time series forecasters via large lan-\nguage models. arXiv preprint arXiv:2402.02370, 2024b.\nLiu, Y., Qin, G., Huang, X., Wang, J., and Long, M. Timer-\nxl: Long-context transformers for unified time series fore-\ncasting. arXiv preprint arXiv:2410.04803, 2024c.\nLiu, Y., Zhang, H., Li, C., Huang, X., Wang, J., and Long,\nM. Timer: Transformers for time series analysis at scale.\narXiv preprint arXiv:2402.02368, 2024d.\nMathur, A. P. and Tippenhauer, N. O. Swat: a water treat-\nment testbed for research and training on ICS security. In\nCySWATER, 2016.\nNie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A\ntime series is worth 64 words: Long-term forecasting with\ntransformers. arXiv preprint arXiv:2211.14730, 2023.\nOreshkin, B. N., Carpov, D., Chapados, N., and Bengio,\nY. N-BEATS: Neural basis expansion analysis for inter-\npretable time series forecasting. ICLR, 2019.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., K¨opf, A., Yang, E., DeVito, Z., Rai-\nson, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,\nL., Bai, J., and Chintala, S. Pytorch: An imperative\nstyle, high-performance deep learning library. In NeurIPS,\n2019.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485–5551, 2020.\nSpyros\nMakridakis.\nM4\ndataset,\n2018.\nURL\nhttps://github.com/M4Competition/\nM4-methods/tree/master/Dataset.\nSu, Y., Zhao, Y., Niu, C., Liu, R., Sun, W., and Pei, D.\nRobust anomaly detection for multivariate time series\nthrough stochastic recurrent neural network. KDD, 2019.\nVan Den Oord, A., Vinyals, O., et al. Neural discrete rep-\nresentation learning. Advances in neural information\nprocessing systems, 30, 2017.\nVan der Maaten, L. and Hinton, G. Visualizing data using\nt-sne. Journal of machine learning research, 9(11), 2008.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In NeurIPS, 2017a.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017b.\nWang, H., Peng, J., Huang, F., Wang, J., Chen, J., and Xiao,\nY. Micn: Multi-scale local and global context model-\ning for long-term series forecasting. In The Eleventh\nInternational Conference on Learning Representations,\n2022.\nWang, Y., Wu, H., Dong, J., Liu, Y., Long, M., and Wang, J.\nDeep time series models: A comprehensive survey and\nbenchmark. 2024.\nWoo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. C. H.\nEtsformer: Exponential smoothing transformers for time-\nseries forecasting.\narXiv preprint arXiv:2202.01381,\n2022.\n10\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nWoo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and\nSahoo, D. Unified training of universal time series fore-\ncasting transformers. arXiv preprint arXiv:2402.02592,\n2024.\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decom-\nposition transformers with auto-correlation for long-term\nseries forecasting. Advances in neural information pro-\ncessing systems, 34:22419–22430, 2021.\nWu, H., Wu, J., Xu, J., Wang, J., and Long, M. Flowformer:\nLinearizing transformers with conservation flows. In\nICML, 2022.\nWu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M.\nTimesnet: Temporal 2d-variation modeling for general\ntime series analysis. arXiv preprint arXiv:2210.02186,\n2023.\nXu, J., Wu, H., Wang, J., and Long, M. Anomaly trans-\nformer: Time series anomaly detection with association\ndiscrepancy. arXiv preprint arXiv:2110.02642, 2021.\nZeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers\neffective for time series forecasting? 2023a.\nZeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers\neffective for time series forecasting? In Proceedings of\nthe AAAI conference on artificial intelligence, volume 37,\npp. 11121–11128, 2023b.\nZerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A., and\nEickhoff, C. A transformer-based framework for multi-\nvariate time series representation learning. In Proceed-\nings of the 27th ACM SIGKDD conference on knowledge\ndiscovery & data mining, pp. 2114–2124, 2021.\nZhang, T., Zhang, Y., Cao, W., Bian, J., Yi, X., Zheng, S.,\nand Li, J. Less is more: Fast multivariate time series\nforecasting with light sampling-oriented mlp structures.\narXiv preprint arXiv:2207.01186, 2022.\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H.,\nand Zhang, W. Informer: Beyond efficient transformer for\nlong sequence time-series forecasting. In AAAI, 2021a.\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H.,\nand Zhang, W. Informer: Beyond efficient transformer for\nlong sequence time-series forecasting. In Proceedings of\nthe AAAI conference on artificial intelligence, volume 35,\npp. 11106–11115, 2021b.\nZhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin,\nR. FEDformer: Frequency enhanced decomposed trans-\nformer for long-term series forecasting. In ICML, 2022.\nZhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits\nall: Power general time series analysis by pretrained lm.\narXiv preprint arXiv:2302.11939, 2023.\n11\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nA. Implementation Details\nA.1. Pre-training\nAll experiments in this paper are implemented in PyTorch (Paszke et al., 2019) and conducted on NVIDIA 4090 GPU.\nWe used AdamW as the optimizer in the pre-training phase with β1 = 0.9, β2 = 0.99, and applied the cosine annealing\nalgorithm for learning rate decay. Specifically, we use 1 × 10−4 as the initial learning rate and 2 × 10−7 as the final learning\nrate. To conduct fair ablation experiments, all our pre-trained models are trained for 30000 steps on a large-scale time series\ncorpus, where the batch size is 40 × 8 = 320 and the context length of each sequence, that is, the number of patches, is 512.\nTo improve the convergence speed, we use the packing (Raffel et al., 2020) for large-scale pre-training.\nFor alignment with BERT (Devlin et al., 2018), our pre-trained model size is the same as BERTbase, i.e., we denote\nthe number of layers as L, the hidden size as H, and the number of self-attention heads as A, then the model size is\nL = 12, H = 768, A = 12, and the total number of parameters is 85.6M. Considering the diversity of downstream datasets,\nwe use models with different patch lengths on different tasks to adapt to diverse understanding tasks, including 36 for\nclassification, 24 for imputation, and 4 for short-term forecasting and anomaly detection tasks.\nA.2. Downstream Tasks\nTable 4. Dataset descriptions. The dataset size is organized in (Train, Validation, Test). * means that different subsets have different values\nTASKS\nDATASET\nVARIATE\nSERIES LENGTH\nDATASET SIZE\nINFORMATION (FREQUENCY)\nM4-YEARLY\n1\n6\n(23000, 0, 23000)\nDEMOGRAPHIC\nM4-QUARTERLY\n1\n8\n(24000, 0, 24000)\nFINANCE\nFORECASTING\nM4-MONTHLY\n1\n18\n(48000, 0, 48000)\nINDUSTRY\n(SHORT-TERM)\nM4-WEAKLY\n1\n13\n(359, 0, 359)\nMACRO\nM4-DAILY\n1\n14\n(4227, 0, 4227)\nMICRO\nM4-HOURLY\n1\n48\n(414, 0, 414)\nOTHER\nIMPUTATION\nETTM1, ETTM2\n7\n192\n(34465, 11521, 11521)\nELECTRICITY (15 MINS)\nETTH1, ETTH2\n7\n192\n(8545, 2881, 2881)\nELECTRICITY (15 MINS)\nELECTRICITY\n321\n192\n(18317, 2633, 5261)\nELECTRICITY (15 MINS)\nWEATHER\n21\n192\n(36792, 5271, 10540)\nWEATHER (10 MINS)\nETHANOLCONCENTRATION\n3\n1751\n(261, 0, 263)\nALCOHOL INDUSTRY\nFACEDETECTION\n144\n62\n(5890, 0, 3524)\nFACE (250HZ)\nHANDWRITING\n3\n152\n(150, 0, 850)\nHANDWRITING\nHEARTBEAT\n61\n405\n(204, 0, 205)\nHEART BEAT\nCLASSIFICATION\nJAPANESEVOWELS\n12\n29\n(270, 0, 370)\nVOICE\nPEMS-SF\n963\n144\n(267, 0, 173)\nTRANSPORTATION (DAILY)\nSELFREGULATIONSCP1\n6\n896\n(268, 0, 293)\nHEALTH (256HZ)\nSELFREGULATIONSCP2\n7\n1152\n(200, 0, 180)\nHEALTH (256HZ)\nSPOKENARABICDIGITS\n13\n93\n(6599, 0, 2199)\nVOICE (11025HZ)\nUWAVEGESTURELIBRARY\n3\n315\n(120, 0, 320)\nGESTURE\nUCR ARCHIVE\n1\n*\n(*, 0, *)\n*\nSMD\n38\n40\n(566724, 141681, 708420)\nSERVER MACHINE\nANOMALY\nMSL\n55\n40\n(44653, 11664, 73729)\nSPACECRAFT\nDETECTION\nSMAP\n25\n40\n(108146, 27037, 427617)\nSPACECRAFT\nSWAT\n51\n40\n(396000, 99000, 449919)\nINFRASTRUCTURE\nPSM\n25\n40\n(105984, 26497, 87841)\nSERVER MACHINE\nA.2.1. CLASSIFICATION\nThe time series classification task is a typical understanding task, which requires the model to make a holistic judgment based\non the global representation. It is worth mentioning that, different from the multivariate time series prediction task that has\nbeen discussed more, part of the variate information and variate correlation in the classification of time series samples may\n12\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nbe indispensable for accurate classification, so the multivariate modeling ability of the model is tested. In this experiment,\nwe choose the multivariate data set UEA Archive (Bagnall et al., 2018) and the univariate UCR dataset (Dau et al., 2019)\ntogether as the benchmark data set. For a fair comparison, we keep our evaluation aligned with Time-Series-Library (Wang\net al., 2024). See Table 4 for detailed information on the dataset.\nA.2.2. ANOMALY DETECTION\nMultivariate time series anomaly detection tasks normally inherit the characteristics of having various anomalous segment\nlengths. Some anomalous segments are within 10 data points while others may be up to hundreds of data points. To study a\nrepresentation that is capable of detecting subtle anomalies, we shrink the patch length to 4 during the large scale pertaining.\nCorrespondingly, we also reduce the hidden dimension to 256 and the attention layers to 4 to avoid over-fitting. During\ndownstream fine-tuning, we adopt a different input length of 40 compared to previous works such as TimesNet (Wu et al.,\n2023), corresponding to 10 input tokens. For a fair comparison, we keep our evaluation process aligned with previous works\nimplemented in the Time Series Library. See Table 4 for detailed details of the dataset.\nA.2.3. IMPUTATION\nThe multivariate time series imputation task we employ requires the model to impute against continuous missing values\nthat occur frequently in real scenarios. At the same time, our test uses the multivariate benchmark data set to test the\nability of the model to use the mutual hint between variates for collaborative imputation. Our benchmark datasets include\n(1) ETT (Zhou et al., 2021b) containing 7 variates of power transformers, (2) ECL (Wu et al., 2021) consisting of hourly\nelectricity consumption data from 321 customers (3) Weather (Wu et al., 2021) consisting of 21 meteorological variates. For\na fair comparison, we keep our evaluation process aligned with previous works implemented in Timer (Liu et al., 2024d).\nSee Table 4 for details of the dataset.\nA.2.4. SHORT-TERM FORECASTING\nCompared with the long-term forecasting of time series, the short-term forecasting of time series has a higher ability demand\nfor the model to capture the global trend and overall pattern of the data in the lookback window, which adapts to the\nmulti-granularity structure capture ability of TimesBERT. The experiment is carried out on the M4 (Spyros Makridakis,\n2018), which includes six univariate sequences with different change frequencies from various real scenes and can effectively\nprove the ability of the model to perform on data with different change patterns. For a fair comparison, we keep our\nevaluation aligned with previous works implemented in the Time-Series-Library. See Table 4 for details of the dataset.\nB. Representation Analysis\nTo verify the effectiveness of TimesBERT representation, we perform 2D t-SNE (Van der Maaten & Hinton, 2008)\nrepresentation visualization on many classification datasets in a zero-shot setting. In Figure 11, we present the example\nvisualization of “SpokenArabicDigits”, “UWaveGestureLibrary”, and “JapeneseVowels” from the UEA archive as well\nas “ElectricDevices”, “Crop”, “ECG5000”, “Wafer”, “ChlorineConcentration”, and “FacesUCR” from the UCR archive.\nSmall clusters of the same color in visualization suggest that samples from a specific class tend to have more similar\nhigh-dimensional representations. Such results show that TimesBERT is capable of acquiring distinct representations of\ndifferent classes after systematic pertaining without data-specific fine-tuning.\nC. Full Results\nDue to the limitation of space in the main text, we present detailed results of all time series understanding tasks in this\nsection, including time series classification, imputation, short-term forecasting, and anomaly detection.\nTime series classification results are as follows: full results of UEA classification in Table 5 and full results of UCR\nclassification in Table 6. Time series imputation results are as follows: imputation with 100% samples in Table 7, imputation\nwith 20% samples in Table 8, imputation with 5% samples in Table 9, and full results of imputation in Table 10. Full results\nof short-term forecasting in Table 11. Full results of anomaly detection in Table 12.\n13\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nFigure 11. The t-SNE representation visualization of pre-trained TimesBERT on examples from the UEA and UCR classification datasets.\nEach color in visualization stands for a specific target class in an example dataset.\nTable 5. Full results for the classification task on UEA Archive. ∗. in the Transformers indicates the name of ∗former. We report the\nclassification accuracy (%) as the result. The standard deviation is within 0.1%.\nDATASETS / MODELS\nCLASSICAL METHODS\nRNN\nTCN\nTRANSFORMERS\nMLP\nCNN\nPRE-TRAINED\nDTW\nXGBOOST ROCKET\nLSTM\nLSTNET\nLSSL\nTCN\nTRANS.\nRE.\nIN.\nPYRA.\nAUTO.\nSTATION.\nFED.\nETS.\nFLOW.\nDLINEAR LIGHTTS. TIMESNET\nMTCN\nMOMENT TIMESBERT\n(1994)\n(2016)\n(2020) (1997) (2018) (2022) (2019) (2017A) (2020) (2021A) (2021) (2021)\n(2022)\n(2022) (2022) (2022) (2023A)\n(2022)\n(2023)\n(2024)\n(2024)\n(OURS)\nETHANOLCONCENTRATION\n32.3\n43.7\n45.2\n32.3\n39.9\n31.1\n28.9\n32.7\n31.9\n31.6\n30.8\n31.6\n32.7\n31.2\n28.1\n33.8\n32.6\n29.7\n35.7\n36.3\n30.0\n34.6\nFACEDETECTION\n52.9\n63.3\n64.7\n57.7\n65.7\n66.7\n52.8\n67.3\n68.6\n67.0\n65.7\n68.4\n68.0\n66.0\n66.3\n67.6\n68.0\n67.5\n68.6\n70.8\n68.9\n68.6\nHANDWRITING\n28.6\n15.8\n58.8\n15.2\n25.8\n24.6\n53.3\n32.0\n27.4\n32.8\n29.4\n36.7\n31.6\n28.0\n32.5\n33.8\n27.0\n26.1\n32.1\n30.6\n35.1\n36.5\nHEARTBEAT\n71.7\n73.2\n75.6\n72.2\n77.1\n72.7\n75.6\n76.1\n77.1\n80.5\n75.6\n74.6\n73.7\n73.7\n71.2\n77.6\n75.1\n75.1\n78.0\n77.2\n73.7\n78.5\nJAPANESEVOWELS\n94.9\n86.5\n96.2\n79.7\n98.1\n98.4\n98.9\n98.7\n97.8\n98.9\n98.4\n96.2\n99.2\n98.4\n95.9\n98.9\n96.2\n96.2\n98.4\n98.8\n95.7\n97.5\nPEMS-SF\n71.1\n98.3\n75.1\n39.9\n86.7\n86.1\n68.8\n82.1\n82.7\n81.5\n83.2\n82.7\n87.3\n80.9\n86.0\n83.8\n75.1\n88.4\n89.6\n89.1\n85.5\n83.8\nSELFREGULATIONSCP1\n77.7\n84.6\n90.8\n68.9\n84.0\n90.8\n84.6\n92.2\n90.4\n90.1\n88.1\n84.0\n89.4\n88.7\n89.6\n92.5\n87.3\n89.8\n91.8\n93.4\n88.7\n93.1\nSELFREGULATIONSCP2\n53.9\n48.9\n53.3\n46.6\n52.8\n52.2\n55.6\n53.9\n56.7\n53.3\n53.3\n50.6\n57.2\n54.4\n55.0\n56.1\n50.5\n51.1\n57.2\n60.3\n55.0\n58.3\nSPOKENARABICDIGITS\n96.3\n69.6\n71.2\n31.9\n100.0 100.0\n95.6\n98.4\n97.0\n100.0\n99.6\n100.0\n100.0\n100.0 100.0\n98.8\n81.4\n100.0\n99.0\n98.7\n99.4\n99.4\nUWAVEGESTURELIBRARY\n90.3\n75.9\n94.4\n41.2\n87.8\n85.9\n88.4\n85.6\n85.6\n85.6\n83.4\n85.9\n87.5\n85.3\n85.0\n86.6\n82.1\n80.3\n85.3\n86.7\n90.0\n95.0\nAVERAGE ACCURACY\n67.0\n66.0\n72.5\n48.6\n71.8\n70.9\n70.3\n71.9\n71.5\n72.1\n70.8\n71.1\n72.7\n70.7\n71.0\n73.0\n67.5\n70.4\n73.6\n74.2\n72.2\n74.5\n14\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nD. Showcases\nIn this section, we provide the visualization results of TimesBERT on two downstream tasks of time series imputation and\nanomaly detection, corresponding to Figures 12 and 13, respectively.\nETTh1\nETTh2\nECL\nWeather\nETTm1\nETTm2\nFigure 12. Visualization of imputation results of TimesBERT on six benchmark datasets.\nHourly\nDaily\nMonthly\nQuarterly\nWeekly\nYearly\nFigure 13. Visualization of short-term forecasting results of TimesBERT on M4 dataset.\n15\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nTable 6. Full results of the classification task on UCR Archive (Dau et al., 2019).\nDATASETS\nDTW TS2VEC T-LOSS\nTNC\nTS-TCC\nTST\nCNN ENCODER\nFCN\nMCDNN RESNET TIMESNET MTCN T-LENET TWIESN MLP DLINEAR PATCHTST MOMENT GPT4TS TIMESBERT\nALLGESTUREWIIMOTEX\n71.6\n77.7\n76.3\n69.7\n69.7\n25.9\n41.1\n47.5\n71.3\n26.1\n74.1\n52.7\n49.1\n10.0\n52.2\n47.7\n31.7\n55.6\n71.7\n23.7\n64.7\nALLGESTUREWIIMOTEY\n72.9\n79.3\n72.6\n74.1\n74.1\n42.3\n47.9\n50.9\n78.4\n42.0\n79.4\n56.3\n50.1\n10.0\n60.0\n57.1\n34.1\n62.3\n76.7\n16.0\n72.1\nALLGESTUREWIIMOTEZ\n64.3\n74.6\n72.3\n68.9\n68.9\n44.7\n37.5\n39.6\n69.2\n28.7\n72.6\n51.1\n48.0\n10.0\n51.6\n43.9\n33.9\n52.7\n72.0\n11.6\n64.0\nARROWHEAD\n70.3\n85.7\n76.6\n73.7\n73.7\n77.1\n71.7\n63.0\n84.3\n67.8\n83.8\n78.9\n76.6\n30.3\n68.9\n78.4\n66.9\n66.3\n86.9\n42.9\n89.1\nBME\n90.0\n99.3\n99.3\n93.3\n93.3\n76.0\n94.7\n82.7\n83.6\n89.6\n99.9\n80.0\n66.7\n33.3\n81.9\n90.5\n70.0\n90.0\n90.0\n36.7\n100.0\nBEEF\n63.3\n76.7\n66.7\n60.0\n60.0\n50.0\n76.7\n70.7\n68.0\n50.7\n75.3\n95.0\n90.0\n20.0\n52.7\n71.3\n90.0\n73.3\n90.0\n16.7\n80.0\nBEETLEFLY\n70.0\n90.0\n80.0\n80.0\n80.0\n100.0 90.0\n62.0\n91.0\n63.0\n85.0\n70.0\n70.0\n50.0\n79.0\n88.0\n80.0\n75.0\n75.0\n70.0\n95.0\nBIRDCHICKEN\n75.0\n80.0\n85.0\n65.0\n65.0\n65.0\n71.0\n51.0\n94.0\n54.0\n88.0\n93.3\n80.0\n50.0\n62.0\n74.0\n90.0\n80.0\n99.3\n55.0\n90.0\nCBF\n99.7\n100.0\n98.3\n99.8\n99.8\n89.8\n95.9\n97.7\n99.4\n90.8\n99.6\n96.1\n83.4\n33.2\n89.6\n86.9\n77.8\n96.3\n97.9\n83.0\n95.9\nCHINATOWN\n95.7\n96.5\n95.1\n98.3\n98.3\n93.6\n97.7\n96.6\n98.0\n94.5\n97.8\n98.3\n96.5\n72.6\n82.5\n87.2\n83.1\n98.3\n98.5\n85.7\n98.0\nCHLORINECONCENTRATION\n64.8\n83.2\n74.9\n75.3\n75.3\n56.2\n60.8\n58.3\n81.7\n66.2\n85.3\n69.2\n62.6\n53.3\n55.4\n80.0\n58.8\n62.2\n73.9\n56.5\n66.3\nCOFFEE\n100.0 100.0 100.0 100.0 100.0\n82.1 100.0\n88.6\n100.0\n97.9\n100.0\n92.9\n100.0\n50.7\n97.9\n99.3\n100.0\n100.0\n100.0\n67.9\n100.0\nCRICKETX\n75.4\n78.2\n71.3\n73.1\n73.1\n38.5\n53.5\n64.4\n79.4\n51.3\n79.9\n58.5\n54.4\n7.4\n62.7\n59.1\n32.1\n64.6\n77.4\n53.1\n74.6\nCRICKETY\n74.4\n74.9\n72.8\n71.8\n71.8\n46.7\n58.2\n63.9\n79.3\n52.1\n81.0\n58.7\n57.4\n8.5\n65.2\n59.8\n39.0\n67.9\n81.5\n52.1\n74.9\nCRICKETZ\n75.4\n79.2\n70.8\n71.3\n71.3\n40.3\n50.1\n65.1\n81.0\n48.4\n80.9\n57.2\n56.7\n6.2\n64.3\n62.9\n31.8\n68.7\n77.9\n39.7\n72.8\nCROP\n66.5\n75.6\n72.2\n74.2\n74.2\n71.0\n67.0\n76.0\n73.8\n68.7\n74.3\n77.5\n76.4\n4.2\n48.9\n61.8\n68.1\n72.5\n74.8\n34.1\n76.2\nDIATOMSIZEREDUCTION\n96.7\n98.4\n98.4\n97.7\n97.7\n96.1\n95.4\n88.0\n34.6\n64.6\n30.1\n95.8\n87.9\n30.1\n91.4\n90.9\n88.9\n88.6\n96.7\n98.7\n98.4\nDISTALPHALANXOUTLINEAGEGROUP\n77.0\n72.7\n72.7\n75.5\n75.5\n74.1\n75.8\n76.1\n71.8\n72.9\n71.8\n78.4\n79.9\n43.3\n70.5\n64.7\n66.9\n75.5\n79.1\n48.9\n80.6\nDISTALPHALANXOUTLINECORRECT\n71.7\n76.1\n77.5\n75.4\n75.4\n72.8\n77.2\n72.4\n76.0\n75.9\n77.0\n77.5\n78.6\n58.3\n71.1\n72.7\n69.9\n75.7\n76.4\n65.9\n78.6\nDISTALPHALANXTW\n59.0\n69.8\n67.6\n67.6\n67.6\n56.8\n67.1\n69.4\n69.5\n68.5\n66.3\n72.7\n73.4\n28.5\n59.1\n61.0\n69.8\n73.4\n70.5\n61.9\n71.2\nDODGERLOOPDAY\n50.0\n56.2\nNAN\nNAN\nNAN\n20.0\n31.2\n48.7\n14.3\n30.5\n15.0\n56.3\n57.5\n16.0\n59.3\n16.0\n57.5\n51.3\n58.8\n20.0\n57.5\nDODGERLOOPGAME\n87.7\n84.1\nNAN\nNAN\nNAN\n69.6\n81.6\n81.0\n76.8\n87.7\n71.0\n87.0\n81.9\n47.8\n71.6\n86.5\n79.0\n83.3\n88.4\n71.7\n86.2\nDODGERLOOPWEEKEND\n94.9\n96.4\nNAN\nNAN\nNAN\n73.2\n97.4\n98.3\n90.4\n97.8\n95.2\n98.6\n97.8\n73.9\n95.4\n97.8\n97.1\n98.6\n98.6\n80.4\n98.6\nECG200\n77.0\n92.0\n94.0\n88.0\n88.0\n83.0\n81.6\n88.4\n88.8\n83.8\n87.4\n74.8\n74.8\n64.0\n87.4\n91.4\n64.7\n91.0\n75.5\n79.0\n93.0\nECG5000\n92.4\n93.5\n93.3\n94.1\n94.1\n92.8\n92.8\n94.1\n94.0\n93.3\n93.5\n89.0\n86.0\n58.4\n92.2\n93.0\n83.0\n94.1\n94.0\n58.4\n95.2\nECGFIVEDAYS\n76.8\n100.0 100.0 87.8\n87.8\n76.3\n87.4\n84.2\n98.5\n80.0\n96.6\n94.0\n94.3\n49.7\n72.3\n97.3\n94.0\n95.8\n94.8\n56.1\n99.9\nEARTHQUAKES\n71.9\n74.8\n74.8\n74.8\n74.8\n74.8\n70.9\n74.0\n72.5\n74.8\n71.2\n90.0\n88.2\n74.8\n74.8\n72.7\n94.3\n74.8\n100.0\n74.8\n79.1\nELECTRICDEVICES\n60.2\n72.1\n70.7\n68.6\n68.6\n67.6\n68.6\n70.2\n70.6\n65.3\n72.8\n70.9\n63.8\n24.2\n60.5\n59.3\n47.6\n64.8\n69.0\n50.6\n73.1\nFACEALL\n80.8\n77.1\n78.6\n81.3\n81.3\n50.4\n77.4\n79.4\n93.8\n72.0\n86.7\n76.9\n75.3\n8.0\n67.3\n79.4\n82.4\n76.2\n78.3\n14.7\n94.1\nFACEFOUR\n83.0\n93.2\n92.0\n77.3\n77.3\n51.1\n90.5\n85.2\n93.0\n71.1\n95.5\n89.8\n54.5\n29.5\n85.7\n83.6\n79.5\n83.0\n89.8\n65.9\n95.5\nFACESUCR\n90.5\n92.4\n88.4\n86.3\n86.3\n54.3\n87.3\n86.7\n94.3\n77.5\n95.4\n84.9\n81.4\n14.3\n64.1\n83.1\n75.9\n83.1\n84.2\n46.2\n89.3\nFIFTYWORDS\n69.0\n77.1\n73.2\n65.3\n65.3\n52.5\n62.4\n65.8\n64.6\n61.1\n74.0\n66.2\n69.5\n12.5\n51.8\n70.8\n59.1\n71.4\n80.0\n49.2\n74.5\nFISH\n82.3\n92.6\n89.1\n81.7\n81.7\n72.0\n85.5\n73.4\n96.1\n72.0\n98.1\n84.0\n85.7\n12.6\n87.8\n84.8\n82.3\n86.3\n92.0\n73.1\n97.1\nFORDA\n55.5\n93.6\n92.8\n93.0\n93.0\n56.8\n89.6\n92.8\n91.4\n86.3\n93.7\n92.9\n94.3\n51.0\n55.5\n81.6\n51.8\n93.1\n94.5\n91.4\n91.5\nFORDB\n62.0\n79.4\n79.3\n81.5\n81.5\n50.7\n74.9\n77.7\n77.2\n69.8\n81.3\n77.0\n80.4\n50.3\n51.2\n70.7\n52.8\n79.3\n82.3\n67.7\n77.4\nFREEZERREGULARTRAIN\n89.9\n98.6\n95.6\n98.9\n98.9\n92.2\n98.7\n76.0\n99.7\n97.3\n99.8\n97.1\n92.5\n50.0\n94.6\n90.6\n78.3\n99.4\n99.6\n82.9\n94.9\nFREEZERSMALLTRAIN\n75.3\n87.0\n93.3\n97.9\n97.9\n92.0\n73.9\n67.6\n68.3\n68.8\n83.2\n76.5\n75.8\n50.0\n91.7\n68.6\n76.7\n76.4\n83.5\n50.0\n78.6\nFUNGI\n83.9\n95.7\n100.0 75.3\n75.3\n36.6\n96.1\n93.4\n1.8\n5.1\n17.7\n96.2\n84.9\n6.3\n43.9\n86.3\n84.4\n85.5\n91.4\n5.4\n99.5\nGESTUREMIDAIRD1\n56.9\n60.8\n60.8\n36.9\n36.9\n20.8\n53.4\n52.8\n69.5\n51.8\n69.8\n71.5\n72.3\n3.8\n54.9\n57.5\n57.7\n63.1\n71.5\n29.2\n77.7\nGESTUREMIDAIRD2\n60.8\n46.9\n54.6\n25.4\n25.4\n13.8\n51.8\n48.0\n63.1\n50.0\n66.8\n54.6\n59.2\n3.8\n57.5\n54.5\n52.3\n53.8\n59.2\n20.0\n62.3\nGESTUREMIDAIRD3\n32.3\n29.2\n28.5\n17.7\n17.7\n15.4\n31.7\n36.8\n32.6\n27.8\n34.0\n42.3\n46.9\n3.8\n27.5\n38.2\n36.2\n41.5\n50.0\n16.2\n49.2\nGESTUREPEBBLEZ1\n79.1\n93.0\n91.9\n39.5\n39.5\n50.0\n84.4\n82.1\n88.0\n76.9\n90.1\n86.6\n82.0\n16.3\n84.0\n79.2\n70.9\n86.0\n89.5\n60.5\n89.0\nGESTUREPEBBLEZ2\n67.1\n87.3\n89.9\n43.0\n43.0\n38.0\n77.8\n79.6\n78.1\n72.0\n77.7\n84.2\n77.2\n18.4\n84.3\n70.1\n61.4\n85.4\n91.1\n28.5\n88.6\nGUNPOINT\n90.7\n98.0\n98.0\n99.3\n99.3\n82.7\n94.8\n78.4\n100.0\n90.7\n99.1\n95.3\n86.0\n49.3\n98.9\n92.8\n80.7\n84.0\n100.0\n84.7\n99.3\nGUNPOINTAGESPAN\n91.8\n98.7\n99.4\n99.4\n99.4\n99.1\n91.2\n89.0\n99.6\n88.7\n99.7\n96.2\n88.3\n49.4\n96.5\n93.4\n88.0\n94.0\n98.1\n49.4\n99.7\nGUNPOINTMALEVERSUSFEMALE\n99.7\n100.0\n99.7\n99.7\n99.7\n100.0 97.7\n97.8\n99.7\n95.2\n99.2\n100.0\n99.7\n52.5\n98.8\n98.0\n91.5\n99.4\n99.1\n47.5\n100.0\nGUNPOINTOLDVERSUSYOUNG\n83.8\n100.0 100.0 100.0 100.0 100.0 92.2\n92.3\n98.9\n92.6\n98.9\n100.0\n100.0\n52.4\n97.5\n94.1\n100.0\n92.1\n97.5\n52.4\n100.0\nHAM\n46.7\n71.4\n72.4\n74.3\n74.3\n52.4\n72.0\n68.2\n70.7\n71.8\n75.8\n77.1\n79.0\n51.4\n76.8\n69.9\n80.0\n83.8\n78.1\n78.1\n77.1\nHERRING\n53.1\n64.1\n59.4\n59.4\n59.4\n59.4\n53.1\n51.2\n64.4\n57.2\n60.0\n62.5\n67.2\n59.4\n62.5\n49.1\n64.1\n67.2\n70.3\n57.8\n75.0\nINSECTWINGBEATSOUND\n35.5\n63.0\n59.7\n41.5\n41.5\n26.6\n58.5\n63.0\n39.2\n58.7\n49.9\n63.6\n65.8\n9.1\n43.5\n60.4\n63.4\n65.1\n65.9\n59.8\n65.8\nITALYPOWERDEMAND\n95.0\n92.5\n95.4\n95.5\n95.5\n84.5\n95.4\n96.4\n96.3\n96.6\n96.2\n96.9\n96.1\n49.9\n87.1\n95.3\n94.2\n97.2\n95.8\n88.0\n96.3\nLIGHTNING7\n72.6\n86.3\n79.5\n68.5\n68.5\n41.1\n64.7\n69.6\n82.5\n55.9\n82.7\n80.8\n68.5\n26.0\n60.8\n61.6\n67.1\n72.6\n82.2\n56.2\n87.7\nMEAT\n93.3\n95.0\n95.0\n88.3\n88.3\n90.0\n91.3\n78.7\n80.3\n78.7\n99.0\n86.7\n60.0\n33.3\n97.0\n89.3\n96.7\n66.7\n96.7\n66.7\n96.7\nMEDICALIMAGES\n73.7\n78.9\n75.0\n74.7\n74.7\n63.2\n67.1\n66.4\n77.8\n62.7\n77.0\n71.2\n72.8\n51.4\n64.9\n71.9\n56.8\n75.4\n75.8\n49.6\n76.7\nMELBOURNEPEDESTRIAN\n79.1\n95.9\n94.4\n94.9\n94.9\n74.1\n81.3\n88.4\n91.2\n84.0\n90.9\n96.4\n91.1\n10.0\n73.0\n86.3\n83.9\n88.7\n88.3\n20.7\n94.6\nMIDDLEPHALANXOUTLINEAGEGROUP\n50.0\n63.6\n65.6\n63.0\n63.0\n61.7\n53.4\n57.7\n53.5\n55.8\n54.5\n63.6\n63.0\n57.1\n57.8\n52.2\n65.6\n66.2\n64.3\n52.6\n65.6\nMIDDLEPHALANXOUTLINECORRECT\n69.8\n83.8\n82.5\n81.8\n81.8\n75.3\n74.4\n75.2\n79.5\n79.6\n82.6\n81.8\n82.1\n57.0\n74.3\n75.5\n61.2\n79.4\n84.5\n51.9\n84.9\nMIDDLEPHALANXTW\n50.6\n58.4\n59.1\n61.0\n61.0\n50.6\n55.1\n59.7\n50.1\n56.2\n49.5\n63.0\n59.7\n28.6\n56.9\n53.6\n61.7\n62.3\n61.0\n57.1\n63.0\nMOTESTRAIN\n83.5\n86.1\n85.1\n84.3\n84.3\n76.8\n88.5\n87.2\n93.6\n69.1\n92.4\n91.4\n83.9\n53.9\n80.9\n85.5\n86.6\n86.7\n89.7\n68.1\n90.8\nOSULEAF\n59.1\n85.1\n76.0\n72.3\n72.3\n54.5\n48.2\n55.4\n97.9\n41.9\n98.0\n54.5\n56.2\n18.2\n62.8\n56.0\n43.0\n56.2\n83.5\n23.1\n80.6\nPHALANGESOUTLINESCORRECT\n72.8\n80.9\n78.4\n80.4\n80.4\n77.3\n79.9\n74.5\n81.8\n79.5\n84.5\n82.8\n81.2\n61.3\n65.6\n75.6\n67.2\n75.9\n82.1\n66.3\n77.7\nPICKUPGESTUREWIIMOTEZ\n66.0\n82.0\n74.0\n60.0\n60.0\n24.0\n60.8\n49.6\n74.4\n41.2\n70.4\n76.0\n70.0\n10.0\n61.6\n60.4\n64.0\n80.0\n84.0\n8.0\n82.0\nPLANE\n100.0 100.0\n99.0 100.0 100.0\n93.3\n96.2\n96.4\n100.0\n95.2\n100.0\n98.1\n98.1\n14.3\n100.0 97.7\n99.0\n98.1\n99.0\n92.4\n100.0\nPOWERCONS\n87.8\n96.1\n90.0\n96.1\n96.1\n91.1\n96.0\n97.1\n86.3\n92.9\n87.9\n100.0\n100.0\n50.0\n85.2\n97.7\n98.9\n99.4\n96.7\n98.9\n100.0\nPROXIMALPHALANXOUTLINEAGEGROUP 80.5\n83.4\n84.4\n83.9\n83.9\n85.4\n81.2\n87.2\n82.5\n83.9\n84.7\n86.3\n87.3\n48.8\n83.9\n84.9\n85.9\n87.8\n87.8\n83.9\n87.8\nPROXIMALPHALANXOUTLINECORRECT\n78.4\n88.7\n85.9\n87.3\n87.3\n77.0\n80.7\n76.8\n90.7\n86.6\n92.0\n88.3\n88.3\n68.4\n81.7\n73.0\n79.7\n82.5\n85.6\n80.1\n86.9\nPROXIMALPHALANXTW\n76.1\n82.4\n77.1\n80.0\n80.0\n78.0\n77.7\n79.1\n76.1\n77.5\n77.3\n81.5\n82.4\n34.1\n78.4\n76.7\n80.0\n81.0\n82.0\n71.2\n82.4\nSHAKEGESTUREWIIMOTEZ\n86.0\n94.0\n92.0\n86.0\n86.0\n76.0\n58.0\n75.6\n88.4\n51.6\n88.0\n82.0\n80.0\n10.0\n86.4\n54.8\n62.0\n82.0\n86.0\n8.0\n94.0\nSHAPELETSIM\n65.0\n100.0\n67.2\n68.3\n68.3\n48.9\n49.7\n51.0\n70.6\n49.8\n78.2\n57.8\n50.0\n50.0\n54.6\n51.3\n47.2\n52.8\n100.0\n48.9\n94.4\nSHAPESALL\n76.8\n90.2\n84.8\n77.3\n77.3\n73.3\n61.7\n67.9\n89.4\n59.9\n92.6\n71.0\n73.2\n1.7\n64.3\n77.6\n63.3\n74.2\n82.8\n23.7\n80.0\nSMOOTHSUBSPACE\n82.7\n98.0\n96.0\n95.3\n95.3\n82.7\n97.6\n96.4\n97.5\n96.3\n98.0\n98.0\n86.7\n33.3\n84.9\n98.0\n83.3\nNAN\n86.0\n45.3\n96.7\nSONYAIBOROBOTSURFACE1\n72.5\n90.3\n90.2\n89.9\n89.9\n72.4\n69.0\n72.9\n95.8\n65.5\n96.1\n80.4\n73.5\n42.9\n72.5\n69.2\n65.6\n85.9\n89.5\n58.9\n96.3\nSONYAIBOROBOTSURFACE2\n83.1\n87.1\n88.9\n90.7\n90.7\n74.5\n83.1\n84.4\n98.0\n80.4\n97.5\n84.2\n83.1\n61.7\n63.5\n83.1\n82.2\n84.1\n89.4\n65.0\n92.5\nSTRAWBERRY\n94.1\n96.2\n95.4\n96.5\n96.5\n91.6\n95.2\n95.9\n97.5\n95.8\n98.0\n97.0\n95.9\n64.3\n91.1\n95.9\n93.5\n94.1\n96.5\n93.5\n95.1\nSWEDISHLEAF\n79.2\n94.1\n91.4\n92.3\n92.3\n73.8\n88.4\n90.2\n96.7\n84.1\n96.3\n90.4\n90.1\n6.4\n83.7\n84.5\n83.2\n89.1\n95.4\n89.9\n92.0\nSYMBOLS\n95.0\n97.6\n96.3\n91.6\n91.6\n78.6\n80.8\n75.4\n95.5\n64.4\n89.3\n88.8\n84.8\n17.4\n79.8\n83.6\n82.0\n87.0\n93.7\n69.4\n97.0\nSYNTHETICCONTROL\n99.3\n99.7\n98.7\n99.0\n99.0\n49.0\n98.7\n97.3\n98.9\n95.3\n99.7\n99.3\n92.7\n16.7\n87.9\n97.3\n88.7\n98.3\n99.0\n43.7\n98.7\nTOESEGMENTATION1\n77.2\n91.7\n93.9\n93.0\n93.0\n80.7\n59.8\n70.6\n96.1\n55.9\n95.7\n66.2\n61.4\n52.6\n88.2\n58.9\n61.4\n64.0\n96.5\n56.1\n93.4\nTOESEGMENTATION2\n83.8\n89.2\n90.0\n87.7\n87.7\n61.5\n75.2\n70.2\n88.9\n64.9\n89.4\n83.1\n63.1\n81.5\n79.4\n74.5\n51.5\n81.5\n94.6\n73.1\n93.1\nTRACE\n100.0 100.0\n99.0 100.0 100.0 100.0 95.2\n74.0\n100.0\n90.2\n100.0\n92.0\n78.0\n24.0\n93.4\n80.6\n68.0\n84.0\n100.0\n71.0\n100.0\nTWOLEADECG\n90.5\n98.6\n99.9\n97.6\n97.6\n87.1\n87.7\n78.4\n99.9\n80.6\n100.0\n78.2\n63.6\n50.0\n94.9\n75.3\n75.2\n60.7\n96.8\n65.8\n99.1\nTWOPATTERNS\n100.0 100.0\n99.9\n99.9\n99.9\n46.6\n99.1\n100.0\n87.0\n97.6\n100.0\n99.4\n94.8\n25.9\n87.5\n94.8\n85.3\n99.4\n100.0\n92.3\n97.9\nUMD\n99.3\n100.0\n99.3\n98.6\n98.6\n91.0\n96.0\n77.1\n98.8\n84.2\n99.0\n100.0\n88.9\n33.3\n83.5\n94.9\n95.1\n93.8\n97.9\n36.8\n100.0\nUWAVEGESTURELIBRARYX\n72.8\n79.5\n78.5\n73.3\n73.3\n56.9\n72.1\n77.1\n75.4\n72.6\n78.1\n72.9\n78.4\n12.7\n60.8\n76.8\n64.1\n78.9\n83.2\n74.9\n82.4\nUWAVEGESTURELIBRARYY\n63.4\n71.9\n71.0\n64.1\n64.1\n34.8\n62.6\n67.6\n64.2\n63.9\n66.6\n62.6\n69.9\n12.1\n49.7\n69.9\n59.3\n70.8\n76.3\n64.8\n75.5\nUWAVEGESTURELIBRARYZ\n65.8\n77.0\n75.7\n69.0\n69.0\n65.5\n63.0\n68.4\n72.7\n64.5\n74.9\n64.8\n69.6\n12.1\n57.3\n69.7\n55.6\n72.8\n75.7\n64.3\n75.2\nWAFER\n98.0\n99.8\n99.2\n99.4\n99.4\n99.1\n96.1\n99.8\n99.7\n99.2\n99.8\n99.8\n99.3\n89.2\n91.6\n99.6\n94.9\n99.3\n99.9\n99.4\n99.9\nWINE\n57.4\n87.0\n81.5\n77.8\n77.8\n50.0\n51.9\n55.6\n61.1\n50.0\n72.2\n70.4\n64.8\n50.0\n74.4\n54.1\n66.7\n50.0\n63.0\n61.1\n74.1\nWORDSYNONYMS\n64.9\n67.6\n69.1\n53.1\n53.1\n42.2\n56.8\n55.7\n56.1\n47.0\n61.7\n57.7\n57.8\n21.9\n50.6\n59.9\n45.3\n59.1\n68.3\n45.1\n66.0\nYOGA\n83.7\n88.7\n83.7\n79.1\n79.1\n83.0\n78.6\n75.3\n83.7\n74.1\n86.7\n79.3\n76.1\n53.6\n62.6\n85.6\n65.7\n79.5\n88.0\n69.1\n84.4\nAVG.\n76.4\n85.2\n83.4\n79.3\n79.3\n65.9\n75.2\n74.3\n80.9\n70.2\n82.5\n80.0\n76.8\n34.8\n72.7\n75.1\n71.0\n78.4\n85.3\n56.7\n85.6\n16\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nTable 7. Imputation with 100% samples. Pre-training benefit ∆% is calculated as the ratio of decreased imputing error in MSE.\nMASK RATIO\n12.5%\n25.0%\n37.5%\n50.0%\nPRE-TRAINED\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nETTH1\n0.097\n0.084\n13.4\n0.112\n0.103\n8.0\n0.140\n0.125\n10.7\n0.173\n0.157\n9.2\nETTH2\n0.137\n0.077\n43.8\n0.114\n0.084\n26.3\n0.128\n0.097\n24.2\n0.150\n0.116\n22.7\nETTM1\n0.076\n0.054\n28.9\n0.080\n0.059\n26.3\n0.101\n0.078\n22.8\n0.125\n0.105\n16.0\nETTM2\n0.062\n0.040\n35.0\n0.052\n0.046\n12.0\n0.072\n0.055\n23.7\n0.084\n0.066\n20.9\nECL\n0.086\n0.081\n5.8\n0.094\n0.088\n6.4\n0.104\n0.098\n5.8\n0.118\n0.110\n6.8\nWEATHER\n0.120\n0.050\n58.1\n0.072\n0.055\n23.9\n0.079\n0.062\n22.0\n0.080\n0.068\n15.5\nTable 8. Imputation with 20% samples. Pre-training benefit ∆% is calculated as the ratio of decreased imputing error in MSE.\nMASK RATIO\n12.5%\n25.0%\n37.5%\n50.0%\nPRE-TRAINED\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nETTH1\n0.294\n0.147\n50.0\n0.286\n0.162\n43.4\n0.291\n0.182\n37.5\n0.298\n0.208\n30.2\nETTH2\n0.228\n0.094\n58.8\n0.234\n0.102\n56.4\n0.237\n0.119\n49.8\n0.250\n0.138\n44.8\nETTM1\n0.545\n0.086\n84.2\n0.396\n0.099\n75.0\n0.379\n0.118\n68.9\n0.380\n0.148\n61.1\nETTM2\n0.143\n0.049\n65.7\n0.127\n0.056\n55.9\n0.125\n0.065\n48.0\n0.132\n0.080\n39.4\nECL\n0.100\n0.088\n12.0\n0.109\n0.095\n12.8\n0.121\n0.105\n13.2\n0.135\n0.119\n11.9\nWEATHER\n0.162\n0.067\n58.6\n0.160\n0.073\n54.4\n0.160\n0.083\n48.1\n0.148\n0.090\n39.2\nTable 9. Imputation with 5% samples. Pre-training benefit ∆% is calculated as the ratio of decreased imputing error in MSE.\nMASK RATIO\n12.5%\n25.0%\n37.5%\n50.0%\nPRE-TRAINED\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nNONE\n260B\n∆%\nETTH1\n0.364\n0.207\n43.1\n0.363\n0.220\n39.4\n0.367\n0.236\n35.7\n0.377\n0.249\n34.0\nETTH2\n0.229\n0.102\n55.5\n0.237\n0.117\n50.6\n0.240\n0.134\n44.2\n0.253\n0.154\n39.1\nETTM1\n0.740\n0.122\n83.5\n0.748\n0.155\n79.3\n0.759\n0.192\n74.7\n0.780\n0.237\n69.6\nETTM2\n0.169\n0.054\n68.0\n0.171\n0.066\n61.4\n0.175\n0.077\n56.0\n0.182\n0.092\n49.5\nECL\n0.168\n0.094\n44.0\n0.171\n0.103\n39.8\n0.176\n0.114\n35.2\n0.183\n0.128\n30.1\nWEATHER\n0.162\n0.079\n51.2\n0.162\n0.082\n49.4\n0.165\n0.089\n46.1\n0.169\n0.104\n38.5\n17\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nTable 10. Full results of the imputation task.\nMODEL\nPATCHTST\nMOMENT\nTIMER\nMASK RATIO\n12.5%\n25.0%\n37.5%\n50.0%\n12.5%\n25.0%\n37.5%\n50.0%\n12.5%\n25.0%\n37.5%\n50.0%\nETTH1\n0.250\n0.250\n0.266\n0.283\n0.238\n0.253\n0.265\n0.285\n0.273\n0.283\n0.294\n0.312\nETTH2\n0.122\n0.131\n0.149\n0.162\n0.128\n0.145\n0.157\n0.179\n0.177\n0.186\n0.195\n0.209\nETTM1\n0.147\n0.173\n0.211\n0.258\n0.137\n0.165\n0.201\n0.248\n0.352\n0.345\n0.371\n0.413\nETTM2\n0.068\n0.076\n0.086\n0.099\n0.077\n0.085\n0.097\n0.114\n0.161\n0.171\n0.176\n0.158\nECL\n0.105\n0.115\n0.128\n0.143\n0.095\n0.102\n0.113\n0.127\n0.122\n0.130\n0.139\n0.152\nWEATHER\n0.073\n0.078\n0.086\n0.099\n0.077\n0.085\n0.100\n0.113\n0.157\n0.146\n0.147\n0.158\nMODEL\nTIMESNET\nMODERNTCN\nDLINEAR\nMASK RATIO\n12.5%\n25.0%\n37.5%\n50.0%\n12.5%\n25.0%\n37.5%\n50.0%\n12.5%\n25.0%\n37.5%\n50.0%\nETTH1\n0.106\n0.186\n0.228\n0.258\n0.086\n0.117\n0.153\n0.179\n0.272\n0.293\n0.318\n0.344\nETTH2\n0.102\n0.114\n0.137\n0.158\n0.075\n0.087\n0.102\n0.122\n0.196\n0.258\n0.316\n0.382\nETTM1\n0.090\n0.122\n0.199\n0.280\n0.045\n0.065\n0.094\n0.133\n0.173\n0.230\n0.286\n0.382\nETTM2\n0.052\n0.059\n0.072\n0.085\n0.040\n0.045\n0.052\n0.062\n0.131\n0.174\n0.218\n0.256\nECL\n0.119\n0.123\n0.128\n0.136\n0.095\n0.102\n0.107\n0.116\n0.128\n0.151\n0.173\n0.194\nWEATHER\n0.068\n0.068\n0.077\n0.089\n0.048\n0.054\n0.063\n0.071\n0.088\n0.108\n0.133\n0.155\nTable 11. Full results of the short-term forecasting task. We follow the same protocol as Wu et al. (2023).\nMETHOD\nTIMESBERT MODERNTCN TIMESNET ITRANS. KOOPA NHITS DLINEAR PATCHTST MICN\nTIDE\nMOMENT NBEATS\n(OURS)\n(2023)\n(2024)\n(2024A) (2023) (2023)\n(2023B)\n(2023)\n(2022) (2023A)\n(2024)\n(2019)\nYEAR\nSMAPE\n13.312\n13.226\n13.387\n13.923 13.352 13.371\n13.866\n13.517\n14.532 15.320\n20.649\n13.466\nMASE\n2.986\n2.957\n2.996\n3.214\n2.997\n3.025\n3.006\n3.031\n3.359\n3.540\n4.757\n3.059\nOWA\n0.783\n0.777\n0.786\n0.830\n0.786\n0.790\n0.802\n0.795\n0.867\n0.910\n1.230\n0.797\nQUARTER\nSMAPE\n10.004\n9.971\n10.100\n10.757 10.159 10.454\n10.689\n10.847\n11.395 11.830\n10.849\n10.074\nMASE\n1.158\n1.167\n1.182\n1.283\n1.189\n1.219\n1.294\n1.315\n1.379\n1.410\n1.305\n1.163\nOWA\n0.876\n0.878\n0.890\n0.956\n0.895\n0.919\n0.957\n0.972\n1.020\n1.050\n0.968\n0.881\nMONTH\nSMAPE\n12.387\n12.556\n12.670\n13.796 12.730 12.794\n13.372\n14.584\n13.829 15.180\n14.497\n12.801\nMASE\n0.908\n0.917\n0.933\n1.083\n0.953\n0.960\n1.014\n1.169\n1.082\n1.190\n1.143\n0.955\nOWA\n0.856\n0.866\n0.878\n0.987\n0.901\n0.895\n0.940\n1.055\n0.988\n1.090\n1.040\n0.893\nOTHERS\nSMAPE\n4.784\n4.715\n4.891\n5.569\n4.861\n4.696\n4.894\n6.184\n6.151\n6.120\n5.634\n5.008\nMASE\n3.175\n3.107\n3.302\n3.940\n3.124\n3.130\n3.358\n4.818\n4.263\n4.330\n4.102\n3.443\nOWA\n1.004\n0.986\n1.035\n1.207\n1.004\n0.988\n1.044\n1.140\n1.319\n1.330\n1.240\n1.070\nAVERAGE\nSMAPE\n11.648\n11.698\n11.829\n12.684 11.863 11.960\n12.418\n13.022\n13.023 13.950\n14.593\n11.910\nMASE\n1.560\n1.556\n1.585\n1.764\n1.595\n1.606\n1.656\n1.814\n1.836\n1.940\n2.161\n1.613\nOWA\n0.837\n0.838\n0.851\n0.929\n0.858\n0.861\n0.891\n0.954\n0.960\n1.020\n1.103\n0.862\n18\n\n\nTimesBERT: A BERT-Style Foundation Model for Time Series Understanding\nTable 12. Full results of the anomaly detection task. The P, R, and F1 represent the precision, recall, and F1-score (%) respectively.\nF1-score is the harmonic mean of precision and recall. A higher value of P, R, and F1 indicates a better performance.\nDATASET\nSMD\nMSL\nSMAP\nSWAT\nPSM\nAVG. F1\nMETRIC\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\n(%)\nLSTM\n(1997) 78.52 65.47 71.41 78.04 86.22 81.93 91.06 57.49 70.48 78.06 91.72 84.34 69.24 99.53 81.67\n77.97\nTRANSFORMER (2017A) 83.58 76.13 79.56 71.57 87.37 78.68 89.37 57.12 69.70 68.84 96.53 80.37 62.75 96.56 76.07\n76.88\nLOGTRANS\n(2019) 83.46 70.13 76.21 73.05 87.37 79.57 89.15 57.59 69.97 68.67 97.32 80.52 63.06 98.00 76.74\n76.60\nTCN\n(2019) 84.06 79.07 81.49 75.11 82.44 78.60 86.90 59.23 70.45 76.59 95.71 85.09 54.59 99.77 70.57\n77.24\nREFORMER\n(2020) 82.58 69.24 75.32 85.51 83.31 84.40 90.91 57.44 70.40 72.50 96.53 82.80 59.93 95.38 73.61\n77.31\nINFORMER\n(2021A) 86.60 77.23 81.65 81.77 86.48 84.06 90.11 57.13 69.92 70.29 96.75 81.43 64.27 96.33 77.10\n78.83\nANOMALY∗\n(2021) 88.91 82.23 85.49 79.61 87.37 83.31 91.85 58.11 71.18 72.51 97.32 83.10 68.35 94.72 79.40\n80.50\nPYRAFORMER\n(2021) 85.61 80.61 83.04 83.81 85.93 84.86 92.54 57.71 71.09 87.92 96.00 91.78 71.67 96.02 82.08\n82.57\nAUTOFORMER\n(2021) 88.06 82.35 85.11 77.27 80.92 79.05 90.40 58.62 71.12 89.85 95.81 92.74 99.08 88.15 93.29\n84.26\nLSSL\n(2022) 78.51 65.32 71.31 77.55 88.18 82.53 89.43 53.43 66.90 79.05 93.72 85.76 66.02 92.93 77.20\n76.74\nNSFORMER\n(2022) 88.33 81.21 84.62 68.55 89.14 77.50 89.37 59.02 71.09 68.03 96.75 79.88 97.82 96.76 97.29\n82.08\nDLINEAR\n(2023A) 83.62 71.52 77.10 84.34 85.42 84.88 92.32 55.41 69.26 80.91 95.30 87.52 98.28 89.26 93.55\n82.46\nETSFORMER\n(2022) 87.44 79.23 83.13 85.13 84.93 85.03 92.25 55.75 69.50 90.02 80.36 84.91 99.31 85.28 91.76\n82.87\nLIGHTTS\n(2022) 87.10 78.42 82.53 82.40 75.78 78.95 92.58 55.27 69.21 91.98 94.72 93.33 98.37 95.97 97.15\n84.23\nFEDFORMER\n(2022) 87.95 82.39 85.08 77.14 80.07 78.57 90.47 58.10 70.76 90.17 96.42 93.19 97.31 97.16 97.23\n84.97\nTIMESNET\n(2023) 88.66 83.14 85.81 83.92 86.42 85.15 92.52 58.29 71.52 86.76 97.32 91.74 98.19 96.76 97.47\n86.34\nMODERNTCN\n(2024) 87.86 83.85 85.81 83.94 85.93 84.92 93.17 57.69 71.26 91.83 95.98 93.86 98.09 96.38 97.23\n86.62\nMOMENT\n(2024) 78.88 92.01 84.94 88.98 75.10 81.45 90.02 56.51 69.43 92.13 91.67 91.90 98.82 89.55 93.96\n84.34\nTIMESBERT\n(OURS) 80.34 92.61 86.04 90.88 85.42 88.07 95.75 62.57 75.69 92.64 95.30 93.95 98.71 97.83 98.27\n88.40\n19\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21245v1.pdf",
    "total_pages": 19,
    "title": "TimesBERT: A BERT-Style Foundation Model for Time Series Understanding",
    "authors": [
      "Haoran Zhang",
      "Yong Liu",
      "Yunzhong Qiu",
      "Haixuan Liu",
      "Zhongyi Pei",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "abstract": "Time series analysis is crucial in diverse scenarios. Beyond forecasting,\nconsiderable real-world tasks are categorized into classification, imputation,\nand anomaly detection, underscoring different capabilities termed time series\nunderstanding in this paper. While GPT-style models have been positioned as\nfoundation models for time series forecasting, the BERT-style architecture,\nwhich has made significant advances in natural language understanding, has not\nbeen fully unlocked for time series understanding, possibly attributed to the\nundesirable dropout of essential elements of BERT. In this paper, inspired by\nthe shared multi-granularity structure between multivariate time series and\nmultisentence documents, we design TimesBERT to learn generic representations\nof time series including temporal patterns and variate-centric characteristics.\nIn addition to a natural adaptation of masked modeling, we propose a parallel\ntask of functional token prediction to embody vital multi-granularity\nstructures. Our model is pre-trained on 260 billion time points across diverse\ndomains. Leveraging multi-granularity representations, TimesBERT achieves\nstate-of-the-art performance across four typical downstream understanding\ntasks, outperforming task-specific models and language pre-trained backbones,\npositioning it as a versatile foundation model for time series understanding.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}