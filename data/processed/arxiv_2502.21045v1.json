{
  "id": "arxiv_2502.21045v1",
  "text": "Incorporating Long-Range Interactions via the Multipole Expansion into\nGround and Excited-State Molecular Simulations\nRhyan Barrett\nLeipzig University, Wilhelm Ostwald Institute for Physical and\nTheoretical Chemistry, Linnéstraße 2, 04103 Leipzig, Germany\nJohannes C. B. Dietschreit\nInstitute of Theoretical Chemistry, Faculty of Chemistry,\nUniversity of Vienna, Währinger Straße 17, 1090 Vienna, Austria\nJulia Westermayr∗\nLeipzig University, Wilhelm Ostwald Institute for Physical and Theoretical Chemistry,\nLinnéstraße 2, 04103 Leipzig, Germany and\nCenter for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI), Dresden/Leipzig, Germany\n(Dated: March 3, 2025)\nSimulating long-range interactions remains a significant challenge for molecular machine\nlearning potentials due to the need to accurately capture interactions over large spatial re-\ngions. In this work, we introduce FieldMACE, an extension of the message-passing atomic\ncluster expansion (MACE) architecture that integrates the multipole expansion to model\nlong-range interactions more efficiently. By incorporating the multipole expansion, Field-\nMACE effectively captures environmental and long-range effects in both ground and excited\nstates.\nBenchmark evaluations demonstrate its superior performance in predictions and\ncomputational efficiency compared to previous architectures, as well as its ability to accu-\nrately simulate nonadiabatic excited-state dynamics. Furthermore, transfer learning from\nfoundational models enhances data efficiency, making FieldMACE a scalable, robust, and\ntransferable framework for large-scale molecular simulations.\nI.\nINTRODUCTION\nOver the past few years, graph neural networks (GNNs) have shown great promise in modeling\nmolecules and materials due to the natural graph-like structure of chemical systems [1–4]. However,\ncapturing long-range interactions, particularly electrostatics, remains a key challenge [5]. These\ninteractions are crucial when considering solvent effects [6, 7] or for phenomena such as protein\n∗julia.westermayr@uni-leipzig.de\narXiv:2502.21045v1  [physics.comp-ph]  28 Feb 2025\n\n\n2\nfolding [8, 9]. However, their slow decay with distance makes them difficult to represent accurately\n[10, 11].\nMany frequently applied GNN models focus on interactions among immediate neighbors [12, 13].\nAs a result, purely local descriptors often miss subtle, but important long-range forces, leading to\ninaccuracies when simulating systems like proteins or hybrid organic-inorganic interfaces [14, 15]\nthat are often dominated by these effects. Various strategies have been proposed to address this\nlimitation [16, 17], among them, message-passing methods, which exchange information among\nneighboring nodes, but their limited depth restricts the effective receptive field, making it difficult\nto account for distant atoms.\nOther approaches use additional physics-based terms [18, 19] or\nincorporate data from large, extended systems [16, 20] such as some of the systems in the OC20\n[21] dataset, which describes molecules on surfaces, although these can be prohibitively expensive\nto generate.\nA more efficient alternative takes inspiration from QM/MM (quantum mechanics/molecular me-\nchanics) [22, 23] methodologies. In these methods, the region undergoing critical chemical transfor-\nmations is modeled quantum mechanically, while the surrounding environment is treated classically.\nAn illustration of this approach is shown in the top right of Fig. 1. This reduces the computational\ncost for large systems without sacrificing essential chemical accuracy.\nTraditional machine learning (ML) models that include long-range effects often assign node\nfeatures to every atom in the system and perform periodic updates thereof, even if many atoms\nare only characterized by a simple charge [24–26]. This leads to excessive computational overhead\nand poor scaling. To address this issue, some approaches incorporate physics-based features from\nthe classically treated region into the quantum region. For example, FieldSchNet [27] includes the\nelectric field generated by the partial charges of the MM atoms as an extra feature for the ML\nmodel that describes the QM region. This significantly reduces the computational time for training\ncompared to methods like SO3krates [24], since only a subset of atoms requires full periodically\nupdated node features, while the rest can be captured through external interactions. While this\nstrategy efficiently represents some long-range effects without sacrificing computational speed, it\nrelies on a single scalar value (the electric field), which may not fully capture the complex nature\nof the interactions between MM and QM region.\nOne way to expand existing ML methods within the QM/MM methodology is via the introduc-\ntion of higher-order features through the multipole expansion [28, 29]. The multipole expansion\noffers a systematic way to decompose electrostatic interactions into hierarchical terms, which nat-\nurally fits with equivariant neural networks, especially those based on spherical harmonics. In this\n\n\n3\nwork, we incorporate these features into the message-passing atomic cluster expansion (MACE)\n[30] architecture, resulting in FieldMACE, and demonstrate how this approach can be applied to\nboth ground- and excited-state simulations. Additionally, transfer learning from the foundational\nMACE-OFF [31] model originally trained on ground-state data without the inclusion of an environ-\nment is shown to significantly reduce the amount of data needed [32, 33] for accurate predictions.\nMost notably, we show that this refinement can replicate reference population curves obtained from\nnonadiabatic excited-state dynamics [34, 35]. These simulations are, when conducted with quan-\ntum chemistry, computationally intensive, often requiring months of computation[36], and still pose\nchallenges to ML models [37–39]. In this work, as little as 30 excited-state data points are required\nwhen using transfer learning for reproducing excited-state nonadiabatic molecular dynamics.\nII.\nPRELIMINARIES AND RELATED WORK\n{R1, ....., Rn}   {Z1, ....., Zn}\nReadouts\nMessage \nConstruction \nMessage \nAggregation \nEmbedding\nLong Range Interaction\nShort Range Interaction\nLong Range Interaction\nLong Range Interaction\nShort Range Interaction\ni\nj\nh1\nht\nht\nh5\nh4\nh3\nProperties\nhQM\nhi\nwij\na)\nb)\nQM\nMM\nh2\nh0\nh1\nh5\nh4\nh3\nh2\nm2\nm1\nmt\nmt\nm3\nm4\nh0\nm5\nht\nht\nht\nht\nht+1\nht+1\nht+1\nht+1\nht+1\nht+1\nmt\nmt\nmt\nShort Range\nQM/MM System \nPairwise Aggregation\n1\n1\n2 3\n3\n4\n4\n5\n5\ni\ni\ni i\ni\n2\nPairwise Attention \nComputation\nMM Atoms\n1 2 3 4 5\nh1  h2  h3  h4  h5  h6 \nhMMhMMhMMhMMhMMhMM\nEmbedding\nQM Node \nFeature\nAttention Multipole \nExpansion Φi \nm0\nm0\n= \n+ \n \nΦi\nQM \nMM \nFIG. 1.\nArchitecture of the message-passing neural network integrated with the multipole\nexpansion a) The short-range blocks operate through a message-passing scheme aggregating information\nin the atoms local neighborhood, whereas the long-range blocks collect information on molecular mechanics\n(MM) nodes through the multipole expansion. b) The long-range block uses pairwise attention weightings\nthat are computed between the quantum mechanics (QM) region and MM atoms. These are combined with\nMM node features and the QM node features in the multipole expansion described in Section III.\n\n\n4\nA.\nMessage-passing\nAtoms in a molecular system are characterized by their position vector ri ∈R3 and atomic\nnumber Zi. These systems can be represented as graphs, where atoms correspond to nodes Vi,\nand all nodes in the neighborhood of central atom i, N(i) = {Vj | ||rij||2 = ||ri −rj||2 < rcut}\nare connected by edges, where rcut is a predefined cutoff radius. The categorical atomic numbers\nare transformed into a learnable invariant embedding vector Xi ∈Rk. During the message-passing\nprocess [40, 41] each atom i is associated with a set of features hi ∈Rk, and each edge (i, j)\nis assigned the vector between the two atoms rij. The features are initialized with the atomic\nembedding h0\ni = Xi and then updated through an iterative process:\nm(t)\nij = ϕmsg\n\u0010\nh(t)\ni , h(t)\nj , rij\n\u0011\n(1)\nm(t)\ni\n=\nX\nj∈N(i)\nm(t)\nij\n(2)\nh(t+1)\ni\n= ϕupdate\n\u0010\nh(t)\ni , m(t)\ni\n\u0011\n(3)\nThis process involves three main steps: i) creation of the messages m(t)\nij that are computed for each\nedge using a learnable function ϕmsg and the vector rij. ii) These messages are aggregated over\nthe neighborhood of each node using a permutation-invariant operation, such as summation. iii)\nFinally, node features hi are updated using another learnable function ϕupdate. Messages passed\nbetween atoms in the neighborhood N(i) can represent scalar or higher-order features depending\non the approach chosen. An illustration of the message-passing process can be seen in Fig. 1a\n(short-range block).\nB.\nEquivariance\nVectorial properties of a molecular system are uniquely defined by the orientation of it in space,\ni.e. if the entire system rotates, associated properties have to transform accordingly, which is known\nas equivariance [42]. Formally the set of features hi is called equivariant under a group G if\nD(R)hi(r1, ..., rn) = hi(R ◦(r1, ..., rn)) ,\n(4)\nwhere R ∈G and D(R) is the equivalent operation acting on hi. The main focus of this paper will\nbe on the special orthogonal group SO(3). In the case of SO(3), these equivariant features can be\nbroken down into smaller building blocks [43], the spherical harmonic functions, Yℓm(θ, ϕ) acting on\n\n\n5\nthe sphere, where ℓrepresents the degree and m corresponds to the order, m ∈{−ℓ, ..., ℓ}. Under\na rotation R ∈SO(3), the spherical harmonics transform as follows:\nYℓm′(θ′, ϕ′) =\nX\nm\nDℓ\nm′m(R)Yℓm(θ, ϕ),\n(5)\nwith Dℓ\nm′m(R) being the Wigner-D matrices. In most cases, the equivariant feature vectors are\nexpressed as a series of vectors where each is associated to a certain spherical harmonic, known\nas channels. The equivariant features are indexed as hℓ\ni,m,k where k is the channel index. The\nmessage-passing framework can be rewritten in terms of these spherical harmonics. In this case\nonly two body features will be considered, describing only interactions between neighbors around\natom i.\nmL,(t)\nM,i,j,k = Rℓ1m1(rij) Yℓ1m1(θ, ϕ)hℓ2,(t)\nj,m2,k\n(6)\nmℓ,(t)\nm,i,k =\nX\nL,M\nClm\nL,M\nX\nj∈N(i)\nmL,(t)\nM,i,j,k\n(7)\nhℓ,(t+1)\ni,m,k\n= wl\nmkkmℓ,(t)\nm,i,k + hℓ,(t)\ni,m,k\n(8)\nL = (ℓ1, ℓ2),\nM = (m1, m2)\nThe angles θ and ϕ correspond to the directionality of the unit vector of rij and rij the distance\nbetween atoms i and j. The values wl\nmkk′ correspond to learnable, linear transformations across\nthe channel of each spherical harmonic, an additional non-linear function can be placed along with\nthe linear transformation. In order to preserve the correct equivariance for each order of spherical\nharmonics we must include the standard Clebsch Gordan coefficients Clm\nL,M [44]. The equations\nabove form the basic block to the short range interactions in the following section. These blocks\ncan also be exchanged with MACE which is discussed in the Appendix A.\nC.\nMultipole Expansion\nThe multipole expansion is a technique that decomposes complex electromagnetic fields into\nhierarchical terms. This approach is particularly useful for analyzing fields far from their sources,\nwhere higher-order contributions become negligible. These moments are constructed in terms of\nspherical harmonics allowing a direct integration into equivariant neural networks that already use\nspherical harmonics such as MACE.\nA scalar potential Φ(r) arising from a charge distribution ρ(r′), where r is the field point and r′\n\n\n6\nis the source point, is defined by Coulomb’s law as\nΦ(r) =\n1\n4πε0\nZ\nρ(r′)\n|r −r′| dr′ .\n(9)\nThe function\n1\n|r−r′| can be expressed as a sum of spherical harmonics when r < r′:\n1\n|r −r′| =\n∞\nX\nℓ=0\nℓ\nX\nm=−ℓ\n4π\n2ℓ+ 1\n\u0010 r\nr′\n\u0011ℓ\nYℓm(θ, ϕ)Yℓm∗(θ′, ϕ′)\n(10)\nSubstituting this expansion into Coulomb’s law yields:\nΦ(r) = 1\nε0\n∞\nX\nℓ=0\nℓ\nX\nm=−ℓ\n1\n2ℓ+ 1\nYℓm(θ, ϕ)\nrℓ+1\nQℓm,\n(11)\nwhere the multipole moment Qℓm is defined as:\nQℓm =\nZ\nr′ℓρ(r′)Yℓm∗(θ′, ϕ′) dr′.\n(12)\nEach term in this expansion corresponds to a specific multipole order, with higher-order terms\ndecaying more rapidly with distance. These multipole moments will be used as feature vectors in\nthe following section.\nIII.\nINTEGRATION OF QM/MM FRAMEWORK WITH EQUIVARIANT NEURAL\nNETWORKS\nOur method splits a chemical system into a QM and an MM region. The descriptors for the\nQM region are derived in Sections 2.1 and 2.2. The MM atoms are described by their positions and\npartial charges that are assigned by a classical force field. While this is sufficient for the construction\nof the multipole expansion, the direct summation leads to significant information loss. To address\nthis, we introduce weightings towards each of the MM atoms. Concretely, we compute pairwise\nattention coefficients [45] between the QM node features and the MM atom features to establish\nappropriate weightings.\nA.\nAttention Weightings\nInitial feature vectors for the MM atoms comprise spherical harmonics values, Y MM\nℓm , computed\nfrom the position values. These values are expanded to produce MM node features,\nhℓ,MM\nj,m,k′ = wℓ\nmk′Y MM\nj,ℓm\n(13)\n\n\n7\nwhere wl\nmk′ is a mapping R →Rk′.\nTo reduce computational costs, k’ can be tuned as a hy-\nperparameter and remains k’< k. A similar contraction operation is performed for the QM node\nfeatures:\nhℓ,QM,(t)\ni,m,k′\n= wℓ\nmk′khℓ,QM,(t)\ni,m,k\n,\n(14)\nwhere wl\nmk′k is a mapping Rk →Rk′. Given these feature vectors, pairwise attention is taken\nbetween QM atoms and each of the MM atoms.\nNote that no attention is computed between\nMM atoms. This significantly reduces the computational cost for large systems, full element-wise\ncomparison scales with O(n2) whereas element-wise comparison between only MM and QM scales\nwith O(n) with the QM region being assumed to be constant. The element-wise comparisons are\nperformed using an L1 norm\nκ(t)\nijk′ = ξℓ\nmk′k′(|hℓ,QM,(t)\ni,m,k′\n−hℓ,MM\nj,m,k′|)\n(15)\nHere ξ consists of a linear transformation followed by a softmax normalization. These weighting\nvalues can be integrated into the multipole expansion to obtain the attention weighted expansion.\nAn illustration of computing the attention weights can be seen beneath the long-range interaction\nblock in Figure 1.\nB.\nAttention Multipole Expansion\nThe charge distribution surrounding the QM region can be described by a set of point charges\nobtained from the MM atoms, qMM\nj\n;\nρ(r′) =\n\n\n\n\n\nqMM\nj\n,\nif r′ = rMM\nj\n,\n0,\notherwise.\n(16)\nThis simplifies the multipole moments, eq. (12), into a sum over these discrete charges.\nQℓ\nm,k′ =\nX\nj\n(rMM\nj\n)lqMM\nj\nhℓ,MM\nj,m,k′ .\n(17)\nIt is important to note that the system is shifted so that the QM region is centered on the origin\nto maintain translational invariance. The multipole expansion can now be written including the\nweighting values for atom i in the QM region,\nΦℓm,(t)\ni,k′\n(rQM\ni\n) = 1\nε0\nX\nj\nκ(t)\nijk′\nQℓ\nm,j,k′\n2ℓ+ 1\nhℓ,QM,(t)\ni,m,k′\n|rQM\ni\n−rMM\nj\n|ℓ+1\n(18)\nQℓ\nm,j,k′ = (rMM\nj\n)lqMM\nj\nhℓ,MM\nj,m,k′\n(19)\n\n\n8\nThe term Φℓm,(t)\ni,k′\n(rQM\ni\n) incorporates the necessary long-range information from the MM atoms for\nnode feature hQM,(t)\ni\nat atom i.\nC.\nMessage Construction\nThe message is updated again after the short range message update eq. 7 by incorporating the\nmultipole expansion term after expanding it to the correct dimensionality:\nmℓ,(t)\nm,i,k = mℓ,(t)\nm,i,k + wℓ,(t)\nmkk′Φℓm,(t)\ni,k′\n(rQM\ni\n)\n(20)\nThe node features are constructed from these messages in the same way as seen in eq. (8). These\nnode features can be expanded to include many body interactions as well as other effects in the\nQM region. An illustration of the long range interaction implementation can be seen in the right\nhalf of Figure 1. Our long-range multipole message is integrated into the MACE framework.\nIV.\nEXPERIMENTS\nTABLE I. Mean Absolute Error (MAE) for Energy and Forces of FieldSchNet and FieldMACE models\nevaluated on the ground states of benzene, uracil, and retinoic acid systems. Results are shown for varying\nrepresentation sizes (8, 16, 32, 64, 128) to assess model performance.\nModel\nSystem\nEnergy (eV)\nForce (eV/Å)\nRepresentation Size\nRepresentation Size\n8\n16\n32\n64\n128\n8\n16\n32\n64\n128\nFieldSchNet\nBenzene\n0.065 0.058 0.049 0.014 0.005\n0.054 0.053 0.047 0.034 0.017\nUracil\n0.182 0.151 0.079 0.016 0.015\n0.128 0.118 0.112 0.054 0.048\nRetinoic acid 0.145 0.053 0.029 0.028 0.015\n0.190 0.141 0.099 0.064 0.031\nFieldMACE\nBenzene\n0.041 0.033 0.018 0.006 0.003\n0.039 0.038 0.031 0.018 0.010\nUracil\n0.060 0.035 0.012 0.006 0.005\n0.082 0.066 0.040 0.026 0.020\nretinoic acid 0.061 0.036 0.019 0.013 0.012\n0.117 0.092 0.065 0.043 0.028\nIn this section, we evaluate the performance of our approach, called FieldMACE, which con-\nstitutes the inclusion of long-range message-passing into the MACE architecture. To this end, we\ntest our model on the datasets by [46] comprising three molecular systems, namely benzene, uracil,\nand retinoic acid modeled under explicit environment. Each data set comprises molecules solvated\n\n\n9\nin water, full details of the dataset can be found in the Appendix B 1 or respective publication.\nWe showcase the performance of our method using different representation sizes and compare it to\nanother state-of-the-art model (FieldSchNet) that can be used for ML/MM, where ML is used to\napproximate the QM part of the system. In addition, we compute the vibrational power spectrum\nwith FieldMACE for benzene solvated in water and compare the spectrum to a quantum chemical\nreference spectrum. In addition, we gauge FieldMACE’s capacity to handle electronically excited\nstates by computing population curves resulting from nonadiabatic photodynamics simulations.\nFinally, we explore the potential for reusing parameters from foundational ground-state models\ntrained without an external environment, particularly the MACE-OFF model, which allows us to\nobtain lower errors with less amount of data compared to directly trained models.\nA.\nGround State Simulations\nTo evaluate the performance of FieldMACE in predicting energies and forces in the presence of an\nenvironment, we benchmark it against FieldSchNet [39], a recently developed model that can be used\nfor QM/MM simulations. Full architecture and training details can be found in the Appendix C. To\nthe best of our knowledge, there are currently few ML approaches capable of effectively describing\nmolecular systems [39, 47–57] in the presence of implicit or explicit environments. Many other\nmodels that are used for ML/MM or similar simulations often do not take the influence of an\nenvironment into account, by relying for instance on so-called mechanical embedding approaches\n(neglecting any electrostatic interactions), or by using specific embedding schemes, like the Buffered\nRegion Neural Network approach (BuRNN),[58] where the whole system is split into three regions\nrather than two and the ML model learns the difference between regions rather than the QM\nregion. Thus, to allow for a fair comparison, the benchmark focuses exclusively on the FieldSchNet\nand FieldMACE models. The results, presented in Table I summarize the mean absolute errors\n(MAEs) for energy and force predictions. As can be seen, FieldMACE consistently outperforms\nFieldSchNet, particularly at smaller representation sizes, demonstrating its capability to capture\nboth short- and long-range interactions while being more compact. For energy predictions, both\nmodels show reduced errors with increasing representation size. Across all systems, FieldMACE\nconsistently exhibits lower errors for both energy and forces.\nIn addition to evaluating test scores, we simulate the photodynamics of solvated benzene and\ncompare the associated power spectra of the ML/MM and QM/MM reference dynamics in Figure 2a,\ncomputational details can be found in Appendix B.2. The trained FieldMACE method produces a\n\n\n10\na)\nb)\nc)\nFIG. 2. a) Power spectrum produced from the dynamics of benzene in a water box. Population curves\nresulting from photodynamics simulations using b) FieldMACE and c) quantum chemistry, showing the\nring-opening reaction of furan after being excited to the second excited state (S2). Transitions to S1 and S0\nare present in both approaches.\npower spectrum in agreement with the QM spectrum, particularly, it reproduces the characteristic\nC–H stretching modes located near 3000 cm−1 as well as the C=C ring stretching and C-H bending\nmodes contained in the distributions centered around 1000 cm−1. In general, the peaks of the ML\nspectrum are located at the same frequencies as in the QM reference, only their relative intensity\nvaries slightly.\nB.\nExcited State Dynamics\nGoing beyond ground state simulations, FieldMACE is used to simulate nonadiabatic dynamics\nusing trajectory surface hopping (TSH) [59, 60].\nThese simulations describe the behavior of a\nmolecule after being excited by light and are computationally much more demanding than ground-\nstate simulations, especially when an explicit environment is included, as is the case here. TSH\nrecovers the quantum nature of the excited wave packet by averaging over many trajectories that\nhave different initial conditions (i.e. configurations and momenta). In Tully’s Fewest Switches TSH\n[59, 61], used here, each trajectory changes its active state via so-called hops stochastically. For\nthe photodynamics simulations, we integrated our FieldMACE approach into the SHARC (Surface\nHopping including Arbitrary Couplings) program suite [34, 62–64].\nWe investigate the molecule furan solvated in water to assess Field-MACE for excited-state\ndynamics. Details on the model and training are specified in Appendix D. Upon excitation, the\nmolecule can undergo a ring-opening reaction. We compare our model to recent simulations of this\nsystem using FieldSchNarc [39].\nFigure 2b and c show the population dynamics obtained using FieldMACE (panel b) and the\n\n\n11\nquantum chemical reference method (panel c). Population curves represent a distribution over the\nactive state showing how the system evolves over time and hops between different electronic states.\nAt the beginning of the simulations, the system is excited to the second excited singlet state, S2.\nThe nonadiabatic dynamics then show non-radiative decay back to the electronic ground state, S0.\nFieldMACE accurately captures the rapid decay from the S2 and the subsequent redistribution of\npopulations to the S1 and the ground state, S0. The predicted population transfer closely matches\nquantum chemical reference populations. As can be seen, in both cases the S2 population transfers\nquickly to the S1 state, which is populated until around 100 fs, at which point the majority of the\ntrajectories fall back into the ground state.\nV.\nTRANSFERABILITY\nA.\nGround State\nIn this section, we analyze the possibility of reducing the amount of data necessary when start-\ning with weights from pre-trained foundational ground-state models that have not been trained\nincluding an environment or excited-state effects. Therefore, we leverage the foundational MACE\nmodel’s pre-trained representations as initial parameters for FieldMACE’s short-range modules to\nimprove performance and data efficiency. Full details on the models and training can be found in\nAppendix B.\nAs can be seen in Figure 3, which shows learning curves using different amounts of data for fine-\ntuning for three molecular systems in solution, initializing the short-range blocks of FieldMACE with\nthe foundational model weights reduces the amount of training data required to achieve comparable\naccuracy to models trained from scratch, particularly for force predictions (second line, panels d-\nf). Across all systems, the pre-trained initialization demonstrates a clear advantage, leading to\nimproved accuracy compared to random initialization, even though the foundational models do not\nincorporate any solvent effects.\nThe benefits of using the foundational representation are most pronounced in systems with few\nstrongly electronegative atoms, as is the case in benzene (first column, panels a and d) or retinoic\nacid (last column, panels c and f). These molecules are characterized by weaker intermolecular\ninteractions and limited hydrogen bonding with the surrounding water molecules. The resulting\npotential energy surfaces for these systems will thus be more similar to the potential energy surfaces\nin a vacuum compared to the uracil system.\n\n\n12\nIn contrast, for polar systems like uracil (middle column, panels b and e), which feature strong\nhydrogen bonding and complex electrostatic interactions, the benefits of the pre-trained initial-\nization are less pronounced. While transfer-learned FieldMACE still outperforms the randomly\ninitialized model, the benefit is smaller compared to the other examples. This suggests that the\nfoundational model’s learned parameters have to be changed more in systems that interact more\nstrongly with the MM environment. Here, additional refinement on another dataset or task-specific\ntraining would improve performance for highly polar systems.\na)\nb)\nc)\nd)\ne)\nf)\nFIG. 3. Comparison of transfer learning from MACE-OFF foundational model (red curves) against randomly\ninitialized FieldMACE models (blue curves).\nEvaluation of transfer and ordinary learning models with\nrespect to energies (top row) and forces (bottom row) of molecules solvated in water: a) and d) benzene ,\nb) and e) uracil and c) and f) retinoic acid .\nB.\nExcited States\nTo further test the ability of FieldMACE to profit from transfer learning by heavily reducing\nthe amount of training data needed, we revisit the furan in water system.\nFull details on the\nmodel and training can be found in Appendix D. Rather than considering the mere test statistics\n(meaning error for energies and forces on the test set), we generate the population curves for the\nsame set of initial conditions used in the previous section with FieldMACE models trained with a\ndecreasing number of data. We compare FieldMACE models trained from scratch with randomly\n\n\n13\ninitialized parameters and those where the short-range block parameters are taken from a MACE-\nOFF model. We use only 600, 150, and 30 data points to train these models. Since excited-state\ndata are computationally much more costly to obtain, this is a realistic use case. The results are\nshown in Figure 4.\nFor the largest dataset with 600 data points (first column), both transfer (panel a) and non-\ntransfer models (panel d) result in similar population curves that align well with the reference\nresults (Figure 2 c). Deviations in the S2, S1, and S0 populations remain minimal throughout the\nsimulation compared to the reference population curves. When the dataset size is reduced to 150\npoints (middle column), the predictions are reasonable for both transfer (panel b) and non-transfer\n(panel e) models.\nIn contrast, for the smallest dataset of 30 data points (last column), a stark difference emerges\nbetween the two initialization strategies. The transfer model (panel c), leveraging the foundational\nrepresentation, maintains a reasonable albeit worse approximation of the population curves and\nless stable simulations, capturing the general trends in decay and redistribution among electronic\nstates. However, the model trained from scratch (panel f) fails to accurately capture the transition\ndynamics or the populations of S2, S1, and S0. These results highlight the critical importance of\ntransfer learning in low-data regimes, where the foundational representation enables FieldMACE\nto generalize effectively despite very limited training data.\nOverall, the results demonstrate that transfer learning significantly enhances FieldMACE’s per-\nformance, particularly when training data is scarce. With sufficient data (600 or 150 data points),\nboth transfer and non-transfer models can perform reasonably well, though transfer learning pro-\nvides an edge in accuracy. In low-data scenarios (30 data points), transfer learning is essential for\nmaintaining reasonable predictions albeit less stable simulations, but the non-transfer model fails\nto capture the dynamics adequately. These findings emphasize the use of pre-trained foundational\nrepresentations for obtaining accurate models.\nVI.\nDISCUSSION\nThe attention weighted multipole expansion presented in this paper highlights an effective way\nof combining spherical harmomics-based equivariant neural networks with external charge distribu-\ntions. In particular, results presented in this study highlight the significant advancements achieved\nby combining the multipole expansion with the MACE neural network architecture, FieldMACE,\nparticularly its transferability and efficiency in modeling both ground-state and excited-state molec-\n\n\n14\na)\nb)\nc)\nf)\ne)\nd)\nFIG. 4. Population curves of furan starting in the third excited singlet state for transfer learning models\nusing a) 600, b) 150, and c) 30 data points and non-transfer learning models using d) 600, e) 150, and f) 30\ndata points.\nular processes. By leveraging the foundational MACE model representations, FieldMACE demon-\nstrates its ability to extrapolate to new molecular systems while requiring less training data, es-\npecially for force predictions.\nThis capability underscores the potential of pre-trained machine\nlearning models in quantum chemistry.\nOne of the most notable findings is the effectiveness of FieldMACE in low-data regimes. For\nexcited-state dynamics, the transfer learning approach from ground-state foundational models en-\nabled FieldMACE to produce reasonable population dynamics even with as little as 30 data points,\nwhereas models initialized with random weights failed to capture the underlying processes. This is\nparticularly impactful given the computational expense associated with generating training data for\nexcited-state simulations. The ability to maintain predictive accuracy with reduced datasets makes\nFieldMACE a practical choice for studying complex systems where data availability is limited.\nWhen comparing FieldMACE with other molecular architectures that incorporate long-range\ninteractions such as SO3Krates, the balance between accuracy and computational expense must be\nconsidered. In these architectures pairwise attention weightings are computed for all pairs in the\nsystem which leads to quadratic scaling, O(n2). In some cases considering interactions between all\natoms in the system may be necessary but in many cases it is not. For instance, in the presented\nuracil system, which contains more than 3000 atoms, the only area of high significance is the uracil\n\n\n15\nmolecule. Training a single epoch while considering all pairwise interactions and assigning large\nnode features can lead to prohibitively large computational costs to train these models. FieldMACE\nsignificantly reduces this computational burden by only considering interactions between the QM\nand MM atoms, thus allowing linear scaling when increasing the size of the external environment.\nThis leads to a dramatic reduction in computational overhead, and thus allows us to train on\nextended systems.\nIn summary, FieldMACE represents a significant step forward in transferable machine learning\nmodels for molecular simulations in the presence of external charge distributions such as QM/MM\nsimulations. Its ability to train on very large systems, combined with its accuracy in low-data\nregimes, makes it a powerful tool for QM/MM simulations in both ground and excited states. Future\nwork will focus on enhancing its performance for highly polar systems, expanding its application to\nbroader chemical spaces, and applying FieldMACE to systems such as proteins where long-range\ninteractions are particularly dominant.\nACKNOWLEDGEMENTS\nThis work is funded in parts by the Deutsche Forschungsgemeinschaft (DFG) – Project-ID\n443871192 - GRK 2721: \"Hydrogen Isotopes 1,2,3H\". The authors acknowledge the ZIH TU Dresden,\nthe URZ Leipzig University, and Paderborn Center for Parallel Computing (PC2) for providing the\ncomputational resources to conduct this study.\nAUTHOR CONTRIBUTIONS\nRB (Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software,\nVisualization, Validation, Writing - original draft). JCBD (Methodology, Writing - review). JW\n(Methodology, Conceptualization, Supervision, Writing - review).\nMATERIALS AND CORRESPONDENCE\nCorrespondence to Julia Westermayr.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\n\n\n16\nCODE AND DATA AVAILABILITY\nThe relevant code and datasets can be found here: https://figshare.com/articles/dataset/\nModels_data_and_code_for_publication_Incorporating_Long-Range_Interactions_via_the_\nMultipole_Expansion_into_Ground_and_Excited-State_Molecular_Simulations_/28497857\nand https://github.com/rhyan10/FieldMACE/tree/master , any additional datasets used can\nbe accessed on request.\n[1] H. E. Sauceda, L. E. Gálvez-González, S. Chmiela, L. O. Paz-Borbón, K.-R. Müller and A. Tkatchenko,\nNat. Commun., 2022, 13, 3733.\n[2] B. Deng, P. Zhong, K. Jun, J. Riebesell, K. Han, C. J. Bartel and G. Ceder, Nat. Mach. Intell., 2023,\n5, 1031–1041.\n[3] X. Gao, F. Ramezanghorbani, O. Isayev, J. S. Smith and A. E. Roitberg, J. Chem. Inf. Model., 2020,\n60, 3408–3415.\n[4] T. W. Ko, J. A. Finkler, S. Goedecker and J. Behler, Nat. Commun., 2021, 12, 398.\n[5] D. M. Anstine and O. Isayev, J. Phys. Chem. A, 2023, 127, 2417–2431.\n[6] C. Reichardt, Org. Process Res. Dev., 2007, 11, 105–113.\n[7] C. Reichardt and T. Welton, Solvents and Solvent Effects in Organic Chemistry, John Wiley & Sons,\n4th edn., 2011.\n[8] N. Go and H. Taketomi, Proc. Natl. Acad. Sci., 1978, 75, 559–563.\n[9] C. Sagui and T. A. Darden, Annu. Rev. Biophys. Biomol. Struct., 1999, 28, 155–179.\n[10] A. Ambrosetti, N. Ferri, R. A. DiStasio and A. Tkatchenko, J. Chem. Phys., 2014, 140, 18A508.\n[11] K. Kang et al., arXiv preprint, 2024.\n[12] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E. Smidt\nand B. Kozinsky, Nat. Commun., 2022, 13, 2453.\n[13] K. Schütt, O. Unke and M. Gastegger, Proc. Int. Conf. Mach. Learn., 2021, pp. 9377–9388.\n[14] M. M. Gromiha and S. Selvaraj, Biophys. Chem., 1999, 77, 49–68.\n[15] J. Westermayr, S. Chaudhuri, A. Jeindl, O. T. Hofmann and R. J. Maurer, Digit. Discov., 2022, 1,\n463–475.\n[16] A. Kosmala, J. Gasteiger, N. Gao and S. Günnemann, Proc. Int. Conf. Mach. Learn., 2023, pp. 17544–\n17563.\n[17] P. Loche, K. K. Huguenin-Dumittan, M. Honarmand, Q. Xu, E. Rumiantsev, W. B. How, M. F. Langer\nand M. Ceriotti, arXiv preprint arXiv:2412.03281, 2024.\n[18] D. M. Anstine and O. Isayev, J. Phys. Chem. A, 2023, 127, 2417–2431.\n[19] A. Gao and R. C. Remsing, Nat. Commun., 2022, 13, 1572.\n\n\n17\n[20] J. Behler and G. Csányi, Eur. Phys. J. B, 2021, 94, 1–11.\n[21] L. Chanussot, A. Das, S. Goyal, T. Lavril, M. Shuaibi, M. Riviere, K. Tran, J. Heras-Domingo, C. Ho,\nW. Hu et al., ACS Catal., 2021, 11, 6059–6072.\n[22] C. E. Tzeliou, M. A. Mermigki and D. Tzeli, Molecules, 2022, 27, 2660.\n[23] J. M. Boereboom, P. Fleurat-Lessard and R. E. Bulo, J. Chem. Theory Comput., 2018, 14, 1841–1852.\n[24] T. Frank, O. Unke and K.-R. Müller, Adv. Neural Inf. Process. Syst., 2022, 35, 29400–29413.\n[25] I. Batatia, L. L. Schaaf, H. Chen, G. Csányi, C. Ortner and F. A. Faber, arXiv preprint\narXiv:2310.10434, 2023.\n[26] J. T. Frank et al., arXiv preprint, 2024.\n[27] M. Gastegger, K. T. Schütt and K.-R. Müller, Chem. Sci., 2021, 12, 11473–11483.\n[28] A. R. Edmonds, Angular momentum in quantum mechanics, Princeton university press, 1996, vol. 4.\n[29] K. S. Thorne, Rev. Mod. Phys., 1980, 52, 299.\n[30] I. Batatia, D. P. Kovacs, G. Simm, C. Ortner and G. Csányi, Adv. Neural Inf. Process. Syst., 2022, 35,\n11423–11436.\n[31] D. P. Kovács, J. H. Moore, N. J. Browning, I. Batatia, J. T. Horton, V. Kapil, W. C. Witt, I.-B.\nMagdău, D. J. Cole and G. Csányi, arXiv preprint arXiv:2312.15211, 2023.\n[32] C. Cai, S. Wang, Y. Xu, W. Zhang, K. Tang, Q. Ouyang, L. Lai and J. Pei, J. Med. Chem., 2020, 63,\n8683–8694.\n[33] D. Buterez, J. P. Janet, S. J. Kiddle, D. Oglic and P. Lió, Nat. Commun., 2024, 15, 1517.\n[34] S. Mai, P. Marquetand and L. González, Wiley Interdiscip. Rev. Comput. Mol. Sci., 2018, 8, e1370.\n[35] T. R. Nelson, A. J. White, J. A. Bjorgaard, A. E. Sifain, Y. Zhang, B. T. Nebgen, S. Fernandez-Alberti,\nD. Mozyrsky, A. E. Roitberg and S. Tretiak, Chem. Rev., 2020, 120, 2215–2287.\n[36] J. Westermayr, S. Ghosh, T. Mori and L. González, Nat. Chem., 2022, 14, 914–919.\n[37] J. Westermayr and P. Marquetand, Chem. Rev., 2021, 121, 9873–9926.\n[38] S. Mausenberger, C. Müller, A. Tkatchenko, P. Marquetand, L. González and J. Westermayr, Chem.\nSci., 2024, 15, 15880–15890.\n[39] M. X. Tiefenbacher, B. Bachmair, C. G. Chen, J. Westermayr, P. Marquetand, J. C. Dietschreit and\nL. González, Excited-state nonadiabatic dynamics in explicit solvent using machine learned interatomic\npotentials, 2025.\n[40] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals and G. E. Dahl, Int. Conf. Mach. Learn., 2017, pp.\n1263–1272.\n[41] P. B. Jørgensen, K. W. Jacobsen and M. N. Schmidt, arXiv preprint arXiv:1806.03146, 2018.\n[42] C. Esteves, arXiv preprint arXiv:2004.05154, 2020.\n[43] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff and P. Riley, arXiv preprint\narXiv:1802.08219, 2018.\n[44] Y. A. Smorodinski˘ı and L. A. Shelepin, Sov. Phys. Usp., 1972, 15, 1.\n[45] A. Vaswani, Adv. Neural Inf. Process. Syst., 2017.\n\n\n18\n[46] L. Boöselt, M. Thürlemann and S. Riniker, J. Chem. Theory Comput., 2021, 17, 2641–2658.\n[47] A. Hofstetter, L. Böselt and S. Riniker, Phys. Chem. Chem. Phys., 2022, 24, 22497–22512.\n[48] G. Song and W. Yang, arXiv preprint, 2025.\n[49] K. Yao, J. E. Herr, D. W. Toth, R. Mckintyre and J. Parkhill, Chem. Sci., 2018, 9, 2261–2269.\n[50] B. Lier, P. Poliak, P. Marquetand, J. Westermayr and C. Oostenbrink, J. Phys. Chem. Lett., 2022, 13,\n3812–3818.\n[51] A. Hofstetter, L. Böselt and S. Riniker, Phys. Chem. Chem. Phys., 2022, 24, 22497–22512.\n[52] K. Zinovjev, J. Chem. Theory Comput., 2023, 19, 1888–1897.\n[53] M. Thürlemann and S. Riniker, Chem. Sci., 2023, 14, 12661–12675.\n[54] K. Zinovjev, L. Hedges, R. Montagud Andreu, C. Woods, I. Tuñón and M. W. van der Kamp, J. Chem.\nTheory Comput., 2024.\n[55] J. A. Semelak, I. Pickering, K. K. Huddleston, J. Olmos, J. S. Grassano, C. Clemente, S. I. Drusin,\nM. Marti, M. C. G. González, A. E. Roitberg et al., 2024.\n[56] J. S. Grassano, I. Pickering, A. E. Roitberg, M. C. González Lebrero, D. A. Estrin and J. A. Semelak,\nJ. Chem. Inf. Model., 2024, 64, 4047–4058.\n[57] Y.-K. Lei, K. Yagi and Y. Sugita, J. Chem. Phys., 2024, 160, 214109.\n[58] B. Lier, P. Poliak, P. Marquetand, J. Westermayr and C. Oostenbrink, J. Phys. Chem. Lett., 2022, 13,\n3812–3818.\n[59] J. C. Tully, J. Chem. Phys., 1990, 92, 1061–1071.\n[60] J. C. Tully, Int. J. Quantum Chem., 1991, 40, 299–309.\n[61] G. Granucci and M. Persico, J. Chem. Phys., 2007, 126, 134114.\n[62] M. Richter, P. Marquetand, J. González-Vázquez, I. Sola and L. González, J. Chem. Theory Comput.,\n2011, 7, 1253–1258.\n[63] S. Mai, D. Avagliano, M. Heindl, P. Marquetand, M. F. S. J. Menger, M. Oppel, F. Plasser, S. Polonius,\nM. Ruckenbauer, Y. Shu, D. G. Truhlar, L. Zhang, P. Zobel and L. González, SHARC3.0: Surface Hop-\nping Including Arbitrary Couplings — Program Package for Non-Adiabatic Dynamics, https://sharc-\nmd.org/, 2023.\n[64] S. Mai, P. Marquetand and L. González, Int. J. Quantum Chem., 2015, 115, 1215–1231.\n[65] R. Drautz, Phys. Rev. B, 2019, 99, 014104.\n[66] I. Batatia, S. Batzner, D. P. Kovács, A. Musaelian, G. N. Simm, R. Drautz, C. Ortner, B. Kozinsky\nand G. Csányi, Nat. Mach. Intell., 2025, 1–12.\n[67] A. D. Becke, Phys. Rev. A, 1988, 38, 3098.\n[68] A. D. Becke, J. Chem. Phys., 1993, 98, 1372–1377.\n[69] S. Grimme, S. Ehrlich and L. Goerigk, J. Comput. Chem., 2011, 32, 1456–1465.\n[70] S. Grimme, J. Chem. Phys., 2006, 124, 034108.\n[71] B. Dunlap, J. Connolly and J. Sabin, J. Chem. Phys., 1979, 71, 4993–4999.\n[72] H. J. Berendsen, J.-R. Grigera and T. P. Straatsma, J. Phys. Chem., 1987, 91, 6269–6271.\n\n\n19\n[73] A. K. Malde et al., J. Chem. Theory Comput., 2011, 7, 4026–4037.\n[74] D. J. Evans and B. L. Holian, J. Chem. Phys., 1985, 83, 4069–4074.\n[75] P. Mark and L. Nilsson, J. Phys. Chem. A, 2001, 105, 9954–9960.\n[76] A. H. Larsen, J. J. Mortensen, J. Blomqvist, I. E. Castelli, R. Christensen, M. Dułak, J. Friis, M. N.\nGroves, B. Hammer, C. Hargus et al., J. Phys. Condens. Matter, 2017, 29, 273002.\n[77] S. Heydari, S. Raniolo, L. Livi and V. Limongelli, Commun. Chem., 2023, 6, 13.\n[78] Y. Shu, L. Zhang, X. Chen, S. Sun, Y. Huang and D. G. Truhlar, J. Chem. Theory Comput., 2022, 18,\n1320–1328.\nAppendix A: MACE Architecture\nTo extend message-passing neural networks (MPNNs) with many-body interactions, the atomic\ncluster expansion (ACE) [65] formulation can be utilized. This expansion incorporates higher-order\nterms, which are essential for accurately modeling complex molecular interactions. The message\nm(t)\ni\nat iteration t is expressed as a sum over many-body contributions:\nm(t)\ni\n=\nX\nj∈N1(i)\nϕ1\n\u0010\nh(t)\ni , h(t)\nj , rij\n\u0011\n+\nX\nj1∈N2(i)\nX\nj2∈N2(j1)\nϕ2\n\u0010\nh(t)\ni , h(t)\nj1 , h(t)\nj2 , rij1, rj1j2\n\u0011\n...\n+\nX\nj1,...,jν∈Nν(i)\nϕν\n\u0010\nh(t)\ni , .., h(t)\njν , rij1, .., rjν−1jν\n\u0011\n,\nwhere Nν(i) denotes the set of ν-th order neighbors of node i, and ϕν represents the learnable\ninteraction function incorporating many-body effects. This operation is computationally expensive\nbut can be significantly reduced by constructing higher-order terms using tensor products of two\nbody features then symmetrizing. The full derivations of the MACE architecture can be seen in\nthe following sources [30, 66].\nAppendix B: Ground State Simulation Details\n1.\nMolecular Datasets\nThe configurations, energies, and forces for the three molecules solvated in water were taken from\nBoöselt et al. [46]. Simulation details are briefly summarized here: All simulations were performed\n\n\n20\nusing a QM/MM approach with the DFT functionals BP86 [67] for retonic acid, ωB97X-D3 [68, 69]\nfor uracil, and B2-PLYP [70] for benzene; the def2-TZVP basis set was used for all calculations. The\nresolution of identity (RI) approximation [71], Grimme’s dispersion correction with Becke–Johnson\ndamping [69], and electrostatic embedding were used to account for long-range interactions. The\nMM region consists of water molecules modeled with the SPC/E water model [72]. The QM/MM\nHamiltonian incorporated MM point charges within a cutoff radius of 0.6 nm for benzene and 1.4 nm\nfor uracil and retinoic acid.\nEach solute was solvated in a periodic water box, with sizes of 1.6 nm, 3.3 nm and 5.0 nm\nrespectively for benzene, uracil and retonic acid. The number of MM partial charges included was\napproximately 1500–2000 for uracil and 2500 for retinoic acid, while benzene had a smaller number\nof MM partial charges of 200 due to the smaller box size. The initial structures were generated\nusing the ATB server [73] and relaxed at 0 K with a gradient descent algorithm until the predicted\nenergy change fell below 0.1 kJ mol−1.\nFor the molecular dynamics simulations that were performed, a time step of 0.5 fs was used,\nwith a Nose–Hoover thermostat [74] (0.1 ps coupling constant) and a weak-coupling barostat (0.5 ps\ncoupling constant). Long-range electrostatics beyond 1.4 nm were included using a reaction-field\nmethod. The system was simulated for 20,000 steps at 400 K and 1 bar, with the first 10,000 steps\ndiscarded for equilibration and the remaining 10,000 were saved.\n2.\nSimulation Details for benzene in water\nThe following parameters were used to generate the power spectrum in Fig. 2a. In the QM/MM\nsimulations, an electrostatic embedding was used. The benzene atoms were treated quantum me-\nchanically, while the surrounding water molecules were treated classically with the TIP3P [75]\nmodel. The simulation was done with the atomic simulation environment (ASE) [76]. The simu-\nlation cell was defined as a box of 16.0 Å per side with periodic boundary conditions applied in\nall directions. The QM region, was evaluated at a B3LYP [67]/def-TZVP level. For the ML/MM\nsimulations of the same system, we used the FieldMACE model trained on the benzene dataset.\nThe model used 128 channels with spherical harmonics up to degree 3. The vectors assigned to the\nMM atoms used spherical harmonics up to degree 3 and with 16 channels.\nThe initial geometry was optimized using the BFGS [76] minimizer until the maximum residual\nforce was reduced to 0.1 eV/A, this was performed in the atomic simulation environment in ASE\n[76]. Initial velocities were drawn from a Maxwell-Boltzmann distribution at 300 K. We performed\n\n\n21\nmolecular dynamics with a Langevin integrator using a 1 fs timestep, a friction coefficient of 10 ps−1,\nand a target temperature of 300 K.The system was propagated for a total of 15,000 steps.\nIn order to facilitate a fair comparison between the ML and QM methods, the benzene data\nset from [46] was recomputed at the same level of theory, B3LYP-D3(BJ)/def-TZVP for the QM\nregion with the TIP3P model for the MM region. The ML model was trained on the recomputed\nframes from this data set (see details for the training in the following Sections). The ML/MM-MD\nproducing the ML-based power spectrum was performed with identical settings as the QM/MM-\nMD.\nAppendix C: Training and Model Details\nIn all systems, a 5 Ångstrom cutoff was applied to the ML region to construct the local graph.\nThe training parameters shown in the FieldMACE section below were also used for the transfer\nlearning models, with the exception that the channel size was fixed to 128 for the transfer learning\n[33, 77] and the short range MACE blocks were initialized using the medium size foundational\nMACE-OFF model parameters for the transfer learning. Additionally, the other blocks such as the\nlong range blocks and the readouts were randomly initialized.\n1.\nFieldMACE\nAll trainings were performed on a single NVIDIA H100 GPU. For the three datasets (see\nSection B.1.)\nwe used all available configurations, which correspond to 10000 frames for ben-\nzene and uracil, and 9719 structures for retinoic acid.\nIn all cases, we performed an 80:10:10\ntrain:validation:test split. For all FieldMACE models, we used two sets of short- and long-range\nlayers. We trained models with varying channel sizes of 8, 16, 32, 64, 128.Additionally, all models\nwere trained with spherical harmonics up to degree 3 and 8 Bessel basis functions. The initial\nvectors for the MM atoms consisted of spherical harmonics up to degree 3, combined with an\nembedding vector of size 16 (not varied between models).\nAll models were trained with the loss function:\nL = cE\nn\nX\ni=1\n\u0000E(i)\nREF −ˆE(i)\u00012+cF\nn\nX\ni=1\n\u0000F(i)\nREF −ˆF(i)\u00012\n(C1)\nThe coefficients cE and cF were both set to 100. The optimization was performed using the default\noptimizer in the MACE library with an initial learning rate of 0.001.\n\n\n22\n2.\nFieldSchNet\nAll models used 6 interaction layers with 8-128 channels. A total of 25 Gaussian basis functions\nwere used to featurize each edge. The learning rate was set to 10−4, with a decay factor of 0.8\ntriggered after 25 training epochs without improvement. Both energy and force terms were assigned\na weight of 1.0 in the loss function, in order to get equal emphasis during optimization. Training\ntook place on a single A30 GPU. The training splits used were identical to FieldMACE.\nAppendix D: Excited State Simulations\n1.\nSurface Hopping Molecular Dynamics\nTrajectory surface hopping (TSH) [59, 60, 62, 64] is a nonadiabatic molecular dynamics ap-\nproach where for each time step nuclear motion evolves classically on a single potential energy\nsurface (PES), while stochastic hops between electronic states occur based on nonadiabatic cou-\npling probabilities. In Tully’s fewest switches surface hopping (FSSH) [59, 60], electronic state\npopulations are propagated via the time-dependent Schrödinger equation,\nd\ndtck(t) = −\nX\nℓ\n\u0014 i\nℏHkℓ+ dkℓ· v\n\u0015\ncℓ(t),\n(D1)\nwhere ck(t) denotes the time-dependent amplitude of electronic state k, Hkℓis the electronic Hamil-\ntonian matrix element of states k and ℓ, dkℓ= ⟨ϕk|∇R|ϕℓ⟩is the nonadiabatic coupling vector\nbetween states k and ℓ, and v is the nuclear velocity vector. The probability of a hop from state k\nto state ℓin a time step ∆t is computed according to\nPk→ℓ= max\n \n0, 2 Re\n\u0000c∗\nncℓ\n\u0002 i\nℏHkℓ+ dkℓ· v\n\u0003\u0001\n|ck|2\n∆t\n!\n.\n(D2)\nAt every time step, a random variable ξ is drawn from the interval [0, 1], and the system tran-\nsitions from state k to state ℓif ξ is less than Pk→ℓ[59]. Since calculating explicit nonadiabatic\ncouplings dkℓis resource-intensive, alternative methods like curvature-driven surface hopping [78]\nhave emerged. The curvature-driven surface hopping was utilized in all surface hopping simulations\nin this study. These techniques enable efficient propagation of surface hopping trajectories while\naccurately incorporating nonadiabatic effects in simulations of excited-state molecular dynamics.\n\n\n23\n2.\nDataset Details\nThe excited-state data set contains calculations of furan solvated in water (Zenodo archive\n10.5281/zenodo.14536036) and was produced for an ML/MM study using FieldSchNet [39]. Com-\nputational details are briefly summarized here: The furan molecule is solvated in a cubix box (side\nlength 15 Å) of TIP3P water (1365 molecules). Initial structures for surface hopping dynamics\nwere obtained from classical MD simulations. These frames were split into two sets for subsequent\nexcited-state QM/MM simulations. One set contains the initial conditions for trajectories used for\nmodel training and validation, the other was used to compare dynamics of the trained model to\nunseen QM/MM simulations. We followed the same scheme to train and evaluate our models. For\nmore details on the initial conditions generation see [39].\n3.\nSimulation and Training Details\nAll nonadiabatic simulations were conducted using an interface between FieldMACE and the\nSHARC engine [62, 63]. For the transfer learning experiments, the short-range blocks were initial-\nized using the medium-size foundational MACE-OFF model parameters. The other blocks, such as\nthe long-range blocks and the readouts, were randomly initialized. Hyperparameters listed below\nwere kept consistent for both the full data set and the smaller subsets used in transfer learning.\nThese smaller datasets were constructed by taking the relevant amount of data randomly from\nthe full dataset. A 5 Ågstrom cutoff radius was applied along with 8 equally spaced Bessel basis\nfunctions. Only two interaction layers were used, with 128 channels and spherical harmonics up to\ndegree 3. The initial vectors for the MM atoms were constructed from spherical harmonics up to\ndegree 3, combined with an embedding vector of up to size 16. The readout functions were modified\nto produce multiple outputs rather than a single node. In this case 3 states were used.\nThe loss function was defined as mean squared error over all states for a property (energy and\nforces)\nL = cE\nnL\nnL\nX\nj=1\nn\nX\ni=1\n\u0000E(i,j)\nREF −ˆE(i,j)\u00012\n+ cF\nnL\nnL\nX\nj=1\nn\nX\ni=1\n\u0000F(i,j)\nREF −ˆF(i,j)\u00012\n(D3)\nwhere cE and cF were both set to 100. The parameter nL represents the number of energy states,\nhere 3. For optimization we used the default optimizer in the MACE library with a learning rate\n\n\n24\nof 0.001. Training was performed on a single NVIDIA H100 GPU. Full details of the parameters\nused for the SHARC simulations can be found in [39].\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21045v1.pdf",
    "total_pages": 24,
    "title": "Incorporating Long-Range Interactions via the Multipole Expansion into Ground and Excited-State Molecular Simulations",
    "authors": [
      "Rhyan Barrett",
      "Johannes C. B. Dietschreit",
      "Julia Westermayr"
    ],
    "abstract": "Simulating long-range interactions remains a significant challenge for\nmolecular machine learning potentials due to the need to accurately capture\ninteractions over large spatial regions. In this work, we introduce FieldMACE,\nan extension of the message-passing atomic cluster expansion (MACE)\narchitecture that integrates the multipole expansion to model long-range\ninteractions more efficiently. By incorporating the multipole expansion,\nFieldMACE effectively captures environmental and long-range effects in both\nground and excited states. Benchmark evaluations demonstrate its superior\nperformance in predictions and computational efficiency compared to previous\narchitectures, as well as its ability to accurately simulate nonadiabatic\nexcited-state dynamics. Furthermore, transfer learning from foundational models\nenhances data efficiency, making FieldMACE a scalable, robust, and transferable\nframework for large-scale molecular simulations.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}