{
  "id": "arxiv_2502.21079v1",
  "text": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation\nYifei Xia1,2\nSuhan Ling1,2\nFangcheng Fu1\nYujie Wang1,2\nHuixia Li2\nXuefeng Xiao2\nBin Cui1\n1Peking University\n2ByteDance\n{yifeixia, lingsuhan}@stu.pku.edu.cn\n{ccchengff, alfredwang, bin.cui}@pku.edu.cn\n{lihuixia, xiaoxuefeng.ailab}@bytedance.com\nCogVideoX1.5-5B, 161 frames, 720p\nHunyuanVideo, 129 frames, 720p\nSparse VideoGen\nPSNR = 27.61\nLatency = 34 min\nMInference\nPSNR = 22.53\nLatency = 42 min\nAdaSpa (ours)\nPSNR = 29.07\nLatency = 30 min\nFull Attention\nLatency = 54 min\nSparse VideoGen\nPSNR = 18.98\nLatency = 34 min\nMInference\nPSNR = 10.31\nLatency = 38 min\nAdaSpa (ours)\nPSNR = 23.25\nLatency = 31 min\nFull Attention\nLatency = 52 min\nPrompt: Clown fish swimming through the coral reef\nPrompt: A cute happy Corgi playing in park, sunset, pan right\nFigure 1. Comparison of the visualization effects of different sparse attention methods on HunyuanVideo [33] and CogVideoX1.5-5B [65].\nOur method AdaSpa consistently achieves the best performance and the best speedup, and keep almost the same as original videos.\nAbstract\nGenerating high-fidelity long videos with Diffusion Trans-\nformers (DiTs) is often hindered by significant latency, pri-\nmarily due to the computational demands of attention mech-\nanisms. For instance, generating an 8-second 720p video\n(110K tokens) with HunyuanVideo takes about 600 PFLOPs,\nwith around 500 PFLOPs consumed by attention computa-\ntions. To address this issue, we propose AdaSpa, the first Dy-\nnamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce\na blockified pattern to efficiently capture the hierarchical\nsparsity inherent in DiTs. This is based on our observation\nthat sparse characteristics of DiTs exhibit hierarchical and\nblockified structures between and within different modalities.\nThis blockified approach significantly reduces the complex-\nity of attention computation while maintaining high fidelity\nin the generated videos. Secondly, to enable Online Pre-\ncise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This\nmethod is motivated by our finding that DiTs’ sparse pattern\nand LSE vary w.r.t. inputs, layers, and heads, but remain\ninvariant across denoising steps. By leveraging this invari-\nance across denoising steps, it adapts to the dynamic nature\nof DiTs and allows for precise, real-time identification of\nsparse indices with minimal overhead. AdaSpa is imple-\nmented as an adaptive, plug-and-play solution and can be\nintegrated seamlessly with existing DiTs, requiring neither\nadditional fine-tuning nor a dataset-dependent profiling. Ex-\ntensive experiments validate that AdaSpa delivers substantial\nacceleration across various models while preserving video\nquality, establishing itself as a robust and scalable approach\nto efficient video generation.\n1. Introduction\nDiffusion models [14, 23, 24, 47, 52] have emerged as a pow-\nerful framework for generative tasks, achieving state-of-the-\nart results across diverse modalities, including text-to-image\nsynthesis [5, 7, 17, 34, 35, 46, 48, 49, 51, 64, 68], realistic\narXiv:2502.21079v1  [cs.CV]  28 Feb 2025\n\n\n2\n8\n16\n32\nVideo Length (s)\n0%\n20%\n40%\n60%\n80%\n100%\nProportion\n0\n2K\n4K\n6K\n8K\nPFLOPs\nHunyuanVideo-Attn\nHunyuanVIdeo-Others\nCogVideoX-Attn\nCogVideoX-Others\nHunyuanVideo-FLOPs\nCogVideoX-FLOPs\nFigure 2. The total FLOPs required and the proportion of attention\nwhen generating 720p videos with different video lengths (16FPS).\nvideo generation [25, 27, 31, 33, 37, 55, 61, 65, 72], and\n3D content creation [6, 9, 26, 28, 43]. Recently, the intro-\nduction of Diffusion Transformers (DiTs) [42], exemplified\nby Sora [4], has set new benchmarks in video generation,\nenabling the production of long, high-fidelity videos.\nDespite these advances, generating high-quality videos\nremains computationally expensive, especially for long\nvideos [8, 16, 22, 54]. The attention mechanism [58] in\nthe Transformer architecture [58], with its O(n2) complex-\nity, is a major bottleneck, where n denotes the sequence\nlength. For instance, generating an 8-second 720p video\nwith HunyuanVideo takes about 600 PFLOPs, with nearly\n500 PFLOPs consumed by attention computations. This pro-\nportion increases with higher resolution or longer duration\nvideos, as illustrated in Figure 2.\nAlthough attention mechanisms are essential for sound\nperformance, they involve significant computational redun-\ndancy [10]. Addressing this redundancy can greatly reduce\ninference costs and accelerate video generation [62]. Sparse\nattention mechanisms [1, 3, 10, 15, 18, 19, 21, 30, 38, 39, 44,\n45, 53, 59, 62, 66, 67, 69], which exploit this redundancy,\nhave shown success in large language models (LLMs) by\nreducing computational costs without compromising perfor-\nmance.\nSparse attention typically characterizes this redundancy\nas sparse patterns (a.k.a. sparse masks), indicating which\ninteractions between tokens can be omitted to reduce com-\nputational load. The specific positions of the selected tokens\nin sparse patterns that are not omitted are called sparse in-\ndices. Based on the flexibility of pattern recognition, existing\nsparse patterns can be broadly categorized into the following\ntwo types:\n• Static Pattern [1, 3, 10, 21, 62, 67] refers to the use of\npredetermined sparse indices that are defined by prior\nknowledge. This category can be further divided into two\ntypes:\nFixed Pattern uses only one fixed sparse pattern based\non empirical experience. For instance, LM-Infinite [21]\nand StreamingLLM [63] (Figure 3a) consistently utilize\nthe sliding window [3] pattern. This approach is straight-\nforward, generally requiring no pattern search, and only\nnecessitates the prior specification of hyperparameters.\nMixed Pattern involves determining several fixed patterns\nbased on experience and then selecting one or more of\nthese patterns during the execution of attention. Exam-\nples include BigBird [67] and Sparse VideoGen [62] (Fig-\nure 3b), which typically perform a rough online switching\nmechanism to estimate and determine which pattern (or\ncombination of patterns) should be applied in each atten-\ntion operation.\n• Dynamic Pattern [19, 30, 38, 44, 45, 53] features ad hoc\nsparse indices that need to be decided in real time. Exam-\nples include DSA [38] and MInference [30] (Figure 3c).\nIt necessitates a search to determine which indices to use\nfor each attention operation. Due to the extensive time\nconsumption involved in searching, current Dynamic Pat-\ntern methods typically rely on offline search and/or online\napproximation search.\nOffline Search\nmethods\ninvolve\nperforming\noffline\nsearches to determine the specific indices. A subset of\nthe target dataset is usually used in the offline search.\nOnline Approximate Search methods involve searching in\nreal-time, yet applying some form of approximation to\nestimate sparse indices during the execution.\nHowever, due to the dynamic complexity and data-\nadaptive nature of DiT patterns, these methods face sig-\nnificant limitations when applied to DiTs.\nFirstly, the Static Pattern is not flexible enough to sum-\nmarize the sparse characteristics of DiTs. In particular, as\nwe will show in Section 3, the sparse patterns of DiTs are\nextremely dynamic and irregular. Thus, static pattern meth-\nods fail to accurately capture the sparse indices and thereby\nsuffer from poor performance (as evaluated in Section 5).\nSecondly, the existing Dynamic Pattern methods are un-\nable to adaptively and accurately identify the sparse pat-\nterns of DiTs. For one thing, our empirical observations\nin Section 3 demonstrate that the sparsity of DiTs exhibits\nconsiderable variation depending on the input, which makes\noffline search in DiTs lack good portability and accuracy.\nFor another, it can be observed that the sparse indices in\nDiTs are complex, with key areas being dispersed and not\nconcentrated and continuous, making it difficult to accu-\nrately estimate sparse indices through approximation search.\nThus, directly applying current dynamic pattern methods\n(e.g., MInference) to DiT also yields poor results (detailed\nin Section 5).\nTherefore, identifying and generalizing sparse patterns\nsuitable for DiTs, and implementing kernel-efficient methods\nfor precise pattern search and attention execution remains an\nurgent problem to be solved.\nMotivated by this, we propose Adaptive Sparse Atten-\n\n\n(a)  StreamingLLM\n(Fixed Pattern)\n(d) AdaSpa\n(Online Precise Search)\n(c) Minference\n(Offline Search + Online Approx. Search)\n(b) Sparse VideoGen\n(Mixed Pattern)\nSink + Window\nDiagonal\nColumn\nBlock\nAttention Sink\nDiagonal\nFirst Frame Sink\nText Sink\nSpatial Head\nTemporal Head\nBlock + Text Sink\nBlock\n① Online\nSwitch\nStatic Patterns\nDynamic Patterns\nFixed sparse indices\nColumn + Diagonal\nColumn\n① Offline \nSearch\nTarget Pattern\n② Online \nApprox. Search\n ① Online \nPrecise Search\nSuboptimal \nDynamic\nsparse indices\nNo Switch\nNo Search\nOptimal\nDynamic\nsparse indices\nProcess\nFixed sparse indices\nFigure 3. Different types of Sparse Pattern recognition methods. (a) StreamingLLM: using a static sink+sliding window pattern, need no\nsearch or switch. (b) Sparse VideoGen: preparing two predefined Static Patterns, and using an online switching method to determine which\nto use. (c) MInference: preparing several dynamic patterns, first do an offline search to determine the target pattern to use, then perform an\nonline approximate search to search suboptimal sparse indices of this pattern. (d) AdaSpa: our method proves that the most suitable pattern\nfor DiT is blockified pattern, and performs an online precise search to find the optimal sparse indices for blockified pattern.\ntion (AdaSpa), the first Dynamic Pattern + Online Precise\nSearch (Figure 3d) method for high-fidelity sparse atten-\ntion. It is a training-free and data-free method designed to\naccelerate video generation in DiTs while preserving gen-\neration quality. It outperforms all other SOTA methods in\nboth Static and Dynamic Patterns, as shown in Figure 1. Our\ncontributions are summarized as follows:\n• Comprehensive Analysis of Attention Sparsity in DiTs.\nWe present an in-depth analysis of sparse characteristics\nin attention mechanisms for DiTs, examining the special\nsparse characteristics of DiTs to reveal optimal sparsity\nstrategies and provide new insights for future research.\nBased on extensive observations and summaries, we found\nthat the sparse characteristics of DiTs have two traits: 1)\nHierarchical and Blockified, 2) Invariant in steps, Adap-\ntive in prompts and heads.\n• First Dynamic Patterns and Precise Online Search\nSparse Attention Solution without Training and Pro-\nfiling. We propose AdaSpa, a novel sparse attention ac-\nceleration framework that is both training-free and data-\nfree. As shown in Figure 3d, AdaSpa is the first effective\nmethod that combines Dynamic Pattern and Online Precise\nSearch, proposing an efficient pipeline for online sparse\npattern search and fine-grained sparse attention computa-\ntion. Leveraging the invariant characteristics across denois-\ning steps, AdaSpa is equipped with Fused LSE-Cached\nOnline Search, which reduces online search time to under\n5% of full attention generation time using our optimized\nkernel, significantly reducing the additional time for search\nwhile ensuring accurate search. Additionally, in order to\nbetter adapt to the sparse characteristics of DiT, we pro-\npose a Head-Adaptive Hierarchical Block Sparse method\nfor AdaSpa to address the head-adaptive sparsity feature\nof DiTs.\n• Implementation and Evaluation.\nAdaSpa provides\na plug-and-play adaspa_attention_handler that seam-\nlessly integrates with DiTs, requiring no fine-tuning or data\nprofiling. It is orthogonal to other acceleration techniques\nlike parallelization, quantization and cache reuse. Exten-\nsive experiments validate AdaSpa’s consistent speedups\nacross models with negligible quality loss.\n2. Preliminaries\n2.1. Diffusion Transformers and 3D Full Attention\nDiffusion Transformers (DiTs) [42] refine predictions with a\ndiffusion process, handling multimodal data like video and\ntext through an attention mechanism that captures spatial,\ntemporal, and cross-modal dependencies. DiTs traditionally\nuse Spatial-Temporal Attention [37, 72], applying spatial\nattention with each video frame, temporal attention across\nall frames, and cross-attention to connect video and text, as\nshown in Figure 4. This separation limits frame continuity\nand fusion.\nFigure 4d illustrates the 3D Full Attention mechanism [27,\n33, 65] in DiTs. It integrates video and text tokens into\n\n\n2-2\n5-2\n2-5\n5-5\n1-1\n4-1\n1-4\n4-4\n5\n2\n3-33-43-5\n1-01-11-5\n2-02-15-5\n3\n4\n1\n5\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n0\n1\n2\n0-00-10-2\n1-01-11-2\n2-02-12-2\n4\n0\n3\n5\n2\n1\n4\n0\n3\n0-0\n3-0\n0-3\n3-3\n0\n1\n2\n3\n4\n5\n6\n7\n0-6\n1-6\n2-6\n3-6\n4-6\n5-6\n0-7\n1-7\n2-7\n3-7\n4-7\n5-7\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n0-0\n1-0\n2-0\n3-0\n4-0\n5-0\n6-0\n7-0\n0-1\n1-1\n2-1\n3-1\n4-1\n5-1\n6-1\n7-1\n0-2\n1-2\n2-2\n3-2\n4-2\n5-2\n6-2\n7-2\n0-3\n1-3\n2-3\n3-3\n4-3\n5-3\n6-3\n7-3\n0-4\n1-4\n2-4\n3-4\n4-4\n5-4\n6-4\n7-4\n0-5\n1-5\n2-5\n3-5\n4-5\n5-5\n6-5\n7-5\n0-6\n1-6\n2-6\n3-6\n4-6\n5-6\n6-6\n7-6\n0-7\n1-7\n2-7\n3-7\n4-7\n5-7\n6-7\n7-7\nVideo Frame1\nVideo Frame2\nText\nQuery\nKey\n(a) Spatial Attention\nKey\nQuery\n(b) Temporal Attention\nKey\nQuery\nKey\nQuery\n(d) 3D Full Attention\n(c) Cross Attention\nFigure 4. Different Attention Mechanisms in DiTs.\na unified sequence and applies self-attention across them.\nOperating in the latent space, DiTs process video frames\nthat have been pre-encoded. Let f be the number of latent\nframes, h × w the spatial resolution of each frame, and t the\ntext token length, with f · h · w ≫t. The total sequence\nlength, L, can be represented as:\nL = f · h · w + t.\n(1)\nThis unified approach enhances modality fusion and boosts\noverall performance.\nDespite the increased computational cost of 3D Full At-\ntention, it marks the future of DiTs, offering superior multi-\nmodal learning compared to Spatial-Temporal Attention.\n2.2. FlashAttention\nIn the self-attention mechanism [58], tokens are pro-\njected into the query, key, and value matrices Q, K, V ∈\nRH×L×D, where H is the number of attention heads, L is\nthe input length, and D is the hidden dimension of each head.\nThe attention weights matrix Wattn ∈RL×L is computed as:\nWattn = softmax\n\u0012QK⊤\n√\nD\n\u0013\n,\n(2)\nwhich quantifies token-to-token interactions across the se-\nquence. To maintain numerical stability during the expo-\nnentiation, the Log-Sum-Exp (LSE) [2] trick is commonly\nemployed. Let Z = QK⊤\n√\nd\nand denote by zj the j-th compo-\nnent of a row z. Then, LSE can be written as:\nLSE(z) = log\nX\nj\nexp(zj)\n= max\nj\nzj + log\nX\nj\nexp\n\u0000zj −max\nk\nzk\n\u0001\n.\n(3)\nUsing this, the safe Softmax can be expressed as:\nSoftmaxsafe(zj) = exp\n\u0000zj −LSE(z)\n\u0001\n,\n(4)\nand the entire dense attention distribution in a numerically\nstable form is:\nWattn = Softmaxsafe(QK⊤\n√\nD\n).\n(5)\nThis operation, however, requires constructing an L × L\nattention matrix, leading to O(L2) time and memory com-\nplexity, which becomes prohibitive for long sequences.\nFlashAttention [12, 13, 50] addresses this issue by per-\nforming attention in a blockwise manner. Instead of storing\nthe full attention matrix, FlashAttention processes smaller\nchunks sequentially. In FlashAttention, attention is com-\nputed for smaller blocks of tokens, and the key idea is to\nperform attention on these blocks without constructing the\nentire attention matrix at once. Specifically, for each block,\nthe attention is computed as:\nW(b)\nattn = online_softmax\n\u0012(QbK⊤\nb )\n√\nD\n\u0013\n,\n(6)\nwhere Qb and Kb represent the query and key matrices\nfor block b, where L ≫b, and online_softmax [41] is a\nblockwise equivalent version of the safe softmax. The result\nis then multiplied by the value matrix for the block, Vb, to\nobtain the final attention output:\nAb = W(b)\nattnVb.\n(7)\nThis block-wise computation significantly reduces the mem-\nory footprint to O(Lb), as only a subset of the full attention\nmatrix is processed at any given time. FlashAttention is\nparticularly effective for large-scale transformers and long-\nsequence tasks, such as 3D Full Attention.\n2.3. Sparse Attention and Sparse Patterns\nAttention mechanisms exhibit inherent sparsity [10], en-\nabling computational acceleration by limiting interactions to\na subset of key-value pairs. Sparse attention reduces com-\nplexity by ignoring interactions where the attention weight\nW(i,j)\nattn\nis small. This principle forms the basis of sparse\nattention.\nFormally, sparse attention is defined by a masking func-\ntion M ∈{0, 1}L×L, which Mij = 1 indicates that token\ni attends to token j, and Mij = 0 removes the interaction.\nThis masking function M is sparse pattern, the indices set of\nMij = 1 is sparse indices, and the proportion of Mij = 0\nis called sparsity. The sparse attention operation is defined\nas:\nAattn = softmax\n\u0012(QK⊤) ⊙M\n√\nD\n\u0013\nV,\n(8)\nwhere ⊙denotes element-wise multiplication. The effective-\nness of a sparse pattern is evaluated using Recall [57], which\n\n\nmeasures how well the sparse pattern preserves the original\ndense attention behavior:\nRecall =\nP\n(i,j)∈sparse indices W(i,j)\nattn\nP\ni,j W(i,j)\nattn\n,\n(9)\nHigher Recall indicates better retention of the original atten-\ntion structure.\n3. Sparse Pattern Characteristic in DiTs\nIn this section, we present the key observations of the sparse\ncharacteristics and opportunities in DiTs that motivate our\nwork.\nObservation 1: DiTs exhibit Hierarchical Structure of\nsparse pattern within and between different Modality, mak-\ning continous patterns unsuitable. As introduced in Sec-\ntion 2, DiTs leverage 3D attention to model spatial and tem-\nporal dependencies across video frames while integrating\ntext tokens for joint attention. Given an input sequence, it\ncomprises video tokens and text tokens, with a total length\nof L = f ·h·w +t (Equation 1). Thus, the attention weights\nmatrix, Wattn ∈RL×L, has a hierarchical organization of\ntext and video tokens. Particularly, as depicted in Figure 5,\nit can be decomposed as follows:\nWattn =\n\u0014Wvideo-video\nWvideo-text\nWtext-video\nWtext-text\n\u0015\n,\n(10)\nwhere:\n• Video-video attention, Wvideo-video ∈R(f·h·w)×(f·h·w),\ncaptures spatial and temporal interactions among video\ntokens.\n• Text-video and text-text attention, Wtext-text ∈Rt×t,\nWtext-video ∈Rt×(f·h·w) and Wvideo-text ∈R(f·h·w)×t,\nmodel interactions involving text tokens, which often serve\nas a global text sink for attention.\nMoreover, within Wvideo-video, attention weights are fur-\nther structured into f × f frame regions:\nWvideo-video =\n\n\nR1,1\nR1,2\n· · ·\nR1,f\nR2,1\nR2,2\n· · ·\nR2,f\n...\n...\n...\n...\nRf,1\nRf,2\n· · ·\nRf,f\n\n,\n(11)\nwhere Ri,j ∈R(h×w)×(h×w) represents interactions be-\ntween the i-th and j-th video frames. As shown in Figure 5,\nthere are clear boundaries between the frames.\nThis hierarchical characteristic makes continuous sparse\npatterns ineffective, as the sparsity structure is no longer\nglobally uninterruptible. In a continuous sparse pattern,\nnonzero elements extend continuously across the entire ma-\ntrix, such as col patterns, where specific columns remain\nactive in all rows, or diag patterns, where nonzero values\nform a diagonal path from one side to the other. However,\ndue to the hierarchical structure of certain attention weight,\ntheir sparse patterns become fragmented rather than main-\ntaining such continuity, making it impossible to describe\nthem using continuous sparse patterns. Nevertheless, while\nthe overall structure lacks continuity, we observe that within\neach frame region, the sparsity pattern remains locally struc-\ntured and can often be well characterized using continuous\npatterns like col or diag.\nThis insight motivates a frame region-wise search strategy\nto capture localized continuous structures and reconstruct\nthe overall sparsity pattern. However, as shown in Figure 5,\nattention distribution varies significantly across different\nframe regions, nonzero weights tend to concentrate in only a\nfew frame regions rather than being evenly distributed. This\nimbalance reduces the effectiveness of the frame region-wise\napproach, as it fails to provide a globally optimized sparse\nrepresentation.\nSolution 1: Using the blockified pattern to describe the\nsparse features of DiT. Although continuous patterns like\ncol or diag do not work well, we find that the sparse pattern\nevolves into a blockified structure globally. For example, as\nshown in Figure 5a, within each frame region, the sparsity\nfollows a col pattern. However, due to weak inter-region\ninteractions, hierarchical sparsity disrupts interlinearly con-\ntinuous col patterns, leading to a blockified structure. As\nobserved in the figure, this blockified characteristic achieves\nbetter Recall, indicating the blockified pattern a more suitable\npattern. Similarly, in Figure 5b, each frame region follows a\nhybrid of diag and col patterns. Yet, due to significant varia-\ntions in inter-frame interactions, the global attention weights\nexhibit a combination of a sliding window pattern and a dis-\ntinct random blockified structure, making it impossible to\ndescribe with standard sparsity patterns. Another example is\nshown in Figure 5c, where individual frame regions lack a\nclear local pattern, while the global attention weights form\na noncontinuous-diag pattern combined with a bottom sink\neffect. As seen in Figure 5c, this characteristic can also be\neffectively modeled using a blockified representation with\nthe best Recall.\nIn summary, due to the hierarchical nature of the DiT\npatterns, conventional continuous patterns fail to provide\nan effective representation. Thus, adopting the blockified\npattern is the optimal choice for capturing the sparsity char-\nacteristics of DiT, because it consistently achieves the best\nrecall, as shown in Figure 5.\nObservation 2: DiTs’ sparse pattern vary w.r.t. inputs,\nlayers and heads, making offline search unsuitable. As\nillustrated in Figure 6a, the sparse patterns in DiTs vary\ndepending on attention head, and layer, which is similar to\nLLMs [30, 38].\nMeanwhile, we observe that the sparse patterns of dif-\nferent prompts also vary significantly. In Figure 6c, we\n\n\nTopK\nCol\nDiag\nDiag+Col\nWeight\n(a)\n(b)\nBlock\nRecall=0.54\nRecall=0.52\nRecall=0.16\nRecall=0.46\nRecall=0.47\nRecall=0.93\nRecall=0.90\nRecall=0.18\nRecall=0.79\nRecall=0.79\nRecall=1.0\nRecall=1.0\nRecall=0.12\nRecall=0.96\nRecall=0.93\n(c)\nFigure 5. Typical attention weight maps from HunyuanVideo. Weight represents the visualization result of attention weights. Topk, Block,\nCol, Diag, Diag+Col represent the visualization results of sparse patterns under sparsity 0.9. The far right shows an enlarged view of the\nattention weights selected from the bottom right corner with a size of (2 ∗h ∗w + t) × (2 ∗h ∗w + t), where a clear hierarchical effect\nbetween frames can be observed. At the same time, there is a distinct boundary between the text modality and the pure video modality,\nexhibiting varying degrees of text sink effect. (720p, 129 frames, block size of the block pattern is 32)\n0\n10\n20\n30\n40\n50\nStep\n0\n5\n10\n15\n20\nHead\n0\n10\n20\n30\n40\n50\n60\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n(a) Sparse pattern’s Recall changes with\nhead and layer, but invariant step.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nStep\n9\n10\n11\n12\n13\n14\n15\n16\n17\nValue\n(b) LSE distribution changes with the varia-\ntion of step.\nprompt1\nseed0\nprompt2\nseed0\nprompt1\nseed42\nprompt2\nseed42\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall Distribution\n(c) Sparse pattern changes with inputs\nFigure 6. (a) Visualization of recall changes with head, layer, and step. Under the condition of fixed sparsity = 0.9, the attention recall of\nHunyuanVideo in the TopK paradigm changes with the variations of Head and Layer, but stay invariant with Step. (b) LSE distribution\namong different steps. We used HunyuanVideo to generate a 720p 8s video and recorded the distribution of LSE at each layer. It is easy to\nsee that as the Step changes, the distribution of LSE remains almost unchanged. (c) Recall Distribution of Different Inputs. We used the best\nsparse pattern obtained from prompt1-seed0 and applied it to different prompts and seeds. The recall decreases when the prompt or seed\nchanges, meaning different inputs do not share the same sparse pattern.\nconducted the following experiment: we searched for the\noptimal sparse pattern for a specific prompt with a fixed spar-\nsity of 0.9. Subsequently, this pattern was directly applied\nto other prompts. We selected various prompts and different\nrandom seeds, and the results revealed that the sparse pat-\ntern optimized for one input is not necessarily optimal for\nother inputs. These observations reveal that the sparse pat-\nterns of different prompts differ significantly, making offline\nsearches likely to fail.\nAnother conventional approach is online approximate\nsearch [30]. However, due to the hierarchical structure and\ndispersed attention distribution described in Observation 1,\nthis method fails to accurately capture the correct sparse in-\ndices, resulting in poor performance within DiT (as evaluated\nin Section 5).\nTherefore, DiT requires a precise online search; how-\never, its prohibitive computational cost makes it impractical,\nwhich is why no prior methods have adopted it.\nSolution 2: DiTs’ sparse pattern and LSE keep invari-\nant in diffusion steps, caching those invariables making\na fast precise online search feasible. DiTs perform an\niterative multi-step denoising process, and we observe an\n\n\nimportant invariance: for a given layer and head, although\nthe specific values of the attention weights change dynami-\ncally across denoising steps, the underlying sparse pattern\nremains consistent throughout the process. Furthermore, we\nstatistically analyze the distribution of the LSE data calcu-\nlated in FlashAttention at different steps within the same\nlayer. The results in Figure 6b show that the distribution of\nLSE remains stable across denoising steps.\nThose similarities between consecutive steps provide an\nopportunity to explore the reuse of sparse patterns and LSE\nto accelerate online search, as detailed in Section 4.\n4. Methodology\nMotivated by those observations, we propose AdaSpa, a\nsparse attention mechanism featuring Dynamic Pattern and\nOnline Precise Search, to accelerate long video generation\nwith DiTs.\n4.1. Problem Formulation\nSection 3 demonstrates that the attention weights of DiTs\ncannot be well represented using patterns such as col or diag\ndue to the discontinuities caused by hierarchical structures,\nwhile the block pattern shows advantages. Thus, to facilitate\nthe online search of dynamic sparse masks, we formulate the\nproblem of how to find the optimal block sparse indices.\nDefinition of Blockified Sparse Attention. Block Sparse\nAttention employs a block-wise attention method similar\nto FlashAttention, with the distinction that Block Sparse\nAttention ignores the computation of certain blocks based on\nits sparse indices, thereby achieving a speedup. Concretely,\npartition the length dimension L into L/B chunks, where\nB is the block size of sparse attention, and define a block-\nlevel sparse pattern MS ∈{0, 1}\nL\nB × L\nB , where S is the\nset of sparse indices of M. By expanding MS to g\nMS ∈\n{0, 1}L×L and applying a large negative bias −c (1 −g\nMS),\nwe can exclude the discarded blocks from the safe Softmax\ncomputation:\nWattn(g\nMS) = Softmaxsafe\n\u0010\nQK⊤\n√\nD\n−c\n\u00001−g\nMS\n\u0001\u0011\n, (12)\nwhere c is sufficiently large.\nOptimal sparse indices. The goal of block sparse atten-\ntion is to retain as much of the attention weights as possible,\nthus to achieve the best Recall.\nWe predefine Wsum_attn as the sum of attention weights\nwithin each block of Wattn:\nWsum_attn =\nB−1\nX\ni=0\nB−1\nX\nj=0\nWattn[B · p + i, B · q + j]\n(13)\nwhere p, q ∈{0, 1, . . . , L\nB −1}, Wsum_attn ∈R\nL\nB × L\nB\nFormally, at a given sparsity, the precise sparse indices\nof block sparse attention can be expressed as:\nS∗= arg min\nS\n\r\rWattn −Wattn(g\nMS)\n\r\r\n= arg max\nS\n\r\rWattn(g\nMS)\n\r\r\n= arg max\nS\n\r\rWsum_attn(MS)\n\r\r\n= arg\nmax\nk∈{1,...,(1−sparsity)( L\nB )2} Wsum_attn[k]\n(14)\nThis indicates that we can obtain the optimal sparse indices\nby calculating Wsum_attn and utilizing topk. We only need\nto calculate the block with index in S∗, while omitting other\nblocks. Thus, under the given sparsity, the complexity can be\nreduced from O(L2d) to O((1 −sparsity)L2d), providing\na significant speedup.\n4.2. Design of Adaptive Sparse Attention\nWe illustrate the overview of AdaSpa in Figure 7. As previ-\nously mentioned, in order to perform a precise search, it is\nnecessary to obtain the complete Wattn, which has a size of\nO(L2). In the context of long video generation, this results\nin significant time and memory overhead. Moreover, since\nthe mask for each attention operation must be determined in\nreal-time, this time consumption is not affordable [30].\nTo address this issue, we exploit the property of DiT’s\nsparse pattern, which exhibits similarity in denoising steps,\nand construct AdaSpa with a two-phase Fused LSE-Cached\nOnline Search and Head-adaptive Hierarchical Block Sparse\nAttention.\nFused LSE-Cached Online Search. The first phase of\nFused LSE-Cached Online Search is a Fused online Search,\nwhich is a two-pass search: the first pass computes the origi-\nnal FlashAttention outputs and stores each row’s LSE, while\nthe second pass uses the previously generated LSE to com-\npute Wsum_attn in a block-wise manner fused with FlashAt-\ntention.\nThe second phase is an LSE-Cached online Search, which\nonly contains one pass. Due to the similarity of LSE in steps,\nwe directly use the LSE obtained from the Fused online\nSearch to calculate Wsum_attn, thereby saving one pass\nof search time and further reducing the search time by half.\nAlgorithm 1 and 2 demonstrate the pseudocode of our precise\nonline search.\nHead-adaptive Hierarchical Block Sparse Attention.\nFigure 6a shows that not all attention heads share the same\nsparsity characteristics. A single uniform sparsity across all\nheads is often suboptimal because certain heads may func-\ntion well with fewer retained blocks, while others require\nmore. However, if each head employs a totally distinct spar-\nsity level, it will cause huge search time and lead to severe\nkernel load imbalance that significant wastage of computa-\ntional resources.\n\n\nFull Attention\nFused Online \nSearch\nHead-adaptive\nHierarchical Block Sparse Attention\nLSE-Cached \nOnline Search\nHead-adaptive\nHierarchical Block Sparse Attention\nBlock mask\nCached LSE\nWarmup \nBlock mask\nSparse Attention\nStep\nLayer\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nFigure 7. Overview of AdaSpa. We define a warm-up step Tw = {1, 2, ..., tw}, and select k steps: Ts = {t1\ns, t2\ns, ..., tk\ns} to perform a precise\nonline search, with t1\nkey = tw. Initially, during steps 1 to tw −1, we use full attention. At step tw, we apply Fused Online Search to do full\nattention and thereby compute block mask, which is then passed to the subsequent steps t1\nkey + 1, t1\nkey + 2, . . . , t2\nkey −1 for Head-adaptive\nHierarchical Block Sparse Attention. Subsequently, for each ti\nkey, where i > 1, we leverage the Cached LSE from the previous t1\nkey\nsearch to perform the LSE-Cached Online Search, thereby obtaining a new mask. This new mask is then passed to the subsequent steps\nti\nkey, ti\nkey + 1, ti\nkey + 2, . . . , ti+1\nkey −1 for Head-adaptive Hierarchical Block Sparse Attention computation.\nAlgorithm 1 Fused Online Search\n1: Input: Q, K, V ,\n2: Output: LSE, Out, Wsum_attn\n3: Initialize: lse ←−∞, row_max ←1, acc ←0\n4: Load query block in parallel: q ←Q[current block]\n5: // First Pass: Compute FlashAttention outputs and store\nLSE.\n6: for each key block k ∈K, value block v ∈V do\n7:\nqk ←Dot(q, k)\n8:\nrow_max ←update(row_max, qk)\n9:\np ←online_softmax(row_max, qk)\n10:\nlse+ = Sum(p, −1)\n11:\nacc ←Dot(p, v, acc)\n12: end for\n13: LSE ←Log(lse) + row_max\n14: Out ←acc\n15: // Second Pass: Use cached LSE to compute Wsum_attn\nand reduce time.\n16: for each key block k ∈K do\n17:\nqk ←Dot(q, k)\n18:\np ←Log(qk −LSE)\n19:\np_sum = Sum(p)\n20:\nStore p_sum to coresponding position in Wsum_attn\n21: end for\n22: Return: LSE, Out, Wsum_attn\nTo utilize the head adaptive feature while mitigate\nwastage of computational resources, we employ a hierar-\nchical search and calculation strategy. Specifically, we start\nby fixing a given sparsity and computing the Recall for each\nAlgorithm 2 LSE-Cached Online Search\n1: Input: Q, K, LSE,\n2: Output: Wsum_attn\n3: Load query block in parallel: q ←Q[current block]\n4: // Only one pass;\nuse cached LSE to compute\nWsum_attn.\n5: for each key block k ∈K do\n6:\nqk ←Dot(q, k)\n7:\np ←Log(qk −LSE)\n8:\np_sum = Sum(p)\n9:\nStore p_sum to coresponding position in Wsum_attn\n10: end for\n11: Return: Wsum_attn\nhead. We then sort all heads according to their respective Re-\ncall. Let n denote the number of heads whose Recall exceeds\n0.8, which is a well-known fine Recall to a sparse attention.\nNext, we increase the sparsity of the n heads with the high-\nest Recall to 1+sparsity\n2\n, and we decrease the sparsity of the n\nheads with the lowest Recall to 3×sparsity−1\n2\n. This hierarchi-\ncal head-adaptive procedure effectively reduces redundancy\namong heads exhibiting higher Recall while improving the\nprecision of those with lower Recall. Consequently, we\nachieve elevated accuracy without altering the average spar-\nsity, thus realizing a head-adaptive mechanism.\n4.3. Implementation\nAdaSpa is implemented with over 2,000 lines of Python and\n1000 lines of Triton [56] codes. It is provided as a plug-\nand-play interface, as shown in Figure 8. Users can enable\n\n\n1  from adaspa import adaspa_attention_handler\n2  # Suppose q, k, v each has shape: [batch_size, head_num, \nseq_len, head_dim]\n3  # One can simply use AdaSpa by replacing origin attention \nwith sparse attention from Adaspa:\n4  q, k, v = get_qkv(hidden_states, qkv_weight)\n    - out = original_attention(query=q, key=k, value=v)\n5  + out = adaspa_attention_handler(query=q, key=k, value=v)\n6  return out\nFigure 8. Minimal usage of AdaSpa.\nAdaSpa with only a one-line change. We use sparsity=0.8,\nblock_size=64, Ts = {10, 30} as the default configuration.\nWe implement our Head-adaptive Hierarchical Block Sparse\nAttention based on Block-Sparse-Attention [20]. Unless\notherwise noted, all other attention mechanisms employ\nFlashAttention 2 [12].\nIn addition, we employ two optimization techniques for\nbetter efficiency. (1) Text Sink. We manually select all the\nindices of video-text, text-video, and text-text parts, which\ncan enhance video modality’s perception to text modality,\nthereby achieving better results. (2) Row Wise. We find that\nensuring each query attends to roughly the same number of\nkeys can improve continuity in generated videos. Otherwise,\ncertain regions deemed “unimportant” might never be at-\ntended to, producing artifacts. Hence, we enforce a per-row\nuniform selection in our block sparse pattern.\n5. Experiments\nModels. We experiment with two state-of-the-art open-\nsource models, namely HunyuanVideo (13B) [33] and\nCogVideoX1.5-5B [65].\nWe generate 720p, 8-second\nvideos for HunyuanVideo, 720p and 10-second videos for\nCogVideoX1.5-5B, with 50 steps for both of these models.\nBaselines. We compare AdaSpa with Sparse VideoGen [62]\n(static pattern) and Minference [30] (dynamic pattern). In\naddition, we also consider two variants of AdaSpa to assess\nthe effectiveness of the proposed methods: (1) AdaSpa (w/o\nhead adaptive), with uses the same sparsity for all heads,\nand (2) AdaSpa (w/o lse cache), which does not leverage\nthe LSE-Cached method. For all methods, the first 10 steps\ngenerate with full attention for warmup.\nDatasets. For all the experiments, we use the default dataset\nfrom VBench [29] for testing. Specially, for CogVideoX1.5-\n5B, we use VBench dataset after applying prompt optimiza-\ntion, following the guidelines provided by CogVideoX [65].\nMetrics. To evaluate the performance of our video genera-\ntion model, we employ several widely recognized metrics\nthat assess both the quality and perceptual similarity of the\ngenerated videos. Following previous works [32, 36, 71],\nwe utilize Peak Signal-to-Noise Ratio [11] (PSNR), Learned\nPerceptual Image Patch Similarity [70] (LPIPS), and Struc-\ntural Similarity Index Measure [60] (SSIM) to evaluate the\nsimilarity of generated videos. As for video quality, we\nintroduce the VBench Score [29], which provides a more\ncomprehensive evaluation by considering both pixel-level\naccuracy and perceptual consistency across frames. For\nefficiency, we report latency and speedup, where both are\nmeasured using a single A100 GPU-80GB.\n5.1. Main Results\nIn Table 1, we present a comprehensive evaluation of\nAdaSpa, comparing it with various baseline methods across\nboth quality and efficiency metrics.\nWe observe that AdaSpa consistently achieves the best\nperformance in both quality and efficiency across all experi-\nments. On HunyuanVideo, AdaSpa ranks first in most met-\nrics and achieves the highest speedup of 1.78×. In contrast,\nboth Sparse VideoGen and MInference show suboptimal\nresults, with speedups of 1.58× and 1.27×, respectively. On\nCogVideoX1.5-5B, AdaSpa delivers the best performance\nacross all quality metrics and achieves a speedup of 1.66×,\nthe highest among the evaluated methods.\nMInference, due to its reliance on online approximate\nsearch, struggles to accurately capture the precise sparse\nindices, leading to the lowest accuracy. Moreover, because\nof the dispersed characteristic of sparse patterns in DiT, the\npatterns obtained through approximate search exhibit a lower\ntrue sparsity, resulting in slower performance with speedups\nof only 1.27× and 1.39×. Sparse VideoGen, which lever-\nages a static pattern that is specifically designed for DiT,\nperforms relatively well, as it can capture some optimal\nsparse patterns for specific heads. However, due to its inabil-\nity to dynamically capture accurate sparse patterns for all\nheads, it fails to outperform AdaSpa in all accuracy metrics.\nFor the two variants of AdaSpa, AdaSpa (w/o head\nadaptive) shows worse performance in terms of quality\nmetrics, providing strong evidence of the effectiveness of\nhead-adaptive sparsity. Additionally, AdaSpa (w/o LSE\ncache) generally performs worse or on par with AdaSpa\nacross most metrics. Due to slower search speeds, it only\nachieves speedups of 1.71× and 1.60× on Hunyuan and\nCogVideoX1.5-5B, respectively, both lower than AdaSpa’s\nperformance. This further corroborates the effectiveness\nof LSE-Cached Search and our Head-adaptive Hierarchical\nmethod in enhancing speedup and quality.\n5.2. Ablation Study\nQuality-Sparsity trade-off. In Figure 9, we compare the\nquality metrics of AdaSpa with MInference and Sparse\n\n\nMethod\nQuality Metrics\nEfficiency Metrics\nVBench (%) ↑\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nLatency (s)\nSpeedup\nHunyuanVideo\n80.10\n-\n-\n-\n3213.76\n1.00×\n+ MInference\n79.17\n22.53\n0.7435\n0.3550\n2532.80\n1.27×\n+ Sparse VideoGen\n79.39\n27.61\n0.8683\n0.1703\n2035.59\n1.58×\n+ AdaSpa (w/o head adaptive)\n79.64\n28.51\n0.8825\n0.1574\n1823.34\n1.76×\n+ AdaSpa (w/o lse cache)\n80.16\n28.97\n0.8898\n0.1481\n1877.13\n1.71×\n+ AdaSpa (ours)\n80.13\n29.07\n0.8905\n0.1478\n1810.23\n1.78×\nCogVideoX1.5\n81.16\n-\n-\n-\n3135.24\n1.00×\n+ MInference\n65.30\n10.31\n0.3113\n0.6820\n2258.35\n1.39×\n+ Sparse VideoGen\n79.40\n18.98\n0.6465\n0.3632\n2061.42\n1.52×\n+ AdaSpa (w/o head adaptive)\n81.54\n22.99\n0.8133\n0.2203\n1915.88\n1.64×\n+ AdaSpa (w/o lse cache)\n81.73\n23.14\n0.8255\n0.2091\n1961.71\n1.60×\n+ AdaSpa (ours)\n81.90\n23.25\n0.8267\n0.2067\n1888.14\n1.66×\nTable 1. Quantitative evaluation of quality and latency for AdaSpa and other methods.\n0.7\n0.8\n0.9\nSparsity\n0.700\n0.725\n0.750\n0.775\n0.800\nVBench\n0.7\n0.8\n0.9\nSparsity\n15\n20\n25\n30\nPSNR\n0.7\n0.8\n0.9\nSparsity\n0.6\n0.7\n0.8\n0.9\nSSIM\n0.7\n0.8\n0.9\nSparsity\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLPIPS\nours\nSparse VideoGen\nMInference\nFigure 9. Quality-Sparsity trade off.\nVideoGen at different sparsity levels. As observed in the\nVBench metric, which measures video quality, AdaSpa con-\nsistently maintains the highest video quality across all spar-\nsity levels, with no significant degradation as sparsity in-\ncreases. In contrast, both Sparse VideoGen and MInfer-\nence experience a considerable drop in quality as sparsity\nincreases. This demonstrates that AdaSpa is capable of\npreserving critical information as much as possible under\nlimited sparsity, thereby ensuring the reliability of video\nquality.\nSimilarly, in the PSNR, SSIM, and LPIPS metrics, which\nmeasure the similarity between the videos generated with\nand without sparse attention, a consistent trend is observed:\nas sparsity increases, the similarity for all video methods\ndeclines. However, AdaSpa maintains significantly higher\nsimilarity compared to other methods, with a gradual linear\ndecrease as sparsity increases. This is in stark contrast to the\nabrupt decline observed in MInference.\nWarmup. As mentioned in many previous works [32, 40,\n62], warmup can significantly enhance the similarity and sta-\nbility of video generation. Therefore, we compared the video\nquality and similarity of AdaSpa, MInference, and Sparse\nVideoGen under different warmup setups in Figure 10. It\n0\n2\n5\n10\nWarmup Steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVBench\n0\n2\n5\n10\nWarmup Steps\n0\n10\n20\n30\nPSNR\nours\nSparse VideoGen\nMInference\nFigure 10. The impact of different warmup steps for AdaSpa,\nSparse VideoGen, and MInference.\ncan be seen that as warmup decreases, the similarity of all\nmethods also decreases, which is consistent with the con-\nclusions of previous works. However, we find that as the\nwarmup period increases, AdaSpa still achieves the best per-\nformance across all setups. Additionally, the video quality\nfor all methods does not show significant improvement with\nthe increase in warmup, remaining almost unchanged. This\nsuggests that warmup has minimal impact on the quality of\nvideo generation itself and primarily affects the similarity\nbetween the generated video and the original video.\n\n\nTable 2. The impact of different Search Strategies for AdaSpa.\nSearch Strategy (Ts)\nPSNR ↑\nSSIM ↑\nLPIPS ↓\n{10}\n28.9629\n0.8879\n0.1509\n{10, 30}\n29.0749\n0.8905\n0.1478\n{10, 20, 30}\n28.9343\n0.8894\n0.1500\n{10, 20, 30, 40}\n28.9313\n0.8898\n0.1494\n3.05x\n3.65x\n2.79x\n4.01x\n2.01x\nFigure 11. Scaling test of AdaSpa.\nSearch Strategy. To verify the impact of our search strat-\negy on video generation, we evaluate AdaSpa on video qual-\nity and similarity with different search strategies, as shown\nin Table 2. The results indicate that increasing the number of\nsearches might be beneficial for improving accuracy, yet to\na limited extent. When the number of searches reaches a cer-\ntain threshold, further increasing the number of searches may\neven lower the video generation quality. This sufficiently\ndemonstrates that the patterns between steps have a strong\nsimilarity, and as the number of searches increases, the video\nquality may actually decline. This suggests that the impact\nof sparse attention has a certain transmissibility and may\naffect subsequent steps, which will be further explored in\nfuture work.\n5.3. Scaling Study\nTo further assess the scalability of our method, we tested the\ngeneration time for videos of different lengths under the con-\nfiguration of sparsity=0.9, block_size=64, and Ts = {0, 30}.\nAs shown in Figure 11, as the length of the generated video\nincreases, AdaSpa’s speedup continues to improve, ulti-\nmately reaching a speedup of 4.01× when the video length\nis 24 seconds. This demonstrates the excellent scalability of\nour method.\n6. Conclusion\nIn this work, we comprehensively analyze the sparse\ncharacteristics in the attention mechanisms when generating\nvideos with DiTs. Based on the observations and analyses,\nwe develop AdaSpa, a brand new sparse attention approach\nfeaturing dynamic pattern and online precise search, to\naccelerate long video generation. Empirical results show\nthat AdaSpa achieves a 1.78× of efficiency improvement\nwhile maintaining high quality in the generated videos.\nReferences\n[1] Shantanu Acharya, Fei Jia, and Boris Ginsburg. Star attention:\nEfficient llm inference over long sequences. arXiv preprint\narXiv:2411.17116, 2024. 2\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate, 2016. 4\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer.\narXiv preprint\narXiv:2004.05150, 2020. 2\n[4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei\nGuo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric\nLuhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.\nVideo generation models as world simulators. 2024. 2\n[5] Chenjie Cao, Yunuo Cai, Qiaole Dong, Yikai Wang, and\nYanwei Fu.\nLeftrefill: Filling right canvas based on left\nreference through generalized text-to-image diffusion model.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7705–7715, 2024. 1\n[6] Cheng Chen, Xiaofeng Yang, Fan Yang, Chengzeng Feng,\nZhoujie Fu, Chuan-Sheng Foo, Guosheng Lin, and Fayao Liu.\nSculpt3d: Multi-view consistent text-to-3d generation with\nsparse 3d prior. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10228–\n10237, 2024. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and\ncontrollable image generation with latent consistency models,\n2024. 1\n[8] Jingyuan Chen, Fuchen Long, Jie An, Zhaofan Qiu, Ting Yao,\nJiebo Luo, and Tao Mei. Ouroboros-diffusion: Exploring con-\nsistent content generation in tuning-free long video diffusion.\narXiv preprint arXiv:2501.09019, 2025. 2\n[9] Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, and Tao Mei.\nVp3d: Unleashing 2d visual prompt for text-to-3d generation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4896–4905, 2024. 2\n[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 2, 4\n[11] OpenCV Contributors. Opencv: Open source computer vision\nlibrary, 2025. Accessed: 2025-02-26. 9\n[12] Tri Dao.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning.\narXiv preprint\narXiv:2307.08691, 2023. 4, 9\n[13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo-\npher Ré. Flashattention: Fast and memory-efficient exact\nattention with io-awareness. Advances in Neural Information\nProcessing Systems, 35:16344–16359, 2022. 4\n[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780–8794, 2021. 1\n\n\n[15] Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang,\nZhijie Deng, Ion Stoica, and Hao Zhang. Efficient-vdit: Effi-\ncient video diffusion transformers with attention tile. arXiv\npreprint arXiv:2502.06155, 2025. 2\n[16] Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, and Xibo\nSun. Pipefusion: Patch-level pipeline parallelism for diffusion\ntransformers inference. arXiv preprint arXiv:2405.14430,\n2024. 2\n[17] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, and Tat-\nSeng Chua. Dysen-vdm: Empowering dynamics-aware text-\nto-video diffusion with llms. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 7641–7653, 2024. 1\n[18] Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang,\nBoju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao\nLi, Shengen Yan, et al. Moa: Mixture of sparse attention for\nautomatic large language model compression. arXiv preprint\narXiv:2406.14909, 2024. 2\n[19] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden\nKwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. Seerat-\ntention: Learning intrinsic sparse attention in your llms. arXiv\npreprint arXiv:2410.13276, 2024. 2\n[20] Junxian Guo, Haotian Tang, Shang Yang, Zhekai Zhang, Zhi-\njian Liu, and Song Han. Block Sparse Attention. https:\n//github.com/mit- han- lab/Block- Sparse-\nAttention, 2024. 9\n[21] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji,\nand Sinong Wang. Lm-infinite: Simple on-the-fly length\ngeneralization for large language models. arXiv preprint\narXiv:2308.16137, 2023. 2\n[22] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. arXiv preprint arXiv:2211.13221,\n2022. 2\n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 1\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020. 1\n[25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[26] Lukas Höllein, Aljaž Božiˇc, Norman Müller, David Novotny,\nHung-Yu Tseng, Christian Richardt, Michael Zollhöfer, and\nMatthias Nießner. Viewdiff: 3d-consistent image generation\nwith text-to-image models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n5043–5052, 2024. 2\n[27] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 2, 3\n[28] Tianyu Huang, Yihan Zeng, Zhilu Zhang, Wan Xu, Hang\nXu, Songcen Xu, Rynson WH Lau, and Wangmeng Zuo.\nDreamcontrol: Control-based text-to-3d generation with 3d\nself-prior. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5364–5373,\n2024. 2\n[29] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang\nSi, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang\nJin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin\nWang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Com-\nprehensive benchmark suite for video generative models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2024. 9\n[30] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui\nWu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H Abdi,\nDongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accel-\nerating pre-filling for long-context llms via dynamic sparse\nattention. arXiv preprint arXiv:2407.02490, 2024. 2, 5, 6, 7,\n9\n[31] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si,\nDahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Video-\nbooth: Diffusion-based video generation with image prompts.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6689–6700, 2024. 2\n[32] Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu,\nMenglin Jia, Chenyang Zhang, Michael S Ryoo, and Tian Xie.\nAdaptive caching for faster video generation with diffusion\ntransformers. arXiv preprint arXiv:2411.02397, 2024. 9, 10\n[33] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\n1, 2, 3, 9\n[34] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2024. 1\n[35] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai,\nJunjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion:\nDistributed parallel inference for high-resolution diffusion\nmodels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 7183–7193,\n2024. 1\n[36] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu\nLi, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu,\nand Song Han.\nSvdqunat: Absorbing outliers by low-\nrank components for 4-bit diffusion models. arXiv preprint\narXiv:2411.05007, 2024. 9\n[37] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu,\nShaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Li-\nuhan Chen, et al. Open-sora plan: Open-source large video\ngeneration model. arXiv preprint arXiv:2412.00131, 2024. 2,\n3\n[38] Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, and Yuan\nXie. Transformer acceleration with dynamic sparse attention.\narXiv preprint arXiv:2110.11299, 2021. 2, 5\n[39] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang,\nChao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi\nWang, et al. Moba: Mixture of block attention for long-\ncontext llms. arXiv preprint arXiv:2502.13189, 2025. 2\n[40] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache:\nAccelerating diffusion models for free. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 15762–15772, 2024. 10\n\n\n[41] Maxim Milakov and Natalia Gimelshein. Online normalizer\ncalculation for softmax, 2018. 4\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 2\n[44] Yifan Pu, Zhuofan Xia, Jiayi Guo, Dongchen Han, Qixiu Li,\nDuo Li, Yuhui Yuan, Ji Li, Yizeng Han, Shiji Song, et al.\nEfficient diffusion transformer with step-wise dynamic atten-\ntion mediators. In European Conference on Computer Vision,\npages 424–441. Springer, 2025. 2\n[45] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih,\nSinong Wang, and Jie Tang. Blockwise self-attention for long\ndocument understanding. arXiv preprint arXiv:1911.02972,\n2019. 2\n[46] Leigang Qu, Wenjie Wang, Yongqi Li, Hanwang Zhang,\nLiqiang Nie, and Tat-Seng Chua. Discriminative probing\nand tuning for text-to-image generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7434–7444, 2024. 1\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 1\n[48] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas\nBlattmann, Patrick Esser, and Robin Rombach. Fast high-\nresolution image synthesis with latent adversarial diffusion\ndistillation. In SIGGRAPH Asia 2024 Conference Papers,\npages 1–11, 2024. 1\n[49] Idan Schwartz, Vésteinn Snæbjarnarson, Hila Chefer, Serge\nBelongie, Lior Wolf, and Sagie Benaim. Discriminative class\ntokens for text-to-image diffusion models. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 22725–22735, 2023. 1\n[50] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar,\nPradeep Ramani, and Tri Dao. Flashattention-3: Fast and ac-\ncurate attention with asynchrony and low-precision. Advances\nin Neural Information Processing Systems, 37:68658–68685,\n2025. 4\n[51] Takahiro Shirakawa and Seiichi Uchida. Noisecollage: A\nlayout-aware text-to-image diffusion model based on noise\ncropping and merging. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8921–8930, 2024. 1\n[52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 1\n[53] Xin Tan, Yuetao Chen, Yimin Jiang, Xing Chen, Kun Yan,\nNan Duan, Yibo Zhu, Daxin Jiang, and Hong Xu.\nDsv:\nExploiting dynamic sparsity to accelerate large-scale video\ndit training. arXiv preprint arXiv:2502.07590, 2025. 2\n[54] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao\nWang. Video-infinity: Distributed long video generation.\narXiv preprint arXiv:2406.16260, 2024. 2\n[55] Genmo Team.\nMochi 1.\nhttps://github.com/\ngenmoai/models, 2024. 2\n[56] Philippe Tillet, H. T. Kung, and David Cox. Triton: an in-\ntermediate language and compiler for tiled neural network\ncomputations. In Proceedings of the 3rd ACM SIGPLAN\nInternational Workshop on Machine Learning and Program-\nming Languages, page 10–19, New York, NY, USA, 2019.\nAssociation for Computing Machinery. 8\n[57] Marcos Treviso, António Góis, Patrick Fernandes, Erick Fon-\nseca, and André F. T. Martins. Predicting attention sparsity in\ntransformers, 2022. 4\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2, 4\n[59] Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin,\nand Xiaodan Liang. Qihoo-t2x: An efficient proxy-tokenized\ndiffusion transformer for text-to-any-task. arXiv preprint\narXiv:2409.04005, 2024. 2\n[60] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.\nImage quality assessment: from error visibility to structural\nsimilarity. IEEE Transactions on Image Processing, 13(4):\n600–612, 2004. 9\n[61] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 2\n[62] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu,\nMuyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang,\nDacheng Li, et al.\nSparse videogen: Accelerating video\ndiffusion transformers with spatial-temporal sparsity. arXiv\npreprint arXiv:2502.01776, 2025. 2, 9, 10\n[63] G Xiao, Y Tian, B Chen, S Han, and M Lewis. Efficient\nstreaming language models with attention sinks, 2023. URL\nhttp://arxiv. org/abs/2309, 17453, 2023. 2\n[64] Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang,\nTianyang Hu, Enze Xie, and Zhenguo Li. Accelerating diffu-\nsion sampling with optimized time steps. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8292–8301, 2024. 1\n[65] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 1, 2, 3, 9\n[66] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang\nZhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang,\nZhiping Xiao, et al.\nNative sparse attention: Hardware-\naligned and natively trainable sparse attention. arXiv preprint\narXiv:2502.11089, 2025. 2\n[67] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham,\nAnirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Trans-\nformers for longer sequences. Advances in neural information\nprocessing systems, 33:17283–17297, 2020. 2\n\n\n[68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 3836–3847, 2023. 1\n[69] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang\nDing, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast\nvideo generation with sliding tile attention. arXiv preprint\narXiv:2502.04507, 2025. 2\n[70] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric, 2018. 9\n[71] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-\ntime video generation with pyramid attention broadcast. arXiv\npreprint arXiv:2408.12588, 2024. 9\n[72] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all. arXiv preprint arXiv:2412.20404, 2024. 2, 3\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21079v1.pdf",
    "total_pages": 14,
    "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
    "authors": [
      "Yifei Xia",
      "Suhan Ling",
      "Fangcheng Fu",
      "Yujie Wang",
      "Huixia Li",
      "Xuefeng Xiao",
      "Bin Cui"
    ],
    "abstract": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}