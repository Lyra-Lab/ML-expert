{
  "id": "arxiv_2502.21086v1",
  "text": "Are foundation models useful feature extractors\nfor electroencephalography analysis?\nÖzgün Turgut1,2\n, Felix S. Bott2,3\n, Markus Ploner2,3,4\n, and\nDaniel Rueckert1,2,5,6\n1 School of Computation, Information and Technology, Technical University of Munich,\nGermany\n{oezguen.turgut,daniel.rueckert}@tum.de\n2 School of Medicine and Health, Klinikum rechts der Isar, Technical University of\nMunich, Germany\n3 Department of Neurology, Technical University of Munich, Germany\n4 Center for Interdisciplinary Pain Medicine, Technical University of Munich, Germany\n5 Munich Center for Machine Learning (MCML), Munich, Germany\n6 Department of Computing, Imperial College London, UK\nAbstract. The success of foundation models in natural language pro-\ncessing and computer vision has motivated similar approaches for general\ntime series analysis. While these models are effective for a variety of tasks,\ntheir applicability in medical domains with limited data remains largely\nunexplored. To address this, we investigate the effectiveness of foundation\nmodels in medical time series analysis involving electroencephalography\n(EEG). Through extensive experiments on tasks such as age prediction,\nseizure detection, and the classification of clinically relevant EEG events,\nwe compare their diagnostic accuracy with that of specialised EEG mod-\nels. Our analysis shows that foundation models extract meaningful EEG\nfeatures, outperform specialised models even without domain adaptation,\nand localise task-specific biomarkers. Moreover, we demonstrate that\ndiagnostic accuracy is substantially influenced by architectural choices\nsuch as context length. Overall, our study reveals that foundation models\nwith general time series understanding eliminate the dependency on large\ndomain-specific datasets, making them valuable tools for clinical practice.\nKeywords: Foundation models · Electroencephalography.\n1\nIntroduction\nRecent breakthroughs in natural language processing and computer vision have\nshown the effectiveness of foundation models on a wide range of tasks. Inspired by\nthis success, a growing number of works have focused on developing similar models\nfor time series analysis [5,10,11,14,20,28,29,32]. However, most of these models are\ndesigned for only a single task like forecasting [5,20,29] or classification [14,32].\nRecent works [10,11,28] have introduced general foundation models that are\neffective on a variety of tasks, including classification, forecasting, and regression.\nParticularly, the open model for general time series analysis (OTiS) [28] has\narXiv:2502.21086v1  [cs.AI]  28 Feb 2025\n\n\n2\nÖ. Turgut et al.\nFoundation\nModel\nFrequency\ndecomposition\nElectroencephalography (EEG)\nRegression\nClassification\nFC\nFC\n...\nModel\nEEG feature space\nTask\nAge : 31\nSeizure :\nFig. 1: Overview. We study the potential of general foundation models to extract\ndemographic and disease-related information from electroencephalography signals.\ndemonstrated a strong understanding of general time series properties including\nfrequency, amplitude, offset, and phase shift. This raises the question of whether\nsuch an understanding of general properties could be translated into a medical\ncontext, potentially benefiting clinical modalities with limited data.\nElectroencephalography (EEG) is a widely accessible and cost-effective modal-\nity used to assess the electrical activity of the brain. Its broadband signal typically\ncontains frequency components in the range of 0.5 −100 Hz, which can be de-\ncomposed into distinct frequency bands using Butterworth filtering [4]. These\ninclude the delta (δ: 0.5 −4 Hz), theta (θ: 4 −8 Hz), alpha (α: 8 −13 Hz), beta (β:\n13 −30 Hz), and gamma (γ: 30 −100 Hz) band. [21] Despite its accessibility, large\nEEG-specific datasets with more than 10 k samples [22] are limited, rendering\nthe learning of general EEG features difficult. Consequently, EEG models have\nmainly remained task-specific, with unique architectures designed and trained\nfor tasks such as motor imagery classification [1,18], sleep stage classification\n[7,8], or emotion recognition [13,17]. This highlights the need for new strategies\nthat are independent of expensive domain and task-specific knowledge to fur-\nther advance EEG analysis. Here, foundation models like OTiS [28] may offer a\npromising solution, given their general time series understanding gained through\npre-training on large datasets with more than 600 k samples.\nTo address this, we systematically investigate OTiS in the medical context\ninvolving EEG analysis, as outlined in Figure 1. To explore how effective OTiS\nwould be as a medical tool, we compare its diagnostic accuracy with that of\nspecialised EEG models across three public datasets. We evaluate whether OTiS\nrequires domain adaptation to be effective for EEG analysis and whether it\nenables the localisation of critical demographic or disease-related information.\nMoreover, we analyse the influence of architectural design choices on its diagnostic\naccuracy. Our key contributions can be summarised as follows:\n1. In extensive experiments across tasks including age prediction, seizure detec-\ntion, and the classification of clinically relevant EEG events, we demonstrate\nthat OTiS extracts high-quality EEG features, achieving state-of-the-art\nperformance on established EEG benchmarks.\n\n\nAre foundation models useful EEG feature extractors?\n3\nTable 1: Overview of datasets used for regression (REG) and classification (CLS).\nThe diverse EEG characteristics and tasks enable a comprehensive evaluation.\nTask\nDataset\n#Samples\n#Variates\n#Time points\nFrequency\nREG\nLEMON [3]\n378\n32\n30, 000\n250 Hz\nCLS\nEpilepsy [2]\n11, 500\n1\n178\n174 Hz\nTUEV [12]\n112, 237\n19\n1, 000\n200 Hz\n2. Moreover, we showcase that OTiS can extract EEG features of higher quality\nthan specialised models, designed and trained exclusively for EEG analysis,\neven without domain adaptation techniques like linear probing or fine-tuning.\n3. We reveal that OTiS captures distinct EEG features across frequency bands,\nenabling the localisation of demographic and disease-related biomarkers.\n4. We demonstrate that key architectural design choices of models, such as their\ncontext length, significantly influence diagnostic accuracy, offering valuable\nguidance for the development of next-generation foundation models.\n2\nMaterials & Methods\n2.1\nModel & Domain adaptation\nIn this study, we analyse the base variant of OTiS [28] with 12 layers, 3 heads,\na width of 192, and 8 M parameters. The model was pre-trained using 640, 187\ntime series samples from 8 domains, including 400, 000 ECG (62.48 %), 203, 340\nweather (31.76 %), 19, 614 audio (3.06 %), 13, 640 engineering (2.13 %), 3, 367\nEEG (0.53 %), 115 economics (0.02 %), and 111 banking (0.02 %) samples. To\nanalyse whether specialised models benefit EEG analysis, we also include OTiSEEG\npre-trained exclusively on the 3, 367 EEG samples totalling 125 recording hours.\nWe evaluate three strategies for adapting the foundation model to EEG\nanalysis. For zero-shot (ZS), OTiS is frozen after pre-training and evaluated\nwithout any fine-tuning. Its output tokens are averaged to obtain a global repre-\nsentation. Class logits are computed via cosine similarity between a test sample’s\nrepresentation and each class representation, i.e. the mean global representation\nof all training samples from a class. This adaptation strategy applies only for\nclassification, while the following two also support regression. For linear prob-\ning, OTiS remains frozen while a randomly initialised linear layer is trained. For\nfine-tuning (FT), both OTiS and a randomly initialised linear layer are trained.\n2.2\nDatasets\nWe evaluate the potential of OTiS for EEG analysis across three datasets, as\ndetailed in Table 1. LEMON [3] comprises resting-state EEG sampled at 250 Hz\nfrom healthy subjects aged 20 −35 years (67 %) and 59 −77 years (33 %).\nEpilepsy [2] includes single-channel EEG from healthy subjects at rest (20 %)\n\n\n4\nÖ. Turgut et al.\nand patients during epileptical seizures (80 %), sampled at 174 Hz and band-pass\nfiltered between 0.5 −40 Hz. TUEV [12] is a large EEG corpus with patient\nrecordings of three clinically relevant events, including spike and sharp waves\n(SPSW: 2 %), generalised periodic epileptiform discharges (GPED: 7.06 %), and\nperiodic lateralised epileptiform discharges (PLED: 12.58 %), as well as three\nnoise events, such as eye movement (EYEM: 1.16 %), artifacts from equipment\nor the environment (ARTF: 7.79 %), and background activity (BCKG: 69.41 %).\n2.3\nExperimental setup\nProcessing & Evaluation. We follow established data processing, splitting, and\nevaluation protocols for age regression on LEMON [9], as well as classification on\nEpilepsy [35] and TUEV [32], reporting results across five seeds for linear probing\nand fine-tuning. Specifically, we measure the coefficient of determination (R2)\nfor regression on LEMON and accuracy (ACC)/balanced accuracy (bACC) for\nclassification on Epilepsy/TUEV. Age regression on LEMON is optimised for 300\nepochs using a mean squared error loss, while Epilepsy and TUEV classification\nare optimised for 30 and 75 epochs, respectively, with a cross-entropy loss. Optimal\nhyperparameters are found through a grid search over the learning rate (3e-5,\n1e-4, 3e-4, 1e-3, 3e-3), batch size (2x, x ∈[2, 3, ..., 7]), drop path (0.1, 0.2), layer\ndecay (0.5, 0.75), weight decay (0.0, 0.1, 0.2), and label smoothing (0.0, 0.1, 0.2).\nAll experiments are conducted on a single NVIDIA RTX A6000-48GB GPU.\nBaselines. We benchmark OTiS against 16 specialised state-of-the-art models (†),\nincluding 2 foundation models (‡), designed exclusively for EEG and 4 statistical\nfeature-based approaches (∗). For age regression, we compare against regression\ntoward the mean (RTM - predictions equal the training data’s mean age)∗,\nhandcrafted features∗[9], the filterbank Riemann model∗[24], the filterbank\nsource model∗[9], shallow ConvNet† [25], and deep ConvNet† [25]. For Epilepsy\nclassification, we include SimCLR† [27], TimesNet† [31], CoST† [30], TS2Vec†\n[34], TF-C† [35], Ti-MAE† [19], and SimMTM† [6], all pre-trained on SleepEEG\n[16] totalling 205 recording hours. For TUEV classification, the baselines comprise\nST-Transformer† [26], CNN-Transformer† [23], FFCL† [18], SPaRCNet† [15], and\nContraWR† [33], BIOT‡ (3 M parameter, pre-trained on 13, 000 recording hours)\n[32], and LaBraM‡ (370 M parameter, pre-trained on 2, 500 recording hours) [14].\nTo eliminate architectural biases and ensure a fair comparison across all tasks, we\nalso include an OTiS variant trained exclusively on EEG, referred to as OTiSEEG.\n3\nResults & Discussion\nFoundation models extract distinct EEG features across frequency bands, as\nshown in Figure 2, but are they valuable for clinical practice? To investigate\nthis, we evaluate the quality of EEG features extracted by such general models\nand compare them to those from specialised models (Section 3.1). We examine\nwhether general models require domain adaptation to extract clinically relevant\n\n\nAre foundation models useful EEG feature extractors?\n5\nn = 340\nv = 77.89%\n-band\n-band\n-band\n-band\n-band\nbroad\ncentroid\n(a) LEMON.\nn = 11,420\nv = 73.16%\n(b) Epilepsy.\nn = 28,305\nv = 66.42%\n(c) TUEV.\nFig. 2: First two principal components of zero-shot EEG features extracted by\nOTiS. The model captures distinct features across frequency bands, enabling the\nlocalisation of demographic and disease-specific biomarkers in clinical practice.\ninformation (Section 3.2), and whether they enable the localisation of biomarkers\n(Section 3.3). Finally, we analyse how key architectural choices, such as the\ncontext length, impact their diagnostic accuracy in clinical routine (Section 3.4).\n3.1\nGeneral versus specialised understanding\nWe investigate whether an understanding of general time series properties, such\nas frequency, amplitude, offset, and phase shift, provides advantages in a medical\ncontext. To this end, we compare OTiS [28] against specialised models designed\nand trained solely for EEG analysis, evaluating the quality of their extracted\nfeatures. The age prediction benchmark demonstrates that the features extracted\nby OTiS are superior to those of statistical approaches and specialised EEG\nmodels (Figure 3). Similarly, benchmarks on seizure detection highlight that\nthe feature quality can remain competitive even without domain adaptation\n(Figure 4). Interestingly, experiments on classification of EEG events reveal that\nOTiS captures more clinically relevant information than specialised foundation\nmodels like BIOT [32] (Figure 5). This is confirmed by the comparison of OTiS\nwith OTiSEEG, proving the effectiveness of general time series understanding.\nWhile EEG-specific pre-training of huge foundation models can yield optimal\nperformance, as indicated by LaBraM [14] in Figure 5, this approach is often\nconstrained by limited data availability. Hence, approaches like OTiS may offer a\npromising solution to eliminate the dependency on large domain-specific datasets.\n3.2\nDomain adaptation strategies\nWe investigate whether foundation models can be used out-of-the-box or require\ndomain adaptation to be effective for EEG analysis. To this end, we evaluate OTiS\nunder zero-shot, linear probing, and fine-tuning settings. The experiments reveal\nthat OTiS’ zero-shot EEG features are as informative as those from specialised\nmodels (Figures 4 and 5). We observe that slight domain adaptation through linear\nprobing provides no benefits for age prediction and seizure detection, offering\n\n\n6\nÖ. Turgut et al.\nRTM\nhandcrafted\nfb_riemann\nfb_source\nshallow\ndeep\nOTiS\nOTiSEEG\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nR2 [10-fold cross validation]\nBenchmark\nbroad\n-band\n-band\n-band\n-band\n-band\nLinear probing\nbroad\n-band\n-band\n-band\n-band\n-band\nFine-tuning\nFig. 3: LEMON results. (Left) OTiS • outperforms OTiSEEG • and other spe-\ncialised models •. (Right) Optimal domain adaptation of OTiS for EEG analysis\nis achieved through fine-tuning. EEG broadband signals contain the most age-\nrelated information, with the information density increasing at higher frequencies.\nCoST\nTi-MAE\nSimCLR\nTS2Vec\nTimesNet\nTF-C\nSimMTM\nOTiS\nOTiSEEG\n60\n70\n80\n90\n100\nACC [%]\nBenchmark\nbroad\n-band\n-band\n-band\n-band\n-band\nZero-shot\nbroad\n-band\n-band\n-band\n-band\n-band\nLinear probing\nbroad\n-band\n-band\n-band\n-band\n-band\nFine-tuning\nFig. 4: Epilepsy results. (Left) OTiS • outperforms OTiSEEG • and is competitive\nwith other specialised models •. (Right) OTiS extracts meaningful EEG features\neven without domain adaptation. EEG broadband signals contain the most ictal-\nrelated information, with no clear frequency-based trend in information density.\nimprovements over zero-shot settings only where large datasets are available,\nsuch as in EEG event classification. Zero-shot features are particularly valuable\nin tasks where visual assessment is feasible. For instance, epileptic seizures are\ntypically characterised by spike and sharp waves (SPSW) [12], which are rarely\nobserved in healthy subjects (Figures 6a and 6b). However, distinguishing artifacts\nof non-cerebral origin (ARTF) from clinically relevant events like generalised\nperiodic epileptiform discharges (GPED) requires domain knowledge [12], which\ncan only be acquired through task-specific fine-tuning (Figures 6c and 6d).\n3.3\nBiomarker localisation\nTo evaluate whether foundation models enable the localisation of clinically relevant\nbiomarkers, we examine the features extracted by OTiS across all frequency bands.\nOur analysis shows that OTiS consistently achieves optimal performance using\nbroadband features, indicating its ability to extract clincally relevant information\nfrom EEG. By grounding OTiS’ prediction on a single frequency band, the source\n\n\nAre foundation models useful EEG feature extractors?\n7\nST-Transf.\nCNN-Transf.\nFFCL\nSParcNet\nContraWR\nBIOT\nLaBraM\nOTiS\nOTiSEEG\n20\n30\n40\n50\n60\n70\nbACC [%]\nBenchmark\nbroad\n-band\n-band\n-band\n-band\n-band\nZero-shot\nbroad\n-band\n-band\n-band\n-band\n-band\nLinear probing\nbroad\n-band\n-band\n-band\n-band\n-band\nFine-tuning\nFig. 5: TUEV results. (Left) OTiS • outperforms OTiSEEG • and other specialised\nmodels •, except for huge models like LaBraM. (Right) While OTiS is competitive\nwith specialised models in the zero-shot setting, optimal domain adaptation is\nachieved through fine-tuning. EEG broadband signals contain the most clinically\nrelevant information, with the information density increasing at lower frequencies.\nn = 11,420\nACC = 93.70%\nhealthy\nseizure\ncentroid\n(a) Epilepsy ZS.\nn = 11,420\nACC = 94.25%\n(b) Epilepsy FT.\nn = 28,305\nbACC = 41.35%\nSPSW\nGPED\nPLED\nEYEM\nARTF\nBCKG\ncentroid\n(c) TUEV ZS.\nn = 28,305\nbACC = 57.43%\n(d) TUEV FT.\nFig. 6: First two principal components of EEG features extracted by OTiS. (a,\nb) OTiS extracts distinct EEG features for healthy subjects and patients, even\nwithout domain adaptation (ZS). (c, d) Its ability to extract clinically relevant\nEEG features - SPSW, GPED, and PLED - that are distinct from noise - EYEM,\nARTF, and BCKG - is substantially enhanced with domain adaptation (FT).\nof information can even be localised: bands with the highest scores are assumed to\ncontain relevant biomarkers. Specifically, we observe that age-related information\nis located in higher frequencies (Figure 3), whereas ictal activity is concentrated\nin lower frequencies (Figure 5). Similarly, we find that data pre-processing heavily\naffects diagnostic accuracy. A radical low-pass filtering of raw EEG at 40 Hz, as\nper the Epilepsy protocol [2], results in chance level prediction of the majority\nclass in the γ-band (80% seizure; Figure 4). Notably, these insights can already\nbe derived from the zero-shot features in Figure 2: broadband features are similar\nto those of the most informative bands, e.g. β-band for LEMON and δ-band for\nTUEV, and distinct from those with less information, e.g γ-band for Epilepsy.\n3.4\nImpact of architectural design choices\nWe analyse the impact of architectural choices, such as the context length, on the\ndetection of clinically relevant EEG events in TUEV. Our experiments show that\n\n\n8\nÖ. Turgut et al.\n1s2s3s4s5s\n0\n20\n40\n60\nbACC [%]\nOverall\n1s\n2s\n3s\n4s\n5s\n0\n20\n40\n60\n80\n100\nSensitivity [%]\nPer-class sensitivity\n1s\n2s\n3s\n4s\n5s\n0\n20\n40\n60\n80\n100\nSpecificity [%]\nPer-class specificity\nClasses\nSPSW\nGPED\nPLED\nEYEM\nARTF\nBCKG\ncontext length [seconds]\nFig. 7: Impact of a model’s context length on the detection of clinically relevant\nEEG events in TUEV. Overall diagnostic accuracy improves with longer context.\nSPSW detection benefits notably from a context length of at least 3 seconds.\nFiltering for noise events such as EYEM improves constantly with increasing\ncontext length, reducing the risk of grading noise as clinically relevant information.\nthe overall performance improves with longer context (Figure 7). However, not all\nEEG events are affected equally: BCKG remains largely unchanged, while EYEM\nis significantly impacted. These phenomena can be explained through domain\nknowledge: events like BCKG and ARTF are continuously present throughout\nthe recording, while other events such as PLED and GPED occur periodically.\n[12] Additionally, there are events including EYEM and SPSW with infrequent\noccurrences during a recording. [12] Our analysis reveals that context length is\nmore crucial for infrequent events than for constant or periodic ones. For instance,\nSPSW detection improves notably with models that can accommodate EEG\nsegments of at least 3 seconds. Since SPSW are infrequent events of 20 −70 ms\nduration, the analysis of very short segments increases the risk of missing the\nsignal entirely. In contrast, events such as BCKG persist throughout the entire\nrecording and thus can be reliably detected regardless of the context length.\n4\nConclusion\nIn this study, we explore the potential of foundation models with general time\nseries understanding as feature extractors for electroencephalography (EEG) anal-\nysis. Through extensive benchmarking on tasks such as age prediction, seizure\ndetection, and the classification of clinically relevant EEG events, we demonstrate\nthat these general models are competitive with specialised EEG models, even\nachieving new state-of-the-art performance. Our findings indicate that general\ntime series understanding is useful in clinical routine, especially when domain-\nspecific data is limited. While foundation models can be used out-of-the-box for\ntasks where visual assessment is feasible, domain adaptation through fine-tuning\nbecomes essential for tasks that require higher level of specialisation. Additionally,\nwe demonstrate that these models enable the localisation of demographic and\ndisease-specific biomarkers through frequency band analysis. Finally, our experi-\nments emphasise the importance of moderate data pre-processing and thoughtful\n\n\nAre foundation models useful EEG feature extractors?\n9\nchoices in the architectural model design. Overall, we believe this work provides\nvaluable guidance for integrating foundation models into clinical practice.\nLimitations. While our study showcases the effectiveness of general models\nacross regression and classification tasks, it is based on a single model that\nconsiders domain-specific information during pre-training. Future work could\nexpand our benchmarking framework to evaluate new foundation models and\nexplore more tasks. Finally, enhancing EEG features with information from fMRI\n(imaging) or health records (text) may be a promising direction for EEG analysis.\nAcknowledgments. This study was supported by the TUM Innovation Network\nNeurotechnology in Mental Health (NEUROTECH).\nDisclosure of Interests. The authors have no competing interests to declare that are\nrelevant to the content of this article.\nReferences\n1. Amin, S.U., Alsulaiman, M., Muhammad, G., Mekhtiche, M.A., Hossain, M.S.:\nDeep learning for eeg motor imagery classification based on multi-layer cnns feature\nfusion. Future Generation computer systems (2019)\n2. Andrzejak, R.G., Lehnertz, K., Mormann, F., Rieke, C., David, P., Elger, C.E.:\nIndications of nonlinear deterministic and finite-dimensional structures in time\nseries of brain electrical activity: Dependence on recording region and brain state.\nPhysical Review E (2001)\n3. Babayan, A., Erbey, M., Kumral, D., Reinelt, J.D., Reiter, A.M., Röbbig, J.,\nSchaare, H.L., et al.: A mind-brain-body dataset of mri, eeg, cognition, emotion,\nand peripheral physiology in young and old adults. Scientific data (2019)\n4. Butterworth, S., et al.: On the theory of filter amplifiers. Wireless Engineer (1930)\n5. Das, A., Kong, W., Sen, R., Zhou, Y.: A decoder-only foundation model for time-\nseries forecasting. In: International Conference on Machine Learning (2024)\n6. Dong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., Long, M.: Simmtm: A simple\npre-training framework for masked time-series modeling. In: Advances in Neural\nInformation Processing Systems (2024)\n7. Eldele, E., Ragab, M., Chen, Z., Wu, M., Kwoh, C., Li, X., Guan, C.: Time-series\nrepresentation learning via temporal and contextual contrasting. In: International\nJoint Conference on Artificial Intelligence (2021)\n8. Eldele, E., Ragab, M., Chen, Z., Wu, M., Kwoh, C.K., Li, X.: Self-supervised\nlearning for label-efficient sleep stage classification: A comprehensive evaluation.\nIEEE Transactions on Neural Systems and Rehabilitation Engineering (2023)\n9. Engemann, D.A., Mellot, A., Höchenberger, R., Banville, H., Sabbagh, D., Gemein,\nL., Ball, T., Gramfort, A.: A reusable benchmark of brain-age prediction from\nm/eeg resting-state signals. Neuroimage (2022)\n10. Gao, S., Koker, T., Queen, O., Hartvigsen, T., Tsiligkaridis, T., Zitnik, M.: Units:\nA unified multi-task time series model. Advances in Neural Information Processing\nSystems (2025)\n\n\n10\nÖ. Turgut et al.\n11. Goswami, M., Szafer, K., Choudhry, A., Cai, Y., Li, S., Dubrawski, A.: Moment: A\nfamily of open time-series foundation models. International Conference on Machine\nLearning (2024)\n12. Harati, A., Golmohammadi, M., Lopez, S., Obeid, I., Picone, J.: Improved eeg event\nclassification using differential energy. In: IEEE Signal Processing in Medicine and\nBiology Symposium (2015)\n13. Jafari, M., Shoeibi, A., Khodatars, M., Bagherzadeh, S., Shalbaf, A., García, D.L.,\nGorriz, J.M., Acharya, U.R.: Emotion recognition in eeg signals using deep learning\nmethods: A review. Computers in Biology and Medicine (2023)\n14. Jiang, W., Zhao, L., liang Lu, B.: Large brain model for learning generic representa-\ntions with tremendous EEG data in BCI. In: International Conference on Learning\nRepresentations (2024)\n15. Jing, J., Ge, W., Hong, S., Fernandes, M.B., Lin, Z., Yang, C., An, S., Struck,\nA.F., et al.: Development of expert-level classification of seizures and rhythmic and\nperiodic patterns during eeg interpretation. Neurology (2023)\n16. Kemp, B., Zwinderman, A.H., Tuk, B., Kamphuisen, H.A., Oberye, J.J.: Analysis\nof a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the\neeg. IEEE Transactions on Biomedical Engineering (2000)\n17. Li, C., Bian, N., Zhao, Z., Wang, H., Schuller, B.W.: Multi-view domain-adaptive\nrepresentation learning for eeg-based emotion recognition. Information Fusion (2024)\n18. Li, H., Ding, M., Zhang, R., Xiu, C.: Motor imagery eeg classification algorithm\nbased on cnn-lstm feature fusion network. Biomedical signal processing and control\n(2022)\n19. Li, Z., Rao, Z., Pan, L., Wang, P., Xu, Z.: Ti-mae: Self-supervised masked time\nseries autoencoders. arXiv preprint arXiv:2301.08871 (2023)\n20. Liu, Y., Zhang, H., Li, C., Huang, X., Wang, J., Long, M.: Timer: Generative\npre-trained transformers are large time series models. International Conference on\nMachine Learning (2024)\n21. Newson, J.J., Thiagarajan, T.C.: Eeg frequency bands in psychiatric disorders: a\nreview of resting state studies. Frontiers in human neuroscience (2019)\n22. Obeid, I., Picone, J.: The temple university hospital eeg data corpus. Frontiers in\nneuroscience (2016)\n23. Peh, W.Y., Yao, Y., Dauwels, J.: Transformer convolutional neural networks for\nautomated artifact detection in scalp eeg. In: 2022 44th Annual International\nConference of the IEEE Engineering in Medicine & Biology Society (EMBC) (2022)\n24. Sabbagh, D., Ablin, P., Varoquaux, G., Gramfort, A., Engemann, D.A.: Predictive\nregression modeling with meg/eeg: from source power to signals and cognitive states.\nNeuroImage (2020)\n25. Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M., et al.:\nDeep learning with convolutional neural networks for eeg decoding and visualization.\nHuman brain mapping (2017)\n26. Song, Y., Jia, X., Yang, L., Xie, L.: Transformer-based spatial-temporal feature\nlearning for eeg decoding. arXiv preprint arXiv:2106.11170 (2021)\n27. Tang, C.I., Perez-Pozuelo, I., Spathis, D., Mascolo, C.: Exploring contrastive learning\nin human activity recognition for healthcare. arXiv preprint arXiv:2011.11542 (2020)\n28. Turgut, Ö., Müller, P., Menten, M.J., Rueckert, D.: Towards generalisable time\nseries understanding across domains. arXiv preprint arXiv:2410.07299 (2025)\n29. Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., Sahoo, D.: Unified training of\nuniversal time series forecasting transformers. International Conference on Machine\nLearning (2024)\n\n\nAre foundation models useful EEG feature extractors?\n11\n30. Woo, G., Liu, C., Sahoo, D., Kumar, A., Hoi, S.: CoST: Contrastive learning of dis-\nentangled seasonal-trend representations for time series forecasting. In: International\nConference on Learning Representations (2022)\n31. Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., Long, M.: Timesnet: Temporal 2d-\nvariation modeling for general time series analysis. In: International Conference on\nLearning Representations (2022)\n32. Yang, C., Westover, M., Sun, J.: Biot: Biosignal transformer for cross-data learning\nin the wild. Advances in Neural Information Processing Systems (2024)\n33. Yang, C., Xiao, C., Westover, M.B., Sun, J., et al.: Self-supervised electroencephalo-\ngram representation learning for automatic sleep staging: model development and\nevaluation study. JMIR AI (2023)\n34. Yue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., Xu, B.: Ts2vec:\nTowards universal representation of time series. In: AAAI Conference on Artificial\nIntelligence (2022)\n35. Zhang, X., Zhao, Z., Tsiligkaridis, T., Zitnik, M.: Self-supervised contrastive pre-\ntraining for time series via time-frequency consistency. Advances in Neural Infor-\nmation Processing Systems (2022)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21086v1.pdf",
    "total_pages": 11,
    "title": "Are foundation models useful feature extractors for electroencephalography analysis?",
    "authors": [
      "Özgün Turgut",
      "Felix S. Bott",
      "Markus Ploner",
      "Daniel Rueckert"
    ],
    "abstract": "The success of foundation models in natural language processing and computer\nvision has motivated similar approaches for general time series analysis. While\nthese models are effective for a variety of tasks, their applicability in\nmedical domains with limited data remains largely unexplored. To address this,\nwe investigate the effectiveness of foundation models in medical time series\nanalysis involving electroencephalography (EEG). Through extensive experiments\non tasks such as age prediction, seizure detection, and the classification of\nclinically relevant EEG events, we compare their diagnostic accuracy with that\nof specialised EEG models. Our analysis shows that foundation models extract\nmeaningful EEG features, outperform specialised models even without domain\nadaptation, and localise task-specific biomarkers. Moreover, we demonstrate\nthat diagnostic accuracy is substantially influenced by architectural choices\nsuch as context length. Overall, our study reveals that foundation models with\ngeneral time series understanding eliminate the dependency on large\ndomain-specific datasets, making them valuable tools for clinical practice.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}