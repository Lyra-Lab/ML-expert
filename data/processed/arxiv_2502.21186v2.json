{
  "id": "arxiv_2502.21186v2",
  "text": "Published as a conference paper at ICLR 2025\nSCALABLE DECISION-MAKING IN STOCHASTIC EN-\nVIRONMENTS THROUGH LEARNED TEMPORAL AB-\nSTRACTION\nBaiting Luo1, Ava Pettet3, Aron Laszka2, Abhishek Dubey1, Ayan Mukhopadhyay1\n1Vanderbilt University, 2Pennsylvania State University, 3Nissan Advanced Technology Center\n{baiting.luo, abhishek.dubey, ayan.mukhopadhyay}@vanderbilt.edu\nalaszka@psu.edu, ava.pettet@nissan-usa.com\nABSTRACT\nSequential decision-making in high-dimensional continuous action spaces, partic-\nularly in stochastic environments, faces significant computational challenges. We\nexplore this challenge in the traditional offline RL setting, where an agent must\nlearn how to make decisions based on data collected through a stochastic behav-\nior policy. We present Latent Macro Action Planner (L-MAP), which addresses\nthis challenge by learning a set of temporally extended macro-actions through\na state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effec-\ntively reducing action dimensionality. L-MAP employs a (separate) learned prior\nmodel that acts as a latent transition model and allows efficient sampling of plau-\nsible actions. During planning, our approach accounts for stochasticity in both the\nenvironment and the behavior policy by using Monte Carlo tree search (MCTS).\nIn offline RL settings, including stochastic continuous control tasks, L-MAP ef-\nficiently searches over discrete latent actions to yield high expected returns. Em-\npirical results demonstrate that L-MAP maintains low decision latency despite\nincreased action dimensionality. Notably, across tasks ranging from continuous\ncontrol with inherently stochastic dynamics to high-dimensional robotic hand ma-\nnipulation, L-MAP significantly outperforms existing model-based methods and\nperforms on-par with strong model-free actor-critic baselines, highlighting the ef-\nfectiveness of the proposed approach in planning in complex and stochastic envi-\nronments with high-dimensional action spaces.\n1\nINTRODUCTION\nPlanning-based reinforcement learning (RL) has achieved remarkable success in domains with dis-\ncrete, low-dimensional action spaces, such as board games and video games (Silver et al., 2017;\nSchrittwieser et al., 2020; Ye et al., 2021), and continuous control tasks (Hubert et al., 2021; Schrit-\ntwieser et al., 2021). However, extending these methods to high-dimensional continuous action\nspaces, especially in stochastic environments, presents significant challenges. Many environments\nare inherently stochastic or appear stochastic to agents with limited capacity to model complex dy-\nnamics. For example, in autonomous driving, the behavior of other vehicles and pedestrians intro-\nduces substantial uncertainty that must be processed in real time (Carvalho et al., 2014; Luo et al.,\n2023a). Recent planning-based offline RL approaches like Trajectory Transformer (Janner et al.,\n2021) face significant latency issues when trying to model and respond to these stochastic behav-\niors (Li et al., 2023). Similarly, in robotic manipulation, sensor noise introduces randomness that\nrequires fast adaptive responses (Yang et al., 2023). In such cases, deterministic models often fail to\ncapture the necessary randomness and intricacies (Antonoglou et al., 2022). Moreover, the vast and\nuncountable nature of continuous action spaces makes traditional planning approaches inefficient\nwhen operating directly in the raw action space (Jiang et al., 2023). These inefficiencies are further\nexacerbated in stochastic settings, leading to “large and long” planning problems where agents must\nmanage numerous continuous variables over extended time horizons. This results in the curse of\ndimensionality and the curse of history, significantly hindering effective decision-making (Hubert\net al., 2021).\n1\narXiv:2502.21186v2  [cs.LG]  3 Mar 2025\n\n\nPublished as a conference paper at ICLR 2025\n...\n...\n...\nDecoder\n...\n...\n(a) Planning with Pre-constructing search space\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nDecision Latency (s)\n52\n56\n60\n64\n68\n72\nPerformance (%)\nHigher Performance\nat Lower Latency\nLower Performance\nat Higher Latency\nVanilla MCTS\nL-MAP\n(b) Decision Latency vs. Performance\nFigure 1: (a) Overview of planning over the pre-constructed search space. (b) As the number of\nMCTS iterations increases (10, 50, 100 from left to right), using a pre-constructed search space with\nMCTS achieves better performance with lower decision latency.\nIn this paper, we posit that planning in such challenging settings could greatly benefit from temporal\nabstractions, i.e., representations of multi-step primitive behaviors such as macro actions (Dietterich,\n2000; Sutton et al., 1999; Barto & Mahadevan, 2003). By leveraging these abstractions, planners\ncan navigate high-dimensional continuous action spaces more efficiently, potentially mitigating the\ncurse of dimensionality and reducing decision-making latency in stochastic environments. This pa-\nper considers the standard setting where an agent can access a set of trajectories (i.e., a sequence\nof state, action, and reward traces) collected through a fixed behavior policy. Given this setting, we\npropose the Latent Macro Action Planner (L-MAP), which constructs a lower dimensional repre-\nsentation of temporally extended primitive actions by using a state-conditioned Vector Quantized\nVariational AutoEncoder (VQ-VAE) (van den Oord et al., 2017). The encoder integrates the current\nstate and macro-action to generate a discrete latent code. Subsequently, a sequential model (in our\ncase, a Transformer) is employed to autoregressively model the distribution of these latent codes,\nconditioned on the current state (and the behavior policy). This Transformer facilitates a two-step\ninference process: initially, given a state, it enables the sampling of probable latent macro-actions\nunder the behavior policy, effectively acting as a prior policy. Subsequently, conditioned on both\nthe state and the sampled macro-action, it generates subsequent latent codes that encapsulate infor-\nmation about expected returns and potential next states. This dual functionality of the Transformer\nenables efficient exploration of promising action trajectories while forming a compact representation\nof the plausible trajectories during planning.\nAs shown in Fig.1a, leveraging these models, we build a latent search space that serves as a struc-\ntured initialization for planning, encapsulating likely trajectories based on the learned environment\ndynamics. To address stochasticity and optimize decision-making, we integrate Monte Carlo Tree\nSearch (MCTS) with progressive widening to efficiently navigate this latent space. Initially, the\nsearch concentrates on the prebuilt latent space, facilitating rapid decision-making grounded in\nlearned abstractions. If additional computation time becomes available, we progressively widen\nthe search tree to extend the search beyond the prebuilt latent space incrementally. This dynamic ex-\npansion strategy enables our method to balance rapid planning using learned abstractions with more\nexhaustive exploration when computational resources permit. As shown in Fig.1b, this strategy\nachieves better performance with lower decision latency compared to planning with vanilla MCTS.\nUpon selecting a latent macro-action, we operate in a polling control mode (He et al., 2011; Gabor\net al., 2019) wherein MCTS returns only the first primitive action of the recommended macro-action.\nThis approach allows for recovery from locally suboptimal decisions by performing planning at each\ntime step.\nWe evaluate L-MAP extensively in the offline RL setting across a diverse range of tasks. In stochas-\ntic MuJoCo environments (Rigter et al., 2023), L-MAP consistently outperforms both model-based\nbaselines like Trajectory Transformer (TT) (Janner et al., 2021) and Trajectory Autoencoding Plan-\nner (TAP) (Jiang et al., 2023), as well as model-free methods such as Conservative Q-Learning\n(CQL) (Kumar et al., 2020) and Implicit Q-Learning (IQL) (Kostrikov et al., 2022). This demon-\n2\n\n\nPublished as a conference paper at ICLR 2025\nstrates L-MAP’s robust capability in handling stochastic dynamics. For deterministic MuJoCo tasks,\nL-MAP shows comparable or superior performance to these baselines, highlighting that our planning\napproach effectively accounts for stochasticity in the behavior policy, leading to competitive perfor-\nmance even in deterministic environments. Notably, L-MAP scales effectively to high-dimensional\ntasks, as evidenced by its strong performance on the challenging Adroit hand manipulation tasks.\nFurthermore, L-MAP’s use of temporal abstraction enables lower latency decision-making com-\npared to methods like TT. These results underscore L-MAP’s versatility and effectiveness across\nvarious types of control problems, from stochastic to deterministic environments, and from low to\nhigh-dimensional action spaces.\n2\nPRELIMINARIES\nWe consider a continuous state and action space Markov Decision Process (MDP) defined by\n{S, A, P, r}, where S ⊆Rn is the state space, A ⊆Rl is the action space, P : S × A →∆(S) is\nthe transition function, and r : S × A →R is the reward function. To manage the complexity of\nthese continuous spaces, we introduce macro actions, which are fixed-length sequences of primitive\nactions. A macro action m ∈M is defined as m = ⟨at, . . . , at+L−1⟩, where each ai ∈A and L is\nthe length of the macro action. Our goal is to compute an optimal macro-level policy π∗: S →Pm\nthat maximizes the expected discounted return Eπ [R(s, π(s))].\nTrajectory Representation:\nConsider a trajectory τ of length T\n=\nκ · L (κ\n∈\nN+),\nwhich is composed of a sequence of states st\n∈S, fixed-size macro actions mt\n∈M,\nand corresponding return-to-go estimates Rt\n= PT\ni=t γi−tri, formally represented as τ\n=\n(R1, s1, m1, RL+1, sL+1, mL+1, . . . , R(κ−1)L+1, s(κ−1)L+1, m(κ−1)L+1).\n3\nMETHOD\nPlanning in continuous action space is hard and computationally challenging, and full enumera-\ntion of all possible actions is infeasible. Discretizing the action space is one way to address this\nchallenge, but in practice, enumerating a large set of discrete actions can also be challenging, partic-\nularly for online approaches. Sample-based methods offer an efficient approach for handling large\nand complex domains. These methods sample a subset of actions rather than exhaustively enu-\nmerating all possibilities, reducing computational costs while computing optimal policies or value\nfunctions (Hubert et al., 2021). Building on these insights, we propose the Latent Macro Action\nPlanner (L-MAP), which learns temporal abstractions in the form of macro-actions and plans using\na latent transition model that serves as both a prior policy and a transition model.\n3.1\nDISCRETIZING STATE-MACRO ACTION SEQUENCES WITH VQ-VAE\nA key insight from prior work is that a learned state-conditioned discretization can be used to con-\nstruct a discretization scheme with relatively few discrete actions while maintaining high granular-\nity (Jiang et al., 2023; Luo et al., 2023b). As shown in Fig.2, our approach leverages a learned\nstate-conditioned discretization to enable planning in a lower-dimensional discrete space. Specifi-\ncally, our encoder processes sequences of state and macro-actions as input. For example, each token\nis defined as xt = (Rt, st, mt) and its subsequent token as xt+L = (Rt+L, st+L, mt+L). The\nencoder function is defined as:\nfenc (xt = (Rt, st, mt), xt+L = (Rt+L, st+L, mt+L)) = (zt, zt+L),\n(1)\nwhere the transition chunk size is two, resulting in two latent codes assigned per chunk. To elaborate,\nthe encoder first concatenates the input return-to-go estimates, states, and macro actions into two\ntransition vectors. It then applies a sequence model, in our case, a causal Transformer, producing\ntwo latent feature vectors for each chunk of transitions.\nIn stochastic environments, executing the same macro-action m from state s can yield different\nreturns R, introducing variability that complicates the vector quantization in VQ-VAE, i.e., note that\nusing the full token xt = (Rt, st, mt) directly can result in different latent codes z for identical\n(st, mt) pairs solely due to differences in Rt. This challenge can cause the latent space to become\nfragmented and reflect return variability more than the underlying structure of available actions.\n3\n\n\nPublished as a conference paper at ICLR 2025\n...\n...\n+\n+\n+\nEnc\nDec\n...\nCodebook\n=\n=\n...\n=\n=\n=\n=\nFigure 2: An overview of our VQ-VAE model that discretizes state-macro action sequences\nConsequently, the agent might overestimate the returns during decision-making by emphasizing\nlatent codes associated with higher observed returns, neglecting the true distribution of the primitive\nactions and their expected returns.\nTo address this issue, we aim to focus the vector quantization process primarily on representations\nof the state s and macro-actions m, while still preserving the ability to reconstruct the return R.\nTo tackle this challenge, our approach involves creating two versions of each token xt: the full\ninput xt = (Rt, st, mt) and a masked version xmask\nt\n= (mask, st, mt), where Rt is masked out. The\nencoder processes both xt and xmask\nt\nto generate two embeddings, ze(xt) and ze(xmask\nt\n), respectively.\nWe use ze(xmask\nt\n) for vector quantization to obtain the quantized latent code zq(xmask\nt\n). To ensure\nthat the codebook embeddings incorporate information from the full input, including Rt, we update\nthe embedding et of the quantized latent code towards ze(xt). We modify the loss function by\nintroducing an additional term that encourages the embedding from the masked input to be close\nto that from the full input. Specifically, we used the embedding from the full input ze(x) as the\nlearning target for the embedding of the masked input ze(xmask). The modified loss function is:\nL = log p(x | zq(xmask)) + ∥sg[ze(x)] −e∥2\n2 + β∥ze(xmask) −sg[e]∥2\n2 + ∥ze(xmask) −ze(x)∥2\n2 (2)\nwhere sg denotes the stopgradient operator and the additional term\n\r\rze(xmask) −ze(x)\n\r\r2\n2 acts as a\nregularizer that aligns the embeddings of the masked and full inputs.\nIncorporating macro-actions within each token is critical, as it enables the model to capture tempo-\nral dependencies across multiple time steps without the need for downsampling. This approach is\nparticularly important in stochastic settings, where downsampling techniques that aggregate states\n(as in Jiang et al. (2023)) can obscure the stochasticity imposed by the environment’s dynamics. The\ndecoder takes the initial state and latent codes as inputs, and outputs the reconstructed trajectories:\nfdec (st, zt, zt+L) = (ˆxt = ( ˆRt, ˆst, ˆmt), ˆxt+L = ( ˆRt+L, ˆst+L, ˆmt+L)).\n(3)\nThe decoding process can be seen as the inverse of the encoding process, except that the initial state\nst is merged into the embeddings of the codes with a linear projection before decoding.\nLatent Transition Model: Following the discretization process, the subsequent step involves mod-\neling sequences of latent codes in an autoregressive manner using a causal Transformer. The Prior\nTransformer is conditioned on the initial state st, achieved by adding the state feature to all token\nembeddings (Jiang et al., 2023). Primarily, it functions as a transition model in the latent space, en-\nabling the sampling of the next latent code zi+1 conditioned on the current code zi and state s. This\ntransition, represented as T : S × Z →Z, implicitly captures the full R × S × M →R × S × M\ntransition in the original space, as each z encodes information about the return-to-go, state and\nmacro-action. Additionally, p(z | s) acts as a prior policy for efficient action sampling, allow-\ning rapid selection of probable macro-actions based on learned behaviors from the offline dataset.\nBy operating in the learned latent space, the model potentially reduces computational complexity\ncompared to modeling transitions in the original state-action space, especially for high-dimensional\nenvironments. The discrete nature of the latent space allows for efficient sampling, which can be\nbeneficial for downstream tasks such as planning.\n4\n\n\nPublished as a conference paper at ICLR 2025\n3.2\nPLANNING WITH A LATENT MACRO ACTION MODEL\nPlanning in high-dimensional environments using learned discrete representations introduces un-\ncertainties from multiple sources.\nFirst, the representation learning process introduces uncer-\ntainty due to the non-injective mapping from the high-dimensional state-action space to a lower-\ndimensional latent space. This can result in many-to-one correspondences, where multiple distinct\nhigh-dimensional inputs map to the same latent representation, creating apparent stochasticity even\nin deterministic environments. Second, the environment itself may be inherently stochastic. The\ndetailed analysis of these uncertainty sources is provided in Appendix C.\nWe argue that taking expectations over latent transitions is beneficial in mitigating all these sources\nof uncertainty, regardless of whether the environment is deterministic or stochastic. By considering\nthe expected outcomes over multiple latent transitions, we can average out the randomness intro-\nduced by the non-injective mapping and inherent stochasticity, leading to more reliable planning\ndecisions. This insight applies broadly to planning methods that employ models with non-injective\nmapping characteristics. Building on this insight, we employ Monte Carlo Tree Search (MCTS) as\nour planning algorithm to mitigate the impact of stochasticity arising from non-injective mappings\nand potential environmental randomness. MCTS iteratively explores the latent space and takes ex-\npectations over transitions, allowing for robust planning in the presence of uncertainty.\nPre-constructing the Latent Search Space. Our approach leverages a learned latent transition\nmodel to generate and evaluate macro actions for planning efficiently.\nStarting from an initial\nstate s0, we sample M latent codes z, each representing a potential macro action.\nFor each\nsampled latent code z, we sample N subsequent latent codes z′ to simulate possible future tra-\njectories, capturing the outcomes of these macro actions.\nWe obtain the corresponding state-\naction transitions and return estimates by decoding these latent pairs (z, z′) conditioned on s.\n...\n...\n...\nDec\n...\n...\n...\n...\n...\n...\nFigure 3:\nPre-construction of the latent\nsearch space by sampling and evaluating la-\ntent macro-action codes, caching the top-\nk candidates, and recursively expanding the\nplanning tree for efficient macro-level plan-\nning.\nTo construct the planning tree efficiently, we cache\nthe initial state s0 along with the top-k latent codes\nz (and their associated information) based on the de-\ncoded returns, where k = λ × M and λ ∈(0, 1]\ncontrols the expansion ratio of the tree. The cached\nlatent codes represent the most promising macro ac-\ntions to consider from the initial state. The latent\ncodes z′ are then decoded to obtain a set of recon-\nstructed tokens, i.e., (R, s, m). For each of these\nstates s, we sample B latent codes z′′, represent-\ning potential macro actions from s (note that B and\nM are exogenously defined hyper-parameters). This\nprocess is recursively applied, allowing us to ex-\npand the planning tree while controlling its growth\nthrough the parameter λ. By focusing on the most\npromising macro actions at each state, we maintain a\ncompact and informative planning structure that ef-\nficiently explores the state-action space at a macro\nlevel.\nSelection. Starting from the cached tree structure,\nMCTS iteratively expands and evaluates nodes, al-\nlowing for a more comprehensive exploration of the\nstate-action space.\nFor each state s in the tree,\nMCTS selects one of the top-k cached latent codes\nz based on the Upper Confidence Bounds for Trees\n(UCT) (Kocsis & Szepesv´ari, 2006): UCT(s, z) =\nQ(s, z) + c\nq\nlog(N(s))\nN(s,z)\nwhere Q(s, z) represents the\nvalue of executing macro action z in state s (estimated through the decoded return-to-go), N(s)\ndenotes the number of times state s has been visited, N(s, z) denotes the number of times macro\nactionz has been chosen in state s, and c is an exploration coefficient.\nProgressively Widening the State Space for Search. Despite these powerful abstraction tech-\nniques, the search space remains challenging due to the underlying high-dimensional nature of the\n5\n\n\nPublished as a conference paper at ICLR 2025\n...\n...\n...\n...\n...\n...\n...\nYes\n...\n...\nNo\nRepeat\nSelection\nExpansion\nBackpropagation\n...\n...\n...\n...\nFigure 4: Illustration of our MCTS process for macro-level planning. The algorithm iteratively\nselects actions using the UCT policy, applies progressive widening to balance exploration and ex-\nploitation, performs parallel expansion of multiple macro actions and their potential outcomes, and\nbackpropagates estimated Q-values to efficiently explore and refine the planning tree.\noriginal state space, residual stochastic characteristics of transitions in the abstracted space, and\nthe complexity of long-horizon planning scenarios. If we were to apply MCTS directly to this ab-\nstracted space, we would encounter two main issues: inefficient utilization of our pre-built search\nspace, with the search potentially diverging prematurely into unexplored regions, and difficulty in\nbuilding sufficiently deep trees for high-quality long-term decision-making, particularly in areas of\nhigh stochasticity or uncertainty (Cou¨etoux et al., 2011). Therefore, we use progressive widening\nto extend MCTS to incrementally expand the search tree. It balances the exploration of new states\nwith the exploitation of already visited states based on two hyperparameters: α ∈[0, 1] and ϵ ∈R+.\nLet |C(s, z)| denote the number of children for the state-action pair (s, z). The key idea is to al-\nternate between adding new child nodes and selecting among existing child nodes, depending on\nthe number of times a state-action pair (s, z) has been visited. A new state is added to the tree if\n|C(s, z)| < ϵ · N(s, z)α, where N(s, z) is the number of times the state-action pair has been visited.\nThe hyperparameter α controls the propensity to select among existing children, with α = 0 lead-\ning to always selecting among existing child and α = 1 leading to vanilla MCTS behavior (always\nadding a new child). In this way, we could enhance our approach by efficiently utilizing the pre-built\nsearch space, prioritizing the exploration of promising macro actions while allowing for incremen-\ntal expansion of the search tree. This technique enables our method to make quick decisions in an\nanytime manner, leveraging the cached information, and further refine the planning tree if additional\ntime is available.\nExpansion. In our approach, the expansion phase differs from standard MCTS by performing par-\nallel expansion of multiple nodes from a leaf node. From the leaf node, a set of B latent codes\n{z(i)}B\ni=1 is sampled, each representing a distinct macro action, drawn from a latent transition model\np(z | s) to ensure diverse action space coverage. For each sampled macro action z(i), N subsequent\nlatent codes {z′(i,j)}N\nj=1 are sampled according to z′(i,j) ∼p(z′ | z(i), s), for j = {1, . . . , N},\nmodeling potential outcomes and capturing the stochastic nature of macro actions. These latent\ntransitions are then decoded to obtain the resulting next states {s′(i,j)}N\nj=1 for each macro action.\nFinally, the search tree is expanded by adding all L child nodes {(s′(i,j), z′(i,j))}N\nj=1 for each macro\naction z(i) to the current leaf node s. This breadth-wise expansion enables simultaneous exploration\nof multiple promising macro actions, enhancing the diversity and comprehensiveness of the search\nand facilitating efficient exploration in complex environments.\nBackpropagation. Following the expansion phase, where multiple macro actions are expanded\nsimultaneously, the backpropagation step updates the estimated Q-values based on the return-to-go\nas shown in Fig.4.\n4\nEXPERIMENTS\nThe empirical evaluation of L-MAP consists of three sets of tasks from D4RL (Fu et al., 2020):\ngym locomotion control, AntMaze, and Adroit. We compare L-MAP to a range of prior offline RL\n6\n\n\nPublished as a conference paper at ICLR 2025\nalgorithms, including both model-free actor-critic methods (Kumar et al., 2020; Kostrikov et al.,\n2022) and model-based approaches (Rigter et al., 2023; Jiang et al., 2023; Janner et al., 2021).\nOur work is conceptually most related to the Trajectory Transformer (TT; Janner et al. (2021)) and\nthe Trajectory Autoencoding Planner (TAP; Jiang et al. (2023)), which are model-based planning\nmethods that predict and plan in continuous state and action spaces. These two baselines serve as\nour main points of comparison for deterministic environments.\nTo demonstrate L-MAP’s ability to make performant decisions in stochastic environments, we com-\npare it with One Risk to Rule Them All (1R2R; Rigter et al. (2023)), a risk-averse model-based\nalgorithm designed for stochastic domains, and model-free actor-critic methods Conservative Q-\nLearning (CQL; Kumar et al. (2020)) and Implicit Q-Learning (IQL; Kostrikov et al. (2022)). We\nevaluate L-MAP on Stochastic MuJoCo tasks (Rigter et al., 2023), which serve as a proof of concept\nin the stochastic continuous control domain.\nWe then test L-MAP on Adroit, which presents a challenge with its high state and action dimen-\nsionality. Finally, we evaluate L-MAP on AntMaze, a sparse-reward continuous-control problem.\nIn this task, L-MAP achieves similar performance to TT, surpassing model-free methods. Through\nthese diverse evaluations, we aim to demonstrate L-MAP’s versatility and effectiveness across dif-\nferent types of control problems, including stochastic environments, high-dimensional spaces, and\nsparse-reward scenarios. Additionally, we conduct an ablation study to analyze the impact of key\ncomponents in L-MAP; detailed results of this study can be found in Appendix A.\nHyperparameters As for the L-MAP-specific hyperparameters, we set our macro action length to\n3. The planning horizon in the raw action space is set to 9 for gym locomotion tasks and 15 for\nAdroit tasks. These horizons are either smaller or equal to those used in TT and TAP. Our choice of\nparameters is to ensure a control rate of approximately 10 Hz for locomotion tasks. For each task,\nwe conduct experiments with 3 different training seeds, and each seed is evaluated for 20 episodes.\nStochastic Mujoco On the Stochastic MuJoCo tasks, with results presented in Table 1, L-MAP\nconsistently outperforms the model-based baselines, TAP and TT, across all datasets and en-\nvironments, demonstrating its superior capacity to handle stochasticity in continuous control tasks.\nNotably, L-MAP achieves the highest performance in multiple datasets for both the Hopper and\nWalker2D environments. When compared to 1R2R, a risk-averse model-based algorithm specifi-\ncally designed for stochastic domains, L-MAP shows competitive or superior results in most cases.\nAn exception is the Medium-Replay-High Hopper dataset, where 1R2R attains a higher score. This\nsuggests that while L-MAP exhibits robustness across a variety of stochastic settings, there are spe-\ncific scenarios where risk-averse strategies like 1R2R may hold an advantage. Additionally, L-MAP\ngenerally outperforms the model-free methods, CQL and IQL. However, CQL surpasses L-MAP in\nthe Medium-Expert-Mod Hopper dataset. It is worth noting that L-MAP is the only method among\nall baselines that achieves performance comparable to CQL in this specific setting.\nTable 1: Results for Stochastic MuJoCo.\nModel-Based\nModel-Free\nDataset Type\nEnv\nL-MAP\nTAP\nTT\n1R2R\nCQL\nIQL\nMedium-Expert-Mod\nHopper\n106.11 ± 2.16\n40.86 ± 5.42\n56.10 ± 3.33\n52.19 ± 8.37\n106.17 ± 2.16\n60.61 ± 3.46\nMedium-Expert-Mod\nWalker2D\n93.43 ± 1.41\n91.40 ± 1.42\n80.93 ± 2.60\n56.48 ± 7.51\n91.44 ± 1.44\n86.66 ± 1.84\nMedium-Mod\nHopper\n55.07 ± 3.06\n43.64 ± 2.25\n44.49 ± 2.47\n65.24 ± 3.31\n49.92 ± 3.00\n56.00 ± 3.60\nMedium-Mod\nWalker2D\n52.94 ± 1.57\n44.46 ± 1.82\n43.61 ± 2.15\n65.16 ± 2.84\n49.38 ± 2.02\n48.82 ± 2.31\nMedium-Replay-Mod\nHopper\n52.30 ± 2.65\n38.10 ± 3.22\n37.85 ± 1.19\n22.82 ± 2.08\n40.53 ± 1.52\n49.12 ± 3.38\nMedium-Replay-Mod\nWalker2D\n51.44 ± 1.65\n43.49 ± 2.27\n27.43 ± 3.33\n52.23 ± 2.22\n40.24 ± 1.67\n40.77 ± 2.72\nMedium-Expert-High\nHopper\n66.93 ± 3.46\n37.31 ± 3.66\n58.04 ± 3.60\n37.99 ± 2.71\n68.03 ± 3.94\n44.83 ± 2.58\nMedium-Expert-High\nWalker2D\n97.18 ± 2.08\n91.09 ± 2.78\n50.01 ± 3.51\n32.38 ± 4.55\n83.18 ± 3.70\n68.61 ± 3.33\nMedium-High\nHopper\n55.32 ± 3.56\n43.93 ± 2.66\n41.26 ± 5.53\n33.99 ± 0.92\n45.21 ± 2.97\n49.69 ± 2.47\nMedium-High\nWalker2D\n68.87 ± 2.21\n52.20 ± 2.76\n59.84 ± 5.03\n32.13 ± 4.51\n61.49 ± 3.24\n47.53 ± 3.05\nMedium-Replay-High\nHopper\n58.05 ± 3.36\n48.69 ± 2.97\n39.24 ± 2.16\n68.25 ± 3.78\n51.70 ± 3.09\n43.27 ± 2.78\nMedium-Replay-High\nWalker2D\n65.87 ± 3.07\n55.15 ± 3.29\n16.55 ± 2.17\n65.63 ± 3.41\n50.33 ± 3.88\n45.13 ± 2.38\nMean\n68.63\n52.53\n46.28\n48.71\n61.47\n53.42\nD4RL MuJoCo On the deterministic MuJoCo tasks, particularly when compared to established\nmodel-free approaches such as CQL and IQL, L-MAP demonstrates notable performance in en-\nvironments like Walker2D and Hopper, matching or exceeding these baselines even in dense\nreward scenarios as shown in Table 2. This highlights L-MAP’s effectiveness across various task\nstructures. When compared to TT, L-MAP consistently delivers comparable results. However, L-\n7\n\n\nPublished as a conference paper at ICLR 2025\nMAP offers a significant practical advantage: its use of temporal abstraction enables lower la-\ntency decision-making for equivalent planning horizons, resulting in improved efficiency during\ndeployment. Furthermore, L-MAP generally outperforms TAP, suggesting that even in determinis-\ntic environments, the expectation-based planning approach proves advantageous by accounting for\nstochasticity in the behavior policy. This leads to more robust policies and, consequently, superior\nresults.\nTable 2: Normalised results for D4RL MuJoCo-v2 following the protocol of Fu et al. (2020)\nModel-Based\nModel-Free\nDataset Type\nEnv\nL-MAP\nTAP\nTT\n1R2R\nCQL\nIQL\nMedium-Expert\nHalfCheetah\n92.14 ± 0.26\n86.40 ± 2.22\n95.0 ± 0.2\n93.99 ± 1.40\n91.6\n86.7\nMedium-Expert\nHopper\n105.74 ± 2.24\n85.55 ± 3.83\n110.0 ± 2.7\n57.40 ± 6.06\n105.4\n91.5\nMedium-Expert\nWalker2D\n109.35 ± 0.08\n105.32 ± 2.03\n101.9 ± 6.8\n73.18 ± 6.29\n108.8\n109.6\nMedium\nHalfCheetah\n45.50 ± 0.10\n44.73 ± 0.39\n46.9 ± 0.4\n73.45 ± 0.15\n44.4\n47.4\nMedium\nHopper\n73.90 ± 1.91\n69.14 ± 2.33\n61.1 ± 3.6\n55.49 ± 3.99\n58.0\n66.3\nMedium\nWalker2D\n80.31 ± 1.20\n51.75 ± 3.30\n79.0 ± 2.8\n55.69 ± 4.97\n72.5\n78.3\nMedium-Replay\nHalfCheetah\n38.45 ± 0.80\n40.83 ± 0.72\n41.9 ± 2.5\n63.85 ± 0.19\n45.5\n44.2\nMedium-Replay\nHopper\n91.18 ± 0.56\n80.92 ± 3.79\n91.5 ± 3.6\n89.67 ± 1.92\n95.0\n94.7\nMedium-Replay\nWalker2D\n81.04 ± 2.62\n72.32 ± 3.26\n82.6 ± 6.9\n90.67 ± 1.98\n77.2\n77.2\nMean\n79.73\n70.77\n78.88\n72.60\n77.60\n77.32\nAdroit Control In the Adroit robotic control tasks, which are characterized by their high-\ndimensional state and action spaces, our proposed method, L-MAP, demonstrates strong and\ncompetitive performance as shown in Table 3. Across the Human, Cloned, and Expert datasets,\nL-MAP exhibits notable effectiveness compared to both model-based approaches (TAP and TT) and\nmodel-free methods (CQL, IQL, and Behavior Cloning (BC)1).\nTable 3: Adroit robotic hand control results.\nModel-Based Approaches\nModel-Free Approaches\nDataset Type\nEnv\nL-MAP\nTAP\nTT\nCQL\nIQL\nBC\nHuman\nPen\n76.26 ± 8.58\n66.86 ± 8.41\n36.4\n37.5\n71.5\n34.4\nHuman\nHammer\n1.71 ± 0.12\n1.57 ± 0.09\n0.8\n4.4\n1.4\n1.5\nHuman\nDoor\n11.24 ± 1.11\n9.51 ± 1.10\n0.1\n9.9\n4.3\n0.5\nHuman\nRelocate\n0.09 ± 0.02\n0.06 ± 0.01\n0.0\n0.2\n0.1\n0.0\nCloned\nPen\n60.68 ± 7.88\n46.44 ± 7.54\n11.4\n39.2\n37.3\n56.9\nCloned\nHammer\n2.43 ± 0.29\n1.32 ± 0.12\n0.5\n2.1\n2.1\n0.8\nCloned\nDoor\n13.22 ± 1.34\n13.45 ± 1.43\n−0.1\n0.4\n1.6\n−0.1\nCloned\nRelocate\n0.15 ± 0.13\n−0.23 ± 0.01\n−0.1\n−0.1\n−0.2\n−0.1\nExpert\nPen\n126.60 ± 5.60\n112.16 ± 6.57\n72.0\n107.0\n–\n85.1\nExpert\nHammer\n127.16 ± 0.29\n128.79 ± 0.52\n15.5\n86.7\n–\n125.6\nExpert\nDoor\n105.24 ± 0.10\n105.86 ± 0.08\n94.1\n101.5\n–\n34.9\nExpert\nRelocate\n107.57 ± 0.76\n106.21 ± 1.61\n10.3\n95.0\n–\n101.3\nMean (All)\n51.40\n49.33\n20.08\n40.32\n14.76\n36.73\nMean (Non-Expert)\n18.79\n17.37\n6.13\n11.70\n14.76\n11.74\nIn the Human dataset, which includes suboptimal human demonstrations, L-MAP achieves the high-\nest score in the Door environment and performs well in other tasks. Although IQL leads in the Pen\ntask and CQL leads in the Hammer and Relocate tasks, L-MAP maintains competitive results, par-\nticularly surpassing TT and BC in most environments. This suggests that L-MAP effectively utilizes\nsuboptimal data to make robust decisions in complex settings. For the Cloned dataset, which con-\ntains a mix of optimal and suboptimal trajectories, L-MAP secures top performance in the Pen and\nRelocate tasks. In the Expert dataset, comprised of optimal demonstrations, L-MAP attains the\nhighest scores in the Pen and Relocate environments while remaining competitive in the Hammer\nand Door tasks. Overall, L-MAP achieves the highest average score of 51.40 across all datasets and\nenvironments, and 18.79 across non-expert datasets, highlighting its effectiveness in handling vary-\ning levels of data optimality. Furthermore, the experimental results indicate that L-MAP effectively\n1We included Behavior Cloning (BC) as an additional baseline since the original 1R2R method was not\nevaluated for Adroit tasks.\n8\n\n\nPublished as a conference paper at ICLR 2025\nmanages the complexities of high-dimensional Adroit environments. Incorporating more action in-\nformation into the single token does not detract from performance; instead, it appears to enhance the\nmodel’s ability to learn nuanced temporal dependencies required for successful task execution.\nAntMaze In the AntMaze environments—a set of sparse-reward continuous-control tasks where an\nagent must navigate a robotic ant to a target location, L-MAP demonstrates strong and competitive\nperformance as shown in Table 4. These tasks are particularly challenging due to the sparse rewards\nand the presence of suboptimal trajectories that lead to various goals other than the target position\nused during testing.\nTable 4: Performance comparison on AntMaze environments. This evaluation demonstrates that our\napproach can achieve comparable performance to TT with a separate Q network, while being more\nefficient during sampling and decision-making.\nDataset Environment\nBC\nCQL\nIQL\nTT (+Q)\nTAP\nL-MAP\nUmaze AntMaze\n54.6\n74.0\n87.5\n100.0 ± 0.0\n78.33 ± 5.32\n93.33 ± 3.22\nMedium-Play AntMaze\n0.0\n61.2\n71.2\n93.3 ± 6.4\n43.33 ± 6.40\n75.00 ± 6.85\nMedium-Diverse AntMaze\n0.0\n53.7\n70.0\n100.0 ± 0.0\n30.00 ± 5.92\n88.33 ± 4.14\nLarge-Play AntMaze\n0.0\n15.8\n39.6\n66.7 ± 12.2\n63.33 ± 6.22\n78.33 ± 5.32\nLarge-Diverse AntMaze\n0.0\n14.9\n47.5\n60.0 ± 12.7\n66.67 ± 6.09\n81.67 ± 5.00\nMean\n10.92\n43.92\n55.16\n84.00\n56.33\n83.33\nSimilar to TAP, our approach integrates goal positions into the observation space, allowing it to\ncondition trajectory generation on specific goals. This conditioning narrows the focus of sampled\ntrajectories towards the target direction, simplifying the planning process. Instead of using the\nIQL critic for value estimation, L-MAP leverages Monte Carlo planning to provide refined value\nestimates. This alternative approach avoids the additional computational cost of sampling with a\nseparate Q-network, as required by TT (+Q).\nOur method achieves an average success rate of 83.33% across all AntMaze environments, which\nis comparable to the 84.00% average of TT (+Q). Notably, L-MAP outperforms TT (+Q) in the\nmore complex Large-Play and Large-Diverse environments, achieving success rates of 78.33% and\n81.67% respectively, compared to TT (+Q)’s 66.7% and 60.0%. This indicates that L-MAP is partic-\nularly effective in larger mazes where navigation complexity is higher. While TT (+Q) attains per-\nfect success rates in smaller environments like Umaze and Medium-Diverse, L-MAP still performs\nexceptionally well with success rates of 93.33% and 88.33% in these settings. This consistency\nsuggests that our method is robust across different scales of environment complexity.\n5\nRELATED WORK\nRecent advancements in reinforcement learning focus on learning temporally extended action prim-\nitives to reduce decision-making horizons and improve learning efficiency. Both model-free and\nmodel-based methods leverage temporal abstraction to manage task complexity.\nModel-free methods such as CompILE (Kipf et al., 2019), RPL (Gupta et al., 2019), OPAL (Ajay\net al., 2021), ACT (Zhao et al., 2023), and PRISE (Zheng et al., 2024) leverage temporal abstraction\nin various ways. For instance, CompILE learns latent codes representing variable-length behavior\nsegments, enabling cross-task generalization. RPL employs a hierarchical policy architecture to\nsimplify long-horizon tasks by decomposing them into sub-policies. OPAL introduces a continuous\nspace of primitive actions to reduce distributional shift in offline RL, enhancing policy robustness.\nPRISE applies sequence compression to learn variable-length action primitives, improving behavior\ncloning by capturing essential behavioral patterns. These approaches demonstrate the versatility\nof temporal abstraction in addressing different challenges in reinforcement learning, particularly in\nmanaging the complexity inherent in sequential decision-making.\nFrom a model-based perspective, recent work has treated reinforcement learning as a sequence mod-\neling problem, utilizing Transformer architectures to model entire trajectories of states, actions, re-\nwards, and values. This approach is exemplified by methods like Trajectory Transformer (TT) (Zhou\net al., 2020), and TAP (Jiang et al., 2023). TAP, in particular, shares conceptual similarities with\n9\n\n\nPublished as a conference paper at ICLR 2025\nour proposed method, L-MAP, in its use of efficient planning solutions for complex action spaces.\nThese sequence modeling approaches have shown promise in capturing long-term dependencies and\nhandling the variability in trajectories, but they often face challenges in stochastic environments\nwhere the outcome is not solely determined by the agent’s actions. As highlighted by Paster et al.\n(2022), reinforcement learning via supervised learning methods may replicate suboptimal actions\nthat accidentally led to good outcomes due to environmental randomness. To address this issue, they\nproposed ESPER, a solution inspired by the decision transformer framework (Chen et al., 2021).\nESPER mitigates the influence of stochasticity on policy learning in discrete action spaces by clus-\ntering trajectories and conditioning on average cluster returns.\nFrom a theoretical perspective, several foundational works have studied continuous-space RL via\nHamilton-Jacobi-Bellman equations. For example, Kim et al. (2021) grounded Q-learning and DQN\nin this theory, characterizing optimal control without explicit optimization, Munos (2000) estab-\nlished convergence results using viscosity solutions, and Han et al. (2017) employed deep learning\nto solve high-dimensional PDEs via backward stochastic differential equations. While providing\ncrucial theoretical foundations, these works focused on deterministic environments or required per-\nfect knowledge about the dynamics of the environment.\nOur approach also has interesting connections to robust RL, though with key distinctions. While\nrobust MDPs (Iyengar, 2005; Nilim & Ghaoui, 2005) deal with varying transition kernels chosen\nadversarially from uncertainty sets, our work focuses on learning and planning with a fixed tran-\nsition kernel in an offline setting where environmental stochasticity is captured through learned\nmodels. Early robust RL addressed planning with known dynamics in tabular settings (Xu & Man-\nnor, 2010), and generalizing to continuous, high-dimensional spaces is challenging (Lim & Autef,\n2019). Our temporal abstraction could complement robust RL by providing structured transition\nfunctions, potentially integrating classical robust RL planning into high-dimensional environments.\nFrom a planning perspective, our work relates to methods like MuZero (Schrittwieser et al., 2020),\nstochastic MuZero (Antonoglou et al., 2022), and Vector Quantized Models for Planning (Ozair\net al., 2021), which primarily operate in discrete action spaces and online settings, limiting their\napplicability to continuous control tasks in offline RL. MuZero Unplugged (Schrittwieser et al.,\n2021) extended MuZero to the offline setting and adapted to low-dimensional continuous action\nspaces using factorized policy representations (Tang & Agrawal, 2020). However, scaling to high-\ndimensional action spaces is challenging due to computational infeasibility and imprecise action\nselection (Luo et al., 2023b). Additionally, MuZero Unplugged focuses on deterministic environ-\nments and may struggle in highly stochastic continuous settings.\nOur method, L-MAP, extends these concepts to high-dimensional continuous action spaces by ef-\nfectively handling stochasticity and complexity. Using an encoder to group similar state-macro-\naction pairs and reconstructing return-to-go estimates via a decoder within the VQ-VAE framework,\nL-MAP captures essential dynamics while abstracting unnecessary details. This approach models\nfuture returns more accurately in stochastic settings. Combined with planning algorithms, L-MAP\nrefines expected return estimates, bridging the gap between temporal abstraction techniques and ro-\nbust performance in stochastic environments. Our latent code representation and transition model\nreduce the need to learn separate policy, dynamics, and value components in the offline setting,\nincreasing planning efficiency and accounting for environmental stochasticity, thereby enhancing\ngeneralization across complex tasks.\n6\nDISCUSSION AND LIMITATIONS\nIn conclusion, we introduced the Latent Macro Action Planner (L-MAP), which leverages temporal\nabstractions learned with a state-conditioned VQ-VAE to construct a discrete latent space of macro-\nactions. This approach enables efficient planning in high-dimensional continuous action spaces\nwithin stochastic environments. Future directions include exploring transfer learning to handle new\ntasks, and adapting L-MAP to online learning scenarios through strategies such as risk-averse ex-\nploration (Luo et al., 2024). These enhancements would enable continuous improvement and help\ntackle more complex challenges, ultimately improving generalization and efficiency in complex,\nreal-world settings.\n10\n\n\nPublished as a conference paper at ICLR 2025\n7\nACKNOWLEDGEMENTS\nThis material is based upon work sponsored by the National Science Foundation (NSF) under Grant\nCNS-2238815 and by the Defense Advanced Research Projects Agency (DARPA) under the Assured\nNeuro Symbolic Learning and Reasoning program. Results presented in this paper were obtained\nusing the Chameleon testbed supported by the National Science Foundation. Any opinions, findings,\nconclusions, or recommendations expressed in this material are those of the authors and do not\nnecessarily reflect the views of the NSF or the DARPA.\nREFERENCES\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: offline prim-\nitive discovery for accelerating offline reinforcement learning. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net,\n2021. URL https://openreview.net/forum?id=V69LGwJ0lIN.\nRobert Almgren and Neil Chriss. Optimal execution of portfolio transactions. Journal of Risk, 3:\n5–40, 2001.\nIoannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K. Hubert, and David Silver. Plan-\nning in stochastic environments with a learned model. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\nURL https://openreview.net/forum?id=X6D9bAHhBQ1.\nWenhang Bao and Xiao-yang Liu. Multi-agent deep reinforcement learning for liquidation strategy\nanalysis. arXiv preprint arXiv:1906.11046, 2019.\nAndrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning.\nDiscrete event dynamic systems, 13:341–379, 2003.\nAshwin Carvalho, Yiqi Gao, St´ephanie Lef`evre, and Francesco Borrelli.\nStochastic predictive\ncontrol of autonomous vehicles in uncertain environments.\n2014.\nURL https://api.\nsemanticscholar.org/CorpusID:14171346.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin,\nPieter Abbeel,\nAravind Srinivas,\nand Igor Mordatch.\nDecision transformer:\nRein-\nforcement learning via sequence modeling.\nIn Marc’Aurelio Ranzato, Alina Beygelz-\nimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances\nin Neural Information Processing Systems 34:\nAnnual Conference on Neural Informa-\ntion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 15084–\n15097,\n2021.\nURL https://proceedings.neurips.cc/paper/2021/hash/\n7f489f642a0ddb10272b5c31057f0663-Abstract.html.\nAdrien Cou¨etoux, Jean-Baptiste Hoock, Nataliya Sokolovska, Olivier Teytaud, and Nicolas Bon-\nnard.\nContinuous upper confidence trees.\nIn Carlos A. Coello Coello (ed.), Learning and\nIntelligent Optimization - 5th International Conference, LION 5, Rome, Italy, January 17-\n21, 2011. Selected Papers, volume 6683 of Lecture Notes in Computer Science, pp. 433–445.\nSpringer, 2011. doi: 10.1007/978-3-642-25566-3\\ 32. URL https://doi.org/10.1007/\n978-3-642-25566-3_32.\nThomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decompo-\nsition. Journal of artificial intelligence research, 13:227–303, 2000.\nDamien Ernst, Guy-Bart Stan, Jorge Goncalves, and Louis Wehenkel. Clinical data based optimal sti\nstrategies for hiv: a reinforcement learning approach. Proceedings of the 45th IEEE Conference\non Decision and Control, pp. 667–672, 2006.\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep\ndata-driven reinforcement learning. CoRR, abs/2004.07219, 2020. URL https://arxiv.\norg/abs/2004.07219.\n11\n\n\nPublished as a conference paper at ICLR 2025\nThomas Gabor, Jan Peter, Thomy Phan, Christian Meyer, and Claudia Linnhoff-Popien. Subgoal-\nbased temporal abstraction in monte-carlo tree search.\nIn Sarit Kraus (ed.), Proceedings of\nthe Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,\nChina, August 10-16, 2019, pp. 5562–5568. ijcai.org, 2019. doi: 10.24963/IJCAI.2019/772. URL\nhttps://doi.org/10.24963/ijcai.2019/772.\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy\nlearning: Solving long-horizon tasks via imitation and reinforcement learning. In Leslie Pack\nKaelbling, Danica Kragic, and Komei Sugiura (eds.), 3rd Annual Conference on Robot Learn-\ning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100\nof Proceedings of Machine Learning Research, pp. 1025–1037. PMLR, 2019.\nURL http:\n//proceedings.mlr.press/v100/gupta20a.html.\nJiequn Han, Arnulf Jentzen, and Weinan E. Overcoming the curse of dimensionality: Solving high-\ndimensional partial differential equations using deep learning.\nCoRR, abs/1707.02568, 2017.\nURL http://arxiv.org/abs/1707.02568.\nRuijie He, Emma Brunskill, and Nicholas Roy. Efficient planning under uncertainty with macro-\nactions. J. Artif. Intell. Res., 40:523–570, 2011. doi: 10.1613/JAIR.3171. URL https://doi.\norg/10.1613/jair.3171.\nThomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon\nSchmitt, and David Silver. Learning and planning in complex action spaces. In Marina Meila\nand Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Re-\nsearch, pp. 4476–4486. PMLR, 2021. URL http://proceedings.mlr.press/v139/\nhubert21a.html.\nGarud Iyengar. Robust dynamic programming. Math. Oper. Res., 30:257–280, 2005. URL https:\n//api.semanticscholar.org/CorpusID:6626684.\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\nmodeling problem. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang,\nand Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pp. 1273–1286, 2021. URL https://proceedings.neurips.cc/\npaper/2021/hash/099fe6b0b444c23836c4a5d07346082b-Abstract.html.\nZhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rockt¨aschel, Edward Grefen-\nstette, and Yuandong Tian.\nEfficient planning in a compact latent action space.\nIn The\nEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net, 2023.\nURL https://openreview.net/forum?id=\ncA77NrVEuqn.\nJeongho Kim, Jaeuk Shin, and Insoon Yang. Hamilton-jacobi deep q-learning for deterministic\ncontinuous-time systems with lipschitz continuous controls. J. Mach. Learn. Res., 22:206:1–\n206:34, 2021. URL https://jmlr.org/papers/v22/20-1235.html.\nThomas Kipf, Yujia Li, Hanjun Dai, Vin´ıcius Flores Zambaldi, Alvaro Sanchez-Gonzalez, Edward\nGrefenstette, Pushmeet Kohli, and Peter W. Battaglia. Compile: Compositional imitation learning\nand execution. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-\nfornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 3418–3428. PMLR,\n2019. URL http://proceedings.mlr.press/v97/kipf19a.html.\nLevente Kocsis and Csaba Szepesv´ari. Bandit based monte-carlo planning. In Johannes F¨urnkranz,\nTobias Scheffer, and Myra Spiliopoulou (eds.), Machine Learning: ECML 2006, 17th Euro-\npean Conference on Machine Learning, Berlin, Germany, September 18-22, 2006, Proceed-\nings, volume 4212 of Lecture Notes in Computer Science, pp. 282–293. Springer, 2006. doi:\n10.1007/11871842\\ 29. URL https://doi.org/10.1007/11871842_29.\n12\n\n\nPublished as a conference paper at ICLR 2025\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-\nlearning. In The Tenth International Conference on Learning Representations, ICLR 2022, Vir-\ntual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/\nforum?id=68n2s9ZJWF8.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\nreinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n0d2b2061826a5df3221116a5085a6052-Abstract.html.\nZenan Li, Fan Nie, Qiao Sun, Fang Da, and Hang Zhao. Uncertainty-aware decision transformer\nfor stochastic driving environments. CoRR, abs/2309.16397, 2023. doi: 10.48550/ARXIV.2309.\n16397. URL https://doi.org/10.48550/arXiv.2309.16397.\nShiau Hong Lim and Arnaud Autef. Kernel-based reinforcement learning in robust markov decision\nprocesses. In International Conference on Machine Learning, 2019. URL https://api.\nsemanticscholar.org/CorpusID:174799876.\nBaiting Luo, Shreyas Ramakrishna, Ava Pettet, Christopher B. Kuhn, Gabor Karsai, and Ayan\nMukhopadhyay.\nDynamic simplex: Balancing safety and performance in autonomous cyber\nphysical systems.\nIn Sayan Mitra, Nalini Venkatasubramanian, Abhishek Dubey, Lu Feng,\nMahsa Ghasemi, and Jonathan Sprinkle (eds.), Proceedings of the ACM/IEEE 14th International\nConference on Cyber-Physical Systems, ICCPS 2023, (with CPS-IoT Week 2023), San Antonio,\nTX, USA, May 9-12, 2023, pp. 177–186. ACM, 2023a. doi: 10.1145/3576841.3585934. URL\nhttps://doi.org/10.1145/3576841.3585934.\nBaiting Luo, Yunuo Zhang, Abhishek Dubey, and Ayan Mukhopadhyay.\nAct as you learn:\nAdaptive decision-making in non-stationary markov decision processes.\nIn Mehdi Dastani,\nJaime Sim˜ao Sichman, Natasha Alechina, and Virginia Dignum (eds.), Proceedings of the\n23rd International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2024,\nAuckland, New Zealand, May 6-10, 2024, pp. 1301–1309. International Foundation for Au-\ntonomous Agents and Multiagent Systems / ACM, 2024. doi: 10.5555/3635637.3662988. URL\nhttps://dl.acm.org/doi/10.5555/3635637.3662988.\nJianlan Luo, Perry Dong, Jeffrey Wu, Aviral Kumar, Xinyang Geng, and Sergey Levine. Action-\nquantized offline reinforcement learning for robotic skill learning. In Jie Tan, Marc Toussaint,\nand Kourosh Darvish (eds.), Conference on Robot Learning, CoRL 2023, 6-9 November 2023,\nAtlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pp. 1348–1361.\nPMLR, 2023b. URL https://proceedings.mlr.press/v229/luo23a.html.\nR´emi Munos. A study of reinforcement learning in the continuous case by the means of viscosity\nsolutions. Mach. Learn., 40(3):265–299, 2000. doi: 10.1023/A:1007686309208. URL https:\n//doi.org/10.1023/A:1007686309208.\nArnab Nilim and Laurent El Ghaoui.\nRobust control of markov decision processes with\nuncertain transition matrices.\nOper. Res., 53:780–798, 2005.\nURL https://api.\nsemanticscholar.org/CorpusID:1537485.\nSherjil Ozair, Yazhe Li, Ali Razavi, Ioannis Antonoglou, A¨aron van den Oord, and Oriol Vinyals.\nVector quantized models for planning. In Marina Meila and Tong Zhang (eds.), Proceedings of\nthe 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learning Research, pp. 8302–8313. PMLR, 2021.\nURL http://proceedings.mlr.press/v139/ozair21a.html.\nKeiran Paster, Sheila A. McIlraith, and Jimmy Ba.\nYou can’t count on luck:\nWhy de-\ncision transformers and rvs fail in stochastic environments.\nIn Sanmi Koyejo, S. Mo-\nhamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural\nInformation Processing Systems 35:\nAnnual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\n2022, 2022.\nURL http://papers.nips.cc/paper_files/paper/2022/hash/\nfe90657b12193c7b52a3418bdc351807-Abstract-Conference.html.\n13\n\n\nPublished as a conference paper at ICLR 2025\nMarc Rigter, Bruno Lacerda, and Nick Hawes.\nOne risk to rule them all: A risk-sensitive\nperspective on model-based offline reinforcement learning.\nIn Alice Oh, Tristan Nau-\nmann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances\nin Neural Information Processing Systems 36:\nAnnual Conference on Neural Informa-\ntion Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\n2023, 2023.\nURL http://papers.nips.cc/paper_files/paper/2023/hash/\nf49287371916715b9209fa41a275851e-Abstract-Conference.html.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap,\nand David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nat.,\n588(7839):604–609, 2020. doi: 10.1038/S41586-020-03051-4. URL https://doi.org/\n10.1038/s41586-020-03051-4.\nJulian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis\nAntonoglou, and David Silver. Online and offline reinforcement learning by planning with a\nlearned model. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang,\nand Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pp. 27580–27591, 2021. URL https://proceedings.neurips.cc/\npaper/2021/hash/e8258e5140317ff36c7f8225a3bf9590-Abstract.html.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Si-\nmonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforce-\nment learning algorithm, 2017. URL https://arxiv.org/abs/1712.01815.\nRichard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-\nwork for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–\n211, 1999.\nYunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization.\nIn The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second\nInnovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Sympo-\nsium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, Febru-\nary 7-12, 2020, pp. 5981–5988. AAAI Press, 2020. doi: 10.1609/AAAI.V34I04.6059. URL\nhttps://doi.org/10.1609/aaai.v34i04.6059.\nA¨aron van den Oord,\nOriol Vinyals,\nand Koray Kavukcuoglu.\nNeural discrete repre-\nsentation learning.\nIn Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\nWallach,\nRob Fergus,\nS. V. N. Vishwanathan,\nand Roman Garnett (eds.),\nAdvances\nin Neural Information Processing Systems 30:\nAnnual Conference on Neural Infor-\nmation Processing Systems 2017,\nDecember 4-9,\n2017,\nLong Beach,\nCA, USA, pp.\n6306–6315, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html.\nHuan Xu and Shie Mannor.\nDistributionally robust markov decision processes.\nIn John D.\nLafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Cu-\nlotta (eds.), Advances in Neural Information Processing Systems 23: 24th Annual Confer-\nence on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-\n9 December 2010, Vancouver, British Columbia, Canada, pp. 2505–2513. Curran Asso-\nciates, Inc., 2010. URL https://proceedings.neurips.cc/paper/2010/hash/\n19f3cd308f1455b3fa09a282e0d496f4-Abstract.html.\nShuo Yang, George J. Pappas, Rahul Mangharam, and Lars Lindemann. Safe perception-based\ncontrol under stochastic sensor uncertainty using conformal prediction. In 62nd IEEE Confer-\nence on Decision and Control, CDC 2023, Singapore, December 13-15, 2023, pp. 6072–6078.\nIEEE, 2023. doi: 10.1109/CDC49753.2023.10384075. URL https://doi.org/10.1109/\nCDC49753.2023.10384075.\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games\nwith limited data. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang,\n14\n\n\nPublished as a conference paper at ICLR 2025\nand Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pp. 25476–25488, 2021. URL https://proceedings.neurips.cc/\npaper/2021/hash/d5eca8dc3820cad9fe56a3bafda65ca1-Abstract.html.\nTony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual\nmanipulation with low-cost hardware. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and\nJingjin Yu (eds.), Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14,\n2023, 2023. doi: 10.15607/RSS.2023.XIX.016. URL https://doi.org/10.15607/RSS.\n2023.XIX.016.\nRuijie Zheng, Ching-An Cheng, Hal Daum´e III, Furong Huang, and Andrey Kolobov. PRISE: llm-\nstyle sequence compression for learning temporal action abstractions in control. In Forty-first\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.\nOpenReview.net, 2024. URL https://openreview.net/forum?id=p225Od0aYt.\nWenxuan Zhou, Sujay Bajracharya, and David Held. PLAS: latent action space for offline rein-\nforcement learning. In Jens Kober, Fabio Ramos, and Claire J. Tomlin (eds.), 4th Conference\non Robot Learning, CoRL 2020, 16-18 November 2020, Virtual Event / Cambridge, MA, USA,\nvolume 155 of Proceedings of Machine Learning Research, pp. 1719–1735. PMLR, 2020. URL\nhttps://proceedings.mlr.press/v155/zhou21b.html.\n15\n\n\nPublished as a conference paper at ICLR 2025\nFigure 5: Results of ablation studies, where the height of the bar is the mean normalized scores on\nhigh noise gym locomotion control tasks.\nA\nABLATION STUDY\nWe present analyses and ablations of key hyperparameters such as macro action length, planning\nhorizon, the use of pUCT (Silver et al., 2017) versus UCT, and the effect of our customized VQ-\nVAE loss function. Figure 5 summarizes the results from ablation studies conducted on high-noise\nstochastic MuJoCo tasks.\nMacro Action Length\nWe tested macro action lengths L = 1, L = 3, and L = 5 to evaluate their impact on L-MAP’s\nperformance. The highest mean score of 68.7 was achieved with L = 3. Increasing L to 5 reduced\nthe mean score to 64.57, while decreasing it to 1 further dropped it to 59.39. This indicates that\na macro action length of 3 optimally balances temporal abstraction and adaptability. A moderate\nlength allows the model to capture important action sequences while remaining responsive to en-\nvironmental changes. Shorter lengths may fail to model temporal dependencies effectively, while\nlonger lengths may hinder quick adaptation in stochastic environments.\nPlanning Horizon\nWe assessed the effect of planning horizon by varying the number of planning steps in L-MAP.\nReducing the planning horizon to 3 steps (expanding a single latent variable) decreased the mean\nscore to 57.51, compared to 68.7 with the default longer planning horizon. This demonstrates that a\nlonger planning horizon significantly enhances performance by enabling the model to better antici-\npate future events and handle uncertainty in high-noise stochastic environments.\nTree Search Algorithm: UCT vs. pUCT\nWe compared standard UCT and pUCT as tree search algorithms in L-MAP. UCT achieved a mean\nscore of 68.7, slightly outperforming pUCT, which scored 66.4. While both methods are effective,\nUCT performs marginally better in this context. A possible explanation is that pUCT leverages a\nlearned prior policy to guide exploration, making it sensitive to the quality of the prior. If the prior\npolicy is suboptimal, pUCT may be less effective due to this dependency.\nVQ-VAE Loss Function We compared our loss function with the standard loss function without\nmasking (mean scores: 68.7 vs 57.7). Our approach outperforms the standard loss by focusing\nprimarily on state and action during vector quantization. This results in less skewed reconstructed\n16\n\n\nPublished as a conference paper at ICLR 2025\nreturns and a more coherent latent space, accurately capturing action and state distributions. Conse-\nquently, the model generates more reliable latent representations for reconstruction.\nProgressive Widening\nWe evaluated the impact of progressive widening on MAP’s performance. Removing progressive\nwidening led to a significant drop in the mean score from 68.70 to 54.77. This substantial decrease\ndemonstrates the importance of controlled state space expansion during planning for a large search\nspace. Progressive widening enables MAP to balance between exploiting existing states in the pre-\nbuilt search space and incrementally adding new states. Without progressive widening, the search\nsuffers from excessive branching, making it difficult to build sufficiently deep trees for meaningful\nplanning in areas of high stochasticity.\nParallel Expansion\nWe assessed the contribution of parallel expansion by comparing L-MAP’s performance with and\nwithout this feature. Removing parallel expansion reduced the mean score from 68.7 to 62.75,\nyielding performance similar to reducing the planning horizon to six steps. This comparison reveals\nthat parallel expansion primarily affects the algorithm’s ability to efficiently explore the search space.\nGiven the same number of MCTS iterations, removing parallel expansion results in less exploration\nof possible trajectories, reducing the algorithm’s planning capability to that of a shorter horizon.\nThis demonstrates that parallel expansion is crucial for maximizing the effectiveness of each MCTS\niteration by enabling broader simultaneous exploration of potential outcomes.\nB\nADDITIONAL STOCHASTIC ENVIRONMENT EXPERIMENTS: HIV\nTREATMENT AND CURRENCY EXCHANGE\nTable 5: Results for HIV Treatment and Currency Exchange.\nModel-Based Approaches\nModel-Free Approaches\nEnv\nL-MAP\nTAP\nTT\n1R2R\nCQL\nIQL\nHIV\n59.08 ± 1.96\n54.95 ± 1.98\n54.46 ± 3.30\n56.45 ± 2.17\n59.74 ± 1.11\n34.1 ± 1.2\nCurrency\n106.78 ± 5.00\n89.72 ± 3.90\n79.28 ± 2.61\n78.52 ± 2.08\n93.96 ± 1.69\n89.41 ± 2.83\nThe HIV Treatment environment, originally introduced by Ernst et al. (2006), simulates treatment\nplanning where an agent controls two drug types (RTI and PI) in a 6-dimensional state space rep-\nresenting cell and virus concentrations. The stochasticity arises from varying drug efficacy at each\nstep. The Currency Exchange environment, based on the Optimal Liquidation problem (Almgren &\nChriss, 2001; Bao & Liu, 2019), involves converting currency under stochastic exchange rates that\nfollow an Ornstein-Uhlenbeck process. Both environments were adapted by Rigter et al. (2023) to\nthe offline RL setting, with datasets collected using partially trained and random policies respec-\ntively.\nFor the HIV Treatment domain, L-MAP and CQL achieve comparable strong performance (59.08\n± 1.96 and 59.74 ± 1.11 respectively), outperforming other baselines. In the Currency Exchange\nenvironment, L-MAP substantially outperforms all other approaches, achieving a score of 106.78\n± 5.00 compared to the next best performer CQL at 93.96 ± 1.69. This superior performance\ndemonstrates L-MAP’s versatility across different types of stochastic environments.\nC\nLATENT SPACE ANALYSIS\nTo empirically demonstrate the uncertainties introduced by non-injective mappings, behavior pol-\nicy, and environmental stochasticity, we generate heatmaps representing the transition probabil-\nities between latent codes.\nWe focus on the Hopper environment and consider three datasets:\nmedium-expert, medium, and medium-replay, in both deterministic and stochastic settings.\nThe heatmaps are constructed by encoding the state-macro-action pairs into latent codes using our\nlearned representation and visualizing the transition probabilities between these codes.\n17\n\n\nPublished as a conference paper at ICLR 2025\n(a) Medium expert\n(b) Medium\n(c) Medium replay\nFigure 6: Heatmaps for Deterministic Hopper Environment (Top 50 Frequent Latent Codes). In\neach heatmap, the intensity of the color at position (i, j) represents the probability of transitioning\nfrom the current latent code zt = i to the next latent code zt+1 = j. The accompanying histograms\ndisplay the frequency of each latent code occurring across the dataset with the learned encoder as\nthe current (zt, right histogram) and next (zt+1, top histogram) codes. The observed spread in the\nheatmaps indicates that, despite the deterministic nature of the environment, transitions from a single\nzt lead to multiple zt+1.\n(a) Medium Expert\n(b) Medium\n(c) Medium Replay\nFigure 7: Heatmaps for Stochastic Hopper Environment (Top 50 Frequent Latent Codes). The ob-\nserved spread in the heatmaps indicates that inherent environmental stochasticity further contributes\nto transitions from a single zt leading to multiple zt+1.\nC.1\nDETERMINISTIC ENVIRONMENT HEATMAPS\nIn analyzing the heatmaps for deterministic environments as shown in Fig. 6, it becomes evident\nthat transitions from a current latent code zt to multiple next latent codes zt+1 are not strictly deter-\nministic. This observed spread in transitions originates from two primary sources: the non-injective\nnature of the learned representation and the stochasticity of the behavior policy employed dur-\ning data collection.\nFirst, the non-injective mapping of the encoder function fenc may result in multiple distinct high-\ndimensional state-macro-action pairs being mapped to the same latent code as shown in the his-\ntograms of Fig.6.\nSpecifically, for different state-macro-action pairs x(1)\nt\n= (s(1)\nt , m(1)\nt ) and\nx(2)\nt\n= (s(2)\nt , m(2)\nt ), it is possible that:\nfenc(x(1)\nt ) = fenc(x(2)\nt ) = zt,\neven though x(1)\nt\n̸= x(2)\nt . Consequently, their corresponding next state-macro-action pairs x(1)\nt+1 and\nx(2)\nt+1 may differ, potentially leading to different next latent codes upon encoding:\nz(1)\nt+1 = fenc(x(1)\nt+1),\nz(2)\nt+1 = fenc(x(2)\nt+1),\nwith\nz(1)\nt+1 ̸= z(2)\nt+1.\nSecond, because the behavior policy πb used for data collection may be stochastic, it introduces\nvariability in the selection of macro-actions at both the current and subsequent time steps. Given a\nstate st, the behavior policy determines the macro-action mt as follows:\nmt ∼πb(m | st).\n18\n\n\nPublished as a conference paper at ICLR 2025\nThis stochastic selection can result in different macro-actions m(1)\nt\nand m(2)\nt\nbeing chosen from the\nsame state st, which naturally introduces stochasticity. Note that even if the encoder maps both\nx(1)\nt\n= (st, m(1)\nt ) and x(2)\nt\n= (st, m(2)\nt ) to the same latent code zt:\nfenc(x(1)\nt ) = fenc(x(2)\nt ) = zt.\nthe next states s(1)\nt+1 and s(2)\nt+1 might differ, even though the environment dynamics Tenv are determin-\nistic, i.e.,\ns(1)\nt+1 = Tenv(st, m(1)\nt ),\ns(2)\nt+1 = Tenv(st, m(2)\nt ),\nwith\ns(1)\nt+1 ̸= s(2)\nt+1.\nThese different next states lead to different next state-macro-action pairs:\nx(1)\nt+1 = (s(1)\nt+1, m(1)\nt+1),\nx(2)\nt+1 = (s(2)\nt+1, m(2)\nt+1).\nUpon encoding, they may yield different next latent codes:\nz(1)\nt+1 = fenc(x(1)\nt+1),\nz(2)\nt+1 = fenc(x(2)\nt+1),\nwith\nz(1)\nt+1 ̸= z(2)\nt+1.\nTherefore, even in a deterministic environment, the combination of a non-injective encoder and\na stochastic behavior policy introduces variability in the latent transitions. The heatmaps for de-\nterministic environments empirically demonstrate this spread, showing that each zt does not map\ndeterministically to a single zt+1 but rather to a distribution of possible next latent codes.\nC.2\nSTOCHASTIC ENVIRONMENT HEATMAPS\nThe heatmaps for stochastic environments as shown in Fig. 7 exhibit a more pronounced spread in\ntransition probabilities. This inherent environmental stochasticity means that for a given st and mt,\nthere are multiple possible next states st+1, leading to a wider distribution of next latent codes zt+1\nupon encoding. When combined with the non-injective mapping of the encoder and the stochasticity\nof the behavior policy, the uncertainties in the latent transitions are further amplified.\nC.3\nTHE IMPACT OF L1 REGULARIZATION ON REPRESENTATION FIDELITY\n(a) L1 norm\n(b) L2 norm\nFigure 8: Transition Probability Heatmaps for Medium-Replay Datasets from the Stochastic Hopper\nEnvironment (Top 50 Frequent Latent Codes). Left: Heatmap depicting transition probabilities when\nembeddings are regularized using the L1 norm. Right: Heatmap illustrating transition probabilities\nunder L2 norm regularization.\nThe heatmaps shown in Fig.8 reveal distinct patterns between transition probabilities for latent codes\nencoded by encoders trained with L1 and L2 norm regularization in the latent space. The L2 norm\ndemonstrates more distributed transition probabilities, with multiple moderate-probability transi-\ntions (shown as light blue dots) for each current state, indicating the encoder preserves more granu-\nlar information. In contrast, the L1 norm exhibits highly deterministic transitions for certain latent\ncodes, shown by the predominantly dark purple background with the bright yellow spot approach-\ning probability 1.0. This suggests that the encoder trained with L1 regularization tends to collapse\ndissimilar inputs into the same latent code, leading to less nuanced representations.\n19\n\n\nPublished as a conference paper at ICLR 2025\nD\nANALYSIS OF PERFORMANCE TRENDS WITH INCREASING\nSTOCHASTICITY\nTable 6: Hopper Environment Results with Increasing Stochasticity\nModel-Based\nModel-Free\nDataset Type\nEnv\nL-MAP\nTAP\nTT\n1R2R\nCQL\nIQL\nDeterministic\nMedium-Expert\nHopper\n105.74 ± 2.24\n85.55 ± 3.83\n110.0 ± 2.7\n57.40 ± 6.06\n105.4\n91.5\nMedium\nHopper\n73.90 ± 1.91\n69.14 ± 2.33\n61.1 ± 3.6\n55.49 ± 3.99\n58.0\n66.3\nMedium-Replay\nHopper\n91.18 ± 0.56\n80.92 ± 3.79\n91.5 ± 3.6\n89.67 ± 1.92\n95.0\n94.7\nMean (Deterministic)\n90.27\n78.54\n87.53\n67.52\n86.13\n84.17\nModerate Stochasticity\nMedium-Expert-Mod\nHopper\n106.11 ± 2.16\n40.86 ± 5.42\n56.10 ± 3.33\n52.19 ± 8.37\n106.17 ± 2.16\n60.61 ± 3.46\nMedium-Mod\nHopper\n55.07 ± 3.06\n43.64 ± 2.25\n44.49 ± 2.47\n65.24 ± 3.31\n49.92 ± 3.00\n56.00 ± 3.60\nMedium-Replay-Mod\nHopper\n52.30 ± 2.65\n38.10 ± 3.22\n37.85 ± 1.19\n22.82 ± 2.08\n40.53 ± 1.52\n49.12 ± 3.38\nMean (Moderate Stochasticity)\n71.16\n40.87\n46.15\n46.75\n65.54\n55.24\nHigh Stochasticity\nMedium-Expert-High\nHopper\n66.93 ± 3.46\n37.31 ± 3.66\n58.04 ± 3.60\n37.99 ± 2.71\n68.03 ± 3.94\n44.83 ± 2.58\nMedium-High\nHopper\n55.32 ± 3.56\n43.93 ± 2.66\n41.26 ± 5.53\n33.99 ± 0.92\n45.21 ± 2.97\n49.69 ± 2.47\nMedium-Replay-High\nHopper\n58.05 ± 3.36\n48.69 ± 2.97\n39.24 ± 2.16\n68.25 ± 3.78\n51.70 ± 3.09\n43.27 ± 2.78\nMean (High Stochasticity)\n60.10\n43.31\n46.18\n46.74\n54.98\n45.93\nThis section examines how L-MAP and baseline methods respond to increasing levels of stochas-\nticity in the Hopper environment. Table 6 presents the performance metrics across deterministic,\nmoderate, and high stochasticity settings.\nIn the deterministic setting, L-MAP achieves a mean score of 90.27, indicating strong performance\nand outperforming all other model-based methods. Among the baselines, TT attains a mean of\n87.53, TAP achieves 78.54, and 1R2R scores 67.52. The model-free methods CQL and IQL also\nperform well, with mean scores of 86.13 and 84.17, respectively. The high scores across all methods\nsuggest that the deterministic environment poses minimal challenges, allowing both L-MAP and the\nbaselines to excel.\nAs the environment introduces moderate stochasticity, L-MAP’s mean performance decreases to\n71.16, reflecting a reduction of approximately 21% from its deterministic performance. The model-\nbased baselines experience larger declines; TAP’s mean drops to 40.87 (a 48% reduction), TT’s to\n46.15 (a 47% reduction), and 1R2R’s to 46.75 (a 31% reduction). The model-free methods also\nsuffer performance losses; CQL’s mean decreases to 65.54 (a 24% reduction), and IQL’s to 55.24 (a\n34% reduction). Despite the reductions, L-MAP maintains a higher mean score than all baselines\nin this setting, indicating better resilience to moderate stochasticity among both model-based and\nmodel-free methods.\nIn the setting of high stochasticity, L-MAP’s mean further decreases to 60.10, representing a total\nreduction of about 33% from the deterministic case. The model-based baselines continue to show\ndeclining trends; TAP’s mean falls to 43.31 (a 45% reduction), TT’s to 46.18 (a 47% reduction),\nand 1R2R’s to 46.74 (a 31% reduction). The model-free methods also see further decreases; CQL’s\nmean drops to 54.98 (a 36% reduction), and IQL’s to 45.93 (a 45% reduction). While all methods\nexperience performance degradation, L-MAP consistently outperforms the model-based baselines\nTAP and TT, and maintains an edge over the model-free methods CQL and IQL. The performance\nof L-MAP shows relatively better robustness among the baselines.\nThe overall trend indicates that increasing stochasticity adversely affects all methods, but L-MAP’s\nperformance diminishes at a slower rate compared to the other model-based methods. These results\nsuggest that L-MAP is more robust to stochastic variations in the environment than most of the\nbaseline methods, particularly the model-based ones.\nE\nPLANNING HYPERPARAMETERS\nFor all environments, we utilize the following hyperparameters for sampling during the search pro-\ncess: α = 0.1 and ϵ = 1, which determine the exploration rate of progressive widening; and set\n20\n\n\nPublished as a conference paper at ICLR 2025\nthe number of Monte Carlo Tree Search (MCTS) iterations to 100. Detailed parameters for each\nenvironment are presented in Table 7.\nTable 7: Planning Hyperparameters\nEnvironment\nM\nN\nB\nλ\nγ\nStochastic MuJoCo\n16\n4\n4\n0.5\n0.99\nD4RL MuJoCo\n16\n4\n4\n0.5\n0.99\nAdroit\n10\n2\n4\n0.5\n0.99\nAntMaze\n16\n2\n4\n0.5\n0.998\nCurrency\n32\n4\n4\n0.5\n0.99\nHIV Treatment\n5\n4\n4\n1.0\n0.99\n21\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21186v2.pdf",
    "total_pages": 21,
    "title": "Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction",
    "authors": [
      "Baiting Luo",
      "Ava Pettet",
      "Aron Laszka",
      "Abhishek Dubey",
      "Ayan Mukhopadhyay"
    ],
    "abstract": "Sequential decision-making in high-dimensional continuous action spaces,\nparticularly in stochastic environments, faces significant computational\nchallenges. We explore this challenge in the traditional offline RL setting,\nwhere an agent must learn how to make decisions based on data collected through\na stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),\nwhich addresses this challenge by learning a set of temporally extended\nmacro-actions through a state-conditional Vector Quantized Variational\nAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs\na (separate) learned prior model that acts as a latent transition model and\nallows efficient sampling of plausible actions. During planning, our approach\naccounts for stochasticity in both the environment and the behavior policy by\nusing Monte Carlo tree search (MCTS). In offline RL settings, including\nstochastic continuous control tasks, L-MAP efficiently searches over discrete\nlatent actions to yield high expected returns. Empirical results demonstrate\nthat L-MAP maintains low decision latency despite increased action\ndimensionality. Notably, across tasks ranging from continuous control with\ninherently stochastic dynamics to high-dimensional robotic hand manipulation,\nL-MAP significantly outperforms existing model-based methods and performs\non-par with strong model-free actor-critic baselines, highlighting the\neffectiveness of the proposed approach in planning in complex and stochastic\nenvironments with high-dimensional action spaces.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}