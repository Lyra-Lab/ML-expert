{
  "id": "arxiv_2502.21067v1",
  "text": "Fast 3D point clouds retrieval for Large-scale 3D Place Recognition\nChahine-Nicolas Zede1\nLaurent Caraffa1\nValérie Gouet-Brunet1\n1 LASTIG, IGN-ENSG, Gustave Eiffel University\nchahine-nicolas.zede@ign.fr, laurent.caraffa@ign.fr, valerie.gouet@ign.fr\nRésumé\nRechercher un nuage de points 3D est une tâche difficile\nqui consiste à identifier, au sein d’une base de référence,\nles nuages les plus similaires à un nuage requête donné.\nLes méthodes actuelles se concentrent principalement sur\nla description de ces nuages de points comme indicateur\nde similarité. Dans ce travail, nous nous focalisons sur\nl’accélération de la recherche en adaptant le Differentiable\nSearch Index (DSI), basé sur les transformers et initiale-\nment conçu pour la recherche d’information textuelle, à\nla recherche dans les nuages de points. Notre approche\ngénère des identifiants 1D, associés à des descripteurs 3D,\npermettant ainsi une recherche très efficace en temps con-\nstant. Pour adapter cette approche aux nuages de points,\nnous utilisons les Vision Transformers qui associent les de-\nscripteurs multidimensionnels à leurs identifiants 1D, tout\nen intégrant un encodage positionnel et sémantique. Les\nperformances de notre proposition sont évaluées face à\nl’état de l’art sur un jeu de données public, dans le con-\ntexte de la reconnaissance de lieux, en termes de qualité et\nrapidité des réponses retournées.\nMots Clef\nRecherche 3D, recherche à grande échelle, descripteurs\n3D, index de recherche différentiable, Reconnaissance de\nlieux basée sur le LiDAR.\nAbstract\nRetrieval in 3D point clouds is a challenging task that con-\nsists in retrieving the most similar point clouds to a given\nquery within a reference of 3D points. Current methods\nfocus on comparing descriptors of point clouds in order\nto identify similar ones.\nDue to the complexity of this\nlatter step, here we focus on the acceleration of the re-\ntrieval by adapting the Differentiable Search Index (DSI), a\ntransformer-based approach initially designed for text in-\nformation retrieval, for 3D point clouds retrieval. Our ap-\nproach generates 1D identifiers based on the point descrip-\ntors, enabling direct retrieval in constant time. To adapt\nDSI to 3D data, we integrate Vision Transformers to map\ndescriptors to these identifiers while incorporating posi-\ntional and semantic encoding. The approach is evaluated\nfor place recognition on a public benchmark comparing its\nretrieval capabilities against state-of-the-art methods, in\nterms of quality and speed of returned point clouds.\nKeywords\n3D retrieval, Scalable retrieval, 3D point cloud descriptors,\nDifferentiable Search Index (DSI), LiDAR-based place\nrecognition\n1\nIntroduction\nWithout any initial information on geolocation, the pro-\ncess of geolocalization from visual content typically be-\ngins with content-based indexing and retrieval [4].\nIn\nthis work, we focus on 3D point clouds data, potentially\nobtained from LiDAR sensors, Structure-from-Motion or\nphotogrammetry. This increasingly popular data is able to\nmap scenes at large scale and thus serving as a reference,\nfor geolocalization and mobile mapping purposes for ex-\nample.\nRetrieval in 3D point clouds involves finding the most\nsimilar point clouds to a given query within a reference\ndatabase of 3D point clouds. Research on retrieval within\n3D point clouds is still in its early stages, with solutions\nmainly focused on the indexing phase (i.e. the production\nof point cloud descriptors) rather than the retrieval phase,\nwhich involves efficiently finding similar descriptors in the\nreference. The current state of the art confirms that the\nmain application of retrieval in point clouds is geolocal-\nization. Unlike approaches relying on visual place recog-\nnition, 3D LiDAR point clouds have the benefits of being\ninvariant to lighting and weather conditions [13], making\nthem particularly relevant for this purpose.\nApplications of retrieval in point clouds. Improving ge-\nolocation accuracy of a system, based on LiDAR acquisi-\ntions, has several key applications. For instance, position\ncorrection of LiDAR-equipped smartphones in dense ur-\nban environments, where urban canyon reduces GNSS ac-\ncuracy. Point cloud retrieval may also apply for digital twin\nupdates between a reference point cloud and a new acqui-\nsition for land surveying or monitoring structural changes\nover time. This is also relevant for autonomous naviga-\ntion, where loop closure detection plays a crucial role in\narXiv:2502.21067v1  [cs.CV]  28 Feb 2025\n\n\n……\nMulti-head self-attention\nFeed forward\nBOS\nEOS\nText decoder\nBOS\nText decoder\n(a) Pre-training/labelling\n……\n:\nx:1345 y:5413\nembed\ntokenize\n84 \n#14 ##50 \n84 \n#14 ##50 \n##56 ##5 \n##56 ##5 \nBeam search\n0.86\n0.65\n0.55\n#1\n#n-1\n#n\n841450151565\nPoint \nencoder\nPoint \nencoder\n8414502151565\n8414543141332\n8413143145645\n(b) inference / retrieval\nquery\nPoint\nNetworks\nLabel\nBeginning-of-sequence \nEnd-of-sequence\nFrozen networks \n tokens :\nFigure 1: Differentiable search index pipeline [21] adapted\nfor 3D data retrieval using GIT architecture [25]. (a) Label-\ning step: for each input of the database (red point cloud),\nthe text decoder learns to generate the label according to\nthe encoded point cloud. (b) Retrieval: given a point cloud\nas a query (black point cloud), a beam search is performed\nby the text decoder to provide the n most probable solu-\ntions.\nreducing drift in Simultaneous Localization and Mapping\n(SLAM) systems [26]. This could extend to media verifica-\ntion, fact checking videos for example, by geolocating 3D\npoint clouds reconstructed from video (e.g with Structure-\nFrom-Motion).\nThe main objective of the article is to contribute to the re-\ntrieval phase, by proposing a scalable retrieval method, fac-\ning the size of the reference dataset and the dimensionality\nof the descriptors. We explore the application of Differen-\ntiable Search Index (DSI) [21] methods, originally devel-\noped for Information Retrieval in large text corpora, to the\ndomain of 3D point clouds. DSI employs a single T5 (text-\nto-text transfer transformer) model to map string queries to\nrelevant document identifiers (docids). To bridge the gap\nbetween the 3D point cloud queries and the textual input\nrequired for Information Retrieval, we explored existing\nPlace Recognition methods, such as PointNetVLAD [22]\nand LoGG3D-Net [24], which have shown promising re-\nsults in loop detection by encoding a point cloud into a\nglobal descriptor, a single feature vector that summarizes\nthe entire point cloud geometry. Our model takes inspira-\ntion from Generative Image-to-text Transformer (GIT) [25]\nwhich is a Vision Transformer (ViT) used for image cap-\ntioning. Fig. 1 illustrates the whole process, where the\nGIT image encoder is replaced with a point cloud encoder\nwhere the text decoder is trained to map 3D data to unique\nidentifiers. The main contributions of this paper are:\n• We propose DSI-3D, an adaptation of DSI methods to\nsupport scalable retrieval in 3D point clouds by train-\ning the model to map a point cloud representation to a\ncorresponding scene docid.\n• We introduce novel indexing strategies to enhance\ndocument representation, incorporating geocoding\nand Hilbert curve-based methods.\nThis paper is organized as follows: Section 2 revisits the\nstate of the art on 3D point cloud retrieval and main con-\ncepts, applied to the problem of place recognition. Section\n3 introduces our proposal while Sections 4 and 5 present\nits evaluation, before concluding in Section 6.\n2\nRelated work and main concepts\nIn this section, we present a state of the art on 3D point\nclouds retrieval in Section 2.1, followed by a closer look at\nthe loss function used by LoGG3D-Net in Section 2.2. We\npresent the concepts of Differentiable Search Index in 2.3\nand how it can be adapted to 3D data with Vision-language\nmodel in Section 2.4.\n2.1\nRetrieval in 3D point clouds\nMost of the approaches of retrieval in 3D point clouds\nencountered are applied to the problem of place recogni-\ntion, i.e. the capability of determining the geolocation of a\npoints cloud query from the most similar clouds retrieved\nin a geolocalized reference. First, the reference dataset is\nusually divided into multiple smaller samples, each of them\nbeing geolocalized and indexed with a descriptor.\nSec-\nond, the same type of descriptor is computed for the query\nand compared to those of the reference dataset. The best\nmatches yield the most likely locations of the query.\nThere are multiple families of representation for 3D point\nclouds. A first distinction can be made between local and\nglobal feature-based strategies.\nLocal features-based. Local features gather local infor-\nmation, like density or local 3D convolution, about a spe-\ncific region of the cloud, such as particular points or voxels.\nThese place recognition techniques may suffer from accu-\nracy loss due to viewpoint changes or the need for dense\npoint clouds.\nConvolutional Neural Networks (CNNs)\nhave demonstrated strong feature learning capabilities for\n2D image data [1], but their application to LiDAR-based\nPlace Recognition is challenging due to the sparse and un-\nordered nature of point clouds. PointNet [16] achieves di-\nrect features learning from raw point cloud, while Point-\nNet++ [17], extends this approach by recursively applying\n[16], learning hierarchical features considering distance\nmetric.\nIn contrast, there is also a global features-based approach\nthat represents the entirety of the query with one object,\noften a vector. This global representation tends to extract\nglobal information from the aggregation of local features.\nThey have the advantage of being viewpoint-invariant and\ncomputationally less expensive for the retrieval.\nGlobal features handcrafted-based. Handcrafted-based\nfeatures do not rely on a previous model learning to be\napplied for place recognition. Among them, we can cite\nScan Context [9] which encodes a scene into one rotation-\ninvariant global descriptor using a polar projection of visi-\nble points, without requiring prior training. However, this\napproach struggles with lateral offsets invariance.\nScan\nContext++ [8] addresses this issue by employing polar and\n\n\nCartesian context.\nGlobal learned features-based. By contrast with hand-\ncrafted methods, learned point clouds descriptors have\nbenefited from deep learning.\nA pioneering end-to-end\nlearning-based method, PointNetVLAD [22], generates a\nglobal descriptor from point cloud data. It combines Point-\nNet [16] for local feature extraction, with NetVLAD [1], a\nlearnable CNN layer that aggregates the local features with\ninto a single global descriptor. Since NetVLAD is a sym-\nmetric function, i.e, it is input order invariant, so can be\napplied to unordered point clouds.\nGlobal features graph-based.\nTo better capture struc-\ntural information in large-scale environments, graph-based\nmethods model the relationship between an object and its\ntopology. LPD-Net [12] introduces graph-based aggrega-\ntion to analyze spatial distribution. It computes ten types\nof local features, including curvature changes, local point\ndensity, and 2D scattering, for each point. A Graph Neural\nNetwork (GNN) is then used to aggregate points distribu-\ntions and local features into a feature space and a Carte-\nsian space respectively. Finally, NetVLAD [1] generates\nthe global descriptor. While LPD-Net improves retrieval\nperformances upon previous results, it may be limited by\nits high computational cost and architectural complexity.\nGlobal features segment-based.\nSegment-based meth-\nods divide a scene into meaningful regions (e.g., roads,\nbuildings, vegetation), providing a more structured repre-\nsentation that captures topological relationships between\nsegments, rather than treating the entire point cloud as a\nsingle entity. Locus [23] aggregates these segment-based\nrepresentations into a global descriptor, by modeling the\ntopological relationships and temporal consistency seg-\nment feature. This is achieved via second-order pooling,\nwhere local features are represented as a matrix, by us-\ning the outer product with its transposed matrix and taking\nthe element-wise maximum. It is followed by Eigen-value\nPower Normalization (ePN) [11] is applied to enhance de-\nscriptor description.\nGlobal features sparse convolution. MinkLoc3D [10] en-\nhances local features extraction by introducing the first 3D\nsparse convolution-based architecture. A local feature ex-\ntraction module is applied to the sparsified point clouds,\nand a global descriptor is produced using Generalized-\nMean pooling [18]. Similarly, LoGG3D-Net [24] builds\nupon sparse volumetric methods, like those in MinkLoc3D,\nbut introduces a local consistency loss function. This func-\ntion enforces local feature similarity between a query and\na positive point cloud (within 3m), leading to more robust\nlocal descriptors. The global descriptor is then obtained\nusing second-order pooling [23]. In Section 2.2, we take a\ncloser look at this approach, especially its quadruplet loss,\nwhich has inspired our work.\nMulti-sourced global features.\nCross-source 3D place\nrecognition concerns the confrontation of point clouds ac-\nquired at different viewpoints, e.g. ground-based LiDAR\nscans with aerial ones. It presents significant challenges\ndue to differences in coverage, density and noise patterns.\nTo bridge the representation gaps of those types of acquisi-\ntion, CrossLoc3D [7] introduces a cross-source 3D match-\ning approach by learning a shared embedding space be-\ntween aerial LiDAR database and ground LiDAR scans.\nIt employs multi-scale sparse convolutions for feature se-\nlection, followed by a NetVLAD aggregator to obtain a\nunique descriptor. An iterative refinement step aligns the\ntwo embedding spaces into a shared one.\n2.2\nLoGG3D-Net\nTo reinforce the global descriptors similarity during the\ntraining, approach LoGG3D-Net applies a quadruplet loss\nLquad ∈[0, +∞[, to a tuple (Pq, Pp, Pn, Pnbis):\nLquad =\nN\nX\ni\n([∥g(Pq)−g(Pp)∥2\n2−∥g(Pq)−g(Pni)∥2\n2+α]+\n+ [∥g(Pq) −g(Pp)∥2\n2 −∥g(Pnbis) −g(Pni)∥2\n2 + β]+)\n(1)\nwhere Pq is the query point cloud, Pp is a positive one\nwithin p = 3m of the query, Pn is a negative point cloud\ndistant from n = 20m of the query, Pnbis respects the neg-\native condition with both Pp and Pn. g(P∗) is the final\nglobal descriptor vector, N is the number of sampled neg-\natives and []+ the hinge loss function, while α and β are\nthe loss margin of the quadruplet. For the retrieval step,\nthe Cosine distance performs global descriptors retrieval\nby similarity between the query and the reference dataset\nitems. In [24], previous entries adjacent to the query in\ntime by less than 30s are excluded from the search to avoid\nmatching with the same instance.\n2.3\nDifferentiable Search Index\nIn text information retrieval, the objective is to find the\nmost relevant document to answer a given question. Pa-\nper [21] introduces the Differentiable Search Index (DSI),\na method that fully parameterized the traditionally multi-\nstage retrieve-then-rank pipeline within a single T5 trans-\nformer model. The DSI architecture also supports beam\nsearch, allowing it to return multiple possible answers\nranked by their sequence scores. These scores, or logits\n∈]−∞, +∞[ represent the raw prediction scores from the\nlanguage modeling head for each token in the vocabulary\nbefore the SoftMax function. The SoftMax function con-\nverts them into probabilities over the classes such as Soft-\nMax(logits) ∈[0, 1]. The probability score returned by the\nT5 model of the generated docid i for a query q can be\ncomputed as [27]:\np(i|q, θ) =\nY\ni=1\np(it|T5θ(q, i0, i1, ...it−1))\n(2)\nwhere it is the i-th token in the docid string. To achieve\nthis, DSI operates in two fundamental modes:\nIndexing: The DSI model learns to associate each doc-\nument with its corresponding document identifier (docid)\n\n\nusing a sequence-to-sequence approach [20]. It takes the\ndocument tokens as input and generates the corresponding\nidentifier as output.\nRetrieval: For a given input query, the DSI model gener-\nates a ranked list of candidate docids through autoregres-\nsive generation [2]. More precisely, three strategies are\nused for representing docids in document retrieval:\nUnstructured Atomic Identifiers is the simplest ap-\nproach, where each document is represented by a unique,\narbitrary integer identifier. The model is trained to emit one\nlogit which is the last layer unscaled score of the model,\nfor each unique docid, with the SoftMax function applied\nto the final layer’s hidden state in the decoder. To retrieve\nthe top-k documents, the logits are sorted, and the indices\nof the highest values are returned.\nNaively Structured String Identifiers represents each\ndocument using arbitrary unique integers as tokenized\nstrings, which avoids the need for a large SoftMax layer\nand simplifies training. A partial beam search tree [19] is\nemployed to generate the top-k scores. It is similar to a\nmulti-label classification.\nSemantically Structured Identifiers encode semantic in-\nformation within each document’s identifier, structuring\nthem to reduce the search space with each decoding step.\nThis results in identifiers where semantically similar doc-\numents share common prefixes, facilitating retrieval. One\nmethod to construct these identifiers is to group similar de-\nscriptors by applying k-means clustering, then concatenat-\ning the resulting cluster indices to form each identifier.\nA limitation of [21] is the size difference between the in-\ndexing phase, which processes an entire text document and\nthe retrieval queries, which are sentence-sized. To address\nthis, [27] proposes representing each document as a set of\nrelevant queries, using the query generation model doc-\nTquery [14], leading to improved retrieval performances\ncompared to the original DSI model. In our study, the re-\ntrieval and the indexing phase are both done on the same\nformat of point clouds descriptors. However query gener-\nation could be a way to reinforce the mapping between a\nscene and its index.\n2.4\nFeature-to-text\nTo bridge the modality gap between text information and\n3D points, we first explored existing Vision-Language\n(VL) models, which are trained on vision-language tasks\nsuch as generating image captions.\nAmong image-to-\ntext transformers, GIT stands out for its simple archi-\ntecture [25], consisting of a single image encoder and a\ntext decoder.\nThe generated captions are decoded from\nthe sequence of tokens with the highest scores. Decod-\ning starts with a [BOS] (Beginning-Of-Sequence) token\nand proceeds in an autoregressive manner until reaching\nthe [EOS] (End-Of-Sequence) token or the maximum se-\nquence length.\nDuring pre-training, for each image-text pair, the language\nmodeling Lm loss ∈[0, +∞[ is applied, with I the image,\nyi for i ∈{1, , N} the text tokens, y0 the [BOS] token and\nyN+1 the [EOS] one:\nLm =\n1\nN + 1\nN+1\nX\ni\n(CE(yi, P(yj|J = 0, ..., i1))\n(3)\nwhere CE is the cross-entropy loss:\nCE = −\nC\nX\nc=1\nωcLog\nexp (xc)\nPC\ni=1 exp (xi)\n.yc\n(4)\nand x is the input logit, y the target, ω the weight, and C\nthe number of classes.\n3\nproposed method\nInspired by the concepts of Section 2 and particularly the\nDSI search method [21], this work aims to bring the scal-\nable aspect of text information retrieval to 3D point clouds\nretrieval.\n3.1\nDSI-3D\nApplying the DSI method requires converting 3D data into\ntext form.\nThis involves a two-step process: first, en-\ncoding the 3D point cloud into an embedding, and then\npassing this embedding through a multimodal transformer\nto generate captions. For the point cloud encoding, we\nchoose LoGG3D-Net [24] due to its local consistency loss,\nwhich significantly enhances retrieval performances for\nplace recognition, but other point cloud global descriptors\ncould be used. The multimodal transformer chosen is GIT\n[25] for its streamlined architecture (see Section 2.4). By\nsubstituting the image encoder of GIT with a 3D encoder,\nwe adapt it to generate captions that serve as docids for\npoint clouds, derived from scene descriptors. For the train-\ning part, we use both the regular GIT loss and a variation of\nthe language modeling Lm loss (3) to be triplet and quadru-\nplet losses Ltripl and Lquad (both in [0, +∞[), as follows:\nLtripl = Lm + (Lmpos −Lmneg + α)\n(5)\nLquad = Ltripl + (Lmpos −Lmnegbis + β)\n(6)\nWith Lmpos, Lmneg and Lmnegbis representing the cross-\nentropy between the scene docid and output logits for a\npositive point cloud, a negative point cloud, and a second\nnegative point cloud, respectively. α and β are resp. fixed\nat 0.5 and 0.3. Finally, the retrieval step is performed with\na beam search with a restricted vocabulary to meaningful\nindexes.\n3.2\nDocid representation\nFor the representation of the docid, we exclude Unstruc-\ntured Atomic identifiers (see Section 2.3), as they are the\nleast scalable option. For Naively Structured String iden-\ntifiers, a natural analogy can be drawn with the naming of\neach point cloud in classical benchmarks with acquisition\n\n\nsequences (e.g. KITTI for mobile mapping), where each\npoint cloud is named sequentially from 0 to N. The Se-\nmantically Structured identifiers strategy is also assessed.\nIn this approach, neighboring point clouds with similar de-\nscriptors, and thus being in the same cluster, will share sim-\nilar identifier prefixes; naturally, this method extends well\nto revisits.\nAdditionally, we introduce Positional Structured iden-\ntifiers, a novel representation tailored for place recog-\nnition where the coordinates of a point cloud are de-\nfined in a Cartesian coordinate system,\ndenoted as\nXN={x0, . . . , xN−1} and YN={y0, . . . , yN−1} (the ZN\ncoordinate being excluded here as we focus on planar ge-\nolocation), corresponding to the set of strings values of the\ncenter coordinates of a sequence of point clouds of length\nN. We propose generating unique identifiers by interlacing\nthe digit strings coordinates of each point cloud n, xn and\nyn, starting from the most significant digit of xn down to\ncentimeter precision of yn. The values of the coordinates\nXN and YN are shifted to include only non-negative values\nand scaled to ensure centimeter-level accuracy, enabling\nunique identifiers even for closely revisited locations.\nAs an example, the docid returned for a point cloud n\nwith center coordinates xn=’1111’ and yn=’2222’ will be\ndocid=’12121212’. This bijective mapping between point\nclouds and identifiers guarantees that each revisit of a scene\nwill have a closely related identifier.\nWe also propose\na variant of this positional indexing, using space-filling\ncurves, which map a multidimensional space onto a 1D\ncurve, such as the Hilbert curve, known for its superior\nlocality preservation [5]. After shifting and rescaling the\nXN and YN coordinates into positive integers, they form a\n2D grid where center coordinates of each point cloud are\nassigned a unique identifier derived from the Hilbert curve.\nFor our experiments with the KITTI dataset, the Hilbert\ncurve is applied with 17 iterations.\n4\nexperimental setup\nIn this section, we present the dataset, the evaluation crite-\nria and the experimental setup chosen. Here, LoGG3D-Net\nserves both as our 3D encoder for DSI-3D as a baseline for\ncomparison.\n4.1\nDataset\nTo ensure comparability with the state of the art, we evalu-\nated our approach for place recognition using the classical\nKITTI LiDAR datasets [6]. We trained and evaluated our\nmodel on the six sequences containing revisits (seq. 00,\n02, 05, 06, 07, and 08) to assess the robustness of each\nvariant of our method. These sequences represent a total of\n18236 unique scenes, divided as follows according to the\nsequence: 4541, 4661, 2761, 1101, 1101 and 4071 point\nclouds, covering approximately 1.13km².\nTo verify the\nscalability of our model, we constructed a new sequence,\nnumbered 22, by concatenating point clouds from the orig-\ninal six ones. Since each sequence is defined in local co-\nordinates, we applied a shift of (i × 1000m, i × 1000m,)\nto each point cloud coordinates, where i represents the se-\nquence number during the combination process, to prevent\noverlapping sequences. 80% of each sequence is allocated\nto the training set, while the remaining 20% is equally di-\nvided between the validation and evaluation sets. Point\nclouds are assigned to these sets based on their Naively\nStructured String identifiers as follows: those with indices\nnot divisible by five are placed in the training set; those\nwith indices divisible by ten are assigned to the evalua-\ntion set; and the remaining point clouds (indices divisible\nby five but not by ten) are included in the validation set.\nThis division creates a gap every five point clouds, allow-\ning the model to be evaluated on its ability to identify the\nclosest neighbors of scenes it was not trained on. During\nLoGG3D-Net evaluation, the point clouds in the training\nset are computed and stored into a ’seen place’ database\nto replicate the parametrization of the training set of our\nmodel.\n4.2\nEvaluation Criteria and baseline\nTwo main criteria are used to evaluate DSI-3D and the\nbaseline:\n• The Hits@N metric, used in information retrieval,\nreports the proportion of correct docid ranked within\nthe top N predictions. Here, we focus on the more\ndiscriminative case with N = 1.\n• From a place recognition perspective, we evaluate en-\ncoding performance based on the maximum F1 score\nor F1max for each sequence [23, 24]. For LoGG3D-\nNet, it is found by iterating over the similarity thresh-\nold that maximizes the F1 score. We adapt this F1\nscore threshold for our model, by iterating over logit\nscore, while maintaining the same geometric revisit\ncriteria as in [24]. A top-1 retrieval is considered pos-\nitive if its logits scores is below the maximum con-\nfidence threshold. A positive retrieval is classified as\ntrue positive if it is within 3m of the ground truth pose\nof the query, and as false positive if it exceeds 20m.\nFor negative retrievals, where the logits exceed the\nmaximum confidence threshold, a true negative is as-\nsigned if the query is not revisited, meaning no point\ncloud within 3m of the location of the query is ob-\nserved with a timestamp greater than 30s; otherwise,\nit is considered as a false negative.\nAs baseline,\nwe compare DSI-3D facing descriptor\nLoGG3D-Net exploited within two retrieval strategies: first\nclassically for exact retrieval based on the exhaustive com-\nparison of all the descriptors in the reference and secondly,\nmore optimally for approximate retrieval based on approxi-\nmate nearest neighbor (ANN) [15] relying on Locality Sen-\nsitive Hashing (LSH) from the FAISS library [3]. More\nprecisely, we used the binary flat LSH index, where the\nnumber of bits H =∈{32, 256} corresponds to the num-\nber of random hyperplane projections (or random rotations\n\n\nin this implementation), respectively, named LSH32 and\nLSH256 in the experiments.\n4.3\nImplementation Details\nThe experiments are conducted using a PyTorch implemen-\ntation on a single V100 GPU. For training the DSI model,\nwe use the pre-trained 3D encoder from LoGG3D-Net and\nthe text decoder from the base-sized GIT checkpoint, fine-\ntuned on COCO1. The evaluation is done using the public\nimplementation of LoGG3D-Net and the checkpoints ex-\nploited for each sequence were selected using the leave-\none-out strategy. For the combined sequence 22, we em-\nployed the LoGG3D-Net checkpoint trained while leaving\nout sequence 00, ensuring that all 18236 scenes were en-\ncoded within the same representation space. The best GIT\ncheckpoints are retained for evaluating each sequence and\nindexing strategy, based on their performance on the val-\nidation set.\nPositive and negative point cloud pairs are\nsampled based on distances set to p = 3 and n = 20m\nduring the training. Setting n = 20m during the evalua-\ntion leads to undiscriminating results between the different\napproaches. A solution to distinguish the best model was\nto set n = 3m during the evaluation. This stricter thresh-\nold highlights the ability of the model to identify neighbors\nclose to the query.\n5\nResults\nThis experimental section is organized as follows: Section\n5.1 evaluates the impact of the loss function on the training\nof DSI-3D in terms of retrieval performance. Then Section\n5.2 evaluates our models, using different indexing strate-\ngies, and compares them to exact and approximate search\nmethods. Finally, Section 5.3 discusses the scalability of\nour approach in terms of query complexity and retrieval\ntime.\n5.1\nLoss function\nTable 1 presents the F1max and Hits@1 scores obtained\nfor the six original sequences on our model, with the GPS\nstrategy, by exploiting the triplet and the quadruplet losses\nof Section 3.1 (again with a negative distance threshold of\n3m). We observe that the triplet loss achieves a marginally\nhigher average F1max score, while the quadruplet one per-\nforms slightly better on the Hits@1 score. For the remain-\ning experiments, we chose the quadruplet loss as it pro-\nvides the best results on both metrics for the longest se-\nquences 00 and 02 and with a shorter amount of training.\nTable 1: Scores for loss function comparison on DSI-3D\nF1max\n00\n02\n05\n06\n07\n08\nLtripl\n0.9775\n0.9725\n0.9946\n0.9674\n0.9954\n0.9875\nLquad\n0.9809\n0.9758\n0.9891\n0.9674\n0.9955\n0.9851\nHits@1\n00\n02\n05\n06\n07\n08\nLtripl\n0.9560\n0.9465\n0.9892\n0.9369\n0.9820\n0.9755\nLquad\n0.9626\n0.9507\n0.9783\n0.9369\n0.9909\n0.9705\n1https://huggingface.co/microsoft/git-base-coco\nTable 2: F1max scores for LoGG3D-Net (with and without\nLSH) and various strategies of DSI-3D.\nModel\n00\n02\n05\n06\n07\n08\n22\nLoGG3D-Net\n0.9956\n0.9859\n1.0\n0.9954\n1.0\n0.9974\n0.9925\nLoGG3D-Net + LSH256\n0.9945\n0.9792\n0.9946\n0.9815\n0.9626\n0.9888\n0.9850\nLoGG3D-Net + LSH32\n0.9416\n0.8979\n0.9681\n0.9051\n0.9670\n0.9448\n0.9290\nOurs (Label)\n0.8991\n0.8774\n0.7906\n0.7954\n0.8844\n0.9044\n/\nOurs (Hierar)\n0.9635\n0.9316\n0.9684\n0.9626\n0.9817\n0.9684\n/\nOurs (GPS)\n0.9809\n0.9758\n0.9891\n0.9674\n0.9955\n0.9851\n/\nOurs (Hilbert)\n0.9933\n0.9781\n0.9909\n0.9817\n0.9955\n0.9913\n0.9819\n5.2\nIndexing strategies comparison\nTable 2 and Table 3 resp. present the F1max scores and\nthe Hits@1 scores obtained for the LoGG3D-Net baseline\n(without and with index LSH) and several variants of DSI-\n3D, namely: names ’Label’, ’Hierar’, ’GPS’ and ’Hilbert’\nrefer to the indexing strategies Naively Structured String\nidentifiers, Semantically Structured identifiers, Positional\nStructured identifiers and Hilbert curve Positional Struc-\ntured identifiers respectively. The sequence 22 was only\ntrained with the Hilbert curve Positional Structured identi-\nfiers as it is the most promising indexing strategy.\nTable 3: Hits@1 scores for LoGG3D-Net (with and with-\nout LSH) and DSI-3D.\nModel\n00\n02\n05\n06\n07\n08\n22\nLoGG3D-Net\n0.9956\n0.9722\n1.0\n0.9910\n1.0\n0.9950\n0.9852\nLoGG3D-Net + LSH256\n0.9890\n0.9572\n0.9892\n0.9550\n0.9820\n0.9755\n0.9693\nLoGG3D-Net + LSH32\n0.6110\n0.5225\n0.6173\n0.5676\n0.8108\n0.6176\n0.5049\nOurs (Label)\n0.8132\n0.7645\n0.8831\n0.6306\n0.7928\n0.8235\n/\nOurs(Hierar)\n0.9319\n0.8630\n0.9387\n0.9279\n0.9640\n0.9387\n/\nOurs (GPS)\n0.9626\n0.9507\n0.9783\n0.9369\n0.9909\n0.9705\n/\nOurs (Hilbert)\n0.9846\n0.9550\n0.9819\n0.9640\n0.9910\n0.9828\n0.9644\nRegardless of the evaluation criteria (Tables 2 and 3), we\nfirst observe that accelerating LoGG3D-Net retrieval with\nLSH affects the quality of the responses more or less, com-\npared to exact retrieval (first line in each Table), depending\non the value chosen for H.\nSecondly, among the various indexing strategies tested\nfor DSI-3D, Hilbert curve indexing yields the best re-\nsults, conducing to the conclusion that the strategies of\nthe state of the art (’Label’ and ’Hierar’) can be out-\ndated for retrieval dedicated to place recognition.\nThis\nversion does not achieve the same performance as the ex-\nact baseline LoGG3D-Net, which provides slightly better\nHits@1 score, by an average improvement of 1.6% (me-\ndian at 1.3%) ; for F1max, ’Hilbert’ is less efficient of\n0.8% on average (median at 0.5%). However, we observe\nthe same behavior, as well as scores comparable to DSI-\n3D ones, for approximate retrieval when adding LSH256 to\nLoGG3D-Net. Globally, DSI-3D with Hilbert curve index-\ning achieves even a higher average F1max than LoGG3D-\nNet with LSH256 by 0.4% (median at 0.6%) and a better\naverage Hits@1 by 0.1% (median at 0.6%).\nAs conclusion of this section, the behavior of DSI-3D, in\nterms of retrieval quality, can be considered at least as ef-\nfective as a conventional state-of-the-art approximate ap-\nproach. In the following section, we discuss its superiority\nfacing the questions of complexity and retrieval time.\n\n\n5.3\nComplexity and retrieval time\nIn its current implementation [24], retrieval with LoGG3D-\nNet is performed over the entire reference dataset by se-\nquential comparisons (first line of Tables 2 and 3), making\nretrieval time proportional to the dataset size (n) with a\ntime complexity O(n). The state-of-the-art solutions listed\nin Section 2.1 proceed in the same way. Similarity search\nin such high-dimensional spaces can be accelerated with\nindexes like the LSH [15], as experimented in the same Ta-\nbles (lines 2 and 3), whose complexity largely differs from\nthe variant used but is generally dominated by O(nρ) with\n0 ≤ρ ≤1, under the approximate retrieval assumption. In\ncontrast, DSI-3D relies on model inference with constant\nO(1) complexity, which is a significant advantage facing\nlarge datasets of point clouds. It also works under the ap-\nproximate retrieval paradigm, but we have experimented in\nthe previous section that the retrieval quality remains fully\ncomparable with the ones of other solutions under the same\nparadigm.\nIn terms of retrieval time, the exact LoGG3D-Net, in its\npublic implementation, ranges from 0.0003 to 0.0093s in\naverage for all the queries. LoGG3D-Net with LSH256 is\n5.98 times faster, while LoGG3D-Net with LSH32 is 7.20\ntimes faster (but with a poor retrieval quality).\nFor DSI-3D indexed with Positional Structured identifiers,\ncurrently retrieval time is 0.45s, by measuring the time it\ntakes for a descriptor to pass through all GIT layers and\nreturn the docid. It is worth noting that the current code\nimplementation used in the article is not yet optimized.\nNevertheless, to give an insight on the current scalability\nof DSI-3D, we fitted a linear equation between the size of\nthe reference datasets Ndesc and retrieval times T with ex-\nact LoGG3D-Net (T = 0.67Ndesc−0.68), which allows to\nestimate that the inversion point occurs at Ndesc = 756.4e3\ndescriptors in favor of DSI-3D. According to the settings\nused, this amount roughly corresponds to a spatial cover-\nage of 50km², at which point DSI-3D would become faster\nin its current not optimized implementation.\nAmong the factors also affecting the retrieval time of DSI-\n3D is the number of beams explored during the beam\nsearch, which is fixed at 10, as well as the indexing strate-\ngies, affecting decoding steps. Reducing the number of\nbeams could improve retrieval time but may result in less\nprecise responses. Such an evaluation will be part of future\nexperiments.\n6\nConclusion\nIn this article, we have presented DSI-3D, a new indexing\nand retrieval approach for 3D point clouds, that drastically\naccelerates retrieval, compared to current literature which\nmainly focuses on point cloud description and performs re-\ntrieval following a linear time complexity as experimented\nhere, it can be slightly improved with off-the-shelf index\nsuch as LSH. The proposal relies on the adaptation of the\nDerivative Search Index, but also introduces two new Posi-\ntional Structured identifiers, fitted for the problem of place\nrecognition. This article has laid the foundations of the\napproach, exhibiting performances similar to those of the\nstate of the art in terms of retrieval quality. In the future, it\nwill be necessary to continue the experiments, in particular\nby varying the model parameters further, and also by con-\nsidering larger datasets to confirm its robustness in terms\nof the quality of the returned responses, its optimality in\nterms of response time, following a constant complexity,\nhaving been proven.\n7\nAcknowledgments\nThis work is supported by the French Ministry of the\nArmed Forces - Defence Innovation Agency (AID). It\nwas performed using HPC resources from GENCI-IDRIS\n(Grant 2024-AD011014030R1).\nReferences\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii,\nTomas Pajdla, and Josef Sivic. Netvlad: Cnn architec-\nture for weakly supervised place recognition. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 5297–5307, 2016.\n[2] Nicola De Cao, Gautier Izacard, Sebastian Riedel,\nand Fabio Petroni.\nAutoregressive entity retrieval.\narXiv preprint arXiv:2010.00904, 2020.\n[3] Matthijs Douze, Alexandr Guzhva, Chengqi Deng,\nJeff Johnson, Gergely Szilvasy, Pierre-Emmanuel\nMazaré, Maria Lomeli, Lucas Hosseini, and Hervé\nJégou.\nThe\nfaiss\nlibrary.\narXiv\npreprint\narXiv:2401.08281, 2024.\n[4] Shiv Ram Dubey. A decade survey of content based\nimage retrieval using deep learning. IEEE Transac-\ntions on Circuits and Systems for Video Technology,\n32(5):2687–2704, 2021.\n[5] Christos Faloutsos and Shari Roseman.\nFractals\nfor secondary key retrieval.\nIn Proceedings of the\neighth ACM SIGACT-SIGMOD-SIGART symposium\non Principles of database systems, pages 247–252,\n1989.\n[6] Andreas Geiger, Philip Lenz, Christoph Stiller, and\nRaquel Urtasun.\nVision meets robotics: The kitti\ndataset.\nThe international journal of robotics re-\nsearch, 32(11):1231–1237, 2013.\n[7] Tianrui\nGuan,\nAswath\nMuthuselvam,\nMontana\nHoover,\nXijun Wang,\nJing Liang,\nAdarsh Ja-\ngan Sathyamoorthy, Damon Conover, and Dinesh\nManocha.\nCrossloc3d: Aerial-ground cross-source\n3d place recognition.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pages 11335–11344, 2023.\n\n\n[8] Giseop Kim, Sunwook Choi, and Ayoung Kim. Scan\ncontext++: Structural place recognition robust to ro-\ntation and lateral variations in urban environments.\nIEEE Transactions on Robotics, 38(3):1856–1874,\n2021.\n[9] Giseop Kim and Ayoung Kim. Scan context: Ego-\ncentric spatial descriptor for place recognition within\n3d point cloud map.\nIn 2018 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems\n(IROS), pages 4802–4809. IEEE, 2018.\n[10] Jacek Komorowski. Minkloc3d: Point cloud based\nlarge-scale place recognition. In Proceedings of the\nIEEE/CVF winter conference on applications of com-\nputer vision, pages 1790–1799, 2021.\n[11] Peihua Li, Jiangtao Xie, Qilong Wang, and Wang-\nmeng Zuo. Is second-order information helpful for\nlarge-scale visual recognition? In Proceedings of the\nIEEE international conference on computer vision,\npages 2070–2078, 2017.\n[12] Zhe Liu, Shunbo Zhou, Chuanzhe Suo, Peng Yin,\nWen Chen, Hesheng Wang, Haoang Li, and Yun-Hui\nLiu. Lpd-net: 3d point cloud learning for large-scale\nplace recognition and environment analysis. In Pro-\nceedings of the IEEE/CVF international conference\non computer vision, pages 2831–2840, 2019.\n[13] Stephanie Lowry, Niko Sünderhauf, Paul Newman,\nJohn J Leonard, David Cox, Peter Corke, and\nMichael J Milford. Visual place recognition: A sur-\nvey. ieee transactions on robotics, 32(1):1–19, 2015.\n[14] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic.\nFrom doc2query to doctttttquery.\nOnline preprint,\n6(2), 2019.\n[15] J.J. Pan, J. Wang, and G Li. Survey of vector database\nmanagement systems. The International Journal on\nVery Large Data Bases, 33, 2024.\n[16] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J\nGuibas.\nPointnet: Deep learning on point sets for\n3d classification and segmentation. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 652–660, 2017.\n[17] Charles Ruizhongtai Qi,\nLi Yi,\nHao Su,\nand\nLeonidas J Guibas. Pointnet++: Deep hierarchical\nfeature learning on point sets in a metric space. Ad-\nvances in neural information processing systems, 30,\n2017.\n[18] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum.\nFine-tuning cnn image retrieval with no human an-\nnotation. IEEE transactions on pattern analysis and\nmachine intelligence, 41(7):1655–1668, 2018.\n[19] Ihsan Sabuncuoglu and M Bayiz. Job shop schedul-\ning with beam search. European Journal of Opera-\ntional Research, 118(2):390–412, 1999.\n[20] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Se-\nquence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27, 2014.\n[21] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Gupta, et al. Transformer memory as a dif-\nferentiable search index. Advances in Neural Infor-\nmation Processing Systems, 35:21831–21843, 2022.\n[22] Mikaela Angelina Uy and Gim Hee Lee.\nPoint-\nnetvlad: Deep point cloud based retrieval for large-\nscale place recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4470–4479, 2018.\n[23] Kavisha Vidanapathirana, Peyman Moghadam, Ben\nHarwood, Muming Zhao, Sridha Sridharan, and Clin-\nton Fookes.\nLocus: Lidar-based place recognition\nusing spatiotemporal higher-order pooling. In 2021\nIEEE International Conference on Robotics and Au-\ntomation (ICRA), pages 5075–5081. IEEE, 2021.\n[24] Kavisha Vidanapathirana, Milad Ramezani, Peyman\nMoghadam, Sridha Sridharan, and Clinton Fookes.\nLogg3d-net: Locally guided global descriptor learn-\ning for 3d place recognition. In 2022 International\nConference on Robotics and Automation (ICRA),\npages 2215–2221. IEEE, 2022.\n[25] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Lin-\njie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. Git: A generative image-to-text\ntransformer for vision and language. arXiv preprint\narXiv:2205.14100, 2022.\n[26] Yongjun Zhang, Pengcheng Shi, and Jiayuan Li.\nLidar-based place recognition for autonomous driv-\ning: A survey. ACM Computing Surveys, 57(4):1–36,\n2024.\n[27] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian\nPei, Ming Gong, Guido Zuccon, and Daxin Jiang.\nBridging the gap between indexing and retrieval for\ndifferentiable search index with query generation.\narXiv preprint arXiv:2206.10128, 2022.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21067v1.pdf",
    "total_pages": 8,
    "title": "Fast 3D point clouds retrieval for Large-scale 3D Place Recognition",
    "authors": [
      "Chahine-Nicolas Zede",
      "Laurent Carrafa",
      "Valérie Gouet-Brunet"
    ],
    "abstract": "Retrieval in 3D point clouds is a challenging task that consists in\nretrieving the most similar point clouds to a given query within a reference of\n3D points. Current methods focus on comparing descriptors of point clouds in\norder to identify similar ones. Due to the complexity of this latter step, here\nwe focus on the acceleration of the retrieval by adapting the Differentiable\nSearch Index (DSI), a transformer-based approach initially designed for text\ninformation retrieval, for 3D point clouds retrieval. Our approach generates 1D\nidentifiers based on the point descriptors, enabling direct retrieval in\nconstant time. To adapt DSI to 3D data, we integrate Vision Transformers to map\ndescriptors to these identifiers while incorporating positional and semantic\nencoding. The approach is evaluated for place recognition on a public benchmark\ncomparing its retrieval capabilities against state-of-the-art methods, in terms\nof quality and speed of returned point clouds.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}