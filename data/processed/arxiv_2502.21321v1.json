{
  "id": "arxiv_2502.21321v1",
  "text": "1\nLLM Post-Training: A Deep Dive into Reasoning\nLarge Language Models\nKomal Kumar∗, Tajamul Ashraf∗, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal,\nMubarak Shah, Ming-Hsuan Yang, Phillip H.S. Torr, Fahad Shahbaz Khan, Salman Khan\nAbstract—Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse\napplications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now\nincreasingly shifting focus toward post-training techniques to achieve further breakthroughs. While pretraining provides a broad\nlinguistic foundation, post-training methods enable LLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and\nalign more effectively with user intents and ethical considerations. Fine-tuning, reinforcement learning, and test-time scaling have\nemerged as critical strategies for optimizing LLMs performance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs\nbeyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. We\nhighlight emerging directions in model alignment, scalable adaptation, and inference-time reasoning, and outline future research\ndirections. We also provide a public repository to continually track developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.\nIndex Terms—Reasoning Models, Large Language Models, Reinforcement Learning, Reward Modeling, Test-time Scaling\n✦\n1\nIntroduction\nC\nontemporary Large Language Models (LLMs) exhibit\nremarkable capabilities across a vast spectrum of tasks,\nencompassing not only text generation [1, 2, 3] and question-\nanswering [4, 5, 6, 7], but also sophisticated multi-step rea-\nsoning [8, 9, 10, 11]. They power applications in natural\nlanguage understanding [12, 13, 14, 15, 16, 17], content gener-\nation [18, 19, 20, 21, 22, 23, 24, 25], automated reasoning [26,\n27, 28, 29], and multimodal interactions [30, 31, 32, 33]. By\nleveraging vast self-supervised training corpora, these models\noften approximate human-like cognition [34, 35, 36, 37, 38],\ndemonstrating impressive adaptability in real-world settings.\nDespite these impressive achievements, LLMs remain prone\nto critical shortcomings. They can generate misleading or\nfactually incorrect content (commonly referred to as “hal-\nlucinations”) and may struggle to maintain logical consis-\ntency throughout extended discourse [41, 42, 43, 44, 45, 46].\nMoreover, the concept of reasoning in LLMs remains a topic\nof debate. While these models can produce responses that\nappear logically coherent, their reasoning is fundamentally\ndistinct from human-like logical inference [47, 34, 48, 49].\nThis distinction is crucial, as it helps explain why LLMs can\n•\n∗Equal\ncontribution.\nCorresponding\nauthors\n(Email:\nko-\nmal.kumar@mbzuai.ac.ae, tajamul.ashraf@mbzuai.ac.ae)\n•\nKomal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muham-\nmad Anwer, Hisham Cholakkal, Salman Khan and Fahad Shahbaz\nKhan are with Mohamed bin Zayed University of Artificial Intel-\nligence, Abu Dhabi, UAE.\n•\nMubarak Shah is with the Center for Research in Computer Vision\nat the University of Central Florida, Orlando, FL 32816, USA.\n•\nMing-Hsuan Yang is with the University of California at Merced,\nMerced, CA 95343 USA, and also with Google DeepMind, Moun-\ntain View, CA 94043, USA.\n•\nPhilip H.S. Torr is with the Department of Engineering Science,\nUniversity of Oxford, Oxford OX1 2JD, UK.\nK\nLLM\nLLM\nPost \ntraining\nGPT-O1, O3\nTuning\nReinforce\nScale\nPolicy\nReward\nOffline Policy\nSearch\nConfidence\nReasoning\nFull Model\nParm. Efficient\nAdapters\nLow-Rank\nPrompt \nEnd-to-End\nDPO\nSelf-Critique\nTree-of-Thoughts\nBeam Search\nBest-of-N Search\nChain-of-Thought\nConfidence Sampling\nConsistency Decoding\nMonte Carlo Search\nRL Optimization\nGRPO\nPPO\nTRPO\nREINFORCE\nVanilla PG\nRLHF\nRLAIF\nBehavior Cloning\nOffline Batch\nLlaMA 3.2 \nXAONE 3.0\nDecoding\nTraining\nQwen\nDeepSeek-R1\nClaude 3.5 Sonnet\nMistral Large 2\nClaude2\nQwen-32B-Preview\nDeepSeek-R1\nLlaMA 3.3 \nGPT-3\nLlaMA 3.3 \nLlaMA 3.1 \nKnowledge\n          Distillation\nStarling-7B\nOREO\nSearch Against Verifiers\nDistilBERT\nALBERT\nMiniLM\nGPT-4, 4O, O1\nClaude3\nMistral Large 2\nGemini 1.5\nAlphaGo\nQwen-32B-Preview\nAlphaGo\nGemini 1.5\nLLM post-training alignment\nAlgorithmic categorization\nAlgorithms\nLLMs\n§ 3\n§ 4\n§ 5\nFig. 1: A taxonomy of post-training approaches for LLMs\n(LLMs), categorized into Fine-tuning, Reinforcement Learn-\ning, and Test-time Scaling methods. We summarize the key\ntechniques used in recent LLM models, such as GPT-4 [39],\nLLaMA 3.3 [13], and Deepseek R1 [40].\nproduce compelling outputs while still stumbling on relatively\nsimple logical tasks. Unlike symbolic reasoning that manipu-\nlates explicit rules and facts, LLMs operate in an implicit and\nprobabilistic manner [50, 42, 51]. For the scope of this work,\narXiv:2502.21321v1  [cs.CL]  28 Feb 2025\n\n\n2\n‘reasoning’ in LLMs refers to their ability to generate logically\ncoherent responses based on statistical patterns in data rather\nthan explicit logical inference or symbolic manipulation. Ad-\nditionally, models trained purely via next-token prediction\ncan fail to align with user expectations or ethical standards,\nespecially in ambiguous or malicious scenarios [4, 52]. These\nissues underscore the need for specialized strategies that ad-\ndress reliability, bias, and context sensitivity in LLM outputs.\nLLMs training can be broadly categorized into two stages:\npre-training, which generally relies on a next-token prediction\nobjective over large-scale corpora, and post-training, encom-\npassing multiple rounds of fine-tuning and alignment. Post-\ntraining mechanisms aim to mitigate LLMs limitations by\nrefining model behavior and aligning outputs with human\nintent, mitigating biases or inaccuracies [53].\nAdapting LLMs to domain-specific tasks often involves\ntechniques like fine-tuning [54, 55, 56], which enables task-\nspecific learning but risks overfitting and incurs high com-\nputational costs. To address these challenges, approaches\nsuch as Reinforcement Learning (RL) [57, 58, 59] enhance\nadaptability by leveraging dynamic feedback and optimizing\nsequential decision-making. Additionally, advances in scal-\ning techniques, including Low-Rank Adaptation (LoRA) [60],\nadapters, and Retrieval-Augmented Generation (RAG) [61,\n62, 63], improve both computational efficiency and factual\naccuracy. These strategies, coupled with distributed train-\ning frameworks, facilitate large-scale deployment and further\nboost the usability of LLMs across diverse applications (Fig-\nure 1). Through these targeted post-training interventions,\nLLMs become better aligned with human intent and ethical\nrequirements, ultimately enhancing their real-world applica-\nbility. Below, we summarize key post-training stages.\na) Fine-Tuning in LLMs: Fine-tuning adapts pre-trained\nLLMs to specific tasks or domains by updating parameters on\ncurated datasets [64, 65, 66, 54, 55, 67, 56]. While LLMs gen-\neralize well after large-scale pretraining, fine-tuning enhances\nperformance in tasks like sentiment analysis [68, 69], question\nanswering, and domain-specific applications such as medical\ndiagnosis [70, 71, 72]. This process, typically supervised,\naligns models with task requirements but poses challenges like\noverfitting, high computational costs, and sensitivity to data\nbiases [56, 31, 16]. To this end, parameter-efficient techniques\nlike LoRA [60] and adapters learn task-specific adaptation by\nupdating explicit parameters, significantly reducing compu-\ntational overhead. As models specialize, they may struggle\nwith out-of-domain generalization, underscoring the trade-off\nbetween specificity and versatility.\nFine-tuning tailors LLMs for specific tasks,\nimproving performance but risking overfitting,\nhigh compute costs, and reduced generalization.\nb) Reinforcement Learning in LLMs: In conventional RL,\nan agent interacts with a structured environment, taking\ndiscrete actions to transition between states while maximiz-\ning cumulative rewards [73]. RL domains—such as robotics,\nboard games, and control systems—feature well-defined state-\naction spaces and clear objectives [74, 75]. RL in LLMs differs\nsignificantly. Instead of a finite action set, LLMs select tokens\nfrom a vast vocabulary, and their evolving state comprises an\never-growing text sequence [16, 59, 76, 57]. This complicates\nplanning and credit assignment, as the impact of token se-\nlection may only emerge later. Feedback in language-based\nRL is also sparse [77], subjective, and delayed, relying on\nheuristic evaluations and user preferences rather than clear\nperformance metrics [78, 79, 58]. Additionally, LLMs must\nbalance multiple, sometimes conflicting, objectives, unlike\nconventional RL, which typically optimizes for a single goal.\nHybrid approaches combining process-based rewards (e.g.,\nchain-of-thought reasoning) with outcome-based evaluations\n(e.g., response quality) help refine learning [8, 80, 81]. Thus,\nRL for LLMs requires specialized optimization techniques to\nhandle high-dimensional outputs, non-stationary objectives,\nand complex reward structures, ensuring responses remain\ncontextually relevant and aligned with user expectations.\nReinforcement in LLMs extends beyond con-\nventional RL as it navigates vast action spaces,\nhandles subjective and delayed rewards, and bal-\nances multiple objectives, necessitating special-\nized optimization techniques.\nc) Scaling in LLMs: Scaling is crucial for enhancing the\nperformance and efficiency of LLMs. It helps improve general-\nization across tasks but introduces significant computational\nchallenges [82, 83]. Balancing performance and resource ef-\nficiency requires targeted strategies at inference. Techniques\nlike CoT [8] reasoning and Tree-of-Thought (ToT) [84] frame-\nworks enhance multi-step reasoning by breaking down com-\nplex problems into sequential or tree-structured steps. Addi-\ntionally, search-based techniques[85, 86, 87, 88] enable itera-\ntive exploration of possible outputs, helping refine responses\nand ensure higher factual accuracy. These approaches, com-\nbined with methods like LoRA [60], adapters, and RAG [61,\n62, 89], optimize the model’s ability to handle complex,\ndomain-specific tasks at scale. RAG enhances factual accu-\nracy by dynamically retrieving external knowledge, mitigating\nlimitations of static training data [62, 24, 90]. Distributed\ntraining frameworks leverage parallel processing to manage\nthe high computational demands of large-scale models. Test-\ntime scaling optimizes inference by adjusting parameters dy-\nnamically based on task complexity [83, 91]. Modifying depth,\nwidth, or active layers balances computational efficiency and\noutput quality, making it valuable in resource-limited or\nvariable conditions. Despite advancements, scaling presents\nchallenges such as diminishing returns, longer inference times,\nand environmental impact, especially when search techniques\nare performed at test time rather than during training [82].\nEnsuring accessibility and feasibility is essential to maintain\nhigh-quality, efficient LLM deployment.\nTest-time scaling enhances the adaptability\nof LLMs by dynamically adjusting computational\nresources during inference.\n1.1\nPrior Surveys\nRecent surveys on RL and LLMs provide valuable insights\nbut often focus on specific aspects, leaving key post-training\n\n\n3\ncomponents underexplored [51, 92, 93, 94]. Many works ex-\namine RL techniques like Reinforcement Learning from Hu-\nman Feedback (RLHF) [58], Reinforcement Learning from AI\nFeedback (RLAIF) [95], and Direct Preference Optimization\n(DPO) [57], yet they overlook fine-tuning, scaling, and critical\nbenchmarks essential for real-world applications. Further-\nmore, these studies has not explored the potential of RL even\nwithout human annotation supervised finetuning in various\nframeworks such as DeepSeek R1 with GRPO [59]. Other sur-\nveys explore LLMs in traditional RL tasks, such as multi-task\nlearning and decision-making, but they primarily classify LLM\nfunctionalities rather than addressing test-time scaling and\nintegrated post-training strategies [96, 97]. Similarly, studies\non LLM reasoning [98, 99, 100, 55, 101, 102, 103, 104] discuss\nlearning-to-reason techniques but lack structured guidance\non combining fine-tuning, RL, and scaling. The absence of\ntutorials, along with reviews of software libraries and imple-\nmentation tools, further limits their practicality. In contrast,\nthis survey offers a comprehensive view of LLM post-training as\nshown in Figure 1 by systematically covering fine-tuning, RL,\nand scaling as interconnected optimization strategies. We offer\npractical resources—benchmarks, datasets, and tutorials—to\naid LLM refinement for real-world applications.\n1.2\nContributions\nThe key contributions of this survey are as follows:\n• We provide a comprehensive and systematic review of\npost-training methodologies for LLMs, covering fine-\ntuning, RL, and scaling as integral components of model\noptimization.\n• We offer a structured taxonomy of post-training tech-\nniques, clarifying their roles and interconnections, and\npresent insights into open challenges and future research\ndirections in optimizing LLMs for real-world deployment.\n• Our survey provides practical guidance by introducing\nkey benchmarks, datasets, and evaluation metrics\nessential for assessing post-training effectiveness, ensur-\ning a structured framework for real-world applications.\n2\nBackground\nThe LLMs have transformed reasoning by learning to predict\nthe next token in a sequence based on vast amounts of text\ndata [105, 4] using Maximum Likelihood Estimation (MLE)\n[106, 3, 107], which maximizes the probability of generating\nthe correct sequence given an input. This is achieved by\nminimizing the negative log-likelihood:\nLMLE =\nT\nX\nt=1\nlog Pθ(yt | y<t, X).\nHere, X represents the input, such as a prompt or context.\nY = (y1, y2, ..., yT ) is the corresponding target sequence, and\nPθ(yt | y<t, X) denotes the model’s predicted probability for\ntoken yt, given preceding tokens.\nToken-wise training can ensure fluency but\nmay cause cascading errors due to uncorrected\nmistakes in inference.\nAs these models scale, they exhibit emergent reasoning\nabilities, particularly when trained on diverse data that in-\nclude code and mathematical content [108, 8]. However, de-\nspite their impressive capabilities, LLMs struggle to maintain\ncoherence and contextual relevance over long sequences. Ad-\ndressing these limitations necessitates a structured approach\nto sequence generation, which naturally aligns with RL.\nSince LLMs generate text autoregressively—where each to-\nken prediction depends on previously generated tokens—this\nprocess can be modeled as a sequential decision-making prob-\nlem within a Markov Decision Process (MDP) [109]. In this set-\nting, the state st represents the sequence of tokens generated\nso far, the action at is the next token, and a reward R(st, at)\nevaluates the quality of the output. An LLM ’s policy πθ is\noptimized to maximize the expected return:\nJ(πθ) = E\nh ∞\nX\nt=0\nγtR(st, at)\ni\n,\nwhere γ is the discount factor that determines how strongly\nfuture rewards influence current decisions. A higher γ places\ngreater importance on long-term rewards. The primary ob-\njective in RL is to learn a policy that maximizes the ex-\npected cumulative reward, often referred to as the return.\nThis requires balancing exploration—trying new actions to\ndiscover their effects—and exploitation—leveraging known\nactions that yield high rewards. While LLMs optimize a like-\nlihood function using static data, RL instead optimizes the\nexpected return through dynamic interactions. To ensure that\nLLMs generate responses that are not only statistically likely\nbut also aligned with human preferences, it is essential to go\nbeyond static optimization methods. While likelihood-based\ntraining captures patterns from vast corpora, it lacks the\nadaptability needed for refining decision-making in interactive\nsettings. By leveraging structured approaches to maximizing\nlong-term objectives, models can dynamically adjust their\nstrategies, balancing exploration and exploitation to improve\nreasoning, coherence, and alignment [110, 111, 49, 48].\nLLMs exhibit emergent abilities due to scale,\nwhile RL refines and aligns them for better rea-\nsoning and interaction.\n2.1\nRL based Sequential Reasoning.\nThe chain-of-thought reasoning observed in modern LLMs is\nnaturally framed as an RL problem. In this perspective, each\nintermediate reasoning step is treated as an action contribut-\ning to a final answer. The policy gradient update is given by:\n∇θJ(πθ) = Eτ\n\" T\nX\nt=1\n∇θ log πθ(xt | x1:t−1) A(st, at)\n#\n,\nwhere the advantage function A(st, at) distributes credit to\nindividual steps, ensuring that the overall reasoning process\nis refined through both immediate and delayed rewards.\nSuch formulations, including step-wise reward decomposition\n[112, 113], have been crucial for enhancing the interpretability\nand performance of LLMs on complex reasoning tasks. In\ntraditional RL formulations, an agent has:\nValue function: V (s) = E\u0002\nfuture return | s\u0003\n,\n\n\n4\nAction-value (Q-) function: Q(s, a) = E\u0002\nfuture return | s, a\u0003\n,\nAdvantage function: A(s, a) = Q(s, a) −V (s).\nIn words, A(s, a) measures how much better or worse it is to\ntake a specific action a in state s compared to what the agent\nwould normally expect (its baseline V (s)).\n2.2\nEarly RL Methods for Language Modeling.\nHere, we briefly overview pioneering methods that laid the\ngroundwork for applying RL to language generation tasks.\nThese initial efforts train a decision-making model (called a\n“policy”) by directly adjusting its parameters to maximize re-\nwards. Some policy gradient approaches are explained below:\nPolicy\nGradient\n(REINFORCE).\nThe\nREINFORCE\nalgo-\nrithm [114, 115] is a method used to improve decision-making\nby adjusting the model’s strategy (policy) based on rewards\nreceived from its actions. Instead of directly learning the best\naction for every situation, the algorithm refines how likely dif-\nferent actions are to be chosen, gradually improving outcomes\nover time. At each step, the model updates its parameters (θ)\nbased on how well its past decisions performed:\nθ ←θ + α\n\u0010\nG −b\n\u0011\nT\nX\nt=1\n∇θ log πθ(at | st).\nHere: G represents the total reward the model accumulates\nover an episode, b is a baseline value that helps reduce\nvariance, making learning more stable, ∇θ log πθ(at | st)\nmeasures how much a small change in θ affects the probability\nof choosing action at given state st, α is the learning rate,\ncontrolling how much the policy updates at each step.\nOptimizing actions based on long-term re-\nwards, rather than immediate outcomes, remains\nfundamental in recent LLMs, enabling explo-\nration of multiple reasoning paths.\nCurriculum Learning with MIXER.. Ranzato et al. [116]\nintroduces a gradual transition from maximum likelihood esti-\nmation (MLE) to RL. The overall loss is a weighted combination:\nL = λ(t) LMLE + \u00001 −λ(t)\u0001\nLRL,\nwhere λ(t) decreases with training time. This curriculum\nhelps the model ease into the RL objective and mitigate the\nmismatch between training and inference.\nSelf-Critical Sequence Training (SCST). SCST [117] re-\nfines the policy gradient method by comparing the model’s\nsampled outputs against its own best (greedy) predictions.\nInstead of using an arbitrary baseline, SCST uses the model’s\nown highest-scoring output, ensuring that updates directly\nimprove performance relative to what the model currently\nconsiders its best response. The gradient update follows:\n∇θJ(πθ) ≈\n\u0010\nr(ys) −r(ˆy)\n\u0011\n∇θ log πθ(ys),\nwhere ys is a sampled sequence, ˆy is the greedy output, and\nr(y) represents an evaluation metric such as BLEU [118] for\ntranslation or CIDEr [119] for image captioning. Since the\nlearning signal is based on the difference r(ys) −r(ˆy), the\nmodel is explicitly trained to generate outputs that score\nhigher than its own baseline under the evaluation metric.\nIf the sampled output outperforms the greedy output, the\nmodel reinforces it; otherwise, it discourages that sequence.\nThis direct feedback loop ensures that training aligns with\nthe desired evaluation criteria rather than just maximizing\nlikelihood. By leveraging the model’s own best predictions\nas a baseline, SCST effectively reduces variance and stabilizes\ntraining while optimizing real-world performance metrics.\nMinimum Risk Training (MRT). MRT [151] directly mini-\nmizes the expected risk over the output distribution. Given a\ntask-specific loss ∆(y, y∗) comparing the generated output y\nwith the reference y∗, the MRT objective is defined as:\nLMRT(θ) =\nX\ny∈Y\npθ(y | x) ∆(y, y∗).\nThis formulation incorporates evaluation metrics (e.g., 1 −\nBLEU) directly into training, enabling fine-grained adjust-\nments of the policy.\nAdvantage Actor-Critic (A2C/A3C). RL methods like\nREINFORCE [114] rely solely on policy gradients, which suf-\nfer from high variance, leading to unstable and inefficient\nlearning. Since the reward signal fluctuates across different\ntrajectories, updates may be noisy, causing slow or erratic\nconvergence. To mitigate this, Actor-Critic methods [152, 153,\n154, 155] combine two components: an actor and a critic. The\nactor is a policy πθ(at | st) that selects actions at at state\nst, while the critic is a value function Vϕ(st) that evaluates\nthe expected return of a state. The critic provides a more\nstable learning signal, reducing variance in policy updates\nand enabling efficient learning in continuous action spaces.\nActor updates are guided by the policy gradient theorem,\nwhere the advantage function A(st, at) defined in Sec. 2.1,\ndetermines how much better an action at is compared to the\nexpected value of state st. The policy with the learning rate α\nis updated as:\nθ ←θ + α A(st, at) ∇θ log πθ(at | st).\nMeanwhile, the critic is updated using temporal difference\nlearning, minimizing the squared error between its estimate\nand the actual return:\nϕ ←ϕ −β ∇ϕ\n\u0010\nVϕ(st) −Gt\n\u00112\n.\nwhere β is a learning rate for critic. To enhance stability\nand efficiency, several improvements have been proposed.\nEligibility traces allow learning from recent states, enabling\nfaster convergence. Function approximation with neural net-\nworks ensures effective handling of high-dimensional inputs.\nAdvanced variants such as Natural Gradient methods [156]\nadjust updates using the Fisher Information Matrix, improv-\ning convergence speed.\nA notable early example is Barto’s Actor-Critic model\n[157], where the critic uses a linear function Vϕ(st) and\nthe actor follows a linear policy. Modern methods like A2C\n(Advantage Actor-Critic) [154] and A3C (Asynchronous Ad-\nvantage Actor-Critic) [155] extend this approach by paralleliz-\ning training across multiple environments, leading to faster\nand more stable learning. By leveraging the critic’s value\nestimation, actor-critic methods stabilize learning, improve\nsample efficiency, and accelerate convergence, making them\nmore effective for complex decision-making tasks.\n\n\n5\nRL Enhanced LLMs\nDeveloper\nSource\n# Params\nRL Methods\nFine-Tuning\nArchitecture Type\nModel\nTTS\nDeepSeek-V2 [16]\nDeepseek\nLink\n236B-A21B\nGRPO\nDPO + GRPO\nMoE\nOpen\n✓\nGPT 4.5 [120]\nOpenAI\nLink\n-\nRLHF, PPO, RBRM\nSFT + RLHF\nMoE\nClosed\n✓\nGemini [15]\nGoogle\nLink\n-\nRLHF\nSFT + RLHF\nSingle Model\nClosed\n✗\nClaude 3.7 [121]\nAnthropic\nLink\n-\nRLAIF\nSFT + RLAIF\nSingle Model\nClosed\n✗\nReka [122]\nReka\nLink\n7B, 21B\nRLHF, PPO\nSFT + RLHF\nSingle Model\nClosed\n✗\nDeepSeekR1 [40]\nDeepseek\nLink\n240B-A22B\nGRPO\nDPO + GRPO\nMoE\nOpen\n✓\nNemotron-4 340B [123]\nNVIDIA\nLink\n340B\nDPO, RPO\nDPO + RPO\nSingle Model\nClosed\n✗\nFalcon [124]\nTII\nLink\n40B\n-\nSFT\nSingle Model\nOpen\n✗\nGPT-4 [39]\nOpenAI\nLink\n-\nRLHF, PPO, RBRM\nSFT + RLHF\nMoE\nClosed\n✓\nLlama 3 [13]\nMeta\nLink\n8B, 70B, 405B\nDPO\nSFT + DPO\nSingle Model\nOpen\n✗\nQwen2 [125]\nAlibaba\nLink\n(0.5-72)B, 57B-A14B\nDPO\nSFT + DPO\nSingle Model\nOpen\n✓\nGemma2 [14]\nGoogle\nLink\n2B, 9B, 27B\nRLHF\nSFT + RLHF\nSingle Model\nOpen\n✗\nStarling-7B [26]\nBerkeley\nLink\n7B\nRLAIF, PPO\nSFT + RLAIF\nSingle Model\nOpen\n✗\nMoshi [126]\nKyutai\nLink\n7B\n-\n-\nMulti-modal\nOpen\n✓\nAthene-70B [127]\nNexusflow\nLink\n70B\nRLHF\nSFT + RLHF\nSingle Model\nOpen\n✗\nGPT-3.5 [39]\nOpenAI\nLink\n3.5B, 175B\nRLHF, PPO\nSFT + RLHF\nMoE\nClosed\n✓\nHermes 3 [128]\nNous\nLink\n8B, 70B, 405B\nDPO\nSFT + DPO\nSingle Model\nOpen\n✗\nZed [129]\nZed AI\nLink\n500B\nRLHF\nRLHF\nMulti-modal\nOpen\n✓\nPaLM 2 [130]\nGoogle\nLink\n-\nRLHF\n-\nSingle Model\nClosed\n✓\nInternLM2 [131]\nSAIL\nLink\n1.8B, 7B, 20B\nRLHF, PPO\nSFT + RLHF\nSingle Model\nClosed\n✗\nSupernova [132]\nNova AI\nLink\n220B\nRLHF\nRLHF\nMulti-modal\nOpen\n✓\nGrok3 [133]\nGrok-3\nLink\n175B\n-\nDPO\nDense\nOpen\n✓\nPixtral [134]\nMistral AI\nLink\n12B, 123B\n-\nPEFT\nMultimodal\nOpen\n✓\nMinimaxtext [135]\nMiniMax\nLink\n456B\n-\nSFT\nSingle Model\nClosed\n✗\nAmazonnova [136]\nAmazon\nLink\n-\nDPO, RLHF, RLAIF\nSFT\nSingle Model\nClosed\n✗\nFugakullm [137]\nFujitsu\nLink\n13B\n-\n-\nSingle Model\nClosed\n✗\nNova [138]\nRubik’s AI\nLink\n-\n-\nSFT\nProprietary\nClosed\n✗\n03 [139]\nOpenAI\nLink\n-\nRL through CoT\nRL through CoT\nSingle Model\nClosed\n✓\nDbrx [140]\nDatabricks\nLink\n136B\n-\nSFT\nSingle Model\nOpen\n✗\nInstruct-GPT [58]\nOpenAI\nLink\n1.3B, 6B, 175B\nRLHF, PPO\nSFT + RLHF\nSingle Model\nClosed\n✗\nOpenassistant [141]\nLAION\nLink\n17B\n-\nSFT\nSingle Model\nOpen\n✗\nChatGLM [142]\nZhipu AI\nLink\n6B, 9B\nChatGLM-RLHF\nSFT + RLHF\nSingle Model\nOpen\n✗\nZephyr [143]\nArgilla\nLink\n141B-A39B\nORPO\nDPO + ORPO\nMoE\nOpen\n✓\nphi-3 [17]\nMicrosoft\nLink\n3.8B, 7B, 14B\nDPO\nSFT + DPO\nSingle Model\nClosed\n✗\nJurassic [144]\nAI21 Labs\nLink\n-\n-\nSFT\nProprietary\nClosed\n✗\nKimi K1.5 [145]\nMoonshot AI\nLink\n150B\n-\nRLHF\nMulti-modal\nOpen\n✓\nPhi-4 [146]\nMicrosoft\nLink\n28B, 70B, 140B\nDPO\nSFT + DPO\nSingle Model\nClosed\n✗\nChameleon [147]\nMeta AI\nLink\n34B\n-\nSFT\nSingle Model\nOpen\n✗\nCerebrasgpt [148]\nCerebras\nLink\n13B\n-\nSFT\nSingle Model\nOpen\n✗\nBloomberggpt [149]\nBloomberg L.P.\nLink\n50B\n-\nSFT\nSingle Model\nClosed\n✗\nChinchilla [150]\nDeepMind\nLink\n70B\nRLHF, PPO\nSFT\nSingle Model\nClosed\n✗\nTABLE 1: An overview of reinforcement learning enhanced LLMs, where the notation ‘141B-A39B’ represents a Mixture of\nExperts (MoE) architecture with a total of 141 billion parameters, out of which 39 billion are actively used during inference.\nConnection\nwith\nModern\nMethods. The aforemen-\ntioned early RL methods—REINFORCE [114], MIXER [116],\nSeqGAN [158], SCST [117], MRT [151], and actor-critic algorithms\nestablished the mathematical foundations for sequential rea-\nsoning in LLMs. These methods provided initial solutions to\nchallenges such as exposure bias and high variance. Mod-\nern techniques such as large-scale RL from Human Feedback\n(RLHF) using PPO [73] and advanced reward models, e.g.,\nGroup Relative Policy Optimization (GRPO) [159] build di-\nrectly upon these ideas. By integrating sophisticated reward\nsignals and leveraging efficient policy updates, contemporary\nLLMs achieve improved reasoning, safety, and alignment with\nhuman values and pave the way for robust multi-step reason-\ning and improved quality of generated text. Table 1 provides\nan overview of recent models, including their parameters,\narchitecture types, and the distilled RL methods employed,\nalong with links for easy access.\n3\nReinforced LLMs\nFrom a methodological perspective, the integration of RL into\nLLM reasoning typically follows three core steps:\n1) Supervised Fine-Tuning (SFT): Commences with a\npretrained language model that is subsequently refined\non a supervised dataset of high-quality, human-crafted\nexamples. This phase ensures the model acquires a base-\nline compliance with format and style guidelines.\n2) Reward Model (RM) Training: Generated outputs\nfrom the fine-tuned model are collected and subjected\nto human preference labeling. The reward model is then\ntrained to replicate these label-based scores or rankings,\neffectively learning a continuous reward function that\nmaps response text to a scalar value.\n3) RL Fine-Tuning: Finally, the main language model is\noptimized via a policy gradient algorithm most e.g PPO\nto maximize the reward model’s output. By iterating this\nloop, the LLM learns to produce responses that humans\nfind preferable along key dimensions such as accuracy,\nhelpfulness, and stylistic coherence.\n4) Reward Modeling and Alignment: Sophisticated\nreward functions are developed—drawing from human\npreferences, adversarial feedback, or automated met-\nrics—to guide the model toward outputs that are coher-\nent, safe, and contextually appropriate. These rewards\nare critical for effective credit assignment across multi-\nstep reasoning processes.\nEarly approaches to aligning LLMs with human preferences\nleveraged classical RL algorithms, such as PPO [73] and Trust\nRegion Policy Optimization (TRPO) [160], which optimize a\npolicy by maximizing the expected cumulative reward while\nenforcing constraints on policy updates via a surrogate ob-\njective function and KL-divergence regularization [161]. Im-\nproved alternatives to these methods for scalable preference-\nbased optimization have emerged, such as Direct Preference\nOptimization (DPO) [57, 162] and Group Relative Policy\nOptimization (GRPO) [159, 59, 16], which reformulate the\nalignment objective as a ranking-based contrastive loss func-\ntion [163] over human-labeled preference data. Unlike PPO\nand TRPO [160], which rely on explicit reward models and\n\n\n6\ncritic networks, DPO and GRPO directly optimize the policy\nby leveraging log-likelihood ratios and group-wise reward\ncomparisons, respectively, eliminating the need for explicit\nvalue function approximation while preserving preference-\nconsistent learning dynamics. This transition from classical\nRL-based alignment to preference-based direct optimization\nintroduces novel formulations such as contrastive ranking loss,\npolicy likelihood ratio regularization, and grouped advantage\nestimation, which are explained in subsequent sections.\n3.1\nReward modeling\nLet X be the space of possible queries (e.g., user prompts). For\neach query x ∈X, we collect one or more candidate responses\n{yj}mx\nj=1 where mx is the number of candidate responses for\nquery x. Typically, these responses are generated by a lan-\nguage model or policy under different sampling or prompting\nconditions. Human annotators provide preference judgments\nfor these responses. These can take various forms:\n• Pairwise preference: For two responses yj and yk to\nthe same query x, an annotator indicates whether yj is\npreferred to yk.\n• Rankings: A partial or total ordering of the candidate\nresponses, e.g. yj1 ≻yj2 ≻· · · ≻yjmx .\nWe denote such human preference data by {rj} for each\nresponse or pair, where rj might be a label, a rank, or an\nindex indicating preference level. The overall dataset D then\nconsists of N annotated examples:\nD =\nn\n(xi, {yi\nj}mi\nj=1, {preferencesi})\noN\ni=1.\nIn practice, a large number of queries x are sampled from\nreal or simulated user requests. Candidate responses {yj}mx\nj=1\nare generated by either sampling from a base language model\nor using beam search or other decoding strategies. Human\nannotators then provide pairwise or ranking feedback on\nwhich responses are better (or worse) according to predefined\ncriteria (e.g., quality, correctness, helpfulness, etc). We train a\nparametric model Rθ(x, y), referred to as the reward model, to\nmap each (query, response) pair (x, y) to a scalar score. The\ngoal is for Rθ to reflect the alignment or preference level, such\nthat:\nRθ : X × Y →R.\nHere Y is the space of all possible responses.\nTo train Rθ, we use the human preference labels in D to\ndefine a suitable ranking-based loss, as explained below.\nI. Bradley–Terry Model (Pairwise). For pairwise pref-\nerences, Bradley-Terry model [164] is often used. Suppose the\ndataset indicates that, for a given query x, human annotators\nprefer yj to yk, we denote it as yj ≻yk. Under Bradley–Terry,\nthe probability of yj being preferred over yk is given by:\nP\u0000yj ≻yk | x; θ\u0001\n=\nexp\u0000Rθ(x, yj)\u0001\nexp\u0000Rθ(x, yj)\u0001\n+ exp\u0000Rθ(x, yk)\u0001.\nWe train Rθ by maximizing the likelihood of observed prefer-\nences (or equivalently minimizing the negative log-likelihood):\nLBT(θ) = −\nX\n(x, yj≻yk) ∈D\nlog P\u0000yj ≻yk | x; θ\u0001\n.\nII. Plackett–Luce Model1 (Rankings). When full or\npartial rankings of m responses are available, i.e.,\nyj1 ≻yj2 ≻· · · ≻yjm,\nthe Plackett–Luce model [165] factorizes the probability of\nthis ranking as:\nP\u0000yj1, . . . , yjm | x; θ\u0001\n=\nm\nY\nℓ=1\nexp\u0000Rθ(x, yjℓ)\u0001\nPm\nk=ℓexp\u0000Rθ(x, yjk)\u0001.\nIts negative log-likelihood is:\nLPL(θ) = −\nX\n(x, rank)∈D\nm\nX\nℓ=1\nlog\n \nexp\u0000Rθ(x, yjℓ)\u0001\nPm\nk=ℓexp\u0000Rθ(x, yjk)\u0001\n!\n.\nIn practice, one minimizes the sum (or average) of the chosen\nranking-based loss over all preference data:\nL(θ) =\n1\n|D|\nX\n(x, {yj}, prefs) ∈D\nLranking\n\u0010\nθ; x, {yj}, prefs\n\u0011\n,\nwhere Lranking could be either LBT or LPL. While the reward\nmodel Rθ(x, y) provides a scalar reward signal reflecting\nhuman preferences, this connects to common RL concepts,\nespecially the advantage function.\nReward modeling uses ranking-based losses\nto learn a function from human preferences for\npolicy optimization.\nReward modeling Types. Rewards can be categorized into\nexplicit and implicit approaches.\n3.1.1\nExplicit Reward Modeling\nExplicit reward modeling defines reward functions directly\nbased on predefined rules, heuristics, or human annotations.\nThis reward structure involves direct, numeric signals from\nhumans or from specialized AI modules trained to approx-\nimate human judgments (e.g., ranking or pairwise compari-\nson). This method can produce precise reward estimates but\nmay be time-consuming or costly at scale. Illustrative use\ncases include ‘red-teaming’ exercises where experts rate the\nseverity of toxic outputs, or domain-specialist tasks in which\ncorrectness must be validated by a subject matter expert.\n3.1.2\nImplicit Reward Modeling\nImplicit reward modeling infers rewards indirectly from ob-\nserved behaviors, interactions, or preference signals, often\nleveraging machine learning techniques to uncover latent re-\nward structures. It derives its signals from user interaction\nmetrics such as upvotes, acceptance rates, click-through pat-\nterns, or session engagement times. While it can accumulate\nvast datasets with minimal overhead, this approach risks\nfostering behaviors that exploit engagement heuristics at the\nexpense of content quality or veracity.\nReward Function. Defining a reward function for text gen-\neration tasks is an ill-posed problem [166, 167]. The existing\nRL methods in LLMs either focus on the generation process\n(Process Reward Modeling) or the outcome (Outcome Reward\nModeling), to shape LLM behaviors. We explain these two\nreward modeling paradigms below.\n1. https://hturner.github.io/PlackettLuce/\n\n\n7\nLLM Post\ntraining\nHuman\nannotation\nPolicy Long CoT\nexamples+SFT\nRelative PO\n Rejection\nSampling & SFT\nRL helpfulness\nalignment PO\nReward\nΔℒ(x, y⁺, y⁻)\nDirect\nOptimization\nPreference\npairs: (x, y⁺, y⁻)\nReference\npolicy SFT\nREINFORCE PO\nSFT\nExpert policy\ndemonstrations\nAdversarial\nReward Signal\nSFT\nReward Model\nTraining\nProximal\nPO\nRegularization\nKL-Divergence\nMultiple paths \nusing policy\nAdvantage\nEstimation\nPPO+KL\nRegularization \nValue Function\nTraining\nV-guided loss\nPO+TTS\nOffline\ntrajectories\nTerminal\nrewards {0,1}\nReasoning and\nActing\nCoT Prompting\nSelf-feedback\nEpisodic\nMemory Agent\nTree of\nThoughts\nSelf-\nconsistency\nKL constraint\nRegularization \nInference time reasoning\nRLHF\nDPO\nRLAIF\nTRPO\nOREO\nGRPO\n§3.2.8\n§3.2.7\n§3.2.5\n§3.2.4\n§3.2.6\n§3.2.3\nFig. 2: Overview of Large Language Models (LLMs) reasoning methods, showcasing pathways for enhancing reasoning\ncapabilities through approaches like Chain-of-Thought (CoT) prompting, self-feedback, and episodic memory. The diagram\nhighlights multiple reinforcement learning-based optimization techniques, including GRPO, RLHF, DPO, and RLAIF, for fine-\ntuning reasoning models with reward mechanisms and preference-based learning.\n3.1.3\nOutcome Reward Modeling\nMeasures the end result (e.g., whether the final answer is\nfactually correct or solves the user’s query). This model\nis straightforward to implement but may offer limited in-\nsight into how the conclusion was reached. It is prevalent\nin short-response tasks, where the user’s primary concern is\nthe correctness or succinctness of the final statement. For\nlong-response tasks, outcome based reward can lead to credit\nassignment problem, i.e., which specific actions or states lead\nto a particular reward outcome.\n3.1.4\nProcess Reward Modeling\nAssigns feedback at intermediate reasoning steps, incentiviz-\ning coherent, logically consistent, and well-structured chains\nof thought. This approach is particularly valuable for tasks\ninvolving mathematical derivations, legal arguments, or code\ndebugging, in which the path to the answer is as significant\nas the final statement. In such problems, the reward assigned\nat individual steps encourages transparency and robust step-\nby-step reasoning. However, it requires a more complex anno-\ntation process, e.g., requires “gold” reasoning steps or partial\ncredit scoring. Process rewards can be combined with outcome\nrewards for a strong multi-phase training signal.\nPRM with last-step aggregation beats ORM.\n3.1.5\nIterative RL with Adaptive Reward Models\nAdaptive Reward Models is a training methodology designed\nto continuously improve the performance of LLMs by iter-\natively refining the reward models and the policy model.\nThis approach addresses the challenges of reward hacking and\nreward model drift, which can occur when the reward model\nbecomes misaligned with the desired objectives during large-\nscale RL training. The RL process is divided into multiple\niterations, where the model is trained in cycles. After each\niteration, the reward model is updated based on the latest\nmodel behavior and human feedback. The reward model is\nnot static but evolves over time to better align with human\npreferences and task requirements. This adaptation ensures\nthat the reward signals remain accurate and relevant as the\nmodel improves. Repeat the iterative process until the model’s\nperformance plateaus or meets the desired benchmarks. The\nreward model and policy model co-evolve, with each iteration\nbringing them closer to optimal alignment.\n3.2\nPolicy Optimization\nOnce we have a trained reward model Rθ(x, y) that captures\nhuman preferences, we can integrate it into a RL framework to\noptimize a policy πϕ. In essence, we replace (or augment) the\nenvironment’s native reward signal with Rθ(x, y) so that the\nagent focuses on producing responses y that humans prefer for\na given query x.\nIn typical RL notation:\n• Each state s here can be interpreted as the partial dia-\nlogue or partial generation process for the next token (in\nlanguage modeling).\n• Each action a is the next token (or next chunk of text) to\nbe generated.\n• The policy πϕ(a | s) is a conditional distribution over the\nnext token, parameterized by ϕ.\nWe seek to find ϕ that maximizes the expected reward under\nRθ. Concretely, let x be a user query, and let y ∼πϕ(· | x) be\nthe generated response. We aim to solve:\nmax\nϕ\nEx∼X\nh\nEy∼πϕ(· | x)\n\u0002\nRθ(x, y)\u0003i\n.\n\n\n8\nThis means that on average, over user queries x and responses\ny drawn from the policy πϕ, we want the reward model’s score\nRθ(x, y) to be as high as possible.\nPolicy Gradient and Advantage. The modern algorithms\n(e.g., PPO [73], GRPO [59], TRPO [160]) rely on policy gradients.\nFigure 5 presents a structured comparison of the these main\nRL frameworks. Each framework builds upon different princi-\nples for policy learning, reference modeling, and reward com-\nputation. Recall that the advantage function A(s, a) quantifies\nhow much better an action a is than the baseline expected\nreturn V (s). At a high level, we update the policy πϕ in the\ndirection that increases πϕ(a | s) for actions a with positive\nadvantage and decreases it for negative-advantage actions.\nFormally, the advantage At at time t can be written as:\nAt = Q(st, at) −V (st),\nwhere Q(st, at) is the expected future return (sum of future\nrewards, including Rθ) starting from st when taking action at.\nWhen using the reward model Rθ:\n1) We interpret Rθ(x, y) as the immediate or terminal re-\nward for the generated response y.\n2) The policy’s future returns thus factor in how likely\nsubsequent tokens are to be positively scored by Rθ.\n3) The advantage function still captures how much better\na particular generation step is compared to the baseline\nperformance V (st).\nThe reward model learns relative preferences\nrather than absolute scores. This avoids the need\nfor calibrated human ratings and focuses on pair-\nwise comparisons.\n3.2.1\nOdds Ratio Preference Optimization (ORPO)\nA simplest method ORPO [168] which directly optimizing a\npolicy from pairwise human preferences. Instead of first learn-\ning a separate reward model and then running standard RL,\nORPO updates the policy to increase the likelihood of preferred\nresponses (according to human labels) relative to dispreferred\nones. The key idea is to look at the odds ratio:\nπϕ(yj | x)\nπϕ(yk | x),\nwhere yj is the preferred response and yk is the less-preferred\nresponse for a given query x.\nPairwise Preference Probability. In many direct pref-\nerence approaches (e.g., Bradley–Terry style), one writes\nPϕ\n\u0000yj ≻yk | x\u0001\n= σ\n\u0010\nln πϕ(yj | x)\nπϕ(yk | x)\n\u0011\n=\n1\n1 + exp\n\u0010\nln πϕ(yk|x)\nπϕ(yj|x)\n\u0011,\nwhere σ(·) is the logistic (sigmoid) function. Intuitively, if the\npolicy πϕ assigns higher probability to yj than to yk, the odds\nπϕ(yj|x)\nπϕ(yk|x) exceed 1, making yj more likely to be the preferred\noutcome under the model.\nIn ORPO, one typically defines a negative log-likelihood loss\nfor all pairs {(x, yj ≻yk)} in the dataset:\nLORPO(ϕ) = −\nX\n(x, yj≻yk) ∈D\nlog\n\u0010\nPϕ\n\u0000yj ≻yk | x\u0001\u0011\n.\nSubstituting the logistic form gives:\nLORPO(ϕ) = −\nX\n(x, yj≻yk) ∈D\nlog\n\u0010\nπϕ(yj | x)\nπϕ(yj | x) + πϕ(yk | x)\n\u0011\n,\nwhich can also be interpreted as maximizing the log odds ratio\nfor the correct (preferred) label in each pairwise comparison.\nInterpretation via Odds Ratios. By treating each\npreference label (yj ≻yk) as a constraint on the odds πϕ(yj|x)\nπϕ(yk|x),\nORPO pushes the policy to increase its probability mass on yj\nwhile decreasing it on yk. When viewed in logarithmic space:\nln\n\u0010\nπϕ(yj|x)\nπϕ(yk|x)\n\u0011\n,\na higher value corresponds to a greater likelihood of selecting\nyj over yk. Hence, minimizing LORPO(ϕ) aligns πϕ with the\nhuman-labeled preferences.\n. ORPO is potentially less flexible for combining\nmultiple reward signals.\n3.2.2\nProximal Policy Optimization (PPO) in LLMs\nA popular method for policy optimization is PPO [73], a\nstrategy adapted to align LLMs with human feedback. Given\na policy πθ parameterized by θ and a reward function R,\nPPO updates the policy by optimizing a clipped objective\nthat balances exploration and stability. Specifically, if rt(θ) =\nπθ(at|st)\nπθref(at|st) denotes the probability ratio for an action at in\nstate st, the clipped PPO objective is:\nLPPO(θ) = Et\nh\nmin \u0000rt(θ) At, clip(rt(θ), 1 −ϵ, 1 + ϵ) At\n\u0001i\n,\nwhere At is an estimator of the advantage function and ϵ is a\nhyperparameter controlling the allowable deviation from the\nprevious policy. At is computed using Generalized Advantage\nEstimation (GAE) [169] based on rewards and a learned value\nfunction. The clipping objective of PPO restricts how dras-\ntically the updated policy distribution can diverge from the\noriginal policy. This moderation averts catastrophic shifts in\nlanguage generation and preserves training stability.\nPolicy Optimization with KL Penalty. During RL fine-\ntuning with PPO, the policy π is optimized to maximize reward\nwhile staying close to the base model ρ. The modified reward\nfunction includes a KL divergence penalty:\nJ(π) = E(x,y)∼D\n\u0002\nr(x, y) −β KL\u0000π(·|x) ∥ρ(·|x)\u0001\u0003\n,\nwhere β controls the penalty strength. The KL term KL(π ∥ρ)\nprevents over-optimization to the proxy reward r(x, y) (i.e.,\nreward hacking).\nThe KL penalty ensure policy retains the base\nmodel’s linguistic coherence and avoids degener-\nate outputs.\n3.2.3\nReinforcement Learning from Human Feedback (RLHF)\nRLHF [58] refines LLMs through direct human preference sig-\nnals, making them more aligned with human expectations.\nThe process involves three main steps. First, SFT is performed\non a pretrained model using high-quality labeled data to\nestablish strong linguistic and factual capabilities. Second, a\n\n\n9\nreward function R is trained using human-annotated rankings\nof generated responses, allowing it to predict preferences and\nprovide a scalar reward signal. Third, PPO is employed in the\nRLHF [58] pipeline by using human-provided preference scores\n(or rankings) to shape R and thereby guide the policy up-\ndates. This ensures that the model prioritizes outputs aligned\nwith human-preferred behavior. The robust performance un-\nder conditions of noisy or partial reward signals makes PPO\nwell-suited for text generation tasks, where large action spaces\nand nuanced reward definitions are common.\n3.2.4\nReinforcement Learning from AI Feedback (RLAIF)\nRLAIF [95] is an alternative to RLHF that replaces human\nannotation with AI-generated feedback. Instead of relying\non human-labeled preferences, RLAIF employs a secondary,\nhighly capable language model to generate preference labels,\nwhich are then used to train a reward model. This reward\nmodel guides reinforcement learning-based fine-tuning of the\ntarget model. RLAIF reduces the cost and time required for\ndata collection by eliminating the need for human annota-\ntors. It enables large-scale model alignment without requiring\nextensive human intervention while maintaining high perfor-\nmance and alignment. Empirical studies indicate that RLAIF\n[95, 170] is a scalable and efficient alternative to RLHF, making\nit a promising direction for reinforcement learning-driven\nlanguage model optimization.\nThe clipping mechanism constrains policy\nupdates to remain within a safe trust region,\nwhich is crucial when dealing with complex, high-\ndimensional action spaces.\n3.2.5\nTrust Region Policy Optimization (TRPO)\nTRPO [160] is another widely used policy optimization method,\npreceding PPO and sharing its fundamental goal: improving\nstability in reinforcement learning updates. TRPO optimizes\npolicy updates while ensuring they remain within a con-\nstrained trust region, measured by KL divergence.\nInstead of using a clipped objective like PPO, TRPO enforces\na hard constraint on policy updates by solving the following\noptimization problem:\nmax\nθ\nEt\n\u0014 πθ(at | st)\nπθold(at | st)At\n\u0015\nsubject to the constraint:\nEt [DKL (πθold(· | st)∥πθ(· | st))] ≤δ.\nwhere δ is a hyperparameter that controls how much the new\npolicy can diverge from the old one.\nUnlike PPO, which approximates this constraint using clip-\nping, TRPO directly solves a constrained optimization problem,\nensuring each update does not move too far in policy space.\nHowever, solving this constrained problem requires computa-\ntionally expensive second-order optimization techniques like\nconjugate gradient methods, making TRPO less efficient for\nlarge-scale models like LLMs. In practice, PPO is preferred\nover TRPO due to its simplicity, ease of implementation, and\ncomparable performance in large-scale applications like RLHF.\nHowever, TRPO remains an important theoretical foundation\nfor stable policy optimization in deep reinforcement learning.\nq\nPolicy\no\nReference\nReward\nValue\nr\nv\nGAE\nA\nq\nPolicy\nGroup\nComputation\nReward\nReference\nq\nPolicy\nr\nDPO\nobjective\nReference\nModel\nPreference\nData\nupdate\nback-prop\nPPO\nKL\nKL\nDPO\nGRPO\npreferences\nupdate\nFig. 3: Comparison of PPO [73], GRPO [59], and DPO [162].\nWe highlight policy models, reference models, rewards and\noptimization flows with corresponding loss functions.\n3.2.6\nDirect Preference Optimization (DPO)\nDPO [162] is a recently proposed method for training LLMs\nfrom human preference data without resorting to the tradi-\ntional RL loop (as in RLHF with PPO). Instead of learning a\nseparate reward function and then running policy-gradient\nupdates, DPO directly integrates human preference signals into\nthe model’s training objective. So instead of the above PPO\nobjective, DPO instead constructs an objective that directly\npushes up the probability of a chosen (preferred) response\n(y+) while pushing down the probability of a less-preferred\nresponse (y−), all within a single log-likelihood framework.\nRather than bounding policy changes with clip, the DPO loss\nuses the difference between log probabilities of ‘winning’ vs.\n‘losing’ responses. This explicitly encodes the user’s preference\nin the updated parameters.\nHere, πθ is the learnable policy, πref is a reference policy\n(often the SFT-trained model), σ(·) is the sigmoid function,\nβ is a scaling parameter, and Dtrain is a dataset of triplets\n(x, y+, y−) where y+ is the preferred output over y−.\nLDPO(θ) = E((x,y+),y−)∼Dtrain\nh\nσ\n\u0010\nβ log πθ(y+ | x)\nπref(y+ | x)\n−β log πθ(y−| x)\nπref(y−| x)\n\u0011i\n.\nThe key insight is that an LLM can be treated as a “hidden\nreward model”: we can reparameterize preference data so that\nthe model’s own log probabilities reflect how preferable one re-\nsponse is over another. By directly adjusting the log-likelihood\nof more-preferred responses relative to less-preferred ones,\nDPO sidesteps many complexities of RL-based methods (e.g.,\nadvantage functions or explicit clipping).\n\n\n10\nThe advantage function Aϕ = Vϕ(st+1) −Vϕ(st)\nquantifies\nper-step\ncontributions,\ncritical\nfor\nidentifying key reasoning errors. This granularity\nis lost in DPO, which treats entire trajectories\nuniformly.\nPerplexity Filtering for On-Distribution Data. To en-\nsure DPO training data is on-distribution (aligned with ρ),\nresponses are filtered using perplexity. The perplexity of a\nresponse y = (y1, y2, . . . , yT ) is defined as:\nPP(y) = exp\n \n−1\nT\nT\nX\ni=1\nlog Pρ(yi | y<i)\n!\n,\nwhere yi is the i-th token. Only responses with perplexity\nbelow a threshold (e.g., the 95th percentile of ρ-generated\nresponses) are retained.\nThe advantage function remains a core con-\ncept to determine which actions (token choices)\nare better than the baseline at each step.\n3.2.7\nOffline Reasoning Optimization (OREO)\nOREO [171] is an offline reinforcement learning method de-\nsigned to enhance LLMs’ multi-step reasoning by optimizing\nthe soft Bellman equation [109]. Unlike DPO, which relies\non paired preference data, OREO uses sparse rewards based\non final outcomes (e.g., correctness of reasoning chains) and\njointly trains a policy model πθ and a value function Vϕ for\nfine-grained credit assignment. The core objective minimizes\nthe inconsistency in the soft Bellman equation:\nVϕ(st) −Vϕ(st+1) = r(st, at) −β log πθ(at | st)\nπref(at | st),\nwhere st+1 = f(st, at) is the next state, r is the sparse reward,\nand β controls KL regularization. The policy and value losses\nare:\nLV (ϕ) = 1\nT\nT −1\nX\nt=0\n \nVϕ(st) −Rt + β\nX\ni≥t\nlog πθ(ai | si)\nπref(ai | si)\n!2\n,\nLπ(θ) = 1\nT\nT −1\nX\nt=0\n\u0012\nVϕ(st) −Rt + β log πθ(at | st)\nπref(at | st)\n\u00132\n+ αLreg,\nwhere Lreg penalizes deviations from πref, and α balances\nregularization.\nOREO’s explicit value function enables test-\ntime beam search (e.g., selecting high-value rea-\nsoning steps) and iterative training, where failed\ntrajectories refine the policy. This contrasts with\nDPO implicit value function, which lacks stepwise\ncredit assignment.\n. OREO’s computational cost scales with trajec-\ntory length and value-model training. While ef-\nfective for math/agent tasks, its generalization\nto broader domains (e.g., coding) requires vali-\ndation. Iterative training also demands careful\ndata curation to avoid overfitting to failure\nmodes.\n3.2.8\nGroup Relative Policy Optimization (GRPO)\nGRPO [59] simplifies the PPO framework by eliminating the\nneed for a separate value function. Instead, GRPO estimates\nthe baseline from the average reward of multiple sampled\noutputs for the same question. The primary contribution in\nGRPO is that it removes the need for a separate value model\n(critic model) and instead estimates the baseline reward from\na group of sampled LLM outputs. This significantly reduces\nmemory usage and stabilizes policy learning. The approach\nalso aligns well with how reward models are trained, i.e.,\nby comparing different LLM-generated outputs rather than\npredicting an absolute value.\nFor each question q, GRPO samples a group of outputs\n{o1, o2, . . . , oG} from the old policy πold\nθ . A reward model\nis used to score each output in the group, yielding rewards\n{r1, r2, . . . , rG}. The rewards are normalized by subtracting\nthe group average and dividing by the standard deviation:\n¯ri = ri −mean(r)\nstd(r)\n.\nThe advantage ˆAi,t for each token in the output is set as the\nnormalized reward ¯ri.\nGRPO first samples a question q ∼P(Q) and then samples\nG outputs {oi}G\ni=1 from πold\nθ (O | q). Define the per-output\nobjective as\nJ(oi, θ, q) =\n1\n|oi|\n|oi|\nX\nt=1\n \nmin\nn\nrratio,i,t ˆAi,t,\nclip\u0000rratio,i,t, 1 −ϵ, 1 + ϵ\u0001 ˆAi,t\no\n−β DKL\nh\nπθ ∥πref\ni!\n.\nThen, the GRPO objective becomes\nJGRP O(θ) = Eq∼P (Q)\n\"\n1\nG\nG\nX\ni=1\nJ(oi, θ, q)\n#\n,\nwhere the probability ratio is defined as\nrratio,i,t ≜πθ(oi,t | q, oi,<t)\nπold\nθ (oi,t | q, oi,<t).\nwhere ϵ is a clipping hyperparameter akin to PPO, and β\nadjusts the KL-divergence penalty encouraging the new policy\nπθ not to deviate excessively from a reference policy πref,\nwhich is typically the initial supervised fine-tuned (SFT)\nmodel [172, 173]. GRPO can be applied in two modes: outcome\nsupervision and process supervision.\nOutcome Supervision: Provides a reward only at the\nend of each output. The advantage ˆAi,t for all tokens in the\noutput is set as the normalized reward ¯ri.\n¯ri = ri −mean(r)\nstd(r)\n.\n\n\n11\nProcess Supervision: Provides a reward at the end of\neach reasoning step. The advantage ˆAi,t for each token is\ncalculated as the sum of the normalized rewards from the\nfollowing steps:\nˆAi,t =\nX\nindex(j)≥t\n¯ri,index(j),\nwhere index(j) is the end token index of the j-th step.\nOverall, GRPO serves as an efficient alternative to classic actor-\ncritic frameworks in DeepSeekR1 [40] by leveraging group-\nlevel advantages, thereby reducing training costs without\nsacrificing the capacity to distinguish fine-grained differences\namong candidate responses.\nFine-grained per-step rewards enable the\nmodel to effectively identify and reinforce high-\nquality responses, boosting overall performance\nin complex, multi-step reasoning tasks.\n3.2.9\nMulti-Sample Comparison Optimization\nInstead of relying solely on single-pair comparisons, multi-\nsample comparison optimization [174] approach compares\nmultiple\nresponses\nsimultaneously\nto\npromote\ndiversity\nand mitigate bias. Specifically, given a set of responses\n{y1, y2, . . . , yn} for a query x, the probability of observing the\nranking y1 > y2 > · · · > yn is determined by the product\nP(y1 > y2 > · · · > yn) =\nY\ni\neR(x,yi)\nP\nj eR(x,yj) .\nIn this formulation, each response yi is jointly evaluated in the\ncontext of all other responses, ensuring that comparisons are\nnot isolated pairwise events but rather part of a broader rank-\ning framework that helps capture more nuanced preferences\nand reduces potential biases.\n3.3\nPure RL Based LLM Refinement\nThe work from Guo et al. (2025) [40] introduces two main\nmodels: DeepSeek-R1-Zero and DeepSeek-R1.\n• DeepSeek-R1-Zero operates with a purely Reinforce-\nment Learning approach, excluding any SFT.\n• DeepSeek-R1 incorporates cold-start data and applies\na multi-stage training pipeline.\nThe methodology encompasses several steps (See Figure 2\nin GRPO for main steps): collecting cold-start data, perform-\ning RL training, carrying out SFT, using distillation to transfer\nknowledge to smaller models, and addressing specific chal-\nlenges such as language mixing and readability. This multi-\nstage pipeline ensures robustness and alignment with human\npreferences, while distillation enables efficient deployment of\nsmaller models without significant performance loss.\n3.3.1\nCold-Start RL Phase\nThe process begins with a cold-start RL phase, where a small\namount of curated data is gathered to fine-tune an initial, or\nbase, model. Following this preliminary fine-tuning, RL is con-\nducted—often via algorithms like GRPO until convergence. The\ncold-start phase is critical for stabilizing the model before full\nRL training, preventing instability that can arise from purely\nRL-driven updates. The cold-start data preparation focuses\non capturing human-readable reasoning patterns to prevent\ninstability from purely RL-driven updates. This step generates\nCoT-style examples with consistent < reasoning_process >\nand < summary > fields, usually involving thousands of\ncarefully curated samples. Structured CoT formats and con-\nsistent fields ensure clarity and robustness in the model’s rea-\nsoning outputs, reducing errors and improving interpretabil-\nity [8, 175, 176, 177].\nCoT examples before RL training provide a\nstronger foundation for reasoning tasks, leading\nto more robust and interpretable outputs.\n3.3.2\nRejection Sampling and Fine-tuning\nThis concept is also used in WebGPT [81]. Once RL stabilizes,\na rejection sampling mechanism is employed to generate high-\nquality responses that are subsequently filtered for correct-\nness, clarity, and other quality metrics. These filtered re-\nsponses are then blended with additional datasets to produce\na new, larger corpus for Supervised Fine-Tuning. Rejection\nsampling ensures that only high-quality outputs are used for\nfurther training, enhancing the model’s overall performance\nand reliability. After RL converges for high-stakes reasoning\ntasks, rejection sampling is used to filter a large number of\ngenerated outputs, expanding the training set. These newly\ngenerated reasoning examples (potentially up to hundreds of\nthousands in quantity) are mixed with existing SFT data to\ncreate a combined dataset of substantial size (often around\n800k samples). Rejection sampling and dataset expansion\nsignificantly enhance the model’s coverage of general tasks\nwhile preserving its reasoning proficiency.\n3.3.3\nReasoning-Oriented RL\nThe reasoning-oriented RL leverages GRPO [59], which samples\na group of outputs from the current policy and computes\nrewards and advantages for each output. Rewards may be\ncomputed via rule-based checks, e.g., ensuring correct solu-\ntions in math or code tasks, enforcing structured CoT tags,\nand penalizing undesired language mixing. GRPO group-based\nsampling and reward computation ensure that the model\nprioritizes high-quality, structured outputs, enhancing its rea-\nsoning capabilities.\n3.3.4\nSecond RL Stage for Human Alignment\nA second RL stage further aligns the model with broader\nhuman preferences (helpfulness, harmlessness, creativity, etc.)\nby introducing additional reward signals and prompt distri-\nbutions. The second RL stage ensures the model aligns with\nhuman values, making it more versatile and contextually\naware. After re-training the base model on this combined\ndataset, a second round of RL can be conducted to align\nthe model more closely with human preferences (e.g., for\nhelpfulness and harmlessness). This RL stage fine-tunes the\nmodel to better align with human values, ensuring outputs\nare not only accurate but also contextually appropriate.\n3.3.5\nDistillation for Smaller Models\nFinally, distillation techniques are used to transfer the refined\ncapabilities of the main model to smaller architectures, en-\nabling more efficient deployments without sacrificing much\n\n\n12\nData\nSystem\nModel\nAccelerators \n(Groq, vLLM, Triton,\netc.) \nData Compression, Data\nFiltering (TokenMerging,\nRecapDataComp-18,\netc.)\nCo-Optimized Architectures\n(FlashAttention, BlockSparse\nIO, DeepSeek v3 etc.)\nParallel Computing, \nDistributed Training\n(LoRA, PEFT,\nDeepSpeed,etc.)\nScaling law, Data mining\n(Chinchilla, RETRO, C4\ndata, etc.)\nModel compression\n(Bitsandbite, GPTQ,\netc.)\nEfficient Finetuning and Deployment\nFig. 4: This Venn diagram illustrates the interplay between Sys-\ntem, Data, and Model for efficient finetuning and deployment.\nIt covers strategies like accelerators (Groq, vLLM), adaptation\n(LoRA, PEFT), co-optimized architectures (FlashAttention), data\ncompression (TokenMerging), scaling laws (Chinchilla), and\nmodel compression (GPTQ) to boost performance and scalability.\nperformance. It allows smaller models to inherit advanced\nreasoning capabilities, making them competitive on challeng-\ning benchmarks without the computational costs of full-scale\nRL training. Finally, distillation plays a pivotal role: the top-\nperforming model, DeepSeek-R1 [40], serves as a teacher to\nsmaller architectures (e.g., Qwen or Llama families, ranging\nfrom 1.5B to 70B parameters). This transfer allows the smaller\nmodels to inherit advanced reasoning capabilities, making\nthem competitive on challenging benchmarks without incur-\nring the computational costs of full-scale RL training.\nDistillation democratizes advanced reasoning\ncapabilities, enabling smaller models to achieve\ncompetitive performance with reduced compu-\ntational overhead.\n4\nSupervised Finetuning in LLMs\nAs shown in Figure 2, finetuning forms a basic component of\nLLM post-training recipes. In this section, we summarize the\ndifferent types of LLM fine-tuning mechanisms.\n4.1\nInstruction finetuning\nIn instruction finetuning, a model is trained on curated pairs\nof instruction (prompt) and response (completion). The main\ngoal is to guide the LLM to follow a user-provided instruction\naccurately and helpfully, regardless of the task domain. This\nusually involves compiling large, diverse instruction-response\ndatasets covering many task types (e.g., summarization, QA,\nclassification, creative writing). Models such as T0 [178],\nFLAN [179], Alpaca [180], Vicuna [181] and Dolly [182]\ndemonstrate how instruction-finetuned LLMs can outperform\nbase models on zero-shot or few-shot tasks by virtue of their\nenhanced instruction-following abilities.\n4.2\nDialogue (Multi-turn) Finetuning\nSome LLMs undergo dialogue-style finetuning to better handle\nmulti-turn conversations. Different from instruction tuning\ndescribed above, here the data takes the form of a contin-\nuous dialogue (multi-turn conversations) instead of a single\nprompt-response pair. In this approach, training data consists\nof chat transcripts with muliple user queries and system re-\nsponses, ensuring the model learns to maintain context across\nturns and produce coherent replies. Models like LaMDA [183]\nand ChatGPT [39] highlight how dialogue-tuned LLMs can\nfeel more interactive and context-aware. While dialogue fine-\ntuning can overlap with instruction finetuning (because many\ninstructions come in a chat format), specialized conversation\ndata often yields more natural, multi-turn user experiences.\n4.3\nCoT Reasoning finetuning\nChain-of-Thought (CoT) reasoning finetuning teaches models\nto produce step-by-step reasoning traces instead of just final\nanswers. By exposing intermediate rationales or thoughts,\nCoT finetuning can improve both interpretability and accu-\nracy on complex tasks (e.g., math word problems, multi-\nhop QA). In practice, CoT finetuning uses supervised rea-\nsoning annotations (often handcrafted by experts) to show\nhow a solution unfolds. Notable early work includes Chain-\nof-Thought Prompting [8] and Self-Consistency [184], which\ninitially applied the idea to prompting; subsequent efforts\n(e.g., Chain-of-Thought Distillation [185]) adapt it to a full\nfinetuning or student-teacher paradigm. These efforts have\nalso been extended to the multimodal domain, e.g., LlaVA-\nCoT [186] and LlamaV-o1 [187] where image, QA and CoT\nreasoning steps are used in LLM finetuning.\n4.4\nDomain-Specific (Specialized) Finetuning\nWhen an LLM needs to excel in a specific domain (e.g.,\nbiomedicine, finance, or legal), domain-specific finetuning is\nused. Here, a curated corpus of domain-relevant text and la-\nbeled examples is employed to finetune the LLM. For instance,\nBioGPT [71] and BiMediX [216] specialize in biomedical\nliterature, FinBERT [217] for financial texts, ClimatGPT\n[218, 219] for climate and sustainability and CodeT5 [220]\nfor code understanding. Supervised finetuning in these do-\nmains often includes classification, retrieval, or QA tasks with\ndomain-specific data, ensuring the model’s parameters adapt\nto the specialized language and concepts of the field. Domain-\nspecific finetuning is also extended to vision-language models\nsuch as, [221] finetuned on remote sensing imagery, [222] on\nmedical imaging modalities, [223, 224, 225] on spatiotemporal\nvideo inputs, and [226] adapted for chart understanding.\n4.5\nDistillation-Based Finetuning\nLarge ‘teacher’ models are sometimes used to produce labeled\ndata or rationales, which a smaller ‘student’ model finetunes\non, this is generally called knowledge distillation [227, 228].\nIn the context of LLMs, CoT Distillation [185] is one example\nwhere a powerful teacher LLM generates intermediate rea-\nsoning steps, and the student LLM is finetuned to reproduce\nboth the final answer and the reasoning chain. Step-by-step\ndistillation [229] generates descriptive rationales alongside\nfinal answers to train smaller models through distillation\nwith smaller datasets. This approach can yield lighter, faster\nmodels that retain much of the teacher’s performance, even in\nzero-shot or few-shot tasks [230].\n\n\n13\nModel\nCategory\nSource\nDescription\n1. Parameter-Efficient Fine-Tuning & Model Compression\nLoRA [60]\nLow-Rank Adaptation\nLink\nInjects trainable low-rank adapters for efficient fine-tuning.\nQLoRA [188]\nQuantized Adaptation\nLink\nCombines 4-bit quantization with LoRA to enable fine-tuning on consumer GPUs\nGPTQ [189]\nPost-Training Quantization\nLink\nOptimal 4-bit quantization method for GPT-style models with minimal loss\nSparseGPT [190]\nPruning\nLink\nOne-shot pruning that preserves model quality with compensation.\nPEFT (HF) [191]\nUnified Fine-Tuning\nLink\nLibrary integrating LoRA, prefix tuning, and other parameter-efficient methods\nBitsAndBytes [192]\nLow-Precision Training\nLink\nEnables 8-bit optimizers and 4-bit quantization for memory-efficient training\nAdaLoRA [193]\nAdaptive Adaptation\nLink\nDynamically allocates parameter budget between layers during fine-tuning\nP-Tuning v2 [194]\nPrompt Optimization\nLink\nLearns continuous prompt embeddings through deep prompt tuning\n2. Data Management & Preprocessing\nHF Datasets [195]\nData Processing\nLink\nUnified API for 30k+ datasets with streaming, versioning, and preprocessing\nWebDataset [196]\nData Streaming\nLink\nEfficient tar-based sharding format for petascale distributed training\nDVC [197]\nData Versioning\nLink\nGit-like version control for datasets and machine learning pipelines\nApache Arrow [198]\nMemory Format\nLink\nLanguage-agnostic columnar memory format for zero-copy data access\nZstandard [199]\nCompression\nLink\nHigh-speed compression algorithm for training data storage/transfer\nCleanlab [200]\nData Quality\nLink\nAutomatic detection of label errors and outliers in training datasets\n3. Distributed Training & Optimization\nDeepSpeed [201]\nTraining Optimization\nLink\nZeRO parallelism, 3D parallelism, and memory optimizations for giant models\nMegatron-LM [202]\nModel Parallelism\nLink\nNVIDIA’s optimized framework for large transformer model training\nColossal-AI [203]\nHeterogeneous Training\nLink\nUnified system supporting multiple parallelization strategies\nHorovod [204]\nDistributed Training\nLink\nMPI-inspired framework for multi-GPU/multi-node synchronization\nRay [205]\nDistributed Computing\nLink\nUniversal framework for distributed Python applications at scale\n4. Efficient Inference & Deployment\nvLLM [206]\nServing Optimization\nLink\nPaged attention implementation for high-throughput LLM serving\nTensorRT [207]\nGPU Optimization\nLink\nNVIDIA’s inference optimizer with kernel fusion and quantization support\nTriton [208]\nServing Framework\nLink\nProduction-grade serving with concurrent model execution support\nONNX [209]\nCross-Platform\nLink\nUnified inference engine with hardware-specific optimizations\nOpenVINO [210]\nIntel Optimization\nLink\nRuntime for Intel CPUs/iGPUs with pruning/quantization support\nXNNPACK [211]\nMobile Inference\nLink\nHighly optimized floating-point kernels for ARM CPUs\nGroq [212]\nAI Accelerator\nLink\nDeterministic low-latency inference via custom tensor streaming processor\n5. Integrated Development Ecosystems\nHF Ecosystem [213]\nFull Stack\nLink\nTransformers + Datasets + Accelerate + Inference Endpoints\nDeepSpeed [201]\nTraining/Inference\nLink\nMicrosoft’s end-to-end solution for billion-parameter models\nPyTorch [214]\nUnified Framework\nLink\nNative LLM support via torch.compile and scaled dot-product attention\nLLM Reasoners [215]\nAdvanced Reasoning\nLink\nEnhances LLM reasoning capabilities using advanced search algorithms.\nTABLE 2: Comprehensive Overview of Modern LLM Methods and Frameworks.\n4.6\nPreference and Alignment SFT\nWhile RLHF is not purely supervised, it starts with a su-\npervised preference or alignment finetuning stage. This stage\nuses human-labeled or human-ranked examples to teach the\nmodel about desirable vs. undesirable outputs (e.g., safe vs.\ntoxic). By training on these explicit preferences, the model\nbecomes more aligned with user values, reducing harmful or\noff-topic completions. Works like InstructGPT [58] illustrate\nhow supervised preference data is critical before reward model\ntraining and RL updates begin.\n4.7\nEfficient Finetuning\nFully finetuning a LLM can be computationally and memory-\nintensive, particularly as model sizes grow into the tens or\nhundreds of billions of parameters. To address these chal-\nlenges, parameter-efficient finetuning (PEFT) techniques intro-\nduce a small set of trainable parameters or learnable prompts\nwhile leaving most of the model weights frozen. Approaches\nsuch as LoRA [60], Prefix Tuning [231], and Adapters [232]\nexemplify this strategy by injecting lightweight modules (or\nprompts) in specific layers, thus significantly reducing the\nmemory footprint.\nFigure 4 illustrates how these techniques fit into a broader\necosystem that involves system-level optimizations, data man-\nagement, and evaluation strategies for LLMs. In particular,\nPEFT approaches can be combined with quantization and\npruning methods [190, 188] to further minimize memory\nusage and compute overhead, enabling finetuning on smaller\nGPUs or even consumer-grade hardware. For instance, QLoRA\nunifies 4-bit quantization with low-rank adaptation, while\nBitsAndBytes provides 8-bit optimizers to make LLM training\nmore practical in constrained environments (Table 2).\nMoreover, these PEFT methods still require supervised\ndata to guide the adaptation process, but the reduction in\nthe number of trainable parameters makes it more feasible\nto use in-domain or task-specific datasets. This is especially\nvaluable for specialized domains (e.g., medical or software\ndevelopment), where data might be limited or expensive to\nannotate. As shown in Table 2, PEFT (HF) integrates several\nof these approaches (LoRA, prefix tuning, and more) into a\nsingle library, streamlining deployment in both research and\nproduction settings.\nCombining efficient tuning designs like LoRA\nand QLoRA with system and data optimizations\n(Figure 4) enables cost-effective LLM adaptation\nfor tasks like domain-specific text generation,\nwithout expensive full fine-tuning.\n5\nTest-time Scaling Methods\nWhile RL fine-tunes the model’s policy, test-time scaling (TTS)\nenhances reasoning during inference typically without model\n\n\n14\nTree-of-Thoughts\ncompute-optimal scaling strategy\nSelf-Consistency\nDecoding\nTest Time Scaling\nSequential \nrevision\nImproved\nReasoning\nAdvanced \nSampling \nScaling \nStrategies\nChain-of-Thought\nPrompting\nConfidence\nBased \nSampling\nSearch \nAgainst \nVerifiers\nBeam\nSearch\nBest-of-N\nSearch\nMonte Carlo\nTree Search\nSelf-Improvement\nvia Refinements \nFig. 5: An overview of Test-time Scaling methods: parallel\nscaling, sequential scaling, and search-based methods. It also\nshows how they integrate into a compute-optimal strategy.\nupdates. Figure 5 presents a taxonomy of TTS methods,\ncategorizing them based on their underlying techniques.\n5.1\nBeam Search\nBeam search was first introduced in the context of speech\nrecognition [233]. It gained prominence as a decoding strategy\nfor sequence models and was later adopted in neural machine\ntranslation and speech systems [234]. With the popularity of\nLLMs, this algorithm has been used for approximate search in\nmany text generation tasks.\nThe concept of Beam search is similar to pruned breadth-\nfirst search, where top N highest-probability partial se-\nquences (the ‘beam’) are kept at each step, discarding lower-\nprobability paths. By limiting the beam width (N), it man-\nages the exponential search space while aiming to find a\nnear-optimal sequence. These beams are expanded at each\ndecoding step to find multiple probable paths. In reasoning\nLLMs, such paths allow us to systematically explore multiple\nreasoning chains in parallel, focusing on the most promising\nones. This ensures that high-likelihood reasoning steps are\nconsidered, which can improve the chances of finding a correct\nand coherent solution compared to greedy decoding. It has\ntraditionally been used in tasks such as translation, sum-\nmarization, and code generation, where the goal is a highly\nprobable correct sequence [93].\nWhile modern LLMs often favor stochastic sampling (e.g.,\ntemperature sampling) to promote diversity in generated text,\nbeam search is still a valuable technique for structured reason-\ning problems. For example, the Tree-of-Thoughts framework\n[84] allows plugging in different search algorithms to explore a\ntree of possible ‘thoughts’ or reasoning steps; one instantiation\nuses a beam search (with beam width b) to maintain the b\nmost promising states at each reasoning step. Here, beam\nsearch is used to systematically explore solution steps for tasks\nlike mathematical puzzles and planning problems, pruning\nless promising reasoning branches and thus improving the\nmodel’s problem-solving accuracy. Beam search remains a\nstrong baseline for test-time reasoning when one wants the\nmodel to output the single most likely reasoning path or\nanswer under the model’s learned distribution.\n5.2\nBest-of-N Search (Rejection Sampling)\nBest-of-N (BoN) [235] search generates N candidate outputs\n(usually via sampling) and then picks the best one according\nto a chosen criterion (e.g., a reward model or the model’s own\nlikelihood) [236, 237, 238]. Conceptually, this is an application\nof rejection sampling: one draws multiple samples and rejects\nall but the top-rated result. Unlike Beam Search [233, 234],\nwhich incrementally expands and prunes partial hypotheses,\nBoN simply samples full solutions independently, allowing for\ngreater diversity but at a higher computational cost. Beam\nSearch systematically aims for the most probable sequence,\nwhile BoN may capture high-quality but lower-probability\nsolutions through brute-force sampling.\nBeam search (effective for harder questions)\noutperforms best-of-N sampling at low compute\nbudgets, while best-of-N scales better for easier\ntasks.\nDuring LLM inference, BoN is used to enhance correctness or\nalignment without retraining the model. By sampling multiple\nanswers and selecting the top candidate (e.g., via a reward\nmodel or a checker), BoN effectively boosts accuracy on tasks\nlike QA or code generation. BoN is easy to understand and\nimplement and is almost hyper-parameter-free, with N being\nthe only parameter that can be adjusted at inference. In\nreinforcement learning contexts, BoN sampling can serve as\na baseline exploration mechanism i.e., to generate many roll-\nouts, pick the best outcome according to the learned reward,\nand proceed, although at increased computational overhead.\nOpenAI’s WebGPT used BoN to pick the best response via\na reward model, yielding strong QA performance [81]. BoN\nis also used as a simple alignment method that is highly com-\npetitive with other post-training techniques e.g., RLHF [58] and\nDPO [78]. Studies have shown BoN can approach or match RLHF\nresults when guided by a sufficiently robust reward model\n[82, 239]. Alternatives such as speculative rejection [240] build\non this idea and utilize a better reward model to improve\nefficiency. The studies also highlight issues of reward hacking\nif the (proxy) reward function used for BoN is imperfect [241]\nor instability issues if the N parameter gets very large.\nUsing process reward models with beam\nsearch vs best-of-N depends on the difficulty and\ncompute budget.\n5.3\nCompute-Optimal Scaling\nThe Compute-Optimal Scaling Strategy (COS) [83] is a dy-\nnamic method designed to allocate computational resources\nefficiently during inference in LLMs, optimizing accuracy with-\nout unnecessary expense. Instead of applying a uniform sam-\npling strategy across all inputs, this approach categorizes\nprompts into five difficulty levels—ranging from easy to\nhard—either by leveraging oracle difficulty (ground-truth suc-\ncess rates) or model-predicted difficulty (e.g., verifier scores\nfrom Preference Ranking Models). Once categorized, the\nstrategy adapts compute allocation: easier prompts undergo\nsequential refinement, where the model iteratively refines\nits output to improve correctness, while harder prompts\ntrigger parallel sampling or beam search, which explores\nmultiple response variations to increase the likelihood of\nfinding a correct solution. This dual approach balances ex-\n\n\n15\nploration (for challenging inputs) and refinement (for near-\ncorrect responses), ensuring optimal performance per unit\nof computational effort. Remarkably, this method achieves\nfour times lower compute usage than traditional best-of-N\nsampling while maintaining equivalent performance. The key\ninsight is that by matching computational strategy to problem\ndifficulty, it avoids wasted resources on trivial cases while\nensuring sufficient sampling diversity for complex tasks. In\nessence, it functions as a “smart thermostat” for LLM inference,\ndynamically adjusting computational effort in response to\ninput complexity, leading to a more efficient and cost-effective\ndeployment of large-scale language models.\nCOS achieves 4× efficiency gains over best-\nof-N baselines by optimally balancing sequen-\ntial/parallel compute. Beam search + revisions\noutperform larger models on easy/intermediate\nquestions.\n5.4\nChain-of-thought prompting\nCoT prompting induces LLMs to produce intermediate reason-\ning steps rather than jumping directly to the final answer.\nBy breaking down problems into logical sub-steps, CoT taps\ninto a model’s latent ability to perform multi-step inferences,\nsignificantly improving performance on tasks like math word\nproblems, logical puzzles, and multi-hop QA.\nWei et al. [8] demonstrated CoT’s effectiveness on arith-\nmetic and logic tasks, showing large gains over direct prompt-\ning. Kojima et al. [242] introduced Zero-Shot CoT, revealing\nthat even adding a simple phrase like “Let’s think step\nby step” can trigger coherent reasoning in sufficiently large\nmodels. Subsequent works (e.g., Wang et al., 2022 [184]) com-\nbined CoT with sampling-based strategies (Self-Consistency)\nfor even higher accuracy. As described in Sec. 5.4, CoT format\ndata have also been used for SFT and are shown to help\nreshape the model responses to be more step-by-step.\nFinetune models to revise answers sequen-\ntially, leveraging previous attempts. Sequential\nrevisions excel on easier questions, while parallel\nsampling (exploration) benefits harder ones\n5.5\nSelf-Consistency Decoding\nSelf-Consistency is a decoding strategy introduced by Wang\net al. [243]. It was proposed as an alternative to simple\ngreedy decoding for chain-of-thought prompts. It built upon\nthe idea of sampling multiple distinct reasoning paths for a\nquestion and was the first to show that marginalizing over\nthose paths can significantly improve accuracy on arithmetic\nand reasoning problems. In other words, it allows the model\nto think in many ways and then trust the consensus, which\nimproves correctness in many reasoning scenarios.\nThe self-consistency method works by sampling a diverse\nset of reasoning chains from the model (via prompt engi-\nneering to encourage different CoTs, and using temperature\nsampling) and then letting the model output a final answer\nfor each chain. Instead of trusting a single chain, the method\nselects the answer that is most consistent across these multiple\nreasoning paths, effectively a majority vote or highest probabil-\nity answer after marginalizing out the latent reasoning. The\nintuition is that if a complex problem has a unique correct\nanswer, different valid reasoning paths should converge to\nthat same answer. By pooling the outcomes of many chains,\nthe model can “decide” which answer is most supported. In\napplication, one might sample, e.g., 20 CoTs for a math prob-\nlem and see what final answer appears most frequently; that\nanswer is then taken as the model’s output. This approach\nturns the one-shot CoT process into an ensemble where the\nmodel cross-verifies its answers. It is especially useful for\narithmetic and commonsense reasoning tasks where reasoning\ndiversity helps.\nSmaller models with test-time compute can\noutperform much larger models in certain sce-\nnarios.\nSelf-consistency is often combined with other methods:\ne.g., sampling multiple chains and then applying a verifier\nto the most common answer. Its strength lies in requiring no\nnew training, only extra sampling, making it a popular test-\ntime scaling strategy to obtain more reliable answers from\nLLMs. It has also inspired other variants, e.g., Universal Self-\nConsistency [244] extend the original idea (which worked only\nwith majority vote on single final answer) to more general\ngeneration tasks such as summarization and open-ended QA.\n5.6\nTree-of-thoughts\nToT framework [84] generalizes the chain-of-thought approach\nby allowing the model to branch out into multiple possible\nthought sequences instead of following a single linear chain. It\nthus formulates the problem of language-model reasoning as a\ntree search, drawing on classic AI search methods inspired by\nhuman problem-solving [245, 37]. Tree of Thoughts treats in-\ntermediate reasoning steps as “nodes” in a search tree and uses\nthe language model to expand possible next steps (thoughts)\nfrom a given state. Rather than sampling one long reasoning\npath, the model explores a tree of branching thoughts and\ncan perform lookahead and backtracking. At each step, the\nLLM might generate several candidate next thoughts, and a\nheuristic or value function evaluates each partial solution\nstate. Then a search algorithm (e.g., depth-first, breadth-first,\nbeam search) navigates this tree, deciding which branches to\nexplore further. This approach allows systematic exploration\nof different reasoning strategies: if one path leads to a dead-\nend, the model can return to an earlier state and try a different\nbranch (unlike standard CoT which commits to one line of\nreasoning). In effect, ToT is an iterative prompting procedure\nwhere the model generates thoughts, evaluates them, and\nrefines its approach, mimicking how a human might mentally\nmap out various ways to solve a problem.\nToT is especially useful for complex problems like puzzles,\nplanning tasks, or games where multiple steps and strategic\nexploration are needed and outperforms simpler CoT methods\nby systematically searching through the solution space. It pro-\nvides a flexible framework – one can plug in various generation\nstrategies (e.g. sampling vs. prompting) and search algorithms\n(BFS, DFS, A*, MCTS) depending on the task. Although\n\n\n16\nInput\nOutput\nInput\nOutput\n...\nInput\nOutput\n...\n...\n...\nInput\nOutput\n...\n...\n...\nDirect\nCoT\nSelf-consistancy\nMultiple CoT\nInput\nOutput\nToT\nInput\nOutput\nGoT\nNot graded\nPositive graded\nNegative graded\nPositive graded\nBack tracking\nSelf-refining\nVoting\nAggregation\nFig. 6: This figure compares reasoning strategies in LLMs, evolving from Direct Prompting, which maps input to output without\nreasoning, to more structured approaches. Chain-of-Thought (CoT) introduces step-by-step reasoning, while Self-Consistency\n(CoT-SC) generates multiple CoT paths and selects the most frequent answer. Multiple CoTs explores diverse reasoning paths\nindependently. Tree-of-Thoughts (ToT) structures reasoning as a tree, enabling backtracking and refinement, whereas Graph-\nof-Thoughts (GoT) generalizes this by dynamically aggregating and connecting thoughts. The legend deciphers key mechanisms\nlike grading, backtracking, and self-refinement, crucial for optimizing reasoning efficiency.\nmore computationally heavy, ToT shows that allocating extra\n“thinking time” (compute) to explore alternatives can yield\nsignificantly better reasoning and planning performance. It\nhas spawned follow-up research aiming to improve or utilize\nit for better reasoning e.g., multi-agent systems have been\ncombined with ToT: different LLM “agents” generate thoughts\nin parallel and a validator agent prunes incorrect branches,\nleading to improved accuracy over the single-agent ToT [246].\nInference-time computation for LLMs can out-\nperform scaling model parameters, especially for\nchallenging reasoning tasks like math problems.\n5.7\nGraph of Thoughts\nThe Graph of Thoughts (GoT) [247] framework extends the\nToT by allowing more flexible and efficient reasoning processes\nthrough graph-based structures rather than strict hierarchical\ntrees. Thought representation differs between the two ap-\nproaches: in ToT, each step in reasoning is structured as a\nnode in a tree with fixed parent-child relationships, whereas\nGoT represents thoughts as nodes in a graph, enabling more\nadaptable dependencies and interconnections.\nIn terms of thought expansion strategies, ToT follows a\ntraditional approach where multiple thought candidates are\ngenerated at each step, explored using tree-based search\nstrategies, and pruned based on heuristics before selecting the\nmost optimal path. In contrast, GoT incorporates graph-based\nthought expansion, allowing thoughts to interconnect dynam-\nically. This enables three key transformations: aggregation\n(merging multiple solutions into a unified answer), refinement\n(iteratively improving thoughts over time), and generation\n(producing diverse candidates). Instead of navigating through\na rigid hierarchy, GoT prioritizes thoughts using a volume\nmetric and explores paths optimally, reducing unnecessary\ncomputations.\nA critical limitation of ToT is its restricted backtrack-\ning—once a branch is discarded, it is not reconsidered. GoT\novercomes this by allowing iterative refinement, where previ-\nous thoughts can be revisited, modified, and improved upon.\nThis iterative nature is particularly useful in complex rea-\nsoning tasks where initial thoughts may require adjustments.\nMoreover, computational efficiency in GoT is significantly\nimproved by reducing redundant calculations through the\nmerging of partial solutions.\nGoT enhances problem-solving efficiency and\nadaptability, making it superior to ToT for tasks\nrequiring complex reasoning.\n5.8\nConfidence-based Sampling\nIn confidence-based sampling, the language model generates\nmultiple candidate solutions or reasoning paths and then\nprioritizes or selects among them based on the model’s own\nconfidence in each outcome [248]. This can happen in two\nways: (a) Selection: Generate N outputs and pick the one with\nthe highest log probability (i.e., the model’s most confident\noutput). This is essentially best-of-N by probability – the\nmodel chooses the answer it thinks is most likely correct.\n(b) Guided exploration: When exploring a reasoning tree or\nmulti-step solution, use the model’s token probabilities to\ndecide which branch to expand (higher confidence branches\nare explored first). In other words, the model’s probability es-\ntimates act as a heuristic guiding the search through solution\nspace [249]. Compared to pure random sampling, confidence-\nbased methods bias the process toward what the model be-\nlieves is right, potentially reducing wasted exploration on low-\nlikelihood (and often incorrect) paths.\nConfidence-based strategies have been incorporated at\ninference time e.g., a tree-based search for LLM generation [248]\nassigns each possible completion (leaf) a confidence score. The\nalgorithm samples leaves in proportion to these confidence\nscores to decide which paths to extend. Similarly, some rea-\nsoning approaches use the model’s estimated likelihood of an\nanswer to decide when to halt or whether to ask a follow-\nup question – essentially if the model’s confidence is low,\n\n\n17\nit might trigger further reasoning (a form of self-reflection).\nConfidence-based selection is also used in ensemble settings:\ne.g., an LLM may generate multiple answers and a secondary\nmodel evaluates the confidence of each answer being correct,\npicking the answer with the highest confidence. This was\nexplored in tasks like medical Q&A, where an LLM gave an\nanswer and a confidence score, and only high confidence\nanswers were trusted or returned [250].\n5.9\nSearch Against Verifiers\nThis verification approach [251] in LLMs enhances answer\nquality by generating multiple candidate responses and select-\ning the best one using automated verification systems. This\napproach shifts focus from increasing pre-training compute\nto optimizing test-time compute, allowing models to “think\nlonger” during inference through structured reasoning steps\nor iterative refinement. The method involves two main steps:\nGeneration: The model (or “proposer” produces multiple\nanswers or reasoning paths, often using methods like high-\ntemperature sampling or diverse decoding.\nVerification: A verifier (e.g., a reward model) evaluates these\ncandidates based on predefined criteria, such as correctness,\ncoherence, or alignment with desired processes. Verifiers are\ncategorized based on their evaluation focus:\n1) Outcome Reward Models (ORM): Judge only the final\nanswer (e.g., correctness of a math solution).\n2) Process Reward Models (PRM): Evaluate the reason-\ning steps (e.g., logical coherence in a thought chain),\nproviding granular feedback to prune invalid paths.\nSeveral techniques fall under this paradigm, enhancing\nverification-based optimization. Best-of-N Sampling involves\ngenerating multiple answers and ranking them via a verifier\n(ORM/PRM), selecting the highest-scoring one, making it a sim-\nple yet effective approach for improving answer correctness.\nBeam Search with PRM tracks top-scoring reasoning paths\n(beams) and prunes low-quality steps early, similar to Tree\nof Thought approaches, balancing breadth and depth in rea-\nsoning path exploration. Monte Carlo Tree Search balances\nexploration and exploitation by expanding promising reason-\ning branches, simulating rollouts, and backpropagating scores,\nproviding an optimal trade-off between search depth and\nverification confidence. Majority Voting (Self-Consistency)\naggregates answers from multiple samples and selects the\nmost frequent one, avoiding explicit verifiers, which works\nwell in settings where consistency across multiple responses\nindicates correctness.\nORM is suitable for tasks where correctness is\nbinary (right/wrong) and can be easily assessed.\nPRM is useful in multi-step reasoning, ensuring\nintermediate steps follows logical progression.\n5.10\nSelf-Improvement via Refinements\nThis approach refers to the ability of LLMs to enhance their\noutputs through self-evaluation and revision iteratively. This\nprocess enables models to refine their responses dynamically\nduring inference rather than relying solely on pre-trained\nweights. One notable method is Self-Refinement [252],\nwhere an LLM generates an initial response, critiques it, and\nthen refines the output based on its self-generated feedback.\nThis iterative process continues until the model achieves a\nsatisfactory result. Such techniques have been shown to im-\nprove performance on various tasks, including mathematical\nreasoning and code generation. This process follows these key\nsteps: a) Initial Generation: The model produces an answer\nor reasoning path. b) Self-Critique: The model reviews its\nown response and identifies errors, inconsistencies, or areas for\nimprovement. c) Refinement: The model adjusts its response\nbased on the critique and generates an improved version.\nd) Iteration: The process repeats until the output meets a\npredefined quality threshold or stops improving.\nAnother approach is called Self-Polish [253], where the\nmodel progressively refines given problems to make them more\ncomprehensible and solvable. By rephrasing or restructuring\nproblems, the model enhances its understanding and pro-\nvides more accurate solutions. Self-Polish involves progres-\nsive refinement of problem statements to make them more\ncomprehensible and solvable. The model first rephrases or\nrestructures the problem for better clarity, then breaks down\ncomplex queries into simpler sub-problems and refines am-\nbiguous inputs to ensure precise understanding. By restruc-\nturing problems before solving them, the model improves its\ncomprehension and generates more accurate solutions.\nSelf-improvement methodologies represent a\nparadigm shift in LLM optimization, emphasiz-\ning active reasoning and internal feedback over\nstatic pre-training. By iterating on their own\nresponses, models achieve greater consistency\nand accuracy across a wide range of applications.\n5.11\nMonte Carlo Tree Search\nMCTS [254] is based on the application of Monte Carlo sim-\nulations to game-tree search. It rose to prominence with\nsuccesses in games, notably, it powered AlphaGo [255] in\n2016 by searching possible moves guided by policy and value\nnetworks. This, as well as the application to other board and\nvideo games, demonstrates the power of MCTS for sequential\ndecision-making under uncertainty.\nMCTS is a stochastic search algorithm that builds a decision\ntree by performing many random simulations. It is best known\nfor finding good moves in game states, but it can be applied to\nany problem where we can simulate outcomes. The algorithm\niteratively: (a) Selects a path from the root according to a\nheuristic (like UCT [256], which picks nodes with a high upper-\nconfidence bound), (b) Expands a new node (a previously\nunvisited state) from the end of that path, (c) Simulates a ran-\ndom rollout from that new state to get an outcome (e.g., win\nor loss in a game, or some reward), and (d) Backpropagates\nthe result up the tree to update the values of nodes and inform\nfuture selections. Repeating these simulations thousands of\ntimes concentrates the search on the most promising branches\nof the tree. In essence, MCTS uses random sampling to evaluate\nthe potential of different action sequences, gradually biasing\nthe search towards those with better average outcomes. In\n\n\n18\nLLM reasoning, we can treat the generation of text as a\ndecision process and use to explore different continuations.\nFor example, at a given question (root), each possible next\nreasoning step or answer is an action; a simulation could\nmean letting the LLM continue to a final answer (perhaps\nwith some randomness), and a reward could be whether the\nanswer is correct. By doing this repeatedly, MCTS can identify\nwhich chain of thoughts or answers has the highest empirical\nsuccess rate. The appeal of MCTS for reasoning is that it can\nhandle large search spaces by sampling intelligently rather\nthan exhaustively, and it naturally incorporates uncertainty\nand exploration.\nTrain verifiers to score intermediate steps\n(via Monte Carlo rollouts) instead of just final\nanswers.\nRecent efforts have integrated MCTS with LLMs to tackle\ncomplex reasoning and decision-making tasks. One example is\nusing MCTS for query planning: Monte Carlo Thought Search\n[257], where an LLM is guided to ask a series of sub-questions to\nfind an answer. Jay et al. [257] used an MCTS-based algorithm\ncalled ‘Monte Carlo Reasoner’ that treats the LLM as an\nenvironment: each node is a prompt (state) and each edge\nis an action (e.g., a particular question to ask or step to\ntake), and random rollouts are used to evaluate outcomes.\nThis approach allowed the system to efficiently explore a space\nof possible reasoning paths and pick a high-reward answer\npath, outperforming naive sampling in a scientific Q&A task.\nSimilarly, MCTS has been applied to code generation with LLMs\n[258] – the algorithm explores different code paths (using the\nmodel to propose code completions and etest them) to find a\ncorrect solution. Another line of work ensembles multiple LLMs\nwith MCTS, treating each model’s output as a branch and using\na reward model to simulate outcomes [259]. Early results show\nthat MCTS-based reasoning can solve problems that single-pass\nor greedy methods often miss, although with more compute\n[74]. The downside is that MCTS can be significantly slower\nthan straightforward sampling or beam search, which recent\nresearch is addressing by improving efficiency (e.g., by state\nmerging [87]). In general, MCTS brings the strength of planning\nalgorithms to LLM inference and enables an LLM to ’look ahead’\nthrough simulated rollouts and make more informed reasoning\nchoices, much like it has done for AI in gameplay.\nTest-time compute is not a 1-to-1 replacement\nfor pretraining but offers a viable alternative in\nmany cases.\n5.12\nChain-of-Action-Thought reasoning\nLLMs excel in reasoning tasks but rely heavily on external\nguidance (e.g., verifiers) or extensive sampling at inference\ntime. Existing methods like CoT [8] lack mechanisms for self-\ncorrection and adaptive exploration, limiting their autonomy\nand generalization. Satori [260] introduced a two-stage train-\ning paradigm, which works by initially tuning the model’s\noutput format and then enhancing its reasoning capabilities\nthrough self-improvement. In Stage 1 (Format Tuning), the\nmodel is exposed to a large set of 10K synthetic trajectories\ngenerated by a multi-agent framework comprising a generator,\na critic, and a reward model. This supervised fine-tuning\nhelps the model to produce outputs in specific reasoning\nformat using meta-action tokens, although it may still have\ndifficulty generalizing beyond these examples. In Stage 2 (Self-\nImprovement via RL), the model employs PPO with a Restart\nand Explore strategy [260], which allows it to restart from\nintermediate steps, whether they were correct or not, to refine\nits reasoning process. The model receives rewards based on a\ncombination of rule-based correctness, reflection bonuses, and\npreference-based Outcome Reward Model feedback explained\nin § 5.9, thereby incentivizing the allocation of more compu-\ntational resources to tougher problems and enabling extended\nreasoning during testing for complex tasks.\nMulti-agent frameworks and advanced fine-tuning strate-\ngies are increasingly being explored to enhance reasoning\nin LLMs. Multi-Agent LLM Training (MALT) [261] introduces\na structured approach where generation, verification, and\nrefinement steps are distributed across specialized agents,\nallowing for iterative self-correction and improved reasoning\nchains. Similarly, optimizing preference alignment remains\na crucial challenge in ensuring both safety and helpfulness\nin LLMs [262]. Approaches like Bi-Factorial Preference Opti-\nmization (BFPO) [263] reframe RLHF objectives into a single\nsupervised learning task, reducing human intervention while\nmaintaining robust alignment. Beyond text-based reason-\ning, multimodal approaches like Multimodal Visualization-of-\nThought (MVoT) [264] extend CoT prompting by incorporating\nvisual representations, significantly enhancing performance\nin spatial reasoning tasks. These advancements highlight the\ngrowing need for structured multi-agent collaboration, safety-\naware optimization, and multimodal reasoning to address\nfundamental limitations in LLM reasoning [265, 266, 267].\n5.13\nPretraining vs. Test-Time Scaling\nPretraining and TTS are two distinct strategies for improving\nLLM performance, each with different tradeoffs in compu-\ntational cost and effectiveness. Pretraining involves scaling\nmodel parameters or increasing training data to enhance\ncapabilities, requiring substantial upfront computational in-\nvestment [3]. In contrast, TTS optimizes inference-time com-\npute (such as iterative refinements, search-based decoding,\nor adaptive sampling), allowing performance improvements\nwithout modifying the base model.\nFrom a performance vs. cost perspective, TTS achieves\nresults comparable to a model 14× larger on easy to in-\ntermediate tasks (e.g., MATH benchmarks), while reducing\ninference costs by 4× fewer FLOPs in compute-intensive\nscenarios [268]. However, pretraining remains superior for\nthe hardest tasks or when inference compute constraints are\nhigh, as larger pretrained models inherently encode deeper\nreasoning capabilities.\nA smaller model with test-time compute can\noutperform a 14× larger model on easy/inter-\nmediate questions when inference tokens (Y) are\nlimited (e.g., self-improvement settings).\n\n\n19\nIn terms of use cases, TTS is useful for scenarios with\nflexible inference budget or when base models already exhibit\nreasonable competence in the task. Conversely, pretraining is\nessential for tasks requiring fundamentally new capabilities\n(e.g., reasoning on novel domains) where inference-time opti-\nmizations alone may not suffice.\nThere are notable tradeoffs between the two approaches.\nTTS reduces upfront training costs, making it attractive for\nflexible, on-the-go optimization, but requires dynamic com-\npute allocation at inference. Pretraining, on the other hand,\nincurs high initial costs but guarantees consistent performance\nwithout additional runtime overhead, making it ideal for\nlarge-scale API deployments or latency-sensitive applications.\nOverall, TTS and pretraining are complementary in nature.\nFuture LLM systems may adopt a hybrid approach, where\nsmaller base models are pretrained with essential knowledge,\nwhile TTS dynamically enhances responses through adaptive,\non-demand computation. This synergy enables more cost-\neffective and efficient large-scale model deployment.\nChoose pretraining for foundational capabil-\nities and test-time scaling for accurate context-\naware refinement.\n6\nBenchmarks for LLM Post-training Evaluation\nTo evaluate the success of LLM post-training phases, a di-\nverse set of benchmarks have been proposed covering mul-\ntiple domains: reasoning tasks, alignment, multilinguality,\ngeneral comprehension, and dialogue and search tasks. A well-\nstructured evaluation framework ensures a comprehensive\nunderstanding of an LLM strengths, and limitations across\nvarious tasks. These benchmarks play a crucial role in LLM\npost-processing stages, where models undergo fine-tuning, cal-\nibration, alignment, and optimization to improve response ac-\ncuracy, robustness, and ethical compliance. Next, we explain\nthe main benchmark gorups. Table 3 provides an overview of\nkey datasets categorized under these benchmark groups.\nReasoning Benchmarks. These benchmarks assess LLMs on\ntheir ability to perform logical, mathematical, and scientific\nreasoning. Mathematical reasoning datasets like MATH [269],\nGSM8K [270], and MetaMathQA [271] test models on\nproblem-solving, multi-step arithmetic, and theorem-based\nproblem formulations. Scientific and multimodal reasoning\nbenchmarks such as WorldTree V2 [272] and MMMU [274]\nevaluate knowledge in physics, chemistry, and multimodal\nunderstanding, which are crucial for fact-checking and veri-\nfication processes in LLM-generated responses. Additionally,\ndatasets like PangeaBench [273] extend reasoning tasks into\nmultilingual and cultural domains, enabling models to refine\ncross-lingual reasoning. These benchmarks help determine\nhow well models can process structured knowledge and apply\nlogical deductions.\nRL Alignment Benchmarks. RL alignment benchmarks\nare central to LLM alignment and post-training optimiza-\ntion. They refine response generation, ethical constraints, and\nuser-aligned outputs through RLHF. Datasets such as Help-\nSteer [280] and UltraFeedback [281] evaluate models based\non multi-attribute scoring and alignment with user instruc-\ntions. Anthropic’s HH-RLHF [121] explores how well mod-\nTABLE 3: Comprehensive Overview of Reasoning, RL Align-\nment, and Multilingual Datasets. Here, pointwise and pairwise\nrefer to different methods of evaluating model performance\nacross various tasks.\nDatasets\nDomain\nType\n#Samples\nEvaluation Criteria\nReasoning Benchmarks\nMATH [269]\nMath Reasoning\nPointwise\n7,500\nStep-by-step solutions\nGSM8K [270]\nMath Reasoning\nPointwise\n8.5K\nMulti-step reasoning\nMetaMathQA [271]\nMath Reasoning\nPointwise\n40K+\nSelf-verification, FOBAR\nWorldTree V2 [272]\nScience QA\nPointwise\n1,680\nMulti-hop explanations\nPangeaBench [273]\nMultimodal Reasoning\nPairwise\n47 Langs.\nCultural understanding\nMMMU [274]\nScience/Math\nPointwise College-Level\nPhysics, Chemistry, Bilingual\nTruthfulQA [275]\nQA/Reasoning\nPointwise\nN/A\nTruthfulness\nMathInstruct [276]\nMath Reasoning\nPointwise\n262K\nCorrectness\nMMLU [277, 278]\nMultitask Reasoning\nPointwise\n57 Tasks\nBroad knowledge evaluation\nMMLU-Fairness [277]\nFairness/Reasoning\nPointwise\nN/A\nBias/Equity Analysis\nDROP [279]\nReading/Reasoning\nPointwise\n96K\nDiscrete reasoning over paragraphs\nBBH [175]\nHard Reasoning\nPairwise\nN/A\nComplex logical problem-solving\nVRC-Bench [187]\nMultimodal Reasoning\nPairwise\nN/A\nVisual Reasoning and Classification\nRL Alignment Benchmarks\nHelpSteer [280]\nRL Alignment\nPairwise\n37K+\nMulti-attribute scoring\nAnthropic HH-RLHF [121]\nRL Alignment\nPairwise\n42.5K\nHarmlessness alignment\nUltraFeedback [281]\nRL Alignment\nPairwise\n64K\nInstruction-following, Truthfulness\nD4RL [282]\nRL/Control\nPointwise\nN/A\nOffline RL across domains\nMeta-World [283]\nRL/Control\nPointwise\nN/A\nMulti-task robotic RL\nMineRL [284]\nRL/Games\nPairwise\nN/A\nImitation learning, rewards\nMultilingual Evaluation\nCulturaX [285]\nMultilingual\nPointwise\n6.3T\nDeduplication, Quality\nPangeaIns [286]\nMultilingual\nPointwise\n6M\nMultilingual instructions\nTydiQA [287]\nMultilingual\nPointwise\nN/A\nCross-lingual QA\nXGLUE [288]\nMultilingual\nPointwise\nN/A\nCross-lingual language tasks\nMM-Eval [289]\nMultilingual\nPairwise\n4,981\nTask-oriented multilingual QA\nALM-Bench [289]\nMultilingual QA\nPointwise\nN/A\nMultilingual Evaluation\nGeneral Comprehension Benchmarks\nBigBench [290]\nGeneral Comprehension Pointwise 200+ Tasks\nBroad multi-domain evaluation\nChatbot Arena [291]\nComprehension\nPairwise\n33K\nUser preference\nMTBench [291]\nComprehension\nPairwise\n3K\nMulti-turn conversations\nRewardBench [167]\nComprehension\nPairwise\n2,998\nUser preference\nGeneral Comprehension Benchmarks\nConvAI2 [292]\nDialogue\nPointwise\nN/A\nEngagingness, Consistency\nMultiWOZ [293]\nDialogue\nPointwise\nN/A\nTask success, Coherence\nTrec DL21&22 [294, 295]\nSearch\nPointwise 1,549/2,673\nRelevance scoring\nBEIR [296]\nSearch\nPointwise 18 Datasets\nInformation retrieval\nStory & Recommendation Benchmarks\nHANNA [297]\nStory\nPointwise\n1,056\nRelevance, Coherence, Complexity\nStoryER [298]\nStory\nPairwise\n100K\nUser preference-based ranking\nPKU-SafeRLHF [299]\nValues\nPairwise\n83.4K\nHelpfulness, Harmlessness\nCvalue [300]\nValues\nPairwise\n145K\nSafety, Responsibility\nNaturalInst. [301, 302]\nInstruction Tuning\nPointwise\n1,600+\nInstruction-following evaluation\nels learn human preference optimization through reinforce-\nment learning with human feedback. D4RL [282] and Meta-\nWorld [283] focus on robotic control and offline RL, which\nhave implications for autonomous model decision-making.\nMineRL [284] extends RL testing into complex environments\nsuch as Minecraft-based interactions, useful for training LLMs\nin adaptive decision-making settings.\nMultilingual Evaluation. Multilingual benchmarks are es-\nsential for LLM post-processing in cross-lingual generalization,\ntranslation adaptation, and fine-tuning for low-resource lan-\nguages. CulturaX [285] and PangeaIns [286] evaluate tok-\nenization, translation, and instruction-following in over 150\nlanguages, ensuring fairness and diversity in model outputs.\nTydiQA [287] and MM-Eval [289] target bilingual and task-\noriented multilingual evaluation, enabling improvements in\nLLM fine-tuning. These datasets ensure that LLMs are not just\nEnglish-centric but optimized for multilingual adaptability.\nGeneral Comprehension Benchmarks. General compre-\nhension benchmarks contribute to model fine-tuning, response\ncoherence, and preference optimization. Datasets such as\nChatbot Arena [291], MTBench [291], and RewardBench [167]\ntest user preference modeling and conversational fluency,\ncrucial for LLM response ranking and re-ranking methods.\nBigBench [290] evaluates broad multi-domain comprehension,\nwhile MMLU [277, 278] measures correctness and informa-\ntiveness. These datasets help in refining LLM fluency, factual\ncorrectness, and open-ended response generation.\nDialogue and Search Benchmarks. Dialogue and search\nbenchmarks play a key role in optimizing LLM retrieval-based\nresponses, multi-turn coherence, and information retrieval ac-\ncuracy. Datasets such as ConvAI2 [292] and MultiWOZ [293]\n\n\n20\nevaluate multi-turn conversational models, essential for di-\nalogue history tracking and adaptive response fine-tuning.\nFor search relevance assessment, BEIR [296] provides large-\nscale human-annotated judgments for retrieval fine-tuning,\nensuring LLMs generate and rank responses effectively. TREC\nDL21/22 [294, 295] contributes to document relevance ranking\nand fact retrieval.\n7\nFuture Directions\nWe gathered all papers related to post-training methods in\nLLMs and analyzed their trends, as shown in Figure 7. Advanc-\ning RL techniques [303, 57, 40] for refining the LLMs have\na noticeable increase in prominence since 2020 (Figure 7a),\nemphasizing the demand for interactive approaches such\nas human-in-the-loop [35, 304] reinforcement and scalability\n[111, 82, 305]. At the same time, reward modeling [306,\n166, 167] (Figure 7b) has seen a steady rise in interest due to\nthe emergence of self-rewarding language models, yet the field\nstill struggles with reward hacking [307, 308] and the design\nof robust [309], failure-aware reward functions beyond reward\nhacking [310]. Decoding and search (Figure 7c) methods in-\nclude tree-of-thoughts [84] and Monte Carlo [311, 257] strate-\ngies aiming to enhance model reasoning through iterative\nself-critique [312, 304, 29], but these techniques also demand\nreliable uncertainty estimators to prevent excessive computa-\ntional overhead [313, 111]. Safety [299, 314, 315], robustness\n[316], and interpretability [317, 318, 319] have likewise become\ncentral concerns (Figure 7d), motivating the development of\nbias-aware [320, 321] and uncertainty-aware [322] RL meth-\nods beyond correlation with human uncertanity [323] that\nsafeguard user trust and prevent adversarial attacks. Another\ncrucial area involves personalization [324, 325] and adap-\ntation [193] (Figure 7e), where efforts to tailor LLMs for\nspecific domains must be balanced against risks to privacy\n[326], particularly when enterprise data or sensitive personal\ninformation is involved. In parallel, process [327, 328] vs.\noutcome reward optimization [329] (Figure 7f) remains an\nopen question: while process-based rewards help guide incre-\nmental improvements, outcome-focused metrics are simpler\nbut may not capture crucial intermediate decision-making\nsteps. Beyond reward structure, fine-tuning LLMs on new\ntasks still encounter issues like catastrophic forgetting\n[330] and potential data leakage [331, 332], underscoring\nthe need for parameter-efficient methods [60] and privacy-\npreserving strategies such as differential privacy [333] and\nfederated learning [334]. Human feedback, while central to\nalignment, is inherently costly and limited in scope; methods\nlike Constitutional AI [53] and RLAIF [95] seek to automate\nparts of this oversight, though they introduce fresh concerns\nabout bias calibration [335] and model self-consistency [184].\nFinally, test-time scaling [111] and dynamic reasoning [336]\nframeworks pose further challenges: models must learn when\nto allocate more computation for complex queries, how to\nadapt verification modules [337] efficiently, and how to main-\ntain robust performance even when facing adversarial inputs.\nThese converging research directions—spanning reward mod-\neling, decoding strategies, interpretability, personalization,\nand safe fine-tuning—highlight the multifaceted role of RL in\nLLMs and collectively shape the future trajectory of large-scale\nlanguage model development. Below, we delve into some of\nthese directions in greater detail.\nFine-tuning challenges. Fine-tuning remains one of the\nmost direct post-training methods to adapt LLMs to specific\ntasks or domains, yet it faces several open challenges. One\nfundamental issue is catastrophic forgetting – when updating\nan LLM on new data causes it to lose or degrade previously\nlearned capabilities. Even advanced PEFT methods like LoRA\n[60], which greatly reduce the number of trainable weights,\ndo not fully solve this problem [330]. Future work can ex-\nplore better continual learning strategies and regularization\ntechniques so that models can acquire new skills without\nerasing old ones. For example, new fine-tuning algorithms\n(e.g. CURLoRA [330]) explicitly aim to stabilize training and\npreserve prior knowledge while adding new tasks. Promising\nresearch directions include curriculum-based fine-tuning [338]\n(introducing new facts gradually or in context with known\nfacts) and hybrid training that combines retrieval or external\nknowledge bases. For instance, rather than solely adjusting\nthe model’s weights, one could fine-tune LLMs to consult a\nknowledge repository or perform tool use (such as database\nqueries or computations) when faced with queries outside\ntheir original training distribution [339, 340]. This retrieval-\naugmented fine-tuning [341] could let models incorporate\nfresh information at inference time, reducing the need to over-\nwrite their internal weights with new facts. Another approach\nis training models to explicitly represent uncertainty about\nnew knowledge, thereby enabling them to say ‘I don’t know’\nor defer to an external source if a query concerns content\nnot seen in pre-training. By blending weight updates with\nexternal knowledge integration, future fine-tuned LLMs will\nmaintain higher factual accuracy and lower hallucination rates\non emerging information.\nSafe Fine-tuning. From an ethical and safety perspective,\nfine-tuning raises important open research questions. Fine-\ntuning data often contains sensitive or proprietary informa-\ntion [326], which can lead to privacy risks if the model mem-\norizes and later regurgitates that data. A recent comprehen-\nsive survey [342] highlights vulnerabilities in the fine-tuning\nstage, such as membership inference attacks (detecting if a\nspecific record was in the fine-tuning set) and data extraction\n(recovering parts of the fine-tuning data from the model’s\noutputs). Mitigating these risks is an open problem: methods\nlike differential privacy fine-tuning [333] (adding noise to the\nweight updates) and federated fine-tuning (where data never\nleaves user devices and only aggregated updates are sent to the\nmodel) are being actively explored. However, these methods\noften come at the cost of model utility or require careful\ncalibration to avoid degrading performance.\nLimitations of Human Feedback. Human feedback is\ncostly and subjective. One promising avenue to address the\nlimitations of human feedback is using AI feedback and\nautomation to assist or replace human evaluators. Constitu-\ntional AI [53], introduced by Anthropic, is a notable example:\ninstead of relying on extensive human feedback for every\nharmful or helpful behavior, the model is guided by a set of\nwritten principles (a ‘constitution’) and is trained to critique\nand refine its own responses using another AI model as the\njudge [343]. Emerging directions here include RLAIF [95] and\nother semi-automated feedback techniques [344]: using strong\nmodels to evaluate or guide weaker models, or even having\n\n\n21\n(a) Growing trend in RL for LLMs, with\na focus on Human-in-the-Loop RL.\n(b) Reward modeling trends show RLHF\nstabilization, with Self-Rewarding Models\nleading, but Reward Hacking persists.\n(c)\nDecoding\nstrategies\nlike\nTree-of-\nThoughts and MCTS are improving LLM\nreasoning and decision-making.\n(d) Safety and Robustness research is\ngrowing, with Uncertainty-Aware RL en-\nsuring RLHF model reliability.\n(e) Personalization and Adaptation focus\non Privacy-Preserving RLHF. On-device\nadaptation remains a challenge.\n(f) Process Reward Modeling dominates\nOutcome-Based Optimization, favoring\niterative strategies for RL-based LLMs.\nFig. 7: Yearly Trends in RL specific post-training methods for LLMs and emerging research directions.\nmultiple AI agents debate a question and using their agree-\nment as a reward signal [345, 346]. Such AI-aided feedback\ncould vastly scale the tuning process and help overcome the\nbottleneck of limited human expert time. However, it raises\nnew theoretical questions: how do we ensure the AI judge is\nitself aligned and correct? There is a risk of feedback loops\nor an echo chamber of biases if the automated preferences are\nflawed. An open gap is the creation of robust AI feedback\nsystems that are calibrated to human values (perhaps peri-\nodically ‘grounded’ by human oversight or by a diverse set\nof constitutional principles). The blending of human and AI\nfeedback in a hierarchical scheme could provide a scalable yet\nreliable RL paradigm for LLMs.\nTest-time scaling challenges. Open challenges in TTS re-\nvolve around how to orchestrate the inference-time processes\nefficiently and reliably. A key question is how much computing\nis enough for a given query, and how to determine this on\nthe fly? Using less resources can result in mistakes, but using\ntoo much is inefficient and could introduce inconsistencies.\nRecent research by Snell et al. [83] tackled it by proposing a\nunified framework with a ‘Proposer’ and a ‘Verifier’ to system-\natically explore and evaluate answers. In their framework, the\nProposer (usually the base LLM) generates multiple candidate\nsolutions, and the Verifier (another model or a heuristic)\njudges and selects the best. The optimal strategy can vary by\nproblem difficulty: for easier queries, generating many answers\nin parallel and picking the top might be sufficient, whereas\nfor harder problems, sequential, step-by-step reasoning with\nverification at each step works better. An important future\ndirection is building adaptive systems where the LLM dy-\nnamically allocates computation based on an estimate of the\nquestion’s complexity. This idea connects to meta-cognition in\nAI [314], enabling models to have a sense of what they don’t\nknow or what deserves more thought. Developing reliable\nconfidence metrics or difficulty predictors for LLMs is an open\nresearch area, but progress here would make TTS far more\npractical i.e., the model would only ‘slow down and think’\nwhen necessary, much like a human spending extra time on\na hard problem. Recent study [347] shows distilling test-time\ncomputations into synthetic training data create synergistic\npretraining benefits which can also be further explored.\nReward Modeling and Credit Assignment. Current RL\napproaches suffer from reward misgeneralization, where mod-\nels over-optimize superficial proxy metrics rather than genuine\nreasoning quality. The sparse nature of terminal rewards\nin multi-step tasks increases credit assignment challenges,\nparticularly in long-horizon reasoning scenarios. Traditional\nmethods like DPO require inefficient pairwise preference data\nand fail to utilize failure trajectories effectively. Hybrid reward\nmodels can be investigated by integrating process supervi-\nsion with outcome-based rewards using contrastive stepwise\nevaluation [348]. This approach enables a more granular as-\nsessment of intermediate decision-making steps while aligning\nwith long-term objectives. Recent work [171] suggests step-\nlevel policy optimization could improve value function ac-\ncuracy while maintaining safety constraints. Dynamic credit\nassignment mechanisms can be explored through temporal\ndifference learning adapted for transformers [349, 350]. Such\nadaptations may enhance the model’s ability to capture long-\nrange dependencies and optimize reward propagation over\nextended sequences. Failure-aware training strategies can be\ndeveloped by incorporating negative examples into the RL loop\nvia adversarial data augmentation [351]. This can improve\nmodel robustness by systematically exposing it to challenging\nscenarios and encouraging more resilient policy learning.\nEfficient RL Training and Distillation. Current RL meth-\nods for LLMs require prohibitive computational resources [352]\nwhile often underperforming knowledge distillation tech-\nniques [93]. This inefficiency limits scalability and practical\n\n\n22\ndeployment, as distilled models frequently surpass RL-trained\ncounterparts despite requiring less training overhead. Addi-\ntionally, pure RL approaches struggle to balance language\nquality with reasoning improvement [97, 93], creating a per-\nformance ceiling.\nThe development of hybrid frameworks that initialize RL\npolicies with distilled knowledge from large models, combining\nthe exploratory benefits of RL with the stability of supervised\nlearning is an interesting direction. Similarly, curriculum sam-\npling strategies that progressively increase task complexity\nwhile using distillation to preserve linguistic coherence can\nalso help. PEFT methods [60] can be leveraged during RL up-\ndates to maintain base capabilities while enhancing reasoning.\nIntegration: Combining PRM-guided tree\nsearch with online distillation achieves 4× effi-\nciency gains over baseline methods while main-\ntaining 94% solution accuracy on MATH dataset.\nPrivacy-Preserving Personalization. Customizing mod-\nels for enterprise and individual use cases raises the risk of\nexposing private training data through memorization, making\nprivacy-preserving [326] adaptation essential. Promising so-\nlutions include homomorphic instruction tuning [353], which\nprocesses encrypted user queries while maintaining end-to-end\nencryption during inference; differential privacy via reward\nnoising [354], which introduces mathematically bounded noise\ninto RLHF preference rankings during alignment; and federated\ndistillation, which aggregates knowledge from decentralized\nuser-specific models without sharing raw data.\nCollaborative Multi-Model Systems. As single-model\n[355, 356, 357] scaling approaches physical limits, alterna-\ntive paradigms such as multi-agent LLM collaboration [358,\n359, 178] become necessary. Researchers are investigating\nemergent communication protocols that train models to de-\nvelop lossy compression “languages” for inter-model knowl-\nedge transfer such as GenAINet [360], robust ensembles where\nstress-test induced specialization drives automatic division of\nproblem spaces based on failure analysis [361], and gradient-\nfree synergy learning through evolutionary strategies designed\nto discover complementary model combinations without rely-\ning on backpropagation [362].\nMultimodal RL Integration. Multimodal reinforcement\nlearning [363, 49, 364] faces the obstacle of a combinatorial\nstate explosion, especially in contexts exceeding 128k tokens.\nPioneering methods to overcome this include hierarchical\nattention frameworks that employ modality-specific policies\nwith cross-attention gating [365], adaptive truncation strate-\ngies that compress context while preserving critical reasoning\nsegments [366], and flash curriculum approaches that leverage\nself-supervised [367, 368, 369] complexity prediction to facili-\ntate progressive multimodal integration.\nEfficient RL Training. Efficient RL training paradigms con-\ntinue to be a critical research frontier as current meth-\nods exhibit significant sample inefficiency and computational\noverhead. Addressing issues like the overthinking [370, 371]\nphenomenon, where excessive reasoning chains waste valu-\nable computation [145], requires approaches such as partial\nrollout strategies [372], adaptive length penalty mechanisms\nemploying learned compression transformers, and hybrid ar-\nchitectures that combine MCTS with advanced RL optimizers.\nThese innovations are essential for scaling RL to long-context\ntasks while minimizing wasted computational resources.\n. Overthinking Phenomenon: Analysis reveals\n22% wasted computation in reasoning chains\nexceeding optimal reasoning length.\nRL methods exhibit sample inefficiency and computational\noverhead, particularly when scaling to contexts exceeding\n128k tokens. The ‘overthinking’ phenomenon, where models\ngenerate excessively long reasoning chains, further reduces\ntoken efficiency and increases deployment costs [373]. Inves-\ntigate partial rollout strategies with flash attention mech-\nanisms for long-context processing. Develop length penalty\nmechanisms using learned compression transformers for itera-\ntive long2short distillation. Hybrid architectures combining\nMCTS [74] with GRPO [59] could enable better exploration-\nexploitation tradeoffs. Parallel work by Xie et. al. [74] demon-\nstrates promising results through adaptive tree search prun-\ning. Several open challenges persist in the field. Uncertainty\npropagation remains problematic as current confidence es-\ntimators add approximately 18% latency overhead, while\ncatastrophic forgetting results in a degradation of 29% of base\ncapabilities during RL fine-tuning [374]. Moreover, benchmark\nsaturation is an issue, with MMLU scores correlating poorly (r\n= 0.34) with real-world performance [375].\n. Adversarial Vulnerabilities: Stress tests re-\nveal a high success rate on gradient-based\nprompt injections.\n8\nConclusion\nThis survey and tutorial provides a systematic review of post-\ntraining methodologies for LLMs, focusing on fine-tuning, re-\ninforcement learning, and scaling. We analyze key techniques,\nalong with strategies for improving efficiency and alignment\nwith human preferences. Additionally, we explore the role of\nRL in enhancing LLMs through reasoning, planning, and multi-\ntask generalization, categorizing their functionalities within\nthe agent-environment paradigm. Recent advancements in\nreinforcement learning and test-time scaling have significantly\nimproved LLMs reasoning capabilities, enabling them to tackle\nincreasingly complex tasks. By consolidating the latest re-\nsearch and identifying open challenges, we aim to guide future\nefforts in optimizing LLMs for real-world applications.\nReferences\n[1]\nA. Vaswani, “Attention is all you need,” Advances in Neural\nInformation Processing Systems, 2017. 1\n[2]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al., “Language models are few-shot learners,” Advances in\nneural information processing systems, vol. 33, pp. 1877–1901,\n2020. 1\n[3]\nZ. Yang, “Xlnet: Generalized autoregressive pretraining for lan-\nguage understanding,” arXiv preprint arXiv:1906.08237, 2019.\n1, 3, 18\n[4]\nJ. Devlin, “Bert: Pre-training of deep bidirectional transformers\nfor language understanding,” arXiv preprint arXiv:1810.04805,\n2018. 1, 2, 3\n\n\n23\n[5]\nZ. Lan, “Albert: A lite bert for self-supervised learning of\nlanguage representations,” arXiv preprint arXiv:1909.11942,\n2019. 1\n[6]\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits\nof transfer learning with a unified text-to-text transformer,”\nJournal of machine learning research, vol. 21, no. 140, pp. 1–\n67, 2020. 1\n[7]\nP. Verga, S. Hofstatter, S. Althammer, Y. Su, A. Piktus,\nA. Arkhangorodsky, M. Xu, N. White, and P. Lewis, “Replacing\njudges with juries: Evaluating llm generations with a panel of\ndiverse models,” arXiv preprint arXiv:2404.18796, 2024. 1\n[8]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,\nQ. V. Le, D. Zhou, et al., “Chain-of-thought prompting elicits\nreasoning in large language models,” Advances in neural infor-\nmation processing systems, vol. 35, pp. 24824–24837, 2022. 1,\n2, 3, 11, 12, 15, 18\n[9]\nC. Wang, Y. Deng, Z. Lyu, L. Zeng, J. He, S. Yan, and B. An,\n“Q*: Improving multi-step reasoning for llms with deliberative\nplanning,” arXiv preprint arXiv:2406.14283, 2024. 1\n[10]\nY. Wu, X. Han, W. Song, M. Cheng, and F. Li, “Mindmap:\nConstructing evidence chains for multi-step reasoning in large\nlanguage models,” in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 38, pp. 19270–19278, 2024. 1\n[11]\nY. Ding, Z. Wang, W. Ahmad, H. Ding, M. Tan, N. Jain, M. K.\nRamanathan, R. Nallapati, P. Bhatia, D. Roth, et al., “Cross-\ncodeeval: A diverse and multilingual benchmark for cross-file\ncode completion,” Advances in Neural Information Processing\nSystems, vol. 36, 2024. 1\n[12]\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,\nF. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman,\nS. Anadkat, et al., “Gpt-4 technical report,” arXiv preprint\narXiv:2303.08774, 2023. 1\n[13]\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,\nA. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan,\net\nal.,\n“The\nllama\n3\nherd\nof\nmodels,”\narXiv\npreprint\narXiv:2407.21783, 2024. 1, 5\n[14]\nG. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhu-\npatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ramé,\net al., “Gemma 2: Improving open language models at a practi-\ncal size,” arXiv preprint arXiv:2408.00118, 2024. 1, 5\n[15]\nG. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al., “Gemini:\na family of highly capable multimodal models,” arXiv preprint\narXiv:2312.11805, 2023. 1, 5\n[16]\nA. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr,\nC. Ruan, D. Dai, D. Guo, et al., “Deepseek-v2: A strong,\neconomical, and efficient mixture-of-experts language model,”\narXiv preprint arXiv:2405.04434, 2024. 1, 2, 5\n[17]\nM. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan,\nN. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, et al., “Phi-\n3 technical report: A highly capable language model locally on\nyour phone,” arXiv preprint arXiv:2404.14219, 2024. 1, 5\n[18]\nA. Fan, M. Lewis, and Y. Dauphin, “Hierarchical neural story\ngeneration,” arXiv preprint arXiv:1805.04833, 2018. 1\n[19]\nC. Chhun, P. Colombo, C. Clavel, and F. M. Suchanek, “Of hu-\nman criteria and automatic metrics: A benchmark of the eval-\nuation of story generation,” arXiv preprint arXiv:2208.11646,\n2022. 1\n[20]\nS. Arif, S. Farid, A. H. Azeemi, A. Athar, and A. A. Raza,\n“The fellowship of the llms: Multi-agent workflows for synthetic\npreference optimization dataset generation,” arXiv preprint\narXiv:2408.08688, 2024. 1\n[21]\nS. Ye, Y. Jo, D. Kim, S. Kim, H. Hwang, and M. Seo, “Selfee:\nIterative self-revising llm empowered by self-feedback genera-\ntion,” Blog post, 2023. 1\n[22]\nM. Musolesi, “Creative beam search: Llm-as-a-judge for im-\nproving response generation,” ICCC, 2024. 1\n[23]\nJ. Ren, Y. Zhao, T. Vu, P. J. Liu, and B. Lakshminarayanan,\n“Self-evaluation improves selective generation in large language\nmodels,” in Proceedings on, pp. 49–64, PMLR, 2023. 1\n[24]\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\nN.\nGoyal,\nH.\nKüttler,\nM.\nLewis,\nW.-t.\nYih,\nT.\nRock-\ntäschel, et al., “Retrieval-augmented generation for knowledge-\nintensive nlp tasks,” Advances in Neural Information Process-\ning Systems, vol. 33, pp. 9459–9474, 2020. 1, 2\n[25]\nY. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer,\nW.-t. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural\nand reliable benchmark for data science code generation,” in\nInternational Conference on Machine Learning, pp. 18319–\n18345, PMLR, 2023. 1\n[26]\nB. Zhu, E. Frick, T. Wu, H. Zhu, K. Ganesan, W.-L. Chiang,\nJ. Zhang, and J. Jiao, “Starling-7b: Improving helpfulness\nand harmlessness with rlaif,” in First Conference on Language\nModeling, 2024. 1, 5\n[27]\nD. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut,\nR. West, and B. Faltings, “Refiner: Reasoning feedback on in-\ntermediate representations,” arXiv preprint arXiv:2304.01904,\n2023. 1\n[28]\nY. Xie, K. Kawaguchi, Y. Zhao, J. X. Zhao, M.-Y. Kan, J. He,\nand M. Xie, “Self-evaluation guided beam search for reasoning,”\nAdvances in Neural Information Processing Systems, vol. 36,\n2024. 1\n[29]\nH.\nYe\nand\nH.\nT.\nNg,\n“Self-judge:\nSelective\ninstruction\nfollowing\nwith\nalignment\nself-evaluation,”\narXiv\npreprint\narXiv:2409.00935, 2024. 1, 20\n[30]\nZ. Luo, H. Wu, D. Li, J. Ma, M. Kankanhalli, and J. Li,\n“Videoautoarena: An automated arena for evaluating large\nmultimodal models in video analysis through user simulation,”\narXiv preprint arXiv:2411.13281, 2024. 1\n[31]\nS. Deng, W. Zhao, Y.-J. Li, K. Wan, D. Miranda, A. Kale,\nand Y. Tian, “Efficient self-improvement in multimodal large\nlanguage models: A model-level judge-free approach,” arXiv\npreprint arXiv:2411.17760, 2024. 1, 2\n[32]\nT. Xiong, X. Wang, D. Guo, Q. Ye, H. Fan, Q. Gu, H. Huang,\nand C. Li, “Llava-critic: Learning to evaluate multimodal mod-\nels,” arXiv preprint arXiv:2410.02712, 2024. 1\n[33]\nD. Chen, R. Chen, S. Zhang, Y. Liu, Y. Wang, H. Zhou,\nQ. Zhang, Y. Wan, P. Zhou, and L. Sun, “Mllm-as-a-judge: As-\nsessing multimodal llm-as-a-judge with vision-language bench-\nmark,” arXiv preprint arXiv:2402.04788, 2024. 1\n[34]\nT. Hagendorff, S. Fabi, and M. Kosinski, “Human-like intu-\nitive behavior and reasoning biases emerged in large language\nmodels but disappeared in chatgpt,” Nature Computational\nScience, vol. 3, no. 10, pp. 833–838, 2023. 1\n[35]\nQ. Pan, Z. Ashktorab, M. Desmond, M. S. Cooper, J. John-\nson, R. Nair, E. Daly, and W. Geyer, “Human-centered\ndesign recommendations for llm-as-a-judge,” arXiv preprint\narXiv:2407.03479, 2024. 1, 20\n[36]\nG. H. Chen, S. Chen, Z. Liu, F. Jiang, and B. Wang, “Humans\nor llms as the judge? a study on judgement biases,” arXiv\npreprint arXiv:2402.10669, 2024. 1\n[37]\nA.\nNewell,\n“Human\nproblem\nsolving,”\nUpper\nSaddle\nRiver/Prentive Hall, 1972. 1, 15\n[38]\nX. Wang, H. Kim, S. Rahman, K. Mitra, and Z. Miao, “Human-\nllm collaborative annotation through effective verification of\nllm labels,” in Proceedings of the CHI Conference on Human\nFactors in Computing Systems, pp. 1–21, 2024. 1\n[39]\nR. OpenAI, “Gpt-4 technical report. arxiv 2303.08774,” View\nin Article, vol. 2, no. 5, 2023. 1, 5, 12\n[40]\nD. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu,\nS. Ma, P. Wang, X. Bi, et al., “DeepSeek-R1: Incentivizing\nreasoning capability in llms via reinforcement learning,” arXiv\npreprint arXiv:2501.12948, 2025. 1, 5, 11, 12, 20\n[41]\nM. T. Hicks, J. Humphries, and J. Slater, “Chatgpt is bullshit,”\nEthics and Information Technology, vol. 26, no. 2, pp. 1–10,\n2024. 1\n[42]\nN. Maleki, B. Padmanabhan, and K. Dutta, “Ai hallucinations:\nA misnomer worth clarifying,” 2024. 1\n[43]\nA. Bruno, P. L. Mazzeo, A. Chetouani, M. Tliba, and M. A.\nKerkouri, “Insights into classifying and mitigating llms’ hallu-\ncinations,” arXiv preprint arXiv:2311.08117, 2023. 1\n[44]\nS. Farquhar, J. Kossen, L. Kuhn, and Y. Gal, “Detecting hal-\nlucinations in large language models using semantic entropy,”\nNature, vol. 630, no. 8017, pp. 625–630, 2024. 1\n[45]\nF. Leiser, S. Eckhardt, V. Leuthe, M. Knaeble, A. Maedche,\nG. Schwabe, and A. Sunyaev, “Hill: A hallucination identifier\nfor large language models,” in Proceedings of the CHI Confer-\nence on Human Factors in Computing Systems, pp. 1–13, 2024.\n1\n[46]\nA. Gunjal, J. Yin, and E. Bas, “Detecting and preventing hallu-\ncinations in large vision language models,” in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 38, pp. 18135–\n18143, 2024. 1\n\n\n24\n[47]\nS. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang,\nand Z. Hu, “Reasoning with language model is planning with\nworld model,” in Proceedings of the 2023 Conference on Empir-\nical Methods in Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023 (H. Bouamor, J. Pino, and\nK. Bali, eds.), pp. 8154–8173, Association for Computational\nLinguistics, 2023. 1\n[48]\nY. Ye, Z. Huang, Y. Xiao, E. Chern, S. Xia, and P. Liu, “Limo:\nLess is more for reasoning,” 2025. 1, 3\n[49]\nC. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vulić,\nand F. Wei, “Imagine while reasoning in space: Multimodal\nvisualization-of-thought,” 2025. 1, 3, 22\n[50]\nT. Xia, B. Yu, Y. Wu, Y. Chang, and C. Zhou, “Language\nmodels can evaluate themselves via probability discrepancy,”\narXiv preprint arXiv:2405.10516, 2024. 1\n[51]\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii,\nY. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination\nin natural language generation,” ACM Computing Surveys,\nvol. 55, no. 12, pp. 1–38, 2023. 1, 3\n[52]\nH. He and W. J. Su, “A law of next-token prediction in large\nlanguage models,” 2024. 2\n[53]\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,\nA. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al., “Con-\nstitutional ai: Harmlessness from ai feedback,” arXiv preprint\narXiv:2212.08073, 2022. 2, 20\n[54]\ninterconnects.ai, “blob reinforcement fine-tuning.” (Accessed:\n2025-12-6). 2\n[55]\nE. Lobo, C. Agarwal, and H. Lakkaraju, “On the impact\nof fine-tuning on chain-of-thought reasoning,” arXiv preprint\narXiv:2411.15382, 2024. 2, 3\n[56]\nL. Trung, X. Zhang, Z. Jie, P. Sun, X. Jin, and H. Li, “Reft:\nReasoning with reinforced fine-tuning,” in Proceedings of the\n62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 7601–7614, 2024. 2\n[57]\nR. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,\nand C. Finn, “Direct preference optimization: Your language\nmodel is secretly a reward model,” Advances in Neural Infor-\nmation Processing Systems, vol. 36, 2024. 2, 3, 5, 20\n[58]\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al.,\n“Training language models to follow instructions with human\nfeedback,” Advances in neural information processing systems,\nvol. 35, pp. 27730–27744, 2022. 2, 3, 5, 8, 9, 13, 14\n[59]\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang,\nM. Zhang, Y. Li, Y. Wu, et al., “Deepseekmath: Pushing the\nlimits of mathematical reasoning in open language models,”\narXiv preprint arXiv:2402.03300, 2024. 2, 3, 5, 8, 9, 10, 11,\n22\n[60]\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,\nL. Wang, and W. Chen, “Lora: Low-rank adaptation of large\nlanguage models,” arXiv preprint arXiv:2106.09685, 2021. 2,\n13, 20, 22\n[61]\nA. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-\nrag: Learning to retrieve, generate, and critique through self-\nreflection,” arXiv preprint arXiv:2310.11511, 2023. 2\n[62]\nY. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai,\nJ. Sun, M. Wang, and H. Wang, “Retrieval-augmented gen-\neration for large language models: A survey,” arXiv preprint\narXiv:2312.10997, 2023. 2\n[63]\nZ. Hu, L. Song, J. Zhang, Z. Xiao, J. Wang, Z. Chen, and\nH. Xiong, “Rethinking llm-based preference evaluation,” arXiv\npreprint arXiv:2407.01085, 2024. 2\n[64]\nS. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou,\nY. Xiao, S. Yun, X. Huang, et al., “Disc-lawllm: Fine-tuning\nlarge language models for intelligent legal services,” arXiv\npreprint arXiv:2309.11325, 2023. 2\n[65]\nY. Luo, Z. Yang, F. Meng, Y. Li, J. Zhou, and Y. Zhang,\n“An empirical study of catastrophic forgetting in large lan-\nguage models during continual fine-tuning,” arXiv preprint\narXiv:2308.08747, 2023. 2\n[66]\nH. Li, J. Chen, W. Su, Q. Ai, and Y. Liu, “Towards better web\nsearch performance: Pre-training, fine-tuning and learning to\nrank,” 2023. 2\n[67]\nOpenAI, “Reinforcement fine-tuning.” (Accessed: 2025-12-6). 2\n[68]\nW. Zhang, Y. Deng, B. Liu, S. J. Pan, and L. Bing, “Sentiment\nanalysis in the era of large language models: A reality check,”\narXiv preprint arXiv:2305.15005, 2023. 2\n[69]\nA. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,\nand C. Potts, “Learning word vectors for sentiment analysis,”\nin Proceedings of the 49th annual meeting of the association\nfor computational linguistics: Human language technologies,\npp. 142–150, 2011. 2\n[70]\nY. Wang, S. Wang, Y. Li, and D. Dou, “Recognizing medical\nsearch query intent by few-shot learning,” in Proceedings of the\n45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pp. 502–512, 2022. 2\n[71]\nR. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, and T.-Y.\nLiu, “Biogpt: generative pre-trained transformer for biomedi-\ncal text generation and mining,” Briefings in bioinformatics,\nvol. 23, no. 6, p. bbac409, 2022. 2, 12\n[72]\nO. Wysocki, M. Wysocka, D. Carvalho, A. T. Bogatu, D. M.\nGusicuma, M. Delmas, H. Unsworth, and A. Freitas, “An llm-\nbased knowledge synthesis and scientific reasoning framework\nfor biomedical discovery,” arXiv preprint arXiv:2406.18626,\n2024. 2\n[73]\nJ.\nSchulman,\nF.\nWolski,\nP.\nDhariwal,\nA.\nRadford,\nand\nO. Klimov, “Proximal policy optimization algorithms,” arXiv\npreprint arXiv:1707.06347, 2017. 2, 5, 8, 9\n[74]\nY. Xie, A. Goyal, W. Zheng, M.-Y. Kan, T. P. Lillicrap,\nK. Kawaguchi, and M. Shieh, “Monte carlo tree search boosts\nreasoning via iterative preference learning,” arXiv preprint\narXiv:2405.00451, 2024. 2, 18, 22\n[75]\nM. Shanahan, K. McDonell, and L. Reynolds, “Role play with\nlarge language models,” Nature, vol. 623, no. 7987, pp. 493–498,\n2023. 2\n[76]\nT. Xu, E. Helenowski, K. A. Sankararaman, D. Jin, K. Peng,\nE. Han, S. Nie, C. Zhu, H. Zhang, W. Zhou, et al., “The perfect\nblend: Redefining rlhf with mixture of judges,” arXiv preprint\narXiv:2409.20370, 2024. 2\n[77]\nM. Cao, L. Shu, L. Yu, Y. Zhu, N. Wichers, Y. Liu, and\nL. Meng, “Beyond sparse rewards: Enhancing reinforcement\nlearning with language model critique in text generation,” 2024.\n2\n[78]\nY. Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba,\nC. Guestrin, P. S. Liang, and T. B. Hashimoto, “Alpacafarm: A\nsimulation framework for methods that learn from human feed-\nback,” Advances in Neural Information Processing Systems,\nvol. 36, 2024. 2, 14\n[79]\nL. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello,\nA. Zipori, H. Noga, O. Keller, B. Piot, I. Szpektor, et al.,\n“Multi-turn reinforcement learning from preference human\nfeedback,” arXiv preprint arXiv:2405.14655, 2024. 2\n[80]\nZ. Li, Y. He, L. He, J. Wang, T. Shi, B. Lei, Y. Li,\nand Q. Chen, “Falcon: Feedback-driven adaptive long/short-\nterm memory reinforced coding optimization system,” arXiv\npreprint arXiv:2410.21349, 2024. 2\n[81]\nR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., “Webgpt:\nBrowser-assisted question-answering with human feedback,”\narXiv preprint arXiv:2112.09332, 2021. 2, 11, 14\n[82]\nL. Gao, J. Schulman, and J. Hilton, “Scaling laws for reward\nmodel overoptimization,” in International Conference on Ma-\nchine Learning, pp. 10835–10866, PMLR, 2023. 2, 14, 20\n[83]\nC. Snell, J. Lee, K. Xu, and A. Kumar, “Scaling llm test-time\ncompute optimally can be more effective than scaling model\nparameters,” arXiv preprint arXiv:2408.03314, 2024. 2, 14, 21\n[84]\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving\nwith large language models,” Advances in Neural Information\nProcessing Systems, vol. 36, 2024. 2, 14, 15, 20\n[85]\nJ. Jiang, D. He, and J. Allan, “Searching, browsing, and clicking\nin a search session: Changes in user behavior by task and over\ntime,” in Proceedings of the 37th international ACM SIGIR\nconference on Research & development in information retrieval,\npp. 607–616, 2014. 2\n[86]\nY. Xie, K. Kawaguchi, Y. Zhao, J. X. Zhao, M.-Y. Kan, J. He,\nand M. Xie, “Self-evaluation guided beam search for reasoning,”\nAdvances in Neural Information Processing Systems, vol. 36,\n2024. 2\n[87]\nY. Tian, B. Peng, L. Song, L. Jin, D. Yu, H. Mi, and D. Yu,\n“Toward self-improvement of llms via imagination, searching,\nand criticizing,” arXiv preprint arXiv:2404.12253, 2024. 2, 18\n[88]\nK. Gandhi, D. H. J. Lee, G. Grand, M. Liu, W. Cheng,\nA. Sharma, and N. Goodman, “Stream of search (SoS): Learn-\n\n\n25\ning to search in language,” in First Conference on Language\nModeling, 2024. 2\n[89]\nK. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu,\nS. Godil, R. Prenger, and A. Anandkumar, “Leandojo: Theo-\nrem proving with retrieval-augmented language models,” 2023.\n2\n[90]\nC. Sun, S. Huang, and D. Pompili, “Retrieval-augmented hier-\narchical in-context reinforcement learning and hindsight mod-\nular reflections for task planning with llms,” 2024. 2\n[91]\nE. Davis, “Testing gpt-4-o1-preview on math and science prob-\nlems: A follow-up study,” arXiv preprint arXiv:2410.22340,\n2024. 2\n[92]\nJ. Y. Wang, N. Sukiennik, T. Li, W. Su, Q. Hao, J. Xu,\nZ. Huang, F. Xu, and Y. Li, “A survey on human-centric llms,”\narXiv preprint arXiv:2411.14491, 2024. 3\n[93]\nJ. Wu, S. Yang, R. Zhan, Y. Yuan, L. S. Chao, and D. F. Wong,\n“A survey on llm-generated text detection: Necessity, methods,\nand future directions,” Computational Linguistics, pp. 1–65,\n2025. 3, 14, 21, 22\n[94]\nY. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen,\nX. Yi, C. Wang, Y. Wang, et al., “A survey on evaluation\nof large language models,” ACM Transactions on Intelligent\nSystems and Technology, vol. 15, no. 3, pp. 1–45, 2024. 3\n[95]\nH. Lee, S. Phatale, H. Mansoor, K. R. Lu, T. Mesnard, J. Ferret,\nC. Bishop, E. Hall, V. Carbune, and A. Rastogi, “Rlaif: Scaling\nreinforcement learning from human feedback with ai feedback,”\n2023. 3, 9, 20\n[96]\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\nJ. Xu, and Z. Sui, “A survey on in-context learning,” arXiv\npreprint arXiv:2301.00234, 2022. 3\n[97]\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Dong, et al., “A survey of large language\nmodels,” arXiv preprint arXiv:2303.18223, 2023. 3, 22\n[98]\nS. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, W. Zhou,\nJ. Coady, D. Peng, Y. Qiao, L. Benson, L. Sun, A. Wardle-\nSolano, H. Szabo, E. Zubova, M. Burtell, J. Fan, Y. Liu,\nB. Wong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang,\nA. R. Fabbri, W. Kryscinski, S. Yavuz, Y. Liu, X. V. Lin,\nS. Joty, Y. Zhou, C. Xiong, R. Ying, A. Cohan, and D. Radev,\n“Folio: Natural language reasoning with first-order logic,” 2024.\n3\n[99]\nZ. Xi, S. Jin, Y. Zhou, R. Zheng, S. Gao, T. Gui, Q. Zhang, and\nX. Huang, “Self-polish: Enhance reasoning in large language\nmodels via problem refinement,” 2024. 3\n[100] A. Saparov and H. He, “Language models are greedy reasoners:\nA systematic formal analysis of chain-of-thought,” 2023. 3\n[101] J. Liu, C. Wang, C. Y. Liu, L. Zeng, R. Yan, Y. Sun, Y. Liu,\nand Y. Zhou, “Improving multi-step reasoning abilities of large\nlanguage models with direct advantage policy optimization,”\narXiv preprint arXiv:2412.18279, 2024. 3\n[102] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa,\n“Large language models are zero-shot reasoners,” Advances\nin neural information processing systems, vol. 35, pp. 22199–\n22213, 2022. 3\n[103] X. Deng, Y. Su, A. Lees, Y. Wu, C. Yu, and H. Sun, “Rea-\nsonbert: Pre-trained to reason with distant supervision,” arXiv\npreprint arXiv:2109.04912, 2021. 3\n[104] T. Sawada, D. Paleka, A. Havrilla, P. Tadepalli, P. Vidas,\nA. Kranias, J. J. Nay, K. Gupta, and A. Komatsuzaki, “Arb:\nAdvanced reasoning benchmark for large language models,”\n2023. 3\n[105] A. Radford, “Improving language understanding by generative\npre-training,” 2018. 3\n[106] I. J. Myung, “Tutorial on maximum likelihood estimation,”\nJournal of mathematical Psychology, vol. 47, no. 1, pp. 90–100,\n2003. 3\n[107] Y. Shao, J. Mao, Y. Liu, W. Ma, K. Satoh, M. Zhang, and\nS. Ma, “Bert-pli: Modeling paragraph-level interactions for\nlegal case retrieval.,” in IJCAI, pp. 3501–3507, 2020. 3\n[108] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,\nD. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al.,\n“Emergent abilities of large language models,” arXiv preprint\narXiv:2206.07682, 2022. 3\n[109] R. Bellman, “A markovian decision process,” Journal of math-\nematics and mechanics, pp. 679–684, 1957. 3, 10\n[110] S. Hao, Y. Gu, H. Luo, T. Liu, X. Shao, X. Wang, S. Xie, H. Ma,\nA. Samavedhi, Q. Gao, Z. Wang, and Z. Hu, “Llm reasoners:\nNew evaluation, library, and analysis of step-by-step reasoning\nwith large language models,” 2024. 3\n[111] J. Geiping, S. McLeish, N. Jain, J. Kirchenbauer, S. Singh, B. R.\nBartoldson, B. Kailkhura, A. Bhatele, and T. Goldstein, “Scal-\ning up test-time compute with latent reasoning: A recurrent\ndepth approach,” 2025. 3, 20\n[112] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker,\nT. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe,\n“Let’s verify step by step,” arXiv preprint arXiv:2305.20050,\n2023. 3\n[113] G. Chen, M. Liao, C. Li, and K. Fan, “Step-level value prefer-\nence optimization for mathematical reasoning,” arXiv preprint\narXiv:2406.10858, 2024. 3\n[114] K. Nguyen, H. Daumé III, and J. Boyd-Graber, “Reinforcement\nlearning for bandit neural machine translation with simulated\nhuman feedback,” arXiv preprint arXiv:1707.07402, 2017. 4, 5\n[115] G. Williams, “Substantive and adjectival law,” in Learning the\nLaw, pp. 19–23, Law Book Company, Limited, 1982. 4\n[116] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence\nlevel training with recurrent neural networks,” 2016. 4, 5\n[117] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel,\n“Self-critical sequence training for image captioning,” in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 7008–7024, 2017. 4, 5\n[118] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a\nmethod for automatic evaluation of machine translation,” in\nProceedings of the 40th annual meeting of the Association for\nComputational Linguistics, pp. 311–318, 2002. 4\n[119] R. Vedantam, C. L. Zitnick, and D. Parikh, “Cider: Consensus-\nbased image description evaluation,” 2015. 4\n[120] OpenAI, “Openai gpt-4.5 system card,” 2025. Accessed: 2025-\n02-28. 5\n[121] Anthropic, “Claude 3.7 sonnet,” 2025. Accessed: 2025-02-26. 5,\n19\n[122] R. Team, A. Ormazabal, C. Zheng, C. d. M. d’Autume, D. Yo-\ngatama, D. Fu, D. Ong, E. Chen, E. Lamprecht, H. Pham, et al.,\n“Reka core, flash, and edge: A series of powerful multimodal\nlanguage models,” arXiv preprint arXiv:2404.12387, 2024. 5\n[123] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya,\nA. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen,\net al., “Nemotron-4 340b technical report,” arXiv preprint\narXiv:2406.11704, 2024. 5\n[124] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Co-\njocaru, M. Debbah, Étienne Goffinet, D. Hesslow, J. Lau-\nnay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, and\nG. Penedo, “The falcon series of open language models,” 2023.\n5\n[125] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu,\nJ. Zhang, B. Yu, K. Lu, et al., “Qwen2. 5-coder technical\nreport,” arXiv preprint arXiv:2409.12186, 2024. 5\n[126] A. Défossez, L. Mazaré, M. Orsini, A. Royer, P. Pérez, H. Jégou,\nE. Grave, and N. Zeghidour, “Moshi: a speech-text foundation\nmodel for real-time dialogue,” tech. rep., 2024. 5\n[127] Nexusflow, “Athene: An rlhf-enhanced language model,” 2024.\nAccessed: 2025-02-26. 5\n[128] R. Teknium, J. Quesnelle, and C. Guang, “Hermes 3 technical\nreport,” arXiv preprint arXiv:2408.11857, 2024. 5\n[129] Z. AI, “Zed,” 2025. 500B, Zed AI, RLHF, Multi-modal, Open.\n5\n[130] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Pas-\nsos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., “Palm 2\ntechnical report,” arXiv preprint arXiv:2305.10403, 2023. 5\n[131] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen,\nZ. Chen, Z. Chen, P. Chu, et al., “Internlm2 technical report,”\narXiv preprint arXiv:2403.17297, 2024. 5\n[132] S. Labs, “Supernova,” 2025.\n220B, Supernova Labs, RLHF,\nMulti-modal, Open. 5\n[133] xAI, “Grok-3: The next generation ai model by xai,” tech. rep.,\nxAI, 2025. Accessed: 2025-02-24. 5\n[134] P. Agrawal, S. Antoniak, et al., “Pixtral 12b,” 2024. 5\n[135] MiniMax, A. Li, et al., “Minimax-01: Scaling foundation models\nwith lightning attention,” 2025. 5\n[136] A. A. G. Intelligence, “The amazon nova family of models:\nTechnical report and model card,” Amazon Technical Reports,\n2024. 5\n[137] Fujitsu and T. I. of Technology, “Fugaku-llm: The largest cpu-\nonly trained language model,” Fujitsu Research Press Release,\n\n\n26\nMay 2024. 5\n[138] R. AI, “Nova: A family of ai models by rubik’s ai,” Rubik’s AI\nResearch, October 2024. 5\n[139] OpenAI, “Openai o3 system card,” technical report, OpenAI,\n2025. 5\n[140] M. R. Team et al., “Introducing dbrx: a new state-of-the-art\nopen llm,” Mosaic AI Research, 2024. 5\n[141] A. Köpf, Y. Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam,\nK. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi,\nS. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire,\nC. Schuhmann, H. Nguyen, and A. Mattick, “Openassistant\nconversations – democratizing large language model align-\nment,” 2023. 5\n[142] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang,\nD. Rojas, G. Feng, H. Zhao, et al., “Chatglm: A family of\nlarge language models from glm-130b to glm-4 all tools,” arXiv\npreprint arXiv:2406.12793, 2024. 5\n[143] A. Bartolome, J. Hong, N. Lee, K. Rasul, and L. Tunstall,\n“Zephyr 141b a39b,” 2024. 5\n[144] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1:\nTechnical details and evaluation,” White Paper. AI21 Labs,\nvol. 1, no. 9, pp. 1–17, 2021. 5\n[145] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li,\nC. Xiao, C. Du, C. Liao, et al., “Kimi k1. 5: Scaling reinforce-\nment learning with llms,” arXiv preprint arXiv:2501.12599,\n2025. 5, 22\n[146] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gu-\nnasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauff-\nmann, J. R. Lee, Y. T. Lee, Y. Li, W. Liu, C. C. T. Mendes,\nA. Nguyen, E. Price, G. de Rosa, O. Saarikivi, A. Salim,\nS. Shah, X. Wang, R. Ward, Y. Wu, D. Yu, C. Zhang, and\nY. Zhang, “Phi-4 technical report,” 2024. 5\n[147] C. Team, “Chameleon: Mixed-modal early-fusion foundation\nmodels,” arXiv preprint arXiv:2405.09818, 2024. 5\n[148] N. Dey, G. Gosal, Zhiming, Chen, H. Khachane, W. Marshall,\nR. Pathria, M. Tom, and J. Hestness, “Cerebras-gpt: Open\ncompute-optimal language models trained on the cerebras\nwafer-scale cluster,” 2023. 5\n[149] S.\nWu,\nO.\nIrsoy,\nS.\nLu,\nV.\nDabravolski,\nM.\nDredze,\nS. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann,\n“Bloomberggpt: A large language model for finance,” 2023. 5\n[150] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl,\nA. Clark, T. Hennigan, E. Noland, K. Millican, G. van den\nDriessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan,\nE. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, “Training\ncompute-optimal large language models,” 2022. 5\n[151] S. Shen, Y. Cheng, Z. He, W. He, H. Wu, M. Sun, and Y. Liu,\n“Minimum risk training for neural machine translation,” 2016.\n4, 5\n[152] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” Advances\nin neural information processing systems, vol. 12, 1999. 4\n[153] S. Bhatnagar, M. Ghavamzadeh, M. Lee, and R. S. Sutton, “In-\ncremental natural actor-critic algorithms,” Advances in neural\ninformation processing systems, vol. 20, 2007. 4\n[154] V. Mnih, “Asynchronous methods for deep reinforcement learn-\ning,” arXiv preprint arXiv:1602.01783, 2016. 4\n[155] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz,\n“Reinforcement\nlearning\nthrough\nasynchronous\nadvantage\nactor-critic on a gpu,” arXiv preprint arXiv:1611.06256, 2016.\n4\n[156] S. M. Kakade, “A natural policy gradient,” Advances in neural\ninformation processing systems, vol. 14, 2001. 4\n[157] A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike\nadaptive elements that can solve difficult learning control prob-\nlems,” IEEE transactions on systems, man, and cybernetics,\nno. 5, pp. 834–846, 1983. 4\n[158] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence\ngenerative adversarial nets with policy gradient,” 2017. 5\n[159] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li,\nD. Liu, F. Huang, H. Wei, et al., “Qwen2. 5 technical report,”\narXiv preprint arXiv:2412.15115, 2024. 5\n[160] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel,\n“Trust region policy optimization,” 2017. 5, 8, 9\n[161] N. Vieillard, T. Kozuno, B. Scherrer, O. Pietquin, R. Munos,\nand M. Geist, “Leverage the average: an analysis of kl regu-\nlarization in reinforcement learning,” Advances in Neural In-\nformation Processing Systems, vol. 33, pp. 12163–12174, 2020.\n5\n[162] S. Guo, B. Zhang, T. Liu, T. Liu, M. Khalman, F. Llinares,\nA. Rame, T. Mesnard, Y. Zhao, B. Piot, et al., “Direct lan-\nguage model alignment from online ai feedback,” arXiv preprint\narXiv:2402.04792, 2024. 5, 9\n[163] C. An, M. Zhong, Z. Wu, Q. Zhu, X. Huang, and X. Qiu,\n“Colo: A contrastive learning based re-ranking framework for\none-stage summarization,” arXiv preprint arXiv:2209.14569,\n2022. 5\n[164] R. A. Bradley and M. E. Terry, “Rank analysis of incom-\nplete block designs: I. the method of paired comparisons,”\nBiometrika, vol. 39, no. 3/4, pp. 324–345, 1952. 6\n[165] R. L. Plackett, “The analysis of permutations,” Journal of the\nRoyal Statistical Society Series C: Applied Statistics, vol. 24,\nno. 2, pp. 193–202, 1975. 6\n[166] A. Setlur, C. Nagpal, A. Fisch, X. Geng, J. Eisenstein, R. Agar-\nwal, A. Agarwal, J. Berant, and A. Kumar, “Rewarding\nprogress: Scaling automated process verifiers for llm reasoning,”\narXiv preprint arXiv:2410.08146, 2024. 6, 20\n[167] N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin,\nK. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al., “Re-\nwardbench: Evaluating reward models for language modeling,”\narXiv preprint arXiv:2403.13787, 2024. 6, 19, 20\n[168] J. Hong, N. Lee, and J. Thorne, “Orpo: Monolithic preference\noptimization without reference model,” 2024. 8\n[169] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel,\n“High-dimensional continuous control using generalized advan-\ntage estimation,” 2018. 8\n[170] R. Rafailov, Y. Chittepu, R. Park, H. S. Sikchi, J. Hejna,\nB. Knox, C. Finn, and S. Niekum, “Scaling laws for re-\nward model overoptimization in direct alignment algorithms,”\nArXiv, vol. abs/2406.02900, 2024. 9\n[171] H. Wang, S. Hao, H. Dong, S. Zhang, Y. Bao, Z. Yang, and\nY. Wu, “Offline reinforcement learning for llm multi-step rea-\nsoning,” 2024. 10, 21\n[172] G. Mukobi, P. Chatain, S. Fong, R. Windesheim, G. Kutyniok,\nK. Bhatia, and S. Alberti, “Superhf: Supervised iterative learn-\ning from human feedback,” arXiv preprint arXiv:2310.16763,\n2023. 10\n[173] C. Li, H. Zhou, G. Glavaš, A. Korhonen, and I. Vulić,\n“On task performance and model calibration with super-\nvised and self-ensembled in-context learning,” arXiv preprint\narXiv:2312.13772, 2023. 10\n[174] C. Wang, Z. Zhao, C. Zhu, K. A. Sankararaman, M. Valko,\nX. Cao, Z. Chen, M. Khabsa, Y. Chen, H. Ma, and S. Wang,\n“Preference optimization with multi-sample comparisons,”\n2024. 11\n[175] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W.\nChung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, , and\nJ. Wei, “Challenging big-bench tasks and whether chain-of-\nthought can solve them,” arXiv preprint arXiv:2210.09261,\n2022. 11, 19\n[176] X. Zhang, C. Du, T. Pang, Q. Liu, W. Gao, and M. Lin,\n“Chain of preference optimization: Improving chain-of-thought\nreasoning in LLMs,” in The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024. 11\n[177] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of\nthought prompting in large language models,” arXiv preprint\narXiv:2210.03493, 2022. 11\n[178] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika,\nZ. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al.,\n“Multitask prompted training enables zero-shot task general-\nization,” arXiv preprint arXiv:2110.08207, 2021. 12, 22\n[179] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester,\nN. Du, A. M. Dai, and Q. V. Le, “Finetuned language models\nare zero-shot learners,” arXiv preprint arXiv:2109.01652, 2021.\n12\n[180] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,\nP.\nLiang,\nand\nT.\nB.\nHashimoto,\n“Stanford\nalpaca:\nAn\ninstruction-following llama model,” 2023. 12\n[181] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,\nL. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and\nE. P. Xing, “Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality,” March 2023. 12\n[182] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah,\nA. Ghodsi, P. Wendell, M. Zaharia, and R. Xin, “Free dolly:\n\n\n27\nIntroducing the world’s first truly open instruction-tuned llm,”\n2023. 12\n[183] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul-\nshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du,\net al., “Lamda: Language models for dialog applications,” arXiv\npreprint arXiv:2201.08239, 2022. 12\n[184] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi,\nS. Narang, A. Chowdhery, and D. Zhou, “Self-consistency\nimproves chain of thought reasoning in language models,” in\nThe Eleventh International Conference on Learning Represen-\ntations, 2023. 12, 15, 20\n[185] L. C. Magister, J. Mallinson, J. Adamek, E. Malmi, and\nA. Severyn, “Teaching small language models to reason,” arXiv\npreprint arXiv:2212.08410, 2022. 12\n[186] G. Xu, P. Jin, H. Li, Y. Song, L. Sun, and L. Yuan, “Llava-cot:\nLet vision language models reason step-by-step,” 2024. 12\n[187] O. Thawakar, D. Dissanayake, K. More, R. Thawkar, A. Heakl,\nN. Ahsan, Y. Li, M. Zumri, J. Lahoud, R. M. Anwer,\nH. Cholakkal, I. Laptev, M. Shah, F. S. Khan, and S. Khan,\n“Llamav-o1: Rethinking step-by-step visual reasoning in llms,”\n2025. 12, 19\n[188] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer,\n“Qlora: Efficient finetuning of quantized llms,” arXiv preprint\narXiv:2305.14314, 2023. 13\n[189] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “GPTQ:\nAccurate post-training compression for generative pretrained\ntransformers,” arXiv preprint arXiv:2210.17323, 2022. 13\n[190] E. Frantar and D. Alistarh, “SparseGPT: Massive language\nmodels can be accurately pruned in one-shot,” arXiv preprint\narXiv:2301.00774, 2023. 13\n[191] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul,\nand B. Bossan, “Peft: State-of-the-art parameter-efficient fine-\ntuning methods,” 2022. 13\n[192] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer,\n“Llm.int8(): 8-bit matrix multiplication for transformers at\nscale,” arXiv preprint arXiv:2208.07339, 2022. 13\n[193] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen,\nand T. Zhao, “Adaptive budget allocation for parameter-\nefficient fine-tuning,” in The Eleventh International Conference\non Learning Representations, 2023. 13, 20\n[194] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, “P-tuning v2:\nPrompt tuning can be comparable to fine-tuning universally\nacross scales and tasks,” CoRR, vol. abs/2110.07602, 2021. 13\n[195] Q. Lhoest, A. Villanova del Moral, Y. Jernite, A. Thakur, P. von\nPlaten, S. Patil, J. Chaumond, M. Drame, J. Plu, L. Tunstall,\nJ. Davison, M. Šaško, G. Chhablani, B. Malik, S. Brandeis,\nT. Le Scao, V. Sanh, C. Xu, N. Patry, A. McMillan-Major,\nP. Schmid, S. Gugger, C. Delangue, T. Matussière, L. Debut,\nS. Bekman, P. Cistac, T. Goehringer, V. Mustar, F. Lagunas,\nA. Rush, and T. Wolf, “Datasets: A community library for natu-\nral language processing,” in Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing: System\nDemonstrations, (Online and Punta Cana, Dominican Repub-\nlic), pp. 175–184, Association for Computational Linguistics,\nNov. 2021. 13\n[196] A. Torralba and Others, “Webdataset: A format for petascale\ndeep learning.” Efficient tar-based sharding format for petascale\ndistributed training. 13\n[197] I. Iterative, “Dvc: Data version control.” Git-like version control\nfor datasets and machine learning pipelines. 13\n[198] N.\nRichardson,\nI.\nCook,\nN.\nCrane,\nD.\nDunnington,\nR. François, J. Keane, D. Moldovan-Grünfeld, J. Ooms,\nJ.\nWujciak-Jens,\nand\nApache\nArrow,\narrow:\nIntegration\nto\n’Apache’\n’Arrow’,\n2025.\nR\npackage\nversion\n19.0.0,\nhttps://arrow.apache.org/docs/r/. 13\n[199] I. Facebook, “Zstandard: High-speed compression algorithm.”\nHigh-speed compression algorithm for training data storage/-\ntransfer. 13\n[200] C. Team, “Cleanlab: The standard data-centric ai package for\nmachine learning with noisy labels.”\nAutomatic detection of\nlabel errors and outliers in training datasets. 13\n[201] R. Y. Aminabadi, S. Rajbhandari, M. Zhang, A. A. Awan,\nC. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, and\nY. He, “Deepspeed inference: Enabling efficient inference of\ntransformer models at unprecedented scale,” 2022. 13\n[202] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and\nB. Catanzaro, “Megatron-lm: Training multi-billion parameter\nlanguage models using model parallelism,” 2020. 13\n[203] S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang,\nand Y. You, “Colossal-ai: A unified deep learning system for\nlarge-scale parallel training,” 2023. 13\n[204] A. Sergeev and M. D. Balso, “Horovod: fast and easy dis-\ntributed deep learning in tensorflow,” 2018. 13\n[205] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw,\nE. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, and\nI. Stoica, “Ray: A distributed framework for emerging ai ap-\nplications,” 2018. 13\n[206] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E.\nGonzalez, H. Zhang, and I. Stoica, “Efficient memory manage-\nment for large language model serving with pagedattention,”\n2023. 13\n[207] Y. Zhou and K. Yang, “Exploring tensorrt to improve real-\ntime inference for deep learning,” in 2022 IEEE 24th Int Conf\non High Performance Computing & Communications; 8th Int\nConf on Data Science & Systems; 20th Int Conf on Smart City;\n8th Int Conf on Dependability in Sensor, Cloud & Big Data\nSystems & Application (HPCC/DSS/SmartCity/DependSys),\npp. 2011–2018, 2022. 13\n[208] P. Tillet, H.-T. Kung, and D. Cox, “Triton: an intermediate lan-\nguage and compiler for tiled neural network computations,” in\nProceedings of the 3rd ACM SIGPLAN International Workshop\non Machine Learning and Programming Languages, pp. 10–19,\n2019. 13\n[209] O. Community, “Onnx: Open neural network exchange.” Uni-\nfied inference engine with hardware-specific optimizations. 13\n[210] I. Corporation, “Openvino: Intel optimization toolkit,” 2025.\nRuntime for Intel CPUs/iGPUs with pruning/quantization\nsupport. 13\n[211] M. Dukhan, “The indirect convolution algorithm,” 2019. 13\n[212] I. Groq, “Groq: Ai accelerator,” 2025.\nDeterministic low-\nlatency inference via custom tensor streaming processor. 13\n[213] J. Castaño, S. Martínez-Fernández, X. Franch, and J. Bogner,\n“Analyzing the evolution and maintenance of ml models on\nhugging face,” 2024. 13\n[214] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Auto-\nmatic differentiation in pytorch,” in NIPS-W, 2017. 13\n[215] S. Hao, Y. Gu, H. Luo, T. Liu, X. Shao, X. Wang, S. Xie, H. Ma,\nA. Samavedhi, Q. Gao, et al., “Llm reasoners: New evaluation,\nlibrary, and analysis of step-by-step reasoning with large lan-\nguage models,” arXiv preprint arXiv:2404.05221, 2024. 13\n[216] S. Pieri, S. S. Mullappilly, F. S. Khan, R. M. Anwer, S. Khan,\nT. Baldwin, and H. Cholakkal, “Bimedix: Bilingual medical\nmixture of experts llm,” arXiv preprint arXiv:2402.13253,\n2024. 12\n[217] Y. Yang, M. C. S. Uy, and A. Huang, “Finbert: A pretrained\nlanguage model for financial communications,” arXiv preprint\narXiv:2006.08097, 2020. 12\n[218] D. Thulke, Y. Gao, P. Pelser, R. Brune, R. Jalota, F. Fok,\nM. Ramos, I. van Wyk, A. Nasir, H. Goldstein, et al., “Cli-\nmategpt: Towards ai synthesizing interdisciplinary research on\nclimate change,” arXiv preprint arXiv:2401.09646, 2024. 12\n[219] S. S. Mullappilly, A. Shaker, O. Thawakar, H. Cholakkal, R. M.\nAnwer, S. Khan, and F. S. Khan, “Arabic mini-climategpt: A\nclimate change and sustainability tailored arabic llm,” arXiv\npreprint arXiv:2312.09366, 2023. 12\n[220] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-\naware unified pre-trained encoder-decoder models for code un-\nderstanding and generation,” arXiv preprint arXiv:2109.00859,\n2021. 12\n[221] K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and\nF. S. Khan, “Geochat: Grounded large vision-language model\nfor remote sensing,” in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 27831–\n27840, 2024. 12\n[222] S. S. Mullappilly, M. I. Kurpath, S. Pieri, S. Y. Alseiari,\nS. Cholakkal, K. Aldahmani, F. Khan, R. Anwer, S. Khan,\nT. Baldwin, et al., “Bimedix2: Bio-medical expert lmm for\ndiverse medical modalities,” arXiv preprint arXiv:2412.07769,\n2024. 12\n[223] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, “Video-\nchatgpt: Towards detailed video understanding via large vision\nand language models,” arXiv preprint arXiv:2306.05424, 2023.\n12\n\n\n28\n[224] B. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan,\n“Video-llava: Learning united visual representation by align-\nment before projection,” arXiv preprint arXiv:2311.10122,\n2023. 12\n[225] H. Zhang, X. Li, and L. Bing, “Video-llama: An instruction-\ntuned audio-visual language model for video understanding,”\narXiv preprint arXiv:2306.02858, 2023. 12\n[226] Y. Han, C. Zhang, X. Chen, X. Yang, Z. Wang, G. Yu, B. Fu,\nand H. Zhang, “Chartllama: A multimodal llm for chart un-\nderstanding and generation,” arXiv preprint arXiv:2311.16483,\n2023. 12\n[227] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, “A survey on model\ncompression for large language models,” Transactions of the\nAssociation for Computational Linguistics, vol. 12, pp. 1556–\n1577, 2024. 12\n[228] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, J. Liu, Z. Qu,\nS. Yan, Y. Zhu, Q. Zhang, et al., “Efficient large language\nmodels: A survey,” arXiv preprint arXiv:2312.03863, 2023. 12\n[229] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii,\nA. Ratner, R. Krishna, C.-Y. Lee, and T. Pfister, “Distill-\ning step-by-step! outperforming larger language models with\nless training data and smaller model sizes,” arXiv preprint\narXiv:2305.02301, 2023. 12\n[230] Y. Gu, L. Dong, F. Wei, and M. Huang, “Minillm: Knowl-\nedge distillation of large language models,” arXiv preprint\narXiv:2306.08543, 2023. 12\n[231] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continu-\nous prompts for generation,” arXiv preprint arXiv:2101.00190,\n2021. 13\n[232] N.\nHoulsby,\nA.\nGiurgiu,\nS.\nJastrzebski,\nB.\nMorrone,\nQ. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly,\n“Parameter-efficient transfer learning for nlp,” 2019. 13\n[233] B. P. Lowerre and B. R. Reddy, “Harpy, a connected speech\nrecognition system,” The Journal of the Acoustical Society of\nAmerica, vol. 59, no. S1, pp. S97–S97, 1976. 14\n[234] A. Graves, “Sequence transduction with recurrent neural net-\nworks,” arXiv preprint arXiv:1211.3711, 2012. 14\n[235] H. Sun, M. Haider, R. Zhang, H. Yang, J. Qiu, M. Yin,\nM. Wang, P. Bartlett, and A. Zanette, “Fast best-of-n decoding\nvia speculative rejection,” 2024. 14\n[236] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,\nA. Jones, N. Joseph, B. Mann, N. DasSarma, et al., “A gen-\neral language assistant as a laboratory for alignment,” arXiv\npreprint arXiv:2112.00861, 2021. 14\n[237] A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu,\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker,\net al., “Improving alignment of dialogue agents via targeted\nhuman judgements,” arXiv preprint arXiv:2209.14375, 2022.\n14\n[238] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss,\nA. Radford, D. Amodei, and P. F. Christiano, “Learning to\nsummarize with human feedback,” Advances in Neural Infor-\nmation Processing Systems, vol. 33, pp. 3008–3021, 2020. 14\n[239] J. Q. Yang, S. Salamatian, Z. Sun, A. T. Suresh, and\nA. Beirami, “Asymptotics of language model alignment,” arXiv\npreprint arXiv:2404.01730, 2024. 14\n[240] H. Sun, M. Haider, R. Zhang, H. Yang, J. Qiu, M. Yin,\nM. Wang, P. Bartlett, and A. Zanette, “Fast best-of-n decoding\nvia speculative rejection,” arXiv preprint arXiv:2410.20290,\n2024. 14\n[241] J. Hilton and L. Gao, “Measuring goodhart’s law,” OpenAI\nResearch Blog, 2022. 14\n[242] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-\nP. Lim, “Plan-and-solve prompting: Improving zero-shot chain-\nof-thought reasoning by large language models,” arXiv preprint\narXiv:2305.04091, 2023. 15\n[243] Y.\nWang,\nY.\nKordi,\nS.\nMishra,\nA.\nLiu,\nN.\nA.\nSmith,\nD. Khashabi, and H. Hajishirzi, “Self-instruct: Aligning lan-\nguage models with self-generated instructions,” arXiv preprint\narXiv:2212.10560, 2022. 15\n[244] X. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin,\nS. Prakash, C. Sutton, X. Wang, and D. Zhou, “Universal self-\nconsistency for large language models,” in ICML 2024 Work-\nshop on In-Context Learning. 15\n[245] A. Newell, “On the analysis of human problem solving proto-\ncols,” 1966. 15\n[246] F. Haji, M. Bethany, M. Tabar, J. Chiang, A. Rios, and\nP. Najafirad, “Improving llm reasoning with multi-agent tree-\nof-thought validator agent,” arXiv preprint arXiv:2409.11527,\n2024. 16\n[247] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Pod-\nstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadom-\nski, P. Nyczyk, and T. Hoefler, “Graph of thoughts: Solving\nelaborate problems with large language models,” Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 38,\np. 17682–17690, Mar. 2024. 16\n[248] D.\nWilson,\n“Llm\ntree\nsearch,”\narXiv\npreprint\narXiv:2410.19117, 2024. 16\n[249] D. Hendrycks and K. Gimpel, “A baseline for detecting mis-\nclassified and out-of-distribution examples in neural networks,”\narXiv preprint arXiv:1610.02136, 2016. 16\n[250] G. Portillo Wightman, A. DeLucia, and M. Dredze, “Strength\nin numbers: Estimating confidence of large language models\nby prompt agreement,” in Proceedings of the 3rd Workshop on\nTrustworthy Natural Language Processing (TrustNLP 2023),\npp. 326–362, 2023. 17\n[251] J. Qi, H. Tang, and Z. Zhu, “Verifierq: Enhancing llm test\ntime compute with q-learning-based verifiers,” arXiv preprint\narXiv:2410.08048, 2024. 17\n[252] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,\nS. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,\nS. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yaz-\ndanbakhsh, and P. Clark, “Self-refine: Iterative refinement with\nself-feedback,” 2023. 17\n[253] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang,\nJ. Wang, S. Jin, E. Zhou, et al., “The rise and potential of\nlarge language model based agents: A survey,” arXiv preprint\narXiv:2309.07864, 2023. 17\n[254] R. Coulom, “Efficient selectivity and backup operators in\nmonte-carlo tree search,” in International conference on com-\nputers and games, pp. 72–83, Springer, 2006. 17\n[255] J. X. Chen, “The evolution of computing: Alphago,” Comput-\ning in Science & Engineering, vol. 18, no. 4, pp. 4–7, 2016. 17\n[256] L. Kocsis and C. Szepesvári, “Bandit based monte-carlo plan-\nning,” in European conference on machine learning, pp. 282–\n293, Springer, 2006. 17\n[257] H. W. Sprueill, C. Edwards, M. V. Olarte, U. Sanyal, H. Ji, and\nS. Choudhury, “Monte carlo thought search: Large language\nmodel querying for complex scientific reasoning in catalyst\ndesign,” arXiv preprint arXiv:2310.14420, 2023. 18, 20\n[258] M. DeLorenzo, A. B. Chowdhury, V. Gohil, S. Thakur,\nR. Karri, S. Garg, and J. Rajendran, “Make every move count:\nLlm-based high-quality rtl code generation using mcts,” arXiv\npreprint arXiv:2402.03289, 2024. 18\n[259] S. Park, X. Liu, Y. Gong, and E. Choi, “Ensembling large\nlanguage models with process reward-guided tree search for\nbetter complex reasoning,” arXiv preprint arXiv:2412.15797,\n2024. 18\n[260] M. Shen, G. Zeng, Z. Qi, Z.-W. Hong, Z. Chen, W. Lu,\nG. Wornell, S. Das, D. Cox, and C. Gan, “Satori: Reinforcement\nlearning with chain-of-action-thought enhances llm reasoning\nvia autoregressive search,” arXiv preprint arXiv:2502.02508,\n2025. 18\n[261] S. R. Motwani, C. Smith, R. J. Das, R. Rafailov, I. Laptev,\nP. H. S. Torr, F. Pizzati, R. Clark, and C. S. de Witt, “Malt:\nImproving reasoning with multi-agent llm training,” 2025. 18\n[262] U. Anwar, A. Saparov, J. Rando, D. Paleka, M. Turpin, P. Hase,\nE. S. Lubana, E. Jenner, S. Casper, O. Sourbut, et al., “Foun-\ndational challenges in assuring alignment and safety of large\nlanguage models,” arXiv preprint arXiv:2404.09932, 2024. 18\n[263] W. Zhang, P. H. Torr, M. Elhoseiny, and A. Bibi, “Bi-factorial\npreference optimization: Balancing safety-helpfulness in lan-\nguage models,” arXiv preprint arXiv:2408.15313, 2024. 18\n[264] C. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vulić,\nand F. Wei, “Imagine while reasoning in space: Multimodal\nvisualization-of-thought,” arXiv preprint arXiv:2501.07542,\n2025. 18\n[265] F. Nowak, A. Svete, A. Butoi, and R. Cotterell, “On the\nrepresentational capacity of neural language models with chain-\nof-thought reasoning,” arXiv preprint arXiv:2406.14197, 2024.\n18\n[266] Z. Li, H. Liu, D. Zhou, and T. Ma, “Chain of thought em-\npowers transformers to solve inherently serial problems,” arXiv\npreprint arXiv:2402.12875, vol. 1, 2024. 18\n\n\n29\n[267] W.\nMerrill\nand\nA.\nSabharwal,\n“The\nexpressive\npower\nof\ntransformers\nwith\nchain\nof\nthought,”\narXiv\npreprint\narXiv:2310.07923, 2023. 18\n[268] C. V. Snell, J. Lee, K. Xu, and A. Kumar, “Scaling test-\ntime compute optimally can be more effective than scaling llm\nparameters,” in The Thirteenth International Conference on\nLearning Representations. 18\n[269] Saxton et al., “Analysing mathematical reasoning abilities of\nneural models,” arXiv:1904.01557, 2019. 19\n[270] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,\nL. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,\nC. Hesse, and J. Schulman, “Training verifiers to solve math\nword problems,” arXiv preprint arXiv:2110.14168, 2021. 19\n[271] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok,\nZ. Li, A. Weller, and W. Liu, “Metamath: Bootstrap your\nown mathematical questions for large language models,” arXiv\npreprint arXiv:2309.12284, 2023. 19\n[272] Z. Xie, S. Thiem, J. Martin, E. Wainwright, S. Marmorstein,\nand P. Jansen, “WorldTree v2: A corpus of science-domain\nstructured explanations and inference patterns supporting\nmulti-hop inference,” in Proceedings of the Twelfth Language\nResources and Evaluation Conference (N. Calzolari, F. Béchet,\nP. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isa-\nhara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk,\nand S. Piperidis, eds.), (Marseille, France), pp. 5456–5473,\nEuropean Language Resources Association, May 2020. 19\n[273] F. Liu, E. Bugliarello, E. M. Ponti, S. Reddy, N. Collier, and\nD. Elliott, “Visually grounded reasoning across languages and\ncultures,” in Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, (Online and Punta\nCana, Dominican Republic), pp. 10467–10485, Association for\nComputational Linguistics, Nov. 2021. 19\n[274] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang,\nS. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan,\nR. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun,\nY. Su, and W. Chen, “Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for expert\nagi,” in Proceedings of CVPR, 2024. 19\n[275] S.\nLin,\nJ.\nHilton,\nand\nO.\nEvans,\n“Truthfulqa:\nMeasur-\ning how models mimic human falsehoods,” arXiv preprint\narXiv:2109.07958, 2021. 19\n[276] X. Yue et al., “Mammoth: Building math generalist mod-\nels\nthrough\nhybrid\ninstruction\ntuning,”\narXiv\npreprint\narXiv:2309.05653, 2023. 19\n[277] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,\nD. Song, and J. Steinhardt, “Measuring massive multitask lan-\nguage understanding,” Proceedings of the International Con-\nference on Learning Representations (ICLR), 2021. 19\n[278] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song,\nand J. Steinhardt, “Aligning ai with shared human values,”\nProceedings of the International Conference on Learning Rep-\nresentations (ICLR), 2021. 19\n[279] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and\nM. Gardner, “DROP: A reading comprehension benchmark\nrequiring discrete reasoning over paragraphs,” in Proc. of\nNAACL, 2019. 19\n[280] Z. Wang, Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar,\nD. Egert, O. Delalleau, J. P. Scowcroft, N. Kant, A. Swope,\net al., “Helpsteer: Multi-attribute helpfulness dataset for\nsteerlm,” arXiv preprint arXiv:2311.09528, 2023. 19\n[281] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu,\nand M. Sun, “Ultrafeedback: Boosting language models with\nhigh-quality feedback,” 2023. 19\n[282] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl:\nDatasets for deep data-driven reinforcement learning,” 2020. 19\n[283] T. Schmied, M. Hofmarcher, F. Paischer, R. Pascanu, and\nS. Hochreiter, “Learning to modulate pre-trained models in rl,”\nAdvances in Neural Information Processing Systems, vol. 36,\n2024. 19\n[284] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel,\nM. Veloso, and R. Salakhutdinov, “Minerl: A large-scale dataset\nof minecraft demonstrations,” 2019. 19\n[285] T. Nguyen, C. V. Nguyen, V. D. Lai, H. Man, N. T. Ngo,\nF. Dernoncourt, R. A. Rossi, and T. H. Nguyen, “CulturaX:\nA cleaned, enormous, and multilingual dataset for large lan-\nguage models in 167 languages,” in Proceedings of the 2024\nJoint International Conference on Computational Linguistics,\nLanguage Resources and Evaluation (LREC-COLING 2024)\n(N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, and\nN. Xue, eds.), (Torino, Italia), pp. 4226–4237, ELRA and ICCL,\nMay 2024. 19\n[286] X. Yue, Y. Song, A. Asai, S. Kim, J. de Dieu Nyandwi,\nS. Khanuja, A. Kantharuban, L. Sutawika, S. Ramamoorthy,\nand G. Neubig, “Pangea: A fully open multilingual multimodal\nllm for 39 languages,” arXiv preprint arXiv:2410.16153, 2024.\n19\n[287] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” CoRR, vol. abs/1810.04805, 2018. 19\n[288] Y. Liang, N. Duan, Y. Gong, N. Wu, F. Guo, W. Qi, M. Gong,\nL. Shou, D. Jiang, G. Cao, X. Fan, R. Zhang, R. Agrawal,\nE. Cui, S. Wei, T. Bharti, Y. Qiao, J.-H. Chen, W. Wu, S. Liu,\nF. Yang, D. Campos, R. Majumder, and M. Zhou, “Xglue: A\nnew benchmark dataset for cross-lingual pre-training, under-\nstanding and generation,” arXiv, vol. abs/2004.01401, 2020. 19\n[289] G. Son, D. Yoon, J. Suk, J. Aula-Blasco, M. Aslan, V. T. Kim,\nS. B. Islam, J. Prats-Cristià, L. Tormo-Bañuelos, and S. Kim,\n“Mm-eval: A multilingual meta-evaluation benchmark for llm-\nas-a-judge and reward models,” 2024. 19\n[290] S. et.al, “Beyond the imitation game: Quantifying and extrap-\nolating the capabilities of language models,” 2022. 19\n[291] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu,\nY. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al., “Judging\nllm-as-a-judge with mt-bench and chatbot arena,” Advances\nin Neural Information Processing Systems, vol. 36, pp. 46595–\n46623, 2023. 19\n[292] E. Dinan, V. Logacheva, V. Malykh, A. H. Miller, K. Shuster,\nJ. Urbanek, D. Kiela, A. Szlam, I. Serban, R. Lowe, S. Prab-\nhumoye, A. W. Black, A. I. Rudnicky, J. Williams, J. Pineau,\nM. S. Burtsev, and J. Weston, “The second conversational\nintelligence challenge (convai2),” CoRR, vol. abs/1902.00098,\n2019. 19\n[293] M. Eric, R. Goel, S. Paul, A. Sethi, S. Agarwal, S. Gao,\nand D. Hakkani-Tur, “MultiWOZ 2.1: A consolidated multi-\ndomain dialogue dataset with state corrections and state track-\ning baselines,” in Proceedings of the 12th Language Resources\nand Evaluation Conference, (Marseille, France), pp. 422–428,\nEuropean Language Resources Association, May 2020. 19\n[294] X. Li and D. Roth, “Learning question classifiers,” in COLING\n2002: The 19th International Conference on Computational\nLinguistics, 2002. 19, 20\n[295] E. Hovy, L. Gerber, U. Hermjakob, C.-Y. Lin, and D. Ravichan-\ndran, “Toward semantics-based answer pinpointing,” in Pro-\nceedings of the First International Conference on Human Lan-\nguage Technology Research, 2001. 19, 20\n[296] N.\nThakur,\nN.\nReimers,\nA.\nRücklé,\nA.\nSrivastava,\nand\nI. Gurevych, “BEIR: A heterogeneous benchmark for zero-\nshot evaluation of information retrieval models,” in Thirty-\nfifth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2), 2021. 19, 20\n[297] C. Chhun, F. M. Suchanek, and C. Clavel, “Do language models\nenjoy their own stories? Prompting large language models for\nautomatic story evaluation,” Transactions of the Association\nfor Computational Linguistics, vol. 12, pp. 1122–1142, 2024. 19\n[298] H. Chen, D. M. Vo, H. Takamura, Y. Miyao, and H. Nakayama,\n“Storyer: Automatic story evaluation via ranking, rating and\nreasoning,” 2022. 19\n[299] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen,\nR. Sun, Y. Wang, and Y. Yang, “Beavertails: Towards improved\nsafety alignment of llm via a human-preference dataset,” Ad-\nvances in Neural Information Processing Systems, vol. 36, 2024.\n19, 20\n[300] G. Xu, J. Liu, M. Yan, H. Xu, J. Si, Z. Zhou, P. Yi, X. Gao,\nJ. Sang, R. Zhang, et al., “Cvalues: Measuring the values of\nchinese large language models from safety to responsibility,”\narXiv preprint arXiv:2307.09705, 2023. 19\n[301] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-\ntask generalization via natural language crowdsourcing instruc-\ntions,” in ACL, 2022. 19\n[302] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik,\nD. Stap, et al., “Super-naturalinstructions:generalization via\ndeclarative instructions on 1600+ tasks,” in EMNLP, 2022. 19\n[303] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,\n\n\n30\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schul-\nman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,\nP. Welinder, P. F. Christiano, J. Leike, and R. Lowe, “Training\nlanguage models to follow instructions with human feedback,”\nin Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems\n2022, NeurIPS 2022, New Orleans, LA, USA, November 28\n- December 9, 2022 (S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh, eds.), 2022. 20\n[304] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and\nJ. Leike, “Self-critiquing models for assisting human evalua-\ntors,” arXiv preprint arXiv:2206.05802, 2022. 20\n[305] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei,\n“Scaling laws for neural language models,” arXiv preprint\narXiv:2001.08361, 2020. 20\n[306] S. Roy and D. Roth, “Solving general arithmetic word prob-\nlems,” 2016. 20\n[307] Y. Jinnai, T. Morimura, K. Ariu, and K. Abe, “Regularized\nbest-of-n sampling to mitigate reward hacking for language\nmodel alignment,” arXiv preprint arXiv:2404.01054, 2024. 20\n[308] L. Chen, C. Zhu, D. Soselia, J. Chen, T. Zhou, T. Goldstein,\nH. Huang, M. Shoeybi, and B. Catanzaro, “Odin: Disentangled\nreward mitigates hacking in rlhf,” ArXiv, vol. abs/2402.07319,\n2024. 20\n[309] T. Liu, W. Xiong, J. Ren, L. Chen, J. Wu, R. Joshi, Y. Gao,\nJ. Shen, Z. Qin, T. Yu, D. Sohn, A. Makarova, J. Liu, Y. Liu,\nB. Piot, A. Ittycheriah, A. Kumar, and M. Saleh, “Rrm: Ro-\nbust reward model training mitigates reward hacking,” ArXiv,\nvol. abs/2409.13156, 2024. 20\n[310] C. Wang, Z. Zhao, Y. Jiang, Z. Chen, C. Zhu, Y. Chen, J. Liu,\nL. Zhang, X. Fan, H. Ma, and S.-Y. Wang, “Beyond reward\nhacking: Causal rewards for large language model alignment,”\n2025. 20\n[311] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I.\nCowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis,\nand S. Colton, “A survey of monte carlo tree search methods,”\nIEEE Transactions on Computational Intelligence and AI in\ngames, vol. 4, no. 1, pp. 1–43, 2012. 20\n[312] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,\nS. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al.,\n“Self-refine: Iterative refinement with self-feedback,” Advances\nin Neural Information Processing Systems, vol. 36, 2024. 20\n[313] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot,\n“Complexity-based prompting for multi-step reasoning,” in\nThe Eleventh International Conference on Learning Represen-\ntations, 2022. 20\n[314] B. Johnson, “Metacognition for artificial intelligence system\nsafety–an approach to safe and desired behavior,” Safety Sci-\nence, vol. 151, p. 105743, 2022. 20, 21\n[315] OpenAI, “Early access for safety testing,” 2024. 20\n[316] Y. Yan, X. Lou, J. Li, Y. Zhang, J. Xie, C. Yu, Y. Wang,\nD. Yan, and Y. Shen, “Reward-robust rlhf in llms,” ArXiv,\nvol. abs/2409.15360, 2024. 20\n[317] W. Yu, Z. Sun, J. Xu, Z. Dong, X. Chen, H. Xu, and J.-\nR. Wen, “Explainable legal case matching via inverse optimal\ntransport-based rationale extraction,” in Proceedings of the\n45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pp. 657–668, 2022. 20\n[318] A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y. Choi,\nand H. Hajishirzi, “Mathqa: Towards interpretable math word\nproblem solving with operation-based formalisms,” 2019. 20\n[319] L. Chen, O. Sinavski, J. Hünermann, A. Karnsund, A. J. Will-\nmott, D. Birch, D. Maund, and J. Shotton, “Driving with llms:\nFusing object-level vector modality for explainable autonomous\ndriving,” 2024 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 14093–14100, 2023. 20\n[320] R. Poulain, H. Fayyaz, and R. Beheshti, “Bias patterns in the\napplication of llms for clinical decision support: A comprehen-\nsive study,” arXiv preprint arXiv:2404.15149, 2024. 20\n[321] Z. Fan, R. Chen, R. Xu, and Z. Liu, “Biasalert: A plug-and-\nplay tool for social bias detection in llms,” arXiv preprint\narXiv:2407.10241, 2024. 20\n[322] M. Li, T. Shi, C. Ziems, M.-Y. Kan, N. F. Chen, Z. Liu, and\nD. Yang, “Coannotating: Uncertainty-guided work allocation\nbetween human and large language models for data annota-\ntion,” arXiv preprint arXiv:2310.15638, 2023. 20\n[323] A. Elangovan, J. Ko, L. Xu, M. Elyasi, L. Liu, S. Bodapati,\nand D. Roth, “Beyond correlation: The impact of human un-\ncertainty in measuring the effectiveness of automatic evaluation\nand llm-as-a-judge,” arXiv preprint arXiv:2410.03775, 2024. 20\n[324] Y. R. Dong, T. Hu, and N. Collier, “Can llm be a personalized\njudge?,” arXiv preprint arXiv:2406.11657, 2024. 20\n[325] D. Wang, K. Yang, H. Zhu, X. Yang, A. Cohen, L. Li,\nand Y. Tian, “Learning personalized story evaluation,” arXiv\npreprint arXiv:2310.03304, 2023. 20\n[326] H. Du, S. Liu, L. Zheng, Y. Cao, A. Nakamura, and L. Chen,\n“Privacy in fine-tuning large language models: Attacks, de-\nfenses, and future directions,” 2024. 20, 22\n[327] L. Yuan, W. Li, H. Chen, G. Cui, N. Ding, K. Zhang, B. Zhou,\nZ. Liu, and H. Peng, “Free process rewards without process\nlabels,” arXiv preprint arXiv:2412.01981, 2024. 20\n[328] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani,\nD. Jayaraman, Y. Zhu, L. Fan, and A. Anandkumar, “Eureka:\nHuman-level reward design via coding large language models,”\nArXiv, vol. abs/2310.12931, 2023. 20\n[329] A. Havrilla, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu,\nM. Zhuravinskyi, E. Hambro, and R. Railneau, “Glore: When,\nwhere, and how to improve llm reasoning via global and local\nrefinements,” ArXiv, vol. abs/2402.10963, 2024. 20\n[330] M. Fawi, “Curlora: Stable llm continual fine-tuning and catas-\ntrophic forgetting mitigation,” 2024. 20\n[331] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu,\nW. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji, “Mme:\nA comprehensive evaluation benchmark for multimodal large\nlanguage models,” ArXiv, vol. abs/2306.13394, 2023. 20\n[332] Y. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang,\nR. Xie, J. Wang, X. Xie, W. Ye, S.-B. Zhang, and Y. Zhang,\n“Pandalm: An automatic evaluation benchmark for llm instruc-\ntion tuning optimization,” ArXiv, vol. abs/2306.05087, 2023. 20\n[333] Y. Sun, Z. Li, Y. Li, and B. Ding, “Improving lora in privacy-\npreserving federated learning,” ArXiv, vol. abs/2403.12313,\n2024. 20\n[334] Y. He, Y. Kang, L. Fan, and Q. Yang, “Fedeval-llm: Federated\nevaluation of large language models on downstream tasks with\ncollective wisdom,” arXiv preprint arXiv:2404.12273, 2024. 20\n[335] J. Park, S. Jwa, M. Ren, D. Kim, and S. Choi, “Offsetbias:\nLeveraging debiased data for tuning evaluators,” arXiv preprint\narXiv:2407.06551, 2024. 20\n[336] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpuro-\nhit, P. Clark, and A. Kalyan, “Dynamic prompt learning via\npolicy gradient for semi-structured mathematical reasoning,”\n2023. 20\n[337] L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar,\nand R. Agarwal, “Generative verifiers: Reward modeling as\nnext-token prediction,” in The 4th Workshop on Mathematical\nReasoning and AI at NeurIPS’24, 2024. 20\n[338] S. Yang and D. Song, “FPC: Fine-tuning with prompt cur-\nriculum for relation extraction,” in Proceedings of the 2nd\nConference of the Asia-Pacific Chapter of the Association for\nComputational Linguistics and the 12th International Joint\nConference on Natural Language Processing (Volume 1: Long\nPapers) (Y. He, H. Ji, S. Li, Y. Liu, and C.-H. Chang, eds.),\n(Online only), pp. 1065–1077, Association for Computational\nLinguistics, Nov. 2022. 20\n[339] Y. Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi,\nand B. Catanzaro, “Rankrag: Unifying context ranking with\nretrieval-augmented generation in llms,” Advances in Neural\nInformation Processing Systems, vol. 37, pp. 121156–121184,\n2025. 20\n[340] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James,\nP. Rodriguez, J. Kahn, G. Szilvasy, M. Lewis, et al., “Ra-dit:\nRetrieval-augmented dual instruction tuning,” in The Twelfth\nInternational Conference on Learning Representations, 2023.\n20\n[341] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica,\nand J. E. Gonzalez, “Raft: Adapting language model to domain\nspecific rag,” in First Conference on Language Modeling, 2024.\n20\n[342] T. Huang, S. Hu, F. Ilhan, S. F. Tekin, and L. Liu, “Harmful\nfine-tuning attacks and defenses for large language models: A\nsurvey,” arXiv preprint arXiv:2409.18169, 2024. 20\n[343] S. R. Bowman, J. Hyun, E. Perez, E. Chen, C. Pettit, S. Heiner,\nK. Lukoši¯ut˙e, A. Askell, A. Jones, A. Chen, et al., “Measuring\n\n\n31\nprogress on scalable oversight for large language models,” arXiv\npreprint arXiv:2211.03540, 2022. 20\n[344] N. Hollmann, S. Müller, and F. Hutter, “Llms for semi-\nautomated data science: Introducing caafe for context-aware\nautomated feature engineering,” CoRR, 2023. 20\n[345] S. R. Motwani, C. Smith, R. J. Das, M. Rybchuk, P. H. Torr,\nI. Laptev, F. Pizzati, R. Clark, and C. S. de Witt, “Malt:\nImproving reasoning with multi-agent llm training,” arXiv\npreprint arXiv:2412.01928, 2024. 21\n[346] A. Estornell, J.-F. Ton, Y. Yao, and Y. Liu, “Acc-debate: An\nactor-critic approach to multi-agent debate,” arXiv preprint\narXiv:2411.00053, 2024. 21\n[347] L. Luo, Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu,\nY. Zhu, L. Meng, J. Sun, et al., “Improve mathematical rea-\nsoning in language models by automated process supervision,”\narXiv preprint arXiv:2406.06592, 2024. 21\n[348] W. Shen, X. Zhang, Y. Yao, R. Zheng, H. Guo, and Y. Liu,\n“Improving reinforcement learning from human feedback using\ncontrastive rewards,” arXiv preprint arXiv:2403.07708, 2024.\n21\n[349] M. Ma, P. D’Oro, Y. Bengio, and P.-L. Bacon, “Long-term\ncredit assignment via model-based temporal shortcuts,” in\nDeep RL Workshop NeurIPS 2021, 2021. 21\n[350] E. Pignatelli, J. Ferret, M. Geist, T. Mesnard, H. van Has-\nselt, O. Pietquin, and L. Toni, “A survey of temporal credit\nassignment in deep reinforcement learning,” arXiv preprint\narXiv:2312.01072, 2023. 21\n[351] H. Zhang and Y. Guo, “Generalization of reinforcement learn-\ning with policy-aware adversarial data augmentation,” 2021. 21\n[352] A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer,\nO. Pietquin, A. Üstün, and S. Hooker, “Back to basics: Re-\nvisiting reinforce style optimization for learning from human\nfeedback in llms,” arXiv preprint arXiv:2402.14740, 2024. 21\n[353] S. Lee, G. Lee, J. W. Kim, J. Shin, and M.-K. Lee, “Hetal: Ef-\nficient privacy-preserving transfer learning with homomorphic\nencryption,” 2024. 22\n[354] Y. Wei, J. Jia, Y. Wu, C. Hu, C. Dong, Z. Liu, X. Chen, Y. Peng,\nand S. Wang, “Distributed differential privacy via shuffling\nversus aggregation: A curious study,” IEEE Transactions on\nInformation Forensics and Security, vol. 19, pp. 2501–2516,\n2024. 22\n[355] W. Zhang, K. Tang, H. Wu, M. Wang, Y. Shen, G. Hou, Z. Tan,\nP. Li, Y. Zhuang, and W. Lu, “Agent-pro: Learning to evolve\nvia policy-level reflection and optimization,” arXiv preprint\narXiv:2402.17574, 2024. 22\n[356] C. Ma, J. Zhang, Z. Zhu, C. Yang, Y. Yang, Y. Jin,\nZ. Lan, L. Kong, and J. He, “Agentboard: An analytical\nevaluation board of multi-turn llm agents,” arXiv preprint\narXiv:2401.13178, 2024. 22\n[357] J. Zhang, J. Xiang, Z. Yu, F. Teng, X. Chen, J. Chen, M. Zhuge,\nX. Cheng, S. Hong, J. Wang, et al., “Aflow: Automating agentic\nworkflow generation,” arXiv preprint arXiv:2410.10762, 2024.\n22\n[358] H. D. Le, X. Xia, and Z. Chen, “Multi-agent causal discovery us-\ning large language models,” arXiv preprint arXiv:2407.15073,\n2024. 22\n[359] D. M. Owens, R. A. Rossi, S. Kim, T. Yu, F. Dernon-\ncourt, X. Chen, R. Zhang, J. Gu, H. Deilamsalehy, and\nN. Lipka, “A multi-llm debiasing framework,” arXiv preprint\narXiv:2409.13884, 2024. 22\n[360] H. Zou, Q. Zhao, L. Bariah, Y. Tian, M. Bennis, S. Lasaulce,\nM. Debbah, and F. Bader, “Genainet: Enabling wireless collec-\ntive intelligence via knowledge transfer and reasoning,” ArXiv,\nvol. abs/2402.16631, 2024. 22\n[361] R. Lee, O. J. Mengshoel, A. Saksena, R. Gardner, D. Genin,\nJ. Silbermann, M. Owen, and M. J. Kochenderfer, “Adaptive\nstress testing: Finding likely failure events with reinforcement\nlearning,” 2020. 22\n[362] A. G. Baydin, B. A. Pearlmutter, D. Syme, F. Wood, and\nP. Torr, “Gradients without backpropagation,” 2022. 22\n[363] Y. Liu, C. Cai, X. Zhang, X. Yuan, and C. Wang, “Arondight:\nRed teaming large vision language models with auto-generated\nmulti-modal jailbreak prompts,” in ACM Multimedia, 2024. 22\n[364] D. Kim, K. Lee, J. Shin, and J. Kim, “Aligning large language\nmodels with self-generated preference data,” arXiv preprint\narXiv:2406.04412, 2024. 22\n[365] S. Ebrahimi, S. Ö. Arik, T. Nama, and T. Pfister, “Crome:\nCross-modal adapters for efficient multimodal llm,” ArXiv,\nvol. abs/2408.06610, 2024. 22\n[366] H. Xia, Y. Li, C. T. Leong, W. Wang, and W. Li, “Tokenskip:\nControllable chain-of-thought compression in llms,” 2025. 22\n[367] Z. Ma, W. Wu, Z. Zheng, Y. Guo, Q. Chen, S. Zhang, and\nX. Chen, “Leveraging speech ptm, text llm, and emotional tts\nfor speech emotion recognition,” ICASSP 2024 - 2024 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pp. 11146–11150, 2023. 22\n[368] Z. Xi, W. Chen, B. Hong, S. Jin, R. Zheng, W. He, Y. Ding,\nS. Liu, X. Guo, J. Wang, H. Guo, W. Shen, X. Fan, Y. Zhou,\nS. Dou, X. Wang, X. Zhang, P. Sun, T. Gui, Q. Zhang,\nand X. Huang, “Training large language models for reasoning\nthrough reverse curriculum reinforcement learning,” ArXiv,\nvol. abs/2402.05808, 2024. 22\n[369] O. Y. Lee, A. Xie, K. Fang, K. Pertsch, and C. Finn,\n“Affordance-guided reinforcement learning via visual prompt-\ning,” ArXiv, vol. abs/2407.10341, 2024. 22\n[370] H. Xu, Z. Zhu, D. Ma, S. Zhang, S. Fan, L. Chen, and K. Yu,\n“Rejection improves reliability: Training llms to refuse un-\nknown questions using rl from knowledge feedback,” ArXiv,\nvol. abs/2403.18349, 2024. 22\n[371] X. Chen, J. Xu, T. Liang, Z. He, J. Pang, D. Yu, L. Song, Q. Liu,\nM. Zhou, Z. Zhang, R. Wang, Z. Tu, H. Mi, and D. Yu, “Do not\nthink that much for 2+3=? on the overthinking of o1-like llms,”\nArXiv, vol. abs/2412.21187, 2024. 22\n[372] M. Kemmerling, D. Lütticke, and R. H. Schmitt, “Beyond\ngames: a systematic review of neural monte carlo tree search\napplications,” Applied Intelligence, vol. 54, no. 1, pp. 1020–\n1046, 2024. 22\n[373] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu,\nW. Xu, X. Wang, Y. Sun, R. Kong, Y. Wang, H. Geng, J. Luan,\nX. Jin, Z.-L. Ye, G. Xiong, F. Zhang, X. Li, M. Xu, Z. Li, P. Li,\nY. Liu, Y. Zhang, and Y. Liu, “Personal llm agents: Insights\nand survey about the capability, efficiency and security,” ArXiv,\nvol. abs/2401.05459, 2024. 22\n[374] H. Li, L. Ding, M. Fang, and D. Tao, “Revisiting catastrophic\nforgetting in large language model tuning,” 2024. 22\n[375] N. Alzahrani, H. A. Alyahya, Y. Alnumay, S. Alrashed, S. Al-\nsubaie, Y. Almushaykeh, F. Mirza, N. Alotaibi, N. Altwairesh,\nA. Alowisheq, M. S. Bari, and H. Khan, “When benchmarks\nare targets: Revealing the sensitivity of large language model\nleaderboards,” 2024. 22\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21321v1.pdf",
    "total_pages": 31,
    "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
    "authors": [
      "Komal Kumar",
      "Tajamul Ashraf",
      "Omkar Thawakar",
      "Rao Muhammad Anwer",
      "Hisham Cholakkal",
      "Mubarak Shah",
      "Ming-Hsuan Yang",
      "Phillip H. S. Torr",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}