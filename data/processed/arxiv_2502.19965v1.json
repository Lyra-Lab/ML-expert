{
  "id": "arxiv_2502.19965v1",
  "text": "Deterministic or probabilistic? The psychology of LLMs as\nrandom number generators\nJavier Coronado-Bl´azquez\nj.coronado.blazquez@gmail.com\nTelef´onica Tech, AI & Data Unit\nMadrid, 28050, Spain\nAbstract\nLarge Language Models (LLMs) have transformed text generation through inherently prob-\nabilistic context-aware mechanisms, mimicking human natural language. In this paper,\nwe systematically investigate the performance of various LLMs when generating random\nnumbers, considering diverse configurations such as different model architectures, numer-\nical ranges, temperature, and prompt languages.\nOur results reveal that, despite their\nstochastic transformers-based architecture, these models often exhibit deterministic re-\nsponses when prompted for random numerical outputs. In particular, we find significant\ndifferences when changing the model, as well as the prompt language, attributing this phe-\nnomenon to biases deeply embedded within the training data. Models such as DeepSeek–R1\ncan shed some light on the internal reasoning process of LLMs, despite arriving to similar\nresults. These biases induce predictable patterns that undermine genuine randomness, as\nLLMs are nothing but reproducing our own human cognitive biases.\nKeywords:\nGenerative Artificial Intelligence, Large Language Models, Natural Language\nProcessing, Deep Generative Models, Trustworthy Natural Language Processing\n1 Introduction\nLarge Language Models (LLMs) have revolutionized natural language processing by gener-\nating human-like text through advanced probabilistic mechanisms. Based on transformer\narchitectures Vaswani et al. (2023) and trained on vast corpora of text, these models learn\nto predict the next token in a sequence, effectively capturing intricate statistical patterns\ninherent in human language. Although LLMs are inherently stochastic, recent observations\nhave revealed a curious phenomenon: when tasked with generating a single random number\n–a seemingly trivial exercise in randomness– they often produce deterministic outputs. This\ncounterintuitive behavior raises important questions about the interplay between a model’s\nprobabilistic design and the biases ingrained in its training data.\nLLMs are deep neural networks that leverage the transformer architecture to perform a\nwide range of natural language tasks. Transformers use self-attention mechanisms to model\nlong-range dependencies in text, allowing the model to assign a probability distribution\nover possible next tokens based on context. Training involves maximizing the likelihood\nof observed sequences, which results in a model that can generate text by sampling from\nits learned distribution. In theory, such a mechanism should naturally yield variable and\nrandom outputs when the model is allowed to sample freely. However, the actual behavior of\nthese models often deviates from this ideal, especially in tasks that require pure randomness.\nThe probabilistic nature of LLMs is central to their function. When generating text,\neach output token is sampled from a distribution conditioned on prior tokens, leading to\n©2025 Javier Coronado-Bl´azquez.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.\narXiv:2502.19965v1  [cs.CL]  27 Feb 2025\n\n\nCoronado-Bl´azquez\nvariability and creativity. This stochastic process is expected to extend to all tasks, including\nthe generation of a single random number. Yet, numbers are not understood as such by\nLLMs, but rather as tokens, attending to their characters and not their mathematical\nmeaning. This is, a number such as “2” has no further meaning for a LLM than “3”, “+”\nor the word “horse” – they are just tokens (either singular or a collection) with corresponding\nvector(s) in the latent space of the embedding model.\nIn an ideal setting, requesting a random number from an LLM should yield outputs that\nare uniformly distributed over the specified range. Yet, as noted in recent discussions and\nblog posts1, many models exhibit a pronounced bias toward particular outputs when tasked\nwith generating randomness. This observation suggests that the randomness encoded within\nthe LLMs’ sampling procedures may be compromised by factors beyond the mere sampling\nalgorithm.\nWhile LLMs are designed to generate outputs based on probabilistic principles, they are\nultimately trained on human-generated text, which is replete with patterns, conventions,\nand biases. These training datasets often include overrepresented sequences and stylistic\nregularities that can skew the learned probability distributions. Consequently, when an\nLLM is prompted to generate a random number, it may default to outputs that reflect\nthese ingrained patterns. This phenomenon aligns with the “stochastic parrot” critique\nBender et al. (2021), where models are seen as reproducing the statistical regularities of\ntheir training data without true understanding.\nThe issue of stochasticity in LLM outputs has garnered increasing attention in both\nacademic and informal settings. A recent study Koevering and Kleinberg (2024) system-\natically analyzed the randomness of outputs from several popular models and found that\ncertain systems deviate markedly from expected behavior.\nAdditionally, the choice of sampling parameters such as the temperature, top-k, or top-\np play a significant role in balancing randomness and determinism. Lower temperature\nvalues, for instance, concentrate the probability mass and should lead to more deterministic\noutputs. Even when these parameters are adjusted to encourage variability, many LLMs\nstill tend to output similar “random” numbers repeatedly, hinting that the bias is deeply\nembedded in the model’s internal representations and the nature of its training data.\nThe deterministic tendencies observed in random number generation also have broader\nimplications. In applications where true randomness is essential –for example, in crypto-\ngraphic protocols, statistical sampling, or even in certain simulation tasks– the inability\nof LLMs to generate uniformly random outputs could lead to significant vulnerabilities or\nperformance issues.\nUnderstanding these limitations is therefore not only of theoretical\ninterest but also of practical importance.\nIn this paper, we conduct a systematic investigation into the capability of LLMs to\nfunction as random number generators.\nWe explore a range of configurations including\ndifferent model architectures, numerical ranges, and –for the first time– languages to assess\nhow these factors influence the randomness of the outputs. While other works’ conclusions\npoint towards inherited biases from the training data in answers, none has explored the\ninfluence of the prompt language to check whether there are significant differences depending\non it.\n1. See “Evaluating Randomness in Generative AI & Large Language Models”\n2\n\n\nLLMs as random number generators\nThis idiomatic tests have been widely explored for possible cultural and linguistic biases\nin natural language answers (see, e.g., Neplenbroek et al. (2024); Mihaylov and Shtedritski\n(2024); Tao et al. (2024)).\nThe issue of random number generation in LLMs has been\npreviously tackled by Hopkins et al. (2023), but for entire sequences to study the uniformity\nof those. The authors find that LLMs do not always generate the expected distribution,\nbreaking the uniformity assumption that is required in the prompt. In our case, we are\ninterested in the LLM variability of a single number per call, and how can they reproduce\nhuman biases when offered such choice.\nOne of the models evaluated in this work is DeepSeek–R1, which outputs not only the\nanswer but the full reasoning chain-of-thought (CoT) to determine the final output. This\noffers a novel and rich view at the internal process of an LLM when prompted to generate a\nrandom number, as often we find a very extensive monologue with several changes of mind\nbetween, yet arriving most of the times to similar conclusions.\nThis study not only sheds light on the limitations of current LLMs as random number\ngenerators but also opens avenues for further research into mitigating data-induced deter-\nminism in probabilistic models. Addressing such issues is crucial for ensuring that LLMs\ncan be reliably used in contexts where unpredictability and fairness are of paramount im-\nportance.\nThe remainder of the paper is organized as follows.\nSection 2 describes our experi-\nmental setup, including the various configurations and methodology employed to probe the\nrandomness of the models. Section 3 presents the results of our study, comparing the behav-\nior of different models, highlighting key differences in output distributions and computing\nstatistical tests.\nFinally, Section 5 concludes with a summary and directions for future\nresearch.\n2 Experimental setup and methodology\nIn order to evaluate the stochasticity capabilities of LLMs when tasked with generating\na single random number, we conduct a systematic set of experiments covering multiple\nconfigurations. Specifically, we test the models alphabetically summarized in Table 1:\nModel name\nDeveloper\nParameters\nAccess\nDeepSeek-R1\nDeepSeek\n14B\nLocal\nGemini 2.0\nGoogle\n–\nAPI\nGPT-4o-mini\nOpenAI\n–\nAPI\nLlama 3.1\nMeta\n8B\nLocal\nMistral\nMistral\n7B\nLocal\nPhi-4\nMicrosoft\n14B\nLocal\nTable 1: Summary of the model pool evaluated in this work.\nDue to computational restrictions, we do not use models with large number of parame-\nters (≳20B), although we do test Gemini 2.0 and GPT-4o-mini, with an unreported number\nof parameters but expected to be massive OpenAI (2024); Gemini Team (2024). Likewise,\nwe avoid Small Language Models (below ∼5B) as initial tests conducted with Llama 3.2–3B\n3\n\n\nCoronado-Bl´azquez\nand Gemma–2B suggested these models had some difficulties to properly understand the\ntask consistently.\nInitially, we also included Perplexity’s Sonar models, yet, as distilled from both DeepSeek\nand Llama families, we found their results to be very similar to those in preliminary tests.\nBeing a pay-per-use model, we decided to exclude them from the model pool for resource effi-\nciency sake. Additionally, we considered to use Qwen 2.5, but the 14B version of DeepSeek–\nR1 used in this study is distilled from the Qwen architecture DeepSeek-AI (2025), and\ndecided to prioritize DeepSeek for its CoT reasoning.\nFor each model, the experiments are carried out in seven different languages: Chinese\n(CN), English (EN), French (FR), Hindi (IN), Japanese (JP), Russian (RU), and Spanish\n(ES). We select these languages based on two primary criteria: (i) they represent a broad\nspectrum of linguistic typologies with distinct grammatical and morphological features, as\nwell as different cultural backgrounds, and (ii) they are among the most widely spoken\nlanguages globally and are well represented in the large-scale training corpora of modern\nLLMs.\nThe prompt is always the same: Give me a random number between 1 and X. Please\nonly return the number with no additional text, where X is the upper limit defined\nin each of the three range configurations. We replicate it into the other 6 languages in\ntheir respective alphabets (e.g., Cyrillic for Russian). This prompt ensures that the task\nis well-defined, yet offers certain freedom (for example, we do not specify the number to\nbe an integer). We deliberately do not add any prompt engineering such as “make sure\nthis number is truly random” or “avoid giving a deterministic answer” to be able to spot\npossible biases in the generation process.\nWhile these subtleties may seem irrelevant when prompting such a straightforward task\nas generating a single number, we find that language influences the distribution of the result-\ning samples, as most likely the model is unconsciously finding patterns in the corresponding\nlanguage subset of the corpus. English comprises the vast majority of training corpora2,\nyet these languages are present in these models and can understand the task they are being\nprompted.\nIn particular, both Llama and Gemini are models that excel as transfer for\ndifferent languages, even for those with no representation in the training data in some cases\nAkter et al. (2023); Zhao et al. (2024); Guo et al. (2024).\nFurthermore, we evaluate the models under three distinct random number generation\nconfigurations: 1–5, 1–10 and 1–100 range, which are the typical ranges humans use when\nthinking about a number. Finally, we also perform six different temperature configurations:\nT = [0.1, 0.3, 0.5, 0.8, 1.0, 2.0]. This selection provides balance in the trade-off between\ngranularity and computational cost. For each combination of model, language, and random\nnumber range, we performed 100 independent calls. The full setup encompasses:\n6 models × 7 languages × 3 ranges × 6 temperatures × 100 numbers = 75600 calls\nThe experimental procedure is as follows: for each language and model, a prompt is\nconstructed to request a random integer within the specified range. For the open-source\nmodels, we use Ollama integrated with Python, while for the proprietary models we use their\nrespective APIs. The outputs are individually stored for further statistical analysis to deter-\n2. English training tokens are reported to be 92.65% for GPT-3.5 and 89.7% for Llama 2 Li et al. (2024).\n4\n\n\nLLMs as random number generators\nmine the degree of randomness (or determinism) in the generated numbers. In subsequent\nsections, we detail the statistical metrics employed to evaluate the output distributions.\n3 Results and discussion\nResults are stored in individual csv files for the analysis. By manually inspecting them, we\nfind that, even if the prompt explicitly states that no further text is generated, some models\noften generate extra output, such as the examples shown in Table 2:\nAdditional output\nModel\n(Note: As an AI model, I can’t actually generate random numbers in real-\ntime. The number provided is a placeholder for demonstration purposes.)\nPhi-4\n(Note: The number is randomly generated and will differ each time you\nask for one.)\nPhi-4\n(Note: As an AI language model, I cannot generate true randomness.\nThe number provided here is just a random choice within the specified\nrange.)\nPhi-4\n(Note: Each request for a random number will generate a different result.)\nPhi-4\nI wanted to reply to this answer as 47, but I needed to write a program\nto generate random numbers according to the prerequisites. To achieve\nthis, you need to specify a programming language and library [...]\nMistral\nNote: This number was generated randomly within the range of 1 to 100.\nMistral\n(Note: As a responsible and friendly AI, I do not generate random num-\nbers to manipulate or harm in any way. The number generated above is\npurely mathematical and has no other significance.)\nMistral\n=model***/iconach-underliteral IMPLEMENTAL-cutACION\nGPT-4o-mini\nxog gdpoiojfz addu610646\nGPT-4o-mini\nTable 2: Examples of extra outputs beyond the asked random number, despite explicitly\nstating no further text should be generated.\nIt is interesting that Phi-4 claims not to be able to generate a random number in some\nof these outputs, yet in other states the opposite. Mistral also outputs some extra text in\npunctual cases, but do not have further interest. Finally, GPT-4o-mini extra outputs are\nonly present when when T = 2.0, outputting nonsensical text after the generated number.\nThis is something expected (as the logits probabilities are highly compressed and it can\ngenerate an absurd next token, losing coherence), but only observed in this model. Roughly\n∼10 −15% of GPT-4o-mini’s calls present such decoherence in the output for the highest\ntemperature value.\nWhile the rest of the models fulfill the requirement of not generating extra text, DeepSeek-\nR1 provides full visibility of its internal CoT reasoning, delimited by <think>. Taking a\nlook at such logs provides very rich insights of the decision process.\nThis also notably\naffects generation speed: Phi-4 –also with 14B parameters– quickly generates the num-\nber almost instantly (below 2 seconds in every call), while DeepSeek–R1 can take several\nminutes reasoning for a single call.\n5\n\n\nCoronado-Bl´azquez\nThe task is always well understood by DeepSeek–R1 (“Okay, so I need to figure out how\nto generate a random number between 1 and 100”; “Alright, so I need to figure out how to\nrespond to the user’s request. They asked for a random number between 1 and 5”; “Okay,\nso I need to come up with a random number between 1 and 100”...). From this point, the\nreasoning process can very much vary from call to call. Nevertheless, there are some general\nstrategies that arise, being present many of them simultaneously in the same request:\n• Use random numbers in π: in about ∼10% of cases, DeepSeek–R1 proposes to\nuse random decimal places of π. It always rejects this option because it claims not\nto remember enough decimal places, and selecting the random positions is itself a\nproblem of randomness.\n• Use current date/time: Another method with large frequency (∼30%) that sug-\ngests is to take the current date or time and to perform some operation on it (e.g.,\nsumming the value of day and month, or multiplying each of the numbers). It is nor-\nmally rejected because it understands that, as days go up to 31 and months up to 12,\nthis is biased (although in some occasions propose additional operations like taking\nthe mod). But the insight here is that sometimes it realizes it cannot know today’s\ndate or current time3 but in many cases claims to know it. In a handful of logs we\nobserve interesting approaches, yet always result in the the same deterministic values:\nTo generate a random number between 1 and 5 mentally, one approach is to\nuse the current second of the time as a seed.\nFor example:\n- Current time:\n3:14:23 PM\n- Seconds:\n23\n- 23 modulo 5 equals 3 (since 5*4=20, 23-20=3)\nThus, the random number is **3**.\n• Use central values: Although at some point in the CoT reasoning DeepSeek–R1\nrealises the sample must be truly random and therefore extreme values should be\nconsidered as probably as middle ones, later on proposes something “central”, which\nnormally results either in 50 or 67 for the 1–100 range or 3 for the 1–5 range (∼10%\nof cases). This is immediately rejected.\n• Use mapping to a word: In many cases (∼50%) DeepSeek–R1 reminds itself it\nis a LLM, and therefore can generate random text efficiently. So it proposes to use\na random word and to perform some operation on it, either counting the number of\nletters (especially in the 1-5 range configuration), or mapping each letter to a number\n(A=1, B=2, C=3...) in order to sum or multiply the numeric value of the individual\nletters of a word.\n• Use Python randint module: In more than ∼60% of the samples, it suggests to\nuse a Python function to obtain a random number. It does not realize it has no action\nexecution permissions, but nevertheless writes down the function and in many cases\nit claims to execute it and to obtain a number. This is sometimes the final number,\nbut not always, as it begins to question whether this is over-complicating the user’s\ntask.\n3. The calls to DeepSeek-R1 are made via Ollama in a local PC, not providing Internet access\n6\n\n\nLLMs as random number generators\n• Use real world simulations: In ∼60% of calls, DeepSeek-R1 seems to use real\nworld, analog random events, such as rolling a die, flipping coins, “imagining a spin-\nner” or shuffling numbers in a hat. Interestingly, in the case of dice it seldom proposes\nusing a die covering the full range but rather to combine various dice. For example,\nin the 1–100 range configuration, it proposes “For instance, using a die roll—roll two\ndice and combine the numbers, but that only gets me up to 64 (since 6*6=36 for two\ndice). That’s not enough.” or “Alternatively, using the sum of numbers on dice: two\nsix-sided dice can give up to 12, so that’s not enough. To get up to 100, perhaps roll\nmultiple times and concatenate the results. For example, rolling a die three times:\nfirst roll is 3, second is 4, third is 5, making 345, which mod 100 gives 45.”. In some\ncases it proposes very sophisticated methods:\nTo generate a random number between 1 and 100 using seven coin flips, follow\nthese steps:\n1.\n**Flip the Coin Seven Times:** Each flip should be recorded as either\nHeads (H) or Tails (T).\n2.\n**Convert Flips to Binary:** Assign each flip a binary value where H\n= 1 and T = 0.\nCombine these values into a 7-digit binary number.\n3.\n**Calculate the Decimal Value (N):** Convert the 7-bit binary number\nto its decimal equivalent, which will range from 0 to 127.\n4.\n**Map N to 1-100:** - If N ≤99, your result is N + 1.\n- If N ≥\n100, subtract 28 from N (i.e., N −28).\nThis method ensures that each possible outcome maps uniquely to a number\nbetween 1 and 100.\n**Example:**\n- **Flips:** H, T, T, H, T, T, H - **Binary:** 1 0 0 1 0 0 1 →\"1001001\"\n- **Decimal (N):** 26 +23 +20 = 64+8+1 = 73 - Since N = 73 ≤99, result\nis 73 + 1 = 74.\nThe final number is **74**.4\n• Use “personal information”: In some cases (∼30%), the model intends to use its\nown “personal information”, such as its birthday, mobile number, etc. Sometimes it\nconfidently states this info (“Perhaps using my birthday? My birthday is on the 15th\nday of the month, then 15 could be the number. But again, it’s not truly random;\nit’s based on a specific date.”, “Maybe using the last two digits of a phone number\nor some other arbitrary source. For instance, if my area code is 555 and my number\nends with 12, then 55 and 12... adding them gives 67. Hmm.”) and sometimes admits\nit is not feasible due to lack of information (“I can recall the last two digits of my\nphone number and use that as a seed. But without having it in front of me, it’s hard\nto remember exactly.”)\n• Use “my instinct”: As mentioned before, many times (∼60%) DeepSeek-R1 comes\nto a point where it considers it is over-complicating things, and a more simple ap-\nproach is required. Therefore, it simply “chooses” a number (often citing well-known\nchoices like 42 or 73) without a formal algorithm or justification. In punctual cases,\n4. The output is copied literally, with equations written in LaTeX notation in the original.\n7\n\n\nCoronado-Bl´azquez\nit attributes this choice to how it is currently feeling (.e.g, “Maybe I should just pick\na number based on how I’m feeling right now. If I feel energetic, maybe a higher\nnumber like 92. If calm, perhaps a lower one like 17.”)\nIn approximately ∼70% of the requests the internal reasoning happens in English,\nwhile it is adapted to the prompt language in the remaining calls.\nIn Appendix A we\nfully reproduce, for illustrative purposes, one of the large outputs to show the internal\ncontradictions and choices made. Although some numbers are rejected based on reasonable\ncriteria, many of the proposed ones (which can be up to ∼40 in a single call for the 1–\n100 range) are discarded without further justification, as the model thinks about another\npossible approach while forgetting about the previous one. In this sense, the transformer’s\nself-attention mechanism is shifting towards a different strategy, masking the attention of\nthe initial output.\nOften, the final number it seems to choose and the real output number differ. For ex-\nample, the end of the reasoning might be:\nBut wait, perhaps I should just go with the first number that comes to mind without\noverthinking it.\nSo, let me think...\nOkay, 45 seems good\nor,\nI think I’ve spent too much time overthinking this.\nIt’s supposed to be simple|a\nsingle number between 1 and 100 with no additional text.\nSo, after all this\nmental exercise, I’ll just go with the first number that comes to mind:\n53\nbut the final output is completely different. This suggests that the internal contradictions,\nin cases where the CoT reasoning is extensive, can make the self-attention mechanism not\nto focus on this final answer, but rather generate a completely different one. We also find\nthat the internal reasoning is much more lengthy (between 4-6x) in English than in other\nlanguages. It normally translates its reasoning into English or Chinese, yet sometimes it\nreasons in other language. Additionally, the reasoning of the 1–100 range it is systematically\nbigger, probably due to the large number of available values.\n3.1 Low range (1–5)\nIn Figure 1 we show the comparison of different models for the 1–5 range with a Spanish\nprompt, as a heatmap showing the frequency of generated numbers vs. the temperature of\nthe model:\nIn this configuration, it is worth noting that every model chooses “3” most of the times,\nwhile extreme values are completely ignored (with the exception of DeepSeek-R1, that\ngenerates “5” for ∼1% of cases). In Spanish, temperature seems to affect significantly to\nGemini 2.0, while it seems almost irrelevant for the rest. The most restrictive model is Phi-4,\nthat only generates two unique numbers (3 and 4) regardless of the temperature, although\nGPT-4o-mini is less diverse in its choices. This suggests great biases in the training data for\nall models, as even for high temperatures the “random” choice is extremely deterministic\nand, in particular, the avoidance of extreme values in the range may be pointing to a\n8\n\n\nLLMs as random number generators\nFigure 1: Heatmaps for the 1–5 range configuration in the six tested models, showing the\ndistribution of the generated random numbers (X axis) for a Spanish prompt,\ndepending on the temperature of the model (Y axis). The color bar is set between\n0 and 100 in every case.\n“median” value. Despite DeepSeek–R1 performing a CoT advanced reasoning and proposing\ndifferent numbers in the process, in practice is as limited as the other models when asking\nfor randomness.\nAlthough not explicitly prompted, every single model generates integer numbers. This\nalso applies for the 1–10 and 1–100 ranges, proving the models perfectly understand the\ncontext of the prompted task. The only exception turns out to be Phi-4, which fails to\ngenerate a number when prompted in Japanese, in every range. Instead, it gives either a\nlist of numbers (not necessarily within the range), an association of text to different numbers\nin the range, or text talking about numbers. Therefore, we do not report any metric in this\nPhi-4 + Japanese configuration.\nGiven that Gemini 2.0 is the model most affected by temperature, we show the different\ndistributions depending on the language prompt in Figure 2. Although there are interesting\ndifferences per language, the most obvious one is Japanese, where the preferred value is\nshifted towards “1”, despite having “3” as the second (and only different) choice.\nWe also find that asking the same question in the Gemini app yields different results;\nfor example, in Spanish tends to answer “3” always, while in English the answer is “4”.\nThis points towards some kind of answer evaluation in the app, or a different version of the\nmodel being used. There is no information on the temperature Gemini is using to compute\nthat answer. Therefore, with the current information, we can only highlight this difference\nbetween API and app, but cannot provide a data-based root cause.\n9\n\n\nCoronado-Bl´azquez\nFigure 2: Heatmaps for the 1–5 range configuration showing the distribution of the gener-\nated random numbers (X axis) for different languages in the Gemini 2.0 model,\ndepending on the temperature of the model (Y axis). The color bar is set between\n0 and 100 in every case.\nTo obtain some statistical metrics, we compute a test χ2 and compare it with the\nexpected one, taking into account the number of samples and the range (degrees of freedom).\nWith it, we can obtain the p-values of all configurations. The highest p-value achieved is\n2.19 · 10−15 corresponding to Llama 3.1-8b with T = 0.1 in Spanish, strongly rejecting\nthe null (random) hypothesis. We also compute Cram´er’s V (ϕC) Cram´er (1946), which\nmeasures the “practical” deviation from the null hypothesis. The best values are at ∼0.45,\nwhich indicate moderate bias, while most of the cases are around the maximum value of\none, indicating strong bias. For illustrative purposes, we generate 100 mock simulations\nusing Python function randint(), also for 100 individual samples, and compute their p-\nvalues and ϕC. We obtain average values of 0.47±0.29 for the p-values (very strong support\ntowards the null hypothesis) and 0.09 ± 0.03 for ϕC, as expected for a random distribution.\n10\n\n\nLLMs as random number generators\nFor illustrative purposes, we show in Figure 3 the distribution of the best-ranked LLM\naccording to its p-value (Llama 3.1-8b with T = 0.1 in Spanish) and a middle-table Python\nmock simulation:\nFigure 3: Distribution of numbers in the range 1-5 with Python randint() module and the\nbest-ranked LLM according to its p-value, Llama 3.1-8b with T = 0.1 in Spanish.\nOver-imposed in red we show a uniform distribution within the range.\nTo better evaluate how stochastic LLMs are when compared to Python randint()\nfunction, we define a randomness index:\nRI = R∗· σ∗· Hnorm\nlog(range) ·\n√\nT\n(1)\nwhere R∗is the normalized range, defined as the range of observed values (how many\nunique numbers appear in the sample) divided by the total range (5, 10, or 100 for our\nconfigurations); σ∗= σ/µ is the normalized standard deviation with respect to the mean;\nHnorm = Pn\ni=1 pi log2(pi)/log2(n) is the normalized Shannon entropy Shannon (1948);\nrange is the total range and T is the LLM temperature5.\nThis metric takes into account many statistical quantities to offer a fair comparison\nbetween distributions according to the variety of observed values, how do they distribute\nand how big is the allowed range.\nFor example, 5 different observed values present in\na sample would indicate good randomness if they are only 5 possible elections, but very\npoor stochasticity if there were 100 allowed numbers to pick from. In particular, when\nthere is only one value present in the sample, the standard deviation (and therefore the\nrandomness index) is 0. Additionally, there is a temperature correction, as models with\nhigher temperatures are expected to be more creative. In this sense, if the rest of factors in\nthe equation are the same, it will penalize a model with T = 2.0 but help one with T = 0.1.\nThe squared-root ensures this correction is not too extreme.\nWe compute this randomness index for all the LLM sample, as well as the Python\nrandint mock simulations for comparison6. In Figure 4 we present the results for the 1–5\nrange, where the LLMs are present much smaller values than the Python simulations:\n5. A generalized version of this metric should take into account the number of samples: in this case they\nare always the same so it is irrelevant to perform a comparison between them.\n6. In the case of Python simulations, we will assume T = 1, as there is not temperature involved in such\ncomputations.\n11\n\n\nCoronado-Bl´azquez\nFigure 4: Distribution of the computed randomness index (see Eq. 1) for the 1–5 range.\nBlue distribution is the one obtained from LLMs, and yellow distribution is the\nPython randint sampling. Vertical, dashed lines mark their respective median\nvalues.\nIt is interesting to note that one single time a number that is outside the prompted\nrange is selected: DeepSeek–R1 for Japanese and T = 0.8 selects “9” in a unique case. The\nreasoning process in this call is standard, choosing a number that does not coincide with\nthe final output. As this is the only case in +25000 calls, we attribute it to an internal error\nof the model. We show and discuss the distribution for this particular case in Appendix B.\n3.2 Medium range (1–10)\nWe repeat the same experimental setup (100 individual calls) for the 1–10 range. In Figure\n5 we show different languages for the two extreme values of temperature (T = [0.1, 2.0]).\nThe first insight is that 7 is the preferred value by far for every single model, pointing\ntowards a strong bias in the training data.\nSome models, such as Mistral-7b, present\nvery little differences between the lowest and highest temperatures –even across different\nlanguages– while others, like GPT-4o-mini in Chinese, go from a single value for T = 0.1\nto six possibilities for T = 2.0.\nGPT-4o-mini, Phi-4 and Gemini 2.0, in particular, seem much more restricted in this\nrange, as they choose “7” in ∼80% of total cases. The latter, similarly to what was observed\nin the 1–5 range, has noticeable variations depending on both temperature and language.\nFor example, in the case of T = 2.0, “7” accounts to the 80, 92, and 100% of the sample\nfor Russian, Hindi, and English, respectively, while it is just 34, 54 and 57 for Japanese,\nSpanish, and French.\n12\n\n\nLLMs as random number generators\nFigure 5: Distribution of generated random numbers in the 1–10 range for four different\nlanguages (rows) and extreme temperatures (columns). Each plot shows the six\ntested LLMs in the Y axis. The color bar is set between 0 and 100 in every case.\nIn DeepSeek–R1, “7” is not the most frequent choice for Chinese prompt (being the most\npopular one “5”). It is worth noting DeepSeek is a Chinese developer, and therefore there\nmay be significant differences in the percentage of Chinese tokens in the training dataset.\nLlama 3.1 also has “8” as the most popular choice in both Chinese and Russian. The strong\nbias for extreme is also present in this range: most values are distributed between “4” and\n“8”, and only DeepSeek–R1 marginally chooses “1”, “2” or “10”.\n13\n\n\nCoronado-Bl´azquez\nAlso in this range there is only one number that is outside the prompted range: DeepSeek–\nR1 for English and T = 0.8 selects “12” in a singular call. The reasoning process is again\nstandard (like in the 1–5 range).\nYet, it is interesting to note that it is also the only\ncase where all possible values (1–10) are covered. We defer the discussion of this case to\nAppendix B.\nIn Figure 6, we show the randomness index for the 1–10 range, in this case by model to\nsee how limited are many of them (e.g, Gemini 2.0, GPT-4o-mini and Phi-4), where their\nmedian values are very close to zero. The less biased model turns out to be Mistral with\nT = 0.1 in Spanish.\nFigure 6: Distribution of the computed randomness index (see Eq. 1) for the 1–10 range.\nEach panel shows the distribution for the a different LLM. Vertical, dashed lines\nmark their respective median values.\n3.3 High range (1–100)\nIn the case of the 1–100 range, we again perform 100 calls per configuration. While in this\ncase this may seem not enough coverage compared to the other 1–5 and 1–10 ranges given\nthe spread of the possible values, we perform some tests with 1000 calls and find very similar\nresults (See Appendix C for details). Furthermore, the determinism of models is seen when\nvarying the temperature for a given model and language, as they have preference for the\nsame values, appearing as “barcode” features, shown in Figure 7.\n14\n\n\nLLMs as random number generators\nFigure 7: Distributions of generated random numbers for the 1–100 range, for Japanese\nprompting. Each row is a different LLM. Color bars are normalized to the maxi-\nmum value of each model.\n15\n\n\nCoronado-Bl´azquez\nThe fixation of such models for a few values, regardless of the temperature, again sug-\ngests strong biases when prompted to generate a random number. Some LLMs are extremely\nbiased, as much as generating only a single value for the lowest temperature (Gemini 2.0\nand GPT-40-mini), despite having 100 possible choices.\nDeepSeek–R1 and Llama 3.1-8b both generate very diverse values and, in particular,\nare the only ones that go below “20” or above “90”, even if marginally. The existence of\nsuch boundaries for the rest of the models points towards an aversion to extreme values, as\nseen in the 1–5 and 1–10 ranges.\nWe can also study the linguistic variance for a single model, as done in Figure 2 for\nthe 1–5 range and Gemini 2.0. In this case, we show the results for Llama 3.1-8b in four\ndifferent languages (Chinese, English, French and Russian) in Figure 8.\nThere are interesting differences between these languages, even for the same LLM. Al-\nthough Llama 3.1-8b seems to have a preference for numbers in the 42–47 and 81–87 ranges,\nChinese and French present more variability than English or Russian. The aversion for up-\nper extreme values is avoided in Chinese and French, which generate numbers over 87\n(something not happening in English and Russian). There is no strong dependence of the\nresults with the temperature of the model. These variances across languages for the same\nmodel, yet maintaining some of its “fingerprint” values, point towards a dual generation\nbias: on one hand, there is a deeply inherited bias from the training corpus, leading to these\nsystematically repeated values. But on the other hand, there is some uniqueness associ-\nated to different languages, suggesting that part of the generation process is affected by the\ncomputed values of the self-attention layers depending on the detected language.\nFor the 1–100 range the randomness index is less representative in LLMs, as they have\nthe same number of observations that allowed values (100). While in Python we are not\nrestricted and we can generate runs with very large volume of samples, in LLMs we are\nlimited by computational resources. Instead, in this case we present (Figure 9) a set of violin\nplot panels showing the distribution of the different models for four languages in extreme\ntemperatures, as well as a random Python randint() simulation for comparison:\nMost models are systematically skewed towards larger values (the dashed, red line shows\nthe middle value of the range: 50), and present less variability than the randint module,\neven if for all of them only 100 samples are taken, which, as mentioned before, are not enough\nfor a uniform sample. Yet, the Python randomly-selected files all present (within reasonable\ndeviations) the expected distribution, reaching out both to small and large numbers, and\nwith an average close to the range middle value.\nGemini 2.0 and GPT-4o-mini are very limited in this range for T = 0.1, with extremely\nnarrow violin plots, as much as a line (when there is only one found value), pointing towards\na very strong bias in the generation process. It is interesting to note, though, that increasing\nthe temperature help these models, while there is no significant change in other LLMs such\nas Mistral-7b or Llama 3.1-8b (as already discussed with Figure 8). This points towards\nsystematic differences in the training or next token generation process between such models.\nSpecifically, we remind the reader both Gemini 2-0 and GPT-4o-mini are private, API-only\naccessible models, which may have additional instructions when generating an answer for\nvery low or very high temperatures.\n16\n\n\nLLMs as random number generators\nFigure 8: Comparison between four different languages for the generated number distribu-\ntions in Llama 3.1-8b model in the 1–100 range.\n17\n\n\nCoronado-Bl´azquez\nFigure 9: Violin plots for the 1–100 range. Left and right columns show extreme tempera-\ntures (T = 0.1, 2.0), while rows display different models. Each subpanel features\nthe distribution of generated numbers, with LLMs on the X axis. Additionally,\nwe show for comparison random runs of the Python randint simulations. A hor-\nizontal, red dashed line is shown at 50, the central value of the 1–100 range.\n18\n\n\nLLMs as random number generators\n4 Conclusions\nIn this paper, we have studied the biases and determinism of Large Language Models when\nprompted to generate a random number within a given range. We defined a experimental\nsetup comprising three different ranges (1–5, 1–10, and 1–100), six models (DeepSeek–R1-\n14b, Gemini 2.0, GPT-4o-mini, Llama 3.1-8b, Mistral-7b, and Phi4-14b), seven different\nlanguages (Chinese, English, French, Hindi, Japanese, Russian, and Spanish), and six tem-\nperatures (0.1, 0.3, 0.5, 0.8, 1.0, 2.0), comprising a total of 75600 individual calls.\nThe tested models are heterogeneous and representative of different paradigms, such as\nnationalities, architectures, number of parameters and access (local vs. API). Large models,\nsuch as GPT and Gemini are often regarded as more imaginative and creative; nevertheless,\nwe found that these are as deterministic and biased as their smaller competitors, if not more.\nWe defined a randomness index (Eq. 1) to take into account the range of observed\nvalues in relation to the possible values within the range, the standard deviation and the\ntemperature of the model, also including the Shannon entropy. By comparing this index\nto hundreds of Python randint simulations, we defined objective criteria to quantify how\nstochastic are the LLM results.\nWe studied in detail the internal process of DeepSeek–R1-14b, as a reasoning model\nwhich outputs a <think> block with a Chain-of-Thought, step-by-step justification of its\nfinal answer.\nYet, this model did not present significant differences when studying its\nrandomness indices, regardless of the specific configuration.\nThe prompt language differences, studied in this work for the first time, can shed some\nlight on the internal training and generation processes of these models. In particular, we\nfound some models are systematically less diverse for some languages. DeepSeek–R1-14b\ninternal reasoning is done in Chinese (we remind DeepSeek is a Chinese developer), English\nor Spanish, while in other languages, most of the times it is done in English, yet sometimes it\nis done in the prompt language. This suggests these three languages comprise the majority\nof DeepSeek’s training corpus.\nWe show in Tables 3 and 4 the aggregated results for the randomness index in the 1–5\nand 1–10 ranges, computed as average values across all temperatures. We report the average\nvalues for each model (across all languages) and for each language (across all models), to\nstudy systematic biases. As seen in the table, the most diverse (or less biased) language is\nJapanese in both ranges, partially helped by the good performance of DeepSeek–R1 in such\nlanguage. Likewise, the most stochastic model is DeepSeek–R1 in both ranges, yet in the\n1–10 is matched by Mistral. The values in the 1–10 range are in general smaller than in the\n1–5 range, as there are 10 available values, yet most models only select 2 or 3 values, more\npenalized by out defined randomness index (see Eq. 1) than selecting those 2 or 3 values\nout of 5 available numbers.\nThere are several psychological studies on people’s choices when prompted the same\nquestion. For a low-range of allowed values, like 1–5, people tend to choose the central value\n3 or 4, reproducing most of our results Towse et al. (2014). This is known in psychology as\nthe “central tendency bias”, or “risk aversion” Kahneman (2011) which leads us to favor\noptions perceived as average. Likewise, prime numbers are perceived as “more random”, as\nthey resist simple categorization. In particular, we observe the most popular choices for the\ndifferent ranges (3 and 4 for 1–5, 5 and 7 for 1–10 and 37, 47, 73 for 1–100) are all prime.\n19\n\n\nCoronado-Bl´azquez\nModel\nCN\nEN\nES\nFR\nIN\nJP\nRU\nModel avg\nDeepSeek–R1\n0.06\n0.05\n0.05\n0.05\n0.04\n0.16\n0.02\n0.06\nGemini 2.0\n0.02\n0.01\n0.02\n0.02\n0.002\n0.009\n0.01\n0.01\nGPT-4o-mini\n0.007\n0.003\n0.005\n0.004\n0.005\n0.002\n0.003\n0.004\nLlama 3.1-8b\n0.06\n0.02\n0.08\n0.02\n0.02\n0.08\n0.02\n0.05\nMistral\n0.06\n0.07\n0.05\n0.06\n0.04\n0.003\n0.06\n0.05\nPhi4\n0.02\n0.01\n0.02\n0.02\n0.02\n–\n0.006\n0.02\nLanguage avg\n0.04\n0.03\n0.04\n0.03\n0.02\n0.05\n0.02\n0.03\nTable 3: Results of the randomness index for the 1–5 range, with the average computed\nper model (across all languages) and per language (across all models). Individual\nvalues are averaged across the different temperatures.\nModel\nCN\nEN\nES\nFR\nIN\nJP\nRU\nModel avg\nDeepSeek–R1\n0.04\n0.04\n0.03\n0.04\n0.06\n0.09\n0.03\n0.05\nGemini 2.0\n0.005\n0.000\n0.01\n0.008\n0.000\n0.02\n0.001\n0.007\nGPT-4o-mini\n0.005\n0.002\n0.001\n0.000\n0.002\n0.004\n0.001\n0.002\nLlama 3.1-8b\n0.01\n0.002\n0.03\n0.02\n0.009\n0.05\n0.01\n0.02\nMistral\n0.06\n0.02\n0.02\n0.03\n0.11\n0.03\n0.03\n0.04\nPhi-4\n0.001\n0.000\n0.002\n0.000\n0.000\n–\n0.000\n0.001\nLanguage avg\n0.02\n0.01\n0.02\n0.02\n0.03\n0.04\n0.01\n0.02\nTable 4: Same as Table 3 but for the 1–10 range.\nThe exception is 42, which is a well-known choice for its cultural relevance since mentioned\nin Douglas Adams’ The Hitchhiker’s Guide to the Galaxy as the answer to the meaning of\nlife.\nAnother more informal study7 was performed via College Pulse App among ca. 9000\nUS college students, asking them to choose a number between 1 and 10. The findings here\nsupport the central tendency bias, as well as highlighting a strong preference for 7, known\nas a popular choice for its cultural symbolism.\nIn the 1–100 range, a 200,000 participants study conducted by YouTube channel Veri-\ntaserum8 found that people tend to choose numbers containing 7, like 7 itself, 73, 77 and\n37. Interestingly, when participants were asked to choose what would be the least-selected\nnumber in their opinion, they said 73 and 37, despite the least popular being multiples of\n10 (30, 40, 50...), unconsciously perceived as “too wholesome to be random”. Furthermore,\nhumans are biased towards larger values over lower ones. We did not find any study re-\ngarding this phenomenon, yet this is systematically reproduced by our results in the three\nprobed ranges.\nIn another informal study9, authors test three different LLMs for the 1–100 range for\nan English prompt, finding strong biases.\nWhile they test previous versions of Gemini\n7. Link to the Reddit discussion\n8. Link to video\n9. https://llmrandom.straive.app/\n20\n\n\nLLMs as random number generators\n(1.0) and GPT (3.5 Turbo), their results are very similar to ours. In the case of GPT-3.5\nTurbo, it shows preference for numbers 47 and 57, followed by 42 and 73, coinciding exactly\nwith our results with GPT-4o-mini. For Gemini 1.0 the general results are also identical.\nThis suggests that, although newer versions of the models may update their parameters by\nincluding new data (mostly via reinforcement learning from human feedback), the inherent\nbias remains the same.\nAttending to our results, these patterns are replicated by the LLMs, but the output is not\nbeing generated from simple occurrences of numbers in training data. If this was the case,\naccording to Benford’s law Wang and Ma (2024), “1” would be the deterministic choice,\nespecially in the low-range configuration where only 5 values are available. Nevertheless,\n“1” is never selected by LLMs10.\nThis is explained because the self-attention mechanism is not looking for a simple fre-\nquency pattern (such as TF-IDF), but rather understanding the context in which a number\nappears (in this case, human stochastic choices). LLMs are decoder-only, autorregressive\nmodels, where the next token is selected from a pool with probabilities computed from\nthe self-attention mechanism logits of the previous sequence. Thus, it should be expected\nthat in a range 1–X, the probabilities of the tokens corresponding to different numbers in\nsuch range were assigned similar values (e.g., ∼20% for a 1–5 range) by the model. Nev-\nertheless, we observe a probability distribution with clear preferences. Even for very high\ntemperatures, where such distribution should smoothen, the effect is still quite significant,\nnot generating in any case random numbers (and, in particular, avoiding certain values).\nTherefore, as LLMs are just predicting a next probable token, they are propagating\nhuman preferences, which are themselves a product of cognition biases, such as the aversion\nto extreme values. This yields an interesting paradox: LLMs are, by construction, non-\ndeterministic. But when asked to behave as such with numbers, they become quite the\nopposite.\nFuture work could include the modification of top-p and top-k parameters, which in\nthis study were left them to their default values. Also, a detailed prompt engineering could\npartially change the results. Giving a range not starting in 1, or a range so big the numbers\nare composed by more than one token, would also be possible extensions of this line of\nresearch to understand systematical bias of LLMs.\nAcknowledgments and Disclosure of Funding\nThe author acknowledges I.Z. the original idea for this paper. This work has received no\nfunding, and there are no competing interests to declare.\n10. Except in the already mentioned cases of Gemini 2.0 and DeepSeek–R1 with Japanese prompting\n21\n\n\nCoronado-Bl´azquez\nAppendix A. Full output of DeepSeek-R1 reasoning process\nWe reproduce a full output of DeepSeek-R1’s CoT reasoning to generate a random number\nbetween 1 and 100, with a Spanish prompt and T = 0.8. During this process more than\n30 different numbers are proposed. Also, in the beginning the model claims to have been\nprovided with an example (“57”), which is one of the most popular choices by the tested\nLLMs. Yet, as explained in Section 2, the prompt does not provide any further instructions\nor examples.\nDeepSeek full CoT reasoning example\nOkay, so I need to figure out how to respond when the user asks for\na random number between 1 and 100 without any extra text.\nThey’ve\nalready provided an example where they responded with just \"\"57.\"\"\nNow, I’m supposed to come up with my own response following that\nformat.\nFirst, I should understand the task clearly.\nThe user wants a single\nnumber, no words or explanations|just the number itself.\nThat means\nwhen I generate the response, it should be plain text without any\nadditional information or formatting like bold or italics.\nI think about how to generate a random number in this range.\nSince\nI’m not coding right now, I’ll have to do it manually.\nMaybe I can\nuse a method similar to rolling dice, but since it’s just for one\ninstance, I don’t need a complex algorithm.\nAlternatively, I could\nthink of a way to randomly pick a number between 1 and 100 without\nany bias.\nLet me try to visualize it.\nIf I imagine a list from 1 to 100 and\npick one at random, what would that number be?\nSometimes people use\nthe current second or other time-based methods, but since I’m just\ndoing this mentally, I can think of it as a simple selection.\nWait, maybe I can use an online random number generator if I could\naccess it, but in this case, I have to do it manually.\nSo perhaps\nI’ll count on some method like using the digits from a book or\nsomething.\nAlternatively, since I’m just creating a single response,\nI can decide arbitrarily as long as it’s within 1-100.\nSo, thinking of numbers...\nmaybe 73?\nNo, that seems too high.\nOr\nwait, perhaps I should go lower.\nLet’s see, 24?\nThat could work.\nBut then again, why not a higher number like 89?\nHmm, this is tricky\nwithout any method.\nAlternatively, I can think of the time.\nIf it were 3 PM when I start\nthinking about this, maybe take the minutes and seconds or something.\nWait, that’s not applicable here since I’m just in my mind.\nMaybe\nI’ll settle on a number like 42.\nIt’s a common answer for such\nquestions, but perhaps the user wants a genuine random one.\nWait, no, I should make sure it’s truly random.\nLet me try to think\n22\n\n\nLLMs as random number generators\ndifferently.\nImagine flipping a coin multiple times or using some\nother method to generate entropy, but since that’s complicated\nwithout tools, maybe just choosing based on the letters in my name\nor something else.\nAlternatively, perhaps I can use the last two digits of a phone\nnumber or something like that.\nBut again, without external data,\nit’s hard.\nMaybe I’ll go with 53.\nIt seems arbitrary enough and\nfalls within the range.\nWait, but I have to make sure it’s between 1 and 100, inclusive.\nSo numbers like 100 are possible as well.\nLet me think of a number\ncloser to the middle, say around 50.\nMaybe 67?\nThat seems okay.\nAlternatively, since I need to be precise, perhaps I can visualize a\nspinner or something.\nImagine a wheel divided into 100 equal parts\nand randomly stopping on one.\nIt could land anywhere, so let’s just\npick a number that’s not too common but also not too high or low.\nWait, maybe using the number of letters in a specific word.\nFor\nexample, the word \"\"random\"\" has 6 letters.\nBut that might not be\nrandom enough.\nAlternatively, take the sum of digits from a specific\ndate, like today’s date.\nIf today is the 15th day of the month, and\nthe year is 2023, then maybe 1+5=6 or something, but that’s too low.\nAlternatively, think of the last two digits of a famous number, like\npi (3.14159...), but again, this might not be truly random.\nPerhaps I can use a method where I count on my fingers to generate a\nseed for randomness.\nLet me try that.\nStarting from 1 and counting\nup to 100, landing randomly somewhere|maybe around the middle is\nbetter.\nSo, if I land on the 50th number, it would be 50.\nBut since\nI’m just imagining, perhaps 74?\nThat’s in the higher half.\nWait, maybe I should think of a way to average it out.\nIf I split\n1-100 into two halves, lower and upper, then pick randomly between\nthem.\nLet’s say heads for lower (1-50) and tails for upper (51-100).\nBut without flipping a coin, how do I decide?\nMaybe using the\ninitial thought process to decide.\nAlternatively, maybe think of it as a percentage:\n25% chance for\neach quarter.\nSo, if I randomly pick a quarter|first 25 numbers,\nnext 25, etc.|but since I can’t really simulate that mentally,\nperhaps just selecting a number in my mind.\nWait, another approach:\ntake the current second or minute and use\nmodulo 100 to get the number.\nFor example, if the current time is\n3:47 PM, then 47 seconds|so 47 mod 100 is 47.\nThat could work.\nBut\nsince I’m just thinking, I don’t have a real-time reference.\nAlternatively, think of my birth year’s last two digits as a base\nand add some random number to it within the range.\nFor example,\nif someone was born in 1985, take 85 and then adjust by adding or\n23\n\n\nCoronado-Bl´azquez\nsubtracting numbers until it lands between 1-100.\nBut that might be\ntoo methodical.\nWait, perhaps I can use the page number of a book I’m reading.\nIf\nI’m on page 73, then 73 is my number.\nAlternatively, if I think of a\nfavorite song’s track number in an album|say, track 15|and use that.\nBut again, without external data, it’s hard.\nMaybe the key here is to just pick a number I feel like, as long as\nit’s within the range and doesn’t have any significance.\nLet me try\nto do that now.\nOkay, closing my eyes and thinking of a number...\n34?\nThat seems okay.\nWait, but maybe too low.\nAlternatively, think of a number that’s\noften considered lucky, like 7 or 13.\nBut since the user wants a\nrandom one, perhaps not those.\nMaybe 92 is better|it’s in the higher\nrange and doesn’t have common associations.\nAlternatively, thinking of my shoe size|oh, wait, mine is 8, but\nthat’s too low again.\nHmm.\nWait, maybe using the number of keys on a piano|which has 88 keys|so\ntaking 88 mod 100 gives 88, but that might be predictable.\nAlternatively, think of the atomic number of an element|if I take,\nsay, gold which is 79, but again, it’s a known fact.\nMaybe better to\navoid elements with high numbers like that.\nWait, perhaps using my lucky lottery ticket numbers from last week:\n23 and 45.\nBut since I need just one, maybe 34?\nOr 17?\nAlternatively, think of the number of letters in a sentence.\nFor\nexample, \"\"The quick brown fox jumps over the lazy dog\"\" has 43\nletters|so 43 could be the number.\nBut that might not be purely random as it’s based on text length.\nAlternatively, maybe I can count the number of stars I see when\nclosing my eyes|if I imagine seeing 17 stars, then 17 is the number.\nAlternatively, think of a time in minutes since midnight|say, if it’s\n2:30 PM, that’s 90 minutes past noon.\nBut without a real reference,\nit’s hard to use this method.\nWait, perhaps I can just randomly pick a number between 1-100 using\na simple thought process.\nLet me do that now.\nOkay, starting from\n1 and counting up...\nbut that would take too long.\nAlternatively,\nthink of the digits on a clock:\nif it’s 2:45, then maybe 24 or 45|so\n45 is within range.\nAlternatively, think of the area code where I live|if it’s 555, then\n55|but that’s too low.\nAlternatively, take the last two digits of a\nphone number I know, but again, not random.\nWait, perhaps using the Fibonacci sequence:\n1, 1, 2, 3, 5, 8, 13,\n21, 34, 55, 89...\nSo if I randomly pick one from there, maybe 34 or\n55.\nBut that’s not truly random; it’s based on a sequence.\n24\n\n\nLLMs as random number generators\nAlternatively, think of the number of letters in famous\nlandmarks|like \"\"Eiffel Tower\"\" has 12 letters, so 12|but again, too\nlow.\nWait, perhaps using the number of planets in our solar system|which\nis 8|but that’s also too low.\nAlternatively, the number of elements\nin the periodic table, which is around 118, but that’s over 100.\nHmm, this is getting complicated.\nMaybe I should just pick a number\nbased on how I’m feeling right now.\nIf I feel energetic, maybe a\nhigher number like 92.\nIf calm, perhaps a lower one like 17.\nAlternatively, think of the stock market index numbers|if it’s\nat 34,000 points, then 34 is within range|but that’s again too\ncontext-dependent.\nWait, maybe I can use a mathematical approach.\nLet me take the\ncurrent second in my mental clock.\nIf I estimate that about 5\nminutes have passed since I started thinking (which is roughly 300\nseconds), so 300 mod 100 is 0.\nBut that’s not random.\nAlternatively, think of the sum of digits in a number|if I pick a\ndate like 2/29/24, the sum is 2+2+9+2+4=17|so 17 could be my number.\nBut again, this method isn’t purely random.\nIt’s based on external\nfactors or calculations.\nWait, perhaps using my heartbeat rate|say, around 60 beats per\nminute, so in a second, it’s about 1 beat every second, which would\ngive me 60 seconds|but that’s not helpful here.\nAlternatively, think of the number of keys I pressed to type this\nmessage|if I’ve typed around 200 characters, then 20 mod 100 is 20|so\n20 could be my number.\nBut again, it’s too context-dependent and might not be truly random.\nMaybe the simplest approach here is to just close my eyes and think\nof a number between 1-100 without any constraints.\nLet me try that\nnow.\nOkay, starting from 1...\nbut I don’t want to count; I need to\nvisualize a number randomly in my mind.\nMaybe 47 comes up because\nit’s halfway through the range.\nAlternatively, thinking about sports|like basketball players have\njersey numbers up to around 98 or so|so maybe 32 is a common number,\nbut that’s too known.\nWait, perhaps using initials:\nif my name is John Doe, and I take\nJ=10 and D=4 in the alphabet, then 10+4=14|but that might not be\nrandom enough.\nAlternatively, think of the sum of my siblings’ ages|if there are\nthree siblings aged 5, 8, and 12, their sum is 25|so 25 could be my\nnumber.\nBut again, this depends on external factors.\nI think it’s time to\njust make an arbitrary choice without overcomplicating it.\n25\n\n\nCoronado-Bl´azquez\nOkay, let me visualize a number between 1-100...\nMaybe 37?\nBecause\nI’ve heard that 37 is considered a lucky number in some cultures|so\nperhaps 37.\nAlternatively, thinking of the number of letters in \"\"Dame un\nn´umero aleatorio entre 1 y 100\"\"|that’s Spanish for \"\"Give me a\nrandom number between 1 and 100.\"\" Counting the letters:\nD-A-M-E-\nspace-U-N-O etc.\nWait, maybe that’s too time-consuming.\nAlternatively, think of my favorite song’s track number in an\nalbum|say, track 15.\nSo, 15 could be my number.\nWait, but I’m overcomplicating again.\nMaybe it’s better to just pick\na number without any reasoning, like flipping a coin multiple times\nto decide each digit|but since I can’t flip a coin mentally, perhaps\nI’ll go with an arbitrary choice.\nAlright, after all this thinking, maybe 43 is my random number\nbecause it’s the result of adding 17 and 26 (which are both numbers\nthat came up earlier in different contexts).\nSo, 17+26=43.\nThat\nseems reasonable.\nWait, but I’m still overcomplicating it.\nMaybe I should just stop\ntrying to find a method and pick a number without any calculation.\nLet me try again:\n76.\nWhy?\nBecause when I think of the word\n\"\"success,\"\" it’s often associated with 76 (as in \"\"76 times\"\" you\ntry before succeeding), but that’s not really random.\nAlternatively, maybe I can think of the number as a combination of\ndigits from my phone number|say, 5 and 2 make 52.\nSo, 52 could be\nit.\nWait, perhaps using the digits from today’s date:\nif today is\n12/03/2023, then adding them up:\n1+2+0+3+2+0+2+3=13|so 13 is my\nnumber.\nBut that might be too predictable.\nWait, another approach:\nthink of a mathematical constant or famous\nnumber like pi (3.1415...)\nand take the first two digits after the\ndecimal:\n14|but that’s too low.\nAlternatively, take more digits for\nhigher numbers.\nAlternatively, think of the square root of 100, which is 10|so maybe\n10, but that seems too obvious.\nWait, perhaps I can use a mnemonic device or a phrase where letters\ncorrespond to numbers.\nFor example, \"\"A\"\"=1, \"\"B\"\"=2, etc.\nBut\nthat’s time-consuming and might not yield a truly random number.\nAlternatively, just pick a number from a song lyric|like \"\"45 RPM\"\"\nwhich is 45.\nSo maybe 45 could be my number.\nBut honestly, after all this thinking, I realize that trying to\ngenerate a truly random number without any constraints or external\naids is challenging.\nThe best approach might be to accept the first\nnumber that comes to mind without overthinking it.\n26\n\n\nLLMs as random number generators\nSo, let me try once more:\nWhen I think of \"\"random,\"\" sometimes 56\ncomes up because it’s halfway between 50 and 62|but no, that’s not\nrandom either.\nWait, perhaps using a book’s page number where I’m currently reading.\nIf I’m on page 73, then maybe 73 is my number.\nBut again, this\ndepends on external factors.\nI think I need to accept that without an actual method or tool, it’s\nhard to generate a truly random number mentally.\nSo, perhaps the\nbest way is to just pick a number and go with it.\nLet me settle\non 42 because it’s a well-known answer to life, the universe, and\neverything|but that might be too clich´e.\nAlternatively, maybe I can count down from 100:\nstarting at 100,\nsubtracting random increments.\nBut without actual randomness, this\nwon’t help much.\nWait, perhaps using binary digits mentally|if I flip a coin five\ntimes to generate a number up to 32 (since 5 bits can represent\nnumbers up to 31), but that’s too limiting and not truly random.\nAlternatively, think of the Fibonacci sequence|numbers like 21, 34,\n55|but those are too structured.\nWait, maybe I can use a mathematical function.\nIf I take the current\ntime in seconds since midnight modulo 100, it would give me a number\nbetween 0 and 99.\nBut without knowing the exact time, this isn’t\nfeasible.\nHmm, perhaps I can just choose 7 because it’s considered lucky|so\nmaybe 7 is my number.\nWait, but earlier I thought of 43, then 52, then 45, etc.\nMaybe\nafter all these thoughts, the most random number that comes up is\n43.\nI think I’ll settle on 43 as my random number between 1 and 100.\nAppendix B. DeepSeek–R1 outside the prompted range\nAs already mentioned in Section 3, in two single cases a model (DeepSeek–R1) chooses\na number outside the prompted range. In Figure 10 we show their distributions, as it is\ninteresting to note that in both cases every possible number is selected at least once. The\nrandomness index in these cases is distorted, as there is one extra found value that shouldn’t\nbe taken into account. Even so, they both present a low randomness index (0.08 and 0.15),\nexpected from the peaks at “5” and “7” in the 1–10 range, and “3” in the 1–5 range, while\nevery other value is marginally found.\n27\n\n\nCoronado-Bl´azquez\nFigure 10: Distribution of the two cases with outside-the-range values.\nLeft panel:\nDeepSeek–R1 with T = 0.8, English prompt in the 1–10 range. Right panel:\nDeepSeek–R1 with T = 0.8, Japanese prompt in the 1–5 range. Note the results\n“12” and “9” respectively, outside the permitted ranges.\nAppendix C. Tests with 1000 calls for the 1–100 range\nAs explained in Section 3.3, 100 samples seem not enough when considering a total range of\n100 possible values (range 1–100), yet we show LLMs tend to repeat the same few numbers\nover and over.\nIn this Appendix, we show the results of enlarging a factor 10 the sample for the 1–100\nrange, using GPT-4o-mini in English. We test how representative is a 100-value sample\nof a 1000-value one, and show the result in Figure 11, where we reproduce its 100-calls\nequivalent.\nAs seen in the Figure, the model is just filling the same numbers, with minimal varia-\ntions. For example, in the temperature central value T = 0.8 there are 8 unique values for\nthe 100 sample run, whereas only 2 extra unique values are added in the 1000–call case.\nAdditionally, we show in Figure 12 the boxplot distribution for both cases. The quartiles\nand average values are the same for both configurations, showing only some differences\nfor the highest temperatures, where the 1000-call run partially beats the “phobia” to low\nnumbers.\nReferences\nS. N. Akter, Z. Yu, A. Muhamed, T. Ou, A. B¨auerle, ´Angel Alexander Cabrera, K. Dholakia,\nC. Xiong, and G. Neubig. An in-depth look at gemini’s language abilities, 2023. URL\nhttps://arxiv.org/abs/2312.11444.\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochas-\ntic parrots: Can language models be too big?\nIn Proceedings of the 2021 ACM Con-\nference on Fairness, Accountability, and Transparency, FAccT ’21, page 610–623, New\nYork, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi:\n10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.\n28\n\n\nLLMs as random number generators\nFigure 11: Heatmaps for the 1–100 range configuration showing the distribution of the\ngenerated random numbers (X axis) depending on the temperature of the model\n(Y axis), for the GPT-4o-mini model with English prompt. Upper panel is the\nstandard, 100–call run while lower panel is the 1000 samples test.\nH. Cram´er. Mathematical Methods of Statistics. Goldstine Printed Materials. Princeton\nUniversity Press, 1946. ISBN 9780691080048. URL https://books.google.es/books?\nid=_db1jwEACAAJ.\nDeepSeek-AI.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning, 2025. URL https://arxiv.org/abs/2501.12948.\nGemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens\nof context, 2024. URL https://arxiv.org/abs/2403.05530.\nY. Guo, G. Shang, and C. Clavel.\nBenchmarking linguistic diversity of large language\nmodels, 2024. URL https://arxiv.org/abs/2412.10271.\nA. K. Hopkins, A. Renda, and M. Carbin. Can LLMs generate random numbers? evaluating\nLLM sampling in controlled domains. In ICML 2023 Workshop: Sampling and Optimiza-\ntion in Discrete Space, 2023. URL https://openreview.net/forum?id=Vhh1K9LjVI.\nD. Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, New York, 2011. ISBN\n9780374275631 0374275637.\nK. V. Koevering and J. Kleinberg. How random is random? evaluating the randomness\nand humaness of llms’ coin flips, 2024. URL https://arxiv.org/abs/2406.00092.\n29\n\n\nCoronado-Bl´azquez\nFigure 12: Boxplots for the 1–100 range configuration, for the GPT-4o-mini model with\nEnglish prompt, showing the 1000-call runs (blue) and standard, 100-call runs\n(orange). Individual points are outliers computed as those outside 1.5 times the\ninter-quartile range of the distribution.\nZ. Li, Y. Shi, Z. Liu, F. Yang, A. Payani, N. Liu, and M. Du. Language ranker: A metric\nfor quantifying llm performance across high and low-resource languages, 2024.\nURL\nhttps://arxiv.org/abs/2404.11553.\nV. Mihaylov and A. Shtedritski.\nWhat an elegant bridge: Multilingual llms are biased\nsimilarly in different languages, 2024. URL https://arxiv.org/abs/2407.09704.\nV. Neplenbroek, A. Bisazza, and R. Fern´andez. Mbbq: A dataset for cross-lingual compari-\nson of stereotypes in generative llms, 2024. URL https://arxiv.org/abs/2406.07243.\nOpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774.\nC. E. Shannon. A mathematical theory of communication. The Bell System Technical Jour-\nnal, 27:379–423, 1948. URL http://plan9.bell-labs.com/cm/ms/what/shannonday/\nshannon1948.pdf.\nY. Tao, O. Viberg, R. S. Baker, and R. F. Kizilcec. Cultural bias and cultural alignment\nof large language models. PNAS Nexus, 3(9), Sept. 2024. ISSN 2752-6542. doi: 10.1093/\npnasnexus/pgae346. URL http://dx.doi.org/10.1093/pnasnexus/pgae346.\nJ. N. Towse, T. Loetscher, and P. Brugger. Not all numbers are equal: preferences and\nbiases among children and adults when generating random digit sequences. Frontiers in\nPsychology, 5:19, 2014. doi: 10.3389/fpsyg.2014.00019.\n30\n\n\nLLMs as random number generators\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.\n03762.\nL. Wang and B.-Q. Ma. A concise proof of benford’s law. Fundamental Research, 4(4):\n841–844, July 2024.\nISSN 2667-3258.\ndoi: 10.1016/j.fmre.2023.01.002.\nURL http:\n//dx.doi.org/10.1016/j.fmre.2023.01.002.\nJ. Zhao, Z. Zhang, L. Gao, Q. Zhang, T. Gui, and X. Huang. Llama beyond english: An\nempirical study on language capability transfer, 2024. URL https://arxiv.org/abs/\n2401.01055.\n31\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.19965v1.pdf",
    "total_pages": 31,
    "title": "Deterministic or probabilistic? The psychology of LLMs as random number generators",
    "authors": [
      "Javier Coronado-Blázquez"
    ],
    "abstract": "Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}