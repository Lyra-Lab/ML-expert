{
  "id": "arxiv_2502.21265v1",
  "text": "Token-level Ensembling of Models with Different Vocabularies\nRachel Wicks,1 Kartik Ravisankar,2 Xinchen Yang,2 Philipp Koehn,1 and Matt Post1,3\n1Johns Hopkins University\n2University of Maryland\n3Microsoft\nrewicks@jhu.edu, kravisan@umd.edu, xcyang@umd.edu\nAbstract\nModel ensembling is a technique to combine\nthe predicted distributions of two or more mod-\nels, often leading to improved robustness and\nperformance. For ensembling in text genera-\ntion, the next token’s probability distribution\nis derived from a weighted sum of the distribu-\ntions of each individual model. This requires\nthe underlying models to share the same sub-\nword vocabulary, limiting the applicability of\nensembling, since many open-sourced models\nhave distinct vocabularies. In research settings,\nexperimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This\npaper proposes an inference-time only algo-\nrithm that allows for ensembling models with\ndifferent vocabularies, without the need to learn\nadditional parameters or alter the underlying\nmodels. Instead, the algorithm ensures that to-\nkens generated by the ensembled models agree\nin their surface form. We apply this technique\nto combinations of traditional encoder-decoder\nmodels and decoder-only LLMs and evaluate\non machine translation. In addition to expand-\ning to model pairs that were previously inca-\npable of token-level ensembling, our algorithm\nfrequently improves translation performance\nover either model individually.\n1\nIntroduction\nText generation takes place as a sequence of token\npredictions. At each time step, the model, condi-\ntioned on some input, produces a probability distri-\nbution over the vocabulary. From this distribution,\nthe next token is selected to extend the hypothesis—\nthe text generated thus far.\nIndividual models may be sensitive to noise or\nlack coverage in certain domains. Model ensem-\nbling is a method to combine outputs from mul-\ntiple models, which often provides for more ro-\nbust outputs and increases in performance. The\ntraditional model ensembling approach assumes a\nshared vocabulary and computes a new distribution\nDas Problem bei der Tokenisierung \nist die Größe des Wortschatzes.\n_issue\n_problem\n_prob\n_diff\nlem\n_with\n_at\n_of\n_with\nmodels agree \non top choice\nsearch for\nagreement\nmodel 2\ncatches up\nsearch for\nagreement\n_The\n_The\nFigure 1: Agreement-Based Ensembling (ABE) enables\nensembling among models with different vocabularies.\nToken generation for each beam item is constrained to\ntokens with agreeing detokenized forms.\nas a weighted sum of its component vocabularies:\np(xt) =\nX\ni\nλipmi(xt | x1..t−1)\n(1)\nwhere all interpolation weights, λi, are nonnegative\nand sum to 1. The new ensembled distribution\nfunctions as if it originated from a single model,\nand the next-token prediction proceeds as usual.\nIn practice, most models do not share vocab-\nularies. When the vocabularies differ, the result-\ning probability distributions are no longer com-\nbinable. Then, it is no longer straightforward to\nensemble these outputs. To address this, we in-\ntroduce Agreement-Based Ensembling (ABE), an\ninference-time ensembling algorithm that requires\nno new parameters or model adaptation, but in-\nstead works by coordinating token selection across\nmodels under the notion of agreement (§ 3.1). At\neach decoding timestep, each model produces its\ndistribution over the next token; our method effi-\nciently searches over their cross-product for tokens\narXiv:2502.21265v1  [cs.CL]  28 Feb 2025\n\n\nthat are contextually compatible with the currently\ngenerated surface string (§ 3.2). When the tokens\nare different (but agreeing), the longer token con-\nstrains future search (§ 3.3). This is caricatured\nin Figure 1. Our approach easily extends to other\ninference algorithms such as beam search (§ 3.4).\nOur contributions are as follows. We\n• introduce an inference-time algorithm for en-\nsembling models with different vocabularies,1\n• demonstrate the ability to ensemble across\nvarying\narchitectures\n(encoder-decoder,\nLLMs, or both), and\n• show improved results in machine translation\nacross a range of models.\nOur code is implemented in Python using the Hug-\ngingface transformers library (Wolf et al., 2019)\nand is open-source.2\n2\nRelated Work\nEnsembling is a generally reliable technique for\nincreasing the quality of model outputs that goes\nback at least as far as Hansen and Salamon (1990).\nAlthough it is more expensive, and therefore often\nprohibitive in production inference settings, it is\nuseful for example in competitions or for produc-\ntion training scenarios, such as for distillation. In\nsuch settings, the user typically has complete con-\ntrol over model training; ensembled models can be\ntaken from different checkpoints (Sennrich et al.,\n2016) or from completely different training runs\ninitialized from different random checkpoints, and\ntherefore all have the same vocabularies. Hoang\net al. (2024) move a step beyond this by ensem-\nbling models with divergent architectures (an MT\nsystem and an LLM) and across contexts longer\nthan are supported by all models, but the models\nstill share the same vocabulary.\nThe situation becomes more difficult when the\nvocabularies are not shared. One way to address\nthis is to work at the sequence level instead of the\ntoken level. One such approach is that of Jiang\net al. (2023), who propose LLM-Blender. It com-\nprises a ranking function that computes pairwise\ncomparisons of complete model outputs and then\nselects from among them; this approach completely\n1Our sole requirement is that all models are open-\nvocabulary and can therefore generate any string.\n2https://github.com/mjpost/abe\n(Apache-\n2.0)\navoids the need to do any kind of token-level en-\nsembling. Farinhas et al. (2023) generate multiple\ntranslation hypotheses and then explore selecting\nfrom among them using voting, minimum Bayes\nrisk, and LLM-based selection.\nSequence-level ensembling has limitations, and\nthe reality of disjoint vocabularies has motivated\nprior work in token-level ensembling even across\ndifferent vocabularies. Existing work, however,\nrequires extra model training. Xu et al. (2024)\nlearned mappings across vocabularies that map\ntoken representations into a joint space, and em-\nploy a variety of filtering methods for efficiency.\nShen et al. (2024) present a “collaborative decod-\ning” framework between a lead and assistant model\nwhere a classifier dynamically selects which of\nthem will produce the next token at each step of\ngeneration; their approach also appears to require\na shared vocabulary.\nOur work is distinct in that it requires no fur-\nther training or parameters. Our approach manages\ntoken-level ensembling across different vocabular-\nies at inference time by ensuring that all models in\nthe ensemble agree on the string being generated,\nand interleaves model steps for models that fall\nbehind.\n3\nAgreement-Based Ensembling\nAutoregressive models produce distributions over\ntheir vocabularies at each decoding time step.\nThis process generally continues until the end-of-\nsequence token is produced or a maximum length\nis reached. Greedy decoding, beam search, and\nsampling are all search algorithms that change how\nthe next token is selected.\nThe traditional model ensembling approach (also\ncalled here interpolation-based ensembling) fits\nnicely within any of these frameworks, but requires\nthe models to share the same vocabulary. This\napproach simply alters the probability distribution\nto be a weighted sum of the distributions from each\nmodel. Any search algorithm proceeds as before,\nselecting a token from this new distribution.\nWhen the vocabularies differ, the distributions\ndo not match and we cannot so nicely factor the\nprobability computation from the algorithm. In\nAgreement-Based Ensembling, each model pro-\nduces its distribution over its own target vocabulary\nas usual, but algorithmic changes are required to\ncoordinate on the selection of the next token to en-\nsure they agree on the detokenized surface string.\n\n\nThe problem with\n_The _prob lem _with\n_The _problem _with\nThe problem with tokenization\nshared\nglobal\nm1\nm2\n_The _problem _with _token\n_The _problem _with _tokenization\nt=4\nt=5\nt=6\nThe problem with tokenization\n_The _problem _with _token  iz\n_The _problem _with _tokenization\n✓\nx\nstalled step (m1)\n✓\n✓\nx\n✓\nFigure 2: A global state maintains the shared detokenized string, which is determined by the local hypotheses.\nAssociated with each model is a flag denoting whether the model is stalled (×) or able to generate (✓). In stalled\nsteps (§ 3.3), only the trailing model(s) generate(s) a token, catching up with the shared string. The stalled model is\nprevented from generating additional content.\nWe note that ABE operates by altering the search\nfor the next token and makes no alterations to the\nunderlying model input allowing models to have\nunique inputs (e.g., prompts) and architectures.\nIn this section, we will describe these changes.\nAt a high level, this requires maintaining a shared\nglobal agreement state (§ 3.1), efficiently search-\ning the cross-product of the models’ vocabularies\n(§ 3.2), and handling the varying token lengths of\nthe models’ differing vocabularies (§ 3.3). For ease\nof presentation, we will describe the algorithm us-\ning two models in a greedy decoding setting; this\nallows us to focus on these new ideas, without the\ncomplexity of beam search. However, the algo-\nrithm works with any number of models, so long\nas they all have open-vocabularies, and the exten-\nsions to beam search (which we used for all our\nexperiments) are straightforward.\n3.1\nAgreement\nThe fundamental difficulty when ensembling mod-\nels with different vocabularies is to ensure that they\nreach consensus on the shared output string, despite\nthe fact that the string will have been generated via\ndifferent tokenizations. In Agreement-Based En-\nsembling, we maintain a shared string—the global\nhypothesis—which is updated at each time step by\nthe predicted tokens. It is important to store and\ncompare against this string in detokenized form3\nfor precise comparison. Each model separately\nmaintains its own local hypothesis under its own\ntokenization, which is a substring of this global\nhypothesis. This is visualized in Figure 2.\nWe define the notion of agreement. Consider\na set of strings, S. The global hypothesis, g, of\nthis set is defined by (1) the shortest terminated\nstring (ends with end-of-sequence token) or (2)\nthe longest unterminated sequence—whichever is\n3We store byte-strings so byte fall-back tokenization and\nnon-Latin scripts to work.\nsatisfied first. We define the strings of S to be in\nagreement if and only if all si ∈S are prefixes of\ng. Note that agreement does not mean the models\nhave produced the exact same string, only that their\nstrings do not disagree. The algorithm provides a\ncore inductive guarantee that the detokenized string\nfor every model will always agree with the shared\nglobal hypothesis.\nFor the purposes of this paper and all experi-\nments, we require that all models have open vocab-\nularies; i.e., they are able to generate any utf-8\nstring. In modern models, this is typically guar-\nanteed via byte-fallback—using bytes instead of\n<unk> when a character is not in the vocabulary.\nThis prevents edge cases while allowing genera-\ntion in diverse languages. As long as all models\nare able to generate the strings of all other models,\nthere will always be at least one set of strings in\nagreement.\n3.2\nEfficient Search\nAt each decoding timestep, each model takes its\nforward step from its current state and produces a\ndistribution over its vocabulary. We need to effi-\nciently search the intersection of their vocabular-\nies for extensions to the current shared hypothesis\nthat are in agreement. This space has dimensions\nV1 × V2 and is too large to search completely.\nWe therefore apply a variant of cube pruning\n(Chiang, 2007; Huang and Chiang, 2007) with an\n“agreement filter” to search this space efficiently.\nThe distributions from each model are sorted and\narranged into a two-dimensional grid. This is de-\npicted visually in Figure 3. Each box in the grid\ndenotes the selection of a token from each vocabu-\nlary, each of which is associated with a score, com-\nputed as the weighted sum of the length-normalized\nmodel scores for each local hypothesis.4 As mod-\n4In all experiments, models are evenly weighted.\n\n\n0.265\n_tok\n_tokeniz\n_tok\n_the\n…\n_the\n_seg\n…\n2\n3\n8\n11\n0.72\n0.09\n0.01\n0.44\n0.41\n0.03\n…\n…\n|V1| = 16k\n|V2| = 128k\n4\n10\n12\n_all\n0.01\n0.58\n0.566\n0.25\n0.375\n0.06\n0.225\n0.21\n0.02\n0.01\n0.05\n0.365\n1\n5\n6\n9\n7\nm2\nm1\nFigure 3: The first 12 candidates in ABE search space\nfor unstalled m1, m2. Each model’s vocabulary is sorted\nby score. The top left corner is pushed onto a heap with\nits weighted score, 0.58. We present probabilities here\nfor simplicity. In practice, each token score is the cumu-\nlative log prob of the local hypothesis with this token as\nthe extension. The loop then pops from the heap, checks\nfor agreement, and adds unvisited neighbors onto the\nheap. Numbers denote visitation order.\nels may be stalled during intermittent time steps,\nthe length of the hypothesis is not guaranteed to\nbe equal to the time step.\nThis makes length-\nnormalization necessary to prevent bias towards\nshorter sequences.\nTo enumerate these items, we maintain a heap,\nwhich stores tuple items (i, j, s), where i and j in-\ndex the candidate vocabulary items, and s records\ntheir weighted score. The neighbors can be enumer-\nated by incrementing exactly one vocabulary index\nat a time—(i, j, s1) has two neighbors: (i+1, j, s2)\nand (i, j + 1, s3). Similar to other search algo-\nrithms, we do not need to add neighbors that have\nalready been visited (pushed onto the heap).\nThe heap is seeded with the tuple (1, 1, s) denot-\ning the top left corner of this grid, representing the\nmost probable token extension from each model.\nWe now iterate as follows:\n1: while True do\n2:\nPop item from heap\n3:\nCompute strings s1 and s2\n4:\nif agrees(s1, s2) then\n5:\nreturn item\n6:\nend if\n7:\nAdd unvisited neighbors of item to heap\n8: end while\nAlthough we need only one valid item for our\ngreedy search example, Figure 3 depicts the first\ntwelve loop iterations for illustrative purposes. At\neach step, the current item is popped from the heap\nand checked for agreement. This item is checked\nto determine whether the set of proposed local hy-\npotheses are in agreement. Arrows denote “neigh-\nbor” items (the next vocabulary extension in each\ndimension), which are used to create updated tuples\nthat are then added to the heap.\nThis algorithm can be generalized to an arbitrary\nnumber of ensembled models by expanding the\ndimensions of the grid, represented by the expan-\nsion of the tuples to include n vocabulary position\nindices.\n3.3\nStalled steps\nModels with larger vocabularies are likely to gener-\nate longer subwords at each timestep. This means\nthat one model may be ahead of the rest and need\nto be stalled. Consider a set of models, M. The set\nof local hypotheses generated by M is S, where\nsi was generated by mi. Recall that the global\nhypothesis is represented by g. A model, mi, is\nstalled when si = g and at least one other model is\nnot stalled: ∃(mj, sj) s.t. sj ̸= g. An example of\nwhen a model becomes stalled is illustrated in time\nsteps t = 5 and t = 6 in Figure 2.\nStalled steps aim to restore this imbalance by al-\nlowing the unstalled models to generate without the\nstalled models in order to catch up. Conceptually,\nstalling a model is simple. We prevent the model\nfrom being able to generate a token by replacing\nits vocabulary with an empty transition, {ϵ}. In\npractice this looks like:\n1: scores ←[]\n2: for m ∈models do\n3:\nseq ←m.sequence_score()\n4:\ns ←m.step() + seq\n5:\ns ←torch.sort(s)\n6:\nif model.is_stalled() then\n7:\ns ←{‘values’ : [seq], ‘idx’ : [−1]}\n8:\nend if\n9:\nscores.append(s)\n10: end for\nThis uses a dummy vocabulary index (-1) to act\nas the epsilon transition. The model’s hypothesis\nscore is unaltered as it is not allowed to generate\nadditional tokens. We illustrate the reduction in\nsearch space in Figure 4. For each stalled model,\nthe dimensionality of the search space is effectively\nreduced by one.\n\n\n휀\nFigure 4: Search space when m1 is stalled. m1 has\ngenerated tokenization while m2 has only generated\n_token iz. We present probabilities here for simplicity.\nIn practice, each token score is the cumulative log-prob\nof the local hypothesis with this token as the extension.\n3.4\nBeam Search\nGreedy decoding is a special case of beam search\nwhere the beam size is 1. It is simple to extend\nABE to handle larger beams. The main conceptual\ndifference is that the search space includes an addi-\ntional dimension, the beam index. For a beam size\nof k, the search space is k × V1× V2. Similar to the\nextension beyond two models (end of Section 3.2),\nwe add an additional index to denote which beam\nitem each vocabulary pair comes from. Then, in-\nstead of terminating after the first valid item, we\niterate until we have encountered k of them. For\ninstance, three models with a beam would have a\n4-dimensional search space of {k × V1 × V2 × V3}.\nThe k items become the beam at the next time step.\nNote that neighbors of a given candidate must come\nfrom the same beam item; beam number 2 cannot\nhave neighbors in beam number 3. This requires\nthat neighbors are only increments in exactly one\nvocabulary dimension—all other dimensions must\nremain unmodified.\nIn neural machine translation, beam search typ-\nically ensures all beam items are the same length\nas exactly one token is generated at each time step.\nIn Agreement-Based Ensembling, one or zero to-\nkens may be generated at each time step. This is\nreminiscent of statistical methods which used bin-\nning in order to compare hypothesis of equal length.\nRather than binning, we use length-normalization\nto compare across hypotheses of different lengths.\n4\nExperiments\nAgreement-Based Ensembling searches the cross-\nproduct of the model vocabularies until an agreed\nterm is found. If these sorted vocabulary distri-\nbutions are not similar enough, the algorithm will\nhave to exhaust more of the search space which\nimposes runtime considerations.\nMachine Translation (MT) has a relatively nar-\nrow set of acceptable outputs which focuses the out-\nput distributions onto similar tokens for any well-\ntrained model. We therefore choose to evaluate\nagainst machine translation as we expect models to\nagree early and often. We primarily evaluate on the\nWMT24 test set (Kocmi et al., 2024) en-de but\nextend to several other out-of-English directions\n(cs, es, uk) from the same test set. For evalua-\ntion, we consider both COMET (Rei et al., 2022)\nand BLEU (Papineni et al., 2002). We computed\nCOMET scores with with pymarian5 (Gowda et al.,\n2024), and BLEU scores with sacrebleu6 (Post,\n2018).\nWe examine ensembling within and between dif-\nferent classes of models:\n• Custom MT. We train our own encoder-\ndecoder models on the same pool of data with\ndifferent vocabulary sizes.\n• Public\nMT.\nLarge-scale,\nmultilingual,\npublicly-available MT models.\n• LLMs.\nDecoder-only LLMs with demon-\nstrated capabilities in MT.\n4.1\nModels\nFor preliminary experiments, we start by ensem-\nbling models that we trained. This allows us to\nhave control over the vocabulary while also guaran-\nteeing the models are reasonably similar and will\nfrequently agree during generation. We then ex-\ntend to off-the-shelf models, covering both encoder-\ndecoder and decoder-only architectures.\nCustom MT\nWe train transformer base mod-\nels using Marian (Junczys-Dowmunt et al., 2018)\non approximately 600m lines of filtered English–\nGerman data downloaded using mtdata (Gowda\net al., 2021) (details in Appendix A). We per-\nform standard data filtering to include dedupli-\ncation, language identifiation, length ratios, and\nmargin-scoring.\nWe train four unigram-based\nsentencepiece tokenization models (Kudo,\n2018; Kudo and Richardson, 2018) with sizes of 8k,\n16k, 32k, and 64k. Using these four tokenizers, we\ntrain four associated machine translation models.\nEach model is a standard transformer base model\n(Vaswani et al., 2023) with 6/6 (encoder/decoder)\n5Version v1.12.31, wmt22-comet-da model\n6Version 2.5.1, standard params.\n\n\nlayers, embeddings size 1024, and hidden sizes of\n8192. The entire configuration can be found in\nTable 6 in the Appendix. The data is randomly\nshuffled for infinite streaming via sotastream\n(Post et al., 2023), so we use logical epochs (1b\ntokens) rather than exact passes over the training\nset. We train for 25 logical epochs on one 24GB\nTitan RTX. In our experiments, we use various\ncheckpoints of these models.7\nPublic MT\nIn addition to custom models that\nonly support English and German, we also consider\ntwo widely used multilingual MT models, M2M\n(Fan et al., 2020) and NLLB (Team et al., 2022) in\nmultiple size and distillation variants. The former\ncovers 100 languages with a 128k multilingual vo-\ncabulary, while the latter covers 202 languages with\na 256k multilingual vocabulary. The huggingface\nrepository ids for all off-the-shelf models are listed\nin Table 7 in the Appendix. We note that Public\nMT models require an additional language tag on\nboth the source and target to signal the language\npair to the model. Again, ABE is agnostic to input\nchanges as it operates on the token distributions.\nLLMs\nWe consider TOWER (Alves et al., 2024)\nand LLaMa 3.x (Grattafiori et al., 2024). TOWER is\nan LLM specifically fine-tuned for the task of trans-\nlation whereas LLaMa is general purpose. LLaMa\nmodels use a vocab of 128k while TOWER uses 32k.\nTOWER was finetuned with the following prompt:\nTranslate the following text from English into Ger-\nman. \\n English: {source sentence} \\n German:\nFor LLaMa models, we use both 0-shot prompts\nand 3-shot prompts8 derived from the WMT24\nbaseline evaluation scripts.9\nExact verbiage of\nprompts can be found in Table 8 in the Appendix.\nLLMs differ in architecture from the previous set-\ntings as they lack an encoder. This further illus-\ntrates that ABE is architecture- and input-agnostic.\n4.2\nBaselines\nTo compare the results of our ensembling, we have\ntwo baseline generation algorithms. The first is sim-\nple translation: using the individual model without\nensembling. For the MT models, this is only pass-\ning the source input (with some language id tags\nfor the multilingual models) to the huggingface\n7Namely epochs {1, 5, 10, 15, 20, 25}\n83-shot experiments were run on one 80GB A100.\n9https://github.com/wmt-conference/\nwmt-collect-translations\ngenerate function. For TOWER and LLaMa,\nwe use the huggingface pipeline function with\nthe aforementioned prompts (explicitly listed in\nTable 8).\nWe additionally consider linear interpolation as\nan ensembling baseline. In this traditional setting,\ntwo models’ output distributions can only be inter-\npolated when they are over the same event space\n(i.e., have the same vocabulary). We therefore only\nrun this baseline over our custom MT models, mak-\ning use of different checkpoints along the training\ntrajectories of the different models.\nFor both baselines and Agreement-Based Ensem-\nbling, we use a beam size of 5 for all models. We\ngenerate with a maximum length of 256 tokens.10\n5\nResults\nWe demonstrate the effectiveness of our ensem-\nbling algorithm by comparing the sequences gen-\nerated by ABE over the performance of the best\nindividual model. Given two models mi, mj, the\ntranslations produced by either model alone are\nTi and Tj, respectively. The translations produced\nby ensembling these two models with ABE are\ndenoted as ABEi,j. We define the delta as:\n∆S = S(ABEi,j) −max(S(Ti), S(Tj))\n(2)\nwhere S may refer to BLEU or COMET scores.\n5.1\nCustom MT Models\nIn Figure 5, we display the ∆COMET scores across\nvarious combinations of custom MT models. We\nprovide the ∆BLEU for all custom models in Ap-\npendix Figure 9. We see consistent positive im-\nprovements across many checkpoints. First, we\nconsider model combinations which are already\ncompatible with traditional ensembling as a proof\nof concept. This is reflected in Figures 5(a) and\n5(b) where the ensembled models have the same\nvocabulary. We ensemble the smallest and largest\ncustom MT models with vocabulary sizes of 8k and\n64k, respectively, across various checkpoints. In\nboth cases, we find consistent improvements when\nusing ensembling.\nNext, we test the new capability of ABE—\nensembling models with differing vocabularies and\ndisplay the results in Figure 5(c). Not only are we\nable to successfully ensemble these differing mod-\nels (a previously impossible task), but we also find\npositive improvements in COMET scores.\n10If a model is stalled at this length, there is no agreed\nhypothesis and we return an empty string.\n\n\n(a) Same Vocabulary (Small)\n(b) Same Vocabulary (Large)\n(c) Different Vocabularies\nFigure 5: ∆COMET results on our custom English–German models using Agreement-Based Ensembling.\n∆COMET is the improvement of ensembling two models via ABE over the best individual model. Individ-\nual COMET scores displayed on axes. Labeling indicates vocab size followed by epoch checkpoint. All results on\nen-de WMT24.\nA persistent trend we find is that under-fitted\nmodels (e.g., Ep. 1) do not ensemble well. This is\nevidenced by negative ∆COMET scores across the\nfirst row. In all other combinations, we see improve-\nment, thus demonstrating the power of ensembling\nvia ABE over using individual models.\nWe also seek to demonstrate that these en-\nsembling results are at least as good as a naive\ninterpolation-based ensembling baseline. In order\nto do this, we compare the relative improvement us-\ning interpolation-based ensembling to the improve-\nment gained from ABE. Note that this restricts the\nsetting in which we can ensemble as the vocabular-\nies must match. In Table 1, we display the relative\n∆BLEU improvements and see that ABE is often a\nbigger improvement in these models.\nBLEU\n∆Interpolation\n∆ABE\n27.7\n0.16\n1.07\nTable 1: All scores are averages across all experiments.\nWe report the average BLEU across models. For all\nmodel pairs we report the average improvement in\nBLEU over the score of m1 or m2 individually when\nusing Interpolation or ABE.\n5.2\nPublic MT Models\nOur custom models are well-suited for ABE, since\nthey were trained on the same data and potentially\nhave related vocabulary distributions even when\ntheir vocabularies differ. We next consider mod-\nels over which we have less control. As large\nmultilingual models, M2M and NLLB are quite\ndifferent from our custom ones. In Figure 6, we\ndisplay ∆BLEU. ABE creates positive improve-\nments though not across all combinations as seen\nin Custom MT.\nFigure 6: ∆BLEU of ensembling different encoder-\ndecoder model pairs using ABE. This includes our\nlargest custom model (bilingual) and publicly available\nmultilingual models. Individual BLEU scores displayed\non axes.\nWe see an improvement when ensembling our\nlargest custom model (64k) with larger multilin-\ngual MT models. We note that the smaller mul-\ntilingual model (M2M 418M) has a significantly\nworse BLEU score than the alternatives which may\nbe an unreasonable ensembling combination. The\n∆COMET scores (displayed in Figure 10) with\nABE are more negative than their BLEU equiva-\nlents. This suggests that while ABE might be more\neffective at surfacing specific n-grams, it could neg-\natively impact other aspects, such as fluency, which\nCOMET or other neural metrics may penalize.\n\n\n5.3\nOff-the-Shelf LLMs\nWe demonstrate the flexible nature of the algorithm\nby extending our ensembling results to distinct\narchitectures—encoder-decoder with decoder-only\nLLMs. In Figure 7, we display ∆BLEU improve-\nments. In this section, we only present 3-shot ex-\nperiments with LLaMa but all results are available\nin Figure 9 in the Appendix.\nFigure 7: ∆BLEU of ensembling various encoder-\ndecoder models with LLMs using ABE. Individual\nBLEU scores displayed on axes.\nWe still see consistent positive gains from en-\nsembling models—particularly when ensembling\nthe bilingual models with the larger multilingual\nmodels. One crucial trend we notice is that poorer\nperforming models, such as the smaller instances\nof M2M or LLaMa, get consistent negative re-\nsults. This indicates that poorer performing models\nwill only deteriorate the performance of the better\nmodel, which is also typical of other ensembling\napproaches. However, we see improvements when\nensembling across architectures: +2.7 BLEU when\nensembling a small bilingual model with Tower or\nLLaMa. We further see improvements when ensem-\nbling two LLMs (+1.4 with Tower and LLaMa8b).\nAs before, we observe more negative results when\nusing COMET (Figure 10 in Appendix).\nWe see here that this new ability to ensemble\nmodels with differing vocabularies does not work\nin all settings. This new algorithm provides the\nframework to spur further research which could\nprovide better recommendations for model combi-\nnations or lambda weights which we leave to future\nwork.\n5.4\nAdditional languages\nWe additionally study the ensembling of these mod-\nels with ABE by comparing the performance in\nother languages (cs, es, uk). We compare NLLB,\nTower, and LLaMa and display the results in Ta-\nble 2. Similar to before, we notice mixed perfor-\nmance across model pairs and target languages. We\nsuspect this is due to underlying model differences.\nm1\nm2\nABE\n∆\nNLLB + Tower\ncs\n26.8\n14.1\n24.0\n-2.8\nes\n43.2\n41.0\n44.4\n+1.2\nuk\n26.3\n6.1\n24.1\n-2.2\nNLLB + LLaMa\ncs\n26.8\n19.6\n25.3\n-1.5\nes\n43.2\n37.1\n42.7\n-0.5\nuk\n26.3\n20.3\n26.2\n-0.1\nTower + LLaMa\ncs\n14.1\n19.6\n21.7\n+2.1\nes\n41.0\n37.1\n42.0\n+1.0\nuk\n6.1\n20.3\n22.4\n+2.1\nTable 2: BLEU scores for different ensembling pairs\nand their individual models. m1 and m2 denote the in-\ndividual model score while ABE denotes the ensembled\nscore. ∆is the difference between ABE and the higher\nof m1 and m2. The model versions are M2M 1.2B,\nNLLB 3.3B, Tower v0.2 7B, LLaMa 3.1 8B 3-shot.\nTower and LLaMa, which have been a consis-\ntently successful ensembling pair, see improve-\nments in all three of these languages. We further\nnote that according to their respective documenta-\ntion, neither model supports cs or uk,11 but we\nsee improvements in both using ABE.\n6\nAnalysis\nWe seek to answer why our ensembling is success-\nful in some scenarios, though not all. ABE provides\na new framework to more thoroughly investigate\nmodel ensembling—particularly in combinations\nthat were previously impossible. There are many\navenues to consider, including something as simple\nas selection of lambda weights. As a preliminary\npath of investigation for future work, we provide\nboth a quantitative and qualitative study.\n6.1\nModel Preference\nOne effect we wish to disentangle is whether this\nalgorithm is an improvement on the search space\nor on the modeling.\nAs previously mentioned,\ninterpolation-based ensembling only affects the in-\ntermediate token probabilities (a modeling change)\nand makes no changes to the search procedure.\nABE does a bit of both by severely altering the\nsearch and mildly altering the modeling (scoring\nby the weighted sum of two models instead of one).\n11There was likely substantial amounts of these languages\nin the pretraining data.\n\n\nTo answer this question, we seek to quantify the\npreferences of each translation under each model.\nGiven m1, m2 and the associated translations T1,\nT2. We can ensemble these models with ABE to\ngenerate ABEm1,m2. We then determine the rank-\ning of these three translations under each modeling\nscheme—m1 and m2—by comparing the models’\nlikelihoods of each translation. In Table 3, we see\nthat models that ensemble well together (top, cus-\ntom models) also consistently rank the ABE output\nas the most likely. They also agree on the most\nlikely output 86% of the time. Conversely, we\nsee more mixed preference with M2M and NLLB\n(∆BLEU=-0.2) suggesting that ABE cannot over-\ncome underlying modeling disagreements. This\nindicates our method is more effectively exploring\nthe search space when models agree.\nT1\nT2\nABE\nSame %\n8k+64k\nm1\n102\n106\n2207\n86.0\nm2\n198\n223\n2028\nM2M+NLLB\nm1\n1002\n1012\n1092\n54.5\nm2\n840\n809\n1096\nTable 3: Preference. Top: m1 and m2 are our bilingual\n8k and 64k models (+∆under ABE). Bottom: m1 and\nm2 are M2M1.2B and NLLB3.3B (-∆with ABE). Ti\nshows counts when outputs of mi were ranked highest\n(or tied). ABE shows counts when the outputs of the\nensemble were ranked highest. “Same %” designates\nwhen models had the same ranking.\n6.2\nConstraining Hallucinations\nStandard (same-vocabulary) ensembling can have a\nnormalizing effect on models, for example helping\nincrease their robustness to noise. Upon examining\noutputs, we found a recurring trend that ABE also\nhelps prevent models that have begun to hallucinate.\nAn example is shown in Table 4. Here, noisy inputs\nthat are included by design in the WMT24 test sets\noccasionally trip up individual models, including\nLLaMa (3B-Instruct-3-SHOT) which responds in\nGerman refusing to translate. Using ABE on all\npairs of these models yields the correct output. This\npoints to the idea that careful selection of ABE\nmodel combinations may allow training a small\nguide model that can be ensembled with a larger\nmodel to constrain its bad tendencies.\n7\nConclusion\nWe have presented an algorithm that enables token-\nlevel ensembling of models with distinct vocabular-\nsource\nlfg $sqqq\n16k\nlfg $sqqq {m} {m} {m} {m} {m} . . .\n64k\nlfg $qqqq$qqqqqqqqqqqqqqqqqqq. . .\nLLaMa\nEs scheint, dass das ursprüngliche\nTextstück fehlt oder nicht verfügbar\nist. Die gegebene Zeichenkombina-\ntion \"lfg $sqqq\" ist nicht...\nABE\nlfg $sqqq\nTable 4: (Truncated) examples of individual models\nhallucinating or becoming overly verbose on noisy input,\nbut in different ways. The MT models hallucinate with\nrepetition while LLaMa responds in German refusing to\ntranslate. Any ABE pairing of these models produces\nthe correct output.\nies. In contrast to prior relevant work, our approach\nrequires no learned mappings of token representa-\ntions (Xu et al., 2024) or other model fine-tuning.\nInstead, we run models in parallel, using a classi-\ncal approach from parsing and statistical machine\ntranslation to efficiently select tokens whose sur-\nface representation all models agree on.\nWe believe the algorithm itself is an interesting\ncontribution to the literature, since it enables (and\nmakes easy) a task that was previously impossible.\nTraditional ensembling is a technique that intro-\nduces improvements in some, but not all, settings.\nIt is therefore interesting that our approach also (a)\nproduces gains in a variety of machine translation\nsettings and (b) also often improves over standard\nensembling. Our analysis shows how this variant\nof ensembling seems to help address search errors\nin the underlying models, since those models often\nprefer (as measured by likelihood) the ensembled\nresults to their own selections.\nMachine translation was a natural task for this\napproach. For one, ensembling is often used to\nproduce higher-quality distilled results. Second,\nthe translation task helps constraint the generative\noutput to a subset of tokens that meaningful cap-\nture the source semantics. Our agreement-based\napproach might falter in less constrained tasks. The\nimplementation is conceptually simple and factored\nand allows for easy experimentation with different\nmethods for agreement-based search. We therefore\nview this as a fruitful topic for future research.\nLimitations and Ethics\nWe note a few limitations with our work. The\nfirst is our focus on one task, machine translation.\nMachine translation is heavily conditioned on the\n\n\ninput, and the accepted translation set is relatively\nsmall compared to other tasks. Though this ap-\nproach works on Large Language Models, it may\nnot easily extend to other more diverse tasks such\nas summarization.\nWe also acknowledge that machine translation\nis still a generation task, and is prone to the typ-\nical generation pitfalls of hallucinations, or erro-\nneous translations—particularly when using LLMs.\nOverly relying on error-prone automated transla-\ntion without a human review can have unintended\nconsequences when used as a means of distributing\ninformation.\nThe authors also acknowledge the assistance of\nLLMs in the work in this paper—in particular using\nAI agents like CoPilot and ChatGPT to write code\nand edit plots.\nReferences\nDuarte M. Alves, José Pombal, Nuno M. Guerreiro, Pe-\ndro H. Martins, João Alves, Amin Farajian, Ben Pe-\nters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal,\nPierre Colombo, José G. C. de Souza, and André\nF. T. Martins. 2024. Tower: An open multilingual\nlarge language model for translation-related tasks.\nPreprint, arXiv:2402.17733.\nMikel Artetxe and Holger Schwenk. 2019. Margin-\nbased parallel corpus mining with multilingual sen-\ntence embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, page 3197–3203. Association for Com-\nputational Linguistics.\nMauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa\nBentivogli, and Marcello Federico. 2013. Report on\nthe 10th IWSLT evaluation campaign. In Proceed-\nings of the 10th International Workshop on Spoken\nLanguage Translation: Evaluation Campaign, Hei-\ndelberg, Germany.\nYu Chen and Andreas Eisele. 2012. MultiUN v2: UN\ndocuments with multilingual alignments. In Proceed-\nings of the Eighth International Conference on Lan-\nguage Resources and Evaluation (LREC‘12), pages\n2500–2504, Istanbul, Turkey. European Language\nResources Association (ELRA).\nDavid Chiang. 2007. Hierarchical phrase-based transla-\ntion. Computational Linguistics, 33(2):201–228.\nAhmed El-Kishky, Vishrav Chaudhary, Francisco Guz-\nman, and Philipp Koehn. 2020. Ccaligned: A mas-\nsive collection of cross-lingual web-document pairs.\nPreprint, arXiv:1911.06154.\nAhmed El-Kishky, Adithya Renduchintala, James Cross,\nFrancisco Guzmán, and Philipp Koehn. 2021. XLEnt:\nMining a large cross-lingual entity dataset with\nlexical-semantic-phonetic word alignment. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 10424–\n10430, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nMiquel Esplà, Mikel Forcada, Gema Ramírez-Sánchez,\nand Hieu Hoang. 2019. ParaCrawl: Web-scale paral-\nlel corpora for the languages of the EU. In Proceed-\nings of Machine Translation Summit XVII: Translator,\nProject and User Tracks, pages 118–119, Dublin, Ire-\nland. European Association for Machine Translation.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\ndeep Baines, Onur Celebi, Guillaume Wenzek,\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\nMichael Auli, and Armand Joulin. 2020.\nBe-\nyond english-centric multilingual machine transla-\ntion. CoRR, abs/2010.11125.\nAntónio Farinhas, José de Souza, and Andre Martins.\n2023. An empirical study of translation hypothesis\nensembling with large language models. In Proceed-\nings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 11956–11970,\nSingapore. Association for Computational Linguis-\ntics.\nThamme Gowda, Roman Grundkiewicz, Elijah Rippeth,\nMatt Post, and Marcin Junczys-Dowmunt. 2024. Py-\nMarian: Fast neural machine translation and evalu-\nation in python. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 328–335,\nMiami, Florida, USA. Association for Computational\nLinguistics.\nThamme Gowda, Zhao Zhang, Chris Mattmann, and\nJonathan May. 2021.\nMany-to-English machine\ntranslation tools, data, and pretrained models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing: System Demonstrations, pages 306–316,\nOnline. Association for Computational Linguistics.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, and 542 others. 2024. The llama 3 herd of\nmodels. Preprint, arXiv:2407.21783.\nL.K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 12(10):993–1001.\nAnna Hätty, Simon Tannert, and Ulrich Heid. 2017. Cre-\nating a gold standard corpus for terminological anno-\ntation from online forum data. In Proceedings of Lan-\nguage, Ontology, Terminology and Knowledge Struc-\n\n\ntures Workshop (LOTKS 2017), Montpellier, France.\nAssociation for Computational Linguistics.\nKenneth Heafield, Elaine Farrow, Jelmer van der Linde,\nGema Ramírez-Sánchez, and Dion Wiggins. 2022.\nThe EuroPat corpus: A parallel corpus of European\npatent data. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n732–740, Marseille, France. European Language Re-\nsources Association.\nHieu Hoang, Huda Khayrallah, and Marcin Junczys-\nDowmunt. 2024. On-the-fly fusion of large language\nmodels and machine translation. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2024, pages 520–532, Mexico City, Mexico. Associ-\nation for Computational Linguistics.\nLiang Huang and David Chiang. 2007. Forest rescoring:\nFaster decoding with integrated language models. In\nProceedings of the 45th Annual Meeting of the Asso-\nciation of Computational Linguistics, pages 144–151,\nPrague, Czech Republic. Association for Computa-\ntional Linguistics.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.\nLLM-blender: Ensembling large language models\nwith pairwise ranking and generative fusion. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 14165–14178, Toronto, Canada. As-\nsociation for Computational Linguistics.\nMarcin Junczys-Dowmunt,\nRoman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heafield,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, André F. T.\nMartins, and Alexandra Birch. 2018. Marian: Fast\nneural machine translation in C++. In Proceedings of\nACL 2018, System Demonstrations, pages 116–121,\nMelbourne, Australia. Association for Computational\nLinguistics.\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden,\nOndˇrej Bojar, Anton Dvorkovich, Christian Feder-\nmann, Mark Fishel, Markus Freitag, Thamme Gowda,\nRoman Grundkiewicz, Barry Haddow, Marzena\nKarpinska, Philipp Koehn, Benjamin Marie, Christof\nMonz, Kenton Murray, Masaaki Nagata, Martin\nPopel, Maja Popovi´c, and 3 others. 2024. Findings\nof the WMT24 general machine translation shared\ntask: The LLM era is here but MT is not solved yet.\nIn Proceedings of the Ninth Conference on Machine\nTranslation, pages 1–46, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nMachine Translation Summit X: Papers, pages 79–86,\nPhuket, Thailand.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66–75,\nMelbourne, Australia. Association for Computational\nLinguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMarco Lui and Timothy Baldwin. 2012. langid.py: An\noff-the-shelf language identification tool. In Proceed-\nings of the ACL 2012 System Demonstrations, pages\n25–30, Jeju Island, Korea. Association for Computa-\ntional Linguistics.\nKhanh Nguyen and Hal Daumé III. 2019.\nGlobal\nVoices: Crossing borders in automatic news sum-\nmarization. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization, pages 90–97,\nHong Kong, China. Association for Computational\nLinguistics.\nByung-Doh Oh and William Schuler. 2024. Leading\nwhitespaces of language models’ subword vocabulary\npose a confound for calculating word probabilities.\nPreprint, arXiv:2406.10851.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nMatt Post, Thamme Gowda, Roman Grundkiewicz,\nHuda Khayrallah, Rohit Jain, and Marcin Junczys-\nDowmunt. 2023.\nSotastream:\nA streaming ap-\nproach to machine translation training.\nPreprint,\narXiv:2308.07489.\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRoberts Rozis and Raivis Skadin, š. 2017. Tilde MODEL\n- multilingual open data for EU languages. In Pro-\nceedings of the 21st Nordic Conference on Computa-\ntional Linguistics, pages 263–265, Gothenburg, Swe-\nden. Association for Computational Linguistics.\n\n\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2021a. Wiki-\nMatrix: Mining 135M parallel sentences in 1620 lan-\nguage pairs from Wikipedia. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 1351–1361, Online. Association for Computa-\ntional Linguistics.\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov,\nEdouard Grave, Armand Joulin, and Angela Fan.\n2021b. CCMatrix: Mining billions of high-quality\nparallel sentences on the web. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 6490–6500, Online. As-\nsociation for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Edinburgh neural machine translation systems\nfor WMT 16. In Proceedings of the First Conference\non Machine Translation: Volume 2, Shared Task Pa-\npers, pages 371–376, Berlin, Germany. Association\nfor Computational Linguistics.\nZejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim,\nand David Sontag. 2024. Learning to decode col-\nlaboratively with multiple language models. In Pro-\nceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 12974–12990, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nRalf Steinberger, Bruno Pouliquen, Anna Widiger,\nCamelia Ignat, Tomaž Erjavec, Dan Tufi¸s, and Dániel\nVarga. 2006.\nThe JRC-Acquis: A multilingual\naligned parallel corpus with 20+ languages.\nIn\nProceedings of the Fifth International Conference\non Language Resources and Evaluation (LREC‘06),\nGenoa, Italy. European Language Resources Associ-\nation (ELRA).\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Barrault,\nGabriel Mejia Gonzalez, Prangthip Hansanti, and\n20 others. 2022.\nNo language left behind: Scal-\ning human-centered machine translation. Preprint,\narXiv:2207.04672.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC‘12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nJörg Tiedemann. 2020. The tatoeba translation chal-\nlenge – realistic data sets for low resource and multi-\nlingual mt. Preprint, arXiv:2010.06354.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2023. Attention is all\nyou need. Preprint, arXiv:1706.03762.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. CoRR,\nabs/1910.03771.\nYangyifan Xu, Jinliang Lu, and Jiajun Zhang. 2024.\nBridging the gap between different vocabularies for\nLLM ensemble. In Proceedings of the 2024 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n7140–7152, Mexico City, Mexico. Association for\nComputational Linguistics.\n\n\nA\nAppendix\nBelow we describe each step of our filtering\npipeline:\n1. Remove items when equal to source and target\npair in our validation set.\n2. Remove lines without both source and target.\n3. Remove lines where langid (Lui and Baldwin,\n2012) on source is < 0.5 for English and on\ntarget is < 0.5 for German.\n4. Remove lines when more than half of the line\nis punctuation.\n5. Remove lines that have too many characters\nwith frequencies outside of the expected lan-\nguage set (Fan et al., 2020).12\n6. LASER based Margin-scoring (Artetxe and\nSchwenk, 2019) (done in 2.5M line chunks\nfor computation).\n7. Deduplicate all training data.\n12https://github.com/facebookresearch/\nfairseq/blob/main/examples/m2m_100/\nREADME.md\n\n\nData Name\nFiltered Size\nPaper (if applicable)\nELRC\n6.5M\nELRA\n66k\nEU (dcep, eac, ecdc)\n1.8M\nWikimatrix\n5.6M\nSchwenk et al. (2021a)\nWikiTitles\n2.9M\nTedTalks\n166k\nBible\n35k\nOPUS Books\n43k\nTiedemann (2012)\nCC-Aligned\n12M\nEl-Kishky et al. (2020)\nCC-Matrix\n244M\nSchwenk et al. (2021b)\nDGT\n4M\nEuropean Central Book (ECB)\n83k\nELITR\n232k\nEMEA\n233k\nEU Bookshop\n5.1M\nEU Const.\n4k\nEuroparl (v3,7,8,10)\n6.3M\nKoehn (2005)\nEuroPat (v1-3)\n47M\nHeafield et al. (2022)\nGlobal Voices\n174k\nNguyen and Daumé III (2019)\nJRC\n457k\nSteinberger et al. (2006)\nKDE/GNome\n110k\nHätty et al. (2017)\nMultiUN\n118k\nChen and Eisele (2012)\nMultiCCAligned\n60M\nMultiParaCrawl\n70M\nNews Commentary (v9,14,16)\n937k\nOPUS Train\n580k\nTiedemann (2012)\nParaCrawl (v9)\n242M\nEsplà et al. (2019)\nPHP\n7k\nQED\n400k\nTanzil\n476k\nTatoeba\n1.8M\nTiedemann (2020)\nTED (2013)\n403k\nCettolo et al. (2013)\nXLEnt\n1.4M\nEl-Kishky et al. (2021)\nTilde\n4.8M\nRozis and Skadin, š (2017)\nStatMT 13 (CommonCrawl)\n1.8M\nDeduplicated\n618M\nTable 5: We aggregate most English–German bitext listed on mtdata (available at https://github.com/\nthammegowda/mtdata). The above is the filtered text sizes.\n\n\n(a) Same Vocabulary (Small)\n(b) Same Vocabulary (Large)\n(c) Different Vocabularies\nFigure 8: BLEU results on our custom English–German models using Agreement-Based Ensembling. These charts\nshow the ∆BLEU improvement of ensembling two models via ABE over the best individual model. Labeling\nindicates vocab size followed by epoch checkpoint.\n\n\nFigure 9: The ∆BLEU scores for all model pairs.\n\n\nFigure 10: The ∆COMET scores for all model pairs.\n\n\nHyper-Parameter\nValue\nlabel smoothing\n0.1\nlearning rate\n0.0005\nlr warmup\n4000\nlr decay inv sqrt\n4000\nmini batch warmup\n4000\nmini batch\n1000\nmini batch words\n500000\nmax length\n256\nmini batch fit\ntrue\nearly stopping\n40\nlogical epoch\n1Gt\nshuffle\nbatches\nfp16\nfalse\ntied embeddings\ntrue\ntied embeddings all\ntrue\ndim emb\n1024\nenc depth\n6\ndec depth\n6\ntransformer dim ffn\n8192\ntransformer decoder dim ffn\n8192\ntransformer depth scaling\ntrue\nlemma dim emb\n0\ntransformer ffn activation\nrelu\ntransformer-heads\n8\ntransformer dropout\n0.1\ntransformer dropout attention\n0\ntransformer dropout ffn\n0.1\nTable 6: The above enumerate the Marian hyperparameters used for all of our custom models.\n\n\nModel Type\nRepo ID/URL\nm Size\nV Size\nLanguages\nLicense\nLLM\nmeta-llama/\nLlama-3.1-8B-\nInstruct\n8B\n128k\nde,es\nLLaMa3\nmeta-llama/\nLlama-3.2-1B-\nInstruct\n1B\n128k\nde, es\nLLaMa3\nmeta-llama/\nLlama-3.2-3B-\nInstruct\n3B\n128k\nde, es\nLLaMa3\nUnbabel/\nTowerInstruct-7B-v0.2\n7B\n32k\nde, es\nCC-BY-NC-4.0,\nLLaMa2\nUnbabel/\nTowerInstruct-\nMistral-7B-v0.2\n7B\n32k\nde, es\nCC-BY-NC-4.0,\nLLaMa2\nPublic MT\nfacebook/m2m100_1.2B\n1.2B\n128k\nde, es, cs, uk\nMIT\nfacebook/m2m100_418M\n418M\n128k\nde, es, cs, uk\nMIT\nfacebook/\nnllb-200-1.3B\n1.3B\n256k\nde, es, cs, uk\nCC-BY-NC\nfacebook/\nnllb-200-3.3B\n3.3B\n256k\nde, es, cs, uk\nCC-BY-NC\nfacebook/nllb-200-\ndistilled-1.3B\n1.3B\n256k\nde, es, cs, uk\nCC-BY-NC\nFacebook/nllb-200-\ndistilled-600M\n600M\n256k\nde, es, cs, uk\nCC-BY-NC\nCustom MT†\nrewicks/\nbaseline_en-de_8k_ep*\n286M\n8k\nde\nApache 2.0\nrewicks/\nbaseline_en-de_16k_ep*\n294M\n16k\nde\nApache 2.0\nrewicks/\nbaseline_en-de_32k_ep*\n310M\n32k\nde\nApache 2.0\nrewicks/\nbaseline_en-de_64k_ep*\n343M\n64k\nde\nApache 2.0\nTable 7: Huggingface Repo Ids for our publicly available models.\nLLaMa3 license refers to https://\nwww.llama.com/llama3/license/. LLaMa2 refers to https://ai.meta.com/llama/license/.\nTower also states the LLaMa license as it uses the LLaMa 2 pretraining weights. Language set only covers those\naddressed in this paper.\n† Each en-de custom MT model has 40 (epoch 1 to epoch 40) checkpoints, all of which are available in the\nabove-mentioned URL-s.\n\n\nShot\nPrompt\n0\n[\n{\"role\":\n\"system\", \"content\":\n\"Cutting Knowledge Date:\nDecember 2023\\nToday Date:\n26 Jul 2024\"}\n{\"role\":\n\"user\", \"content\":\n\"Translate the following segment\ninto XX. Do not add any additional content.\nDo not add\nparentheticals.\nOnly provide the translation.\nThe English\nsegment:\"}\n]\n3\nThe example translations are identical to the WMT24 evaluation scripts specific to the target\nlanguage. The examples can be found at https://github.com/wmt-conference/\nwmt-collect-translations/tree/main/few_shots. Each example is put in the\nsame format. Language names exchanged when necessary:\n[\n{\"role\":\n\"user\", \"content\":\n\"Translate the following text\nfrom English into German.\nThe English Segment:\nexample\nsource}\n{\"role\":\n\"assistant\", \"content\":\n\"{example translation}\"}\n]\nTable 8: LLaMa prompting messages.\n\n\nB\nSampling\nOne common use case with autoregressive mod-\nels is sampling. As with other search procedures,\nstandard ensembling works transparently with sam-\npling. As a procedure, sampling is easy to imple-\nment with ABE. Instead of searching over the grid,\nwe sample from each model consecutively (skip-\nping over stalled models). The vocabulary which\nwe sample from is renormalized to only allow for\nagreeing tokens.\nWe experimented with adding sampling to\nAgreement-Based Ensembling but found that it\ndid not work well. We hypothesize the instabil-\nity of sampling with this method stems in some\npart from the underlying idea that most tokenizers\ndenote whitespace as leading (designating word\nbeginnings) and not as trailing (designating word\nendings). This idea has been shown to have inter-\nesting effects on probability distributions (Oh and\nSchuler, 2024).\nAs an illustrative example, consider the follow-\ning German indefinite articles: “ein” and “eine.”\nThe key difference being that “eine” is feminine.\nBoth of these words are short and fundamental to\nthe German vocabulary, so it is almost guaranteed\nthat both words in their full form are in the model\nvocabulary. We further suspect that models with\nboth of these words in their vocabulary have never\nseen “eine” tokenized as “_ein” + “e” in their train-\ning data.\nNow consider our previously stated sampling\nprocedure. Assume from m1, we sample “_Eine.”\nWhen conditioned on this decision, we are likely to\nsee both “_Ein“ and “_Eine” holding most of the\nprobability mass of m2. Let’s assume we sample\n“_Ein” from m2. Since the local hypothesis of m1\n(“_Eine”) and the local hypothesis of m2 (“_Ein”)\nare in agreement, this is a valid state to be in. How-\never, when we next sample from m2 to catch up\nto m1 it is not going to have a high probability on\n“e” because it has never seen “Eine” tokenized that\nway during training.\nWe understand that m1 has implicitly decided to\ngenerate the entire word “Ein”, but it was unable\nto convey that it was also modeling the end of that\nword due to the tokenization scheme.\nNow consider a word-ending tokenization\nscheme. Now, m1 samples “Eine_” signifying that\nit is done with this word. When we constrain the\noutput of m2 on this hypothesis, “Ein_” is not go-\ning to be sampled because it does not agree. In\norder to get into the same predicament, it would\nneed to place high probability on “Ein”, specifi-\ncally not ending the word which is unlikely if both\nmodels wish to generate some version of the word\n“a.”\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21265v1.pdf",
    "total_pages": 21,
    "title": "Token-level Ensembling of Models with Different Vocabularies",
    "authors": [
      "Rachel Wicks",
      "Kartik Ravisankar",
      "Xinchen Yang",
      "Philipp Koehn",
      "Matt Post"
    ],
    "abstract": "Model ensembling is a technique to combine the predicted distributions of two\nor more models, often leading to improved robustness and performance. For\nensembling in text generation, the next token's probability distribution is\nderived from a weighted sum of the distributions of each individual model. This\nrequires the underlying models to share the same subword vocabulary, limiting\nthe applicability of ensembling, since many open-sourced models have distinct\nvocabularies. In research settings, experimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This paper proposes an inference-time\nonly algorithm that allows for ensembling models with different vocabularies,\nwithout the need to learn additional parameters or alter the underlying models.\nInstead, the algorithm ensures that tokens generated by the ensembled models\n\\textit{agree} in their surface form. We apply this technique to combinations\nof traditional encoder-decoder models and decoder-only LLMs and evaluate on\nmachine translation. In addition to expanding to model pairs that were\npreviously incapable of token-level ensembling, our algorithm frequently\nimproves translation performance over either model individually.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}