{
  "id": "arxiv_2502.20979v1",
  "text": "1\nReal-Time Aerial Fire Detection on\nResource-Constrained Devices Using Knowledge\nDistillation\nSabina Jangirova, Branislava Jankovic, Waseem Ullah, Latif U. Khan, Mohsen Guizani\nAbstract—Wildfire catastrophes cause significant environmen-\ntal degradation, human losses, and financial damage. To mitigate\nthese severe impacts, early fire detection and warning systems\nare crucial. Current systems rely primarily on fixed CCTV\ncameras with a limited field of view, restricting their effectiveness\nin large outdoor environments. The fusion of intelligent fire\ndetection with remote sensing improves coverage and mobility,\nenabling monitoring in remote and challenging areas. Existing\napproaches predominantly utilize convolutional neural networks\nand vision transformer models. While these architectures provide\nhigh accuracy in fire detection, their computational complexity\nlimits real-time performance on edge devices such as UAVs. In\nour work, we present a lightweight fire detection model based on\nMobileViT-S, compressed through the distillation of knowledge\nfrom a stronger teacher model. The ablation study highlights the\nimpact of a teacher model and the chosen distillation technique\non the model’s performance improvement. We generate activation\nmap visualizations using Grad-CAM to confirm the model’s\nability to focus on relevant fire regions. The high accuracy\nand efficiency of the proposed model make it well-suited for\ndeployment on satellites, UAVs, and IoT devices for effective fire\ndetection. Experiments on common fire benchmarks demonstrate\nthat our model suppresses the state-of-the-art model by 0.44%,\n2.00% while maintaining a compact model size. Our model\ndelivers the highest processing speed among existing works,\nachieving real-time performance on resource-constrained devices.\nIndex Terms—Aerial images; Knowledge distillation; Vision\nTransformer; Convolution Neural Network; Fire detection; Wild-\nfires.\nI. INTRODUCTION\nOver many years, fire remains one of the most significant\nnatural disasters, dangerous in its destructive character and\nspeed of spreading. In 2023, the northern parts of Kazakhstan\nexperienced massive wildfires, which burned more than 60,000\nhectares of forest and killed 15 people [1]. Together with the\nloss of lives and environmental damage, fires lead to financial\nharm. These consequences can be reduced by early detection\nand correct classification of the ignited fire, ensuring a reactive\nresponse. Nowadays, intelligent fire detection systems are\nmainly deployed on CCTV cameras, which have a fixed line of\nvision and position. Therefore, there is a high risk of missing\nthe start of the fire if it is located in the ”blind” spot not\ncovered by the cameras. Thus, there is still a need for more\nreliable solutions capable of classifying different types of fire\nunder varied conditions and complex environments. By using\nunmanned aerial vehicles, we can monitor much larger, distant\nterritories and also come closer to suspicious objects if it is\nhard to classify them as fire or non-fire. The constraint of such\ndevices is that they have limited storage and computational\ncapabilities. Therefore, they won’t be able to utilize computer\nvision models with large, complex architectures. Motivated\nby the described challenges, our research takes advantage of\nstate-of-the-art (SOTA) vision transformers and knowledge\ndistillation (KD) techniques to enhance the accuracy and\nefficiency of fire detection systems on remote sensing devices,\ncontributing to more effective fire prevention and management.\nIn the early stages, fire detection was performed using\nscalar sensor-based methods, such as the installation of smoke,\nparticle, temperature, and flame detection sensors [2]. These\nmethods are cheap and easy to install, but scalar sensors can\nmonitor only indoor environments and thus have limited usage\nscenarios. Vision sensor-based methods work with video and\nimage data, presenting a broad region coverage, reduced hu-\nman intervention requirements, rapid response times, environ-\nmental resilience, and additional information on fire character-\nistics (e.g. the size of the affected area). Mostly, conventional\nmachine learning (CML) and deep learning (DL) models are\nused for these methods [3]. CML methods commonly employ\nfeatures such as motion, color, shape, and texture [4], [5], and\nthe performance of such models is correlated with the quality\nof the features. Moreover, they failed to generalize to cases\nwith poor weather conditions and complex, unseen scenarios.\nAlternatively, DL methods proved to be effective in extract-\ning characteristics, especially for the fire detection task [6]–\n[11]. A more complex architecture of such models allows for\ncapturing intricate patterns and dependencies from the images.\nDL methods have demonstrated a prominent ability to enhance\nclassification performance in adverse weather conditions and\ncomplex scenes, further validating their use despite the in-\ncreased computational demands.\nAlthough DL models reduce false alarm rates compared\nto CML models, they require heavy computations and have\nlimited capabilities to distinguish between fire and fire-like ob-\njects [12]. Many researchers developed solutions to overcome\nthese limitations ( [3], [13]–[21]), yet there’s usually a trade-\noff between the model’s processing speed and the accuracy of\npredictions. In practical scenarios, quick detection and correct\nresponse to the forming fire is necessary to prevent significant\nlosses. This limitation drives the demand for solutions that\nbalance accuracy with efficiency, enabling their deployment\nin real-time applications.\nTo address these challenges, we propose a novel approach\nutilizing MobileViT-S [22] as the best backbone model op-\ntimized with KD techniques. Our approach combines the\narXiv:2502.20979v1  [cs.CV]  28 Feb 2025\n\n\n2\nprecision of large-scale teacher models with the compactness\nand efficiency of student models, allowing for robust and\nscalable fire detection systems. The main contributions of this\nwork include:\n• We develop a model for fire detection employing the\nbest backbone and implementing a KD approach, trans-\nferring crucial insights from a larger teacher model to a\nlightweight student model.\n• We conduct an extensive ablation study to evaluate the\neffectiveness of the proposed teacher model, student\nmodel, and KD technique for performance.\n• We evaluated the performance of our model on three fire\nclassification benchmarks. Our model achieves the same\nresults and even exceeds the accuracy of the existing\nmethods while being significantly more compact.\n• We demonstrate our model’s ability to focus on relevant\nareas within the images by using the Grad-CAM tool.\nOur proposed method provides meaningful information\non the decision-making process of our proposed model,\nincreasing its explainability.\nSection II describes the related work in the domain. Section\nIII contains the framework proposed in this project, while Sec-\ntion IV discusses the experiments and their results and presents\nthe ablation study on the effect of different teacher models\nand KD techniques. Finally, Section V presents the conclusion\nof the work done in this research, possible implications, and\nfuture directions.\nII. RELATED WORK\nEarly CML methods for fire detection relied on color analy-\nsis and image processing techniques to extract fire and smoke\nfeatures [23]–[27], while later approaches integrated motion\nfeatures, such as optical flow and spatiotemporal analysis\n[28]–[30]. Chen et al. [23] proposed a method that used\ncolor segmentation in the RGB color space to isolate fire-\nlike regions in images, while Marbach et al. [24] explored\ndynamic color modeling to adapt to varying fire hues. Celik\nand Demirel [26] introduced a statistical model for fire pixel\ndetection based on brightness and color properties. Borges and\nIzquierdo [27] took a probabilistic approach, combining color\nand temporal information for fire classification. However, these\nmethods suffered from high false alarm rates due to the diverse\ncharacteristics of fire. To improve robustness, later methods\nincorporated motion features. Foggia et al. [28] employed\noptical flow to capture the dynamic nature of fire, while Chen\net al. [29] used spatiotemporal analysis to distinguish between\nfire and non-fire motion patterns. Ha et al. [30] combined\nmotion and texture features to enhance detection reliability.\nRecently, Xu et al. combined a Modified Pixel Swapping\nAlgorithm with mixed-pixel unmixing and threshold-weighted\nfusion to detect forest fires, which improved accuracy and\nreduced false alarms [5]. Although these methods reduced\nfalse alarms to some extent, they struggled in scenarios with\ncamera movements or other moving objects that could mimic\nfire behavior.\nDeep learning (DL) methods, particularly convolutional\nneural network (CNN) models, have shown improved perfor-\nmance in fire detection [16], [31], [32]. Lightweight CNNs\nlike those proposed by Muhammad et al. [6] and Daoud et\nal. [19] addressed computational constraints, but challenges\nin detecting small, distant fires in adverse conditions remain.\nSome researchers employed CNN-based models with attention\nmechanisms. Li et al. proposed a fire detection approach with\nmultiscale feature extraction, deep supervision, and channel\nattention mechanism [17]. Wang et al. proposed a Dynamic\nEquilibrium Network to detect fire based on the data from\ndifferent types of sensors [9]. In [13], the authors integrated the\nspatial attention (SA) and channel attention (CA) modules into\nthe Inceptionv3 architecture and improved the performance\nof the backbone model. Similarly, [15] introduced the SA\nand CA modules to the ConvNeXtTiny architecture. Yar et\nal. [3] proposed a modified MobileNetV3 architecture with\nan added Modified Soft Attention Mechanism (MSAM) and\n3D convolutional operations. Dilshad et al. [20] developed an\noptimized fire attention network (OFAN) that consisted of a\nMobileNetV3Small as a backbone model, CA and SA mech-\nanisms to capture global dependencies. Rui et al. developed\na multi-modal RGB-T wildfire segmentation framework that\nlearns both modality-specific and shared features via parallel\nencoders and a shared decoder [11]. Alternatively, Yar et\nal. [21] proposed a ViT-inspired model with a shifted patch\ntokenization (SPT) module for spatial details, a locality self-\nattention (LSA) module to optimize the softmax temperature,\nand dense layers instead of the multi-head to reduce the\ncomplexity of the model. However, these methods still need\nmore robustness and capability to capture small fire regions in\ncomplex scenes, like fog or hazy weather.\nThe challenge of detecting small, distant fire sources in\npoor weather persists in the current research. While CNNs\nwith attention modules enhance feature representation through\nchannel-wise attention, they primarily focus on local spatial\ninformation and channel dependencies. To address this limi-\ntation, we utilize the architecture that combines the efficiency\nof CNNs with the global modeling capabilities of Vision\nTransformers [22].\nIII. PROPOSED FRAMEWORK\nThe challenges mentioned in the previous section must be\naddressed with more sophisticated approaches. We propose a\nframework for developing an effective, compact, and robust\nmodel for fire detection. This section describes the model\narchitecture and the training process with KD. The overall\nprocess is depicted in Fig. 1 and Algorithm 1. The training\nphase begins with the pretraining of the teacher model, ViT-\nBase Patch32 (ViT/32), on the selected fire dataset. The\nteacher model learns to extract complex features through its\ntransformer-based architecture. Then, the student model, based\non MobileViT [22], is trained using a KD framework, where\nthe teacher model guides the student by transferring its learned\nknowledge. This process ensures the student model inherits\nthe teacher’s capability to recognize fire-related patterns while\nmaintaining a more compact and lightweight architecture.\nOnce training is complete, the trained student model is de-\nployed on monitoring devices, such as drones. These drones\npatrol assigned areas, periodically capturing aerial images of\n\n\n3\nFig. 1: Proposed framework for fire detection using KD. The training phase involves distilling knowledge from a transformer-\nbased teacher model (ViT/32) to the student model (MobileViT-S). The teacher model processes image patches with linear\nprojections, positional embeddings, and a transformer encoder to produce logits, which guide the student model’s learning\nthrough the distillation loss (LKD). The student model combines convolutional layers and MobileViT modules to efficiently\nlearn both local and global features. The trained student model is deployed on resource-constrained devices, such as drones,\nfor real-time fire detection. The framework enables effective identification of fire regions, as illustrated by attention heatmaps\ngenerated during inference.\nthe environment. The deployed model processes these images\nin real-time, accurately detecting fire instances. This approach\nenables an efficient response to potential fire hazards, which\ncan be detected even by resource-constrained gadgets.\n1) Feature extraction and Model Architecture: The pro-\nposed model’s architecture uses transformers as convolutions\n[22]; in other words, by using a stack of transformers, the\nMobileViT module can capture global representations while\nalso keeping the spatial order of pixels. The architecture\nbegins with a 3×3 convolutional layer, followed by Mo-\nbileNetV2 blocks, to extract local spatial features, capturing\nfine-grained details essential for object recognition. To model\nlong-range dependencies and global context, the architecture\nutilizes MobileViT blocks. In these blocks, feature maps are\nunfolded into non-overlapping patches and processed using\ntransformer layers without losing the spatial order of pixels\nwithin each patch. This approach maintains spatial inductive\nbias, preserving critical spatial relationships. The patches\nare folded back to reconstruct the feature map with local\nand global representations. This reconstructed feature map\nis projected back to a lower-dimensional space using point-\nwise convolutions and combined with the original features via\nconcatenation. A final convolutional layer is then used to fuse\nthese combined features. In fire detection, MobileViT’s ability\nto model fine-grained details and global context enhances its\ncapability to detect small or distant fires under challenging\nconditions.\n2) Teacher Model Architecture: The teacher model, ViT/32,\nprocesses an input image by dividing it into non-overlapping\npatches, each of size 32x32 pixels, which are then linearly\nprojected into a fixed-dimensional embedding space using a\nfully connected layer. To preserve the spatial order of the\npatches, positional encodings are added to these embeddings.\nThe embedded patch tokens are then fed into a transformer\nencoder, which comprises multiple layers of multi-head self-\nattention and feed-forward networks. The self-attention mech-\nanism allows the model to capture global dependencies across\nthe entire image, enabling it to understand both local and\ncontextual information critical for tasks like fire detection. The\nfinal output from the encoder is passed through a multilayer\nperceptron (MLP) head to generate logits representing the\nmodel’s predictions [33].\n3) Knowledge Distillation: KD is a technique that allows\nthe transfer of knowledge from a complex model or an\nensemble of models, known as a ”teacher” model, to a simpler,\nsmaller ”student” model [34]. We employ KD because it is\ncritical that the compact and fast-inference model deployed in\nUAVs and surveillance systems also has high performance.\n\n\n4\nAlgorithm 1 Teacher Model Training and KD Framework\nRequire: Dataset D = {(xi, yi)}N\ni=1, Teacher model Mt,\nStudent model Ms, Temperature T, Weighting factor α,\nLearning rates ηt, ηs, Number of epochs Et, Es.\n1: Teacher Model Training:\n2: Initialize Mt.\n3: for epoch e = 1 to Et do\n4:\nShuffle dataset D.\n5:\nfor each mini-batch B = {(xb, yb)} in D do\n6:\nCompute teacher predictions st = Mt(xb).\n7:\nCalculate\ncross-entropy\nloss:\nLCE\n=\n1\n|B|\nP|B|\nb=1 CrossEntropy(st, yb).\n8:\nUpdate Mt parameters using optimizer: θt ←θt −\nηt∇θtLCE.\n9:\nend for\n10: end for\n11: KD to Student Model:\n12: Initialize Ms.\n13: for epoch e = 1 to Es do\n14:\nShuffle dataset D.\n15:\nfor each mini-batch B = {(xb, yb)} in D do\n16:\nCompute\nteacher\npredictions\nst\n=\nSoftmax(Mt(xb)/t).\n17:\nCompute\nstudent\npredictions\nss\n=\nSoftmax(Ms(xb)/t).\n18:\nCalculate distillation loss:\nLKD = T 2 · LKLD(st, ss).\n19:\nCompute\ncross-entropy\nloss:\nLCE\n=\n1\n|B|\nP|B|\nb=1 CrossEntropy(ss, yb).\n20:\nCombine losses: L = (1 −α)LCE + αLKD.\n21:\nUpdate Ms parameters using optimizer: θs ←θs −\nηs∇θsL.\n22:\nend for\n23: end for\n24: return Ms.\nIn our proposed framework, we implement soft target KD\nas described in [34]. The soft target KD involves training\nthe student model using the teacher model’s softened output\nprobabilities (soft targets). Specifically, the total loss L can be\nexpressed as:\nL = (1 −α)LCE(y, ys) + αT 2LKLD(st, ss),\n(1)\nwhere LCE(y, ys) is the cross-entropy loss between the true\nlabels y and the predicted probabilities of the student model\nys. LKLD(st, ss) is the Kullback-Leibler divergence (KLD)\nbetween the teacher’s soft targets teacher st and the student’s\noutput ss, that are computed with a temperature-scaled soft-\nmax function. T is the temperature parameter, and α is a\nweighting factor.\nWe use ViT/32 [33] as the teacher model to implement\nthe KD techniques. This combination of the teacher model\narchitecture and the KD technique proved the most effective\nbased on extensive experiments. Their results can be found in\nsubsection IV-D.\nIV. EXPERIMENTAL RESULTS\nThis section describes the experimental setup, datasets used\nfor evaluation, the performance and visual evaluation results,\nthe complexity of our proposed model, and the ablation study.\nA. Model Implementation Details and Evaluation Metrics\nThe proposed fire detection model was implemented using\nthe PyTorch deep learning framework. We conducted the\nexperiments on one NVIDIA A100 GPU and AMD EPYC\n7402 CPU with a 2.80 GHz processor. The model was trained\nfor 300 epochs with early stopping after 10 epochs, using a\nbatch size of 32, and images had a resolution of 224x224.\nThe training was done using a learning rate of 1e-4, AdamW\noptimizer with a weight decay of 1e-3 to prevent overfitting.\nWe divided all datasets into train, validation, and test splits\nwith 70%, 20%, and 10% of images, respectively, applying\nthe approach from previous research for fair comparison.\nThe performance evaluation metrics include precision (P),\nrecall (R), F1-score (F1), and accuracy (Acc). These metrics\nprovide a fundamental assessment of the model’s effectiveness\nin making accurate predictions across the entire dataset [7],\n[13], [35].\nB. Datasets\nIn this section, we present the datasets used to evaluate the\nperformance of our model. Some sample images from each\ndataset are depicted in Fig. 2.\n1) BowFire: The BoWFire dataset [12] is a small-scale fire\ndetection dataset consisting of 119 fire images and 107 non-\nfire images of different resolutions. The fire images present\nvarious emergency scenarios, while non-fire images contain\nimages without fire and images with fire-like objects (sunsets,\nred and yellow objects).\n2) ADSF: The ASDF dataset was introduced in [35], con-\ntaining images from drones and satellites. This dataset consists\nof 3000 fire images and 3000 normal images shot outdoors.\nThe ADSF dataset provides a range of images in different\nconditions, such as time of the day, landscape, and altitude.\n3) DFAN: The DFAN dataset is a medium-scale dataset that\nconsists of 3,804 images of different fire scenarios, split into\n12 imbalanced classes. Proposed by Yar et al. [13], this dataset\nchallenges models with the diversity of classes. Training a\nmodel on this dataset allows us to identify the characteristics\nof the fire and respond to it according to the level of the crisis.\nC. Visual Results\nThe visual results showcased in Fig. 3 demonstrate the\neffectiveness of the proposed model in localizing fire regions\nacross diverse environments. The left column displays the in-\nput images, while the right column presents the corresponding\nattention heatmaps visualized with Grad-CAM. Input images\nand attention heatmaps demonstrate the model’s ability to\n\n\n5\nBoWFire: Fire\nADSF: Fire\nDFAN: Forest Fire\nFig. 2: Sample images from the fire benchmarks showcasing the diverse nature of fire detection scenarios. Each image is\nlabeled with its respective class for training and evaluation purposes.\ndetect flames in drone and satellite imagery, even in chal-\nlenging scenarios. For instance, in the first row, the model\ncorrectly highlights the area of active flames in a satellite\nimage of a building fire. Similarly, in the second row, the\nmodel successfully identifies fire spread over vegetation in a\ndrone-captured image. However, limitations are observed, such\nas misinterpreting clouds as fire smoke due to their visual\nsimilarity. This signifies the need for further improvement in\ndistinguishing fire-related features from non-fire elements in\ncomplex scenes.\nMoreover, Fig. 4 displays sample images from the DFAN\ndataset, the predictions made by our proposed model, and\nthe ground truth labels for each image. Our model clearly\ndifferentiates between visually unalike classes but can make\nmistakes in related classes. For example, ”Car Fire”, ”SUV\nFire”, and ”Van Fire” classes are often confused with each\nother. These examples highlight the model’s challenges in\nhandling visually related categories, particularly in scenarios\nwhere subtle differences in object shape or fire intensity can\nmislead predictions.\nThe visual evaluation provides valuable insights into the\npossible improvement directions of the model.\nD. Ablation study\nWe conducted numerous experiments to distill knowledge\nfrom stronger models to improve our proposed model’s per-\nformance on ADSF and DFAN datasets. The knowledge\ntechniques used in the experiments include soft target KD [34],\nDistillation from A Stronger Teacher (DIST) [36], and One-\nfor-All KD (OFA-KD) [37].\nMoreover, we employed ViT/32 [33] from the transformers\nfamily and ConvNeXt-Base (ConvNeXt) [38] from the CNN\nfamily as the teacher models. These architectures were selected\nfor their ability to provide different knowledge to our proposed\nmodel. The student models tested included MobileViT-S and\nMobileViT-XS to examine the effects of model size on perfor-\nmance. The baseline performance of MobileViT-S on the test\nsplits of the DFAN and ADSF datasets without KD is 90.29%\nFig. 3: Visualization of the model attention on drone and\nsatellite images. The left column displays the input images,\nwhile the right column presents the corresponding Grad-CAM-\nbased attention visualizations. The top row shows a fire in an\nurban environment captured by a drone, with the attention map\nclearly highlighting the fire region amidst surrounding objects.\nThe model is able to effectively focus on fire regions across\ndiverse environmental conditions and input modalities.\nand 95.50%, respectively. For MobileViT-XS, the performance\nis 87.93% and 94.00%.\nGiven resource constraints, we avoided an exhaustive grid\nsearch for hyperparameter optimization. Instead, we incremen-\ntally optimized individual parameters based on their observed\nimpact on performance. For soft target KD, we found that\nT = 2 and α = 0.1 provided the best balance between the\n\n\n6\nTABLE I: Performance comparison of our model using different KD techniques on the fire benchmarks. The table highlights\nthe accuracies achieved by the MobileViT-S and MobileViT-XS student models under three distillation techniques: Soft Target\nKD, DIST, and OFA. The results in bold signify the best accuracies for each model architecture.\nDataset\nTeacher\nSoft Target KD\nDIST\nOFA\nMobileViT-S\nMobileViT-XS\nMobileViT-S\nMobileViT-XS\nMobileViT-S\nMobileViT-XS\nDFAN\nViT/32\n91.08\n90.55\n89.50\n89.24\n89.76\n86.09\nConvNeXt\n88.98\n88.71\n90.55\n89.76\n88.71\n88.45\nADSF\nViT/32\n91.33\n94.83\n94.17\n93.00\n95.33\n95.00\nConvNeXt\n92.00\n92.33\n95.00\n95.00\n95.17\n95.50\nFig. 4: Demonstration of correctly and incorrectly labeled\nDFAN images. The top row displays correctly classified ex-\namples, including a ”Building Fire,” ”Forest Fire,” and ”Non-\nFire” scene. The bottom row presents misclassified examples,\nwhere a ”Cargo Fire” was predicted as ”Car Fire,” an ”SUV\nFire” was correctly labeled as ”Car Fire,” and a ”Van Fire”\nwas predicted as ”Car Fire.”\ndistillation loss and the standard cross-entropy loss. For DIST,\nwe used α = 0.1, β = 2, γ = 2, and τ = 1, while for OFA-\nKD, the optimal parameters were ϵ = 1.2 and T = 3, as\ndescribed in their corresponding papers [36], [37].\nTable I highlights that KD significantly enhances the per-\nformance of MobileViT-S on the DFAN dataset. The best\nresult, an accuracy of 91.08%, was achieved using soft target\nKD with ViT/32 as the teacher model. This improvement\nunderscores the importance of global contextual knowledge\nprovided by the transformer-based teacher. In comparison,\nMobileViT-XS achieves slightly lower performance, with a\nmaximum accuracy of 90.55% under the same configuration.\nThis demonstrates that while MobileViT-XS is lightweight,\nit is less effective in handling the complex scenarios present\nin the DFAN dataset. OFA-KD and DIST also improved\nperformance compared to the baseline but showed slightly\nlower results than soft target KD, likely due to the specific\nproperties of the DFAN dataset, which benefits from the\nglobal context provided by ViT/32. The results also reveal that\nConvNeXt, a CNN-based teacher, does not provide as much\nperformance improvement as ViT/32. For instance, the best\naccuracy achieved with ConvNeXt as the teacher was 88.98%\nfor MobileViT-S, indicating that the global feature extraction\nof ViT/32 is better suited for challenging datasets like DFAN.\nOn the ADSF dataset, MobileViT-XS achieves the best\naccuracy of 95.50% using OFA-KD with ConvNeXt as the\nteacher. This result slightly surpasses MobileViT-S, which\nachieves a maximum accuracy of 95.33% under the same\nconfiguration. The ADSF dataset, with only two classes,\nis relatively simpler than DFAN, making it less reliant on\nthe global contextual features provided by ViT/32. Conse-\nquently, the lightweight MobileViT-XS model performs com-\npetitively on this dataset. Interestingly, the baseline accuracy\nfor MobileViT-S on ADSF is already 95.50%, indicating that\nthe dataset’s simplicity limits the impact of KD.\nWhile MobileViT-XS achieves competitive performance on\nthe ADSF dataset, MobileViT-S outperforms it on the more\nchallenging DFAN dataset, with an accuracy of 91.08% com-\npared to 90.55%. This suggests that MobileViT-S is better\nsuited for complex scenarios requiring robust feature extrac-\ntion and generalization.\nE. Performance Evaluation\nIn this section, we compare the performance and the com-\nplexity of our proposed model with the existing solutions. The\nmethods are compared on the datasets described above.\n1) Performance on the Evaluation Metrics: Table II com-\npares the performance of our proposed model to existing\nmethods on the three fire datasets. The evaluation highlights\nthe effectiveness of our approach across multiple scenarios,\nshowcasing both strengths and areas for improvement. On the\nBoWFire dataset, our model achieves perfect scores across\nall metrics even without implementing KD, with 100% Acc,\nF1, Rec, and Pre, demonstrating its exceptional capability\nto generalize on this dataset. In comparison, previous SOTA\nmethods, such as MAFire-Net [15], achieved strong results\nwith 97.82% Acc and an F1 of 97.77%, but our model still\noutperforms them. However, it is important to note that the\nsmall size of the BoWFire dataset limits its representativeness\nand may lead to inflated performance metrics. Due to this, we\nfurther evaluated our model on other datasets.\nFig. 5b shows the confusion matrix of our proposed model\non the test split of the ADSF dataset. The results demonstrate\nthat our model outperforms all previous works across all\nmetrics, achieving an Acc, F1, Rec, and Pre of 95.50%.\nAmong the existing methods, MobileNetV3 + MSAM [3]\nshows strong performance, with an accuracy and F1-score\nof 93.50% and 93.51%, respectively. However, our model\n\n\n7\n(a) BoWFire dataset.\n(b) ADSF dataset.\n(c) DFAN dataset.\nFig. 5: Confusion matrices for the fire benchmarks. For the BoWFire dataset, the model achieves perfect classification with\nno misclassifications. On the ADSF dataset, the confusion matrix demonstrates high accuracy, with minor misclassifications\nbetween fire and non-fire classes. The DFAN dataset’s confusion matrix captures the complexity of multiclass fire detection,\nwith most classes achieving high classification accuracy, but for some classes, the accuracy falls behind, such as ”Car Fire,”\n”SUV Fire,” and ”Van Fire”.\nsurpasses this by a margin of 2.0%, reflecting its stronger capa-\nbility in fire detection tasks. Overall, the consistent superiority\nof our model across all metrics demonstrates its robustness\nand effectiveness in accurately identifying fire regions under\ndiverse scenarios of the ADSF dataset.\nAs seen from Table II, our model achieves an accuracy of\n91.08%, an F1-score of 90.75%, a recall of 90.27%, and a\nprecision of 91.43%. While these results position our model\ncompetitively among existing works, it falls slightly short in\nsome metrics. Specifically, our model achieves the second-best\nresult in accuracy, with a 0.12% gap between MobileNetV3\n+ MSAM [3]. Our model shows the best F1 and precision\nscores, but ADFireNet [35] and MobileNetV3 + MSAM have\nhigher recall of 90.49% and 91.17%, respectively. Notably, our\nmodel outperforms earlier approaches, indicating improved\ngeneralization compared to earlier architectures. Insights from\nthe confusion matrix in Fig. 5c further validate the robustness\nof our model. The matrix highlights its ability to accurately\nclassify critical fire categories like ”Forest Fire” and ”Car\nFire,” achieving high classification counts in these categories.\nHowever, limitations are observed in classes like ”Cargo\nFire” and ”Pickup Fire,” where some misclassifications occur,\npotentially due to visual similarities with other categories. This\nunderscores an area for improvement in further refining the\nmodel’s attention mechanism to reduce misclassification of\nvisually similar categories.\n2) Complexity Analysis: Due to the resource constraints of\nsurveillance systems, UAVs, and IoT devices, the fire detection\nmodel should be able to quickly predict the class of an image\non any system. Table III presents the system specifications,\nmodel size, and frames-per-second (FPS) of our proposed\nmodel and the existing work.\nThe proposed model demonstrates significant improvements\nin performance compared to existing methods, as shown in\nTable III. It achieved an impressive 431.07 FPS on an Nvidia\nA100, 28.04 FPS on an AMD EPYC 7402 (2.80 GHz), and\n\n\n8\nTABLE II: Comparison of the performance of our proposed model to the existing work across the fire benchmarks. The metrics\nin bold represent the best performance, while the underscored metrics are the second-best performance.\nMethods\nBoWFire Dataset\nADSF Dataset\nDFAN Dataset\nAcc\nF1\nRec\nPre\nAcc\nF1\nRec\nPre\nAcc\nF1\nRec\nPre\nEFDNet [17]\n83.33\n81.85\n83.00\n81.81\n88.00\n87.75\n88.00\n87.50\n77.50\n77.49\n77.00\n78.00\nANetFire [39]\n88.05\n88.00\n98.00\n80.00\n-\n-\n-\n-\n-\n-\n-\n-\nXception [8]\n91.41\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nEMNFire [6]\n92.04\n92.00\n93.00\n90.00\n-\n-\n-\n-\n-\n-\n-\n-\nDFAN (comp.) [13]\n93.00\n93.10\n92.00\n94.30\n-\n-\n-\n-\n86.50\n86.00\n87.00\n86.00\nDFAN [13]\n95.00\n95.00\n94.00\n95.00\n89.36\n89.84\n94.00\n86.01\n88.00\n87.00\n88.00\n88.00\nOFAN [20]\n96.23\n96.00\n95.00\n96.00\n-\n-\n-\n-\n-\n-\n-\n-\nMAFire-Net [15]\n97.82\n97.77\n98.15\n97.05\n-\n-\n-\n-\n88.83\n87.53\n86.44\n89.35\nFireClassNet [19]\n99.56\n99.58\n99.44\n99.72\n-\n-\n-\n-\n-\n-\n-\n-\nResNet50 + FAN [13]\n-\n-\n-\n-\n-\n-\n-\n-\n86.12\n85.00\n86.00\n88.00\nNASNetM + FAN [13]\n-\n-\n-\n-\n-\n-\n-\n-\n82.56\n81.00\n82.00\n82.00\nMobileNet + FAN [13]\n-\n-\n-\n-\n-\n-\n-\n-\n85.30\n85.00\n85.00\n85.00\nADFireNet [35]\n-\n-\n-\n-\n90.86\n89.84\n90.86\n90.90\n90.00\n89.99\n90.49\n90.43\nMobileNetV3 + MSAM [3]\n-\n-\n-\n-\n93.50\n93.51\n93.51\n93.57\n91.20\n90.63\n91.17\n90.36\nOur Model\n100\n100\n100\n100\n95.50\n95.50\n95.50\n95.50\n91.08\n90.75\n90.27\n91.43\n9.36 on Raspberry Pi 4 with a compact model size of 19.73\nMB. In comparison, EMNFire [6] has the smallest model size\nof 13.0 MB, but its FPS values were lower, achieving 34.0\non a TITAN X (12GB) and 5.0 on a Raspberry Pi. While\nMobileNetV3 + MSAM [3] provides competitive FPS on high-\nperformance systems (75.15 FPS on a GeForce RTX-3090), it\nfalls behind on Raspberry Pi with 8.0 FPS and has a larger\nmodel size of 25.20 MB. Similarly, DFAN (compressed) [13]\nachieves a high 125.33 FPS on an RTX 2070, but its larger\nmodel size of 41.09 MB makes it less suitable for resource-\nconstrained devices, where our model delivers a better balance\nof compactness and speed. Despite the strong performance of\nthese models, the proposed model outperformed all others in\nterms of FPS and model size. It achieved the highest FPS\nvalues on all system specifications while maintaining a smaller\nmodel size, demonstrating its efficiency and scalability for\ndeployment on various devices, including resource-constrained\nenvironments.\nTABLE III: Complexity analysis of the proposed model com-\npared with existing research on different devices.\nModel\nSystem\nSize (MB)\nFPS\nEMNFire [6]\nTITAN X (12GB)\n13.0\n34.0\nRaspberry Pi\n5.0\nGNetFire [18]\nTITAN X (12GB)\n43.3\n20.0\nRaspberry Pi\n4.0\nSE-EFFNet [40]\nRTX 2070 (12GB)\n47.75\n45.0\nRaspberry Pi\n6.0\nDFAN (comp.) [13]\nRTX 2070 (12GB)\n41.09\n125.33\nIntel i9 (3.60GHz)\n22.73\nRaspberry Pi\n3.21\nOFAN [20]\nIntel i9 (5.00GHz)\n12.20\n25.50\nRaspberry Pi\n8.37\nMAFire-Net [15]\nGeForce RTX-3090\n74.43\n78.31\nIntel i10 (5.3GHz)\n14.32\nRaspberry Pi\n0.92\nMobileNetV3\n+ MSAM [3]\nGeForce RTX-3090\n25.20\n75.15\nIntel i9 (3.60GHz)\n24.0\nRaspberry Pi\n8.0\nOur Model\nA100\n19.73\n431.07\nEPYC 7402 (2.80 GHz)\n28.04\nRaspberry Pi\n9.36\nV. CONCLUSION\nIn this work, we proposed a lightweight and efficient\nfire detection model based on the MobileViT-S architecture,\noptimized through KD techniques to achieve high accu-\nracy and real-time inference on resource-constrained devices.\nBy leveraging the inherent hybrid structure of MobileViT-\nS, which combines the local feature extraction capabilities\nof CNNs with the global context modeling of transformers,\nour model demonstrates exceptional performance in detecting\nfire and wildfire regions under diverse surveillance condi-\ntions. Through rigorous experiments on benchmark datasets\nsuch as BoWFire, ADSF, and DFAN, the proposed model\nachieved 100%, 95.50%, and 91.08% accuracies and lowered\nfalse positive rate. Notably, the model not only surpassed\nor matched SOTA results but also achieved the highest FPS\nacross all tested devices, demonstrating its suitability for real-\ntime applications.\nNevertheless, our approach has some limitations. First,\nthe model’s ability to differentiate between visually similar\nelements, such as smoke and clouds, needs improvement to\nminimize false positives. Second, exploring advanced data\naugmentation techniques or incorporating temporal informa-\ntion from video sequences could enhance the model’s gen-\neralization capability in dynamic environments. Lastly, future\nwork could investigate more sophisticated KD strategies to\nbetter utilize diverse teacher models and further improve the\nstudent’s performance. By addressing the identified limitations\nand exploring the proposed directions, the robustness and\nfeasibility of fire monitoring systems can be further enhanced.\nREFERENCES\n[1] [Online].\nAvailable:\nhttps://eurasianet.org/\nkazakhstan-mass-wildfire-deaths-provoke-anger-at-corruption\n[2] H. Yar, A. S. Imran, Z. A. Khan, M. Sajjad, and Z. Kastrati, “Towards\nsmart home automation using iot-enabled edge-computing paradigm,”\nSensors, vol. 21, no. 14, p. 4932, 2021.\n[3] H. Yar, Z. A. Khan, I. Rida, W. Ullah, M. J. Kim, and S. W. Baik, “An\nefficient deep learning architecture for effective fire detection in smart\nsurveillance,” Image and Vision Computing, vol. 145, p. 104989, 2024.\n[4] H. Harkat, J. M. Nascimento, A. Bernardino, and H. F. T. Ahmed, “Fire\nimages classification based on a handcraft approach,” Expert Systems\nwith Applications, vol. 212, p. 118594, 2023.\n\n\n9\n[5] H. Xu, G. Zhang, R. Chu, J. Zhang, Z. Yang, X. Wu, and H. Xiao,\n“Detecting forest fire omission error based on data fusion at subpixel\nscale,” International Journal of Applied Earth Observation and Geoin-\nformation, vol. 128, p. 103737, 2024.\n[6] K. Muhammad, S. Khan, M. Elhoseny, S. H. Ahmed, and S. W. Baik,\n“Efficient fire detection for uncertain surveillance environment,” IEEE\nTransactions on Industrial Informatics, vol. 15, no. 5, pp. 3113–3122,\n2019.\n[7] T. Khan, H. ˙I. Aslan et al., “Performance evaluation of enhanced\nconvnexttiny-based fire detection system in real-world scenarios,” 2023.\n[8] V. E. Sathishkumar, J. Cho, M. Subramanian, and O. S. Naren, “Forest\nfire and smoke detection using deep learning-based learning without\nforgetting,” Fire ecology, vol. 19, no. 1, p. 9, 2023.\n[9] M. Wang, D. Yu, W. He, P. Yue, and Z. Liang, “Domain-incremental\nlearning for fire detection in space-air-ground integrated observation\nnetwork,” International Journal of Applied Earth Observation and\nGeoinformation, vol. 118, p. 103279, 2023.\n[10] S. Jin, T. Wang, H. Huang, X. Zheng, T. Li, and Z. Guo, “A self-\nadaptive wildfire detection algorithm by fusing physical and deep\nlearning schemes,” International Journal of Applied Earth Observation\nand Geoinformation, vol. 127, p. 103671, 2024.\n[11] X. Rui, Z. Li, X. Zhang, Z. Li, and W. Song, “A rgb-thermal based adap-\ntive modality learning network for day–night wildfire identification,” In-\nternational Journal of Applied Earth Observation and Geoinformation,\nvol. 125, p. 103554, 2023.\n[12] D. Y. Chino, L. P. Avalhais, J. F. Rodrigues, and A. J. Traina, “Bowfire:\ndetection of fire in still images by integrating pixel color and texture\nanalysis,” in 2015 28th SIBGRAPI conference on graphics, patterns and\nimages.\nIEEE, 2015, pp. 95–102.\n[13] H. Yar, T. Hussain, M. Agarwal, Z. A. Khan, S. K. Gupta, and S. W.\nBaik, “Optimized dual fire attention network and medium-scale fire\nclassification benchmark,” IEEE Transactions on Image Processing,\nvol. 31, pp. 6331–6343, 2022.\n[14] M. Park, J. Bak, S. Park et al., “Advanced wildfire detection using\ngenerative adversarial network-based augmented datasets and weakly\nsupervised object localization,” International Journal of Applied Earth\nObservation and Geoinformation, vol. 114, p. 103052, 2022.\n[15] T. Khan, Z. A. Khan, and C. Choi, “Enhancing real-time fire detection:\nan effective multi-attention network and a fire benchmark,” Neural\nComputing and Applications, pp. 1–15, 2023.\n[16] J. Sharma, O.-C. Granmo, M. Goodwin, and J. T. Fidje, “Deep convo-\nlutional neural networks for fire detection in images,” in Engineering\nApplications of Neural Networks: 18th International Conference, EANN\n2017, Athens, Greece, August 25–27, 2017, Proceedings.\nSpringer,\n2017, pp. 183–193.\n[17] S. Li, Q. Yan, and P. Liu, “An efficient fire detection method based\non multiscale feature extraction, implicit deep supervision and channel\nattention mechanism,” IEEE Transactions on Image Processing, vol. 29,\npp. 8467–8475, 2020.\n[18] K. Muhammad, J. Ahmad, I. Mehmood, S. Rho, and S. W. Baik,\n“Convolutional neural networks based fire detection in surveillance\nvideos,” Ieee Access, vol. 6, pp. 18 174–18 183, 2018.\n[19] Z. Daoud, A. Ben Hamida, and C. Ben Amar, “Fireclassnet: a deep\nconvolutional neural network approach for pjf fire images classification,”\nNeural Computing and Applications, vol. 35, no. 26, pp. 19 069–19 085,\n2023.\n[20] N. Dilshad, S. U. Khan, N. S. Alghamdi, T. Taleb, and J. Song, “Toward\nefficient fire detection in iot environment: A modified attention network\nand large-scale data set,” IEEE Internet of Things Journal, vol. 11, no. 8,\npp. 13 467–13 481, 2024.\n[21] H. Yar, Z. A. Khan, T. Hussain, and S. W. Baik, “A modified vision\ntransformer architecture with scratch learning capabilities for effective\nfire detection,” Expert Systems with Applications, vol. 252, p. 123935,\n2024.\n[22] S.\nMehta\nand\nM.\nRastegari,\n“Mobilevit:\nlight-weight,\ngeneral-\npurpose,\nand\nmobile-friendly\nvision\ntransformer,”\narXiv\npreprint\narXiv:2110.02178, 2021.\n[23] T.-H. Chen, P.-H. Wu, and Y.-C. Chiou, “An early fire-detection method\nbased on image processing,” in 2004 International Conference on Image\nProcessing, 2004. ICIP’04., vol. 3.\nIEEE, 2004, pp. 1707–1710.\n[24] G. Marbach, M. Loepfe, and T. Brupbacher, “An image processing\ntechnique for fire detection in video images,” Fire safety journal, vol. 41,\nno. 4, pp. 285–289, 2006.\n[25] A. Rafiee, R. Dianat, M. Jamshidi, R. Tavakoli, and S. Abbaspour, “Fire\nand smoke detection using wavelet analysis and disorder characteristics,”\nin 2011 3rd International conference on computer research and devel-\nopment, vol. 3.\nIEEE, 2011, pp. 262–265.\n[26] T. Celik and H. Demirel, “Fire detection in video sequences using a\ngeneric color model,” Fire safety journal, vol. 44, no. 2, pp. 147–158,\n2009.\n[27] P. V. K. Borges and E. Izquierdo, “A probabilistic approach for vision-\nbased fire detection in videos,” IEEE transactions on circuits and systems\nfor video technology, vol. 20, no. 5, pp. 721–731, 2010.\n[28] P. Foggia, A. Saggese, and M. Vento, “Real-time fire detection for video-\nsurveillance applications using a combination of experts based on color,\nshape, and motion,” IEEE TRANSACTIONS on circuits and systems for\nvideo technology, vol. 25, no. 9, pp. 1545–1556, 2015.\n[29] J. Chen, Y. He, and J. Wang, “Multi-feature fusion based fast video\nflame detection,” Building and Environment, vol. 45, no. 5, pp. 1113–\n1122, 2010.\n[30] C. Ha, U. Hwang, G. Jeon, J. Cho, and J. Jeong, “Vision-based fire\ndetection algorithm using optical flow,” in 2012 Sixth international\nconference on complex, Intelligent, and Software Intensive Systems.\nIEEE, 2012, pp. 526–530.\n[31] S. Frizzi, R. Kaabi, M. Bouchouicha, J.-M. Ginoux, E. Moreau, and\nF. Fnaiech, “Convolutional neural network for video fire and smoke de-\ntection,” in IECON 2016-42nd Annual Conference of the IEEE Industrial\nElectronics Society.\nIEEE, 2016, pp. 877–882.\n[32] W. Lee, S. Kim, Y.-T. Lee, H.-W. Lee, and M. Choi, “Deep neural\nnetworks for wild fire detection with unmanned aerial vehicle,” in 2017\nIEEE international conference on consumer electronics (ICCE).\nIEEE,\n2017, pp. 252–253.\n[33] A. Dosovitskiy, “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” arXiv preprint arXiv:2010.11929, 2020.\n[34] G. Hinton, “Distilling the knowledge in a neural network,” arXiv preprint\narXiv:1503.02531, 2015.\n[35] H. Yar, W. Ullah, Z. A. Khan, and S. W. Baik, “An effective attention-\nbased cnn model for fire detection in adverse weather conditions,” ISPRS\nJournal of Photogrammetry and Remote Sensing, vol. 206, pp. 335–346,\n2023.\n[36] T. Huang, S. You, F. Wang, C. Qian, and C. Xu, “Knowledge distillation\nfrom a stronger teacher,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 33 716–33 727, 2022.\n[37] Z. Hao, J. Guo, K. Han, Y. Tang, H. Hu, Y. Wang, and C. Xu,\n“One-for-all: Bridge the gap between heterogeneous architectures in\nknowledge distillation,” Advances in Neural Information Processing\nSystems, vol. 36, 2024.\n[38] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A\nconvnet for the 2020s,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2022, pp. 11 976–11 986.\n[39] K. Muhammad, J. Ahmad, and S. W. Baik, “Early fire detection using\nconvolutional neural networks during surveillance for effective disaster\nmanagement,” Neurocomputing, vol. 288, pp. 30–42, 2018.\n[40] Z. A. Khan, T. Hussain, F. U. M. Ullah, S. K. Gupta, M. Y. Lee, and\nS. W. Baik, “Randomly initialized cnn with densely connected stacked\nautoencoder for efficient fire detection,” Engineering Applications of\nArtificial Intelligence, vol. 116, p. 105403, 2022.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20979v1.pdf",
    "total_pages": 9,
    "title": "Real-Time Aerial Fire Detection on Resource-Constrained Devices Using Knowledge Distillation",
    "authors": [
      "Sabina Jangirova",
      "Branislava Jankovic",
      "Waseem Ullah",
      "Latif U. Khan",
      "Mohsen Guizani"
    ],
    "abstract": "Wildfire catastrophes cause significant environmental degradation, human\nlosses, and financial damage. To mitigate these severe impacts, early fire\ndetection and warning systems are crucial. Current systems rely primarily on\nfixed CCTV cameras with a limited field of view, restricting their\neffectiveness in large outdoor environments. The fusion of intelligent fire\ndetection with remote sensing improves coverage and mobility, enabling\nmonitoring in remote and challenging areas. Existing approaches predominantly\nutilize convolutional neural networks and vision transformer models. While\nthese architectures provide high accuracy in fire detection, their\ncomputational complexity limits real-time performance on edge devices such as\nUAVs. In our work, we present a lightweight fire detection model based on\nMobileViT-S, compressed through the distillation of knowledge from a stronger\nteacher model. The ablation study highlights the impact of a teacher model and\nthe chosen distillation technique on the model's performance improvement. We\ngenerate activation map visualizations using Grad-CAM to confirm the model's\nability to focus on relevant fire regions. The high accuracy and efficiency of\nthe proposed model make it well-suited for deployment on satellites, UAVs, and\nIoT devices for effective fire detection. Experiments on common fire benchmarks\ndemonstrate that our model suppresses the state-of-the-art model by 0.44%,\n2.00% while maintaining a compact model size. Our model delivers the highest\nprocessing speed among existing works, achieving real-time performance on\nresource-constrained devices.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}