{
  "id": "arxiv_2502.21278v1",
  "text": "Does Generation Require Memorization?\nCreative Diffusion Models using Ambient Diffusion\nKulin Shah ∗\nUT Austin\nAlkis Kalavasis †\nYale University\nAdam R. Klivans ‡\nUT Austin\nGiannis Daras §\nMIT\nMarch 3, 2025\nAbstract\nThere is strong empirical evidence that the state-of-the-art diffusion modeling paradigm leads\nto models that memorize the training set, especially when the training set is small. Prior methods\nto mitigate the memorization problem often lead to a decrease in image quality. Is it possible to\nobtain strong and creative generative models, i.e., models that achieve high generation quality\nand low memorization? Despite the current pessimistic landscape of results, we make significant\nprogress in pushing the trade-off between fidelity and memorization. We first provide theoretical\nevidence that memorization in diffusion models is only necessary for denoising problems at low\nnoise scales (usually used in generating high-frequency details). Using this theoretical insight,\nwe propose a simple, principled method to train the diffusion models using noisy data at large\nnoise scales. We show that our method significantly reduces memorization without decreasing\nthe image quality, for both text-conditional and unconditional models and for a variety of data\navailability settings.\n1\nIntroduction\nDiffusion models [SE19, HJA20, SSDK+20] have become a widely used framework for unconditional\nand text-conditional image generation. However, recent works [SSG+22, CHN+23, DSD+23, SSG+23,\nDDD24, RKW+24] have shown that the trained models memorize the training data and often replicate\nthem at generation time. This issue has raised important privacy and ethical concerns [SSG+22,\nTKC22, ANS23], especially in applications where the training set contains sensitive or copyrighted\ninformation [CBLC22]. [CHN+23] conjectures that the improved performance over alternative\nframeworks may come from the increased memorization [LYM+24].\nThis raises the following\nquestion:\nCan we improve the memorization of diffusion models\nwithout decreasing the image generation quality?\n∗Email: kulinshah@utexas.edu, supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n†Email: alkis.kalavasis@yale.edu, supported by the Institute for Foundations of Data Science at Yale (FDS).\n‡Email: klivans@cs.utexas.edu, supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n§Email: gdaras@mit.edu, supported by the NSF AI Institute for Foundations of Machine Learning (IFML) and\nthe Computer Science & Artificial Intelligence Laboratory at MIT (CSAIL).\n1\narXiv:2502.21278v1  [cs.LG]  28 Feb 2025\n\n\n0\n10\n20\n30\n40\n50\nPercentage of generated images that are training data duplicates\n14\n16\n18\n20\n22\n24\n26\nFréchet Inception Distance (FID)\ntn = 0.0\ntn = 0.1\ntn = 0.25\ntn = 0.3\ntn = 0.4\ntn = 0.5\ntn = 2\ntn = 4\ntn = 10\nTrade-off between FID and Memorization\nFFHQ with 300 Training Images\nData Points\nPareto Frontier\nFigure 1: (FID, Memorization) pairs for different values of 𝜎𝑡n used in our proposed Algorithm 1\n(presented in Section 3) for training diffusion models from limited data. The standard DDPM\nobjective corresponds to 𝜎𝑡n = 0 and it is not in the Pareto frontier. Setting 𝜎𝑡n too low or too\nhigh reverts back to the DDPM behavior. Values for 𝜎𝑡n ∈[0.4, 4] strike different balances between\nmemorization and quality of generated images. The models in this Figure are trained on only 300\nimages from FFHQ.\nPrior work has shown that the optimal solution to the diffusion objective is a model that merely\nreplicates the training points [DB22, SBS23, BBDBM24, KG24, BBDD24]. The experimentally\nobserved creativity in diffusion modeling happens when the models fail to perfectly minimize their\ntraining loss [KG24]. As the training dataset becomes smaller, overfitting becomes easier, memo-\nrization increases and output diversity decreases [SSG+23, DSD+23, GDP+23]. Text-conditioning is\nalso known to exacerbate memorization [SSG+22, CHN+23, SSG+23] and text-conditional diffusion\nmodels are known to memorize individual training points even when trained on billions of image-text\npairs [CHN+23, DDDD23].\nRelated work.\nSeveral methods have been proposed to reduce the memorization in diffusion\nmodels [SSG+23, DSD+23, GDP+23, WLCL24, DCD24, KSH+24, CLX24, RLZ+24, WLHH24,\nLGWM24, RKW+24, WCS+24, ZLL+24, JKS+24, HSK+25]. A line of work proposes sampling adap-\ntations that guide the generation process away from training points [KSH+24, WLCL24, CLX24].\n[KYKM23, WBZ+25] propose decreasing the receptive field of the generative model to avoid mem-\norization. Another line of work corrupts the images [DSD+23, DDD24] or the text-embedding in\ntext-conditioned image models [SSG+23]. While effective in reducing memorization, these methods\noften decrease the image generation quality. Feldman [Fel20] theoretically showed strong trade-offs\nbetween memorization and generalization by showing that memorization is necessary for (optimal)\nclassification. This raises the natural question of whether this trade-off also applies to generative\nmodeling.\nThe need for memorization in [Fel20] is associated with the frequencies of different subpopulations\n(e.g., cats, dogs, etc.) that appear in the dataset. The key observation is that the distribution of the\nfrequencies is usually heavy-tailed [ZAR14], i.e., roughly speaking in a dataset of size 𝑛, there will be\n2\n\n\nTraining Image\n[WLCL24]\n[WLCL24] + Ours\n”RISE 24” TriFecta Dishwasher\nCANYON CARGO - Outdoor shorts - dark moss\nWestern Chief Down Hill Trot (Black) Women’s Rain Boots\nFigure 2: Qualitative results for reducing the memorization of Stable Diffusion 2. Combining our\nmethod with [WLCL24] helps generate novel samples for the above prompts. See Section 3 for our\nmethod and Section 5.2 for more details on the experiment.\nmany classes with frequency around 1/𝑛. This means that the training algorithm will only observe\na single representative from those subpopulations and cannot distinguish between the following two\ncases:\nCase 1. If the unique example comes from an extremely rare subpopulation (with frequency ≪1/𝑛),\nthen memorizing it has no significant benefits, and,\nCase 2. If the unique example comes from a subpopulation with 1/𝑛frequency, then memorizing it\nwill probably improve the accuracy on the entire subpopulation and decrease the generalization\nerror by Ω(1/𝑛). Hence, the optimal classifier should memorize these unique examples to avoid\npaying Case 2 in the error.\nThe key assumption above seems to break when noise is added to the images. That is because\ndifferent subpopulations start to merge and the heavy-tails of the weights’ distribution disappear.\nInterestingly, diffusion models learn the (score of the) distribution at different levels of noise. This\n3\n\n\nInput images\n𝜎= 17\n𝜎= 8\nOutputs of diffusion model trained on 52k images\nOutputs of diffusion model trained on 300 images\nOutputs of Ambient Diffusion trained on 300 images\nFigure 3: Comparison of denoised images under different noise levels and training conditions.\nStandard diffusion modeling leads to overconfident predictions (row 3) even for very highly noised\ninputs when it is trained on small datasets. Our algorithm (row 4), has a similar behavior (blurry\noutputs) to a model trained with significantly more data (row 1), indicating less memorization.\nindicates that, in principle, it is feasible to avoid memorization in the high-noise regime (without\nsacrificing too much quality). Despite that, regular diffusion model training, e.g., the DDPM [HJA20]\nobjective, results in score functions that have attractors around the training points, even for highly\nnoisy inputs, as shown in Figure 3.\nThe discussion above suggests that it should be possible to train high-quality diffusion models\nthat do not memorize in the high-noise part of the diffusion. It has been empirically established that\nthis part controls the structural information of the outputs and hence the diversity of the generated\ndistribution [Die24, LC24]. To avoid memorization in the high-noise regime, we propose a simple,\nprincipled framework that trains the diffusion model only with noisy data at large noise scales. We\ngive theoretical evidence that the noisy targets used for learning leak much less information about\nthe training set, and further they are harder to memorize since they are less compressible.\nOur contributions:\n• We propose a simple framework to train diffusion models that achieve reduced memorization\nand high-quality sample generation even when trained on limited data.\n• We experimentally validate our approach on various datasets and data settings, showcasing\nsignificantly reduced memorization and improved generation quality compared to natural\n4\n\n\nbaselines, in both the unconditional and text-conditional settings 1.\n• On the theory side, we adapt the theoretical framework of [Fel20] for studying memorization\nto diffusion models. Based on that, we argue about the necessity of memorizing the training\nset in different noise scales indicating that memorization is only essential at the low-noise\nregime.\n• We quantify the information leakage of our proposed algorithm in the high-noise regime\nshowing significant benefits over the standard diffusion modeling objective.\n2\nBackground and Related Work\n2.1\nDiffusion Modeling\nThe first step in diffusion modeling is to design a corruption process. For the ease of presentation,\nwe focus on the widely used Variance Preserving (VP) corruption [HJA20, SSDK+20]. We define a\nsequence of increasing corruption levels indexed by 𝑡∈[0, 1], with:\n𝑋𝑡=\nq\n1 −𝜎2\n𝑡𝑋0 + 𝜎𝑡𝑍,\n𝑍∼𝒩(0, 𝐼𝑑),\n(1)\nwhere the map 𝜎𝑡:= 𝜎(𝑡) is the noise schedule and 𝑋0 is drawn from the clean distribution 𝑝0.\nWe remark that our framework extends to other noise schedules, diffusion models [SE19, BBC+22,\nKAAL22, DDT+23] and flow matching [LCBH+22, LGL22, ABVE23].\nOur ultimate goal is to sample from the unknown distribution 𝑝0.\nThe key idea behind\ndiffusion modeling is to learn the score functions, defined as ∇log 𝑝𝑡(·), for different noise levels\n𝑡, where 𝑋𝑡∼𝑝𝑡. The latter is related to the optimal denoiser 𝔼[𝑥0|𝑋𝑡= 𝑥𝑡] through Tweedie’s\nformula [Efr11]:\n∇log 𝑝𝑡(𝑥𝑡) =\nq\n1 −𝜎2\n𝑡𝔼[𝑥0|𝑋𝑡= 𝑥𝑡] −𝑥𝑡\n𝜎2\n𝑡\n.\n(2)\nThe conditional expectation is typically learned from the available data with supervised learning\nover some parametric class of models ℋ= {ℎ𝜃: 𝜃∈Θ}, using the training objective:\n𝐽(𝜃) = 𝔼𝑥0𝔼(𝑥𝑡,𝑡)|𝑥0\n\u0002\n∥ℎ𝜃(𝑥𝑡, 𝑡) −𝑥0∥2\u0003\n.\n(3)\nPost training, the score function ∇log 𝑝𝑡(𝑥𝑡) is approximated by plugging the optimal solution of\n(3) to (2). Alternatively, one can train directly for the score function using the noise prediction\nloss [Vin11, HJA20]:\n𝐽(𝜃) = 𝔼𝑥0,𝑥𝑡,𝑡\n\"\r\r\r\r\r𝑠𝜃(𝑥𝑡, 𝑡) −\nq\n1 −𝜎2\n𝑡𝑥0 −𝑥𝑡\n𝜎2\n𝑡\n\r\r\r\r\r\n2#\n.\n(4)\nGiven access to the score function for different times 𝑡, one can sample from the distribution of 𝑝0\nby running the process [SSDK+20]:\nd𝑥=\n\u0012\n−𝑥−(d𝜎𝑡/d𝑡)𝜎𝑡\n1 + 𝜎2\n𝑡\n∇log 𝑝𝑡(𝑥𝑡)\n\u0013\nd𝑡.\n(5)\n1We open-source our code: https://github.com/kulinshah98/memorization_noisy_data\n5\n\n\n2.2\nMemorization in Diffusion Models\nThe first expectation of (3) is taken over the distribution of 𝑥0. The underlying distribution of 𝑥0\nis continuous, but in practice we only optimize this objective over a finite distribution of training\npoints. Prior work has shown that when the expectation is taken over an empirical distribution\nb𝑝0, the optimal score can be written in closed form [DB22, SBS23, BBDBM24, KG24, BBDD24].\nSpecifically, the optimal score for the empirical distribution, which corresponds to a finite amount\nof examples 𝑆, can be written as:\nb𝑠∗(𝑥𝑡, 𝑡) = 1\n𝜎2\n𝑡\n1\nÍ\n𝑥0∈𝑆𝒩(𝑥𝑡;\nq\n1 −𝜎2\n𝑡𝑥0, 𝜎𝑡𝐼)\n·\nÕ\n𝑥0∈𝑆\n(\nq\n1 −𝜎2\n𝑡𝑥0 −𝑥𝑡) 𝒩(𝑥𝑡;\nq\n1 −𝜎2\n𝑡𝑥0, 𝜎𝑡𝐼) .\nattraction to 𝑥0\nweight of attraction\nIntuitively, each point 𝑥0 in the finite sample 𝑆(i.e., the empirical distribution b𝑝0) is pulling\nthe noisy iterate 𝑥𝑡towards itself, where the weight of the pull depends on the distance of each\ntraining point to the noisy point. The above solution will lead to a diffusion model that only\nreplicates the training points during sampling [SBS23, KG24]. Hence, any potential creativity that\nis observed experimentally in diffusion models comes from the failure to perfectly optimize the\ntraining objective.\n2.3\nAmbient Score Matching\nOne way to mitigate memorization is to never see the training data.\nRecent techniques for\ntraining with corrupted data allow learning of the score function without ever seeing a clean\nimage [KEME23, DDDD23, DDD24, BWCS24, WBL+24, RALL24]. Consider the case where we\nare given samples from a noisy distribution 𝑝𝑡n (where 𝑡n stands for 𝑡-nature) and we desire to learn\nthe score at time 𝑡for 𝑡> 𝑡n. The Ambient Score Matching loss [DDD24], defined as:\n𝐽ambient(𝜃) = 𝔼𝑥𝑡n𝔼(𝑥𝑡,𝑡)|𝑥𝑡n\n\u0014\r\r\r\r\n𝜎2\n𝑡−𝜎2\n𝑡n\n𝜎2\n𝑡\nq\n1 −𝜎2\n𝑡n\nℎ𝜃(𝑥𝑡, 𝑡) +\n𝜎2\n𝑡n\n𝜎2\n𝑡\ns\n1 −𝜎2\n𝑡\n1 −𝜎2\n𝑡n\n𝑥𝑡−𝑥𝑡n\n\r\r\r\r\n2\u0015\n,\n(6)\ncan learn the conditional expectation 𝔼[𝑥0|𝑥𝑡] (similar to Equation (3)) without ever looking at\nclean data from 𝑝0. The intuition behind this objective is that to denoise the noisy sample 𝑥𝑡, we\nneed to find the direction of the noise and then rescale it appropriately. The former can be found\nby denoising to an intermediate level 𝑡n and the rescaling ensures that we denoise all the way to the\nlevel of clean images. Once the conditional expectation 𝔼[𝑥0|𝑥𝑡] is recovered, we get the score by\nusing Tweedie’s Formula.\nWe remark that this objective can only be used for 𝑡> 𝑡n. While there are ways to train for\n𝑡≤𝑡n without any clean data (e.g., see [DDDD23, DDD24, BWCS24, WBL+24, RALL24]), this\nleads to performance deterioration unless a massive noisy dataset is available [DCD24]. For what\nfollows, we refer to Eq.(3) as the DDPM training objective and to Eq.(6) as the Ambient Diffusion\ntraining objective for noisy data.\n3\nMethod\nWe are now ready to present our framework for training diffusion models with limited data that will\nallow creativity without sacrificing quality. Our key observation is that the diversity of the generated\n6\n\n\nimages is controlled in the high-noise part of the diffusion trajectory [Die24, LC24]. Hence, if we\ncan avoid memorization in this regime, it is highly unlikely that we will replicate training examples\nat inference time, even if we memorize at the low-noise part. Our training algorithm can “copy”\ndetails from the training samples and still produce diverse outputs.\nAlgorithm 1 Algorithm for training diffusion models using limited data.\nRequire: untrained network ℎ𝜃, set of samples 𝑆, noise level 𝑡n, noise scheduling 𝜎(𝑡), batch size\n𝐵, diffusion time 𝑇\n1: 𝑆𝑡n ←{\nq\n1 −𝜎2\n𝑡n𝑥(𝑖)\n0 + 𝜎𝑡n𝜀(𝑖)|𝑥(𝑖)\n0 ∈𝑆, 𝜀(𝑖) ∼𝒩(0, 𝐼𝑑)}\n⊲Noise the training set at level 𝑡n.\n2: while not converged do\n3:\nForm a batch ℬof size 𝐵uniformly sampled from 𝑆∪𝑆𝑡n\n4:\nloss ←0\n⊲Initialize loss.\n5:\nfor each sample 𝑥∈ℬdo\n6:\n𝜀∼𝒩(0, 𝐼)\n⊲Sample noise.\n7:\nif 𝑥∈𝑆𝑡n then\n8:\n𝑥𝑡n ←𝑥\n⊲We are dealing with a noisy sample.\n9:\n𝑡∼𝒰(𝑡n, 𝑇)\n⊲Sample diffusion time for noisy sample.\n10:\n𝑥𝑡←\nr\n1−𝜎2\n𝑡\n1−𝜎2\n𝑡n\n𝑥𝑡n +\nr\n𝜎2\n𝑡−𝜎2\n𝑡n\n1−𝜎2\n𝑡n\n𝜀\n⊲Add additional noise.\n11:\nloss ←loss +\n\r\r\r\r\r\n𝜎2\n𝑡−𝜎2\n𝑡n\n𝜎2\n𝑡\nq\n1−𝜎2\n𝑡n\nℎ𝜃(𝑥𝑡, 𝑡) +\n𝜎2\n𝑡n\n𝜎2\n𝑡\nr\n1−𝜎2\n𝑡\n1−𝜎2\n𝑡n\n𝑥𝑡−𝑥𝑡n\n\r\r\r\r\r\n2\n⊲Ambient Score Matching.\n12:\nelse\n13:\n𝑥0 ←𝑥\n⊲We are dealing with a clean sample.\n14:\n𝑡∼𝒰(0, 𝑡n)\n⊲Sample diffusion time for clean sample.\n15:\n𝑥𝑡←\nq\n1 −𝜎2\n𝑡𝑥0 + 𝜎𝑡𝜀\n⊲Add noise.\n16:\nloss ←loss + ∥ℎ𝜃(𝑥𝑡, 𝑡) −𝑥0∥2\n⊲Regular Denoising Score Matching.\n17:\nend if\n18:\nend for\n19:\nloss ←loss\n𝐵\n⊲Compute average loss.\n20:\n𝜃←𝜃−𝜂∇𝜃loss\n⊲Update network parameters via backpropagation.\n21: end while\nOur training framework is presented in Algorithm 1. It works by splitting the diffusion training\ntime into two parts, 𝑡≤𝑡n and 𝑡> 𝑡n, where 𝑡n2 (𝑡-nature) is a free parameter to be controlled.\nFor the regime, 𝑡≤𝑡n, we train with the regular diffusion training objective, and (assuming perfect\noptimization) we know the exact score, which is as given in Section 2.2. To train for 𝑡> 𝑡n, we first\ncreate the set 𝑆𝑡n which has one noisy version of each image in the training set. Then, we train\nusing the set 𝑆𝑡n and the Ambient Score Matching loss introduced in Section 2.3.\nIt is useful to build some intuition about why this algorithm avoids memorization and at the\nsame time produces high-quality outputs. Regarding memorization: 1) the learned score function\nfor times 𝑡≥𝑡n does not point directly towards the training points since Ambient Diffusion aims\nto predict the noisy points (recall that the optimal DDPM solution points towards scalings of the\ntraining points) and 2) the noisy versions 𝑥𝑡n are harder to memorize than 𝑥0, since noise is not\n2We often use the symbol 𝑛for sample size; the notation 𝑡n is unrelated to the size 𝑛.\n7\n\n\ncompressible. At the same time, if the dataset size were to grow to infinity, both our algorithm and\nthe standard diffusion objective would find the true solution: the score of the underlying continuous\ndistribution. In fact, Algorithm 1 learns the same score function for times 𝑡≤𝑡n as DDPM. This\ncontributes to generating samples with high-quality details, copied from the training set.\n4\nTheoretical Results\n4.1\nInformation Leakage\nIn this section, we attempt to formalize the intuition of why our proposed algorithm reduces\nmemorization of the dataset. We start by showing the following Lemma that characterizes the\nsampling distribution of our algorithm for 𝑡= 𝑡n.\nLemma 4.1 (Ambient Diffusion solution at 𝑡n). Let 𝑆𝑡n be the noisy training set as in L1 of\nAlgorithm 1.\nFor a fixed 𝑆𝑡n, let b𝑝𝑡n be the distribution at time 𝑡= 𝑡n that arises by using\nthe score of Algorithm 1 in the reverse process of Eq.(5) initialized at 𝒩(0, 𝐼𝑑).\nIt holds that\nb𝑝𝑡n =\n1\n|𝑆𝑡n |\nÍ\n𝑥𝑡n∈𝑆𝑡n 𝛿(𝑥−𝑥𝑡n).\nFor the proof, we refer to Section C.1.1. This Lemma extends the result of Kamb and Ganguli\n[KG24] from the standard diffusion objective of Eq.(3) to the training objective of Eq.(6). Given\nthis result, we can compare the information leakage of Ambient Diffusion at time 𝑡n compared to\nthe optimal distribution b𝑞𝑡n learned by DDPM at that time.\nLemma 4.2 (Information Leakage). Consider point 𝑥0 ∼𝒩(𝜇, Σ), a set 𝐴of size 𝑚generated i.i.d.\nby b𝑝𝑡n (optimal ambient solution at time 𝑡n with input 𝑥0) and a set 𝐷of size 𝑚generated i.i.d.\nby b𝑞𝑡n (optimal DDPM solution at time 𝑡n with input 𝑥0). Then the mutual information satisfies:\n𝐼(𝐷; 𝑥0) = 𝑚· 𝐼(𝐴; 𝑥0) = 𝑚\n2 log det(\n1−𝜎2\n𝑡n\n𝜎2\n𝑡n\nΣ + 𝐼).\nFor the proof, we refer to Section C.1.2. The above means that DDPM leaks much more information\nabout the training point compared to Ambient Diffusion, when asked to generate a collection of\nsamples from the model at time 𝑡n. Another way to see it, is that given 𝑚samples from DDPM at\ntime 𝑡n, one can get an estimator for 𝑥0 with error poly(1/𝑚), while with Ambient Diffusion, no\nconsistent estimation is possible. As expected, as 𝜎𝑡n ≈0, then no noise is added to create 𝑆𝑡n and\nhence the mutual information blows up. On the other extreme, as 𝜎𝑡n ≈1, then the models reveal\nno information about the original point. If the dataset contains multiple points, similar results\nabout the mutual information can be obtained (see Section C.1.3).\nThe above indicates that Ambient Diffusion can only memorize the noisy images. Our justification\nfor the improved performance in practice is that memorizing noise is much harder since noise is\nnot compressible. Even if the noisy images are perfectly memorized, they do not contain enough\ninformation to perfectly recover the training set (as shown above) and hence creativity will emerge. A\npossible conjecture is that under reasonable smoothness assumptions the concatenation of Ambient\nDiffusion (i.e., of a non-memorized trajectory (up to 𝑡n)) and of DDPM (i.e., of a memorized\none (from 𝑡n to 0)) will not lead to memorized outputs. Under this conjecture, controlling the\nhigh noise case is all you need to decrease memorization, and this is what our algorithm achieves.\nShowing non-trivial upper/lower bounds between the distribution learned by our algorithm and the\ndistribution learned by DDPM is an interesting theoretical problem that remains to be addressed.\n8\n\n\n4.2\nConnections to Feldman [Fel20]\nIn the previous section, we discussed ways to reduce the memorization. In this section, we consider\nwhat is the price to pay for reduced memorization, i.e., we analyze the trade-off between memorization\nand fidelity.\nWhile there is a significant amount of empirical research on connections between memorization\nand generation for diffusion models, our rigorous theoretical understanding is still lacking. In terms\nof theory, there are many works studying memorization-generalization trade-offs for machine learning\nalgorithms [Fel20, FZ20, BBF+21, BBS22, CDK22, Liv24, ADH+24] with several connections to\ndifferential privacy and stability in learning [BE02, XR17, BMN+18, RZ19, Fel20, SZ20]. Our work\nstudies this trade-off in diffusion models, inspired by the work of [Fel20].\nSection Overview.\nWe study the memorization-generalization trade-offs in the diffusion models\nwhen the data distribution is modeled as a mixture [SCK23, CKS24, GKL24]. In Section 4.2.1, we\ndefine the distribution to be learned as a mixture of distributions of subpopulations (e.g., dogs, cats,\netc.) with unknown mixing weights. This distribution is learned given a finite set 𝑍of size 𝑛and\nwe are interested in the generalization error of the trained model (at some fixed noise scale 𝜎𝑡). In\nTheorem 4.3 we express this generalization error into two terms, one of which is the error of the\nalgorithm for populations that are seen only once during training. We consider that the trained\nmodel “memorizes” when the error of these rare examples is small. Due to the error decomposition,\ngeneralization is related to the memorization error and its multiplying constant 𝜏1 that appears in\nTheorem 4.3. In Section 4.2.3 we analyze how this constant changes for different noise levels under\nthe assumption of [ZAR14, Fel20] that the mixing weights are heavy-tailed. We argue that when\nthe noise level is small, 𝜏1 is large and due to the decomposition, the only way to achieve good\ngeneralization is to memorize. For high noise levels, 𝜏1 becomes smaller and hence it is in principle\npossible to achieve generalization without excessive memorization.\n4.2.1\nSubpopulations Model of Feldman [Fel20]\nLet us consider a continuous data domain 𝑋⊆ℝ𝑑(e.g., images). We model the data distribution as a\nmixture of 𝑁fixed distributions 𝑀1, ..., 𝑀𝑁, where each component corresponds to a subpopulation\n(e.g., dogs, cats, etc.). For simplicity, we follow Feldman [Fel20] and assume that each component\n𝑀𝑖has disjoint support 𝑋𝑖(this can be relaxed, see Remark 1). Without loss of generality, let\n𝑋= ∪𝑖𝑋𝑖.\nWe will now describe the procedure of [Fel20] that assigns frequencies to each subpopulation of\nthe mixture.\n1. Consider a list of frequencies 𝜋= (𝜋1, 𝜋2, ..., 𝜋𝑁).\n2. For each component 𝑖∈[𝑁] of the mixture, select randomly and independently an element 𝑝𝑖\nfrom 𝜋.\n3. Finally, to obtain the mixing weights, we normalize the elements 𝑝1, ..., 𝑝𝑁, i.e., the weight of\ncomponent 𝑖is 𝐷𝑖=\n𝑝𝑖\nÍ\n𝑗∈[𝑁] 𝑝𝑗.\nWe denote by 𝒟𝜋the distribution over the mixing coefficients tuple (𝐷1, ..., 𝐷𝑁). A sample\n𝐷∼𝒟𝜋is just a list of the normalized frequencies of the 𝑁subpopulations. If 𝐷∼𝒟𝜋, then we\ncan define the true mixture as\n9\n\n\n𝑀𝐷(𝑥) =\nÕ\n𝑖∈[𝑁]\n𝐷𝑖𝑀𝑖(𝑥) .\nmixing weight of class 𝑖\ndistribution of class 𝑖\nThe above random distribution corresponds to the subpopulations model introduced by Feldman\n[Fel20].\n4.2.2\nAdaptation to Diffusion\nAs explained in the Background Section 2, one way to train a generative model in order to generate\nfrom the target 𝑀𝐷is to estimate the score function ∇𝑥log 𝑀𝐷𝑡for all levels of noise indexed by\n𝑡. For the analysis of this Section, we consider the case of a single fixed 𝑡. We define learning\nalgorithms 𝐴as (potentially randomized) mappings from datasets 𝑍to score functions 𝑠𝜃∼𝐴(𝑍).\nAs in Feldman [Fel20], we are interested about the expected error of 𝐴conditioned on dataset\nbeing equal to 𝑍∈𝑋𝑛as\nerr(𝜋, 𝐴|𝑍) = 𝔼𝐷∼𝒟𝜋(·|𝑍)𝔼𝑠𝜃∼𝐴(𝑍)err𝑀𝐷(𝑠𝜃) ,\nwhere 𝐷∼𝒟𝜋is a (random) collection of mixing weights and err𝑀𝐷(𝑠𝜃) = 𝔼𝑥0∼𝑀𝐷𝐿(𝑠𝜃; 𝑥0) for some\nloss function 𝐿is the expected loss of the score function 𝑠𝜃under the true population 𝑀𝐷. The\nresults we will present shortly are agnostic to the choice of 𝐿, but the reader should think of 𝐿as\nthe noise prediction loss used in (4) for a fixed time 𝑡.\nThe quantity err(𝜋, 𝐴|𝑍) measures the generalization error of the score function of the learning\nalgorithm 𝐴conditional on the training set being 𝑍. We will show that the population loss of an\nalgorithm given a dataset 𝑍is at least:\n1. its loss on the unseen part of the domain, i.e., the population loss in 𝑋\\ 𝑍plus\n2. its loss on the elements of 𝑍that belong to subpopulations that are represented only once\nin 𝑍(i.e., the dataset contains a single image of a dog or a single image of a car). This loss,\ndenoted by err𝑍(𝐴, 1), is scaled up by a coefficient 𝜏1, which expresses the ”likelihood” of\nhaving such subpopulations.\nTypically, we define:\n𝜏1 = 𝔼𝛼∼𝜋[𝛼2(1 −𝛼)𝑛−1]\n𝔼𝛼∼𝜋[𝛼(1 −𝛼)𝑛−1] ,\nwhere 𝜋is the marginal distribution 𝜋(𝑎) = ℙ𝐷[𝐷𝑖= 𝑎]. Note that, because the random process of\npicking the mixing weights is run independently for any 𝑖∈[𝑁], the marginal is the same across\ndifferent 𝑖’s (and hence we omit the index 𝑖from 𝜋). We are now ready to present our result.\nTheorem 4.3 (Informal, see Theorem A.1). It holds that\nerr(𝜋, 𝐴|𝑍) ≥errunseen(𝜋, 𝐴|𝑍) + 𝜏1 · err𝑍(𝐴, 1) .\nThe above result can be extended to subpopulations represented by 2 or more examples in 𝑍(see\nAppendix A). The above inequality relates the population error of the model with its loss on some\nparts of the training set. The crucial parameter that relates the two quantities is the coefficient\n𝜏1. If the coefficient 𝜏1 is large, it means that if the model does not fit the ”rare examples” of\nthe dataset, it will have to pay roughly 𝜏1 in the generalization error. As shown by [Fel20], 𝜏1 is\ncontrolled by how much heavy-tailed is the distribution of the frequencies of the mixture model.\nThis is the topic of the next section, where we also investigate the effect of adding noise to the\ntraining set.\n10\n\n\n4.2.3\nHeavy Tails and the Role of Noise\nIn this section, we are going to formally explain what it means for the frequencies of the original\ndataset to be heavy-tailed [ZAR14, Fel20]. This heavy-tailed structure will then allow us to control\nthe generalization error in Theorem 4.3. We will be interested in subpopulations that have only one\nrepresentative in the training set 𝑍(these are the examples that will cost roughly 𝜏1 in the error\nof Theorem 4.3). We will refer to them as single subpopulations. For this to happen given that\n|𝑍| = 𝑛, it should be roughly speaking the case where some frequencies 𝐷𝑖are of order 1/𝑛. The\nquantity that controls how many of the frequencies 𝐷𝑖will be of order 1/𝑛is the mass that the\ndistribution 𝜋(𝑎) = ℙ𝐷[𝐷𝑖= 𝑎] assigns to the interval [1/(2𝑛), 1/𝑛]. Typically, we will call a list of\nfrequencies 𝜋heavy-tailed if\nweight\n\u0012\n𝜋,\n\u0014 1\n2𝑛, 1/𝑛\n\u0015\u0013\n= Ω(1) .\n(7)\nIn words, there should be a constant number of subpopulations with frequencies of order 𝑂(1/𝑛).\nThis definition is important because it can then lower bound the value 𝜏1 in Theorem 4.3 and hence\nit can lower bound the generalization loss of not fitting single subpopulations.\nLemma 4.4 (Informal, see Lemma A.2 and Lemma 2.6 in [Fel20]). Consider a dataset of size 𝑛\nand assume that 𝜋is heavy-tailed, as in (7). Then 𝜏1 = Ω(1/𝑛).\nOn the contrary, when 𝜋is not heavy-tailed, 𝜏1 will be small and hence generalization is not hurt\nby not memorizing (see Lemma A.3). Next, we are going to inspect how the noise scale affects\nthe heavy-tailed structure of the frequencies and hence the value of 𝜏1. For an illustration, we will\nconsider the most standard model, that of a mixture of Gaussian subpopulations (similar results are\nexpected for more general population models; we note that the previous results can be naturally\nadapted for the GMM and other cases, see Remark 1 and the discussion in [Fel20]). Let us consider\na density 𝑞0 = Í𝑁\n𝑖=1 𝑤𝑖𝒩(𝜇𝑖, 𝐼) = Í\n𝑖𝑤𝑖𝒩𝑖. We will say that two components 𝒩𝑖, 𝒩𝑗are 𝜀-separated\nif TV(𝒩𝑖, 𝒩𝑗) > 2𝜀and can be 𝜀-merged if TV(𝒩𝑖, 𝒩𝑗) ≤𝜀. If 𝒩𝑖and 𝒩𝑗are merged, we consider\nthat the new coefficient is 𝑤𝑖+ 𝑤𝑗.\nLemma 4.5 (Informal, see Section A.4). Consider the GMM density 𝑞0 and let 𝑞𝑡be the density of\nthe forward diffusion process at time 𝑡with schedule 𝜎𝑡∈[0, 1]. Consider any pair of components\n𝒩𝑖, 𝒩𝑗in 𝑞0 with total variation 𝐶𝑖𝑗for some absolute constant 𝐶𝑖𝑗and let 𝒩𝑡\n𝑖, 𝒩𝑡\n𝑗be the associated\ndistributions in 𝑞𝑡.\n• (Low Noise) If 𝜎𝑡≤\np\n1 −(2𝜀/𝐶𝑖𝑗)2, then 𝒩𝑡\n𝑖, 𝒩𝑡\n𝑗are 𝜀-separated.\n• (High Noise) If 𝜎𝑡≥\np\n1 −(𝜀/𝐶𝑖𝑗)2, then 𝒩𝑡\n𝑖, 𝒩𝑡\n𝑗are 𝜀-merged with coefficient 𝑤𝑖+ 𝑤𝑗.\nFor a more formal treatment, we refer to Section A.4.\nThe above Lemma has the following\ninterpretations. If the noise level is small, the originally separated subpopulations (at 𝑡= 0) will\nremain separated. This implies that if the frequencies (i.e., the mixing weights) were originally\nheavy-tailed (as in the above discussion), they will remain heavy-tailed even in the low-noise regime,\ni.e. Lemma 4.4 applies (𝜏1 is large). On the other side, as we increase 𝑡, the clusters start to merge\nand the heavy-tailed distribution of the mixing coefficients becomes lighter (until all the clusters\nare merged into a single one). Hence, 𝜏1 will be small. This conceptually indicates that there is no\nreason for memorizing the training noisy images 𝑥𝑡(and hence the original images 𝑥0 which do not\nappear during training).\n11\n\n\nTable 1: FID and Memorization results comparing DDPM and Algorithm 1. Memorization is\nmeasured as DINOv2 similarity between generated samples and their nearest training neighbors.\nWe achieve the same or better FID with significantly lower memorization.\n# Train Images\n300\n1k\n3k\nDDPM Ours\nDDPM Ours\nDDPM Ours\nCIFAR-10\nFID\n25.1\n23.91\n10.46 10.36\n14.73 14.26\nS>0.9\n78.96 44.84\n75.86 69.08\n53.40 52.24\nS>0.925\n67.2\n20.22\n57.98 47.26\n11.92 11.36\nS>0.95\n56.56\n9.64\n43.44 26.34\n0.08\n0.06\nFFHQ\nFID\n16.21 15.05\n12.26\n11.3\n6.42\n6.46\nS>0.85\n63.38 49.68\n55.36 32.08\n21.58 20.08\nS>0.875\n55.48 40.01\n43.82 17.48\n4.98\n4.53\nS>0.9\n47.86 29.86\n33.92\n7.52\n0.46\n0.42\nImageNet\nFID\n——–\n50.2\n47.19\n40.66 39.87\nS>0.9\n——–\n54.72 26.68\n32.86 28.40\nS>0.925\n——–\n41.66 15.56\n12.32\n9.44\nS>0.95\n——–\n25.86\n5.54\n6.08\n4.02\n5\nExperiments\nTable 2: Comparison between DDPM, our Algorithm 1 and results obtained by training with\nonly corrupted data (masking or additive Gaussian noise). As shown, our algorithm achieves low\nmemorization since it uses noisy data in the high-noise regime, but it also achieves low FID (contrary\nto the algorithms only using corrupted data) as it can copy the high-frequency details from the\ntraining samples.\nMetric\n# Training Images\n300\n1k\n3k\nDDPM Masking Noise Ours DDPM Masking Noise Ours DDPM Masking Noise Ours\nFID\n16.21\n23.40\n27.92 15.05\n12.26\n15.73\n25.57 11.3\n6.42\n7.44\n16.28 6.46\nSim > 0.85\n63.38\n53.73\n29.12 49.68\n55.36\n38.74\n14.83 32.08\n21.58\n19.74\n12.08 20.08\nSim > 0.875\n55.48\n41.37\n18.73 40.01\n43.82\n22.94\n9.37 17.48\n4.98\n4.56\n3.32\n4.53\nSim > 0.9\n47.86\n30.34\n10.60 29.86\n33.92\n10.08\n6.49\n7.52\n0.46\n0.43\n0.36\n0.42\n5.1\nMemorization in Unconditional Models\nWe start our experimental evaluation by measuring the memorization and performance of un-\nconditional diffusion models in several controlled settings.\nSpecifically, we train models from\nscratch on CIFAR-10, FFHQ, and (tiny) ImageNet using 300, 1000 and 3000 training samples. For\neach one of these settings, we compute the Fr´echet Inception Distance [HRU+17] (FID) between\n50,000 generated samples and 50,000 dataset samples as a measure of quality. Following prior\n12\n\n\nwork [SSG+22, SSG+23, DSD+23], we measure memorization by computing the similarity score\n(i.e., inner product) of each generated sample to its nearest neighbor in the embedding space of\nDINOv2 [ODM+23]. For all these experiments, we compare the performance of Algorithm 1 against\nthe regular training of diffusion models (see Eq.(3)).\nChoice of 𝑡n.\nOur method has a single parameter 𝑡n that needs to be controlled. We argue\nthat there is an interval (𝑡min, 𝑡max) that contains reasonable choices of 𝑡n. Setting 𝑡n too low, i.e.,\n(𝑡n ≤𝑡min), essentially reverts back to the original algorithm that produces memorized images\nof good quality. But also, setting 𝑡n too high, i.e., 𝑡n ≥𝑡max, will also lead to memorization as\nthere is more time in the sampling trajectory (the interval [0, 𝑡max]), where we use the memorized\nscore. Values in the range (𝑡min, 𝑡max) achieve low memorization and strike good balances in the\nquality-memorization trade-off.\nDecreasing memorization without sacrificing quality.\nMost of the prior mitigation strategies\nfor memorization often decrease the image generation quality. Here, we ask: how much do we need\nto memorize to achieve a given image quality? To answer this, we tune the value 𝑡n to train models\nusing Algorithm 1 that match the FID obtained by DDPM, and we measure their memorization\nlevels. To report memorization, we use three thresholds in the similarities of DINOv2 embeddings\nthat semantically correspond to: i) potentially memorized image, ii) (partially) memorized image,\nand, iii) exact copy of an image in the training set. The thresholds are tuned separately for each\ndataset to express these semantics. We present analytic results for 300, 1k and 3k training images\nfrom CIFAR-10, FFHQ and (tiny)-ImageNet in Table 13. As shown, for the same or better FID,\nour models achieve significantly lower memorization levels. This leads to the surprising conclusion\nthat models learned by the DDPM loss are not Pareto optimal for small datasets. That said, the\nbenefit from our algorithm in both FID and memorization shrinks as the dataset grows.\nOther points in the Pareto frontier.\nSo far, our goal was to reduce memorization while keeping\nFID the same as DDPM. However, by appropriately tuning the value 𝑡n, we can achieve other points\nin the Pareto frontier that achieve varying trade-offs between memorization and quality of generated\nimages. We present these results for a model trained on 300 images from FFHQ in Figure 1. We see\nthat setting 𝜎𝑡n ∈[0.4, 4] corresponds to Pareto optimal points, while setting the value of 𝑡n too low\nor too high brings us back to the DDPM performance, as expected. For 𝜎𝑡n = 4, we almost match\nthe FID that DDPM gets with 1000 images, while we only use 300 images for training, establishing\nour Algorithm as much more data-efficient than DDPM.\nComparison with other mitigation strategies.\nFor completeness, we include comparisons\nwith two other mitigation strategies that reduce memorization in the unconditional setting. These\nmethods are known to achieve lower memorization but at the expense of FID. We compare with a\nmodel trained on linearly corrupted data (random inpainting), as in the work of [DSD+23], and a\nmodel trained with only noisy data as in [DDD24]. We present the results in Table 2. As shown,\nour algorithm produces superior behavior as it achieves lower memorization for the same or better\nFID. The superior performance comes from the ability our method has to generate high-frequency\n3For tiny ImageNet, we do not report results in the 300 samples setting since there are 200 different classes and so\nfor some of the classes we do not observe any samples.\n13\n\n\ndetails, contrary to the existing methods that only use solely noisy data and are not capable of such\nbehavior.\n5.2\nMemorization in Text-Conditional Models\nWe continue our evaluation in text-conditional models. Here, the primal source of memorization is\nthe text-conditioning itself. Wen, Liu, Chen, and Lyu [WLCL24] observe that for certain trigger\nprompts, the prediction of the network always converges to the same training point, independent of\nthe image initialization. Our method mitigates image memorization by training with noisy images,\nso by itself, it cannot mitigate memorization that arises from the text-conditioning. However,\nwe will show that when we combine our method with strategies that mitigate the impact of text\nmemorization, we achieve state-of-the-art results in memorization reduction while keeping the quality\nof the generated images high.\nTable 3: Memorization and FID results for text-conditional models. Sim denotes the average similarity\nbetween a generated sample and its nearest neighbor in the dataset, while 95% is the 95% percentile of the\nsimilarities distribution. CLIP measures the image-text alignment. The combination of our method with\nexisting methods from Somepalli et al. [SSG+23] (S23) and Wen, Liu, Chen, and Lyu [WLCL24] (W24)\nachieves strong CLIP/FID results with reduced memorization.\nMethod\nSim\n95%\nCLIP\nFID\nWithout text mitigation:\nBaseline\n0.378\n0.649\n0.306\n18.18\nOurs\n0.373\n0.636\n0.305\n18.34\nText mitigation:\nS23\n0.319\n0.573\n0.302\n20.55\nS23+ ours\n0.308\n0.547\n0.306\n21.30\nW24\n0.208\n0.300\n0.293\n21.44\nW24+ ours\n0.192\n0.267\n0.293\n20.74\nFollowing prior work [SSG+23], we finetune Stable Diffusion on 10k image-text pairs from a\ncurated subset of LAION [SBV+22] and we measure image quality and memorization of the resulting\nmodels. We compare with existing state-of-the-art methods for reducing memorizing arising from\nthe text-conditioning. Specifically, we compare with the work of Somepalli et al. [SSG+23] where\ncorruption is added to the text-embedding during training and with the work of Wen, Liu, Chen,\nand Lyu [WLCL24] where the model is explicitly trained to pay attention to the visual content (for\ndetails, we refer the reader to the associated papers).\nWe include all the results in Table 3. As shown, the combination of our work with existing\nmethods achieves state-of-the-art memorization performance while performing on par in terms of\nimage quality. As expected, without any text-mitigation our algorithm fails to improve significantly\nthe memorization since the model remains heavily reliant on the text-conditioning, effectively\nignoring the visual content.\n14\n\n\n6\nConclusion and Future Work\nOur work provides a positive note on the rather pessimistic landscape of results regarding the\nmemorization-quality trade-off in diffusion models. We manage to push the Pareto frontier in various\ndata availability settings for both text-conditional and unconditional models. We further provide\ntheoretical evidence for the plausibility of generation of diverse structures without memorization.\nWe remark that our method does not come with any privacy guarantees or optimality properties and\nthat despite some encouraging first theoretical evidence, an end-to-end analysis for the proposed\nalgorithm is currently lacking. We believe that these constitute exciting research directions for\nfuture research.\nReferences\n[AAL23] Jamil Arbas, Hassan Ashtiani, and Christopher Liaw. Polynomial time and private\nlearning of unbounded gaussian mixture models. In International Conference on\nMachine Learning, pages 1018–1040. PMLR, 2023.\n[ABVE23] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants:\nA unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\n[ADH+24] Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, and Daniel M\nRoy.\nInformation complexity of stochastic convex optimization: Applications to\ngeneralization and memorization. arXiv preprint arXiv:2402.09327, 2024.\n[ANS23] Gil Appel, Juliana Neelbauer, and David A Schweidel. Generative ai has an intellectual\nproperty problem. Harvard Business Review, 7, 2023.\n[BBC+22] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang,\nMicah Goldblum, Jonas Geiping, and Tom Goldstein.\nCold diffusion: Inverting\narbitrary image transforms without noise. arXiv preprint arXiv:2208.09392, 2022.\n[BBDBM24] Giulio Biroli, Tony Bonnaire, Valentin De Bortoli, and Marc M´ezard. Dynamical\nregimes of diffusion models. Nature Communications, 15(1):9957, 2024.\n[BBDD24] Joe Benton, VD Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-linear\nconvergence bounds for diffusion models via stochastic localization. 2024.\n[BBF+21] Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When\nis memorization of irrelevant training data necessary for high-accuracy learning? In\nProceedings of the 53rd annual ACM SIGACT symposium on theory of computing,\npages 123–132, 2021.\n[BBS22] Gavin Brown, Mark Bun, and Adam Smith. Strong memory lower bounds for learning\nnatural models. In Conference on Learning Theory, pages 4989–5029. PMLR, 2022.\n[BE02] Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. The Journal of\nMachine Learning Research, 2:499–526, 2002.\n15\n\n\n[BMN+18] Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff.\nLearners that use little information. In Algorithmic Learning Theory, pages 25–55.\nPMLR, 2018.\n[BWCS24] Weimin Bai, Yifei Wang, Wenzheng Chen, and He Sun. An expectation-maximization\nalgorithm for training clean diffusion models from corrupted observations. arXiv\npreprint arXiv:2407.01014, 2024.\n[CBLC22] Pierre Chambon, Christian Bluethgen, Curtis P Langlotz, and Akshay Chaudhari.\nAdapting pretrained vision-language foundational models to medical imaging domains.\narXiv preprint arXiv:2210.04133, 2022.\n[CDK22] Chen Cheng, John Duchi, and Rohith Kuditipudi. Memorize to generalize: on the\nnecessity of interpolation in high dimensional linear regression. In Conference on\nLearning Theory, pages 5528–5560. PMLR, 2022.\n[CHN+23] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian\nTramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data\nfrom diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23),\npages 5253–5270, 2023.\n[CKS24] Sitan Chen, Vasilis Kontonis, and Kulin Shah. Learning general gaussian mixtures\nwith efficient score matching, 2024.\n[CLX24] Chen Chen, Daochang Liu, and Chang Xu. Towards memorization-free diffusion\nmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8425–8434, 2024.\n[DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\nhypothesis. arXiv preprint arXiv:2208.05314, 2022.\n[DCD24] Giannis Daras, Yeshwanth Cherapanamjeri, and Constantinos Daskalakis. How much\nis a noisy image worth? data scaling laws for ambient diffusion. arXiv preprint\narXiv:2411.02780, 2024.\n[DDD24] Giannis Daras, Alexandros G Dimakis, and Constantinos Daskalakis. Consistent\ndiffusion meets tweedie: Training exact ambient diffusion models with noisy data.\narXiv preprint arXiv:2404.10177, 2024.\n[DDDD23] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis.\nConsistent diffusion models: Mitigating sampling drift by learning to be consistent.\narXiv preprint arXiv:2302.09057, 2023.\n[DDT+23] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alex Dimakis, and Peyman\nMilanfar. Soft diffusion: Score matching with general corruptions. Transactions on\nMachine Learning Research, 2023.\n[Die24] Sander Dieleman. Diffusion is spectral autoregression, 2024.\n16\n\n\n[DSD+23] Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alex Dimakis, and\nAdam Klivans. Ambient diffusion: Learning clean distributions from corrupted data.\nIn Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n[Efr11] Bradley Efron.\nTweedie’s formula and selection bias.\nJournal of the American\nStatistical Association, 106(496):1602–1614, 2011.\n[Fel20] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In\nProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,\npages 954–959, 2020.\n[FZ20] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why:\nDiscovering the long tail via influence estimation. Advances in Neural Information\nProcessing Systems, 33:2881–2891, 2020.\n[GDP+23] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On\nmemorization in diffusion models. arXiv preprint arXiv:2310.02664, 2023.\n[GKL24] Khashayar Gatmiry, Jonathan Kelner, and Holden Lee. Learning mixtures of gaussians\nusing diffusion models. arXiv preprint arXiv:2404.18869, 2024.\n[HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.\nAdvances in Neural Information Processing Systems, 33:6840–6851, 2020.\n[HRU+17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp\nHochreiter. Gans trained by a two time-scale update rule converge to a local nash\nequilibrium. Advances in neural information processing systems, 30, 2017.\n[HSK+25] Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, and Franziska\nBoenisch. Finding nemo: Localizing neurons responsible for memorization in diffusion\nmodels. Advances in Neural Information Processing Systems, 37:88236–88278, 2025.\n[JKS+24] Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian\nTogelius, and Yuki Mitsufuji. Classifier-free guidance inside the attraction basin may\ncause memorization. arXiv preprint arXiv:2411.16738, 2024.\n[KAAL22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design\nspace of diffusion-based generative models. Advances in neural information processing\nsystems, 35:26565–26577, 2022.\n[KEME23] Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad. Gsure-based diffusion\nmodel training with corrupted data. arXiv preprint arXiv:2305.13128, 2023.\n[KG24] Mason Kamb and Surya Ganguli. An analytic theory of creativity in convolutional\ndiffusion models. arXiv preprint arXiv:2412.20292, 2024.\n[KSH+24] Joshua Kazdan, Hao Sun, Jiaqi Han, Felix Petersen, and Stefano Ermon. Cpsample:\nClassifier protected sampling for guarding training data during diffusion. arXiv preprint\narXiv:2409.07025, 2024.\n17\n\n\n[KYKM23] Vladimir Kulikov, Shahar Yadin, Matan Kleiner, and Tomer Michaeli. Sinddm: A\nsingle image denoising diffusion model. In International conference on machine learning,\npages 17920–17930. PMLR, 2023.\n[LC24] Marvin Li and Sitan Chen. Critical windows: non-asymptotic theory for feature\nemergence in diffusion models. arXiv preprint arXiv:2403.01633, 2024.\n[LCBH+22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.\nFlow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.\n[LGL22] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to\ngenerate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.\n[LGWM24] Xiao Liu, Xiaoliu Guan, Yu Wu, and Jiaxu Miao. Iterative ensemble training with\nanti-gradient control for mitigating memorization in diffusion models. In European\nConference on Computer Vision, pages 108–123. Springer, 2024.\n[Liv24] Roi Livni. Information theoretic lower bounds for information theoretic upper bounds.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[LY15] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. CS231N Course\nReport, Stanford University, 2015.\n[LYM+24] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim\nGupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al.\nHolistic evaluation of text-to-image models. Advances in Neural Information Processing\nSystems, 36, 2024.\n[ODM+23] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil\nKhalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\net al. Dinov2: Learning robust visual features without supervision. arXiv preprint\narXiv:2304.07193, 2023.\n[RALL24] Fran¸cois Rozet, G´erˆome Andry, Fran¸cois Lanusse, and Gilles Louppe.\nLearning\ndiffusion priors from observations by expectation maximization.\narXiv preprint\narXiv:2405.13712, 2024.\n[RKW+24] Brendan Leigh Ross, Hamidreza Kamkari, Tongzi Wu, Rasa Hosseinzadeh, Zhaoyan Liu,\nGeorge Stein, Jesse C Cresswell, and Gabriel Loaiza-Ganem. A geometric framework\nfor understanding memorization in generative models. arXiv preprint arXiv:2411.00113,\n2024.\n[RLZ+24] Jie Ren, Yaxin Li, Shenglai Zeng, Han Xu, Lingjuan Lyu, Yue Xing, and Jiliang Tang.\nUnveiling and mitigating memorization in text-to-image diffusion models through cross\nattention. In European Conference on Computer Vision, pages 340–356. Springer,\n2024.\n[RZ19] Daniel Russo and James Zou. How much does your data exploration overfit? controlling\nbias via information usage. IEEE Transactions on Information Theory, 66(1):302–323,\n2019.\n18\n\n\n[SBS23] Christopher Scarvelis, Haitz S´aez de Oc´ariz Borde, and Justin Solomon. Closed-form\ndiffusion models. arXiv preprint arXiv:2310.12395, 2023.\n[SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\net al. Laion-5b: An open large-scale dataset for training next generation image-text\nmodels. Advances in Neural Information Processing Systems, 35:25278–25294, 2022.\n[SCK23] Kulin Shah, Sitan Chen, and Adam Klivans. Learning mixtures of gaussians using the\nddpm objective. In Advances in Neural Information Processing Systems, volume 36,\npages 19636–19649. Curran Associates, Inc., 2023.\n[SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the\ndata distribution. Advances in Neural Information Processing Systems, 32, 2019.\n[SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano\nErmon, and Ben Poole. Score-based generative modeling through stochastic differential\nequations. arXiv preprint arXiv:2011.13456, 2020.\n[SSG+22] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\nDiffusion art or digital forgery? investigating data replication in diffusion models.\narXiv preprint arXiv:2212.03860, 2022.\n[SSG+23] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Gold-\nstein. Understanding and mitigating copying in diffusion models. arXiv preprint\narXiv:2305.20086, 2023.\n[SZ20] Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via con-\nditional mutual information. In Conference on Learning Theory, pages 3437–3452.\nPMLR, 2020.\n[TKC22] Florian Tram`er, Gautam Kamath, and Nicholas Carlini. Position: Considerations\nfor differentially private learning with large-scale public pretraining. In Forty-first\nInternational Conference on Machine Learning, 2022.\n[Vin11] Pascal Vincent. A connection between score matching and denoising autoencoders.\nNeural computation, 23(7):1661–1674, 2011.\n[WBL+24] Yifei Wang, Weimin Bai, Weijian Luo, Wenzheng Chen, and He Sun. Integrating\namortized inference with diffusion models for learning clean distribution from corrupted\nimages. arXiv preprint arXiv:2407.11162, 2024.\n[WBZ+25] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan,\nand Houqiang Li. Sindiffusion: Learning a diffusion model from a single natural image.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2025.\n[WCS+24] Zhenting Wang, Chen Chen, Vikash Sehwag, Minzhou Pan, and Lingjuan Lyu.\nEvaluating and mitigating ip infringement in visual generative ai. arXiv preprint\narXiv:2406.04662, 2024.\n19\n\n\n[WLCL24] Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, explaining, and\nmitigating memorization in diffusion models. In The Twelfth International Conference\non Learning Representations, 2024.\n[WLHH24] Jing Wu, Trung Le, Munawar Hayat, and Mehrtash Harandi. Erasediff: Erasing data\ninfluence in diffusion models. arXiv preprint arXiv:2401.05779, 2024.\n[XR17] Aolin Xu and Maxim Raginsky.\nInformation-theoretic analysis of generalization\ncapability of learning algorithms. Advances in neural information processing systems,\n30, 2017.\n[ZAR14] Xiangxin Zhu, Dragomir Anguelov, and Deva Ramanan. Capturing long-tail distribu-\ntions of object subcategories. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 915–922, 2014.\n[ZLL+24] Benjamin J Zhang, Siting Liu, Wuchen Li, Markos A Katsoulakis, and Stanley J Osher.\nWasserstein proximal operators describe score-based generative models and resolve\nmemorization. arXiv preprint arXiv:2402.06162, 2024.\n20\n\n\nA\nSubpopulations Model and Connections to Diffusion Models\nIn this section, we present a more extensive exposition of the framework of the work of [Fel20].\nMoreover, we adapt this framework to diffusion models.\nA.1\nSubpopulations Model of Feldman [Fel20]\nLet us recall the subpopulations model of [Fel20]. Let us consider a continuous data domain 𝑋⊆ℝ𝑑.\nWe model the data distribution as a mixture of 𝑁fixed distributions 𝑀1, ..., 𝑀𝑁, where each\ncomponent corresponds to a subpopulation. For simplicity, we follow Feldman [Fel20] and assume\nthat each component 𝑀𝑖has disjoint support 𝑋𝑖(we can relax this condition, see Remark 1).\nWithout loss of generality, let 𝑋= ∪𝑖𝑋𝑖. We will now describe the procedure of [Fel20] that assigns\nfrequencies to each subpopulation of the mixture.\n1. First consider a (fixed) list of frequencies 𝜋= (𝜋1, 𝜋2, ..., 𝜋𝑁).\n2. For each component 𝑖∈[𝑁] of the mixture, we select randomly and independently an element\n𝑝𝑖from the list 𝜋.\n3. Finally, to obtain the mixing weights, we normalize the weights 𝑝1, ..., 𝑝𝑁, i.e., the weight of\ncomponent 𝑖is 𝐷𝑖=\n𝑝𝑖\nÍ\n𝑗∈[𝑁] 𝑝𝑗.\nWe summarize the above as follows:\nDefinition 1 (Random Frequencies [Fel20]). For the mixing weights, we first consider a list of\nsubpopulation frequencies 𝜋= (𝜋1, ..., 𝜋𝑁). The procedure is the following: we randomly pick 𝑝𝑖\nfrom the list 𝜋for any index 𝑖∈[𝑁] and then we normalize (𝑝𝑖/Í\n𝑗𝑝𝑗denotes the frequency of\nsubpopulation 𝑖). We denote by 𝒟𝜋the distribution over probability mass functions on [𝑁] induced\nby the above procedure.\nA sample 𝐷∼𝒟𝜋is just a list of the frequencies of the 𝑁subpopulations.\nWe also denote by 𝜋the resulting marginal distribution over the frequency of any single element\nin 𝑖, i.e.,\n𝜋(𝑎) = ℙ𝐷∼𝒟𝜋[𝐷𝑖= 𝑎] .\n(8)\nHence, if 𝐷∼𝒟𝜋, then we can define the true mixture as\n𝑀𝐷(𝑥) =\nÕ\n𝑖∈[𝑁]\n𝐷𝑖𝑀𝑖(𝑥) .\nThe above random distribution corresponds to the subpopulations model introduced by Feldman\n[Fel20]. Intuitively the choice of the random coefficients for the mixture corresponds to the fact\nthat the learner does not know the true frequencies of the subpopulations.\nA.2\nAdaptation of [Fel20]’s Result to Diffusion Models\nAs explained in the Background Section 2, one way to train a generative model is to estimate the\nscore function ∇log 𝑀𝐷𝑡for all levels of noise indexed by 𝑡. For the analysis of this Section, we\nconsider the case of a single fixed 𝑡. We define learning algorithms 𝐴as (potentially randomized)\n21\n\n\nmappings from datasets 𝑍to score functions 𝑠𝜃∼𝐴(𝑍). We further define the expected error of 𝐴\nconditioned on dataset being equal to 𝑍∈𝑋𝑛(eventually 𝑍will be drawn i.i.d. from 𝑀𝐷) as\nerr(𝜋, 𝐴|𝑍) = 𝔼𝐷∼𝒟𝜋(·|𝑍)𝔼𝑠𝜃∼𝐴(𝑍)err𝑀𝐷(𝑠𝜃) ,\nwhere 𝐷∼𝒟𝜋is a random list of frequencies according to Definition 1 and err𝑀𝐷(𝑠𝜃) = 𝔼𝑥0∼𝑀𝐷𝐿(𝑠𝜃; 𝑥0)\nfor some loss function 𝐿. The results we will present shortly are agnostic to the choice of loss\nfunction 𝐿, but the reader should think of 𝐿as the noise prediction loss used in (4) for a fixed time\n𝑡.\nWe remark that the quantity err(𝜋, 𝐴|𝑍) measures the generalization error of the output score\nfunction (according to loss function 𝐿) of the learning algorithm 𝐴conditional on the training set\nbeing 𝑍. We will relate this generalization error with the loss in the training set. Recall that any\nsubpopulation 𝑖∈[𝑁] of the mixture is associated with a domain 𝑋𝑖(and 𝑋𝑖∩𝑋𝑗= ∅for 𝑖≠𝑗).\nLet 𝑛be the training set size. For any ℓ∈[𝑛], consider all the subpopulations 𝐼ℓ⊆[𝑁] such\nthat 𝑋𝑖∩𝑍= ℓfor 𝑖∈𝐼ℓ; in words, 𝑖∈𝐼ℓif there are exactly ℓrepresentatives of cluster 𝑖in the\ndataset 𝑍. We can now define 𝑍ℓ= {𝑥∈𝑋𝑖∩𝑍: 𝑖∈𝐼ℓ} ⊆𝑍. Note that the sets 𝑍1, ..., 𝑍𝑁partition\nthe training set 𝑍. For ℓ∈[𝑛], we define\nerrn𝑍(𝐴, ℓ) = 𝔼𝑠𝜃∼𝐴(𝑍)\nÕ\n𝑥∈𝑍ℓ\n𝑀𝑖𝑥(𝑥)𝐿(𝑠𝜃; 𝑥) .\n(9)\nHere 𝑖𝑥∈[𝑁] is the unique index of the component whose support contains 𝑥. In words, errn𝑍(𝐴, ℓ)\nis the loss of the algorithm 𝐴evaluated on the elements of the training set 𝑍that belong to\nsubpopulations will exactly ℓrepresentatives in 𝑍.\nWe show the following result, which is an adaptation of a result of [Fel20] and relates the\npopulation loss with the empirical losses errn𝑍(𝐴, 1), ..., errn𝑍(𝐴, 𝑛).\nTheorem A.1. Fix a number of samples 𝑛. Let {𝑀𝑖}𝑖∈[𝑁] be densities of subpopulations over\ndisjoint subdomains {𝑋𝑖}𝑖∈[𝑁]. Let 𝜋be the fixed list of frequencies as in Definition 1 and let 𝜋𝑁\nthe marginal distribution of (8). For any learning algorithm 𝐴and any fixed dataset 𝑍∈𝑋𝑛, it\nholds that\nerr(𝜋, 𝐴|𝑍) = errunseen(𝜋, 𝐴|𝑍) +\nÕ\nℓ∈[𝑛]\n𝜏ℓ· errn𝑍(𝐴, ℓ) ,\n(10)\nwhere\n1. errunseen(𝜋, 𝐴|𝑍) corresponds to the expected 𝑍-conditional loss of the algorithm 𝐴on the\npoints that do not appear in the training set 𝑍.\n2. 𝜏ℓis a coefficient that corresponds to the weight of having subpopulations with exactly ℓ\nrepresentatives. Given 𝒟𝜋and ℓ∈[𝑛], we define\n𝜏ℓ= 𝔼𝛼∼𝜋[𝛼ℓ+1(1 −𝛼)𝑛−ℓ]\n𝔼𝛼∼𝜋[𝛼ℓ(1 −𝛼)𝑛−ℓ] .\nFor the proof we refer to Section C.2.1. The above general form relates the population error of the\nmodel with its loss on the training set. The crucial parameters that relate the two quantities are\nthe coefficients 𝜏1, ..., 𝜏𝑛. If the coefficient 𝜏1 is large, it means that if the model does not fit the\ntraining examples that appear once in the dataset (”rare examples”), it will have to pay roughly 𝜏1\nin the generalization error. As shown by [Fel20], 𝜏1 is controlled by how much heavy-tailed is the\ndistribution of the frequencies of the mixture model. This is the topic of the next section, where we\nalso investigate the effect of adding noise to the training set.\n22\n\n\nRemark 1 (Gaussian Mixture Models). Subpopulations are often modeled as Gaussians. If the\nprobability of the overlap between the subpopulations is sufficiently small (the means are far),\nthen one can reduce this case to the disjoint one by modifying the components 𝑀𝑖to have disjoint\nsupports while changing the marginal distribution over 𝑍by at most 𝛿in the TV distance.\nA.3\nHeavy-Tailed Distributions of Frequencies\nIn this section, we are going to formally explain what it means for the frequencies of the original\ndataset to be heavy-tailed [ZAR14, Fel20]. This heavy-tailed structure will then allow us to control\nthe generalization error in Theorem 4.3. Following Feldman [Fel20], we will assume that the mixing\ncoefficients 𝐷1, ..., 𝐷𝑁are drawn from a heavy-tailed distribution since this is the case in most\ndatasets [Fel20, FZ20]. We will be interested in subpopulations that have only one representative in\nthe training set 𝑍(these are the examples that will cost roughly 𝜏1 in the error of Theorem 4.3).\nWe will refer to them as single subpopulations. For this to happen given that |𝑍| = 𝑛, it should be\nroughly speaking the case where some frequencies 𝐷𝑖are of order 1/𝑛.\nThe quantity that controls how many of the frequencies 𝐷𝑖will be of order 1/𝑛is the marginal\ndistribution 𝜋(𝑎) = ℙ𝐷[𝐷𝑖= 𝑎]. We first note that the expected number of singleton examples is\ndetermined by the weight of the entire tail of frequencies below 1/𝑛in 𝜋. In particular, one can\nshow (see [Fel20]) that the expected number of singleton points is at least\n𝑛\n2 · weight(𝜋, [0, 1/𝑛]) ,\nwhere\nweight(𝜋, [0, 1/𝑛]) := 𝔼𝐷∼𝒟\n\nÕ\n𝑖∈[𝑁]\n𝐷𝑖1{𝐷𝑖∈[0, 1/𝑛]}\n\n= 𝑁· 𝔼𝑎∼𝜋[𝑎1{𝑎∈[0, 1/𝑛]}] .\nThe above weight function essentially controls how heavy-tailed our distribution over frequencies\nis. Typically, we will call a list of frequencies 𝜋heavy-tailed if\nweight\n\u0012\n𝜋,\n\u0014 1\n2𝑛, 1/𝑛\n\u0015\u0013\n= Ω(1) .\nIn words, there should be a constant number of subpopulations with frequencies of order 1/𝑛. This\ndefinition is important because it can then lower bound the value 𝜏1 in Theorem 4.3 and hence it\ncan lower bound the generalization loss of not fitting single subpopulations.\nLemma A.2 (Lemma 2.6 in [Fel20]). For any 𝜋, it holds that 𝜏1 ≥\n1\n5𝑛· weight(𝜋, [ 1\n3𝑛, 2\n𝑛]).\nAs an illustration, if 𝜋is the Zipf distribution and the number of clusters 𝑁≥𝑛then 𝜏1 = Ω(1/𝑛)\nand weight(𝜋, [0, 1/𝑛]) = Ω(1) (see [Fel20] for more examples). On the contrary, when 𝜋is not\nheavy-tailed, 𝜏1 will be small.\nLemma A.3 (Lemma 2.7 in [Fel20]). Let 𝜋be a frequency prior such that for some 𝜃≤1/(2𝑛),\nweight(𝜋, [𝜃, 𝑡/𝑛]) = 0, where 𝑡= ln(1/(𝜃𝛽)), 𝛽= weight(𝜋, [0, 𝜃]). Then 𝜏1 ≤2𝜃.\nThe above lemma indicates that when the frequencies are not heavy-tailed then 𝜏1 is small (and\nhence generalization is not hurt by not memorizing).\n23\n\n\nA.4\nThe Effects of Noise\nIn this section we analyze the effect of adding noise to the training set. We distinguish two cases:\nthe low noise regime and the high noise regime.\nLow Noise Regime.\nWhen the noise level is small, the originally separated subpopulations (at\n𝑡= 0) will remain separated. This implies that if the frequencies of the subpopulations were originally\nheavy-tailed (as in the above discussion), they will remain heavy-tailed even in the low-noise regime.\nThis will imply that some clusters will be represented by singletons (ℓ= 1) and any algorithm\nthat satisfies errn𝑍(𝐴, 1) ≠0 has to pay 𝜏1 · errn𝑍(𝐴, 1) in the population error with 𝜏1 being lower\nbounded as in Lemma A.2. We interpret errn𝑍(𝐴, 1) ≈0 as evidence for memorization. To be more\nconcrete, we will need the following definition that is a smooth generalization of single representative\nof a subpopulation.\nDefinition 2. We will say that a subpopulation 𝐶has an 𝜀-smoothed single representative in a set\nof points 𝑆belonging to 𝐶if for any 𝑥, 𝑥′ ∈𝑆, it holds that ∥𝑥−𝑥′∥≤𝜀.\nIntuitively this means that if there are more than one images in the training set 𝑍from 𝐶, they are\nall very close to each other. This will be the case in diffusion with low noise.\nLemma A.4 (Subpopulations Remain Heavy-Tailed). Consider an example 𝑥0 ∈ℝ𝑑that is the\nunique representative of a subpopulation 𝑗∈[𝑁] in the training set 𝑍with ∥𝑥0∥≤poly(𝑑). Consider\n𝑚noisy copies {𝑥𝑖\n𝑡}𝑖∈[𝑚] of 𝑥0 at noise level 𝑡: 𝑥𝑖\n𝑡=\nq\n1 −𝜎2\n𝑡𝑥0 + 𝜎𝑡𝑧𝑖\n𝑡, 𝑧𝑖\n𝑡∼𝒩(0, 𝐼𝑑) . Then the\nsubpopulation 𝑗has a poly(1/𝑑)-smoothed single representative in the set {𝑥𝑖\n𝑡}𝑖∈[𝑚] for 𝜎𝑡= poly(1/𝑑)\nwith probability at least 1 −𝑚exp(−𝑑/2).\nThe proof appears in Section C.2.2. The above lemma implies that if the original dataset contains\nvarious well separated images (in the sense that correspond to representatives of single subpop-\nulations), then after adding noise to each one of them (and even if we create multiple copies for\neach example), the clusters will remain separated when 𝜎𝑡is small. This implies that the single\nsubpopulations remain and Lemma A.2 applies (𝜏1 is large).\nFor an illustration, let us consider the GMM density function 𝑞= Í𝑁\n𝑖=1 𝑤𝑖𝒩(𝜇𝑖, 𝐼).\nIt is\na standard calculation to see that at time 𝑡, the pdf of the forward diffusion process is 𝑞𝑡=\nÍ𝑁\n𝑖=1 𝑤𝑖𝒩(\nq\n1 −𝜎2\n𝑡𝜇𝑖, 𝐼), which means that the clusters are starting to concentrate around 0 as 𝑡→1\nand the images from different subpopulations are starting to look more and more indistinguishable\n(since the TV distance between the components is contracting with 𝑡). We will say that two\ncomponents 𝒩, 𝒩′ are 𝜀-separated if TV(𝒩, 𝒩′) > 2𝜀.\nLemma A.5 (Clusters Are Separated in Low Noise). Any pair of Gaussians with original total\nvariation 𝐶= 1/600 will be 𝜀-separated at noise scale 𝜎𝑡≤\np\n1 −(2𝜀/𝐶)2.\nFor the proof, see Section C.2.3.\nHigh Noise Regime.\nAs we increase 𝑡, we add more and more noise to the images. This means that\nthe clusters start to merge and the heavy-tailed distribution of the mixing coefficients becomes lighter\n(until all the clusters are merged into a single one). To illustrate this phenomenon, we will consider\na mixture of Gaussians, which is the standard model for clustering tasks (we expect similar behavior\n24\n\n\nfor more general mixture models). Let us again consider the density function 𝑞= Í𝑁\n𝑖=1 𝑤𝑖𝒩(𝜇𝑖, 𝐼).\nAlso, let the pdf of the forward diffusion process be 𝑞𝑡= Í𝑁\n𝑖=1 𝑤𝑖𝒩(\nq\n1 −𝜎2\n𝑡𝜇𝑖, 𝐼). We will say that\ntwo components 𝒩, 𝒩′ can be 𝜀-merged if TV(𝒩, 𝒩′) ≤𝜀.\nLemma A.6 (Clusters Merge in High Noise). Any pair of Gaussians with original total variation\n𝐶= 1/600 will be 𝜀-merged at noise scale 𝜎𝑡≥\np\n1 −(𝜀/𝐶)2.\nFor the proof, see Section C.2.3. As the clusters are getting merged, then their coefficients are\nadded up and their distribution is no more heavy-tailed. Hence, Lemma A.3 implies that 𝜏1 will be\nsmall. This conceptually indicates that there is no reason for memorizing the training noisy images\n𝑥𝑡(and hence the original images 𝑥0 which do not appear during training).\nGiven the above discussion, we reach the conclusion that if the frequencies of the original\nsubpopulations are heavy-tailed then, in the low-noise regime, the training set will have single\nsubpopulations and, in that case, fitting these single representatives is required for successful\ngeneralization. However, in the high-noise regime, the noisy training set does not have isolated\nexamples and, in principle, there is no reason to memorize its elements (and hence even elements of the\noriginal set). We believe that this discussion sheds some light on the nature of memorization needed\nfor optimal generative modeling and motivates our training Algorithm 1 that avoids memorization\nonly in the high-noise regime.\nB\nNoisy Data Training of stable Diffusion using v-Prediction\nThe variance-preserving forward process defines the following transition probability distribution:\n𝑝(𝑋𝑡= 𝑥𝑡|𝑋0) = 𝒩(𝑥𝑡; 𝛼𝑡𝑋0, 𝜎2\n𝑡𝐼) and 𝑝(𝑋𝑡= 𝑥𝑡|𝑋𝑠) = 𝒩(𝑥𝑡; (𝛼𝑡/𝛼𝑠)𝑋𝑠, 𝜎2\n𝑡|𝑠𝐼).\nwhere 𝜎2\n𝑡|𝑠= (1−\n𝛼2\n𝑡𝜎2\n𝑠\n𝜎2\n𝑡𝛼2𝑠)𝜎2\n𝑡. Let 𝑡n be the noise scale corresponding to the noisy data and the noisy data\n𝑥𝑡n from the clean data 𝑥0 has the probability distribution 𝑝(𝑋𝑡n = 𝑥𝑡n|𝑋0) = 𝒩(𝑥𝑡n; 𝛼𝑡n𝑋0, 𝜎2\n𝑡n𝐼).\nIn this case, the following Lemma holds.\nLemma B.1. 𝔼[𝑋𝑡n|𝑋𝑡] =\n𝛼𝑡n 𝜎2\n𝑡|𝑡n\n𝜎2\n𝑡\n𝔼[𝑋0|𝑋𝑡] +\n𝛼𝑡𝜎2\n𝑡n\n𝜎2\n𝑡𝛼𝑡n 𝑋𝑡.\nProof. Let 𝑝𝑡(·) denote the probability density of the random variable 𝑋𝑡. Observe that 𝑋𝑡=\n𝛼𝑡𝑋0 + 𝜎𝑡𝑍. Using Tweedie’s formula, we have\n∇log 𝑝𝑡(𝑋𝑡) = 𝛼𝑡𝔼[𝑋0|𝑋𝑡] −𝑋𝑡\n𝜎2\n𝑡\n.\nAdditionally, the random variable 𝑋𝑡= (𝛼𝑡/𝛼𝑡n)𝑋𝑡n + 𝜎𝑡|𝑡n𝑍. Using Tweedie’s formula, we can write\nthe score function\n∇log 𝑝𝑡(𝑋𝑡) = (𝛼𝑡/𝛼𝑡n)𝔼[𝑋𝑡n|𝑋𝑡] −𝑋𝑡\n𝜎2\n𝑡|𝑡n\n.\n25\n\n\nUsing the above two equations, we have\n(𝛼𝑡/𝛼𝑡n)𝔼[𝑋𝑡n|𝑋𝑡] −𝑋𝑡\n𝜎2\n𝑡|𝑡n\n= 𝛼𝑡𝔼[𝑋0|𝑋𝑡] −𝑋𝑡\n𝜎2\n𝑡\n𝔼[𝑋𝑡n|𝑋𝑡] =\n𝛼𝑡n𝜎2\n𝑡|𝑡n\n𝛼𝑡𝜎2\n𝑡\n(𝛼𝑡𝔼[𝑋0|𝑋𝑡] −𝑋𝑡) + 𝛼𝑡n𝑋𝑡\n𝛼𝑡\n=\n𝛼𝑡n𝜎2\n𝑡|𝑡n\n𝜎2\n𝑡\n𝔼[𝑋0|𝑋𝑡] +\n𝛼𝑡𝜎2\n𝑡n\n𝜎2\n𝑡𝛼𝑡n\n𝑋𝑡\n.\n□\nLemma B.2. Predicting 𝛼𝑡𝑍−𝜎𝑡\n(𝑋𝑡n−\n𝛼𝑡𝜎2\n𝑡n\n𝜎2\n𝑡𝛼𝑡n\n𝑋𝑡)\n𝛼𝑡n 𝜎2\n𝑡|𝑡n\n𝜎2\n𝑡\ngives us that the optimal 𝑣-prediction.\nC\nProofs\nC.1\nTechnical Details about Information Leakage\nC.1.1\nProof of Lemma 4.1\nProof. The distribution of the training data conditioned on the dataset 𝑆𝑡n is 𝑞0(𝑥) = 1\n𝑛\nÍ\n𝑥𝑡n∈𝑆𝑡n 𝛿(𝑥−\n𝑥𝑡n). To obtain iterates at time 𝑡, we add additional noise to points 𝑥𝑡n ∈𝑆𝑡n. Particularly, the\nfollowing relation holds for any 𝑡∈(𝑡n, 𝑇]:\n𝑋Amb\n𝑡\n=\ns\n1 −𝜎2\n𝑡\n1 −𝜎2\n𝑡n\n𝑋𝑡n +\nv\nt\n𝜎2\n𝑡−𝜎2\n𝑡n\n1 −𝜎2\n𝑡n\n𝜀, 𝜀∼𝒩(0, 𝐼) .\nThis induces a distribution for each time 𝑡:\n𝑞𝑡(𝑥|𝑆𝑡n) = 1\n𝑛\nÕ\n𝑥𝑡n∈𝑆𝑡n\n𝒩\n \n𝑥;\ns\n1 −𝜎2\n𝑡\n1 −𝜎2\n𝑡n\n𝑥𝑡n,\n𝜎2\n𝑡−𝜎2\n𝑡n\n1 −𝜎2\n𝑡n\n· 𝐼\n!\n.\nThe score of the Gaussian mixture 𝑞𝑡is given by\n𝑠Amb\n𝑡\n(𝑥|𝑆𝑡n) =\n1\n𝜎2\n𝑡−𝜎2\n𝑡n\n1−𝜎2\n𝑡n\nÕ\n𝑥𝑡n∈𝑆𝑡n\n s\n1 −𝜎2\n𝑡\n1 −𝜎2\n𝑡n\n𝑥𝑡n −𝑥\n!\n𝒩(𝑥;\nr\n1−𝜎2\n𝑡\n1−𝜎2\n𝑡n\n𝑥𝑡n,\n𝜎2\n𝑡−𝜎2\n𝑡n\n1−𝜎2\n𝑡n\n· 𝐼)\nÍ\n𝑦∈𝑆𝑡n 𝒩(𝑥;\nr\n1−𝜎2\n𝑡\n1−𝜎2\n𝑡n\n𝑦,\n𝜎2\n𝑡−𝜎2\n𝑡n\n1−𝜎2\n𝑡n\n· 𝐼)\n.\nSince the reverse flow of Eq.(5) provably reverses the forward diffusion [SSDK+20], the distribution\n𝑞←\n0 equals the empirical data distribution 𝑞0, which is a sum of delta functions on the noisy training\nset 𝑆𝑡n.\n□\n26\n\n\nC.1.2\nProof of Lemma 4.2\nProof. For two random variables 𝑋, 𝑌, recall that 𝐼(𝑋;𝑌) = 𝐻(𝑋) + 𝐻(𝑌) −𝐻(𝑋, 𝑌), where 𝐻(𝑋)\nis the entropy of 𝑋and 𝐻(𝑋, 𝑌) is the joint entropy of 𝑋and 𝑌. Without loss of generality, let\n𝜇= 0. Let 𝑥0 ∼𝒩(0, Σ). For the Ambient Diffusion at time 𝑡n, conditional on the noisy point being\n𝑥𝑡n, the optimal distribution learned is 𝛿(𝑥−𝑥𝑡n), where 𝑥𝑡n =\nq\n1 −𝜎2\n𝑡n𝑥0 + 𝜎𝑡n𝑍. Note that 𝑛i.i.d.\ndraws from this distribution (denoted by 𝐴) are identical and hence\n𝐼(𝐴; 𝑥0) = 𝐼(𝑥𝑡n; 𝑥0) .\nNow observe that\n𝑥0 ∼𝒩(0, Σ)\nand\n𝑥𝑡n ∼𝒩(0, (1 −𝜎2\n𝑡n)Σ + 𝜎2\n𝑡n𝐼) .\nMoreover, for the random column vector 𝜁= [𝑥0, 𝑥𝑡n]⊤, we have that\n𝔼[𝜁𝜁⊤] =\n\nΣ\nq\n1 −𝜎2\n𝑡nΣ\nq\n1 −𝜎2\n𝑡nΣ\n(1 −𝜎2\n𝑡n)Σ + 𝜎2\n𝑡n𝐼\n\n.\nNow it remains to control the mutual information of Gaussians. Given that Σ−1 exists, we note\nthat det(𝔼[𝜁𝜁⊤]) = det(Σ) · det(𝜎2\n𝑡n𝐼). We can hence write\n𝐼(𝑥𝑡n; 𝑥0) = 1\n2 log\ndet(𝔼[𝑥0𝑥⊤\n0 ])det(𝔼[𝑥𝑡n𝑥⊤\n𝑡n])\ndet(𝔼[𝜁𝜁⊤])\n= 1\n2 log\ndet((1 −𝜎2\n𝑡n)Σ + 𝜎2\n𝑡n𝐼)\ndet(𝜎2\n𝑡n𝐼)\n.\n(11)\nThis simplifies to\n1\n2 log det\n \n𝐼+\n1 −𝜎2\n𝑡n\n𝜎2\n𝑡n\nΣ\n!\n,\nwhere\n1−𝜎2\n𝑡n\n𝜎2\n𝑡n\ncorresponds to the signal-to-noise ratio. (An equivalent way to see the above, is by\ntaking the conditional distribution 𝑥𝑡n|𝑥0, which has covariance 𝜎2\n𝑡n𝐼, and hence directly get (11).)\nOn the other side, for DDPM, the learned distribution is 𝒩(\nq\n1 −𝜎2\n𝑡n𝑥0, 𝜎2\n𝑡n𝐼). Let 𝑋1 be a single\ndraw from that distribution. Hence, 𝑚i.i.d. draws 𝑆from that measure correspond to mutual\ninformation\n𝐼(𝑆; 𝑥0) = 𝑚· 𝐼(𝑋1; 𝑥0) = 𝑚· 𝐼(𝑥𝑡n; 𝑥0) .\nThis concludes the proof.\n□\nC.1.3\nAdditional Bounds on Mutual Information for Ambient Diffusion\nThe following lemma gives a bound on the mutual information of a generated set of size 𝑚from\nAmbient Diffusion at time 𝑡n given a training set of size 𝑁. On the other side, the mutual information\nof the DDPM solution at time t-nature should be 𝑚times larger.\n27\n\n\nLemma C.1. Consider a dataset 𝑆of size 𝑁drawn i.i.d. from 𝒩(𝜇, 𝐼). Consider the optimal\nambient solution at time 𝑡n with input 𝑆. Consider a set 𝐴of size 𝑚generated i.i.d. by that\ndistribution. Then 𝐼(𝐷; 𝑆) ≤𝑚𝑑/2 · log\n\u0010\n1/𝜎2\n𝑡n\n\u0011\n.\nProof. Let 𝑋𝑖be 𝑁i.i.d. draws from 𝒩(𝜇, 𝐼). For each 𝑖, let 𝑌𝑖= (1 −𝜎2\n𝑡n)𝑋𝑖+ 𝜎𝑡n𝑍𝑖for some\nindependent normal 𝑍𝑖∼𝒩(0, 𝐼). We know that the optimal ambient solution is the empirical\ndistribution\n𝑃(𝑦) = 1\n𝑁\nÕ\n𝑖\n𝛿(𝑦−𝑌𝑖) ,\nconditioned on the realization of the noisy dataset 𝑌= {𝑌1, ..., 𝑌𝑁}.\nBy the data processing inequality, we know that 𝐼(𝐴; 𝑆) ≤𝐼(𝑌; 𝑆). Since 𝑌𝑖are generated all in\nthe same way and independently, we can write\n𝐼(𝑌; 𝑆) =\nÕ\n𝑖\n𝐼(𝑌𝑖; 𝑋𝑖) = 𝑚𝐼(𝑌1; 𝑋1) .\nWe have that 𝐼(𝑌1; 𝑋1) = 𝐻(𝑌1)−𝐻(𝑌1|𝑋1). Recall that 𝑋1 ∼𝒩(𝜇, 𝐼) and 𝑌1|𝑋1 ∼𝒩((1−𝜎2\n𝑡n)𝑋1, 𝜎2\n𝑡n𝐼).\nMoreover, note that 𝑌1 ∼𝒩((1 −𝜎2\n𝑡n)𝜇, 𝐼). These imply that\n𝐻(𝑌1) = 𝑑\n2 log(2𝜋𝑒)\n(since it has identity covariance) and\n𝐻(𝑌1|𝑋1) = 𝑑\n2 log\n\u0010\n2𝜋𝑒𝜎2\n𝑡n\n\u0011\n.\nThis means that\n𝐼(𝑌1; 𝑋1) = 𝑑\n2 log\n\u0010\n1/𝜎2\n𝑡n\n\u0011\n.\nThis concludes the proof.\n□\nC.2\nTechnical Details about the Subpopulations Model\nC.2.1\nProof Theorem A.1\nProof. For each subpopulation with exactly ℓrepresentatives, we put in the set 𝑋𝑍#ℓthose representa-\ntives. Observe that the collection of sets {𝑋𝑍#ℓ} partitions 𝑍for ℓ∈{1, ..., 𝑛}. Set 𝑋𝑍= ∪ℓ∈[𝑛]𝑋𝑍#ℓ.\nThe unseen points correspond to the set 𝑋𝑍#0.\nWith this notation in hand, we define\nerrn𝑍(𝐴, ℓ) = 𝔼𝑠𝜃∼𝐴(𝑍)\nÕ\n𝑥∈𝑋𝑍#ℓ\n𝑀𝑖𝑥(𝑥)𝐿(𝑠𝜃, 𝑥) .\nwhere 𝑖𝑥is the index of the unique component whose support contains 𝑥. We have that\nerr(𝜋, 𝐴|𝑍) = 𝔼𝐷∼𝐷𝑋\n𝜋(·|𝑍)𝔼𝑠𝜃∼𝐴(𝑍)\nÕ\n𝑥∈𝑋\n𝑀𝐷(𝑥) · 𝐿(𝑠𝜃, 𝑥) .\n28\n\n\nWe now decompose 𝑋= 𝑋𝑍∪𝑋𝑍#0 and write\nerr(𝜋, 𝐴|𝑍) =\nÕ\n𝑥∈𝑋𝑍\n𝔼𝐷,𝑠𝜃[𝑀𝐷(𝑥) · 𝐿(𝑠𝜃, 𝑥)] +\nÕ\n𝑥∈𝑋𝑍#0\n𝔼𝐷,𝑠𝜃[𝑀𝐷(𝑥) · 𝐿(𝑠𝜃, 𝑥)] .\nLet us first deal with the second term. For any 𝑥∈𝑋𝑍#0, it holds\n𝔼𝐷,𝑠𝜃[𝑀𝐷(𝑥) · 𝐿(𝑠𝜃, 𝑥)] = 𝔼𝐷∼𝐷𝜋(·|𝑍)𝑀𝐷(𝑥) · 𝔼𝑠𝜃∼𝐴(𝑍)𝐿(𝑠𝜃, 𝑥),\nbecause the way we choose 𝐷is independent of the random variable 𝐿(𝑠𝜃, 𝑥) which only depends on\nthe way the algorithm picks the score function given the dataset.\nSet 𝑝(𝑥, 𝑍) = 𝔼𝐷∼𝐷𝜋(·|𝑍)𝑀𝐷(𝑥). Hence, from the elements that do not appear in 𝑍, we get a\ncontribution\nÕ\n𝑥∈𝑋𝑍#0\n𝑝(𝑥, 𝑍) · 𝔼𝑠𝜃∼𝐴(𝑍)𝐿(𝑠𝜃, 𝑥) .\n(12)\nLet us now deal with the elements appearing in 𝑍. Fix ℓ∈[𝑛]. For any 𝑥∈𝑋𝑍#ℓ, we have that\n𝔼𝐷,𝑠𝜃[𝑀𝐷(𝑥) · 𝐿(𝑠𝜃, 𝑥)] = 𝔼𝐷∼𝐷𝜋(·|𝑍)[𝑀𝐷(𝑥)] · 𝔼𝑠𝜃∼𝐴(𝑍)𝐿(𝑠𝜃, 𝑥) ,\nsince the random variables 𝐿(𝑠𝜃, 𝑥) and 𝑀𝐷(𝑥) are independent given 𝑍. By Lemma 2.1 in [Fel20]\nand since the supports of the components 𝑀1, ..., 𝑀𝑁are disjoint, we know that 𝔼𝐷∼𝐷𝜋(·|𝑍)[𝑀𝐷(𝑥)] =\n𝔼[𝐷(𝑖𝑥)𝑀𝑖𝑥(𝑥)] = 𝔼[𝐷(𝑖𝑥)]𝑀𝑖𝑥(𝑥) = 𝜏ℓ𝑀𝑖𝑥(𝑥), where 𝑖𝑥is the index of the component whose support\ncontains 𝑥. Hence, we have that\nÕ\n𝑥∈𝑋𝑍\n𝔼[𝑀𝐷(𝑥)·𝐿(𝑠𝜃, 𝑥)] =\nÕ\nℓ∈[𝑛]\nÕ\n𝑥∈𝑋𝑍#ℓ\n𝜏ℓ·𝑀𝑖𝑥(𝑥)·𝔼𝑠𝜃∼𝐴(𝑍)𝐿(𝑠𝜃, 𝑥) =\nÕ\nℓ\n𝜏ℓ·\nÕ\n𝑥∈𝑋𝑍#ℓ\n𝑀𝑖𝑥(𝑥)𝔼𝑠𝜃∼𝐴(𝑍)𝐿(𝑠𝜃, 𝑥)\nIn total, we have shown that\nerr(𝜋, 𝐴|𝑍) =\nÕ\nℓ∈[𝑛]\n𝜏ℓ· errn𝑍(𝐴, ℓ) + errunseen(𝜋, 𝐴|𝑍) .\nThis completes the proof.\n□\nC.2.2\nProof of Lemma A.4\nProof of Lemma A.4. We have that the 𝑖-th noisy example can be written as 𝑥𝑖\n𝑡=\nq\n1 −𝜎2\n𝑡𝑥𝑖\n0 + 𝜎𝑡𝑧𝑖\n𝑡.\nLet us set 𝜎𝑡= 𝑜(1/∥𝑥0∥)) = poly(1/𝑑). Using Taylor’s approximation for\n√\n1 −𝑥around 𝑥= 0\n(\n√\n1 −𝑥= 1 −𝑥/2 −𝑜(𝑥)) , we can write\n∥𝑥𝑖\n𝑡−(1 −poly(1/𝑑))𝑥0 −poly(1/𝑑)𝑧𝑖\n𝑡∥≤𝜀,\nfor some 𝜀= poly(1/𝑑) sufficiently small. This means that\n∥𝑥𝑖\n𝑡−𝑥0∥≤𝜀+ poly(1/𝑑)∥𝑥0∥+ poly(1/𝑑)∥𝑧𝑖\n𝑡∥\nBy Gaussian concentration, we have that\nℙ𝑧𝑖\n𝑡[∥𝑧𝑖\n𝑡∥>\n√\n𝑑] ≤exp(−𝑑/2) .\nLet us define the bad event 𝐸𝑚which corresponds to ”subpopulation 𝑗does not have a poly(1/𝑑)-\nsmoothed single representative in the set {𝑥𝑖\n𝑡}𝑖∈[𝑚] for 𝜎𝑡= poly(1/𝑑)”. A union bound over the 𝑚\nnoisy examples gives that\nℙ𝑧1\n𝑡,...,𝑧𝑚\n𝑡[𝐸𝑚] ≤𝑚· exp(−𝑑/2) .\n□\n29\n\n\nC.2.3\nProofs of Lemma A.5 and Lemma A.6\nProofs of Lemma A.5 and Lemma A.6. The proof relies on the fact that when the total variation\ndistance between two identity-covariance Gaussians is smaller than an absolute constant, then the\ntotal variation is up to constants characterized by the distance between the means [AAL23]. When\nthe original total variation is at most 1/600, [AAL23] shows that\nTV(𝒩(𝜇, 𝐼), 𝒩(𝜇′, 𝐼)) = Θ(∥𝜇−𝜇′∥) .\nThe lemmas follow by noting that the densities of 𝒩(𝜇𝑖, 𝐼), 𝒩(𝜇′\n𝑖, 𝐼) at noise scale 𝜎𝑡(denoted as\n𝒩𝑡, 𝒩′\n𝑡) satisfy\n𝒩𝑡= 𝒩(\nq\n1 −𝜎2\n𝑡𝜇, 𝐼),\n𝒩′\n𝑡= 𝒩(\nq\n1 −𝜎2\n𝑡𝜇′, 𝐼) .\nSince\nq\n1 −𝜎2\n𝑡≤1, the means are contracting and so TV(𝒩𝑡, 𝒩′\n𝑡) ≤1/600. Also:\nTV(𝒩𝑡, 𝒩′\n𝑡) ≤∥𝜇𝑡−𝜇′\n𝑡∥/\n√\n2 =\nq\n1 −𝜎2\n𝑡· ∥𝜇−𝜇′∥/\n√\n2 .\nIf we want to make this quantity at most 𝜀, it suffices to take 𝜎𝑡≥\np\n1 −2(𝜀/∥𝜇−𝜇′∥)2.\nFor the other side, by [AAL23], TV(𝒩𝑡, 𝒩′\n𝑡) ≥∥𝜇𝑡−𝜇′\n𝑡∥/200 =\nq\n1 −𝜎2\n𝑡∥𝜇−𝜇′∥/200. (we assume\nthat the original variation is smaller than 1/600 and we contract it by adding noise). If this should\nbe at least 2𝜀, then it should be that 𝜎𝑡≤\np\n1 −2002(2𝜀/∥𝜇−𝜇′∥)2. This concludes the proof.\n□\nD\nExperimental Details\nWe open-source our code: https://github.com/kulinshah98/memorization_noisy_data\nFor all of our experiments regarding unconditional generation, we use the Adam optimizer with\na learning rate of 0.0001, betas (0.9, 0.999), an epsilon value of 1e-8, and a weight decay of 0.01.\nThe model for FFHQ and CIFAR-10 is trained for 30,000 iterations with a batch size of 256 and\nthe model for Imagenet is trained for 512 batch size for 80,000 iterations. For experiments on the\nImagenet dataset, we train a class-conditional model.\nFor FFHQ and CIFAR-10 experiments, we randomly sample 300, 1000 and 3000 samples from the\ncomplete dataset to create the dataset with limited size. We use Tiny Imagenet dataset which\nconsists of 200 classes [LY15]. We sample 5 images randomly from each class to create a dataset\nconsisting of 1000. Similarly, we sample 15 images from each class to create a dataset consisting\nof 3000 images. For the unconditional and conditional generation experiments, we used with the\nimplementation of [KAAL22] and default parameters of the implementation.\nFor text-conditioned experiments, we use the implementation of [SSG+23] and implement additional\nbaseline [WLCL24] and our method in the implementation. Similar to previous works, we use\nLAION-10k dataset to train the stable diffusion v2 model for 100000 number of iterations using\nbatch size 16. We use the final checkpoint after the complete training to evaluate the memorization,\nclipscore and fidelity. For the text-conditioned experiments, we tried adding nature noise at noise\nscale {25, 50, 100} and chose the model with best image quality.\n30\n\n\nFigure 4: Images generated using a model trained with our method on 300 samples\nE\nImages Generated using our Method\nIn this section, we present various images generates using our method. The images can be found in\nFigures 4 to 6.\n31\n\n\nFigure 5: Images generated using a model trained with our method on 1000 samples\n32\n\n\nFigure 6: Images generated using a model trained with our method on 3000 samples\n33\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21278v1.pdf",
    "total_pages": 33,
    "title": "Does Generation Require Memorization? Creative Diffusion Models using Ambient Diffusion",
    "authors": [
      "Kulin Shah",
      "Alkis Kalavasis",
      "Adam R. Klivans",
      "Giannis Daras"
    ],
    "abstract": "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}