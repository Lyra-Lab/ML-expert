{
  "id": "arxiv_2502.21278v1",
  "text": "Does Generation Require Memorization?\nCreative Diffusion Models using Ambient Diffusion\nKulin Shah ‚àó\nUT Austin\nAlkis Kalavasis ‚Ä†\nYale University\nAdam R. Klivans ‚Ä°\nUT Austin\nGiannis Daras ¬ß\nMIT\nMarch 3, 2025\nAbstract\nThere is strong empirical evidence that the state-of-the-art diffusion modeling paradigm leads\nto models that memorize the training set, especially when the training set is small. Prior methods\nto mitigate the memorization problem often lead to a decrease in image quality. Is it possible to\nobtain strong and creative generative models, i.e., models that achieve high generation quality\nand low memorization? Despite the current pessimistic landscape of results, we make significant\nprogress in pushing the trade-off between fidelity and memorization. We first provide theoretical\nevidence that memorization in diffusion models is only necessary for denoising problems at low\nnoise scales (usually used in generating high-frequency details). Using this theoretical insight,\nwe propose a simple, principled method to train the diffusion models using noisy data at large\nnoise scales. We show that our method significantly reduces memorization without decreasing\nthe image quality, for both text-conditional and unconditional models and for a variety of data\navailability settings.\n1\nIntroduction\nDiffusion models [SE19, HJA20, SSDK+20] have become a widely used framework for unconditional\nand text-conditional image generation. However, recent works [SSG+22, CHN+23, DSD+23, SSG+23,\nDDD24, RKW+24] have shown that the trained models memorize the training data and often replicate\nthem at generation time. This issue has raised important privacy and ethical concerns [SSG+22,\nTKC22, ANS23], especially in applications where the training set contains sensitive or copyrighted\ninformation [CBLC22]. [CHN+23] conjectures that the improved performance over alternative\nframeworks may come from the increased memorization [LYM+24].\nThis raises the following\nquestion:\nCan we improve the memorization of diffusion models\nwithout decreasing the image generation quality?\n‚àóEmail: kulinshah@utexas.edu, supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n‚Ä†Email: alkis.kalavasis@yale.edu, supported by the Institute for Foundations of Data Science at Yale (FDS).\n‚Ä°Email: klivans@cs.utexas.edu, supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n¬ßEmail: gdaras@mit.edu, supported by the NSF AI Institute for Foundations of Machine Learning (IFML) and\nthe Computer Science & Artificial Intelligence Laboratory at MIT (CSAIL).\n1\narXiv:2502.21278v1  [cs.LG]  28 Feb 2025\n\n\n0\n10\n20\n30\n40\n50\nPercentage of generated images that are training data duplicates\n14\n16\n18\n20\n22\n24\n26\nFr√©chet Inception Distance (FID)\ntn = 0.0\ntn = 0.1\ntn = 0.25\ntn = 0.3\ntn = 0.4\ntn = 0.5\ntn = 2\ntn = 4\ntn = 10\nTrade-off between FID and Memorization\nFFHQ with 300 Training Images\nData Points\nPareto Frontier\nFigure 1: (FID, Memorization) pairs for different values of ùúéùë°n used in our proposed Algorithm 1\n(presented in Section 3) for training diffusion models from limited data. The standard DDPM\nobjective corresponds to ùúéùë°n = 0 and it is not in the Pareto frontier. Setting ùúéùë°n too low or too\nhigh reverts back to the DDPM behavior. Values for ùúéùë°n ‚àà[0.4, 4] strike different balances between\nmemorization and quality of generated images. The models in this Figure are trained on only 300\nimages from FFHQ.\nPrior work has shown that the optimal solution to the diffusion objective is a model that merely\nreplicates the training points [DB22, SBS23, BBDBM24, KG24, BBDD24]. The experimentally\nobserved creativity in diffusion modeling happens when the models fail to perfectly minimize their\ntraining loss [KG24]. As the training dataset becomes smaller, overfitting becomes easier, memo-\nrization increases and output diversity decreases [SSG+23, DSD+23, GDP+23]. Text-conditioning is\nalso known to exacerbate memorization [SSG+22, CHN+23, SSG+23] and text-conditional diffusion\nmodels are known to memorize individual training points even when trained on billions of image-text\npairs [CHN+23, DDDD23].\nRelated work.\nSeveral methods have been proposed to reduce the memorization in diffusion\nmodels [SSG+23, DSD+23, GDP+23, WLCL24, DCD24, KSH+24, CLX24, RLZ+24, WLHH24,\nLGWM24, RKW+24, WCS+24, ZLL+24, JKS+24, HSK+25]. A line of work proposes sampling adap-\ntations that guide the generation process away from training points [KSH+24, WLCL24, CLX24].\n[KYKM23, WBZ+25] propose decreasing the receptive field of the generative model to avoid mem-\norization. Another line of work corrupts the images [DSD+23, DDD24] or the text-embedding in\ntext-conditioned image models [SSG+23]. While effective in reducing memorization, these methods\noften decrease the image generation quality. Feldman [Fel20] theoretically showed strong trade-offs\nbetween memorization and generalization by showing that memorization is necessary for (optimal)\nclassification. This raises the natural question of whether this trade-off also applies to generative\nmodeling.\nThe need for memorization in [Fel20] is associated with the frequencies of different subpopulations\n(e.g., cats, dogs, etc.) that appear in the dataset. The key observation is that the distribution of the\nfrequencies is usually heavy-tailed [ZAR14], i.e., roughly speaking in a dataset of size ùëõ, there will be\n2\n\n\nTraining Image\n[WLCL24]\n[WLCL24] + Ours\n‚ÄùRISE 24‚Äù TriFecta Dishwasher\nCANYON CARGO - Outdoor shorts - dark moss\nWestern Chief Down Hill Trot (Black) Women‚Äôs Rain Boots\nFigure 2: Qualitative results for reducing the memorization of Stable Diffusion 2. Combining our\nmethod with [WLCL24] helps generate novel samples for the above prompts. See Section 3 for our\nmethod and Section 5.2 for more details on the experiment.\nmany classes with frequency around 1/ùëõ. This means that the training algorithm will only observe\na single representative from those subpopulations and cannot distinguish between the following two\ncases:\nCase 1. If the unique example comes from an extremely rare subpopulation (with frequency ‚â™1/ùëõ),\nthen memorizing it has no significant benefits, and,\nCase 2. If the unique example comes from a subpopulation with 1/ùëõfrequency, then memorizing it\nwill probably improve the accuracy on the entire subpopulation and decrease the generalization\nerror by Œ©(1/ùëõ). Hence, the optimal classifier should memorize these unique examples to avoid\npaying Case 2 in the error.\nThe key assumption above seems to break when noise is added to the images. That is because\ndifferent subpopulations start to merge and the heavy-tails of the weights‚Äô distribution disappear.\nInterestingly, diffusion models learn the (score of the) distribution at different levels of noise. This\n3\n\n\nInput images\nùúé= 17\nùúé= 8\nOutputs of diffusion model trained on 52k images\nOutputs of diffusion model trained on 300 images\nOutputs of Ambient Diffusion trained on 300 images\nFigure 3: Comparison of denoised images under different noise levels and training conditions.\nStandard diffusion modeling leads to overconfident predictions (row 3) even for very highly noised\ninputs when it is trained on small datasets. Our algorithm (row 4), has a similar behavior (blurry\noutputs) to a model trained with significantly more data (row 1), indicating less memorization.\nindicates that, in principle, it is feasible to avoid memorization in the high-noise regime (without\nsacrificing too much quality). Despite that, regular diffusion model training, e.g., the DDPM [HJA20]\nobjective, results in score functions that have attractors around the training points, even for highly\nnoisy inputs, as shown in Figure 3.\nThe discussion above suggests that it should be possible to train high-quality diffusion models\nthat do not memorize in the high-noise part of the diffusion. It has been empirically established that\nthis part controls the structural information of the outputs and hence the diversity of the generated\ndistribution [Die24, LC24]. To avoid memorization in the high-noise regime, we propose a simple,\nprincipled framework that trains the diffusion model only with noisy data at large noise scales. We\ngive theoretical evidence that the noisy targets used for learning leak much less information about\nthe training set, and further they are harder to memorize since they are less compressible.\nOur contributions:\n‚Ä¢ We propose a simple framework to train diffusion models that achieve reduced memorization\nand high-quality sample generation even when trained on limited data.\n‚Ä¢ We experimentally validate our approach on various datasets and data settings, showcasing\nsignificantly reduced memorization and improved generation quality compared to natural\n4\n\n\nbaselines, in both the unconditional and text-conditional settings 1.\n‚Ä¢ On the theory side, we adapt the theoretical framework of [Fel20] for studying memorization\nto diffusion models. Based on that, we argue about the necessity of memorizing the training\nset in different noise scales indicating that memorization is only essential at the low-noise\nregime.\n‚Ä¢ We quantify the information leakage of our proposed algorithm in the high-noise regime\nshowing significant benefits over the standard diffusion modeling objective.\n2\nBackground and Related Work\n2.1\nDiffusion Modeling\nThe first step in diffusion modeling is to design a corruption process. For the ease of presentation,\nwe focus on the widely used Variance Preserving (VP) corruption [HJA20, SSDK+20]. We define a\nsequence of increasing corruption levels indexed by ùë°‚àà[0, 1], with:\nùëãùë°=\nq\n1 ‚àíùúé2\nùë°ùëã0 + ùúéùë°ùëç,\nùëç‚àºùí©(0, ùêºùëë),\n(1)\nwhere the map ùúéùë°:= ùúé(ùë°) is the noise schedule and ùëã0 is drawn from the clean distribution ùëù0.\nWe remark that our framework extends to other noise schedules, diffusion models [SE19, BBC+22,\nKAAL22, DDT+23] and flow matching [LCBH+22, LGL22, ABVE23].\nOur ultimate goal is to sample from the unknown distribution ùëù0.\nThe key idea behind\ndiffusion modeling is to learn the score functions, defined as ‚àálog ùëùùë°(¬∑), for different noise levels\nùë°, where ùëãùë°‚àºùëùùë°. The latter is related to the optimal denoiser ùîº[ùë•0|ùëãùë°= ùë•ùë°] through Tweedie‚Äôs\nformula [Efr11]:\n‚àálog ùëùùë°(ùë•ùë°) =\nq\n1 ‚àíùúé2\nùë°ùîº[ùë•0|ùëãùë°= ùë•ùë°] ‚àíùë•ùë°\nùúé2\nùë°\n.\n(2)\nThe conditional expectation is typically learned from the available data with supervised learning\nover some parametric class of models ‚Ñã= {‚ÑéùúÉ: ùúÉ‚ààŒò}, using the training objective:\nùêΩ(ùúÉ) = ùîºùë•0ùîº(ùë•ùë°,ùë°)|ùë•0\n\u0002\n‚à•‚ÑéùúÉ(ùë•ùë°, ùë°) ‚àíùë•0‚à•2\u0003\n.\n(3)\nPost training, the score function ‚àálog ùëùùë°(ùë•ùë°) is approximated by plugging the optimal solution of\n(3) to (2). Alternatively, one can train directly for the score function using the noise prediction\nloss [Vin11, HJA20]:\nùêΩ(ùúÉ) = ùîºùë•0,ùë•ùë°,ùë°\n\"\r\r\r\r\rùë†ùúÉ(ùë•ùë°, ùë°) ‚àí\nq\n1 ‚àíùúé2\nùë°ùë•0 ‚àíùë•ùë°\nùúé2\nùë°\n\r\r\r\r\r\n2#\n.\n(4)\nGiven access to the score function for different times ùë°, one can sample from the distribution of ùëù0\nby running the process [SSDK+20]:\ndùë•=\n\u0012\n‚àíùë•‚àí(dùúéùë°/dùë°)ùúéùë°\n1 + ùúé2\nùë°\n‚àálog ùëùùë°(ùë•ùë°)\n\u0013\ndùë°.\n(5)\n1We open-source our code: https://github.com/kulinshah98/memorization_noisy_data\n5\n\n\n2.2\nMemorization in Diffusion Models\nThe first expectation of (3) is taken over the distribution of ùë•0. The underlying distribution of ùë•0\nis continuous, but in practice we only optimize this objective over a finite distribution of training\npoints. Prior work has shown that when the expectation is taken over an empirical distribution\nbùëù0, the optimal score can be written in closed form [DB22, SBS23, BBDBM24, KG24, BBDD24].\nSpecifically, the optimal score for the empirical distribution, which corresponds to a finite amount\nof examples ùëÜ, can be written as:\nbùë†‚àó(ùë•ùë°, ùë°) = 1\nùúé2\nùë°\n1\n√ç\nùë•0‚ààùëÜùí©(ùë•ùë°;\nq\n1 ‚àíùúé2\nùë°ùë•0, ùúéùë°ùêº)\n¬∑\n√ï\nùë•0‚ààùëÜ\n(\nq\n1 ‚àíùúé2\nùë°ùë•0 ‚àíùë•ùë°) ùí©(ùë•ùë°;\nq\n1 ‚àíùúé2\nùë°ùë•0, ùúéùë°ùêº) .\nattraction to ùë•0\nweight of attraction\nIntuitively, each point ùë•0 in the finite sample ùëÜ(i.e., the empirical distribution bùëù0) is pulling\nthe noisy iterate ùë•ùë°towards itself, where the weight of the pull depends on the distance of each\ntraining point to the noisy point. The above solution will lead to a diffusion model that only\nreplicates the training points during sampling [SBS23, KG24]. Hence, any potential creativity that\nis observed experimentally in diffusion models comes from the failure to perfectly optimize the\ntraining objective.\n2.3\nAmbient Score Matching\nOne way to mitigate memorization is to never see the training data.\nRecent techniques for\ntraining with corrupted data allow learning of the score function without ever seeing a clean\nimage [KEME23, DDDD23, DDD24, BWCS24, WBL+24, RALL24]. Consider the case where we\nare given samples from a noisy distribution ùëùùë°n (where ùë°n stands for ùë°-nature) and we desire to learn\nthe score at time ùë°for ùë°> ùë°n. The Ambient Score Matching loss [DDD24], defined as:\nùêΩambient(ùúÉ) = ùîºùë•ùë°nùîº(ùë•ùë°,ùë°)|ùë•ùë°n\n\u0014\r\r\r\r\nùúé2\nùë°‚àíùúé2\nùë°n\nùúé2\nùë°\nq\n1 ‚àíùúé2\nùë°n\n‚ÑéùúÉ(ùë•ùë°, ùë°) +\nùúé2\nùë°n\nùúé2\nùë°\ns\n1 ‚àíùúé2\nùë°\n1 ‚àíùúé2\nùë°n\nùë•ùë°‚àíùë•ùë°n\n\r\r\r\r\n2\u0015\n,\n(6)\ncan learn the conditional expectation ùîº[ùë•0|ùë•ùë°] (similar to Equation (3)) without ever looking at\nclean data from ùëù0. The intuition behind this objective is that to denoise the noisy sample ùë•ùë°, we\nneed to find the direction of the noise and then rescale it appropriately. The former can be found\nby denoising to an intermediate level ùë°n and the rescaling ensures that we denoise all the way to the\nlevel of clean images. Once the conditional expectation ùîº[ùë•0|ùë•ùë°] is recovered, we get the score by\nusing Tweedie‚Äôs Formula.\nWe remark that this objective can only be used for ùë°> ùë°n. While there are ways to train for\nùë°‚â§ùë°n without any clean data (e.g., see [DDDD23, DDD24, BWCS24, WBL+24, RALL24]), this\nleads to performance deterioration unless a massive noisy dataset is available [DCD24]. For what\nfollows, we refer to Eq.(3) as the DDPM training objective and to Eq.(6) as the Ambient Diffusion\ntraining objective for noisy data.\n3\nMethod\nWe are now ready to present our framework for training diffusion models with limited data that will\nallow creativity without sacrificing quality. Our key observation is that the diversity of the generated\n6\n\n\nimages is controlled in the high-noise part of the diffusion trajectory [Die24, LC24]. Hence, if we\ncan avoid memorization in this regime, it is highly unlikely that we will replicate training examples\nat inference time, even if we memorize at the low-noise part. Our training algorithm can ‚Äúcopy‚Äù\ndetails from the training samples and still produce diverse outputs.\nAlgorithm 1 Algorithm for training diffusion models using limited data.\nRequire: untrained network ‚ÑéùúÉ, set of samples ùëÜ, noise level ùë°n, noise scheduling ùúé(ùë°), batch size\nùêµ, diffusion time ùëá\n1: ùëÜùë°n ‚Üê{\nq\n1 ‚àíùúé2\nùë°nùë•(ùëñ)\n0 + ùúéùë°nùúÄ(ùëñ)|ùë•(ùëñ)\n0 ‚ààùëÜ, ùúÄ(ùëñ) ‚àºùí©(0, ùêºùëë)}\n‚ä≤Noise the training set at level ùë°n.\n2: while not converged do\n3:\nForm a batch ‚Ñ¨of size ùêµuniformly sampled from ùëÜ‚à™ùëÜùë°n\n4:\nloss ‚Üê0\n‚ä≤Initialize loss.\n5:\nfor each sample ùë•‚àà‚Ñ¨do\n6:\nùúÄ‚àºùí©(0, ùêº)\n‚ä≤Sample noise.\n7:\nif ùë•‚ààùëÜùë°n then\n8:\nùë•ùë°n ‚Üêùë•\n‚ä≤We are dealing with a noisy sample.\n9:\nùë°‚àºùí∞(ùë°n, ùëá)\n‚ä≤Sample diffusion time for noisy sample.\n10:\nùë•ùë°‚Üê\nr\n1‚àíùúé2\nùë°\n1‚àíùúé2\nùë°n\nùë•ùë°n +\nr\nùúé2\nùë°‚àíùúé2\nùë°n\n1‚àíùúé2\nùë°n\nùúÄ\n‚ä≤Add additional noise.\n11:\nloss ‚Üêloss +\n\r\r\r\r\r\nùúé2\nùë°‚àíùúé2\nùë°n\nùúé2\nùë°\nq\n1‚àíùúé2\nùë°n\n‚ÑéùúÉ(ùë•ùë°, ùë°) +\nùúé2\nùë°n\nùúé2\nùë°\nr\n1‚àíùúé2\nùë°\n1‚àíùúé2\nùë°n\nùë•ùë°‚àíùë•ùë°n\n\r\r\r\r\r\n2\n‚ä≤Ambient Score Matching.\n12:\nelse\n13:\nùë•0 ‚Üêùë•\n‚ä≤We are dealing with a clean sample.\n14:\nùë°‚àºùí∞(0, ùë°n)\n‚ä≤Sample diffusion time for clean sample.\n15:\nùë•ùë°‚Üê\nq\n1 ‚àíùúé2\nùë°ùë•0 + ùúéùë°ùúÄ\n‚ä≤Add noise.\n16:\nloss ‚Üêloss + ‚à•‚ÑéùúÉ(ùë•ùë°, ùë°) ‚àíùë•0‚à•2\n‚ä≤Regular Denoising Score Matching.\n17:\nend if\n18:\nend for\n19:\nloss ‚Üêloss\nùêµ\n‚ä≤Compute average loss.\n20:\nùúÉ‚ÜêùúÉ‚àíùúÇ‚àáùúÉloss\n‚ä≤Update network parameters via backpropagation.\n21: end while\nOur training framework is presented in Algorithm 1. It works by splitting the diffusion training\ntime into two parts, ùë°‚â§ùë°n and ùë°> ùë°n, where ùë°n2 (ùë°-nature) is a free parameter to be controlled.\nFor the regime, ùë°‚â§ùë°n, we train with the regular diffusion training objective, and (assuming perfect\noptimization) we know the exact score, which is as given in Section 2.2. To train for ùë°> ùë°n, we first\ncreate the set ùëÜùë°n which has one noisy version of each image in the training set. Then, we train\nusing the set ùëÜùë°n and the Ambient Score Matching loss introduced in Section 2.3.\nIt is useful to build some intuition about why this algorithm avoids memorization and at the\nsame time produces high-quality outputs. Regarding memorization: 1) the learned score function\nfor times ùë°‚â•ùë°n does not point directly towards the training points since Ambient Diffusion aims\nto predict the noisy points (recall that the optimal DDPM solution points towards scalings of the\ntraining points) and 2) the noisy versions ùë•ùë°n are harder to memorize than ùë•0, since noise is not\n2We often use the symbol ùëõfor sample size; the notation ùë°n is unrelated to the size ùëõ.\n7\n\n\ncompressible. At the same time, if the dataset size were to grow to infinity, both our algorithm and\nthe standard diffusion objective would find the true solution: the score of the underlying continuous\ndistribution. In fact, Algorithm 1 learns the same score function for times ùë°‚â§ùë°n as DDPM. This\ncontributes to generating samples with high-quality details, copied from the training set.\n4\nTheoretical Results\n4.1\nInformation Leakage\nIn this section, we attempt to formalize the intuition of why our proposed algorithm reduces\nmemorization of the dataset. We start by showing the following Lemma that characterizes the\nsampling distribution of our algorithm for ùë°= ùë°n.\nLemma 4.1 (Ambient Diffusion solution at ùë°n). Let ùëÜùë°n be the noisy training set as in L1 of\nAlgorithm 1.\nFor a fixed ùëÜùë°n, let bùëùùë°n be the distribution at time ùë°= ùë°n that arises by using\nthe score of Algorithm 1 in the reverse process of Eq.(5) initialized at ùí©(0, ùêºùëë).\nIt holds that\nbùëùùë°n =\n1\n|ùëÜùë°n |\n√ç\nùë•ùë°n‚ààùëÜùë°n ùõø(ùë•‚àíùë•ùë°n).\nFor the proof, we refer to Section C.1.1. This Lemma extends the result of Kamb and Ganguli\n[KG24] from the standard diffusion objective of Eq.(3) to the training objective of Eq.(6). Given\nthis result, we can compare the information leakage of Ambient Diffusion at time ùë°n compared to\nthe optimal distribution bùëûùë°n learned by DDPM at that time.\nLemma 4.2 (Information Leakage). Consider point ùë•0 ‚àºùí©(ùúá, Œ£), a set ùê¥of size ùëögenerated i.i.d.\nby bùëùùë°n (optimal ambient solution at time ùë°n with input ùë•0) and a set ùê∑of size ùëögenerated i.i.d.\nby bùëûùë°n (optimal DDPM solution at time ùë°n with input ùë•0). Then the mutual information satisfies:\nùêº(ùê∑; ùë•0) = ùëö¬∑ ùêº(ùê¥; ùë•0) = ùëö\n2 log det(\n1‚àíùúé2\nùë°n\nùúé2\nùë°n\nŒ£ + ùêº).\nFor the proof, we refer to Section C.1.2. The above means that DDPM leaks much more information\nabout the training point compared to Ambient Diffusion, when asked to generate a collection of\nsamples from the model at time ùë°n. Another way to see it, is that given ùëösamples from DDPM at\ntime ùë°n, one can get an estimator for ùë•0 with error poly(1/ùëö), while with Ambient Diffusion, no\nconsistent estimation is possible. As expected, as ùúéùë°n ‚âà0, then no noise is added to create ùëÜùë°n and\nhence the mutual information blows up. On the other extreme, as ùúéùë°n ‚âà1, then the models reveal\nno information about the original point. If the dataset contains multiple points, similar results\nabout the mutual information can be obtained (see Section C.1.3).\nThe above indicates that Ambient Diffusion can only memorize the noisy images. Our justification\nfor the improved performance in practice is that memorizing noise is much harder since noise is\nnot compressible. Even if the noisy images are perfectly memorized, they do not contain enough\ninformation to perfectly recover the training set (as shown above) and hence creativity will emerge. A\npossible conjecture is that under reasonable smoothness assumptions the concatenation of Ambient\nDiffusion (i.e., of a non-memorized trajectory (up to ùë°n)) and of DDPM (i.e., of a memorized\none (from ùë°n to 0)) will not lead to memorized outputs. Under this conjecture, controlling the\nhigh noise case is all you need to decrease memorization, and this is what our algorithm achieves.\nShowing non-trivial upper/lower bounds between the distribution learned by our algorithm and the\ndistribution learned by DDPM is an interesting theoretical problem that remains to be addressed.\n8\n\n\n4.2\nConnections to Feldman [Fel20]\nIn the previous section, we discussed ways to reduce the memorization. In this section, we consider\nwhat is the price to pay for reduced memorization, i.e., we analyze the trade-off between memorization\nand fidelity.\nWhile there is a significant amount of empirical research on connections between memorization\nand generation for diffusion models, our rigorous theoretical understanding is still lacking. In terms\nof theory, there are many works studying memorization-generalization trade-offs for machine learning\nalgorithms [Fel20, FZ20, BBF+21, BBS22, CDK22, Liv24, ADH+24] with several connections to\ndifferential privacy and stability in learning [BE02, XR17, BMN+18, RZ19, Fel20, SZ20]. Our work\nstudies this trade-off in diffusion models, inspired by the work of [Fel20].\nSection Overview.\nWe study the memorization-generalization trade-offs in the diffusion models\nwhen the data distribution is modeled as a mixture [SCK23, CKS24, GKL24]. In Section 4.2.1, we\ndefine the distribution to be learned as a mixture of distributions of subpopulations (e.g., dogs, cats,\netc.) with unknown mixing weights. This distribution is learned given a finite set ùëçof size ùëõand\nwe are interested in the generalization error of the trained model (at some fixed noise scale ùúéùë°). In\nTheorem 4.3 we express this generalization error into two terms, one of which is the error of the\nalgorithm for populations that are seen only once during training. We consider that the trained\nmodel ‚Äúmemorizes‚Äù when the error of these rare examples is small. Due to the error decomposition,\ngeneralization is related to the memorization error and its multiplying constant ùúè1 that appears in\nTheorem 4.3. In Section 4.2.3 we analyze how this constant changes for different noise levels under\nthe assumption of [ZAR14, Fel20] that the mixing weights are heavy-tailed. We argue that when\nthe noise level is small, ùúè1 is large and due to the decomposition, the only way to achieve good\ngeneralization is to memorize. For high noise levels, ùúè1 becomes smaller and hence it is in principle\npossible to achieve generalization without excessive memorization.\n4.2.1\nSubpopulations Model of Feldman [Fel20]\nLet us consider a continuous data domain ùëã‚äÜ‚Ñùùëë(e.g., images). We model the data distribution as a\nmixture of ùëÅfixed distributions ùëÄ1, ..., ùëÄùëÅ, where each component corresponds to a subpopulation\n(e.g., dogs, cats, etc.). For simplicity, we follow Feldman [Fel20] and assume that each component\nùëÄùëñhas disjoint support ùëãùëñ(this can be relaxed, see Remark 1). Without loss of generality, let\nùëã= ‚à™ùëñùëãùëñ.\nWe will now describe the procedure of [Fel20] that assigns frequencies to each subpopulation of\nthe mixture.\n1. Consider a list of frequencies ùúã= (ùúã1, ùúã2, ..., ùúãùëÅ).\n2. For each component ùëñ‚àà[ùëÅ] of the mixture, select randomly and independently an element ùëùùëñ\nfrom ùúã.\n3. Finally, to obtain the mixing weights, we normalize the elements ùëù1, ..., ùëùùëÅ, i.e., the weight of\ncomponent ùëñis ùê∑ùëñ=\nùëùùëñ\n√ç\nùëó‚àà[ùëÅ] ùëùùëó.\nWe denote by ùíüùúãthe distribution over the mixing coefficients tuple (ùê∑1, ..., ùê∑ùëÅ). A sample\nùê∑‚àºùíüùúãis just a list of the normalized frequencies of the ùëÅsubpopulations. If ùê∑‚àºùíüùúã, then we\ncan define the true mixture as\n9\n\n\nùëÄùê∑(ùë•) =\n√ï\nùëñ‚àà[ùëÅ]\nùê∑ùëñùëÄùëñ(ùë•) .\nmixing weight of class ùëñ\ndistribution of class ùëñ\nThe above random distribution corresponds to the subpopulations model introduced by Feldman\n[Fel20].\n4.2.2\nAdaptation to Diffusion\nAs explained in the Background Section 2, one way to train a generative model in order to generate\nfrom the target ùëÄùê∑is to estimate the score function ‚àáùë•log ùëÄùê∑ùë°for all levels of noise indexed by\nùë°. For the analysis of this Section, we consider the case of a single fixed ùë°. We define learning\nalgorithms ùê¥as (potentially randomized) mappings from datasets ùëçto score functions ùë†ùúÉ‚àºùê¥(ùëç).\nAs in Feldman [Fel20], we are interested about the expected error of ùê¥conditioned on dataset\nbeing equal to ùëç‚ààùëãùëõas\nerr(ùúã, ùê¥|ùëç) = ùîºùê∑‚àºùíüùúã(¬∑|ùëç)ùîºùë†ùúÉ‚àºùê¥(ùëç)errùëÄùê∑(ùë†ùúÉ) ,\nwhere ùê∑‚àºùíüùúãis a (random) collection of mixing weights and errùëÄùê∑(ùë†ùúÉ) = ùîºùë•0‚àºùëÄùê∑ùêø(ùë†ùúÉ; ùë•0) for some\nloss function ùêøis the expected loss of the score function ùë†ùúÉunder the true population ùëÄùê∑. The\nresults we will present shortly are agnostic to the choice of ùêø, but the reader should think of ùêøas\nthe noise prediction loss used in (4) for a fixed time ùë°.\nThe quantity err(ùúã, ùê¥|ùëç) measures the generalization error of the score function of the learning\nalgorithm ùê¥conditional on the training set being ùëç. We will show that the population loss of an\nalgorithm given a dataset ùëçis at least:\n1. its loss on the unseen part of the domain, i.e., the population loss in ùëã\\ ùëçplus\n2. its loss on the elements of ùëçthat belong to subpopulations that are represented only once\nin ùëç(i.e., the dataset contains a single image of a dog or a single image of a car). This loss,\ndenoted by errùëç(ùê¥, 1), is scaled up by a coefficient ùúè1, which expresses the ‚Äùlikelihood‚Äù of\nhaving such subpopulations.\nTypically, we define:\nùúè1 = ùîºùõº‚àºùúã[ùõº2(1 ‚àíùõº)ùëõ‚àí1]\nùîºùõº‚àºùúã[ùõº(1 ‚àíùõº)ùëõ‚àí1] ,\nwhere ùúãis the marginal distribution ùúã(ùëé) = ‚Ñôùê∑[ùê∑ùëñ= ùëé]. Note that, because the random process of\npicking the mixing weights is run independently for any ùëñ‚àà[ùëÅ], the marginal is the same across\ndifferent ùëñ‚Äôs (and hence we omit the index ùëñfrom ùúã). We are now ready to present our result.\nTheorem 4.3 (Informal, see Theorem A.1). It holds that\nerr(ùúã, ùê¥|ùëç) ‚â•errunseen(ùúã, ùê¥|ùëç) + ùúè1 ¬∑ errùëç(ùê¥, 1) .\nThe above result can be extended to subpopulations represented by 2 or more examples in ùëç(see\nAppendix A). The above inequality relates the population error of the model with its loss on some\nparts of the training set. The crucial parameter that relates the two quantities is the coefficient\nùúè1. If the coefficient ùúè1 is large, it means that if the model does not fit the ‚Äùrare examples‚Äù of\nthe dataset, it will have to pay roughly ùúè1 in the generalization error. As shown by [Fel20], ùúè1 is\ncontrolled by how much heavy-tailed is the distribution of the frequencies of the mixture model.\nThis is the topic of the next section, where we also investigate the effect of adding noise to the\ntraining set.\n10\n\n\n4.2.3\nHeavy Tails and the Role of Noise\nIn this section, we are going to formally explain what it means for the frequencies of the original\ndataset to be heavy-tailed [ZAR14, Fel20]. This heavy-tailed structure will then allow us to control\nthe generalization error in Theorem 4.3. We will be interested in subpopulations that have only one\nrepresentative in the training set ùëç(these are the examples that will cost roughly ùúè1 in the error\nof Theorem 4.3). We will refer to them as single subpopulations. For this to happen given that\n|ùëç| = ùëõ, it should be roughly speaking the case where some frequencies ùê∑ùëñare of order 1/ùëõ. The\nquantity that controls how many of the frequencies ùê∑ùëñwill be of order 1/ùëõis the mass that the\ndistribution ùúã(ùëé) = ‚Ñôùê∑[ùê∑ùëñ= ùëé] assigns to the interval [1/(2ùëõ), 1/ùëõ]. Typically, we will call a list of\nfrequencies ùúãheavy-tailed if\nweight\n\u0012\nùúã,\n\u0014 1\n2ùëõ, 1/ùëõ\n\u0015\u0013\n= Œ©(1) .\n(7)\nIn words, there should be a constant number of subpopulations with frequencies of order ùëÇ(1/ùëõ).\nThis definition is important because it can then lower bound the value ùúè1 in Theorem 4.3 and hence\nit can lower bound the generalization loss of not fitting single subpopulations.\nLemma 4.4 (Informal, see Lemma A.2 and Lemma 2.6 in [Fel20]). Consider a dataset of size ùëõ\nand assume that ùúãis heavy-tailed, as in (7). Then ùúè1 = Œ©(1/ùëõ).\nOn the contrary, when ùúãis not heavy-tailed, ùúè1 will be small and hence generalization is not hurt\nby not memorizing (see Lemma A.3). Next, we are going to inspect how the noise scale affects\nthe heavy-tailed structure of the frequencies and hence the value of ùúè1. For an illustration, we will\nconsider the most standard model, that of a mixture of Gaussian subpopulations (similar results are\nexpected for more general population models; we note that the previous results can be naturally\nadapted for the GMM and other cases, see Remark 1 and the discussion in [Fel20]). Let us consider\na density ùëû0 = √çùëÅ\nùëñ=1 ùë§ùëñùí©(ùúáùëñ, ùêº) = √ç\nùëñùë§ùëñùí©ùëñ. We will say that two components ùí©ùëñ, ùí©ùëóare ùúÄ-separated\nif TV(ùí©ùëñ, ùí©ùëó) > 2ùúÄand can be ùúÄ-merged if TV(ùí©ùëñ, ùí©ùëó) ‚â§ùúÄ. If ùí©ùëñand ùí©ùëóare merged, we consider\nthat the new coefficient is ùë§ùëñ+ ùë§ùëó.\nLemma 4.5 (Informal, see Section A.4). Consider the GMM density ùëû0 and let ùëûùë°be the density of\nthe forward diffusion process at time ùë°with schedule ùúéùë°‚àà[0, 1]. Consider any pair of components\nùí©ùëñ, ùí©ùëóin ùëû0 with total variation ùê∂ùëñùëófor some absolute constant ùê∂ùëñùëóand let ùí©ùë°\nùëñ, ùí©ùë°\nùëóbe the associated\ndistributions in ùëûùë°.\n‚Ä¢ (Low Noise) If ùúéùë°‚â§\np\n1 ‚àí(2ùúÄ/ùê∂ùëñùëó)2, then ùí©ùë°\nùëñ, ùí©ùë°\nùëóare ùúÄ-separated.\n‚Ä¢ (High Noise) If ùúéùë°‚â•\np\n1 ‚àí(ùúÄ/ùê∂ùëñùëó)2, then ùí©ùë°\nùëñ, ùí©ùë°\nùëóare ùúÄ-merged with coefficient ùë§ùëñ+ ùë§ùëó.\nFor a more formal treatment, we refer to Section A.4.\nThe above Lemma has the following\ninterpretations. If the noise level is small, the originally separated subpopulations (at ùë°= 0) will\nremain separated. This implies that if the frequencies (i.e., the mixing weights) were originally\nheavy-tailed (as in the above discussion), they will remain heavy-tailed even in the low-noise regime,\ni.e. Lemma 4.4 applies (ùúè1 is large). On the other side, as we increase ùë°, the clusters start to merge\nand the heavy-tailed distribution of the mixing coefficients becomes lighter (until all the clusters\nare merged into a single one). Hence, ùúè1 will be small. This conceptually indicates that there is no\nreason for memorizing the training noisy images ùë•ùë°(and hence the original images ùë•0 which do not\nappear during training).\n11\n\n\nTable 1: FID and Memorization results comparing DDPM and Algorithm 1. Memorization is\nmeasured as DINOv2 similarity between generated samples and their nearest training neighbors.\nWe achieve the same or better FID with significantly lower memorization.\n# Train Images\n300\n1k\n3k\nDDPM Ours\nDDPM Ours\nDDPM Ours\nCIFAR-10\nFID\n25.1\n23.91\n10.46 10.36\n14.73 14.26\nS>0.9\n78.96 44.84\n75.86 69.08\n53.40 52.24\nS>0.925\n67.2\n20.22\n57.98 47.26\n11.92 11.36\nS>0.95\n56.56\n9.64\n43.44 26.34\n0.08\n0.06\nFFHQ\nFID\n16.21 15.05\n12.26\n11.3\n6.42\n6.46\nS>0.85\n63.38 49.68\n55.36 32.08\n21.58 20.08\nS>0.875\n55.48 40.01\n43.82 17.48\n4.98\n4.53\nS>0.9\n47.86 29.86\n33.92\n7.52\n0.46\n0.42\nImageNet\nFID\n‚Äî‚Äî‚Äì\n50.2\n47.19\n40.66 39.87\nS>0.9\n‚Äî‚Äî‚Äì\n54.72 26.68\n32.86 28.40\nS>0.925\n‚Äî‚Äî‚Äì\n41.66 15.56\n12.32\n9.44\nS>0.95\n‚Äî‚Äî‚Äì\n25.86\n5.54\n6.08\n4.02\n5\nExperiments\nTable 2: Comparison between DDPM, our Algorithm 1 and results obtained by training with\nonly corrupted data (masking or additive Gaussian noise). As shown, our algorithm achieves low\nmemorization since it uses noisy data in the high-noise regime, but it also achieves low FID (contrary\nto the algorithms only using corrupted data) as it can copy the high-frequency details from the\ntraining samples.\nMetric\n# Training Images\n300\n1k\n3k\nDDPM Masking Noise Ours DDPM Masking Noise Ours DDPM Masking Noise Ours\nFID\n16.21\n23.40\n27.92 15.05\n12.26\n15.73\n25.57 11.3\n6.42\n7.44\n16.28 6.46\nSim > 0.85\n63.38\n53.73\n29.12 49.68\n55.36\n38.74\n14.83 32.08\n21.58\n19.74\n12.08 20.08\nSim > 0.875\n55.48\n41.37\n18.73 40.01\n43.82\n22.94\n9.37 17.48\n4.98\n4.56\n3.32\n4.53\nSim > 0.9\n47.86\n30.34\n10.60 29.86\n33.92\n10.08\n6.49\n7.52\n0.46\n0.43\n0.36\n0.42\n5.1\nMemorization in Unconditional Models\nWe start our experimental evaluation by measuring the memorization and performance of un-\nconditional diffusion models in several controlled settings.\nSpecifically, we train models from\nscratch on CIFAR-10, FFHQ, and (tiny) ImageNet using 300, 1000 and 3000 training samples. For\neach one of these settings, we compute the Fr¬¥echet Inception Distance [HRU+17] (FID) between\n50,000 generated samples and 50,000 dataset samples as a measure of quality. Following prior\n12\n\n\nwork [SSG+22, SSG+23, DSD+23], we measure memorization by computing the similarity score\n(i.e., inner product) of each generated sample to its nearest neighbor in the embedding space of\nDINOv2 [ODM+23]. For all these experiments, we compare the performance of Algorithm 1 against\nthe regular training of diffusion models (see Eq.(3)).\nChoice of ùë°n.\nOur method has a single parameter ùë°n that needs to be controlled. We argue\nthat there is an interval (ùë°min, ùë°max) that contains reasonable choices of ùë°n. Setting ùë°n too low, i.e.,\n(ùë°n ‚â§ùë°min), essentially reverts back to the original algorithm that produces memorized images\nof good quality. But also, setting ùë°n too high, i.e., ùë°n ‚â•ùë°max, will also lead to memorization as\nthere is more time in the sampling trajectory (the interval [0, ùë°max]), where we use the memorized\nscore. Values in the range (ùë°min, ùë°max) achieve low memorization and strike good balances in the\nquality-memorization trade-off.\nDecreasing memorization without sacrificing quality.\nMost of the prior mitigation strategies\nfor memorization often decrease the image generation quality. Here, we ask: how much do we need\nto memorize to achieve a given image quality? To answer this, we tune the value ùë°n to train models\nusing Algorithm 1 that match the FID obtained by DDPM, and we measure their memorization\nlevels. To report memorization, we use three thresholds in the similarities of DINOv2 embeddings\nthat semantically correspond to: i) potentially memorized image, ii) (partially) memorized image,\nand, iii) exact copy of an image in the training set. The thresholds are tuned separately for each\ndataset to express these semantics. We present analytic results for 300, 1k and 3k training images\nfrom CIFAR-10, FFHQ and (tiny)-ImageNet in Table 13. As shown, for the same or better FID,\nour models achieve significantly lower memorization levels. This leads to the surprising conclusion\nthat models learned by the DDPM loss are not Pareto optimal for small datasets. That said, the\nbenefit from our algorithm in both FID and memorization shrinks as the dataset grows.\nOther points in the Pareto frontier.\nSo far, our goal was to reduce memorization while keeping\nFID the same as DDPM. However, by appropriately tuning the value ùë°n, we can achieve other points\nin the Pareto frontier that achieve varying trade-offs between memorization and quality of generated\nimages. We present these results for a model trained on 300 images from FFHQ in Figure 1. We see\nthat setting ùúéùë°n ‚àà[0.4, 4] corresponds to Pareto optimal points, while setting the value of ùë°n too low\nor too high brings us back to the DDPM performance, as expected. For ùúéùë°n = 4, we almost match\nthe FID that DDPM gets with 1000 images, while we only use 300 images for training, establishing\nour Algorithm as much more data-efficient than DDPM.\nComparison with other mitigation strategies.\nFor completeness, we include comparisons\nwith two other mitigation strategies that reduce memorization in the unconditional setting. These\nmethods are known to achieve lower memorization but at the expense of FID. We compare with a\nmodel trained on linearly corrupted data (random inpainting), as in the work of [DSD+23], and a\nmodel trained with only noisy data as in [DDD24]. We present the results in Table 2. As shown,\nour algorithm produces superior behavior as it achieves lower memorization for the same or better\nFID. The superior performance comes from the ability our method has to generate high-frequency\n3For tiny ImageNet, we do not report results in the 300 samples setting since there are 200 different classes and so\nfor some of the classes we do not observe any samples.\n13\n\n\ndetails, contrary to the existing methods that only use solely noisy data and are not capable of such\nbehavior.\n5.2\nMemorization in Text-Conditional Models\nWe continue our evaluation in text-conditional models. Here, the primal source of memorization is\nthe text-conditioning itself. Wen, Liu, Chen, and Lyu [WLCL24] observe that for certain trigger\nprompts, the prediction of the network always converges to the same training point, independent of\nthe image initialization. Our method mitigates image memorization by training with noisy images,\nso by itself, it cannot mitigate memorization that arises from the text-conditioning. However,\nwe will show that when we combine our method with strategies that mitigate the impact of text\nmemorization, we achieve state-of-the-art results in memorization reduction while keeping the quality\nof the generated images high.\nTable 3: Memorization and FID results for text-conditional models. Sim denotes the average similarity\nbetween a generated sample and its nearest neighbor in the dataset, while 95% is the 95% percentile of the\nsimilarities distribution. CLIP measures the image-text alignment. The combination of our method with\nexisting methods from Somepalli et al. [SSG+23] (S23) and Wen, Liu, Chen, and Lyu [WLCL24] (W24)\nachieves strong CLIP/FID results with reduced memorization.\nMethod\nSim\n95%\nCLIP\nFID\nWithout text mitigation:\nBaseline\n0.378\n0.649\n0.306\n18.18\nOurs\n0.373\n0.636\n0.305\n18.34\nText mitigation:\nS23\n0.319\n0.573\n0.302\n20.55\nS23+ ours\n0.308\n0.547\n0.306\n21.30\nW24\n0.208\n0.300\n0.293\n21.44\nW24+ ours\n0.192\n0.267\n0.293\n20.74\nFollowing prior work [SSG+23], we finetune Stable Diffusion on 10k image-text pairs from a\ncurated subset of LAION [SBV+22] and we measure image quality and memorization of the resulting\nmodels. We compare with existing state-of-the-art methods for reducing memorizing arising from\nthe text-conditioning. Specifically, we compare with the work of Somepalli et al. [SSG+23] where\ncorruption is added to the text-embedding during training and with the work of Wen, Liu, Chen,\nand Lyu [WLCL24] where the model is explicitly trained to pay attention to the visual content (for\ndetails, we refer the reader to the associated papers).\nWe include all the results in Table 3. As shown, the combination of our work with existing\nmethods achieves state-of-the-art memorization performance while performing on par in terms of\nimage quality. As expected, without any text-mitigation our algorithm fails to improve significantly\nthe memorization since the model remains heavily reliant on the text-conditioning, effectively\nignoring the visual content.\n14\n\n\n6\nConclusion and Future Work\nOur work provides a positive note on the rather pessimistic landscape of results regarding the\nmemorization-quality trade-off in diffusion models. We manage to push the Pareto frontier in various\ndata availability settings for both text-conditional and unconditional models. We further provide\ntheoretical evidence for the plausibility of generation of diverse structures without memorization.\nWe remark that our method does not come with any privacy guarantees or optimality properties and\nthat despite some encouraging first theoretical evidence, an end-to-end analysis for the proposed\nalgorithm is currently lacking. We believe that these constitute exciting research directions for\nfuture research.\nReferences\n[AAL23] Jamil Arbas, Hassan Ashtiani, and Christopher Liaw. Polynomial time and private\nlearning of unbounded gaussian mixture models. In International Conference on\nMachine Learning, pages 1018‚Äì1040. PMLR, 2023.\n[ABVE23] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants:\nA unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\n[ADH+24] Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, and Daniel M\nRoy.\nInformation complexity of stochastic convex optimization: Applications to\ngeneralization and memorization. arXiv preprint arXiv:2402.09327, 2024.\n[ANS23] Gil Appel, Juliana Neelbauer, and David A Schweidel. Generative ai has an intellectual\nproperty problem. Harvard Business Review, 7, 2023.\n[BBC+22] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang,\nMicah Goldblum, Jonas Geiping, and Tom Goldstein.\nCold diffusion: Inverting\narbitrary image transforms without noise. arXiv preprint arXiv:2208.09392, 2022.\n[BBDBM24] Giulio Biroli, Tony Bonnaire, Valentin De Bortoli, and Marc M¬¥ezard. Dynamical\nregimes of diffusion models. Nature Communications, 15(1):9957, 2024.\n[BBDD24] Joe Benton, VD Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-linear\nconvergence bounds for diffusion models via stochastic localization. 2024.\n[BBF+21] Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When\nis memorization of irrelevant training data necessary for high-accuracy learning? In\nProceedings of the 53rd annual ACM SIGACT symposium on theory of computing,\npages 123‚Äì132, 2021.\n[BBS22] Gavin Brown, Mark Bun, and Adam Smith. Strong memory lower bounds for learning\nnatural models. In Conference on Learning Theory, pages 4989‚Äì5029. PMLR, 2022.\n[BE02] Olivier Bousquet and Andr¬¥e Elisseeff. Stability and generalization. The Journal of\nMachine Learning Research, 2:499‚Äì526, 2002.\n15\n\n\n[BMN+18] Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff.\nLearners that use little information. In Algorithmic Learning Theory, pages 25‚Äì55.\nPMLR, 2018.\n[BWCS24] Weimin Bai, Yifei Wang, Wenzheng Chen, and He Sun. An expectation-maximization\nalgorithm for training clean diffusion models from corrupted observations. arXiv\npreprint arXiv:2407.01014, 2024.\n[CBLC22] Pierre Chambon, Christian Bluethgen, Curtis P Langlotz, and Akshay Chaudhari.\nAdapting pretrained vision-language foundational models to medical imaging domains.\narXiv preprint arXiv:2210.04133, 2022.\n[CDK22] Chen Cheng, John Duchi, and Rohith Kuditipudi. Memorize to generalize: on the\nnecessity of interpolation in high dimensional linear regression. In Conference on\nLearning Theory, pages 5528‚Äì5560. PMLR, 2022.\n[CHN+23] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian\nTramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data\nfrom diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23),\npages 5253‚Äì5270, 2023.\n[CKS24] Sitan Chen, Vasilis Kontonis, and Kulin Shah. Learning general gaussian mixtures\nwith efficient score matching, 2024.\n[CLX24] Chen Chen, Daochang Liu, and Chang Xu. Towards memorization-free diffusion\nmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8425‚Äì8434, 2024.\n[DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\nhypothesis. arXiv preprint arXiv:2208.05314, 2022.\n[DCD24] Giannis Daras, Yeshwanth Cherapanamjeri, and Constantinos Daskalakis. How much\nis a noisy image worth? data scaling laws for ambient diffusion. arXiv preprint\narXiv:2411.02780, 2024.\n[DDD24] Giannis Daras, Alexandros G Dimakis, and Constantinos Daskalakis. Consistent\ndiffusion meets tweedie: Training exact ambient diffusion models with noisy data.\narXiv preprint arXiv:2404.10177, 2024.\n[DDDD23] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis.\nConsistent diffusion models: Mitigating sampling drift by learning to be consistent.\narXiv preprint arXiv:2302.09057, 2023.\n[DDT+23] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alex Dimakis, and Peyman\nMilanfar. Soft diffusion: Score matching with general corruptions. Transactions on\nMachine Learning Research, 2023.\n[Die24] Sander Dieleman. Diffusion is spectral autoregression, 2024.\n16\n\n\n[DSD+23] Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alex Dimakis, and\nAdam Klivans. Ambient diffusion: Learning clean distributions from corrupted data.\nIn Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n[Efr11] Bradley Efron.\nTweedie‚Äôs formula and selection bias.\nJournal of the American\nStatistical Association, 106(496):1602‚Äì1614, 2011.\n[Fel20] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In\nProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,\npages 954‚Äì959, 2020.\n[FZ20] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why:\nDiscovering the long tail via influence estimation. Advances in Neural Information\nProcessing Systems, 33:2881‚Äì2891, 2020.\n[GDP+23] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On\nmemorization in diffusion models. arXiv preprint arXiv:2310.02664, 2023.\n[GKL24] Khashayar Gatmiry, Jonathan Kelner, and Holden Lee. Learning mixtures of gaussians\nusing diffusion models. arXiv preprint arXiv:2404.18869, 2024.\n[HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.\nAdvances in Neural Information Processing Systems, 33:6840‚Äì6851, 2020.\n[HRU+17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp\nHochreiter. Gans trained by a two time-scale update rule converge to a local nash\nequilibrium. Advances in neural information processing systems, 30, 2017.\n[HSK+25] Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, and Franziska\nBoenisch. Finding nemo: Localizing neurons responsible for memorization in diffusion\nmodels. Advances in Neural Information Processing Systems, 37:88236‚Äì88278, 2025.\n[JKS+24] Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian\nTogelius, and Yuki Mitsufuji. Classifier-free guidance inside the attraction basin may\ncause memorization. arXiv preprint arXiv:2411.16738, 2024.\n[KAAL22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design\nspace of diffusion-based generative models. Advances in neural information processing\nsystems, 35:26565‚Äì26577, 2022.\n[KEME23] Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad. Gsure-based diffusion\nmodel training with corrupted data. arXiv preprint arXiv:2305.13128, 2023.\n[KG24] Mason Kamb and Surya Ganguli. An analytic theory of creativity in convolutional\ndiffusion models. arXiv preprint arXiv:2412.20292, 2024.\n[KSH+24] Joshua Kazdan, Hao Sun, Jiaqi Han, Felix Petersen, and Stefano Ermon. Cpsample:\nClassifier protected sampling for guarding training data during diffusion. arXiv preprint\narXiv:2409.07025, 2024.\n17\n\n\n[KYKM23] Vladimir Kulikov, Shahar Yadin, Matan Kleiner, and Tomer Michaeli. Sinddm: A\nsingle image denoising diffusion model. In International conference on machine learning,\npages 17920‚Äì17930. PMLR, 2023.\n[LC24] Marvin Li and Sitan Chen. Critical windows: non-asymptotic theory for feature\nemergence in diffusion models. arXiv preprint arXiv:2403.01633, 2024.\n[LCBH+22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.\nFlow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.\n[LGL22] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to\ngenerate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.\n[LGWM24] Xiao Liu, Xiaoliu Guan, Yu Wu, and Jiaxu Miao. Iterative ensemble training with\nanti-gradient control for mitigating memorization in diffusion models. In European\nConference on Computer Vision, pages 108‚Äì123. Springer, 2024.\n[Liv24] Roi Livni. Information theoretic lower bounds for information theoretic upper bounds.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[LY15] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. CS231N Course\nReport, Stanford University, 2015.\n[LYM+24] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim\nGupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al.\nHolistic evaluation of text-to-image models. Advances in Neural Information Processing\nSystems, 36, 2024.\n[ODM+23] Maxime Oquab, Timoth¬¥ee Darcet, Th¬¥eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil\nKhalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\net al. Dinov2: Learning robust visual features without supervision. arXiv preprint\narXiv:2304.07193, 2023.\n[RALL24] Fran¬∏cois Rozet, G¬¥erÀÜome Andry, Fran¬∏cois Lanusse, and Gilles Louppe.\nLearning\ndiffusion priors from observations by expectation maximization.\narXiv preprint\narXiv:2405.13712, 2024.\n[RKW+24] Brendan Leigh Ross, Hamidreza Kamkari, Tongzi Wu, Rasa Hosseinzadeh, Zhaoyan Liu,\nGeorge Stein, Jesse C Cresswell, and Gabriel Loaiza-Ganem. A geometric framework\nfor understanding memorization in generative models. arXiv preprint arXiv:2411.00113,\n2024.\n[RLZ+24] Jie Ren, Yaxin Li, Shenglai Zeng, Han Xu, Lingjuan Lyu, Yue Xing, and Jiliang Tang.\nUnveiling and mitigating memorization in text-to-image diffusion models through cross\nattention. In European Conference on Computer Vision, pages 340‚Äì356. Springer,\n2024.\n[RZ19] Daniel Russo and James Zou. How much does your data exploration overfit? controlling\nbias via information usage. IEEE Transactions on Information Theory, 66(1):302‚Äì323,\n2019.\n18\n\n\n[SBS23] Christopher Scarvelis, Haitz S¬¥aez de Oc¬¥ariz Borde, and Justin Solomon. Closed-form\ndiffusion models. arXiv preprint arXiv:2310.12395, 2023.\n[SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\net al. Laion-5b: An open large-scale dataset for training next generation image-text\nmodels. Advances in Neural Information Processing Systems, 35:25278‚Äì25294, 2022.\n[SCK23] Kulin Shah, Sitan Chen, and Adam Klivans. Learning mixtures of gaussians using the\nddpm objective. In Advances in Neural Information Processing Systems, volume 36,\npages 19636‚Äì19649. Curran Associates, Inc., 2023.\n[SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the\ndata distribution. Advances in Neural Information Processing Systems, 32, 2019.\n[SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano\nErmon, and Ben Poole. Score-based generative modeling through stochastic differential\nequations. arXiv preprint arXiv:2011.13456, 2020.\n[SSG+22] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\nDiffusion art or digital forgery? investigating data replication in diffusion models.\narXiv preprint arXiv:2212.03860, 2022.\n[SSG+23] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Gold-\nstein. Understanding and mitigating copying in diffusion models. arXiv preprint\narXiv:2305.20086, 2023.\n[SZ20] Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via con-\nditional mutual information. In Conference on Learning Theory, pages 3437‚Äì3452.\nPMLR, 2020.\n[TKC22] Florian Tram`er, Gautam Kamath, and Nicholas Carlini. Position: Considerations\nfor differentially private learning with large-scale public pretraining. In Forty-first\nInternational Conference on Machine Learning, 2022.\n[Vin11] Pascal Vincent. A connection between score matching and denoising autoencoders.\nNeural computation, 23(7):1661‚Äì1674, 2011.\n[WBL+24] Yifei Wang, Weimin Bai, Weijian Luo, Wenzheng Chen, and He Sun. Integrating\namortized inference with diffusion models for learning clean distribution from corrupted\nimages. arXiv preprint arXiv:2407.11162, 2024.\n[WBZ+25] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan,\nand Houqiang Li. Sindiffusion: Learning a diffusion model from a single natural image.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2025.\n[WCS+24] Zhenting Wang, Chen Chen, Vikash Sehwag, Minzhou Pan, and Lingjuan Lyu.\nEvaluating and mitigating ip infringement in visual generative ai. arXiv preprint\narXiv:2406.04662, 2024.\n19\n\n\n[WLCL24] Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, explaining, and\nmitigating memorization in diffusion models. In The Twelfth International Conference\non Learning Representations, 2024.\n[WLHH24] Jing Wu, Trung Le, Munawar Hayat, and Mehrtash Harandi. Erasediff: Erasing data\ninfluence in diffusion models. arXiv preprint arXiv:2401.05779, 2024.\n[XR17] Aolin Xu and Maxim Raginsky.\nInformation-theoretic analysis of generalization\ncapability of learning algorithms. Advances in neural information processing systems,\n30, 2017.\n[ZAR14] Xiangxin Zhu, Dragomir Anguelov, and Deva Ramanan. Capturing long-tail distribu-\ntions of object subcategories. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 915‚Äì922, 2014.\n[ZLL+24] Benjamin J Zhang, Siting Liu, Wuchen Li, Markos A Katsoulakis, and Stanley J Osher.\nWasserstein proximal operators describe score-based generative models and resolve\nmemorization. arXiv preprint arXiv:2402.06162, 2024.\n20\n\n\nA\nSubpopulations Model and Connections to Diffusion Models\nIn this section, we present a more extensive exposition of the framework of the work of [Fel20].\nMoreover, we adapt this framework to diffusion models.\nA.1\nSubpopulations Model of Feldman [Fel20]\nLet us recall the subpopulations model of [Fel20]. Let us consider a continuous data domain ùëã‚äÜ‚Ñùùëë.\nWe model the data distribution as a mixture of ùëÅfixed distributions ùëÄ1, ..., ùëÄùëÅ, where each\ncomponent corresponds to a subpopulation. For simplicity, we follow Feldman [Fel20] and assume\nthat each component ùëÄùëñhas disjoint support ùëãùëñ(we can relax this condition, see Remark 1).\nWithout loss of generality, let ùëã= ‚à™ùëñùëãùëñ. We will now describe the procedure of [Fel20] that assigns\nfrequencies to each subpopulation of the mixture.\n1. First consider a (fixed) list of frequencies ùúã= (ùúã1, ùúã2, ..., ùúãùëÅ).\n2. For each component ùëñ‚àà[ùëÅ] of the mixture, we select randomly and independently an element\nùëùùëñfrom the list ùúã.\n3. Finally, to obtain the mixing weights, we normalize the weights ùëù1, ..., ùëùùëÅ, i.e., the weight of\ncomponent ùëñis ùê∑ùëñ=\nùëùùëñ\n√ç\nùëó‚àà[ùëÅ] ùëùùëó.\nWe summarize the above as follows:\nDefinition 1 (Random Frequencies [Fel20]). For the mixing weights, we first consider a list of\nsubpopulation frequencies ùúã= (ùúã1, ..., ùúãùëÅ). The procedure is the following: we randomly pick ùëùùëñ\nfrom the list ùúãfor any index ùëñ‚àà[ùëÅ] and then we normalize (ùëùùëñ/√ç\nùëóùëùùëódenotes the frequency of\nsubpopulation ùëñ). We denote by ùíüùúãthe distribution over probability mass functions on [ùëÅ] induced\nby the above procedure.\nA sample ùê∑‚àºùíüùúãis just a list of the frequencies of the ùëÅsubpopulations.\nWe also denote by ùúãthe resulting marginal distribution over the frequency of any single element\nin ùëñ, i.e.,\nùúã(ùëé) = ‚Ñôùê∑‚àºùíüùúã[ùê∑ùëñ= ùëé] .\n(8)\nHence, if ùê∑‚àºùíüùúã, then we can define the true mixture as\nùëÄùê∑(ùë•) =\n√ï\nùëñ‚àà[ùëÅ]\nùê∑ùëñùëÄùëñ(ùë•) .\nThe above random distribution corresponds to the subpopulations model introduced by Feldman\n[Fel20]. Intuitively the choice of the random coefficients for the mixture corresponds to the fact\nthat the learner does not know the true frequencies of the subpopulations.\nA.2\nAdaptation of [Fel20]‚Äôs Result to Diffusion Models\nAs explained in the Background Section 2, one way to train a generative model is to estimate the\nscore function ‚àálog ùëÄùê∑ùë°for all levels of noise indexed by ùë°. For the analysis of this Section, we\nconsider the case of a single fixed ùë°. We define learning algorithms ùê¥as (potentially randomized)\n21\n\n\nmappings from datasets ùëçto score functions ùë†ùúÉ‚àºùê¥(ùëç). We further define the expected error of ùê¥\nconditioned on dataset being equal to ùëç‚ààùëãùëõ(eventually ùëçwill be drawn i.i.d. from ùëÄùê∑) as\nerr(ùúã, ùê¥|ùëç) = ùîºùê∑‚àºùíüùúã(¬∑|ùëç)ùîºùë†ùúÉ‚àºùê¥(ùëç)errùëÄùê∑(ùë†ùúÉ) ,\nwhere ùê∑‚àºùíüùúãis a random list of frequencies according to Definition 1 and errùëÄùê∑(ùë†ùúÉ) = ùîºùë•0‚àºùëÄùê∑ùêø(ùë†ùúÉ; ùë•0)\nfor some loss function ùêø. The results we will present shortly are agnostic to the choice of loss\nfunction ùêø, but the reader should think of ùêøas the noise prediction loss used in (4) for a fixed time\nùë°.\nWe remark that the quantity err(ùúã, ùê¥|ùëç) measures the generalization error of the output score\nfunction (according to loss function ùêø) of the learning algorithm ùê¥conditional on the training set\nbeing ùëç. We will relate this generalization error with the loss in the training set. Recall that any\nsubpopulation ùëñ‚àà[ùëÅ] of the mixture is associated with a domain ùëãùëñ(and ùëãùëñ‚à©ùëãùëó= ‚àÖfor ùëñ‚â†ùëó).\nLet ùëõbe the training set size. For any ‚Ñì‚àà[ùëõ], consider all the subpopulations ùêº‚Ñì‚äÜ[ùëÅ] such\nthat ùëãùëñ‚à©ùëç= ‚Ñìfor ùëñ‚ààùêº‚Ñì; in words, ùëñ‚ààùêº‚Ñìif there are exactly ‚Ñìrepresentatives of cluster ùëñin the\ndataset ùëç. We can now define ùëç‚Ñì= {ùë•‚ààùëãùëñ‚à©ùëç: ùëñ‚ààùêº‚Ñì} ‚äÜùëç. Note that the sets ùëç1, ..., ùëçùëÅpartition\nthe training set ùëç. For ‚Ñì‚àà[ùëõ], we define\nerrnùëç(ùê¥, ‚Ñì) = ùîºùë†ùúÉ‚àºùê¥(ùëç)\n√ï\nùë•‚ààùëç‚Ñì\nùëÄùëñùë•(ùë•)ùêø(ùë†ùúÉ; ùë•) .\n(9)\nHere ùëñùë•‚àà[ùëÅ] is the unique index of the component whose support contains ùë•. In words, errnùëç(ùê¥, ‚Ñì)\nis the loss of the algorithm ùê¥evaluated on the elements of the training set ùëçthat belong to\nsubpopulations will exactly ‚Ñìrepresentatives in ùëç.\nWe show the following result, which is an adaptation of a result of [Fel20] and relates the\npopulation loss with the empirical losses errnùëç(ùê¥, 1), ..., errnùëç(ùê¥, ùëõ).\nTheorem A.1. Fix a number of samples ùëõ. Let {ùëÄùëñ}ùëñ‚àà[ùëÅ] be densities of subpopulations over\ndisjoint subdomains {ùëãùëñ}ùëñ‚àà[ùëÅ]. Let ùúãbe the fixed list of frequencies as in Definition 1 and let ùúãùëÅ\nthe marginal distribution of (8). For any learning algorithm ùê¥and any fixed dataset ùëç‚ààùëãùëõ, it\nholds that\nerr(ùúã, ùê¥|ùëç) = errunseen(ùúã, ùê¥|ùëç) +\n√ï\n‚Ñì‚àà[ùëõ]\nùúè‚Ñì¬∑ errnùëç(ùê¥, ‚Ñì) ,\n(10)\nwhere\n1. errunseen(ùúã, ùê¥|ùëç) corresponds to the expected ùëç-conditional loss of the algorithm ùê¥on the\npoints that do not appear in the training set ùëç.\n2. ùúè‚Ñìis a coefficient that corresponds to the weight of having subpopulations with exactly ‚Ñì\nrepresentatives. Given ùíüùúãand ‚Ñì‚àà[ùëõ], we define\nùúè‚Ñì= ùîºùõº‚àºùúã[ùõº‚Ñì+1(1 ‚àíùõº)ùëõ‚àí‚Ñì]\nùîºùõº‚àºùúã[ùõº‚Ñì(1 ‚àíùõº)ùëõ‚àí‚Ñì] .\nFor the proof we refer to Section C.2.1. The above general form relates the population error of the\nmodel with its loss on the training set. The crucial parameters that relate the two quantities are\nthe coefficients ùúè1, ..., ùúèùëõ. If the coefficient ùúè1 is large, it means that if the model does not fit the\ntraining examples that appear once in the dataset (‚Äùrare examples‚Äù), it will have to pay roughly ùúè1\nin the generalization error. As shown by [Fel20], ùúè1 is controlled by how much heavy-tailed is the\ndistribution of the frequencies of the mixture model. This is the topic of the next section, where we\nalso investigate the effect of adding noise to the training set.\n22\n\n\nRemark 1 (Gaussian Mixture Models). Subpopulations are often modeled as Gaussians. If the\nprobability of the overlap between the subpopulations is sufficiently small (the means are far),\nthen one can reduce this case to the disjoint one by modifying the components ùëÄùëñto have disjoint\nsupports while changing the marginal distribution over ùëçby at most ùõøin the TV distance.\nA.3\nHeavy-Tailed Distributions of Frequencies\nIn this section, we are going to formally explain what it means for the frequencies of the original\ndataset to be heavy-tailed [ZAR14, Fel20]. This heavy-tailed structure will then allow us to control\nthe generalization error in Theorem 4.3. Following Feldman [Fel20], we will assume that the mixing\ncoefficients ùê∑1, ..., ùê∑ùëÅare drawn from a heavy-tailed distribution since this is the case in most\ndatasets [Fel20, FZ20]. We will be interested in subpopulations that have only one representative in\nthe training set ùëç(these are the examples that will cost roughly ùúè1 in the error of Theorem 4.3).\nWe will refer to them as single subpopulations. For this to happen given that |ùëç| = ùëõ, it should be\nroughly speaking the case where some frequencies ùê∑ùëñare of order 1/ùëõ.\nThe quantity that controls how many of the frequencies ùê∑ùëñwill be of order 1/ùëõis the marginal\ndistribution ùúã(ùëé) = ‚Ñôùê∑[ùê∑ùëñ= ùëé]. We first note that the expected number of singleton examples is\ndetermined by the weight of the entire tail of frequencies below 1/ùëõin ùúã. In particular, one can\nshow (see [Fel20]) that the expected number of singleton points is at least\nùëõ\n2 ¬∑ weight(ùúã, [0, 1/ùëõ]) ,\nwhere\nweight(ùúã, [0, 1/ùëõ]) := ùîºùê∑‚àºùíü\nÔ£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\n√ï\nùëñ‚àà[ùëÅ]\nùê∑ùëñ1{ùê∑ùëñ‚àà[0, 1/ùëõ]}\nÔ£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª\n= ùëÅ¬∑ ùîºùëé‚àºùúã[ùëé1{ùëé‚àà[0, 1/ùëõ]}] .\nThe above weight function essentially controls how heavy-tailed our distribution over frequencies\nis. Typically, we will call a list of frequencies ùúãheavy-tailed if\nweight\n\u0012\nùúã,\n\u0014 1\n2ùëõ, 1/ùëõ\n\u0015\u0013\n= Œ©(1) .\nIn words, there should be a constant number of subpopulations with frequencies of order 1/ùëõ. This\ndefinition is important because it can then lower bound the value ùúè1 in Theorem 4.3 and hence it\ncan lower bound the generalization loss of not fitting single subpopulations.\nLemma A.2 (Lemma 2.6 in [Fel20]). For any ùúã, it holds that ùúè1 ‚â•\n1\n5ùëõ¬∑ weight(ùúã, [ 1\n3ùëõ, 2\nùëõ]).\nAs an illustration, if ùúãis the Zipf distribution and the number of clusters ùëÅ‚â•ùëõthen ùúè1 = Œ©(1/ùëõ)\nand weight(ùúã, [0, 1/ùëõ]) = Œ©(1) (see [Fel20] for more examples). On the contrary, when ùúãis not\nheavy-tailed, ùúè1 will be small.\nLemma A.3 (Lemma 2.7 in [Fel20]). Let ùúãbe a frequency prior such that for some ùúÉ‚â§1/(2ùëõ),\nweight(ùúã, [ùúÉ, ùë°/ùëõ]) = 0, where ùë°= ln(1/(ùúÉùõΩ)), ùõΩ= weight(ùúã, [0, ùúÉ]). Then ùúè1 ‚â§2ùúÉ.\nThe above lemma indicates that when the frequencies are not heavy-tailed then ùúè1 is small (and\nhence generalization is not hurt by not memorizing).\n23\n\n\nA.4\nThe Effects of Noise\nIn this section we analyze the effect of adding noise to the training set. We distinguish two cases:\nthe low noise regime and the high noise regime.\nLow Noise Regime.\nWhen the noise level is small, the originally separated subpopulations (at\nùë°= 0) will remain separated. This implies that if the frequencies of the subpopulations were originally\nheavy-tailed (as in the above discussion), they will remain heavy-tailed even in the low-noise regime.\nThis will imply that some clusters will be represented by singletons (‚Ñì= 1) and any algorithm\nthat satisfies errnùëç(ùê¥, 1) ‚â†0 has to pay ùúè1 ¬∑ errnùëç(ùê¥, 1) in the population error with ùúè1 being lower\nbounded as in Lemma A.2. We interpret errnùëç(ùê¥, 1) ‚âà0 as evidence for memorization. To be more\nconcrete, we will need the following definition that is a smooth generalization of single representative\nof a subpopulation.\nDefinition 2. We will say that a subpopulation ùê∂has an ùúÄ-smoothed single representative in a set\nof points ùëÜbelonging to ùê∂if for any ùë•, ùë•‚Ä≤ ‚ààùëÜ, it holds that ‚à•ùë•‚àíùë•‚Ä≤‚à•‚â§ùúÄ.\nIntuitively this means that if there are more than one images in the training set ùëçfrom ùê∂, they are\nall very close to each other. This will be the case in diffusion with low noise.\nLemma A.4 (Subpopulations Remain Heavy-Tailed). Consider an example ùë•0 ‚àà‚Ñùùëëthat is the\nunique representative of a subpopulation ùëó‚àà[ùëÅ] in the training set ùëçwith ‚à•ùë•0‚à•‚â§poly(ùëë). Consider\nùëönoisy copies {ùë•ùëñ\nùë°}ùëñ‚àà[ùëö] of ùë•0 at noise level ùë°: ùë•ùëñ\nùë°=\nq\n1 ‚àíùúé2\nùë°ùë•0 + ùúéùë°ùëßùëñ\nùë°, ùëßùëñ\nùë°‚àºùí©(0, ùêºùëë) . Then the\nsubpopulation ùëóhas a poly(1/ùëë)-smoothed single representative in the set {ùë•ùëñ\nùë°}ùëñ‚àà[ùëö] for ùúéùë°= poly(1/ùëë)\nwith probability at least 1 ‚àíùëöexp(‚àíùëë/2).\nThe proof appears in Section C.2.2. The above lemma implies that if the original dataset contains\nvarious well separated images (in the sense that correspond to representatives of single subpop-\nulations), then after adding noise to each one of them (and even if we create multiple copies for\neach example), the clusters will remain separated when ùúéùë°is small. This implies that the single\nsubpopulations remain and Lemma A.2 applies (ùúè1 is large).\nFor an illustration, let us consider the GMM density function ùëû= √çùëÅ\nùëñ=1 ùë§ùëñùí©(ùúáùëñ, ùêº).\nIt is\na standard calculation to see that at time ùë°, the pdf of the forward diffusion process is ùëûùë°=\n√çùëÅ\nùëñ=1 ùë§ùëñùí©(\nq\n1 ‚àíùúé2\nùë°ùúáùëñ, ùêº), which means that the clusters are starting to concentrate around 0 as ùë°‚Üí1\nand the images from different subpopulations are starting to look more and more indistinguishable\n(since the TV distance between the components is contracting with ùë°). We will say that two\ncomponents ùí©, ùí©‚Ä≤ are ùúÄ-separated if TV(ùí©, ùí©‚Ä≤) > 2ùúÄ.\nLemma A.5 (Clusters Are Separated in Low Noise). Any pair of Gaussians with original total\nvariation ùê∂= 1/600 will be ùúÄ-separated at noise scale ùúéùë°‚â§\np\n1 ‚àí(2ùúÄ/ùê∂)2.\nFor the proof, see Section C.2.3.\nHigh Noise Regime.\nAs we increase ùë°, we add more and more noise to the images. This means that\nthe clusters start to merge and the heavy-tailed distribution of the mixing coefficients becomes lighter\n(until all the clusters are merged into a single one). To illustrate this phenomenon, we will consider\na mixture of Gaussians, which is the standard model for clustering tasks (we expect similar behavior\n24\n\n\nfor more general mixture models). Let us again consider the density function ùëû= √çùëÅ\nùëñ=1 ùë§ùëñùí©(ùúáùëñ, ùêº).\nAlso, let the pdf of the forward diffusion process be ùëûùë°= √çùëÅ\nùëñ=1 ùë§ùëñùí©(\nq\n1 ‚àíùúé2\nùë°ùúáùëñ, ùêº). We will say that\ntwo components ùí©, ùí©‚Ä≤ can be ùúÄ-merged if TV(ùí©, ùí©‚Ä≤) ‚â§ùúÄ.\nLemma A.6 (Clusters Merge in High Noise). Any pair of Gaussians with original total variation\nùê∂= 1/600 will be ùúÄ-merged at noise scale ùúéùë°‚â•\np\n1 ‚àí(ùúÄ/ùê∂)2.\nFor the proof, see Section C.2.3. As the clusters are getting merged, then their coefficients are\nadded up and their distribution is no more heavy-tailed. Hence, Lemma A.3 implies that ùúè1 will be\nsmall. This conceptually indicates that there is no reason for memorizing the training noisy images\nùë•ùë°(and hence the original images ùë•0 which do not appear during training).\nGiven the above discussion, we reach the conclusion that if the frequencies of the original\nsubpopulations are heavy-tailed then, in the low-noise regime, the training set will have single\nsubpopulations and, in that case, fitting these single representatives is required for successful\ngeneralization. However, in the high-noise regime, the noisy training set does not have isolated\nexamples and, in principle, there is no reason to memorize its elements (and hence even elements of the\noriginal set). We believe that this discussion sheds some light on the nature of memorization needed\nfor optimal generative modeling and motivates our training Algorithm 1 that avoids memorization\nonly in the high-noise regime.\nB\nNoisy Data Training of stable Diffusion using v-Prediction\nThe variance-preserving forward process defines the following transition probability distribution:\nùëù(ùëãùë°= ùë•ùë°|ùëã0) = ùí©(ùë•ùë°; ùõºùë°ùëã0, ùúé2\nùë°ùêº) and ùëù(ùëãùë°= ùë•ùë°|ùëãùë†) = ùí©(ùë•ùë°; (ùõºùë°/ùõºùë†)ùëãùë†, ùúé2\nùë°|ùë†ùêº).\nwhere ùúé2\nùë°|ùë†= (1‚àí\nùõº2\nùë°ùúé2\nùë†\nùúé2\nùë°ùõº2ùë†)ùúé2\nùë°. Let ùë°n be the noise scale corresponding to the noisy data and the noisy data\nùë•ùë°n from the clean data ùë•0 has the probability distribution ùëù(ùëãùë°n = ùë•ùë°n|ùëã0) = ùí©(ùë•ùë°n; ùõºùë°nùëã0, ùúé2\nùë°nùêº).\nIn this case, the following Lemma holds.\nLemma B.1. ùîº[ùëãùë°n|ùëãùë°] =\nùõºùë°n ùúé2\nùë°|ùë°n\nùúé2\nùë°\nùîº[ùëã0|ùëãùë°] +\nùõºùë°ùúé2\nùë°n\nùúé2\nùë°ùõºùë°n ùëãùë°.\nProof. Let ùëùùë°(¬∑) denote the probability density of the random variable ùëãùë°. Observe that ùëãùë°=\nùõºùë°ùëã0 + ùúéùë°ùëç. Using Tweedie‚Äôs formula, we have\n‚àálog ùëùùë°(ùëãùë°) = ùõºùë°ùîº[ùëã0|ùëãùë°] ‚àíùëãùë°\nùúé2\nùë°\n.\nAdditionally, the random variable ùëãùë°= (ùõºùë°/ùõºùë°n)ùëãùë°n + ùúéùë°|ùë°nùëç. Using Tweedie‚Äôs formula, we can write\nthe score function\n‚àálog ùëùùë°(ùëãùë°) = (ùõºùë°/ùõºùë°n)ùîº[ùëãùë°n|ùëãùë°] ‚àíùëãùë°\nùúé2\nùë°|ùë°n\n.\n25\n\n\nUsing the above two equations, we have\n(ùõºùë°/ùõºùë°n)ùîº[ùëãùë°n|ùëãùë°] ‚àíùëãùë°\nùúé2\nùë°|ùë°n\n= ùõºùë°ùîº[ùëã0|ùëãùë°] ‚àíùëãùë°\nùúé2\nùë°\nùîº[ùëãùë°n|ùëãùë°] =\nùõºùë°nùúé2\nùë°|ùë°n\nùõºùë°ùúé2\nùë°\n(ùõºùë°ùîº[ùëã0|ùëãùë°] ‚àíùëãùë°) + ùõºùë°nùëãùë°\nùõºùë°\n=\nùõºùë°nùúé2\nùë°|ùë°n\nùúé2\nùë°\nùîº[ùëã0|ùëãùë°] +\nùõºùë°ùúé2\nùë°n\nùúé2\nùë°ùõºùë°n\nùëãùë°\n.\n‚ñ°\nLemma B.2. Predicting ùõºùë°ùëç‚àíùúéùë°\n(ùëãùë°n‚àí\nùõºùë°ùúé2\nùë°n\nùúé2\nùë°ùõºùë°n\nùëãùë°)\nùõºùë°n ùúé2\nùë°|ùë°n\nùúé2\nùë°\ngives us that the optimal ùë£-prediction.\nC\nProofs\nC.1\nTechnical Details about Information Leakage\nC.1.1\nProof of Lemma 4.1\nProof. The distribution of the training data conditioned on the dataset ùëÜùë°n is ùëû0(ùë•) = 1\nùëõ\n√ç\nùë•ùë°n‚ààùëÜùë°n ùõø(ùë•‚àí\nùë•ùë°n). To obtain iterates at time ùë°, we add additional noise to points ùë•ùë°n ‚ààùëÜùë°n. Particularly, the\nfollowing relation holds for any ùë°‚àà(ùë°n, ùëá]:\nùëãAmb\nùë°\n=\ns\n1 ‚àíùúé2\nùë°\n1 ‚àíùúé2\nùë°n\nùëãùë°n +\nv\nt\nùúé2\nùë°‚àíùúé2\nùë°n\n1 ‚àíùúé2\nùë°n\nùúÄ, ùúÄ‚àºùí©(0, ùêº) .\nThis induces a distribution for each time ùë°:\nùëûùë°(ùë•|ùëÜùë°n) = 1\nùëõ\n√ï\nùë•ùë°n‚ààùëÜùë°n\nùí©\n \nùë•;\ns\n1 ‚àíùúé2\nùë°\n1 ‚àíùúé2\nùë°n\nùë•ùë°n,\nùúé2\nùë°‚àíùúé2\nùë°n\n1 ‚àíùúé2\nùë°n\n¬∑ ùêº\n!\n.\nThe score of the Gaussian mixture ùëûùë°is given by\nùë†Amb\nùë°\n(ùë•|ùëÜùë°n) =\n1\nùúé2\nùë°‚àíùúé2\nùë°n\n1‚àíùúé2\nùë°n\n√ï\nùë•ùë°n‚ààùëÜùë°n\n s\n1 ‚àíùúé2\nùë°\n1 ‚àíùúé2\nùë°n\nùë•ùë°n ‚àíùë•\n!\nùí©(ùë•;\nr\n1‚àíùúé2\nùë°\n1‚àíùúé2\nùë°n\nùë•ùë°n,\nùúé2\nùë°‚àíùúé2\nùë°n\n1‚àíùúé2\nùë°n\n¬∑ ùêº)\n√ç\nùë¶‚ààùëÜùë°n ùí©(ùë•;\nr\n1‚àíùúé2\nùë°\n1‚àíùúé2\nùë°n\nùë¶,\nùúé2\nùë°‚àíùúé2\nùë°n\n1‚àíùúé2\nùë°n\n¬∑ ùêº)\n.\nSince the reverse flow of Eq.(5) provably reverses the forward diffusion [SSDK+20], the distribution\nùëû‚Üê\n0 equals the empirical data distribution ùëû0, which is a sum of delta functions on the noisy training\nset ùëÜùë°n.\n‚ñ°\n26\n\n\nC.1.2\nProof of Lemma 4.2\nProof. For two random variables ùëã, ùëå, recall that ùêº(ùëã;ùëå) = ùêª(ùëã) + ùêª(ùëå) ‚àíùêª(ùëã, ùëå), where ùêª(ùëã)\nis the entropy of ùëãand ùêª(ùëã, ùëå) is the joint entropy of ùëãand ùëå. Without loss of generality, let\nùúá= 0. Let ùë•0 ‚àºùí©(0, Œ£). For the Ambient Diffusion at time ùë°n, conditional on the noisy point being\nùë•ùë°n, the optimal distribution learned is ùõø(ùë•‚àíùë•ùë°n), where ùë•ùë°n =\nq\n1 ‚àíùúé2\nùë°nùë•0 + ùúéùë°nùëç. Note that ùëõi.i.d.\ndraws from this distribution (denoted by ùê¥) are identical and hence\nùêº(ùê¥; ùë•0) = ùêº(ùë•ùë°n; ùë•0) .\nNow observe that\nùë•0 ‚àºùí©(0, Œ£)\nand\nùë•ùë°n ‚àºùí©(0, (1 ‚àíùúé2\nùë°n)Œ£ + ùúé2\nùë°nùêº) .\nMoreover, for the random column vector ùúÅ= [ùë•0, ùë•ùë°n]‚ä§, we have that\nùîº[ùúÅùúÅ‚ä§] =\nÔ£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\nŒ£\nq\n1 ‚àíùúé2\nùë°nŒ£\nq\n1 ‚àíùúé2\nùë°nŒ£\n(1 ‚àíùúé2\nùë°n)Œ£ + ùúé2\nùë°nùêº\nÔ£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª\n.\nNow it remains to control the mutual information of Gaussians. Given that Œ£‚àí1 exists, we note\nthat det(ùîº[ùúÅùúÅ‚ä§]) = det(Œ£) ¬∑ det(ùúé2\nùë°nùêº). We can hence write\nùêº(ùë•ùë°n; ùë•0) = 1\n2 log\ndet(ùîº[ùë•0ùë•‚ä§\n0 ])det(ùîº[ùë•ùë°nùë•‚ä§\nùë°n])\ndet(ùîº[ùúÅùúÅ‚ä§])\n= 1\n2 log\ndet((1 ‚àíùúé2\nùë°n)Œ£ + ùúé2\nùë°nùêº)\ndet(ùúé2\nùë°nùêº)\n.\n(11)\nThis simplifies to\n1\n2 log det\n \nùêº+\n1 ‚àíùúé2\nùë°n\nùúé2\nùë°n\nŒ£\n!\n,\nwhere\n1‚àíùúé2\nùë°n\nùúé2\nùë°n\ncorresponds to the signal-to-noise ratio. (An equivalent way to see the above, is by\ntaking the conditional distribution ùë•ùë°n|ùë•0, which has covariance ùúé2\nùë°nùêº, and hence directly get (11).)\nOn the other side, for DDPM, the learned distribution is ùí©(\nq\n1 ‚àíùúé2\nùë°nùë•0, ùúé2\nùë°nùêº). Let ùëã1 be a single\ndraw from that distribution. Hence, ùëöi.i.d. draws ùëÜfrom that measure correspond to mutual\ninformation\nùêº(ùëÜ; ùë•0) = ùëö¬∑ ùêº(ùëã1; ùë•0) = ùëö¬∑ ùêº(ùë•ùë°n; ùë•0) .\nThis concludes the proof.\n‚ñ°\nC.1.3\nAdditional Bounds on Mutual Information for Ambient Diffusion\nThe following lemma gives a bound on the mutual information of a generated set of size ùëöfrom\nAmbient Diffusion at time ùë°n given a training set of size ùëÅ. On the other side, the mutual information\nof the DDPM solution at time t-nature should be ùëötimes larger.\n27\n\n\nLemma C.1. Consider a dataset ùëÜof size ùëÅdrawn i.i.d. from ùí©(ùúá, ùêº). Consider the optimal\nambient solution at time ùë°n with input ùëÜ. Consider a set ùê¥of size ùëögenerated i.i.d. by that\ndistribution. Then ùêº(ùê∑; ùëÜ) ‚â§ùëöùëë/2 ¬∑ log\n\u0010\n1/ùúé2\nùë°n\n\u0011\n.\nProof. Let ùëãùëñbe ùëÅi.i.d. draws from ùí©(ùúá, ùêº). For each ùëñ, let ùëåùëñ= (1 ‚àíùúé2\nùë°n)ùëãùëñ+ ùúéùë°nùëçùëñfor some\nindependent normal ùëçùëñ‚àºùí©(0, ùêº). We know that the optimal ambient solution is the empirical\ndistribution\nùëÉ(ùë¶) = 1\nùëÅ\n√ï\nùëñ\nùõø(ùë¶‚àíùëåùëñ) ,\nconditioned on the realization of the noisy dataset ùëå= {ùëå1, ..., ùëåùëÅ}.\nBy the data processing inequality, we know that ùêº(ùê¥; ùëÜ) ‚â§ùêº(ùëå; ùëÜ). Since ùëåùëñare generated all in\nthe same way and independently, we can write\nùêº(ùëå; ùëÜ) =\n√ï\nùëñ\nùêº(ùëåùëñ; ùëãùëñ) = ùëöùêº(ùëå1; ùëã1) .\nWe have that ùêº(ùëå1; ùëã1) = ùêª(ùëå1)‚àíùêª(ùëå1|ùëã1). Recall that ùëã1 ‚àºùí©(ùúá, ùêº) and ùëå1|ùëã1 ‚àºùí©((1‚àíùúé2\nùë°n)ùëã1, ùúé2\nùë°nùêº).\nMoreover, note that ùëå1 ‚àºùí©((1 ‚àíùúé2\nùë°n)ùúá, ùêº). These imply that\nùêª(ùëå1) = ùëë\n2 log(2ùúãùëí)\n(since it has identity covariance) and\nùêª(ùëå1|ùëã1) = ùëë\n2 log\n\u0010\n2ùúãùëíùúé2\nùë°n\n\u0011\n.\nThis means that\nùêº(ùëå1; ùëã1) = ùëë\n2 log\n\u0010\n1/ùúé2\nùë°n\n\u0011\n.\nThis concludes the proof.\n‚ñ°\nC.2\nTechnical Details about the Subpopulations Model\nC.2.1\nProof Theorem A.1\nProof. For each subpopulation with exactly ‚Ñìrepresentatives, we put in the set ùëãùëç#‚Ñìthose representa-\ntives. Observe that the collection of sets {ùëãùëç#‚Ñì} partitions ùëçfor ‚Ñì‚àà{1, ..., ùëõ}. Set ùëãùëç= ‚à™‚Ñì‚àà[ùëõ]ùëãùëç#‚Ñì.\nThe unseen points correspond to the set ùëãùëç#0.\nWith this notation in hand, we define\nerrnùëç(ùê¥, ‚Ñì) = ùîºùë†ùúÉ‚àºùê¥(ùëç)\n√ï\nùë•‚ààùëãùëç#‚Ñì\nùëÄùëñùë•(ùë•)ùêø(ùë†ùúÉ, ùë•) .\nwhere ùëñùë•is the index of the unique component whose support contains ùë•. We have that\nerr(ùúã, ùê¥|ùëç) = ùîºùê∑‚àºùê∑ùëã\nùúã(¬∑|ùëç)ùîºùë†ùúÉ‚àºùê¥(ùëç)\n√ï\nùë•‚ààùëã\nùëÄùê∑(ùë•) ¬∑ ùêø(ùë†ùúÉ, ùë•) .\n28\n\n\nWe now decompose ùëã= ùëãùëç‚à™ùëãùëç#0 and write\nerr(ùúã, ùê¥|ùëç) =\n√ï\nùë•‚ààùëãùëç\nùîºùê∑,ùë†ùúÉ[ùëÄùê∑(ùë•) ¬∑ ùêø(ùë†ùúÉ, ùë•)] +\n√ï\nùë•‚ààùëãùëç#0\nùîºùê∑,ùë†ùúÉ[ùëÄùê∑(ùë•) ¬∑ ùêø(ùë†ùúÉ, ùë•)] .\nLet us first deal with the second term. For any ùë•‚ààùëãùëç#0, it holds\nùîºùê∑,ùë†ùúÉ[ùëÄùê∑(ùë•) ¬∑ ùêø(ùë†ùúÉ, ùë•)] = ùîºùê∑‚àºùê∑ùúã(¬∑|ùëç)ùëÄùê∑(ùë•) ¬∑ ùîºùë†ùúÉ‚àºùê¥(ùëç)ùêø(ùë†ùúÉ, ùë•),\nbecause the way we choose ùê∑is independent of the random variable ùêø(ùë†ùúÉ, ùë•) which only depends on\nthe way the algorithm picks the score function given the dataset.\nSet ùëù(ùë•, ùëç) = ùîºùê∑‚àºùê∑ùúã(¬∑|ùëç)ùëÄùê∑(ùë•). Hence, from the elements that do not appear in ùëç, we get a\ncontribution\n√ï\nùë•‚ààùëãùëç#0\nùëù(ùë•, ùëç) ¬∑ ùîºùë†ùúÉ‚àºùê¥(ùëç)ùêø(ùë†ùúÉ, ùë•) .\n(12)\nLet us now deal with the elements appearing in ùëç. Fix ‚Ñì‚àà[ùëõ]. For any ùë•‚ààùëãùëç#‚Ñì, we have that\nùîºùê∑,ùë†ùúÉ[ùëÄùê∑(ùë•) ¬∑ ùêø(ùë†ùúÉ, ùë•)] = ùîºùê∑‚àºùê∑ùúã(¬∑|ùëç)[ùëÄùê∑(ùë•)] ¬∑ ùîºùë†ùúÉ‚àºùê¥(ùëç)ùêø(ùë†ùúÉ, ùë•) ,\nsince the random variables ùêø(ùë†ùúÉ, ùë•) and ùëÄùê∑(ùë•) are independent given ùëç. By Lemma 2.1 in [Fel20]\nand since the supports of the components ùëÄ1, ..., ùëÄùëÅare disjoint, we know that ùîºùê∑‚àºùê∑ùúã(¬∑|ùëç)[ùëÄùê∑(ùë•)] =\nùîº[ùê∑(ùëñùë•)ùëÄùëñùë•(ùë•)] = ùîº[ùê∑(ùëñùë•)]ùëÄùëñùë•(ùë•) = ùúè‚ÑìùëÄùëñùë•(ùë•), where ùëñùë•is the index of the component whose support\ncontains ùë•. Hence, we have that\n√ï\nùë•‚ààùëãùëç\nùîº[ùëÄùê∑(ùë•)¬∑ùêø(ùë†ùúÉ, ùë•)] =\n√ï\n‚Ñì‚àà[ùëõ]\n√ï\nùë•‚ààùëãùëç#‚Ñì\nùúè‚Ñì¬∑ùëÄùëñùë•(ùë•)¬∑ùîºùë†ùúÉ‚àºùê¥(ùëç)ùêø(ùë†ùúÉ, ùë•) =\n√ï\n‚Ñì\nùúè‚Ñì¬∑\n√ï\nùë•‚ààùëãùëç#‚Ñì\nùëÄùëñùë•(ùë•)ùîºùë†ùúÉ‚àºùê¥(ùëç)ùêø(ùë†ùúÉ, ùë•)\nIn total, we have shown that\nerr(ùúã, ùê¥|ùëç) =\n√ï\n‚Ñì‚àà[ùëõ]\nùúè‚Ñì¬∑ errnùëç(ùê¥, ‚Ñì) + errunseen(ùúã, ùê¥|ùëç) .\nThis completes the proof.\n‚ñ°\nC.2.2\nProof of Lemma A.4\nProof of Lemma A.4. We have that the ùëñ-th noisy example can be written as ùë•ùëñ\nùë°=\nq\n1 ‚àíùúé2\nùë°ùë•ùëñ\n0 + ùúéùë°ùëßùëñ\nùë°.\nLet us set ùúéùë°= ùëú(1/‚à•ùë•0‚à•)) = poly(1/ùëë). Using Taylor‚Äôs approximation for\n‚àö\n1 ‚àíùë•around ùë•= 0\n(\n‚àö\n1 ‚àíùë•= 1 ‚àíùë•/2 ‚àíùëú(ùë•)) , we can write\n‚à•ùë•ùëñ\nùë°‚àí(1 ‚àípoly(1/ùëë))ùë•0 ‚àípoly(1/ùëë)ùëßùëñ\nùë°‚à•‚â§ùúÄ,\nfor some ùúÄ= poly(1/ùëë) sufficiently small. This means that\n‚à•ùë•ùëñ\nùë°‚àíùë•0‚à•‚â§ùúÄ+ poly(1/ùëë)‚à•ùë•0‚à•+ poly(1/ùëë)‚à•ùëßùëñ\nùë°‚à•\nBy Gaussian concentration, we have that\n‚Ñôùëßùëñ\nùë°[‚à•ùëßùëñ\nùë°‚à•>\n‚àö\nùëë] ‚â§exp(‚àíùëë/2) .\nLet us define the bad event ùê∏ùëöwhich corresponds to ‚Äùsubpopulation ùëódoes not have a poly(1/ùëë)-\nsmoothed single representative in the set {ùë•ùëñ\nùë°}ùëñ‚àà[ùëö] for ùúéùë°= poly(1/ùëë)‚Äù. A union bound over the ùëö\nnoisy examples gives that\n‚Ñôùëß1\nùë°,...,ùëßùëö\nùë°[ùê∏ùëö] ‚â§ùëö¬∑ exp(‚àíùëë/2) .\n‚ñ°\n29\n\n\nC.2.3\nProofs of Lemma A.5 and Lemma A.6\nProofs of Lemma A.5 and Lemma A.6. The proof relies on the fact that when the total variation\ndistance between two identity-covariance Gaussians is smaller than an absolute constant, then the\ntotal variation is up to constants characterized by the distance between the means [AAL23]. When\nthe original total variation is at most 1/600, [AAL23] shows that\nTV(ùí©(ùúá, ùêº), ùí©(ùúá‚Ä≤, ùêº)) = Œò(‚à•ùúá‚àíùúá‚Ä≤‚à•) .\nThe lemmas follow by noting that the densities of ùí©(ùúáùëñ, ùêº), ùí©(ùúá‚Ä≤\nùëñ, ùêº) at noise scale ùúéùë°(denoted as\nùí©ùë°, ùí©‚Ä≤\nùë°) satisfy\nùí©ùë°= ùí©(\nq\n1 ‚àíùúé2\nùë°ùúá, ùêº),\nùí©‚Ä≤\nùë°= ùí©(\nq\n1 ‚àíùúé2\nùë°ùúá‚Ä≤, ùêº) .\nSince\nq\n1 ‚àíùúé2\nùë°‚â§1, the means are contracting and so TV(ùí©ùë°, ùí©‚Ä≤\nùë°) ‚â§1/600. Also:\nTV(ùí©ùë°, ùí©‚Ä≤\nùë°) ‚â§‚à•ùúáùë°‚àíùúá‚Ä≤\nùë°‚à•/\n‚àö\n2 =\nq\n1 ‚àíùúé2\nùë°¬∑ ‚à•ùúá‚àíùúá‚Ä≤‚à•/\n‚àö\n2 .\nIf we want to make this quantity at most ùúÄ, it suffices to take ùúéùë°‚â•\np\n1 ‚àí2(ùúÄ/‚à•ùúá‚àíùúá‚Ä≤‚à•)2.\nFor the other side, by [AAL23], TV(ùí©ùë°, ùí©‚Ä≤\nùë°) ‚â•‚à•ùúáùë°‚àíùúá‚Ä≤\nùë°‚à•/200 =\nq\n1 ‚àíùúé2\nùë°‚à•ùúá‚àíùúá‚Ä≤‚à•/200. (we assume\nthat the original variation is smaller than 1/600 and we contract it by adding noise). If this should\nbe at least 2ùúÄ, then it should be that ùúéùë°‚â§\np\n1 ‚àí2002(2ùúÄ/‚à•ùúá‚àíùúá‚Ä≤‚à•)2. This concludes the proof.\n‚ñ°\nD\nExperimental Details\nWe open-source our code: https://github.com/kulinshah98/memorization_noisy_data\nFor all of our experiments regarding unconditional generation, we use the Adam optimizer with\na learning rate of 0.0001, betas (0.9, 0.999), an epsilon value of 1e-8, and a weight decay of 0.01.\nThe model for FFHQ and CIFAR-10 is trained for 30,000 iterations with a batch size of 256 and\nthe model for Imagenet is trained for 512 batch size for 80,000 iterations. For experiments on the\nImagenet dataset, we train a class-conditional model.\nFor FFHQ and CIFAR-10 experiments, we randomly sample 300, 1000 and 3000 samples from the\ncomplete dataset to create the dataset with limited size. We use Tiny Imagenet dataset which\nconsists of 200 classes [LY15]. We sample 5 images randomly from each class to create a dataset\nconsisting of 1000. Similarly, we sample 15 images from each class to create a dataset consisting\nof 3000 images. For the unconditional and conditional generation experiments, we used with the\nimplementation of [KAAL22] and default parameters of the implementation.\nFor text-conditioned experiments, we use the implementation of [SSG+23] and implement additional\nbaseline [WLCL24] and our method in the implementation. Similar to previous works, we use\nLAION-10k dataset to train the stable diffusion v2 model for 100000 number of iterations using\nbatch size 16. We use the final checkpoint after the complete training to evaluate the memorization,\nclipscore and fidelity. For the text-conditioned experiments, we tried adding nature noise at noise\nscale {25, 50, 100} and chose the model with best image quality.\n30\n\n\nFigure 4: Images generated using a model trained with our method on 300 samples\nE\nImages Generated using our Method\nIn this section, we present various images generates using our method. The images can be found in\nFigures 4 to 6.\n31\n\n\nFigure 5: Images generated using a model trained with our method on 1000 samples\n32\n\n\nFigure 6: Images generated using a model trained with our method on 3000 samples\n33\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21278v1.pdf",
    "total_pages": 33,
    "title": "Does Generation Require Memorization? Creative Diffusion Models using Ambient Diffusion",
    "authors": [
      "Kulin Shah",
      "Alkis Kalavasis",
      "Adam R. Klivans",
      "Giannis Daras"
    ],
    "abstract": "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}