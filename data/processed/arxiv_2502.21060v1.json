{
  "id": "arxiv_2502.21060v1",
  "text": "Efficient Transformer-based Decoder for Varshamov-Tenengolts Codes\nYali Wei,∗Alan J.X. Guo,† and Yufan Dai‡\nCenter for Applied Mathematics, Tianjin University,Tianjin, China\nZihui Yan§\nFrontiers Science Center for Synthetic Biology and Key Laboratory of Systems Bioengineering (Ministry of Education),\nSchool of Chemical Engineering and Technology, Tianjin University, China\n(Dated: March 3, 2025)\nIn recent years, the rise of DNA data storage technology has brought significant attention to the\nchallenge of correcting insertion, deletion, and substitution (IDS) errors. Among various coding\nmethods for IDS correction, Varshamov-Tenengolts (VT) codes, primarily designed for single-error\ncorrection, have emerged as a central research focus. While existing decoding methods achieve high\naccuracy in correcting a single error, they often fail to correct multiple IDS errors. In this work,\nwe observe that VT codes retain some capability for addressing multiple errors by introducing a\ntransformer-based VT decoder (TVTD) along with symbol- and statistic-based codeword embed-\nding. Experimental results demonstrate that the proposed TVTD achieves perfect correction of a\nsingle error. Furthermore, when decoding multiple errors across various codeword lengths, the bit\nerror rate and frame error rate are significantly improved compared to existing hard decision and\nsoft-in soft-out algorithms. Additionally, through model architecture optimization, the proposed\nmethod reduces time consumption by an order of magnitude compared to other soft decoders.\nINTRODUCTION.\nDNA has recently attracted widespread attention as a\nlong-term and high-density data storage medium [1–4].\nAn important challenge in DNA information encoding\nis the correction of various errors generated during DNA\nsynthesis and sequencing, particularly insertion, deletion,\nand substitution (IDS) errors [5–7].\nThese IDS errors\nnot only affect data integrity but also potentially com-\npromise the reliability of storage and retrieval processes.\nTherefore, effectively addressing this issue has become a\nsignificant research topic in the field of DNA data stor-\nage.\nTo correct IDS errors, several coding methods have\nbeen adopted, including convolutional codes [8–10], wa-\ntermark codes [11], time-varying block codes [12], and\nVarshamov-Tenengolts (VT) codes [13].\nAmong these\nmethods, VT codes are considered an effective error cor-\nrection scheme for DNA storage due to their asymptotic\noptimality. Specifically, VT codes require only ⌈log n⌉+1\nredundant bits to correct a single IDS error, making them\nhighly efficient in theory. When a single IDS error occurs\nin VT codewords, traditional hard-decision (HD) decod-\ning methods can effectively restore the original informa-\ntion [13–15]. However, in practical applications, multiple\nrandom IDS errors may be introduced, causing a signif-\nicant deterioration in the performance of hard-decision\ndecoding, thus severely affecting decoding accuracy and\nefficiency.\nTo address this problem, Yan et al. proposed a soft-in\nsoft-out (SISO) decoding algorithm [16], which improves\ndecoding performance by incorporating probabilistic in-\nformation. Compared to hard-decision decoding, SISO\nmethods offer better robustness and error correction ca-\npability when handling multiple errors. However, despite\nthe improvements of SISO methods, they still face bot-\ntlenecks in computational accuracy and speed when pro-\ncessing a large number of errors, particularly when deal-\ning with longer codewords.\nTo overcome these limitations, this paper proposes\na transformer-based VT decoder (TVTD) . The trans-\nformer architecture ([17]) can effectively capture complex\nrelationships between input data, enabling it to better\nhandle multiple IDS errors. Experimental results show\nthat, compared to traditional decoding methods, TVTD\ncan effectively decode multiple errors and accelerate the\ndecoding processes through parallel computing, thereby\nsignificantly improving decoding efficiency. Specifically,\nthe main contributions of this paper are as follows:\n• The first deep learning-based binary VT code de-\ncoder: We use symbol- and statistic-based code-\nword embedding as positional encoding, and lever-\nage the self-attention and cross-attention mecha-\nnisms in the transformer decoder to study the rela-\ntionships between bits in VT codewords for efficient\ndecoding.\n• Significant improvement in decoding performance:\nCompared to existing methods, TVTD achieved\nthe best experimental results across various code-\nword lengths,\nespecially with long codewords.\nSpecifically, as the codeword length varies, the bit\nerror rate (BER) decreased by 2% to 20%, and the\nframe error rate (FER) decreased by 20% to 86%.\narXiv:2502.21060v1  [cs.LG]  28 Feb 2025\n\n\n2\n• Faster computational speed:\nBy optimizing the\nmodel, TVTD accelerates training speed by 40%\ncompared to the full transformer models. Addition-\nally, in practical tests, the decoding speed of TVTD\nincreased by over 5 times compared to SISO decod-\ning, and for long codewords, the decoding speed\nimproved by over 46 times, greatly enhancing its\npotential for large-scale data storage applications.\nRELATED WORK.\nThe decoding of VT codewords with IDS errors has\nlong been a central research topic in the field of in-\nformation coding, particularly in terms of decoding ac-\ncuracy and efficiency.\nTo address this issue, N. J. A.\nSloane et al.\nproposed an effective decoding method\nbased on the weights and checksums of the received se-\nquence. This method accurately locates errors by evalu-\nating the weights of the received sequence and combin-\ning the checksums for correction, enabling the effective\ndetection and correction of single IDS errors [13]. How-\never, when multiple errors occur in VT codewords, this\nHD decoding method experiences a significant decline in\naccuracy.\nTo address this problem, Yan et al. proposed an im-\nproved algorithm based on the SISO decoding strategy\n[16]. This method relies on the Bahl-Cocke-Jelinek-Raviv\n(BCJR) algorithm and adopts the bitwise maximum a\nposteriori (MAP) decoding strategy. Compared to tra-\nditional hard decision decoding, the SISO method firstly\ntackled multiple errors correction.\nHowever, when the\nnumber of errors increases, the decoding accuracy de-\nclines.\nMoreover, the high computational complexity\nmakes it difficult to be applied in real practice.\nError Correcting Codes (ECC) improve data reliability\nby adding redundant bits, enabling the detection and cor-\nrection of errors during transmission or storage. Common\nECCs include Hamming code, BCH code, LDPC code,\nand Turbo code, which are widely used to ensure data\nintegrity [18].\nIn recent years, the rapid development\nof deep learning technology has brought revolutionary\nbreakthroughs to the field of deep learning-based ECC,\nwhere Belief Propagation (BP) decoders have gradually\nbecome the main solution [19]. Compared to traditional\ndecoding methods, such BP decoders based on neural\nnetworks not only improve decoding efficiency but also\nsignificantly enhance the systems robustness to noise in-\nterference [20, 21]. Considering the outstanding perfor-\nmance of transformer models in various tasks [17], they\nhave also been gradually introduced into the research\nof ECC. Choukroun et al.\nadvanced this field by uti-\nlizing transformer models for decoding linear codewords\n[22–24]. Their research demonstrated that transformer\nmodels not only improve decoding accuracy but also re-\nduce the computational complexity of decoding, signifi-\ncantly surpassing existing state-of-the-art decoding tech-\nnologies.\nHowever, despite the success of transformer models in\nlinear codes, their application in VT code decoding re-\nmains a relatively unexplored research area.\nBACKGROUND\nIn this study, we focus on decoding binary VT code-\nwords. The following provides background on VT codes\nand related decoding algorithms that correct IDS errors.\nVT code\nThis study focuses on the original structure of VT\ncodes, which are a class of algebraic block codes com-\nposed of all binary vectors of length n that satisfy the\nfollowing condition:\nVTa,m(n) =\n\b\nv ∈{0, 1}n :\nn\nX\ni=1\nivi ≡a\n(mod m)\n\t\n. (1)\nHere, m is a predefined integer, and a is an integer satis-\nfying 0 ≤a ≤m−1, typically referred to as the parity of\nthe sequence v. When m ≥n + 1, the code can correct\na deletion or insertion error, and when m ≥2n, it can\ncorrect an IDS error [13]. In this work,the VT codes that\ncorrect IDS errors (m = 2n + 1) are investigated.\nVT encoder\nA systematic encoding scheme for VT codes [25], re-\nferred to as the VT encoder, is summarized as follows:\nFor any message sequence u = {u1, u2, . . . , up} ∈{0, 1}p,\nthe VT encoder inserts these information bits into a code-\nword v = V T(u) ∈V Ta,2n+1(n), where y = n−⌈log n⌉−\n1. The encoder inserts parity check bits at binary posi-\ntions v2i, for 0 ≤i ≤n −y −2, and vn. The parities are\ngenerated with the message symbols to ensure:\nn\nX\ni=1\nivi ≡a\n(mod 2n + 1).\n(2)\nExample 1. Given a message sequence u = 11011 with\na fixed parity check a = 0, we have n = 10, y = 5, and\nm = 21. The codeword v = (v1, v2, . . . , v10) must satisfy:\nP10\ni=1 ivi ≡P4\ni=1 2i−1v2i−1+1·3+1·5+0·6+1·7+1·9+10·\nv10 ≡0 (mod 21), we have P4\ni=1 2i−1v2i−1 +10·v10 = 18.\nThis means v10 = 1 and v8 = 1, and v1 = v2 = v4 = 0.\nBy expanding 18−10 = 8 into binary form 1·23, the final\ncodeword is ¯0¯01¯0101¯11¯1, where the overlined positions are\nparity check bits.\n\n\n3\nHard decision decoder\nThe HD decoder is used to detect and correct a single\nerror in the codeword. The basic steps are as follows:\n• For a received sequence is r = (r1, r2, . . . , rN), the\nparity check condition a = PN\ni=1 i·ri (mod 2n+1)\nis checked.\nIf this condition is not satisfied, the\ncodeword is corrupted.\n• The received sequence r is a corrupted form\nof the original sequence,\nrepresented as r\n=\n[v1, . . . , vt−1, p, vt, . . . , vn], where v1, . . . , vt−1 is the\npart before the error, vt, . . . , vn is the part after\nthe error, and p is the error symbol. The difference\nbetween a and a is calculated as:\na −a ≡\nn\nX\ni=j\nvi + j · p\n(mod 2n + 1).\n(3)\nThis difference is then used to correct the error.\n• If a −a ≤PN\ni=1 ri, we deduce that p = 0, and\nthis insertion occurs before the (a −a)-th ”1” in\nthe sequence, counted from back to front. Other-\nwise, the insertion is ”1”, and its position is after\nthe (a −a −PN\ni=1 ri + 1)-th ”0” in the sequence.\nThe handling of substitution and deletion errors is\nsimilar to that of insertion errors.\nSISO decoder\nIn the SISO algorithm, the core idea is inspired by\nbit-by-bit optimal decoding methods based on concate-\nnated structure codes (e.g., BCJR algorithm).\nBased\non the bit-level MAP criterion, the goal is to calcu-\nlate the posterior probability (APP) of a specific bit\nv = (v1, v2, . . . , vn) being 0 or 1, given the received word\nr = (r1, r2, . . . , rN). Specifically, the state St is defined\nas the aggregate value at time t, representing the dic-\ntionary state and the drift value caused by deletion and\ninsertion errors, while Dt denotes the drift value, and dt\nrepresents the difference between insertion and deletion\nerrors.\nThe SISO decoder decodes both single and multiple\nerrors, the basic steps are as follows:\n• Given the received word and initial state, the\nMAP decoding criterion is represented by the fac-\ntor graph as follows:\nPr[vn\n1 |rN\n1 , S0 = s0, D0 = d0]Pr[rN\n1 |S0 = s0, D0 = d0]\n= Qn\nt=1 F[vt, rdt+t\ndt−1+t, st, dt],\n(4)\nwhere each factor is\nF[vt, rdt+t\ndt−1+t, st, dt]\n(5)\n= Pr[vt]Pr[rdt+t\ndt−1+t|vt, st, st−1, dt, dt−1]Pr[st, dt|st−1, dt−1].\nThis factorization is represented by a factor graph,\nwhere each factor corresponds to a node in the\ngraph.\n• To compute the posterior probability of the bit vt,\na SISO decoder based on the BCJR algorithm is\nused. Given the received word r, the log-posterior\nprobability (log-APP) of the bit being 0 or 1 can\nbe computed by the following formula:\nL(vt) = log Pr[vt = 0|r, S]\nPr[vt = 1|r, S]\n= log Pr[r, S|vt = 0]\nPr[r, S|vt = 1] + log Pr[vt = 0]\nPr[vt = 1].\n(6)\nwhere S represents the aggregated state at each\ntime step. Next, when calculating Pr[r, S|vt = b],\nthe following decomposition can be used:\nPr[r, S|vt = b] =\nX\n(d′,d)∈Dt,(s′,s)∈Pb\nt\nαt−1(s′, d′) · γt(s′, d′, s, d) · βt(s, d).\n(7)\nwhere αt−1(s′, d′)\n=\nPr[rd′+t−1\n1\n, s′, d′] is the\nmessage computed by the ”forward recursion”;\nβt(s, d) = Pr[rN\nd+t+1|s, d] is the message computed\nby the ”backward recursion”; γt(s′, d′, s, d) is the\nprobability computed based on the relationships\nbetween the received word, dictionary state, and\ndrift state.\n• To compute the above probabilities, αt(s, d) and\nβt(s, d) can be obtained through the following re-\ncursion relations:\nαt(s, d) =\nX\n(s′,s)∈P\nt,(d′,d)∈Dt\nγt(s′, d′, s, d) · αt−1(s′, d′).\nβt(s′, d′) =\nX\n(s′,s)∈P\nt+1,(d′,d)∈Dt+1\nγt+1(s′, d′, s, d) · βt+1(s, d).\n(8)\nwhere P\nt represents all possible dictionary states.\n• Using the above recursion relations and tran-\nsition formulas, the state transition probability\nγt(s′, d′, s, d) at each time step can be calculated\nusing the following formula:\nγt(s′, d′, s, d) = Pr[rd+t\nd′+t, s, d|s′, d′]\n= Pr[s|s′] · Pr[rd+t\nd′+t, d|s′, d′, s].\n(9)\nwhere\nPr[s|s′]\nrepresents\nthe\nconditional\nprobability\nbetween\ndictionary\nstates,\nand\nPr[rd+t\nd′+t, d|s′, d′, s] is the conditional probability of\nthe received word and drift state.\n• Once the log-posterior probability (log-APP) of the\nbit is computed, a hard decision can be made based\non the sign of L(vt):\nˆvt = sign(L(vt)).\n(10)\n\n\n4\nThen, the original information bits can be recov-\nered through the inverse operation.\nThe above methods provide the main model architec-\ntures for traditional HD and SISO decoding algorithms,\nlaying the theoretical foundation for our research.\nMETHOD\nIn this section, the overall framework, the symbol- and\nstatistic-based codeword embedding, and the masking\nstrategy of the attention mechanism are introduced.\nFramework\nAs shown in figure 1, the TVTD adopts a simplified\ntransformer-based sequence-to-sequence (seq2seq) archi-\ntecture [17].\nThe encoder of the seq2seq model is re-\nduced to the embedding of the corrupted codeword, en-\nhancing computational efficiency. Such embedding com-\nprises two parts: the symbol embedding and the statis-\ntic embedding of the corrupted codeword. The decoder\nfollows a next-symbol prediction scheme, which predicts\nthe transmitted/groundtruth codeword bit by bit using\nmulti-layered attention mechanisms with the corrupted\ncodeword embedding as memory.\nIn this process, the\nstandard upper triangular mask for next-symbol predic-\ntion is combined with a window mask, further reducing\nthe method’s complexity.\nDuring training, the teacher forcing strategy is em-\nployed, where the masked and shifted groundtruth code-\nword is used to predict the next symbol. The model is op-\ntimized using a cross-entropy loss function. In the testing\nphase, the model generates the codeword autonomously,\nusing predictions from previous iterations as input.\nSymbol and statistic-based codeword embedding\nThe embedding process for codewords is discussed. It\nis observed that using only the symbol embedding of a\ncodeword results in poor correction accuracy. Motivated\nby the HD decoder, the statistics of the symbol posi-\ntions are integrated into the codeword embedding in the\nproposed method.\nBoth received/corrupted codewords\nand transmitted/groundtruth codewords are embedded\nin the same manner. For each VT codeword, we perform\nthe following operations to help the model capture its\nfeature information.\nLet c = (c1, c2, . . . , cn), ci ∈{0, 1} be a codeword of\nlength n. The codeword embedding\nΦ(c) = concat(Φstat(c), Φsym(c))\n(11)\nis a concatenation of the symbol embedding Φsym(c) and\nthe statistic embedding Φstat(c).\nFIG. 1.\nFramework of the TVTD architecture.\nA seq2seq\nmodel is employed, where the encoder is simplified to the\ncodeword embedding of the corrupted codeword.\nThe de-\ncoder, on the other hand, adopts a next-symbol prediction\nscheme enhanced by a combined masking strategy.\nSymbol embedding of a codeword\nIn general, the embedding of sequences in a seq2seq\nmodel consists of word embeddings and positional em-\nbeddings, which are typically added or concatenated.\nThe positional embedding encodes the positional infor-\nmation of the words and can be either fixed or learnable.\nIn this work, due to the simplicity of the binary sym-\nbols in the codeword, the learnable positional embed-\nding is integrated into the symbol embedding. Specif-\nically, two learnable embedding vectors are associated\nwith each position of the codeword, corresponding to\nsymbol 0 and symbol 1 at that position.\nThe symbol\nembedding Φsym(c) = (ϕ1, ϕ2, . . . , ϕn) for codeword c is\nformulated as\nϕi = (1 −ci) ∗ei0 + ci ∗ei1,\n(12)\nwhere ei0 and ei1 are the learnable embedding vectors\nassociated with position i. An example is illustrated in\nfig. 2, where the embedding vectors are gathered based\non the symbols in the codeword.\nFIG. 2. Symbol embedding of a codeword. For a codeword\nc = (0, 1, 0, 1, 1), the vectors (e00, e11, e20, e31, e41) are gath-\nered to form the symbol embedding of the codeword.\n\n\n5\nStatistic embedding of a codeword\nThe VT code corrects IDS errors using redundancy in-\nformation based on joint symbol-position constraints, as\ndescribed in eq. (1) and section VT encoder . Motivated\nby this, the joint statistics of position and symbol are\nembedded as part of the final codeword embedding.\nMimicking eq. (2), the two statistics that sum the po-\nsition indices of symbol 0s and the position indices of\nsymbol 1s, respectively, are computed as\ns0 =\nn\nX\ni=1\ni · 1{ci=0},\n(13)\ns1 =\nn\nX\ni=1\ni · 1{ci=1},\n(14)\nwhere 1X denotes the indicator function of set X.\nTwo lookup tables ϕ0, ϕ1 are used to store the em-\nbedding vectors for the possible values of the two statis-\ntics, respectively. The statistic embedding of the code-\nword is obtained by retrieving the embedding vectors\nΦstat = (ϕ0(s0), ϕ1(s1)) from the tables according to s0\nand s1, respectively.\nCombined masking strategy\nAn upper triangular mask is commonly used in the\ntraining phase of a transformer in a seq2seq task to pre-\nvent the model from “seeing” the future context to be\npredicted.\nTo improve the efficiency of the proposed\nmodel, a combined mask, consisting of both the triangu-\nlar mask and a window mask, is employed during train-\ning. In this setup, the model predicts the next symbol\nbased on the corrupted codeword and the sliding window\nsegment of the predicted codeword.\nUpper triangular masking\nThe upper triangular mask Mupper has a size of L × L,\nwhere L is the length of the codeword embedding. The\nmask is defined as\nMupper(k, j) =\n(\n0,\nif k ≤j,\n−∞,\nif k > j,\n(15)\nwhere the k and j represent the row and column indices\nof the attention matrix, respectively.\nWindow masking\nThe concept of performing attention within a local win-\ndow was first introduced in the Swin Transformer [26],\nwhich was designed as a backbone for computer vision\ntasks.\nIn this work, the window mask Mwindow, with a size of\nL×L, restricts each position’s attention to a local context\nwithin a window size w. Its definition is as follows:\nMwindow(k, j) =\n(\n0,\nif |k −j| ≤w,\n−∞,\notherwise.\n(16)\nBy controlling the window size w, the information used to\npredict the next symbol of the groundtruth codeword can\nbe varied, achieving a balance between time consumption\nand model performance.\nCombining the masks\nDuring the training phase, the combined mask applied\nin the attention calculation is the sum of the upper tri-\nangular mask and the window mask.\nSpecifically, the\nattention is calculated as\nAH(Q, K, V ) = Softmax\n\u0012QKT\n√\nd\n+ M\n\u0013\nV,\n(17)\nwhere the mask M\n=\nMupper + Mwindow, and the\n(Q, K, V ) ∈RL×d are the query, key, and value matri-\nces of the attention mechanism [17].\nDuring the testing phase, the local segment c[−w :] of\nthe codeword is used, along with the corrupted codeword\nembedding, to predict the next symbol in each iteration.\nEXPERIMENTAL EVALUATION\nExperimental setup\nTo ensure that the experimental results are widely\nrepresentative, experiments were conducted with code-\nwords of three lengths: 20-bit (short codewords), 68-bit\n(medium codewords), and 120-bit (long codewords). The\nlearning rate was set to 0.0001 and adjusted using a co-\nsine annealing scheduler [27].\nThe model was trained\nfor 200 epochs.\nThe embedding dimension was set to\n512, with the feed-forward network dimension being four\ntimes the embedding dimension [28]. The decoder con-\nsists of N = 3 transformer layers, each with 8 attention\nheads. A quarter of the codeword length was chosen as\nthe window size for medium and long codewords, while\nthe window size is 20 for short codewords. The source\ncode is uploaded in the Supplementary Material, and will\nbe made publicly accessible upon the manuscript’s pub-\nlication.\nFor the experiments with 20-bit codewords, 80% of the\ncodewords and their corrupted counterparts were selected\nas the training set, while the remaining 20% were used as\nthe testing set. For medium and long codewords, since\n\n\n6\nthe number of codewords is numerous, 240, 000 (resp.\n60, 000) random codewords were used for training (resp.\ntesting).\nMetrics\nTo comprehensively evaluate the performance of the\nproposed method, both the Bit Error Rate (BER) and\nFrame Error Rate (FER) are employed. The BER repre-\nsents the fraction of erroneous bits in the decoded code-\nwords, while the FER is the ratio of codewords corrected\nin error to the total number of corrupted codewords re-\nceived. It is worth noting that since the codewords are\nbinary and randomly generated, the BER cannot exceed\n50%.\nComparison experiments\nExperiments were conducted across two methods, in-\ncluding HD and SISO, for comparison. Metrics were col-\nlected under two tasks: correcting a fixed number of er-\nrors and correcting errors independently introduced by a\nfixed error rate.\nResults on correcting fixed number of errors\nTo evaluate the performance of the compared ap-\nproaches in correcting multiple errors, fixed numbers of\n1, 2, 3, or 4 IDS errors were introduced to each code-\nword before correction, with each type of IDS error ran-\ndomly distributed. For each test involving n IDS errors\nof TVTD, the model was trained to correct a random\nnumber of errors ranging from 0 to n, and tested to cor-\nrect n IDS errors. The experimental results are presented\nin table I.\nAs shown in table I, all three compared approaches\ndemonstrate satisfactory performance when correcting a\nsingle IDS error.\nThis is expected, as the VT code is\nspecifically designed for single IDS error correction. How-\never, when more than one error is introduced, the per-\nformance of HD and SISO declines sharply, while the\nproposed TVTD maintains commendable accuracy, out-\nperforming the other methods by a significant margin.\nThe FER of TVTD on multiple errors suggests that the\nVT code for correcting IDS errors may not be ‘optimal’,\nnor will 1-FER should be no more than 50%.\nIt was also observed that a higher number of errors\nleads to higher error rates, as expected.\nFurthermore,\nwhen the length of the code increases, the performance\ndegradation is mitigated as the number of errors grows.\nIt is conjectured that the ratio of errors in the codeword is\nmore closely related to the final performance, as a longer\ncode length results in a lower proportion of errors.\nResults on independently distributed errors\nIn some applications like DNA storage, IDS errors typ-\nically occur independently within the sequence, posing a\nchallenge for decoding methods.\nTo evaluate the per-\nformance of the proposed TVTD under such conditions,\nexperiments were conducted by introducing independent\nIDS errors to the codeword.\nThe error rates were set\nto 1%, 3%, and 5%, with each error type having equal\nprobability.\nThe experimental results are presented in\ntable II.\nFor short codewords like VT(20, 14), the likelihood of\nencountering multiple IDS errors in a codeword is much\nlower compared to the other two codes.\nAll three de-\ncoders effectively correct single IDS errors, as shown in\ntable I. As a result, the three approaches exhibit similar\nperformance on VT(20, 14).\nOn the other hand, since\nmultiple errors are likely to occur in a sequence of length\n68 or 120, the proposed method outperforms HD and\nSISO by a large margin when decoding VT(68, 60) and\nVT(120, 112). Specifically, when the channel error rate\nreaches 5%, HD and SISO fail to decode meaningful in-\nformation, while the TVTD still achieves more than 98%\nbit accuracy and 70% frame accuracy.\nAblation study\nThe ablation study and hyperparameter optimization\nwere also conducted to evaluate the effectiveness of the\nproposed methods. All the experiments in the ablation\nstudy are performed on correcting codes from VT(68, 60).\nWindow size\nThe window size w controls the size of the segment\nfrom which the model predicts the next symbol.\nIt\nalso affects the time consumption of the method. The\nBERs and FERs were recorded with different choices of\nwindow size w on correcting corrupted codewords from\nVT(68, 60) with fixed number of errors. The results are\nillustrated in table IV[29].\nAs suggested by table IV, the accuracies improve as the\nwindow size increases. For correcting 2 and 3 errors, the\nperformance reaches a plateau after w = 16. Meanwhile,\nfor correcting 4 errors, the plateau point occurs later,\nindicating that correcting multiple errors is more closely\nrelated to global dependencies within the sequence.\nStatistic embedding\nThe statistic embedding of the codeword, introduced\nas part of the codeword embedding in section Symbol and\n\n\n7\nVT(20, 14)\nVT(68, 60)\nVT(120, 112)\n# Errors\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nHD\n1-BER (%) 100.0 67.9 65.3 60.7 100.0 69.0 65.9 61.6 100.0 69.2 65.4 60.7\n1-FER (%) 100.0\n9.0\n8.9\n2.5 100.0\n3.1\n2.6\n0.2 100.0\n2.6\n2.2\n0.0\nSISO\n1-BER (%)\n99.9 82.1 75.4 72.4 100.0 86.3 83.6 76.5 100.0 85.4 79.7 77.6\n1-FER (%)\n97.9 13.1\n5.0\n1.2\n99.6\n4.6\n1.4\n0.1 100.0\n1.7\n0.1\n0.0\nTVTD 1-BER (%) 100.0 83.7 76.5 71.9\n99.9 98.3 98.2 97.0 100.0 99.8 99.5 99.3\n1-FER (%)\n99.9 32.5 13.6\n6.4\n99.0 88.0 76.7 63.3\n99.3 91.7 83.0 75.3\nTABLE I. Performance of different VT decoders: HD, SISO, and TVTD, at various numbers of errors. The VT codes with\ndifferent code/message lengths VT(20, 14), VT(68, 60), and VT(120, 112) were tested.\nVT(20, 14)\nVT(68, 60)\nVT(120, 112)\nError Rate\n1%\n3%\n5%\n1%\n3%\n5%\n1%\n3%\n5%\nHD\n1-BER (%)\n99.6 97.0 93.7 96.5 85.3 73.6 92.4 72.9 61.7\n1-FER (%)\n98.9 90.3 80.9 89.0 52.1 22.2 73.4 20.0\n4.8\nSISO\n1-BER (%)\n99.7 97.7 95.2 97.7 87.9 83.9 94.9 80.0 71.6\n1-FER (%)\n98.1 89.1 77.4 85.5 33.9 10.5 70.1\n6.0\n0.3\nTVTD 1-BER (%)\n99.8 97.7 94.8 99.8 98.9 98.2 99.9 99.3 98.6\n1-FER (%)\n98.9 90.3 79.1 97.5 86.1 70.3 96.9 81.0 67.4\nTABLE II. Performance of different VT decoders: HD, SISO, and TVTD, at various error rates. The VT codes with different\ncode/message lengths VT(20, 14), VT(68, 60), and VT(120, 112) were tested.\nstatistic-based codeword embedding , is evaluated in ab-\nlation studies, as shown in table V[30]. In this table, the\ncolumn headers like “w/w” indicate whether the statistic\nembedding is applied to the embedding of the corrupted\ncodeword and the predicted codeword.\nFor instance,\n“wo/w” means that the embedding of the corrupted code-\nword is performed without the statistic embedding, while\nthe model is trained and tested to predict the code-\nword with the statistic embedding. Section Symbol and\nstatistic-based codeword embedding suggests that intro-\nducing the statistic embedding into the input codeword\nembedding significantly improves the frame accuracies.\nMeanwhile, in the case of “wo/wo”, where the statistic\nembedding is absent, the model’s performance degener-\nates by a large margin.\nDifferent encoder of the seq2seq architecture\nA commonly used seq2seq model consists of the en-\ncoder and decoder, which are both stacks of transformer\nlayers. In this work, direct embedding of the corrupted\ncodeword is engaged for the cross-attentions to predict\nthe groundtruth codeword. Experiments were conducted\nto find out whether such a setting declines the perfor-\nmance while improving the complexity efficiency.\nLet TVTD(0+3) be the proposed default architec-\nture which uses codeword embedding directly as the\nencoder memory and 3-layered decoder to predict the\ngroundtruth codeword, while TVTD(0+3) denotes the\nseq2seq model with 3-layered encoder and 3-layered de-\ncoder. The results are illustrated in table III. It is sug-\ngested that using more complex architecture results in\nslight performance improvement. However, it is also ob-\nserved that the model slows down by about 44% to 66%\ncomparing to the simple model.\nComplexity\nIn the TVTD model, the key sources of computational\ncomplexity come from the cross-attention, self-attention,\nand feedforward networks. The complexity of the cross-\nattention is O(ℓ2d), where ℓis the length of the target\nand source sequences, and d is the embedding dimension.\nThe complexity of the self-attention is O(ℓwd), where w\nis the window size, typically smaller than the target se-\nquence length ℓ.\nThe feedforward network’s computa-\ntional complexity is O(ℓd2), indicating the computation\ncost for each target position.\nOverall, the complexity\nof the decoder can be expressed as O(ℓ2d + ℓwd + ℓd2).\nTherefore, the computational complexity of the model is\nmainly influenced by the target sequence length, source\nsequence length, and embedding dimension, with the\ncross-attention computation being the primary bottle-\nneck.\nAlthough the typical transformer has a complexity of\nO(ℓ2), the TVTD still benefits from parallel computa-\ntion on a GPU. The time consumed for correcting 10, 000\ncorrupted codewords with 2 errors was recorded. The re-\nsults are as follows: HD (Xeon): 7.6s @ VT(68, 60); SISO\n(Xeon): 15747.4s @ VT(20, 14); and TVTD (NVIDIA\n\n\n8\nVT(20, 14)\nVT(68, 60)\nVT(120, 112)\n# Errors\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nTVTD(3+3) 1-BER(%) 100.0 83.9 76.3 72.2 99.9 99.3 98.6 97.7\n99.9 99.8 99.7 99.4\n1-FER(%)\n100.0 34.5 15.8\n7.1 99.7 90.5 80.5 67.8\n99.0 93.0 87.0 77.3\nTVTD(0+3) 1-BER (%) 100.0 83.7 76.5 71.9 99.9 98.3 98.2 97.0 100.0 99.8 99.5 99.3\n1-FER (%)\n99.9 32.5 13.6\n6.4 99.0 88.0 76.7 63.3\n99.3 91.7 83.0 75.3\nTABLE III. Performance of TVTD with different seq2seq encoder architectures. TVTD(3+3) uses a 3-layered transformer as\nthe seq2seq encoder, while TVTD(0+3) is the default setup using direct embedding as the encoder.\n# Error Metric\nw = 1 w = 2 w = 4 w = 8 w = 16 w = 32\nall\n1 1-BER (%) 100.0 100.0 100.0 100.0\n100.0\n100.0 100.0\n1-FER (%)\n100.0 100.0\n99.8 100.0\n100.0\n100.0 100.0\n2 1-BER (%)\n99.1\n99.1\n99.2\n99.2\n99.2\n99.2\n99.2\n1-FER (%)\n78.7\n81.4\n84.2\n85.4\n87.0\n86.9\n87.4\n3 1-BER (%)\n97.9\n97.8\n98.0\n98.1\n98.2\n98.2\n98.2\n1-FER (%)\n57.4\n60.4\n66.9\n69.5\n73.8\n74.7\n74.7\n4 1-BER (%)\n96.5\n96.6\n96.5\n96.6\n96.7\n96.9\n96.9\n1-FER (%)\n35.2\n43.7\n44.1\n47.8\n51.5\n61.4\n61.5\nTABLE IV. Performance of TVTD correcting fixed number of errors with various window sizes w ranging from 1 to the whole\nthe sequence.\n# Error Metric\nw/w w/wo wo/w wo/wo\n1 1-BER (%) 100.0 100.0\n99.9\n99.7\n1-FER (%) 100.0 100.0\n97.3\n89.4\n2 1-BER (%)\n99.2\n99.2\n98.8\n98.7\n1-FER (%)\n87.7\n87.3\n73.5\n73.1\n3 1-BER (%)\n98.2\n98.2\n98.1\n97.7\n1-FER (%)\n73.4\n72.9\n71.2\n62.2\n4 1-BER (%)\n96.8\n96.8\n96.9\n96.6\n1-FER (%)\n51.6\n52.3\n57.2\n47.4\nTABLE V. Performance metrics for different methods at var-\nious error rates across different VT codes.\nA40): 80.7s @ VT(68, 60).\nThese results suggest that\nTVTD consumes significantly less time than the other\nsoft decoder, SISO, and requires about 10 times more\ntime than HD to achieve the capability of multiple error\ncorrection.\nCONCLUSION\nIn this paper, a transformer-based binary VT code de-\ncoder was proposed, marking the first neural network im-\nplementation of a VT decoder. Within the TVTD frame-\nwork, symbol- and statistic-based codeword embeddings,\nand a combined masking strategy were introduced to en-\nhance model performance and efficiency. Experimental\nresults demonstrated that TVTD significantly outper-\nforms HD and SISO, particularly in its capacity to cor-\nrect multiple errors, which beyonds the original design\nscope of VT codes. Abaltion studies further validated\nthe effectiveness of the proposed techniques.\n∗yaliwei222˙˙@tju.edu.cn\n† jiaxiang.guo@tju.edu.cn\n‡ daiyufan@tju.edu.cn\n§ yanzh@tju.edu.cn\n[1] N. Goldman, P. Bertone, S. Chen, C. Dessimoz, E. M.\nLeProust, B. Sipos, and E. Birney, Towards practical,\nhigh-capacity, low-maintenance information storage in\nsynthesized dna, Nature 494, 77 (2013).\n[2] Y. Erlich and D. Zielinski, Dna fountain enables a ro-\nbust and efficient storage architecture, Science 355, 950\n(2017).\n[3] M. A. Sini and E. Yaakobi, Reconstruction of sequences\nin dna storage, in 2019 IEEE International Symposium\non Information Theory (ISIT) (IEEE, 2019) pp. 290–294.\n[4] L. Song, F. Geng, Z.-Y. Gong, X. Chen, J. Tang,\nC. Gong, L. Zhou, R. Xia, M.-Z. Han, J.-Y. Xu, B.-\nZ. Li, and Y.-J. Yuan, Robust data storage in dna by\nde bruijn graph-based de novo strand assembly, Nature\nCommunications 13, 5361 (2022).\n[5] S. M. H. T. Yazdi, H. M. Kiah, E. Garcia-Ruiz, J. Ma,\nH. Zhao, and O. Milenkovic, Dna-based storage: Trends\nand methods, IEEE Transactions on Molecular, Biologi-\ncal and Multi-Scale Communications 1, 230 (2015).\n[6] R. Heckel, G. Mikutis, and R. N. Grass, A characteriza-\ntion of the dna data storage channel, Scientific Reports\n9, 1 (2019).\n[7] X. Li, M. Chen, and H. Wu, Multiple errors correction\nfor position-limited dna sequences with gc balance and\nno homopolymer for dna-based data storage, Briefings in\nBioinformatics 24, bbac484 (2023).\n\n\n9\n[8] I. Maarouf,\nA. Lenz,\nL. Welter,\nA. Wachter-Zeh,\nE. Rosnes, and A. G. i. Amat, Concatenated codes for\nmultiple reads of a dna sequence, IEEE Transactions on\nInformation Theory , 1 (2022).\n[9] V. Buttigieg and N. Farrugia, Improved bit error rate\nperformance of convolutional codes with synchronization\nerrors, in 2015 IEEE International Conference on Com-\nmunications (ICC) (2015) pp. 4077–4082.\n[10] W. Press, J. Hawkins, S. Jones, J. Schaub, and I. Finkel-\nstein, Hedges error-correcting code for dna storage cor-\nrects indels and allows sequence constraints, Proceedings\nof the National Academy of Sciences 117, 18489 (2020).\n[11] M. Davey and D. Mackay, Reliable communication over\nchannels with insertions, deletions, and substitutions,\nIEEE Transactions on Information Theory 47, 687\n(2001).\n[12] V. Buttigieg, S. Wesemeyer, and J. Briffa, Time-varying\nblock codes for synchronisation errors: maximum a pos-\nteriori decoder and practical issues, The Journal of En-\ngineering 2014, 1 (2014).\n[13] V. I. Levenshtein, Binary codes capable of correcting\ndeletions, insertions, and reversals, Soviet physics. Dok-\nlady 10, 707 (1965).\n[14] R. Varˇsamov and G. Tenengolts, A code which cor-\nrects single asymmetric errors, Avtomat. i Telemeh 26,\n4 (1965).\n[15] K. Cai, Y. M. Chee, R. Gabrys, H. M. Kiah, and\nT. T. Nguyen, Correcting a single indel/edit for dna-\nbased data storage:\nLinear-time encoders and order-\noptimality, IEEE Transactions on Information Theory\n67, 3438 (2021).\n[16] Z. Yan, G. Qu, and H. Wu, A novel soft-in soft-out de-\ncoding algorithm for vt codes on multiple received dna\nstrands, in 2023 IEEE International Symposium on In-\nformation Theory (ISIT) (IEEE, Taipei, Taiwan, 2023)\npp. 1–6.\n[17] A. Vaswani,\nN. Shazeer,\nN. Parmar,\nJ. Uszkoreit,\nL. Jones, A. N. Gomez,  Lukasz Kaiser, and I. Polosukhin,\nAttention is all you need, in Advances in Neural Infor-\nmation Processing Systems (2017) pp. 5998–6008.\n[18] H. Kim, Error correction codes, in Wireless Communica-\ntions Systems Design (Wiley Telecom, 2015) Chap. 139,\npp. 123–207.\n[19] E. Nachmani and L. Wolf, Hyper-graph-network decoders\nfor block codes, in Advances in Neural Information Pro-\ncessing Systems (2019) pp. 2326–2336.\n[20] L. Lugosch and W. J. Gross, Neural offset min-sum de-\ncoding, in 2017 IEEE International Symposium on In-\nformation Theory (ISIT) (IEEE, 2017) pp. 1361–1365.\n[21] E. Nachmani, Y. Be’ery, and D. Burshtein, Learning to\ndecode linear codes using deep learning, in 2016 54th An-\nnual Allerton Conference on Communication, Control,\nand Computing (Allerton) (IEEE, 2016) pp. 341–346.\n[22] Y. Choukroun and L. Wolf, Error correction code trans-\nformer, in Advances in Neural Information Processing\nSystems 35 (NeurIPS 2022) Main Conference Track\n(2022).\n[23] Y. Choukroun and L. Wolf, A foundation model for er-\nror correction codes, in Proceedings of the 2024 Inter-\nnational Conference on Learning Representations (ICLR\n2024) (2024) submitted to ICLR 2024.\n[24] Y. Choukroun and L. Wolf, Learning linear block error\ncorrection codes, in Proceedings of the 41st International\nConference on Machine Learning, PMLR, Vol. 235 (2024)\npp. 8801–8814.\n[25] K. Saowapa, H. Kaneko, and E. Fujiwara, Systematic\ndeletion/insertion error correcting codes with random er-\nror correction capability, in Proceedings 1999 IEEE In-\nternational Symposium on Defect and Fault Tolerance in\nVLSI Systems (EFT’99) (1999) pp. 284–292.\n[26] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, and Z. Zhang,\nSwin transformer: Hierarchical vision transformer using\nshifted windows, in 2021 IEEE/CVF International Con-\nference on Computer Vision (ICCV) (IEEE, 2021) pp.\n10012–10022.\n[27] I. Loshchilov and F. Hutter, SGDR: Stochastic gradient\ndescent with warm restarts, in International Conference\non Learning Representations (2017).\n[28] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing,\nH. Zhang, Y. Lan, L. Wang, and T.-Y. Liu, On layer\nnormalization in the transformer architecture, arXiv\npreprint arXiv:2002.04745 (2020).\n[29] This ablation study used smaller training and testing\nsets, consisting of 160, 000 and 40, 000 samples, respec-\ntively.\n[30] This ablation study used smaller training and testing\nsets, consisting of 160, 000 and 40, 000 samples, respec-\ntively.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21060v1.pdf",
    "total_pages": 9,
    "title": "Efficient Transformer-based Decoder for Varshamov-Tenengolts Codes",
    "authors": [
      "Yali Wei",
      "Alan J. X. Guo",
      "Zihui Yan",
      "Yufan Dai"
    ],
    "abstract": "In recent years, the rise of DNA data storage technology has brought\nsignificant attention to the challenge of correcting insertion, deletion, and\nsubstitution (IDS) errors. Among various coding methods for IDS correction,\nVarshamov-Tenengolts (VT) codes, primarily designed for single-error\ncorrection, have emerged as a central research focus. While existing decoding\nmethods achieve high accuracy in correcting a single error, they often fail to\ncorrect multiple IDS errors. In this work, we observe that VT codes retain some\ncapability for addressing multiple errors by introducing a transformer-based VT\ndecoder (TVTD) along with symbol- and statistic-based codeword embedding.\nExperimental results demonstrate that the proposed TVTD achieves perfect\ncorrection of a single error. Furthermore, when decoding multiple errors across\nvarious codeword lengths, the bit error rate and frame error rate are\nsignificantly improved compared to existing hard decision and soft-in soft-out\nalgorithms. Additionally, through model architecture optimization, the proposed\nmethod reduces time consumption by an order of magnitude compared to other soft\ndecoders.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}