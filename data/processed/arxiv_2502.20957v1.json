{
  "id": "arxiv_2502.20957v1",
  "text": "Published as a conference paper at ICLR 2025\nREWARD DIMENSION REDUCTION\nFOR SCALABLE\nMULTI-OBJECTIVE REINFORCEMENT LEARNING\nGiseung Park, Youngchul Sung ∗\nSchool of Electrical Engineering\nKorea Advanced Institute of Science and Technology (KAIST)\nDaejeon 34141, Republic of Korea\n{gs.park,ycsung}@kaist.ac.kr\nABSTRACT\nIn this paper, we introduce a simple yet effective reward dimension reduction\nmethod to tackle the scalability challenges of multi-objective reinforcement learn-\ning algorithms. While most existing approaches focus on optimizing two to four\nobjectives, their abilities to scale to environments with more objectives remain\nuncertain. Our method uses a dimension reduction approach to enhance learning\nefficiency and policy performance in multi-objective settings. While most tradi-\ntional dimension reduction methods are designed for static datasets, our approach\nis tailored for online learning and preserves Pareto-optimality after transformation.\nWe propose a new training and evaluation framework for reward dimension reduc-\ntion in multi-objective reinforcement learning and demonstrate the superiority of\nour method in environments including one with sixteen objectives, significantly\noutperforming existing online dimension reduction methods.\n1\nINTRODUCTION\nReinforcement Learning (RL) is a powerful machine learning paradigm focused on training agents\nto make sequential decisions by interacting with their environment. Through trial and error, RL\nalgorithms allow agents to iteratively improve their decision-making policies, with the ultimate goal\nof maximizing cumulative rewards. In recent years, the field of Multi-Objective Reinforcement\nLearning (MORL) has gained considerable attention due to its relevance in solving real-world con-\ntrol problems involving multiple, often conflicting, objectives. These problems span across domains\nsuch as advanced autonomous control (Weber et al., 2023), power system management, and logistics\noptimization (Hayes et al., 2022), where balancing trade-offs among competing objectives is crucial\n(Roijers et al., 2013).\nMORL extends the traditional RL framework by enabling agents to handle multiple objectives si-\nmultaneously. This requires methods capable of identifying and managing trade-offs among these\nobjectives. MORL specifically focuses on learning a set of policies that approximate the Pareto\nfrontier, representing solutions where no objective can be improved without compromising others.\nMost of the current approaches scalarize vector rewards into scalar objectives to generate a diverse\nset of policies (Abels et al., 2019; Yang et al., 2019; Xu et al., 2020; Basaklar et al., 2023; Lu et al.,\n2023), thereby avoiding the need for retraining during the test phase.\nAlthough these methods have proven effective in standard MORL benchmarks (Felten et al., 2023),\nmost benchmarks involve only two to four objectives, leaving open the question of whether existing\nMORL algorithms can scale effectively to environments with more objectives (Hayes et al., 2022).\nIndeed, various practical applications demand optimizing many objectives simultaneously (Li et al.,\n2015). For example, Fleming et al. (2005) introduced an example of optimizing a complex jet engine\ncontrol system that requires balancing eight physical objectives. In military contexts, a commander\nshould manage dozens of objectives that directly influence decision-making (Dagistanli & ¨Ust¨un,\n2023), including the positions of allies and enemies, casualty rates, combat capabilities of allies and\nenemies, and time estimations for achieving strategic goals. When planning for multiple potential\n∗Youngchul Sung is the corresponding author.\n1\narXiv:2502.20957v1  [cs.LG]  28 Feb 2025\n\n\nPublished as a conference paper at ICLR 2025\nbattle scenarios, exploring such high-dimensional objective space in its raw form is inefficient and\nvery challenging due to the complexity of the original space (Wang & Sebag, 2013).\nAn advantageous feature of many real-world MORL applications is that objectives often exhibit\ncorrelations, leading to inherent conflicts or trade-offs. For example, an autonomous vehicle must\nbalance safety and speed, where optimizing one can compromise the other. Similarly, traffic light\ncontrol must manage multiple interrelated objectives to ensure smooth traffic flow (Hayes et al.,\n2022). These correlations suggest that reducing the dimensionality of the reward space while pre-\nserving its essential features could be a viable strategy to make current MORL algorithms scalable\nto many objective spaces.\nDimension reduction techniques (Roweis & Saul, 2000; Tenenbaum et al., 2000; Zass & Shashua,\n2006; Lee et al., 2007; Cardot & Degras, 2018; McInnes & Healy, 2018; Bank et al., 2020), widely\nused in other machine learning domains, capture the most significant features of high-dimensional\ndata while filtering out irrelevant noise. However, typical approaches operate on static datasets,\nwhereas RL necessitates continuous data collection during online training. This introduces a unique\nchallenge: applying dimension reduction in MORL while retaining the essential structure of the\noriginal reward space. To our knowledge, few studies have addressed this challenge within the\nMORL context.\nIn this paper, we address these challenges by introducing a simple yet effective reward dimension\nreduction method that scales MORL algorithms to higher-dimensional reward spaces. We propose\na new training and evaluation framework tailored for the online reward dimension reduction setting.\nOur approach ensures that Pareto-optimality is preserved after transformation, allowing the agent to\nlearn and execute policies that remain effective in the original reward space.\nOur contributions are as follows. First, we propose a new training and evaluation framework for\nonline reward dimension reduction in MORL. We also derive conditions and introduce learning\ntechniques to ensure that online training and Pareto-optimality are maintained, providing a stable\nand efficient approach for scalable MORL. Lastly, our method demonstrates superior performance\ncompared to existing online dimension reduction methods in MORL environments including one\nwith sixteen objectives.\n2\nBACKGROUND\nA multi-objective Markov decision process (MOMDP) is defined by the tuple ⟨S, A, P, µ0, r, γ⟩.\nHere, S represents the set of states, A the set of actions, P the state transition probabilities, µ0\nthe initial state distribution, r the reward function, and γ ∈[0, 1) the discount factor. Unlike the\ntraditional single-objective MDP, the reward function r : S × A →RK in a MOMDP is vector-\nvalued, where K ≥2 is the number of objectives. This vector-valued nature of the reward function\nallows the agent to receive multiple rewards for each state-action pair, each corresponding to a\ndifferent objective.\nIn the context of MORL, the performance of a policy π is evaluated by its expected cumulative\nreward, denoted as J(π) = (J1(π), · · · , JK(π)) := Eπ [P∞\nt=0 γtrt] ∈RK. To compare vector-\nvalued rewards, we use the notion of Pareto-dominance (Roijers et al., 2013), denoted >P . For two\nvector returns, J(π) and J(π′), we have:\nJ(π′) >P J(π) ⇐⇒(∀i ∈{1, . . . , K}, Ji(π′) ≥Ji(π)) and (∃j ∈{1, . . . , K}, Jj(π′) > Jj(π)).\n(1)\nThis means that J(π′) Pareto-dominates J(π) if it is at least as good as J(π) in all objectives and\nstrictly better in at least one.\nThe goal of MORL is to identify a policy π whose J(π) lies on the Pareto frontier (or boundary) F\nof all achievable return tuples J = {(J1(π), · · · , JK(π)) | π ∈Π}, where Π denotes the set of all\npossible policies. The formal definition of the Pareto frontier 1 is as follows (Roijers et al., 2013;\n1Strictly speaking, the Pareto frontier can also be defined as the set of non-dominated policies, {π ∈Π |\n∄π′ s.t. J(π′) >P J(π)} (Hayes et al., 2022), rather than the set of non-dominated vector returns as shown in\nequation 2 . In this case, multiple policies may achieve the same vector return (Hayes et al., 2022). To avoid this\nredundancy, in this paper, we define the Pareto frontier and the convex coverage set as presented in equation 2\nand equation 3, respectively.\n2\n\n\nPublished as a conference paper at ICLR 2025\nYang et al., 2019):\nF = {J(π) | ∄π′ s.t. J(π′) >P J(π)}.\n(2)\nIn other words, no single policy achieving F can improve one objective without sacrificing at least\none other objective. Finding a policy achieving the Pareto frontier ensures an optimal balance among\nthe competing objectives with the best possible trade-offs.\nResearchers are also interested in obtaining policies that cover the convex coverage set (CCS) of a\ngiven MOMDP defined as follows (Yang et al., 2019):\nCCS = {J(π) | ∃ω ∈∆K s.t. ω⊤J(π) ≥ω⊤J(π′), ∀π′ ∈Π with J(π′) ∈F}\n(3)\nJ1\nJ2\nA\nB\nC\nD\nO\n𝐹= {A,B,C,D,E}\nCCS = {A,B,D,E}\n𝜔\nE\nC’\nD’\nOC’ < OD’\nFigure 1:\nComparison of the Pareto\nfrontier F and the CCS for K = 2,\nwhere C′ and D′ represent the projec-\ntions of points C and D onto the pref-\nerence vector ω, respectively.\nYellow\ndashed line represents the outer convex\nboundary of F.\nwhere ∆K is the (K−1)-simplex and ω ∈∆K represents\na preference vector that specifies the relative importance\nof each objective (i.e., PK\nk=1 ωk = 1, ωk ≥0, ∀k).\nFigure 1 illustrates the relationship between Pareto fron-\ntier and CCS. In Figure 1, we assume that the achievable\npoints {A, B, C, D, E} form the Pareto frontier. Then,\nfor the preference vector ω in Figure 1, the inner product\nbetween ω and the return vector at the point C is smaller\nthan the inner product between ω and the return vector at\nthe point D. The inner product between ω and the return\nvector at the point C is smaller than that between ω and\nany other point in the Pareto frontier. Hence, the point C\nis not included in the CCS. Note that the CCS represents\nthe set of achievable returns that are optimal for some lin-\near combination of objectives, and it is a subset of the\nPareto frontier F by definition. Since the weighted sum\nis widely used in real-world applications to express spe-\ncific preferences over multiple objectives (Hayes et al.,\n2022), CCS is a proper refinement of the Pareto frontier.\nIn the context of multi-policy MORL (Roijers et al.,\n2013), the goal is to find multiple policies that cover (an approximation of) either the Pareto fron-\ntier or the CCS so that during test phases, we perform well across various scenarios without having\nto retrain from scratch. Specifically, we aim to achieve Pareto-optimal points that maximize the\nhypervolume while minimizing the sparsity (Hayes et al., 2022).\nReference point\n0\nJ1\nJ2\nGreen dots:\nPareto frontier\nJ1\nJ2\nA\nB\nC\nD\nSP of {A,B,D} = 0.5(|AB|2 + |BD|2)\nSP of {A,C,D} = 0.5(|AC|2 + |CD|2)\n \n0\nFigure 2: Evaluation metrics in multi-policy MORL: hypervolume and sparsity. (Left) Hypervolume\nis represented by the pink area in the figure. (Right) The sparsity of the solution set {A, B, D} is\nlower than that of {A, C, D} when points C and D are close, indicating that {A, B, D} offers a\nmore diverse set of solutions than {A, C, D}.\nAs seen in the left figure of Figure 2, the hypervolume measures the volume in the objective\nspace dominated by the set of current Pareto frontier points and bounded by a reference point.\nIn the figure, the hypervolume corresponds to the area of the pink region.\nThis metric pro-\nvides a scalar value quantifying how well the policies cover the objective space. Formally, let\nX = {x1, · · · , xN} ⊂RK be a set of N Pareto frontier points and x0 ∈RK be a reference point,\n3\n\n\nPublished as a conference paper at ICLR 2025\nwhere xi = (xi1, . . . , xiK), i = 0, . . . , N. Then, the hypervolume metric HV (X, x0) is defined by\nthe K-dimensional volume of the union of hybercubes SN\ni=1 CK\nk=1[x0k, xik], where CK\nk=1[x0k, xik]\nis the hypercube of which side at the k-th dimension is given by the line segment [x0k, xik].\nAnother metric is sparsity, which assesses the distribution of policies within the objective space. As\nseen in the right figure of Figure 2, a set of Pareto frontier points with low sparsity ensures that the\nsolutions are well-distributed, offering a diverse range of trade-offs among the objectives. If there is a\nset of N Pareto frontier points X = {x1, · · · , xN} ⊂RK with xi = (xi1, . . . , xiK) (i = 1, . . . , N),\nsparsity is defined as:\nSP(X) :=\n1\nN −1\nK\nX\nk=1\nN−1\nX\ni=1\n(Sk[i] −Sk[i + 1])2\n(4)\nwhere Sk = Sort{xik, 1 ≤i ≤N} in descending order in the k-th objective, 1 ≤k ≤K. Given\na dimension k and its two endpoints, Sk[1] and Sk[N], the Cauchy–Schwarz inequality implies that\nPN−1\ni=1 (Sk[i] −Sk[i + 1])2 is minimized when the differences Sk[i] −Sk[i + 1] are constant for\nall 1 ≤i ≤N −1. Therefore, sparsity acts as an indicator of how well-distributed a set of return\nvectors is. Reducing sparsity while maintaining a high hypervolume helps avoid situations where\nonly a few objectives perform well. Therefore, considering both low sparsity and high hypervolume\noffers a more comprehensive evaluation criterion than relying solely on hypervolume.\n3\nRELATED WORK\nThere are mainly two branches in MORL. The first branch is single-policy MORL, where the goal is\nto obtain an optimal policy π∗= arg maxπ h(J(π)) where h : RK →R is a fixed non-decreasing\nutility function, mostly for non-linear one (Siddique et al., 2020; Park et al., 2024). The other branch\nis the multi-policy MORL, where we aim to acquire multiple policies that cover an approximation\nof the Pareto frontier or CCS. Beyond several classical methods such as iterative single-policy ap-\nproaches (Roijers et al., 2014), current approaches in the multi-policy MORL either train a set of\nmultiple policies (Xu et al., 2020) or train a single network to cover multiple policies (Abels et al.,\n2019; Yang et al., 2019; Basaklar et al., 2023; Lu et al., 2023). For completeness, these approaches\nshould be followed by a preference elicitation method for the test phase given that additional inter-\nactive approaches are allowed (Hayes et al., 2022) (e.g., Zintgraf et al. (2018) inferred unknown user\npreference using queries of pairwise comparison on the Pareto frontier). Nonetheless, the area of\nelicitation has received far less attention than the learning methods themselves. Researchers usually\nfocus solely on the learning algorithms during the training phase assuming that test preferences will\nbe explicitly given.\nXu et al. (2020) trains a set of multiple policies in parallel using the concept of evolutionary learn-\ning, and the best policy in the policy set is used for evaluation during the test phase. Other works\nconstruct a single policy network parameterized by ω ∈∆K to cover the CCS, which is easier than\ndirect parameterization over the set of non-decreasing functions to cover the Pareto frontier. Abels\net al. (2019) and Yang et al. (2019) constructed single-policy networks to exploit the advantages\nof CCS. Specifically, Yang et al. (2019) defined the optimal multi-objective action-value function\nfor all ω ∈∆K: Q∗(s, a, ω) = EP,π∗(·|·;ω)|s0=s,a0=a [P∞\nt=0 γtrt] ∈RK, where the optimal pol-\nicy π∗is given by π∗(·|·; ω) = arg supπ ω⊤EP,π [P∞\nt=0 γtrt]. Based on a new definition of the\nmulti-objective optimality operator, the authors proposed an algorithm for training a neural network\nQθ(s, a, ω) to approximate Q∗(s, a, ω). Basaklar et al. (2023) modified the multi-objective optimal-\nity operator to match each direction of the learned action-value function and preference vector, and\nLu et al. (2023) tackled a learning stability issue of multi-policy MORL by providing theoretical\nanalysis on linear scalarization.\nWhile these methods have demonstrated promising performance in MORL benchmarks with two\nto four objectives, it remains an open question whether current algorithms can effectively scale to\nenvironments with more objectives (Hayes et al., 2022). The challenge lies in effectively covering\nall possible preferences during training. In most previous MORL algorithms, agents sample ran-\ndom preferences in each episode to collect diverse behaviors. However, performing this sampling\nnaively in high-dimensional spaces becomes computationally expensive because the coverage (or\nhypervolume) grows exponentially with the number of objectives (Wang & Sebag, 2013).\n4\n\n\nPublished as a conference paper at ICLR 2025\nIn this paper, we address the scalability issue by proposing a reward dimension reduction technique\nwith a suitable training and evaluation framework to narrow down the search space while preserving\nthe most relevant information. Our approach is motivated by the observation that objectives are\ncorrelated in many real-world cases. While a variety of dimension reduction techniques exist in\nmachine learning (Roweis & Saul, 2000; Tenenbaum et al., 2000; Lee et al., 2007; McInnes &\nHealy, 2018), most are designed for static (batch-based) datasets. Only a few methods are suitable\nfor online settings, and in some cases, no online version exists at all (McInnes & Healy, 2018).\nDeveloping online variants of batch-based dimension reduction techniques is itself an active area\nof research. Currently, incremental principal component analysis (PCA) and online autoencoders\nare commonly used for online dimension reduction (Cardot & Degras, 2018; Bank et al., 2020).\nHowever, we will demonstrate that they fail to preserve Pareto-optimality after transformation in the\ncontext of multi-policy MORL.\nTo our knowledge, few studies have explored reward dimension reduction in MORL. For instance,\nGiuliani et al. (2014) applied non-negative principal component analysis (NPCA) to a fixed set of\nreturn vectors collected from several pre-defined scenarios, identifying the principal components.\nHowever, they did not perform any further online interactions, but multi-policy MORL algorithms\nrequire online learning. In this paper, we propose a simple yet stable method for online dimension\nreduction that preserves Pareto-optimality after transformation, as described in the following section.\n4\nMETHOD\n4.1\nTRAINING AND EVALUATION FRAMEWORK\n𝓜=<S,A,P,𝜇0,R,𝛾>\nR: K-dim\n𝓜’=<S,A,P,𝜇0,f(R),𝛾>\nf(R): m-dim\nf: RK →Rm\n(K > m)\nJ1\nJ2\nJ3\nJ1\nJ2\n𝜔m\n𝜔\nFigure 3: Our proposed reward dimension reduction framework. We design a mapping function\nf : RK →Rm from the original reward space to the reduced reward space.\nAs seen in Figure 3, we aim to design a mapping function f : RK →Rm for reward dimension\nreduction, where K > m ≥2. f transforms the original MOMDP M = ⟨S, A, P, µ0, r, γ⟩into\nanother MOMDP M′ = ⟨S, A, P, µ0, f(r), γ⟩, reducing the dimensionality of the reward space\nwhile preserving essential features. We assume that standard multi-policy MORL approaches, such\nas Yang et al. (2019), perform adequately in the reduced-dimensional reward space of M′. Then\nfor any preference vector ωm ∈∆m, the (m −1)-simplex, the optimal multi-objective action-value\nfunction and the optimal policy are defined as:\nQ∗\nm(s, a, ωm) = EP,π∗m(·|·;ωm)|s0=s,a0=a\n\" ∞\nX\nt=0\nγtf(rt)\n#\n∈Rm, where\n(5)\nπ∗\nm(·|·; ωm) = arg sup\nπ ω⊤\nmEP,π\n\" ∞\nX\nt=0\nγtf(rt)\n#\n= arg sup\nπ EP,π\n\" ∞\nX\nt=0\nγt(ω⊤\nmf(rt))\n#\n.\n(6)\nπ∗\nm is also expressed as π∗\nm(a|s, ωm) = 1 if a = arg maxa′ ω⊤\nmQ∗\nm(s, a′, ωm), π∗\nm(a|s, ωm) = 0\notherwise. Note that the key aspect of multi-policy MORL is that we learn the action-value function\nQ∗\nm(s, a, ωm) not just for a particular linear combination weight ωm but for all possible weights\n{ωm ∈∆m} in the training phase so that we can choose the optimal policy for any arbitrary combi-\nnation weight depending on the agent’s preference in the test or evaluation phase.\n5\n\n\nPublished as a conference paper at ICLR 2025\nOur goal is to design a dimension reduction function, f, such that the policies learned in the reduced-\ndimensional space achieve high performance in the original reward space while satisfying two\nkey requirements: (i) online updates for dimension reduction and (ii) the preservation of Pareto-\noptimality in the sense that ∀ωm ∈∆m,\nEπ∗m(·|·,ωm)\n\" ∞\nX\nt=0\nγtf(rt)\n#\n∈CCSm ⇒Eπ∗m(·|·,ωm)\n\" ∞\nX\nt=0\nγtrt\n#\n∈F\n(7)\nwhere CCSm represents the convex coverage set in the reduced reward space and F ⊂RK repre-\nsents the Pareto frontier in the original reward space.\nTo the best of our knowledge, research on reward dimension reduction in MORL is limited, and\nthere is no well-established evaluation protocol for this task. In this section, we propose a new\ntraining and evaluation framework tailored to the reward dimension reduction problem, along with\nthe algorithm itself (outlined in Section 4.2). This framework facilitates a fair comparison of online\ndimension reduction techniques within the context of the original MOMDP.\nDuring the training phase, we aim to learn the optimal multi-objective action-value function\nQ∗\nm(s, a, ωm) while we simultaneously update the dimension reduction function f online. For\nthe action-value function update, we sample data (s, a, r, s′) from a replay buffer but utilize the\nreduced-dimensional rewards f(r) instead of the original rewards r. Our goal is to ensure that\nEπ∗m(·|·,ωm) [P∞\nt=0 γtf(rt)] ∈CCSm after the training phase ends.\nIn the evaluation phase, the learned policy π∗\nm(·|·, ωm) is tested on a set of Ne preferences Ωm,Ne ⊂\n∆m, with Ne = |Ωm,Ne|, where the points are evenly distributed on the (m −1)-simplex. For each\nωm ∈Ωm,Ne, we compute the expected cumulative reward Eπ∗m(·|·,ωm) [P∞\nt=0 γtrt] ∈RK in the\noriginal reward space, as the MOMDP provides the high-dimensional vector reward at each timestep.\nOur goal is for the Pareto frontier points of {Eπ∗m(·|·,ωm) [P∞\nt=0 γtrt] ∈RK|ωm ∈Ωm,Ne} to\nmaximize hypervolume while minimizing sparsity.\n4.2\nDESIGN OF DIMENSION REDUCTION FUNCTION\nTo preserve the Pareto-optimality as shown in equation 7, we impose two minimal conditions on the\ndimension reduction function f:\nTheorem 1. If f is affine and each element of the matrix is positive, then equation 7 is satisfied.\nProof. First, if f is affine, then f(r) = Ar+b ∈Rm, where A ∈Rm×K. By linearity, ∀ωm ∈∆m,\nEπ∗m(·|·,ωm) [P∞\nt=0 γt(Art + b)] = A\n\u0012\nEπ∗m(·|·,ωm) [P∞\nt=0 γtrt]\n\u0013\n+\n1\n1−γ b ∈CCSm.\nNext, if each element of A is positive, we claim that Eπ∗m(·|·,ωm) [P∞\nt=0 γtrt] ∈F so that the\nPareto-optimality in equation 7 is preserved.\nThis is proved by contradiction. Given ωm ∈∆m, suppose ∃π′ ∈Π s.t. Eπ′ [P∞\nt=0 γtrt] >P\nEπ∗m(·|·,ωm) [P∞\nt=0 γtrt] in the original reward space. By the definition of >P in equation 1, each\ndimension of Eπ′ [P∞\nt=0 γtrt] −Eπ∗m(·|·,ωm) [P∞\nt=0 γtrt] ∈RK is non-negative and at least one\ndimension is positive.\nFor each 1 ≤j ≤m, let a⊤\nj\n∈R1×K be the j-th row vector of A and bj be the j-th ele-\nment of b. Then a⊤\nj (Eπ′ [P∞\nt=0 γtrt] −Eπ∗m(·|·,ωm) [P∞\nt=0 γtrt]) > 0. In other words, we have\nAEπ′ [P∞\nt=0 γtrt] >P AEπ∗\nm(·|·,ωm) [P∞\nt=0 γtrt] in the reduced-dimensional space. By linearity,\nadding Eπ′ [P∞\nt=0 γtb] = Eπ∗m(·|·,ωm) [P∞\nt=0 γtb] =\n1\n1−γ b to both sides gives\nEπ′\n\" ∞\nX\nt=0\nγt(Art + b)\n#\n>P Eπ∗m(·|·,ωm)\n\" ∞\nX\nt=0\nγt(Art + b)\n#\n.\n(8)\nSince CCSm is by definition a subset of the Pareto frontier in the reduced-dimensional\nspace, CCSm consists of vector returns in the Pareto frontier.\nTherefore, equation 8\ngives a contradiction since Eπ∗m(·|·,ωm) [P∞\nt=0 γt(Art + b)] ∈CCSm is Pareto dominated by\nEπ′ [P∞\nt=0 γt(Art + b)].\n6\n\n\nPublished as a conference paper at ICLR 2025\nIn short, the condition of f(r) = Ar + b with A ∈Rm×K\n+\nguarantees the preservation of Pareto-\noptimality in equation 7. From equation 6,\nπ∗\nm(·|·, ωm) = sup\nπ E\n\" ∞\nX\nt=0\nγt(ω⊤\nmArt)\n#\n+\n1\n1 −γ ω⊤\nmb = sup\nπ E\n\" ∞\nX\nt=0\nγt(ω⊤\nmArt)\n#\n.\n(9)\nIn discounted reward settings, the bias term b does not affect the determination of π∗\nm, so we set\nb = 0 for simplicity.\nIn addition, we impose another condition that A is row-stochastic: PK\nk=1 Ajk = 1, 1 ≤j ≤m.\nThen\nK\nX\nk=1\n(A⊤ωm)k =\nK\nX\nk=1\nm\nX\nj=1\nAjk(ωm)j =\nm\nX\nj=1\n(ωm)j\nK\nX\nk=1\nAjk =\nm\nX\nj=1\n(ωm)j = 1.\n(10)\nIn other words, ∀ωm ∈∆m, we have the corresponding preference vector A⊤ωm ∈∆K in the\noriginal reward space. Let A⊤= [a1, · · · , am] ∈RKm\n+\nwhere aj ∈∆K, 1 ≤j ≤m. Then\nA⊤ωm ∈∆K and the set {A⊤ωm|ωm ∈∆m} ⊂∆K is the convex combination of aj ∈∆K, 1 ≤\nj ≤m. Conceptually, the role of the matrix A is to narrow down the preference search space from\n∆K to a proper subset of ∆K.\nThe next question is “How should we design the affine transform A to preserve the information\nof the original vector reward function r?” To address this question, we propose constructing a\nreconstruction neural network gϕ, where the input is the reduced-dimensional reward f(r). The\nnetwork gϕ is trained to minimize the reconstruction loss:\nmin\nA>0, A row-stochastic,ϕ Es,a∥r(s, a) −gϕ(f(r(s, a)))∥2\n(11)\nwhere A > 0 denotes that each element of A is positive. This approach, combining compression\nwith reconstruction, is widely employed to capture the essential features of input data while dis-\ncarding irrelevant information (Baldi, 2012; Kingma & Welling, 2014; Berahmand et al., 2024).\nHowever, solving the optimization problem in equation 11 is more challenging than conventional\nautoencoder-style learning, where the encoder is a general neural network trained without con-\nstraints. In contrast, our method must ensure that the matrix A satisfies both the positivity constraint\nA > 0 and row-stochasticity during online training.\nTo overcome this challenge, we introduce a novel approach by parameterizing A using softmax\nparameterization, ensuring both positivity and row-stochasticity constraints are satisfied through-\nout training. Our implementation in PyTorch (Paszke et al., 2019) effectively applies this param-\neterization, and we solve the optimization in equation 11 using stochastic gradient descent in an\nonline setting. The reconstruction loss is minimized alongside the training of the parameterized\nmulti-objective action-value function Qθ(s, a, ωm), which approximates Q∗\nm(s, a, ωm) as defined\nin equation 5. Additionally, we found that applying dropout (Srivastava et al., 2014) to gϕ during\ntraining in equation 11 further enhances overall performance.\n5\nEXPERIMENTS\n5.1\nENVIRONMENT AND BASELINES\nWhile various practical applications require addressing many objectives (Fleming et al., 2005; Li\net al., 2015; Hayes et al., 2022), there currently exist few MORL simulation environments with\nreward dimensions exceeding four (Hayes et al., 2022; Felten et al., 2023). To address this issue, we\nconsidered the following two MORL environments: LunarLander-5D (Hung et al., 2023) and our\nmodified implementation of an existing traffic light control environment (Alegre, 2019) to create a\nsixteen-dimensional reward setting.\nLunarLander-5D is a challenging MORL environment with a five-dimensional reward function\nwhere the agent aims to land a lunar module on the moon’s surface successfully. Each reward\ndimension represents: (i) a sparse binary indicator for successful landing (+ for success, - for crash),\n(ii) a combined measure of the module’s position, velocity, and orientation, (iii) the fuel cost of\nthe main engine, (iv) the fuel cost of the side engines, and (v) a time penalty. This environment\n7\n\n\nPublished as a conference paper at ICLR 2025\npresents significant challenges because failing to balance these objectives effectively can easily lead\nto unsuccessful landings (Felten et al., 2023; Hung et al., 2023).\nFigure 4: A snapshot of our considered\nenvironment: traffic light control.\nTraffic light control is a practical example of a prob-\nlem that can be formulated as MORL, where efficiently\nbalancing many correlated objectives is crucial (Hayes\net al., 2022). As shown in Figure 4, the traffic intersec-\ntion features four road directions (North, South, East, and\nWest), each with four inbound and four outbound lanes.\nAt each time step, the agent receives a state represent-\ning traffic flow information. The traffic light controller\nselects a phase as its action, and the reward is a sixteen-\ndimensional vector where each dimension corresponds to\na measure proportional to the negative total waiting time\nof cars on the respective inbound lanes.\nIn our experiments, we used the MORL algorithm from\nYang et al. (2019) as the base algorithm for the original\nreward space (Base). We incorporated several online di-\nmension reduction methods to the vector rewards in the\nbase algorithm, including online autoencoder (AE) (Bank et al., 2020), incremental PCA (Cardot\n& Degras, 2018), our online implementation of conventional batch-based NPCA (Zass & Shashua,\n2006), and our proposed approach. We followed the training and evaluation framework outlined in\nSection 4.1, setting m = 16 when evaluating the base algorithm alone.\nFor incremental PCA, we update the sample mean vector µ ∈RK and the sample covariance matrix\nC ∈RK×K at each timestep t using vector reward rt. We periodically perform eigendecomposition\non C and select the top m eigenvectors u1, . . . , um ∈RK corresponding to the largest eigenvalues,\nmaximizing Pm\nl=1 u⊤\nl Cul. We construct the matrix U = [u1, . . . , um] ∈RK×m so that U ⊤(r −\nµ) ∈Rm represents the reduced vector for r, following the PCA assumption that the transformed\nvectors are centered (Cardot & Degras, 2018).\nFor the online implementation of original NPCA (Zass & Shashua, 2006), we directly parameterize\nU = [u1, . . . , um] ∈RK×m with a non-negativity constraint for efficient training (which we denote\nas U ≥0). This direct parameterization removes an extra hyperparameter tuning for the constraint\nU ≥0 and gives a fair implementation compared with our method that also uses direct parameteri-\nzation. We optimize the objective maxU≥0\nPm\nl=1 u⊤\nl Cul −β∥U ⊤U −Im∥2 using gradient descent,\nwith a hyperparameter β > 0. Balancing the PCA loss with the orthonormality constraint creates\na trade-off between capturing principal component information and maintaining orthonormal basis\nvectors. Both PCA and NPCA do not use reconstructor gϕ.\nIn the traffic environment, we set m = 4 for all online dimension reduction methods. We set m = 3\nfor LunarLander-5D. To enhance the statistical reliability of our experimental results, we applied\na 12.5% trimmed mean by excluding the seeds with maximum and minimum hypervolume values\nover eight random seeds and reporting the averages of the metrics over the remaining six random\nseeds. This offers better robustness against outliers than the standard average (Maronna et al., 2019).\n(Additional implementation details can be found in Appendix E.)\n5.2\nRESULTS\nBase\nPCA\nAE\nNPCA\nOurs\nHV(×107, ↑)\n3.1 ± 4.7\n3.2 ± 4.2\n0\n1.7 ± 3.1\n25.6 ± 6.9\nSP(×102, ↓)\n31.2 ± 25.3\n188.6 ± 180.7\n31.3 ± 30.6\n53.0 ± 47.7\n1.1 ± 1.2\nTable 1:\nPerformance comparison in LunarLander-5D environment, with the reference point for\nhypervolume evaluation set to (0, −100, −100, −100, −100) ∈R5. HV: hypervolume, SP: sparsity.\nTable 1 demonstrates that in the LunarLander-5D environment, our algorithm outperforms the base-\nline methods in both hypervolume and sparsity metrics. Specifically, our approach improves the base\nalgorithm’s hypervolume by a factor of 8.3. It also reduces sparsity to the ratio of\n1\n28.4, resulting in\n8\n\n\nPublished as a conference paper at ICLR 2025\nmore diverse and better-performing solutions. Note that the hypervolume values reflect successful\nlanding episodes, so our dimension reduction is more efficient for achieving successful landing and\nbalancing remaining objectives simultaneously than the baselines.\nBase\nPCA\nAE\nNPCA\nOurs\nHV(×1061, ↑)\n4.4 ± 6.8\n0\n0.007 ± 0.018\n19.4 ± 15.3\n166.9 ± 48.1\nSP(×105, ↓)\n1842 ± 1290\n3837 ± 2164\n7834 ± 3323\n34.2 ± 52.3\n2.3 ± 1.0\nTable 2: Performance comparison in our traffic experiment where we set reference point for hyper-\nvolume evaluation to (−104, −104, · · · , −104) ∈R16 . HV: hypervolume, SP: sparsity.\nNext, Table 2 demonstrates that our algorithm consistently outperforms the baseline methods in the\ntraffic environment with sixteen-dimensional reward. Our algorithm improves the base algorithm’s\nhypervolume by a factor of 37.9 while significantly reducing sparsity, indicating that reward dimen-\nsion reduction effectively scales the base algorithm to higher-dimensional spaces. The PCA-based\ndimension reduction is an affine transformation, but because the matrix does not meet the positivity\ncondition in Theorem 1, it fails to guarantee Pareto-optimality as outlined in equation 7. Simi-\nlarly, the AE method uses a nonlinear transformation that fails to satisfy the linearity requirement in\nTheorem 1, producing worse hypervolume and sparsity than the base case.\nNPCA\nNPCA-ortho\nOurs\nHV(×1061, ↑)\n19.4 ± 15.3\n0.3 ± 0.5\n166.9 ± 48.1\nSP(×105, ↓)\n34.2 ± 52.3\n203.7 ± 24.1\n2.3 ± 1.0\nRank\n1\n4\n4\nTable 3: Performance comparison in the traffic experiment with NPCA and NPCA-ortho where\n“Rank” refers to the rank of the matrix in each method.\nIn Table 2, our method outperforms NPCA by significantly increasing hypervolume and reducing\nsparsity, with improvements of 8.6x in hypervolume. Although NPCA employs an affine transfor-\nmation with a nonnegative matrix, its online variant encounters instability due to the conflicting\nobjectives of optimizing the principal component loss while maintaining the orthonormality con-\nstraint. As shown in Table 3, the best-performing NPCA models, in terms of hypervolume and\nsparsity, had matrices of rank 1. The learning process prioritized maximizing the PCA loss, at the\nexpense of enforcing the orthonormality constraint, producing completely overlapping basis vectors.\nTo address this, we tuned hyperparameters to emphasize the orthonormality constraint, denoting\nthis variant as NPCA-ortho. However, Table 3 shows that this adjustment led to a performance de-\ncline compared to NPCA. The reason is that assigning more weight to the orthonormality constraint\nweakened the PCA update, significantly reducing its ability to capture relevant information from the\noriginal reward space. Additionally, we found that balancing the two losses was highly sensitive\nand difficult to fine-tune. In contrast, our method avoids these issues, offering a more stable and\neffective solution for reward dimension reduction without the trade-offs inherent in NPCA’s design.\nTo better illustrate results in high-dimensional space, we visualized the Pareto frontier points ob-\ntained from the traffic environment using t-SNE (Van der Maaten & Hinton, 2008), as detailed in\nAppendix B.1. We also present hypervolume values for different reference points and an additional\nmetric called Expected Utility Metric (EUM) (Zintgraf et al., 2015; Hayes et al., 2022) in Appendix\nB.2 and B.3, respectively.\n5.3\nABLATION STUDY\nIn this section, we investigate the impact of key components in our proposed dimension reduction\napproach. First, we analyze the effect of the constraints imposed on the dimension reduction function\nf. Specifically, we examine three aspects: bias, row-stochasticity, and positivity to assess their\ninfluence on preserving Pareto-optimality. Next, we evaluate the effect of applying dropout to the\nreconstructor gϕ in equation 11. We conducted our ablation study in the traffic environment.\nTable 4 presents the results of our first ablation study. Adding a bias term to f results in a slight de-\ncrease in hypervolume and an increase in sparsity compared to our method. However, the impact is\n9\n\n\nPublished as a conference paper at ICLR 2025\nOurs\n+bias\n-rowst\n-positivity\n-rowst, -positivity\nHV(×1061, ↑)\n166.9\n132.9\n46.8\n0\n0\nSP(×105, ↓)\n2.3\n2.7\n38.8\n4066.6\n5310.7\nTable 4:\nAblation study examining the effect of different conditions on the dimension reduction\nfunction f. “+bias” adds a bias term b in f; “-rowst” removes the row-stochasticity constraint while\nretaining the positivity condition; “-positivity” removes the positivity condition.\nless severe than the other modifications. While, in theory, the bias term b does not affect the determi-\nnation of the optimal policy under discounted reward settings (as shown in equation 9), in practice,\nintroducing a bias term offers minimal benefit, so a purely linear transformation is sufficient.\nWe next observe a performance drop when the row-stochasticity condition is removed. Notably,\nsparsity increased sharply by a factor of 16.9, highlighting the detrimental impact of this removal.\nNote that the direction of each preference vector ωm, not the magnitude, matters for the deter-\nmination of optimal policy π∗\nm in equation 6. By confining the search space to the simplex, the\nlearning process can focus on finding the correct direction to extract essential reward information,\nrather than expending unnecessary effort on adjusting magnitudes. Consequently, enforcing the\nrow-stochasticity constraint enhances learning efficiency, leading to more diverse solutions.\nIf we remove the positivity condition while maintaining the row-stochasticity constraint, the algo-\nrithm produces zero hypervolume. This is due to the lack of the positivity condition required by\nTheorem 1. Finally, further removing the row-stochasticity gives f(r) = Ar with a generic linear\nmatrix A that also fails to preserve Pareto-optimality in equation 7.\nIn summary, the positivity condition is essential for maintaining Pareto-optimality, while the row-\nstochasticity constraint improves the efficiency of online learning under the positivity condition.\nOurs\nWithout dropout\nHV(×1061, ↑)\n166.9\n127.1\nSP(×105, ↓)\n2.3\n9.5\nTable 5: Ablation study on the effect of applying dropout to the reconstructor gϕ in equation 11.\nNext, we evaluated the effect of applying dropout to gϕ in equation 11. As shown in Table 5, omitting\ndropout resulted in a slight decrease in hypervolume, while sparsity increased by a factor of 4.1.\nDropout can be interpreted as a form of regularization, approximately equivalent to L2 regularization\nafter normalizing the input vectors (Wager et al., 2013). L2 regularization helps prevent certain\nweights in a neural network from becoming excessively large.\nIn our context, each output feature of the reconstructor gϕ corresponds to an objective in MORL,\nand there are implicit correlations among these K objectives. By applying dropout, we mitigate the\nrisk of the model prematurely focusing on a subset of the K objectives by preventing the weights\nof gϕ from growing excessively large. This explains why sparsity increases when dropout is not\nused: the model may lose the opportunity to generate diverse solutions along the Pareto frontier, as\nlearning tends to focus prematurely on a subset of the K objectives without weight regularization.\n(We also provide an ablation study on the effect of the reduced dimensionality m in Appendix C.)\n6\nCONCLUSION AND FUTURE WORK\nIn this paper, we proposed a simple yet effective reward dimension reduction technique to address\nthe scalability challenges of multi-policy MORL algorithms. Our dimension reduction method effi-\nciently captures the key features of the reward space, enhancing both learning efficiency and policy\nperformance while preserving Pareto-optimality during online learning. We also introduced a new\ntraining and evaluation framework tailored to reward dimension reduction, demonstrating superior\nperformance compared to existing methods. Our future work includes developing more benchmarks\nand informative metrics for high-dimensional MORL scenarios to gain deeper insight and further\nadvance the field.\n10\n\n\nPublished as a conference paper at ICLR 2025\nREPRODUCIBILITY STATEMENT\nWe provide detailed descriptions of each algorithm in Section 5.1 and Appendix E, including the\ntechniques, fine-tuned hyperparameters, and infrastructures used in our experiments. The evaluation\nprotocol for performance comparison is described in Section 4.1. Furthermore, Theorem 1 is self-\ncontained, so it is easy to verify the theoretical results. The link to our code is https://github.\ncom/Giseung-Park/Dimension-Reduction-MORL.\nACKNOWLEDGMENTS\nThis work was supported in part by Institute of Information & Communications Technology Plan-\nning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS-2024-00457882,\nAI Research Hub Project) and in part by Institute of Information & Communications Technology\nPlanning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00469,\nDevelopment of Core Technologies for Task-oriented Reinforcement Learning for Commercializa-\ntion of Autonomous Drones).\nREFERENCES\nAxel Abels, Diederik M. Roijers, Tom Lenaerts, Ann Now´e, and Denis Steckelmacher. Dynamic\nweights in multi-objective deep reinforcement learning.\nIn Kamalika Chaudhuri and Ruslan\nSalakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,\nICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine\nLearning Research, pp. 11–20. PMLR, 2019. URL http://proceedings.mlr.press/\nv97/abels19a.html.\nLucas N. Alegre. SUMO-RL. https://github.com/LucasAlegre/sumo-rl, 2019.\nPierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Isabelle Guyon,\nGideon Dror, Vincent Lemaire, Graham W. Taylor, and Daniel L. Silver (eds.), Unsupervised\nand Transfer Learning - Workshop held at ICML 2011, Bellevue, Washington, USA, July 2, 2011,\nvolume 27 of JMLR Proceedings, pp. 37–50. JMLR.org, 2012. URL http://proceedings.\nmlr.press/v27/baldi12a.html.\nDor Bank, Noam Koenigstein, and Raja Giryes. Autoencoders. CoRR, abs/2003.05991, 2020. URL\nhttps://arxiv.org/abs/2003.05991.\nToygun Basaklar, Suat Gumussoy, and ¨Umit Y. Ogras.\nPD-MORL: preference-driven multi-\nobjective reinforcement learning algorithm. In The Eleventh International Conference on Learn-\ning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL\nhttps://openreview.net/pdf?id=zS9sRyaPFlJ.\nKamal Berahmand, Fatemeh Daneshfar, Elaheh Sadat Salehi, Yuefeng Li, and Yue Xu.\nAu-\ntoencoders and their applications in machine learning:\na survey.\nArtif. Intell. Rev., 57\n(2):28, 2024.\ndoi: 10.1007/S10462-023-10662-6.\nURL https://doi.org/10.1007/\ns10462-023-10662-6.\nHerv´e Cardot and David Degras. Online principal component analysis in high dimension: Which\nalgorithm to choose? International Statistical Review, 86(1):29–50, 2018.\nHakan Ayhan Dagistanli and ¨Ozden ¨Ust¨un. An integrated multi-criteria decision making and multi-\nchoice conic goal programming approach for customer evaluation and manager assignment. De-\ncision Analytics Journal, 8:100270, 2023.\nFlorian Felten, Lucas N. Alegre, Ann Now´e, Ana L. C. Bazzan, El Ghazali Talbi, Gr´egoire Danoy,\nand Bruno Castro da Silva. A toolkit for reliable benchmarking and research in multi-objective\nreinforcement learning. In Proceedings of the 37th Conference on Neural Information Processing\nSystems (NeurIPS 2023), 2023.\n11\n\n\nPublished as a conference paper at ICLR 2025\nPeter J. Fleming, Robin C. Purshouse, and Robert J. Lygoe.\nMany-objective optimization: An\nengineering design perspective.\nIn Carlos A. Coello Coello, Arturo Hern´andez Aguirre, and\nEckart Zitzler (eds.), Evolutionary Multi-Criterion Optimization, Third International Conference,\nEMO 2005, Guanajuato, Mexico, March 9-11, 2005, Proceedings, volume 3410 of Lecture Notes\nin Computer Science, pp. 14–32. Springer, 2005. doi: 10.1007/978-3-540-31880-4\\ 2. URL\nhttps://doi.org/10.1007/978-3-540-31880-4_2.\nMatteo Giuliani, Stefano Galelli, and Rodolfo Soncini-Sessa. A dimensionality reduction approach\nfor many-objective markov decision processes: Application to a water reservoir operation prob-\nlem. Environ. Model. Softw., 57:101–114, 2014. doi: 10.1016/J.ENVSOFT.2014.02.011. URL\nhttps://doi.org/10.1016/j.envsoft.2014.02.011.\nAdam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike. Quantifying differences\nin reward functions. arXiv preprint arXiv:2006.13900, 2020.\nConor F. Hayes, Roxana Radulescu, Eugenio Bargiacchi, Johan K¨allstr¨om, Matthew Macfarlane,\nMathieu Reymond, Timothy Verstraeten, Luisa M. Zintgraf, Richard Dazeley, Fredrik Heintz,\nEnda Howley, Athirai A. Irissappane, Patrick Mannion, Ann Now´e, Gabriel de Oliveira Ramos,\nMarcello Restelli, Peter Vamplew, and Diederik M. Roijers.\nA practical guide to multi-\nobjective reinforcement learning and planning.\nAuton. Agents Multi Agent Syst., 36(1):\n26, 2022.\ndoi:\n10.1007/S10458-022-09552-Y.\nURL https://doi.org/10.1007/\ns10458-022-09552-y.\nWei Hung, Bo-Kai Huang, Ping-Chun Hsieh, and Xi Liu. Q-pensieve: Boosting sample efficiency\nof multi-objective RL through memory sharing of q-snapshots. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRe-\nview.net, 2023. URL https://openreview.net/pdf?id=AwWaBXLIJE.\nDiederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\nIn Yoshua\nBengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\n//arxiv.org/abs/1412.6980.\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann\nLeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,\nCanada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/\nabs/1312.6114.\nJohn A Lee, Michel Verleysen, et al. Nonlinear dimensionality reduction, volume 1. Springer, 2007.\nBingdong Li, Jinlong Li, Ke Tang, and Xin Yao. Many-objective evolutionary algorithms: A survey.\nACM Comput. Surv., 48(1):13:1–13:35, 2015. doi: 10.1145/2792984. URL https://doi.\norg/10.1145/2792984.\nHaoye Lu, Daniel Herman, and Yaoliang Yu.\nMulti-objective reinforcement learning: Convex-\nity, stationarity and pareto optimality. In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.\nURL\nhttps://openreview.net/pdf?id=TjEzIsyEsQ6.\nRicardo A Maronna, R Douglas Martin, Victor J Yohai, and Mat´ıas Salibi´an-Barrera. Robust statis-\ntics: theory and methods (with R). John Wiley & Sons, 2019.\nLeland McInnes and John Healy. UMAP: uniform manifold approximation and projection for di-\nmension reduction. CoRR, abs/1802.03426, 2018. URL http://arxiv.org/abs/1802.\n03426.\nGiseung Park, Woohyeon Byeon, Seongmin Kim, Elad Havakuk, Amir Leshem, and Youngchul\nSung. The max-min formulation of multi-objective reinforcement learning: From theory to a\nmodel-free algorithm. In Forty-first International Conference on Machine Learning, ICML 2024,\nVienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.\nnet/forum?id=cY9g0bwiZx.\n12\n\n\nPublished as a conference paper at ICLR 2025\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK¨opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.\nPytorch: An imperative style,\nhigh-performance deep learning library.\nIn Hanna M. Wallach, Hugo Larochelle, Alina\nBeygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.\n8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/\nbdbca288fee7f92f2bfa9f7012727740-Abstract.html.\nTabish Rashid, Mikayel Samvelyan, Christian Schr¨oder de Witt, Gregory Farquhar, Jakob N. Foer-\nster, and Shimon Whiteson. QMIX: monotonic value function factorisation for deep multi-agent\nreinforcement learning. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Swe-\nden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 4292–4301.\nPMLR, 2018. URL http://proceedings.mlr.press/v80/rashid18a.html.\nDiederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-\nobjective sequential decision-making. J. Artif. Intell. Res., 48:67–113, 2013. doi: 10.1613/JAIR.\n3987. URL https://doi.org/10.1613/jair.3987.\nDiederik M. Roijers, Shimon Whiteson, and Frans A. Oliehoek. Linear support for multi-objective\ncoordination graphs. In Ana L. C. Bazzan, Michael N. Huhns, Alessio Lomuscio, and Paul Scerri\n(eds.), International conference on Autonomous Agents and Multi-Agent Systems, AAMAS ’14,\nParis, France, May 5-9, 2014, pp. 1297–1304. IFAAMAS/ACM, 2014. URL http://dl.\nacm.org/citation.cfm?id=2617454.\nSam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-\nding. science, 290(5500):2323–2326, 2000.\nUmer Siddique, Paul Weng, and Matthieu Zimmer. Learning fair policies in multi-objective (deep)\nreinforcement learning with average and discounted rewards. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol-\nume 119 of Proceedings of Machine Learning Research, pp. 8905–8915. PMLR, 2020. URL\nhttp://proceedings.mlr.press/v119/siddique20a.html.\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1):\n1929–1958, 2014. doi: 10.5555/2627435.2670313. URL https://dl.acm.org/doi/10.\n5555/2627435.2670313.\nHao Sun, Lei Han, Rui Yang, Xiaoteng Ma, Jian Guo, and Bolei Zhou. Exploit reward shifting\nin value-based deep-rl: Optimistic curiosity-based exploration and conservative exploitation via\nlinear reward shaping. Advances in neural information processing systems, 35:37719–37734,\n2022.\nJoshua B Tenenbaum, Vin de Silva, and John C Langford.\nA global geometric framework for\nnonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nStefan Wager, Sida Wang, and Percy Liang. Dropout training as adaptive regularization. In Christo-\npher J. C. Burges, L´eon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), Advances\nin Neural Information Processing Systems 26: 27th Annual Conference on Neural Information\nProcessing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,\nNevada, United States, pp. 351–359, 2013. URL https://proceedings.neurips.cc/\npaper/2013/hash/38db3aed920cf82ab059bfccbd02be6a-Abstract.html.\nWeijia Wang and Michele Sebag.\nHypervolume indicator and dominance reward based multi-\nobjective monte-carlo tree search. Machine learning, 92:403–429, 2013.\n13\n\n\nPublished as a conference paper at ICLR 2025\nMarc Weber, Phillip Swazinna, Daniel Hein, Steffen Udluft, and Volkmar Sterzing. Learning control\npolicies for variable objectives from offline data. In 2023 IEEE Symposium Series on Computa-\ntional Intelligence (SSCI), pp. 1674–1681. IEEE, 2023.\nJie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech Matusik.\nPrediction-guided multi-objective reinforcement learning for continuous robot control. In Pro-\nceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July\n2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 10607–\n10616. PMLR, 2020. URL http://proceedings.mlr.press/v119/xu20h.html.\nRunzhe Yang, Xingyuan Sun, and Karthik Narasimhan.\nA generalized algorithm for multi-\nobjective reinforcement learning and policy adaptation. In Hanna M. Wallach, Hugo Larochelle,\nAlina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Ad-\nvances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,\npp. 14610–14621, 2019.\nURL https://proceedings.neurips.cc/paper/2019/\nhash/4a46fbfca3f1465a27b210f4bdfe6ab3-Abstract.html.\nRon Zass and Amnon Shashua. Nonnegative sparse pca. Advances in neural information processing\nsystems, 19, 2006.\nLuisa M Zintgraf, Timon V Kanters, Diederik M Roijers, Frans Oliehoek, and Philipp Beau. Quality\nassessment of morl algorithms: A utility-based approach. In Benelearn 2015: proceedings of the\n24th annual machine learning conference of Belgium and the Netherlands, 2015.\nLuisa M. Zintgraf, Diederik M. Roijers, Sjoerd Linders, Catholijn M. Jonker, and Ann Now´e.\nOrdered preference elicitation strategies for supporting multi-objective decision making.\nIn\nElisabeth Andr´e, Sven Koenig, Mehdi Dastani, and Gita Sukthankar (eds.), Proceedings of\nthe 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS\n2018, Stockholm, Sweden, July 10-15, 2018, pp. 1477–1485. International Foundation for Au-\ntonomous Agents and Multiagent Systems Richland, SC, USA / ACM, 2018.\nURL http:\n//dl.acm.org/citation.cfm?id=3237920.\n14\n\n\nPublished as a conference paper at ICLR 2025\nA\nLIMITATION AND FUTURE WORK\nWhile our approach offers a promising solution for scaling MORL algorithms, several avenues re-\nmain for future research. First, as discussed in Section 5.1, the lack of benchmarks for environments\nwith more than ten objectives limits the comprehensive validation of our method. Developing robust\nbenchmarks for high-dimensional MORL scenarios is a crucial direction for our future research.\nSecond, recent research in MORL has focused on developing additional metrics to better evaluate\nperformance, recognizing that the data behavior in MORL is more complex than in standard RL.\nHigh-dimensional scenarios are difficult to visualize, and data behavior often deviates from intuitive\nexpectations (Lee et al., 2007). Therefore, designing informative metrics beyond standard measures\nlike hypervolume and sparsity is essential for gaining deeper insights and advancing the field.\nThird, although we provided mathematical conditions for preserving Pareto-optimality in Theorem\n1, these are only sufficient conditions. We investigated the effect of each condition in Section 5.3.\nHowever, our method’s theoretical guarantees will be more solid if we establish the necessary con-\nditions that pinpoint when Pareto-optimality fails. We provide a detailed discussion in Appendix\nD.\nFourth, while our approach enables effective training for scalable MORL, for practical use, test\npreference vectors in their original high-dimensional form must be reduced to the lower-dimensional\nspace learned by our model. Developing methods for preference vector reduction, and potentially\nintegrating preference elicitation mentioned in Section 3, will be essential for making our approach\nmore practical and complete.\nLastly, our method can be extended in various directions. For example, constrained MORL rep-\nresents a promising direction, especially for safety-critical tasks where additional constraints must\nbe considered. This extension could open up new applications where optimality and safety are\nparamount. Also, combining reward dimension reduction with reward canonicalization (Gleave\net al., 2020) and extending reward linear shifting (Sun et al., 2022) to high-dimensional offline\nMORL represent promising avenues for extending our work, both theoretically and experimentally.\n15\n\n\nPublished as a conference paper at ICLR 2025\nB\nADDITIONAL EXPERIMENTS\nB.1\nVISUALIZATION OF THE PARETO FRONTIERS\nFigure 5: t-SNE visualization of the acquired Pareto frontier points\nIn Figure 5, we visualized the Pareto frontier obtained in the traffic environment for each algorithm\nusing t-SNE (Van der Maaten & Hinton, 2008). We emphasize that our primary objective is\nto cover a broad region of the Pareto frontier, not merely to cover a wide region of a high-\ndimensional reward space itself. Although AE solutions may appear widely distributed, this does\nnot necessarily imply extensive coverage of the Pareto frontier because the Pareto frontier is a subset\nof the original space. Given that AE yields a low hypervolume, it is less likely to represent a wide\nrange of the Pareto frontier.\nAccording to the overlap analysis, smaller overlaps with the Base method suggest a different local\nstructure, for example, either in a way of our method (with high hypervolume) or a way of PCA\n(with very low hypervolume). This qualitative difference suggests that our method and PCA are\ndistinct in their approach to exploring the solution space. Also, NPCA overlaps with more points\nfrom the Base and AE methods than ours, demonstrating the insufficiency of NPCA in covering a\nlarger region of the Pareto frontier than the Base method.\nB.2\nHYPERVOLUMES WITH DIFFERENT REFERENCE POINTS\nIn our traffic environment, we selected the reference point (−104, −104, . . . , −104) ∈R16 based\non observations that, after the initial exploration phase, most points in the current Pareto fronts fell\nwithin this defined region (except for PCA). However, some points may deviate from this region.\nTo account for these outliers, the reference points can be adjusted accordingly. We evaluated the\nhypervolume using different reference points in both the traffic environment and LunarLander-5D.\nAs shown in Tables 6 and 7, our algorithm consistently outperforms the baseline methods.\nBase\nPCA\nAE\nNPCA\nOurs\nHV1(×107, ↑)\n3.1 ± 4.7\n3.2 ± 4.2\n0\n1.7 ± 3.1\n25.6 ± 6.9\nHV2(×108, ↑)\n7.6 ± 9.0\n8.8 ± 7.9\n0\n4.9 ± 6.8\n37.1 ± 6.7\nTable 6:\nPerformance comparison in the LunarLander-5D environment.\nThe reference\npoints for hypervolume evaluation are set to (0, −100, −100, −100, −100) ∈R5 for HV1 and\n(0, −150, −150, −150, −150) ∈R5 for HV2. HV: hypervolume.\n16\n\n\nPublished as a conference paper at ICLR 2025\nBase\nPCA\nAE\nNPCA\nOurs\nHV1(×1061, ↑)\n4.4 ± 6.8\n0\n0.007 ± 0.018\n19.4 ± 15.3\n166.9 ± 48.1\nHV2(×1067, ↑)\n5.0 ± 6.7\n0\n0.3 ± 0.7\n11.5 ± 4.8\n29.3 ± 3.4\nHV3(×1073, ↑)\n1.6 ± 0.8\n0\n0.2 ± 0.3\n1.9 ± 0.4\n2.9 ± 0.2\nTable 7: Performance comparison in the traffic experiment. The reference points for hypervolume\nevaluation are set to (−104, −104, · · · , −104) ∈R16 for HV1, (−2 × 104, −2 × 104, · · · , −2 ×\n104) ∈R16 for HV2, and (−4×104, −4×104, · · · , −4×104) ∈R16 for HV3. HV: hypervolume.\nB.3\nDISCUSSION ON EUM METRIC\nTo ensure that the current Pareto frontier F captures a diverse set of return vectors in the original\nreward space, we introduce an additional metric: the Expected Utility Metric (EUM) (Zintgraf et al.,\n2015; Hayes et al., 2022). The EUM is defined as follows:\nEUM(F, fs, ΩK, ¯\nNe) := Eω∈ΩK, ¯\nNe [max\nr∈F fs(ω, r)].\n(12)\nHere, ΩK, ¯\nNe ⊂∆K represents a set of ¯Ne preferences in the original reward space and fs is a\nscalarization function. We consider EUM because it effectively evaluates an agent’s performance\nacross a broad range of preferences (Hayes et al., 2022), aiming for a higher EUM to prevent the\nPareto frontier from covering only a narrow region within the original reward space. To compute\nEUM, we define ΩK, ¯\nNe as the set of ¯Ne normalized points evenly distributed on the (K −1)-\nsimplex ∆K. We also set the scalarization function by fs(ω, r) = ∥projω[r]∥, representing the\nprojected length of the vector r onto ω ∈ΩK, ¯\nNe. We set ¯Ne to 126 and 15,504 for LunarLander and\nthe traffic environment, respectively.\nLunarLander-5D\nBase\nPCA\nAE\nNPCA\nOurs\nEUM(↑)\n−25.8 ± 24.3\n−20.2 ± 21.5\n−76.2 ± 48.6\n−28.4 ± 13.9\n−11.5 ± 5.4\nTable 8: Performance comparison in LunarLander-5D experiment. EUM: expected utility metric.\nTraffic\nBase\nPCA\nAE\nNPCA\nOurs\nEUM(×103, ↑)\n−3.4 ± 2.9\n−35.1 ± 15.2\n−16.1 ± 8.8\n−4.4 ± 1.2\n−2.0 ± 1.0\nTable 9:\nPerformance comparison in our traffic experiment. EUM: expected utility metric.\nAs demonstrated in Tables 8 and 9, our method outperforms the baseline approaches in terms of the\nEUM. It is important to note that during training, the Base method uses equidistant points in the\noriginal reward space, which naturally leads to high EUM values, especially when the reward space\nhas a high dimensionality. However, our algorithm demonstrates superior performance compared\nto other reward dimension reduction methods, particularly in higher-dimensional environments like\nthe traffic scenario.\nC\nEFFECT OF THE REDUCED DIMENSIONALITY\nm\n2\n4\n6\n8\n10\nHV(×1063, ↑)\n1.4\n1.7\n1.7\n1.9\n1.1\nSP(×105, ↓)\n1.5\n2.3\n30\n20\n81\nTable 10: Ablation study on the effect of the reduced dimensionality m of f.\nTable 10 presents the effect of varying the reduced dimensionality m in the traffic environment.\nAs m increases from 4 to 6, the sparsity increases significantly while the hypervolume remains\nunchanged, resulting in scenarios where only a few objectives perform well. When m increases\nfurther from 8 to 10, both sparsity and hypervolume decrease, leading to a lower-quality set of\nreturns in the original reward space.\n17\n\n\nPublished as a conference paper at ICLR 2025\nm is a hyperparameter for our algorithm, and selecting an appropriate value in practice is achievable.\nThis is because we can estimate the effective rank of the sample covariance matrix recursively (Car-\ndot & Degras, 2018) during the early exploration phase of the RL algorithm, rather than throughout\nthe entire training process. We found that this straightforward approach performs effectively in our\nexperiments.\nD\nDISCUSSION ON THE NECESSARY CONDITION OF THEOREM 1\nWe acknowledge that theoretically analyzing the opposite direction of Theorem 1 is challeng-\ning. To find conditions for a counterexample of f beyond f(r) = Ar + b with A ∈Rm×K\n+\n,\nwe may follow a similar flow in the proof of the Theorem 1.\nGiven ωm ∈∆m, suppose\n∃π′ ∈Π s.t.\nEπ′ [P∞\nt=0 γtrt] >P Eπ∗m(·|·,ωm) [P∞\nt=0 γtrt] in the original reward space.\nWe\nfirst impose that f = [f1, · · · , fm] be a monotonically strictly increasing function satisfying\nA >P\nB ⇒fj(A) > fj(B) for all 1 ≤j ≤m (Hayes et al., 2022).\nThen we have\nf(Eπ′ [P∞\nt=0 γtrt]) >P f(Eπ∗m(·|·,ωm) [P∞\nt=0 γtrt]) in the reduced-dimensional space. If f further\nsatisfies\nEπ′\n\" ∞\nX\nt=0\nγtf(rt)\n#\n>P Eπ∗m(·|·,ωm)\n\" ∞\nX\nt=0\nγtf(rt)\n#\n,\n(13)\nthis gives a contradiction and f becomes our target counterexample. This is directly satisfied when\nf is affine. However, it is difficult to find such an example other than affine functions, primarily due\nto the inequality in >P .\nAlternatively, we conducted an empirical analysis by relaxing the condition in equation 13 and\ndirectly optimizing f under the sole constraint of a strictly monotonically increasing function. As\npart of our ablation study, we parameterized f as a strictly monotonically increasing function using a\nneural network (similar to approaches like Rashid et al. (2018) but maintaining strict monotonicity)\nand trained it within the traffic environment, denoted by “Monotone.”\nOurs\nMonotone\nHV(×1061, ↑)\n166.9\n0\nSP(×105, ↓)\n2.3\n5353.0\nTable 11: Ablation study on the effect of imposing strict monotonicity on f.\nTable 11 shows that this approach resulted in a hypervolume of zero, similar to the “-positivity” and\n“-rowst, -positivity” cases in Table 4. This suggests that merely imposing a strictly monotonically\nincreasing function condition is insufficient to construct a meaningful counterexample in practice.\nImportantly, nonzero hypervolume was only achieved when both the affine and positivity conditions\nwere satisfied, as demonstrated in the “-rowst” case from Table 4. These results underscore the\nempirical effectiveness of our algorithm based on Theorem 1.\nE\nIMPLEMENTATION DETAILS\nE.1\nSOURCE CODE AND ENVIRONMENT\nFor our implementation, we adapted morl-baselines (Felten et al., 2023) and integrated it with sumo-\nrl (Alegre, 2019), a toolkit designed for traffic light control simulations, as discussed in Section 5.\nFor LunarLander-5D, we used morl-baselines (Felten et al., 2023) with the reward function provided\nby the source code of Hung et al. (2023).\nThe traffic light system offers four distinct phases: (i) Straight and right turns for North-South traffic,\n(ii) Left turns for North-South traffic, (iii) Straight and right turns for East-West traffic, and (iv)\nLeft turns for East-West traffic. At each time step, the agent receives a 37-dimensional state, which\nincludes a one-hot encoded vector representing the current traffic light phase, the number of vehicles\n18\n\n\nPublished as a conference paper at ICLR 2025\nin each incoming lane, and the number of vehicles traveling at less than 0.1 meters per second for\neach lane. The simulation starts with a one-hot vector where the first element is set to one. Based\non this state, the controller chooses the next traffic light phase. The time between phase changes\nis 20 seconds, with each episode spanning 4000 seconds, or 200 timesteps. When transitioning\nto a different phase, the last 2 seconds of the interval display a yellow light to minimize vehicle\ncollisions. The reward, represented by a sixteen-dimensional vector, is calculated as the negative\ntotal waiting time for vehicles on each inbound lane. The simulation runs for 52,000 timesteps in\ntotal. For LunarLander-5D, the simulation runs for 2M timesteps.\nE.2\nBASELINES\nFor our proposed method and the baselines, we set the discount factor γ = 0.99 and use a buffer size\nof 52,000 and 1M for traffic and LunarLander, respectively. In Base algorithm (Yang et al., 2019),\nwe utilize a multi-objective action-value network Qθ with an input size of observation dimension\nplus K, two hidden layers of 128(LunarLander)/256(traffic) units each, and ReLU activations after\neach hidden layer. The output layer has a size of |A| × K. For the dimension reduction methods,\nthe Qθ network has an input size of input size of observation dimension plus m, two hidden layers\nof 128(LunarLander)/256(traffic) units with ReLU activations, and an output layer of size |A| × m.\nWe train Qθ using the Adam optimizer (Kingma & Ba, 2015), applying the loss function after\nthe first 200 timesteps, with a learning rate of 0.0003 and a minibatch size of 32. Exploration\nfollows an ϵ-greedy strategy, with ϵ linearly decaying from 1.0 to 0.05 over the first 10% of the\ntotal timesteps. The target network is updated every 500 timesteps. We update θ using the gradient\n∇θL(θ), L(θ) = (1 −λ)Lmain(θ) + λLaux(θ), where Lmain(θ) is the primary loss and Laux(θ) is the\nauxiliary loss in Yang et al. (2019). The weight λ is linearly scheduled from 0 to 1 over the first 75%\nand 25% percent of the total timesteps in traffic and LunarLandar, respectively. Sampling preference\nvectors ωm ∈∆m during training and execution follows the uniform Dirichlet distribution.\nFor the three online dimension reduction methods (our approach, the autoencoder, and our imple-\nmentation of online NPCA), we utilize the Adam optimizer for updates. In our method, the matrix\nA is initialized with each entry set to 1/K. The neural network gϕ has an input dimension of m,\ntwo hidden layers of 32 units each, and ReLU activations after each hidden layer. The output layer\nhas a size of K. We use a dropout rate of 0.75 and 0.25 in in traffic and LunarLandar, respectively\n(with 0 meaning no dropout). Equation 11 is optimized with a learning rate of 0.0003 and an update\ninterval of 5 timesteps.\nFor the autoencoder, the encoder network has an input size of K, two hidden layers with 32 units\neach, and ReLU activations after each hidden layer. The output layer has a size of m. The de-\ncoder follows the same architecture as gϕ, but without dropout. The reward reconstruction loss is\noptimized with a learning rate of 0.0001 and an update interval of 20 timesteps.\nFor the online NPCA, we use ReLU parameterization for efficient learning (also implemented in\nPyTorch (Paszke et al., 2019)) to meet the constraint on matrix U. The matrix U is initialized\nsimilarly with each entry set to 1/K. NPCA is optimized with a learning rate of 0.0001, an update\ninterval of 20(traffic)/50(LunarLander) timesteps, and β = 50000(traffic)/1000(LunarLander). The\nreduced vector representation of r is U T (r −µ) ∈Rm, following the PCA assumption that the\ntransformed vectors are centered (Zass & Shashua, 2006; Cardot & Degras, 2018). For NPCA-ortho\nin traffic, increasing the value of β did not yield better orthonormality, so we set the update interval\nto 5 timesteps, keeping the same β value.\nFor incremental PCA, we recursively update the sample mean vector of rewards as µt+1 =\nt\nt+1µt +\n1\nt+1rt+1 ∈RK and the sample covariance matrix as Ct+1 =\nt\nt+1Ct +\nt\n(t+1)2 (rt+1 −\nµt)(rt+1 −µt)⊤∈RK×K for each timestep t (Cardot & Degras, 2018). Every 20 timesteps, we\neigen-decompose the covariance matrix, selecting the top m eigenvectors u1, . . . , um ∈RK corre-\nsponding to the largest eigenvalues, and update U = [u1, . . . , um] ∈RK×m. The reduced vector\nrepresentation of r is U T (r−µ) ∈Rm, assuming the vectors are centered (Cardot & Degras, 2018).\nU is initialized as a matrix with each entry set to 1/K. For evaluation, we generated fifteen and thirty\nfive equidistant points on the simplex for LunarLander and the traffic environment, respectively. For\nevenly distributed sampling and calculating hypervolume and sparsity, we use the implementation\nprovided in Felten et al. (2023). We use infrastructures of Intel Xeon Gold 6238R CPU @ 2.20GHz\nand Intel Core i9-10900X CPU @ 3.70GHz.\n19\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20957v1.pdf",
    "total_pages": 19,
    "title": "Reward Dimension Reduction for Scalable Multi-Objective Reinforcement Learning",
    "authors": [
      "Giseung Park",
      "Youngchul Sung"
    ],
    "abstract": "In this paper, we introduce a simple yet effective reward dimension reduction\nmethod to tackle the scalability challenges of multi-objective reinforcement\nlearning algorithms. While most existing approaches focus on optimizing two to\nfour objectives, their abilities to scale to environments with more objectives\nremain uncertain. Our method uses a dimension reduction approach to enhance\nlearning efficiency and policy performance in multi-objective settings. While\nmost traditional dimension reduction methods are designed for static datasets,\nour approach is tailored for online learning and preserves Pareto-optimality\nafter transformation. We propose a new training and evaluation framework for\nreward dimension reduction in multi-objective reinforcement learning and\ndemonstrate the superiority of our method in environments including one with\nsixteen objectives, significantly outperforming existing online dimension\nreduction methods.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}