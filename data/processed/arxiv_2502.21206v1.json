{
  "id": "arxiv_2502.21206v1",
  "text": "Chronologically Consistent Large Language\nModels∗\nSongrun He\nLinying Lv\nAsaf Manela\nJimmy Wu\nThis draft: February 2025.\nAbstract\nLarge language models are increasingly used in social sciences, but their training\ndata can introduce lookahead bias and training leakage.\nA good chronologically\nconsistent language model requires efficient use of training data to maintain accuracy\ndespite time-restricted data. Here, we overcome this challenge by training chrono-\nlogically consistent large language models timestamped with the availability date of\ntheir training data, yet accurate enough that their performance is comparable to state-\nof-the-art open-weight models.\nLookahead bias is model and application-specific\nbecause even if a chronologically consistent language model has poorer language\ncomprehension, a regression or prediction model applied on top of the language\nmodel can compensate. In an asset pricing application, we compare the performance\nof news-based portfolio strategies that rely on chronologically consistent versus biased\nlanguage models and estimate a modest lookahead bias.\nJEL Classification: G11, G12, G17\nKeywords: Large language model, chronological consistency, lookahead bias, training leakage,\nbacktesting\n∗Songrun He is at Washington University in St. Louis (h.songrun@wustl.edu). Linying Lv is at Washington University in St.\nLouis (llyu@wustl.edu). Asaf Manela is at Washington University in St. Louis (amanela@wustl.edu). Jimmy Wu is at Washington\nUniversity in St. Louis (jimmywu@wustl.edu).\narXiv:2502.21206v1  [q-fin.GN]  28 Feb 2025\n\n\n“Obviously, the time continuum has been disrupted, creating a new temporal\nevent sequence resulting in this alternate reality.”\n– Dr. Brown, Back to the Future Part II\n1\nIntroduction\nThe increasing adoption of large language models (LLMs) in economics and finance\nhas opened up new possibilities for analyzing unstructured textual data. By capturing\nnuanced language patterns, these models provide powerful tools for testing flexible and\npreviously unquantifiable hypotheses in financial economics (Hoberg and Manela, 2025).\nHowever, their reliance on historical textual training data presents significant challenges:\nlookahead bias and training leakage, which undermine the validity of conclusions from\nany predictive analysis that should only use realtime available information from the past\n(Glasserman and Lin, 2023; Sarkar and Vafa, 2024; Ludwig et al., 2025).\nWe address this challenge by training chronologically consistent LLMs trained exclu-\nsively on historical textual data available at the time. While training language models\nwith historical data timestamped at the point of its availability is conceptually straightfor-\nward, ensuring these models are competitive with state-of-the-art counterparts remains a\nsignificant challenge.\nA major constraint in this approach is twofold: the significant computational resources\nrequired to pretrain a high-quality model and the limited availability of historical data.\nTo overcome the computational challenge, we employ recent innovations in efficient\npretraining algorithms from Warner et al. (2024) and Jordan et al. (2024). Simultaneously,\nleveraging the insights from Gunasekar et al. (2023), we compiled a pretraining corpus of\ndiverse and high-quality textual content to mitigate the data limitation. By combining\nthese strategies, we scale up our model in this resource-constrained environment to\n1\n\n\nultimately reach strong language understanding performance.\nUsing this approach, we introduce ChronoBERT, a series of chronologically consistent\nlanguage models pretrained on timestamped text data.\nThrough evaluation on the\nGLUE benchmark (Wang et al., 2019), even the earliest available version of ChronoBERT\nsignificantly outperforms prior lookahead-bias-free models, such as StoriesLM (Sarkar\nand Vafa, 2024), and domain-specific finance models, such as FinBERT (Huang et al.,\n2023), in language understanding tasks.\nFurthermore, ChronoBERT achieves GLUE\nperformance comparable to or even better than the widely used BERT model (Devlin et al.,\n2019), which ranks first in downloads among all language models on Hugging Face as of\nFebruary 2025.1 This positions ChronoBERT as the strongest language model to date that\nis free from lookahead bias.\nFinance is probably the research field most concerned with lookahead bias, and\nfor good reason. Tests of market efficiency (Fama, 1970) are only valid if asset prices\nare compared to information available at the time.\nIt would not be surprising if one\ncould predict returns after traveling to the future and bringing back a record of future\nnewspaper coverage. While researchers could keep a held-out sample of recent data to\navoid lookahead bias, the limited panel of asset prices commonly studied forces most\nstudies to rely on backtesting. Hence a chronologically inconsistent language model can\nbias measures of risk and market inefficiency.\nEquipped with our chronologically consistent models, we quantify the impact of\nlookahead bias when using LLMs to predict stock returns based on financial news.\nLeveraging extensive financial newswire data, we find that the portfolio performance of\nChronoBERT matches that of the state-of-the-art Llama 3.1 (Dubey et al., 2024), with both\nmodels delivering economically substantial and statistically significant gains compared\n1Based on download statistics from Hugging Face at https://huggingface.co/models?sort=\ndownloads. Its relatively small size makes it a popular choice for embeddings tasks and for fine-tuning\nmodels and applying them to large data pipelines where using the largest models (e.g.\nLlama) is\ncomputationally infeasible.\n2\n\n\nto StoriesLM (Sakar, 2024) and FinBERT (Huang et al., 2023). Our findings suggest that\nlookahead bias in this context is relatively modest.\nAn important observation is that the impact of lookahead bias is model- and application-\nspecific. While ChronoBERT may exhibit lower language comprehension on general tasks\ncompared to unconstrained models, downstream predictive models built on ChronoBERT\ncan adapt to these limitations, mitigating potential drawbacks in financial forecasting.\nOverall, our contribution is threefold. First, we quantify the extent of lookahead bias in\nnews return prediction tasks, demonstrating that its effects are modest. Second, we pro-\npose a framework for training chronologically consistent LLMs that preserves the integrity\nof economic and financial analyses while achieving competitive performance. Third, we\nprovide a constructive solution to address concerns of lookahead bias, not only in asset\npricing but also across broader applications in the social sciences. By demonstrating that\nchronological consistency need not come at the expense of performance, our findings pave\nthe way for more robust and reliable applications of LLMs in social science.\nOur work is related to two broad strands of literature.\nThere is a large literature\nlooking into return predictability using financial news.\nStarting from Tetlock et al.\n(2008), Jiang et al. (2021), and Ke et al. (2019), researchers document there is a short-\nterm underreaction of stock returns to textual information in financial news. The NLP\nmethodology used is mainly a word-counting approach and the economic magnitude is\nsmaller than the results using state-of-the-art LLMs.\nThe advent of large language models has further advanced this field. Chen et al.\n(2023) show that news embeddings derived from LLMs can effectively predict next-day\nreturns, leading to strong portfolio performance. Likewise, Lopez-Lira and Tang (2023)\ndemonstrate that interacting with LLMs via prompts can also generate promising portfolio\nresults.\nOur contribution to this literature lies in demonstrating that the robust return pre-\n3\n\n\ndictability achieved through LLMs is not driven by lookahead bias, thereby addressing a\ncritical concern in the use of these advanced models for financial forecasting.\nA second strand of literature pertains to the application and development of natural\nlanguage processing (NLP) tools for financial economics research (Hoberg and Manela,\n2025). Methodological advancements in this area have evolved from early dictionary-\nbased approaches (Tetlock, 2007; Loughran and McDonald, 2011), to text regressions\n(Manela and Moreira, 2017; Kelly et al., 2021), to topic modeling (Bybee et al., 2024), and\nmost recently, to the integration of LLMs (Jha et al., 2025; Chen et al., 2023; Lv, 2024). These\ndevelopments underscore the growing demand for more advanced and scalable tools to\nanswer research questions in finance and economics.\nWe contribute to this literature by developing a framework for pretraining high-quality\nlookahead-bias-free LLMs and providing a constructive solution to address the concerns\nof lookahead bias. Our approach does not require masking (Glasserman and Lin, 2023),\nwhich can destroy information. We train a set of chronologically consistent models with\nknowledge cutoffs from 1999 to 2024 which offer superior language understanding and\nmore up-to-date knowledge to previous such models (e.g., Sakar, 2024).\nThe rest of the paper is organized as follows: Section 2 describes our approach and\ndata for model pretraining and evaluation. Section 3 presents the empirical performance\nof our model. Section 4 concludes.\n2\nMethodology and Data\nIn this section, we outline the methodology we use to pretrain ChronoBERT and describe\nthe approach for evaluating its performance. Specifically, we assess its ability in both\nlanguage understanding and asset pricing tasks.\n4\n\n\n2.1\nPretraining Methodology for ChronoBERT\nWhen pretraining ChronoBERT, we incorporate a state-of-the-art BERT architecture from\nPortes et al. (2023) and Warner et al. (2024).2 Compared to the original BERT model by\nDevlin et al. (2019), this enhanced architecture integrates recent advancements in rotary\npositional embeddings that support longer context lengths and employ flash attention,\nsignificantly improving pretraining efficiency and computational speed.\nFor the pretraining task, we follow Warner et al. (2024) by adopting masked token\nprediction while omitting the next sequence prediction task, as prior research has shown\nthe latter increases training overhead without meaningful performance gains.\nThe quality of pretraining data is critical to achieving BERT-level performance. Gu-\nnasekar et al. (2023) demonstrate that using high-quality, “textbook-like” data leads to\nfaster convergence and improved model outcomes. Motivated by this insight, we filter our\npretraining corpus using the FineWeb-edu classifier from Penedo et al. (2024), retaining\nonly texts with scores above two.3\nHowever, restricting the corpus to texts with historical dates—particularly from early\nhistorical periods—introduces data scarcity challenges. Muennighoff et al. (2023) explore\nthe scaling laws of LLMs under data constraints, highlighting the benefits of iterative\ntraining on limited high-quality data.\nFollowing their insights, we train our model\nover multiple epochs to maximize learning from the available corpus. Our first model\ncheckpoint ChronoBERT1999 is trained on 460 billion tokens, with more than 70 epochs\nthrough the dataset.\n2We thank the authors for providing their pretraining code at https://github.com/mosaicml/\nexamples/tree/main/examples/benchmarks/bert and https://github.com/AnswerDotAI/ModernBERT.\n3We thank the authors for providing the classifier at https://huggingface.co/HuggingFaceFW/\nfineweb-edu-classifier.\n5\n\n\n2.2\nEvaluation Methodology\nTo evaluate ChronoBERT’s performance, we assess both its language understanding\ncapabilities and economic forecasting performance. We also apply the same evaluation\nmethodology to several other LLMs for benchmarking.\n2.2.1\nGLUE Evaluation for Language Understanding\nThe GLUE evaluation framework, introduced by Wang et al. (2019), comprises multiple\nclassification tasks designed to measure a model’s language understanding.4 This frame-\nwork was also the primary evaluation metric used in Devlin et al. (2019) to assess BERT’s\nlanguage capabilities.\nFollowing pretraining, we further fine-tune the model on task-specific training datasets\nand evaluate its performance on a held-out test set.\nFor fine-tuning, we adopt the training specifications and hyperparameters outlined in\nWarner et al. (2024). Among the eight GLUE tasks, RTE, MRPC, and STS-B are initialized\nfrom the MNLI checkpoint to enhance performance.\n2.2.2\nPredicting Stock Returns using Financial News\nWe investigate whether improved language understanding translates into economic gains\nby using different language models to predict stock returns from economic news. Based\non these predictions, we construct portfolios and evaluate the performance of long-short\nstrategies.\nFollowing Chen et al. (2023), we first aggregate all news articles’ headlines for a stock\non a given trading day together. Next, we transform this text into embeddings. Specifically,\n4The details and leaderboard of GLUE evaluation can be found at https://gluebenchmark.com/. In\nAppendix A, we outline details of eight GLUE tasks.\n6\n\n\nwe process each piece of text through different language models and extract the hidden\nstates of all tokens. The final embedding for each text is obtained by averaging the token\nembeddings.5\nNext, we link each news article to stock returns on the following trading day and\nfit a Fama-MacBeth regression with a ridge penalty to map news embeddings to return\npredictions. Each month m, we estimate the following cross-sectional ridge regression:\nri,t+1 = αm +β ′\nmei,t +εi,t+1,\nfor i = 1,···N and t = 1···T,6\n(1)\nwhere ei,t represents of the embedding of all news for firm i on day t. To construct real-\ntime out-of-sample forecasts, in month m′, we use an average of forecasts over all previous\nmonths’ cross-sectional models:\nˆri,t+1 = ¯αm′ + ¯β ′\nm′ei,t,\nfor i = 1,···N and t = 1···T,\n(2)\nwhere ¯αm′ =\n1\nm′−1 ∑m′−1\nm=1 ˆαm and ¯βm′ =\n1\nm′−1 ∑m′−1\nm=1 ˆβm.\nUsing these out-of-sample predictions, we sort stocks into decile portfolios at the end\nof each trading day, based on forecasts from different language models. We then evaluate\nthe performance of daily-rebalanced long-short decile portfolios constructed from these\npredictions.\n2.3\nData\nIn this section, we present the data we used for pretraining our language models and the\nfinancial newswire data we used to evaluate our language models.\n5Coleman (2020) provides a rationale for averaging token embeddings to get sequence embeddings.\n6We use a leave-one-out cross-validation algorithm to determine the ridge penalty λ chosen from grid\npoints ranging from 10−10 to 1010.\n7\n\n\n2.3.1\nPretraining Data for ChronoBERT\nChronoBERT1999 is pretrained on a corpus of 460 billion tokens comprising chronologically\nstructured English text sourced from diverse domains, including historical web content,\narchived news articles, and scientific publications. The dataset is fully open-source and is\ncarefully curated to include only text published before the year 2000, ensuring a focus on\nno leakage of lookahead knowledge. The final composition of our pretraining corpus was\ndetermined through extensive ablation studies to optimize model performance.\nWe further conduct incremental training from 2000 to 2024 on a corpus of 65 billion\ntokens with similar high-quality, diverse, timestamped, and open-source textual data to\nupdate knowledge for the model.\n2.3.2\nFinancial Newswire Data\nWe utilize the Dow Jones Newswire dataset, a real-time newswire subscribed to by major\ninstitutional investors.\nThis dataset provides extensive coverage of financial markets,\neconomies, and companies, aggregating reports from leading media sources such as Wall\nStreet Journal, Barron’s, and MarketWatch.\nThe dataset includes news headlines, full article texts, and precise display timestamps,\nwith microsecond-level accuracy for when the news becomes available to clients. Fol-\nlowing Ke et al. (2019), we focus on firm-specific news that can be attributed to a single\ncompany. For each firm-day observation, we aggregate all news headlines related to the\nfirm within the trading day window – spanning from 4:00 p.m. EST on day t −1 to 4:00\np.m. EST on day t – and treat the combined text as the firm’s textual information. Each\nconcatenated set of headlines is then processed through our embedding framework to\ngenerate numerical text representations.\nAfter the embedding step, we merge the news dataset with close-to-close returns on\n8\n\n\ntrading day t +1 from CRSP together to examine the predictability of stock returns using\nLLMs trained with real-time available textual data.\nOur dataset covers the period from January 2007 to July 2023. The first year serves as\na burn-in period to estimate the initial return prediction model, resulting in a final asset\npricing test sample spanning January 2008 to July 2023.\n3\nEmpirical Performance\nIn this section, we first describe the pretraining process of ChronoBERT, focusing on\nvalidation loss and GLUE scores as the model is trained on an increasing number of\ntokens. We then benchmark ChronoBERT’s language understanding capabilities against\nthree other language models. Finally, we assess its asset pricing performance in the news\nreturn prediction exercise.\nFirstly on pretraining performance, our results confirm the scaling law proposed by\nMuennighoff et al. (2023) in a data-limited environment. As shown in Figure 1, validation\nloss (measured via cross-entropy) decreases consistently as the number of training tokens\nincreases.7 Simultaneously, masked language prediction accuracy improves with training\nprogress.\nThese improvements translate into enhanced language understanding, as reflected\nin the GLUE scores. ChronoBERT begins outperforming BERT after approximately 350\nbillion training tokens and continues to improve thereafter.\nStarting from this high-quality base model in the early period, we continue incremental\npretraining using textual data afterward. We create model checkpoints for each year from\n1999 to 2024 (26 models in total). Figure 2 presents our models’ validation loss and GLUE\n7We use a subset of the C4 corpus from https://huggingface.co/datasets/allenai/c4 as the\nvalidation set. The C4 data is a cleaned version of the Common Crawl data.\n9\n\n\nscores as we train with incremental textual data over time.\nWe find that with the introduction of new data, the validation loss continues to drop.\nStarting in the year 2013, we introduce high-quality common crawl data. We witness a\nsignificant decrease in validation loss with the increase in data diversity. In the right panel\nof Figure 2, the GLUE scores for nearly all models exceed that of BERT, highlighting their\nsuperior language understanding and overall quality.\nFigure 1\nValidation Loss and GLUE Scores versus Training Steps\nThe left panel shows the validation loss, measured using cross-entropy loss and masked language\nprediction accuracy, as the ChronoBERT1999 is trained on an increasing number of tokens. The\nright panel displays the GLUE scores as training progresses. The final model checkpoint is trained\non 460 billion tokens. The training corpus consists of text up to December 1999.\nWe further compared ChronoBERT’s language understanding against three other\nmodels. Table 1 summarizes key characteristics of these models, including parameter\ncounts, context lengths, and knowledge cutoffs.\nThe models evaluated include:\nChronoBERTt: Our initial BERT-based model, ChronoBERT1999 is pretrained on 460\nbillion tokens of pre-2000, diverse, high-quality, and open-source text data to ensure no\nleakage of data afterward. Then, each year t starting in 2000, we start from the model\ntrained in the previous year and continue training it on data available in year t. Our final\ncheckpoint of the ChronoBERT series, ChronoBERT2024, is pretrained on 525 billion tokens\n10\n\n\nFigure 2\nValidation Loss and GLUE Scores over Time\nThe left panel shows the validation loss, measured using cross-entropy loss and masked language\nprediction accuracy as ChronoBERT is trained over time. The right panel displays the GLUE scores\nas training progresses over time.\nParameters\nContext\nKnowledge Cutoff\nChronoBERT1999\n149M\n1024\nDecember, 1999\n...\n...\n...\n...\nChronoBERT2024\n149M\n1024\nDecember, 2024\nBERT\n110M\n512\nOctober, 2018\nFinBERT\n110M\n512\nDecember, 2019\nStoriesLM\n110M\n512\nDecember, 1963\nTable 1\nCharacteristics and Knowledge Cutoffs of Different LLMs\nThis table provides an overview of four pretrained large language models, including their number\nof parameters, maximum context length, and knowledge cutoff dates.\nof diverse, high-quality, and open-source text until December 2024.\nBERT: The original BERT model trained on Wikipedia and the BookCorpus dataset by\nDevlin et al. (2019).8\nFinBERT: A domain-specific model pretrained on financial texts, including regulatory\nfilings, analyst reports, and earnings call transcripts, from Huang et al. (2023).9\nStoriesLM: A pretrained model from Sarkar and Vafa (2024), trained on historical\n8Model downloaded from https://huggingface.co/google-bert/bert-base-uncased.\n9Model downloaded from https://huggingface.co/yiyanghkust/finbert-pretrain.\n11\n\n\nnews articles. We use the final version trained on data up to 1963.10\nChronoBERT1999 ChronoBERT2024\nBERT\nStoriesLM\nFinBERT\nCOLA\n57.32\n56.32\n57.59\n46.85\n28.99\nSST2\n91.82\n92.58\n92.62\n90.44\n89.03\nMRPC\n92.71\n92.45\n90.76\n89.33\n88.59\nSTSB\n89.57\n89.93\n90.07\n87.01\n85.72\nQQP\n88.54\n88.90\n88.21\n86.88\n86.60\nMNLI\n86.19\n86.89\n84.98\n79.78\n79.23\nQNLI\n90.61\n92.04\n91.52\n87.44\n86.12\nRTE\n80.94\n85.20\n80.43\n67.15\n67.00\nGLUE\n84.71\n85.54\n84.52\n79.36\n76.41\nTable 2\nGLUE Score Evaluations for Different LLMs\nThis table compares the GLUE benchmark scores of four language models. Tasks are grouped\ninto three categories: (1) Single-sentence classification (COLA, SST2), (2) Paraphrase/semantic\nsimilarity (MRPC, STSB, QQP), and (3) Natural language inference (MNLI, QNLI, RTE). The final\nrow shows the average GLUE score across all tasks.\nTable 2 displays the GLUE scores for the five different models. ChronoBERT2024 and\nChronoBERT1999 achieves the top 2 performance in overall GLUE score (85.54 and 84.71),\nsurpassing performance from BERT (84.52) while substantially outperforming StoriesLM\n(79.36) and FinBERT (76.41). Notably, both ChronoBERT models Pareto dominate the\nlast two models—they outperform StoriesLM (designed to avoid lookahead bias) and\nFinBERT (domain-specific) across all individual tasks, with particularly large margins on\nCOLA, RTE, and MNLI. This performance advantage persists even in BERT-competitive\ntasks like MRPC and QQP.\nThese results highlight that ChronoBERT has better language understanding com-\npared to previous models. The performance gap between ChronoBERT and StoriesLM\nlikely stems from differences in training scale (460B versus 19B tokens) and the quality\nand diversity of the training data (ChronoBERT’s high-quality corpus versus StoriesLM’s\nunfiltered news-only dataset). Similarly, ChronoBERT’s significant edge over FinBERT\n10Model downloaded from https://huggingface.co/StoriesLM/StoriesLM-v1-1963.\n12\n\n\nhighlights the importance of diverse and high-quality pretraining data, as FinBERT’s\ndomain-specific financial texts lack comprehensive quality checks.\nImportantly, ChronoBERT also matches or exceeds the performance of BERT despite\nits chronologically bounded pretraining data, demonstrating that high-quality LLMs can\nbe constructed without chronological leakage.\nThis makes ChronoBERT particularly\nvaluable for applications requiring good language understanding and no lookahead bias.\nIts ability to achieve BERT-level performance while maintaining chronological integrity\nestablishes it as a strong candidate for a wide range of downstream tasks.\nTo quantify the economic gains from enhanced language understanding, we analyze\nstock return predictions using news embeddings from different language models. We\nconstruct portfolios by sorting stocks based on each model’s return forecasts and evaluate\nthe performance of long-short spreads generated by these rankings.11\nTable 3 presents the decile portfolio performance for realtime ChronoBERT model\n(ChronoBERTRealtime)12 against three other benchmarks: (1) BERT; (2) StoriesLM; and (3)\nLlama-3.1-8B.13 In this news return prediction setting, the H-L portfolio from ChronoBERTRealtime\ngenerates a Sharpe ratio of 4.80, outperforming both StoriesLM and BERT. These results\ndemonstrate that increased language understanding indeed translates into significant\neconomic gains. We also report the p-value of the pairwise Sharpe ratio difference test\nusing the Ledoit and Wolf (2008) approach in Table 4. The SR difference tests against\nboth models are significant at the 1% level.\nComparing ChronoBERT against the StoriesLM, we find the models’ improved lan-\n11Following He et al. (2024), we conduct a robustness check by forecasting the probability of a positive\nstock return on the subsequent trading day. The outcomes closely mirror those obtained from the return\nforecasts.\n12For example, in the year t, we would use the latest available model checkpoint ChronoBERTt−1 to embed\nthe news articles in that year and make predictions based on the embeddings.\n13We also evaluated FinBERT (Huang et al., 2023).\nAlthough not shown in the tables, our results\nindicate that FinBERT generated a H-L portfolio Sharpe ratio of 3.86 over the same sample period. Notably,\nChronoBERTRealtime H-L portfolio achieved a Sharpe ratio that is significantly larger than that of FinBERT\nat the 1% level.\n13\n\n\nChronoBERTRealtime\nBERT\nMean\nSD\nSR\nMean\nSD\nSR\nLow(L)\n-23.30\n25.86\n-0.90\n-22.52\n26.21\n-0.86\n2\n-2.43\n25.20\n-0.10\n-5.05\n25.55\n-0.20\n3\n4.17\n25.64\n0.16\n3.12\n24.92\n0.13\n4\n4.17\n24.58\n0.17\n8.14\n24.62\n0.33\n5\n3.94\n24.22\n0.16\n10.81\n24.44\n0.44\n6\n10.81\n24.13\n0.45\n9.38\n24.02\n0.39\n7\n14.56\n24.23\n0.60\n14.54\n23.83\n0.61\n8\n16.38\n23.64\n0.69\n18.51\n24.04\n0.77\n9\n23.95\n24.45\n0.98\n19.68\n23.90\n0.82\nHigh(H)\n37.71\n24.53\n1.54\n33.37\n24.88\n1.34\nH-L\n61.02\n12.72\n4.80\n55.89\n13.38\n4.18\nStoriesLM\nLlama 3.1\nMean\nSD\nSR\nMean\nSD\nSR\nLow(L)\n-17.80\n26.52\n-0.67\n-23.71\n26.15\n-0.91\n2\n-1.19\n25.26\n-0.05\n-4.77\n25.31\n-0.19\n3\n1.86\n24.92\n0.07\n-0.24\n24.86\n-0.01\n4\n5.90\n24.62\n0.24\n3.84\n24.62\n0.16\n5\n4.99\n24.30\n0.21\n7.47\n24.65\n0.30\n6\n11.88\n23.90\n0.50\n12.03\n24.23\n0.50\n7\n12.41\n23.66\n0.52\n13.31\n24.33\n0.55\n8\n18.93\n24.19\n0.78\n15.13\n23.79\n0.64\n9\n23.25\n24.30\n0.96\n24.68\n23.88\n1.03\nHigh(H)\n29.73\n24.78\n1.20\n42.20\n25.05\n1.68\nH-L\n47.53\n13.90\n3.42\n65.91\n13.46\n4.90\nTable 3\nPerformance of the LLM Portfolios\nThis table presents annualized performance metrics (mean return, standard deviation, and Sharpe\nratio) for decile portfolios sorted by next-day return predictions from financial news. Portfolios are\nrebalanced daily, with the ”H-L” row representing a strategy of longing the top decile and shorting\nthe bottom decile. All values are in percentage points except Sharpe ratios. All portfolios are equal-\nweighted. Data spans January 2008–July 2023.\nguage understanding increases the investment Sharpe Ratio by more than 40%, a value\nthat is both economically meaningful and statistically significant.\nComparing the in-\nvestment performance using ChronoBERT against the state-of-the-art Llama 3.1 model,\nwe find both models generate comparable Sharpe ratios (4.80 vs 4.90). This suggests\n14\n\n\nBERT\nStoriesLM\nLlama 3.1\nChronoBERTRealtime\n0.009\n0.000\n0.630\nBERT\n0.009\n0.002\nStoriesLM\n0.000\nTable 4\nP-value of Pairwise Sharpe Ratio Difference Tests\nThis table reports the p-value from the Ledoit and Wolf (2008) Sharpe ratio difference test of the\n‘H-L’ portfolios from different LLMs in Table 3. Each entry presents the test of the model reported\nin the row versus the model in the column. The portfolio sample spans from January 2008 to July\n2023.\nFigure 3\nSharpe Ratios of Long-Short Portfolios across Models over Time\nThis figure illustrates the Sharpe ratios of long-short portfolios constructed using predictions\nderived from financial news, with language models pretrained on text data up to the time points\nindicated on the x-axis. The blue dashed line represents the performance of the ChronoBERTRealtime\nmodel.\nChronoBERT’s performance is comparable to state-of-the-art LLMs in this financial ap-\nplication, despite its constrained training using historical textual data.\nFigure 3 further presents the trading performance of the whole series of ChronoBERTRealtime\nmodels. Specifically, for each time t in the figure, we use the ChronoBERTt model to embed\nnews articles and run predictions using embeddings from the model. We find consistent\nperformance across all models in the series. The results again highlight (1) the return\n15\n\n\nprediction exercise has modest lookahead bias; (2) the enhanced language understanding\nshown in Figure 2 indeed translates into significant economic gains.\n4\nConclusion\nIn this paper, we address the critical challenge of lookahead and survivorship bias in LLMs\nused in social science applications, particularly in financial forecasting. By introducing\nChronoBERT, a chronologically consistent language model trained on timestamped text\ndata, we demonstrate that chronological consistency can be achieved without compro-\nmising performance.\nOur results show that ChronoBERT matches or surpasses the\nlanguage comprehension abilities of BERT while maintaining robustness in asset pricing\napplications.\nOur findings reveal that the impact of lookahead bias in return prediction tasks is\nmodest.\nWe also highlight that the influence of lookahead bias is both model- and\napplication-specific. Notably, downstream predictive models can adapt to limitations in\nlanguage comprehension, ensuring economically and statistically significant gains.\nIn addition to quantifying the impact of lookahead bias in return prediction using\nfinancial news, we propose a scalable framework for training chronologically consistent\nLLMs.\nThis framework offers a constructive solution to deal with lookahead bias in\npredictive modeling, addressing a fundamental challenge in the application of LLMs to\nfinance and other social sciences. By ensuring chronological consistency, our approach\nlays the foundation for more reliable applications of LLMs in these domains.\nWe make ChronoBERT models publicly available at: https://huggingface.co/manelalab.\n16\n\n\nReferences\nBybee, Leland, Bryan Kelly, Asaf Manela, and Dacheng Xiu, 2024, Business news and\nbusiness cycles, The Journal of Finance 79, 3105–3147.\nChen, Yifei, Bryan T. Kelly, and Dacheng Xiu, 2023, Expected returns and large language\nmodels, SSRN Electronic Journal .\nColeman, Ben, 2020, Why is it okay to average embeddings?, Randorithms 16.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, 2019, BERT: Pre-\ntraining of deep bidirectional transformers for language understanding, Proceedings of\nthe 2019 Conference of the Association for Computational Linguistics 1, 4171–4186.\nDubey, Abhimanyu, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al., 2024,\nThe llama 3 herd of models, arXiv preprint arXiv:2407.21783 .\nFama, Eugene F, 1970, Efficient capital markets, The Journal of Finance 25, 383–417.\nGlasserman, Paul, and Caden Lin, 2023, Assessing look-ahead bias in stock return\npredictions generated by gpt sentiment analysis, SSRN Electronic Journal .\nGunasekar, Suriya, Yi Zhang, Jyoti Aneja, Caio C´esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi,\net al., 2023, Textbooks are all you need, arXiv preprint arXiv:2306.11644 .\nHe, Songrun, Linying Lv, and Guofu Zhou, 2024, Empirical asset pricing with probability\nforecasts, Available at SSRN .\nHoberg, Gerard, and Asaf Manela, 2025, The natural language of finance, Foundations and\nTrends in Finance .\nHuang, Allen H, Hui Wang, and Yi Yang, 2023, FinBERT: A large language model for\nextracting information from financial text, Contemporary Accounting Research 40, 806–\n841.\nJha, Manish, Hongyi Liu, and Asaf Manela, 2025, Does finance benefit society? a language\nembedding approach, Review of Financial Studies .\nJiang, Hao, Sophia Zhengzi Li, and Hao Wang, 2021, Pervasive underreaction: Evidence\nfrom high-frequency data, Journal of Financial Economics 141, 573–599.\n17\n\n\nJordan, Keller, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado,\nYou Jiacheng, Franz Cesista, Braden Koszarsky, and @Grad62304977, 2024, modded-\nnanogpt: Speedrunning the nanogpt baseline.\nKe, Zheng Tracy, Bryan T. Kelly, and Dacheng Xiu, 2019, Predicting returns with text data,\nSSRN Electronic Journal .\nKelly, Bryan, Asaf Manela, and Alan Moreira, 2021, Text selection, Journal of Business &\nEconomic Statistics 39, 859–879.\nLedoit, Oliver, and Michael Wolf, 2008, Robust performance hypothesis testing with the\nsharpe ratio, Journal of Empirical Finance 15, 850–859.\nLopez-Lira, Alejandro, and Yuehua Tang, 2023, Can ChatGPT forecast stock price\nmovements? return predictability and large language models, SSRN Electronic Journal .\nLoughran, Tim, and Bill McDonald, 2011, When is a liability not a liability?\ntextual\nanalysis, dictionaries, and 10-ks, The Journal of Finance 66, 35–65.\nLudwig, Jens, Sendhil Mullainathan, and Ashesh Rambachan, 2025, Large language\nmodels: An applied econometric framework, Technical report, National Bureau of\nEconomic Research.\nLv, Linying, 2024, The value of information from sell-side analysts, arXiv preprint\narXiv:2411.13813 .\nManela, Asaf, and Alan Moreira, 2017, News implied volatility and disaster concerns,\nJournal of Financial Economics 123, 137–162.\nMuennighoff, Niklas, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi,\nAleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel, 2023, Scaling\ndata-constrained language models, Advances in Neural Information Processing Systems 36,\n50358–50376.\nPenedo, Guilherme, Hynek Kydl´ıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel,\nLeandro Von Werra, Thomas Wolf, et al., 2024, The fineweb datasets: Decanting the\nweb for the finest text data at scale, arXiv preprint arXiv:2406.17557 .\nPortes, Jacob, Alexander Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin\nNadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle, 2023, MosaicBERT: A\nbidirectional encoder optimized for fast pretraining, Advances in Neural Information\nProcessing Systems 36, 3106–3130.\n18\n\n\nSakar, Suproteem, 2024, StoriesLM: A family of language models with time-indexed\ntraining data, SSRN Electronic Journal .\nSarkar, Suproteem, and Keyon Vafa, 2024, Lookahead bias in pretrained language models,\nSSRN Electronic Journal .\nTetlock, Paul C., 2007, Giving content to investor sentiment: The role of media in the stock\nmarket, The Journal of Finance 62, 1139–1168.\nTetlock, Paul C., Maytal Saar-Tsechansky, and Sofus Macskassy, 2008, More than words:\nQuantifying language to measure firms’ fundamentals, The Journal of Finance 63, 1437–\n1467.\nWang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman, 2019, GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding, in Proceedings of the International Conference on Learning\nRepresentations (ICLR).\nWarner, Benjamin, Antoine Chaffin, Benjamin Clavi´e, Orion Weller, Oskar Hallstr¨om, Said\nTaghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, et al., 2024,\nSmarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient,\nand long context finetuning and inference, arXiv preprint arXiv:2412.13663 .\n19\n\n\nAppendix\nA\nGLUE Evaluation\nIn this part, we lay out the details of the GLUE evaluation process. Following Warner et al.\n(2024), we use the same evaluation hyperparameters. Here are the details on learning rate,\nweight decay, and maximum number of epochs for each task. We use early stopping for\nall the fine-tuning tasks based on validation loss. The RTE, MRPC, and STS-B tasks are\nfinetuned starting from the checkpoint of MNLI.\n• CoLA (Corpus of Linguistic Acceptability): learning rate: 8e-5; weight decay: 1e-6;\nmaximum epochs: 5.\n• SST-2 (Stanford Sentiment Treebank - Binary Classification): learning rate: 8e-5;\nweight decay: 1e-5; maximum epochs: 2.\n• MNLI (Multi-Genre Natural Language Inference): learning rate: 5e-5; weight decay:\n5e-6; maximum epochs: 1.\n• MRPC (Microsoft Research Paraphrase Corpus): learning rate: 5e-5; weight decay:\n5e-6; maximum epochs: 10.\n• QNLI (Question Natural Language Inference): learning rate: 8e-5; weight decay:\n5e-6; maximum epochs: 2.\n• QQP (Quora Question Pairs): learning rate: 5e-5; weight decay: 5e-6; maximum\nepochs: 10.\n• RTE (Recognizing Textual Entailment): learning rate: 5e-5; weight decay: 1e-5;\nmaximum epochs: 3.\n• STS-B (Semantic Textual Similarity Benchmark): learning rate: 8e-5; weight decay:\n5e-6; maximum epochs: 10.\n20\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21206v1.pdf",
    "total_pages": 21,
    "title": "Chronologically Consistent Large Language Models",
    "authors": [
      "Songrun He",
      "Linying Lv",
      "Asaf Manela",
      "Jimmy Wu"
    ],
    "abstract": "Large language models are increasingly used in social sciences, but their\ntraining data can introduce lookahead bias and training leakage. A good\nchronologically consistent language model requires efficient use of training\ndata to maintain accuracy despite time-restricted data. Here, we overcome this\nchallenge by training chronologically consistent large language models\ntimestamped with the availability date of their training data, yet accurate\nenough that their performance is comparable to state-of-the-art open-weight\nmodels. Lookahead bias is model and application-specific because even if a\nchronologically consistent language model has poorer language comprehension, a\nregression or prediction model applied on top of the language model can\ncompensate. In an asset pricing application, we compare the performance of\nnews-based portfolio strategies that rely on chronologically consistent versus\nbiased language models and estimate a modest lookahead bias.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}