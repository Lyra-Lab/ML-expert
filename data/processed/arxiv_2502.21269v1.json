{
  "id": "arxiv_2502.21269v1",
  "text": "Dynamical Decoupling of Generalization and\nOverfitting in Large Two-Layer Networks\nAndrea Montanari∗\nPierfrancesco Urbani†\nMarch 3, 2025\nAbstract\nThe inductive bias and generalization properties of large machine learning models are –to a\nsubstantial extent– a byproduct of the optimization algorithm used for training. Among others,\nthe scale of the random initialization, the learning rate, and early stopping all have crucial\nimpact on the quality of the model learnt by stochastic gradient descent or related algorithms.\nIn order to understand these phenomena, we study the training dynamics of large two-layer\nneural networks. We use a well-established technique from non-equilibrium statistical physics\n(dynamical mean field theory) to obtain an asymptotic high-dimensional characterization of this\ndynamics. This characterization applies to a Gaussian approximation of the hidden neurons\nnon-linearity, and empirically captures well the behavior of actual neural network models.\nOur analysis uncovers several interesting new phenomena in the training dynamics: (i) The\nemergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity;\n(ii) As a consequence, algorithmic inductive bias towards small complexity, but only if the\ninitialization has small enough complexity; (iii) A separation of time scales between feature\nlearning and overfitting; (iv) A non-monotone behavior of the test error and, correspondingly,\na ‘feature unlearning’ phase at large times.\nContents\n1\nIntroduction\n3\n1.1\nGeneral questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.2\nA preview of our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2\nMain results\n7\n2.1\nTechnique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2\nTraining on pure noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.1\nFixed a, lazy initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.2\nDynamical a, lazy initialization . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.2.3\nDynamical a, mean field initialization\n. . . . . . . . . . . . . . . . . . . . . .\n11\n∗Department of Statistics and Department of Mathematics, Stanford University\n†Universit´e Paris-Saclay, CNRS, CEA, Institut de Physique Th´eorique, Gif-Sur-Yvette, France\n1\narXiv:2502.21269v1  [stat.ML]  28 Feb 2025\n\n\n2.2.4\nAdiabatic evolution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3\nTraining on data with latent structure . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3.1\nLazy initialization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3.2\nMean field initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3\nLower bounding the overfitting timescale\n17\n4\nDiscussion\n18\nA Setting\n24\nB Dynamical Mean Field Theory (DMFT)\n26\nB.1\nGeneral DMFT equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nB.2\nExpressions for train and test error . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nB.3\nSymmetric initialization and solutions . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nB.4\nDMFT equations for symmetric initialization (SymmDMFT ) . . . . . . . . . . . . . .\n29\nB.5\nExpressions for train and test error under symmetric initialization\n. . . . . . . . . .\n31\nC Numerical integration of the DMFT equations\n31\nC.1\nIntegration technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nC.2\nAccuracy of the numerical integration scheme . . . . . . . . . . . . . . . . . . . . . .\n32\nC.3\nTesting the numerical accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nC.4\nConstruction of the Gaussian process fg( · ) . . . . . . . . . . . . . . . . . . . . . . .\n35\nD Dynamical regimes: General preliminaries\n36\nE Dynamical regimes: Lazy initialization\n36\nE.1\nPure noise model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nE.1.1\nFirst dynamical regime: t = O(1/m) . . . . . . . . . . . . . . . . . . . . . . .\n37\nE.1.2\nSecond dynamical regime: t = Θ(1)\n. . . . . . . . . . . . . . . . . . . . . . .\n40\nE.1.3\nThe algorithmic interpolation transition . . . . . . . . . . . . . . . . . . . . .\n42\nE.1.4\nThird dynamical regime: t = Θ(m) . . . . . . . . . . . . . . . . . . . . . . . .\n44\nE.2\nMulti-index model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nE.2.1\nFirst dynamical regime: t = O(1/m) . . . . . . . . . . . . . . . . . . . . . . .\n47\nE.2.2\nSecond dynamical regime: t = Θ(1)\n. . . . . . . . . . . . . . . . . . . . . . .\n48\nE.2.3\nThe algorithmic interpolation threshold . . . . . . . . . . . . . . . . . . . . .\n50\nE.2.4\nDependence on m\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\nE.2.5\nThird dynamical regime: t = Θ(m) . . . . . . . . . . . . . . . . . . . . . . . .\n53\n2\n\n\nF Dynamical regimes: Mean field initialization\n55\nF.1\nPure noise model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nF.1.1\nFirst dynamical regime: t = O(1) . . . . . . . . . . . . . . . . . . . . . . . . .\n56\nF.1.2\nSecond dynamical regime: t = Θ(√m) . . . . . . . . . . . . . . . . . . . . . .\n57\nF.1.3\nThird dynamical regime: t = Θ(m) . . . . . . . . . . . . . . . . . . . . . . . .\n61\nF.2\nMulti-index model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nF.2.1\nFirst dynamical regime: t = Θ(1) . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nF.2.2\nEscape from the mean field dynamical regime . . . . . . . . . . . . . . . . . .\n65\nF.2.3\nSecond dynamical regime: t = Θ(m) and beyond . . . . . . . . . . . . . . . .\n71\nG Dynamics under mean field initialization for n/d = α fixed\n73\nG.1 Interpolation threshold at fixed a(t) = a0\n. . . . . . . . . . . . . . . . . . . . . . . .\n73\nG.2 Infinite width limit at fixed α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nH Lower bound on the overfitting timescale\n76\nH.1 Proof of Lemma 3.1\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\nH.2 Proof of Proposition 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\nI\nDynamical mean field theory for non-Gaussian model\n83\nJ\nDerivation of the dynamical mean field theory equations\n84\nJ.1\nUnfolding the Grassmann structure . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n1\nIntroduction\nLarge machine learning (ML) models are trained using stochastic gradient descent (SGD), or one\nof its variants to minimize the error on training data (empirical risk function). Classical ML theory\ndecouples the analysis of the resulting model from the optimization algorithm, under the assumption\nthat the latter converges to a good approximation of the global minimum [SSBD14]. The model\ncomplexity is assumed to be well controlled by suitable regularization techniques, leading to uniform\ngeneralization bounds.\nIn contrast, in modern ML the empirical risk is highly non-convex and overparametrized: the\nnumber of parameters is comparable with the number of training data points. More importantly,\nthe model complexity is only weakly controlled and, as a consequence, the empirical risk landscape\nabounds with near optima that have different generalization properties. It is by now a well-accepted\nhypothesis that a specific near-global optimum is selected implicitly by the specification of the\noptimization algorithm. Hence, generalization and inductive bias cannot be decoupled from the\ndynamics of training.\nSeveral striking consequences of this lack of decoupling are documented in the literature (and\nhave long been familiar to practitioners): (i) The network weights are initialized at random, and the\ntest error after training is observed to depend strongly on the initial distribution [GB10]. (ii) Several\n3\n\n\nclasses of optimization algorithms are used in practice (SGD, RMSProp, ADAM, and so on), and\ntest error depends strongly on the selected algorithm, even when they achieve the same train error\n[WRS+17]. (iii) Each of these algorithms comes with several hyperparameters, with learning rate\nschedule and batch size playing an important role. Careful choice of these hyperparameters is crucial\n[LWM19, YLWJ19], and the optimal choice is often different from the one that minimizes train\nerror. (iv) Early stopping SGD has long been known to play a regularization role [MB89, Bis95]:\nmodels learned by training for a shorter time have smaller complexity and can generalize better.\nThe generalization properties depend strongly (and non-monotonically) on the time at which the\noptimization algorithm is stopped.\nIn summary, the model learned by SGD is not simply the minimizer of an empirical risk function,\nbut is determined by the training dynamics. This observation has motivated a broad effort to encap-\nsulate the effect of the dynamics as ‘implicit regularization’ [SHN+18, ACHL19, CB20, WGL+20].\nThe main hypothesis of this line of work can be summarized as follows: the optimization algorithm\nselects, among many equivalent near-optima, one that is minimizes a specific notion of ‘model com-\nplexity.’ As a toy example, in the case of linear models, SGD converges to the minimizer that is\nclosest to the initialization in ℓ2 sense.\nWhile the implicit regularization hypothesis has been extremely fruitful, it has two important\nlimitations: (i) It is a priori unclear whether the effect of the training dynamics can be encapsulated\nby any explicit notion of ‘model complexity’; (ii) This hypothesis can only be validated to the extent\nthat we can precisely understand the training dynamics, and general tools for the latter are sorely\nmissing.\nIn this work, we address directly the central problem of developing tools to analyze the training\ndynamics and derive quantitative predictions on the implicit bias of neural network training. This\nallows us to capture feature learning and lazy/overfitting regimes within the same unified picture.\nWe discover a time-scale separation in the training dynamics, between an early stage in which the\nmodel learns the relevant features representation of the data, and a late stage of training that is\ncharacterized by overfitting, feature ‘unlearning,’ and hence worse test error.\nWe study two-layer fully connected neural networks f( · ; θ) : Rd →R,\nf(x; θ) = 1\nm\nm\nX\ni=1\nai σ(⟨wi, x⟩) .\n(1.1)\nThis model class is parametrized by θ = (a, W ), where W = (w1, . . . , wm) ∈Rd×m and a =\n(a1, . . . , am) ∈Rm are, respectively, first- and second-layer weights. For mathematical convenience,\nwe will assume the first-layer weights to be normalized, i.e. wi ∈Sd−1.\nWe apply model (1.1) to a classical supervised learning task. We are given i.i.d. data (yi, xi),\ni ≤n, with yi ∈R a response variable and xi ∈Rd a feature vector, and try to learn a model\nf( · ; θ) to predict the response ynew corresponding to a new input xnew. We use gradient flow to\nminimize the empirical risk under square loss, namely\n˙θ(t) = −n\ndP θ∇b\nRn(θ(t)) ,\nb\nRn(θ) := 1\n2n\nn\nX\ni=1\n\u0000yi −f(xi; θ)\n\u00012 .\n(1.2)\nHere P θ is a projection matrix1 that guarantees that wi(t) ∈Sd−1 at all times. The factor 1/d\nis introduced for mathematical convenience and simply amounts to a rescaling of time. We will\n1Explicitly, P θ is block-diagonal, with block corresponding to wi given by I −wiwT\ni , and equal to the identity\non the coordinates corresponding to a.\n4\n\n\ntypically initialize the training by setting (wi)i≤m ∼iid Unif(Sd−1), and ai = a0 for all i ≤m, and\nstudy the dependence of the training dynamics on three key parameters:\nNetwork width: m,\nOverparametrization ratio: α := n\nmd,\nInitialization scale: a0 .\nAlongside the train error, we will be interested in the test error at time t, i.e. R(θ(t)) := E{(ynew −\nf(xnew; θ(t)))2}/2, and the generalization error R(θ(t)) −b\nRn(θ(t)).\n1.1\nGeneral questions\nWhile much simpler than models of current use in AI, the fully connected network (1.1) is an\nimportant component of many state-of-the-art architectures (e.g., transformers [VSP+17]). Most\nimportantly, it exhibits some of the crucial phenomena that are ubiquitous in modern ML and\nallows to investigate several general questions which we summarize next.\nConvergence to global minimizers.\nWhen the network is sufficiently overparametrized (α\nsmall) and the initialization is large (a0 large), neural tangent kernel (NTK) theory [JGH18,\nDLL+19, COB19] predicts that gradient-based training converges to a global minimum with van-\nishing training error. On the other hand, models obtained with such initializations are known to\nhave suboptimal generalization properties [GMMM21, MMM22].\nQ1. For which values α, a0 does convergence take place (beyond the linear/NTK regime)?\nQ2. Does the selected model (with vanishing training error) provide good generalization?\nFeature learning.\nWhen the network initialization scale is small (a0 small) suitable gradient-\nbased algorithms can learn non-linear low-dimensional representation of the data [BES+22, DLS22],\nalong a hierarchy of increasing complexity [AAM22, BEG+22]. In these analysis, the difference\nbetween train and test error (generalization error) is negligible, during training. In other words,\nthe model does not overfit.\nQ3. Can we reconcile this feature-learning/no-overfitting behavior with the lazy-training/overfitting\nregime described previously?\nThe role of the number of iterations.\nSGD dynamics simplifies significantly in the first\nepoch, during which each training sample is visited only once. As long as data is not revisited,\nthe generalization error vanishes (under weak assumptions). Further, the dynamics has a simple\nMarkovian structure (we can reveal the k-th batch only at step k) which allows simpler asymptotic\ncharacterizations [SS95, MMN18].\nHowever, training for multiple epochs is beneficial in most\napplications, while also leading to overfitting and non-vanishing generalization error.\nQ4. How does the generalization error increase with the number of epochs? At what point in the\ntraining is the tradeoff optimized?\n5\n\n\n100\n101\n102\nt\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTrain/Test error\nMF Feat. learning\nFeat. learning\nOverﬁt/Unlearn\ntmf(m) ≍1\ntof(m) ≍m\n∥W2nd∥1 ≍1\n∥W2nd∥1 ≍√m\ngeneraliz.\nerror\n0\n1\n2\n3\n4\n5\n2nd layer ℓ1-norm\nFigure 1: Three dynamical regimes of learning in a two-layer neural networks, with m hidden\nneurons. Training data comprises n points in d dimensions distributed according to a single index\nmodel. We assume n, m, d all large with n/md = α (here α = 0.3). Blue: test error. Purple: train\nerror. Red: ℓ1 norm of second-layer weights (a proxy for model complexity).\nThe role of network size.\nHow does the answer to the previous question change with the number\nof neurons (and hence the number of parameters)?\nNaively one would expect that increasing\nnetwork size makes overfitting easier.\nHowever, on short time scales, the mean field theory of\n[MMN18, CB18, RVE22] suggests this not to happen.\nQ5. How does the generalization error depend on network size and number of iterations?\nQ6. Does overfitting start earlier for larger networks or later?\n1.2\nA preview of our results\nWe address the above general questions within model (1.1), by assuming a simple data distribution.\nNamely, we assume unstructured feature vectors xi ∼N(0, Id), and response variables that depend\non a low-dimensional projection of these data:\nyi = φ(U Txi) + εi ,\nεi ∼N(0, τ 2) ,\n(1.3)\nwhere it is understood that the noise εi is independent of xi, U ∈Rd×k is an orthogonal matrix\n(U TU = Ik) and φ : Rk →R is a nonlinear function, E{φ(g)2} < ∞for g standard Gaussian.\nOur main focus will be on the simplest case, namely k = 1, with φ a generic function (in particular\nE{φ(G)G} ̸= 0 for G ∼N(0, 1)).\n6\n\n\nThis data distribution is known as ‘k-index model’ and, despite its simplicity, presents a rich\nvariety of phenomena. In particular, when the dimension d becomes large, discovering the latent\nfeatures U Tx is crucial for learning and requires nonlinear processing of the labels yi. Indeed, many\nof the studies mentioned above are preoccupied with studying specific aspects of this model or its\nvariations [BES+22, DLS22, AAM22, BEG+22].\nA visual summary of our results is presented in Figure 1. From a technical viewpoint, we achieve\ntwo sets of goals:\n1. We use techniques from theoretical physics to derive an approximate asymptotic character-\nization of the gradient flow dynamics (1.2) in the limit n, d →∞, with n/d →α.\nThis\ncharacterization is a ‘dynamical mean field theory’ (DMFT) which is asymptotically exact\nfor a well-defined Gaussian version of the original model.\n2. We study this DMFT, with special attention to the large network limit m →∞at α = mα\nfixed, for a generic single index model (k = 1). In this limit, we obtain a separation of time\nscales in the learning dynamics, with each timescale corresponding to a regime of learning.\nFigure 1 illustrates this separation of time scales and emergence of learning regimes (curves are\nobtained by solving numerically the DMFT equations, see the appendices. This figures refers to\nlearning a single index noisy (τ > 0) data distribution with an overparametrized model α = 0.3. We\nidentify three regimes (below W 2nd := a/m is the vector of second-layer weights in model (1.1)):\n(i) Mean field feature learning. t = O(1). The network learns the low-dimensional features U Tx;\nthe train error and test error decrease while their difference (generalization error) is negligible; the\nsecond layer weights remain small ∥W 2nd∥1 = O(1).\n(ii) Extended feature learning. 1 ≪t ≪m. The train error decreases slowly; the generalization error\nincreases but remains small R(θ(t))−b\nRn(θ(t)) = o(1); the test error can evolve non-monotonically,\nbut remains approximately constant. Second-layer weights become large 1 ≪∥W 2nd∥1 ≪√m.\n(iii) Overfitting and feature unlearning. t ≳m. Train error and test error diverge significantly, i.e.\nR(θ(t)) −b\nRn(θ(t)) becomes of order one. At the end of this regime, the train error converges to\n0, i.e. the neural network interpolates the noisy data. The test error instead grows, and its limit\nvalue is the one of a (data independent) kernel method: in other words, the model unlearns the\nlow-dimensional structure. Finally, the second weights grow to ∥W 2nd∥1 ≍√m, which indeed is\nthe scale required for interpolation.\nIn Section 2 we provide a more complete account of our results, focusing on the insights it\nprovides on the training dynamics. The derivation of the DMFT equations as well as their detailed\nanalysis is deferred to the appendices. In Section 3 we state two simple rigorous results that confirm\nthe picture arising from DMFT, and in Section 4 we draw some conclusions. We point out some\nof our results apply to k-index models for general fixed k. However, a complete analysis of the\ngeneric k ≥2 case would require to consider time t diverging logarithmically in d, and hence a\nmore complex asymptotics. We expect qualitative results to carry through, but defer this analysis\nto future work.\n2\nMain results\nWe begin by describing the nature of our DMFT approximation in Section 2.1. In Section 2.2,\nwe use this approximation to study the simplest possible setting: pure noise data. Already this\n7\n\n\ncase presents a rich phenomenology, and provides useful background.\nFinally we consider the\nsingle-index model in Section 2.3.\nAs mentioned, we study the high dimensional limit n, d →∞, with n/d →α ∈(0, ∞) (and take\nthe large network limit afterwards.) Throughout, we index sequences by n, and it is understood\nthat d = dn satisfies the limit.\n2.1\nTechnique\nNotice that each fitting error Fi(θ) = yi −f(xi; θ), i ∈{1, . . . , n} is a random function of the model\nparameters θ. The randomness is due to the randomness in xi and in the noise εi. The empirical\nrisk in Eq. (1.2) can be rewritten as\nb\nRn(θ) = 1\n2n∥F (θ)∥2 ,\nF (θ) =\n\u0000F1(θ), . . . , Fn(θ)\n\u0001\n.\n(2.1)\nOur key approximation consists in replacing the i.i.d. random functions (Fi)i≤n by i.i.d. Gaussian\nprocesses (F g\ni )i≤n with matching mean and covariance. While DMFT equations have been recently\nproven without recurring to this approximation (see [CCM21] and appendices), their structure is\nsimpler in the Gaussian case, which allows us to carry out the large-m analysis.\nComputing the covariance of F ( · ) is a straightforward exercise. We assume for simplicity that\nan intercept is subtracted so that E[σ(G)] = 0, E[φ(G)] = 0 and otherwise these functions are\ngeneric (G, G1, G and so on will denote standard Gaussian vectors). We then have\nE\n\b\nf(x; θ1)f(x; θ2)\n\t\n= 1\nm2 ⟨a1, h(W T\n1 W 2)a2⟩,\n(2.2)\nE\n\b\nf(x; θ)y\n\t\n= 1\nm2 ⟨a, bφ(W TU)⟩.\n(2.3)\nRecall that θ = (a, W ) where a ∈Rm, W = (w1, . . . , wm) ∈Rd×m are the first layer weights\nFinally, h : R →R, bφ : Rk →R encode the activations σ and the target function φ, with h applied\nentrywise to the matrix W T\n1 W 2.\nThe covariance of Fi(θ) = yi −fi(x; θ) is easily computed from the above, and this defines\ncompletely the corresponding Gaussian process (F g\ni )i≤n. We denote the associated risk function\nb\nRg\nn(θ) := ∥F g(θ)∥2/2n.\nLet us emphasize that the cost function b\nRg\nn(θ) remains highly non-trivial despite the fact that\nthe functions Fi are replaced by Gaussian processes. Near-minima of high-dimensional Gaussian\nprocesses have a very rich structure, which is a central theme in spin glass theory [MPV87, Tal10].\nAdditional layers of complexity arise here for two reasons. First,\nb\nRg\nn(θ) is a sum of squares of\nGaussians and, second, the underlying Gaussian process has a significantly more intricate covari-\nance than in standard spin glasses (where typically depends only on the inner product ⟨θ1, θ2⟩).\nRecent work explored the simpler case in which F g\ni ( · ) is a Gaussian process with covariance\nE{F g\ni (θ1)F g\ni (θ2)} = ξ(⟨θ1, θ2⟩) depending uniquely on the inner product [Fyo19, FT22, Urb23,\nSub23, MS23, MS24, KD24]. Gradient descent dynamics on these models has been recently studied\nvia DMFT in [KU23a, KU23b]: our work builds on these advances. DMFT was leveraged before\nto address other questions in high-dimensional statistics and ML [MKUZ19, BP22]. We refer to\n[BADG06, CCM21] for mathematical results on the DMFT approach.\nWhile b\nRg\nn(θ) has a non-trivial structure, methods from statistical physics can be brought to\n8\n\n\nbear to derive an asymptotic characterization. Namely, define the functions\nCn\nij(t1, t2) = ⟨wi(t1), wj(t2)⟩,\nvn\ni (t) := U Tθi(t) ,\nan\ni (t) .\n(2.4)\nThese functions are random (because of the random initialization and the randomness in F g)\nand depend on n, d. However, as n, d →∞with n/d →α, they converge to non-random limits\n(Cij(t1, t2))i<j≤m, (vi(t))i≤m, (ai(t))i<j≤m that are the unique solution of a set of coupled integro-\ndifferential equations, see the appendices. We refer to these as to the DMFT equations.\nOur main focus is on the behavior of the solutions of these equations for large m and, at first\nsight, the complexity of the DMFT increases with m. An important simplification arises when\nchoosing a symmetric initial condition ai(0) = a0 for all i ≤m, and (wi(0))i≤m ∼iid Unif(Sd−1).\nNamely, the solution of the DMFT equations is symmetric under permutations of the neurons:\nCii(t1, t2) = Cd(t1, t2) for i ≤m and Cij(t1, t2) = Co(t1, t2) for i ̸= j ≤m, while ui(t) = u(t),\nai(t) = a(t) for i ≤m. We then have a reduction to a set of integro-differential equations on k + 3\nfunctions, that depend parametrically on m.\nWe use two approaches to study these equations (see appendix):\n(a) Numerical integration for increasing values of m under different initial conditions.\n(b) Asymptotics as m →∞(at fixed α = α/m) via singular perturbation theory [Ber01, Hol13].\nFor (b), a specific dynamical regime is identified by a scaling of the time variable, which in our case\nwill take the form t = t#(m) · ˆt for a certain fixed function t#(m) and ˆt = O(1) a scaled time. The\nasymptotics of DMFT quantities in that regime takes the form\nlim\nm→∞v\n\u0010\nt#(m) · ˆt; m, α = α\nm\n\u0011\n= v∗(ˆt; α) .\n(2.5)\n2.2\nTraining on pure noise\nWe begin by the case in which the data is pure noise: yi = εi ∼N(0, τ 2).\nA by-now-classic\nexperiment [ZBH+21] showed that deep learning models have sufficient capacity to achieve vanishing\ntraining error even when actual labels are replaced by random ones: they ‘interpolate pure noise.’\nThe ability of a model FΘ = (f( · ; θ) : θ ∈Θ) to interpolate pure noise is intimately connected\nto its Gaussian complexity G(FΘ; n) := E supθ∈Θ⟨g, f(X; θ)⟩/n [Ver18] (Rademacher complexity\nfor binary labels). Indeed, interpolation is impossible unless G(FΘ; n) ≥τ. Viceversa, G(FΘ; n) ≪τ\nensures good generalization.\nAn important theorem of Bartlett [Bar96] shows that, for the network (1.1), with a bound on\nthe ℓ1 norm of second-layer weights, G(FΘ; n) ≤Lσ∥a/m∥1\np\nd/n (with Lσ depending uniquely on\nthe activation function). This means that, in order to interpolate noise, the average magnitude\nof second layer weights must be ∥a∥1/m ≥Cσ,ατ√m (where Cσ,α = √α/Lσ and we recall that\nα = (n/md) = Θ(1) is the overparametrization ratio).\nDoes gradient flow converge to interpolators? Does the neural network produced by gradient\nflow have vanishing train error (and hence ∥a∥1 ≳√m), and yet generalize well?\nWe will study the dynamics of training on pure noise under three different setings for the\nevolution of second layer weights: a(0) ≍√m not evolving with gradient flow; a(0) ≍√m evolving;\na(0) ≍1 evolving. The first setting is the simpler to study, but we will show that the long-time\nbehavior of the other two is closely related to the first one via an adiabatic approximation.\n9\n\n\n10−1\n100\n101\n102\n103\nt\n0\n1\n2\n3\n4\n5\n6\na\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\n10−1\n100\n101\n102\n103\nt\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nTrain error\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nFigure 2: Evolution of second-layer weights (left) and train error (right) when fitting white noise\nvariance with standard deviation τ = 0.6. Here we use mean field initialization and h(z) = (9/10)z+\n(1/6)z3. Symbols: SGD results on actual 2-layer networks. Continuous viridis lines: Numerical\nsolution of the DMFT equations.\nAs an introduction, Figure 2 compares the DMFT predictions to simulations using SGD to\ntrain an actual two layer networks. In this figure we initialize a(0) = 1, and a(t) evolving. We\nobserve that the theory describes well —both qualitatively and quantitatively— the empirical\nresults, despite the Gaussian approximation in our DMFT and the difference between SGD and\ngradient flow. We also observe that second-layer weights remain roughly equal until a large time\nt#(m), which appears to increase with m. Roughly at the same time, train error starts to decrease\nrapidly and converges to zero. Our analysis will make precise this qualitative description.\nWe also note that, for φ = 0, the problem depends on a, τ only through the ratio a/τ. Unless\nstated otherwise, we can think that τ = 1 is fixed.\n2.2.1\nFixed a, lazy initialization\nWe set a(t) = γ√m with γ independent of m which does not change with time.\nNote that\nG(FΘ; n) ≍γ/√α and hence such a network can interpolate pure noise if γ is larger than a suitable\nconstant (depending on α).\nIndeed, DMFT predicts a sharp phase transition.\nFor α ∈(0, 1),\nGF converges to vanishing train error with high probability if γ > γGF(α, m), and converges to\na strictly positive training error if γ < γGF(α, m). The threshold γGF(α, m) converges to a limit\nγ∗\nGF(α) ∈(0, 1) as m →∞. Informally γ∗\nGF(α) is the minimum complexity γ for a very large network\nto interpolate noise.\nIn other words, for γ < γ∗\nGF(α), we have limn,d→∞b\nRg\nn(θ(t)) = etr(t; m, γ0), and\nlim\nt→∞lim\nm→∞etr(t; m, γ0) =\n(\ne∗(γ0) > 0\nfor γ0 < γ∗\nGF(α),\n0\nfor γ0 ≥γ∗\nGF(α).\n(2.6)\nThe function e∗(γ) will play an important role below.\n10\n\n\n2.2.2\nDynamical a, lazy initialization\nWe use the initialization a(0) = γ0\n√m with γ0 independent of m, and let a(t) evolve according\nto gradient flow (1.2); γ(t) = a(t)/√m evolves accordingly. DMFT predicts that, if γ0 is a small\nconstant (γ0 < γ∗\nGF(α)), then γ(t) grows in time until it becomes large enough for interpolation to\ntake place.\nMore precisely, for large m, we identify three dynamical regimes.\nFirst dynamical regime: t = O(1/m). γ(t) = γ0+om(1): second layer weights remain approximately\nconstant. The train error decreases because of the evolution of first layer weights which change\n∥wi(t)−wi(0)∥= O(1/√m). At the end of this regime, the network approximates the zero function\nf( · ; θ) ≈0.\nSecond dynamical regime: t = Θ(1).\nAlso in this regime γ(t) = γ0 + om(1) and hence the\nmodel complexity remains unchanged. However, first-layer weights change significantly: ∥wi(t) −\nwi(0)∥= Θ(1). The train error decreases appreciably in this phase and has a well defined limit\nlimm→∞b\nRg\nn(θ(t)) = elz2\ntr (t; γ0), with t 7→elz2\ntr (t; γ0) a monotone decreasing function.\nThe long-time behavior of elz2\ntr (t; γ0) depends on the complexity at initialization γ0. If γ0 >\nγGF(α), then limt→∞elz2\ntr (t; γ0) = 0 and interpolation takes place within this dynamical regime.\nIf γ0 < γGF(α), then limt→∞elz2\ntr (t; γ0) = e∗(γ0) > 0. This coincides with the asymptotic train\nerror at fixed γ, cf. Sec. 2.2.1. In the latter case, a third dynamical regime emerges and leads to\ninterpolation on significantly longer time scales.\nThird dynamical regime: t = Θ(m). γ(t) increases significantly: γ(t) −γ0 = Θ(1). At the same\ntime, the train error decreases on the same time scale, reaching 0 when γ(t) ≈γ∗\nGF(α). The model\ncomplexity γ(t) and training error b\nRg\nn(θ(t)) obey a scaling law. Namely, for any fixed z ∈(0, ∞),\nas m →∞, we have\na(mz) = γlz3(z; γ0)√m + om(√m) ,\nb\nRg\nn(θ(mz)) = elz3\ntr (z; γ0) + om(1) .\n(2.7)\nInformally, the scaling law implies that, in this time scale, a(t) ≈√mγlz3(t/m; γ0) and b\nRg\nn(θ(t)) ≈\nelz3\ntr (t/m; γ0). We have limz→0 γlz3(z; γ0) = γ0 hence matching the previous two regimes.\nAs further discussed below, the DMFT solution points at a separation of dynamical degrees\nof freedom, with the complexity γ(t) acting as a slow variable, while the others variables evolving\nrapidly [VK85, PS08].\n2.2.3\nDynamical a, mean field initialization\nWe now choose an initialization a(0) = a0 that is independent of m, and let a(t) evolve with\ngradient flow. Figure 3 compares numerical experiments with SGD on actual two-layer networks\nand numerical integration of the DMFT equations.\nAt initialization, the Gaussian complexity of the models with the given ∥a∥1 is G(FΘ; n) ≲\np\nd/n = 1/√mα. In order to achieve interpolation, a(t) has to grow and become of order √m.\nFor large m, DMFT predicts that the growth of a(t) (and hence model complexity) takes place in\nthree distinct dynamical regimes, each corresponding to a different time scale, as described below.\nFirst dynamical regime: t = O(1). Second layer weights do not move a(t) = a0 + om(1), while first\nlayer weights change by a small amount ∥wi(0)−wi(t)∥= om(1) to achieve approximately 0 mean:\nP\ni≤m wi ≈0. The model learned at the end of this regime is a good approximation of the null\n11\n\n\n10−2\n10−1\n100\n101\nt/m\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\na/√m\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\na/√m\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nTrain error\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nFigure 3: Evolution of second-layer weights (left) and train error vs second-layer weights (right),\nunder mean field initialization. Same setting as in Fig. 2, except we scale t and a according to the\ninterpolation scaling law of Eq. (2.7). Symbols: SGD results. Continuous viridis lines: DMFT.\nContinuous black line: scaling law (computed with fixed second layer weights).\nmodel f(x; θ) ≈0. Correspondingly, b\nRn(θ(t)) = b\nRn,null + om(1) with b\nRn,null := τ 2/2 the error of\nthe null model.\nSecond dynamical regime: t = Θ(√m). On this time scale, second layer begin to evolve according\nto a(t) = amf2(t/√m)+om(1), while first layer weights change significantly ∥wi(t)−wi(0)∥= Θ(1).\nWhile the dynamics on this regime is highly non-trivial, the change is not large enough to change\nthe train error b\nRn(θ(t)) appreciably. We still have b\nRn(θ(t)) ≈b\nRn,null + om(1). Equations for the\nscaling function amf2( · ) are given in the appendices. It turns out that, in this regime, the dynamics\nof the m neurons is asymptotically equivalent to the dynamics of m systems coupled uniquely\nvia a(t). Further, each of the neurons follows a dynamics that is equivalent to the dynamics of\na ‘reduced model.’ In the Gaussian process setting, the reduced model is the celebrated mixed\nspherical spin glass model [CK93, FFRT20].\nThird dynamical regime: t = Θ(m). On this scale, second layer weights finally grow to be of order\n√m and interpolation becomes possible. Namely, for any fixed z, as m →∞, we have\na(mz) = √mγmf3(z) + om(√m) ,\nb\nRn(θ(mz)) = emf3\ntr (z) + om(1) .\n(2.8)\nThe functions γmf3, emf3\ntr\nare close (and potentially identical to) the γ0 →0 limit of the functions\nγlz3, elz3\ntr of Eq. (2.7). We have γmf3(z) = γ1z + o(z) as z →0 which suggests to a(t) ≍t/√m for\n√m ≪t ≪m, thus matching the previous time scale. At the other extreme, γmf3(z) →γ∗\nGF(α) ≈\nγ∗\nGF(α) as z →∞, i.e. weights grow until a(t) ≈γ∗\nGF(α)√m and at that point interpolation takes\nplace.\nThis third dynamical regime is illustrated in Fig. 3 which compares numerical simulations with\nSGD and DMFT in terms of the rescaled variables t/m, a(t)/√m and b\nRn(θ(t)). In the left plot, we\nplot a(t)/√m versus t/m for several values of m. We obtain approximate collapse of data obtained\nfor different values of m, confirming the the ansatz in the third dynamical regime.\nRemark 2.1. While our analytical derivations are based on the Gaussian approximation b\nRg\nn(θ) to\nthe empirical risk function, we expect our qualitative conclusions to apply to the original empirical\nrisk b\nRn(θ). In particular, we expect the same time-scale separation to arise in this context. This\nis partially confirmed by our numerical simulations, and by the rigorous results of Section 3.\n12\n\n\n2.2.4\nAdiabatic evolution\nConsider again the third dynamical regime under lazy initialization. In Fig. 3 right frame, we plot\netr(t) parametrically against a(t)/√m for several values of m. As m increase, the curves appear\nto converge to a well-defined limit. To characterize this limit, recall the ansatz (2.8) and the fact\nthat function γmf3 describing the increase in complexity is monotone, mapping (0, ∞) to (0, γ∗\nGF(α))\n(with γ∗\nGF(α)) ≈γ∗\nGF(α)). We can therefore define the inverse function γ 7→(γmf3)−1(γ) and\nεmf(γ) := emf3\ntr ((γmf3)−1(γ)) .\n(2.9)\nUnder the ansatz (2.8), the finite-m curves of Fig. 3 converge to the limit (γ, εmf(γ)).\nNote that we could have defined an analogous curve for the lazy initialization.\nNamely,\nfor γ ∈(γ0, γ∗\nGF(α)) εlz(γ) := elz3\ntr ((γlz3)−1(γ; γ0); γ0) (where the inverse function is defined by\nγlz3((γlz3)−1(γ; γ0); γ0) = γ). Our analysis of the DMFT equations implies that εlz(γ) ≈εmf(γ).\nIn Fig. 3, we also plot the curve e∗(γ) defined in Sec. 2.2.1 which gives the asymptotic training\nerror when keeping second-layer weights fixed (as computed from numerically solving the DMFT\nequations). The finite-m curves appear to converge to the latter, i.e.:\nεmf(γ) ≈εlz(γ) ≈e∗(γ) .\n(2.10)\nThis identity has a simple interpretation. For large m, we have a fast-slow separation of degrees\nof freedom (a classical phenomenon in multiscale analysis [PS08]).\nThe model complexity γ(t)\ngrows slowly, while all other quantities in the DMFT equations relax rapidly. At any given t, the\ndynamics of the other degrees of freedom is nearly the same as if the complexity was kept fixed at\nγ(t) (as in Sec. 2.2.1).\nWhile our numerical integration of the DMFT equations is consistent with the hypothesis that\n(2.10) is an exact equality, establishing whether exact equality holds is an open problem.\n2.3\nTraining on data with latent structure\nWe next consider training on data from a single-index model.\nWhile we expect the time-scale\nseparation discussed hear to hold for general k-index models, some subtleties arise with the high-\ndimensional asymptotics studied here. We discuss the generalization to k ≥2 in Section 4. As\nbefore, we separately analyze two types of initializations: lazy and mean field.\n2.3.1\nLazy initialization\nWe initialize a(0) = γ0\n√m, and let γ(t) evolve according to gradient flow alongside first-layer\nweights. DMFT predicts the emergence of three dynamical regimes for large m, in analogy to the\ncase of pure noise data. For an illustration, we refer to Fig. 4.\nFirst dynamical regime: t = O(1/m). Second layer weights do not change significantly γ(t) =\nγ0 + om(1), while first layer-weights move by ∥wi(t) −wi(0)∥= Θ(1/√m). Because the weights\nai(t) are of order √m, even an O(1/√m) change in the wi leads to a significant decrease in test\nerror and train error.\nTrain and test error are close to each other. Namely, the following limits are well defined\nlim\nn,d→∞\nb\nRn(θ(t)) = etr(t; φ, γ0, m, α) ,\nlim\nn,d→∞R(θ(t)) = ets(t; φ, γ0, m, α) .\n(2.11)\n13\n\n\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1\n10\n100\n1000\nTrain/Test error\ntm\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nelz1\ntr(tm, 1)\nTrain/Test error\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n10−4\n10−3\n10−2\n10−1\n1\n10\n102\n103\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nelz2\ntr(t, 1)\nFigure 4: Train/test error (right) when fitting data from a single index model, with h(z) = bφ(z) =\n(9/10)z + z2/2, τ = 0.3 and α = 0.3. Lines correspond to predictions from the DMFT (continuous:\ntrain error; dashed: test error). The inset on the right reports the value of the test error at the\nplateau, as a function of m (green line is the m →∞value). Right: Same data plotted versus t.\nwith limm→∞etr(ˆt/m; φ, γ0, m, α) = limm→∞ets(ˆt/m; φ, γ0, m, α) =: elz1(ˆt; φ, γ0, α).\nFor large scaled time ˆt, the error elz1(ˆt; φ, γ0, α) converges to the error of the best linear ap-\nproximation to f∗. This dynamical regime follows the qualitative predictions of NTK theory, and\nis essentially linear in the weights wi.\nSecond dynamical regime: t = Θ(1). This regime is completely analogous to the second dynamical\nregime with pure noise data and lazy initialization, cf. Section 2.2.2. Second layer weights do\nnot change significantly: γ(t) = γ0 + om(1), while first layer weights change significantly ∥wi(t) −\nwi(0)∥= Θ(1). However they change orthogonally to the latent subspace U and hence the test\nerror does not change: no actual learning takes place in this regime.\nMore formally, train and test error have well defined limits\nelz2\ntr (t; φ, γ0, α) := lim\nm→∞etr(t; φ, γ0, m, α) ,\nelz2\nts (t; φ, γ0, α) := lim\nm→∞ets(t; φ, γ0, m, α) .\n(2.12)\nHowever, elz2\nts (t; φ, γ0, α) is constant\nelz2\nts (t; φ, γ0, α) = lim\nˆt→∞\nelz1(ˆt; φ, γ0, α) = 1\n2\n\u0012\nτ 2 + ∥φ∥2 −∥∇bφ(0)∥2\nh′(0)\n+ γ2\n0(h(1) −h′(0))\n\u0013\n.\n(2.13)\nSince the wi’s move orthogonally to the latent space, their dynamics is equivalent (for large m) to\nthe one in the pure noise setting, modulo a redefinition of the covariance h. The right plot in Fig. 4\nillustrates this.\nThird dynamical regime: t = Θ(m). Also this dynamical regime is analogous to the one in the\ncase of pure noise data, cf. Sec. 2.2.2. Its qualitative properties depend whether or not γ0 is larger\nthan an interpolation threshold γ∗\nGF(α, φ, τ), which generalizes the threshold γ∗\nGF(α) = γ∗\nGF(α, 0, 1)\nintroduced in Sec. 2.2.2. Because of the equivalence between the two dynamics mentioned above,\nthe following relation exists:\nγ∗\nGF(α, φ, τ) =\n\u0010\nτ 2 + ∥φ∥2 −∥∇ˆφ(0)∥2\nh′(0)\n\u00111/2\nγ∗\nGF(α) .\n(2.14)\n14\n\n\n100\n101\n102\n103\nt\n100\n101\na\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\n100\n101\n102\n103\nt\n0.0\n0.2\n0.4\n0.6\n0.8\nTrain/Test\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nFigure 5: Training dynamics under a single-index model with h(q) = bφ(q) = (9/10)q+q3/6, τ = 0.3\nand α = 0.3. Left: second-layer weights. Right: train and test error. Symbols are empirical results\nfor SGD with actual two-layer neural networks with d = 200, n = αmd.\nLines correspond to\npredictions from the DMFT (on the right, continuous: train error; dashed: test error).\nFor γ0 > γ∗\nGF(α, φ, τ), interpolation is achieved during the second dynamical regime, no further\nevolution takes place. For γ0 < γ∗\nGF(α, φ, τ), we have, for any z ∈(0, ∞), as m →∞,\nγ(mz) = γlz3(z) + om(1),\netr(mz) = elz3\ntr (z) + om(1),\nets(mz) = elz3\nts (z) + om(1) ,\n(2.15)\nwith γlz3(z) growing to γ∗\nGF(α, φ, τ) ≈γ∗\nGF(α, φ, τ) and elz3\ntr (z) decreasing to 0 as z →∞. In other\nwords, interpolation is achieved on this third regime. Further elz3\nts (z) increase from γ0 approximately\nto the test error predicted in Eq. (2.13), with γ0 replaced by γ∗\nGF(α, φ, τ).\n2.3.2\nMean field initialization\nWe initialize a(0) = a0, independent of m and let second layer weights evolve according to gradient\nflow. By analyzing the DMFT equations, we observe two dynamical regimes for large m. We will\nrefer to them as ‘first’ and ‘third regime’ since they are in correspondence with the first and third\ndynamical regimes in the pure noise case, see Sec. 2.2.3. For an illustration, we refer to Figs. 5, 6.\nFirst dynamical regime: t = O(1).\nBoth first and second layer weights change by order one:\na(t) = a0 + Θ(1) and ∥wi(t) −wi(0)∥= Θ(1). and as a consequence test and train error decrease\nsignificantly. In this regime, the two errors remain close to each other and their evolution is well\ncaptured by the mean field theory of [MMN18, CB18], as specialized to the case of spherically\ninvariant distributions [BMZ24, ASKL23].\nNamely, limt→∞a(t) = amf1(t), limt→∞v(t) = vmf1(t), and DMFT reduces to a system of k + 1\nordinary differential equations for the k + 1 scalar variables (amf1(t), vmf1(t))\n∂tvmf1(t) = αamf1(t)Qvmf1(t)\n\u0010\n∇ˆφ(vmf1(t)) −amf1(t)h′(∥vmf1(t)∥2)vmf1(t)\n\u0011\n,\n∂tamf1(t) = α ˆφ(vmf1(t)) −αamf1(t)h(∥vmf1(t)∥2) ,\n(2.16)\n15\n\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n10−3\n10−2\n10−1\n1\n10\n102\na(t)/√m\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.001\n0.01\n0.1\n1\n10\n100\nv(t)\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nets −etr\na/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nLazy\nFigure 6: Left Panel: the second layer weights on the scale √m as a function of t/m. Curves appear\nto collapse on a master curve. The rose arrow denotes γ∗\nGF and the curves appear to converge to\nthat limit. Central panel: the projection of the first layer weights on the latent space in the single\nindex model as a function of time on timescales of order m. Right panel: the difference between test\nand train error as a function of the second layer weights on the scale √m. The finite m curve are\napproaching a scaling curve which coincides with the one obtained by evaluating the same quantity\nbut with a lazy initialization and fixed second layer weights.\nwhere Qv := Ik −vvT. As mentioned above, train and test error coincide in the large width limit\nlim\nm→∞etr(t) = lim\nm→∞ets(t) = emf1(t) ,\n(2.17)\nemf1(t) = 1\n2\n\u0002\nτ 2 + ∥φ∥2 −2amf1(t) ˆφ(vmf1(t)) + amf1(t)2h(∥vmf1(t)∥2)\n\u0003\n.\n(2.18)\nIn the case k = 1 and bφ(z) = h(z), we have that amf1 = 1, vmf1 = 1 is a fixed point of Eq. (2.16),\nand indeed the only fixed point with vmf1 > 0. If h′(0) > 0, then, we have (amf1(t), vmf1(t)) →(1, 1)\nas t →∞, and therefore test and train error converge to the Bayes error emf1(t) →τ 2/2. This is\nof course significantly smaller than the test error achieved with lazy initialization, cf. Sec. 2.3.1.\nThe separation between lazy and mean-field initialization is expected because feature learning takes\nplace in the mean field regime.\nInstability of the first dynamical regime. It is possible to compute O(1/m) corrections to the mean\nfield asymptotics described above (see Appendix). We obtain, in particular:\na(t) = amf1(t) + 1\nm˜a(t) + o(1/m) ,\nlim\nt→∞\n˜a(t)\nt\n= a∗∈(0, ∞) .\n(2.19)\nIf we assume this result holds beyond t = O(1), then we obtain a(t) ≈amf1(t) + a∗t/m for t ≫1,\nwhich suggests that the mean field asymptotics breaks down for t = Θ(m).\nThird dynamical regime: t = Ω(m). For t ≳m, we observe that the second layer weights grow to\nachieve a(t) ≍√m, the projection onto the latent space decreases to v(t) ≍1/√m, and train and\ntest error diverge, eventually achieving etr(t) ≈0 and test error significantly larger than the Bayes\nerror achieved earlier. We refer to this phenomenon as ‘feature unlearning.’\nDynamics on this time scale is illustrated by Fig. 6 that reports solutions of the DMFT equa-\ntions. We observe the increase of a(t) and decrease of v(t). Further, the increase in generalization\nerror ets(t) −etr(t) is directly related to a(t)/√m becoming of order 1.\nDenoting by t0(m; c) the time at which a(t) = c√m (for c a small constant), we expect the\nexistence of a window size w(m) such that\nlim\nm→∞\na\n\u0000t0(m; c) + z w(m)\n\u0001\n√m\n= γmf3(z) ,\nlim\nm→∞etr/ts\n\u0000t0(m; c) + z w(m)\n\u0001\n= emf3\ntr/ts(z) ,\n(2.20)\n16\n\n\nwhere γmf3(z), emf3\ntr (z), emf3\nts (z) are scaling functions describing the dynamics on this timescale. We\nexpect t0(m; c) = t∗(c)m+o(m), and w(m) ≲t0(m; c), but our numerical solutions are not sufficient\nto determine the precise scaling. On the other hand, it appears that at large times, the complexity\nconverges close the interpolation threshold (see left plot in Fig. 6):\nlim\nz→∞γmf3(z) = γ∗\nGF(α, φ, τ) ≈γ∗\nGF(α, φ, τ) .\n(2.21)\nFinally, the evolution of train and test error for a(t) ≍√m appears to match the behavior at\nfixed second-layer weights. Namely, in analogy with Section 2.2.4, we define two functions\nεmf\ntr/ts(γ) := lim\nm→∞etr/ts(t0(m; γ), m) .\n(2.22)\nWe observe that the limit curves (γ, εmf\ntr(γ)), (γ, εmf\nts(γ)), match closely asymptotic train and test\nerror obtained by fixing a(t) = γ√m, and not letting second-layer weight evolve. This confirms the\nhypothesis that γ(t) is a slow variabes, while others converge as if γ was fixed.\n3\nLower bounding the overfitting timescale\nIn this section we rigorously establish two results that confirm some elements of the scenario devel-\noped via DMFT the previous sections. The first result implies that (under mean field initialization)\noverfitting cannot take place too early.\nLemma 3.1. Under the GF dynamics (1.2), and the data distribution in the introduction, further\nassume |σ(0)|, ∥σ∥Lip ≤L, |φ(0)|, ∥φ∥Lip ≤L, ∥a(0)∥∞≤a0, for some a0 ≥1 and that the wi(0),\ni ≤m are independent of the data. Then, there exists a universal constant C0 and c = c(L, τ) > 0\nsuch that, with probability at least 1 −2 exp(−cn), the following holds for all t ≥0,\n∥a(t)∥∞≤a0 + a1 t ,\na1 := C0αL(τ + a0L) .\n(3.1)\nAs a consequence, if further ∥σ∥∞≤L, we have with the same probability\nR(a(t), W (t)) −b\nRn(a(t), W (t)) ≤C(α, L, τ)(a0 + a1t)2 ·\n1\n√m .\n(3.2)\nUnder mean field initialization, a0 is a fixed constant independent of m, n, d, and hence a1 is also\nbounded. As a consequence, the generalization error in Eq. (3.2) is small as long as t = o(m1/4).\nIn other words, under mean field initialization, overfitting takes place only on time scales diverging\nwith m, confirming the picture developed within DMFT.\nThe second result implies that, up to time-scale of order one, the dynamics is closely tracked\nby the mean field equations (2.16).\nProposition 3.2. Under the the GF dynamics (1.2), and the data distribution in the introduction,\nfurther assume that ∥φ∥∞, ∥φ∥Lip ≤L, ∥σ∥∞, σ′∥∞, ∥σ′∥Lip ≤L. Further assume the initialization\nai(0) = a0 for all i ≤m, (wi(0))i≤m ∼iid Unif(Sd−1). Then for any ε > 0 there exist constants c0\nc1, C depending on L, τ, α, ε such that, letting Tlb(m) = (c0 log m)1/3, the following happens with\nprobability at least 1 −2 exp(−c1(d ∧m)),\nsup\nt≤Tlb(m)\n1\nm\nm\nX\ni=1\n\u0010\n|ai(t) −amf1\ni\n(t)| + ∥vi(t) −vmf1\ni\n(t)∥\n\u0011\n≤Cmεn 1\n√m + 1\n√\nd\no\n,\n(3.3)\nsup\nt≤Tlb(m)\n\f\f\fR(a(t), W (t)) −ets(t)\n\f\f\f ≤Cmεn 1\n√m + 1\n√\nd\no\n.\n(3.4)\n17\n\n\n4\nDiscussion\nWe conclude by highlighting a few qualitative conclusions of our work, and how they address\nquestions raised in Section 1.1.\nIn the following remarks, we consider the overparametrization\nα = n/md as constant.\nInterpolation mechanism. In the current setting, the neural model complexity is proportional to\n∥a(t)∥1/√m = γ(t)+on(1). We observe two alternative scenario. If the complexity at initialization\nis large enough γ0 > γ∗\nGF(α), then the gradient flow rapidly converges to a near interpolator without\nsignificant change in γ(t).\nIf instead, γ0 < γ∗\nGF(α), then γ(t) grows to reach the interpolation\nthreshold at which point the training error converges to 0.\nAdiabatic evolution of model complexity. In the latter case, the complexity γ(t) evolves on\na slower time scale than other degrees of freedom. The dynamics on shorter timescales is well\napproximated by the one at fixed γ (given by the current value γ(t)). The generalization error\nbecomes of order one only when γ(t) is of order one.\nDecoupling of learning and overfitting.\nWhen γ0 = om(1), the above implies a large-m\ndecoupling between learning (which takes place on faster timescales, as long as γ(t) = om(1)),\nand overfitting (which takes place on slower timescales, when γ(t) = Ωm(1)). This has several\nimplications for the questions outlined in the introduction. Question Q3: Lazy initialization a(0) ≍\n√m leads to poor generalization because the feature-learning phase is skipped either partially or\naltogether. Question Q2: Training until intepolation is generally suboptimal. Question Q4: The\noptimal tradeoff is obtained at the end of the first phase. Further, at fixed overerparametrization\nn/md = α, overfitting starts later for larger models (questions Q5, Q6).\nOverfitting and feature unlearning. The above description points at a nonmonotonicity of\nthe model quality, which improves on short time scales, and deteriorates at larger time scales.\nReciprocally, early stopping acts as a regularization. While this phenomenon is well understood\nfor linear models [FHT00, YRC07], our analysis provides an analogous (quantitative) scenario for\ntraining neural network models. In particular, it clarifies the underlying mechanism: in the same\ndynamical regime in which network complexity grows (γ(t) becomes of order one), and training\nerror becomes negligible, the low-dimensional latent features are ‘unlearned’ (v(t) becomes of order\n1/√m).\nAcknowledgments\nThis work was supported by the NSF through award DMS-2031883, the Simons Foundation through\nAward 814639 for the Collaboration on the Theoretical Foundations of Deep Learning, and the ONR\ngrant N00014-18-1-2729. This work was supported by the French government under the France 2030\nprogram (PhOM - Graduate School of Physics) with reference ANR-11-IDEX-0003.\n18\n\n\nReferences\n[AAM22]\nEmmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz, The merged-staircase\nproperty: a necessary and nearly sufficient condition for sgd learning of sparse func-\ntions on two-layer neural networks, Conference on Learning Theory, PMLR, 2022,\npp. 4782–4887. 5, 7, 55\n[ACHL19]\nSanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo, Implicit regularization in deep\nmatrix factorization, Advances in Neural Information Processing Systems 32 (2019).\n4\n[ASKL23]\nLuca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro, From high-\ndimensional and mean-field dynamics to dimensionless odes: A unifying approach to\nsgd in two-layers networks, The Thirty Sixth Annual Conference on Learning Theory,\nPMLR, 2023, pp. 1199–1227. 15, 55, 64\n[BADG06]\nG´erard Ben Arous, Amir Dembo, and Alice Guionnet, Cugliandolo-kurchan equations\nfor dynamics of spin-glasses, Probability theory and related fields 136 (2006), no. 4,\n619–660. 8, 59\n[Bar96]\nPeter Bartlett, For valid generalization the size of the weights is more important than\nthe size of the network, Advances in neural information processing systems 9 (1996).\n9\n[BEG+22]\nBoaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril\nZhang, Hidden progress in deep learning: SGD learns parities near the computational\nlimit, Advances in Neural Information Processing Systems 35 (2022), 21750–21764. 5,\n7, 55\n[Ber01]\nNils\nBerglund,\nPerturbation\ntheory\nof\ndynamical\nsystems,\narXiv\npreprint\nmath/0111178 (2001). 9, 36\n[BES+22]\nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang,\nHigh-dimensional asymptotics of feature learning: How one gradient step improves the\nrepresentation, Advances in Neural Information Processing Systems 35 (2022), 37932–\n37946. 5, 7, 55\n[Bis95]\nChristopher M Bishop, Regularization and complexity control in feed-forward networks,\nProceedings International Conference on Artificial Neural Networks ICANN’95, 1995,\npp. 141–148. 4\n[BMZ24]\nRapha¨el Berthier, Andrea Montanari, and Kangjie Zhou, Learning time-scales in two-\nlayers neural networks, Foundations of Computational Mathematics (2024), 1–84. 15,\n55, 64, 82\n[BP22]\nBlake Bordelon and Cengiz Pehlevan, Self-consistent dynamical field theory of kernel\nevolution in wide neural networks, Advances in Neural Information Processing Systems\n35 (2022), 32240–32256. 8\n[CB18]\nLenaic Chizat and Francis Bach, On the global convergence of gradient descent for\nover-parameterized models using optimal transport, Advances in neural information\nprocessing systems 31 (2018). 6, 15, 55, 63, 64\n19\n\n\n[CB20]\n, Implicit bias of gradient descent for wide two-layer neural networks trained\nwith the logistic loss, Conference on learning theory, PMLR, 2020, pp. 1305–1338. 4\n[CCM21]\nMichael Celentano, Chen Cheng, and Andrea Montanari, The high-dimensional asymp-\ntotics of first order methods with random data, arXiv:2112.07572 (2021). 8, 33, 34, 83\n[CD95]\nLeticia F Cugliandolo and David S Dean, Full dynamical solution for a spherical spin-\nglass model, Journal of Physics A: Mathematical and General 28 (1995), no. 15, 4213.\n60\n[CHS93]\nAndrea Crisanti, Heinz Horner, and H J Sommers, The spherical p-spin interaction\nspin-glass model: the dynamics, Zeitschrift f¨ur Physik B Condensed Matter 92 (1993),\n257–271. 59\n[CK93]\nLeticia F Cugliandolo and Jorge Kurchan, Analytical solution of the off-equilibrium\ndynamics of a long-range spin-glass model, Physical Review Letters 71 (1993), no. 1,\n173. 12, 59, 60\n[COB19]\nLenaic Chizat, Edouard Oyallon, and Francis Bach, On lazy training in differentiable\nprogramming, Advances in neural information processing systems 32 (2019). 5\n[Cug23]\nLeticia F Cugliandolo, Recent applications of dynamical mean-field methods, Annual\nReview of Condensed Matter Physics 15 (2023). 84\n[DLL+19]\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, Gradient descent finds\nglobal minima of deep neural networks, International conference on machine learning,\nPMLR, 2019, pp. 1675–1685. 5\n[DLS22]\nAlexandru Damian, Jason Lee, and Mahdi Soltanolkotabi, Neural networks can learn\nrepresentations with gradient descent, Conference on Learning Theory, PMLR, 2022,\npp. 5413–5452. 5, 7, 55\n[FFRT20]\nGiampaolo Folena, Silvio Franz, and Federico Ricci-Tersenghi, Rethinking mean-field\nglassy dynamics and its relation with the energy landscape: The surprising case of the\nspherical mixed p-spin model, Physical Review X 10 (2020), no. 3, 031045. 12, 59, 60\n[FHT00]\nJerome Friedman, Trevor Hastie, and Robert Tibshirani, Additive logistic regression:\na statistical view of boosting (with discussion and a rejoinder by the authors), The\nannals of statistics 28 (2000), no. 2, 337–407. 18\n[FT22]\nYan V Fyodorov and Rashel Tublin, Optimization landscape in the simplest constrained\nrandom least-square problem, Journal of Physics A: Mathematical and Theoretical 55\n(2022), no. 24, 244008. 8\n[Fyo19]\nYan V Fyodorov, A spin glass model for reconstructing nonlinearly encrypted signals\ncorrupted by noise, Journal of Statistical Physics 175 (2019), 789–818. 8\n[GB10]\nXavier Glorot and Yoshua Bengio, Understanding the difficulty of training deep feed-\nforward neural networks, Proceedings of the thirteenth international conference on ar-\ntificial intelligence and statistics, JMLR Workshop and Conference Proceedings, 2010,\npp. 249–256. 3\n20\n\n\n[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Lin-\nearized two-layers neural networks in high dimension, The Annals of Statistics 49\n(2021), no. 2. 5\n[Hol13]\nMark Holmes, Introduction to perturbation methods, Springer, 2013. 9, 36\n[JGH18]\nArthur Jacot, Franck Gabriel, and Cl´ement Hongler, Neural tangent kernel: Conver-\ngence and generalization in neural networks, Advances in neural information processing\nsystems 31 (2018). 5\n[KD24]\nJaron Kent-Dobias, On the topology of solutions to random continuous constraint sat-\nisfaction problems, arXiv preprint arXiv:2409.12781 (2024). 8, 84\n[KU23a]\nPersia Jana Kamali and Pierfrancesco Urbani, Dynamical mean field theory for models\nof confluent tissues and beyond, SciPost Physics 15 (2023), no. 5, 219. 8, 33, 34, 44,\n84, 85\n[KU23b]\n, Stochastic gradient descent outperforms gradient descent in recovering a high-\ndimensional signal in a glassy energy landscape, arXiv preprint arXiv:2309.04788\n(2023). 8, 34, 84\n[LWM19]\nYuanzhi Li, Colin Wei, and Tengyu Ma, Towards explaining the regularization effect of\ninitial large learning rate in training neural networks, Advances in neural information\nprocessing systems 32 (2019). 4\n[Mau16]\nAndreas Maurer, A vector-contraction inequality for rademacher complexities, Algo-\nrithmic Learning Theory: 27th International Conference, Springer, 2016, pp. 3–17. 77,\n78\n[MB89]\nNelson Morgan and Herv´e Bourlard, Generalization and parameter estimation in feed-\nforward nets: Some experiments, Advances in neural information processing systems\n2 (1989). 4\n[MKUZ19]\nStefano Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova,\nPassed & spurious: Descent algorithms and local minima in spiked matrix-tensor mod-\nels, international conference on machine learning, PMLR, 2019, pp. 4333–4342. 8\n[MKUZ20]\nFrancesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborov´a,\nDynamical mean-field theory for stochastic gradient descent in gaussian mixture clas-\nsification, Advances in Neural Information Processing Systems 33 (2020), 9540–9550.\n32, 34\n[MMM22]\nSong Mei, Theodor Misiakiewicz, and Andrea Montanari, Generalization error of ran-\ndom feature and kernel methods: hypercontractivity and kernel matrix concentration,\nApplied and Computational Harmonic Analysis 59 (2022), 3–84. 5\n[MMN18]\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen, A mean field view of the land-\nscape of two-layer neural networks, Proceedings of the National Academy of Sciences\n115 (2018), no. 33, E7665–E7671. 5, 6, 15, 55, 63, 64\n[MPV87]\nMarc M´ezard, Giorgio Parisi, and Miguel Angel Virasoro, Spin glass theory and beyond,\nvol. 9, World Scientific, 1987. 8\n21\n\n\n[MS23]\nAndrea Montanari and Eliran Subag, Solving overparametrized systems of random\nequations: I. model and algorithms for approximate solutions, arXiv:2306.13326 (2023).\n8, 84\n[MS24]\n, On Smale’s 17th problem over the reals, arXiv preprint arXiv:2405.01735\n(2024). 8\n[MU22]\nFrancesca Mignacco and Pierfrancesco Urbani, The effective noise of stochastic gradi-\nent descent, Journal of Statistical Mechanics: Theory and Experiment 2022 (2022),\nno. 8, 083405. 34\n[PS08]\nGrigorios A Pavliotis and Andrew Stuart, Multiscale methods: averaging and homog-\nenization, vol. 53, Springer Science & Business Media, 2008. 11, 13\n[RVE22]\nGrant Rotskoff and Eric Vanden-Eijnden, Trainability and accuracy of artificial neu-\nral networks: An interacting particle system approach, Communications on Pure and\nApplied Mathematics 75 (2022), no. 9, 1889–1935. 6, 55, 64\n[Sel24]\nMark Sellke, The threshold energy of low temperature langevin dynamics for pure spher-\nical spin glasses, Communications on Pure and Applied Mathematics 77 (2024), no. 11,\n4065–4099. 60\n[SHN+18]\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Sre-\nbro, The implicit bias of gradient descent on separable data, The Journal of Machine\nLearning Research 19 (2018), no. 1, 2822–2878. 4\n[SS95]\nDavid Saad and Sara Solla, Dynamics of on-line gradient descent learning for multi-\nlayer neural networks, Advances in neural information processing systems 8 (1995).\n5\n[SSBD14]\nShai Shalev-Shwartz and Shai Ben-David, Understanding machine learning: From\ntheory to algorithms, Cambridge University Press, 2014. 3\n[Sub23]\nEliran Subag, Concentration for the zero set of random polynomial systems, arXiv\npreprint arXiv:2303.11924 (2023). 8\n[Tal10]\nMichel Talagrand, Mean field models for spin glasses: Volume i: Basic examples,\nvol. 54, Springer Science & Business Media, 2010. 8\n[Urb23]\nPierfrancesco Urbani, A continuous constraint satisfaction problem for the rigidity\ntransition in confluent tissues, Journal of Physics A: Mathematical and Theoretical\n56 (2023), no. 11, 115003. 8, 84\n[VBN22]\nNikhil Vyas, Yamini Bansal, and Preetum Nakkiran, Limitations of the ntk for under-\nstanding generalization in deep learning, arXiv preprint arXiv:2206.10012 (2022). 51,\n52\n[Ver18]\nRoman Vershynin, High-dimensional probability: An introduction with applications in\ndata science, vol. 47, Cambridge university press, 2018. 9, 76\n[VK85]\nNicolaas Godfried Van Kampen, Elimination of fast variables, Physics Reports 124\n(1985), no. 2, 69–160. 11\n22\n\n\n[VSP+17]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin, Attention is all you need, Advances in\nNeural Information Processing Systems (NeurIPS), vol. 30, Curran Associates, Inc.,\n2017, pp. 5998–6008. 5\n[WGL+20]\nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese,\nItay Golan, Daniel Soudry, and Nathan Srebro, Kernel and rich regimes in over-\nparametrized models, Conference on Learning Theory, PMLR, 2020, pp. 3635–3673.\n4\n[WRS+17]\nAshia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht,\nThe marginal value of adaptive gradient methods in machine learning, Advances in\nneural information processing systems 30 (2017). 4\n[YLWJ19]\nKaichao You, Mingsheng Long, Jianmin Wang, and Michael I Jordan, How does learn-\ning rate decay help modern neural networks?, arXiv preprint arXiv:1908.01878 (2019).\n4\n[YRC07]\nYuan Yao, Lorenzo Rosasco, and Andrea Caponnetto, On early stopping in gradient\ndescent learning, Constructive Approximation 26 (2007), no. 2, 289–315. 18\n[ZBH+21]\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals,\nUnderstanding deep learning (still) requires rethinking generalization, Communications\nof the ACM 64 (2021), no. 3, 107–115. 9\n[ZJ21]\nJean Zinn-Justin, Quantum field theory and critical phenomena, vol. 171, Oxford uni-\nversity press, 2021. 84\n23\n\n\nA\nSetting\nWe recall for reference some basic definitions and notations.\nWe consider the 2-layer network\ndefined by\nf(x; a, W ) = 1\nm\nm\nX\ni=1\nai σ(⟨wi, x⟩) .\n(A.1)\nThroughout, we assume an offset to be subtracted so that Eσ(G) = 0, for G ∼N(0, 1). The network\ninput x is a d-dimensional real vector and the output is a scalar variable. The parameters of the\nnetwork are the weights of the first layer collected in the matrix W defined as\nW =\n\n\n\n\n\n\nw1\nw2\n·\n·\nwm\n\n\n\n\n\n\n∈Rm×d ,\nwi ∈Rd .\n(A.2)\nWe will assume that ∥wi∥2 = 1. The weights of the second layer are instead (a1, . . . , am) and are\nreal, possibly unbounded, variables.\nWe consider a dataset of n points independent and identically distributed (yi, xi)i≤n where\nxi ∼N(0, Id), and the labels yi are generated according to the following k-index models:\nyi = φ(U Txi) + εi .\n(A.3)\nTherefore, labels depend on the projection of the covariates on a fixed subspace U ∈Rd×k, with\nU TU = Ik (there is no loss of generality in assuming U orthogonal). Efficient learning requires to\nestimate this subspace. Since we consider learning with square loss, we assume\n∥φ∥2\n2 := E\n\b\nφ(U Txi)2\t\n= E{φ(g)2} ,\nwhere g ∼N(0, Ik). We refer to the case φ = 0 as the ‘pure noise case’ or ‘pure noise data’.\nWe now discuss the covariance structure of the network given by Eq. (A.1). For two sets of\nweights (a1, W 1) and (a2, W 2) we have\nE\n\b\nf(x; a1, W 1) f(x; a2, W 2)\n\t\n= 1\nm2\nm\nX\ni,j=1\na1,ia2,jh (⟨w1,i, w2,j⟩) ,\n(A.4)\nwhere\nh(q) = E{σ(G1)σ(G2)}\n(A.5)\nfor (G1, G2) centered jointly Gaussian with E{G2\ni } = 1, E{G1G2} = q.\nFurthermore we have that:\nE{f(x; a, W ) φ(U Tx)} = 1\nm\nm\nX\ni=1\nai bφ(U Twi) .\n(A.6)\n24\n\n\nwhere bφ is given by\nbφ(v) := E\nn\nσ(⟨v, G⟩+\np\n1 −∥v∥2G0)φ(G)\no\n,\n(A.7)\nfor G ∼N(0, Ik) independent of G0 ∼N(0, 1).\nWe consider Gaussian process fg(a, W ), φg with the same covariance function defined above\nand define the empirical risk under Gaussian approximation as\nb\nRg\nn(a, W ) = 1\n2n\nn\nX\ni=1\n\u0000fg\ni (a, W ) −φg\ni −εi)2\n(A.8)\n= 1\n2n\n\r\rf g(a, W ) −φg −ε\n\r\r2 ,\nwhere f g(· · · ) = (fg\ni (· · · ) : i ≤n), φg = (φg\ni : i ≤n), ε = (εi : i ≤n) are vectors containing n i.i.d.\ncopies of the above processes. We will also write yg = φg + ε.\nGiven a model with estimated parameters ˆa, c\nW , the test error is given by\nR(ˆa, c\nW ) = 1\n2E\n\b\u0000fg(ˆa, c\nW ) −φg −ε\n\u00012\t\n(A.9)\n= 1\n2E\n\b\u0000f(x, ˆa, c\nW ) −φ(U Tx) −ε\n\u00012\t\n,\nwhere the expectation in the first line is over a triple (fg, φg, ε) independent of the data, and in\nthe second line with respect to x. The two expectations coincide because they depend uniquely on\nthe second moments of these processes.\nWe are interested in studying the gradient flow dynamics in the random landscape b\nRn(a, W )\n˙a(t) = −n\nd∇a b\nRn(a(t), W (t)) ,\n˙wi(t) = −n\nd∇wi b\nRn(a(t), W (t)) −νi(t)wi(t)\n∀i = 1, . . . , m .\n(A.10)\nThe Lagrange multipliers νi are added to enforce the spherical constraint ∥wi(t)∥2 = 1. While we\nconsider the case of normalized first-layer weights, our approach can be generalized to unconstrained\nweights or to include weight decay (ridge regularization). As explained in the main text, we will\nreplace this by gradient flow in the Gaussian model b\nRg\nn(a, W ). We refer to Section I for a discussion\nof DMFT in the original non-Gaussian model.\nIn our analysis we will always consider the proportional asymptotics\nn, d →∞,\nn\nd →α ∈(0, ∞) .\n(A.11)\nWe typically index sequences and limits by n, but it is understood that d = d(n) →∞as well.\nAfter n, d →∞proportionally, we will consider the large network asymptotics m →∞at fixed\nα = α/m.\nIn the following we will drop the superscript g and write, for instance\nb\nRn(a, W ) instead of\nb\nRg\nn(a, W ) whenever clear from the context. All of our analytical predictions (except for Section 3)\nare obtained within the Gaussian model.\n25\n\n\nB\nDynamical Mean Field Theory (DMFT)\nIn this section we state the results of Dynamical Mean Field Theory (DMFT). We will outline a\nheuristic derivation in Section J. We first introduce the general DMFT equations in Section B.1\nand the corresponding predictions for certain observable of interest in Section B.2. These are a set\nof Θ(m2) integro-differential equations in as many unknown functions.\nWe then specialize these equations to the case of a symmetric initialization, in which wi(0) ∼\nUnif(Sd−1) and ai(0) = a0 for all i ≤m, see Section B.3 In this case, the dynamics is characterized\nby a set of k + 3 equations which are stated in Sections B.4 and B.5.\nB.1\nGeneral DMFT equations\nLet an\ni (t), wn\ni (t), νn\ni (t) the the solution of Eq (A.10) when the dynamics is initialized at non-\nrandom an\ni (0) = a0,i, i ≤n and possibly random, wn\ni (0) such that ⟨wn\ni (0), wn\nj (0)⟩→C0\nij for\ni, j ≤n, U Twn\ni (0) →v0\ni for i ≤n. While random, the wn\ni (0) are assumed here to be independent\nof the random processes f g, φg, ε.\nFor t, s ≥0 consider the quantities\nCn\nij(t, s) := ⟨wn\ni (t), wn\nj (s)⟩,\nvn\ni (t) := U Twn\ni (t) .\n(B.1)\nThen DMFT predicts that these quantities have a well defined non-random limit as n, d →∞,\nCij(t, s) =\nlim\nn,d→∞Cn\nij(t, s) ,\nvi(t) =\nlim\nn,d→∞vn\ni (t) ,\nai(t) =\nlim\nn,d→∞an\ni (t) ,\n(B.2)\nwhere the limits are understood to hold in almost sure sense. These limits are the unique solution\nof a set of integro-differential equations in the unknowns {Cij(t, s), Rij(t, s), vi(t), ai(t) : i, j ≤m},\nwhich we next state as three sets: (1) Dynamical equations; (2) Equations for auxiliary functions;\n(3) Boundary conditions. Before that, we mention some constraints that need to be satisfied by\nthe solution of these equations.\n(0) Constraints.\nThe functions Cij(t, s), Rij(t, s) satisfy:\nCii(t, t) = 1\n∀0 ≤t ,\n(B.3)\nCij(t, s) = Cji(s, t)\n∀0 ≤t, s ,\n(B.4)\nRij(t, s) = 0\n∀0 ≤t < s .\n(B.5)\nThe first condition in particular implies the following useful relation:\ndCij(t, t)\ndt\n= lim\nt′→t\n\u0014∂Cij(t, t′)\n∂t\n+ ∂Cji(t, t′)\n∂t\n\u0015\n.\n(B.6)\nWe refer to the property (B.5) (and similar ones for R functions appearing below) as ‘causality\nconstraint.’\n(1) Dynamical equations.\nThese equations determine the dynamics of {Cij(t, s), Rij(t, s), vi(t), ai(t) :\ni, j ≤m}, and involve the auxiliary functions (memory kernels) MC\nij (t, s), MR\nij (t, s) and (Lagrange\n26\n\n\nmultipliers) νi(t) (the last equations assume implicitly ta > tb):\ndai(t)\ndt\n= −α\nm\nZ t\n0\nRA(t, s)\n\"\n1\nm\nm\nX\nl=1\nal(s)h (Cli(s, t)) −ˆφ(vi(t))\n#\nds\n(B.7)\n−α\nm\nZ t\n0\nCA(t, s) 1\nm\nm\nX\nl=1\nal(s)h′\u0000Cli(s, t)\n\u0001\nRil(t, s) ds ,\ndvi(t)\ndt\n= −νi(t)vi(t) + α\nmai(t)∇ˆφ(vi(t))\nZ t\n0\nRA(t, s) ds −1\nm\nm\nX\nj=1\nZ t\n0\nMR\nij (t, s) vj(s) ds , (B.8)\n∂Cij(ta, tb)\n∂ta\n= −νi(ta)Cij(ta, tb) + α\nmai(ta)⟨∇ˆφ(vi(ta)), vj(tb)⟩\nZ ta\n0\nRA(ta, s) ds\n(B.9)\n−1\nm\nm\nX\nl=1\nZ ta\n0\nMR\nil (ta, s) Clj(s, tb) ds −1\nm\nm\nX\nl=1\nZ tb\n0\nMC\nil (ta, s) Rjl(tb, s)ds ,\n∂Rij(ta, tb)\n∂ta\n= −νi(ta)Rij(ta, tb) + δijδ(ta −tb) −1\nm\nm\nX\nl=1\nZ ta\ntb\nMR\nil (ta, s) Rlj(s, tb) ds .\n(B.10)\nWe point out that the δ(ta −tb) in the last equation (together with Eq. (B.5)) has to be interpreted\nas follows: Rij(t, t′) = 0 for t < t′ while, for ε > 0, Rij(t + ε, t) = δij + oε(1).\nEquations (B.9) and (B.10) can also be written in terms of an effective stochastic process in\nRm: we(t) = (we\ni (t) : i ≤m). This is defined as the solution of the following set of ODEs (for\ni ∈{1, . . . , m}):\ndwe\ni (t)\ndt\n= −νi(t)we\ni (t) + αai(t)⟨∇bφ(v(t)), v(t′)⟩\nZ t\n0\nRA(t, s) ds\n(B.11)\n−1\nm\nm\nX\nl=1\nZ t\n0\nMR\nil (t, s)we\nl (s) ds + ηi(t) + bi(t) ,\n(B.12)\nwe\ni (0) ∼N(0, 1) ,\n(B.13)\nwhere (ηi(t) : i ≤m) is a centered Gaussian process with covariance\nE[ηi(t)ηj(t′)] = −1\nmMC\nij (t, t′) .\n(B.14)\nDefine b(t) = (bi(t) : i ≤m). The solution of Eqs. (B.9) and (B.10) can be written as\nCij(t, t′) = lim\nb→0 E\n\u0002\nwi(t)wj(t′)\n\u0003\n,\n(B.15)\nRij(t, t′) = lim\nb→0\nδE[wi(t)]\nδbj(t′)\n.\n(B.16)\nIn fact the stochastic process of Eq. (B.11) is expected to describe the limit distribution of the\nsecond-layer weights W (t). Namely, for i ≤d, define ˜wi(t) = W (t)ei ∈Rm be a vector containing\nthe i-th coordinate of each neuron. Then, for any fixed i and any T,\n( ˜wi(t) : 0 ≤t ≤T) d⇒(we(t) : 0 ≤t ≤T) .\n(B.17)\nHere\nd⇒denotes convergence in distribution as n, d →∞, in C([0, T], Rm).\n27\n\n\n(2) Equations for auxiliary functions.\nThe memory kernels MR and MC are defined by\nMR\nij (t, s) = α\nm\n\u0002\nRA(t, s)h′(Cij(t, s)) + CA(t, s)h′′(Cij(t, s))Rij(t, s)\n\u0003\nai(t)aj(s) ,\nMC\nij (t, s) = α\nmCA(t, s)h′(Cij(t, s))ai(t)aj(s) .\n(B.18)\nwhere the functions RA and CA satisfy the symmetry properties CA(t, s) = CA(s, t) and RA(t, s) = 0\nfor t < s, and are the unique solution\nZ t\nt′ [δ(t −s) + ΣR(t, s)] RA(s, t′) ds = δ(t −t′) ,\nZ t\n0\n[δ(t −s) + ΣR(t, s)] CA(s, t′) ds +\nZ t′\n0\nΣC(t, s)RA(t′, s) ds = 0 ,\n(B.19)\nwhere\nΣC(t, s) := τ 2 + ∥φ∥2 + 1\nm2\nm\nX\ni,j=1\nai(t)aj(s)h\n\u0000Cij(t, s)\n\u0001\n−1\nm\nm\nX\nl=1\nal(t) ˆφ(vl(t)) −1\nm\nm\nX\nl=1\nal(s) ˆφ(vl(s)) ,\nΣR(t, s) := 1\nm2\nm\nX\ni,j=1\nai(t)aj(s)h′\u0000Cij(t, s)\n\u0001\nRij(t, s) .\n(B.20)\nThe Lagrange multipliers νi(t) have to be fixed to enforce the constraint Cii(t, t) = 1 which follows\nfrom wα ∈Sd−1. The corresponding equations are\nνi(ta) = α\nkmai(ta)⟨vi(ta), ∇ˆφ(vi(ta))⟩\nZ ta\n0\nRA(ta, s) ds\n(B.21)\n−1\nm\nm\nX\nj=1\nZ ta\n0\nMR\nij (ta, s) Cij(s, ta) ds −1\nm\nm\nX\nj=1\nZ ta\n0\nMC\nij (ta, s) Rji(ta, s) ds .\n(3) Boundary conditions.\nThe dynamical equations (B.7) to (B.10) can be integrated from a\nset of initial conditions that reflect initial conditions of the GF dynamics:\nvi(0) = v0\ni ,\nai(0) = a0\ni\n∀i ∈{1, . . . , m} ,\nCij(0, 0) = C0\nij\n∀i, j ∈{1, . . . , m} ,\nRij(0, 0) = 0\n∀i, j ∈{1, . . . , m} .\n(B.22)\nB.2\nExpressions for train and test error\nThe asymptotics of many quantities of interest can be expressed in terms of the solutions of the\nDMFT equations stated in the last section. In particular, the train error b\nRn(W (t), a(t)) and test\nerror R(W (t), a(t)) at time t have well defined limits under the proportional asymptotics:\nlim\nn→∞\nb\nRg\nn(W (t), a(t)) = etr(t) ,\nlim\nn→∞Rg(W (t), a(t)) = ets(t) .\n(B.23)\n28\n\n\nThe functions etr(t) ets(t) are given by\netr(t) = −1\n2CA(t, t) ,\n(B.24)\nets(t) = 1\n2\nn\nτ 2 + 1\nk∥φ∥2 + 1\nm2\nm\nX\ni,j=1\nh\n\u0000Cij(t, t)\n\u0001\n−2\nm\nm\nX\ni=1\nˆφ(vi(t))\no\n(B.25)\nMore generally, CA(t, s) gives the asymptotics of the correlation of residuals:\nlim\nn→∞\n1\nn\n\n∆(t), ∆(s)\n\u000b\n= −CA(t, s) ,\n(B.26)\n∆(t) := yg −f g(a(t), W (t)) .\n(B.27)\nwhere we recall that yg = φg + ε.\nB.3\nSymmetric initialization and solutions\nAs anticipated, we consider the uninformative initialization wn\ni (0) ∼Unif(Sd−1) and an\ni (0) = a0 for\nall i ≤m. This results in the following initialization for the DMFT equations of\nvi(0) = v0\ni = 0\n∀i ∈{1, . . . , m} ,\nCi̸=j(0, 0) = C0\ni̸=j = 0\n∀i ̸= j, i, j ∈{1, . . . , m} ,\nCii(0, 0) = C0\nii = 1\n∀i ∈{1, . . . , m} .\n(B.28)\nThis initialization is invariant under permutations of the m neurons. Since the DMFT equations\nof Section B.1 are equivariant under such permutations, their solution is also invariant under\npermutations. This means that it takes the form:\nCij(t, t′) =\n(\nCd(t, t′)\nif i = j,\nCo(t, t′)\nif i ̸= j,\nRij(t, t′) =\n(\nRd(t, t′)\nif i = j,\nRo(t, t′)\nif i ̸= j,\n(B.29)\nvi(t) = v(t) ,\nνi(t) = ν(t) ,\nai(t) = a(t)\n∀i .\n(B.30)\nAs a consequence, the memory kernels in Eq. (B.18) take the form\nMC\nij (t, t′) =\n(\nMC\nd (t, t′)\nif i = j,\nMC\no (t, t′)\nif i ̸= j,\nMR\nij (t, t′) =\n(\nMR\nd (t, t′)\nif i = j,\nMR\no (t, t′)\nif i ̸= j. .\n(B.31)\nWe will refer to the reduced DMFT under symmetry as to the SymmDMFT .\nB.4\nDMFT equations for symmetric initialization (SymmDMFT )\n(1) Dynamical equations.\nSubstituting the ansats of the previous section in the equations of\nSection B.1, we obtain the following equations for the functions a(t), v(t), Cd(t, s), Co(t, s), Rd(t, s),\nRo(t, s):\nda\ndt (t) = α\nm ˆφ(v(t))\nZ t\n0\nRA(t, s) ds\n(B.32)\n29\n\n\n−α\nm\nZ t\n0\nRA(t, s)a(s)\n\u0014 1\nmh(Cd(t, s)) + m −1\nm\nh(Co(t, s))\n\u0015\nds\n−α\nm\nZ t\n0\nCA(t, s)a(s)\n\u0014 1\nmh′(Cd(t, s))Rd(t, s) + m −1\nm\nh′(Co(t, s))Ro(t, s)\n\u0015\nds ,\ndv\ndt (t) = −ν(t)v(t) + α\nm∇ˆφ(v(t))a(t)\nZ t\n0\nRA(t, s) ds\n(B.33)\n−1\nm\nZ t\n0\nh\nM(d)\nR (t, s) + (m −1)M(o)\nR (t, s)\ni\nv(s) ds ,\n∂tCd(t, t′) = −ν(t)Cd(t, t′) + α\nm⟨∇ˆφ′(v(t)), v(t′)⟩a(t)\nZ t\n0\nRA(t, s) ds\n(B.34)\n−1\nm\nZ t\n0\nh\nM(d)\nR (t, s)Cd(t′, s) + (m −1)M(o)\nR (t, s)Co(t′, s)\ni\nds\n−1\nm\nZ t′\n0\nh\nM(d)\nC (t, s)Rd(t′, s) + (m −1)M(o)\nC (t, s)Ro(t′, s)\ni\nds ,\n∂tCo(t, t′) = −ν(t)Co(t, t′) + α\nm⟨∇ˆφ(v(t)), v(t′)⟩a(t)\nZ t\n0\nRA(t, s) ds\n(B.35)\n−1\nm\nZ t\n0\nh\nM(d)\nR (t, s)Co(t′, s) + M(o)\nR (t, s)Cd(t′, s) + (m −2)M(o)\nR (t, s)Co(t′, s)\ni\nds\n−1\nm\nZ t′\n0\nh\nM(d)\nC (t, s)Ro(t′, s) + M(o)\nC (t, s)Rd(t′, s) + (m −2)M(o)\nC (t, s)Ro(t′, s)\ni\nds ,\n∂tRd(t, t′) = −ν(t)Rd(t, t′) + δ(t −t′)\n(B.36)\n−1\nm\nZ t\nt′\nh\nM(d)\nR (t, s)Rd(s, t′) + (m −1)M(o)\nR (t, s)Ro(s, t′)\ni\nds ,\n∂tRo(t, t′) = −ν(t)Ro(t, t′) −1\nm\nZ t\nt′\nh\nM(d)\nR (t, s)Ro(s, t′) + M(o)\nR (t, s)Rd(s, t′)\n(B.37)\n+(m −2)M(o)\nR (t, s)Ro(s, t′)\ni\nds .\n(2) Equations for auxiliary functions.\nThe memory kernels M(s)\nR (t, s), M(o)\nR (t, s) and M(s)\nC (t, s),\nM(o)\nC (t, s) are given by:\nM(d)\nR (t, s) = α\nma(t)a(s)\n\u0002\nRA(t, s)h′(Cd(t, s)) + CA(t, s)h′′(Cd(t, s))Rd(t, s)\n\u0003\n,\n(B.38)\nM(o)\nR (t, s) = α\nma(t)a(s)\n\u0002\nRA(t, s)h′(Co(t, s)) + CA(t, s)h′′(Co(t, s))Ro(t, s)\n\u0003\n,\n(B.39)\nM(d)\nC (t, s) = α\nma(t)a(s)CA(t, s)h′(Cd(t, s)) ,\n(B.40)\nM(o)\nC (t, s) = α\nma(t)a(s)CA(t, s)h′(Co(t, s)) .\n(B.41)\nFurther, CA(t, s), RA(t, s) are given by the same equations (B.19), where ΣC, ΣR are simplified\n30\n\n\nas follows:\nΣC(t, s) = τ 2 + ∥φ∥2 −a(t) ˆφ(v(t)) −a(s) ˆφ(v(s)) + a(t)a(s)\nm\nh(Cd(t, s))\n+ m −1\nm\na(t)a(s)h(Co(t, s))\nΣR(t, s) = a(t)a(s)\nm\nh′(Cd(t, s))Rd(t, s) + m −1\nm\na(t)a(s)h′(Co(t, s))Ro(t, s)\n(B.42)\nFinally, the Lagrange multipliers are determined by\nν(t) = α\nm⟨∇ˆφ(v(t)), v(t)⟩a(t)\nZ t\n0\nRA(t, s) ds\n−1\nm\nZ t\n0\nh\nM(s)\nR (t, s)Cd(t, s) + (m −1)M(o)\nR (t, s)Co(t, s)\ni\nds\n−1\nm\nZ t\n0\nh\nM(s)\nC (t, s)Rd(t, s) + (m −1)M(o)\nC (t, s)Ro(t, s)\ni\nds .\n(B.43)\n(3) Boundary conditions.\nAs anticipated the SymmDMFT is initialized as\nv(0) = 0,\nCd(0, 0) = 1\nCo(0, 0) = 0 .\n(B.44)\nB.5\nExpressions for train and test error under symmetric initialization\nThe general expression for train and test error given in Section B.2 specialize to:\netr(t) = −1\n2CA(t, t) ,\n(B.45)\nets(t) = 1\n2\n\u0014\nτ 2 + ∥φ∥2 −2a(t) ˆφ(v(t)) + 1\nma2(t)h(1) + m −1\nm\na2(t)h(Co(t, t))\n\u0015\n.\n(B.46)\nC\nNumerical integration of the DMFT equations\nC.1\nIntegration technique\nWe integrate the SymmDMFT equations (B.32) to (B.37) using a standard Euler discretization.\nNamely, we discretize time on an equi-spaced grid t ∈T := {0, η, 2η, . . . } and approximate deriva-\ntives by differences and integrals by sums on this grid. As an example, Eq. (B.32) is replaced\nby\na(t + η) −a(t)\nη\n= α\nm ˆφ(v(t))\nX\ns∈T,s≤t\nRA(t, s) η\n(C.1)\n−α\nm\nX\ns∈T,s≤t\nRA(t, s)a(s)\n\u0014 1\nmh(Cd(t, s)) + m −1\nm\nh(Co(t, s))\n\u0015\nη\n−α\nm\nX\ns∈T,s≤t\nCA(t, s)a(s)\n\u0014 1\nmh′(Cd(t, s))Rd(t, s) + m −1\nm\nh′(Co(t, s))Ro(t, s)\n\u0015\nη .\n31\n\n\nOf course, the solution of this system of difference equation does not coincide with the solution of\nthe original equations (B.32) to (B.37), and in this section we will write a(t; η), Co(t, s; η) and so\non to emphasize the distinction.\nEquations (B.42) can be directly interpreted as determining ΣC(t, s) and ΣR(t, s) on the grid\nt, s ∈T. Finally, we discretize Eq. (B.19) as\nX\ns∈T\n[1t=s + ΣR(t, s)η] RA(s, t′) = 1\nη1t=t′ ,\nX\ns∈T\n[1t=s + ΣR(t, s)η] CA(s, t′) +\nX\ns∈T\nΣC(t, s)RA(t′, s)η = 0 .\n(C.2)\nNote that we dropped the integration limits here, since they are enforced by the causality constraints\nimplying ΣR(t, s) = 0, RA(t, s) = 0 for t < s. Defining the matrices ΣR = (ΣR(t, s) : t, s ∈T), and\nsimilarly for ΣC, CA, RA, we can rewrite (C.2) as\n[I + ηΣR] RA = 1\nηI ,\n(C.3)\n[I + ηΣR] CA + ηΣCRA = 0 .\n(C.4)\nWe truncate these matrices (which are infinite) to a maximum time T (e.g., redefine ΣR = (ΣR(t, s) :\nt, s ∈T, s, t ≤T)) and solve these equations by matrix inversion:\nRA = 1\nη\n\u0000I + ηΣR\n\u0001−1 ,\n(C.5)\nCA = −\n\u0000I + ηΣR\n\u0001−1ΣC\n\u0000I + ηΣR\n\u0001−1 .\n(C.6)\nWe denote by a(t; η), v(t; η), Co(t, s; η), Cd(t, s; η), Ro(t, s; η), Rd(t, s; η), the functions obtained\nvia the Euler integration scheme. We will assume that this solution is interpolated continuously\nfor t, s ̸∈T. For instance, for i, j ∈N a, b ∈[0, 1), we let\nCd((i + a)η, (j + b)η; η) =(1 −a)(1 −b) Cd(iη, jη; η) + a(1 −b) Cd((i + 1)η, jη; η)\n(C.7)\n+ (1 −a)b Cd((i + 1)η, jη; η) + ab Cd((i + 1)η, (j + 1)η; η) .\nFinally, while we described the discretization procedure for the SymmDMFT , the discussion\nabove applies verbatimly for the full DMFT of Section B.1.\nThe DMFT equations and their symmetric specialization have a causal structure which means\nthat they can be integrated by progressively by increasing T.\nFurthermore there is no self-\nconsistency condition in the integration scheme at variance with the non-Gaussian settings, see\nfor example [MKUZ20]. This simplification allows to investigate the long time behavior of the\ndynamics in a numerical, rather efficient, way.\nC.2\nAccuracy of the numerical integration scheme\nThe discretization of DMFT is expected to converge to the actual solution with errors of order η.\nNamely, we expect\nCd(t, t′; η) = Cd(t, t′) + O(η) ,\nCo(t, t′; η) = Co(t, t′) + O(η) ,\n(C.8)\n32\n\n\nand similarly for the other functions.\nWe refer to [CCM21] for related examples in which the\nconvergence was proved rigorously, and to [KU23a] for an empirical study in a closely related\nmodel.\nIn order to test the accuracy of our approach, and the correctness of the DMFT equations,\nwe simulated the gradient descent (GD) dynamics for the Gaussian model. Namely, we generate\nrealizations of the process f g(a, W ) = (fg\ni (a, W ) : i ≤n) with the prescribed covariance (A.4),\nand the vector φg = (φg\ni : i ≤n) with same covariance as in Eq. (A.6) (see Section C.4.) We define\nb\nRn(a, W ) via Eq. (A.8) and implement the following GD iteration\na(t + η) = a(t) −ηGDn\nd\n∇a b\nRn(a(t), W (t)) ,\nwi(t + η) = P Sd−1\n\u0010\nwi(t) −ηGDn\nd\n∇wi b\nRn(a(t), W (t))\n\u0011\n,\n(C.9)\nwhere P Sd−1 is the projector to the unit sphere, i.e. P Sd−1(x) = x/∥x∥if x ̸= 0 and P Sd−1(0) = 0.\nNote that the trajectories of Eq. (C.9) depend on the sample size n (and hence the dimension\nd = dn) and the stepsize ηGD. We denote them by an(t; ηGD) W n(t; ηGD).\nWe expect the GD trajectories defined by Eq. (C.9) approach the GF trajectories defined by\nEq. (A.10) as ηGD →0 uniformly in n, d. Namely,\nlim\nηGD→0 lim sup\nn,d→∞\n∥W n(t; ηGD) −W n(t)∥F = 0 ,\n(C.10)\nlim\nηGD→0 lim sup\nn,d→∞\n∥an(t; ηGD) −an(t)∥2 = 0 ,\n(C.11)\nwhere the limits are understood to hold in probability for any fixed t. Informally, for fixed small\nηGD, GD dynamics is a good approximation to GF dynamics, irrespective of the dimension.\nWe generate several realizations of the processes f g, φg, and of the gradient descent trajectories\n(C.9). We average observables of interest over these realizations and compare these with the Euler\ndiscretization of the DMFT equations. For instance, consider the correlation functions Cij(t, s).\nThen we can compare:\n• Cn\nij(t, s; ηGD) = E⟨wn\ni (t; ηGD), wn\nj (s; ηGD)⟩where the expectation is taken with respect to the\nGD process (C.9).\n• Cij(t, s; η), the solution of the Euler discretization of the DMFT, described in the previous\nsection.\nSome results of this comparison are presented in the next subsection. This comparison allows us\nto gauge two types of systematic effects:\n1. The effect of finite n, d. Indeed, the DMFT equations characterize the n, d →∞limit of the\nGD dynamics (C.9).\n2. The non-zero stepsize η.\nNote that the effect of discretization introduced in the DMFT\nequations are different from the ones in the gradient descent (C.9). Therefore the disagreement\nbetween the two is a measure of the nonzero-η effects.\nTo clarify further the last point, we emphasize that, despite the notation, Cij(t, s; η) is not the\nn, d →∞limit of Cn\nij(t, s; η).\n33\n\n\n−2\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n2\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nt\na1(t) - DMFT\na1(t) - Simulations\na2(t)/2 - DMFT\na2(t)/2 - Simulations\na3(t) - DMFT\na3(t) - Simulations\na4(t)/2 - DMFT\na4(t)/2 - Simulations\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0\n2\n4\n6\n8\n10\n12\n14\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n10\n20\n30\n40\n50\netr\nt\netr(t) - DMFT - η = 0.8\netr(t) - DMFT - η = 0.4\netr(t) - DMFT - η = 0.2\netr(t) - DMFT - η = 0.1\netr(t) - Simulations - η = 0.1\nC11(t, 0)\nt\nFigure 7: Comparison between discretized DMFT and GD dynamics for the Gaussian model (la-\nbeled as ‘Simulations’). GD results are averaged over N = 104 realizations of the Gaussian process,\nunder Setting 1 described in the main text. Left frame: Second layer for DMFT and GD with\nηGD = η = 0.1. Right frame: Train error and correlation function for DMFT with a few values of\nη, and for GD.\nWe note in passing that it is possible to derive DMFT equations for GD, hence characteriz-\ning limn→∞Cn\nij(t, s; ηGD). Similar characterizations were obtained for related (simpler) models in\n[MKUZ20, CCM21, MU22, KU23a, KU23b]. We defer the analysis of GD with large stepsizes to\nfuture work.\nC.3\nTesting the numerical accuracy\nFigures 7 and 8 we present examples of the numerical comparison described in the previous section,\nunder two different settings, as described below.\nSetting 1.\nWe assume pure noise data with τ = 1 and train a network with m = 4 neurons\nand covariance structure given by h(z) = z/10 + z2/2. We simulate GD trajectories, according to\nEq. (C.9) with d = 100, n = 150, and correspondingly evaluate the Euler discretization of DMFT,\ncf. Section C.1 for α = n/d = 1.5.\nWe choose an initialization that is not symmetric and therefore we have to use the full DMFT\nequations of Section B.1. More precisely, we initialize second layer weights as follows:\na1(0) = a2(0) = 1\na3(0) = a4(0) = −1\n(C.12)\nThe weights of the first layer are instead initialized by generating two random vectors y1, y2 ∼\nUnif(Sd−1), and setting\nw1(0) = w3(0) = y1\nw2(0) = w4(0) = y2\n(C.13)\nThis initialization results in initializing the DMFT equations with\nC11(0, 0) = C22(0, 0) = C33(0, 0) = C44(0, 0) = 1 ,\nC13(0, 0) = C24(0, 0) = 1 ,\nC12(0, 0) = C14(0, 0) = C23(0, 0) = C34(0, 0) = 0 .\n(C.14)\n34\n\n\n−2.5\n−2\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n2\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nt\na1(t) - DMFT\na1(t) - Simulations\na2(t) - DMFT\na2(t) - Simulations\na3(t) - DMFT\na3(t) - Simulations\na4(t) - DMFT\na4(t) - Simulations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0\n2\n4\n6\n8\n10\n12\n14\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n10\n20\n30\n40\n50\netr\nt\netr(t) - DMFT - η = 0.8\netr(t) - DMFT - η = 0.4\netr(t) - DMFT - η = 0.2\netr(t) - DMFT - η = 0.1\netr(t) - Simulations - η = 0.1\nC11(t, 0)\nt\nFigure 8: Comparison between discretized DMFT and GD dynamics (labeled as ‘Simulations’). GD\nresults are averaged over N = 104 realizations of the Gaussian process, under Setting 2 described\nin the main text. Results for GD are averaged over N = 104 samples.\nBoth for the discretized DMFT and for GD for several values of the stepsize. The results of\nthis analysis are plotted in Fig. 7.\nSetting 2.\nWe consider again pure noise with τ = 1, a network with m = 4, input dimension\nd = 100 and sample size n = 150. We use hidden neurons with the same covariance structure as in\nthe Setting 1.\nHowever, we change the initialization with respect to Setting 1.\nFirst layer are initialized\nindependently and uniformly at random. It follows that\nCij(0, 0) = δij\n∀i, j = 1, . . . , 4\n(C.15)\nSecond layer weights are initialized according to\na1(0) = −1 ,\na2(0) = −1\n2 ,\na3(0) = 1\n3 ,\na4(0) = 2\n3 .\n(C.16)\nWe use stepsize η = 0.1.\nC.4\nConstruction of the Gaussian process f g( · )\nThe Gaussian process fg can be constructed as follows. Define a sequence of independent Gaussian\ntensors J(k) ∈(Rd)⊗k, k ≥1, with entries (J(k)\ni1,...,ik : ij ≤d) ∼iid N(0, 1). We then let\nfg(a, W ) = 1\nm\nm\nX\ni=1\nai\n∞\nX\nk=0\nck\nd\nX\ni1,...,ik=1\nJ(k)\ni1,...,ikwi,i1 . . . wi,ik\n(C.17)\nIt is easy to check that this stochastic process has the prescribed covariance, with\nh(z) =\n∞\nX\nk=0\nc2\nkzk ,\n(C.18)\nhas long as the series above has radius of convergence larger than 1. An analogous construction\nholds for φg.\n35\n\n\nD\nDynamical regimes: General preliminaries\nIn the next two sections, we will study the SymmDMFT equations of Section B.4 and characterize\ndifferent dynamical regimes in the large network limit. From a technical viewpoint, we develop a\nsingular perturbation theory of the DMFT equations as m →∞for fixed overparametrization ratio\nα = α/m.\nWhile singular perturbation theory is a classical domain of mathematics [Ber01, Hol13], mak-\ning this type of analysis rigorous is notoriously challenging. We will proceed heuristically as fol-\nlows: (i) Hypothesize a certain asymptotic behavior of the DMFT solution in a specific time-scale;\n(ii) Check consistency with the DMFT equations; (iii) Check that this behavior is observed in the\nnumerical solution of the DMFT equations.\nMore precisely, a specific dynamical regime is identified by a scaling of the time variable, which\nin our case will take the form t = t#(m) · ˆt for a certain fixed function t#(m) and ˆt a scaled time\nof order one. The asymptotics of DMFT quantities in that regime takes the form (for instance)\nlim\nm→∞\n1\nc#(m)Co\n\u0010\nt#(m) · ˆt, t#(m) · ˆs; m, α = α\nm\n\u0011\n= co(ˆt, ˆs; α) ,\n(D.1)\nwhere c#(m), co(ˆt, ˆs; α) are two fixed functions, the limit is understood to hold at fixed ˆt, ˆs, α ∈\n(0, ∞), and we made explicit the dependence of Co on m, α. More concisely, we will often write\nthe above formula as\nCo\n\u0010\nt#(m) · ˆt, t#(m) · ˆs; m, α = α\nm\n\u0011\n= c#(m)co(ˆt, ˆs; α) + o(c#(m)) ,\n(D.2)\nand we will typically use t, s instead of ˆt, ˆs for the dummy variables.\nThe behavior of the DMFT equations depends in a crucial way in the initialization of the second\nlayer weights:\n• In Section E, we will consider the case of a ‘lazy initialization,’ i.e. we will assume a(0) =\nγ0\n√m for some constant γ0 ∈(0, ∞) independent of m.\n• In Section F, we will consider the ‘mean field initialization’ i.e.\nassume a(0) = a0 to be\nconstant and independent of m.\nE\nDynamical regimes: Lazy initialization\nAs anticipated, in this section we study dynamical regimes under lazy initialization. In subsection\nE.1, we will consider the case of pure noise data and in subsection E.2 the k-index model.\nThroughout this section, we let γ(t) = a(t)/√m (in particular, γ(0) = γ0).\nE.1\nPure noise model\nUnder the pure noise model, we have φ = ˆφ = 0. Further, the variable v(t) is not defined and can\nbe dropped (equivalently, we can set v(t) = 0).\nWe identify three dynamical regimes:\n36\n\n\n−1\n−0.8\n−0.6\n−0.4\n−0.2\n0\n0.1\n1\n10\n100\n1000\nm(Cd(t, 0) −1)\ntm\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nm = 215\nm = 216\nClz1\nd (t, 0)\nFigure 9: Rescaled correlation function m(Cd(t, s) −1) in the first dynamical regime as a function\nof the scaled time tm for a model initialized with a lazy scaling and fixed second layer weights.\nDifferent curves correspond to the numerical integration of the SymmDMFT equations at various\nvalues of m. They appear to converge to the scaling solution in the large m limit described by\nEqs. (E.6). Here α = 0.5, ˜h(z) = (3/10)z + z2/2 and τ = 1.\n1. t = O(1/m): γ(t) = γ0 + om(1), train error decreases, and the network approximates the null\nfunction (Section E.1.1).\n2. t = Θ(1): γ(t) = γ0 + om(1), first-layer weights move significantly and train error converges\nto a limit e∗(γ0) (Section E.1.2). If γ0 is larger than the interpolation threshold, then train\nerror vanishes in this regime.\n3. t = Θ(m): This regime emerges only if γ0 is smaller than the interpolation threshold. (We\ndiscuss the identification of the interpolation transition of gradient flow in Section E.1.3.)\nIf this is the case, γ(t) grows on the time scale t = Θ(m) until it crosses the interpolation\nthreshold. At that point the train error vanishes (Section E.1.4).\nSince in the first two regimes γ(t) does not change appreciably, the dynamics in these time\nscales is essentially equivalent to the one of a network in which second-layer weights are fixed and\ndo not evolve by GF. In Section E.1.1 and E.1.2 we first consider this case.\nWe note that the pure noise model is unchanged if we rescale τ →cτ, γ0 →cγ0. More precisely,\nthis results in a rescaling of the risk by c2 and hence of time by the same factor. As a consequence\nquantities of interest often depend on γ, τ uniquely through their ratio γ/τ.\nE.1.1\nFirst dynamical regime: t = O(1/m)\nWe first consider the case in which the (scaled) second layer weights are not updated and fixed to\ntheir initialization, i.e. γ(t) = γ0.\n37\n\n\nIt is possible to check that, up to higher-order terms, the SymmDMFT equations are solved by\nfunctions of the form (the first equation holds in weak sense, i.e. after integrating against a test\nfunction)\nRA(t/m, s/m) = m δ(t −s) + om(m)\nCA(t/m, s/m) = Clz1\nA (t, s) + om(1)\n(E.1)\nRo(t/m, s/m) = 1\nmRlz1\no (t, s) + om(1/m)\nCo(t/m, s/m) = 1\nmClz1\no (t, s) + om(1/m)\n(E.2)\nRd(t/m, s/m) = ϑ(t −s) + om(1)\nCd(t/m, s/m) = 1 + 1\nmClz1\nd (t, s) + om(1/m)\n(E.3)\nν(t/m) = νlz1(t) + om(1) .\n(E.4)\nwhere Clz1\nA , Clz1\nd , Clz1\no , νlz1 and Rlz1\no\nare suitable functions independent of m. Here and below, we\nuse the notation ϑ(t) = 1(t > 0).\nNote that Eq. (E.3) implies that on this dynamical regime the weights of the first layer change\nby order 1/m.\nPlugging the asymptotic form in Eqs. (E.1) to (E.4) into the SymmDMFT equations and match-\ning the leading orders for large m, we obtain that the functions Clz1\nA , Clz1\nd , Clz1\no , νlz1 and Rlz1\no\nmust\nsatisfy\nνlz1(t) = −αγ2\n0h′(1) −αγ2\n0h′(0)Clz1\no (t, t) ,\n∂tRlz1\no (t, t′) = −αγ2\n0h′(0)\n\u00001 + Rlz1\no (t, t′)\n\u0001\n,\n∂tClz1\no (t, t′) = −αγ2\n0h′(0)\n\u00001 + Clz1\no (t, t′)\n\u0001\n,\n∂tClz1\nd (t, t′) = αγ2\n0h′(0)\n\u0000Clz1\no (t, t) −Clz1\no (t, t′)\n\u0001\n,\nClz1\nA (t, s) = −\n\u0002\nτ 2 + γ2\n0h(1) −γ2\n0h(0)Clz1\no (t, s)\n\u0003\n.\n(E.5)\nThese are a set of ordinary differential equations that can be solved explicitly. We get\nRlz1\no (t, t′) = ϑ(t −t′)\nh\ne−αγ2\n0h′(0)(t−t′) −1\ni\n,\nClz1\no (t, t) = e−2αγ2\n0h′(0)t −1 ,\nClz1\no (t, t′) = −1 + e−αγ2\n0h′(0)(t−t′)(Clz1\no (t′, t′) + 1)\nfor t > t′ ,\nClz1\nd (t, t′) = 1 + e−αγ2\n0h′(0)(t+t′) −1\n2\n\u0010\ne−2αγ2\n0h′(0)t + e−2αγ2\n0h′(0)t′\u0011\n,\nfor t > t′ .\n(E.6)\nIn particular, Eqs. (E.6) imply\nlim\nt→∞Clz1\no (t, t) = −1 ,\nlim\nt,t′→∞, t−t′→∞Rlz1\no (t, t′) = −1 .\n(E.7)\nRecalling Eq. (E.2) we conclude that\nlim\nt→∞lim\nm→∞m Co(t/m, t/m) = −1 ,\n(E.8)\nor, using the interpretation of Co,\nlim\nt→∞lim\nm→∞lim\nn→∞m · ⟨wi(t/m), wj(t/m)⟩= −1\n∀i ̸= j .\n(E.9)\n38\n\n\nIn other words, at the end of this dynamical regime, the first-layer weights form a regular simplex,\nwith center w(t/m) := m−1 Pm\ni=1 wi(t/m) satisfying ∥w(t/m)∥2 = om(1).\nHence, at the end of the first dynamical regime, the first-layer weights are such that the linear\ncomponent of the activation function σ is removed. In other words, for t a large constant, we have\nfg( · ; a(t/m), W (t/m)) = γ0\n√m\nm\nX\ni=1\nσnl\nG (wi(t/m)) + err ,\n(E.10)\nwhere σnl\nG (w) is a Gaussian process with covariance structure given by h(z) −zh′(0), and err is\nsmall in mean square.\nNotice also that this is achieved by a O(1/√m) change in each of the first layer weights. Indeed,\nby Eq. (E.3), we have\nlim\nn→∞∥wi(0) −wi(t/m)∥2 = 2 −2Cd(0, t/m) = −2\nmClz1\nd (0, t) + om(1/m) .\n(E.11)\nEquations (E.1) to (E.4) can be used to compute the behavior of the train error in this dynamical\nregime:\nlim\nm→∞etr(t/m) = elz1\ntr (t) .\n(E.12)\nUsing Eqs. (E.6), we get the expression:\nelz1\ntr (t) = 1\n2\n\u0002\nτ 2 + γ2\n0h(1) + γ2\n0h′(0)Clz1\no (t, t)\n\u0003\n.\n(E.13)\nIn particular, the train error at the end of this dynamical regime is\nlim\nt→∞lim\nm→∞etr(t/m) = lim\nt→∞elz1\ntr (t) = 1\n2\n\u0002\nτ 2 + γ2\n0h(1) −γ2\n0h′(0)\n\u0003\n.\n(E.14)\nThis is in agreement with (E.10). Indeed, note that\nb\nRn(a, W ) = 1\n2n∥ε∥2 −1\nn⟨ε, f g(a, W )⟩+ 1\n2n∥f g(a, W )∥2 .\n(E.15)\nTraining in this timescale attempts to minimize ∥f g(a, W )∥2 without fitting the noise.\nThis picture is confirmed by the fact that Eqs. (E.6) depend on h only through h′(0). This\nmeans that the dynamics on timescales of order 1/m is controlled by the linear part of the covariance\nstructure of the hidden layer.\nIn Fig.9 we test the correctness of the asymtotic ansatz of Eqs. (E.1) to (E.4). Namely, we\ncompare the results of numerical integration of the SymmDMFT equations for various values of m,\nwith the prediction of Eqs. (E.6). The match is excellent.\nSo far we assumed that second-layer weights are not optimized and γ(t) = γ0. What happens\nif drop this constraint?\nIt can be checked that the form given in Eqs. (E.1)-(E.4) still solves\nthe SymmDMFT equations when a(t) is allowed to evolve, and γ(t/m) = γ0 + om(1) for all fixed\nt ∈(0, ∞). In other words, second layer weights do not change significantly during this dynamical\nregime.\n39\n\n\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n10−6\n10−5\n10−4\n10−3\n10−2\n10−1\n1\n10\n102\n103\nCd(t, 0)\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nm = 215\nm = 216\nClz2\nd (t, 0)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n10−5\n10−3\n10−1\n10\n0\n0.2\n0.4\n0.6\n0.8\n1\n10\n100\netr\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nm = 215\nm = 216\nelz2\ntr (t, 1)\ntm\nelz1\ntr (t, 1)\nFigure 10: Training with pure noise data under lazy initialization: second dynamical regime t =\nΘ(1). Left panel: First-layer weights correlation function Cd(t, 0) measuring the inner product\nbetween neurons at time 0 and time t, plotted versus t for several values of m, and compared with\nthe large m-asymptotics Clz2\nd . Right panel: training error a etr(t, γ0, m) plotted versus t for several\nvalues of m, and compared with the large-m asymptotics in this regime elz2\ntr (t, 1). Notice the two-\nsteps decrease of the training error, corresponding to the two regimes t = O(1/m) and t = Θ(1).\nInset: Same curves plotted versus tm, and compared with the asymptotic prediction elz1\ntr ( · , 1) in\nthe first dynamical regime. For both panels we use α = 0.5, ˜h(z) = (3/10)z + z2/2, γ0 = 1 and\nτ = 1.\nE.1.2\nSecond dynamical regime: t = Θ(1)\nThe second dynamical regime arises when t = Θ(1). Recall from the previous subsection that, for\nt = om(1), the train error remains close (for large m) to the plateau characterized at the end of the\nfirst dynamical regime, see Eq. (E.14). When t is of order one, the first layer weights start changing\nby an amount of order one as well, and the model starts to fit the noise.\nAs before, we begin by considering the simplified setting in which γ(t) = γ0 is fixed and not\noptimized by GF.\nWe claim that the SymmDMFT equations are solved by the following ansatz, up to lower order\nterms as m →∞:\nν(t) = νlz2(t) + om(1) ,\n(E.16)\nCd(t, t′) = Clz2\nd (t, t′) + om(1) ,\n(E.17)\nRd(t, t′) = Rlz2\nd (t, t′) + om(1) ,\n(E.18)\nCo(t, t′) = 1\nmClz2\no (t, t′) + om(1/m) = −1\nmClz2\nd (t, t′) + om(1/m) ,\n(E.19)\nRo(t, t′) = 1\nmRlz2\no (t, t′) + om(1/m) = −1\nmRlz2\nd (t, t′) + om(1/m) .\n(E.20)\nHere Clz2\nd , Rlz2\nd , Clz2\no , Rlz2\no and νlz2 are certain functions independent of m. Equations (E.19), (E.20)\nstate in particular that Clz2\no (t, t′) = −Clz2\nd (t, t′) and Rlz2\no (t, t′) = −Rlz2\nd (t, t′), and the therefore we are\nleft with the task of determining Clz2\nd (t, t′), Rlz2\nd (t, t′). By substituting Eqs. (E.16) to (E.20) into the\nSymmDMFT equations and matching leading order terms, we get a set of two integral-differential\nequations for Clz2\nd (t, t′), Rlz2\nd (t, t′), which we next state.\n40\n\n\nWe first define\nΣlz2\nR (t, s) := γ2\n0\n\u0000h′(Clz2\nd (t, s)) −h′(0)\n\u0001\nRlz2\nd (t, s) ,\nΣlz2\nC (t, s) := τ 2 + γ2\n0h(Clz2\nd (t, s)) −γ2\n0h′(0)Clz2\nd (t, s) ,\n(E.21)\nthen we define Rlz2\nA and Clz2\nA as the solution of\nδ(t −t′) =\nZ t\nt′ [δ(t −s) + Σlz2\nR (t, s)] Rlz2\nA (s, t′) ds ,\n0 =\nZ t\n0\n[δ(t −s) + Σlz2\nR (t, s)] Clz2\nA (s, t′)ds +\nZ t′\n0\nΣlz2\nC (t, s)Rlz2\nA (t′, s) ds .\n(E.22)\nWe next define the asymptotic form for the memory kernels\nM lz2\nR (t, s) := α\nh\nRlz2\nA (t, s)˜h′(Clz2\nd (t, s)) + Clz2\nA (t, s)˜h′′(Clz2\nd (t, s))Rlz2\nd (t, s)\ni\n,\nM lz2\nC (t, s) := α˜h′(Clz2\nd (t, s))Clz2\nA (t, s) .\n(E.23)\nand we have defined\n˜h(z) := h(z) −h′(0)z .\n(E.24)\nThe equations for νlz2, Clz2\nd\nand Rlz2\nd\nare then given by\nνlz2(t) = −\nZ t\n0\n[M lz2\nR (t, s)Clz2\nd (t, s) + M lz2\nC (t, s)Rlz2\nd (t, s)] ds ,\n(E.25)\n∂tRlz2\nd (t, t′) = δ(t −t′) −νlz2(t)Rlz2\nd (t, t′) −\nZ t\nt′ M lz2\nR (t, s)Rlz2\nd (s, t′) ds ,\n(E.26)\n∂tClz2\nd (t, t′) = −νlz2(t)Clz2\nd (t, t′) −\nZ t\n0\nM lz2\nR (t, s)Clz2\nd (t′, s) ds −\nZ t′\n0\nM lz2\nC (t, s)Rlz2\nd (t′, s) ds .\n(E.27)\n(As before, in the second and last equation, it is understood that t ≥t′, and the last equation is\nunderstood to hold in weak sense.)\nGiven the constraints on Cd, Rd, we have the following constraints on Clz2\nd , Rlz2\nd ,\nClz2\nd (t, t) = 1 ,\n(E.28)\nClz2\nd (t, s) = Clz2\nd (s, t) ,\n(E.29)\nRlz2\nd (t, s) = 0\n∀t ≤s .\n(E.30)\nIn particular, the last condition, together with Eq. (E.26) implies Rlz2\nd (t+, t) = 1.\nThe evolution of the the train error in this second dynamical regime is given by\nlim\nm→∞etr(t) = elz2\ntr (t, γ0) ,\n(E.31)\nelz2\ntr (t, γ0) = −1\n2Clz2\nA (t, t) ,\n(E.32)\nwhere we have made explicit the dependence on the initialization of second-layer weights γ0.\nNote that Eq. (E.22) implies Clz2\nA (0, 0) = −Σlz2\nC (0, 0), and Eq. (E.21) yields Σlz2\nC (0, 0) = τ 2 +\nγ2\n0h(1) −γ2\n0h′(0). Therefore\nlim\nt→0 elz2\ntr (t, γ0) = 1\n2\nh\nτ 2 + γ2\n0˜h(1)\ni\n= lim\nt→∞elz1\ntr (t, γ0) .\n(E.33)\n41\n\n\n10\n100\n1000\n0.001\n0.01\n0.1\n1\n300\n600\n0.3\n0.6\ntrel\nα∗\nGF −α\nγ0 = 10/7 αalg = 0.835\nγ0 = 1 αalg = 0.605\nγ0 = 5/9 αalg = 0.262\nx−2\nα\nFigure 11: Training with pure noise data under lazy initialization. We plot the time for GF to\nconverge to near-zero training error as a function of α, for various values of γ0, as computed using\nthe theory of Section E.1.2. The divergence of trel signals the phase transition for GF interpolation\nα∗\nGF. Inset: τrel versus α in linear scale. Main panel: trel versus α∗\nGF −α (with the fitted value of\nα∗\nGF). Here we use h(z) = (3/10)z + z2/2.\nIn other words, this second dynamical regime captures the decrease of the training error which\nstarts at the plateau reached in the first regime, cf. Eq. (E.14). which coincides with the long time\nextrapolation of the first dynamical regime.\nThis second dynamical regime is fully non-linear and depends on the entire covariance function\n˜h. Further, the first order weights move by an amount ∥wi(t) −wi(0)∥= Θ(1), as follows from the\nfact that Clz2\nd (t, 0) < 1 strictly.\nIn order to confirm the ansatz (E.16) to (E.20), we compared the solution of the full Sym-\nmDMFT equations, with the solution of the asymptotic equations (E.25), (E.27). An example of\nsuch a comparison is presented in Fig. 10: the agreement is excellent.\nThe treatment above assumed the constraint γ(t) = γ0. However, as in the first dynamical\nregime, if we let second layer weights evolve, they do not change appreciably. Namely, the asymp-\ntotic form given in Eqs. (E.16) to (E.20) still solves the SymmDMFT equations when a(t) is allowed\nto evolve. We have γ(t) = γ0 + om(1) on this timescale.\nE.1.3\nThe algorithmic interpolation transition\nFor the discussion in this section, we denote by etr(t, γ0, m, α) the train error as a function of t,\nwhere we emphasized the dependence on the initial condition γ0, on the number of neurons m, and\non the overparametrization ratio α. We further assume that second layer weigths are not evolved\nand therefore γ(t) = γ0 for all t. We define the asymptotic train error achieved by GF as\netr,∞(γ0, m, α) := lim\nt→∞etr(t, γ0, m, α)\n(E.34)\n42\n\n\n= lim\nt→∞lim\nn→∞\nb\nRn(a, W (t)) .\n(E.35)\nAgain, in this definition ai = γ0\n√m is kept fixed and does not evolve with time.\nNotice that it is in principle possible that limn→∞b\nRn(a, W (tn)) is strictly smaller than etr,∞(γ0, m, α)\nif we let tn diverge with n at sufficiently fast rate. However, based on results on related models in\nspin-glass theory we expect this not to be the case as long as tn is polynomial in n. Explicitly, we\nexpect that, for any sequence tn →∞\ntn ≤nC\n⇒\nlim\nn→∞\nb\nRn(a, W (tn)) = etr,∞(γ0, m, α) .\n(E.36)\nUsing the reduced equations for t = Θ(1) timescale, i.e. Eqs. (E.25) to (E.27), we can also\ndefine\nelz2\ntr,∞(γ0, α) := lim\nt→∞elz2\ntr (t, γ0, α)\n(E.37)\n= lim\nt→∞lim\nm→∞lim\nn→∞\nb\nRn(a, W (t)) .\nA natural question is whether the large m limit of etr,∞(γ0, m, α) coincides with elz2\ntr,∞(γ0, α).\nThis amounts to asking whether there exists dynamical regime with timescale t(m) diverging with m\nat which etr(t(m), γ0, m, α) starts diverging significantly from the value at the end of the second dy-\nnamical regime namely elz2\ntr,∞(γ0, α). If elz2\ntr,∞(γ0, α) = 0 then of course limm→∞etr(t(m), γ0, m, α) =\n0 as well.\nIf however elz2\ntr,∞(γ0, α) > 0, then the answer depends upon whether the second layer weights\nare evolved with GF:\n• In the constrained setting in which second-layer weights do not evolve, we observe (from\nnumerical solutions of SymmDMFT ) that\nlim\nm→∞etr,∞(γ0, m, α) = elz2\ntr,∞(γ0, α) .\n(E.38)\n• In the next section we will see that if γ(t) evolves with GF then the train error achieved on\na diverging timescale t = Θ(m) is strictly smaller than elz2\ntr,∞(γ0, α) and vanishes for large\nenough t.\nNote that etr,∞(γ0, m, α) and elz2\ntr,∞(γ0, α) also depend on the noise variance τ 2.\nHowever,\nbecause of the invariance under rescaling discussed at the beginning of this section (adding τ as an\nargument):\nelz2\ntr,∞(γ0, α, τ 2) = τ 2 · elz2\ntr,∞(γ0/τ, α, τ 2 = 1) ,\n(E.39)\nand similarly for etr,∞(γ0, m, α). Because of this relation, we can think that τ 2 is fixed throughout,\ne.g. τ 2 = 1.\nWe expect etr,∞(γ0, m, α), elz2\ntr,∞(γ0, α) to be non-increasing in γ0, and define the thresholds\nγGF(α, m) := inf\n\b\nγ0 : etr,∞(γ0, m, α) = 0\n\t\n,\n(E.40)\nγ∗\nGF(α) := inf\n\b\nγ0 : elz2\ntr,∞(γ0, α) = 0\n\t\n.\n(E.41)\n43\n\n\n(These definitions need to be modified if γ0 7→etr,∞(γ0, m, α) is non-monotone.)\nOf course, Eq. (E.38) implies\nlim\nm→∞γGF(α, m) = γ∗\nGF(α) .\n(E.42)\nThe numerical solution of the SymmDMFT equations imply that the curve γ∗\nGF(α) is monotone\nincreasing with α, as also suggested by the Gaussian complexity bound (see Section 2.2 in the main\ntext). Hence we can invert it to get a threshold α∗\nGF(γ0): the two descriptions are equivalent.\nIn order to determine α∗\nGF(γ0), we adopt a procedure already implemented in [KU23a] for a\nsimpler model. The procedure is based on the observation (from numerical solutions) that when\nelz2\ntr,∞(γ0, α) = 0, elz2\ntr (t, γ0, α) = exp(−t/trel(α; ε) + o(t)) for some trel(α) > 0 which diverges as\nα ↑αGF.\n1. Define a grid of values of α, A0 = {α1, α2, . . . , αK}, which are expected to be smaller than\nα∗\nGF(γ0).\n2. For each value α ∈A0, integrate numerically the reduced equations (E.25) to (E.27). Verify\nthat elz2\ntr (t, γ0, α) appear to converge to 0 with t →∞. Let A ⊆A0 be the subset of values\nfor which this happens.\n3. For each α ∈A, define trel(αi; ε) := inf{t : elz2\ntr (t, γ0, αi) < ε · τ 2} where ε is a small threshold\nvalue (we use ε = 10−7).\n4. Estimate parameters α∗\nGF(γ0), c, ν by fitting the relation trel(αi; ε) ∼c(α∗\nGF −αi)−ν.\nFigure 11 illustrates the calculation of α∗\nGF(γ0) for three values of γ0. In the inset we plot trel\nfor three values of γ0 as a function of α. In the main panel, we demonstrate the divergence of trel\nwhen (α∗\nGF −α) vanishes. In practice, we observe ν = 2 fit well the data across a variety of settings,\nsuggesting this is the universal exponent for the divergence of trel.\nE.1.4\nThird dynamical regime: t = Θ(m)\nIn the first two dynamical regimes, the large-m behavior did not depend on whether we would let\nsecond layer evolve with GF or we kept them fixed, i.e. γ(t) = γ0.\nIn contrast, the behavior on timescales diverging with m depends significantly on the dynamics\nof second-layer weights.\n• If second layer weights are fixed, no significant further evolution takes place. In particular,\nthe training error does not decrease significantly below the value reached at the end of the\nsecond dynamical regime, i.e. eℓ\ntr,∞(γ0, α). This is stated formally in Eq. (E.38).\n• If second layer weights evolve according to GF, then the dynamics on time-scales diverging\nwith m can be non-trivial and depends on the second-layer weights initialization γ0.\nIf\nγ0 > γ∗\nGF(α), then GR reaches vanishing training error during the second dynamical regime,\nand no further evolution takes place.\nHowever, if γ0 < γ∗\nGF(α), second layer weights start evolving when t = Θ(m), thus giving rise\nto a third dynamical regime. This is the object of the present subsection.\n44\n\n\n0.95\n1\n1.05\n1.1\n1.15\n1.2\n1.25\n1.3\n1.35\n1.4\n1.45\n0.001\n0.01\n0.1\n1\n10\n100\nγ(t)\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nFigure 12: Training with pure noise data under lazy initialization: third dynamical regime. Evo-\nlution of the (rescaled) weights of the second layer as a function of t/m. Here τ = 2.5 and γ0 = 1,\nα = 0.3, and covariance structure for the neurons given by h(z) = (9/10)z + z2/2.\nIn Fig. 12, left frame, we plot the rescaled second layer weights γ(t) (as predicted by numerical\nintegration of the SymmDMFT equation) as a function of time for several values of m.\nHere,\nobviously, we do not constrain γ(t) = γ(0).\nWe observe that γ(t) changes only when t = Θ(m). Indeed, when plotted against t/m, curves\nobtained for different values of m collapse onto each other.\nThis suggests that, for t = o(m)\nγ(t) = γ(0) + om(1) (recall that γ(0) = γ0 by definition). Further, the curve collapse suggests that,\nfor any fixed ˆt ∈(0, ∞):\nlim\nm→∞γ(ˆt m, γ0) = γlz3(ˆt, γ0) ,\n(E.43)\nwhere we have made explicit the dependence on γ0. Of course, the case γ0 > γ∗\nGF(α) fits in this\nframework with γlz3(z, γ0) = γ0 identically.\nWe next consider the evolution of the train error. In Fig. 13, left frame, we plot the train error\n(again, as predicted by numerical integration of the SymmDMFT equation) as a function of time\nfor several values of m.\nAgain, when plotted as a function of t/m, curves for different values of m reach a plateau, and\ncollapse below the plateau. This suggests the following limit behavior, which is consistent with\nEq. (E.43)\nlim\nm→∞˜etr(ˆt m, γ0, m) = elz3\ntr (ˆt, γ0) .\n(E.44)\n(Here we use ˜etr(ˆt m, γ0) to denote the train error when second-layer weights evolve, in contrast\nwith etr(ˆt m, γ0) which we used for the setting in which second-layer weights are constrained.)\nMatching the present dynamical regime (t = Θ(m)) with previous one (t = Θ(1), cf. Section\nE.1.2), implies that\nlim\nˆt→0+\nelz3\ntr (ˆt, γ0) = lim\nt→∞elz2\ntr (t, γ0) = elz2\ntr,∞(γ0)\n(E.45)\nIn other words, the function elz3\ntr describes the decrease of the train error below the level elz2\ntr,∞(γ0)\nachieved during the second dynamical regime.\n45\n\n\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n0.01\n0.1\n1\n10\netr\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nelz2\ntr,∞(1, 3/10)\n0\n0.5\n1\n1.5\n2\n1\n1.1\n1.2\n1.3\n1.4\n1.5\netr\nγ\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nelz2\ntr,∞(γ, 3/10)\nFigure 13: Training with pure noise data under lazy initialization: third dynamical regime. Left\nframe: Train error on timescales of order m. Right frame: GF trajectories in the plane γ (second\nlayer weights) — etr (train error). Black dots represent pairs (γ, elz2\ntr,∞(γ)), where elz2\ntr,∞(γ) is the\ntrain error achieved at the end of the first dynamical regime, cf. Section E.1.3. The data has been\nproduced from the same model as in Fig. 12.\nIn order to characterize the scaling function elz3\ntr , in Fig. 13, right frame, we plot parametrically\nthe the train error for different values of m as a function of the second layer weights γ(t). We also\nplot the curve (γ, elz2\ntr,∞(γ)). This plot is consistent with the following behavior as m →∞. In a\nfirst regimes (corresponding to t = o(m)) the train error has a drop that becomes vertical in the\nm →∞limit, implying that γ(t) does not evolve while the train error decreases until it reaches\neℓ\ntr,∞(γ0). In the last regime (corresponding to t = Θ(m)), γ(t) increases together with the decrease\nof the train error eℓ\ntr(t, γ0). Remarkably, they follow the curve (γ, elz2\ntr,∞(γ)).\nIn order to describe the last regime, we point out that t 7→γlz3(t) is monotone increasing.\nTherefore we can re-parametrize time by the value of the second layer weights. Namely, define ˜γ−1\nthe inverse function, so that\nˆt = ˜γ−1(γlz3(ˆt, γ0), γ0) .\n(E.46)\nUsing this reparametrization of time, the behavior in Fig. 13 can be formalized as\nlim\nt,m→∞:γ(t,γ0,m)=˜γ elz3\ntr (t, γ0, m) = elz3\ntr (˜γ−1(˜γ, γ0), γ0) =: ε(˜γ, γ0) .\n(E.47)\nThe collapse on finite m curves in Fig. 13, right frame, onto the curve (γ, elz2\ntr,∞(γ)) suggests that\nγ > γ0\n⇒\nε(˜γ, γ0) = elz2\ntr,∞(γ) .\n(E.48)\nIn other words, the dynamics on timescales of order m is adiabatic: at each increase of γ(t) on\ntimescales of order m, the train error relaxes to the the value it would have had if the second layer\nweights would have been fixed in time at the corresponding value of γ.\nA remarkable consequence of Eq. (E.48) is that that\nlim\nˆt→∞\nγlz3(ˆt) = lim\nm→∞γGF(α, m) = γ∗\nGF(α) .\n(E.49)\nIn words, in the large network limit, the norm of second-layer weights at the end of training is\nasymptotically the minimum norm that allows for interpolation.\n46\n\n\nE.2\nMulti-index model\nIn this section we generalize the computations of Section E.1 to the case in which the dataset has a\nstructure produced via a k-index model. The weights of the second layer are set to a(t) = γ(t)√m\nand evolve with GF. The initialization scale γ(0) = γ0 is fixed and independent of m.\nAs in the pure noise case, we identify three dynamical regimes:\n1. t = O(1/m): γ(t) = γ0 + om(1), ∥wi(t) −wi(0)∥= Θ(1/√m). On this scale the network only\nlearns a linear approximation of the target. Test and train error remain close to each other\n(Section E.2.1).\n2. t = Θ(1): γ(t) = γ0 + om(1), ∥wi(t) −wi(0)∥= Θ(1). Test error does not change but train\nerror decreases significantly (Section E.2.2).\n3. t = Θ(m): This regime only emerges if γ0 is below a certain interpolation threshold, i.e.\nγ0 < γ∗\nGF(α, φ, τ). In this regime γ(t) grows until the threshold, and train error decreases to\n0 while test error decreases to 0 (Section E.2.5).\nE.2.1\nFirst dynamical regime: t = O(1/m)\nOn this timescale, the SymmDMFT equations are solved, up to higher order terms, by the following\nansatz:\nCd(t/m, s/m) = 1 + om(1) ,\nRd(t/m, s/m) = ϑ(t −s) + om(1) ,\n(E.50)\nCo(t/m, s/m) = 1\nmClz1\no (t, s) + om(m−1) ,\nRo(t/m, s/m) = 1\nmRlz1\no (t, s) + om(m−1) ,\n(E.51)\n1\nmRA(t/m, s/m) = δ(t −s) + om(1) ,\nCA(t/m, s/m) = −Σlz1\nC (t, s) + om(1) ,\n(E.52)\na(t/m)√m = γ0 + om(1) ,\nv(t/m) =\n1\n√mvlz1(t) + om(m−1/2) ,\n(E.53)\nwith\nΣlz1\nC (t, s) = τ 2 +∥φ∥2 −γ0⟨∇ˆφ(0), vlz1(t)⟩−γ0⟨∇ˆφ(0), vlz1(s)⟩+γ2\n0\n\u0000h(1) + h′(0)Clz1\no (t, s)\n\u0001\n. (E.54)\nIn particular, Eq. (E.50) implies ∥wi(0) −wi(t/m)∥= om(1): weights of the first layer change by\nsmall amount.\nThe scaling functions defined in Eqs. (E.50)-(E.53) satisfy a set of equations that can be derived\ndirectly from the SymmDMFT equations:\n∂tvlz1(t) = αγ0∇ˆφ(0) −αγ2\n0h′(0)vlz1(t) ,\n∂tClz1\no (t, t′) = αγ0⟨∇ˆφ′(0), vlz1(t′)⟩−αγ2\n0h′(0)\n\u00001 + Clz1\no (t, t′)\n\u0001\n∂tRlz1\no (t, s) = −αˆγ2\n0h′(0) (1 + Rlz1\no (t, s)) .\n(E.55)\nNote that\ndClz1\no (t, t)\ndt\n= 2 lim\nt′→t−∂tClz1\no (t, t′) .\n(E.56)\n47\n\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\n1\n10\n102\n103\n√mv(t)\ntm\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nvlz1(tm)\n0.8\n0.85\n0.9\n0.95\n1\n10−5\n10−4\n10−3\n10−2\n10−1\n1\n10\n102\nCd(t, 0)\nt\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nClz2\nd (t, 0)\nFigure 14: SymmDMFT predictions and large network scaling for lazy training in a single index\nmodel. Left: Projection v(t) of the first layer weights onto the latent direction on timescales of the\norder 1/m. The result for m →∞, vlz1, has been obtained by integrating analytically Eq. (E.55).\nRight: The behavior of Cd(t, 0) on timescales t = Θ(1), compared with the scaling theory for\nm →∞, namely Clz2\nd . In both cases with h(z) = ˆφ(z) = (9/10)z + z2/2, τ = 0.3 and α = 0.3,\nγ0 = 1.\nThe solution of Eqs. (E.55) implies that\nvlz1\n∞:= lim\nt→∞vlz1(t) = ∇ˆφ(0)\nγ0h′(0) ,\nlim\nt→∞Clz1\no (t, t) = −\n\u00001 −∥vlz1\n∞∥2\u0001\n.\n(E.57)\nFurthermore, on this timescale, the train and test error coincide and are given by\nlim\nm→∞etr(t/m) = lim\nm→∞ets(t/m) = 1\n2Σlz1\nC (t, t) .\n(E.58)\nThe corresponding asymptotic value is given by\nlim\nt→∞lim\nm→∞ets(t/m) = elz1\nts,∞= 1\n2\n\u0012\nτ 2 + ∥φ∥2 −\n1\nh′s(0)∥∇bφ(0)∥2 + γ2\n0˜h(1)\n\u0013\n(E.59)\nwhere\n˜h(z) = h(z) −h′(0) .\n(E.60)\nThe interpretation of this dynamical regime is analogous to the one of the same regime in the\npure-noise setting, as confirmed by Eq. (E.59) : the network learns the linear component of the\ndata distribution.\nIn the left panel of Fig. 14 we test the scaling theory in this dynamical regime, as given by\nEqs. (E.50) to (E.53). We plot the solution of the SymmDMFT equations, versus tm, for increasing\nvalues of m: the curve collapse well on their conjectured m →∞limit.\nE.2.2\nSecond dynamical regime: t = Θ(1)\nWe next consider t = Θ(1). One can show that the SymmDMFT equations are solved, up to higher\norder terms as m →∞, by the following ansatz\nCd(t, s) = Clz2\nd (t, s) + om(1) ,\nRd(t, s) = Rlz2\nd (t, s) + om(1) ,\n48\n\n\nCo(t, s) = 1\nmClz2\no (t, s) + om(m−1) ,\nRo(t, s) = 1\nmRlz2\no (t, s) + om(m−1) ,\n(E.61)\nv(t) =\n1\n√mvlz1\n∞+ om(m−1/2) ,\nν(t) = νlz2(t) + om(1) ,\nwith γ(t) = γ0 + om(1) and\nClz2\no (t, s) = −Clz2\nd (t, s) + ∥vlz2\n∞∥2 ,\nRlz2\no (t, s) = −Rlz2\nd (t, s) .\n(E.62)\nIn other words, on this time scale first layer weights move by order one ∥wi(t) −wi(0)∥= Θ(1),\nbut in a linear subspace that is orthogonal to the latent space. Second layer weights do not move\nappreciably. As a consequence, no additional learning takes place in this regime, but the model\nbegins to overfit the data.\nNote that the above scaling form is compatible with the long time limit of the previous dynamical\nregime.\nIn order to define the equations for the functions on the right-hand side of Eq. (E.61) we define\nRlz2\nA and Clz2\nA to be the solution of\nδ(t −t′) =\nZ t\nt′ [δ(t −s) + Σlz2\nR (t, s)] Rlz2\nA (s, t′) ds ,\n0 =\nZ t\n0\n[δ(t −s) + Σlz2\nR (t, s)] Clz2\nA (s, t′) ds +\nZ t′\n0\nΣlz2\nC (t, s)Rlz2\nA (t′, s) ds ,\n(E.63)\nwhere\nΣlz2\nR (t, s) = γ2\n0\n\u0000h′(Clz2\nd (t, s)) −h′(0)\n\u0001\nRlz2\nd (t, s) ,\nΣlz2\nC (t, s) = τ 2 + ∥φ∥2 −2γ0⟨∇ˆφ(0), vlz1\n∞⟩+ γ2\n0\n\u0000h(Clz2\nd (t, s)) + h′(0)Clz2\no (t, s)\n\u0001\n.\n(E.64)\nDefine the following memory kernels\nM lz2\nR,d(t, s) = αγ2\n0\n\u0002\nRlz2\nA (t, s)h′(Clz2\nd (t, s)) + Clz2\nA (t, s)h′′(Clz2\nd (t, s))Rlz2\nd (t, s)\n\u0003\n,\nM lz2\nR,o(t, s) = αγ2\n0h′(0)Rlz2\nA (t, s) ,\nM lz2\nC,d(t, s) = αγ2\n0h′(Clz2\nd (t, s))Clz2\nA (t, s) ,\nM lz2\nC,o(t, s) = αγ2\n0h′(0)Clz2\nA (t, s) .\n(E.65)\nSubstituting the ansatz (E.61) into the SymmDMFT equations, and using Eqs. (E.62), we obtain\nthe following equations for Clz2\nd (t, t′), Rlz2\nd (t, t′), νlz2(t)\n∂tClz2\nd (t, t′) = −νlz2(t)Clz2\nd (t, t′) + αγ0⟨∇ˆφ′(0), vlz1\n∞⟩\nZ t\n0\nRlz2\nA (t, s)ds\n−\nZ t\n0\nds\n\u0002\nM lz2\nR,d(t, s)Clz2\nd (t′, s) + M lz2\nR,o(t, s)Clz2\no (t′, s)\n\u0003\nds\n(E.66)\n−\nZ t′\n0\n\u0002\nM lz2\nC,d(t, s)Rlz2\nd (t′, s) + M lz2\nC,o(t, s)Rlz2\no (t′, s)\n\u0003\nds ,\n∂tRlz2\nd (t, t′) = −νlz2(t)Rlz2\nd (t, t′) + δ(t −t′)\n(E.67)\n−\nZ t\nt′\n\u0002\nM lz2\nR,d(t, s)Rlz2\nd (s, t′) + M lz2\nR,o(t, s)Rlz2\no (s, t′)\n\u0003\nds ,\n49\n\n\nνlz2(t) = αγ0⟨∇ˆφ′(0), vlz1\n∞⟩\nZ t\n0\nRlz2\nA (t, s) ds −\nZ t\n0\n\u0002\nM lz2\nR,d(t, s)Clz2\nd (t, s) + M lz2\nR,o(t, s)Clz2\no (t, s)\n\u0003\nds\n−\nZ t\n0\n\u0002\nM lz2\nC,d(t, s)Rlz2\nd (t, s) + M lz2\nC,o(t, s)Rlz2\no (t, s)\n\u0003\nds .\n(E.68)\nFinally, the train and test errors converge to well defined limits for t fixed and m →∞:\netr(t, γ0) = elz2\ntr (t, γ0) + om(1) ,\nets(t, γ0) = elz2\nts (t, γ0) + om(1) .,\n(E.69)\nwhere\nelz2\ntr (t, γ0) = −1\n2Clz2\nA (t, t) ,\nelz2\nts (t, γ0) = 1\n2Σlz2\nC (t, t) .\n(E.70)\nNote that, using Eqs. (E.62), (E.64), and the fact that Clz2\nd (t, t) = 1 (because of the unit norm\nconstraint on the first layer weights), we get\nelz2\nts (t, γ0) = 1\n2\nn\nτ 2 + ∥φ∥2 −2γ0⟨∇ˆφ(0), vlz1\n∞⟩+ γ2\n0\n\u0000h(1) −h′(0) + h′(0)∥vlz1\n∞∥2\u0001o\n.\n(E.71)\nUsing Eq. (E.57), we obtain that the asymptotic test error in this dynamical regime is constant\nand equal to the test error achieved at the end of the previous regime, namely elz2\nts (t, γ0) = elz1\nts,∞,\ncf. Eq. (E.59). As anticipated, no learning takes place on this timescale.\nThe predictions of Eqs. (E.61) are tested in the right panel of Fig. 14. We plot the correlation\nfunction Cd(t, 0) for several values of m, as obtained by solving the SymmDMFT equations. We com-\npare these results with the m →∞prediction Clz2\nd (t, 0) obtained by solving Eqs. (E.66) to (E.68).\nWe observe collapse of finite m curves on the large m asymptotics supporting our conclusions.\nIn Fig. 15 we plot the behavior of the train and test error both on timescales t = Θ(1/m)\n(left frame, plotting etr(t, γ0), ets(t, γ0) versus tm) and t = Θ(1) (right frame, plotting etr(t, γ0),\nets(t, γ0) versus tm). We the solutions of SymmDMFT equations at increasing values of m with the\ntheory scaling theory presented in the previous section (for t = Θ(1/m), left frame) and in this\nsection (for t = Θ(1), right frame). As anticipated, we observe the following:\n• On the time scale t = Θ(1/m) (left panel), test and train error collapse (as m →∞) on\na common limiting curve elz1\ntr (ˆt, γ0) = elz1\nts (ˆt, γ0) which converges, for large ˆt, to the positive\nlimiting value elz1\nts,∞characterized in the previous section.\n• On the time scale t = Θ(1) (right panel), test and train error collapse (as m →∞) on two\ndistinct limiting curves. The first one is constant and equal to elz1\nts,∞. The second one decreases\nfrom elz1\nts,∞to 0 and is predicted by the asymptotic theory in this section, cf. Eq. (E.70).\nNote that, in the example of Fig. 15, the initialization γ0 is sufficiently large that the train error de-\ncreases to zero on the time scale Θ(1), namely γ0 > γ∗\nGF(α, φ, τ), for a suitable threshold γ∗\nGF(α, φ, τ).\nAs we will see in the next section, a third dynamical regime emerges when γ0 < γ∗\nGF(α, φ, τ).\nE.2.3\nThe algorithmic interpolation threshold\nThe asymptotic theory within the second dynamical regime, described in Section E.2.2, turns out\nto be equivalent to the one in the pure-noise model, Section E.1.2, up to a change of variables.\nNamely, defining\n˜Co(t, s) = Clz2\no (t, s) + ∥vlz1\n∞∥2 ,\n(E.72)\n50\n\n\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1\n10\n100\n1000\nTrain/Test error\ntm\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nelz1\ntr(tm, 1)\nTrain/Test error\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n10−4\n10−3\n10−2\n10−1\n1\n10\n102\n103\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nelz2\ntr(t, 1)\nFigure 15: SymmDMFT predictions and large network scaling for lazy training in a single index\nmodel: train and test error.\nLeft frame: train and test error on the time scale t = Θ(1/m)\nfor several values of m, together with the asymptotic prediction as m →∞on this time scale\nelz1\ntr (ˆt, γ0) = elz1\nts (ˆt, γ0). Right: train and test error on the time scale t = Θ(1) for several values of\nm, together with the asymptotic prediction as m →∞on this time scale elz2\ntr (t, γ0). Here γ0 = 1,\nh(z) = ˆφ(z) = (9/10)z + z2/2, τ = 0.3, α = 0.3.\nwith initial condition ˜Co(0, 0) = −1 , reduce the equations of Section E.2.2 to the ones of Section\nE.1.2 with noise level τ replaced by\nτ ′2 = τ 2 + ∥φ∥2 −∥∇ˆφ(0)∥2\nh′(0)\n.\n(E.73)\nThe interpretation of this reduction is simple. On the time scale t = Θ(1), the first layer weights\nmove orthogonally to the latent subspace spanned by U. Hence, the dynamics on this timescale\nis not affected by the signal and only attempts to fit the labels noise. The noise is inflated as per\nEq. (E.73), because the network is not able to fit beyond the linear part of the target distribution.\nAs a corollary of the above equivalence, the interpolation threshold of the k-index model coin-\ncides with with the interpolation threshold on pure noise data with noise level given by Eq. (E.73).\nUsing the extended notation γ∗\nGF(α, φ, τ) to indicate the dependence on the underlying data distri-\nbution (which is parametrized by φ, τ), we can write the stated relation as\nγ∗\nGF(α, φ, τ) =\n\u0010\nτ 2 + ∥φ∥2 −∥∇ˆφ(0)∥2\nh′(0)\n\u00111/2\nγ∗\nGF(α, 0, 1) .\n(E.74)\n(Here we used the invariance under rescaling in the pure noise model, which implies γ∗\nGF(α, 0, τ 2) =\nτγ∗\nGF(α, 0, 1).)\nE.2.4\nDependence on m\nWithin NTK theory, it is normally assumed that optimal models are achieved at very large network\nsizes m →∞. Empirical results contradicting this expectation have been put forward in [VBN22],\nbut no theoretical analysis was provided either in [VBN22] or in subsequent work. We can use the\nSymmDMFT theory to fill this gap and study the dependence of test error on the number of neurons\nm under lazy initialization. We choose γ0 > γ∗\nGF(α, φ, τ), and therefore vanishing training error is\n51\n\n\n−0.1\n−0.05\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n1\n10\n102\n103\n104\n105\neℓ\n∞(m) −elz1\nts,∞\nm\nhs(z) = z2/2 + 0.9z\nhs(z) = z3/6 + 0.9z\nhs(z) = z4/24 + 0.9z\nhs(z) = z3/6 + z2/2 + 0.9z\n10−5\n10−4\n10−3\n10−2\n10−1\n1\n1\n10\n102\n103\n104\n|eℓ\n∞(m) −elz1\nts,∞|\nm\nhs(z) = z2/2 + 0.9z\nhs(z) = z3/6 + 0.9z\nhs(z) = z4/24 + 0.9z\nhs(z) = z3/6 + z2/2 + 0.9z\n∼x−1\n∼x−1/2\nFigure 16: The asymptotic behavior of the test error as a function of m for different h(z) = ˆφ(z).\nWe observe that soon as h(z) contains a z2 term, the NTK limit for m →∞is approached from\nbelow (left panel). Furthermore the speed of the convergence to the limiting value depends crucially\non whether a z2 monomial is present in the Taylor expansion of h(z) (right panel). The data has\nbeen produced with α = 0.3 and τ = 0.6.\nreached during the second dynamical regime, i.e. for t = Θ(1): this is therefore the last dynamical\nregime. Throughout this regime, we have γ(t) = γ0 + om(1).\nRecalling that ets(t, γ0, m, α) is the test error at time t in this setting, as predicted by Sym-\nmDMFT we consider the limit\neℓ\n∞(γ0, m, α) = lim\nt→∞ets(t, γ0, m, α) .\n(E.75)\nWe note that, for γ0 > γ∗\nGF(α, φ, τ), we expect\nlim\nm→∞eℓ\n∞(γ0, m, α) = elz1\nts,∞,\n(E.76)\nto be given by Eq. (E.59).\nIn Fig. 16 we plot the SymmDMFT prediction for eℓ\n∞(γ0, m, α) as a function of m for several\nchoices of h (we use h = ˆφ here). The limit m →∞of these curves matches elz1\nts,∞as expected.\nHowever we empirically observe that eℓ\n∞(γ0, m, α) approaches elz1\nts,∞in two qualitatively different\nways:\n• In the cases we consider that have h′′(0) ̸= 0, elz1\nts,∞is approached from below as m →∞,\nand eℓ\n∞(γ0, m, α) is non-monotone. We also observe that, for the values of m we consider, the\napproach to the asymptotic value is compatible with a rate m−1/2: eℓ\n∞(γ0, m, α) = elz1\nts,∞−\nΘ(m−1/2).\n• In the cases we consider that have h′′(0) ̸= 0, then elz1\nts,∞is approached from above as m →∞,\nand eℓ\n∞(γ0, m, α) is typically monotone. In this case the approach to the limiting behavior is\ncompatible with a rate m−1: eℓ\n∞(γ0, m, α) = elz1\nts,∞+ Θ(m−1).\nThe first scenario is the generic one, and similar to what is observed in [VBN22] for actual neural\nnetworks. An intuitive explanation is that –at finite m– the projection of neurons onto the latent\n52\n\n\nTrain/Test error\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n10−5\n10−4\n10−3\n10−2\n10−1\n1\n10\n102\n103\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nelz2\ntr(t, γ0)\nTrain/Test error\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n1 × 10−8\n1 × 10−6\n0.0001\n0.01\n1\n100\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nFigure 17: Train error (solid curves) and test error (dashed curves) for a model trained on a single\nindex data with h(z) = (9/10)z + z2/2 = ˆφ(z).\nThe noise level is τ = 2.5 and initialization\na(0) = γ0\n√m, γ0 < γ∗\nGF(α, φ, τ).\nLeft panel: timescales of order one.\nThe grey dashed line\ncorresponds to the scaling solution for m →∞when the second layer does not evolve with GF.\nRight panel: same data plotted versus t/m, to explore timescales of order m. The arrows show\nscaling appearing and curves collapsing on a master curve.\nspace ∥vlz1\n∞∥= Θ(1/√m) is sufficient for the network to partially learn the quadratic component of\nthe target function. In order to establish on more solid grounds these empirical observations one\nshould study the 1/m corrections to the scaling theory developed here. This is left for future work.\nE.2.5\nThird dynamical regime: t = Θ(m)\nAs for the pure noise case, beyond the time scale t = Θ(1), we distinguish two situations.\nIf\nγ0 > γ∗\nGF(α, φ, τ), then vanishing training error is reached within the second dynamical regime\nt = Θ(1). If γ0 < γ∗\nGF(α, φ, τ), GF dynamics develops an additional regime for t = Θ(m). In this\nsection, we study this third regime.\nIn Figure 17, we plot the SymmDMFT predictions for train and test errors as a function of time\nfor several values of m, for a setting with γ0 < γ∗\nGF(α, φ, τ). In particular, in Fig.17-left we plot train\nand test error as a function of t. The curves for the train error for increasing value of m collapse\non limit curve given by elz2\ntr (t, γ0) characterized in Section E.2.2. In other words, the dynamics on\nthis timescales follows the scaling theory of Section E.2.2. However in this case γ0 < γ∗\nGF(α, φ, τ),\nwhence by definition elz2\ntr,∞> 0. This correspond to the limit curve in Fig. 17-left having a strictly\npositive asymptote.\nFigure 17-right shows train and test error plotted against t/m. We observe that curves training\nerror curves collapse on a common limit, that decreases from elz2\ntr,∞to 0, while test error curves\nincrease above the plateau elz1\nts,∞. This suggests the following limit behavior\nlim\nm→∞etr(mˆt, γ0, m) = elz3\ntr (ˆt, γ0)\nlim\nm→∞ets(mˆt, γ0, m) = elz3\nts (ˆt, γ0) .\n(E.77)\nIn order to further explore the GF dynamics in this regime, in Fig. 18-left we plot the evolution\nof the second layer rescaled weights against t/m. The curves for increasing values of m collapse on\n53\n\n\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n10−8\n10−6\n10−4\n10−2\n1\n102\n0.1\n0.1\n0.1\n0.1\nγ(t)\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nγ∗\nGF −γm(t →∞)\n1/m\nγ∗\nGF −γm(t →∞)\n1/m\n∼m−1/2\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\netr\nγ\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nelz2\ntr,∞(γ)\nFigure 18: Training a two layer network in the same setting of Figure 17. Left panel: second layer\nweights on the timescale of order m. The black arrow corresponds to the interpolation threshold\nfor a model, γ∗\nGF (α, τ) obtained by fitting the relaxation time as a function of the weights of an\nlazy initialized model for γ0 > γ∗\nGF(α, τ). The second layer weights, at finite m develop a plateau\nat long time. In the inset we show the approach of this plateaus to the limiting value given by\nγ∗\nGF(α, φ, τ). Right panel: parametric plot of the train error as a function of the scaled weights of\nthe second layer. The dashed gray dashed line corresponds to the extrapolated train error for an\nnetwork with second layer weights fixed to the corresponding value in the m →∞(as extracted\nfrom the numerical integration of the scaling theory).\na master curve, suggesting the existence of a limit\nlim\nm→∞γ(mˆt, γ0) = γlz3(ˆt, γ0) .\n(E.78)\nThe limit curve γlz3(ˆt, γ0) increases from γ0 to a limit value:\nlim\nˆt→∞\nγlz3(ˆt, γ0) = γlz3\n∞(γ0).\n(E.79)\nAs in Section E.2.5 we consider the inverse function of t 7→γlz3(t, γ0), denoted by γ 7→˜γ−1(γ, γ0).\nIn Fig. 18-right we plot the train error as a function of the second layer weights γ(t). Again, for\nincreasing values of m the curves collapse on a master curve which is given by\nε(γ, γ0) = elz3\ntr (˜γ−1(γ, γ0), γ0)\n(E.80)\nWe then also plot in Fig.18-right the asymptotic value of the train error for a network initialized\nwith second layer weights blocked at an initialization scale γ, call it elz2\ntr,∞(γ).\nThe curves ε(γ, γ0) appear to have a vertical segment (corresponding to t = o(m)) in which the\ntraining error decreases, while γ(t) = γ0+om(1) is nearly unchanged, and a continuously decreasing\nsegment in which γ(t) increases while etr(t, γ0) decreases to 0 (corresponding to t = Θ(m)). In the\nsecond phase, the curves appear to converge to elz2\ntr,∞(γ) as m →∞. This suggests\nε(γ, γ0) = elz2\ntr,∞(γ)\n∀γ ≥γ0 .\n(E.81)\nIn other words the dynamics on timescales of order m is adiabatic also in the multi index case.\nFor a small change of the second layer weights on a scale of order √m, the train error relaxes to\n54\n\n\nits asymptotic value on timescales of order one. This graph suggests that the limit value of γ(t)\ncoincides with the critical value for interpolation. Namely recalling the definition (E.79) for the\nasymptotic value of γ(t), we have\nγlz3\n∞(γ0) = γ∗\nGF(α, φ, τ)\n(E.82)\nwhere the interpolation threshold in the multi-index model γ∗\nGF(α, φ, τ) is related to the interpola-\ntion threshold in the pure noise model via Eq. (E.74).\nF\nDynamical regimes: Mean field initialization\nIn this section we assume the initialization of the weights of the second layer is kept of order one.\nTo be definite, we set a(0) = a0, independent of m. This corresponds to the mean field initialization\nstudied in [MMN18, CB18, RVE22].\nSpecializing to the data distribution considered here, earlier work characterized the dynamics\nup to time T, under a few settings (which prove equivalent in this regime):\n• One-pass SGD, with stepsize ε ≪1/d and therefore time horizons such that T ≪d/n\n(the latter inequality follows from T ≤nε for one-pass SGD). In this case, the dynamics is\ncharacterized by a set of ODEs for for the projections of the weights on the latent space and\ninner products between weights.\n• Gradient flow in the population risk, which admits the same characterization and corresponds\nto the limit n →∞of the above.\n• The limit of the above regimes for large width m →∞. This is characterized by a partial\ndifferential equation for the distribution of projections of first layer weights onto the latent\nspace, provided T ≤c0 log m, for c0 a sufficiently small constant.\nWe refer to [BES+22, DLS22, AAM22, BEG+22, ASKL23, BMZ24] for a few pointers to this\nliterature. In all of these settings, the train error remains close to the test error. In contrast, the\nanalysis presented here allow us to explore the overfitting regime.\nSection F.1, we will focus on a pure noise data distribution, while Section F.2, considers a\nmulti-index model. As in the case of lazy initializations, we consider first the limit n, d →∞at\nn/md = α and m fixed (hence characterized by SymmDMFT ) and subsequently study dynamical\nregimes emerging as m →∞at n/md = α fixed.\nF.1\nPure noise model\nUnder the pure noise model, we have φ = ˆφ = 0. We identify three distinct dynamical regimes:\n• t = O(1): a(t) = a0 + om(1), etr(t) = τ 2/2 + om(1), and ∥wi(t) −wi(0)∥= om(1). In words,\nthe weights change minimally and the train error remains close to the one of the null network\nf(x; θ) ≈0 (Section F.1.1).\n• t = Θ(√m): a(t) = Θ(1), etr(t) = τ 2/2 + om(1), and ∥wi(t) −wi(0)∥= Θ(1). Namely,\nweights change but the train error does not change significantly. (Section F.1.2).\n55\n\n\n−1\n−0.9\n−0.8\n−0.7\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\n0\n0.1\n1\n10\n100\nCo(t, 0)m\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nCmf1\no\n(t, 0)\n−1\n−0.8\n−0.6\n−0.4\n−0.2\n0\n0.2\n0.4\n0.1\n1\n10\n100\nCo(t, t)m\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nCmf1\no\n(t, t)\nFigure 19: Training on pure noise data under mean-field initialization: t = Θ(1) regime. We plot\nCo(t, 0) and Co(t, t) as given by solving the SymmDMFT equations for different values of m and\ncompare them with the asymptotic solution of Section F.1.1. Here we use τ = 0.6, α = 0.3 and\nh(z) = (9/10)z + z3/6. Note that the vertical axis is multiplied by a factor m, in agreement with\nthe prediction of Eq. (F.2).\n• t = Θ(m). In this regime a(t) = √mγ(t/m) + om(1), and therefore the network complexity\nbecomes large enough for it to fit the noise. The dynamics on this timescale is closely related\nto the one under lazy initialization, studied in Section E.1.4. In particular, γ(ˆt) converge to\nthe interpolation threshold γ∗\nGF(α, τ) if ˆt →∞(after m →∞). (Section F.1.3).\nF.1.1\nFirst dynamical regime: t = O(1)\nIn this dynamical regime, the SymmDMFT equations are solved by the following scaling ansatz\nCd(t, s) = 1 + om(1)\nRd(t, s) = ϑ(t −s) + om(1) ,\n(F.1)\nmCo(t, s) = Cmf1\no\n(t, s) + om(1)\nmRo(t, s) = Rmf1\no (t, s) + om(1) ,\n(F.2)\na(t) = a0 + om(1)\nν(t) = om(1) .\n(F.3)\nFurthermore we have\nRmf1\nA (t, s) = δ(t −s) + om(1)\nCmf1\nA (t, s) = −τ 2 + om(1) ,\n(F.4)\nPlugging the scaling ansatz in the SymmDMFT , we obtain equations determining the scaling\nfunctions Cmf1\no\n, Rmf1\no . Defining\nρ0 := αa2\n0h′(0)\n(F.5)\nwe have\nRmf1\no (t, s) =\nh\ne−ρ0(t−s) −1\ni\nϑ(t −s) ,\nCmf1\no\n(t, t′) =\n\u0014\u00142τ 2\nρ0\n−1\nρ0\n\u0000τ 2 −ρ0\n\u0001\u0015\ne−2ρ0t′ −τ 2\nρ0\ne−ρ0t′\u0015\ne−ρ0(t−t′)\n+ τ 2 −ρ0\nρ0\n−τ 2\nρ0\ne−ρ0t′ .\n(F.6)\n56\n\n\nIn particular\nlim\nt→∞Cmf1\no\n(t, t) = τ 2 −ρ0\nρ0\n,\nlim\nt→∞Cmf1\no\n(t, t′) = τ 2 −ρ0\nρ0\n−τ 2\nρ0\ne−ρ0t′ ,\nlim\nt→∞,t′→∞,t−t′≥0 Cmf1\no\n(t, t′) = τ 2 −ρ0\nρ0\n.\n(F.7)\nThe equations (F.4) imply that the train error is constant in this regime and equal to\netr(t) = τ 2\n2 + om(1) .\n(F.8)\nIn other words, in this regime both first and second layer weights change minimally and the resulting\nerror remains close to the one to the null function f(x; θ) ≈0. We will see that this regime is\nsignificantly more interesting for the case of data with a signal, see Section F.2. We note in passing\nthat the limit value ⟨wj, wj⟩≈τ 2−ρ0\nmρ0\nfor i ̸= j corresponds to minimizing the empirical risk under\nthe linear approximation in which σ(z) is replaced by\np\nh′(0)z.\nThe above predictions are tested in Fig. 19 where we plot Co(t, t) and Co(t, 0) for different\nvalues of m and check their approach to the scaling functions Cmf1\no\n(t, 0) and Cmf1\no\n(t, t).\nF.1.2\nSecond dynamical regime: t = Θ(√m)\nWe now consider the case in which time scales as √m. The following asymptotic forms can be\nchecked to solve the SymmDMFT equations, up to higher order terms, for suitable choices of the\nscaling functions on the right-hand side:\nCd(t√m, s√m) = Cmf2\nd\n(t, s) + om(1)\nRd(t√m, s√m) = Rmf2\nd (t, s) + om(1) ,\n(F.9)\nCo(t√m, s√m) = 1\nmCmf2\no\n(t, s) + om(m−1)\nRo(t√m, s√m) = 1\nmRmf2\no (t, s) + om(m−1) ,\n(F.10)\n√mRA(t√m, s√m) = δ(t −s) + om(1)\nCA(t√m, s√m) = −τ 2 + om(1) ,\n(F.11)\n√mν(t√m) = νmf2(t) + om(1)\na(t√m) = amf2(t) + om(1) .\n(F.12)\nPlugging this scaling ansatz into the SymmDMFT equations we get the constraints\nRmf2\no (t, s) = −Rmf2\nd (t, s) ,\nCmf2\no\n(t, s) = −Cmf2\nd\n(t, s) +\nτ 2\nαh′(0)(amf2(t))2 .\n(F.13)\nWe also obtain that the following equations must be satisfied by Cmf2\nd\n(t, t′), Rmf2\nd (t, t′), amf2(t),\nνmf2(t),\n∂tCmf2\nd\n(t, t′) = −νmf2(t)Cmf2\nd\n(t, t′) + ατ 2amf2(t)\nZ t\n0\namf2(s)h′′(Cmf2\nd\n(t, s))Rmf2\nd (t, s)Cmf2\nd\n(t′, s) ds\n(F.14)\n57\n\n\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n0.1\n1\n10\n100\nν(t)√m\nt/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nˆνmf2(t/√m)\n−1\n−0.9\n−0.8\n−0.7\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\n0\n0.1\n1\n10\n100\nCo(t, 0)m\nt/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nCmf2\no\n(t/√m, 0)\n−1\n−0.8\n−0.6\n−0.4\n−0.2\n0\n0.2\n0.4\n0.01\n0.1\n1\n10\n100\n1000\n10000\nˆCo(t, t) = Co(t, t)m\nt/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nCmf2\no\n\u0010\nt\n√m,\nt\n√m\n\u0011\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n0.1\n1\n10\n100\na(t)\nt/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\namf2 \u0010\nt\n√m\n\u0011\nFigure 20: Training on pure noise data under mean-field initialization: t = Θ(1) regime, under the\nsame setting as in Fig. 19. We plot the solutions of the SymmDMFT equations for several values of\nm as a function of t/√m. We compare these to the m →∞scaling theory of Section F.1.2, i.e. to\nnumerical solutions of Eqs. (F.14) to (F.17).\n+ ατ 2amf2(t)\nZ t′\n0\namf2(s)\n\u0002\nh′(Cmf2\nd\n(t, s)) −h′(0)\n\u0003\nRmf2\nd (t′, s) ds ,\n∂tRmf2\nd (t, t′) = δ(t −t′) −νmf2(t)Rmf2\nd (t, t′)\n(F.15)\n+ ατ 2amf2(t)\nZ t\nt′ amf2(s)h′′(Cmf2\nd\n(t, s))Rmf2\nd (t, s)Rmf2\nd (s, t′) ds ,\nνmf2(t) = ατ 2amf2(t)\nZ t\n0\n\u0002\namf2(s)h′′(Cmf2\nd\n(t, s))Rmf2\nd (t, s)Cmf2\nd\n(t, s)\n\u0003\nds\n(F.16)\n+ ατ 2amf2(t)\nZ t\n0\namf2(s)\n\u0002\nh′(Cmf2\nd\n(t, s)) −h′(0)\n\u0003\nRmf2\nd (t, s) ds ,\ndamf2(t)\ndt\n= ατ 2\nZ t\n0\namf2(s)\n\u0002\nh′(Cmf2\nd\n(t, s)) −h′(0)\n\u0003\nRmf2\nd (t, s) ds ,\n(F.17)\nwith initial conditions given by\nCmf2\nd\n(0, 0) = 1\nRmf2\nd (0+, 0) = 1\namf2(0) = a0 .\n(F.18)\nWe test these predictions in Fig. 20. We plot several quantities in the solution of the Sym-\nmDMFT equations for increasing values of m and compare them with the solution of the asymptotic\n58\n\n\n0\n5\n10\n15\n20\n25\n30\n0\n20\n40\n60\n80\n100\n120\n140\namf2(t)\nt\nFigure 21: Evolution of second layer weights in the he weights of the second layer, as predicted by\nthe numerical solution of Eq. (F.17). Here we use h(z) = (9/10)z + z3/6, α = 0.3 and τ = 0.6. The\nstraight line is just a guide to the eyes to test the prediction of Eq. (F.29).\nequations (F.14) to (F.17). We observe convergence to the predicted asymptotic behavior.\nEquations (F.14) to (F.17) can be further simplified. The right-hand side of Eq. (F.17) is a\npositive. Therefore amf2(t) is a monotone increasing function. Define the time change\n˜t(t) = τ√α\nZ t\n0\namf2(s) ds ,\n(F.19)\nand the corresponding time-changed scaling functions\n˜ν(˜t(t)) =\nνmf2(t)\namf2(t)τ√α ,\n˜Cmf\nd (˜t(t), ˜t(t′)) = Cmf2\nd\n(t, t′) ,\n˜Rmf\nd (˜t(t), ˜t(t′)) = Rmf2\nd (t, t′) .\n(F.20)\nEquations (F.14) to (F.17) imply that these time-changed function functions satisfy\n∂t ˜Cmf\nd (t, t′) = −˜νmf(t) ˜Cmf\nd (t, t′) +\nZ t\n0\n˜h′′( ˜Cmf\nd (t, s)) ˜Rmf\nd (t, s) ˜Cmf\nd (t′, s) ds\n(F.21)\n+\nZ t′\n0\n˜h′( ˜Cmf\nd (t, s)) ˜Rmf\nd (t′, s) ds ,\n∂t ˜Rmf\nd (t, t′) = δ(t −t′) −˜νmf(t) ˜Rmf\nd (t, t′) +\nZ t\nt′\n˜h′′( ˜Cmf\nd (t, s)) ˜Rmf\nd (t, s) ˜Rmf\nd (s, t′) ds\n(F.22)\n˜νmf(t) =\nZ t\n0\n˜h′′( ˜Cmf\nd (t, s)) ˜Rmf\nd (t, s) ˜Cmf\nd (t, s) ds +\nZ t\n0\n˜h′( ˜Cmf\nd (t, s)) ˜Rmf\nd (t, s) ds ,\n(F.23)\nwhere again ˜h(z) = h(z) −h′(0)z.\nEquations (F.21), (F.23) are independent of the dynamics of the second layer weights. These\nequations are nothing but the DMFT equations describing gradient descent dynamics of the cele-\nbrated spherical mixed p-spin glass model [CHS93, CK93, BADG06, FFRT20], whose definition we\n59\n\n\nrecall next. Consider a random cost function H(x) indexed x ∈Sd−1, which is a centered Gaussian\nprocess with covariance structure given by\nE (H(x)H(y)) = d ˜h(⟨x, y⟩) .\n(F.24)\nDefine the gradient flow dynamics\n˙x(t) = −P ⊥\nx(t)∇H(x(t)) ,\n(F.25)\nwhere P ⊥\nx(t) is the projector orthogonal to x(t). Then the high-dimensional asymptotics of this\ndynamics is characterized by Eqs. (F.21), (F.23). In particular limd→∞⟨x(t), x(s)⟩= ˜Cmf\nd (t, t′)\nalmost surely.\nA particularly interesting quantity is the asymptotic energy value in the mixed p-spin model:\nE = lim\nt→∞lim\nd→∞\n1\ndH(x(t)) .\n(F.26)\nThe DMFT analysis for this problem implies that\nE = −lim\nt→∞\nZ t\n0\n˜h′( ˜Cmf\nd (t, s)) ˆRmf\nd (t, s) ds .\n(F.27)\nFor ˜h(z) = c2\nkzk, k ≥2, we have the explicit expression [CK93, CD95, Sel24]\nE = −2ck\nr\nk −1\nk\n.\n(F.28)\nAn explicit expression for E for general covariance structure is an unknown [FFRT20].\nThe asymptotic energy E has an interesting interpretation for the dynamics of two-layer net-\nworks –within the SymmDMFT theory. Eq. (F.28) implies that\nlim\nt→∞\namf2(t)\nt\n= −τ√αE =: A∞.\n(F.29)\nIn Fig. 21 we test the prediction of Eq. (F.29) by integrating numerically Eqs. (F.14) to (F.17)\nand plotting the prediction for the second-layer weigths amf(t). We observe that at large t, amf2(t) ≈\nA∞t, with A∞given by Eq. (F.29)as predicted.\nWe also note that CA(t, t) = −τ 2 also in this timescale, and hence the train error does not\nchange significantly. Namely , for any constant t, we have\netr(t√m) = 1\n2τ 2 + om(1) .\n(F.30)\nIf we use heuristically Eq. (F.28) and Eq. (F.12) beyond the\n√\nt time scale, we obtain\na(t) ≈amf2\u0000t/√m\n\u0001\n≈A∞\nt\n√m .\n(F.31)\nThis suggests that a(t) becomes of order √m on timescale of order m. When this happens, the\nnetwork complexity is large enough to allow for interpolation, and hence we expect the dynamics\nto change. Indeed a new dynamical regime emerges for t = Θ(m), as we will study next.\n60\n\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0\n2\n4\n6\n8\n10\na(t)/√m\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nA∞t/m\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n10−4\n10−3\n10−2\n10−1\n1\n10\n102\n103\na(t)/√m\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nFigure 22: Training on pure noise data under mean-field initialization: t = Θ(m) regime. Rescaled\nsecond layer weights a(t)/√m as a function of t/m. We plot solutions of the SymmDMFT equations\nfor the setting of Fig. 19.\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.001\n0.01\n0.1\n1\n10\n100\netr\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\n−0.02\n−0.01\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n10−6\n10−5\n10−4\n10−3\n10−2\n10−1\n1\n101\n10−2\nν(t)\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nFigure 23: Train error and Lagrange multiplier ν(t) on timescales of order m under mean field\ninitialization. Solutions of the SymmDMFT equations for the setting of Fig. 19. Finite m curves\naccumulate on master curves suggesting the existence of scaling functions.\nF.1.3\nThird dynamical regime: t = Θ(m)\nAs anticipated, an additional regime arises on timescales of order m. In Figure 22 we plot the\nevolution of the weights of the second layer as a function of t/m for increasing values of the width\nm. The different curves collapse suggesting the following limit to exist\nlim\nm→∞\na(tm)\n√m\n= γmf3(t) .\n(F.32)\nThe limit curve appears to grows linearly at small t, γmf3(t) = A∞t + o(t), where A∞is the\ncoefficient computed in the previous section, cf. Eq. (F.29). Hence, this third dynamical regime\nmatches directly with the previous one. As can be seen from the right plot, there appear to be a\nfinite limit limt→∞γmf3(t) < ∞.\nWe now turn to the analysis of the train error. Recall that on the previous timescales, the\ntrain error stays approximately constant, and equal to the train error of the null network, namely\n61\n\n\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n10−3\n10−2\n10−1\netr\nγ\nelz2\ntr(∞, γ)\n−0.06\n−0.04\n−0.02\n0\n0.02\n0.04\n0.06\n0.08\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nν(t)\nγ\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nνlz2\n∞(γ)\nFigure 24: Train error, rescaled second-layer weights and the Lagrange multiplier ν(t) on timescales\nof order m under mean field initialization. Left Panel: parametric plot of the train error as a function\nof the weights of the second layer on the scale √m, namely γ = a(t)/√m. The inset shows the\nsame data on a logarithmic scale. Right Panel: same plot for the Lagrange multiplier ν. Data is\nthe same as in Fig. 19.\netr(t√m) = τ 2/2 + om(1) for any fixed t. In Fig. 23 we plot both the train error and the Lagrange\nmultiplier ν as a function of t/m. Again, as m grows, these curve converge to limit curves. This\nsuggests the existence of the following limits\nlim\nm→∞etr(tm) = emf3\ntr (t) ,\n(F.33)\nlim\nm→∞ν(tm) = νmf3(t) .\n(F.34)\nNote that in this case, differently from the lazy initialization setting, the corresponding scaling\nfunction do not depend on the initialization parameter a0.\nIn order to characterize the limits in Eqs. (F.33)-(F.34), we proceed as in Sec. E.1.4. Namely,\nin Fig. 24 we plot the train error and the Lagrange multiplier ν as a function of the rescaled second\nlayer weights γ = a(t)/√m. We also plot the asymptotic value of train error and Lagrange multiplier\nunder the constrained GF dynamics in which second layer weigths are fixed to a(t) = γ√m and\ndo not evolve with time: elz2\ntr,∞(γ) := limt→∞elz2\ntr (t, γ) and νlz2\n∞(γ) := limt→∞νlz2(t, γ). These are\ncomputed by integration of the scaling theory in Section E.1.2.\nThe good collapse on these curves suggests to consider the the following construction, analogous\nto Sec. E.1.4. Define the inverse function of t 7→γmf3(t), denoted by (γmf3)−1. Then, define\nεmf3(γ) = emf3\ntr ((γmf3)−1(γ)) ,\nνmf3\n∗\n(γ) = νmf3((γmf3)−1(γ)) .\n(F.35)\nFigure 24 suggests that\nεmf3(γ) ≈elz2\ntr,∞(γ) ,\n(F.36)\nνmf3\n∗\n(γ) ≈νlz2\ntr,∞(γ) .\n(F.37)\nEquations (F.36), (F.36) imply that on timescales of order m the dynamics is adiabatic. For\neach incremental change of a on a the scale √m, all one-time quantities relax to the asymptotic\nvalue which turns out to be the same as a constrained model with a(t)/√m = γ fixed.\n62\n\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n1\n10\n100\n1000\nv(t)\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nemf(t)\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n5.5\n6\n0.1\n1\n10\n100\n1000\na(t)\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\namf1(t)\nFigure 25: Gradient flow dynamics under mean field initialization in the first dynamical regime\nt = O(1). for data distributed according to a single index model. Curves are numerical solutions\nof the SymmDMFT equations: we plot v(t) and a(t) for different values of m and compare them\nto the mean field predictions. Data is distributed according to a single index model with ht(z) =\nˆφ(z) = h(z) = (9/10)z + z3/6 with τ = 0.6 and α = 0.3.\nThe consequence of Eqs. (F.36)-(F.36) is that\nlim\nt→∞γmf3(t) ≈γ∗\nGF(α, τ) .\n(F.38)\nwhere γ∗\nGF(α, τ) corresponds to the interpolation value of the initialization scale of a lazy model.\nF.2\nMulti-index model\nIn this section we consider the case in which the dataset is distributed according to a multi-index\nmodel. For time scales beyond t = O(1), we will assume that h(z) = ˆφ(z). This simplifies the\nasymptotics for t large but of order one.\nWe identify two dynamical regimes emerging as m →∞:\n• t = O(1): a(t) = O(1) but is not constant. Also, the projection v(t) of first layer weights\nonto the latent space evolve as well as do train and test error. We further have etr(t) =\nets(t) + om(1): there is no overfitting. This evolution is captured the mean field theory of\n[MMN18, CB18] which we recover as m →∞limit of SymmDMFT .\n• t = Θ(m): a(t) = Θ(√m), v(t) decreases towards 0 and train and test error diverge. In this\ndynamical regime the network unlearns to a large extent the latent structure of the data and\noverfit it.\nF.2.1\nFirst dynamical regime: t = Θ(1)\nFor timescales of order one, the SymmDMFT equations are solved, up to subleading terms as m →\n∞, by the following ansatz\nCd(t, s) = Cmf1\nd\n(t, s) + om(1) ,\nCo(t, s) = Cmf1\no\n(t, s) + om(1) ,\n(F.39)\n63\n\n\n−0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n10−1\n1\n101\n102\n103\nTrain/Test error\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nτ 2/2\n−0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n10−6\n10−4\n10−2\n1\n102\nTrain/Test error\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nτ 2/2\nFigure 26: Gradient flow dynamics under mean field initialization a(0) = 1. The train (solid curves)\nand test (dashed curves) errors as a function of time t (left panel) and scaled time t/m (right panel).\nCurves are numerical solutions of the SymmDMFT equations for h(z) = (9/10)z+z3/6, ˆφ(z) = h(z),\nτ = 0.6 and α = 0.3. The arrow on the right panel corresponds to the asymptotic test error for a\nmodel with second layer weights fixed to the corresponding interpolation threshold.\nRd(t, s) = Rmf1\nd (t, s) + om(1) ,\nmRo(t, s) = Rmf1\no (t, s) + om(1)\n(F.40)\nv(t) = vmf1(t) + om(1) ,\na(t) = amf1(t) .\n(F.41)\nThe corresponding scaling equations are then given by\n∂tRmf1\no (t, t′) = −νmf1(t)Rmf1\no (t, t′) −αamf1(t)2h′(Cmf1\no\n(t, t))\n\u0000Rmf1\nd (t, t′) + Rmf1\no (t, t′)\n\u0001\n,\n∂tCmf1\no\n(t, t′) = −νmf1(t)Cmf1\no\n(t, t′) + α⟨∇ˆφ(vmf1(t)), vmf1(t′)⟩amf1(t) −αamf1(t)2h′(Cmf1\no\n(t, t))Cmf1\no\n(t, t′) ,\nνmf1(t) = α⟨∇ˆφ(vmf1(t)), vmf1(t)⟩amf1(t) −αamf1(t)2h′(Cmf1\no\n(t, t))Cmf1\no\n(t, t)\n∂tCmf1\nd\n(t, t′) = −νmf1(t)Cmf1\nd\n(t, t′) + α⟨∇ˆφ(vmf1(t)), vmf1(t′)⟩amf1(t) −αamf1(t)2h′(Cmf1\no\n(t, t))Cmf1\no\n(t, t′) ,\n∂tRmf1\nd (t, t′) = −νmf1(t)Rmf1\nd (t, t′) + δ(t −t′) ,\n∂tvmf1(t) = −νmf1(t)vmf1(t) + α∇ˆφ(vmf1(t))amf1(t) −αamf1(t)2h′(Cmf1\no\n(t, t))vmf1(t) ,\n∂tamf1(t) = α ( ˆφ(vmf1(t)) −amf1(t)h(Cmf1\no\n(t, t))) .\n(F.42)\nThese equations are solved by setting:\nCmf1\no\n(t, t′) = ⟨vmf1(t), vmf1(t′)⟩\n(F.43)\nwith vmf1(t), amf1(t) the solution of\n∂tvmf1(t) = αamf1(t)\n\u0000Ik −vmf1(t)vmf1(t)T\u0001 \u0000∇ˆφ(vmf1(t)) −amf1(t)h′(∥vmf1(t)∥2)vmf1(t)\n\u0001\n,\n∂tamf1(t) = α\n\u0000ˆφ(vmf1(t)) −amf1(t)h(∥vmf1(t)∥2)\n\u0001\n,\n(F.44)\nwith initial conditions given by vmf1(0) = 0 and amf1(0) = a0.\nEquations (F.44) coincide with the mean field theory of [MMN18, CB18, RVE22], when the lat-\nter are specialized to the multi-index model studied here, under symmetric initializations [BMZ24,\nSection 4.2]. (See also [ASKL23].) Using the ansatz of Eqs. (F.39) to (F.41) in the formulas for\ntraining and test error (B.45), (B.46), we get\nlim\nm→∞etr(t) = lim\nm→∞ets(t) = emf1(t) ,\n(F.45)\n64\n\n\n−5\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n0\n10\n20\n30\n40\n50\nm(a(t) −amf1(t))\nt\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\n˜a(t)\n−30\n−25\n−20\n−15\n−10\n−5\n0\n0\n10\n20\n30\n40\n50\nm(v(t) −vmf1(t))\nt\nm3\nm4\nm5\nm6\nm7\nm8\nm9\nm10\nm11\nm12\nm13\nm14\n˜v(t)\nFigure 27: The 1/m corrections to the second layer weights and the projection on the latent space of\nthe single index model on timescales of order 1. Dashed lines are obtained by integrating numerically\nEqs. (F.56) to (F.59) determining the limits m →∞. Here, ˆφ(z) = h(z) = (9/10)z + z3/6 with\nτ = 0.6 and α = 0.3.\nwith\nemf1(t) = 1\n2\n\u0002\nτ 2 + ∥φ∥2 −2amf1(t) ˆφ(vmf1(t)) + amf1(t)2h(∥vmf1(t)∥2)\n\u0003\n.\n(F.46)\nA particularly simple case is the one in which k = 1 (single index model) and φ = σ (whence\nˆφ = h). For a class of such activations with h′(0) > 0, we have amf1(t), vmf1(t) →1 as t →∞and\ntherefore\nlim\nt→∞emf1(t) = τ 2\n2 .\n(F.47)\nIn other words, neurons align perfectly with latent direction, the generalization error vanishes, and\nand train and test error converge for large constant t to the Bayes error τ 2/2.\nIn Fig. 25 we compare the solution of Eqs. (F.44) with the numerical integrations of the Sym-\nmDMFT equations for a range of values of m. As m increases, the SymmDMFT solutions converge\nto the asymptotic predictions vmf1(t), amf1(t), confirming the above ansatz.\nSimilarly, in Fig. 26-left panel we compute the train and test error by solving the SymmDMFT equations\nand compare the results to the asymptotic prediction provided by Eq. (F.46). We observe that –as\npredicted– train and test error match on an increasingly long time interval. At a certain point,\nthey diverge: we will next characterize the timescale on which this happens.\nF.2.2\nEscape from the mean field dynamical regime\nIn order to understand on which time scale the dynamics diverges from mean field theory described\nabove, we will study small deviations from this theory. We expect that these deviations will diverge\nwith time. Characterizing this divergence will allow to determine time scale on which we exit the\nmean field regime.\nWe focus on the case of a single index model k = 1, with ˆφ = h, and set a(0) = 1. We believe\nthat the qualitative conclusions obtained in this case apply more generally. We also assume h to\nbe such that the long time asymptotics of mean field dynamical solutions is\nlim\nt→∞amf1(t) = 1 ,\nlim\nt→∞vmf1(t) = 1 .\n(F.48)\n65\n\n\nAs mentioned in the previous section, this holds for a broad class of activations. In other words,\nfor time t large and yet of order one, the neurons are very well aligned.\nWe next study the corrections to the mean field solution. We claim that such corrections are\nof order 1/m and define the functions ˜a(t), ˜v(t), dots , ˜Ro(t, t′), via\nm(a(t) −amf1(t)) = ˜a(t) + om(1) ,\n(F.49)\nm(v(t) −vmf1(t)) = ˜v(t) + om(1) ,\n(F.50)\nm(Cd(t, t′) −Cmf1\nd\n(t, t′)) = ˜Cd(t, t′) + om(1) ,\n(F.51)\nm(Co(t, t′) −Cmf1\no\n(t, t′)) = ˜Co(t, t′) + om(1) ,\n(F.52)\nm(Rd(t, t′) −Rmf1\nd (t, t′)) = ˜Rd(t, t′) + om(1) ,\n(F.53)\nm(Ro(t, t′) −Rmf1\no (t, t′)) = ˜Ro(t, t′) + om(1) ,\n(F.54)\nm(ν(t) −νmf1(t)) = ˜ν(t) + om(1) .\n(F.55)\nSubstituting the above form into the SymmDMFT equations and matching the next-to-leading order\nin m we can obtain the equations for the 1/m corrections. It turns out that equations for ˜a, ˜v, ˜Co\nand ˜ν decouple from the equations for ˜Cd, ˜Rd and ˜Ro. Given that we are interested in the former\nquantities we only report the corresponding equations:\nd˜a(t)\ndt\n=α ˆφ′(vmf1(t))˜v(t) −α ˆφ(vmf1(t))\nZ t\n0\nΣ(1)\nR (t, s) ds −αamf1(t) [h(1) −h(Cmf1\no\n(t, t))]\n(F.56)\n+ α\nZ t\n0\nΣ(1)\nR (t, s)amf1(s)h(Cmf1\no\n(t, s))ds −α˜a(t)h(Cmf1\no\n(t, t)) −αamf1(t)h′(Cmf1\no\n(t, t)) ˜Co(t, t)\n−α\nZ t\n0\nCmf1\nA (t, s)amf1(s)\n\u0002\nh′(Cmf1\nd\n(t, s))Rmf1\nd (t, s) + h′(Cmf1\no\n(t, s))Rmf1\no (t, s)\n\u0003\nds ,\nd˜v(t)\ndt\n= −νmf1(t)˜v(t) −˜ν(t)vmf1(t) + α ˆφ(vmf1(t))˜a(t) + α ˆφ′′(vmf1(t))˜v(t)amf1(t)\n(F.57)\n−α ˆφ′(vmf1(t))amf1(t)\nZ t\n0\nΣ(1)\nR (t, s) ds −\nZ t\n0\nh\n˜\nM(d)\nR (t, s) −M(0)\nR,o(t, s)\ni\nvmf1(s) ds\n−\nZ t\n0\nh\nM(1)\nR,o(t, s)vmf1(s) + M(0)\nR,o(t, s)˜v(s)\ni\nds ,\n˜ν(t) =α ˆφ′(vmf1(t))˜v(t)amf1(t) + α ˆφ′′(vmf1(t))vmf1(t)˜v(t)amf1(t)\n(F.58)\n+ α ˆφ′(vmf1(t))vmf1(t)˜a(t) −α ˆφ′(vmf1(t))vmf1(t)\nZ t\n0\nΣ(1)\nR (t, s) ds\n−\nZ t\n0\nh\n˜\nM(d)\nR (t, s)Cmf1\nd\n(t, s) −M(0)\nR,o(t, s)Cmf1\no\n(t, s)\ni\nds\n−\nZ t\n0\nh\nM(1)\nR,o(t, s)Cmf1\no\n(t, s) + M(0)\nR,o(t, s) ˜Co(t, s)\ni\nds\n−\nZ t\n0\nh\n˜\nM(d)\nC (t, s)Rmf1\nd (t, s) + M(0)\nC,o(t, s)Rmf1\no (t, s)\ni\nds ,\n∂˜Co(t, t′)\n∂t\n= −νmf1(t) ˜Co(t, t′) −˜ν(t)Cmf1\no\n(t, t′) + α ˆφ′′(vmf1(t))˜v(t)vmf1(t′)amf1(t)\n(F.59)\n+ α ˆφ′(vmf1(t))˜v(t′)amf1(t) + α ˆφ′(vmf1(t))vmf1(t′)˜a(t)\n−α ˆφ′(vmf1(t))vmf1(t′)amf1(t)\nZ t\n0\nΣ(1)\nR (t, s) ds\n66\n\n\n−\nZ t\n0\nh\n˜\nM(d)\nR (t, s)Cmf1\no\n(t′, s) + M(0)\nR,o(t, s)Cmf1\nd\n(t′, s) −2M(0)\nR,o(t, s)Cmf1\no\n(t′s)\ni\nds\n−\nZ t\n0\nh\nM(0)\nR,o(t, s) ˜Co(t′, s) + M(1)\nR,o(t, s)Cmf1\no\n(t′, s)\ni\nds\n−\nZ t′\n0\nh\n˜\nM(d)\nC (t, s)Rmf1\nd (t′, s) + M(0)\nC,o(t, s)Rmf1\no (t′, s)\ni\nds .\nHere, we used the following auxiliary functions\nΣ(1)\nR (t, s) = amf1(t)amf1(s)\n\u0002\nh′(Cmf1\nd\n(t, s))Rmf1\nd (t, s) + h′(Cmf1\no\n(t, s))Rmf1\no (t, s)\n\u0003\n,\n(F.60)\nCmf1\nA (t, s) = −\n\u0002\nτ 2 + ht(1) −amf1(t)φ(vmf1(t)) −amf1(s)φ(vmf1(s)) + amf1(t)amf1(s)h(Cmf1\no\n(t, s))\n\u0003\n,\n(F.61)\n˜\nM(d)\nR (t, s) = αamf1(t)amf1(s)\n\u0002\nh′(1)δ(t −s) + Cmf1\nA (t, s)h′′(Cmf1\nd\n(t, s))Rmf1\nd (t, s)\n\u0003\n,\n(F.62)\nM(0)\nR,o(t, s) = α(amf1(t))2h′(Cmf1\no\n(t, s))δ(t −s) ,\n(F.63)\nM(1)\nR,o(t, s) = α\nh\n2amf1(t)˜a(t)h′(Cmf1\no\n(t, t)) + (amf1(t))2h′′(Cmf1\no\n(t, t)) ˜C(t, t)\ni\nδ(t −s)\n(F.64)\n−αamf1(t)amf1(s)Σ(1)\nR (t, s)h′(Cmf1\no\n(t, s))\n(F.65)\n+ αamf1(t)amf1(s)Cmf1\nA (t, s)h′′(Cmf1\no\n(t, s))Rmf1\no (t, s) ,\n(F.66)\n˜\nM(d)\nC (t, s) = αamf1(t)amf1(s)Cmf1\nA (t, s)h′(Cmf1\nd\n(t, s)) ,\n(F.67)\nM(0)\nC,o(t, s) = αamf1(t)amf1(s)Cmf1\nA (t, s)h′(Cmf1\no\n(t, s)) .\n(F.68)\nNote that Eqs. (F.56) to (F.59) are a set of four integral-differential equations for the four functions\n˜a(t), ˜v(t), ˜ν(t), ˜Co(t, t′). The original SymmDMFT equations involve three other functions: ˜Cd(t, t′),\n˜Rd(t, t′), ˜Ro(t, t′)? We also remark that: (i) These equations are linear in the unknowns ˜a(t), ˜v(t),\n˜ν(t), ˜Co(t, t′); (ii) They can be integrated numerically with the same strategy used to integrate the\nSymmDMFT equations.\nIn Fig. 27 we plot the deviations from the mean field limit m(a(t)−amf1(t)) and m(v(t)−vmf1(t))\nas a function of time t, as obtained by solving the SymmDMFT equations2, for several values of m.\nWe also plot the predicted limits ˜a(t), ˜v(t), which are obtained by integrating Eqs. (F.56) to (F.59)\nAs m gets large, the finite-m curves appear to converge to the predictions ˜a(t), ˜v(t).\nIn Figure 28 we plot the result of integrating Eqs. (F.56) to (F.59) over a wider time window.\nWe observe that ˜v, ˜a, ˜ν and ˜Co(t, t) diverge linearly with t. This suggests the following asymptotics\nfor these corrections\nlim\nt→∞\n˜a(t)\nt\n= a∗,\nlim\nt→∞\n˜v(t)\nt\n= v∗,\n(F.69)\nlim\nt→∞\n˜ν(t)\nt\n= ν∗,\nlim\nt→∞\n˜Co(t, t)\nt\n= c∗.\n(F.70)\nThe values of the constant a∗, v∗, ν∗and c∗can be obtained analytically by using the above ansatz\nin Eqs. (F.56) to (F.59). We obtain that they solve the following linear equations\n0 = ˆφ′(1)v∗+ ˆφ(1)a∗,\n(F.71)\n0 = ˆφ′(1)c∗+ 2 ˆφ(1)a∗,\n(F.72)\n2We note that solving the SymmDMFT equations accurately enough to capture these corrections requires either\nto use very fine discretization, or a higher-order integration method.\n67\n\n\n1\n10\n100\n1000\n10000\n10\n100\n1000\nt\n˜a(t)\na∗t\n−˜v(t)\n−v∗t\n−˜Co(t, t)\n−c∗t\n−˜ν(t)\n−ν∗t\nFigure 28: The 1/m corrections to v(t), a(t), Co(t, t) and ˜ν(t) as a function of time as extracted\nfrom the numerical integration of the corresponding equations. The dashed lines are the asymptotic\npredictions for t →∞which show that the divergence of all quantities is linear with time. Here,\nˆφ(z) = h(z) = (9/10)z + z3/6 with τ = 0.6 and α = 0.3.\n0 = −ˆφ′(1)ν∗−ˆφ′(1)\n\u0000α ˆφ′′(1) −α ˆφ′(1) −α( ˆφ′(1))2\u0001\na∗+ 2α ˆφ(1) ˆφ′′(1) ,\n(F.73)\n0 = −1\n2c∗−ν1c∗−2ν∗v1 + 4v1ατ 2 ,\n(F.74)\nwhere\nv1 := lim\nt→∞(v(t) −1)t ,\n(F.75)\nν1 := lim\nt→∞˜ν(t)t .\n(F.76)\nThe asymptotic linear behavior predicted by Eqs. (F.69), (F.70), with the coefficients determined by\nEqs. (F.71)-(F.74) is plotted in Fig. 28. We observe good agreement with the numerical integration\nof Eqs. (F.56) to (F.59).\nThe above analysis implies that (considering to be definite second layer weights, and projection\nof first layer weigths onto the latent direction), for m ≫1, t ≫1,\na(t) = amf1(t) + 1\nm\n\u0000a∗t + o(t)\n\u0001\n+ 1\nm∆a(t, m) ,\n(F.77)\nv(t) = vmf1(t) + 1\nm\n\u0000v∗t + o(t)\n\u0001\n+ 1\nm∆v(t, m) ,\n(F.78)\nwhere limm→∞∆a/v(t, m) = 0. If we neglect the error terms, and assume that this expression\nholds for t larger than O(1) in m, then it indicates that a(t), v(t) differ significantly from the mean\nfield prediction when t/m becomes of order one. We expect therefore a third dynamical regime for\nt = Θ(m), which will be the object of the next section.\n68\n\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n10−3\n10−2\n10−1\n1\n10\n102\na(t)/√m\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.001\n0.01\n0.1\n1\n10\n100\nv(t)\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nFigure 29: Gradient flow under mean field initialization on timescales of order m. Left: rescaled\nsecond layer weights a(t)/m as a function of the rescaled time t/m. The arrow on the right points at\nthe threshold γ∗\nGF(α, φ, τ) for interpolation under gradient flow, see Section E.2.3. Right: projection\nof the first layer weights on the latent space in the single index model as a function of rescaled time\nt/m. Here, ˆφ(z) = h(z) = (9/10)z + z3/6 with τ = 0.6 and α = 0.3. v = 1/γ in (E.57).\n0.1\n1\n10\n100\n1000\n10000\n0.01\n0.1\n1\nv√m\nγ = a/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\n1/γ\nFigure 30: Same data as in Fig. 29: parametric plot of the rescaled projection onto the latent\ndirection v√m against rescaled second layer weights γ = a/√m. Dashed line is v√m = 1/γ.\n69\n\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.001\n0.01\n0.1\n1\netr\na/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nτ 2/2\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nets\netr\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nFigure 31: Gradient flow under mean field initialization, for increasing values of m. Left: train\nerror as a function of rescaled weights a(t)/√m. Dashed line is the Bayes error τ 2/2. Curves are\ntraversed in time from top to bottom. Right: test error versus train error. Curves are traversed in\ntime from right to left. Here ˆφ(z) = h(z) = (9/10)z + z3/6 with τ = 0.6 and α = 0.3.\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nets −etr\na/√m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nLazy\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.1\n1\n10\n100\nets −etr\nt/m\nm = 23\nm = 24\nm = 25\nm = 26\nm = 27\nm = 28\nm = 29\nm = 210\nm = 211\nm = 212\nm = 213\nm = 214\nFigure 32: Left panel: the difference between test and train error plotted as a function of a/√m\nand compared to what is obtained from a model with fixed second layer weights initialized with\nLazy scaling. Right panel: the difference between test and train error on timescales of order m.\nHere, ˆφ(z) = h(z) = (9/10)z + z3/6 with τ = 0.6 and α = 0.3.\n70\n\n\nF.2.3\nSecond dynamical regime: t = Θ(m) and beyond\nAs pointed out at the end of the previous section, we expect a third dynamical regime when\nt = Θ(m). By this time, the stability calculation in the previous section indicates that second\nlayer weights become of order √m. Figure 29 confirms this, and shows that, in the same regime\nv(t) becomes small. In fact, numerical solution of the SymmDMFT equations are consistent with\na(t) = Θ(√m), v(t) = Θ(1/√m), and a(t)v(t) ≈1 for t = Θ(m).\nFor a small constant c denote by t0(m; c) the time at which a(t0(m; c)) = c√m. We then expect\nthat the following exists\nlim\nm→∞\na(t0(m; c) + θ w(m))\n√m\n= γmf3(θ) ,\n(F.79)\nlim\nm→∞v(t0(m; c) + θ w(m))√m = v+(θ) ,\n(F.80)\nprovided w(m) is a suitable function (with w(m) = O(t0(m; c))). The stability analysis in the\nprevious section suggests that t0(m; c) ≤t∗(c)m + o(m). Our numerical solutions do not cover a\nlarge enough range of values of m to verify this ansatz, and determine the scaling of w(m) with m.\nOn the other hand, they indicate that indeed t0(m; c) = Θ(m).\nSince the second layer weights become of order √m in this dynamical regime, train and test\nerror start to differ significantly. We expect\nlim\nm→∞etr(t0(m; c) + θ w(m)) = emf3\ntr (θ) ,\n(F.81)\nlim\nm→∞etr(t0(m; c) + θ w(m)) = emf3\nts (θ) .\n(F.82)\nThis picture is confirmed by Fig. 31, which reports train and test error as predicted by numerical\nsolutions of the SymmDMFT equations for increasing values of m. On the left, we plot the train\nerror as a function of the rescaled second layer weights γ = a/√m. We observe that curves for\ndifferent values of m decrease until they reach the Bayes error τ 2. On this phase however different\ncurves do not collapse corresponding to the fact that γ vanishes. In the second phase, γ grows to\nbe of order one and correspondingly the train error decreases below the Bayes error: this is the\nthird dynamical regime. Overfitting takes place at this point.\nIn the right frame of Fig. 31, we plot test error versus train error. We observe, again, the two\nphases emerging for large m. In the first phase train error and test error are closely matched. In\nthe second phase, train error decreases and test error correspondingly increases. Again, this takes\nplace when t = Θ(m).\nFinally, in Fig. 32, we repeat similar plots for the generalization error (difference between test\nand train error).\nWhen t/m is large, the train error vanishes. We observe from Figure 29, left frame that, as\nt →∞, rescaled second layer weights reach a finite limit that is close to the interpolation threshold\ncharacterized in Section E.2.3. Namely\nlim\nτ→∞γ+(θ) ≈γ∗\nGF(α, φ, τ) .\n(F.83)\n71\n\n\n1\n10\n100\n1000\n0.2\n0.4\n0.6\n0.8\n1\ntrel/m\nα\nm = 5\nm = 10\nm = 20\nm = 25\nm = 40\nm = 100\nm = 500\nm = 1000\n1\n10\n100\n1000\n0.1\n1\ntrel/m\n|α −αGF(m)|\nm = 5\nm = 10\nm = 20\nm = 25\nm = 40\nm = 100\nm = 500\nm = 1000\nf(x) = 6.4x−2\n1.08\n1.09\n1.1\n1.11\n1.12\n1.13\n1.14\n1.15\n1.16\n1.17\n1.18\n10\n100\n1000\nαGF(m)\nm\n1\n10\n100\n1000\n0.1\n1\ntrel\nαGF −α\nm →∞\n∼x−2\nFigure 33: The interpolation transition for pure noise data and a network with second layer weights\nthat do not evolve with time, fixed at a = 1, see Section G. The noise level is fixed to τ = 1 and\nwe considered h(z) = (3/10)z + z2/2. Top left panel: relaxation time ((rate for convergence to\nvanishing error) for different values of m. Top right panel: logarithmic plot of the relaxation time.\nThe value of the algorithmic threshold for different values of m is a fitting parameter. Bottom left\npanel: values of the algorithmic thresholds as a function of m. Bottom right panel: the relaxation\ntime as extracted from the scaling limit of the SymmDMFT equations in the m →∞limit. The\nalgorithmic threshold is in this case αGF(∞) ≈1.18 which fits well the behavior plotted in the left\nbottom plot.\n72\n\n\nG\nDynamics under mean field initialization for n/d = α fixed\nG.1\nInterpolation threshold at fixed a(t) = a0\nIn this section, we consider an alternative scaling in the large width limit. As before, we use the\nSymmDMFT equations, and therefore study the limit n, d →∞with n/d →α. In the previous\nsections we studied the large width limit m →∞with α = α/m fixed. In that setting interpolation\nis only possible when the network complexity scales, i.e. second-layer weights are a = Θ(√m)\nHere instead we keep a(t) = 1 and do not let evolve second-layer weights with GF. We consider\npure noise data, and show that interpolation takes place if α < αGF(m), while the train error remains\nbounded away from zero for α > αGF(m). As expected from Gaussian complexity considerations,\nthe threshold αGF(m) has a finite limit as m →∞. In particular, for any α > 0, a network with a\nbounded cannot interpolate pure noise data.\nAs thorough in Sec.E we fix α and integrate numerically the SymmDMFT equations for finite\nbut increasing values of m. We fix the initialization scale a0 and the noise level τ and change only\nα.\nWe observe that for α small enough the train error decreases exponentially fast to zero. Namely,\nrecalling that etr(t; α) := limn,d→∞b\nRn(a(t), W (t)), we have that\nα < αGF(m)\n⇒\netr(t; α) = exp{−t/t∗\nrel(α, m) + o(t)} .\n(G.1)\nHowever, the relaxation time time t∗\nrel(α, m) increases as α ↑αGF(m).\nConcretely, we define\ntrel(α, m, c) as the infimum time such that etr(t; α) ≤c, where c is some small constant. In practice,\nwe set c = 10−7. The results are plotted as a function of α for several values of m in Fig. 33, top\nleft plot.\nFor each value of m the relaxation time appears to diverge at the critical point αGF(m) as an\ninverse power of αGF(m) −α, namely:\nα ↑αGF(m)\n⇒\ntrel(α, m, c) =\nL(m, c)\n(αGF(m) −α)ν\n\u00001 + o(1)\n\u0001\n.\n(G.2)\nThe exponent ν appears to be independent of m. We fit this form to our data and extract the\ninterpolation thresholds αrel(m). In Fig. 33, top right, we plot trel(α, m, c)/m as a function of the\ngap to this threshold. This plot confirms the form (G.2), with exponent ν ≈2. Also, the fact that\ndifferent curves superimpose indicate that L(m, c) ≈L∗(c)m.\nThe estimated interpolation thresholds αGF(m) are plotted as a function of m in the bottom\nleft of Fig. 33. These data are consistent with the existence of a finite limit\nαGF(∞) = lim\nm→∞αGF(m) ,\n(G.3)\nand numerically αGF(∞) ≈1.18.\nIn the next subsection, we derive equations describing the m →∞limit for α = O(1), a = O(1)\nfixed. Studying these equations yields further support to Eq. (G.3).\nG.2\nInfinite width limit at fixed α\nIn order to study the limit m →∞at fixed α, we discuss the limit of the SymmDMFT equations\nwhen m →∞. As we have seen previously, the relaxation time of the train error is proportional to\n73\n\n\nm. This is clearly visible in Fig. 33-top/left. This suggests that for m →∞, dynamics takes place\non timescales of order m. Therefore we propose the following asymptotic ansatz\nmRo(tm, sm) = ˜Rα\no (t, s) + om(1) ,\nCo(tm, sm) = ˜Cα\no (t, s) + om(1) ,\n(G.4)\nRd(tm, sm) = ˜Rα\nd (t, s) + om(1) ,\nCd(tm, sm) = ˜Cα\nd (t, s) + om(1) ,\n(G.5)\nmν(tm) = ˜να(t) + om(1) ,\n(G.6)\nwhich defines a set of functions, ˜Rα\nd , ˜Cα\nd , ˜Rα\no , ˜Cα\no and ˜να. We now describe the equations that these\nscaling functions satisfy satisfy. First we define ˜Cα\nA and ˜Rα\nA as the solution of\nδ(t −t′) =\nZ t\nt′\nh\nδ(t −s) + ˜ΣR(t, s)\ni\n˜Rα\nA(s, t′ds , )\n0 =\nZ t\n0\nh\nδ(t −s) + ˜ΣR(t, s)\ni\n˜Cα\nA(t′, s) ds +\nZ t′\n0\nds˜ΣC(t, s) ˜Rα\nA(t′, s) ds ,\n(G.7)\nwhere\n˜ΣR(t, s) = h′( ˜Cα\nd (t, s)) ˜Rα\nd (t, s) + h′( ˜Cα\no (t, s)) ˜Rα\no (t, s)\n˜ΣC(t, s) = τ 2 + h( ˜Cα\no (t, s)) .\n(G.8)\nThen we define the limit memory kernels:\n˜\nM(d)\nR (t, s) = α ˜CA(t, s)h′′( ˜Cα\nd (t, s)) ˜Rα\nd (t, s) ,\n˜\nM(d)\nC (t, s) = α ˜Cα\nA(t, s)h′( ˜Cα\nd (t, s)) ,\n˜\nM(o)\nR (t, s) = α\nh\n˜CA(t, s)h′′( ˜Cα\no (t, s)) ˜Rα\no (t, s) + ˜Rα\nA(t, s)h′( ˜Cα\no (t, s))\ni\n,\n˜\nM(o)\nC (t, s) = α ˜Cα\nA(t, s)h′( ˜Cα\no (t, s)) .\n(G.9)\nSubstituting the above ansatz in the SymmDMFT equations and matching the leading order terms,\nwe get the following equations that determine ˜Rα\nd , ˜Cα\nd , ˜Rα\no , ˜Cα\no and ˜να:\n∂t ˜Cα\nd (t, t′) = −˜να(t) ˜Cα\nd (t, t′) −\nZ t\n0\nh\n˜\nM(d)\nR (t, s) ˜Cα\nd (t′, s) + ˜\nM(o)\nR (t, s) ˜Cα\no (t′, s)\ni\nds\n−\nZ t′\n0\nh\n˜\nM(d)\nC (t, s) ˜Rα\nd (t′, s) + ˜\nM(o)\nC (t, s) ˜Rα\no (t′, s)\ni\nds ,\n(G.10)\n∂t ˜Cα\no (t, t′) = −˜να(t) ˜Cα\no (t, t′) −\nZ t\n0\nh\n˜\nM(d)\nR (t, s) + ˜\nM(o)\nR (t, s)\ni\n˜Cα\no (t′, s) ds\n−\nZ t′\n0\nh\n˜Rα\no (t′, s) + ˜Rα\nd (t′, s)\ni\n˜\nM(o)\nC (t, s) ds ,\n(G.11)\n∂t ˜Rα\nd (t, t′) = −˜να(t) ˜Rα\nd (t, t′) + δ(t −t′) −\nZ t\nt′ ds ˜\nM(d)\nR (t, s) ˜Rα\nd (s, t′) ,\n(G.12)\n∂t ˜Rα\no (t, t′) = −˜να(t) ˜Rα\no (t, t′) −\nZ t\nt′\nh\n˜\nM(d)\nR (t, s) ˜Rα\no (s, t′) + ˜\nM(o)\nR (t, s) ˜Rα\nd (s, t′)\n+ ˜\nM(o)\nR (t, s) ˜Rα\no (s, t′)\ni\nds ,\n(G.13)\n74\n\n\n0.001\n0.01\n0.1\n1\n0.1\n1\n10\n100\netr\nt/m\nm = 5\nm = 10\nm = 50\nm = 100\nm = 200\nm = 1000\nm →∞\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nCd(t, 0)\nt/m\nm = 5\nm = 10\nm = 50\nm = 100\nm = 200\nm = 1000\nm →∞\nFigure 34: Check of the convergence of the numerical solution of the SymmDMFT for α fixed to\nthe scaling solution for m →∞. The left panel shows the behavior of the train error while the\nright panel shows the behavior of the correlation Cd(t, 0). Both panels refer to a model where the\nteacher is pure noise with τ = 1 and the student is made of of neurons whose covariance structure\nis given by h(z) = (3/10)z + z2/2.\n˜να(t) = −\nZ t\n0\nds\nh\n˜\nM(d)\nR (t, s) ˜Cd(t, s) + ˜\nM(o)\nR (t, s) ˜Cα\no (t, s)\ni\nds\n−\nZ t\n0\nh\n˜\nM(d)\nC (t, s) ˜Rα\nd (t, s) + ˜\nM(o)\nC (t, s) ˜Rα\no (t, s)\ni\nds .\n(G.14)\nThese are to be solved with boundary condition\n˜Cα\no (0, 0) = 0 ,\n˜Rα\no (0, 0) = 0 ,\n(G.15)\n˜Cα\nd (0, 0) = 1 ,\n˜Rα\nd (0+, 0) = 1 .\n(G.16)\nThe scaling behavior of the train error is then given by\nlim\nm→∞etr(t) = −1\n2\n˜Cα\nA(t, t) =: eα\ntr(t) .\n(G.17)\nIn order to test the accuracy of the asymptotic analysis developed in this sections, we solved\nnumerically the SymmDMFT equations for increasing values of m and compare the results to the\nnumerical integration of Eqs. (G.10), (G.14) presented in this section. Some results of this compar-\nison are presented in Fig. 34, which shows good agreement between finite-m curves and m →∞\nlimit.\nThe solution of Eqs. (G.10), (G.14) provides another route to estimate the large-m interpolation\nthreshold αGF(∞) at fixed a(t) = 1. Namely, we solve the equations numerically and extract the\ntrel(α, ∞, c), which is defined analogously to above. We then fit the divergence of trel(α, ∞, c) at\nαGF(∞) according to Eq. (G.2).\nWe obtain αGF(∞) ≈1.18, in agreement with the threshold\nobtained by extrapolating the finite-m thresholds αGF(m). In the bottom right plot of Fig. 33 we\nplot trel(α, ∞, c) as function of αGF(∞) −α. This confirms the behavior of Eq. (G.2) with ν ≈2.\nWe conclude by emphasizing that, throughout this section α(t) = 1 and τ = 1 were fixed. If we\ngeneralize to arbitrary α(t) = a0 and arbitrary τ > 0, the threshold αGF(m) will of course on these\nquantities through the ratio a0/τ.\n75\n\n\nH\nLower bound on the overfitting timescale\nH.1\nProof of Lemma 3.1\nBy computing the derivative ∂ai b\nR(a(t), W (t)), we get\nd\ndt\n\f\faℓ(t)\n\f\f ≤α ·\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\n\u0000yi −f(xi; a(t), W (t)\n\u0001\nσ(wℓ(t)Txi)\n\f\f\f\f\f\n≤α\nq\n2 b\nRn(a(t), W (t)) ·\nv\nu\nu\nt 1\nn\nℓ\nX\ni=1\nσ(wT\nℓxi)2\n≤4Lα\nq\n2 b\nRn(a(0), W (0)) ·\nv\nu\nu\nt 1\nn\nℓ\nX\ni=1\n(1 + (wℓ(t)Txi)2)\n≤10Lα\nq\n2 b\nRn(a(0), W (0)) ,\nwhere the last inequality holds with probability at least 1 −2 exp(−cn) (for some universal c > 0)\nby standard upper bounds on the norm of random matrices [Ver18]. Further\nq\n2n b\nRn(a(0), W (0)) =\n\r\r\ry −1\nm\nm\nX\ni=1\naiσ(Xwi)\n\r\r\r\n(a)\n≤∥y∥+ a0 max\ni≤m\n\r\rσ(Xwi)\n\r\r\n(b)\n≤τ∥g∥+ ∥φ(XU)∥+ a0 max\ni≤m\n\r\rσ(Xwi)\n\r\r\n≤τ∥g∥+ Lφ∥XU∥+ a0L\n\r\rX\n\r\r\nop + a0\n√n|σ(0)| ,\nwhere in (a) it is understood that σ is applied entrywise to Xwii ∈Rn and in (b) we have\ng ∼N(0, In) and φ is applied row-wise to XU ∈Rn×k. By using standard concentration on the\nnorm of random matrices, with the same probability\nq\n2 b\nRn(a(0), W (0)) = C(τ + a0L) .\nSummarizing the above bounds\nd\ndt\n\f\faℓ(t)\n\f\f ≤b0 ,\nwhich implies the first claim by integration.\nTo prove the second claim, we consider the following sets of parameters W∞\nm,d(a) ⊆Wm,d(a)\n(which will also prove useful in the next section)\nWm,d(a) :=\nn\n(a, W ) ∈Rm × Rm×d : ∥a∥1\nm\n≤a, ∥wi∥2 = 1 ∀i ≤m\no\n,\n(H.1)\nW∞\nm,d(a) :=\nn\n(a, W ) ∈Rm × Rm×d : ∥a∥∞≤a, ∥wi∥2 = 1 ∀i ≤m\no\n.\n(H.2)\n76\n\n\nThe second claim follows in turn if we prove that\nsup\n(a,W )∈Wm,d(a)\n\f\f b\nRn(a, W ) −R(a, W )\n\f\f ≤C(α, L, τ) a2\n√m .\n(H.3)\nThis is a well-known result, that we reproduce for the readers’ convenience. By usual concentra-\ntion, it is sufficient to bound the expectation. This in turns is bounded by symmetrization and\ncontraction inequality:\nE\nsup\n(a,W )∈Wm,d(a)\n\f\f b\nRn(a, W ) −R(a, W )\n\f\f ≤E\nsup\n(a,W )∈Wm,d(a)\n1\nn\nn\nX\ni=1\nεi\n\u0000yi −f(xi; a, W )\n\u00012\n≤2E\nsup\n(a,W )∈Wm,d(a)\n1\nn\nn\nX\ni=1\nεiyif(xi; a, W ) + E\nsup\n(a,W )∈Wm,d(a)\n1\nn\nn\nX\ni=1\nεif(xi; a, W )2\n=: E1 + E2 .\nWe bound term E2, since E1 follows analogously:\nE1 =E\nsup\n(a,W )∈Wm,d(a)\nm\nX\ni,l=1\najal\nm2\n1\nn\nn\nX\ni=1\nεiσ(wT\nj xi)σ(wT\nl xi)\n≤a2E\nsup\nw, ˜w∈Sd−1\n1\nn\nn\nX\ni=1\nεiσ(wTxi)σ( ˜wT\nl xi)\n≤L2a2E\nsup\nw∈Sd−1\n1\nn\nn\nX\ni=1\nεiwTxi ,\nwhere the last inequality follows by applying the contraction inequality of [Mau16] to ϕ(t1, t2) =\nσ(t1)σ(t2).\nH.2\nProof of Proposition 3.2\nWe also introduce the notations:\ngw\nn,ℓ(a, W ) :=\nn\nd|aℓ|\n\u0002\n∇wℓb\nRn(a, W ) −∇wℓR(a, W )\n\u0003\n,\n(H.4)\nga\nn,ℓ(a, W ) := n\nd\n\u0002\n∇aℓb\nRn(a, W ) −∇aℓR(a, W )\n\u0003\n.\n(H.5)\nWe begin by establishing a uniform convergence lemma.\nLemma H.1. Under the data distribution of Section A, assume ∥φ∥∞≤L and the activation\nfunction to be bounded differentiable with Lipschitz continuous first derivative ∥σ∥∞, ∥σ′∥∞, ∥σ∥∞≤\nL. Then there exists a universal constant C1, and a constant c0 > 0 dependent on L, τ, α such that,\nletting Cα = C1(α ∨1), with probability at least 1 −2 exp(−nc0),\nsup\n(a,W )∈Wm,d(a)\nmax\nℓ≤m\n\r\rgw\nn,ℓ(a, W )\n\r\r ≤Cα(L2a + τ 2)\nr\nlog(2m)\nm\n,\n(H.6)\nsup\n(a,W )∈Wm,d(a)\nmax\nℓ≤m\n\f\fga\nn,ℓ(a, W )\n\f\f ≤Cα(L2a + τ 2) 1\n√m .\n(H.7)\n77\n\n\nProof. Gradient with respect to wℓ. By a concentration argument, it is sufficient to consider\nthe expected supremum. Writing the formula for ∇wℓb\nRn and using a standard symmetrization\nargument, we get\nE\nsup\n(a,W )∈Wm,d(a)\n\r\rgw\nn,ℓ(a, W )\n\r\r = E\nsup\n(a,W )∈Wm,d(a),∥u∥≤1\n⟨u, gw\nn,ℓ(a, W )⟩\n≤2α E sup\na,W ,u\n1\nn\nn\nX\ni=1\nεiyiσ′(wT\nℓxi)uTxi + 2α E sup\na,W ,u\n1\nn\nn\nX\ni=1\nεi\nm\nX\nj=1\naj\nmσ(wT\nj xi)σ′(wT\nℓxi)uTxi\n=: B1 + B2 ,\nwhere the εi are i.i.d. Radamacher random variables and in the last two lines it is understood that\nthe supremum is over (a, W ) ∈Wm,d(a), ∥u∥≤1. Consider the second term in the last expression:\nB2 = 2αa E sup\nW ,u\nmax\nj≤m\n1\nn\nn\nX\ni=1\nεiσ(wT\nj xi)σ′(wT\nℓxi)uTxi\n≤2αa E\nsup\nw,w,u∈Bd(1)\n1\nn\nn\nX\ni=1\nεiσ(wTxi)σ′(wTxi)uTxi\n≤2αa E\nsup\nw,w,u∈Bd(1)\n1\nn\nn\nX\ni=1\nεiσ(wTxi)σ′(wTxi)η(uTxi)\n+ 2αa E\nsup\nw,w,u∈Bd(1)\n1\nn\nn\nX\ni=1\nεiσ(wTxi)σ′(wTxi)η(uTxi)\n=: B2,1 + B2,2 ,\nwhere η(x) = x1|x|≤M + M(1x>M −1x<−M), and η(x) = x −η(x).\nDefining ϕ(t1, t2, t3) :=\nσ(t1)σ′(t2)η(t3) (which is CL2M-Lipschitz for M ≥1) and using a generalization of Talagrand’s\ncontraction inequality [Mau16],\nB2,1 ≤CL2Mαa\n(\nE\nsup\nw∈Bd(1)\n1\nn\nn\nX\ni=1\nεiwTxi + E\nsup\nw∈Bd(1)\n1\nn\nn\nX\ni=1\nεiwTxi + E\nsup\nu∈Bd(1)\n1\nn\nn\nX\ni=1\nεiuTxi\n)\n≤CL2Mαa\nr\nd\nn\n≤CαL2a M\n√m .\nNext consider B2,2:\nB2,2 ≤2αaL2E\nsup\nu∈Bd(1)\n1\nn\nn\nX\ni=1\n|η(uTxi)|\n≤2αaL2\nsup\nu∈Bd(1)\nE|η(uTxi)| + 2αaL2E\nsup\nu∈Bd(1)\n1\nn\nn\nX\ni=1\nεi|η(uTxi)|\n≤CαL2ae−M2/4 + CαL2a\nr\nd\nn ,\nwhere the last inequality holds because uTxi is Gaussian with variance ∥u∥2, and using again the\ncontraction inequality. Collecting various terms and optimizing over M ≥1, we obtain\nB2 ≤CαL2a\nn M\n√m + e−M2/4o\n78\n\n\n≤CαL2a\nr\nlog m ∨1\nm\n.\nThe proof of Eq. (H.6) is completed by bounding B1 along the same lines.\nGradient with respect to aℓ. Writing ∇aℓb\nRn and using symmetrization, we get\nE\nsup\n(a,W )∈Wm,d(a)\n\f\fga\nn,ℓ(a, W )\n\f\f = E\nsup\n(a,W )∈Wm,d(a),∥u∥≤1\nga\nn,ℓ(a, W )\n≤2α E sup\nW ,u\n1\nn\nn\nX\ni=1\nεiyiσ(wT\nℓxi) + 2α E sup\na,W ,u\n1\nn\nn\nX\ni=1\nεi\nm\nX\nj=1\naj\nmσ(wT\nj xi)σ(wT\nℓxi)\n=: D1 + D2 .\nConsider term D2, and define the L2-Lipschitz function ψ(t1, t2) := σ(t1)σ(t2),\nD2 ≤2αa E sup\nW ,u\nmax\nj≤m\n1\nn\nn\nX\ni=1\nεiσ(wT\nj xi)σ(wT\nℓxi)\n≤2αa E\nsup\nw,w∈Bd(1)\n1\nn\nn\nX\ni=1\nεiψ(wTxi, wTxi)\n≤CL2aα\nr\nd\nn ≤CαL2a 1\n√m .\nTerm D1 is controlled analogously, yielding the proof of Eq. (H.7).\nWe next prove some continuity properties of the population risk R. It is useful to recall the\nform:\nR(a, W ) = 1\n2(τ 2 + ∥φ∥2) −1\nm\nm\nX\ni=1\nai bφ(U Twi) +\n1\n2m2\nm\nX\ni,j=1\naiajh(wT\ni wj) .\n(H.8)\nLemma H.2. Under the data distribution of Section A, assume ∥φ∥∞≤L and the activation\nfunction to be bounded differentiable with Lipschitz continuous first derivative ∥σ∥∞, ∥σ′∥∞, ∥σ∥∞≤\nL. Then, there exists an absolute constant C such that for any (a, W ), (a, ˜\nW ) ∈Wm,d(a):\n\r\r∇wℓR(a, ˜\nW ) −∇wℓR(a, W )\n\r\r ≤CL2 |aℓ|\nm (1 + a) max\nj≤m\n\r\r ˜wj −wj\n\r\r ,\n(H.9)\n\f\f∂aℓR(a, ˜\nW ) −∂aℓR(a, W )\n\f\f ≤CL2\nm (1 + a) max\nj≤m\n\r\r ˜wj −wj\n\r\r ,\n(H.10)\nand\n\r\r∇wℓR(˜a, W ) −∇wℓR(a, W )\n\r\r ≤CL\nm (1 + a)|˜aℓ−aℓ| + CL|aℓ|\nm2 ∥˜a −a∥1 ,\n(H.11)\n\f\f∂aℓR(˜a, ˜\nW ) −∂aℓR(a, W )\n\f\f ≤CL\nm2\n\r\r˜a −a\n\r\r\n1 .\n(H.12)\nProof. Proof of Eq. (H.9). Differentiating Eq. (H.8)\nm\naℓ\n∇wℓR(a, W ) = −U∇bφ(U Twℓ) +\nm\nX\nj=1\naj\nmh′\ns(wT\nℓwj)wj .\n(H.13)\n79\n\n\nTherefore\nm\n|aℓ|\n\r\r∇wℓR(a, ˜\nW ) −∇wℓR(a, W )\n\r\r ≤\n\r\r∇bφ(U T ˜wℓ) −∇bφ(U Twℓ)∥\n+\nm\nX\nj=1\n|aj|\nm\n\r\rh′\ns( ˜wT\nℓ˜wj) ˜wj −h′\ns(wT\nℓwj)wj\n\r\r\n≤CL2∥˜wℓ−wℓ∥+ a max\nj≤m\n\r\rh′\ns( ˜wT\nℓ˜wj) ˜wj −h′\ns(wT\nℓwj)wj\n\r\r .\nThe assumptions on σ imply that ∥h′∥Lip ≤L2, whence\n\r\rh′\ns( ˜wT\nℓ˜wj) ˜wj −h′\ns(wT\nℓwj)wj\n\r\r ≤CL2∥˜wj −wj∥+ CL2∥˜wℓ−wℓ∥.\nSubstituting above, this yields the claim (H.9).\nProof of Eq. (H.10). We proceed analogously to the previous point. Namely\nm∂aℓR(a, W ) = −bφ(U Twℓ) +\nm\nX\nj=1\naj\nmh(wT\nℓwj) ,\n(H.14)\nwhence\nm\n\f\f∂aℓR(a, ˜\nW ) −∂aℓR(a, W )\n\f\f ≤\n\f\fbφ(U T ˜wℓ) −bφ(U Twℓ)\n\f\f +\nm\nX\nj=1\n|aj|\nm\n\f\fh( ˜wT\nℓ˜wj) −h(wT\nℓwj)\n\f\f\n≤CL2∥˜wℓ−wℓ∥+ CaL2\u0000∥˜wℓ−wℓ∥+ ∥˜wj −wj∥\n\u0001\n,\nwhich implies immediately Eq. (H.10)\nProof of Eq. (H.11). Recalling Eq. (H.13), we have\nm\n\r\r∇wℓR(˜a, W ) −∇wℓR(a, W )\n\r\r ≤\n\r\r∇bφ(U Twℓ)∥|˜aℓ−aℓ| +\nm\nX\nj=1\n1\nm\n\r\rh′\ns(wT\nℓwj)wj\n\r\r |˜aℓ˜aj −aℓaj|\n≤CL|˜aℓ−aℓ| + CL|aℓ|\nm ∥˜a −a∥1 + CLa |˜aℓ−aℓ| ,\nwhich proves the desired claim.\nProof of Eq. (H.11). Recalling Eq. (H.13), we have\nm\n\f\f∂aℓR(˜a, W ) −∂aℓR(a, W )\n\f\f ≤\nm\nX\nj=1\n1\nm|h(wT\nℓwj)| · |˜aj −aj|\n≤CL\nm ∥˜a −a∥1\nUsing the last lemma and triangle inequality we get the following.\nCorollary H.3. Under the assumptions of Lemma H.2, we have, for all (a, W ), (a, ˜\nW ) ∈W∞\nm,d(a):\nmax\nℓ≤m\n\r\r∇wℓR(˜a, ˜\nW ) −∇wℓR(a, W )\n\r\r ≤CLa\nm (1 + a) max\nj≤m ∥˜wj −wj∥+ CL\nm (1 + a)∥˜a −a∥∞,\n(H.15)\nmax\nℓ≤m\n\f\f∂aℓR(˜a, ˜\nW ) −∂aℓR(a, W )\n\f\f ≤CL\nm (1 + a) max\nj≤m ∥˜wj −wj∥+ CL\nm ∥˜a −a∥∞.\n(H.16)\n80\n\n\nWe next consider a(t), W (t) that follows GF with respect to the empirical risk, as per Eq. (A.10),\nwhich we rewrite as\n˙a(t) = −n\nd∇aR(a(t), W (t)) ,\n˙wi(t) = −n\ndP ⊥\nwi∇wiR(a(t), W (t))\n∀i = 1, . . . , m ,\n(H.17)\nand denote by a0(t), W 0(t) the GF with respect to population risk:\n˙a0(t) = −n\nd∇aR(a0(t), W 0(t)) ,\n˙w0,i(t) = −n\ndP ⊥\nwi∇wiR(a0(t), W 0(t))\n∀i = 1, . . . , m .\n(H.18)\nLemma H.4. Under the data distribution of Section A, assume ∥φ∥∞≤L and the activation\nfunction to be bounded differentiable with Lipschitz continuous first derivative ∥σ∥∞, ∥σ′∥∞, ∥σ∥∞≤\nL. Let (a(t), W (t)), (a0(t), W 0(t)), be defined as above, with W (0) = W 0(0) and a(0) = a0(0)\nsuch that ∥a(0)∥1/m ≤a0. Define\nT∗(m; c) := inf\nn\nt :\n\u0000∥a(t)∥∞∨∥a0(t)∥∞\n\u0001\n≥(c log m)1/3o\n∧(c log m)1/3 .\n(H.19)\nThen there exist positive constants c1 = c1(L), c0 = c0(L, α, τ), C = C(α) such that, with probability\nat least 1 −2 exp(−c0n),\nsup\nt≤T∗(m;c1)\n∆(t) ≤\nC\nm1/4 ,\n∆(t) := max\nℓ≤m ∥˜wℓ(t) −wℓ(t)∥+ ∥˜a(t) −a(t)∥∞.\n(H.20)\nProof. We will prove that the desired bound holds on the high-probability event of Lemma H.1,\nwhere by we set a = (c1 log m)1/4. Throughout the proof, we use c0, c1, C to denote constants\nthat might change from line to line, with dependence on the parameters of the problem as per the\nstatement of the lemma.\nWe start by noting that, letting vi = −(n/d)∇wi b\nRn(a, W ) and v0,i = −(n/d)∇w0,iR(a0, W 0,\nand P ⊥\nw := I −wwT the projector orthogonal to w.\n\r\rP ⊥\nwivi −P ⊥\nw0,iv0,i\n\r\r ≤\n\r\rP ⊥\nwi(vi −v0,i)\n\r\r +\n\r\r(P ⊥\nwi −P ⊥\nw0,i)v0,i∥\n≤∥vi −v0,i∥+ ∥wiwT\ni −w0,iwT\n0,i∥op∥v0,i∥\n≤∥vi −v0,i∥+ 2∥wi −w0,i∥op∥v0,i∥.\nHence, comparing the evolution of wi(t) and w0,i(t), we get\nd\ndt∥wi(t) −w0,i(t)∥≤n\nd\n\r\r∇wi b\nRn(a(t), W (t)) −∇wiR(a0(t), W 0(t))\n\r\r\n+ n\nd∥∇wiR(a0(t), W 0(t)))∥· ∥wi(t) −wi,0(t)∥\n=: D1 + D2 · ∥wi(t) −wi,0(t)∥.\nSince we are working on the event of Lemma H.1, and using Corollary H.3, we get, for t ≤T∗(m; c).\nD1 ≤n\nd\n\r\r∇wi b\nRn(a(t), W (t)) −∇wiR(a(t), W (t))\n\r\r\n+ n\nd\n\r\r∇wiR(a(t), W (t)) −∇wiR(a0(t), W 0(t))\n\r\r\n81\n\n\n≤C(L2a + τ 2)\nr\nlog(2m)\nm\n+ CLα(1 + a2) max\nj≤m ∥wj(t) −w0,j(t)∥\n+ CLα(1 + a)∥a(t) −a0(t)∥∞.\nFurther\nD2 = α|ai|\n\r\r\rU∇bφ(U Twi) −1\nm\nm\nX\nj=1\najh′\ns(wT\ni wj)wj\n\r\r\r\n≤Cαa\n\u0000L + aL2\u0001\n.\nCollecting all the terms, we get\nd\ndt∥wi(t) −w0,i(t)∥≤C(L2a + τ 2)\nr\nlog(2m)\nm\n+ CLα(1 + a2)∆(t) .\n(H.21)\nWe next consider the evolution of second-layer weights:\nd\ndt|ai(t) −a0,i(t)∥≤n\nd\n\r\r∂ai b\nRn(a(t), W (t)) −∂aiR(a0(t), W 0(t))\n\r\r\n≤n\nd\n\r\r∂ai b\nRn(a(t), W (t)) −∂aiR(a(t), W (t))\n\r\r\n+ n\nd\n\r\r∂aiRn(a(t), W (t)) −∂aiR(a0(t), W 0(t))\n\r\r\n≤C(L2a + τ 2) 1\n√m + CLα(1 + a) max\nj≤m ∥wj(t) −w0,j(t)∥+ CLα∥a(t) −a0(t)∥∞\n≤C(L2a + τ 2) 1\n√m + CLα(1 + a)∆(t) .\nUsing the last bound together with Eq. (H.21), we get\nd\ndt∆(t) ≤C(L2a + τ 2)\nr\nlog(2m)\nm\n+ CLα(1 + a2)∆(t)\nwhence the claim follows by Gromwall inequality for sufficiently small c1.\nWe finally need a lemma from [BMZ24] approximating GF in the population risk by the mean\nfield dynamics.\nLemma H.5 (Corollary 1 and Proposition 3 [BMZ24]). Let a0(t), W 0(t) be GF with respect to the\npopulation risk (H.18) with initialization a0,i(0) = a0 and (w0,i(0))i≤m ∼Unif(Sd−1). Recall that\namf1(t), fv(t) is the solution of the ODEs (F.44) with initialization amf1 Under the assumptions of\nProposition 3.2, for any ε > 0 there exist constants c0, c1 depending uniquely on L, τ, ε, such that\nletting Tlb(m) = (c0 log m)1/3, the following happens with probability at least 1 −2 exp(−c1m ∧d),\nsup\nt≤Tlb(m)\n1\nm\nm\nX\ni=1\n\u0010\n|ai(t) −amf1\ni\n(t)| + ∥vi(t) −vmf1\ni\n(t)∥\n\u0011\n≤C mεn 1\n√m + 1\n√\nd\no\n,\n(H.22)\nsup\nt≤Tlb(m)\n\u0010\nR(a(t), W (t)) −ets(t)\n\u0011\n≤C mεn 1\n√m + 1\n√\nd\no\n.\n(H.23)\n82\n\n\nProof of Proposition 3.2. Throughout the proof L, τ, α are assumed to be fixed, and constants\nC, c0, . . . depend on them and can change from line to line. We will further work on the high prob-\nability events of Lemma 3.1, Lemma H.4, and Lemma H.5. By Lemma 3.1, for all t ≤Tlb(m) we have\n∥a(t)∥∞≤c2(log 2m)1/3 (where the constant c2 can be made sufficiently small, by eventually re-\nducing c1). An analogous of of Lemma 3.1 for the population risk implies ∥a0(t)∥∞≤c2(log 2m)1/3\nas well for all t ≤Tlb(m). Hence we can apply Lemma H.4 and Lemma H.5, which yields the\nclaim.\nI\nDynamical mean field theory for non-Gaussian model\nThe DMFT equations for GF in the original non-Gaussian model can be derived from the general\ntheory of [CCM21].\nGiven a (positive semi-definite) kernel Q : R≥0 × R≥0 →Rm×m, (t, z) 7→Q(t, s), we write\nz ∼GP(0, Q) if z is a centered Gaussian process with values in Rm and covariance E[z(t)z(s)T] =\nQ(t, s).\nThe DMFT equations can be interpreted as a set of fixed point equations for the functions\nCij, Rij, ai...\nWe define the deterministic processes a(t), νi(t) and stochastic processes we(t) = (we\ni (t) : i ≤\nm), r(t) = (ri(t) : i ≤m), as the solution of\ndai(t)\ndt\n= α\nmE\n\b\nE(t) σ(ri(t))\n\t\n,\n(I.1)\nνi(t) = α\nmai(t)E\n\b\nE(t)σ′(ri(t))ri(t)\n\t\n,\n(I.2)\ndwe\ni (t)\ndt\n= −νi(t)we\ni (t) −1\nm\nm\nX\nl=1\nZ t\n0\nMi,l(t, s)we\nl (s) ds\n(I.3)\n−\nk\nX\nj=1\nMi,j(t, ∗)uj + ηi(t) ,\nη ∼GP(0, CE) ,\nri(t) = 1\nm\nm\nX\nl=1\nZ t\n0\nRil(t, s) al(s)E(s) σ′(rl(s)) ds + ξi(t) ,\nξ ∼GP(0, C) ,\n(I.4)\nE(t) := y −1\nm\nm\nX\nl=1\nal(t)σ(rl(t)) .\n(I.5)\nHere, in the first equation, (we(0), u) ∼N(0, Im) ⊗N(0, Ik) are independent of η. In the second\nequation, y = φ(r0) + ε with (r0, ε) ∼N(0, Ik) ⊗N(0, τ 2) independent of ξ.\nMij(t, s) =αE{Sij(t)} δ(t −s) + α\nm\nX\nl=1\nE\nn\nSil(t) ∂rl(t)\n∂ξj(s)\n\t\n,\n(I.6)\nMij(t, ∗) = −αai(t)\nm E\n\b\nσ′(ri(t))∇jφ(r0)\n\t\n,\n(I.7)\nCE\ni,j(t, s) = αai(t)aj(s)\nm2\nE\n\b\nE(t)E(s) σ′(ri(t))σ′(rj(s))\n\t\n,\n(I.8)\nSij(t) := −ai(t) E(t)σ′′(ri(t))δij + ai(t)aj(t)\nm\nσ′(ri(t))σ′(rj(t)) ,\n(I.9)\n83\n\n\nand\nCij(t, s) = E\nn\nwe\ni (t)we\nj(s)\no\n,\n(I.10)\nRij(t, s) = E\nn∂we\ni (t)\n∂ηj(s)\no\n.\n(I.11)\nIn solving the above, the random functions ∂we\ni (t)\n∂ηj(s) and ∂ri(t)\n∂ξj(s) (for t > s) are defined to be solutions\nof the following linear ODEs:\nd\ndt\n∂we\ni (t)\n∂ηj(s) = −νi(t)∂we\ni (t)\n∂ηj(s) −1\nm\nm\nX\nl=1\nZ t\ns\nMi,l(t, t′)∂we\nl (t′)\n∂ηj(s) dt ,\n(I.12)\n∂ri(t)\n∂ξj(s) = 1\nm\nm\nX\nl=1\nZ t\ns\nRil(t, t′) al(t′)E(t′) σ′′(rl(t′))\nh∂rl(t′)\n∂ξj(s) + δljδ(t′ −s)\ni\ndt′\n(I.13)\n−1\nm2\nm\nX\nl,q=1\nZ t\ns\nRil(t, t′) aq(t′)al(t′)σ′(rq(t′)) σ′(rl(t′))\nh∂rq(t′)\n∂ξj(s) + δqjδ(t′ −s)\ni\ndt′ .\nwith boundary condition ∂we\ni (t)\n∂ηj(t) = δij for the first equation.\nJ\nDerivation of the dynamical mean field theory equations\nThe study of the dynamics in such high-dimensional limit can be done via dynamical mean field\ntheory (DMFT) [Cug23]. The theoretical technology that we will employ is an evolution of the one\nfirst derived in [KU23a, KU23b] to study gradient flow and stochastic gradient descent on models\nthat are very much related to the Gaussian process we are discussing here [Urb23, MS23, KD24].\nWe remark that the formalism considered here can be used to study both the single index model\nand the pure noise case. To obtain the pure noise model, one can set ht = ˆφ = 0. Furthermore,\nthe extension to multi-index models can be also done easily on the same lines.\nThe analysis of Eqs. (A.10) can be done by recasting them into a path integral representation.\nWe follow the same procedure presented in [KU23a]. Eqs.(A.10) can be packed into a dynamical\npartition function\n1 = Zdyn =\nZ\nDaD˜a\nZ\nDW D ˆ\nW exp\nh\nA[a, ˜a, W , ˆ\nW ]\ni\n(J.1)\nwhere the path measure Da(t)D˜a(t)DW D ˆ\nW is implicitly defined. The action A reads\nA = i\nm\nX\nl=1\nZ\n˜al(t)\n\"\nddal(t)\ndt\n+ n ∂b\nRn\n∂al(t)\n#\ndt + i\nm\nX\nl=1\nZ\n⟨ˆwl(t), dwl(t)\ndt\n+ dνi(t)wi(t) + n ∂b\nRn\n∂wl(t)⟩dt .\n(J.2)\nEq. (J.2) can be rewritten by introducing Grassmann variables [ZJ21]. Call ˆa = (ta, θa) a supertime\ncoordinate, with θa a Grassmann variable. Define, with a slight abuse of notation\nwl(ˆa) = wl(ta) + iθa ˆwl\nal(ˆa) = al(ta) + iθa˜al(ta)\nl ≤m .\n(J.3)\n84\n\n\nEq. (J.2) can be written as\nA = d\n2\nm\nX\ni,j=1\nZ\nˆa,ˆb\nKij(ˆa,ˆb)⟨wi(ˆa), wj(ˆb)⟩+ d\n2\nm\nX\ni,j=1\nZ\nˆa,ˆb\n˜Kij(ˆa,ˆb)ai(ˆa)aj(ˆb) −n\nZ\nˆa\nb\nRn(θ(ˆa)) .\n(J.4)\nThe first two terms of the sum describe the kinetic terms of the dynamical equations of motion.\nThe last term instead contains the interaction between the weights of the network. The empirical\nrisk b\nRn depends on the training dataset. We are interested in understanding the behavior of the\ndynamics of gradient flow when we average over its realizations. Since the dynamical partition\nfunction is identically one we can average it directly over the dataset 3. In this way we have\n1 = Zdyn =\nZ\nDa(ˆa)DW (ˆa) exp\n\nd\n2\nm\nX\ni,j=1\nZ\nˆa,ˆb\nKij(ˆa,ˆb)⟨wi(ˆa), wj(ˆb)⟩\n+d\n2\nm\nX\nl,l′=1\nZ\nˆa,ˆb\n˜Kll′(ˆa,ˆb)al(ˆa)al′(ˆb)\n\nE\n\u0014\nexp\n\u0012\n−n\nZ\nˆa\nb\nRn(θ(ˆa))\n\u0013\u0015\n.\n(J.5)\nPerforming standard manipulation, see [KU23a], the dynamical partition function, for d →∞, can\nbe written as\nZdyn =\nZ\nD(a, ˜Q, R) exp\nh\nSdyn(a, ˜Q, R)\ni\n.\n(J.6)\nThe dynamical action Sdyn is given by\nSdyn = d\n2\nm\nX\nll′=1\nZ\nˆaˆb\nKll′(ˆa,ˆb)\n\u0010\n˜Qll′(ˆa,ˆb) + rl(ˆa)rl′(ˆb)\n\u0011\n+ d\n2 ln det( ˜Q) + αd\n2 ln det(I + Σ+)\n+ d\n2\nZ\nˆaˆb\nX\nll′\n˜Kll′(ˆa,ˆb)al(ˆa)al′(ˆb)\n(J.7)\nwhere α = n/d and\nΣ+(ˆa,ˆb) = τ 2 + ht(1) + 1\nm2\nm\nX\nl,l′=1\nal(ˆa)al′(ˆb)h\n\u0010\n˜Qll′(ˆa,ˆb) + rl(ˆa)rl′(ˆb)\n\u0011\n−1\nm\nm\nX\nl=1\nal(ˆa) ˆφ(rl(ˆa)) −1\nm\nm\nX\nl=1\nal(ˆb) ˆφ(rl(ˆb)) .\n(J.8)\nThe kinetic kernels K and ˜K are implicitly defined in such a way that they reproduce the time\nderivative part of the dynamical equations (A.10).\nIn the large d limit, fixing m and α, the path integral in Eq. (J.5) concentrates on its saddle\npoint. The corresponding equations are\n0 =\nm\nX\nγ=1\nZ\nˆc\nKlγ(ˆa, ˆc)Qγl′(ˆc,ˆb) + α\nmal(ˆa) ˆφ′(rl(ˆa))rl′(ˆb)\nZ\nˆd\n(I + Σ)−1(ˆa, ˆd)\n−α\nm2\nm\nX\nγ=1\nZ\nˆc\n(I + Σ)−1(ˆa, ˆc)al(ˆa)aγ(ˆc)h′(Qlγ(ˆa, ˆc))Qγl′(ˆc,ˆb) + δll′(ˆa,ˆb)\n(J.9)\n3We emphasize anyway that the average over the dataset is not mandatory: the resulting DMFT equations are\nself-averaging.\n85\n\n\nand\n0 =\nm\nX\nγ=1\nZ\nˆc\nKlγ(ˆa, ˆc)rγ(ˆc) + α\nmal(ˆa)φ′(rl(ˆa))\nZ\nˆd\n(I + Σ)−1(ˆa, ˆd)\n−α\nm2\nm\nX\nγ=1\nZ\nˆc\n(I + Σ)−1(ˆa, ˆc)al(ˆa)aγ(ˆc)h′(Qlγ(ˆc))rγ(ˆc)\n(J.10)\nwhere\nQll′(ˆa,ˆb) = ˜Qll′(ˆa,ˆb) −rl(ˆa)rl′(ˆb)\nΣ(ˆa,ˆb) = τ 2 + ht(1) + 1\nm2\nm\nX\nll′\nal(ˆa)al′(ˆb)h\n\u0010\nQll′(ˆa,ˆb)\n\u0011\n−1\nm\nm\nX\nl=1\nal(ˆa) ˆφ(rl(ˆa)) −1\nm\nm\nX\nl=1\nal(ˆb) ˆφ(rl(ˆb)) .\n(J.11)\nIf Lagrange multipliers are added to constrain the norm of the the weights of the first layer, one\nshould provide additional equations for them. Finally the equations for the dynamics of the second\nlayer weights are given by\nX\nγ=1\nZ\nˆc\n˜Klγ(ˆa, ˆc)aγ(ˆc) = −α\nZ\nˆc\n(I + Σ)−1 (ˆc, ˆa)\n\n1\nm2\nm\nX\nγ=1\naγ(ˆc)h [Qγl(ˆc, ˆa)] −1\nm ˆφ(rl(ˆa))\n\n\n(J.12)\nEqs. (J.9)-(J.12) contain all the information about the dynamics. In order to fully specify the\nbehavior of physical quantities such has the train and test error, it is useful to unfold the Grassmann\nstructure of Eqs. (J.9)-(J.12).\nJ.1\nUnfolding the Grassmann structure\nCausality of the dynamics implies that the following parametrization is the most general solution\nof the saddle point equations\nrα(ˆa) = rα(ta)\naα(ˆa) = aα(ta)\nQαβ(ˆa,ˆb) = Cαβ(ta, tb) + θaRβα(tb, ta) + θbRαβ(ta, tb)\n(I + Σ)−1(ˆa,ˆb) = CA(ta, tb) + θbRA(ta, tb) + θaRA(tb, ta) .\n(J.13)\nPlugging this parametrization into the saddle point equations we get that the correlators in Eqs. (J.13)\nsatisfy the following DMFT equations\ndaα(t)\ndt\n= −α\nm\nZ t\n0\nRA(t, s)\n\"\n1\nm\nm\nX\nl=1\nal(s)h [Clα(s, t)] −ˆφ(rα(t))\n#\nds\n−α\nm\nZ t\n0\nCA(t, s) 1\nm\nm\nX\nl=1\nal(s)h′[Clα(s, t)]Rαl(t, s)ds\n(J.14)\n86\n\n\ndrα(t)\ndt\n= −να(t)rα(t) + α\nmaα(t) ˆφ′(rα(t))\nZ t\n0\nRA(t, s)ds\n−aα(t)\nm\nm\nX\nγ=1\nZ t\n0\nMR\nαγ(t, s)aγ(s)rγ(s)ds\n(J.15)\n∂Cαβ(ta, tb)\n∂ta\n= −να(ta)Cαβ(ta, tb) + α\nmaα(ta) ˆφ′(rα(ta))rβ(tb)\nZ ta\n0\nRA(ta, s)ds\n−aα(ta)\nm\nm\nX\nγ=1\nZ ta\n0\nMR\nαγ(ta, s)aγ(s)Cγβ(s, tb)ds\n−aα(ta)\nm\nm\nX\nγ=1\nZ tb\n0\nMC\nαγ(ta, s)aγ(s)Rβγ(tb, s)ds\n(J.16)\n∂Rαβ(ta, tb)\n∂ta\n= −να(ta)Rαβ(ta, tb) + δαβ(ta −tb)\n−aα(ta)\nm\nm\nX\nγ=1\nZ ta\ntb\nMR\nαγ(ta, s)aγ(s)Rγβ(s, tb)ds .\n(J.17)\nNote that we used the notation according to which the prime sign denotes the derivatives of the\nfunctions with respect to their argument. The memory kernels MR and MC are defined by\nMR\nαγ(t, s) = α\nm\n\u0002\nRA(t, s)h′(Cαγ(t, s)) + CA(t, s)h′′(Cαγ(t, s))Rαγ(t, s)\n\u0003\nMC\nαγ(t, s) = α\nmCA(t, s)h′(Cαγ(t, s)) .\n(J.18)\nThe kernels in Eq. (J.18) depend on RA and CA that are defined in Eqs. (J.13). The corresponding\nequations are\nZ t\nt′ [δ(t −s) + ΣR(t, s)] RA(s, t′)ds = δ(t −t′)\nZ t\n0\n[δ(t −s) + ΣR(t, s)] CA(s, t′)ds +\nZ t′\n0\nΣC(t, s)RA(t′, s)ds = 0\n(J.19)\nwhere\nΣC(t, s) = τ 2 + ht(1) + 1\nm2\nX\nll′=1\nal(t)al′(s)h[Cll′(t, s)]\n−1\nm\nm\nX\nl=1\nal(t) ˆφ(rl(t)) −1\nm\nm\nX\nl=1\nal(s) ˆφ(rl(s))\nΣR(t, s) = 1\nm2\nm\nX\nll′=1\nal(t)al′(s)h′[Cll′(t, s)]Rll′(t, s) .\n(J.20)\n87\n\n\nThe Lagrange multipliers να(t) have to be fixed self-consistently to enforce that Cα,α(t, t) = 1 given\nthat wα ∈Sd−1. The corresponding equations are\nνα(ta) = α\nkm\nk\nX\nτ=1\naα(ta) ˆφ′(rτα(ta))rτα(ta)\nZ ta\n0\nRA(ta, s)ds\n−aα(ta)\nm\nm\nX\nγ=1\nZ ta\n0\nMR\nαγ(ta, s)aγ(s)Cγα(s, ta)ds\n−aα(ta)\nm\nm\nX\nγ=1\nZ ta\n0\nMC\nαγ(ta, s)aγ(s)Rγα(ta, s)ds\n(J.21)\nFinally we need to add a set of equation to propagate the diagonal elements of the correlation\nmatrix:\ndCαβ(ta, ta)\ndta\n= lim\nt′→ta\n\u0014∂Cαβ(ta, t′)\n∂ta\n+ ∂Cβα(ta, t′)\n∂ta\n\u0015\n.\n(J.22)\nThese dynamical equations can be integrated from a set of initial conditions that fully specify the\ninitial status of the neurons. We will consider a random initial condition for the weights of the first\nlayer so that\nrα(0) = 0\n∀α = 1, . . . , m\nCα̸=β(0, 0) = 0\n∀α ̸= β = 1, . . . , m\nCαα(0, 0) = 1\n∀α = 1, . . . , m\nRαβ(0, 0) = 0\n∀α, β = 1, . . . , m .\n(J.23)\nFinally, the initial conditions for the weights of the last layer aα(0) are completely arbitrary. The\nsolution of the DMFT equations gives access to the dynamics of the train and test error. The train\nerror as a function of time is defined as\netr(t) = lim\nd→∞\nb\nRn(t) .\n(J.24)\nA simple way to derive the expression of etr as a function of the solution of the DMFT equations\nin the d →∞limit is to consider a deformation of Eq. (J.5) which consists in replacing\nexp\n\u0012\n−n\nZ\nˆa\nb\nRn(ˆa)\n\u0013\n→exp\n\u0012\n−n\nZ\nˆa\nP(ˆa) b\nRn(ˆa)\n\u0013\n.\n(J.25)\nFor P(ˆa) = 1 we get back the original expression. The main idea of the derivation is to use P(ˆa)\nas a source field. In particular we have that\netr(t) = −\nZ\ndθa\nδ\nδP(ˆa) ln Zdyn[P]\n\f\f\f\f\nP=1\n.\n(J.26)\nNote that the deformed dynamical partition function Zdyn[P] does not equal 1 for generic P so\nthat the formula above makes perfectly sense. The deformation of the partition function produces\na deformation of Sdyn in Eq. (J.7) which consist in replacing\nαd\n2 ln det(I + Σ+) →αd\n2 ln det(I + Σ∗)\nΣ∗(ˆa,ˆb) = P(ˆa)Σ+(ˆa,ˆb) .\n(J.27)\n88\n\n\nPerforming explicitly the derivatives with respect to P one gets\netr(t) = 1\n2\nZ t\n0\n[RA(t, s)ΣC(t, s) + CA(t, s)ΣR(t, s)] ds .\n(J.28)\nThe computation of the test error can be done in analogous way\nets(t) = lim\nd→∞\n1\n2E\n\u0014\u0010\nynew) −y(s)\nnew\n\u00112\u0015\n= 1\n2\n\"\nτ 2 + 1\nkht(1) + 1\nm2\nm\nX\nll′\nh[Cll′(t, t)] −2 1\nm\nm\nX\nl\nˆφ(rl(t))\n#\n.\n(J.29)\nThe average in Eq. (J.29) is performed over the training set and an additional datapoint, not\npresented in the training set and having the same statistical structure.\nIn summary, the solution of the DMFT equations gives access to the train and test error\ndynamics in the large dimensional limit.\nThese equations can be integrated numerically very\nefficiently. Our goal is to understand their behavior for infinite number of neurons, m →∞at\nfixed sample complexity α. We will be mostly interested in two types of questions: first, given\na dataset that is pure noise, what are the sample complexities at which the network is able to\ninterpolate the dataset. Second: given a dataset built out of a single index process what is the\ndynamics of the test and train error.\n89\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21269v1.pdf",
    "total_pages": 89,
    "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
    "authors": [
      "Andrea Montanari",
      "Pierfrancesco Urbani"
    ],
    "abstract": "The inductive bias and generalization properties of large machine learning\nmodels are -- to a substantial extent -- a byproduct of the optimization\nalgorithm used for training. Among others, the scale of the random\ninitialization, the learning rate, and early stopping all have crucial impact\non the quality of the model learnt by stochastic gradient descent or related\nalgorithms. In order to understand these phenomena, we study the training\ndynamics of large two-layer neural networks. We use a well-established\ntechnique from non-equilibrium statistical physics (dynamical mean field\ntheory) to obtain an asymptotic high-dimensional characterization of this\ndynamics. This characterization applies to a Gaussian approximation of the\nhidden neurons non-linearity, and empirically captures well the behavior of\nactual neural network models.\n  Our analysis uncovers several interesting new phenomena in the training\ndynamics: $(i)$ The emergence of a slow time scale associated with the growth\nin Gaussian/Rademacher complexity; $(ii)$ As a consequence, algorithmic\ninductive bias towards small complexity, but only if the initialization has\nsmall enough complexity; $(iii)$ A separation of time scales between feature\nlearning and overfitting; $(iv)$ A non-monotone behavior of the test error and,\ncorrespondingly, a `feature unlearning' phase at large times.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}