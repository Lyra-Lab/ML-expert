{
  "id": "arxiv_2502.20853v1",
  "text": "Oscillation-Reduced MXFP4 Training for Vision Transformers\nYuxiang Chen 1 2 Haocheng Xi 3 Jun Zhu 1 Jianfei Chen 1\nAbstract\nPre-training Transformers in FP4 precision is be-\ncoming a promising approach to gain substan-\ntial speedup, but it comes with a considerable\nloss of accuracy. Microscaling (MX) data for-\nmat provides a fine-grained per-group quantiza-\ntion method to improve the representation ability\nof the FP4 format and is supported by the next-\ngeneration Blackwell GPU architecture. However,\ntraining with MXFP4 data format still results in\nsignificant degradation and there is a lack of sys-\ntematic research on the reason.\nIn this work, we propose a novel training method\nTetraJet for a more accurate FP4 training. We\ncomprehensively evaluate all of the quantizers\ninvolved in the training, and identify the weight\noscillation problem in the forward pass as the\nmain source of the degradation in MXFP4 train-\ning. Therefore, we introduce two novel methods,\nEMA Quantizer (Q-EMA) and Adaptive Ramp-\ning Optimizer (Q-Ramping), to resolve the oscil-\nlation problem. Extensive experiments on Vision\nTransformers demonstrate that TetraJet consis-\ntently outperforms the existing 4-bit training meth-\nods, and Q-EMA & Q-Ramping can provide addi-\ntional enhancement by effectively reducing oscil-\nlation. We decreased the accuracy degradation by\nmore than 50% compared to the baseline, and can\neven achieve competitive performance compared\nto full precision training. The codes are avail-\nable at https://github.com/thu-ml/\nTetraJet-MXFP4Training\n1. Introduction\nLow-precision training has emerged as a promising tech-\nnique for accelerating the training process of large-scale\nneural networks. By quantizing tensors in both the for-\n1Department of Computer Science and Technology, Tsinghua\nUniversity 2Zhili College, Tsinghua University 3University of\nCalifornia, Berkeley. Correspondence to: Jianfei Chen <jian-\nfeic@tsinghua.edu.cn>.\nPreprint.\nward and backward passes to lower-precision formats, low-\nprecision training leverages specialized compute units in\nmodern hardware to enhance computational efficiency sig-\nnificantly. While BF16 and FP16 precision remain the most\nwidely used formats for deep learning training (Narang et al.,\n2017; Kalamkar et al., 2019), FP8 training (Sun et al., 2019;\nMicikevicius et al., 2022; NVIDIA, 2024c; Xi et al., 2024a)\nis becoming increasingly mature in these years, with suc-\ncessful application in training state-of-the-art large language\nmodels (Liu et al., 2024).\nThere is a growing interest in pushing the training precision\ndown to 4-bit. While earlier works attempt to train the net-\nwork with FP4 (Sun et al., 2020), logarithm format (Chmiel\net al., 2021), and INT4 (Xi et al., 2023), these works have\nrather large accuracy degradation (e.g., 1-2%) even on sim-\nple tasks such as ResNet training, and are not practically\nfavorable. Recently, a Microscaling (MX) data format has\nbeen proposed for accurate low-precision training and in-\nference (Rouhani et al., 2023b;a). MX applies fine-grained\nper-group quantization, where each small group of 32 ele-\nments shares a scaling factor. This fine-grained quantization\nscheme significantly mitigates the impact of outliers, and\nthus reduces quantization error. Particularly, the MXFP4\nformat utilizes an E2M1 (Exponent / Mantissa) FP4 with\nan E8M0 scaling factor. MXFP4 is supported on the latest\nNvidia Blackwell architecture and is 2 times faster than\nFP8/MXFP6 and 4 times faster than FP16/BF16 (NVIDIA,\n2024a;b) when doing matrix multiplications. However, the\nlow-precision training method proposed in the original Mi-\ncroscaling paper uses MXFP6 activation/gradient, which is\nas slow as FP8 training. The fast pure MXFP4 training still\nhas major accuracy degradation as tested in our experiments,\nwhich makes it infeasible to use in practice.\nIn this work, we propose TetraJet, a novel training method\nfor transformer (Vaswani, 2017) with MXFP4 computation\nin both forward and backward pass. All weight/activation/-\ngradient tensors in linear layers are quantized to MXFP4\nto fully unlock the acceleration potential of the hardware.\nWe propose several techniques to improve the accuracy of\nMXFP4 training. First, we propose a truncation-free scaling\nmethod for quantizing full-precision values to MXFP4 to\navoid information loss in truncation. We further propose\na double quantization method to deal with the non-square\nquantization group of MXFP4. With these techniques, we\n1\narXiv:2502.20853v1  [cs.LG]  28 Feb 2025\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nprove that TetraJet can estimate the gradient unbiasedly.\nWe then conduct a comprehensive evaluation of the impact\nof individual quantizers on the final model performance, and\nfind that activation and weight quantizers in the forward pass\ncontribute the most to accuracy degradation, due to a weight\noscillation problem: the master weight fluctuates around the\nquantization boundary, causing the model to be quantized\ninto different values across iterations, which consequentially\nbrings significant instability in the optimization process. We\npropose two methods to alleviate the oscillation problem:\nthe EMA quantizer (Q-EMA) conducts rounding based on\nthe moving average of historical weights rather than only\ndepending on the current weight matrix; and the Adapting\nRamping optimizer (Q-Ramping) adaptively identifies and\nreduces the update frequency of oscillating weights.\nExtensive experiments on Vision Transformers prove that\nTetraJet consistently outperforms Microscaling’s original\nmethod (Rouhani et al., 2023b), and Q-EMA & Q-Ramping\ncan provide additional improvement through oscillation re-\nduction. We decreased the accuracy degradation by more\nthan 50% compared to the baseline, and even achieve com-\npetitive performance compared to full-precision training.\n2. Related Work\nLow-Precision Training\nLow-precision training has be-\ncome a prominent technique in modern deep learning to\nspeed up the training process.\nFP16 and BF16 (half-\nprecision) training (Narang et al., 2017; Kalamkar et al.,\n2019) is currently the most common low-precision method.\nFP8 and INT8 training (Sun et al., 2019; Zhu et al., 2020;\nMicikevicius et al., 2022; Wortsman et al., 2023; NVIDIA,\n2024c; Peng et al., 2023; Xi et al., 2024b;a; Liu et al., 2024)\nfurther improves efficiency and uses more fine-grained per-\ntensor / per-row / per-block quantization. When it comes\ndown to 4-bit training (Sun et al., 2020; Chmiel et al., 2021;\nXi et al., 2023), more techniques are being applied (e.g.\nHadamard transformation) to compromise the degradation\ncaused by the low representation ability. Still, their accuracy\ndegradation is not negligible.\nFor a more fine-grained quantization, the Microscaling\n(MX) format (Rouhani et al., 2023a;b) in the Blackwell\narchitecture (NVIDIA, 2024a) offers a 1 × 32 per-group\nquantization and could potentially double the speed com-\npared to FP8 training. Rouhani et al. (2023b) also propose\na low-precision training method with computation flow in\nMX formats in 4, 6, and 8 bits. In this paper, we refer to\nthe 4-bit MX format as MXFP4, and refer to their train-\ning method as Microscaling. We propose a better training\nmethod TetraJet with accuracy improvement compared to\nMicroscaling.\nOscillation Problem\nIn low-precision training, weight\noscillation has been proven to be a serious problem that\naffects optimization. Nagel et al. (2022) revealed that the os-\ncillation of weight quantization does harm to Quantization-\nAware Training (QAT) of CNNs. Besides, Liu et al. (2023)\nproved that oscillation was a key factor causing the degrada-\ntion of accuracy in QAT of Vision Transformers. However,\nthey were both based on QAT, that is, they fine-tunes a\nlow-precision model based on a pre-trained full-precision\nnetwork rather than pre-training from scratch. They both\nutilized pre-tensor Learned Step Size Quantization (LSQ)\nto train the models. There is a lack of research on oscil-\nlation problems about pre-training and more fine-grained\nquantization methods (e.g., MX Format).\nTo reduce weight oscillation, Liu et al. (2023) proposed\nseveral methods, but the application is restricted to LSQ\nor QAT, while Nagel et al. (2022) proposed methods that\ncan be generalized to reduce oscillation in MXFP4 pre-\ntraining: The method “Dampen” tried to encourage latent\nweights to be closer to the quantized value to avoid fluctu-\nating around the quantization boundary, by adding a regu-\nlation term Ldampen = ∥W −Q(W)∥2\nF in the loss func-\ntion; The method “Freeze” tracks the oscillation frequency\nf for each weight element, and freezes those frequently\noscillating weights (f > fth) to a running average value.\nThe frozen weights would never be updated again in the\nwhole training process, which may harm the optimization\nin pre-training. In this work, we propose two novel meth-\nods Q-EMA & Q-Ramping to better reduce oscillation in\nMXFP4 pre-training.\n3. Our TetraJet Training Method\nIn this section, we review and identify several drawbacks\nof the existing low-precision training method Microscal-\ning (Rouhani et al., 2023b), and propose a more accurate\ntraining method TetraJet. The effectiveness of our method\nis shown in Section 7.\n3.1. Preliminary\nMXFP4 Format\nFloating points have three components:\nsign-bit, exponent-bits, and mantissa bits. If a format has x\nexponent bits and y mantissa bits, we usually denote it as\nExMy. We use Qp, Qn to represent the max positive value\nand the min negative value the format can represent. For\nE2M1, Qp = 6, Qn = −6.\nThe MXFP4 (Microscaling Floating-Point 4-bit) data for-\nmat (Rouhani et al., 2023a) follows a per-group quantization\nscheme where a group of N = 32 elements shares a com-\nmon 8-bit exponential scaling factor s. Each element Xi\nin the group is represented by Pi in E2M1 format. The\nreconstruction of a floating-point value Xi from its MXFP4\n2\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nrepresentation follows the formula:\nXi = Pi × 2s, i = 1, 2, . . . , 32\nQuantization\nTo quantize a matrix to MXFP4, we need\nto split it into blocks of size 1 × 32 (or 32 × 1), and then\nquantize each block to MXFP4. To quantize a block of 32\nfull-precision values {Xi}32\ni=1 to MXFP4, we first determine\nthe E8M0 scale factor S = 2s with |s| ≤127. Each value\nXi is then mapped to a 4-bit FP4 representation Pi, such\nthat:\nXi = roundFP4\n\u0012Xi\nS\n\u0013\n,\nXi ≈Pi · S.\n(1)\nThe quantized representation is stored as\n\u0000{Pi}32\ni=1, S\n\u0001\n,\nwhere S is an 8-bit exponent, and Pi is an FP4 value.\n3.2. Quantization with Truncation-Free Scaling\nComputation of Scaling Factor\nMicroscaling computes\nthe scale factor as follows\ns = ⌊log2 M⌋−Emax, S = 2s,\n(2)\nwhere M = max1≤i≤32 |Xi| is the largest absolute value\nof the block, Emax represents the largest exponent in FP4\nformat1. A drawback of the approach is that the scaled value\nXi/S may fall outside the range [Qn, Qp], and exceeding\nvalues will be truncated. For instance, if M = 31, the\nscaling factor will be S = 2s = 24−2 = 4. Since M/S =\n31/4 = 7.75 exceeds the maximum representable value\nQp = 6, the value M will be truncated to 6. Intuitively,\nlarge values carry more information, and such truncation\nwill be harmful to retaining the precision of the network.\nTetraJet equips a truncation-free scaling method:\ns =\n&\nlog2\n2f\nM\nQp −Qn\n'\n, S = 2s,\nwhere f\nM equals to M in most cases except when M = 0,\nwe set f\nM to a small number ϵ = 10−8 to avoid numerical\nissues. Compared to Eq. (2), we replace floor ⌊·⌋with ceil-\ning ⌈·⌉to avoid truncation, and replace the numerical range\nfrom [−2Emax, +2Emax] to a more accurate range [Qn, Qp].\nIn this way, Qn ≤M/S ≤Qp always holds. For example,\nwhen M = 31, the scaling factor will be S = 23 = 8, so\nM/S = 3.875 still lies in the representation range of FP4.\nDeterministic & Stochastic Rounding of FP4 format\nNow we discuss the roundFP4(·) operation in Eq. (1). With\nour scaling, all the values Xi/S are in the range [Qn, Qp].\nTherefore, we can always find two consecutive FP4 value\nq1, q2(q1 < q2) satisfying q1 ≤Xi/S ≤q2 for every Xi.\n1for E2M1, Emax = 22−1 = 2\nA direct way of rounding Xi/S is to select the nearest FP4\nvalue between q1, q2, which we call deterministic rounding\nor round to nearest. Here we denote it as\nroundD(Xi/S) =\n\u001a\nq1, |Xi/S −q1| < |Xi/S −q2|\nq2, otherwise\nMicroscaling always applies deterministic quantization to\nminimize the quantization error. However, we find it subop-\ntimal to apply deterministic quantization to gradients, since\nthe gradient will no longer be unbiased. To this end, we\napply stochastic rounding (Courbariaux et al., 2015) to gra-\ndients to maintain an unbiased gradient. Stochastic rounding\ngenerates random variable ξ ∼Uniform(−q2−q1\n2\n, q2−q1\n2\n)\nfor each value Xi independently, and computes\nroundS(Xi/S) =\n(\nq1, Xi/S + ξ < q1+q2\n2\nq2, otherwise\nStochastic rounding is unbiased: E[roundS(Xi/S)] =\nXi/S. We show the superiority of stochastic rounding in\nthe ablation study in Sec. 7.3,\n3.3. TetraJet Linear Layer\nWhen training the transformer, linear layers usually take\nmost of the computation. Following previous works on\nlow-precision training (Xi et al., 2023), we mainly focus on\naccelerating the linear layer with MXFP4, whose forward\nand backward pass are defined as:\nY = XW⊤,\n∇XL = (∇YL)W,\n∇WL = (∇YL)⊤X,\nwhere X\n∈\nRN×D, W\n∈\nRC×D, Y\n∈\nRN×C, L\nis a loss function, and ∇XL/∇YL/∇WL are the in-\nput/output/weight gradient matrices with the same size of\nX, Y, W.\nTo accelerate training, we need to compute all three matrix\nmultiplications (MMs) in MXFP4. To achieve this, we\nneed to quantize the six input matrices of the three MMs to\nMXFP4, which can be formulated as:\nY = Q(1)\nD (X) × Q(2)\nD (W⊤)\n(3)\n∇XL = Q(3)\nS (∇YL)\n× Q(4)\nS\n\u0012\nQ(2)\nD (W⊤)\n⊤\u0013\n(4)\n∇WL = Q(5)\nS\n\u0000(∇YL)⊤\u0001\n× Q(6)\nS\n\u0010\nQ(1)\nD (X)\n\u0011\n(5)\nwhere QD/QS refers to the deterministic/stochastic round-\ning quantizer. We explain the design of TetraJet linear layer\nas follows.\nBlock Format\nAs a fine-grained format, doing MM with\nMXFP4 is more subtle than other coarser-grained formats\n3\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nMatmul\nQuantize\nQuantize\nModel Weights\nMatmul\nMatmul\nQuantize\nQuantize\nMatmul\nQuantize\nQuantize\nModel Weights\nMatmul\nMatmul\nQuantize\nQuantize\nQuantize\nQuantize\nQuantize\nQuantize\nMicroscaling's\nScaling Factor\nTetraJet's\nTruncation-Free\nScaling Factor\nDeterministic\nRounding\nStochastic\nRounding\nMXFP4\nTensor\n(a) Microscaling's Linear Layer\n(b) TetraJet's Linear Layer\nFigure 1: Visualization of MXFP4 Linear Layer.\nsuch as per-tensor quantization. For hardware-accelerated\nMM to be possible, MXFP4 format requires quantization\ngroup shape to be 1 × 32 for the first matrix and 32 × 1 for\nthe second matrix. Therefore, quantizer (1)(3)(5) should\nuse 1 × 32 group shape, and quantizer (2)(4)(6) should use\n32 × 1 group shape. This means that weight W, activation\nX, and gradient ∇YL should be quantized along different\naxes in different quantizers. For example, the quantization\nblock size of activation X should be 1 token × 32 channels\nin forward and 32 tokens × 1 channel in backward.\nDouble Quantization\nWe propose a double quantiza-\ntion strategy to satisfy MXFP4’s block format requirement.\nSpecifically, Q(1)\nD (X) is a quantized activation with 1 × 32\ngroup size, which is used in the forward pass. We quantize\nthe already quantized Q(1)\nD (X) again with a different 32 × 1\ngroup size to compute the gradient in Eq. (5). By doing\nso, we ensure the activation is quantized with the required\ngroup size for both forward and backward pass. Similarly,\nthe weight is also doubly quantized.\nIn contrast, Microscaling takes a different approach:\n∇XL = Q(3)\nD (∇YL) × Q(4)\nD (W)\n(6)\n∇WL = Q(5)\nD\n\u0000(∇YL)⊤\u0001\n× Q(6)\nD (X)\n(7)\nwhere the activation used in the backward pass is deter-\nministically quantized from the full-precision X rather than\nQ(1)\nD (X), which is biased as we will discuss.\n3.4. Gradient Bias\nWe first derive the correct gradient formula with Straight\nThrough Estimator (STE) (Bengio et al., 2013): Given the\nforward pass in Eq. (3), the correct gradient should be\n∇XL\nSTE\n≈∇Q(1)\nD (X)L = (∇YL) × Q(2)\nD (W⊤)\n⊤\n(8)\n∇WL\nSTE\n≈∇Q(2)\nD (W)L = (∇YL)⊤× Q(1)\nD (X).\n(9)\nTable 1: Impact analysis on MXFP4 quantizers. We report\nthe top-1 Acc.% after 90-epoch pre-training. Qi means we\nonly activate the i-th quantizer Q(i).\nDeiT-T\nDeiT-S\nFull Precision\n63.73\n73.33\nQ1\n61.50\n71.66\nQ2\n62.77\n72.45\nQ3\n63.46\n72.97\nQ4\n63.37\n72.79\nQ5\n63.81\n73.25\nQ6\n63.78\n73.13\nAll Quantizers\n59.75\n71.03\nNote that microscaling’s gradient Eq. (6,7) does not equal\nto the correct gradient Eq. (8,9). Particularly, Q(4)\nD (W) ̸=\nQ(2)\nD (W⊤)\n⊤. Microscaling is actually computing the gra-\ndient for another network with the forward pass Y =\nQ32×1(X)Q1×32(W⊤), where both operands are quan-\ntized in the wrong direction.\nIn contrast, TetraJet gives an unbiased estimation of\nEq. (8,9).\nTake ∇XL in Eq. (4) as an example, since\nQ(3), Q(4) are stochastic and truncation-free, the expecta-\ntion of our gradient is\nE\nh\nQ(3)\nS (∇YL) × Q(4)\nS\n\u0010\nQ(2)\nD (W⊤)⊤\u0011i\n= E\nh\nQ(3)\nS (∇YL)\ni\n× E\nh\nQ(4)\nS\n\u0010\nQ(2)\nD (W⊤)⊤\u0011i\n= ∇YL × Q(2)\nD (W⊤)⊤\nwhich is right side of Eq. (8). Similarly, the estimation\nin Eq. (5) for ∇WL is also unbiased. Given that each\nlinear layer is unbiased, the final gradient calculated with\nbackpropagation is unbiased, which ensures the convergence\nof SGD, as discussed by (Chen et al., 2020).\n3.5. Impact Analysis of Six Quantizers\nBefore making any attempts to improve the training, it is\nnecessary to understand which among the 6 quantizers in\n4\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\n20\n40\n60\n80\nEpoch\n10\n4\n10\n3\n10\n2\nRate of Change for Weight\nRate of Change for Weight \nAt Different Stage of Training\nFull-Precision Training\nMXFP4 Training\n20\n40\n60\n80\nEpoch\n10\n3\n10\n2\n10\n1\nRate of Change for Activation\nRate of Change for Activation \nAt Different Stage of Training\nFull-Precision Training\nMXFP4 Training\nFigure 2: Rate of change for weight and activation at differ-\nent stages of 90-epoch DeiT-Tiny pre-training. We calculate\nthe average rate for all quantized weights and select a trans-\nformer block to test output activation given fixed input.\nEq. (3,4,5) is the bottleneck. We test the impact of quan-\ntizers by activating them separately: for the i-th test, we\nonly activate Q(i) while leaving all other matrices in full\nprecision, train the model from scratch, and compute vali-\ndation accuracy. As shown in Tab. 1, the activation/weight\nquantizers Q(1)/Q(2) in the forward pass lead to most accu-\nracy degradation. For example, MXFP4 training on DeiT-T\nhas a 3.98% accuracy loss, while only quantizing the activa-\ntion/weight in the forward pass accounts for 2.23% / 0.96%,\nrespectively. We reveal in the next section this is due to the\ninstability of low-precision training.\n4. Oscillation Phenomenon\n4.1. Instability of MXFP4 Training\nDuring the final stage of training, the learning-rate (LR) typ-\nically approaches zero, so the model can stop exploration\nand quickly descend to a local minimum. However, we\nfind that MXFP4 training cannot converge even with a suf-\nficiently small learning rate due to the oscillation between\nquantization points. To explain this phenomenon, we define\nrate of change for a tensor X as\nr(X) = 1\nT0\nT0\nX\nt=1\n\r\rXt −Xt−1\r\r\nF\n\r\rXt−1\r\r\nF\n,\nwhere t refers to training step, and step 0 ∼T0 refers to\na short training interval. During pre-training, we can test\nthe rate of change for the master weight W, the quantized\nweight matrix Q(2)(W⊤)⊤, and activation Y at different\nstages.\nAs shown in Fig. 2, for full-precision models, the rate of\nchange can gradually decrease to near zero, while for quan-\ntized models the rate of change would stay high in the final\nof training, indicating that there are still large changes inside\nthe models.\n1.00\n0.75\n0.50\nQuantized Weights\n0\n100\n200\n300\n400\n500\nIteration\n0.755\n0.750\n0.745\nLatent Weights\nFigure 3: Trajectory of some oscillation elements in DeiT-\nTiny during the last epoch of training. The top plot shows\nthe change of quantized FP4 value, and the bottom plot\nshows the oscillating latent weight around the quantization\ndecision threshold thrd = −0.75.\nWe find that the weight oscillation is the source of this\nproblem. To be clear, we refer to w/S as latent weight,\nwhere S is the quantization scale factor of weight element\nw. As illustrated in the top plot in Fig. 4, a large amount of\nlatent weights lies around the quantization thresholds (the\nmidpoints of two quantized values) at the end of the training\nprocess. For these elements, little perturbation on their\ncorresponding master weights will change the quantized\nvalues, which results in a giant jump from one quantized\nvalue to another. This makes the rate of change of the\nquantized weight matrix much higher than its corresponding\nmaster weight, and meanwhile contributes to the instability\nof activation, which aligns with our finding.\nWe tracked several oscillating weight elements during the\nfinal epoch of training for a better understanding of this\noscillation phenomenon. As shown in Fig. 3, these latent\nweights are changing with small steps around the quanti-\nzation threshold thrd = −0.75, which is the midpoint of\ntwo FP4 values q1 = −1, q2 = −0.5. When the latent\nweight crosses thrd = −0.75 caused by a small update, the\nquantized weight would shift from q1 to q2 (or from q2 to\nq1). Frequently crossing thrd causes the frequent flipping\nbetween q1 and q2. Therefore, a direct characterization of\noscillating weight is that, the oscillating weight elements\nwill have their latent value stay closely around the quantiza-\ntion threshold and frequently cross the threshold.\n4.2. Quantization Confidence of Weight Distribution\nTo quantitatively assess the severity of the oscillation prob-\nlem, we define quantization confidence for each weight\nelement w, which measures the normalized distance to the\nnearest quantization threshold:\nQuantConf(w) := mini |w −thrdi|\nMaxDist(wFP4) ,\n5\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nMore Conﬁdent\nLess Conﬁdent\nFigure 4: The change of latent weight and quantization con-\nfidence during 90-epoch pre-training of DeiT-Tiny. The top\nplot shows the distribution of latent weight, and the bottom\nplot shows the distribution of quantization confidence.\nwhere wFP4 denotes the quantized FP4 value of w,\n{thrdi} denotes all the quantization thresholds, and\nMaxDist(wFP4) denotes the maximum possible distance\nif quantized to wFP4. It is ensured that QuantConf(w) ∈\n[0, 1].\nWe can also define quantization confidence for a matrix W\nas the average QuantConf(w) of all the element w in W.\nThe rationale behind this metric is that the closer a latent\nweight is to a quantization decision threshold, the more\nlikely it is to oscillate, making it harder for the weight to\nconverge to a stable FP4 value.\nAs shown in the bottom plot of Fig. 4, we observe a grad-\nual decline in quantization confidence throughout training.\nThis trend indicates an increasing prevalence of oscillation\nas training progresses. Consequently, effective solutions\nto mitigate oscillation should be dynamic, adapting to the\nspecific conditions of each stage of the training process.\n5. EMA Quantizer\nWe firstly propose an EMA Quantizer (Q-EMA) to solve\nthe oscillation phenomenon. Since the weight will oscillate\nbetween the two possible choices randomly even with small\nperturbations, we hope to find a better way to choose from\nthese two possible values after quantization.\nWe find that the Exponential Moving Average (EMA) can\nbe used to alleviate the oscillation problem. EMA on weight\nis determined as:\nWt\nEMA = βWt−1\nEMA + (1 −β)Wt,\n(10)\nwhere Wt is the BF16 weight. A Typical choice of β is\n0.998. Therefore, even when weight makes a very large step,\nWEMA only moves slightly. When the weight oscillates\nbetween two quantized values, as EMA weight is always left\nbehind the actual optimization process and is updated slowly,\nEMA weight is less likely to be affected by oscillations.\nConsequently, this makes the optimization process much\nmore stable.\nOur EMA quantizer first maintains an EMA weight through-\nout the training process. When doing quantization to each\nweight element w with scale factor S and its EMA value\nwEMA, we first use the latent weight w/S to propose two\ncandidate quantized values wq1 and wq2, as they are the\ntwo values that give the smallest MSE. We then use the\nEMA weight to check which is closer to wEMA, and use\nthis as the quantized value. This algorithm is formalized as\nAlgorithm 1 in Appendix C.\n6. Adaptive Ramping Optimizer\nBesides smoothing the weight quantization with EMA quan-\ntizer, another effective approach to reducing oscillations\nis to manually decrease the update frequency of oscillat-\ning weights. Building on this idea, we propose Adaptive\nRamping Optimizer (Q-Ramping), which directly locates\nthe frequently oscillating weights according to their updat-\ning trajectory, and then adaptively decrease their updating\nfrequency by using a higher gradient accumulation step for\nthese oscillating weights to reduce the oscillation frequency.\n6.1. Identifying Oscillating Weights\nThe first thing is to locate the oscillating weights and quan-\ntify their degree of oscillation. To achieve this, we would\nrecord information about the weight update trajectory for\neach element. During a training stage with T0 steps, we sum\nup updating distance for each master weight element w and\nits quantized weight wQ:\ndistW =\nT0\nX\nt=1\n|wt −wt−1|,\ndistQ =\nT0\nX\nt=1\n|wt\nQ −wt−1\nQ |,\nAnd then, we define oscillation ratio Rw for each weight\nelement as\nRw := distQ/distW ,\nrepresenting the degree of oscillation.\nDuring training, if a weight element w doesn’t fall into the\noscillation process, the master weight w and the quantized\nweight wQ would move with a similar trajectory. In this\nsituation, distQ ≈distW , so Rw would not be too large.\nIn contrast, for oscillating weight elements, the quantized\nweight would switch frequently between two discrete quan-\ntization values q1 and q2. Each switch from q1 to q2 (or\nfrom q2 to q1) will increase distQ by |q1 −q2|, making it\nrelatively large. Meanwhile, the master weight w would be\n6\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nTable 2: Results on the 90-epoch pretraining of Vision Transformers. We report the Top-1 Accuracy% on validation dataset.\nPRE-TRAINING METHODS\nBIT WIDTH\nQUANTIZATION\nDEIT-T\nDEIT-S\nDEIT-B\nSWIN-T\nSWIN-S\nFULL PRECISION\nA16W16G16\n-\n63.73\n73.33\n75.57\n78.35\n80.44\nINT4\nA4W4G4\nPER-TENSOR\n40.14\n60.07\n68.13\n74.22\n75.74\nMICROSCALING (BASELINE)\nA4W4G4\nPER-GROUP\n58.56\n70.10\n74.54\n76.87\n79.45\nTETRAJET (OURS)\nA4W4G4\nPER-GROUP\n59.75\n71.03\n74.91\n77.12\n79.51\nTETRAJET + Q-EMA(OURS)\nA4W4G4\nPER-GROUP\n60.00\n72.25\n77.32\n77.30\n79.74\nTETRAJET + Q-RAMPING(OURS)\nA4W4G4\nPER-GROUP\n60.31\n71.32\n75.62\n77.33\n79.67\noscillating around the quantization threshold, and the step-\nsize would be ≪|q1 −q2|, so in this situation, we would\nget distW ≪distQ, and Rw will be quite large.\nTherefore, the larger Rw, the more frequently and severely\nthe weight element w oscillates, which means that we should\nput more effort into suppressing the oscillation of w.\n6.2. Suppressing Weight Oscillation Adaptively\nBased on periodically detecting and quantifying weight\noscillation, we propose Adaptive Ramping Optimizer (Q-\nRamping) to alleviate the oscillation problem of these\nweights. We adaptively decrease the updating frequency\nof these oscillating weights, by setting larger batch-size for\nthem. We also expand their corresponding learning-rate pro-\nportional to their batch-size. The adapted batch-size would\nbe an integer multiple of the global batch-size, and we would\naccumulate the gradient for each oscillating weight accord-\ning to its own batch-size. This algorithm can be formalized\nas Algorithm 2 in Appendix C.\nBy applying Q-Ramping, the update frequency is reduced\nfor oscillating weights, so that their oscillation frequency\nis also reduced. Additionally, through larger batch-size\nand larger learning-rate, the oscillating weights near the\nquantization thresholds can be updated to a place further\naway from the quantization threshold. Therefore, the weight\ndistribution will have a higher quantization confidence, and\nthe oscillation phenomenon can be alleviated.\n7. Experiments\n7.1. Vision Transformer Pre-Training\nWe evaluate our TetraJet training method and oscillation\nreduction method Q-EMA & Q-Ramping on Vision Trans-\nformers pre-training. During training, we quantize the for-\nward and backward process of all the linear layers in the At-\ntention module and the MLP module of transformer blocks.\nWe do pre-training for DeiT-Tiny, DeiT-Small, and DeiT-\nBase (Touvron et al., 2020) using Facebook’s training\nrecipe2, and pre-train Swin-Tiny and Swin-Small (Liu et al.,\n2021) based on the official implementation3. All the models\nare trained for 90 epochs on ImageNet1K (Russakovsky\net al., 2015) with default training recipes. For Q-EMA & Q-\nRamping, we show the insensitivity to their hyperparameter\nchoice in Appendix C.3.\nWe compared our MXFP4 training method, TetraJet, with\nfull-precision training, 4-bit per-tensor quantization method\nINT4 (Xi et al., 2023), and original Microscaling’s MXFP4\ntraining method (Rouhani et al., 2023b). The detailed results\nare listed in Tab. 2.\nAs a result, our TetraJet can consistently outperform the orig-\ninal method Microscaling, and we can further improve the\nperformance of MXFP4 training by overcoming oscillation\nproblems in forward pass with Q-EMA / Q-Ramping.\n7.2. Quantitative Analysis on Oscillation Reduction\nTo validate our improvements in mitigating oscillation, we\nanalyzed different statistics to show how our methods work\nin oscillation reduction in real training.\nImprovement of Training Stability\nAs described in\nSec. 4.1, the rate of change for weights and activation can-\nnot converge to zero in MXFP4, which reflects the model\ncannot converge stably. In Tab. 3, we can see our methods\ncan effectively reduce the instability of both the weight and\nactivation in forward.\nImprovement of Quantization Confidence\nAs described\nin Sec. 4.2, weight confidence indicates the risk of weight os-\ncillation. If the confidence is lower at the end of the training,\nmore weights are still oscillating around the quantization\nthreshold, and it is harder for these parameters to converge\nto decisive values.\nIn Fig. 5, we can see the unique function of Q-Ramping in\nimproving quantization confidence. It successfully reduced\nthe weights that are prone to oscillate (those with low confi-\n2https://github.com/facebookresearch/deit\n3https://github.com/microsoft/Swin-Transformer\n7\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\n0.00\n0.25\n0.50\n0.75\n1.00\nQuantConf(w)\n7000\n8000\n9000\n10000\nblocks.6.mlp.fc2 in deit_small\nTetraJet\nTetraJet + Q-Ramping\n0.00\n0.25\n0.50\n0.75\n1.00\nQuantConf(w)\n26000\n28000\n30000\n32000\n34000\n36000\nblocks.6.mlp.fc2 in deit_base\nTetraJet\nTetraJet + Q-Ramping\nFigure 5: Q-Ramping’s unique effect on improving the\ndistribution of quantization confidence of the final model.\n10\n20\n30\n40\n50\n60\n70\n80\n90\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nOscillating Weights% (Rw > 16)\nTetraJet (Ours)\nTetraJet + Dampen\nTetraJet + Q-Ramping (Ours)\nTetraJet + Q-EMA (Ours)\nFigure 6: Q-EMA & Q-Ramping’s effect on oscillating\nweights reduction during the whole training process. We\npresent the 90-epoch pre-training of DeiT-T.\nTable 3: Effect of Q-EMA & Q-Ramping on stabilizing\nweight and activation at the end of DeiT-T training. r(·)\nrefers to the rate of change for tensors, WQ is the quantized\nweights, and Y is the output of 9th transformer block given\nfixed input.\nr(WQ)↓\nr(Y)↓\nTetraJet\n0.0045\n0.0401\nTetraJet + Dampen\n0.0044\n0.0394\nTetraJet + Q-EMA (Ours)\n0.0018\n0.0251\nTetraJet + Q-Ramping (Ours)\n0.0028\n0.0318\nTable 4: Comparison of our oscillation reduction methods\nwith other methods for DeiT MXFP4-Pretraining on Ima-\ngeNet Classification. We report the top-1 Acc.% of the final\nmodel.\nDeiT-T\nDeiT-S\nTetraJet\n59.75\n71.03\nTetraJet + Dampen\n59.75\n70.75\nTetraJet + Freeze\n16.45\n22.04\nTetraJet + Q-EMA (Ours)\n60.00\n72.25\nTetraJet + Q-Ramping (Ours)\n60.31\n71.32\ndence) by identifying them, reducing their update frequency,\nand increasing their gradient accumulation steps.\nOscillation Reduction throughout the Training\nWe use\nOscillation Ratio Rw (defined in Sec. 6.1) to characterize\nthe oscillation problem during the whole training process.\nWe define that those weights with Rw > 16 are oscillating\nweights. As shown in Fig. 6, both of our methods can\neffectively reduce the Oscillating Weights. Among them,\nQ-EMA reduces the most oscillating weights by directly\nsmoothing weight quantization. Q-Ramping also reduces\nthe oscillating level, while method “Dampen” from Nagel\net al. (2022) cannot effectively reduce oscillation in MXFP4\npre-training.\n7.3. Ablation Study\nTraining Method\nWe investigate the quantization method\nin the MXFP4 training. We find that double quantization\nconsistently outperforms Microscaling’s incorrect gradient\ncomputation. Besides, when we ensure unbiased gradient\nestimation by double quantization and truncation-free scal-\ning, we can get the optimal result with stochastic rounding.\nThe detailed results are listed in Tab. 5 in Appendix B.\nOther Methods on Oscillation Reduction\nFollowing the\nconfiguration in Nagel et al. (2022), we compared Q-EMA\n& Q-Ramping with their methods. As a result in Tab. 4, their\n“Dampen” method cannot work well on reducing oscillation\nin pre-training, and the “Freezing” method would encounter\nsevere degradation when adapted to pre-training tasks.\nStability Improvement\nWe removed weight quantizers in\nforward to simulate an oscillation-free training (set Q(1) to\nidentity function), and removed both activation and weight\nquantizers in forward to simulate a MXFP4 training with\nstable forward process (set Q(1) and Q(2) to identity func-\ntion). Consequently, our stabilization method Q-EMA and\nQ-Ramping can counteract the influence of weight oscilla-\ntion, and approach a comparable accuracy to training with a\nfull-precision forward process. Results are listed in Tab. 6\nin Appendix B.\n8. Conclusion\nIn this work, we not only proposed a new MXFP4 train-\ning method TetraJet for a more accurate 4-bit training in\nMXFP4 format, but also introduced novel approaches to an-\nalyzing and resolving the instability of forward pass, which\nis the bottleneck of MXFP4 training. Extensive experiments\nrevealed that our TetraJet consistently surpasses current 4-\n8\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nbit training methods, and Q-EMA / Q-Ramping can provide\nadditional enhancement with effective oscillation reduction,\nand even achieve competitive performance compared to\nfull-precision training.\nImpact Statement\nOur MXFP4 low-precision training method enhances AI\nefficiency, reduces energy consumption, and improves ac-\ncessibility by lowering hardware costs. This can help bridge\ntechnological gaps and promote sustainable AI develop-\nment. However, the reduced computational cost could lower\nbarriers to malicious uses, such as deepfake generation or\nautomated disinformation. Ensuring that such technologies\nare used ethically and for the benefit of society is essential\nto maximizing their positive impact.\nReferences\nBengio, Y., L´eonard, N., and Courville, A. Estimating\nor propagating gradients through stochastic neurons for\nconditional computation, 2013.\nChen, J., Gai, Y., Yao, Z., Mahoney, M. W., and Gonzalez,\nJ. E. A statistical framework for low-bitwidth training of\ndeep neural networks. Advances in neural information\nprocessing systems, 33:883–894, 2020.\nChmiel, B., Banner, R., Hoffer, E., Yaacov, H. B., and\nSoudry, D. Logarithmic unbiased quantization: Practical\n4-bit training in deep learning. 2021.\nCourbariaux, M., Bengio, Y., and David, J.-P. Binarycon-\nnect: Training deep neural networks with binary weights\nduring propagations. Advances in neural information\nprocessing systems, 28, 2015.\nKalamkar, D., Mudigere, D., Mellempudi, N., Das, D.,\nBanerjee, K., Avancha, S., Vooturi, D. T., Jammala-\nmadaka, N., Huang, J., Yuen, H., et al.\nA study of\nbfloat16 for deep learning training.\narXiv preprint\narXiv:1905.12322, 2019.\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437, 2024.\nLiu, S.-Y., Liu, Z., and Cheng, K.-T.\nOscillation-free\nquantization for low-bit vision transformers. In Inter-\nnational Conference on Machine Learning, pp. 21813–\n21824. PMLR, 2023.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,\nS., and Guo, B. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the\nIEEE/CVF international conference on computer vision,\npp. 10012–10022, 2021.\nMicikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey,\nP., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P.,\nKamalu, J., et al. Fp8 formats for deep learning. arXiv\npreprint arXiv:2209.05433, 2022.\nNagel,\nM.,\nFournarakis,\nM.,\nBondarenko,\nY.,\nand\nBlankevoort, T. Overcoming oscillations in quantization-\naware training. In International Conference on Machine\nLearning, pp. 16318–16330. PMLR, 2022.\nNarang, S., Diamos, G., Elsen, E., Micikevicius, P., Alben,\nJ., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., et al. Mixed precision training. In Int.\nConf. on Learning Representation, 2017.\nNVIDIA.\nNvidia\nblackwell\narchitecture,\n2024a.\nURL\nhttps://resources.nvidia.com/\nen-us-blackwell-architecture.\nAccessed:\n2025-01-30.\n9\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nNVIDIA.\nNvidia rtx blackwell gpu architecture,\n2024b.\nURL https://images.nvidia.cn/\naem-dam/Solutions/geforce/blackwell/\nnvidia-rtx-blackwell-gpu-architecture.\npdf. Accessed: 2025-01-30.\nNVIDIA.\nTransformer engine, 2024c.\nURL https:\n//github.com/NVIDIA/TransformerEngine.\nAccessed: 2025-01-30.\nPeng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z.,\nXiong, Y., Yang, Z., Ni, B., Hu, J., et al.\nFp8-lm:\nTraining fp8 large language models.\narXiv preprint\narXiv:2310.18313, 2023.\nRouhani, B. D., Garegrat, N., Savell, T., More, A., Han,\nK.-N., Zhao, Ritchie amd Hall, M., Klar, J., Chung, E.,\nYu, Y., Schulte, M., Wittig, R., Bratt, I., Stephens, N.,\nMilanovic, J., Brothers, J., Dubey, P., Cornea, M., Hei-\nnecke, A., Rodriguez, A., Langhammer, M., Deng, S.,\nNaumov, M., Micikevicius, P., Siu, M., and Verrilli, C.\nOcp microscaling (mx) specification. Technical report,\n2023a.\nRouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi,\nA., Deng, S., Choudhary, D., Cornea, M., Dellinger, E.,\nDenolf, K., et al. Microscaling data formats for deep\nlearning. arXiv preprint arXiv:2310.10537, 2023b.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of\nComputer Vision (IJCV), 115(3):211–252, 2015. doi:\n10.1007/s11263-015-0816-y.\nSun, X., Choi, J., Chen, C.-Y., Wang, N., Venkataramani,\nS., Srinivasan, V. V., Cui, X., Zhang, W., and Gopalakr-\nishnan, K. Hybrid 8-bit floating point (hfp8) training and\ninference for deep neural networks. Advances in neural\ninformation processing systems, 32, 2019.\nSun, X., Wang, N., Chen, C.-Y., Ni, J., Agrawal, A., Cui, X.,\nVenkataramani, S., El Maghraoui, K., Srinivasan, V. V.,\nand Gopalakrishnan, K. Ultra-low precision 4-bit training\nof deep neural networks. Advances in Neural Information\nProcessing Systems, 33:1796–1807, 2020.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablay-\nrolles, A., and J´egou, H. Training data-efficient image\ntransformers & distillation through attention. volume\nabs/2012.12877, 2020. URL https://arxiv.org/\nabs/2012.12877.\nVaswani, A. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017.\nWortsman, M., Dettmers, T., Zettlemoyer, L., Morcos, A.,\nFarhadi, A., and Schmidt, L. Stable and low-precision\ntraining for large-scale vision-language models.\nAd-\nvances in Neural Information Processing Systems, 36:\n10271–10298, 2023.\nXi, H., Li, C., Chen, J., and Zhu, J. Training transform-\ners with 4-bit integers. Advances in Neural Information\nProcessing Systems, 36:49146–49168, 2023.\nXi, H., Cai, H., Zhu, L., Lu, Y., Keutzer, K., Chen, J., and\nHan, S. Coat: Compressing optimizer states and activa-\ntion for memory-efficient fp8 training. arXiv preprint\narXiv:2410.19313, 2024a.\nXi, H., Chen, Y., Zhao, K., Teh, K. J., Chen, J., and Zhu,\nJ. Jetfire: Efficient and accurate transformer pretraining\nwith int8 data flow and per-block quantization. arXiv\npreprint arXiv:2403.12422, 2024b.\nZhu, F., Gong, R., Yu, F., Liu, X., Wang, Y., Li, Z., Yang,\nX., and Yan, J. Towards unified int8 training for convolu-\ntional neural network. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 1969–1979, 2020.\n10\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nA. Statistics to Measure Oscillation and Instability in MXFP4 Training\nIn this section, we formally define and explain the statistics we use in this paper to measure weight oscillation and training\ninstability.\nA.1. Oscillation Ratio\nDefinition\nDuring a training stage with T0 steps, we sum up updating distance for each master weight element w and its\nquantized weight wQ = Q(w):\ndistW =\nT0\nX\nt=1\n|wt −wt−1|,\ndistQ =\nT0\nX\nt=1\n|wt\nQ −wt−1\nQ |.\nWe define oscillation ratio Rw for each weight element, representing the degree of oscillation:\nRw := distQ/distW .\nIn the Q-Ramping method for pre-training, we set T0 = 30 to minimize the additional cost of identifying oscillating weights.\nIn the validation experiment (Tab. 6), we set T0 = 200 to fully validate the oscillation reduction.\nInterpretation\nIf a weight element w has higher Rw at a certain stage of training, it means that it shows more characteristics\nof oscillation. The larger Rw, the more frequently and severely the weight element w oscillates, which means that we should\nput more effort into suppressing the oscillation of w.\nCompare Oscillation Ratio and Previous Metric\nNagel et al. (2022) also define a metric flipping frequency f (average\nfrequency of quantization flipping, defined for each weight element) to find out oscillating weights and measure oscillation\nseverity, but it is only suitable for the small learning-rate training (e.g. fine-tuning, or near the end of pre-training), because\nwhen the learning-rate is relatively large (e.g. the early or middle stage of pre-training), the latent weight would be updated\nwith large step size and the quantized weights also change frequently during training, but f would falsely recognize some of\nthem as quantization oscillation. This is also a reason why the ”Freeze” method performs badly in pre-training (see the\nresult in Tab. 4).\nOscillation Ratio Rw overcomes the issue of oscillation detection in the early stage of pre-training. Only the weights that\nfall into real quantization oscillation would get a large Rw: these weights are with small moves around the quantization\nthreshold (distW is relatively small) but with frequent switch between quantization values (distQ is relatively large).\nA.2. Quantization Confidence\nDefinition\nTo quantitatively assess the severity of the oscillation problem, we define quantization confidence for each\nweight element w, which measures the normalized distance to the nearest quantization threshold.\nQuantConf(w) := mini |w −thrdi|\nMaxDist(wFP4)\nwhere wFP4 denotes the quantized FP4 value of w, {thrdi} denotes all the quantization thresholds, and MaxDist(wFP4)\ndenotes the maximum possible distance when quantized to wFP4. It is ensured that QuantConf(w) ∈[0, 1].\nWe can also define quantization confidence for a matrix W as the average QuantConf(w) of all the element w in W.\nInterpretation\nIf an element w has less quantization confidence, it is more prone to oscillate, because it is closer to the\nquantization threshold and little perturbation would make its quantized value switch frequently. If a weight matrix W has\nmore elements with low confidence, we call the weight distribution is less confident, which indicates that the optimization to\nthis weight is more unstable. For example, in Fig. 4, weights in Epoch 90 are less confident than weights in Epoch 30.\n11\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nA.3. Rate of Change for Weight and Activation\nDefinition\nwe define rate of change for a tensor X as\nr(X) = 1\nT0\nT0\nX\nt=1\n\r\rXt −Xt−1\r\r\nF\n\r\rXt−1\r\r\nF\nwhere t refers to training step, and step 0 ∼T0 refers to a short training interval.\nDuring pre-training, we can test the rate of change for the master weight W, the quantized weight matrix Q(2)(W⊤)⊤, and\nactivation Y in different stages.\nInterpretation\nThis metric is useful in the end of training. When Learning Rate (LR) is approaching zero to push the\nmodel to quickly descend to a local minimum and converge, we expect the rate of change for quantized weight and activation\ncan also be near zero to ensure stability of training. However, in Section 4.1, we have found that the rate of change stays\nhigh at the end of MXFP4 training.\nTherefore, if we can decrease the rate of change for quantized weight and output activation of quantized layers, it means we\neffectively improve the training stability. We have shown the results in Tab. 3.\n12\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nB. More Detailed Results of Ablation Study\nQuantization Methods\nWe do an ablation study to compare our training method TetraJet and Microscaling’s original\ntraining method. Through Tab. 5, we conclude that: (a) Our double quantization corrects the gradient estimation in MXFP4\nLinear Layers, and is consistently better than Microscaling’s original design. (b) As long as we give unbiased gradient\nestimation, which is guaranteed by double quantization and truncation-free scaling, we can reach the optimal strategy\nwith stochastic quantization in backward. (c) It is necessary to ensure unbiasedness. Only in the unbiased situation, can\nstochastic quantization exert its advantage.\nTable 5: Comparison on quantization methods. We report the accuracy on the validation set of 90-epoch DeiT-T pre-training.\nBackward Quant\nXW For Grad Computing\nComputation of Shared Scale\nTop-1%\nTop-5%\nNote\nStochastic\nDouble Quantization\nTruncation-Free Scaling\n59.75\n82.67\nTetraJet(unbiased gradient)\nStochastic\nDouble Quantization\nMicroscaling’s Scaling\n59.18\n82.64\nStochastic\nMicroscaling’s Design\nTruncation-Free Scaling\n56.98\n80.60\nStochastic\nMicroscaling’s Design\nMicroscaling’s Scaling\n57.49\n81.27\nDeterministic\nDouble Quantization\nTruncation-Free Scaling\n58.60\n82.11\nDeterministic\nDouble Quantization\nMicroscaling’s Scaling\n59.02\n82.18\nDeterministic\nMicroscaling’s Design\nTruncation-Free Scaling\n58.40\n81.57\nDeterministic\nMicroscaling’s Design\nMicroscaling’s Scaling\n58.56\n81.92\nMicroscaling\nStability Improvement\nWe simulated an oscillation-free training by removing the weight quantizer in forward, and\nsimulated a stable forward process by removing both weight & activation quantizers in forward. As a result in Tab. 6, our\nmethods Q-EMA & Q-Ramping can fully eliminate the negative effects of weight oscillation, and can approach better\naccuracy with a more stable forward process.\nData Format\nWe study the choice of FP4 format for the forward and backward computation. In Tab. 7, although E3M0 is\nanother possible FP4 format, E2M1 is always a better format for weight, activation, and gradient.\nTable 6: Ablation study on quantization stability. We re-\nport the accuracy on validation set of 90-epoch DeiT-B\npre-training. WQ: Weight Quantization in forward; AQ: Ac-\ntivation Quantization in forward.\nTop-1 Acc.%\nTetraJet\n74.91\nTetraJet w/o WQ\n75.16\nTetraJet w/o WQ & AQ\n75.86\nTetraJet + Q-EMA\n77.32\nTetraJet + Q-Ramping\n75.62\nTable 7: MXFP4 Data Format Selection. We report the\ntop-1 Acc.% of DeiT-T Pre-Training.\nA&W\nGrad\nE2M1\nE3M0\nE2M1\n59.75\n58.90\nE3M0\n54.21\n53.72\n13\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nC. Detailed Implementation of Q-EMA and Q-Ramping\nC.1. Algorithm: EMA Quantizer (Q-EMA)\nAlgorithm 1 EMA Quantizer for a Micro-Block (Q-EMA)\ninput Weight Block W; EMA weight block WEMA.\noutput Quantized Weight Block (WFP4, s) in MXFP4 format\n1: Assume W and WEMA are vectors of size 32.\n2: M ←max1≤i≤32 |Vi|, f\nM ←M + ε · I (M = 0)\n3: s ←\nl\nlog2\n2f\nM\nQp−Qn\nm\n, S ←2s\n4: for i ←1 to 32 do\n5:\nq1, q2 ←two nearest MXFP4 values to Wi\nS\n6:\nif\n\f\f WEMAi\nS\n−q1\n\f\f <\n\f\f WEMAi\nS\n−q2\n\f\f then\n7:\nWFP4\ni\n←q1\n8:\nelse\n9:\nWFP4\ni\n←q2\n10:\nend if\n11: end for\n12: Return MXFP4 block (WFP4, s)\nC.2. Algorithm: Adaptive Ramping Optimizer (Q-Ramping)\nAlgorithm 2 Adaptive Ramping Algorithm for MXFP4 Training (Q-Ramping)\n1: Hyperparameter: k1, k2.\n2: function OscillationDetection(Model M, Global Learning-Rate LR, Global Batch-Size BS)\n3:\nTrain the model M for T0 ≪Tupdate steps on a calibration dataset without Q-Ramping, to detect oscillating weight.\n4:\nfor each weight element w in quantized layers do\n5:\nCompute the oscillation ratio Rw according the length of trajectory of master weight w & quantized weight wQ;\n6:\nLRw ←min(k2⌊Rw/k1⌋+ 1, Nmax) · LR;\n7:\nBSw ←min(k2⌊Rw/k1⌋+ 1, Nmax) · BS;\n8:\n// k1, k2 are coefficients for amplifying LR & BS (that is using a higher gradient accumulation step).\n9:\n// Nmax denotes the maximum amplification factor.\n10:\nend for\n11: end function\n12: function ModelTraining with Q-Ramping(Initial Model M, Steps T, Learning-Rate LR, Batch-Size BS)\n13:\nfor t ←0 to T do\n14:\nif t mod Tupdate = 0 then\n15:\ncall OscillationDetection(M, LR, BS) to adaptively adjust LRw & BSw for each element w;\n16:\nend if\n17:\nfor each weight element w in quantized layers do\n18:\nupdate w according to LRw & BSw by Customized AdamW;\n19:\nend for\n20:\nfor each parameter W in non-quantized layers do\n21:\nupdate W by normal AdamW;\n22:\nend for\n23:\nend for\n24: end function\n14\n\n\nOscillation-Reduced MXFP4 Training for Vision Transformers\nC.3. Selection of Hyperparameter & Insensitity to Hyperparameter\nFor Q-EMA, the momentum β = 0.998 for calculating WEMA is a good default choice. For Q-Ramping, k1 = 16 is a good\nthreshold to measure the severity of oscillation, and k2 = 5 is a default ratio for amplifying the Learning Rate & Batch Size\n(meanwhile, reducing the frequency of oscillation). We can reach better performance through minor tuning. The detailed\nsettings are listed in Tab. 8.\nTable 8: Selection of hyperparameter in Q-EMA & Q-Ramping.\nDeiT-T\nDeiT-S\nDeiT-B\nSwin-T\nSwin-S\nTetraJet\n59.75\n71.03\n74.91\n77.12\n79.51\nTetraJet + Q-EMA (default: β = 0.998)\n59.69\n71.51\n77.18\n77.23\n79.74\nTetraJet + Q-EMA (best: β tuned)\n60.00\n(β = 0.9983)\n72.25\n(β = 0.9972)\n77.32\n(β = 0.999)\n77.30\n(β = 0.9975)\n79.74\n(β = 0.998)\nTetraJet + Q-Ramping (default: k1 = 16, k2 = 5)\n60.31\n71.32\n75.62\n77.23\n79.52\nTetraJet + Q-Ramping (best: k1 = 16, k2 tuned)\n60.31\n(k2 = 5)\n71.32\n(k2 = 5)\n75.62\n(k2 = 5)\n77.33\n(k2 = 3)\n79.67\n(k2 = 4)\nWe also validate Q-EMA / Q-Ramping’s insensitivity to hyperparameter choice in Tab. 9 & 10.\nTable 9: Insensitivity to hyperparameters (TetraJet + Q-EMA) on DeiT-B.\nβ\n0.993\n0.995\n0.997\n0.998\n0.999\n0.9995\nw/o Q-EMA\nAccuracy\n75.39\n76.37\n77.23\n77.18\n77.32\n77.30\n74.91\nTable 10: Insensitivity to hyperparameters (TetraJet + Q-Ramping) on DeiT-B.\nk1\n16\n16\n16\n16\n16\n16\n8\n12\n16\n20\nw/o Q-Ramping\nk2\n3\n4\n5\n6\n7\n8\n5\n5\n5\n5\nAccuracy\n75.35\n75.33\n75.62\n74.96\n75.29\n75.13\n75.19\n75.60\n75.62\n74.85\n74.91\nC.4. Other Discussion on Q-EMA & Q-Ramping\nQ: Why cannot we combine two algorithms?\nA: When we use Q-EMA, there are two variables (W & WEMA) that determine the result of weight quantization, so training\nwith Q-EMA results in a different MXFP4 training dynamic. Therefore, it is more complicated to identify and track the\noscillating weights in this situation. Therefore, it is not proper to simply combine Q-EMA & Ramping.\n15\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20853v1.pdf",
    "total_pages": 15,
    "title": "Oscillation-Reduced MXFP4 Training for Vision Transformers",
    "authors": [
      "Yuxiang Chen",
      "Haocheng Xi",
      "Jun Zhu",
      "Jianfei Chen"
    ],
    "abstract": "Pre-training Transformers in FP4 precision is becoming a promising approach\nto gain substantial speedup, but it comes with a considerable loss of accuracy.\nMicroscaling (MX) data format provides a fine-grained per-group quantization\nmethod to improve the representation ability of the FP4 format and is supported\nby the next-generation Blackwell GPU architecture. However, training with MXFP4\ndata format still results in significant degradation and there is a lack of\nsystematic research on the reason.\n  In this work, we propose a novel training method TetraJet for a more accurate\nFP4 training. We comprehensively evaluate all of the quantizers involved in the\ntraining, and identify the weight oscillation problem in the forward pass as\nthe main source of the degradation in MXFP4 training. Therefore, we introduce\ntwo novel methods, EMA Quantizer (Q-EMA) and Adaptive Ramping Optimizer\n(Q-Ramping), to resolve the oscillation problem. Extensive experiments on\nVision Transformers demonstrate that TetraJet consistently outperforms the\nexisting 4-bit training methods, and Q-EMA & Q-Ramping can provide additional\nenhancement by effectively reducing oscillation. We decreased the accuracy\ndegradation by more than $50\\%$ compared to the baseline, and can even achieve\ncompetitive performance compared to full precision training. The codes are\navailable at https://github.com/thu-ml/TetraJet-MXFP4Training",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}