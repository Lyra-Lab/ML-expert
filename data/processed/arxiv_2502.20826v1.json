{
  "id": "arxiv_2502.20826v1",
  "text": "CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free\nZero-Shot Composed Image Retrieval\nZelong Sun*, Dong Jing∗, Zhiwu Lu †\nGaoling School of Artificial Intelligence\nRenmin University of China, Beijing, China\nzelongsun@ruc.edu.com, luzhiwu@ruc.edu.com\nAbstract\nZero-Shot Composed Image Retrieval (ZS-CIR) aims to re-\ntrieve target images by integrating information from a com-\nposed query (reference image and modification text) with-\nout training samples. Existing methods primarily combine\ncaption models and large language models (LLMs) to gen-\nerate target captions based on composed queries but face\nvarious issues such as incompatibility, visual information\nloss, and insufficient reasoning. In this work, we propose\nCoTMR, a training-free framework crafted for ZS-CIR with\nnovel Chain-of-thought (CoT) and Multi-scale Reasoning.\nInstead of relying on caption models for modality transfor-\nmation, CoTMR employs the Large Vision-Language Model\n(LVLM) to achieve unified understanding and reasoning for\ncomposed queries. To enhance the reasoning reliability, we\ndevise CIRCoT, which guides the LVLM through a step-by-\nstep inference process using predefined subtasks. Consid-\nering that existing approaches focus solely on global-level\nreasoning, our CoTMR incorporates multi-scale reasoning\nto achieve more comprehensive inference via fine-grained\npredictions about the presence or absence of key elements\nat the object scale. Further, we design a Multi-Grained\nScoring (MGS) mechanism, which integrates CLIP similar-\nity scores of the above reasoning outputs with candidate\nimages to realize precise retrieval. Extensive experiments\ndemonstrate that our CoTMR not only drastically outper-\nforms previous methods across three prominent benchmarks\nbut also offers appealing interpretability.\n1. Introduction\nZero-Shot Composed Image Retrieval (ZS-CIR) [3, 38, 42]\naims to retrieve the target image from gallery images by\nintegrating information from a reference image and a mod-\nification text, without training with annotated triplets data.\n*Equal Contribution\n†Corresponding Author\nTarget Image Caption:\nA golden retriever is sleeping on a blanket indoors.\nExistent Objects:\n[Indoor setting. Golden Retriever ......]\nNonexistent Objects:\n[Outdoor setting. Sitting dog]\n“A puppy with a big\nsmile on its face.”\nCIRCoT Prompt:\nStep 1, Understand the Reference Image.\nStep 2, Analyze ......\nModification Text:\n“A photo of [S*]\nthat have same \nbreed dog inside \nasleep on a blanket.”\nTextual \nImversion\nCaptioner\nLLM\nLVLM\nText-to-Image \nRetrieval\n“Have same breed\ndog inside asleep \non a blanket.”\nTarget Image Caption:\nThe dog is asleep on a blanket inside.\nText-to-Image Retrieval\nMulti-Grained Scoring Mechanism\nRetrieved Image\nRetrieved Image\nRetrieved Image\nReference Image\nReference Image\nReference Image\nModification Text\nModification Text\nS*\nContent Lost\nWeak Reasoning\n(a)  Pseudo-Token\n(b)  LLM-based Method\n(c)  Ours (CoTMR)\n“Have same breed\ndog inside asleep \non a blanket.”\nFigure 1. Flowcharts of existing ZS-CIR methods and our pro-\nposed CoTMR. Methods (a) and (b) face serious issues of vi-\nsual information loss and insufficient reasoning. In contrast, our\nmethod (c) fully perceives image content, enhances reasoning pro-\ncess with CIRCoT, and augments multi-grained descriptions with\nmulti-scale reasoning.\nIn contrast to traditional image retrieval tasks [9–11, 26],\nwhich typically involve a single modality, CIR queries ne-\ncessitate precise “editing” to the reference image based on\nthe modification text. Therefore, successfully completing\nthe CIR task entails (1) Advanced multimodal composed\nunderstanding abilities to accurately interpret the visual\ncontext and user’s modification intent in modification text,\nand (2) Robust multimodal reasoning abilities to imple-\nment the modifications appropriately.\nAs shown in Figure 1 (a), previous methods [3, 38]\nprimarily propose a textual inversion module to generate\npseudo-tokens from the reference image and concatenate it\nwith the modification text. However, these methods still\nrequire extensive data for training, and the relatively short\nlength of pseudo-tokens limits the model’s ability to fully\nunderstand the reference image. Notably, these statistical\nmethods lack enough logical reasoning for the CIR task.\nRecent works [17, 48] leverage Large Language Models\n(LLMs) to identify the user’s modification intent. As shown\nin Figure 1 (b), these methods use pre-trained caption mod-\n1\narXiv:2502.20826v1  [cs.CV]  28 Feb 2025\n\n\nels to generate a caption for the reference image, and then\nemploy the LLM to edit this caption based on the modifi-\ncation text. However, the cascading combination of differ-\nent models introduces several challenges. (1) Component\nIncompatibility: There are domain gaps in language style\nand way of thought between caption models and LLMs;\n(2) Visual Information Loss: During the caption generation\nprocess, some detailed information about the reference im-\nage is inevitably lost; (3) Single-scale Reasoning: Existing\nmethods focus solely on image-scale reasoning, neglecting\nfine-grained details; (4) Insufficient Reasoning: As a key\ncomponent, current approaches have not fully leveraged the\nreasoning capability of LLM. Therefore, as shown in Fig-\nure 1, the aforementioned methods make it hard to preserve\nthe “golden retriever” characteristics in the reference image.\nIn this work, we propose CoTMR, a training-free and\nhighly interpretable framework crafted for ZS-CIR with\nnovel Chain-of-Thought (CoT) and Multi-scale Reasoning.\nAs shown in Figure 1 (c), instead of relying on the combina-\ntion of caption models and LLMs, our CoTMR employs the\nLarge Vision-Language Model (LVLM) to achieve unified\nunderstanding and reasoning for composed queries. This\nframework offers several appealing benefits, including rich\nvisual information, unified reasoning, and simplified work-\nflow. Furthermore, we propose a novel CoT method, named\nCIRCoT, to further enhance the reasoning capability and\ninterpretability of the LVLM in the CIR task. Unlike previ-\nous works [51] that entirely delegate the task decomposition\nprocess to the model, CIRCoT pre-divides the CIR task into\nmultiple subtasks and allows the model to reason each pre-\ndefined subtask step-by-step. Additionally, a few examples\ncan also be included for reference in CoT [46]. This struc-\ntured reasoning process not only guides the LVLM through\na step-by-step inference process but also provides high-level\ninterpretability, allowing users to intervene for more precise\nretrieval when necessary.\nWith this structured reasoning process, we further pro-\npose Multi-Scale Reasoning to obtain both the global de-\nscription and fine-grained details of the target image from\nthe composed query. As shown in Figure 1 (c), in addition\nto reasoning the “target image caption” at the image scale,\nwe further conduct object-scale reasoning to emphasize key\nobjects and attributes. Notably, aligning with the require-\nment of CIR, we should not only infer the objects that\nshould be present in the target image (“existent objects”)\nbut also naturally take those that should not be present\n(“nonexistent objects”) into account. The existent objects\nfurther supplement the target image caption, while nonexis-\ntent objects are used to reduce distracting information. Sub-\nsequently, Multi-Grained Scoring (MGS) mechanism is\ndesigned to enable a precise retrieval process. This mod-\nule comprehensively considers the characteristics of these\nmulti-grained outputs and separately calculates their simi-\nlarity scores with the candidate images via CLIP [35]. Ul-\ntimately, MGS integrates these scores together to achieve a\nbalanced evaluation by rewarding the presence of relevant\ncontent while penalizing irrelevant or conflicting content.\nOur main contributions can be summarized as follows:\n(1) We propose CoTMR, a novel training-free LVLM-based\nframework for ZS-CIR. (2) We present multi-scale reason-\ning and a novel scoring module to provide multi-grained de-\nscriptions and evaluations. (3) We design a novel CIRCoT,\nwhich standardizes the LVLM’s reasoning process, allow-\ning it to focus on specific goals at each subtask. (4) Ex-\ntensive experiments demonstrate that our CoTMR not only\nsignificantly outperforms state-of-the-art methods across\nthree prominent benchmarks but also offers appealing in-\nterpretability for CIR.\n2. Related Work\n2.1. Zero-Shot Composed Image Retrieval\nCIR [2, 5, 19, 44] integrates concepts from compositional\nlearning [16, 29] and cross-modal retrieval [33, 34].\nTo\nmitigate the high cost and time-consuming nature of train-\ning dataset annotation for CIR, ZS-CIR has recently been\nintroduced. Currently, two prominent directions exist: one\napproach [3, 13, 38] trains a textual inversion module us-\ning only image-caption data, representing the reference im-\nage with a single pseudo-token that is then concatenated\nwith the reference caption. This method not only requires\ntraining but also is limited by the length of the pseudo-\ntoken, which constrains the representation of the reference\nimage. The other approach [17, 41, 48] forms a training-\nfree method by cascading multiple off-the-shelf tools. It\nfirst converts reference image into a textual description us-\ning a captioning model and then edits this caption accord-\ning to the modification text by a LLM. Finally, the edited\ncaption is used to compute CLIP scores with candidate im-\nages for retrieval. However, such methods face several chal-\nlenges, including component incompatibility, visual infor-\nmation loss, and insufficient reasoning. In this work, we\npropose a unified, training-free, and interpretable frame-\nwork with CIRCoT and Multi-Scale Reasoning.\n2.2. Vison-Language Model\nThere are two main types of Vision-Language Models\n(VLMs). The first type, including models like CLIP [35]\nand BLIP [20], is pre-trained on large-scale image-caption\ndatasets, enabling them to map images and text into a shared\nembedding space for cross-modal retrieval [4, 36] or open-\nvocabulary classification [32, 43].\nIn this work, we use\nCLIP for the multimodal retrieval process. The second type\nis LVLM [7, 24, 45], which are pre-trained to integrate vi-\nsual information into LLM and are post-trained to align with\nusers. Thus, LVLMs could understand user intent and pro-\n2\n\n\nCIRCoT — ����\nLVLM\n    To generate the target image caption, \nplease follow these subtasks:\n    1. Understand the Reference Image\n    2. Analyze the Modification Text        \n    3. Apply the Modifications\n    4. Generate the Final Caption\n“Just two manta rays \nsurrounded by yellow \nfish, chasing each other”\nTarget Image Caption\nImage-Scale Reasoning\nReference Image\nModification Text\n[ “diving  human”, \n“jellyfish” ]\n    Infer objects and attributes that should \nand should not appear in the target image. \nPlease follow these subtasks:\n    1. List the objects and their attributes \n        present in the reference image\n    2. Analyze the modification text to\n        identify changes to objects\n    3. Update the objects and attributes\n    4. Determine the content of objects\nObject-Scale Reasoning\nMulti-Grained Scoring Mechanism\nTE\nIE\nCandidate Images \nmean\n......\nBase Scores\n......\nPositive Scores\n......\nNegative Scores\nRetrieved Image\n0.6\n0.9\n0.3\n0.7\n0.4\n0.2\n0.1\n0.8\n0.5\n...\nComposed Query\nLVLM\nCIRCoT — ����\n“Two manta rays \nchasing each other \nsurrounded by yellow \nfish, no human and \nno jellyfish.”\nCombination of \nExistent Objects\nList of Nonexistent Objects\n “two manta rays. \nyellow fish. ocean.” \n......\nFinal Scores\n0.5\n1.2\n0.0\nFigure 2. Overview architecture of CoTMR: (1) The LVLM equipped with CIRCoT, PImg and PObj, performs reasoning on the\ncomposed query at both image and object scales, to provide multi-grained outputs. (2) The Multi-Grained Scoring Mechanism combines\nthe similarities of the three outputs with candidate images in the CLIP space through a reward-penalty calculation. IE and TE represent the\nimage encoder and text encoder of CLIP, respectively.\ncess various visual tasks, such as image captioning [1, 22],\nVQA [12, 14], and OCR [28, 39]. In this work, we utilize\nthe LVLM to inference both the global description and fine-\ngrained details based on composed queries.\n2.3. Chain of Thought\nRecently, zero-shot [18] and few-shot [37, 46] multi-step\nreasoning prompts have shown significant enhancement to\nthe reasoning capabilities of LLMs.\nConsequently, CoT\nstrategy raises increasing research attention and is also ex-\ntended into multimodal domains. MM-CoT [50] designs\na two-stage framework where the model initially learns to\ngenerate rationales based on real annotations and then uses\nall available information to produce the final answer. DD-\nCoT [51] focuses on text understanding, breaking down\nquestions into sub-questions for step-by-step responses.\nCCoT [30], on the other hand, is based on image under-\nstanding, generating scene graphs of images to provide an-\nswers. However, several works [40, 49] suggest that CoT\nseems to work effectively only in some specific domains.\nIn this work, we propose CIRCoT, which pre-divides the\ntask into multiple subtasks and allows the model to reason\nthese subtasks step-by-step.\n3. Methodology\n3.1. Preliminary\nGiven a composed query Q = {Ir, Tm}, where Ir denotes\nthe reference image and Tm denotes the modification text,\nand a candidate set D = {I1\nt , I2\nt , ..., IND\nt\n} consisting of ND\nimages, the goal of CIR is to identify the k target images\nfrom the candidate set D that are most relevant to the query\nQ, with k ≪ND. ZS-CIR further requires that no training\ndata triplets be used.\nDifferent from the traditional multi-modal retrieval\ntask [9–11, 26], CIR requires the model to retrieve images\nthat both preserve the key features of the reference image\nand satisfy the modifications described in the modification\ntext. Successfully completing the CIR task requires: (1)\ncorrectly understanding the content of the reference image\nand the modification text, (2) accurately applying the mod-\nifications, and (3) an effective score mechanism for the re-\ntrieval. Therefore, CIR methods should possess advanced\nmultimodal composed understanding and reasoning capa-\nbilities, as well as a comprehensive score mechanism.\n3.2. Overall Architecture\nOur proposed CoTMR is an effective, training-free and\ninterpretable CIR framework based on public pre-trained\nVLMs.\nAs shown in Figure 2, our CoTMR consists of\n3\n\n\ntwo steps: reasoning the composed query by LVLM and re-\ntrieving the target image by CLIP. In the reasoning process,\nto enhance the interpretability and reliability of reasoning,\nwe first propose CIRCoT, a novel CoT strategy with pre-\ndefined subtask divisions tailored for CIR. Moreover, we\nconduct image-scale and object-scale reasoning, both with\nCIRCoT, to obtain the global description and fine-grained\ndetails for the target image. For the retrieval process, we de-\nsign a novel Multi-Grained Scoring (MGS) mechanism that\ncomprehensively considers the characteristics of the above\nreasoning outputs at different scales via a reward-penalized\nformulation. We describe the three modules below.\n3.3. CIRCoT\nCIR requires precise understanding and reasoning of the\nmulti-modal composed query, making it a complex task.\nTo achieve a more accurate and reliable reasoning process,\nwe propose using CoT to facilitate multi-step reasoning of\nLVLM. However, we find that traditional CoT approaches,\nsuch as DDCoT [51], which typically rely on LVLM itself\nto independently develop problem-solving and task decom-\nposition strategies, tend to work effectively only in a few\nspecific domains [40]. Given the certainty of CIR inputs\n(reference image and modification text) and the clarity of\nCIR task (editing the reference image according to the mod-\nification text), we thus propose CIRCoT, which decomposes\nthe CIR task into multiple subtasks in advance.\nAs illustrated in Figure 3, we divide the task of gener-\nating the target image caption using the LVLM into four\nkey subtasks: (1) Image understanding; (2) Modification\ntext understanding; (3) Modification implementation and\n(4) Target image caption generation.\nThese four funda-\nmental subtasks structure the overall reasoning process of\nthe LVLM. For each subtask, we adhere to the traditional\nCoT approach, enabling the model to reason in a step-by-\nstep manner (as represented by italicized prompts in Fig-\nure 3). Additionally, we incorporate several step-wise rea-\nsoning examples to further stimulate the LVLM’s reasoning\ncapability like [46]. We emphasize that CIRCoT not only\ncapitalizes on the LVLM’s reasoning capability but also of-\nfers significant interpretability. Users can clearly follow the\nLVLM’s inference process and, if needed, intervene to mod-\nify it. More details can be found in Appendix 10.\nCombined with the multi-scale reasoning strategy to be\nintroduced, we have two CIRCoT prompts in this work, de-\nnoted as PImg and PObj, which are applied at image scale\nand object scale, respectively. Here, we take PImg as an\nexample shown in Figure 3, and PObj follows a similar pro-\ncess (see Appendix 8 for details).\n3.4. Mutil-Scale Reasoning\nWhen applying LVLM to address ZS-CIR task, a straight-\nforward approach is to directly infer the target image cap-\nYour task is to modify the reference image based on the modification instructions \nand generate the updated image description. The description should be complete and \ncan cover various semantic aspects, such as cardinality, addition, negation, direct \naddressing, compare & change, comparative, conjunction, spatial relations & \nbackground, viewpoint.\nTo complete the task accurately, please follow these steps:\n### Understand the Reference Image ###\n1. Identify all the objects, attributes, and their relationships in the image.\n2. Pay attention to the spatial relations, background, viewpoint in the image.\n3. Please complete this task step by step.\n### Analyze the Modification Instructions ###\n1. Break down the modification instructions into separate modification steps.\n2. Determine which objects or attributes need to be modified and how.\n3. Pay attention to any additions, deletions, or changes to attributes.\n4. Please complete this task step by step.\n### Apply the Modifications###\n1. Apply the modifications step by step to update the content of the reference image.\n### Generate the Target Image Caption ###\n1. Write a coherent and concise image caption.\n2. Ensure the caption accurately reflects all the modifications.\n3. The edited caption needs to be as simple as possible.\n4. Do not mention the content that will not be present in the target image. \nHere are some examples:\nExample 1: .......             Example 2: .......\nCIRCoT — 𝑷𝑰𝒎𝒈\nFigure 3.\nIllustration of CIRCoT in image-scale reason-\ning (PImg), which includes four predefined subtasks and allows\nLVLM to reason step-by-step within each subtask. CIRCoT in\nobject-scale reasoning (PObj) follows a similar process (see ap-\npendix 8 for details).\ntion based on the composed query. However, this global\ndescription presents several challenges: (1) The dense se-\nmantic content in the generated caption overshadows the\nkey objects and attributes that need more attention. (2) In\ncomplex scenarios, the model may be confused by irrele-\nvant details in the reference image (e.g. the “human” and\n“jellyfish” in Figure 2).\nTo alleviate the negative impact of unclear key features\nand irrelevant information contained in the global caption,\nin addition to global caption generation, we propose to rea-\nson at the object scale to obtain supplementary fine-grained\ndetails. As shown in Figure 2, at image-scale reasoning, we\nutilize the LVLM to reason the editing process and generate\nthe target image caption, which is formulated as:\nTtc = LV LM(Ir, Tm, PImg)\n(1)\nwhere Ttc is the target image caption, LV LM(·) denotes\nthe reasoning process with LVLM and PImg denotes the\nCIRCoT prompt at image-scale reasoning.\nIn object-scale reasoning, we let LVLM focus on specific\nobjects and their attributes, specifying the set of objects that\nshould be present in the target image (“existent objects”),\nand those should not be present (“nonexistent objects”).\nThis process is expressed as:\nEO, NEO = LV LM(Ir, Tm, PObj)\n(2)\n4\n\n\nHere, EO = [T i\neo]Le\ni=0 denotes the list of “existent objects”,\nwhere T i\neo represents the i−th object that should be present\nand Le is the total number of these existent objects. Sim-\nilarly, NEO = [T i\nneo]Lu\ni=0 denotes the list of “nonexistent\nobjects”. T i\nneo and Lu represents the i −th nonexistent ob-\nject and the total number of these objects. PObj denotes the\nCIRCoT prompt used at object-scale reasoning.\nAs shown in Figure 2, the “existent objects” further em-\nphasize the key elements that require extra attention (“Two\nmanta rays”, “yellow fish” and “ocean”), while the “nonex-\nistent objects” mitigate the influence of irrelevant informa-\ntion from the reference image (“human” and “jellyfish”).\n3.5. Multi-Grained Scoring\nAfter obtaining the above outputs at multiple scales (target\nimage caption, existent objects, and nonexistent objects),\nwe further design this MGS mechanism to comprehensively\nconsider their impact on the final retrieval process.\nSpecifically, as illustrated in Figure 2, we first compute\nsimilarities between the “target image caption” and candi-\ndate images using CLIP as the base scores Sbase:\nSbase = CLIP(Ttc, D)\n(3)\nwhere D denotes the set of candidate images,\nand\nCLIP(·, ·) computes the similarity between text and im-\nages in the CLIP space.\nAt object scale, considering that “existent objects” typi-\ncally have inherent correlations, they should be treated as a\nwhole to collectively influence the matching result. There-\nfore, we concatenate these objects into one string and then\ncompute its similarities with candidate images to obtain\npositive scores Spos:\nSpos = CLIP(Concat([T i\neo]Le\ni=0), D)\n(4)\nwhere Concat(·) denotes the concatenation of strings.\nIn contrast, “nonexistent objects” usually have no inher-\nent correlation with each other. Thus, we first calculate their\nsimilarities with candidate images individually, and then av-\nerage their similarities to derive the negative scores Sneg:\nSneg = Avg([CLIP(T i\nneo, D)]Lu\ni=0)\n(5)\nwhere Avg(·) denotes the average of scores. This strategy\nensures an equal contribution of each undesired object.\nFinally, we combine base scores, positive scores, and\nnegative scores using weighted aggregation to obtain the fi-\nnal scores S served as selection criteria:\nS = Sbase + λ · Spos −µ · Sneg\n(6)\nwhere λ and µ are the weights assigned to the positive score,\nand negative score, respectively.\nNotably, unlike previous works that mostly focused on\nthe updated content of the target image, our MGS mech-\nanism assigns negative scores to objects that should not\nbe present, which is advantageous in better filtering out\nmisleading candidate images. Furthermore, by rewarding\nthe presence of relevant content while penalizing irrelevant\ncontent, our score mechanism ensures a much more com-\nprehensive evaluation for CIR.\n4. Experiments\n4.1. Implementation Details\nFor the LVLM, we use Qwen2-VL-72B [45]. For the re-\ntrieval model, we experiment with different CLIP variants,\nincluding ViT-B/32, ViT-L/14, and ViT-G/14 CLIP from\nOpenCLIP [15]. The hyperparameter λ and µ are set to\n1 and 0.5 for the FashionIQ dataset, 1 and 0.3 for the CIRR\ndataset, and 0.5 and 0.3 for the CIRCO dataset, respectively.\nThe entire model is implemented using PyTorch [31] on 8\nNVIDIA A800 GPUs.\n4.2. Baselines\nWe use the “image-only” and “text-only” to denote directly\nperforming retrieval with CLIP using only the reference im-\nage and modification text. PALAVRA [6], Pic2Word [38],\nSEARLE [3] are the textual inversion methods either de-\nsigned or adapted for ZS-CIR. CIReVL [17] and LDRE [48]\nare LLM-based, training-free methods that cascade caption-\ning models and LLMs to generate textual descriptions of the\ntarget image. Among them, CIReVL is most similar to our\nmethod and serves as the most direct baseline for CoTMR.\n4.3. Datasets and Evaluation Metrics\nWe make performance evaluations on three CIR bench-\nmarks, including a fashion-domain dataset Fashion-IQ\n[47], as well as two open-domain datasets CIRR [27] and\nCIRCO [3]. FashionIQ contains garment images that can\nbe divided into three categories: dress, shirt, and toptee.\nCIRR is the first natural image dataset designed specifically\nfor CIR. CIRCO is based on real-world images from the\nCOCO 2017 unlabeled set [23] and is the first dataset for\nCIR to provide multiple ground truths.\nFor FashionIQ, we adopt Recall@K (R@K) as the eval-\nuation metric, which refers to the fraction of queries for\nwhich the correct item is retrieved among the top K results.\nWe also report Rmean, the mean of all R@K values, to eval-\nuate the overall retrieval performance. For CIRR, beside\nRecall@K, we additionally report Recallsubset@K and the\naverage score of Recall@5 and Recallsubset@1 as in [27].\nFor CIRCO, since there are multiple positives, we use the\nmean average precision@k (mAP@k) as the metric.\n5\n\n\nBackbone\nMethod\nTraining-free\nShirt\nDress\nTops&Tee\nAvg.\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nRmean\nViT-B/32\nPALAVRA\n✗\n21.49\n37.05\n17.25\n35.94\n20.55\n38.76\n19.76\n37.25\n28.50\nSEARLE\n✗\n24.44\n41.61\n18.54\n39.51\n25.70\n46.46\n22.89\n42.53\n32.71\nCIReVL\n28.36\n47.84\n25.29\n46.36\n31.21\n53.85\n28.29\n49.35\n38.82\nLDRE\n27.38\n46.27\n19.97\n41.84\n27.07\n48.78\n24.81\n45.63\n35.22\nCoTMR\n33.42\n53.93\n31.09\n54.54\n38.40\n61.14\n34.30\n56.54\n45.42\nViT-L/14\nPic2Word\n✗\n26.2\n43.6\n20.00\n40.2\n27.9\n47.40\n24.70\n43.70\n34.20\nSEARLE\n✗\n26.89\n45.58\n20.48\n43.13\n29.32\n49.97\n25.56\n46.23\n35.89\nCIReVL\n29.49\n47.40\n24.79\n44.76\n31.36\n53.65\n28.55\n48.57\n38.56\nLDRE\n31.04\n51.22\n22.93\n46.76\n31.57\n53.64\n28.51\n50.54\n39.52\nCoTMR\n35.43\n54.91\n31.18\n55.04\n38.55\n61.33\n35.05\n57.09\n46.50\nViT-G/14\nCIReVL\n33.71\n51.42\n27.07\n49.53\n35.80\n56.14\n32.19\n52.36\n42.27\nLDRE\n35.94\n58.58\n26.11\n51.12\n35.42\n56.67\n32.49\n55.46\n43.97\nCoTMR\n38.32\n62.24\n34.51\n57.36\n41.90\n64.30\n38.25\n61.32\n49.78\nTable 1. Comparison with the state-of-the-art methods on the Fashion-IQ dataset. Rmean indicates the average results across all the\nmetrics. The best results are in boldface, while the second-best results are underlined.\nBenchmark\nCIRCO\nCIRR\nMetric\nmAP@k\nRecall@k\nRecallsub@k\nAvg.\nBackbone\nMethod\nTraining-free\nk=5\nk=10\nk=25\nk=50\nk=1\nk=5\nk=10\nk=50\nk=1\nk=2\nk=3\nViT-B/32\nPALAVRA\n✗\n4.61\n5.32\n6.33\n6.80\n16.62\n43.49\n58.51\n83.95\n41.61\n65.30\n80.94\n42.55\nSEARLE\n✗\n9.35\n9.94\n11.13\n11.84\n24.00\n53.42\n66.82\n89.78\n54.89\n76.60\n88.19\n54.15\nCIReVL\n14.94\n15.42\n17.00\n17.82\n23.94\n52.51\n66.00\n86.95\n60.17\n80.05\n90.19\n56.34\nLDRE\n17.96\n18.32\n20.21\n21.11\n25.69\n55.13\n69.04\n89.90\n60.53\n80.65\n90.70\n57.83\nCoTMR\n22.23\n22.78\n24.68\n25.74\n31.50\n60.80\n73.04\n91.06\n66.61\n84.50\n92.55\n63.71\nViT-L/14\nCaptioning\n✗\n1.65\n1.96\n2.42\n2.71\n4.05\n15.88\n25.69\n49.21\n20.87\n40.60\n60.89\n18.37\nPic2Word\n✗\n8.72\n9.51\n10.64\n11.29\n23.90\n51.70\n65.30\n87.80\n-\n-\n-\n-\nSEARLE\n✗\n11.68\n12.73\n14.33\n15.12\n24.24\n52.48\n66.29\n88.84\n53.76\n75.01\n88.19\n53.12\nCIReVL\n18.57\n19.01\n20.89\n21.80\n24.55\n52.31\n64.92\n86.34\n59.54\n79.88\n89.69\n55.92\nLDRE\n23.35\n24.03\n26.44\n27.50\n26.53\n55.57\n67.54\n88.50\n60.43\n80.31\n89.90\n58.00\nCoTMR\n27.61\n28.22\n30.61\n31.70\n35.02\n64.75\n76.18\n92.51\n69.39\n85.75\n93.33\n67.07\nViT-G/14\nCIReVL\n26.77\n27.59\n29.96\n31.03\n34.65\n64.29\n75.06\n91.66\n67.95\n84.87\n93.21\n66.12\nLDRE\n31.12\n32.24\n34.95\n36.03\n36.15\n66.39\n77.25\n93.95\n68.82\n85.66\n93.76\n67.60\nCoTMR\n32.23\n32.72\n35.60\n36.83\n36.36\n67.52\n77.82\n93.99\n71.19\n86.34\n93.87\n69.36\nTable 2. Comparison with the state-of-the-art methods on CIRCO and CIRR test sets. Avg. indicates the average results of Recall@5\nand Recallsub@1. The best results are in boldface, while the second-best results are underlined.\n4.4. Comparison with Bselines\nFashion-IQ. Table 1 presents the comparative results on the\nFashion-IQ dataset. Based on the results, we have the fol-\nlowing observations: (1) Compared to pseudo-word-based\nmethods such as SEARLE, CoTMR achieves impressive\nperformance across multiple metrics even without any train-\ning. This indicates that generating captions for the target\nimage using LVLM provides semantic information that is\nmore suitable for CLIP’s text encoding than concatenat-\ning modification texts with pseudo-words. (2) Compared\nto LDRE, which also uses large language models to model\nthe modified images, our approach achieves significant im-\nprovements. This is attributed to CoTMR’s superior preser-\nvation of image semantics, more refined reasoning process,\nand finer-grained feature recognition. (3) Across all metrics\nwith different CLIP backbones, CoTMR consistently out-\nperforms all baseline methods. Using ViT-B/32 as an ex-\nample, our method relatively outperforms LDRE by 9.49%\nin average R@10 and 10.91% in average R@50. These re-\nsults strongly support CoTMR’s effectiveness.\nCIRR. When applied to the open-domain dataset CIRR,\nCoTMR still shows compelling results, summarized in the\nright section of Table 2.\nBased on the results, we have\nthe following observations: (1) Notably, the CIRR dataset\nis quite noisy, with minimal correlation between the refer-\nence image and the target image, especially compared to the\nmodification text. Therefore, CoTMR’s ability to capture\nrich information from reference images also means it may\nreceive more distracting information.\nDespite this chal-\nlenge, CoTMR consistently outperforms all baseline met-\nrics across all CLIP architectures. These findings highlight\nthe robustness of our approach, demonstrating its ability\nto deliver significant results even in the presence of noisy\ndata and its adaptability across diverse scenarios. (2) CIRR\nalso provides another evaluation, where the task is to re-\n6\n\n\nBenchmark\nFashionIQ-Avg\nCIRCO\nMetric\nRecall@k\nmAP@k\nMethod\nk=10\nk=50\nk=5\nk=10 k=25 k=50\nA. Multi-Grained Scoring\nA.1 Base\n33.99\n56.34\n26.40 27.98 30.35 31.43\nA.2 Pos + Neg\n30.50\n52.65\n14.92 16.49 18.33 19.12\nA.3 Base + Pos\n35.62\n58.39\n27.54 29.59 32.24 33.26\nA.4 Base + Neg\n34.42\n56.95\n27.28 28.30 30.63 31.82\nA.5 Full\n37.72\n60.92\n28.87 30.61 33.30 34.32\nB. Chain of Thought\nB.1 No COT\n31.03\n51.01\n20.07 21.12 23.43 24.44\nB.2 DDCOT\n29.21\n48.25\n17.41 18.84 21.25 22.17\nB.3 ZS CIRCoT\n33.41\n53.50\n23.54 24.88 27.42 28.40\nB.4 CIRCoT\n33.99\n56.34\n26.40 27.98 30.35 31.43\nC. Scoring for Objects\nC.1 Pos + mean\n35.16\n56.48\n28.57 30.46 33.00 34.14\nC.2 Neg + concat\n37.25\n59.29\n28.71 30.46 33.04 34.06\nC.3 Normal\n37.72\n60.92\n28.87 30.61 33.30 34.32\nD. Scale of LVLM\nD.1 Qwen2-VL-2B\n20.27\n36.68\n4.90\n5.50\n6.15\n6.34\nD.2 Qwen2-VL-7B\n33.35\n54.02\n16.10 17.05 19.45 19.41\nD.3 Qwen2-VL-72B 37.72\n60.92\n28.87 30.61 33.30 34.32\nTable 3. Ablation study results for the proposed components\non Fashion-IQ val set and CIRCO val sets. All experiments are\nperformed with the ViT-G/14 CLIP model.\ntrieve the correct image from six curated samples. In this\nevaluation, our approach also significantly surpasses pre-\nvious methods (Our method outperforms LDRE by 6.08%\nin Recallsub@1 when using ViT-B/32 CLIP). This shows\nthe versatility of our method, enabling it to perform well\nacross different contexts. (3) Using ViT-L/14 as an exam-\nple, compared to our most direct baseline model, CIReVL,\nour method achieves significant improvement in Recall@5\nand Recallsub@1 by 12.44% and 9.85% respectively. This\nfurther demonstrates the effectiveness of our proposed mod-\nules, such as CIRCoT and multi-scale reasoning.\nCIRCO. In the left section of Table 2, we present the com-\npetitive results of CoTMR. Based on the results, we make\nthe following observations: (1) Since CIRCO uses mAP\nas the evaluation metric, the incorrect selection of negative\nsamples has a significant impact on the results. CoTMR, by\nintroducing a negative scoring mechanism, effectively elim-\ninates incorrect samples, achieving optimal performance\nacross multiple metrics. (2) Thanks to multi-scale reasoning\nand the pre-defined subtask decomposition, CoTMR shows\nsubstantial improvements over LLM-based methods such as\nCIReV and LDRE across several metrics. Using ViT-B/32\nas an example, our method relatively outperforms LDRE\nby 4.27% and CIReVL by 7.27% in mAP@5. This further\ndemonstrates the effectiveness of CoTMR for ZS-CIR.\nFigure 4. Ablation study on the value of λ and µ on Fashion-IQ\nval set and CIRR val set. All experiments are performed with the\nViT-B/32 CLIP model.\n5. Ablation Study\nEffects of Multi-Grained Scoring Mechanism. In Table 3\nA, we investigate the impact of multi-scale reasoning and\nmulti-grained scoring mechanism. Our observations are as\nfollows: (1) Compared to using only the base score Sbase\n(A.1), only rely on the object-level reasoning output (A.2\nSpos and Sneg) leads to a significant decline of CoTMR’s\nperformance. This indicates that the logical relationships\nbetween objects captured in the image-level reasoning are\nstill essential for effective retrieval. (2) However, when the\nbase score is combined with either the positive score (A.3)\nor the negative score (A.4), the model’s performance im-\nproves. This further confirms the effectiveness of object-\nlevel reasoning. (3) When both Spos and Sneg are used with\nSbase (A.5), the performance significantly improves com-\npared to using only one of them. This suggests that Spos\nand Sneg have complementary roles in the retrieval process.\nEffects of CIRCoT. In Table 3 B, taking image-level\nreasoning as an example, we compare four different ap-\nproaches for subtask decomposition: without using CoT\n(B.1), autonomously decompose the sub-problem by LVLM\n(B.2, DDCoT [51]), CIRCoT without examples (B.3) and\nour proposed CIRCoT (B.3). The results show that DDCoT\ncaused slight performance degradation compared to not us-\ning CoT, indicating that the reasoning process constructed\nby the model itself may cause confusion in the CIR task. In\ncontrast, when the CIR task is decomposed in advance, the\nmodel’s performance improves significantly, which clearly\nshows the effectiveness of the predefined subtasks. More-\nover, adding step-wise reasoning examples also provides as-\nsistance in helping the model understand task requirements\nand the reasoning process. For more analysis on efficiency,\nplease refer to Appendix 9.\nScoring Methods for Objects: In Table 3 C, we conduct\nexperiment on two different scoring strategies for Spos and\nSneg: (C.1) computing individual scores for each object in\nEO and then averaging them, and (C.2) concatenating all\nobjects in NEO for scoring. Our observations indicate that\nscoring objects in EO individually results in a significant\nperformance decline compared to the full model. We as-\nsume that because of the intrinsic correlations among “exis-\n7\n\n\n“add one more \ndeer and add \nsome sunlight”\n1. Understand the Reference Image:\n   - The image shows a single brown deer standing in a grassy field.\n   \n2. Analyze the Modification Text:\n   - \"Add one more deer\" means adding another deer to the scene.\n   - \"Add some sunlight\" implies enhancing the lighting to give the \nimpression of sunlight.\n3. Apply the Modifications:\n   - Introduce a second deer into the scene.\n   - Enhance the lighting to simulate sunlight.\n   - Keep the background as grassy field.\n4. Generate the Final Description:\n  Two deer are standing in a sunlit grassy field.\n### Describe the Reference Image ###\nObjects: Deer.\nAttributes:\n  - Color: Brown.\n  - Horns: Long and curved.\n  - Background: Green grass field.\n### Understand the Modification Text ###\n1. \"add one more deer\":\n     Add another deer to the image.\n2. \"add some sunlight\":\n     Add sunlight to the image.\n### Apply the Modifications ###\n1. One deer become two deer.\n2. Add more sunlight to the image.\n### Determine the Content of the Target Image ###\n    {\"Existent Object\": [\"Two deer\", \"Brown color\", \"Long \nand curved horns\", \"Green grass field\", \"Sunlight\"],\n    \"Unexist Object\": []}\nObject Scale Reasoning\nImage Scale Reasoning\nReference Image\nTarget Image\nModification Text\nFigure 5. An example of a reasoning process with CIRCoT\nfrom CIRR val set. The LVLM focuses on specific objectives in\neach subtask within CIRCoT and gradually completes the overall\nreasoning goal.\ntent objects”—treating them separately overlooks these de-\npendencies. On the other hand, concatenating the uncor-\nrelated objects in NEO may introduce potential bias that\ncauses the decline in performance, as the overall score may\nbe disproportionately affected by certain individual objects.\nScale of LVLM: As a core component, the performance of\nthe LVLM directly impacts the overall effectiveness. We\nconduct experiments in Table 3 D, using Qwen2-VL mod-\nels of different scales: 2-billion parameter (D.1), 7-billion\nparameter (D.2), and 72-billion parameter (D.3). The re-\nsults show a sharp decline in performance as the parame-\nter size decreases. However, we observed that with the 7B\nmodel, our method achieves relatively satisfactory perfor-\nmance, particularly in tasks like Fashion-IQ, which heavily\ndepend on the reference image. This suggests that the 7B\nmodel can adequately comprehend both the visual content\nof the image and the user’s intent to a certain extent.\nImpact of hyperparameter λ and µ: To analyze the sen-\nsitivity of the hyperparameters in CoTMR, we conduct con-\ntrolled experiments as shown in Figure 4. First, we set µ to\n0 to better demonstrate the effect of λ. As shown in Figure 4\n(a), when the value of λ increases from 0, all four metrics\nshow a rapid rise, stabilizing and slightly declining when λ\nreaches 1. Next, we fix λ at 1 to explore the effect of µ. As\nshown in Figure 4 (b), for the Fashion-IQ dataset, the im-\npact of µ is relatively mild, with the metrics reaching their\npeak at µ = 0.5. However, for the CIRR dataset, due to the\nhigher noise in the data, increasing µ too much leads to a\nsignificant drop in the R@5 metric. Therefore, for CIRR,\nthe best average performance is achieved when µ = 0.3.\n6. Qualitative Results\nReasoning process with CIRCoT: In Figure 5, we illus-\ntrate the reasoning process generated by the LVLM when\nusing CIRCoT at both image and object scale.\nDuring\nimage-scale reasoning, the LVLM analyzes the global con-\ntent of the reference image to ensure comprehensive in-\nformation coverage. By incrementally breaking down the\n“Change the cart to \na white, unmanned \ncarriage in daylight \nwith no horses”\nTarget Image Caption\n“A white, unmanned \ncarriage is moving along a \nstreet in daylight, without \nany horses pulling it.”\nExistent Objects: \n      [“white carriage.”,\n          “daylight.”] \nNonexistent Objects:\n    [“horses.” ,“people.” ]\n“Shows a smaller, \nsimilarly shaped \ndog with lighter \nbrown fur standing \non stone tile path.”\nTarget Image Caption\n“A smaller dog with lighter \nbrown fur is standing on a \nstone tile path.”\nExistent Objects: \n       [“small dog.”,\n   “lighter brown fur.”\n     “stone tile path. ”] \nNonexistent Objects:\n[“white fur.” , “lying \ndown.” ,”brown patches.”]\nReference Image\nModification Text\nReference Image\nModification Text\nFigure 6. Successful retrieval examples with muti-scale reason-\ning from CIRR val set. The ground-truth image is highlighted\nwith the red box. Red underlined text indicates distracting infor-\nmation that causes mistake retrieval, while green italicized text\nrepresents key objects that help in correct retrieval.\nmodification text and executing the modification process,\neach user modification intent is accurately and completely\nexecuted. At object-scale reasoning, the LVLM focuses on\nthe objects and their attributes in the reference image, ac-\ncurately reasoning which objects and attributes should or\nshould not be present by executing the modification process\nstep-by-step. As a result, the LVLM successfully noticed\nthe key object, i.e., ”long and curved horns”. This prede-\nfined structured reasoning process standardizes the model’s\nreasoning path, preventing user modification intents from\nbeing overlooked or incorrectly propagated.\nExamples of successful retrieval.\nFigure 6 visualizes\ncases where the combination of image-scale and object-\nscale reasoning leads to successful retrievals. In the first\nexample, the target image caption includes “without any\nhorses”, which, while meeting user requirements, is detri-\nmental to CLIP retrieval. However, the undesired objects\n(“horses” and “people”) identified through object-scale rea-\nsoning eliminates this interference, successfully retrieving\nthe target image. In the second example, we observe that the\nretrieval results initially overlooked the attribute “stone tile\npath”. Object-scale reasoning, however, highlighted this at-\ntribute with existent objects, leading to the successful re-\ntrieval of the target image. These examples clearly demon-\nstrate that object-scale reasoning can supplement emphasis\nand eliminate distracting information. More examples can\nbe found in Appendix 13.\n7. Conclusion and Future Work\nIn this work, we propose CoTMR, an effective, training-\nfree, and interpretable method for ZS-CIR. It provides\na unified understanding and reasoning framework for\ncomposed queries, utilizing a step-by-step process guided\nby CIRCoT. To incorporate fine-grained details, multi-scale\n8\n\n\nreasoning (alongside a novel scoring mechanism) is devised\nfor multi-grained generation and evaluation.\nExtensive\nexperiments demonstrate the effectiveness of our CoTMR.\nMoreover, our CoTMR also offers appealing interpretabil-\nity for user intervention. However, there are still room for\nimprovement, e.g., designing more suitable CoT modules,\nor exploiting fine-grained CLIP [8] or open-vocabulary\nobject detection models [21, 25] for scoring mecha-\nnism.\nWe leave these directions for future exploration.\nReferences\n[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste-\nfan Lee, and Peter Anderson. Nocaps: Novel object cap-\ntioning at scale. In IEEE/CVF International Conference on\nComputer Vision, pages 8948–8957, 2019. 3\n[2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Al-\nberto Del Bimbo.\nConditioned and composed image re-\ntrieval combining and partially fine-tuning clip-based fea-\ntures.\nIn IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 4959–4968, 2022. 2\n[3] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Al-\nberto Del Bimbo. Zero-shot composed image retrieval with\ntextual inversion. In IEEE/CVF International Conference on\nComputer Vision, pages 15338–15347, 2023. 1, 2, 5, 3\n[4] Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu,\nand Samuel Albanie. Cross modal retrieval with querybank\nnormalisation. In IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5194–5205, 2022. 2\n[5] Yanbei Chen, Shaogang Gong, and Loris Bazzani. Image\nsearch with text feedback by visiolinguistic attention learn-\ning. In IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3001–3011, 2020. 2\n[6] Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and\nYuval Atzmon. “this is my unicorn, fluffy”: Personalizing\nfrozen vision-language representations. In European confer-\nence on computer vision, pages 558–577. Springer, 2022. 5\n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\n2\n[8] Zhiwu Lu Dong Jing, Xiaolong He et al.\nFineclip: Self-\ndistilled region-based clip for better fine-grained understand-\ning. Advances in Neural Information Processing Systems,\n2024. 9\n[9] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,\nJeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. De-\nvise: A deep visual-semantic embedding model. Advances\nin Neural Information Processing Systems, 26:2121–2129,\n2013. 1, 3\n[10] Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li,\nYi Wei, Yi Hu, and Hao Wang. Fashionbert: Text and im-\nage matching with adaptive loss for cross-modal retrieval. In\nSIGIR, pages 2251–2260, 2020.\n[11] Albert Gordo, Jon Almaz´an, Jerome Revaud, and Diane Lar-\nlus. Deep image retrieval: Learning global representations\nfor image search. In European Conference on Computer Vi-\nsion, pages 241–257, 2016. 1, 3\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning. In IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6904–6913, 2017. 3\n[13] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang,\nand Sangdoo Yun. Language-only training of zero-shot com-\nposed image retrieval. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n13225–13234, 2024. 2, 3\n[14] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6700–6709, 2019. 3\n[15] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, et al. Openclip. 2021. 5\n[16] Shyamgopal Karthik, Massimiliano Mancini, and Zeynep\nAkata. Kg-sp: Knowledge guided simple primitives for open\nworld compositional zero-shot learning. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9336–9345, 2022. 2\n[17] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini,\nand Zeynep Akata.\nVision-by-language for training-\nfree\ncompositional\nimage\nretrieval.\narXiv\npreprint\narXiv:2310.09291, 2023. 1, 2, 5, 3\n[18] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models are\nzero-shot reasoners. Advances in Neural Information Pro-\ncessing Systems, 35:22199–22213, 2022. 3\n[19] Seungmin Lee, Dongwan Kim, and Bohyung Han. Cosmo:\nContent-style modulation for image retrieval with text feed-\nback. In IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 802–812, 2021. 2\n[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\npages 12888–12900, 2022. 2\n[21] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10965–\n10975, 2022. 9\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision, pages 740–755.\nSpringer, 2014. 3\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740–755.\nSpringer, 2014. 5\n9\n\n\n[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 2\n[25] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang,\nHang Su, et al.\nGrounding dino:\nMarrying dino with\ngrounded pre-training for open-set object detection. arXiv\npreprint arXiv:2303.05499, 2023. 9\n[26] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xi-\naoou Tang. Deepfashion: Powering robust clothes recogni-\ntion and retrieval with rich annotations. In IEEE Conference\non Computer Vision and Pattern Recognition, pages 1096–\n1104, 2016. 1, 3\n[27] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and\nStephen Gould. Image retrieval on real-life images with pre-\ntrained vision-and-language models. In IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 2125–2134,\n2021. 5\n[28] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In ICDAR, pages 947–952. IEEE,\n2019. 3\n[29] Ishan Misra, Abhinav Gupta, and Martial Hebert. From red\nwine to red tomato: Composition with context. In IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n1792–1801, 2017. 2\n[30] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei\nHerzig. Compositional chain-of-thought prompting for large\nmultimodal models. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14420–14431, 2024.\n3\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in Neural Information Processing Systems, 32, 2019.\n5\n[32] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What\ndoes a platypus look like? generating customized prompts\nfor zero-shot image classification.\nIn IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 15691–15701,\n2023. 2\n[33] Shengsheng Qian, Dizhan Xue, Quan Fang, and Changsheng\nXu. Adaptive label-aware graph convolutional networks for\ncross-modal retrieval. IEEE Transactions on Multimedia, 24:\n3520–3532, 2021. 2\n[34] Shengsheng Qian, Dizhan Xue, Quan Fang, and Changsheng\nXu.\nIntegrating multi-label contrastive learning with dual\nadversarial graph neural networks for cross-modal retrieval.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 45(4):4794–4811, 2022. 2\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In ICML, pages 8748–8763, 2021. 2\n[36] Karsten Roth, Oriol Vinyals, and Zeynep Akata. Integrating\nlanguage guidance into vision-based deep metric learning.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16177–16189, 2022. 2\n[37] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learn-\ning to retrieve prompts for in-context learning. arXiv preprint\narXiv:2112.08633, 2021. 3\n[38] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li,\nChen-Yu Lee, Kate Saenko, and Tomas Pfister. Pic2word:\nMapping pictures to words for zero-shot composed image\nretrieval. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 19305–19314, 2023. 1, 2, 5, 3\n[39] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In European Conference\non Computer Vision, pages 742–758. Springer, 2020. 3\n[40] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez,\nDongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu\nZhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or\nnot to cot? chain-of-thought helps mainly on math and sym-\nbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. 3,\n4\n[41] Shitong Sun, Fanghua Ye, and Shaogang Gong. Training-\nfree zero-shot composed image retrieval with local concept\nreranking. arXiv preprint arXiv:2312.08924, 2023. 2, 3\n[42] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang\nXiong, Yue Hu, and Qi Wu. Context-i2w: Mapping images\nto context-dependent words for accurate zero-shot composed\nimage retrieval.\nIn AAAI Conference on Artificial Intelli-\ngence, pages 5180–5188, 2024. 1\n[43] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie.\nSus-x: Training-free name-only transfer of vision-language\nmodels.\nIn IEEE/CVF International Conference on Com-\nputer Vision, pages 2725–2736, 2023. 2\n[44] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li\nFei-Fei, and James Hays.\nComposing text and image for\nimage retrieval-an empirical odyssey.\nIn IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n6439–6448, 2019. 2\n[45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 2, 5\n[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824–24837, 2022. 2, 3, 4\n[47] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven\nRennie, Kristen Grauman, and Rogerio Feris. Fashion iq:\nA new dataset towards retrieving images by natural language\nfeedback. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11307–11317, 2021. 5\n[48] Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming\nDong, and Changsheng Xu.\nLdre: Llm-based divergent\nreasoning and ensemble for zero-shot composed image re-\ntrieval. In SIGIR, pages 80–90, 2024. 1, 2, 5, 3\n10\n\n\n[49] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen.\nTowards better chain-of-thought prompting strategies: A sur-\nvey. arXiv preprint arXiv:2310.04959, 2023. 3\n[50] Zhuosheng Zhang,\nAston Zhang,\nMu Li,\nHai Zhao,\nGeorge Karypis, and Alex Smola.\nMultimodal chain-of-\nthought reasoning in language models.\narXiv preprint\narXiv:2302.00923, 2023. 3\n[51] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei\nYang. Ddcot: Duty-distinct chain-of-thought prompting for\nmultimodal reasoning in language models. Advances in Neu-\nral Information Processing Systems, 36:5168–5191, 2023. 2,\n3, 4, 7, 1\n11\n\n\nCoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free\nZero-Shot Composed Image Retrieval\nSupplementary Material\nYou are provided with two inputs:\nReference Image: The image that will be modified. \nModification Text: Instructions that specify changes to be applied to the reference image.\nYour goal is to:\n1. Infer the objects and attributes that should appear in the target image, based on the \nreference image and modification text.\n2. Infer the objects and attributes that should not appear in the target image, based on the \nchanges described in the modification text.\n3. Attribute assignment: Where attribute changes are described, clearly associate them \nwith the relevant objects (e.g., color change of a shirt).\nTo complete the task accurately, please follow these steps:\n### Describe the Reference Image ###\nList the objects and their attributes present in the reference image step-by-step .\n### Understand the Modification Instructions ###\nAnalyze modification instruction step-by-step to identify changes to objects and attributes, \nincluding additions, deletions, or modifications.\n### Apply the Modifications ###\n1. Update the objects and attributes from the reference image according to the modification \ninstructions to obtain the expected content of the target image.\n2. Please complete this task step by step.\n### Determine the Content of the Target Image ###\nExistent Object (Objects and Attributes that Must Exist):\n1. List the objects and attributes that must be present in the target image.\n2. Be specific, especially if attributes are provided in the modification text.\nNonexistent Object (Objects and Attributes that Must Not Exist):\n1. List the objects and attributes that must not be present in the target image.\n2. Include any objects or attributes explicitly removed or modified to no longer exist.\nHere are some examples:\nExample 1: .......             Example 2: .......\nCIRCoT — 𝑷𝑶𝒃𝒋\nFigure 7.\nIllustration of CIRCoT in object-scale reasoning\n(PO), which includes four predefined subtasks and allows LVLM\nto reason step-by-step within each subtasks.\n8. CIRCoT in Object-Scale Reasoning\nIn Figure 7, we show the details of CIRCoT used in object-\nscale reasoning (PO). Similar to image-scale reasoning, we\ndivide the task of generating “existent objects” and “nonex-\nistent objects” into four subtasks: (1) Describe the Refer-\nence Image; (2) Understand the Modification Instructions;\n(3) Apply the Modifications and (4) Determine the Content\nof the Target Image. These four fundamental tasks serve to\nstructure the overall reasoning process of the LVLM. For\neach subtask, we allow the model to reason step-by-step.\nAdditionally, we add several reasoning examples to further\nstimulate the model’s reasoning capabilities.\n9. Efficiency Analysis\nWe conducted a comprehensive timing analysis to mea-\nsure the average computational overhead for processing one\ncomposed query at the image scale under three different\nconfigurations.\nThe results show that the baseline with-\nMake the dog older and \nhave two birds next to him \nand make everything look \nlike a painting.\n### Understand the Modification Text ###\n1. \"Make the dog older\":\n    Modify the dog's age from young to old.\n2. \"Have two birds next to him\":\n    Add two birds to the right of the dog.\n3. \"Make everything look like a painting\":\n    Modify the overall appearance of the image to \nresemble a painting.\n### Apply the Modifications ###\n1. I will make the dog older.\n2. I will add two birds to the right of the dog.\n3. I will modifiy the image to look like a painting.\n4. I will keep the dog sitting on a red surface.\n### Generate the Final Description ###\n“An older white dog sits on a red surface with two \nbirds on the right, all depicted in a painted style .”\nRetrieved Image:\nModifications:\n1. The birds are on the left of the dog.\n2. No need to keep the red surface. \n### Apply the Modifications ###\n1. I will make the dog older.\n2. I will add two birds to the left of the dog.\n3. I will modifiy the image to look like a painting.\n### Generate the Final Description ###\n“There is an older white dog and two birds on it’s \nleft, all depicted in a painted style.”\nRetrieved Image:\nThat's exactly what I want!\nFigure 8. The demonstration of making user interventions to\nenhance ZS-CIR performance with CoTMR. For instance, by\nfixing the mistakes in the reasoning process, users are able to cor-\nrectly retrieve the desired image with further dialogue.\nout CoT requires 0.86s, while implementations with DD-\nCoT [51] and CIRCoT take 3.145s and 3.183s, respectively.\nWe have the following observations: (1) While CIRCoT\nintroduces additional computational overhead compared to\nthe non-CoT baseline, this trade-off is justified by the sub-\nstantial performance improvements it delivers.\n(2) No-\ntably, CIRCoT achieves significantly enhanced model per-\nformance while maintaining comparable computational ef-\nficiency to DDCoT, with only a marginal increase in pro-\ncessing time. Future research directions could focus on op-\ntimizing CIRCoT’s computational efficiency while preserv-\ning its superior performance characteristics.\n10. Example of User Interventions\nCIRCoT enables a highly transparent and interpretable rea-\nsoning process, which facilitates error tracking and correc-\ntion through user intervention when necessary. We demon-\nstrate this capability through illustrative cases in Figure 8,\nwhere initial reasoning processes led to suboptimal retrieval\nresults. The structured nature of our reasoning framework\nallows users to precisely identify problematic reasoning\nsteps and initiate corrective interactions with the LVLM.\nAs illustrated in Figure 8, we present instances where users\nsuccessfully identified and addressed two reasoning errors:\nthe bird is to the right of the dog” and red surface”. Through\nsubsequent dialogue-based refinement, the model’s retrieval\n1\n\n\nMethod\nShirt\nDress\nTops&Tee\nAvg.\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nRmean\nOne process\nimage-scale\n29.44\n47.11\n26.82\n48.79\n32.33\n54.77\n29.53\n50.22\n39.87\nmulti-scale\n32.14\n50.25\n30.04\n51.81\n35.34\n59.10\n32.50\n53.72\n43.11\nTwo processes\nimage-scale\n30.03\n48.58\n26.57\n48.69\n34.12\n56.35\n30.24\n51.21\n40.72\nmulti-scale\n33.42\n53.93\n31.09\n54.54\n38.40\n61.14\n34.30\n56.54\n45.42\nTable 4. Ablation study on the impact of process quantity in Multi-Scale Reasoning on Fashion-IQ dataset. All experiments are\nperformed with the ViT-B/32 CLIP model.\nRecall@k\nRecallsub@k\nMethod\nk=1\nk=5\nk=10\nk=50\nk=1\nk=2\nk=3\nAvg.\nimage-scale\n30.76\n59.01\n70.75\n90.34\n66.08\n83.74\n91.96\n62.54\nOne process\nmulti-scale\n29.56\n58.69\n70.27\n89.72\n65.61\n83.52\n91.68\n62.15\nimage-scale\n30.11\n58.10\n70.58\n89.95\n65.08\n83.07\n91.41\n61.59\nTwo processes\nmulti-scale\n31.88\n61.27\n72.90\n91.03\n67.85\n85.00\n92.68\n64.56\nTable 5. Ablation study on the impact of process quantity in Multi-Scale Reasoning on CIRR val dataset. All experiments are\nperformed with the ViT-B/32 CLIP model.\naccuracy was effectively improved, highlighting the practi-\ncal value of our interpretable reasoning approach.\n11. Ablation Study on Multi-Scale Reasoning\nTables 4 and 5 present a comparative analysis of single-\nprocess versus dual-process approaches in the multi-scale\nreasoning module, evaluated on the Fashion-IQ and CIRR\ndatasets using the ViT-B/32 CLIP model. “One process”\nrefers to generating all three responses with LVLM simul-\ntaneously in a single inference pass. “Two processes” rep-\nresents our default methodology, which conducts reason-\ning separately at different scales through independent infer-\nence processes. Analysis of Table 4 reveals that employ-\ning a single process not only compromises the effective-\nness of image-scale reasoning but also diminishes the per-\nformance gains typically achieved through object-scale rea-\nsoning integration. In the more challenging CIRR dataset,\nas shown in Table 5, while concurrent reasoning of target\nimage caption and key objects enhances image-scale rea-\nsoning accuracy, the incorporation of object-scale reasoning\nresults yields a marginal performance degradation. We at-\ntribute these observations to two primary factors: (1) The\nutilization of identical reasoning logic across both scales\npotentially limits the semantic richness of the reasoning out-\ncomes. (2) Qwen2-VL’s current capabilities in managing\nmultiple concurrent tasks may be insufficient, where the in-\ncreased cognitive load adversely affects the precision of the\nresults.\n12. Qualitative Comparison of DDCoT and\nCIRCoT\nIn Figure 9, we compare the reasoning processes of DDCoT\nand CIRCoT. It can be observed that when using DDCoT,\nthe definition of subtasks is entirely left to the LVLM, which\nDDCoT\n### Sub-questions ### \n1. What is the current attire of the person in the \nreference image?\n2. How can we make the person look like they are \nwearing jeans?\n3. How can we add a ruffled skirt to the outfit?\n### Step-by-step reasoning process ###\n1. The person in the reference image is currently \nwearing a white dress with a black ruffled collar.\n2. To make the person look like they are wearing jeans, \nwe need to add a pair of jeans over the dress.\n3. To add a ruffled skirt, we need to replace the lower \npart of the dress with a ruffled skirt.\n###  Updated Image Description  ###\nThe person is now wearing a pair of jeans over a white \ndress with a black ruffled collar. The lower part of the \ndress has been replaced with a ruffled skirt.\n“looks like jeans and \nhas a ruffled skirt”\nCIRCoT (Ours)\n### Understand the Reference Image ###\n   - The reference image shows a woman wearing a \nwhite dress with a ruffled neckline and sheer sleeves.\n   \n### Analyze the Modification Text ### \n   - \"Looks like jeans\": Change the material or \ncolor of the dress to a denim style.\n   - \"Has a ruffled skirt\": Add or modify to have a \nruffled hem.\n### Apply the Modifications ### \n   - Change the material of the dress to resemble denim.\n   - Modify the hem to have a ruffled design.\n   - Keep the ruffled neckline.\n### Generate the Final Description ### \n The woman is wearing a denim-like dress with a \nruffled neckline,  and a ruffled skirt.\nReference Image\nModification Text\nTarget Image\nFigure 9. Comparison between DDCoT and CIRCoT prompt-\ning strategies.\nsometimes results in confusing subtasks, such as “How can\nwe make the person look like they are wearing jeans?” in\nFigure 9. Such subtasks can mislead the LVLM into provid-\ning incorrect answers, as seen in Figure 9, where the LVLM\ndecides on “a pair of jeans” to correspond to the modifi-\ncation text’s requirement of “looks like jeans”. This exam-\nple highlights the critical importance of subtask definition;\ninappropriate subtasks can lead to erroneous logical reason-\ning and cause confusion. In contrast, our proposed CIRCoT,\nwith its four predefined subtasks, offers a stable and correct\nreasoning process, leading to more accurate outputs.\n13. More Qualitative Examples\nFigure 10 visualizes more cases where the combination of\nimage-scale and object-scale reasoning leads to successful\n2\n\n\nBenchmark\nCIRCO\nCIRR\nMetric\nmAP@k\nRecall@k\nRecallsub@k\nAvg.\nBackbone\nMethod\nTraining-free\nk=5\nk=10\nk=25\nk=50\nk=1\nk=5\nk=10\nk=50\nk=1\nk=2\nk=3\nViT-L/14\nLinCIR\n✗\n12.62\n13.40\n14.81\n15.69\n25.08\n53.63\n67.30\n88.72\n56.36\n76.96\n88.57\n54.99\nCoTMR\n27.61\n28.22\n30.61\n31.70\n35.02\n64.75\n76.18\n92.51\n69.39\n85.75\n93.33\n67.07\nViT-G/14\nLinCIR\n✗\n19.71\n20.79\n22.99\n24.00\n35.34\n65.08\n76.28\n93.22\n63.73\n82.62\n92.12\n64.41\nCoTMR\n31.73\n32.72\n35.30\n36.43\n36.36\n66.92\n77.82\n93.99\n70.69\n86.34\n93.87\n68.81\nTable 6. Comparison with the LinCIR [13] on CIRCO and CIRR test sets. Avg. indicates the average results of Recall@5 and\nRecallsub@1. We reproduce the results of LinCIR. The best results are in boldface, while the second-best results are underlined.\nBackbone\nMethod\nTraining-free\nShirt\nDress\nTops&Tee\nAvg.\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nRmean\nViT-L/14\nLinCIR\n✗\n28.81\n46.61\n20.72\n41.7\n29.07\n49.77\n26.21\n46.03\n36.12\nCoTMR\n35.43\n54.91\n31.18\n55.04\n38.55\n61.33\n35.05\n57.09\n46.50\nViT-G/14\nLinCIR\n✗\n44.08\n62.56\n38.48\n60.63\n48.58\n69.10\n43.71\n64.10\n53.91\nCoTMR\n38.32\n62.24\n33.96\n56.22\n40.90\n64.30\n37.72\n60.92\n49.32\nCoTMR + LinCIR\n✗\n46.24\n67.95\n40.06\n63.11\n49.59\n68.03\n45.13\n66.36\n55.74\nTable 7. Comparison with LinCIR [13] on the Fashion-IQ dataset. Rmean indicates the average results across all the metrics. We\nreproduce the results of LinCIR. The best results are in boldface, while the second-best results are underlined.\nretrievals on both Fashion-IQ and CIRR datasets. (1) In the\nfirst example, the top two images retrieved using the target\nimage caption overlooked the semantics of “Asian-inspired\ndesign”. However, after emphasizing this part with existent\nobjects, the target image was successfully ranked first. (2)\nIn the second example, the target image caption contained\ndistracting information (religious message and cross de-\nsign), causing the top three images to include some religious\nelements. By incorporating nonexistent objects, our model\nsuccessfully reduced the impact of this distracting informa-\ntion. (3) In the third example, the reference image had very\nlittle relevance to the target image, meaning the model could\neasily be misled by distracting information from the refer-\nence image, such as “in a basket”. Object-scale reasoning\ncan reduce such distractions because it doesn’t need to con-\nsider the logical relationships between objects. Thus, our\nmodel successfully categorized “Blue basket” as a nonexis-\ntent object. (4) In the fourth example, the target image cap-\ntion was similarly affected by the “Pepsi logos” in the refer-\nence image. Our model mitigated the distracting influence\nby emphasizing “Green bottles” and successfully ranked the\ntarget image first.\n14. Comparison and Integration with LinCIR\nCurrently, there are two mainstream approaches in ZS-\nCIR: pseudo-token-based methods [3, 13, 38] and textual\ncaption-based methods (including LLM-based [17, 41, 48]\nmethods and LVLM-based methods, such as our CoTMR).\nIn this section, we compare CoTMR with LinCIR [13],\na state-of-the-art pseudo-token-based method, and prelim-\ninarily explore the potential for their collaboration.\nBased on the results shown in Tables 7 and 6, we ob-\nserve that: (1) With ViT-L/14 CLIP, CoTMR achieves sig-\nnificant performance gains over LinCIR across all metrics\non three datasets, demonstrating remarkable improvements\nsuch as a 12.08% increase in Avg. on CIRR and a 10.38%\nincrease in Rmean on Fashion-IQ. These results strongly\nvalidate the effectiveness of CoTMR and suggest that tex-\ntual descriptions are more compatible with smaller-scale\nCLIP models compared to pseudo-tokens. (2) When em-\nploying ViT-G/14 CLIP, CoTMR significantly outperforms\nLinCIR on both CIRCO and CIRR datasets, achieving no-\ntable improvements such as a 12.02% increase in mAP@5\non CIRCO. However, we observe that LinCIR shows con-\nsiderable advantages on the Fashion-IQ dataset. This obser-\nvation indicates that pseudo-token-based methods may be\nmore sensitive to specific CLIP versions or training datasets,\nwhile our approach demonstrates more consistent improve-\nments across different CLIP versions.\nFurthermore, we conduct a preliminary investigation into\nthe potential of combining pseudo-token-based and textual\ncaption-based methods to achieve superior performance.\nSpecifically, we first convert the reference image into a\npseudo-token following LinCIR. Then, for the multi-scale\nreasoning outputs in CoTMR, we concatenated the pseudo-\ntoken with both the “target image caption” and the combi-\nnation of “existent objects”, while maintaining the “nonex-\nistent objects” unchanged. We then retrieved target images\nusing the same scoring mechanism as CoTMR. As shown\nin the last row of Table 7, CoTMR with additional pseudo-\ntokens achieves substantial improvements (e.g., a 6.42% in-\ncrease in Rmean). Similarly, the multi-grained descriptions\ngenerated through CoTMR’s reasoning process help en-\nhance LinCIR’s performance (e.g., a 1.83% improvement\nin Rmean). This experiment suggests a promising direction\nfor ZS-CIR research: optimizing the text that concatenated\nwith pseudo-tokens with LVLM. We leave this exploration\nfor future work.\n3\n\n\n“Show the brown \ndogs with water and \nsnow behind them.”\nTarget Image caption\n“Brown dogs are in a \nbasket with water and \nsnow behind them.”\nExistent Objects: \n      [Brown dogs,\n          Water, Snow. ] \nNonexistent Objects:\n[Crabs , Blue basket ，    \n            Netting.  ]\nReference Image\nModification Text\n“the bottles are on \na table, and they \nare green.”\nTarget Image caption\n“Several green glass \nbottles with Pepsi logos \nare placed on a table.”\nExistent Objects: \n      [Green bottles,\n             Table.] \nNonexistent Objects:\n     [Clear glass bottles. ]\nReference Image\nModification Text\n“Is longer and more \nAsian-inspired and \nshiny black.”\nTarget Image caption\n“The woman is wearing a \nlong, shiny black dress \nwith an Asian-inspired \ndesign and short sleeves.”\nExistent Objects: \n[Long dress, Shiny black,\n Asian-inspired elements.] \nNonexistent Objects:\n[Purple dress , Knee-length]\nReference Image\nModification Text\n“is white colored \nand is less religious \nand more humorous.”\nTarget Image caption\n“A white t-shirt with a \nhumorous graphic or text, \nreplacing the original \nreligious message and \ncross design.”\nExistent Objects: \n          [White t-shirt, \n        Humorous design.] \nNonexistent Objects:\n[Gray t-shirt, cross design,\n         Religious design.]\nReference Image\nModification Text\nFigure 10. Successful retrieval examples with muti-scale reasoning from Fashion-IQ and CIRR val set. The ground-truth image is\nhighlighted with the red box. Red underlined text indicates distracting information that causes mistake retrieval, while green italicized text\nrepresents key objects that help in correct retrieval.\n4\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20826v1.pdf",
    "total_pages": 15,
    "title": "CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image Retrieval",
    "authors": [
      "Zelong Sun",
      "Dong Jing",
      "Zhiwu Lu"
    ],
    "abstract": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by\nintegrating information from a composed query (reference image and modification\ntext) without training samples. Existing methods primarily combine caption\nmodels and large language models (LLMs) to generate target captions based on\ncomposed queries but face various issues such as incompatibility, visual\ninformation loss, and insufficient reasoning. In this work, we propose CoTMR, a\ntraining-free framework crafted for ZS-CIR with novel Chain-of-thought (CoT)\nand Multi-scale Reasoning. Instead of relying on caption models for modality\ntransformation, CoTMR employs the Large Vision-Language Model (LVLM) to achieve\nunified understanding and reasoning for composed queries. To enhance the\nreasoning reliability, we devise CIRCoT, which guides the LVLM through a\nstep-by-step inference process using predefined subtasks. Considering that\nexisting approaches focus solely on global-level reasoning, our CoTMR\nincorporates multi-scale reasoning to achieve more comprehensive inference via\nfine-grained predictions about the presence or absence of key elements at the\nobject scale. Further, we design a Multi-Grained Scoring (MGS) mechanism, which\nintegrates CLIP similarity scores of the above reasoning outputs with candidate\nimages to realize precise retrieval. Extensive experiments demonstrate that our\nCoTMR not only drastically outperforms previous methods across four prominent\nbenchmarks but also offers appealing interpretability.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}