{
  "id": "arxiv_2502.20723v1",
  "text": "Variational Transformer Ansatz for the Density Operator of Steady States in Dissipative Quantum\nMany-Body Systems\nLu Wei,1, 2 Zhian Jia ,3, 4, ∗Yufeng Wang ,5 Dagomir Kaszlikowski,3, 4 and Haibin Ling5, 2, †\n1Department of Applied Mathematics & Statistics, Stony Brook University, Stony Brook, NY 11794, USA\n2Program of Data Science, Stony Brook University, Stony Brook, NY 11794, USA\n3Centre for Quantum Technologies, National University of Singapore, SG 117543, Singapore\n4Department of Physics, National University of Singapore, SG 117543, Singapore\n5Department of Computer Science, Stony Brook University, Stony Brook, NY 11794, USA\nThe transformer architecture, known for capturing long-range dependencies and intricate patterns, has ex-\ntended beyond natural language processing. Recently, it has attracted significant attention in quantum infor-\nmation and condensed matter physics. In this work, we propose the transformer density operator ansatz for\ndetermining the steady states of dissipative quantum many-body systems. By vectorizing the density operator\nas a many-body state in a doubled Hilbert space, the transformer encodes the amplitude and phase of the state’s\ncoefficients, with its parameters serving as variational variables. Our design preserves translation invariance\nwhile leveraging attention mechanisms to capture diverse long-range correlations. We demonstrate the effec-\ntiveness of our approach by numerically calculating the steady states of dissipative Ising and Heisenberg spin\nchain models, showing that our method achieves excellent accuracy in predicting steady states.\nIntroduction. — The investigation of open quantum sys-\ntems has experienced a surge in interest in recent years. From\na fundamental perspective, despite significant experimental\nstrides in isolating quantum systems, a finite coupling to the\nenvironment is unavoidable, imparting dynamic characteris-\ntics that encompass a diverse range of features not observed\nin equilibrium systems [1, 2]. In practical terms, these sys-\ntems offer a platform for employing controlled dissipation\nchannels to engineer captivating quantum states as the station-\nary outcome of their dynamics, thus holding potential appli-\ncations in quantum information tasks [3–5]. Diverging from\nclosed quantum systems, where a wave function is commonly\nused to represent the quantum state, the focus of study in\nopen quantum systems shifts to the density operator ρ. Effec-\ntively describing interacting open quantum many-body sys-\ntems presents a significant challenge for both theoretical and\nnumerical approaches [2].\nThe evolution of an open quantum system is governed by\nthe master equation, and several methods have been devel-\noped to solve it in recent years. These include analytic ap-\nproaches based on the Keldysh formalism [6], tensor net-\nwork techniques such as the density matrix renormalization\ngroup and matrix product operator methods [7–12], the clus-\nter mean-field approach [13], phase space methods [14], and\ncorner-space renormalization [15], among others.\nVariational methods are fundamental in the study of quan-\ntum many-body systems, offering deep insights into the prop-\nerties of highly complex physical systems. Neural networks\nhave the capacity to efficiently extract hidden patterns from\nlarge datasets [16, 17]. In recent years, neural network-based\nvariational ansatz states have garnered significant attention for\nsolving quantum problems, see e.g. [18–24]. The most well-\nstudied examples are Restricted Boltzmann Machine (RBM)\nstates [25]. Beyond RBMs, other architectures, such as deep\n∗giannjia@foxmail.com\n† hling@cs.stonybrook.edu\nBoltzmann machines, convolutional neural networks (CNN),\nand feedforward neural networks have also been employed to\nconstruct neural network ansatz states. Many of these neural\nnetwork ansatz methods have been extended to open quantum\nsystems [26–32], where density operators are encoded into\nneural networks.\nThe transformer architecture has recently gained significant\nattention due to its success in natural language processing\ntasks [33]. It has also been successfully applied to many-body\nproblems in closed quantum systems [34–36]. However, its\napplication as an ansatz for solving open quantum systems re-\nmains relatively unexplored.\nIn Ref. [14], the quantum state is mapped to a probability\ndistribution in phase space, and its evolution is reformulated\nas a probabilistic equation, enabling the transformer to sim-\nulate open quantum system dynamics. In this work, we in-\nTransformer\nVariantional\nmanifold\nρ\nDissipation\nFIG. 1: Illustration of a dissipative spin chain with periodic\nboundary condition and its transformer representation of den-\nsity operator. The dissipative rate γ describes the strength of\nthe coupling to the environment, which leads to decoherence\nand information loss in the system.\narXiv:2502.20723v1  [quant-ph]  28 Feb 2025\n\n\n2\ntroduce the transformer density operator ansatz, based on the\nvectorization of the density operator—an approach that has\nrecently gained attention in studies of open-system quantum\nphases, weak and strong symmetries, tenfold classification,\nand related topics (see, e.g., [27, 37–40]). We employ this\nansatz variationally to solve for the steady state of dissipative\nquantum systems. As we will demonstrate using the dissipa-\ntive spin chain model, this approach can efficiently capture the\nsteady state with high precision.\nTransformer density operator ansatz. — Consider an N-\nparticle system with the Hilbert space H spanned by the basis\nstates |ααα⟩, where ααα = (α1,...,αN) labels the states of the N\ndegrees of freedom composing the system. For example, in a\nqubit system, αi take values in {0,1}. For a density operator\nρ ∈B(H) (where B(H) denotes the space of all linear oper-\nators), which is a positive semidefinite, trace-one and Hermi-\ntian operator, we can express its matrix elements as ρ(ααα,βββ) =\n⟨ααα|ρ|βββ⟩in the basis of |ααα⟩and |βββ⟩. By vectorizing, ρ can be\ntransformed into a vector |ρ⟩⟩= ∑ααα,βββ ρ(ααα,βββ)|ααα⟩|βββ⟩in the\ndoubled Hilbert space H⊗H (see supplementary material for\nfurther details). In order to construct a variational transformer\nrepresentation of the density operator, we express the vector-\nized density operator as\n|ρθθθ(J)⟩⟩= ∑\nααα,βββ\nρθθθ(ααα,βββ,J)|ααα⟩|βββ⟩,\n(1)\nwhere ρθθθ(ααα,βββ,J) is the density operator’s complex ampli-\ntude corresponding to the configuration of (ααα,βββ). The varia-\ntional parameters θθθ define the model, and J denotes the phys-\nical parameters of the open quantum system, which will be\nignored for simplicity in the following description.\nOur transformer density operator ansatz is mainly param-\neterized by incorporating convolutional layers for local fea-\nture extraction and the transformer block with a self-attention\nmechanism to capture long-range correlations within the den-\nsity matrix structure.\nSpecifically, our ansatz is entirely\nparametrized by real-valued parameters, and the final com-\nplex output is obtained by combining two real-valued outputs\nthat represent its real and imaginary components. For illustra-\ntion in Fig. 2, we set the batch number to 1 in the schematic,\nwith additional details provided in Sec. II of the supplemen-\ntary material.\nOur goal is to compute ρθθθ(ααα,βββ) for each configuration\n(ααα,βββ). This requires sampling from the configuration space,\nas detailed in Sec. III A of the supplementary material. Below,\nwe provide a step-by-step discussion on obtaining the steady\nstate.\nThe sampled input (ααα,βββ) is first reshaped and passed\nthrough two convolutional layers that serve as a feature en-\ncoding stage. In this stage, the input configuration is con-\nvolved with a bank of learnable convolutional filters, each\nwith a kernel size of two-by-one, to encode local feature em-\nbeddings. To preserve the periodic boundary conditions of the\nsystem, circular padding is applied in the convolutional layers,\nensuring that the first and last sites are treated equivalently.\nEach convolutional operation is followed by a nonlinear acti-\nvation function. This process gives us a set of feature vectors\n{xxx1,...,xxxi,...,xxxN}, where xxxi represents an embedded feature\nfor the i-th spin.\nSubsequently, these local feature embeddings are processed\nby a transformer encoder block [33] with the self-attention\nmodule to capture long-range correlations that can emerge\nglobally in a strongly interacting Ising chain. We introduce\nthree learnable matrices Q, K, and V, each of which trans-\nforms an input feature vector into a corresponding query, key,\nor value representation. Concretely, for any feature vector xxxi,\nwe define\nqqqi = Qxxxi,\nkkki = K xxxi,\nvvvi = V xxxi,\n(2)\nwhere Q, K, and V share the same shape but are learned\nindependently to capture different aspects of the input fea-\ntures. The attention mechanism allows each site to attend to\nall other sites by computing the learned attention weights us-\ning a scaled dot product, followed by a softmax operation\nω(qqqi,kkk j) =\nexp\n\u0010 ⟨qqqi,kkk j⟩\n√\nd\n\u0011\n∑N\nj=1 exp\n\u0010 ⟨qqqi,kkk j⟩\n√\nd\n\u0011,\n(3)\nwhere d is the dimension of the query, key, and value vectors,\nqqqi,kkk j,vvvj ∈Rd. Dividing by\n√\nd keeps the dot-product within a\nmore stable numeric range, preventing large vector sizes from\ncausing the exponential function to overflow. The attention\nweights ω measure how much the j-th input should contribute\nto the i-th context vector. Using these attention weights, the\ncontext vector for each site is constructed as\naaai =\nN\n∑\nj=1\nω(qqqi,kkk j)vvvj.\n(4)\nThe context vectors {aaa1,...,aaaN} encode global correlations\nacross the entire system. The output context vectors aaa1,...,aaaN\nare computed in parallel and added with feature vectors\n{xxx1,...,xxxN}. To further enhance the model’s capacity to cap-\nture diverse interactions, the self-attention mechanism can be\nextended to multi-head attention.\nIn this setting, the fea-\nture channels are split into m heads, with independent sets\nof query, key, and value matrices Qµ, Kµ, and V µ (for µ =\n1,...,m) applied to each head; the outputs from all heads are\nthen concatenated to form the final representation.\nThe attention mechanism enables the network to capture\narbitrary pairwise relationships, which is particularly benefi-\ncial in open quantum systems where dissipation and quantum\ncoherence can induce correlations of long-range neighbors.\nWhat’s more, with the multi-head attention, the ansatz may\nexhibit multiple distinct correlation stereotypes since differ-\nent heads can specialize in capturing different scales of corre-\nlation.\nTo ensure translation invariance in the final output, which is\ncrucial for homogeneous spin systems under periodic bound-\nary condition, we average over the positions in the chain\nhhh = 1\nN ∑N\ni=1 (aaai +xxxi). This eliminates explicit dependence on\nsite indices and dramatically reduces the number of free pa-\nrameters in the subsequent layer.\n\n\n3\nFIG. 2: Schematic representation of the transformer density operator ansatz for the steady-state density operator of an open\nquantum spin chain with periodic boundary condition. The input spin configurations (ααα,βββ) are split into left and right compo-\nnents and then stacked and reshaped to form the input to two convolutional layers with circular padding, which embed the local\nfeatures. A self-attention block then captures long-range dependencies by allowing each spin site to attend to all others through\nlearned attention weights. Global average pooling ensures translation invariance, followed by a fully connected layer that maps\nthe spatially averaged vectors into the real and imaginary parts of a complex output. The last step symmetrizes this output to\nenforce the Hermiticity of the density operator. For the experiments on both Ising and Heisenberg chains presented in this paper,\nwe employ the exact same architecture.\nThe mean-pooled vector hhh is then fed into a fully-connected\nlayer that produces two real-valued outputs, which correspond\nto the real part and the imaginary part of a complex number\nz(ααα,βββ). To ensure Hermiticity, the final representation of the\nsteady-state density matrix element ρθθθ(ααα,βββ) is obtained by\nsymmetrizing the previous complex output:\nρθθθ(ααα,βββ) = log\n\u0010\nexp\n\u0002\nz(ααα,βββ)\n\u0003\n+exp\n\u0002\nz(βββ,ααα)\n\u0003∗\u0011\n.\n(5)\nThis transformation ensures that the resulting quantity satis-\nfies ρθθθ(ααα,βββ) = ρθθθ(βββ,ααα)∗and also maintains numerical sta-\nbility. However, positive semidefiniteness of the density is\nnot explicitly guaranteed and is instead learned through opti-\nmization as described in [41]. Now, for a configuration pair\n(ααα,βββ), we are able to give the corresponding complex ampli-\ntude ρθθθ(ααα,βββ) of its steady state in Eq. (1) through the trans-\nformer density operator ansatz.\nTo summarize, our transformer density operator ansatz uses\nconvolutional filters with circular padding for local encoding\nand multi-head self-attention to capture long-range correla-\ntions. Global pooling enforces translation invariance, and a\nsubsequent symmetrization ensures Hermiticity. By parame-\nterizing the complex amplitude ρθ(ααα,βββ) in Eq. (1) with trans-\nformer density operator ansatz, we are able to maintain the\nessential properties of Hermiticity and approximate positivity.\nThe self-attention mechanism enables the ansatz to effectively\ncapture intricate correlation patterns inherent in open quantum\nsystems and scale efficiently to larger spin chains.\nVariational algorithm for searching steady state based on\ntransformer density operator ansatz. — Consider a quantum\nsystem HS with dimHS = d. When coupled to a Markovian\nenvironment HE, the evolution equation of the system takes\nthe form of the Gorini–Kossakowski–Sudarshan–Lindblad\n(GKSL) equation [42, 43], also known as the quantum Liou-\nville equation or master equation:\nd ˆρ\ndt = L( ˆρ) = 1\ni¯h[H, ˆρ]+∑\ni>0\nγi\n\u0012\nLi ˆρL†\ni −1\n2{L†\ni Li, ˆρ}\n\u0013\n, (6)\nwhere the Lindbladian L is a superoperator, H is the Hamilto-\nnian, and Li’s are the jump operators associated with the dis-\nsipative processes induced by the environment. The γi’s are\nthe dissipation rates. There are at most d2 −1 jump operators\nover HS. The GKSL equation is the most general equation\nsatisfying the following constraints: (i) local in time, (ii) en-\nsures the positivity ρ(t) ≥0 for all t, (iii) is trace-preserving,\ni.e., Tr ˆρ(t) = 1 for all t, and (iv) forms a quantum dynamical\nsemigroup.\nIn the vectorization form, we have\nd\ndt |ρ⟩⟩= ˆL|ρ⟩⟩,\n(7)\nwhere the Lindblad operator ˆL is of the form\nˆL =−i(H ⊗I−I⊗HT)\n+∑\ni>0\nγi[Li ⊗L∗\ni −1\n2(L†\ni Li ⊗I+I⊗LT\ni L∗\ni )].\n(8)\nThe steady state plays a crucial role in real applications and is\ndefined as the fixed point of the dynamical semigroup, ˆρSS =\nlimt→∞ˆρ(t). It can be equivalently expressed as the null state\nfor the Lindbladian L:\nL ˆρSS = 0.\n(9)\n\n\n4\nFIG. 3: We employ a variational transformer density operator as the steady-state ansatz for a 16-site dissipative transverse-field\nIsing chain with periodic boundary conditions, uniform dissipative rate, and an interaction strength of V = 2γ. The model is\ntrained using a combination of Stochastic Gradient Descent and the Stochastic Reconfiguration method to optimize the varia-\ntional parameters. The red points in the figure represent the expectation values ⟨σx⟩, ⟨σy⟩, and ⟨σz⟩, computed from the optimized\ntransformer ansatz, demonstrating its capability to accurately capture the steady-state properties of the system. The exact curve\nis calculated using NetKet.\nWhen the steady state is a pure state, it is referred to as a dark\nstate. A dark state is decoherence-free, making it a crucial\nresource for quantum computing and various quantum infor-\nmation tasks [44, 45].\nSolving for the steady state is a challenging task, especially\nfor many-body systems in condensed matter physics. Since ˆL\nis generally non-Hermitian, we introduce L = ˆL\n† ˆL, which has\na real and non-negative spectrum. The steady state satisfies\nL|ρSS⟩⟩= 0. It is worth mentioning that, in general, a solution\nto the above equation is a state vector in the doubled Hilbert\nspace, but it may not correspond to a valid density operator.\nHowever, for many physical systems, the uniqueness of the\nsteady state ensures that this does not pose a significant issue\n[46–50]. Using transformer density operator ρθθθ as an ansatz,\nthe loss function can be defined as\nLoss(θθθ) = ⟨⟨ρθθθ|L|ρθθθ⟩⟩.\n(10)\nSince Loss(θθθ) ≥0. The loss function becomes zero if and\nonly if the steady state is reached.\nThe parameters θθθ of\nthe transformer that achieve this will give the desired steady\nstate. This ansatz can be modeled by neural network and op-\ntimized using variational Monte Carlo methods as introduced\nin Sec. III E in supplementary material. Through variational\nMonte Carlo optimization, the parameters θθθ are adjusted so\nthat the resulting density operator accurately represents the\nsteady state of the open quantum system under study. The full\noptimization procedure is described in detail in Sec. III of the\nsupplementary material.\nNumerical results for the dissipative transverse-field Ising\nchain. — The Hamiltonian of the transverse-field Ising model\nis\nH = V\n4\nN\n∑\ni=1\nσz\ni σz\ni+1 + g\n2 ∑\ni\nσx\ni ,\n(11)\nwhere V is the interaction strength, g is the transverse field\nstrength, and σz\ni , σx\ni are Pauli matrices acting on the i-th spin\nwhile acting as the identity operator on all other spins in the\nsystem. Dissipation is introduced via local spin decay, mod-\neled by the jump operators Li = σ−\ni = 1\n2(σx\ni −iσy\ni ), where σ−\ni\nis the lowering operator acting on site i. The system’s evo-\nFIG. 4:\nOptimization of the variational loss function\nLoss(θθθ) = ⟨ˆL\n† ˆL⟩ρθθθ . We plot the optimization processes of\nthe transformer density operator ansatz in approximating the\nsteady-state density operator of an open quantum Ising chain.\nWe consider a 16-site dissipative transverse-field Ising chain\nwith periodic boundary conditions. The system has a uniform\ndissipation rate γ, an interaction strength V = 2γ, and a fixed\ntransverse field of magnitude g = 1.6. The optimization em-\nploys simple stochastic gradient descent and stochastic recon-\nfiguration with respective fixed learning rates. As shown in\nthe figure, although moderate fluctuations occur in the early\nstages of training, the loss function ultimately decreases by\nseveral orders of magnitude, demonstrating successful con-\nvergence toward the steady state.\n\n\n5\nFIG. 5: The observables ⟨σx⟩, ⟨σy⟩, and ⟨σz⟩are evaluated as functions of the transverse magnetic field By/γ for the case N = 5,\nwith parameters Jx/γ = 1.4, Jy/γ = 2.0, Jz/γ = 1.0, Bx/γ = −1.0, and Bz/γ = 0.1. The exact expectation values (black line)\nwere obtained using exact diagonalization via NetKet, serving as a baseline for comparison. Our transformer density operator\nansatz (red dots) achieves excellent agreement across the entire range of By/γ, validating its effectiveness in approximating the\nquantum state and observable dynamics.\nlution is governed by the Lindblad master equation, with the\ncorresponding Lindblad superoperator given by Eq. (8).\nIn our numerical simulations, we study a system of 16 lat-\ntice sites with periodic boundary conditions.\nThe dissipa-\ntion rate γi in Eq. (8) is taken to be uniform across all sites,\nand we set the coupling constant to V = 2γ. To obtain the\nsteady-state density matrix, we employ our transformer den-\nsity operator ansatz, optimizing it using the SGD or Adam\noptimizer in combination with the stochastic reconfiguration\nalgorithm [51].\nTo evaluate the accuracy of our ansatz, we compute the ex-\npectation values of local observables, specifically the steady-\nstate magnetization components along the x, y, and z direc-\ntions, given by ⟨σk⟩ss = 1\nN ∑N\ni=1⟨σk\ni ⟩,k ∈{x,y,z}. These ex-\npectation values are estimated via Monte Carlo sampling, fol-\nlowing the procedure outlined in Sec. III B of the supplemen-\ntary material. Specifically, we obtain matrix elements of the\ndensity operator for a set of sampled configurations, from\nwhich we approximate the expectation values using a local\nestimator approach. A detailed formulation of the method, in-\ncluding the probability distribution used in the sampling pro-\ncess and the construction of the local estimator, can be found\nin Secs. III A and III B of the supplementary material.\nIn Fig. 3, we present ⟨σx⟩, ⟨σy⟩, and ⟨σz⟩as a function of\nthe normalized transverse field strength g/γ. The solid black\nlines correspond to the exact steady-state values, while the red\npoints denote the results obtained using our transformer den-\nsity operator ansatz. The exact result is obtained using the it-\nerative BiCGStab solver for large systems as described in III F\nof the supplementary material. As shown in Fig. 3, the trained\nresults closely match the exact values. This demonstrates that\nour ansatz effectively captures the essential physics of the\nsteady state in the dissipative quantum system.\nTo provide a more comprehensive demonstration of the per-\nformance of our ansatz, we also show the optimization curve\nfor a transverse field of magnitude g = 1.6, as presented in\nFig. 4, which illustrates the convergence of the loss function\nLoss(θθθ) = ⟨ˆL\n† ˆL⟩ρθθθ as a function of the number of iterations.\nDetailed optimization strategy is introduced in Sec. III E 3 of\nthe supplementary material. The transformer density opera-\ntor ansatz of the blue curve demonstrates strict convergence.\nThis indicates that the transformer density operator ansatz is\nwell equipped to capture correlations inherent in the quantum\nIsing chain, owing to its self-attention mechanism. Such cor-\nrelations are essential for accurately modeling the steady-state\nproperties of dissipative quantum systems. This makes the\ntransformer density operator a promising ansatz for address-\ning systems with complex correlation structures.\nNumerical results for dissipative Heisenberg spin chain\nmodel. — We then test our model on the Heisenberg lattice\nspin system. The system under consideration is governed by\nthe following Hamiltonian:\nH =\nN\n∑\ni=1 ∑\nk=x,y,z\n\u0010\nJkσk\ni σk\ni+1 +Bkσk\ni\n\u0011\n,\n(12)\nwhere Jk denotes the interaction strength for the spin compo-\nnent along the k-th axis (k = x,y,z) between nearest-neighbor\nspins i and i + 1.\nThe term Bkσk\ni represents the effect of\nan external magnetic field applied along the k-th direction at\nsite i, with Bk being the corresponding field strength. This\nHamiltonian captures both the anisotropic exchange interac-\ntions and the influence of an external magnetic field on the\nquantum spin system. For the dissipative part, we consider a\nuniform dissipation rate across all sites, setting γj = γ for all\nj = 1,...,N. The corresponding jump operators are chosen as\nLj = σ−\nj , representing local spin lowering at each site.\nIn our numerical test on the Heisenberg lattice spin sys-\ntem, we evaluate the observables ⟨σx⟩, ⟨σy⟩, and ⟨σz⟩as func-\ntions of the transverse magnetic field ratio By/γ. The system\nconsists of N = 5 sites, with interaction strengths Jx/γ = 1.4,\nJy/γ = 2.0, and Jz/γ = 1.0. The other components of the local\nmagnetic field vector are set to Bx/γ = −1.0 and Bz/γ = 0.1.\nWe employ the same model structure as in the previous exam-\nple of the Ising model. For optimization, we use the Adam\noptimizer in combination with the stochastic reconfiguration\n\n\n6\nmethod [51] without a scheduler, which is enough to ensure\nstable convergence during the variational minimization pro-\ncess.\nThe experimental results, presented in Fig. 5, compare the\nperformance of our proposed transformer density operator\nansatz with the exact values. The exact curve is calculated\nby exact diagonalization as described in III F in the supple-\nmentary material. As shown in the graph, the result from our\nansatz matches the exact curve well. These results highlight\nthe robustness of the transformer density operator ansatz in\ncapturing the complex features of the spin system across vary-\ning magnetic field strengths.\nConclusion and discussion. — In this work, we introduce a\ntransformer variational ansatz to efficiently encode and solve\nthe steady states of dissipative quantum systems. By vectoriz-\ning the density operator into a many-body state, we first em-\nbed the input configurations into feature vectors and then uti-\nlize the self-attention mechanisms to model long-range corre-\nlation, which is crucial in the open system. Numerical ex-\nperiments on paradigmatic models, such as the dissipative\ntransverse-field Ising model and the Heisenberg model, show\nthat our approach accurately reproduces steady state of vari-\nous dissipative systems while maintaining a compact parame-\nterization and achieving fast convergence.\nSeveral promising avenues for future research exist. First,\nextending the current scheme to systems with more com-\nplex interactions, such as long-range couplings or higher-\ndimensional lattices, could uncover richer dynamical and cor-\nrelation structures. Second, exploring systems with more in-\ntricate boundary conditions (especailly in two and higher di-\nmensions) would provide further insights into the robustness\nand expressiveness of the transformer density operator ansatz.\nThird, adapting this framework to predict unknown quantum\nstates by refining the cost function could offer a novel ap-\nproach to quantum state reconstruction and tomography. Fi-\nnally, incorporating advanced techniques like self-supervised\nlearning, hyperparameter optimization, or attention-based\nmodules tailored to specific physical symmetries may enhance\nthe model’s generalization capability. We expect that this flex-\nible framework will serve as a strong foundation for tackling\nmore complex open quantum systems and advancing our un-\nderstanding of dissipative many-body physics.\nAcknowledgments. — We acknowledge Di Luo, Filippo\nVicentini, Yuan-Hang Zhang, and Chen Zhuo for beneficial\ncommunications. We thank Filippo Vicentini and Di Luo for\nsharing their codes with us. The numerical implementation of\nthe variational transformer density operator ansatz was done\nusing JAX. The variational quantum Monte Carlo and stochas-\ntic reconfiguration optimizers are available in NetKet. Z. J.\nand D. K. are supported by the National Research Foundation\nin Singapore and A*STAR under its CQT Bridging Grant and\nCQT- Return of PIs EOM YR1- 10 Funding.\n[1] H.-P. Breuer and F. Petruccione, The theory of open quantum\nsystems (Oxford University Press, USA, 2002).\n[2] H. Weimer, A. Kshetrimayum, and R. Or´us, “Simulation meth-\nods for open quantum many-body systems,” Rev. Mod. Phys.\n93, 015008 (2021), arXiv:1907.07079 [quant-ph].\n[3] C. Gardiner and P. Zoller, Quantum noise: a handbook of\nMarkovian and non-Markovian quantum stochastic methods\nwith applications to quantum optics (Springer Science & Busi-\nness Media, 2004).\n[4] S. Diehl, A. Micheli, A. Kantian, B. Kraus, H. P. B¨uchler, and\nP. Zoller, “Quantum states and phases in driven open quan-\ntum systems with cold atoms,” Nature Physics 4, 878 (2008),\narXiv:0803.1482 [quant-ph].\n[5] F. Verstraete, M. M. Wolf,\nand J. Ignacio Cirac, “Quantum\ncomputation and quantum-state engineering driven by dissipa-\ntion,” Nature Physics 5, 633 (2009), arXiv:0803.1447 [quant-\nph].\n[6] L. M. Sieberer, M. Buchhold, and S. Diehl, “Keldysh field the-\nory for driven open quantum systems,” Reports on Progress in\nPhysics 79, 096001 (2016).\n[7] U. Schollw¨ock, “The density-matrix renormalization group,”\nRev. Mod. Phys. 77, 259 (2005).\n[8] R. Orus, “Tensor networks for complex quantum systems,”\narXiv preprint arXiv:1812.04011 (2018).\n[9] J. I. Cirac, D. P´erez-Garc´ıa, N. Schuch, and F. Verstraete, “Ma-\ntrix product states and projected entangled pair states: Con-\ncepts, symmetries, theorems,” Rev. Mod. Phys. 93, 045003\n(2021), arXiv:2011.12127 [quant-ph].\n[10] J. Cui, J. I. Cirac, and M. C. Ba˜nuls, “Variational matrix prod-\nuct operators for the steady state of dissipative quantum sys-\ntems,” Phys. Rev. Lett. 114, 220601 (2015).\n[11] A. Kshetrimayum, H. Weimer, and R. Or´us, “A simple tensor\nnetwork algorithm for two-dimensional steady states,” Nature\ncommunications 8, 1291 (2017).\n[12] A. H. Werner, D. Jaschke, P. Silvi, M. Kliesch, T. Calarco,\nJ. Eisert,\nand S. Montangero, “Positive tensor network ap-\nproach for simulating open quantum many-body systems,”\nPhys. Rev. Lett. 116, 237201 (2016).\n[13] J. Jin, A. Biella, O. Viyuela, L. Mazza, J. Keeling, R. Fazio,\nand D. Rossini, “Cluster mean-field approach to the steady-\nstate phase diagram of dissipative spin systems,” Phys. Rev. X\n6, 031011 (2016).\n[14] D. Luo, Z. Chen, J. Carrasquilla, and B. K. Clark, “Autoregres-\nsive neural network for simulating open quantum systems via a\nprobabilistic formulation,” Physical review letters 128, 090501\n(2022), arXiv:2009.05580 [cond-mat.str-el].\n[15] S. Finazzi, A. Le Boit´e, F. Storme, A. Baksic, and C. Ciuti,\n“Corner-space renormalization method for driven-dissipative\ntwo-dimensional correlated systems,” Phys. Rev. Lett. 115,\n080604 (2015).\n[16] C. Bishop, C. M. Bishop, et al., Neural networks for pattern\nrecognition (Oxford university press, 1995).\n[17] L. V. Fausett et al., Fundamentals of neural networks: architec-\ntures, algorithms, and applications, Vol. 3 (Prentice-Hall En-\nglewood Cliffs, 1994).\n[18] Z.-A. Jia, B. Yi, R. Zhai, Y.-C. Wu, G.-C. Guo,\nand G.-\nP. Guo, “Quantum neural network states: A brief review of\nmethods and applications,” Advanced Quantum Technologies\n2, 1800077 (2019), arXiv:1808.10601 [quant-ph].\n[19] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld,\n\n\n7\nN. Tishby, L. Vogt-Maranto,\nand L. Zdeborov´a, “Machine\nlearning and the physical sciences,” Rev. Mod. Phys. 91,\n045002 (2019), arXiv:1903.10563 [physics.comp-ph].\n[20] Z.-A. Jia, Y.-H. Zhang, Y.-C. Wu, L. Kong, G.-C. Guo, and G.-\nP. Guo, “Efficient machine-learning representations of a surface\ncode with boundaries, defects, domain walls, and twists,” Phys.\nRev. A 99, 012307 (2019), arXiv:1802.03738 [quant-ph].\n[21] Z.-A. Jia, L. Wei, Y.-C. Wu, G.-C. Guo,\nand G.-P. Guo,\n“Entanglement area law for shallow and deep quantum neu-\nral network states,” New Journal of Physics 22, 053022 (2020),\narXiv:1907.11333 [quant-ph].\n[22] X. Gao and L.-M. Duan, “Efficient representation of quantum\nmany-body states with deep neural networks,” Nature Commu-\nnications 8, 662 (2017).\n[23] D.-L. Deng, X. Li, and S. Das Sarma, “Quantum entanglement\nin neural network states,” Phys. Rev. X 7, 021021 (2017).\n[24] Y.-H. Zhang, Z. Jia, Y.-C. Wu, and G.-C. Guo, “An efficient al-\ngorithmic way to construct boltzmann machine representations\nfor arbitrary stabilizer code,” (2022), arXiv:1809.08631 [quant-\nph].\n[25] G. Carleo and M. Troyer, “Solving the quantum many-body\nproblem with artificial neural networks,” Science 355, 602\n(2017).\n[26] G. Torlai and R. G. Melko, “Latent space purification via neural\ndensity operators,” Phys. Rev. Lett. 120, 240503 (2018).\n[27] N. Yoshioka and R. Hamazaki, “Constructing neural stationary\nstates for open quantum many-body systems,” Phys. Rev. B 99,\n214306 (2019).\n[28] M. J. Hartmann and G. Carleo, “Neural-network approach to\ndissipative quantum many-body dynamics,” Phys. Rev. Lett.\n122, 250502 (2019).\n[29] A. Nagy and V. Savona, “Variational quantum monte carlo\nmethod with a neural-network ansatz for open quantum sys-\ntems,” Phys. Rev. Lett. 122, 250501 (2019).\n[30] F. Vicentini, A. Biella, N. Regnault, and C. Ciuti, “Variational\nneural-network ansatz for steady states in open quantum sys-\ntems,” Phys. Rev. Lett. 122, 250503 (2019).\n[31] D. Nigro, “Invariant neural network ansatz for weakly symmet-\nric open quantum lattices,” Phys. Rev. A 103, 062406 (2021),\narXiv:2101.03511 [quant-ph].\n[32] J. Mellak, E. Arrigoni, and W. von der Linden, “Deep neural\nnetworks as variational solutions for correlated open quantum\nsystems,” Communications Physics 7, 268 (2024).\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” Advances in Neural Information Processing Systems\n(2017), arXiv:1706.03762 [cs.CL].\n[34] Y.-H. Zhang and M. Di Ventra, “Transformer quantum state: A\nmultipurpose model for quantum many-body problems,” Phys.\nRev. B 107, 075147 (2023), arXiv:2208.01758 [quant-ph].\n[35] L. L. Viteritti, R. Rende,\nand F. Becca, “Transformer\nvariational\nwave\nfunctions\nfor\nfrustrated\nquantum\nspin\nsystems,” Physical Review Letters 130, 236401 (2023),\narXiv:2211.05504 [cond-mat.dis-nn].\n[36] R. Rende, L. L. Viteritti, F. Becca, A. Scardicchio, A. Laio,\nand G. Carleo, “Foundation neural-network quantum states,”\n(2025), arXiv:2502.09488 [quant-ph].\n[37] K. Kawabata, A. Kulkarni, J. Li, T. Numasawa, and S. Ryu,\n“Symmetry of open quantum systems: Classification of dis-\nsipative quantum chaos,” PRX Quantum 4, 030328 (2023),\narXiv:2212.00605 [cond-mat.mes-hall].\n[38] Y. Bao, R. Fan, A. Vishwanath,\nand E. Altman, “Mixed-\nstate topological order and the errorfield double formulation of\ndecoherence-induced transitions,”\n(2023), arXiv:2301.05687\n[quant-ph].\n[39] R. Sohal and A. Prem, “Noisy approach to intrinsically mixed-\nstate topological order,” PRX Quantum 6, 010313 (2025).\n[40] R. Ma and A. Turzillo, “Symmetry protected topological\nphases of mixed states in the doubled space,”\n(2024),\narXiv:2403.13280 [quant-ph].\n[41] F. Vicentini, D. Hofmann, A. Szab´o, D. Wu, C. Roth, C. Giu-\nliani, G. Pescia, J. Nys, V. Vargas-Calder´on, N. Astrakhant-\nsev, et al., “Netket 3: Machine learning toolbox for many-body\nquantum systems,” SciPost Physics Codebases , 007 (2022).\n[42] G. Lindblad, “On the generators of quantum dynamical semi-\ngroups,” Communications in Mathematical Physics 48, 119\n(1976).\n[43] V. Gorini, A. Kossakowski, and E. C. G. Sudarshan, “Com-\npletely positive dynamical semigroups of n-level systems,”\nJournal of Mathematical Physics 17, 821 (1976).\n[44] D. A. Lidar and K. Birgitta Whaley, “Decoherence-free sub-\nspaces and subsystems,” in Irreversible quantum dynamics\n(Springer, 2003) pp. 83–120, arXiv:quant-ph/0301032 [quant-\nph].\n[45] R. Blume-Kohout, H. K. Ng, D. Poulin, and L. Viola, “Charac-\nterizing the structure of preserved information in quantum pro-\ncesses,” Phys. Rev. Lett. 100, 030501 (2008), arXiv:0705.4282\n[quant-ph].\n[46] S. G. Schirmer and X. Wang, “Stabilizing open quantum sys-\ntems by markovian reservoir engineering,” Phys. Rev. A 81,\n062306 (2010).\n[47] Z. Cai and T. Barthel, “Algebraic versus exponential decoher-\nence in dissipative many-particle systems,” Phys. Rev. Lett.\n111, 150403 (2013).\n[48] B. Horstmann, J. I. Cirac, and G. Giedke, “Noise-driven dy-\nnamics and phase transitions in fermionic systems,” Phys. Rev.\nA 87, 012108 (2013).\n[49] T. Prosen, “Comments on a boundary-driven open xxz chain:\nasymmetric driving and uniqueness of steady states,” Physica\nScripta 86, 058511 (2012).\n[50] J.-T. Hsiang and B. Hu, “Nonequilibrium steady state in open\nquantum systems: Influence action, stochastic equation and\npower balance,” Annals of Physics 362, 139 (2015).\n[51] A. Chen and M. Heyl, “Empowering deep neural quantum\nstates through efficient optimization,” Nature Physics 20, 1476\n(2024).\n[52] S. Sorella, “Green function monte carlo with stochastic re-\nconfiguration,” Phys. Rev. Lett. 80, 4558 (1998), arXiv:cond-\nmat/9803107 [cond-mat].\n\n\n1\nSUPPLEMENTARY MATERIAL: VARIATIONAL TRANSFORMER ANSATZ FOR THE DENSITY OPERATOR\nOF STEADY STATES IN DISSIPATIVE QUANTUM MANY-BODY SYSTEMS\nIn this supplementary material, we provide a detailed discussion of our transformer density operator ansatz for solving the\nsteady state. In Section I, we discuss the vectorized Lindblad equation, and in Section II and Section III, we provide an in-depth\ndescription of our transformer density operator ansatz and the optimization machanism.\nI.\nSTEADY STATE AND THE VECTORIZATION OF DENSITY OPERATOR\nIn this section, we review the vectorization formalism of the density operator and its dynamics, a powerful framework for\nrepresenting the steady states of dissipative open quantum systems. This approach has recently gained significant attention in\nstudies of open-system quantum phases, weak and strong symmetries, tenfold classification, and other related topics (see, e.g.,\n[27, 37–40]). Unlike the traditional density matrix representation, the vectorized form offers a more convenient computational\nframework for steady-state analysis. By expressing the density operator in this form, steady states can be obtained by solving\nfor the ground state of a specially constructed operator.\nTo solve for the steady state ˆρSS of a Lindbliadian L, we introduce the vectorization of the density operator ˆρ in a fixed basis\n{|α⟩}. Given the representation\nˆρ = ∑\nα,β\nρα,β|α⟩⟨β|,\n(S1)\nThe vectorized form is defined as\n|ρ⟩⟩= ∑\nα,β\nρα,β|α⟩|β⟩.\n(S2)\nMore generally, we have\n|(|ψ⟩⟨φ|)⟩⟩= |ψ⟩|φ ∗⟩,\n(S3)\nwhere φ ∗is the complex conjugate of φ in the given basis. It is clear that vectorization is a basis-dependent operation.\nLet A and B be two operators acting on separate subsystems. Their vectorized forms are denoted as |A⟩⟩and |B⟩⟩, respectively.\nNote that in the vectorization process, the reordering of kets and bras for each local degree of freedom must be taken into account.\nConsequently, the vectorization of the tensor product does not satisfy a simple factorization\n|A⊗B⟩⟩̸= |A⟩⟩⊗|B⟩⟩.\n(S4)\nThis distinction arises because vectorization is performed in a specific basis, and care must be taken in handling the ordering of\nindices when working with composite systems.\nIn the vectorized form, a superoperator acting as AρB is represented as\nAρB 7→(A⊗BT)|ρ⟩⟩.\n(S5)\nFor a multipartite system of N spins, the vectorized representation of the density operator is given by\n|ρ⟩⟩=\n∑\nα1,...,αN;β1,...,βN\nρα1,...,αN;β1,...,βN|α1,...,αN⟩⊗|β1,...,βN⟩.\n(S6)\nThe vectorized form encodes the state of multiple subsystems by introducing auxiliary degrees of freedom.\nConsider the Gorini–Kossakowski–Sudarshan–Lindblad (GKSL) equation (set ¯h = 1)\ndρ\ndt = L(ρ) = −i[H,ρ]+∑\ni>0\nγi(LiρL†\ni −1\n2{L†\ni Li,ρ}),\n(S7)\nIn the vectorization form, we have\nd\ndt |ρ⟩⟩= ˆL|ρ⟩⟩,\n(S8)\n\n\n2\nwhere the Lindblad operator ˆL is of the form\nˆL =−i(H ⊗I−I⊗HT)+∑\ni>0\nγi[Li ⊗L∗\ni −1\n2(L†\ni Li ⊗I+I⊗LT\ni L∗\ni )].\n(S9)\nThe density matrix in the doubled Hilbert space is the state that\nˆL|ρ⟩⟩= 0.\n(S10)\nIt is important to highlight that, in general, a solution to the above equation corresponds to a state vector in the doubled Hilbert\nspace, but it may not always be a valid density operator. However, steady states of open systems may not always be unique.\nThe uniqueness of |ρss⟩⟩depends on the spectral properties of ˆL, and certain symmetries can lead to degenerate steady states. In\ncases where the steady state is not unique, additional selection rules or symmetry constraints may be necessary to determine the\ncorrect physical solution. Many open systems have a unique steady state [46–50], which guarantees that the resulting state in\nthe doubled Hilbert space automatically corresponds to a steady-state density operator.\nSince ˆL is generally non-Hermitian, its eigenvalues are complex. Directly solving ˆL|ρ⟩⟩= 0 may lead to numerical instability.\nInstead, we introduce the Hermitian operator\nL = ˆL\n† ˆL,\n(S11)\nwhere L is a Hermitian matrix with real and non-negative eigenvalues. The zero-eigenvalue solution of L|ρ⟩⟩= 0 corresponds\nto the steady-state solution of the original Lindblad equation. This formulation enables the use of variational minimization\ntechniques to efficiently approximate the steady state.\nThe lowest eigenstate with eigenvalue λ = 0 of ˆL\n† ˆL corresponds to the steady state. Therefore, solving the equation\nˆL\n† ˆL|ρ⟩⟩= 0\n(S12)\nprovides the steady state, as shown in equation (S10). We can then optimize the energy functional to find the ground state, akin\nto the approach in closed systems. The energy functional is given by\nE = ⟨⟨ρ|L|ρ⟩⟩= ⟨⟨ρ| ˆL\n† ˆL|ρ⟩⟩.\n(S13)\nThis expression serves as the optimization objective and loss function, represented as the expectation value of the operator ˆL\n† ˆL\nin the vectorized form.\nIn the vectorized form, the expectation value of an observable is calculated as\nTr(A†B) = ⟨⟨A|B⟩⟩.\n(S14)\nThe expectation value of an observable O in the vectorized formalism is given by\n⟨O⟩= Tr(Oρ) = ⟨⟨O|ρ⟩⟩,\n(S15)\nwhere the ”†” has been omitted for the second equality since O is an Hermitian operator.\nII.\nTRANSFORMER DENSITY OPERATOR ARCHITECTURE\nIn this work, we employ a transformer density operator approach to parameterize the steady state of spin chains with periodic\nboundary conditions. While convolutional layers primarily capture local dependencies through learnable filters, the multi-head\nself-attention modules—adapted from the transformer framework—enable the model to capture dependencies across the entire\nsystem. This makes them particularly effective for representing quantum states with complex interactions. By dynamically\nextracting multiple similarity patterns along the chain, these modules can model both short- and long-range correlations. This\ncapability is especially crucial in systems with periodic boundary conditions, where distant spins remain strongly correlated,\nnecessitating a global perspective for accurate representation.\nConcretely, the network begins by embedding each spin configuration pair (σ,σ′) into a continuous feature space using a\nshallow CNN stage. The resulting features are then passed through transformer-based attention layers, which aggregate infor-\nmation across all sites in a translation-invariant manner. This attention mechanism naturally captures various forms of spin\ncorrelation, as each attention head can learn to focus on different regions or subsets of sites within the chain. Finally, a small\ndense module outputs the complex amplitudes (or the real and imaginary components) that define the desired quantum state\nor density operator. This design combines the local feature extraction capabilities of CNNs with the global context modeling\npower of multi-head self-attention, making the network particularly well-suited for representing the steady-state properties of\nopen quantum spin systems.\n\n\n3\n1.\nOverview of the Transformer Density Operator Architecture\nIn the standard transformer [33], positional encodings, multi-layer encoder-decoder blocks, and feed-forward networks are\ntypically used to capture long-range correlations in sequential data. However, positional encoding and the decoder mechanism\nare not used for steady-state representations. We modify the transformer architecture to respect the symmetries of the steady-state\ndensity operator while preserving its ability to capture long-range correlations.\nSpecifically, we introduce a transformer density operator that replaces conventional positional encoding with a translation-\ninvariant representation and focuses on self-attention mechanisms to learn multiple similarity patterns across spins. The network\nfirst applies two convolutional blocks to encode input qubit configurations while respecting periodic boundary conditions. A self-\nattention block then captures rich correlation patterns across the entire spin chain. Next, a global mean-pooling step enforces\ntranslation invariance. Finally, a dense layer maps the extracted features to produce the real and imaginary components of the\ncomplex amplitudes. To ensure Hermitian symmetry in the steady-state density operator, the network computes the logarithm of\nthe sum of two symmetrized exponentials of these amplitudes to obtain an ansatz density operator.\n2.\nFeature Embedding\nTo encode B batches of configurations of a vectorized steady state (x(ℓ),x(r)) ∈RB×2L of a one-dimensional chain with L\nspins, we stack these two spin configurations x(ℓ),x(r) ∈RB×L as a two-channel input\n(x(ℓ),x(r))\n7−→\nh\nx(ℓ)\n1 ,...,x(ℓ)\nL\ni\n|\n{z\n}\nchannel 1\n|\nh\nx(r)\n1 ,...,x(r)\nL\ni\n|\n{z\n}\nchannel 2\n, ∈RB×L×2\neffectively producing a L × 2 array. This two-dimensional format allows convolutional layers to scan each pair (x(ℓ)\ni ,x(r)\ni )\njointly. A dummy dimension was added.\nWe then apply circular padding in the L dimension to respect the periodic boundary conditions. Specifically,\nX(1) = Convcircular\n\u0000Xinput\n\u0001\n∈RB×L×1×C1,\n(S16)\nwhere C1 is the number of filters, followed by a non-linear activation. A second convolution with C2 filters is applied in the same\nmanner:\nX(2) = Convcircular\n\u0000X(1)\u0001\n∈RB×L×1×C2.\n(S17)\nThese layers extract local patterns by sliding kernels of size (2×1) across the stacked spin inputs. Hence, short-range correla-\ntions are encoded in a hierarchy of convolutional feature maps.\n3.\nSelf-Attention for Global Correlations\nIn open quantum systems with periodic boundary conditions, the system may exhibit different scales of correlation due to\ncompetition between spin-spin interactions, external fields, and dissipative channels. To capture the variety of correlation patterns\namong spins, we incorporate a self-attention module that calculates a dot-product attention among the spin-site embeddings. In\nparticular, we use either a single-head attention layer or a multi-head architecture that splits the hidden dimension into several\nheads, each learning a distinct similarity pattern. This allows any spin site to directly attend to all others, thus modeling extended\nor global correlations that are often found in the steady state of the dissipative spin chain. We reshape X(2) by removing the\ndummy axis:\nX = squeeze\n\u0000X(2), axis = 2\n\u0001\n∈RB×L×C2.\n(S18)\nWe then apply either a single-head or multi-head self-attention block. Let Q,K,V ∈RB×L×(C2/h) be the query, key, and value\nembeddings for h heads, obtained via learned linear projections:\nQ = XWQ,\nK = XWK,\nV = XWV,\n(S19)\nwhere WQ,WK,WV ∈RC2×(C2/h) in each attention head. For each head i, the attention weights\nheadi = softmax\n\u0010 Qi K⊤\ni\np\nC2/h\n\u0011\nVi\n(S20)\n\n\n4\nare computed, then concatenated and projected back to dimension C2, with a final residual connection:\nX′ = Concat(head1,...,headh)WO + X.\n(S21)\nHere, WO acts as a learned linear transformation to map the concatenated multi-head outputs back to the original embedding\ndimension. The resulting tensor X′ encodes long-range correlations between all sites, and we omit additional feed-forward\nsub-layers and normalization for simplicity.\n4.\nGlobal Mean-Pooling and Final Output\nAlthough the convolutional filters reuse parameters across different lattice sites, the feature maps still encode a positional\nfootprint. To impose strict translation invariance, we add a global average pooling operation over the spatial dimension\nX(pool) = 1\nL\nL\n∑\nj=1\n\u0000X′\n:,j,:\n\u0001\n,\nX(pool) ∈RB×C2.\n(S22)\nConsequently, the final output of the transformer density operator becomes independent of site indexing. This design not only\nenforces physical symmetry under cyclic shifts but also reduces the fully connected layer’s parameters and enables transfer\nlearning to systems of different sizes [32]. After pooling, we flatten the feature maps and feed them into a dense layer with two\noutput neurons, [F0, F1], representing real and imaginary components of a complex number, which is then used to generate the\nfinal complex amplitude ρθθθ of the steady state. Hence, we obtained a complex number z = F0 +iF1 after the global pooling. In\nthis way all parameters remain real-valued, which simplifies optimization routines. However, in our steady-state representation,\nwe further combine these amplitudes via log\n\u0000exp(z1) + exp(z2)∗\u0001\n, where z1,z2 are the complex outputs from x(ℓ) and x(r) of\na different order. This construction naturally enforces Hermiticity and is well-suited to describing the density operator of an\nopen-system quantum spin chain.\nIII.\nOPTIMIZATION AND EVALUATION OF TRANSFORMER DENSITY OPERATOR ANSATZ\nIn this section, we present the full procedure for training and validating the transformer density operator ansatz. We begin\nby outlining our Metropolis-Hastings sampling strategy for mixed states, which enables efficient estimates of both expectation\nvalues and gradients. We then explain how to compute observables in the mixed-state setting and describe our use of stochas-\ntic reconfiguration (also known as natural gradient descent) to stabilize and accelerate optimization. Finally, we discuss two\nbenchmark approaches—Exact Diagonalization and the iterative BiCGStab method—against which we compare our results to\nconfirm the accuracy and scalability of our approach.\nA.\nEfficient Sampling Strategy\nInstead of the autoregressive sampling method [34], the Markov chain Monte Carlo approach based on the Metropolis-\nHastings algorithm, implemented via NetKet’s MetropolisLocal sampler, is employed here. This method generates con-\nfigurations by proposing local updates to the spin configuration and accepting them according to the Metropolis rule. These\nconfigurations are then used to estimate expectation values and gradients afterwards.\nGiven the variational density operator ansatz with current parameters θθθ:\n|ρθθθ⟩⟩= ∑\nααα,βββ\nρθθθ(ααα,βββ)|ααα⟩⊗|βββ⟩.\n(S23)\nWe sample from the probability distribution given by:\nPθθθ(ααα,βββ) ∝|ρθθθ(ααα,βββ)|2\n(S24)\nThe Metropolis-Hastings algorithm generates a sequence of configurations according to Pθθθ(ααα,βββ) by iteratively proposing and\naccepting new configurations. The algorithm works in the following steps:\n1. A local spin configuration update (ααα,βββ) →(ααα′,βββ ′) is proposed.\n\n\n5\n2. The new configuration is accepted with probability:\nAaccept((ααα,βββ) →(ααα′,βββ ′)) = min\n\u0012\n1, Pθθθ(ααα′,βββ ′)g((ααα,βββ)|(ααα′,βββ ′))\nPθθθ(ααα,βββ)g((ααα′,βββ ′)|(ααα,βββ))\n\u0013\n,\n(S25)\nwhere g((ααα′,βββ ′)|(ααα,βββ)) is the probability of proposing (ααα′,βββ ′) given (ααα,βββ). Since the algorithm only modifies a single\nspin degree of freedom per step, this transition kernel is symmetric, simplifying the acceptance ratio.\n3. If accepted, the configuration is updated; otherwise, the previous state is retained.\n4. This process is repeated to generate a Markov chain of configurations for estimating expectation values.\nThis sampling strategy efficiently explores the configuration space of the variational density operator ansatz.\nB.\nMixed-State Observables\nWhen evaluating observables for a mixed-state density operator, one can exploit a slightly different identity that rewrites the\nquantum expectation value as a classical expectation over the distribution given by the diagonal of ˆρθθθ. Specifically, for an\noperator ˆA, the expectation value can be expressed as\n⟨ˆA⟩= Tr\n\u0000 ˆρθθθ ˆA\n\u0001\nTr\n\u0000 ˆρθθθ\n\u0001 = ∑\nααα∈M\nρθθθ(ααα,ααα)\nTr\n\u0000 ˆρθθθ\n\u0001\n˜Aρθθθ (ααα),\n(S26)\nwhere the local estimator is defined as\n˜Aρθθθ (ααα) = ∑\nβββ\nρθθθ(ααα,βββ)\nρθθθ(ααα,ααα)⟨βββ| ˆA|ααα⟩.\n(S27)\nHere, ααα and βββ label basis configurations in the Hilbert space. The probability distribution\nPθθθ(ααα) = ρθθθ(ααα,ααα)\nTr( ˆρθθθ)\nplays the role of a classical distribution over the diagonal elements of ˆρθθθ, and the local estimator ˜Aρθθθ (ααα) here involves an inner\nsum over all basis states (or a suitably chosen subset) to capture the off-diagonal contributions ρθθθ(ααα,βββ) to the observable ˆA.\nIn this manner, the quantum expectation value reduces to a standard classical average over a series of sampled configurations\nM, allowing one to use the same Metropolis-Hastings sampling scheme described in Sec. III A to estimate both observables and\ntheir gradients for the mixed-state variational ansatz.\nC.\nMetric Tensor in Stochastic Reconfiguration\nThe stochastic reconfiguration method, also known as the natural gradient descent, introduces a metric tensor S that accounts\nfor the curvature of the variational parameter space. This tensor approximates the Fisher information matrix and ensures that the\noptimization follows a path that respects the geometry of the variational manifold.\nThe metric tensor S is defined as the covariance matrix of the logarithmic derivatives of the variational wavefunction:\nSi j = ⟨∆O∗\ni ∆Oj⟩−⟨∆O∗\ni ⟩⟨∆O j⟩,\n(S28)\nwhere\n∆Oi = ∂logρθθθ(ααα,βββ)\n∂θi\n(S29)\nis the derivative of the log-probability with respect to the variational parameters θi.\nIn the context of mixed-state variational ansatz, this metric tensor is computed using Monte Carlo sampling as described\nin Sec. III B. Once S is constructed, it is used in the natural gradient update step to precondition the gradient of the energy\nfunctional.\n\n\n6\nD.\nRegularization and Stabilization\nTo ensure numerical stability in the inversion of the metric tensor S, a small regularization term was introduced by adding a\ndiagonal shift λ [52]:\nS′ = S+λI,\n(S30)\nwhere I is the identity matrix and λ is a small positive constant. This regularization prevents the metric tensor from becoming\nsingular and ensures robust optimization updates.\nThis is implemented as the stochastic reconfiguration method as a gradient preconditioner in NetKet, where S is constructed\nfrom Monte Carlo estimates as described in Section III C. This approach stabilizes the training dynamics of the transformer\ndensity operator ansatz and improves convergence in the steady-state optimization.\nE.\nOptimization Procedure\nThe training of the Transformer Density Operator Ansatz is carried out in a variational framework. Our goal is to find the\nsteady state that satisfies\nˆL|ρθθθ⟩⟩= ˆLρρρθθθ = 0,\n(S31)\nby minimizing the loss functional\nLoss(θθθ) = ⟨⟨ρθθθ|L|ρθθθ⟩⟩,\n(S32)\nwhere the squared Lindblad superoperator is defined as\nL = ˆL\n† ˆL.\n(S33)\nwith ˆL being the Lindblad superoperator in its vectorized form. Minimizing this loss function is equivalent to minimizing the\nFrobenius norm of the time derivative of the density matrix (To distinguish it from the previously mentioned loss function, we\nrefer to it here as a cost function, though both terms are interchangeable in the context of machine learning.):\nCost(θθθ) = ∥ˆLρρρθθθ∥2\n2\n∥ρρρθθθ∥2\n2\n=\nTr\n\u0010\nρρρ†\nθθθ ˆL\n† ˆLρρρθθθ\n\u0011\nTr\n\u0010\nρρρ†\nθθθρρρθθθ\n\u0011\n,\n(S34)\nwhich reaches its global minimum when the steady-state condition ˆLρρρθθθ = 0 holds.\nIn our implementation, we use a hybrid optimization approach that combines standard first-order gradient updates with second-\norder corrections via stochastic reconfiguration introduced in III D. The Stochastic Gradient Descent or Adam optimizer performs\nstandard gradient updates, while the stochastic reconfiguration accounts for the curvature of the variational manifold by intro-\nducing a metric tensor S, effectively implementing a natural gradient descent strategy. Their methods are provided in NetKet via\na dedicated variational driver nk.SteadyState and Eq. (S34) is the cost function that NetKet uses in its steady-state driver.\n1.\nHybrid Optimization Approach\nAt each optimization step, the parameters θθθ are updated using two complementary components:\nFirst-Order Gradient Update:\nStandard gradient-based methods are used to update the parameters\nθθθ (k+1) = θθθ (k) −η∇θθθ Cost(θθθ),\n(S35)\nwhere η is the learning rate. In our experiments, we primarily use stochastic gradient descent with an appropriate learning rate\nschedule, while Adam is also applied in our Heisenberg model example.\n\n\n7\nSecond-Order Correction via Stochastic Reconfiguration:\nTo account for the geometry of the variational parameter space,\nwe incorporate stochastic reconfiguration, which introduces a metric tensor S that is an approximation of the Fisher information\nmatrix. The update rule is modified to\nθθθ (k+1) = θθθ (k) −ηS−1∇θθθ Cost(θθθ),\n(S36)\nwhere η is the learning rate, S is the metric tensor, and ∇θθθ Cost(θθθ) is the gradient of the energy functional with respect to the\nparameters θθθ.\nThe stochastic gradient ∇θθθ Cost(θθθ) is estimated over the probability distribution defined by the entries of the vectorized\ndensity matrix Pθθθ(ααα,βββ) ∝|ρθθθ(ααα,βββ)|2 as in Sec.III B. The gradient of the cost function with respect to the complex conjugate\nof the ith parameter can be expressed as\n∂\n∂θ ∗\ni\nCost(θθθ) = ⟨˜Li∇∗\ni ˜Li⟩−⟨O∗\ni ˜L2⟩,\n(S37)\nwhere the local estimator is defined by\n˜L(ααα,βββ) =\n∑ααα′,βββ ′ ˆL(ααα,βββ;ααα′,βββ ′)ρθθθ(ααα′,βββ ′)\nρθθθ(ααα,βββ)\n.\n(S38)\n2.\nOptimization Workflow\nThe full optimization process is summarized as follows:\n1. Initialize the network parameters θθθ randomly.\n2. Sample a batch of configurations from the current transformer density operator ansatz using a Markov chain Monte Carlo\nsampler introduced in Sec. III A.\n3. Compute the loss (cost) functional Cost(θθθ) and its gradient.\n4. Compute the metric tensor S for stochastic reconfiguration introduced in Sec. III C.\n5. Regularize the metric tensor with a small constant λ, i.e., S′ = S + λ I, to ensure numerical stability as introduced in\nSec. III D.\n6. Solve for the preconditioned gradient update using S′−1∇θθθ Cost(θθθ).\n7. Update the parameters θθθ using the rule\nθθθ (k+1) = θθθ (k) −ηS′−1∇θθθ Cost(θθθ).\n8. Repeat the above steps until convergence.\nThis comprehensive optimization strategy—integrating first-order gradient updates, second-order corrections via Stochastic\nReconfiguration, and the conceptual framework of NetKet’s steady-state variational driver—ensures that our transformer density\noperator ansatz accurately converges to the steady state of open quantum systems.\n3.\nLearning Rate Scheduling\nTo improve training stability and convergence, we employ a learning rate schedule that combines an initial warm-up step with\na cosine decay strategy. Specifically, the learning rate is defined as:\nlr(istep) =\n(\nη0,\nistep < iswitch\nη0 ·\n1+cos(π(istep−iswitch)/idecay)\n2\n+αη0,\nistep ≥iswitch\n(S39)\nwhere η0 is the initial learning rate, istep is the current training step, iswitch denotes the step at which the decay begins, idecay is\nthe decay period, and α is a scaling factor for the minimum learning rate. In our implementation, we set:\nη0 = 0.0061,\niswitch = 30000,\nidecay = 40000,\nα = 0.001.\n\n\n8\nThis schedule ensures a stable learning rate during the initial phase, facilitating rapid exploration of the parameter space, followed\nby a smooth decay to refine the variational ansatz.\nAdditionally, we apply a similar schedule to the stochastic reconfiguration preconditioner, adjusting the diagonal shift dynam-\nically to improve numerical stability and convergence:\nλSR(istep) =\n(\nλ0,\nistep < iswitch,SR\nλ0 ·\n1+cos(π(istep−iswitch,SR)/idecay,SR)\n2\n+αSRλ0,\nistep ≥iswitch,SR\n(S40)\nwhere λ0 = 0.004, iswitch,SR = 30000, idecay,SR = 40000, and αSR = 0.01. This approach dynamically adjusts the regularization\nstrength of the stochastic reconfiguration method, ensuring robustness while maintaining efficiency in parameter updates.\nBy incorporating these schedules, we balance initial exploration with controlled optimization, leading to improved stability\nand convergence of the transformer density operator ansatz.\nF.\nBenchmark Methods for Steady-State Computation\nTo validate our variational approach, we compare the steady-state observables computed with our transformer-based ansatz\nagainst two benchmark methods implemented by NetKet. Both methods aim to solve for the steady state of an open quantum\nsystem, which satisfies\nˆL|ρ⟩⟩= 0,\n(S41)\nwhere ˆL is the Lindblad superoperator.\n1.\nExact Diagonalization\nFor small system sizes of less than 7 spins, one can fully diagonalize the Lindblad superoperator. In the realization, the\nfollowing operator is constructed\nL = ˆL\n† ˆL.\n(S42)\nExact diagonalization proceeds by solving the eigenvalue problem\nL|ρ⟩⟩= λ|ρ⟩⟩.\n(S43)\nThe steady state is identified as the eigenvector corresponding to the zero eigenvalue (λ = 0):\nL|ρss⟩⟩= 0.\n(S44)\n2.\nIterative Biconjugate Gradient Stabilized Method\nFor larger system sizes, the Hilbert space grows exponentially, making full diagonalization computationally infeasible. To\nefficiently obtain the steady state in such cases, we employ the iterative Biconjugate Gradient Stabilized (BiCGStab) method.\nAs before, we seek to solve\nˆL\n† ˆL|ρ⟩⟩= L|ρ⟩⟩= 0,\n(S45)\nwhere L is a positive semi-definite Hermitian operator. The BiCGStab algorithm allows us to iteratively converge to the steady-\nstate solution without requiring explicit matrix inversion or full diagonalization, making it particularly suitable for large-scale\ndissipative quantum systems.\nResidual and Krylov Subspace. For a given approximate solution |ρ(k)⟩⟩at iteration k, the residual is defined as\n|r(k)⟩= L|ρ(k)⟩⟩.\n(S46)\nThe BiCGStab algorithm constructs approximate solutions by searching within the so-called Krylov subspace generated by\nrepeatedly applying L to the initial residual. Concretely, starting from the initial residual |r(0)⟩, the Krylov subspace of dimension\nm is given by\nKm\n\u0000L,|r(0)⟩\n\u0001\n= span\nn\n|r(0)⟩, L|r(0)⟩, L2|r(0)⟩,...,Lm−1|r(0)⟩\no\n.\n(S47)\n\n\n9\nAt each iteration, BiCGStab refines |ρ(k)⟩⟩within this subspace to reduce the norm of the residual ∥|r(k)⟩∥.\nAlgorithmic Steps.\n1. Initialization: Choose an initial guess |ρ(0)⟩⟩. Compute the initial residual |r(0)⟩= L|ρ(0)⟩⟩. Often, one sets |ρ(0)⟩to a\nrandom vector or a simple ansatz.\n2. Iteration: At iteration k, BiCGStab updates |ρ(k)⟩⟩by forming a new approximation |ρ(k+1)⟩⟩that ideally satisfies a\nsmaller residual within a Krylov subspace:\n|r(k+1)⟩= L|ρ(k+1)⟩⟩.\n(S48)\nThe method employs additional auxiliary vectors (e.g., search directions and a “shadow” residual) to stabilize convergence\nand avoid breakdowns inherent in BiCGStab.\n3. Convergence: Once the norm of the residual\n\r\r|r(k)⟩\n\r\r is smaller than a prescribed tolerance (e.g., ε = 10−7), the current\napproximation |ρ(k)⟩⟩is taken as the steady state:\n|ρss⟩⟩≡|ρ(k)⟩⟩.\nBy constructing and updating these Krylov subspace approximations, BiCGStab efficiently converges to the zero-eigenvalue\nsolution of L = ˆL\n† ˆL, even in high-dimensional spaces.\nOnce the steady state |ρss⟩⟩is obtained, the expectation value of an observable ˆO is computed via\n⟨ˆO⟩= Tr( ˆO ˆρss)\nTr( ˆρss) ,\n(S49)\nor, equivalently in the vectorized notation,\n⟨ˆO⟩= ⟨⟨ˆO| ˆρss⟩⟩\n⟨⟨I| ˆρss⟩⟩.\n(S50)\nThis approach enables us to compute observables without explicitly constructing the full Hilbert space, making it well-suited\nfor large systems as a baseline.\nG.\nEvaluation of the Optimized Ansatz\nAfter training our transformer density operator ansatz, we obtain an optimized parameter set θθθ that approximates the steady-\nstate density operator. To assess the accuracy of our transformer-based density operator ansatz, we compute the expectation\nvalues of local observables via Monte Carlo sampling, as discussed in Sec. III A.\nFor example, consider the spatial average of the Pauli operator σz,\n⟨σz⟩= 1\nN\nN\n∑\ni=1\n⟨σz\ni ⟩.\n(S51)\nThere are two ways to obtain ⟨σz\ni ⟩: variational approach and exact approach.\nIn the variational approach, the expectation value of an arbitrary operator ˆO is computed as\n⟨ˆO⟩= Tr\n\u0000 ˆρθθθ ˆO\n\u0001\nTr\n\u0000 ˆρθθθ\n\u0001 .\n(S52)\nThis can be recast as a classical expectation value over a probability distribution defined on the diagonal elements of ρθθθ:\n⟨ˆO⟩= ∑\nααα\nPθθθ(ααα) ˜Oρθθθ (ααα),\n(S53)\nwhere the probability distribution over diagonal elements of the density matrix is\nPθθθ(ααα) = ρθθθ(ααα,ααα)\nTr\n\u0000 ˆρθθθ\n\u0001 ,\n(S54)\n\n\n10\nand the local estimator ˜Oρθθθ (ααα) is defined by\n˜Oρθθθ (ααα) = ∑\nβββ\nρθθθ(ααα,βββ)\nρθθθ(ααα,ααα)⟨βββ| ˆO|ααα⟩.\n(S55)\nFor ˆO = σz\ni this becomes\n˜σz\nρθθθ (ααα) = ∑\nβββ\nρθθθ(ααα,βββ)\nρθθθ(ααα,ααα)⟨βββ|σz\ni |ααα⟩,\n(S56)\nso that\n⟨σz\ni ⟩= ∑\nα\nPθθθ(ααα) ˜σz\nρθθθ (ααα).\n(S57)\nThere is another way to calculate the expected value\n⟨σz\ni ⟩= Tr\n\u0000σz\ni ˆρ\n\u0001\n= ⟨⟨σz\ni |ρ⟩⟩,\n(S58)\nwith\n⟨⟨σz\ni |ρ⟩⟩= ∑\nααα,βββ\nρααα,βββ ⟨σz\ni |ααα,βββ⟩,\n(S59)\nand the identification\n⟨σz\ni |ααα,βββ⟩≡⟨βββ|σz\ni |ααα⟩.\n(S60)\nIf one could sum over the entire Hilbert space, this method would yield exact expectation values. However, for systems with\na large number of particles, the Hilbert space grows exponentially, making such a full summation computationally infeasible.\nTherefore, for large systems, we rely on Monte Carlo sampling methods, which offer an efficient and approximate approach to\nevaluating observables without the need to compute the full density matrix.\nIn summary, after optimizing the ansatz, we compute observables (e.g., ⟨σx⟩, ⟨σy⟩, and ⟨σz⟩) by sampling from Pθθθ(ααα)\nin Eq. (S54) and evaluating the corresponding local estimators as described above. We compare our results with baselines\ncalculated using NetKet (see Sec. III F). For small system sizes (N < 7), we employ exact diagonalization of the full Lindblad\nsuperoperator. For larger system sizes, we use the iterative BiCGStab method, which directly solves the steady-state equation\nˆLρ = 0. The excellent agreement between these measurements under our optimized ansatz and benchmark solutions (obtained\nvia exact diagonalization for small systems or iterative solvers for larger systems) confirms that our transformer-based ansatz\naccurately captures the steady-state properties of open quantum systems.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20723v1.pdf",
    "total_pages": 17,
    "title": "Variational Transformer Ansatz for the Density Operator of Steady States in Dissipative Quantum Many-Body Systems",
    "authors": [
      "Lu Wei",
      "Zhian Jia",
      "Yufeng Wang",
      "Dagomir Kaszlikowski",
      "Haibin Ling"
    ],
    "abstract": "The transformer architecture, known for capturing long-range dependencies and\nintricate patterns, has extended beyond natural language processing. Recently,\nit has attracted significant attention in quantum information and condensed\nmatter physics. In this work, we propose the transformer density operator\nansatz for determining the steady states of dissipative quantum many-body\nsystems. By vectorizing the density operator as a many-body state in a doubled\nHilbert space, the transformer encodes the amplitude and phase of the state's\ncoefficients, with its parameters serving as variational variables. Our design\npreserves translation invariance while leveraging attention mechanisms to\ncapture diverse long-range correlations. We demonstrate the effectiveness of\nour approach by numerically calculating the steady states of dissipative Ising\nand Heisenberg spin chain models, showing that our method achieves excellent\naccuracy in predicting steady states.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}