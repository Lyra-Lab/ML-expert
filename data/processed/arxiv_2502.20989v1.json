{
  "id": "arxiv_2502.20989v1",
  "text": "1 \n \n \n \n \nFORECASTING MONTHLY RESIDENTIAL NATURAL GAS \nDEMAND IN TWO CITIES OF TURKEY  \nUSING JUST-IN-TIME-LEARNING MODELING \n \n \n \nBurak Alakent1 (burak.alakent@boun.edu.tr), Erkan Isikli2 (isiklie@itu.edu.tr),  \nCigdem Kadaifci2 (kadaifci@itu.edu.tr), Tonguc S. Taspinar3 (tonguc.taspinar@socar.com.tr) \n \n \n \n1  \nDepartment of Chemical Engineering, Bogazici University, Bebek, 34342 Istanbul, Turkey. \n2  \nDepartment of Industrial Engineering, Faculty of Management, Istanbul Technical University, Macka, \nIstanbul, 34367, Turkey. \n3  \nSOCAR TÃ¼rkiye, Vadistanbul Bulvar, AyazaÄŸa Mahallesi, Azerbaycan Caddesi, No:109-E, 1D Blok, \n34485, SarÄ±yer, Istanbul, Turkey. \n \n \n\n\n2 \n \nABSTRACT \n \nNatural gas (NG) is relatively a clean source of energy, particularly compared to fossil fuels, \nand worldwide consumption of NG has been increasing almost linearly in the last two decades. \nA similar trend can also be seen in Turkey, while another similarity is the high dependence on \nimports for the continuous NG supply. It is crucial to accurately forecast future NG demand \n(NGD) in Turkey, especially, for import contracts; in this respect, forecasts of monthly NGD \nfor the following year are of utmost importance. In the current study, the historical monthly NG \nconsumption data between 2014 and 2024 provided by SOCAR, the local residential NG \ndistribution company for two cities in Turkey, Bursa and Kayseri, was used to determine out-\nof-sample monthly NGD forecasts for a period of one year and nine months using various time \nseries models, including SARIMA and ETS models, and a novel proposed machine learning \nmethod. The proposed method, named Just-in-Time-Learning-Gaussian Process Regression \n(JITL-GPR), uses a novel feature representation for the past NG demand values; instead of \nusing past demand values as column-wise separate features, they are placed on a two-\ndimensional (2-D) grid of year-month values. For each test point, a kernel function, tailored for \nthe NGD predictions, is used in GPR to predict the query point. Since a model is constructed \nseparately for each test point, the proposed method is, indeed, an example of JITL. The JITL-\nGPR method is easy to use and optimize, and offers a reduction in forecast errors compared to \ntraditional time series methods and a state-of-the-art combination model; therefore, it is a \npromising tool for NGD forecasting in similar settings. \n \nKeywords: Forecast Combination, Gaussian Process Regression, Online Learning, Time \nSeries, SARIMA \n \n \n\n\n3 \n \nINTRODUCTION \n \nIn the last few decades, there has been a shift in energy sources from fossil fuels to cleaner \nenergy sources, such as wind and solar energy, mainly due to environmental concerns and \nrelated government regulations. However, these latter sources are dependent on weather \nconditions and require integration with grid technologies for continuous power generation. \nNatural gas (NG), typically, consists of (up to) ~95% of methane and 2-2.5% ethane-hexane+, \nwith the remainder consisting of nitrogen, CO2, oxygen and hydrogen, making NG closer to the \ncleaner side of the energy spectrum than fossil fuels. NG power plants are easy to build and \nhighly reliable, making them invaluable for â€œcleanâ€ energy production. On the other hand, most \ncountries depend on imports to maintain their NG supplies, and there is a delicate balance \nbetween imports and domestic demand. Storing excess imported gas above actual demand is \ndifficult and would result in economic losses, while importing less than actual demand could \nresult in a nationwide shortage. Therefore, accurate NGD predictions (NGDPs) are of utmost \nimportance. \nNGDP studies can be grouped by application area, which can vary from the whole world to \ncities, or by forecast horizon, which can be divided into (i) short-term, i.e., hourly to daily basis, \n(ii) medium-term, i.e., weekly to monthly basis, (iii) long-term, i.e., annual to decade basis, (iv) \nmulti-term forecasting methods [1]. The focus in the literature has shifted from annual to daily \nforecasting methods from the 1970s to the present, possibly in line with advances in machine \nlearning and forecasting methods; however, the share of medium-term studies in the entire \nNGDP literature has decreased from ~20% to ~15% during the same period [2], and the number \nof studies generating monthly forecasts is rather limited [3,4]. This may, in fact, be due to the \ndominant factors at different forecast horizons; monthly to quarterly forecasts are mostly and \nalmost equally affected by temperature and economy [2], and both variables are difficult to \npredict for monthly periods [4,5]. \n \nA Brief Survey of NGDP Models \nHere, we adopt the perspective proposed by a review study [2] for the analysis of the historical \nevolution of the NGPD literature. Early studies on short-time NGDP in the 1960s focused on \nusing statistical models with weather-related variables as features, while recognizing the time-\nseries nature, particularly the seasonality of the data, and suggesting the inclusion of lagged \ninput and output variables in the models [6,7]. The influence of time series models, proposed \nby Box and Jenkins [8], on NGDP became particularly evident during the 1970s and 1980s. For \n\n\n4 \n \nmedium-term forecasting, a linear regression model consisting of heating degree days (HDD), \ncooling degree days (CDD), price of NG (PoG), number of customers (NoC), and NGD with a \n12-month lag as inputs was used to obtain monthly NGDPs for residential customers in the US \n[9]. Monthly NGD for Taiwan was modeled using nonstationary seasonal autoregressive \nintegrated moving average exogenous input (SARIMAX) models with lagged terms of \ntemperature and PoG [10].  \nThe priority given to nonlinear models increased by the late 1990s, possibly as a result of \nthe advent of statistical learning methods [2]. Monthly NGDPs for Belgium were performed \nusing an artificial neural network (ANN) model including exogenous features such as \ntemperature, oil price and NoC [11]. The generation of one-to-three-day-ahead NG load \nforecasts was achieved by using a combination of ANN models, which incorporated the \nprevious output, temperature, and wind speed as features. The integration of learners and the \nupdating of learner weights during forecasting were shown to enhance prediction accuracy [12]. \nSimilarly, combination of ANN models was shown to provide more accurate daily-to-weekly \nforecasts of NGD in Poland, compared to linear and quadratic regression models [13]. In \nanother study, daily NGD loads for a one-month period were forecasted using two years of \nhistorical data in a Support Vector Regression (SVR) model, in which daily temperature \nforecasts during the test period were taken as simple averages of those of the previous two years \nduring the same period [14]. In short-term NGD forecasting, Gil and Deferrari used a semi-\nempirical model to describe the relation of NGD with the number of users and temperature for \nshort-term forecasts for Argentina [15].  \nIn the last ~15 years, statistical models, time series methods and machine learning tools \nhave been combined in coherent frameworks for accurate NGDPs. In short-term NGD \nforecasting, features related to weather, number of subscribers, and previous demand values \nhave often been used to train learners, such as Linear Regression, Local SVR, Gradient \nBoosting (GB), and ANN models [16â€“18]. In recent studies, special attention has been given to \nthe â€œdata leakageâ€ problem, especially related to weather-related variables; i.e., forecasts of \nexogenous variables are used in models for predicting future NGD values. For instance, one \nhour-to-60 hours ahead forecasts of temperature have been used in a model consisting of \ntemperature-independent and temperature-dependent terms [19], and in a long short-term \nmemory (LSTM) ANN model [20]. More recently, various deep learning architectures, such as \nthe combination of convolutional NN (CNN) with attention modules to extract short-time \nrelations, and LSTM models to capture longer-time relations, have been tailored for NGDP of \nvarious cities in China using training sets of moderate size, e.g., ï¾200-350 observations [21,22]. \n\n\n5 \n \nAmong the few medium-term forecasting studies, mutual information on the exogenous \nvariables, such as population, gross domestic product (GDP), PoG, and HDD, was used to select \na subset of features to be incorporated into local linear neuro-fuzzy learner and Hodrickâ€“\nPrescott filter to determine U.S. NGDPs for a 12-month period [23]. Extreme Learning Machine \n(ELM) and ANN models were shown to yield accurate monthly residential NGDPs for Karaj \n[24] and Tehran [25] in Iran, using weather-related variables as exogenous features; however, \nthese studies used actual weather data during the test period, which makes the models difficult \nto use in practice. \n \nNGDP Studies for Turkey \nDemand for NG in Turkey is constantly increasing, and residential sector has accounted for \n>35% of its domestic NG consumption in 2023 [26]. While Turkey has been investing on NG \nresources, a large portion of Turkeyâ€™s source of NG is imports from Russia, Azerbaijan, Iran \nvia the pipelines, and shipped Liquefied NG from Nigeria and Algeria [27]. NGDP for the entire \ncountry and individual cities in Turkey has been studied for the last ~25 years using various \nstatistical and machine learning methods. One-day-ahead forecasts were conducted using \nSARIMAX and ANN models, and moving window linear regression model for Sakarya [28,29], \nmultivariate adaptive regression splines (MARS), Lasso and Ridge Regression for Ankara [30â€“\n32], Fourier Series for Istanbul, Ankara and Eskisehir [33], SARIMAX, nonlinear ARX \n(NARX), ANN and LSTM models for Turkey [34], and ANN model for Istanbul [35]. The \ninput features in these studies usually consisted of past demand values, instantaneous weather-\nrelated variables, the number of gas users, exchange rates and other economic variables. \nIn one of the earliest medium-term NGDP studies in Turkey, regression models, including \nHDD and previous prediction residuals as input features, were shown to improve monthly \nprediction accuracy compared to a single model for Eskisehir [36]. NGD in Istanbul was \nforecasted for a seven-month test period using ANN models, incorporating separately coded \nmonths, NoC and monthly average customers [37]. Relation of monthly NGD in Turkey and \nmeteorological variables was examined using metaheuristic algorithms on additive regression \nmodels [38]. In a long-term NGDP analysis of Turkey spanning from 2005 to 2023, employing \ndifferent base temperature scenarios, it was predicted that a 1oC change in the HDD base \ntemperature would result in ~10% change in NG consumption [39]. In another study, daily \nNGDPs were modeled using HDD, exchange rate, and PoG as features in linear regression \nmodels. These models were developed for the purpose of annual forecasting, with the \nunderstanding that they may be applicable under stable and unstable economy scenarios [40]. \n\n\n6 \n \nSubset regression methods were used to select features among gross national product (GNP), \npopulation, and growth rate [41], while simulated annealing (SA) [42], hybrid SA+genetic \nalgorithm (GA) [43], artificial bee colony algorithm [44], and cause-effect relationship-based \nGrey-prediction model [45] were used for forecasting Turkeyâ€™s NGD under different scenarios. \nIn contrast to scenario-based forecasts, long-term (>15 years) point predictions of annual NGD \nwere obtained via a high-order ARI model [46], and modified logistic and linear equations, \ninvolving the (forecasted) population as the single predictor [47]. In a recent study, the \nfractional nonâ€‘linear grey Bernoulli model, which was optimized by grey wolf optimization, \nwas shown to yield better one-year ahead forecasting results compared to linear regression and \nARIMA models using a limited data set of only 14 years [48]. A synopsis of NGDM studies \nconducted in Turkey is provided in the Appendix. \n \nNGDP Studies Focusing on Medium-Term Forecasting Horizon \nThe extant studies have notable value for policy decisions and design purposes; however, most \nof them, particularly those pertaining to Turkey, use exogenous-variable models. In these \nmodels, either the actual (realized) values of the input features were used during the test period \n(ex-post forecasts), or the details of how input features were obtained during the test period \nwere discussed superficially. One-day ahead estimations of variables related to weather might \nbe helpful for short-term forecasting; however, forecasts of weather-related variables for longer \nthan a week period would be less reliable, thereby decreasing the accuracy of exogenous \nvariable models for monthly NGDP [15,49] Scenario-based approaches, which use a palette of \ndifferent forecast sets for future values of input features, have proven effective in long-term \nforecasting. However, the low-frequency change of exogenous variables, such as GDP, coupled \nwith the challenging nature of accurately predicting variables like the inflation rate, poses a \nsignificant challenge in obtaining reliable monthly point NGDPs [50]. Consequently, methods \nemployed for monthly forecasting (ex-ante forecasts) are rather limited in number and can be \nclassified into two groups: (i) models that use the forecasted values of exogenous variables for \nfuture points and (ii) models that use previous NGD values, along with the possible inclusion \nof calendar information. \nAs an example of the former approach, Gil and Deferrari expanded their daily temperature \npredictions to monthly predictions by assuming a normal distribution of daily temperatures \naround a monthly mean, approximated by a simple periodic function within a year [15]. In a \nsimilar study, where one-day and one-month ahead residential NGD in Turkey were predicted \nusing linear regression models with HDD, CDD, price and holiday effects as input features, \n\n\n7 \n \ndaily temperature was estimated using Ornstein-Uhlenbeck stochastic process models [51]. A \nnonlinear model including the predicted seasonal temperature effect was used to determine \nmonthly NGDP in the Czech Republic [52].  \nThe second approach entailed the integration of linear Hinges models to describe the \nseasonal behavior, augmented by a trend component, to determine monthly NGDP in Spain. \nThe incorporation of explanatory variables proved instrumental in elucidating weekly effects \n[5]. ANN and adaptive neuro-fuzzy inference system (ANFIS) models using only previous \nNGD values without auxiliary features were shown to yield higher prediction accuracy \ncompared to ARIMA model in forecasting NGD over a 15-week-period in Turkey [53]. In a \nstudy on forecasting monthly NG consumption for Istanbul, Turkey, over a one-year period, \nthe authors used a set of features comprising the seasonal index, NG consumption at lag 12 \n(month), temperature, city population, and PoG [54]. NGDPs using an adjusted seasonal grey \nmodel over a four-year period [55], and Holt-Winters exponential smoothing and SARIMAX \nmodels for monthly periods [56], both yielded acceptable forecast accuracy for Turkey. \nNonlinear grey Bernoulli model based on Hodrick-Prescott (HP) filter was shown to yield the \nsmallest prediction error among other seasonal models in predicting monthly NGD values of \nEuropean Union countries [4]. A comparison of ARIMA, NARNN, SVR, and LSTM models \nusing the weekly NGD in Turkey during Covid pandemic, was conducted in a recent study for \none-week ahead forecasts [57]. A study that combined CNN and LSTM models for forecasting \ndaily NGDPs in Chinese cities used only previous NGD values for model training, \ndemonstrating that deep learning architectures can also be used for medium-term NGDPs under \na similar setting [21]. Quarterly NG consumption data in China was analyzed using moving \naverage and seasonal indices to yield NG consumption forecasts until 2035 [58]. A recent study \non an ensemble of ETS and SARIMA models, obtained via resampling the remainder of \nSeasonal-Trend decomposition using LOESS (STL) on NGD for 18 European countries, \nyielded a good prediction accuracy for a 12-month ahead period [3]. The Salp swarm algorithm, \nin conjunction with Extreme Gradient Boosting (XGB), was employed to forecast five-month \nahead NGDPs for the UK and the Netherlands within a moving window (MW) frame [59]. \n \nPerspective of the Present Study \nThe monthly NGD data of Bursa and Kayseri, two cities in Turkey, from 2014 to 2024 were \nprovided by The State Oil Company of Azerbaijan Republic (SOCAR), an Azerbaijani local \nNG distribution company employed in Turkey. SOCAR was established in 1992 with the \nmission of managing the nationâ€™s oil and natural gas industry. This globally recognized energy \n\n\n8 \n \ngiant plays a pivotal role in delivering the abundant energy resources of the Caspian Sea to \nglobal markets. SOCARâ€™s activities span a broad range, including: (i) the exploration and \noperation of Azerbaijanâ€™s oil and natural gas reserves through the use of advanced technologies. \nAs of 2023, SOCARâ€™s production from reserves in the Caspian Sea amounted to approximately \n34 million tons of oil and 35 billion cubic meters of natural gas annually. SOCARâ€™s role in the \nrefining of oil and the production of energy products is also noteworthy. The companyâ€™s BaÅŸkal \nand Yeni Heydar Aliyev refineries possess a combined processing capacity of 7 million tons of \ncrude oil per year. Furthermore, SOCAR has strategically invested in transportation \ninfrastructure to facilitate the delivery of energy products to domestic and international markets, \nestablishing a robust logistics network comprising pipelines and terminals. Approximately 50 \nmillion tons of oil are transported annually via the Baku-Tbilisi-Ceyhan Pipeline. Additionally, \nSOCAR Turkey Research, Development and Innovation Inc. (SOCAR Turkey R&D), \nestablished in 2019, develops innovative, sustainable, environmentally friendly, and market-\noriented products and digital technologies, and provides R&D services to all its stakeholders \nwith its deep-rooted experience in the energy sector. SOCAR has invested in various projects \nin Turkey, Georgia, and other countries to be among the global players in the energy sector. \nNotably, SOCAR has augmented its regional effectiveness through substantial investments, \namounting to a total of 18.5 billion dollars. These investments have been made through \ninitiatives such as the acquisition of PETKÄ°M in Turkey and its subsequent integration with the \nrecently established STAR Refinery, the completion of the Trans Anatolian Natural Gas \nPipeline (TANAP) Project, and the acquisition and operation of natural gas distribution \ncompanies such as Bursagaz and Kayserigaz. \nTo comply with the specifications of the import agreements, it is necessary to make monthly \nNGD forecasts for the upcoming year. In the present study, a novel ex-ante NGDP method, \nnamely Just-In-Time-Learning-Gaussian Process Regression (JITL-GPR), is proposed. The \nJITL-GPR model uses only the previous NGD values and calendar data, offering a more \nnuanced interpretation from an online learning perspective as opposed to batch learning. In \nbatch learning, a constant model (or an ensemble of models) is constructed from a given \n(historical) dataset, and the response variable value of a new query point is predicted using this \nmodel. Conversely, in online learning, a model is continuously adapted or changed upon \nvarying operating conditions [60â€“62]. The accuracy of a constant model relating inputs to \noutputs, derived using batch learning, is likely to degrade over time; therefore, relaxing the \nconstant model assumption is an advantage of online learning [63]. Indeed, online learning \nmethods have already been successfully used (rather implicitly) for NGDP: Linear learning \n\n\n9 \n \nmethods were found to demonstrate enhanced efficacy when used simultaneously with \nrecursive [16], MW [1,19,59], or nearest neighbor (NN) [17] methods. Furthermore, the \nprediction accuracy of nonlinear learning methods was shown to increase when trained in an \nadaptive manner [12].  \nThe novelties of JITL-GPR are as follows: (i) Instead of extracting trend-seasonality \ninformation directly and globally from the entire data, as most of the time series methods do, \nthis information is captured indirectly through the selection of a convenient â€œwindowâ€ of \nobservations on a two-dimensional grid of the historical year-month plane. (ii) A convenient \nkernel is designed to exploit the trend-seasonality information to be used in GPR. (iii) The \nprocedure is repeated for each query point, akin to Just-In-Time-Learning (JITL) or local \nlearning [64], while recent information is also included, similar to a MW method [65,66]. \nHence, the method yields predictions consistent with previous years, while adapting to changes \nin recent months. In comparison to conventional time series methodologies such as SARIMA \nand ETS, JITL-GPR was shown to exhibit superior accuracy in terms of NGDP estimation. \nSection 2 encompasses a concise overview of Time Series Methods, GPR and JITL. Following \nthe presentation of the dataâ€™s specifics, a comprehensive discourse on the results obtained using \ntime series methods and JITL-GPR is provided in Section 4. The final section proposes several \nfuture research directions. \n \n \n\n\n10 \n \nTHEORETICAL BACKGROUND \n \nTime Series Modeling \nSeasonal and Trend Decomposition using LOESS (STL) \nThe pattern of a time series can be decomposed into sub-patterns in many cases, thereby \ndefining the various components of the series individually. This type of decomposition \nfrequently enhances the accuracy of forecasting by facilitating a more profound comprehension \nof the seriesâ€™ behavior [67]. Consequently, decomposition of a time series is a useful instrument \nfor the analysis of a series and the subsequent selection of a method for its modeling. In the \npresent study, we employed the STL (seasonal-trend decomposition) procedure that consists of \nan inner loop nested inside an outer loop. Developed by [68], this non-parametric additive \ndecomposition method provides a simple design based on local regression smoothing (LOESS) \nthat is flexible in specifying the amounts of variation in the trend and seasonal components. It \nis widely regarded as a valuable tool for elucidating the underlying patterns in datasets that \nexhibit substantial seasonal effects. A notable strength of STL lies in its capacity to handle \nseries with missing values, yielding robust trend and seasonal components that are not distorted \nby transient, aberrant behavior in the data [68].  In other words, it can accommodate non-fixed \nseasonal patterns and handle non-linear trends in the presence of outliers. The general form of \nthe STL model can be expressed as follows in Equation 1: \n \nğ‘¦ğ‘¡= (ğ›¼+ ğ›½ğ‘¡) + ğ‘†ğ‘¡+ ğœ€ğ‘¡= ğ‘¦Ì‚ğ‘¡+ ğœ€ğ‘¡  \n \n \n \n \n \n \n \n \n(1) \n \nwhere ğ‘¦ğ‘¡ is the actual value of the time series at time point ğ‘¡; ğ›¼+ ğ›½ğ‘¡ corresponds to the trend \ncomponent; ğ‘†ğ‘¡ is the seasonal index at time point ğ‘¡; ğœ€ğ‘¡ is the error component at time point ğ‘¡. \nWhile STL can manage any kind of seasonality (i.e., weekly, monthly, or quarterly), allowing \nusers to determine the trendâ€™s smoothness and enable the seasonal component to alter over time, \nit only offers additive decomposition capabilities and does not automatically handle calendar \nor trading day variations [69]. It can be implemented easily since it relies on numerical methods \nrather than mathematical modeling [70]. \n \nSeasonal Autoregressive Integrated Moving Average (SARIMA) \nAlong with smoothing methods, ARIMA is one of the most widely used and effective time \nseries modeling approaches. It aims to describe the behavior of a time series based on \nautocorrelations [71]. This type of models posits that future values of a time series are generated \n\n\n11 \n \nfrom a linear function of its past observations and some white noise errors. The model is \nspecified by three parameters: ğ‘, the order of the autoregressive component; ğ‘‘, the order of the \ndifferencing required for the series to be mean stationary; and ğ‘, the order of the moving \naverage component. Equation 2 illustrates an ARIMA(ğ‘, ğ‘‘, ğ‘) model: \n \nÎ”ğ‘‘ğ‘¦ğ‘¡= ğœ‡+ ğœ™1Î”ğ‘‘ğ‘¦ğ‘¡âˆ’1 + ğœ™2Î”ğ‘‘ğ‘¦ğ‘¡âˆ’2 + â‹¯+ ğœ™ğ‘Î”ğ‘‘ğ‘¦ğ‘¡âˆ’ğ‘âˆ’ğœƒ1ğœ€ğ‘¡âˆ’1 âˆ’ğœƒ2ğœ€ğ‘¡âˆ’2 âˆ’â‹¯âˆ’ğœƒğ‘ğœ€ğ‘¡âˆ’ğ‘  \n(2) \n \nwhere Î”ğ‘‘ğ‘¦ğ‘¡ is the d-th difference of the original series, ğœ™ğ‘– is the parameter for the ith \nautoregressive term, ğœƒğ‘— is the parameter for the jth moving average term, and ğ‘–= 1, ğ‘\nÌ…Ì…Ì…Ì…Ì… and ğ‘—=\n1, ğ‘\nÌ…Ì…Ì…Ì…Ì…. For a comprehensive exposition on the underpinnings of ARIMA-type models, readers are \ndirected to the seminal contributions of [72]. The ARIMA methodology was extended to \nincorporate seasonal trends along with primary stochastic trend to facilitate the modeling of \ntime series characterized by periodicity and regular patterns across diverse time scales. Known \nas SARIMA, this type of model is denoted by ARIMA(ğ‘, ğ‘‘, ğ‘)(ğ‘ƒ, ğ·, ğ‘„)ğ‘ , where ğ· denotes the \ndegree of seasonal differencing, ğ‘ƒ denotes the number of seasonal autoregressive components, \nğ‘„ denotes the number of seasonal moving average components, and ğ‘  denotes the seasonal \ncycle (e.g., for monthly data with yearly seasonality, ğ‘ = 12). Expressing a SARIMA model \nexplicitly is tedious; therefore, lag polynomials are most often used. A scrambled mathematical \nrepresentation of an ARIMA(0,1,1)(0,1,1)12 model is formulated in Equation 3 below as an \nexample: \n \nÎ”12\nğ·Î”ğ‘‘ğ‘¦ğ‘¡= ğœ‡+ Î˜1ğœƒ1ğœ€ğ‘¡âˆ’13 âˆ’Î˜1ğœ€ğ‘¡âˆ’12 âˆ’ğœƒ1ğœ€ğ‘¡âˆ’1 + ğœ€ğ‘¡  \n \n \n \n \n \n \n(3) \n \nwhere Î”12\nğ·ğ‘¦ğ‘¡ is the D-th seasonal difference of the original series and Î˜1 is the parameter for \nthe seasonal moving average component. \n \nExponential Smoothing State Space Models (ETS)  \nState space models, first introduced in the late 1970s, offer a high degree of flexibility in the \nspecification of parameters for level, trend, and seasonal components. This is primarily \nachieved through the use of two equations: the observation equation and the state equation [73]. \nAs intrinsically non-parametric models, they are reliable and computationally efficient. For a \nmore in-depth exploration of this topic, readers are directed to [74]. A notable distinction of \nstate space models for exponential smoothing is the inclusion of an error component, in addition \nto the trend and seasonal components, which distinguishes them from the smoothing methods \npreviously introduced in the related literature. This class of models is commonly denoted by \n\n\n12 \n \nthe acronym ETS(ğ‘¥, ğ‘¦, ğ‘§), where ğ‘¥ designates whether errors are additive (A) or multiplicative \n(M), ğ‘¦ indicates the trend is additive or multiplicative nature, and ğ‘§ denotes the seasonality is \nadditive or multiplicative character. To illustrate, the following two representations of the \nETS(M,A,N) model are provided below. The general representation is given in Equations 4-6, \nwhereas the state space representation is given in Equations 7-8. This is somewhat equivalent \nto Holtâ€™s linear method, but the errors in this case are multiplicative rather than additive. \n \nForecast:  \n \nğ‘¦Ì‚ğ‘¡= (ğ¿ğ‘¡âˆ’1 + ğµğ‘¡âˆ’1)(1 + ğœ€ğ‘¡)  \n \n \n \n \n \n(4) \nLevel:  \n \n \nğ¿ğ‘¡= (1 + ğ›¼ğœ€ğ‘¡)(ğ¿ğ‘¡âˆ’1 + ğµğ‘¡âˆ’1)  \n \n \n \n \n \n(5) \nTrend:  \n \n \nğµğ‘¡= ğ›¼ğ›½(ğ¿ğ‘¡âˆ’1 + ğµğ‘¡âˆ’1)ğœ€ğ‘¡+ ğµğ‘¡âˆ’1  \n \n \n \n \n(6) \nObs. Equation:  \nğ‘¦ğ‘¡= [1\n1]\nğ‘‡\nğ’™ğ‘¡âˆ’1(1 + ğœ€ğ‘¡) \n \n \n \n \n \n \n \n(7) \nState Equation: \nğ’™ğ‘¡= [1\n1\n0\n1] ğ’™ğ‘¡âˆ’1 + [1\n1]\nğ‘‡\nğ’™ğ‘¡âˆ’1 [ğ›¼\nğ›½] ğœ€ğ‘¡  \n \n \n \n(8) \n \nwhere ğ‘¦Ì‚ğ‘¡= ğ¸[ğ‘¦ğ‘¡|ğ‘¦ğ‘¡âˆ’1, ğ‘¦ğ‘¡âˆ’2, â€¦ , ğ‘¦1] is the one-step-ahead forecast, ğ¿ğ‘¡ denotes the baseline value \nof the series at time point ğ‘¡, ğ’™ğ‘¡= [ ğ¿ğ‘¡\nğµğ‘¡ ] is the state vector and the one-step ahead forecast errors \n(estimation data), ğœ€ğ‘¡=\nğ‘¦ğ‘¡âˆ’(ğ¿ğ‘¡âˆ’1+ğµğ‘¡âˆ’1) \nğ¿ğ‘¡âˆ’1+ğµğ‘¡âˆ’1\n , follow a normal distribution with zero mean and constant \nvariance. The selection of the ETS model is contingent upon the characteristics of the time \nseries, and estimation of model parameters is frequently executed through the Maximum \nLikelihood Estimation (MLE) technique. This approach involves maximizing the â€œlikelihoodâ€ \nand thereby minimizing the sum of squared errors while optimizing the parameters. Table 1 \npresents a selection of the most commonly used ETS methods, along with their respective \nnotations and references. A comprehensive overview of the equations in state space for every \nmodel in the ETS framework can be found in [69]. \n \nTable 1. Commonly used ETS models \nMethod \nETS Notation \nReference \nSimple Exponential Smoothing (SES) \nETS(A,N,N) \n[75] \nHoltâ€™s Linear Trend method \nETS(A,A,N) \n[76] \nHolt-Wintersâ€™ Additive Method \nETS(A,A,A) \n[77] \nHolt-Wintersâ€™ Multiplicative Method \nETS(M,A,M) \n[77] \nHolt-Wintersâ€™ Damped Method \nETS(A,Ad,A) \n[78] \n \n\n\n13 \n \nTrigonometric Box-Cox transform, ARMA errors, Trend, and Seasonal Components (TBATS): \nIntegrating the principles of ETS and other methodologies, TBATS handle multi-frequent \nseasonality and non-linear trends. TBATS was introduced by [79] as an extension of ETS to \nmodel multiple seasonal patterns in a time series simultaneously. In the abbreviation TBATS, \nthe first letter of each word indicates a particular aspect of the model: B = Box-Cox \ntransformation, A = ARMA errors, T = Trend, and S = Seasonality. The first T indicates that a \ntrigonometric formulation was adopted to model seasonality. The method employs a Box-Cox \ntransformation to stabilize variance and incorporates ARMA components to enhance the \naccuracy of error modeling. For a detailed exposition of TBATS, readers are directed to consult \n[80]. \n \nThe Aggregated Forecast Through Exponential Reweighting (AFTER): This simple and \nefficient method was initially proposed by [81] as a means of combining heterogeneous forecast \nmodels. Subsequent modifications have been made [82], and the method has been applied to \nvarious cases [83] since its initial introduction. Considering individual model performance \nhistory and focusing on minimizing prediction error over time, AFTER adjusts the combination \nweights, which undergo an exponential decay that is based on the performance of the models. \nThis approach stands in contrast to classical forecast combination methods, which are limited \nto fixed or predetermined weights. For a more detailed description of AFTER, the reader is \nreferred to [84] and [85]. \n \nGaussian Process Regression (GPR)  \nBayesian linear regression (BLR) is a parametric model, consisting of basis functions evaluated \nat finite training points. Given a set of N data points {(ğ’™1, ğ‘¦1), (ğ’™2, ğ‘¦2) â€¦ (ğ’™ğ‘, ğ‘¦ğ‘)} with ğ’™ğ‘›ğœ–â„ğ‘ \nand ğ‘¦ğ‘›ğœ–â„, the response variable ğ‘¦ğ‘› is assumed to be normally distributed about the regression \nfunction, which is determined by the product of a basis function ğœ™(âˆ™) evaluated at ğ’™ğ‘› and a \nparameter vector ğ’˜ğœ–â„ğ‘. This model is augmented with a Gaussian prior, specified by a zero \nmean and an identity covariance matrix: \n \nğ’˜~ğ‘(ğŸğ‘, ğœğ‘¤\n2ğ‘°ğ‘),   ğ‘¦ğ‘›|ğ’™ğ‘›= ğ‘(ğœ™(ğ’™ğ‘›)ğ‘‡ğ’˜, ğœğœ€\n2)                               (9) \n \nIn the above equation, ğŸğ‘ and ğ‘°ğ‘ represent the zeros vector and identity matrix, respectively, \neach with p elements. Meanwhile, ğœğ‘¤\n2 and ğœğœ€\n2 represent the variance terms for the prior of w \n\n\n14 \n \nand the independent additive noise, respectively. Defining the regression function ğ‘“(ğ’™ğ‘›, ğ’˜) =\nğ¸ğ‘¦{ğ‘¦|ğ’™ğ‘›, ğ’˜} yields:  \n \nğ‘“(ğ’™ğ‘›, ğ’˜) = ğœ™(ğ’™ğ‘›)ğ‘‡ğ’˜                                                      (10) \nğ’‡(ğ’˜) = [ğœ™(ğ’™1)ğ‘‡ğ’˜   ğœ™(ğ’™2)ğ‘‡ğ’˜   â€¦ ğœ™(ğ’™ğ‘)ğ‘‡ğ’˜]ğ‘‡= ğš½(ğ‘¿)ğ’˜                    (11)        \n \nIn the equation above, ğ‘¿= {ğ’™ğŸ ğ’™ğŸ, â€¦ ğ’™ğ‘µ} and ğš½(ğ‘¿)ğœ–â„ğ‘µÃ—ğ’‘. Since ğ’‡ is a linear function of \nğ’˜ with isotropic Gaussian prior, the following relations can be written [86]:  \n \nğ¸ğ’˜{ğ’‡(ğ’˜)} = ğŸğ‘µ                                                                  (12) \nğ‘ğ‘œğ‘£\nğ’˜(ğ’‡(ğ’˜)) = ğ¸ğ’˜{ğ’‡(ğ’˜)ğ’‡(ğ’˜)ğ‘»} = ğœğ‘¤\n2 ğš½(ğ‘¿)ğš½(ğ‘¿)ğ‘»                           (13) \n \nThe inner product between the basis functions, as depicted in the equation above, suggests \na kernel definition, such that  ğ‘˜(ğ’™ğ‘¢, ğ’™ğ‘£) = ğœğ‘¤\n2 ğœ™(ğ’™ğ‘¢)ğ‘‡ğœ™(ğ’™ğ‘£) for any two points ğ’™ğ‘¢ and ğ’™ğ‘£ in \nthe dataset. This yields ğ‘ğ‘œğ‘£(ğ’‡) = ğ‘²(ğ‘¿), where ğ‘²(ğ‘¿) âˆˆâ„ğ‘µÃ—ğ‘µ is the Gram matrix. From the \nfunction-space perspective, any subset of random variables sampled from the GP ğ‘“(ğ’™) has a \njoint Gaussian distribution with a mean of ğ‘š(ğ’™), that has been taken to be equal to zero in the \nweight-space perspective discussed above, and a covariance function defined via a kernel \nğ‘˜(ğ’™, ğ’™â€²) [87]: \n \nğ‘“(ğ’™)~ğºğ‘ƒ(ğ‘š(ğ’™), ğ‘˜(ğ’™, ğ’™â€²)) \n \n \n \n \n \n \n       (14) \n \nIn contrast to the BLR approach, the basis function in GPS is evaluated at an infinite number \nof points GPR, thereby yielding a nonparametric model. The resulting covariance relation \nshows that the change of the regression function at different points in feature space, i.e., the \ncovariance, is completely determined by the kernel function defined in the original feature \nspace. This implies that the behavior of the function will be correlated for â€œsimilarâ€ points in \nthe original space. Using n = 1, 2, â€¦N for the training set and n = q for a test point, the marginal \ndistributions of  ğ’š= [ğ‘¦1  ğ‘¦2 â€¦ ğ‘¦ğ‘]ğ‘‡ (also called the evidence function),  ğ’šğ‘+1 =\n[ğ‘¦1  ğ‘¦2 â€¦ ğ‘¦ğ‘ ğ‘¦ğ‘]ğ‘» and the conditional distribution of ğ‘¦ğ‘|ğ’š may be determined using matrix \nnormal distribution properties as follows: \n \n \n \n \n\n\n15 \n \nğ‘(ğ’š|ğ‘¿) = âˆ«ğ‘(ğ’š|ğ’‡, ğ‘¿) ğ‘(ğ’‡|ğ‘¿)ğ‘‘ğ’‡ ~ ğ‘(ğŸğ‘, ğ‘ªğ‘) with ğ‘ªğ‘= ğœğœ€\n2ğ‘°ğ‘+ ğ‘²(ğ‘¿)  \n(15) \n                      ğ‘(ğ’šğ‘+1|ğ‘¿, ğ‘¥ğ‘) = âˆ«ğ‘(ğ’šğ‘+1|ğ’‡, ğ‘¿, ğ‘¥ğ‘) ğ‘(ğ’‡|ğ‘¿, ğ‘¥ğ‘)ğ‘‘ğ’‡ ~ ğ‘(ğŸğ‘+1, ğ‘ªğ‘+1), with  \nğ‘ªğ‘+1 = [\nğ‘ªğ‘\nğ‘˜(ğ‘¿, ğ‘¥ğ‘)\nğ‘˜(ğ‘¿, ğ‘¥ğ‘)ğ‘‡\nğœğœ€\n2 + ğ‘˜(ğ‘¥ğ‘, ğ‘¥ğ‘)]\n(ğ‘+1)Ã—(ğ‘+1)\n               (16) \nğ‘¦ğ‘|ğ’š~ğ‘(ğ‘˜(ğ‘¿, ğ‘¥ğ‘)\nğ‘‡ğ‘ªğ‘\nâˆ’1ğ’š, ğœğœ€\n2 + ğ‘˜(ğ‘¥ğ‘, ğ‘¥ğ‘) âˆ’ğ‘˜(ğ‘¿, ğ‘¥ğ‘)\nğ‘‡ğ‘ªğ‘\nâˆ’1ğ‘˜(ğ‘¿, ğ‘¥ğ‘)              (17) \n \nIn the above formulation, ğ‘ªğ‘ should be positive definite. As a result, the expected value of \nthe response variable at ğ’™ğ‘ is a linear combination of response variables in the training set \nweighted with respect to the scaled kernel distance to the query point (67):  \n \nğ¸{ğ‘¦ğ‘|ğ‘¥ğ‘, ğ‘¿, ğ’š} = ğ‘“(ğ‘¥ğ‘|ğ‘¿, ğ’š) = ğ‘˜(ğ‘¿, ğ’™ğ‘)\nğ‘‡ğ‘ªğ‘\nâˆ’1ğ’š= âˆ‘\nğ‘ğ‘›ğ‘˜(ğ’™ğ‘›, ğ’™ğ‘)ğ‘¦ğ‘›\nğ‘µ\nğ’=ğŸ\n         (18) \n \nA typical example of a kernel function is the radial basis function (RBF) ğ‘˜(ğ’™ğ‘¢, ğ’™ğ‘£) =\nğœğ‘“\n2 ğ‘’ğ‘¥ğ‘(\nâ€–ğ’™ğ‘¢âˆ’ğ’™ğ‘£â€–22\n2ğœğ‘™\n2\n), in which ğœğ‘“\n2 and ğœğ‘™\n2 denote the variance of the function (signal) and the \nsquared length scale of the kernel, i.e., the range of the influence of the kernel function in the \ninput space. It is possible to determine the optimum values of the kernel parameters, i.e., \nhyperparameters of the GPR. Using  ğœ½= [ğœƒ1 ğœƒ2 â€¦ ğœƒğ‘˜] as hyperparameters of GPR, e.g.,         \nğœ½= [ğœğ‘“\n2 ğœğ‘™\n2] for the RBF kernel defined above, one may obtain (using Equation 15) the \nfollowing log-likelihood: \nlog ğ‘(ğ’š|ğœ½, ğ‘¿) = âˆ’\n1\n2 |ğ‘ªğ‘|-\nğŸ\nğŸ ğ’šğ‘»ğ‘ªğ‘µ\nâˆ’ğŸğ’šâˆ’\nğ‘\n2 log 2ğœ‹  \n \n                  (19) \n \nThe function above may be minimized using gradient-based techniques; however, \ninitialization of the hyperparameters is important due to the non-convex nature of the log-\nlikelihood function. Lastly, for a full Bayesian treatment, it is essential to define the priors of ğœ½ \nand incorporate log ğ‘(ğœ½) into log ğ‘(ğ’š|ğœ½, ğ‘¿). \n \nJust-In-Time-Learning (JITL) \nThe development of JITL can be traced back to two seminal studies, in which local weighted \nregression and instance-based learning were proposed. Contrary to global models that seek to \ndescribe data across the entire domain using a single representation, local weighted regression \nwas proposed for approximating nonlinear data over different regions [88]. Similarly, instance-\nbased classification was based on the label of samples similar to the query point, instead of a \nsingle decision rule [89]. The accuracy of instance-based predictions for real-valued functions \n\n\n16 \n \nunder realistic conditions was also theoretically justified [90]. These studies merged in JITL (or \nLazy Learning) modeling, which has been employed in numerous modeling and process control \napplications [91â€“93] , including time series analysis [64]. \nIn the context of JITL modeling, the objective is to identify a subset of training points Î¦, \nthat exhibits â€œsimilarityâ€ to the given query point ğ±ğ‘ consisting of the input features from the \nentire available historical dataset TS, such that Î¦ âŠ†TS. The training of the learner ğ¿(âˆ™,âˆ™) is \nperformed on Î¦ with the aim of predicting the response for the query point ğ²ğ‘, given by ğ²Ì‚ğ‘=\nğ¿(ğ±ğ‘, Î¦). This process is repeated for each query point, and the decision of whether to \nconcatenate the predicted query point (along with its real or predicted response variable value) \nto TS is application-dependent. The key to obtaining accurate predictions via JITL lies in the \nprocedure of selecting the relevant subset. Typically, proximity in feature space, as measured \nby Euclidian distance between feature values, has been used to determine the similarity of \nobservations. The k-neighbors of each query point (k usually determined by a validation set) \nare determined as the k-observations with the smallest Euclidian distance to the query point \n[91]. However, the efficacy of this criterion is contingent on the learner and the data \nenvironment. For instance,  a (weighted) linear regression may be preferred over more advanced \nlearning methods for small k values, since it would be difficult to train the latter using a small \nsized Î¦ due to a presumable high capacity. As another example, MW modeling is usually used \nto adapt to the most recent operating conditions, and MW can be classified as a type of JITL \nmethod, in which relevant data is selected not in relation to proximity in feature space, but \nrather to proximity to sampling time. Consequently, various methods for instance selection have \nbeen proposed in the literature on soft-sensors [63,94â€“96]. In the present study, proximity on a \n2-D grid comprised of years and months is adopted as the similarity measure. A relevant subset \nof the training set is selected to span Wy years and Wm months prior to the year and month of \nthe query point, respectively (see Section 4.2 for details). In this context, the hyperparameters \nWy and Wm, which are instrumental in determining the size of the local training set, are tuned \nusing rolling-origin forecasting in the training set, starting from a buffer period of 48 \nobservations. \n \nMetrics for Assessing Prediction Accuracy \nIn the context of validating and selecting the most suitable models, as well as for parameter \ntuning (i.e., model fit), comparing alternative models, and evaluating the overall forecast \nperformance [97], the most frequently used performance measures are the root mean square \n\n\n17 \n \nerror (RMSE), the mean absolute error (MAE), and the mean absolute percentage error \n(MAPE).   \nEquations 20-22 present the formulae of these performance measures, where ğ‘¦ğ‘– represents \nthe actual values, ğ‘¦Ì‚ğ‘– denotes the predicted values, and n corresponds to the number of \nobservations in the dataset. For a more in-depth exploration of additional performance \nmeasures, readers are encouraged to refer to the works of [98] and [99]. \n \nMAE =\n1\nğ‘›âˆ‘\n|ğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ğ‘–|\nğ‘›\nğ‘–=1\n \n \n \n \n \n \n \n \n \n \n \n(20) \nRMSE = âˆšâˆ‘\n(ğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ğ‘–)2\nğ‘›\nğ‘–=1\nğ‘›\n  \n \n \n \n \n \n \n \n \n \n \n(21) \nMAPE =\n100\nğ‘›âˆ‘\n|\nğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ğ‘–\nğ‘¦ğ‘–|\nğ‘›\nğ‘–=1\n  \n \n \n \n \n \n \n \n \n \n(22) \n \n \n\n\n18 \n \nDATA DESCRIPTION \n \nThe tariff methodology is instrumental in determining the revenues of natural gas distribution \ncompanies with regulated tariffs by calculating the System Usage Fee (SUF) based on \nconsumption levels. A tariff period is set to last five years, during which SUF is calculated at \nthe beginning based on past realizations using statistical models as follows: SUF = Revenue \nRequirement (OPEX + CAPEX) / Total Consumption, where OPEX stands for operating \nexpenses and CAPEX stands for capital expenditures. \nAccurate natural gas consumption estimates are critical for regulated companies, as \ndeviations in five-year estimates can lead to unexpected income losses or gains due to \nregulatory revisions. While OPEX and CAPEX transfers are mathematically predictable, \nexternal and environmental factors complicate long- and short-term consumption forecasts. \nAdvanced modeling techniques and data analysis methods are used to improve these estimates, \nbut monthly accuracy remains challenging. Enhancing natural gas consumption forecasts is \nstrategically vital for efficient resource management in the natural gas distribution sector. \nThe existing dataset, provided by SOCAR, consists of monthly residential NGD \nconsumption in million standard m3 (msm3) in Bursa and Kayseri from January 2014 to August \n2024. This dataset was divided into a training/validation set of 108 observations (9 full years) \nbetween 2014 and 2022 and a test set of 19 observations (1 year and 7 months) between 2023 \nand 2024. The former set was used for building models and tuning the hyperparameters, while \nthe latter was exclusively used for giving unbiased test set performance, similar to the monthly \nNGDP values for a one-year-period, as stipulated by the company. The NG consumption is \nascertained through the monitoring of local sensors situated in residential areas and buildings. \nIt was reported by SOCAR that the summer period readings, i.e., July, August and September \nin Bursa, and June, July, August and September in Kayseri, during 2014 to 2019, were not \nconducted exactly on time; the readings for the first two months were delayed, and the last \nmonthsâ€™ readings included some of the consumption of the prior months. Consequently, the \nResults section details the correction of summer period data in the training set prior to \nforecasting. The construction of time series and JITL-GPR models was then executed using the \ncorrected training dataset. Figures 1 and 2 below illustrate depict the logarithmically \ntransformed time series of NG consumption, in addition to its primary components, for Bursa \nand Kayseri, respectively. A visual examination of these plots indicates the presence of trend \nand additive seasonality in the datasets of both cities. \n\n\n19 \n \n \nFigure 1. The STL decomposition of the logarithm of Bursaâ€™s corrected NG consumption data \n(January 2014 to December 2022). \n \n \nFigure 2. The STL decomposition of the logarithm of Kayseriâ€™s corrected NG consumption data \n(January 2014 to December 2022). \n \n \n\n\n20 \n \nRESULTS AND DISCUSSION \n \nCorrection of NGD Data during Summer Season \nFor the correction of NGD values in these months, a linear model is proposed: \n \nğ·ğ‘¦,ğ‘š= ğ‘‡ğ‘¦,ğ‘š+ ğ‘…ğ‘¦,ğ‘š+ ğ‘’ğ‘¦,ğ‘š                                              (23) \n \nHere, ğ·ğ‘¦,ğ‘š denotes the demand observed in the yth year and mth month; ğ‘‡ğ‘¦,ğ‘š is the long-term \neffect on demand, to be extracted from the annual trend for the mth month. The second additive \nterm, which is assumed to be orthogonal to the first, is ğ‘…ğ‘¦,ğ‘š, which denotes the short-time \neffect, to be extracted from NGD in the recent months, and the last term is the unpredictable \nzero-mean error term. Trend estimates can be obtained by employing simple linear regression \non ğ·ğ‘¦,ğ‘š vs. year data for each month since a single global fit (to untransformed NGD) seems \nto be a poor fit for all the months (Figure 3A): \n \nğ·Ì‚ğ‘¦,ğ‘š= ğ‘‡ğ‘¦,ğ‘š= ğ›½Ì‚0ğ‘š+ ğ›½Ì‚1ğ‘šğ‘¦                                                    (24) \n \nIt is assumed that there exists (at least) lag-1 (one month) correlation between ğ·ğ‘¦,ğ‘š values \nfor successive months, stemming from ğ‘…ğ‘¦,ğ‘š terms, i.e., ğ¶ğ‘œğ‘Ÿğ‘Ÿ(ğ‘…ğ‘¦,ğ‘šâˆ’1, ğ‘…ğ‘¦,ğ‘š). Noting that \nğ‘…ğ‘¦,ğ‘š+ ğ‘’ğ‘¦,ğ‘š= ğ·ğ‘¦,ğ‘šâˆ’ğ‘‡ğ‘¦,ğ‘š, and the random error terms are independent, the lag-1 correlation \ncan be estimated from ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘«ğ‘šâˆ’1 âˆ’ğ‘»ğ‘šâˆ’1, ğ‘«ğ‘šâˆ’ğ‘»ğ‘š) using ğ‘«ğ‘š= [ğ·1,ğ‘š ğ·2,ğ‘š â‹¯ğ·9,ğ‘š]ğ‘‡ and \nğ‘»ğ‘š= [ğ‘‡1,ğ‘š ğ‘‡2,ğ‘š â‹¯ğ‘‡9,ğ‘š]ğ‘‡ for the mth month (see Text S1 in Supporting Materials for more \ninformation). In the light of the above modeling perspective, the correction procedure of NGD \nvalues during the summer season for six successive years, yielding 6ï‚´3 = 18, and 6ï‚´4 = 24 data \npoints for Bursa and Kayseri, respectively, is formulated as an optimization problem. Given \nmonthly NGD data for nine years in ğ‘«ğ‘š, m=1, 2, ..12, it is desired to replace ğ·ğ‘¦,ğ‘š with ğ‘¦=\n1, 2, . .6 and ğ‘š= ğ‘š0, ğ‘š0 + 1, â€¦ 9 (m0 is taken to be 7 and 6 for Bursa and Kayseri, respectively) \nwith the optimum ğ‘«ğ‘= {ğ·ğ‘¦,ğ‘š\nğ‘\n; ğ‘¦= 1, 2, . .6; ğ‘š= ğ‘š0, ğ‘š0 + 1, â€¦ 9} values (c stands for \ncorrected data), resulting in ğ‘«ğ‘š\nğ‘= [ğ·1,ğ‘š\nğ‘\n  ğ·2,ğ‘š\nğ‘\nâ€¦  ğ·6,ğ‘š \nğ‘\n ğ·7,ğ‘š ğ·8,ğ‘š  ğ·9,ğ‘š]\nğ‘»(note that the last \nthree ğ·ğ‘¦,ğ‘š values are correct readings and are therefore treated as constants). Defining Î¦ =\n[\n1\n1\n1\n2\nâ‹®\nâ‹®\n1\n9\n]\n9Ã—2\n, the following objective function is proposed for minimization (see Text S2 for more \ndetailed information about the optimization function and the accompanying constraints): \n\n\n21 \n \nÎ¨(ğ‘«ğ‘) = âˆ’{ âˆ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘«ğ‘š\nğ‘, ğ‘»ğ‘š) (9 âˆ’ğ‘š0 + 1)\nâ„\n9\nğ‘š=ğ‘š0\n+ âˆ‘|ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘«ğ‘šâˆ’1\nğ‘\nâˆ’ğ‘»ğ‘šâˆ’1, ğ‘«ğ‘š\nğ‘âˆ’ğ‘»ğ‘š)| (10 âˆ’ğ‘š0 + 1)\nâ„\n10\nğ‘š=ğ‘š0\n} \n(25) \n \nwhere ğ‘»ğ‘š= Î¦(Î¦ğ‘‡Î¦)âˆ’1Î¦ğ‘‡ğ‘«ğ‘š\nğ‘ = ğ‡ğ‘«ğ‘š\nğ‘ for m=ğ‘š0, ğ‘š0 + 1, â€¦ 9, and ğ‘»ğ‘š= ğ‡ğ‘«ğ‘š otherwise, \nsubject to the following constraints: \n \nâˆ‘ğ·ğ‘¦,ğ‘š\nğ‘\n9\nğ‘š=ğ‘š0\n= âˆ‘ ğ·ğ‘¦,ğ‘š \n9\nğ‘š=ğ‘š0\n, ğ‘¦= 1,2 â€¦ 6 \n(26) \nğ·ğ‘¦,ğ‘š\nğ‘\nâ‰¥0, ğ‘¦= 1, 2, . .6; ğ‘š= ğ‘š0, ğ‘š0 + 1, â€¦ 9                                   (27) \n \nThe optimization was performed with fmincon in the Optimization Function of MATLABTM \nusing the interior-point method [100] with the initial parameter values, i.e., the monthly NGD \nvalues during the summer period, all assumed to be equal to 10 msm3, slightly lower than the \nactual readings obtained during the last three years. The results of the optimization are shown \nin Figure 3B-D for Bursa. In addition, Figs. 3E-F show that the irregular behavior during the \nsummer season seen in the original time series has been corrected for both cities. \n\n\n22 \n \n \nFigure 3. (A) NGD values in Bursa for May and October over 2014-2022. The original and \ncorrected NGD values in Bursa for (B) July, (C) August, and (D) September 2014-2022. The \noriginal and corrected (â€œsmoothedâ€ gray lines during the summer periods in the figures) \nmonthly NGD values in the training sets of (E) Bursa and (F) Kayseri. \n \nTime Series Modeling \nOutliers were identified in both â€œcorrectedâ€ datasets: five in the training set for Bursa and two \nfor Kayseri. During the modeling process, cases in which these values were not smoothed and \ncases in which they were smoothed were treated separately. Both datasets indicated the utility \nof a logarithmic transformation; therefore, models were constructed separately for the original \ndata and the transformed data. In light of the observed trend and seasonality in the data, we \nproduced forecasts using four primary statistical models as a benchmark for the proposed \napproach: STL, SARIMA, ETS, and TBATS. We also implemented a machine learning-based \n\n\n23 \n \ncombination technique, the AI-AFTER algorithm. The elapsed time for forecasting did not \nexceed 25 seconds in any case. The models were constructed using the training set, and the \nestimated parameters were subsequently applied to the validation data. As the performance of \nthe models on the training and validation sets did not exhibit any anomalies, there was no \nnecessity to update the parameters. All of the model orders were verified in several in-sample \nforecasting trials. Table 2 below provides descriptions of some of the models considered in this \nstudy. Once the model parameters were estimated using the training sample, 19-step-ahead \nforecasts were produced by the candidate models to evaluate their post-sample performances. \n \nTable 2. Details on the time series models applied to the logarithms of the corrected NG data \nAbbreviation \nCity \nDescription and References \nETS \nBursa \nAdditive Holt-Wintersâ€™ Method with multiplicative errors and \nparameters ğ›¼= 0.00113, ğ›½= 0.00010, and ğ›¾= 0.00011.  \nKayseri \nAdditive Holt-Wintersâ€™ Method with additive errors and parameters \nğ›¼= 0.00290, ğ›½= 0.00289, and ğ›¾= 0.00013. \nSARIMA \nBursa \nSeasonal Autoregressive Integrated Moving Average model with drift, \ndesignated as ğ‘= 1, ğ‘‘= 0, ğ‘= 0; ğ‘ƒ= 2, ğ·= 1, ğ‘„= 0. \nKayseri \nSeasonal Autoregressive Integrated Moving Average model with drift, \ndesignated as ğ‘= 1, ğ‘‘= 0, ğ‘= 0; ğ‘ƒ= 0, ğ·= 1, ğ‘„= 1. \nSTL \nBursa \nThe data for both cities was subjected to additive seasonality with \nparameters s.window = â€œperiodicâ€ and t.window = 13, and robust \nfitting was adopted in the LOESS procedure. \nKayseri \nTBATS \nBursa \nOmega, the Box-Cox parameter = 0.91; ARMA(1,0) model was fitted; \nno damping; # of seasonal periods = 12; # of Fourier terms used for \neach seasonality = 4. \nKayseri \nOmega, the Box-Cox parameter = 1.00; ARMA(0,0) model was fitted; \nno damping; # of seasonal periods = 12; # of Fourier terms used for \neach seasonality = 5. \n \nThe predictive performances of the time series models examined in this study are evaluated \nusing the accuracy metrics provided in Equations 20-22. TBATS and the AI-AFTER algorithm \nexhibited superior performance in comparison to the other models for Bursa and Kayseri, \nrespectively, as illustrated in Table 3. It must be noted that the initial 35 observations in the \ntraining set were used to train the candidate forecasts by the AI-AFTER algorithm, and were \nconsequently not accessible to the analyst. Thus, the performance metrics of this method on the \nKayseri training set were determined using a total of 73 observations, rather than108. \n \n \n \n \n\n\n24 \n \nTable 3. Accuracy metrics for the benchmark time series model on the training and test sets. \nCity \nBursa \nKayseri \nModel \nSARIMA \nETS \nSTL \nTBATS \nAI- \nAFTER \nSARIMA \nETS \nSTL \nTBATS \nAI- \nAFTER \nRMSE1-step (msm3) \n9.212 \n9.569 \n9.366 \n9.024 \n8.863 \n5.383 \n5.120 \n5.956 \n5.221 \n5.275 \nMAE1-step (msm3) \n5.917 \n6.185 \n6.300 \n6.355 \n6.023 \n3.593 \n3.409 \n3.888 \n3.446 \n3.657 \nMAPE1-step (%) \n10.43 \n10.02 \n11.40 \n11.08 \n9.39 \n13.21 \n13.53 \n16.86 \n13.19 \n12.59 \nRMSEtest (msm3) \n19.221 \n20.361 \n19.566 \n14.857 \n16.751 \n10.778 \n8.286 \n9.013 \n6.786 \n6.332 \nMAEtest (msm3) \n14.367 \n13.244 \n14.211 \n9.666 \n11.208 \n7.822 \n5.828 \n7.189 \n5.094 \n4.140 \nMAPEtest (%) \n19.79 \n18.34 \n20.74 \n14.24 \n15.60 \n18.21 \n14.62 \n20.12 \n13.26 \n11.87 \nNGD in 2023 (msm3) \n816.8 \n816.8 \n816.8 \n816.8 \n816.8 \n477.6 \n477.6 \n477.6 \n477.6 \n477.6 \nNGD in 2023  \nprediction (msm3) \n850.9 \n867.7 \n690.8 \n779.3 \n797.9 \n519.7 \n502.1 \n401.6 \n451.0 \n475.4 \nYearly PE in 2023 \n4.18 \n6.23 \n-15.42 \n-4.58 \n-2.31 \n8.83 \n5.15 \n-15.90 \n-5.55 \n-0.45 \n \nFigures 4 and 5 illustrate the training and test data (black lines), and the predictions (blue \nlines) and forecasts (red dashed lines) generated by the TBATS model for Bursa and the AI-\nAFTER algorithm for Kayseri, respectively. Finally, Figures 6 and 7 demonstrate how well the \nstandardized residuals from each of these two models fit to a proper normal distribution. While \nthree of the residuals generated by the TBATS model are potential outliers, the number of such \ncases is two for the AI-AFTER algorithm. However, the Ljung-Box test statistic was found \ninsignificant at lag 4 in both cases (ğ‘„= 3.092 for Bursa and ğ‘„= 4.362 for Kayseri), indicating \nthat autocorrelation in the residuals is not an issue at ğ›¼= 0.05. \n \n \nFigure 4. The real NGD values, one-step ahead training set predictions, and multi-step ahead \ntest set predictions using the TBATS model for Bursa. \n\n\n25 \n \n \n \nFigure 5. The real NGD values, one-step ahead training set predictions, and multi-step ahead \ntest set predictions using the AI-AFTER algorithm for Kayseri. \n \n \nFigure 6. QQ Plot for the multi-step ahead test set prediction residuals for Bursa. \n \n\n\n26 \n \n \nFigure 7. QQ Plot for the multi-step ahead test set prediction residuals for Kayseri. \n \nJITL-GPR Modeling  \nThe JITL-GPR forecasting method consists of two steps: (i) selecting a convenient subset of \ntraining set, i.e., the local training set, for each query point, and (ii) applying GPR to the local \ntraining set and predicting the query point.  \n \nLocal subset selection \nIn the traditional time series representation for regression in NGDP, including the studies using \nmachine learning tools, the features (excluding exogenous variables) correspond to the response \nvariable sampled at previous time lags [23,59,101]:  \n \n ğ·Ì‚ğ‘¡+1 = ğ‘“(ğ’™ğ‘¡),     ğ’™ğ‘¡= [ğ·ğ‘¡ ğ·ğ‘¡âˆ’1 â€¦ ğ·ğ‘¡âˆ’ğ‘+1]ğ‘‡âˆˆâ„ğ‘                          (28) \n \nIn the present study, however, a different feature representation is adopted. The input \nfeatures are assumed to be year and month values, i.e., ğ‘¦= 1, 2, . . ğ‘›ğ‘‡; ğ‘š= 1,2, â€¦ 12, and for \neach point on this two-dimensional (2-D) grid, similar to a 2-D Gaussian random field, the NGD \nvalue ğ·ğ‘¦,ğ‘š is taken to be the response variable. Thus, the training (historical) dataset is defined \nas ğ‘‡ğ‘†= {(ğ‘¦, ğ‘š, ğ·ğ‘¦,ğ‘š); ğ‘¦= 1, 2, . . ğ‘›ğ‘‡; ğ‘š= 1,2, â€¦ 12}. To predict the future query points (test \nset), a data segment with convenient window widths (Wy, Wm), representing the number of \nprevious years and months, respectively, is extracted from the historical dataset, and GPR is \nemployed on this 2-D data window. This procedure is to be repeated for all query points, thereby \n\n\n27 \n \nrendering the proposed method an application of JITL. It is to be noted that the similarity of the \nquery point to the training data is based on proximity in years and months (on the 2-D grid), \nand is computed using modular arithmetic. In other words, the month indices 1 (January) and \n12 (December) in two successive years are assumed to be in close proximity, similar to the \nrationale in time series analysis. Thus, for a query point with the index ğ‘= 12ğ‘¦+ ğ‘š, in which \ny and m values are known, the following function Î¦(ğ‘‡, ğ‘, ğ‘Šğ‘¦, ğ‘Šğ‘š) is used to extract the relevant \nsubset from the training dataset TS with the help of an accompanying function ğ‘“(ğ‘›), which \nextracts year, month and NGD value of the (ğ‘›âˆ’1)ğ‘¡â„ observation via ğ‘“(ğ‘›) = (ğ‘ğ‘’ğ‘–ğ‘™((ğ‘›âˆ’\n1)/12), ğ‘šğ‘œğ‘‘(ğ‘›âˆ’2,12) + 1, ğ·(ğ‘›âˆ’1)): \n \nÎ¦(ğ‘‡, ğ‘, ğ‘Šğ‘¦, ğ‘Šğ‘š) = {ğ‘“(ğ‘›) = (ğ‘¦, ğ‘š, ğ·ğ‘¦,ğ‘š), ğ‘›âˆˆ{1,2, . . ğ‘}} = \n{ğ‘“(ğ‘), ğ‘“(ğ‘âˆ’1), â€¦ , ğ‘“(ğ‘âˆ’ğ‘Šğ‘š+ 1), ğ‘“(ğ‘âˆ’11), ğ‘“(ğ‘âˆ’12), â€¦ , ğ‘“(ğ‘âˆ’11 âˆ’(ğ‘Šğ‘šâˆ’1)), â€¦ \n ğ‘“(ğ‘âˆ’(ğ‘›Ã— 12 âˆ’1)), ğ‘“(ğ‘âˆ’ğ‘›Ã— 12) , â€¦ ğ‘“(ğ‘âˆ’(ğ‘›Ã— 12 âˆ’1 âˆ’(ğ‘Šğ‘šâˆ’1))), â€¦, \nğ‘“(ğ‘âˆ’((ğ‘Šğ‘¦âˆ’1) Ã— 12 âˆ’1)) , â€¦ ğ‘“(ğ‘âˆ’((ğ‘Šğ‘¦âˆ’1) Ã— 12 âˆ’1 âˆ’(ğ‘Šğ‘š+ 1)))} (29) \n \nIn the equation above, ceil() and mod() are ceiling and mod functions, respectively. While \na quick look at Equation 29 gives the impression of a rather complicated local dataset \nconstruction method, it is indeed quite simple. As an example, the construction of the local \ntraining set for the first query point in the test set (the 109th point, January 2023) is demonstrated \nfor Wy = 4 and Wm = 3. Table 4 shows the years and months of the data in the TS in 2-D grid \nform, and NGD values (in time-index format) are placed in each corresponding cell. The query \npoint in the 11th year is shown with a dark gray cell, where ğ·ğ‘ is unknown. Since Wm = 3, the \nlast three months, including the query month, are included in the local training set, hence D106 \nand D107, corresponding to November and December of 2022, are selected (in light gray color). \nFor one-year lagged observations, NGD values for November 2021, December 2021, and \nJanuary 2022 are selected, corresponding to D95, D96, and D97, respectively. This selection \nmethod is repeated to include observations up to Wy years, which corresponds to the selection \nmethod proposed in Equation 29. In this way, the historical samples that are similar to the \ncurrent query point in terms of the seasons are selected. To construct the local training set for \nthe next query point (110th point, February 2024), the same procedure is repeated using the Wy \nand Wm values for February, while the already predicted value for January is used as a training \nset point. Thus, multi-step-ahead forecasting is achieved in an iterative manner. \n \n\n\n28 \n \nTable 4. An example of how a local training set is selected in JITL-GPR model. \nYears/ \nMonths \n1 \n2 \nâ€¦ \n7 \n8 \n9 \n10 \n11 \n1 \nD1 \nD13 \nâ€¦ \nD73 \nD85 \nD85 \nD97 \nDq=D109 \n2 \nD2 \nD14 \nâ€¦ \nD74 \nD86 \nD86 \nD98 \n \nâ€¦ \nâ€¦ \nâ€¦ \nâ€¦ \nâ€¦ \nâ€¦ \nâ€¦ \nâ€¦ \n \n10 \nD10 \nD22 \nâ€¦ \nD82 \nD94 \nD94 \nD106 \n \n11 \nD11 \nD23 \nâ€¦ \nD83 \nD95 \nD95 \nD107 \n \n12 \nD12 \nD24 \nâ€¦ \nD84 \nD96 \nD96 \nD108 \n \n \nThe kth element of the (ğ‘¦, ğ‘š) pair in the subset extracted by Î¦(ğ‘‡, ğ‘, ğ‘Šğ‘¦, ğ‘Šğ‘š) is denoted by \nğ’™ğ‘˜, i.e., Î¦(ğ‘‡, ğ‘, ğ‘Šğ‘¦, ğ‘Šğ‘š) = {(ğ’™ğ‘˜, ğ·ğ‘˜), ğ‘˜= {1,2, . . ğ‘Šğ‘¦Ã— ğ‘Šğ‘šâˆ’1}}; furthermore, ğ‘¿ğ½ğ¼ğ‘‡ğ¿=\n{ğ’™ğ‘˜, ğ‘˜= {1,2, . . ğ‘Šğ‘¦Ã— ğ‘Šğ‘šâˆ’1}}  and ğ‘«ğ½ğ¼ğ‘‡ğ¿= {ğ·ğ‘˜, ğ‘˜= {1,2, . . ğ‘Šğ‘¦Ã— ğ‘Šğ‘šâˆ’1}} are used for \nease of representation. For a given query point ğ’™ğ‘, ğ‘¿ğ½ğ¼ğ‘‡ğ¿ and ğ‘«ğ½ğ¼ğ‘‡ğ¿ are determined and z-score \nscaled. Finally, GPR is used (see Equation 18) to determine the expected value of NGD at the \nquery point, ğ¸{ğ·ğ‘|ğ‘¥ğ‘, ğ‘¿, ğ‘«ğ½ğ¼ğ‘‡ğ¿} = ğ‘“(ğ‘¥ğ‘|ğ‘¿ğ½ğ¼ğ‘‡ğ¿, ğ‘«ğ½ğ¼ğ‘‡ğ¿) (please note that the symbol y used in \nSections 2.1 and 2.2 for the output variable is now reserved for the feature year number) using \nthe fitgrp function in the Statistics and Machine Learning Toolbox of MATLABTM. \n \nKernel Design \nBased on the observation that NGD values exhibit a nearly linear trend in the long-run, with \nsinusoidal-like behavior within each year, an additive kernel that consists of a homogeneous \nlinear kernel for years and a second order polynomial kernel for months is designed. Given two \nvectors ğ’™ğ‘¢= [ğ‘¦ğ‘¢  ğ‘šğ‘¢]ğ‘‡ and ğ’™ğ‘£= [ğ‘¦ğ‘£  ğ‘šğ‘£]ğ‘‡, with u, v ïƒ {1, 2, â€¦}, index of the observation, \ntwo basic kernels are added to form a convenient kernel function.  \n \nğ‘˜(ğ’™ğ‘¢, ğ’™ğ‘£) = ğœğ‘“\n2(ğ›½2ğ‘¦ğ‘¢\nğ‘‡ğ‘¦ğ‘£+ (ğ‘šğ‘¢\nğ‘‡ğ‘šğ‘£+ ï¡2)2)                                 (30) \n \nThe three hyperparameters in GPR, ğœğ‘“\n2, ğ›½2, and ï¡2, are expected to adjust the variance of \nthe response variable, the weight of the â€œyear effectâ€ relative to the â€œmonth effectâ€ (or scale \nparameter), and the curvature of the NGD in recent months. These three hyperparameters were \nall initialized with unity in the optimization of marginal loglikelihood, since all variables were \nz-normalized, and identical results were obtained using initial values between 0.2 and 5. Finally, \nthe mean of the GPR function (see Equation 14) was set equal to zero, i.e., no fixed basis \nfunctions were used.  \n \n\n\n29 \n \nWindow size tuning \nTo adjust the hyper-parameters of the JITL-GPR model, the first 48 observations in the TS were \nreserved as a â€œbufferâ€ region, and observations with indices 49 to 108 (60 data points) in the \nTS were predicted using a moving horizon estimation method, i.e., once a query point is \npredicted, it is included in the training set accessible for the next query sample. Here, a grid \nsearch over the local dataset window sizes of ğ‘Šğ‘¦= {2, 3, â€¦ 8} and ğ‘Šğ‘š= {2, 3, â€¦ 6} (see Text \nS3 for the rationale of selecting these ranges for the hyperparameters) is performed to find a \nsingle (ğ‘Šğ‘¦, ğ‘Šğ‘š) parameter pair (applicable to all query points) with the smallest one-step-ahead \nRMSE value. Using this method on Bursa yielded an RMSE of 14.1 msm3 at (ğ‘Šğ‘¦, ğ‘Šğ‘š) =\n(8,3); however, a surface plot of RMSE values over all the parameter value combinations \nshowed that RMSEï‚£15.5 msm3 was obtained for all ğ‘Šğ‘š values and Wy ï‚³ 4 (figure not shown). \nThis indicated that the prediction accuracy did not change with respect to the number of \nprevious months included in the model as long as more than four successive years of data were \nincluded. This was indeed unexpected; hence we examined the phenomena in more detail. We \ngrouped 60 predictions into 12 groups of five observations (years) each, and found that the \nRMSE surfaces with respect to parameter values changed significantly from month to month. \nThis indicated that the flat RMSE surface with respect to window sizes is due to an â€œaveraging \noutâ€ effect over 12-months, i.e., using the same window size for all months yields an inferior \nand almost constant performance over a range of window sizes.  \nDue to the limited number of historical observations, with each month having only five \nobservations available for tuning the window size, it is not feasible to tune a separate (ğ‘Šğ‘¦, ğ‘Šğ‘š) \npair for each month. Combining â€œsimilarâ€ months into single units is a better strategy, which \nhas been frequently employed in the NGDP literature for predictive purposes [5,9]. To this end, \nthe similarity of RMSE surfaces with respect to parameter values for all pairs of successive \nmonths was checked and series of similar months were grouped together (Text S4 and Figure \nS1). Consequently, All NGD values during January-May (five months), June-September (four \nmonths), and October-December (three months) are grouped as Group I, Group II and Group \nIII, respectively. \n \nPredictions of JITL-GPR for the Test Set \nFor each group, which now contained 25, 20, and 15 observations, (ğ‘Šğ‘¦, ğ‘Šğ‘š) parameter values \nthat yield the minimum RMSE, were found to be equal to 13.22 msm3 (~1 msm3 smaller than \nthe previous value), were determined. The results for the two cities are shown in Table 5. It is \n\n\n30 \n \ninteresting to note that the optimum values of the window parameters show a striking similarity \nfor both cities, indicating a more general NGD tendency that is not localized to a specific \nlocation. The optimal value of ğ‘Šğ‘¦ varies less between the groups compared to that of ğ‘Šğ‘š; only \nthe NGD at the beginning of winter (Group III) seems to be less affected by the slow annual \ntrend, but related to the very recent demand values. The information of the previous months \nwas found to be more important for the months with higher heating demand. We can speculate \nthat this information could be helpful for the GPR model in determining the recent trend in \nweather and economic conditions that affect the heating habits of the residents. \n \nTable 5. ğ‘Šğ‘¦, ğ‘Šğ‘š values that yield the minimum RMSE values of one-step ahead predictions \nCity \nBursa \nKayseri \nGroups \nI \nII \nIII \nI \nII \nIII \nOptimum ğ‘¾ğ’š \n8 \n8 \n5 \n8 \n8 \n2 \nOptimum ğ‘¾ğ’ 6 \n2 \n4 \n6 \n2 \n4 \n \nTwo examples of multi-step-ahead predictions for the test set of Kayseri are demonstrated in \nFigures 8A and 8B for January (one-step-ahead) and November (11-step-ahead) 2023, \nrespectively. Note that {ğ‘Šğ‘¦, ğ‘Šğ‘š} pair is taken to be equal to {8, 6} for January, and {2, 4} for \nNovember (see Table 5). Here, the 0th year corresponds to the query time and the ğ‘Šğ‘š-1 months \nbefore it, so the year is not taken as an absolute time measure. In both figures, the GPR surfaces \nfit the historical data well, and predict the test point quite accurately. Table 6 shows that the \naccuracy of the JITL-GPR predictions for the test set exceeds that of the time-series models; \nmoreover, the percentage error of the prediction of the total NGD in 2023 is less than 1% for \nboth cities, indicating that the government regulations for annual predictions are satisfactorily \nmet. A comparison of the results of JITL-GPR with those from time series methods, as \npresented in Section 4.2, indicates that the JITL-GPR approach led to a reduction in out-of-\nsample RMSE values by 14.6% for Bursa and 19.3% for Kayseri, as well as a decrease in MAE \nvalues by 2.5% for Bursa and 10.4% for Kayseri (see Figure 9). The proposed model led to a \n5.1% reduction in MAPE for Bursa, yet it did not achieve the same outcome for Kayseri. Figure \n10 shows one-step ahead predictions for the training set and the multi-step ahead predictions \nfor the test set for both cities, again indicating that the forecasts adequately follow the actual \ndemand values. Finally, the prediction residuals, defined as the difference between the predicted \nand actual NGD values, generally show a similar picture for both cities (see Figure 11). \nOverpredicted NGD values in the first year may be due to small number of historical data (only \n\n\n31 \n \nthe first four years of data are available to predict the 49th observation), especially to accurately \ndetermine the annual trend, while underpredicted NGD values in the winter of 2021-2022 \n(observations #95-100) may be due to a concept drift, such as economic conditions imposed by \nthe COVID pandemic. However, the advantage of using an adaptive JITL mechanism is seen \nhere; the inclusion of these â€œunexpectedâ€ NGD values within the window of the JITL-GPR \nleads to more accurate predictions of the next yearâ€™s NGDPs. \n \n \nFigure 8. NGDP for (A) January, and (B) November 2023. The filled circles represent the \ntraining sets in both subfigures, and also NGDPs for August-October 2023 (in the left \nsubfigure); while the large diamonds denote the true value of the query point. \n \nTable 6. Accuracy metrics for the JITL-GPR model on the training and test sets. \nCity \nBursa \nKayseri \nRMSE1-step (msm3) \n13.23 \n6.407 \nMAE1-step (msm3) \n8.485 \n4.590 \nMAPE1-step (%) \n12.20 \n16.34 \nRMSEtest (msm3) \n12.69 \n5.110 \nMAEtest (msm3) \n9.424 \n3.711 \nMAPEtest (%) \n13.51 \n14.63 \nNGD in 2023 (msm3) \n816.8 \n477.6 \nNGD in 2023 prediction (msm3) \n824.3 \n473.6 \nYearly PE in 2023 \n0.90 \n-0.83 \n \n\n\n32 \n \n \nFigure 9. Comparison of RMSE and MAPE values of different methods for the test set. \n \n \nFigure 10. Comparison of real, one-step ahead training set and multi-step ahead test set \npredictions of NGD values. \n\n\n33 \n \n \nFigure 11. One-step ahead training set and multi-step ahead test set prediction residuals. \n  \n \n \n\n\n34 \n \nCONCLUSION AND RECOMMENDATIONS \n \nNG is a pivotal source of energy, exhibiting a nearly constant yearly increase in worldwide \ndemand over the past ~30 years. The reliance of numerous countries, including Turkey, on \nimports to maintain a sufficient NG supply underscores the significance of accurate NGDP in \neconomic planning by governments. The present study examined the monthly NGDPs of two \nTurkish cities employing various time series models and a novel online learning method, JITL-\nGPR. Moreover, the JITL-GPS model demonstrated remarkable precision in forecasting the \nyearly NGD for 2023, achieving an out-of-sample prediction error of less than 1% for both \nBursa and Kayseri. \nThe current methodology, in contrast to the utilization of each previous NGD value as a \nseparate feature, i.e., in a separate column of a regressor matrix, employs a weighting system \nthat considers the proximity of previous NGD values to the query point in months and years. \nThis methodology then adjusts the weights to achieve the best fitting (in Bayesian sense) via \nGPR (see Equation 18). The present approach bears resemblance to the â€œkrigingâ€ method \nemployed in geostatistics, wherein Gaussian processes are used in low-dimensional spatial \ndimensional settings [102]. The proposed method can be regarded as a regularized version of \nthe standard prediction methods, wherein the input features are derived from lagged output \nvalues. In contrast to the SARIMAX method, which seeks to maximize the likelihood under the \nconstraints of stability and invertibility, the proposed method employs a regularization \napproach that restricts the impact of NGD values from previous years and months on the current \nyear, thereby ensuring proximity in year and month (weighted) directions. While this \nregularization may have led to an increase in bias in predictions, particularly under a batch \nlearning strategy, the application of a nonlinear learner, such as GPR, is likely to maintain bias \nat a reasonable level, while effectively combating variance, and adapting to changing \nconditions. Resistance to outliers appears to be another advantage of JITL-GPR, attributable to \nthe smoothing-out effect of the low-order kernel. \nDespite the geographical separation of Bursa and Kayseri, which is approximately 660 km \napart, both exhibit a Mediterranean and continental climate. However, comparable optimal \nwindow parameters for GPR-JITL were obtained in both locations. This congruence may be \ndue to two potential factors: (i) an artifact resulting from the approximation used in aggregating \nmonths from January to May into a single category to address the issue of inadequate sample \nsize, or (ii) medium-term (monthly) NGD values exhibiting reduced sensitivity to weather \nconditions, compared to social variables affecting a broader community or nation. This \n\n\n35 \n \nobservation, in conjunction with the selection of window parameters during winter months, \nrequires further investigation. The automated procedure of lumping â€œsimilarâ€ months is \nrecommended to avoid selection bias. Additionally, the autocorrelation structure of the \nprediction residuals of GPR-JITL suggests that the proposed method may require further \nrefinement to adapt to the rapid fluctuations in NGD, such as abrupt drifts. This may necessitate \nthe incorporation of recent observations using, for instance, weighted GPR [103] or the \nintegration of predictions with time series models within a transfer learning framework [66]. \nFinally, the incorporation of â€œRamadan effectâ€ [104] into the JITL-GPR framework, if it exists, \ncould potentially enhance the accuracy of NGDP values. Further research is necessary to \ninvestigate these aspects and improve the prediction accuracy of NGD values.     \n \n \n\n\n36 \n \nREFERENCES \n1.  \nTaÅŸpÄ±nar F, Ã‡elebi N, Tutkun N. Forecasting of daily natural gas consumption on regional \nbasis in Turkey using various computational methods. Energy Build. 2013;56: 23â€“31. \ndoi:10.1016/j.enbuild.2012.10.023 \n2.  \nLiu J, Wang S, Wei N, Chen X, Xie H, Wang J. Natural gas consumption forecasting: A \ndiscussion on forecasting history and future challenges. J Nat Gas Sci Eng. 2021;90: \n103930. doi:10.1016/j.jngse.2021.103930 \n3.  \nMeira E, Cyrino Oliveira FL, de Menezes LM. Forecasting natural gas consumption using \nBagging and modified regularization techniques. Energy Econ. 2022;106: 105760. \ndoi:10.1016/j.eneco.2021.105760 \n4.  \nLi N, Wang J, Liu R, Zhong Y. What is the short-term outlook for the EUâ€™s natural gas \ndemand? Individual differences and general trends based on monthly forecasts. Environ \nSci Pollut Res. 2022;29: 78069â€“78091. doi:10.1007/s11356-022-21285-9 \n5.  \nSÃ¡nchez-Ãšbeda EFco, Berzosa A. Modeling and forecasting industrial end-use natural gas \nconsumption. Energy Econ. 2007;29: 710â€“742. doi:10.1016/j.eneco.2007.01.015 \n6.  \nBerrisford HG. The Relation between Gas Demand and Temperature: A Study in \nStatistical Demand Forecasting. OR. 1965;16: 229â€“246. doi:10.2307/3007504 \n7.  \nDurrer EJ, Somerton WH, Mueller TD. Short-Range Multivariate Process Prediction for \nUse in Automatized Systems. Available: https://dx.doi.org/10.2118/2630-MS \n8.  \nBox GEP, Jenkins GM. Time Series Analysis: Forecasting and Control. Holden-Day; \n1970.  \n9.  \nHerbert JH. Data analysis of sales of natural gas to households in the United States. J Appl \nStat. 1986;13: 199â€“211. doi:10.1080/02664768600000028 \n10.  Liu L-M, Lin M-W. Forecasting residential consumption of natural gas using monthly and \nquarterly time series. Int J Forecast. 1991;7: 3â€“16. doi:10.1016/0169-2070(91)90028-T \n11.  Suykens J, Lemmerling Ph, Favoreel W, de Moor B, Crepel M, Briol P. Modelling the \nBelgian gas consumption using neural networks. Neural Process Lett. 1996;4: 157â€“166. \ndoi:10.1007/BF00426024 \n12.  Khotanzad A, Elragal HM. Natural gas load forecasting with combination of adaptive \nneural networks. IEEE; 1999. doi:10.1109/IJCNN.1999.830812 \n13.  Viet NH, Mandziuk J. Neural and fuzzy neural networks in prediction of natural gas \nconsumption. Neural Parallel Sci Comput. 2005;265â€“286.  \n14.  Liu H, Liu D, Zheng G, Liang Y, Ni Y. Research on natural gas load forecasting based on \nsupport vector regression. Fifth World Congress on Intelligent Control and Automation \n(IEEE Cat No04EX788). 2004. pp. 3591A â€“ 3595. doi:10.1109/WCICA.2004.1343263 \n15.  Gil S, Deferrari J. Generalized Model of Prediction of Natural Gas Consumption. J Energy \nResour Technol. 2004;126: 90â€“98. doi:10.1115/1.1739239 \n16.  PotoÄnik P, Soldo B, Å imunoviÄ‡ G, Å ariÄ‡ T, Jeromen A, Govekar E. Comparison of static \nand adaptive models for short-term residential natural gas forecasting in Croatia. Appl \nEnergy. 2014;129: 94â€“103. doi:10.1016/j.apenergy.2014.04.102 \n17.  Zhu L, Li MS, Wu QH, Jiang L. Short-term natural gas demand prediction based on \nsupport vector regression with false neighbours filtered. Energy. 2015;80: 428â€“436. \ndoi:10.1016/j.energy.2014.11.083 \n18.  Sharma V, Cali Ãœ, Sardana B, Kuzlu M, Banga D, Pipattanasomporn M. Data-driven \nshort-term natural gas demand forecasting with machine learning techniques. J Pet Sci \nEng. 2021;206: 108979. doi:10.1016/j.petrol.2021.108979 \n19.  Khani H, Farag HEZ. An Online-Calibrated Time Series Based Model for Day-Ahead \nNatural Gas Demand Forecasting. IEEE Trans Ind Inform. 2019;15: 2112â€“2123. \ndoi:10.1109/TII.2018.2861390 \n\n\n37 \n \n20.  Hribar R, PotoÄnik P, Å ilc J, Papa G. A comparison of models for forecasting the \nresidential natural gas demand of an urban area. Energy. 2019;167: 511â€“522. \ndoi:10.1016/j.energy.2018.10.175 \n21.  Du J, Zheng J, Liang Y, Lu X, KlemeÅ¡ JJ, Varbanov PS, et al. A hybrid deep learning \nframework for predicting daily natural gas consumption. Energy. 2022;257: 124689. \ndoi:10.1016/j.energy.2022.124689 \n22.  Bai S, Huang X, Luo M, Su J. Deep hybrid models for daily natural gas consumption \nforecasting and complexity measuring. Energy Sci Eng. 2023;11: 654â€“674. \ndoi:10.1002/ese3.1352 \n23.  Iranmanesh H, Abdollahzade M, Miranian A. Mid-Term Energy Demand Forecasting by \nHybrid Neuro-Fuzzy Models. Energies. 2012;5: 1â€“21. doi:10.3390/en5010001 \n24.  Izadyar N, Ong HC, Shamshirband S, Ghadamian H, Tong CW. Intelligent forecasting of \nresidential heating demand for the District Heating System based on the monthly overall \nnatural \ngas \nconsumption. \nEnergy \nBuild. \n2015;104: \n208â€“214. \ndoi:10.1016/j.enbuild.2015.07.006 \n25.  Safiyari MH, Shavvalpour S, Tarighi S. From traditional to modern methods: Comparing \nand introducing the most powerful model for forecasting the residential natural gas \ndemand. Energy Rep. 2022;8: 14699â€“14715. doi:10.1016/j.egyr.2022.10.397 \n26.  Sector Report, Republic of Turkiye Energy Market Regulatory Authority. [cited 19 Feb \n2025]. Available: https://www.epdk.gov.tr/Detay/Icerik/3-0-94/dogal-gazyillik-sektor-\nraporu \n27.  Berk I, Ediger VÅ. A historical assessment of Turkeyâ€™s natural gas import vulnerability. \nEnergy. 2018;145: 540â€“547. doi:10.1016/j.energy.2018.01.022 \n28.  Soldo B. Forecasting natural gas consumption. Appl Energy. 2012;92: 26â€“37. \ndoi:10.1016/j.apenergy.2011.11.003 \n29.  AKPINAR M, YUMUÅAK N. Naive forecasting of household natural gas consumption \nwith sliding window approach. Turk J Electr Eng Comput Sci. 2017;25: 30â€“45. \ndoi:10.3906/elk-1404-378 \n30.  Ã–zmen A, YÄ±lmaz Y, Weber G-W. Natural gas consumption forecast with MARS and \nCMARS \nmodels \nfor \nresidential \nusers. \nEnergy \nEcon. \n2018;70: \n357â€“381. \ndoi:10.1016/j.eneco.2018.01.022 \n31.  Ã–zmen A. Sparse regression modeling for short- and longâ€term natural gas demand \nprediction. Ann Oper Res. 2023;322: 921â€“946. doi:10.1007/s10479-021-04089-x \n32.  Ozmen A. Multi-objective regression modeling for natural gas prediction with ridge \nregression and CMARS. Int J Optim Control Theor Appl IJOCTA. 2022;12: 56â€“65. \ndoi:10.11121/ijocta.2022.1084 \n33.  Yukseltan E, Yucekaya A, Bilge AH, Agca Aktunc E. Forecasting models for daily natural \ngas consumption considering periodic variations and demand segregation. Socioecon \nPlann Sci. 2021;74: 100937. doi:10.1016/j.seps.2020.100937 \n34.  Yucesan M, Pekel E, Celik E, Gul M, Serin F. Forecasting daily natural gas consumption \nwith regression, time series and machine learning based methods. Energy Sources Part \nRecovery Util Environ Eff. 2025;47: 4605â€“4620. doi:10.1080/15567036.2021.1875082 \n35.  BalÄ±kÃ§Ä± V, Gemici Z, Taner T, DalkÄ±lÄ±Ã§ AS. Forecasting natural gas demand in Istanbul by \nartificial neural networks method and planning of city gate stations. 2024 [cited 19 Feb \n2025]. doi:10.17341/gazimmfd.1165734 \n36.  ARAS H, ARAS N. Forecasting Residential Natural Gas Demand. Energy Sources. \n2004;26: 463â€“472. doi:10.1080/00908310490429740 \n37.  Kizilaslan R, Karlik B. Combination of neural networks forecasters for monthly natural \ngas consumption prediction. Neural Netw World. 2009;19: 191â€“199.  \n\n\n38 \n \n38.  Bilici Z, Ã–zdemir D, TemurtaÅŸ H. Comparative analysis of metaheuristic algorithms for \nnatural gas demand forecasting based on meteorological indicators. J Eng Res. 2023;11: \n259â€“265. doi:10.1016/j.jer.2023.100127 \n39.  Sarak H, Satman A. The degree-day method to estimate the residential heating natural gas \nconsumption in Turkey: a case study. Energy. 2003;28: 929â€“939. doi:10.1016/S0360-\n5442(03)00035-5 \n40.  GORUCU FB. Evaluation and Forecasting of Gas Consumption by Statistical Analysis. \nEnergy Sources. 2004;26: 267â€“276. doi:10.1080/00908310490256617 \n41.  Karadede Y, Ozdemir G, Aydemir E. Breeder hybrid algorithm approach for natural gas \ndemand \nforecasting \nmodel. \nEnergy. \n2017;141: \n1269â€“1284. \ndoi:10.1016/j.energy.2017.09.130 \n42.  Toksari M. Predicting the Natural Gas Demand Based on Economic Indicators: Case of \nTurkey. Energy Sources Part Recovery Util Environ Eff. 2010;32: 559â€“566. \ndoi:10.1080/15567030802578823 \n43.  Ozdemir G, Aydemir E, Olgun MO, Mulbay Z. Forecasting of Turkey natural gas demand \nusing a hybrid algorithm. Energy Sources Part B Econ Plan Policy. 2016;11: 295â€“302. \ndoi:10.1080/15567249.2011.611580 \n44.  ArÄ±k OA. Artificial bee colony algorithm to forecast natural gas consumption of Turkey. \nSN Appl Sci. 2019;1: 1138. doi:10.1007/s42452-019-1195-8 \n45.  ES HA, Baban P, Hamzacebi C. Prediction of natural gas demand by considering \nimplications of energy-related policies: The case of TÃ¼rkiye. Energy Sources Part B Econ \nPlan Policy. 2023;18: 2274865. doi:10.1080/15567249.2023.2274865 \n46.  Erdogdu E. Natural gas demand in Turkey. Appl Energy. 2010;87: 211â€“219. \ndoi:10.1016/j.apenergy.2009.07.006 \n47.  Melikoglu M. Vision 2023: Forecasting Turkeyâ€™s natural gas demand between 2013 and \n2030. Renew Sustain Energy Rev. 2013;22: 393â€“400. doi:10.1016/j.rser.2013.01.048 \n48.  Ã–zcan T, KonyalÄ±oÄŸlu AK, ApaydÄ±n T. Forecasting natural gas consumption in Turkey \nusing fractional non-linear grey Bernoulli model optimized by grey wolf optimization \n(GWO) \nalgorithm. \nEuro-Mediterr \nJ \nEnviron \nIntegr. \n2024;9: \n2039â€“2055. \ndoi:10.1007/s41207-024-00618-9 \n49.  Mittakola RT, Ciais P, Zhou C. Short-to-medium range forecast of natural gas use in the \nUnited \nStates \nresidential \nbuildings. \nJ \nClean \nProd. \n2024;437: \n140687. \ndoi:10.1016/j.jclepro.2024.140687 \n50.  Khan MA. Modelling and forecasting the demand for natural gas in Pakistan. Renew \nSustain Energy Rev. 2015;49: 1145â€“1159. doi:10.1016/j.rser.2015.04.154 \n51.  GÃ¶ncÃ¼ A, Karahan MO, KuzubaÅŸ TU. Forecasting daily residential natural gas \nconsumption: A dynamic temperature modelling approach. Bogazici J. 2019;33. \ndoi:10.21773/boun.33.1.3 \n52.  VondrÃ¡Äek J, PelikÃ¡n E, KonÃ¡r O, ÄŒermÃ¡kovÃ¡ J, Eben K, MalÃ½ M, et al. A statistical model \nfor the estimation of natural gas consumption. Appl Energy. 2008;85: 362â€“370. \ndoi:10.1016/j.apenergy.2007.07.004 \n53.  Kaynar O, Yilmaz I, Demirkoparan F. Forecasting of natural gas consumption with neural \nnetwork and neuro fuzzy system. 2011 [cited 19 Feb 2025]. Available: \nhttps://hdl.handle.net/20.500.12418/9677 \n54.  Beyca OF, Ervural BC, Tatoglu E, Ozuyar PG, Zaim S. Using machine learning tools for \nforecasting natural gas consumption in the province of Istanbul. Energy Econ. 2019;80: \n937â€“949. doi:10.1016/j.eneco.2019.03.006 \n55.  Es HA. Monthly natural gas demand forecasting by adjusted seasonal grey forecasting \nmodel. Energy Sources Part Recovery Util Environ Eff. 2021;43: 54â€“69. \ndoi:10.1080/15567036.2020.1831656 \n\n\n39 \n \n56.  Akpinar M, Yumusak N. Year Ahead Demand Forecast of City Natural Gas Using \nSeasonal Time Series Methods. Energies. 2016;9: 727. doi:10.3390/en9090727 \n57.  Ceylan Z. Comparative analysis of deep learning and classical time series methods to \nforecast natural gas demand during COVID-19 pandemic. Energy Sources Part B Econ \nPlan Policy. 2023;18: 2241455. doi:10.1080/15567249.2023.2241455 \n58.  Xu G, Chen Y, Yang M, Li S, Marma KJS. An outlook analysis on Chinaâ€™s natural gas \nconsumption forecast by 2035: Applying a seasonal forecasting method. Energy. \n2023;284: 128602. doi:10.1016/j.energy.2023.128602 \n59.  Zhang L, Ma X, Zhang H, Zhang G, Zhang P. Multi-Step Ahead Natural Gas Consumption \nForecasting Based on a Hybrid Model: Case Studies in The Netherlands and the United \nKingdom. Energies. 2022;15: 7437. doi:10.3390/en15197437 \n60.  Gama J. A survey on learning from data streams: current and future trends. Prog Artif \nIntell. 2012;1: 45â€“55. doi:10.1007/s13748-011-0002-6 \n61.  Ditzler G, Roveri M, Alippi C, Polikar R. Learning in Nonstationary Environments: A \nSurvey. IEEE Comput Intell Mag. 2015;10: 12â€“25. doi:10.1109/MCI.2015.2471196 \n62.  Hinder F, Vaquet V, Hammer B. One or two things we know about concept driftâ€”a survey \non monitoring in evolving environments. Part A: detecting concept drift. Front Artif Intell \nSec Mach Learn Artif Intell. 2024;7. doi:10.3389/frai.2024.1330257. \n63.  Urhan A, Alakent B. Integrating adaptive moving window and just-in-time learning \nparadigms \nfor \nsoft-sensor \ndesign. \nNeurocomputing. \n2020;392: \n23â€“37. \ndoi:10.1016/j.neucom.2020.01.083 \n64.  Bontempi G, Ben Taieb S, Le Borgne Y-A. Machine Learning Strategies for Time Series \nForecasting. In: Aufaure M-A, ZimÃ¡nyi E, editors. Business Intelligence: Second \nEuropean Summer School, eBISS 2012, Brussels, Belgium, July 15-21, 2012, Tutorial \nLectures. Berlin, Heidelberg: Springer; 2013. pp. 62â€“77. doi:10.1007/978-3-642-36318-\n4_3 \n65.  Alakent B. Soft sensor design using transductive moving window learner. Comput Chem \nEng. 2020;140: 106941. doi:10.1016/j.compchemeng.2020.106941 \n66.  Alakent B. Soft-sensor design via task transferred just-in-time-learning coupled \ntransductive moving window learner. J Process Control. 2021;101: 52â€“67. \ndoi:10.1016/j.jprocont.2021.03.006 \n67.  Makridakis S, Wheelwright SC, Hyndman RJ. Forecasting methods and applications. New \nYork, US: Wiley; 1998.  \n68.  Cleveland RB, Cleveland WS, McRae JE, Terpenning I. STL: A seasonal-trend \ndecomposition procedure based on loess. J Off Stat. 1990;6: 3â€“73. Available: \nhttp://www.nniiem.ru/file/news/2016/stl-statistical-model.pdf \n69.  Hyndman RJ, Athanasopoulos G. Forecasting: principles and practice. 3rd ed. Melbourne, \nAustralia: \nOTexts; \n2021. \nAvailable: \nhttps://www.academia.edu/download/64659947/Athanasopoulos__George__Hyndman_\n_Rob_J._-_Forecasting__Principles_and_Practice_2018.pdf \n70.  Theodosiou M. Forecasting monthly and quarterly time series using STL decomposition. \nInt J Forecast. 2011;27: 1178â€“1195. doi:10.1016/j.ijforecast.2010.11.002 \n71.  Hyndman RJ, Athanasopoulos G. Forecasting: principles and practice. 2nd ed. Melbourne, \nAustralia: \nOTexts; \n2018. \nAvailable: \nhttps://www.academia.edu/download/64659947/Athanasopoulos__George__Hyndman_\n_Rob_J._-_Forecasting__Principles_and_Practice_2018.pdf \n72.  Box GE, Jenkins GM, Reinsel GC. Time series analysis: forecasting and control. \nEnglewood Cliffs, NJ: Prentice Hall; 1994.  \n73.  Hyndman R, Koehler AB, Ord JK, Snyder RD. Forecasting with exponential smoothing: \nthe state space approach. Berlin: Springer-Verlag; 2008.  \n\n\n40 \n \n74.  Aoki M. State space modeling of time series. 2nd ed. Berlin: Springer-Verlag; 1990.  \n75.  Brown RG. Statistical forecasting for inventory control. McGraw-Hill; 1959.  \n76.  Holt CC. Forecasting seasonals and trends by exponentially weighted moving averages. \nInt J Forecast. 2004;20: 5â€“10. doi:10.1016/j.ijforecast.2003.09.015 \n77.  Winters PR. Forecasting Sales by Exponentially Weighted Moving Averages. Manag Sci. \n1960;6: 324â€“342. doi:10.1287/mnsc.6.3.324 \n78.  Gardner ES. Exponential smoothing: The state of the art. J Forecast. 1985;4: 1â€“28. \ndoi:10.1002/for.3980040103 \n79.  De Livera AM, Hyndman RJ, Snyder RD. Forecasting Time Series With Complex \nSeasonal Patterns Using Exponential Smoothing. J Am Stat Assoc. 2011;106: 1513â€“1527. \ndoi:10.1198/jasa.2011.tm09771 \n80.  Isikli E, Temizer L, Kazdaloglu AE, Ari E. Time Series Analysis. In: Ustundag A, \nCevikcan E, Beyca OF, editors. Business Analytics for Professionals. Cham: Springer \nInternational Publishing; 2022. pp. 113â€“125. doi:10.1007/978-3-030-93823-9_4 \n81.  Yang Y. Combining forecasting procedures: some theoretical results. Econom Theory. \n2004;20: 176â€“222. doi:10.1017/S0266466604201086 \n82.  Qian W, Rolling CA, Cheng G, Yang Y. Combining forecasts for universally optimal \nperformance. Int J Forecast. 2022;38: 193â€“208. doi:10.1016/j.ijforecast.2021.05.004 \n83.  Borthakur P, Goswami B. Short term load forecasting: A hybrid approach using data \nmining \nmethods. \nPatna, \nIndia: \nIEEE; \n2020. \npp. \n1â€“6. \ndoi:10.1109/ICEFEET49149.2020.9187009 \n84.  Zou H, Yang Y. Combining time series models for forecasting. Int J Forecast. 2004;20: \n69â€“84. doi:10.1016/S0169-2070(03)00004-9 \n85.  Wei X, Yang Y. Robust forecast combinations. J Econom. 2012;166: 224â€“236. Available: \nhttps://www.sciencedirect.com/science/article/pii/S0304407611002168 \n86.  Bishop CM. Pattern recognition and machine learning. New York Springer; 2006.  \n87.  Rasmussen CE, Williams CKI. Gaussian Processes for Machine Learning. Available: \nhttps://direct.mit.edu/books/monograph/2320/Gaussian-Processes-for-Machine-Learning \n88.  Cleveland WS. Robust Locally Weighted Regression and Smoothing Scatterplots. J Am \nStat Assoc. 1979;74: 829â€“836. doi:10.1080/01621459.1979.10481038 \n89.  Aha DW, Kibler D, Albert MK. Instance-based learning algorithms. Mach Learn. 1991;6: \n37â€“66. doi:10.1007/BF00153759 \n90.  Kibler D, Aha DW, Albert MK. Instance-based prediction of real-valued attributes. \nComput Intell. 1989;5: 51â€“57. doi:10.1111/j.1467-8640.1989.tb00315.x \n91.  Bontempi G, Birattari M, Bersini H. Lazy learning for local modelling and control design. \nInt J Control. 1999;72: 643â€“658. doi:10.1080/002071799220830 \n92.  Ã–berg T, Liu T. Global and Local PLS Regression Models to Predict Vapor Pressure. \nQSAR Comb Sci. 2008;27: 273â€“279. doi:10.1002/qsar.200730038 \n93.  MacciÃ² D, Cervellera C. Local Models for data-driven learning of control policies for \ncomplex \nsystems. \nExpert \nSyst \nAppl. \n2012;39: \n13399â€“13408. \ndoi:10.1016/j.eswa.2012.05.063 \n94.  Cheng C, Chiu M-S. A new data-based methodology for nonlinear process modeling. \nChem Eng Sci. 2004;59: 2801â€“2810. doi:10.1016/j.ces.2004.04.020 \n95.  Fujiwara K, Kano M, Hasebe S, Takinami A. Soft-sensor development using correlation-\nbased just-in-time modeling. AIChE J. 2009;55: 1754â€“1765. doi:10.1002/aic.11791 \n96.  Alakent B. Online tuning of predictor weights for relevant data selection in just-in-time-\nlearning. \nChemom \nIntell \nLab \nSyst. \n2020;203: \n104043. \ndoi:10.1016/j.chemolab.2020.104043 \n97.  Karunasingha DSK. Root mean square error or mean absolute error? Use their ratio as \nwell. Inf Sci. 2022;585: 609â€“629. doi:10.1016/j.ins.2021.11.036 \n\n\n41 \n \n98.  Shcherbakov MV, Brebels A, Shcherbakova NL, Tyukov AP, Janovsky TA, Kamaev VA. \nA survey of forecast error measures. World Appl Sci J. 2013;24: 171â€“176. \ndoi:10.5829/idosi.wasj.2013.24.itmies.80032 \n99.  Isikli E, Serdarasan S. The Power of Combination Models in Energy Demand Forecasting. \nIn: Kayakutlu G, Kayalica MÃ–, editors. Decision Making Using AI in Energy and \nSustainability. Cham: Springer International Publishing; 2023. pp. 153â€“167. \ndoi:10.1007/978-3-031-38387-8_9 \n100.  Wright MH. The interior-point revolution in optimization: History, recent developments, \nand lasting consequences. Bull Am Math Soc. 2005;42: 39â€“56. doi:10.1090/S0273-0979-\n04-01040-7 \n101.  GaweÅ‚ B, PaliÅ„ski A. Global and Local Approaches for Forecasting of Long-Term Natural \nGas Consumption in Poland Based on Hierarchical Short Time Series. Energies. 2024;17: \n347. doi:10.3390/en17020347 \n102.  Spatial Prediction and Kriging. Statistics for Spatial Data. John Wiley & Sons, Ltd; 1993. \npp. 105â€“209. doi:10.1002/9781119115151.ch3 \n103.  Sheng H, Liu X, Bai L, Dong H, Cheng Y. Small sample state of health estimation based \non weighted Gaussian process regression. J Energy Storage. 2021;41: 102816. \ndoi:10.1016/j.est.2021.102816 \n104.  Sonjaya AR, Wahyudi I. The Ramadan effect: Illusion or reality? Arab Econ Bus J. \n2016;11: 55â€“71. doi:10.1016/j.aebj.2016.03.001 \n105.  Akpinar M, Adak MF, Yumusak N. Day-ahead natural gas demand forecasting using \noptimized ABC-based neural network with sliding window technique: The case study of \nregional basis in Turkey. Energies. 2017;10: 781. doi:10.3390/en10060781 \n\n\n42 \n \nAppendix: \nSummary of NGDM Studies in Turkey \nReference \nCity * \nPrediction (Forecast) Horizon ** \nData Frequency \nDatabase \nTrain/Test/Validation Ratio \nMethods Used \nBest Method \nPerformance Indicators Used \nTemperature \nHumidity \nWind \nPublic Holidays \nCalendar Data (Weekday/Weekend) \n[35] \nI \nD, H \n2008-2018 \ndaily \nconsumption \nIstanbul Gas Distribution \nCompany (IGDAS) \n75-15-10 \nANN \n- \nMSE \nâˆš \nX \nX \nâˆš \nX \n[48] \n- \nA \n2004-2012 \nannual \nconsumption \nRepublic of Turkey \nEnergy Market Regulatory \nAuthority \n75-25-0 \nGWO-FANGBM, GWO-\nGM, GM, ARIMA, LR \nGWO-FANGBM \nMAE, MAPE, \nRMSE \nX \nX \nX \nX \nX \n[57] \n- \nW \nMarch 2020-\nMarch 2022 \nTurkish Ministry of \nEnergy and Natural \nResources \n70-30-0 \nARIMA, NARNN, SVR, \nLSTM \nLSTM \nRMSE, MAE \nX \nX \nX \nX \nX \n[31] \nAN \nA, D, W \n2004-2013 \ndaily \nconsumption \nBaÅŸkentgaz \n60-40-0 \nLR, MARS, LASSO \nMARS \nMAPE, \nmaxAPE, \nAAE, RMSE, \nğ‘…2 \nâˆš \nâˆš \nâˆš \nX \nâˆš \n[45] \n- \nA \n2000 to 2019 \nannual \nconsumption \nBP Report \n(https://www.bp.com/) \n80-20-0 \nGrey Prediction Models, \nGM Verhulst, Dynamic \nGrey Models \nGM(1,5) \nMAPE, \nRMSE, MSPE \nX \nX \nX \nX \nX \n[38] \n- \nM \n2010-2020 \nmonthly \nconsumption \njodidata.org \n73-27-0 \nDEA, PSO, GSA and \nBSO Linear and Quadratic \nModels \nPSO-Q \nMAE, MAPE, \nRMS, \nMARNE, ğ‘…2 \nâˆš \nâˆš \nâˆš \nX \nX \n[32] \nAN \nD, W \n2004-2013 \ndaily \nconsumption \nBaÅŸkentgaz \n60-40-0 \nRidge Regression, \nCMARS \nCMARS \nMAPE, AAE, \nRMSE, ğ‘…2 \nâˆš \nâˆš \nâˆš \nX \nX \n\n\n43 \n \nReference \nCity * \nPrediction (Forecast) Horizon ** \nData Frequency \nDatabase \nTrain/Test/Validation Ratio \nMethods Used \nBest Method \nPerformance Indicators Used \nTemperature \nHumidity \nWind \nPublic Holidays \nCalendar Data (Weekday/Weekend) \n[33] \nAN, B, \nE, I \nA, D, \nM, W \n2002â€“2017 \ndaily \nconsumption \nBOTAS Petroleum \nPipeline Corporation \n- \nFourier Series Expansion \n(FSE), Fourier Series \nExpansion with \nTemperature (FSET), \nFourier Series Expansion \nwith \nTemperature and \nFeedback (FSETF), AR \nModels \nFourier Series \nExpansion with \nTemperature and \nFeedback (FSETF) \nMAPE, \nRMSE \nâˆš \nX \nX \nX \nX \n[34] \n- \nD \n2017â€“2019 \ndaily \nconsumption \nMinistry of Energy and \nNatural \nResources \n70-30-0 \nARIMAX, SARIMAX, \nANN, \nNARX, LSTM, \nARIMAX-ANN, \nSARIMAX-ANN, GA-\nANN, PSO-ANN \nSARIMAX-ANN \nMAPE, \nRMSE, MSE, \nğ‘…2 \nX \nX \nX \nX \nâˆš \n[55] \n- \nM \n2000-2018 \nmonthly \nconsumption \nInternational Energy \nAgency \n75-25-0 \nSeasonal Grey Forecasting \nModel, Adjusted Seasonal \nGrey Forecasting Model, \nSARIMA \nAdjusted Seasonal \nGrey Forecasting \nModel \nMAPE, MAE, \nRMSE, post-\nerror ratio \nX \nX \nX \nX \nX \n[44] \n- \nA \n1998-2017 \nannual \nconsumption \nEnerData \nhttps ://www.enerd ata.net/ \n- \nArtificial Bee Colony \n(ABC) Algorithm, \nMultiple Linear \nRegression \nArtificial Bee Colony \n(ABC) Algorithm \nMAPE \nX \nX \nX \nX \nX \n[54] \nI \nM \n2005â€“2015 \nmonthly \nconsumption \nIstanbul Gas Distribution \nCompany (IGDAS) \n90-10-0 \nMultiple Linear \nRegression, ANN, SVM \nSVM \nMAPE, ğ‘…2 \nâˆš \nX \nX \nX \nX \n[30] \nAN \nD \n2009â€“2013 \ndaily \nconsumption \nBaÅŸkentgaz \n80-20-0 \nLinear Regression, NN, \nMARS, CMARS \nCMARS \nAAE, RMSE, \nMAPE \nâˆš \nâˆš \nâˆš \nX \nâˆš \n\n\n44 \n \nReference \nCity * \nPrediction (Forecast) Horizon ** \nData Frequency \nDatabase \nTrain/Test/Validation Ratio \nMethods Used \nBest Method \nPerformance Indicators Used \nTemperature \nHumidity \nWind \nPublic Holidays \nCalendar Data (Weekday/Weekend) \n[41] \n- \nA \n1985-2000 \nannual \nconsumption \nTurkish Statistical Institute \n- \nNonlinear Regression, \nNonlinear Regression-\nbased Breeder Genetic \nAlgorithm, Nonlinear \nRegression-based Breeder \nGenetic \nAlgorithm and Simulated \nAnnealing \nNonlinear \nRegression-based \nBreeder Genetic \nAlgorithm and \nSimulated Annealing \nMAPE \nX \nX \nX \nX \nX \n[105] \nA \nD \n2011-2014 \nhouseholds \nand low-\nconsuming \ncommercial \nusersâ€™ daily \nconsumption \nAdapazarÄ± Natural Gas \nDistribution Company \n(AGDAS) \n75-25-0 \nArtificial Bee Colony-\nbased Artificial Neural \nNetworks (ANN-ABC), \nANN-BP Algorithm \nArtificial Bee \nColony-based \nArtificial Neural \nNetworks (ANN-\nABC) \nMAPE, MSE, \nğ‘…2 \nX \nX \nX \nX \nX \n[29] \nA \nD \n2011-43 days \n2012-366 \ndays \nAdapazarÄ± Natural Gas \nDistribution Company \n(AGDAS) \n- \nMultiple Linear \nRegression \n- \nMAPE \nâˆš \nâˆš \nX \nâˆš \nâˆš \n[56] \nA \nM \n2011â€“2014 \nmonthly \nconsumption \nAdapazarÄ± Natural Gas \nDistribution Company \n(AGDAS) \n75-25-0 \nHolt-Winters Exponential \nSmoothing, ARIMA, \nSARIMA \nARIMA \nMAPE, ğ‘…2, \nAIC, BIC \nX \nX \nX \nX \nX \n[43] \n- \nA \n1985-2010 \nannual \nconsumption \nMinistry of Energy and \nNatural \nResources \n65-35-0 \nHybrid Genetic \nAlgorithm-Simulated \nAnnealing (GA-SA) \nAlgorithm, Multiple \nLinear Regression \nGA-SA \nMAPE, RE \nX \nX \nX \nX \nX \n[1] \nA \nD \n2007-2011 \ndaily \nconsumption \nAdapazarÄ± Natural Gas \nDistribution Company \n(AGDAS) \n71-29-0 \nSARIMAX, ANN-MLP, \nANN-RBF, Multivariate \nOLS \nANN-MLP \nMAPE, \nRMSE \nâˆš \nâˆš \nâˆš \nX \nX \n\n\n45 \n \nReference \nCity * \nPrediction (Forecast) Horizon ** \nData Frequency \nDatabase \nTrain/Test/Validation Ratio \nMethods Used \nBest Method \nPerformance Indicators Used \nTemperature \nHumidity \nWind \nPublic Holidays \nCalendar Data (Weekday/Weekend) \n[47] \n- \nA \n1987-2011 \nannual \nconsumption \nBOTAS Petroleum \nPipeline Corporation \n- \nLinear and Logistic \nModels \nLinear Models \nRMSE, \nMAPE, ğ‘…2 \nX \nX \nX \nX \nX \n[51] \nI \nD \n2004-2011 \ndaily \nconsumption \nIstanbul Gas Distribution \nCompany (IGDAS) \n- \nAnalytical Model, Monte \nCarlo Simulation \n- \nRMSE \nâˆš \nâˆš \nX \nâˆš \nâˆš \n[53] \n- \nW \n2002-2006 \nweekly \nconsumption \nBOTAS Petroleum \nPipeline Corporation \n80-20-0 \nARIMA, ANN-MLP, \nANN-RBF, ANNFIS \nANFIS \nMAPE, \nRMSE \nX \nX \nX \nX \nX \n[42] \n- \nA \n1984-2006 \nannual \nconsumption \nMinistry of Energy and \nNatural \nResources \n- \nLinear and Quadratic \nSimulated Annealing \n- \nAbsolute \nValue of \nRelative \nErrors \nX \nX \nX \nX \nX \n[37] \nI \nM \n2004-2007 \nmonthly \nconsumption \nIstanbul Gas Distribution \nCompany (IGDAS) \n60-20-20 \nANN \n- \nARE, ğ‘…2 \nâˆš \nX \nX \nX \nX \n[40] \nAN \nA \n1991-2001  \nBOTAS Petroleum \nPipeline Corporation \n- \nMultiple Linear \nRegression \n- \nğ‘…2 \nâˆš \nX \nX \nX \nX \n[36] \nI, AN,  \nB, E, K \nM \n1996-2001 \nmonthly \nconsumption \nEGO, IGDAS, \nBURSAGAZ, ESGAZ, \nIZGAZ \n- \nAutoregressive Time \nSeries Models \n- \nMAPE, MAE, \nMSE, ğ‘…2 \nâˆš \nX \nX \nX \nX \n* \nA: AdapazarÄ±, AN: Ankara, B: Bursa, E: Eskisehir, I: Istanbul, K: Kocaeli \n**  \nA: Annually, D: Daily, M: Monthly, Q: Quarterly, W: Weekly \n \n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20989v1.pdf",
    "total_pages": 45,
    "title": "Forecasting Monthly Residential Natural Gas Demand Using Just-In-Time-Learning Modeling",
    "authors": [
      "Burak Alakent",
      "Erkan Isikli",
      "Cigdem Kadaifci",
      "Tonguc S. Taspinar"
    ],
    "abstract": "Natural gas (NG) is relatively a clean source of energy, particularly\ncompared to fossil fuels, and worldwide consumption of NG has been increasing\nalmost linearly in the last two decades. A similar trend can also be seen in\nTurkey, while another similarity is the high dependence on imports for the\ncontinuous NG supply. It is crucial to accurately forecast future NG demand\n(NGD) in Turkey, especially, for import contracts; in this respect, forecasts\nof monthly NGD for the following year are of utmost importance. In the current\nstudy, the historical monthly NG consumption data between 2014 and 2024\nprovided by SOCAR, the local residential NG distribution company for two cities\nin Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD\nforecasts for a period of one year and nine months using various time series\nmodels, including SARIMA and ETS models, and a novel proposed machine learning\nmethod. The proposed method, named Just-in-Time-Learning-Gaussian Process\nRegression (JITL-GPR), uses a novel feature representation for the past NG\ndemand values; instead of using past demand values as column-wise separate\nfeatures, they are placed on a two-dimensional (2-D) grid of year-month values.\nFor each test point, a kernel function, tailored for the NGD predictions, is\nused in GPR to predict the query point. Since a model is constructed separately\nfor each test point, the proposed method is, indeed, an example of JITL. The\nJITL-GPR method is easy to use and optimize, and offers a reduction in forecast\nerrors compared to traditional time series methods and a state-of-the-art\ncombination model; therefore, it is a promising tool for NGD forecasting in\nsimilar settings.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}