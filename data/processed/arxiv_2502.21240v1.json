{
  "id": "arxiv_2502.21240v1",
  "text": "arXiv:2502.21240v1  [cs.DS]  28 Feb 2025\n1–36\nPREPRINT\nThe Structural Complexity of Matrix-Vector Multiplication\nEmile Anand\nEMILE@GATECH.EDU\nGeorgia Institute of Technology\nJan van den Brand\nVDBRAND@GATECH.EDU\nGeorgia Institute of Technology\nRose McCarty\nRMCCARTY3@GATECH.EDU\nGeorgia Institute of Technology, Princeton University\nAbstract\nWe consider the problem of preprocessing an n × n matrix M, and supporting queries that, for any vector\nv, returns the matrix-vector product Mv. This problem has been extensively studied in both theory and\npractice: on one side, practitioners have developed algorithms that are highly efﬁcient in practice, whereas\non the other side, theoreticians have proven that the problem cannot be solved faster than naive multiplication\nin the worst-case. This lower bound holds even in the average-case, implying that existing average-case\nanalyses cannot explain this gap between theory and practice. Therefore, we study the problem for structured\nmatrices. We show that for n × n matrices of VC-dimension d, the matrix-vector multiplication problem\ncan be solved with eO(n2) preprocessing and eO(n2−1/d) query time. Given the low constant VC-dimensions\nobserved in most real-world data, our results posit an explanation for why the problem can be solved so much\nfaster in practice. Moreover, our bounds hold even if the matrix does not have a low VC-dimension, but is\nobtained by (possibly adversarially) corrupting at most a subquadratic number of entries of any unknown\nlow VC-dimension matrix.\nIn turn, our results also yield the ﬁrst non-trivial upper bounds for many applications. In previous works,\nthe online matrix-vector (OMv) hypothesis (conjecturing that quadratic time is needed per query, even over\nthe boolean semi-ring) was used to prove many conditional lower bounds, showing that it is impossible to\ncompute and maintain high-accuracy estimates for shortest paths, Laplacian solvers, effective resistance,\nand triangle detection in graphs subject to node insertions and deletions in subquadratic time. Yet, via a\nreduction to our matrix-vector-multiplication result, we show we can maintain the aforementioned prob-\nlems efﬁciently if the input is structured, providing the ﬁrst subquadratic upper bounds in the high-accuracy\nregime.\nKeywords: Online Matrix-Vector Multiplication, Boolean Matrix Multiplication, VC dimension, Parame-\nterized Complexity,\n© E. Anand, J.v.d. Brand & R. McCarty.\n\n\nANAND BRAND MCCARTY\nContents\n1\nIntroduction\n3\n1.1\nOur Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.3\nFurther Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2\nPreliminaries\n9\n3\nThe Structural Complexity of Matrix-Vector Multiplication\n9\n3.1\nMatrix Vector Products via Minimum Spanning Trees . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nExistence of Low Weight ∆-labeled Minimum Spanning Trees . . . . . . . . . . . . . . . .\n11\n3.3\nDynamic Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4\nConclusion\n14\n5\nAcknowledgements\n14\nA Online Matrix Vector Data Structure\n23\nA.1\nStatic Online Matrix Vector Multiplication Data Structure . . . . . . . . . . . . . . . . . . .\n23\nA.2\nDynamic OMv Data Structure\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nA.3\nExtension to Structured Non-Boolean Matrices\n. . . . . . . . . . . . . . . . . . . . . . . .\n28\nB\nApplications\n31\nB.1\nHigh-accuracy Dynamic Laplacian Solver . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nB.2\nEffective Resistance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nB.3\nTriangle Detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nB.4\nSingle-Source Shortest Paths and k-Center . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nC Characterization of matrices with constant VC-dimension\n34\nC.1\nExamples of low VC-dimension matrices\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n2\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\n1. Introduction\nComputing sequential matrix-vector products is a workhorse of many iterative algorithms in machine learn-\ning. In optimization (Renegar, 1996; Vempala et al., 2020; Lin et al., 2023a; Chaudhari et al., 2024), com-\nputational geometry (Har-Peled, 2009; Welzl, 1992; Fisikopoulos and Penaranda, 2016), online algorithms\n(Lin et al., 2024; Anand and Qu, 2024; Murray et al., 2021), and dynamic algorithms (Liu, 2024; Jiang et al.,\n2023; Anand et al., 2024b; Jin and Xu, 2022; Lin et al., 2023b), sequential matrix-vector products are essen-\ntial subroutines underlying performant algorithms. The current learning revolution is powered by hardware\nspeciﬁcally designed to perform such products, as both neural network evaluation and back-propagation re-\nquire repeated matrix-vector products (Sørensen, 2012; Rumelhart et al., 1986). Hence, matrix-vector prod-\nucts are arguably one of the most important and fundamental subroutines, with any complexity improvement\nhaving wide-ranging implications, and is thus a prevalent research topic in both theory and practice.\nThe problem can be modeled via the following data structure task: construct a data structure that prepro-\ncesses a given n × n matrix M. After preprocessing, we want to multiply vectors with M faster than naive\nmatrix-vector multiplication. Depending on the application, this matrix M is ﬁxed or the data structure may\nneed to handle changes to M. For instance, when calculating probabilities of a random walk (e.g., in the\nPageRank algorithm (Page et al., 1999)), performing the power-method, and evaluating a neural network,\nthe matrix is ﬁxed. On the other hand, in convex optimization problems, such as in solving a linear or semi-\ndeﬁnite program, applying Newton-Raphson’s method, and training a neural network, the matrix is either\nthe inverse of a Hessian, or is given by the network’s weights, which changes from one iteration to the next.\nBeyond speedups due to improving hardware accelerators, practitioners have made tremendous progress\nin accelerating the computation of matrix-vector products through heuristics that run in nnz(M) (the number\nof non-zero entries of M) worst-case time, but which are much faster in practice. This has led to speedups\nin training graph neural networks (Alves et al., 2024) and other learning algorithms (Floros et al., 2024).\nWhile practitioners have had success in accelerating the computation of matrix vector products, from\na theory perspective there are substantial lower bounds.\nGronlund and Larsen (2015) showed that any\npoly(n)-space data structure for arbitrary matrix-vector multiplication over sufﬁciently large ﬁelds (such\nas R) require Ω(n2/ log n) time. In particular, any “generic” algorithm that does not make assumptions\nabout the speciﬁc ﬁeld needs Ω(n2/ log n) time. This quadratic lower bound was also proven for arith-\nmetic circuits (Frandsen et al., 2001). While these are worst-case lower bounds, they also hold for the\naverage case (Henzinger et al., 2022). The only non-trivial upper bounds beating quadratic time are over\nthe Boolean semi-ring (i.e., {0, 1} with x ⊕y = min(1, x + y)). Here, the famous mailman algorithm\n(Liberty and Zucker, 2006) achieves O(n2/ log n) runtime. In a line of work that reduced the problem\nto ﬁnding neighborhoods in a graph and smaller algebraic products, this query complexity was improved\nto O(n2/ log2 n) (Williams, 2007) and O(n2−o(1)) (Larsen and Williams, 2017; Chakraborty et al., 2018;\nAbboud et al., 2024). However, none of these algorithms are truly subquadratic, i.e., O(n2−ǫ) for some\nconstant ǫ > 0.\nMoreover, while the Ω(n2) lower bounds only hold for ﬁelds, even over the Boolean semi-ring it is\nconjectured that no truly subquadratic time algorithm exists (Henzinger et al., 2015), referred to as the OMv\n(online matrix-vector multiplication) conjecture. This has led to a line of works that use the conditional\nhardness of OMv to prove tight time lower bounds for a host of dynamic algorithms, such as dynamic\nmatrix inversion (Brand et al., 2019a,b), dynamic subgraph connectivity (Henzinger and Neumann, 2016),\ndynamic regression (Jiang et al., 2023), dynamic range mode (Jin and Xu, 2022), Erickson’s problem and\nLangerman’s problem (Jin and Xu, 2022), as well as high-dimensional generalizations of Klee’s measure\nproblem (Yildiz and Suri, 2012).\nIn summary, there is a substantial difference between the perspectives of this problem in theory and\npractice: there are heuristics that are fast in practice, but from a theoretical perspective the problem is hard,\nand an entire subarea of ﬁne-grained-complexity has been built on this hardness. Moreover, average-case\n3\n\n\nANAND BRAND MCCARTY\nanalysis cannot explain this difference between theory and practice, as even the average-case is provably\nhard. In this work, we resolve this conﬂict. Since the average-case (i.e., random non-structured input) is\nhard, the observed efﬁciency of practical algorithms must stem from some inherent structure in the real-\nworld input data. Thus, we must study the complexity for structured inputs.\nA popular measure for structural complexity is the Vapnik-Chervonenkis (VC) dimension, which ﬁnds\nmany applications in machine learning (Shalev-Shwartz and Ben-David, 2014; Bartlett et al., 2019), struc-\ntural graph theory (Nguyen et al., 2024; Alon et al., 2016; Karczmarz and Zheng, 2024), computational ge-\nometry (Har-Peled, 2009; B. Chazelle, 1989; Fisikopoulos and Penaranda, 2016) (see the preliminaries in\nSection 2 for a deﬁnition of the VC-dimension). Many structured objects studied by theoreticians have in-\nherently low VC dimension. For instance, the hypothesis class of intervals on R, star and interval graphs,\nand rank-one projection matrices, have VC-dimension 2, planar graphs have VC dimension 4, and any H-\nminor free graph has VC dimension at most |V (H)| −1. Beyond being an established complexity measure\nin theory, it has also been empirically veriﬁed that real-world data has low VC dimension (Coudert et al.,\n2024). This observation can be explained via the following structural characterization of matrices with low\nVC dimension:\nFact 1 (Proof in Appendix C) Consider a hereditary class of 0/1-matrices M, meaning that M is closed\nunder row/column deletion (i.e., each matrix M ∈M is still in M after deleting a row or column). If M is\nnon-trivial, (i.e., does not contain every possible matrix), then there exists an absolute constant c ∈N such\nthat the VC-dimension of any matrix in M is at most c.\nPresumably, whatever unknown structure is common in real-world matrices should not be lost when\ndeleting rows/columns. We note that Fact 1 does not preclude other matrices, that do not satisfy this heredi-\ntary property, from having constant VC dimension, e.g., adjacency matrices of minor-free graphs.\n1.1. Our Results\nWhile theoretical objects such as grid-graphs, intersection graphs, and kernel-matrices have a low VC-\ndimension, they are not perfect representations of real-world data. For instance, while city street networks\nin the US are predominantly grid-graphs, there are always exceptions in form of bridges or a few diagonal\nstreets. Exceptions can also occur due to errors in measurements or other corruptions. To capture such\nalmost low VC-dimension objects, we deﬁne the “corrupted VC-dimension.”\nDeﬁnition 2 (Corrupted VC-dimension d)\nWe say that a set system F on a ground set of size n has\n“corrupted VC-dimension d” if there is another set system F′ of VC-dimension ≤d such that F can be\nobtained from F′ by adding and/or removing (i.e., corrupting) at most O(n1−1/d) elements to/from each\nset.\nFor matrices M ∈{0, 1}m×n, the corrupted VC-dimension d is equivalent to the existence of matrices\nL ∈{0, 1}m×n and S ∈{−1, 0, 1}m×n with M = L + S where L has VC-dimension d and the corruption\nmatrix S has at most O(n1−1/d) non-zero entries per row, i.e., ﬂipping at most O(n1−1/d) bits per row of L.\nNote that merely the existence of such an F′ (i.e., L) of VC-dimension d sufﬁces for F (i.e., M) to have\ncorrupted VC-dimension d. We do not need to know F′ (i.e., L) or which sets have been corrupted (i.e.,\nS). Furthermore, set families of corrupted VC-dimension d are strict generalizations of set families of VC-\ndimension exactly d. Therefore, while we state our main results for matrices with corrupted VC-dimension\nd, they also hold for general matrices with VC-dimension d.\nWe now state the theoretical guarantees of our accelerated matrix-vector multiplication algorithm in\nTheorem 3, where the runtimes are parameterized by the corrupted VC dimension measure.\n4\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nTheorem 3 (Static Online Matrix-Vector Multiplication). If a matrix M ∈{0, 1}m×n has corrupted VC-\ndimension d, then after an eO(mn)-time preprocessing, there is a data structure D that can compute Mv for\nany v ∈Rn in eO(mn1−1/d + n) time, with high probability.\nThe problem of approximating the VC-dimension of any set system within a (2 −ǫ) accuracy for any\nǫ > 0 is known to be ΣP\n3 -hard (Mossel and Umans, 2002); however, for our theorem, the algorithm does not\nneed to know or compute the VC-dimension to achieve this runtime.\nThis data structure is based on Gasieniec and Lingas (2003) which proposed an algorithm for Boolean\nmatrix multiplication. Their algorithm computes the matrix-matrix product via n matrix-vector products,\nthus implicitly providing a matrix-vector multiplication data structure as well. Their complexity depends on\nthe weight of the minimum spanning tree deﬁned with respect to the Hamming-distance between the rows\nof M. While O(n2) in worst-case, they conjecture this should be unlikely in real-world data. This data\nstructure was recently independently rediscovered in the graph neural networks community (Alves et al.,\n2024) where they experimentally veriﬁed that the algorithm is indeed efﬁcient on real-world inputs. Our\nTheorem 3 now gives a theoretical explanation as to why these practical algorithm behave so much more\nefﬁcient in practice than the worst-case and average-case lower bounds of Ω(mn).\nWe obtain Theorem 3 via techniques from computational geometry. A line of works (Welzl, 1988;\nB. Chazelle, 1989; Matoušek, 1991; Har-Peled, 2009) consider the following geometric intersection data\nstructure problem: given a set of points in Rd, preprocess them. Then, given a convex polytope for each\nquery, return whether any of the points intersect the polytope.\nMatrix vector products can be interpreted as O(m) intersection problems if we only care about Boolean\noutputs, because in the Boolean case (Mv)i ̸= 0 if and only if there is an intersection between the position of\nthe non-zero elements in v and the non-zero elements in the i-th row of M. We show that the computational\ngeometry techniques work even for non-boolean outputs, and we also extend them from bounded VC-\ndimension to our notion of corrupted VC-dimension. We provide details of this proof in Section 3.2.\nAt last, we remark that prior work for the geometric intersection problem had exponential preprocessing\ntime (Welzl, 1992) or unspeciﬁed polynomial time (Matoušek, 1991).\nIn addition to our novel complexity bound, we also provide alternative algorithms to that of Gasieniec and Lingas\n(2003); Alves et al. (2024). We provide more details below.\nDynamic Setting\nWe extend Theorem 3 by allowing M to undergo row and column updates. The proof\nis given in Appendix A.2.\nTheorem 4 (Dynamic Online Matrix-Vector Multiplication).\nGiven a matrix M ∈{0, 1}m×n, there is a\ndata structure D with eO(mn) preprocessing time that supports row and column updates (insertions/deletions)\nto M in eO(n) and eO(m) time respectively. Upon querying the data structure D with a vector v ∈Rn, it out-\nputs Mv in eO(mn1−1/d∗+ n) time, with high probability, where d∗is the largest corrupted VC-dimension\nof M throughout the history of its updates.\nIn the special case where the updates to M keep its corrupted VC-dimension d invariant, each matrix-\nvector product is computed with high probability in eO(mn1−1/d + n) time.\nExtensions to Transposed Matrices.\nAssume we have a small VC-dimension bound on M⊤, then the\nVC-dimension of M could be much larger than that of M⊤. If d bounds the VC-dimension of M⊤,\nthen the best bound on the VC-dimension of M is 2d+1 (Assouad, 1983), so Theorem 3 would have only\neO(mn1−1/2d) query time. To alleviate this substantial slow-down in Theorems 3 and 4, we extend the\nalgorithms so their complexity is also bounded by the VC-dimension of M⊤.\nWhen M⊤∈{0, 1}n×m has corrupted VC-dimension d′, we prove analogous versions of Theorems 3\nand 4 that allow computing Mv with high probability in O(nm1−1/d′ + m) time after a eO(mn)-time\npreprocessing. Thus, if M ∈{0, 1}m×n has corrupted VC-dimension d and M⊤∈{0, 1}n×m has corrupted\n5\n\n\nANAND BRAND MCCARTY\nVC-dimension d′, then after an eO(mn)-time preprocessing, we can compute Mv in time min{O(mn1−1/d+\nn, nm1−1/d′ + m)}. For this, we do not need to know d or d′, as the algorithm automatically achieves the\nminimum runtime.\nExtension to Non-Boolean Matrices.\nThe Pollard pseudodimension is a popular extension of the VC-\ndimension to non-binary thresholds, which also ﬁnds many applications in machine learning (Shalev-Shwartz and Ben-David,\n2014; Bartlett et al., 2019) and structural graph theory (Karczmarz and Zheng, 2024) (see the preliminaries\nin Section 2 for a deﬁnition of the Pollard pseudodimension). For a real-valued matrix M ∈{0, 1}m×n,\nif d bounds the Pollard pseudodimension of M and if A bounds the number of thresholds of M, then by\nlemma 29, Mv can be computed in time eO(Amn1−1/d + n). For A ≤eO(1), this provides a subquadratic\nruntime.\n1.2. Applications\nA Scaling technique with Applications to Random Walks and PageRank.\nLet A ∈Rn×n be the\nadjacency matrix of an n-node graph and let D ∈Rn×n be the diagonal matrix with vertex-degrees on the\ndiagonal. Then the normalized Laplacian is eL = I−D−1/2AD−1/2. Even though eL is not Boolean, we can\nstill use Theorem 3 since eLv = D−1/2(w −Aw) for w = D−1/2v and the product Aw can be computed\nvia Theorem 3. The same technique extends to random-walk probability matrices. In turn, this allows fast\nsimulations of Markov chains to compute stationary distributions and iterative computation of the PageRank\n(Page et al., 1999) via Theorem 3.\nComplexity-Theoretic Implications.\nThe Boolean matrix multiplication (BMM) conjecture states that no\nalgorithm can multiply two boolean n × n matrices in O(n3−ǫ) time for constant ǫ > 0 (Basch et al., 1995;\nWilliams and Williams, 2010). This conjecture is used to prove lower bounds on combinatorial algorithms\n(i.e., algorithms that do not make use of Strassen-style fast matrix multiplication methods), motivated by the\nproperty that fast matrix multiplication, while being theoretically fast (currently O(n2.372) Williams et al.\n(2024)), is very slow in practice due to large constants hidden in O-notation. Hence BMM gives a lower\nbound on more practical algorithms and can be used to prove that any further complexity improvement\nwould require fast matrix multiplication.\nTheorem 3 implies that for matrices with corrupted VC-dimension d, BMM can be solved in eO(n3−1/d)\ntime, allowing for improved combinatorial upper bounds. BMM was the original motivation for Gasieniec and Lingas\n(2003). Moreover, as demonstrated in Alves et al. (2024), this algorithm is also highly efﬁcient in practice\nand does not encounter the large constant of fast matrix multiplication. For matrices of VC-dimension\n1, Theorem 3 achieves the optimal matrix-multiplication time of eO(n2) (upto polylogarithmic factors).\nThrough this view, Theorem 3 implies that problems with lower bounds based on BMM, may be solved\nmore efﬁciently on structured inputs.\nCorollary 5 Suppose we are given Boolean matrices M1, M2 ∈{0, 1}n×n, such that Mi has corrupted\nVC-dimension di and M⊤\ni has corrupted VC dimension d′\ni for i ∈{1, 2}. Let s = min{d1, d2, d′\n1, d′\n2}.\nThen, there is an algorithm for boolean matrix multiplication with in O(n3−1\ns ) time complexity.\nAnother relevant conjecture is the OMv conjecture, which states that no algorithm can preprocess a\nBoolean matrix M ∈{0, 1}n×n in polynomial time, and then answer n queries that compute Mv(i) for\nan online sequence of Boolean vectors v(1), ..., v(n) ∈{0, 1}n in total time O(n3−ǫ). Observe that here the\nvector v(i+1) is only given after the data structure returned Mv(i), hence the use of fast matrix multiplication\nto compute the matrix product M [v(1)|...|v(n)] is ruled out.\nThis conjecture is used to prove conditional lower bounds for many dynamic problems, such as maintain-\ning shortest paths, effective resistances, reachability, maximum ﬂow, bipartite matching, triangle detection,\nand many others in graphs undergoing vertex insertions and deletions (Henzinger et al., 2015). Even with\n6\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nuse of fast matrix multiplication, no algorithm can beat O(n2) time on dense graphs, which for many of\nthese problems is equivalent to re-solving them from scratch whenever the graph changes. Since Theorem 3\nshows the OMv conjecture does not hold for structured matrices, it opens up improved dynamic algorithms.\nWe next list some examples of new dynamic algorithms obtained via this technique, whose upper bounds\nare only achievable due to their structured nature, otherwise violating either the BMM or OMv conjecture.\nImplications for Dynamic Algorithms.\nIn the following, we state results for dynamic graph algorithms.\nThese are data structures that maintain some graph property while the underlying graph changes over time.\nWe defer the formal proofs for these results to Appendix B.\nAn important class of instances of linear equations that arise in practice has the form Lx = b, where\nL is the Laplacian of an undirected graph G = (V, E) given by L := D −A where D is a diagonal\nmatrix such that Di,i = deg(i) for i ∈[n] where |V | = n, and A is the adjacency matrix of the graph.\nThere are Laplacian solvers that run in time nearly linear in the number of edges of the underlying graph\n(Spielman and Teng, 2004). We study the problem of constructing a dynamic Laplacian solver for Ltx = bt\nwhere the Laplacian changes over time through vertex updates to the graph, and each bt is an arbitrary new\nvector.\nPrevious upper bounds could only handle a poly(1/ǫ) accuracy by maintaining spectral sparsiﬁers\n(Abraham et al., 2016; Bernstein et al., 2022) or vertex sparsiﬁers (Durfee et al., 2019; Chen et al., 2020).\nMoreover, exact or O(log 1/ǫ)-time dependencies are ruled out by the OMv conjecture (since no algorithm\nbeats naive recomputation from scratch in eO(|E|) = eO(n2) time via nearly-linear time Laplacian system\nsolvers (Spielman and Teng, 2004)). We provide the ﬁrst result faster than naive recomputation from scratch\nin the high-accuracy log(1/ǫ) regime.\nTheorem 6 (Dynamic Laplacian Solver). There is a dynamic algorithm that, given a dynamic graph G =\n(V, E) with corrupted VC-dimension bounded by d, maintains a Laplacian system solver. The data structure\nsupports queries that receive a vector b ∈R|V | and error parameter ǫ > 0. Then, in eO(n2−1/d log 1/ǫ)\ntime, the algorithm returns the (approximate) solution x to Lx∗= b where ∥x −x∗∥L ≤ǫ∥x∗∥L. Each\nvertex update to G takes eO(n) time.\nUsing the fast dynamic Laplacian solver, we maintain dynamic effective resistances of a graph, which is\na critical computation for graph clustering (Alev et al., 2017), fault-tolerant computing (Ghosh et al., 2008),\nnetwork analysis (Tizghadam and Leon-Garcia, 2008; Anand and Umans, 2023; Anand et al., 2024a), and\nbiological systems (Klein et al., 2022), as it signiﬁes the degree of connectivity between two vertices. The\neffective resistance rG(a, b) between vertex a and vertex b in a graph G is a graph-theoretic analog of\nleverage scores that represents the energy needed to route one unit of electric ﬂow from a to b, and has a\nnatural linear algebraic formulation: let ei denote the i-th standard basis vector. Then, for all a, b ∈V ,\nrG(a, b) = (ea −eb)⊤L†(ea −eb), where L is the Laplacian of G. Finally, L† denotes the Moore-Penrose\npseudoinverse of L.\nTheorem 7 (Dynamic Effective Resistance).\nThere is a dynamic algorithm that, given a dynamic graph\nG = (V, E) with corrupted VC-dimension bounded by d, maintains effective resistances in G. The data\nstructure supports queries that receive a pair of vertices u, v ∈V and error parameter ǫ > 0. Then, in\neO(n2−1/d log 1/ǫ) time, the algorithm returns a (1±ǫ)-approximation of the effective resistance. Moreover,\neach node update to G in the dynamic data structure takes eO(n) time.\nOur dynamic matrix-vector multiplication data structure also leads to faster dynamic triangle detection\nalgorithms for graphs with corrupted VC-dimension d, which are critical components of fraud detection sys-\ntems and community-detection algorithms. No subquadratic time algorithm exists for vertex updates, condi-\ntional on BMM (Abboud and Vassilevska Williams, 2014), and also conditional on OMv (Henzinger et al.,\n2015). We show that these lower bounds can be broken on structured graphs.\n7\n\n\nANAND BRAND MCCARTY\nTheorem 8 (Dynamic Triangle Detection).\nThere is a dynamic algorithm that, given a dynamic graph\nG = (V, E) with corrupted VC-dimension d, maintains whether G contains a triangle or not. Each vertex\nupdate to G takes O(n2−1/d) time and returns a Boolean indicator for whether G contains a triangle or not.\nFurthermore, our techniques lead to faster dynamic approximate algorithms for single-source distances. In\nthis problem, we are tasked with ﬁnding the distances from a designated source vertex to every other vertex.\nTheorem 9 (Dynamic Approximate Single-Source Shortest Paths). There is a dynamic algorithm that main-\ntains (1 + ǫ)-approximate single source distances on a dynamic unweighted undirected graph G = (V, E).\nIf the corrupted VC-dimension of G is bounded by d, each vertex update to G takes eO(kn2−1/(2d)/ǫ) time,\nand querying the distances for any source node takes O(n2−1/(2d)/ǫ) time.\nGiven a metric space M with n points and a positive integer k ≤n, the k-center problem asks one to\nselect k points, referred to as centers, such that the maximum distance of any point in the metric space to its\nclosest center is minimized. It is known that the k-center is NP-hard to approximate within a factor of (2−ǫ)\nfor any ǫ > 0 (Hsu and Nemhauser, 1979). The best existing dynamic algorithms (Cruciani et al., 2024) only\nwork with edge updates and use fast matrix multiplication as a black-box. We present an algorithm without\nfast matrix multiplication that supports more powerful vertex updates.\nTheorem 10 (Dynamic Approximate k-center). Given an unweighted undirected graph G = (V, E), there\nis a dynamic algorithm for (2 + ǫ)-approximate k-center with node update time O(kn2−1/2d/ǫ), where d is\na bound on the corrupted VC-dimension of G.\n1.3. Further Related Work\nResults for general Boolean matrix multiplication.\nA line of work shows that the complexity of matrix-\nvector multiplication for general Boolean matrices (where the multiplication is over the Boolean semiring)\ncan attain a subpolynomial speed-up. For instance, Williams (2007) showed that BMM can be computed\nin O(n3/ log n) time. Larsen and Williams (2017) extended this to O(n3/2Ω(√log n)). This was further\nextended in Abboud et al. (2024) to O(n3/2Ω( 7√log n)).\nResults for structured matrices.\nWhen the underlying matrix is Vandermonde, Toeplitz, Hankel, or\nCauchy, a body of work shows that they admit fast matrix-vector multiplications (Motzkin, 1951; Gohberg and Olshevsky,\n1994; Olshevsky and Shokrollahi, 2000; Pan and Tsigaridas, 2014). Orthogonal-vector (OV) matrices are\nstructured matrices where each row and column receives a label v ∈{0, 1}d and Mi,j = 1 if and only if the\ncorresponding two labels are orthogonal. Similar to our corrupted VC-dimension results, Alman and Vassilevska Williams\n(2020) show that such OV matrices allow for fast matrix-vector products if there are few corruptions to the\nmatrix.\nAnother approach to handle structured inputs is through the algorithms with predictions regime, where\nsome learning algorithm extracts the structure and can predict information about future vectors, leading to\nfaster matrix-vector multiplication algorithms Henzinger et al. (2024); Brand et al. (2024).\nResult for dynamic structured graphs.\nOur dynamic algorithms hold for any structured graph, even if\nwe do not know their structure. When the structure is known, there exist specialized dynamic algorithms\ntailored to the speciﬁc graph class. These include, for instance, interval graphs Crespelle (2019); Chen et al.\n(2024), geometric graphs deﬁned from Kernels (Alman et al., 2020), planar graphs (Korhonen et al., 2024),\ndynamic planar graphs (Abboud and Dahlgaard, 2016), and H-minor free graphs (Dorn et al., 2012).\n8\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nMatrix vector multiplication in optimization.\nFor convex optimization algorithms, there is a long history\non developing data structures that facilitate fast matrix-vector products. Here the matrices are generally pro-\njection matrices that require computation of some matrix inverse. While historically, most research was on\naccelerating the maintenance of the matrix inverse (Karmarkar, 1984; Vaidya, 1989; Lee and Sidford, 2015;\nCohen et al., 2021; Lee et al., 2019; Brand, 2020; Jiang et al., 2021, 2020a; Huang et al., 2022; Jiang et al.,\n2022; Lee et al., 2015; Jiang et al., 2020b), this research has progressed so far that the current bottleneck for\nthe fastest linear program solvers are simple matrix-vector products (Brand et al., 2020, 2021).\n2. Preliminaries\nNotation.\nLet [n] = {1, . . . , n}. We use O(·) to hide constant factors, and eO to hide polylogarithmic\nfactors. Let ∥· ∥1 denote the ℓ1 (Manhattan distance). For any matrix M, let Mi and M:,i denote the i’th\ncolumn and i’th row (respectively) of M. For a set V we write M ∈Rm×V for the m × |V | matrix where\nwe can index columns by x ∈V , i.e., Mx ∈Rm. For any vector x ∈Rn, the Hamming weight of x is the\nnumber of non-zero entries of x also denoted by nnz(x). For Boolean vectors x, y ∈{0, 1}n, the Hamming\ndistance between x and y is given by ∥x −y∥1. Finally, for any vector x ∈Rn and positive deﬁnite matrix\nL ∈Rn×n, deﬁne the L-induced norm of x by ∥x∥L =\n√\nx⊤Lx.\nVC-Dimension of Range Spaces.\nA range space (also called a set system) is a pair R = (X, R) where\nX is a set and R is a set of subsets of X. Let ΠR(A) = {A ∩r : r ∈R}. The VC-dimension of R is\ngiven by VC(R) = max{|S| : S ⊆X and |ΠR(S)| = 2|S|}. The primal shatter function π of R is given by\nπ(m) = max{|ΠR(A)| : A ⊆X, |A| = m} for m ≥0. Let B be a set of ranges in R. Then, let Π∗\nX(B) =\n{C : C ⊆Xand C is not crossed by any range in Q} and let Π∗\nX(B) be maximal. Then, for m ≥0, the\ndual shatter function π∗of the range space is given by π∗(m) = max{|Π∗\nX(B)| : B ⊆R, |B| = m}.\nVC-Dimension of Matrices.\nAny matrix M ∈{0, 1}m×n corresponds to a set family given by FM =\n{{j : Mi,j = 1} : i ∈[m]}, i.e., consider each row of M as the indicator vector of some set. Through it,\nwe can encode M by the range space RM = ([n], FM). Then, the VC-dimension of M is deﬁned as the\nVC-dimension of RM. Here, a subset S of the columns of M is said to be shattered if each of the 2|S| many\n0/1 strings appears in some row in the restriction of M to the columns in S. Then, the VC-dimension of\nM is the maximum size of a shattered subset of the columns of M. Moreover, the VC-dimension dm,n of a\nmatrix M ∈{0, 1}m×n satisﬁes 1 ≤dm,n ≤log n.\nPollard Pseudodimension of Matrices.\nFor a family of functions F mapping V to {0, . . . , ℓ−1},\nthe Pollard pseudodimension of F, denoted by Pdim(F), is the largest integer d such that there exists\nx1, . . . , xd ∈V and thresholds y1, . . . , yd ∈[ℓ] such that for any d-length bit vector b = (b1, . . . , bd) ∈\n{0, 1}d, there exists an f ∈F such that for all i ∈[d], f(xi) ≤yi if and only if bi = 1. For a real\nmatrix M, deﬁne the pseudo-dimension of M, denoted by Pdim(M) by thinking of the rows of M as\nfunctions and taking the pseudo-dimension of the resulting class of functions. Speciﬁcally, if M is an\nm × n matrix, for each i ∈{1, . . . , m}, deﬁne fM,i : {1, . . . , n} →R by fM,i(j) = Mi,j, and let\nPdim(M) = Pdim({fM,i : i ∈{1, . . . , m}}).\n3. The Structural Complexity of Matrix-Vector Multiplication\nWe present a technical overview of the algorithms and the main theorems. We begin this section with a\nquick recap of previous work Björklund and Lingas (2001); Alves et al. (2024) who established a connection\nbetween matrix vector products and minimum spanning trees on n points {0, 1}m, where the edge weights\nare given by the Hamming distance between two points (i.e., matrix M deﬁnes a collection of points).\n9\n\n\nANAND BRAND MCCARTY\nHaving established this connection, we prove one of our main results in Section 3.2. We prove via\ntechniques from computational geometry that the weight of the minimum spanning tree must be bounded,\nif the matrix M has corrupted VC-dimension at most d. This then yields our main result Theorem 3.\n3.1. Matrix Vector Products via Minimum Spanning Trees\nWe here recap the algorithm idea by Björklund and Lingas (2001), which was later independently rediscov-\nered by Alves et al. (2024). We also explain how to extend the results from M to M⊤.\nDifferential Compression.\nLet M ∈{0, 1}m×n be a binary matrix, and let Mi denote the i’th column\nof M for i ∈[n]. We differentially compress M by writing each column Mx as the sum of another column\nMy and a change vector ∆x,y such that My = Mx + ∆x,y and ∆x,y = Mx −My.\nIf Mx and My are similar, then ∥∆x,y∥1 likely to be smaller than the number of nonzero elements of\nMx. Therefore, it is more efﬁcient to indirectly represent Mx with respect to My, rather than through the\nmatrix M. This necessitates an algorithm that ﬁnds a suitable chain of ∆’s to represent all columns of M:\nfor each column Mx, identify a similar column My that characterizes the former, such that the Hamming\nweight of ∆x,y required to represent Mx is minimized subject to My.\nSince the algorithm measures the Hamming distance for each pair of columns in M, we can model this\ncompression by an undirected weighted graph on G on n vertices, where vertex i represents column i of M,\nand the weight of each edge (x, y) corresponds to the Hamming distance between columns x and y. We can\nthen ﬁnd a Minimum Spanning Tree (MST) of G, which by deﬁnition spans G with the minimum sum of\nedge weights possible. Therefore, any MST of G rooted at vertex r deﬁnes a chain of ∆’s that, starting at r,\ncan represent all the columns of M.\nWe formalize this intuition by deﬁning the ∆-labeled spanning tree of a matrix and its weight.\nDeﬁnition 11 (∆-labeled spanning tree of matrix M) A ∆-labeled tree is a tree T = (V, E) with some\nexplicit root x, where each edge e ∈E is directed away from x and labeled by a vector ∆e ∈Rm. We say\nT is a ∆-labeled spanning tree for matrix M ∈Rm×n if Mx = 0 for root x, and for all edges (i, j) ∈E\nwe have ∆i,j := Mj −Mi.\nDeﬁnition 12 (Weight of a ∆-labeled spanning tree)\nFor any ∆-labeled spanning tree of M denoted by\nT with directed edge-set E, deﬁne weight(T) as the total number of non-zero entries given by P\ne∈T nnz(∆e).\nThe minimum spanning tree of M is then the ∆-labeled spanning tree with the smallest possible weight.\nGiven any spanning tree of M, we use the weight of the ∆-labeled spanning tree to parameterize the runtime\nof the matrix-vector multiplication algorithm. Here, the lower the weight of the ∆-labeled spanning tree,\nthe faster the runtime.\nLemma 13 (Gasieniec and Lingas (2003); Alves et al. (2024)) Given a ∆-labeled spanning tree T of M⊤∈\n{0, 1}n×m and a vector v ∈Rn, there is an algorithm that computes Mv in O(weight(T) + n + m) time.\nWe describe this procedure in Algorithm 1.\nAs a quick recap, the algorithm computes Mv by starting at a root node r ∈T of the spanning tree\nand performing depth-ﬁrst search (DFS). The algorithm initializes an output vector and computes (Mv)r\nin O(n + m) time. For each edge (x, y) traversed in the recursion tree, the algorithm computes (Mv)y =\n(Mv)x + ∆⊤\ny,xv. The complexity of this iterative procedure is proportional to weight(T) = P\ne∈E(T) δe\nwhere for each edge e = (x, y) in the ∆-labeled spanning tree, δe is the number of non-zeros in ∆e, i.e.,\nthe Hamming distance between row x and row y of M. For completeness sake, we provide a proof for\nLemma 13 in Appendix A.1.\nSince the weight of the ∆-labeled spanning tree of M⊤can be different from the weight of the ∆-labeled\nspanning tree of M, we provide a new algorithm with an analogous theoretical guarantee in lemma 14.\n10\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nLemma 14\nGiven a ∆-labeled spanning tree T of M ∈{0, 1}m×n and a vector v ∈Rn, there is an\nalgorithm that can compute Mv in O(weight(T) + n + m) time.\nTo utilize the ∆-labeled spanning tree of M, note that Mv = Pn\ni=1 Mivi. For simplicity, consider the\nspecial case where M has only two columns, then Mv = M1vx + M2v2 = M1(v1 + v2) + ∆1,2v2. For\nmore columns, this can be extended to require only a product with M1 and products with each ∆e of the\n∆-labeled spanning tree. Thus the complexity is bounded by O(m + n + weight(T)).\nWe provide proofs for Lemma 14 in Appendix A.1.\nSince we wish to minimize the time complexity, we compute a minimum spanning tree. In Alves et al.\n(2024) this was done naïvely, whereas in Björklund and Lingas (2001) this is done via Theorem 4.2 of\nIndyk and Motwani (1998).\nLemma 15 (Theorem 4.2 of Indyk and Motwani (1998))\nFor any ǫ > 0, we can compute an (1 + ǫ)-\napproximate MST with respect to the Hamming distance of n points in {0, 1}m in eO(mn1+1/(1+ǫ)) time.\nTheorem 16 Given Boolean m×n matrix M, we can construct in eO(nm) time a ∆-labeled spanning tree\nwhose weight is at most a O(log n)-factor larger than the minimum spanning tree (MST) of M.\nProof of Theorem 16\nRun Lemma 15 on the columns of M for ǫ = log n to construct an O(log n)-\napproximate MST in eO(mn) time. Next, iterate over the edges of the MST and compute the ∆-labels, i.e.,\nlabel each tree edge (x, y) by ∆x,y = Mx −My. This takes eO(mn) time as there are only n −1 tree edges\nand ∆x,y ∈Rm.\n3.2. Existence of Low Weight ∆-labeled Minimum Spanning Trees\nThe Hamming distance between any two columns in M is at most m. Since there are O(n) edges in the\n∆-labeled MST, this gives a coarse bound of O(mn) which is achieved for Hadamard matrices. One of our\nmain technical contributions is to prove that ∆-labeled MST’s with low weight exist for structure matrices.\nWe give a bound on the MST weight parameterized by the corrupted VC-dimension of the matrix.\nTheorem 17\nIf a matrix has corrupted VC-dimension d, then the weight of the corresponding MST is at\nmost O(mn1−1/d log n).\nWe show that for matrices of bounded VC-dimension, the minimum spanning tree T has small weight.\nFor this, we work directly with the deﬁnition of VC-dimension and use set terminology, which allows us\nto use relevant work from Welzl (1988); B. Chazelle (1989); Matoušek (1991); Har-Peled (2009). This\nprevious work studied bounded VC-dimensions in the context of computational geometry. We here use their\nresults to bound the complexity of matrix vector products.\nDeﬁnition 18 (Set crossings) Let A and r be sets. We say that A crosses r if the symmetric difference is\nnon-empty, i.e. A△r ̸= ∅. In other words, A crosses r if neither A ⊆r nor A ∩r = ∅holds.\nDeﬁnition 19 (Spanning Tree) Let A ⊂X be a ﬁnite set of elements of a range space (X, R). A spanning\ntree on A is an undirected (unrooted) tree with node set A.\nDeﬁnition 20 (Crossing number κ(T) of spanning tree T) A crossing of a range r ∈R in T is an edge\nin T that is crossed by r (i.e., an edge that has one endpoint in r and the other endpoint in [n] \\ r). The\ncrossing number κr(T) or a range r ∈R is the number of crossings of r in T. The crossing number κ(T)\nof T is the maximum crossing number of a range r ∈R in T. In other words, κ(T) := maxr∈R κr(T).\n11\n\n\nANAND BRAND MCCARTY\nThis notion of a spanning tree corresponds to the notion of the ∆-labeled spanning tree for matrix M. The\nset system (X, R) induced by M ∈{0, 1}m×n uses set X = [n] (i.e., column indices) with the rows of M\nbeing the indicator vectors of m subsets of X. Thus, a spanning tree TX for (X, R) and a spanning tree\nTM for M both consist of edges that connect elements from [n]. Furthermore, the number of crossings for\nan edge (x, y) ∈TX is exactly the number of indices i where (Mx)i and (My)i differ. Thus the crossing\nnumber of a tree TX for (X, R) is exactly the weight of the corresponding ∆-labeled spanning tree for M.\nFor the rest of this section, we will argue that range spaces (X, R) of corrupted VC-dimension d have\nspanning trees with small crossing number.\nThe literature on ﬁnding a spanning tree on X with a low crossing number stems from a line of ear-\nlier works on computational geometry from the 1900s (Welzl, 1988; B. Chazelle, 1989; Matoušek, 1991;\nHar-Peled, 2009). They used the trees so preprocess a set of points in Rd to quickly answer queries where\nfor any arbitrary convex polytope, the query returns a point within the polytope. Here, we transfer these\ntechniques to the online matrix-vector multiplication problem and extend them to the notion of corrupted\nVC-dimension.\nLemma 21 (Lemma 4.1 of Welzl (1988)) Let (X, R) be a range space VC-dimension at most d. For every\nA ⊆X, |A| = n ≥1 and every multiset Q of ranges in R with |Q| = m, there are two elements x, y ∈A\nwhere x ̸= y such that the number of ranges in Q crossing (x, y) is at most O(m · n−1/d log n).\nThis lemma allows us to prove the existence of a ∆-labeled spanning tree for matrices with bounded\nVC-dimension. We later extend this to corrupted VC-dimension.\nLemma 22 Let (X, R) be a range-space with |X| = n, |R| = m of VC-dimension at most d. Then there\nexists a spanning tree T of crossing number κ(T) = O(mn1−1/d).\nProof Let A0 = X. Consider the following process where initially i = 0. By lemma 21, there exists distinct\nxi, yi ∈Ai such that the number of ranges in R crossing (xi, yi) is at most O(m · |Ai|−1/d log n). Letting\nAi+1 ←Ai \\ xi and repeating, constructs a spanning tree T.\nBy lemma 21, the total number of ranges that cross the edges in T is at most\nn\nX\ni=1\nO(m(n −i)−1/d log(n −i)) ≤O\n\u0012\nm log n\nZ n\n1\nx−1/ddx\n\u0013\n= O(mn1−1/d log n).\nwhich proves the claim.\nWe now extend Lemma 22 to corrupted VC-dimension. For this, we want to linearize the tree constructed\nin Lemma 22, i.e., we make sure the tree is a single line of vertices. This is to reduce the impact of\ncorruptions, so that any single corruption does not affect the number of crossings too much.\nConsider for example where X = [n], R is a multi-set of m empty ranges (corresponding to an m × n\nzero matrix M), and the spanning tree is a star with 1 ∈X at the center. The corruption matrix S now\nadds 1 to each of the empty sets in R (equivalently, turning the ﬁrst column of M to an all-1-column). This\nwould still ﬁt into the deﬁnition of corrupted VC-dimension since each set is corrupted by only one element.\nNow the number of crossings of each edge is O(m) because 1 is in each of the m sets, but none of the other\nelements in X are in any set, and there are n −1 edges each connecting 1 to another element of X. Thus\nthere are O(m) crossings and the tree has O(mn) crossings in total.\nIf, however, the tree had been a single line, then the corruption would affect at most 2 edges, and the\nnumber of crossings in the tree would go up by only O(m).\n12\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nLemma 23 For range space (X, R), |X| = n with spanning tree T, there exists a permutation π ∈Sn such\nthat the crossing number of the corresponding spanning tree T ′ (which is just the line π(1), π(2), ..., π(n))\nis at most twice that of T.\nProof\nEnumerate all the ranges in R by R1, . . . , Rm, where Ri := {c : Mi,c = 1} for i ∈[m].\nThe crossing number of Ri in T counts the number of edges in T such that one endpoint of the edge is\nin Ri and the other endpoint is in [n] \\ Ri. Suppose the spanning tree T on X has a crossing number of κi\nfor just Ri and let κ = Pm\ni=1 κi be the crossing number of T for (X, R).\nPerform a DFS traversal on T, and write the in-order traversal I. We use the permutation π(i) = Ii for\ni ∈[n]. Let T ′ be the corresponding linear tree.\nWe see then that this vertex path can (in the worst case) cross Ri 2κi times, that is because DFS uses\neach edge (x, y) of T twice: once when visiting y (going down one level in the tree) and once when it is\ndone processing y (DFS pops the current level from the stack).\nThus the total number of crossings κ(T ′) for the linear tree T ′ is κ(T ′) ≤Pm\ni=1 2κi ≤2κ(T).\nProof of Theorem 17 Suppose that M ∈{0, 1}m×n has corrupted VC-dimension d. Let L ∈{0, 1}m×n, S ∈\n{−1, 0, 1}m×n with M = L+S where L has VC-dimension d and S has at most O(n1−1/d) non-zero entries\nper row. These matrices exist by deﬁnition of corrupted VC-dimension, i.e., S describes the corruption.\nLet (X, R) be the set system induced by L, i.e., X = [n] and each row of L is the indicator vector of\na set in R with |R| = m. Then (X, R) also has VC-dimension d and by Lemma 22 there exists a spanning\ntree T of (X, R) with crossing number bounded by O(mn1−1/d log n).\nThis also implies the existence of a spanning tree T ′ which is a single line that has crossing number\nO(mn1−1/d log n) by Lemma 23.\nNow consider the range space (X, Rc) for the corrupted matrix M. The spanning tree T ′ has the same\ncrossing number on (X, RC) as on (X, R), except for extra crossings induced by the corruption. We con-\nsider a range r ∈Rc and count the number of new crossings it has incurred due to the corruption: an edge\n(x, y) only has a new crossing for some range r ∈Rc if x or y was corrupted in r. Since each x ∈X\nhas at most 2 incident edges in T ′ (since it is a line) each corruption to r leads to at most 2 new crossings.\nSince each range has at most O(n1−1/d) corruptions by deﬁnition of VC-corruption, the total number of\nnew crossing over all ranges is at most O(mn1−1/d log n).\nThus, T ′ is a spanning tree for (X, Rc) with crossing number bounded by O(mn1−1/d log n). This\nspanning tree corresponds to a ∆-labeled spanning tree for M, as X = [n] are the column indices and the\nnumber of crossings for an edge (x, y) is exactly the number of entries where Mx and My differ.\nSince the minimum spanning tree can only have lower weight, we get an upper bound of O(mn1−1/d log n)\non the weight of the MST.\nWe can now put everything together to prove Theorem 3.\nTheorem 3 (Static Online Matrix-Vector Multiplication). If a matrix M ∈{0, 1}m×n has corrupted VC-\ndimension d, then after an eO(mn)-time preprocessing, there is a data structure D that can compute Mv for\nany v ∈Rn in eO(mn1−1/d + n) time, with high probability.\nProof Given matrix M, preprocess it by constructing a ∆-labeled spanning tree T via Theorem 16. This\ntakes eO(mn) time.\nWhen receiving a vector v ∈Rn, pass v together with T to the algorithm of Lemma 14. It returns the\nproduct Mv in time eO(m + n + weight(T)).\nBy Theorem 17, if M has corrupted VC-dimension bounded by d, then the MST of M has weight\nbounded by O(mn1−1/d). Since T is a O(log n)-approximation of the MST, answering a query takes\neO(mn1−1/d + n) time.\n13\n\n\nANAND BRAND MCCARTY\n3.3. Dynamic Matrices\nAt last, we outline how to handle matrices M that are allowed to update over time, i.e., Theorem 4.\nTheorem 4 (Dynamic Online Matrix-Vector Multiplication).\nGiven a matrix M ∈{0, 1}m×n, there is a\ndata structure D with eO(mn) preprocessing time that supports row and column updates (insertions/deletions)\nto M in eO(n) and eO(m) time respectively. Upon querying the data structure D with a vector v ∈Rn, it out-\nputs Mv in eO(mn1−1/d∗+ n) time, with high probability, where d∗is the largest corrupted VC-dimension\nof M throughout the history of its updates.\nWe sketch how to handle column insertions: Let M be the current matrix. We split it into smaller\nmatrices M(0), M(1), ..., M(log n), where each M(i) contains at most 2i columns of M. When a new column\nis inserted to M, we insert that column to M(0). When any M(i) contains more than 2i columns, then all its\ncolumns are inserted to M(i+1) and M(i) is reset to an empty matrix. Thus, any M(i) receives new columns\nat most once every 2i iterations to M.\nWhen an M(i) receives new columns, we initialize a new matrix-vector multiplication data structure\n(Theorem 3) on it. This takes eO(m2i) time, but amortizes to eO(m) time per insertion to M since it only\nhappens at most once every 2i insertions. This yields an amortized time of Plog n\ni=0 eO(m) = eO(m) time to\nmaintain all M(i) for i = 0, ..., log n.\nTo answer a matrix-vector multiplication query Mv, we split v into vectors v(0), ..., v(log n) such that\nMv = Plog n\ni=0 M(i)v(i). For the time complexity, note that each M(i) is a submatrix of M. Thus, the the\ntime complexity needed to multiply a vector by M(i) is bounded by eO(mn1−1/d).\nFor column deletions, we can ignore them until M(i) is reinitialized: simply set vj = 0 for the columns\nthat should have been deleted. We can handle row deletions updates to M in a similar way, by splitting the\nmatrix again into log m matrices, each containing at most 2j rows. The full proof is given in Appendix A.2.\n4. Conclusion\nIn this work, we study the structural complexity of sequential matrix-vector multiplication. We propose a\ntheoretical framework to study heuristic algorithms that have achieved tremendous practical success. Build-\ning on the VC-dimension literature, we propose the notion of a corrupted VC-dimension d and provide\nstructural characterizations in Fact 1. In theorem 3, we show that if a matrix M ∈{0, 1}m×n has corrupted\nVC-dimension d, then its matrix-vector product Mv can be computed in O(mn1−1/d) time, providing poly-\nnomial speed-ups over existing methods. Moreover, our algorithm maintains updates to M in eO(m) and\neO(n) time. This leads to the ﬁrst O(n2−ǫ)-time algorithms for maintaining high-accuracy estimates for\nLaplacian solvers, effective resistance, and triangle detection in graphs with node insertions and deletions.\nOur results motivate interesting future work: for instance, it would be interesting if alternate generaliza-\ntions of the VC-dimension, such as the Natarajan dimension, can also parameterize the structural complexity\nof matrix-vector multiplication. Moreover, we conjecture that our algorithm can be used to provide combi-\nnatorial accelerations to the context-free grammar parsing and the k-clique detection problem in structured\ninstances. Finally, for the non-Boolean setting where we provide subquadratic runtimes in matrices with\nbounded Pollard pseudodimension, we conjecture that the dependence of the number of thresholds A in the\nruntime can be reduced to be polylogarithmic instead of linear.\n5. Acknowledgements\nThis work was supported by NSF Grant CCF 2338816. We also express our thanks to Ce Jin, Shunhua\nJiang, Elia Gorokhovsky, and Jingtong Sun for insightful discussions.\n14\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nReferences\nAmir Abboud and Søren Dahlgaard.\nPopular conjectures as a barrier for dynamic planar graph al-\ngorithms.\nIEEE 57th Annual Symposium on Foundations of Computer Science, 2016.\nURL\nhttps://arxiv.org/abs/1605.03797.\nAmir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong lower bounds for dy-\nnamic problems. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014,\nPhiladelphia, PA, USA, October 18-21, 2014, pages 434–443. IEEE Computer Society, 2014.\ndoi:\n10.1109/FOCS.2014.53. URL https://doi.org/10.1109/FOCS.2014.53.\nAmir Abboud, Nick Fischer, Zander Kelley, Shachar Lovett, and Raghu Meka. New graph decompositions\nand combinatorial boolean matrix multiplication algorithms. STOC 2024: Proceedings of the 56th Annual\nACM Symposium on Theory of Computing, 2024.\nIttai Abraham, David Durfee, Ioannis Koutis, Sebastian Krinninger, and Richard Peng. On fully dynamic\ngraph sparsiﬁers. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS),\npages 335–344, 2016. doi: 10.1109/FOCS.2016.44.\nVedat Levi Alev, Nima Anari, Lap Chi Lau, and Shayan Oveis Gharan.\nGraph clustering using effec-\ntive resistance. 9th Innovations in Theoretical Computer Science Conference (ITCS 2018), 2017. URL\nhttps://arxiv.org/abs/1711.06530.\nJosh Alman and Virginia Vassilevska Williams.\nOV graphs are (probably) hard instances.\nIn Thomas\nVidick, editor, 11th Innovations in Theoretical Computer Science Conference, ITCS 2020, Jan-\nuary 12-14, 2020, Seattle, Washington, USA, volume 151 of LIPIcs, pages 83:1–83:18. Schloss\nDagstuhl - Leibniz-Zentrum für Informatik, 2020.\ndoi:\n10.4230/LIPICS.ITCS.2020.83.\nURL\nhttps://doi.org/10.4230/LIPIcs.ITCS.2020.83.\nJosh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness for linear algebra on\ngeometric graphs. In Sandy Irani, editor, 61st IEEE Annual Symposium on Foundations of Computer\nScience, FOCS 2020, Durham, NC, USA, November 16-19, 2020, pages 541–552. IEEE, 2020. doi: 10.\n1109/FOCS46700.2020.00057. URL https://doi.org/10.1109/FOCS46700.2020.00057.\nNoga Alon, Shay Moran, and Amir Yehudayoff.\nSign rank versus VC dimension.\nIn Vitaly Feldman,\nAlexander Rakhlin, and Ohad Shamir, editors, 29th Annual Conference on Learning Theory, volume 49\nof Proceedings of Machine Learning Research, pages 47–80, Columbia University, New York, New York,\nUSA, 23–26 Jun 2016. PMLR. URL https://proceedings.mlr.press/v49/alon16.html.\nJoão N. F. Alves, Samir Moustafa, Siegfried Benkner, Alexandre P. Francisco, Wilfried N. Gansterer, and\nLuís M. S. Russo. Accelerating graph neural networks with a novel matrix compression format, 2024.\nURL https://arxiv.org/abs/2409.02208.\nEmile Anand and Guannan Qu.\nEfﬁcient reinforcement learning for global decision making\nin the presence of local agents at scale.\narXiv preprint arXiv:2403.00222,\n2024.\nURL\nhttps://arxiv.org/abs/2403.00222.\nEmile Anand and Chris Umans. Pseudorandomness of the sticky random walk. California Institute of\nTechnology, Thesis Archive, 2023. URL https://arxiv.org/abs/2307.11104.\nEmile\nAnand,\nIshani\nKarmarkar,\nand\nGuannan\nQu.\nMean-ﬁeld\nsampling\nfor\ncoop-\nerative\nmulti-agent\nreinforcement\nlearning.\nArXiv,\nabs/2412.00661,\n2024a.\nURL\nhttps://arxiv.org/abs/2412.00661.\n15\n\n\nANAND BRAND MCCARTY\nEmile Anand, Jan van den Brand, Mehrdad Ghadiri, and Daniel J. Zhang. The Bit Complexity of Dy-\nnamic Algebraic Formulas and Their Determinants. In Karl Bringmann, Martin Grohe, Gabriele Puppis,\nand Ola Svensson, editors, 51st International Colloquium on Automata, Languages, and Programming\n(ICALP 2024), volume 297 of Leibniz International Proceedings in Informatics (LIPIcs), pages 10:1–\n10:20, Dagstuhl, Germany, 2024b. Schloss Dagstuhl – Leibniz-Zentrum für Informatik. ISBN 978-3-\n95977-322-5. doi: 10.4230/LIPIcs.ICALP.2024.10.\nP. Assouad.\nDensité et dimension.\nAnnales de l’Institut Fourier, 33:233–282, 1983.\nURL\nhttps://api.semanticscholar.org/CorpusID:123874242.\nEmo Welzl B. Chazelle. Quasi-optimal range searching in spaces of ﬁnite vc-dimension. Discrete & com-\nputational geometry, 4(5):467–490, 1989. URL http://eudml.org/doc/131092.\nPeter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and\npseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research,\n20(63):1–17, 2019. URL http://jmlr.org/papers/v20/17-612.html.\nJulien Basch, Sanjeev Khanna, and Rajeev Motwani. On diameter veriﬁcation and boolean matrix multipli-\ncation. Technical report, Stanford University, Stanford, CA, USA, 1995.\nAaron Bernstein, Jan van den Brand, Maximilian Probst Gutenberg, Danupon Nanongkai, Thatchaphol\nSaranurak, Aaron Sidford, and He Sun. Fully-dynamic graph sparsiﬁers against an adaptive adversary.\nIn Mikolaj Bojanczyk, Emanuela Merelli, and David P. Woodruff, editors, 49th International Colloquium\non Automata, Languages, and Programming, ICALP 2022, July 4-8, 2022, Paris, France, volume 229\nof LIPIcs, pages 20:1–20:20. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2022. doi: 10.4230/\nLIPICS.ICALP.2022.20. URL https://doi.org/10.4230/LIPIcs.ICALP.2022.20.\nAndreas Björklund and Andrzej Lingas. Fast boolean matrix multiplication for highly clustered data. In\nFrank Dehne, Jörg-Rüdiger Sack, and Roberto Tamassia, editors, Algorithms and Data Structures, pages\n258–263, Berlin, Heidelberg, 2001. Springer Berlin Heidelberg. ISBN 978-3-540-44634-7.\nNicolas Bousquet, Aurélie Lagoutte, Zhentao Li, Aline Parreau, and Stéphan Thomassé.\nIdentifying\ncodes in hereditary classes of graphs and vc-dimension. SIAM Journal on Discrete Mathematics, 29\n(4):2047–2064, January 2015. ISSN 1095-7146. doi: 10.1137/14097879x.\nJan\nvan\nden\nBrand.\nA\ndeterministic\nlinear\nprogram\nsolver\nin\ncurrent\nma-\ntrix\nmultiplication\ntime.\nIn\nSODA,\npages\n259–278.\nSIAM,\n2020.\nURL\nhttps://dl.acm.org/doi/abs/10.5555/3381089.3381105.\nJan van den Brand, Danupon Nanongkai, and Thatchaphol Saranurak. Dynamic matrix inverse: Improved\nalgorithms and matching conditional lower bounds. In FOCS, pages 456–480. IEEE Computer Society,\n2019a. URL https://arxiv.org/pdf/1905.05067.pdf.\nJan van den Brand, Danupon Nanongkai, and Thatchaphol Saranurak. Dynamic matrix inverse: Improved\nalgorithms and matching conditional lower bounds. In 2019 IEEE 60th Annual Symposium on Founda-\ntions of Computer Science (FOCS), pages 456–480. IEEE, 2019b.\nJan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense linear programs in nearly\nlinear time. In Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC\n2020, Chicago, IL, USA, June 22-26, 2020, pages 775–788. ACM, 2020. doi: 10.1145/3357713.3384309.\nURL https://dl.acm.org/doi/abs/10.1145/3357713.3384309.\n16\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nJan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and\nDi Wang. Minimum cost ﬂows, mdps, and ℓ1)-regression in nearly linear time for dense instances. In\nSTOC, pages 859–869. ACM, 2021.\nJan van den Brand, Sebastian Forster, Yasamin Nazari, and Adam Polak. On dynamic graph algorithms with\npredictions. In David P. Woodruff, editor, Proceedings of the 2024 ACM-SIAM Symposium on Discrete\nAlgorithms, SODA 2024, Alexandria, VA, USA, January 7-10, 2024, pages 3534–3557. SIAM, 2024. doi:\n10.1137/1.9781611977912.126. URL https://doi.org/10.1137/1.9781611977912.126.\nDiptarka Chakraborty, Lior Kamma, and Kasper Green Larsen. Tight cell probe bounds for succinct boolean\nmatrix-vector multiplication. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors, Pro-\nceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Ange-\nles, CA, USA, June 25-29, 2018, pages 1297–1306. ACM, 2018. doi: 10.1145/3188745.3188830. URL\nhttps://doi.org/10.1145/3188745.3188830.\nShreyas Chaudhari, Srinivasa Pranav, Emile Anand, and José M. F. Moura. Peer-to-peer learning dynamics\nof wide neural networks. IEEE International Conference on Acoustics, Speech, and Signal Processing\n2025, 2024. URL https://arxiv.org/abs/2409.15267.\nJingbang Chen, Meng He, J. Ian Munro, Richard Peng, Kaiyu Wu, and Daniel J. Zhang. Distance queries\nover dynamic interval graphs.\nComput. Geom., 122:102103, 2024.\ndoi: 10.1016/J.COMGEO.2024.\n102103. URL https://doi.org/10.1016/j.comgeo.2024.102103.\nLi Chen, Gramoz Goranci, Monika Henzinger, Richard Peng, and Thatchaphol Saranurak. Fast dynamic\ncuts, distances and effective resistances via vertex sparsiﬁers. In Sandy Irani, editor, 61st IEEE Annual\nSymposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19,\n2020, pages 1135–1146. IEEE, 2020. doi: 10.1109/FOCS46700.2020.00109.\nMichael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication\ntime. J. ACM, 68(1):3:1–3:39, 2021. Announced at STOC’19.\nDavid Coudert, Mónika Csikós, Guillaume Ducoffe, and Laurent Viennot.\nPractical Computa-\ntion of Graph VC-Dimension.\nIn Leo Liberti, editor, 22nd International Symposium on Ex-\nperimental Algorithms (SEA 2024),\nvolume 301 of Leibniz International Proceedings in In-\nformatics (LIPIcs),\npages 8:1–8:20,\nDagstuhl,\nGermany,\n2024. Schloss Dagstuhl – Leibniz-\nZentrum für Informatik.\nISBN 978-3-95977-325-6.\ndoi:\n10.4230/LIPIcs.SEA.2024.8.\nURL\nhttps://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.SEA.2024.8.\nChristophe Crespelle. Fully dynamic representations of interval graphs. Theor. Comput. Sci., 759:14–49,\n2019. doi: 10.1016/J.TCS.2019.01.007.\nEmilio Cruciani,\nSebastian Forster,\nGramoz Goranci,\nYasamin Nazari,\nand Antonis Skarlatos.\nDynamic\nalgorithms\nfor-center\non\ngraphs.\nIn\nSODA,\npages\n3441–3462,\n2024.\nURL\nhttps://doi.org/10.1137/1.9781611977912.123.\nFrederic Dorn, Fedor V. Fomin, and Dimitrios M. Thilikos. Catalan structures and dynamic programming\nin h-minor-free graphs. Journal of Computer and System Sciences, 78(5):1606–1622, 2012. ISSN 0022-\n0000. doi: https://doi.org/10.1016/j.jcss.2012.02.004. JCSS Special Issue: Cloud Computing 2011.\nDavid Durfee, Yu Gao, Gramoz Goranci, and Richard Peng.\nFully dynamic spectral vertex spar-\nsiﬁers and applications.\nIn Moses Charikar and Edith Cohen,\neditors,\nProceedings of the\n51st Annual ACM SIGACT Symposium on Theory of Computing,\nSTOC 2019, Phoenix,\nAZ,\n17\n\n\nANAND BRAND MCCARTY\nUSA, June 23-26, 2019, pages 914–925. ACM, 2019.\ndoi:\n10.1145/3313276.3316379.\nURL\nhttps://doi.org/10.1145/3313276.3316379.\nDavid Eppstein. Graphs with bounded radial order are small. In Proceedings of the 3rd Annual European\nSymposium on Algorithms (ESA), volume 979 of Lecture Notes in Computer Science, pages 392–404.\nSpringer, 1995. doi: 10.1007/3-540-60313-1_165.\nVissarion Fisikopoulos and Luis Penaranda. Faster geometric algorithms via dynamic determinant compu-\ntation. Computational Geometry, 54:1–16, 2016.\nDimitris Floros, Nikos Pitsianis, and Xiaobai Sun. Algebraic vertex ordering of a sparse graph for adjacency\naccess locality and graph compression, 2024. URL https://arxiv.org/abs/2408.08439.\nGudmund Skovbjerg Frandsen, Johan P. Hansen, and Peter Bro Miltersen.\nLower bounds for dynamic\nalgebraic problems.\nInf. Comput., 171(2):333–349, 2001.\ndoi: 10.1006/INCO.2001.3046.\nURL\nhttps://doi.org/10.1006/inco.2001.3046.\nLeszek Gasieniec and Andrzej Lingas.\nAn improved bound on boolean matrix multiplication for\nhighly clustered data.\nIn Frank K. H. A. Dehne, Jörg-Rüdiger Sack, and Michiel H. M. Smid,\neditors, Algorithms and Data Structures, 8th International Workshop, WADS 2003, Ottawa, On-\ntario, Canada, July 30 - August 1, 2003, Proceedings, volume 2748 of Lecture Notes in Com-\nputer Science, pages 329–339. Springer, 2003.\ndoi:\n10.1007/978-3-540-45078-8\\_29.\nURL\nhttps://doi.org/10.1007/978-3-540-45078-8_29.\nArpita\nGhosh,\nStephen\nBoyd,\nand\nAmin\nSaberi.\nMinimizing\neffective\nresistance\nof\na\ngraph.\nSIAM\nReview,\n50(1):37–66,\n2008.\ndoi:\n10.1137/050645452.\nURL\nhttps://web.stanford.edu/~boyd/papers/pdf/eff_res.pdf.\nI.\nGohberg\nand\nV.\nOlshevsky.\nFast\nalgorithms\nwith\npreprocessing\nfor\nmatrix-\nvector\nmultiplication\nproblems.\nJournal\nof\nComplexity,\n10(4):411–427,\n1994.\nISSN\n0885-064X.\ndoi:\nhttps://doi.org/10.1006/jcom.1994.1021.\nURL\nhttps://www.sciencedirect.com/science/article/pii/S0885064X84710211.\nRaphael Clifford. Allan Gronlund and Kasper Green Larsen.\nNew unconditional hardness re-\nsults for dynamic and online problems.\nIn Proceedings of the 2015 IEEE 56th Annual\nSymposium on Foundations of Computer Science (FOCS), FOCS ’15, page 1089–1107, USA,\n2015. IEEE Computer Society.\nISBN 9781467381918.\ndoi:\n10.1109/FOCS.2015.71.\nURL\nhttps://doi.org/10.1109/FOCS.2015.71.\nSariel Har-Peled. Approximating spanning trees with low crossing number. ArXiv, abs/0907.1131, 2009.\nURL https://api.semanticscholar.org/CorpusID:15811439.\nMonika Henzinger and Stefan Neumann.\nIncremental and Fully Dynamic Subgraph Connectiv-\nity For Emergency Planning.\nIn Piotr Sankowski and Christos Zaroliagis, editors, 24th Annual\nEuropean Symposium on Algorithms (ESA 2016), volume 57 of Leibniz International Proceed-\nings in Informatics (LIPIcs), pages 48:1–48:11, Dagstuhl, Germany, 2016. Schloss Dagstuhl –\nLeibniz-Zentrum für Informatik. ISBN 978-3-95977-015-6. doi: 10.4230/LIPIcs.ESA.2016.48. URL\nhttps://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ESA.2016.48.\nMonika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak. Unifying and\nstrengthening hardness for dynamic problems via the online matrix-vector multiplication conjecture. In\nProceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing, STOC ’15, page\n18\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\n21–30, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450335362. doi:\n10.1145/2746539.2746609. URL https://doi.org/10.1145/2746539.2746609.\nMonika Henzinger, Andrea Lincoln, and Barna Saha.\nThe complexity of average-case dynamic sub-\ngraph counting. In Joseph (Sefﬁ) Naor and Niv Buchbinder, editors, Proceedings of the 2022 ACM-\nSIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA,\nJanuary 9 - 12, 2022, pages 459–498. SIAM, 2022.\ndoi:\n10.1137/1.9781611977073.23.\nURL\nhttps://doi.org/10.1137/1.9781611977073.23.\nMonika Henzinger, Barna Saha, Martin P. Seybold, and Christopher Ye.\nOn the complexity of al-\ngorithms with predictions for dynamic graph problems.\n15th Innovations in Theoretical Computer\nScience Conference, ITCS 2024, 287:62:1–62:25, 2024.\ndoi: 10.4230/LIPICS.ITCS.2024.62.\nURL\nhttps://doi.org/10.4230/LIPIcs.ITCS.2024.62.\nWen-Lian\nHsu\nand\nGeorge\nL.\nNemhauser.\nEasy\nand\nhard\nbottleneck\nloca-\ntion\nproblems.\nDiscrete\nApplied\nMathematics,\n1(3):209–215,\n1979.\nISSN\n0166-218X.\ndoi:\nhttps://doi.org/10.1016/0166-218X(79)90044-1.\nURL\nhttps://www.sciencedirect.com/science/article/pii/0166218X79900441.\nBaihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving SDP faster: A robust\nIPM framework and efﬁcient implementation. In 63rd IEEE Annual Symposium on Foundations of Com-\nputer Science, FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022, pages 233–244. IEEE,\n2022. doi: 10.1109/FOCS54457.2022.00029.\nPiotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimen-\nsionality. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, STOC ’98,\npage 604–613, New York, NY, USA, 1998. Association for Computing Machinery. ISBN 0897919629.\ndoi: 10.1145/276698.276876. URL https://doi.org/10.1145/276698.276876.\nHaotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point\nmethod for semideﬁnite programming. In FOCS, pages 910–918. IEEE, 2020a.\nHaotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane method\nfor convex optimization, convex-concave games, and its applications. In STOC, pages 944–953. ACM,\n2020b.\nShunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving general lps.\nIn STOC, pages 823–832. ACM, 2021.\nShunhua Jiang, Bento Natura, and Omri Weinstein. A faster interior-point method for sum-of-squares opti-\nmization. In ICALP, volume 229 of LIPIcs, pages 79:1–79:20. Schloss Dagstuhl - Leibniz-Zentrum für\nInformatik, 2022.\nShunhua Jiang, Binghui Peng, and Omri Weinstein. The complexity of dynamic least-squares regression. In\n64th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2023, Santa Cruz, CA, USA,\nNovember 6-9, 2023, pages 1605–1627. IEEE, 2023. doi: 10.1109/FOCS57990.2023.00097.\nCe Jin and Yinzhan Xu. Tight dynamic problem lower bounds from generalized bmm and omv. In Pro-\nceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2022, page\n1515–1528, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392648.\ndoi: 10.1145/3519935.3520036. URL https://doi.org/10.1145/3519935.3520036.\n19\n\n\nANAND BRAND MCCARTY\nAdam\nKarczmarz\nand\nDa\nWei\nZheng.\nSubquadratic\nalgorithms\nin\nminor-free\ndigraphs:\n(weighted)\ndistance\noracles,\ndecremental\nreachability,\nand\nmore.\narXiV,\n2024.\nURL\nhttps://arxiv.org/abs/2410.12003.\nN. Karmarkar. A new polynomial-time algorithm for linear programming. Combinatorica, 4(4):373–396,\n1984.\nToni Maria Klein, Christine Blome, C. Elise Kleyn, Curdin Conrad, Paul G. Sator, Mona Ståhle,\nKilian Eyerich, Marc Alexander Radtke, Christine Bundy, Myriam Cordey, Christopher E. M.\nGrifﬁths, and Matthias Augustin.\nReal-world experience of patient-relevant beneﬁts and treat-\nment satisfaction with apremilast in patients with psoriasis:\nAn analysis of the appreciate study.\nDermatology and Therapy,\n12(1):81–95,\n2022.\ndoi:\n10.1007/s13555-021-00628-3.\nURL\nhttps://doi.org/10.1007/s13555-021-00628-3.\nTuukka Korhonen, Wojciech Nadara, Michal Pilipczuk, and Marek Sokolowski. Fully dynamic approxi-\nmation schemes on planar and apex-minor-free graphs. In David P. Woodruff, editor, Proceedings of the\n2024 ACM-SIAM Symposium on Discrete Algorithms, SODA 2024, Alexandria, VA, USA, January 7-10,\n2024, pages 296–313. SIAM, 2024. doi: 10.1137/1.9781611977912.12.\nRasmus Kyng and Sushant Sachdeva. Approximate gaussian elimination for laplacians - fast, sparse, and\nsimple. In FOCS, pages 573–582. IEEE Computer Society, 2016.\nKasper Green Larsen and Ryan Williams. Faster online matrix-vector multiplication. In Proceedings of\nthe Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’17, page 2182–2189,\nUSA, 2017. Society for Industrial and Applied Mathematics.\nYin Tat Lee and Aaron Sidford. Efﬁcient inverse maintenance and faster algorithms for linear programming.\nIn FOCS, pages 230–249. IEEE Computer Society, 2015.\nYin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its implications\nfor combinatorial and convex optimization. In FOCS, pages 1049–1065. IEEE Computer Society, 2015.\nYin Tat Lee, Zhao Song, and Qiuyi Zhang.\nSolving empirical risk minimization in the current matrix\nmultiplication time. In COLT, volume 99 of Proceedings of Machine Learning Research, pages 2140–\n2157. PMLR, 2019.\nEdo\nLiberty\nand\nSteven\nW.\nZucker.\nThe\nmailman\nalgorithm:\nA\nnote\non\nma-\ntrix–vector\nmultiplication.\nInformation\nProcessing\nLetters,\n109(3):179–182,\n2006.\nISSN\n0020-0190.\ndoi:\nhttps://doi.org/10.1016/j.ipl.2008.09.028.\nURL\nhttps://www.sciencedirect.com/science/article/pii/S0020019008002949.\nYiheng Lin, James A Preiss, Emile Timothy Anand, Yingying Li, Yisong Yue, and Adam Wier-\nman.\nOnline adaptive policy selection in time-varying systems:\nNo-regret via contractive pertur-\nbations.\nIn Thirty-seventh Conference on Neural Information Processing Systems, 2023a.\nURL\nhttps://openreview.net/forum?id=hDajsofjRM.\nYiheng\nLin,\nJames\nA\nPreiss,\nEmile\nTimothy\nAnand,\nYingying\nLi,\nYisong\nYue,\nand\nAdam\nWierman.\nLearning-augmented\ncontrol\nvia\nonline\nadaptive\npolicy\nselec-\ntion:\nNo\nregret\nvia\ncontractive\nperturbations.\nIn\nACM\nSIGMETRICS,\nWorkshop\non\nLearning-augmented\nAlgorithms:\nTheory\nand\nApplications\n2023,\n2023b.\nURL\nhttps://learning-augmented-algorithms.github.io/papers/sigmetrics23-lata-posters-\n20\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nYiheng Lin, James A. Preiss, Fengze Xie, Emile Anand, Soon-Jo Chung, Yisong Yue, and Adam Wier-\nman.\nOnline policy optimization in unknown nonlinear systems.\nIn Shipra Agrawal and Aaron\nRoth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Pro-\nceedings of Machine Learning Research, pages 3475–3522. PMLR, 30 Jun–03 Jul 2024.\nURL\nhttps://proceedings.mlr.press/v247/lin24a.html.\nYang P. Liu. On approximate fully-dynamic matching and online matrix-vector multiplication, 2024. URL\nhttps://arxiv.org/abs/2403.02582.\nAlane Marie de Lima, Murilo da Silva, and André Vignatti. A range space with constant vc dimension for\nall-pairs shortest paths in graphs. Journal of Graph Algorithms and Applications, 27:603–619, 08 2023.\ndoi: 10.7155/jgaa.00636.\nJiˇrí Matoušek. Spanning trees with low crossing number. RAIRO - Theoretical Informatics and Applications\n- Informatique Théorique et Applications, 25(2):103–123, 1991.\nJiˇrí Matoušek. Lectures on Discrete Geometry, volume 212 of Graduate Texts in Mathematics. Springer,\n2002. ISBN 978-0-387-95373-0. doi: 10.1007/978-1-4613-0039-7.\nElchanan Mossel and Christopher Umans. On the complexity of approximating the vc dimension. Journal\nof Computer and System Sciences, 65(4):660–671, 2002. ISSN 0022-0000. doi: https://doi.org/10.1016/\nS0022-0000(02)00022-3. Special Issue on Complexity 2001.\nTheodore S. Motzkin.\nEvaluation of polynomials and evaluation of rational functions.\nBulletin of the\nAmerican Mathematical Society, 57(2):163–169, 1951.\nRiley Murray, Venkat Chandrasekaran, and Adam Wierman.\nNewton polytopes and relative entropy\noptimization.\nFoundations of Computational Mathematics, 21(6):1703–1737, 2021.\ndoi: 10.1007/\ns10208-021-09497-w.\nTung Nguyen, Alex Scott, and Paul Seymour. Induced subgraph density. vi. bounded vc-dimension. arXiV,\n2024. URL https://arxiv.org/abs/2312.15572.\nVadim Olshevsky and Amin Shokrollahi.\nMatrix-vector product for conﬂuent cauchy-like matrices\nwith application to conﬂuent rational interpolation.\nIn Proceedings of the Thirty-Second Annual\nACM Symposium on Theory of Computing, STOC ’00, page 573–581, New York, NY, USA, 2000.\nAssociation for Computing Machinery.\nISBN 1581131844.\ndoi: 10.1145/335305.335380.\nURL\nhttps://doi.org/10.1145/335305.335380.\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.\nThe pagerank citation rank-\ning:\nBringing order to the web.\nTechnical Report 1999-66, Stanford InfoLab, 1999.\nURL\nhttps://ilpubs.stanford.edu:8090/422/.\nVictor Y. Pan and Elias P. Tsigaridas. Nearly optimal computations with structured matrices. In Proceedings\nof the 2014 Symposium on Symbolic-Numeric Computation, SNC ’14, page 21–30, New York, NY, USA,\n2014. Association for Computing Machinery. ISBN 9781450329637. doi: 10.1145/2631948.2631954.\nURL https://doi.org/10.1145/2631948.2631954.\nJames Renegar. Condition numbers, the barrier method, and the conjugate-gradient method. SIAM Journal\non Optimization, 6(4):879–912, 1996.\nDavid E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams.\nLearning representations by back-\npropagating errors. Nature, 323(6088):533–536, 1986. doi: 10.1038/323533a0.\n21\n\n\nANAND BRAND MCCARTY\nShai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms.\nCambridge University Press, Cambridge, UK, 2014. ISBN 978-1-107-05713-5.\nHans Henrik Brandenborg Sørensen. High-performance matrix-vector multiplication on the gpu. In Michael\nAlexander, Pasqua D’Ambra, Adam Belloum, George Bosilca, Mario Cannataro, Marco Danelutto, Be-\nniamino Di Martino, Michael Gerndt, Emmanuel Jeannot, Raymond Namyst, Jean Roman, Stephen L.\nScott, Jesper Larsson Traff, Geoffroy Vallée, and Josef Weidendorfer, editors, Euro-Par 2011: Paral-\nlel Processing Workshops, pages 377–386, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. ISBN\n978-3-642-29737-3.\nDaniel A. Spielman and Shang-Hua Teng.\nNearly-linear time algorithms for graph partitioning, graph\nsparsiﬁcation, and solving linear systems. In STOC’04: Proceedings of the 36th Annual ACM Symposium\non the Theory of Computing, pages 81–90. ACM, 2004.\nAli Tizghadam and Alberto Leon-Garcia. On robust trafﬁc engineering in transport networks. In IEEE\nGLOBECOM 2008 - 2008 IEEE Global Telecommunications Conference, pages 1–6, 2008. doi: 10.1109/\nGLOCOM.2008.ECP.456.\nPravin M. Vaidya. Speeding-up linear programming using fast matrix multiplication (extended abstract). In\nFOCS, pages 332–337. IEEE Computer Society, 1989.\nJan van den Brand, Sebastian Forster, and Yasamin Nazari. Fast deterministic fully dynamic distance ap-\nproximation. In 63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2022, Denver,\nCO, USA, October 31 - November 3, 2022, pages 1011–1022. IEEE, 2022. doi: 10.1109/FOCS54457.\n2022.00099. URL https://doi.org/10.1109/FOCS54457.2022.00099.\nSantosh S Vempala, Ruosong Wang, and David P Woodruff. The communication complexity of optimiza-\ntion. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages\n1733–1752. SIAM, 2020.\nE. Welzl. Partition trees for triangle counting and other range searching problems. In Proceedings of the\nFourth Annual Symposium on Computational Geometry, SCG ’88, page 23–33, New York, NY, USA,\n1988. Association for Computing Machinery. ISBN 0897912705. doi: 10.1145/73393.73397.\nEmo Welzl. On spanning trees with low crossing numbers. Lecture Notes in Computer Science, 1992.\nRyan Williams. Matrix-vector multiplication in sub-quadratic time: (some preprocessing required). In\nProceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’07, page\n995–1001, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245.\nVirginia Vassilevska Williams and Ryan Williams. Subcubic equivalences between path, matrix and triangle\nproblems. In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October\n23-26, 2010, Las Vegas, Nevada, USA, pages 645–654. IEEE Computer Society, 2010. doi: 10.1109/\nFOCS.2010.67. URL https://doi.org/10.1109/FOCS.2010.67.\nVirginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multipli-\ncation: from alpha to omega. In SODA, pages 3792–3835. SIAM, 2024.\nHakan Yildiz and Subhash Suri. On klee’s measure problem for grounded boxes. In Proceedings of the\nTwenty-Eighth Annual Symposium on Computational Geometry, SoCG ’12, page 111–120, New York,\nNY, USA, 2012. Association for Computing Machinery. ISBN 9781450312998. doi: 10.1145/2261250.\n2261267. URL https://doi.org/10.1145/2261250.2261267.\n22\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nAppendix A. Online Matrix Vector Data Structure\nIn this section we prove all the intermediate lemmas required for our ﬁrst two main results on subquadratic\nonline matrix-vector multiplication. The proof that combined the technical lemmas was already given in\nSection 3.\nTheorem 3 (Static Online Matrix-Vector Multiplication). If a matrix M ∈{0, 1}m×n has corrupted VC-\ndimension d, then after an eO(mn)-time preprocessing, there is a data structure D that can compute Mv for\nany v ∈Rn in eO(mn1−1/d + n) time, with high probability.\nTheorem 4 (Dynamic Online Matrix-Vector Multiplication).\nGiven a matrix M ∈{0, 1}m×n, there is a\ndata structure D with eO(mn) preprocessing time that supports row and column updates (insertions/deletions)\nto M in eO(n) and eO(m) time respectively. Upon querying the data structure D with a vector v ∈Rn, it out-\nputs Mv in eO(mn1−1/d∗+ n) time, with high probability, where d∗is the largest corrupted VC-dimension\nof M throughout the history of its updates.\nA.1. Static Online Matrix Vector Multiplication Data Structure\nDeﬁnition 11 (∆-labeled spanning tree of matrix M) A ∆-labeled tree is a tree T = (V, E) with some\nexplicit root x, where each edge e ∈E is directed away from x and labeled by a vector ∆e ∈Rm. We say\nT is a ∆-labeled spanning tree for matrix M ∈Rm×n if Mx = 0 for root x, and for all edges (i, j) ∈E\nwe have ∆i,j := Mj −Mi.\nDeﬁnition 12 (Weight of a ∆-labeled spanning tree) For any ∆-labeled spanning tree of M denoted by T\nwith directed edge-set E, deﬁne weight(T) as the total number of non-zero entries given by P\ne∈T nnz(∆e).\nLemma 13 (Gasieniec and Lingas (2003); Alves et al. (2024)) Given a ∆-labeled spanning tree T of M⊤∈\n{0, 1}n×m and a vector v ∈Rn, there is an algorithm that computes Mv in O(weight(T) + n + m) time.\nProof Consider Algorithm 1 which takes as input M ∈{0, 1}m×n, a ∆-labeled spanning tree of M⊤\n(denoted by T), and a vector v ∈Rn.\nWe inductively prove that Φ = Mv over the number of visited vertices. This is true when only one\nvertex is visited, as that is the root r and by deﬁnition Φr = (Mv)r.\nFor the inductive step, assume DFS visits the (k + 1)st vertex x. Let y be the parent of the vertex, then\nthe algorithm assigns\nΦx = Φy + ∆⊤\ny,xv\n= (Mv)y + (Mx −My)v\n= (Mv)u.\nHere Φy is already computed since it was among the ﬁrst k visited vertices.\nRuntime analysis. Let δi,j be the number of non-zero entries in ∆i,j for all (i, j) ∈E(T). Initializing\nthe vector Φ and populating its ﬁrst element Φr takes O(n + m) time. The cost of populating the remaining\nentries is proportional to the number of non-zero entries in each ∆x,y, since we can store each ∆via a\nsparser representation of only its non-zero entries and use this to compute ∆⊤\ny,xv in time O(δy,x). Thus,\nthe total time complexity is O(P\n(x,y)∈E(T) δx,y) = O(weight(T)). Hence, the runtime of algorithm 1 is\nO(weight(T) + n + m), proving the lemma.\n23\n\n\nANAND BRAND MCCARTY\nAlgorithm 1 Online Matrix-vector multiplication (given a ∆-labeled tree of M⊤)\nRequire: Input matrix M ∈{0, 1}m×n, and a ∆-labeled tree of M⊤denoted by T and rooted at r.\nRequire: Input vector v ∈Rn.\nInitialize: Φ = 0m\nΦr = M⊤\nr v\nRun DFS on T from root r. Whenever DFS visits a vertex x ∈V via some edge (y, x), then\nset Φx = Φy + ∆x,yv\nReturn Φ\nDeﬁnition 24 (Path-∆matrix N(T)) Given a ∆-labeled subtree T = (V, E) with root r, deﬁne the path-∆\nmatrix N(T) ∈Rm×V such that for u ∈V ,\nN(T)\nu\n:=\nX\ne∈P (r→u)\n∆e.\nHere, P(r →u) denotes the edges of the unique (since T is acyclic) directed path from r to u.\nLemma 25 For ∆-labeled tree T = (V, E), and a vector v ∈RV , let σ ∈RE with σ(y,c) = P\nz∈V (Tc) vz,\nwhere Tc is the subtree rooted at c ∈V . Let ∆∈Rm×E be the matrix with columns being the labels of tree\nT. Then ∆σ = N(T)v.\nProof By the deﬁnition of N(T), we have\nN(T)v =\nX\ny∈V\nN(T)\ny\n· vy\n=\nX\ny∈V\nX\ne∈P (r→y)\n∆e · vy\nNow consider how often any one edge e shows up in this sum: an edge e = (x, z) is in P(r →y) if and only\nif z is an ancestor of y (or y = z) (or, in other words, if y is a descendant of z (or y = z)). This condition\ncan be written as y ∈Tz. Thus we write the sum as\nX\ny∈V\nX\ne∈P (r→y)\n∆e · vy =\nX\n(x,z)∈E\nX\ny∈Tz\n∆x,z · vy\n=\nX\n(x,z)∈E\n∆x,z\nX\ny∈Tz\nvy\n=\nX\n(x,y)∈E\n∆x,yσy\n= ∆σ,\nwhich proves the claim.\nLemma 26 Let T be a ∆-labeled spanning tree of M where the root r of T has Mr = 0, then N(T) = M.\nProof We prove this by induction over the width of M.\nBase Case.\nFor width 1, T is just the root r, so N(T) = 0 = M.\n24\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nInductive Step.\nSuppose the statement holds for matrices of width < n and we now have a matrix\nM of width n. Let L ⊂V be the leaves of T. Let T ′ be the tree T with these leafs removed, and let\nM′ ∈{0, 1}m×(V \\L) be matrix M with all columns corresponding to L removed. Then for any y ∈V \\ L\nMy = M′\ny\n(1)\n= N(T ′)\ny\n(2)\n= N(T)\ny\nwhere equality (1) uses the induction hypothesis and equality (2) uses the fact that any path in the leaf-\nremoved tree T ′ also exists in T.\nFor any y ∈L let z be its ancestor in T. Therefore, z /∈L. Then, we have\nN(T)\ny\n= N(T)\nz\n+ ∆z,y\n(1)\n= N(T ′)\nz\n+ ∆z,y\n(2)\n= M′\nz + ∆z,y\n(3)\n= Mz + (My −Mz) = My,\nwhere equation 1 uses the fact that the path from root r to y is a path from root r to z with one extra edge\n(z, y). Equation 2 uses the fact the path from r to z in T also exists in T ′, and ﬁnally equation 3 follows by\nthe induction hypothesis. Together, they prove the lemma.\nLemma 14\nGiven a ∆-labeled spanning tree T of M ∈{0, 1}m×n and a vector v ∈Rn, there is an\nalgorithm that can compute Mv in O(weight(T) + n + m) time.\nProof\nAlgorithm:\nWe compute N(T)v via Lemma 25 which requires us to compute the vector σ ∈RE with\nσ(y,c) = P\nz∈V (Tc) vz for all (y, c) ∈E where Tc is the subtree of T rooted at c. This can be done in O(n)\ntime via a simple “dynamic programming on trees” algorithm: For any c ∈V , let C be its children. Then\nσ(y,c) =\nX\nz∈V (Tc)\nvz\n= vc +\nX\nx∈C\nX\nz∈V (Tx)\nvz\n= vc +\nX\nx∈C\nσ(c,x).\nSo we can compute σ in O(n) by starting at the leaf-edges, then propagating the sums up towards the root.\nAt last, we multiply ∆σ in O(weight(T)) time as that is the number of non-zeros in ∆(which is the matrix\nconsisting of the tree labels as column-vectors).\nCorrectness.\nWithout loss of generality, assume that for root r of tree T we have Mr = 0. Otherwise,\nappend a 0-column to M, and add edge (n + 1, r) to T with label ∆n+1,r = Mr −Mn+1 = Mr, and make\nn + 1 the new root of T. This increases the tree weight by at most m, resulting in an additive O(m) in the\ntime complexity.\nAppend another entry to v so that the dimensions match and Mv is well-deﬁned. By lemma 26, we thus\nhave N(T)v = Mv.\n25\n\n\nANAND BRAND MCCARTY\nA.2. Dynamic OMv Data Structure\nIn this section we give a black-box reduction from dynamic OMv to static OMv. In the static version, we\nare given a matrix M to preprocess. This matrix stays ﬁxed, while we receive an online sequence of vectors\nand must repeatedly compute Mv. In the dynamic version, we can inform the data structure about changes\nto M. In particular, we are allowed to add/remove/change rows and columns of the matrix.\nUsing the reduction from dynamic to static, we obtain Theorem 4.\nTheorem 4 (Dynamic Online Matrix-Vector Multiplication).\nGiven a matrix M ∈{0, 1}m×n, there is a\ndata structure D with eO(mn) preprocessing time that supports row and column updates (insertions/deletions)\nto M in eO(n) and eO(m) time respectively. Upon querying the data structure D with a vector v ∈Rn, it out-\nputs Mv in eO(mn1−1/d∗+ n) time, with high probability, where d∗is the largest corrupted VC-dimension\nof M throughout the history of its updates.\nThe idea of this reduction is to split the columns of M into log n matrices M0, ..., Mlog n where Mi\ncontains at most 2i columns. Each Mi is used as input to a static OMv data structure which is reinitialized\nwhenever Mi changes which occurs at most once every 2i updates to M. This allows to amortize the\nreinitialization cost over multiple updates.\nLemma 27 Assume there is a static OMv data structure for matrices of size m×n with preprocessing time\nP(m, n) and query time Q(m, n). Then there exists a dynamic OMv data structure supporting row and\ncolumn insertions and deletions. The preprocessing time is eO(P(m, n)), the amortized row update time is\nO\n\n\nlog m\nX\nj=0\nP(2j, n)\n2j\n\n\nand for column updates\nO\n log n\nX\ni=0\nP(m, 2i)\n2j\n!\nand query time is eO(Q(m, n)). This reduction runs several static OMv data structures, each receiving a\nsubmatrix of the dynamic input matrix.\nWe start by proving column updates only, then extend the reduction to support row updates as well.\nLemma 28\nAssume there is a static OMv data structure for matrices of size m × n with preprocessing\ntime P(m, n) and query time Q(m, n). Then there exists a dynamic OMv data structure supporting column\ninsertions and deletions. The preprocessing time is eO(P(m, n)), the query time is eO(Q(m, n)), and the\namortized update time is\nO\n log m\nX\ni=0\nP(m, 2i)\n2i\n!\nThis reduction runs several static OMv data structures, each receiving a submatrix of the dynamic input\nmatrix.\nProof Let M be the input matrix that changes over time.\nInitialization.\nWe run log n instances of the static OMv data structure, where the ith instance is guaranteed\nto run on a matrix with at most 2i columns. Let M(0), ..., M(log n) be the respective input matrices to these\ninstances. Initially, only the last (i = log n) data structure receives the initial matrix M as input, and all\nother instances of the data structure initialize on an empty matrix. Hence initialization takes O(P(m, n))\ntime.\n26\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nInsertions.\nWhenever there is a column insertion to M(i) we reinitialize the ith static OMv data structure\non that new matrix. When M(i) has more than 2i columns, then all columns are removed (i.e., we set\nM(i) = 0) and inserted to matrix M(i+1). All column insertions to M are passed to M(0). Observe that any\nM(i) is updated every 2i updates to M, hence the amortized update time is\nO\n log n\nX\ni=0\nP(m, 2i)/2i\n!\n.\nQueries.\nWe can answer queries by observing that\nM = [M(log m)|...|M(1)|M(0)].\nSo, for a query vector v we split it into corresponding pieces v(0), ...v(log m) and query each M(i)v(i). The\ntime for this is\nO(Q(m, n) log n)\nbecause each M(i) is a submatrix of M and we have log n such matrices.\nDeletions.\nWhen deleting a column of M, we do not delete the column from the respective M(i). Instead,\nall future query vectors v will have an extra 0 entry for the deleted column.\nWhen all columns from M(i) are appended to M(i+1) because of some insertions, then the deleted\ncolumns are not appended to M(i+1). Hence whenever a static OMv data structure is initialized, it is a\nsubmatrix of the input matrix M.\nWhen there have been 2i/2 postponed deletions to any M(i), then the columns are actually deleted and\nthe respective static OMv data structure is reinitialized on the new M(i). The remaining columns in M(i)\nall exist in the current dynamic input M, so again, the static OMv data structure receives a submatrix of M\nas input. For any one i, this takes amortized O(P(n, 2i)/2i) time which is subsumed by the complexity of\nhandling insertions.\nObserve that internally, our matrices M(i) are always at most twice their intended size because of the\nyet to be removed columns. Thus the query complexity increases by at most a constant factor.\nProof of Lemma 27 (Row & Column Updates)\nHere, we extend Lemma 28 to support row updates as\nwell. The reduction follows the same idea as Lemma 28.\nLet N be the dynamic input matrix. We run log n copies of the column update data structure Lemma 28\non matrices N(0), ..., N(log n), with the ith matrix having at most 2i rows. When inserting new rows to N,\nthey are passed to N(0). When N(i) has more than 2i rows, all rows are appended to N(i+1). When rows\nare appended to any N(i), the respective data structure is reinitialized. To handle deletions, we remove the\nrespective row from the output until 2i/2 rows have been deleted from N(i), then the rows are removed from\nN(i) and the respective data structure is reinitialized.\nThe amortized row update time thus becomes\nO\n\n\nlog n\nX\nj=0\nTime to initialize Lemma 28 on 2j × n matrix/2j\n\n= O\n\n\nlog m\nX\nj=0\nP(2j, n)/2j\n\n.\nFor column updates, each of the N(i) receives a new column, so each of the log m column update data\nstructures is updated which takes:\nO\n\n\nlog m\nX\nj=0\nlog n\nX\ni=0\nP(2j, 2i)/2i\n\n= O\n log n\nX\ni=0\nP(m, 2i)/2i\n!\n27\n\n\nANAND BRAND MCCARTY\nbecause geometric sums are bounded by their largest term in O-notation.\nFinally, the query time is\neO\n\n\nlog m\nX\nj=0\nQ(2j, n)\n\n= eO(Q(m, n)),\nwhich proves the lemma.\nProof of Theorem 4 (Dynamic OMv for corrupted VC-dimension d)\nThe initialization cost of static\nOMv is P(m, n) = eO(mn), and the query complexity is Q(m, n) = eO(mn1−1/d) by Theorem 3. This\ncomplexity also holds for submatrices, i.e., Q(a, b) = eO(mn2−1/d + n) for any a × b sized submatrix.\nThus we have update time for column updates:\nO\n log n\nX\ni=0\nP(m, 2i)/2i\n!\n= eO\n log n\nX\ni=0\nm2i/2i\n!\n= eO(m)\nand for row updates we have\nO\n\n\nlog m\nX\nj=0\nP(2j, n)/2j\n\n= eO\n\n\nlog m\nX\nj=0\n2jn/2j\n\n= eO(n)\nand for query time we have\neO(Q(m, n)) ≤eO(mn1−1/d + n),\nwhich proves the theorem.\nA.3. Extension to Structured Non-Boolean Matrices\nIn this section, we prove the following lemma.\nLemma 29 Suppose that the Pollard pseudodimension of the matrix M ∈Rm×n is d, and that the number\nof thresholds is A. Then, after an eO(Amn) time preprocessing, there is a data structure that, upon receiving\na vector v ∈Rn, can return Mv in time O(Amn1−1/d + An).\nRecall the deﬁnition of the Pollard pseudodimension:\nDeﬁnition 30 (Pollard Pseudodimension) For a family of functions F mapping V to {0, . . . , ℓ−1}, the\nPollard pseudodimension of F, denoted by Pdim(F), is the largest integer d such that there exists x1, . . . , xd ∈\nV and thresholds y1, . . . , yd ∈[ℓ] such that for any d-length bit vector b = (b1, . . . , bd) ∈{0, 1}d, there\nexists an f ∈F such that ∀i, f(xi) ≤yi ⇐⇒bi = 1.\nAs an example, consider an integer matrix and split each row into the different thresholds.\n\u0002\n1\n2\n4\n2\n\u0003\n→\n\n\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n1\n1\n1\n1\n1\n\n\nThen, the right matrix has VC dimension d, if and only if the left matrix has Pollard pseudodimension d.\n28\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nDeﬁnition 31 (Multiball vectors) Let v ∈V be a vertex of the graph G. Let r be a real number and ∆be\na set of distances −∞= δ0 < δ1 < · · · < δℓ−1 < δℓ= ∞. Deﬁne the multiball vectors as follows:\nMB(v, r, ∆) = (yu)u∈V where yu ∈[ℓ] is the smallest integer such that u ∈B(v, r + δyu).\nLet MBG,∆= {MB(v, r, ∆)|v ∈V, r ∈R} be the set of all multiball vectors in G with shifts ∆.\nTheorem 32 (Karczmarz and Zheng (2024)) For a Kh-minor-free graph G, for any set ∆of ℓdistances,\nthe set of multiball vectors MBG,∆has pseudodimension at most h −1.\nLemma 33 (Generalized Sauer-Shelah lemma) Let F be a family of functions mapping a ground set with\nn elements to [ℓ]. If the pseudodimension of F is at most d, then |F| ≤O(ndℓd).\nLemma 34 Let F be a family of functions mapping a ground set with n elements to [ℓ]. Let Fs = F ∩[ℓ]S\nbe a restriction of F to S. Then, Pdim(FS) ≤Pdim(F).\nCorollary 35 Let F be a family of functions mapping a ground set with n elements to [ℓ], and let S be any\nsubset of the ground set of F. Suppose Pdim(F) = d. Then, |FS| = O(|S|dℓd).\nDeﬁnition 36 For a real matrix M, deﬁne the pseudo-dimension of M, denoted by Pdim(M) by thinking of\nthe rows of M as functions and taking the pseudo-dimension of the resulting class of functions. Speciﬁcally,\nif M is an m × n matrix, for each i ∈{1, . . . , m}, deﬁne fM,i : {1, . . . , n} →R by fM,i(j) = Mi,j and\nlet Pdim(M) = Pdim({fM,i : i ∈{1, . . . , m}}).\nLemma 37 Given any matrix M ∈Zn×n, construct the Boolean matrix N ∈{0, 1}n×n such that Ni,j =\n1{Mi,j ≥0} for i, j ∈[n]. Then, VC(N) ≤Pdim(M).\nProof Given a matrix M, we have threshold functions fM,i such that fM,i(j) = Mi,j and can use this this\nto deﬁne a set family\nF = {fM,i : {1, . . . , n} →R|i = 1, . . . , m},\nand Pdim(M) = Pdim(F).\nBy deﬁnition, Pdim(M) is the largest integer d such that ∃x1, . . . , xd ∈{0, . . . , ℓ−1} and thresholds\ny1, . . . , yd ∈[ℓ] such that for any vector b1, . . . , bd ∈{−1, 1}d, there is an f ∈F such that ∀i, sign(f(xi)−\nyi) = bi.\nDeﬁne the following 0/1 proﬁle of matrix M. For each row i of M, deﬁne the binary function\ngM.i : {1, . . . , n} →{0, 1}\nby\ngM,i(j) =\n1{fM,i(j) ≥0} :=\n1{Mi,j ≥0},\nand use this to deﬁne the set family\nG = {gM,i : i = 1, . . . , m}.\nWe want to show that if Pdim(M) is low (Pdim(M) = d), then the VC-dimension of G is at most d. So,\nby way of contradiction, suppose Pdim(F) = d and VC(G) ≥d + 1.\nThen, there must exist S ⊂{1, . . . , n} with |S| = d+1 that is shattered by G. So, for every 0/1 labeling\nof c : S →{0, 1}, there is some row i for which gM,i(j) = c(j), ∀j ∈S.\n29\n\n\nANAND BRAND MCCARTY\nAlso, ∃x1, . . . , xd ∈{0, . . . , ℓ−1} and thresholds y1, . . . , yd ∈[ℓ] such that for any vector b1, . . . , bd ∈\n{−1, 1}d, there is an f ∈F such that ∀i ∈[d], sign(f(xi) −yi) = bi.\nAlso, note that the condition\ngM,i(j) =\n1{fM,i(j) ≥0} = c(j)\nis equivalent to\nsign(fM,i(j) −0) = b∗\nj, for b∗\nj =\n(\n+1,\nc(j) = 1,\n−1,\nc(j) = 0\nbut this just means that S is pseudo-shattered by the class F, where the thresholds y1, . . . , yd = 0 for all\nj ∈S. So, this implies Pdim(M) = d + 1, which is a contradiction. So, VC(G) ≤Pdim(F).\nLemma 38 Suppose that the Pollard pseudodimension of the matrix M ∈Rm×n is d, and that the number\nof thresholds is A. Then, after an eO(Amn) time preprocessing, there is a data structure that, upon receiving\na vector v ∈Rn, can return Mv in time O(Amn1−1/d + An).\nProof Given the matrix M ∈Rn×n with A thresholds τ1, τ2, . . . , τA ∈RA, construct the Boolean matrix\nN ∈{0, 1}Am×n as follows: for row i ∈[n] of M, deﬁne the block B(i) := B(Mi) ∈{0, 1}A×n by\nB(Mi) =\n\n\n1{Mi,1≥τ1}\n1{Mi,2≥τ1}\n. . .\n1{Mi,n−1≥τ1}\n1{Mi,n≥τ1}\n1{Mi,1≥τ2}\n1{Mi,2≥τ2}\n. . .\n1{Mi,n−1≥τ2}\n1{Mi,n≥τ2}\n...\n1{Mi,1≥τA}\n1{Mi,2≥τA}\n. . .\n1{Mi,n−1≥τA}\n1{Mi,n≥τA}\n\n\nThen, matrix N ∈{0, 1}Am×n is given by\nN =\n\n\nB(1)\nB(2)\n...\nB(m)\n\n\nBy lemma 37, the VC-dimension of N is at most the Pollard pseudodimension of M, given by d. Con-\nstructing N takes O(mnA) time. Then, using theorem 3, we can preprocess N in eO(Amn) time to support\nqueries that return Nv for any vector v ∈Rn in time eO(Amn1−1/d + n). To convert Nv ∈RAm to\nMv ∈Rm, iterate over each of the m sets of A entries of Nv. For i ∈[m], denote the i’th set of A entries\nby x(1)\ni , x(2)\ni , . . . , x(A)\ni\n, where x(j)\ni\n:= (Mv)A(i−1)+j. Then, deﬁning yi as the linear combination of the\nthreshold maps with the outputs x(j)\ni , we have yi := PA\nj=1 x(A)\ni\nτi. Therefore, Nv is given by\nNv =\n\n\ny1\ny2\n...\nym\n\n,\nwhich proves the claim.\n30\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nAppendix B. Applications\nIn this section, we consider some applications of our OMv result in Theorem 4. These applications have\nquadratic Ω(n2)-time lower bounds, conditional on the OMv conjecture. We show that on structured graph\nthis lower bound can be beaten.\nB.1. High-accuracy Dynamic Laplacian Solver\nTheorem 6 (Dynamic Laplacian Solver). There is a dynamic algorithm that, given a dynamic graph G =\n(V, E) with corrupted VC-dimension bounded by d, maintains a Laplacian system solver. The data structure\nsupports queries that receive a vector b ∈R|V | and error parameter ǫ > 0. Then, in eO(n2−1/d log 1/ǫ)\ntime, the algorithm returns the (approximate) solution x to Lx∗= b where ∥x −x∗∥L ≤ǫ∥x∗∥L. Each\nvertex update to G takes eO(n) time.\nWe consider the setting where the Laplacian L corresponds to a graph with bounded VC-dimension.\nFirst, recall the notion of spectral sparsiﬁers.\nDeﬁnition 39 ((1 + ǫ)-spectral sparsiﬁers) Let LX denote the Laplacian of any undirected graph X. Then,\na (1 ± ǫ)-spectral sparsiﬁer H of a graph G is a subgraph of G such that for every vector x ∈Rn,\n(1 −ǫ)x⊤LHx ≤x⊤LGx ≤(1 + ǫ)x⊤LHx,\nThen, a result from Abraham et al. (2016) provides a construction for a dynamic spectral sparsiﬁer under\nedge deletions and insertions with polylogarithmic amortized update time:\nTheorem 40 (Theorem 4.1 of Abraham et al. (2016)) There exists a fully dynamic randomized algorithm\nwith polylogarithmic update time for maintaining a (1 ± ǫ)-spectral sparsiﬁer H of a graph G, with proba-\nbility at least 1−1/nc for any 0 < ǫ ≤1 and c ≥1. Speciﬁcally, the amortized update time of the algorithm\nis O(cǫ−2 log3 ρ log6 n) and the size of H is O(cnǫ−2 log3 ρ log5 n log W + mρ−1), where 1 ≤ρ ≤m is a\nparameter of choice. Here, W is the ratio between the largest and the smallest edge weight in G.\nWe use the following result for solving Laplacian systems in the static setting.\nLemma 41 (Kyng and Sachdeva (2016))\nThere is a randomized procedure that given any n-vertex m-\nedge graph G with Laplacian matrix LG, and vector b ∈RV such that there exists an x ∈RV with\nLGx = b computes x ∈RV with ∥x −x∥LG ≤ǫ∥x∥LG in eO(m log ǫ−1) with high probability.\nLemma 42 Let G be a dynamic graph and let A be its dynamic adjacency matrix. Assume there is a\ndynamic OMv data structure for this A with update time U(n) and query time Q(n).\nThen there exists a dynamic Laplacian System solver for the same dynamic graph G, supporting updates\nto G in O(U(n) + n) time. The data structure supports queries which for any given b ∈RV and ǫ > 0\nin eO(Q(n) log 1/ǫ) time return x ∈RV with ∥x −x∗∥LG ≤ǫ∥x∗∥LG where x∗is the exact solution for\nLGx = b.\nVia Theorem 4 we have U(n) = eO(n) and Q(n) = eO(n2−1/d), thus obtaining Theorem 6.\nProof We run two separate data structures. The ﬁrst is a dynamic spectral sparsiﬁer Theorem 40 which\nmaintains a spectral sparsiﬁer H ≈LG with error ǫ = 1/2. We also run the dynamic OMv data structure\nfor matrix A (adjacency matrix).\nThe update time is thus eO(n+U(n)) as the spectral sparsiﬁer needs polylog time per edge updates, thus\neO(n) time for node updates.\n31\n\n\nANAND BRAND MCCARTY\nQueries\nWhen given a vector b ∈RV and error parameter ǫ > 0, we use H−1 as preconditioner for the\nlinear system LGx = b. We can multiply with H−1 by running a static Laplacian system solver Lemma 41\nwhich takes eO(n) time per product as H has eO(n) edges. In particular, we let\nx0 = H−1b\nand then perform iterative reﬁnement (Richardson iteration)\nxt+1 ←xt −H−1(LGxt −b).\nAfter eO(log 1/ǫ) iterations we have ∥xt −x∗∥LG ≤ǫ∥x∗∥LG.\nEach iteration takes eO(n + Q(n)) time, where eO(n) is the time for multiplying by H−1, and Q(n) the\ntime for multiplying by LG. Observe that LGv = Dv −Av where D is a diagonal matrix and A is an\nadjacency matrix. So the ﬁrst product takes O(n) time and the second product is handled by the dynamic\nOMv data structure in O(Q(n)) time. Together, this proves the lemma.\nB.2. Effective Resistance\nThe following Theorem 7 is a corollary of the dynamic Laplacian solver from Theorem 6.\nTheorem 7 (Dynamic Effective Resistance).\nThere is a dynamic algorithm that, given a dynamic graph\nG = (V, E) with corrupted VC-dimension bounded by d, maintains effective resistances in G. The data\nstructure supports queries that receive a pair of vertices u, v ∈V and error parameter ǫ > 0. Then, in\neO(n2−1/d log 1/ǫ) time, the algorithm returns a (1±ǫ)-approximation of the effective resistance. Moreover,\neach node update to G in the dynamic data structure takes eO(n) time.\nProof The effective resistance of a pair u, v ∈V is the energy of an electric ﬂow, routing one unit of\nﬂow between u and v. The energy of an electric ﬂow f ∈RE is P\ne f 2\ne and the electric ﬂow is given by\nf = BL†(eu −ev) where B ∈{−1, 0, 1}E×V is the incidence matrix of G with arbitrary directions. Thus\nthe effective resistance is\n∥BL†(eu −ev)∥2\n2 = (eu −ev)⊤L†B⊤BL†(eu −ev)\n= (eu −ev)⊤L†(eu −ev).\nThis can be computed via the dynamic Laplacian system solver in Theorem 6.\nB.3. Triangle Detection\nTheorem 8 (Dynamic Triangle Detection).\nThere is a dynamic algorithm that, given a dynamic graph\nG = (V, E) with corrupted VC-dimension d, maintains whether G contains a triangle or not. Each vertex\nupdate to G takes O(n2−1/d) time and returns a Boolean indicator for whether G contains a triangle or not.\nThe result follows form the following lemma for U(n) = eO(n) and Q(n) = eO(n2−1/d) from Theorem 4.\nLemma 43 Let G be a dynamic graph and let A be its dynamic adjacency matrix. Assume there is a\ndynamic OMv data structure for this A with update time U(n) and query time Q(n).\nThen there exists a dynamic triangle detection algorithm for the same dynamic graph G, supporting\nupdates in O(U(n) + Q(n)) time.\n32\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nProof Let A be the adjacency matrix of the graph, then the diagonal entries (A3)v,v are non-zero if and only\nif v participates in a triangle. Thus by maintaining the sum P\ni(A3)i we can detect if there is any triangle\nin the graph.\nWe maintain the sum as follows: Let A be the incidence matrix before, and A after a node v is updated.\nThen\nX\ni\n(A′3)i = (\nX\ni\n(A3)i) + 3(\n(A′3)v,v −(A3)v,v\n|\n{z\n}\ndifference in # of triangles v participates in\n).\nThis is because each triangle v participates in is counted thrice in the sum (once for each vertex of the\ntriangle). Thus we can maintain the sum by computing (A′3)v,v and (A3)v,v. Observe that this is the\nvector-matrix-vector product of (i) the vth row of A, (ii) matrix A, (iii) and v-th column vector of A. Thus\nit can be computed in O(Q(n)) time, with an extra U(n) time to update A to A′.\nB.4. Single-Source Shortest Paths and k-Center\nLemma 44 (van den Brand et al. (2022)) Assume there is a dynamic distance oracle for unweighted undi-\nrected graphs with the following operations.\nAn initialization procedure that is given G = (V, E) and a threshold 1 ≤d ≤n.\nA node update operation with update time U(n, d).\nA query operation which for any source node, given during the query, returns the d-bounded single\nsource distances in O(Q(n, d)). That is, return for each v ∈V the distance if it is at most d, or ∞if the\ndistance is > d.\nThen for any 1 ≤k ≤n and ǫ > 0 there exists a dynamic (1 + ǫ)-approximate single source distance\ndata structure on unweighted undirected graphs. It supports node updates in\nO\n\u0012\nU\n\u0012\nn, 4\nǫ\n\u0013\n+ k\nǫ · Q\n\u0012\nn, 4\nǫ\n\u0013\n+ n2\nk\n\u0013\ntime.\nTheorem 9 (Dynamic Approximate Single-Source Shortest Paths). There is a dynamic algorithm that main-\ntains (1 + ǫ)-approximate single source distances on a dynamic unweighted undirected graph G = (V, E).\nIf the corrupted VC-dimension of G is bounded by d, each vertex update to G takes eO(kn2−1/(2d)/ǫ) time,\nand querying the distances for any source node takes O(n2−1/(2d)/ǫ) time.\nProof We ﬁrst describe how to obtain a dynamic distance oracle that yields distances up to 1/ǫ. This is\ndone by running the dynamic OMv data structure of Theorem 4 on the adjacency matrix of G. Then single\nsource distances up to 1/ǫ from source vertex u can be computed by repeatedly multiplying w ←Aw for\ninitial w = eu, i.e., we compute Aieu for i = 1, 2, ..., 1/ǫ. The smallest i with (Aieu)v ̸= 0 is the smallest\nnumber of steps to reach from u to v, i.e., the distance between the vertices.\nThis data structure internally constructs the a ∆-labeled spanning tree of the boolean matrix A, where\nthe weight of the tree gives us the time complexity per query.\nHence, while we do not know the corrupted VC-dimension d, we do know how much time each query\nis going to take.\n33\n\n\nANAND BRAND MCCARTY\nSo when running Lemma 44 on the above distance data structure, we can pick k = n/\n√\nT where T ≥n\nis the weight of the spanning tree. Thus, we get the update time\neO\n\u0012\nn + k\nǫ T + nk + n2\nk\n\u0013\n= eO\n \nn + n\n√\nT\nǫ\n+\nn\n√\nT\n!\n= eO(n2−1\n2d ),\nwhich proves the claim.\nTheorem 10 (Dynamic Approximate k-center). Given an unweighted undirected graph G = (V, E), there\nis a dynamic algorithm for (2 + ǫ)-approximate k-center with node update time O(kn2−1/2d/ǫ), where d is\na bound on the corrupted VC-dimension of G.\nThis result is a corollary of Theorem 9, as k-center can be reduced to computing k-single source distances.\nLemma 45 (Theorem 7.3 by Cruciani et al. (2024)) Given a graph G = (V, E), a positive parameter\nǫ ≤1/2, and a fully-dynamic data structure that maintains (1 + ǫ)-approximate single source distances\nwith update time T(n, m, ǫ) and Q(n, m, ǫ) query time, there is a dynamic algorithm that maintains a\n2(1 + 4ǫ)-approximate solution to fully-dynamic k-center in time O(T(n, m, ǫ) + k · (Q(n, m, ǫ) + n)).\nAppendix C. Characterization of matrices with constant VC-dimension\nIn this section, we provide a characterization of matrices with constant VC-dimension. Particularly, we\nprove Fact 1 and state a number of examples of other constant VC-dimension matrices.\nFact 1.\nConsider a hereditary class of 0/1-matrices M, meaning that M is closed under row/column\ndeletion (i.e., each matrix M ∈M is still in M after deleting a row or column). If M is non-trivial,\n(i.e., does not contain every possible matrix), then there exists an absolute constant c ∈N such that the\nVC-dimension of any matrix in M is at most c.\nFact 1 provides a structural characterization of some matrices with constant VC-dimension, and it is an\nanalogous result to the fact that non-trivial hereditary classes of graphs have low VC-dimension. To prove\nFact 1, we introduce the notion of a bipartisation of a graph.\nDeﬁnition 46 A bipartite graph H = (A ∪B, E) is a bipartisation of graph G if removing all edges in A\nand B in G yields H for some partition A, B of V (G). A graph class G is said to be hereditary if it is closed\nunder induced subgraphs.\nLemma 47 (Lemma 3.4 of Bousquet et al. (2015))\nFor any hereditary class C of graphs and for any bi-\npartite graph H, if the graphs in C have inﬁnite VC-dimension, then C contains a graph G whose bipartisa-\ntion is H.\nBy transposition, we get the following corollary:\nCorollary 48 The equivalent contrapositive statement is that for any hereditary class C of graphs and for\nany bipartite graph H, if C does not contain a graph G whose bipartisation is H = (A ∪B, E) where\nA ∪B = V (G), then the graphs in C have ﬁnite VC-dimension.\n34\n\n\nTHE STRUCTURAL COMPLEXITY OF MATRIX-VECTOR MULTIPLICATION\nLemma 49 The VC-dimension of any matrix in M is upper bounded by a ﬁnite constant.\nProof Since M ∈M is Boolean, we can treat it as a bipartite graph G, with rows representing left vertices\nand columns representing right vertices. In particular, we can treat M as a class of bipartite graphs closed\nunder vertex deletion (row deletion = deleting a left vertex, column deletion = deleting a right vertex). Thus\nwe can apply lemma 47 to M.\nSince M is non-trivial, there must be some graph H not contained in it. For this excluded graph H,\ndeﬁne H′ as the same graph but add one extra left vertex and one extra right vertex. Connect this new left\nvertex to every vertex on the right, and connect the new right vertex to every vertex on the left. Note that\nsince H was not contained in M and it is closed under vertex deletion, H′ also cannot be contained in M.\nNow assume there is G ∈M that contains H′ as bipartization. Then there is a partition A, B of V ,\nsuch that keeping only the edges between A and B of G, results in H′. Because of the left vertex in H′ that\nconnects to all right vertices, and the right vertex connecting to all left vertices, we must have that A are all\nleft and B are all the right vertices of G (or the other way around). But that means no edges were removed\nfrom G when restricting to edges between A and B, since a bipartite graph by deﬁnition only has edges\nconnecting left and right vertices. So G = H′ and H′ ∈M. This is a contradiction because by construction\nof H′, it does not exist in M.\nWe conclude, that set of bipartite graphs corresponding to M does not contain any graph whose biparti-\nsation is H′. Hence by Theorem 48 the VC-dimension must be a ﬁnite constant. Note that this is equivalent\nto the VC-dimension of the adjacency matrices being bounded, but M are not adjacency matrices. The\nadjacency matrices are of form\n\u0014 0\nM\nM⊤\n0\n\u0015\nfor matrices M ∈M. We already argued that each adjacency matrix has constant VC-dimension, and since\nremoving rows does not increase the VC-dimension (it is equivalent to deleting sets), this implies M also\nhas constant VC-dimension. In conclusion, each matrix in M has constant VC-dimension.\nC.1. Examples of low VC-dimension matrices\nWe provide some broad families of matrices with constant VC dimension.\nAdjacency matrices of H-minor free graphs.\nEppstein (1995) showed that H-minor free directed graphs\n(where the minor can be obtained through vertex deletion, edge deletion, and edge contraction) has VC-\ndimension at most |V (H)| −1. As an immediate consequence, since bipartite-graphs are triangle (K3)\nfree, the VC-dimension of any bipartite graph is 2. As a consequence, given any adjacency matrix A of\na H-minor free graph G, for any k ≥1, Ak is the adjacency matrix of a H-minor free graph G′ (since\nAk corresponds to the reachability graph where an edge (u, v) ∈E(G′) corresponds to the existence of a\nk-length walk between u and v in G). Therefore, the VC-dimension of Ak is also at most |V (H)| −1.\nAdjacency matrices of interval graphs.\nInterval graphs are the intersection graphs of a family of intervals\non the real line. Then, since the hypothesis class of intervals on the real line H = {1a,b : a ≤x ≤b} has\nVC-dimension 2 (any two points can be shattered by choosing an interval that includes one or both points,\nbut no set of three points can always be shattered), every interval graph has VC-dimension at most 2.\n35\n\n\nANAND BRAND MCCARTY\nBoolean kernel matrices.\nA matrix where each column i (and each row) receives some label ℓi ∈L from\nsome (possibly inﬁnite) set of possible labels L. Then, suppose each entry of M satisﬁes Mi,j = f(ℓi, ℓj)\nfor some function f : L →{0, 1}. For example, the labels could be points in L = Rd with f being indicator\nthat the distance is at most some threshold. If f, L is ﬁxed (i.e., independent of the number of rows/columns)\nthen these graphs are closed under row/column deletion and thus have bounded VC-dimension by Fact 1.\nShortest-path structures\nLet G be an undirected graph with non-negative edge weights and let S be a\nsubset of its shortest paths such that, for every pair (u, v) of distinct vertices, S contains exactly one shortest\npath between u and v. Marie de Lima et al. (2023) deﬁnes a range space associated with S and proves that\nits VC dimension is 2.\nAdjacency matrices of planar graphs.\nLet G be a planar graph. Then, there is a planar drawing of G,\nand deleting vertices and edges retains this property. Therefore, these graphs are closed under row/column\ndeletion and thus have bounded VC-dimension by Fact 1.\nAdjacency matrices of semi-algebraic graphs of bounded description complexity.\nBy the Milnor-\nThom theorem in real algebraic geometry (Matoušek, 2002) and the Sauer-Shelah lemma (Assouad, 1983),\nsemi-algebraic graphs of bounded description complexity have constant VC-dimensions. In turn, this ex-\ntends to adjacency matrices of string graphs where every two curves intersect.\nPractical matrices.\nCoudert et al. (2024) computes the VC-dimension of families of graphs that arise in\npractice, from protein interaction networks to autonomous Internet systems to web graphs. Even for such\ngraphs with millions of nodes, the VC-dimension of the graphs are typically between 3 and 8.\n36\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21240v1.pdf",
    "total_pages": 36,
    "title": "The Structural Complexity of Matrix-Vector Multiplication",
    "authors": [
      "Emile Anand",
      "Jan van den Brand",
      "Rose McCarty"
    ],
    "abstract": "We consider the problem of preprocessing an $n\\times n$ matrix M, and\nsupporting queries that, for any vector v, returns the matrix-vector product\nMv. This problem has been extensively studied in both theory and practice: on\none side, practitioners have developed algorithms that are highly efficient in\npractice, whereas theoreticians have proven that the problem cannot be solved\nfaster than naive multiplication in the worst-case. This lower bound holds even\nin the average-case, implying that existing average-case analyses cannot\nexplain this gap between theory and practice. Therefore, we study the problem\nfor structured matrices. We show that for $n\\times n$ matrices of VC-dimension\nd, the matrix-vector multiplication problem can be solved with $\\tilde{O}(n^2)$\npreprocessing and $\\tilde O(n^{2-1/d})$ query time. Given the low constant\nVC-dimensions observed in most real-world data, our results posit an\nexplanation for why the problem can be solved so much faster in practice.\nMoreover, our bounds hold even if the matrix does not have a low VC-dimension,\nbut is obtained by (possibly adversarially) corrupting at most a subquadratic\nnumber of entries of any unknown low VC-dimension matrix. Our results yield the\nfirst non-trivial upper bounds for many applications. In previous works, the\nonline matrix-vector hypothesis (conjecturing that quadratic time is needed per\nquery) was used to prove many conditional lower bounds, showing that it is\nimpossible to compute and maintain high-accuracy estimates for shortest paths,\nLaplacian solvers, effective resistance, and triangle detection in graphs\nsubject to node insertions and deletions in subquadratic time. Yet, via a\nreduction to our matrix-vector-multiplication result, we show we can maintain\nthe aforementioned problems efficiently if the input is structured, providing\nthe first subquadratic upper bounds in the high-accuracy regime.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}