{
  "id": "arxiv_2502.20635v1",
  "text": "Can LLM Assist in the Evaluation of the Quality of Machine\nLearning Explanations?\nBO WANG, Data Science Institute, University of Technology Sydney, Australia\nYIQIAO LI, Data Science Institute, University of Technology Sydney, Australia\nJIANLONG ZHOU, Data Science Institute, University of Technology Sydney, Australia\nFANG CHEN, Data Science Institute, University of Technology Sydney, Australia\nEXplainable machine learning (XML) has recently emerged to address the mystery mechanisms of machine\nlearning (ML) systems by interpreting their ’black box’ results. Despite the development of various explanation\nmethods, determining the most suitable XML method for specific ML contexts remains unclear, highlighting\nthe need for effective evaluation of explanations. The evaluating capabilities of the Transformer-based large\nlanguage model (LLM) present an opportunity to adopt LLM-as-a-Judge for assessing explanations. In this\npaper, we propose a workflow that integrates both LLM-based and human judges for evaluating explanations.\nWe examine how LLM-based judges evaluate the quality of various explanation methods and compare their\nevaluation capabilities to those of human judges within an iris classification scenario, employing both subjective\nand objective metrics. We conclude that while LLM-based judges effectively assess the quality of explanations\nusing subjective metrics, they are not yet sufficiently developed to replace human judges in this role.\nCCS Concepts: • Human-centered computing →Human computer interaction (HCI).\nAdditional Key Words and Phrases: Machine Learning Explanation, Transformers, Human-Computer Interac-\ntion, Large Language Models, LLM-as-a-Judge, Subjective, Objective.\nACM Reference Format:\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen. 2024. Can LLM Assist in the Evaluation of the Quality of\nMachine Learning Explanations?. 1, 1 (March 2024), 21 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nMachine learning (ML) systems have seen significant growth in popularity due to their increasing\nproblem-solving capabilities, which have been applied across diverse fields such as medical health-\ncare [17], biology analysis [25], and fraud detection [5]. However, these systems often operate\nas ’black boxes,’ lacking transparency and preventing users from understanding the rationale\nbehind their decisions. To address this issue, researchers have developed eXplainable ML (XML)\ntechniques, which aim to provide interpretability and insight into these opaque models, enhancing\ntheir trustworthiness and reliability.\nVarious methods have been proposed in the field of XML, including SHAP [23], LIME [29], and\nsimilarity-based explanations [4]. Despite the advancement of these methods, evaluating the quality\nof explanations remains a complex and unresolved issue [26, 37]. Thus, there is a pressing need to\nAuthors’ Contact Information: Bo Wang, bo.wang-11@student.uts.edu.au, Data Science Institute, University of Technology\nSydney, Sydney, Australia; Yiqiao Li, yiqiao.li@uts.edu.au, Data Science Institute, University of Technology Sydney, Sydney,\nAustralia; Jianlong Zhou, jianlong.zhou@uts.edu.au, Data Science Institute, University of Technology Sydney, Sydney,\nAustralia; Fang Chen, fang.chen@uts.edu.au, Data Science Institute, University of Technology Sydney, Sydney, Australia.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM XXXX-XXXX/2024/3-ART\nhttps://doi.org/XXXXXXX.XXXXXXX\n, Vol. 1, No. 1, Article . Publication date: March 2024.\narXiv:2502.20635v1  [cs.HC]  28 Feb 2025\n\n\n2\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\nevaluate the quality of explanations and identify the most appropriate XML methods for practical\napplications.\nExisting research has explored various approaches to evaluating explainability, encompass-\ning functionality-grounded, application-grounded, and human-grounded methods [9]. However,\nthese evaluation techniques have notable limitations. Functionality-grounded evaluations rely\non quantitative metrics that often fail to capture the nuances of human perception. Meanwhile,\napplication-grounded and human-grounded evaluations, which involve experiments with experts\nand non-experts, are resource-intensive, requiring significant time, cost, and ethical approvals. Al-\nternatively, researchers have investigated the use of Transformer models, specifically large language\nmodels (LLMs) as judges, positioning them as alternatives to human evaluators [15, 27, 28, 31].\nTherefore, employing the Transformer-based LLMs as judges to evaluate ML explanations represents\na promising approach.\nHowever, employing LLMs as evaluators introduces new challenges. Although LLMs have\ndemonstrated high agreement with human evaluations in some tasks, they are not human and\nmay produce errors distinct from human evaluators [2]. Therefore, it is essential to calibrate\nLLMs evaluations against human assessments using relevant datasets to ensure their validity and\nreliability. Currently, the effectiveness and capabilities of LLMs in evaluating the quality of ML\nexplanations compared to humans remain unexamined. Driven by these gaps, this study aims to\nanswer the following research questions:\nRQ1: How do different judges, including GPT-4o, Mistral-7.2B, and humans, evaluate the quality\nof various explanations?\nRQ2: How do LLM-based judges, such as GPT-4o and Mistral-7.2B, compare to human judges in\nevaluating the quality of explanations?\nTo address the research questions comprehensively, we design a workflow for evaluating machine\nlearning explanations using GPT-4o, Mistral-7.2B, and human judges based on iris classification.\nOur study is based on three types of explanations: those produced by LIME [29], similarity-based\nexplanations [13], and without explanations. To ensure fair comparisons among the judges, we\nconduct a forward simulation experiment involving 38 LLMs/human participants to analyze the\ncorrelation between LLM-based and human evaluations. To thoroughly assess the quality of ML\nexplanations, we employ both subjective and objective measures. Specifically, we develop five\nsubjective statements rated on a 5-point Likert scale and use accuracy as an objective metric to\nevaluate explanation quality. The main contributions of this study are as follows:\n• To the best of our knowledge, this study is the first to assess the validity and reliability of\nusing LLMs as judges for evaluating ML explanations.\n• This study examines and validates the judgments of LLM-based evaluators, specifically\nGPT-4o and Mistral-7.2B, against human judgments across various explanation methods.\n• This study focuses on evaluating the quality of various explanation methods, including LIME,\nsimilarity-based explanations, and cases without explanations.\n• We employ a combination of subjective and objective measures to evaluate the quality of ML\nexplanations in both LLM-based and human judgments.\nThe rest of the paper is organized as follows. In Section 2, the related work in the evaluation of ML\nexplanations and Transformers is overviewed. Following that, our hypotheses, considering various\njudges and explanations, are formulated in Section 3. Afterward, our methodologies including a\nworkflow that integrates both LLM-based and human judges for evaluating explanations, LLMs’\nprompt design, study design, and subjective and objective measurements are introduced in Section 4.\nNext, our experimental setup is presented, detailing the dataset and classified used, LLMs and\nexplainers employed, and the online user study conducted in Section 5. The results based on\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n3\nsubjective and objective measurements are analysed via statistical tests in Section 6. The main\nexperimental results, implications, and limitations of the study are summarized in Section 7. Finally,\nthe conclusions of this paper are drawn in Section 8.\n2\nRelated Work\nIn this section, we first survey current categories for evaluating ML explanations. Subsequently, we\nexamine the literature on advancements in the field of Transformers. At last, we conclude with a\ndiscussion of recent research efforts on the use of LLMs as judges.\n2.1\nEvaluation of ML Explanations\nCurrently, there are three categories of evaluation of ML explanations: functionality-grounded,\napplication-grounded, and human-grounded evaluations [9]. Functionality-grounded evaluation\nrequires no human experiments; instead, it employs formal definitions of interpretability as a proxy\nto evaluate explanation quality. For instance, Dai et al. [6] focus on fidelity, stability, consistency,\nand sparsity to evaluate the quality of explanations. Additionally, the depth of a decision tree has\nbeen used as an indicator of explanation quality [11]. Application-grounded evaluation, on the\nother hand, requires human-subject experiments within a real-world application context, typically\ninvolving domain experts. Goel et al. [12] adopt application-grounded evaluation by using ML\nexplanations on real-world diagnosis of COVID-19 CT images to study how well explanations\ninfluence clinicians’ trust in an automated decision-making task, suggesting that explanations\nenhance clinicians’ trust on the system.\nHuman-grounded evaluation refers to conducting simpler human-subject experiments that\ncapture the essence of the target application. Unlike application-grounded evaluation, this category\nof evaluation does not involve domain experts but lay humans. Wang et al. [33] conduct human-\ngrounded evaluation to explore the relationships between user trust and fidelity and robustness\nof explanations through a simulated user study, revealing user trust is significantly impacted by\ndifferent fidelity and robustness levels. Similarly, Lertvittayakumjorn and Toni [20] propose three\nhuman-grounded evaluation tasks to assess the quality of explanation methods in the context of\ntext classification for different purposes. Their findings indicate that good explanations can justify\npredictions and assist humans in investigating uncertain predictions, while using explanations to\nreveal model behavior remains a challenge. In this paper, we conduct an in-depth examination\nof human capabilities in assessing the quality of ML explanations through human-grounded\nevaluations. We subsequently compare these human capabilities with those of LLMs to investigate\nthe differences in their evaluating capabilities.\n2.2\nTransformer\nTransformer models replace the recurrent layers most commonly used in encoder-decoder archi-\ntectures with multi-headed self-attention, which is initially proposed by Vaswani et al. [32]. The\nadvent of Transformer models facilitates the development of a new age in various fields, such as\ncomputer vision [21], time-series prediction [19], and natural language processing (NLP) [1, 35]. Liu\net al. [21] present a new Transformer, called Swin Transformer, that provides a hierarchical vision\nTransformer whose representations can be computed on a local and global basis for computer\nvision. Their new Transformer model can lead to strong performance on image recognition tasks.\nIn addition, Sangwon et al. [19] propose a time-series forecasting optimized Transformer model,\ncalled TS-Fastformer, with three new optimizations including Sub Window Tokenizer, Time-series\nPre-trained Encoder, and Past Attention Decoder, delivering a faster training speed and a lower\nmean-square error. Moreover, Acheampong et al. [1] examine that Transformer-based models like\nGPT and its variants, Transformer-XL model, and BERT, yield significant improvements in NLP\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n4\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\ntasks, especially text-based emotion detection. Similarly, Yadav [35] discusses that Transformers\nmake NLP technologies become rather human-like in understanding and mirroring human lan-\nguage, leveraging sequence transduction architectures based entirely on attention mechanisms.\nAs a result, LLMs, such as GPT [24] and Mistral [16] style, built upon Transformer architectures,\nexhibit human-like capabilities and excel in learning complex patterns and linguistic constructions.\nSpecifically, to explore LLMs’ capabilities in the evaluation of the quality of ML explanations, we\nadopt two Transformer-based LLMs including GPT-4o and Mistral-7.2B in this paper.\n2.3\nApplying LLMs as Judges\nThe use of LLMs as judges is a newly developing topic, with numerous academic studies investigat-\ning and validating the performance of LLM-as-a-Judge. LLM-as-a-Judge is initially employed to\nevaluate the output of other LLMs [36]. In their work, they propose three types of LLM-as-a-Judge:\n(1) pairwise comparison, (2) single-answer grading, and (3) reference-guided grading. Through\nextensive examinations, they find out that LLM-as-a-judge is a scalable and explainable way to\napproximate human preferences on open-ended questions. Following this, Koucheme et al. [18]\nutilize GPT-4 as a single answer grading judge to assess the quality of GPT-3.5 feedback on incorrect\nstudent-written programs. Through comparing to the expert annotator, they notice that GPT-4 can\nbe quite reliable in evaluating the quality of automatically generated feedback. Similarly, Dong et\nal. [8] introduce an LLM-as-a-Personalized-Judge pipeline, where LLMs assess user preferences,\nachieving accuracy comparable to human evaluations and even surpassing human performance on\nhigh-certainty samples. On the other hand, Bavaresco et al. [2] argue that LLMs are not ready to\nsystematically replace human judges in NLP. Their study, which involved the application of 20 NLP\ndatasets with human annotations, evaluates LLMs on their ability to replicate these annotations,\nrevealing significant variability in LLM performance across datasets in relation to human judgments.\nHowever, to date, little work has been found in using LLMs as judges to evaluate the quality of ML\nexplanation methods. This paper aims to study the evaluating capabilities of different LLMs in the\nquality of different explanations and to compare these capabilities with those of human judges for\nthe same explanation. Specifically, we apply LLM-as-a-Judge with our tailored prompt design for\nevaluating explanations in our work.\n3\nHypotheses\nTo address our research questions (RQ1 and RQ2), this study formulates the corresponding hy-\npotheses (H1 and H2), considering different judges (GPT-4o, Mistral-7.2B, and humans) and varying\nexplanations (LIME, similarity-based, and without explanation).\nH1: When evaluated by the same judge, there are significant differences in explanation quality\nacross various explanations. Specifically, we expect that without explanation will result in the\nlowest quality. For H1, we have three sub-hypotheses:\n- H1.1: There are significant differences in quality across various explanations when evaluated\nby GPT-4o.\n- H1.2: There are significant differences in quality across various explanations when evaluated\nby Mistral-7.2B.\n- H1.3: There are significant differences in quality across various explanations when evaluated\nby humans.\nH2: When the same explanation is assessed, there are no significant differences in explanation\nquality across various judges. We anticipate that the evaluation capabilities of LLM-based judges\nwill be comparable to those of human judges. For H2, we have three sub-hypotheses:\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n5\n- H2.1: There are no significant differences in quality across various judges when assessing\nLIME.\n- H2.2: There are no significant differences in quality across various judges when assessing\nsimilarity-based.\n- H2.3: There are no significant differences in quality across various judges when assessing\nwithout explanation.\nH1 pertains to the variability in explanation quality across different explanations as detected by\nthe same judge, while H2 focuses on the consistency in explanation quality across different judges\nwhen they assess the same explanation.\n4\nMethodologies\nThis section starts by introducing our proposed workflow and case study. It then details the\nLLMs prompt design tailored for iris classification. Subsequently, the study design is discussed,\nincorporating conditions based on judges and explanations. Finally, the section outlines the approach\nused to evaluate the quality of explanations, integrating both subjective and objective aspects.\n4.1\nWorkflow and Case Study\nFigure 1 illustrates the process for involving both LLM-based and human judges in evaluating the\nquality of ML explanations. This workflow emphasizes consistency across all steps, including the\nuse of input data, the implementation of ML models and explanations, and the evaluation metrics,\nto ensure effective comparison between LLM-based and human judges. Our aim is for this workflow\nnot only to validate the evaluating capabilities of judges across distinct explanations but also to\ninvestigate the difference in evaluating capabilities between LLM-based and human judges.\nFig. 1. Workflow of Judges for Evaluating Explanations.\nThis paper uses iris classification, specifically targeting an iris instance and its corresponding\nexplanation, as a case study. The original iris dataset contains three iris labels (setosa, versicolor, and\nvirginica) determined by four features including sepal length, sepal width, petal length, and petal\nwidth. Specifically, in our study, the ML model predicts the target label, focusing on distinguishing\nbetween ’versicolor’ and ’virginica’ based on these features. Furthermore, the provided explanations\nelucidate the rationale behind the ML model’s decisions. Different explanation methods with various\nalgorithms and the presentation of their outputs would result in distinct quality levels. To investigate\nthe evaluating capabilities of judges, we design a forward simulation [9] of the iris classification\ntask, where judges predict the target label based on a given input and explanation. To this end,\nthe experiment is set up to determine how judges differ in their assessments of the quality of\nexplanations and to identify any evaluation differences between LLM-based and human judges.\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n6\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\nFig. 2. The Prompt for LLMs Evaluating Explanations. We provide (1) LLMs role, (2) task description, and (3)\ncontextual information\n4.2\nLLMs Prompt Design\nIn this study, we aim to investigate the capability of LLMs as judges in evaluating the quality of\nML explanations. Specifically, we task GPT-4o and Mistral-7.2B as LLM-based judges in a zero-\nshot manner to investigate their capabilities in evaluating explanations using both subjective and\nobjective metrics. A crucial element in enabling effective evaluation by LLM-based judges is the\ndesign of a well-crafted prompt [34]. Therefore, we develop an appropriate prompt design tailored\nto our iris classification task. We begin by prompting LLM-based judges to determine the iris\nlabels based on the given information, such as explanations. Following this, we use a single answer\ngrading prompt design of LLM-as-a-Judge [36], in which an LLM-based judge is asked to directly\nassign a score to a single subjective question. This allows LLM-based judges to rate their perceptions\nof explanations based on the specified subjective metrics. We also include additional instructions\nto constrain the LLMs’ output: [(<instance>, <label>), (<Question 1>, <integer score>), ...(<Question\n5>, <integer score>)]. In order to ensure LLMs generate the desired outputs, we iteratively refine\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n7\nour evaluation prompt [7, 34]. Consequently, our prompt design is structured around three main\ncomponents, as illustrated in Figure 2:\n(1) LLMs role: At the beginning of the prompt design, a comprehensive overview of LLM-based\njudges’ role is provided, emphasizing their task of evaluating explanations. This component\nmakes sure that LLMs grasp their overarching objective, particularly in evaluating XML.\n(2) Task description: The prompt design includes a detailed description of the tasks assigned\nto the LLM-based judges, providing clear instructions on the actions they need to perform\nbased on the given information. Specifically, this component instructs the LLMs to first\nmake a prediction of the target iris label and then assign scores to the subjective questions.\nAdditionally, it outlines the five metrics used for subjective evaluation.\n(3) Contextual information: The contextual information in our prompt design includes back-\nground details on the two iris labels with means of their four features, the target iris instance,\nand one of the three explanations. This component ensures that the LLMs have all the\nnecessary information to make informed evaluations.\nThis prompt design involves the systematic arrangement of overall LLMs’ behaviour, task\ndescription, and contextual information into a coherent format, aligning with the evaluating\ncapabilities of LLM-based judges.\nTable 1. Experiment designed conditions\nJudges\nExplainers\nLIME\nSimilarity-based\nWithout\nGPT-4o\nCondition 1\nCondition 2\nCondition 3\nMistral-7.2B\nCondition 4\nCondition 5\nCondition 6\nHuman\nCondition 7\nCondition 8\nCondition 9\n4.3\nStudy Design\nTo address our two RQs, a study is designed in which judges conduct a tabular-based iris classifica-\ntion task with the assistance of an ML explanation system. In this system, a convolutional neural\nnetwork (CNN) is implemented to do the classification tasks, and its decisions are explained by three\nexplanation methods (LIME, similarity-based, and without explanation). To test our hypotheses,\nwe first examine the evaluating capabilities of judges across distinct explanations. Additionally, we\ncompare the evaluating capabilities of judges (specifically comparison of LLM-based and human\njudges) within each explanation.\nIn this study, there are two independent variables. The first variable pertains to various judges,\nspecifically GPT-4o, Mistral-7.2B, and humans. Additionally, the second variable refers to explana-\ntion methods, which refer to LIME, similarity-based, and without explanation. The without expla-\nnation serves as our baseline explanation method where no explanation is provided. By considering\nboth distinct judges and varying explanations, we establish a total of 9 experimental conditions, as de-\ntailed in Table 1. Each judge is required to conduct 18 tasks (3 explanation methods×6 iris instances\n= 18 tasks).\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n8\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\n4.4\nEvaluation Measurements\nTo thoroughly evaluate the quality of various ML explanations, we employ a combination of\nsubjective and objective metrics. Our subjective metrics are designed to identify judges’ perceptions\nof the quality of ML explanations during task completion. Besides, our objective metric is designed\nto focus on measuring judges’ behaviours, which serve as indicators of the explanations’ quality.\nSubjective metrics. In order to measure judges’ perceptions of ML explanations, we adopt five\nsubjective statements derived from the measurements proposed by Hoffman et al. [14], ensuring\nto mitigate subjective bias. These subjective statements measure five distinct aspects including\nunderstandability, satisfaction, completeness, usefulness, and trustworthiness, as presented in\nTable 2. Specifically, each subjective statement is rated using a 5-point Likert scale from a range of\n1 (Strongly Disagree) to 5 (Strongly Agree).\nObjective metrics. To measure the presence of behaviours linked with the quality of ML explana-\ntions, we employ judge accuracy as our objective metric. We introduce this approach that takes into\naccount the judges’ decisions across three different explanation methods. Specifically, this approach\nquantifies the quality level of ML explanations as judge accuracy by measuring the proportion of\naccurate decisions made by judges out of the total decisions. High human accuracy is paramount for\nhigh-quality of ML explanations, as emphasized in XML systems [30]. In this work, judge accuracy\nis defined as the fraction in which judges’ decisions are accurate. As a result, the quality level of ML\nexplanations reflects the fraction that judges rely on the provided explanations to make accurate\ndecisions under distinct explanations. A higher fraction implies that judges utilize higher quality\nlevels of ML explanations to complete their decision-making processes.\nTable 2. Five Subjective Statements for Measuring Quality of ML Explanations\nUnderstandability\n1. I understand this explanation, which thereby\naids in my decision-making.\nSatisfication\n2. This explanation is satisfying as it effectively\nsupports my decision-making process.\nCompleteness\n3. This explanation seems complete enough to\nsupport my decision-making process.\nUsefullness\n4. This explanation is useful to assist me with\ndecision-making.\nTrustworthiness\n5. I trust this explanation to guide my decision-\nmaking process.\n5\nExperiments\nThis section presents the setup for both the LLM-based automatic study and the online user study. It\nbegins with an introduction to the dataset and the ML classifier employed, followed by an overview\nof the LLMs and ML explanation methods utilized. Finally, it provides a comprehensive discussion\nof the overall online user study.\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n9\n5.1\nDataset and Classifier\nThis study employs the iris dataset [10] as the primary data source. The original dataset contains\n150 instances with 4 features (sepal length, sepal width, petal length, and petal width) used to\ndetermine iris labels (setosa, versicolor, and virginica). To simplify the study, we reduced the dataset\nto 100 instances, with 70% allocated for training and the remainder for testing, focusing on binary\nclassification between the ’versicolor’ and ’virginica’ labels. For each iris label, six iris instances\n(three of each label) are selected from the testing dataset. Each iris instance has its explanations\ngenerated by three explanation methods.\nIn our work, a CNN model is employed for iris classification. The CNN is configured with three\nhidden layers, consisting of 5, 4, and 3 neurons respectively, and utilized activation functions\nsigmoid, Rectified Linear Unit (ReLU), and tanh in each layer. The output layer contained 2 neurons\nwith a softmax activation function. This configuration achieved a model accuracy of 93%.\n5.2\nLLMs and Explainers\nTo explore the evaluating capabilities of LLM-based judges on various explanations, we select\ntwo representative proprietary and open-source language models: GPT-4o and Mistral-7.2B. These\nmodels are chosen due to their widespread adoption and extensive documentation. We obtain 38\nmodel responses (matching the number of participant responses) by setting a temperature of 0.7\nfor both GPT-4o and Mistral-7.2B throughout their respective APIs. To ensure consistency with\nhuman evaluations, the settings for LLM-based judges mirror those for human judges, including\nthe provision of iris background information, target iris instances, and explanations, along with\ninstructions and evaluation measurements. Both LLMs are prompted using our tailored prompt\ndesign to complete 18 tasks.\nTo broaden the scope of the study in XML, we choose two prominent explainers from different\ncategories: LIME, a feature-based method, and similarity-based explanations, an exemplar method.\nSpecifically, LIME employs a surrogate model to interpret the predictions of the underlying task\nmodel by generating data samples with slight input perturbations [29]. Besides, similarity-based\nidentifies the instances in the training set that are similar to the test instance in question and their\ncorresponding model predictions [13]. Both explanation methods are implemented with default\nhyperparameter settings. In our data source, LIME and similarity-based explanation methods\nachieve a 93% and 96% accuracy, respectively. The labels of the selected six instances predicted by\ntwo explanations are aligned with their true labels, ensuring the correct explanations are provided\nto judges.\n5.3\nOnline User Study\nTo compare the evaluating capabilities of LLM-based judges to those of humans, an online user study\nis conducted via the Qualtrics platform. This user study, which is approved by the Human Research\nEthics Committee (HREC) of our university, takes approximately 15-20 minutes to complete. Figure 3\npresents an example of the interface used in the online user study\n5.3.1\nParticipants. 38 participants were invited to take part in our online user study through\nvarious communication channels, such as social media posts and emails. The participants, who\nwere primarily researchers and students, included 17 females, 18 males, and a few who chose not to\ndisclose their gender. The majority of participants were between their twenties and forties, with an\naverage age of 29 years. In terms of educational background, 4 participants held bachelor’s degrees,\nfollowed by 10 participants who held Ph.D. degrees, 22 participants who completed their master’s\ndegrees, and the remaining participants had their under bachelor’s degrees or honors qualifications.\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n10\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\nFig. 3. An Interface Example of Tasks in the Online User Study.\n5.3.2\nProcedure. At the beginning of the online study, participants are provided with a welcome\npage describing the objectives of this study. Following this, upon agreeing to take part in this\nstudy with a consent page, participants can formally participate in this study. Subsequently, an\nexperiment introduction page is presented to clarify participants’ roles and tasks briefly.\nParticipants then start one random task of the 16 tasks. For each main task, participants would\nsee: (1) a background table about two iris labels with means of their four features, (2) a target iris\ninstance, and (3) an additional explanation based on the explanation methods used in this study.\nBased on the provided information, participants are asked to make their own predictions about the\ntarget iris label. We record participants’ predictions where participants predict correct target iris\ninstance labels as a way of calculating participants’ accuracy in the objective aspect. Additionally,\nparticipants are tasked to rate their perceptions of quality levels concerning the explanations using\na 5-point Likert scale in the subjective aspect. During the conduction of 16 tasks, participants\ncan not go back to change their past responses. After completing 16 tasks, participants are then\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n11\nrequired to complete a demographics page with three questions: age, gender, and education level.\nParticipation in the user study is voluntary for each participant.\n6\nResults\nIn this section, we provide a detailed examination of how each judge assesses the quality of different\nexplanations(RQ1), in Section 6.1 and how LLM-based judges compare against human judges in the\nevaluation of explanations (RQ2) in Section 6.2. Furthermore, we conduct a comprehensive analysis\nof the results from subjective and objective metrics, employing statistical tests such as one-way\nANOVA and Tukey’s HSD post-hoc tests.\nFig. 4. Results for Judges across Explanations Based on Subjective and Objective Metrics. In this figure, error\nbars represent the 95% confidence interval of a mean. The (a), (b), (c), (d), and (e) refer to subjective metrics\nincluding understandability, satisfaction, completeness, usefulness, and trustworthiness, respectively. The (f)\nrefers to the objective metric - accuracy.\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n12\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\n6.1\nEvaluation of Explanations by Each Judge (RQ1)\nTo address RQ1 (H1), we engage different judges to evaluate explanations using subjective and\nobjective metrics. Subsequently, we apply a one-way ANOVA followed by Tukey HSD post-hoc\ntests. These statistical analyses aim to quantify differences in the quality of diverse explanations\nwhen assessed by the same judge (GPT-4o, Mistral-7.2B, and humans respectively) using subjective\nand objective metrics. The results from each subjective metric reveal significant differences in\nquality assessed by judges across a variety of explanations. However, the results from objective\nmetrics indicate statistically significant differences in quality across different explanations are\nfound in the evaluations of Mistral-7.2B and humans, rather than that of GPT-4o.\nTable 3. Each Judge Evaluation across Explanation Methods of One-way ANOVA. In this table, F, representing\nthe F value, aligns with the degrees of freedom (2, 111), and p, indicating the p-value, refers to the probability\nthat the differences between LIME, similarity-based, and without explanation. The p-values are less than .05\nare highlighted, indicating a statistically significant difference.\nGPT-4o Mistral-7.2B Human\nSubjective\nUnderstandability\nF 5385.00\n56.48\n22.86\np\n< .000\n< .000\n< .000\nSatisfaction\nF 6071.00\n84.50\n28.91\np\n< .000\n< .000\n< .000\nCompleteness\nF 5036.00\n133.91\n26.57\np\n< .000\n< .000\n< .000\nUsefulness\nF 5790.00\n148.54\n29.15\np\n< .000\n< .000\n< .000\nTrustworthiness\nF 5636.00\n147.45\n21.67\np\n< .000\n< .000\n< .000\nObjective\nAccuracy\nF\n1.00\n125.89\n6.47\np\n> .050\n< .000\n= .002\n6.1.1\nSubjective Evaluation of Explanations by Each Judge. The quality of ML explanations, evalu-\nated through five subjective statements, is shown in Figure 4, showcasing the average scores of the\njudges across three explanations. Using these scores, the one-way ANOVA and post-hoc Tukey\nHSD tests are conducted for each subjective statement to explore variations in quality assessments\nof each judge on the evaluation of different explanations.\nUnderstandability. The analysis of the one-way ANOVA results shows there are significant\ndifferences in understandability assessed by GPT-4o, Mistral-7.2B, and humans based on Table 3.\nThe post-hoc tests via Tukey HSD, as shown in Table 4, further find that, for GPT-4o, without\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n13\nexplanation is significantly different from LIME and similarity-based, and similarity-based is\nsignificantly different from LIME, thereby supporting H1.1. Furthermore, for both Mistral-7.2B and\nhumans, Tukey HSD tests illuminate that without explanation is significantly different from LIME\nand similarity-based, confirming H1.2 and H1.3. These outcomes suggest that judges effectively\nassess the understandability regarding different explanations, thereby supporting H1.\nTable 4. Each Judge Evaluation across Explanation Methods of Tukey HSD. In this table, 𝑝1, 𝑝2, and 𝑝3 refer to\nthe p-value for comparison between similarity-based and LIME, without explanation and LIME, and without\nexplanation and similarity-based, respectively. The p-values are less than .05 are highlighted, indicating a\nstatistically significant difference.\nGPT-4o Mistral-7.2B Human\nSubjective\nUnderstandability\n𝑝1\n< .000\n> .050\n> .050\n𝑝2\n< .000\n< .000\n< .000\n𝑝3\n< .000\n< .000\n< .000\nSatisfaction\n𝑝1\n< .000\n= .017\n> .050\n𝑝2\n< .000\n< .000\n< .000\n𝑝3\n< .000\n< .000\n< .000\nCompleteness\n𝑝1\n< .000\n< .000\n> .050\n𝑝2\n< .000\n< .000\n< .000\n𝑝3\n< .000\n< .000\n< .000\nUsefulness\n𝑝1\n< .000\n< .000\n> .050\n𝑝2\n< .000\n< .000\n< .000\n𝑝3\n< .000\n< .000\n< .000\nTrustworthiness\n𝑝1\n< .000\n< .000\n> .050\n𝑝2\n< .000\n< .000\n< .000\n𝑝3\n< .000\n< .000\n< .000\nObjective\nAccuracy\n𝑝1\n> .050\n< .000\n> .050\n𝑝2\n> .050\n< .000\n= .002\n𝑝3\n> .050\n< .000\n= .040\nSatisfaction. The one-way ANOVA tests, as illustrated in Table 3, detect there are significant\ndifferences in satisfaction evaluated by GPT-4o, Mistral-7.2B, and humans respectively. Post-hoc\nTukey HSD tests, as shown in Table 4, further indicate that without explanation is significantly\ndifferent from both LIME and similarity-based, and similarity-based is significantly different from\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n14\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\nLIME for GPT-4o, accepting H1.1. Similarly, for both Mistral-7.2B and humans, the Tukey HSD\ntests find without explanation is significantly different from LIME and similarity-based, hence\nsupporting H1.2 and H1.3. The results imply that judges effectively evaluate the satisfaction among\ndifferent explanations, thus accepting H1.\nCompleteness. According to the one-way ANOVA (Table 3), significant variations in completeness\nare observed for GPT-4o, Mistral-7.2B, and humans resepctively. Furthermore, we evaluate the\ndifferences in each judge by Tukey HSD (Table 4. For both GPT-4o and Mistral-7.2B, we find\nwithout explanation is significantly different from LIME and similarity-based, and similarity-based\nis significantly different from without explanation, thereby supporting H1.1 and H1.2. For humans,\nwe also find that without explanation is significantly different from LIME and similarity-based,\nconfirming H1.3. The results imply that judges rate completeness differently across explanations,\nhence supporting H1.\nUsefulness. The ANOVA tests (see Table 3) also reveal statistically significant differences in\nusefulness for all judges (GPT-4o, Mistral-7.2B, and humans). Additionally, the Tukey HSD tests (see\nTable 4) elucidate that without explanation exhibits significant differences from LIME and similarity-\nbased for all judges, while similarity-based shows significant differences from LIME for LLM-based\njudges (GPT-4o and Mistral-7.2B). These findings support H1.1, H1.2, and H1.3, suggesting that\njudges evaluate usefulness differently across various explanations, thereby confirming H1.\nTrustworthiness. The one-way ANOVA results (Table 3) detect that there are significant differences\nin trustworthiness for GPT-4o, Mistral-7.2B, and humans respectively. The Tukey HSD tests (Table 4)\nfurther show that for GPT-4o and Mistral-7.2B, without explanation significantly differs from both\nLIME and similarity-based, and similarity-based significantly differs from LIME, confirming H1.1\nand H1.2. Besides, for humans, the Tukey HSD tests find that similarity-based significantly differs\nfrom LIME, accepting H1.3. The outcomes present that judges assess trustworthiness differently\nacross explanations, hence supporting H1.\n6.1.2\nObjective Evaluation of Explanations by Each Judge. Quality, quantified by accuracy, is pre-\nsented in Figure 4, illustrating the average accuracy of the judges across three explanations. The\none-way ANOVA and Tukey HSD tests are employed to investigate variations in accuracy of each\njudge on the evaluation of explanations. Based on Table 3, the ANOVA shows no statistically\nsignificant accuracy difference across the distinct explanations for GPT-4o, leading to the rejection\nof H1.1.\nHowever, a one-way ANOVA test indicates that significant variations in accuracy are observed\nfor Mistral-7.2B and humans. As shown in Table 4, Post-hoc Tukey HSD tests further indicate\nthat without explanation is significantly different from both LIME and similarity-based for both\nMistral-7.2B and humans, while similarity-based is significantly different from LIME solely for\nMistral-7.2B. This supports H1.2 and H1.3, suggesting that Mistral-7.2B and humans assess accuracy\ndifferently across various explanations. Thus, these findings provide partial support for H1.\nOverall, these findings demonstrate that while judges assess the quality of different explanations\ndifferently based on subjective metrics, GPT-4o fails to evaluate the quality of ML explanations\nusing the objective metric. Thus, our H1 is partially accepted.\n6.2\nComparison of Evaluations by Different Judges (RQ2)\nTo address RQ2 (H2), we compare the evaluations of judges when assessing the same explanations\n(LIME, similarity-based, and without explanation) using both subjective and objective metrics. We\nthen conduct a one-way ANOVA followed by Tukey HSD post-hoc tests to analyze the results.\nOur analysis reveals significant differences in quality across different judges for each explanation\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n15\nTable 5. Comparision Evaluations for Each Explanation of One-way ANOVA. In this table, F, representing the\nF value, corresponds to the degrees of freedom (2, 111), and p, indicating the p-value, refers to the probability\nthat the observed differences between GPT-4o, Mistral-7.2B, and humans. The p-values are less than .05 are\nhighlighted, indicating a statistically significant difference.\nLIME Similarity-based Without\nSubjective\nUnderstandability\nF\n1.67\n2.63\n68.07\np > .050\n> .050\n< .000\nSatisfaction\nF\n3.67\n4.88\n63.40\np = .029\n= .009\n< .000\nCompleteness\nF\n23.53\n4.29\n53.40\np < .000\n= .016\n< .000\nUsefulness\nF\n3.77\n4.07\n57.79\np = .026\n= .020\n< .000\nTrustworthiness\nF\n20.38\n7.36\n57.66\np < .000\n< .001\n< .000\nObjective\nAccuracy\nF\n64.48\n28.46\n88.72\np < .000\n< .000\n< .000\nbased on most subjective metrics, rather than relying on the subjective understandability metric.\nFurthermore, the results from objective metrics observe there are significant differences in quality\nacross different judges for each explanation.\n6.2.1\nSubjective Comparison of Evaluations by Different Judges. The one-way ANOVA and post-\nhoc Tukey HSD tests are conducted for each subjective metric to examine consistency in quality\nassessments by different judges for each explanation (see Figure 4).\nUnderstandability. According to the results of the ANOVA tests (Table 5), we find no significant\ndifferences in understandability across judges both in LIME and similarity-based, hence accepting\nH2.1 and H2.2; however, there are significant differences in without explanation. Furthermore,\nwe evaluate the differences in without explanation by Tukey HSD (Table 6). We find GPT-4o is\nsignificantly different from Mistral-7.2B and humans, supporting H2.3. These findings show that\nGPT-4o’s understandability differs from that of humans in without explanation, resulting in a\npartial acceptance of H2.\nSatisfaction. The one-way ANOVA tests detect that there are significant differences in satisfaction\nin LIME, similarity-based, and without explanation, respectively (see Table 5). Post-hoc Tukey\nHSD tests further indicate that humans exhibit significantly different satisfaction compared to\nMistral-7.2B in LIME and compared to GPT-4o in similarity-based (see Table 6), leading to the\nrejection of H2.1 and H2.2. Besides, the Tukey HSD tests find GPT-4o exhibits significant differences\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n16\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\nTable 6. Comparision Evaluations for Each Explanation of Tukey HSD. In this table, 𝑝1, 𝑝2, and 𝑝3 refer to the\np-value for comparison between human and GPT-4o, Mistral-7.2B and GPT-4o, and Mistral-7.2B and human,\nrespectively. The p-values are less than .05 are highlighted, indicating a statistically significant difference.\nLIME Similarity-based Without\nSubjective\nUnderstandability\n𝑝1 > .050\n> .050\n< .000\n𝑝2 > .050\n> .050\n< .000\n𝑝3 > .050\n> .050\n> .050\nSatisfaction\n𝑝1 > .050\n= .009\n< .000\n𝑝2 > .050\n> .050\n< .000\n𝑝3 = .024\n> .050\n> .050\nCompleteness\n𝑝1 > .050\n= .011\n< .000\n𝑝2 < .000\n> .050\n< .000\n𝑝3 < .000\n> .050\n> .050\nUsefulness\n𝑝1 > .050\n= .022\n< .000\n𝑝2 > .050\n> .050\n< .000\n𝑝3 = .028\n> .050\n> .050\nTrustworthiness\n𝑝1 > .050\n< .001\n< .000\n𝑝2 < .000\n> .050\n< .000\n𝑝3 < .000\n> .050\n< .001\nObjective\nAccuracy\n𝑝1 < .000\n< .000\n< .000\n𝑝2 < .000\n> .050\n< .000\n𝑝3 < .000\n< .000\n< .000\ncompared to both Mistral-7.2B and humans in without explanation, thus rejecting H2.3. The results\nimply that the satisfaction of LLM-based judges is different from that of human judges, thereby\nleading to the rejection of H2.\nCompleteness. The ANOVA tests, presented in Table 5, reveal that significant variations in\ncompleteness are observed across judges in all three explanations. The post-hoc tests via Tukey HSD,\ndetailed in Table 6, elucidate that differs significantly from GPT-4o and humans in LIME, leading\nto the rejection of H2.1. Additionally, the Tukey HSD tests find that GPT-4o differs significantly\nfrom humans in similarity-based and from both Mistral-7.2B and humans in without explanation,\nresulting in the rejection of H2.2 and H2.3. The outcomes suggest that completeness assessed by\nLLM-based judges differs significantly from that assessed by human judges, thus rejecting H2.\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n17\nUsefulness. The one-way ANOVA results reveal there are significant differences in usefulness\nacross judges in LIME, similarity-based, and without explanation, respectively (as illustrated in\nTable 5). Post-hoc via Tukey HSD tests, based on Table 6, show that humans are significantly\ndifferent from Mistral-7.2B in LIME, leading to the rejection of H2.1. In similarity-based, GPT-4o\nis significantly different from humans, resulting in the rejection of H2.2. Moreover, in without\nexplanation, GPT-4o is significantly different from both Mistral-7.2B and humans, thus rejecting\nH2.3. The outcomes imply that the assessment of usefulness in ML explanations varies between\nLLM-based and human judges, thus leading to the rejection of H2.\nTrustworthiness. Regarding Table 5, the one-way ANOVA tests show that there are significant\nvariations in trustworthiness among judges in LIME, similarity-based, and without explanation.\nBased on Table 6, the Tukey HSD tests show that Mistral-7.2B differs significantly from GPT-4o\nand humans in LIME, leading to the rejection of H2.1. In similarity-based, GPT-4o is significantly\ndifferent from humans in similarity-based, rejecting H2.2. Besides, in without explanation, GPT-4o\ndiffers significantly from both Mistral-7.2B and humans, and Mistral-7.2B also differs significantly\nfrom humans, rejecting H2.3. These results demonstrate that the evaluation of trustworthiness by\nLLM-based judges differs from that of human judges, thereby rejecting H2.\n6.2.2\nObjective Comparison of Evaluations by Different Judges. Using our objective metric - accuracy,\nwe conduct the one-way ANOVA and Tukey HSD tests to examine consistency in accuracy evaluated\nby different judges when assessing each explanation (see Figure 4). The ANOVA analysis (see\nTable 5) reveals that significant differences in accuracy are observed among the judges in each\nexplanation. Post-hoc Tukey HSD (see Table 6) tests show that GPT-4o is significantly different\ncompared to Mistral-7.2B and humans in LIME. Additionally, humans are significantly different from\nMistral-7.2B in this explanation. These findings lead to the rejection of H2.1. In similarity-based,\nTukey HSD finds that humans are significantly different compared to GPT-4o and Mistral-7.2B,\nrejecting H2.2. Besides, in without explanation, GPT-4o is significantly different compared to both\nMistral-7.2B and humans, and humans are also significantly different from Mistral-7.2B, thereby\nrejecting H2.3. These findings indicate that the accuracy in evaluating ML explanations differs\nbetween LLM-based and human judges, thus leading to the rejection of H2.\nIn summary, the results indicate that the evaluating capabilities of judges are comparable in\nLIME and similarity-based according to the subjective understandability metric, rather than relying\non the other subjective metrics. Besides, regarding the objective metrics, significant differences are\nobserved in the evaluation of the same explanation across judges. As a result, our H2 is partially\nrejected.\n7\nDiscussion\nIn this section, we first have a comprehensive description of our research findings. Also, we provide\na discussion of the implications of these results. At last, we delineate a reflection of the limitations\nand future directions of our study.\n7.1\nFindings\nIn this paper, we propose a workflow that incorporates judges (GPT-4o, Mistral-7.2B, and humans)\nto evaluate the quality of explanations. We conduct an experiment using iris classification and three\nexplanation methods—LIME, similarity-based, and a baseline ’without explanation’—to investigate\nand compare the capabilities of different judges.\nResults show that while judges effectively assess the quality of different explanation methods\nbased on subjective metrics, judges fail it according to objective metrics. Specifically, Mistral-7.2B\nand humans exhibit significant differences in accuracy among the different explanations, however,\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n18\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\nGPT-4o does not show significant differences in accuracy across the explanations. Hence, based on\nthese findings, our H1 is partially affirmed.\nRegarding the evaluating capabilities of LLM-based judges compared to those of humans, their\ncapabilities are comparable in LIME and similarity-based when assessed using the subjective\nunderstandability metric, rather than relying on other subjective metrics. Furthermore, significant\ndifferences are observed across judges concerning the same explanation when evaluated using\nobjective metrics. Consequently, H2 is partially rejected.\nIn summary, our experiment first confirms the capabilities of judges for evaluating the quality of\ndifferent explanations are significantly different primarily through subjective metrics. On the other\nhand, LLM-based judges’ evaluating capabilities of ML explanations are significantly different from\nthose of humans according to subjective (satisfaction, completeness, usefulness, and trustworthiness)\nand objective metrics; however, this is less evident when using the subjective understandability\nmetric in LIME and similarity-based.\n7.2\nImplications\nThe experiment outcomes reveal clear insights into the capabilities of LLM-based judges in assessing\nML explanations. However, our findings indicate that LLM-based judges are not yet capable of fully\nreplacing human judges. Since the goals of explainability methods are inherently human-centric,\nhuman-centered evaluations remain essential to the assessment of ML explanations. Instead, LLMs\nshould be viewed as valuable tools that complement traditional human evaluations due to their\nhuman-like capabilities. Based on our analysis of subjective metrics, LLM-as-a-Judge proves to be a\nuseful and effective tool in evaluating ML explanations. Specifically, LLMs are able to identify that\nthe baseline method (without explanation) exhibits a lower quality level compared to both LIME\nand similarity-based explanations subjectively.\nMoreover, our findings reveal the evaluation of ML explanation contexts where LLM-based\nevaluations differ significantly from human evaluations based on most of our subjective and\nobjective metrics. These insights are indispensable for programmers and designers who work on\nLLMs. While LLM-as-a-Judge is increasingly applied in many evaluating fields, it is important\nto note that LLMs are not actual humans and are prone to systematic biases that differ from\nthose of human judges. This is particularly evident in our case -the classification of iris instances\nunder the without explanation method considering the objective aspect. This underscores the need\nfor ongoing improvements in LLMs algorithms to enhance their human-like comprehension and\nevaluation capabilities. Ultimately, with further refinement, LLMs could become a cost-efficient\nalternative to traditional human evaluation methods.\nBesides, our findings reveal their judgment capabilities are comparable in specific dimensions\nsuch as subjective understandability metric for both LIME and similarity-based, rather than based\non other metrics (see Table 5). This highlights insights for people who focus on the evaluation of\nML explanations that further exploration is necessary before directly adopting LLMs to assess ML\nexplanations.\n7.3\nLimitations and Future Directions\nThough our experiment unveils significant findings about the evaluating capabilities of LLMs of the\nML explanations, several inherent limitations in the experimental design should be acknowledged.\nOne such limitation is that our dataset for the experiment is tabular data (iris dataset) with only four\nfeatures. Real XML systems and LLM-based judgments often deal with more complex data types\nand feature sets, potentially leading to greater variability in outputs. Therefore, future research\nshould explore XML systems and LLMs evaluations using a variety of data types (such as images or\ntext) and more extensive feature sets. Although studying more complex scenarios poses challenges,\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n19\nit presents valuable opportunities to examine LLM-based judgments across diverse contexts and\ndetermine whether significant differences arise.\nAnother limitation is the reliance on high-accuracy explanation methods for evaluating the\nquality of explanations. However, the XML systems may generate explanations with low accuracy\nin practice, which is acceptable if the ML model’s accuracy is also low. While existing research\nhighlights that accuracy impacts individual explanations [3, 22], other properties such as robustness\nand novelty also play crucial roles in determining explanation quality. Future research should\ninvestigate how LLMs assess explanations that exhibit lower accuracy and evaluate additional\nproperties of explanations to provide a more comprehensive assessment of their capabilities.\n8\nConclusion\nThe human-based experiment is one of the foolproof methods of evaluating Ml explanation methods\ndue to the goals of explanation methods being human-centric. However, human-subject experiments\nalways require time and cost to conduct. With the recent advances and human-like capabilities in\nLLMs, we propose a workflow comprising LLM-based and human judges to study the correlation\nbetween LLM-based and human judgments in the evaluation of ML explanations. To achieve this,\nwe conduct an experiment where judges evaluate the quality of different ML explanations based on\nthe iris classifications, employing a combination of subjective and objective metrics. Our results\nshow several key insights: 1) judges (LLM-based and humans) effectively assess the quality of\ndifferent ML explanations supported by subjective metrics, contrary to the results from objective\nmetrics; 2) the evaluating capabilities of LLM-based judges are significantly different from those\nof humans in three explanations respectively based on subjective and objective metrics; however,\nthis is evident beyond the subjective understandability metric in LIME and similarity-based. As a\nresult, we conclude that while LLM-based judges are capable of evaluating explanations through\nsubjective metrics, they are not yet sufficiently developed to replace human judges in this role.\nAcknowledgments\nThe ethical approval for our research is granted by the HREC of the University of Technology\nSydney with the number ETH22-7616. We thank the participants who took part in our studies.\nReferences\n[1] Francisca Adoma Acheampong, Henry Nunoo-Mensah, and Wenyu Chen. 2021. Transformer models for text-based\nemotion detection: a review of BERT-based approaches. Artificial Intelligence Review 54, 8 (2021), 5789–5829.\n[2] Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam\nGhaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. 2024. LLMs instead of Human Judges? A Large\nScale Empirical Study across 20 NLP Evaluation Tasks. arXiv preprint arXiv:2406.18403 (2024).\n[3] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. 2019. Machine learning interpretability: A survey on\nmethods and metrics. Electronics 8, 8 (2019), 832.\n[4] Guillaume Charpiat, Nicolas Girard, Loris Felardos, and Yuliya Tarabalka. 2019. Input similarity from the neural\nnetwork perspective. Advances in Neural Information Processing Systems 32 (2019).\n[5] Chiao-Ting Chen, Chi Lee, Szu-Hao Huang, and Wen-Chih Peng. 2024. Credit Card Fraud Detection via Intelligent\nSampling and Self-supervised Learning. ACM Transactions on Intelligent Systems and Technology 15, 2 (2024), 1–29.\n[6] Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen H Bach, and Himabindu Lakkaraju. 2022. Fairness via explanation\nquality: Evaluating disparities in the quality of post hoc explanations. In Proceedings of the 2022 AAAI/ACM Conference\non AI, Ethics, and Society. 203–214.\n[7] Paul Denny, Viraj Kumar, and Nasser Giacaman. 2023. Conversing with copilot: Exploring prompt engineering for\nsolving cs1 problems using natural language. In Proceedings of the 54th ACM Technical Symposium on Computer Science\nEducation V. 1. 1136–1142.\n[8] Yijiang River Dong, Tiancheng Hu, and Nigel Collier. 2024. Can LLM be a Personalized Judge? arXiv preprint\narXiv:2406.11657 (2024).\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n20\nBo Wang, Yiqiao Li, Jianlong Zhou, and Fang Chen\n[9] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint\narXiv:1702.08608 (2017).\n[10] R. A. Fisher. 1988. Iris. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C56C76.\n[11] Alex A Freitas. 2014. Comprehensible classification models: a position paper. ACM SIGKDD explorations newsletter 15,\n1 (2014), 1–10.\n[12] Kanika Goel, Renuka Sindhgatta, Sumit Kalra, Rohan Goel, and Preeti Mutreja. 2022. The effect of machine learning\nexplanations on user trust for automated diagnosis of COVID-19. Computers in Biology and Medicine 146 (2022),\n105587.\n[13] Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, and Kentaro Inui. 2021. Evaluation of Similarity-based Explanations.\nhttp://arxiv.org/abs/2006.04528 arXiv:2006.04528 [cs, stat].\n[14] Robert R. Hoffman, Shane T. Mueller, Gary Klein, and Jordan Litman. 2019. Metrics for Explainable AI: Challenges and\nProspects. http://arxiv.org/abs/1812.04608 arXiv:1812.04608 [cs].\n[15] Taojun Hu and Xiao-Hua Zhou. 2024. Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions. arXiv\npreprint arXiv:2404.09135 (2024).\n[16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint\narXiv:2310.06825 (2023).\n[17] Simi Job, Xiaohui Tao, Lin Li, Haoran Xie, Taotao Cai, Jianming Yong, and Qing Li. 2024. Optimal treatment strategies\nfor critical patients with deep reinforcement learning. ACM Transactions on Intelligent Systems and Technology 15, 2\n(2024), 1–22.\n[18] Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Juho Leinonen, and Paul Denny. 2024. Open Source\nLanguage Models Can Provide Feedback: Evaluating LLMs’ Ability to Help Students Using GPT-4-As-A-Judge. http:\n//arxiv.org/abs/2405.05253 arXiv:2405.05253 [cs].\n[19] Sangwon Lee, Junho Hong, Ling Liu, and Wonik Choi. 2024. TS-Fastformer: Fast Transformer for Time-Series\nForecasting. ACM Transactions on Intelligent Systems and Technology 15, 2 (2024), 1–20.\n[20] Piyawat Lertvittayakumjorn and Francesca Toni. 2019. Human-grounded evaluations of explanation methods for text\nclassification. arXiv preprint arXiv:1908.11355 (2019).\n[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer:\nHierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on\ncomputer vision. 10012–10022.\n[22] Helena Löfström, Karl Hammar, and Ulf Johansson. 2022. A meta survey of quality evaluation criteria in explanation\nmethods. In International Conference on Advanced Information Systems Engineering. Springer, 55–63.\n[23] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural\ninformation processing systems 30 (2017).\n[24] Yuhong Mo, Hao Qin, Yushan Dong, Ziyi Zhu, and Zhenglin Li. 2024. Large language model (llm) ai text generation\ndetection based on transformer deep learning algorithm. arXiv preprint arXiv:2405.06652 (2024).\n[25] Dylan Molho, Jiayuan Ding, Wenzhuo Tang, Zhaoheng Li, Hongzhi Wen, Yixin Wang, Julian Venegas, Wei Jin, Renming\nLiu, Runze Su, et al. 2024. Deep learning in single-cell analysis. ACM Transactions on Intelligent Systems and Technology\n15, 3 (2024), 1–62.\n[26] Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice\nVan Keulen, and Christin Seifert. 2023. From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic\nReview on Evaluating Explainable AI. Comput. Surveys 55, 13s (Dec. 2023), 1–42. https://doi.org/10.1145/3583558\n[27] Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, and Yun-Nung Chen. 2024. A Survey\nof Useful LLM Evaluation. arXiv preprint arXiv:2406.00936 (2024).\n[28] Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakkar. 2024. Constructing Domain-Specific Evaluation\nSets for LLM-as-a-judge. arXiv preprint arXiv:2408.08808 (2024).\n[29] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i trust you?\" Explaining the predictions\nof any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data\nmining. 1135–1144.\n[30] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-Precision Model-Agnostic Explanations.\nProceedings of the AAAI Conference on Artificial Intelligence 32, 1 (April 2018). https://doi.org/10.1609/aaai.v32i1.11491\n[31] Shreya Shankar, JD Zamfirescu-Pereira, Björn Hartmann, Aditya G Parameswaran, and Ian Arawjo. 2024. Who\nValidates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. arXiv preprint\narXiv:2404.12272 (2024).\n[32] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).\n[33] Bo Wang, Jianlong Zhou, Yiqiao Li, and Fang Chen. 2023. Impact of Fidelity and Robustness of Machine Learning\nExplanations on User Trust. In Australasian Joint Conference on Artificial Intelligence. Springer, 209–220.\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\nCan LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?\n21\n[34] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-\nSmith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv\npreprint arXiv:2302.11382 (2023).\n[35] B Yadav. 2024. Generative AI in the Era of Transformers: Revolutionizing Natural Language Processing with LLMs.\n[36] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with\nMT-Bench and Chatbot Arena. http://arxiv.org/abs/2306.05685 arXiv:2306.05685 [cs].\n[37] Jianlong Zhou, Amir H Gandomi, Fang Chen, and Andreas Holzinger. 2021. Evaluating the quality of machine learning\nexplanations: A survey on methods and metrics. Electronics 10, 5 (2021), 593.\n, Vol. 1, No. 1, Article . Publication date: March 2024.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20635v1.pdf",
    "total_pages": 21,
    "title": "Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?",
    "authors": [
      "Bo Wang",
      "Yiqiao Li",
      "Jianlong Zhou",
      "Fang Chen"
    ],
    "abstract": "EXplainable machine learning (XML) has recently emerged to address the\nmystery mechanisms of machine learning (ML) systems by interpreting their\n'black box' results. Despite the development of various explanation methods,\ndetermining the most suitable XML method for specific ML contexts remains\nunclear, highlighting the need for effective evaluation of explanations. The\nevaluating capabilities of the Transformer-based large language model (LLM)\npresent an opportunity to adopt LLM-as-a-Judge for assessing explanations. In\nthis paper, we propose a workflow that integrates both LLM-based and human\njudges for evaluating explanations. We examine how LLM-based judges evaluate\nthe quality of various explanation methods and compare their evaluation\ncapabilities to those of human judges within an iris classification scenario,\nemploying both subjective and objective metrics. We conclude that while\nLLM-based judges effectively assess the quality of explanations using\nsubjective metrics, they are not yet sufficiently developed to replace human\njudges in this role.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}