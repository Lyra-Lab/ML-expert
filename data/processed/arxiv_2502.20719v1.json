{
  "id": "arxiv_2502.20719v1",
  "text": "GENERATING CLINICALLY REALISTIC EHR DATA VIA A\nHIERARCHY- AND SEMANTICS-GUIDED TRANSFORMER\nA PREPRINT (UNDER REVIEW)\nGuanglin Zhou1,* and Sebastiano Barbieri1\n1The University of Queensland, Brisbane, Australia\n*Corresponding author: guanglin.zhou@uq.edu.au\nABSTRACT\nGenerating realistic synthetic electronic health records (EHRs) holds tremendous promise for ac-\ncelerating healthcare research, facilitating AI model development and enhancing patient privacy.\nHowever, existing generative methods typically treat EHRs as flat sequences of discrete medical\ncodes. This approach overlooks two critical aspects: the inherent hierarchical organization of clinical\ncoding systems and the rich semantic context provided by code descriptions. Consequently, synthetic\npatient sequences often lack high clinical fidelity and have limited utility in downstream clinical\ntasks. In this paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT), a\nnovel framework that leverages both hierarchical and semantic information for the generative pro-\ncess. HiSGT constructs a hierarchical graph to encode parent-child and sibling relationships among\nclinical codes and employs a graph neural network to derive hierarchy-aware embeddings. These\nare then fused with semantic embeddings extracted from a pre-trained clinical language model (e.g.,\nClinicalBERT), enabling the Transformer-based generator to more accurately model the nuanced\nclinical patterns inherent in real EHRs. Extensive experiments on the MIMIC-III and MIMIC-IV\ndatasets demonstrate that HiSGT significantly improves the statistical alignment of synthetic data\nwith real patient records, as well as supports robust downstream applications such as chronic disease\nclassification. Our evaluations, based on fidelity, utility, and privacy metrics, indicate that HiSGT not\nonly enhances the quality of synthetic EHR data but also preserves patient privacy. By addressing\nthe limitations of conventional raw code-based generative models, HiSGT represents a significant\nstep toward clinically high-fidelity synthetic data generation and a general paradigm suitable for\ninterpretable medical code representation, offering valuable applications in data augmentation and\nprivacy-preserving healthcare analytics.\nKeywords Electronic health records · Synthetic data generation · Hierarchy and semantics-guided · Generative\ntransformer\n1\nIntroduction\nElectronic Health Records (EHRs) are a cornerstone of modern healthcare, playing a pivotal role in patient care, clinical\ndecision-making, and medical research [Jha et al., 2009, Cowie et al., 2017, Xiao and Sun, 2021, Xiao et al., 2018].\nDespite their importance, strict regulations surrounding privacy and data confidentiality have made it challenging to\nshare and utilize EHR data at scale [Johnson et al., 2016, 2023]. As a result, there is growing interest in synthetic EHR\ndata generation, a paradigm that aims to produce clinically realistic datasets mimicking the distributional properties of\nreal EHRs while protecting sensitive patient information [Gonzales et al., 2023, McDuff et al., 2023, van Breugel et al.,\n2024, Ghosheh et al., 2024]. By balancing privacy preservation and data utility, synthetic EHRs hold the promise of\naccelerating model benchmarking, enabling data augmentation, and facilitating collaborative research.\nRecent advances in generative models have spurred significant progress in synthetic EHR data generation. Many\napproaches use Generative Adversarial Networks (GANs) [Choi et al., 2017, Zhang et al., 2021, Torfi and Fox, 2020, Cui\net al., 2020, Zhang et al., 2020, Rashidian et al., 2020, Kuo et al., 2022] and Variational AutoEncoders (VAEs) [Biswal\narXiv:2502.20719v1  [cs.LG]  28 Feb 2025\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\n(a) Tokenization\nMIMIC-III & MIMIC-IV\npatients.csv\nadmissions.csv\ndiagnoses_icd.csv\nSort \nGroupby\nTokenize\nMapping\nSTART_RECORD\nHypertension \nwith \ncomplications\nPneumonia\nChronic \nkidney \ndisease\nFluid and \nelectrolyte \ndisorders\nComplications of \nsurgical/medical \ncare\nFluid and \nelectrolyte \ndisorders\nEND_LABEL\nICD9-40301: \nHypertensive \nchronic \nkidney \ndisease\nICD9-486: \nPneumonia, \norganism \nunspecified\nICD9-5855: \nChronic kidney \ndisease, Stage V\nICD9-4254: \nOther \nprimary \ncardiomyo\npathies\nEND_VISIT\nICD9-58381: \nNephritis and \nnephropathy\nICD9-28521: \nAnemia in \nchronic kidney \ndisease\nICD9-45829: \nOther \niatrogenic \nhypotension\nICD9-33829: \nOther \nchronic pain\nEND_VISIT\nEND_RECORD\nPADDING\nPADDING\n...\nToken Details\n(c) Model Structure\nSemantic \nEmbd\nPre-trained \nClinical \nLMs\nSection\nHierarchical\nEmbd\nGNN\nN00-N99\nN18.5\nN18\nN17-N19\nChapter\nCategory\nSubcategory_1\n(b) Hierarchical & Semantic Embds\nICD9-5855\nto \nICD10-N18.5\nHierarchical levels\n“Chronic kidney \ndisease, stage 5”\nCode\nToken\nHierarchical\nEmbd\nSemantic \nEmbd\nTransformer Decoder Blocks\nPositional \nEmbd\n+\nInput Embd\nConsistency Regularization\nx 6\nHidden States\nCode \nHead\nHierarchical \nHead\nSemantic \nHead\nText \ndescription \nlookup\nNext code \nprediction\nSubcategory_2\nN18.3\nN18.32\n…\nA00-B99\n…\n…\nFigure 1: Overview of HiSGT for Synthetic EHR Generation. (a) Tokenization: We extract structured patient records\nfrom the MIMIC-III and MIMIC-IV datasets, and process hospital admissions and diagnosis codes. The tokenization\nprocess encodes patient timelines into structured sequences, strictly following the format: {START_RECORD, phenotype\nlabel tokens, END_LABEL, event tokens per visit, END_VISIT, · · · , END_RECORD, PADDING}. (b) Hierarchical &\nSemantic Embeddings: Medical codes (e.g., ‘N18.5’) are enriched with hierarchical embeddings from a constructed\nICD-based graph and semantic embeddings from a pre-trained clinical language model (LM), e.g, ClinicalBERT. The\nGNN-derived hierarchical embeddings capture multi-level taxonomic relationships, while ClinicalBERT encodes text\ndescriptions. (c) Model Structure: HiSGT integrates hierarchical and semantic embeddings into Transformer-based\ndecoder blocks. Three prediction heads—hierarchical, semantic, and next-code prediction—jointly optimize the\nmodel. During inference, HiSGT autoregressively generates synthetic patient records, starting from the first token\nSTART_RECORD and sequentially predicting phenotype labels, medical visit/events until reaching END_RECORD.\net al., 2021]. Other methods have drawn on the Transformer architecture [Vaswani et al., 2017] to capture long-\nrange dependencies in clinical sequences. In these models, medical codes and clinical events are typically treated as\ndiscrete tokens, with a next-token prediction objective used to model the sequential structure of EHR data [Theodorou\net al., 2023, Kraljevic et al., 2024, Pang et al., 2024, Renc et al., 2024]. However, they overlook the rich semantic\ninformation embedded in clinical code descriptions and the inherent hierarchical relationships in clinical coding systems.\nConsequently, the synthesized patient sequences frequently lack high clinical fidelity, which limits the utility of the\ngenerated synthetic EHR data. In particular, two major research gaps remain.\nFirst, medical coding systems such as the International Classification of Diseases (ICD) [Organization et al., 1978] are\norganized hierarchically, with codes exhibiting parent–child and sibling relationships that reflect key clinical knowledge.\nFor example, in ICD-10, the code ‘E11’ (Type 2 diabetes mellitus) serves as a parent for ‘E11.6’ (type 2 diabetes mellitus\n2\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nwith other specified complications) and as an ancestor for ‘E11.65’ (Type 2 diabetes mellitus with hyperglycemia),\nwhile sibling codes such as ‘E11.65’ and ‘E11.64’ (Type 2 diabetes mellitus with hypoglycemia) capture different\ndiabetes complications at a more granular level. In clinical practice, a patient may first be diagnosed at a coarse level\nwith ‘E11’ and later be assessed for specific complications (e.g., ‘E11.64’ or ‘E11.65’). However, current generative\nmodels often treat each code as an isolated symbol, failing to leverage these multi-level taxonomic properties. As a\nresult, they may generate conflicting sibling codes that cannot clinically co-occur (e.g., ‘E11.64’ hypoglycemia and\n‘E11.65’ hyperglycemia in the same visit) or neglect important subcodes. Second, each medical code is accompanied\nby a textual description that clarifies its clinical semantics and distinguishes between disease subtypes. For instance,\n‘E11.65’ denotes ‘Type 2 diabetes mellitus with hyperglycemia’, while ‘Z13.6’ specifies ‘screening for cardiovascular\ndisorders’. Such text descriptions are essential for determining the precise clinical focus of a patient’s visit. In practice,\neach patient’s visit to the hospital often centers on a primary diagnosis (or a small set of diagnoses) with additional\nprocedures tailored to that theme until the patient’s condition is confirmed by the physician. Consequently, the codes\nassigned within a single visit tend to be semantically related, reflecting the shared focus on a particular disease process.\nHowever, existing generative methods typically disregard these semantic descriptions and treat codes solely as discrete\ntokens. Without leveraging the underlying semantics, these models may synthesize codes that are logically or clinically\nincompatible.\nBeyond these challenges, EHR data are inherently high-dimensional and sparse. For example, datasets like MIMIC-\nIII [Johnson et al., 2016] and MIMIC-IV [Johnson et al., 2023] contain thousands of unique diagnostic codes, many\nof which occur infrequently. This sparsity and high dimensionality pose challenges in capturing the statistical and\nclinical relationships among codes. Without incorporating additional information—such as textual descriptions or\nhierarchical structures—purely raw code-based methods struggle with these nuances. As a result, many generative\napproaches mitigate this issue by reducing data complexity, often through aggregating medical codes or removing rare\ncodes [Theodorou et al., 2023]. Consequently, the generated synthetic patient sequences often exhibit low clinical\nfidelity, ultimately limiting their practical utility in real-world healthcare applications.\nTo address these limitations, we propose a novel framework called Hierarchy and Semantics-Guided Transformer\n(HiSGT), which integrates both hierarchical relationships and semantic information into the generative process. HiGST\nbegins by constructing a hierarchical graph to mirror the taxonomy of real coding systems such as ICD, preserving\nthe parent–child and sibling relationships. This graph helps to encode and extract clinically coherent paths for every\nsingle code (e.g., from ‘E11.6’ to ‘E11.65’). A graph neural network (GNN) is then employed to learn hierarchy-aware\nembeddings that capture structured dependencies among medical codes. Additionally, HiGST enriches each medical\ncode with semantic embeddings derived from a pre-trained clinical language model (e.g. ClinicalBERT [Alsentzer\net al., 2019]). These embeddings are consistent with their text description to help inject essential domain knowledge\ninto the Transformer backbone. Thus, concepts like ‘Type 2 diabetes’ carry meaningful distinctions rather than being\ntreated solely as a meaningless token. Finally, these refined embeddings are fed into a Transformer-based generative\nmodel to synthesize EHR data that are both semantically consistent and structurally coherent. Our work makes the\nfollowing key contributions to the field of synthetic EHR generation: (1) we propose a novel framework that leverages\nboth hierarchical graph representations and clinical semantic embeddings, as an approach to embed medical codes in\ngeneral, to overcome the limitations of raw code-only generative methods; (2) we develop an enhanced Transformer\nmodel that integrates refined code representations, enabling the generation of synthetic EHRs with high clinical fidelity;\n(3) we demonstrate the effectiveness of HiSGT on two real-world EHR datasets, showing significant improvements in\ndata quality, downstream task performance (e.g., disease classification), and privacy preservation.\n2\nResults\n2.1\nFidelity Evaluation\nTo assess the clinical fidelity of synthetic EHR data, we employ four complementary metrics that assess the alignment\nbetween real and synthetic records at different levels of granularity, capturing both individual event distributions and\ntheir relationships within and across patient visits. First, we compute the Unigram score, which measures how well the\ngenerated data preserves the marginal distribution of individual medical events. For each event, we count its occurrences\nin both the real and synthetic datasets and compute the coefficient R2 between the two frequency distributions. A\nhigher R2 value indicates that the generated data preserves the marginal distribution of real-world medical events.\nBeyond individual event distributions, we assess co-occurrence structures by computing the Bigram score, which\ncaptures the statistical dependencies between pairs of medical events within the same visit. This score reflects intra-visit\ncoherence—an essential property of high-fidelity EHR synthesis, as medical events within a visit often exhibit strong\ncorrelations. Similarly, the sequential bigram score (Seq Bigram) extends this evaluation to inter-visit dependencies,\nquantifying how well the synthetic data captures longitudinal patterns by examining the transitions between medical\ncodes across consecutive visits. Finally, we compute the dimension-wise correlation (DimWise). Unlike Unigram that\n3\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nTable 1: Performance on Fidelity Metrics. The table reports the R2 scores for four metrics—Unigram (marginal event\ndistribution), Bigram (intra-visit co-occurrence), Seq Bigram (inter-visit transitions), and DimWise (dimension-wise\ncorrelation)—evaluated on both MIMIC-III and MIMIC-IV.\nMethods\nMIMIC-III\nMIMIC-IV\nUnigram\nBigram\nSeq Bigram\nDimWise\nUnigram\nBigram\nSeq Bigram\nDimWise\nLSTM\n0.939\n0.522\n0.339\n0.806\n0.970\n0.736\n0.737\n0.822\nGPT\n0.935\n0.858\n0.811\n0.940\n0.969\n0.935\n0.907\n0.957\nEVA\n0.986\n0.749\n0.779\n0.869\n-\n-\n-\n-\nSynTEG\n0.036\n-2.562\n-0.579\n-0.052\n0.909\n0.733\n0.630\n0.735\nHALO-Coarse\n0.933\n0.774\n0.637\n0.701\n0.927\n0.743\n0.712\n0.900\nHALO\n0.936\n0.867\n0.869\n0.764\n0.973\n0.932\n0.924\n0.947\nHiSGT (Ours)\n0.984\n0.949\n0.879\n0.976\n0.989\n0.967\n0.948\n0.989\nonly captures overall dataset-level frequency distributions, DimWise measures the per-patient probability distribution of\nmedical codes between real and synthetic data. We first construct code probabilities matrices for real and synthetic\ndatasets, where each row represents a patient and each column represents an ICD code. The elements of the matrix\ncorrespond to the relative frequency of each ICD code within a given patient’s sequence. Specifically, for a patient\nwith n total medical codes, the probability of each code is computed by dividing its occurrence count by n. We then\ncompute the mean probability of each ICD code across all patients, reducing the original patient-code matrix to a single\nprobability vector per dataset. The coefficient of determination (R2 score) is computed between these two vectors (one\nfor real and one for synthetic data) as the final DimWise score.\nThe results in Table 1 provide evidence of HiSGT’s superior ability to synthesize clinically faithful EHR data. For\nthe MIMIC-III dataset, HiSGT achieves an Unigram score of 0.984, closely approximating the real data’s marginal\ndistribution. More importantly, the intra-visit Bigram score reaches 0.949—an 8.2% improvement over HALO’s\n0.867—indicating that HiSGT more accurately captures the co-occurrence patterns of medical events within a visit.\nThe sequential dependencies, as measured by the Seq Bigram score, are also better modeled by HiSGT (0.879 versus\n0.869 for HALO), while the DimWise metric (0.976) further confirms its effectiveness in preserving dimension-level\ncorrelations. Similarly, on the MIMIC-IV dataset, HiSGT consistently outperforms competing methods by recording\na Unigram score of 0.989, a Bigram score of 0.967, and a Seq Bigram score of 0.948, compared to HALO’s scores\nof 0.973, 0.932, and 0.924, respectively. The DimWise score for HiSGT reaches 0.989, reinforcing its capability to\ncapture both the marginal distributions and the complex dependencies inherent in clinical data. These improvements\nunderscore the importance of incorporating hierarchical and semantic structures in generating synthetic EHR data.\nTo further evaluate the fidelity of synthetic EHR data, we visualize the alignment between real and synthetic medical\ncode distributions using a scatter plot in Figure 2. Each point represents a unique medical code or medical code pair,\nand its occurrence frequency in the synthetic dataset plotted against its corresponding frequency in real data. Ideally, all\npoints should lie along the diagonal red line, indicating a perfect match between synthetic and real distributions. From\nthe figure, we observe that HiSGT exhibits a significantly tighter clustering of points along the diagonal compared to\nbaseline models, while baseline models such as LSTM, GPT, and SynTEG show greater dispersion. This visualization\nfigure supports the results in Table 1, where HiSGT consistently outperforms competing methods on Unigram, Bigram,\nand Seq Bigram scores.\n2.2\nUtility Evaluation\nTo evaluate the clinical utility of synthetic EHR data, we conduct a disease classification experiment that assesses\nwhether predictive models trained on synthetic clinical data can generalize to real-world patient records. This experiment\nfollows the Train on Synthetic, Test on Real (TSTR) evaluation paradigm, where models are first trained on synthetic\ndata and then evaluated on real patient outcomes. This methodology ensures that synthetic data captures meaningful\nclinical patterns that can support downstream predictive modeling tasks [Esteban et al., 2017]. In our setting, we\nadopt the phenotype classification task, considering 25 phenotypes, a widely used benchmark in clinical predictive\nmodeling [Harutyunyan et al., 2019]. For real data, phenotype labels are assigned using a rule-based mapping from ICD\ndiagnosis codes to phenotype categories, based on the HCUP Clinical Classifications Software (CCS) [Harutyunyan\net al., 2019]. Each patient in the real dataset is assigned one or more phenotype labels based on their recorded ICD\ncodes. For synthetic data, phenotype labels are first generated by the model and then used to conditionally generate\n4\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nLSTM\nSynTEG\nHALO\nHiSGT\nFigure 2: Scatter plot of synthetic vs. real distributions for Unigram, Bigram and Seq Bigram on MIMIC-IV.\nEach point represents a unique medical code or medical code pair, where its frequency in the synthetic dataset is plotted\nagainst its frequency in real data. The red diagonal line indicates perfect alignment with real distributions. HiSGT\nexhibits a tighter clustering along the diagonal compared to baselines, demonstrating superior fidelity in preserving\nreal-world code distributions.\npatient visit sequences containing medical codes, following the approach in [Esteban et al., 2017]. This conditional\ngeneration strategy ensures that patient trajectories are synthesized in a way that maintains meaningful associations\nbetween phenotype labels and clinical events. For the classification model, we employ a bi-directional LSTM classifier\nwith a fully connected output layer [Theodorou et al., 2023]. For each dataset, we randomly extract 5,000 patient records\nfor training, maintaining a balanced distribution of positive and negative labels for every single chronic disease category.\nFurther, we reserve a separate validation set of 250 records for model selection and 500 real patient records as a held-out\ntest set. Table 2 presents the classification results for various baseline models. We report accuracy, precision, recall,\nand F1-score, with standard deviations computed over the 25 phenotype labels. The performance of models trained on\nreal data serves as an upper bound for comparison. Among synthetic data models, HiSGT consistently achieves the\nhighest performance, surpassing HALO and other baseline methods in both MIMIC-III and MIMIC-IV datasets. These\nresults highlight the effectiveness of our conditional generation strategy, demonstrating that HiSGT-generated patient\n5\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nTable 2: Performance on Phenotype Classification. This table presents the results of the Train on Synthetic, Test on\nReal (TSTR) evaluation for phenotype classification. Models are trained on synthetic patient records and evaluated on\nreal MIMIC-III and MIMIC-IV datasets. We report the average accuracy, precision, recall, and F1-score (with standard\ndeviations) across 25 phenotype categories. Higher scores indicate better generalization to real-world patient data.\nMethods\nMIMIC-III\nMIMIC-IV\nAcc\nPrecision\nRecall\nF1-Score\nAcc\nPrecision\nRecall\nF1-Score\nReal Data\n0.951±0.02\n0.946±0.02\n0.957±0.03\n0.951±0.02\n0.945± 0.02\n0.936±0.02\n0.956±0.02\n0.946±0.02\nLSTM\n0.493±0.06\n0.480±0.12\n0.673±0.26\n0.538±0.17\n0.545±0.10\n0.534±0.15\n0.580±0.18\n0.546±0.14\nGPT\n0.885±0.04\n0.862±0.05\n0.919±0.05\n0.889±0.04\n0.883±0.05\n0.864±0.05\n0.912±0.05\n0.887±0.04\nEVA\n0.500±0.07\n0.470±0.15\n0.636±0.26\n0.526±0.17\n-\n-\n-\n-\nSynTEG\n0.515±0.03\n0.605±0.17\n0.680±0.38\n0.513±0.22\n0.563±0.08\n0.622±0.11\n0.558±0.26\n0.533±0.12\nHALO-Coarse\n0.858±0.05\n0.855±0.05\n0.865±0.07\n0.859±0.05\n0.827±0.05\n0.827±0.05\n0.830±0.08\n0.826±0.05\nHALO\n0.892±0.03\n0.871±0.04\n0.920±0.04\n0.895±0.03\n0.893±0.04\n0.880±0.04\n0.909±0.05\n0.894±0.04\nHiSGT (Ours)\n0.898±0.04\n0.878±0.05\n0.927±0.05\n0.901±0.04\n0.905±0.04\n0.888±0.03\n0.929±0.04\n0.907±0.04\ntrajectories retain the clinical coherence necessary for accurate phenotype classification. Table 3 provides a detailed\nbreakdown of classification performance across different disease types (acute, chronic, and mixed). The macro-averaged\naccuracy and F1-score across each disease category offer insights into how well synthetic data generalizes across\ndifferent clinical conditions. Across all disease types, HiSGT demonstrates superior performance over current SOTA\nmethod HALO.\n2.3\nPrivacy Evaluation\nIn addition to ensuring high fidelity and utility, it is critical that synthetic EHR data do not compromise patient\nprivacy [Giuffrè and Shung, 2023]. To this end, we evaluate the privacy protection of our synthetic data by simulating\ntwo common attacks: a membership inference attack (MIA) and an attribute inference attack (AIA). These attacks are\ndesigned to assess whether an adversary could either determine if a particular patient record was used during training or\ninfer sensitive attributes from the synthetic records. Specifically, MIA tests whether an adversary can correctly classify\na real patient record as having been used (or not used) during the synthetic data generation process. We construct an\nattack dataset by randomly sampling 500 records from the training set (labeled as positive) and 500 records from the test\nset (labeled as negative). For each record in this attack set, we compute the minimum Hamming distance to the synthetic\ndataset. By comparing each distance to the median distance of the entire attack set, records are classified as members\n(positive) if the distance is below the median and as non-members otherwise. An accuracy close to 50% indicates that\nthe synthetic data do not leak membership information beyond what would be expected by random guessing. The\nAIA aims to determine whether sensitive attributes can be reliably predicted from the synthetic data. We transform\nboth real and synthetic patient records by aggregating visit-level information and then select a set of common codes\n(derived from the most frequent 100 codes in the training data) to serve as a basis for inference. For each record, we\ncompare the known label set with those of its nearest neighbors (based on a simple set-based distance) and predict\nadditional codes via a majority vote. The performance of the attack is measured using an F1-score (AIA-F1), where\nlower scores indicate better privacy protection by the synthetic data. Table 4 summarizes the results of both MIA and\nAIA on MIMIC-III and MIMIC-IV. The reported metrics include accuracy, precision, recall, and F1-score for MIA, as\nwell as the AIA-F1 score. As evidenced by the results, all evaluated models—including HiSGT—exhibit an attack\naccuracy near 50% and low AIA-F1 scores, which confirms that the synthetic data do not reveal substantial information\nabout individual patient membership or sensitive attributes. In particular, Table 4 reveals an inherent trade-off between\nfidelity and utility with privacy in synthetic EHR generation. Models such as LSTM, EVA, and SynTEG achieve\nthe most robust privacy protection. Their MIA accuracies are around 50%—consistent with random guessing—and\nthey report the lowest AIA-F1 scores (e.g., 0.007–0.008 on MIMIC-III), indicating that these methods leak minimal\ninformation regarding patient membership and sensitive attributes. However, this strong privacy comes at a cost: these\nsame methods exhibit considerably lower fidelity and utility (as shown in Tables 1 and 2), meaning that their synthetic\ndata deviate substantially from the true data distributions and present limited utility. Conversely, SOTA methods like\nHALO and the proposed HiSGT deliver higher fidelity and utility. When compared to HALO, HiSGT achieves similar\nprivacy protection with an MIA F1-score of 0.472 and an AIA-F1 of 0.034 on MIMIC-III, while continuing to improve\nfidelity and utility.\n6\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nTable 3: Phenotype Classification Performance Across Disease Categories. This table provides a detailed breakdown\nof phenotype classification performance for different disease types (acute, chronic, and mixed). We compare models\ntrained on synthetic data (HALO and HiSGT) with real data as the ground truth. Accuracy and F1-scores are reported\nfor each phenotype category, with macro-averaged scores included for all acute, mixed, and chronic disease groups.\nResults demonstrate the ability of synthetic data to support downstream predictive modeling across a diverse range of\nclinical conditions.\nPhenotype\nType\nHALO (SOTA)\nHiSGT (Ours)\nReal Data\nAcc\nF1-Score\nAcc\nF1-Score\nAcc\nF1-Score\nAcute and unspecified renal failure\nacute\n0.917\n0.919\n0.921\n0.924\n0.960\n0.960\nAcute cerebrovascular disease\nacute\n0.879\n0.876\n0.907\n0.912\n0.959\n0.959\nAcute myocardial infarction\nacute\n0.887\n0.890\n0.917\n0.919\n0.952\n0.953\nCardiac dysrhythmias\nmixed\n0.898\n0.899\n0.893\n0.895\n0.937\n0.938\nChronic kidney disease\nchronic\n0.956\n0.957\n0.946\n0.947\n0.975\n0.975\nChronic obstructive pulmonary disease\nchronic\n0.913\n0.916\n0.909\n0.911\n0.936\n0.937\nComplications of surgical/medical care\nacute\n0.865\n0.868\n0.892\n0.893\n0.916\n0.918\nConduction disorders\nmixed\n0.878\n0.881\n0.879\n0.885\n0.937\n0.938\nCongestive heart failure; nonhypertensive\nmixed\n0.930\n0.932\n0.941\n0.943\n0.981\n0.981\nCoronary atherosclerosis and other heart disease\nchronic\n0.927\n0.928\n0.919\n0.920\n0.956\n0.956\nDiabetes mellitus with complications\nmixed\n0.916\n0.916\n0.919\n0.922\n0.956\n0.956\nDiabetes mellitus without complication\nchronic\n0.913\n0.912\n0.931\n0.932\n0.955\n0.956\nDisorders of lipid metabolism\nchronic\n0.916\n0.917\n0.923\n0.926\n0.963\n0.963\nEssential hypertension\nchronic\n0.946\n0.946\n0.939\n0.940\n0.957\n0.958\nFluid and electrolyte disorders\nacute\n0.893\n0.896\n0.925\n0.926\n0.962\n0.963\nGastrointestinal hemorrhage\nacute\n0.886\n0.885\n0.902\n0.905\n0.930\n0.932\nHypertension with complications\nchronic\n0.941\n0.941\n0.939\n0.940\n0.977\n0.977\nOther liver diseases\nmixed\n0.875\n0.877\n0.892\n0.891\n0.946\n0.947\nOther lower respiratory disease\nacute\n0.842\n0.848\n0.855\n0.850\n0.952\n0.952\nOther upper respiratory disease\nacute\n0.749\n0.723\n0.767\n0.770\n0.923\n0.924\nPleurisy; pneumothorax; pulmonary collapse\nacute\n0.883\n0.887\n0.884\n0.888\n0.949\n0.949\nPneumonia\nacute\n0.880\n0.885\n0.896\n0.899\n0.942\n0.944\nRespiratory failure; insufficiency; arrest\nacute\n0.886\n0.890\n0.897\n0.900\n0.941\n0.942\nSepticemia (except in labor)\nacute\n0.913\n0.914\n0.923\n0.924\n0.974\n0.974\nShock\nacute\n0.900\n0.899\n0.921\n0.924\n0.965\n0.965\nAll acute diseases (macro-averaged)\n0.893\n0.894\n0.905\n0.907\n0.945\n0.946\nAll mixed diseases (macro-averaged)\n0.892\n0.893\n0.901\n0.903\n0.943\n0.944\nAll chronic diseases (macro-averaged)\n0.923\n0.925\n0.932\n0.933\n0.963\n0.964\nAll diseases (macro-averaged)\n0.893\n0.894\n0.905\n0.907\n0.945\n0.946\n2.4\nInterpretable Input Embeddings via t-SNE Visualizations\nTo further evaluate the ability of our model to capture meaningful hierarchical and semantic information, we perform\nt-SNE visualizations of input embeddings after training HiSGT and use HALO as a comparison. These visualizations\nhelp assess whether the model successfully groups medical codes into coherent clusters. From Figure 3, we observe that\nHALO exhibits more scattered clusters, with many points overlapping across different ICD categories. This suggests\nthat HALO’s embedding space does not strongly enforce category-specific separation. In contrast, HiSGT demonstrates\nmore defined and compact clusters, indicating that it captures category-level information more effectively. The improved\nclustering structure also leads to a more interpretable embedding space. These findings indicate that the incorporation\nof hierarchical and semantic representations in HiSGT enhances the clinical coherence of medical code embeddings.\nBy preserving these relationships more effectively, HiSGT has the potential to improve fidelity and utility in synthetic\nEHR generation, since the generated sequences better reflect real-world clinical distributions.\n7\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nTable 4: Privacy Evaluation Results. This table reports the performance of the membership inference attack (MIA)\nand attribute inference attack (AIA) on both MIMIC-III and MIMIC-IV. The metrics include accuracy, precision, recall,\nand F1-score for MIA, and the AIA-F1 score for AIA. An accuracy near 50% indicates that the synthetic data do not\nleak membership information beyond chance levels, while low AIA-F1 scores reflect robust protection against inferring\nsensitive attributes. Notably, the results highlight a trade-off between fidelity and utility with privacy: while models\nsuch as LSTM, EVA, and SynTEG achieve very strong privacy (evidenced by the lowest AIA-F1 scores), they suffer\nfrom poor fidelity and utility. In contrast, SOTA methods like HALO—and our proposed HiSGT—strike a better\nbalance by delivering higher fidelity and utility with only a moderate increase in privacy risk.\nMethods\nMIMIC-III\nMIMIC-IV\nAcc.\nPrecision\nRecall\nF1-score\nAIA-F1\nAcc.\nPrecision\nRecall\nF1-score\nAIA-F1\nLSTM\n0.500\n0.500\n0.494\n0.497\n0.008\n0.507\n0.507\n0.502\n0.505\n0.011\nGPT\n0.504\n0.504\n0.462\n0.482\n0.036\n0.511\n0.512\n0.480\n0.495\n0.044\nEVA\n0.500\n0.500\n0.462\n0.480\n0.007\n-\n-\n-\n-\n-\nSynTEG\n0.509\n0.510\n0.482\n0.495\n0.007\n0.516\n0.517\n0.484\n0.500\n0.022\nHALO-Coarse\n0.501\n0.501\n0.478\n0.489\n0.022\n0.513\n0.514\n0.494\n0.504\n0.027\nHALO\n0.494\n0.493\n0.448\n0.470\n0.035\n0.509\n0.509\n0.494\n0.502\n0.046\nHiSGT (Ours)\n0.493\n0.492\n0.454\n0.472\n0.034\n0.504\n0.504\n0.478\n0.491\n0.045\n3\nDiscussion\nIn this study, we presented HiSGT, a novel Hierarchy- and Semantics-Guided Transformer for generating synthetic EHR\ndata with high clinical fidelity. By integrating hierarchical information from clinical coding taxonomies and semantic\nrepresentations from pre-trained clinical language models, HiSGT overcomes key limitations of prior methods that treat\nmedical codes as isolated tokens. Our extensive evaluations on the MIMIC-III and MIMIC-IV datasets demonstrate\nthat HiSGT produces synthetic patient records that more accurately replicate real-world clinical distributions while\nmaintaining robust privacy and competitive utility. The core fidelity metrics reveal that HiSGT substantially improves\nthe preservation of both marginal event distributions and intra- and inter-visit dependencies compared to baseline\nmethods. For instance, our model achieved an 8.2% improvement in the intra-visit Bigram score over HALO, indicating\nbetter capturing of the co-occurrence patterns among clinical events. These improvements are particularly important\nbecause accurate modeling of such dependencies is crucial for ensuring that synthetic data support downstream clinical\napplications. Further, HiSGT is a fully Transformer-based model and, contrary to HALO, does not rely on a dense\n40\n20\n0\n20\n40\nt-SNE1\n40\n20\n0\n20\n40\nt-SNE2\nInput Embeddings\nR\nC\nV\nB\nI\nE\nM\nG\nW\nO\n(a) HALO\n80\n60\n40\n20\n0\n20\n40\n60\n80\nt-SNE1\n75\n50\n25\n0\n25\n50\n75\nt-SNE2\nInput Embeddings\nR\nC\nV\nB\nI\nE\nM\nG\nW\nO\n(b) HiSGT\nFigure 3: t-SNE visualizations of input embeddings. Each point represents a medical code, colored by ICD category.\nHALO (left) and HiSGT (right) show distinct clustering behaviors, where HALO presents more scattered clusters while\nHiSGT exhibits more compact and category-level clusters. The legend represents the first character of ICD chapters,\ne.g., ‘R’ corresponds to ‘Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified’.\n8\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nTable 5: Ablation Study of HiSGT Components. Each row adds a different form of embedding or consistency\nconstraint to a base Transformer. A concurrent work, MEDTOK [Su et al., 2025], (like our variant ‘+ Hierarchy &\nSemantic Embds’) improves upon the code-only baseline but cannot match our full model’s fidelity, highlighting the\nimportance of consistency regularization.\nMethods\nMIMIC-III\nMIMIC-IV\nUnigram\nBigram\nSeq Bigram\nDimWise\nUnigram\nBigram\nSeq Bigram\nDimWise\nBase (Code Only)\n0.956\n0.898\n0.821\n0.964\n0.965\n0.929\n0.910\n0.975\n+ Hierarchical Embd\n0.967\n0.914\n0.845\n0.972\n0.983\n0.950\n0.941\n0.988\n+ Semantic Embd\n0.964\n0.913\n0.836\n0.971\n0.982\n0.957\n0.948\n0.985\n+ Hierarchical & Semantic Embds\n0.976\n0.927\n0.859\n0.984\n0.987\n0.960\n0.953\n0.989\nSimilar to MEDTOK\n+ Hierarchical Embd & Consistency\n0.973\n0.926\n0.844\n0.967\n0.984\n0.949\n0.933\n0.986\n+ Semantic Embd & Consistency\n0.967\n0.912\n0.847\n0.966\n0.983\n0.952\n0.942\n0.988\n+ Both Embds & Consistencies\n0.984\n0.949\n0.879\n0.976\n0.989\n0.967\n0.948\n0.989\nOur Full HiSGT\nlayer to capture intra-visit correlations (with parameters that scale quadratically with vocabulary size), making it easier\nto scale to very large code vocabularies. Utility evaluations further confirm that HiSGT-generated data enable the\ntraining of disease classification models that generalize well to real patient records. Although our experiments indicate\nthat the classification performance using synthetic data is on par with SOTA baselines like HALO, HiSGT achieves\nthis while also exhibiting higher fidelity. In our setting, the synthetic labels are generated as part of the conditional\nprocess, ensuring that the joint distribution of features and labels is preserved. This self-contained generation process\nminimizes the need for post-hoc label assignment and contributes to a more natural alignment between the synthetic\nrecords and their clinical interpretations. The privacy evaluation underscores a crucial trade-off between fidelity, utility,\nand privacy. Models such as LSTM, EVA, and SynTEG offer strong privacy protection—as evidenced by near-chance\nmembership inference accuracy and low attribute inference F1-scores—but suffer from poor fidelity and limited utility.\nIn contrast, HiSGT strikes an optimal balance by higher fidelity and utility with only a moderate increase in privacy\nrisk. This balance is particularly valuable for clinical applications where preserving the nuanced relationships in patient\ndata is as important as ensuring confidentiality. While this study focuses on integrating hierarchical and semantic\nrepresentations for ICD diagnosis codes, the proposed paradigm is inherently generalizable to other medical code type\nincluding procedure and medication codes.\nTo evaluate the contributions of hierarchical embeddings, semantic embeddings, and consistency constraints, we conduct\nan ablation study (Table 5) to help analyze the different components in HiSGT. Starting with a base Transformer trained\nsolely on medical codes, without structured knowledge, we observe that it struggles to capture co-occurrence and\nlongitudinal dependencies. Introducing hierarchical or semantic embeddings leads to gains in all four metrics, especially\nBigram and Seq Bigram. The combined use of hierarchical and semantic embeddings yields further improvements. We\nnotice a concurrent approach, MEDTOK [Su et al., 2025], that also explores relational structure and text descriptions for\nmedical code representations. However, its methodology differs significantly: MEDTOK trains a tokenizer with multiple\nloss functions to improve code embeddings for downstream tasks, whereas HiSGT not only encodes hierarchical and\nsemantic knowledge but also enforces consistency constraints during generation. In our ablation study, the closest\nequivalent to MEDTOK is the ‘+ Hierarchical & Semantic Embds’ variant. While this model improves fidelity compared\nto a base Transformer, it remains inferior to HiSGT’s full model due to the lack of consistency regularization.\nDespite these promising results, several limitations warrant further investigation. First, while HiSGT successfully\nintegrates hierarchical and semantic information, its performance is still partially constrained by the quality and\ngranularity of the underlying clinical ontologies and pre-trained language models. For example, even though we\nemploy ClinicalBERT—which is pre-trained on the MIMIC dataset—it is still limited in capturing the precise semantic\ncontext. Our preliminary attempts to use contrastive learning to refine the semantic embeddings incurred significant\ncomputational overhead while yielding only marginal improvements. Future work should consider exploring alternative\nstrategies or more domain-specific pre-training to further enhance semantic representation. Second, our utility evaluation\nwas based on a multi-class classification paradigm for disease classification. While this task is representative, it may not\nfully capture the diverse clinical decision-making processes. Evaluating HiSGT on a broader range of downstream\ntasks—such as risk stratification or temporal event prediction—could provide a more comprehensive assessment of its\nclinical utility. Finally, scalability poses an important challenge. As the complexity of EHR data increases—with richer\nfeature sets and longer patient histories—the computational cost of incorporating detailed hierarchical and semantic\n9\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nrepresentations may become prohibitive. Even after optimizing the model architecture to include the most essential\nembeddings and consistency regularization terms, scalability remains a potential issue. Future research should focus on\noptimizing training strategies and exploring more efficient model architectures to ensure that HiSGT can be applied\neffectively in large-scale settings.\nIn conclusion, our findings suggest that HiSGT represents a significant step forward in synthetic EHR data generation\nby balancing high clinical fidelity, robust privacy protection, and practical utility. By leveraging structured domain\nknowledge through hierarchical and semantic embeddings, HiSGT not only produces synthetic data that closely mirror\nreal patient records but also facilitates downstream clinical modeling tasks. Importantly, the use of hierarchically\nand semantically meaningful code embeddings will also lead to more meaningful patient representations when codes\nare aggregated, e.g. for clustering purposes. This work opens new avenues for developing advanced synthetic data\ngenerators that can ultimately support a wide range of healthcare applications—from model benchmarking and data\naugmentation to collaborative research in settings where access to real patient data is limited.\n4\nMethods\n4.1\nProblem Formulation\nElectronic health record data naturally exhibits a multi-level structure comprising patients, visits and clinical events.\nLet X, V and E denote the sets of patients, visits and clinical events, respectively. For each patient x ∈X, the medical\nhistory is represented as a sequence of visits x = {v1, v2, . . . , vnx}, where nx is the number of visits. Each visit vi\ncomprises a set of medical codes representing diagnoses, procedures, and other clinical events: vi = {ei\n1, ei\n2, . . . , ei\nmi},\nwith mi denoting the number of medical codes recorded during visit vi. The objective of synthetic EHR generation\nis to model the joint distribution of patient trajectories while preserving both intra-visit (within-visit) and inter-visit\n(longitudinal) dependencies. The model estimates the conditional probability of each clinical event given past visits and\npreviously observed events within the same visit:\nL(Θ) =\nX\nx∈X\nnx\nX\ni=1\nmi\nX\nj=1\nlog P\n\u0010\nei\nj\n\f\f\f v1, · · · , vi−1\n|\n{z\n}\ninter-visit\n, ei\n1, · · · , ei\nj−1\n|\n{z\n}\nintra-visit\n; Θ\n\u0011\n(1)\nwhere Θ represents the model parameters. The generative model seeks to maximize this likelihood to ensure that the\ngenerated sequences closely resemble real-world clinical patterns.\nOnce trained, a generative model can synthesize realistic EHR trajectories by predicting medical events based on past\nevents:\nei\nj ∼P\n\u0010\nei\nj\n\f\f\f v1, · · · , vi−1, ei\n1, · · · , ei\nj−1\n\u0011\n(2)\nAt each step j, the predicted event ei\nj is appended to the sequence, and the updated sequence is fed back into the model to\ngenerate the next event. The process continues until a predefined maximum length L is reached or an ‘END_RECORD’\ntoken is generated, signaling the completion of a synthetic patient record.\n4.2\nDataset and Baseline Details\nWe conduct experiments on two widely used real-world EHR datasets: MIMIC-III [Johnson et al., 2016] and MIMIC-\nIV [Johnson et al., 2023]. To construct patient-level longitudinal records, we first extract hospital admissions and ICD-9\ndiagnosis codes from both datasets. Admissions are sorted chronologically based on timestamps to maintain the correct\ntemporal sequence of patient visits. Diagnosis codes are then aggregated at the hospital visit level, and visits are grouped\nby patient index, forming structured patient records that capture the progression of medical events over time. For utility\nevaluation, we map ICD codes to phenotype categories using a predefined benchmark taxnomy [Harutyunyan et al.,\n2019]. Each patient is assigned one or more phenotype labels (up to 25 labels) based on their recorded diagnoses. The\ndataset is then tokenized by constructing a vocabulary that includes all unique ICD codes in the dataset, 25 phenotype\nlabel tokens, and five special tokens used for sequence structuring. These special tokens: START_RECORD indicates the\nbeginning of a patient record, END_LABEL denotes the transition from phenotype labels to visit sequences, END_VISIT\nmarks the end of a hospital visit, END_RECORD signifies the completion of a patient record, and ‘PADDING’ ensures\nstandardized sequence lengths. After preprocessing, MIMIC-III consists of 46,520 patients with 6,984 unique ICD\ndiagnosis codes and 7,012 unique tokens, while MIMIC-IV includes 124,525 patients with 9,072 unique ICD codes and\n9,102 unique tokens. Both datasets contain structured longitudinal patient records with multiple hospital visits, making\nthem well-suited for evaluating synthetic EHR generation models.\nTo assess the effectiveness of HiSGT, we compare it against six state-of-the-art (SOTA) generative models used for\nEHR synthesis. LSTM [Lee, 2018] is a recurrent neural network that models sequential dependencies. GPT [Radford\n10\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\net al., 2019] applies a transformer-based autoregressive approach for next-token prediction. EVA [Biswal et al., 2021]\nuses a variational autoencoder (VAE) to learn latent representations of EHR sequences. SynTEG [Zhang et al., 2021]\nemploys a GAN-based framework to generate synthetic patient visits. HALO-Coarse [Theodorou et al., 2023] treats\neach visit as a single token, modeling longitudinal dependencies at a coarse level, while HALO [Theodorou et al., 2023]\nrefines this approach by representing visits at a more granular level.\n4.3\nExperimental Setting\nWe conduct experiments using the MIMIC-III v1.4 and MIMIC-IV v2.2 datasets. The data is split into 80%-20% for\ntraining and testing, with an additional 90%-10% split within the training set for validation. For HiSGT, we use a\nsix-layer Transformer architecture with eight attention heads and a hidden size of 384. Training is performed using the\nAdam optimizer with a learning rate of 10−4, a batch size of 48, and a dropout rate of 0.1. The input sequence length is\nset to 768 tokens, and training is run for 100 epochs with early stopping (patience = 10) based on validation loss. The\nbest model checkpoint is saved according to minimum validation loss. The model is implemented in Python 3.13.0\nusing PyTorch 2.5.0+cu121, along with scikit-learn 1.5.2, NumPy 2.1.2, and transformers 4.46.2. All experiments\nare conducted on a single NVIDIA H100 GPU (80GB VRAM). The HiSGT source code is publicly available at:\nhttps://github.com/jameszhou-gl/HiSGT.\n4.4\nHiSGT Framework\nHiSGT is a Transformer-based generative model designed for synthetic EHR data generation. To enhance clinical\nfidelity, the model incorporates hierarchical and semantic embeddings to enrich medical code representations, as\nillustrated in Figure 1.\n4.4.1\nTokenization\nThe tokenization process in HiSGT structures raw EHR data into a sequence of discrete tokens while preserving temporal\ndependencies betweeen medical events. We extract longitudinal patient records from the MIMIC-III and MIMIC-IV\ndatasets, using key tables such as patients.csv, admissions.csv and diagnoses_icd.csv. Each patient’s health\ntimeline consists of one or more hospital visits, with each visit containing diagnosis codes that describe the patient’s\nconditions. To ensure an accurate representation of patient trajectories, the raw data undergoes preprocessing, where\nhospital visits are first sorted chronologically to maintain the correct sequence of admissions. Diagnosis codes are\nthen grouped by patient ID, ensuring that all events for a single patient remain together. Additionally, each patient is\nassigned phenotype labels derived from their diagnoses, according to predefined clinical mappings. Once preprocessed,\nthe dataset is tokenized by constructing a vocabulary that includes all unique ICD codes, along with phenotype label\ntokens and special tokens indicating sequence boundaries. As shown in Figure 1(a), each patient sequence follows a\nstructured format. It begins with a START_RECORD token, marking the initialization of a new patient record. This is\nfollowed by phenotype label tokens that summarize the patient’s conditions, such as Hypertension with complications,\nPneumonia and Chronic kidney disease. These label tokens play a crucial role in conditioning the generation of\nsubsequent visits, ensuring consistency with standard utility evaluations. The END_LABEL token separates phenotype\nlabels from individual hospital visits. Each visit is represented as a collection of medical event tokens corresponding to\nICD codes, e.g., ICD9-40301 (Hypertensive chronic kidney disease) and ICD9-486 (Pneumonia, organism unspecified).\nTo preserve temporal dependencies, events from the same visit are grouped and concluded with an END_VISIT token.\nThe sequence may progress through multiple visits, maintaining the correct ordering of hospital admissions. Finally, the\nEND_RECORD token indicates the end of a patient’s medical history. Since patient trajectories vary in length, PADDING\ntokens are appended where necessary to ensure consistent input dimensions.\n4.4.2\nHierarchical & Semantic Embeddings\nHiSGT enriches medical code representations by incorporating hierarchical and semantic embeddings, as shown in\nFigure 1(b). These embeddings capture structured relationships among medical codes and encode clinical meanings\nbeyond raw code-only identifiers.\nHierarchical Embeddings. Medical coding systems such as ICD follow structured taxonomies with multiple levels\nof granularity, including chapter, section, category, subcategory_1, subcategory_2. For example, in ICD-10, the code\n‘N18.32’ is nested within broader categories such as ‘N18.3’, ‘N18’, ‘N17-N19’ and ‘N00-N99’. To capture these\nrelationships, we construct a hierarchical graph based on ICD-10 version G = (V, E), where each node e ∈V represents\nan ICD-10 code and edges\n\u0000u →v\n\u0001\n∈E capture parent–child or sibling relationships extracted from the ICD ontology.\nThis graph is represented by an adjacency matrix A ∈{0, 1}|V|×|V|, with Au,v = 1 if u and v share a hierarchical link.\n11\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nEach code e is initially represented by an identity feature vector ze ∈R|V|. A graph neural network (GNN) propagates\ninformation across this hierarchy to produce clinically meaningful embeddings: he = GNN(ze, G), where he is the\nlearned hierarchical embedding for each code e. The GNN is trained using adjacency reconstruction, ensuring that\nembeddings for related medical codes remain close in the latent space: ˆAu,v = σ\n\u0000h⊤\nu hv\n\u0001\n; where σ(·) is a sigmoid\nactivation function. The reconstruction loss, defined as Lrecon = ∥ˆA −A∥with mean-squared error, encourages\nstructurally related codes (e.g., ‘N18’ and ‘N18.5’) to have similar hierarchical embeddings, while unrelated codes\nremain distinct. The resulting hierarchical embedding Zh is then incorporated into the final input representation.\nSemantic Embeddings. Beyond hierarchical relationships, medical codes are accompanied by textual descriptions\nthat provide additional clinical context. These descriptions are essential for distinguishing between related conditions,\nprocedures, and medications. For example, the ICD-10 code ‘E11.65’ specifies ‘Type 2 diabetes mellitus with\nhyperglycemia’ whereas ‘Z13.6’ specifies ‘screening for cardiovasscular disorders’. Ignoring these textual descriptions\ncan lead to synthetically generated records that fail to reflect meaningful clinical distinctions. HiSGT employs\nClinicalBERT [Alsentzer et al., 2019], a pre-trained clinical language model, to encode these descriptions into dense\nsemantic representations. Given the text description T(e) corresponding to medical code e, ClinicalBERT generates\na semantic embedding: se = BERT(T(e)). Since ClinicalBERT outputs embeddings in a fix and high-dimensional\nspace, we apply a linear transformation to project them into the model’s latent space: Zs = Wsse + bs; where\nWs ∈Rds×d and bs ∈Rd are learnable parameters.\nFinal Input Representation. The final input representation Z integrates three components:\nZ = Zt + Zh + Zs\n(3)\nHere, Zt is the standard token embedding obtained from the embedding layer in Transformer. Zh is the hierarchical\nembedding, derived from the GNN trained to capture relationships among medical codes. Zs is the semantic embedding,\nextracted from the ClinicalBERT. This enriched representation allows HiSGT to generate synthetic EHR sequences that\nare bot structure-aware and clinically meaningful.\n4.4.3\nModel Structure\nHiSGT follows a decoder-only Transformer architecture [Radford et al., 2019], generating synthetic EHR sequences\nin an auto-regressive (AR) manner. The model processes tokenized sequences and predicts the next medical event\nbased on prior context. As shown in Figure 1(c), the input to HiSGT consists of three components: a code token\nembedding Zt, a hierarchical embedding Zh, and a semantic embedding Zs. These embeddings are summed together\nto form the input embedding Z and positional embeddings are added further to ensure the model understands the\nsequential nature of patient trajectories. The processed embeddings pass through six stacked Transformer decoder\nblocks, each comprising a masked multi-head self-attention mechanism and a feed-forward network. The attention\nmechanism selectively attends to relevant past events while maintaining temporal dependencies across hospital visits. A\ncausal mask ensures that the model generates future events based only on past visits, preserving a realistic medical\nprogression. Following the self-attention layer, the feed-forward network refines token representations through nonlinear\ntransformations. The output of each decoder block is iteratively refined through multiple layers to progressively learn\ncontextual dependencies. At the final stage, the hidden states of the Transformer blocks are projected onto different\noutput heads. The next medical event in the sequence is predicted through the code head, which computes probabilities\nover the medical code vocabulary. To ensure that generated sequences remain clinically structured and meaningful,\nHiSGT introduces two additional regularization heads. The hierarchical consistency head predicts a hierarchy-aware\nrepresentation, reinforcing the structured taxonomic relationships within the ICD system. The semantic consistency\nhead maps generated medical codes back into the semantic space, ensuring that their meaning aligns with real-world\nclinical descriptions. While the primary objective is next-code prediction, the hierarchical and semantic consistency\nheads serve as essential constraints that guide the model toward more realistic and interpretable patient trajectories.\n4.4.4\nTraining and Inference\nHiSGT is trained using a multi-objective optimization approach that balances next-code prediction with hierarchical and\nsemantic consistency constraints. The next-code prediction task is framed as a standard cross-entropy loss, where the\nmodel learns to predict the next code given the prior sequence of tokens. Besides, we introduce two auxiliary loss terms\nto ensure that generated sequences adhere to real-world medical knowledge. HiSGT employs two lightweight auxiliary\nheads: a hierarchy head and a semantic head. The hierarchy head maps the hidden state of each generated token back to\nthe hierarchical embedding space, ensuring that medical codes respect their taxonomic relationships. Similarly, the\nsemantic head projects hidden states onto the semantic embedding space, aligning generated codes with clinically\nmeaningful representations obtained from ClinicalBERT. Both consistency objectives are formulated as mean squared\nerror (MSE) losses, minimizing the discrepancy between predicted embeddings and their original input counterparts.\n12\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nThese auxiliary heads act as implicit regularization mechanisms, guiding the model toward generating sequences that\nmaintain both structural coherence and semantic fidelity. Once trained, HiSGT generates synthetic patient trajectories\nin an AR manner. Given an initial START_RECORD token, the model first generates phenotype labels, summarizing the\npatient’s overall conditions. Upon detecting the special token END_LABEL, it proceeds to generate medical codes for\neach visit while preserving temporal dependencies. The process continues until the model outputs the END_RECORD\ntoken, marking the valid completion of a synthetic patient sequence.\n4.5\nComparison of HiSGT to Related Work\nEHR Data Generation. The generation of synthetic EHRs have evolved across several categories [Chen et al., 2024].\nEearly rule-based methods rely on predefined clinical guidelines to simulate patient records [Buczak et al., 2010,\nFranklin et al., 2014, McLachlan et al., 2018]. While these methods ensure clinically interpretability, they fail to capture\ncomplex correlations in real-world EHR data. GAN-based approaches model the distribution of real EHR data through\nadversarial training. A generator synthesizes patient records, while a discriminator distinguishes between real and\nsynthetic samples [Zhang et al., 2020, Yale et al., 2020, Wang et al., 2024, Li et al., 2023, Baowaly et al., 2019, Yang\net al., 2019, Lee et al., 2020, Yan et al., 2021, Torfi and Fox, 2020, Kuo et al., 2022]. VAE-based methods learn a\nlatent representation of EHR data and generate new samples by sampling from the learned distribution [Biswal et al.,\n2021, Sun et al., 2024]. However, both of GANs and VAEs struggle with rare events in high-dimensional EHR data\nand capturing fine-grained longitudinal dependencies in EHR data. Recently, Transformer-based generative models\ngained traction due to their ability to capture long-range dependencies in patient records. HALO [Theodorou et al.,\n2023] applies a self-attention mechanisms to model the sequential structure of medical codes, significantly expanding\nthe vocabulary of synthetic data from fewer than 100 tokens to over thousands. However, existing methods rely solely\non modeling raw code sequences. As result, the generated records lack high clinical fidelity. In this work, we propose a\nTransformer-based framework with two inductive biases of hierarchical and semantic information.\nGenerative Transformer. The Transformer architecture, introduced by [Vaswani et al., 2017], has revolutionized\nsequence modeling by effectively capturing long-range dependencies through self-attention mechanisms. In natural\nlanguage processing (NLP), generative Transformer models such as GPTs series [Radford et al., 2019, Brown et al.,\n2020] have demonstrated SOTA performance in text generation. These models predict tokens sequentially while\nconditioning on all previous tokens, making them highly effective for learning complex dependencies. Beyond NLP,\nTransformers have been successfully adapted to other domains including computer vision [Dosovitskiy et al., 2021]\nand multimodal learning [Hurst et al., 2024, Yang et al., 2023]. In healthcare, Transformer-based models for synthetic\nEHR generation typically treat patient trajectories as sequences of discrete medical codes [Theodorou et al., 2023, Pang\net al., 2024]. Although they enable scaling to synthetic data with thousands of unique tokens, they model medical codes\npurely as discrete symbols, thereby the generated EHR lack clinical fidelity. Our work extends Transformer-based\nEHR generation by explicitly incorporating structured knowledge into the generative process—leveraging hierarchical\nrepresentations and semantic embeddings inherent in clinical coding systems.\nAdditional Information\nData Availability\nThe MIMIC-III [Johnson et al., 2016] and MIMIC-IV [Johnson et al., 2023] electronic health record (EHR) datasets\nused in this study are publicly available. Access to these datasets requires completion of the necessary training and\ncredentialing process via PhysioNet. Detailed information on obtaining access can be found at https://physionet.\norg.\nCode Availability\nThe source code for all experiments, including dataset preprocessing, model implementation, training procedures,\nsynthetic data generation, and evaluation, is publicly available at https://github.com/jameszhou-gl/HiSGT.\nReferences\nAshish K Jha, Catherine M DesRoches, Eric G Campbell, Karen Donelan, Sowmya R Rao, Timothy G Ferris, Alexandra\nShields, Sara Rosenbaum, and David Blumenthal. Use of electronic health records in us hospitals. New England\nJournal of Medicine, 360(16):1628–1638, 2009.\n13\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nMartin R Cowie, Juuso I Blomster, Lesley H Curtis, Sylvie Duclaux, Ian Ford, Fleur Fritz, Samantha Goldman, Salim\nJanmohamed, Jörg Kreuzer, Mark Leenay, et al. Electronic health records to facilitate clinical research. Clinical\nResearch in Cardiology, 106:1–9, 2017.\nCao Xiao and Jimeng Sun. Introduction to deep learning for healthcare. Springer Nature, 2021.\nCao Xiao, Edward Choi, and Jimeng Sun. Opportunities and challenges in developing deep learning models using\nelectronic health records data: a systematic review. Journal of the American Medical Informatics Association, 25\n(10):1419–1428, 2018.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin\nMoody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database.\nScientific data, 3(1):1–9, 2016.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng\nHao, Benjamin Moody, Brian Gow, et al. Mimic-iv, a freely accessible electronic health record dataset. Scientific\ndata, 10(1):1, 2023.\nAldren Gonzales, Guruprabha Guruswamy, and Scott R Smith. Synthetic data in health care: A narrative review. PLOS\nDigital Health, 2(1):e0000082, 2023.\nDaniel McDuff, Theodore Curran, and Achuta Kadambi. Synthetic data in healthcare. arXiv preprint arXiv:2304.03243,\n2023.\nBoris van Breugel, Tennison Liu, Dino Oglic, and Mihaela van der Schaar. Synthetic data in biomedicine via generative\nartificial intelligence. Nature Reviews Bioengineering, pages 1–14, 2024.\nGhadeer O Ghosheh, Jin Li, and Tingting Zhu. A survey of generative adversarial networks for synthesizing structured\nelectronic health records. ACM Computing Surveys, 56(6):1–34, 2024.\nEdward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart, and Jimeng Sun. Generating multi-label\ndiscrete patient records using generative adversarial networks. In Machine learning for healthcare conference, pages\n286–305. PMLR, 2017.\nZiqi Zhang, Chao Yan, Thomas A Lasko, Jimeng Sun, and Bradley A Malin. Synteg: a framework for temporal\nstructured electronic health data simulation. Journal of the American Medical Informatics Association, 28(3):\n596–604, 2021.\nAmirsina Torfi and Edward A Fox. Corgan: correlation-capturing convolutional generative adversarial networks for\ngenerating synthetic healthcare records. In The thirty-third international flairs conference, 2020.\nLimeng Cui, Siddharth Biswal, Lucas M Glass, Greg Lever, Jimeng Sun, and Cao Xiao. Conan: complementary pattern\naugmentation for rare disease detection. In Proceedings of the AAAI conference on artificial intelligence, volume 34,\npages 614–621, 2020.\nZiqi Zhang, Chao Yan, Diego A Mesa, Jimeng Sun, and Bradley A Malin. Ensuring electronic medical record simulation\nthrough better training, modeling, and evaluation. Journal of the American Medical Informatics Association, 27(1):\n99–108, 2020.\nSina Rashidian, Fusheng Wang, Richard Moffitt, Victor Garcia, Anurag Dutt, Wei Chang, Vishwam Pandya, Janos\nHajagos, Mary Saltz, and Joel Saltz. Smooth-gan: towards sharp and smooth synthetic ehr data generation. In\nArtificial Intelligence in Medicine: 18th International Conference on Artificial Intelligence in Medicine, AIME 2020,\nMinneapolis, MN, USA, August 25–28, 2020, Proceedings 18, pages 37–48. Springer, 2020.\nNicholas I-Hsien Kuo, Mark N Polizzotto, Simon Finfer, Federico Garcia, Anders Sönnerborg, Maurizio Zazzi, Michael\nBöhm, Rolf Kaiser, Louisa Jorm, and Sebastiano Barbieri. The health gym: synthetic health-related datasets for the\ndevelopment of reinforcement learning algorithms. Scientific data, 9(1):693, 2022.\nSiddharth Biswal, Soumya Ghosh, Jon Duke, Bradley Malin, Walter Stewart, Cao Xiao, and Jimeng Sun. Eva:\nGenerating longitudinal electronic health records using conditional variational autoencoders. In Machine Learning\nfor Healthcare Conference, pages 260–282. PMLR, 2021.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https:\n//api.semanticscholar.org/CorpusID:13756489.\nBrandon Theodorou, Cao Xiao, and Jimeng Sun. Synthesize high-dimensional longitudinal electronic health records\nvia hierarchical autoregressive language model. Nature communications, 14(1):5305, 2023.\nZeljko Kraljevic, Dan Bean, Anthony Shek, Rebecca Bendayan, Harry Hemingway, Joshua Au Yeung, Alexander\nDeng, Alfred Baston, Jack Ross, Esther Idowu, et al. Foresight—a generative pretrained transformer for modelling\n14\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nof patient timelines using electronic health records: a retrospective modelling study. The Lancet Digital Health, 6(4):\ne281–e290, 2024.\nChao Pang, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S Kalluri, Elise L Minto, Jason Patterson,\nLinying Zhang, George Hripcsak, Gamze Gürsoy, Noémie Elhadad, et al. Cehr-gpt: Generating electronic health\nrecords with chronological patient timelines. arXiv preprint arXiv:2402.04400, 2024.\nPawel Renc, Yugang Jia, Anthony E Samir, Jaroslaw Was, Quanzheng Li, David W Bates, and Arkadiusz Sitek. Zero\nshot health trajectory prediction using transformer. NPJ Digital Medicine, 7(1):256, 2024.\nWorld Health Organization et al. International classification of diseases:[9th] ninth revision, basic tabulation list with\nalphabetic index. World Health Organization, 1978.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott.\nPublicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.\nCristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch. Real-valued (medical) time series generation with recurrent\nconditional gans. arXiv preprint arXiv:1706.02633, 2017.\nHrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan. Multitask learning and\nbenchmarking with clinical time series data. Scientific data, 6(1):96, 2019.\nMauro Giuffrè and Dennis L Shung. Harnessing the power of synthetic data in healthcare: innovation, application, and\nprivacy. NPJ digital medicine, 6(1):186, 2023.\nXiaorui Su, Shvat Messica, Yepeng Huang, Ruth Johnson, Lukas Fesser, Shanghua Gao, Faryad Sahneh, and Marinka\nZitnik. Multimodal medical code tokenizer. arXiv preprint arXiv:2502.04397, 2025.\nScott H Lee. Natural language generation for electronic health records. NPJ digital medicine, 1(1):63, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nXingran Chen, Zhenke Wu, Xu Shi, Hyunghoon Cho, and Bhramar Mukherjee. Generating synthetic electronic health\nrecord (ehr) data: A review with benchmarking. arXiv preprint arXiv:2411.04281, 2024.\nAnna L Buczak, Steven Babin, and Linda Moniz. Data-driven approach for creating synthetic electronic medical\nrecords. BMC medical informatics and decision making, 10:1–28, 2010.\nJessica M Franklin, Sebastian Schneeweiss, Jennifer M Polinski, and Jeremy A Rassen. Plasmode simulation for the\nevaluation of pharmacoepidemiologic methods in complex healthcare databases. Computational statistics & data\nanalysis, 72:219–226, 2014.\nScott McLachlan, Kudakwashe Dube, Thomas Gallagher, Bridget Daley, Jason Walonoski, et al. The aten framework\nfor creating the realistic synthetic electronic health record. 2018.\nAndrew Yale, Saloni Dash, Ritik Dutta, Isabelle Guyon, Adrien Pavao, and Kristin P Bennett. Generation and evaluation\nof privacy preserving synthetic health data. Neurocomputing, 416:244–255, 2020.\nWenjie Wang, Pengfei Tang, Jian Lou, Yuanming Shao, Lance Waller, Yi-an Ko, and Li Xiong. Igamt: Privacy-\npreserving electronic health record synthesization with heterogeneity and irregularity. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 38, pages 15634–15643, 2024.\nJin Li, Benjamin J Cairns, Jingsong Li, and Tingting Zhu. Generating synthetic mixed-type longitudinal electronic\nhealth records for artificial intelligent applications. NPJ Digital Medicine, 6(1):98, 2023.\nMrinal Kanti Baowaly, Chia-Ching Lin, Chao-Lin Liu, and Kuan-Ta Chen. Synthesizing electronic health records using\nimproved generative adversarial networks. Journal of the American Medical Informatics Association, 26(3):228–241,\n2019.\nFan Yang, Zhongping Yu, Yunfan Liang, Xiaolu Gan, Kaibiao Lin, Quan Zou, and Yifeng Zeng. Grouped correlational\ngenerative adversarial networks for discrete electronic health records. In 2019 IEEE International Conference on\nBioinformatics and Biomedicine (BIBM), pages 906–913. IEEE, 2019.\nDongha Lee, Hwanjo Yu, Xiaoqian Jiang, Deevakar Rogith, Meghana Gudala, Mubeen Tejani, Qiuchen Zhang, and\nLi Xiong. Generating sequential electronic health records using dual adversarial autoencoder. Journal of the American\nMedical Informatics Association, 27(9):1411–1419, 2020.\nChao Yan, Ziqi Zhang, Steve Nyemba, and Bradley A Malin. Generating electronic health records with multiple data\ntypes and constraints. In AMIA annual symposium proceedings, volume 2020, page 1335, 2021.\nHongda Sun, Hongzhan Lin, and Rui Yan. Collaborative synthesis of patient records through multi-visit health state\ninference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19044–19052, 2024.\n15\n\n\nGenerating Clinically Realistic EHR Data via a Hierarchy and Semantics-Guided Transformer\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL\nhttps://arxiv.org/abs/2005.14165.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\nis worth 16x16 words: Transformers for image recognition at scale, 2021. URL https://arxiv.org/abs/2010.\n11929.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila\nWelihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of\nlmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023.\n16\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20719v1.pdf",
    "total_pages": 16,
    "title": "Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer",
    "authors": [
      "Guanglin Zhou",
      "Sebastiano Barbieri"
    ],
    "abstract": "Generating realistic synthetic electronic health records (EHRs) holds\ntremendous promise for accelerating healthcare research, facilitating AI model\ndevelopment and enhancing patient privacy. However, existing generative methods\ntypically treat EHRs as flat sequences of discrete medical codes. This approach\noverlooks two critical aspects: the inherent hierarchical organization of\nclinical coding systems and the rich semantic context provided by code\ndescriptions. Consequently, synthetic patient sequences often lack high\nclinical fidelity and have limited utility in downstream clinical tasks. In\nthis paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT),\na novel framework that leverages both hierarchical and semantic information for\nthe generative process. HiSGT constructs a hierarchical graph to encode\nparent-child and sibling relationships among clinical codes and employs a graph\nneural network to derive hierarchy-aware embeddings. These are then fused with\nsemantic embeddings extracted from a pre-trained clinical language model (e.g.,\nClinicalBERT), enabling the Transformer-based generator to more accurately\nmodel the nuanced clinical patterns inherent in real EHRs. Extensive\nexperiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGT\nsignificantly improves the statistical alignment of synthetic data with real\npatient records, as well as supports robust downstream applications such as\nchronic disease classification. By addressing the limitations of conventional\nraw code-based generative models, HiSGT represents a significant step toward\nclinically high-fidelity synthetic data generation and a general paradigm\nsuitable for interpretable medical code representation, offering valuable\napplications in data augmentation and privacy-preserving healthcare analytics.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}