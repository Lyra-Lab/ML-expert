{
  "id": "arxiv_2502.20823v1",
  "text": "Can We Simplify Slide-level Fine-tuning of\nPathology Foundation Models?\nJiawen Li1,∗, Jiali Hu1,∗, Qiehe Sun1, Renao Yan4, Minxi Ouyang1, Tian\nGuan1, Anjia Han3, Chao He2,†, and Yonghong He1,†\n1 Shenzhen International Graduate School, Tsinghua University\n2 Department of Engineering Science, University of Oxford\n3 Department of Pathology, The First Affiliated Hospital of Sun Yat-sen University\n4 Department of Mechanical Engineering, University of Washington\nchao.he@eng.ox.ac.uk, heyh@sz.tsinghua.edu.cn\nAbstract. The emergence of foundation models in computational pathol-\nogy has transformed histopathological image analysis, with whole slide\nimaging (WSI) diagnosis being a core application. Traditionally, weakly\nsupervised fine-tuning via multiple instance learning (MIL) has been\nthe primary method for adapting foundation models to WSIs. However,\nin this work we present a key experimental finding: a simple nonlinear\nmapping strategy combining mean pooling and a multilayer perceptron,\ncalled SiMLP, can effectively adapt patch-level foundation models to\nslide-level tasks without complex MIL-based learning. Through extensive\nexperiments across diverse downstream tasks, we demonstrate the supe-\nrior performance of SiMLP with state-of-the-art methods. For instance,\non a large-scale pan-cancer classification task, SiMLP surpasses popular\nMIL-based methods by 3.52%. Furthermore, SiMLP shows strong learn-\ning ability in few-shot classification and remaining highly competitive\nwith slide-level foundation models pretrained on tens of thousands of\nslides. Finally, SiMLP exhibits remarkable robustness and transferabil-\nity in lung cancer subtyping. Overall, our findings challenge the con-\nventional MIL-based fine-tuning paradigm, demonstrating that a task-\nagnostic representation strategy alone can effectively adapt foundation\nmodels to WSI analysis. These insights offer a unique and meaningful\nperspective for future research in digital pathology, paving the way for\nmore efficient and broadly applicable methodologies.\nKeywords: Whole slide image · Fine-tuning · Foundation model\n1\nIntroduction\nWith advancements in self-supervised learning and large-scale whole-slide digiti-\nzation, foundation model-based pathology AI workflows are transforming compu-\ntational pathology [23,37]. Self-distillation across millions of pathology images\n* Contributed equally. † Corresponding authors.\narXiv:2502.20823v1  [cs.CV]  28 Feb 2025\n\n\n2\nJiawen Li et al.\nenhances region-of-interest representation [3,33,28], while contrastive learning\nwith natural language descriptions enables multimodal pathology models to in-\ntegrate semantic knowledge [18,31,7]. As foundation models evolve, their ability\nto generalize across clinical tasks is becoming a key driver of future pathology\nAI development.\nRepresenting and analyzing gigapixel-level WSIs remains a critical challenge.\nTraditional visual models pretrained on low-resolution natural images strug-\ngle as WSI encoders [11]. A common approach is to extract tissue-containing\npatches and aggregate their patch-level features for slide representation fine-\ntuning [23,19,14,35]. Given its alignment with clinical needs, weakly supervised\nmethods, particularly multiple instance learning (MIL), have become a widely\nadopted fine-tuning strategy [12,21,32,17]. While pathology foundation mod-\nels enable direct histopathology image encoding, their adaptation to WSIs still\nlargely depends on MIL or its variants [3,33,28].\nMany MIL-based fine-tuning methods integrate complex feature transfor-\nmations [2,39,4,21,13] or high-order aggregation strategies [38,36,24,35,17], yet\ntheir necessity in the foundation model era remains uncertain. In fact, the per-\nformance of MIL-based fine-tuning with foundation models is task-dependent\n[34,26]. For example, traditional MIL has outperformed more complex methods\nin metastasis detection [16] and breast morphological subtyping [9] but under-\nperformed in lung cancer subtyping [17]; meanwhile, compared to unsupervised\nstrategies [22], MIL-based fine-tuning have shown unstable generalization. Given\nthe strong features extracted by pretrained models, the advantages of complex\nfine-tuning strategies may be limited. Thus, exploring simplified fine-tuning ap-\nproaches could offer greater efficiency, deployment flexibility, and enhanced gen-\neralization in adapting foundation models to WSIs.\nIn this work, we demonstrated the feasibility of simplifying slide-level fine-\ntuning for foundation models through extensive experiments. Using a simple\ncombination of task-agnostic average pooling and a non-linear MLP, termed\nSiMLP, we seamlessly adapted foundation models to slide-level tasks. To compre-\nhensively evaluate SiMLP, we fine-tuned three representative foundation models\non six large-scale WSI classification tasks across TCGA, CPTAC, EBRAINS [20],\nand HEROHE [5] cohorts, achieving state-of-the-art performance. Few-shot ex-\nperiments on pan-cancer tasks from TCGA and CPTAC further confirmed its\nsuperior feature representation capabilities over MIL-based methods. To assess\ncompetitiveness against pretrained slide-level foundation models, we tested it on\ntwo challenging tasks in the BRACS cohort [1], where it remained highly com-\npetitive despite other models being pretrained on tens of thousands of WSIs. Fi-\nnally, transferability experiments on non-small cell lung cancer subtyping across\nthree cohorts showed that SiMLP maintains stability with minimal standard\ndeviation, making it well-suited for scaling to large external test cohorts.\n\n\nCan We Simplify Slide-level Fine-tuning of Pathology Foundation Models?\n3\nb.\nSegmentation & patching\nFoundation model\nMultiple instance learning\n(or variants)\nLinear classifier\n……\nPan-cancer \nstudy\nTumor \nsubtyping\nBiomarker \nPrediction\nTask-specific fine-tune\nFoundation model encoding\nSegmentation & patching\nFoundation model\nGlobal average pooling\nMLP classifier\n(non-linear)\nTask-agnostic\nFoundation model encoding\nFine-tune\n……\nPan-cancer \nstudy\nTumor \nsubtyping\nBiomarker \nPrediction\na.\nc.\n0.70\n0.74\n0.78\n0.82\n0.86\nUNI\nAverage balanced accuracy\n0.70\n0.74\n0.78\n0.82\n0.86\nCONCH\nAverage balanced accuracy\nLinear probe\nABMIL\nDTFD-MIL\nACMIL\nRRTMIL\nDiffMIL\nSiMLP\n0.70\n0.74\n0.78\n0.82\n0.86\nProv-GigaPath\nAverage balanced accuracy\nFig. 1. Transition of slide-level adaption in pathology foundation models. a.\nConventional fine-tuning strategy using task-specific supervised learning. b. Simplified\nfine-tuning strategy using task-agnostic pooling and nonlinear classifier (SiMLP). c.\nComparison of SiMLP and other MIL-based fine-tuning methods across three pathology\nfoundation models.\n2\nMethodology\n2.1\nWeakly supervised learning on fine-tuning slide-level tasks\nBefore the large-scale development of pathology foundation models, visual mod-\nels pretrained on natural images struggled to extract effective features from\npathology images due to their limited pathology domain understanding. Con-\nsequently, weakly supervised learning has been necessary to obtain slide repre-\nsentations from patch features (Fig.1a). Specifically, given a WSI with patch\nfeature set denoted as P = {p1, p2, ..., pn}, feature transformation F(·) and ag-\ngregation G(·) are proposed:\n( ˜p1, ˜p2, ..., ˜pn) = F(p1, p2, ..., pn),\ns = G({ ˜p1, ˜p2, ..., ˜pn}),\n(1)\nwhere F and G respectively denote vector and scalar-valued functions. MIL-\nbased fine-tuning typically follows the composition of these two functions. For\ninstance, in the classical ABMIL [8], F is the identity mapping and G is a\ngated attention mechanism; whereas in TransMIL [21], F applies a nonlinear self-\nattention transformation and G outputs a class token. Regardless of the specific\nmethod, the composite function invariably contains learnable parameters that\nmust be optimized using the slide-level labels from downstream tasks:\nˆy = Softmax(Ws),\nL(s, y) = −\nK\nX\nk=1\nykln(ˆyk),\n(2)\n\n\n4\nJiawen Li et al.\nTable 1. Slide-level classification on TCGA and CPTAC cohort in terms of\nbalanced accuracy. Best performing fine-tuning approach for each metric is bolded.\n95% CI is included in parentheses.\nApproach\nTCGA (OncoTree)\nTCGA (Pan Cancer)\nCPTAC (Pan Cancer)\n(30 classes, 2703 WSIs)\n(22 classes, 2703 WSIs)\n(12 classes, 1772 WSIs)\nCONCH [18]\nLinear probe\n0.8090 (0.8032-0.8148)\n0.8702 (0.8642-0.8763)\n0.9143 (0.9126-0.9160)\nABMIL [8]\n0.8008 (0.7877-0.8140)\n0.8539 (0.8457-0.862)\n0.8988 (0.8918-0.9059)\nDTFD-MIL [36]\n0.7770 (0.7720-0.7821)\n0.8423 (0.8381-0.8465)\n0.8970 (0.8922-0.9017)\nACMIL [38]\n0.8095 (0.8021-0.8169)\n0.8618 (0.8547-0.8690)\n0.9068 (0.9033-0.9103)\nRRTMIL [25]\n0.8221 (0.8157-0.8286)\n0.8725 (0.8584-0.8865)\n0.8116 (0.8079-0.8152)\nDiffMIL\n0.8171 (0.8089-0.8253)\n0.8720 (0.8669-0.8771)\n0.8961 (0.8916-0.9007)\nSiMLP\n0.8273 (0.8250-0.8295)\n0.8788 (0.8729-0.8847)\n0.9251 (0.9203-0.9298)\nUNI [3]\nLinear probe\n0.8295 (0.8229-0.8360)\n0.8816 (0.8780-0.8851)\n0.8997 (0.8965-0.9029)\nABMIL [8]\n0.7906 (0.7842-0.7970)\n0.8541 (0.8484-0.8598)\n0.8770 (0.8712-0.8827)\nDTFD-MIL [36]\n0.8127 (0.8090-0.8165)\n0.8560 (0.8495-0.8626)\n0.8595 (0.8286-0.8904)\nACMIL [38]\n0.8240 (0.8152-0.8329)\n0.8712 (0.8651-0.8773)\n0.8968 (0.8913-0.9023)\nRRTMIL [25]\n0.8342 (0.8198-0.8486)\n0.8720 (0.8634-0.8806)\n0.7801 (0.7713-0.7890)\nDiffMIL\n0.8346 (0.8318-0.8374)\n0.8833 (0.8772-0.8895)\n0.8790 (0.8736-0.8844)\nSiMLP\n0.8488 (0.8440-0.8537)\n0.8846 (0.8821-0.8872)\n0.9147 (0.9117-0.9176)\nProv-GigaPath [33]\nLinear probe\n0.8039 (0.7991-0.8087)\n0.8674 (0.8584-0.8764)\n0.8959 (0.8885-0.9034)\nABMIL [8]\n0.7738 (0.7669-0.7807)\n0.8389 (0.8302-0.8475)\n0.8837 (0.8804-0.8869)\nDTFD-MIL [36]\n0.7852 (0.7780-0.7924)\n0.8352 (0.8256-0.8449)\n0.8827 (0.8810-0.8844)\nACMIL [38]\n0.7996 (0.7936-0.8056)\n0.8600 (0.8478-0.8721)\n0.8947 (0.8859-0.9035)\nRRTMIL [25]\n0.8147 (0.8062-0.8233)\n0.8368 (0.8253-0.8483)\n0.7849 (0.7788-0.7909)\nDiffMIL\n0.8237 (0.8167-0.8306)\n0.8650 (0.8599-0.8701)\n0.8814 (0.8780-0.8849)\nSiMLP\n0.8247 (0.8190-0.8304)\n0.8739 (0.8643-0.8835)\n0.9109 (0.9053-0.9164)\nwhere W is a linear classifier and L represents the cross-entropy loss. While ef-\nfective, this approach yields task-dependent slide representations, limiting gen-\neralizability and robustness to distributional shifts.\n2.2\nSlide representation with task-agnostic pooling\nPathology foundation models pretrained over millions of histopathology images\nprovide the possibility of obtaining task-agnostic slide representation. For in-\nstance, by clustering patch features extracted from the foundation model, WSI\nfeatures can be represented as a combination of morphological prototypes [22].\nAdditionally, further training a slide encoder with proxy tasks based on large-\nscale patch features has been shown to be an effective aggregation strategy for\ngenerating generic slide-level features, both in visual [33,30] and multimodal\n[9,6,27] settings. Although these approaches have demonstrated promising re-\nsults, they often rely on additional signals for guidance. In contrast, a more\nstraightforward approach is to leverage pooling layers, which represent one of\nthe simplest feature aggregation methods. Pooling has been widely adopted in\nfine-tuning modules across various vision tasks and requires no additional learn-\nable parameters. Therefore, the aggregation capability of pooling-based methods\nis worth exploring as a baseline, providing a simplified solution for slide-level\nfine-tuning and validating its transferability across diverse tasks.\n\n\nCan We Simplify Slide-level Fine-tuning of Pathology Foundation Models?\n5\nTable 2. Slide-level classification on EBRAINS and HEROHE cohort in terms\nof balanced accuracy. Best performing fine-tuning approach for each metric is bolded.\n95% CI is included in parentheses.\nApproach\nEBRAINS (Subtyping)\nEBRAINS (IDH Pred.)\nHEROHE (HER2 Pred.)\n(27 classes, 649 WSIs)\n(2 classes, 208 WSIs)\n(2 classes, 149 WSIs)\nCONCH [18]\nLinear probe\n0.6391 (0.6312-0.6471)\n0.8456 (0.8281-0.8632)\n0.7578 (0.7493-0.7663)\nABMIL [8]\n0.6366 (0.6283-0.6449)\n0.8398 (0.8246-0.8549)\n0.7268 (0.6885-0.7652)\nDTFD-MIL [36]\n0.5323 (0.5251-0.5395)\n0.6871 (0.6649-0.7093)\n0.7036 (0.6707-0.7365)\nACMIL [38]\n0.6620 (0.6474-0.6766)\n0.8650 (0.8584-0.8716)\n0.7518 (0.7438-0.7598)\nRRTMIL [25]\n0.6084 (0.5875-0.6292)\n0.8325 (0.8126-0.8524)\n0.6770 (0.6379-0.7162)\nDiffMIL\n0.6628 (0.6411-0.6844)\n0.8388 (0.8293-0.8482)\n0.7433 (0.7227-0.7639)\nSiMLP\n0.6763 (0.6641-0.6884)\n0.8567 (0.8417-0.8716)\n0.7149 (0.6720-0.7578)\nUNI [3]\nLinear probe\n0.6818 (0.6728-0.6908)\n0.8879 (0.8797-0.8961)\n0.7325 (0.7144-0.7507)\nABMIL [8]\n0.6501 (0.6312-0.6690)\n0.8371 (0.8069-0.8672)\n0.6829 (0.6498-0.7161)\nDTFD-MIL [36]\n0.5843 (0.5737-0.5949)\n0.7058 (0.6884-0.7233)\n0.7090 (0.6874-0.7307)\nACMIL [38]\n0.6873 (0.6713-0.7032)\n0.8671 (0.8578-0.8764)\n0.6920 (0.6403-0.7437)\nRRTMIL [25]\n0.6189 (0.6035-0.6342)\n0.8565 (0.8495-0.8634)\n0.6776 (0.6449-0.7103)\nDiffMIL\n0.6850 (0.6694-0.7006)\n0.8263 (0.8173-0.8352)\n0.7193 (0.7084-0.7302)\nSiMLP\n0.6940 (0.6865-0.7014)\n0.8790 (0.8660-0.8919)\n0.6703 (0.6348-0.7057)\nProv-GigaPath [33]\nLinear probe\n0.6915 (0.6859-0.6971)\n0.8567 (0.8395-0.8739)\n0.6913 (0.6762-0.7064)\nABMIL [8]\n0.6717 (0.6593-0.6841)\n0.8492 (0.8390-0.8594)\n0.6869 (0.6312-0.7425)\nDTFD-MIL [36]\n0.5456 (0.4963-0.5949)\n0.7692 (0.7190-0.8193)\n0.6305 (0.6135-0.6476)\nACMIL [38]\n0.7069 (0.6916-0.7221)\n0.8646 (0.8508-0.8785)\n0.6576 (0.6326-0.6826)\nRRTMIL [25]\n0.6171 (0.6050-0.6291)\n0.8290 (0.8085-0.8496)\n0.6368 (0.5943-0.6794)\nDiffMIL\n0.7161 (0.7037-0.7285)\n0.8570 (0.8352-0.8788)\n0.7092 (0.6800-0.7384)\nSiMLP\n0.6978 (0.6841-0.7116)\n0.8726 (0.8545-0.8906)\n0.6778 (0.6487-0.7069)\n2.3\nNon-linear classification head\nUsing linear probe, a simple linear transformation, general-purpose slide repre-\nsentations can be widely adapted to various WSI-based clinical tasks. However,\nits linear nature limits its ability to effectively align representations with the\nlower-dimensional space of downstream tasks. To enhance the transferability\nof slide representations, we adopt a non-linear classifier based on a two-layer\nMLP. Notably, modern deep learning frameworks can efficiently optimize ma-\ntrix multiplications and the additional activation layer, this adjustment strikes a\nbalance between improving representation flexibility and maintaining efficiency.\nThe overall of SiMLP is shown in Fig.1b.\n3\nExperiments\n3.1\nDatasets and experimental settings\nWe conducted extensive experiments on seven large-scale datasets, including\nTCGA (OncoTree, 30 classes), TCGA (Pan Cancer, 22 classes), CPTAC\n(Pan Cancer, 12 classes), EBRAINS (Subtyping, 27 classes), EBRAINS (IDH\nPrediction, 2 classes), HEROHE (HER2 Prediction, 2 classes) and BRACS\n(Coarse-grained, 3 classes; Fine-grained 7 classes). We trained all the fine-tuning\napproaches with AdamW optimizer (learning rate: 10−4, betas=[0.9, 0.98], weight\ndecay: 10−4) and a batch size of 1 for 20 epochs. All approaches were trained on\n\n\n6\nJiawen Li et al.\n0.10\n0.30\n0.50\n0.70\n0.90\nTCGA-CANCER\nTraining labels per class\nBalanced accuracy\n1\n5\n10\n20\n50\nABMIL\nACMIL\nSiMLP\n0.30\n0.45\n0.60\n0.75\n0.90\nCPTAC-CANCER\nTraining labels per class\nBalanced accuracy\nABMIL\nACMIL\nSiMLP\n1\n5\n10\n20\n50\nFig. 2. Few-shot slide-level performance on TCGA and CPTAC cohort with\nK ∈{1, 5, 10, 20, 50} slides per class.\nTable 3. Comparison with slide-level foundation models on BRACS cohort in\nterms of balanced accuracy, ROC AUC, and weighted F1 score (reported as averages).\nBest performing approach for each metric is bolded.\nApproach\nBRACS (Coarse-grained, 3 classes)\nBRACS (Fine-grained, 7 classes)\nBal ACC\nROC AUC\nWeighted F1\nBal ACC\nROC AUC\nWeighted F1\nCHIEF [30] with Linear probe\n0.5438\n0.8195\n0.5089\n0.2732\n0.7353\n0.2506\nCHIEF [30] with Full tuning\n0.5833\n0.8249\n0.5457\n0.2780\n0.7663\n0.2665\nSiMLP with CTransPath [29]\n0.5155\n0.7433\n0.5250\n0.2518\n0.6534\n0.2955\nGigaPath [33] with Linear probe\n0.3771\n0.7298\n0.3220\n0.2289\n0.6757\n0.2393\nGigaPath [33] with Full tuning\n0.3333\n0.4409\n0.1978\n0.1429\n0.5047\n0.0335\nSiMLP with Prov-GigaPath [33]\n0.5409\n0.7419\n0.5474\n0.2516\n0.6772\n0.2959\n1 × 24GB NVIDIA 4090 with 5 fixed random seeds. Additional details of imple-\nmentation, datasets, and baselines will be available in the Github codebase.\n3.2\nSiMLP outperforms in diverse slide-level classification\nTo evaluate SiMLP across slide-level tasks, we selected three representative\npathology foundation models: CONCH [18], UNI [3], and Prov-GigaPath [33].\nWe conducted experiments on six tasks across four cohorts and performed a fair\ncomparison against linear probe, four popular MIL-based methods (ABMIL [8],\nDTFD-MIL [36], ACMIL [38], and RRT-MIL [25]), and a differential attention-\nbased MIL method (DiffMIL) that we specifically designed (Table 1-2). Over-\nall, SiMLP achieved superior performance across all three foundation models\n(81.32%, 81.52%, 80.96% in Fig.1c), demonstrating stronger adaptability than\ntask-specific weakly supervised learning. Notably, SiMLP achieved the best re-\nsults in three pan-cancer tasks, improving upon ABMIL by 3.52% and ACMIL\nby 1.83% in TCGA OncoTree classification. While SiMLP underperformed in\nHER2 prediction, the linear probe, which also uses mean pooling, performed\nwell, suggesting that task-agnostic simplified aggregation can still produce effec-\ntive representations.\n\n\nCan We Simplify Slide-level Fine-tuning of Pathology Foundation Models?\n7\n1.65%\n2.19%\n3.89%\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.92\n0.94\n0.96\n0.98\n1.00\nCPTAC-NSCLC\nRandom seed\nROC  AUC\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.76\n0.82\n0.88\n0.94\n1.00\nInternal-NSCLC\nRandom seed\nROC  AUC\n17.40%\n6.09%\n15.07%\n3.31%\nABMIL\nDiffMIL\nSiMLP\n11.58%\n6.35%\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.80\n0.84\n0.88\n0.92\n0.96\nTCGA-NSCLC\nRandom seed\nROC  AUC\nFig. 3. Robustness and transfer testing evaluation on CPTAC, TCGA, and\nin-house NSCLC cohort by sweeping 10 random seeds.\nTable 4. Ablation study with on pooling and activation functions on TCGA-\nOncoTree in tems of balanced accuracy, ROC AUC, and weighted F1 score (reported\nas averages). Best performing configuration for each metric is bolded.\nConfiguration\nBal ACC\nROC AUC\nWeighted F1\nConfiguration\nBal ACC\nROC AUC\nWeighted F1\nMean + ReLU\n0.8488\n0.9946\n0.8893\nMax + ReLU\n0.7456\n0.9927\n0.8173\nMean + GeLU\n0.8509\n0.9949\n0.8901\nMax + GeLU\n0.7140\n0.9920\n0.8027\nMean + SwigLU\n0.8054\n0.9833\n0.8533\nMax + SwigLU\n0.5871\n0.9645\n0.7049\n3.3\nSiMLP outperforms in few-shot learning classification\nTo evaluate learning efficiency and generalization with limited data, we con-\nducted few-shot classification on TCGA and CPTAC pan-cancer tasks using\nUNI (Fig.2). We trained SiMLP, ABMIL, and ACMIL with K ∈{1, 5, 10, 20, 50}\nsamples per class. The results show that SiMLP consistently outperformed other\nmethods across nearly all shot settings while exhibiting lower variance across\nrandom seeds (std. < 0.01 per shot). These results highlight that SiMLP has\npotential for screening rare and underrepresented clinical conditions.\n3.4\nSiMLP is competitive with slide-level foundation models\nWe compared SiMLP with two pretrained slide-level foundation models, CHIEF\n[30] and GigaPath [33] (Table 3), using the BRACS cohort, a challenging\nbreast cancer subtype classification dataset with coarse-grained (3-class) and\nfine-grained (7-class) tasks. CHIEF employs CTransPath [29] as its patch feature\nextractor, while GigaPath uses Prov-GigaPath. For fair comparison, we evalu-\nated SiMLP under the same patch-level foundation model, applying both linear\nprobing and full parameter fine-tuning. Results show that while SiMLP under-\nperforms CHIEF overall, it achieves higher weighted F1 scores in fine-grained\nclassification. Compared to GigaPath, SiMLP outperforms across all metrics in\nboth tasks, likely due to the high computational complexity and large parameter\nsize of GigaPath, which may hinder convergence during downstream fine-tuning.\nGiven that CHIEF and GigaPath were pretrained on tens of thousands of WSIs,\nthe competitive performance of SiMLP is particularly noteworthy.\n\n\n8\nJiawen Li et al.\n3.5\nSiMLP has a good transfer capability\nWe further evaluated the transferability across cohorts by constructing an NSCLC\nsubtype classification task using LSCC and LUAD cases from CPTAC, TCGA,\nand an in-house (IH-LUNG) cohort. We used UNI to train ABMIL, DiffMIL,\nand SiMLP on CPTAC with 10 random seeds, followed by CPTAC internal test-\ning and TCGA, IH-LUNG external testing (Fig.3). Results show that SiMLP\noutperforms other methods in internal testing and exhibits greater stability than\nboth baselines in external test sets. This highlights that SiMLP provides better\ngeneralization and robustness in transfer learning scenarios.\n3.6\nAblation study\nFinally, we conducted an ablation study on SiMLP. Specifically, we replaced\nmean pooling with max pooling and examined the effect of substituting the\nReLU activation function with GeLU and SwigLU in different combinations.\nThese modifications were evaluated on the TCGA-OncoTree task with UNI en-\ncoder (Table 4). The results show that slide representations generated using\nmax pooling perform worse than those generated with mean pooling, indicating\nthat capturing global features remains crucial for task-agnostic aggregation. Ad-\nditionally, we observed that the combination of GeLU and mean pooling led to\nimproved performance, suggesting that adjusting the non-linear classifier further\nenhances adaptation to downstream tasks.\n4\nConclusion and future direction\nIn this work we found that SiMLP, a simple fine-tuning method, enables pathol-\nogy foundation models to effectively adapt to slide-level tasks. Extensive ex-\nperiments demonstrate that SiMLP outperforms widely used MIL-based weakly\nsupervised learning, confirming its strong performance and generalization ability.\nOur findings provide four key insights for the future of computational pathol-\nogy in the foundation model era:\n1. Patch-level foundation model development remains crucial. While\nexisting pretrained encoders enhance WSI analysis, balancing data redundancy\nand model complexity is essential. For instance, ViT-Base (CONCH) performed\ncompetitively against ViT-Giant (Prov-GigaPath). We encourage future research\nto explore efficient architectures, diverse multimodal models, and improved data-\ndriven preprocessing strategies.\n2. Task-agnostic slide representation learning may be more impact-\nful than weakly supervised learning. Such representations improve gener-\nalization and stability while enabling broader applications like slide embedding\nretrieval and convenient multimodal integration.\n3. Advancing slide-level foundation models enhances clinical per-\nformance. Pretraining slide encoders on large-scale datasets not only supports\ntask-agnostic representation learning but also allows for performance improve-\nments through diverse fine-tuning strategies.\n\n\nCan We Simplify Slide-level Fine-tuning of Pathology Foundation Models?\n9\n4. Tailored weakly supervised learning remains necessary for slide-\nlevel tasks. SiMLP performs well broadly, however, weakly supervised learning\nstill holds advantages in specific tasks, highlighting its effectiveness for clinically\ntailored applications. For example, it remains valuable for biomarker prediction,\nhierarchical classification of rare diseases [10,15], and long-tailed data analysis.\nIn summary, as pathology foundation models continue to evolve, simplifying\ntraditional weakly supervised learning paradigms and pioneering a new gener-\nation of research directions will be key to further enhancing performance and\nenabling broader real-world applications in computational pathology.\n5\nAcknowledgement\nThis work was supported by the National Natural Science Foundation of China\n(NSFC) under Grant No.82430062, the Shenzhen Engineering Research Centre\nunder Grant XMHT20230115004. We thank the Jilin FuyuanGuan Food Group\nCo., Ltd for their collaboration. C.H. was also supported by the St John’s Col-\nlege, the University of Oxford, and the Royal Society (URF\\R1\\241734). The\nauthors have no competing interests to declare that are relevant to the content\nof this paper.\nReferences\n1. Brancati, N., Anniciello, A.M., Pati, P., et al.: Bracs: A dataset for breast carci-\nnoma subtyping in h&e histology images. Database 2022, baac093 (2022)\n2. Chen, R.J., Chen, C., Li, Y., et al.: Scaling vision transformers to gigapixel images\nvia hierarchical self-supervised learning. In: CVPR. pp. 16144–16155 (2022)\n3. Chen, R.J., Ding, T., Lu, M.Y., et al.: Towards a general-purpose foundation model\nfor computational pathology. Nature Medicine 30(3), 850–862 (2024)\n4. Chu, H., Sun, Q., Li, J., et al.: Retmil: Retentive multiple instance learning\nfor histopathological whole slide image classification. In: MICCAI. pp. 437–447.\nSpringer (2024)\n5. Conde-Sousa, E., Vale, J., Feng, M., et al.: Herohe challenge: predicting her2 status\nin breast cancer from hematoxylin–eosin whole-slide imaging. Journal of Imaging\n8(8), 213 (2022)\n6. Ding, T., Wagner, S.J., Song, A.H., et al.: Multimodal whole slide foundation model\nfor pathology. arXiv preprint arXiv:2411.19666 (2024)\n7. Ikezogwo, W., Seyfioglu, S., Ghezloo, F., et al.: Quilt-1m: One million image-text\npairs for histopathology. NeurIPS 36, 37995–38017 (2023)\n8. Ilse, M., Tomczak, J., Welling, M.: Attention-based deep multiple instance learning.\nIn: ICML. pp. 2127–2136. PMLR (2018)\n9. Jaume, G., Vaidya, A., Zhang, A., et al.: Multistain pretraining for slide represen-\ntation learning in pathology. In: ECCV. pp. 19–37. Springer (2024)\n10. Jin, C., Luo, L., Lin, H., et al.: Hmil: Hierarchical multi-instance learning for fine-\ngrained whole slide image classification. IEEE Transactions on Medical Imaging\n(2024)\n11. Van der Laak, J., Litjens, G., Ciompi, F.: Deep learning in histopathology: the\npath to the clinic. Nature medicine 27(5), 775–784 (2021)\n\n\n10\nJiawen Li et al.\n12. Li, B., Li, Y., Eliceiri, K.W.: Dual-stream multiple instance learning network for\nwhole slide image classification with self-supervised contrastive learning. In: CVPR.\npp. 14318–14328 (2021)\n13. Li, J., Chen, Y., Chu, H., et al.: Dynamic graph representation with knowledge-\naware attention for histopathology whole slide image analysis. In: CVPR. pp.\n11323–11332 (2024)\n14. Li, J., Cheng, J., Meng, L., et al.: Deeptree: Pathological image classification\nthrough imitating tree-like strategies of pathologists. IEEE Transactions on Med-\nical Imaging 43(4), 1501–1512 (2023)\n15. Li, J., Sun, Q., Yan, R., et al.: Diagnostic text-guided representation learning\nin hierarchical classification for pathological whole slide image. arXiv preprint\narXiv:2411.10709 (2024)\n16. Ling, X., Lei, Y., Li, J., et al.: Towards a comprehensive benchmark for pathological\nlymph node metastasis in breast cancer sections. arXiv preprint arXiv:2411.10752\n(2024)\n17. Ling, X., Ouyang, M., Wang, Y., et al.: Agent aggregator with mask denoise mech-\nanism for histopathology whole slide image analysis. In: ACM Multimedia. pp.\n2795–2803 (2024)\n18. Lu, M.Y., Chen, B., Williamson, D.F., et al.: A visual-language foundation model\nfor computational pathology. Nature Medicine 30(3), 863–874 (2024)\n19. Lu, M.Y., Williamson, D.F., Chen, T.Y., et al.: Data-efficient and weakly super-\nvised computational pathology on whole-slide images. Nature Biomedical Engi-\nneering 5(6), 555–570 (2021)\n20. Roetzer-Pejrimovsky, T., Moser, A.C., Atli, B., et al.: The digital brain tumour\natlas, an open histopathology resource. Scientific Data 9(1), 55 (2022)\n21. Shao, Z., Bian, H., Chen, Y., et al.: Transmil: Transformer based correlated mul-\ntiple instance learning for whole slide image classification. NeurIPS 34, 2136–2147\n(2021)\n22. Song, A.H., Chen, R.J., Ding, T., et al.: Morphological prototyping for unsuper-\nvised slide representation learning in computational pathology. In: CVPR. pp.\n11566–11578 (2024)\n23. Song, A.H., Jaume, G., Williamson, D.F., et al.: Artificial intelligence for digi-\ntal and computational pathology. Nature Reviews Bioengineering 1(12), 930–949\n(2023)\n24. Sun, Q., Jiang, D., Li, J., Yan, R., He, Y., Guan, T., Cheng, Z.: Nciemil: Rethinking\ndecoupled multiple instance learning framework for histopathological slide classi-\nfication. In: MIDL (2024)\n25. Tang, W., Zhou, F., Huang, S., et al.: Feature re-embedding: Towards foundation\nmodel-level performance in computational pathology. In: CVPR. pp. 11343–11352\n(2024)\n26. Vaidya, A., Chen, R.J., Williamson, D.F., et al.: Demographic bias in misdiagnosis\nby computational pathology models. Nature Medicine 30(4), 1174–1190 (2024)\n27. Vaidya, A., Zhang, A., Jaume, G., et al.: Molecular-driven foundation model for\noncologic pathology. arXiv preprint arXiv:2501.16652 (2025)\n28. Vorontsov, E., Bozkurt, A., Casson, A., et al.: A foundation model for clinical-grade\ncomputational pathology and rare cancers detection. Nature Medicine 30(10),\n2924–2935 (2024)\n29. Wang, X., Yang, S., Zhang, J., et al.: Transformer-based unsupervised contrastive\nlearning for histopathological image classification. Medical Image Analysis 81,\n102559 (2022)\n\n\nCan We Simplify Slide-level Fine-tuning of Pathology Foundation Models?\n11\n30. Wang, X., Zhao, J., Marostica, E., et al.: A pathology foundation model for cancer\ndiagnosis and prognosis prediction. Nature 634(8035), 970–978 (2024)\n31. Xiang, J., Wang, X., Zhang, X., et al.: A vision–language foundation model for\nprecision oncology. Nature pp. 1–10 (2025)\n32. Xiang, J., Zhang, J.: Exploring low-rank property in multiple instance learning for\nwhole slide image classification. In: ICLR (2023)\n33. Xu, H., Usuyama, N., Bagga, J., et al.: A whole-slide foundation model for digital\npathology from real-world data. Nature pp. 1–8 (2024)\n34. Xu, H., Wang, M., Shi, D., et al.: When multiple instance learning meets foundation\nmodels: advancing histological whole slide image analysis. Medical Image Analysis\n101, 103456 (2025)\n35. Yan, R., Sun, Q., Jin, C., et al.: Shapley values-enabled progressive pseudo bag\naugmentation for whole-slide image classification. IEEE Transactions on Medical\nImaging (2024)\n36. Zhang, H., Meng, Y., Zhao, Y., et al.: Dtfd-mil: Double-tier feature distillation\nmultiple instance learning for histopathology whole slide image classification. In:\nCVPR. pp. 18802–18812 (2022)\n37. Zhang, S., Metaxas, D.: On the challenges and perspectives of foundation models\nfor medical image analysis. Medical Image Analysis 91, 102996 (2024)\n38. Zhang, Y., Li, H., Sun, Y., et al.: Attention-challenging multiple instance learning\nfor whole slide image classification. In: ECCV. pp. 125–143. Springer (2024)\n39. Zheng, Y., Gindra, R.H., Green, E.J., et al.: A graph-transformer for whole slide\nimage classification. IEEE Transactions on Medical Imaging 41(11), 3003–3015\n(2022)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20823v1.pdf",
    "total_pages": 11,
    "title": "Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?",
    "authors": [
      "Jiawen Li",
      "Jiali Hu",
      "Qiehe Sun",
      "Renao Yan",
      "Minxi Ouyang",
      "Tian Guan",
      "Anjia Han",
      "Chao He",
      "Yonghong He"
    ],
    "abstract": "The emergence of foundation models in computational pathology has transformed\nhistopathological image analysis, with whole slide imaging (WSI) diagnosis\nbeing a core application. Traditionally, weakly supervised fine-tuning via\nmultiple instance learning (MIL) has been the primary method for adapting\nfoundation models to WSIs. However, in this work we present a key experimental\nfinding: a simple nonlinear mapping strategy combining mean pooling and a\nmultilayer perceptron, called SiMLP, can effectively adapt patch-level\nfoundation models to slide-level tasks without complex MIL-based learning.\nThrough extensive experiments across diverse downstream tasks, we demonstrate\nthe superior performance of SiMLP with state-of-the-art methods. For instance,\non a large-scale pan-cancer classification task, SiMLP surpasses popular\nMIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability in\nfew-shot classification and remaining highly competitive with slide-level\nfoundation models pretrained on tens of thousands of slides. Finally, SiMLP\nexhibits remarkable robustness and transferability in lung cancer subtyping.\nOverall, our findings challenge the conventional MIL-based fine-tuning\nparadigm, demonstrating that a task-agnostic representation strategy alone can\neffectively adapt foundation models to WSI analysis. These insights offer a\nunique and meaningful perspective for future research in digital pathology,\npaving the way for more efficient and broadly applicable methodologies.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}