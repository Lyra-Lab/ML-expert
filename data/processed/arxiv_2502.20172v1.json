{
  "id": "arxiv_2502.20172v1",
  "text": "Multimodal Representation Alignment for Image Generation:\nText-Image Interleaved Control Is Easier Than You Think\nLiang Chen1\nShuai Bai2\nWenhao Chai3\nWeichu Xie4\nHaozhe Zhao1\nLeon Vinci 5\nJunyang Lin2\nBaobao Chang1\n1 Peking University\n2 Alibaba Group\n3 University of Washington\n4 Beijing Institute of Technology\n5 Bainance Labs\nhttps://github.com/chenllliang/DreamEngine\nFigure 1. Generation examples of DREAM ENGINE. Leveraging powerful text-to-image diffusion model and large multimodal models,\nDREAM ENGINE is capable of generating image with text-image interleaved control by merging concepts from different images.\n1\narXiv:2502.20172v1  [cs.CV]  27 Feb 2025\n\n\nAbstract\nThe field of advanced text-to-image generation is wit-\nnessing the emergence of unified frameworks that integrate\npowerful text encoders, such as CLIP and T5, with Diffu-\nsion Transformer backbones. Although there have been ef-\nforts to control output images with additional conditions,\nlike canny and depth map, a comprehensive framework for\narbitrary text-image interleaved control is still lacking. This\ngap is especially evident when attempting to merge concepts\nor visual elements from multiple images in the generation\nprocess. To mitigate the gap, we conducted preliminary ex-\nperiments showing that large multimodal models (LMMs)\noffer an effective shared representation space, where im-\nage and text can be well-aligned to serve as a condition\nfor external diffusion models. Based on this discovery, we\npropose DREAM ENGINE, an efficient and unified frame-\nwork designed for arbitrary text-image interleaved control\nin image generation models.\nBuilding on powerful text-\nto-image models like SD3.5, we replace the original text-\nonly encoders by incorporating versatile multimodal infor-\nmation encoders such as QwenVL. Our approach utilizes a\ntwo-stage training paradigm, consisting of joint text-image\nalignment and multimodal interleaved instruction tuning.\nOur experiments demonstrate that this training method is\neffective, achieving a 0.69 overall score on the GenEval\nbenchmark, and matching the performance of state-of-the-\nart text-to-image models like SD3.5 and FLUX.\n1. Introduction\nRecent years have witnessed remarkable advancements in\ntext-to-image generation, primarily driven by powerful dif-\nfusion models [3, 13, 19, 37]. While these models excel\nat generating images that align with simple text prompts,\nthey struggle to handle more complex instructions that in-\nterweave graphical and textual elements. Although condi-\ntion augmentation methods like IP-Adapter [52] and Con-\ntrolNet [54] enhance text-to-image models with additional\nlow-level control signals such as canny edges, depth maps,\nor reference images, they lack the flexibility to process\ncomplex and high-level text-image interleaved instructions,\nfor example, merging visual elements from multiple im-\nages using natural language descriptions.\nThis inability\nrestricts more creative image generation processes where\nusers might want to precisely orchestrate visual composi-\ntions by combining and manipulating elements from multi-\nple sources with simple text instructions.\nMeanwhile, Large Multimodal Models (LMMs) [1, 6,\n26, 45] have shown remarkable progress in understanding\nvisual content and natural language instructions, enabling\nvarious tasks such as image captioning, visual question an-\nswering, and visual grounding. This advancement raises an\nintriguing question: Can we take advantage of the advanced\nLMM\nDiffusion Model\nHidden States\n(a) Emu2\n(b) Seed\n(c) Dream Engine (Ours)\nFigure 2. Overview Comparison. Among all types of works con-\nnecting LMM and diffusion model, our DREAM ENGINE adopts\nthe simplest design yet achieves the best performance.\nvisual language understanding capabilities of LMMs to im-\nprove diffusion-based image generation models, enabling\nmore flexible text-image interleaved control?\nSeveral recent works have explored integrating LMMs\nwith diffusion models to enhance image generation control.\nAs shown in Figure 2, Emu-1 and 2 [41, 42] incorporate\na specialized regression head on the hidden output states of\nLMM tokens following multimodal input processing. Seed-\nTokenizer [15] expands the LMM vocabulary with discrete\nvision tokens, which serve as condition for the diffusion\nmodel during image generation. BLIP-Diffusion [23] em-\nploys a multimodal query-transformer encoder to extract\nsubject representations, which are then combined with text\nprompts to guide the generation process.\nHowever, many of these approaches merely add text-to-\nimage generation capabilities to LMMs without improving\ngeneration quality [57] or expanding potential applications.\nSome methods [23] are designed for specific tasks and can\nonly process a single conditioning image, limiting their util-\nity in scenarios involving multiple image inputs. To the\nbest of our knowledge, no existing models can effectively\nperform compositional image generation tasks, indicating a\ngap in understanding text-image interleaved control, partic-\nularly when multiple images are involved.\nOur insight is that the fundamental challenge lies in\neffectively representing multimodal interleaved control,\nwhere mapping both text and images into a unified seman-\ntic space is crucial for coherent alignment. In this work,\nwe demonstrate that Large Multimodal Models inherently\nprovide a unified representation space, eliminating the need\nfor additional architectural components such as regression\nheads or specialized tokens. We propose DREAM ENGINE,\nan efficient and effective framework for image generation\nthat accepts arbitrary text-image interleaved control signals.\nBuilding upon open-source text-to-image diffusion models\nlike Stable Diffusion v3.5 [13], we replace its text encoders\nwith a LMM along with a lightweight projector layer to en-\ncode the text-image interleaved controls. We introduce a\ntwo-stage training paradigm that efficiently aligns the repre-\nsentation spaces between these backbone models, enabling\n2\n\n\nMM-DIT BLOCK\nNoised \nLatent \nEmbedding\nMM-DIT BLOCK\nMM-DIT BLOCK\nLMM\nInterleaved \nText & Image\nAdapter\nx\nc\nText2Image (SD3)\nDream Engine\nT5+CLIP\nText\nLinear\nReplaced By\ny\nTimestep\nEmbedding\nCondition \nEmbedding\n+\n avg pooling\nMM-DIT\n……\nOutput\nLMM\nInterleaved \nText & Image\nAdapter\nAdd a white hat:\n……\nTransformer Block\nTransformer Block\nTransformer Block\nText Input Tokens\nImage Input Tokens\nOutput Hidden States\nBlended Visual Hidden States\n+\nVIT\nLMM with Blended Visual Feature\nFigure 3. DREAM ENGINE architecture.\nthe generation of images guided by interleaved text and im-\nage instructions. We also design a new task called objects\ndriven generation, which leverages object detection and im-\nage captioning data to enable compositional generation.\nOur experiments demonstrate the effectiveness of our ar-\nchitecture design and training recipe. By fine-tuning only\nan MLP layer on 20 millions data during Stage-I training,\nour model achieves an overall score of 0.69 on the GenEval\nbenchmark, matching the performance of state-of-the-art\ntext-to-image models such as SDv3.5 (0.71) and surpass-\ning FLUX.1 Dev (0.66).\nThis result highlights the effi-\ncacy of our alignment tuning method and demonstrates that\npowerful multimodal encoders can replace text encoders\nwithout compromising the original diffusion model’s im-\nage generation quality. Furthermore, our Stage-II model\nexhibits strong text-image interleaved instruction following,\nsignificantly outperforming comparable models like Emu2-\ngen with substantially less training data. Notably, it can\neven synthesize concepts from different input images based\non the text prompt to generate a cohesive output image as\nshown in Figure 1. Our contributions are threefold:\n• We found that Large Multi-Modal Models can be easily\nadapted into the text encoder of the text-to-image diffu-\nsion models even without updating the parameters.\n• We achieve object-driven generation, combining object\ndetection and captioning for compositional generation.\n• Our method allows for complex, interwoven guidance\nfrom both text and images, resulting in highly customized\noutputs and state-of-the-art quality.\n2. Methods\nWe target at enabling diffusion models to take different text-\nimage interleaved control in a unified manner by introduc-\ning a large multimodal model. We first concisely introduce\nLMM and MM-DiT architecture in Section 2.1, which are\nthe foundational components of our method. Next, we in-\ntroduce the structure design of DREAM ENGINE, explaining\nhow do we align LMM with the diffusion model and how\nto maintain visual features consistency in Section 2.2. Last,\nwe introduce the two-stage training recipe and how to curate\ndata from different tasks in in Section 2.3.\n2.1. Preliminary: LMM and MM-DiT\nLarge Multimodal Models\nLarge multimodal models\ntypically include three major modules, a visual encoder\nusually a ViT [12] structure, a large language model back-\nbone and an alignment layer used to align the representation\nspace between the visual encoder and LLM [1, 9, 26, 45,\n55]. Given an input image i, this data first passes through\nthe visual encoder followed by the alignment layer, result-\ning in a transformed representation denoted as hViT. Subse-\nquently, hViT is processed by the LLM backbone consisting\nof multiple transformer decoder blocks, evolving into the\noutput image hidden states hLLM used for text generation.\nMultimodal Diffusion Transformer\nMM-DiT struc-\nture [13], is the basic structure for state-of-the-art text-to-\nimage diffusion models such as SD3.5 [13] and FLUX [3].\n3\n\n\nInstruction: A cat <Image1> is sitting in the forest <Image2>. \nImage1\nImage2\nInstruction : A girl <Image1> is standing in the forest <Image2>. \nInstruction : A girl <Image1> is standing along the beach <Image2>. \nDream Engine (Ours)\nEmu2-Gen\nInstruction : A cat <Image1> is sitting on the beach <Image2>. \nInstruction : A cat <Image1> and a girl <Image2> on the beach <Image3> \nDream Engine (Ours)\nFigure 4. Performance demonstration on Natural Object Background Merging where DREAM ENGINE can understand complex text-image\ninput. It can even set more than one object in the background (last line).\n4\n\n\nIt is built upon the Latent Diffusion Model (LDM) [37] and\nDiffusion Transformer (DiT) [32]. It concatenates textual\nconditioning information c with noised latent embeddings\nx into a unified sequence. Within the DiT module, MM-\nDiT employs distinct LayerNorm and MLP layers for each\nmodality while merging their sequences during the atten-\ntion mechanism.\nThis design allows each representation\nto evolve within its own specialized space while still con-\nsidering the influence of the other modality. We employ\nthe embeddings of the timestep t and pooled representation\nof text condition in the modulation mechanism of DiT. We\nuse rectified flow matching [28] as the training objectives to\nconduct text-to-image generation in Section 2.2.\n2.2. Model Design\nWe adopt the MM-DiT module from Stable Diffusion 3 [13]\nmodel as the DiT model and Qwen2VL [45] as the LMM to\ncompose DREAM ENGINE.\nAlign LMM and MM-DiT\nAs shown in Figure 3, we\ncompletely replace the text encoders including CLIP [34]\nand T5 [51] from the text-to-image diffusion models with\nthe LMM to get a unified representation of text and image\nc. To align the representation space of pretrained LMM with\nthat of previous encoders and enable the MM-DiT module\nto take image input, we add a straight-forward adapter layer\nconsisting of a two-layer MLP. The adapter maps the out-\nput hidden states of LMM to the conditioning feature space\nof MM-DiT. We add the average pooling representations of\nthe LMM condition and timestep embedding as the modu-\nlation embedding y in the MM-DiT model. We remove the\ntoken length limit of original text encoders so that DREAM\nENGINE can take any sequence length of text-image input.\nBlending Visual Feature for Better Consistency\nTo\ncontrol the visual consistency in image editing and objects-\ndriven generation tasks, we add a skip connection for visual\nfeatures in the LMM to avoid visual information loss in the\nLMM backbone model. As shown in Figure 3, the final hid-\nden states of image patches hI are a weighted sum of the\nLLM output hidden states hLLM and ViT image features\nhViT:\nhI = (1 −r) · hLLM + r · hViT\n(1)\nThe adjustable blending ratio r allows for the control of\nimage feature consistency between the input and output im-\nages, tailored to specific applications. During the training\nphase, r ∈[0, 1] adheres to a uniform distribution, which\nenables the flexibility to assign various values to r during\nthe inference process as shown in Figure 9.\nTraining Objectives\nWe adopt rectified flows [28] to\nlearn the transition between the target data distribution x0\nand a standard normal distribution ϵ, i.e.\nzt = (1 −t) · x0 + t · ϵ,\n(2)\nwhere t ∈[0, 1] represents the timestep, and zt denotes\nthe corresponding distribution at the t-th step. At each step,\nbased on the current distribution zt, a condition c, and the\ntimestep t, the model directly parameterizes the velocity\nvθ(zt, c, t). This velocity is expected to approximate x0 −ϵ\nduring the flow matching process. It is important to note\nthat the condition c can include interleaved text-image con-\ntrol, as opposed to solely text-based information as seen in\nthe original text-to-image diffusion models. The training\nobjective is to minimize the expected L2 loss by updating\nthe model parameters vθ, with a weight wt assigned to each\ntimestep, i.e.\nmin\nvθ\nZ 1\n0\nE\nh\nwt ∥(x0 −ϵ) −vθ(zt, c, t)∥2i\ndt\n(3)\nWe use the Euler Discrete Scheduler [21] following\nSD3 [13] to set the timestep. The target data distribution\nx0 comes from the latent representation of VAE, which is\nthe same as the VAE of SD3 following a standard Latent\nDiffusion Model [37] training process.\n2.3. Training Stages\nGiven that DREAM ENGINE comprises two individually\npretrained components—the LMM and the DiT—it is cru-\ncial to ensure their alignment. Adhering to the established\npractices outlined in the LMM literature [1, 8, 26, 44], we\nhave structured the training process into distinct phases,\neach designed to unfreeze specific model components to\npromote stable and effective training. As shown in Fig-\nure 5, our approach involves two primary training stages,\nwhere each stage has its own training tasks and trainable\nmodules. In the S1 stage, we focus on training only the\nadapter layer, which facilitates the alignment of the repre-\nsentation spaces between the LMM and the DiT. During the\nS2 stage, we train both the adapter and the DiT, allowing for\nmore sophisticated control over the generation process. We\nalso show the training examples of each task in Figure 5.\nStage 1: Joint Text and Image Alignment\nIn the first\nstage, we focus on aligning the representation spaces of the\nLMM and DiT modules by training a dedicated adapter,\nwhile keeping the parameters of both the LMM and DiT\nfrozen. This alignment process involves two complemen-\ntary tasks.\n• Task A: Text-to-Image Alignment.\nIt leverages high-\nquality image-caption pairs to establish a foundational\n5\n\n\nStage 1: Joint Text and Image Alignment\nStage 2: Interleaved Condition Instruction Tuning\nLMM\nAdapter\nMM-DIT\n❄\n❄\n🔥\nLMM\nAdapter\nMM-DIT\n🔥\n🔥\n❄\nTask3: Free Form Image Edit\nTask1: Text-to-Image Alignment\nCaption\nImage\nImage\nReconstructed Image\nInstruction\nEdited Image\nDescription\nImage\nImage\n…\nModel\nModel\nModel\nModel\nTask2: Image-to-Image Alignment\nTask4: Objects Driven Generation\n“A cat and a dog \non the grass.”\n“A cat and a dog \non the grass.”\n“Replace the tie with \na superhero cape.”\nFigure 5. Training stages and tasks of DREAM ENGINE.\ncorrespondence between textual descriptions and gener-\nated images, effectively replacing the original text en-\ncoders.\n• Task B: Image-to-Image Alignment.\nIt is a self-\nsupervised task that enables DiT to condition on image in-\nputs. Specifically, DiT is trained to reconstruct input im-\nages based on the LMM’s image representations, thereby\nenhancing the consistency and fidelity of visual elements.\nUpon completing Stage 1 training, DREAM ENGINE ac-\nquires two core capabilities: text-to-image generation and\nimage variation. Interestingly, we observe that the two\ntasks mutually reinforce each other.\nEven when trained\nsolely on one task, the model demonstrates a certain degree\nof capability in the other task in a zero-shot manner. This\nfinding suggests that the LMM inherently provides a unified\nrepresentation space for text and images, which the DiT can\neffectively leverage during training. As shown in Table 3,\nour model trained without the Image-to-Image Alignment\ntask can still achieve a relatively high (0.7+) CLIP score in\nthe image reconstruction evaluation.\nStage 2: Interleaved Condition Instruction Tuning\nIn\nthe second stage, we unfreeze the DiT module and train it on\ntwo tasks that require interleaved image-text conditioning.\n• Task C: Free-Form Image Editing. It takes an input image\nalong with an editing instruction and outputs the edited\nimage. We use the UltraEdit [56] dataset as the dataset.\n• Task D: Objects Driven Generation. It accepts multiple\ninput images and a textual instruction, composing ele-\nments from the input images based on the given text to\ngenerate the output. For Task 4, we construct the training\ndata using object detection datasets, such as COCO [25],\npairing images with captions that describe the objects\npresent.\nAfter the second stage, the model gains the ability to\nhandle interleaved image-text conditions during generation.\nSurprisingly, we observe emergent capabilities in DREAM\nENGINE. Notably, it can synthesize elements from differ-\nent objects to generate cohesive images, as demonstrated\nin Figure 6, despite such compositions not being explicitly\npresent in the training data.\n3. Experiments\n3.1. Dataset\nTable 2 provides an overview of the datasets used for train-\ning DREAM ENGINE, along with the number of exam-\nples drawn from each source. In Stage 1, for the Text-to-\nImage Alignment task, we compile public image-caption\ndatasets, including real-world images from CC12M [4] and\nmodel-generated images from JourneyDB [39]. Addition-\nally, we synthesize a subset of high-quality images using\ndiverse prompts with open text-to-image models, such as\nFlux.1 dev [3] and Stable-Diffusion v3.5 Large [13]. For\nthe Image-to-Image Alignment task, we rely solely on im-\nages from JourneyDB, as lower-aesthetic-quality images,\nsuch as images from CC12M, tend to degrade overall image\nreconstruction and text-to-image performance. In Stage 2,\nwe utilize the UltraEdit [56] dataset for the Free-Form Im-\nage Editing task and an internal object detection dataset for\nObject-Driven Generation. For the latter, we randomly se-\nlect three objects from each image. Additionally, the name\nof each selected object must appear in the text caption to\ncompose the conditioning input.\n6\n\n\nTable 1. Performances on GenEval benchmark. We split the methods to autoregressive and diffusion based models.\nMethod\nSingle\nObject\nTwo\nObject\nCounting\nColors\nPosition\nAttribute\nBinding\nOverall\nAutoregressive\nChameleon [43]\n-\n-\n-\n-\n-\n-\n0.39\nLWM [27]\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\nLlamaGen [40]\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\nShow-o [50]\n0.95\n0.52\n0.49\n0.82\n0.11\n0.28\n0.53\nEmu3-Gen [46]\n0.98\n0.71\n0.34\n0.81\n0.17\n0.21\n0.54\nJanus [48]\n0.97\n0.68\n0.30\n0.84\n0.46\n0.42\n0.61\nDiffusion\nLDM [36]\n0.92\n0.29\n0.23\n0.70\n0.02\n0.05\n0.37\nSDv1.5 [36]\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nPixArt-α [5]\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nSDv2.1 [36]\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\nDALL-E 2 [35]\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nSDXL [33]\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nIF-XL [10]\n0.97\n0.74\n0.66\n0.81\n0.13\n0.35\n0.61\nDALL-E 3 [2]\n0.96\n0.87\n0.47\n0.83\n0.43\n0.45\n0.67\nSDv3 Medium [13]\n0.98\n0.74\n0.63\n0.67\n0.34\n0.36\n0.62\nFlux.1 Dev [3]\n0.98\n0.81\n0.74\n0.79\n0.22\n0.45\n0.66\nSDv3.5 Large [13]\n0.98\n0.89\n0.73\n0.83\n0.34\n0.47\n0.71\nDREAM ENGINE\n1.00\n0.94\n0.64\n0.81\n0.27\n0.49\n0.69\nTable 2.\nDetails on datasets used in training DREAM EN-\nGINE within the two training stages.\nStage\nDataset\nTask\nNumber\n1\nJourneyDB [39]\nText-to-Image Alignment\n4M\nCC12M [4]\nText-to-Image Alignment\n4M\nSynthetic Data\nText-to-Image Alignment\n4M\nJourneyDB [39]\nImage-to-Image Alignment\n4M\n2\nUltraEdit [56]\nFree Form Image Edit\n1M\nInternal Data\nObject-Driven Generation\n4M\n3.2. Model and Training Details\nWe initialize the LMM and DiT module of DREAM\nENGINE from Qwen2VL-2B-Instruct [45] and Stable-\nDiffusion-3.5-Large [13]. The Adapter consists of a two-\nlayer MLP with a middle projection dimension of 4,096 and\nuses SiLU as the activation function following the DiT mod-\nule. In Stage 1, we freeze the parameters of the LMM and\nDiT modules and train the Adapter on the composed dataset\nfor one epoch with a global batch size of 128. The learning\nrate is set to 1e-4, with 5% warmup steps and a cosine learn-\ning rate scheduler. In Stage 2, we also fine-tune the DiT\nmodule using LoRA [20] with a rank of 32 on all attention\nlayers. The learning rate is set to 5e-5, while all other set-\ntings remain the same as in Stage 1. We do not fine-tune the\nLMM component of the model, thus preserving its original\nmultimodal understanding capabilities. This design choice\nallows the model to be easily adapted into an omni-model,\ncapable of performing both multimodal understanding and\ngeneration simultaneously. On the other hand, unfreezing\nTable 3. Image reconstruction performance comparison on COCO\nand JourneyDB datasets.\nMethod\nCOCO\nJourneyDB\nCLIP (↑)\nL2 (↓)\nCLIP (↑)\nL2 (↓)\nSeedTokenizer\n0.7760\n0.5102\n0.7921\n0.5291\nEMU2-Gen\n0.8537\n0.3828\n0.9299\n0.2869\nSEED-X\n0.8595\n0.4317\n0.9017\n0.4352\nDREAM ENGINE\n0.8714\n0.2065\n0.9221\n0.2052\n- w/o I-to-I Alignment\n0.7184\n0.6541\n0.7536\n0.6543\nthe LMM during training has large potential in further im-\nproving the generation performance.\n3.3. Results and Comparisons\nText-to-Image Generation\nWe\nevaluate\nthe\ntext-to-\nimage generation capability of DREAM ENGINEon the\nGenEval [18] benchmark following Stage 1 training. The\nresults, including fine-grained scores, are presented in\nTable 1. Built upon the SDv3.5 [13] model, DREAM EN-\nGINEachieves a competitive overall score of 0.69, closely\nmatching the original model’s 0.71 despite excluding its na-\ntive text encoders. Moreover, DREAM ENGINEoutperforms\nall other counterparts, demonstrating the effectiveness and\nefficiency of our text-to-image alignment training in pre-\nserving instruction-following capabilities while replacing\nthe original text encoders of diffusion models to enable\nmore complex interleaved conditions.\nImage Reconstruction\nWe introduce an Image Recon-\nstruction Benchmark to evaluate the preservation of visual\n7\n\n\nInstruction : A dog <Image1> wearing head scarf \nlike the girl <Image2>. \nInstruction : A cat <Image1> wearing head scarf \nlike the girl <Image2>. \nImage1\nImage2\nDream Engine (Ours)\nEmu2-Gen\nFigure 6. Performance demonstration on Object Driven Feature Mixing task. DREAM ENGINE can understand the complex instruction\nwhile Emu2-Gen fails on the task.\nChange the girl to a dog\nChange the girl to a boy\nChange the girl to a singer with \nuniverse as the background\nSource Image\nSource Image\nSource Image\nChange the dog to a tiger\nChange the animal to a llama\nEmu2-Gen\nEmu2-Gen\nEmu2-Gen\nOurs\nOurs\nOurs\nEmu2-Gen\nOurs\nEmu2-Gen\nOurs\nFigure 7. Performance demonstration on Free Form Image Editing task. DREAM ENGINE outperforms the counterpart Emu2-Gen model\nin both instruction following and output image quality.\n8\n\n\nSource Image\nImage Reconstruction Results\nTraining Compute (Joint Image-Language Alignment)\nFigure 8. Image reconstruction performance dynamics during training. We can see that there is a concept-to-detail transition during the\ntraining period.\nfeatures in our Image-to-Image alignment task during Stage\n1. This capability is essential for generating images con-\nditioned on input images. To construct the benchmark, we\nrandomly sample 100 images from the JourneyDB devel-\nopment set and 100 images from the COCO development\nset. We assess the similarity between the original and recon-\nstructed images using the CLIP [34] score and L2-Distance.\nAs shown in Table 3, we compare the performance of\nDREAM ENGINE\nagainst several baselines with similar\narchitectures, including SeedTokenizer [15], EMU-2 [42],\nand SeedX [17], which also integrate LMMs and diffusion\nmodels for generation.\nThe results demonstrate that our\nmodel achieves the best average image reconstruction per-\nformance across both subsets of the benchmark. Notably, it\nachieves outstanding performance on the L2 distance met-\nric, which emphasizes pixel-level consistency, surpassing\nthe second-best model by 46% on the COCO subset and\n28% on the JourneyDB subset.\nGeneration with Text-Image Interleaved Control\nAfter\nStage 2 training, DREAM ENGINE acquires the capability to\nincorporate text-image interleaved control during the image\ngeneration process. We showcase several applications of\nthe model in this paper and compare its performance with\nEmu-2 [42], the most relevant baseline that also supports\ntext-image interleaved control.\n1. Natural Object Background Merging: Figure 4 illus-\ntrates an example application where objects are merged\ninto different backgrounds based on a provided hint im-\nage. The results demonstrate that DREAM ENGINEcan\nseamlessly place the main object into various back-\ngrounds in a more natural manner even when there are\nmultiple objects, rather than merely copying and pasting\nthe objects.\n2. Object Driven Feature Mixing: Figure 6 demonstrates\nan emergent ability of DREAM ENGINE to generate im-\nages by combining visual features from given images\nbased on text instructions—an area where the EMU-2\nmodel fails to follow instructions accurately.\nNotably,\nthese examples are not present in the training dataset,\nwhich was constructed directly from an object detection\ndataset. This highlights the significant potential of LMMs\nas unified multimodal instruction encoders for image gen-\neration. DREAM ENGINE effectively decouples complex\nelements from different images using simple text prompts\nand produces a unified representation, showcasing its ver-\nsatility and robustness.\n3. Free Form Image Editing: Figure 7 presents the re-\nsults of our free-form image editing task. DREAM EN-\nGINE consistently demonstrates superior ability to fol-\nlow edit instructions compared to EMU-2.\nNotably,\nDREAM ENGINE can handle complex editing instruc-\n9\n\n\nBlending Ratio = 0 \nBlending Ratio = 1 \nSource Image\nBlending Ratio = 0.5 \nℎ! = 1 −𝑟⋅ℎ\"\"# + 𝑟⋅ℎ$%&\nImage Alignment Results (Random Sample 4 Images)\n𝒓: Blending Ratio\nFigure 9. The ablation on visual blending ratio in the ViT module. It reveals that a higher blending ratio results in greater consistency\nduring image reconstruction tasks.\ntions, such as simultaneously modifying both the object\nand background. This further highlights the effectiveness\nof LMMs in providing a unified representation space that\nseamlessly integrates image and text conditions.\n3.4. Discussions\nUnderstand the training dynamics of DREAM ENGINE\nTo better understand how DREAM ENGINE leverages an\nLMM and a text-to-image model to achieve complex text-\nimage instruction following ability during the training pro-\ncess, we examine the image reconstruction results at dif-\nferent training stages in Stage-1. The results are shown in\nFigure 8. We observe a clear concept-to-detail progression\nduring the training of DREAM ENGINE. For example, as\nillustrated in the first two columns of Figure 8, the model\ninitially reconstructs the primary concepts in the images,\nsuch as “girl,” “house with snow,” and “dog” in the given\nexamples.\nIn the later stages, the model begins to learn to recon-\nstruct more fine-grained details, such as colors, shapes, and\nposes. We believe this unique training dynamic stems from\nthe nature of the LMM, which provides a unified represen-\ntation space where images and text are well aligned. Thus,\nat the beginning of training, even if the text-to-image dif-\nfusion model has not yet seen image conditions, the image\nrepresentations provided by the LMM are aligned with text\nfeatures. This alignment enables the model to generate con-\nceptually aligned images through the bridge of text.\nAblation on Balancing Visual Consistency\nWe conduct\nan ablation study on the Blending Visual Feature mech-\nanism within the Vision Transformer (ViT) module.\nAs\nshown in Figure 9, varying the blending ratio in the image-\nimage alignment task produces notably different results.\nHigher blending ratios lead to greater consistency in im-\nage reconstruction, while lower ratios introduce more vari-\nation in the output. This mechanism offers flexible con-\ntrol over object consistency, benefiting various downstream\ntasks such as image editing and object-driven feature mix-\ning.\n4. Related Work\n4.1. Image Generation with Complex Control\nRecent progress in controlled image generation using dif-\nfusion models has been significant.\nResearchers have\nexplored various conditioning strategies—ranging from\nlow-level cues like canny edges and depth maps [52,\n54] to higher-level guidance provided by reference im-\nages [30]—to steer the generative process. For instance,\nmethods such as IP-Adapter [52] and ControlNet [54] in-\ncorporate additional control signals into standard text-to-\nimage frameworks, thereby allowing more precise manip-\nulation of generated content.\nIn parallel, several works\nhave leveraged visual elements from input images to further\nguide the generation process. DreamBooth [38] and Textual\nInversion [14], for example, adopt optimization-based ap-\nproaches to adapt models to specific reference images. Al-\nthough effective, these methods typically require extensive\nfine-tuning for each new input, limiting their practicality.\nTo address these limitations, approaches like SuTI [7] and\nSubject-diffusion [29] have aimed to scale the fine-tuning\nprocess so that models can generalize across diverse refer-\nence images. However, these strategies still tend to be both\ntime- and resource-intensive, highlighting the ongoing need\nfor more efficient mechanisms for image generation with\ncomplex controls.\n10\n\n\n4.2. Connecting LMMs with Diffusion Models\nRecent studies integrate LMMs with diffusion generators,\nleveraging the strengths of both paradigms. One straight-\nforward approach employs LMMs to interpret complex\ntext-image conditions and generate pure textual represen-\ntations, which then guide image generation models [24].\nMoreover, Seed-Tokenizer [15] expands the LMM vocab-\nulary by introducing discrete vision tokens that serve as ro-\nbust conditioning signals for diffusion models, while Seed-\nLLama [16] pre-trains a discrete image tokenizer that de-\ncodes visual codes into realistic images using pretrained\ndiffusion models.\nSimilarly, M-VADER [47] aligns se-\nmantic consistency between language models and diffu-\nsion decoders through training on extensive image-text pair\ndatasets.\nMethods such as GILL [22], MiniGPT5 [58]\nEmu [41] further advance this integration by mapping the\nembedding spaces of language models to diffusion mod-\nels, and NExT-GPT [49] and Any-GPT [53] even broad-\nens the scope to include modalities like audio and video.\nAdditionally, DreamLLM [11] employs a novel strategy\nby transferring differential gradients from image diffusion\nmodels to language models, thereby enabling free-form, in-\nterleaved content generation. To enhance flexible control\nin image generation, BLIP-Diffusion [23] leverages LMMs\nto jointly encode image and text inputs, projecting them\ninto the text conditioning space of diffusion models to bet-\nter handle complex instructions. Moreover, Kosmos-g [31]\nand Emu-2 [42] explore the multimodal in-context control\nfor image generation. Seed-X [17] using a similar architec-\nture to model multi-granularity visual semantics for better\ngeneration in the real world applications. While promis-\ning, these approaches mainly extend text-to-image genera-\ntion and often fall short in improving image quality or man-\naging compositional tasks with arbitrary text-image inter-\nleaved control, limiting their real-world applicability.\n5. Conclusion\nIn this work, we introduced DREAM ENGINE, a novel\nframework that enables sophisticated text-image interleaved\ncontrol without complex architectural modifications. Our\nmethod bridges Large Multimodal Models and text-to-\nimage diffusion models through a lightweight projector\nlayer and efficient two-stage training paradigm. It demon-\nstrates superior capabilities in handling multiple image in-\nputs and compositional instructions while also achieving\ncompetitive performance on the GenEval benchmark (0.69).\nThe success of DREAM ENGINE demonstrates that LMMs\ncan effectively replace traditional text encoders in text-to-\nimage diffusion models while expanding their capabilities\nto include sophisticated multimodal control.\nLooking ahead, our work opens up new possibilities for\ncreative image generation applications where users can pre-\ncisely orchestrate visual compositions through natural lan-\nguage instructions and multiple reference images. Future\nresearch directions could explore extending this framework\nto other modalities, such as video or 3D content, and inves-\ntigating ways to further enhance the semantic understanding\nof complex multimodal instructions.\nReferences\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. ArXiv preprint, abs/2308.12966, 2023. 2,\n3, 5\n[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, et al. Improving image generation with better\ncaptions. Computer Science, 2023. 7\n[3] BlackForestLabs. Announcing black forest labs, 2024. 2, 3,\n6, 7\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. In IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 3558–3568, 2021. 6,\n7\n[5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al. PixArt-alpha: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023. 7\n[6] Liang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe\nZhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang,\nYizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge\nZhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen,\nJunyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Min-\njia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, and Baobao\nChang. Next token prediction towards multimodal intelli-\ngence: A comprehensive survey, 2024. 2\n[7] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz,\nXuhui Jia, Ming-Wei Chang, and William W. Cohen.\nSubject-driven text-to-image generation via apprenticeship\nlearning, 2023. 10\n[8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-\nwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang\nYan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin,\nChao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang,\nBo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min\nDou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao,\nJifeng Dai, and Wenhai Wang. How far are we to gpt-4v?\nclosing the gap to commercial multimodal models with open-\nsource suites, 2024. 5\n[9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng\nDai.\nInternvl: Scaling up vision foundation models and\naligning for generic visual-linguistic tasks, 2024. 3\n11\n\n\n[10] DeepFloyd. DeepFloyd IF, 2023. 7\n[11] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng\nMa, and Li Yi.\nDreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv: 2309.11499,\n2023. 11\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale, 2021. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified\nflow transformers for high-resolution image synthesis. 2024.\n2, 3, 5, 6, 7\n[14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion, 2022. 10\n[15] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023. 2, 9, 11\n[16] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,\nXintao Wang, and Ying Shan. Making llama see and draw\nwith seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\n11\n[17] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin\nSong, Chen Li, Xiaohan Ding, and Ying Shan. SEED-X:\nMultimodal models with unified multi-granularity compre-\nhension and generation. arXiv preprint arXiv:2404.14396,\n2024. 9, 11\n[18] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment, 2023. 7\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 2\n[20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\nIn\nThe Tenth International Conference on Learning Represen-\ntations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\n7\n[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels, 2022. 5\n[22] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models. NeurIPS,\n2023. 11\n[23] Dongxu Li, Junnan Li, and Steven C. H. Hoi. Blip-diffusion:\nPre-trained subject representation for controllable text-to-\nimage generation and editing, 2023. 2, 11\n[24] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-gemini: Mining the potential of multi-modality\nvision language models, 2024. 11\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014. 6\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. ArXiv preprint, abs/2304.08485,\n2023. 2, 3, 5\n[27] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024. 7\n[28] Xingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data with\nrectified flow, 2022. 5\n[29] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-\ndiffusion:open domain personalized text-to-image genera-\ntion without test-time fine-tuning, 2024. 10\n[30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions, 2022. 10\n[31] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-g: Generating images\nin context with multimodal large language models, 2024. 11\n[32] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers, 2023. 5\n[33] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach. SDXL: Improving latent diffusion models\nfor high-resolution image synthesis. In ICLR, 2024. 7\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, pages 8748–\n8763, 2021. 5, 9\n[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with CLIP latents. arXiv preprint arXiv:2204.06125,\n2022. 7\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 7\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2022. 2, 5\n[38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration, 2023. 10\n[39] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong\nDuan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin,\nYi Wang, et al. JourneyDB: A benchmark for generative im-\nage understanding. In NeurIPS, 2024. 6, 7\n12\n\n\n[40] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion:\nLLaMA for scalable image generation.\narXiv preprint arXiv:2406.06525, 2024. 7\n[41] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality, 2023. 2, 11\n[42] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiy-\ning Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang.\nGenerative multimodal mod-\nels are in-context learners. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 14398–14409, 2024. 2, 9, 11\n[43] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n7\n[44] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun\nWoo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang,\nRob Fergus, Yann LeCun, and Saining Xie.\nCambrian-1:\nA fully open, vision-centric exploration of multimodal llms,\n2024. 5\n[45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui\nMen, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-\nyang Lin. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution, 2024. 2, 3, 5, 7\n[46] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, et al. Emu3: Next-token prediction is\nall you need. arXiv preprint arXiv:2409.18869, 2024. 7\n[47] Samuel Weinbach, Marco Bellagente, Constantin Eichen-\nberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Bj¨orn\nDeiseroth, Koen Oostermeijer, Hannah Teufel, and An-\ndres Felipe Cruz-Salinas. M-vader: A model for diffusion\nwith multimodal context, 2022. 11\n[48] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma,\nXingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, et al. Janus: Decoupling visual encoding\nfor unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024. 7\n[49] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng\nChua. Next-gpt: Any-to-any multimodal llm. arXiv preprint\narXiv: 2309.05519, 2023. 11\n[50] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,\nWeihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou.\nShow-o:\nOne single transformer to unify multimodal understanding\nand generation. arXiv preprint arXiv:2408.12528, 2024. 7\n[51] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin\nRaffel. mt5: A massively multilingual pre-trained text-to-\ntext transformer. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, pages\n483–498, 2021. 5\n[52] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 2, 10\n[53] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong\nZhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang,\nLinyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun,\nYugang Jiang, and Xipeng Qiu.\nAnygpt: Unified mul-\ntimodal llm with discrete sequence modeling.\nArXiv,\nabs/2402.12226, 2024. 11\n[54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2, 10\n[55] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai\nAn, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han,\nand Baobao Chang. Mmicl: Empowering vision-language\nmodel with multi-modal in-context learning. ArXiv preprint,\nabs/2309.07915, 2023. 3\n[56] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Ru-\njie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and\nBaobao Chang. Ultraedit: Instruction-based fine-grained im-\nage editing at scale, 2024. 6, 7\n[57] Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, and\nKwan-Yee K. Wong.\nBridging different language models\nand generative vision models for text-to-image generation.\nECCV, 2024. 2\n[58] Kaizhi Zheng, Xuehai He, and Xin Eric Wang.\nMinigpt-\n5: Interleaved vision-and-language generation via generative\nvokens, 2023. 11\n13\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20172v1.pdf",
    "total_pages": 13,
    "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
    "authors": [
      "Liang Chen",
      "Shuai Bai",
      "Wenhao Chai",
      "Weichu Xie",
      "Haozhe Zhao",
      "Leon Vinci",
      "Junyang Lin",
      "Baobao Chang"
    ],
    "abstract": "The field of advanced text-to-image generation is witnessing the emergence of\nunified frameworks that integrate powerful text encoders, such as CLIP and T5,\nwith Diffusion Transformer backbones. Although there have been efforts to\ncontrol output images with additional conditions, like canny and depth map, a\ncomprehensive framework for arbitrary text-image interleaved control is still\nlacking. This gap is especially evident when attempting to merge concepts or\nvisual elements from multiple images in the generation process. To mitigate the\ngap, we conducted preliminary experiments showing that large multimodal models\n(LMMs) offer an effective shared representation space, where image and text can\nbe well-aligned to serve as a condition for external diffusion models. Based on\nthis discovery, we propose Dream Engine, an efficient and unified framework\ndesigned for arbitrary text-image interleaved control in image generation\nmodels. Building on powerful text-to-image models like SD3.5, we replace the\noriginal text-only encoders by incorporating versatile multimodal information\nencoders such as QwenVL. Our approach utilizes a two-stage training paradigm,\nconsisting of joint text-image alignment and multimodal interleaved instruction\ntuning. Our experiments demonstrate that this training method is effective,\nachieving a 0.69 overall score on the GenEval benchmark, and matching the\nperformance of state-of-the-art text-to-image models like SD3.5 and FLUX.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}