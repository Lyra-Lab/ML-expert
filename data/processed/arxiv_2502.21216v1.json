{
  "id": "arxiv_2502.21216v1",
  "text": "arXiv:2502.21216v1  [cs.AI]  28 Feb 2025\nAn Algebraic Framework for Hierarchical\nProbabilistic Abstraction\nNijesh Upreti, Vaishak Belle\nThe University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK\nAbstract. Abstraction is essential for reducing the complexity of sys-\ntems across diverse ﬁelds, yet designing eﬀective abstraction methodol-\nogy for probabilistic models is inherently challenging due to stochastic\nbehaviors and uncertainties. Current approaches often distill detailed\nprobabilistic data into higher-level summaries to support tractable and\ninterpretable analyses, though they typically struggle to fully represent\nthe relational and probabilistic hierarchies through single-layered ab-\nstractions. We introduce a hierarchical probabilistic abstraction frame-\nwork aimed at addressing these challenges by extending a measure-theoretic\nfoundation for hierarchical abstraction. The framework enables mod-\nular problem-solving via layered mappings, facilitating both detailed\nlayer-speciﬁc analysis and a cohesive system-wide understanding. This\napproach bridges high-level conceptualization with low-level perceptual\ndata, enhancing interpretability and allowing layered analysis. Our frame-\nwork provides a robust foundation for abstraction analysis across AI sub-\nﬁelds, particularly in aligning System 1 and System 2 thinking, thereby\nsupporting the development of diverse abstraction methodologies.\nKeywords: probabilistic abstraction · hierarchical models · algebra.\n1\nIntroduction\nAbstraction serves as a core concept across knowledge domains, simplifying com-\nplex relationships into more comprehensible forms by focusing on essential de-\ntails and discarding irrelevant ones [14,33,4]. In probabilistic models, abstrac-\ntion is challenging due to inherent uncertainties and the stochastic nature of the\nsystems involved [26,16]. Various methodologies have been developed to sum-\nmarize detailed probabilistic information into higher-level representations while\npreserving essential characteristics and relationships, making such frameworks\ntractable and interpretable under uncertainty [4,21,22,25,31]. Yet, single-layered\nprobabilistic abstractions are often inadequate for capturing the full relational\nand probabilistic hierarchy in a single low-to-high-level mapping.\nRecent works emphasize multi-layered representations to model real-world hi-\nerarchical complexities, such as in disease diagnosis [18] and epidemic modeling\n[15]. These layered approaches are essential for analyzing phenomena across mul-\ntiple levels, providing interpretable insights into complex interactions and uncer-\ntainties [12,16,17]. Further, hierarchical modeling using directed acyclic graphs\n\n\n2\nN. Upreti and V. Belle\n(DAGs) has become instrumental in ﬁelds like cognitive science, where hierarchi-\ncal DAGs model complex concepts across layers [9]. However, constructing inter-\npretable high-level theories from low-level data requires a structured framework\nfor hierarchical relational and probabilistic information. A formal mathematical\ntreatment of abstraction hierarchies is thus critical, oﬀering a robust founda-\ntion for reasoning in complex spaces. Starting from single-layer mappings and\nprogressing to multi-layered models captures complexities at each level while\nclarifying relationships between diﬀerent layers [17]. Such a framework would\nfurther enable the development of methods to learn abstraction hierarchies in\ndiverse contexts, supporting detailed decomposition for in-depth analysis [35,23].\nTo position our work within this existing research landscape, we draw upon\nseveral foundational studies. First, Hofmann and Belle’s work established an\nabstraction framework for robot programs with probabilistic beliefs, utilizing the\nDS logic and bisimulation in probabilistic dynamic systems to demonstrate sound\nand complete abstraction [20]. Second, Beckers et al. contributed a framework\nfor analyzing abstraction and approximation within causal models, providing\nformal deﬁnitions and results for approximate causal abstractions [3]. Third,\nSegal et al.’s study on probabilistic abstraction hierarchies (PAH) introduced a\nprincipled approach for learning abstraction hierarchies from data, addressing\ncomplexities in modeling multi-level systems [35]. Our work is inspired by these\nresearch endeavors and directly builds on Holtzen et al.’s papers on probabilistic\nprogram abstraction, which establishes single-layered mappings from concrete\nprograms to their abstract versions with soundness properties [21,22].\nIn this paper, we propose a hierarchical probabilistic abstraction framework\nthat generalizes the measure-theoretic approach as introduced by Holtzen et\nal. to multi-layered settings while providing a structured, modular foundation\nfor abstraction in complex systems. By decomposing abstraction into layered\nmappings, the framework supports detailed, layer-speciﬁc analysis alongside a\ncomprehensive system-level understanding, enabling dual-level reasoning: indi-\nvidual transformations and probabilistic relationships at each layer, and cumu-\nlative eﬀects across layers. This structured approach facilitates exploration of\nabstractions across AI subﬁelds, notably in System 1 and System 2 cognition, by\nconnecting high-level conceptual reasoning with low-level perceptual data [24].\nThis comprehensive perspective is vital for advancing research in cognitive AI,\nstatistical relational learning, and neurosymbolic AI, laying a solid foundation\nfor the development of diverse abstraction methodologies.\n2\nMotivating Example\nImagine a personalized learning system designed to optimize educational out-\ncomes for students with diverse backgrounds and abilities. The goal is to create\nan adaptive education platform that tailors learning experiences based on indi-\nvidual cognitive proﬁles, learning environments, and behavioral patterns. This\nexample is a slightly more complex version of examples as outlined in Belle’s\nwork [4]. Towards modeling such a system, consider a probabilistic relational\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n3\nmodel (PRM) for a university database U, representing student learning and\nperformance. The model instantiates constraints for a parameterized Bayesian\nnetwork as follows:\nEnvironmental Factors →Cognitive Processes ←Learning Behaviors →Educational Outcomes\nLow-Level Theory Ul:\n□0.7\nHLE(x, High)\n□0.2\nHLE(x, Medium)\n□0.1\nHLE(x, Low)\n□0.6\nPI(x, High)\n□0.3\nPI(x, Medium)\n□0.1\nPI(x, Low)\n□0.5\nSES(x, High)\n□0.3\nSES(x, Medium)\n□0.2\nSES(x, Low)\n□0.4\nWM(x, High)\n∧\nHLE(y, High)\n∧\nPI(z, High) ∧SES(w, High)\n=⇒\ncogAbility(x, y, z, w, u)\nfor\nu\n∈\n{Strong, Moderate}\n□0.6\nWM(x, Medium) ∧HLE(y, Medium) ∧\nPI(z, Medium) ∧SES(w, Medium)\n=⇒\ncogAbility(x, y, z, w, u)\nfor\nu\n∈\n{Moderate, Weak}\nHigh-Level Theory Uh:\n□0.7\nengagement(x, High)\n□0.3\nengagement(x, Low)\n□0.5\ncogAbility(x, Strong) ∧\nengagement(y, High) ∧adaptation(z, High)\n=⇒\nacademicPerformance(x, y, z, u) for\nu ∈{Excellent, Good}\n□0.5\ncogAbility(x, Weak) ∧\nengagement(y, Low) ∧adaptation(z, Low)\n=⇒\nacademicPerformance(x, y, z, u) for\nu ∈{Average, Poor}\nIn the given example case, while a two-layer abstraction from Ul to Uh cap-\ntures some relationships between variables, it falls short due to the complexity\nand interdependencies in educational systems. In the formal model, we see nu-\nmerous variables such as Home Learning Environment (HLE), Parental Involve-\nment (PI), and Socioeconomic Status (SES), each aﬀecting Cognitive Abilities\n(WM) and Learning Styles (LS). The interaction of these variables in a two-\nlayer mapping is insuﬃcient to capture the nuanced impacts across all layers.\nFor instance, a high-quality home learning environment (HLE = High) might\nsigniﬁcantly enhance working memory (WM = High), which in turn inﬂuences\nengagement (engagement = High) and ultimately leads to better academic per-\nformance (academicPerformance = Excellent). However, this pathway also inter-\nsects with other variables like parental involvement and socioeconomic status,\ncreating a web of interdependencies that a two-layer model cannot fully address.\nTo address complex interdependencies, we can use hierarchical abstractions\nto decompose the problem into multiple layers, each capturing speciﬁc relation-\nships and interactions. This approach accommodates detailed representations\nthat reﬂect the complex interdependencies among variables, such as those be-\ntween home learning environment, parental involvement, and socioeconomic sta-\ntus. By maintaining probabilistic integrity across layers, this hierarchical model\nbreaks down these complexities, providing interpretable insights that can guide\neducators and policymakers in addressing the nuanced factors impacting educa-\ntional outcomes.\n2.1\nNew Directions in Hierarchical Probabilistic Abstraction\nIn exploring probabilistic abstraction, we encounter pivotal questions that chal-\nlenge our understanding of hierarchical models. These largely unexplored ques-\n\n\n4\nN. Upreti and V. Belle\ntions require a structured methodology to articulate the foundational princi-\nples of hierarchical abstraction. Our approach—constructing complex abstrac-\ntion layers from foundational taxonomy—serves as a starting point to address\nthese inquiries, oﬀering a systematic way to navigate and clarify the complexities\nof such models.\nBefore delving into a detailed examination of hierarchical abstractions, it is\nessential to outline the key questions guiding our inquiry:\n1. Deﬁning Hierarchical Abstraction: What deﬁnes a hierarchical abstraction\nwithin probabilistic systems, and how can this deﬁnition be systematically\napplied across layers?\n2. Validating Hierarchical Structures: How do we ascertain the accuracy of a\nhierarchical model in abstracting its underlying probabilistic layers?\n3. Navigating Abstraction Layers: What mechanisms facilitate the eﬀective\ntranslation and navigation between diﬀerent levels of abstraction within a\nhierarchical model?\n4. Managing Complexity: How does hierarchical abstraction aid in the simpliﬁ-\ncation and management of the inherent complexity in probabilistic systems?\n5. Operationalizing Hierarchical Compositions: How can operations and com-\npositions across various hierarchical layers be conceptualized and applied to\nenhance probabilistic modeling?\n3\nFoundational Taxonomy for Probabilistic Abstraction\nIn this section 1, we delineate the taxonomy of abstraction processes by exam-\nining one-layered mappings, which we use subsequently to build multi-layered\nhierarchies in compositional manner. Firstly, we analyze the transition from a\nconcrete state to an abstract state without considering subsequent layers of ab-\nstractions. This focused approach clariﬁes and classiﬁes foundational abstrac-\ntion methods, forming a basis for more complex, multi-layered abstractions. Re-\nstricting our examination to one-layered mappings, we categorize abstraction\nprocesses based on structural characteristics and mapping nature. These include\nDirect Abstraction (each concrete state maps to a unique abstract state), Diver-\ngent Abstraction (a single concrete state maps to multiple abstract states), and\nConvergent Abstraction (multiple concrete states aggregate into a single abstract\nstate). Each type oﬀers insights into how information and characteristics from the\nconcrete domain are represented, reduced, or combined in the abstract domain.\nBy focusing on these initial mappings, we establish a foundational taxonomy,\nfacilitating the systematic exploration of abstraction strategies and laying the\ngroundwork for investigating multi-layered hierarchical abstractions.\n1 For any schematic representation presented in this paper, the upward arrow (↑)\nsymbolizes an abstraction operation, indicating the transformation from a lower-\nlevel probability space to a higher-level abstracted probability space. Each step in\nthe chain, represented by a unique abstraction operation Ai, transitions the system\nfrom one probabilistic space (Ωi−1, Σi−1, Pi−1) to another (Ωi, Σi, Pi).\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n5\n3.1\nDirect Abstraction (One-to-One)\nIn our taxonomy, the ﬁrst category, direct abstraction, entails a one-to-one map-\nping between each concrete state and its abstract counterpart. Holtzen et al.’s\nwork on probabilistic program abstraction exempliﬁes this approach, using direct\nabstraction speciﬁcally in the context of probabilistic programs [22]. Building on\nthis example and retaining the same measure-theoretic approach, we generalize\nthe framework to encompass a wider range of one-to-one mappings that extend\nbeyond program-speciﬁc applications, thereby broadening the applicability of\nprobabilistic abstraction, as outlined schematically below.\n(Ωa, Σa, µa)\nx\n[[A]]\n(Ωc, Σc, µc)\nDeﬁnition 1 (Direct Abstraction): Let (Ωc, Σc, µc) be a concrete prob-\nability space and (Ωa, Σa, µa) an abstract probability space. Direct abstraction\nis facilitated by a bijective measurable function A : Ωc →Ωa, ensuring a one-\nto-one correspondence between elements of Ωc and Ωa.\nFormally, A is a measurable function such that for any measurable set B ∈\nΣa, the pre-image A−1(B) is measurable in Σc, i.e., ∀B ∈Σa, A−1(B) ∈Σc.\nThe abstract measure µa is the pushforward of the concrete measure µc via A,\ndeﬁned by µa(B) = µc(A−1(B)) for all B ∈Σa, ensuring the preservation of\nprobabilistic information through the abstraction process.\n3.2\nDivergent Abstraction (One-to-Many)\nDivergent abstraction allows a single set of low-level evidence to be abstracted\ninto multiple high-level theories or concepts, as illustrated below:\n(Ωc, Σc, µc)\n(Ωa(1), Σa(1), µa(1))\n(Ωa(2), Σa(2), µa(2)) . . . . . . (Ωa(i), Σa(i), µa(i))\n[[A1]]\n[[A2]]\n[[Ai]]\nDeﬁnition 2 (Divergent Abstraction): Given a concrete probability space\n(Ωc, Σc, µc), divergent abstraction maps this space to a collection of abstract\nprobability spaces {(Ωa(i), Σa(i), µa(i))}i∈I, where I is an index set, via measur-\nable functions {Ai : Ωc →Ωa(i)}i∈I. Each Ai is measurable, ensuring that for\nevery i ∈I and Ai ∈Σa(i), A−1\ni\n(Ai) ∈Σc. The uniﬁed abstract sigma-algebra Σ\n′\nis deﬁned as σ\n\u0000S\ni∈I Ai(Σc)\n\u0001\n, the smallest sigma-algebra containing the union of\n\n\n6\nN. Upreti and V. Belle\nthe images of Σc under all Ai. A uniﬁed abstract measure µ\n′ on Σ\n′ is constructed\nsuch that for any E ∈Σ\n′, µ\n′(E) integrates or aggregates µa(i)(E ∩Ωa(i)) for all\ni ∈I, normalized to ensure µ\n′ is a probability measure.\n3.3\nConvergent Abstraction (Many-to-One)\nConversely, in convergent abstraction, multiple sets of low-level evidence are\nintegrated into a single high-level theory, allowing for a comprehensive synthesis\nof low-level evidence through the abstraction process as illustrated below:\n(Ωa, Σa, µa)\n(Ωc(1), Σc(1), µc(1))\n(Ωc(2), Σc(2), µc(2)) . . . . . . (Ωc(i), Σc(i), µc(i))\n[[A1]]\n[[A2]]\n[[Ai]]\nDeﬁnition 3 (Convergent Abstraction): Let {(Ωc(i), Σc(i), µc(i))}i∈I be\na collection of concrete probability spaces and (Ωa, Σa, µa) an abstract proba-\nbility space. Convergent abstraction is achieved through measurable functions\n{Ai : Ωc(i) →Ωa}i∈I, which map elements from Ωc(i) to Ωa, thereby aggre-\ngating disparate concrete spaces into a single abstract space. The formal char-\nacterization includes two main criteria: ﬁrst, each Ai must be measurable, en-\nsuring that for every B ∈Σa, the pre-image A−1\ni\n(B) is measurable in Σc(i);\nformally, ∀i ∈I, ∀B ∈Σa, A−1\ni (B) ∈Σc(i). Second, the abstract measure µa\nintegrates the measures from all concrete spaces, deﬁned for any B ∈Σa as\nµa(B) = P\ni∈I µc(i)(A−1\ni\n(B)), with appropriate normalization to ensure µa re-\nmains a probability measure, thus coherently consolidating probabilistic infor-\nmation from multiple concrete spaces into the abstract space.\n4\nAdvancing into Hierarchical Probabilistic Abstraction\nUsing the foundational types of abstraction (direct, convergent, and divergent)\nvarious hierarchical taxonomies can be constructed by combining these single-\nlayered abstractions into structured, tree-like models. Layering direct abstrac-\ntions sequentially creates a multi-layered hierarchy known as sequential abstrac-\ntion, where each level builds directly on the previous one, enabling a systematic\nprogression from low-level details to high-level concepts.\nBeyond simple sequential structures, hybrid hierarchical probabilistic ab-\nstractions introduce more complex conﬁgurations. For instance, a model begin-\nning with a divergent layer followed by sequential direct abstractions is termed\na divergent sequential abstraction. A divergent sequential abstraction enables\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n7\nmultiple sequential abstractions on the same foundational data, whereas a con-\nvergent sequential abstraction starts with diﬀerent foundational spaces, applies\nsequential abstractions to each, and uniﬁes them at higher levels.\nThis structured approach enables the construction of hierarchical models that\nmaintain consistency and interpretability across abstraction layers, supporting\ndiverse modeling needs. In certain cases, divergent abstractions may even sim-\nplify into a sequential form when no further divergent paths exist, underscoring\nthe importance of understanding structural relationships between abstraction\ntypes. By grounding higher-level abstractions in low-level details, we ensure that\nthe entire hierarchy retains coherence and accuracy across layers.\nIn our approach to representing hierarchical abstract representations, we cat-\negorize into two types of Hierarchical Probabilistic Abstraction Models (HPAMs)\nbased on the complexities they capture:\n1. HPAM-DAG (Directed Acyclic Graphs) 2: Uses a tree-like structure for sys-\ntems with clear, sequential progressions and no cycles. This type is ideal for\nunidirectional hierarchical relationships, simplifying analysis.\n2. HPAM-CD (Cyclical and Dynamic): Captures systems with feedback loops,\ncycles, and dynamic interactions, reﬂecting real-world complexity. Due to\nthese complexities, this type requires more formal and mathematical treat-\nment and is reserved for future work.\nFor now, we focus exclusively on HPAM-DAGs to simplify and analyze com-\nplex probabilistic systems, ensuring coherent, unidirectional mapping and main-\ntaining clear hierarchical relationships.\nDeﬁnition 4 (HPAM-DAG): A Hierarchical Probabilistic Abstraction,\ndenoted as H, is deﬁned as a triple (V, E, {Pv}v∈V) where:\n– V is a set of vertices in a Directed Acyclic Graph (DAG), each corresponding\nto a distinct probabilistic space.\n– E ⊆V × V is a set of directed edges in the DAG, each representing an\nabstraction mapping between probabilistic spaces.\n– {Pv}v∈V is a family of probabilistic spaces associated with vertices in V. For\neach vertex v, the probabilistic space Pv is a tuple (Ωv, Σv, Pv), where:\n• Ωv is the sample space for vertex v, representing all possible outcomes.\n• Σv is a σ-algebra over Ωv, deﬁning the set of events for which probabil-\nities are assigned.\n• Pv : Σv →[0, 1] is a probability measure that assigns probabilities to\nevents in Σv.\n2 The decision to employ DAG-based models is motivated by their ability to enable a\nclear, systematic transition from concrete to abstract representations in a unidirec-\ntional manner, facilitating straightforward analysis and interpretation of probabilis-\ntic relationships.\n\n\n8\nN. Upreti and V. Belle\nFor every directed edge (vi, vj) ∈E within the DAG, there exists an ab-\nstraction mapping Aij : Ωvi →Ωvj that facilitates a transformation between\nprobabilistic spaces Pvi and Pvj. This mapping satisﬁes the condition:\nPvj(Avj) = Pvi(A−1\nij (Avj)),\n(1)\nfor all Avj ∈Σvj, where A−1\nij (Avj) is the pre-image of Avj under Aij, ensuring\nthe preservation of probabilistic measures through the abstraction process.\n4.1\nBoundary of Abstraction\nEstablishing a clear boundary for abstraction in probabilistic models is essential,\nand the Highest Possible Abstraction (HPoA) serves to mark this boundary. Be-\nyond the HPoA, further abstraction risks obscuring critical probabilistic or rela-\ntional details, diminishing the model’s interpretability and analytical rigor. The\nHPoA thus deﬁnes the upper limit of meaningful abstraction, ensuring that each\nlayer within the abstraction hierarchy contributes substantively to the model’s\nintegrity. This approach preserves essential information at every level, supporting\nthe construction of hierarchical models that are both robust and interpretable.\nDeﬁnition 5 (HPoA). Let’s consider a HPAM-DAG denoted as H =\n(V, E, {Pv}v∈V). Within this framework, HPoA is formalized as a speciﬁc prob-\nabilistic space (ΩHP oA, ΣHP oA, PHP oA) that meets the following rigorously de-\nﬁned criteria:\n1. Preservation of Probabilistic Integrity: For every set AHP oA ∈ΣHP oA,\nit holds that PHP oA(AHP oA) is congruent with the probabilistic measure\nassigned to its corresponding set in any precursor probabilistic space within\nH. Formally, this means:\n∀AHP oA ∈ΣHP oA, ∃APv ∈ΣPv for Pv ∈{Pv}v∈V :\nPHP oA(AHP oA) = PPv(A−1\nv→HP oA(AHP oA)),\nwhere Av→HP oA denotes the abstraction mapping from Ωv to ΩHP oA.\n2. Maximal Generalization: (ΩHP oA, ΣHP oA, PHP oA) achieves the highest\ndegree of abstraction permissible under the constraints of probabilistic in-\ntegrity. No further abstraction (Ω′, Σ′, P ′) can be derived from HPoA with-\nout a resultant loss in the preservation of essential probabilistic relationships.\nThis is expressed as: ̸ ∃(Ω′, Σ′, P ′) : ΣHP oA ⊊Σ′ ∧∀A′ ∈Σ′, P ′(A′) ̸=\nPHP oA(A−1\nHP oA→′(A′)).\nIn essence, the HPoA, (ΩHP oA, ΣHP oA, PHP oA), encapsulates the terminus\nof the abstraction process within H, marking the juncture beyond which no\nadditional abstraction can be achieved without compromising the model’s fun-\ndamental probabilistic structure.\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n9\nProposition 1 (Existence of the HPoA in HPAM-DAG). Given a\nHPAM-DAG H = (V, E, {Pv}v∈V), there exists at least one HPoA, (ΩHP oA\n, ΣHP oA, PHP oA), such that for any probabilistic space (Ω′, Σ′, P ′) that can be\nderived from (ΩHP oA, ΣHP oA, PHP oA) via an abstraction mapping, it results in\nthe loss of at least one essential probabilistic relationship.\n5\nHPAM-DAG Types\nDiﬀerent types of HPAM-DAGs emerge as we combine foundational abstraction\ntypes in various ways. Each variant of HPAM-DAG has unique properties based\non the speciﬁc combination and conﬁguration of these foundational mappings.\nBy arranging these mappings into multilayered structures, we can create HPAM-\nDAGs that represent increasing levels of complexity within probabilistic systems.\nIn this section, we outline two types of HPAM-DAGs: sequential abstraction\nand a more complex hybrid variant. While these examples provide a foundation,\nit is important to note that many other types of HPAM-DAGs can be formally\ndeﬁned by varying combinations of foundational abstraction types. These addi-\ntional conﬁgurations oﬀer potential avenues for exploration in future works.\n5.1\nSequential Abstraction\nSequential abstraction represents the simplest form of HPAM, as it layers direct\nabstractions in a straightforward, linear progression from low-level to high-level\nconcepts, without incorporating branching or merging complexities.\nDeﬁnition 6 (Sequential Abstractions): Let (Ω0, Σ0, P0) represent the\nfoundational probability space. Sequential abstraction is characterized by a ﬁnite\nseries of probability spaces {(Ωi, Σi, Pi)}n\ni=0, where each space (Ωi+1, Σi+1, Pi+1)\nis derived from (Ωi, Σi, Pi) via an abstraction operation Ai. The culmination of\nthis process, the HPoA, (ΩHP oA, ΣHP oA, PHP oA), is distinguished as the end-\npoint of the sequence. For each i, ranging from 1 to n, this is formally represented\nas:\nAi : (Ωi−1, Σi−1, Pi−1) →(Ωi, Σi, Pi),\nunder the stipulations that: (1) for every measurable set A ∈Σi−1, the condi-\ntion Pi−1(A) = Pi(Ai(A)) holds true (Preservation of Probability Mass), and\n(2) there is no σ-algebra smaller than Σi, denoted as Σ′ ⊊Σi, for which the\npreservation of probability mass criteria remains valid (Minimality).\nProposition 2 (Uniqueness of HPoA in Sequential Abstractions):\nGiven a concrete probability space (Ω0, Σ0, P0), the HPoA, (ΩHP oA, ΣHP oA,\nPHP oA), achieved through a sequential abstraction process is unique. This im-\nplies that for any two HPoAs, (ΩHP oA, ΣHP oA, PHP oA) and (Ω′\nHP oA, Σ′\nHP oA,\nP ′\nHP oA) derived from (Ω0, Σ0, P0) via any sequence of abstraction operations, it\nmust hold that ΣHP oA = Σ′\nHP oA.\n\n\n10\nN. Upreti and V. Belle\nProposition 3 (Existence of Intermediate States): Given a concrete\nprobability space (Ω0, Σ0, P0) and a HPoA, (ΩHP oA, ΣHP oA, PHP oA), obtained\nthrough a sequential abstraction process, for any intermediate abstraction (Ωi, Σi\n, Pi) where 0 < i < n and (Ωi, Σi, Pi) ̸= (ΩHP oA, ΣHP oA, PHP oA), there exists at\nleast one intermediate state (Ωint, Σint, Pint) that facilitates the decomposition\nof the abstraction sequence into:\n[[Apre]] : Ω0 →Ωint\nand\n[[Apost]] : Ωint →ΩHP oA.\nProperty 1 (Comprehensibility): Given the existence of an intermediate\nabstraction state (Ωint, Σint, Pint), there exists a structured pathway, [[Apre]] :\nΩ0 →Ωint and [[Apost]] : Ωint →ΩHP oA, that enhances the comprehensibility\nof the abstraction process. This is formalized as the ability to sequentially de-\nconstruct the abstraction hierarchy into comprehensible steps, each represented\nby measurable transformations preserving essential probabilistic structures.\nInspired by Ai et al. ﬁndings on sequential learning tasks, our abstraction\nprocess is designed with intermediate stages to enhance comprehensibility [1].\nAi et al. noted that structured and sequential presentation of concepts improves\nhuman understanding. By preserving intermediate abstraction states, we ensure\neach step remains comprehensible and accessible.\nProperty 2 (Tractability): The identiﬁcation and utilization of the in-\ntermediate abstraction state (Ωint, Σint, Pint) within the sequential abstraction\nprocess conﬁrm the modular nature of the abstraction and signiﬁcantly improve\nthe tractability of conducting analyses across the hierarchy. This modularity al-\nlows for targeted adjustments and reﬁnements at speciﬁc levels of abstraction,\neﬀectively navigating and understanding probabilistic relationships at each hi-\nerarchical level.\nEnsuring tractability is vital for the practical application of abstraction\nmethodologies. Results from Holtzen et al. demonstrate that maintaining com-\nputational manageability at each step allows for eﬃcient analysis and reasoning,\nmaking the overall process more eﬀective [21,22].\n5.2\nHybrid HPAMs\nHybrid HPAM-DAGs amalgamate sequential, divergent, and convergent abstrac-\ntion methodologies within hierarchical probabilistic modeling to adeptly cap-\nture the complexities of intricate systems. Deﬁned over a base probability space\n(Ω0, Σ0, P0), Hybrid HPAM-DAG employs a dynamic mix of abstraction op-\nerations that either progress linearly, branch out divergently, and/or converge\nfrom various sequences into a uniﬁed model. This integration allows for a nu-\nanced exploration and understanding of the system’s probabilistic behaviors from\nmultiple perspectives, ensuring the model comprehensively reﬂects the system’s\ndynamics and dependencies. By leveraging the strengths of each abstraction\nmethod, Hybrid HPAM-DAG oﬀers a versatile framework that maintains the\nsystem’s probabilistic integrity while navigating through its complex landscape,\nmaking it a powerful tool for modeling sophisticated systems with high ﬁdelity.\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n11\nTo exemplify, consider a hybrid model that combines direct, sequential, di-\nvergent, and convergent abstraction processes to model the dynamic nature of\ncomplex systems like Alzheimer’s disease. It begins with a foundational concrete\nspace (Ω0, Σ0, P0), where Ω0 represents the population at risk for Alzheimer’s\ndisease, Σ0 includes measurable events indicating risk factors for Alzheimer’s,\nand P0 quantiﬁes the initial distribution of these risk factors. For more details,\nrefer to the appendix section for a pseudocode algorithm.\nSequential Abstraction: On the ﬁrst layer, the abstraction transitions the\nmodel from broad risk factors to speciﬁc biological markers indicative of Alzheimer’s:\nA1 : (Ω0, Σ0, P0) →(Ω1, Σ1, P1)\nwhere (Ω1, Σ1, P1) focuses on biological markers such as amyloid-beta levels and\ntau protein tangles. The model employs sequential abstractions to reﬁne the\nunderstanding of how these biological markers inﬂuence disease progression:\nA2 : (Ω1, Σ1, P1) →(Ω2, Σ2, P2)\nwhere (Ω2, Σ2, P2) might include cognitive decline metrics and early symptoms\nof Alzheimer’s disease.\nDivergent Abstraction: From this point, the model branches into distinct\nintervention pathways:\nA3,i : (Ω2, Σ2, P2) →(Ω3,i, Σ3,i, P3,i),\ni ∈{1, 2}\nEach path i represents a diﬀerent intervention strategy, such as cognitive therapy\nor medication, with outcomes speciﬁc to those treatments. For example:\n1. A3,1 could represent cognitive therapy, focusing on improving cognitive func-\ntions through exercises and mental activities.\n2. A3,2 could represent medication, focusing on pharmacological treatments\naimed at slowing the progression of the disease.\nConvergent Abstraction: The insights from these divergent paths are then\nsynthesized into a uniﬁed model:\nA4 : (Ω3,1 ∪Ω3,2, Σ3,1 ∪Σ3,2, P3,1 ⊕P3,2) →(Ω4, Σ4, P4)\nresulting in a comprehensive understanding of the eﬃcacy and outcomes of var-\nious Alzheimer’s disease management strategies.\nThis hybrid methodology can be applied in several contexts: integrating ge-\nnetic, environmental, and clinical data to study Alzheimer’s disease progression\nand treatment eﬀectiveness in research; developing personalized treatment plans\nthat consider multiple intervention strategies in clinical practice; and inform-\ning healthcare policies based on comprehensive models that incorporate various\n\n\n12\nN. Upreti and V. Belle\nrisk factors and treatment outcomes in policy making. The Hybrid HPAM-DAG\nframework eﬀectively integrates sequential, divergent, and convergent abstrac-\ntion processes, providing a structured approach to understanding and managing\nAlzheimer’s disease. It progressively abstracts and reﬁnes data, oﬀering valuable\ninsights at each hierarchical level.\n6\nRelated Works\nAbstraction serves as a critical tool for simplifying and understanding the com-\nplexity inherent in drawing analogies, parsing through inherent relational in-\nformation, employing reasoning by formally representing uncertainty [33,16,13].\nThe work of Saitta and Zucker, for instance, provides a comprehensive explo-\nration of abstraction, from its fundamental role in planning and solving con-\nstraint satisfaction problems to its implementation in the coordination of multi-\nagent systems [33]. Moreover, abstraction plays a crucial role in domains such\nas program analysis and veriﬁcation, probabilistic games, labeled transition sys-\ntems, probabilistic programs, complex systems, and inductive logic program-\nming, enhancing the innovative problem-solving capabilities and methodological\ndevelopments within each ﬁeld [2,8,34,17,7,29,30]. While some research has con-\ncentrated on the speciﬁc challenges and opportunities presented by probabilistic\nsettings, others have oﬀered theoretical insights on ideal speciﬁcations and the\ndynamics of mappings within abstractions more generally [27,11,25]. Through\nthese diverse treatments, abstraction emerges not only as a key concept for\ntheoretical exploration but also as a practical tool for accommodating complex\nproblem-solving methodologies across a spectrum of scientiﬁc inquiries [31].\nNumerous ﬁelds have developed sophisticated frameworks to formalize ab-\nstraction, providing a mathematical foundation for this complex concept and\noﬀering structured analyses of the abstraction process [14,2]. Milner’s work on\nthe semantics of concurrent processes has signiﬁcantly inﬂuenced our exploration\nof algebraic formalisms for probabilistic abstraction [28]. Similarly, Hennessy and\nMilner’s algebraic laws, which introduce observational congruence to study non-\ndeterministic and concurrent programs, oﬀer valuable insights into comparing\ndiﬀerent levels of abstractions within probabilistic abstraction hierarchies [19].\nFurther, studies on probabilistic bisimulation and cocongruence for probabilistic\nsystems could inspire novel approaches in this area as exempliﬁed by the work\nby Hofmann and Belle on abstracting noisy robot programs [10,32,20,5]. The\nalgebraic tools as discussed, developed for deﬁning and verifying concurrent pro-\ngram properties, hold promise for extending into designing and verifying proba-\nbilistic abstraction hierarchies. However, practical application of these theories\nhas been mixed—some have been successfully integrated into real-world applica-\ntions, while others face challenges in bridging theoretical insights with practical\nimplementation [14,27,6].\nIn Belle’s work, for instance, a novel semantic framework for analyzing and\nabstracting probabilistic models is unveiled, signiﬁcantly contributing to the dis-\ncourse on abstraction [4]. This research builds upon and extends the foundational\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n13\nwork of Banihashemi et al., which investigated isomorphism and thorough ab-\nstractions in the context of situation calculus agent programs, primarily from\na non-probabilistic perspective [2]. Belle aims to adapt these foundational con-\ncepts to probabilistic settings, such as in probabilistic relational models (PRMs),\nthrough the introduction of unweighted abstractions that are compatible with\ncategorical settings in terms of satisfaction and entailment [4,2]. This founda-\ntion facilitates the advancement of weighted abstractions and the integration\nof evidence, while also examining the linkage between these abstractions and\nstochastic models, notably through the lens of weak exact abstractions and the\nautomation of abstraction generation.\nMoreover, Belle also highlights the potential of incorporating methodolo-\ngies from Giunchiglia and Walsh’s research on logical theory operations into the\nprobabilistic framework, necessitating some adjustments [14,4]. Albeit with some\nlimitations in the unweighted and weighted approaches outlined, Belle provides\ndetailed discussions on Banihashemi et al.’s approach of abstraction in knowl-\nedge representation and automated planning, especially hierarchical planning.\nBelle’s work opens new avenues for both theoretical exploration and practical\napplication within probabilistic modeling and planning [14,2,4,23]. We start our\ninquiry by following up on the call for newer approaches in representing abstrac-\ntion hierarchies as Belle and Banihashemi et al. outline [2,4].\nFurther, Holtzen et al. explores the concept of abstraction within probabilistic\nprogramming, speciﬁcally investigating how to abstract a probability distribu-\ntion deﬁned by such a program [21,22]. The study identiﬁes and automatically\ngenerates abstractions that semantically represent conditional independence as-\nsumptions, thereby aiding in the design of algorithms for simplifying inference\nprocesses. This is achieved by adapting techniques from program veriﬁcation to\nanalyze deterministic code, applying a reﬁned form of predicate abstraction to\ncreate abstract probabilistic programs that preserve the essential characteristics\nof the original programs [21] . A key contribution of their work is the introduc-\ntion of the concept of distributional soundness, which ensures the consistency\nof probability distributions between abstract and concrete programs. Holtzen\net al. outlines a theory and methodology for developing distributionally sound\nabstractions across a broad range of probabilistic programs, leveraging complex\nindependence structures to streamline the original program [21]. Through this\napproach, Holtzen et al. illustrates the practical advantages of abstraction, such\nas enhanced eﬃciency in inference procedures, across various statistical models\nformulated as probabilistic programs [21,22].\n7\nDiscussion\nOur approach seeks to unify diverse theoretical perspectives while advancing the\nunderstanding and practical application of abstraction, bridging the gap between\ntheoretical foundations and real-world utility in hierarchical representations. De-\nspite its strengths, our framework has several limitations. The integration of\nmultiple abstraction layers and the hybrid nature of our framework can lead to\n\n\n14\nN. Upreti and V. Belle\nincreased computational complexity, potentially making it less scalable for very\nlarge datasets or highly complex systems. While our approach aims to enhance\ninterpretability by structuring abstractions hierarchically, there is a trade-oﬀbe-\ntween the level of detail retained and the simplicity of the abstracted models,\nmaking it challenging to balance these aspects. Furthermore, our framework,\nthough robust in the contexts we’ve explored, may face challenges when gener-\nalized to entirely new domains or types of data that were not considered during\nits development.\nThe eﬀectiveness of our framework heavily relies on the quality and complete-\nness of the initial data. Poor quality data can lead to inaccurate abstractions and\npotentially ﬂawed conclusions. Additionally, the hierarchical nature and the need\nfor intermediate abstractions require signiﬁcant computational resources, which\nmay not be readily available in all settings. The current use of Directed Acyclic\nGraphs (DAGs) for representation, while useful for capturing dependencies and\ncausal relationships, can be limiting in expressing more complex, multi-layered\nabstractions. A more rigorous and holistic framework is necessary to represent\nabstraction hierarchies eﬀectively, ensuring that multi-layered representations\nare comprehensively understood and utilized.\nThis work introduces the concepts of hierarchical probabilistic abstraction\nand a speciﬁc hybrid approach, yet many other taxonomies and abstraction\nmethods remain unexplored and warrant further study to fully understand ab-\nstraction frameworks. Recognizing the importance of hierarchical representations\nfor interpreting multi-layered abstractions, future work will provide a formal\ntreatment of this framework by developing a rigorous mathematical founda-\ntion and delivering a comprehensive analysis of the Hybrid HPAM-DAG ap-\nproach. We also intend to address current limitations by reﬁning computational\nstrategies, investigating additional taxonomies of abstraction, and conducting\nextensive testing across diverse domains and applied settings. These eﬀorts will\nenhance our approach, expand its applicability, and ultimately lead to a more\nrobust and holistic representation of abstraction hierarchies.\n8\nConclusion\nIn this paper, we introduce a conceptual framework for hierarchical probabilistic\nabstraction that extends a measure-theoretic foundation to address challenges\nin modeling complex systems. By structuring the abstraction process into lay-\nered mappings, this framework supports modular problem-solving and allows for\nboth detailed layer-speciﬁc analysis and a cohesive, system-wide understanding.\nThis dual-level approach enhances interpretability and computational tractabil-\nity by bridging high-level conceptual insights with low-level perceptual data. The\nmodular structure ensures that each layer of the abstraction hierarchy can be in-\ndependently developed, analyzed, and comprehended, fostering the construction\nof interpretable and tractable models. Additionally, this hierarchical framework\nprovides ﬂexibility in integrating multiple abstraction levels, facilitating the ex-\nploration of complex phenomena with both depth and breadth.\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n15\nReferences\n1. Ai, L., Langer, J., Muggleton, S.H., Schmid, U.: Explanatory machine learning for\nsequential human teaching. Machine Learning 112(10), 3591–3632 (2023)\n2. Banihashemi, B., De Giacomo, G., Lespérance, Y.: Abstraction in situation calculus\naction theories. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence.\nvol. 31 (2017)\n3. Beckers, S., Eberhardt, F., Halpern, J.Y.: Approximate causal abstractions. In:\nUncertainty in artiﬁcial intelligence. pp. 606–615. PMLR (2020)\n4. Belle, V.: Abstracting probabilistic models: Relations, constraints and beyond.\nKnowledge-Based Systems 199, 105976 (2020)\n5. Castellani, I.: Bisimulations and abstraction homomorphisms. In: Colloquium on\nTrees in Algebra and Programming. pp. 223–238. Springer (1985)\n6. Clarke, E., Grumberg, O., Jha, S., Lu, Y., Veith, H.: Counterexample-guided ab-\nstraction reﬁnement. In: Computer Aided Veriﬁcation: 12th International Confer-\nence, CAV 2000, Chicago, IL, USA, July 15-19, 2000. Proceedings 12. pp. 154–169.\nSpringer (2000)\n7. Cousot, P., Monerau, M.: Probabilistic abstract interpretation. In: European Sym-\nposium on Programming. pp. 169–193. Springer (2012)\n8. Cropper, A., Muggleton, S.: Learning higher-order logic programs through abstrac-\ntion and invention. In: Twenty-Fifth International Joint Conference on Artiﬁcial\nIntelligence. Association for the Advancement of Artiﬁcial Intelligence (2016)\n9. Danks, D.: Unifying the mind: Cognitive representations as graphical models. Mit\nPress (2014)\n10. Danos, V., Desharnais, J., Laviolette, F., Panangaden, P.: Bisimulation and cocon-\ngruence for probabilistic systems. Information and Computation 204(4), 503–523\n(2006)\n11. Dehnert, C., Gebler, D., Volpato, M., Jansen, D.N.: On abstraction of probabilistic\nsystems. International Autumn School on Rigorous Dependability Analysis Using\nModel Checking Techniques for Stochastic Systems pp. 87–116 (2012)\n12. Fagin, R., Halpern, J.Y.: Reasoning about knowledge and probability. Journal of\nthe ACM (JACM) 41(2), 340–367 (1994)\n13. Gentner, D., Hoyos, C.: Analogy and abstraction. Topics in cognitive science 9(3),\n672–693 (2017)\n14. Giunchiglia, F., Walsh, T.: A theory of abstraction. Artiﬁcial intelligence 57(2-3),\n323–389 (1992)\n15. Gubar, E., Taynitskiy, V., Fedyanin, D., Petrov, I.: Hierarchical epidemic model on\nstructured population: Diﬀusion patterns and control policies. Computation 10(2),\n31 (2022)\n16. Halpern, J.Y.: Reasoning about uncertainty. MIT press (2017)\n17. Halpern, J.Y., Koller, D.: Representation dependence in probabilistic inference.\nJournal of Artiﬁcial Intelligence Research 21, 319–356 (2004)\n18. Han, W., Kang, X., He, W., Jiang, L., Li, H., Xu, B.: A new method for disease\ndiagnosis based on hierarchical brb with power set. Heliyon 9(2) (2023)\n19. Hennessy, M., Milner, R.: Algebraic laws for nondeterminism and concurrency.\nJournal of the ACM (JACM) 32(1), 137–161 (1985)\n20. Hofmann, T., Belle, V.: Abstracting noisy robot programs. In: Proceedings of the\n2023 International Conference on Autonomous Agents and Multiagent Systems.\npp. 534–542 (2023)\n\n\n16\nN. Upreti and V. Belle\n21. Holtzen, S., Broeck, G., Millstein, T.: Sound abstraction and decomposition of\nprobabilistic programs. In: International Conference on Machine Learning. pp.\n1999–2008. PMLR (2018)\n22. Holtzen, S., Millstein, T., Van den Broeck, G.: Probabilistic program abstractions.\nIn: Proceedings of the 33rd Conference on Uncertainty in Artiﬁcial Intelligence\n(UAI) (2017)\n23. Junges, S., Spaan, M.T.: Abstraction-reﬁnement for hierarchical probabilistic mod-\nels. In: International Conference on Computer Aided Veriﬁcation. pp. 102–123.\nSpringer (2022)\n24. Kahneman, D.: Thinking, fast and slow. Farrar, Straus and Giroux (2011)\n25. Koller, D., Friedman, N.: Probabilistic graphical models: principles and techniques.\nMIT press (2009)\n26. Lüdtke, S., Schröder, M., Krüger, F., Bader, S., Kirste, T.: State-space abstractions\nfor probabilistic inference: a systematic review. Journal of Artiﬁcial Intelligence\nResearch 63, 789–848 (2018)\n27. McIver, A., Morgan, C.: Abstraction, reﬁnement and proof for probabilistic sys-\ntems. Springer Science & Business Media (2005)\n28. Milner, R.: Operational and algebraic semantics of concurrent processes. In: Formal\nModels and Semantics, pp. 1201–1242. Elsevier (1990)\n29. Monniaux, D.: An abstract monte-carlo method for the analysis of probabilistic\nprograms. In: Proceedings of the 28th ACM SIGPLAN-SIGACT symposium on\nPrinciples of programming languages. pp. 93–101 (2001)\n30. Muggleton, S., De Raedt, L.: Inductive logic programming: Theory and methods.\nThe Journal of Logic Programming 19, 629–679 (1994)\n31. Murphy, K.P.: Machine learning: a probabilistic perspective. MIT press (2012)\n32. Panangaden, P.: Probabilistic bisimulation. ACM SIGLOG News 2(3), 72–84\n(2015)\n33. Saitta, L., Zucker, J.D.: Abstraction in Artiﬁcial Intelligence. Springer (2013)\n34. Sandholm, T., Singh, S.: Lossy stochastic game abstraction with bounds. In: Pro-\nceedings of the 13th ACM Conference on Electronic Commerce. pp. 880–897 (2012)\n35. Segal, E., Koller, D., Ormoneit, D.: Probabilistic abstraction hierarchies. Advances\nin Neural Information Processing Systems 14 (2001)\n\n\nAn Algebraic Framework for Hierarchical Probabilistic Abstraction\n17\nA\nHybrid HPAM for Alzheimer’s Disease Management\nIn this section, we provide a high-level pseudocode algorithm to illustrate the\napplication of HPAM-DAGs in Alzheimer’s disease management. The algorithm\ndemonstrates how direct, divergent, and convergent abstractions can model the\nrelationships among risk factors, biological markers, treatment pathways, and\npatient outcomes.\nPseudocode Algorithm\n1: Initialize DAG G with initial probability space (Ω0, Σ0, P0).\n2: Initialize (Final_Model, HPoA) = {}, HPoA = undeﬁned.\n3: while True do\n4: Step 1: Apply Direct Abstraction to map low-level risk factors to biological markers.\n5: for each risk_factor in (Ω0, Σ0, P0) do\n6: Deﬁne Direct Abstraction:\n(Ω1, Σ1, P1) = ADirect((Ω0, Σ0, P0))\n7: Add Node G : (Ω1, Σ1, P1).\n8: Add Edge G : (Ω0, Σ0, P0) →(Ω1, Σ1, P1).\n9: end for\n10: Step 2: Apply Divergent Abstraction to generate multiple treatment pathways.\n11: for each (Ω1, Σ1, P1) in Bio_Markers do\n12: Deﬁne Divergent Abstraction:\n(Ω2i , Σ2i, P2i ) = ADivergent((Ω1, Σ1, P1))\n13: Add Node G : (Ω2i , Σ2i, P2i ).\n14: Add Edge G : (Ω1, Σ1, P1) →(Ω2i , Σ2i, P2i ).\n15: end for\n16: Step 3: Apply Convergent Abstraction to unify treatment outcomes.\n17: Combine using Convergent Abstraction:\n(Ω3, Σ3, P3) = AConvergent({(Ω21 , Σ21, P21 ), . . . , (Ω2n, Σ2n, P2n )})\n18: Add Node G : (Ω3, Σ3, P3).\n19: Add Edge G : {(Ω21 , Σ21, P21 ), . . . , (Ω2n , Σ2n, P2n )} →(Ω3, Σ3, P3).\n20: Step 4: Calculate Highest Possible Abstraction (HPoA).\n21: Construct HPoA:\n(ΩHPoA, ΣHPoA, PHPoA) = AHPoA((Ω3, Σ3, P3))\n22: Step 5: Validate if HPoA matches observed outcomes in Alzheimer’s patients.\n23: if PHPoA matches observed outcomes O then\n24:\nreturn Success: Valid HPoA achieved for Alzheimer’s Disease Management.\n25: else if no further abstractions can be made\n26:\nreturn Failure: HPoA cannot be constructed.\n27: else\n28:\nPerform interventions or update models and iterate.\n29:\nUpdate DAG G with new treatment data.\n30: end if\n31: end while\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21216v1.pdf",
    "total_pages": 17,
    "title": "An Algebraic Framework for Hierarchical Probabilistic Abstraction",
    "authors": [
      "Nijesh Upreti",
      "Vaishak Belle"
    ],
    "abstract": "Abstraction is essential for reducing the complexity of systems across\ndiverse fields, yet designing effective abstraction methodology for\nprobabilistic models is inherently challenging due to stochastic behaviors and\nuncertainties. Current approaches often distill detailed probabilistic data\ninto higher-level summaries to support tractable and interpretable analyses,\nthough they typically struggle to fully represent the relational and\nprobabilistic hierarchies through single-layered abstractions. We introduce a\nhierarchical probabilistic abstraction framework aimed at addressing these\nchallenges by extending a measure-theoretic foundation for hierarchical\nabstraction. The framework enables modular problem-solving via layered\nmappings, facilitating both detailed layer-specific analysis and a cohesive\nsystem-wide understanding. This approach bridges high-level conceptualization\nwith low-level perceptual data, enhancing interpretability and allowing layered\nanalysis. Our framework provides a robust foundation for abstraction analysis\nacross AI subfields, particularly in aligning System 1 and System 2 thinking,\nthereby supporting the development of diverse abstraction methodologies.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}