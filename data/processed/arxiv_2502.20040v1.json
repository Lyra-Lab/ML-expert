{
  "id": "arxiv_2502.20040v1",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n1\nCleanMel: Mel-Spectrogram Enhancement for\nImproving Both Speech Quality and ASR\nNian Shao, Rui Zhou, Pengyu Wang, Xian Li, Ying Fang, Yujie Yang and Xiaofei Li\nAbstract—In this work, we propose CleanMel, a single-channel\nMel-spectrogram denoising and dereverberation network for\nimproving both speech quality and automatic speech recogni-\ntion (ASR) performance. The proposed network takes as input\nthe noisy and reverberant microphone recording and predicts\nthe corresponding clean Mel-spectrogram. The enhanced Mel-\nspectrogram can be either transformed to speech waveform\nwith a neural vocoder or directly used for ASR. The proposed\nnetwork is composed of interleaved cross-band and narrow-\nband processing in the Mel-frequency domain, for learning the\nfull-band spectral pattern and the narrow-band properties of\nsignals, respectively. Compared to linear-frequency domain or\ntime-domain speech enhancement, the key advantage of Mel-\nspectrogram enhancement is that Mel-frequency presents speech\nin a more compact way and thus is easier to learn, which will\nbenefit both speech quality and ASR. Experimental results on\nfour English and one Chinese datasets demonstrate a significant\nimprovement in both speech quality and ASR performance\nachieved by the proposed model. Code and audio examples of\nour model are available online 1.\nIndex Terms—Speech enhancement, Mel-frequency, speech\ndenoising, speech dereverberation, automatic speech recognition\nI. INTRODUCTION\nT\nHIS work studies single-channel speech enhancement\nusing deep neural networks (DNNs), to improve both\nspeech quality and automatic speech recognition (ASR) perfor-\nmance. A large class of speech enhancement methods employ\nDNNs to map from noisy and reverberant speech to corre-\nsponding clean speech, conducted either in time domain [1],\n[2] or in time-frequency domain [3]–[5]. These methods can\nefficiently suppress noise, but not necessarily improve ASR\nperformance due to the speech artifacts/distortions caused\nby speech enhancement networks [6]. In [7], it was found\nthat time-domain enhancement is more ASR-friendly than\nfrequency-domain enhancement. A time domain progressive\nlearning method is proposed in [8], which also shows the\nsuperiority of time domain speech enhancement, and the\nprogressive learning mechanism is very effective for robust\nASR by mitigating the over suppression of speech. In [9], [10],\nASR performance is largely improved by decoupling frontend\nNian Shao, Pengyu Wang, Ying Fang and Yujie Yang are with Zhejiang\nUniversity, and also with Westlake University, Hangzhou, China (e-mail:\n{shaonian, wangpengyu, fangying, yangyujie}@westlake.edu.cn).\nRui Zhou, Xian Li and Xiaofei Li are with the School of Engineering,\nWestlake University, and also with the Institute of Advanced Technology,\nWestlake Institute for Advanced Study, Hangzhou, China (e-mail: {zhourui,\nlixian, lixiaofei}@westlake.edu.cn).\nNian Shao and Rui Zhou equally contributed to this work. Corresponding:\nlixiaofei@westlake.edu.cn\n1https://audio.westlake.edu.cn/Research/CleanMel.html\nenhancement and backend recognition. In [10], it is shown that\nan advanced time-frequency domain network, i.e. CrossNet,\ncan even outperform the time domain ARN network.\nSpeech enhancement in Mel-frequency domain, or similarly\nin rectangular bandwidth (ERB) domain, has been developed\nunder various contexts in the literature. Mel-frequency and\nERB bands model human speech perception of spectral en-\nvelope and signal periodicity, within which speech enhance-\nment is more perceptually and computationally efficient than\nwithin linear-frequency domain or time domain. In [11]–\n[13], spectral envelope enhancement is performed within the\nERB bands, and then applying pitch filtering [11], [12] or\ndeep filtering [13] to recover the enhanced speech. Sub-band\nnetworks [3], [14] and full-band/sub-band fusion networks,\ni.e. FullSubNet [15], [16], have been recently proposed and\nachieved outstanding performance. However, separately pro-\ncessing sub-bands in the linear-frequency domain leads to a\nlarge computational complexity. To reduce the number of sub-\nbands and thus the computational complexity, Fast FullSubNet\n[17] and the work of [18] proposed to perform sub-band\nprocessing in the Mel-frequency domain, and then transform\nback to linear frequency with a joint post-processing network.\nIn [19], [20], speech enhancement is directly conducted in\nthe Mel-frequency domain and then a separate neural vocoder\nis used to recover speech waveform. These methods improve\nspeech enhancement capability by alleviating the burden of\nenhancing full-band speech details and also by leveraging\nthe powerful full-band speech recovery capacity of advanced\nneural vocoder, as a result, achieve higher speech quality\ncompared to their linear-frequency counterparts.\nIn this work, we propose CleanMel, a single-channel Mel-\nspectrogram enhancement network for improving both speech\nquality and ASR performance. Different from the previous\nworks [11]–[13], [17] that performing speech enhancement\nin the ERB or Mel domain, and then applying a joint pitch\nfiltering or deep filtering to obtain the enhanced speech, this\nwork decouples the Mel-spectrogram enhancement and post-\nprocessing parts by targeting the enhancement network with\nclean Mel-spectrogram. The enhanced Mel-spectrogram can\nbe directly used for ASR, or transformed back to wave-\nform with a separate neural vocoder as is done in [19].\nCompared to linear-frequency spectrogram or time-domain\nwaveform, Mel-frequency presents speech in a more compact\nand less-detailed way (but still perceptually efficient) and\nhas a lower feature dimension (number of frequencies) from\nthe perspective of machine learning, which would result in\nlower the prediction error. This is beneficial for both speech\nquality improvement and ASR: (i) Neural vocoders have been\narXiv:2502.20040v1  [eess.AS]  27 Feb 2025\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n2\nextensively studied in the field of Text-to-Speech, and are\ncapable of efficiently transforming Mel-spectrogram back to\ntime-domain waveform. Therefore, the low-error property of\nMel-spectrogram enhancement can be hopefully maintained by\nthe neural vocoders and higher speech quality can be achieved.\n(ii) From the perspective of ASR, there seems no need to\nfirst recover the less-accurate full-band speech details and then\ncompress to Mel-frequency. A direct and more-accurate Mel-\nspectrogram estimation would be preferred.\nWe adapt the network architecture of our previous proposed\n(online) SpatialNet [21], [22] with some modifications to better\naccommodate Mel-spectrogram enhancement. SpatialNet is\ncomposed of interleaved cross-band and narrow-band blocks\noriginally proposed for processing multichannel STFT fre-\nquencies. The narrow-band block processes STFT frequencies\nindependently to learn the spatial information presented in\nnarrow-band (one frequency), such as the convolutive sig-\nnal propagation and the spatial correlation of noise. And\nthe cross-band block was designed for learning the across-\nfrequency dependencies of narrow-band information. As for\nsingle-channel Mel-spectrogram enhancement in this work, the\nnarrow-band block processes Mel frequencies independently to\nalso learn the (single-channel) convolutive signal propagation\nof target speech, which is crucial not only for conducting\ndereverberation of target speech but also for discriminating\nbetween target speech and interfering signals. The cross-band\nblock is now reinforced and utilized to learn the full-band\nspectral pattern in the Mel-frequency domain.\nIn addition, we have studied several critical issues when\ndecoupling the Mel-spectrogram enhancement front-end and\nASR/Vocoder back-ends. i) We have systematically studied\nand compared different learning targets that can be used for\nMel-spectrogram enhancement, including logMel mapping,\nMel ratio masking and the clipping issue of logMel; ii) We\nhave developed a data normalization scheme to align the signal\nlevels of cascaded front-end and back-end models; iii) We have\ndeveloped an online neural vocoder to enable online speech\nenhancement.\nExperiments are conducted on five public datasets (four\nEnglish and one Chinese) for speech denoising and derever-\nberation individually or jointly. Importantly, we adopt a more\nrealistic evaluation setup: from multiple data sources of clean\nspeech, real-measured room impulse responses (RIRs) and\nnoise signals, we collected and organized a relatively large-\nscale training set, based on which we train the network for\nonce and directly test it on all the five test sets. Experiments\nshow that the proposed model achieves the state-of-the-art\n(SOTA) speech enhancement performance in term of speech\nperceptual quality. Moreover, on top of various pre-trained\nand advanced ASR models, the proposed model prominently\nimproves the ASR performance on all datasets. These results\ndemonstrate that our trained models have the potential to be\ndirectly employed to real applications.\nII. PROBLEM FORMULATION\nThe noisy and reverberant single-channel speech signals can\nbe represented in the time domain as\ny(n) = s(n) ∗a(n) + e(n)\n(1)\nwhere n stands for the discrete time index. s(n) and e(n)\nrepresents the clean source speech and ambient noise, respec-\ntively. a(n) denotes RIR and ∗the convolution operation.\nIn this work, only static speaker is considered, thence the\nRIR is time-invariant. RIR is composed of the direct-path\npropagation, early reflections and late reverberation.\nWe conduct joint speech denoising and dereverberation in\nthis work, which amounts to estimate the (Mel-spectrogram\nof) desired direct-path speech x(n) = s(n) ∗adp(n) from\nmicrophone recording y(n), where adp(n) denotes the direct-\npath part in RIR. The training target of the proposed network\nand the training signals of neural vocoders will all be derived\nwith x(n).\nThe proposed method is performed in the time-frequency\ndomain. By applying STFT to Eq. (1), based on the convolu-\ntive transfer function approximation [23], we can obtain:\nY (f, t) ≈S(f, t) ∗A(f, t) + E(f, t)\n(2)\nwhere f ∈{0, ..., F −1} and t ∈{1, ..., T} denote the\nindices of frequency and time frame, respectively. Y (f, t),\nS(f, t) and E(f, t) are the STFT of respective signals, and\nX(f, t) is the STFT of direct-path speech. A(f, t) is the\nconvolutive transfer function associated to a(n). Convolution\n∗is conducted along time. In the STFT domain, the time\ndomain convolution s(n)∗a(n) is decomposed as (frequency-\nindependently) narrow-band convolutions S(f, t) ∗A(f, t).\nSpeech dereverberation in this work highly relies on learning\nthis narrow-band convolution. For nosie reduction, one im-\nportant way for discriminating between speech and stationary\nnoise is to test the signal stationarity, which can be modeled\nin narrow-band as well.\nIII. MEL-SPECTROGRAM ENHANCEMENT\nIn this work, we propose to enhance the Mel-spectrograms,\nwhich then can be directly fed into an ASR model, or\ntransformed to waveforms with a neural vocoder.\nA. Learning Target: Clean Mel-spectrogram\nThe power-based or magnitude-based Mel-spectrogram of\nthe target speech X(f, t), denoted as Xmel(fmel, t), can be ob-\ntained by weighted summing the squared magnitude |X(f, t)|2\nor magnitude |X(f, t)| over frequencies with the triangle\nweight functions of Mel filterbanks, where fmel ∈{1, ..., Fmel}\ndenotes the index of Mel-frequency. Our preliminary exper-\niments showed that using power-based or magnitude-based\nMel-spectrograms achieve similar enhancement performance,\nthence we can choose either of them according to the Mel\nsetup of ASR or neural vocoder backend models. In this\nwork, we use the power-based Mel-spectrogram according to\nthe setup of commonly-used ASR models. Then, the logMel-\nspectrogram, namely the logarithm of Xmel(fmel, t)\nXlogmel(fmel, t) = log(max{Xmel(fmel, t), ϵ})\n(3)\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n3\ncan be taken as the input feature of ASR or neural Vocoder\nbackends. The base of logarithm (e or 10) should be consistent\nto the one of back-ends as well, while e is used in this work.\nThe Mel-spectrogram is clipped with a small value of ϵ to\navoid applying logarithm to close-zero values.\nNormally, ϵ is set to be very small, e.g. 1e-10, to maintain\ncomplete speech information. However, in our preliminary\nexperiments, we found that very small speech values are\nnot very informative for both ASR and neural vocoder, and\nthus can be clipped without harming performance. Moreover,\nsince those small values are highly contaminated by noise\nor reverberation, the prediction error of them could be very\nlarge. For these reasons, we set ϵ to a relatively large value,\ne.g. 1e-5 (when the maximum value of time domain signal\nis normalized. The signal normalization methods will be\npresented in Section III-D). Fig. 1 gives an example of our\ntarget logMel-spectrogram, in which about 40% TF bins are\nclipped.\nIn this work, we evaluate two different learning targets as\nfor Mel-spectrogram enhancement.\nLogMel mapping: The clean logMel-spectrogram can be\ndirectly predicted with the network. The training loss is set\nto the mean absolute error (MAE) loss between the predicted\nand the clean logMel-spectrogram, namely\nLMAE =\n1\nFmelT\nFmel\nX\nfmel=1\nT\nX\nt=1\n| ˆXlogmel(fmel, t) −Xlogmel(fmel, t)|,\n(4)\nwhere ˆXlogmel(fmel, t) is the network output.\nMel ratio mask: Ratio mask is one type of popular learning\ntargets for speech magnitude enhancement [24]. For each time-\nmel-frequency bin, the Mel ratio mask is defined as\nM(fmel, t) = min\n s\nXmel(fmel, t)\nYmel(fmel, t) , 1\n!\n,\n(5)\nwhere Ymel(fmel, t) denotes the power level of noisy Mel-\nspectrogram. The square root function transforms the power\ndomain to the magnitude domain. The min(·) function rectifies\nthe mask into the range of [0, 1]. The mean squared error\n(MSE) of the ratio mask is taken as the training loss, namely\nLMRM =\n1\nFmelT\nFmel\nX\nfmel=1\nT\nX\nt=1\n(M(fmel, t) −ˆ\nM(fmel, t))2,\n(6)\nwhere ˆ\nM(fmel, t) denotes the model prediction of M(fmel, t).\nThen, the enhanced logMel-spectrogram can be obtained as\nˆXlogmel(fmel, t) = log(max{ ˆ\nM(fmel, t)2Ymel(fmel, t), ϵ}). (7)\nB. The CleanMel Network\nFig. 2 shows the network architecture. The proposed net-\nwork takes as input (the real (R(·)) and imaginary (I(·)) parts\nof) the STFT of microphone recording, i.e. Y (f, t), denoted\nas y:\ny[f, t, :] = [R(Y (f, t)), I(Y (f, t))] ∈R2,\n(8)\n0\n20\n40\n60\n80\nMel Frequency\n(a) ϵ =1e-10\n0.0\n1.0\n2.0\n3.0\n4.0\nTime (s)\n0\n20\n40\n60\n80\nMel Frequency\n(b) ϵ =1e-5\nln(1e-10)\nln(1e-5)\n0\nln(1e-10)\nln(1e-5)\n0\nFig. 1. An example of logMel spectrogram with a clip value of ϵ =1e-10 or\nϵ =1e-5.\nwhere [:] represents to take values of one dimension of a tensor.\nThe network is composed of an input layer, interleaved cross-\nband and narrow-band blocks first in the linear-frequency do-\nmain and then in the Mel-frequency domain, a Mel-filterbank,\nand finally a Linear output layer. The input layer conducts\ntemporal convolution on y with a kernel size of 5, obtaining\nthe hidden representation with the dimensions of F × T × H.\nThen one cross-band block and one narrow-band block process\nthe hidden tensor in linear-frequency. Mel-filterbank (with\ntriangle weight functions) transforms the frequency dimension\nfrom F linear frequencies to Fmel Mel frequencies, which is\nrealized with a non-trainable matrix multiplication. Then L\ninterleaved cross-band and narrow-band blocks process the\ntensor in Mel-frequency. After the final narrow-band block, the\noutput Linear layer transforms H-dim to 1-dim as either the\nenhanced logMel-spectrogram or the Mel ratio mask. Note that\nSigmoid activation is applied for predicting Mel ratio mask.\n1) Narrow-band block: As shown in Eq. (2), the time\ndomain\nconvolution\ncan\nbe\ndecomposed\nas\nfrequency-\nindependently narrow-band convolutions, while the latter has\nmuch smaller complexity compared to the former in terms\nof the order of room filters. Therefore, modeling the narrow-\nband convolution would be much more efficient than mod-\neling the time domain convolution. The convolution model\nof target speech provides not only necessary information for\ndereverberation of the target speech but also discriminative\ninformation between the target speech and other interfering\nsources. In addition, in narrow-band, non-stationary speech\nand stationary noise can be well discriminated by testing the\nsignal stationarity. For these reasons, we propose the narrow-\nband network, which processes frequencies independently\nalong the time dimension, and all frequencies share the same\nnetwork.\nThe narrow-band convolution in Eq. (2) is defined between\nthe complex-valued STFT coefficients of source speech and\nroom filter. Thence we process the complex-valued STFT\ncoefficients of noisy signal (in a hidden space), instead of\nother features such as magnitude, to retain the convolution\ncorrelation. We first process in (the finer) linear-frequency\nwith one narrow-band block to fully exploit the convolution\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n4\nc) Cross-band Block\n…\nF-GConv1d\nPReLU\nF-GConv1d\nPReLU\nF-Linear\nF-Linear\nLinear + SiLU\nLinear + SiLU\nLayer Norm\nLayer Norm\nLayer Norm\na) System overview\nb) (Online/Offline) Narrow-band Block\nLayer Norm\nForward\nMamba\nBackward\nMamba\navg.\nCross-band Block\nNarrow-band Block\nT-Conv1d\nLinear\nNoisy and reverberant \nSTFT Coefficients\n𝐹𝐹× (𝑇𝑇× 2)\n𝑇𝑇× (𝐹𝐹× 𝐻𝐻)\n𝐹𝐹× (𝑇𝑇× 𝐻𝐻)\n𝑇𝑇× 𝐻𝐻× (𝐹𝐹)\nVocoder\nASR model\nBack-ends\nTraining Loss\nEnhanced  \nLogMel\nCross-band Block\nMel-Filterbank\nNarrow-band Block\n𝑇𝑇× (𝐹𝐹mel × 𝐻𝐻)\n𝐹𝐹mel × (𝑇𝑇× 𝐻𝐻)\n𝐹𝐹mel × 𝑇𝑇× (𝐻𝐻)\n𝐹𝐹mel × 𝑇𝑇× 1\nrepeat 𝐿𝐿times\nFig. 2. Model architecture of the proposed speech enhancement system. a) System overview. The input dimension of neural blocks/layers are presented before\neach of them in the form ‘batch dimension x (dimension of one sample in batch)”. b) The online/offline narrow-band block. The solid lines stands for online\nprocessing. The solid plus dashed lines stands for offline processing. c) The cross-band block.\ninformation, then after Mel-filterbanks, we process in (the\ncoarser) Mel-frequency with more narrow-band blocks.\nThe narrow-band network is composed of Mamba blocks\n[25]. Mamba is a recently proposed network architecture based\non structured state space sequence models, which was shown\nvery efficient for learning both short-term and long-term de-\npendencies of sequential data. Besides short-term correlations\nof signals, there exist some long-term dependencies should be\nexploited for narrow-band speech enhancement. For example,\nthe convolution model is time invariant in a very long period\nof time for static speakers. Moreover, Mamba has a linear\ncomputational complexity w.r.t time, which is suitable for\nstreaming processing of long audio signals. Specifically, one\nnarrow-band block consists of a forward Mamba for online\nprocessing and an optional backward Mamba (averaging with\nforward output) for offline processing.\n2) Cross-band block: The cross-band block is used to learn\nfull-band/cross-band dependencies of signals. In the original\nSpatialNet [21], the cross-band block is designed for learning\nthe linear relationship of inter-channel features (e.g. inter-\nchannel phase different) across frequencies. In this work,\nsingle channel does not have such inter-channel information.\nInstead, the cross-band block can learn the full-band spectral\npattern of signals in the linear/Mel-frequency domain, which\nis also critical information for (especially single-channel)\nspeech enhancement. The cross-band block processes frames\nindependently along the frequency dimension, and all frames\nshare the same network.\nSpecifically, we adopt the original cross-band block as in\nSpatialNet, which is composed of cascaded frequency con-\nvolutional layer (F-GConv1d), across-frequency linear layers\n(F-Linear) and a second frequency convolution layer. The\nfrequency convolutional layers perform 1-D convolution along\nfrequency to learn correlations between adjacent frequencies.\nThe across-frequency linear layer processes all frequencies\ntogether to learn full-band dependencies. One cross-band block\nis first applied in the linear-frequency domain to learn de-\ntailed full-band correlations. To reduce the model complexity,\nthe hidden dimension H is compressed to a much smaller\ndimension, such as H/12, and then each hidden dimension is\nindependently processed by the across-frequency linear layer\nwith a complexity of F 2. After Mel-filterbank, the cross-band\nblocks learn full-band correlations across Mel frequencies,\nwhere the model complexity is largely reduced from F 2 to\nF 2\nmel. Correspondingly, the hidden dimension is remained as\nH (no longer compressed) to reinforce the capability of full-\nband learning. We set all the Mel-frequency cross-band blocks\nto share the same across-frequency linear layers.\nC. Back-ends\nAt the inference stage, CleanMel is followed by either an\nASR model or a neural vocoder. The ASR model and neural\nvocoder are separately trained with the CleanMel network.\n1) ASR: At inference, the enhanced logMel-spectrogram\nis directly fed to an already-trained ASR system, without\nperforming any fine-tuning or joint-training. Different ASR\nsystems may have different configurations in STFT settings,\nnumber of mel frequencies and base of logarithm. To seam-\nlessly integrate the enhanced logMel-spectrogram into one\nASR model, our CleanMel would adopt the same configu-\nrations as the ASR model. The training cost of CleanMel\nis not very high, so it can be easily re-trained for a new\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n5\nASR system, especially for those large-scale ASR systems and\nalready-deployed ASR systems. In this work, we only conduct\noffline ASR combined with offline CleanMel.\n2) Neural vocoder: The vocoder we adopt in this work\nis Vocos [26], a recently proposed Generative Adversarial\nNetwork (GAN)-based neural vocoder. The generator of Vocos\npredicts the STFT coefficients of speech at frame level and\nthen generates waveform through inverse STFT. Vocos uses\nthe multiple-discriminators and multiple-losses proposed in\nHiFi-GAN [27], but it significantly improves the computa-\ntional efficiency compared to HiFi-GAN that directly generates\nwaveform at sample level. In this work, to unify the front-\nend and back-end processing, we have made several necessary\nmodifications to Vocos as follows:\n• The magnitude-based Mel-spectrogram of original Vocos\nis modified as power-based to be consistent with the\nfront-end and ASR models, where the two cases were\nshown to achieve similar performance in our preliminary\nexperiments.\n• The sampling rate of signals, the STFT configurations and\nthe number of Mel-frequencies of the Vocos are adjusted\naccording to the setup of front-end and ASR models.\n• The original Vocos is designed for offline processing,\nas it employs non-causal convolution layers. To enable\nonline processing, we modified Vocos to be causal by\nsubstituting each non-causal convolution layer with its\ncausal version. The online vocoder still performs quite\nwell. In addition, to reduce the computational complexity\nof online processing, the 75% STFT overlap of original\nVocos is reduced to be 50% overlap, which still achieves\ncomparable performance.\nThe offline and online Vocos are used to work with the offline\nand online CleanMel, respectively. Vocos models are trained\nwith our direct-path target speech x(n).\nD. Signal Normalization\nWhen separate front-end and back-end models are cascaded,\nsignal normalization should be performed not only to facilitate\nthe training of respective models but also to align the signal\nlevel of cascaded models. For offline processing, the normal-\nization method used in Vocos is also applied for CleanMel.\nSpecifically, a random gain is applied to time domain signal\nto ensure that the maximum level of the resulting signal\nlies within the range of -1 to -6 dBFS. This normalization\nmanner ensures the maximum level of sample values is close\nto and smaller than 1, thence the generated waveform can be\ndirectly played with full volume and without clipping effect.\nFor CleanMel, we apply this normalization to noisy signal\nand utilize the same gain of noisy signal to the corresponding\nclean target signal. In this way, the enhanced Mel-spectrogram\ncan be directly fed to Vocos. When applying this time-\ndomain normalization, the clip value ϵ for computing logMel-\nspectrogram is set to 1e-5 for both CleanMel and Vocos. As for\nASR, ASR models normally have a separate Mel-spectrogram\nnormalization operation, which will be applied to re-normalize\nthe enhanced Mel-spectrogram.\nFor online processing, the time domain normalization\nmethod is no longer applicable. Instead, an online STFT-\ndomain normalization is used for both CleanMel and Vocos.\nSpecifically, for CleanMel, the noisy and target speech are\nnormalized in the STFT domain as ˜Y (f, t) = Y (f, t)/µ(t)\nand\n˜X(f, t) = X(f, t)/µ(t), where µ(t) is a recursively\ncalculated meanvalue of STFT magnitude of Y (f, t), i.e.\nµ(t) = αµ(t −1) + (1 −α) 1\nF\nPF −1\nf=0 |Y (f, t)|. The smoothing\nweight is set to α = K−1\nK+1, by which the recursive smoothing\nis equivalent to using a K-long rectangle smoothing window.\nWhen training Vocos using target speech signal x(n), we still\nfirst apply the time domain normalization mentioned above,\nthen apply an extra online normalization. Specifically, µ(t)\nis computed with and also applied to X(f, t), and then the\ncorresponding logMel-spectrogram is computed as the input of\nVocos generator. Accordingly, the output of Vocos generator\n(before applying inverse STFT) would be an estimation of\nnormalized X(f, t). To go back to the signal level of time\ndomain normalization, the recursive normalization factor µ(t)\nis multiplied back to the estimation of normalized X(f, t)\nand then applying inverse STFT, after which Vocos losses\n(including Mel loss and discriminator losses) are computed.\nAt inference, online normalization is applied to the noisy\ninput, and the enhanced logMel-spectrogram is directly fed\ninto Vocos. The recursive normalization factor computed with\nthe noisy input is multiplied to the estimated STFT coefficients\nby Vocos and then applying inverse STFT to obtain the\nfinal waveform which is time-domain normalized and can be\ndirectly played. When applying this online normalization, the\nclip value ϵ for computing logMel-spectrogram is set to 1e-4\nfor CleanMel and the input of Vocos generator.\nIV. EXPERIMENTAL SETUPS\nIn the section, we present the experiment datasets, ex-\nperimental configurations, evalution metrics and comparison\nmethods.\nA. Dataset\n1) Speech enhancement training dataset: The proposed\nmodel is trained with synthetic noisy/clean speech pairs.\nReverberant speech signals are generated by convolving source\nspeech signals with RIRs, then added with noise signals. Clean\nspeech signals are generated by convolving source speech\nsignals with the direct-path part of RIRs.\nIn this work we conduct speech enhancement for both\nMandarin Chinese and English, and we will evaluate our\nmodel on five different datasets (as will be shown later). We\nattempt to train the model once and test it on all the datasets,\nwhich will reflect the general capability of the model under\nvarious situations. To do this, we collect data (in terms of\nsource speech, RIRs and noise) from multiple public datasets,\nand form a training set with sufficient speech quality and\nenvironment/device diversity.\nSource speech: Source speech signals are collected from 6\ndatasets, including AISHELL I [28], AISHELL II [29] and\nTHCHS30 [30] for Chinese, and EARS [31], VCTK [32]\nand DNS I challenge [33] for English. For each language,\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n6\nTABLE I\nCLEAN SOURCE SPEECH USED IN THIS WORK.\nLanguage\nSpeech dataset\nDuration (hours) #Speakers\nChinese\nAISHELLI [28]\n25\n240\nAISHELL II [29]\n178\n987\nTHCHS30 [30]\n18\n49\nEnglish\nEARS [31]\n81\n92\nVCTK [32]\n32\n49\nDNS I challenge [33]\n94\n1,263\nTABLE II\nREAL-MEASURED RIRS USED IN THIS WORK.\nRIR dataset\n#RIRs\nT60 range (second)\nACE [35]\n84\n[0.33, 1.22]\nAIR [36]\n204\n[0.08, 4.50]\nARNI [37]\n1,000\n[0.51, 1.20]\ndEchorate [38]\n594\n[0.17, 0.75]\nMultiChannel [39]\n234\n0.16, 0.36, 0.61\nNaturalReverb [40]\n270\n[0.02, 2.00]\nREVERB [41]\n240\n[0.25, 0.70]\nRWCP [42]\n182\n[0.10, 0.72]\nabout 200 hours of high-quality speech data are selected from\nthe original datasets, based on the raw score of DNSMOS\nP.835 [34]. The selection thresholds are set to 3.6 and 3.5\nfor Chinese and English data, respectively. Except that the\nentire EARS training set is included, since EARS involves\nvarious emotional speech that cannot be well evaluated by the\nDNSMOS. The amount of data selected from each dataset is\nsummarized in Table I.\nRIR: We use real-measured RIRs from multiple public\ndatasets [35]–[42]. Table II shows the statistics of these RIR\ndatasets. For all (even multi-channel) datasets, all RIRs are\nused, except that we uniformly sampled 1,000 RIRs from the\nvery large original ARNI dataset.\nThe reverberation time, i.e. T −60, of RIRs mostly lie in\nthe range of (0, 1.5) seconds, except for a few rooms in AIR\n[36] and NaturalReverb [40]. The distribution of the T60s are\nshown in Fig. 3. Besides the wide distribution range of T60s,\nthese RIRs also have large diversity in terms of environments\nand measuring devices. For example, NaturalReverb [40] is\nrecorded in 271 spaces encountered by humans during daily\nlife, AIR [36] is measured with a dummy head for binaural\napplications, RWCP [42] use a Head-Torso as source speaker,\netc. When synthesizing reverberant speech, 80% source speech\nsamples are convolved with a randomly selected RIR, while\nthere is no RIR convolution for the rest 20% samples to\naccount for the near-field applications where reverberation is\nnegligible.\nNoise: Speech and noise are mixed with a random signal-to-\nnoise ratio (SNR) between -5 dB and 20 dB. We use the noise\nsignals from the DNS challenge [43] and the RealMAN dataset\n[44]. The DNS challenge dataset has about 181 hours of noise\nsampled from AudioSet [45] and Freesound. The RealMAN\ndataset has 106 hours of ambient noise recorded in 31 daily\nlife scenes, including various indoor, semi-outdoor, outdoor\nand transportation scenes.\n0.0\n0.15\n0.3\n0.45\n0.6\n0.75\n0.9\n1.05\n1.2\n1.35\n1.5\n4.5\nT60 (s)\n0\n5\n10\n15\n20\n25\nPercentage (%)\n...\nFig. 3. T60 distribution of RIRs used in this work.\nTABLE III\nMODEL CONFIGURATIONS OF THE PROPOSED MEL SPECTROGRAM\nENHANCEMENT NETWORKS.\nMode\nModel size\nDepth(L + 1) Hidden(H) #Param(M) FLOPs(G/s)\nOnline CleanMel-S\n16\n96\n2.7\n18.1\nOffline CleanMel-S\n8\n96\n2.5\n32.9\nCleanMel-L\n16\n144\n7.2\n127.8\n2) Speech enhancement evaluation datasets: The speech\nenhancement performance of the proposed model is evaluated\non five datasets. For Chinese, we use the public test set\n(static speaker) of the RealMAN dataset [44]. For English,\nthe evaluation is conducted on 4 different datasets: (1) the\nCHiME4 challenge ‘isolated1chtrack’ test set [46], (2) the one-\nchannel test set of REVERB [41], (3) the test set of DNS I\nchallenge [33], (4) the public EARS blind test dataset [31].\n3) ASR evaluation datasets: The ASR performance of the\nproposed model is evaluated on three datasets. For Chinese,\nRealMAN (static) [44] is evaluated, with the pre-trained\nWenetSpeech ASR model provided in ESPNet 2. For English,\nCHiME4 [46] and REVERB [41] are evaluated, with their\nrespective pre-trained ASR models obtained using ESPNet.\nNote that, the ASR models used in this work have the same\nSTFT and mel-frequency configurations, thence one CleanMel\nmodel can be used for all of them.\nB. Configurations\n1) Data configurations: The sampling rate of all data is\nset to 16 kHz. STFT is applied using Hanning window with\na length of 512 samples (32 ms) and a hop size of 128\nand 256 samples (8 and 16 ms) for the offline and online\nmodels, respectively. The offline model has a finer temporal\nresolution than the online model since it is used for ASR\nin this work and its temporal resolution is aligned with the\nASR models. However, we empirically found that, compared\nto the 16-ms hope size, the 8-ms hope size does not benefit\nmuch for the speech enhancement performance. The number\n2https://github.com/espnet/espnet/tree/master/egs2\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n7\nof Mel frequencies is set to FMel = 80 for the frequency\nrange of 0-8 kHz. The same STFT implementation (ESPNet\nimplementation3) is used for the CleanMel networks, neural\nvocoders and ASR models, to avoid configuration mismatch.\nThe natural logarithm (base of e) is used.\n2) Network configurations: Follow [21], the kernel size of\nthe T-Conv1d in the input module and the F-GConv1d layers\nin the cross-band block are both set to 5. As shown in Fig. 2,\nin narrow-band block, forward-only and forward/backward\nMamba layers are set for online and offline processing,\nrespectively. We set up two model scales for the offline\nmodels, referred to as CleanMel-S and CleanMel-L. And the\nonline model scale is set approximately to CleanMel-S. The\nconfigurations are shown in Table IV-B. The depth L of online\nmodels are set to twice the one of corresponding offline models\nto have the similar model size. Due to the different setups\nof STFT hop size, the computational complexity, i.e. FLOPs,\nof offline models are roughly twice the one of corresponding\nonline models.\n3) Training and inference setups: For CleanMel, AdamW\noptimizer [47] with an initial learning rate of 10−3 is used\nfor training. The learning rate exponentially decays with\nlr ←0.001 × 0.99epoch. Gradient clipping is applied with a\ngradient norm threshold 10. The batch size are set to 32.\nTraining samples are synthesized in an on-the-fly manner, and\n100,000 samples are considered as one training epoch. The\nCleanMel-S and CleanMel-L models are trained by 100 and\n150 epochs, respectively. Afterward, we average the model\nweights of the last 10 epochs as the final model for inference.\nFor training the Vocos neural vocoder [26], we synthesized\n400,000 (direct-path) clean speech samples with both English\nand Chinese data used for CleanMel training. The training\nconfigurations keep unchanged as in its original work. For\nASR evaluation, as already mentioned, the pre-trained ASR\nmodels obtained using in ESPNet are used.\nC. Evaluation metrics\nSpeech enhancement performance is evaluated with Percep-\ntual Evaluation of Speech Quality (PESQ) [48], DNSMOS\nP.808 [49] and P.835 [34], where the background, signal and\noverall scores for P.835 are all reported. Word Error Rate\n(WER) and Character Error Rate (CER) are used to evaluate\nEnglish and Chinese ASR performances, respectively.\nD. Comparison models\nWe compare with five advanced speech enhancement mod-\nels, which were all claimed in their original papers to be\nable to conduct joint speech denoising and dereverberation,\nincluding (i) FullSubNet [15] is a LSTM-based full-band and\nsub-band fusion network originally proposed for online speech\ndenoising, and extended to speech dereverberation in [16].\nFor offline processing, we change the uni-directional LSTMs\nto be bi-directional. (ii) Demucs [50] is an online speech\nenhancement model that operates directly on waveforms with\na U-net. (iii) VoiceFixer [19] also performs Mel-spectrogram\n3https://github.com/espnet/espnet/blob/master/espnet2/layers/stft.py\nenhancement (using a ResUNet) and generates the waveform\nusing a neural vocoder, but is developed only for offline speech\nenhancement. (iv) StoRM [51] is a powerful diffusion-based\noffline speech enhancement system. (v) SpatialNet [21] and\noSpatialNet [22] perform speech enhancement in the STFT\nlinear-frequency domain, and offer the backbone network for\nthe proposed CleanMel model. In SpatialNet and oSpatialNet,\nself-attention (and temporal convolution) and Mamba are\nadopted for learning the narrow-band spatial information in\nan offline and an online manner, respectively. For offline\nprocessing, besides comparing with the original SpatialNet,\nwe also implemented a variant of it with bi-directional Mamba\nfor narrow-band blocks (same architecture with the proposed\nmethod), which is referred to as SpatialNet-Mamba, and\nserves as the linear-frequency baseline for the proposed Mel-\nfrequency model. Note that, SpatialNet and oSpatialNet were\noriginally proposed for multi-channel speech enhancement,\nand this work is the first one to fully evaluate and analyze the\ncapability of them for single-channel speech enhancement.\nTo conduct fair comparisons, we re-train the FullSubNet,\nStoRM and oSpatialNet/SpatialNet models using the same\ntraining data utilized for the proposed model. However, we\nfound that it is not easy to re-train the Demucs and Voicefixer\nmodels. In [50], different data augmentation strategies are\napplied when training Demucs on different datasets. Voice-\nfixer [19] was designed for 44.1 kHz signals. Re-training\nDemucs and VoiceFixer request careful data engineering or\nhyperparameter search. Therefore, we use the pre-trained\ncheckpoints of Demucs and VoiceFixer to perform speech\nenhancement on only English data. For Demucs, we use the\n‘dns64’ model provided by the authors 4. For VoiceFixer, we\nuse the default pre-trained model provided in the open-source\npackage 5. Following the VoiceFixer default inference setup,\ntest waveforms are first up-sampled to 44.1 kHz to perform\nspeech enhancement and then down-sampled back to 16 kHz\nfor evaluation.\nV. RESULTS AND ANALYSES\nIn this section, we present and analyze the speech enhance-\nment performance and ASR results, and compare the model\nsize and computational complexity as well.\nA. Speech enhancement results\nTable IV and V show the DNSMOS and PESQ scores,\nrespectively. We analyze the results in the following aspects:\n1) Comparing the training targets of logMel mapping and\nMel ratio mask: For online processing, mapping consistently\noutperforms mask in DNSMOS, mainly due to the higher\nresidual noise of mask, which can be reflected by the much\nlower BAK scores of mask on the very noisy CHiME4 and\nRealMAN test sets. When applying the predicted ratio mask\non the noisy spectrogram, if speech is highly contaminated\nby noise, there will exist certain residual noise even if the\npredicted error of mask is small. By contrast, mapping directly\n4https://github.com/facebookresearch/denoiser\n5https://github.com/haoheliu/voicefixer\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n8\nTABLE IV\nDNSMOS SCORES ON REAL TEST SETS.\nMode\nEnhancement\nmethod\nCHiME4 real\nEARS\nREVERB real\nDNS w. reverb.\nRealMAN static\nP.835\nP.808\nP.835\nP.808\nP.835\nP.808\nP.835\nP.808\nP.835\nP.808\nOVL SIG BAK\nOVL SIG BAK\nOVL SIG BAK\nOVL SIG BAK\nOVL SIG BAK\nunproc.\n1.37 1.99 1.34 2.43 2.02 2.94 2.04 2.80 1.39 1.74 1.53 2.81 1.42 1.89 1.50 2.74 1.53 2.03 1.75 2.56\nOnline\nFullSubNet [15]\n2.41 3.05 3.08 3.22 2.73 3.12 3.55 3.52 2.72 3.13 3.60 3.61 2.41 2.98 3.15 3.36 2.49 2.97 3.33 3.12\nDemucs [50]\n2.84 3.12 3.90 3.26 2.94 3.21 3.94 3.38 2.96 3.25 3.94 3.50 2.50 2.82 3.66 3.17\n-\noSpatialNet [22]\n2.61 3.15 3.36 3.52 2.85 3.22 3.65 3.65 3.03 3.33 3.93 3.84 2.79 3.16 3.70 3.67 2.79 3.19 3.66 3.46\nCleanMel-S-map (ours) 2.95 3.26 3.86 3.77 2.89 3.24 3.69 3.72 3.17 3.45 4.02 3.94 3.08 3.36 3.97 3.84 2.90 3.24 3.81 3.52\nCleanMel-S-mask (ours) 2.74 3.26 3.43 3.59 2.83 3.19 3.64 3.70 3.07 3.40 3.91 3.80 2.95 3.30 3.81 3.75 2.73 3.18 3.53 3.36\nOffline\nFullSubNet [15]\n2.39 2.93 3.18 3.31 2.47 3.10 3.11 3.27 2.71 3.11 3.62 3.62 2.74 3.16 3.51 3.54 2.47 2.97 3.27 3.13\nVoiceFixer [19]\n2.95 3.25 3.89 3.65 2.94 3.25 3.83 3.55 2.88 3.26 3.61 3.62 3.11 3.40 3.97 3.84\n-\nStoRM [51]\n3.29 3.57 4.03 3.87 2.97 3.37 3.65 3.79 3.25 3.53 4.01 4.01 3.25 3.53 4.03 3.87 3.04 3.42 3.80 3.68\nSpatialNet [21]\n2.76 3.32 3.39 3.51 2.86 3.27 3.58 3.59 3.07 3.41 3.87 3.84 2.91 3.33 3.68 3.65 2.88 3.32 3.66 3.45\nSpatialNet-Mamba\n2.94 3.35 3.71 3.65 2.94 3.31 3.69 3.67 3.09 3.43 3.85 3.86 2.89 3.35 3.58 3.68 2.95 3.37 3.69 3.50\nCleanMel-S-map (ours) 3.33 3.58 4.11 3.96 3.02 3.36 3.73 3.79 3.39 3.62 4.13 4.08 3.33 3.58 4.10 4.03 3.25 3.55 4.03 3.82\nCleanMel-S-mask (ours) 3.26 3.56 4.01 3.81 2.97 3.34 3.68 3.71 3.31 3.58 4.08 4.01 3.25 3.53 4.05 3.97 3.15 3.49 3.93 3.71\nCleanMel-L-mask (ours) 3.30 3.59 4.04 3.87 2.99 3.35 3.70 3.74 3.31 3.58 4.08 4.02 3.25 3.53 4.05 4.00 3.15 3.49 3.93 3.71\nTABLE V\nPESQ SCORES ON TEST SETS WITH CLEAN REFERENCE SPEECH.\nMode\nEnhancement\nmethod\nCHiME4\nsimu.\nDNS\nw.o. reverb.\nREVERB\nsimu.\nRealMAN\nstatic\nunproc.\n1.20\n1.58\n1.50\n1.14\nOnline\nFullSubNet [15]\n1.80\n2.71\n2.39\n1.36\nDemucs [50]\n1.64\n2.63\n1.92\n-\noSpatialNet [22]\n1.81\n2.77\n2.56\n1.87\nCleanMel-S-map (ours)\n1.77\n2.73\n2.63\n1.79\nCleanMel-S-mask (ours)\n1.99\n2.82\n2.63\n1.75\nOffline\nFullSubNet [15]\n1.87\n2.82\n2.48\n1.61\nVoiceFixer [19]\n1.57\n1.92\n1.67\n-\nStoRM [51]\n1.76\n2.74\n2.52\n1.71\nSpatialNet [21]\n1.90\n2.82\n2.87\n1.93\nSpatialNet-Mamba\n1.90\n2.90\n3.06\n2.10\nCleanMel-S-map (ours)\n2.09\n2.95\n2.92\n2.00\nCleanMel-S-mask (ours)\n2.17\n2.97\n2.85\n1.95\nCleanMel-L-mask (ours)\n2.35\n3.07\n2.97\n2.01\npredicts the logMel-spectrogram of speech, which can avoid\nsuch residual noise. However, as shown in Table V, mask\nachieves higher PESQ scores than mapping on the highly noisy\nCHiME4 and DNS (w.o. reverb) sets. PESQ measures the\nperceptual similarity of enhanced speech and reference clean\nspeech. The higher PESQ scores indicates that mask performs\nbetter on retrieving the target speech through extracting the\ntarget speech from the noisy speech with a mask. Directly\nmapping may erroneously remove or boost those speech com-\nponents highly contaminated by noise. The enhanced logMel-\nspectrograms are transformed to waveforms with a neural\nvocoder, and the speech quality measured with DNSMOS is\n0\n20\n40\n60\n80\nMel Frequency\n(a) noisy\n0\n20\n40\n60\n80\nMel Frequency\n(b) clean\n0\n20\n40\n60\n80\nMel Frequency\n(c) mask\n0.0\n1.0\n2.0\n3.0\n4.0\nTime (s)\n0\n20\n40\n60\n80\nMel Frequency\n(d) mapping\nln(1e-5)\n0\nln(1e-5)\n0\nln(1e-5)\n0\nln(1e-5)\n0\nFig. 4. An example of Mel spectrogram enhancement with the targets of Mel\nratio mask and logMel mapping. The orange box marks the removed part of\nspeech by mapping and the red box marks the residual noise by mask.\nmore affected by the residual noise caused by mask than the\nmapping error of logMel-spectrogram. Fig 4 shows an example\nof the enhanced logMel-spectrogram by mask and mapping,\nwhich verifies our discussions.\nFor offline processing, the differences between mask and\nmapping discussed above still present, but get much smaller,\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n9\nfor example the difference of BAK scores on the CHiME4\nand RealMAN test sets become much smaller. Compared to\nthe online network, the offline network achieves much smaller\nprediction errors for both mask and mapping, thence their\ndifferences also become smaller as they both target the same\nclean speech.\n2) Comparing with oSpatialNet and SpatialNet-Mamba:\noSpatialNet and SpatialNet-Mamba adopt the same network\narchitectures and serve as the linear-frequency baselines for\nthe proposed CleanMel networks. For both online and offline\nprocessing, the proposed Mel-spectrogram enhancement net-\nworks noticeably outperform their linear-frequency baselines\nin DNSMOS, but not necessarily in PESQ. This phenomenon\nis consistent with the findings in the VoiceFixer paper [19].\nThe GAN-based nerual vocoder is good at generating high-\nquality speech, but will possibly reduce the (PESQ) similarity\nof the enhance speech with the reference speech.\n3) Comparing with online baseline networks: Demucs [50]\nperforms well for denoising but not for dereverberation, as\nit achieves leading performance in DNSMOS P.835 on the\n(low-reverberation) CHiME4 and EARS sets, but not for\nother reverberant test sets. Although it was developed for\nmulti-channel speech enhancement in [22], oSpatialNet also\nperforms very well for single-channel speech enhancement\ncompared with other comparison models. oSpatialNet and\nthe proposed CleanMel network work better especially for\ndereverberation due to their narrow-band processing of room\nfilter convolution.\n4) Comparing with offline baseline networks: VoiceFixer\n[19] is an important baseline for the proposed CleanMel\nnetwork, as they both perform Mel-spectrogram enhancement.\nVoiceFixer adopts an advanced ResUNet. We can see that the\nproposed network noticeably outperforms VoiceFixer, which\nverifies the strong capability of the proposed cross-band and\nnarrow-band combination network. StoRM [51] is a diffusion-\nmodel-based generative network, which often achieves the\nSOTA performance for speech enhancement. The proposed\nnetwork consistently outperforms StoRM. Their performance\ngaps of DNSMOS scores are not large, but the gaps of PESQ\nscores are very large. This indicates that although generative\nmodel can generate high-quality speech, but its fidelity to\nthe source speech is hard to be ensured. SpatialNet [21],\nespecially its Mamba variant, i.e. SpatialNet-Mamba, as a\npure discriminative speech enhancement network, achieves\npromising performance.\nOverall, by combining a powerful discriminative Mel-\nspectrogram enhancement network and a GAN-based genera-\ntive neural vocoder, the proposed network achieves new SOTA\nspeech enhancement performance for both online and offline\nprocessing, and for both denoising and dereverberation. By\nscaling up the proposed network, the performance can be fur-\nther improved. Taking the mask scheme as an example, com-\npared to the small model, the large model noticeably improves\nthe PESQ scores (although not the DNSMOS scores). The\nperformance improvement of large model is clearly audible\nwhen listening to the enhanced speech demos.\nTABLE VI\nASR RESULTS, WER (%) FOR THE ENGLISH CHIME4 AND REVERB\nSETS, AND CER (%) FOR THE CHINESE REALMAN SET.\nEnhancement\nmethod\nCHiME4\nREVERB simu. REVERB real RealMAN\nstatic\nsimu. real\nnear\nfar\nnear\nfar\nunprocessed\n15.3 13.1\n3.7\n4.7\n6.0\n6.3\n20.1\nclean\n3.1\n-\n3.4\n3.4\n-\n-\n7.7\nMulti-channel*\n13.0 10.8\n3.5\n3.7\n3.6\n4.4\n-\n1ch WPE [52]\n-\n-\n3.7\n4.6\n5.5\n5.8\n-\nFullSubNet [15]\n19.4 17.3\n4.1\n5.6\n6.1\n6.0\n22.0\nVoiceFixer† [19]\n24.3 26.4\n5.9\n8.8\n8.9\n10.1\n-\nStoRM [51]\n21.9 18.2\n4.2\n5.5\n7.3\n7.3\n28.4\nSpatialNet [21]\n20.5 16.9\n4.2\n5.2\n5.7\n5.1\n18.5\nSpatialNet-Mamba 17.6 13.7\n3.9\n4.0\n4.3\n4.9\n16.5\nCleanMel-S-map\n12.5\n9.8\n3.7\n3.9\n4.2\n4.5\n16.7\nCleanMel-S-mask\n11.9\n9.3\n3.6\n3.7\n3.9\n4.3\n16.5\nCleanMel-L-mask\n9.6\n7.7\n3.5\n3.7\n3.5\n3.6\n14.4\n* 5-channel Beamformit and 8-channel WPE [52] + Beamformit serve\nas the multi-channel baseline methods for CHiME4 and REVERB,\nrespectively.\n†VoiceFixer is evaluated in the same way as the proposed model, namely\nthe enhanced Mel-spectrogram is directly fed to ASR models. To fit the\nSTFT setups of VoiceFixer, the ASR models used for VoiceFixer were re-\ntrained, which are different from the ones used for other methods. Please\nrefer to Table VII for more information about the re-trained ASR models.\nB. ASR results\nTable VI presents the ASR results. We can observe that the\nASR results are consistent with the common finding in the\nfiled [53] that single-channel speech enhancement networks\nusually do not help for ASR due to the speech artifacts\ncaused by the networks. VoiceFixer performs Mel-spectrogram\nenhancement using an advanced ResUNet. The poor ASR\nresults of VoiceFixer show that ResUNet brings severe artifacts\nto the enhanced Mel-spectrogram. StoRM does not work well\nfor ASR, which further verifies that the generated speech has\nlow fidelity to the source speech.\nOne exception is SpatialNet, especially SpatialNet-Mamba,\nwho show clear positive ASR effects on the REVERB-\nreal and RealMAN sets. We believe that the advantages of\nSpatialNet lie in its narrow-band processing mechanism: i)\nnarrow-band processing is especially efficient for modeling\nthe much simpler narrow-band room filter convolution; ii) as\ndiscussed in [14], narrow-band processing can avoid the so-\ncalled wide-band artifacts, such as the blurred and/or wrongly\ndeleted/inserted wide-band spectra, which will be very harmful\nfor ASR.\nThe proposed CleanMel networks further improve the ASR\nperformance on top of SpatialNet-Mamba. Directly enhancing\nthe Mel-frequency spectra is more optimal for ASR than\nfirst enhancing the detailed linear-frequency spectra and then\ncompressing to Mel-frequency. Compared to linear-frequency\nprocessing, the lower-dimensional Mel-frequency representa-\ntion is easier to learn. However, this is more valid for the cross-\nband processing part, but not for the narrow-band (frequency-\nwise or dimension-wise) processing part. That is possibly why\nthe proposed CleanMel networks are more advantageous on\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n10\nTABLE VII\nASR RESULT COMPARISONS BETWEEN TAKING AS INPUT ENHANCED\nMEL-SPECTROGRAM AND VOCODER WAVEFORM. WER (%) FOR THE\nENGLISH CHIME4 AND REVERB SETS, AND CER (%) FOR THE\nCHINESE REALMAN SET.\nEnhancement\nmethod\nCHiME4\nREVERB simu. REVERB real RealMAN\nstatic\nsimu. real\nnear\nfar\nnear\nfar\nunprocessed\n15.6 13.5\n4.3\n5.6\n6.8\n7.4\n-\nVoiceFixer\n24.3 26.4\n5.9\n8.8\n8.9\n10.1\n-\nwaveform\n27.6 30.6\n8.1\n11.9\n10.9\n11.5\n-\nunprocessed\n15.3 13.1\n3.7\n4.7\n6.0\n6.3\n20.1\nCleanMel-S-mask 11.9\n9.3\n3.6\n3.7\n3.9\n4.3\n16.5\nwaveform\n13.4 10.1\n3.7\n4.2\n4.0\n4.4\n17.9\nCleanMel-L-mask\n9.6\n7.7\n3.5\n3.7\n3.5\n3.6\n14.4\nwaveform\n10.2\n8.4\n3.8\n3.9\n3.4\n3.7\n15.7\nthe denoising task of CHiME4 (more relying on cross-band\nprocessing) than the dereverberation task of REVERB (more\nrelying on narrow-band processing). For ASR, Mel ratio mask\nperforms better than logMel mapping, but the performance\ngaps are not large. This indicates that ASR is less affected by\nthe larger residual noise caused by mask than the prediction\nerror of logMel mapping. Scaling up is very effective for\nfurther improving the ASR performance, as shown by the\nresults of the large mask model.\nOverall, by adopting an advanced backbone network (i.e.\nSpatialNet-Mamba), developing an effective Mel-spectrogram\nenhancement framework, and scaling up the model size, the\nproposed CleanMel-L-mask network achieves substantial ASR\nimprovements relative to unprocessed speech.\nC. Influence of neural vocoder to ASR performance\nWe use a GAN-based neural vocoder to transform the\nenhanced Mel-spectrogram to waveform, which may modify\nthe original Mel-spectrogram and reduce the fidelity, although\na Mel-spectrogram loss is adopted in the vocoder to maintain\nthe Mel-spectrogram fidelity. Table VII compares the ASR\nresults between taking as input the enhanced Mel-spectrogram\nand the vocoder waveform. Note that the ASR models used\nfor VoiceFixer is re-trained accroding to its STFT and Mel-\nfrequency setups. We can see that the neural vocoders in-\ndeed reduce the Mel-spectrogram fidelity and thus the ASR\nperformance. However, the performance reductions are not\nsignificant, and the ASR results of vocoder waveform for\neach model closely insist on the results of its corresponding\nenhanced Mel-spectrogram. This means the vocoder waveform\ncould be also an alternative/suboptimal choice for ASR. If it is\nnot easy to re-train a CleanMel model according to the setups\nof a new ASR model, since vocoder waveform is irrelevant to\nthose setups, our pre-trained checkpoints can be used.\nD. Model size and computational complexity\nTable VIII shows the model size and computational com-\nplexity of the proposed model and comparison models. The\nTABLE VIII\nMODEL SIZE AND COMPUTATIONAL COMPLEXITY OF DIFFERENT MODELS.\nFOR CLEANMEL AND VOICEFIXER, THE #PARAM AND FLOPS OF\nMEL-SPECTROGRAM ENHANCEMENT NETWORK AND VOCODER ARE\nRESPECTIVELY GIVEN IN ADDITION, PRESENTING THE COMPUTATIONAL\nCOST FOR ASR (ONLY THE FORMER TERM) AND SPEECH ENHANCEMENT\n(THE ADDITION).\nMode\nModel\n#Param(M)\nFLOPs(G/s)\nOnline\nFullSubNet [15]\n5.6\n59.6\nDemucs [50]\n33.5\n15.4\noSpatialNet [22]\n1.7\n36.8\nCleanMel-S (ours)\n2.7 + 13.2\n18.1 + 1.7\nOffline\nFullSubNet [15]\n14.6\n157.9\nVoiceFixer [19]\n88.3 + 33.8\n20.9 + 103.0\nStoRM [51]\n55.1\n4600\nSpatialNet [21]\n1.6\n46.3\nSpatialNet-Mamba\n1.7\n34.9\nCleanMel-S (ours)\n2.5 + 13.2\n32.9 + 3.3\nCleanMel-L (ours)\n7.2 + 13.2\n127.8 + 3.3\nproposed model is composed of a Mel-spectrogram enhance-\nment network and an optional neural vocoder. The Mel-\nspectrogram enhancement network has a small model size\nand a large computational complexity, mainly due to the\nindependent computation of frequencies with shared narrow-\nband blocks. Compared to linear-frequency sub-band/narrow-\nband processing networks, including FullSubNet, oSpatialNet,\nSpatialNet(-Mamba), the proposed Mel-spectrogram enhance-\nment network has a smaller computational complexity, due\nto the much less frequencies to be processed, i.e. 80 Mel\nfrequencies versus 256 linear frequencies. As a diffusion\nmodel, the computation of StoRM is much more expensive\nthan other models.\nVI. CONCLUSION\nThis work proposed a single-channel Mel-spectrogram de-\nnoising and dereverberation network, named CleanMel. The\nlearning targets of logMel mapping and ratio mask have\nbeen compared, while the former suffers from less residual\nnoise and the latter preserves better the target speech. The\nadopted network architecture, i.e. interleaved cross-band and\nnarrow-band blocks, has been proven working well for single-\nchannel speech denoising and dereverberation in both the\nlinear-frequency domain and the proposed Mel-frequency do-\nmain. The high-quality enhanced Mel-spectrogram can be well\ntransformed to waveform with a neural vocoder and can also\nbe used for boosting the ASR performance. Mel-spectrogram\nplays a key role in the field of speech processing, so the\nproposed CleanMel model can be potentially used for many\nother tasks, such as self-supervised speech pre-training and\nhigh-quality speech synthesis.\nREFERENCES\n[1] Y. Luo and N. Mesgarani, “Conv-tasnet: Surpassing ideal time–\nfrequency magnitude masking for speech separation,” IEEE/ACM Trans-\nactions on Audio, Speech, and Language processing, vol. 27, no. 8, pp.\n1256–1266, 2019.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n11\n[2] A. D´efossez, G. Synnaeve, and Y. Adi, “Real time speech enhancement\nin the waveform domain,” in Interspeech 2020, 2020, pp. 3291–3295.\n[3] F. Xiong, W. Chen, P. Wang, X. Li, and J. Feng, “Spectro-temporal\nsubnet for real-time monaural speech denoising and dereverberation,” in\nInterspeech 2022, 2022, pp. 931–935.\n[4] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and\nL. Xie, “Dccrn: Deep complex convolution recurrent network for phase-\naware speech enhancement,” in Interspeech 2020, 2020, pp. 2472–2476.\n[5] A. Li, C. Zheng, L. Zhang, and X. Li, “Glance and gaze: A collaborative\nlearning framework for single-channel speech enhancement,” Applied\nAcoustics, vol. 187, p. 108499, 2022.\n[6] K. Iwamoto, T. Ochiai, M. Delcroix, R. Ikeshita, H. Sato, S. Araki, and\nS. Katagiri, “How bad are artifacts?: Analyzing the impact of speech\nenhancement errors on asr,” in Interspeech 2022, 2022, pp. 5418–5422.\n[7] K. Kinoshita, T. Ochiai, M. Delcroix, and T. Nakatani, “Improving noise\nrobust automatic speech recognition with single-channel time-domain\nenhancement network,” in ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2020,\npp. 7009–7013.\n[8] Z. Nian, J. Du, Y. Ting Yeung, and R. Wang, “A time domain progressive\nlearning approach with snr constriction for single-channel speech en-\nhancement and recognition,” in ICASSP 2022 - 2022 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2022,\npp. 6277–6281.\n[9] Y. Yang, A. Pandey, and D. Wang, “Time-domain speech enhancement\nfor robust automatic speech recognition,” in Interspeech 2023, 2023, pp.\n4913–4917.\n[10] ——, “Towards decoupling frontend enhancement and backend recog-\nnition in monaural robust asr,” arXiv preprint arXiv:2403.06387, 2024.\n[11] J.-M. Valin, “A hybrid dsp/deep learning approach to real-time full-\nband speech enhancement,” in 2018 IEEE 20th International Workshop\non Multimedia Signal Processing (MMSP), 2018, pp. 1–5.\n[12] J.-M. Valin, U. Isik, N. Phansalkar, R. Giri, K. Helwani, and A. Kr-\nishnaswamy, “A perceptually-motivated approach for low-complexity,\nreal-time enhancement of fullband speech,” in Interspeech 2020, 2020,\npp. 2482–2486.\n[13] H. Schr¨oter, A. N. Escalante-B., T. Rosenkranz, and A. Maier, “Deep-\nfilternet: Perceptually motivated real-time speech enhancement,” in In-\nterspeech 2023, 2023, pp. 2008–2009.\n[14] X. Li and R. Horaud, “Narrow-band deep filtering for multichannel\nspeech enhancement,” arXiv preprint arXiv:1911.10791, 2019.\n[15] X. Hao, X. Su, R. Horaud, and X. Li, “Fullsubnet: A full-band and sub-\nband fusion model for real-time single-channel speech enhancement,”\nin ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2021, pp. 6633–6637.\n[16] R. Zhou, W. Zhu, and X. Li, “Speech dereverberation with a reverbera-\ntion time shortening target,” in ICASSP 2023 - 2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2023,\npp. 1–5.\n[17] X. Hao and X. Li, “Fast fullsubnet: Accelerate full-band and sub-band\nfusion model for single-channel speech enhancement,” arXiv preprint\narXiv:2212.09019, 2022.\n[18] V. Kothapally, Y. Xu, M. Yu, S.-X. Zhang, and D. Yu, “Deep neural mel-\nsubband beamformer for in-car speech separation,” in ICASSP 2023 -\n2023 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2023, pp. 1–5.\n[19] H. Liu, X. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and\nY. Wang, “Voicefixer: A unified framework for high-fidelity speech\nrestoration,” in Interspeech 2022, 2022, pp. 4232–4236.\n[20] Y. Tian, W. Liu, and T. Lee, “Diffusion-based mel-spectrogram enhance-\nment for personalized speech synthesis with found data,” in 2023 IEEE\nAutomatic Speech Recognition and Understanding Workshop (ASRU),\n2023, pp. 1–7.\n[21] C. Quan and X. Li, “Spatialnet: Extensively learning spatial information\nfor multichannel joint speech separation, denoising and dereverberation,”\nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\nvol. 32, pp. 1310–1323, 2024.\n[22] ——, “Multichannel long-term streaming neural speech enhancement for\nstatic and moving speakers,” IEEE Signal Processing Letters, vol. 31,\npp. 2295–2299, 2024.\n[23] X. Li, L. Girin, S. Gannot, and R. Horaud, “Multichannel speech\nseparation and enhancement using the convolutive transfer function,”\nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\nvol. 27, no. 3, pp. 645–659, 2019.\n[24] D. Wang and J. Chen, “Supervised speech separation based on deep\nlearning: An overview,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 26, no. 10, pp. 1702–1726, 2018.\n[25] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\nselective state spaces,” in First Conference on Language Modeling, 2024.\n[26] H. Siuzdak, “Vocos: Closing the gap between time-domain and fourier-\nbased neural vocoders for high-quality audio synthesis,” in The Twelfth\nInternational Conference on Learning Representations, 2024.\n[27] J. Kong, J. Kim, and J. Bae, “Hifi-gan: generative adversarial networks\nfor efficient and high fidelity speech synthesis,” in Proceedings of\nthe 34th International Conference on Neural Information Processing\nSystems, 2020.\n[28] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An open-source\nmandarin speech corpus and a speech recognition baseline,” in 2017\n20th conference of the oriental chapter of the international coordinating\ncommittee on speech databases and speech I/O systems and assessment\n(O-COCOSDA).\nIEEE, 2017, pp. 1–5.\n[29] J. Du, X. Na, X. Liu, and H. Bu, “AISHELL-2: Transforming Mandarin\nASR Research Into Industrial Scale,” ArXiv, Aug. 2018.\n[30] D. Wang and X. Zhang, “Thchs-30: A free chinese speech corpus,” arXiv\npreprint arXiv:1512.01882, 2015.\n[31] J. Richter, Y.-C. Wu, S. Krenn, S. Welker, B. Lay, S. Watanabe,\nA. Richard, and T. Gerkmann, “Ears: An anechoic fullband speech\ndataset benchmarked for speech enhancement and dereverberation,” in\nInterspeech 2024, 2024, pp. 4873–4877.\n[32] J. Yamagishi, C. Veaux, and K. MacDonald, “Cstr vctk corpus: English\nmulti-speaker corpus for cstr voice cloning toolkit (version 0.92),”\n2019.\n[Online].\nAvailable:\nhttps://api.semanticscholar.org/CorpusID:\n213060286\n[33] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey,\nS. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan,\nand J. Gehrke, “The interspeech 2020 deep noise suppression challenge:\nDatasets, subjective testing framework, and challenge results,” in Inter-\nspeech 2020, 2020, pp. 2492–2496.\n[34] C. K. Reddy, V. Gopal, and R. Cutler, “Dnsmos p. 835: A non-intrusive\nperceptual objective speech quality metric to evaluate noise suppressors,”\nin ICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP).\nIEEE, 2022, pp. 886–890.\n[35] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor, “Estimation of\nroom acoustic parameters: The ace challenge,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 24, no. 10, pp. 1681–\n1693, 2016.\n[36] M. Jeub, M. Schafer, and P. Vary, “A binaural room impulse response\ndatabase for the evaluation of dereverberation algorithms,” in 2009 16th\nInternational Conference on Digital Signal Processing, 2009, pp. 1–5.\n[37] K. Prawda, S. J. Schlecht, and V. V¨alim¨aki, “Calibrating the sabine and\neyring formulas,” The Journal of the Acoustical Society of America, vol.\n152, no. 2, pp. 1158–1169, 2022.\n[38] D. D. Carlo, P. Tandeitnik, C. Foy, N. Bertin, A. Deleforge, and\nS. Gannot, “dechorate: a calibrated room impulse response dataset for\necho-aware signal processing,” EURASIP Journal on Audio, Speech, and\nMusic Processing, vol. 2021, pp. 1–15, 2021.\n[39] E. Hadad, F. Heese, P. Vary, and S. Gannot, “Multichannel audio\ndatabase in various acoustic environments,” in 2014 14th International\nWorkshop on Acoustic Signal Enhancement (IWAENC), 2014, pp. 313–\n317.\n[40] J. Traer and J. H. McDermott, “Statistics of natural reverberation enable\nperceptual separation of sound and space,” Proceedings of the National\nAcademy of Sciences, vol. 113, no. 48, pp. E7856–E7865, 2016.\n[41] K. Kinoshita, M. Delcroix, T. Yoshioka, T. Nakatani, E. Habets, R. Haeb-\nUmbach, V. Leutnant, A. Sehr, W. Kellermann, R. Maas, S. Gannot, and\nB. Raj, “The reverb challenge: A common evaluation framework for\ndereverberation and recognition of reverberant speech,” in 2013 IEEE\nWorkshop on Applications of Signal Processing to Audio and Acoustics,\n2013, pp. 1–4.\n[42] S. Nakamura, “Acoustic sound database collected for hands-free speech\nrecognition and sound scene understanding,” in International Workshop\non Hands-Free Speech Communication, 2001, pp. 43–46.\n[43] C. K. Reddy, H. Dubey, K. Koishida, A. Nair, V. Gopal, R. Cutler,\nS. Braun, H. Gamper, R. Aichner, and S. Srinivasan, “Interspeech 2021\ndeep noise suppression challenge,” in Interspeech 2021, 2021, pp. 2796–\n2800.\n[44] B. Yang, C. Quan, Y. Wang, P. Wang, Y. Yang, Y. Fang, N. Shao,\nH. Bu, X. Xu, and X. Li, “RealMAN: A real-recorded and annotated\nmicrophone array dataset for dynamic speech enhancement and localiza-\ntion,” in The Thirty-eight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2024.\n[45] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and\nhuman-labeled dataset for audio events,” in ICASSP 2017 - 2017 IEEE\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n12\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2017, pp. 776–780.\n[46] E. Vincent, S. Watanabe, J. Barker, and R. Marxer, “The 4th chime\nspeech separation and recognition challenge,” URL: http://spandh. dcs.\nshef. ac. uk/chime challenge/(last accessed on 1 August, 2018), 2016.\n[47] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin International Conference on Learning Representations, 2019.\n[48] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual\nevaluation of speech quality (pesq)-a new method for speech quality\nassessment of telephone networks and codecs,” in ICASSP 2001 -\n2001 IEEE international conference on acoustics, speech, and signal\nprocessing (ICASSP), vol. 2.\nIEEE, 2001, pp. 749–752.\n[49] C. K. Reddy, V. Gopal, and R. Cutler, “Dnsmos: A non-intrusive\nperceptual objective speech quality metric to evaluate noise suppressors,”\nin ICASSP 2021-2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 6493–6497.\n[50] A. D´efossez, G. Synnaeve, and Y. Adi, “Real time speech enhancement\nin the waveform domain,” in Interspeech 2020, 2020, pp. 3291–3295.\n[51] J.-M. Lemercier, J. Richter, S. Welker, and T. Gerkmann, “Storm: A\ndiffusion-based stochastic regeneration model for speech enhancement\nand dereverberation,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 31, pp. 2724–2737, 2023.\n[52] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, and B.-H. Juang,\n“Speech dereverberation based on variance-normalized delayed linear\nprediction,” IEEE Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 18, no. 7, pp. 1717–1731, 2010.\n[53] K. Iwamoto, T. Ochiai, M. Delcroix, R. Ikeshita, H. Sato, S. Araki, and\nS. Katagiri, “How bad are artifacts?: Analyzing the impact of speech\nenhancement errors on asr,” in Interspeech 2022, 2022, pp. 5418–5422.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20040v1.pdf",
    "total_pages": 12,
    "title": "CleanMel: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR",
    "authors": [
      "Nian Shao",
      "Rui Zhou",
      "Pengyu Wang",
      "Xian Li",
      "Ying Fang",
      "Yujie Yang",
      "Xiaofei Li"
    ],
    "abstract": "In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising\nand dereverberation network for improving both speech quality and automatic\nspeech recognition (ASR) performance. The proposed network takes as input the\nnoisy and reverberant microphone recording and predicts the corresponding clean\nMel-spectrogram. The enhanced Mel-spectrogram can be either transformed to\nspeech waveform with a neural vocoder or directly used for ASR. The proposed\nnetwork is composed of interleaved cross-band and narrow-band processing in the\nMel-frequency domain, for learning the full-band spectral pattern and the\nnarrow-band properties of signals, respectively. Compared to linear-frequency\ndomain or time-domain speech enhancement, the key advantage of Mel-spectrogram\nenhancement is that Mel-frequency presents speech in a more compact way and\nthus is easier to learn, which will benefit both speech quality and ASR.\nExperimental results on four English and one Chinese datasets demonstrate a\nsignificant improvement in both speech quality and ASR performance achieved by\nthe proposed model. Code and audio examples of our model are available online\nin https://audio.westlake.edu.cn/Research/CleanMel.html.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}