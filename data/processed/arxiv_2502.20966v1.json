{
  "id": "arxiv_2502.20966v1",
  "text": "Proceedings of the 7th Symposium on Advances in Approximate Bayesian Inference, 2025 1–10\nPost-Hoc Uncertainty Quantification in Pre-Trained Neural\nNetworks via Activation-Level Gaussian Processes\nRichard Bergna\nrsb63@cam.ac.uk\nDepartment of Engineering, University of Cambridge\nStefan Depeweg\nSiemens AG\nSergio Calvo-Ordoñez\nMathematical Institute, University of Oxford\nJonathan Plenk\nMathematical Institute, University of Oxford\nAlvaro Cartea\nMathematical Institute, University of Oxford\nJose Miguel Hernández-Lobato\nDepartment of Engineering, University of Cambridge\nAbstract\nUncertainty quantification in neural networks through methods such as Dropout, Bayesian\nneural networks and Laplace approximations is either prone to underfitting or computa-\ntionally demanding, rendering these approaches impractical for large-scale datasets. In this\nwork, we address these shortcomings by shifting the focus from uncertainty in the weight\nspace to uncertainty at the activation level, via Gaussian processes. More specifically, we\nintroduce the Gaussian Process Activation function (GAPA) to capture neuron-level un-\ncertainties. Our approach operates in a post-hoc manner, preserving the original mean\npredictions of the pre-trained neural network and thereby avoiding the underfitting issues\ncommonly encountered in previous methods. We propose two methods. The first, GAPA-\nFree, employs empirical kernel learning from the training data for the hyperparameters\nand is highly efficient during training. The second, GAPA-Variational, learns the hyperpa-\nrameters via gradient descent on the kernels, thus affording greater flexibility. Empirical\nresults demonstrate that GAPA-Variational outperforms the Laplace approximation on\nmost datasets in at least one of the uncertainty quantification metrics.\n1. Introduction\nDeep neural networks (DNNs) have achieved state-of-the-art performance in a wide range\nof pattern recognition tasks (Krizhevsky et al., 2012; Kenton and Toutanova, 2019; Mnih\net al., 2015; Hinton et al., 2012; Litjens et al., 2017). However, traditional DNNs do not\nquantify epistemic uncertainty, limiting their reliability in risk-sensitive applications such\nas autonomous driving (Shafaei et al., 2018), healthcare (Begoli et al., 2019), and finance\n(Blasco et al., 2024). To address this limitation, numerous surrogate methods have been\ndeveloped for downstream decision-making under uncertainty, particularly for anomaly de-\ntection and out-of-distribution detection (Li et al., 2023; Liu et al., 2023). Yet, a more\nprincipled Bayesian approach has been proposed to model uncertainty directly. This has\nled to methods that approximate distributions over weight space, including Bayesian Neural\n© R. Bergna, S. Depeweg, S. Calvo-Ordoñez, J. Plenk, A. Cartea & J.M. Hernández-Lobato.\narXiv:2502.20966v1  [stat.ML]  28 Feb 2025\n\n\nBergna Depeweg Calvo-Ordoñez Plenk Cartea Hernández-Lobato\nFigure 1: (Left) The architecture of the pre-trained backbone neural network. (Right) The\nGAPA module, applied post-hoc to the first layer to quantify uncertainty without modifying the\noriginal predictions. Illustration based on a toy regression problem from (Ortega et al., 2023).\nNetworks (Neal, 2012), deep ensembles (Lakshminarayanan et al., 2017), and Markov Chain\nMonte Carlo methods. Additionally, regularization-based methods such as Dropout (Gal and\nGhahramani, 2016) and SWAG (Maddox et al., 2019), as well as explicit modeling of weight\nuncertainty (Blundell et al., 2015), have shown promise in improving uncertainty estimates\nin deep learning models. However, there are many challenges that hinder the widespread\napplications of Bayesian modelling: In general, these methods are computationally expen-\nsive or even intractable in practice, for instance requiring the training of multiple DNNs\nor learning a distribution over each weight (Graves, 2011; Hernández-Lobato and Adams,\n2015).\nWith the rise of large pre-trained models in many domains like computer vision\nand natural language, the need to incorporate uncertainty-aware methods already during\nthe model training phase is another limiting factor in their applications (Fort et al., 2019;\nIzmailov et al., 2021). Even methods, such as Monte-Carlo dropout, which may be present\nduring training to act as a regularizer, require multiple forward passes to generate samples\n(Gal and Ghahramani, 2016; Neal, 2012; Lakshminarayanan et al., 2017). In addition, many\nBayesian methods tend to suffer from underfitting, because uncertainty modelling is often\ninherently linked to regularization (mostly via the prior) (Wenzel et al., 2020; Osawa et al.,\n2019). Recently, Laplace approximations have become popular, arguably because they can\nbe applied as a post-processing method to a pre-trained neural network without affecting its\nprediction and empirically capture uncertainty well without requiring sampling. Neverthe-\nless, they demand the calculation of the Jacobian, which is computationally intensive. In\naddition, for scalability reasons, they are typically only employed in the last layer of a model,\nwhich potentially hinders their flexibility (Daxberger et al., 2021; Ortega et al., 2023).\nIn this work, we approach this problem from a different perspective: What if we shift\nour focus from uncertainty in the weight space to uncertainty in the activa-\ntions? Specifically, we model uncertainty at each neuron’s postactivation by fitting a one-\ndimensional Gaussian process to each neuron in the first layer. This approach is inexpensive\nto fit, and can be applied to pre-trained neural networks, without the need of re-training\nor fine-tuning. The second key ingredient is to propagate the obtained a uncetainties at\nthe GP-infused layer (GAPA) through the network using deterministic propagation rules\nakin to determistic variational inference (Wu et al., 2018). Unlike Laplace approximation\nthis combination allows us to model uncertainty at any layer of the network. The method\nis purely post-hoc (it only needs access to the pre-trained model and some training data),\ndoes not require fine-tuning of the model, and, unlike for instance dropout, can express\nuncertainty in a single forward-pass. Importantly, infusing uncertainty in this way does not\n2\n\n\nPost-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian P\nchange the original prediction of the pre-trained model in any way, thereby preserving the\nmodels predictive quality.\nSpecifically, we propose two Gaussian Process Activation function (GAPA) methods.\nThe first, GAPA-Free, is a cost-effective approach that employs empirical kernel methods\nto compute the hyperparameters of the Gaussian process. The second, GAPA-Variational,\nuses variational inducing points to learn the hyperparameters, thereby allowing for greater\nflexibility. Our contributions are as follows:\n• A post-hoc method for pre-trained neural networks that extends them through uncertainty\nmodelling without affecting their predictions.\n• A delta approximation method to propagate the uncertainty from the activation space to\nthe output space.\n• Empirical demonstration that GAPA—and in particular GAPA-Variational—delivers ex-\nceptional performance in uncertainty quantification, outperforming Laplace approxima-\ntions on most datasets.\n• A novel approach to uncertainty quantification by focussing on modelling the uncertainty\nat the activation level rather than in the weight space.\n2. Model Proposition: GAPA + Uncertainty Propagation\nWe begin by presenting the GAPA method, which aims to quantifiy uncertainty in a pre-\ntrained neural network. We assume the network was first trained in a supervised manner on\na dataset D = {(xn, yn)}N\nn=1. Then, to estimate uncertainty, we augment the network by\napplying a Gaussian Process (GP) to the output of each neuron in a layer. To highlight the\ngenerality of the approach we assume here, that this method is applied to the first hidden\nlayer of the network. Figure 1 illustrates the backbone network and the GAPA module.\n2.1. Pretrained Neural Network\nConsider a standard feedforward neural network with L layers: For l = 0, . . . , L, the (l+1)-th\nlayer contains Dl neurons with weight matrix W l ∈RDl×Dl−1, biases bl ∈RDl and activation\nfunction al. For an input x ∈RD0, the network’s prediction is given by\nˆyx = W LaL\u0010\nW L−1aL−1\u0000· · · a1(W 0x + b0) · · ·\n\u0001\n+ bL−1\u0011\n+ bL.\nThis pre-trained network is optimised using standard supervised learning on D, and its\nparameters are subsequently fixed.\n2.2. GAPA: Gausisan Process Activations\nTo quantify the uncertainty of a pre-trained network without affecting its mean predic-\ntions, we attach an independent one-dimensional GP to each neuron in the first layer.\nHere, the pre-trained network (with fixed parameters) has been optimised on the dataset\nD = {(xn, yn)}N\nn=1 using standard supervised learning. Let X := W 0x + b0 ∈RD1 denote\nthe neurons of the first layer. For d ∈{1, . . . , D1}, let Yd := a1(Xd) be the activation of\n3\n\n\nBergna Depeweg Calvo-Ordoñez Plenk Cartea Hernández-Lobato\n−8\n−6\n−4\n−2\n0\n2\n4\n6\nx\n−3\n−2\n−1\n0\n1\n2\n3\ny\nTanh: Neuron 1\nTraining Data\nGP Predictive Mean\n−3\n−2\n−1\n0\n1\nx\n−3\n−2\n−1\n0\n1\n2\n3\ny\nTanh: Neuron 10\nTraining Data\nGP Predictive Mean\n−0.9\n−0.8\n−0.7\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\nx\n−3\n−2\n−1\n0\n1\n2\n3\ny\nTanh: Neuron 11\nTraining Data\nGP Predictive Mean\n−0.9\n−0.8\n−0.7\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\nx\n−3\n−2\n−1\n0\n1\n2\n3\ny\nGAPA: Neuron 11\nTraining Data\nGP Predictive Mean\n95% Confidence Interval\n−3\n−2\n−1\n0\n1\nx\n−3\n−2\n−1\n0\n1\n2\n3\ny\nGAPA: Neuron 10\nTraining Data\nGP Predictive Mean\n95% Confidence Interval\n−0.9\n−0.8\n−0.7\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\nx\n−3\n−2\n−1\n0\n1\n2\n3\ny\nGAPA: Neuron 11\nTraining Data\nGP Predictive Mean\n95% Confidence Interval\nFigure 2: Baseline activations (Top) versus GAPA activations (bottom) for neurons 1, 10, 11.\nGAPA preserves the mean activation while providing an uncertainty estimate.\nthe d-th neuron. We introduce uncertainty at the activation-level by replacing a1(Xd) with\na GP fd(Xd) + ϵd.\nHere, we assume a GP prior fd ∼GP(md, kd), with mean function\nmd(Xd) := a1(Xd), and a covariance kernel kd (specifically, the RBF kernel with hyperpa-\nrameters learned via an empirical method; see Appendix A for further details). Denote the\nneurons and activations of the training data at the first layer by X and Yd = a1(Xd). The\nposterior mean is computed as\nµd(Xd) = md(Xd) + kd(Xd, Xd)\nh\nKd(Xd, Xd) + σ2\nnIN\ni−1\u0010\nYd −md(Xd)\n\u0011\n.\nAs we have Yd = md(Xd) by construction, it follows that µd(Xd) = md(Xd) = a1(Xd).\nHence the pre-trained network’s original activation is preserved. The posterior covariance\nΣd(Xd, X′\nd) = kd(Xd, X′\nd) −kd(Xd, Xd)\nh\nKd(Xd, Xd) + σ2\nnIN\ni−1\nkd(Xd, X′\nd),\nquantifies the epistemic uncertainty in the d-th neuron’s activation. Note, that this doesn’t\ndepend on the prior mean. As shown in Figure 2 for neurons 1, 10, and 11, the GAPA\nmodel preserves the baseline activations while adding a principled uncertainty estimate. In\nsummary, by using a GP whose prior mean is set equal to the neuron’s true activation\n(i.e. its label), we preserve the pre-trained network’s mean predictions while simultaneously\nproviding a rigorous uncertainty epistemic estimate via the GP’s posterior covariance.\n2.3. Propagating the Variance through the Network\nSince the GP at the first layer is constructed to preserve the pre-trained network’s mean\nactivations, the mean forward pass remains identical to that of the pre-trained model. We\nnow need to define a variance-forward path. For this we identify two scenarios: linear layers\n(such as in dense and convolutional layers) and non-linear activation functions.\nLinear Transformation of Variance.\nSince a linear transformation of a Gaussian re-\nmains Gaussian, if the input variance is Σa and the linear layer applies a transformation\nz = Wa, then the resulting variance is given by Σz = W Σa W ⊤.\n4\n\n\nPost-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian P\nPropagation Rules for Non-Linear Activations.\nFor a non-linear activation y = g(z)\napplied to a Gaussian random variable z ∼N(µ, σ2), we approximate g(z) by a first-order\nTaylor expansion (delta approximation)\ng(z) ≈g(µ) + g′(µ)(z −µ).\nSince z −µ ∼N(0, σ2), this yields an approximate variance of Var(y) ≈(g′(µ))2 σ2.\nOverall Variance Propagation.\nBy sequentially applying the linear transformation rule\nfor variance and the delta approximation for non-linear activations, we obtain a tractable,\nlayer-wise method for propagating uncertainty from the first layer (where the GP is applied)\nto the final network output.\n2.4. GAPA-Free: Linear Scaling of the Output Variance\nAfter propagating uncertainty to the network output, we refine the variance using a simple\nlinear transformation:\nVarfinal = θ1 Varoutput +θ2,\nwhere θ1 (a scaling factor) and θ2 (an offset) are learned to capture any residual uncer-\ntainty. This calibration is computationally efficient since it involves only two parameters\nand requires no additional backpropagation through the network.\n2.5. GAPA-Variational\nIn GAPA-Variational, rather than applying a fixed linear scaling, the GP variational pa-\nrameters (similar to those used in variational GPs (Titsias, 2009)) are optimized via max-\nimum likelihood.\nFor each neuron d, we assume a GP prior fd ∼GP\n\u0000md, kd\n\u0001\nwith\nmd(Xd) = a1(Xd) (i.e. the neuron’s activation) and a covariance kernel kd (e.g. the RBF\nkernel with empirically determined hyperparameters). We introduce inducing variables ud\nwith fixed inducing inputs Zd = Xd (taken from the training data of the first layer) and\nset the inducing mean to md(Zd) = a1(Zd). The corresponding variational distribution is\ndefined as q(ud) = N\n\u0000md(Zd), Sd\n\u0001\n, where Sd (the variational covariance) and the kernel hy-\nperparameters θd are learned. Let yi denote the target for the ith input, and let µi and σ2\ni\nbe the predictive mean and variance obtained by propagating the GP uncertainties through\nthe network (using, for example, the delta approximation). Because the GP prior mean is\nfixed to the pre-trained activation, the posterior mean remains unchanged and only the un-\ncertainty (variance) is learned. Consequently, the overall training objective is the Gaussian\nnegative log-likelihood (NLL) L = PN\ni=1\n1\n2 log\n\u00002πσ2\ni\n\u0001\n+ (yi−µi)2\n2σ2\ni\n.\nThis loss function is optimized by backpropagating the NLL from the network’s final\noutput while keeping the pre-trained network weights fixed. In this way, GAPA-Variational\nprovides a flexible, data-driven uncertainty estimate through the learned GP covariance, all\nwhile preserving the original mean predictions of the pre-trained network.\n3. Results\nWe compare GAPA’s predictive distribution with state-of-the-art Laplace-based methods\nfor post-hoc uncertainty quantification in pre-trained networks—including VaLLA, LLA\n5\n\n\nBergna Depeweg Calvo-Ordoñez Plenk Cartea Hernández-Lobato\nTable 1: Results on regression datasets. Best values are in purple, second-best in teal, and\nthird-best in bronze. An asterisk (*) indicates a last-layer LLA variant.\nModel\nAirline\nYear\nTaxi\nNLL\nCRPS\nCQM\nNLL\nCRPS\nCQM\nNLL\nCRPS\nCQM\nMAP\n5.087\n18.436\n0.158\n3.674\n5.056\n0.164\n3.763\n3.753\n0.227\nLLA Diag\n5.096\n18.317\n0.144\n3.650\n4.957\n0.122\n3.714\n3.979\n0.270\nLLA KFAC\n5.097\n18.317\n0.144\n3.650\n4.955\n0.121\n3.705\n3.977\n0.270\nLLA*\n5.097\n18.319\n0.144\n3.650\n4.954\n0.120\n3.718\n3.965\n0.270\nLLA* KFAC\n5.097\n18.317\n0.144\n3.650\n4.954\n0.120\n3.705\n3.977\n0.270\nELLA\n5.086\n18.437\n0.158\n3.674\n5.056\n0.164\n3.753\n3.754\n0.227\nVaLLA 100\n4.923\n18.610\n0.109\n3.527\n5.071\n0.084\n3.287\n3.968\n0.188\nVaLLA 200\n4.918\n18.615\n0.107\n3.493\n5.026\n0.076\n3.280\n3.993\n0.188\nGAPA-Free\n5.083\n18.394\n0.115\n3.644\n4.909\n0.084\n3.668\n4.01\n0.274\nGAPA-Variational\n5.067\n18.282\n0.135\n3.545\n4.796\n0.053\n3.268\n3.552\n0.154\nvariants, and ELLA (Daxberger et al., 2021; Izmailov et al., 2020; Ortega et al., 2023)—on\nthree benchmark regression datasets: (i) the UCI Year dataset, (ii) the US flight delay\n(Airline) dataset (Dutordoir, 2020), and (iii) the Taxi dataset (Salimbeni and Deisenroth,\n2017). We follow the original train/test splits used in prior studies.\nTable 1 summarizes the performance of our proposed models compared to state-of-the-\nart post-processing methods on several regression datasets. Our evaluation metrics include\nNegative Log-Likelihood (NLL), Continuous Ranked Probability Score (CRPS) (Gneiting\nand Raftery, 2007), and the Centered Quantile Metric (CQM) (Ortega et al., 2023). In the\ntable, the best values are highlighted in purple, the second-best in teal, and the third-best in\nbronze. Our experimental results show that both GAPA-Free and GAPA-Variational achieve\ncompetitive performance.\nNotably, GAPA-Variational consistently enhances uncertainty\nquantification. For example, on the Airline dataset, it attains the best CRPS while its NLL\nand CQM values rank among the top three. On the Year dataset, GAPA-Variational records\nthe best CRPS and CQM scores with a competitive NLL. Most importantly, on the Taxi\ndataset, it outperforms all other methods across all metrics. These findings indicate that\nour approach successfully propagates uncertainty from the activation space to the network’s\nfinal output without altering the pre-trained network’s predictions. As a result, GAPA-\nVariational preserves the base network’s predictive accuracy while providing a more reliable\nand nuanced uncertainty estimate, making it well suited for risk-sensitive applications.\n4. Related Work\nIn Morales-Alvarez et al. (2020), auNN replaces activations with GPs and trains them jointly\nacross layers via variational inference, requiring multiple samples at inference time.\nIn\ncontrast, our method uses the original activation (e.g., ReLU) as the GP prior mean—\nthereby preserving the pre-trained network’s predictions—and fits GPs solely to quantify\nthe uncertainty of the activation function. This post-hoc approach avoids re-training the\nnetwork and achieves uncertainty estimation with a single forward pass.\n6\n\n\nPost-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian P\n5. Conclusion\nIn this work, we have introduced the Gaussian Process Activation function (GAPA), a novel\nframework designed to quantify uncertainty in pre-trained neural networks. We have also\npresented a theoretically principled method to propagate uncertainty from the activations\nspace to the output space using the delta approximation approach. Our approach empirically\noutperforms the Laplace approximation method, achieving faster training times. Neverthe-\nless, Gaussian processes remain computationally expensive at inference time. Future work\nwill focus on exploring scalable models or approximations to Gaussian processes to optimise\ncomputational efficiency, as well as extending the model to classification task.\nReferences\nEdmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov. The need for uncertainty\nquantification in machine-assisted medical decision making. Nature Machine Intelligence,\n1(1):20–23, 2019.\nTxus Blasco, J Salvador Sánchez, and Vicente García. A survey on uncertainty quantification\nin deep learning for financial time series prediction. Neurocomputing, 576:127339, 2024.\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncer-\ntainty in neural network. In International conference on machine learning, pages 1613–\n1622. PMLR, 2015.\nPeter Daxberger et al. Laplace approximations for bayesian neural networks. In Proceedings\nof the 38th International Conference on Machine Learning, 2021.\net al. Dutordoir. Us flight delay dataset, 2020. Dataset available at https://...\nStanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape\nperspective. arXiv preprint arXiv:1912.02757, 2019.\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing\nmodel uncertainty in deep learning.\nIn international conference on machine learning,\npages 1050–1059. PMLR, 2016.\nTilmann Gneiting and Adrian E Raftery.\nStrictly proper scoring rules, prediction, and\nestimation. Journal of the American statistical Association, 102(477):359–378, 2007.\nAlex Graves. Practical variational inference for neural networks. Advances in neural infor-\nmation processing systems, 24, 2011.\nJosé Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable\nlearning of bayesian neural networks. In International conference on machine learning,\npages 1861–1869. PMLR, 2015.\nGeoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep\nJaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep\nneural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal processing magazine, 29(6):82–97, 2012.\n7\n\n\nBergna Depeweg Calvo-Ordoñez Plenk Cartea Hernández-Lobato\nPavel Izmailov, Wesley J Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, and\nAndrew Gordon Wilson. Subspace inference for bayesian deep learning. In Uncertainty\nin Artificial Intelligence, pages 1169–1179. PMLR, 2020.\nPavel Izmailov, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Gordon Wilson.\nWhat are bayesian neural network posteriors really like? In International conference on\nmachine learning, pages 4629–4640. PMLR, 2021.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of naacL-HLT,\nvolume 1. Minneapolis, Minnesota, 2019.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep\nconvolutional neural networks. Advances in neural information processing systems, 25,\n2012.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable\npredictive uncertainty estimation using deep ensembles. Advances in neural information\nprocessing systems, 30, 2017.\nJingyao Li, Pengguang Chen, Zexin He, Shaozuo Yu, Shu Liu, and Jiaya Jia. Rethinking\nout-of-distribution (ood) detection: Masked image modeling is all you need. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 11578–\n11589, 2023.\nGeert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio,\nFrancesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken,\nand Clara I Sánchez. A survey on deep learning in medical image analysis. Medical image\nanalysis, 42:60–88, 2017.\nXixi Liu, Yaroslava Lochman, and Christopher Zach. Gen: Pushing the limits of softmax-\nbased out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 23946–23955, 2023.\nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon\nWilson. A simple baseline for bayesian uncertainty in deep learning. Advances in neural\ninformation processing systems, 32, 2019.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning.\nnature, 518(7540):529–533,\n2015.\nPablo Morales-Alvarez,\nDaniel Hernández-Lobato,\nRafael Molina,\nand José Miguel\nHernández-Lobato. Activation-level uncertainty in deep neural networks. In International\nConference on Learning Representations, 2020.\nRadford M Neal. Bayesian learning for neural networks, volume 118. Springer Science &\nBusiness Media, 2012.\n8\n\n\nPost-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian P\nLuis A Ortega,\nSimón Rodríguez Santana,\nand Daniel Hernández-Lobato.\nVaria-\ntional linearized laplace approximation for bayesian deep learning.\narXiv preprint\narXiv:2302.12565, 2023.\nKazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan, Anirudh Jain, Runa\nEschenhagen, Richard E Turner, and Rio Yokota. Practical deep learning with bayesian\nprinciples. Advances in neural information processing systems, 32, 2019.\nHannes Salimbeni and Marc Peter Deisenroth. Deep gaussian processes for regression using\nexpectation propagation. In Advances in Neural Information Processing Systems, 2017.\nSina Shafaei, Stefan Kugele, Mohd Hafeez Osman, and Alois Knoll. Uncertainty in machine\nlearning: A safety perspective on autonomous driving. In Computer Safety, Reliability,\nand Security: SAFECOMP 2018 Workshops, ASSURE, DECSoS, SASSUR, STRIVE,\nand WAISE, Västerås, Sweden, September 18, 2018, Proceedings 37, pages 458–464.\nSpringer, 2018.\nMichalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In\nArtificial intelligence and statistics, pages 567–574. PMLR, 2009.\nFlorian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Świątkowski, Linh Tran, Stephan\nMandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin.\nHow good is the bayes posterior in deep neural networks really?\narXiv preprint\narXiv:2002.02405, 2020.\nAnqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, Jose Miguel Hernandez-\nLobato, and Alexander L Gaunt. Deterministic variational inference for robust bayesian\nneural networks. arXiv preprint arXiv:1810.03958, 2018.\nAppendix A. Empirical Estimation of Inducing Inputs and RBF Kernel\nHyperparameters\nInducing Input Selection:\nTo set the RBF kernel hyperparameters in a data-driven\nmanner, we first select inducing inputs for each neuron’s GP based on the empirical cu-\nmulative distribution function (CDF) of its pre-activation values. Let x denote the one-\ndimensional pre-activation values for a given neuron, and assume these values are sorted\nas\nx(1) ≤x(2) ≤· · · ≤x(N).\nThe empirical CDF is then given by\nF(x(i)) = i\nN ,\ni = 1, . . . , N.\nTo robustly capture the data distribution—especially the boundaries critical for out-of-\ndistribution detection—we always include the minimum x(1) and maximum x(N) as inducing\npoints.\nThe remaining inducing inputs are selected by partitioning the CDF into equal\nquantile intervals. Specifically, if M inducing points are desired (with two reserved for the\n9\n\n\nBergna Depeweg Calvo-Ordoñez Plenk Cartea Hernández-Lobato\nminimum and maximum), then the other M −2 inducing points correspond to quantile\nlevels\npm = m + 1\nM −1,\nm = 1, 2, . . . , M −2.\nEach inducing input is chosen as the x(i) whose empirical CDF value is closest to the\ncorresponding pm.\nRBF Kernel Hyperparameter Estimation:\nThe RBF kernel is defined as\nk(x, x′) = σ2\nf exp\n\u0012\n−(x −x′)2\n2ℓ2\n\u0013\n,\nwhere:\n• ℓis the lengthscale, and\n• σ2\nf is the output scale (variance constant).\nWe estimate the lengthscale ℓas a chosen quantile (e.g., the 25th percentile) of the pairwise\nEuclidean distances among the selected inducing inputs:\nℓ= quantile\n\u0010\n{|xi −xj| : i ̸= j}, q\n\u0011\n,\nwith q = 0.25.\nThe output scale is set based on the variance of the training outputs (activation function):\nσ2\nf = max\n\u0010\n1, Var(ytrain)\n\u0011\n.\n10\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20966v1.pdf",
    "total_pages": 10,
    "title": "Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian Processes",
    "authors": [
      "Richard Bergna",
      "Stefan Depeweg",
      "Sergio Calvo Ordonez",
      "Jonathan Plenk",
      "Alvaro Cartea",
      "Jose Miguel Hernandez-Lobato"
    ],
    "abstract": "Uncertainty quantification in neural networks through methods such as\nDropout, Bayesian neural networks and Laplace approximations is either prone to\nunderfitting or computationally demanding, rendering these approaches\nimpractical for large-scale datasets. In this work, we address these\nshortcomings by shifting the focus from uncertainty in the weight space to\nuncertainty at the activation level, via Gaussian processes. More specifically,\nwe introduce the Gaussian Process Activation function (GAPA) to capture\nneuron-level uncertainties. Our approach operates in a post-hoc manner,\npreserving the original mean predictions of the pre-trained neural network and\nthereby avoiding the underfitting issues commonly encountered in previous\nmethods. We propose two methods. The first, GAPA-Free, employs empirical kernel\nlearning from the training data for the hyperparameters and is highly efficient\nduring training. The second, GAPA-Variational, learns the hyperparameters via\ngradient descent on the kernels, thus affording greater flexibility. Empirical\nresults demonstrate that GAPA-Variational outperforms the Laplace approximation\non most datasets in at least one of the uncertainty quantification metrics.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}