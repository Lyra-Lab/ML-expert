{
  "id": "arxiv_2502.21134v1",
  "text": "IEEE ROBOTICS AND AUTOMATION LETTERS\n1\nDynamically Local-Enhancement Planner for\nLarge-Scale Autonomous Driving\nNanshan Deng, Weitao Zhou, Bo Zhang, Junze Wen, Kun Jiang, Zhong Cao, Diange Yang\nAbstract—Current autonomous vehicles operate primarily\nwithin limited regions, but there is increasing demand for broader\napplications. However, as models scale, their limited capacity\nbecomes a significant challenge for adapting to novel scenarios. It\nis increasingly difficult to improve models for new situations using\na single monolithic model. To address this issue, we introduce\nthe concept of dynamically enhancing a basic driving planner\nwith local driving data, without permanently modifying the\nplanner itself. This approach, termed the Dynamically Local-\nEnhancement (DLE) Planner, aims to improve the scalability\nof autonomous driving systems without significantly expanding\nthe planner’s size. Our approach introduces a position-varying\nMarkov Decision Process formulation coupled with a graph\nneural network that extracts region-specific driving features from\nlocal observation data. The learned features describe the local\nbehavior of the surrounding objects, which is then leveraged\nto enhance a basic reinforcement learning-based policy. We\nevaluated our approach in multiple scenarios and compared\nit with a one-for-all driving model. The results show that our\nmethod outperforms the baseline policy in both safety (collision\nrate) and average reward, while maintaining a lighter scale. This\napproach has the potential to benefit large-scale autonomous\nvehicles without the need for largely expanding on-device driving\nmodels.\nIndex Terms—Autonomous Driving, Reinforcement Learning,\nDriving Policy\nI. INTRODUCTION AND MOTIVATION\nAutonomous driving systems has achieved remarkable\nprogress in recent years [1], with companies like Waymo and\nCruise demonstrating the ability to operate over 10,000 miles\nwithout disengagement [2].\nAlthough current road tests are conducted primarily in\nspecific regions or road types, the ultimate goal is to en-\nable large-scale deployment. However, significant variations in\ndriving features across regions pose considerable challenges.\nThe performance of autonomous vehicles may degrade when\noperating in regions with different driving characteristics.\nWaymo’s safety report underscores the considerable effort\nrequired to familiarize autonomous systems with new regu-\nlations, road rules, and local driving styles before entering\nnew areas. Similarly, Tesla claims that its FSD autonomous\ndriving system encountered challenges with traffic rules when\nadapting to Chinese roads.\nNanshan Deng, Weitao Zhou, Junze Wen, Kun Jiang, Diange Yang are with\nthe School of Vehicle and Mobility, Tsinghua University, Beijing, China.\nBo Zhang is with Didi Global, China.\nZhong Cao is with Department of Civil and Environmental Engineering,\nUniversity of Michigan.\nW.\nZhou\nand\nD.\nYang\nare\nthe\ncorresponding\nauthors.\n(zhouwt,\nydg@mail.tsinghua.edu.cn)\nAs the driving region expands, the failure rate increases, and\nmaintaining consistent, non-conflicting decision-making logic\nbecomes progressively more challenging. This issue raises\nconcerns about the adaptability of autonomous systems and\nimpedes their broader application. The goal of this work is to\naddress the adaptability challenges of autonomous vehicles in\nlarge-scale environments.\nOne intuitive approach is to continually enhance the ca-\npacity of autonomous driving models to cover more driving\nregions. However, large-scale applications impose significantly\nhigher requirements on these models. For instance, as the\ntraining dataset expands, the model size tends to increase sub-\nstantially [3]. A similar issue arises when relying on a single\nmodel to handle all driving regions. In rule-based systems, this\nchallenge manifests as contradictions among existing rules.\nIntroducing a new rule for a novel scenario requires ensuring\nthere is no conflict with previously established rules, a task that\nbecomes increasingly difficult as the number of rules grows.\nThere are two primary solutions to this problem: increasing\nmodel capacity or restructuring scenarios for simplification.\nModel enlargement methods include meta-learning, transfer\nlearning, and exploring novel neural network structures, which\nwill be briefly discussed in Section II. However, these methods\nstill fall short of delivering satisfactory real-world perfor-\nmance.\nIn practice, many companies focus on scenario restructuring,\nemploying strategies such as simplified road structures [4], cat-\negorized scenarios, and abstract driving behaviors. For exam-\nple, planners use Frenet coordinates to represent lanes, treating\nall lanes as straight with uniform width. Some planners adopt\nscenario classification approaches, creating dedicated planners\nfor specific driving conditions, such as highway or intersection\ndriving [5]. Additionally, approaches based on finite state\nmachines (FSM) define abstract behaviors to adapt to new\ndriving scenarios. However, these simplifications still rely on\nthe goal of using a single model for all driving scenarios and\nmay introduce safety risks by overlooking regional differences\n[6].\nOur approach stems from the observation that autonomous\ndriving systems exhibit strong regional differences. For exam-\nple, a vehicle operating in China does not need to incorporate\ndriving data from the U.S. Similarly, a driver relocating to a\nnew city only requires minor adjustments to their driving style\nfor adaptation.\nMotivated by this insight, we propose a dynamic local-\nenhancement planner (DLE) that enhances a basic planner with\nlocal driving data. This approach adapts to regional variations\nwithout significantly increasing the onboard computational\narXiv:2502.21134v1  [cs.RO]  28 Feb 2025\n\n\nIEEE ROBOTICS AND AUTOMATION LETTERS\n2\nburden. The key contributions of this work are as follows:\n• We propose a Dynamic Local-Enhancement Planner\n(DLE) framework that adapts autonomous vehicles to\nregional driving styles by augmenting a base policy with\nlocal driving data, eliminating the need for significant\nscaling of model capacity.\n• We propose a graph-based local feature extraction method\nthat dynamically captures local driving patterns through\nhierarchical spatiotemporal representation, integrating\nroad structure for local traffic behavior modeling and\nstorage.\n• We develop a reinforcement learning-based policy en-\nhancement method that dynamically optimizes basic driv-\ning policy through regional feature integration, enabling\nautonomous vehicles to autonomously adjust decision-\nmaking mechanisms according to localized environmental\ncharacteristics, thereby achieving cross-regional behav-\nioral adaptability and performance improvement.\nII. RELATED WORKS\nDue to the lack of stability in driving policies, particu-\nlarly in regions with dynamic changes, researchers often rely\non approximation, transformation, or simplification methods.\nThese approaches reframe the problem into more manageable\nforms, enabling the exploration of effective decision-making\nstrategies in dynamic driving environments while alleviating\nthe computational challenges posed by the original problem.\nThis section reviews existing methods aimed at scaling\nautonomous driving by enlarging driving models. These meth-\nods can be broadly categorized into three types: improving\nrobustness, transfer learning, and representation learning.\nA. Improving robustness\nImproving the adaptability of autonomous driving algo-\nrithms is crucial for ensuring robust decision-making in diverse\nenvironments. Methods like the Reachable Set [7] and the\nResponsibility Model [8] enhance safety by adopting conserva-\ntive driving policies focused on physical constraints. However,\nthis increased conservatism can limit the set of feasible actions,\nreducing flexibility. Other approaches improve adaptability by\nintroducing noise during training or by incorporating envi-\nronmental assumptions. A common technique for simplifying\nplanning is unifying the planner’s coordinate system, such\nas using the Frenet coordinate system. While this simplifies\nplanning, human driving behavior varies significantly across\nregions [9], and generalized policies based on such assump-\ntions can conflict with human cognition, leading to poor\nperformance in unfamiliar traffic environments.\nB. Transfer Learning\nRecent studies have approached autonomous driving as a\ncollection of distinct Markov Decision Process (MDP) prob-\nlems, each tailored to specific regions with different state and\naction spaces. Transfer learning in reinforcement learning (RL)\naims to improve decision-making efficiency by transferring\nknowledge across tasks. This approach addresses the challenge\nof maintaining stability in RL performance across various\ndriving scenarios, including urban [10], highway [11], and\nother settings.\nTo mitigate sparse reward problems, reward shaping [12]\nis employed, where external knowledge is incorporated as a\nlatent function to augment rewards. Policy distillation [13] is\nalso used to minimize the distributional difference between\nteacher and student policies. Meta-reinforcement learning [14],\nwhich uses a meta-network to optimize parameters, enables\nagents to quickly adapt to different tasks. Enhancements to\ntraining data, such as adding expert demonstrations [15] or ad-\njusting data distribution based on expert knowledge [16], have\nalso been shown to improve decision-making performance.\nThis paper focuses on leveraging regional data to enhance\nadaptability in dynamic environments, rather than assuming\nconsistent dynamics [17].\nC. Representation learning\nThe quality of state representation plays a crucial role in\ndecision-making performance, as it determines the information\ncontent and dynamics of the environment. Effective state\nrepresentations leverage prior knowledge to improve model\nperformance [18]. Vehicle state can be represented using con-\ntinuous variables like position, velocity, and orientation [19].\nThe Frenet coordinate system simplifies road structures but\nmay introduce issues such as state confusion and difficulties\nwith varying vehicle numbers. Real-world driving is inherently\na Partially Observable Markov Decision Process (POMDP)\n[20], so the state representation must account for incomplete\ninformation and the complexity of policy generation.\nTraditional approaches often use images to represent the\nstate space, such as main-view camera images [21] or bird’s-\neye view images [22], but this can increase the complexity of\nextracting valid information. Other approaches, like occupancy\ngrids [23], process raw data into custom resolution grids,\navoiding issues related to dimensional changes. Graph Neural\nNetworks (GNNs) offer an inductive bias model [18] that uses\na flexible graph structure to model complex interactions and\nenhance decision-making [24]. GNNs have been successfully\napplied to solve hidden state inference in POMDPs [25]. In\nthis work, a two-layer GNN structure is used to construct a\nbasic decision state, integrating normalized decision-making\nwith local information.\nIII. DYNAMIC LOCAL ENHANCEMENT PROBLEM\nDESCRIPTION\nA. Preliminaries\nMarkov decision process (MDP) is commonly used to\ndescribe the autonomous driving planning problem. An MDP\nis defined by the tuple ⟨S, A, R, T ⟩, where S denotes the\nstate space, A denotes the action space, R : S →R is\nthe reward function, and T : S × A →R is the transition\nprobability. The objective of policy π : S × A →[0.1] is to\nselect actions that maximize the expected reward over time\nVπ(s) := Eπ[PH+t\ni=t γi−tri|sh = s], donated as Vπ(s), where\nγ ∈[0, 1] denotes discount factor, and H ∈N denotes the\nplanning horizon.\n\n\nIEEE ROBOTICS AND AUTOMATION LETTERS\n3\nIn large-scale autonomous driving tasks, treating all envi-\nronments as a single MDP model may lead to differences\nbetween the actual situation and the model from a local region\nperspective. In this case, the policy π obtained using all data\nD of sufficient size nδ has a performance difference from the\noptimal policy ˜π in each real region, the difference donated\nas:\n∆sVπ = ||ΣsVπ −ΣsV˜π||\n(1)\nThis work focuses on minimizing ∆sVπ without directly\naddressing the values of nδ.\nB. Problem description\nThe proposed Dynamically Local Enhancement (DLE) plan-\nner operates as follows:\nA vehicle uses a basic planner for general driving. To adapt\nto local driving conditions, it collects driving data at its current\nposition and dynamically updates the planner. Once the vehicle\nleaves the area, the planner reverts to the basic policy. Figure\n1 illustrates the DLE planner’s autonomous driving process.\nOur proposed dynamically local enhancement (DLE) planner\noperates as follows:\nFigure 1: Main idea: Dynamically enhancing a basic driving\npolicy with local driving data when the autonomous vehicle\ndriving to different regions, improving the scalability of the\nsystem without significantly expanding the planner’s size.\nThis problem can be formalized as:\nπ(g) = f(πb, D(g))\ns.t., ∆s(g)Vπ(g) ≤∆s(g)Vπ\n(2)\nwhere g represents the global position parameters. Equation 2\nshows that the policy only needs to account for future state\npossibilities at the current position, rather than considering all\npossible future states. This allows the planner to avoid en-\nlarging the driving model while achieving better performance\ncompared to a general policy.\nC. Framework\nAs shown in Fig. 2, our framework consists of three com-\nponents: the basic policy, the local-enhancement information,\nand the dynamic policy enhancement. Its goal is to ensure the\nenhanced policy outperforms the basic one.\nThe basic policy uses a reinforcement learning model that\nlearns from available data. The local-enhancement module\ncollects regional statistical data, while the dynamic policy\nenhancement adjusts decisions based on this data and historical\nperformance.\nWe assume local data can be gathered via road facilities or\nother vehicles, stored in High-Definition (HD) maps. Several\nHD maps and driving datasets already support this function-\nality.\nFigure 2: Our framework establishes a hybrid policy archi-\ntecture that enhance a basic policy with dynamic local adap-\ntation: The basic policy πb works globally, while parallelly,\na local-enhancement module extracts region-specific features\ng. Finally, the basic policy will be updated dynamically via\ninformation-theoretic optimization. This dual-stream design\nenables local adaption of driving policy through historical\ndriving data collected locally.\nIV. METHOD\nA. Region-Related Driving Processes\nTraditional MDP models assume that transition probabili-\nties, T (s, a, s′), are independent of geographic location. How-\never, in practice, driving policies that work well in one region\nmay not be suitable for others due to differing driving styles\nand environmental factors. To address this, we propose the\nPosition-Varying Markov Decision Process (POVMDP), where\nthe transition model is modified to account for the global\nposition g:\nT (s, a, s′, g) = Pr(st+1 = s′|st = s, at = a, G = g)\n(3)\nThus,\nthe\nPOVMDP\nis\ndefined\nas\nM(g)\n=\n{S, A, R, T (g)}, and the value function becomes:\nVπ(s, g) := Eπ[PH\nt=h γt−hrt|sh = s, G = g]\n(4)\nIn this setup, the value function is the weighted average of\nthe POVMDP value functions Vπ(s, g) across regions, which\ncan lead to overestimation or underestimation in different\nareas.\nDirectly incorporating the global position into the state\nspace (e.g., s := {s, g}) increases data sparsity, requiring\nimpractical amounts of data for policy fitting. Furthermore,\nadapting to new regions through extensive environmental\ninteractions is not feasible.\nOur approach mitigates this by using regional data to obtain\nthe value function Vπ(s, g), which helps reduce data sparsity.\n\n\nIEEE ROBOTICS AND AUTOMATION LETTERS\n4\nWe transform the problem of directly computing the value\nfunction into obtaining an expressive representation space X\nthat captures the driving environment’s dynamics:\nV (x) = V (u(s, D(g))) = Eπ[Gt|sh = s, G = g]\n(5)\nHere, u is a spatial mapping function that maps the state space\nS to a new space X based on data D(g). This approach yields\na more compact representation, addressing the challenges of\ndirectly incorporating global positions into the state space.\nB. Local Enhancement Information Extraction\nTo enable adaptive decision-making based on local driving\ndata, we design a basic policy that incorporates regional\ndetails, making it suitable for various autonomous driving\nscenarios. The policy processes decision states, typically repre-\nsented as images or physical attributes, using a graph network\napproach [26].\nIn the basic planner’s experience phase, the state s ∈Rn×F\nrepresents the F-dimensional attributes of n vehicles in the\nFrenet coordinate system. Each vehicle’s state is expressed in\nthe lane coordinate system (LCS). Environmental features are\nrepresented as nodes in the road coordinate system, forming\na basic reinforcement learning decision module.\nThe LCS is constructed with vehicle nodes nv and a main\nreference node nr\n0. Each vehicle node nv\ni includes attributes\nsuch as longitudinal and lateral distances to the centerline\n(si, li), speed ˙si, lateral speed ˙li, and orientation ∆θi. The\nset of vehicle nodes is denoted as N v = [nv\n0, nv\n1, .., nv\ni ].\nFor regional information incorporation, we use graph net-\nworks with decision state encoding. Vehicle nodes are encoded\nand aggregated with other nodes using the same node encoder\nparameters θv:\nxv\ni = MLPθv(nv\ni )\n(6)\nThis ensures normalized coordinate systems. Decision state\nencoding facilitates reinforcement learning processing of both\nstates with and without regional information. Neural network\nsparsity maps states to unique representations, with the univer-\nsal approximation theorem [27] ensuring efficient mapping.\nEach vehicle node xv\ni receives related regional information\nxr\ni through the graph network structure. The network param-\neters (θv and θr) enable an overparameterized mapping. The\nfinal encoded decision state x is defined as:\nx = gϕ(xv, xr) = xvW0 + P\nj\nσxr\njW1\n(7)\nwithout and with regional information. The implementation\nis based on the sparse features of neural networks, mapping\nstates with different meanings to unique states. According to\nthe universal approximation theorem [27], a single mapping\nfunction is carried out through the neural network. Each\nvehicle node xv\ni receives related local information node xr\ni\nthrough the graph network structure.\nThe parameters (θv and θr) of the network MLPθv and\nMLPθr enable an over-parameterized mapping. The final\nencoded decision state (x) is defined as:\nx = gϕ(xv, xr) = xvW0 + P\nj\nσxr\njW1\n(8)\nwhere W0, W1 are weight matrices, σ is the nonlinear layer,\nand xr\nj represents related road nodes.\nIn summary, the basic decision method processes deci-\nsion states by incorporating vehicle and regional information\nthrough graph networks and encoding, ensuring adaptability to\ndiverse traffic environments and supporting any reinforcement\nlearning method.\nC. Dynamically Driving Policy Enhancement\nWe use a regional data feedback module to store local\ntraffic characteristics and integrate them into the basic decision\nalgorithm. Local characteristics induce changes in environ-\nmental dynamics within a unified state space, with graph\nneural networks employed to handle interactions between local\nknowledge features and standardized decision states.\n1) Regional data Container: Autonomous driving maps\nstore geographical data hierarchically, which we leverage as\nthe regional data carrier for local information. This data is en-\ncoded using graph networks to represent dynamic differences\nin the environment. Road nodes are connected based on lane\nstructure and maximum speed distance, forming a network\nwhere vehicle nodes interact with the road nodes.\nThe connection relationships of road nodes are illustrated\nin Fig. 3. The road node features are defined by their relative\nposition to the reference node nr\n0, encoded as:\nThe connection relationships of road nodes are illustrated\nin Fig. 3. Lane nodes align along the road’s center line, with\nadjacent points in the same lane set at half of the 1s travel\ndistance at the maximum speed limit. Each node connects to its\npredecessor node, and road nodes connect to their associated\nvehicle nodes.\nhr\ni = MLPθr(nr\ni −nr\n0)\n(9)\nFigure 3: Connection relationships between local feature\nnodes. Road nodes connect to previous road nodes, and vehicle\nnodes connect through nodes of interest.\nGraphsage is used to aggregate information from neighbor-\ning nodes [28]. The aggregation process is:\nxk\nN(n) = σ(MEAN(xk−1\nu\n, ∀u ∈N(n))\nxk\nv = σ(W k · CONCAT(xk−1\nv\n, xk\nN(n)))\n(10)\nwhere MEAN, CONCAT denote the mean clustering\nfunction and the vector concatenation function, respectively.\nN(n) denotes the neighborhood node connected to node n. σ,\nW k and k denote the non-linear function, the weight matrix,\n\n\nIEEE ROBOTICS AND AUTOMATION LETTERS\n5\nand the depth of collection. The state xk at depth k is defined\nas xr, forming local characteristic parameters x with Eq. 10.\nFigure 4: Local information extraction process.\n2) Historical Information Feedback: As shown in Fig.4, the\npolicy state x is divided into xc and xl, where xc = gϕ(xv)\nis the state without local info , and xl = gϕ(xv, xr) indicates\nthe combined information. The mutual information between\nthe state-action trajectory Y\n: y = [s0, a0, s1, ..., sh] and\nlocal information Xc = gϕ(Eθs(s)) is maximized to reduce\nuncertainty in decision-making.\nLocal mutual information I(Y ; Xl) is estimated using the-\nMutual Information Neural Estimation [29] method, and com-\nmon mutual information I(S; Xc) is estimated similarly.\nI(Y ; Xl) := DKL(PY,Xl||PY PXl) ≥bI(DV )\nw\n(Y ; Xl)\n:= EPY,Xl [Tw(y, xl)] −log EPY PXl\n\u0002\neTw(y,xl)\u0003\n(11)\nwhere Tw : Y × Xl →R is a function modeled by parameter\nw. Xl = gϕ(Eθv(s), Eθr(nr)). Local mutual information\nI(Y ; Xl) is estimated by maximizing bIw (Y ; Xc):\nwl, φ, θs, θr = arg max\nwl,φ,θs,θr\nbIwl (Y ; gϕ(Eθv(s), Eθr(nr)))\n(12)\nwhere wl denotes local function parameters. φ, θs, θr are the\nparameters of the decision state encoder, the vehicle, and the\nlocal structure encoder mentioned earlier.\nCommon mutual information I(S; Xc) is estimated in the\nsame way with common function parameter wc:\nwc, φ, θs = arg max\nwc,φ,θs\nbIwc (S; gϕ(Eθs(s)))\n(13)\nThe two objectives update the three state encoding networks\nbefore the input to the decision system, so they are updated\ntogether:\narg max\nwl,wc,φ,θs,θr\n(bIwc (S; Xc) + αbIwl (Y ; Xl))\n(14)\nwhere α is a hyperparameter. The difference from the original\nmethod is that the data is constantly updated and changed after\nthe planning process.\n3) Training Process: We take the DQN algorithm as an\nexample to illustrate the update method. With two replay\nbuffers: one for storing the reinforcement learning transitions\nAlgorithm 1 DLE Planner\n1: Initialize replay memory D, eD\n2: Initialize action-value function Qθrl, wl, wc, φ, θs, θr\n3: while episode < M do\n4:\nInitial state s0\n5:\nChoose to use local information with a probability of\n0.5\n6:\nwhile not Done do\n7:\nx =\n(\ngϕ(Eθs(s)), without local info\ngϕ(Eθs(s), Eθr(nr)), with local info\n8:\nSelect action at with ε-greedy with input x\n9:\nObserve and store (xt, at, rt, xt+1) into D\n10:\nEncoding and store (y, xl, xc, s)t into eD\n11:\nDraw minibatch samples from joint distribution\nPs,xc, Py,xl and marginal distribution Ps, Pxl\n12:\nUpdate wl, wc, φ, θs, θr with target Eq.16\n13:\nSample minibatch from D\n14:\nUpdate θrl, wl, wc, φ, θs, θr with target Eq.15\n15: End While\nand one for storing the corresponding encoded states. The loss\nfunction for reinforcement learning is:\nLrl(θrl, θenb) = E\n\u0014\n(r + γmax\na‘ Q (s′, a′) −Q (s, a)2\n\u0015\n(15)\nwhere θrl denotes the parameters of Q-net and θenb denotes\nall encoding parameters. The loss of the decision also updates\nthe encoding network, while the encoding network performs\nasynchronous updates. Changes in the planned outcomes will\naffect the dynamics of the environment, resulting in fluctua-\ntions in planning. To eliminate the oscillation caused by such\nupdates, the loss function of the encoding part of the network\nis modified as:\nLenb(wl, wc, φ, θs, θr) = −β(bIwc (S; Xc) + αbIwl (Y ; Xl))\n(16)\nwhere β is a hyperparameter that drops to 0 over the training\nprocess , allowing the system to stabilize in later stages.\nV. EXPERIMENT\nThis section aims to conduct a comprehensive evaluation of\nthe proposed dynamically local enhancement planner (DLE)\nby comparing it with planners that do not take local in-\nformation into account. We will first introduce the region-\nrelated test scenarios, the design of baseline planners, and the\nperformance metrics. Subsequently, the training process will\nbe presented. Finally, we will assess the performance of the\nproposed planner.\nA. Design of Region-related Test Scenarios\nTo validate the adaptability of the proposed method to\nregion-specific driving scenarios, two test scenarios are de-\nsigned from different regions, as shown in Fig. 5. In these\nscenarios, the autonomous vehicle interacts with merging\nvehicles, where road structures and vehicle behaviors differ\n\n\nIEEE ROBOTICS AND AUTOMATION LETTERS\n6\nbetween regions. Specifically, the lane-changing probabilities\nof merging vehicles vary; in one region, they are more likely\nto change lanes in front of the autonomous vehicle, creating\ndifferent optimal driving policies for each scenario. This tests\nwhether the trained policy can adapt to the unique character-\nistics of each region. Additionally, within each scenario, the\ninitial positions of vehicles and their behaviors are generated\nusing probabilistic sampling, introducing further variability.\nFigure 5: The test scenarios sourced from different regions\nare designed to have distinct road structures and surrounding\nvehicle behaviors. In the upper scenario, the merging - in\nvehicle is likely to maintain its lane. Conversely, in the lower\nscenario, it is more prone to changing lanes.\nThe scenarios are implemented in the highway-env simula-\ntor based on real traffic data [30]. The longitudinal behavior\nof the environmental vehicle is modeled using the Intelligent\nDriver Model (IDM) [31], and the lateral behavior is mod-\neled using the Minimizing Overall Braking Induced by Lane\nChange Model (MOBIL) [32]. The parameters of the IDM and\nMOBIL model are shown in Table.I.\nTable I: Parameters of IDM and MOBIL model\nParameters\nSymbol\nValue\nDesired velocity\n˙x0\n50 km/h\nDesired time gap\nT\n1.5 s\nJam distance\ng0\n5 m\nExponent for velocity\nδ\n3.4-4.5\nMax acceleration\na\n3 m/s2\nDesired deceleration\nb\n-5 m/s2\nPoliteness index\np\n0.2\nLane change threshold\nδath\n0.2 m/s2\nWe use the positions, speeds, and orientations of the seven\nnearby vehicles in the lane coordinate system as the state\nspace, with discrete actions consisting of acceleration, decel-\neration, lane change, and hold. Both testing scenarios shares\nthe same set of reward function, which consists of four parts:\ncollision penalty rc, velocity reward rv, lane change penalty\nrl, and politeness rp, as follows:\nr = wc · rc + wv · rv + wl · rl + wp · rp\n(17)\nwhere w is the weight. When a collision occurs, rc = 1;\notherwise, it is 0. rv is normalized to [0, 1] according to the\nmaximum speed limit of the current road. rl = 1 when the lane\nchange action is successfully executed. rl is used to reduce the\nimpact of the self-vehicle on the environmental vehicle and is\ndefined as the proportion of the environmental vehicle speed\nreduction caused by the ego-vehicle.\nrv = (vtarget−v)\nvtarget\n(18)\nwhere vtarget is the target speed of the surrounding vehicle.\nThe weight of each part is wc = −1, wv = 0.2, wl = −0.05,\nand wp = −0.1,with a discount factor γ = 0.95.\nB. Baseline planner design\nWe compare the proposed DLE model with two baselines: a\nsingle-model (SM) driving policy of the same parameter size,\nand a large-parameter global model.\n1) Large-Parameter Global Driving Policy: The first base-\nline verifies if a large-parameter model can adapt to diverse\nregional driving characteristics. Drawing on the Mixture of\nExperts (MoE)[33] in large language models, we designed a\nlarge parameter global driving policy (GM). It selectively acti-\nvates a specific expert policy according to location, combining\nregional expert policies into a large-parameter driving model.\nIn this way, the parameter number of GM policy is ns(number\nof regions) times of the proposed DLE planner.\n2) Local Driving Policy: We employ the deep Q-learning\n(DQN) method to generate local driving policies. For compar-\nison, we design three local policies: LM1, trained solely in\nregion 1; LM2, trained in region 2; and LM12, trained using\ndata from both regions. The parameter count of the single-\nmodel policies is consistent across these cases. The training\ndata for each baseline are shown in Table II.\nTable II: Comparsion of Different Baselines\nModel\nparameters\nnumber\nData 1\nData 2\nGlobal\nLocation\nLM1\n9.6 × 106\n✓\n×\n×\nLM2\n9.6 × 106\n×\n✓\n×\nLM12\n9.6 × 106\n✓\n✓\n×\nGM\nns × 9.6 × 106\n✓\n✓\n✓\nDLE\n9.6 × 106\n✓\n✓\n✓\nC. Performance metrics\n1) Average performance ratio (APR): To evaluate policy\nperformance across regions, we define APR as:\nAPR =\nPns\ni=1 Gi\nπ\nPns\ni=1 Gi\nopt\n(19)\nwhere ns is the number of regions, Gi\nπ is the average reward\nof the target policy in the local region i, Gi\nopt is the average\n\n\nIEEE ROBOTICS AND AUTOMATION LETTERS\n7\nreward of optimal policy in the local region i. Optimal policy\nrefers to the policy that interacts only with the local interaction\nenvironment.\n2) Collision Rate: The collision rate Rc measures the fre-\nquency of collisions of a driving policy during the evaluation\ntesting, which is defined as:\nRc = NC\nNT\n(20)\nD. Result\nThe training process is shown in Figure 6. To evaluate and\ncompare the overall performance across regions, we present\nthe average performance of LM12, GM, and DLE Planner\nin two scenarios. The solid line represents the mean episode\nreward from 10 random tests during training, with the shaded\narea indicating the 90% confidence interval.\nDuring training, the GM method converges more slowly\ndue to its large parameter size and sparse updates. The LM12\nmodel, trained with mixed data from both regions, lags behind\nthe other methods in final performance.\nFigure 6: Training Process Episode Reward. The solid line\nrepresents the mean episode reward across the two scenarios,\nwhile the shaded area indicates the 90% confidence interval.\nTable III: The average performance ratio results of different\nmethods\nDriving\nPolicy\nTraining\nScenario\nTest\nScenario\nAPR\nCollision\nRate Rc\nGM\n1&2\n1&2\n1.00\n0%\nLM1\n1\n1&2\n0.70\n36%\n1\n2\n0.41\n72%\nLM2\n2\n1&2\n0.90\n0%\n2\n1\n0.81\n0%\nLM12\n1&2\n1&2\n0.91\n6%\nDLE\n1&2\n1&2\n0.99\n0%\nWe conducted 100 tests for each policy in both scenarios,\nsummarized in Table III. The GM model’s APR value was\nused as the benchmark (1.0) for comparison.\nThe LM1 model, trained on Scenario 1 alone, showed\nhigher collision rates in Region 2, causing a drop in APR.\nThe LM2 model avoided collisions but saw performance drop\nto 0.90. The LM12 model, trained on both regions, performed\n(a) Example Case of the LM planner\n(b) Example Case of the proposed DLE planner\nFigure 7: Test results in a Specific Region.The upper subgraph\nillustrates the planning outcomes of LM planner, while the\nlower subgraph displays the planning outcomes with the\ninclusion of extracted local driving feature. Distinct decision\nactions emerge within the state space described by the lane\ncoordinate system.\nslightly better than the first two but still fell short of GM. The\ntrade-off in performance across regions highlights that simply\nincreasing training data diversity does not always improve\nplanning performance.\nIn contrast, the DLE model outperformed the single-region\nmodels and closely approached GM’s performance. The DLE\nplanner proves superior in dynamic environments by lever-\naging storage space to optimize planning results, surpassing\ngeneral planners.\nWe show a case to compare the proposed DLE planner with\na trained LM12 driving policy at a T-junction.The autonomous\nvehicle drives straight on the main road, while an environmen-\ntal vehicle may turn left from the intersection. Both policies\ngather information about surrounding vehicles, and the DLE\nplanner uses historical driving data at the intersection.\nFigure 7 shows the ego vehicle’s trajectory, speed, steering\nangle, and behavioral decisions. The LM12 planner initiates\ndeceleration upon encountering merging vehicles and switches\nlanes to the left to avoid them. However, half of the vehicles\nin the right lane do not interact with the ego lane, causing the\npolicy to prioritize deceleration.\nThe DLE planner, in contrast, maintains maximum speed\nand preemptively changes to the left lane to avoid potential en-\ncounters with merging traffic. This approach minimizes lane-\nchange losses and reduces stop-and-go occurrences, showcas-\ning the DLE planner’s ability to adapt to local traffic dynamics\n\n\nIEEE ROBOTICS AND AUTOMATION LETTERS\n8\nby incorporating historical behavior into its decisions.\nVI. CONCLUSION\nIn this study, we proposed the DLE planner, a model\ndesigned to enhance the adaptability of autonomous driving\npolicies across regions with diverse driving characteristics.\nBy leveraging a regional driving model that integrates local\nenvironmental data and historical driving behavior, the DLE\nplanner demonstrates superior performance compared to tra-\nditional approaches, such as single-region models and large-\nparameter global models. The DLE planner strikes a balance\nbetween generalization and localization, closely matching the\nperformance of the large-parameter model while maintaining\nmore efficient training and decision-making processes. The\nability to incorporate historical data allows the DLE planner\nto optimize decisions in dynamic environments, reducing\ncollision risks.\nOverall, the DLE planner proves to be a robust and scalable\nsolution for autonomous driving, offering enhanced flexibility\nand performance across diverse driving scenarios, and has\nthe potential to enable large-scale deployment of autonomous\nvehicles without the need for substantial expansion of on-\ndevice driving models.\nREFERENCES\n[1] S. Xu, R. Zidek, Z. Cao, P. Lu, X. Wang, B. Li, and H. Peng,\n“System and experiments of model-driven motion planning and control\nfor autonomous vehicles,” IEEE Transactions on Systems, Man, and\nCybernetics: Systems, vol. 52, no. 9, pp. 5975–5988, 2021.\n[2] “Disengagement\nreport,”\nhttps://www.dmv.ca.gov/portal/\nvehicle-industry-services/autonomous-vehicles/disengagement-reports/.\n[3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\nmodels to follow instructions with human feedback,” Advances in Neural\nInformation Processing Systems, vol. 35, pp. 27 730–27 744, 2022.\n[4] B. J. Patz, Y. Papelis, R. Pillat, G. Stein, and D. Harper, “A practical\napproach to robotic design for the darpa urban challenge,” Journal of\nField Robotics, vol. 25, no. 8, pp. 528–566, 2008.\n[5] D. Xu, Z. Ding, X. He, H. Zhao, M. Moze, F. Aioun, and F. Guillemard,\n“Learning from naturalistic driving data for human-like autonomous\nhighway driving,” IEEE Transactions on Intelligent Transportation Sys-\ntems, vol. PP, no. 99, pp. 1–14, 2020.\n[6] W. Zhou, Z. Cao, Y. Xu, N. Deng, X. Liu, K. Jiang, and D. Yang, “Long-\ntail prediction uncertainty aware trajectory planning for self-driving\nvehicles,” in 2022 IEEE 25th International Conference on Intelligent\nTransportation Systems (ITSC).\nIEEE, 2022, pp. 1275–1282.\n[7] M. Althoff, O. Stursberg, and M. Buss, “Safety assessment of driving\nbehavior in multi-lane traffic for autonomous vehicles,” in 2009 IEEE\nIntelligent Vehicles Symposium.\nIEEE, 2009, pp. 893–900.\n[8] W. Zhou, Z. Cao, N. Deng, X. Liu, K. Jiang, and D. Yang, “Dynamically\nconservative self-driving planner for long-tail cases,” IEEE Transactions\non Intelligent Transportation Systems, vol. 24, no. 3, pp. 3476–3488,\n2022.\n[9] P. Angkititrakul, C. Miyajima, and K. Takeda, “Impact of driving\ncontext on stochastic driver-behavior model: Quantitative analysis of car\nfollowing task,” in 2012 IEEE International Conference on Vehicular\nElectronics and Safety (ICVES 2012).\nIEEE, 2012, pp. 163–168.\n[10] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement\nlearning for urban autonomous driving,” IEEE, 2019.\n[11] P. Wang and C. Y. Chan, “Formulation of deep reinforcement learning\narchitecture toward autonomous driving for on-ramp merge,” in 2017\nIEEE 20th International Conference on Intelligent Transportation Sys-\ntems (ITSC), 2018.\n[12] A. Harutyunyan, S. Devlin, P. Vrancx, and A. Now´e, “Expressing\narbitrary reward functions as potential-based advice,” in Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 29, no. 1, 2015.\n[13] Y. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell,\nN. Heess, and R. Pascanu, “Distral: Robust multitask reinforcement\nlearning,” Advances in neural information processing systems, vol. 30,\n2017.\n[14] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning\nfor fast adaptation of deep networks,” in International conference on\nmachine learning.\nPMLR, 2017, pp. 1126–1135.\n[15] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,\nL. D. Jackel, M. Monfort, U. Muller, and J. Zhang, “End to end learning\nfor self-driving cars,” 2016.\n[16] B. Piot, M. Geist, and O. Pietquin, “Boosted bellman residual mini-\nmization handling expert demonstrations,” Springer Berlin Heidelberg,\n2014.\n[17] Z. Zhu, K. Lin, and J. Zhou, “Transfer learning in deep reinforcement\nlearning: A survey,” arXiv preprint arXiv:2009.07888, 2020.\n[18] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zam-\nbaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner\net al., “Relational inductive biases, deep learning, and graph networks,”\narXiv preprint arXiv:1806.01261, 2018.\n[19] S. Feng, X. Yan, H. Sun, Y. Feng, and H. X. Liu, “Intelligent driving in-\ntelligence test for autonomous vehicles with naturalistic and adversarial\nenvironment,” Nature communications, vol. 12, no. 1, pp. 1–14, 2021.\n[20] H. Bai, S. Cai, N. Ye, D. Hsu, and W. S. Lee, “Intention-aware online\npomdp planning for autonomous driving in a crowd,” in 2015 ieee\ninternational conference on robotics and automation (icra).\nIEEE,\n2015, pp. 454–460.\n[21] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End\nto end learning for self-driving cars,” arXiv preprint arXiv:1604.07316,\n2016.\n[22] C. Finn and S. Levine, “Deep visual foresight for planning robot\nmotion,” in 2017 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2017, pp. 2786–2793.\n[23] S. Brechtel, T. Gindele, and R. Dillmann, “Probabilistic mdp-behavior\nplanning for cars,” in 2011 14th International IEEE Conference on\nIntelligent Transportation Systems (ITSC). IEEE, 2011, pp. 1537–1542.\n[24] E. Meirom, H. Maron, S. Mannor, and G. Chechik, “Controlling graph\ndynamics with reinforcement learning and graph neural networks,” in\nInternational Conference on Machine Learning.\nPMLR, 2021, pp.\n7565–7577.\n[25] X. Ma, J. Li, M. J. Kochenderfer, D. Isele, and K. Fujimura, “Reinforce-\nment learning for autonomous driving with latent state inference and\nspatial-temporal relationships,” in 2021 IEEE International Conference\non Robotics and Automation (ICRA).\nIEEE, 2021, pp. 6064–6071.\n[26] Z. Zhu and H. Zhao, “A survey of deep rl and il for autonomous driv-\ning policy learning,” IEEE Transactions on Intelligent Transportation\nSystems, 2021.\n[27] K. Hornik, M. Stinchcombe, and H. White, “Universal approximation of\nan unknown mapping and its derivatives using multilayer feedforward\nnetworks,” Neural networks, vol. 3, no. 5, pp. 551–560, 1990.\n[28] N. K. Ahmed, R. A. Rossi, R. Zhou, J. B. Lee, X. Kong, T. L. Willke,\nand H. Eldardiry, “Inductive representation learning in large attributed\ngraphs,” arXiv preprint arXiv:1710.09471, 2017.\n[29] M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio,\nA. Courville, and D. Hjelm, “Mutual information neural estimation,”\nin International conference on machine learning.\nPMLR, 2018, pp.\n531–540.\n[30] E. Leurent, “An environment for autonomous driving decision-making,”\nhttps://github.com/eleurent/highway-env, 2018.\n[31] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in\nempirical observations and microscopic simulations,” Physical review E,\nvol. 62, no. 2, p. 1805, 2000.\n[32] B. Paden, M. ˇC´ap, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of\nmotion planning and control techniques for self-driving urban vehicles,”\nIEEE Transactions on Intelligent Vehicles, vol. 1, no. 1, pp. 33–55, 2016.\n[33] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21134v1.pdf",
    "total_pages": 8,
    "title": "Dynamically Local-Enhancement Planner for Large-Scale Autonomous Driving",
    "authors": [
      "Nanshan Deng",
      "Weitao Zhou",
      "Bo Zhang",
      "Junze Wen",
      "Kun Jiang",
      "Zhong Cao",
      "Diange Yang"
    ],
    "abstract": "Current autonomous vehicles operate primarily within limited regions, but\nthere is increasing demand for broader applications. However, as models scale,\ntheir limited capacity becomes a significant challenge for adapting to novel\nscenarios. It is increasingly difficult to improve models for new situations\nusing a single monolithic model. To address this issue, we introduce the\nconcept of dynamically enhancing a basic driving planner with local driving\ndata, without permanently modifying the planner itself. This approach, termed\nthe Dynamically Local-Enhancement (DLE) Planner, aims to improve the\nscalability of autonomous driving systems without significantly expanding the\nplanner's size. Our approach introduces a position-varying Markov Decision\nProcess formulation coupled with a graph neural network that extracts\nregion-specific driving features from local observation data. The learned\nfeatures describe the local behavior of the surrounding objects, which is then\nleveraged to enhance a basic reinforcement learning-based policy. We evaluated\nour approach in multiple scenarios and compared it with a one-for-all driving\nmodel. The results show that our method outperforms the baseline policy in both\nsafety (collision rate) and average reward, while maintaining a lighter scale.\nThis approach has the potential to benefit large-scale autonomous vehicles\nwithout the need for largely expanding on-device driving models.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}