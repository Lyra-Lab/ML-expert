{
  "id": "arxiv_2502.21318v1",
  "text": "How far can we go with ImageNet for Text-to-Image generation?\nLucas Degeorge * 1 2 3 Arijit Ghosh * 3 Nicolas Dufour 1 3 David Picard † 3 Vicky Kalogeiton † 1\n1 LIX, ´Ecole Polytechnique, CNRS, IP Paris, France\n2 AMIAD, Pole recherche\n3 LIGM, ´Ecole Nationale des Ponts et Chauss´ees, IP Paris, Univ Gustave Eiffel, CNRS, France\n106\n107\n108\n109\n1010\n0.4\n0.5\n0.6\n0.7\nOurs\nPixart-α\nCAD\nSDXL\nSD2.1\nTrain Dataset Size (log. scale)\nGeneval Overall scores\n106\n107\n108\n109\n1010\n0.6\n0.7\n0.8\nOurs\nPixart-α\nSDXL\nSD 1.5\nTrain Dataset Size (log. scale)\nDPG Benchmark Overall scores\nFigure 1. Results of our proposed text-to-image (T2I) generation method when trained solely on ImageNet. Left: Images generated\nwith our 300M T2I model (CAD-I architecture) show good text understanding even for out-of-distribution prompts (e.g.: the pink elephant\nor the neon turtle). Right: Quantitative results on GenEval (top) and DPGBench (bottom). The size of the bubble represents the number of\nparameters. In both cases, we outperform models of 10× the parameters and models trained on 1000× the number of images.\nAbstract\nRecent text-to-image (T2I) generation models\nhave achieved remarkable results by training on\nbillion-scale datasets, following a ‘bigger is bet-\nter’ paradigm that prioritizes data quantity over\nquality. We challenge this established paradigm\nby demonstrating that strategic data augmenta-\ntion of small, well-curated datasets can match\nor outperform models trained on massive web-\nscraped collections. Using only ImageNet en-\nhanced with well-designed text and image aug-\nmentations, we achieve a +2 overall score over\nSD-XL on GenEval and +5 on DPGBench while\nusing just 1/10th the parameters and 1/1000th the\ntraining images. Our results suggest that strategic\ndata augmentation, rather than massive datasets,\ncould offer a more sustainable path forward for\nT2I generation. Code, models and dataset avail-\nable here.\n1. Introduction\nThe prevailing wisdom in text-to-image (T2I) generation\nholds that larger training datasets inevitably lead to bet-\nter performance.\nThis “bigger is better” paradigm has\ndriven the field to billion-scale image-text paired datasets\nlike LAION-5B (Schuhmann et al., 2022), DataComp-\n12.8B (Gadre et al., 2023) or ALIGN-6.6B (Pham et al.,\n2023). While this massive scale is often justified as nec-\nessary to capture the full text-image distribution, in this\nwork, we challenge this assumption and argue that data\nquantity overlooks fundamental questions of data efficiency\nand quality in model training.\nOur critique of the data-scaling paradigm comes from crit-\nical observations.\nWhile the community acknowledges\ndataset quality issues (Birhane et al., 2023) and employs\nextensive curation pipelines (Radenovic et al., 2023; Ab-\nbas et al., 2023), this approach still carries three funda-\nmental flaws: first, the default paradigm still consists of\ncollecting and curating massive web-scraped datasets, thus\n1\narXiv:2502.21318v1  [cs.CV]  28 Feb 2025\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nleading to notable computational cost (Schuhmann et al.,\n2022). Second, current curation processes fail to eliminate\nsocietal biases, inappropriate content, copyrighted material,\nand privacy concerns, which ultimately manifest directly\nin the trained models. This raises serious questions about\nthe underlying distributions learned by these models (Luc-\ncioni et al., 2024; Birhane et al., 2024). Third, the problem\nbecomes particularly crucial for specialized applications,\nwhere creating aligned text-image pairs is prohibitively time\nand resource-intensive.\nThese dataset challenges are particularly concerning given\nthe remarkable progress of T2I models. Recent diffusion-\nbased approaches, popularized by Rombach et al. (2022);\nSaharia et al. (2022); Ramesh et al. (2022), excel in gener-\nating high-fidelity images with photorealistic details, artis-\ntic sophistication, and complex compositional understand-\ning (Chen et al., 2023; Betker et al., 2023; Podell et al.,\n2023). The field has rapidly evolved through several ad-\nvances in architectures like transformed-based DiT (Peebles\n& Xie, 2023) or PixArt-α (Chen et al., 2023) and scaled-up\nmodels like Stable Diffusion (SD) (Podell et al., 2023); yet,\nthese are all coupled with billion-scale datasets for training.\nInstead of tackling core data quality issues, the community’s\nresponse has been: to collect more data. This brute-force\napproach only magnifies the challenges of computational\ncosts, curation complexity, and dataset bias.\nWe propose a radical shift: training text-to-image gener-\nation models with smaller, carefully curated datasets en-\nhanced through strategic data augmentation. We leverage\nImageNet (Russakovsky et al., 2015), a well-known dataset\nwhose biases and limitations are thoroughly studied. While\nImageNet alone has never been used for T2I diffusion mod-\nels due to its simple labels and object-centric nature, we\novercome these limitations through two key contributions:\n(1) applying text augmentations by generating rich, descrip-\ntive captions with LLaVA (Liu et al., 2024b) to capture full\nscene complexity, and (2) applying image augmentations\nusing CutMix (Yun et al., 2019) to create novel concept\ncombinations absent from the original dataset.\nUsing only ImageNet, we train several small diffusion mod-\nels with different architectures (DiT (Peebles & Xie, 2023)\nand RiN (Jabri et al., 2023; Dufour et al., 2024a)). As shown\nin Figure 1, our approach achieves a +2 point improve-\nment on the GenEval benchmark and a +5 point improve-\nment on the DPGBench over Stable Diffusion-XL (Podell\net al., 2023), despite using only 1/10-th the parameters\nand 1/1000-th the training images. This demonstrates\nthat strategic data augmentation can match or exceed the\nperformance of models trained on massive datasets while\nreducing computational costs.\nIn summary, our contributions are:\n• Albeit popular beliefs (Gokaslan et al., 2024), we show\nthat high-quality T2I models can be trained on just 1.2M\naugmented image-text pairs, challenging the necessity of\nbillion-scale datasets.\n• We highlight the importance of simple data diversification\ntechniques both in pixel space (via CutMix augmentation)\nand text space (via rich, long captioning).\n• Our method results in state-of-the-art performances on\nGenEval (Ghosh et al., 2024) and DPGBench (Hu et al.,\n2024) for models with less than 1B parameters, with\norders-of-magnitude reduction in training data (x1000).\n2. Related Work\nDiffusion Models\n(Song et al., 2020b; Ho et al., 2020;\nSohl-Dickstein et al., 2015) have demonstrated remarkable\nsuccess across various domains (Huang et al., 2023; Courant\net al., 2025; Dufour et al., 2024b). While image generation\nremains their most prominent application (Dhariwal &\nNichol, 2021; Song et al., 2020b; Karras et al., 2022),\ntext-to-image (T2I) synthesis (Rombach et al., 2022;\nSaharia et al., 2022; Ramesh et al., 2022) has emerged\nas a particularly impactful use case.\nThese models\noperate by learning to reverse a gradual Gaussian noise\ncorruption process. At extreme noise levels, the model\neffectively samples from a standard normal distribution to\nproduce realistic images. The core optimization objective is:\nmin\nθ\nE(x0,c)∼pdata,ϵ∼N(0,1)\nh\n∥ϵ −ϵθ(xt, c, t)∥2i\n(1)\nwhere xt=\np\nγ(t)x0+\np\n1 −γ(t)ϵ denotes the noised im-\nage at timestep t, x0 the original image, c the corresponding\ncondition (such as text), ϵ is standard normal noise, ϵθ the\nlearned noise predictor, and γ(t) the variance schedule.\nComputational Efficiency\nTraditional diffusion models\nrequire substantial computational resources, with leading\nimplementations consuming hundreds of thousands of GPU\nhours (Rombach et al., 2022). Recent advances have sig-\nnificantly improved training efficiency. (Wei et al., 2023;\nYu et al., 2024) identified limitations in the diffusion loss’s\nrepresentation learning capabilities, demonstrating that sup-\nplementary representation losses accelerate convergence.\n(Chen et al., 2023) achieved dramatic compute reduction\nby repurposing class-conditional models for text-to-image\ngeneration. (Dufour et al., 2024a) introduced architectural\nimprovements and coherence-aware mechanisms, matching\nStable Diffusion’s performance (Rombach et al., 2022) with\n100x fewer GPU hours.\nData Efficiency\nEarly T2I models relied on billion-scale\nweb-scraped datasets (Rombach et al., 2022), creating ac-\ncessibility barriers due to storage requirements and repro-\nducibility challenges from copyright restrictions. (Chen\n2\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\net al., 2023) pioneered dataset reduction using 20M high-\nquality images from recaptioned SAM data (Kirillov et al.,\n2023), though portions remain proprietary. Subsequent\nwork explored CC12M (Changpinyo et al., 2021) and\nYFCC100M’s public subset (Thomee et al., 2016; Gokaslan\net al., 2024), revealing overfitting below 10M samples. Our\napproach diverges by leveraging ImageNet (Russakovsky\net al., 2015) – a reproducible, well-established benchmark\nwith standardized metrics (Heusel et al., 2017). We trans-\nform this classification dataset into T2I training data through\nsynthetic captions and image augmentations.\nCopy-Paste augmentation\nprovides an object-aware data\naugmentation method by extracting source image objects\nand pasting them onto target images (Ghiasi et al., 2021).\nPrior work (Dvornik et al., 2018; Dwibedi et al., 2017;\nFang et al., 2019; Ghiasi et al., 2021) demonstrated its ef-\nfectiveness for instance segmentation and object detection,\nwith some approaches focusing on paste location optimiza-\ntion (Dvornik et al., 2018; Fang et al., 2019; Ge et al., 2024)\nwhile others showing random placement suffices (Ghiasi\net al., 2021). The merging of objects serves as a regular-\nization technique (Yun et al., 2019), which recent methods\nlike UCC (Fan et al., 2022) and NeMo (Ha et al., 2024)\nbuild upon. Learning paradigms leverage contrastive learn-\ning (Wang et al., 2022) and teacher-student networks (Bai\net al., 2023). While existing approaches use StableDiffusion\nto generate paste elements (Zhao et al., 2023; Lin et al.,\n2023; Ge et al., 2023; Rombach et al., 2022), our work in-\nstead leverages mixing training images to train T2I models.\nSynthetic captions\nSynthetic image captioning has ben-\nefited several tasks. For instance, visual question answer-\ning (Sharifzadeh et al., 2024) and visual representation learn-\ning (Tian et al., 2023) achieve state-of-the-art performances\nby enhancing the captioning output of Vision-Language\nModels (VLMs) (Lai et al., 2024; Sharifzadeh et al., 2024).\nSimilarly, training with synthetic captions for text-to-image\ngeneration is becoming the defacto protocol for large diffu-\nsion models, such as DALL-E (Betker et al., 2023), Pixart-\nα (Chen et al., 2023) and Stable Diffusion-3 (Esser et al.,\n2024). More recently, some approaches (Liu et al., 2024a; Li\net al., 2024) extend this approach by training text-to-image\n(T2I) models on multi-level captions. Inspired by these, our\nmethod deploys the popular LLaVA captioner (Liu et al.,\n2024b) to augment existing textual captions and use them\nto train text-to-image generation models.\n3. Method for text-pixel diversification\nWe present a systematic approach to train text-to-image dif-\nfusion models using ImageNet, demonstrating that strategic\ndata augmentation can match the performance of models\ntrained on billion-scale datasets. Our method exploits two\nModel Training\nData Curation\nCutMix\nImageNet\nImageNet CutMix\nLLaVA\nLLaVA\nLong captions\nNormal batch\nLong captions\nCutMix batch\nt\np\nt > τ and p > ρ\nLong captions\nCombined Batch\nDenoising\nModel\nPredicted Noise\nFigure 2. Pipeline of our Data Curation and Training process.\nStarting from ImageNet, we a) use LLaVa VLM to caption the\nimages into long detailed caption (top branch left) and b) use\nseveral CutMix strategies to create new images combining several\nImageNet concepts and caption them using LLaVa into long and\ndetailed captions (bottom branch left). During training, we sample\nbatches of normal and CutMix images and we select from each\nbatch depending on the timestep t at which the CutMix strategy is\nvalid and a probability p of sampling CutMix images.\nkey characteristics of ImageNet: its high curation quality\nand its object-centric nature, where main subjects are con-\nsistently positioned centrally. While ImageNet lacks the\nedge cases and concept combinations found in web-scraped\ndatasets (e.g., abstract art, digital renderings, unusual object\ncombinations), we show this limitation can be systemati-\ncally addressed through structured augmentations.\nOur framework operates along two complementary axes:\n• Text-space augmentation, which converts ImageNet’s\nclass labels into semantically rich scene descriptions (Sec-\ntion 3.1).\n• Pixel-space augmentation, where we introduce novel\nconcept combinations through geometrically-controlled\nimage mixing without compromising image coherence\n(Section 3.2).\nEach component is designed to preserve ImageNet’s inher-\nent quality while addressing specific limitations. Figure 2\nillustrates our complete pipeline. Additionally, Section 3.3\nreports our training procedure with image augmentations.\n3.1. Improvement in text diversity\nImageNet is a class-conditional dataset, initially used for\nclassification and object-detection tasks. To overcome Ima-\ngeNet’s limited class-conditional annotations, we implement\na two-stage text enrichment pipeline:\nBaseline Captioning (AIO). We establish a baseline using\nthe standard “An image of <class-name>” (AIO) format\nfollowing (Radford et al., 2021).\nLimitations. The two main limitations of AIO captions for\nImageNet are: First, AIO captions lack detailed descrip-\n3\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nLLaVA w/o CM\nCM1/2\nCM1/4\nCM1/9\nCM1/16\nA delicate white butterfly\n[...]. The flower, a stunning\nshade of purple, [...].\nThe butterfly, positioned\nslightly to the left of the\nflower’s center, [...].\nOn the left side, a person\nis playing the trumpet on a\nstreet. [...]. On the right\nside of the image, there are\ntwo penguins standing [...].\n[...] a silver sports car\nin the background. [...]\na Golden Retriever, is on\nthe left side of the frame\n[...]. The sports car,\npositioned on the right, [...]\n[...] a palace or manor house,\n[...] In front of the building\nis a well-maintained garden\n[...] In the sky, there is a\nsingle hot air balloon [...].\n[...] a husky dog resting\nin the snow. [...] Next to\nthe dog’s side, there is a\nwine glass with red wine and\na few purple flowers [...]\nFigure 3. Long synthetic captions for (left) original and (right) CutMix pixed-augmented images. All captions, as generated by\nLLaVa, are highly diverse and add intricate details of compositionality, colors as well as concepts, which are not present in the original\nImageNet dataset. For ease of visualization the captions are trimmed; full captions in Appendix B.\ntions, which constrains the diversity in the text-condition\nspace. For example, a caption “an image of golden retriever”\nmentions the class name but leaves out details and concepts.\nSecond, ImageNet lacking any ‘person’ class results in hu-\nmans not being represented in the AIO text space.\nLong captions. We employ LLaVA (Liu et al., 2024b) to\ngenerate comprehensive captions that capture: (i) Scene\ncomposition and spatial relationships; (ii) Background ele-\nments and environmental context; (iii) Secondary objects\nand participants; (iv) Visual attributes (color, size, texture);\nand (v) Actions and interactions between elements.\nThis enhancement addresses critical gaps in ImageNet’s\nannotations, particularly for images containing humans or\nmultiple interacting elements that the original class labels\nmiss entirely. Figure 3 shows examples of the richer cap-\ntions generated by LLaVA.\n3.2. Improvement in pixel space diversity\nFor image augmentation, we introduce a structured CutMix\nframework that systematically combines concepts while\npreserving object centrality. Our framework defines four\nprecise augmentation patterns, each designed to maintain\nvisual coherence while introducing novel concept combina-\ntions. These are briefly described below:\n1. CM1/2 (Half-Mix):\nScale: Both images maintain their original resolution.\nPosition: Deterministic split along height or width.\nCoverage: Each concept occupies 50% of final image.\nPreservation: Both concepts maintain full resolution.\n2. CM1/4 (Quarter-Mix):\nScale: CutMix image resized to 50% side length.\nPosition: Fixed placement at one of four corners.\nCoverage: 2nd concept occupies 25% of final image.\nPreservation: Base image center region remains intact.\n3. CM1/9 (Ninth-Mix):\nScale: CutMix image resized to 33.3% side length.\nPosition: Fixed placement along image borders.\nCoverage: 2nd concept occupies 11.1% of final image.\nPreservation: Base image center, corners remain intact.\n4. CM1/16 (Sixteenth-Mix):\nScale: CutMix image resized to 25% side length.\nPosition: Random placement not central 10% region.\nCoverage: 2nd concept occupies 6.25% of final image.\nPreservation: Base image center region remains intact.\nEach augmentation strategy generates 1,281,167 samples,\nmatching ImageNet’s training set size. Figure 3 shows\nexamples of the different structured augmentations.\nWe also define CMall, which uniformly samples from all\nfour patterns. The CMall variant combines equal propor-\ntions (25%) from each pattern to maintain the same total\nsample count. Post-augmentation, we apply LLaVA caption-\ning to all generated images, ensuring semantic alignment\nbetween visual and textual representations. This produces\ndetailed descriptions that accurately reflect the augmented\ncontent while maintaining natural language fluency.\n3.3. Training with image augmentations\nBecause our image augmentations have strong artifacts cor-\nresponding to the boundaries of the mixing, we have to\nprevent the model from learning those salient features and\nreproducing them. To that end, we propose to train on im-\nage augmentation only at timesteps t where the noisy image\nxt is sufficiently noisy that the artifacts no longer matter.\nIn practice, this corresponds to sampling either from the\noriginal image training set A or from the augmented image\ntraining set AIA conditionally to t, compared to an addi-\ntional hyperparameter τ deciding whether t is sufficiently\nlarge for image augmentation. This extra condition leads to\n4\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nAlgorithm 1 Batch with image augmentation\nInput: dataset A, AIA, augmentation time τ, augmenta-\ntion probability p batch size m,\nB = {}\nfor i = 1 to m do\nt ∼U(0, T)\n(x0, c) ∼A\nif t > τ then\nρ ∼Bp\nif p then\n(x0, c) ∼AIA\nend if\nend if\nϵ ∼N(0, 1)\nxt =\np\nγ(t)x0 +\np\n1 −γ(t)\nB = B ∪(xt, c, t)\nend for\nReturn: B\nreplacing the original diffusion loss in Equation (1) with\nmin\nθ\nE t∼U(0,T ),\nρ∼B(τ,p)(t),\n(x,c)∼A(ρ),\nϵ∼N(0,1)\n\u0002\n∥ϵ −ϵθ(xt, c, t)∥2\u0003\n.\n(2)\nIn this novel loss, the timestep t ∼U(0, T) is still sampled\nuniformly. We introduce a new random variable ρ that is\nsampled conditionally to t, where Bτ,p(t) denotes a specific\ndistribution that corresponds to:\nBτ,p(t) =\n(\n0, if t ≤τ,\nBp, else.\n(3)\nHere, Bp a Bernoulli distribution of parameter p. The text-\nimage pair (x0, c) is then sampled conditionally to ρ, where\nA(ρ) is a distribution that uniformly samples from the orig-\ninal or the augmented datasets depending on ρ:\nA(ρ) =\n(\nA, if ρ = 0,\nAIA, else.\n(4)\nThe noise ϵ is sampled from the Normal distribution, as in\nthe usual diffusion equation. Similarly, the noisy image xt\nis obtained by xt=\np\nγ(t)x0 +\np\n1 −γ(t).\nThis novel loss function is more involved than the regular\ndiffusion training; yet, in practice, it is very easy to im-\nplement and can be done entirely during the mini-batch\nconstruction as described in Algorithm 1.\n4. Experiments\n4.1. Experimental Setup\nDatasets.\nFor training, we use the ImageNet dataset (Rus-\nsakovsky et al., 2015). Each image is rescaled to a reso-\nlution of 256 × 256. We adopt the framework of latent\ndiffusion (Rombach et al., 2022), using a pre-trained vari-\national auto-encoder provided by (Rombach et al., 2022).\nWe encode the text condition using the T5 text encoder\nmodel (Chung et al., 2024). We precompute both the im-\nage latent and the text embeddings for our dataset to make\ntraining efficient.\nWe use two architectures for our experiments: DiT-I (our\nadaptation of DiT (Peebles & Xie, 2023) to handle text) and\nCAD-I (Dufour et al., 2024a). The suffix ”I” is added to\nindicate the model being trained only on ImageNet. Sim-\nilar to prior works, for both the models, we maintain an\nexpected moving average (EMA) model (Song et al., 2020b)\nof the online models. The results described in the following\nsubsections are based on the EMA models. Details about\nthe architecture can be found in Appendix A.\nThe threshold τ that enables image augmentations (Sec-\ntion 3.3) is empirically chosen (see Appendix C). The abla-\ntion of the probability p is discussed in Section 4.3.2. We\nalways train with a batch size of 1024.\nEvaluation\nWe evaluate all our models using 250 steps of\nDDIM (Song et al., 2020a) using the following metrics:\n(1)\nFID\nWe\nuse\nthe\nFr´echet\nInception\nDistance\n(FID) (Heusel et al., 2017) to evaluate the image quality\nw.r.t. two datasets: the 50k in-distribution ImageNet valida-\ntion set and the 30k out-of-distribution MSCOCO captions\nvalidation set (Lin et al., 2014). For FID calculation, in ad-\ndition to the standard Inception-v3 backbone (Szegedy\net al., 2016), we employ the Dinov2 backbone (Oquab\net al., 2024) to assess image quality. This dual approach\nensures a robust evaluation across both in-distribution and\nout-of-distribution datasets.\n(2) P,R,D,C To further evaluate the fidelity and diversity\nof our generated samples, we adopt the combined Preci-\nsion and Recall (Kynk¨a¨anniemi et al., 2019), Density and\nCoverage (Naeem et al., 2020) metrics. All of them were\ncalculated using Dinov2 backbone.\n(3) CLIPScore (CS) We also evaluate the alignment\nof text prompts with the generated images using\nCLIPScore (Hessel et al., 2021). As CLIPScore trun-\ncates the text input sequence at 77 tokens, we also evaluate\nthe models using an updated version of CLIPScore using em-\nbeddings from jina-clip-v2 (Koukounas et al., 2024);\nthis successfully accommodates text-image alignment eval-\nuation on long text prompts.\n5\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nModel\nTA\nIA\nCOCO 30k\nImageNet Val 50k\nFID Inc.↓\nFID DINOv2↓\nP↑\nR↑\nD↑\nC↑\nCS↑\nJina-CS↑\nFID Inc.↓\nFID DINOv2↓\nP↑\nR↑\nD↑\nC↑\nCS↑\nJina-CS↑\n×\n×\n252.22\n2372.27\n0.24\n0.05\n0.15\n0.02\n13.16\n8.64\n23.13\n354.84\n0.66\n0.17\n0.62\n0.41\n25.71\n32.93\n✓\n×\n34.19\n664.20\n0.57\n0.35\n0.46\n0.23\n24.44\n33.94\n8.22\n112.55\n0.75\n0.73\n0.78\n0.68\n9.06\n38.77\nDiT-I\n✓\n✓\n36.46\n656.57\n0.57\n0.36\n0.47\n0.24\n24.85\n34.51\n8.52\n114.54\n0.75\n0.73\n0.78\n0.68\n9.22\n38.75\n×\n×\n46.35\n858.43\n0.52\n0.18\n0.45\n0.15\n12.89\n14.06\n84.77\n904.50\n0.75\n0.05\n1.40\n0.10\n8.94\n20.55\n✓\n×\n46.93\n655.37\n0.66\n0.42\n0.61\n0.28\n26.37\n35.72\n6.16\n91.53\n0.80\n0.72\n0.89\n0.76\n8.69\n38.01\nCAD-I\n✓\n✓\n49.41\n646.51\n0.66\n0.41\n0.57\n0.29\n26.60\n36.51\n6.62\n91.72\n0.80\n0.70\n0.90\n0.76\n8.53\n38.17\nTable 1. Image quality metrics for DiT-I L/2 and CAD-I L models. Models are trained for 250k steps.\nOverall↑\nOne obj.↑\nTwo obj.↑\nCount.↑\nCol.↑\nPos.↑\nCol. attr.↑\nModel\nNb of\nparams\nTraining\nset size\nTA\nIA\n⋄\n⋆\n⋄\n⋆\n⋄\n⋆\n⋄\n⋆\n⋄\n⋆\n⋄\n⋆\n⋄\n⋆\nSD v1.5\n0.9B\n5B+\n×\n×\n0.43\n0.38\n0.97\n0.97\n0.38\n0.25\n0.35\n0.32\n0.76\n0.64\n0.04\n0.04\n0.06\n0.05\nSD v2.1\n0.9B\n5B+\n×\n×\n0.50\n0.47\n0.98\n0.98\n0.51\n0.45\n0.44\n0.46\n0.85\n0.73\n0.07\n0.10\n0.17\n0.10\nPixArt-α\n0.6B\n0.025B\n✓\n×\n0.48\n0.49\n0.98\n0.99\n0.50\n0.53\n0.44\n0.46\n0.80\n0.76\n0.08\n0.09\n0.07\n0.11\nSDXL\n3.5B\n5B+\n×\n×\n0.55\n0.52\n0.98\n0.99\n0.74\n0.60\n0.39\n0.43\n0.85\n0.86\n0.15\n0.11\n0.23\n0.15\nSD3 M\n2B\n1B+\n✓\n×\n0.62\n-\n0.98\n-\n0.74\n-\n0.63\n-\n0.67\n-\n0.34\n-\n0.36\n-\nSD v1.5\n0.9B\n5B+\n×\n×\n0.13\n0.17\n0.48\n0.63\n0.04\n0.04\n0.01\n0.04\n0.23\n0.27\n0.00\n0.01\n0.00\n0.00\nPixArt-α\n0.6B\n0.025B\n✓\n×\n0.48\n0.49\n0.96\n0.99\n0.51\n0.51\n0.48\n0.47\n0.78\n0.76\n0.07\n0.09\n0.08\n0.11\nCAD\n0.4B\n0.020B\n×\n×\n0.50\n0.49\n0.95\n0.92\n0.56\n0.55\n0.40\n0.36\n0.76\n0.72\n0.11\n0.20\n0.22\n0.21\nSD3 M\n2B\n1B+\n✓\n×\n0.49\n0.56\n0.88\n0.95\n0.62\n0.73\n0.28\n0.28\n0.64\n0.75\n0.22\n0.32\n0.31\n0.32\n0.4B\n0.001B\n×\n×\n0.07\n0.00\n0.33\n0.00\n0.00\n0.00\n0.00\n0.00\n0.06\n0.00\n0.00\n0.00\n0.00\n0.00\n0.4B\n0.001B\n✓\n×\n0.25\n0.55\n0.71\n0.95\n0.16\n0.61\n0.17\n0.36\n0.39\n0.80\n0.05\n0.28\n0.04\n0.33\nDiT-I\n0.4B\n0.001B\n✓\n✓\n0.24\n0.57\n0.65\n0.96\n0.10\n0.68\n0.17\n0.36\n0.44\n0.74\n0.03\n0.28\n0.02\n0.39\n0.3B\n0.001B\n×\n×\n0.17\n0.04\n0.82\n0.20\n0.04\n0.01\n0.03\n0.00\n0.13\n0.02\n0.25\n0.00\n0.25\n0.00\n0.3B\n0.001B\n✓\n×\n0.51\n0.55\n0.95\n0.97\n0.56\n0.60\n0.37\n0.42\n0.79\n0.74\n0.14\n0.26\n0.29\n0.35\nCAD-I\n0.3B\n0.001B\n✓\n✓\n0.55\n0.57\n0.94\n0.94\n0.64\n0.68\n0.38\n0.40\n0.77\n0.70\n0.21\n0.35\n0.37\n0.36\nTable 2. Results on GenEval. Models are evaluated at 2562 resolution. ⋄means original GenEval prompts. ⋆means extended GenEval\nprompts. Gray color represents results reported with a native resolution of 5122 or above. Bold indicates best, underline second best.\nModel\nParams\nTraining\nset size\nGlobal↑\nEntity↑\nAttribute↑\nRelation↑\nOther↑\nOverall↑\nSDv1.5\n0.9B\n5B+\n74.63\n74.23\n75.39\n73.49\n67.81\n63.18\nPixart-α\n0.6B\n25M\n74.97\n79.32\n78.60\n82.57\n76.96\n71.11\nCAD\n0.4B\n20M\n84.50\n85.25\n84.66\n91.53\n74.8\n77.55\nSDXL\n3.5B\n5B+\n83.27\n82.43\n80.91\n86.76\n80.41\n74.65\nSD3-Medium\n2B\n1B+\n87.90\n91.01\n88.83\n80.70\n88.68\n84.08\nJanus\n1.3B\n1B+\n82.33\n87.38\n87.70\n85.46\n86.41\n79.68\nDiT-I (Ours)\n0.4B\n1.2M\n76.29\n84.77\n83.34\n92.22\n70.80\n75.99\nCAD-I (Ours)\n0.3B\n1.2M\n80.85\n87.48\n85.32\n93.54\n78.00\n79.94\nTable 3. Results on DPG-Bench. We compare our models to the\nresults reported in (Wu et al., 2024). bold for best, second best.\n(4) GenEval and DPGBench Finally, to evaluate the com-\npositionality of the models, we use the GenEval (Ghosh\net al., 2024) and DPG (Hu et al., 2024) benchmarks. In\naddition to using the default short text prompts provided\nby GenEval, we artificially extend these prompts us-\ning Llama-3.1 to approximate the distribution of long\nprompts used during training.\n4.2. Main results\nHere, we analyze the impact of our augmentation strategies\non T2I generation. We use two models (DiT-I and CAD-\nI) and train them solely on ImageNet using our proposed\naugmentation strategies discussed in 3.1 and 3.2. Below we\nreport the results of our models and compare them against\nthe state of the art.\nQuantitative results: Comparison to the state of the\nart on GenEval and DPG benchmarks\nOne of the main\npurposes of our training augmentation strategy is to improve\nthe diversity of concept combinations in the training set. As\nsuch, we test the composition ability of both our DiT-I and\nCAD-I models on the GenEval and DPGBench benchmarks\nand compare our performances to the ones of popular state-\nof-the-art models.\nGenEval Table 2 reports the results with and without using\nextended prompts (⋆denotes scores with extended prompts\nand ⋄without), noting that our method is tailored to ex-\ntended prompts, given that short ones are out-of-distribution\nwith respect to the training set.\nCompared to SD3, we observe that our models perform\nbetter on average (CAD-I 0.57, DiT-I 0.57) than SD3 (0.56)\nat a resolution of 2562, when evaluated with the extended\nprompt ⋆. Our models also outperform SD1.5 (0.43), SD2.1\n(0.50), SDXL (0.55) and PixArt-α (0.48), despite these\nmodels being evaluated at higher resolution. Resolution is\ncrucial in this benchmark, as shown by the drop in perfor-\nmance of SD3 M when evaluated at 5122 (0.62) compared\nto 2562 (0.49). Even without extended prompts ⋄, our CAD-\nI model successfully reaches the performance of SDXL at\nits full resolution, while having 10x fewer parameters and\n6\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nAIO\nTA\nTA+IA\nFigure 4. Qualitative comparison across models. From left to right:\n‘An image of {class-name}’ AIO, Text-Augmentation (TA), and\nImage-Augmentation with Text-Augmentation (TA + IA). The\nexamples show generated images of (a) a pirate ship sailing on a\nsteaming soup, (b) a hedgehog and an hourglass, and (c) a crab\nsculpted from yellow cheese. While text augmentation improves\nthe model’s understanding, image augmentation leads to better text\ncomprehension and higher image quality overall.\nbeing trained on only 0.1% of the data.\nDPGBench Table 3 reports the results on DPGBench, a\nrecent benchmark similar to Geneval but with a more com-\nplex prompt. We observe similar trends as for GenEval:\ncompared to the current leaderboard, we achieve an overall\naccuracy of 76% with DiT-I, which improves over SDXL\nby 1.3%. We reach an overall score of 79.94% for CAD-I,\noutperforming SDXL by +5% and PixArt-α by +8%. Im-\npressively, our models reach accuracies comparable to that\nof Janus (Wu et al., 2024), a 1.3B parameters VLM with\ngeneration capabilities. Notably, both our models are partic-\nularly good at relations, achieving state-of-the-art of 93.5%\nfor CAD-I and 92.2% for DiT-I.\nQuantitative results: Image Quality\nWe analyze the\nimpact of our augmentations on image quality. We use DiT-\nI and CAD-I and train them on ImageNet with either short\ncaptions “An image of ...”, or the long captions obtained\nfrom Llava. Table 1 reports the results when tested on the\nvalidation set of ImageNet (right part) and COCO (left part).\nFor ImageNet, as a point of reference, we remind the reader\nthat models of this size (below 0.5B parameters) typically\nhave an FID of 9 using the class-conditional setup (Peebles\n100\n200\n300\n400\n17\n18\n19\n20\nTraining Steps in thousands of steps\nFID\nTA\nTA+IA\nFigure 5. Training dynamics showing FID scores vs training steps.\nTA+IA (red) maintains better FID scores throughout training com-\npared to TA only (blue), demonstrating improved resistance to\noverfitting. Lower FID scores indicate better image quality.\n& Xie, 2023). We observe that models trained with the short\nAIO captions fail to achieve this mark (23 for DiT-I and 85\nfor CAD-I), probably because the condition representation\nfrom the text encoder is more ambiguous than a simple class\nlabel. In contrast, our augmentations allow us to reach lower\nFID (8.52 for DiT-I and 6.62 for CAD-I) and much better\nprecision, recall, density, and coverage scores.\nFor COCO, this trend is all the more dramatic, which is a\nzero-shot task. Our augmented models are the only ones\nable to correctly follow the prompt as attested by the much\nimproved CLIP score (DiT-I from 13.16 to 24.85; CAD-I\nfrom 12.89 to 26.60), while keeping similar image quality\n(slightly higher FID, but much lower FID using Dinov2\nbackbone).\nQualitative results\nComparison between our different\nmodel variants are shown in Figure 4. Using the “An image\nof ...” prompt format, the baseline model (AIO) struggles\nto generate coherent images when prompted with concepts\noutside of ImageNet classes. With text augmentation (TA),\nthe model demonstrates improved concept understanding\nand composition abilities, though image quality remains\nlimited. Combining text and image augmentations (TA\n+ IA) leads to enhanced image quality and better prompt\nunderstanding. This improvement is particularly evident\nin the pirate ship scene: while the TA model generates a\nship awkwardly positioned with a bowl of soup, the TA\n+ IA model creates a more natural composition with the\npirate ship appropriately sailing in the bowl. Similarly,\nthe hedgehog and hourglass example shows more refined\ndetails and aesthetically pleasing composition with TA + IA,\nwhereas the TA model struggles to render a recognizable\nhedgehog.\n7\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\n4.3. Ablations\n4.3.1. ABLATION ON CUTMIX SETTINGS\nFirst, we analyze the performances of the pixel augmenta-\ntions for {CM1/2 , CM1/4 , CM1/9 , CM1/16 , CMall }\nsettings. We fix the probability of using a pixel-augmented\nimage in the batch when t > τ to p = 0.5 and we measure\nboth image quality and composition ability. Results are\nreported in Table 4.\nFor image quality, all settings seem to perform similarly,\nwith CM1/2 being the best at 6.13 FID and CMall being the\nworst at 6.81 FID. This indicates that all settings are able\nto avoid producing uncanny images that would disturb the\ntraining too much.\nFor composition ability, CM1/16 is able to improve over\nthe baseline on extended prompts, whereas CMall is able\nto improve over the baseline on original prompts. Overall,\nonly CMall manages to keep closer performances between\nthe original prompts and the extended ones. Since CMall\nis a mixture of all other settings, it also has the most di-\nverse training set and is thus harder to overfit. As such, we\nconsider CMall for the best models.\nGenEval↑\nModel\nCutMix\nSettings\nFID↓\n⋄\n⋆\nCM1/2\n8.74\n0.37\n0.53\nCM1/4\n8.40\n0.29\n0.57\nCM1/9\n8.68\n0.21\n0.53\nCM1/16\n8.31\n0.25\n0.54\nDiT-I L/2\nCMall\n8.41\n0.29\n0.57\nCM1/2\n6.13\n0.46\n0.55\nCM1/4\n6.41\n0.49\n0.53\nCM1/9\n6.63\n0.51\n0.51\nCM1/16\n6.42\n0.47\n0.56\nCAD-I\nCMall\n6.81\n0.53\n0.55\nTable 4. Ablation study on CutMix settings. The probability of\nsampling CutMix images used here is ρ = 0.5. Models are trained\nfor 250k steps. FID is computed on the ImageNet val set with long\nprompts, using the Inception-v3 backbone. ⋄means original\nGenEval prompts. ⋆means extended GenEval prompts.\n4.3.2. ABLATION ON CUTMIX PROBABILITY\nNext, we analyse the influence of the probability p of using\na pixel augmented image in the batch, when the condition\non t is met. Results for p ∈{0.25, 0.5, 0.75, 1.0} are shown\nin Table 5, using CM1/4 pixel augmentations.\nAs we can see in terms of image quality, the FID is slightly\ndegraded by having too frequent pixel augmentation (p >\n0.5). This can be explained by the fact that pixel-augmented\nimages are only seen when t > τ. As such, a high value\nfor p creates a distribution gap between the images seen for\nGenEval↑\nModel\nρ\nFID↓\n⋄\n⋆\n0\n8.22\n0.25\n0.55\n0.25\n8.52\n0.33\n0.58\n0.5\n8.40\n0.29\n0.57\n0.75\n8.73\n0.35\n0.54\nDiT-I L/2\n1\n8.34\n0.29\n0.49\n0\n6.16\n0.51\n0.55\n0.25\n5.99\n0.55\n0.58\n0.5\n6.41\n0.49\n0.53\n0.75\n6.71\n0.45\n0.53\nCAD-I\n1\n6.07\n0.48\n0.49\nTable 5. Ablation study on probability ρ of sampling a CutMix\nimage during training. The CutMix setting is CM1/4. Models are\ntrained for 250k steps. FID is computed on ImageNet val set with\nlong prompts, using the Inception-v3 backbone. ⋄means\noriginal GenEval prompts. ⋆means extended GenEval prompts.\nt > τ and the images seen for t ≤τ.\nComposition ability shows a similar behavior with the\nGenEval overall score decreasing when p increases for both\nthe original and the extended prompts. As such, we consider\np ≤0.5 for the best models.\n4.3.3. DETAILED GENEVAL RESULTS\nTable 2 reports the influence of text and image augmen-\ntations for both models. We observe that models trained\nwithout any augmentation perform very poorly on GenEval.\nText augmentations allow for better prompt understanding,\nespecially when using extended prompts. Pixel space aug-\nmentations greatly improve compositionality-related tasks\nlike Two Objects and Count, leading to higher overall scores.\n4.3.4. MITIGATING OVERFITTING\nFigure 5 shows that the TA+IA models can successfully\nmaintain a lower FID score than the TA models throughout\ntraining. While the FID of TA models starts increasing\nafter 300k steps, indicating overfitting, the FID of TA+IA\nmodels continues to decrease. This demonstrates that image\naugmentation (IA) effectively mitigates overfitting, allowing\nthe model to improve image quality using longer training.\n5. Discussion\nIn this work, we challenged the prevailing wisdom that\nbillion-scale datasets are necessary for high-quality text-to-\nimage generation. Through careful visual and textual aug-\nmentation techniques, we demonstrated that models trained\non just 1.2M image-text pairs can match or exceed the per-\nformance of those trained on thousand-fold larger datasets.\nOur approach, combining rich generated captions with Cut-\nMix augmentations, enables efficient learning of complex\n8\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nvisual concepts while dramatically reducing computational\ncosts and environmental impact.\nThe implications of our work extend beyond just compu-\ntational efficiency. By showing that smaller, well-curated\ndatasets can achieve state-of-the-art results, we open new\npossibilities for specialized domain adaptation where large-\nscale data collection is impractical. Our work also suggests\na path toward more controllable and ethical development\nof text-to-image models, as smaller datasets enable more\nthorough content verification and bias mitigation.\nLooking forward, we believe our results will encourage the\ncommunity to reconsider the “bigger is better” paradigm.\nFuture work could explore additional augmentation strate-\ngies, investigate the theoretical foundations of data effi-\nciency, and develop even more compact architectures opti-\nmized for smaller datasets. Ultimately, we hope this work\nstarts a shift toward more sustainable and responsible devel-\nopment of text-to-image generation models.\n6. Impact Statement\nText-to-image generative models have raised concerns about\ntheir impact on society. A non-exhaustive list of these con-\ncerns includes deep-fakes proliferation, racial and gender\nbiases, copyright infringement, private data stealing through\nweb-scraping, and environmental costs associated with train-\ning very large models on gigantic datasets. In this paper,\nwe demonstrate that text-to-image generative models can be\ntrained on smaller, well-curated datasets and still be compet-\nitive with models trained on web-scale datasets. This opens\nthe door to better control over the data used for training\nsuch models and curating them to tackle bias, copyright or\nprivacy issues. It also allows practitioners to significantly\nreduce the cost of training such models.\nAcknowledgment\nThis work was granted access to the HPC resources of\nIDRIS under the allocation 2025-AD011015436 and 2025-\nAD011015594 made by GENCI, and by the SHARP ANR\nproject ANR-23-PEIA-0008 funded in the context of the\nFrance 2030 program. The authors would like to thank\nThibaut Loiseau, Yannis Siglidis, Yohann Perron, Louis\nGeist, Robin Courant and Sinisa Stekovic for their insight-\nful comments, suggestions, and discussions.\nReferences\nAbbas, A., Tirumala, K., Simig, D., Ganguli, S., and Mor-\ncos, A. S. Semdedup: Data-efficient learning at web-scale\nthrough semantic deduplication. ICLR, 2023.\nBai, Y., Chen, D., Li, Q., Shen, W., and Wang, Y. Bidi-\nrectional copy-paste for semi-supervised medical image\nsegmentation. In CVPR, 2023.\nBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,\nOuyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving\nimage generation with better captions. OpenAI, 2023.\nBirhane, A., Prabhu, V., Han, S., Boddeti, V. N., and Luc-\ncioni, A. S. Into the laions den: Investigating hate in\nmultimodal datasets. NeurIPS, 2023.\nBirhane, A., Han, S., Boddeti, V., Luccioni, S., et al. Into\nthe laion’s den: Investigating hate in multimodal datasets.\nNeurIPS, 2024.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\nceptual 12M: Pushing web-scale image-text pre-training\nto recognize long-tail visual concepts. In CVPR, 2021.\nChen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang,\nZ., Kwok, J., Luo, P., Lu, H., et al. Pixart-alpha: Fast\ntraining of diffusion transformer for photorealistic text-\nto-image synthesis. ICLR, 2023.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S.,\net al. Scaling instruction-finetuned language models. J.\nMachine Learning Research, 2024.\nCourant, R., Dufour, N., Wang, X., Christie, M., and Kalo-\ngeiton, V. Et the exceptional trajectories: Text-to-camera-\ntrajectory generation with character awareness. In Euro-\npean Conference on Computer Vision, 2025.\nDhariwal, P. and Nichol, A. Diffusion models beat gans on\nimage synthesis. NeurIPS, 2021.\nDufour, N., Besnier, V., Kalogeiton, V., and Picard, D. Don’t\ndrop your samples! coherence-aware training benefits\nconditional diffusion. In CVPR, 2024a.\nDufour, N., Picard, D., Kalogeiton, V., and Landrieu, L.\nAround the world in 80 timesteps: A generative approach\nto global visual geolocation. arXiv, 2024b.\nDvornik, N., Mairal, J., and Schmid, C. Modeling visual\ncontext is key to augmenting object detection datasets. In\nECCV, 2018.\nDwibedi, D., Misra, I., and Hebert, M. Cut, paste and learn:\nSurprisingly easy synthesis for instance detection. In\nICCV, 2017.\nEsser, P., Kulal, S., Blattmann, A., Entezari, R., M¨uller, J.,\nSaini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.\nScaling rectified flow transformers for high-resolution\nimage synthesis. In Proc. ICML, 2024.\n9\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nFan, J., Gao, B., Jin, H., and Jiang, L. Ucc: Uncertainty\nguided cross-head co-training for semi-supervised seman-\ntic segmentation. In CVPR, 2022.\nFang, H.-S., Sun, J., Wang, R., Gou, M., Li, Y.-L., and\nLu, C. Instaboost: Boosting instance segmentation via\nprobability map guided copy-pasting. In ICCV, 2019.\nGadre, S. Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis,\nG., Nguyen, T., Marten, R., Wortsman, M., Ghosh, D.,\nZhang, J., Orgad, E., Entezari, R., Daras, G., Pratt, S.,\nRamanujan, V., Bitton, Y., Marathe, K., Mussmann, S.,\nVencu, R., Cherti, M., Krishna, R., Koh, P. W., Saukh, O.,\nRatner, A., Song, S., Hajishirzi, H., Farhadi, A., Beau-\nmont, R., Oh, S., Dimakis, A., Jitsev, J., Carmon, Y.,\nShankar, V., and Schmidt, L. Datacomp: In search of the\nnext generation of multimodal datasets. NeurIPS, 2023.\nGe, Y., Xu, J., Zhao, B. N., Joshi, N., Itti, L., and Vineet, V.\nBeyond generation: Harnessing text to image models for\nobject detection and segmentation. arXiv, 2023.\nGe, Y., Yu, H.-X., Zhao, C., Guo, Y., Huang, X., Ren, L.,\nItti, L., and Wu, J. 3d copy-paste: Physically plausible\nobject insertion for monocular 3d detection. NeurIPS,\n2024.\nGhiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk,\nE. D., Le, Q. V., and Zoph, B. Simple copy-paste is a\nstrong data augmentation method for instance segmenta-\ntion. In CVPR, 2021.\nGhosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An\nobject-focused framework for evaluating text-to-image\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\nGokaslan, A., Cooper, A. F., Collins, J., Seguin, L., Ja-\ncobson, A., Patel, M., Frankle, J., Stephenson, C., and\nKuleshov, V. Commoncanvas: Open diffusion models\ntrained on creative-commons images. In CVPR, 2024.\nHa, S., Kim, C., Kim, D., Lee, J., Lee, S., and Lee, J.\nFinding nemo: Negative-mined mosaic augmentation for\nreferring image segmentation. ECCV, 2024.\nHenry, A., Dachapally, P. R., Pawar, S., and Chen, Y. Query-\nkey normalization for transformers. Proc. EMNLP, 2020.\nHessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,\nY. Clipscore: A reference-free evaluation metric for im-\nage captioning. arXiv, 2021.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. NeurIPS, 2017.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. NeurIPS, 2020.\nHu, X., Wang, R., Fang, Y., Fu, B., Cheng, P., and Yu,\nG. Ella: Equip diffusion models with llm for enhanced\nsemantic alignment. arxiv, 2024.\nHuang, Q., Park, D. S., Wang, T., Denk, T. I., Ly, A.,\nChen, N., Zhang, Z., Zhang, Z., Yu, J., Frank, C., et al.\nNoise2music: Text-conditioned music generation with\ndiffusion models. arXiv, 2023.\nJabri, A., Fleet, D., and Chen, T. Scalable adaptive compu-\ntation for iterative generation. Proc. ICML, 2023.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models.\nNeurIPS, 2022.\nKingma, D. P. Auto-encoding variational bayes. ICLR,\n2014.\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,\nGustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo,\nW.-Y., et al. Segment anything. In ICCV, 2023.\nKoukounas, A., Mastrapas, G., Wang, B., Akram, M. K.,\nEslami, S., G¨unther, M., Mohr, I., Sturua, S., Martens, S.,\nWang, N., et al. jina-clip-v2: Multilingual multimodal\nembeddings for text and images. arXiv, 2024.\nKynk¨a¨anniemi, T., Karras, T., Laine, S., Lehtinen, J., and\nAila, T. Improved precision and recall metric for assess-\ning generative models. NeurIPS, 2019.\nLai, Z., Saveris, V., Chen, C., Chen, H.-Y., Zhang, H.,\nZhang, B., Tebar, J. L., Hu, W., Gan, Z., Grasch, P.,\nCao, M., and Yang, Y. Revisit large-scale image-caption\ndata in pre-training multimodal foundation models. arxiv,\n2024.\nLi, X., Tu, H., Hui, M., Wang, Z., Zhao, B., Xiao, J., Ren, S.,\nMei, J., Liu, Q., Zheng, H., Zhou, Y., and Xie, C. What if\nwe recaption billions of web images with llama-3? arxiv,\n2024.\nLin, S., Wang, K., Zeng, X., and Zhao, R. Explore the\npower of synthetic data on few-shot object detection. In\nICCV, 2023.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft\ncoco: Common objects in context. In ECCV, 2014.\nLiu, B., Akhgari, E., Visheratin, A., Kamko, A., Xu, L.,\nShrirao, S., Lambert, C., Souza, J., Doshi, S., and Li, D.\nPlayground v3: Improving text-to-image alignment with\ndeep-fusion large language models. arxiv, 2024a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. NeurIPS, 2024b.\n10\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nLuccioni, S., Akiki, C., Mitchell, M., and Jernite, Y. Sta-\nble bias: Evaluating societal representations in diffusion\nmodels. NeurIPS, 2024.\nNaeem, M. F., Oh, S. J., Uh, Y., Choi, Y., and Yoo, J. Reli-\nable fidelity and diversity metrics for generative models.\nIn Proc. ICML, 2020.\nOquab, M., Darcet, T., Moutakanni, T., Vo, H. V.,\nSzafraniec, M., Khalidov, V., Fernandez, P., HAZIZA,\nD., Massa, F., El-Nouby, A., Assran, M., Ballas, N.,\nGaluba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra,\nI., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou,\nH., Mairal, J., Labatut, P., Joulin, A., and Bojanowski,\nP. DINOv2: Learning robust visual features without su-\npervision. Transactions on Machine Learning Research\n(TMLR), 2024.\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In ICCV, 2023.\nPham, H., Dai, Z., Ghiasi, G., Kawaguchi, K., Liu, H., Yu,\nA. W., Yu, J., Chen, Y.-T., Luong, M.-T., Wu, Y., Tan, M.,\nand Le, Q. V. Combined scaling for zero-shot transfer\nlearning. arXiv, 2023.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M¨uller, J., Penna, J., and Rombach, R. Sdxl: Im-\nproving latent diffusion models for high-resolution image\nsynthesis. arXiv, 2023.\nRadenovic, F., Dubey, A., Kadian, A., Mihaylov, T., Van-\ndenhende, S., Patel, Y., Wen, Y., Ramanathan, V., and\nMahajan, D. Filtering, distillation, and hard negatives for\nvision-language pre-training. CVPR, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In Proc. ICML, 2021.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv, 2022.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In CVPR, 2022.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale\nVisual Recognition Challenge. IJCV, 2015.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image diffu-\nsion models with deep language understanding. NeurIPS,\n2022.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.\nNeurIPS, 2022.\nSharifzadeh, S., Kaplanis, C., Pathak, S., Kumaran, D., Ilic,\nA., Mitrovic, J., Blundell, C., and Banino, A. Synth2:\nBoosting visual-language models with synthetic captions\nand image embeddings. arxiv, 2024.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. Proc. ICML, 2015.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. ICLR, 2020a.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. ICLR, 2020b.\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In CVPR, 2016.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM, 2016.\nTian, Y., Fan, L., Chen, K., Katabi, D., Krishnan, D., and\nIsola, P. Learning vision from models rivals learning\nvision from data. CVPR, 2023.\nTouvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and\nJ´egou, H. Going deeper with image transformers. In\nICCV, 2021.\nVan Den Oord, A., Vinyals, O., et al. Neural discrete repre-\nsentation learning. NeurIPS, 2017.\nWang, F., Wang, H., Wei, C., Yuille, A., and Shen, W.\nCp 2: Copy-paste contrastive pretraining for semantic\nsegmentation. In ECCV, 2022.\nWei, C., Mangalam, K., Huang, P.-Y., Li, Y., Fan, H., Xu,\nH., Wang, H., Xie, C., Yuille, A., and Feichtenhofer,\nC. Diffusion models as masked autoencoders. In ICCV,\n2023.\nWu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu,\nW., Xie, Z., Yu, X., Ruan, C., et al. Janus: Decoupling\nvisual encoding for unified multimodal understanding and\ngeneration. arXiv, 2024.\nYu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J.,\nand Xie, S. Representation alignment for generation:\nTraining diffusion transformers is easier than you think.\narXiv, 2024.\n11\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nYun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y.\nCutmix: Regularization strategy to train strong classifiers\nwith localizable features. In ICCV, 2019.\nZhao, H., Sheng, D., Bao, J., Chen, D., Chen, D., Wen,\nF., Yuan, L., Liu, C., Zhou, W., Chu, Q., Zhang, W.,\nand Yu, N. X-paste: Revisiting scalable copy-paste for\ninstance segmentation using clip and stablediffusion. In\nProc. ICML, 2023.\n12\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nA. Implementation details\nLayerNorm\nMulti-head\nSelf-Attention\nLayerNorm\nMulti-head\nCross-Attention\n+\nLayerNorm\nPointwise\nFeedforward\n+\n+\nTime\nZ-Norm\nText\nTime\nText\nCondition Block-1\nCondition Block-2\nLayerNorm\nMulti-head Self-Attention\nLayerNorm\nPointwise Feedforward\n+\n+\nCondition Block\nCross Attention\nFrom Condition-\ning to Latents\nCross Attention\nFrom Pixels\nto Latents\nSelf Attention\nxN\nSelf Attention\nx2\nshared across RIN-I blocks\nCross Atten-\ntion From La-\ntents to Pixels\nLatents\nTime\nFLAN T5 XL\nEmbeddings\nLatents\nFigure 6. Fundamental architecture blocks used in our experiments. Left: DiT-I block and Right: CAD-I block.\nIn this work, we use both DiT (Peebles & Xie, 2023) and RIN (Jabri et al., 2023) architectures. To adapt DiT for text-\nconditional setting, we replace AdaLN-Zero conditioning with cross-attention to input the text condition into the model,\nas in (Chen et al., 2023). Before feeding the text condition to the model, we refine it using two self-attention layers. Similar\nto (Esser et al., 2024), we add QK-Normalization (Henry et al., 2020) in each of the self-attention and cross-attention blocks\nto mitigate sudden growths of attention entropy and reduce training loss instability. We also add LayerScale (Touvron et al.,\n2021) to each of the residual blocks of DiT for further stability. Figure 6 details our DiT-I architecture.\nTo adapt the RIN (Jabri et al., 2023) for the text-conditional setting, we used the off-the-shelf architecture from (Dufour\net al., 2024a), an adaptation of the RIN architecture detailed in the Appendix of (Dufour et al., 2024a). Figure 6 details our\nCAD-I architecture.\nWe use the framework of latent diffusion (Rombach et al., 2022). For encoding the images into the latent space, we\nuse the pre-trained variational autoencoder (Kingma, 2014; Van Den Oord et al., 2017) provided by the authors of\nStable Diffusion (Rombach et al., 2022). The checkpoint used is available on HuggingFace: https://huggingface.\nco/stabilityai/sd-vae-ft-ema. For text conditions, we encode the captions using the T5 text encoder. The\ncheckpoint is available on HuggingFace: https://huggingface.co/google/flan-t5-xl.\nB. Captioning details\nCaptioning efficiently with LLaVA\nTo caption images, we use the checkpoint llama3-llava-next-8b-hf (avail-\nable on HuggingFace: https://huggingface.co/llava-hf/llama3-llava-next-8b-hf) with the prompt\n”Describe this image”. LLaVA encodes images using a dynamic resolution scheme. It processes both the entire image and\nfour distinct patches as unique images and concatenates them. For 256x256 images, LLaVA uses around 2500 image tokens.\nTo make the captioning process more efficient, we prune the image tokens, retaining only the tokens of the entire image and\ndiscarding patch-specific tokens. This optimization increased inference speed by a factor of 2.7, without compromising\nperformances. Examples of long captions generated by LLaVA are given in Figure 8.\nCaptioning CutMix images\nWe caption CutMix images from CM1/2 with similar settings used for captioning the original\nImageNet images. However, to ensure that LLaVA does not describe both the base and the CutMix images independently,\nwe use a different prompt: “Describe this image. Consider all the objects in the picture. Describe them, describe their\nposition and their relation. Do not consider the image as a composite of images. The image is a single scene image”.\nFor settings CM1/4, CM1/9 and CM1/16, LLaVA tends to either ignore the smaller CutMix image or describe the image as a\ncomposite of two images. To avoid this behaviour, we encode the image by using the entire image patch and add tokens\n13\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nfrom the patch to which the CutMix image belongs. We use the following prompt: “Describe this image. Consider all the\nobjects in the picture. Describe them, describe their position and their relation. Do not consider the image as a composite\nof images. The image is a single scene image”. Examples of long captions generated by LLaVA for CutMix images are\ngiven in Figure 8.\nC. Threshold τ for image augmentation (IA)\nThe threshold τ that enables image augmentations (Section 3.3) is empirically chosen. Figure 7 shows the noise level for\neach timestep for the four image augmentation settings. We found that above timestep 400, the contents of the images\nare mostly lost. As a result, the timestep threshold, τ, is set to 400 for our experiments. This helps in not mitigating the\nsuperimposition artifact to the model as it never sees such a distribution of images and only sees the noisy counterparts.\n0\nIncreasing Timesteps\n100\n1000\nFigure 7. Noise levels across for the four image augmentation settings\n14\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nSetting\nLong Caption\nLLaVA w/o CM\nIn the heart of the verdant background, a delicate white butterfly with a hint\nof yellow around its edges is captured in mid-flight.\nIts wings, a blend of\ntwo shades of cream and white, add a soft contrast to the vibrant colors of the\nflower it’s hovering over.\nThe flower, a stunning shade of purple, is adorned\nwith a yellow center that speaks of the sun’s rays it might have been basking in.\nSurrounding it is a lush green foliage, adding depth to this tranquil scene.\nThe\nbutterfly, positioned slightly to the left of the flower’s center, seems to be in\nthe process of alighting or perhaps taking off, adding a dynamic element to this\notherwise serene tableau.\nCM1/2\nOn the left side, there is a person playing the trumpet on a street.\nThe\nindividual is standing on the ground, wearing a long-sleeved shirt and jeans.\nOn the ground next to the player, there is a trash can.\nThe street is made\nof asphalt, with buildings in the background and a clear sky overhead.\nOn the\nright side of the image, there are two penguins standing on a gravel surface,\npossibly a road or a rocky beach.\nThe penguins are frozen in place, displaying\na contrast to the dynamic scene on the left side.\nThe penguins’ yellow beaks,\nblack bodies, and white feathers are clearly visible.\nThe image captures an\neveryday moment of music played on the street, juxtaposed with the surreal and\nunexpected presence of the penguins.\nThe image does not provide information\nabout the time of day, specific location, or any actions being performed by the\npenguins or the person.\nThe image is static, presenting two moments captured in\ntime rather than a sequence of events.\nCM1/4\nThe image shows a large, muscular dog in the foreground and a silver sports car\nin the background.\nThe dog, appearing to be a Golden Retriever, is on the left\nside of the frame, occupying a significant portion of the space.\nIt has a yellow\nhue with darker markings, and its ears are floppy.\nThe dog is heavily coated\nwith snowflakes, suggesting that the photo was taken in a snowy environment.\nThe\nsports car, positioned on the right, is a two-door coupe with sleek curves and a\nnotable design, featuring the Mercedes-Benz logo on its front grille.\nThe car\nhas a silver finish, and the photo captures it from a perspective that shows\nthe front and side profile.\nThe car is parked on an asphalt surface, possibly\na parking lot or a driveway.\nThe dog is facing the camera with direct gaze,\nwhile the car is positioned slightly towards the side, away from the viewer’s\nperspective.\nCM1/9\nThe image depicts a picturesque outdoor scene featuring an ornate building,\nwhich appears to be a palace or manor house, with classical architectural\nelements including symmetrical windows, a central cupola, and multiple chimneys.\nIn front of the building is a well-maintained garden with pathways and neatly\ntrimmed hedges or borders.\nAbove the garden, there is a clear blue sky with\na few scattered clouds.\nIn the sky, there is a single hot air balloon with a\nbright orange and yellow pattern.\nThe balloon is floating at a considerable\nheight above the garden and the building, suggesting it might be part of a\nleisure activity or a special event.\nThe image is a photograph with natural\nlighting, indicative of a sunny day.\nCM1/16\nThe image is a photograph featuring a husky dog resting in the snow.\nThe dog\nhas a light coat with darker markings around its face and ears, and it is lying\non its side with its head up, looking directly at the camera.\nIts eyes are open\nand its mouth is slightly open, showing teeth and a pink tongue, which suggests\nthe dog might be panting or in a relaxed state.\nNext to the dog’s side, there\nis a wine glass with red wine and a few purple flowers, which could be lilacs,\npositioned on the left side of the glass stem.\nThe wine glass and flowers are\nset against a blurred background that gives the impression of greenery.\nFigure 8. Long captions generated by our synthetic LLaVA captioner. The captions generated are highly diverse and add in much more\nintricate details of compositionality, colors as well as concepts which are not present in the original ImageNet dataset. The captions\ngenerated for our augmented images are also highly coherent and explain the scene in a much more realistic way.\n15\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nD. Additional Qualitative Results\nD.1. Based on ImageNet validation set captions\nDiT-I\nCAD-I\n16\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nD.2. Based on DPG bench captions\nDiT-I\nCAD-I\n17\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nE. Teaser prompts\n1. Hummingbird with Metallic Flower\nThis is a captivating image of a hummingbird in mid-flight, its wings a blur of motion as it approaches\na uniquely crafted flower. The flower is not a natural bloom but an artistic creation of polished metal and\nvibrant rubies, its petals catching the light and reflecting a spectrum of colors. The hummingbird, with its\nlong, slender beak, is in the process of feeding from the flower, its delicate form contrasting with the solidity\nof the metal and stone. The background of the image is a picturesque beach scene, with the ocean stretching\nout to the horizon and the sandy shore providing a soft, natural counterpoint to the artificial beauty of the\nflower. The image conveys a sense of wonder and the unexpected beauty that can arise from the juxtaposition\nof nature and art.\n2. Dancing Turtle in Neonwave\nThe image features a turtle engaged in a dance within a vibrant neonwave environment. The turtle is\ndepicted standing upright, its posture suggesting movement and rhythm. The surrounding environment is\ncharacterized by bright, glowing neon colors, creating a dynamic and energetic atmosphere. The turtle’s shell\nand skin may reflect some of the neon light, adding to the visual spectacle. The overall scene evokes a sense\nof fun and celebration, with the turtle’s dance and the neon wave backdrop combining to create a unique and\ncaptivating image. The lighting is likely dynamic and colorful, enhancing the neonwave aesthetic.\n3. Pink Elephant on Beach\nThis is a whimsical image of a pink elephant enjoying a day at the beach. The elephant, a large and\nmajestic creature, is colored bright, bubblegum pink, making it stand out against the natural surroundings. It\nis walking along the shoreline, its large feet leaving prints in the wet sand. The beach is a typical tropical\nparadise, with fine white sand and clear turquoise water. The sun is shining brightly, casting a warm glow\nover the scene. The image evokes a sense of fun and lightheartedness, suggesting a playful and carefree\natmosphere. The contrast between the elephant’s unnatural pink color and the natural beauty of the beach\ncreates a surreal and captivating image.\n4. Metallic Giant Ant\nThe image features a colossal ant, constructed entirely of polished metal, standing amidst a desolate\nlandscape. The ant’s metallic exoskeleton gleams under the harsh light, reflecting the barren surroundings.\nIts segmented body is intricately detailed, showcasing the rivets and joints that hold its metallic form together.\nThe ant’s powerful legs, also crafted from metal, are firmly planted on the cracked earth, suggesting a sense\nof strength and stability. Its antennae, long and delicate, reach out into the empty air, perhaps sensing the\nenvironment. The background is a vast, empty plain, stretching out to the horizon under a pale, cloud-strewn\nsky. The overall image evokes a sense of science fiction or fantasy, where giant metallic creatures roam\ndesolate worlds.\n5. Two Corgis Portrait\nThe image depicts two corgi dogs arranged side-by-side against a backdrop that features a warm color\npalette with subtle floral designs. Both dogs appear to be of the Pembroke Welsh Corgi breed, characterized\nby their short legs and long bodies. They have a predominantly white coat with brown markings. The dog\non the left is looking directly at the camera, while the dog on the right is not looking towards the camera,\ninstead gazing slightly to the right. The dog on the right has its mouth slightly open, as if panting or cooling\ndown, and its tongue is partially visible. Both dogs have a calm demeanor, and there is no visible text on the\nimage. The style of the photograph seems to be a professional studio shot, likely taken for the purpose of\nshowcasing the dogs.\n6. Chimpanzee with Windmill\nThe image captures a playful moment with a chimpanzee interacting with a brightly colored paper\nwindmill. The chimpanzee, with its expressive face and dexterous hands, is gently blowing on the windmill,\ncausing its four blades to spin. The windmill is crafted from vibrant paper, featuring a variety of colors that\n18\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\ncontrast nicely with the chimpanzee’s dark fur. The background is a soft, neutral tone, allowing the focus to\nremain on the chimpanzee and the windmill. The scene evokes a sense of childlike wonder and innocent\namusement, highlighting the chimpanzee’s intelligence and curiosity. The lighting is natural, illuminating the\nscene and adding depth to the image.\n7. Hedgehog eating a mushroom\nThe image features a small hedgehog in a forest setting, focused on consuming a mushroom. The\nhedgehog, with its spiky brown and white coat, is positioned with its head down, its tiny mouth engaged\nwith the mushroom. The mushroom itself is a classic toadstool shape, with a red cap speckled with white\ndots. The forest floor around the hedgehog is covered in fallen leaves and other natural debris, suggesting\nan autumnal setting. The lighting is soft and diffuse, highlighting the textures of the hedgehog’s quills and\nthe mushroom’s cap. The overall scene conveys a sense of quiet woodland life and the hedgehog’s natural\nforaging behavior.\n8. Steampunk Robot in Forest\nPhotography closeup portrait of an adorable rusty broken-down steampunk robot covered in budding\nvegetation, surrounded by tall grass, misty futuristic sci-fi forest environment.\n9. Tropical Floral Arrangement\nThis image captures a close-up view of a tropical floral arrangement, emphasizing the intricate details of\nthe individual flowers and leaves. The arrangement features a mix of vibrant tropical blooms, including a fiery\norange bird of paradise flowers with their distinctive beak-like shape, delicate pink and white ginger flowers\nwith their spiralling petals, and clusters of tiny, fragrant jasmine blossoms. The flowers are interspersed\nwith lush tropical foliage, such as broad, heart-shaped anthurium leaves and slender, cascading orchids.\nThe lighting is soft and directional, highlighting the textures and colors of the flowers and leaves. The\nbackground is a blurred, out-of-focus representation of a tropical setting, perhaps a beach or a lush garden,\nfurther enhancing the sense of place. The overall image conveys a feeling of warmth, abundance, and the\nexotic beauty of the tropics.\n10. Otter with Rubber Duck\nThis image features an otter and a yellow rubber duck engaged in a playful encounter. The otter, its fur a\nrich, dark brown, is seen from a slightly elevated angle, partially submerged in water. Its paws are actively\nengaged with the rubber duck, a bright yellow object floating on the water’s surface. The duck, a classic\nrepresentation of a bath toy, has its typical orange beak and black dot eyes. The otter’s attention is fully\nfocused on the duck, suggesting a moment of playful exploration. The water around them is calm, with\ngentle ripples created by the otter’s movements. The background is a slightly blurred depiction of a natural\nenvironment, possibly a rocky shoreline or a pond. The lighting is soft and diffused, emphasizing the textures\nof the otter’s fur and the smooth surface of the rubber duck. The image captures a moment of lighthearted\ninteraction, conveying a sense of the otter’s natural playfulness.\n11. Scuba Diver at Coral Reef\nThe image features a scuba diver exploring a vibrant coral reef. The diver, clad in full scuba gear including\na black wetsuit, a bright yellow oxygen tank, and a diving mask, is positioned near a large, fan-shaped coral\nformation. The coral is a mix of colors, including deep reds, vibrant oranges, and soft yellows, creating a\nvisually stunning underwater landscape. The diver’s fins are visible, suggesting gentle movement through the\nwater. Sunlight filters down from the surface, illuminating the scene and highlighting the intricate details\nof the coral polyps. Small, brightly colored fish dart in and out of the coral branches, adding to the lively\natmosphere of the reef. The water is clear and blue, offering excellent visibility of the underwater world. The\noverall scene evokes a sense of wonder and tranquillity, showcasing the beauty of marine ecosystems.\n12. Warhol-Style Parrot\n19\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nThe image depicts a vibrant parrot, rendered in the iconic style of Andy Warhol. The parrot’s plumage is a\nkaleidoscope of bold, contrasting colors, reminiscent of Warhol’s screen printing technique. The background\nis a flat, solid color, perhaps a bright pink or electric blue, further emphasizing the subject. The parrot’s\npose is simple and direct, possibly perched on a branch or presented against the stark backdrop. The overall\ncomposition evokes a sense of pop art, with the repetition of color and form characteristic of Warhol’s work.\nThe image captures the essence of Warhol’s fascination with celebrity and mass production, applying it to the\nnatural beauty of a parrot.\n13. Dewy Cobweb\nThe image showcases a delicate cobweb, glistening with morning dew. The intricate structure of the\nweb is highlighted by the tiny droplets, each one catching and reflecting the soft morning light. The dew\nclings to the fine strands, outlining the geometric pattern of the web against a blurred background of early\nmorning foliage. The overall effect is ethereal and fragile, capturing a fleeting moment of natural beauty. The\nlight catches the water droplets, creating tiny sparkling jewels along the silken threads. The scene evokes a\nsense of peace and tranquillity, characteristic of a quiet morning in nature. The focus is sharp on the web,\nemphasizing its delicate construction and the ephemeral nature of the dew.\n14. Astronaut in Jungle\nThe image features an astronaut standing amidst a dense, vibrant jungle. The astronaut is clad in a sleek,\nwhite spacesuit, complete with a helmet that reflects the dappled sunlight filtering through the thick canopy.\nThe suit shows subtle signs of wear and tear, perhaps hinting at a long journey or unexpected landing. The\nvisor of the helmet partially obscures the astronaut’s face, but a hint of curiosity can be discerned. The jungle\nenvironment is lush, with towering trees draped in vines, and exotic flowers blooming in vibrant colors. The\nground is covered in a thick carpet of moss and ferns. The scene evokes a sense of wonder and exploration,\njuxtaposing the advanced technology of the spacesuit with the raw, untamed beauty of the jungle.\n15. Mexican Tacos\nThe image captures a close-up view of several delicious Mexican tacos, arranged on a traditional ceramic\nplate. The tacos are filled with a generous portion of seasoned ground beef, topped with shredded lettuce,\ndiced tomatoes, crumbled cheese, and a dollop of sour cream. The ground beef is cooked thoroughly, with a\nrich brown color and visible bits of seasoning. The lettuce adds a crisp green element to the tacos, while\nthe diced tomatoes provide a juicy burst of red. The crumbled cheese adds a creamy texture and a touch of\nwhite against the other ingredients. The dollop of sour cream adds a smooth, tangy finish to the tacos. The\ntraditional ceramic plate, with its colorful patterns and designs, complements the vibrant colors of the tacos.\nThe background is slightly blurred, focusing attention on the details of the tacos. The lighting is bright and\neven, showcasing the textures and colors of the food. The overall image conveys the appetizing and authentic\nnature of Mexican street food.\nF. Qualitative results prompts\nHere we show the prompts used to make the Figure 4. Note that for AIO we use the short version of the prompt as it is\ncloser to its train distribution:\n1. A pirate ship sailing on a streaming soup\nThe image showcases a colossal, exquisitely crafted pirate ship, its presence commanding and larger-\nthan-life, as it sails triumphantly across a boundless sea of steaming soup. The ship’s hull, made of dark,\npolished wood, is adorned with intricate carvings of dragons and waves, while its three towering masts\nsupport vast, billowing sails that glow faintly in the warm, golden light radiating from the broth. The soup is\na vibrant, aromatic masterpiece, with swirls of rich broth, floating islands of noodles, and vibrant vegetables\nlike carrots, bok choy, and mushrooms creating a textured, immersive landscape. The ship’s deck is alive with\ndetail—ropes coiled neatly, barrels stacked high, and a crow’s nest peeking above the sails, all slightly damp\nfrom the soup’s rising steam. The bowl, an enormous, ornate vessel, is crafted from gleaming porcelain, its\nsurface painted with delicate, hand-drawn scenes of mountains and rivers, adding a layer of cultural richness\n20\n\n\nHow far can we go with ImageNet for Text-to-Image generation?\nto the surreal composition. The scene is both absurd and breathtaking, blending the grandeur of a seafaring\nadventure with the comforting, whimsical charm of a bowl of soup, creating an image that is unforgettable\nand endlessly imaginative.\n2. A hedgehog and an hourglass\nThe image features a small, brown hedgehog with its characteristic spiky coat, standing near an hourglass\nin the middle of a dense forest. The hourglass is made of clear glass, and fine grains of sand are visible as\nthey fall from the top chamber to the bottom. The forest surrounding the hedgehog and the hourglass is lush\nand green, with tall trees and thick undergrowth. Sunlight filters through the leaves, creating dappled patterns\non the forest floor. The scene evokes a sense of tranquillity and the passage of time. The hedgehog appears\nto be observing the falling sand, perhaps contemplating the fleeting nature of time.\n3. A crab sculpted from yellow cheese\nA quirky crab, entirely sculpted from various types of yellow cheese, sits proudly on a white plate. Its\nbody is a smooth, golden wheel of cheese, round and rich in color, while soft, creamy cheese legs extend\noutward in neatly shaped segments, each one gently curled as if the crab is about to scuttle away. The claws\nare crafted from sharp, crumbly yellow cheese, carefully carved to resemble the pincers, with tiny bits of\ngrated Parmesan scattered across to give it texture. The eyes are tiny olives, carefully set in place with\ndelicate toothpicks, adding a playful touch to the cheese creation. Surrounding the crab, fresh basil leaves are\nplaced to resemble seaweed, completing the amusing and mouthwatering oceanic scene on the plate.\n21\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21318v1.pdf",
    "total_pages": 21,
    "title": "How far can we go with ImageNet for Text-to-Image generation?",
    "authors": [
      "L. Degeorge",
      "A. Ghosh",
      "N. Dufour",
      "D. Picard",
      "V. Kalogeiton"
    ],
    "abstract": "Recent text-to-image (T2I) generation models have achieved remarkable results\nby training on billion-scale datasets, following a `bigger is better' paradigm\nthat prioritizes data quantity over quality. We challenge this established\nparadigm by demonstrating that strategic data augmentation of small,\nwell-curated datasets can match or outperform models trained on massive\nweb-scraped collections. Using only ImageNet enhanced with well-designed text\nand image augmentations, we achieve a +2 overall score over SD-XL on GenEval\nand +5 on DPGBench while using just 1/10th the parameters and 1/1000th the\ntraining images. Our results suggest that strategic data augmentation, rather\nthan massive datasets, could offer a more sustainable path forward for T2I\ngeneration.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}