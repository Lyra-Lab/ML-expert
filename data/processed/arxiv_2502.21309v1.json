{
  "id": "arxiv_2502.21309v1",
  "text": "FANformer: Improving Large Language Models Through Effective\nPeriodicity Modeling\nYihong Dong1, Ge Li1,2, Xue Jiang1, Yongding Tao1, Kechi Zhang1, Hao Zhu1, Huanyu Liu1,\nJiazheng Ding2, Jia Li ♂1, Jinliang Deng3, and Hong Mei1,4\n1School of Computer Science, Peking University 2aiXcoder\n3The Hong Kong University of Science and Technology 4Advanced Institute of Big Data\ndongyh@stu.pku.edu.cn, lige@pku.edu.cn\nAbstract\nPeriodicity, as one of the most important basic\ncharacteristics, lays the foundation for facil-\nitating structured knowledge acquisition and\nsystematic cognitive processes within human\nlearning paradigms. However, the potential\nflaws of periodicity modeling in Transformer\naffect the learning efficiency and establishment\nof underlying principles from data for large lan-\nguage models (LLMs) built upon it. In this\npaper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning\nefficiency and performance of LLMs. We in-\ntroduce FANformer, which integrates Fourier\nAnalysis Network (FAN) into attention mecha-\nnism to achieve efficient periodicity modeling,\nby modifying the feature projection process of\nattention mechanism. Extensive experimental\nresults on language modeling show that FAN-\nformer consistently outperforms Transformer\nwhen scaling up model size and training tokens,\nunderscoring its superior learning efficiency.\nTo further validate the effectiveness of FAN-\nformer, we pretrain a FANformer-1B on 1 tril-\nlion tokens. FANformer-1B exhibits marked\nimprovements on downstream tasks compared\nto open-source LLMs with similar model pa-\nrameters or training tokens. The results posi-\ntion FANformer as an effective and promising\narchitecture for advancing LLMs. Our code\nand pretrained model are available at https:\n//github.com/YihongDong/FANformer.\n1\nIntroduction\nIn recent years, large language models (LLMs)\nhave achieved remarkable progress across vari-\nous natural language processing tasks, establishing\nthemselves as a cornerstone of modern artificial\nintelligence (Brown et al., 2020; Zhao et al., 2023;\nMinaee et al., 2024).\nThe decoder-only Trans-\nformer architecture, in particular, has emerged as\nthe de facto standard for LLM development due to\nits superior performance and scalability (OpenAI,\n2023; DeepSeek-AI et al., 2024b; Groeneveld et al.,\n2024). Besides these advancements, Transformer-\nbased models are also known for their immense\ndemand for data and computational resources dur-\ning training (Kaplan et al., 2020; Hoffmann et al.,\n2022; Chowdhery et al., 2023). In comparison, hu-\nmans are able to accomplish similar learning tasks\nwith far fewer resources. This discrepancy sug-\ngests that existing LLM architectures still suffer\nfrom low learning efficiency, leaving substantial\nroom for improvement in their ability to extract\nand generalize the knowledge from data.\nPeriodicity, characterized by recurring patterns,\nis a ubiquitous phenomenon in human life and\nlearning processes (Buzsaki, 2006; Lake et al.,\n2017). The human brain leverages pattern recogni-\ntion mechanisms to process information and ac-\nquire knowledge efficiently (Zalta et al., 2020;\nEdalati et al., 2023; Zhan et al., 2018). However,\ngeneral network architectures represented by Trans-\nformers have potential flaws in periodicity model-\ning, which could hinder their learning efficiency\n(Dong et al., 2024b; Liu et al., 2020). As shown\nin Figure 1 (a), even for a simple mod function,\nTransformer demonstrates suboptimal performance\ndespite being provided with sufficient training data\nand model capacity1. This inefficiency can be exac-\nerbated during the training process of LLMs to af-\nfect their performance, considering the periodicity\nhidden in large amounts of language data. Fourier\nAnalysis Network (FAN) (Dong et al., 2024b) has\nshown preliminary success in tasks with explicit or\nimplicit periodic features, but its integration with\nTransformer architectures for large-scale language\nmodeling remains an open challenge.\nIn this paper, we present FANformer, a novel\nfoundation architecture for LLMs that integrates\nFAN into the attention mechanism of Transformer\nto improve the learning efficiency and performance\n1We sample 400K training data from the function of mod\n5 and train a 110M parameter Transformer for 4K epochs.\n1\narXiv:2502.21309v1  [cs.CL]  28 Feb 2025\n\n\nParameter Count\n3B\n1.5B\n1.1B\n(a) Performance of Transformer and FANformer on periodicity modeling.\n(b) Performance of FANformer on language modeling.\nLoss\ny = x mod 5\nFigure 1: The performance of FANformer on periodicity modeling and language modeling. (a) shows the training\nloss of Transformer and FANformer on the fitting mod function with scalar input and their performance at the\n4,000th epoch. (b) shows the average performance of FANformer-1B and the open-source LLMs with comparable\nmodel parameters and training tokens.\nof LLMs, by achieving efficient periodicity mod-\neling. It leverages FAN to introduce Fourier prin-\nciples for capturing and representing periodic pat-\nterns, thereby enhancing the Transformer’s capabil-\nity to learn and generalize from data. Specifically,\nwe modify the feature projection process of atten-\ntion mechanism to incorporate frequency-domain\nrepresentations to facilitate capturing and modeling\nperiodicity. Figure 1 (a) demonstrates the signifi-\ncant advantages of FANformer over Transformer\non periodicity modeling, with faster convergence\nspeed and better results. In Figure 1 (b), we can ob-\nserve that FANformer-1B achieves superior perfor-\nmance with higher utilization efficiency of model\nparameter and training token when benchmarked\nagainst comparable Transformer-based LLMs.\nTo comprehensively validate the effectiveness\nand scalability of FANformer, we conducted ex-\ntensive experiments on language modeling tasks.\nThe results of scaling both model parameters and\ntraining tokens highlight that FANformer consis-\ntently surpasses Transformer, requiring only 69.2%\nof model parameters or 79.7% of training tokens\nto achieve comparable performance. We also im-\nplement a complete pretraining pipeline to pretrain\na 1.1-billion parameter FANformer (FANformer-\n1B) on 1 trillion tokens. Experiments on various\ndownstream tasks demonstrate that FANformer-1B\noutperforms open-source LLMs of the same size\nwith fewer training tokens, and exceeds LLMs with\nthree times the parameters when using the same\ntraining token. Moreover, through further detailed\nanalysis, we reveal that FANformer is a superior\nchoice compared to other variant architectures and\ndiscover three interesting findings: 1) By observ-\ning the training process, we discover the notable\nenhancements in FANformer’s learning efficiency\nover Transformer as the model continues to learn\nfrom the data. 2) For larger FANformers, the op-\ntimal hyperparameter to control the proportion of\nperiodicity modeling exhibits an increasing trend,\nsuggesting that more powerful FANformers could\nextract richer hidden periodicity features. 3) FAN-\nformer facilitates the rule-based learning paradigm,\neffectively mitigating the occurrence of \"holes\" in-\nherent in the case-based reasoning of Transformer\n(Hu et al., 2024). These findings underscore the\npotential of FANformer as an effective and scalable\narchitecture for advancing LLMs.\nThe main contributions of our work can be sum-\nmarized as follows:\n• We first demonstrate that integrating effective\nperiodicity modeling can improve the learning\nefficiency and performance of LLMs.\n• We propose FANformer, a novel LLM archi-\ntecture, which uses a simple yet effective ap-\nproach to integrate FAN into attention mecha-\nnism for efficient periodicity modeling, consis-\ntently outperforming Transformers in scaling\nmodel parameters and training tokens.\n• We pretrain and open-source FANformer-1B,\nwhich surpasses state-of-the-art publicly avail-\nable LLMs with similar parameter counts or\ntraining token budgets on downstream tasks.\n2\nPreliminary Knowledge\nFourier Analysis Network (FAN) (Dong et al.,\n2024b) enhances neural networks by introducing\n2\n\n\nX\nLinear\nLinear\nLinear\n\"!\n#!\n$!\n%&'()*+ #\"\" $\nConcat\nLinear\nAdd & Norm\nMulti-head ATF\nFeed Foward Network\nAdd & Norm\ncos (W#X) ||sin (W#X)|| B#$ + W#$X ]\nN ×\n; Heads\nMulti-head ATF\nX%&'\nX%\nFANformer\nX!\ndef FANLayer_(X, W_F, p):\n# X_p: (B, L, d*p), X_ṗ: (B, L, d*(1-2*p))\nX_p, X_p = Linear(X_F, W_F).split([d*p, d*(1-2*p)])\nreturn Concat(cos(X_p), sin(X_p), X_p)\ndef ATF(X, W_QKV, W_F, p):\n# X: (B, L, d), X_F: (B, L, d) \n# W_QKV: (d, 3d), W_F: (d, d*(1-p))\nX_F = FANLayer_(X, W_F, p)\nQKV_F = Linear(X_F, W_QKV) \nQ_F, K_F, V_F = QKV_F.split([d, d, d]) \nreturn Softmax((Q_F @ K_F.T) / sqrt(d)) @ V_F\ndef MultiHeadATF(X, W_QKV, W_o):\nHeads = [ATF(X, W_QKVi, W_Fi, p) for i in range(k)]\nreturn Concat(Heads) @ W_o\n_\n_\nNorm\nMulti-head ATF\nFeed Foward Network\nNorm\nX!\"#\nX!\nFigure 2: Left: The illustration of FANformer’s architecture. Right: The pseudocode of Multi-head ATF, where p is\nthe hyperparameter that controls the proportion of periodicity modeling for Xp.\nFourier principles for effective periodicity model-\ning. The core component of FAN is its layer design,\nwhich combines periodic basis functions with stan-\ndard linear transformations. Given a input X, the\nFAN layer is defined as:\nFANLayer(X) =[cos(WpX)∥sin(WpX)\n∥σ(W¯pX + B¯p)]\n(1)\nwhere Wp and W¯p are learnable projection matri-\nces, B¯p is a bias term, σ denotes an activation func-\ntion, and ∥represents concatenation. Compared to\nMLP layer, FAN layer explicitly encodes periodic\npatterns through Fourier series while maintaining\ngeneral-purpose modeling capabilities.\n3\nFANformer\nIn this section, we provide a detailed description\nof FANformer for sequence modeling and adopt a\ndecoder-only model to illustrate the architecture.\nGiven an input sequence s = {s1, s2, · · · , sl} ∈\nRl, where si denotes the i-th token and l represents\nthe length of sequence s, it is first mapped to the in-\nput embedding as X0 = [x1, x2, · · · , xl] ∈Rl×dh,\nwhere dh represents the model’s hidden dimension.\nThe embedding is subsequently fed into the model\nto obtain the final output XN, with each n-th layer\nof FANformer processing Xn−1, where n ∈[1, N].\nThe core of each FANformer layer lies in a revised\nattention module that integrates FAN, referred to\nas the ATtention-Fourier (ATF) module.\n3.1\nATF\nThe attention mechanism serves as a core com-\nponent of Transformer architectures, enabling dy-\nnamic interaction between tokens through query-\nkey-value (QKV) projections. While effective for\ngeneral sequence modeling, its standard implemen-\ntation exhibits limitations in capturing periodic pat-\nterns due to the inherent locality of linear projec-\ntions in the time domain. To address this, we pro-\npose the ATF module, which incorporates the op-\nerations of FAN into the QKV projection process\nto explicitly model periodicity in the frequency do-\nmain. Specifically, given the input X ∈Rl×dh, we\nfirst calculate XF ∈Rl×dh as:\nXF = FANLayer′(X),\n= [cos(Wpx)|| sin(Wpx)||(B¯p + W¯px)],\n(2)\nwhere FANLayer′ represents a variant of Eq. (1)\nwith the activation function σ in Eq. (1) replaced\nby the identity function, i.e., σ(x) = x, in this\npaper. On this basis, we employ the linear trans-\nform to XF to compute QKV projections, i.e.,\nQF , KF , VF ∈Rl×dh, as follows:\n[QF , KF , VF ] = XF [WQ, WK, WV ],\n(3)\nwhere WQ, WK, WV ∈Rdh×dh are learnable\nparameters. Similar to the standard attention mech-\nanism, the computation of ATF is defined as:\nATF(X) = softmax\n\u0012QF K⊤\nF\n√dh\n\u0013\nVF ,\n(4)\nwhere QF , KF , VF are computed using the in-\nput X via Eq. (2) and Eq. (3). To enhance the\nmodel’s capacity, we extend the ATF module to\nmultiple heads. Given input X ∈Rl×dh, the Multi-\nhead ATF first projects X into k independent heads\nthrough the ATF module. For the i-th head, we\nhave:\nHeadi = ATF(X|Wi\nQ, Wi\nK, Wi\nV ; k),\n(5)\n3\n\n\nwhere Wi\nQ, Wi\nK, Wi\nV ∈Rdh×dk are learnable pa-\nrameters for query, key, and value projections re-\nspectively, with dk = dh/k. The outputs of all\nheads are concatenated and linearly transformed:\nMultiHeadATF(X) = [Head1∥...∥Headk]WO,\n(6)\nwhere WO ∈Rdh×dh is the learnable parameter of\nout projection matrix. Note that ATF(X) is mathe-\nmatically equivalent to Attention(FANLayer′(X))\n(the detailed derivations are provided in Appendix\nF). This equivalence enables a simple yet effective\nimplementation of Multi-head ATF as shown in\nFigure 2, which can seamlessly incorporate various\nadvancements in traditional attention mechanisms,\nsuch as FlashAttention (Dao et al., 2022).\n3.2\nOverall Architecture\nThe FANformer model comprises N stacked FAN-\nformer layers, where each FANformer layer con-\nsists of a Multi-head ATF module and a feed-\nforward network (FFN) module. Following the\nprevious work (Groeneveld et al., 2024; Touvron\net al., 2023b), we employ SwiGLU (Ramachandran\net al., 2018; Shazeer, 2020) and pre-norm (Zhang\net al., 2019) as the enhancements to Transformer-\nbased LLMs. Specifically, the n-th FANformer\nlayer can be defined as:\nYn = MultiHeadATF(Norm(Xn)) + Xn,\n(7)\nXn+1 = FFN(Norm(Yn)) + Yn,\n(8)\nwhere the MultiHeadATF module is computed via\nEq. (6) and the FFN module, which leverages the\nSwiGLU activation, is expressed as:\nFFN(X) = (Swish(XW1) ⊗XW2)W3,\n(9)\nwhere W1, W2 ∈Rdh×df , W3 ∈Rdf×dh are\nlearnable parameters, ⊗denotes element-wise mul-\ntiplication, and df is the intermediate dimension.\nThe overview of FANformer’s architecture is illus-\ntrated in Figure 2.\n4\nEvaluation\nWe begin with the implementation details of our\nexperiments (Section 4.1), followed by a compre-\nhensive evaluation of FANformer from three dis-\ntinct perspectives: First, we investigate the scala-\nbility of FANformer by examining its performance\ntrends on language modeling tasks with respect to\nmodel size and training tokens (Section 4.2). Sec-\nond, we evaluate the capabilities of the pre-trained\nFANformer-1B model across multiple downstream\ntasks (Section 4.3). Third, we conduct an in-depth\nempirical analysis of FANformer, including abla-\ntion study, learning efficiency, hyperparameter im-\npact, reasoning mechanism, etc. (Section 4.4). See\nAppendix A-E for more experiments.\n4.1\nImplementation Details\nThe experiments in this paper are conducted on\neight GPUs of Tesla A100-PCIe-40G. We adopt\nthe open language model OLMo (Groeneveld et al.,\n2024) as the baseline Transformer. Building upon\nthis foundation, we apply ATF into OLMo to con-\nstruct our FANformer, where hyperparameter p is\nset to 0.25 by default. For pertaining FANformer-\n1B, we sample 1T training tokens from OLMo’s\ntraining data, i.e., Dolma (Soldaini et al., 2024).\nFor other experiments, we train models on a smaller\nsample of Dolma, i.e., Dolma v1_6-sample (Al-\nlenAI, 2023), with roughly 10B tokens. The de-\ntailed pretraining and experimental setups are pro-\nvided in Appendix G.\n4.2\nScalability of FANformer\nWe explore the scalability of FANformer compared\nwith Transformer to investigate performance trends\nin the construction of much larger models.\nSetup.\nWe follow OLMo’s configuration and\nvary the FFN’s intermediate dimension df to keep\nthe number of parameters consistent for all models\nin this experiment. For scaling up mode parameters,\nwe adopt Dolma v1_6-sample as training data and\ntrain LLMs from 268M to 7B. We compare FAN-\nformer with the standard Transformer and a variant\nof FANformer, termed Transformer+ATM, which\nuses MLP layer instead of FAN layer in FANformer.\nFor scaling up training tokens, we train 1B LLMs\non the first 200 billion of our sampled 1T tokens.\nResults.\nAs shown in Figure 3, the scaling law\n(Kaplan et al., 2020) empirically aligns well with\nthe results obtained from our FANformer, under-\nscoring its superior scalability properties. Figure\n3 (left) reveals that the implementation of FAN\nconsistently surpasses the performance of the stan-\ndard Transformer across a range of model sizes.\nThis finding highlights FAN’s enhanced scalabil-\nity in terms of parameter efficiency, as it achieves\ncomparable performance with only 69.2% of the\nparameters required by the standard Transformer.\n4\n\n\nTable 1: Zero-shot performance of FANformer-1B versus other comparable open-source LLMs on 8 core tasks\nfrom the downstream evaluation suite following OLMo (Groeneveld et al., 2024). The results of baselines are taken\nfrom the work (Groeneveld et al., 2024; Ye et al., 2024; Dong et al., 2024a).\nModels\nParam.\nTokens\nARC-C\nARC-E\nBoolQ\nHella.\nOBQA\nPIQA\nSCIQ\nWino.\nAvg.\nLLMs around 1B parameters\nQwen2.5-1.5B\n1.5B\n18T\n41.2\n75.5\n74.0\n50.2\n52.4\n75.7\n94.7\n63.3\n65.9\nR1-Distill-Qwen1.5B\n1.5B\n18T+\n36.2\n54.4\n69.1\n41.8\n35.4\n65.1\n89.5\n55.3\n55.9\nLlama-3.2-1B\n1.2B\n9T\n31.4\n65.6\n64.3\n47.8\n46\n74.5\n92.3\n60.7\n60.4\nTinyLlama-v1.1-3T\n1.1B\n3T\n34.8\n53.2\n64.6\n58.7\n43.6\n71.1\n90.5\n58.9\n59.4\nOLMo-1B\n1.1B\n2T\n34.5\n58.1\n60.7\n62.5\n46.4\n73.7\n88.1\n58.9\n60.4\nLLMs trained on 1T tokens\nOpenLLaMA-v2-3B\n3B\n1T\n33.9\n67.6\n65.7\n70.0\n26\n76.7\n92.9\n62.9\n62.0\nStableLM-base-alpha-v2-3B\n3B\n1T\n32.4\n67.3\n64.6\n68.6\n26.4\n76\n89.5\n62.1\n60.9\nTinyLlama-v1.1-1T\n1.1B\n1T\n33.1\n49.5\n58.4\n52.5\n37.8\n70.4\n86.4\n55.2\n55.4\nFANformer-1B\n1.1B\n1T\n43.8\n72.5\n64.9\n64.7\n48.2\n75.5\n94.8\n61.3\n65.6\n31% Fewer Params\n20% Fewer Tokens\nFigure 3: Language modeling loss of scaling up model\nparameter and training tokens. Left: we train LLMs\nfrom 268M to 7B parameters. Right: we evaluate 1.0B\nLLMs every 20B tokens up to 200B tokens.\nNotably, the scaling curve of Transformer+ATM\nclosely overlaps with that of the standard Trans-\nformer, indicating that merely revising attention\nmechanisms using MLP Layer is insufficient. This\nobservation further emphasizes that FANformer’s\nperformance gains are not attributable to network\ndepth increase, but rather to its special architec-\ntural design. Figure 3 (right) demonstrates that\nFANformer achieves performance parity with the\nstandard Transformer while utilizing significantly\nfewer training tokens. Specifically, FANformer re-\nquires only 159.6B training tokens to match the\nperformance of the standard Transformer trained\non 200B tokens, representing a 20.3% reduction\nin training resource requirements. These findings\nsuggest that FANformer exhibits superior utiliza-\ntion efficiency in terms of both model parameters\nand training tokens compared to the standard Trans-\nformer architecture.\n4.3\nPerformance of FANformer-1B\nWe pretrain FANformer-1B on 1 trillion tokens and\nreport zero-shot performance on a set of common-\nsense downstream tasks, following previous work\n(Brown et al., 2020; Touvron et al., 2023a; Groen-\neveld et al., 2024, inter alia).\nSetup.\nThe downstream evaluation suite consists\nof 8 core tasks, including ARC-C (Clark et al.,\n2018), ARC-E (Clark et al., 2018), BoolQ (Clark\net al., 2019), HellaSwag (Zellers et al., 2019),\nOBQA (Mihaylov et al., 2018), PIQA (Bisk et al.,\n2020), SCIQ (Welbl et al., 2017), and WinoGrande\n(Sakaguchi et al., 2020). We compare pretrained\nFANformer-1B to seven open-source LLMs with\ncomparable model parameters or training tokens,\nincluding Qwen2.5-1.5B (Team, 2024), R1-Distill-\nQwen1.5B (DeepSeek-AI et al., 2025), Llama-\n3.2-1B (Dubey et al., 2024), TinyLlama-v1.1-1B\n(Zhang et al., 2024), OLMo-1B (Groeneveld et al.,\n2024), OpenLLaMA-v2-3B (Geng et al., 2023),\nand StableLM-base-alpha-v2-3B (Tow, 2023).\nResults.\nTable 1 presents the evaluation results\nof our pre-trained FANformer-1B on downstream\ntasks. It is evident that FANformer-1B surpasses\nLLMs with comparable parameter sizes, such as\nLlama-3.2-1B, TinyLlama-v1.1-3T, and OLMo-\n1B, while utilizing significantly fewer training\ndata.\nCompared to the base model OLMo-1B,\nFANformer-1B achieves a relative improvement\nof 8.8% in the average performance of downstream\ntasks using only half the training data. On these\ntasks, FANformer-1B also demonstrates perfor-\nmance comparable to Qwen2.5-1.5B, which is the\ncurrent SOTA LLM around 1B. For LLMs train-\ning on 1T tokens, FANformer-1B even exceeds\nLLMs with three times the parameters, showing\nan average relative performance improvement of\n6.0-7.9% across all tasks. Moreover, while R1-\nDistill-Qwen1.5B shows notable improvements in\nreasoning capabilities based on its reported per-\n5\n\n\nTable 2: Results of ablation study and variant analysis on LLMs with 1B parameters trained on Dolma v1_6-sample\ndataset (about 10B tokens). The complete experimental results can be found in Table 4 and Table 5 of Appendix.\nVariants\nParam.\nTraining\nLoss ↓\nV2 Eval\nLoss ↓\nV2 Eval\nPPL ↓\nV3 Eval\nLoss ↓\nV3 Eval\nPPL ↓\nDownStream\nAvg Acc. ↑\nTransformer\n1.0×\n2.889\n3.33\n30.20\n3.07\n24.28\n53.10\nTransformer (FFN ←FAN)\n1.0×\n2.880\n3.31\n29.79\n3.05\n23.96\n53.95\nSame Parameter\nTransformer + ATM\n1.0×\n2.890\n3.33\n30.31\n3.07\n24.36\n53.69\nTransformer + ATL\n1.0×\n2.882\n3.31\n29.68\n3.05\n23.97\n53.46\nFANformer + Activation\n1.0×\n2.893\n3.34\n30.64\n3.07\n24.50\n53.61\nFANformer\n1.0×\n2.863\n3.30\n29.40\n3.04\n23.62\n55.19\nSame Dimension\nTransformer + ATM\n1.06×\n2.886\n3.33\n30.18\n3.06\n24.28\n52.86\nTransformer + ATL\n1.06×\n2.879\n3.31\n29.76\n3.05\n23.94\n54.23\nFANformer + Activation\n1.04×\n2.887\n3.34\n30.57\n3.07\n24.39\n53.13\nFANformer\n1.04×\n2.856\n3.29\n29.22\n3.03\n23.47\n54.88\nformance, it exhibits significantly weaker general\nperformance on these commonsense downstream\ntasks. This observation shows the shortcomings of\ndistillation, highlighting the necessity of the pre-\ntraining stage and the importance of research into\nmore efficient model architectures.\n4.4\nFurther Analysis\n4.4.1\nAblation Study and Variant Analysis\nSetup.\nWe compare FANformer to other vari-\nant architectures, including the above-mentioned\nTransformer+ATM, Transformer+ATL: use two lin-\near transforms to compute QKV projection, FAN-\nformer +Activation: employ Eq. (1) with GELU\n(Hendrycks et al., 2016) activation function instead\nof Eq. (2), Transformer (FFN ←FAN): replace\nFFN with FAN (Dong et al., 2024b), and standard\nTransformer as their ablations.\nResults.\nFrom Table 2, we have the following\nfindings: 1) FANformer consistently outperforms\nother variant architectures in both scenarios of\nthe same parameter and same dimension on all\nevaluation metrics. 2) The performance of Trans-\nformer+ATM and Transformer+ATL is notably in-\nferior to that of FANformer, indicating that the core\nimprovement stems from the ATF module we de-\nsigned. 3) Although Transformer (FFN ←FAN)\nyields some improvement, this enhancement is infe-\nrior to the gains achieved by FANformer, suggest-\ning that integrating periodicity modeling within\nattention is more advantageous than FFN on lan-\nguage modeling. 4) Incorporating activation func-\ntions such as GELU into the attention mechanism\ntends to degrade model performance. Specifically,\nFANformer+Activation and Transformer+ATM ex-\nhibit weaker performance compared to FANformer\nand Transformer+ATL, likely because these acti-\nvation functions suppress certain features, thereby\nhindering subsequent attention operations.\n100\n200\n300\n400\n500\n4\n5\n6\n7\n8\n9\n10\nLoss\nSteps\n— Transformer\n— FANformer\nFigure 4: Training loss of FANformer and Transformer\non early training steps. The complete training loss is\nprovided in Figure 8 of Appendix.\n4.4.2\nTraining Dynamics\nWe perform a comparative analysis of the loss\ntrends during the training process between our\nFANformer and Transformer, as illustrated in Fig-\nure 4. The experimental results indicate that the\nloss of FANformer decreases more slowly in the\nearly stages compared to Transformer, which we\nhypothesize may be due to the initial lack of es-\ntablished periodic modeling. As the training pro-\ngresses and periodic modeling gradually improves,\nFANformer demonstrates a faster convergence rate,\nwith its loss decreasing more rapidly than that of\nTransformer. This result suggests that as the model\nprogressively learns from the data, the learning ef-\nficiency of our FANformer notably surpasses the\nstandard Transformer.\n6\n\n\nTransformer\nFANformer\n(a) Modular Addition Task\nTransformer\nFANformer\n(b) Linear Regression Task\nFigure 5: Performance of FANformer and Transformer on modular addition and linear regression tasks, where the\ndarkened regions indicate areas where the model performance approaches zero, signifying the emergence of the\n\"hole\" as described in the work (Hu et al., 2024).\n0.15\n0.2\n0.25\n0.3\n0.35\nHyperparameter p\n2.850\n2.855\n2.860\n2.865\n2.870\n2.875\n2.880\n2.885\n2.890\nLoss\n53.00\n53.25\n53.50\n53.75\n54.00\n54.25\n54.50\n54.75\n55.00\nPerformance of Downstream Tasks\nFigure 6: Effect of hyperparameter p in FANformer\non its training loss and downstream task performance,\nwhere the red dashed line represents the training loss\nof Transformer, while the blue dashed line denotes the\nperformance on downstream tasks of Transformer.\n4.4.3\nEffect of hyperparameter p\nWe systematically investigate the impact of hyper-\nparameter p, which controls the proportion of pe-\nriodicity modeling in FANformer, on model per-\nformance across its value range. The experimental\nresults from the 1B-scale FANformer (as shown\nin Figure 6) demonstrate that our model exhibits\nstrong robustness in terms of training loss and\ndownstream task accuracy, with relatively small\nperformance fluctuations. Furthermore, regardless\nof the variation in p values, FANformer consistently\noutperforms the standard Transformer (horizontal\nbaseline). Analysis of experimental results from\nmodels of different scales (300M, 1B, 3B) (as il-\nlustrated in Figure 9 of Appendix) reveals a clear\ntrend: larger models tend to exhibit higher optimal\np values. This observation suggests that more pow-\nerful FANformers are better equipped to extract\nmore intricate latent periodicity features.\n4.4.4\nCase-based and Rule-based Reasoning\nSetup.\nFollowing the work Hu et al. (2024), we\nevaluate the case-based and rule-based reasoning\nof Transformer and our FANformer on two tasks,\nincluding: (1) Modular addition: c = (a + b)\nmod 113 with a, b ∈[0, 112]; (2) Linear regres-\nsion: c = a+2b+3 with a, b ∈[0, 99]. We finetune\npretained LLMs, i.e., OLMo-1B and FANformer-\n1B, on each task dataset for 500 epochs and their\nperformance is measured via the Leave-Square-Out\nmethod, sampling 10 generations at temperature\n0.5 per test point.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTraining Accuracy\nTest Accuracy\nTraining Accuracy\nTest Accuracy\nModular Addition\nLinear Regression\nTransformer\nFANformer\nFigure 7: Training accuracy and test accuracy of FAN-\nformer and Transformer on modular addition and linear\nregression tasks.\nResults.\nAs illustrated in Figure 5, both Tran-\nformer and our FANformer achieve near-perfect ac-\ncuracy on the training set of modular addition and\nlinear regression tasks, approaching approximately\n100%. However, a critical divergence emerges in\ntheir performance on the test sets. Specifically,\nas shown in Figure 5, Transformer exhibits a pro-\nnounced failure to generalize, resulting in a \"black\nhole\" in the center of the figure, indicating that its\naccuracy on the test dataset drops to nearly zero.\n7\n\n\nThis observation is consistent with the findings re-\nported in the work (Hu et al., 2024). In contrast,\nFANformer demonstrates a marked improvement\nin addressing the \"hole\" issue. In the linear re-\ngression and modular addition tasks, there is no\nobvious hole observed, further corroborating the\nhypothesis that, relative to the Transformer-based\nmodel, FANformer possesses a stronger tendency\nto learn underlying rules, thereby achieving supe-\nrior generalization performance.\n5\nRelated Work\nIn this section, we outline the three most relevant\ndirections and associated papers of this work.\n5.1\nLarge Language Models\nThe rapid advancement of language models has\nrevolutionized natural language processing and ar-\ntificial intelligence research (Radford, 2018; Dubey\net al., 2024; DeepSeek-AI et al., 2025). The emer-\ngence of GPT-3 (Brown et al., 2020), with 175B pa-\nrameters, showcased remarkable few-shot prompt-\ning abilities, suggesting that scaling laws (Kaplan\net al., 2020) could unlock emergent capabilities.\nRecent notable LLMs like PaLM (Chowdhery et al.,\n2023), LLaMA (Touvron et al., 2023a), GPT-4\n(Chowdhery et al., 2023), and DeepSeek (Bi et al.,\n2024) further pushed the boundaries of model size\nand performance. Additionally, the open-source\nrelease of OLMo (Groeneveld et al., 2024) has\nprovided valuable resources for the community, en-\nabling more accessible training of LLMs.\n5.2\nAdvances in Transformer Architecture\nRecent advancements in Transformer architecture\nprimarily address two fundamental limitations:\ncomputational inefficiency in long-context pro-\ncessing and insufficient expressiveness of atten-\ntion mechanisms. To tackle long-context process-\ning limitations, Sparse Transformer (Child et al.,\n2019) and Longformer (Beltagy et al., 2020) intro-\nduce sparsity and local Attention. Query mecha-\nnism innovations like MQA (Shazeer, 2019) and\nGQA (Ainslie et al., 2023) optimize token interac-\ntion efficiency. For inference acceleration, MLA\n(DeepSeek-AI et al., 2024a) pioneers a low-rank\nlatent space compression technique for Key-Value\npairs, dramatically reducing cache memory re-\nquirements. Hardware-level optimizations emerge\nthrough FlashAttention (Dao et al., 2022), which\nstrategically minimizes GPU memory access over-\nhead during attention computation. To improve the\nexpressiveness of networks, Probabilistic Attention\nKeys (Nguyen et al., 2022) replace deterministic\nkey-query interactions with Gaussian kernels, en-\nabling better capture of semantic relationships. Se-\nlective Attention (Leviathan et al., 2024) introduces\ndynamic token masking to suppress irrelevant fea-\ntures to refine attention mechanism. Differential\nTransformer (Ye et al., 2024) addresses attention\nnoise in long contexts by computing dual softmax\nmaps and performing subtraction.\nDifferent from previous work, we improve lan-\nguage modeling by addressing the challenge of\nmodeling periodicity in Transformers, which can\nseamlessly incorporate the aforementioned works\nfor revising the attention mechanism, as demon-\nstrated in the derivation provided in Appendix F.\n5.3\nFourier-based Neural Networks\nPrevious research on Fourier-based Neural Net-\nworks was aimed at solving some domain-specific\napplications (Zuo et al., 2005; Tan, 2006; Chen\net al., 2022; Li et al., 2021). Some studies specifi-\ncally explored the use of sinusoidal activations (e.g.,\ncosine (Silvescu, 1999) (Ngom et al., 2021) or sine\n(Parascandolo et al., 2016; Sitzmann et al., 2020))\nto approximate periodic patterns (Liu, 2013). How-\never, these approaches lacked generalizability be-\nyond narrow domains due to rigid frequency param-\neterization and limited scalability (Uteuliyeva et al.,\n2020; Liu et al., 2020). FAN (Dong et al., 2024b)\nintroduces Fourier Principle into neural networks to\neffectively solve periodicity modeling challenges\nwhile maintaining broad applicability similar to\nMLP. FNet (Lee-Thorp et al., 2022) replaces self-\nattention with Fourier Transform to achieve linear\ncomplexity, but it sacrifices the performance of lan-\nguage modeling. In contrast, we employ effective\nperiodicity modeling to improve LLMs.\n6\nConclusion\nWe propose FANformer, a novel LLM architec-\nture that enhances learning efficiency by integrat-\ning Fourier Analysis Network into self-attention\nmechanism for effective periodicity modeling. Ex-\nperiments demonstrate that FANformer outper-\nforms Transformer when scaling model parameters\nand training tokens, achieving better performance\nwith 31% fewer parameters and 20% fewer tokens.\nPretrained FANformer-1B surpasses open-source\nLLMs of comparable size or training scale on var-\nious downstream tasks, highlighting the benefits\n8\n\n\nof periodicity-aware architecture design. The dis-\ncovery of FANformer’s enhanced scalability, learn-\ning efficiency, and rule-based learning advantages\nsuggests potential pathways for developing more\nefficient and high-performance language models.\n7\nLimitations\nOur work has several limitations, which we aim to\naddress in our future work:\nFirst, due to constraints in computational re-\nsources, we only pretrain the FANformer-1B on\n1 trillion tokens. However, our experimental re-\nsults regarding FANformer’s scalability indicate\nthat our FANformer demonstrates favorable scaling\nbehavior during training, suggesting that increasing\nthe model size and training tokens could lead to\nmore significant performance improvements. To\nexplore this further, we plan to seek additional com-\nputational resources to train larger-scale language\nmodels.\nSecond, our work is orthogonal to the existing\napproaches for revising the attention mechanism,\ni.e., our work can seamlessly incorporate them, as\nverified in the derivation provided in Appendix F.\nThere are numerous variants of attention mecha-\nnisms, as discussed in the related work (Section\n5.2), such as Flash Attention (Dao et al., 2022),\nMQA (Shazeer, 2019), and MLA (DeepSeek-AI\net al., 2024a). In this work, we only incorporate\nFlash Attention for necessary acceleration, while\nleaving the exploration of other approaches for fu-\nture work.\nThird, although we have observed that enhancing\nthe ability of language models to model periodic\npatterns can improve language modeling perfor-\nmance, the underlying mechanisms responsible for\nthis improvement remain underexplored. To the\nbest of our knowledge, it has hardly been studied\nthe role of periodicity or the potential periodic be-\nhaviors of LLMs on language modeling. Therefore,\nin future work, we will conduct a more comprehen-\nsive investigation into the fundamental mechanisms\nof periodicity in language modeling.\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong,\nYury Zemlyanskiy, Federico Lebrón, and Sumit Sang-\nhai. 2023. GQA: training generalized multi-query\ntransformer models from multi-head checkpoints. In\nEMNLP, pages 4895–4901. Association for Compu-\ntational Linguistics.\nAllenAI. 2023. Dolma.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. CoRR,\nabs/2004.05150.\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,\nDamai Dai, Chengqi Deng, Honghui Ding, Kai Dong,\nQiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun\nGao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong\nGuo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie\nHu, Panpan Huang, Erhang Li, Guowei Li, Jiashi\nLi, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin,\nAlex X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin\nLiu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo,\nShirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Jun-\njie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong\nRuan, Zhangli Sha, Zhihong Shao, Junxiao Song,\nXuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui\nTang, Bingxuan Wang, Peiyi Wang, Shiyu Wang,\nYaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin\nXie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei\nXu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang\nYou, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei\nZhang, Lecong Zhang, Liyue Zhang, Mingchuan\nZhang, Minghua Zhang, Wentao Zhang, Yichao\nZhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou,\nShunfeng Zhou, Qihao Zhu, and Yuheng Zou. 2024.\nDeepseek LLM: scaling open-source language mod-\nels with longtermism. CoRR, abs/2401.02954.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: reasoning about\nphysical commonsense in natural language. In AAAI,\npages 7432–7439. AAAI Press.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nGyorgy Buzsaki. 2006. Rhythms of the Brain. Oxford\nuniversity press.\nHanlong Chen, Luzhe Huang, Tairan Liu, and Aydo-\ngan Ozcan. 2022. Fourier imager network (FIN):\nA deep neural network for hologram reconstruction\nwith superior external generalization. Light: Science\n& Applications.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers. CoRR, abs/1904.10509.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\n9\n\n\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2023. Palm: Scaling language mod-\neling with pathways. J. Mach. Learn. Res., 24:240:1–\n240:113.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL-\nHLT (1), pages 2924–2936. Association for Compu-\ntational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In MLCW, volume 3944 of Lecture Notes\nin Computer Science, pages 177–190. Springer.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nIn NeurIPS.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107–124.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingx-\nuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng,\nChong Ruan, Damai Dai, Daya Guo, Dejian Yang,\nDeli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli\nLuo, Guangbo Hao, Guanting Chen, Guowei Li,\nHao Zhang, Hanwei Xu, Hao Yang, Haowei Zhang,\nHonghui Ding, Huajian Xin, Huazuo Gao, Hui Li,\nHui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Ji-\naqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie\nQiu, Junxiao Song, Kai Dong, Kaige Gao, Kang\nGuan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia,\nLiang Zhao, Liyue Zhang, Meng Li, Miaojun Wang,\nMingchuan Zhang, Minghua Zhang, Minghui Tang,\nMingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du,\nR. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin\nXu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shaoqing Wu, Shengfeng\nYe, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuip-\ning Yu, Shunfeng Zhou, Size Zheng, Tao Wang, Tian\nPei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding\nZeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun\nGao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xi-\nanzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang,\nXiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiao-\ntao Nie, and Xiaowen Sun. 2024a. Deepseek-v2: A\nstrong, economical, and efficient mixture-of-experts\nlanguage model. CoRR, abs/2405.04434.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx-\nuan Wang, Bochao Wu, Chengda Lu, Chenggang\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Daya Guo, Dejian Yang, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai,\nFuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng\nWang, Haowei Zhang, Honghui Ding, Huajian Xin,\nHuazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang,\nJianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang,\nJin Chen, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu,\nKaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean\nWang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao,\nLitong Wang, Liyue Zhang, Meng Li, Miaojun Wang,\nMingchuan Zhang, Minghua Zhang, Minghui Tang,\nMingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu\nChen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, Runxin\nXu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao\nLu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu,\nShengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu\nWang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou,\nShuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun,\nW. L. Xiao, and Wangding Zeng. 2024b. Deepseek-\nv3 technical report. CoRR, abs/2412.19437.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn IWP@IJCNLP. Asian Federation of Natural Lan-\nguage Processing.\nXin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon,\nZijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-\nYang Liu, Matthijs Van Keirsbilck, Min-Hung Chen,\nYoshi Suhara, Yingyan Lin, Jan Kautz, and Pavlo\nMolchanov. 2024a.\nHymba: A hybrid-head ar-\nchitecture for small language models.\nCoRR,\nabs/2411.13676.\nYihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi\nZhang, Jia Li, Jing Su, Jun Zhang, and Jingjing\n10\n\n\nXu. 2024b. Fan: Fourier analysis networks. arXiv\npreprint arXiv:2410.02675.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nMohammadreza Edalati, Fabrice Wallois, Javad Safaie,\nGhida Ghostine, Guy Kongolo, Laurel J Trainor, and\nSahar Moghimi. 2023. Rhythm in the premature\nneonate brain: Very early processing of auditory beat\nand meter. Journal of Neuroscience, 43(15):2794–\n2802.\nXinyang Geng and Hao Liu. 2023. Openllama: An open\nreproduction of llama.\nDirk Groeneveld, Iz Beltagy, Evan Pete Walsh, Ak-\nshita Bhagia, Rodney Kinney, Oyvind Tafjord,\nAnanya Harsh Jha, Hamish Ivison, Ian Magnusson,\nYizhong Wang, Shane Arora, David Atkinson, Rus-\nsell Authur, Khyathi Raghavi Chandu, Arman Cohan,\nJennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hes-\nsel, Tushar Khot, William Merrill, Jacob Morrison,\nNiklas Muennighoff, Aakanksha Naik, Crystal Nam,\nMatthew E. Peters, Valentina Pyatkin, Abhilasha\nRavichander, Dustin Schwenk, Saurabh Shah, Will\nSmith, Emma Strubell, Nishant Subramani, Mitchell\nWortsman, Pradeep Dasigi, Nathan Lambert, Kyle\nRichardson, Luke Zettlemoyer, Jesse Dodge, Kyle\nLo, Luca Soldaini, Noah A. Smith, and Hannaneh\nHajishirzi. 2024. Olmo: Accelerating the science of\nlanguage models. In ACL (1), pages 15789–15809.\nAssociation for Computational Linguistics.\nDan Hendrycks and Kevin Gimpel. 2016.\nGaus-\nsian error linear units (gelus).\narXiv preprint\narXiv:1606.08415.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models. CoRR, abs/2203.15556.\nYi Hu, Xiaojuan Tang, Haotong Yang, and Muhan\nZhang. 2024. Case-based or rule-based: How do\ntransformers do the math?\nIn ICML. OpenRe-\nview.net.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenen-\nbaum, and Samuel J Gershman. 2017. Building ma-\nchines that learn and think like people. Behavioral\nand brain sciences, 40:e253.\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and\nSantiago Ontañón. 2022. Fnet: Mixing tokens with\nfourier transforms. In NAACL-HLT, pages 4296–\n4313. Association for Computational Linguistics.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2024.\nSelective attention improves transformer.\nCoRR, abs/2410.02703.\nShen Li. 2018. Getting started with distributed data\nparallel.\nZongyi Li, Nikola Borislavov Kovachki, Kamyar Az-\nizzadenesheli, Burigede Liu, Kaushik Bhattacharya,\nAndrew M. Stuart, and Anima Anandkumar. 2021.\nFourier neural operator for parametric partial differ-\nential equations. In ICLR. OpenReview.net.\nShuang Liu. 2013. Fourier neural network for machine\nlearning. In 2013 international conference on ma-\nchine learning and cybernetics, volume 1, pages 285–\n290. IEEE.\nZiyin Liu, Tilman Hartwig, and Masahito Ueda. 2020.\nNeural networks fail to learn periodic functions and\nhow to fix it. In NeurIPS.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR (Poster). Open-\nReview.net.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question an-\nswering. In EMNLP, pages 2381–2391. Association\nfor Computational Linguistics.\n11\n\n\nShervin Minaee, Tomás Mikolov, Narjes Nikzad,\nMeysam Chenaghlu, Richard Socher, Xavier Am-\natriain, and Jianfeng Gao. 2024. Large language\nmodels: A survey. CoRR, abs/2402.06196.\nMarieme Ngom and Oana Marin. 2021. Fourier neural\nnetworks as function approximators and differential\nequation solvers. Statistical Analysis and Data Min-\ning: The ASA Data Science Journal, 14(6):647–661.\nTam Minh Nguyen, Tan Minh Nguyen, Dung D. D. Le,\nDuy Khuong Nguyen, Viet-Anh Tran, Richard G.\nBaraniuk, Nhat Ho, and Stanley J. Osher. 2022.\nImproving transformers with probabilistic attention\nkeys.\nIn ICML, volume 162 of Proceedings of\nMachine Learning Research, pages 16595–16621.\nPMLR.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nGiambattista Parascandolo, Heikki Huttunen, and Tuo-\nmas Virtanen. 2016. Taming the waves: sine as acti-\nvation function in deep neural networks.\nAlec Radford. 2018. Improving language understanding\nby generative pre-training.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: memory optimizations\ntoward training trillion parameter models. In SC,\npage 20. IEEE/ACM.\nPrajit Ramachandran, Barret Zoph, and Quoc V. Le.\n2018. Searching for activation functions. In ICLR\n(Workshop). OpenReview.net.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S. Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI Spring Symposium: Logical Formal-\nizations of Commonsense Reasoning. AAAI.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In AAAI,\npages 8732–8740. AAAI Press.\nNoam Shazeer. 2019. Fast transformer decoding: One\nwrite-head is all you need. CoRR, abs/1911.02150.\nNoam Shazeer. 2020.\nGLU variants improve trans-\nformer. CoRR, abs/2002.05202.\nAdrian Silvescu. 1999. Fourier neural networks. In\nIJCNN’99. International Joint Conference on Neu-\nral Networks. Proceedings (Cat. No. 99CH36339),\nvolume 1, pages 488–491. IEEE.\nVincent Sitzmann, Julien Martel, Alexander Bergman,\nDavid Lindell, and Gordon Wetzstein. 2020. Implicit\nneural representations with periodic activation func-\ntions. Advances in neural information processing\nsystems, 33:7462–7473.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y. Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP, pages 1631–1642. ACL.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin\nSchwenk, David Atkinson, Russell Authur, Ben Bo-\ngin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai\nElazar, Valentin Hofmann, Ananya Harsh Jha, Sachin\nKumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian\nMagnusson, Jacob Morrison, Niklas Muennighoff,\nAakanksha Naik, Crystal Nam, Matthew E. Peters,\nAbhilasha Ravichander, Kyle Richardson, Zejiang\nShen, Emma Strubell, Nishant Subramani, Oyvind\nTafjord, Pete Walsh, Luke Zettlemoyer, Noah A.\nSmith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groen-\neveld, Jesse Dodge, and Kyle Lo. 2024.\nDolma:\nan open corpus of three trillion tokens for language\nmodel pretraining research. In ACL (1), pages 15725–\n15788. Association for Computational Linguistics.\nHS Tan. 2006. Fourier neural networks and generalized\nsingle hidden layer networks in aircraft engine fault\ndiagnostics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nJonathan Tow. 2023. Stablelm alpha v2 models.\nMalika Uteuliyeva, Abylay Zhumekenov, Rustem\nTakhanov, Zhenisbek Assylbekov, Alejandro J. Cas-\ntro, and Olzhas Kabdolov. 2020.\nFourier neural\n12\n\n\nnetworks: A comparative study. Intell. Data Anal.,\n24(5):1107–1120.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nIn NUT@EMNLP, pages 94–106. Association for\nComputational Linguistics.\nTianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu,\nGao Huang, and Furu Wei. 2024. Differential trans-\nformer. CoRR, abs/2410.05258.\nArnaud Zalta, Spase Petkoski, and Benjamin Morillon.\n2020. Natural rhythms of periodic temporal attention.\nNature communications, 11(1):1051.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019.\nHellaswag: Can\na machine really finish your sentence? In ACL (1),\npages 4791–4800. Association for Computational\nLinguistics.\nLexia Zhan, Dingrong Guo, Gang Chen, and Jiongjiong\nYang. 2018. Effects of repetition learning on associa-\ntive recognition over time: Role of the hippocampus\nand prefrontal cortex. Frontiers in human neuro-\nscience, 12:277.\nBiao Zhang and Rico Sennrich. 2019. Root mean square\nlayer normalization.\nIn NeurIPS, pages 12360–\n12371.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and\nWei Lu. 2024. Tinyllama: An open-source small\nlanguage model. CoRR, abs/2401.02385.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\nDefa Zhu, Hongzhi Huang, Zihao Huang, Yutao\nZeng, Yunyao Mao, Banggu Wu, Qiyang Min,\nand Xun Zhou. 2024. Hyper-connections. CoRR,\nabs/2409.19606.\nWei Zuo and Lilong Cai. 2005. Tracking control of\nnonlinear systems using fourier neural network. In\nProceedings, 2005 IEEE/ASME International Confer-\nence on Advanced Intelligent Mechatronics., pages\n670–675. IEEE.\n13\n\n\nA\nTraining Loss Curves of OLMO and FANformer\nWe present the training loss curves for OLMO and FANformer trained on 1 trillion tokens (i.e., 250K\nsteps) in Figure 8.\nOLMo-19\nOLMo-18\nOLMo-17\nOLMo-16\nOLMo-15\nOLMo-14\nOLMo-13\nOLMo-12\nOLMo-11\nOLMo-10\nOLMo-9\nOLMo-8\nOLMo-7\nOLMo-6\nOLMo-5\nOLMo-4\nOLMo-3\nOLMo-2\nOLMo-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n50k\n100k\n150k\n200k\n250k\n4\n6\n8\n10\nFinal Loss: 2.4944\nStep\nLoss\n(a) Training loss of OLMO-1B\nFANformer-9\nFANformer-8\nFANformer-7\nFANformer-6\nFANformer-5\nFANformer-4\nFANformer-3\nFANformer-2\nFANformer-1\n\n\n\n\n\n\n\n\n\n50k\n100k\n150k\n200k\n250k\n4\n6\n8\n10\nFinal Loss: 2.3769\nStep\nLoss\n(b) Training loss of FANformer-1B\nFigure 8: The training process of OLMO and FANformer. The data in Figure (a) is sourced from the publicly\navailable results of OLMO (https://wandb.ai/ai2-llm/OLMo-1B?nw=nwuserdirkgr).\nB\nExtended Results about Hyperparameter p for Section 4.4.3\nWe investigate the impact of hyperparameter p on training loss across FANformer models of varying\nscales, with the results illustrated in Figure 9.\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n3.09\n3.10\n3.11\nLoss\nFANformer-300M\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n2.855\n2.860\n2.865\nLoss\nFANformer-1B\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nHyperparameter p\n2.748\n2.750\n2.752\n2.754\nLoss\nFANformer-3B\nFigure 9: The impact of hyperparameter p on FANformer models of varying sizes.\n14\n\n\nC\nInstruction Following with SFT\nModel\nMMLU AlpacaEval ToxiGen TruthfulQA (Info+True)\n0-shot ↑\n%win ↑\n% Toxic ↓\nAccuracy ↑\nOLMo-1B-SFT\n24.3\n1.90\n2.8\n55.8\nFANformer-1B-SFT\n26.7\n2.51\n10.4\n83.0\nTable 3: Evaluation results of OLMo-1B-SFT and FANformer-1B-SFT on MMLU, AlpacaEval, ToxiGen, and\nTruthfulQA (Info+True). Higher values are better for MMLU, AlpacaEval, and TruthfulQA, while lower values are\nbetter for ToxiGen.\nC.1\nModels\nFANformer-1B-SFT: Our pretrained model on 1 trillion tokens, fine-tuned using supervised fine-tuning\n(SFT) on the tulu-3-sft-olmo-2-mixture dataset.\nOLMo-1B-SFT: A 1B parameter version of OLMo, pre-trained on 3 trillion tokens and fine-tuned\nusing supervised fine-tuning (SFT) on the tulu-3-sft-olmo-2-mixture dataset. Model available at\nallenai/OLMo-1B-hf.\nFor both models, we follow the tokenizer’s chat template for prompt formatting when available.\nC.2\nEvaluation Setup\nWe evaluate the models on four benchmarks: MMLU, TruthfulQA, AlpacaEval, and ToxiGen. The\nevaluation is conducted using the open-instruct evaluation suite, which has been widely adopted for\nevaluating instruction-tuned language models. Below, we describe the setup for each benchmark.\nMMLU\nWe use the official MMLU evaluation script with 0-shot prompting. The maximum token\nlength is set to 2048, and we do not employ chain-of-thought (CoT) reasoning. The evaluation reports the\naverage accuracy across test examples.\nAlpacaEval\nWe use the AlpacaEval V1 benchmark with the default setup from the official repository\n2. The evaluated models generate responses for 805 prompts, and GPT-4 is employed to compare\nthe responses with those from the reference model (gpt-4-1106-preview). Given the updates in the\nAlpacaEval repository, the default models have changed over time. Currently, the default setup uses the\nweighted_alpaca_eval_gpt4_turbo annotator as the annotator and gpt-4-1106-preview as the reference\nmodel. Therefore, our evaluation aligns with the current default configuration in the official AlpacaEval\nrepository.\nToxiGen\nFor ToxiGen, we focus on the prompts specifically designed to elicit toxic language (‘hateful’\nprompts). To reduce evaluation costs, we use a subset of 500 prompts per group for testing. The toxicity\nclassifier used is toxigen_roberta. We report the percentage of generations classified as toxic by the\nclassifier.\nTruthfulQA\nFor TruthfulQA, we use the generation setting with the default QA prompt format, in-\ncluding 6 in-context examples. The judge model for evaluating truthfulness and informativeness is\nallenai/truthfulqa-truth-judge-llama2-7B, which is adopted in the open-instruct evaluation\nsuite and OLMo’s evaluation. We report the percentage of responses deemed both informative and truthful.\nD\nDetailed Results of Ablation Study for Section 4.4.1\nIn ablation study, we report the average results across various tasks on V2 Validation Sets, V3 Validation\nSets, and Downstream tasks, with the specific tasks detailed in Section G.3. The complete results are\ndetailed in Table 4 and Table 5.\n2https://github.com/tatsu-lab/alpaca_eval\n15\n\n\nTable 4: The detailed results of ablation study (Part One). All models keep the same number of parameters and are\npretrained on Dolma v1_6-sample dataset (about 10B tokens).\nTransformer\nTransformer + ATM\nTransformer + ATL\nFANformer + Activation\nFANformer\nV2 Validation Sets\n4chan\nLoss\n2.68\n2.68\n2.66\n2.70\n2.66\nPPL\n14.60\n14.53\n14.36\n14.88\n14.34\nc4_100_domains\nLoss\n3.11\n3.11\n3.10\n3.12\n3.08\nPPL\n22.38\n22.52\n22.18\n22.63\n21.87\nc4_en\nLoss\n3.27\n3.28\n3.27\n3.29\n3.25\nPPL\n26.40\n26.54\n26.22\n26.78\n25.85\ngab\nLoss\n3.90\n3.90\n3.89\n3.91\n3.87\nPPL\n49.58\n49.64\n49.11\n50.05\n47.83\nice\nLoss\n3.20\n3.21\n3.19\n3.21\n3.17\nPPL\n24.59\n24.77\n24.25\n24.82\n23.93\nm2d2_s2orc\nLoss\n3.56\n3.57\n3.56\n3.59\n3.56\nPPL\n35.34\n35.47\n34.99\n36.24\n35.05\nm2d2_wiki\nLoss\n3.14\n3.14\n3.13\n3.15\n3.11\nPPL\n23.17\n23.13\n22.90\n23.29\n22.48\nmanosphere\nLoss\n3.47\n3.48\n3.46\n3.48\n3.45\nPPL\n32.21\n32.46\n31.85\n32.62\n31.48\nmc4_en\nLoss\n3.02\n3.02\n3.01\n3.03\n2.99\nPPL\n20.53\n20.52\n20.22\n20.76\n19.91\npile\nLoss\n2.76\n2.76\n2.74\n2.77\n2.73\nPPL\n15.84\n15.74\n15.53\n15.99\n15.30\nptb\nLoss\n3.68\n3.70\n3.64\n3.71\n3.66\nPPL\n39.68\n40.51\n38.23\n40.74\n38.75\ntwitterAEE\nLoss\n4.10\n4.10\n4.07\n4.11\n4.07\nPPL\n60.25\n60.18\n58.79\n61.10\n58.54\nwikitext_103\nLoss\n3.33\n3.33\n3.30\n3.35\n3.29\nPPL\n28.03\n28.07\n27.15\n28.48\n26.88\nAverage\nLoss\n3.33\n3.33\n3.31\n3.34\n3.30\nPPL\n30.20\n30.31\n29.68\n30.64\n29.40\nV3 Validation Sets\nc4_en\nLoss\n3.21\n3.21\n3.20\n3.22\n3.19\nPPL\n24.80\n24.86\n24.60\n25.04\n24.24\ndolma_books\nLoss\n3.56\n3.56\n3.54\n3.57\n3.52\nPPL\n34.98\n35.32\n34.43\n35.57\n33.96\ndolma_common-crawl\nLoss\n3.23\n3.24\n3.23\n3.24\n3.21\nPPL\n25.32\n25.42\n25.16\n25.47\n24.76\ndolma_pes2o\nLoss\n2.86\n2.85\n2.84\n2.86\n2.83\nPPL\n17.45\n17.35\n17.09\n17.53\n16.88\ndolma_reddit\nLoss\n3.44\n3.44\n3.43\n3.45\n3.42\nPPL\n31.13\n31.35\n30.94\n31.42\n30.54\ndolma_stack\nLoss\n1.42\n1.41\n1.40\n1.42\n1.39\nPPL\n4.13\n4.10\n4.06\n4.13\n4.01\ndolma_wiki\nLoss\n3.04\n3.04\n3.03\n3.04\n3.01\nPPL\n20.89\n20.84\n20.62\n20.97\n20.26\nice\nLoss\n3.19\n3.20\n3.18\n3.20\n3.17\nPPL\n24.41\n24.56\n24.09\n24.63\n23.75\nm2d2_s2orc\nLoss\n3.70\n3.70\n3.69\n3.70\n3.68\nPPL\n40.35\n40.61\n40.22\n40.56\n39.50\npile\nLoss\n2.74\n2.73\n2.72\n2.75\n2.70\nPPL\n15.44\n15.35\n15.16\n15.58\n14.92\nwikitext_103\nLoss\n3.34\n3.34\n3.31\n3.35\n3.30\nPPL\n28.21\n28.21\n27.33\n28.57\n27.03\nAverage\nLoss\n3.07\n3.07\n3.05\n3.07\n3.04\nPPL\n24.28\n24.36\n23.97\n24.50\n23.62\nDownstream Benchmarks\npiqa\nACC\n66.43\n66.54\n65.45\n66.10\n66.45\nhellaswag\nACC\n33.87\n33.84\n34.02\n33.75\n34.37\nwinogrande\nACC\n52.80\n51.62\n49.96\n48.78\n51.72\nopenbook_qa\nACC\n28.00\n28.20\n28.00\n28.20\n29.00\nsciq\nACC\n70.30\n72.10\n69.00\n67.20\n71.80\narc_easy\nACC\n45.44\n46.14\n47.19\n47.02\n45.61\ncopa\nACC\n62.00\n66.00\n65.00\n66.00\n66.00\nrte\nACC\n51.26\n52.35\n52.71\n48.74\n57.04\ncommitment_bank\nACC\n42.86\n41.07\n46.43\n53.57\n44.64\nmrpc\nACC\n81.05\n81.22\n81.22\n81.22\n81.47\nsst2\nACC\n50.11\n51.49\n49.08\n49.08\n59.11\nAverage\nACC\n53.10\n53.69\n53.46\n53.61\n55.19\n16\n\n\nTable 5: The detailed results of ablation study (Part Two). All models keep the same dimension and are pretrained\non Dolma v1_6-sample dataset (about 10B tokens).\nTransformer\nTransformer + ATM\nTransformer + ATL\nFANformer + Activation\nFANformer\nV2 Validation Sets\n4chan\nLoss\n2.68\n2.68\n2.67\n2.68\n2.66\nPPL\n14.60\n14.54\n14.43\n14.63\n14.29\nc4_100_domains\nLoss\n3.11\n3.11\n3.10\n3.11\n3.08\nPPL\n22.38\n22.43\n22.11\n22.49\n21.69\nc4_en\nLoss\n3.27\n3.28\n3.26\n3.28\n3.24\nPPL\n26.40\n26.51\n26.12\n26.54\n25.61\ngab\nLoss\n3.90\n3.90\n3.89\n3.91\n3.87\nPPL\n49.58\n49.41\n48.97\n50.11\n47.79\nice\nLoss\n3.20\n3.20\n3.19\n3.21\n3.17\nPPL\n24.59\n24.62\n24.22\n24.90\n23.69\nm2d2_s2orc\nLoss\n3.56\n3.58\n3.56\n3.58\n3.54\nPPL\n35.34\n35.73\n35.17\n35.78\n34.58\nm2d2_wiki\nLoss\n3.14\n3.14\n3.13\n3.14\n3.10\nPPL\n23.17\n23.04\n22.81\n23.10\n22.27\nmanosphere\nLoss\n3.47\n3.48\n3.46\n3.48\n3.45\nPPL\n32.21\n32.44\n31.78\n32.43\n31.36\nmc4_en\nLoss\n3.02\n3.02\n3.01\n3.03\n2.99\nPPL\n20.53\n20.51\n20.22\n20.61\n19.86\npile\nLoss\n2.76\n2.76\n2.74\n2.77\n2.72\nPPL\n15.84\n15.78\n15.54\n15.90\n15.24\nptb\nLoss\n3.68\n3.67\n3.67\n3.73\n3.63\nPPL\n39.68\n39.19\n39.15\n41.67\n37.82\ntwitterAEE\nLoss\n4.10\n4.10\n4.08\n4.11\n4.07\nPPL\n60.25\n60.19\n59.12\n60.97\n58.62\nwikitext_103\nLoss\n3.33\n3.33\n3.31\n3.34\n3.29\nPPL\n28.03\n27.96\n27.29\n28.22\n26.98\nAverage\nLoss\n3.33\n3.33\n3.31\n3.34\n3.29\nPPL\n30.20\n30.18\n29.76\n30.57\n29.22\nV3 Validation Sets\nc4_en\nLoss\n3.21\n3.21\n3.20\n3.21\n3.18\nPPL\n24.80\n24.78\n24.52\n24.82\n24.00\ndolma_books\nLoss\n3.56\n3.56\n3.54\n3.56\n3.52\nPPL\n34.98\n35.10\n34.41\n35.24\n33.64\ndolma_common-crawl\nLoss\n3.23\n3.23\n3.22\n3.23\n3.20\nPPL\n25.32\n25.25\n25.09\n25.35\n24.55\ndolma_pes2o\nLoss\n2.86\n2.85\n2.84\n2.86\n2.82\nPPL\n17.45\n17.37\n17.12\n17.44\n16.79\ndolma_reddit\nLoss\n3.44\n3.44\n3.43\n3.44\n3.41\nPPL\n31.13\n31.22\n30.83\n31.28\n30.31\ndolma_stack\nLoss\n1.42\n1.41\n1.40\n1.42\n1.39\nPPL\n4.13\n4.09\n4.07\n4.13\n4.02\ndolma_wiki\nLoss\n3.04\n3.03\n3.03\n3.04\n3.00\nPPL\n20.89\n20.78\n20.61\n20.88\n20.10\nice\nLoss\n3.19\n3.20\n3.18\n3.21\n3.16\nPPL\n24.41\n24.44\n24.04\n24.72\n23.55\nm2d2_s2orc\nLoss\n3.70\n3.70\n3.69\n3.70\n3.67\nPPL\n40.35\n40.50\n39.99\n40.56\n39.17\npile\nLoss\n2.74\n2.73\n2.72\n2.74\n2.70\nPPL\n15.44\n15.39\n15.17\n15.50\n14.87\nwikitext_103\nLoss\n3.34\n3.34\n3.31\n3.35\n3.30\nPPL\n28.21\n28.12\n27.46\n28.36\n27.12\nAverage\nLoss\n3.07\n3.06\n3.05\n3.07\n3.03\nPPL\n24.28\n24.28\n23.94\n24.39\n23.47\nDownstream Benchmarks\npiqa\nACC\n66.43\n65.13\n66.76\n66.38\n66.59\nhellaswag\nACC\n33.87\n33.96\n34.22\n33.92\n35.15\nwinogrande\nACC\n52.80\n51.62\n50.12\n51.07\n51.38\nopenbook_qa\nACC\n28.00\n28.00\n28.80\n28.60\n28.40\nsciq\nACC\n70.30\n70.90\n70.40\n70.20\n70.30\narc_easy\nACC\n45.44\n48.60\n47.02\n44.91\n48.95\ncopa\nACC\n62.00\n67.00\n67.00\n65.00\n69.00\nrte\nACC\n51.26\n51.99\n54.87\n54.51\n54.87\ncommitment_bank\nACC\n42.86\n32.14\n41.07\n37.50\n39.29\nmrpc\nACC\n81.05\n81.17\n80.59\n81.17\n81.11\nsst2\nACC\n50.11\n50.92\n55.73\n51.15\n60.55\nAverage\nACC\n53.10\n52.86\n54.23\n53.13\n54.88\n17\n\n\nE\nExtended results of Section 4.4.4\nThe training and testing performance metrics, including loss and accuracy, for case-based and rule-based\nreasoning are presented in Figure 10 and Figure 11, respectively.\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\nStep\n0.6\n0.8\n1.0\n1.2\n1.4\nLoss\nTransformer\nFANformer\n(a) Training loss on modular addition task\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nStep\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nLoss\nTransformer\nFANformer\n(b) Training loss of linear regression task\nFigure 10: Training loss of FAN and Transformer on case-based and rule-based reasoning.\nF\nDerivation of ATF(X) = Attention(FANLayer′(X))\nATF(X) = softmax\n\u0012QF K⊤\nF\n√dk\n\u0013\nVF\n(by definition of ATF via Eq. (4))\n= softmax\n\u0012(XF WQ)(XF WK)⊤\n√dk\n\u0013\n(XF WV )\n(substitute QF , KF , VF from Eq. (3))\n= Attention(XF )\n(matches standard attention: Attention(Z) = softmax\n\u0012ZWQ(ZWK)⊤\n√dk\n\u0013\nZWV )\n= Attention(FANLayer′(X))\n(since XF = FANLayer′(X) by Eq. (2))\nG\nComprehensive Experimental Details\nG.1\nDetailed training settings of FANformer\nWe train FANformer-1B using the ZeRO optimizer strategy (Rajbhandari et al., 2020) via PyTorch’s\nDDP framework (Li, 2018). Following OLMo (Groeneveld et al., 2024), we use a constant global batch\nsize of approximately 4M tokens (2048 instances, each with a sequence length of 2048 tokens). To\nimprove throughput, we employ PyTorch’s amp module with the bfloat16 format. We employ the AdamW\noptimizer (Loshchilov et al., 2019) for the model’s training process. The learning rate for all LLMs is\nset to 4.0e-4. We warm up the learning rate over 2000 steps ( 8B tokens) and then decay it in a cosine\nmanner from there down to a tenth of the peak learning rate over the remainder of training. We employ\nFlashAttention (Dao et al., 2022) to accelerate the model training and inference processes, leveraging its\nability to optimize memory usage and computational efficiency. The total GPU computational cost for\npre-training FANformer-1B amounts to approximately 4,700 GPU hours.\nG.2\nDetailed Setup for Section 4.2\nFor different model sizes in Figure 3, the hidden dimension, number of layers, and number of heads are\nlisted in Table 6.\n18\n\n\n0\n100\n200\n300\n400\n500\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Accuracy\nTransformer\nFANformer\n(a) Training accuracy on modular addition task\n0\n100\n200\n300\n400\n500\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\nTest Accuracy\nTransformer\nFANformer\n(b) Test accuracy of modular addition task\n0\n100\n200\n300\n400\n500\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Accuracy\nTransformer\nFANformer\n(c) Training accuracy on linear regression task\n0\n100\n200\n300\n400\n500\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\nTransformer\nFANformer\n(d) Test accuracy of linear regression task\nFigure 11: Accuracy of FAN and Transformer during training and testing on case-based and rule-based reasoning.\n19\n\n\nTable 6: Model size and setup used for FANformer in Section 4.2, where Transformers follows the setups of previous\nwork OLMo (Groeneveld et al., 2024).\nModel\nSize\nHidden Dim.\nNum Layers\nNum Heads\nWeight Tying\nFANformer-300M\n268M\n1024\n16\n16\nTrue\nFANformer-600M\n604M\n1536\n16\n16\nTrue\nFANformer-1B\n1.1B\n2048\n16\n16\nTrue\nFANformer-3B\n2.6B\n2560\n24\n20\nFalse\nFANformer-7B\n6.7B\n4096\n24\n32\nFalse\nG.3\nValidation Set And Downstream Tasks\nFollowing (Zhu et al., 2024), we use V2 Validation Sets, V3 Validation Sets, and Downstream tasks\nto evaluate our approach. The specific tasks included in V2 validation sets, V3 validation sets, and\ndownstream tasks are listed in Table 7.\nG.4\nDetailed Setup of Case-based and Rule-based Reasoning.\nFollowing the work (Hu et al., 2024), we focus on binary operations that take two numbers, a and b, as\ninputs. Denoting c as the target label, the constructed datasets are in the form of D = {((ai, bi), ci)} for\ntwo mathematical tasks: modular addition and linear regression. The two tasks are defined as follows:\n• Modular addition. The input to the model is “a + b =”, and the output is c, where c = (a + b)\nmod P. The values of a and b range from 0 to 112. The constant P is 113 here.\n• Linear regression. This task involves the model learning a linear regression function. The input is\ngiven by “(a, b) =”, and the output is c, where c = m · a + n · b + p. The values of a and b range\nfrom 0 to 99. The constants are set as m = 1, n = 2, and p = 3.\nLeave-Square-Out\nThe work (Hu et al., 2024) employs the Leave-Square-Out method to evaluate the\ngeneralization ability of the Transformer. In this approach, a square test set is created to isolate the test\nsamples from the training samples. For instance, consider the center of the square at (ak, bk) with a side\nlength of lk. The square test set is defined as Tk = {((ai, bi), ci) | ak −lk\n2 ≤ai ≤ak + lk\n2 , bk −lk\n2 ≤\nbi ≤bk + lk\n2 }, and all remaining samples from the training set. This division creates a \"hole\" in the center\nof the training set, which is more challenging for the model compared to a random split. Since there are\nno similar cases in the training set to aid the model in solving the problem, this method tests whether the\nmodel has truly learned the underlying rules. In the experiments of the work (Hu et al., 2024), they found\nthat Transformer-based models fail to generate correct answers for the test set in the \"hole\". Therefore,\nwe use this method to assess the generalization ability of FANformer.\nSettings\nWe finetune both the Transformer and FANformer models on each dataset for 500 epochs. The\nbatch size is set to 336, and the learning rate is initialized at 10−4. A warm-up ratio of 0.01 is used, and\nwe apply cosine decay to adjust the learning rate throughout the training process.\nDuring generation, we set the model temperature to 0.5 and sample 10 generations to evaluate the\naccuracy on each test point. The square center (ak, bk) is (50, 50) for linear regression and (56, 56) for\nmodular addition.\nFollowing the work (Hu et al., 2024), we apply the Leave-Square-Out method to each dataset. Specifi-\ncally, we extract a square comprising 441 samples (from a total of approximately 10,000 samples) with\na side length of 20 to form our test set, leaving the remainder as the training set. It is important to note\nthat, despite removing a small portion of training samples, we ensure that all tokens present in the dataset\nappear in the training set. This precaution is to prevent the models from failing simply due to encountering\nunseen tokens. We then proceed to finetune Transformer and FANformer models using this specific\ntraining-test split for each dataset.\n20\n\n\nTable 7: Validation Set And Downstream Tasks.\nV2 Validation Sets\nv2-small-4chan-validation\nv2-small-c4_100_domains-validation\nv2-small-c4_en-validation\nv2-small-gab-validation\nv2-small-ice-validation\nv2-small-m2d2_s2orc-validation\nv2-small-m2d2_wiki-validation\nv2-small-manosphere-validation\nv2-small-mc4_en-validation\nv2-small-pile-validation\nv2-small-ptb-validation\nv2-small-twitterAEE-validation\nv2-small-wikitext_103-validation\nV3 Validation Sets\nv3-small-c4_en-validation\nv3-small-dolma_books-validation\nv3-small-dolma_common_crawl-validation\nv3-small-dolma_pes2o-validation\nv3-small-dolma_reddit-validation\nv3-small-dolma_stack-validation\nv3-small-dolma_wiki-validation\nv3-small-ice-validation\nv3-small-m2d2_s2orc-validation\nv3-small-pile-validation\nv3-small-wikitext_103-validation\nDownstream Benchmarks\npiqa (Bisk et al., 2020)\nhellaswag (Zellers et al., 2019)\nwinogrande (Sakaguchi et al., 2020)\nopenbook_qa (Mihaylov et al., 2018)\nsciq (Welbl et al., 2017)\narc_easy (Clark et al., 2018)\ncopa (Roemmele et al., 2011)\ncommitment_bank (De Marneffe et al., 2019)\nmrpc (Dolan et al., 2005)\nrte (Dagan et al., 2005)\nsst2 (Socher et al., 2013)\n21\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21309v1.pdf",
    "total_pages": 21,
    "title": "FANformer: Improving Large Language Models Through Effective Periodicity Modeling",
    "authors": [
      "Yihong Dong",
      "Ge Li",
      "Xue Jiang",
      "Yongding Tao",
      "Kechi Zhang",
      "Hao Zhu",
      "Huanyu Liu",
      "Jiazheng Ding",
      "Jia Li",
      "Jinliang Deng",
      "Hong Mei"
    ],
    "abstract": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}