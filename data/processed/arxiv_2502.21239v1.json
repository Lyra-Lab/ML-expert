{
  "id": "arxiv_2502.21239v1",
  "text": "Semantic Volume: Quantifying and Detecting both\nExternal and Internal Uncertainty in LLMs\nXiaomin Li∗\nHarvard University\nZhou Yu\nAmazon\nZiji Zhang\nAmazon\nYingying Zhuang\nAmazon\nSwair Shah\nAmazon\nAnurag Beniwal\nAmazon\nAbstract\nLarge language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information, often\naccompanied by high uncertainty. Existing methods for hallucination detection\nprimarily focus on quantifying internal uncertainty, which arises from missing\nor conflicting knowledge within the model. However, hallucinations can also\nstem from external uncertainty, where ambiguous user queries lead to multiple\npossible interpretations. In this work, we introduce Semantic Volume, a novel\nmathematical measure for quantifying both external and internal uncertainty in\nLLMs. Our approach perturbs queries and responses, embeds them in a semantic\nspace, and computes the determinant of the Gram matrix of the embedding vectors,\ncapturing their dispersion as a measure of uncertainty. Our framework provides\na generalizable and unsupervised uncertainty detection method without requiring\nwhite-box access to LLMs. We conduct extensive experiments on both external and\ninternal uncertainty detection, demonstrating that our Semantic Volume method\nconsistently outperforms existing baselines in both tasks. Additionally, we provide\ntheoretical insights linking our measure to differential entropy, unifying and extend-\ning previous sampling-based uncertainty measures such as the semantic entropy.\nSemantic Volume is shown to be a robust and interpretable approach to improving\nthe reliability of LLMs by systematically detecting uncertainty in both user queries\nand model responses.\n1\nIntroduction\nLarge language models encode extensive knowledge from massive training data and have shown\nremarkable achievements on diverse tasks [10, 1, 41, 3, 6, 18, 5]. Despite their success, LLMs\nstill exhibit hallucination: generating information or conclusions that are incorrect, incomplete,\nfabricated, or misleading [21, 20, 8, 17, 11, 9]. These hallucinations can propagate false information,\nundermine decision-making, and damage the credibility of AI systems. Detecting the hallucination\nis a challenging task, and a growing stream of research leverages the uncertainty in LLMs for\nhallucination detection [27, 14, 13, 23, 35, 15, 25, 32, 33, 38]. Existing methods focus on internal\nuncertainty, which generally arises from missing relevant knowledge, conflicting information, or\noutdated data in the training corpus, and is assumed to reflect the model’s intrinsic limitations\n[27, 14, 13, 25]. Nonetheless, such internal confusion, and consequently hallucinations, can also\nstem from external uncertainty, which occurs when the user’s query is ambiguous, such as lacking\ncontext or having multiple possible interpretations due to typos, missing information, or ambiguous\n∗Work done during internship at Amazon. Correspondence to: Xiaomin Li (email: xiaominli@g.harvard.edu)\narXiv:2502.21239v1  [cs.CL]  28 Feb 2025\n\n\nentities [47, 36, 26, 24, 12, 29]. Cases of external uncertainty should be handled separately by\nrequesting clarification from the user [47, 36, 26, 29]. For example, when asked the ambiguous\nquestion “Who played Spiderman?\", an LLM should ask the user to specify which movie they are\nreferring to. It is important to note that internal uncertainty reflects true limitations of the model only\nafter external uncertainty has been ruled out. To detect external uncertainty (query ambiguity), current\nmethods often rely on LLMs themselves to assess ambiguity via specialized prompting strategies\n[26, 24, 12, 47]. In contrast, internal uncertainty (response uncertainty) detection follows two main\nparadigms: 1. Probability-based methods that utilize the token probabilities or entropy, requiring\ninternal access to the model [23, 35, 38, 22]. 2. Sampling-based approaches, which sample multiple\nresponses and propose measures to quantify uncertainty [13, 15, 46, 27, 14, 25, 32]. A representative\nmethod in this category is the Semantic Entropy, which clusters sampled responses into semantic\nequivalence classes and computes entropy across these clusters [27, 14].\nIn this work, we introduce a unified sampling-based approach called the Semantic Volume, which\ncan generally be applied to detect both internal and external uncertainty in LLMs (see Figure 1).\nOur method generates perturbations of queries and responses, obtains their semantic embedding\nvectors, and use a mathematical measure that essentially computes the determinant of the Gram\nmatrix formed by the vectors, in order to quantify the semantic dispersion. Larger dispersion indicates\nhigher uncertainty. More precisely, for external uncertainty, we prompt the LLM to generate multiple\naugmented versions of each query as perturbations, while for internal uncertainty, we sample multiple\nresponses as perturbations. Then we take the normalized embedding vectors to be their representations.\nPutting these vectors as column vectors in a matrix V , the value det(V ⊤V ) mathematically measures\nthe squared volume of the parallelepiped formed by these vectors. The log of this value essentially\ndefines our Semantic Volume (see 3.1). The idea is that if uncertainty is low, all perturbations should\nbe similar or close to each other, resulting in a small dispersion (i.e., a smaller volume).\nWe conduct comprehensive experiments on both query ambiguity detection and response uncertainty\ndetection, demonstrating that our semantic volume method outperforms various baselines in both\ntasks. Additionally, we provide theoretical justification showing that our measure essentially captures\nthe differential entropy of perturbation vectors, effectively quantifying overall semantic dispersion.\nNotably, semantic entropy emerges as a special case of our approach, highlighting the broader\ngeneralization of semantic volume over existing sampling-based methods. Our findings suggest\nthat semantic volume provides a robust, interpretable framework for improving LLM reliability by\nsystematically detecting and addressing uncertainty in both user queries and model responses. Below\nis a list of our main contributions:\n• We propose a novel mathematical measure for uncertainty detection, using the determinant\nof the Gram matrix (equivalently, parallelepiped volume) of embedding vectors to quantify\nsemantic dispersion.\n• Our method is training-free and black-box applicable (does not require white-box access\nto the model’s internal states or token probabilities).\n• To the best of our knowledge, this is the first framework to study both external and\ninternal uncertainty in LLMs.\n• We validate our approach through comprehensive experiments on both external and internal\nuncertainty detection, demonstrating superior performance compared to various baselines 2.\n• We provide theoretical interpretations of our semantic volume, linking it to differential\nentropy and generalizing existing sampling-based uncertainty measures.\n2\nRelated Work\n2.1\nHallucination\nLLM hallucinations occur when the model generates incorrect, incomplete, fabricated, or misleading\noutputs [21, 20, 8, 17, 11]. Generally the hallucination can be caused by the lack of knowledge\nof the LLM itself (internal uncertainty), but could also originate from the ambiguity in the user’s\nquery (external uncertainty). Most LLMs are not explicitly trained to handle ambiguous queries\n2Code of our experiments: https://anonymous.4open.science/r/LLMUncertainty-5C61.\n2\n\n\nFigure 1: Pipeline for external and internal uncertainty detection using semantic volume. Step 1.\nGenerate perturbations. For external uncertainty, we augment each query using an LLM, treating\nthe augmentations as perturbations. For internal uncertainty, perturbations refer to multiple sampled\ncandidate responses. Step 2. Compute semantic volume (essentially log det(V ⊤V ) where columns\nof V are normalized embedding vectors). Step 3. Cases with high semantic volume are predicted as\nambiguous queries (external uncertainty) or hallucinated responses (internal uncertainty).\nand often generate incorrect responses instead, leading to hallucinations [24]. Addressing external\nuncertainty requires a different approach, such as prompting the LLM to ask clarification questions\nbefore generating a response [47, 36, 26, 29]. In contrast, internal uncertainty caused by knowledge\ngaps can be mitigated through methods like retrieval-augmented generation for additional context\n[30], reasoning-based techniques to improve understanding [43, 18, 37], or simply turning to stronger\nLLMs or human agents.\n2.2\nExternal Uncertainty\nQuery ambiguity detection is typically performed using LLMs with various prompting techniques\n[26, 24, 36, 47, 12, 45, 29]. For instance, Kuhn et al. [26] use LLM prompting for both detecting\nambiguity and generating clarification questions. Kim et al. [24] prompts LLM to disambiguate\nquestion x itself, and then measure the difference between the xnew and x. Larger difference above\na threshold indicates ambiguity. Min et al. [36] introduced the AmbigNQ dataset, which contains\nambiguous queries and their answers. Building on this, Zhang et al. [47] proposed a taxonomy\nfor different types of query ambiguity and released the CLAMBER benchmark, which provides\nbinary labels indicating whether a query is ambiguous. They further evaluated various models on\nambiguity detection under different settings, including zero-shot vs. few-shot and chain-of-thought\n(CoT) prompting vs. standard prompting.\n2.3\nInternal Uncertainty\nThere is a growing stream of works proposing an uncertainty measures to detect hallucination\n[27, 14, 13, 23, 35, 15, 25, 32, 33, 38] (similar to all these studies, we focus on scenarios where the\nLLM exhibits high uncertainty when making mistakes, and cases where an LLM hallucinates with\nhigh confidence are beyond the scope of this paper). There are generally two genres: Probability-\nbased, using information such as the token probability or entropy, and Sampling-based, which\nsamples more responses and measure the dispersion of the answers.\nProbability-based. Last Token Entropy, which essentially uses the entropy of the vocabulary\ndistribution at the last token, is a widely used measure of uncertainty [35]. Log Probabilities\naverage the log of the conditional probabilities of the tokens (equivalently the log of the products of\nconsecutive token probabilities) [35, 38]. Quevedo et al. [38] tries multiple ways to aggregate the\ntoken probabilities, such as the minimal and averaged token probabilities.\n3\n\n\nSampled-based. Kuhn et al. [27] proposed Semantic Entropy to measure uncertainty in natural\nlanguage and Farquhar et al. [14] applied it to detect hallucinations in large language models.\nEssentially for each query, they generates multiple answers, and then clusters them by the same\nsemantic meanings. Then discrete entropy calculated from the sizes of different clusters is defined\nas the semantic entropy. In Kadavath et al. [23] and Cole et al. [13], they samples multiple answers\nand let LLM to judge the uncertainty based on these answers. this method is called p(True) in Kuhn\net al. [27]. The Lexical Similarity method [15, 16] considers the averaged similarity of the sampled\nanswers, and lower similarity indicates higher dispersion.\n3\nMethod\n3.1\nDefinitions and Notations:\nDenote [k]\ndef\n= {1, 2, . . . , k} for any k ∈N. For the task of external uncertainty detection, we\ndenote the query dataset by DQ = {qi}i∈[NQ], along with a tiny labeled subset LQ ⊆DQ (used to\ndetermine optimal semantic volume threshold and each query is assigned a binary label indicating\nwhether it is ambiguous). For internal uncertainty detection, we define the query-response dataset as\nDR = {(qi, ri)}i∈[NR] with a labeled subset LR ⊆DR. Since our method and analysis apply to both\ntasks, we often drop the subscript and use the general notation D = {si}i∈[N] and L ∈D, where si\nrepresents a query for external uncertainty and a query-response pair for internal uncertainty.\nVolume. Given normalized embedding vectors V = [v1v2 . . . vn] (each ∥vi∥= 1), we define the\nsquared volume as:\nVol2(V )\ndef\n= det(V ⊤V ).\n(1)\nThe term “volume” originates from the fact that geometrically,\np\ndet(V ⊤V ) represents the volume\nof the parallelepiped spanned by vectors {vi}. For example, in the three-dimensional case, where\nV = [v1 v2 v3], it can be verified that\np\ndet(V ⊤V ) precisely computes |v⊤\n1 (v2 × v3)|, which\ncorresponds to the volume of the three-dimensional parallelepiped formed by {v1, v2, v3}. A more\ndetailed discussion on the geometric interpretation of this measure is provided in Appendix B).\nSemantic Volume. A key limitation of equation 1 is that if duplicate embeddings exist in V (e.g., two\nsampled responses or extended queries are identical), then det(V ⊤V ) is immediately zero because\nV ⊤V is singular. To address this, we introduce a small perturbation ϵI with ϵ = 10−10 and compute\ndet(V ⊤V + ϵI) to maintain numerical stability (in Appendix D, we show that ϵ is negligible to the\nspectral norm ∥V ⊤V ∥and hence it only serves to ensure numerical stability and does not affect the\nquantification). In practice, the absolute values of squared volumes are often small due to the nature\nof our perturbations. Therefore we take the logarithm, leading to the formulation\nlog Vol2(V )\ndef\n= log det\n\u0000V ⊤V + ϵ In\n\u0001\n(2)\nMoreover, we apply Principal Component Analysis (PCA) to reduce dimensionality by projecting\nthe vectors onto the top d principal components, obtaining ˜V = [˜v1˜v2 . . . ˜vn] ∈Rd×n, where each\n˜vi\ndef\n= PP CAvi. Here, PP CA is the projection matrix. This results in the general form of our final\nsemantic volume measure:\nSemanticVolume(V )\ndef\n= log Vol2(PP CAV ) = log det\n\u0010\n˜V ⊤˜V + ϵ In\n\u0011\n(3)\n3.2\nSemantic Volume Uncertainty Detection Algorithm:\nOur algorithm using semantic volume to detect high uncertainty is outlined below (the overall pipeline\nis illustrated in Figure 1 and the detailed pseudocode is provided in Algorithm 1).\nStep 1: Augmentation. For each s ∈D, we augment it with n perturbations. Precisely, for external\nuncertainty, we prompt an LLM to augment/paraphrase each query s to obtain n perturbed versions,\nwhile for internal uncertainty, we sample n candidate responses.\nStep 2: Compute semantic volume. We obtain embedding vectors using Sentence-Transformer\n[39], normalize them, and apply PCA dimension reduction, yielding ˜V = [˜v1˜v2 . . . ˜vn] ∈Rd×n for\nn perturbations. Then compute the semantic volume according to equation 3.\n4\n\n\nStep 3: Uncertainty detection. A higher semantic volume indicates greater uncertainty. To determine\nthe optimal threshold τ ∗, we use a tiny labeled random subset L ⊆D with size 100 for threshold\ntuning 3 (the exact formula for τ ∗is characterized in Proposition 1). Finally, we classify the entire\ndataset D by assigning binary uncertainty labels based on the semantic volume threshold.\n4\nExperiment: External Uncertainty\n4.1\nExperimental Setup\nData. We use the CLAMBER dataset[47], a benchmark for evaluating LLM queries using a well-\norganized taxonomy. It is a balanced dataset containing 3K queries, each annotated with a binary\nlabel indicating whether it is ambiguous.\nModels. For query augmentation, we use Claude3.5-Sonnet [6] and the prompt is provided in Ap-\npendix J. Qwen2-1.5B-instruct [44] is used as the sentence-transformer to generate embeddings.\nEvaluation. We conduct experiments for binary classification task on ambiguity of the queries. The\nperformance is assessed by comparing the predicted binary labels against the ground truth labels,\nreporting both accuracy and F1 score.\nBaselines. Note that the sampling-based methods discussed in Section 2.3 can be naturally extended\nto query ambiguity detection if we can generate analogous perturbations of the queries, similar to how\ncandidate responses are sampled in response uncertainty detection. Below, we outline the baseline\nmethods we consider. Some of these were originally designed for response uncertainty, but we readily\nadapt their methodology to query ambiguity.\n• Type 1: Prompting-based. Directly prompt LLMs to determine whether a given query\nis ambiguous. We evaluate the following models: Vicuna-13B [34], Llama2-13B [42],\nLlama2-70B [42], Llama3.2-3B [2], and ChatGPT [1]. Additionally, we include results for\nChatGPT with few-shot learning and chain-of-thought reasoning, as reported in the original\nCLAMBER paper [47].\n• Type 2: Probability-based. Using the token probabilities to quantify uncertainty. Note that\nthese methods typically requires white-box models for access to the internal probabilities.\nWe consider the follow methods and use Llama3.2-1B-Instruct [2] to obtain the token\nprobabilities. (a) Last Token Entropy [23, 7, 35]: computes the entropy of the vocabulary\ndistributions at the last token of the query. (b) Log Probabilities [35, 38]: measures\nuncertainty by computing the log of the product of conditional token probabilities, which is\nequivalent to summing the log conditional probabilities across all tokens in the query.\n• Type 3: Sampling-based. These methods employ a perturbation approach similar to ours.\nThe original text is perturbed to generate multiple variations, and the uncertainty is quantified\nby measuring the variation among the generated samples. (a) p(True) [23, 13]: the original\np(True) method quantifies uncertainty based on the probability of the LLM’s output. Instead,\nwe directly use the LLM’s answer and compare it against the ground truth binary labels. (b)\nLexical Similarity [32, 15]: computes the averaged pairwise similarity of perturbed queries.\n(c) Semantic Entropy [27]: clusters the perturbations into semantic equivalence classes and\ncomputes the entropy over the clusters.\n4.2\nResults\nThe performance of our method and baseline approaches on the query ambiguity classification task is\npresented in Table 1. Here we choose n = 20 for the augmentations for queries (the discussion on\nvarying the perturbation size n is provided in Appendix F). The original CLAMBER dataset includes\na diverse range of ambiguities, such as queries involving unfamiliar entities, self-contradictions,\nmultiple meanings, and missing context (e.g., personal, temporal, spatial, or task-specific). We\nobserve that identifying ambiguous queries remains challenging for LLMs, even for powerful models\nlike ChatGPT, despite various prompting strategies (few-shot and CoT). Among the baselines,\nprobability-based methods achieve higher F1 scores but a critical limitation of them is that they\n3Note that this labeled subset is only used for finding a more precise threshold. In practice, one can consider\na completely unsupervised setting and use a simpler heuristic threshold such as the median.\n5\n\n\nrequire access to token probabilities, which is not provided for most of the close-sourced models.\nAmong sampling-based methods, semantic entropy generally performs better. Nonetheless, our\nsemantic volume method significantly outperforms all three categories of baselines, demonstrating its\neffectiveness in detecting ambiguous queries.\nMethod\nAccuracy\nF1\nVicuna-13B (zero-shot)\n50.6\n39.9\nLlama2-13B (zero-shot)\n45.6\n43.6\nLlama2-70B (zero-shot)\n50.3\n34.2\nLlama3.2-3B (zero-shot)\n51.5\n37.7\nChatGPT (zero-shot)\n54.3\n53.4\nChatGPT (few-shot)\n51.6\n49.2\nChatGPT (zero-shot + CoT)\n57.3\n56.9\nChatGPT (few-shot + CoT)\n53.6\n51.4\nLast Token Entropy\n52.2\n67.3\nLog Probabilities\n45.5\n66.0\npTrue (Llama3-8B)\n52.5\n53.3\npTrue (Mistral-7B)\n47.1\n26.6\nLexical Similarity\n52.7\n53.6\nSemantic Entropy\n50.1\n62.8\nSemantic Volume (ours)\n58.0\n69.0\nTable 1: External Uncertainty: Performance comparison based on Accuracy and F1 score.\n5\nExperiment: Internal Uncertainty\n5.1\nExperimental Setup\nData. We use a subset of the TriviaQA dataset[47], a reading comprehension dataset containing\nquestions with reference answers. We generate responses using Llama3.2-1B-Instruct at zero\ntemperature and a response y is flagged as a hallucination if the ROUGE-L score with respect to the\nreference answer yref falls below 0.3 (i.e. the label is defined as 1RougeL(y,yref )<0.3), following the\nsame metric used in Kossen et al. [25]. This process yields the hallucination labels for the dataset.\nWe retain 2500 data labeled as hallucinations and 2500 labeled as correct, constructing a balanced 5K\ndataset with binary hallucination labels.\nModels. For sampling-based methods, we generate candidate responses from the same Llama3.2-\n1B-Instruct with temperature 1. For embeddings, we use the same sentence-transformer as in\nSection 4.\nEvaluation. We compare the predicted binary hallucination labels against the ground truth labels and\nreport both accuracy and F1 score. Furthermore, we also add the evaluation based on the AUROC\n(area under the receiver operator characteristic curve) metric, which compares the raw uncertainty\nscores against the ground truth labels. In fact, for a given uncertainty measure m(·), the AUROC score\nis equivalent to P[m(yhallucinated) > m(ycorrect)], where yhallucinated and ycorrect are randomly\nchosen hallucinated and correct answers, respectively. Hence a higher AUROC (closer to 1) indicates\nthat the uncertainty measure more effectively distinguishes hallucinated responses by assigning them\nhigher uncertainty scores. AUROC is a widely used metric in many existing studies on response\nhallucination detection [27, 25, 23].\nBaselines. We adapt the baseline methods from Section 4.1, applying them to sampled responses\ninstead of augmented queries.\n5.2\nResults\nThe performance results are presented in Tables 2 and 3. For sampling-based methods, we set the\nresponse sampling size to n = 20. Notably, our semantic volume method significantly outperforms\nall baselines in both accuracy and F1 score. Furthermore, the AUROC results in Table 3 confirm\nthat our semantic volume serves as a highly effective uncertainty signal for hallucination detection.\nAdditionally, we observe that when comparing p(True), which includes sampled candidate responses\n6\n\n\nas context, to direct prompting, the inclusion of context degrades the performance. Moreover, for\nmethods that rely on prompting LLMs (including pTrue), we find that LLMs sometimes exhibit a\nstrong bias toward answering almost all “Yes” or all “No”. In fact in Table 2, both Prompt Llama3.2-\n1B and pTrue (Llama3.2-1B) exhibit this behavior, nearly predicting all response as hallucinations\n(further discussions are provided in Appendix H). This instability highlights another drawback of\nsuch methods that rely on LLM prompting for uncertainty estimation. From both tables, we observe\nthat sampling-based methods that measure the dispersion of sampled responses (particularly lexical\nsimilarity and semantic entropy) generally outperform probability-based methods, which aligns with\nthe findings in Cole et al. [13].\nMethod\nAccuracy\nF1\nPrompt Llama3.2-1B\n50.5\n66.8\nPrompt Llama3-8B\n65.5\n60.1\nPrompt Mistral-7B\n68.7\n61.8\nLast Token Entropy\n60.1\n59.9\nLog Probabilities\n60.1\n62.9\npTrue (Llama3.2-1B)\n49.6\n64.0\npTrue (Llama3-8B)\n63.7\n45.4\npTrue (Mistral-7B)\n62.9\n45.8\nLexical Similarity\n64.2\n72.0\nSemantic Entropy\n63.5\n69.4\nSemantic Volume (ours)\n72.8\n75.4\nTable 2: Internal Uncertainty: Performance compar-\nison based on Accuracy and F1 score.\nMethod\nAUROC\nLast Token Entropy\n63.9\nLog Probabilities\n65.5\npTrue (Llama3.2-1B)\n61.3\npTrue (Llama3-8B)\n58.3\npTrue (Mistral-7B)\n65.4\nLexical Similarity\n73.4\nSemanticEntropy\n73.4\nSemantic Volume (ours)\n79.6\nTable 3: Internal Uncertainty: Performance\ncomparison based on AUROC.\n5.3\nDistribution Separation\nIn this section, we compare the distributions of various uncertainty measures using both visualization\nand the Kolmogorov–Smirnov (KS) test [4, 40]. Specifically, we plot histograms for the hallucinated\nsubset (hallucination label = 1) and the correct subset (hallucination label = 0) from our TriviaQA\ndataset. Ideally, a well-performing measure should yield two distinct bulks, with greater separation\nindicating stronger discriminative power. To quantitatively assess the separation, we perform a two-\nsample Kolmogorov–Smirnov test, a non-parametric test that compares two empirical distributions by\nmeasuring the maximum distance between their empirical cumulative distribution functions (ECDFs).\nA large KS statistic combined with a small p-value suggests that the two distributions are significantly\ndifferent.\nThe plots and statistics are shown in Figure 2. Combined the KS statistics and the histograms, we\nobserve that indeed Last Token Entropy and Log Probabilities struggle to effectively separate the two\ngroups of data, while Lexical Similarity, Semantic Entropy, and our Semantic Volume exhibit stronger\nseparation. Particularly, we note that the distribution of semantic entropy closely resembles that of\nour semantic volume measure. In fact, we will provide theoretical analysis in Section 7 showing that\nour semantic volume can be interpreted as the differential entropy of the semantic embedding vectors,\nand can be viewed as a more general and continuous version of semantic entropy.\n6\nAblation Study and Hyperparameter Analysis\nThe original embedding dimension from the sentence-transformer is 1536. In the external uncertainty\nexperiment, we reduce the dimensionality using PCA with d = 10, while in the internal uncertainty\nexperiment, we set d = 20. To assess the impact of this reduction, we conduct an ablation study,\ndemonstrating that using the projected vectors ˜V leads to better performance than the original raw\nembeddings V (see Appendix E). This suggests that lower-dimensional projections help separate\nperturbation vectors more effectively. Furthermore, we explore various values of d to analyze the\neffect of dimensionality on performance. Our findings indicate that the optimal dimension d is\ntask-dependent, with diminishing improvements beyond a certain point. Nonetheless, it is important\nto note that even without PCA dimension reduction, our method still outperforms various baselines in\nboth external and internal uncertainty detection tasks.\n7\n\n\n(a) Last Token Entropy\n(KS Statistic 0.238)\n(b) Log Probabilities\n(KS Statistic 0.226)\n(c) Lexical Similarity\n(KS Statistic 0.428)\n(d) Semantic Entropy\n(KS Statistic: 0.376)\n(e) Semantic Volume\n(KS Statistic 0.440)\nFigure 2: Distribution of subsets for both labels across different uncertainty measures. Two-sample\nKolmogorov–Smirnov statistics are computed to quantify the separation of two bulks (all p-values of\nthem are less than 10−11).\nTo demonstrate the generalizability of our method and study the effect of various hyperparameters\nand model choices. In Appendix F, we analyze the effect of varying n, the number of perturbations.\nAs expected, larger n yields more accurate uncertainty estimation but increases computational cost,\nwhile smaller n reduces cost but may sacrifice accuracy. We choose n = 20 to balance performance\nand efficiency. In Appendices G and H, we examine the impact of different embedding models\nand response generation models. We find that larger embedding models provide little additional\nperformance gain. Furthermore, when detecting hallucinations in responses generated by a larger\nLLM, the performance of most methods slightly declines. However, our Semantic Volume method\nstill outperforms all baselines.\n7\nTheoretical Analysis\nIn this section, we present theoretical analyses, with proofs and additional supporting lemmas provided\nin Appendix C. First, we derive the exact formula for the optimal threshold τ ∗in Proposition 1. Then\nin Theorem 1 and Theorem 2, we show that under the assumption that the perturbations follow a\nGaussian distribution, our semantic volume measure effectively computes the differential entropy of\nthe embedding vectors, using entropy as a measure of uncertainty detection. This insight allows us to\nnaturally view our method as a generalization of semantic entropy [27]. Notably, semantic entropy\ninvolves a manual clustering step, and only considers the entropy between clusters while ignoring\ndiscrepancies within clusters (since the responses in the same cluster are semantically similar but\nnot exactly identical). In contrast, our method more generally captures the overall semantic\ndispersion across all sampled perturbations, providing a more comprehensive uncertainty measure.\nProposition 1 (Formula for optimal threshold τ ∗). Denote L ∈D as a labeled subset with inputs\n{si} and labels {yi}:\nL = {(si, yi)}M\ni=1\nwith\nyi ∈{0, 1},\nFor each si, denotes its semantic volume as m(si). Define a classification rule\nˆyi(τ) =\n\u001a1,\nm(si) > τ,\n0,\notherwise.\nThen the optimal threshold τ that maximizes the F1 score on L is given by\nτ ∗def\n= arg max\nτ∈R F1(τ) = arg max\nτ∈R\n \n2 PM\ni=1 1ˆyi(τ)=yi=1\nPM\ni=1 1ˆyi(τ)=1 + 1yi=1\n!\n.\n(4)\n8\n\n\nNote that the F1 score can be replaced by other metrics, such as the accuracy.\nTheorem 1. Denote the embedding vector and normalized embedding vector of the original text\n(either a query or a response) as ¯x and ¯v, respectively. Denote the perturbation embeddings as\nX\ndef\n= [x1x2 . . . xn] ∈Rd×n and the normalized perturbation embeddings as V\ndef\n= [v1v2 . . . vn] ∈\nRd×n (i.e. xi = vi/∥vi∥for each i ∈[n]). Assume Gaussian distribution xi ∼N(¯x, Σ). Then\nin high-dimensional regime, log det(V ⊤V ) corresponds to the shifted differential entropy of the\nperturbations {xi}k∈[n]. That is,\nlog det(V ⊤V ) ˙= H(X) + C,\nwhere H(X)\ndef\n= −Ex∼X [log pX(x)] is the differential entropy and C is a constant offset term.\nThen we obtain the following Theorem, which is a direct consequence of Theorem 1 and Lemma 1 in\nAppendix C.\nTheorem 2. Under the same setting and notations of the Theorem 1, our Semantic Volume method\nessentially generates same binary decisions compared to using differential entropy of the perturbation\nembedding vectors. That is, denote the Semantic Volume measure and differential entropy measure as\nm(·) and ˜m(·) respectively. For the labels\nyi\ndef\n= 1m(si)<τ ∗\nand\n˜yi\ndef\n= 1 ˜m(si))<˜τ ∗,\nwhere ˜τ ∗is the optimal threshold for the differential entropy measure, we have\n˜yi = yi\nfor all si ∈D \\ L.\n8\nConclusion\nOne limitation of our current study is that our work considers the same scope as the references in 2.3:\nwe focus on the situation where uncertainty aligns with incorrectness, without addressing confidently\nwrong responses, meaning an LLM hallucinates with high confidence (low uncertainty). We believe\nthat addressing such cases requires different strategies, such as factuality checking, detecting self-\ncontradictions in chain-of-thought reasoning or incorporating external knowledge for verification.\nWe leave these explorations for future work.\nIn summary, we have introduced Semantic Volume, a novel and general-purpose measure for detecting\nboth external uncertainty (query ambiguity) and internal uncertainty (response uncertainty) in large\nlanguage models. By generating perturbations, embedding these perturbations as normalized vectors,\nand computing the determinant of their Gram matrix, we obtain a measure that captures the overall\nsemantic dispersion. Extensive experiments on benchmark datasets showed that semantic volume\nsignificantly outperforms various types of existing baselines (prompting-based, probability-based,\nand sampling-based) for both ambiguous query classification and response hallucination detection.\nFurthermore, from a theoretical standpoint, we established that semantic volume can be viewed as\nthe differential entropy of the embedding vectors, thereby unifying and extending prior sampling-\nbased metrics (e.g., semantic entropy). This interpretation highlights why our measure is robust and\ncomprehensive: unlike purely clustering-based approaches, we account for the overall dispersions in\nthe embedding space. Moreover, our method is applicable even when only black-box access to the\nLLM is available, making it broadly practical across closed-source or API-based models. Overall,\nour findings suggest that semantic volume is a promising step toward more reliable, interpretable\nuncertainty detection for both external and internal uncertainty of LLMs.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[2] Meta\nAI.\nLlama\n3.2:\nAdvancing\nai\nfor\nvision,\nedge,\nand\nmobile\nde-\nvices.\nMeta\nAI\nBlog,\n2024.\nURL\nhttps://ai.meta.com/blog/\nllama-3-2-connect-2024-vision-edge-mobile-devices/.\nAccessed:\nFeb 11,\n2025.\n9\n\n\n[3] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\n[4] Kolmogorov An. Sulla determinazione empirica di una legge didistribuzione. Giorn Dell’inst\nItal Degli Att, 4:89–91, 1933.\n[5] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: A family of\nhighly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[6] Anthropic. Introducing the Claude 3 family of models, 2023. URL https://www.anthropic.\ncom/news/claude-3-family. Accessed: 2025-01-02.\n[7] Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect\nthem. arXiv preprint arXiv:2109.06827, 2021.\n[8] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Love-\nnia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation\nof chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023,\n2023.\n[9] Alexander Bastounis, Paolo Campodonico, Mihaela van der Schaar, Ben Adcock, and Anders C\nHansen. On the consistent reasoning paradox of intelligence and optimal trust in ai: The power\nof’i don’t know’. arXiv preprint arXiv:2408.02357, 2024.\n[10] Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[11] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness\nin abstractive summarization. Advances in Neural Information Processing Systems, 35:24516–\n24528, 2022.\n[12] Yizhou Chi, Jessy Lin, Kevin Lin, and Dan Klein. Clarinet: Augmenting language models to\nask clarification questions for retrieval. arXiv preprint arXiv:2405.15784, 2024.\n[13] Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhin-\ngra, and Jacob Eisenstein.\nSelectively answering ambiguous questions.\narXiv preprint\narXiv:2305.14613, 2023.\n[14] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in\nlarge language models using semantic entropy. Nature, 630(8017):625–630, 2024.\n[15] Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark\nFishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation\nfor neural machine translation. Transactions of the Association for Computational Linguistics,\n8:539–555, 2020.\n[16] Yashvir S Grewal, Edwin V Bonilla, and Thang D Bui. Improving uncertainty quantification in\nlarge language models via semantic embeddings. arXiv preprint arXiv:2410.22685, 2024.\n[17] Nuno M Guerreiro, Duarte M Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre\nColombo, and André FT Martins. Hallucinations in large multilingual translation models.\nTransactions of the Association for Computational Linguistics, 11:1500–1517, 2023.\n[18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[19] David A Harville. Matrix algebra from a statistician’s perspective, 1998.\n[20] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qian-\nglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in\nlarge language models: Principles, taxonomy, challenges, and open questions. arXiv preprint\narXiv:2311.05232, 2023.\n10\n\n\n[21] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.\nACM Computing Surveys, 55(12):1–38, 2023.\n[22] Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, and\nPascale Fung. Llm internal states reveal hallucination risk faced with a query. arXiv preprint\narXiv:2407.03282, 2024.\n[23] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,\nNicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language\nmodels (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.\n[24] Hyuhng Joon Kim, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang Min\nYoo, Sang-goo Lee, and Taeuk Kim. Aligning language models to explicitly handle ambiguity.\narXiv preprint arXiv:2404.11972, 2024.\n[25] Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal.\nSemantic entropy probes: Robust and cheap hallucination detection in llms. arXiv preprint\narXiv:2406.15927, 2024.\n[26] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Clam: Selective clarification for ambiguous\nquestions with generative language models. arXiv preprint arXiv:2212.07769, 2022.\n[27] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances\nfor uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664,\n2023.\n[28] Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Founda-\ntions and Trends® in Machine Learning, 5(2–3):123–286, 2012.\n[29] Dongryeol Lee, Segwang Kim, Minwoo Lee, Hwanhee Lee, Joonsuk Park, Sang-Woo Lee, and\nKyomin Jung. Asking clarification questions to handle ambiguity in open-domain qa. arXiv\npreprint arXiv:2305.13808, 2023.\n[30] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. Advances in Neural Information Processing\nSystems, 33:9459–9474, 2020.\n[31] Xiaomin Li, Mingye Gao, Zhiwei Zhang, Chang Yue, and Hong Hu. Rule-based data selection\nfor large language models. arXiv preprint arXiv:2410.04715, 2024.\n[32] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty\nquantification for black-box large language models. arXiv preprint arXiv:2305.19187, 2023.\n[33] Linyu Liu, Yu Pan, Xiaocheng Li, and Guanting Chen. Uncertainty estimation and quantification\nfor llms: A simple supervised approach. arXiv preprint arXiv:2404.15993, 2024.\n[34] LMSys. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality. LMSys\nBlog, 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. Accessed: Feb 11,\n2025.\n[35] Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction.\narXiv preprint arXiv:2002.07650, 2020.\n[36] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering\nambiguous open-domain questions. arXiv preprint arXiv:2004.10645, 2020.\n[37] OpenAI. Learning to reason with llms. September 2024. https://openai.com/index/\nlearning-to-reason-with-llms/.\n[38] Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, and Tomas Cerny. Detecting\nhallucinations in large language model generation: A token probability approach. arXiv preprint\narXiv:2405.19648, 2024.\n11\n\n\n[39] N Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint\narXiv:1908.10084, 2019.\n[40] Nickolay Smirnov. Table for estimating the goodness of fit of empirical distributions. The\nannals of mathematical statistics, 19(2):279–281, 1948.\n[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Siddhartha Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL\nhttps://arxiv.org/abs/2307.09288.\n[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems, 35:24824–24837, 2022.\n[44] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong\nTang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu,\nJingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin\nYang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,\nRunji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin\nGe, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng\nRen, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,\nZeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL\nhttps://arxiv.org/abs/2407.10671.\n[45] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do\nlarge language models know what they don’t know? arXiv preprint arXiv:2305.18153, 2023.\n[46] Michael JQ Zhang and Eunsol Choi. Clarify when necessary: Resolving ambiguity through\ninteraction with lms. arXiv preprint arXiv:2311.09469, 2023.\n[47] Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan\nJin, Hongru Liang, and Tat-Seng Chua. Clamber: A benchmark of identifying and clarifying\nambiguous information needs in large language models. arXiv preprint arXiv:2405.12063,\n2024.\n12\n\n\nA\nSemantic-Volume Uncertainty Detection Algorithm\nHere we provide the pseudocode of the Semantic-Volume uncertainty detection algorithm.\nAlgorithm 1 Uncertainty Detection via Semantic-Volume\nRequire:\n1:\n• Dataset D = {si}N\ni=1 of queries or query-response pairs.\n• A small labeled subset L ⊆D with binary labels.\n• SentenceTransformer model M.\n2: for each string s ∈D do\n3:\nExtend s via query extension/response sampling to obtain: [s1, s2, . . . , sN]\n4:\nGet normalized embedding vectors: V = [v1v2 . . . vN]\ndef\n=\nh\nM(s1)\n∥Ms1∥, M(s2)\n∥Ms2∥, . . . , M(sN)\n∥MsN∥\ni\n5:\nApply PCA dimension reduction: ˜V\ndef\n= PP CAV .\n6:\nCompute Semantic Volume:\nm(s) ←log Vol(PP CAV )\ndef\n= log det\n\u0010\n˜V ⊤˜V + ϵ In\n\u0011\n7: end for\n8: Threshold Tuning: Using the labeled set L, find the threshold τ ∗that maximizes the F1 score:\nτ ∗←arg max\nτ∈R F1(τ)\ndef\n= arg max\nτ∈R\n \n2 P|L|\ni=1 1ˆyi(τ)=yi=1\nP|L|\ni=1 1ˆyi(τ)=1 + 1yi=1\n!\n.\n9: for each (s, m(s)) do\n10:\nPredict s with:\nˆys ←\n\u001a1,\nif m(s) > τ ∗,\n0,\notherwise.\n11: end for\n12: return The fully labeled dataset: {(s, ˆys) : s ∈D}.\nTime Complexity\nMost of the computational cost in our algorithm comes from perturbation sampling, similar to other\nsampling-based methods. Using the standard LLM implementation to generate multiple candidate\nsequences, the cost is equivalent to one LLM inference per data point. Embedding generation is\nhighly efficient, taking approximately 5 minutes for our CLAMBER-3K dataset and 3 minutes\nfor the TriviaQA-5K dataset on a single H100-80GB GPU. The computation of Semantic Volume,\nwhich involves matrix multiplication and determinant calculation for each set of perturbations, is\ncomputationally negligible (typically within seconds for the entire dataset), compared to the sampling\nstep. This results in an overall complexity of O(n) for our algorithm.\nB\nGeometric interpretation as volume\nWe illustrate why det(V ⊤V ) represents the squared volume using n = 2 and n = 3 as examples\n(see Figure 3). For n = 2, where V = [v1 v2], the volume of the parallelepiped (equivalently, the\narea of the parallelogram) is simply sin(θ), where θ is the angle between v1 and v2. In this case,\nV =\n\u0014\nv⊤\n1 v1\nv⊤\n1 v2\nv⊤\n2 v1\nv⊤\n2 v2\n\u0015\n=\n\u0014\n1\ncos θ\ncos θ\n1\n\u0015\n.\nTherefore,\np\ndet(V ⊤V ) =\n√\n1 −cos2 θ = sin θ. Similarly, for V = [v1 v2 v3], the\np\ndet(V ⊤V )\nexactly computes |v⊤\n1 (v2 × v3)|, which is the volume of the parallelepiped.\nBeyond its geometric interpretation, det(V ⊤V ) also quantifies the orthogonality of the vectors and\nis closely related to the determinantal point process (DPP) [28]. DPPs favor diverse or orthogonal\nsubsets by assigning higher probabilities to sets with dissimilar elements, using determinants to\n13\n\n\nFigure 3: Parallelogram for n = 2 (left) and parallelepiped for n = 3 (right)\nmodel repulsion among points. The Gram matrix V ⊤V is commonly used as the kernel matrix in\nDPPs, where the determinant of its submatrices determines subset probabilities. For a more detailed\ndiscussion, see Li et al. [31] and Kulesza et al. [28]. Moreover, some prior work has considered a\nsimilar volume measure to quantify vector diversity and orthogonality in LLM topics, such as the\nsecond orthogonality measure in [31], which motivates our design of semantic volume.\nC\nProofs\nC.1\nProof of Proposition 1\nProof. The formula equation 4 directly follows from the definition of F1 scores. Recall we have\nPrecision(τ) =\nTP(τ)\nTP(τ) + FP(τ),\nRecall(τ) =\nTP(τ)\nTP(τ) + FN(τ),\nwhere TP(τ)\n=\nPM\ni=1 1ˆyi(τ)=1∧yi=1,\nFP(τ)\n=\nPM\ni=1 1ˆyi(τ)=1∧yi=0,\nand FN(τ)\n=\nPM\ni=1 1ˆyi(τ)=0∧yi=1. F1 score is the harmonic mean of Precision and Recall:\nF1(τ) = 2 · Precision(τ) × Recall(τ)\nPrecision(τ) + Recall(τ) =\n2 · TP(τ)\n2 · TP(τ) + FP(τ) + FN(τ)\n=\n2 PM\ni=1 1ˆyi(τ)=yi=1\n2 PM\ni=1 1ˆyi(τ)=1∧yi=1 + PM\ni=1 1ˆyi(τ)=1∧yi=0 + PM\ni=1 1ˆyi(τ)=0∧yi=1\n=\n2 PM\ni=1 1ˆyi(τ)=yi=1\nPM\ni=1(1ˆyi(τ)=1 + 1yi=1)\n.\nC.2\nProof of Theorem 1\nFirst, in Lemma 1, we formally justify that our semantic volume measure for uncertainty detection is\ninvariant under linear transformations applied to the uncertainty measure. This invariance follows\ndirectly from the step of searching for optimal threshold τ ∗in our method.\nLemma 1 (Invariance of semantic volume method under linear transformation). Our method is\ninvariant under linear transformation on the semantic volume measure m(·). More precisely, denote\nτ ∗as the decision boundary defined in equation 4, we label\nyi\ndef\n= 1m(si)<τ ∗.\nGiven any linear transformation T(mi)\ndef\n= αmi + β (α > 0) applied to the measure m(·), denote\nthe new labels of our method under the new measure T(m(·)) as\n˜yi\ndef\n= 1T (m(si))<˜τ ∗.\nThen we have\n˜yi = yi\nfor all si ∈D \\ L.\n14\n\n\nProof. If we transform mi to ˜mi = T(mi), the new threshold ˜τ ∗that maximizes the same metric\nstill yields the same partition of the labeled samples. In other words,\nmi > τ ∗⇐⇒T(mi) = ˜mi > T(τ ∗),\na result of the simple fact that mi > τ ∗⇐⇒α(mi) + β > α(τ ∗) + β. Thus our new threshold ˜τ ∗\nis effectively T(τ ∗). Note that here if α < 0 in T(mi)\ndef\n= αmi + β, then naturally we should flip the\ndecisions of our method and then the labels still remain the same.\nLemma 2 below provides the formula for differential entropy of Gaussian vectors. This is a known\nresult but we include this lemma and provide a proof here for completeness.\nLemma 2 (Differential entropy of Gaussian vectors). For X ∈Rd that follows a multivariate normal\ndistribution\nX ∼N(µ, Σ).\nThe differential entropy of X is given by\nH(x) = 1\n2(log det(Σ) + d log(2π) + d).\nProof. By the definition of the differential entropy and the probability density function of the multi-\nvariate normal distribution, we have\nH(X) = −\nZ\nX\np(x) log p(x)dx = −Ex∼X [log p(x)]\n= −E log\n\"\n1\np\n(2π)d det(Σ)\nexp\n\u0012\n−1\n2(x −µ)⊤Σ−1(x −µ)\n\u0013#\n= E log\nq\n(2π)d det(Σ) + E\n\u00141\n2(x −µ)⊤Σ−1(x −µ)\n\u0015\n= 1\n2E [d log(2π) + log det(Σ)] + 1\n2E\n\u0002\n(x −µ)⊤Σ−1(x −µ)\n\u0003\n= 1\n2\n\u0000d log(2π) + log det(Σ) + E\n\u0002\n(x −µ)⊤Σ−1(x −µ)\n\u0003\u0001\nBy the cyclic property of trace, we have\nE\n\u0002\n(x −µ)⊤Σ−1(x −µ)\n\u0003\n= E Tr\n\u0002\n(x −µ)⊤Σ−1(x −µ)\n\u0003\n= E Tr\n\u0002\nΣ−1(x −µ)(x −µ)⊤\u0003\n= Tr Σ−1E\n\u0002\n(x −µ)(x −µ)⊤\u0003\n= Tr Σ−1Σ\n= d.\nCombine these steps, we get\nH(X) = 1\n2(log det(Σ) + d log(2π) + d).\nLemma 3 (Eigenvalue distribution of rank-one perturbed matrix). For matrix M ∈Rd×d and any\nrank-one perturbation S\ndef\n= M + uv⊤, we have\nlog det(S) = log det(M) + log(1 −v⊤M −1u),\n(5)\nProof. By the matrix determinant lemma [19], we have\ndet(S) = det(M + uv⊤) = det(M)(1 −v⊤M −1u)\n=⇒log det(S) = log det(M) + log(1 −v⊤M −1u).\n15\n\n\nNext, we combine the results above to derive the proof of Theorem 1.\nProof of Theorem 1. For each s (either a query or response) with embedding ¯x ∈Rd, we extend\nit to get n perturbations x1, x2, . . . , xn. Equivalently, {vi}i∈[n] are perturbations of ¯v. Assume\nthe perturbation is Gaussian with covariance matrix Σ. Then {xi}i∈[n] are samples for a Gaussian\nvector X ∼N(¯x, Σ). Denote λi(A) as the i-th largest eigenvalue of matrix A. Note that V ⊤V and\nV V ⊤are both positive semi-definite matrices and share the same nonzero eigenvalues (can be easily\nproved using SVD of V ). Similarly for X⊤X and XX⊤. We can diagonalize Σ as Σ = QΛQ⊤\nwhere Q is an orthogonal matrix and Λ = diag(λ1, . . . , λd). Define an isotropic Gaussian vector\nZ ∼N(0, Id). Then we can write X = QΛ1/2Z. Hence we have\nE∥X∥2 = E[X⊤X] = E[Z⊤Λ1/2Q⊤QΛ1/2Z] = E[Z⊤ΛZ] = E\n\" d\nX\ni=1\nλiZ2\ni\n#\n=\nd\nX\ni=1\nλiE[Z2\ni ] = Tr(Σ).\nNote that in high-dimensions, ∥X∥2 is concentrated around its mean E∥X∥2 = Tr(Σ) (particularly\nfor isotropic Gaussian, this is\n√\nD). Under this regime, we have V\ndef\n= [v1, v2, . . . , vn] ≈\n1\n√\nTr(Σ)X.\nLet α = n/ Tr(Σ). Then\nlog det(V ⊤V ) ≈log det\n\u0012\n1\nTr(Σ)X⊤X\n\u0013\n= log det\n\u0012\nα · 1\nnX⊤X\n\u0013\n= log det\n\u0012 1\nnX⊤X\n\u0013\n+n log α.\n(6)\nSince our method sets a threshold τ ∗on the log-determinant value, searched using a held-out set L ∈\nD, the final classification is invariant to shifts (and more generally, to any linear transformation. See\nProposition 1). Hence we can ignore the constant term n log α and only focus on log det\n\u0000 1\nnX⊤X\n\u0001\n.\nWe have\nlog det\n\u0012 1\nnX⊤X\n\u0013\n= log\nn\nY\ni=1\nλi\n\u0012 1\nnX⊤X\n\u0013\n=\nn\nX\ni=1\nlog λi\n\u0012 1\nnX⊤X\n\u0013\n=\nn\nX\ni=1\nlog λi\n\u0012 1\nnXX⊤\n\u0013\n.\nThus, it suffices to study the eigenvalues of 1\nnXX⊤. Note that we have\nXX⊤−nΣ =\n n\nX\ni=1\nxix⊤\ni\n!\n−\n n\nX\ni=1\n(xi −¯x)(xi −¯x)⊤\n!\n=\nn\nX\ni=1\n(¯xx⊤\ni −xi ¯x⊤+ ¯x¯x⊤)\n= ¯x\n n\nX\ni=1\nxi\n!⊤\n−\n n\nX\ni=1\nxi\n!\n¯x⊤+ n¯x¯x⊤)\n≈n¯x¯x⊤−n¯x¯x⊤+ n¯x¯x⊤\n= n¯x¯x⊤.\nThis implies that\n1\nnXX⊤−Σ ≈¯x¯x⊤.\n(7)\nThus Σ is essentially a rank-one perturbation of 1\nnXX⊤. By Lemma 2, we know the differential\nentropy of X is:\nH(X) = 1\n2(log det(Σ) + d log(2π) + d).\nAgain due to the invariance of our method to linear transformation, we can only focus on log det(Σ).\nWhen d is large, by Lemma 3, let M\ndef\n=\n1\nnXX⊤, we have log det(Σ) = log det M + log(1 +\n¯x⊤M −1 ¯x⊤). Note here ¯x is constant, and recall that M concentrate around a deterministic matrix\nΣ + ¯x¯x⊤from equation 7 above (one can show ∥M −(Σ + ¯x¯x⊤)∥\np→0 as n →∞, using standard\nrandom matrix theory tools). Therefore combining all above, we have derived that log det M is\nessentially shifted log det(Σ) in high-dimensional regimes, and hence same for log det 1\nnX⊤X.\nThis completes the proof.\n16\n\n\nD\nNumerical Stability with ϵ\nHere, we present the distribution of the matrix norm ∥V ⊤V ∥(spectral norm) across our datasets in\nFigure 4. The histograms indicate that the chosen value of ϵ = 10−10 is negligible compared to the\ntypical magnitude of ∥V ⊤V ∥. This confirms that ϵ only serves to ensure numerical stability rather\nthan influencing the quantification. Moreover, in practice, we observe that exact repetition among\nsampled perturbations is rare\nFigure 4: Distribution of the spectral norm ∥V ⊤V ∥for the perturbation embedding vectors (the left\nhistogram corresponds to queries perturbations and the right is for response perturbations.\nE\nVariation: Reduced Dimension d\nIn Figure 5, we illustrate the impact of varying d on performance. Additionally, we compare our\nmethod with the Semantic Volume that uses the original embedding vectors instead of the projected\nones. Our results show that projecting to a lower-dimensional space improves performance, possibly\nbecause PCA projections better separate perturbation vectors. Furthermore, our study suggests that\nthere may be an optimal, task-dependent choice of d. Exploring this further and characterizing the\noptimal d is left for future work.\nFigure 5: F1 scores for different dimension d in PCA dimension reduction. The dashed line represents\nthe F1 score of the semantic volume method without dimension reduction. The left figure corresponds\nto the external uncertainty task, while the right figure corresponds to the internal uncertainty task.\nF\nVariation: Perturbation Sample Size n\nThe perturbation sample size n is a hyperparameter in our method. A larger n provides a more\ncomprehensive estimate of semantic dispersion but increases computational cost, while a smaller n\nmay fail to capture sufficient variation, reducing the reliability of uncertainty quantification.\nFigure 6 illustrates how varying n affects classification performance. In general, increasing n tends\nto improve F1 scores, likely due to a more accurate estimation of dispersion. However, beyond a\ncertain point, the improvements diminish while the cost continues to rise. To balance efficiency and\nperformance, we set n = 20 for external uncertainty detection and internal uncertainty detection.\n17\n\n\n(a) External uncertainty: best accuracy\nfor different n.\n(b) External uncertainty: best F1-score\nfor different n.\n(c) Internal uncertainty: best accuracy\nfor different n.\n(d) Internal uncertainty: best F1-score\nfor different n.\nFigure 6\nG\nVariation: Embedding Models\nTo examine the effect of different embedding models on our method, we explore two larger sentence-\ntransformers: Alibaba-NLP/gte-Qwen2-7B-instruct (embedding dimension 3584) from the\nsame Qwen family as used in our main results, and nvidia/NV-Embed-v2 (embedding dimension\n4096) from a different model family. Using the external uncertainty task as a case study, we present\nthe results in Table 4. Our findings indicate that models from different families but with similar sizes\nproduce comparable results. Compared to the Qwen-1.5B sentence transformer used in the main\nresults, performance remains largely similar—while the 7B model achieves slightly higher accuracy\nbut lower F1 score, this difference may not be statistically significant.\nQwen2-7B\nNV-Embed\nAccuracy\nF1\nAccuracy\nF1\nn = 25\n57.8\n68.0\n57.4\n67.5\nn = 20\n58.1\n68.0\n58.4\n67.9\nn = 15\n57.7\n68.1\n57.9\n67.8\nn = 10\n57.2\n67.7\n57.4\n67.5\nn = 5\n56.8\n66.9\n57.7\n67.0\nTable 4: Performance with different embedding models.\nH\nVariation: Response Generation Model\nTo assess the generalizability of our method, we repeat the experiment for internal uncertainty\ndetection but replaced Llama3.2-1B with LLama3-8B, a larger LLM. Notebly, with a larger model,\nthe probability-based methods show higher latency. In our experiment, both Last Token Entropy and\nLog Probabilities took more than 4 hours on a H100-80GB GPU. Furthermore, similar to Table 2,\nwe observe that methods relying on prompting the LLM (including pTrue) sometimes exhibit a\nstrong bias toward predicting almost exclusively “Yes” or “No”. In Table 5, for example, Prompt\nLlama3.2-1B and pTrue (Llama3.2-1B) predict nearly all samples as “Yes”, whereas pTrue (Mistral-\n18\n\n\n7B) predicts almost all as “No”. This instability highlights another drawback of using prompted\nLLMs for uncertainty prediction.\nMethod\nAccuracy\nF1\nPrompt Llama3.2-1B\n50.1\n66.5\nPrompt Llama3-8B\n65.5\n64.0\nPrompt Mistral-7B\n60.4\n54.3\nLast Token Entropy\n56.2\n55.9\nLog Probabilities\n53.4\n51.1\npTrue (Llama3.2-1B)\n50.0\n66.6\npTrue (Llama3-8B)\n55.2\n23.2\npTrue (Mistral-7B)\n50.8\n4.3\nLexical Similarity\n64.2\n72.0\nSemantic Entropy\n63.5\n69.4\nSemantic Volume (ours)\n72.0\n74.9\nTable 5: Internal Uncertainty: Performance compar-\nison based on Accuracy and F1 score.\nMethod\nAUROC\nLast Token Entropy\n63.9\nLog Probabilities\n54.4\npTrue (Llama3.2-1B)\n55.1\npTrue (Llama3-8B)\n52.8\npTrue (Mistral-7B)\n72.7\nLexical Similarity\n78.7\nSemanticEntropy\n73.2\nSemantic Volume (ours)\n79.7\nTable 6: Internal Uncertainty: Performance\ncomparison based on AUROC.\nI\nHallucination Detection Pipeline using both External and Internal\nUncertainty\nIn this paper, we conducted separate experiments on external uncertainty detection and internal\nuncertainty detection to demonstrate that the Semantic Volume is an effective method that can be\ngenerally applied to both tasks. In practice, these two tasks can be combined into a unified pipeline\nfor hallucination detection (see Figure 7): First, we perform the internal uncertainty detection. If high\nuncertainty is detected in the response, then hallucination is likely to happen. Then at the second\nstep, we check the external uncertainty: If the query is detected to be ambiguous, the LLM should\nask a clarification question to the user. After clarification is provided or additional information is\nprovided to resolve the ambiguity in the query, the LLM can then generate the answer to the query.\nOn the other hand, if external uncertainty is ruled out, then the hallucination is likely caused by\ninternal lack of knowledge of the LLM. This can be addressed through various methods such as\nretrieval-augmented generation (RAG), reasoning-based approaches, or by leveraging stronger LLMs\nor human agents.\nFigure 7: Complete pipeline for hallucination detection utilizing both external and internal uncertainty.\nMoreover, all of our experiments are set up as classification tasks for uncertainty detection, relying on\na threshold-based decision. However, these uncertainty measures can also be applied to comparison\nor ranking tasks.\n19\n\n\nJ\nPrompts\nJ.1\nPrompt template to extend queries.\nProvide a paraphrase of the following question with a contextual expansion,\nwhile maintaining its core meaning.\nThe filled context information should\nbe diverse but must be concrete and specific (it cannot be a placeholder or\na template).\nOnly reply with the new version of the question and nothing\nelse.\nQuestion:\n{question}\nJ.2\nPrompt template for query ambiguity detection\nIs the following question ambiguous?\nA question is ambiguous if it can\nbe interpreted in multiple ways or has multiple possible answers.\nIf the\nquestion is ambiguous, then reply ‘Yes’, otherwise reply ‘No’.\nOnly reply\nwith ‘Yes’ or ‘No’ and nothing else.\nQuestion:\n{question}\n20\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21239v1.pdf",
    "total_pages": 20,
    "title": "Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs",
    "authors": [
      "Xiaomin Li",
      "Zhou Yu",
      "Ziji Zhang",
      "Yingying Zhuang",
      "Swair Shah",
      "Anurag Beniwal"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}