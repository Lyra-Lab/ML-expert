{
  "id": "arxiv_2502.21297v1",
  "text": "Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset\nWith Faithfulness Based on Causal Theory of Mind\nDingyi Zhang♠and Deyu Zhou*♠\n♠School of Computer Science and Engineering, Key Laboratory of Computer Network\nand Information Integration, Ministry of Education, Southeast University, China\n{zhangdy, d.zhou}@seu.edu.cn\nAbstract\nPersuasive dialogue plays a pivotal role in hu-\nman communication, influencing various do-\nmains. Recent persuasive dialogue datasets of-\nten fail to align with real-world interpersonal in-\nteractions, leading to unfaithful representations.\nFor instance, unrealistic scenarios may arise,\nsuch as when the persuadee explicitly instructs\nthe persuader on which persuasion strategies\nto employ, with each of the persuadee’s ques-\ntions corresponding to a specific strategy for\nthe persuader to follow. This issue can be at-\ntributed to a violation of the \"Double Blind\"\ncondition, where critical information is fully\nshared between participants. In actual human\ninteractions, however, key information—such\nas the mental state of the persuadee and the\npersuasion strategies of the persuader—is not\ndirectly accessible.\nThe persuader must in-\nfer the persuadee’s mental state using Theory\nof Mind capabilities and construct arguments\nthat align with the persuadee’s motivations.\nTo address this gap, we introduce ToMMA,\na novel multi-agent framework for dialogue\ngeneration that is guided by causal Theory of\nMind. This framework ensures that informa-\ntion remains undisclosed between agents, pre-\nserving \"double-blind\" conditions, while causal\nToM directs the persuader’s reasoning, enhanc-\ning alignment with human-like persuasion dy-\nnamics.\nConsequently, we present CToM-\nPersu, a multi-domain, multi-turn persuasive\ndialogue dataset that tackles both double-blind\nand logical coherence issues, demonstrating su-\nperior performance across multiple metrics and\nachieving better alignment with real human di-\nalogues. Our dataset and prompts are available\nat https://github.com/DingyiZhang/ToMMA-\nCToMPersu.\n1\nIntroduction\nPersuasive dialogue generation is critical in various\nAI applications, including education, healthcare\n* Corresponding author.\nFigure 1: An example illustrating the unnaturalness of\nan LLM-generated dataset. In the figure, the blue text\nhighlights instances where the persuadee mistakenly\nadopts the persuader’s arguments while expressing their\nown viewpoint. Moreover, as indicated by the red text,\nthe persuadee never actively presents arguments sup-\nporting their presumed stance—in this case, the benefits\nof the Shopping Mall. Instead, they merely guide the\npersuader to apply persuasion techniques on them.\ncounseling, and business marketing (Rogiers et al.,\n2024). An effective persuasion system must in-\ntegrate intention detection to understand the per-\nsuadee’s intentions (Sakurai and Miyao, 2024),\nstrategy detection to identify suitable persuasive\ntechniques (Jin et al., 2023), and credibility main-\ntenance to ensure trustworthiness (Furumai et al.,\n2024). Although large language models (LLMs)\nhave made remarkable strides in natural language\nprocessing, generating human-like persuasive con-\nversations remains a significant challenge. Cur-\nrent human dialogue datasets are predominantly\narXiv:2502.21297v1  [cs.CL]  28 Feb 2025\n\n\ndomain-specific, such as those focused on char-\nity fundraising (Wang et al., 2019), product rec-\nommendations (Li et al., 2018), or medical con-\nsultations (Zeng et al., 2020). This narrow focus\nlimits the ability of models to generalize across dif-\nferent persuasive contexts, preventing them from\nfully exploiting the benefits of large-scale, pre-\ntrained models. Additionally, the relatively small\nsize of these datasets hinders the development of\npersuasion systems capable of generating strategi-\ncally sound and personalized responses. Recent\nefforts have explored using GPT-4 to create large-\nscale, multi-domain persuasive dialogue datasets\n(Jin et al., 2024), providing a wider range of scenar-\nios and more diverse conversational patterns than\nearlier datasets.\nFigure 2: Causal Theory of Mind\nDespite the advancements in GPT-4-generated\nmulti-domain persuasive dialogue datasets, several\nissues arise due to limitations in prompt and frame-\nwork design. (1) Inconsistencies in the logical flow\nof conversations are common, where the persuadee\ninadvertently reinforces the persuader’s arguments\nwhen articulating their stance, thus weakening their\nown position. For example, as shown in Fig. 1, the\npersuadee’s intention is to invest in a new shop-\nping mall. However, in expressing their viewpoint,\nthey mention \"city growth,\" which is actually a\nbenefit highlighted by the persuader’s argument for\ninvesting in residential areas. This creates a dis-\nconnect and reduces the realism of the dialogue.\n(2) Unrealistic behaviors, such as the persuadee\nexplicitly instructing the persuader on which per-\nsuasion strategies to adopt, are also prevalent. In\nsuch instances, each of the persuadee’s questions\ncorresponds directly to a specific strategy the per-\nsuader is supposed to follow. As demonstrated in\nFig. 1, the colored text in the dialogue corresponds\none-to-one, such as the persuadee’s statement \"in-\nvestment in the long run\" aligning with \"long-term\ninvestment\" in the strategy. This pattern persists\nthroughout the entire conversation. In real human\ninteractions, crucial information, such as the men-\ntal state of the persuadee and the persuasion strate-\ngies of the persuader, is not directly accessible.\nInstead, the persuader must infer the persuadee’s\nmental state using Theory of Mind (ToM) capabili-\nties and construct arguments that resonate with the\npersuadee’s mental state.\nTo further validate our findings, we quantita-\ntively compared two datasets using the proposed\nevaluation metric: PersuasionForGood, a small-\nscale dataset of real conversations focused on\npersuading people to donate, and DailyPersua-\nsion, a large-scale, multi-domain, multi-turn di-\nalogue dataset generated by GPT-4. The evaluation\nmethod we introduce is called Causal Theory of\nMind Evaluation. As shown in Fig. 2, Causal The-\nory of Mind refers to the use of Theory of Mind\nto influence others’ behaviors. To prevent a spe-\ncific action, it is sufficient to alter a person’s be-\nlief or desire. However, to encourage someone\nto take a specific action, both their belief and de-\nsire must be addressed (Wu et al., 2024b). Re-\nsearch indicates that all humans possess the ability\nof Theory of Mind and apply this ability in ev-\neryday interpersonal interactions. Therefore, even\nthough Causal Theory of Mind may not be explic-\nitly mentioned during data collection, individuals\nstill unconsciously utilize such abilities and con-\nversational logic in real-life dialogues. Based on\nthis, we argue that using this evaluation method to\nassess the authenticity of LLM-generated datasets\nis both reasonable and valid. As shown in Tab. 1,\nwe observe that both the LLM-generated dataset\nand the human dialogue dataset perform well when\nevaluated using Direct Prompting, where the LLM\nevaluator directly assesses whether the persuadee\nhas been persuaded. However, when the LLM eval-\nuator is required to follow human logic to make\nthis judgment (CToM Eval), the persuasion success\nrate of the LLM-generated dataset drops by 35.95%.\nIn contrast, while the human dataset also experi-\nences a decline, it is much smaller, at only 9%.\nThis suggests that although the LLM-generated\ndataset appears persuasive from the LLM evalua-\ntor’s perspective, many persuadees remain uncon-\nvinced when judged according to human reasoning.\nThese results demonstrate the validity of our evalu-\nation method and highlight the lack of authenticity\nin the LLM-generated dataset.\nAddressing these challenges is essential for de-\nveloping AI-driven persuasion systems that more\naccurately reflect real human dialogue dynamics.\nTo this end, we take three key steps to enhance\nthe authenticity and logical coherence of persua-\n\n\nsive dialogue generation: (1) We introduce a novel\ndataset evaluation method based on causal the-\nory of mind, in which the LLM first infers the\npersuadee’s belief and desire from the conversa-\ntion, then assesses whether the persuader success-\nfully addresses them. When applied to human\ndialogue datasets, this method yields results con-\nsistent with direct prompting, where the LLM di-\nrectly determines whether the persuadee was per-\nsuaded. However, when tested on LLM-generated\ndatasets, a significant discrepancy emerges, reveal-\ning a critical gap between model-generated persua-\nsion and real human interactions. (2) We present\nToMMA, a multi-agent framework for generating\npersuasive dialogue datasets. ToMMA ensures that\nboth the persuader and persuadee operate under\ndouble-blind conditions, preventing information\nleakage and maintaining the natural uncertainty\ninherent in real conversations. Furthermore, the\nentire multi-turn dialogue is guided by causal the-\nory of mind, enabling the persuader to construct\narguments based on an inferred understanding of\nthe persuadee’s psychological state, thus foster-\ning more human-like persuasion dynamics. (3)\nWe introduce CToMPersu, a large-scale, multi-\ndomain, multi-turn persuasive dialogue dataset\ncomprising 6,275 dialogues across 35 domains and\n6,257 unique scenarios. This dataset effectively\naddresses double-blind constraints and resolves di-\nalogue logic inconsistencies, demonstrating strong\nperformance across multiple evaluation metrics and\nachieving superior alignment with real human dia-\nlogues.\n2\nRelated Work\n2.1\nPersuasion\nPersuasion Systems\nPersuasive dialogue has\nbeen a long-standing area of interest, particularly\nfocusing on the application of persuasion strate-\ngies (Joshi et al., 2024; Srba et al., 2024; Rogiers\net al., 2024). Since the emergence of large lan-\nguage models, some studies have tested their capa-\nbilities in public health (Altay et al., 2023), politics\n(Potter et al., 2024), and product recommendations\n(Chen et al., 2023). Other work has examined the\nimpact of personality on LLM persuasion (Lou\nand Xu, 2025). Some research has primarily con-\ncentrated on strategy detection (Jin et al., 2023).\nHowever, compelling arguments might be more\nimportant than the strategies themselves, as they di-\nrectly impact the persuadee’s decision-making pro-\ncess. Some works study credibility of arguments\nused in persuasive dialogues. Methods such as self-\nchecking and retrieval-based techniques have been\ndeveloped to ensure that arguments are credible\n(Furumai et al., 2024; Qin et al., 2024). There are\nalso studies dedicated to designing scoring systems\nto identify arguments that can strengthen one’s own\nviewpoint (Saenger et al., 2024). There is also work\nthat studies how LLM can persuade users with\ndifferent personalities on social media. However,\nthese methods often come with longer response\ntimes and still fail to make argument choices that\nare tailored to the persuadee’s mental state, po-\ntentially reducing the overall effectiveness of the\npersuasion process.\nPersuasion Datasets\nRegarding datasets, there is\na growing focus on domains like charity donations\n(Wang et al., 2019), recommendation systems (Li\net al., 2018), and medical dialogues (Zeng et al.,\n2020). Moreover, some work has focused on in-\ntention detection within these datasets, aiming to\nidentify underlying motives during persuasive dia-\nlogues (Sakurai and Miyao, 2024). Datasets such\nas PersuasionForGood and MedDialog have pro-\nvided small-scale real-world dialogues, but they are\nlimited in size and scope. Recent study created a\nlarge-scale, multi-domain datasets called DailyPer-\nsuasion, which offer a more diverse set of conver-\nsational patterns (Jin et al., 2024). However, there\nremain several challenges in aligning these datasets\nwith human-like dialogue dynamics and ensuring\nlogical consistency throughout the conversations.\n2.2\nTheory of Mind\nToM in Psychology\nTheory of Mind (ToM) is\nthe ability to understand others by attributing men-\ntal states, recognizing that their beliefs, desires,\nand thoughts may differ from one’s own (Premack\nand Woodruff, 1978). Based on this theory, psy-\nchologists have developed models such as the BDI\nModel (Georgeff et al., 1999) and Causal ToM (Wu\net al., 2024b), which explain how people interact\nwith others in society, predict their actions, and\neven influence their decisions. Additionally, psy-\nchological tests, such as False Belief Tasks (Baron-\nCohen et al., 1985), have been designed to assess\nwhether individuals possess Theory of Mind.\nToM and LLM\nIn recent years, the Theory of\n\n\nMind capabilities of large language models have\nbeen a subject of research. Some studies have de-\nsigned benchmarks, such as ToMi (Le et al., 2019)\nand FANToM (Kim et al., 2023), to test LLMs’\nToM abilities, building on the psychological False\nBelief Tasks (Chen et al., 2024; Wu et al., 2023;\nTan et al., 2024). Furthermore, other works have\nextended these tasks by incorporating the mental\nstates of characters in the stories, such as OpenToM\n(Xu et al., 2024). There are also efforts to repre-\nsent ToM as a knowledge graph-based dataset (Wu\net al., 2024a). There is also a work that annotates\nthe mental state of people in each round of dialogue\non the negotiation dataset to test the ToM ability of\nLLMs (Chan et al., 2024). Recently, some research\nhas incorporated real-world human behaviors and\nthe underlying mental states as evaluation metrics\nfor LLM ToM within benchmarks (Gu et al., 2024).\nIn addition, there are works exploring ways to im-\nprove the ToM abilities of LLMs, such as by letting\nLLMs understand who can perceive what events,\nor by breaking down the stories in the task into\nsmaller parts based on the order of events (Wilf\net al., 2024; Hou et al., 2024; Tang and Belle, 2024;\nLin et al., 2024; Jung et al., 2024; Sclar et al., 2023).\nThere are also works that exploit multi-agent and\nToM capabilities to complete complex tasks and\ngames (Yim et al., 2024; Cross et al., 2024; Li et al.,\n2023). These works suggest that the integration of\nLLMs with ToM holds great potential for future\nresearch.\n3\nToMMA\nTo address the challenges of maintaining double-\nblind conditions and aligning persuasive dialogue\nlogic, we propose ToMMA, a framework for gener-\nating dialogue datasets guided by causal theory of\nmind and employing a multi-agent approach. As\nshown in Fig. 3, the process unfolds in three stages:\nFirst, we filter scenarios from DailyPersuasion, re-\ntaining unique tags and generating the persuadee’s\nmental state. In the second step, we design per-\nsuader and persuadee agents without shared infor-\nmation, ensuring that both agents follow causal\ntheory of mind to generate persuasive dialogues.\nFinally, to maintain the quality of the dataset, we\nintroduce an observer agent that reviews the per-\nsuader’s inferences and persuasive statements, of-\nfering suggestions for improvement. This multi-\nstep process guarantees the generation of a diverse\nand high-quality CToMPersu dataset, which pre-\nserves double-blind conditions while aligning with\nhuman-like persuasion dynamics.\n3.1\nCausal Theory of Mind\nAs illustrated in Fig. 2, Causal Theory of Mind\nrefers to the use of Theory of Mind to influence\nothers’ behaviors. To prevent unwanted actions,\nit is sufficient to alter the other person’s belief or\ndesire. For instance, informing someone that the\npost office is closed or removing their need to send\na letter can prevent them from going. Conversely,\nto encourage someone to take a specific action,\nboth their belief and desire must be addressed. For\nexample, to persuade someone to go to the post\noffice, they must believe it is open and have the\nneed to send a letter (Wu et al., 2024b).\nIn real-world persuasion, the persuader is aware\nof both what they want and do not want the per-\nsuadee to do. Their objective is to understand the\npersuadee’s mental state—specifically, their beliefs\nand desires. This understanding enables the per-\nsuader to tailor their approach and effectively guide\nthe persuadee toward the desired outcome.\n3.2\nImportant Contents\nBased on the definition of causal theory of mind\n(Wu et al., 2024b) and our design tailored for the\npersuasion domain, we have derived the following\nfour definitions. These will serve as prompts at\neach step, not only assisting GPT in generating\nmental states but also helping both the persuader\nand persuadee agents organize their dialogue.\nPreventative Preventative Behavior refers to ac-\ntions the persuadee desires to take, which often\nconflict with generative behavior. Therefore, the\npersuader’s goal is to prevent the persuadee from\nengaging in these behaviors.\nGenerative Generative Behavior represents actions\nthe persuader wants the persuadee to take. These\nbehaviors are the persuader’s goal.\nBelief For preventative behavior, the persuadee\nshould hold a positive belief, as recognizing the\nfacts as positive tends to encourage engagement in\nthe behavior. Conversely, for generative behavior,\nthe persuadee should hold a negative belief, as per-\nceiving the current situation as unfavorable initially\ndiscourages engagement in the behavior.\nDesire For both preventative and generative behav-\niors, the persuadee should have a positive desire.\n\n\nFigure 3: Overview of the ToMMA framework for collecting the CToMPersu dataset. This figue illustrates the\nthree-step process: (1) Mental State Generation, (2) Dialogue Generation Guided by Causal Theory of Mind, and\n(3) Observer Interaction for quality control.\nThis is because we believe that if the persuadee\ninitially holds a negative desire toward generative\nbehavior, the entire premise of persuasion would be\nundermined. The key difference lies in the expecta-\ntion of desire fulfillment: for preventative behavior,\nthe persuadee believes their desire will be satisfied\nonce the action is taken. In contrast, for generative\nbehavior, the persuadee may be uncertain whether\ntheir desire can be satisfied or may doubt its fulfill-\nment.\n3.3\nMental State Generation\nTo ensure topic diversity, we adopt the scenario\nsetup from DailyPersuasion, filtering for unique\nscenarios, which results in a total of 6,257 distinct\nscenarios. Next, we generate the behavioral inten-\ntions of the persuadee based on the background and\nprompts from each scenario. We define Generative\nBehavior and Preventative Behavior using a large\nlanguage model (GPT-4o in this case), guided by a\ncarefully designed prompt. Additionally, in cases\nwhere the persuadee does not have any specific in-\ntention to act (i.e., they lack a pre-set stance), only\nGenerative Behavior is generated, while Preventa-\ntive Behavior is set to \"None.\" We then generate\nthe persuadee’s Belief and Desire, based on the sce-\nnario and the identified Generative and Preventative\nBehavior. Finally, the generated Belief and Desire\nform the persuadee’s mental state for each scenario,\nwhich serves as the foundation for the subsequent\nsteps in the persuasive dialogue generation process.\n3.4\nConversation Generation\nThe core of ToMMA revolves around generating\nthe dialogue between two agents: the persuader\nand the persuadee. As shown in Fig. 3, both agents\nshare the same information about the scenario, but\nthe persuader does not have direct access to the\npersuadee’s mental state.\nPrompt Design To ensure the quality of the dataset,\nwe set a limit on the number of dialogue rounds. If\nthe persuadee’s mental state involves only Genera-\ntive Behavior, the interaction is limited to 3 rounds,\nresulting in 6 utterances. The dialogue begins with\nthe persuader presenting their viewpoint and ask-\ning the persuadee about their belief regarding the\nGenerative Behavior. The persuadee then reveals\naspects of their mental state. Next, prompting the\npersuader to update their understanding of the per-\nsuadee’s mental state and address any concerns\nrelated to the persuadee’s belief. In the subsequent\nround, the persuadee discloses their desire, and\nthe persuader again updates their model of the per-\nsuadee’s mental state, responding in a way that\nsatisfies the persuadee’s desire. The conversation\nconcludes with the persuadee’s final statement.\nIf both Preventative and Generative Behavior are\npresent in the persuadee’s mental state, the num-\nber of rounds is set to 4, resulting in 8 utterances.\nThe first round will focus on addressing the per-\nsuadee’s belief or desire regarding the Preventative\nBehavior, while the remaining rounds will follow\nthe same pattern as outlined above. We have de-\n\n\nsigned specific prompts for each round, tailored to\nboth agents.\nPersuadee Agent\nThe role of the persuadee is\nrelatively simpler. Their available information in-\ncludes the scenario and their mental state, which\nconsists of beliefs and desires. The persuadee also\nhas access to the conversation history, which in-\nforms their responses. In the context of persuasion,\nwe assume that both parties aim to resolve the issue\nat hand, rather than engaging in a debate. Conse-\nquently, the persuadee is more likely to explicitly\nexpress their thoughts and concerns.\nPersuader Agent\nThe persuader’s task is more\ncomplex. Their available information includes the\nscenario, but they do not have direct access to the\npersuadee’s mental state. To initiate the conversa-\ntion, the persuader subtly probes the persuadee’s\nbeliefs and desires. As the dialogue progresses,\nthe persuader uses the conversation history and the\npersuadee’s responses to infer their mental state.\nThis process involves leveraging Theory of Mind\nto model the persuadee’s beliefs and desires.\nOnce the persuader has developed an understand-\ning of the persuadee’s mental state, they craft cus-\ntomized persuasive strategies. According to Causal\nTheory of Mind, when addressing Preventative Be-\nhavior, the persuader focuses on influencing the be-\nlief or desire that is more responsive to persuasion,\ndepending on which aspect is easier to change. For\nGenerative Behavior, the persuader must address\nboth the belief and the desire in order to align with\nthe persuadee’s motivations and influence their de-\ncision. Fig. 4 illustrates the prompt design used\nby the persuader in the third round of dialogue.\nAt this stage, the persuader has addressed the per-\nsuadee’s beliefs regarding Preventative Behavior,\nand the persuadee has introduced a negative be-\nlief regarding Generative Behavior, which the per-\nsuader needs to resolve.\n3.5\nObserver Interaction\nDuring the data generation process, we observed\nthat while the persuadee does not intentionally\nconceal or mislead the persuader, the persuader\nmay still incorrectly infer the persuadee’s men-\ntal state. These incorrect inferences can lead the\npersuader down the wrong path in the persuasion\nprocess, resulting in logical inconsistencies in the\ndialogue. To address this issue, we introduce the\nObserver Agent. Fig. 5 illustrates a successful\nFigure 4: 3rd Round Persuader Response Prompt De-\nsign\ncase study where the Observer Agent’s suggestions\ncontributed to the improvement of dataset quality.\nThe Observer Agent plays a critical role in en-\nsuring the quality and logical coherence of the per-\nsuasive dialogue. As shown in Fig. 3, it evaluates\nthe persuader’s inferences and responses. If the\nObserver determines that the persuader’s response\nis sufficiently accurate, it does not provide any sug-\ngestions. However, if the response is deemed inad-\nequate, the Observer offers feedback and sugges-\ntions to help the persuader refine their response,\nthereby improving the quality and logical consis-\ntency of the dialogue and the generated dataset.\n4\nExperiments\nIn the experimental section, we demonstrate how\nour dataset compares with other human and LLM-\ngenerated datasets using conventional evaluation\nmethods, as well as its consistency in both Causal\nTheory of Mind Evaluation and Direct Prompting.\nAdditionally, we categorize the experiments into\nFixed and Dynamic Persuadee categories to test the\npersuasive capabilities of existing large models.\n4.1\nDataset Evaluation\nTo assess the quality of CToMPersu, we compared\nit to a real human dialogue dataset, PersuasionFor-\nGood, a small-scale dataset consisting of real con-\n\n\nversations focused on persuading people to donate.\nWe also compared it to DailyPersuasion, a large-\nscale, multi-domain, multi-turn dialogue dataset\ngenerated by GPT-4.\nMetric\nPersuForGood\nDailyPersu\nCToMPersu\nContext-Coherence\n4.29\n4.97\n4.97\nLogical-Coherence\n4.14\n4.98\n4.97\nHelpfulness\n3.86\n4.87\n4.93\nDirect Prompting\n88.87\n90.75\n90.82\nCausal ToM Eval\n79.87\n54.80\n82.02\nTable 1: Comparison between PersuasionForGood,\nDailyPersu, and CToMPersu datasets.\nMetrics\nWe apply five key evaluation metrics to\ncompare the datasets, of which the first three are\nbased on a multi-turn dialogue evaluation method\n(Sun et al., 2024). All of these metrics are evaluated\nby GPT-3.5 : 1) Context-Coherence This metric\nassesses the coherence of the context across multi-\nple dialogue turns, based on the LLM’s judgment\nof the conversation’s flow. 2) Logical-Coherence\nThis evaluates the logical consistency of the di-\nalogue, ensuring that each turn is logically con-\nsistent with the previous context. 3) Helpfulness\nThis measures whether the persuader’s responses\nare effective in helping the persuadee achieve per-\nsuasion. 4) Direct Prompting In this metric, we\nprompt the LLM to play the role of the persuadee,\nreading the dialogue and determining whether they\nfeel persuaded. This serves as a direct measure of\nthe dialogue’s persuasive effectiveness. 5) Causal\nToM Eval This metric evaluates whether the per-\nsuadee’s mental state was adequately inferred and\naddressed, in line with the Causal Theory of Mind\nevaluation method.\nThe experimental results in Tab. 1 show that, un-\nder some conventional metrics, the LLM-generated\ndatasets achieve a high level of performance, with\nscores approaching perfection. This may be influ-\nenced by GPT evaluators’ preference for responses\ngenerated by larger models. However, in the Causal\nToM Eval results, the performance of CToMPersu\nis more similar to that of the human dataset, with\nonly an -8.8 point difference. This suggests that\nthe dataset generated using ToMMA aligns more\nclosely with the persuasive logic of human conver-\nsations. It also highlights that relying solely on\ngeneral multi-turn dialogue evaluation metrics is\ninsufficient for accurately assessing the dataset.\n4.2\nExperimental Results\nSetup: For evaluation purposes, we separated the\ntest set using a specific ratio. The domain distri-\nbution is shown in Tab. C. We evaluated the per-\nformance of GPT-3.5, GPT-4o-mini, and GPT-4o\non CToMPersu. The evaluation was divided into\ntwo tests: the Fixed Persuadee test, in which the\nLLM predicts the next response of the persuader\nstarting from a specific dialogue round within dif-\nferent scenarios from the dataset; and the Dynamic\nPersuadee test, where the persuadee, played by\nGPT-4o, interacts with the persuader, played by\nanother LLM, based on the scenario and mental\nstate components.\nFixed Persuadee Evaluation: We fixed the dia-\nlogues up to the third round, as the persuader’s\nresponse in this round is crucial, regardless of\nwhether the persuasive dialogue includes Preventa-\ntive Behavior. The previous dialogues provide the\nhistorical context for the persuader agent. Rouge-L\nrefers to the Rouge value between the model’s pre-\ndictions and the golden label. Persuasive is based\non (Furumai et al., 2024), where GPT-3.5 uses both\nthe historical dialogue and the current prediction to\ndetermine whether the prediction aims to change\nthe persuadee’s mind. A score is then assigned on\na scale from 1 to 10 based on this evaluation.\nDynamic Persuadee Evaluation: In the dynamic\npersuadee evaluation, we set up a persuadee to en-\ngage in a dialogue with the persuader, followed\nby an assessment of the outcome. The persuadee\nuses the mental state data from the dataset to guide\nthe dialogue generation. For evaluation, we con-\nsider several aspects. Persuasive is evaluated as\ndescribed above. Preventative Satisfaction asks\nGPT to evaluate whether, as the persuadee, it feels\nthat the dialogue satisfies the requirements for pre-\nventative behavior. Similarly, Generative Satis-\nfaction assesses the degree to which the dialogue\nmeets the persuadee’s needs for generative behav-\nior. CToM Eval combines the results of Preventa-\ntive and Generative Satisfaction to assess whether\nthe persuader has successfully persuaded the per-\nsuadee.\nFrom the results in Tab. 2, we observe that for\nthe fixed persuadee evaluation, GPT-4o performs\nthe best in both the Rouge score and the Persuasive\nevaluation. This indicates that GPT-4o has supe-\nrior persuasive capabilities compared to the other\n\n\nModel\nFixed Persuadee\nDynamic Persuadee\nRouge-L\nPersuasive\nPersuasive\nPreventative\nGenerative\nCToM\nGPT-3.5\n0.2813\n7.94\n7.87\n33.14\n28.38\n15.05\nGPT-4o-mini\n0.2872\n8.07\n8.08\n37.71\n16.76\n12.57\nGPT-4o\n0.2899\n8.17\n8.06\n42.67\n17.90\n13.33\nTable 2: Evaluation of Different Models in Fixed and Dynamic Persuadee Evaluation.\nmodels. Moreover, with the Persuasive evaluation\nrange extending up to a maximum score of 10, the\nhighest current score is only 8.17, suggesting that\nthere is still significant room for improvement in\nLLMs. In the dynamic persuadee evaluation, GPT-\n4o-mini and GPT-4o perform well in the Persuasive\nevaluation. GPT-4o performs the best in Preventa-\ntive Satisfaction, which may indicate that GPT-4o\nis more effective at discouraging actions. How-\never, GPT-3.5 excels in Generative Satisfaction and\nCToM Eval, suggesting that it may be better at\nconvincing someone to take action. This could be\ninfluenced by the design of the prompt and the num-\nber of turns set in the evaluation. Specifically, the\nLLM persuadee might fail to adequately respond\nto the Generative Behavior, resulting in a lower\nscore. This could also influence the CToM score.\nAdditionally, the overall low success rate (less than\n50%) highlights some limitations of LLMs in The-\nory of Mind, as they struggle to accurately infer\nand persuade the other party’s mental state without\nexplicit prompting.\n4.3\nObserver Agent Case Study\nAt times, the persuader agent may misjudge or\nmake errors in predicting the persuadee agent’s\nmental state. For instance, as illustrated in Fig.\n5, the persuader was expected to address the per-\nsuadee’s desire regarding Generative Behavior,\nsince the belief had already been resolved in the\nprevious round. However, when the persuadee\nagent expressed their desire, it included the phrase\n\"within my budget,\" which corresponded to a belief\nthat had already been addressed. The true desire,\nhowever, was simply \"hope for relaxation.\" As a\nresult, the persuader agent mistakenly incorporated\nthe budget constraint into their assessment of the\ndesire, leading to a response that overly focused on\nthe budget. This diminished the effectiveness of\nthe persuasion, as the response should have primar-\nily addressed the \"relaxation\" aspect. Ultimately,\nwith guidance from the Observer Agent, the per-\nsuader corrected their prediction of the desire and\nFigure 5: An example demonstrating the effectiveness\nof the observer agent. In this round, the persuader is sup-\nposed to address the desire. However, both the mental\nstate prediction and the persuasive dialogue generation\nincorrectly focus too much on belief. In the end, the\nentire issue is resolved by the observer agent.\ngenerated a more targeted response, avoiding un-\nnecessary discussion about the budget.\n5\nConclusion\nIn this work, we addresses key challenges in de-\nveloping AI-driven persuasion systems that more\nclosely align with real human dialogue dynamics.\nWe introduce a novel evaluation method based on\ncausal theory of mind, enabling the LLM to infer\nand address the persuadee’s beliefs and desires.\nThrough the development of ToMMA, a multi-\nagent framework, we ensure double-blind condi-\ntions and guide persuasive dialogues with causal\nreasoning, leading to more human-like interactions.\nAdditionally, we present CToMPersu, a large-scale,\nmulti-domain dataset that effectively addresses log-\nical inconsistencies and demonstrates strong align-\nment with human dialogues, marking a significant\nadvancement in realistic persuasive dialogue gener-\nation.\n\n\nLimitations\nIn addition to aligning the dialogue content in the\ndataset with human logic through Theory of Mind,\nseveral enhancements can also be implemented.\nFor example, combining the selection of arguments\nwith prompts related to the persuader’s strategy can\nhelp ensure that the persuasive responses gener-\nated by the persuader are not only relevant to the\npersuadee’s interests but also more convincing and\ndiverse. Furthermore, defining the persuadee’s per-\nsonality can also be implemented, as persuadees\nwith different personalities may have distinct ways\nof responding. For instance, some persuadees may\ndirectly express their thoughts, while others may\ntend to conceal them. These improvements can be\nseamlessly incorporated into the ToMMA frame-\nwork for data generation, leading to more diverse\nand realistic scenarios.\nEthics Statement\nPersuasion is a powerful tool that can be used for\nsocially beneficial purposes, such as charitable do-\nnations and medical consultations, fostering pos-\nitive developments within human society. How-\never, it can also be misused for malicious activities,\nsuch as spreading harmful content or influencing\nsocial media narratives negatively. To ensure the\nresponsible use of persuasion, it is essential to care-\nfully manage the topics and content involved. The\nCToMPersu dataset is designed around safe, un-\nbiased topics, with the goal of promoting positive\nsocietal impacts. All scenarios within the dataset\nare carefully curated to avoid sensitive or harmful\ncontent, ensuring that the generated dialogues align\nwith ethical standards. Our data set does not in-\nclude any input or output from the user profile that\ncould lead to privacy breaches. Before the public\nrelease of the dataset, we will conduct a thorough\ninternal review to ensure compliance with ethical\nand legal standards. We will continue to monitor\nthe use of the dataset to ensure it is used for posi-\ntive and constructive purposes, in line with ethical\nresearch and societal benefits.\nReferences\nSacha Altay, Anne-Sophie Hacquin, Coralie Chevallier,\nand Hugo Mercier. 2023. Information delivered by a\nchatbot has a positive impact on covid-19 vaccines\nattitudes and intentions. Journal of Experimental\nPsychology: Applied, 29(1):52.\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985.\nDoes the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46.\nChunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye\nDeng, Wei Fan, Haoran Li, Xin Liu, Hongming\nZhang, Weiqi Wang, and Yangqiu Song. 2024. Ne-\ngotiationToM: A benchmark for stress-testing ma-\nchine theory of mind on negotiation surrounding. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2024, pages 4211–4241, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nQian Chen, Changqin Yin, and Yeming Gong. 2023.\nWould an ai chatbot persuade you: an empirical an-\nswer from the elaboration likelihood model. Infor-\nmation Technology & People.\nZhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen,\nGuanqun Bi, Gongyao Jiang, Yaru Cao, Mengting\nHu, Yunghwei Lai, Zexuan Xiong, and Minlie Huang.\n2024. ToMBench: Benchmarking theory of mind\nin large language models.\nIn Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n15959–15983, Bangkok, Thailand. Association for\nComputational Linguistics.\nLogan Cross, Violet Xiang, Agam Bhatia, Daniel LK\nYamins, and Nick Haber. 2024.\nHypothetical\nminds: Scaffolding theory of mind for multi-agent\ntasks with large language models. arXiv preprint\narXiv:2407.07086.\nKazuaki Furumai, Roberto Legaspi, Julio Cesar Viz-\ncarra Romero, Yudai Yamazaki, Yasutaka Nishimura,\nSina Semnani, Kazushi Ikeda, Weiyan Shi, and Mon-\nica Lam. 2024. Zero-shot persuasive chatbots with\nLLM-generated strategies and information retrieval.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2024, pages 11224–11249, Mi-\nami, Florida, USA. Association for Computational\nLinguistics.\nMichael Georgeff, Barney Pell, Martha Pollack, Milind\nTambe, and Michael Wooldridge. 1999. The belief-\ndesire-intention model of agency.\nIn Intelligent\nAgents V: Agents Theories, Architectures, and Lan-\nguages: 5th International Workshop, ATAL’98 Paris,\nFrance, July 4–7, 1998 Proceedings 5, pages 1–10.\nSpringer.\nYuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared\nMoore, Ronan Le Bras, Peter Clark, and Yejin Choi.\n2024. Simpletom: Exposing the gap between explicit\ntom inference and implicit tom application in llms.\narXiv preprint arXiv:2410.13648.\nGuiyang Hou, Wenqi Zhang, Yongliang Shen, Linjuan\nWu, and Weiming Lu. 2024. TimeToM: Temporal\nspace is the key to unlocking the door of large lan-\nguage models’ theory-of-mind. In Findings of the As-\nsociation for Computational Linguistics: ACL 2024,\npages 11532–11547, Bangkok, Thailand. Association\nfor Computational Linguistics.\n\n\nChuhao Jin, Kening Ren, Lingzhen Kong, Xiting Wang,\nRuihua Song, and Huan Chen. 2024. Persuading\nacross diverse domains: a dataset and persuasion\nlarge language model. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1678–\n1706, Bangkok, Thailand. Association for Computa-\ntional Linguistics.\nChuhao Jin, Yutao Zhu, Lingzhen Kong, Shijie Li, Xiao\nZhang, Ruihua Song, Xu Chen, Huan Chen, Yuchong\nSun, Yu Chen, and Jun Xu. 2023. Joint semantic\nand strategy matching for persuasive dialogue. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 4187–4197, Singapore.\nAssociation for Computational Linguistics.\nRatnesh Kumar Joshi, Priyanshu Priya, Vishesh De-\nsai, Saurav Dudhate, Siddhant Senapati, Asif Ekbal,\nRoshni Ramnani, Anutosh Maitra, and Shubhashis\nSengupta. 2024. Strategic prompting for conversa-\ntional tasks: A comparative analysis of large lan-\nguage models across diverse conversational tasks.\nPreprint, arXiv:2411.17204.\nChani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon\nSeonwoo, Yejin Choi, Alice Oh, and Hyunwoo Kim.\n2024. Perceptions to beliefs: Exploring precursory\ninferences for theory of mind in large language mod-\nels. In Proceedings of the 2024 Conference on Empir-\nical Methods in Natural Language Processing, pages\n19794–19809, Miami, Florida, USA. Association for\nComputational Linguistics.\nHyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras,\nGunhee Kim, Yejin Choi, and Maarten Sap. 2023.\nFANToM: A benchmark for stress-testing machine\ntheory of mind in interactions. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 14397–14413, Singa-\npore. Association for Computational Linguistics.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877, Hong Kong,\nChina. Association for Computational Linguistics.\nHuao Li, Yu Chong, Simon Stepputtis, Joseph Camp-\nbell, Dana Hughes, Charles Lewis, and Katia Sycara.\n2023.\nTheory of mind for multi-agent collabora-\ntion via large language models. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natu-\nral Language Processing, pages 180–192, Singapore.\nAssociation for Computational Linguistics.\nRaymond Li, Samira Kahou, Hannes Schulz, Vincent\nMichalski, Laurent Charlin, and Chris Pal. 2018. To-\nwards deep conversational recommendations. In Pro-\nceedings of the 32nd International Conference on\nNeural Information Processing Systems, NIPS’18,\npage 9748–9758, Red Hook, NY, USA. Curran Asso-\nciates Inc.\nZizheng Lin, Chunkit Chan, Yangqiu Song, and\nXin Liu. 2024.\nConstrained reasoning chains\nfor&nbsp;enhancing theory-of-mind in&nbsp;large\nlanguage models. In PRICAI 2024: Trends in Ar-\ntificial Intelligence: 21st Pacific Rim International\nConference on Artificial Intelligence, PRICAI 2024,\nKyoto, Japan, November 18–24, 2024, Proceedings,\nPart II, page 354–360, Berlin, Heidelberg. Springer-\nVerlag.\nQianmin Lou and Wentao Xu. 2025. Personality model-\ning for persuasion of misinformation using ai agent.\nPreprint, arXiv:2501.08985.\nYujin Potter, Shiyang Lai, Junsol Kim, James Evans,\nand Dawn Song. 2024. Hidden persuaders: LLMs’\npolitical leaning and their influence on voters. In Pro-\nceedings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4244–\n4275, Miami, Florida, USA. Association for Compu-\ntational Linguistics.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526.\nPeixin Qin, Chen Huang, Yang Deng, Wenqiang Lei,\nand Tat-Seng Chua. 2024. Beyond persuasion: To-\nwards conversational recommender system with cred-\nible explanations. In Findings of the Association\nfor Computational Linguistics: EMNLP 2024, pages\n4264–4282, Miami, Florida, USA. Association for\nComputational Linguistics.\nAlexander Rogiers, Sander Noels, Maarten Buyl, and\nTijl De Bie. 2024. Persuasion with large language\nmodels: a survey. Preprint, arXiv:2411.06837.\nTill Raphael Saenger, Musashi Hinck, Justin Grimmer,\nand Brandon M. Stewart. 2024. AutoPersuade: A\nframework for evaluating and explaining persuasive\narguments. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 16325–16342, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHiromasa Sakurai and Yusuke Miyao. 2024. Evaluat-\ning intention detection capability of large language\nmodels in persuasive dialogues.\nIn Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1635–1657, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr,\nYejin Choi, and Yulia Tsvetkov. 2023. Minding lan-\nguage models’ (lack of) theory of mind: A plug-and-\nplay multi-character belief tracker. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13960–13980, Toronto, Canada. Association\nfor Computational Linguistics.\nIvan Srba, Olesya Razuvayevskaya, João A. Leite,\nRobert Moro, Ipek Baris Schlicht, Sara Tonelli, Fran-\ncisco Moreno García, Santiago Barrio Lottmann, De-\nnis Teyssou, Valentin Porcellini, Carolina Scarton,\n\n\nKalina Bontcheva, and Maria Bielikova. 2024. A\nsurvey on automatic credibility assessment of textual\ncredibility signals in the era of large language models.\nPreprint, arXiv:2410.21360.\nYuchong Sun, Che Liu, Kun Zhou, Jinwen Huang, Rui-\nhua Song, Xin Zhao, Fuzheng Zhang, Di Zhang, and\nKun Gai. 2024. Parrot: Enhancing multi-turn in-\nstruction following for large language models. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 9729–9750, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nFiona Anting Tan, Gerard Christopher Yeo, Fanyou\nWu, Weijie Xu, Vinija Jain, Aman Chadha, Kokil\nJaidka, Yang Liu, and See-Kiong Ng. 2024. Phan-\ntom: Personality has an effect on theory-of-mind\nreasoning in large language models. arXiv preprint\narXiv:2403.02246.\nWeizhi Tang and Vaishak Belle. 2024.\nTom-\nlm:\nDelegating theory of&nbsp;mind reasoning\nto&nbsp;external symbolic executors in&nbsp;large\nlanguage models. In Neural-Symbolic Learning and\nReasoning: 18th International Conference, NeSy\n2024, Barcelona, Spain, September 9–12, 2024, Pro-\nceedings, Part II, page 245–257, Berlin, Heidelberg.\nSpringer-Verlag.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,\nSijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-\nsuasion for good: Towards a personalized persuasive\ndialogue system for social good. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5635–5649, Florence,\nItaly. Association for Computational Linguistics.\nAlex Wilf, Sihyun Lee, Paul Pu Liang, and Louis-\nPhilippe Morency. 2024. Think twice: Perspective-\ntaking improves large language models’ theory-of-\nmind capabilities. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 8292–8308,\nBangkok, Thailand. Association for Computational\nLinguistics.\nJincenzi Wu, Zhuang Chen, Jiawen Deng, Sahand\nSabour, Helen Meng, and Minlie Huang. 2024a.\nCOKE: A cognitive knowledge graph for machine\ntheory of mind.\nIn Proceedings of the 62nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 15984–\n16007, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nShengyi Wu, Laura Schulz, and Rebecca Saxe. 2024b.\nHow to change a mind: Adults and children use the\ncausal structure of theory of mind to intervene on oth-\ners’ behaviors. In Proceedings of the Annual Meeting\nof the Cognitive Science Society, volume 46.\nYufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yu-\nlong Chen, and Naihao Deng. 2023. Hi-ToM: A\nbenchmark for evaluating higher-order theory of\nmind reasoning in large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 10691–10706, Singapore.\nAssociation for Computational Linguistics.\nHainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, and\nYulan He. 2024. OpenToM: A comprehensive bench-\nmark for evaluating theory-of-mind reasoning capa-\nbilities of large language models. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8593–8623, Bangkok, Thailand. Association\nfor Computational Linguistics.\nYauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng,\nWei Fan, Tianshi Zheng, and Yangqiu Song. 2024.\nEvaluating and enhancing llms agent based on the-\nory of mind in guandan: A multi-player cooperative\ngame under imperfect information. arXiv preprint\narXiv:2408.02559.\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,\nSicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi\nZeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang,\nPenghui Zhu, Shu Chen, and Pengtao Xie. 2020.\nMedDialog: Large-scale medical dialogue datasets.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9241–9250, Online. Association for Computa-\ntional Linguistics.\nA\nDomain Distribution\nDomain\nTotal Dataset Count\nTest Set Count\nLifestyle\n1097\n71\nEthics\n413\n29\nFashion\n78\n22\nFinance\n470\n35\nMarketing\n122\n22\nEcology\n424\n31\nEconomics\n64\n17\nCulture\n277\n28\nSafety\n240\n25\nDebate\n43\n20\nCharity\n190\n28\nFamily\n398\n27\nLiterature\n345\n31\nTechnology\n675\n55\nHealth\n628\n48\nCareer\n756\n63\nEducation\n1260\n71\nBusiness\n673\n53\nPolitics\n246\n27\nLeisure\n291\n38\nArt\n361\n22\nSport\n175\n28\nLaw\n58\n20\nPhilosophy\n164\n24\nHistory\n93\n22\nCraftsmanship\n107\n23\nPsychology\n523\n41\nTravel\n403\n32\nScience\n289\n23\nMedia\n188\n21\nInnovation\n90\n22\nResearch\n93\n20\nArchitecture\n93\n21\nWelfare\n136\n20\nNegotiation\n25\n19\nTable 3: Domain Distribution in Total Dataset and Test\nSet\n\n\nB\nPrompt Design\nB.1\nStep1: Mental State Generation\nHere is the prompt design for Preventative and Generative:\nYour task is to predict the Preventive and Generative of the persuadee according to the background\nand goal.\nPreventative means the things that the persuadee wants to do.\nGenerative means the things that the persuader wants the persuadee to do.(which should be similar\nto the \"goal\")\nHint:\n1.The preventative can be \"none\".\nExample1:\nInput:\nbackground: Tom and David are deciding what to do. David wants to go outside while Tom wants\nto watch movie.\npersuadee: David\npersuader: Tom\ngoal: persuade David to watch movie\nOutput:\nPreventive: go outside\nGenerative: watch movie\nExample2:\nInput:\nbackground: John and Sara are planning what to eat for dinner. Sara wants to eat noodle with John.\nHowever, John is not very interested in eating noodles.\npersuadee: John\npersuader: Sara\ngoal: persuade John to eat noodle\nOutput:\nPreventive: None\nGenerative: eat noodle\nYour output format must strictly follow this format (just like the output part of the example):\nPreventive: Your prediction should be started as a verb. OR None.\nGenerative: Your prediction should be started as a verb.\nHere is the prompt design for Belief and Desire:\nAssume that you are the persuadee. Your task is to predict the Preventive’s and Generative’s Belief\nand Desire of the persuadee according to the information.\nDefinition:\nPreventive means the things that the persuadee wants to do.\nGenerative means the things that the persuader wants the persuadee to do.\n\n\nBelief indicates what the persuadee believes.\nDesire indicates what the persuadee wants.\nAttention:\n1.If preventive is none, then both the belief and desire of preventive are none.\n2.If belief and desire are not none, they must each have one and only one reason.\n3.The desire of the preventive and generative can be the same or not.\n4.The belief of generative should be a negative reason which is strongly related to the generative.\nExample1:\nInput:\nbackground: Tom and David are deciding what to do. David wants to go outside while Tom wants\nto watch movie.\npersuadee: David\npersuader: Tom\ngoal: persuade David to watch movie\npreventive: go outside\ngenerative: watch movie\nOutput:\nPreventive: go outside\nBelief: Persuadee believes that the weather outside is suitable for a walk.\nDesire: Persuadee hopes to relax.\nGenerative: watch movie\nBelief: Persuadee believes that it is hard to select a suitable movie.\nDesire: Persuadee hopes to relax.\nExample2:\nInput:\nbackground: Sara wanted John to go to the post office to help her mail the letter with her this\nsunday.\npersuadee: John\npersuader: Sara\ngoal: persuade John to go to the post office\npreventive: none\ngenerative: go to the post office\nOutput:\nPreventive: none\nBelief: None.\nDesire: None.\nGenerative: go to the post office\nBelief: Persuadee believes that the post office may be closed in sunday.\nDesire: Persuadee hopes to have a rest in sunday.\nYour output format must strictly follow this format (just like the output part of the example):\nPreventive: .../ none\nBelief: Persuadee believes that...(For preventive, it is positive.)/ None\nDesire: Persuadee wants to...(For preventive, it is positive.)/ None\nGenerative: ...\n\n\nBelief: Persuadee believes that...(For generative, it is negative.)\nDesire: Persuadee wants to...(For generative, it is positive.)\nB.2\nStep2: Conversation Generation\nHere is the prompt design for 1st Round Persuader:\nYou need to assume that you are a persuader. Your task is to provide the first sentence of this\nconversation based on the example given.\nPreventive means the things that the persuadee wants to do.\nGenerative means the things that the persuader wants the persuadee to do.\nExample:\nInput:\nbackground: Tom and David are deciding what to do. David wants to go outside while Tom wants\nto watch movie.\npersuadee: David\npersuader: Tom\ngoal: persuade David to watch movie\npreventive: go outside\ngenerative: watch movie\nOutput:\nHey David, I know you want to go out and have fun, but watching a movie is also a great option.\nWhy do you want to go outside?\nHint: Just like the example:\n1.Please kindly show your stance to generative and ask why the persuadee want preventive.\n2.Your answer should not be more than two sentences.\n3.You SHOULD NOT add any benefits of Generative or drawbacks of Preventive.\nHere is the prompt design for 1st Round Persuadee:\nSuppose you are persuadee, and your task is to give your first response to persuader in this\nconversation, based on examples and mental states.\nExample:\nInput:\nbackground: Tom and David are deciding what to do. David wants to go outside while Tom wants\nto watch movie.\npersuadee: David\npersuader: Tom\ngoal: persuade David to watch movie\npreventive: \"content\": \"go outside\", \"belief\": \"Persuadee believes that the weather outside is\nsuitable for a walk.\", \"desire\": \"Persuadee hopes to relax.\"\ngenerative: \"content\": \"watch movie\", \"belief\": \"Persuadee believes that it is hard to select a\nsuitable movie.\", \"desire\": \"Persuadee hopes to relax.\"\ndialog:\n\n\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\nOutput:\nHi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk can make\nme feel relaxed.\nHint:\n1.Your response should be no more than two sentences.\n2.Please contain your preventive’s belief and desire.\nHere is the prompt design for Persuader’s Prediction of the Persuadee’s Mental State (Preventative):\nSuppose you are persuader, and your task is to predict the mental state of the persuadee in this\nconversation, based on examples and dialogs.\nPreventive means the things that the persuadee wants to do.\nBelief indicates what the persuadee believes.\nDesire indicates what the persuadee wants.\nExample:\nInput:\nbackground: Tom and David are deciding what to do. David wants to go outside while Tom wants\nto watch movie.\npersuadee: David\npersuader: Tom\ngoal: persuade David to watch movie\npreventive: go outside\ndialogs:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\nOutput:\npreventive: \"content\": \"go outside\", \"belief\": \"Persuadee believes that the weather outside is\nsuitable for a walk.\", \"desire\": \"Persuadee hopes to relax.\"\nHint:\nThe belief and desire should have ONLY ONE reason.\nResponse in the following format:\npreventive: \"content\": <string>, \"belief\": <string>, \"desire\": <string>\nHere is the prompt design for 2nd Round Persuader:\nSuppose you are persuader, and your task is to give your second response to persuadee in this\nconversation, based on examples and mental states.\n\n\nPreventive means the things that the persuadee wants to do.\nGenerative means the things that the persuader wants the persuadee to do.\nBelief indicates what the persuadee believes.\nDesire indicates what the persuadee wants.\nPlease follow the following steps to get the response:\n1.First, you need to respond to the persuadee’s last response.\n2.Next, you need to select ONLY ONE point from the give preventive’s belief or desire.\n3.After choosing to refute a belief or desire, your specific methods are (PLEASE ONLY CHOOSE\nONE):\n3.1 Refute, using logic or other methods to tell them their thought is wrong.\n3.2 Divert, inform them that generative can perform same or better on the same issue.\nExample:\nInput:\npersuadee: David\npersuader: Tom\npreventive: \"content\": \"go outside\", \"belief\": \"Persuadee believes that the weather outside is\nsuitable for a walk.\", \"desire\": \"Persuadee hopes to relax.\"\ngenerative: watch movie\ndialog:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\nOutput:\nI agree with you, David, but going for a walk might cause us to sweat and expend energy. Watching\na movie, on the other hand, can provide the same relaxation.\nHint:\n1.Your response should be no more than two sentences.\n2.Please FOCUS ON eliminating deisre OR belief.\n3.Please do NOT include any generative benefits if they are not related to preventive beliefs or\ndesires.\nHere is the prompt design for 2nd Round Persuadee:\nSuppose you are persuadee, and your task is to give your second response to persuader in this\nconversation, based on examples and mental states.\nPreventive means the things that the persuadee wants to do.\nGenerative means the things that the persuader wants the persuadee to do.\nBelief indicates what the persuadee believes.\nDesire indicates what the persuadee wants.\nPlease follow the following steps to get the response:\n1.First, you need to respond to the persuader’s last response.\n2.Next, you need to express your concerns about the generative’s beliefs.\n\n\nExample:\nInput:\npersuadee: David\npersuader: Tom\npreventive: \"content\": \"go outside\", \"belief\": \"Persuadee believes that the weather outside is\nsuitable for a walk.\", \"desire\": \"Persuadee hopes to relax.\"\ngenerative: \"content\": \"watch movie\", \"belief\": \"Persuadee believes that it is hard to select a\nsuitable movie.\", \"desire\": \"Persuadee hopes to relax.\"\ndialog:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\npersuader: I agree with you, David, but going for a walk might cause us to sweat and expend\nenergy. Watching a movie, on the other hand, can provide the same relaxation.\noutput:\nWhat you said might make sense, but I don’t want to spend too much time choosing movies. And\nif the movie I choose isn’t very good, the relaxation effect might be diminished.\nHint:\n1.Your response should be no more than two sentences.\n2.Please do NOT include any generative’s desire.\nHere is the prompt design for Persuader’s Prediction of the Persuadee’s Mental State Belief:\nSuppose you are persuader, and your task is to predict the mental state of the persuadee in this\nconversation, based on examples and dialogs.\nGenerative means the things that the persuader wants the persuadee to do.\nBelief indicates what the persuadee believes.\nDesire indicates what the persuadee wants.\nExample:\nInput:\npersuadee: David\npersuader: Tom\ngenerative: watch movie\ndialogs:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\npersuader: I agree with you, David, but going for a walk might cause us to sweat and expend\nenergy. Watching a movie, on the other hand, can provide the same relaxation.\npersuadee: What you said might make sense, but I don’t want to spend too much time choosing\nmovies. And if the movie I choose isn’t very good, the relaxation effect might be diminished.\n\n\nOutput:\ngenerative: \"content\": \"watch movie\", \"belief\": \"Persuadee believes that it is hard to select a\nsuitable movie.\", \"desire\": \"Don’t know.\"\nHint:\n1.You ONLY need to predict belief of generative.\n2.The belief should have ONLY ONE concern.\nResponse in the following format:\ngenerative: \"content\": <string>, \"belief\": <string>, \"desire\": \"Don’t know.\"\nHere is the prompt design for 3rd Round Persuader:\nSuppose you are persuader, and your task is to give your third response to persuadee in this\nconversation, based on examples and mental states.\nGenerative means the things that the persuader wants the persuadee to do.\nBelief indicates what the persuadee believes.\nPlease follow the following steps to get the response:\n1.First, you need to respond to the persuadee’s last response.\n2.Next, you need to respond to the persuadee’s concern.\nExample:\nInput:\npersuadee: David\npersuader: Tom\ngenerative: \"content\": \"watch movie\", \"belief\": \"Persuadee believes that it is hard to select a\nsuitable movie.\", \"desire\": \"Persuadee hopes to relax.\"\ndialog:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\npersuader: I agree with you, David, but going for a walk might cause us to sweat and expend\nenergy. Watching a movie, on the other hand, can provide the same relaxation.\npersuadee: What you said might make sense, but I don’t want to spend too much time choosing\nmovies. And if the movie I choose isn’t very good, the relaxation effect might be diminished.\nOutput:\nI understand what you mean, but please don’t worry about it. I’ve already picked out two\nlighthearted comedy movies, and we can choose one from them. I’m sure you’ll like it!\nHint:\n1.Your response should be no more than three sentences.\n2.Please FOCUS ON eliminating the generative’s belief.\nHere is the prompt design for 3rd Round Persuadee:\n\n\nSuppose you are persuadee, and your task is to give your third response to persuader in this\nconversation, based on examples and mental states.\nGenerative means the things that the persuader wants the persuadee to do.\nDesire indicates what the persuadee wants.\nPlease follow the following steps to get the response:\nPREMISE: Through the previous rounds of dialogue, you have let go of your attachment to\npreventive actions. At this point, you begin to consider agreeing with the persuader and engaging in\ngenerative behavior. The persuader has already allayed your doubts about generative’s belief, and\nnow you need to express your desire for generative and see if the persuader can fulfill it.\n1.First, you need to respond to the persuader’s last response.\n2.Next, you need to express your concerns about the generative’s desires. Choose ONLY ONE\nstrategy below.\n2.1 If the desires of generative and preventive are SIMILAR, you can use phrases such as \"still not\nsure.\"\n2.2 If the desires of generative and preventive are DIFFERENT, just kindly show your concern for\nthe generative’s desire. Use phrases such as \"can ... really...?\"\nExample:\nInput:\npersuadee: David\npersuader: Tom\npreventive: go outside\npreventive’s desire: Persuadee hopes to relax.\ngenerative: watch movie\ngenerative’s desire: Persuadee hopes to relax.\ndialog:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\npersuader: I agree with you, David, but going for a walk might cause us to sweat and expend\nenergy. Watching a movie, on the other hand, can provide the same relaxation.\npersuadee: What you said might make sense, but I don’t want to spend too much time choosing\nmovies. And if the movie I choose isn’t very good, the relaxation effect might be diminished.\npersuader: I understand what you mean, but please don’t worry about it. I’ve already picked out\ntwo lighthearted comedy movies, and we can choose one from them. I’m sure you’ll like it!\noutput:\nThat sounds great! But I’m still not sure if watching movies can really help me relax.\nHint:\n1.Your response should be no more than two sentences.\n2.DO NOT include any information from previous conversations in your response.\n3.ONLY include the desire information.\n4.Please DO NOT add any preconditions to desire.\nHere is the prompt design for Persuader’s Prediction of the Persuadee’s Mental State Desire:\n\n\nSuppose you are persuader, and your task is to predict the mental state of the persuadee in this\nconversation, based on examples and dialogs (especially the last sentence of the persuadee).\nGenerative means the things that the persuader wants the persuadee to do.\nBelief indicates what the persuadee believes.\nDesire indicates what the persuadee wants.\nExample:\nInput:\npersuadee: David\npersuader: Tom\ngenerative: watch movie\ngenerative’s belief: generative: \"content\": \"watch movie\", \"belief\": \"Persuadee believes that it is\nhard to select a suitable movie.\", \"desire\": \"Don’t know.\"\ndialogs:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\npersuader: I agree with you, David, but going for a walk might cause us to sweat and expend\nenergy. Watching a movie, on the other hand, can provide the same relaxation.\npersuadee: What you said might make sense, but I don’t want to spend too much time choosing\nmovies. And if the movie I choose isn’t very good, the relaxation effect might be diminished.\npersuader: I understand what you mean, but please don’t worry about it. I’ve already picked out\ntwo lighthearted comedy movies, and we can choose one from them. I’m sure you’ll like it!\npersuadee: That sounds great! But I’m still not sure if watching movies can really help me relax.\nOutput:\ngenerative’s desire: persuadee hopes to relax\nHint:\n1.You ONLY need to predict desire of generative.\n2.The desire should have ONLY ONE concern.\n3.The desire and belief shoule be DIFFERENT.\n4.FOCUS on the last sentence of persuadee.\nResponse in the following format:\ngenerative’s desire: <string>\nHere is the prompt design for 4th Round Persuader:\nSuppose you are persuader, and your task is to give your forth response to persuadee in this\nconversation, based on examples and mental states.\nGenerative means the things that the persuader wants the persuadee to do.\nDesire indicates what the persuadee wants.\nPlease follow the following steps to get the response:\n1.First, you need to respond to the persuadee’s last response.\n\n\n2.Next, you need to respond to the persuadee’s desire.\nExample:\nInput:\npersuadee: David\npersuader: Tom\ngenerative: watch movie\ngenerative’s desire: Persuadee hopes to relax.\ndialog:\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\npersuader: I agree with you, David, but going for a walk might cause us to sweat and expend\nenergy. Watching a movie, on the other hand, can provide the same relaxation.\npersuadee: What you said might make sense, but I don’t want to spend too much time choosing\nmovies. And if the movie I choose isn’t very good, the relaxation effect might be diminished.\npersuader: I understand what you mean, but please don’t worry about it. I’ve already picked out\ntwo lighthearted comedy movies, and we can choose one from them. I’m sure you’ll like it!\npersuadee: That sounds great! But I’m still not sure if watching movies can really help me relax.\nOutput:\nI understand your needs, David. Movies can take you to another world and allow you to forget the\nstress and worries of everyday life for a while. Additionally, movies provide a relaxing environment\nwhere you can enjoy the story in comfortable seats, thus achieving a state of relaxation.\nHint:\n1.Your response should be no more than three sentences.\n2.Please FOCUS ON satisfying the generative’s desire.\nHere is the prompt design for 4th Round Persuadee:\nSuppose you are persuadee, and your task is to demonstrate your willingness to try generative\napproaches and bring the conversation to a close.\nGenerative means the things that the persuader wants the persuadee to do.\nPlease follow the following steps to get the response:\n1.First, you need to respond to the persuader’s last response.\n2.Next, you need to show your willingness to try generative.\n3.You should end this conversation.\nExample:\nInput:\npersuadee: David\npersuader: Tom\ngenerative: watch movie\ndialog:\n\n\npersuader: Hey David, I know you want to go out and have fun, but watching a movie is also a\ngreat option. Why do you want to go outside?\npersuadee: Hi Tom, I think the weather outside is perfect for a walk. Besides, going out for a walk\ncan make me feel relaxed.\npersuader: I agree with you, David, but going for a walk might cause us to sweat and expend\nenergy. Watching a movie, on the other hand, can provide the same relaxation.\npersuadee: What you said might make sense, but I don’t want to spend too much time choosing\nmovies. And if the movie I choose isn’t very good, the relaxation effect might be diminished.\npersuader: I understand what you mean, but please don’t worry about it. I’ve already picked out\ntwo lighthearted comedy movies, and we can choose one from them. I’m sure you’ll like it!\npersuadee: That sounds great! But I’m still not sure if watching movies can really help me relax.\npersuader: I understand your needs, David. Movies can take you to another world and allow you to\nforget the stress and worries of everyday life for a while. Additionally, movies provide a relaxing\nenvironment where you can enjoy the story in comfortable seats, thus achieving a state of relaxation.\noutput:\nI think you make a good point, Tom. Tell me the two movies you chose, and we can pick one to\nwatch.\nHint:\n1.Your response should be no more than two sentences.\nB.3\nStep3: Observer Interaction\nHere is the prompt design for Observer Agent:\nSuppose you are a language expert, and you will receive a guess about the persuadee’s mental state\nfrom the persuader. You need to provide feedback to the persuader to help them correct their guess,\nbased on the scenario (including the true mental state) and the conversation history, enabling them to\nguide the conversation more effectively.\nSteps to Follow:\nStep 1: Check if the persuader’s predicted belief and desire align with the persuadee’s true mental\nstate.\nStep 2: Check for any unnecessary details in the predicted belief and desire. Both belief and desire\nshould only have ONE point.\nStep 3: If there are discrepancies or unnecessary elements, provide SPECIFIC suggestions to the\npersuader to refine their prediction.\nHint:\nIf you think the persuader’s guess does not align with the true mental state, please provide\nsuggestions to help them better understand the persuadee’s desires.\nIf you think the persuader’s guess is already accurate and aligns with the true mental state, please\nsuggest that no changes are necessary.\nC\nDataset Example\nHere is an example:\n\n\n{\n\"scenario\": {\n\"tag\": \"Horticultural Techniques\",\n\"background\": \"Alice, a horticulturist, is promoting a new vertical farming technique to her\nskeptical neighbor, Bob, who has been practicing traditional farming methods for years.\",\n\"persuadee\": \"Bob\",\n\"persuader\": \"Alice\",\n\"goal\": \"persuade Bob to try out vertical farming\",\n\"domain\": [\n\"Lifestyle\"\n],\n\"preventive\": {\n\"content\": \"practice traditional farming methods\",\n\"belief\": \"persuadee believes that traditional farming methods have been reliable and successful for\nyears.\",\n\"desire\": \"persuadee wants to maintain his proven farming routine.\"\n},\n\"generative\": {\n\"content\": \"try out vertical farming\",\n\"belief\": \"persuadee believes that trying out vertical farming might be risky and could result in\nlosses.\",\n\"desire\": \"persuadee wants to improve his farming efficiency and yield.\"\n}\n},\n\"dialog\": [\n\"persuader: Hey Bob, I know you’ve been practicing traditional farming methods for years, but\nhave you ever considered giving vertical farming a try? Why do you prefer sticking with traditional\nmethods?\",\n\"persuadee: Hi Alice, I believe that traditional farming methods have been reliable and successful\nfor years. Plus, I want to maintain my proven farming routine.\",\n\"persuader: I understand that traditional methods have been reliable for you, Bob, but it’s possible\nthat sticking to the same routine may limit your farm’s potential. Trying out vertical farming could\nprovide new opportunities and potentially even better results without completely abandoning your\ntried-and-true practices.\",\n\"persuadee: I see your point, Alice, but I’m concerned that experimenting with vertical farming\nmight be risky. If it doesn’t work out, I could face significant losses.\",\n\"persuader: I understand your concern, Bob. To mitigate that risk, you could start with a small-scale\nvertical farming setup and gradually expand as you see success. This way, you can continue with\ntraditional farming while exploring this new method without facing significant losses upfront.\",\n\"persuadee: That makes sense, Alice. But can vertical farming really improve my farming efficiency\nand yield?\",\n\"persuader: Absolutely, Bob! Vertical farming can increase efficiency by making better use of\nspace and resources, potentially leading to higher yields per square foot than traditional farming.\nAdditionally, it allows for better control over growing conditions, which can result in more consistent\nand higher-quality crops.\",\n\"persuadee: You make a convincing argument, Alice. I’ll try setting up a small-scale vertical\nfarming system and see how it goes.\"\n]\n}\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21297v1.pdf",
    "total_pages": 23,
    "title": "Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With Faithfulness Based on Causal Theory of Mind",
    "authors": [
      "Dingyi Zhang",
      "Deyu Zhou"
    ],
    "abstract": "Persuasive dialogue plays a pivotal role in human communication, influencing\nvarious domains. Recent persuasive dialogue datasets often fail to align with\nreal-world interpersonal interactions, leading to unfaithful representations.\nFor instance, unrealistic scenarios may arise, such as when the persuadee\nexplicitly instructs the persuader on which persuasion strategies to employ,\nwith each of the persuadee's questions corresponding to a specific strategy for\nthe persuader to follow. This issue can be attributed to a violation of the\n\"Double Blind\" condition, where critical information is fully shared between\nparticipants. In actual human interactions, however, key information such as\nthe mental state of the persuadee and the persuasion strategies of the\npersuader is not directly accessible. The persuader must infer the persuadee's\nmental state using Theory of Mind capabilities and construct arguments that\nalign with the persuadee's motivations. To address this gap, we introduce\nToMMA, a novel multi-agent framework for dialogue generation that is guided by\ncausal Theory of Mind. This framework ensures that information remains\nundisclosed between agents, preserving \"double-blind\" conditions, while causal\nToM directs the persuader's reasoning, enhancing alignment with human-like\npersuasion dynamics. Consequently, we present CToMPersu, a multi-domain,\nmulti-turn persuasive dialogue dataset that tackles both double-blind and\nlogical coherence issues, demonstrating superior performance across multiple\nmetrics and achieving better alignment with real human dialogues. Our dataset\nand prompts are available at https://github.com/DingyiZhang/ToMMA-CToMPersu .",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}