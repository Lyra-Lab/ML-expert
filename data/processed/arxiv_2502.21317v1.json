{
  "id": "arxiv_2502.21317v1",
  "text": "Assessing zero-shot generalisation behaviour in\ngraph-neural-network interatomic potentials\nChiheb Ben Mahmoud*1, Zakariya El-Machachi1, Krystian A. Gierczak1,\nJohn L. A. Gardner1, and Volker L. Deringer1\n1Inorganic Chemistry Laboratory, Department of Chemistry, University of Oxford, Oxford\nOX1 3QR, UK\n*chiheb.benmahmoud@chem.ox.ac.uk\n1\narXiv:2502.21317v1  [physics.chem-ph]  28 Feb 2025\n\n\nAbstract\nWith the rapidly growing availability of machine-learned interatomic potential (MLIP)\nmodels for chemistry, much current research focuses on the development of generally\napplicable and “foundational” MLIPs. An important question in this context is whether,\nand how well, such models can transfer from one application domain to another. Here,\nwe assess this transferability for an MLIP model at the interface of materials and molec-\nular chemistry. Specifically, we study GO-MACE-23, a model designed for the extended\ncovalent network of graphene oxide, and quantify its zero-shot performance for small,\nisolated molecules and chemical reactions outside its direct scope—in direct compari-\nson with a state-of-the-art model which has been trained in-domain. Our work provides\nquantitative insight into the transfer and generalisation ability of graph-neural-network\npotentials and, more generally, makes a step towards the more widespread applicability\nof MLIPs in chemistry.\nIntroduction\nMachine-learned interatomic potentials (MLIPs) for atomistic simulations, trained on quantum-\nmechanical energy and force data, have advanced remarkably in recent years1–3 and now\nalmost routinely allow researchers to address a wide range of questions in chemistry and ma-\nterials science4–7. Recently, MLIPs incorporating graph-based representations, commonly\nreferred to as graph neural networks (GNNs)8–11, have emerged as cost-effective yet chemi-\ncally rich models of atomic interactions. The favourable scaling of GNN-based MLIPs with\nthe number of atomic species means that they are, in principle, able to cover elements from\nacross the Periodic Table all in a single model11–14.\nThe enhanced chemical versatility provided by GNNs has inspired the development of so-\ncalled “pre-trained”11, “foundational”12, or “universal”14,15 interatomic potentials. These\nmodels have been trained on large, structurally and chemically diverse datasets; they show\npromising baseline performance for a range of systems16,17 and thus provide a practical tool\nfor starting computational projects, as well as a basis for fine-tuning18. In the long run,\n2\n\n\none might want to employ these pre-trained MLIPs “as is”, in a zero-shot manner, without\nadditional training or adaptation. Zero-shot performance also yields an important indication\nof how well the underlying model generalises to unseen tasks and chemistries. Understanding\nand improving the zero-shot behaviour of MLIPs is therefore an important challenge.\nHerein, we study the zero-shot generalisation behaviour of GO-MACE-23 (Ref. 19), an MLIP\nmodel that was initially developed specifically for graphene oxide (GO). Conceptually, GO\nbridges the gap between pristine graphene and organic chemistry: its structural landscape\ninvolves a variety of bonding motifs from sp2 carbon sheets to oxygen-rich domains and re-\nactive edge sites20. We test whether this structural and chemical complexity may serve as\na basis for transferability (albeit initially we thought of GO-MACE-23 as a single- rather than\ngeneral-purpose MLIP!), subjecting GO-MACE-23 to a range of out-of-domain benchmarks,\nfrom energetics to high-temperature molecular-dynamics (MD) simulations of chemical reac-\ntions. In this way, our present study explores: (i) the role of a chemically rich training dataset\nin building robust and generalisable MLIPs21; (ii) the importance of GNN-based architec-\ntures in doing so; and (iii) the question whether GO-MACE-23 could form a starting point for\nfoundational MLIPs bridging materials and molecular chemistry.\nMethodology\nThe GO-MACE-23 and MACE-OFF models\nWe focus on the GO-MACE-23 model, which was built using the MACE architecture9,10 to-\ngether with a bespoke data-generation protocol19. Initial training data were generated “from\nscratch” using CASTEP+ML22 (accelerating ab initio MD through on-the-fly fitting of GAP\nmodels23), and then largely augmented through subsequent iterative training from MD tra-\njectories driven by intermediate versions of MACE models. Over time, configurations with\nfunctionalised edges, involving hydroxyl (–OH), aldehyde (–CHO), and carboxylic acid\n(–CO2H) moieties, were added to ensure good coverage of the structural and chemical fea-\ntures that might be expected to appear in a “real-world” GO sheet. Training labels, viz. total\n3\n\n\nenergies and forces, were obtained from density-functional-theory (DFT) computations per-\nformed with the plane-wave software CASTEP24 using on-the-fly generated pseudopotentials\nand the Perdew–Burke–Ernzerhof (PBE) exchange–correlation functional25.\nAs a baseline for the current state-of-the-art (SOTA) in molecular modelling, we choose two\nvariants of the MACE-OFF family of MLIPs26: the “large” version of MACE-OFF23, commonly\nreferred to as MACE-OFF23(L), which is trained on the SPICE dataset of molecular data ver-\nsion 127, and MACE-OFF24, which is trained on the SPICE dataset version 228. MACE-OFF24 is\nmore similar to GO-MACE-23 in terms of architecture, with the exception of the radial cut-off:\n3.7 ˚Afor GO-MACE-23 and 6.0 ˚Afor MACE-OFF24. More details about the hyperparameters of\nall the GNNs used in this work are provided in the Supplementary Information. In the re-\nmainder of this work, we refer to MACE-OFF23(L) simply as MACE-OFF23. In using MACE-OFF\nmodels as benchmarks, it is important to note the different DFT levels of theory compared\nto GO-MACE-23: the SPICE labels were obtained from DFT computations with the ωB97M-\nD3(BJ) exchange–correlation functional29,30 and the def2-TZVPPD basis set31,32.\nBenchmark data\nWe carry out initial tests using the revised version of the MD17 dataset (rMD17)33. We select\nthe 6 molecules from rMD17 that only contain the elements C, H, and O – the only ones in\nthe GO dataset, and thus the only ones that GO-MACE-23 and other models directly fitted to\nits dataset can handle. For each molecule, we randomly select 1,000 configurations from the\navailable trajectories. The rMD17 labels were obtained in the original work using the PBE\nfunctional and the def2-SVP basis set25,31.\nThe other test sets used in the present study are generated either by running MD simulations\nin the NVT ensemble or by relaxing molecules. In both cases, we use GO-MACE-23 to perform\nthese tasks. We compute reference data using DFT, matching the settings for GO-MACE-23\nand MACE-OFF, where applicable. For comparison to GO-MACE-23, labels are obtained from\nCASTEP by placing the molecules in large periodic cells (> 20 ˚A). For MACE-OFF, compati-\nble labels are obtained using the Atomic Simulation Environment (ASE)34 Python interface\n4\n\n\nCH4\nethanol\nmalonaldehyde\naspirin\nsalicylic acid\ntoluene\nnaphthalene\nH2O\nCH3CHO\nCO2\nCH2O\nCH3CHO@C60\nCH4@C60\nCH2O@C60\nH2O@C60\nCO2@C60\nFigure 1: Visualising the structural and chemical space explored in the present study. We show a\ntwo-dimensional embedding of the MACE descriptor trained on the GO dataset19, using principal\ncomponent analysis. The points of the map correspond to the training set of GO-MACE-23 (blue),\nmolecules containing C, H, and O atoms, representing ≈5% of the SPICE (version 1) dataset27 (red),\nconfigurations from rMD17 trajectories33 (purple), a series of fullerenes with sizes ranging between\n20 and 100 (magenta), five molecules encapsulated in C60 fullerene cages (yellow), and the same\nmolecules in vacuum (black crosses).\nof Psi435, version 1.4.\nData overlap between molecules and graphene oxide\nBefore benchmarking GO-MACE-23, it is important to set performance expectations based on\nthe similarity of the various test sets and the GO training set. In Fig. 1, we present a two-\ndimensional embedding, from principal component analysis (PCA), of the average atomistic\nfeatures per snapshot as learned by GO-MACE-23. The use of average features eliminates\nthe system-size dependence of the descriptors. We observe that static rMD17 molecules\nlie outside the scope of the training data (blue), but fall within the SPICE dataset domain\n(red), which constitutes the training data of MACE-OFF. We should thus expect MACE-OFF to\noutperform GO-MACE-23 for static molecules. “Acyclic” molecules such as ethanol seem to\n5\n\n\nbe farther from the GO domain compared to cyclic molecules, such as aspirin. As a result,\nwe expect GO-MACE-23 to provide more accurate predictions for cyclic molecules compared\nto acyclic configurations. Fullerenes (magenta) and encapsulated molecular species (“M @\nC60”, yellow) are located on the outskirts of the GO region of the map in Fig. 1—this is\nunexpected at first glance, as fullerenes are not part of the GO training data. However, some\nof their key characteristics can be learned from the GO backbone.\nZero-shot performance of GO-MACE-23\nIn this section, we evaluate the performance of GO-MACE-23 in predicting the energies and\nforces of small molecules, as well as vibrational spectra. Throughout this section, we use the\nterms “error” and “root mean square error” (RMSE) interchangeably.\nNumerical performance for MD17\nA common starting point in evaluating MLIP performance is in testing prediction errors for\nenergies and forces. These tests can be more complex than they look at first glance, because\ntheir outcome will strongly depend on the type of data used for testing (see, e.g., Ref. 36). In\nthe present work, we are interested in zero-shot generalisability (without further modification\nof the model), which we here test by changing the application domain from extended GO\nstructures to isolated (small) molecules.\nWe begin our series of zero-shot tests by evaluating the performance of GO-MACE-23 for the\nrelevant trajectories from the rMD17 dataset. In Fig. 2, we summarise the prediction errors\non total energies and atomic forces relative to the QM targets of the rMD17 molecules. De-\nspite the differences in the levels of theory between GO-MACE-23 and rMD17, we observe\nRMSE values below the often-quoted “chemical accuracy” of 1 kcal mol−1 or ≈40 meV\nat.−1. However, these errors can be significantly higher than the model’s internal internal\nvalidation error for GO (1.8 meV atom−1 for energies and 109 meV ˚A−1 for forces, shown\nas dashed lines in Fig. 2), which is the case for malonaldehyde. The latter is an example\n6\n\n\nO\nOH\nO\nO\nOH\nO\nO\nOH\nO\nOH\nFigure 2:\nEnergy and force errors on six trajectories from the revised MD17 dataset using\nGO-MACE-23.\nThe bars represent the RMSE of quantities between GO-MACE-23 predictions and\nrMD17 labels. The dashed area represents the errors between the DFT levels of theory used to label\nthe GO dataset and the rMD17 dataset. The dashed line is the internal validation error of GO-MACE-23.\nof an acyclic molecule (not containing an aromatic ring) that is not well represented in the\nGO-MACE-23 dataset. To explore the origin of these errors, we performed DFT calculations,\nusing the same parameters as used for training GO-MACE-23, on the different test snapshots,\nand we report the RMSE between the levels of theory, as shown by hatched bars in Fig. 2. We\nfind that GO-MACE-23 is primarily constrained by its own training QM labels, as systematic\ndiscrepancy errors account for approximately 30% to 90% of the errors. For aspirin, naph-\nthalene, and salicylic acid, GO-MACE-23 introduces almost no additional errors beyond those\ninherent to its DFT training labels, and its prediction errors are comparable to its internal val-\nidation errors. GO-MACE-23 introduces almost no additional errors to the predictions made\non cyclic molecules, although the magnitude of the errors varies notably.\nThis evaluation highlights the importance of contextualising zero-shot performance of pre-\ntrained ML models across datasets.\nMost of the force prediction errors for the rMD17\nmolecules stem from discrepancies in the underlying DFT, with the exception of toluene\n7\n\n\nFigure 3: (a) Visualisation of a toluene molecule obtained using OVITO37. Red- and blue-coloured\natoms are carbon atoms part of the aromatic ring and the attached methyl group, respectively. (b)\nForce components parity plot of the DFT-computed and GO-MACE-23-predicted forces for the carbon\natoms labelled red and blue in panel (a). (c) Force parity plot of the sum of forces of the red- and blue-\nlabelled carbon atoms.\n(which we address in the following). Figure 2 suggests that molecules with structural motifs\nresembling those in a GO sheet are better captured by GO-MACE-23, reinforcing the impor-\ntance of dataset choice for generalisability. While the ideal situation is to always compare\ndata coming from uniform sources, we understand that this might not always be computa-\ntionally feasible, underscoring the need for robust contextual analysis in ML model evalua-\ntions.\nToluene as a special case\nTo better understand the performance limits of GO-MACE-23, we analyse the errors for toluene\nin more detail, as it exhibits the highest force prediction RMSE among all 6 rMD17 molecules\nconsidered here. Fig. 3 summarises our approach to exploring possible sources of error. The\ntoluene molecule contains an aromatic carbon atom directly bonded to an sp3 carbon atom in\na methyl group (–CH3), coloured in red and blue in Fig. 3a, respectively. These two carbon\natoms have the highest overall force errors exceeding 1.2 eV ˚A−1 (Fig. 3b). The high force\nerrors on these specific atoms indicate that GO-MACE-23 is incapable of faithfully modelling\ntheir behaviour, due to the under-representation of similar atomic environments in the GO\n8\n\n\nnaphthalene\n0\n20\ntoluene\n0\n20\nmalonaldehyde\n0\n20\nfrequency (THz)\nVibrational spectrum (arb. units)\nGO-MACE-23\nMACE-OFF23\nMACE-OFF24\nQM\nFigure 4: Molecular vibrational spectra computed with MLIPs (solid lines) and DFT (“QM”, dashed\nlines) for GO-MACE-23-relaxed naphthalene, toluene, and malonaldehyde molecules. The upper row\ncharacterises the out-of-domain performance of GO-MACE-23 (red). The lower row shows the perfor-\nmance of SOTA MLIPs for molecules, viz. MACE-OFF26 (dark and light blue). Note that the DFT data\nhave been computed at the level corresponding to the training data of the respective MLIP model; the\nDFT data in the upper and lower rows are therefore slightly different.\n9\n\n\ntraining set.\nMost current MLIPs (including the MACE architecture) describe the total energy of a chemi-\ncal system as a sum of atomic energies, following Refs. 38 and 23. While this decomposition\nis useful for training and extrapolating ML models, it is not inherently physical and has no\ndirect counterpart in a quantum-mechanical computation: so it is possible for the MLIP to re-\nproduce the global behaviour without capturing the expected local energy distribution. This\nissue is evident in the present case of toluene (Fig. 3c): the combined error for the sum of\nthe forces is only one-third of the individual force-component errors. The predicted atomic\nenergies confirm this limitation (Fig. S1): the “red” atom of the aromatic ring has the lowest\npredicted atomic energy of all the carbon atoms, while the “blue” atom of the methyl group\nhas the highest. When averaging the energies of these two atoms, the methyl carbon and its\ndirect neighbour have the lowest local energy across the randomly selected 200 snapshots in\nthe trajectory (Fig. S1). More generally, further work is necessary to fully understand the\nlocal predictions of MLIPs, and steps towards this goal have been made39,40.\nVibrational spectra\nThe vibrational spectrum—which provides information about bending, twisting, and stretch-\ning of individual bonds—is a fingerprint of a molecule (and experimentally accessible), and\nis therefore an important test for an MLIP to accurately reproduce. To assess the ability of\nGO-MACE-23 to predict vibrational spectra, we focus on three molecules from the rMD17\ndataset: naphthalene and toluene representing the best and worst force predictions, respec-\ntively (cf. Fig. 2), and malonaldehyde as an example of a molecule without a 6-membered\naromatic ring (the principal structural fragment of graphene). We start by selecting a random\nsnapshot from the three trajectories, then relax the molecules using GO-MACE-23. The force\nerrors for the relaxed structures are 0.05 eV ˚A−1 for naphthalene, 0.32 eV ˚A−1 for toluene,\nand 0.22 eV ˚A−1 for malonaldehyde. Then, we compute the spectra with the MLIP and DFT\nat the corresponding level, using finite displacements, from phonopy41,42. We present the re-\nsulting vibrational spectra in the upper panels of Fig. 4. The GO-MACE-23-predicted spectra\n10\n\n\nagree qualitatively with their DFT counterparts, and the quality of the prediction correlates\nwell with the model’s force accuracy. The low-frequency modes, in particular, are well re-\nproduced, while the accuracy decreases for the high-frequency modes. A recent study in\nRef. 43 suggests that these discrepancies may arise from a softened potential-energy surface\nnear the relevant snapshots, which could explain the reduced accuracy for high-frequency\nmodes.\nWe compare GO-MACE-23 to MACE-OFF23 and MACE-OFF24, two SOTA molecular MLIP\nmodels trained on different versions of the SPICE molecular dataset (see Methodology sec-\ntion). We compute the vibrational spectra on the GO-MACE-23-relaxed molecules using MACE-OFF\nand their corresponding DFT level of theory. The force errors of MACE-OFF23 are 0.003,\n0.002, and 0.016 eV ˚A−1 for naphthalene, toluene, and malonaldehyde, respectively. The\nforce errors of MACE-OFF24 are 0.005, 0.003, and 0.005 eV ˚A−1 for naphthalene, toluene,\nand malonaldehyde, respectively. We report the spectra in the lower panels of Fig. 4. As\nshown in Fig. 1, the rMD17 molecules fall within the training domain of the MACE-OFF mod-\nels, which explains the models’ high accuracy in predicting atomic forces. As a result, both\nMACE-OFF models produce more accurate vibrational spectra, reproducing both high- and\nlow-frequency modes.\nFullerenes and encapsulated molecules\nWe use a series of fullerene molecules as another benchmark to quantify the transferability of\nGO-MACE-23 (and MACE-OFF). The smallest fullerene is C20, containing only five-membered\nrings of carbon atoms and no six-membered ones. Consequently, its curvature is large, and\nthe fullerene is found to be the most stable C20 conformer using MP2 calculations45. Larger\nfullerenes are energetically and structurally closer to graphene and graphite, and should there-\nfore be closer to the training domain of GO-MACE-23 (cf. Fig. 1).\nBoth GO-MACE-23 and MACE-OFF variants reproduce the general trend of growing stabilisa-\ntion with fullerene size, as shown in Fig. 5. Prediction errors are highest for the smaller\nfullerenes, with RMSE values higher than > 100 meV at.−1, likely due to their high cur-\n11\n\n\nC20\nC60\nC100\nFigure 5: Evolution of the per-atom energy of fullerenes, obtained from Ref. 44, of sizes between\n20 and 100 atoms computed with GO-MACE-23 and its corresponding DFT level of theory (red), and\nMACE-OFF and their corresponding DFT level of theory (dark and light blue). Similar to Fig. 4,\nlines represent the ML predictions, and the dashed lines represent the QM reference calculations. All\nenergies are referenced to C60. The lower panel describes the difference between energies computed\nwith ML and QM, and expressed per atom. The rendered images show three fullerenes: C20, C60, and\nC100.\n12\n\n\nFigure 6: Evolution of energy and force RMSE between GO-MACE-23 predictions and the corre-\nsponding DFT level of theory (left column), as well as between both MACE-OFF variants and their\nrespective DFT levels of theory (middle and right columns). The errors are calculated from 1 ns tra-\njectories at 500 K for H2O, CH4, and CH2O enclosed in a C60 fullerene. The trajectories are driven\nby GO-MACE-23.\n13\n\n\nvature. For C60, the RMSE decreases to around 50 meV at.−1 for all MLIPs. For small\nfullerenes (< 60 carbon atoms), GO-MACE-23 performs better than both MACE-OFFmodels:\nwe presume that this is due to the fact that it has encountered some curved graphene sheets,\nincluding various odd-membered rings, during training. Note, however, that the latter are only\na small fraction of the training data: the ring-size distribution in the GO-MACE-23 dataset is\n1:600 for 5:6-membered rings. MACE-OFF24 significantly outperforms both GO-MACE-23 and\nMACE-OFF23 for fullerenes with the sizes of 42 and 50 atoms, hinting towards the existence\nof relevant motifs within the updated version of the SPICE dataset. This requires further\ninvestigation.\nIn a recent study, Vyas at al. showed how formaldehyde (CH2O) can be inserted into a C60\nmolecule by subsequent organic reaction steps46, expanding on existing work on endohedral\nfullerenes47,48. In the context of the present work, we show in Fig. 6 three case studies that\nhave been discussed in the literature: encapsulated water (written as “H2O@C60”)49, encap-\nsulated methane (“CH4@C60”)50, and encapsulated formaldehyde (“CH2O@C60”)46.\nWe use GO-MACE-23 to drive long MD trajectories of the three species in the NVT ensem-\nble at T = 500 K, for 1 ns with a 0.5 fs timestep. Such simulations can be challenging test\ncases51, especially given the fusion temperature of C60 is estimated to be around 550 K52.\nWe re-label snapshots from these MD trajectories with GO-MACE-23 and its corresponding\nDFT method, as well as MACE-OFF and its corresponding DFT method. In Fig. 6, we show\nthe errors, expressed as absolute errors (AE) for energies and mean absolute errors (MAE)\nfor forces rather than our usual RMSE, for snapshots sampled every 20 ps. Both MLIPs ex-\nhibit similar energy prediction errors, with GO-MACE-23 performing better for the larger en-\ncapsulated molecules, and MACE-OFF23 for H2O@C60. However, GO-MACE-23 consistently\nyields lower force prediction errors across all of the test cases. This poorer performance of\nMACE-OFF23 and MACE-OFF24 may be attributed to the fact that fullerenes and encapsulated\nmolecules are not present within the two versions of the SPICE training set. Additionally,\nGO-MACE-23 has encountered small molecules, such as CO and H2O, near GO surfaces in\nits training data. Also, it is possible that GO-MACE-23 is accessing regions of configurational\n14\n\n\nTable 1: Energy and force prediction RMSE as a function of the maximum rank of the equivariant\nhidden messages in the MACE architecture for trajectories from the rMD17 dataset. Errors are com-\nputed with respect to the DFT level of theory of rMD17. The lowest RMSE values for each molecules\nare highlighted in bold\nEnergy RMSE (meV at.−1)\nForce RMSE (eV ˚A−1)\nmax L\n0\n1\n2\n0\n1\n2\naspirin\n6.2\n6.6\n4.9\n0.25\n0.22\n0.28\nethanol\n12.3\n10.6\n12.2\n0.49\n0.35\n0.48\nmalonaldehyde\n7.7\n12.3\n9.2\n0.28\n0.33\n0.25\nnaphthalene\n3.3\n4.0\n3.6\n0.18\n0.18\n0.17\nsalicylic acid\n5.3\n4.9\n6.8\n0.22\n0.22\n0.26\ntoluene\n5.6\n9.1\n6.9\n0.32\n0.51\n0.25\nspace that would be deemed unphysical by MACE-OFF. Uncertainty estimation of predictions\nmade by these models could provide an answer, even partially, to this question.\nIn the Supplementary Information, we show two additional cases of encapsulated molecules,\ncarbon dioxide and acetaldehyde, the heavier homologue of CH2O. Acetaldehyde is a chal-\nlenging test case for GO-MACE-23, and has most likely not been seen during training (cf.\nFig. 1). It is a thought experiment, of course, for the time being.\nExperiments\nBeyond the zero-shot performance evaluation so far, we carry out additional numerical ex-\nperiments. These explore aspects of MLIP fitting methodology and provide an initial test for\ndescriptions of gas-phase fragmentation reactions.\nModel choice (I): Effect of equivariant messages\nThe MACE architecture underlying GO-MACE-23 incorporates both invariant hidden features\nand equivariant hidden features of rank L = 1. To test the role of equivariance, we trained two\nmodified versions of the model by varying MACE’s internal symmetry rank. Specifically, we\ntrained an invariant model by setting the highest rank of the internal features to max L =\n0, and a higher-order equivariant model by setting max L = 2. This allows us to explore\n15\n\n\nthe possible correlation between the physical symmetries of an MLIP and its out-of-domain\nperformance.\nIn Table 1, we compare the performance of MACE models using invariant vs equivariant\nmessages with different maximum rank max L ∈{0, 1, 2}. We calculate the prediction er-\nrors for all relevant rMD17 molecules, using MACE models re-fitted with different max L\nvalues. We notice that the original GO-MACE-23 model (max L = 1) does not systemati-\ncally outperform its invariant counterpart (max L = 0). For example, the invariant model\nyields better energy predictions for toluene, aspirin, and naphthalene, as well as better force\npredictions for salicylic acid, compared to GO-MACE-23. A similar trend is observed when\ncomparing GO-MACE-23 to the max L = 2 MACE model. Regardless of the benchmark ref-\nerence calculation, we observe no clear correlation between max L and model performance,\nsuggesting that equivariance and symmetry preservation play a limited role in generalisation\nfor these domains. A particularly notable case is the toluene trajectory, where GO-MACE-23\nis the worst-performing model of the three, in terms of total energy and force predictions (cf.\nFig. 3).\nModel choice (II): Other GNN architectures\nTo further investigate the effect of design choices made for several popular GNNs on their\ngeneralisability, we trained multiple models on the GO-MACE-23 training dataset, using the\nuniversal interface graph-pes53. Particularly, we used the SchNet54, PaiNN55, Tensor-\nNet56, and NequIP8 architectures. Details about hyperparameters and validation errors on\nthe GO-MACE-23 dataset are provided in the Supplementary Information.\nTable 2 shows that GO-MACE-23, TensorNet, and NequIP generally yield low RMSE on most\nmolecules for both energy and force predictions. For instance, NequIP achieves low en-\nergy errors on aspirin and malonaldehyde, whereas TensorNet performs best for toluene.\nMeanwhile, GO-MACE-23 has the best errors in force predictions for ethanol and naphtha-\nlene. These variations demonstrate that even closely related equivariant models can extract\ndistinct mappings from the same data, influenced by subtle differences in model design and\n16\n\n\nTable 2: Energy and force prediction RMSE different GNN architectures trained on the GO dataset\nfor trajectories from the revised MD17 dataset. Errors are computed with respect to the DFT level of\ntheory of rMD17. The lowest RMSE values for each molecules are highlighted in bold\nEnergy RMSE (meV at.−1)\nGO-MACE-23\nSchNet\nTensorNet\nNequIP\nPaiNN\naspirin\n6.6\n22.4\n6.6\n5.7\n11.3\nethanol\n10.6\n33.4\n17.4\n17.2\n27.6\nmalonaldehyde\n12.3\n38.5\n10.8\n8.8\n17.2\nnaphthalene\n4.0\n9.9\n5.0\n3.9\n5.8\nsalicylic\n4.9\n19.7\n5.6\n3.9\n7.4\ntoluene\n9.1\n16.8\n8.7\n24.0\n14.0\nForce RMSE (eV ˚A−1)\nGO-MACE-23\nSchNet\nTensorNet\nNequIP\nPaiNN\naspirin\n0.22\n0.86\n0.38\n0.31\n0.57\nethanol\n0.35\n1.13\n0.61\n0.47\n1.01\nmalonaldehyde\n0.33\n0.98\n0.34\n0.33\n0.38\nnaphthalene\n0.18\n0.54\n0.21\n0.21\n0.30\nsalicylic\n0.22\n0.42\n0.24\n0.19\n0.25\ntoluene\n0.51\n0.59\n0.28\n0.38\n0.32\nhyperparameters.\nThese results highlight the importance of the MLIP architecture in capturing relevant atom-\nistic information and transferring it beyond the training set. The extrapolation is not trivial\nand depends not only on the quality of the training data or the fit but also on the architec-\nture itself. Notably, as shown in the Supplementary Information, GO-MACE-23 has the lowest\nenergy validation errors on the GO dataset, yet NequIP outperforms it for several rMD17\nmolecules. These results underscore the need for systematic out-of-domain validation to\nfully assess model generalisation.\nTransferability to chemical reactions\nThe long-term goal of molecular interatomic potentials is to describe entire reaction mech-\nanisms, rather than just the reactants and products.\nMLIPs are increasingly being used\nto describe transition states of reactions in vacuum57,58 and in explicit solvent7.\nWhile\nGO-MACE-23 has been trained on various rearrangements, decarbonylation reactions, etc.,\nit has not been explicitly trained on molecular reaction mechanisms. This makes it a partic-\n17\n\n\n*\n**\n*\naspirin\nO\nO\nOH\nO\nΔ\n2\nOH\nOH\nO\nO\nCH\n+\nketene\nsalicylic acid\nO\nC\nO\nOH\nO\nH\nH\nH\nO\nO\nOH\nO\naspirin\nΔ\nO\nO\nO\nH\nOH\nCH3\n–CO2\n–CO\nOH\nO\nCH3\nO\nCH3\nOH\no-cresol\n*\n* *\n*\n**\n*\n*\n* *\nFigure 7: Energy profiles of two exemplary high-temperature molecular-dynamics simulations com-\nputed with GO-MACE-23, MACE-OFF23, MACE-OFF24, and their respective QM references. The MD\ntrajectories are driven by GO-MACE-23 and maintained at 1,500 K. The first panel describes a reaction\npathway to produce salicylic acid and ketene (H2CCO) from aspirin. The third panel describes the de-\ncomposition of aspirin through a series of decarbonylations and decarboxylations to produce o-cresol.\nThe second and fourth panels describe the difference between energies computed with ML and QM,\nfor the first and second reactions, respectively, and expressed per atom. The asterisks correspond to\nfailed DFT calculations after 30 self-consistent cycles.\n18\n\n\nularly challenging and relevant “real-world” benchmark for complex chemical transforma-\ntions.\nWe use GO-MACE-23 to run a series of MD trajectories of an aspirin molecule in a periodic\nsimulation cell of 30 ˚A3, using the NVT ensemble at T = 1,500 K. We also re-label the tra-\njectories using the DFT reference method of GO-MACE-23, as well as using both MACE-OFF\nvariants and their DFT reference method. In Fig. 7, we report two reaction pathways demon-\nstrating the thermally driven decomposition of aspirin in vacuum into radical species which\nthen recombine forming different molecules.\nThe upper panels of Fig. 7 depict the formation of reactive ketene and salicylic acid, a pro-\ncess involving the breaking of an ester bond. The reverse reaction was first described in Ref.\n59. Both GO-MACE-23 and the MACE-OFF variants accurately capture the energetics of the\nreactants and products. However, they significantly underestimate the energy of the inter-\nmediates. Despite this underestimation, the predicted average energy of the intermediates\nremains higher than that of the more stable reactants or products. This poor performance of\nboth MLIPs is expected, as they are not explicitly trained on reaction pathways, and their\nunderlying datasets do not include radicals or ions. In addition, these MLIPs were not able\nto reproduce the energy of the isolated radicals. Stocker et al.60 have previously discussed\nthe limitations of MLIPs in accurately describing chemical reactions when radicals are not\nexplicitly incorporated in the training data.\nThe lower panels of Fig. 7 illustrate the formation of an o-cresol molecule through a series\nof decarboxylation and decarbonylation steps. This reaction pathway shares the first set of\nradicals with the upper panel, with similar geometries, before developing into a different\npathway. As with the previous pathway, all tested MLIPs underestimate the energy of the\nintermediate steps. The two models from the MACE-OFF family in particular overestimate the\nenergy of the product system.\n19\n\n\nConclusions\nLocated at the interface of materials and molecular modelling, graphene oxide offers an op-\nportunity to connect these different domains of atomistic machine learning. In the present\nwork, we have systematically assessed the zero-shot transferability of GO-MACE-23, an MLIP\ntrained on data for GO, across relevant chemical benchmarks. We found good—perhaps\nsurprisingly good—zero-shot performance compared to MACE-OFF, a pre-trained model for\nmolecular chemistry. The accuracy of both models decreases when describing reaction path-\nways, especially when radical species are involved.\nOur study has tested the behaviour of recently proposed GNN MLIP models and their trans-\nferability, and we think that it can have implications for the future development of “foun-\ndational” models for atomistic simulations. Our results emphasise that including chemical\nreactivity in the training data is important in finding reaction pathways: in the process of\nbuilding the GO-MACE-23 model19, we have sampled this reactivity in high-temperature MD\nsimulations, and a similar approach has been taken, e.g., for the bulk carbon–hydrogen61 and\ncarbon–oxygen62 systems. We think that local-environment diversity will be as important\nas the chemical space coverage (e.g., the number of chemical species) in defining founda-\ntional models – this might include the addition of radical species (cf. Fig. 7) to the training\ndata, either through very-high-temperature MD exploration or perhaps by explicitly involving\n“broken” bonds in the training protocol.\nDespite its limitation to the three elements C, H, and O, the GO-MACE-23 model seems to\nprovide a suitable starting point to study a wider range of chemistry-related questions than\nit was initially intended for, and we view this as a highly encouraging finding. We believe\nthat together with improved data-generation strategies21 as well as suitable workflows and\nautomation approaches63–66, truly universal MLIPs for molecular systems, and for extended\nmaterial structures built up from them, are coming within reach.\n20\n\n\nAuthor contributions\nC.B.M., Z.E.-M., and V.L.D. designed the research. K.A.G. carried out pilot studies, and\nC.B.M. and Z.E.-M. carried out the final numerical experiments. J.L.A.G. provided code and\nmethodology for MLIP fitting. All authors contributed to discussions. C.B.M. and V.L.D.\nwrote the manuscript, and all authors reviewed and approved the final version.\nData availability\nData supporting the present study are available at [GitHub link].\nAcknowledgements\nWe thank J. Holownia for useful discussions, and F. Duarte for useful discussions and com-\nments on the manuscript. C.B.M. acknowledges funding from the Swiss National Science\nFoundation (SNSF) under grant number 217837. We are grateful for support from the EP-\nSRC Centre for Doctoral Training in Theory and Modelling in Chemical Sciences (TMCS),\nunder grant EP/L015722/1 (Z.E.M.). J.L.A.G. acknowledges a UKRI Linacre - The EPA\nCephalosporin Scholarship, support from an EPSRC DTP award [grant number EP/T517811/1],\nand from the Department of Chemistry, University of Oxford. V.L.D. acknowledges support\nfrom the Engineering and Physical Sciences Research Council [grant number EP/V049178/1]\nand UK Research and Innovation [grant number EP/X016188/1]. We are grateful for com-\nputational support from the UK national high performance computing service, ARCHER2,\nfor which access was obtained via the UKCP consortium and funded by EPSRC grant ref\nEP/X035891/1.\nReferences\n1. J. Behler, Angewandte Chemie International Edition, 2017, 56, 12828–12840.\n21\n\n\n2. V. L. Deringer, M. A. Caro and G. Cs´anyi, Advanced Materials, 2019, 31, 1902765.\n3. O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Sch¨utt,\nA. Tkatchenko and K.-R. M¨uller, Chemical Reviews, 2021, 121, 10142–10186.\n4. A. P. Bart´ok, J. Kermode, N. Bernstein and G. Cs´anyi, Physical Review X, 2018, 8,\n041048.\n5. B. Cheng, G. Mazzola, C. J. Pickard and M. Ceriotti, Nature, 2020, 585, 217–220.\n6. Y. Zhou, W. Zhang, E. Ma and V. L. Deringer, Nature Electronics, 2023, 6, 746–754.\n7. H. Zhang, V. Juraskova and F. Duarte, Nature Communications, 2024, 15, 6114.\n8. S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari,\nT. E. Smidt and B. Kozinsky, Nature Communications, 2022, 13, 2453.\n9. I. Batatia, D. P. Kovacs, G. Simm, C. Ortner and G. Csanyi, Advances in neural informa-\ntion processing systems, 2022, pp. 11423–11436.\n10. I. Batatia, S. Batzner, D. P. Kov´acs, A. Musaelian, G. N. C. Simm, R. Drautz, C. Ortner,\nB. Kozinsky and G. Cs´anyi, Nature Machine Intelligence, 2025, 7, 56–67.\n11. B. Deng, P. Zhong, K. Jun, J. Riebesell, K. Han, C. J. Bartel and G. Ceder, Nature\nMachine Intelligence, 2023, 5, 1031–1041.\n12. I. Batatia, P. Benner, Y. Chiang, A. M. Elena, D. P. Kov´acs, J. Riebesell, X. R. Ad-\nvincula, M. Asta, W. J. Baldwin, N. Bernstein, A. Bhowmik, S. M. Blau, V. C˘arare,\nJ. P. Darby, S. De, F. Della Pia, V. L. Deringer, R. Elijoˇsius, Z. El-Machachi, E. Fako,\nA. C. Ferrari, A. Genreith-Schriever, J. George, R. E. A. Goodall, C. P. Grey, S. Han,\nW. Handley, H. H. Heenen, K. Hermansson, C. Holm, J. Jaafar, S. Hofmann, K. S.\nJakob, H. Jung, V. Kapil, A. D. Kaplan, N. Karimitari, N. Kroupa, J. Kullgren, M. C.\nKuner, D. Kuryla, G. Liepuoniute, J. T. Margraf, I.-B. Magd˘au, A. Michaelides, J. H.\nMoore, A. A. Naik, S. P. Niblett, S. W. Norwood, N. O’Neill, C. Ortner, K. A. Pers-\nson, K. Reuter, A. S. Rosen, L. L. Schaaf, C. Schran, E. Sivonxay, T. K. Stenczel,\n22\n\n\nV. Svahn, C. Sutton, C. van der Oord, E. Varga-Umbrich, T. Vegge, M. Vondr´ak,\nY. Wang, W. C. Witt, F. Zills and G. Cs´anyi, A foundation model for atomistic materials\nchemistry, 2023, http://arxiv.org/abs/2401.00096, arXiv:2401.00096 [cond-mat,\nphysics:physics].\n13. A. Merchant, S. Batzner, S. S. Schoenholz, M. Aykol, G. Cheon and E. D. Cubuk, Nature,\n2023, 624, 80–85.\n14. H. Yang, C. Hu, Y. Zhou, X. Liu, Y. Shi, J. Li, G. Li, Z. Chen, S. Chen, C. Zeni, M. Hor-\nton, R. Pinsler, A. Fowler, D. Z¨ugner, T. Xie, J. Smith, L. Sun, Q. Wang, L. Kong, C. Liu,\nH. Hao and Z. Lu, MatterSim: A Deep Learning Atomistic Model Across Elements, Tem-\nperatures and Pressures, 2024, https://arxiv.org/abs/2405.04967, Version Num-\nber: 2.\n15. C. Chen and S. P. Ong, Nature Computational Science, 2022, 2, 718–728.\n16. B. Focassio, L. P. M. Freitas and G. R. Schleder, ACS Applied Materials & Interfaces,\n2024, acsami.4c03815.\n17. S. Ju, J. You, G. Kim, Y. Park, H. An and S. Han, Application of pretrained\nuniversal machine-learning interatomic potential for physicochemical simulation of\nliquid electrolytes in Li-ion battery, 2025, http://arxiv.org/abs/2501.05211,\narXiv:2501.05211 [cond-mat].\n18. H. Kaur, F. Della Pia, I. Batatia, X. R. Advincula, B. X. Shi, J. Lan, G. Cs´anyi,\nA. Michaelides and V. Kapil, Faraday Discussions, 2025, 256, 120–138.\n19. Z. El-Machachi, D. Frantzov, A. Nijamudheen, T. Zarrouk, M. A. Caro and V. L. De-\nringer, Angewandte Chemie International Edition, 2024, e202410088.\n20. D. R. Dreyer, S. Park, C. W. Bielawski and R. S. Ruoff, Chem. Soc. Rev., 2010, 39,\n228–240.\n21. C. Ben Mahmoud, J. L. A. Gardner and V. L. Deringer, Nature Computational Science,\n2024, 4, 384–387.\n23\n\n\n22. T. K. Stenczel, Z. El-Machachi, G. Liepuoniute, J. D. Morrow, A. P. Bart´ok, M. I. J.\nProbert, G. Cs´anyi and V. L. Deringer, The Journal of Chemical Physics, 2023, 159,\n044803.\n23. A. P. Bart´ok, M. C. Payne, R. Kondor and G. Cs´anyi, Physical Review Letters, 2010,\n104, 136403.\n24. S. J. Clark, M. D. Segall, C. J. Pickard, P. J. Hasnip, M. I. J. Probert, K. Refson and M. C.\nPayne, Zeitschrift f¨ur Kristallographie - Crystalline Materials, 2005, 220, 567–570.\n25. J. P. Perdew, K. Burke and M. Ernzerhof, Physical Review Letters, 1996, 77, 3865–3868.\n26. D. P. Kov´acs, J. H. Moore, N. J. Browning, I. Batatia, J. T. Horton, V. Kapil, W. C.\nWitt, I.-B. Magd˘au, D. J. Cole and G. Cs´anyi, MACE-OFF23: Transferable Machine\nLearning Force Fields for Organic Molecules, 2023, http://arxiv.org/abs/2312.\n15211, arXiv:2312.15211 [physics].\n27. P. Eastman, P. K. Behara, D. L. Dotson, R. Galvelis, J. E. Herr, J. T. Horton, Y. Mao,\nJ. D. Chodera, B. P. Pritchard, Y. Wang, G. De Fabritiis and T. E. Markland, Scientific\nData, 2023, 10, 11.\n28. P. Eastman, B. P. Pritchard, J. D. Chodera and T. E. Markland, Journal of Chemical\nTheory and Computation, 2024, 20, 8583–8593.\n29. N. Mardirossian and M. Head-Gordon, The Journal of Chemical Physics, 2016, 144,\n214110.\n30. A. Najibi and L. Goerigk, Journal of Chemical Theory and Computation, 2018, 14,\n5725–5738.\n31. F. Weigend and R. Ahlrichs, Physical Chemistry Chemical Physics, 2005, 7, 3297.\n32. D. Rappoport and F. Furche, The Journal of Chemical Physics, 2010, 133, 134105.\n33. A. S. Christensen and O. A. Von Lilienfeld, Machine Learning: Science and Technology,\n2020, 1, 045018.\n24\n\n\n34. A. Hjorth Larsen, J. Jørgen Mortensen, J. Blomqvist, I. E. Castelli, R. Christensen,\nM. Dułak, J. Friis, M. N. Groves, B. Hammer, C. Hargus, E. D. Hermes, P. C. Jen-\nnings, P. Bjerre Jensen, J. Kermode, J. R. Kitchin, E. Leonhard Kolsbjerg, J. Kubal,\nK. Kaasbjerg, S. Lysgaard, J. Bergmann Maronsson, T. Maxson, T. Olsen, L. Pastewka,\nA. Peterson, C. Rostgaard, J. Schiøtz, O. Sch¨utt, M. Strange, K. S. Thygesen, T. Vegge,\nL. Vilhelmsen, M. Walter, Z. Zeng and K. W. Jacobsen, Journal of Physics: Condensed\nMatter, 2017, 29, 273002.\n35. D. G. A. Smith, L. A. Burns, A. C. Simmonett, R. M. Parrish, M. C. Schieber, R. Galvelis,\nP. Kraus, H. Kruse, R. Di Remigio, A. Alenaizan, A. M. James, S. Lehtola, J. P.\nMisiewicz, M. Scheurer, R. A. Shaw, J. B. Schriber, Y. Xie, Z. L. Glick, D. A. Siri-\nanni, J. S. O’Brien, J. M. Waldrop, A. Kumar, E. G. Hohenstein, B. P. Pritchard, B. R.\nBrooks, H. F. Schaefer, A. Y. Sokolov, K. Patkowski, A. E. DePrince, U. Bozkaya, R. A.\nKing, F. A. Evangelista, J. M. Turney, T. D. Crawford and C. D. Sherrill, The Journal of\nChemical Physics, 2020, 152, 184108.\n36. D. F. Thomas Du Toit, Y. Zhou and V. L. Deringer, Journal of Chemical Theory and\nComputation, 2024, 20, 10103–10113.\n37. A. Stukowski, Modelling and Simulation in Materials Science and Engineering, 2010,\n18, 015012.\n38. J. Behler and M. Parrinello, Physical Review Letters, 2007, 98, 146401.\n39. S. Chong, F. Grasselli, C. Ben Mahmoud, J. D. Morrow, V. L. Deringer and M. Ceriotti,\nJournal of Chemical Theory and Computation, 2023, 19, 8020–8031.\n40. S. Chong, F. Bigi, F. Grasselli, P. Loche, M. Kellner and M. Ceriotti, Faraday Discus-\nsions, 2025, 256, 322–344.\n41. A. Togo, L. Chaput, T. Tadano and I. Tanaka, Journal of Physics: Condensed Matter,\n2023, 35, 353001.\n42. A. Togo, Journal of the Physical Society of Japan, 2023, 92, 012001.\n25\n\n\n43. B. Deng, Y. Choi, P. Zhong, J. Riebesell, S. Anand, Z. Li, K. Jun, K. A. Persson and\nG. Ceder, npj Computational Materials, 2025, 11, 9.\n44. A. Barnard and G. Opletal, Fullerene Data Set, 2023, https://data.csiro.au/\ncollection/csiro%3A59022v1.\n45. V. Parasuk and J. Alml¨of, Chemical Physics Letters, 1991, 184, 187–190.\n46. V. K. Vyas, G. R. Bacanu, M. Soundararajan, E. S. Marsden, T. Jafari, A. Shugai, M. E.\nLight, U. Nagel, T. R˜o˜om, M. H. Levitt and R. J. Whitby, Nature Communications, 2024,\n15, 2515.\n47. A. A. Popov, S. Yang and L. Dunsch, Chemical Reviews, 2013, 113, 5989–6113.\n48. S. Bloodworth and R. J. Whitby, Communications Chemistry, 2022, 5, 121.\n49. O. Carrillo-Bohorquez, A. Valdes and R. Prosmiti, Journal of Chemical Theory and\nComputation, 2021, 17, 5839–5848.\n50. S. Bloodworth, G. Sitinova, S. Alom, S. Vidal, G. R. Bacanu, S. J. Elliott, M. E. Light,\nJ. M. Herniman, G. J. Langley, M. H. Levitt and R. J. Whitby, Angewandte Chemie\nInternational Edition, 2019, 58, 5038–5043.\n51. S. Stocker, J. Gasteiger, F. Becker, S. G¨unnemann and J. T. Margraf, Machine Learning:\nScience and Technology, 2022, 3, 045010.\n52. Ullmann’s encyclopedia of industrial chemistry, ed. B. Elvers and G. Bellussi, Wiley-\nVCH, Weinheim, 7th edn, 2011.\n53. J. L. A. Gardner, Graph PES, 2025, https://github.com/jla-gardner/graph-pes.\n54. K. T. Sch¨utt, P. Kessel, M. Gastegger, K. A. Nicoli, A. Tkatchenko and K.-R. M¨uller,\nJournal of Chemical Theory and Computation, 2019, 15, 448–455.\n55. K. T. Sch¨utt, O. T. Unke and M. Gastegger, Equivariant message passing for the pre-\ndiction of tensorial properties and molecular spectra, 2021, http://arxiv.org/abs/\n2102.03150, arXiv:2102.03150 [cs].\n26\n\n\n56. G. Simeon and G. d. Fabritiis, TensorNet: Cartesian Tensor Representations for Effi-\ncient Learning of Molecular Potentials, 2023, http://arxiv.org/abs/2306.06482,\narXiv:2306.06482 [cs].\n57. E. Komp and S. Valleau, Chemical Science, 2022, 13, 7900–7906.\n58. S. Choi, Nature Communications, 2023, 14, 1168.\n59. D. A. Nightingale, US Pat., 1604472, 1926.\n60. S. Stocker, G. Cs´anyi, K. Reuter and J. T. Margraf, Nature Communications, 2020, 11,\n5505.\n61. R. Ibragimova, M. S. Kuklin, T. Zarrouk and M. A. Caro, Chemistry of Materials, 2025,\n37, 1094–1110.\n62. T. Zarrouk, R. Ibragimova, A. P. Bart´ok and M. A. Caro, Journal of the American Chem-\nical Society, 2024, 146, 14645–14659.\n63. E. V. Podryabinkin and A. V. Shapeev, Computational Materials Science, 2017, 140,\n171–180.\n64. T. A. Young, T. Johnston-Wood, V. L. Deringer and F. Duarte, Chemical Science, 2021,\n12, 10944–10955.\n65. C. Van Der Oord, M. Sachs, D. P. Kov´acs, C. Ortner and G. Cs´anyi, npj Computational\nMaterials, 2023, 9, 168.\n66. Y. Liu, J. D. Morrow, C. Ertural, N. L. Fragapane, J. L. A. Gardner, A. A. Naik,\nY. Zhou, J. George and V. L. Deringer, An automated framework for exploring\nand learning potential-energy surfaces, 2024, http://arxiv.org/abs/2412.16736,\narXiv:2412.16736 [physics].\n27\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21317v1.pdf",
    "total_pages": 27,
    "title": "Assessing zero-shot generalisation behaviour in graph-neural-network interatomic potentials",
    "authors": [
      "Chiheb Ben Mahmoud",
      "Zakariya El-Machachi",
      "Krystian A. Gierczak",
      "John L. A. Gardner",
      "Volker L. Deringer"
    ],
    "abstract": "With the rapidly growing availability of machine-learned interatomic\npotential (MLIP) models for chemistry, much current research focuses on the\ndevelopment of generally applicable and ``foundational'' MLIPs. An important\nquestion in this context is whether, and how well, such models can transfer\nfrom one application domain to another. Here, we assess this transferability\nfor an MLIP model at the interface of materials and molecular chemistry.\nSpecifically, we study GO-MACE-23, a model designed for the extended covalent\nnetwork of graphene oxide, and quantify its zero-shot performance for small,\nisolated molecules and chemical reactions outside its direct scope--in direct\ncomparison with a state-of-the-art model which has been trained in-domain. Our\nwork provides quantitative insight into the transfer and generalisation ability\nof graph-neural-network potentials and, more generally, makes a step towards\nthe more widespread applicability of MLIPs in chemistry.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}