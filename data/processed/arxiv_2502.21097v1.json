{
  "id": "arxiv_2502.21097v1",
  "text": "Deep learning-based filtering of cross-spectral\nmatrices using generative adversarial networks\nChristof Puhle\nDepartment of Signal Processing\nSociety for the Advancement of Applied Computer Science (GFaI) e.V.\nBerlin, Germany\npuhle@gfai.de\nAbstract—In this paper, we present a deep-learning method\nto filter out effects such as ambient noise, reflections, or source\ndirectivity from microphone array data represented as cross-\nspectral matrices. Specifically, we focus on a generative adver-\nsarial network (GAN) architecture designed to transform fixed-\nsize cross-spectral matrices. Theses models were trained using\nsound pressure simulations of varying complexity developed\nfor this purpose. Based on the results from applying these\nmethods in a hyperparameter optimization of an auto-encoding\ntask, we trained the optimized model to perform five distinct\ntransformation tasks derived from different complexities inherent\nin our sound pressure simulations.\nIndex Terms—Deep learning, cross-spectral matrix, GAN.\nI. INTRODUCTION\nAs extensively investigated in [5], state-of-the-art deep-\nlearning methods for acoustical sound source localization\n(SSL) aim to directly reconstruct the direction of arrival of\nsources or, more generally, the parameters describing the\nacoustic scene in the presence of reverberation or diffuse noise.\nThis article addresses the problem from a different perspective\nby employing generative adversarial networks (GANs) to\neither remove or, at least, reduce the effects of ambient noise,\nreflections, or source directivity in microphone array data (that\nis, cross-spectral matrices) before any potential SSL analysis\nbegins. On the one hand, this approach improves the starting\npoint for solving the SSL problem, on the other hand, it\nenables a more effective use of traditional mapping methods\nsuch as standard beamforming or CLEAN-SC.\nThis paper proceeds by first outlining our sound pressure\nsimulation approach in II, then presenting the machine learning\nmodel (see III) that we designed to filter simulated cross-\nspectral matrices. Finally, we discuss results for five different\ntransformation or filtering tasks in IV.\nII. ACOUSTIC SIMULATIONS\nA. Basics\nLet ¯p : R3 →C be the complex amplitude of a time-\nharmonic sound pressure field of angular frequency ω > 0\nThis research has been funded by German Federal Ministry for Economic\nAffairs and Climate Action (Bundesministerium f¨ur Wirtschaft und Kli-\nmaschutz BMWK) under project AntiLerM registration number 49VF220063.\nand speed of propagation c > 0. By definition, ¯p satisfies the\nHelmholtz equation\n∂2¯p\n∂x2 + ∂2¯p\n∂y2 + ∂2¯p\n∂z2 + k2¯p = 0,\nk = ω\nc .\n(1)\nOur sign convention for a time-harmonic function is t 7→\nexp(iωt). For example,\n¯p(x, y, z) =\nexp\n\u0010\n−ik\np\nx2 + y2 + z2\n\u0011\np\nx2 + y2 + z2\n(2)\nrepresents an outgoing spherical wave with source in (0, 0, 0).\nLet p : [0, ∞]×[0, π]×[0, 2π] →C be ¯p’s representation in\nspherical coordinates (r, θ, ϕ). Solutions to the corresponding\nHelmholtz equation can be found analytically by assuming\nthat p is separable, i.e. there exist functions R : [0, ∞] →C,\nΘ : [0, π] →C, Φ : [0, 2π] →C such that\np(r, θ, ϕ) = R(r) · Θ(θ) · Φ(ϕ).\n(3)\nIn this case, the Helmholtz equation leads necessarily to\nR(r) = A · h(1)\nl\n(kr) + B · h(2)\nl\n(kr)\n(4)\nfor some constants A, B ∈C, l ∈N0 = {0, 1, . . .}, and h(1)\nl\n,\nh(2)\nl\ndenote the spherical Hankel functions of the first and\nsecond kind of degree l, respectively. Moreover, we have\nΘ(θ) · Φ(ϕ) = Y m\nl (θ, ϕ),\n(5)\nwhere m ∈{−l, −l + 1, . . . , l}, and Y m\nl\nis the spherical\nharmonic of degree l and order m.\nB. Smooth spherical pistons\nA vibrating spherical cap piston with aperture angle α ∈\n(0, π] centered on the north pole of an otherwise rigid sphere\nwith radius r0 > 0 can be described by its surface velocity\nvα : [0, π] × [0, 2π] →R,\nvα(θ, ϕ) = V · aα(θ, ϕ),\nV > 0,\n(6)\nthe corresponding aperture function aα : [0, π] × [0, 2π] →R\nis given by\naα(θ, ϕ) = 1 −H\n\u0010\nθ −α\n2\n\u0011\n,\nH(x) =\n(\n0\nx < 0\n1\nx ≥0 .\n(7)\narXiv:2502.21097v1  [cs.SD]  28 Feb 2025\n\n\nThe spherical wave spectrum of vα,\nvα(θ, ϕ) =\n∞\nX\nl=0\nl\nX\nm=−l\nvα\nlm · Y m\nl (θ, ϕ),\n(8)\ncan be computed via integration of the corresponding associ-\nated Legendre polynomials:\nvα\nlm = V δm0\np\n(2l + 1)π\nZ 1\ncos( α\n2 )\nP 0\nl (x)dx.\n(9)\nRotating the spherical cap to be centered in the direction (˜θ, ˜ϕ)\nresults in\n˜vα\nlm =\nr\n4π\n2l + 1Y m\nl (˜θ, ˜ϕ)∗· vα\nlm,\n(10)\nrespectively. Finally, the radiated pressure in the region r > r0\nis completely determined by the surface velocity spectrum (see\nfor example [11]):\npvα(r, θ, ϕ) = −iρ0c\n∞\nX\nl=0\nl\nX\nm=−l\nh(2)\nl\n(kr)\nh(2) ′\nl\n(kr0)\nvα\nlm · Y m\nl (θ, ϕ).\n(11)\nAs most of the higher degrees in (8) are present to form\nthe discontinuity at the boundary of the spherical cap, we\nopt for a smooth one-parameter family of spherical pistons\nas fundamental building block of our acoustical models. Its\nsurface velocity wα : [0, π] × [0, 2π] →R is defined via\nwα(θ, ϕ) =\n\n\n\nV · exp\n\u0012\n−(1−cos(θ))2\n(1−cos( α\n2 ))\n2\n\u0013\nα ≤π,\n2π−α\nπ\n· wπ(θ, ϕ) + α−π\nπ\n· V\nπ < α ≤2π.\n(12)\nAgain, we will call α the aperture angle of this piston, but now,\nwhen varying θ from 0 to α/2, the particle velocity smoothly\nchanges from V to V/e in the case α ≤π. As before, the\nspherical wave spectrum of wα can be determined by one-\ndimensional integration,\nwα\nlm = δm0\np\n(2l + 1)π\nZ 1\n−1\nP 0\nl (x) ˆwα(x)dx,\n(13)\nwhere ˆwα(cos (θ)) ≡wα(θ, ϕ). Moreover, transformation rule\n(10) also holds for the coefficients wα\nlm when rotating the\nsmooth spherical piston to be centered in the direction (˜θ, ˜ϕ),\nand the radiated sound pressure pwα corresponding to wα can\nbe computed in complete analogy to (11).\nC. Acoustic models\nThe simulations involved a set of 5000 acoustic models\nfixed beforehand and each consisting of three (outgoing)\nsmooth spherical pistons which were rotated and translated\nuniformly at random along the plane z = 0 within a cube of\nedge length 2.56 m that is centered at the origin. Moreover,\neach source is furnished with its own reflection plane together\nwith a reflection coefficient between −3 dB and −15 dB. The\naperture angles of the pistons vary from 3π/2 to 2π (acoustic\nmonopole) and source radii r0 are chosen randomly and\nuniformly between 0.1 m and 0.3 m. Within each model, the\nsource V ’s are chosen to be at most 15 dB below the model\nsound velocity level, which ranges uniformly between 35 dB\nand 85 dB across the model set. Consequently, the maximum\ndynamic range between sources within each model is 15 dB.\nModel temperatures are taken from a normal distribution with\na mean of 20 C◦and a standard deviation of 2.5 C◦.\nWe approximated the sound pressures of these models up to\nHelmholtz degree 15 at the positions of a virtual microphone\narray of spherical shape (48 microphones, diameter 0.35 m,\ncentered at (0, 0, d) with d = 2.56 m) for 16 distinct frequen-\ncies:\n10 · ∆f, . . . , 25 · ∆f,\n∆f = 192000\n1024 Hz = 187.5 Hz. (14)\nIn this process, each source of the model set was furnished\nwith its own spectral distribution function g : R →[0, 1],\ng(f) = exp\n\u0012\n−1\n2\n(f −fc)2\nf 2w\n\u0013\n,\n(15)\nwhere the center frequency fc was ranging uniformly from\n4 · ∆f to 35 · ∆f, and the frequency width parameter fw was\nchosen between ∆f/2 and 64·∆f, again, uniformly at random.\nFinally, we included an arbitrary pressure field of degree\nlmax = 15 (incoming towards the origin) to model ambient\nsound. The upper bound u : {0, . . . , lmax} →R for the\nmagnitude of the corresponding randomly chosen complex\ncoefficients is given by\nu(l) = u0 exp\n\u0012\n−(lmax + 1) ·\n(l + 1)2 −1\n(lmax + 1)2 −1\n\u0013\n,\n(16)\nwhere u0 is at least 10 dB below the model sound velocity\nlevel.\nIII. MACHINE LEARNING MODEL\nA. Generative Adversarial Networks\nThe machine learning model we present below is based on\na GAN architecture (see [4]), where, when training the model,\na pass through the learning loop can be interpreted as a round\nof a zero-sum game in the sense of game theory. Here, the\ntwo players, generator and discriminator, confront each other\nand aim to optimize their respective objective functions. More\nprecisely, the generator and discriminator are artificial neural\nnetworks whose parameters are optimized according to loss\nfunctions derived from their respective objective functions.\nIn general, the generator G : Z →B is a mapping between\nspaces Z and B, where B, on the one hand, contains the set\nX ⊂B of training data (also referred to as real data) and,\non the other hand, defines what is considered to be achievable\nthrough generation: the elements of the image G(Z) are called\nthe fake data generated by G from Z. For example, in what\nfollows, B = C48×48×16, therefore, it contains the cross-\nspectral matrices built from our sound pressure simulations\nin II-C.\nNow, the discriminator D : B →[0, 1] is a mapping from\nB to the unit interval. In each pass through the learning loop,\na finite set X ⊆X of training data is drawn randomly (this is\nalso called mini-batching approach [3]) and complemented by\n\n\na finite set Z of realizations of a random variable with a fixed\nprobability distribution taking values in Z. The goal of the\ndiscriminator is now to distinguish between the real data X\nand the fake data G(Z) generated from Z. More specifically,\nD is optimized according to the loss function\nLD = −1\n#X\nX\nx∈X\nlog(D(x)) −\n1\n#Z\nX\nz∈Z\nlog(1 −D(G(z))),\n(17)\nwhere log denotes the natural logarithm. On the opposite\nside, the objective of the generator is to make the fake data\nG(Z) generated from Z appear as real as possible from the\ndiscriminator’s point of view. This is achieved by optimizing\nthe parameters of G using the loss function\nLG = −1\n#Z\nX\nz∈Z\nlog(D(G(z))).\n(18)\nIn summary, through this adversarial process, both networks\nare optimized to improve their respective capabilities in pro-\ncessing the training data (see [8]).\nB. Complex model building blocks\nAs the ability to take into account phase information will be\ncrucial when working with cross-spectral matrices, all building\nblocks of the deep neural networks that follow will be real\nrepresentations of complexifications of their traditional real-\nvalued counterparts (see [9]): Suppose L(p) ∈RN×M is a\nreal matrix representation of a linear network operation, where\np ∈RK is the corresponding vector of learnable parameters.\nThen, simply by switching to the field of complex numbers,\nwe deduce that L(pr) + i L(pi) ∈CN×M for pr, pi ∈RK\nsatisfies\n(L(pr) + i L(pi))(x + i y) =(L(pr)x −L(pi)y)\n(19)\n+ i (L(pr)y + L(pi)x)\n(20)\nfor all x, y ∈RM. Therefore, we will call\n\u0012L(pr)\n−L(pi)\nL(pi)\nL(pr)\n\u0013\n∈R2N×2M\n(21)\na real matrix representation of the complexification of the\nnetwork operation of L(p).\nIn addition to linear network operations, we make use of two\ncomplex phase-preserving activation functions in order to build\nnetworks that represent non-linear complex functions. Firstly,\nwe employ a so-called modified ReLU activation function\nFmReLU : C →C with bias b ∈R, which is defined as\nfollows (see [1]):\nFmReLU(z) = ReLU(|z| + b) z\n|z|\n(22)\n=\n(\n(|z| + b) z\n|z|\nif |z| + b ≥0,\n0\notherwise.\n(23)\nAnd secondly, we apply a leaky variant FlCard : C →C,\nFlCard(z) = 1\n2 ((1 + α) + cos(arg(z))) z,\nα > 0,\n(24)\nof the complex cardioid activation function of [10]. The\nabsolute value of the latter depends on the phase of z, whereas\nthe former does not.\nC. Models for generator and discriminator\nThe generator G = GE ◦GD we are going to employ is a\ncomposition of two convolutional neural networks, encoder\nGE and decoder GD. The input of the encoder GE takes\nvalues of C48×48×16, hence, it can be applied to cross-spectral\nmatrices built out of the simulations of II-C. The kernel of\nthe utilized (transposed) two-dimensional complex bias-free\nconvolutions is 48×48 with trivial stride. Therefore, a padding\nstrategy was not necessary. The encoder GE is composed of\none such convolutional layer c and one to four complex bias-\nfree dense layers d (cf. III-B),\n\n\n48\n48\n16\n\n→\nc,a\n\n\n1\n1\nngen\n\n→\nd,a\n\n\n1\n1\nnden\n\n→\nd,a . . . →\nd,a\n\n\n1\n1\nnden\n\n. (25)\nHere, every convolution or dense layer entails an activation a\nusing one of the functions of III-B,\n\n\n\nd1\n...\ndk\n\n\n\n(26)\nis an alternative notation for Cd1×...×dk, the number of con-\nvolution filters is ngen ∈{32, 64}, and the number of dense\nunits is nden ∈{512, 1024}. The decoder GD realizes\n\n\n1\n1\nnden\n\n→\nd,a . . . →\nd,a\n\n\n1\n1\nnden\n\n→\nd,a\n\n\n1\n1\nngen\n\n→\nct,a\n\n\n48\n48\n16\n\n\n(27)\n(ct denotes transposed convolution) followed by a Hermitian-\nizing operation H : C48×48×16 →C48×48×16,\n(H(C))ijk = 1\n2\n\u0000Cijk + C∗\njik\n\u0001\n.\n(28)\nThe discriminator D is similar in structure to the encoder,\nbut with removed dense layers, and the number of convolution\nfilters now is ndis ∈{16, 32}:\n\n\n48\n48\n16\n\n→\nc,a\n\n\n1\n1\nndis\n\n→\nsig [0, 1].\n(29)\nMoreover, a real sigmoid activation function sig is applied the\nreal and the imaginary parts of the convolution output (a so-\ncalled split-type A activation, see [2]), and the corresponding\nresult is averaged over all dimensions of its real representation.\nD. Training set and loop\nFollowing the notation of III-A, we now fix\nZ = C48×48×16,\nB = C48×48×16.\n(30)\nMoreover, we decide on a transformation rule the generator\nG is intended to learn (cf. IV-B). More precisely, we decide\non sets X ⊂B and Y ⊂B and a map f : X →Y. For\n\n\nexample, in the most trivial case, G could be trained to work\nas an auto-encoder on a fixed set X ⊂B by setting Y = X\nand f = Id.\nIn each pass through the training loop, zxi ∈ZX and zyi ∈\nZY is generated out of xi ∈X and yi = f(xi) ∈Y = f(X)\n(with X being a mini-batch drawn randomly from X, and Z =\nZX, see III-A) by adding noise, respectively. Based on III-A,\nwe add another term to LG to integrate the transformation rule\nintended for G,\nLG = −1\nN\nN\nX\ni=1\nlog(D(G(zxi)))\n(31)\n+ λ\n2N\nN\nX\ni=1\nε(yi, G(zxi)) + ε(yi, G(zyi)).\n(32)\nHere, N = #X, λ > 0,\nε(a, b) = 1\nK\nK\nX\nk=1\nd(πk(a), πk(b)),\n(33)\nd(ma, mb) =κ\n\u0012\n1 −tr(ma · mb)\n∥ma∥∥mb∥\n\u0013\n(34)\n+ (1 −κ) |∥ma∥−∥mb∥| ,\n(35)\nK = 16, κ = 9/10. Moreover, ∥· ∥denotes the Frobenius\nnorm and πk : C48×48×16 →C48×48 is the projection onto\nthe k-th component,\nπk(C) = (Cijk)i,j=1,...,48.\n(36)\nTo summarize (31) and (32), G is sought to be a denoising\nopponent to the discriminator that, on the one hand, realizes\nthe map f on elements of X, and, on the other hand, is an\nauto-encoder for the elements of Y.\nIV. RESULTS\nA. Hyperparameter optimization\nSome of the parameters involved in the training process\nwere chosen based on preliminary experiments or computa-\ntional limitations. For example, we fixed the mini-batch size\nto be N = 16, and the size of the training data set was\nchosen as #X = 2560. Moreover, each component πk(C)\nof C ∈X, Y was normalized with respect to its Frobenius\nnorm as a preprocessing step after the pressure simulations\nof II-C. As a consequence, we were able to balance (31) and\n(32) in terms of their magnitude by setting λ = 200. For the\nstochastic gradient descent of generator and discriminator, we\nused Adam optimizers with β1 = 0.5, β2 = 0.999, ϵ = 10−7\nwithout exponential moving average (see [7]).\nThe remaining parameters were chosen in a hyperparameter\noptimization. As metric to assess the quality of this optimiza-\ntion, we opted for the average accuracy\ngacc(G) = 1 −\n1\n#Xtest\nX\nx∈Xtest\nε(f(x), G(x))\n(37)\non a test data set Xtest of 512 elements disjoint to X\nusing our weighted distance function ε (see (33)) that utilizes\nthe correlation matrix distance of [6]. The 512 parameter\ncombinations that were tested included the number of con-\nvolution filters in the generator ngen ∈{32, 64} and the\ndiscriminator ndis ∈{16, 32}, the number of dense units\nnden ∈{512, 1024} and dense layers nlay ∈{1, 2, 3, 4} in the\nencoder and decoder, the learning rates lr ∈{2·10−4, 2·10−5}\nof the Adam optimizers for generator and discriminator, and\nthe used activation function: FmReLU with b ∈{−1/8, −1/4}\nor FlCard with α ∈{0, 1/2}. The combination\nngen = 64,\nndis = 16,\nnden = 512,\n(38)\nnlay = 1,\nlgen\nr\n= ldis\nr\n= 2 · 10−5\n(39)\ntogether with FlCard, α = 1/2 yielded the maximum value of\ngacc(G) = 0.9866 after 100 epochs of training using Y = X\nand f = Id (more specifically, task 1) in IV-B).\nB. Transformation tasks\nAfter hyperparameter optimization, we investigated 5 trans-\nformation tasks G was intended to learn. For each of these\ntasks, we started by selecting a set M = {m1, . . . , mM}\nof M = 2560 models from our model set we presented\nin II-C. We then simulated each of these models mi in\ntwo different complexities creating xi\n∈C48×48×16 and\nyi ∈C48×48×16. The latter built up X and Y, respectively,\nand we set f(xi) = yi. We considered the following pairings:\n1) Auto-encoder pairing, xi = yi\n2) xi exhibits ambient sound, yi does not\n3) xi exhibits reflections, yi does not\n4) xi exhibits directivity, yi does not\n5) xi exhibits directivity, reflections and ambient sound, yi\nnone of these\nIf not stated otherwise here, a model was simulated with\nmonopole sources, without reflections and with no ambient\nsound present.\nAfter 1000 epochs of training, we tested the resulting\ngenerator G by reiterating the previous steps for a model set\nMtest = {m1, . . . , mMtest} disjoint to M, Mtest = 512. For\neach element x of the corresponding set Xtest, we evaluated\ngx\nacc(G) = 1 −ε(f(x), G(x)),\n(40)\nand gacc(G) (see (37)) is simply the average of these results,\ngacc(G) =\n1\n#Xtest\nX\nx∈Xtest\ngx\nacc(G).\n(41)\nFor example, the results of the auto-encoder transformation\ntask 1) can be found in fig. 1. The average accuracy was\ngacc(G) = 0.9948 in this case serving as baseline, as the\ncorresponding results of all other cases are expected to be\nlower or equal. In addition, we place these values side-by-\nside with gx\nacc(Id) = 1 −ε(f(x), x) and its average gacc(Id),\nrespectively, in order to quantify the initial situation in com-\nparison for all transformation tasks (see figs. 1-5).\nThe lowest resulting gacc(G) values were achieved in cases\n4) and 5) where the corresponding transformation task in-\ncluded to remove the directivity information from the data.\n\n\nThis could be expected as the microphone array used in the\nsimulations was only of small aperture compared to size of the\ntotal acoustic scene. Moreover, when analyzing gacc(G) as a\nfunction of learning epoch, it became clear, that the training\nprogress was much slower and did not converge within 1000\nepochs of training in the cases 4) and 5). Studying fig. 3 more\ncarefully, we realize that there are many models mi ∈Mtest\nwith ε(f(xi), xi) = 0 for transformation task 3). This is due\nto the fact that although a reflection plane was present, its\nposition did not render a reflection possible.\nFig. 1. Accuracy scatter plot for transformation task 1) (auto-encoder)\nFig. 2. Accuracy scatter plot for transformation task 2) (ambient sound)\nFig. 3. Accuracy scatter plot for transformation task 3) (reflections)\nFig. 4. Accuracy scatter plot for transformation task 4) (directivity)\nFig. 5. Accuracy scatter plot for transformation task 5) (directivity, reflections\nand ambient sound)\nREFERENCES\n[1] M. Arjovsky, A. Shah, and Y. Bengio, “Unitary evolution recurrent\nneural networks,” Proceedings of the 33rd International Conference on\nMachine Learning (ICML’16), 2016.\n[2] J. Bassey, “A Survey of Complex-Valued Neural Networks,” 2021,\narXiv:2101.12249. [Online]. Available: https://arxiv.org/abs/2101.12249.\n[3] L. Bottou: “Online Algorithms and Stochastic Approximations,” Cam-\nbridge University Press, 1998.\n[4] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative Adversarial Nets,”\nProceedings of the 27th International Conference on Neural Information\nProcessing Systems (NIPS 2014), 2014.\n[5] P.-A. Grumiaux, S. Kiti´c, L. Girin, and A. Gu´erin, “A survey of sound\nsource localization with deep learning methods,” J. Acoust. Soc. Am.\n152, 107–151, 2022.\n[6] M. Herdin, N. Czink, H. Ozcelik, and E. Bonek, “Correlation matrix\ndistance, a meaningful measure for evaluation of non-stationary MIMO\nchannels ,” Proceedings of the 2005 IEEE 61st Vehicular Technology\nConference (VETECS 2005), 2005.\n[7] D. Kingma, and J. L. Ba, “Adam: A Method for Stochastic Optimiza-\ntion,” Proceedings of the 3rd International Conference for Learning\nRepresentations (ICLR 2015), 2015.\n[8] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and\nX. Chen, “Improved techniques for training GANs,” Advances in Neural\nInformation Processing Systems 29 (NIPS 2016), 2016.\n[9] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J. F.\nSantos, S. Mehri, N. Rostamzadeh, Y. Bengio, and C. J. Pal, “Deep\nComplex Networks,” Proceedings of the 6th International Conference\non Learning Representations (ICLR 2018), 2018.\n[10] P. Virtue, X. Y. Stella, and M. Lustig, “Better than real: complex-\nvalued neural nets for MRI fingerprinting,” Proceedings of the 2017\nIEEE International Conference on Image Processing (ICIP 2017), 2017.\n[11] E. G. Williams, “Fourier acoustics: sound radiation and nearfield acous-\ntical holography,” Academic Press, 1999.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21097v1.pdf",
    "total_pages": 5,
    "title": "Deep learning-based filtering of cross-spectral matrices using generative adversarial networks",
    "authors": [
      "Christof Puhle"
    ],
    "abstract": "In this paper, we present a deep-learning method to filter out effects such\nas ambient noise, reflections, or source directivity from microphone array data\nrepresented as cross-spectral matrices. Specifically, we focus on a generative\nadversarial network (GAN) architecture designed to transform fixed-size\ncross-spectral matrices. Theses models were trained using sound pressure\nsimulations of varying complexity developed for this purpose. Based on the\nresults from applying these methods in a hyperparameter optimization of an\nauto-encoding task, we trained the optimized model to perform five distinct\ntransformation tasks derived from different complexities inherent in our sound\npressure simulations.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}