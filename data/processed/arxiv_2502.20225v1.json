{
  "id": "arxiv_2502.20225v1",
  "text": "DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with\nContrastive Training Strategy for Deepfake Speech Detection\nLam Pham‚àó, Dat Tran‚àó, Florian Skopik, Alexander Schindler,\nSilvia Poletti, Fischinger David, Martin Boyer\nAbstract‚Äî In this paper, we propose a deep neural network\napproach for deepfake speech detection (DSD) based on a low-\ncomplexity Depthwise-Inception Network (DIN) trained with a\ncontrastive training strategy (CTS). In this framework, input\naudio recordings are first transformed into spectrograms using\nShort-Time Fourier Transform (STFT) and Linear Filter (LF),\nwhich are then used to train the DIN. Once trained, the DIN\nprocesses bonafide utterances to extract audio embeddings,\nwhich are used to construct a Gaussian distribution repre-\nsenting genuine speech. Deepfake detection is then performed\nby computing the distance between a test utterance and this\ndistribution to determine whether the utterance is fake or\nbonafide. To evaluate our proposed systems, we conducted\nextensive experiments on the benchmark dataset of ASVspoof\n2019 LA. The experimental results demonstrate the effective-\nness of combining the Depthwise-Inception Network with the\ncontrastive learning strategy in distinguishing between fake and\nbonafide utterances. We achieved Equal Error Rate (EER),\nAccuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and\n98.9% respectively using a single, low-complexity DIN with just\n1.77 M parameters and 985 M FLOPS on short audio segments\n(4 seconds). Furthermore, our proposed system outperforms the\nsingle-system submissions in the ASVspoof 2019 LA challenge,\nshowcasing its potential for real-time applications.\nItems‚Äî deepfake audio, spectrogram, feature extraction,\nclassification model.\nI. INTRODUCTION\nThanks to advancements in deep learning technologies,\nspeech generation systems now beneficially support a wide\nrange of real-world applications including text-to-speech for\nindividuals with speech disorders, voice chatbots in call\ncenters, cross-linguistic speech translation, etc. However,\ndeep learning has also facilitated the creation of fake speech\nfor malicious purposes, such as spoofing attacks, raising\nserious security concerns. As a result, deepfake speech de-\ntection (DSD) has recently gained significant attention from\nthe research community. The state-of-the-art systems, which\nhave been proposed for the task of deepfake speech detection,\napproach neural network architectures and deep learning\ntechniques [1], [2], [3]. To achieve a high-performance\nDSD system, ensembles of input features or models are\nleveraged. In particular, multiple input features of LFCC,\nPSCC, LLFB were used in [4]. Similarly, authors in [5]\nmade uses of MEL, CQT, GAM, LFCC, and Wavelet based\nspectrograms. Regarding model ensembling, two approaches\nare commonly used. The first involves fusing the individual\nL. Pham, F. Skopik, A. Schindler, S. Poletti, F. David, and M. Boyer are\nwith Austrian Institute of Technology, Vienna, Austria\nD. Tran is with FPT University, Vietnam\n(*) Main and equal contribution into the paper.\nresults of multiple models, as seen in [6] with the use of\nLCNN and ResNet. The second approach integrates mul-\ntiple branches within a single network architecture, where\nfeature maps extracted from different pre-trained models\nare combined. For an example, feature maps from XLS-R,\nWavLM, and Hubert are merged in [7]. Although ensemble\nmodels prove effective to achieve high performance (i.e.,\nthe best systems proposed in deepfake speech detection\nchallenges of ASVspoof 2019 [8], 2021 [9], and 2024 [10]\nleveraged ensemble models), this method leads an issue of\na high-complexity model with a large number of trainable\nparameters and FLOPS. This poses a challenge for applying\nensemble models to real-world applications that require a\nreal-time inference or are constrained by hardware limita-\ntions. Recently, single models with encoder-decoder based\narchitectures have become popular [1]. In these systems,\nencoder architectures leverage pre-trained models such as\nWhisper [11], WavLM [12], or Wave2Vec2.0 [13] (i.e., these\nmodels were trained on large-scale datasets of human speech\nin advance) to extract general feature maps. Meanwhile,\ndecoders present diverse architectures such as Multilayer\nPerceptron [5], GAN-based architecture [14], Multi-feature\nattention [15], Graph Attention Network [16] to explore\nthe feature maps extracted from the encoders. Although\nthis approach proposes single models for the DSD task,\nleveraging pre-trained models as encoders still results in a\nhighly complex system. For examples, the smallest Whisper\nmodel presents 39 M parameters and the largest Whisper\nmodel has 1550 M parameters. Meanwhile, the pre-trained\nWave2Vec2.0 BASE and LARGE models presents around 95\nM and 300 M of parameters, respectively. Additionally, this\napproach mainly focuses on exploring encoder and decoder\narchitectures rather than analyzing training strategies which\nenforce the model to separate the distributions of bonafide\nand fake utterances.\nTo tackle these mentioned limitations, we propose a deep-\nlearning-based model for the deepfake speech detection\n(DSD) and highlight the contributions: (1) We first propose\na novel and low-complexity deep neural network for DSD\ntask that is inspired by depthwise convolution and incep-\ntion architectures, referred to as the Depthwise-Inception\nNetwork (DIN). (2) To train DIN model, we propose a\ncontrastive training strategy (CTS) that proves effective to\nseparate distribution of bonafide and fake utterances. (3) By\ncombining DIN model and the CTS method, we achieved a\nlow-complexity and high-performance DSD system.\narXiv:2502.20225v1  [cs.SD]  27 Feb 2025\n\n\nBonafide loss\n(L3)\nBackbone\n.\n.\n.\n.\nXb\n.\n.\n.\n.\nBonafide\nsegments\nN ( ùúá, ‚àë)\nBackbone\n.\n.\nA-Softmax loss\n(L1)\nContrastive loss\n(L2)\n.\n.\nFC ‚Äì BN ‚Äì GELU\nFC ‚Äì Softmax\n.\n.\nFC ‚Äì BN ‚Äì GELU\nFC  ‚Äì BN ‚Äì GELU\n.\n.\n.\n.\n.\n.\nX\n.\n.\n.\n.\n.\n.\n.\n.\nY\nZ\n.\n.\n.\n.\nFrozen\n4-second \nsegments\nContrastive\nHead\nSoftmax\nHead\nConv [4,4] ‚Äì BN \n- GELU\nDew-Inc\nDownSample\nDew-Inc\nDownSample\nDew-Inc\nDownSample\nDew-Inc\nGM Pooling\n3x\n3x\n9x\n3x\nConv [1x1] ‚Äì BN -\nGELU\nConv [1x1] ‚Äì BN ‚Äì\nGELU. ‚ÄìCcnv [3x3]\nConv [1x1] ‚Äì BN ‚Äì\nGELU. ‚ÄìCcnv [3x1]\nConv [1x1] ‚Äì BN ‚Äì\nGELU. ‚ÄìConv [5x1]\nConcat\nBN ‚Äì GELU ‚Äì Conv [1,1] ‚Äì BN\nShortcut\n+\nBackbone\n.\n.\n.\n.\nX\n.\n.\n.\n.\n4-second \nsegments\nFC ‚Äì Softmax\nEntropy loss\nEntropy\nHead\nTransfer\nStage 1\nStage 2\nStage 3\nMultiple Classes & Multiple Losses Training\nTwo-class Finetuning\nBonafide Distribution\nFig. 1.\nThe proposed Depthwise-Inception Network architecture and Contrastive Training Strategy\nII. DEPTHWISE-INCEPTION NETWORK ARCHITECTURE\nAND CONTRASTIVE TRAINING STRATEGY\nThe detailed architecture of the proposed deep-learning-\nbased model is denoted in Fig. 1 and comprises two main\nparts: spectrogram feature extraction and deep learning\nmodel.\nA. Spectrogram Feature Extraction\nThe input utterances are first split into 4-second audio\nsegments. This segment length generally provides sufficient\ncontext to capture important features and allows faster train-\ning and inference for applications that require real-time\ndetection. Then, Short Time Fourier Transform (STFT) and\nLinear Filter (LF) are applied on 4-second segments to\ngenerate STFT-LF spectrograms. By setting window size,\nhop size, and linear filter number of 1024, 512, and 64\nrespectively, we obtain STFT-LF spectrograms of 128√ó128.\nThen, each spectrogram is concatenated with its first and sec-\nond deviations to create a 3-channel representation, resulting\nin a size of 3√ó128√ó128. Finally, we apply the SpecAug data\naugmentation [17] to the spectrograms before feeding into\nthe backend classification model.\nB. Depthwise-Inception Network Architecture\nBy comparing pairs of bonafide and fake utterances, we\nobserved that the power distribution across the frequency\naxis in fake spectrograms follows a regular pattern, whereas\nin bonafide utterances, it is more variable. This inspired\nus to make use of inception network layers which are\neffective to learn the minor difference in local regions\nof the spectrograms. Additionally, we leverage depthwise\nconvolution layers and pointwise convolution layers rather\nthan the traditional convolution layers to reduce the model\nsize, but still preserve the power of convolution computation\nto be able to generate distinct feature maps between fake and\nbonafide utterances. By combining depthwise convolution\nand inception layers, we construct the novel Depthwise-\nInception Network (DIN). Fig. 1 illustrates the proposed\nDIN architecture, which can be divided into two main\ncomponents: the backbone and the heads. The backbone\nas shown in the right side of the Fig. 1 presents a dense\nlayer (Conv [4x4], BatchNorm (BN) - Gaussian Error Linear\nUnits (GELU)), followed by four Dew-Inc blocks, and a final\nGlobal Max Pooling (GM Pooling) layer. For each Dew-Inc\nblock, it presents inception and residual based architecture\nwith four branches of inception-convolution layers with\ndifferent convolutional kernels (i.e., [1√ó1], [3√ó3], [3√ó1],\n[5√ó1]) and one residual shortcut. The output of the backbone\nis fed into two heads: Softmax head and Contrastive head.\nSoftmax head presents two dense layers. The first dense layer\npresents Fully Connected layer (FC), followed by BN and\nGELU. The second dense layer comprises FC and Softmax\nlayers. Meanwhile, Contrastive head performs two dense\nlayers which share the same configuration of FC, BN, and\nGELU.\nC. Contrastive Training Strategy\nTo train the proposed DIN model, we introduce a con-\ntrastive training strategy with three training stages. In the first\nstage, we train DIN model with multiple classes and multiple\nloss functions. While bonafide utterance is considered as one\nclass, fake generators are the other classes. Training with\nmultiple classes enforces DIN model to learn distinct features\namong bonafide and fake utterances.\nTo further enforce the training process, we propose three\nloss functions. Let N be the number of spectrograms\nin each training batch. Then, the output feature vector\nof the backbone, the Softmax head and the Contrastive\nhead are indicated as X = {x1, x2, ..., xN}, the output\nfeature maps of Softmax head and Contrastive head as\nY = {y1, y2, ..., yN} and Z = {z1, z2, ..., zN}, respec-\ntively. The first loss function L1 is A-Softmax loss as shown\nin Equation (1).\nL1 = 1\nN\nPN\nn=1 ‚àílog\n\u0010\nexp(sœï(Œ∏n\nc ))\nexp (sœï(Œ∏n\nc ))+P\njÃ∏=c exp(s cos Œ∏n\nj )\n\u0011\n(1)\n\n\nwhere œï(Œ∏) = (‚àí1)k cos (mŒ∏) ‚àí2k, m is the margin, s is\nthe scalar factor; Œ∏n\nc is the angle between the embedding yn\nand the class weight wyn, c is the label of the embedding\nyn. This loss is applied to the feature maps Y for detecting\nmultiple classes of bonafide and fake generators. We use A-\nSoftmax loss [18] rather than the traditional Entropy loss\nto maximize angles between feature maps from different\nclasses. m and s are empirically set to 4 and 30, respectively.\nThe second loss L2 is a type of contrastive loss which\nis applied for self-supervised learning [19]. To apply this\nloss, we consider only two fake speech generators, namely\nText-To-Speech (TTS) generator and Voice Conversion (VC)\ngenerator. The loss, which is applied on the feature maps Z,\naims to minimize the distances among feature maps of the\nsame classes ZC and maximize the distances among feature\nmaps of the different classes of ZC and ZJ, where ZC and\nZJ are subsets of Z with C + J = N. The loss is defined\nas:\nL2 = 1\nN\nPN\nn=1\n‚àí1\nC\nPC\nc=1 log\n\u0012\nexp(zn¬∑zc/œÑ)\nexp(zn¬∑zc/œÑ)+PJ\nj=1 exp(zn¬∑zj/œÑ)\n\u0013\n(2)\nwhere œÑ is empirically set to 0.01.\nA key consideration is that, with the advancement of deep\nlearning, SDS systems should be continuously updated to\nkeep pace with emerging fake speech generators. However,\ngiven the rapid proliferation of new generators, this is often\nimpractical. As a result, SDS models frequently encounter a\nlarge number of unseen fake speech they were not trained\non, posing a challenge to their effectiveness. To tackle this\nissue, we develop a loss function which focuses on bonafide\nrather than fake speech. In particular, the loss function aims\nto minimize the variance of the distribution of feature maps\nof bonafide utterances. The loss is presented by Equation (3)\nL3 = 1\nK\nK\nX\nk=1\n||xk ‚àíc||2\n2\n(3)\nwhere c is the central feature map of bonafide utterances,\nxk is a bonafide feature map obtained from the backbone,\nK is the number of bonafide utterances in the bach of N\nspectrograms (K < N). The central feature map c is the\naverage of bonafide feature maps in each training batch, but\nit is recomputed from all bonafide feature maps in the entire\ntraining set every 5 epoches.\nBy combining three loss function, we obtain the final loss\nL to train the DIN archtiecture as the Equation (4)\nL = Œ±L1 + Œ≤L2 + Œ≥L3\n(4)\nwhere Œ±, Œ≤, and Œ≥ are empirically set to 0.2, 0.4, 0.4,\nrespectively.\nIn the second stage, we replace the A-Softmax and Con-\ntrastive heads by a new head, referred to as the Entropy head\nwith only a FC layer and a Softmax layer. While the Entropy\nhead is fine-tuned with a high learning rate, the trainable\nparameters in the backbone are fine-tuned with a low learning\nrate at this stage.\nTABLE I\nPROPOSED CONTRASTIVE TRAINING STRATEGY\nAlgorithm 1: Contrastive Training Strategy\nInput: A set of T spectrograms split into batches of Bt = {I1, I2, . . . , IN}\nOutput: Trained model for fake/bonafide speech detection.\nComponents:\n- The backbone E to extract a set of embeddings X = {x1, x2, . . . , xN} from batch B\n- The Softmax Head SH to extract embeddings Y = {y1, y2, . . . , yN} from X\n- The Contrastive Head CH to extract embeddings Z = {z1, z2, . . . , zN} from X\n- A set of 4 losses represented by functions: H = {H1, H2, H3, H4}\nThe first stage:\nfor e = 1 to Training Epochs do:\nfor t = 1 to T/N do:\n- Extract embeddings X, Y, Z from batch Bt:\nX ‚ÜêE(Bt)\nY ‚ÜêSH(X)\nZ ‚ÜêCH(X)\n- Compute losses:\nL1 ‚ÜêH1(Y )\nL2 ‚ÜêH2(Z)\nL3 ‚ÜêH3(X)\n- Compute final loss:\nL ‚ÜêŒ±L1 + Œ≤L2 + Œ≥L3\n- Backward pass and update weights of backbone and heads\nend for\nend for\nThe second stage:\nfor e = 1 to Finetune Epochs do:\nfor t = 1 to T/N do:\n- Extract embeddings X from batch Bt:\nX ‚ÜêE(Bt)\n- Compute Entropy loss:\nLEntropy ‚ÜêH4(X)\n- Backward pass and update weights\nend for\nend for\nThe third stage:\n- Extract bonafide embeddings Xb from all batches Bt:\nXb ‚ÜêE(Bt)\n- Compute distribution of bonafide embeddings:\nXb ‚àºN(¬µ, Œ£)\nIn the third stage, we fed only bonafide utterances into\nthe pre-trained DIN model obtained from the second stage,\nextracting the output feature maps of the backbone that are\ndenoted by the Xb in Fig. 1. Then, the mean ¬µ and the\nco-variance matrix Œ£ are computed from the feature maps\nXb. In other words, we obtain the Gaussian distribution\nof bonafide utterances Xb ‚àºN(¬µ, Œ£). In summary, the\nproposed contrastive training strategy is described in Table I.\nD. Inference Process\nIn the inference process, an testing utterance is fed into\nthe pre-trained DIN model in the second stage to extract the\noutput feature map of the backbone xt. The Mahalanobis\ndistance d between the testing feature map xt and the\nGaussian distribution of bonafide utterances N(¬µ, Œ£) is\ncomputed by Equation (5) to decide whether the testing\nutterance is fake or bonafide.\nd{xt, N(¬µ, Œ£)} =\nq\n(xt ‚àí¬µ)T Œ£‚àí1(xt ‚àí¬µ)\n(5)\nIII. EXPERIMENTS AND RESULTS\nA. Datasets and evaluation metrics\nWe evaluate the proposed models on the Logic Access\ndataset of ASVspoof 2019 challenge (ASV2019-LA). The\nmodels are trained on the ‚ÄòTrain‚Äô subset and evaluated on\n\n\nTABLE II\nPERFORMANCE COMPARISON BETWEEN BASELINE (RESNET18) AND OUR PROPOSED DEEP LEARNING MODELS ON ASV2019-LA\nNetwork & Settings\nAcc ‚Üë\nF1 ‚Üë\nAUC ‚Üë\nEER ‚Üì\nParameters ‚Üì\nFLOPS ‚Üì\n(Baseline) ResNet18 w/ Entropy head & Entropy loss\n92.0%\n95.4%\n96.6%\n8.0%\n11.18M\n1192M\n(M1) DIN backbone w/ Entropy head & Entropy loss\n92.1%\n95.5%\n97.5%\n7.9%\n1.77M\n985M\n(M2) DIN backbone w/ multiple heads, multiple losses (stage 1&2)\n94.6%\n97.0%\n98.4%\n5.3%\n1.77M\n985M\n(M3) DIN backbone w/ multiple heads, multiple losses (3 stages)\n95.4%\n97.3%\n98.9%\n4.6%\n1.77M\n985M\nFig. 2.\nHistogram of Mahalanobis distances between the training bonafide distribution (‚ÄòTrain‚Äô subset) and the test bonafide & fake utterances (‚ÄòEvaluating‚Äô\nsubset) with the model M1 on the left side and the model M3 on the right side\n‚ÄòDevelopment‚Äô subset from ASV2019-LA. These subsets\ncomprise bonafide utterances and fake utterances from six\nspeech generators (2 VC and 4 TTS systems), referred to\nas A01 to A06. Finally, the models are tested on the ‚ÄòEval-\nuation‚Äô subsets from ASV2019-LA. This subset comprises\nbonafide utterances and fake utterances from 13 speech gen-\nerators, referred to as A07 to A19, which are different from\nfake generators in ‚ÄòTrain‚Äô and ‚ÄòDevelopment‚Äô subsets. We\nfollow the ASVspoof challenge guidelines and use the Equal\nError Rate (EER) as the primary evaluation metric for the\nproposed models. Additionally, we report Accuracy (Acc.),\nF1 score, and AUC to facilitate performance comparison\namong the models.\nB. Experimental settings\nThe proposed SDS system has been developed within the\nPytorch framework. The architecture has been trained for 60\nepochs in total, using Titan RTX 24GB GPU. The first 50\nepochs have been used for the Stage 1 of the training. Then,\nthe model is finetuned on two classes (fake/bonafide) for the\nremaining 10 epoches in Stage 2. The Adam method [20] is\nused for the optimization.\nC. Results and discussion\nTo evaluate our proposed model, we first construct a\nbaseline leveraging ResNet18 backbone for feature map ex-\ntraction and an Entropy head for classification (This Entropy\nhead presents the architecture mentioned in stage 2 of the\ntraining process in section II-C). The baseline is trained on\ntwo classes (fake and bonafide) using Cross-Entropy loss.\nWe also evaluated three other models, referred to as M1,\nM2, and M3. M1 uses the architecture in the stage 2 which\npresents the depthwise-inception backbone as shown in right\nside of Fig. 1 and the Entropy head. This model is train\nfrom scratch on two classes (fake and bonafide) using Cross-\nEntropy loss. We compare M1 with the baseline to evaluate\nthe role of proposed DIN architecture. M2 presents the\narchitecture in the stage 1. Then, this model is fine-tuned\nin stage 2, but it is not applied in the stage 3. We evaluate\nthe role of contrastive training strategy with multiple classes\nand multiple loss functions in M2 model. Finally, M3 is the\nproposed full model of Fig. 1 including the entire three-stage\ntraining strategy which leads to the estimation of the bonafide\ndistribution in stage 3.\nExperimental results in Table II indicate that M1 with\nDIN architecture outperforms the ResNet18 baseline over\nall metrics. While M1 presents a complexity with 1.77 M\nparameters and 985 M FLOPS, ResNet baseline shows a\nmuch higher complexity with 11.18 M parameters and 1192\nM FLOPS. When the proposed contrastive training strategy\nin stages 1 and 2 is applied to M2, it significantly improves\nthe EER performance by 2.6%. Applying stage 3 where the\nbonafide distribution is used to measure the Mahalanobis\ndistances among utterances helps to further improve all\nmetric scores.\nThe visualization shown in Fig. 2 indicates that the pro-\nposed three-stage contrastive training strategy is effective to\nseparate the distributions of fake and bonafide utterances.\nThis leads a smaller overlapping region between bonafide and\nfake utterances in M3 model compared with M1 model. The\nvisualization shown in Fig. 3 again proves that the contrastive\n\n\nFig. 3. TSNE-based visualization of the output feature maps extracted from\nthe backbone (embedding X extracted from model M1 on the left side and\nextracted from model M3 on the right side) among testing bonafide and\nfake utterances (‚ÄòEvaluation‚Äô subset with A07 to A19 generators)\ntraining strategy is effective to separate bonafide utterances\nand fake utterances from different generators.\nAs a result, we achieve the single model M3 with the\nbest EER score of 4.6%, which outperforms submitted single\nsystems in the challenge of ASV-2019 LA task (Top-4 single\nsystem in the challenges: T32 (4.92%), T04 (5.74%), T01\n(6.01%), and T58 (6.14%) in [21]). With a low complexity\nof 1.77 M parameters and Acc., F1, AUC scores of 95.4%,\n97.3%, and 98.9%, the model M3 shows strong potential for\nreal-time deployment in DSD systems.\nIV. CONCLUSION\nThis paper has presented a deep-learning-based model for\nthe task of deepfake speech detection. By combining the\ndepthwise-inception network architecture and the three-stage\ncontrastive training strategy, we achieve a low-complexity\nand high-performance single model (M3) which shows great\nfor real-time DSD applications.\nACKNOWLEDGMENTS\nEUCINF project is co-funded by European Union un-\nder grant agreement N¬∞101121418. Views and opinions\nexpressed are however those of the author(s) only and do\nnot necessarily reflect those of the European Union or the\nEuropean Commission. Neither the European Union nor the\ngranting authority can be held responsible for them.\nDefame Fakes is funded by the Austrian security research\nprogramme KIRAS of the Federal Ministry of Finance\n(BMF).\nREFERENCES\n[1] Lam Pham et al., ‚ÄúA comprehensive survey with critical analysis for\ndeepfake speech detection,‚Äù arXiv preprint arXiv:2409.15180, 2024.\n[2] Menglu Li, Yasaman Ahmadiadli, and Xiao-Ping Zhang, ‚ÄúAudio anti-\nspoofing detection: A survey,‚Äù arXiv preprint arXiv:2404.13914, 2024.\n[3] Zahra Khanjani, Gabrielle Watson, and Vandana P Janeja,\n‚ÄúAudio\ndeepfakes: A survey,‚Äù\nFrontiers in Big Data, vol. 5, pp. 1001063,\n2023.\n[4] Woo Hyun Kang, Jahangir Alam, and Abderrahim Fathan, ‚ÄúCrim‚Äôs\nsystem description for the asvspoof2021 challenge,‚Äù in Proc. INTER-\nSPEECH, 2021, pp. 100‚Äì106.\n[5] Lam Pham, Phat Lam, Truong Nguyen, Huyen Nguyen, and Alexander\nSchindler, ‚ÄúDeepfake audio detection using spectrogram-based feature\nand ensemble of deep learning models,‚Äù in Proc. IS2, 2024, pp. 1‚Äì5.\n[6] Anton\nTomilov,\nAleksei\nSvishchev,\nMarina\nVolkova,\nArtem\nChirkovskiy, Alexander Kondratev, and Galina Lavrentyeva,\n‚ÄúStc\nantispoofing systems for the asvspoof2021 challenge,‚Äù\nin Proc.\nINTERSPEECH, 2021, pp. 61‚Äì67.\n[7] Yujie Yang, Haochen Qin, Hang Zhou, Chengcheng Wang, Tianyu\nGuo, Kai Han, and Yunhe Wang, ‚ÄúA robust audio deepfake detection\nsystem via multi-view feature,‚Äù in Proc. ICASSP, 2024, pp. 13131‚Äì\n13135.\n[8] Xin Wang et al., ‚ÄúAsvspoof 2019: A large-scale public database of\nsynthesized, converted and replayed speech,‚Äù\nComputer Speech &\nLanguage, vol. 64, pp. 101114, 2020.\n[9] Junichi Yamagishi et al., ‚ÄúAsvspoof 2021: accelerating progress in\nspoofed and deepfake speech detection,‚Äù in Proc. INTERSPEECH,\n2021, pp. 10744‚Äì10753.\n[10] Xin Wang et al., ‚ÄúAsvspoof 5: Crowdsourced speech data, deepfakes,\nand adversarial attacks at scale,‚Äù in Proc. INTERSPEECH, 2024, pp.\n1008‚Äì1012.\n[11] Alec Radford et al., ‚ÄúRobust speech recognition via large-scale weak\nsupervision,‚Äù in Proc. ICML, 2023, pp. 28492‚Äì28518.\n[12] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie\nLiu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong\nXiao, et al., ‚ÄúWavlm: Large-scale self-supervised pre-training for full\nstack speech processing,‚Äù IEEE Journal of Selected Topics in Signal\nProcessing, vol. 16, no. 6, pp. 1505‚Äì1518, 2022.\n[13] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael\nAuli,\n‚Äúwav2vec 2.0: a framework for self-supervised learning of\nspeech representations,‚Äù in Proc. NIPS, 2020.\n[14] Junlong Deng, Yanzhen Ren, Tong Zhang, Hongcheng Zhu, and\nZongkun Sun,\n‚ÄúVFD-net: Vocoder fingerprints detection for fake\naudio,‚Äù in Proc. ICASSP, 2024, pp. 12151‚Äì12155.\n[15] Yinlin Guo, Haofan Huang, Xi Chen, He Zhao, and Yuehai Wang,\n‚ÄúAudio deepfake detection with self-supervised wavlm and multi-\nfusion attentive classifier,‚Äù in Proc. ICASSP, 2024, pp. 12702‚Äì12706.\n[16] Hemlata Tak, Massimiliano Todisco, Xin Wang, Jee-weon Jung, Ju-\nnichi Yamagishi, and Nicholas Evans, ‚ÄúAutomatic speaker verification\nspoofing and deepfake detection using wav2vec 2.0 and data augmen-\ntation,‚Äù in Proc. INTERSPEECH, 2022, pp. 112‚Äì119.\n[17] D. S. Park, W. Chan, Y. Zhang, C. Chiu, B. Zoph, E. D. Cubuk, and\nQ. V. Le,\n‚ÄúSpecaugment: A simple data augmentation method for\nautomatic speech recognition,‚Äù preprint arXiv:1904.08779, 2019.\n[18] Yutian Li, Feng Gao, Zhijian Ou, and Jiasong Sun, ‚ÄúAngular softmax\nloss for end-to-end speaker verification,‚Äù in Proc. ISCSLP, 2018, pp.\n190‚Äì194.\n[19] Patrick Feeney and Michael C Hughes,\n‚ÄúSincere: Supervised in-\nformation noise-contrastive estimation revisited,‚Äù\narXiv preprint\narXiv:2309.14277, 2023.\n[20] P. K. Diederik and B. Jimmy,\n‚ÄúAdam: A method for stochastic\noptimization,‚Äù CoRR, vol. abs/1412.6980, 2015.\n[21] Massimiliano Todisco et al.,\n‚ÄúAsvspoof 2019: Future horizons in\nspoofed and fake audio detection,‚Äù in Proc. INTERSPEECH, 2019,\npp. 1008‚Äì1012.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20225v1.pdf",
    "total_pages": 5,
    "title": "DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with Contrastive Training Strategy for Deepfake Speech Detection",
    "authors": [
      "Lam Pham",
      "Dat Tran",
      "Florian Skopik",
      "Alexander Schindler",
      "Silvia Poletti",
      "Fischinger David",
      "Martin Boyer"
    ],
    "abstract": "In this paper, we propose a deep neural network approach for deepfake speech\ndetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)\ntrained with a contrastive training strategy (CTS). In this framework, input\naudio recordings are first transformed into spectrograms using Short-Time\nFourier Transform (STFT) and Linear Filter (LF), which are then used to train\nthe DIN. Once trained, the DIN processes bonafide utterances to extract audio\nembeddings, which are used to construct a Gaussian distribution representing\ngenuine speech. Deepfake detection is then performed by computing the distance\nbetween a test utterance and this distribution to determine whether the\nutterance is fake or bonafide. To evaluate our proposed systems, we conducted\nextensive experiments on the benchmark dataset of ASVspoof 2019 LA. The\nexperimental results demonstrate the effectiveness of combining the\nDepthwise-Inception Network with the contrastive learning strategy in\ndistinguishing between fake and bonafide utterances. We achieved Equal Error\nRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%\nrespectively using a single, low-complexity DIN with just 1.77 M parameters and\n985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposed\nsystem outperforms the single-system submissions in the ASVspoof 2019 LA\nchallenge, showcasing its potential for real-time applications.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}