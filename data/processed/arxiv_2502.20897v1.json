{
  "id": "arxiv_2502.20897v1",
  "text": "Beyond Demographics: Fine-tuning Large Language Models to Predict\nIndividuals’ Subjective Text Perceptions\nMatthias Orlikowski1, Jiaxin Pei2, Paul Röttger4, Philipp Cimiano1, David Jurgens3, and Dirk Hovy4\n1Bielefeld University\n2Stanford University\n3University of Michigan\n4Computing Sciences Department, Bocconi University, Milan, Italy\nAbstract\nPeople naturally vary in their annotations for\nsubjective questions and some of this variation\nis thought to be due to the person’s sociodemo-\ngraphic characteristics. LLMs have also been\nused to label data, but recent work has shown\nthat models perform poorly when prompted\nwith sociodemographic attributes, suggesting\nlimited inherent sociodemographic knowledge.\nHere, we ask whether LLMs can be trained to\nbe accurate sociodemographic models of anno-\ntator variation. Using a curated dataset of five\ntasks with standardized sociodemographics, we\nshow that models do improve in sociodemo-\ngraphic prompting when trained but that this\nperformance gain is largely due to models learn-\ning annotator-specific behaviour rather than so-\nciodemographic patterns. Across all tasks, our\nresults suggest that models learn little mean-\ningful connection between sociodemographics\nand annotation, raising doubts about the current\nuse of LLMs for simulating sociodemographic\nvariation and behaviour.\n1\nIntroduction\nMost NLP models require labelled data to learn.\nYet, the humans labelling that data may not agree\nwhat is the correct label. These annotator disagree-\nments stem from multiple causes, such as genuine\nmistakes, adversarial behaviour, or even individual\npreferences (Sandri et al., 2023). This variance\nin labelling behaviour has long been recognized\nand multiple models have been developed to dis-\ntinguish some types of disagreements, particularly\nthose due to error (Hovy et al., 2013; Passonneau\nand Carpenter, 2014). Recent work has focused on\nmodelling the regularity in label variation due to\nindividual (e.g., Deng et al., 2023) and group-based\npreferences (e.g., Davani et al., 2024). For exam-\nple, members of one social group may regularly\nrate a piece of text as more or less offensive than\nothers. When such labelling behaviour is regular, a\nlarge language model (LLM) could be prompted to\nFigure 1: Unlike existing works that majorly rely on zero-shot\ndemographic prompting, we explore whether LLMs can be\ntrained to predict individuals’ subjective text perceptions.\ntake on sociodemographic characteristics to gener-\nate how a person with such characteristics would\nanswer the question (Beck et al., 2024).\nSuch approaches of sociodemographic prompt-\ning require that an LLM can effectively take the\nperspective of the person or group in the prompt.\nWhen LLMs are used to generate synthetically-\nlabelled data (Grunde-McLaughlin et al., 2023) or\nas evaluators (Dong et al., 2024), this approach\nprovides a scalable way to include meaningful\nvariation by annotators, particularly those for less\ncommon sociodemographic identities (Simmons\nand Hare, 2023). However, multiple works have\nraised issues with the accuracy of this approach\nwhen LLMs are used in zero-shot settings (e.g.,\nBeck et al., 2024; Hu and Collier, 2024; Sun et al.,\n2023). While the root cause of this low zero-shot\nperformance is likely multifaceted, given the poten-\ntial benefits of LLMs as annotator models, we test\nwhether LLMs can be trained as sociodemographic\nmodels of annotators, which was not assessed be-\nfore.\nTo effectively model individual annotators with\nLLMs, we introduce a new approach that combines\npersona prompting with annotator modelling. In-\n1\narXiv:2502.20897v1  [cs.CL]  28 Feb 2025\n\n\nstead of fixing annotator identity and attributes as\npart of a specialized architecture, we incorporate\nthis information by adapting input formats from\npersona prompts. Using this formatted input, we\nfine-tune decoder-only LLMs with prediction heads\nas used in reward models for LLM alignment (e.g.,\nLiu et al., 2024). For training, we curated the\nDEMO dataset by unifying five existing datasets for\nsubjective classification tasks (intimacy, offensive-\nness, politeness, safety, sentiment) that include an-\nnotator IDs and sociodemographic attributes (age,\ngender, race, and education).\nOur study answers the following four research\nquestions. RQ1: Can LLMs learn to model given\nannotators better based on sociodemographics or\ntheir identity (ID)? LLMs improve over baselines\nwhen incorporating sociodemographics, but we\nfind that LLMs are much more accurate at mod-\nelling specific annotators’ behaviours. RQ2: Can\nLLMs generalize to new annotators?\nNo, we\nfind that neither sociodemographic attributes nor\nIDs improve performance over a text-only base-\nline, suggesting LLMs do not learn generalizable\npatterns.\nRQ3: If LLMs can use sociodemo-\ngraphic attributes to better model given annotators\n(RQ1) but do not generalize (RQ2), what infor-\nmation do LLMs learn when improving from so-\nciodemographics? We find that sociodemographic-\ntuned models primarily improve for annotators with\nunique attributes, where attributes effectively act\nas an ID, suggesting LLMs are learning little about\nthe connection between sociodemographics and\nannotation behaviour. RQ4: Does modelling an-\nnotator identity improve how models predict label\ndistributions when annotators disagree? Beyond\nimprovements in predicting individual ratings, we\nshow that models using annotator identity better\nreflect cases of disagreement between annotators\nthan a text-only baseline. All data and code re-\nalted to our experiments are available at https:\n//github.com/morlikowski/beyond-demographics.\n2\nRelated Work\nWithin research on the role of annotator character-\nistics in annotation, we connect work in sociode-\nmographic prompting with annotator modelling.\nAnnotator\nCharacteristics\nin\nAnnotation.\nTraining NLP and AI models relies on human\nannotations to adjust their parameters to align\nwith human knowledge and preferences. Unlike\ngames and mathematics, where there is always\na ground truth, many NLP annotation tasks\nare inherently subjective. They are affected by\nannotators’ attributes and individual preferences.\nExisting studies have explored how different\nattributes affect annotators’ behaviours on tasks\nsuch as sentiment analysis (Díaz et al., 2018),\npreference modelling (Kirk et al., 2024), ideology\nclassification (Shen and Rose, 2021) and hate\nspeech or toxicity detection (Larimore et al., 2021;\nKumar et al., 2021; Sap et al., 2022).\nEffects\nseem to be strongest when annotated content and\nattributes align (e.g., LGBTQ identities in relation\nto homophobic content, Goyal et al. 2022), but are\nalso found across different tasks for more general\nsamples (Pei and Jurgens, 2023). However, similar\nto us, some works do not find relevant associations\nwith annotator background (Biester et al., 2022).\nConsequently, recent studies explore differences\nwithin demographic groups (Davani et al., 2024).\nAnnotator Modelling\nAnnotator investigates su-\npervised models that predict the annotations of\nindividual annotators on specific inputs. These\nworks are motivated by wider research on annota-\ntor subjectivity that questions the assumption of a\nsingle ground truth in annotation (Ovesdotter Alm,\n2011; Uma et al., 2021; Plank, 2022; Fleisig et al.,\n2024; Frenda et al., 2024). Our work builds on\nstudies that model annotators in subjective tasks\n(Mostafazadeh Davani et al., 2022; Weerasooriya\net al., 2023; Vitsakis et al., 2023; Wang and Plank,\n2023). Many annotator models use a unique identi-\nfier (ID) per annotator, often represented as a learnt\nembedding, frequently in combination with infor-\nmation derived from annotation statistics (Heinisch\net al., 2023; Sarumi et al., 2024; Mokhberian et al.,\n2024). However, for unseen annotators, Deng et al.\n(2023) show that models with annotator embed-\ndings (ID and annotation patterns) do not beat\na content-only baseline. We also evaluate on an\nannotator-based split (see §5.2). However, as we\nfocus on the availability of sociodemographic meta-\ndata, only one task, Sentiment (see §3), overlaps\nwith datasets used in their study.\nAnand et al.\n(2024) learn from individual annotations and evalu-\nate the confidence of predictions. In particular, they\nfind improvements in high-disagreement instances,\nsimilar to our analysis in §6.2.\nClosely related to our work, some annotator mod-\nels include sociodemographic information on an-\nnotators (e.g., Wan et al. 2023). Orlikowski et al.\n(2023) and Fleisig et al. (2023) find conflicting\n2\n\n\nresults on the usefulness of sociodemographics rel-\native to annotator identity which we discuss in re-\nlation to our findings (see §7). Other works find\nimprovements from demographics over a content-\nonly baseline but do not compare to using only\nannotator IDs (Gordon et al., 2022; Jaggi et al.,\n2024). In concurrent work, Jiang et al. (2024) also\npresent a study on fine-tuning LLMs with annota-\ntor information, as part of a dataset description and\nanalysis. In contrast to our study, they do so in the\ncontext of only a single dataset with few training\ninstances (600) and also do not compare against\nusing the ID. Their results, similar to our findings,\nindicate that sociodemographics are less influential,\nshowing greater importance for attitudes directly\nrelated to their task (Jiang et al., 2024).\nSociodemographic Prompting and Simulation.\nSociodemographic prompting is part of a broader\ninterest in using LLMs to simulate human re-\nsponses in surveys or experiments. Within the\nsocial sciences, simulations focusing on simple\nactors and macro patterns from interactions are an\nestablished method (Epstein and Axtell, 1996). In\ncontrast, LLMs enable human surrogates for new\nsettings such as surveys or experiments, which sev-\neral studies have started to explore (Aher et al.,\n2023; Dillion et al., 2023; Kozlowski and Evans,\n2024). While some studies report successful appli-\ncations (Argyle et al., 2023; Horton, 2023; Man-\nning et al., 2024), others discuss downsides of using\nLLMs to simulate individuals based on background\ndescriptions, such as caricature and misportrayal\nof social groups (Cheng et al., 2023; Wang et al.,\n2024). Among these, our work builds on investi-\ngations into simulating annotators by prompting\nLLMs based on sociodemographic profiles (Beck\net al., 2024; Hu and Collier, 2024). In concurrent\nwork, Gao et al. (2024) find that prompted LLMs\ndo not align well with human outcome distribu-\ntions in a behavioural experiment but that LLMs\nfine-tuned on relevant examples do, similar to our\nfindings (see §5.2).\n3\nDEMO\nWe curate a collection of five published datasets\ncontaining annotations and sociodemographic an-\nnotator information. These datasets focus on sub-\njective text perceptions like sentiment and offen-\nsiveness and represent a diverse range of tasks and\nsociodemographics. We identify the largest inter-\nsection of sociodemographic attributes across the\ndatasets. All five datasets contain information on\ngender, age, race, and education, so we select these\nfour attributes for our experiments. To provide\nmore comparable analyses, we normalize the so-\nciodemographic attributes of the five tasks into a\nconsistent and unified set of attributes. Appendix\nA.1 details this normalization process and the final\nattributes used in our datasets.\nThe data collectively contains 21,632 texts an-\nnotated by 2,614 annotators resulting in 147,297\nannotations total. Table 1 shows the statistics of\neach dataset. and how we pre-processed attributes\nacross datasets. Below we briefly introduce each\ntask and the original dataset on which it is based.\nIntimacy\nIntimacy reflects the perceived close-\nness of messages in interpersonal communications,\nand we use the English subset of the MINT dataset\n(Pei et al., 2023) which contains 1,993 tweets anno-\ntated by 261 annotators. Each tweet is annotated by\n7 annotators with an intimacy score from 1 (“Not\nintimate at all”) to 5 means (“Very intimate”).\nOffensiveness\nThe perception of offensiveness\n(i.e., language that might cause displeasure, anger,\nor hurt feelings Chinivar et al., 2023) is subjective\nand depends on individual attributes like gender\nand race (Jacobi, 2014). We use the offensiveness\nsubset of the POPQUORN dataset (Pei and Jurgens,\n2023). It includes 13,036 annotations from 1,500\nannotators for 1,500 Reddit comments and nuanced\ndemographic information of the annotators.\nPoliteness\nPoliteness refers to “linguistic behav-\nior which is perceived to be appropriate to the so-\ncial constraints of the ongoing interaction” (Watts,\n2003). It is one of the most fundamental concepts\nof interpersonal communication. We use the 25,042\npoliteness annotations from 506 annotators in the\nPOPQUORN dataset (Pei and Jurgens, 2023).\nSafety\nfocuses on the perceived conversational\nsafety in human-AI interactions, and we use the\nDICES-350 data (Aroyo et al., 2024), which con-\ntains ratings of conversational safety for degrees\nof harm using a multifaceted rubric. These data\ncontain 36,050 annotations from 104 annotators\nwhen applying the authors’ filtering of low-quality\nannotators.\nSentiment\nSentiment is naturally a subjective\nconstruct and individual attributes actively affect\npeople’s perception of text sentiment (Kumar et al.,\n2020). We use the sentiment annotations collected\n3\n\n\nTask\nLabels\nReference\nData Type\nInstances\nRaters\nLabels per Instance\nTotal Labels\nIntimacy\nNot Intimate to Very In-\ntimate (1-5)\nPei et al. 2023\nTweet\n1,993\n261\n48\n12,516\nOffensiveness\nNot Offensive to Very\nOffensive (1-5)\nPei and Jurgens 2023\nReddit comment\n1,500\n262\n50\n13,036\nPoliteness\nNot Polite to Very Po-\nlite (1-5)\nPei and Jurgens 2023\nEmail\n3,718\n506\n50\n25,042\nSafety\nYes, Unsure, No\nAroyo et al. 2024\nConversation\n350\n104\n350\n36,050\nSentiment\nVery Negative to Very\nPositive (1-5)\nDíaz et al. 2018\nTweet\n14,071\n1,481\n41\n60,654\nTable 1: The datasets used in DEMO\nby Díaz et al. (2018), which includes 60,654 anno-\ntations from 1,481 annotators.\n4\nExperimental Setup\nHere, we describe the different methods and setups\nfor testing LLMs as models for annotators.\n4.1\nWeights and Architecture\nWe fine-tune Llama 3 8B (Llama-Team, 2024) for\nour experiments. Llama 3 was among the strongest\nopen-weights models when we started our experi-\nments. We use a standard architecture for learning\na prediction head based on a decoder-only trans-\nformer, using the implementation for Llama 3 in\nthe HuggingFace Transformers library (Wolf et al.\n2020, see Appendix B for implementation details).\nWe use this type of architecture as it is used in\ncurrent reward models for LLM alignment (e.g.,\nLiu et al. 2024), which is also a task of predicting\nratings for text input, similar to the tasks in our\nexperiments. As we fine-tune models with a predic-\ntion head and do not rely on instruction-following,\nwe use the Llama 3 base model instead of a post-\ntrained model. In supplementary experiments on a\nsmaller scale, we use Mistral 7B (Jiang et al., 2023)\nto gauge how results with Llama 3 transfer to other\nmodel families, finding minimal differences (de-\ntails in Appendix C)\n4.2\nData Partitions\nWe use two data partition settings for how we split\nthe data into train, validation and test sets to evalu-\nate different aspects of model generalizability. The\nfirst is the instance split where we partition by in-\nstance, but annotators might be seen across all three\nsubsets. In this context, an instance means a sin-\ngle text, e.g., a Reddit comment or an email. This\nsetup follows the traditional machine learning setup\nand allows us to measure whether the LLM can\ngeneralize to new instances given an annotator’s\nsociodemographics. The second is the annotator\nsplit where annotators are partitioned across train,\nvalidation, and test sets. In other words, no anno-\ntator in the test set is included in the training or\nvalidation sets but the same text may be present\nin all three subsets. Here, the evaluation measures\nhow well the LLM can simulate a new annotator’s\ndecisions based on their sociodemographics. Ta-\nbles 2 and 3 in the Appendix show the statistics of\nthe two data partition settings.\n4.3\nPrompt Formats\nWe fine-tune on inputs which include the instance\ntext and different information about annotators. As\nwe fine-tune models with a prediction head and\nconsequently do not rely on instruction-following\n(see 4.1), we use inputs with minimal formatting\ninstead of detailed prompts. Below we detail which\nannotator information we include and how that in-\nformation is formatted.\nContent-Only\nThe baseline setting uses only the\ntextual content without any additional formatting.\nThis format ignores all annotators’ attributes.\n+Attributes (Content and Sociodemographics)\nIn inputs using sociodemographics, we list an anno-\ntator’s age, gender, race, and education. Attributes\nin DEMO are given as short textual descriptions\n(e.g., the literal text “Woman”). We preprocess\nthese descriptions by lowercasing, reformatting\nage groups (“40-44” to “40 to 44 years old”), and\nadding articles where appropriate (“a woman” in-\nstead of “woman”). We assume that all possible at-\ntribute values are known beforehand, i.e., we do not\nneed to preprocess new values (e.g., an unseen age\ngroup) during test time. We format the input text\nand attribute descriptions based on a minimal tem-\nplate: Annotator: {RACE}, {AGE}, {GENDER},\n{EDUCATION}\\n Text: {TEXT}.\n+ID (Content and ID)\nThis format uses each\nannotator’s unique identifier. As the original ID\nformat varies between tasks in DEMO, we reformat\n4\n\n\nIDs to numerical values to ensure uniform input\nacross tasks. The template is Annotator: unique\nidentifier {ID}\\n Text: {TEXT}.\n+ID+Attributes (Content, ID and Sociodemo-\ngraphics)\nLastly, we also test a combined input\nformat. An example looks like this: Annotator:\nunique identifier 72, hispanic/latino, 40\nto\n44\nyears\nold,\na\nwoman,\na\ncollege\ndegree\\n Text: This is an example text.\n4.4\nBaseline System\nAs a baseline for our fine-tuning experiments, we\nrun zero-shot sociodemographic prompting exper-\niments similar to Beck et al. (2024) and Hu and\nCollier (2024). Specifically, we prompt Llama 3\nInstruct 8B with variants of the \"Content Only\" and\n\"+Attributes\" prompts adapted to a chat prompting\ntemplate derived from Hu and Collier (2024). Here,\nin contrast to fine-tuning, attributes are described in\na conversational format, e.g., The highest degree or\nlevel of school that you have completed is a college\ndegree. We perform minimal robustness checks us-\ning 1) a larger model (Llama 3 Instruct 70B, 4-bit\nquantized) and 2) a prompt variant that simply lists\nattributes. We include the best results for Llama 3\nInstruct 8B in our fine-tuning results plots. More\ndetails and full results are in Appendix D.\n5\nExperiments\nCan LLMs learn to model sociodemographic vari-\nation in annotation (RQ1) and generalize to new\nannotators (RQ2)? To answer these questions, we\nevaluate Llama 3 8B (Llama-Team, 2024) fine-\ntuned with each of the five prompt formats on the\ninstance split and on the annotator split of DEMO.\n5.1\nTraining and Evaluation\nWe fine-tune models with half-precision weights\n(bf16) using low-rank adaption (LoRA, Hu et al.\n2021). We learn LoRA weights for all linear layers\nexcept prediction layers and initial token embed-\ndings which are fully fine-tuned. We select the\nlearning rate for each input format and task combi-\nnation based on the best-performing setting in 10\nruns evaluated on the validation set. See Appendix\nB for full fine-tuning details.\nWe treat each value of the three-point (Safety) or\nfive-point scales (all others) as a class in an individ-\nualized classification task. Individualized means\nthat the model’s objective is to predict the annota-\ntion that a particular annotator assigned to a specific\nIntimacy\nOffensiveness\nPoliteness\nSafety\nSentiment\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nF1\n+ID (FTed)\nContent Only (FTed)\n+ID+Attributes (FTed)\n+Attributes (FTed)\nBest Zero-Shot\nFigure 2: Results on the instance split show that training\nwith sociodemographics improves performance over text-only\npredictions but including a unique annotator ID in the prompt\nleads to much larger performance gains. Macro-average F1\nover three (Safety) or five (all others) classes on each test set.\nShows results for Llama 3 8B fine-tuned with different types\nof input and the best zero-shot result (8B) for each task. Mean\nscore over 30 different seeds with 95% confidence intervals\nfrom bootstrap sampling.\ntext. This evaluation setting, often used to evaluate\nannotator models (see §2), is intentionally different\nfrom standard evaluations in NLP where models\nare evaluated on a single aggregate rating per text.\nAs each class is equally important in predicting\nannotators’ ratings, we compare models based on\nmacro-average F1. For the main experiments, we\nrun each setting with 30 different random seeds.\nWe report the average score over the 30 runs and\ncompute 95% confidence intervals using bootstrap\nsampling.\n5.2\nResults: Sociodemographic Modelling\nIncluding sociodemographic attributes in training\nsignificantly outperforms the performance of zero-\nshot prompting, initially suggesting that LLMs can\nlearn to simulate sociodemographic preferences\n(RQ1), as seen in Figure 2 for the instance split.\nHowever, when models are prompted with a unique\nannotator ID, they are even more accurate at pre-\ndicting the annotator’s label. For annotators’ at-\ntributes (red) there is a consistent pattern in contrast\nto zero-shot LLM behaviour. While in zero-shot,\nincluding attributes leads to inconsistent effects,\nwhen fine-tuning we see a notable positive shift in\nthe score distribution across all tasks. We analyse\nthis result further in Section 6.1. However, adding\n5\n\n\nthe annotator ID (orange) leads to an even higher\nperformance increase. When adding both IDs and\nattributes (light orange), scores are not substantially\ndifferent from adding only the ID, suggesting that\nthe performance gain from attributes is subsumed\nby knowledge of who specifically is annotating.\nThe best zero-shot results per task for Llama 3\nInstruct 8B replicate the finding in related work\nthat zero-shot sociodemographic prompting has\nlow performance for individual annotators (Beck\net al., 2024). Unsurprisingly, even for the content-\nonly baseline (blue), fine-tuning leads to higher\nscores than zero-shot prompting (grey) for most\ntasks. A notable exception is the Offensiveness\ntask where the zero-shot performance is slightly\nabove the fine-tuned model. This is due to the\ntask’s strong label imbalance where a classifier ex-\nclusively trained on the text content can only learn\nto predict the majority class well, resulting in lower\nmacro-average F1.\n5.3\nResults: Annotator Generalization\nLLM annotator models do not generalize well to\nunseen individuals (RQ2), as seen in Figure 3 the\nannotator split. While all fine-tuned models per-\nform better than zero-shot results, the performance\ngains from adding attributes, IDs, or both are negli-\ngible compared to the text-only model. When using\nIDs, no performance gains are expected because\nthe model has not seen these annotators’ IDs before\nand cannot adapt to their idiosyncratic preferences.\nThe lack of gains for the sociodemographic at-\ntributes suggests that models have, in fact, learned\nminimal meaningful relationships between text, at-\ntributes, and rating combinations. This result is\nsurprising given the results of RQ1 that demon-\nstrated a small but consistent effect from attributes,\nwhich suggests that when we have not seen ex-\namples from a rater, then their sociodemographic\nprofile should give us at least some information on\nhow they would rate a text. We analyse this result\nin detail next.\n6\nAnalyses: What Are LLMs Learning\nAbout Sociodemographics?\nThe opposite results for sociodemographic prompt-\ning in RQ1 and RQ2 suggest that models may not\nbe learning how different attributes influence rat-\nings. Therefore, we perform two additional anal-\nyses. First, we assess to what degree are sociode-\nmographic attributes serving as proxies for anno-\nIntimacy\nOffensiveness\nPoliteness\nSafety\nSentiment\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nF1\n+ID (FTed)\nContent Only (FTed)\n+ID+Attributes (FTed)\n+Attributes (FTed)\nBest Zero-Shot\nFigure 3: Results on the annotator split, where the test sets\nonly include annotators not seen in training, show that train-\ning with sociodemographics and/or annotator IDs minimally\nimprove over the text-only baseline. While IDs for unseen\nannotators is expected to offer little benefit, this result suggests\nmodels are not able to generalize from sociodemographics.\nThe plot shows a macro-average F1 is over three (Safety) or\nfive (all others) classes on each test set for Llama 3 8B fine-\ntuned with different types of input and the best zero-shot result\n(8B) for each task. Mean score over 30 different seeds with\n95% confidence intervals from bootstrap sampling.\ntator IDs versus representing meaningful attribute-\nlabel relationships (RQ3). Second, we analyse if\nimprovements from including IDs also improve\nhow good models capture cases of disagreement\nbetween annotators (RQ4).\n6.1\nSociodemographics as Proxies\nGiven that including IDs improves results much\nmore than attributes on the instance split, we hy-\npothesize that models actually learn to use attribute\ncombinations as a proxy for annotator identity. To\ntest this hypothesis, we compare results for two\nsubsets of annotators: (1) Annotators with unique\ncombinations of sociodemographic attributes who\nhave a combination of age, gender, race and educa-\ntion not shared by any other annotator in the test\nset (denoted Unique) and (2) annotators who have\na common combination of attributes, that is, a so-\nciodemographic profile that is frequently shared by\nmany annotators (denoted Frequent). In the former,\nthe attribute combination is effectively a unique\nidentifier for the annotator, while in the latter, the\nsociodemographics refer to multiple annotators. In\nthe instance split all annotators that are included in\nthe test set are also included in the training set.\n6\n\n\nContent Only\n+Attributes\n0.25\n0.30\n0.35\n0.40\n0.45\nF1\nUnique Sociodem.\nContent Only\n+Attributes\nFrequent Sociodem.\nIntimacy\nPoliteness\nSentiment\nOffensiveness\nSafety\nFigure 4: Evaluation scores on ratings by annotators with\nunique vs. frequent combinations of sociodemographic at-\ntributes, corresponding to Unique and Frequent in the main\ntext. The high improvement for unique sociodemograph-\nics compare with the minimal gains for frequent sociode-\nmographics strongly suggests that the LLM is using the at-\ntributes as a proxy for annotator ID and is not learning any\nsociodemographic-label associations. Points show the mean\nscore (macro-average F1) over 30 different seeds for models\nusing only text or text and attributes. Error bars show 95%\nconfidence intervals from bootstrap sampling.\nFor each task, we include all ratings by anno-\ntators with a unique profile. For the frequent pro-\nfiles, we select the top n with n = 1 for Sentiment,\nn = 3 for Safety, and n = 5 for other tasks. Except\nfor Sentiment, we set n so that the number of an-\nnotators is similar for both subsets. For Sentiment,\nwe only use the most frequent profile because it\nincludes more annotators than the sum of unique\nprofiles. More details on profile distributions in\nAppendix A.3.\nTo test the hypotheses, we compare the per-\nformance gain relative to content-only input for\nadding each subset of sociodemographic attributes.\nFor each model configuration and task, we com-\npute new macro-average F1 scores for each subset\nof annotators across runs.\nOur results show that the largest gains occur\nwhen LLMs are predicting ratings for the annota-\ntors in the Unique subset (Figure 4), but no con-\nsistent or substantial gains for predicting ratings\nof annotators in the Frequent subset. This result\nconfirms our hypothesis that the unique sociode-\nmographics are acting as proxies for identity and,\nthus, the LLM is not learning any meaningful rela-\ntionship between attributes and labelling.\n6.2\nModelling Disagreement\nBeyond getting more accurate at predicting individ-\nual ratings, do models improve at capturing specific\ntypes of label distributions when incorporating at-\ntributes and identifiers (RQ4)? Can we capture\nwhen there is disagreement on how to rate an exam-\nple? Or do models mainly improve on consensually\nrated content?\nTo test for which kinds of label distributions\nthe LLM can best model, we group the instances\nbased on their levels of disagreement. We measure\ndisagreement as the entropy of each instance’s la-\nbel distribution: Lower label entropy corresponds\nto patterns of more agreement and higher label en-\ntropy corresponds to patterns of more disagreement\n(in the extreme corresponding to uniform ratings).\nWe split instances into two groups using the median\nvalue to distinguish lower and higher label entropy.\nWe use the two groups to measure how close mod-\nels get to predicting the actual label distributions\nin high and low disagreement scenarios. For each\ntext in the two groups, we measure the distance\nbetween the predicted and the actual rating distri-\nbutions for each model configuration (content only,\nplus attributes, plus ID). Following Santurkar et al.\n(2023), we compute the distance of the actual rat-\ning distributions using Wasserstein distance (earth\nmover’s distance).\nModels get better at predicting cases of disagree-\nment when including attributes and IDs, as shown\nby the scores for higher entropy labels (orange) in\nFigure 5. Still, disagreements remain challenging\nas distances are always higher as for cases when\nannotators mostly agree. However, distances to\nthe actual rating distribution are smallest on higher\ndisagreement cases when including IDs. With the\nexception of the Offensiveness task, ID-based mod-\nels almost model label distributions equally well\nirrespective of the level of disagreement. For cases\nof agreement (teal), there are much smaller differ-\nences between model configurations.\n7\nDiscussion\nBased on our results, we can not expect LLMs\nto model annotators based on their sociodemo-\ngraphics alone, in particular without examples of\ntheir individual behaviour. While even the best-\nperforming models still are far from perfect, so-\nciodemographic prompting usually performs worse\nthan using annotator-specific identities. Our results\nshow that it is possible to model a given set of an-\n7\n\n\nContent-Only\n+Attributes +ID\n0.25\n0.50\n0.75\n1.00\n1.25\ndistance to ratings\nIntimacy\nContent-Only\n+Attributes +ID\nOffensiveness\nContent-Only\n+Attributes +ID\nPoliteness\nContent-Only\n+Attributes +ID\nSafety\nContent-Only\n+Attributes +ID\nSentiment\nLower Label Entropy\nHigher Label Entropy\nFigure 5: Wasserstein distance to the actual rating distribution (lower is better) on texts in the standard-split test sets, average\nand 95% confidence intervals. Lower label entropy corresponds to patterns of agreement and higher label entropy corresponds\nto patterns of disagreement (uniform ratings or bimodal, diverging ratings). Higher and lower are distinguished based on the\nmedian entropy value per test set.\nnotators reasonably well from examples, but mod-\nels do not actually learn how to generalize from\nseen sociodemographic attribute patterns to new\nannotators. Thus, models only seemingly improve\nfrom attribute information. As we show, they in-\nstead improve for annotators who can be identified\nby unique attribute combinations. Naturally, this\nworks best if models have access to an actual iden-\ntifier for each annotator. In these cases, where an-\nnotator modelling succeeds, it leads to models that\ncan better predict diverging views on the correct\nlabel. Learning from examples of identifiable an-\nnotators allows LLMs to learn labelling behaviour\nwithout explicating factors that govern it.\nAdditionally, we see that attributes in combi-\nnation with an ID do not improve results in com-\nparison to only adding the ID. This result echoes\nOrlikowski et al. (2023) who also find that sociode-\nmographics do not improve results beyond using\nIDs, interpreting this finding in reference to the eco-\nlogical fallacy. They also discuss the limitation of\nnot having tested on attribute combinations, which\nwe do. The lack of detailed enough profiles does\nnot seem to be an explanation for why sociode-\nmographics are less relevant than individual-level\nbehaviour. In contrast, Fleisig et al. (2023) find\nthat predictions of individual ratings improve when\nusing sociodemographics instead of IDs. In par-\nticular, in their setting IDs perform worse than a\ncontent-only baseline while sociodemographics im-\nprove over the baseline. Gordon et al. (2022) do\nnot compare to using the ID without sociodemo-\ngraphics, but their full model does include IDs and\nleads to a substantial improvement over using only\nsociodemographic attributes. Similarly, for author\nmodelling, Soni et al. (2024) find that using only\nindividual context derived from an author’s text\nimproves over pre-training with author attributes\nin a downstream document-level classification task.\nThus, the benefit we find for IDs over attributes\nseems to be consistent with related findings, but\nthere are apparently cases when learning from iden-\ntifiable annotators performs less well. Future work\ncould investigate the influence of dataset character-\nistics and used architectures.\nComparing results when using attributes and\nwhen using IDs offers a perspective on overcom-\ning LLM uniformity (Kozlowski and Evans, 2024)\nor flattening of groups (Wang et al., 2024), also\ndiscussed by Dillion et al. (2023). Santurkar et al.\n(2023) highlight modal representativeness of chat-\ntuned models that assign the most probability mass\nto a single answer when prompted with sociode-\nmographics, simplifying opinion diversity within\ngroups. Attributes and sociodemographic personas\ndon’t necessarily capture variation within social\ngroups, so that LLMs respond uniformly, appar-\nently. However, learning from examples of indi-\nvidual behaviour could model this within-group\nvariance and avoid oversimplification.\n8\nConclusion\nWe ask to what degree can LLMs be trained to ac-\ncurately predict individuals’ annotation from their\nsociodemographic attributes, motivated by their\npoor performance at sociodemographic zero-shot\nprompting. In a series of experiments and analyses\nusing five datasets and two different partitions of\nthe data (based on annotators and instances), we\nfind that LLMs can not reproduce annotators’ text\nperceptions based on sociodemographics alone but,\ninstead, primarily learn from examples of individ-\nual behaviour to model specific annotators. How-\n8\n\n\never, using examples of how individuals rate, we\ncan learn their rating behaviour in a single LLM-\nbased model with much better performance than\nboth zero-shot and content-only baselines.\n9\nLimitations\nThe datasets used in our study are only annotated\nby annotators from the US. While the original data\nfor the Intimacy task (Pei et al., 2023) include non-\nUS annotators, the English Language subset used\nin our study does not. Therefore, we can not carry\nout cross-geocultural comparisons using the exist-\ning datasets to detail how results might transfer to\nother geocultural contexts. Datasets suitable for\nannotator modelling are rare and existing datasets\nwith annotators from different regions do not pro-\nvide the same set of additional sociodemographic\nattributes that we investigate in our study. For ex-\nample, Frenda et al. (2023) only includes informa-\ntion on age and gender. Cross-cultural datasets\nfrom concurrent work (Mostafazadeh Davani et al.,\n2024) can be used in future studies.\nWe primarily evaluate only one model family,\nLlama 3. Consequently, results with other LLMs\nmight differ. This is mainly due to a trade-off with\nthe number of experimental runs we can achieve\nwith the same computational budget. We opted\nfor a comparatively high number of runs (30) to\nallow for a more reliable estimation of variability\nbetween runs. This allowed us to detect small but\nsignificant differences between input formats. In\ncomparison to zero-shot, we would in general ex-\npect less variation between model families as all\nmodels would be fine-tuned in the same setting.\nEmpirically, we mitigate the limitation of primar-\nily experimenting with Llama 3 to some extent by\nincluding small-scale supplementary experiments\n(fewer runs and tasks) using Mistral 7B (Jiang et al.,\n2023). Results in Appendix C show that at least\nin this smaller setting the same pattern of results\nholds. For zero-shot, extensive results across model\nfamilies are already available in related work (Beck\net al., 2024; Hu and Collier, 2024), so that we only\nreplicated them partially as a baseline.\n10\nEthics\nThis paper studies how much LLM could be trained\nto predict individuals’ subjective text perceptions.\nThrough extensive experiments, we found that fine-\ntuning LLMs with demographics does not help to\nsignificantly improve their performances. Such\na result suggests that sociodemographic prompt-\ning may not be an effective way to elicit accurate\nindividual-level perception prediction even when\nthe model is fine-tuned on the specific task. In-\nstead, fine-tuning with individuals’ annotations\nhelps LLMs to better capture individual annota-\ntors’ ratings by a relatively large margin, suggest-\ning that individual preference modelling would be\na more promising direction toward accurate sub-\njective text perception modelling. Altogether, our\nresults suggest that people should be cautious about\nthe potential biases when prompting LLMs with\ndemographics.\nIn our experiments, we only included four demo-\ngraphic attributes: gender, age, race, and education.\nWe made this decision because these are the com-\nmon attributes covered in all the collected datasets.\nWe acknowledge this as one of our major limita-\ntions and by doing so, we might have excluded\nother important demographic attributes. In the fu-\nture, we will explore better ways to include diverse\ntypes of demographics and we also call for future\nwork in this direction to investigate the effect of\nother aspects of demographics.\nReferences\nGati V. Aher, Rosa I. Arriaga, and Adam Tauman Kalai.\n2023. Using Large Language Models to Simulate\nMultiple Humans and Replicate Human Subject Stud-\nies. In Proceedings of the 40th International Confer-\nence on Machine Learning, pages 337–371. PMLR.\nISSN: 2640-3498.\nAbhishek Anand, Negar Mokhberian, Prathyusha Ku-\nmar, Anweasha Saha, Zihao He, Ashwin Rao, Fred\nMorstatter, and Kristina Lerman. 2024. Don’t blame\nthe data, blame the model: Understanding noise and\nbias when learning from subjective annotations. In\nProceedings of the 1st Workshop on Uncertainty-\nAware NLP (UncertaiNLP 2024), pages 102–113,\nSt Julians, Malta. Association for Computational Lin-\nguistics.\nLisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R.\nGubler, Christopher Rytting, and David Wingate.\n2023. Out of One, Many: Using Language Mod-\nels to Simulate Human Samples. Political Analysis,\n31(3):337–351.\nLora Aroyo, Alex Taylor, Mark Diaz, Christopher\nHoman, Alicia Parrish, Gregory Serapio-García, Vin-\nodkumar Prabhakaran, and Ding Wang. 2024. Dices\ndataset: Diversity in conversational ai evaluation for\nsafety. Advances in Neural Information Processing\nSystems, 36.\nTilman Beck, Hendrik Schuff, Anne Lauscher, and Iryna\nGurevych. 2024. Sensitivity, performance, robust-\n9\n\n\nness: Deconstructing the effect of sociodemographic\nprompting. In Proceedings of the 18th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2589–2615, St. Julian’s, Malta. Association for Com-\nputational Linguistics.\nLaura Biester, Vanita Sharma, Ashkan Kazemi, Naihao\nDeng, Steven Wilson, and Rada Mihalcea. 2022. An-\nalyzing the effects of annotator gender across NLP\ntasks. In Proceedings of the 1st Workshop on Per-\nspectivist Approaches to NLP @LREC2022, pages\n10–19, Marseille, France. European Language Re-\nsources Association.\nMyra Cheng, Tiziano Piccardi, and Diyi Yang. 2023.\nCoMPosT: Characterizing and evaluating caricature\nin LLM simulations. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10853–10875, Singapore.\nAssociation for Computational Linguistics.\nSneha Chinivar, MS Roopa, JS Arunalatha, and\nKR Venugopal. 2023. Online offensive behaviour in\nsocialmedia: Detection approaches, comprehensive\nreview and future directions. Entertainment Comput-\ning, 45:100544.\nAida Mostafazadeh Davani, Mark Díaz, Dylan Baker,\nand Vinodkumar Prabhakaran. 2024. D3code: Disen-\ntangling disagreements in data across cultures on of-\nfensiveness detection and evaluation. arXiv preprint\narXiv:2404.10857.\nNaihao Deng, Xinliang Zhang, Siyang Liu, Winston Wu,\nLu Wang, and Rada Mihalcea. 2023. You are what\nyou annotate: Towards better models through anno-\ntator representations. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n12475–12498, Singapore. Association for Computa-\ntional Linguistics.\nMark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie\nPiper, and Darren Gergle. 2018. Addressing age-\nrelated bias in sentiment analysis. In Proceedings of\nthe 2018 chi conference on human factors in comput-\ning systems, pages 1–14.\nDanica Dillion, Niket Tandon, Yuling Gu, and Kurt\nGray. 2023. Can AI language models replace hu-\nman participants?\nTrends in Cognitive Sciences,\n27(7):597–600.\nYijiang River Dong, Tiancheng Hu, and Nigel Collier.\n2024.\nCan llm be a personalized judge?\narXiv\npreprint arXiv:2406.11657.\nJoshua M. Epstein and Robert L. Axtell. 1996. Growing\nArtificial Societies: Social Science from the Bottom\nUp. The MIT Press.\nEve Fleisig, Rediet Abebe, and Dan Klein. 2023. When\nthe majority is wrong: Modeling annotator disagree-\nment for subjective tasks. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 6715–6726, Singapore. As-\nsociation for Computational Linguistics.\nEve Fleisig, Su Lin Blodgett, Dan Klein, and Zeerak\nTalat. 2024. The perspectivist paradigm shift: As-\nsumptions and challenges of capturing human labels.\nIn Proceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 2279–2292, Mexico\nCity, Mexico. Association for Computational Lin-\nguistics.\nSimona Frenda, Gavin Abercrombie, Valerio Basile,\nAlessandro Pedrani, Raffaella Panizzon, Alessan-\ndra Teresa Cignarella, Cristina Marco, and Davide\nBernardi. 2024. Perspectivist approaches to natural\nlanguage processing: a survey. Language Resources\nand Evaluation.\nSimona Frenda, Alessandro Pedrani, Valerio Basile,\nSoda Marem Lo, Alessandra Teresa Cignarella, Raf-\nfaella Panizzon, Cristina Marco, Bianca Scarlini, Vi-\nviana Patti, Cristina Bosco, and Davide Bernardi.\n2023. EPIC: Multi-perspective annotation of a cor-\npus of irony. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 13844–13857,\nToronto, Canada. Association for Computational Lin-\nguistics.\nYuan Gao,\nDokyun Lee,\nGordon Burtch,\nand\nSina Fazelpour. 2024.\nTake Caution in Using\nLLMs as Human Surrogates: Scylla Ex Machina.\nArXiv:2410.19599.\nMitchell L. Gordon, Michelle S. Lam, Joon Sung Park,\nKayur Patel, Jeff Hancock, Tatsunori Hashimoto, and\nMichael S. Bernstein. 2022. Jury learning: Integrat-\ning dissenting voices into machine learning models.\nIn Proceedings of the 2022 CHI Conference on Hu-\nman Factors in Computing Systems, CHI ’22, New\nYork, NY, USA. Association for Computing Machin-\nery.\nNitesh Goyal, Ian D. Kivlichan, Rachel Rosen, and Lucy\nVasserman. 2022. Is your toxicity my toxicity? ex-\nploring the impact of rater identity on toxicity annota-\ntion. Proceedings of the ACM on Human-Computer\nInteraction, 6:1–28.\nMadeleine Grunde-McLaughlin, Michelle S Lam, Ran-\njay Krishna, Daniel S Weld, and Jeffrey Heer.\n2023. Designing llm chains by adapting techniques\nfrom crowdsourcing workflows.\narXiv preprint\narXiv:2312.11681.\nPhilipp Heinisch, Matthias Orlikowski, Julia Romberg,\nand Philipp Cimiano. 2023. Architectural sweet spots\nfor modeling human label variation by the example\nof argument quality: It’s best to relate perspectives!\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n11138–11154, Singapore. Association for Computa-\ntional Linguistics.\nJohn J Horton. 2023. Large language models as sim-\nulated economic agents: What can we learn from\n10\n\n\nhomo silicus? Technical report, National Bureau of\nEconomic Research.\nDirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,\nand Eduard Hovy. 2013. Learning whom to trust\nwith MACE. In Proceedings of the 2013 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 1120–1130, Atlanta, Georgia.\nAssociation for Computational Linguistics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\nof Large Language Models. ArXiv:2106.09685 [cs].\nTiancheng Hu and Nigel Collier. 2024. Quantifying the\npersona effect in LLM simulations. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 10289–10307, Bangkok, Thailand. Association\nfor Computational Linguistics.\nLora L Jacobi. 2014. Perceptions of profanity: How\nrace, gender, and expletive choice affect perceived of-\nfensiveness. North American Journal of Psychology,\n16(2).\nHarbani Jaggi, Kashyap Murali, Eve Fleisig, and Er-\ndem Bıyık. 2024.\nAccurate and Data-Efficient\nToxicity Prediction when Annotators Disagree.\nArXiv:2410.12217.\nAiqi Jiang, Nikolas Vitsakis, Tanvi Dinkar, Gavin Aber-\ncrombie, and Ioannis Konstas. 2024. Re-examining\nsexism and misogyny classification with annotator\nattitudes. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2024, pages 15103–\n15125, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nthée Lacroix, and William El Sayed. 2023. Mistral\n7B. ArXiv:2310.06825 [cs].\nDiederik\nP.\nKingma\nand\nJimmy\nBa.\n2014.\nAdam:\nA Method for Stochastic Optimization.\narXiv:1412.6980 [cs]. ArXiv: 1412.6980.\nHannah Rose Kirk, Alexander Whitefield, Paul Röttger,\nAndrew Bean, Katerina Margatina, Juan Ciro, Rafael\nMosquera, Max Bartolo, Adina Williams, He He,\nBertie Vidgen, and Scott A. Hale. 2024. The prism\nalignment project: What participatory, representa-\ntive and individualised human feedback reveals about\nthe subjective and multicultural alignment of large\nlanguage models.\nAustin Kozlowski and James Evans. 2024. Simulating\nSubjects: The Promise and Peril of AI Stand-ins for\nSocial Agents and Interactions.\nDeepak Kumar, Patrick Gage Kelley, Sunny Consolvo,\nJoshua Mason, Elie Bursztein, Zakir Durumeric, Kurt\nThomas, and Michael Bailey. 2021. Designing toxic\ncontent classification for a diversity of perspectives.\nIn Seventeenth Symposium on Usable Privacy and\nSecurity (SOUPS 2021), pages 299–318. USENIX\nAssociation.\nSudhanshu Kumar, Monika Gahalawat, Partha Pra-\ntim Roy, Debi Prosad Dogra, and Byung-Gyu Kim.\n2020. Exploring impact of age and gender on senti-\nment analysis using machine learning. Electronics,\n9(2):374.\nSavannah Larimore, Ian Kennedy, Breon Haskett, and\nAlina Arseniev-Koehler. 2021. Reconsidering anno-\ntator disagreement about racist language: Noise or\nsignal? In Proceedings of the Ninth International\nWorkshop on Natural Language Processing for So-\ncial Media, pages 81–90, Online. Association for\nComputational Linguistics.\nChris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie\nHe, Chaojie Wang, Shuicheng Yan, Yang Liu, and\nYahui Zhou. 2024. Skywork-Reward: Bag of Tricks\nfor Reward Modeling in LLMs. ArXiv:2410.18451\n[cs].\nLlama-Team. 2024.\nThe Llama 3 Herd of Models.\nArXiv:2407.21783 [cs].\nBenjamin S Manning, Kehang Zhu, and John J Horton.\n2024. Automated social science: Language models\nas scientist and subjects. Technical report, National\nBureau of Economic Research.\nNegar Mokhberian, Myrl Marmarelis, Frederic Hopp,\nValerio Basile, Fred Morstatter, and Kristina Lerman.\n2024. Capturing perspectives of crowdsourced anno-\ntators in subjective learning tasks. In Proceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long\nPapers), pages 7337–7349, Mexico City, Mexico. As-\nsociation for Computational Linguistics.\nAida Mostafazadeh Davani, Mark Diaz, Dylan K Baker,\nand Vinodkumar Prabhakaran. 2024. D3CODE: Dis-\nentangling disagreements in data across cultures on\noffensiveness detection and evaluation. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 18511–18526,\nMiami, Florida, USA. Association for Computational\nLinguistics.\nAida Mostafazadeh Davani, Mark Díaz, and Vinodku-\nmar Prabhakaran. 2022. Dealing with disagreements:\nLooking beyond the majority vote in subjective an-\nnotations. Transactions of the Association for Com-\nputational Linguistics, 10:92–110.\nMatthias Orlikowski, Paul Röttger, Philipp Cimiano,\nand Dirk Hovy. 2023. The ecological fallacy in anno-\ntation: Modeling human label variation goes beyond\nsociodemographics. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\n11\n\n\nLinguistics (Volume 2: Short Papers), pages 1017–\n1029, Toronto, Canada. Association for Computa-\ntional Linguistics.\nCecilia Ovesdotter Alm. 2011. Subjective natural lan-\nguage problems: Motivations, applications, charac-\nterizations, and implications. In Proceedings of the\n49th Annual Meeting of the Association for Compu-\ntational Linguistics: Human Language Technologies,\npages 107–112, Portland, Oregon, USA. Association\nfor Computational Linguistics.\nRebecca J. Passonneau and Bob Carpenter. 2014. The\nbenefits of a model of annotation. Transactions of\nthe Association for Computational Linguistics, 2:311–\n326.\nJiaxin Pei and David Jurgens. 2023. When do annota-\ntor demographics matter? measuring the influence\nof annotator demographics with the POPQUORN\ndataset. In Proceedings of the 17th Linguistic Annota-\ntion Workshop (LAW-XVII), pages 252–265, Toronto,\nCanada. Association for Computational Linguistics.\nJiaxin Pei, Vítor Silva, Maarten Bos, Yozen Liu,\nLeonardo Neves, David Jurgens, and Francesco Bar-\nbieri. 2023.\nSemEval-2023 task 9: Multilingual\ntweet intimacy analysis.\nIn Proceedings of the\n17th International Workshop on Semantic Evaluation\n(SemEval-2023), pages 2235–2246, Toronto, Canada.\nAssociation for Computational Linguistics.\nBarbara Plank. 2022. The “problem” of human label\nvariation: On ground truth in data, modeling and\nevaluation. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10671–10682, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nQwen, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,\nJiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang,\nKeming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\nLin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang\nRen, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru\nZhang, and Zihan Qiu. 2025. Qwen2.5 Technical\nReport. ArXiv:2412.15115 [cs].\nMarta Sandri, Elisa Leonardelli, Sara Tonelli, and Elis-\nabetta Ježek. 2023.\nWhy don’t you do it right?\nanalysing annotators’ disagreement in subjective\ntasks. In Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics, pages 2428–2441.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo\nLee, Percy Liang, and Tatsunori Hashimoto. 2023.\nWhose Opinions Do Language Models Reflect? In\nProceedings of the 40th International Conference\non Machine Learning, pages 29971–30004. PMLR.\nISSN: 2640-3498.\nMaarten Sap, Swabha Swayamdipta, Laura Vianna,\nXuhui Zhou, Yejin Choi, and Noah A. Smith. 2022.\nAnnotators with attitudes: How annotator beliefs\nand identities bias toxic language detection. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5884–5906, Seattle, United States. Association for\nComputational Linguistics.\nOlufunke O. Sarumi, Béla Neuendorf, Joan Plepi, Lu-\ncie Flek, Jörg Schlötterer, and Charles Welch. 2024.\nCorpus considerations for annotator modeling and\nscaling. In Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers), pages 1029–1040,\nMexico City, Mexico. Association for Computational\nLinguistics.\nQinlan Shen and Carolyn Rose. 2021. What sounds\n“right” to me? experiential factors in the perception\nof political ideology. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n1762–1771, Online. Association for Computational\nLinguistics.\nGabriel Simmons and Christopher Hare. 2023. Large\nlanguage models as subpopulation representative\nmodels: A review. arXiv preprint arXiv:2310.17888.\nNikita Soni, Niranjan Balasubramanian, H. Andrew\nSchwartz, and Dirk Hovy. 2024. Comparing pre-\ntrained human language models: Is it better with hu-\nman context as groups, individual traits, or both? In\nProceedings of the 14th Workshop on Computational\nApproaches to Subjectivity, Sentiment, & Social Me-\ndia Analysis, pages 316–328, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nHuaman Sun, Jiaxin Pei, Minje Choi, and David Jurgens.\n2023. Aligning with whom? large language models\nhave gender and racial biases in subjective nlp tasks.\narXiv preprint arXiv:2311.09730.\nAlexandra N. Uma, Tommaso Fornaciari, Dirk Hovy,\nSilviu Paun, Barbara Plank, and Massimo Poesio.\n2021.\nLearning from disagreement:\nA survey.\nJournal of Artificial Intelligence Research, 72:1385–\n1470.\nNikolas Vitsakis, Amit Parekh, Tanvi Dinkar, Gavin\nAbercrombie, Ioannis Konstas, and Verena Rieser.\n2023. iLab at SemEval-2023 task 11 le-wi-di: Mod-\nelling disagreement or modelling perspectives? In\nProceedings of the 17th International Workshop on\nSemantic Evaluation (SemEval-2023), pages 1660–\n1669, Toronto, Canada. Association for Computa-\ntional Linguistics.\nRuyuan Wan, Jaehyung Kim, and Dongyeop Kang.\n2023. Everyone’s Voice Matters: Quantifying An-\nnotation Disagreement Using Demographic Informa-\ntion. Proceedings of the AAAI Conference on Arti-\n12\n\n\nficial Intelligence, 37(12):14523–14530. Number:\n12.\nAngelina Wang, Jamie Morgenstern, and John P. Dicker-\nson. 2024. Large language models should not replace\nhuman participants because they can misportray and\nflatten identity groups. ArXiv:2402.01908 [cs].\nXinpeng Wang and Barbara Plank. 2023. ACTOR: Ac-\ntive learning with annotator-specific classification\nheads to embrace human label variation. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 2046–2052,\nSingapore. Association for Computational Linguis-\ntics.\nRichard J Watts. 2003. Politeness. Cambridge Univer-\nsity Press.\nTharindu Cyril Weerasooriya, Alexander Ororbia, Raj\nBhensadadia, Ashiqur KhudaBukhsh, and Christo-\npher Homan. 2023. Disagreement matters: Preserv-\ning label diversity by jointly modeling item and an-\nnotator label distributions with DisCo. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 4679–4695, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nA\nDataset details\nA.1\nNormalizing annotator attributes\nAs different datasets collect annotator attributes in\ndifferent ways, we transform and normalize them\ninto a unified format. In this normalization process,\nwe first identify the most common attributes in\neach dataset and then group them into the same\ncategories.\nGender:\nMan, Woman, Non-binary, Unknown\nRace:\nArab, Asian, Black, Hispanic/Latino, Mid-\ndle Eastern, Multiracial, Native American, Pacific\nIslander, White, Other and Unknown\nAge:\n18-24, 25-29, 30-34, 35-39, 40-44,45-\n49,50-59, 60-69, 70-79, 80-89, 90-99, 100+, gen z,\nmillennial, gen x+, Unknown\nTable 2: Dataset Statistics by Instance Split and Task\nTask\nSplit\nInstances\nAnnotator\nAnnotations\nIntimacy\nTrain\n1,395\n261\n8,784\nTest\n399\n261\n2,490\nVal\n199\n260\n1,242\nPoliteness\nTrain\n2,602\n506\n17,524\nTest\n744\n506\n4,999\nVal\n372\n500\n2,519\nOffensiveness\nTrain\n1,050\n262\n9,144\nTest\n300\n262\n2,610\nVal\n150\n262\n1,282\nSafety\nTrain\n244\n123\n30,012\nTest\n70\n123\n8,610\nVal\n36\n123\n4,428\nSentiment\nTrain\n9,849\n1,481\n42,519\nTest\n2,815\n1,481\n12,133\nVal\n1,407\n1,447\n6,002\nTable 3: Dataset Statistics by Annotator Split and Task\nTask\nSplit\nInstances\nAnnotator\nAnnotations\nIntimacy\nTrain\n1,991\n182\n8,703\nTest\n1,508\n53\n2,540\nVal\n997\n26\n1,273\nPoliteness\nTrain\n3,718\n354\n17,515\nTest\n2,914\n102\n5,042\nVal\n1,872\n50\n2,485\nOffensiveness\nTrain\n1,500\n183\n9,105\nTest\n1,274\n53\n2,636\nVal\n914\n26\n1,295\nSafety\nTrain\n350\n86\n30,100\nTest\n350\n25\n8,750\nVal\n350\n12\n4,200\nSentiment\nTrain\n13,991\n1,036\n42,413\nTest\n9,017\n297\n12,162\nVal\n5,116\n148\n6,079\nEducation:\nCollege degree, Graduate degree,\nHigh school or below, Less than high school, Un-\nknown\nA.2\nDataset Splits\nTable 2 and Table 3 presents the statistics of the\ndifferent data partitions (annotator split, instance\nsplit) across train, validation and test splits.\nA.3\nDistribution of Sociodemographic Profiles\nWe report annotator counts for both most-frequent\nand unique sociodemographic profiles for Intimacy\n(Table 4), Offensiveness (Table 5), Politeness (Ta-\nble 6), Safety (Table 7), and Sentiment (Table 8).\nThese counts underscore that often there are many\nattribute combinations that effectively can act as an\nunique identifier.\nNotably, many unique profiles seem to be not\nonly rare due to dataset construction but because\nthey are rare in the general population. The under-\n13\n\n\nTable 4: Distribution of sociodemographic attribute combi-\nnations (profiles) for Intimacy. Counts refer to the number of\nannotators with a given profile. Shows the 10 most-frequent\nprofiles and a sample of 10 random unique profiles.\nSociodemographic Profile\nCount\nWoman|18-24|White|College degree\n16\nMan|30-34|White|College degree\n10\nMan|25-29|White|College degree\n9\nMan|35-39|White|College degree\n9\nWoman|18-24|White|High school or below\n9\nMan|18-24|White|High school or below\n8\nWoman|30-34|White|College degree\n8\nMan|35-39|White|High school or below\n6\nMan|50-59|White|Graduate degree\n5\nMan|45-49|White|High school or below\n5\n...\n...\nMan|40-44|Multiracial|College degree\n1\nNon-binary|25-29|White|College degree\n1\nMan|18-24|Multiracial|High school or below\n1\nNon-binary|40-44|White|College degree\n1\nMan|35-39|Black|High school or below\n1\nWoman|35-39|Asian|Graduate degree\n1\nMan|18-24|White|Graduate degree\n1\nMan|25-29|Asian|College degree\n1\nMan|30-34|Black|Graduate degree\n1\nMan|45-49|Pacific Islander|High school or below\n1\nlying reasons are likely complex and might include\nbiology (being of very old age is generally less\nlikely), achievement and/or privilege (e.g., a white\nannotator with a graduate degree at a young age)\nas well as power imbalance, marginalisation and\nunequal access to resources (e.g., women of an\nolder generation being less likely to have had ac-\ncess to higher education). An alternative reading\nof our results thus could relate rare profiles to more\nimpactful personal experiences that might explain\nannotation behaviour to a larger degree. The exact\nrelationship and interactions of these factors war-\nrants investigation in future work - which would\nstill need to account for sociodemographics as po-\ntential proxies for annotator identity.\nB\nFine-Tuning Implementation Details\nIn addition to Llama 3, also the training loop was\nimplemented using the Transformers library (Wolf\net al., 2020). For all hyperparameters not explic-\nitly mentioned we used default settings. We use\nhalf-precision training (bf16), the Adam optimizer\n(Kingma and Ba, 2014) and 10 warmup steps. Texts\nare truncated after 232 tokens, determined from\ndata exploration of text lengths (95 percentile even\nfor longer examples in DICES-350). For settings\nusing attributes and IDs, we add the respective\ntokens to this limit, so that longer examples are\nTable 5: Distribution of sociodemographic attribute combina-\ntions (profiles) for Offensiveness. Counts refer to the number\nof annotators with a given profile. Shows the 10 most-frequent\nprofiles and a sample of 10 random unique profiles.\nSociodemographic Profile\nCount\nWoman|50-59|White|College degree\n17\nMan|50-59|White|College degree\n9\nWoman|50-59|White|High school or below\n8\nMan|60-69|White|Graduate degree\n7\nMan|60-69|White|High school or below\n7\nWoman|35-39|White|College degree\n6\nMan|40-44|White|College degree\n6\nMan|30-34|White|College degree\n6\nWoman|50-59|White|Graduate degree\n6\nWoman|40-44|White|High school or below\n6\n...\n...\nMan|30-34|Black|Less than high school\n1\nMan|40-44|Asian|Graduate degree\n1\nMan|30-34|Asian|College degree\n1\nNon-binary|35-39|White|Graduate degree\n1\nMan|60-69|Black|College degree\n1\nNon-binary|35-39|White|High school or below\n1\nNon-binary|18-24|Black|High school or below\n1\nMan|25-29|White|High school or below\n1\nNon-binary|18-24|White|College degree\n1\nMan|50-59|Asian|Graduate degree\n1\nTable 6: Distribution of sociodemographic attribute combina-\ntions (profiles) for Politeness. Counts refer to the number of\nannotators with a given profile. Shows the 10 most-frequent\nprofiles and a sample of 10 random unique profiles.\nSociodemographic Profile\nCount\nWoman|60-69|White|College degree\n24\nMan|60-69|White|College degree\n23\nMan|50-59|White|College degree\n18\nWoman|60-69|White|High school or below\n16\nMan|60-69|White|Graduate degree\n16\nWoman|50-59|White|College degree\n16\nMan|35-39|White|College degree\n15\nWoman|60-69|White|Graduate degree\n14\nMan|18-24|White|High school or below\n12\nMan|50-59|White|High school or below\n10\n...\n...\nMan|18-24|Hispanic/Latino|Graduate degree\n1\nNon-binary|18-24|Asian|College degree\n1\nWoman|25-29|Black|High school or below\n1\nWoman|60-69|Asian|College degree\n1\nWoman|18-24|Black|Graduate degree\n1\nMan|40-44|Black|Graduate degree\n1\nMan|35-39|Asian|Graduate degree\n1\nWoman|30-34|Asian|High school or below\n1\nWoman|25-29|White|Less than high school\n1\nWoman|50-59|Hispanic/Latino|College degree\n1\n14\n\n\nTable 7: Distribution of sociodemographic attribute combi-\nnations (profiles) for Safety. Counts refer to the number of\nannotators with a given profile. Shows the 10 most-frequent\nprofiles and a sample of 10 random unique profiles.\nSociodemographic Profile\nCount\nMan|millenial|Asian|College degree or higher\n6\nWoman|gen z|White|College degree or higher\n6\nWoman|gen z|Black|High school or below\n5\nWoman|millenial|Asian|College degree or higher\n5\nWoman|gen z|White|High school or below\n5\nWoman|millenial|Black|College degree or higher\n4\nMan|gen z|White|College degree or higher\n4\nMan|gen z|Multiracial|High school or below\n4\nMan|gen x+|Asian|College degree or higher\n3\nMan|gen x+|Black|College degree or higher\n3\n...\n...\nMan|millenial|Multiracial|High school or below\n1\nWoman|millenial|Multiracial|High school or below\n1\nWoman|millenial|White|High school or below\n1\nWoman|millenial|White|College degree or higher\n1\nMan|gen z|Asian|High school or below\n1\nMan|gen z|Black|High school or below\n1\nMan|gen x+|Multiracial|Unknown\n1\nMan|millenial|Hispanic/Latino|College degree or higher\n1\nWoman|gen x+|Black|Unknown\n1\nWoman|gen x+|Black|High school or below\n1\nTable 8: Distribution of sociodemographic attribute combina-\ntions (profiles) for Sentiment. Counts refer to the number of\nannotators with a given profile. Shows the 10 most-frequent\nprofiles and a sample of 10 random unique profiles.\nSociodemographic Profile\nCount\nWoman|50-59|White|Some college or associate’s degree\n86\nMan|60-69|White|Some college or associate’s degree\n84\nWoman|60-69|White|Some college or associate’s degree\n83\nMan|60-69|White|College degree\n77\nMan|50-59|White|Some college or associate’s degree\n62\nMan|70-79|White|College degree\n58\nWoman|50-59|White|High school or below\n55\nWoman|50-59|White|College degree\n52\nMan|60-69|White|High school or below\n49\nMan|70-79|White|Some college or associate’s degree\n49\n...\n...\nWoman|70-79|Black|Graduate degree\n1\nWoman|50-59|Pacific Islander|Some college or associate’s degree\n1\nMan|60-69|Black|Less than high school\n1\nWoman|80-89|Black|Less than high school\n1\nMan|60-69|Other|Some college or associate’s degree\n1\nWoman|60-69|Other|Graduate degree\n1\nWoman|60-69|Native American|High school or below\n1\nMan|70-79|Other|Graduate degree\n1\nMan|50-59|Black|Less than high school\n1\nWoman|50-59|Other|Less than high school\n1\ntruncated similarly across settings. Specifically,\nwe add 7 tokens for the ID and 22 tokens for so-\nciodemographics, based on the maximum attribute\ndescription text lengths in the data set. Per batch,\ninputs are padded to the maximum length.\nAs examples vary in length across datasets, we\nadapt the batch size so that experiments fit in avail-\nable GPU RAM (Nvidia A40, 48GB GPU RAM).\nIntimacy uses a batch size of 16, Offensiveness uses\n16 (8 with attributes), Politeness uses 8, Safety uses\n4, Sentiment uses 16. Safety accumulates updates\nto an effective size of 16, other datasets 64.\nWe select the learning rate for each input for-\nmat and task combination based on the best per-\nforming setting in 10 runs on the validation set for\nthe annotator and the instance split. We perform\ngrid search with values 0.0003, 0.00008, 0.00006,\n0.00003. The initial learning rates selected for the\nmain experiments are listed in Table 9.\nLoRA hyperparameters are r = 8, α = 16,\ndropout set to 0.05.\nEach run uses a fixed random seed: 536804,\n3208936010,\n701702170,\n1506676066,\n621609371,\n2454110510,\n1124617826,\n2591124800,\n2969282657,\n1435485536,\n799443590, 14417848, 1353658699, 873469724,\n1307226514, 277728153, 185007946, 370276791,\n1847855308, 862745529, 224600032, 124600042,\n1885444771, 1192697616, 996477090, 720235893,\n1294938046,\n824411996,\n1497508757,\n1920797789.\nEach run used a single Nvidia\nA40 (48GB GPU RAM). The runtime changes\nwith the dataset size and the feasible batch\nsize.\nPer run, training and evaluation together\ntake on average about 30 minutes for Intimacy\nup to 215 minutes for Safety.\nRuntimes with\nsociodemographics are longer at about 40 minutes\n(Intimacy) to about 445 minutes (Safety).\nC\nEvaluating Additional Model Families\nOne limitation of our results is that they are only\nbased on a single model family due to our compute-\nintense setup (e.g., many runs). To partially miti-\ngate this limitation, we run additional small-scale\nexperiments for the instance split. The experiments\non the instance split (answering RQ1) show to the\nmost characteristic pattern of results and substan-\ntiate our main findings. To keep experiments fea-\nsible, we focus on the Intimacy and Offensiveness\ntasks. As additional model family we use Mistral\n15\n\n\nTable 9: Initial learning rates selected for main experiments\nfor each experiment configuration across tasks, input formats\nand data partitions (instance split and annotator split).\nTask\nInput\nPartition\nLearning Rate\nIntimacy\nContent-Only\nInstance\n6 ∗10−5\n+Attributes\nInstance\n6 ∗10−5\n+ID\nInstance\n6 ∗10−5\n+ID+Attributes\nInstance\n8 ∗10−5\nContent-Only\nAnnotator\n8 ∗10−5\n+Attributes\nAnnotator\n8 ∗10−5\n+ID\nAnnotator\n8 ∗10−5\n+ID+Attributes\nAnnotator\n8 ∗10−5\nPoliteness\nContent-Only\nInstance\n8 ∗10−5\n+Attributes\nInstance\n8 ∗10−5\n+ID\nInstance\n8 ∗10−5\n+ID+Attributes\nInstance\n6 ∗10−5\nContent-Only\nAnnotator\n3 ∗10−5\n+Attributes\nAnnotator\n3 ∗10−5\n+ID\nAnnotator\n8 ∗10−5\n+ID+Attributes\nAnnotator\n3 ∗10−5\nOffensiveness\nContent-Only\nInstance\n3 ∗10−5\n+Attributes\nInstance\n8 ∗10−5\n+ID\nInstance\n8 ∗10−5\n+ID+Attributes\nInstance\n8 ∗10−5\nContent-Only\nAnnotator\n8 ∗10−5\n+Attributes\nAnnotator\n8 ∗10−5\n+ID\nAnnotator\n8 ∗10−5\n+ID+Attributes\nAnnotator\n8 ∗10−5\nSafety\nContent-Only\nInstance\n3 ∗10−5\n+Attributes\nInstance\n6 ∗10−5\n+ID\nInstance\n6 ∗10−5\n+ID+Attributes\nInstance\n6 ∗10−5\nContent-Only\nAnnotator\n3 ∗10−5\n+Attributes\nAnnotator\n3 ∗10−5\n+ID\nAnnotator\n6 ∗10−5\n+ID+Attributes\nAnnotator\n3 ∗10−5\nSentiment\nContent-Only\nInstance\n3 ∗10−5\n+Attributes\nInstance\n6 ∗10−5\n+ID\nInstance\n6 ∗10−5\n+ID+Attributes\nInstance\n3 ∗10−5\nContent-Only\nAnnotator\n3 ∗10−5\n+Attributes\nAnnotator\n3 ∗10−5\n+ID\nAnnotator\n3 ∗10−5\n+ID+Attributes\nAnnotator\n3 ∗10−5\nLlama3 - Intimacy\nMistral - Intimacy\nMistral - Offensiveness\nLlama3 - Offensiveness\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nF1\nContent Only\n+ID+Attributes\n+Attributes\n+ID\nFigure 6: Results on the instance split of the Intimacy and\nOffensiveness tasks show that also for other model families\ntraining with sociodemographics improves performance over\ntext-only predictions but including a unique annotator ID in\nthe prompt leads to much larger performance gains. Macro-\naverage F1 over five classes on the test set. Shows results for\nLlama 3 8B and Mistral 7B fine-tuned with different types of\ninput. Mean score over 10 different seeds with 95% confidence\nintervals from bootstrap sampling.\n7B (Jiang et al., 2023) in version 0.31 with the\nimplementation available using the Transformers\nlibrary (Wolf et al., 2020). We attempted to also\nrun experiments with Qwen2.5 (Qwen et al., 2025)\nbut were not able to archive stable results across\nmodel configurations with given hyperparameters,\nindicating the need for further hyperparmeter tun-\ning which was out of scope for these supplementary\nexperiments.\nFor experiments with Mistral 7B we use the\nsame fine-tuning settings as for Llama 3 (see Ap-\npendix B) but we use only the first 10 seeds. Re-\nsults in Figure 6 show that while exact macro-\naverage F1 scores differ by a few points and are\nless stable (likely due to non-optimal hyperparam-\neters), the overall pattern of results also holds for\nMistral 7B: training with sociodemographics im-\nproves performance over using only the content but\nincluding an unique annotator ID leads to much\nlarger gains.\n1https://huggingface.co/mistralai/Mistral-7B-v0.3\n16\n\n\nD\nZero-Shot Sociodemographic\nPrompting\nHere, we describe the zero-shot experiments that\nserve as a baseline in our fine-tuning experiments.\nD.1\nZero-Shot Prompt Templates\nThe prompt templates briefly describe the task,\npresent the text, and list possible answers in a\nmultiple-choice format.\nTemplates are derived\nfrom Hu and Collier (2024). Here, the sociode-\nmographic profile descriptions use a conversational\nformat and describe individual attributes in full\nsentences, e.g., The highest degree or level of\nschool that you have completed is a college de-\ngree. Prompt templates below are described in a\npseudo-JSON format, in actual experiments they\nare converted to the Llama 3 chat template.\nSociodemographic System Prompt\n’system’:\n’In terms of race or ethnicity, you are {RACE}. You\nare {AGE}. In terms of gender, you are {GENDER}.\nThe highest degree or level of school that you have\ncompleted is {EDUCATION}.\\n’\nIntimacy Task Prompt\n’user’: ’**Question**:\nConsider you read this post on Twitter, how inti-\nmate do you think it is?\\n**Post**: {TEXT}\\n(A)\nnot intimate at all\\n(B) barely intimate\\n(C) some-\nwhat intimate\\n(D) moderately intimate\\n(E) very\nintimate\\n’, ’assistant’: ’**Answer**: (’\nOffensiveness Task Prompt\n’user’: ’**Ques-\ntion**: Consider you read this comment on Red-\ndit, how offensive do you think it is?\\n**Post**:\n{TEXT}\\n(A) not offensive at all\\n(B) barely offen-\nsive\\n(C) somewhat offensive\\n(D) moderately of-\nfensive\\n(E) very offensive\\n’, ’assistant’: ’**An-\nswer**: (’\nPoliteness Task Prompt\n’user’: ’**Question**:\nConsider you read this email from a colleague,\nhow polite do you think it is?\\n**Email**:\n{TEXT}\\n(A) not polite at all\\n(B) barely po-\nlite\\n(C) somewhat polite\\n(D) moderately po-\nlite\\n(E) very polite\\n’, ’assistant’: ’**Answer**:\n(’\nSafety Task Prompt\n’user’:\n’**Question**:\nConsider you read this conversation between a\nchat bot and a human, do you think it is safe or\nunsafe in terms of harmful content, unfair bias\nor safety policy violations?\\n**Conversation**:\n{TEXT}\\n(A) safe\\n(B) unsure\\n(C) unsafe\\n’,\n’assistant’: ’**Answer**: (’\nSentiment Task Prompt\n’user’: ’**Question**:\nConsider you read this text, what do you think is the\nsentiment it expresses?\\n**Text**: {TEXT}\\n(A)\nVery negative\\n(B) Somewhat negative\\n(C) Neu-\ntral\\n(D) Somewhat positive\\n(E) Very posi-\ntive\\n’, ’assistant’: ’**Answer**: (’\nD.2\nZero-Shot Experiments\nWe evaluate the zero-shot performance of LLMs\nprompted with and without sociodemographic at-\ntributes on DEMO. The baseline setting uses the\ntextual content only and ignores annotators’ at-\ntributes associated with each rating, using one of\nthe task prompts. To derive rating values, we take\nthe first character from the model’s completion\n(e.g., B) and map it to the respective numeric label,\ndepending on the task (e.g., B to 1). In models us-\ning annotator attributes, we additionally describe\neach individual’s sociodemographic attributes in\nthe system prompt using the sociodemographic sys-\ntem prompt template and use it in combination with\nthe same task prompts. Attribute values are pre-\nprocessed in the same way as for the fine-tuning\nexperiments (see §4.3).\nChat-tuned LLMs are used for all zero-shot ex-\nperiments because they perform slightly better in\npreliminary experiments than base models. In par-\nticular, we evaluate Llama 3 Instruct 8B in the\nmain experiments. Additionally, we check for the\neffect of model size based on experiments using\nthe 70B variant with 4-bit quantization. Due to\nlimited computational resources, we quantize the\nmodel’s weights to 4-bit precision using the bitsand-\nbytes library. To investigate prompt robustness, we\nalso run experiments with an alternative format for\nprofile descriptions, where we simply list attribute\nvalues.\nAll models are evaluated on the test sets of the\nfive tasks in DEMO using the same setup as in the\nfine-tuning experiments with one exception: The\nrobustness experiments with attribute lists and the\nlarger model are performed only on the instance\nsplit.\nD.3\nResults: Inconsistent Effects of\nSociodemographic Prompting\nResults for prompting Llama 3 Instruct 8B and 70B\nare shown in Figure 7. We find using a list-like for-\nmat to describe attributes leads to less accurate\npredictions than using a conversational profile de-\nscription in full sentences. Consequently, all other\n17\n\n\nIntimacy\nOffensiveness\nPoliteness\nSafety\nSentiment\n0.20\n0.22\n0.24\n0.26\n0.28\n0.30\n0.32\n0.34\nF1\nContent Only (70B)\nContent Only\n+Attributes (70B)\n+Attributes\nAttributes List\nFigure 7: Results for zero-shot experiments on the instance\nsplit. Macro-average F1 over three (Safety) or five (all others)\nclasses for zero-shot prompted LLMs on each test set. Mean\nscore over 30 different seeds with 95% confidence intervals\nfrom bootstrap sampling.\nzero-shot experiments use the conversational for-\nmat.\nPrompting the 8B model with and without pro-\nviding annotator attributes, we mostly see no or\nslightly negative effects from adding attributes. A\nclear exception is the Politeness task where we\nsee a robust increase of about 3 points in macro-\naverage F1. In sum, the performance difference\nfrom including attributes is inconsistent across\ntasks. While not directly comparable, Beck et al.\n(2024) use the same dataset from which we create\nour Sentiment test set and find scores in a similar\nrange of .26 to .31 macro-averaged F1.\nFor the larger 70B model, we find slightly\nstronger effects from including annotator attributes\nbut no clear direction of effects. For Intimacy, Po-\nliteness and Sentiment scores improve slightly, for\nSafety and Offensiveness they decrease. These re-\nsults underscore that sociodemographic prompting\nhas inconsistent effects on performance on DEMO.\n18\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20897v1.pdf",
    "total_pages": 18,
    "title": "Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals' Subjective Text Perceptions",
    "authors": [
      "Matthias Orlikowski",
      "Jiaxin Pei",
      "Paul Röttger",
      "Philipp Cimiano",
      "David Jurgens",
      "Dirk Hovy"
    ],
    "abstract": "People naturally vary in their annotations for subjective questions and some\nof this variation is thought to be due to the person's sociodemographic\ncharacteristics. LLMs have also been used to label data, but recent work has\nshown that models perform poorly when prompted with sociodemographic\nattributes, suggesting limited inherent sociodemographic knowledge. Here, we\nask whether LLMs can be trained to be accurate sociodemographic models of\nannotator variation. Using a curated dataset of five tasks with standardized\nsociodemographics, we show that models do improve in sociodemographic prompting\nwhen trained but that this performance gain is largely due to models learning\nannotator-specific behaviour rather than sociodemographic patterns. Across all\ntasks, our results suggest that models learn little meaningful connection\nbetween sociodemographics and annotation, raising doubts about the current use\nof LLMs for simulating sociodemographic variation and behaviour.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}