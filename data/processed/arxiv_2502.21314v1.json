{
  "id": "arxiv_2502.21314v1",
  "text": "Raccoon: Multi-stage Diffusion Training with Coarse-to-Fine Curating Videos\nZhiyu Tan 1\nJunyan Wang 2\nHao Yang 3\nLuozheng Qin 3\nHesen Chen 4\nQiang Zhou 3\nHao Li 1*\n1 Fudan University\n2 The University of Adelaide\n3 INF Tech\n4 Shanghai Academy of Artificial Intelligence for Science\nA close-up of milk being poured into a cup of coffee, creating swirling patterns.\nAt sunset by a serene lake, a young blonde woman with wavy hair subtly smiles, adjusts her earring, and wears a black top under natural light.\nA raccoon with a black mask and ringed tail balances on a green surfboard with a yellow stripe, calmly riding a large, frothy wave in deep blue water.\nFigure 1. RACCOON Samples for Text-to-Video Generation. Our approach can generate high-resolution, temporally consistent, photore-\nalistic videos from text prompts. The samples displayed have a resolution of 512 × 512, last 4 seconds, and play at 8 frames per second.\nAbstract\nText-to-video generation has demonstrated promising\nprogress with the advent of diffusion models, yet existing ap-\nproaches are limited by dataset quality and computational\nresources. To address these limitations, this paper presents\na comprehensive approach that advances both data cura-\ntion and model design. We introduce CFC-VIDS-1M, a\nhigh-quality video dataset constructed through a systematic\ncoarse-to-fine curation pipeline. The pipeline first evalu-\nates video quality across multiple dimensions, followed by\na fine-grained stage that leverages vision-language mod-\nels to enhance text-video alignment and semantic richness.\nBuilding upon the curated dataset’s emphasis on visual\nquality and temporal coherence, we develop RACCOON,\n*Corresponding author\na transformer-based architecture with decoupled spatial-\ntemporal attention mechanisms.\nThe model is trained\nthrough a progressive four-stage strategy designed to effi-\nciently handle the complexities of video generation. Exten-\nsive experiments demonstrate that our integrated approach\nof high-quality data curation and efficient training strat-\negy generates visually appealing and temporally coherent\nvideos while maintaining computational efficiency. We will\nrelease our dataset, code, and models.\n1. Introduction\nText-to-video generation, which aims to synthesize video\nsequences from textual descriptions, has emerged as a sig-\nnificant research direction in AIGC. Recent years have wit-\nnessed remarkable progress in generative models, particu-\n1\narXiv:2502.21314v1  [cs.CV]  28 Feb 2025\n\n\nTable 1. Comparison of our dataset and other popular video-text datasets. We’ve compiled statistics on video-text datasets, specifically\nanalyzing metrics such as the domain, the number of video clips after scene detection, average duration, average text length.\nDataset\nVenue & Year\nText\nDomain\nClip Num\nAvg Video Len\nAvg Text Len\nResolution\nMSVD [8]\nACL\n2011\nHuman\nOpen\n2K\n9.7s\n8.7 words\n-\nUCF101 [37]\nICCV 2013\nHuman\nAction\n13K\n7.2s\n4.3 words\n240p\nLSMDC [31]\nCVPR 2015\nHuman\nMovie\n118K\n4.8s\n7.0 words\n1080p\nActivityNet [6]\nCVPR 2015\nHuman\nAction\n100K\n36.0s\n13.5 words\n-\nMSR-VTT [45]\nCVPR 2016\nHuman\nOpen\n10K\n15.0s\n9.3 words\n240p\nDiDeMo [1]\nICCV 2017\nHuman\nFlickr\n27K\n6.9s\n8.0 words\n-\nYouCook2 [51]\nAAAI 2018\nHuman\nCooking\n14K\n19.6s\n8.8 words\n-\nVATEX [43]\nICCV 2019\nHuman\nOpen\n41K\n10s\n15.2 words\n-\nHowTo100M [28]\nICCV 2019\nASR\nOpen\n136M\n3.6s\n4.0 words\n240p\nYT-Temporal-180M [48]\nNIPS 2021\nASR\nOpen\n180M\n-\n-\n-\nHD-VILA-100M [46]\nCVPR 2022\nASR\nOpen\n103M\n13.4s\n32.5 words\n720p\nPanda-70M [10]\nCVPR 2024\nAuto\nOpen\n70.8M\n8.5s\n13.2 words\n720p\nCFC-VIDS-1M\nAuto\nOpen\n1M\n10.6s\n89.3 words\n720p\nlarly in the realm of diffusion-based approaches [5]. The\nsuccess of these models in image synthesis has opened\nnew possibilities for video generation, with potential appli-\ncations spanning creative content creation, educational re-\nsources, and visual storytelling [16] [27] [49]. The synthe-\nsis of videos that faithfully align with textual descriptions\nrequires both comprehensive training data and sophisticated\nlearning approaches to handle the complexities of spatial-\ntemporal modeling.\nRecent text-to-video generation models [3, 11, 16, 26,\n27, 44] have demonstrated promising results by leverag-\ning large-scale video datasets [10, 28, 46]. Despite these\nadvances, existing datasets present limitations that affect\nmodel training effectiveness. These datasets face two pri-\nmary challenges: 1) video quality issues, including tem-\nporal inconsistencies from imprecise scene detection and\nprevalence of static content lacking motion dynamics; 2)\ncaption quality limitations, manifested in frame-by-frame\ndescriptions rather than coherent narratives, insufficient de-\nscriptive details with limited word counts, and inadequate\nsemantic alignment between text and video content.\nTo\naddress these challenges, we introduce CFC-VIDS-1M,\nconstructed through a systematic coarse-to-fine curation\npipeline that ensures both visual and caption quality.\nOur proposed coarse-to-fine curation pipeline addresses\ndataset quality through two systematic stages.\nAt the\ncoarse level, we evaluate video quality across multiple di-\nmensions including aesthetic appeal, temporal consistency,\nOCR presence, motion dynamics, and category distribution.\nEach dimension is quantitatively assessed using specialized\nmodels. The fine-grained stage focuses on enhancing text-\nvideo alignment through a two-step process that leverages\nvision-language and large language models, ensuring both\ncaption informativeness and semantic accuracy.\nTo fully leverage the curated CFC-VIDS-1M dataset\nfor text-to-video synthesis, efficient architectures and train-\ning strategies are crucial. We develop a transformer-based\nmodel RACCOON with decoupled spatial-temporal atten-\ntion mechanisms, enabling efficient processing of video\nsequences.\nBuilding upon this architecture, we propose\na four-stage training strategy that progressively enhances\nmodel capabilities: beginning with semantic learning from\npre-trained image models, followed by temporal model-\ning at low resolution, then scaling to high-resolution video\ngeneration, and finally refining visual quality through tar-\ngeted fine-tuning. This systematic approach effectively ad-\ndresses the computational challenges of video generation\nwhile maintaining generation quality.\nThis paper presents a comprehensive approach to text-to-\nvideo generation through both data curation and model de-\nsign. The proposed coarse-to-fine curation pipeline ensures\nhigh-quality training data across multiple dimensions, while\nthe progressive training strategy enables efficient learning\nof both semantic and temporal aspects. Extensive experi-\nments demonstrate the effectiveness of our approach in gen-\nerating high-quality and temporally coherent videos.\n2. Related Work\nVision-Language Dataset. To advance video understand-\ning and generation, numerous video-text datasets have been\ndeveloped, differing in aspects such as video length, res-\nolution, domain, and scale.\nThese datasets are com-\npared in Table 1.\nEarly datasets like MSVD [8] and\nMSRVTT [45] rely on human annotation for video caption-\ning, ensuring high-quality text-video alignment but limiting\ndataset scale. To address scalability, datasets such as YT-\nHowTo100M [28] and HD-VILA-100M [46] adopt auto-\nmatic annotation through ASR-generated subtitles, though\nthis approach often leads to caption inaccuracies.\nRe-\ncent work Panda-70M [10] utilizes a cross-modality teacher\n2\n\n\n(a) Data Pre-process\nLong Videos\nSplited Videos\nSampling\nFiltering\n(c) Fine-grained Curation\nThe video shows a white boat moving through the \nwater at a high speed, creating a large wake \nbehind it. The boat is moving from the left to the \nright of the frame, and the wake it leaves behind \nis white and frothy. The water is a deep blue color, \nand the sky is clear and blue. In the background, \nthere is a hilly coastline with green vegetation.\nThe first frame shows a small cat running …\n… accents and glowing light blue trim trim... \n… at the camera. The next shot shows …\nMLLM Captioning & LLM Filtering\nCurated Videos\n(b) Coarse-level Curation\nAes = 4.0\nAes = 6.0\nVideo Quality\nCategory Distribution\n…\nBalance \nCategory\nOCR = 6\nOCR = 0\nORC Detection\nAnimal\nPeople\nVehicles\nFood\nVideo \nGame\nTC = 0.9\nTemporal Consistency\nTC = 0.02\nMotion Analysis\nMotion = 3.3\nMotion = 0.1\nMotion = 7.9\nFigure 2. Overview of data curation. Firstly, we employ a scene splitting algorithm to divide long videos with multiple scenes into single\nscene shots. We filter and sample videos based on five aspects: video quality, OCR, temporal consistency, category, and motion. Finally,\nwe use a Large Language Model (LLM) to curate video-text pairs for error captions.\nmodel to generate 70 million text-video pairs, demonstrat-\ning the potential of model-based caption generation.\nIn\nthis work, we present a two-stage coarse-to-fine curation\npipeline that combines multi-dimensional quality filtering\nand model-based caption enhancement to ensure both vi-\nsual quality and text-video alignment.\nVideo Generation Methods.\nRecent advances in video\nsynthesis have explored various architectural paradigms.\nAutoregressive methods like VQGAN extensions [20, 47]\npredict sequential video tokens but face computational con-\nstraints due to their sequential nature.\nGAN-based ap-\nproaches [22, 23] achieve efficient generation but often\nstruggle with stability and temporal consistency. More re-\ncently, transformer-based diffusion models have emerged\nas a promising direction for video generation [3, 11, 16,\n19, 26, 27, 44, 49]. These approaches demonstrate supe-\nrior generation quality and temporal coherence, though at\nthe cost of substantial computational requirements.\nFor\ninstance, W.A.L.T [16] achieves excellent performance\nthrough window-based attention and unified latent space for\nimages and videos. In this work, we present a transformer-\nbased diffusion model trained with a multi-stage strategy\nfor high-quality video generation.\nEfficient Training Strategies.\nRecent works in diffu-\nsion models have explored various approaches to improve\ntraining efficiency. Training strategy decomposition meth-\nods segment the learning process into sequential stages,\nas demonstrated in PixelArt [9] and OmniDiffusion [38],\nwhere text-image alignment and visual quality are opti-\nmized progressively. Alternatively, model decomposition\napproaches utilize cascaded sub-models for generation, as\nimplemented in Imagen [18] and W.A.L.T [16]. More re-\ncently, these efficient training strategies have been adapted\nfor video generation tasks [16, 18, 33], demonstrating\npromising results in managing computational constraints\nwhile maintaining generation quality.\nIn this work, we\npropose a progressive multi-stage training strategy that ef-\nfectively balances computational efficiency and generation\nquality while maintaining temporal consistency.\n3. Coarse-to-Fine Curation\nThis section describes the construction of RACCOON, a\ncomprehensively curated video dataset derived from HD-\nVILA [46]. The initial HD-VILA dataset comprises 3.8M\nhigh-resolution videos, which are processed into 108M\nvideo clips through systematic segmentation.\nFollowing\nestablished practices [2, 10, 49], scene transitions in raw\nvideos are first detected and removed using PySceneDe-\ntect [7] to eliminate motion inconsistencies that could affect\nmodel performance. The curation process comprises two\nmain stages: coarse-level filtering for basic quality control\nand fine-grained refinement for enhanced text-video align-\nment.\n3.1. Coarse-Level Curation\nHigh-quality training data is crucial for text-to-video syn-\nthesis models, requiring careful curation across multiple di-\nmensions including visual quality, category balance, and\ntemporal coherence. To establish baseline quality control,\nwe design a coarse curation strategy that evaluates video\nsegments through five key aspects: video quality assess-\nment, OCR detection, temporal consistency verification,\n3\n\n\nFigure 3. Comparison of statistics between uncurated and curated datasets. (a) through (d) present comparative statistics of uncurated\nand curated datasets across multiple dimensions: (a) aesthetics, (b) motion, (c) Optical Character Recognition (OCR), (d) temporal consis-\ntency.\ncategory distribution balancing, and motion analysis. Each\nvideo segment is assigned corresponding quality tags to fa-\ncilitate the filtering process. Fig 3 illustrates the comparison\nbetween uncurated and curated datasets.\nVideo Quality. High-quality visual content fundamentally\ndetermines the generation capability of text-to-video mod-\nels, as the model learns to synthesize videos by replicating\nthe visual characteristics of training data. We employ the\nLAION Aesthetics model [34] to evaluate and filter videos\nbased on aesthetic scores, eliminating those with low visual\nappeal. This approach effectively removes visually inappro-\npriate content, including videos with irregular color distri-\nbutions or undesirable visual elements.\nOCR Detection. The presence of subtitles and text overlays\nin videos can negatively impact the visual quality of gen-\nerated content and introduce undesirable patterns in text-\nto-video synthesis. To address this issue, we employ PP-\nOCR [24] for automated text detection in video frames.\nSpecifically, we compute OCR scores for keyframes in each\nvideo clip and use these scores as filtering criteria during the\ndata curation process. Videos with high OCR scores, indi-\ncating substantial text content, are filtered out to maintain\nthe visual purity of our training dataset.\nTemporal Consistency. Incorrect scene splitting in videos\ncan impair model training by introducing semantic gaps\nin visual flow and content coherence.\nTo address this,\nwe leverage the CLIP model to ensure temporal coherence\nwithin video clips. Specifically, we compute the cosine sim-\nilarity between the initial and final frames to assess frame-\nlevel consistency quantitatively. Videos exhibiting low sim-\nilarity scores, which often indicate sudden scene changes or\nsemantic inconsistencies, are filtered out from the dataset.\nMotion Analysis. The quality of motion representation in\ntraining videos directly impacts a model’s ability to gener-\nate natural and smooth video sequences. To quantify motion\ncharacteristics, we utilize the RAFT [39] model to compute\noptical flow scores for each video clip. Videos exhibiting\nFigure 4. The distribution of categories in RACCOON. The\ndataset contains a total of 14 categories, with a balanced distri-\nbution across the primary categories.\neither minimal motion or excessive movement are filtered\nout, as both extremes can lead to degraded motion model-\ning and unrealistic video generation.\nCategory Distribution.\nCategory imbalance in training\ndata can substantially degrade model performance, leading\nto biased generation results and limited diversity across dif-\nferent video types. To address the category imbalance in-\nherent in the existing datasets, we implement a CLIP-based\ncategorization system. Our approach computes the average\nCLIP features from the initial, middle, and final frames of\neach video, and then assigns tags based on their similarity\nto predefined category embeddings.\nBased on the established quality assessment, we im-\nplement a hierarchical sampling strategy that first filters\nout videos with quality scores below predefined thresholds.\nSpecifically, videos are removed if they exhibit low aes-\nthetic scores, high OCR presence, poor temporal consis-\ntency, and extreme motion patterns. The remaining videos\nare then sampled according to their category distribution to\nensure a balanced representation. Fig.3 shows the compar-\nison of statistics between uncurated and curated datasets.\n4\n\n\nTable 2. Statistics of noun and verb concepts for different datasets.\nVN: valid distinct nouns (appearing more than 10 times); DN: total\ndistinct nouns; Avg N: average noun count per video. VV: valid\ndistinct verbs (appearing more than 10 times); DV: total distinct\nverbs; Avg N: average verbs count per video.\nDataset\nVN/DN\nVV/DV\nAvg N\nAvg V\nPandas-70M\n16.1%\n19.2%\n4.3\n1.9\nOurs\n20.3%\n41.1%\n22.5\n15.9\nThese coarse-level quality tags establish the foundation for\nsubsequent fine-grained curation while maintaining diver-\nsity across categories.\n3.2. Fine-Grained Curation\nHigh-quality text-video alignment is essential for effective\ntext-to-video synthesis models. Following the coarse-level\nfiltering, a two-step fine-grained curation strategy is pro-\nposed that focuses on generating informative captions and\nfiltering problematic text-video pairs.\nDespite CLIP-based filtering in coarse-level curation,\nseveral critical issues persist in video captioning: scene\ntransition errors where captions fail to recognize scene\nboundaries, token generation failures resulting in repetitive\ncontent, and frame-level descriptions that lack temporal co-\nherence. To address these challenges, a two-phase curation\nis implemented. First, the state-of-the-art vision-language\nmodel ViLA [25] is employed to generate informative and\ndetailed video captions. Subsequently, LLAMA [12] is uti-\nlized to identify and filter out problematic captions through\nprompt-based evaluation:\nPlease respond with ‘Yes’ or ‘No’ to the fol-\nlowing questions:\n• Given the preceding video caption, is there an\nindication of a possible scene transition?\n• Does the preceding video caption suggest a\nshift towards a series of descriptive image cap-\ntions?\n• Does the video caption conclude with repetitive\nphrases or sentences?\nAs quantitatively demonstrated in Table 2, the enhanced\ncaptions show substantial improvements in vocabulary di-\nversity and semantic density. Through this systematic fine-\ngrained curation approach, both caption quality and text-\nvideo alignment are significantly enhanced, enabling more\nprecise control over video generation through enriched tex-\ntual descriptions.\n4. RACCOON\nOur method consists of a transformer-based diffusion ar-\nchitecture and a systematic training strategy that enables\nefficient text-to-video synthesis.\nThe model architecture\nleverages efficient latent space processing, while the train-\ning strategy progressively enhances generation capabilities\nacross multiple stages.\n4.1. Model Architecture\nGiven an input video x ∈R(1+T )×H×W ×C, we employ\na 3D Causal VAE to address the computational challenges\nin video generation through efficient dimensionality reduc-\ntion. The encoder maps the input into a low-dimensional\nrepresentation z ∈R(1+t)×h×w×c, achieving compres-\nsion along both spatial (fs = H/h = W/w) and tempo-\nral (ft = T/t) dimensions. By independently encoding\nthe first frame, this architecture establishes a unified latent\nspace that effectively bridges image and video domains.\nThe transformer backbone processes these latent repre-\nsentations through a series of attention blocks. The input\nlatents are first patchified with size p and enhanced with\npositional information through a combination of 2D spa-\ntial and 1D temporal embeddings based on sinusoidal func-\ntions [41].\n4.2. Training Strategy\nAn efficient text-to-video generation model requires a well-\nstructured training strategy capable of handling the complex\nnature of synthesizing video content across both spatial and\ntemporal dimensions. Thus, we present a four-stage train-\ning pipeline that progressively enhances the model’s ca-\npacity, transitioning seamlessly from basic semantic com-\nprehension to sophisticated video generation. Throughout\nall stages, the spatial attention mechanism operates within\neach frame, using tokens of dimensions 1×hp ×wp, where\nhp = h/p, wp = w/p and p is patchify size.\nSimultaneously, the temporal attention mechanism is\nmarked by a systematic expansion of the 3D window size.\nBeginning from an initial size of (t×1×1), the window pro-\ngressively accommodates the total number of tokens in the\nvideo latent size (t×hp×wp). This adaptive approach facil-\nitates the model in incrementally learning and synthesizing\nmore complex spatio-temporal patterns. A comprehensive\nillustration of this four-stage training can be found in Fig.5.\nStage 1: Semantic Learning. Text-to-video generation re-\nquires robust semantic understanding while training such\ncapabilities directly on limited video datasets remains com-\nputationally intensive and inefficient. To address this limita-\ntion, we leverage pre-trained text-to-image models that have\ndeveloped comprehensive semantic knowledge from large-\nscale image datasets. Specifically, we employ a pre-trained\nCausal 3D VAE to establish a unified latent space for both\nimages and videos. This shared representation space allows\nus to effectively bridge image and video domains by treat-\ning images as single-frame videos. During this stage, we\nexclusively optimize the semantic module parameters with\n5\n\n\nVAE\nEncoder\n3D Causal \nVAE\nStage 1: Semantic Learning\nA small cat running …\nT5\nText Encoder\nText Prompt\nDiffusion \nTransformer\nSpatial Attention\n(1 ×  ȾɆ ×  ɍɆ)\nText Feature\nImage data\n(256 × 256 × 1)\n× L\n3D Causal \nVAE\nA small cat running …\nT5\nText Encoder\nText Prompt\nDiffusion \nTransformer\nText Feature\nImage data (256 × 256 × 1)\nVideo data (256 × 256 × 17)\nSpatial\n(1 ×  ȾɆ ×  ɍɆ)\nTemporal\n(t ×  Έ ×  Έ)\n× L\nStage 2: Temporal Learning\nVAE\nEncoder\n3D Causal \nVAE\nA small cat running …\nT5\nText Encoder\nText Prompt\nDiffusion \nTransformer\nText Feature\nImage data \n(256 × 256 × 1)\nVideo data \n(256 × 256 × 17)\nImage data \n(512 × 512 × 1)\nVideo data \n(512 × 512 × 17)\nStage 3: R & D  Enhancement\nVAE\nEncoder\nSpatial\n(1 ×  ȾɆ ×  ɍɆ)\nTemporal\n(t ×  Ⱦɍ ×  ɍɍ)\n× L\nTunable\n3D Causal \nVAE\nA small cat running …\nT5\nText Encoder\nText Prompt\nDiffusion \nTransformer\nText Feature\nImage data \n(256 × 256 × 1)\nVideo data \n(512 × 512 × 17)\nImage data \n(512 × 512 × 1)\nVideo data \n(512 × 512 × 33)\nStage 4: Quality Refinement\nVAE\nEncoder\nSpatial\n(1 ×  ȾɆ ×  ɍɆ)\nTemporal\n(t ×  ȾɆ ×  ɍɆ)\n× L\nFrozen\nFigure 5. Four-stage training pipeline. Leverages pre-trained text-to-image models to establish semantic understanding capabilities as the\nfoundation for video generation. Jointly trains image and video data at low resolution to efficiently optimize temporal modules. Enhances\nspatial details and temporal coherence through high-resolution training, enabling long video generation. Fine-tunes the model using a\ncurated high-quality dataset to improve visual consistency and aesthetic quality of generated videos.\nthrough text-to-image tasks. This unified approach enables\nefficient knowledge transfer from image pre-training, allow-\ning subsequent stages to focus on temporal dynamics.\nStage 2: Temporal Learning. The optimization process\nof temporal modules necessitates significant computational\nresources due to its complexity. To improve training ef-\nficiency, we propose to jointly train the model with low-\nresolution data. Specifically, we conduct joint training with\nboth image-text and video-text pairs at 256 × 256 reso-\nlution. This joint optimization strategy enables fast con-\nvergence of temporal modules while maintaining semantic\nconsistency through continued image-text training. It’s im-\nportant to note that a temporal attention size of (t×1×1) is\nemployed for efficient training. The low-resolution training\napproach effectively balances computational efficiency and\ntemporal modeling capability, establishing the foundation\nfor subsequent training stages.\nStage 3: Resolution & Duration Enhancement. While\nStage 2 establishes temporal modeling capabilities, the low-\nresolution outputs remain insufficient for practical applica-\ntions that demand high-quality video generation. To address\nthis limitation, we extend our training to high-resolution,\nlong-duration video synthesis. Specifically, we fine-tune the\nmodel using high-resolution images and videos while main-\ntaining the joint training strategy from Stage 2. In contrast\nto Stage 2 training, we apply a temporal attention size of\n(t × hw × ww) to enhance temporal consistency. Here, hw\nand ww denote the height and width of the 3D sub-window\nsize among video latents. Specifically, we set both hw and\nww to 8. This approach enhances both spatial details and\ntemporal coherence over extended durations, enabling the\nmodel to generate videos that meet contemporary quality\nstandards.\nThe transition to high-resolution training im-\nproves the model’s capability to produce visually detailed\nand temporally consistent videos.\nStage 4:\nQuality Refinement Despite achieving high-\nresolution video generation in Stage 3, the output quality\nexhibits inconsistency in visual aesthetics. To stabilize gen-\neration quality and enhance visual appeal, we implement a\nfocused fine-tuning strategy. Specifically, from our curated\ndataset, we further select 50K videos and 100k images with\nstricter quality criteria, requiring aesthetic scores above 5.5\nfor video and 7.0 for image. This carefully filtered dataset,\nsupplemented by human evaluation, guides the model to-\nward generating visually consistent and refined videos. The\nfine-tuning process stabilizes generation quality while en-\nhancing the visual aesthetics of the generated videos. Dur-\ning this training stage, we use 3D full temporal attention,\nrepresented as t × hp × wp, to further improve the quality\nof the generated videos.\nThrough this progressive training pipeline, our model\nevolves from basic semantic understanding to high-quality\nvideo generation capabilities. Each stage addresses specific\nchallenges, from semantic learning to aesthetic refinement,\nenabling our model to generate visually refined and tempo-\nrally coherent videos from text prompts.\n6\n\n\n5. Experiment\nDatasets. For the ablation study of the training strategy,\nwe utilize the standard video benchmark UCF-101 [37] for\nclass-conditional generation. All training and testing splits\nfrom UCF-101 are used for training purposes. For the text-\nto-video generation, we employ a joint training approach on\ntext-image and text-video pairs. The training dataset con-\nsists of 43 million text-image pairs and three categories of\ntext-video pairs: (1) a complete version with 4 million pairs,\n(2) an open-source version with 1 million curated pairs, and\n(3) an open-source version with 1.5 million uncurated pairs.\nEvaluation metrics.\nFor quantitative comparisons, we\nutilize three evaluation metrics: Fr´echet Video Distance\n(FVD) [40]. Our primary emphasis is on FVD, given that its\nimage-based counterpart, FID, closely corresponds with hu-\nman subjective judgment. In accordance with the evaluation\nguidelines established by StyleGAN-V [36], FVD scores\nare computed by analyzing 2,048 video clips in zero-shot\nmanner. To further investigate the visual quality of video\nproduced by RACCOON, we conduct a user study to dis-\ncern human preferences among different models. In this\nstudy, we ask users to rate the videos generated by each\nmodel on a scale of 1 to 5 across five dimensions: faithful-\nness, text-video alignment, temporal consistency, dynamic\ndegree, and imaging quality. For more details, please refer\nto the supplementary materials.\nImplementation details.\nWe employ the AdamW op-\ntimizer for training all models, using a constant learn-\ning rate of 2 × 10−4. The only data augmentation tech-\nnique we utilize is horizontal flipping. In accordance with\nstandard practices in generative modeling studies [29], we\nmaintain an Exponential Moving Average (EMA) of Latte\nweights throughout the training process, using a decay rate\nof 0.9999. All the results reported in this study are directly\nobtained from the EMA model.\n5.1. Ablation Study\nTo validate the effectiveness of each stage in our four-\nstage training pipeline, we conduct ablation studies by re-\nmoving different stages while keeping other components\nunchanged. The quantitative results are presented in Tab.\n3. By prioritizing the training of semantic modules early\nin Stage 1, computational costs for subsequent stages are\nreduced, facilitating faster convergence in Stage 2. The in-\ntegration of Stage 1 and Stage 2 training strengthens the\nsemantic foundation and accelerates the training of tempo-\nral modules. This efficient progression is important for im-\nproving training efficiency in Stage 3, which focuses on pro-\ncessing high-resolution, long-duration video content, lead-\ning to substantially faster convergence. Note that, Stage 4 is\noptimized for high aesthetic quality generation. Therefore,\nwe have not conducted experiments on Stage 4 within the\nTable 3. Ablative results of different training stages. Under the\nsame computational resources, Stage 1 training efficiently primes\nthe model’s semantic modules, significantly accelerating the con-\nvergence of subsequent stages, particularly Stage 2 for tempo-\nral modules, and culminating in faster training of high-resolution,\nlong-duration videos in Stage 3.\nTraining Stages\nRes & Frames\nFVD\nGPU\nstage 1\nstage 2\nstage 3\nDays\n✓\n256 × 256 × 17\n267\n1.5\n✓\n✓\n256 × 256 × 17\n144\n1.5\n✓\n512 × 512 × 33\n478\n3\n✓\n✓\n✓\n512 × 512 × 33\n313\n3\nTable 4. Comparison of Models on UCF101, where the symbol ∗\nindicates evaluation based on 10k generated videos, while others\nare based on 2048 videos. § denotes uncleaned data.\nMethod\nScale\n(#) P-T\nRes & Frames\nUCF101\nVideos\nFVD\nCogVideo [20]\n15.5B\n5.4M\n256 × 256 × 16\n701\nMagicVideo [50]\n-\n10M\n256 × 256 × 16\n699\nMake-A-Video [35]\n9.7B\n-\n256 × 256 × 16\n367\nPYoCo [14]\n0.3B\n-\n256 × 256 × 16\n355\nLVDM [17]\n1.2B\n18K\n256 × 256 × 16\n641\nModelScope [42]\n1.7B\n10M\n256 × 256 × 16\n639\nVideoLDM [4]\n4.2B\n10M\n256 × 256 × 16\n550\nAnimateDiff∗[15]\n-\n2M\n256 × 256 × 16\n421\nours\n1B\n1M\n256 × 256 × 17\n387\nLatte [27]\n1B\n-\n512 × 512 × 16\n463\nOpenSorav1.2 [49]\n1.2B\n10M\n512 × 512 × 17\n1274\nours\n1B\n1M\n512 × 512 × 17\n437\nOpenSorav1.2 [49]\n1.2B\n10M\n512 × 512 × 33\n1355\nours\n1B\n1M\n512 × 512 × 33\n469\nours\n1B\n1.5M§\n512 × 512 × 33\n705\nours\n1B\n4M\n512 × 512 × 33\n435\nours\n3B\n4M\n512 × 512 × 33\n412\nUCF101 dataset.\n5.2. Results\nQuantitative Results\nWe evaluate our method against\nstate-of-the-art text-to-video generation techniques using\nthe FVD metric on the UCF-101 dataset, considering var-\nious resolutions and durations. Our 1B text-to-video model\nis trained on a diverse datasets: a 1M curated dataset, a\n1.5M uncurated dataset, and a 4M curated dataset.\nAs\nshown in Table 4, the results demonstrate that the model,\ntrained using our proposed curation pipeline, consistently\noutperforms others across multiple resolutions and dura-\ntions. Additionally, our 3B video generation model, trained\non the 4M curated dataset, also achieves the best perfor-\nmance. For a fair comparison, we standardize the sampling\nsteps for both Latte [27] and Opensora1.2 [49] to 100.\nQualitative Results Figure 6 illustrates the video gener-\n7\n\n\nIn a close-up against a dark background, a red-headed matchstick strikes a silver honeycomb-patterned matchbox, igniting a bright flame.\nAn elderly man with a white beard, wearing a straw hat, plaid shirt, blue overalls, and white gloves stands in a colorful garden, waving his hands.\nIn a close-up against a black background, water droplets cascade over a vibrant bunch of green parsley, splashing and scattering in slow motion.\nA black and white panda balances upright on a yellow surfboard in murky water, with a dense bamboo forest in the background.\nFigure 6. Qualitative Results. Example videos generated by our method at a resolution of 512 × 512 pixels, with a duration of 4 seconds at\n8 frames per second. Our model is capable of generating temporally consistent, photorealistic videos that align with the provided prompts.\nation results from RACCOON based on various prompts.\nRACCOON consistently delivers realistic, high-resolution,\nand long-duration video generation results across all scenar-\nios. It effectively captures detailed motion and maintains\ntemporal consistency, ensuring that the generated videos\nalign closely with the provided textual prompts. Whether\nthe prompts involve complex actions, subtle movements, or\nintricate scenes, our model demonstrates its ability to pro-\nduce photorealistic videos that are both visually compelling\nand contextually accurate.\n6. Conclusion\nIn conclusion, this paper introduces a robust methodology\nthat significantly advances data curation and model training\nstrategies for video generation. We present CFC-VIDS-\n1M, a meticulously curated, high-quality video dataset\ndeveloped through a systematic coarse-to-fine pipeline.\nThis process not only enables rigorous multi-dimensional\nvideo quality assessment but also employs cutting-edge\nlanguage models to enhance text-video alignment and\nenrich captioning detail. Complementing CFC-VIDS-1M,\nwe propose RACCOON, a transformer-based model archi-\ntecture designed with a focus on prompt adherence, visual\nquality, and temporal consistency. By employing decoupled\nspatial-temporal attention mechanisms and a progressive\nfour-stage training strategy, our model optimizes compu-\ntational efficiency and enhances video generation fidelity.\nOur extensive experimental validation demonstrates that\nRACCOON\nconsistently\ngenerates\nvisually\nengaging\nand temporally coherent videos, all while maintaining\nimpressive computational efficiency.\nThe pioneering\nmethodologies introduced in this work establish a solid\nfoundation for future research, paving the way for more\nadvanced, high-fidelity video generation models that push\nthe boundaries of current capabilities in this field. We an-\nticipate these contributions will inspire further innovations.\n8\n\n\nReferences\n[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef\nSivic, Trevor Darrell, and Bryan Russell. Localizing mo-\nments in video with natural language.\nIn Proceedings of\nthe IEEE international conference on computer vision, pages\n5803–5812, 2017. 2\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 3\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023. 2, 3\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023. 7\n[5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, et al. Video generation models as world\nsimulators. 2024. URL https://openai. com/research/video-\ngeneration-models-as-world-simulators, 3, 2024. 2\n[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\nand Juan Carlos Niebles. Activitynet: A large-scale video\nbenchmark for human activity understanding. In Proceed-\nings of the ieee conference on computer vision and pattern\nrecognition, pages 961–970, 2015. 2\n[7] Brandon Castellano. Pyscenedetect. https://github.\ncom/Breakthrough/PySceneDetect, 2014. 3\n[8] David Chen and William B Dolan. Collecting highly paral-\nlel data for paraphrase evaluation. In Proceedings of the 49th\nannual meeting of the association for computational linguis-\ntics: human language technologies, pages 190–200, 2011.\n2\n[9] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao,\nEnze Xie, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training\nof diffusion transformer for photorealistic text-to-image syn-\nthesis. In The Twelfth International Conference on Learning\nRepresentations. 3\n[10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace,\nEkaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon,\nYuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang,\net al.\nPanda-70m: Captioning 70m videos with multiple\ncross-modality teachers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13320–13331, 2024. 2, 3\n[11] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu.\nSeine: Short-to-long video diffu-\nsion model for generative transition and prediction. In The\nTwelfth International Conference on Learning Representa-\ntions, 2023. 2, 3\n[12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024. 5, 13\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873–12883, 2021. 14\n[14] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 22930–22941, 2023. 7\n[15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang,\nYaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin,\nand Bo Dai. Animatediff: Animate your personalized text-\nto-image diffusion models without specific tuning.\narXiv\npreprint arXiv:2307.04725, 2023. 7\n[16] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos´e Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2, 3\n[17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation.\narXiv preprint arXiv:2211.13221,\n2022. 7\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 3\n[19] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. Advances in Neural Information Processing\nSystems, 35:8633–8646, 2022. 3\n[20] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 3, 7\n[21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si,\nYuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin,\nNattapol Chanpaisit, et al. Vbench: Comprehensive bench-\nmark suite for video generative models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21807–21818, 2024. 15, 16\n[22] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo pure transformers can make one strong gan, and that\ncan scale up. Advances in Neural Information Processing\nSystems, 34:14745–14758, 2021. 3\n[23] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang,\nZhuowen Tu, and Ce Liu. Vitgan: Training gans with vi-\nsion transformers. arXiv preprint arXiv:2107.04589, 2021.\n3\n9\n\n\n[24] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao\nJiang, Yongkun Du, Yuning Du, Lingfeng Zhu, Baohua Lai,\nXiaoguang Hu, et al. Pp-ocrv3: More attempts for the im-\nprovement of ultra lightweight ocr system. arXiv preprint\narXiv:2206.03001, 2022. 4, 13\n[25] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Moham-\nmad Shoeybi, and Song Han. Vila: On pre-training for vi-\nsual language models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n26689–26699, 2024. 5\n[26] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu,\nPing Luo, and Mingyu Ding. Vdt: General-purpose video\ndiffusion transformers via mask modeling. arXiv preprint\narXiv:2305.13311, 2023. 2, 3\n[27] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Zi-\nwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte:\nLatent diffusion transformer for video generation.\narXiv\npreprint arXiv:2401.03048, 2024. 2, 3, 7, 16\n[28] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowto100m: Learning a text-video embedding by watching\nhundred million narrated video clips. In Proceedings of the\nIEEE/CVF international conference on computer vision,\npages 2630–2640, 2019. 2\n[29] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 7\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 13\n[31] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt\nSchiele. A dataset for movie description. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 3202–3212, 2015. 2\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 14\n[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in neural information\nprocessing systems, 35:36479–36494, 2022. 3\n[34] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278–25294, 2022. 4,\n12\n[35] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 7\n[36] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 3626–3636, 2022. 7\n[37] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 2, 7\n[38] Zhiyu Tan, Mengping Yang, Luozheng Qin, Hao Yang, Ye\nQian, Qiang Zhou, Cheng Zhang, and Hao Li.\nAn em-\npirical study and analysis of text-to-image generation us-\ning large language model-powered textual representation. In\nEuropean Conference on Computer Vision, pages 472–489.\nSpringer, 2025. 3\n[39] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow.\nIn Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part II 16, pages 402–419. Springer,\n2020. 4, 13\n[40] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 7\n[41] A Vaswani. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017. 5\n[42] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 7\n[43] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang. Vatex: A large-scale, high-\nquality multilingual dataset for video-and-language research.\nIn Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 4581–4591, 2019. 2\n[44] Yaohui Wang, Xin Ma, Xinyuan Chen, Cunjian Chen, An-\ntitza Dantcheva, Bo Dai, and Yu Qiao.\nLeo: Generative\nlatent image animator for human video synthesis. Interna-\ntional Journal of Computer Vision, pages 1–13, 2024. 2, 3\n[45] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5288–5296, 2016. 2\n[46] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing high-resolution video-language representation with\nlarge-scale video transcriptions.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5036–5045, 2022. 2, 3\n[47] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021. 3\n[48] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Mer-\nlot: Multimodal neural script knowledge models. Advances\nin neural information processing systems, 34:23634–23651,\n2021. 2\n10\n\n\n[49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 2, 3, 7, 16\n[50] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 7\n[51] Luowei Zhou, Chenliang Xu, and Jason Corso.\nTowards\nautomatic learning of procedures from web instructional\nvideos. In Proceedings of the AAAI Conference on Artificial\nIntelligence, 2018. 2\n11\n\n\nA. Ethical and Social Impacts\nThe proposed text-to-video curation pipeline and efficient\ntraining strategy represent a significant leap forward in gen-\nerative AI capabilities, making video production more ac-\ncessible, cost-effective, and scalable. These advancements\nhave the potential to empower creators across diverse in-\ndustries such as education, entertainment, marketing, and\naccessibility technologies, enabling them to produce high-\nquality video content with minimal resources.\nHowever, these advancements are accompanied by ethi-\ncal challenges, particularly the potential misuse of text-to-\nvideo technology. One primary concern is the generation\nof realistic but deceptive content, such as deepfakes, which\ncould be used to propagate misinformation, defame indi-\nviduals, or manipulate public opinion.\nFurthermore, the\nautomated curation pipeline might inadvertently propagate\nharmful stereotypes or biases present in training datasets,\nraising concerns about unintended discrimination or harm.\nOn a broader societal level, text-to-video technology\ncould disrupt traditional media production industries, po-\ntentially displacing jobs in areas such as video editing, an-\nimation, and scriptwriting. Moreover, the increased avail-\nability of hyper-realistic AI-generated content may blur the\nlines between authentic and synthetic media, challenging\nthe public’s ability to discern credible sources. Addition-\nally, access to such technologies could exacerbate existing\ndigital divides if it remains available only to resource-rich\norganizations.\nTo address these challenges, we propose integrating\ntransparency mechanisms, such as metadata tagging to\nclearly identify AI-generated content, and robust dataset\ncuration to mitigate bias and harmful outputs. Collabora-\ntion with policymakers and industry stakeholders is crucial\nto establish ethical guidelines and regulatory frameworks.\nFurthermore, access to training resources and educational\nmaterials could ensure equitable opportunities for all users,\nminimizing potential digital divides.\nGenerative AI technologies demand continuous ethical\noversight. As text-to-video models evolve, it is imperative\nto regularly evaluate their societal impact and refine safe-\nguards to prevent misuse. This responsibility extends be-\nyond researchers to involve multidisciplinary efforts, fos-\ntering an ecosystem where generative AI benefits society\nwhile minimizing harm.\nB. Limitations\nWhile our proposed approach demonstrates significant ad-\nvancements in text-to-video generation, several limitations\npersist. Addressing these challenges presents key oppor-\ntunities for future research and development. Below, we\noutline the main limitations and potential solutions:\nComputational Resources.\nDespite the efficiency of\nour training strategy, large-scale text-to-video models still\nrequire substantial computational resources.\nThis con-\nstraint may restrict the accessibility of such technologies\nto resource-rich organizations, especially during the ini-\ntial training phases.\nFuture work should explore further\noptimizations to make these models more accessible in\nresource-constrained environments.\nDataset Quality and Diversity. The performance of our\nmodel is closely tied to the quality and diversity of the\ntraining dataset. While significant effort was made to cu-\nrate a representative dataset, certain domains or cultural nu-\nances may remain underrepresented, limiting the model’s\nability to generate contextually accurate or culturally sensi-\ntive videos. Expanding and refining the dataset to cover a\nbroader range of contexts is a critical area for improvement.\nTemporal Consistency and High-Resolution Generation.\nThe model occasionally struggles to maintain temporal con-\nsistency in complex or fast-paced video sequences. Addi-\ntionally, generating high-resolution videos at scale presents\nchallenges due to the trade-off between computational effi-\nciency and output quality. These limitations highlight the\nneed for advanced temporal modeling techniques and op-\ntimization strategies to enhance video resolution without\ncompromising performance.\nSemantic Accuracy. The model’s performance with am-\nbiguous or abstract text prompts is inconsistent, resulting\nin videos that may lack semantic precision. Improving the\ntext-video alignment mechanisms and developing more ro-\nbust embeddings could help address this issue.\nModel Interpretability. The black-box nature of deep gen-\nerative models limits understanding of how specific features\nor patterns influence generated video content.\nDevelop-\ning interpretable architectures or incorporating explainabil-\nity into the pipeline remains an open area of exploration.\nEthical and Regulatory Considerations. Although safe-\nguards were proposed to mitigate misuse, these measures\nare not foolproof and depend on downstream enforcement\nmechanisms. The rapidly evolving landscape of ethical and\nregulatory standards for generative AI will require periodic\nupdates to the pipeline to ensure compliance and responsi-\nble use.\nC. CFC-VIDS-1M Details\nC.1. Coarse-level Curation Details\nVideo Quality. To assess the quality of a video clip, we\nsample three frames: the start frame (fstart), the middle\nframe (fmid), and the end frame (fend). For each frame, an\naesthetic score is computed based on the LAION Aesthetic\nv2 model [34], denoted as Sa\nstart, Sa\nmid, and Sa\nend. The over-\nall Video Quality Score is calculated as the average of these\n12\n\n\nthree scores:\nSquality = Sa\nstart + Sa\nmid + Sa\nend\n3\n.\nOCR Score. To evaluate the presence of textual content,\nwe sample three frames: the start frame (fstart), the middle\nframe (fmid), and the end frame (fend). An OCR detection\nmodel [24] is used to count the number of detected text re-\ngions in each frame, denoted as Sc\nstart, Sc\nmid, and Sc\nend. The\nfinal OCR Score is the average of these counts:\nSocr = Sc\nstart + Sc\nmid + Sc\nend\n3\n.\nTemporal Consistency. Temporal consistency measures\nthe similarity of visual features between the start frame\n(fstart) and the end frame (fend).\nVisual features are ex-\ntracted using the CLIP image encoder [30], denoted as\nϕ(fstart) and ϕ(fend). The Temporal Consistency Score\nis computed as the cosine similarity between these feature\nvectors:\nStc = sim(ϕ(fstart), ϕ(fend)),\nwhere sim denotes cosine similarity.\nMotion Score. To assess motion in the video, we sample\nthree frames: the start frame (fstart), the middle frame (fmid),\nand the end frame (fend). An optical flow model [39] is used\nto compute motion flow scores for the transitions from start\nto middle (mstart→mid) and middle to end (mmid→end). The\nfinal Motion Score is the average of these flow scores:\nSm = mstart→mid + mmid→end\n2\n.\nCategory.\nOur proposed method addresses the task of\nzero-shot video classification by leveraging CLIP [30], a\npre-trained vision-language model. The approach consists\nof three key steps: sampling representative frames from\nthe video, extracting visual features using the CLIP im-\nage encoder, and performing similarity-based classification\nagainst pre-defined class tags.\nGiven a video V and a set of pre-defined class tags\n{c1, c2, . . . , cn}, the goal is to assign V to the most semanti-\ncally relevant class tag. Specifically, the class tags used are:\nPeople, Animal, Plants, Architecture, Food, Vehicles, Nat-\nural Scenery, Urban landscape, Ocean, Outer space, Video\ngame, 2D cartoon, 3D cartoon, Technology. Unlike con-\nventional methods requiring labeled datasets and extensive\ntraining, our approach utilizes CLIP’s shared image-text\nembedding space to perform classification in a zero-shot\nsetting.\n• Frame Sampling: To represent the video, we sample\nthree key frames: the start frame (fstart), the middle frame\n(fmid), and the end frame (fend). These frames capture\nthe video’s temporal structure and provide a concise sum-\nmary of its content.\n• Class Tag Conversion: Each class tag ci is converted\ninto a natural language prompt pi, such as ”a photo of a\nanimal” for the tag ”animal.” These prompts align with\nthe training paradigm of CLIP’s text encoder, which was\ntrained on descriptive textual data.\n• Frame Features: Each sampled frame {fstart, fmid, fend}\nis passed through CLIP’s image encoder, generating vi-\nsual embeddings {ϕ(fstart), ϕ(fmid), ϕ(fend)}, where ϕ(·)\nrepresents the CLIP image encoding function.\n• Mean Frame Embedding: To aggregate information\nacross frames, we compute the mean embedding of the\nthree frame features, producing a single feature vector\nthat represents the video:\nϕ(V ) = ϕ(fstart) + ϕ(fmid) + ϕ(fend)\n3\n.\nTo classify the video, the mean video embedding ϕ(V )\nis compared to text embeddings {et1, et2, . . . , etn}, which\nare derived by encoding the prompts {p1, p2, . . . , pn} using\nCLIP’s text encoder. Cosine similarity is used to measure\nthe alignment between the video and each class tag:\nSj =\nϕ(V ) · etj\n∥ϕ(V )∥∥etj∥,\nj ∈{1, . . . , n}.\nThe predicted class corresponds to the prompt pk with the\nhighest similarity score:\nk = arg max\nj\nSj.\nThis approach efficiently maps the video to the closest se-\nmantic class in the shared image-text embedding space.\nC.2. Fine-Grained Curation Details\nWe propose leveraging large language models (LLMs) [12]\nfor zero-shot caption classification to address several per-\nsistent challenges in video captioning. To mitigate the high\ncomputational cost of LLM-based curation, we first employ\na CLIP-based filtering mechanism to identify and exclude\nmisaligned text-video pairs before applying LLM curation.\nThe LLM curation process targets the following key issues:\n• Frame-Level Descriptions: Captions that lack temporal\ncoherence, resulting in fragmented or inconsistent narra-\ntives across frames;\n• Scene Transition Errors:\nErrors during the pre-\nprocessing stage of scene splitting can result in incoherent\ncaptions and disjointed scene transition sequences in the\nvideo; and\n• Token Generation Failures: Repetitive content genera-\ntion, which detracts from the quality and informativeness\nof the captions.\nAs illustrated in Figure 7, we present detailed captions\nfor correct text-video pairs alongside examples of three\ncommon error cases in text-video alignment.\n13\n\n\nThe video shows a white boat moving through the water at a high speed, creating a \nlarge wake behind it. The boat is moving from the left to the right of the frame, and \nthe wake it leaves behind is white and frothy. The water is a deep blue color, and the \nsky is clear and blue. In the background, there is a hilly coastline with green \nvegetation.\ngood video\ngood caption\nThe video opens with a close-up shot of a small green frog sitting on a white surface. \nThe frog has a smooth texture and is looking directly at the camera. The next shot \nshows a brown frog being held in a person's hand. The frog has a rough texture and is \nlooking forward. The video includes text overlays that read \"THIS RARE FROG IS THE \nUNLIKELY SYMBOL OF THE BATTLE OVER ENDANGERED SPECIES\". \nscene transition \nvideo\nThe video shows a figure in a blue and black armored suit standing on a sidewalk. \nThe suit is predominantly blue with black accents and glowing light blue trim trim\ntrim trim trim trim trim trim trim trim trim trim trim trim trim trim trim trim trim\ntrim trim trim trim trim trim trim trim trim trim trim ...\nrepetitive phrase \ncaption\nThis video is an animated cartoon featuring a group of cats. The setting is a green field \nwith a small pond. The first frame shows a small cat running towards the pond. The \nsecond frame shows a larger cat and a smaller cat standing by the pond. The third \nframe shows the small cat jumping into the pond. The fourth frame shows the small cat \nswimming in the pond. The fifth frame shows the small cat jumping out of the pond. \nframe-level \ncaption\nFigure 7. Examples of text-video alignment using LLM-based\nzero-shot caption classification. The figure highlights three com-\nmon error cases in video captioning: (1) frame-level descriptions\nthat lack temporal coherence; (2) scene transition errors, where\ncaptions fail to recognize changes between scenes; and (3) token\ngeneration failures, resulting in repetitive content.\nD. RACCOON Details\nD.1. Background\nDiffusion Models. Diffusion models are a class of gener-\native models that learn to produce data through an iterative\ndenoising process. These models begin with samples drawn\nfrom a predefined noise distribution and progressively re-\nfine them to generate realistic outputs. In the framework\nof Gaussian diffusion models, the forward noising process\ngradually adds noise, denoted as ϵ, to real data (x0 ∼pdata).\nThis process is mathematically expressed as:\nxt =\np\nγ(t)x0 +\np\n1 −γ(t)ϵ,\n(1)\nwhere t ∈[0, 1], and γ(t) is a noise scheduler that mono-\ntonically decreases from 1 to 0.\nThe reverse process, designed to denoise the corrupted\nsamples, reconstructs clean data by iteratively predicting\nand subtracting the added noise at each step. The learning\nobjective for this process can be formulated as:\nL(θ) = Eϵ∼N(0,I),t\nh\n∥ϵ −ϵθ(xt; t, c)∥2i\n,\n(2)\nwhere ϵθ is the neural network-based denoising model, pa-\nrameterized by θ. The term c represents input conditions,\nsuch as class labels, textual prompts, or other contextual in-\nformation.\nThese properties make diffusion models effective in var-\nious generative tasks, including text-to-image and text-to-\nvideo synthesis.\nLatent Diffusion Models (LDMs).\nDirectly processing\nhigh-resolution images and videos using raw pixel rep-\nTable 5. Training Hyperparameter Details for Raccoon.\nWe\nshow the key training configurations across the four stages of\nour pipeline. The settings include maximum resolution, duration,\nbatch size, and training steps. Stage 4 (FT) represents the fine-\ntuning phase.\nTraining Stage\nStage1\nStage2\nStage3\nStage4(FT)\nMax Resolution\n256 × 256\n256 × 256\n512 × 512\n512 × 512\nMax duration\n-\n2s\n2s\n4s\nBatch Size\n2048\n1024\n1152\n1024\nTraining Steps\n100k\n80k\n40k\n10k\nresentations is computationally intensive.\nLatent Diffu-\nsion Models (LDMs) [32] address this challenge by oper-\nating in a compact, lower-dimensional latent space derived\nfrom a Vector Quantized-Variational AutoEncoder (VQ-\nVAE) [13].\nIn this framework, a VQ-VAE consists of:\n• Encoder (E(x)): Maps an input video x ∈RT ×H×W ×3\ninto a latent representation z ∈Rt×h×w×c, where T, H,\nand W denote the temporal, height, and width dimensions\nof the video, respectively. The downsampling factors are\nfs = H/h = W/w for spatial dimensions and ft = T/t\nfor the temporal dimension.\n• Decoder (D): Reconstructs the original video ˆx from the\nlatent representation z.\nTo enhance reconstruction quality, adversarial and per-\nceptual losses—similar to those employed in VQ-GAN—\nare incorporated into the training process. Operating within\nthe latent space significantly reduces computational costs,\nmaking LDMs suitable for generating high-resolution and\ntemporally consistent video data.\nBy leveraging LDMs, the proposed text-to-video model\nachieves efficient synthesis while maintaining high fidelity\nand scalability.\nD.2. Model & Training Hyperparameters\nThe detailed configurations of the model architecture and\nthe training process, including all relevant hyperparameters,\nare provided in Table 5 and Table 6. These tables compre-\nhensively outline the parameter settings, enabling precise\nreplication of our experiments and facilitating comparisons\nwith other approaches.\nE. Additional Results\nE.1. Human Evaluation\nHuman evaluation plays a vital role in assessing text-\nto-video models due to its fairness, reliability and inter-\npretability, making it widely used in text-to-video works.\nIn light of this, we include a fine-grained and comprehen-\nsive human evaluation to thoroughly validate the effective-\nness of our method. Specifically, the evaluation is formu-\n14\n\n\nTable 6. Model Hyperparameter Details for Raccoon-1B and\nRaccoon-3B Models. We show the key architectural and train-\ning hyperparameters for the Raccoon-1B and Raccoon-3B mod-\nels. The comparison highlights the scalability of the Raccoon ar-\nchitecture, with Raccoon-3B incorporating larger dimensions and\nadditional capacity to handle more complex tasks.\nHyperparameter\nRaccoon-1B\nRaccoon-3B\nNumber of Layers\n28\n32\nAttention Heads\n16\n24\nHidden Size\n1152\n1728\nPosition Encoding\nsinusoidal\nsinusoidal\nTime Embedding Size\n6912\n10368\nWeight Decay\n0\n0\nAdam ϵ\n1e-7\n1e-7\nAdam β1\n0.9\n0.9\nAdam β2\n0.999\n0.999\nLearning Rate Decay\ncosine\ncosine\nGradient Clipping\n0.1\n0.1\nText Length\n200\n200\nTraining Precision\nbf16\nbf16\nFigure 8. Results of our fine-grained human evaluations. “TC”,\n“VF”, “IQ”, “DD”, “AC”, are abbreviations for “text consistency”,\n“video faithfulness”, “image quality”, “dynamic degree”, and “ac-\ntion continuity”, respectively.\nlated as a set of scoring tasks, where human annotators are\nrequired to respectively scoring the delegated one synthe-\nsized video considering its performance on video faithful-\nness, text consistency, action continuity, dynamic degree.\nEach criterion is scored on a scale from 1 to 5, based on\nclear and discriminative standards. Since human evalua-\ntion is consuming in both time and money, the evaluation is\nconducted on three competitive text-to-video models, two\ncompetitive baseline methods (Latte and OpenSora v1.2),\nand RACCOON. Additionally, all models involved in the\nhuman evaluation use a unified generation configuration,\nwith a resolution of 512x512 and a video duration on 2s.\nThe evaluation results is illustrated in Figure 8.\nAs can\nbe concluded, our models demonstrate remarkably compet-\nitive performance, compared to the baseline methods. Par-\nticularly, our models surpass Latte and OpenSora v1.2 on\naction continuity, dynamic degree and image quality by a\nlarge margin, exhibiting the advantages of RACCOON.\nHuman Evaluation Procedure. To secure the quality of\nhuman evaluation, we employ a series of measures such as\nprompt selection, user interface design, annotation training\nand multi-turn quality inspection. The evaluation is based\non model-synthesized videos conditioned on 200 prompts,\nwhich are balanced in topic and sourced from various pub-\nlic datasets to guarantee diversity and comprehensiveness.\nBesides, throughout of the annotation procedure, the eval-\nuation is conducted via a specifically designed user inter-\nface. As exhibited in Figure 9, the user interface presents\na video alongside its conditioned prompt to human anno-\ntators. Then users are required score the video on a scale\nfrom 1 to 5, with each score accompanied by an emoji to\nclearly convey the extent of their evaluation, along with de-\nscriptive sentences about what types of videos correspond\nto each score. Once a score is selected, it is highlighted by\na red column for emphasis. At the bottom of the user in-\nterface are nine buttons. The first five serve as indicators\nand toggles for the five scoring perspectives; they are ini-\ntially red and turn green once users complete their scoring\ntasks. The remaining four buttons facilitate the submission\nof annotation results, reporting, and navigation between\nsamples. After the preparation of data and user interface,\nwe train the human annotators about the purpose, formula-\ntion, and the principle of the evaluation and annotate several\ncases as demonstration. During this training, annotators are\nencouraged to ask questions about any unclear principles,\nwhich we address through further demonstrations and ex-\nplanations. Once trained, annotators are free to begin their\nevaluations. Throughout the annotation process, we con-\nduct multi-turn quality inspections, sharing samples that do\nnot meet our standards and providing explanations for their\nfailure.\nE.2. VBench Evaluation\nTo further verify the effectiveness of RACCOON, we also\ninclude VBench [21], a comprehensive and popular text-to-\nvideo benchmark, as a supplement to the FVD and human\nevaluations.\nThe results of VBench evaluation are illus-\ntrated in Table 7, where our models demonstrate superior\nperformance compared to baseline methods. Particularly,\nRACCOON outperforms baseline methods in terms of “Hu-\n15\n\n\nTable 7. Results of VBench [21] evaluation. We present a comparative analysis of video generation models evaluated on the VBench\nbenchmark across six key metrics: Subject Consistency, Background Consistency, Image Quality, Human Action, Scene, and Appearance\nStyle. The results highlight the performance of Latte [27], OpenSora v1.2 [49], and our proposed model (RACCOON).\nModels\nSubject\nBackground\nImage\nHuman\nScene\nAppearance\nConsistency\nConsistency\nQuality\nAction\nStyle\nLatte [27]\n88.9\n95.4\n61.9\n90.0\n36.3\n23.7\nOpenSora v1.2 [49]\n94.5\n97.9\n60.9\n85.8\n42.5\n23.9\nRACCOON\n94.7\n98.0\n62.3\n95.0\n48.7\n24.4\nNo. 143\nPrompt: A farmer, dressed in a brown jacket, blue shirt, and khaki pants, is chasing a badger across a grassy field dotted with small \nshrubs. The badger, a protected species, scurries away with its distinctive black and white stripes visible. The farmer, with a look \nof determination, follows closely behind, passing by a wooden fence and a tree in the background. \n 5. The image quality is excellent, with no blur or unclear areas. \n 4. The image quality is quite high, with only a small part of the \nvideo showing some blur or clarity issues, but objects are still easily \nrecognizable. \n 3. The image quality is average, with a significant portion of the \nvideo being blurry or unclear. Objects can be recognized, but it \nrequires some effort. \n 2. The image quality is poor, with only a small part of the video \nbeing clear, making it difficult to identify objects. \n 1.The image quality is very poor, with everything unclear and \nthe whole image looking blurred; objects can only be recognized by \nvague outlines. \nIQ: Evaluate the image quality of the video (considering blur, clarity, \nand lighting), and give it a score from 1 to 5.\nVF\nTC\nAC\nDD\nIQ\nFigure 9. Demonstration of the user interface. The use interface is specifically designed for our fine-grained human evaluations to\nlower its costs and enhance its efficiency. Each time, the user interface delegate a synthesized video along with its conditioned prompt to\nhuman annotators. Then, the human annotators are supposed to follow the guidance of the user interface and score the videos from five\naforementioned perspectives. The labels on the bottom buttons,“TC”, “VF”, “IQ”, “DD”, “AC”, represent for “text consistency”, “video\nfaithfulness”, “image quality”, “dynamic degree”, and “action continuity”, respectively.\nman Action” and “Scene” by a large margin, suggesting\nthat advantages of RACCOON in human-centric and com-\nplicated scene video generation.\nE.3. Additional Qualitative Results\nMore text-to-video examples are shown in Table 10. We se-\nlected prompts from various scenes and styles to generate\nvideos. As a result, RACCOON more faithfully follows the\nuser’s text instructions in the generated videos. Addition-\nally, RACCOON demonstrates an understanding not only of\nwhat the user specifies in the prompt but also of how these\nelements exist and interact in the physical world.\nE.4. Bad Case Analysis\nIn the analysis of our model’s limitations, we identified sev-\neral categories of Bad Cases that highlight the challenges\nin video generation. First, the limited scale of training data\nsignificantly impacts the model’s ability to generate videos\nwith diverse semantics and scenes.\nWhen encountering\nless-represented or under-sampled scenarios, the generated\nvideos often fail to capture the desired details or exhibit a\nlack of diversity. Second, data bias further exacerbates this\nissue, as overrepresented patterns in the training set lead the\nmodel to generate videos that reflect these biases while ne-\nglecting less common patterns. This results in outputs that\nmay align poorly with the intended semantics, especially in\nscenarios requiring fairness or neutrality. Finally, the gener-\nation of complex scenes, particularly those involving multi-\nple interacting objects, poses a considerable challenge. The\nmodel struggles to maintain coherence in object dynamics\nand spatial relationships, often producing artifacts or incon-\nsistencies when tasked with such scenarios.\n16\n\n\nTo address these issues, future research can focus on\nscaling up and diversifying the training data, ensuring bet-\nter representation across varied semantic and scene distri-\nbutions. Additionally, techniques such as data augmenta-\ntion or synthetic data generation could mitigate the effects\nof data bias. For complex scenes, incorporating stronger\nspatial-temporal reasoning mechanisms—such as graph-\nbased object modeling or hierarchical architectures—could\nenhance the model’s ability to capture intricate interactions\nand maintain coherence across frames. By tackling these\nchallenges, the robustness and generalization capability of\nvideo generation models can be significantly improved.\nF. Future work\nWhile this paper introduces significant advancements in\ntext-to-video generation through a novel dataset curation\npipeline and an efficient transformer-based architecture,\nseveral avenues remain open for future exploration.\nFirst,\nalthough\nCFC-VIDS-1M\nemphasizes\nhigh-\nquality video data with strong text-video alignment, its di-\nversity could be further enhanced by incorporating videos\nfrom more diverse domains, cultural contexts, and styles.\nExpanding the dataset in these directions could improve\nmodel generalization and robustness across a wider range\nof applications.\nSecond, while RACCOON demonstrates computational\nefficiency through a progressive four-stage training strat-\negy, future work could focus on optimizing resource uti-\nlization further. For example, leveraging sparse attention\nmechanisms or integrating adaptive computation methods\nmay enable scaling to larger models or datasets without pro-\nportional increases in computational costs.\nThird,\nour\ncurrent\narchitecture\nprioritizes\nspatial-\ntemporal decoupling for effective video generation, but fu-\nture research could explore joint spatial-temporal modeling\nor hybrid approaches that balance efficiency and coherence.\nAdditionally, incorporating audio-visual alignment into the\nframework could unlock possibilities for multimodal video\ngeneration, further enriching the user experience.\nFinally, as text-to-video models become more accessible,\naddressing ethical concerns such as content bias, misuse,\nand fairness remains critical. Future work could focus on\ndeveloping robust content filtering mechanisms, improving\ninterpretability, and aligning generation with ethical guide-\nlines and societal values.\nWe envision these future directions not only enhanc-\ning the technical capabilities of text-to-video generation but\nalso promoting responsible and impactful applications.\n17\n\n\nBlueberries plunging into water, surrounded by rising bubbles against a vibrant purple background.\nWaves gently flowing through a coastal landscape at sunrise, with green hills and distant mountains bathed in golden light.\nA panda exuding effortless coolness, sporting large, reflective sunglasses, and strumming bold rock riffs with energy and flair.\nA magical raccoon with glowing blue eyes pauses to inspect a shimmering, glowing mushroom in the forest, surrounded by floating orbs of light.\nA majestic lion with a golden mane carries an astronaut in a white suit across rolling sand dunes, bathed in warm desert light.\nGolden egg yolk being poured into the center of a mound of white flour.\nA close up shot of a vibrant red grapefruit slice with a juicy texture.\nFigure 10. Text-to-video showcases. The generated videos exhibit significant motion, diverse styles, and demonstrate strong temporal\nconsistency.\n18\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21314v1.pdf",
    "total_pages": 18,
    "title": "Raccoon: Multi-stage Diffusion Training with Coarse-to-Fine Curating Videos",
    "authors": [
      "Zhiyu Tan",
      "Junyan Wang",
      "Hao Yang",
      "Luozheng Qin",
      "Hesen Chen",
      "Qiang Zhou",
      "Hao Li"
    ],
    "abstract": "Text-to-video generation has demonstrated promising progress with the advent\nof diffusion models, yet existing approaches are limited by dataset quality and\ncomputational resources. To address these limitations, this paper presents a\ncomprehensive approach that advances both data curation and model design. We\nintroduce CFC-VIDS-1M, a high-quality video dataset constructed through a\nsystematic coarse-to-fine curation pipeline. The pipeline first evaluates video\nquality across multiple dimensions, followed by a fine-grained stage that\nleverages vision-language models to enhance text-video alignment and semantic\nrichness. Building upon the curated dataset's emphasis on visual quality and\ntemporal coherence, we develop RACCOON, a transformer-based architecture with\ndecoupled spatial-temporal attention mechanisms. The model is trained through a\nprogressive four-stage strategy designed to efficiently handle the complexities\nof video generation. Extensive experiments demonstrate that our integrated\napproach of high-quality data curation and efficient training strategy\ngenerates visually appealing and temporally coherent videos while maintaining\ncomputational efficiency. We will release our dataset, code, and models.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}