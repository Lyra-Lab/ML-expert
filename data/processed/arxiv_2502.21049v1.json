{
  "id": "arxiv_2502.21049v1",
  "text": "arXiv (2025)\nContents lists available at ScienceDirect\narXiv\njournal homepage: www.elsevier.com/locate/media\nSynthesizing Individualized Aging Brains in Health and Disease with Generative Models\nand Parallel Transport\nJingru Fua,∗,\nYuqi Zhenga,d,\nNeel Deye,\nDaniel Ferreirab,c,\nRodrigo Morenoa,f,∗\naDivision of Biomedical Imaging, KTH Royal Institute of Technology, Stockholm, Sweden\nbDivision of Clinical Geriatrics, Center for Alzheimer Research, Karolinska Institute, Stockholm, Sweden\ncFacultad de Ciencias de la Salud, Universidad Fernando Pessoa Canarias, Las Palmas, Spain\ndDivision of Gene Technology, KTH Royal Institute of Technology, Stockholm, Sweden\neComputer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, MA, USA\nfMedTechLabs, BioClinicum, Karolinska University Hospital, Solna, Sweden\nA R T I C L E I N F O\nArticle history:\nKeywords: Diffeomorphic Registration,\nParallel Transport, Brain Aging, Medical\nImage Generation, Alzheimer’s Disease\nA B S T R A C T\nSimulating prospective magnetic resonance imaging (MRI) scans from a given indi-\nvidual brain image is challenging, as it requires accounting for canonical changes in\naging and/or disease progression while also considering the individual brain’s current\nstatus and unique characteristics. While current deep generative models can produce\nhigh-resolution anatomically accurate templates for population-wide studies, their abil-\nity to predict future aging trajectories for individuals remains limited, particularly in\ncapturing subject-specific neuroanatomical variations over time. In this study, we in-\ntroduce Individualized Brain Synthesis (InBrainSyn), a framework for synthesizing\nhigh-resolution subject-specific longitudinal MRI scans that simulate neurodegenera-\ntion in both Alzheimer’s disease (AD) and normal aging. InBrainSyn uses a parallel\ntransport algorithm to adapt the population-level aging trajectories learned by a gener-\native deep template network, enabling individualized aging synthesis. As InBrainSyn\nuses diffeomorphic transformations to simulate aging, the synthesized images are topo-\nlogically consistent with the original anatomy by design. We evaluated InBrainSyn\nboth quantitatively and qualitatively on AD and healthy control cohorts from the Open\nAccess Series of Imaging Studies - version 3 dataset. Experimentally, InBrainSyn can\nalso model neuroanatomical transitions between normal aging and AD. An evaluation\nof an external set supports its generalizability. Overall, with only a single baseline\nscan, InBrainSyn synthesizes realistic 3D spatiotemporal T1w MRI scans, producing\npersonalized longitudinal aging trajectories. The code for InBrainSyn is available at\nhttps://github.com/Fjr9516/InBrainSyn.\n© 2025 Elsevier B. V. All rights reserved.\n∗Corresponding author\ne-mail: jingruf@kth.se (\nJingru Fu), yuqizh@kth.se (\nYuqi\nZheng), dey@csail.mit.edu (\nNeel Dey),\ndaniel.ferreira.padilla@ki.se (\nDaniel Ferreira), rodmore@kth.se\n(\nRodrigo Moreno)\n1. Introduction\nThe ability to predict future individual brain trajectories\nholds great promise for providing valuable insights to clinicians\nand researchers by providing the estimation of local volume\nchanges in brains. For example, such capabilities can formu-\nlate hypotheses on the temporal dynamics of aging and disease\n(Ziegler et al., 2012; Khanal et al., 2016). However, achiev-\narXiv:2502.21049v1  [cs.CV]  28 Feb 2025\n\n\n2\nJingru Fu et al. / arXiv (2025)\ning accurate predictions at the individual level is challenging,\ngiven substantial inter-subject variation (Xia et al., 2021). Deep\ngenerative models have recently proven effective in generat-\ning high-quality population-level spatiotemporal atlases (also\nknown as templates) for studying longitudinal advancements\nin large biomedical imaging datasets (Dalca et al., 2019a; Dey\net al., 2021). However, existing population-level solutions offer\nonly on-average trajectories of disease or aging for a given pop-\nulation and are limited in their predictive power for a specific\nindividual subject. To that end, given only a single brain scan,\nwe propose Individualized Brain Synthesis (InBrainSyn), a\nframework that predicts individualized spatiotemporal brain ag-\ning trajectories in both normal aging and disease. To do so, we\ndevelop a method that adapts a cross-sectional population-level\nspatiotemporal atlas model to individual brain scans using par-\nallel transport.\nMotivation. Although scalar values derived from MRI scans\n(e.g., cortical thickness and volume) can guide treatment in crit-\nical neurodegenerative disorders, imaging itself still serves the\nunique role of understanding properties that are not represented\nin scalar values. For example, shape changes in brain struc-\ntures often precede detectable neurodegeneration, with overt\nbrain volume reductions only manifesting in later disease stages\n(Cury et al., 2016, 2019). While large Alzheimer’s Disease\n(AD) datasets have emerged to facilitate our understanding of\ndisease progression, collecting longitudinal data presents chal-\nlenges due to logistics, expense, and the common occurrence\nof missing data (Thung et al., 2016; Pathan and Hong, 2018;\nFu et al., 2023b). In the context of AD, simulating individual-\nized brain shape changes based on healthy status is important\nbecause both AD and normal aging are associated with varied\nlevels of brain atrophy (Fjell et al., 2009; Whitwell et al., 2008;\nRaz et al., 2005; Fu et al., 2023a). Being able to predict differ-\nences according to different health statuses can help neuroradi-\nologists test hypotheses regarding localized neuronal degrada-\ntion and resultant brain shape changes. It also could serve as\n“virtual control” when drug treatment is applied.\nGenerative models for longitudinal synthesis. Several stud-\nies on AD and aging employ deformable image registration to\nquantify geometric changes through pairwise warping between\nMRI images. These methods establish one or more spatiotem-\nporal brain templates to predict the on-average trajectories of a\npopulation, studying the overall trends of the disease (Huizinga\net al., 2018; Dalca et al., 2019a; Dey et al., 2021). However,\nAD and aging are both highly heterogeneous processes with\nwide inter-individual variability (Bagarinao et al., 2022; Wal-\nhovd et al., 2005; Eavani et al., 2018; Wrigglesworth et al.,\n2023; Ferreira et al., 2017), making clear a strong need for\nindividual-level subject-specific aging models. In recent years,\ndeep generative models have been used to simulate and predict\nhuman brains at retrospective and prospective time points us-\ning existing (e.g. baseline) scans (Ravi et al., 2019, 2022; Xia\net al., 2021; Fu et al., 2023b; Rakic et al., 2020; Pombo et al.,\n2023). For instance, Ravi et al. (2019) and Ravi et al. (2022)\nintroduced models to simulate 2D and 3D brains, respectively,\nemploying generative adversarial training and a range of spa-\ntiotemporal and biologically informed constraints to ensure the\nT0\nHC\nT0\nAD\nT2\nAD\nT1\nHC\nMean geodesic trajectory \nfor normal aging\nAD \ntemplates\nSubject I\nTime\nt0\nt1\nt2\nLongitudinal intra-template registration\nMean geodesic trajectory \nfor Alzheimer’s disease\nHC \ntemplates\nI0\nInter-subject registration\nI2\nI1\nParallel transport\n)\nexp(\nAD\nv\n)\nexp( 1\nHC\nu\n)\nexp(\n2\nAD\nu\n)\nˆ\nexp( 1\nHC\nu\n)\nˆ\nexp(\n2\nAD\nu\n)\nexp( HC\nv\nFig. 1: Schematic representation of the InBrainSyn framework, demonstrating\nthe simulation of normal aging and Alzheimer’s disease progression from a\nsingle subject observation. It comprises of two main steps: 1) learning cohort-\nlevel templates T from healthy control (HC) or AD subjects, and 2) transporting\nmorphological changes from the cohort level to the subject level using parallel\ntransport. Canonical patterns from the given subject’s age t0 to age ti (i.e., i\ncan be 1 or 2 in the example, t2 > t1) for normal aging and AD trajectories\nare captured through longitudinal intra-template registration and parametrized\nas the exponential of the stationary velocity field (SVF) u (i.e., exp(ui\nHC) or\nexp(ui\nAD)). Subsequently, these patterns are parallel transported to the target\nsubject I0 along the curve from the age-matched templates T HC\n0\nand T AD\n0\nto\nthe subject, which is parameterized using SVF v obtained through inter-subject\nregistration (i.e., exp(vHC) or exp(vAD)). The transported u is indicated by ˆu =\nΠBCH(u, v), where BCH stands for Baker-Campbell-Hausdorff formula. The\npicture shows the transport of the normal aging trajectory from time t0 to t1 to\nsimulate an individual scan at t1, followed by the transport of the AD trajectory\nfrom time t0 to t2 to simulate an individual scan at t2. The solid boxes indicate\nthe given templates or subject scan, the dashed boxes indicate the simulated\nones.\nrealism of the generated images. However, their models re-\nquire fine-tuning on each new scan to obtain individualization\nafter the overall model training, which is infeasible in practice.\nMoreover, these methods require longitudinal data, the lack of\nwhich is a known issue in brain MRI datasets. Xia et al. (2021)\nproposed a brain 2D MRI simulator that does not require longi-\ntudinal data. They propose an autoencoder framework and em-\nbed health status and age information in its latent space. How-\never, these adversarial formulation-based models do not impose\nexplicit anatomical constraints, which can result in unrealistic\nstructural changes that deviate from known biological patterns.\nThe lack of control complicates the assessment of their reliabil-\nity, limiting their practical utility in longitudinal studies.\nDiffeomorphic registration for longitudinal synthesis. A dif-\nfeomorphism is a smooth, bijective, and invertible function,\nwidely used for medical image analysis and registration. In our\ncontext, diffeomorphic transformations ensure that generated\nimages reflect normal anatomical changes (e.g., tissue bound-\naries remain continuous, and distinct anatomical regions do not\nmerge or fragment). Previous work by Fu et al. (2023b) showed\nthe potential of diffeomorphic registration for brain aging sim-\nulation.\nThey used diffeomorphic registration to interpolate\nbetween two MRI scans with the maximum age gap from the\nsame individual in the dataset. However, this setup cannot han-\n\n\nJingru Fu et al. / arXiv (2025)\n3\ndle more flexible situations, such as having only one available\nscan from a subject during the inference stage. The idea of us-\ning diffeomorphic registration for brain MRI simulations is also\nshown in Rakic et al. (2020), where they simulate aging based\non input attributes and constrain the learning using diffeomor-\nphic registration. However, the success of this model relies on\nobtaining longitudinal data for training, and thus, true changes\nat the individual level are counted. A recent work Pombo et al.\n(2023) also used a similar strategy to infer counterfactual im-\nages under different conditions. It introduced diffeomorphic\ndeformations in the format of starGAN (Choi et al., 2018), a\nmodel capable of multi-domain image-to-image translation, to\nensure the anatomical plausibility of synthesis. Although their\nwork was developed and evaluated in a down-sampled space,\nthey showed the usefulness of using diffeomorphisms in simu-\nlations for medical images.\nContributions. We propose InBrainSyn, a framework that syn-\nthesizes potential brain aging trajectories in MRI in both normal\naging and disease. InBrainSyn generates personalized predic-\ntions based on a single given image while also explicitly en-\nsuring the anatomical plausibility of generated images by us-\ning diffeomorphic deformations. A schematic representation\nof the proposed framework, depicting the simulation of future\nnormal aging or AD progression based on an observed subject,\nis presented in Fig. 1. Our approach develops a deep cohort-\nlevel generative model to learn canonical normal aging and AD\nprogression trajectories using only cross-sectional data. These\npopulation-level trajectories are then used for individualized ag-\ning prediction using an efficient parallel transport algorithm.\nOur contributions enable the integration of population models\nto subject-specific brain synthesis while remaining computa-\ntionally efficient by working on the deformation field. As we\ntransport deformations from population models to individuals\nwith diffeomorphic constraints on the deformations, our pre-\ndicted trajectories are both anatomically plausible and are free\nof the intensity artifacts that commonly occur in deep genera-\ntive models.\n2. Related Work\nResearchers\noften\nleverage\nlongitudinal\nneuroimaging\ndatasets representing heterogeneous populations to investigate\nneurodegenerative diseases.\nFor example, datasets like the\nAlzheimer’s Disease Neuroimaging Initiative (ADNI) (Petersen\net al., 2010) and the Open Access Series of Imaging Stud-\nies (OASIS) (LaMontagne et al., 2019), dedicated to AD re-\nsearch, have tracked populations for decades. Through these\nexpansive datasets, researchers can compare and analyze differ-\nences across populations or sub-populations, i.e., by establish-\ning study-specific templates (Avants and Gee, 2004; Joshi et al.,\n2004; Avants et al., 2010; Dalca et al., 2019a; Dey et al., 2021).\nThe primary focus of template establishment has revolved\naround the development of image mapping algorithms (Miller\nand Younes, 2001; Thompson and Toga, 2002; Rohlfing et al.,\n2003) capable of estimating the optimal spatial transformation\nto align the structures of interest, i.e., between a fixed and mov-\ning image pair. Template creation for large-scale neuroimaging\ndata poses challenges due to the high dimensionality and vol-\nume of general neuroimaging images. Automatic image map-\nping or registration algorithms have streamlined the creation of\nbrain atlases/templates, reducing the need for extensive man-\nual intervention, such as landmark annotation (Lorenzen et al.,\n2006; Allassonni`ere et al., 2007; Wu et al., 2011). While tra-\nditional automatic image registration methods iteratively solve\noptimization problems, such as LDDMM (Beg et al., 2005) and\nSyN (Avants et al., 2008), they can be computationally expen-\nsive and slow in practice. Recent advances in unsupervised reg-\nistration networks have led to learning-based methods for gen-\nerating deformable templates (De Vos et al., 2017; Balakrishnan\net al., 2019; Kim et al., 2021a; Chen et al., 2022). For exam-\nple, Dalca et al. (2019a) introduced conditional templates us-\ning a generator linked with a DL-based registration network to\nlearn the deformable template of each (sub-)population. More\nrecent work, such as Dey et al. (2021), incorporates generative\nadversarial learning and FiLM (Perez et al., 2018) conditioning\nto estimate sharp and central spatiotemporal templates across\npopulations. While robust template creation methods exhibit\nthe potential to learn valuable representations of spatiotempo-\nral changes from a population, when the focus shifts to subject-\nspecific aging, preserving individualization remains unsolved.\nDespite the shared anatomy of the human brain, cortical fold-\ning patterns (e.g., gyri and sulci) are unique to an individual\n(Demirci et al., 2023) and require distinct modeling for esti-\nmating aging trajectories.\nExisting\nbrain\nMRI\nsimulators\nfor\nindividuals\ncan\nbe\ncategorized\ninto\ntwo\nfundamental\ntypes:\nbiophysical/biomechanics-based\nand\ndata-driven/learning-\nbased models.\nBiophysical/biomechanics-based methods\nderive brain displacement fields from prescribed atrophy\nvalues estimated by, for example, segmentation or registration.\nAtrophy values are incorporated into a strain energy function\nthat factors in material properties, loading conditions, and\nboundary constraints (Khanal et al., 2016; Silva et al., 2021).\nThis type of method mainly uses biomechanical assumptions to\nreason about changes at the image level, often necessitating the\nredesign of assumptions based on specific problems, e.g., using\ndifferent material assumptions for diseased tissues. It does not\ndirectly learn spatiotemporal neuroanatomical changes given\nobservations. Moreover, this type of method also has a high\ncomputational cost, such as in (Khanal et al., 2017), and there\nis a trade-off between image resolution, feasible computation\ntime, and computational resources. Data-driven/learning-based\nmethods leverage deep learning techniques to directly extract\ninsights from data to model organ evolution or disease pro-\ngression. Among learned frameworks, Generative Adversarial\nNetworks (GANs) (Goodfellow et al., 2014) have shown strong\nperformance for generative neuroimaging.\nSeveral GAN-based methods have been introduced for mod-\neling aging and the progression of AD (Wegmayr et al., 2019;\nBowles et al., 2018; Kim et al., 2021b; Xia et al., 2021; Ravi\net al., 2022; Pombo et al., 2023). Training GANs using 3D brain\nimages is normally challenging due to the high dimensionality\nof these images, training instability, and the sparse temporal\ndistribution of current longitudinal brain datasets (Ravi et al.,\n\n\n4\nJingru Fu et al. / arXiv (2025)\n2022). To mitigate these issues, some approaches simplify the\nproblem significantly by considering only a single 2D slice per\nsubject (Wegmayr et al., 2019; Xia et al., 2021) or downsam-\npling the original images during simulation (Ravi et al., 2022).\nHowever, these strategies may lead to a suboptimal understand-\ning of inherently 3D anatomy and not scale to true 3D neu-\nroimaging tasks. To address these limitations, Jung et al. (2021)\nintroduced a technique to synthesize high-quality 3D medical\nimages by incorporating a 3D discriminator into a conventional\n2D GAN architecture. Nevertheless, keeping the internal con-\nsistency of 3D MRI scans using this adversarial way poses a\nchallenge since it is difficult to assess explicitly and potentially\ncompromises their anatomical plausibility. More recently, Ravi\net al. (2022) introduced a 3D training consistency mechanism\nand a 3D super-resolution module, representing the state-of-\nthe-art in simulating subject-specific aging and disease progres-\nsion. To address individualization, they introduced a transfer\nlearning strategy after training, although this does not enable\nthe direct prediction of new data without fine-tuning. Another\nstrategy for ensuring individualization is using a reconstruction\nloss, as shown by Xia et al. (2021), which proposed a brain 2D\nMRI simulator that does not require longitudinal data. It uti-\nlizes an encoder-decoder framework, embedding health status\nand age information in the latent space.\nMore recently, diffusion models have shown superior perfor-\nmance over GAN-based models in generating high-quality im-\nages, yet few studies have adapted diffusion models for this task\n(Yoon et al., 2023a; Puglisi et al., 2024, 2025). To efficiently\nhandle the high-dimensional 3D nature of the problem, a more\nstraightforward approach is to use a latent diffusion model, as\ndemonstrated in Puglisi et al. (2024). In this framework, the\nfinal image is reconstructed separately after predicting a robust\nlatent space representation.\nIn addition to high computational cost and individualiza-\ntion challenges, another challenge faced by deep generative ap-\nproaches is maintaining biomedical plausibility. For example,\nRavi et al. (2022) proposed a biologically-informed constraint,\naligning MRI image intensity with a monotonically decreasing\npattern (Vemuri et al., 2010). The authors attempted to resolve\nthis problem further by using generative adversarial learning\nwith the guidance of real distribution. However, anatomical\nplausibility is not explicitly constrained in adversarial learn-\ning in that neuroanatomical topology is not necessarily pre-\nserved, an important aspect for downstream tasks like morpho-\nmetric studies (Lorenzi et al., 2015; Hadj-Hamou et al., 2016;\nSivera et al., 2020). Fu et al. (2023b) introduced a straight-\nforward method using deep diffeomorphic registration to inter-\npolate temporally intermediate MRI images between two given\nscans, explicitly enforcing anatomical plausibility. However,\nthis setting is inapplicable to scenarios where only a single scan\nis available. Recent work (Pombo et al., 2023) employed a\nsimilar strategy, integrating diffeomorphic deformations akin to\nstarGAN (Choi et al., 2018), a model capable of multi-domain\ntransfer with domain labels to ensure realism. StarGAN model\nnormally requires training on a sufficiently large dataset to en-\nsure adequate representations in each subdomain, posing prac-\ntical challenges since large training data is normally unavailable\nfor many cases. With the aforementioned concerns, we found it\nnecessary to develop a more flexible individualized brain syn-\nthesis model.\nOur work is inspired by traditional deformation-based mor-\nphometric methods that statistically identify and characterize\nstructural differences across populations or establish correla-\ntions between brain shapes using deformable registration (Ash-\nburner and Friston, 2000). After obtaining a study-specific tem-\nplate, parallel transport is normally employed to extract intra-\nsubject changes and reduce inter-subject variation into a uni-\nfied template space, facilitating subsequent group comparisons.\nConsequently, different groups can be compared and analyzed\nconsistently, irrespective of their initial poses (Younes et al.,\n2008). However, unlike conventional subject-to-template paral-\nlel transport in morphometric methods, we use parallel transport\nin a reverse way in that we aim to apply the observed canonical\ntemplate space changes to a given individual. Notably, parallel\ntransport has found extensive utility in the morphological in-\nvestigation of neurodegenerative conditions (most notably AD\n(Lorenzi et al., 2015; Hadj-Hamou et al., 2016; Sivera et al.,\n2020)), but, to our knowledge, it has not yet been used for indi-\nvidualized aging synthesis in a fast DL setting.\nConcluding our review of past work, we summarize three\ncommon challenges one must face to develop a successful brain\nMRI simulator: i) High computational cost; ii) Preservation of\nindividualization; and iii) Maintenance of anatomical plausibil-\nity. In response to these challenges, we propose the InBrain-\nSyn that effectively addresses these three concerns as detailed\nin the sections below. Our approach first utilizes a deep cohort-\nlevel template generation model to obtain canonical aging and\nAD progression trajectories. These trajectories are then trans-\nferred to individuals using parallel transport. Instead of directly\nworking on high-dimensional scans, we generate images using\na downsampled deformation field derived from a DL-based dif-\nfeomorphic registration model that maintains topological con-\nsistency.\n3. Methods\n3.1. Framework of InBrainSyn\nFig. 2 illustrates a preprocessing phase followed by the three\nmain steps of InBrainSyn.\nInitially, raw T1w MRI scans\nand clinical scores (such as Clinical Dementia Rating (CDR)\nscores) were gathered and assigned to each subject (LaMon-\ntagne et al., 2019). Subsequently, standard preprocessing proce-\ndures, as commonly delineated in previous neuroimaging stud-\nies (Balakrishnan et al., 2019; Hoffmann et al., 2022; Dey et al.,\n2021) were applied to the scans, including bias field correction,\nskull stripping, and affine standardization through FreeSurfer\n1. In the second phase, we use the diffeomorphic registration-\nbased framework of Dey et al. (2021) (here referred to as Atlas-\nGAN), which is a state-of-the-art deep deformable spatiotem-\nporal template generation model to create population-level spa-\ntiotemporal deformable templates that exhibit realistic anatom-\nical features for two target cohorts: the healthy control (HC)\n1https://surfer.nmr.mgh.harvard.edu/\n\n\nJingru Fu et al. / arXiv (2025)\n5\n1) Preprocess data\n3D T1w MRI \nCognitive Scores\nPreprocessed MRIs\nAD\nAD\nHC\nHC\nHC\nBias Correction\nSkull Stripping\nAffine Registration\n2) Learn cohort-level models\nAverage \nMRI\nSynthesized \ntemplate\nFixed \nMRI\nU-Net\n...\nU-Net\n...\nage\nDecoder\nage\nDecoder\nv\nIntegration\nSTN\nSVF\nMoved\nMRI\nTrain to match\n3) Learn individual-level transformations\nTarget ages\nLearned\nCohort-level\nDecoder\nLearned\nCohort-level\nDecoder\nSubject MRI\nI0\nLearned\nRegistration\nU-Net\nLearned\nRegistration\nU-Net\nTime Series Templates\nT0\nTs\nT0\nTs\nSeries of template-to-\ntemplate SVFs\nvT0-T1\nvT0-Ts\nvT0-T1\nvT0-Ts\nvT0-I0\ntemplate-to-subject \nSVF\nvI0-I1\nvI0-Is\nvI0-I1\nvI0-Is\nTime Series of individual SVFs\nParallel transport \nwith the pole ladder\n4) Synthesize individual time series images\nvI0-I1\nvI0-Is\nvI0-I1\nvI0-Is\nTime Series of individual SVFs\nIntegration\nSTN\nSubject MRI\nI0\nSynthesized  \nMRI scans for S\nFig. 2: InBrainSyn. (1) Preprocessing: 3D T1w MRI scans and their corresponding clinical scores were collected and processed following standard protocols and\npartitioned into healthy control (HC) and AD groups; (2) Learn cohort-level models: A cohort-level decoder and a registration U-Net are trained using AtlasGAN\nfor HC and AD cohorts to learn spatiotemporal cohort-specific neurodegeneration models, respectively; (3) Estimate subject-specific transformations: given a\nsingle scan I0 at age t0 from subject S , we first generate an age-matched template T0 and a series of templates from other target ages (e.g., from t1 to ts) using the\nlearned cohort-level decoder. We then obtain s cohort-level deformations (i.e., SVFs) and a template-to-subject SVF using the learned registration U-net. Finally,\nwe parallel transport those cohort-level deformations along the template-to-subject SVF, and the individual SVFs are obtained; (4) Synthesize subject-specific time\nseries images: The final synthesized individual scans are obtained by integrating the estimating SVFs and using the resulting displacements to warp a given MRI\nscan.\ncohort and the AD cohort. AtlasGAN incorporates a registra-\ntion sub-network, ensuring precise alignment and the genera-\ntion of diffeomorphic deformations by following the station-\nary velocity field (SVF) framework. Template synthesis is per-\nformed by a decoder that takes age and/or disease conditions\nas input. In the third phase, we propose a method to derive\nindividual-level transformations from the cohort-level cross-\nsectional deformations and the template-to-subject correspon-\ndence encoded within the template-to-subject SVF. To obtain\nthese SVFs, the learned registration U-Net within AtlasGAN is\nused. In the last phase, once the individual-level SVFs are ob-\ntained, they can be integrated and applied to individualize the\ncohort-level morphological changes over time. Further details\nregarding the three main phases are explained in subsequent\nsections.\n3.1.1. Learn Cohort-Level Template Creation and Registration\nModels\nAccurately capturing population-level voxel-wise morpho-\nlogical changes associated with normal brain aging and AD\nprogression requires consideration of both spatial (inter-subject\nvariation) and temporal (intra-subject variation) dynamics. In\nthe context of deformable templates, the ideal template is de-\nfined as an unbiased barycentric representation of the target\n(sub-)population (Avants and Gee, 2004; Joshi et al., 2004;\nAvants et al., 2010; Dey et al., 2021). From an intuitive perspec-\ntive, these templates are constructed from a reference database\nby optimizing for images that minimize the average deforma-\ntion to each subject within the specific population.\nIn this study, we use AtlasGAN (Dey et al., 2021) to gen-\nerate optimal spatiotemporal cohort-level templates with real-\nistic anatomy. Its template generator component is illustrated\nin Fig. 2 2), comprising two primary sub-networks: a template\nsynthesis decoder and a VoxelMorph-like (Dalca et al., 2019b)\nregistration sub-network. The decoder learns to map the con-\nditions (e.g., age and/or disease condition) to the residual map\nderived from an average scan obtained from the target database\nto produce the desired template. Simultaneously, the registra-\ntion sub-network warps the template to instances of the target\npopulation with a smooth, invertible, and small displacement.\nPlease refer to the original paper by Dey et al. (2021) for more\ndetails. The primary loss function of the model and important\nconsiderations when integrating this method into the InBrain-\nSyn are discussed as follows.\nDiffeomorphic deep learning-based registration:\nAs\nshown in Fig. 2 2), SVF is the output of the U-Net model.\nThe diffeomorphic displacement field ϕ(t) is then determined by\n\n\n6\nJingru Fu et al. / arXiv (2025)\nthe following Ordinary Differential Equation (ODE),\n∂ϕ(t)\n∂t\n= v(ϕ(t)),\n(1)\nwith ϕ(0) = Id, representing the initial identity transformation,\nand t representing time. The resulting diffeomorphic displace-\nment field ϕ is obtained by integrating the SVF v over the unit\ntime interval t ∈[0, 1]. Due to its efficiency, the numerical\nintegration technique of scaling and squaring (Arsigny et al.,\n2006; Ashburner, 2007) is typically used. The integration of\na stationary ODE forms a continuous one-parameter group of\ndiffeomorphisms. The SVF v can be considered as an element\nof the Lie algebra in the tangent space of a manifold, and the\ndisplacement field ϕ can be seen as a member of the Lie group\nin a topological manifold space. The displacement field ϕ is\nobtained through exponentiating the SVF v as shown in Eq. 2:\nexp(v) ≡ϕ(1) =\nZ 1\n0\nv(ϕ(t)) dt.\n(2)\nFrom the properties of the one-parameter subgroup, for any\nscalar t and t′, the property stated in Eq. 3 is satisfied:\nexp((t + t′)v) = exp(tv) ◦exp(t′v),\n(3)\nwhere ◦represents the composition map associated with the Lie\ngroup. For more details, please refer to Arsigny et al. (2006);\nBossa et al. (2007); Dalca et al. (2019b).\nLoss Functions: The generator loss function of AtlasGAN\nis defined as:\nL = LLNCC + λregReg(ϕ) + λGANLGAN,\n(4)\nwhere LLNCC is the image matching similarity term, which\nis associated with the squared localized normalized cross-\ncorrelation objective, ensuring the standardization of the im-\nage intensity in local windows (Avants et al., 2011); λGANLGAN\nis the least-squares GAN term (Mao et al., 2017), which en-\nsures the realism of the moved templates; and λregReg(ϕ) serves\nas the deformation regularization penalty, which ensures the\nsmoothness and centrality of the displacement (Dalca et al.,\n2019b). In particular, the latter comprises three terms:\nλregReg(ϕ) = λ1∥¯u∥2\n2 + λ2Σp∈Ω∥∇u(p)∥2\n2 + λ3Σp∈Ω∥u(p)∥2\n2, (5)\nwhere p represents the voxel, u represents the spatial displace-\nment that satisfies ϕ = Id+u, and ¯u denotes the moving average\nof u in a window of n updates (set to 100 in our experiments),\nthat is, ¯u = 1\nnΣp∈Ωu(p), where Ω⊂R3 is the 3D spatial domain.\nThe first and third terms in Eq. 5 are the magnitude terms that\nensure small deformations between the template and image on\nboth a population and individual level, respectively. The second\nterm is the smoothness term that ensures smooth deformations,\npenalized with diffusion-like L2 regularization, where λreg is the\nvector [λ1, λ2, λ3].\nFor the discriminator, the R1 gradient penalty proposed in\nMescheder et al. (2018) is incorporated to enhance the stability\nof GAN training, and is defined as:\nR1 = λgp\n2 Ex∼Preal[∥∇D(x)∥]2\n2,\n(6)\nwhere Preal indicates the real distribution, and ∇D(x) is the gra-\ndient of the discriminator for a specific input image x. Notably,\nthe penalty weight λgp plays a crucial role and requires careful\ntuning based on the dataset used for training.\n3.1.2. Learning Subject-Specific Aging Deformations\nAs shown in Fig. 2 3), the learned decoder and registra-\ntion networks from the previous step are used to both generate\ncohort-specific templates and extract SVFs.\nTemplate Creation Decoder: The template synthesis net-\nwork takes age ti and an average population image as inputs\nand generates the corresponding template T(ti) for each time\npoint i ∈[0, ..., s], where s represents the total number of time\npoints. In our work, we train independent networks for HC and\nAD subjects. For simplicity, T(ti) is denoted as Ti, as shown in\nFig. 2 3), which illustrates the example of synthesizing a time\nseries of templates T0, ..., Ts (represented by orange cubes in the\nfigure) given an individual MRI I0.\nRegistration U-Net: For our purposes, to derive individual-\nlevel transformations to predict subject-specific aging from the\ncohort-level registration network trained for AtlasGAN, it is\nnecessary to define both cohort-level transformations and the\ngeodesic path between the template and the subject at equiva-\nlent ages.\nTo accomplish this, cohort-level transformations are initially\nobtained using the templates estimated in the preceding step.\nFor instance, considering that an MRI scan of an individual,\ndenoted as I0, was acquired at age t0, a series of template-to-\ntemplate SVFs, represented as vT0−Ti, can be extracted:\ngθ(Ti, T0) = vT0−Ii,\n(7)\nwhere θ represents the parameters of the learned registration\nnetwork denoted as g. These SVFs correspond to the registra-\ntion between T0 and Ti, with i ranging from 1 to s to denote\nvarious target time points, as illustrated in the bottom right cor-\nner of Fig. 2 (indicated by the purple cubes).\nNext, the template-subject correspondence is estimated be-\ntween T0 and the subject MRI I0, represented as vT0−I0.\nParallel Transport with the Pole Ladder: Following the\nextraction of a series of template-to-template SVFs and a\ntemplate-to-subject SVF, the pole ladder (Lorenzi and Pennec,\n2014) is employed for obtaining the trajectory of individual\nmorphological evolution.\nWe will briefly outline the procedure for transporting a sin-\ngle deformation from the template space to the individual space\nusing the pole ladder and note that this method can be readily\nextended to scenarios involving multiple deformations. Addi-\ntionally, we emphasize the compatibility of the pole ladder with\na DL-based diffeomorphic registration setting.\nTo provide a straightforward illustration of the pole ladder’s\napplication for transporting a single deformation, let’s consider\ntwo templates as shown in the left of Fig. 3: a reference T0 and\na target Ti. T0 is chosen as the template that matches the age of\nthe given MRI scan I0. Thus, the goal of the pole ladder is to\nparallel transport the deformation between T0 and Ti to estimate\nthe SVF connecting I0 and I′\ni within the individual space. This\nSVF is used to obtain the diffeomorphic deformation field ϕ\n\n\nJingru Fu et al. / arXiv (2025)\n7\nFig. 3: Illustration of the pole ladder adapted from Lorenzi and Pennec (2014)\nfor transporting a single deformation between templates T0 and Ti to estimate\nthe stationary velocity field (SVF) connecting the reference individual image I0\nand the predicted individual image I′\ni . Steps include obtaining longitudinal SVF\nbetween templates (u), SVF between template and individual image (v), and de-\nriving diffeomorphisms for parallel transport. The resulting parallel transported\ndeformation is obtained through the conjugate actions of diffeomorphisms pa-\nrameterized by SVFs u and v. A truncated Baker-Campbell-Hausdorff (BCH)\nformula is used for computations (denotes as Π), and a scaling factor n ensures\ncomputational efficiency.\nthat is used to compute I′\ni as I′\ni = ϕ ∗I0, where ∗represents\nthe warping operator. I′\ni represents the appearance of the given\nMRI in ti −t0 years ahead (or backwards if it is negative).\nAs introduced in Section 3.1.1, these diffeomorphisms can be\nparameterized by SVFs through Lie group exponentials. Con-\nsequently, geodesic paths within the space of diffeomorphisms\ncan be defined using such one-parameter subgroup parametriza-\ntion, enabling the definition of corresponding geodesics within\nthe space of images. In this manner, for each template defor-\nmation from deformations set {vT0−Ti | i = 1, . . . , s}, the imple-\nmentation steps for the parallel transport of longitudinal defor-\nmation can be summarized as follows:\n1. Take the longitudinal SVF u = vT0−Ti between the template\nT0 and the template Ti such that Ti = exp(u) ∗T0.\n2. Take the SVF v = vT0−I0 between the template T0 and\nthe reference individual image I0. The half-space image\nis given by I 1\n2 = exp(v/2) ∗T0 = exp(−v/2) ∗I0.\n3. The diffeomorphism from the template Ti to the half-space\nimage I 1\n2 is denoted as ρ = exp(v/2) ◦exp(−u), and thus,\nthe diffeomorphism from the reference individual image I0\nto the predicted individual image I′\ni, i.e., the parallel trans-\nported longitudinal deformation is represented as Eq. 8:\nexp(Π(u, v)) = exp(v/2)◦ρ−1 = exp(v/2)◦exp(u)◦exp(−v/2).\n(8)\nAs outlined in Eq. 8, the resulting parallel transported de-\nformation, denoted as exp(Π(u)), is achieved through the con-\njugate actions associated with the diffeomorphism parameter-\nized by v/2 and u. In practice, a truncated Baker-Campbell-\nHausdorff (BCH) formula, as proposed in Bossa et al. (2007),\nis employed to approximate the logarithm of the composition\nof the diffeomorphisms, working within the Lie algebra. Addi-\ntionally, when dealing with non-infinitesimal diffeomorphisms\nduring the parallel transport process, computations need to be\nexecuted recursively to construct a successive ladder. To ac-\ncommodate this, a scaling factor denoted as n is introduced to\nensure that v/n remains sufficiently small, as shown in the right\nInput: u: the template-to-template SVF,\nv: the template-to-subject SVF\nOutput: ΠBCH(u, v): the parallel transported SVF u\nalong the SVF v\n// Scaling step:\ncompute n such that v/n\nis smaller than the voxel size in all\ndimensions\nn =\nl maxp∈Ω∥v(p)∥\nvoxelsize\nm\n;\n// ⌈·⌉is the ceiling\noperation\n// Parallel transporting step using the\nBaker-Campbell-Hausdorff (BCH) formula\nu0 = u;\nfor j ←1 to n do\nuj = uj−1 +\nh v\n2n, uj−1\ni\n+ 1\n2\nh v\n2n,\nh v\n2n, uj−1\nii\n; // [, ] is\nthe Lie bracket\nend\nΠBCH(u, v) = un;\nAlgorithm 1: Parallel transport of SVFs with the pole lad-\nder (Lorenzi and Pennec, 2014)\npart of Fig. 3. The pole ladder algorithm utilizing the BCH for-\nmula is further detailed in Algorithm 1.\nHence, by leveraging the pole ladder, the series of extracted\nlongitudinal template-to-template SVFs is parallel transported\ninto an individualized series of subject-to-subject SVFs as\nshown in Fig. 2 3) (represented by red cubes).\n3.1.3. Synthesizing Individual Time Series Images\nThe final subject-specific synthesis is achieved by individu-\nalized transformations as illustrated in Fig. 2 4). First, the indi-\nvidualized SVFs are integrated to yield a series of longitudinal\ndisplacement fields. Second, these displacement fields are then\napplied to warp the reference individual scan. This pipeline\nensures that the predicted scans possess both anatomical plausi-\nbility via diffeomorphic deformations and personalized features\nvia parallel transport.\nWe investigated two scenarios of longitudinal synthesis.\nFirst, we synthesize scans at new time points that align with an\nindividual’s current health status. These predictions can follow\neither a normal aging trajectory or an AD progression trajec-\ntory; In this case, all templates T0...Ts come from the same co-\nhort. Second, for subjects undergoing a transition from health\nto disease, we use first T0...Tk templates from the healthy co-\nhort and then Tk+1...Ts from the disease cohort. This way, we\ncan simulate the onset of AD at a specific time point k + 1.\n4. Results\n4.1. Dataset\nWe use the OASIS version 3 (OASIS-3) dataset (LaMon-\ntagne et al., 2019) as the reference database. OASIS-3 is a\nlongitudinal multimodal neuroimaging, clinical, cognitive, and\n\n\n8\nJingru Fu et al. / arXiv (2025)\nTable 1: A summary of the original OASIS-3 dataset and the number of subjects\nand scans included in our study.\n# subjects\n# T1w scans\nCollected\n1,316\n2,681\nExcluded\n1\n3\nRemained\n(HC/AD/non-AD)\n1,315\n(739/419/157)\n2,678\n(1,678/688/312)\nIncluded in this study\n(HC/AD)\n1,158\n(739/419)\n2,366\n(1,678/688)\nbiomarker dataset for normal aging and AD2. OASIS-3 has\ndata from 1,378 participants, including 755 cognitively normal\nadults and 622 individuals at various stages of cognitive decline\nranging from 42 to 95 years old. It contains over 2,000 mag-\nnetic resonance (MR) sessions and includes T1w MR scans,\namong other sequences. The data was collected using differ-\nent Siemens scanners, including both 1.5T and 3T. More infor-\nmation on the dataset can be found in the OASIS-3 image data\ndictionary3. In this study, we use T1w scans.\nData Preparation: We collected the FreeSurfer processed\nOASIS-3 dataset released in July 2022. The summary of this\ncollection is shown in Table 1, in which the number of subjects\nand scans are counted separately. There were 2,681 success-\nfully collected scans, in which two scans failed the FreeSurfer\nquality control (QC) procedure (i.e., marked as Quarantined)\nand one scan failed to define clinical dementia rating (CDR)\nscore, which, in our application, is relevant when considering\ntransition cases. Therefore, they were discarded for the study.\nWe also filtered the remaining scans according to the diagnoses\nprovided in OASIS-3 and further removed non-AD dementia\ntypes to keep the focus on AD progression in our study. For\nthis, we only chose subjects with “AD” or “DAT” (Dementia of\nthe Alzheimer’s Type) as the diagnosis provided by the dataset.\nIn this cohort, the clinical stage was defined by CDR following\nstandards, as follows: a CDR of 0 corresponds to normal cogni-\ntive function, CDR = 0.5 indicates very mild cognitive impair-\nment, CDR = 1 indicates mild dementia, CDR = 2 indicates\nmoderate dementia, and CDR = 3 indicates severe dementia.\nAccording to OASIS-3’s acquisition protocol, participants who\nreached CDR = 2 were no longer eligible for in-person assess-\nments. For this reason, subjects with CDR = 2 are scarce in this\ndataset and CDR=3 are non-existent.\nAfter curation, 2,366 scans from 1,158 subjects were in-\ncluded in this study. We define HC subjects as those with CDR\n= 0 at all visits. 1,678 scans from 739 subjects complied with\nthis requirement. In turn, the AD cohort is composed of indi-\nviduals who progressed to clinical AD dementia at some point\nduring follow-up visits but could have CDR>= 0 at the ear-\nlier MRI scanning time. While some scans of the AD group\n2https://www.oasis-brains.org/#about\n3https://sites.wustl.edu/oasisbrains/files/2024/04/\nOASIS-3_Imaging_Data_Dictionary_v2.3-a93c947a586e7367.pdf\ncan have CDR = 0, they are not mixed with the HC group since\nthese images might already show some signs of AD progression\nthat were not yet clinically signaled by the CDR test. Thus, we\nused 688 images from 419 subjects for the AD group with CDR\nscores from 0 to 2. Approximately 80% of the data was allo-\ncated to the training set, while the remaining 20% was assigned\nto the test set. Only the test set was used for the results reported\nin Sect. 4.2, while the whole database was used for the results\nreported in Sect. 4.3.\nNeuroimaging Processing: We followed a similar prepro-\ncessing protocol to Dey et al. (2021). Specifically, we used\nT1w data preprocessed with FreeSurfer (i.e., norm.mgz). These\nimages have an isotropic resolution of 1mm. FreeSurfer per-\nforms skull-stripping and bias field correction according to the\nFreeSurfer process flow4. Then, affine registration was applied\nto the image using the FreeSurfer mri vol2vol command, utiliz-\ning Talairach space encoded in talairach.xfm to Montreal Neu-\nrological Institute (MNI) 305 space. The segmentation masks\nfor each image can be obtained by SynthSeg (Billot et al., 2023)\nfor assessment purposes. We then rescaled the image intensity\nrange to [0,1] and cropped the input scan to [208, 176, 160].\nThis grid size was calculated to preserve most foreground infor-\nmation using 200 randomly selected images from the dataset.\nHyperparameter Selection: In this study, following the\nhyperparameters adopted in the previous works of Dey et al.\n(2021); Dalca et al. (2019a); Mescheder et al. (2018), the hy-\nperparameters were set as follows:\n• λGAN = 0.1,\n• λreg = [λ1, λ2, λ3] = [1, 1, 0.01],\nRegarding\nλgp,\nwe\ntested\nfour\nvalues:\n10−4, 5\n×\n10−4, 10−3, and 5 × 10−3.\nThese four values were selected\nbased on previous experience with neuroimaging datasets.\nSubsequently, the weight yielding the best sharpness of the\nsynthetic templates for the trained AtlasGAN was chosen for\nfurther experiments.\n4.2. Evaluation of Template Generation\nThree assessments were performed on template synthesis in\nterms of its quality and accuracy in representing HC and AD\ncohorts: 1) visual inspection of learned templates; 2) Entropy\nFocus Criterion (EFC) (Atkinson et al., 1997): to evaluate the\nsharpness of the synthetic templates and determine the best\nmodel used for sequencing steps; 3) Volumetric trends: to com-\npare the volumes of anatomical structures within the synthe-\nsized templates to underlying real distribution by comparing\nsegmentation-derived volumes.\nFirstly, the results of template generation are visually pre-\nsented in Fig. 4, where we show the synthesized deformable\ntemplates from 60 to 80 years old with a 5-year interval for the\nHC and AD cohorts. Specifically, the second row and fourth\n4https://surfer.nmr.mgh.harvard.edu/fswiki/\nReconAllDevTable\n\n\nJingru Fu et al. / arXiv (2025)\n9\nFig. 4: Synthetic deformable templates for Healthy Control (HC) and Alzheimer’s Disease (AD) cohorts from 60 to 90 years old with 5-year intervals from the axial\nplane at the 80th slice (the first and the third rows, respectively). The intensity difference maps for each cohort are calculated between the corresponding synthetic\ntemplate and the one at age 60 (the second and the fourth rows, respectively). The last row shows the difference between HC and AD cohorts at matched ages.\n1 × 10−4\n5 × 10−4\n1 × 10−3\n5 × 10−3\nλgp\n0.670\n0.675\n0.680\nEntropy Focus Criteria\na) EFC values for HC cohort\n1 × 10−4\n5 × 10−4\n1 × 10−3\n5 × 10−3\nλgp\n0.6600\n0.6625\n0.6650\n0.6675\n0.6700\n0.6725\nEntropy Focus Criteria\nb) EFC values for AD cohort\nFig. 5: Entropy focus criteria (EFC) values of synthetic templates from integer\nage ranging from 60 to 80, using different λgp for healthy control (HC) and\nAlzheimer’s disease (AD) cohorts. Lower EFC values indicate sharper images.\nrow of Fig. 4 illustrate the residual maps of the generated tem-\nplates, depicting the deviations between each subsequent tem-\nplate and the template corresponding to the age of 60 years old.\nThese residual maps were derived by subtracting the 60-year-\nold template from the subsequent templates. We also show the\ndifference maps between the HC and AD templates at equiva-\nlent ages in the last row.\n60\n65\n70\n75\n80\nAge\n0.2\n0.4\n0.6\n0.8\n1.0\nNumber of Voxels\n×105\na) Ventricles\nTest/HC\nTest/AD\nTemplates/HC\nTemplates/AD\n60\n65\n70\n75\n80\nAge\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nNumber of Voxels\n×104\nb) Hippocampi\nTest/HC\nTest/AD\nTemplates/HC\nTemplates/AD\nFig. 6: Volumetric trends of synthetic template segmentations for healthy con-\ntrol (HC) and Alzheimer’s disease (AD) cohorts overlaid upon the volumetric\ntrends for the underlying HC (blue) and AD (orange) test sets.\nSecondly, the individual template quality is assessed via\nEFC. AtlasGAN models trained with four different R1 gradient\npenalty weights λgp of the discriminator are obtained for each\ncohort. We used the trained decoder to synthesize the templates\nfrom integer age from 60 to 80 for each model. The results\nare shown in Fig. 5 for the two cohorts, the model employ-\ning a λgp value of 10−4 exhibit the smallest EFC values for the\n\n\n10\nJingru Fu et al. / arXiv (2025)\nlearned templates. Therefore, we choose this model to evaluate\nthe subsequent experiments.\nThirdly, it is also necessary to verify whether the templates\ncorrectly capture the morphological changes in the two cohorts;\nthus, we use a segmentation-based volumetric analysis. Synth-\nSeg (Billot et al., 2023) was employed to get the segmentation\nmasks for real MRI scans from the database and the synthetic\ntemplates. In particular, two specific brain regions—the ventri-\ncles and the hippocampi—are of greater interest in normal ag-\ning or AD studies. The results of the ventricles and hippocampi\nvolumetric count trend are shown in Fig. 6.\n4.3. Evaluation of Individual-Level Generation\nAssessments of individual-level generation focus on two as-\npects: the quality of synthetic MRI scans and the accuracy\nof modeling individual characteristics. For assessing the qual-\nity of synthetic MRI scans, six distinct and widely recognized\nsimilarity metrics were employed following Fu et al. (2023b):\nmean absolute error (MAE), structural similarity index (SSIM),\nnormalized cross-correlation (NCC), peak signal-to-noise ratio\n(PSNR), normalized Frobenius norm (NFN), and Dice score\n(DSC). For assessing the accuracy of modeling individual char-\nacteristics, we compute the mean absolute error (MAE) be-\ntween the volumes of actual follow-up scans and the generated\nbrain MRIs across five subcortical structures—hippocampi,\namygdalae, thalami, caudates, and putamina—as well as ventri-\ncles and cerebrospinal fluid (CSF) regions. The MAE between\nthe synthetic scans and the ground truth scans is estimated as:\nMAEr =\n\f\f\f\f\f\nVol(Is, r)\nVol(Is, wb) −Vol(I∗\ns, r)\nVol(I∗s, wb)\n\f\f\f\f\f ∗100\n(9)\nwhere Is represents the ground truth scan for subject s, I∗\ns is\nthe simulated scan, Vol(Is, r) denotes the estimated regional\nvolumes in mm3 for structure r, and wb refers to the whole\nbrain.\nWe also visually inspect individual time-series MRI\nscans, compared with baselines.\nBaselines: We compare InBrainSyn with two methods: a no\nparallel transport (No-PT) baseline and Brain Latent Progres-\nsion (BrLP) (Puglisi et al., 2024, 2025). The No-PT is achieved\nby registering the given scans to cohort-level templates without\nacquiring individual-level transformations (i.e., without parallel\ntransport). This setting serves as an ablation study to highlight\nthe importance of accounting for inter-subject differences in the\ngeneration process. BrLP is a spatiotemporal disease progres-\nsion model based on latent diffusion. BrLP has been reported\nto outperform previous learning-based methods (Puglisi et al.,\n2025), such as CounterSynth (Pombo et al., 2023), DANI-Net\n(Ravi et al., 2022) and Latent-SADM (Yoon et al., 2023b). We\nutilize the pre-trained models from the original study to gen-\nerate results. Since BrLP synthesizes MRI scans at 1.5 mm3\nresolution; we apply linear interpolation to obtain 1 mm3 reso-\nlution to match the resolution of real data.\nIn addition to these two methods, we compare the results\nwith two reference methods that require longitudinal data:\nSimul@trophy and Two Visits. These methods can be seen as\nupper bounds for the methods that use a single MRI volume\nas an input. Simul@trophy uses a biomechanical approach to\ngenerate the images (Khanal et al., 2017). In addition to the\ninput image, Simul@trophy requires a predefined atrophy map.\nIn our experiments, we estimate such atrophy maps to guide\nthe generation process using actual longitudinal segmentation\nmaps of CSF and brain parenchyma. We down-sample the MRI\nresolution (by a factor of 2) to achieve computationally feasi-\nble reference times and fit in the memory. Subsequently, the\nresolution was restored to its original state using linear interpo-\nlation. Two Visits is a registration-based method we introduced\nin our previous work (Fu et al., 2023b). It uses diffeomorphic\nregistration to interpolate between two MRI scans of the same\nindividual. Since it relies on both an initial and a final time\npoint, it requires longitudinal data.\n4.3.1. Evaluation on HC and AD Cohorts\nQualitative comparison: We present an example subject in\nFig. 7, comparing the generated scans among methods with real\nlongitudinal ground truth data as reference. The first row dis-\nplays the real ground truth images alongside zoomed-in regions\nto aid visualization. InBrainSyn generates the most realistic im-\nages compared to the No-PT and BrLP. These images are also\nsharper than the ones generated with Simul@trophy. Two Visits\nis the method that generates the most accurate images, which is\nexpected as it requires longitudinal data.\nIn Fig. 8, we present the volumetric trends for two specific\nbrain structures across the No-PT and four brain MRI simu-\nlators, along with the ground truth scans for a subject with\nfour real scans in OASIS-3. Notice that all three methods start\nfrom the same initial scan, which is the first available scan\nat age 51.7. As shown, InBrainSyn yields better results than\nother methods that require a single image, especially in the hip-\npocampi, and is competitive with the Two Visits method.\nQuantitative comparison: Table 2 and 3 show the results on\na large sample size (n = 1, 0525) from OASIS-3. In particular,\nTable 2 presents MAE of regional brain volumes across seven\nregions in both cohorts. Notably, Simul@trophy and Two Visits\nachieve the best or second-best performance for many regions,\nbenefiting from exposure to longitudinal images or segmenta-\ntion maps. However, in the more realistic scenario of only a sin-\ngle observation being available, InBrainSyn outperforms other\ntested single-observation methods.\nThe results of six similarity metrics are summarized in Ta-\nble 3. We observe that Two Visits and Simul@trophy, both of\nwhich leverage longitudinal data, achieve strong performance\nacross all metrics. However, Simul@trophy performs worse,\nlikely due to the downsampling effect required for computa-\ntional feasibility. In contrast, our method circumvents this lim-\nitation by operating directly on vector fields. Meanwhile, our\nproposed InBrainSyn shows competitive image-level similarity\nwhile achieving the highest DSC accuracy among methods that\nrely on a single observation.\n\n\nJingru Fu et al. / arXiv (2025)\n11\n51.7\n57.1\n61.2\n64\nAge\nGiven\nGiven\nGiven\nBrLP\nGiven\nBrLP\nGiven\nGiven\nGiven\nInBrainSyn \n(Ours)\nGiven\nInBrainSyn \n(Ours)\nReal\nReal\nGiven\nGiven\nGiven\nNo-PT\nGiven\nNo-PT\nGiven\nGiven\nGiven\nTwo Visits*\nGiven\nTwo Visits*\nGiven\nGiven\nGiven\nSimul@trophy*\n(Longitudinal)  \n(Cross-sectional) \n↑\n↓\nFig. 7: Qualitative comparison: the synthetic MRI scans for a longitudinal representative subject OASIS31167 across the No-PT and three brain MRI simulators.\nThe real scan is enclosed within the green solid box, while the synthetic scans are enclosed within the dashed box. Methods requiring longitudinal data or real\nlongitudinal segmentation maps are marked with an asterisk (*). All other competing methods rely on a single input for prediction. (For better visualization, please\nrefer to the online version.)\n52\n54\n56\n58\n60\n62\n64\nAge\n1.5\n2.0\n2.5\n3.0\n3.5\nNumber of Voxels\n×104\na) Ventricles\nGround Truth\nTwo Visits\nSimul@trophy\nInBrainSyn\nBrLP\nNo-PT\n52\n54\n56\n58\n60\n62\n64\nAge\n1.08\n1.10\n1.12\n1.14\nNumber of Voxels\n×104\nb) Hippocampi\nGround Truth\nTwo Visits\nSimul@trophy\nInBrainSyn\nBrLP\nNo-PT\nFig. 8: The volumetric trends on ventricles and hippocampi regions for syn-\nthetic MRI scans of a longitudinal representative subject OASIS31167 across\nthe No-PT and four brain MRI simulators. The ground truth is depicted as a\nsolid blue line for reference.\n4.3.2. Evaluation on Disease Transition Cases\nAD is characterized as a progressive disease, signifying that\na subject may transition from healthy to cognitively unimpaired\nto mild cognitive impairment and to AD dementia stages (Kel-\nley and Petersen, 2007). Simulating the evolution of a subject,\nespecially transitioning from cognitive normal to AD dementia,\ninvolves determining the age at which this transition occurs. To\nevaluate disease transition cases, we select subjects based on the\nfollowing criteria: i) the number of scans per subject should be\ngreater than 2; ii) the sequence should start with an HC state\n(CDR=0) and conclude with an AD state (CDR>=1). Only\nthree subjects meet these specific requirements in the OASIS-\n3 dataset: OAS30331, OAS30869 and OAS30899. We evaluate\n5Includes all follow-up scans with at least a one-year age difference per\nsubject in the dataset.\n\n\n12\nJingru Fu et al. / arXiv (2025)\nTable 2: Mean absolute error (MAE) (mean ± standard deviation) between predicted and ground truth MRI scans across brain regions, expressed as a percentage of\ntotal brain volume. Methods requiring longitudinal data or real longitudinal segmentation maps are shown as a reference and marked with an asterisk (*). The best\nresults of methods requiring a single image are highlighted in bold.\nCohort\nMethod\nRegions (MAE % (↓))\nVentricles\nHippocampi\nAmygdalae\nThalami\nCaudates\nPutamina\nCSF\nHC\n(n = 868)\nSimul@trophy∗\n0.329 (0.320)\n0.013 (0.011)\n0.010 (0.009)\n0.034 (0.025)\n0.015 (0.014)\n0.014 (0.013)\n0.569 (0.439)\nTwo Visits∗\n0.177 (0.122)\n0.017 (0.015)\n0.012 (0.011\n0.026 (0.022)\n0.022 (0.021)\n0.019 (0.016)\n0.778 (0.718)\nNo-PT\n0.622 (0.503)\n0.035 (0.027)\n0.019 (0.015)\n0.048 (0.036)\n0.071 (0.046)\n0.077 (0.047)\n1.669 (0.924)\nBrLP\n0.316 (0.333)\n0.021 (0.016)\n0.015 (0.011)\n0.041 (0.031)\n0.021 (0.016)\n0.031 (0.023)\n0.859 (0.671)\nInBrainSyn (Ours)\n0.204 (0.238)\n0.014 (0.011)\n0.008 (0.006)\n0.019 (0.017)\n0.018 (0.018)\n0.020 (0.014)\n0.636 (0.662)\nAD\n(n = 184)\nSimul@trophy∗\n0.498 (0.480)\n0.018 (0.017)\n0.011 (0.009)\n0.041 (0.037)\n0.018 (0.018)\n0.018 (0.017)\n0.538 (0.433)\nTwo Visits∗\n0.256 (0.143)\n0.022 (0.018)\n0.011 (0.011)\n0.027 (0.019)\n0.024 (0.020)\n0.023 (0.021)\n0.834 (0.756)\nNo-PT\n1.126 (0.784)\n0.046 (0.036)\n0.021 (0.017)\n0.053 (0.038)\n0.060 (0.039)\n0.070 (0.043)\n2.057 (1.070)\nBrLP\n0.544 (0.511)\n0.033 (0.025)\n0.021 (0.014)\n0.049 (0.039)\n0.021 (0.018)\n0.041 (0.027)\n1.265 (0.767)\nInBrainSyn (Ours)\n0.433 (0.459)\n0.026 (0.021)\n0.016 (0.010)\n0.049 (0.033)\n0.031 (0.021)\n0.028 (0.020)\n0.681 (0.681)\nthe generated images by comparing them to ground truth data\nacross three variants of our method and three other baselines.\nTo ensure a fair comparison, we use CDR to guide the genera-\ntion of BrLP, as it allows for disease conditioning.\nQualitative comparison : In this section, we randomly se-\nlect one subject (OAS30331) and compare baseline methods\nwith three settings of our method as shown in Fig.\n9: i)\nInBrainS yn HC: This setting only uses healthy templates to\nsimulate the subject’s evolution over the available time.\nii)\nInBrainS yn AD: In this scenario, only AD templates are em-\nployed to simulate the subject’s evolution over the available\ntime. iii) InBrainS yn Inter: This setting considers the tran-\nsition age to be when the next session results in a non-zero\nCDR score (i.e., 75.27 for the selected subject).\nIt simu-\nlates the evolution using healthy templates before this transi-\ntion age and switches to AD templates afterwards. As shown\nin the highlighted ventricular region (red arrows), the syn-\nthetic scans from InBrainS yn Inter are closer to the ground\ntruth, as disease-induced changes are accounted for by tran-\nsitioning from healthy to AD cohort templates after the sub-\nject’s conversion to AD (CDR>0) for this subject. We note that\nSimul@tropy and BrLP tend to generate blurred images, due to\nperforming simulations in a downsampled space.\nQuantitative comparison: Table 4 provides MAE of re-\ngional brain volumes across seven regions for the three sub-\njects (n = 9) transitioning from healthy to unhealthy stages\n(n = 3 for OAS30331, n = 2 for OAS30869, and n = 4 for\nOAS30899). From the table, we can observe that our proposed\nmethod achieves the smallest MAE in most regions and is com-\nparable with the methods that use longitudinal data.\nThe results of six similarity metrics are summarized in Table\n5. Our proposed InBrainSyn variants demonstrate competitive\nperformance. From Tables 4 and 5, it is not clear which of\nthe three variants of the proposed InBrainSyn method gives the\nbest results for both volume-based and image-based evaluation\nmeasures. This might be caused by the limited sample size, the\ninter-individual heterogeneity of AD, and potential trade-offs\nbetween optimal results for each evaluation strategy. However,\nwe note that all variants of InBrainSyn outperform the single-\nobservation baselines.\n4.4. Evaluation of Model Generalization\nThe design of our method allows simulating MRI scans from\nnew datasets with varying MRI protocols and contrasts with-\nout requiring retraining of the template generation model. To\ntest this potential for generalization, we use a subset of the\nADNI dataset (Petersen et al., 2010). We categorized ADNI\nsubjects into HC and AD cohorts based on PET amyloid and\ntau biomarkers. Specifically, we defined AD subjects as those\nwho tested positive for both amyloid and tau biomarkers. Tau\npositivity was determined using the [F-18] Flortaucipir (FTP)\ntracer. To determine tau positivity, we used the Standardized\nUptake Value Ratio (SUVR), a commonly used PET imaging\nmetric that quantifies the relative tracer uptake in a target re-\ngion compared to a reference region. We focused on the meta-\ntemporal region, a key area for Alzheimer’s-related tau accu-\nmulation, and used META TEMPORAL SUVR ≥1.37 as the\npositivity threshold (Meyer et al., 2020). Using this stricter se-\nlection criterion, we identified eight AD subjects who met these\nconditions, on top of the criteria established in our previous\nwork (Fu et al., 2023b). To maintain balance, we also randomly\nselected eight HC subjects, forming two separate cohorts for\nevaluation. This resulted in 10 MRI scans to simulate for the\nAD cohort and 25 scans for the HC cohort.\nFor comparison, we evaluated four competing methods as de-\nscribed in Section 4.3, including two reference methods that\nutilize longitudinal data. Notably, among the baselines, BrLP is\npartially trained on ADNI, which gives it an inherent advantage\nin this experiment. While we include BrLP for completeness, it\nis important to recognize that its results do not reflect true gen-\neralization to new datasets. The results are presented in Table 6.\nAs in previous experiments, Two Visits consistently yields good\n\n\nJingru Fu et al. / arXiv (2025)\n13\nTable 3: Evaluation of six similarity metrics (mean ± standard deviation) for cohort-level longitudinal predictions. Methods requiring longitudinal data or real\nlongitudinal segmentation maps are shown as a reference and marked with an asterisk (*). The best results of the methods requiring a single image are highlighted\nin bold.\nCohort\nMethod\nNFN (↓)\nMAE (↓)\nPSNR (↑)\nSSIM (↑)\nNCC (↑)\nDSC (↑)\nHC\n(n = 868)\nSimul@trophy∗\n0.064 (0.024)\n0.031 (0.014)\n24.45 (3.10)\n0.889 (0.057)\n0.971 (0.021)\n0.811 (0.075)\nTwo Visits∗\n0.052 (0.023)\n0.025 (0.013)\n26.53 (3.63)\n0.936 (0.043)\n0.985 (0.014)\n0.862 (0.045)\nNo-PT\n0.087 (0.016)\n0.040 (0.010)\n21.36 (1.45)\n0.800 (0.025)\n0.937 (0.012)\n0.730 (0.031)\nBrLP\n0.076 (0.016)\n0.036 (0.010)\n22.51 (1.73)\n0.848 (0.036)\n0.958 (0.012)\n0.832 (0.025)\nInBrainSyn (Ours)\n0.061 (0.025)\n0.029 (0.014)\n24.97 (3.31)\n0.903 (0.055)\n0.974 (0.020)\n0.851 (0.060)\nAD\n(n = 184)\nSimul@trophy∗\n0.062 (0.024)\n0.030 (0.013)\n24.74 (3.31)\n0.893 (0.055)\n0.968 (0.022)\n0.809 (0.079)\nTwo Visits∗\n0.048 (0.021)\n0.023 (0.011)\n27.07 (3.59)\n0.944 (0.032)\n0.986 (0.011)\n0.865 (0.035)\nNo-PT\n0.087 (0.015)\n0.039 (0.008)\n21.35 (1.40)\n0.802 (0.018)\n0.929 (0.013)\n0.718 (0.040)\nBrLP\n0.077 (0.016)\n0.036 (0.009)\n22.44 (1.73)\n0.852 (0.029)\n0.954 (0.015)\n0.817 (0.041)\nInBrainSyn (Ours)\n0.061 (0.022)\n0.028 (0.012)\n24.83 (3.00)\n0.901 (0.044)\n0.970 (0.018)\n0.828 (0.046)\nresults leveraging longitudinal information. We observe that\nour single image-only method achieved the best performance\non the HC cohort, sometimes even surpassing Two Visits. In\nturn, in the AD cohort, BrLP performed better in most of the re-\ngions, as it was trained partially on ADNI, whereas our method\nwas not.\n5. Discussion\nIn this section, we discuss the results presented in Sect. 4 in\nterms of cohort-level and individual-level generation, as well as\nthe limitations of the proposed InBrainSyn method.\n5.1. Template Generation\nAs shown in Fig. 4, the constructed templates derived from\ntwo cohorts are sharp and have well-defined boundaries and ro-\nbust image contrast. Furthermore, there is a clear progressive\nenlargement in the lateral ventricles over time, which can also\nbe seen in the residual maps. Across the series of generated\ndeformable templates, residual maps, and the series of differ-\nence maps, it is evident that for both the HC cohort and the\nAD cohort, a discernible enlargement of the ventricles occurs\nas individuals age. Moreover, according to the residual maps,\nwhile both the HC cohort and the AD cohort templates follow a\nsimilar deformation pattern with increasing age, the brain mor-\nphological changes of the AD cohort are notably more complex\nand pronounced. When observed from an inter-cohort perspec-\ntive, it is evident that at each age, the ventricles of the AD co-\nhort exhibit a larger volume than those of the HC cohort. This\ndistinction is particularly pronounced in the early stages of ag-\ning, gradually diminishing as individuals grow older. This is\nlikely due to the increasing overlap between age-related and\nAD-related neurodegeneration, where both processes contribute\nto structural changes over time. These findings align with es-\ntablished medical literature concerning aging and AD (Fox and\nSchott, 2004; Fjell et al., 2009; Risacher et al., 2010).\nThe EFC results are shown in Fig. 5; we can observe that\namong models, the one employing a λgp value of 10e−4 exhibits\nthe smallest EFC values for the learned templates, indicating the\nsharpest templates. Consequently, this model was selected for\nsubsequent experiments. In Fig. 6, it is evident that during the\naging process, the brain ventricles exhibit an expanding trend,\nwhile the hippocampi display a trend of volumetric reduction.\nFor each of the two brain regions of interest, the volumetric\ntrends of the templates of both the HC cohort and the AD cohort\nare generally aligned with the volumetric trends observed in the\nreal MRI scans in the real database. Furthermore, the curves\nof templates mostly lie within the variation scope of curves of\nthe scans of the corresponding cohort. Notably, for each region,\ndiscernible distinctions emerge between the volumes of the HC\ncohort and the AD cohort, evident in both real underlying MRI\nscans and the learned deformable templates. However, such\nobservations are more pronounced at younger ages. Beyond the\nage of 80 years old, there are more drastic fluctuations in real\ndata, and the curves of the HC and AD MRI scans even overlap\nseverely. This likely arises from the intricacies of neurodegen-\nerative processes. As individuals age, the influence of AD on\nbrain morphology diminishes in comparison to the effects of\nnormal aging (Rhodius-Meester et al., 2017; Meysami et al.,\n2021; Ouyang et al., 2022; Mehta et al., 2024). This transition\nin predominant influence affects the learning of templates, as\nalso indicated by the observations from the difference maps.\n5.2. Individual-Level Image Generation\nEvaluation on HC and AD Cohorts: Fig. 7 visually illus-\ntrates the comparison results. The first row shows the ground\n\n\n14\nJingru Fu et al. / arXiv (2025)\n73.64\n75.27\n77.06\n80.39\nAge\n0\n0\n0.5\n1.0\nCDR\nReal\nInBrainSyn_Inter\nGiven\nInBrainSyn_AD\nGiven\nInBrainSyn_HC\nGiven\nSimul@trophy*\nGiven\nTwo Visits*\nGiven\nBrLP\nGiven\nGiven\nNo-PT\n(Longitudinal)  \n(Cross-sectional) \n↑\n↓\nFig. 9: Qualitative comparison: the synthetic MRI scans for a longitudinal representative subject OASIS30331 using different brain MRI simulators, including\nthree variants of our approach for simulating AD transition case. In CDR, the green solid line indicates healthy evolution, while the red dashed line indicates AD\nevolution. A ROI of the real scans are enclosed within green solid boxes, while the synthetic scans are enclosed within the dashed boxes with different colors.\nMethods requiring longitudinal data or real longitudinal segmentation maps are marked with an asterisk (*). All other competing methods rely on a single input for\nprediction. We mark a ROI with red arrows to visually highlight the age evolution in this subject. (For better visualization, please refer to the online version.)\n\n\nJingru Fu et al. / arXiv (2025)\n15\nTable 4: Mean absolute error (MAE) (mean ± standard deviation) between predicted MRI scans and ground truth scans for three subjects (n = 9) transitioning from\nhealthy to unhealthy stages, represented by the percentage of total brain volume. Methods requiring longitudinal data or real longitudinal segmentation maps are\nshown as a reference and marked with an asterisk (*). The best results of the methods requiring a single image are highlighted in bold.\nMethod\nRegions (MAE % (↓))\nVentricles\nHippocampi\nAmygdalae\nThalami\nCaudates\nPutamina\nCSF\nSimul@trophy∗\n0.580 (0.534)\n0.021 (0.016)\n0.010 (0.006)\n0.054 (0.027)\n0.015 (0.012)\n0.015 (0.015)\n0.567 (0.580)\nTwo Visits∗\n0.268 (0.156)\n0.024 (0.026)\n0.014 (0.010)\n0.022 (0.026)\n0.044 (0.032)\n0.017 (0.010)\n0.498 (0.417)\nNo-PT\n1.033 (0.705)\n0.046 (0.033)\n0.018 (0.011)\n0.086 (0.023)\n0.054 (0.042)\n0.079 (0.038)\n0.925 (0.845)\nBrLP\n0.631 (0.576)\n0.034 (0.033)\n0.013 (0.010)\n0.040 (0.043)\n0.018 (0.013)\n0.058 (0.019)\n1.581 (0.764)\nInBrainSyn HC\n0.502 (0.545)\n0.026 (0.021)\n0.014 (0.007)\n0.055 (0.021)\n0.038 (0.033)\n0.025 (0.023)\n0.964 (0.886)\nInBrainSyn AD\n0.423 (0.514)\n0.023 (0.018)\n0.013 (0.007)\n0.033 (0.016)\n0.021 (0.046)\n0.017 (0.023)\n1.109 (0.936)\nInBrainSyn Inter\n0.446 (0.383)\n0.026 (0.016)\n0.010 (0.008)\n0.028 (0.020)\n0.049 (0.050)\n0.022 (0.016)\n1.189 (0.917)\nTable 5: Evaluation of six similarity metrics (mean ± standard deviation) for three subjects (n = 9) transitioning from healthy to unhealthy stages. Methods requiring\nlongitudinal data or real longitudinal segmentation maps are shown as a reference and marked with an asterisk (*). The best results of the methods requiring a single\nimage are highlighted in bold.\nMethod\nNFN (↓)\nMAE (↓)\nPSNR (↑)\nSSIM (↑)\nNCC (↑)\nDSC (↑)\nSimul@trophy∗\n0.059 (0.015)\n0.027 (0.008)\n24.832 (2.278)\n0.898 (0.029)\n0.971 (0.012)\n0.780 (0.080)\nTwo Visits∗\n0.047 (0.013)\n0.022 (0.008)\n26.855 (2.545)\n0.942 (0.024)\n0.986 (0.007)\n0.854 (0.046)\nNo-PT\n0.087 (0.006)\n0.039 (0.005)\n21.242 (0.616)\n0.799 (0.010)\n0.925 (0.007)\n0.709 (0.035)\nBrLP\n0.075 (0.013)\n0.034 (0.009)\n22.585 (1.467)\n0.851 (0.020)\n0.952 (0.014)\n0.797 (0.044)\nInBrainSyn HC\n0.055 (0.014)\n0.025 (0.008)\n25.518 (2.247)\n0.918 (0.027)\n0.976 (0.010)\n0.828 (0.061)\nInBrainSyn AD\n0.059 (0.012)\n0.026 (0.007)\n24.798 (1.796)\n0.906 (0.024)\n0.972 (0.009)\n0.806 (0.051)\nInBrainSyn Inter\n0.061 (0.012)\n0.027 (0.007)\n24.439 (1.770)\n0.897 (0.027)\n0.968 (0.011)\n0.814 (0.056)\ntruth longitudinal MRI scans, while the following rows repre-\nsent benchmark solutions as well as the proposed InBrainSyn\nin the bottom row. As compared to the No-PT method, the\nother methods keep personalization of the input subject, while\nthe No-PT method only represents template evolutions. Our\napproach outperforms Simul@trophy and BrLP in terms of ac-\ncuracy in modeling the evolution process and provides superior\nimage resolution with reduced smoothing. While Two Visits\ngenerates scans that are most similar to real ones, it requires\nlongitudinal information. In contrast, the primary objective of\nInBrainSyn is to address scenarios where only a single visit is\navailable, providing greater flexibility in modeling individual\nlongitudinal changes. From Fig. 7, the intensity similarity and\naging progression can be assessed on a representative slice for\nthis subject. The figure shows that BrLP and Simul@trophy ap-\npear to produce blurrier scans with less noticeable changes over\ntime. To further analyze these temporal changes, Fig. 8 presents\nvolumetric trends for the ventricles and hippocampi in synthetic\nMRI scans of this subject. The results indicate that BrLP and\nSimul@trophy exhibit relatively flat volumetric trends for both\nstructures, likely due to the lower resolution of the generated\nscans, which negatively impacts the performance of the seg-\nmentation algorithm. This highlights the importance of resolu-\ntion in synthetic scans for downstream tasks such as segmenta-\ntion. The primary reason BrLP and Simul@trophy operate at\nhalf resolution (or can only be practically used at half resolu-\ntion) is the high computational and memory demands required\nfor processing 3D medical images. In contrast, InBrainSyn is\nunaffected by this limitation as it operates on half-resolution ve-\nlocity fields while allowing upsampling to full resolution with-\nout loss of much information. Typically, downsampling the ve-\nlocity field introduces significantly less loss compared to di-\nrectly downsampling images.\nWe also present six similarity metrics in Table 3. The results\nshow that the proposed InBrainSyn consistently generates high-\nquality MRI scans, outperforming both the No-PT and BrLP.\nThis further validates InBrainSyn in terms of overall synthesis\nfidelity.\nDisease Transition Cases: Fig.\n9 visually presents the\ncomparison results.\nFirstly, all three proposed settings out-\nperform Simul@trophy and BrLP in terms of superior image\nresolution with less smoothing, while also surpassing the No-\n\n\n16\nJingru Fu et al. / arXiv (2025)\nTable 6: Mean absolute error (MAE) (mean ± standard deviation) between predicted and ground truth MRI segmentations across brain regions, expressed as a\npercentage of total brain volume. The table presents the results evaluated on a subset of the ADNI dataset (external set) for both HC and AD cohorts. We selected\neight subjects to form each cohort separately. Methods requiring longitudinal data or real longitudinal segmentation maps are shown as a reference and marked with\nan asterisk (*). The best results of methods requiring a single image are highlighted in bold.\nCohort\nMethod\nRegions (MAE % (↓))\nVentricles\nHippocampi\nAmygdalae\nThalami\nCaudates\nPutamina\nCSF\nHC\n(n = 25)\nSimul@trophy∗\n0.176 (0.158)\n0.008 (0.007)\n0.006 (0.004)\n0.019 (0.011)\n0.010 (0.010)\n0.012 (0.012)\n0.459 (0.356)\nTwo Visits∗\n0.143 (0.135)\n0.008 (0.005)\n0.004 (0.004)\n0.014 (0.007)\n0.009 (0.007)\n0.010 (0.008)\n0.380 (0.347)\nNo-PT\n0.790 (0.741)\n0.039 (0.023)\n0.026 (0.017)\n0.034 (0.036)\n0.035 (0.020)\n0.073 (0.038)\n1.953 (0.525)\nBrLP\n0.236 (0.194)\n0.015 (0.010)\n0.018 (0.011)\n0.030 (0.017)\n0.017 (0.012)\n0.019 (0.016)\n0.619 (0.324)\nInBrainSyn (Ours)\n0.148 (0.116)\n0.010 (0.007)\n0.009 (0.006)\n0.011 (0.007)\n0.009 (0.006)\n0.018 (0.010)\n0.265 (0.193)\nAD\n(n = 10)\nSimul@trophy∗\n0.173 (0.096)\n0.007 (0.005)\n0.009 (0.008)\n0.019 (0.015)\n0.007 (0.005)\n0.013 (0.006)\n0.538 (0.250)\nTwo Visits∗\n0.080 (0.059)\n0.006 (0.005)\n0.009 (0.007)\n0.018 (0.011)\n0.009 (0.007)\n0.012 (0.008)\n0.326 (0.270)\nNo-PT\n1.234 (0.548)\n0.032 (0.026)\n0.028 (0.012)\n0.043 (0.051)\n0.064 (0.047)\n0.053 (0.039)\n1.675 (0.620)\nBrLP\n0.095 (0.080)\n0.016 (0.013)\n0.014 (0.012)\n0.039 (0.033)\n0.019 (0.014)\n0.026 (0.021)\n0.627 (0.486)\nInBrainSyn (Ours)\n0.102 (0.052)\n0.013 (0.012)\n0.018 (0.012)\n0.043 (0.019)\n0.028 (0.015)\n0.020 (0.009)\n0.299 (0.152)\nPT in preserving individualization. Among the three settings,\nInBrainS yn Inter provides the most accurate simulation of the\nevolution process. This is probably due to its consideration of\ndisease transition during generation. By incorporating inter-\ncohort template registration, disease-induced changes can be\nbetter captured.\nTable 4 and Table 5 present the quantitative results for sub-\njects who transitioned from cognitively normal to AD dementia.\nWe observe that InBrainSyn variants achieve the best perfor-\nmance in terms of both similarity and accuracy measures across\ndifferent regions. As expected, the regional MAE results do not\nindicate an absolute winner among the three variants for AD\nsubjects, given the high heterogeneity of the disease. However,\nsimilarity measures clearly identify InBrainSyn HC as the top-\nperforming variant. Upon manually inspecting the three sub-\njects in this experiment, we observed that the subject with the\nhighest number of scans (n=4) exhibited an atypical atrophy\npattern—the ventricular region remained relatively stable with\nminimal changes, while the hippocampal region was more af-\nfected. This may have biased the majority of cases toward a pat-\ntern more similar to the HC cohort, thereby influencing the sim-\nilarity measures in Table 5, which evaluate whole scans rather\nthan individual regions as in Table 4.\n5.3. Comparison of Inference Time and Memory Usage\nTo fairly compare the inference time of the InBrainSyn with\nother 3D brain MRI simulators, we conducted an experiment\nusing the same machine with Ubuntu 20.04.5 OS and Intel(R)\nXeon(R) Bronze 3204 CPU @ 1.90GHz with 64 GB RAM.\nWe evaluated BrLP, Simul@trophy, Two Visits, and InBrain-\nSyn under CPU-only conditions. The inference steps of the\nInBrainSyn consist of two template creations (∼26s per cre-\nation), two SVF extractions (∼18s per extraction), a parallel\ntransport (∼40s), and the integration (∼2s). In total, the In-\nBrainSyn takes around 2 minutes on the CPU to get a single 3D\nMRI simulation. If the templates are precomputed, this time is\nreduced to one minute. The inference time for Simul@trophy\nis around 7m30s, though this was measured on undersampled\nimages due to memory constraints. An issue of Simul@trophy\nis that it needs to get a prescribed atrophy map. If not avail-\nable, this map can be estimated from the segmentation differ-\nences between two longitudinal scans. BrLP was tested under\ntwo configurations: when running single-threaded, it took ap-\nproximately 27m, while in its default multi-threaded setting, it\nrequired about 23m. Two Visits is the fastest, requiring only\n18s, but it also relies on two observations to perform the in-\nference. Compared to InBrainSyn, BrLP is substantially more\ntime-consuming on CPU, even with multi-threading enabled,\ndue to its diffusion-based generative modeling approach, which\nrelies on iterative sampling (n = 10 in all our experiments).\nSimul@trophy and Two Visits both benefit from additional lon-\ngitudinal information, which is not required for InBrainSyn,\nmaking it a more flexible solution for cases where only a single\nbaseline scan is available. As a broader reference, Ravi et al.\n(2022) reported an inference time of a few minutes for a single\n3D MRI simulation using a cluster of GPUs with a total number\nof 50 NVIDIA GTX TITAN-X.\nWe also measured the peak memory usage for each method.\nBrLP had the highest memory consumption, reaching 15.7 GB\nat peak usage on the CPU (around 18 GB at peak usage on\nthe GPU). Simul@trophy used 8.3 GB, while InBrainSyn re-\nquired 4.5 GB for steps 1 and 3 combined and only 0.2 GB\nfor the parallel transport step. Two Visits had variable memory\nrequirements depending on its configuration: 8.2 GB for the\nfull pipeline and 2.8 GB when disabling stopping point search-\ning. While BrLP and Simul@trophy require significantly more\nmemory, this does not necessarily imply a higher level of com-\nputational demand.\nMemory consumption depends not only\non the nature of the task but also on implementation details\nsuch as data handling, intermediate storage, and parallelism.\n\n\nJingru Fu et al. / arXiv (2025)\n17\nInBrainSyn efficiently manages memory by leveraging diffeo-\nmorphic transformations rather than storing multiple interme-\ndiate states. In practical deployment, the choice of method de-\npends on the available computational resources and the specific\napplication. BrLP benefits from GPU acceleration, where in-\nference time significantly improves (e.g., 25s on an NVIDIA\nRTX A6000 GPU). InBrainSyn offers a balance between ef-\nficiency and memory, making it a practical solution for real-\nworld clinical applications with single-scan inference. Two Vis-\nits provides the fastest runtime but requires two observations.\nWhile memory consumption varies widely across these meth-\nods, it is not necessarily a direct indicator of efficiency. Each\napproach trades off between memory usage, processing time,\nand required input information, depending on its underlying\nmethodology and design choices.\n5.4. Limitations\nThe primary limitation encountered is the accurate model-\ning of AD progression across the cohort. Thus far, we have\ntrained the model on the entire AD cohort to derive a series of\ndeformable templates, which consists of individuals at varying\ndisease stages. These stages encompass subjects with a diag-\nnostic result of AD or DAT as recorded in the OASIS-3 datasets,\nall grouped under the AD cohort in our study. However, AD\nis known to follow diverse pathways in terms of morphologi-\ncal changes (Poulakis et al., 2022), resulting in significant het-\nerogeneity. As illustrated in Fig. 6, the real volumetric trends\nwithin the AD group exhibit substantial noise and fluctuations,\nespecially when compared to the more stable trends observed\nin the healthy cohort. Furthermore, the scarcity of data in the\nearlier age groups, such as individuals younger than 65, poses\na challenge for the template learning process within the DL\nmodel. Consequently, these factors contribute to the relatively\nflat trends observed in the learned cohort-level AD templates.\nThe second limitation is associated with disease transition\ncases, where we observed less satisfactory cortical generation\nresults when attempting to transport the SVF obtained by reg-\nistering two templates learned from different cohorts. The pri-\nmary issue arises because parallel transport assumes that the\ntrajectory being transported should capture only evolutionary\nchanges and not subject-specific differences. However, the tem-\nplate generation model is not designed to produce anatomically\nsimilar templates across different cohorts. This issue could be\nalleviated by jointly learning templates from multiple cohorts\nwhen more balanced data across cohorts becomes available.\n5.5. Future work\nWe identify several promising directions for future research.\nFirst, the generated synthetic images can be used for a vari-\nety of downstream tasks, such as enhancing data augmentation\nstrategies for deep learning models, improving longitudinal seg-\nmentation and registration, and facilitating disease progression\nmodeling in the absence of complete longitudinal data. Sec-\nond, the proposed approach can be extended to model subtype-\nspecific changes in AD by learning AD subtype templates. An\neven more direct strategy would involve transporting real indi-\nvidual disease progression patterns from well-established lon-\ngitudinal datasets. This could be achieved by developing an\neffective retrieval method that infers subject-specific changes\nbased on a single scan.\n6. Conclusions\nIn conclusion, this study addresses the challenge of synthe-\nsizing individualized high-dimensional brain MRI scans from a\nsingle scan. Current brain simulators face various challenges,\nincluding high computational cost, individualization preserva-\ntion, and explicit anatomical plausibility constraints. To over-\ncome these challenges, we introduced InBrainSyn. In this work,\nwe show that InBrainSyn efficiently synthesizes high-resolution\nlongitudinal MRI scans by combining a deep-learning-based\ntemplate creation model with a diffeomorphic deformation-\nbased parallel transport algorithm, enabling individual-level\nsynthesis. We developed our framework using T1w MRI scans\nfrom the OASIS-3 dataset and evaluated it on both OASIS-3\nand an external subset from the ADNI dataset. Our evaluations,\nencompassing quantitative and qualitative analyses, show the\neffectiveness of InBrainSyn. Our results reveal that InBrainSyn\noutperforms benchmark models in terms of image quality, and\naccuracy, particularly in the context of AD and aging. By de-\nsign, the use of diffeomorphic registration ensures the anatomi-\ncal plausibility of the generated images. This study opens new\navenues for improving the understanding of neurodegenerative\nprocesses and paves the way for more accurate and individual-\nized MRI image synthesis in the field of medical imaging.\nConflict of Interest\nDr. D. Ferreira consults for BioArctic and has received hon-\noraria from Esteve. Otherwise, the authors declare no conflict\nof interest.\nData Availability Statement\nThe data that support the findings of this study are openly\navailable in OASIS at http://doi.org/10.1101/2019.12.\n13.19014902, reference number (LaMontagne et al., 2019).\nThe source code and pretrained networks will be available at\nthe time of publication in our GitHub repository.\nFunding\nThis study has been partially funded by the Swedish Child-\nhood Cancer Foundation (Barncancerfonden; MT2019-0019,\nMT2022-0008), by Vinnova through AIDA, project ID: 2108,\nby the China Scholarship Council (CSC) for PhD studies at\nKTH Royal Institute of Technology, by Digital Futures, project\ndBrain, by the Swedish Research Council (Vetenskapsrådet,\ngrants 2022-03389, 2022-00916), MedTechLabs, the Center\nfor Innovative Medicine (CIMED, grants 20200505 and FoUI-\n988826), the regional agreement on medical training and clin-\nical research of Stockholm Region (ALF Medicine, grants\nFoUI-962240 and FoUI-987534), the Swedish Brain Foun-\ndation (Hj¨arnfonden FO2023-0261, FO2022-0175, FO2021-\n0131), the Swedish Alzheimer Foundation (Alzheimerfonden\n\n\n18\nJingru Fu et al. / arXiv (2025)\nAF-968032, AF-980580), the Swedish Dementia Foundation\n(Demensfonden), the Gamla Tj¨anarinnor Foundation, the Gun\noch Bertil Stohnes Foundation, Funding for Research from\nKarolinska Institutet, Neurofonden, and Foundation for Geri-\natric Diseases at Karolinska Institutet. The funders of the study\nhad no role in the study design nor the collection, analysis, and\ninterpretation of data, writing of the report, or decision to sub-\nmit the manuscript for publication.\nAcknowledgements\nData were provided in part by OASIS-3:\nPrincipal In-\nvestigators: T. Benzinger, D. Marcus, J. Morris; NIH P50\nAG00561, P30 NS09857781, P01 AG026276, P01 AG003991,\nR01 AG043434, UL1 TR000448, R01 EB009352. AV-45 doses\nwere provided by Avid Radiopharmaceuticals, a wholly-owned\nsubsidiary of Eli Lilly.\nReferences\nAllassonni`ere, S., Amit, Y., Trouv´e, A., 2007.\nTowards a coherent statis-\ntical framework for dense deformable template estimation.\nJournal of\nthe Royal Statistical Society Series B: Statistical Methodology 69, 3–29.\ndoi:10.1111/j.1467-9868.2007.00574.x.\nArsigny, V., Commowick, O., Pennec, X., Ayache, N., 2006. A log-Euclidean\nframework for statistics on diffeomorphisms, in: Medical Image Computing\nand Computer-Assisted Intervention–MICCAI 2006: 9th International Con-\nference, Copenhagen, Denmark, October 1-6, 2006. Proceedings, Part I 9,\nSpringer. pp. 924–931. doi:10.1007/11866565_113.\nAshburner, J., 2007. A fast diffeomorphic image registration algorithm. Neu-\nroImage 38, 95–113. doi:10.1016/j.neuroimage.2007.07.007.\nAshburner, J., Friston, K.J., 2000.\nVoxel-based morphometry–the methods.\nNeuroImage 11, 805–821. doi:10.1006/nimg.2000.0582.\nAtkinson, D., Hill, D.L., Stoyle, P.N., Summers, P.E., Keevil, S.F., 1997. Au-\ntomatic correction of motion artifacts in magnetic resonance images using\nan entropy focus criterion. IEEE Transactions on Medical Imaging 16, 903–\n910. doi:10.1109/42.650886.\nAvants, B., Gee, J.C., 2004.\nGeodesic estimation for large deformation\nanatomical shape averaging and interpolation. NeuroImage 23, S139–S150.\ndoi:10.1016/j.neuroimage.2004.07.010.\nAvants, B.B., Epstein, C.L., Grossman, M., Gee, J.C., 2008. Symmetric dif-\nfeomorphic image registration with cross-correlation: evaluating automated\nlabeling of elderly and neurodegenerative brain. Medical Image Analysis\n12, 26–41. doi:10.1016/j.media.2007.06.004.\nAvants, B.B., Tustison, N.J., Song, G., Cook, P.A., Klein, A., Gee, J.C., 2011. A\nreproducible evaluation of ants similarity metric performance in brain image\nregistration. NeuroImage 54, 2033–2044. doi:10.1016/j.neuroimage.\n2010.09.025.\nAvants, B.B., Yushkevich, P., Pluta, J., Minkoff, D., Korczykowski, M., Detre,\nJ., Gee, J.C., 2010. The optimal template effect in hippocampus studies\nof diseased populations.\nNeuroImage 49, 2457–2466.\ndoi:10.1016/j.\nneuroimage.2009.09.062.\nBagarinao, E., Watanabe, H., Maesawa, S., Kawabata, K., Hara, K., Ohdake,\nR., Ogura, A., Mori, D., Yoneyama, N., Imai, K., et al., 2022. Reserve and\nmaintenance in the aging brain: a longitudinal study of healthy older adults.\neNeuro 9. doi:10.1523/ENEURO.0455-21.2022.\nBalakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V., 2019. Vox-\nelmorph: A learning framework for deformable medical image registration.\nIEEE Transactions on Medical Imaging 38, 1788–1800. doi:10.1109/TMI.\n2019.2897538.\nBeg, M.F., Miller, M.I., Trouv´e, A., Younes, L., 2005. Computing large de-\nformation metric mappings via geodesic flows of diffeomorphisms.\nIn-\nternational Journal of Computer Vision 61, 139–157.\ndoi:10.1023/B:\nVISI.0000043755.93987.aa.\nBillot, B., Greve, D.N., Puonti, O., Thielscher, A., Van Leemput, K., Fischl,\nB., Dalca, A.V., Iglesias, J.E., 2023. Synthseg: Segmentation of brain MRI\nscans of any contrast and resolution without retraining. Medical Image Anal-\nysis 86, 102789. doi:10.1016/j.media.2023.102789.\nBossa, M., Hernandez, M., Olmos, S., 2007.\nContributions to 3D diffeo-\nmorphic atlas estimation: application to brain images, in: Medical Im-\nage Computing and Computer-Assisted Intervention–MICCAI 2007: 10th\nInternational Conference, Brisbane, Australia, October 29-November 2,\n2007, Proceedings, Part I 10, Springer. pp. 667–674.\ndoi:10.1007/\n978-3-540-75757-3_81.\nBowles, C., Gunn, R., Hammers, A., Rueckert, D., 2018. Modelling the pro-\ngression of Alzheimer’s disease in MRI using generative adversarial net-\nworks, in: SPIE Medical Imaging 2018: Image Processing, Houston, US, p.\n105741K. doi:10.1117/12.2293256.\nChen, J., Frey, E.C., He, Y., Segars, W.P., Li, Y., Du, Y., 2022. Transmorph:\nTransformer for unsupervised medical image registration. Medical Image\nAnalysis 82, 102615. doi:10.1016/j.media.2022.102615.\nChoi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J., 2018. StarGAN:\nUnified generative adversarial networks for multi-domain image-to-image\ntranslation, in: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Salt Lake City, US, June 18-22, 2018, pp.\n8789–8797. doi:10.1109/CVPR.2018.00916.\nCury, C., Durrleman, S., Cash, D.M., Lorenzi, M., Nicholas, J.M., Bocchetta,\nM., van Swieten, J.C., Borroni, B., Galimberti, D., Masellis, M., Tartaglia,\nM.C., Rowe, J.B., Graff, C., Tagliavini, F., Frisoni, G.B., Laforce, R.,\nFinger, E., de Mendonc¸a, A., Sorbi, S., Ourselin, S., Rohrer, J.D., Mo-\ndat, M., Andersson, C., Archetti, S., Arighi, A., Benussi, L., Black, S.,\nCosseddu, M., Fallstrm, M., Ferreira, C., Fenoglio, C., Fox, N., Freed-\nman, M., Fumagalli, G., Gazzina, S., Ghidoni, R., Grisoli, M., Jelic, V.,\nJiskoot, L., Keren, R., Lombardi, G., Maruta, C., Meeter, L., van Minke-\nlen, R., Nacmias, B., ijerstedt, L., Padovani, A., Panman, J., Pievani, M.,\nPolito, C., Premi, E., Prioni, S., Rademakers, R., Redaelli, V., Rogaeva, E.,\nRossi, G., Rossor, M., Scarpini, E., Tang-Wai, D., Tartaglia, C., Thonberg,\nH., Tiraboschi, P., Verdelho, A., Warren, J., 2019. Spatiotemporal analysis\nfor detection of pre-symptomatic shape changes in neurodegenerative dis-\neases: Initial application to the GENFI cohort. NeuroImage 188, 282–290.\ndoi:10.1016/j.neuroimage.2018.11.063.\nCury, C., Lorenzi, M., Cash, D., Nicholas, J.M., Routier, A., Rohrer, J.,\nOurselin, S., Durrleman, S., Modat, M., 2016. Spatio-temporal shape anal-\nysis of cross-sectional data for detection of early changes in neurodegen-\nerative disease, in: International Workshop on Spectral and Shape Analy-\nsis in Medical Imaging–SeSAMI 2016, Athens, Greece, October 21, 2016,\nSpringer. pp. 63–75. doi:10.1007/978-3-319-51237-2_6.\nDalca, A., Rakic, M., Guttag, J., Sabuncu, M., 2019a. Learning conditional\ndeformable templates with convolutional networks, in: Proceedings of the\n33rd International Conference on Neural Information Processing Systems\n(NeurIPS 2019), Vancouver, BC, Canada, December 8-14, 2019, Curran As-\nsociates, Inc.. p. 806–818. doi:10.48550/arXiv.1908.02738.\nDalca, A.V., Balakrishnan, G., Guttag, J., Sabuncu, M.R., 2019b. Unsupervised\nlearning of probabilistic diffeomorphic registration for images and surfaces.\nMedical Image Analysis 57, 226–236. doi:10.1016/j.media.2019.07.\n006.\nDe Vos, B.D., Berendsen, F.F., Viergever, M.A., Staring, M., Iˇsgum, I.,\n2017. End-to-end unsupervised deformable image registration with a con-\nvolutional neural network, in: Deep Learning in Medical Image Analy-\nsis and Multimodal Learning for Clinical Decision Support: Third Inter-\nnational Workshop, DLMIA 2017, and 7th International Workshop, ML-\nCDS 2017, Held in Conjunction with MICCAI 2017, Qu´ebec City, QC,\nCanada, September 14, Proceedings 3, Springer. pp. 204–212.\ndoi:10.\n1007/978-3-319-67558-9_24.\nDemirci, N., Jafarabadi, F., Wang, X., Wang, S., Holland, M.A., 2023. Consis-\ntency and variation in the placement of cortical folds: A perspective. Brain\nMultiphysics 5, 100080. doi:10.1016/j.brain.2023.100080.\nDey, N., Ren, M., Dalca, A.V., Gerig, G., 2021. Generative adversarial registra-\ntion for improved conditional deformable templates, in: Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), October\n11-17, 2021, pp. 3929–3941. doi:10.1109/ICCV48922.2021.00390.\nEavani, H., Habes, M., Satterthwaite, T.D., An, Y., Hsieh, M.K., Honnorat,\nN., Erus, G., Doshi, J., Ferrucci, L., Beason-Held, L.L., et al., 2018. Het-\nerogeneity of structural and functional imaging patterns of advanced brain\naging revealed via machine learning methods. Neurobiology of Aging 71,\n41–50. doi:10.1016/j.neurobiolaging.2018.06.013.\nFerreira, D., Machado, A., Molina, Y., Nieto, A., Correia, R., Westman, E.,\nBarroso, J., 2017. Cognitive variability during middle-age: possible asso-\nciation with neurodegeneration and cognitive reserve. Frontiers in Aging\nNeuroscience 9, 188. doi:10.3389/fnagi.2017.00188.\n\n\nJingru Fu et al. / arXiv (2025)\n19\nFjell, A.M., Walhovd, K.B., Fennema-Notestine, C., McEvoy, L.K., Hagler,\nD.J., Holland, D., Brewer, J.B., Dale, A.M., 2009. One-year brain atrophy\nevident in healthy aging. Journal of Neuroscience 29, 15223–15231. doi:10.\n1523/JNEUROSCI.3252-09.2009.\nFox, N.C., Schott, J.M., 2004.\nImaging cerebral atrophy:\nnormal age-\ning to Alzheimer’s disease.\nThe Lancet 363, 392–394.\ndoi:10.1016/\nS0140-6736(04)15441-X.\nFu, J., Ferreira, D., Smedby, ¨O., Moreno, R., 2023a. A deformation-based mor-\nphometry framework for disentangling Alzheimer’s disease from normal ag-\ning using learned normal aging templates. arXiv preprint arXiv:2311.08176\ndoi:10.48550/arXiv.2311.08176.\nFu, J., Tzortzakakis, A., Barroso, J., Westman, E., Ferreira, D., Moreno, R.,\n2023b.\nFast three-dimensional image generation for healthy brain aging\nusing diffeomorphic registration. Human Brain Mapping 44, 1289–1380.\ndoi:10.1002/hbm.26165.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y., 2014. Generative adversarial nets, in: Proceed-\nings of the 27th International Conference on Neural Information Processing\nSystems (NIPS), Montr´eal, Canada, December 8-11, 2014, Curran Asso-\nciates, Inc. doi:10.48550/arXiv.1406.2661.\nHadj-Hamou, M., Lorenzi, M., Ayache, N., Pennec, X., 2016. Longitudinal\nanalysis of image time series with diffeomorphic deformations: a compu-\ntational framework based on stationary velocity fields. Frontiers in Neuro-\nscience 10, 236. doi:10.3389/fnins.2016.00236.\nHoffmann, M., Billot, B., Greve, D.N., Iglesias, J.E., Fischl, B., Dalca, A.V.,\n2022.\nSynthmorph: Learning contrast-invariant registration without ac-\nquired images.\nIEEE Transactions on Medical Imaging 41, 543–558.\ndoi:10.1109/TMI.2021.3116879.\nHuizinga, W., Poot, D.H., Vernooij, M.W., Roshchupkin, G.V., Bron, E.E.,\nIkram, M.A., Rueckert, D., Niessen, W.J., Klein, S., 2018.\nA spatio-\ntemporal reference model of the aging brain.\nNeuroImage 169, 11–22.\ndoi:10.1016/j.neuroimage.2017.10.040.\nJoshi, S., Davis, B., Jomier, M., Gerig, G., 2004.\nUnbiased diffeomorphic\natlas construction for computational anatomy. NeuroImage 23, S151–S160.\ndoi:10.1016/j.neuroimage.2004.07.068.\nJung, E., Luna, M., Park, S.H., 2021. Conditional GAN with an attention-based\ngenerator and a 3D discriminator for 3D medical image generation, in: Inter-\nnational Conference on Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2021: 24th International Conference, Strasbourg,\nFrance, September 27-October 1, 2021, Proceedings, Part VII, Springer. pp.\n318–328. doi:/10.1007/978-3-030-87231-1_31.\nKelley, B.J., Petersen, R.C., 2007. Alzheimer’s disease and mild cognitive im-\npairment. Neurologic clinics 25, 577–609. doi:10.1016/j.ncl.2007.03.\n008.\nKhanal, B., Ayache, N., Pennec, X., 2017. Simulating longitudinal brain MRIs\nwith known volume changes and realistic variations in image intensity. Fron-\ntiers in Neuroscience 11, 132. doi:10.3389/fnins.2017.00132.\nKhanal, B., Lorenzi, M., Ayache, N., Pennec, X., 2016. A biophysical model\nof brain deformation to simulate and analyze longitudinal MRIs of pa-\ntients with Alzheimer’s disease. NeuroImage 134, 35–52. doi:10.1016/\nj.neuroimage.2016.03.061.\nKim, B., Kim, D.H., Park, S.H., Kim, J., Lee, J.G., Ye, J.C., 2021a.\nCyclemorph: Cycle consistent unsupervised deformable image registra-\ntion. Medical Image Analysis 71, 102036. doi:10.1016/j.media.2021.\n102036.\nKim, S.T., K¨uc¸¨ukaslan, U., Navab, N., 2021b. Longitudinal brain MR image\nmodeling using personalized memory for Alzheimer’s disease. IEEE Access\n9, 143212–143221. doi:10.1109/ACCESS.2021.3121609.\nLaMontagne, P.J., Benzinger, T.L., Morris, J.C., Keefe, S., Hornbeck, R.,\nXiong, C., Grant, E., Hassenstab, J., Moulder, K., Vlassenko, A.G., Raichle,\nM.E., Cruchaga, C., Marcus, D., 2019. OASIS-3: Longitudinal neuroimag-\ning, clinical, and cognitive dataset for normal aging and Alzheimer disease.\nmedRxiv doi:10.1101/2019.12.13.19014902.\nLorenzen, P., Prastawa, M., Davis, B., Gerig, G., Bullitt, E., Joshi, S., 2006.\nMulti-modal image set registration and atlas formation.\nMedical Image\nAnalysis 10, 440–451. doi:10.1016/j.media.2005.03.002.\nLorenzi, M., Pennec, X., 2014. Efficient parallel transport of deformations in\ntime series of images: from schild’s to pole ladder. Journal of Mathematical\nImaging and Vision 50, 5–17. doi:10.1007/s10851-013-0470-3.\nLorenzi, M., Pennec, X., Frisoni, G.B., Ayache, N., 2015. Disentangling normal\naging from Alzheimer’s disease in structural magnetic resonance images.\nNeurobiology of Aging 36, S42–S52. doi:10.1016/j.neurobiolaging.\n2014.07.046.\nMao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Paul Smolley, S., 2017. Least\nsquares generative adversarial networks, in: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), Venice, Italy, Octo-\nber 22-29, 2017, pp. 2794–2802. doi:10.1109/ICCV.2017.304.\nMehta, R.I., Keith, C.M., Teixeira, C.V.L., Worhunsky, P.D., Phelps, H.E.,\nWard, M., Miller, M., Navia, R.O., Pockl, S., Rajabalee, N., et al., 2024. The\nearly-onset Alzheimer’s disease MRI signature: a replication and extension\nanalysis in early-stage AD. Cerebral Cortex 34. doi:10.1093/cercor/\nbhae475.\nMescheder, L., Geiger, A., Nowozin, S., 2018. Which training methods for\nGANs do actually converge?, in: Proceedings of the 35th International Con-\nference on Machine Learning (ICML), Stockholm, Sweden, July 10-15,\n2018, PMLR. pp. 3481–3490. doi:10.48550/arXiv.1801.04406.\nMeyer, P.F., Binette, A.P., Gonneaud, J., Breitner, J.C., Villeneuve, S., Investi-\ngators, A., et al., 2020. Characterization of Alzheimer disease biomarker\ndiscrepancies using cerebrospinal fluid phosphorylated tau and AV1451\npositron emission tomography. JAMA neurology 77, 508–516. doi:10.\n1001/jamaneurol.2019.4749.\nMeysami, S., Raji, C.A., Merrill, D.A., Porter, V.R., Mendez, M.F., 2021.\nQuantitative MRI differences between early versus late onset Alzheimer’s\ndisease. American Journal of Alzheimer’s Disease & Other Dementias 36.\ndoi:10.1177/15333175211055325.\nMiller, M.I., Younes, L., 2001. Group actions, homeomorphisms, and match-\ning: A general framework. International Journal of Computer Vision 41,\n61–84. doi:10.1023/A:1011161132514.\nOuyang, J., Zhao, Q., Adeli, E., Zaharchuk, G., Pohl, K.M., 2022. Disentan-\ngling normal aging from severity of disease via weak supervision on lon-\ngitudinal MRI.\nIEEE Transactions on Medical Imaging 41, 2558–2569.\ndoi:10.1109/TMI.2022.3166131.\nPathan, S., Hong, Y., 2018. Predictive image regression for longitudinal studies\nwith missing data. arXiv preprint arXiv:1808.07553 doi:10.48550/arXiv.\n1808.07553.\nPerez, E., Strub, F., De Vries, H., Dumoulin, V., Courville, A., 2018. FiLM:\nVisual reasoning with a general conditioning layer, in: Proceedings of the\nAAAI conference on artificial intelligence (AAAI), New Orleans, USA,\nFebruary 2–7, 2018. doi:10.1609/aaai.v32i1.11671.\nPetersen, R.C., Aisen, P.S., Beckett, L.A., Donohue, M.C., Gamst, A.C., Har-\nvey, D.J., Jack, C.R., Jagust, W.J., Shaw, L.M., Toga, A.W., et al., 2010.\nAlzheimer’s disease neuroimaging initiative (ADNI): clinical characteriza-\ntion. Neurology 74, 201–209. doi:10.1212/WNL.0b013e3181cb3e25.\nPombo, G., Gray, R., Cardoso, M.J., Ourselin, S., Rees, G., Ashburner, J.,\nNachev, P., 2023. Equitable modelling of brain imaging by counterfactual\naugmentation with morphologically constrained 3D deep generative mod-\nels. Medical Image Analysis 84, 102723. doi:10.1016/j.media.2022.\n102723.\nPoulakis, K., Pereira, J.B., Muehlboeck, J.S., Wahlund, L.O., Smedby, ¨O.,\nVolpe, G., Masters, C.L., Ames, D., Niimi, Y., Iwatsubo, T., et al., 2022.\nMulti-cohort and longitudinal bayesian clustering study of stage and sub-\ntype in Alzheimer’s disease. Nature Communications 13, 4566. doi:10.\n1038/s41467-022-32202-6.\nPuglisi, L., Alexander, D.C., Rav`ı, D., 2024. Enhancing spatiotemporal dis-\nease progression models via latent diffusion and prior knowledge, in: Inter-\nnational Conference on Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2024: 27th International Conference, Marrakesh,\nMorocco, October 6–10, 2024, Proceedings, Part II, Springer. pp. 173–183.\ndoi:10.1007/978-3-031-72069-7_17.\nPuglisi, L., Alexander, D.C., Rav`ı, D., 2025.\nBrain latent progression:\nIndividual-based spatiotemporal disease progression on 3D brain MRIs\nvia latent diffusion.\nURL: https://arxiv.org/abs/2502.08560,\narXiv:2502.08560.\nRakic, M., Guttag, J., Dalca, A.V., 2020. Anatomical predictions using subject-\nspecific medical data, in: Medical Imaging with Deep Learning (MIDL),\nMontr´eal, Canada, July 6-9, 2020. doi:10.48550/arXiv.2006.00090.\nRavi, D., Alexander, D.C., Oxtoby, N.P., Alzheimer’s Disease Neuroimag-\ning Initiative, 2019.\nDegenerative adversarial neuroimage nets: gener-\nating images that mimic disease progression, in:\nInternational Confer-\nence on Medical Image Computing and Computer-Assisted Intervention–\nMICCAI 2019: 22nd International Conference, Shenzhen, China, October\n13–17, 2019, Proceedings, Part I, Springer. pp. 164–172. doi:10.1007/\n978-3-030-32248-9_19.\nRavi, D., Blumberg, S.B., Ingala, S., Barkhof, F., Alexander, D.C., Ox-\n\n\n20\nJingru Fu et al. / arXiv (2025)\ntoby, N.P., Alzheimer’s Disease Neuroimaging Initiative, et al., 2022. De-\ngenerative adversarial neuroimage nets for brain scan simulations: Ap-\nplication in ageing and dementia.\nMedical Image Analysis 75, 102257.\ndoi:10.1016/j.media.2021.102257.\nRaz, N., Lindenberger, U., Rodrigue, K.M., Kennedy, K.M., Head, D.,\nWilliamson, A., Dahle, C., Gerstorf, D., Acker, J.D., 2005. Regional brain\nchanges in aging healthy adults: general trends, individual differences and\nmodifiers. Cerebral Cortex 15, 1676–1689. doi:10.1093/cercor/bhi044.\nRhodius-Meester, H.F., Benedictus, M.R., Wattjes, M.P., Barkhof, F., Schel-\ntens, P., Muller, M., van der Flier, W.M., 2017. MRI visual ratings of brain\natrophy and white matter hyperintensities across the spectrum of cognitive\ndecline are differently affected by age and diagnosis. Frontiers in Aging\nNeuroscience 9, 117. doi:10.3389/fnagi.2017.00117.\nRisacher, S.L., Shen, L., West, J.D., Kim, S., McDonald, B.C., Beckett, L.A.,\nHarvey, D.J., Jack Jr, C.R., Weiner, M.W., Saykin, A.J., et al., 2010.\nLongitudinal MRI atrophy biomarkers: relationship to conversion in the\nadni cohort.\nNeurobiology of Aging 31, 1401–1418.\ndoi:10.1016/j.\nneurobiolaging.2010.04.029.\nRohlfing, T., Russakoff, D.B., Maurer, C.R., 2003. Extraction and applica-\ntion of expert priors to combine multiple segmentations of human brain tis-\nsue, in: Medical Image Computing and Computer-Assisted Intervention–\nMICCAI 2003: 6th International Conference, Montr´eal, Canada, Novem-\nber 15-18, 2003. Proceedings 6, Springer. pp. 578–585.\ndoi:10.1007/\n978-3-540-39903-2_71.\nSilva, M.D., Sudre, C.H., Garcia, K., Bass, C., Cardoso, M.J., Robinson, E.C.,\n2021. Distinguishing healthy ageing from dementia: A biomechanical sim-\nulation of brain atrophy using deep networks, in: Machine Learning in Clin-\nical Neuroimaging: 4th International Workshop, MLCN 2021, Held in Con-\njunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Pro-\nceedings 4, Springer. pp. 13–22. doi:10.1007/978-3-030-87586-2_2.\nSivera, R., Capet, N., Manera, V., Fabre, R., Lorenzi, M., Delingette, H.,\nPennec, X., Ayache, N., Robert, P., et al., 2020.\nVoxel-based assess-\nments of treatment effects on longitudinal brain changes in the multido-\nmain Alzheimer preventive trial cohort. Neurobiology of Aging 94, 50–59.\ndoi:10.1016/j.neurobiolaging.2019.11.020.\nThompson, P.M., Toga, A.W., 2002.\nA framework for computational\nanatomy. Computing and Visualization in Science 5, 13–34. doi:10.1007/\ns00791-002-0084-6.\nThung, K.H., Wee, C.Y., Yap, P.T., Shen, D., 2016.\nIdentification of pro-\ngressive mild cognitive impairment patients using incomplete longitudinal\nMRI scans. Brain Structure and Function 221, 3979–3995. doi:10.1007/\ns00429-015-1140-6.\nVemuri, P., Wiste, H., Weigand, S., Knopman, D.S., Trojanowski, J., Shaw, L.,\nBernstein, M.A., Aisen, P., Weiner, M., Petersen, R.C., et al., 2010. Serial\nMRI and CSF biomarkers in normal aging, MCI, and AD. Neurology 75,\n143–151. doi:10.1212/WNL.0b013e3181e7ca82.\nWalhovd, K.B., Fjell, A.M., Reinvang, I., Lundervold, A., Dale, A.M., Eilert-\nsen, D.E., Quinn, B.T., Salat, D., Makris, N., Fischl, B., 2005. Effects of\nage on volumes of cortex, white matter and subcortical structures. Neurobi-\nology of Aging 26, 1261–1270. doi:10.1016/j.neurobiolaging.2005.\n05.020.\nWegmayr, V., H¨orold, M., Buhmann, J.M., 2019. Generative aging of brain\nMR-images and prediction of Alzheimer progression, in: German Confer-\nence on Pattern Recognition (GCPR), Dortmund, Germany, September 10-\n13, 2019, Springer. pp. 247–260. doi:10.1007/978-3-030-33676-9_17.\nWhitwell, J.L., Jack Jr, C.R., Pankratz, V.S., Parisi, J.E., Knopman, D.S.,\nBoeve, B.F., Petersen, R.C., Dickson, D.W., Josephs, K.A., 2008. Rates\nof brain atrophy over time in autopsy-proven frontotemporal dementia\nand Alzheimer disease.\nNeuroImage 39, 1034–1040.\ndoi:10.1016/j.\nneuroimage.2007.10.001.\nWrigglesworth, J., Ryan, J., Ward, P.G., Woods, R.L., Storey, E., Egan,\nG.F., Murray, A., Espinoza, S.E., Shah, R.C., Trevaks, R.E., et al., 2023.\nHealth-related heterogeneity in brain aging and associations with longitu-\ndinal change in cognitive function. Frontiers in Aging Neuroscience 14,\n1063721. doi:10.3389/fnagi.2022.1063721.\nWu, G., Jia, H., Wang, Q., Shen, D., 2011. Sharpmean: groupwise registration\nguided by sharp mean image and tree-based registration. NeuroImage 56,\n1968–1981. doi:10.1016/j.neuroimage.2011.03.050.\nXia, T., Chartsias, A., Wang, C., Tsaftaris, S.A., Alzheimer’s Disease Neu-\nroimaging Initiative, et al., 2021. Learning to synthesise the ageing brain\nwithout longitudinal data. Medical Image Analysis 73, 102169. doi:10.\n1016/j.media.2021.102169.\nYoon, J.S., Zhang, C., Suk, H.I., Guo, J., Li, X., 2023a. Sadm: Sequence-aware\ndiffusion model for longitudinal medical image generation, in: International\nConference on Information Processing in Medical Imaging, Springer. pp.\n388–400. doi:10.1007/978-3-031-34048-2_30.\nYoon, J.S., Zhang, C., Suk, H.I., Guo, J., Li, X., 2023b. Sadm: Sequence-aware\ndiffusion model for longitudinal medical image generation, in: Frangi, A.,\nde Bruijne, M., Wassermann, D., Navab, N. (Eds.), Information Processing\nin Medical Imaging, Springer Nature Switzerland, Cham. pp. 388–400.\nYounes, L., Qiu, A., Winslow, R.L., Miller, M.I., 2008. Transport of relational\nstructures in groups of diffeomorphisms. Journal of Mathematical Imaging\nand Vision 32, 41–56. doi:10.1007/s10851-008-0074-5.\nZiegler, G., Dahnke, R., Gaser, C., Alzheimer’s Disease Neuroimaging Ini-\ntiative, 2012. Models of the aging brain structure and individual decline.\nFrontiers in Neuroinformatics 6, 3. doi:10.3389/fninf.2012.00003.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21049v1.pdf",
    "total_pages": 20,
    "title": "Synthesizing Individualized Aging Brains in Health and Disease with Generative Models and Parallel Transport",
    "authors": [
      "Jingru Fu",
      "Yuqi Zheng",
      "Neel Dey",
      "Daniel Ferreira",
      "Rodrigo Moreno"
    ],
    "abstract": "Simulating prospective magnetic resonance imaging (MRI) scans from a given\nindividual brain image is challenging, as it requires accounting for canonical\nchanges in aging and/or disease progression while also considering the\nindividual brain's current status and unique characteristics. While current\ndeep generative models can produce high-resolution anatomically accurate\ntemplates for population-wide studies, their ability to predict future aging\ntrajectories for individuals remains limited, particularly in capturing\nsubject-specific neuroanatomical variations over time. In this study, we\nintroduce Individualized Brain Synthesis (InBrainSyn), a framework for\nsynthesizing high-resolution subject-specific longitudinal MRI scans that\nsimulate neurodegeneration in both Alzheimer's disease (AD) and normal aging.\nInBrainSyn uses a parallel transport algorithm to adapt the population-level\naging trajectories learned by a generative deep template network, enabling\nindividualized aging synthesis. As InBrainSyn uses diffeomorphic\ntransformations to simulate aging, the synthesized images are topologically\nconsistent with the original anatomy by design. We evaluated InBrainSyn both\nquantitatively and qualitatively on AD and healthy control cohorts from the\nOpen Access Series of Imaging Studies - version 3 dataset. Experimentally,\nInBrainSyn can also model neuroanatomical transitions between normal aging and\nAD. An evaluation of an external set supports its generalizability. Overall,\nwith only a single baseline scan, InBrainSyn synthesizes realistic 3D\nspatiotemporal T1w MRI scans, producing personalized longitudinal aging\ntrajectories. The code for InBrainSyn is available at:\nhttps://github.com/Fjr9516/InBrainSyn.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}