{
  "id": "arxiv_2502.20018v1",
  "text": "Multi-Keypoint Affordance Representation for Functional\nDexterous Grasping\nFan Yang1,2, Dongsheng Luo1, Wenrui Chen1,2,âˆ—, Jiacheng Lin3, Junjie Cai1,\nKailun Yang1,2, Zhiyong Li1,2,3, and Yaonan Wang1,2\nAbstractâ€” Functional dexterous grasping requires precise\nhand-object interaction, going beyond simple gripping. Existing\naffordance-based methods primarily predict coarse interaction\nregions and cannot directly constrain the grasping posture,\nleading to a disconnection between visual perception and ma-\nnipulation. To address this issue, we propose a multi-keypoint\naffordance representation for functional dexterous grasping,\nwhich directly encodes task-driven grasp configurations by\nlocalizing functional contact points. Our method introduces\nContact-guided Multi-Keypoint Affordance (CMKA), leverag-\ning human grasping experience images for weak supervision\ncombined with Large Vision Models for fine affordance feature\nextraction, achieving generalization while avoiding manual key-\npoint annotations. Additionally, we present a Keypoint-based\nGrasp matrix Transformation (KGT) method, ensuring spatial\nconsistency between hand keypoints and object contact points,\nthus providing a direct link between visual perception and dex-\nterous grasping actions. Experiments on public real-world FAH\ndatasets, IsaacGym simulation, and challenging robotic tasks\ndemonstrate that our method significantly improves affordance\nlocalization accuracy, grasp consistency, and generalization\nto unseen tools and tasks, bridging the gap between visual\naffordance learning and dexterous robotic manipulation. The\nsource code and demo videos will be publicly available at\nhttps://github.com/PopeyePxx/MKA.\nI. INTRODUCTION\nFunctional dexterous grasping is a key capability for\nrobots to perform complex object manipulations based on\nhuman instructions. Unlike traditional simple grasping, it\nrequires a robotic dexterous hand to generate diverse grasp-\ning postures and make contact with different object regions\ndepending on the task. This involves intricate physical in-\nteractions between the fingers and the object. For instance,\nin the â€œHold Drillâ€ task, the robotâ€™s five fingers must firmly\ngrasp the drill head, while in the â€œPress Drillâ€ task, the index\nfinger presses the drill switch while the other four fingers\nstabilize the handle. Thus, how to infer task-relevant object\ncontact regions and grasping postures from visual perception\nis a fundamental challenge in functional dexterous grasping.\nIn the field of vision, affordance-based methods [1], [2],\n[3], [4], [5] have been widely explored to predict potential\nhuman interaction regions. Deep-learning-based approaches\nestimate heatmaps [4], [5] or segmentation masks [2], [3] to\nindicate feasible interaction areas. However, existing meth-\nods [6], [7] can only provide coarse region predictions given\n1The authors are with the School of Robotics, Hunan University, China.\n2The authors are also with the National Engineering Research Center of\nRobot Visual Perception and Control Technology, Hunan University, China.\n3The authors are with the College of Computer Science and Electronic\nEngineering, Hunan University, Changsha, China.\nâˆ—Corresponding authors: Wenrui Chen.\n(a) Existing work\n(b) Ours\nGaussian Heatmap\nSegmentation masks\nSingle approximate \narea of interaction\nNo certain \ninteraction pose\n2\n0\n1\nUnique \ninteraction pose\nThe muti-keypoint \ncorresponds to the \nkey MPC joints\n2\n0\n1\nGaussian Heatmap\nKeypoints\nor\nâ€œPressâ€\nFig. 1: Comparison between existing affordance-based grasping\nmethods and our proposed Multi-Keypoint Affordance representa-\ntion. (a) Existing methods identify only a rough interaction region,\nleading to uncertain interaction poses. (b) Our method localizes\nmultiple keypoints corresponding to dexterous hand joints, enabling\na unique and constrained grasping posture.\nan image and a task. A rough affordance map cannot specify\nthe exact interaction posture, leading to uncertainty in the\ngrasping motion and insufficient constraints for functional\ndexterous grasping, as shown in Fig. 1(a). Therefore, how\nto find a novel visual representation that not only identifies\ntask-relevant contact areas but also directly constrains the\ndexterous grasping posture, ensuring a well-defined interac-\ntion between the hand and the object, remains a challenging\nproblem.\nKeypoint-based representations offer a potential solution\nby structuring high-dimensional visual data into a compact\nand interpretable form. Many studies [8], [9], [10], [11]\nhave demonstrated the effectiveness of keypoint-based ap-\nproaches in robotic manipulation, often decomposing grasp-\ning tasks into object and environment keypoints. For in-\nstance, KETO [10] defines three types of keypoints: grasp\npoints, functional points, and operation points. SKP [11]\ndirectly defines five keypoints on the objectâ€™s surface to\nsupport parallel grasping. However, these methods exhibit\nlimitations in their visual representation: either the keypoints\nare manually defined for specific tasks, limiting general-\nization to novel objects and tasks, or they rely heavily\non simulated environments for training, reducing their real-\nworld applicability. Additionally, many of these approaches\nrequire extensive manual annotations, further increasing data\ncollection costs.\nTo\nimprove\nthe\ngeneralization\nand\napplicability\nof\nkeypoint-based representations, VRB [12] introduces a more\narXiv:2502.20018v1  [cs.RO]  27 Feb 2025\n\n\nflexible visual representation by learning contact points and\nmotion trajectories from human operation videos, demon-\nstrating enhanced performance in robotic manipulation tasks.\nHowever, this method relies on post-processing steps, and its\nvisual representation remains indirect. More recent advances\nin Large Vision Models (LVM) have significantly improved\nobject feature extraction. For instance, ReKep [13] leverages\nLVMs to automatically extract candidate keypoints, and then\nfilters them using vision-language models, directly guiding\nrobotic operations. This approach enhances task generaliza-\ntion and establishes a more direct link between vision and\naction.\nDespite successes, the above methods primarily focus on\nsimple two-finger pinch grasps and do not extend to dex-\nterous grasping tasks. In dexterous grasping, keypoints must\nnot only determine the grasping location but also constrain\nthe entire hand configuration, ensuring functional stability, as\nshown in Fig. 1(b). Achieving this goal introduces three key\nchallenges: (1) Fine-grained feature extraction: Dexterous\ngrasping involves small, detailed interaction regions between\nfingers and the object. How can part-level keypoint features\nbe extracted from the object? (2) Data annotation cost:\nDexterous grasping requires precise keypoint annotations,\nwhich are costly to acquire. How can reliance on manual\nannotation be reduced? (3) Keypoint correspondence: Es-\ntablishing a consistent mapping between object keypoints\nand hand keypoints is essential for stable grasping. How can\nrobust keypoint correspondence be ensured?\nTo address the challenges, we propose the Multi-KeyPoint\nAffordance representation for Functional Dexterous Grasp-\ning. By localizing multiple keypoints on the object and\nthe hand, a unique dexterous grasping posture with clear\nconstraints is determined. First, we introduce the Contact-\nguided Multi-Keypoint Affordance (CMKA) learning, which\nleverages LVMs (e.g., SAM [14] and DINOv2 [15]) for fine-\ngrained affordance feature extraction. The CMKA supervises\nEgocentric (Ego)-view images using hand-object interaction\nregions in Exocentric (Exo)-view images as contact priors\nvia CAM [16], guiding keypoint learning towards meaningful\nfunctional contact areas and eliminating the need for manual\nkeypoint annotations. Then, we introduce the Keypoint-based\nGrasp matrix Transformation (KGT) method to ensure con-\nsistent mapping between hand and object keypoints. We ob-\nserve that the wrist, functional fingers (index or thumb), and\nlittle finger MCP joints effectively reflect the relative contact\nposture between the hand and the object. The positional\nrelationship of these three points forms a unique triangular\nstructure, providing a direct connection between hand and\nobject keypoints. We conduct comprehensive experiments to\nevaluate the proposed framework for multi-point affordance\nlocalization across 6 tasks and 18 tool shapes on the public\nFAH dataset [17], achieving a 45.35% improvement over\nthe state-of-the-art method in the KLD metric. In both\nIsaacGym [18] and real robot experiments, we successfully\nestablish the geometric constraint relationship between tool\nand hand keypoints.\nThe main contributions of this work are as follows:\nâ€¢ A novel multi-keypoint affordance representation is\nproposed, which constrains dexterous grasping postures\nthrough the geometric relationships of keypoints in the\nhand-object interaction region, directly establishing a\nlink between vision and dexterous grasping actions.\nâ€¢ CMKA,\na\nmulti-keypoint\naffordance\nlocalization\nmethod based on a weakly-supervised framework,\nand KGT, a keypoint-based hand-object relative pose\ntransformation\nmethod,\nare\nintroduced,\nleveraging\nexisting human interaction image data and LVMs for\nlearning, effectively reducing data costs, and enabling\nfunctional dexterous grasping.\nâ€¢ The proposed algorithm is validated in both simulation\nand real robot experiments, demonstrating its ability to\ndirectly map tasks to grasping actions while exhibiting\ngood generalization across tasks and objects, especially\nexcelling in complex functional grasping scenarios.\nII. RELATED WORK\nA. Object Representation for Dexterous Grasping\nGrasping and manipulation are fundamental topics in\nrobotics. Traditional methods [19], [20], [21] often rely on\nsix Degrees of Freedom (6DoF) poses to represent objects\nfor parallel gripper tasks. However, these methods are in-\nsufficient for dexterous grasping, which requires handling\nmultiple contact points and complex interactions beyond\nsimple position and orientation. Early methods such as\nrigid body modeling [22], [23] and template matching [24],\n[25] are task-specific and lack generalization, limiting their\napplicability to diverse tasks. Recent studies have focused\non object structure-based grasp affordance representations,\nsuch as ContactDB [26], which annotates object-finger con-\ntact relationships; the method in [27], which maps contact\npoints to finger regions and intent codes; and F2F [28],\nwhich uses knowledge graphs to associate functional object\nparts with functional fingers. While these methods improve\ntask performance, they depend on ideal perception systems\nthat assume precise segmentation or localization of func-\ntional regionsâ€”an assumption rarely achievable in real-\nworld settings. To address these limitations, we propose\na robust object representation method specifically designed\nfor dexterous grasping, eliminating the need for idealized\nperception inputs and enabling reliable handling of complex\ninteractions.\nB. Keypoint Representation and Robotic Manipulation\nKeypoint-based methods have been widely used in com-\nputer vision tasks such as face recognition [29], [30], human\npose estimation [31], and tracking [32], where keypoints\ntypically serve as low-level features or part-level object\ndescriptors. In robotics, keypoints provide compact repre-\nsentations of the environment and objects. For example,\nKETO [10] defines three types of keypointsâ€”grasp points,\nfunctional points, and operation pointsâ€”to describe tasks,\nwhereas SKP [11] defines five keypoints directly on the\nobject surface to support parallel grasping. However, these\nmethods are task-specific and struggle to generalize to new\n\n\nExo\nEgo\nTransfer\nconstraint\nR,T,J\nControl\nKeypoint\nCMKA\nKGT\nFig. 2: The key process of learning and connecting visual perception to functional dexterous grasping actions. The CMKA learning module\nlearns from Exo images with human operation experience and transfers the knowledge to Ego images, locating three keypoints constrained\nby dexterous grasping. The KGT method maps the hand-object relative pose, calculating the dexterous handâ€™s rotation and translation\nparameters (R, T) to control the grasping task.\ntasks. Recent advancements in large models have introduced\ngeneralizable representations for robotic manipulation, such\nas ReKep [13], which employs LVMs [15], [14] to extract\ncandidate keypoints and vision-language models to filter\ntask-relevant keypoints for direct operational guidance. How-\never, ReKep [13] focuses on simple parallel gripper tasks\nand requires additional reasoning steps, making it unsuit-\nable for dexterous manipulation. Inspired by human hand\ninteractions [33], we propose a multi-keypoint representation\nbased on the wrist, functional fingers, and the little finger.\nThis design directly constrains dexterous grasping postures,\nproviding effective and robust solutions for complex manip-\nulation tasks.\nC. Visual Affordance and Interaction\nVisual affordance learning explores potential object re-\ngions for specific actions and is a key topic in robotic\ngrasping. Early fully supervised methods [34], [35] relied\non large-scale annotated datasets, which were both expen-\nsive and time-consuming to create. To reduce annotation\ncosts, recent research has shifted toward weakly super-\nvised methods, leveraging keypoints [36], [37] or image-\nlevel labels [4], [38], [39]. In this work, we utilize human\ninteraction images to supervise Ego-view images through\ncontact features, significantly reducing training data costs by\nleveraging existing resources. Existing affordance methods\nfor robotic manipulation, such as VRB [12], learn contact\npoints and trajectories from human operation videos, whereas\nRobo-ABC [40] generates hand-object contact datasets to\nenable zero-shot generalization. Similarly, GAT [7] proposes\npixel-level affordance learning to capture precise regions.\nHowever, these methods often depend on post-processing\nsteps and additional modules, and their affordance regions\nare typically coarse, failing to provide the fine-grained con-\nstraints required for dexterous grasping. To address these\nlimitations, we propose a multi-region keypoint affordance\nlearning approach that directly provides fine-grained con-\nstraints tailored for dexterous grasping tasks.\nIII. METHODOLOGY\nIn this study, our goal is to develop a complete system\nthat establishes a direct visual representation for functional\ndexterous grasping and achieves cross-task and cross-object\ngeneralization. As shown in Fig. 2, during the training\nprocess, the proposed Contact-guided Multi-Keypoint Af-\nfordance (CMKA) learning module acquires human opera-\ntion experience from exocentric (Exo) images and transfers\nit to egocentric (Ego) images, identifying three keypoints\nconstrained by dexterous grasping in the Ego image. The\ndetails of this process will be explained in Sec. III-A. We\nthen use the Keypoint-based Grasp matrix Transformation\n(KGT) method to map and constrain the hand-object relative\npose using these keypoints, calculating the rotation and\ntranslation parameters (R, T) of the dexterous hand, thereby\nenabling control of the grasping task. The relevant details\nwill be further described in Sec. III-B. During the inference\nprocess, only Ego images are required as input, and the\nparameters learned by CMKA can be used to identify the\nthree affordance keypoints of the object.\nA. Contact-guided Multi-KeyPoint Affordances Learning\nTo identify the keypoint regions on the object surface\nwhere the fingers should make contact, robust fine-grained\nfeature extraction is required. To achieve this, as shown in\nFig. 3, we first employ LVM-based Multi-Scale Clustering\n(LMSC) to obtain candidate keypoints from different parts of\nthe object surface (see Sec. III-A.1). Next, we perform key-\npoint feature extraction from the Ego-view (see Sec. III-A.2)\nand design a keypoint weighting learning mechanism, which\ncomputes a weighted score for each candidate keypoint and\nselects the top three keypoints. Then, a keypoint-guided\nfeature extraction module is used to perform deep feature\nextraction on the selected regions in the Ego-view images.\nFinally, we leverage cues from the Exo-view for knowledge\ntransfer of the contact geometry relationship (see Sec. III-\nA.3), using learnable Class Activation Mapping (CAM)\ntechnology [16] to extract hand-object interaction features\nfrom the Exo-view images, and employ cosine similarity\nloss to supervise keypoint selection in the Ego-view images,\nensuring that the selected keypoints are concentrated around\nthe hand-object contact regions.\n1) LVM-based Multi-Scale Clustering Module: Inspired\nby the ReKep [13], we propose an LMSC module, which\naims to focus clustering on finger contact regions, as shown\n\n\nğ¹\nğ‘’ğ‘”ğ‘œ\nğ‘“ğ‘˜ğ‘–\nKeypoint-guided \nFeature Extract\nKeypoints \nlearnable weights \n...\nX\nLVM-based \nMulti-Scale \nClustering\nEgo\nâ€œPressâ€\nDINOv2\nSelect\nğ¹\nğ‘’ğ‘¥ğ‘œ\nExo\nğ‹cam\nExtract\nâ€¦\nPartSelect\nLOCATE\nSum\nâ„’ğ‘ğ‘™ğ‘ \nâ„’ğ‘ğ‘œğ‘ \n1\n2\n3\n0\n1\n2\n5\n3\n4\nN\nlearnable\nfixed\nDINOv2\nğ‘“ğ‘”ğ‘˜\nğ‘“ğ‘œğ‘\nFig. 3: Framework of the proposed CMKA. The framework includes: (1) a LVM-based Multi-Scale Clustering (LMSC) module for\nextracting candidate keypoints; (2) Keypoint feature extraction from egocentric (Ego) view; (3) Contact geometry knowledge transfer from\nexocentric (Exo) view to Ego view.\nSAM\nğ‘€1\nğ‘€2\nğ‘€3\nClustering\nPCA\nMulti-scale feature fusion\nEgo\nğ¹ğ‘’ğ‘”ğ‘œ\n0\n1\n2\n5\n3\n4\nN\nLayer\nLayer\nâ€¦\nLinear\nFusion\nMLP\nBilinear\nSum\nğ‘“â€²\nğ¾ğ‘ğ‘›\nğ¹\nğ‘“\nFig. 4: Candidate keypoint selection using the proposed LMSC\nmodule.\nin Fig. 4.\nFirst, we extract multi-level features from multiple inter-\nmediate layers of the DINOv2 model [15] to capture informa-\ntion from low to high levels. Specifically, we extract features\nfrom the first three layers, denoted as Fli (where i = 1,2,3)\nand apply linear transformations and normalization to each\nlayer. Next, a learnable weight vector Î±i is used to perform\nweighted fusion, where the weights are normalized using\nthe softmax function. The final fused feature representation\nis obtained as:\nF =\n3\nâˆ‘\ni=1\nÎ±iFli.\nThen, F is passed through a multi-layer perceptron (MLP)\nto obtain the feature representation f. To further incorporate\nmulti-scale information, we perform upsampling and down-\nsampling on f to obtain fâ†‘and fâ†“, respectively. The final\nmulti-scale feature representation is obtained by summation:\nf â€² = f + fâ†“+ fâ†‘.\nMeanwhile, recent vision foundation models like Segment\nAnything Model\n[14] (SAM) have demonstrated strong\ncapacity to produce robust zero-shot segmentation in real-\nworld scenes. we apply SAM [14] to the Ego-view image\nto obtain multi-region masks Mi and perform region-wise\nclustering on the multi-scale feature representation f â€² us-\ning the non-background masks. Specifically, we first apply\nPCA to reduce the dimensionality of each regionâ€™s features,\nobtaining the reduced feature representation XPCA âˆˆRnÃ—k,\nwhere n is the number of pixels in the region and k is the\nreduced dimension. Then, we perform K-means clustering on\nthe reduced features to obtain multiple candidate keypoints\nKcn, selecting the center of each cluster as the final candidate\nkeypoint. The clustering aims to minimize the distance\nbetween samples and cluster centers:\nKcn = argmin\nC\nn\nâˆ‘\ni=1\nâˆ¥X(i)\nPCA âˆ’Cjâˆ¥2,\nwhere Kcn represents the n-th candidate keypoint, Cj is the\ncenter of the j-th cluster, and X(i)\nPCA is the feature of the i-th\nsample in the region. Finally, we select the center of each\ncluster as the candidate keypoint. If no valid candidates are\nfound, the pixel closest to the object centroid is chosen as a\nfallback keypoint.\n2) Keypoint Feature Extraction from Egocentric View: To\nextract keypoint features from the Ego view image, we define\na set of learnable weights W âˆˆRtÃ—n, where t represents\nthe number of task types and n is the number of candidate\nkeypoints. These weights are multiplied with the candidate\nkeypoints Kcn to select the three keypoints Ki (where i =\n1,2,3) for feature extraction from the corresponding regions\nin the Ego view image.\nFor the selected keypoints Ki, we define a circular region\ncentered at each keypoint with a radius r and extract features\nfrom these regions. The extracted region features are denoted\nas Fki, representing the features from the regions centered on\nthe selected keypoints.\nTo align the features from the Ego and Exo views in a\nunified feature space, we apply a linear transformation to the\nextracted keypoint features Fki, resulting in the final keypoint\nfeatures fop:\nfki = proj(Fki),\nwhere the projection layer proj is a linear transformation that\nmaps the Ego view features to the feature space aligned with\n\n\nI\nğ’™ğ’Š\nğ’šğ’Š\nğ’›ğ’Š\nF\nL\nW\nğ’šğ’‰\nH\nğ’›ğ’‰\nğ’™ğ’‰\nO\nğ’šğ’\nğ’™ğ’\nğ’›ğ’\nğ‘¾ğ‘¶\nğ‘³ğ’\nğ‘­ğ’\nAdjust\nğ‘¾ğ‘¶\nğ‘³ğ’\nğ‘­ğ’\n(a)\n(b)\nğ‘²ğŸ\nğ‘²ğŸ\nğ‘²ğŸ\nFig. 5: Illustration of KGT method in IsaacGym [18], showing the\nkeypoints on the object and the hand (functional finger, little finger,\nand wrist) and their role in coordinate frame construction.\nthe Exo view.\n3) Contact Geometry Knowledge Transfer: In the final\nstep, we utilize the CAM technique [16] to classify Exoâ€™s\nfeatures for the specific task, with the classification loss\ndenoted as Lcls. Additionally, we extract object features from\nthe interactive regions using the Extract and PartSelect [38]\nmodules, obtaining the prototype features fop for the key-\npoint regions.\nFor the three keypoint features fi extracted from the Ego\nview, we compute their sum to obtain the global keypoint\nfeature fgk, which encapsulates the contact geometry infor-\nmation from the Ego perspective:\nfgk =\n3\nâˆ‘\ni=1\nfki.\nNext, we calculate the cosine similarity loss Lcos between\nthe Exo prototype features fop and the global Ego keypoint\nfeatures fgk:\nLcos = 1âˆ’\nfop Â· fgk\nâˆ¥fopâˆ¥âˆ¥fgkâˆ¥.\nThe final loss is the combination of the classification\nloss and the cosine similarity loss, ensuring that the contact\ngeometry knowledge is accurately transferred between the\ntwo views:\nL = Lcls +Lcos.\nB. Keypoint-based Grasp Matrix Transformation\nAfter obtaining the three keypoints Ki on the object, we\napply the KGT to obtain the relative pose transformation\nmatrix (R,T) between the dexterous hand and the tool.\nSpecifically, as shown in the Fig. 5 (a), we take K0 as the\nreference point, determine the direction from K0 to K1, and\nform a plane using K0, K1, and K2. Based on the hand model\n(yellow triangle), we adjust the keypoints, resulting in the\ncorrected contact points positions in the world coordinate\nsystem Fo, Lo, and Wo.\nThen, as shown in the Fig. 5 (b), we define the object\ncoordinate system O with Wo as the origin, the x-axis as\nxo =\nâˆ’âˆ’âˆ’â†’\nWoFo\nâˆ¥âˆ’âˆ’âˆ’â†’\nWoFoâˆ¥, the z-axis as zo =\nâˆ’âˆ’âˆ’â†’\nWoFoÃ—âˆ’âˆ’âˆ’â†’\nWoLo\nâˆ¥âˆ’âˆ’âˆ’â†’\nWoFoÃ—âˆ’âˆ’âˆ’â†’\nWoLoâˆ¥, and the y-axis\nas yo = zo Ã—xo, leading to the object rotation matrix in the\nworld coordinate system:\nRI\nO = [xo,yo,zo].\nAt the same time, obtain the transformation matrix between\nthe world coordinate system I and the object coordinate\nsystem O:\nT I\nO =\n\u0014\nRI\nO\nWo\n0\n1\n\u0015\nSimilarly, the hand coordinate system H is defined with W\nas the origin, the x-axis as xh =\nâˆ’âˆ’â†’\nWF\nâˆ¥âˆ’âˆ’â†’\nWFâˆ¥, the z-axis as zh =\nâˆ’âˆ’â†’\nWFÃ—âˆ’â†’\nWL\nâˆ¥âˆ’âˆ’â†’\nWFÃ—âˆ’â†’\nWLâˆ¥, and the y-axis as yh = zh Ã— xh, where F, L, and\nW represent the keypoints positions on the hand in the world\ncoordinate system, yielding the hand rotation matrix:\nRI\nH = [xh,yh,zh].\nThe relative rotation matrix between the hand and the object\nis then computed as:\nR = (RI\nO)âˆ’1RI\nH,\nwhile the translation vector is given by:\nT = (T I\nO)âˆ’1(W âˆ’Wo).\nIV. EXPERIMENTS\nA. Setup\nDatasets: The public challenging FAH benchmark [17]\nis a large-scale affordance-annotated dataset specifically de-\nsigned for hand-object interactions. It contains 6 functional\ngrasp affordances and 18 tools, with 5,858 images spanning\nboth exocentric (Exo) and egocentric (Ego) views. The\ndataset provides image-level affordance labels for weakly\nsupervised learning and annotations for coarse dexterous\ngrasp gestures targeting specific â€œTask Toolâ€ pairs. However,\nits test set only includes heatmap annotations for functional\nfinger contact regions. To address this limitation, we sparsely\nannotate two additional contact points (little finger and\nwrist projection points). Specifically, polygons with up to\nfive points are constructed around finger keypoints within\na 5mm radius, and Gaussian kernels are applied at each\npoint to generate dense annotations. During training, point\nannotations are added to the object regions in each Ego-\nview image to distinguish foreground and background during\nsegmentation with SAM [14].\nImplementation Details: Experiments are conducted on\nan NVIDIA RTX A6000 GPU. The model is trained using\nthe SGD optimizer with a learning rate of 0.01 over 15\niterations. Images are resized to a resolution of 448Ã—448.\nFollowing prior works [38], [4], we evaluate the results using\nKullback-Leibler Divergence (KLD), Similarity (SIM), and\nNormalized Scanpath Saliency (NSS) metrics.\nB. Results of Functional Affordance Grounding\nIn this section, we present qualitative and quantitative\nresults to demonstrate the effectiveness and efficiency of our\nmethod on the FAH test set [17]. As weakly- or unsupervised\nmethods for multi-region affordance localization are scarce\nin the state of the art, we use ReKep* [13], a keypoint\nprediction method, as our baseline.\n\n\nGT\nOurs\nReKep\nâ€œPressâ€\nâ€œClickâ€\nâ€œGripâ€\nâ€œClampâ€\nâ€œPressâ€\nImage\nâ€œClampâ€\nâ€œHoldâ€\nâ€œHoldâ€\nFig. 6: Qualitative comparison between our approach and the state-of-the-art multi-keypoint affordance grounding method (ReKep* [13])\non the FAH test set [17]. Our proposed method predicts keypoints that are more concentrated in the contact areas and captures the\ngeometric information of the grasping posture.\nTABLE I: Comparison to state-of-the-art method on the FAH test\nset [17]. The best results are highlighted in bold. (â†‘/â†“means\nhigher/lower is better).\nModel\nKLD (â†“)\nSIM (â†‘)\nNSS (â†‘)\nReKep* [13]\n9.213\n0.203\n0.429\nOurs\n5.035\n0.313\n0.865\nQuantitative Results. As shown in Tab. I, our method sig-\nnificantly outperforms ReKep* [13] across multiple metrics.\nSpecifically, it improves KLD by 45.35%, increases SIM by\n54.19%, and improves NSS by 101.63%. These improve-\nments stem from ReKep*â€™s lack of adaptation to dexterous\ngrasping. While ReKep* [13] originally relies on manually\nselected keypoints, its modified version ReKep* [13] ran-\ndomly generates three keypoints without explicit modeling\nof functional contact regions. In contrast, our method em-\nploys a learnable weighting mechanism to generate keypoints\nspecifically for dexterous grasping, ensuring their alignment\nwith functional contact regions.\nHyperparameter Analysis. We further investigate the\nimpact of the candidate keypoint number N={6,9,12,15}\non model performance, In Tab. II, we show the effects\nof different values on KLD, SIM, and NSS. The results\nindicate that N=12 achieves the best performance across\nall metrics. This aligns with our design principle: too few\nkeypoints lead to insufficient representation of affordance\nregions, hindering fine-grained grasp modeling, while too\nmany introduce redundancy, diluting feature importance and\nreducing the modelâ€™s focus on functional regions.\nAblation Study. The object priors provided by SAM [14]\nare crucial for constraining keypoint proposals to objects in\nthe scene rather than the background. Thus, we focus on\nanalyzing the critical visual feature extraction network in\nTABLE II: Impact of the candidate keypoint number N.\nN\nKLD (â†“)\nSIM (â†‘)\nNSS (â†‘)\n6\n5.409\n0.308\n0.849\n9\n5.748\n0.282\n0.737\n12\n5.035\n0.313\n0.865\n15\n5.766\n0.279\n0.723\nTABLE III: Ablation study on different feature extractors. FFL:\nfeed-forward layer. MSFF: multi-scale feature fusion.\nDINOv2 DINO-ViT MSFF FFL\nKLD (â†“) SIM (â†‘)\nNSS (â†‘)\nâœ“\nâœ“\n5.035\n0.313\n0.865\nâœ“\nâœ“\n5.517\n0.302\n0.67\nâœ“\nâœ“\n5.807\n0.267\n0.77\nâœ“\nâœ“\n6.075\n0.253\n0.65\nCMKA. As shown in Tab. III, DINOv2 [15], as the backbone\nnetwork combined with our designed multi-scale feature\nfusion (MSFF) module, achieves the best performance. In\nthe backbone network, DINOv2 generates clearer features\ncompared to DINO-ViT [41], better distinguishing fine-\ngrained object regions and leading to improved performance.\nFurthermore, compared to replacing MSFF with a simple\nfully connected network, MSFF, with its multi-layer and\nmulti-scale feature mapping, demonstrates superior potential.\nQualitative Analysis. As shown in Fig. 6, the visibility\ngrounding visualizations include Ground Truth (GT), our\nmethod, and the baseline method ReKep* [13]. Compared\nto the baseline, our method localizes keypoints within the\nhand-object contact region while preserving the relative\nspatial relationships among the functional finger, little finger,\nand wrist projection, ensuring a meaningful distribution for\ndexterous grasping. For example, in the â€œClick Flashlightâ€\ntask, keypoint 0 is localized on the thumb and keypoint 1\n\n\nï¼ˆaï¼‰â€œPress Drill â€\nï¼ˆbï¼‰â€œHold Drill â€\nï¼ˆcï¼‰â€œClick Flashlight â€\nï¼ˆdï¼‰â€œHold Flashlight â€\nFig. 7: Visualization of initial and final hand-object states in\nIsaacGym [18] for different â€œTask Toolâ€ combinations. The red\nspheres represent the three keypoints used for grasp transformation.\nUR5 Robotic \nArm\nInspire \nDexterous Hand\nIntel D435i\nRGB-D Camera\nTool Rack\nControl Host\nAruco Code\nTool\n(a)\n(b)\nFig. 8: Experimental setup in a real-world scenario: (a) Hardware\nplatform; (b) Calibration standards for the geometric relationship\nbetween keypoints when the functional finger is the index finger\n(upper) and the thumb (lower).\non the little finger, accurately reflecting the contact regions.\nIn the â€œPress Drillâ€ task, keypoint 0 is placed on the\nindex finger, keypoint 1 on the little finger, and keypoint\n2 on the wrist projection, maintaining a reasonable spatial\nrelationship. In contrast, ReKep* [13] relies on manual post-\nprocessing, failing to constrain keypoints within the contact\nregion and lacking spatial consistency, resulting in scattered\nkeypoints and reduced accuracy of the affordance region.\nC. Evaluation of Keypoint-Based Grasp Transformation\nTo validate the effectiveness of the keypoint-based dex-\nterous grasp transformation method KGT, we conduct ex-\nperiments on four â€œTask Toolâ€ combinations: â€œPress Drillâ€,\nâ€œHold Drillâ€, â€œClick Flashlightâ€, and â€œHold Flashlightâ€.\nAs shown in Fig. 7, we visualized the initial and final hand-\nobject states in the simulation environment IsaacGym [18].\nThe results demonstrate that our method accurately computes\nthe grasp transformation matrix, enabling precise hand-object\ninteraction across different task-tool combinations with vary-\ning initial hand-object relative postures. For functional inter-\naction tasks, such as â€œPress Drillâ€ and â€œClick Flashlightâ€,\nthe method ensures correct contact between the functional\nfingers and the target components. For general grasping\ntasks, such as â€œHoldâ€, our method achieves a natural grasp,\nensuring a reasonable hand posture.\nInitial\nLocation\nAdjust\nControl\nw\n4/5ï¼ˆ3/5ï¼‰\n3/5ï¼ˆ0/5ï¼‰\n2/5ï¼ˆ0/5ï¼‰\nğ‘Šğ‘œ\nğ¹ğ‘œ\nğ¿ğ‘œ\nğ¹ğ‘œ\nğ¿ğ‘œ\nğ‘Šğ‘œ\n(R,T)\nğ‘Šğ‘œ\nğ¿ğ‘œ\nğ¹ğ‘œ\n(R,T)\n(R,T)\nFig. 9: Experiments with three typical â€œTask Toolsâ€ in real-world\nscenarios: â€œClick Flashlightâ€, â€œPress Drillâ€ and â€œPress Spraybottleâ€\n(from top to bottom). The upper right corner of the fourth column\ncompares the functional dexterous grasping success rate with our\nmethod and GAAF-Dex [17] (in bracket), where the total grasp\nnumber of each instance is 5.\nD. Performance in Real-World Scenarios\nReal-world Experiments Setup: As shown in Fig. 8(a),\nthe real-world platform consists of an Inspire hand, a UR5\nindustrial robotic arm, an Intel RealSense camera, a tool rack,\nand a control computer. To address real-world uncertainties,\nwe introduce keypoint relative position calibration annota-\ntions based on the Inspire model during the grasping process,\nas shown in Fig. 8(b).\nIn the experiments, we use tool instances commonly found\nin daily life, which were unseen in the training set. As shown\nin Fig. 9, we selected three â€œTask Toolâ€ with strict functional\ngrasping requirements for the experiment: â€œClick Flashlightâ€,\nâ€œPress Drillâ€, and â€œPress Spraybottleâ€. We recorded four\nstates: the initial state, followed by the localization of three\naffordance keypoints on the tool surface using the CMKA\nmethod to estimate their initial planar relationship. Simulta-\nneously, we utilized an RGB-D camera to obtain the depth z\nfrom the (x,y) coordinates of the keypoints in the depth map,\nthus acquiring their (x,y,z) coordinates in 3D space. We then\nadjusted the relative geometric positions of the keypoints on\nthe dexterous hand palm using calibration standards (yellow\ntriangles in the third column). Finally, we apply the KGT\nmethod to compute the final wrist grasp pose W(R,T), and\nuse the coarse gesture labels from the FAH [17] to obtain\nthe coarse grasp angle J for each â€œTask Tool,\" which controls\nthe final grasping of the dexterous hand.\nThe results demonstrate that for complex tasks requiring\nprecise finger-object alignment, such as Press and Click,\nour method effectively bridges the gap between multi-point\nperception and dexterous grasping, showing broad practi-\ncal value. Furthermore, due to the lack of direct methods\ncombining perception and dexterous grasping, we compared\nour method with the state-of-the-art GAAF-Dex [17] by\nthe number of successful functional grasps. We define a\nsuccessful grasp as the functional finger combining with\nthe toolâ€™s functional component while the remaining fingers\nsecurely grasp other parts of the tool. As shown in the top left\n\n\ncorner of Fig. 9, our method improves the grasp success rate\nby an average of 40% across three complex tasks. GAAF-\nDex [17] is only effective when the initial and final grasp\nrotations are similar, such as in the Click Flashlight scenario,\ndue to its lack of adaptive rotation handling. In contrast, our\nmethod can handle arbitrary initial grasp poses.\nV. CONCLUSION AND FUTURE WORK\nThis work proposes a keypoint-based affordance repre-\nsentation for functional dexterous grasping. By leveraging\nhuman experience data for weak supervision and inte-\ngrating the CMKA module with large visual models, our\napproach achieves precise multi-point contact localization,\nreducing data annotation costs and improving generaliza-\ntion. The KGT method enables the mapping of dexterous\nhand postures to object keypoints, ensuring a direct con-\nnection between perception and action. Experimental results\ndemonstrate that the proposed method outperforms existing\napproaches in localization accuracy and functional grasp\nsuccess rate. Practical experiments show that relying solely\non 2D vision for localization fails to provide stable grasp\nconstraints. In the future, we aim to utilize multimodal\ninformation to enhance the accuracy and stability of multi-\npoint 3D localization in real-world scenarios.\nREFERENCES\n[1] J. J. Gibson, â€œThe theory of affordances,â€ Hilldale, USA, vol. 1, no. 2,\npp. 67â€“82, 1977.\n[2] A. Myers, C. L. Teo, C. FermÃ¼ller, and Y. Aloimonos, â€œAffordance\ndetection of tool parts from geometric features,â€ in Proc. ICRA, 2015,\npp. 1374â€“1381.\n[3] R. Xu, F.-J. Chu, C. Tang, W. Liu, and P. A. Vela, â€œAn affordance\nkeypoint detection network for robot manipulation,â€ IEEE Robotics\nand Automation Letters, vol. 6, no. 2, pp. 2870â€“2877, 2021.\n[4] H. Luo, W. Zhai, J. Zhang, Y. Cao, and D. Tao, â€œLearning affordance\ngrounding from exocentric images,â€ in Proc. CVPR, 2022, pp. 2242â€“\n2251.\n[5] L. Xu, Y. Gao, W. Song, and A. Hao, â€œWeakly supervised multimodal\naffordance grounding for egocentric images,â€ in Proc. AAAI, vol. 38,\nno. 6, 2024, pp. 6324â€“6332.\n[6] T. Nguyen et al., â€œLanguage-conditioned affordance-pose detection in\n3D point clouds,â€ in Proc. ICRA, 2024, pp. 3071â€“3078.\n[7] G. Li et al., â€œLearning precise affordances from egocentric videos for\nrobotic manipulation,â€ arXiv preprint arXiv:2408.10123, 2024.\n[8] H. S. Koppula, R. Gupta, and A. Saxena, â€œLearning human activi-\nties and object affordances from RGB-D videos,â€ The International\nJournal of Robotics Research, vol. 32, no. 8, pp. 951â€“970, 2013.\n[9] L. Manuelli, W. Gao, P. Florence, and R. Tedrake, â€œKPAM: KeyPoint\naffordances for category-level robotic manipulation,â€ in Proc. ISRR,\n2019, pp. 132â€“157.\n[10] Z. Qin, K. Fang, Y. Zhu, L. Fei-Fei, and S. Savarese, â€œKETO: Learning\nkeypoint representations for tool manipulation,â€ in Proc. ICRA, 2020,\npp. 7278â€“7285.\n[11] Z. Luo, W. Xue, J. Chae, and G. Fu, â€œSKP: Semantic 3D keypoint\ndetection for category-level robotic manipulation,â€ IEEE Robotics and\nAutomation Letters, vol. 7, no. 2, pp. 5437â€“5444, 2022.\n[12] S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak, â€œAffordances\nfrom human videos as a versatile representation for robotics,â€ in Proc.\nCVPR, 2023, pp. 1â€“13.\n[13] W. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei, â€œReKep:\nSpatio-temporal reasoning of relational keypoint constraints for robotic\nmanipulation,â€ arXiv preprint arXiv:2409.01652, 2024.\n[14] A. Kirillov et al., â€œSegment anything,â€ in Proc. ICCV, 2023, pp. 3992â€“\n4003.\n[15] M. Oquab et al., â€œDINOv2: Learning robust visual features without\nsupervision,â€ Transactions on Machine Learning Research, vol. 2024,\n2024.\n[16] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. S. Huang, â€œAdversarial\ncomplementary learning for weakly supervised object localization,â€ in\nProc. CVPR, 2018, pp. 1325â€“1334.\n[17] F. Yang et al., â€œLearning granularity-aware affordances from human-\nobject interaction for tool-based functional grasping in dexterous\nrobotics,â€ arXiv preprint arXiv:2407.00614, 2024.\n[18] V. Makoviychuk et al., â€œIsaac gym: High performance GPU based\nphysics simulation for robot learning,â€ in Proc. NeurIPS, 2021.\n[19] S. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel,\nâ€œCombined task and motion planning through an extensible planner-\nindependent interface layer,â€ in Proc. ICRA, 2014, pp. 639â€“646.\n[20] S. Tyree et al., â€œ6-DoF pose estimation of household objects for robotic\nmanipulation: An accessible dataset and benchmark,â€ in Proc. IROS,\n2022, pp. 13 081â€“13 088.\n[21] B. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose:\nUnified 6D pose estimation and tracking of novel objects,â€ in Proc.\nCVPR, 2024, pp. 17 868â€“17 879.\n[22] C. Rosales, R. SuÃ¡rez, M. Gabiccini, and A. Bicchi, â€œOn the synthesis\nof feasible and prehensile robotic grasps,â€ in Proc. ICRA, 2012, pp.\n550â€“556.\n[23] S. El-Khoury, R. De Souza, and A. Billard, â€œOn computing task-\noriented grasps,â€ Robotics and Autonomous Systems, vol. 66, pp. 145â€“\n158, 2015.\n[24] C. Gabellieri et al., â€œGrasp it like a pro: Grasp of unknown objects\nwith robotic hands based on skilled human expertise,â€ IEEE Robotics\nand Automation Letters, vol. 5, no. 2, pp. 2808â€“2815, 2020.\n[25] M. Kokic, D. Kragic, and J. Bohg, â€œLearning task-oriented grasping\nfrom human activity datasets,â€ IEEE Robotics and Automation Letters,\nvol. 5, no. 2, pp. 3352â€“3359, 2020.\n[26] S. Brahmbhatt, C. Ham, C. C. Kemp, and J. Hays, â€œContactDB:\nAnalyzing and predicting grasp contact via thermal imaging,â€ in Proc.\nCVPR, 2019, pp. 8709â€“8719.\n[27] T. Zhu, R. Wu, J. Hang, X. Lin, and Y. Sun, â€œToward human-like grasp:\nFunctional grasp by dexterous robotic hand via object-hand semantic\nrepresentation,â€ IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 45, no. 10, pp. 12 521â€“12 534, 2023.\n[28] F. Yang et al., â€œTask-oriented tool manipulation with robotic dexterous\nhands: A knowledge graph approach from fingers to functionality,â€\nIEEE Transactions on Cybernetics, vol. 55, no. 1, pp. 395â€“408, 2025.\n[29] M. Mayo and E. Zhang, â€œ3d face recognition using multiview keypoint\nmatching,â€ in Proc. AVSS, 2009, pp. 290â€“295.\n[30] S. Berretti, B. Ben Amor, M. Daoudi, and A. Del Bimbo, â€œ3D facial\nexpression recognition using sift descriptors of automatically detected\nkeypoints,â€ The Visual Computer, vol. 27, pp. 1021â€“1036, 2011.\n[31] V. Belagiannis and A. Zisserman, â€œRecurrent human pose estimation,â€\nin Proc. FG, 2017, pp. 468â€“475.\n[32] S. Chan, X. Zhou, and S. Chen, â€œRobust adaptive fusion tracking based\non complex cells and keypoints,â€ IEEE Access, vol. 5, pp. 20 985â€“\n21 001, 2017.\n[33] J. Hang et al., â€œDexFuncGrasp: A robotic dexterous functional grasp\ndataset constructed from a cost-effective real-simulation annotation\nsystem,â€ in Proc. AAAI, vol. 38, no. 9, 2024, pp. 10 306â€“10 313.\n[34] A. Nguyen, D. Kanoulas, D. G. Caldwell, and N. G. Tsagarakis,\nâ€œObject-based affordances detection with convolutional neural net-\nworks and dense conditional random fields,â€ in Proc. IROS, 2017,\npp. 5908â€“5915.\n[35] Y. Yang, W. Zhai, H. Luo, Y. Cao, J. Luo, and Z.-J. Zha, â€œGrounding\n3D object affordance from 2D interactions in images,â€ in Proc. ICCV,\n2023, pp. 10 871â€“10 881.\n[36] J. Sawatzky, A. Srikantha, and J. Gall, â€œWeakly supervised affordance\ndetection,â€ in Proc. CVPR, 2017, pp. 5197â€“5206.\n[37] J. Sawatzky and J. Gall, â€œAdaptive binarization for weakly supervised\naffordance segmentation,â€ in Proc. ICCVW, 2017, pp. 1383â€“1391.\n[38] G. Li, V. Jampani, D. Sun, and L. Sevilla-Lara, â€œLOCATE: Localize\nand transfer object parts for weakly supervised affordance grounding,â€\nin Proc. CVPR, 2023, pp. 10 922â€“10 931.\n[39] T. Nagarajan, C. Feichtenhofer, and K. Grauman, â€œGrounded human-\nobject interaction hotspots from video,â€ in Proc. ICCV, 2019, pp.\n8687â€“8696.\n[40] Y. Ju, K. Hu, G. Zhang, G. Zhang, M. Jiang, and H. Xu, â€œRobo-ABC:\nAffordance generalization beyond categories via semantic correspon-\ndence for robot manipulation,â€ in Proc. ECCV, vol. 15099, 2024, pp.\n222â€“239.\n[41] S. Amir, Y. Gandelsman, S. Bagon, and T. Dekel, â€œDeep ViT features\nas dense visual descriptors,â€ arXiv preprint arXiv:2112.05814, 2021.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20018v1.pdf",
    "total_pages": 8,
    "title": "Multi-Keypoint Affordance Representation for Functional Dexterous Grasping",
    "authors": [
      "Fan Yang",
      "Dongsheng Luo",
      "Wenrui Chen",
      "Jiacheng Lin",
      "Junjie Cai",
      "Kailun Yang",
      "Zhiyong Li",
      "Yaonan Wang"
    ],
    "abstract": "Functional dexterous grasping requires precise hand-object interaction, going\nbeyond simple gripping. Existing affordance-based methods primarily predict\ncoarse interaction regions and cannot directly constrain the grasping posture,\nleading to a disconnection between visual perception and manipulation. To\naddress this issue, we propose a multi-keypoint affordance representation for\nfunctional dexterous grasping, which directly encodes task-driven grasp\nconfigurations by localizing functional contact points. Our method introduces\nContact-guided Multi-Keypoint Affordance (CMKA), leveraging human grasping\nexperience images for weak supervision combined with Large Vision Models for\nfine affordance feature extraction, achieving generalization while avoiding\nmanual keypoint annotations. Additionally, we present a Keypoint-based Grasp\nmatrix Transformation (KGT) method, ensuring spatial consistency between hand\nkeypoints and object contact points, thus providing a direct link between\nvisual perception and dexterous grasping actions. Experiments on public\nreal-world FAH datasets, IsaacGym simulation, and challenging robotic tasks\ndemonstrate that our method significantly improves affordance localization\naccuracy, grasp consistency, and generalization to unseen tools and tasks,\nbridging the gap between visual affordance learning and dexterous robotic\nmanipulation. The source code and demo videos will be publicly available at\nhttps://github.com/PopeyePxx/MKA.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}