{
  "id": "arxiv_2502.20237v1",
  "text": "Teasing Apart Architecture and Initial Weights\nas Sources of Inductive Bias in Neural Networks\nGianluca Bencomo1, Max Gupta1, Ioana Marinescu2, R. Thomas McCoy3, Thomas L. Griffiths1,4\ngb5435@princeton.edu, mg7411@princeton.edu, im2178@nyu.edu, tom.mccoy@yale.edu, tomg@princeton.edu\n1Department of Computer Science, Princeton University\n2Center for Data Science, New York University\n3Department of Linguistics and Wu Tsai Institute, Yale University\n4Department of Psychology, Princeton University\nAbstract\nArtificial neural networks can acquire many aspects of human\nknowledge from data, making them promising as models of\nhuman learning. But what those networks can learn depends\nupon their inductive biases – the factors other than the data\nthat influence the solutions they discover – and the inductive\nbiases of neural networks remain poorly understood, limiting\nour ability to draw conclusions about human learning from the\nperformance of these systems. Cognitive scientists and ma-\nchine learning researchers often focus on the architecture of a\nneural network as a source of inductive bias. In this paper we\nexplore the impact of another source of inductive bias – the\ninitial weights of the network – using meta-learning as a tool\nfor finding initial weights that are adapted for specific prob-\nlems.\nWe evaluate four widely-used architectures – MLPs,\nCNNs, LSTMs, and Transformers – by meta-training 430 dif-\nferent models across three tasks requiring different biases and\nforms of generalization. We find that meta-learning can sub-\nstantially reduce or entirely eliminate performance differences\nacross architectures and data representations, suggesting that\nthese factors may be less important as sources of inductive\nbias than is typically assumed. When differences are present,\narchitectures and data representations that perform well with-\nout meta-learning tend to meta-train more effectively. More-\nover, all architectures generalize poorly on problems that are\nfar from their meta-training experience, underscoring the need\nfor stronger inductive biases for robust generalization.\nKeywords: inductive biases; neural networks; meta-learning\nIntroduction\nArtificial neural networks have been used to explain how as-\npects of human knowledge that have been claimed to depend\nupon an extensive degree of innateness – such as elements of\nlanguage – might be learned from data by systems that do not\nhave strong built-in assumptions (e.g., Rumelhart & McClel-\nland, 1986). These networks offer a new perspective on cen-\ntral questions in cognitive science, such as what information\nwe need to assume is innate to human learners (Elman et al.,\n1996). In machine learning, a parallel set of questions focuses\non the inductive biases of neural networks – defined as those\nfactors other than the data that influence the solutions that\nthey find (Mitchell, 1997). The convergence of these litera-\ntures offers an opportunity to explore different ways in which\ninnate knowledge might be implicitly expressed in artificial\nneural networks.\nDifferent neural network architectures display different in-\nductive biases. For instance, one clear signature of induc-\ntive bias is the amount of data needed to learn a task, and\nconvolutional neural networks can learn image classification\ntasks from less data than multi-layer perceptrons (Chen et\nal., 2021).\nIn addition to network architecture, however,\nrecent work has highlighted the importance of a network’s\ninitial weights as a source of inductive bias (Finn, Abbeel,\n& Levine, 2017). Specifically, techniques based on meta-\nlearning can optimize the initial weights of a neural network\n(leaving the architecture unchanged) in ways that enable the\nnetwork to learn new tasks from far less data than it would re-\nquire using standard, randomly-selected initial weights. For\ninstance, a network with meta-learned initial weights can\nlearn new linguistic rules from just 100 examples, compared\nto the roughly 20,000 examples needed by the same archi-\ntecture with non-meta-learned initial weights (McCoy, Grant,\nSmolensky, Griffiths, & Linzen, 2020). Such meta-learning\nresults show that a given neural network architecture can re-\nalize very different inductive biases thanks to the flexibility\nafforded by the initial weights.\nHere we consider this flexibility from the opposite direc-\ntion: can a given inductive bias be realized equally well\nin very different network architectures?\nThis question di-\nrectly engages with the issue of whether architecture or initial\nweights provide a better focus for understanding the innate\nconstraints on learning implicitly instantiated in a neural net-\nwork. Prior work using meta-learning typically makes com-\nparisons within a fixed architecture, comparing a version of\nthat architecture with meta-learned initial weights to a version\nwith randomly-selected initial weights. These comparisons\nmake it clear that the initial weights afford a substantial de-\ngree of flexibility, but they leave open the question of whether\nthat flexibility is extensive enough to override the influence of\narchitecture such that a given inductive bias could be realized\nequally well in different architectures.\nTo address this, we explore several inductive biases, inves-\ntigating how compatible each inductive bias is with differ-\nent types of network architectures and data representations.\nWe consider four widely-used, general-purpose neural archi-\ntectures—multilayer perceptrons (MLPs; Rosenblatt, 1962),\nconvolutional neural networks (CNNs; LeCun, Bottou, Ben-\ngio, & Haffner, 1998), long short-term memory networks\n(LSTMs; Hochreiter & Schmidhuber, 1997), and Transform-\ners (Vaswani, 2017)—with variations in depth and width,\nmeta-training a total of 430 models. To establish baselines\nwhere differences across architectures and data representa-\ntions should be more pronounced—free from task-specific bi-\narXiv:2502.20237v1  [cs.LG]  27 Feb 2025\n\n\nases introduced by meta-learning—we compare these meta-\ntrained models to the same architectures trained under typical\nregimes, starting from random initialization and optimizing\nalong that trajectory. This design enables us to isolate how\nmuch of the performance variation can be attributed to ar-\nchitectural and data representation choices, as opposed to the\nlearning processes that are agnostic to those choices.\nAcross both data representation and architecture, we ob-\nserve substantial performance differences when models are\ntrained using the usual approach of setting the initial weights\nrandomly. However, introducing meta-learned inductive bi-\nases reduces, and in some cases completely eliminates, these\ndifferences, demonstrating that a given inductive bias can be\ninstantiated in multiple, disparate architectures. Interestingly,\narchitectures and data representations that perform well un-\nder random initialization also tend to meta-train more effec-\ntively, suggesting that some residual biases remain important\nfor certain tasks. In few-shot learning, for example, models\nthat excel without meta-learning are less sensitive to shifts\nin the training task distribution.\nDespite this, when mod-\nels are required to learn tasks that lie far outside the dis-\ntribution of tasks they encountered during meta-training, all\narchitectures—regardless of inductive bias—fail catastroph-\nically. This highlights that these general-purpose architec-\ntures may require stronger inductive biases for more robust\nforms of generalization but remain general enough to realize\na wide range of biases given appropriate choices for the initial\nweights and learning rate.\nBackground\nInductive biases—the assumptions that guide learning—can\nmanifest through the choice of model architecture, data repre-\nsentation, error metric, and training algorithm (Baxter, 2000).\nIn this work, we investigate the extent to which model ar-\nchitecture and data representation influence performance out-\ncomes after optimizing the initial weights and learning rate\nthrough meta-learning. This section introduces the kinds of\nbiases inherent to the neural architectures we explore and ad-\ndresses how meta-learning distills task-specific knowledge of\nthe learning problem into the training algorithm.\nInductive Biases across Neural Architectures\nMulti-Layer\nPerceptrons\n(MLPs)\nMLPs\n(Rosenblatt,\n1962) can approximate any function given sufficient depth\nand width (Hornik, Stinchcombe, & White, 1989) but make\nno explicit assumption about the structure of the input data\nbeyond static size. The lack of built-in equivariances make\nthem highly sensitive to nearly all spatial and temporal vari-\nations. All-to-all connections between layers imply global\nfeature mixing and deeper layers can capture progressively\nmore abstract representations.\nConvolutional Neural Networks (CNNs)\nCNNs (LeCun\net al., 1998) were designed with an explicit bias towards grid-\nstructured data such as images. Convolutions with shared\nweights prioritize spatially local relationships and ensure\ntranslation equivariance. Pooling layers provide partial ro-\nbustness to variations in scale, though CNNs generally lack\ninherent rotation or scale equivariances. CNNs preserve spa-\ntial order, making them sensitive to input permutations. Like\nMLPs, they build hierarchical features, with deeper layers\ncapturing abstract patterns composed of simpler ones. These\nrepresentations often resemble those in the mammalian visual\ncortex (Yamins & DiCarlo, 2016).\nLong Short-Term Memory Networks (LSTMs)\nLSTMs\n(Hochreiter & Schmidhuber, 1997) were designed to cap-\nture both long- and short-term dependencies in sequences by\nmaintaining an internal state that tracks temporal dynamics.\nInput, forget, and output gates regulate the flow of informa-\ntion, enabling the model to selectively retain or discard data.\nLSTMs rely on a memory structure with a bottleneck defined\nby the size of the hidden and cell states. They assume order\nmatters, making them sensitive to input permutations. Se-\nquential processing encodes position-awareness. Stacked lay-\ners allow LSTMs to capture hierarchical temporal patterns.\nTransformers (TFs)\nTransformers (Vaswani, 2017) are de-\nsigned to capture both local and global dependencies in se-\nquences using a self-attention mechanism. Unlike LSTMs,\nwhich process input sequentially, Transformers compute at-\ntention over all input positions simultaneously. Self-attention\nenables the model to dynamically focus on relevant parts of\nthe input, giving it direct access to long-range dependencies\ninstead of through memory as in the LSTM. Transformers\nrequire explicit positional encodings since they lack an inher-\nent sense of order. Stacked attention and feedforward layers\nenable the learning of hierarchical patterns, similar to deep\nCNNs when the input image is patched as is done with Vi-\nsion Transformers (Dosovitskiy et al., 2021).\nMeta-Learning\nWe adopt a meta-learning approach called Meta-SGD (Li,\nZhou, Chen, & Li, 2017), which learns a model initialization,\nlearning rate, and update direction that solves a set of tasks in\na fixed number of steps. Meta-SGD is an extension of Model-\nAgnostic Meta-Learning (MAML; Finn et al., 2017), which\nonly learns the model initialization. This enhanced flexibil-\nity allows for a more complete training algorithm that can\nquickly adapt to new, unseen tasks with minimal data. Dur-\ning meta-training, the goal is to learn a model initialization,\nlearning rate, and update direction that extract shared struc-\nture across tasks and embed inductive biases into the model\noutside what is explicitly defined by architecture or data rep-\nresentation.\nFormally, Meta-SGD aims to find parameters θ and α that\nminimize the expected test loss across tasks:\nmin\nθ,α Eτ∼p(τ)\n\u0002LTest\n\u0000θ′\u0001\u0003\n,\nwhere\nθ′ = θ−α⊙∇θLTrain(θ).\nHere, p(τ) denotes the task distribution, LTrain is the training\nloss for an individual task given a support set, and LTest is the\ntest loss given a query set. The parameter θ represents the\n\n\nTable 1: Hyperparameter search space for each architecture.\nArchitecture\nNumber of Layers\nHidden Width\nMLP\n{2,4,6,8}\n{8,16,32,64}\nCNN\n{2,4,6,8}\nSee Table 2.\nLSTM\n{1,2,3,4}\n{8,16,32,64}\nTransformer\n{1,2,3,4}\n{8,16,32,64}\nmodel’s initial weights, while α represents the learned task-\nspecific learning rate and step direction for adaptation.\nPrior work has demonstrated that meta-learning can embed\ninductive biases beyond those defined by a model’s archi-\ntecture. For example, Snell, Bencomo, and Griffiths (2024)\nshowed how meta-learned neural circuits can perform com-\nplex, task-specific probabilistic reasoning by distilling the bi-\nases required to perform nonparametric Bayesian inference.\nSimilarly, meta-learning has been applied to address the prob-\nlem of catastrophic forgetting, a major challenge in online\nlearning, by helping neural networks retain knowledge across\ntasks (Javed & White, 2019). For tasks that require produc-\ning novel combinations from known components, Lake and\nBaroni (2023) demonstrated meta-learning’s ability to distill\nhuman-like compositional skills into neural networks, despite\nFodor and Pylyshyn (1988) famously arguing that artificial\nneural networks lacked this capacity. While these approaches\nfocus on learning capabilities that neural networks may not\ninherently possess, our work investigates the extent to which\nspecific inductive biases can be encoded and expressed within\ndifferent architectures through meta-learning; see Abnar, De-\nhghani, and Zuidema (2020) for a different approach that en-\ncourages similarity between architectures by directly training\none architecture to reproduce the outputs from another, rather\nthan having different architectures meta-learn from the same\ntask distribution.\nApproach\nWe evaluate two types of scenarios to address constraints be-\ntween neural architectures, data representation, and training\nalgorithms. First, we assess the best-case scenario for meta-\nlearning, where test tasks are fully in-distribution and ample\nmeta-training data is provided. Second, we test more chal-\nlenging conditions, with test and training tasks from differ-\nent distributions and limited meta-training data, where other\nsources of bias are more persistent. For both cases, we also\ncompare performance against randomly-initialized baselines\nto highlight the impact of architectural bias without meta-\nlearned adaptations. We follow a consistent meta-learning\nprocedure across three tasks, defining variations in depth and\nwidth for each architecture, selecting models based on per-\nformance on a meta-validation set, meta-training each archi-\ntecture using Meta-SGD, and testing against controlled base-\nlines.\nTable 2: Hidden Widths for CNN.\n# Layers\nHidden Widths\n2\n{(2n,2n−1)|n ∈{4,5,6,7}}\n4\n{(2n,2n−1,2n−2,2n−3)|n ∈{4,5,6,7}}\n6\nSame as for 4 layers\n8\nSame as for 4 layers\nNeural Architectures\nTo isolate the core inductive biases, we remove non-essential\ncomponents, such as dropout. MLPs maintain a fixed hidden\nwidth across all layers, with batch normalization and ReLU\nactivation applied after each hidden layer. CNNs use 3 × 3\nkernels with a stride of 1 and zero-padding. Each convolu-\ntional layer is followed by batch normalization, a ReLU ac-\ntivation, and average pooling with a stride of 2. CNNs be-\ngin with either 2 or 4 convolutional layers (depending on the\narchitecture depth) followed by fully connected layers with\na fixed hidden width equal to the dimensionality of the fi-\nnal, flattened output of the convolution layers. LSTMs fol-\nlow the original implementation in Hochreiter and Schmid-\nhuber (1997) but use pre-layer normalization and a projec-\ntion layer to align input and hidden dimensions. Transform-\ners (Vaswani, 2017) use sinusoidal positional encoding, four\nattention heads, and a feedforward network with a dimension-\nality twice the hidden size. Pre-layer normalization and a pro-\njection layer are both used. There are 16 variations of each\narchitecture, with ranges over depth, width, and parameter\ncounts that are comparable (see Table 1).\nMeta-Training and Sampling Architectures\nWe meta-train each architecture using Meta-SGD (Li et al.,\n2017), with AdamW (Loshchilov & Hutter, 2017) as the outer\noptimizer. We set the learning rate to 0.001 and weight decay\nto 0.01. For all 16 variations of each architecture, we perform\n10,000 episodes of meta-training. After this initial phase, we\nselect the best-performing architecture based on its perfor-\nmance on a meta-validation set filled with 100 unseen training\ntasks and then continue optimizing the selected architecture\nto convergence. We repeat for 10 random seeds. This pro-\nduces 10 independent samples of meta-learned weights and\narchitectures for each architecture class. We meta-train with\nbatches of 4 tasks and use 1 adaptation step throughout.\nTasks\nWe consider three tasks: concept learning, modular arith-\nmetic, and few-shot learning with Omniglot (Lake, Salakhut-\ndinov, & Tenenbaum, 2011). Concept learning involves in-\ndistribution test tasks, providing an ideal scenario for meta-\nlearning to succeed. Modular arithmetic tests both in- and\nout-of-distribution generalization by splitting training and\ntesting tasks across different moduli. Few-shot classification\nwith Omniglot introduces a more complex scenario, where\nlimited training data leads to out-of-distribution test tasks.\n\n\nTable 3: Average accuracy for the concept learning task across different architectures, data types, and support set sizes. The table\ncompares performance under meta-learning (M1) and random initialization conditions with 1, 10, and 200 steps of AdamW\n(R1, R10, R200). All 95% confidence intervals (CIs) are below 0.01.\nnsupport = 5\nnsupport = 10\nnsupport = 15\nArch.\nData\nR1\nR10\nR200\nM1\nR1\nR10\nR200\nM1\nR1\nR10\nR200\nM1\nMLP\nImage\n0.499\n0.551\n0.635\n0.823\n0.515\n0.613\n0.775\n0.935\n0.489\n0.616\n0.859\n0.955\nCNN\nImage\n0.498\n0.588\n0.717\n0.842\n0.518\n0.633\n0.842\n0.937\n0.484\n0.632\n0.887\n0.961\nLSTM\nImage\n0.521\n0.623\n0.715\n0.849\n0.519\n0.661\n0.818\n0.946\n0.501\n0.667\n0.871\n0.964\nTF\nImage\n0.521\n0.638\n0.716\n0.858\n0.515\n0.674\n0.807\n0.943\n0.500\n0.691\n0.860\n0.964\nMLP\nBits\n0.505\n0.571\n0.671\n0.829\n0.510\n0.619\n0.799\n0.936\n0.484\n0.612\n0.862\n0.965\nLSTM\nBits\n0.528\n0.636\n0.734\n0.856\n0.498\n0.648\n0.859\n0.947\n0.493\n0.633\n0.908\n0.963\nTF\nBits\n0.505\n0.581\n0.702\n0.856\n0.503\n0.620\n0.804\n0.938\n0.489\n0.626\n0.864\n0.955\nHere, we assess networks’ sensitivity to task distribution\nshifts by meta-training on different Omniglot subsets.\nData Representation\nWe generate two types of data representations: 32×32 images\nand bitstring encodings. For concept learning, each concept is\nrepresented by a 4-bit feature vector. These features include\nattributes such as color (red or blue), shape (square or trian-\ngle), size (big or small), and pattern (striped or solid). We\nvisualize features as RGB images for input to the networks\n(see Figure 1). For modular arithmetic, input numbers are\nencoded as 8-bit binary strings and synthetically generated\nimages (see Figure 2). For our few-shot learning experiments\nwith Omniglot (Lake et al., 2011), we consider exclusively\nthe image data, downsampled to 32x32 for consistency across\ntasks.\nMLPs flatten the image input and process bitstrings as\nfloating-point vectors. CNNs operate exclusively on image\ninputs. LSTMs and Transformers divide each 32×32 image\ninto 4×4 patches, resulting in sequences of 64 tokens.\nMeta-Testing and Control Conditions\nWe generate 100 random tasks for 10 different seeds and\nmeta-test each of the 10 different models for each architec-\nture class on every seed. We compare to a baseline of a ran-\ndom initialization using the same 10 architectures and fit 1,\n10, 50, 100 and 200 steps of AdamW with a learning rate of\n0.001 and a weight decay of 0.01. All tasks had converged\nor began to overfit after 200 steps. We use mean square error\n(MSE) loss as a performance metric for modular arithmetic\nand prediction accuracy for concept learning and Omniglot.\nResults\nFor each of our three tasks, we evaluate the roles of architec-\nture, data representation, and training algorithms.\nConcept Learning\nThis experiment involves learning concepts based on objects\nthat have 4 features, represented as a 4-bit vector or an image.\nA concept, such as f1(x) = 0∧f3(x) = 1, assigns true or false\nlabels to the 16 possible objects based on whether they sat-\nisfy the concept. Following the procedure from Marinescu,\nMcCoy, and Griffiths (2024), we generate concepts from a\nprobabilistic context-free grammar (Goodman, Tenenbaum,\nFeldman, & Griffiths, 2008). Concepts are sampled for meta-\ntraining with variable support sizes and meta-tested on new\nconcepts with support sizes of 5, 10, and 15. We evaluate both\nmeta-learned models and randomly initialized models with\nvariable gradient step counts (see Table 3). The meta-learned\nmodels perform comparably across all architectures and both\ndata representations, with accuracy improving as support size\nincreases. In contrast, randomly initialized models trained\nwith 200 steps show significantly greater performance varia-\ntion across architectures and data representations, indicating\nthat meta-learning can not only enhance task performance but\nalso reduce the influence of architectural and representational\nbiases on model behavior under ideal conditions.\nModular Arithmetic\nWe frame modular arithmetic as a non-linear regression task\nover the integer domain [0,100) in this experiment. The goal\nis to infer the underlying function given noisy samples from\nsome modulus m. We explore two versions of the task: Odd-\nEven, where we meta-train with odd moduli and meta-test\nwith even moduli in the range [1,40], and 20-20, where we\nmeta-train with [1,20] and meta-test with [21,40]. We sample\nvariable support sizes during meta-training and meta-test with\nsupport set sizes 20, 40, and 100, where 20, 40 are uniformly\n0000\n0001\n0010\n0011\n0100\n0101\n0110\n0111\n1000\n1001\n1010\n1011\n1100\n1101\n1110\n1111\nFigure 1: Input data for all 16 objects used in concept-\nlearning with their bitstring and image representations.\n\n\nTable 4: Average MSE for the Odd-Even Modular Arithmetic Task. Meta-Val reports training moduli while Meta-Test reports\ntest moduli. The table compares performance under meta-learning (M1) and random initialization conditions with 1 and 10\nsteps of AdamW (R1, R10). All 95% CIs are below 0.05 for M1, 0.5 for R1, and 0.3 for R10.\nnsupport = 20\nnsupport = 40\nnsupport = 100\nArch.\nData\nMeta-Val\nMeta-Test\nMeta-Val\nMeta-Test\nMeta-Val\nMeta-Test\nMLP (M1)\nImage\n0.310\n0.978\n0.246\n0.895\n0.211\n0.984\nCNN (M1)\nImage\n0.441\n1.205\n0.350\n1.042\n0.293\n0.966\nLSTM (M1)\nImage\n0.123\n0.873\n0.116\n1.048\n0.113\n1.300\nTF (M1)\nImage\n0.884\n2.476\n0.527\n1.731\n0.460\n1.414\nMLP (M1)\nBits\n0.404\n1.021\n0.315\n1.041\n0.247\n1.050\nLSTM (M1)\nBits\n0.066\n0.997\n0.050\n0.813\n0.045\n2.057\nTF (M1)\nBits\n1.218\n2.095\n0.888\n1.351\n0.769\n1.566\nMLP (R1)\nImage\n40.482\n39.134\n35.161\n36.544\n32.130\n34.546\nCNN (R1)\nImage\n40.001\n41.962\n41.696\n37.731\n37.666\n37.431\nLSTM (R1)\nImage\n12.437\n21.215\n18.155\n20.419\n15.191\n19.602\nTF (R1)\nImage\n19.195\n16.217\n16.014\n17.809\n18.164\n16.270\nMLP (R1)\nBits\n38.627\n35.308\n38.724\n39.583\n36.076\n37.420\nLSTM (R1)\nBits\n7.012\n10.377\n9.211\n12.000\n8.387\n12.347\nTF (R1)\nBits\n16.298\n16.145\n13.475\n13.885\n12.355\n16.443\nMLP (R10)\nImage\n35.497\n35.056\n29.221\n28.507\n22.461\n24.813\nCNN (R10)\nImage\n33.965\n35.403\n33.180\n31.533\n26.195\n26.609\nLSTM (R10)\nImage\n11.585\n12.637\n11.565\n8.870\n8.722\n9.965\nTF (R10)\nImage\n11.837\n8.843\n8.259\n9.156\n8.680\n8.753\nMLP (R10)\nBits\n33.066\n31.307\n28.354\n29.302\n24.694\n25.449\nLSTM (R10)\nBits\n3.763\n5.252\n6.022\n5.956\n4.071\n4.665\nTF (R10)\nBits\n8.507\n8.415\n8.389\n6.834\n7.026\n9.658\nsampled and 100 includes every integer in the domain, offer-\ning a noisy version of the true moduli function over the entire\ndomain. Noise is injected via independent samples from a\nGaussian with σ = 0.1.\nWe find that this task produces variations across both\ndata representation and architecture in meta-learned models.\nMeta-Val reports the same moduli seen during meta-training\nbut with a different noise seed and Meta-Test reports unseen\nmoduli (see Table 4). Meta-learned LSTMs usually perform\nthe best across data representations and support sizes. Ev-\nery model performs reasonably well, to varying degrees, for\nin-distribution tasks but shows a drop in performance for out-\nof-distribution tasks in Odd-Even. The same is observed in\n20-20, but the drop in performance with out-of-distribution\ntasks is catastrophic (see Table 6): all meta-trained architec-\ntures have MSE’s > 50.0. This is likely due to the harder\nform of generalization that is required.\nRandomly initialized architectures start to overfit after just\n10 steps of AdamW across all support sizes in this task, de-\nspite AdamW’s explicit regularization towards simpler solu-\ntions. We report 1-step and 10-step updates (R1, R10). These\nrandomly initialized models are significantly outperformed\nby their meta-trained counterparts. Notably, large variations\nacross architectures and data representations are for the most\npart eliminated when the networks are meta-trained. LSTMs\nfar outperform the other architectures, perhaps explaining\ntheir superior performance when meta-trained.\nFew-Shot Learning\nWe replicate the 20-way, 5-shot Omniglot challenge (Lake\net al., 2011) to explore how task distribution affects perfor-\nmance across architectures. Handwritten alphabets are di-\nvided into four categories: Ancient (pre-500 A.D.), Asian,\nMiddle Eastern, and European. Each neural architecture is\nmeta-trained on these subsets. The fictional alphabets, Futu-\nrama and Magi, are excluded from the subsets but included in\nthe base task distribution, All, which contains all 30 training\nalphabets. The training baseline, N/A, denotes the random\ninitialization cases (R1, R10, R50, R100, R200) where we\noptimize for 1, 10, 50, 100, and 200 steps. All architectures\nare converged beyond this point.\nMeta-testing on the 20 held-out alphabets reveals drops in\nperformance for Ancient and Asian categories across all ar-\nchitectures (see Table 5). CNNs retain the highest percentage\nof their original accuracy compared to All, while Transform-\ners and LSTMs suffer larger drops. Transformers generalize\n\n\nTable 5: Average accuracy for the 20-way 5-shot classification task on the Omniglot experiment. The table compares perfor-\nmance under meta-learning (M1) and random initialization conditions with 1, 10, 50, 100, and 200 steps of AdamW (R1, R10,\nR50, R100, R200). Accuracy is reported for the full training set (All) and for subsets, including Ancient (12 alphabets), Asian\n(11 alphabets), Middle Eastern (7 alphabets), and European (5 alphabets). All 95% CIs are below 0.005.\nR1\nR10\nR50\nR100\nR200\nM1\nArch.\nN/A\nN/A\nN/A\nN/A\nN/A\nAll\nAncient\nAsian\nMiddle Eastern\nEuropean\nMLP\n0.056\n0.149\n0.270\n0.279\n0.285\n0.753\n0.601\n0.605\n0.753\n0.753\nCNN\n0.074\n0.403\n0.707\n0.726\n0.736\n0.949\n0.898\n0.905\n0.945\n0.948\nLSTM\n0.053\n0.056\n0.070\n0.078\n0.083\n0.260\n0.107\n0.146\n0.256\n0.256\nTF\n0.053\n0.075\n0.115\n0.118\n0.119\n0.896\n0.554\n0.614\n0.896\n0.896\nworse than MLPs when trained on Ancient alphabets. How-\never, generalization performance largely recovers on Middle\nEastern and European alphabets.\nMLPs and Transformers can outperform randomly initial-\nized CNNs when meta-trained. However, CNNs exhibit de-\nsirable equivariances, making them less sensitive to distribu-\ntion shifts and allowing them to achieve better random ini-\ntialization performance with fewer gradient steps. The perfor-\nmance gap between architectures narrows considerably under\nmeta-learning. However, LSTMs still struggle with the task\ndespite meta-learning improving their performance.\nDiscussion\nNeural network architectures are often designed with specific\nproblems in mind (e.g., next-word prediction, image classi-\nfication), so it is natural to expect them to perform poorly\non problems they were not explicitly designed for. Indeed,\nwe found that the standard neural networks, trained without\nmeta-learned inductive biases, perform significantly worse\nwhen the requirements of the task were misaligned with the\ninductive biases of the architecture. For example, tasks in-\nvolving bitstrings require an explicit understanding of po-\nsitional order, which is naturally encoded in sequential ar-\nchitectures like LSTMs and Transformers. As might be ex-\npected from these architectural properties, these architectures\nlearned faster and performed better than MLPs, which lack\nthis inductive bias. Similarly, in the Omniglot task, CNNs\noutperformed other architectures, likely due to their useful\nspatial inductive biases, demonstrating their superior ability\nto generalize when spatial structure is critical. However, even\n11000000\n3\n10001000\n17\n10111000\n29\n01010100\n42\nFigure 2: Input data for modular arithmetic for 4 example\nnumbers, with number, image, and bitstring representations.\nthese task-suitable inductive biases were not enough to enable\nmodels to perfectly solve these tasks.\nClassical analyses of learnability in cognitive science have\nargued for innate cognitive structures that support rapid learn-\ning from limited data, such as Chomsky’s notion of Uni-\nversal Grammar (UG) as a description of an innate device\nthat supports efficient language acquisition (Chomsky, 1980),\nand Fodor’s theory of domain-specific encapsulated mod-\nules (Fodor, 1983).\nNeural networks, by contrast, do not\nexplicitly have these structures.\nHowever, meta-learning\ncan embed task-specific knowledge into a network’s initial\nweights, allowing networks to overcome limitations in ar-\nchitecture or data representation when exposed to a suffi-\nciently rich task distribution. For example, McCoy and Grif-\nfiths (2023) showed that meta-learning enables networks to\nlearn linguistic patterns from a few examples, mimicking\nUG-like rapid learning. Similarly, Zintgraf, Shiarli, Kurin,\nHofmann, and Whiteson (2019) found that networks can de-\nvelop task-specific specializations with minimal data, resem-\nbling Fodor’s idea of modularity. Our findings reinforce these\nresults, showing that meta-learning vastly improves few-shot\nlearning performance and reduces variations across archi-\ntectures, suggesting that certain kinds of innate knowledge\ncan be implicitly expressed in neural networks (Elman et al.,\n1996) and that architecture is a weak constraint on what a\nneural network can do.\nMeta-learning still lacks the ability to distill stronger forms\nof generalization. Humans excel at both interpolation (learn-\ning within the range of observed examples) and extrapolation\n(generalizing beyond those examples). However, in modu-\nlar arithmetic, we showed some generalization capabilities\nwhen fitting moduli between known moduli (interpolation)\nin Odd-Even but catastrophic performance for 20-20, where\nthe testing moduli were far outside the training task domain\n(extrapolation). Meta-learning can perhaps then be viewed\nas a “blind” optimization process (Hasson, Nastase, & Gold-\nstein, 2020), where we can only distill structures present\nin the training task distribution but not outside of it.\nTo\nachieve stronger forms of generalization, we may need reg-\nularity that comes from outside of pure optimization alone,\nwhich is where architectural constraints come to play.\nIn\nour Omniglot experiments, the CNN was the least sensitive\n\n\nto shifts in the training task distribution, demonstrating how\narchitecture can make up the difference for what might not be\navailable in the training data. Alternatively, we can consider\ntechniques like reinforcement learning, that explicitly incen-\ntivize stronger forms of generalization (Akkaya et al., 2019),\nor variations on meta-learning that encourage the training al-\ngorithm to find structures outside of what the data can offer\n(Irie & Lake, 2024).\nConclusion\nWhile neural architectures do impose constraints on the kinds\nof problems neural networks can solve, these constraints\nare weak relative to the inductive biases afforded by initial\nweights. Meta-learning offers a path to distilling task-specific\nknowledge that is less influenced by the architecture and data\nrepresentation than typical training regimes. We conclude\nthat the flexibility of initial weights is extensive enough to\noverride the influence of architecture in some settings, but\nsubstantial architectural differences persist when extensive\ngeneralization beyond the meta-task distribution is required.\nAcknowledgments\nWe would like to thank Jake Snell and Logan Nelson for\ntheir valuable feedback and discussions that contributed to\nthis work. GB and TLG acknowledge grant DBI-2229929\nfrom the National AI Institute for Artificial and Natural Intel-\nligence (ARNI) and grant N00014-23-1-2510 from the Office\nof Naval Research, both of which supported this work. MG\nacknowledges support from the Princeton AI Teaching Fel-\nlowship. IM is supported by the NRT-HDR: FUTURE grant.\nWe also gratefully acknowledge the computational resources\nprovided by the Della high-performance computing cluster at\nPrinceton University, which were essential for meta-training\nthe large suite of models presented in this study.\nReferences\nAbnar, S., Dehghani, M., & Zuidema, W. (2020). Transfer-\nring inductive biases through knowledge distillation. arXiv\npreprint arXiv:2006.00555.\nAkkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., Mc-\nGrew, B., Petron, A., ... others (2019). Solving rubik’s\ncube with a robot hand. arXiv preprint arXiv:1910.07113.\nBaxter, J. (2000). A model of inductive bias learning. Journal\nof Artificial Intelligence Research, 12, 149–198.\nChen, L., Li, S., Bai, Q., Yang, J., Jiang, S., & Miao, Y.\n(2021). Review of image classification algorithms based\non convolutional neural networks. Remote Sensing, 13(22),\n4712.\nChomsky, N. (1980). Rules and representations. Columbia\nUniversity Press.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., ... Houlsby, N.\n(2021).\nAn image is worth 16x16 words: Transformers for im-\nage recognition at scale.\nIn International Conference\non Learning Representations (ICLR).\nRetrieved from\nhttps://arxiv.org/abs/2010.11929\nElman, J., Bates, E., Johnson, M. H., Karmiloff-Smith, A.,\nParisi, D., & Plunkett, K. (1996). Rethinking innateness:\nA connectionist perspective on development. MIT Press.\nFinn, C., Abbeel, P., & Levine, S. (2017). Model-agnostic\nmeta-learning for fast adaptation of deep networks. In In-\nternational Conference on Machine Learning (pp. 1126–\n1135).\nFodor, J. A. (1983). The modularity of mind. MIT press.\nFodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and\ncognitive architecture: A critical analysis. Cognition, 28(1-\n2), 3–71.\nGoodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths,\nT. L. (2008). A rational analysis of rule-based concept\nlearning. Cognitive Science, 32(1), 108–154.\nHasson, U., Nastase, S. A., & Goldstein, A. (2020). Direct\nfit to nature: an evolutionary perspective on biological and\nartificial neural networks. Neuron, 105(3), 416–434.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term\nmemory. Neural Computation, 9(8), 1735–1780.\nHornik, K., Stinchcombe, M., & White, H. (1989). Mul-\ntilayer feedforward networks are universal approximators.\nNeural Networks, 2(5), 359–366.\ndoi:\n10.1016/0893-\n6080(89)90020-8\nIrie, K., & Lake, B. M. (2024). Neural networks that over-\ncome classic challenges through practice. arXiv preprint\narXiv:2410.10596.\nJaved, K., & White, M. (2019). Meta-learning representa-\ntions for continual learning. Advances in Neural Informa-\ntion Processing Systems, 32.\nLake, B. M., & Baroni, M. (2023). Human-like systematic\ngeneralization through a meta-learning neural network. Na-\nture, 623(7985), 115–121.\nLake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2011).\nOne shot learning of simple visual concepts. In Proceed-\nings of the 33rd Annual Conference of the Cognitive Sci-\nence Society (pp. 2568–2573).\nLeCun, Y., Bottou, L., Bengio, Y., & Haffner, P.\n(1998).\nGradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11), 2278–2324.\nLi, Z., Zhou, F., Chen, F., & Li, H. (2017). Meta-SGD: Learn-\ning to learn quickly for few-shot learning. arXiv preprint\narXiv:1707.09835.\nLoshchilov, I., & Hutter, F. (2017). Decoupled weight decay\nregularization. In International Conference on Learning\nRepresentations.\nMarinescu, I., McCoy, R. T., & Griffiths, T. (2024). Distilling\nsymbolic priors for concept learning into neural networks.\nIn Proceedings of the Annual Meeting of the Cognitive Sci-\nence Society (Vol. 46).\nMcCoy, R. T., Grant, E., Smolensky, P., Griffiths, T. L., &\nLinzen, T. (2020). Universal linguistic inductive biases via\nmeta-learning. In Proceedings of the 42nd Annual Meeting\nof the Cognitive Science Society (CogSci) (pp. 132–138).\nCognitive Science Society.\nMcCoy, R. T., & Griffiths, T. L. (2023). Modeling rapid lan-\n\n\nguage learning by distilling bayesian priors into artificial\nneural networks. arXiv preprint arXiv:2305.14701.\nMitchell, T. M. (1997). Machine learning. New York: Mc-\nGraw Hill.\nRosenblatt, F. (1962). Principles of neurodynamics. Percep-\ntrons and the theory of brain mechanisms.\nRumelhart, D. E., & McClelland, J. L. (1986). On learn-\ning the past tenses of English verbs. In J. L. McClelland,\nD. E. Rumelhart, & the PDP research group (Eds.), Parallel\ndistributed processing: Explorations in the microstructure\nof cognition (Vol. 2). MIT Press.\nSnell, J. C., Bencomo, G., & Griffiths, T. L.\n(2024).\nA\nmetalearned neural circuit for nonparametric bayesian in-\nference.\nIn Advances in Neural Information Processing\nSystems.\nVaswani, A. (2017). Attention is all you need. Advances in\nNeural Information Processing Systems.\nYamins, D. L., & DiCarlo, J. J. (2016). Using goal-driven\ndeep learning models to understand sensory cortex. Nature\nNeuroscience, 19, 356–365. doi: 10.1038/nn.4244\nZintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., & Whiteson,\nS. (2019). Fast context adaptation via meta-learning. In\nInternational Conference on Machine Learning (pp. 7693–\n7702).\n\n\nAppendix\nAdditional Results: Modular Arithmetic\nTable 6 shows the average MSE errors over 10 sampled architectures for 10 random seeds for the 20-20 task, where the moduli\nwere split into a 1-20 training group and a 21-40 testing group. This task requires more robust forms of generalization since the\ntest task distribution is much farther away than the Odd-Even task reported in the main text. We found that performance across\nall meta-trained models was very poor on unseen tasks and comparable to the Odd-Even task on validation tasks. This suggests\nthat meta-learning failed to distill the knowledge necessary to generalize to unseen moduli functions, limiting its ability to fit\nmoduli beyond those encountered or closely related to those seen during training.\nIn Figures 5 and 6, we visualize representative curves for a single LSTM trained with image data and given 20 support points\non the 20-20 task. Performance is near perfect on in-distribution tasks but the LSTM attempts to fit curves that resemble training\nmoduli despite significant signal to support a different function. Figures 3 and 4 show the same curves but for the Odd-Even\nversion of the modular arithmetic task that we report in the main text.\nnsupport = 20\nnsupport = 40\nnsupport = 100\nArch.\nData\nMeta-Val\nMeta-Test\nMeta-Val\nMeta-Test\nMeta-Val\nMeta-Test\nMLP (M1)\nImage\n0.224\n62.303\n0.137\n52.401\n0.085\n53.860\nCNN (M1)\nImage\n0.291\n65.640\n0.204\n59.214\n0.127\n55.021\nLSTM (M1)\nImage\n0.072\n89.356\n0.049\n87.886\n0.042\n92.151\nTF (M1)\nImage\n0.265\n79.551\n0.203\n77.452\n0.163\n84.379\nMLP (M1)\nBits\n0.248\n60.441\n0.165\n52.989\n0.099\n52.352\nLSTM (M1)\nBits\n0.027\n99.372\n0.021\n103.318\n0.018\n112.794\nTF (M1)\nBits\n0.275\n71.229\n0.270\n75.373\n0.200\n77.683\nMLP (R1)\nImage\n27.606\n66.857\n23.333\n55.392\n22.971\n45.168\nCNN (R1)\nImage\n28.265\n63.105\n24.337\n51.644\n22.828\n43.167\nLSTM (R1)\nImage\n5.229\n56.915\n6.812\n51.979\n7.534\n50.939\nTF (R1)\nImage\n11.432\n54.120\n11.611\n50.490\n13.394\n47.850\nMLP (R1)\nBits\n26.195\n61.145\n22.190\n50.674\n21.967\n40.578\nLSTM (R1)\nBits\n3.895\n47.907\n4.837\n40.954\n4.258\n37.623\nTF (R1)\nBits\n8.872\n47.622\n9.543\n43.548\n9.126\n42.618\nMLP (R10)\nImage\n24.947\n60.052\n19.750\n47.102\n16.940\n31.091\nCNN (R10)\nImage\n24.992\n55.891\n20.042\n43.738\n16.926\n29.164\nLSTM (R10)\nImage\n4.421\n57.613\n4.386\n50.533\n4.892\n49.009\nTF (R10)\nImage\n6.381\n49.464\n5.687\n45.311\n5.393\n39.720\nMLP (R10)\nBits\n22.877\n52.845\n17.809\n39.234\n15.418\n25.106\nLSTM (R10)\nBits\n2.331\n45.800\n2.525\n35.331\n2.174\n21.841\nTF (R10)\nBits\n6.782\n47.109\n5.671\n42.674\n4.163\n38.185\nTable 6: Average MSE for the 20-20 Modular Arithmetic Task. Meta-Val reports training moduli while Meta-Test reports test\nmoduli. The table compares performance under meta-learning (M1) and random initialization conditions with 1 and 10 steps\nof AdamW (R1, R10). All 95% confidence intervals (CIs) are below 0.05 on Meta-Val and 0.5 on Meta-Test for M1, below 0.5\nfor R1, and below 0.3 for R10.\n\n\nFigure 3: Visualization of Meta-Validation curve fitting for Odd-Even task using a meta-trained LSTM with image inputs and\n20 support points. LSTMs were meta-trained on odd moduli (shown above) and meta-tested on even moduli. Steps 0 denotes\nthe function before observing the support set (green). Steps 1 (red) shows the adaptation after 1 step of gradient descent. True\nfunction (blue) denotes the ground truth moduli function.\nFigure 4: Visualization of Meta-Test curve fitting for Odd-Even task using a meta-trained LSTM with image inputs and 20\nsupport points. LSTMs were meta-trained on odd moduli and meta-tested on even moduli (shown above).\n\n\nFigure 5: Visualization of Meta-Validation curve fitting for 20-20 task using a meta-trained LSTM with image inputs and 20\nsupport points. LSTMs were meta-trained on moduli 1-20 (shown above) and meta-tested on moduli 21-40.\nFigure 6: Visualization of Meta-Test curve fitting for 20-20 task using a meta-trained LSTM with image inputs and 20 support\npoints. LSTMs were meta-trained on moduli 1-20 and meta-tested on moduli 21-40 (shown above).\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20237v1.pdf",
    "total_pages": 11,
    "title": "Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks",
    "authors": [
      "Gianluca Bencomo",
      "Max Gupta",
      "Ioana Marinescu",
      "R. Thomas McCoy",
      "Thomas L. Griffiths"
    ],
    "abstract": "Artificial neural networks can acquire many aspects of human knowledge from\ndata, making them promising as models of human learning. But what those\nnetworks can learn depends upon their inductive biases -- the factors other\nthan the data that influence the solutions they discover -- and the inductive\nbiases of neural networks remain poorly understood, limiting our ability to\ndraw conclusions about human learning from the performance of these systems.\nCognitive scientists and machine learning researchers often focus on the\narchitecture of a neural network as a source of inductive bias. In this paper\nwe explore the impact of another source of inductive bias -- the initial\nweights of the network -- using meta-learning as a tool for finding initial\nweights that are adapted for specific problems. We evaluate four widely-used\narchitectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430\ndifferent models across three tasks requiring different biases and forms of\ngeneralization. We find that meta-learning can substantially reduce or entirely\neliminate performance differences across architectures and data\nrepresentations, suggesting that these factors may be less important as sources\nof inductive bias than is typically assumed. When differences are present,\narchitectures and data representations that perform well without meta-learning\ntend to meta-train more effectively. Moreover, all architectures generalize\npoorly on problems that are far from their meta-training experience,\nunderscoring the need for stronger inductive biases for robust generalization.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}