{
  "id": "arxiv_2502.20132v2",
  "text": "Regional climate projections using a deep-learning–based\nmodel-ranking and downscaling framework: Application to\nEuropean climate zones\nParthiban Loganathan1*, Elias Zea1, Ricardo Vinuesa1, Evelyn Otero1\n1Department of Engineering Mechanics, KTH Royal Institute of Technology,\nStockholm, Sweden.\n*Corresponding author(s). E-mail(s): parthi@kth.se;\nAbstract\nAccurate regional climate forecast calls for high-resolution downscaling of Global Climate Mod-\nels (GCMs). This work presents a deep-learning-based multi-model evaluation and downscaling\nframework ranking 32 Coupled Model Intercomparison Project Phase 6 (CMIP6) models using\na Deep Learning-TOPSIS (DL-TOPSIS) mechanism and so refines outputs using advanced deep-\nlearning models. Using nine performance criteria, five K¨oppen-Geiger climate zones—Tropical,\nArid, Temperate, Continental, and Polar—are investigated over four seasons. While TaiESM1\nand CMCC-CM2-SR5 show notable biases, ranking results show that NorESM2-LM, GISS-E2-\n1-G, and HadGEM3-GC31-LL outperform other models. Four models contribute to downscaling\nthe top-ranked GCMs to 0.1° resolution. Vision Transformer (ViT), Geospatial Spatiotemporal\nTransformer with Attention and Imbalance-Aware Network (GeoSTANet), CNN-LSTM, CNN-\nLong Short-Term Memory (ConvLSTM). Effectively capturing temperature extremes (TXx, TNn),\nGeoSTANet achieves the highest accuracy (Root Mean Square Error (RMSE) = 1.57°C, Kling-\nGupta Efficiency (KGE) = 0.89, Nash-Sutcliffe Efficiency (NSE) = 0.85, Correlation (r) = 0.92, so\nreducing RMSE by 20% over ConvLSTM. CNN-LSTM and ConvLSTM do well in Continental and\nTemperate zones; ViT finds fine-scale temperature fluctuations difficult. These results confirm that\nmulti-criteria ranking improves GCM selection for regional climate studies and transformer-based\ndownscaling exceeds conventional deep-learning methods. This framework offers a scalable method\nto enhance high-resolution climate projections, benefiting impact assessments and adaptation plans.\nKeywords Climate Downscaling, GCM Ranking, High-Resolution Climate Projections, K¨oppen-\nGeiger Climate Zones, Regional Climate Impact, Transformer-Based Downscaling.\n1 Introduction\nConsidered one of the most pressing global concerns of the twenty-first century, climate change has\npossibly major consequences on ecosystems, infrastructure, and society all around Summary for Pol-\nicymakers (2023). Understanding and forecasting climate dynamics has never been more important\nas world temperatures rise and severe storms become more frequent. Expanding our scientific knowl-\nedge of past, current, and future climatic conditions in this setting depends on numerical models\nthat faithfully depict the intricate physical processes of the Earth system. Among climate modelling\nagencies, the Coupled Model Intercomparison Project (CMIP) has become a core cooperative effort\npromoting worldwide cooperation and standardizing GCMs Eyring et al. (2016). Currently in its sixth\nphase (CMIP6), the project has driven the creation of ever more advanced GCMs simulating climate\nvariability and change with improved realism Intergovernmental Panel on Climate Change (IPCC)\n(2023). These models usually run at coarse resolutions—often between 50 and 250 km grid cells—that\n1\narXiv:2502.20132v2  [cs.LG]  28 Feb 2025\n\n\nare insufficient for resolving localized climate events, even if they provide insightful analysis at global\nand continental scales. Such coarse resolution reduces the capacity to represent important regional\naspects including localized precipitation extremes, temperature changes in complicated topography,\nand subtle land-sea contrasts Fowler et al. (2007); Haarsma et al. (2016). Effective regional climate\nimpact assessments and adaptation planning depend on downscaling techniques that can convert\ncoarse GCM data into high-resolution localized estimates, so there is a great demand for them.\nEmerging as a vital answer to the gap between the coarse spatial resolution of GCM outputs and\nthe high-resolution data needed for local decision-making are downsizing methods. Dynamic and sta-\ntistical downscaling are the two main approaches that are now in use. Dynamic downscaling nests\nhigh-resolution simulations inside the boundary conditions given by global models using Regional\nClimate Models (RCMs). This method enables the explicit simulation of local physical processes,\nincluding those affected by intricate topography and land-use features Rosenzweig et al. (2014); Cava-\nzos et al. (2024). Statistical downscaling techniques, on the other hand, depend on experimentally\ngenerated connections between local observations and major climate factors. Fine-resolution climate\ndata has been produced using extensively applied techniques like quantile mapping, weather typing,\nweather generators, and regression-based processes. Generally speaking, statistical approaches are\npreferred because of their reduced computational requirements and capacity to include site-specific\nobservational data—which can be very helpful when addressing localized climate events von Storch\n(2011). Both dynamic and statistical downscaling techniques have difficulties notwithstanding their\nstrengths, especially in areas with high climatic fluctuation where observational data may be few. The\nneed to choose the most appropriate GCMs for downscaling becomes even more evident as regional\nstudies progressively need customized climate projections, enabling sophisticated model evaluation\nand selection methods.\nChoosing the best GCM for regional climate research is a difficult and important chore since dif-\nferent models may show differing performances depending on the certain region and climate variable\nunder examination. Variability in model physics, parameterizations, initial circumstances, and the\nmodelling of important feedback processes means no single model can routinely outperform others\nacross all scenarios Riahi et al. (2017); Santer et al. (2014). Multi-criteria decision-making (MCDM)\ntechniques provide a disciplined framework for assessing several, occasionally contradictory, perfor-\nmance indicators to negotiate this complexity. The Technique for Order Preference by Similarity\nto Ideal Solution (TOPSIS), which ranks options by evaluating their proximity to an ideal solution\nwhile concurrently calculating their distance from the anti-ideal, or worst-case scenario Chen et al.\n(1981); Yoon and Hwang (1995), is one well-established MCDM method. Conversely, conventional\nTOPSIS implementations depend on either fixed or subjectively set weights for every performance\ncriterion, a method that could result in biased or inconsistent model selections Wang et al. (2009).\nDL-TOPSIS has been created as a novel approach to solve this restriction. DL-TOPSIS offers a more\nobjective and flexible ranking system by learning a neural network to determine the ideal weights\nstraight from the data. This method combines a wide range of performance criteria—including bias,\nRoot Mean Square Error (RMSE), Pearson correlation (r), Kling-Gupta Efficiency (KGE), Nash-\nSutcliffe Efficiency (NSE), and measures of distribution overlap—to guarantee that the chosen GCM\nis best suited for catching the climatic characteristics relevant to a given region Dwivedi et al. (2019);\nSchuster (2004).\nDefined as the variation between the daily maximum (tasmax) and minimum (tasmin) tempera-\ntures, the diurnal temperature range (DTR) has become more important as a climatic indicator with\nbroad effects on agricultural output, human health, and ecological systems Makowski et al. (2008);\nDai et al. (1999). A significant indicator of underlying changes in air circulation, cloud cover, and\nland–surface interactions, variations in DTR can reflect Wild et al. (2017); Alexander et al. (2006)\nclimatic dynamics. Downscaling attempts historically concentrated on mean temperature or precip-\nitation, but increasing attention is being paid to precisely capturing the DTR. Most statistical early\ndownscaling strategies use quantile mapping, weather typing, and linear regression. While these meth-\nods are computationally efficient, they frequently fail to adequately depict non-linear connections\nand intricate inter-dependencies between climatic variables Gudmundsson et al. (2012).\nDeep learning has transformed pattern identification in big datasets in recent years thanks to\nearly applications modelling correlations between coarse GCM outputs and local observations using\nmulti-layer perceptrons (MLPs) Alzubaidi et al. (2021); Zorita and von Storch (1999). Recent devel-\nopments in deep learning methods spanning from convolutional neural networks to transformer-based\narchitectures have shown a strong ability for modelling and predicting challenging physical events\nin atmospheric modelling Jardines et al. (2024); Solera-Rico et al. (2024); Yousif et al. (2023).\n2\n\n\nMLPs could not, however, efficiently use spatial information included in climate data. By allowing\nthe extraction of spatial information capturing localized meteorological events, Convolutional Neu-\nral Networks (CNNs) offered a major improvement. Recurrent Neural Networks (RNNs) and Long\nShort-Term Memory (LSTM) networks have been used concurrently to model temporal dependen-\ncies, such as seasonal cycles and sequential variability, so improving the capacity to replicate daily\nand seasonal oscillations in DTR Vandal (2018); Hochreiter and Schmidhuber (1997). To handle the\ncomplexity of downscaling DTR Goodfellow et al. (2017); Zhang et al. (2019), hybrid models com-\nbining CNNs with LSTMs, as well as methods using Generative Adversarial Networks (GANs), have\nshown great potential.\nBuilding on these advances, transformer-based deep-learning models have brought fresh capabil-\nities for climate downscaling. Representing the forefront in modelling spatiotemporal climate data,\nViTs and the specialized GeoSTANet capture long-range spatial dependencies using self-attention\nmechanisms, therefore allowing the modelling of broad atmospheric circulation patterns. Though\nViTs show great promise, they can find it challenging to generalize across areas with quite different\nclimate conditions. Conversely, GeoSTANet is especially meant to solve these difficulties by combin-\ning transformer-based attention mechanisms with an imbalance-aware training approach, therefore\nenabling it to thrive in both catching extreme occurrences and regular climate patterns. In recreating\nfine-scale characteristics, diurnal cycles, and seasonal transitions, these advanced hybrid architec-\ntures—including CNN-LSTM and ConvLSTM models—have been demonstrated to be quite superior\nto conventional statistical methods Zhu et al. (2023). Notwithstanding their remarkable performance,\nthese deep-learning techniques have several difficulties including the need for large, high-quality\ndatasets like ERA5 data Hersbach et al. (2020), rising computational demands connected with high-\nresolution spatiotemporal grids Dueben and Bauer (2018), possible over-fitting risk, and problems\nwith model interpretability Ham et al. (2019); Hunt et al. (2021). Reliable support of advanced\ndownscaling models for regional climate impact assessments depends on addressing these problems.\nThe present study defines five climate zones (Tropical, Arid, Temperate, Continental, and Polar)\nreflecting the approximate climatological variations across the region, each characterized by unique\ntemperature and precipitation patterns influencing GCM performance and the representation of\nextremes Cohen et al. (2014); Cook et al. (2016). The present work’s core argument is that the initial\nchoice of the best-performing GCMs for a specific region determines optimal downscaling perfor-\nmance Loganathan and Mahindrakar (2020a,b, 2021). First, the DL-TOPSIS methodology is used\nto objectively rank CMIP6 models based on a comprehensive set of performance metrics includ-\ning extreme temperature indicators maximum tasmax (TXx) and lowest tasmin (TNn), Probability\nDensity Function overlap (PDF Overlap), standard deviation differences (SD Diff) as well as tra-\nditional metrics like bias, RMSE, correlation, NSE, and KGE Hempel et al. (2013); Xie and Arkin\n(1997). Advanced deep-learning downscaling methods including CNN-LSTM, ConvLSTM, ViT, and\nGeoSTANet are then fed, top-ranked models. Essential for climate services and impact assessments\nin sectors including agriculture, urban planning, health, and energy, this combined approach not\nonly reduces the transmission of biases from coarse-scale models but also improves the accuracy of\nhigh-resolution forecasts Olsson et al. (2015); Urban and Fricker (2010). Moreover, by using high-\nperformance computational resources and large-scale observational data, the proposed methodology\noffers a scalable and repeatable blueprint for closing the gap between local adaptation techniques and\nglobal climate modelling Eyring et al. (2019); Rasp et al. (2018). In the end, this multidisciplinary\napproach merging advanced machine learning with operations research has the potential to produce\nmore accurate and useful climate projections for practitioners and legislators all around.\n2 Study area and Data description\nAdvanced downscaling methods find a useful proving ground in Europe’s varied climatic zones, which\nrange from Mediterranean beaches and temperate coastal areas to continental interiors and Arctic\ntundra Beck et al. (2018); Kottek et al. (2006). From the warm Mediterranean basin to the Arctic\ntundra, Europe offers one of the most climatically varied areas worldwide and is thus a perfect testbed\nfor assessing climate models and downscaling methods. Based on temperature and precipitation\npatterns, the K¨oppen-Geiger climate classification offers a disciplined approach to split the continent\ninto five main zones: Tropical, Arid, Temperate, Continental, and Polar. These categories allow one\nto evaluate model performance geographically with a customizing effect. Different seasonal changes\nin every zone affect local temperature extremes, air circulation patterns, and precipitation dynamics.\nThe North Atlantic Oscillation (NAO) significantly affects Winter, thereby influencing storm courses\n3\n\n\nand temperature variation. Spring marks a change in temperature and a changing precipitation\nzone phase. Heat waves and convective activity define Summer, especially in Temperate and Arid\nzones; Autumn marks a resumption of baroclinic activity and mid-latitude storm systems. Evaluating\nmodel performance depends on these seasonal dynamics as various GCMs might struggle with certain\nregional climatic variables. Figure 1 shows the K¨oppen-Geiger classification for Europe, stressing the\nspatial distribution of the climatic zones applied in this work.\nFig. 1: Spatial representation of K¨oppen-Geiger climate zones classification across Europe.\nThirty-two CMIP6 models were chosen to evaluate climate model accuracy using a variety of geo-\ngraphical resolutions (50–250 km) and many parameterizing approaches. Coordinated by the World\nClimate Research Program (WCRP), the CMIP6 repository offers the most developed suite of GCMs\naccessible for historical and future climate simulations. Different physical parameterization, land-\natmosphere coupling, and climate sensitivity among these models affect their capacity to replicate\nregional climatic variability. Data availability, the inclusion of tasmax and tasmin, and the adop-\ntion of a standardized r1i1p1f1 ensemble member to provide consistent initialization-guided model\nselection. Developed by ECMWF, observational benchmarks originate from ERA5 data and offer\nhigh-resolution data at 0.1° x 0.1° (∼10km). ERA5 produces continuous, high-quality climate records\nby combining satellite, in situ, and reanalysis data, unlike crude GCM outputs. Selected to provide\nconsistent assessment between CMIP6 historical simulations and observational data was the histori-\ncal period 1985–2014. Table 1 gives a summary of the thirty-two CMIP6 GCMs together with their\ngeographic resolution, original institution, and salient features.\n4\n\n\nTable 1: List of CMIP6 models used in the study.\nS. No\nModel\nInstitute (Country)\nResolution (km)\nKey Features\n1\nACCESS-CM2\nCSIRO (Australia)\n∼100\nImproved ocean-atmosphere coupling\n2\nACCESS-ESM1-5\nCSIRO (Australia)\n∼100\nEarth System Model with Advanced Carbon Cycle\n3\nBCC-CSM2-MR\nBeijing Climate Center (China)\n∼100\nMedium resolution; aerosol-cloud interactions\n4\nBCC-ESM1\nBeijing Climate Center (China)\n∼280\nCoupled Earth System Model with biogeochemistry\n5\nCAMS-CSM1-0\nChinese Academy of Meteorological Sciences\n∼100\nAtmospheric physics enhancements\n6\nCAS-ESM2-0\nChinese Academy of Sciences (China)\n∼100\nIntegrated land-vegetation-atmosphere coupling\n7\nCESM2\nNCAR (USA)\n∼110\nBiogeochemical cycles; high-resolution processes\n8\nCESM2-FV2\nNCAR (USA)\n∼50\nA high-resolution version of CESM2\n9\nCESM2-WACCM\nNCAR (USA)\n∼110\nWhole Atmosphere Coupling; upper-atmosphere focus\n10\nCIESM\nChinese Institute of Earth System Modeling\n∼100\nImproved monsoon simulations\n11\nCMCC-CM2-SR5\nCMCC (Italy)\n∼100\nHigh-resolution atmosphere-ocean coupling\n12\nCMCC-ESM2\nCMCC (Italy)\n∼100\nDynamic vegetation and Earth system processes\n13\nCNRM-CM6-1\nCNRM (France)\n∼100\nAdvanced surface energy balance representation\n14\nCNRM-ESM2-1\nCNRM (France)\n∼100\nEarth System Model with carbon-climate feedbacks\n15\nCanESM5\nCCCma (Canada)\n∼250\nEnhanced aerosol-radiation interactions\n16\nEC-Earth3\nEC-Earth Consortium (Europe)\n∼100\nFocus on European climate variability\n17\nFGOALS-f3-L\nIAP (China)\n∼100\nPrecipitation physics improvement\n18\nFGOALS-g3\nIAP (China)\n∼280\nGlobal hydrological cycle representation\n19\nGFDL-CM4\nGFDL (USA)\n∼100\nHigh-resolution ocean-atmosphere coupling\n20\nGFDL-ESM4\nGFDL (USA)\n∼100\nEarth System Model with advanced biogeochemistry\n21\nINM-CM4-8\nINM (Russia)\n∼150\nCoupled dynamical processes\n22\nINM-CM5-0\nINM (Russia)\n∼150\nImproved cloud parametrization\n23\nIPSL-CM6A-LR\nIPSL (France)\n∼250\nAdvanced aerosol-cloud interactions\n24\nKACE-1-0-G\nKMA (Korea)\n∼100\nFocus on regional climate dynamics\n25\nMIROC6\nMIROC (Japan)\n∼100\nMulti-scale atmosphere-ocean coupling\n26\nMPI-ESM1-2-HR\nMPI-M (Germany)\n∼100\nA high-resolution version of MPI Earth System Model\n27\nMPI-ESM1-2-LR\nMPI-M (Germany)\n∼250\nLow-resolution Earth System Model\n28\nMRI-ESM2-0\nMRI (Japan)\n∼100\nOcean biogeochemistry and carbon dynamics\n29\nNESM3\nNUIST (China)\n∼100\nImproved monsoon and hydrological cycles\n30\nNorESM2-LM\nNCC (Norway)\n∼250\nLow-resolution Nordic Earth System Model\n31\nNorESM2-MM\nNCC (Norway)\n∼100\nMedium-resolution Nordic Earth System Model\n32\nUKESM1-0-LL\nMet Office (UK)\n∼100\nCoupled Earth System Model with land-atmosphere focus\n5\n\n\nPerformance measures are computed at a constant 0.1° × 0.1° spatial grid for complete model\nevaluation, guaranteeing a direct comparison between ERA5 and regridded CMIP6 results. Calcu-\nlated from tasmax and tasmin, the DTR is a fundamental indication of radiative and land-surface\nprocesses. Multiple statistical tests are run to measure model biases, including Bias, RMSE, r, KGE,\nNSE, and PDF Overlap. These measurements evaluate multi-dimensional model quality by cap-\nturing mean biases, variability, distributional integrity, and extreme event representation. Seasonal\nstratification and zone-based categorization let one thoroughly examine model strengths and short-\ncomings in several climatic regimes. Following downscaling studies will apply the processed datasets\nand assessment methodology to guarantee that high-resolution climate forecasts are produced from\nthe best-performing models.\n3 Methodology\nThis study proposes a hybrid DL-TOPSIS framework to rank CMIP6 climate models based on their\nhistorical performance and applies advanced deep-learning architectures for statistical downscaling.\nThe methodological workflow consists of three primary blocks, as presented in Figure 2:\nFig. 2: Methodology workflow for model ranking and downscaling.\n• Data Block: Daily maximum and minimum temperature data (tasmax, tasmin) from ERA5\nreanalysis (observations) and 32 CMIP6 models (simulations) are pre-processed.\n• Ranking Block: The top CMIP6 models are identified using a DL-TOPSIS ranking system, which\ndynamically assigns weights to performance metrics.\n• Downscaling Block: The selected top GCM models are downscaled to high-resolution cli-\nmate projections using deep-learning architectures such as CNN-LSTM, ConvLSTM, ViT, and\nGeoSTANet.\nTo ensure a strong model assessment, both ranking and downscaling steps use an independent\nset of performance measures. The proposed framework improves regional climate projections for\nadaptation and mitigating strategies and allows a fair evaluation of climate model integrity.\n6\n\n\n3.1 Data pre-processing block:\nThe data pre-processing stage ensures that ERA5 and CMIP6 datasets are aligned for direct\ncomparison and statistical downscaling. This process involves multiple steps:\n1. Data acquisition: a) Observational data: ERA5 reanalysis dataset (0.1° × 0.1° resolution,\n1985–2014), and b) Climate model simulations: 32 CMIP6 models, historical experiments (r1i1p1f1\nensemble), daily tasmax and tasmin.\n2. Regridding and land-only masking: CMIP6 models have varying spatial resolutions (50 km to 250\nkm). To ensure smooth processing, all CMIP6 GCM outputs are scaled using bilinear interpolation\nto match the ERA5 grid (0.1° × 0.1°), and a land-only mask is applied to exclude oceanic regions\nto maintain consistency in model evaluation.\n3. Computation of the DTR: DTR is derived from tasmax and tasmin for both ERA5 and CMIP6\ndatasets:\nDTR = tasmax −tasmin\n(1)\nDTR is an important climate analysis metric, representing the day-night temperature contrast\nand serving as a regional climate variability indicator.\n4. Seasonal and climate zone classification: For climate-specific assessment, the data is categorized\naccording to: a) Seasons: 1) Winter (December, January, and February), 2) Spring (March, April,\nand May), 3) Summer (June, July, and August), 4) Autumn (September, October, and November),\nand 5) Annual; b) Climate Zones: The K¨oppen-Geiger classification is used to classify data into\nsix zones: 1) Tropical, 2) Arid, 3) Temperate, 4) Continental, 5) Polar, and 6) Entire Europe.\n5. Calculation of evaluation metrics: A comprehensive set of statistical and physical performance\nmetrics is computed to assess the fidelity of each CMIP6 model: a) Bias, b) RMSE, c) KGE, d)\nNSE, e) r, f) PDF Overlap, g) Extreme temperature indices: TXx and TNn. The pre-processing\nsteps standardize the ERA5 and CMIP6 datasets, facilitating impartial model evaluation and\nefficient statistical downscaling in later phases. The performance evaluation metrics and their\ndescriptors are shown in Table 2.\n7\n\n\nTable 2: Performance metrics for CMIP6 model evaluation.\nParameter\nFormula\nSignificance\nRMSE (Root Mean Square Error)\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(Mi −Oi)2\nMeasures overall deviation between predictions and observations; lower is\nbetter.\nBias\n1\nn\nn\nX\ni=1\n(Mi −Oi)\nQuantifies systematic error; values near zero indicate minimal bias.\nNSE (Nash-Sutcliffe Efficiency)\n1 −\nPn\ni=1(Oi −Mi)2\nPn\ni=1(Oi −¯O)2\nEvaluates predictive skill; values close to 1 indicate high performance.\nKGE (Kling-Gupta Efficiency)\n1 −\np\n(r −1)2 + (β −1)2 + (γ −1)2\nβ = µM/µO,\nγ = (σM/µM)/(σO/µO)\nCombines correlation, bias, and variability; optimal value is 1.\nr2 (Coefficient of Determination)\n\n\nPn\ni=1(Mi −¯\nM)(Oi −¯O)\nqPn\ni=1(Mi −¯\nM)2 Pn\ni=1(Oi −¯O)2\n\n\n2\nProportion of variance in observations explained by the model; higher values\nindicate better performance.\nr (Correlation Coefficient)\nPn\ni=1(Mi −¯\nM)(Oi −¯O)\nqPn\ni=1(Mi −¯\nM)2 Pn\ni=1(Oi −¯O)2\nMeasures linear association; values near 1 indicate strong positive correlation.\nPDF (Probability Density Function Overlap)\nZ\nmin\n\u0010\nPM(x), PO(x)\n\u0011\ndx\nQuantifies similarity between model and observed distributions; 1 indicates\nperfect overlap.\nTXx (Maximum Temperature)\nmax\n\u0000tasmax\n\u0001\nIndicates extreme high temperatures; critical for assessing heat events.\nTNn (Minimum Temperature)\nmin\n\u0000tasmin\n\u0001\nIndicates extreme low temperatures; essential for evaluating cold events.\n8\n\n\n3.2 Model ranking block:\nA hybrid DL-TOPSIS architecture objectively ranks CMIP6 models throughout several climate zones\nand seasons. This method combines multi-criteria decision-making (MCDM) methodologies with a\nneural network-based dynamic weighting mechanism to guarantee a strong and data-driven ranking\nof models. Through their proximity to an ideal solution and distance from an anti-ideal solution,\nthe Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) ranks models. This\nguarantees that the models with maximum performance also minimize bias and maximize correlation\nwith observable data. The TOPSIS rating comprises five steps:\n1. Normalization of performance metrics: Each performance metric Cij for model i and metric j is\nnormalized to remove scale variations:\nNij =\nCij\nqPm\ni=1 C2\nij\n(2)\nwhere, Cij is the computed value of metric j for model i, and m is the total number of CMIP6\nmodels.\n2. Weighted normalization: Each normalized metric is multiplied by its respective weight wj, which\nis learned dynamically using a neural network:\nWij = wj · Nij\n(3)\n3. Calculation of ideal and anti-ideal solutions: The ideal solution represents the best possible values\nacross all models, while the anti-ideal solution represents the worst:\nA+ = {max(Wij), for benefit metrics; min(Wij), for cost metrics},\n(4)\nA−= {min(Wij), for benefit metrics; max(Wij), for cost metrics},\n(5)\nwhere, the higher the benefit metrics (KGE, NSE, correlation, and PDF overlap) the better, and\nthe lower the cost metrics (Bias, RMSE) the better.\n4. Euclidean distance calculation: The Euclidean distance of each model from the ideal and anti-ideal\nsolutions is computed:\nD+\ni =\nv\nu\nu\nt\nn\nX\nj=1\n(Wij −A+\nj )2,\nD−\ni =\nv\nu\nu\nt\nn\nX\nj=1\n(Wij −A−\nj )2\n(6)\nwhere, D+\ni represents the distance from the ideal solution, and D−\ni represents the distance from\nthe anti-ideal solution.\n5. Computation of Closeness Coefficient: The closeness coefficient (CC) determines the final ranking\nof each model:\nCCi =\nD−\ni\nD+\ni + D−\ni\n(7)\nwhere a higher CCi value indicates a better-performing model. Models are ranked in descending\norder of CCi.\n3.2.1 Dynamic weighting with deep-learning:\nFor every performance evaluation parameter, traditional TOPSIS implementations used fixed, sub-\njective weights. Whereas, climate model evaluation requires a data-driven approach, where the\nimportance of certain criteria varies based on region and season. A deep neural network (DNN) that\ndynamically learns ideal metric weights to handle this is presented.\n3.2.2 Neural network architecture\nThe neural network is meant to decide ideal ranking metric weights. Its design comprises:\n• Input Layer: Performance metrics of each model such as Bias, RMSE, KGE, Correlation, etc.\n• Hidden Layers: Two fully connected layers with 64 and 32 neurons, ReLU activation.\n• Output Layer: Softmax activation with normalized weights output for each metric.\n9\n\n\n3.2.3 Neural network training setup\nMinimizing the reconstruction error between expected and observed ranking patterns helps the neural\nnetwork discover the ideal weighting scheme. The loss function is:\nLoss =\nn\nX\ni=1\n(Wij −ˆWij)2\n(8)\nwhere, Wij represents the ground-truth weight of metric j for model i, and ˆWij is the predicted\nweight.\n• Optimizer: Adam\n• Learning Rate: 0.001\n• Batch Size: 32\n• Epochs: 50\n• Loss Function: Mean Squared Error (MSE)\nThe model is trained using stochastic gradient descent (SGD) and then refined using an Adam\noptimizer Jardines et al. (2024). Once trained, the learned weights derived from the neural network\nreplace the TOPSIS system’s static weights. This implies that the ranking process is adaptable\nenough for regional climate conditions, in which multiple performance criteria could have different\nrelevance. The top-ranked models are selected for every season and climate zone after the computation\nof closeness coefficients (CCi) for every 32 CMIP6 model. The best-performing models then go to\nthe downscaling block, where statistical downscaling techniques rooted in deep-learning are applied.\nThis hybrid DL-TOPSIS approach ensures that high-resolution climate projections only rely on the\nmost trustworthy CMIP6 models, hence improving the accuracy of the next climate assessments.\n3.3 Statistical downscaling block\nThis section describes the deep-learning architectures used to downscale coarse-resolution CMIP6\noutputs (50–250 km) to a high-resolution grid (0.1◦×0.1◦). Four models are developed to capture\nboth spatial and temporal features: (i) CNN-LSTM, (ii) ConvLSTM, (iii) ViT, and (iv) GeoSTANet.\n1. CNN-LSTM: This model combines CNNs for spatial feature extraction and LSTM networks for\ntemporal sequence modelling. The architecture is described in what follows.\n(a) Input: A climate data cube\nX ∈Rt×h×w×c\n(9)\nwhere t is the number of time steps, h × w are the spatial dimensions, and c is the number of\nvariables.\n(b) CNN block: Spatial features are extracted through a series of convolutional layers. For the\nlth layer, the feature map is computed as:\nF (l)\nij = ReLU\n\n\np\nX\nk=−p\np\nX\nm=−p\nW (l)\nkm X(l−1)\n(i+k)(j+m) + b(l)\n\n\n(10)\nWhere, W (l)\nkm and b(l) are the convolutional weights and bias, respectively, and p defines the\nradius of the convolution kernel, so that the full kernel size is (2p+1)×(2p+1). Here, ReLU(·)\ndenotes the Rectified Linear Unit activation function (Solera-Rico et al. 2024).\n(c) Flattening: The resulting spatial feature maps are flattened into a one-dimensional vector.\n(d) LSTM block: Temporal dependencies are modelled using LSTM cells. At each time step t,\nthe following equations are computed:\nit = σ (Wixt + Uiht−1 + bi)\n(11)\nft = σ (Wfxt + Ufht−1 + bf)\n(12)\not = σ (Woxt + Uoht−1 + bo)\n(13)\nct = ft ⊙ct−1 + it ⊙tanh (Wcxt + Ucht−1 + bc)\n(14)\nht = ot ⊙tanh(ct)\n(15)\n10\n\n\nwhere W{·}, U{·}, and b{·} are learnable parameters and ⊙denotes element-wise multiplication.\n(e) Output layer: A fully connected layer maps the LSTM output to produce the high-resolution\nprojection.\n2. ConvLSTM: This model integrates two-dimensional convolution operations within the LSTM\ngates to preserve spatial structure while modelling temporal dynamics (Yousif et al. 2023). at each\ntime step t, the ConvLSTM cell computes:\nit = σ\n\u0010\nWi ∗Xt + Ui ∗ht−1 + bi\n\u0011\n(16)\nft = σ\n\u0010\nWf ∗Xt + Uf ∗ht−1 + bf\n\u0011\n(17)\not = σ\n\u0010\nWo ∗Xt + Uo ∗ht−1 + bo\n\u0011\n(18)\nct = ft ⊙ct−1 + it ⊙tanh\n\u0010\nWc ∗Xt + Uc ∗ht−1 + bc\n\u0011\n(19)\nht = ot ⊙tanh(ct)\n(20)\nwhere ∗denotes 2D convolution over the spatial dimensions. In this architecture, multiple stacked\nConvLSTM layers are employed, batch normalization is applied after each convolution to stabilize\ntraining, and an up-sampling module is integrated to achieve the target resolution.\n3. ViT: The model divides the input climate grid into patches and applies self-attention mechanisms\nto capture long-range spatial dependencies.\n(a) Patch embedding: The input grid is partitioned into N fixed-size patches. Each patch Xi is\nflattened and projected linearly:\nZ0 =\n\n\nX1E\nX2E\n...\nXNE\n\n+ Epos\n(21)\nwhere E ∈R(p×p×c)×d is the learnable projection matrix and Epos provides positional encoding.\n(b) Transformer Encoder: The sequence of patch embeddings is processed by a transformer\nencoder. The self-attention mechanism is given by:\nAttention(Q, K, V ) = softmax\n\u0012QKT\n√dk\n\u0013\nV\n(22)\nwhere Q, K, and V denote the query, key, and value matrices, and dk is the key dimension.\n(c) Regression Head: The encoder output is passed through a fully connected regression head\nto generate the high-resolution output.\n4. GeoSTANet: This model extends the ViT architecture by integrating geospatial and tempo-\nral encoding to capture the spatiotemporal variability in climate data. GeoSTANet specifically\nincorporates the geographic coordinates of each patch to increase spatial context, and it analy-\nses the generated sequence along the temporal dimension. Each image patch is associated with\ngeospatial coordinates - latitude and longitude. These coordinates are embedded into a higher-\ndimensional space using a learnable projection matrix Wgeo. Specifically, given a 2D coordinate\nvector Xlatlon ∈R2, the geospatial encoding is computed as:\nGeoEnc = Wgeo Xlatlon\n(23)\nWhere Wgeo ∈Rd×2 is a learnable matrix that projects the 2-dimensional coordinates into a d-\ndimensional embedding. This geospatial encoding is then combined (e.g., via concatenation or\naddition) with the corresponding patch embedding to provide explicit spatial context.\n(a) Temporal transformer: To model the temporal dynamics of the data, the sequence of\nenriched patch embeddings is processed by a dedicated transformer encoder that operates along\nthe temporal dimension. Let Ht denote the hidden state at time t. The temporal evolution is\ndefined as:\nHt = TransformerEncoder(Ht−1),\nfor t ≥1\n(24)\n11\n\n\nWith the initial state H0 set as the geospatial enriched embedding from the first time step.\nThis block enables the model to capture sequential dependencies over time, integrating both\nvisual and geospatial information.\n(b) Upsampling: After the temporal processing, the final feature representation is passed through\nan upsampling module—such as a transposed convolution layer—to reconstruct the output\non a high-resolution grid. This step is essential for applications like climate forecasting where\ndetailed spatial predictions are required.\n(c) Input and output: GeoSTANet accepts as input a time-ordered sequence of image patches,\neach accompanied by its geospatial coordinates. Initially, each patch is embedded using the\nstandard ViT patch embedding method. The geospatial coordinates are then projected via Wgeo\ninto a d-dimensional space and combined with the patch embeddings. The resulting sequence\nis processed by the temporal transformer block to model the temporal dependencies, and\nfinally, the features are up-sampled to produce a high-resolution output. This pipeline distin-\nguishes GeoSTANet from the previously described CNN and ConvLSTM models by explicitly\nincorporating geographic context and dedicated temporal processing.\nThis complete approach combines advanced deep-learning-based downscaling systems with a data-\ndriven DL-TOPSIS ranking algorithm. By combining multi-criteria model evaluation with robust\nspatial and temporal feature extraction, the framework produces high-resolution climate projections\nthat accurately capture both mean behaviour and extremes, thereby supporting informed regional\nclimate impact assessments.\n4 Results and discussions\nThe reported results cover the performance of model ranking via the proposed DL-TOPSIS framework\nand the statistical downscaling via the four deep-learning models discussed above. Starting with\ncoarse-resolution CMIP6 model evaluation, these results offer a whole picture of model fidelity at\nseveral levels, and then deep-learning transforms top-ranked GCM outputs into high-resolution fields.\nThis approach yields model strengths and limits in various seasons and climate zones as well as\nmeasuring gains made by the downscaling process.\n4.1 Model ranking through DL-TOPSIS\nThe DL-TOPSIS approach was used to do a complete ranking of the thirty-two CMIP6 models. These\nranking results for every climate zone (Tropical, Arid, Temperate, Continental, and Polar) spanning\nwinter, spring, summer, autumn, and the whole year are shown on the heat map in Figure 3. Better\nratings are indicated by cooler (blue) tones; poorer ratings by warmer (red) tones. Particularly in\ntemperate and continental areas, which are difficult due to great seasonal variability and complex pre-\ncipitation regimes, NorESM2-LM, HadGEM3-GC31-LL, and GISS-E2-1-G routinely ranked among\nthe best models across many zones. Among the lowest-ranked models were TaiESM1, CMCC-CM2-\nSR5, BCC-CSM2-MR, FGOALS-g3, and showing notable temperature mean representation biases\nand errors. While many models battled with cold extremes, NorESM2-LM and MPI-ESM1-2-LR\nexcelled others in showing subzero temperature ranges (TNn) in Polar zones.\n12\n\n\nFig. 3: CMIP6 model ranking heatmap across European climate zones and seasons.\n13\n\n\nTable 3, represents the top 5 CMIP6 Models for each season and over Europe, Using performance\nevaluation criteria such as Score, Bias, RMSE, KGE, NSE, and PDF Overlap the table provides\na seasonal evaluation of CMIP6 models throughout six climate zones (Tropical, Arid, Temperate,\nContinental, Polar, and Overall). Especially in the Polar region, Winter suggests NorESM2-LM\nperforms better in many zones with the best RMSE. GISS-E2-1-G leads in the Tropical zone, Spring\ndenotes UKESM1-0-LL as the top performer. Summer shows HadGEM3-GC31-LL, and MPI-ESM1-\n2-LR perform well in the Tropical zone. In all zones, MIROC-ES2L does well in Autumn showing\ngreat PDF Overlap and NSE. Over the Full Year, MIROC-ES2L turns out to be the best overall\nmodel, and NorESM2-LM performs well in the Polar and Continental zones. Although higher KGE\nand NSE show a stronger correlation with observations, bias and RMSE trends indicate better\naccuracy from lower levels. The results highlight seasonal changes in model performance. These multi-\nmodel assessments are important since they expose no one universal model that performs under all\ncircumstances. Rather, the top five can be seen as a group of good candidates, each with particular\ncharacteristics (e.g., strong skill in heat extremes or cold extremes) that can be used for different\nclimate-sensitive applications.\n14\n\n\nTable 3: Top 5 CMIP6 models by climate zone and season.\nSeason\nZone\nModel\nScore\nBias\nRMSE\nKGE\nNSE\nPDF Overlap\nWinter (DJF)\nTropical\nACCESS-CM2\n0.9501\n0.3596\n4.6423\n0.0585\n-0.8327\n0.6908\nArid\nNorESM2-LM\n0.9333\n-0.4068\n5.0143\n0.1041\n-0.8527\n0.9626\nTemperate\nCMCC-ESM2\n0.9277\n-0.0098\n5.1524\n0.0817\n-1.5013\n0.8205\nContinental\nGISS-E2-1-G\n0.9103\n0.3599\n6.6133\n0.0913\n-2.0429\n0.8450\nPolar\nNorESM2-LM\n0.9542\n-1.8734\n4.3212\n0.0307\n-1.5815\n0.4112\nOverall\nNorESM2-LM\n0.9234\n0.0008\n5.7904\n0.1376\n-1.3970\n0.8968\nSpring (MAM)\nTropical\nGISS-E2-1-G\n0.9335\n0.8253\n5.1894\n0.1516\n-0.5558\n0.8763\nArid\nMIROC-ES2L\n0.9142\n-0.6355\n5.1720\n0.1324\n-0.5849\n0.8500\nTemperate\nNorESM2-MM\n0.9305\n-0.2988\n5.4334\n0.1444\n-0.6687\n0.7891\nContinental\nUKESM1-0-LL\n0.9253\n-0.0932\n6.0145\n0.1288\n-0.9561\n0.8234\nPolar\nGFDL-CM4\n0.9042\n-1.1415\n3.9518\n0.0189\n-1.3881\n0.4376\nOverall\nUKESM1-0-LL\n0.9286\n-0.2803\n5.6790\n0.1821\n-0.7016\n0.8384\nSummer (JJA)\nTropical\nMPI-ESM1-2-LR\n0.9429\n1.1819\n4.8028\n0.2363\n-0.2472\n0.7864\nArid\nNorESM2-LM\n0.8794\n-0.1021\n4.4709\n0.2579\n-0.5831\n0.7724\nTemperate\nACCESS-CM2\n0.9189\n-0.3081\n5.2273\n0.0928\n-0.9402\n0.8255\nContinental\nHadGEM3-GC31-LL\n0.9131\n0.2221\n4.8654\n0.1478\n-0.7857\n0.8489\nPolar\nMIROC-ES2L\n0.9235\n-1.0053\n2.7864\n0.0616\n-0.6326\n0.4830\nOverall\nHadGEM3-GC31-LL\n0.9322\n0.0261\n4.9257\n0.2812\n-0.5076\n0.8623\nAutumn (SON)\nTropical\nCanESM5\n0.9324\n1.1847\n4.7562\n0.3193\n-0.2525\n0.8518\nArid\nMIROC-ES2L\n0.9096\n-0.2656\n5.0993\n0.3271\n-0.2698\n0.8453\nTemperate\nMIROC-ES2L\n0.9097\n-0.2064\n4.7253\n0.2887\n-0.3334\n0.8233\nContinental\nMIROC-ES2L\n0.9257\n-0.0333\n4.3083\n0.2709\n-0.4697\n0.9152\nPolar\nMIROC-ES2L\n0.9412\n-1.7524\n3.4698\n0.0641\n-1.2028\n0.3976\nOverall\nMIROC-ES2L\n0.9502\n-0.1184\n4.5932\n0.4399\n-0.0892\n0.8851\nFull Year\nTropical\nNorESM2-LM\n0.9192\n0.8440\n4.8845\n0.3179\n-0.2764\n0.8386\nArid\nUKESM1-0-LL\n0.9201\n-0.2241\n5.1109\n0.3529\n-0.3425\n0.8980\nTemperate\nMIROC-ES2L\n0.9105\n-0.2939\n4.9621\n0.3198\n-0.3238\n0.8455\nContinental\nNorESM2-LM\n0.9225\n0.1673\n5.5920\n0.2487\n-0.8120\n0.8843\nPolar\nNorESM2-LM\n0.9408\n-1.5537\n3.7186\n0.0498\n-1.2792\n0.4842\nOverall\nMIROC-ES2L\n0.9312\n-0.1362\n5.0937\n0.3607\n-0.3015\n0.8934\n15\n\n\nFigure 4 illustrates the overall CMIP6 GCMs Model performance by climate zones and seasons.\nThe plot shows the CMIP6 GCMs’ average performance ratings across several climate zones and\nseasons, ranging from 0.775 to 0.868. A higher score indicates a better match with the actual obser-\nvation. Temperate and tropical zones exhibit optimal conditions in the spring and winter, indicating\nthat models adequately represent seasonal fluctuations in these regions (scores ¿ 0.86). In contrast,\nthe Continental zone has the poorest Winter performance (0.775), indicating difficulties in modelling\ncold-season climate processes. The Overall category shows model endurance by maintaining consis-\ntent performance (0.84-0.85) across seasons. The Polar zone fluctuates; it peaks in the Full Year\n(0.844) but drops in the Spring (0.810), indicating difficulties in maintaining high-latitude activ-\nity. The arid zone is stable ( 0.81-0.83) with minimal seasonal impact. These findings highlight the\nimportance of model evaluations based on regional and seasonal contexts, as they show that climate\nmodels are generally more reliable in certain seasons and locations. Figure 5 displays the best-ranked\nCMIP6 GCM spatially across Europe over various K¨oppen-Geiger Climate Zones.\nFig. 4: Overall Performance of CMIP6 Models in Europe.\n4.2 Downscaling performance results\nAfter determining the best-performing models for every season and zone, four advanced deep-learning\narchitectures CNN-LSTM, ConvLSTM, ViT, and GeoSTANet downscaled the selected GCM outputs\nto a fine-resolution grid (0.1°×0.1°). Across the same temperature zones and seasons, Figure 6 offers a\nrelative visualization of different structures. Deeper greens indicate better competence in performance\nmeasures like Bias, RMSE, NSE, KGE, correlation (r), and PDF overlap—which are color-coded.\nEspecially in capturing DTR and extremes TXx and TNn, this picture emphasizes how each design\nmanages the spatiotemporal complexity of daily temperature fields. Figure 6 represents a heat map\nof performance evaluation metrics of downscaling for various zones and seasons over Europe using\nvarious deep-learning models.\nIn areas with moderate geographical variability—that is, temperate and continental zones,\nCNN-LSTM showed particularly high accuracy. While the LSTM units modelled daily-to-season tem-\nperature variations, the convolutional layers efficiently identified spatially localized characteristics.\nCNN-LSTM did, however, occasionally show over-smoothing in locations with steep gradients, such\nas coastal zones or mountainous areas, clearly shown in somewhat higher RMSE values. In areas\nwith fast temperature transitions e.g., mountainous borders between continental and polar zones -\nConvLSTM sustained spatial coherence better than CNN-LSTM, by incorporating convolution oper-\nations directly into the LSTM gating mechanism. This benefit was particularly evident in winter\nwhen daily maximum and minimum temperatures can be quite influenced by convective processes.\nConvLSTM did, however, occasionally show training stability problems that needed careful learning\nrate and batch size tweaking to prevent over-fitting.\nThe ViT model downscaled the results by treating each climate map as a set of patches and using\nmulti-head self-attention. Particularly in tropical and subtropical zones, where broad-scale circulation\ncan control temperature distributions, this method was quite good in catching large-scale atmospheric\n16\n\n\nFig. 5: Spatial distribution of top-ranked CMIP6 models in Europe.\npatterns and interconnections. ViT did, however, sometimes suffer from local topographic effects\nsince self-attention may not always prioritize fine-scale terrain characteristics unless the patch size\nand positional embeddings are precisely optimized. ViT yielded competitive KGE and correlation\nvalues in arid zones; small-scale extremes occasionally seemed unnaturally smoothed, suggesting that\npatch-based embeddings may need more fine-grasping to manage localized events.\nParticularly in arid zones and TNn in polar zones, GeoSTANet, the geospatial-spatiotemporal\ntransformer, routinely outperformed the other architectures in capturing temperature extremes.\nGeoSTANet dynamically learned which areas and time steps were most important for forecasting\ndaily maxima and minima by including explicit geographical encoding (latitude and longitude embed-\ndings) and temporal attention blocks. In demanding environments—including the transitional zones\nbetween temperate and arctic climates—this capacity produced reduced bias and RMSE values.\nHigher PDF overlap scores in some zones suggest that GeoSTANet was able to faithfully replicate\nthe distribution tails for daily temperature using synergy between attention-based mechanisms and\nan imbalance-aware training method (focusing on rare extremes). For climate impact studies, which\nusually rely on accurate forecasts of unusual events like heat waves or severe cold spells, such an\nadvantage is highly desired.\nDownscaling accuracy is significantly impacted by the interaction of the GCM choice with\ndownscaling architecture. Using highly ranked GCMs with low bias and variability minimizes the cor-\nrectional load on statistical downscalers, thus lowering residual errors. On the other hand, even the\nmost sophisticated downscaling model inherits large-scale biases when associated with poorly ranked\nGCMs including CMCC-CM2-SR5, TaiESM1, and BCC-CSM2-MR, which displayed systematic\nbiases and high RMSE across many climate zones. Particularly in the Continental and Temper-\nate zones, where temperature variability is high, our analysis shows that pairing top-performing\nGCMs—NorESM2-LM, GISS-E2-1-G, and HadGEM3-GC31-LL—with GeoSTANet routinely per-\nforms better than other combinations across seasonal and annual scales. Maintaining strong KGE and\nNSE values, this combination achieves RMSE cuts of up to 20% over the next-best alternative. These\ndata highlight the need for a two-stage strategy: a) Strong multi-metric evaluation of GCMs to choose\nthe most dependable climate forecasts. b) Using GeoSTANet, advanced deep-learning-based down-\nscaling captures spatiotemporal dependencies and sub-grid processes, guaranteeing high-resolution\nclimate forecasts.\n17\n\n\nFig. 6: Performance comparison of downscaling models across zones and seasons.\n18\n\n\n4.3 Seasonal performance results\nSeasonal assessments of the downscaled outputs show important variations in the capacity of every\narchitecture to replicate temperature extremes. Especially in continental and dry zones, convective\nand radiative processes produce significant diurnal variations throughout summer. While ConvLSTM\nperformed better, CNN-LSTM periodically under-predicted TXx, presumably because the convolu-\ntional gating preserved local convective fingerprints. Although ViT’s patch-based method usually\nperformed well in capturing more general patterns, it may have missed small-scale heat islands, which\nwould have somewhat understated the peak daily maximum. Higher correlation values and better\nPDF overlap for TXx distributions show GeoSTANet’s most skilful resolution of these localized hot\nspots. Errors in winter tended to gather around significant temperature inversions or cold extremes.\nWhile GeoSTANet once more excelled by using temporal attention to track the development of cold\nair masses, ConvLSTM controlled spatiotemporal transitions well. In particular, biases were usually\nsmaller in winter than in summer, implying that downscaling designs may find simpler large-scale\nsynoptic circumstances to record.\n4.3.1 Regional performance results\nFrom a regional standpoint, the desert zone (B) presented special difficulties because of sharp daily\nfluctuations; the polar zone (E) necessitated strong handling of negative temperature extremes.\nCNN-LSTM and ConvLSTM both demonstrated a rather good ability in arid zones to capture daily\ntemperature fluctuations; nevertheless, if the GCM inputs included systematic warm or cold biases,\nthey could overstate the magnitude of extremes. Advanced attention levels of GeoSTANet regu-\nlarly reduced these biases, suggesting that attention-based designs are appropriate for arid areas\nwith high radiative forcing. In polar areas, model evaluation proved much dependent on the abil-\nity to depict negative temperature extremes (TNn). While ConvLSTM performed better because of\nits inherent spatiotemporal gating, CNN-LSTM occasionally suffered with capturing extended cold\nspells if they were not prominent in the training process. In these high-latitude areas, GeoSTANet’s\ngeographic encoding was particularly helpful since it more closely matched observed data to temper-\nature projections. PDF overlap was utilized to assess not only mean values but also the distribution\nof temperature, therefore evaluating each architecture. GeoSTANet’s better depiction of the whole\ntemperature distribution, including both central trends and tails, clearly showed consistently higher\noverlap scores than the other models. For research on climate change, where precise tail behaviour\ncan imply the difference between an underestimated or realistically expressed risk of extreme events,\nthis advantage is essential. The performance of ViT in PDF overlap was partially reliant on patch size\nand training procedures; hence, it suggests possible improvements if patch embedding or positional\nencodings were optimized for tasks related to the climate. Although CNN-LSTM and ConvLSTM\nusually showed modest overlap scores, they occasionally revealed small changes in the distribution\ntails depending on the training data size or if the GCM inputs included persistent biases.\n4.4 Discussion and outlook\nThe results naturally validate that advanced deep-learning architectures could significantly improve\ndaily temperature data’s spatial and temporal accuracy. The success of GeoSTANet highlights how\nimportant specialized design choices, geospatial encoding and attention-based mechanisms are to\ncapture climate-related variability. Regardless, CNN-LSTM and ConvLSTM are competitive options,\nparticularly in computationally limited environments, and can create downscaled fields that exceed\nconventional statistical approaches. ViT distinguishes itself for recording notable connections and\npatterns, even though it may need more fine-tuning for localized aspects. It is crucial to underline\nthe wider implications for climate adaptation and decision-making even as one discusses these out-\ncomes. Reliable high-quality temperature fields assist in enhancing risk assessments in public health,\ninfrastructure design, and agriculture. For instance, whereas accurate modelling of TXx in desert\nand temperate zones can guide early warning systems for heat waves, a strong representation of TNn\nhelps winter hazard planning and ecosystem preservation activities. Moreover, the suggested DL-\nTOPSIS structure takes advantage of the synergy between the downscaling technique and improved\nmodel choice to give decision-makers more consistent data. Instead of depending just on single GCM\noutputs or simpler downscaling methods, this two-tiered approach targets the most reliable global\nmodels and improves them with state-of-the-art neural architectures for finer detail.\n19\n\n\nFuture studies should consider extending this approach to other variables such as precipitation,\nwind speed, or soil moisture, where the interaction of local effects and large-scale circulation may\ndiffer greatly from temperature fields. Further increasing confidence in the downscaled products\ncould be ensemble-based methods including Bayesian uncertainty quantification or combining sev-\neral deep-learning architectures. Employing several emission scenarios e.g., SSP1-2.6, SSP5-8.5 these\narchitectures would also guide changes in model biases under future warming and whether sophis-\nticated downscalers are still robust. Combining several observational or reanalysis products such as\nground-based station networks or MERRA-2 may provide a more complete training and validation\ndataset, possibly improving model performance in data-sparse areas such as mountainous or high-\nlatitude regions. Considering these, modern deep-learning models seem to be quite able to refine the\ncoarse outputs of top-ranked CMIP6 GCMs. GeoSTANet shows to be the most consistent design\nacross several climate zones and seasons especially in capturing distribution extremes TXx and TNn\nand obtaining high PDF overlap. Still, excellent choices are CNN-LSTM and ConvLSTM; ConvLSTM\nis especially good at preserving spatial coherence in regions with strong temperature gradients. ViT\nshows promise in gathering general climate characteristics but might need local-scale event-specific\ncorrections. These findings confirm the need of exactly match strong GCMs with advanced down-\nscaling models to produce high-resolution climate forecasts that regularly direct scientific research,\npolicy, and adaptation strategies.\n5 Conclusions\nCombining a data-driven GCM ranking system (DL-TOPSIS) with deep-learning-based downscal-\ning, this work presents a strong two-stage framework for high-resolution climate projections over\nEurope. Objectively evaluating 32 CMIP6 models across five K¨oppen-Geiger climate zones (Tropical,\nArid, Temperate, Continental, and Polar) and several seasons (Winter, Spring, Summer, Autumn,\nand Full Year) the ranking system dynamically assigns weights to performance metrics to lower\nbias. While CMCC-CM2-SR5, TaiESM1, and BCC-CSM2-MR show systematic biases and weak\ncorrelation with observations, so less suitable for high-resolution downscaling, the results confirm\nthat NorESm2-LM, GISS-E2-1-G, HadGEM3-GC31-LL, MPI-ESm1-2-LR, and ACCESS-CM2-SR5\nconsistently outperform other models in different climate conditions.\nFour advanced deep-learning architectures—CNN-LSTM, CNN-LSTM, ViT, GeoSTANet—were\nused in the second stage to downscale top-ranked GCM outputs to a fine-scale resolution (0.1° ×\n0.1°). With a 20% RMSE decrease, GeoSTANet stands out as the most successful downscaling model\nsince it achieves statistically significant improvements over other methods. Extreme temperature\nfluctuations are faithfully captured by its geospatial and temporal attention mechanisms, preserv-\ning high KGE (0.89), NSE (0.85), and PDF overlap scores (0.91). Whereas CNN-LSTM improves\ntemporal coherence, ConvLSTM also performs well in areas with fast spatial transitions. ViT needs\nmore tuning to improve fine-scale resolution even if it shines in catching broad climatological trends.\nThese findings underline the need to combine advanced downscaling methods with ideal model\nselection to improve regional climate projections. Through better daily temperature forecasts, this\nframework offers insightful analysis for infrastructure design, climate risk assessments, and adapta-\ntion strategies. Future research will concentrate on extending this framework using multi-variable\ndownscaling architectures to other important climate variables including precipitation extremes, wind\nfields, and soil moisture. We also wish to evaluate model confidence levels by including Bayesian uncer-\ntainty quantification. We will also discuss the generalization of this method to other geographical\nareas, including East Asia and North America. At last, ensemble-based approaches will be examined\nto increase resilience under several emission scenarios (SSP1-2.6, SSP5-8.5).\nDeclarations\nAcknowledgements The authors acknowledge the SESAR 3 Joint Undertaking and its members\nfor their support in funding this research under grant agreement No. 101114795 as part of the E-\nCONTRAIL project. We also appreciate ECMWF for providing the ERA5 reanalysis dataset and\nWCRP for facilitating CMIP6 data access. Special thanks to our colleagues and collaborators for\ntheir valuable insights.\nFunding This research was funded by the SESAR 3 Joint Undertaking under the E-CONTRAIL\nproject (Grant Agreement No. 101114795).\n20\n\n\nAuthors’ Contributions Parthiban Loganathan: Conceptualization, data collection, inves-\ntigation, writing, and visualization. Elias Zea, Ricardo Vinuesa, and Evelyn Otero: Project\nadministration, review, and editing.\nEthical Approval Not applicable.\nConsent to Participate Not applicable.\nConsent to Publish All authors consent to the publication of this manuscript.\nCompeting Interests The authors declare no competing interests.\nData Availability Statement The data used in this study is included in the manuscript.\nAdditional data or supplementary materials can be provided upon reasonable request.\nReferences\nAlexander, L.V., X. Zhang, T.C. Peterson, J. Caesar, B. Gleason, A.M.G.K. Tank, M. Haylock,\nD. Collins, B. Trewin, F. Rahimzadeh, A. Tagipour, K.R. Kumar, J. Revadekar, G. Griffiths,\nL. Vincent, D.B. Stephenson, J. Burn, E. Aguilar, M. Brunet, others, and J.L. Vazquez-Aguirre.\n2006. Global observed changes in daily climate extremes of temperature and precipitation. J.\nGeophys. Res. Atmos. 111(D5). https://doi.org/10.1029/2005jd006290 .\nAlzubaidi, L., J. Zhang, A.J. Humaidi, A. Al-Dujaili, Y. Duan, O. Al-Shamma, J. Santamar´ıa,\nM.A. Fadhel, M. Al-Amidie, and L. Farhan. 2021. Review of deep learning: Concepts, cnn archi-\ntectures, challenges, applications, future directions. J. Big Data 8(1). https://doi.org/10.1186/\ns40537-021-00444-8 .\nBeck, H.E., N.E. Zimmermann, T.R. McVicar, N. Vergopolan, A. Berg, and E.F. Wood. 2018. Present\nand future k¨oppen-geiger climate classification maps at 1-km resolution. Sci. Data 5(1). https:\n//doi.org/10.1038/sdata.2018.214 .\nCavazos, T., M.L. Bettolli, D. Campbell, R.A.S. Rodr´ıguez, M. Mycoo, P.A. Arias, J. Rivera, M.S.\nReboita, C. Gulizia, H.G. Hidalgo, E.J. Alfaro, T.S. Stephenson, A.A. S¨orensson, R. Cerezo-Mota,\nE. Castellanos, D. Ley, and R. Mahon. 2024. Challenges for climate change adaptation in latin\namerica and the caribbean region. Front. Clim. 6. https://doi.org/10.3389/fclim.2024.1392033 .\nChen, S.J., C.L. Hwang, M.J. Beckmann, and W. Krelle. 1981. Multiple attribute decision making:\nMethods and applications.\nCohen, J., J.A. Screen, J.C. Furtado, M. Barlow, D. Whittleston, D. Coumou, J. Francis, K. Dethloff,\nD. Entekhabi, J. Overland, and J. Jones. 2014. Recent arctic amplification and extreme mid-\nlatitude weather. Nat. Geosci. 7(9): 627–637. https://doi.org/10.1038/ngeo2234 .\nCook, B.I., K.J. Anchukaitis, R. Touchan, D.M. Meko, and E.R. Cook. 2016. Spatiotemporal drought\nvariability in the mediterranean over the last 900 years. J. Geophys. Res. Atmos. 121(5): 2060–\n2074. https://doi.org/10.1002/2015jd023929 .\nDai, A., K.E. Trenberth, and T.R. Karl. 1999. Effects of clouds, soil moisture, precipitation, and\nwater vapor on diurnal temperature range. J. Clim. 12(8): 2451–2473. https://doi.org/10.1175/\n1520-0442(1999)012 .\nDueben, P.D. and P. Bauer. 2018. Challenges and design choices for global weather and climate\nmodels based on machine learning. Geosci. Model Dev. 11(10): 3999–4009. https://doi.org/10.\n5194/gmd-11-3999-2018 .\nDwivedi, Y.K., L. Hughes, E. Ismagilova, G. Aarts, C. Coombs, T. Crick, Y. Duan, R. Dwivedi,\nJ. Edwards, A. Eirug, V. Galanos, P.V. Ilavarasan, M. Janssen, P. Jones, A.K. Kar, H. Kizgin,\nB. Kronemann, B. Lal, B. Lucini, others, and M.D. Williams. 2019. Artificial intelligence (ai):\nMultidisciplinary perspectives on emerging challenges, opportunities, and agenda for research,\npractice and policy. Int. J. Inf. Manag. 57: 101994. https://doi.org/10.1016/j.ijinfomgt.2019.08.002\n.\n21\n\n\nEyring, V., S. Bony, G.A. Meehl, C.A. Senior, B. Stevens, R.J. Stouffer, and K.E. Taylor. 2016.\nOverview of the coupled model intercomparison project phase 6 (cmip6) experimental design and\norganization. Geosci. Model Dev. 9(5): 1937–1958. https://doi.org/10.5194/gmd-9-1937-2016 .\nEyring, V., P.M. Cox, G.M. Flato, P.J. Gleckler, G. Abramowitz, P. Caldwell, W.D. Collins, B.K.\nGier, A.D. Hall, F.M. Hoffman, G.C. Hurtt, A. Jahn, C.D. Jones, S.A. Klein, J.P. Krasting,\nL. Kwiatkowski, R. Lorenz, E. Maloney, G.A. Meehl, others, and M.S. Williamson. 2019. Taking\nclimate model evaluation to the next level. Nat. Clim. Change 9(2): 102–110. https://doi.org/10.\n1038/s41558-018-0355-y .\nFowler, H.J., S. Blenkinsop, and C. Tebaldi. 2007. Linking climate change modelling to impacts stud-\nies: Recent advances in downscaling techniques for hydrological modelling. Int. J. Climatol. 27(12):\n1547–1578. https://doi.org/10.1002/joc.1556 .\nGoodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,\nand Y. Bengio. 2017.\nGan (generative adversarial nets).\nJ. Jpn. Soc. Fuzzy Theory Intell.\nInformat. 29(5): 177. https://doi.org/10.3156/jsoft.29.5 177 2 .\nGudmundsson, L., J.B. Bremnes, J.E. Haugen, and T. Engen-Skaugen. 2012. Technical note: Down-\nscaling rcm precipitation to the station scale using statistical transformations – a comparison of\nmethods. Hydrol. Earth Syst. Sci. 16(9): 3383–3390. https://doi.org/10.5194/hess-16-3383-2012 .\nHaarsma, R.J., M.J. Roberts, P.L. Vidale, C.A. Senior, A. Bellucci, Q. Bao, P. Chang, S. Corti, N.S.\nFuˇckar, V. Guemas, J. von Hardenberg, W. Hazeleger, C. Kodama, T. Koenigk, L.R. Leung, J. Lu,\nJ. Luo, J. Mao, M.S. Mizielinski, and J. von Storch. 2016. High resolution model intercomparison\nproject (highresmip v1.0) for cmip6. Geosci. Model Dev. 9(11): 4185–4208. https://doi.org/10.\n5194/gmd-9-4185-2016 .\nHam, Y., J. Kim, and J. Luo. 2019. Deep learning for multi-year enso forecasts. Nature 573(7775):\n568–572. https://doi.org/10.1038/s41586-019-1559-7 .\nHempel, S., K. Frieler, L. Warszawski, J. Schewe, and F. Piontek. 2013. A trend-preserving bias\ncorrection – the isi-mip approach.\nEarth Syst. Dyn. 4(2): 219–236.\nhttps://doi.org/10.5194/\nesd-4-219-2013 .\nHersbach, H., B. Bell, P. Berrisford, S. Hirahara, A. Hor´anyi, J. Mu˜noz-Sabater, J. Nicolas, C. Peubey,\nR. Radu, D. Schepers, A. Simmons, C. Soci, S. Abdalla, X. Abellan, G. Balsamo, P. Bechtold,\nG. Biavati, J. Bidlot, M. Bonavita, others, and J. Th´epaut. 2020. The era5 global reanalysis. Q.\nJ. R. Meteorol. Soc. 146(730): 1999–2049. https://doi.org/10.1002/qj.3803 .\nHochreiter, S. and J. Schmidhuber. 1997. Long short-term memory. Neural Comput. 9(8): 1735–1780.\nhttps://doi.org/10.1162/neco.1997.9.8.1735 .\nHunt, K.M.R., A.G. Turner, and R.K.H. Schiemann. 2021. How interactions between tropical depres-\nsions and western disturbances affect heavy precipitation in south asia.\nMon. Weather Rev..\nhttps://doi.org/10.1175/MWR-D-20-0373.1 .\nIntergovernmental Panel on Climate Change (IPCC). 2023, Jun. Climate Change 2022 – Impacts,\nAdaptation and Vulnerability. Cambridge, England: Cambridge University Press.\nJardines, A., H. Eivazi, E. Zea, J. Garc´ıa-Heras, J. Simarro, E. Otero, M. Soler, and R. Vinuesa.\n2024. Thunderstorm prediction during pre-tactical air-traffic-flow management using convolutional\nneural networks. Expert Syst. Appl. 241: 122466. https://doi.org/10.1016/j.eswa.2023.122466 .\nKottek, M., J. Grieser, C. Beck, B. Rudolf, and F. Rubel. 2006. World map of the k¨oppen-geiger\nclimate classification updated. Meteorol. Z. 15(3): 259–263. https://doi.org/10.1127/0941-2948/\n2006/0130 .\nLoganathan, P. and A.B. Mahindrakar. 2020a. Assessment and ranking of cmip5 gcms performance\nbased on observed statistics over cauvery river basin – peninsular india. Arab. J. Geosci. 13(22).\n22\n\n\nhttps://doi.org/10.1007/s12517-020-06217-6 .\nLoganathan, P. and A.B. Mahindrakar. 2020b. Intercomparing the robustness of machine learning\nmodels in simulation and forecasting of streamflow. J. Water Clim. Change 12(5): 1824–1837.\nhttps://doi.org/10.2166/wcc.2020.365 .\nLoganathan, P. and A.B. Mahindrakar. 2021.\nStatistical downscaling using principal component\nregression for climate change impact assessment at the cauvery river basin.\nJ. Water Clim.\nChange 12(6): 2314–2324. https://doi.org/10.2166/wcc.2021.223 .\nMakowski, K., M. Wild, and A. Ohmura. 2008. Diurnal temperature range over europe between 1950\nand 2005. Atmos. Chem. Phys. 8(21): 6483–6498. https://doi.org/10.5194/acp-8-6483-2008 .\nOlsson, T., J. Jakkila, N. Veijalainen, L. Backman, J. Kaurola, and B. Vehvil¨ainen. 2015. Impacts\nof climate change on temperature, precipitation and hydrology in finland – studies using bias\ncorrected regional climate model data. Hydrol. Earth Syst. Sci. 19(7): 3217–3238. https://doi.\norg/10.5194/hess-19-3217-2015 .\nRasp, S., M.S. Pritchard, and P. Gentine. 2018. Deep learning to represent subgrid processes in\nclimate models. Proc. Natl. Acad. Sci. USA 115(39): 9684–9689. https://doi.org/10.1073/pnas.\n1810286115 .\nRiahi, K., D.P. van Vuuren, E. Kriegler, J. Edmonds, B.C. O’Neill, S. Fujimori, N. Bauer, K. Calvin,\nR. Dellink, O. Fricko, W. Lutz, A. Popp, J.C. Cuaresma, S. Kc, M. Leimbach, L. Jiang, T. Kram,\nS. Rao, J. Emmerling, and M. Tavoni. 2017. The shared socioeconomic pathways and their energy,\nland use, and greenhouse gas emissions implications: An overview. Glob. Environ. Change 42:\n153–168. https://doi.org/10.1016/j.gloenvcha.2016.05.009 .\nRosenzweig, C., J. Elliott, D. Deryng, A.C. Ruane, C. M¨uller, A. Arneth, K.J. Boote, C. Folberth,\nM. Glotter, N. Khabarov, K. Neumann, F. Piontek, T.A.M. Pugh, E. Schmid, E. Stehfest, H. Yang,\nand J.W. Jones. 2014.\nAssessing agricultural risks of climate change in the 21st century in a\nglobal gridded crop model intercomparison. Proc. Natl. Acad. Sci. USA 111(9): 3268–3273. https:\n//doi.org/10.1073/pnas.1222463110 .\nSanter, B.D., S. Solomon, C. Bonfils, M.D. Zelinka, J.F. Painter, F. Beltran, J.C. Fyfe, G. Johan-\nnesson, C. Mears, D.A. Ridley, J. Vernier, and F.J. Wentz. 2014. Observed multivariable signals\nof late 20th and early 21st century volcanic activity.\nGeophys. Res. Lett. 42(2): 500–509.\nhttps://doi.org/10.1002/2014gl062366 .\nSchuster, C. 2004. A note on the interpretation of weighted kappa and its relations to other rater\nagreement statistics for metric scales. Educ. Psychol. Meas. 64(2): 243–253. https://doi.org/10.\n1177/0013164403260197 .\nSolera-Rico, A., C. Sanmiguel Vila, M. G´omez-L´opez, Y. Wang, A. Almashjary, S.T.M. Dawson, and\nR. Vinuesa. 2024. ß-variational autoencoders and transformers for reduced-order modelling of fluid\nflows. Nat. Commun. 15(1): 1361. https://doi.org/10.1038/s41467-024-45578-4 .\nSummary for Policymakers. 2023, Jul. Summary for Policymakers. Cambridge, England: Cambridge\nUniversity Press.\nUrban, N.M. and T.E. Fricker. 2010. A comparison of latin hypercube and grid ensemble designs\nfor the multivariate emulation of an earth system model. Comput. Geosci. 36(6): 746–755. https:\n//doi.org/10.1016/j.cageo.2009.11.004 .\nVandal, N. 2018. Statistical downscaling of global climate models with image super-resolution and\nuncertainty quantification.\nvon Storch, H. 2011.\nReview of benestad, r. and i. hanssen-bauer (2008): Empirical-statistical\ndownscaling. Meteorol. Z. 20(1): 85–88 .\n23\n\n\nWang, J., Y. Jing, C. Zhang, and J. Zhao. 2009.\nReview on multi-criteria decision analysis aid\nin sustainable energy decision-making. Renew. Sustain. Energy Rev. 13(9): 2263–2278. https:\n//doi.org/10.1016/j.rser.2009.06.021 .\nWild, M., A. Ohmura, C. Sch¨ar, G. M¨uller, D. Folini, M. Schwarz, M.Z. Hakuba, and A. Sanchez-\nLorenzo. 2017. The global energy balance archive (geba) version 2017: A database for worldwide\nmeasured surface energy fluxes. Earth Syst. Sci. Data 9(2): 601–613. https://doi.org/10.5194/\nessd-9-601-2017 .\nXie, P. and P.A. Arkin. 1997.\nGlobal precipitation: A 17-year monthly analysis based on gauge\nobservations, satellite estimates, and numerical model outputs. Bull. Am. Meteorol. Soc. 78(11):\n2539–2558. https://doi.org/10.1175/1520-0477(1997)078 .\nYoon, K. and C.L. Hwang. 1995.\nMultiple attribute decision making: An introduction, SAGE\nPublications eBooks.\nYousif, M.Z., M. Zhang, L. Yu, R. Vinuesa, and H. Lim. 2023.\nA transformer-based synthetic-\ninflow generator for spatially developing turbulent boundary layers.\nJ. Fluid Mech. 957: A6.\nhttps://doi.org/10.1017/jfm.2022.1088 .\nZhang, C., P. Patras, and H. Haddadi. 2019.\nDeep learning in mobile and wireless networking:\nA survey. IEEE Commun. Surv. Tutor. 21(3): 2224–2287. https://doi.org/10.1109/comst.2019.\n2904897 .\nZhu, H., H. Liu, Q. Zhou, and A. Cui. 2023. Towards an accurate and reliable downscaling scheme for\nhigh-spatial-resolution precipitation data. Remote Sens. 15(10): 2640. https://doi.org/10.3390/\nrs15102640 .\nZorita, E. and H. von Storch. 1999. The analog method as a simple statistical downscaling technique:\nComparison with more complicated methods. J. Clim. 12(8): 2474–2489. https://doi.org/10.1175/\n1520-0442(1999)012 .\n24\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20132v2.pdf",
    "total_pages": 24,
    "title": "Regional climate projections using a deep-learning-based model-ranking and downscaling framework: Application to European climate zones",
    "authors": [
      "Parthiban Loganathan",
      "Elias Zea",
      "Ricardo Vinuesa",
      "Evelyn Otero"
    ],
    "abstract": "Accurate regional climate forecast calls for high-resolution downscaling of\nGlobal Climate Models (GCMs). This work presents a deep-learning-based\nmulti-model evaluation and downscaling framework ranking 32 Coupled Model\nIntercomparison Project Phase 6 (CMIP6) models using a Deep Learning-TOPSIS\n(DL-TOPSIS) mechanism and so refines outputs using advanced deep-learning\nmodels. Using nine performance criteria, five K\\\"oppen-Geiger climate zones --\nTropical, Arid, Temperate, Continental, and Polar -- are investigated over four\nseasons. While TaiESM1 and CMCC-CM2-SR5 show notable biases, ranking results\nshow that NorESM2-LM, GISS-E2-1-G, and HadGEM3-GC31-LL outperform other models.\nFour models contribute to downscaling the top-ranked GCMs to 0.1$^{\\circ}$\nresolution: Vision Transformer (ViT), Geospatial Spatiotemporal Transformer\nwith Attention and Imbalance-Aware Network (GeoSTANet), CNN-LSTM, and CNN-Long\nShort-Term Memory (ConvLSTM). Effectively capturing temperature extremes (TXx,\nTNn), GeoSTANet achieves the highest accuracy (Root Mean Square Error (RMSE) =\n1.57$^{\\circ}$C, Kling-Gupta Efficiency (KGE) = 0.89, Nash-Sutcliffe Efficiency\n(NSE) = 0.85, Correlation ($r$) = 0.92), so reducing RMSE by 20% over ConvLSTM.\nCNN-LSTM and ConvLSTM do well in Continental and Temperate zones; ViT finds\nfine-scale temperature fluctuations difficult. These results confirm that\nmulti-criteria ranking improves GCM selection for regional climate studies and\ntransformer-based downscaling exceeds conventional deep-learning methods. This\nframework offers a scalable method to enhance high-resolution climate\nprojections, benefiting impact assessments and adaptation plans.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}