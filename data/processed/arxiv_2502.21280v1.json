{
  "id": "arxiv_2502.21280v1",
  "text": "BACK TO THE FUTURE CYCLOPEAN STEREO:\nA Human Perception Approach Unifying Deep and Geometric Constraints\nSherlon Almeida da Silva¹²\nDavi Geiger²\nLuiz Velho³\nMoacir Antonelli Ponti¹\n¹Institute of Mathematics and Computer Science, University of S˜ao Paulo (ICMC-USP)\n²Courant Institute of Mathematical Sciences, New York University (NYU)\n³Institute for Pure and Applied Mathematics (IMPA)\n{sherlon.a, dg1}@nyu.edu, lvelho@impa.br, moacir@icmc@usp.br\nAbstract\nWe innovate in stereo vision by explicitly providing analyti-\ncal 3D surface models as viewed by a cyclopean eye model\nthat incorporate depth discontinuities and occlusions. This\ngeometrical foundation combined with learned stereo fea-\ntures allows our system to benefit from the strengths of both\napproaches. We also invoke a prior monocular model of\nsurfaces to fill in occlusion regions or texture-less regions\nwhere data matching is not sufficient. Our results already\nare on par with the state-of-the-art purely data-driven meth-\nods and are of much better visual quality, emphasizing the\nimportance of the 3D geometrical model to capture criti-\ncal visual information. Such qualitative improvements may\nfind applicability in virtual reality, for a better human expe-\nrience, as well as in robotics, for reducing critical errors.\nOur approach aims to demonstrate that understanding and\nmodeling geometrical properties of 3D surfaces is benefi-\ncial to computer vision research.\n1. Introduction\nThis paper is about stereo vision, and, as in many vision\nproblems, the state-of-the-art for benchmark datasets is ob-\ntained via deep learning (DL). For such methods, the in-\nput is a stereo pair of images, and training is done with\na loss function being the error between the output and the\nground truth (GT) disparity. Then, a DL-based stereo model\nis ready to output disparity data for a new stereo pair of im-\nages. In this context, the more variety and volume of train-\ning data, the better the results. What is concerning is that,\ndespite good performance, it remains a “black box”, and\nwe (humans) do not understand exactly how it comes to the\nresults.\nWhat does it mean to “understand stereo vision” and\nwhy does it matter? Understanding means discovering in-\nternal representations of the problem at hand, and often it is\nneeded to generalize to new situations or to allow us to ap-\nply such knowledge broadly. Suppose that a camera records\nvideo data of objects falling to the ground. It is plausible\nthat an algorithm for machine learning (ML) could make\npredictions of such trajectories with an accuracy equal to\nor greater than any physics model, assuming enough vari-\nety and quantity of data are provided. However, such an\nML solution would not have revealed the underlying laws\nof motion that govern them, such as the formula F = ma\nand a = 9.8m/s2, an abstraction of reality that alone does\nnot even give the most accurate models when the wind plays\na role in the trajectory. Still, it has been a key insight for\nengineering and our civilization. Our goal is to better un-\nderstand stereo vision with the mandate that to demonstrate\nbetter understanding the new solution must perform on par\nor better with respect to the state-of-the-art stereo vision. In\nthis way, the abstractions of the 3D reality we make here\ncan help not only to improve efficiency and performance of\nstereo algorithms, but perhaps such 3D models of reality\ncan be used for other activities in computer vision.\nFurthermore, ML extracts features that enable abstrac-\ntions of the visual world to be employed and tested. This\npaper does not claim to have fully understood 3D reality, but\ninstead that some significant understanding of it is made to\nhelp produce solutions on par with the state-of-the-art (pure\ndata-driven) stereo algorithms and with clear superior depth\nmaps visual appearance as depicted in Figure 1.\nLet us point out that some of the ideas developed here are\nrooted in previous stereo studies dating back to the early\nwork of Helmholtz [22] and Julesz [12] that created and\nadvocated for the cyclopean eye view of stereo vision, and\nalso several computer vision studies before the dominance\nof pure data-driven DL methods.\n1.1. Previous Work\n“Back then” in 1499, in his Treatise on Painting, Leonardo\nda Vinci explained that different parts of the background\n1\narXiv:2502.21280v1  [cs.CV]  28 Feb 2025\n\n\nFigure 1. A Comparison of 256x256 results: we present the left image followed by RAFT-Stereo [16], Selective-IGEV [24], and B2FS\n(Ours). In each image a rectangle is selected and zoomed in (overlayed over the image) to show the visual differences in such areas.\nare occluded when viewed from the left eye compared to\nthe right eye [6].\nHermann von Helmoltz’s [22] in the late 1800’s and\nearly 1900’s was a pioneer for pointing out the use of binoc-\nular vision to achieve depth perception. Helmholtz concep-\ntualized the “cyclopean eye” as a hypothetical single eye\nlocated in the middle of the head, representing the brain’s\nintegration of the two eyes’ input. Bela Julesz pushed the\nstudy of stereo vision toward human perception and model-\ning [12] while creating the random dot stereogram (showing\nthat stereo works even without recognition).\nMarr and Poggio [17] attempted to mathematically\nmodel human stereo vision, matching zero crossings in fil-\ntered images that correspond to edges and features in the\nvisual field, and delivering the 2 1\n2 sketch (including the sur-\nface reconstruction process). This work was followed by\nseveral computational models in the 1980’s, 1990’s, and\n2000’s, where in particular global modeling of occlusions\nand discontinuities with dynamic programming (DP) opti-\nmization [1, 7, 8] and with the minimum cut optimization\n[4, 10], and all their citations, were the dominant approach\nto stereo.\nIn all these models, relatively little progress\nwas achieved in improving feature extraction for matching,\nthough there were efforts, e.g. with an overcomplete set of\nlinear filters [11] and with left and right windows per image\npixel to compete for matching [7].\nWith the advent of DL and the availability of datasets, a\nmuch better level of accuracy has been achieved in stereo\nby RAFT-Stereo [16], CREStereo [14], DLNR [29], and\nSelective-IGEV [24]. These techniques learn to produce\ndisparity maps with performance higher than that of all pre-\nvious stereo algorithms. In close examination, these meth-\nods first extract features from the left and right images and\nlater perform disparity estimation at each pixel.\nMore recently, advances in qualitative depth recovery\nfrom the monocular view, through DL techniques and\nlarge datasets, have produced excellent results, e.g. Mi-\nDaS [19], DPT [20], Metric3D [9, 28], ZoeDepth [2], Patch-\nFusion [15], UniDepth [18], Marigold [13], Depth Any-\nthing [26, 27], and DepthPro [3]. Upon reflection, these\nmethods likely have excellent implicit surface models.\n1.2. Our Contribution\nBroadly presented, here are our main contributions.\n– Hybrid Geometric–Learning Framework:\na novel\nstereo vision approach that combines geometric reasoning\nwith the adaptability of DL (see Figure 5).\n– Cyclopean Eye Model: a yet novel framework that han-\ndles depth discontinuities and occlusions, where for opaque\nsurfaces there is one and only one disparity solution per\ncyclopean spatial coordinate (e, x), allowing for a more\nhuman-like perception of 3D scenes (see Figures 2, 3, 4).\n– Occlusion and Texture-Less Region Recovery: incor-\nporate a prior monocular surface model to fill in occluded\nand texture-deficient regions, enhancing disparity estima-\ntion where traditional matching fails (see Figure 4 and 6).\n– Foundational Contribution to Vision Understanding:\ndeepens our understanding of 3D scene geometry, paving\nthe way for more interpretable and reliable vision systems.\n2\n\n\nFigure 2. Space Transformation from L×R CS (left) to the XD\n(right). The colors represent a disparity value. Empty positions\nin XD space are disallowed, while the ’red dot’ data are obtained\nvia a bilinear interpolation from LR space data. The XD space has\ntwice the resolution of the LR space, for each epipolar line.\n2. Back to GCs: Occlusions and Discontinuities\nOur approach goes ”back” to the cyclopean coordinate sys-\ntem (XD) where the geometric constraints (GCs) of the 3D\nworld can be best described.\n2.1. Cyclopean Coordinate System (XD)\nConsider a left image IL with height and width M × N\nand a pixel coordinate system (CS) (e, l) ∈M × N and\nconsider the M × N right image IR and its CS (e, r) ∈\nM × N where e ∈(0, 1, . . . M −1) index their respective\nepipolar lines (see Figure 2 and 4). For many datasets, such\nas Middlebury [21], the images have been rectified by the\nepipolar lines, which are then simply the horizontal lines of\nthe images. The space (e, x, d) ∈M × 2N × D allows us\nto describe the matching of (e, l) ↔(e, r) as an assignment\nof a disparity d to (e, x) (see Figure 2).\nSuch matching and associated assignment is described\nby the invertible coordinate transformation:\n\u0012x\nd\n\u0013\n= 1\n2\n\u0012\n1\n1\n1\n−1\n\u0013 \u0012\nr\nl\n\u0013\n.\n(1)\nOne consequence of this transformation is that the discrete\ncyclopean width coordinate x ∈(0, 1\n2, 1, 3\n2, . . . , N −1, N −\n1\n2) has subpixel resolution, twice as much as the image pixel\nwidth resolution of l, r ∈(0, 1, . . . , N −1) (see Figure 2).\nThe XD provides a depth value D(e, x), for each match\n(e, l) ↔(e, r), as follows\nD(e, x) = f\nB\nd(e, x) ,\n(2)\nwhere f is the focal length, B is the baseline (distance be-\ntween the left and right camera centers), see Figure 3. The\ndepth provided by man-made datasets is typically obtained\nwith a light projection or laser that simulates the view of the\nXD, and not the depth from the L or R.\nDefinition 2.1 (Opaque Surfaces and Stereo) 3D opaque\nsurfaces do not let light pass through them. Consider a 3D\npoint P = (X, Y, Z) that belongs to an opaque surface. If\nit is visible by L we can describe it as (e, l, DL(e, l)) and if\nit can be seen by R it can be described as (e, r, DR(e, r)).\nNote that (i) if P is visible by both eyes, still in general\nDL(e, l) ̸= DR(e, r), but the disparities satisfy dL(e, l) =\ndR(e, r = 2dL(e, l)+l); (ii) Also, in general, given a match\n(e, l) ↔(e, r) and the assignment d(e, x) derived from (1)\nleading to D(e, x), obtained from (2), we have D(e, x) ̸=\nDL(e, l), DR(e, r).\nDefinition 2.2 (Transparent Surfaces and Stereo)\nTransparent surfaces allow some light to pass through.\nTwo distinct points P1,2\n= (X1,2, Y1,2, Z1,2) are in a\ntransparent pair state if both can be seen by the XD and\nshare the same (e, x) coordinates.\nTransparent stereo\nsurfaces are a set of contiguous transparent-pair states.\nAs we describe next, the XD allows for a simple descrip-\ntion of the geometrical constraints of a stereo vision system.\n2.2. Depth from L, R, and XD\nBased in Figure 3 we infer the following triangle relations\n\u0000DL(e, l)\n\u00012 = D2(e, x) +\n\u0012B\n2 −x\n\u00132\nand\n\u0000DR(e, r)\n\u00012 = D2(e, x) +\n\u0012B\n2 + x\n\u00132\n(3)\nThese relationships introduce a bias in the depth estimation\nfrom the L and R cameras relative to the XD depth values.\nTo the best of our knowledge, these have not been derived\npreviously. The impact becomes evident when comparing\ndepth estimates with man-made GT scenarios. A conse-\nquence of mismatches in L/R depth estimation yield, for\nexample, motion sickness in virtual reality [25].\n2.3. Occlusions, Discontinuities and GCs\nDefinition 2.3 (Occlusions and Discontinuities) R-\n(L-)\nocclusions are regions that are seen by the R (L) eye but\nnot by L (R). R- (L-) discontinuities are places where\njumps of disparity (or of depth) occur in the R (L).\nL- and R-occlusions, together as a group, are termed\nhalf-occlusions [1, 23].\nAs first observed by Da Vinci and recently pointed out\nin [1, 7, 8, 10], 3D opaque surfaces follow geometric con-\nstraints that link discontinuities to occlusions. [7, 8] pro-\nposed a monotonicity constraint for the L and R disparity\nmaps for the visible areas for both eyes. We next describe\nour new proposed GCs.\nProposition 2.4 GCs for opaque surfaces\nGC1. The size of the jump, along an epipolar line, of a R-\n3\n\n\nFigure 3. DL/R(e, l/r) is the depth from L/R CS, respectively. A\npoint P in 3D is described by the XD as P = P C(e, x, Z =\nD(e, x)). The same point can be described by the L/R CS as\nP = P L,R(e, Xl,r, D(e, x)), where Xl,r ̸= l, r, since l, r are\nthe projective projection of P into the L/R CS, while Xl,r is the\nsimpler orthogonal projection of P into the L/R CS. Note that\nB = Xl −Xr. The distance to P measured by the L, R, and\ncyclopean eye are DL(e, l), DR(e, r), D(e, x), respectively, and\nthey are all different values. Note that the relation D = f B\nd as-\nsumes d = r −l, but our definition of disparity requires a factor 2.\n(L-) discontinuity is equal to the size of the L- (R-) occlu-\nsion.\nGC2. Each cyclopean coordinate (e, x) has one and only\none disparity, i.e., d is a function d : (e, x) →R.\nFigure 4 illustrates these two constraints. To our knowl-\nedge, GC2 has not been proposed before. Note that even re-\ncently Wang [23] argued that a unique disparity constraint\nshould not be used for the left eye nor for the right eye,\nwhich we agree, but nothing was mentioned for the cyclo-\npean eye. Clearly, GC2 will not be satisfied for transparent\nsurfaces. Although GC1 is present in the work [7], and\nlater, e.g. [1, 10], it has not been precisely modeled in the\nXD as we do next. Moreover, the trust of our approach is\nthe integration of GCs with deep learning to create B2FS.\nFollowing Proposition 2.4 we model surfaces S\n=\n{(e, Pe); e = 0, . . . , M −1} as a set of paths Pe in the cy-\nclopean space and indexed by each epipolar line e. A path\nPe = {[O(e, x), d(e, x)]; x = 0, 1\n2, 1, 3\n2, . . . N −1, N −1\n2}\nis described by a sequence of states [O(e, x), d(e, x)] for\neach (e, x), where a binary occlusion variable O(e, x) =\n0, 1 and the disparity value d(e, x) are unique (that is, satis-\nfying GC2). Here we do not distinguish whether a surface\nis described in terms of depth or disparity values. In order\nto impose GC1 the path Pe obeys the following constraint\nIf 0 = O(e, x) = O(e, x′)\nand\n1 = O(e, x + 1\n2) = O(e, x + 1) = . . . = O(e, x′ −1\n2)\nThen,\n|d(e, x′) −d(e, x)| = x′ −x\n(4)\nand if d(e, x′)−d(e, x) > 0 it is a left occlusion and discon-\ntinuity, else if d(e, x′) −d(e, x) < 0 it is a right occlusion\nand discontinuity.\nFigure 4. An epipolar slice of a surface with left occlusion region\nand its description by the XD. a. A top view of the epipolar slice\nof the surface and the two eyes projections. The baseline B con-\nnects L to R focal centers. The depth axis describe the inverse\nof disparity (Equation (2) depicted). b. A discrete XD, a rotation\nof the L-R CS described by Equation (1). Note that the R CS is\npointing down. The two red vertical dashed lines delimit the L oc-\nclusion area which are associated with a R discontinuity along the\nhorizontal blue dashed line with a jump of the same size as the left\nocclusion, as described by GC1 in Proposition 2.4). Note that the\nonly two (2) light green squares (the ones without an ”y” in them)\nare seen by the XD associated with the L occlusion, satisfying one\ndisparity per coordinate x (as postulated by GC2), which is half\nof the size of the L occlusion area.\nGC1, given by (4), can be imposed in a local form as\nO(e, x)O(e, x −1\n2) = 1 →d(e, x) −d(e, x −1\n2) = ±1\n2 .\n(5)\nNote that the disparity values assigned at such occlusion lo-\ncations are not part of the final solution, they are just place-\nholders to insure that GC1 is satisfied. This specific charac-\nterization in XD, to the best of our knowledge, has not been\nproposed before, and it is a critical description to impose\nGC1 locally. Later, the disparity values at the occlusions\nwill have to be estimated.\nIn order to find S given a pair of left and right images,\none must define a criterion of what makes S the correct so-\nlution, as well as a method to obtain it (including the con-\nstraint (5)). The method we use is DP as we discuss next.\n2.4. Optimization Criteria and DP\nAlong an epipolar line a path must have good matches be-\ntween left and right image features in regions of no occlu-\nsions.\nDefine a measure of Feature Matching Similarity\n(FMS) as a scalar product between features in L and R, i.e.,\nFMS(e, x, d(e, x)) = F L(e, x −d(e, x)) · F R(e, x + d(e, x))\n(6)\nand so FMS(e,x)(d(e, x)) must be large when there is\nno occlusion.\nIn addition, the regions of the occlusions\ntend to be contiguous (not isolated points).\nFigure 6 il-\nlustrates the data quality of such matching (a distance\n4\n\n\nFM(e,x)(d(e, x)) = 1−\nF MS(e,x,d(e,x))\nmax(F MS(e,x,d(e,x)) is used so that\ngood matches render these values small and bounded to the\n[0, 1] range).\nWe propose a cost criteria associated with each Pe, struc-\ntured as a sum of local costs\nC(Pe) =\nX\nx\nλ O(e, x) −ϵ O(e, x) O(e, x −1\n2)\n+(1 −O(e, x))FM(e, x, d(e, x)) ,\n(7)\nwhere the hyperparameters λ, ϵ estimates, for a given path,\nif the quality of the match between features at each (e, x)\nis better than assigning to it an occlusion O(e, x) = 1.\nThe lower λ is the better the match must be in order not\nto be chosen as an occlusion. The higher ϵ, the more likely\nO(e, x)O(e, x ± 1\n2) = 1.\nThus, the solution S∗= ({[O∗(e, x), d∗(e, x)]; x =\n0, 1\n2, 1, 3\n2, . . . N −1, N −1\n2}, for each epipolar line e, min-\nimizes (7). DP is an optimization method for obtaining the\noptimal set of disparities {d∗(e, x)} and binary variables\n{O∗(e, x)}, for each epipolar line e.\nGiven a state [O(e, x), d(e, x)], and due to the continuity\nof surfaces and / or occlusions (and then the constraint GC1\n(5) is used), the DP search is restricted to the following six\nprevious neighbors\nN[O(e,x),d(e,x)]\n=\n\u001a\n[O(e, x −1\n2) = 0, 1; d(e, x −1\n2) = d(e, x) ± 0, 1\n2\n\u001b\n.\nNote that the pair [O(e, x) = 1, d(e, x)]; [O(e, x −1\n2) =\n1, d(e, x −1\n2) = d(e, x)] does not satisfy GC1, but is still\nconsidered to capture homogeneous regions.\n2.5. Detection of Homogeneous Regions\nThe detection of homogeneous regions – regions with a\nlack of texture – occurs during the detection of occlusion in\nDP. The assignment of occlusions in homogeneous regions\nis caused by lack of a good match (they are equally bad\nmatches) stereo features in homogeneous regions.\nMore\nprecisely, we simply assign h(e, x) = 1 and h(e, x−1\n2) = 1\nto locations (e, x), (e, x−1\n2) where DP assigned O(e, x) =\nO(e, x −1\n2) = 1 and yet (5) is not satisfied, i.e., where also\n0 = d(e, x) −d(e, x −1\n2). An illustrative example of the\nresults of DP is shown in Figure 5 Step (8). Note that the\ndisparity values at the occlusions and homogeneous regions\nare not reliable values and will still have to be estimated.\nFor space reasons, we do not elaborate here further on\nthe exact details of the DP algorithm; instead, we make it\navailable on Github1.\n1https://github.com/SherlonAlmeida/B2FS.git\n3. Back to the Future\nThe model we developed in section 2 focused on geomet-\nric reasoning, a set of abstractions about the 3D scene as\nviewed from the cyclopean eye.\nThe DP algorithm em-\nployed is a method to obtain occlusions and homogeneous\nregions as well as disparity (depth) in visible regions (where\nthe stereo features F L, F R do match well) with subpixel\naccuracy. We still must obtain such features F L, F R, and\ninvoke a surface model to fill in depth at occlusions and ho-\nmogeneous regions, and possibly improve the accuracy of\nthe depth obtained from DP to a decimal value. We refer\nto this approach as B2FS and it is depicted in Figure 5 with\nall steps of B2FS shown and described. We now describe\nthe steps in more depth and at a high level, pointing to the\nfigures and their captions for more detail information.\n3.1. Image Features for Stereo Matching\nConsidering the promising results from the DL methods al-\nready developed that extract stereo features from stereo im-\nage pairs, we adopted RAFT-Stereo [16] features as input\nto the DP program (see Figure 6). However, RAFT-Stereo\nproduces features at 1\n4 of the input image resolution. An\ninterpolation must be considered to achieve full resolution\nfeatures and is shown in Figure 5, steps (6) and (7), lead-\ning to the LR correlation and data filling of the XD space.\nNote that Figure 6 already shows the LR correlation at full\nresolution (after interpolation).\n3.2. Surface Model\nOur approach to model surfaces, needed to complete the\nsurfaces where occlusions and homogeneous regions are\nmasked by the DP solution, relies on results in monocular\ndepth using DL. Excellent depth gradient information is ob-\ntained with Depth Anything V2 [27] and DepthPro [3]. As\nwe experimented, the depth values offered are not scaled\nto precision (see Figure 8 for the analysis). Nevertheless,\nthe accuracy of the depth-gradient information, which gives\nthe surface normal unit vectors, is in par with the decimal\ndisparity accuracy we aim to obtain. Thus, we treat monoc-\nular depth solutions as the best models today for providing\nsurface normals.\nAfter conducting a study on Depth Anything V2 and\nDepthPro we observed that the range of the 3D environ-\nment captured for DepthPro is wider and edges more accu-\nrate than Depth Anything V2. In addition, it provides an\nextra feature: an estimation of focal length for a monocu-\nlar input image, which allows us to estimate the disparity\nmap from depth using the Equation (2). Note that monoc-\nular depth solutions provide surface information from the\nview of the provided image and not from the cyclopean eye\nview. The differences in such depth values are captured by\nFigure 3 and Equation (3).\n5\n\n\nFigure 5. Workflow. Step (1) our approach receives an image-pair with resolution rH,W,3. In order to be obtain full resolution features\nand in anticipation of RAFT-Stereo resolution reduction, Step (2) upscale to r4H,4W,3 using Hybrid Attention Transformer [5]. Step (3),\nRAFT-Stereo performs at 1\n4 of r4H,4W,3, resulting in rH,W,256 for F L, F R. Step (4) uses bilinear interpolation to produce twice as much\nthe width resolution rH,2W,256, and achieve the subpixel data for the XD space (the red dots in Figure 2). Steps (5) and (6) perform the\nfeature dot products to create the correlation matrix by epipolar lines at resolution r2W,2W,1. Step (7) transfer the correlation data to the\nXD space, with the max disparity considered shown by a solid red horizontal line (cutting the need to search beyond such disparity). Step\n(8) run DP to obtain a Disparity mask at good data matching coordinates i.e., where a binary Data mask indicates the non occluded and\nnon homogeneous regions. Step (8) also performs monocular depth. Step(9) performs the fill in information of the surface where DP did\nnot have a solution (homogeneous and occlusions) via a FCRN where the input is a normalized monocular depth and the output is the DP\ndisparity map only where such solution is available by the data mask.\nFigure 6.\nLR Correlation: The graph displays the distance\nFM(e, x, d(e, x)) = 1 −\nF MS(e,x,d(e,x))\nmax(F MS(e,x,d(e,x)), where FM is\nobtained from (6). Dark regions (low values of FM) represent\ngood matches. The disparity inverse relation to depth guarantees\nthat disparities are always equal or greater than zero, avoiding the\nsearch for negative disparities. One observes that typically one\ndisparity stands out (a good match), unless we have occlusions\nor homogeneous regions. One can identify regions of occlusions\nwhere (i) no good match along the disparity axis occur and where\n(ii) the occlusion width is similar to the disparity changes from\nneighbor dark segment, as proposed by constraint GC1.\nThe challenge is then how to combine both results: the\naccurate monocular depth gradient or surface normals, from\nthe L/R view, with the DP output from the cyclopean view.\n3.3. Combining Stereo x Mono Disparities\nWe integrate the surface normal from monocular depth with\nthe output of DP (the disparity and the data mask) via\na Fully Convolutional Regression Network (FCRN) com-\nposed of a sequence of six (6) Conv2D layers followed by\nReLU activation function (see Figure 7).\nThe regression is guided using MSELoss, applied only\nto the data coordinates obtained from DP through stereo\nmatching.\nFinally, we employ Hybrid Attention Trans-\nformer (HAT) [5] to enhance the edge sharpness of the\nFCRN solution.\nFigure 7. FCRN architecture: The input is the DepthPro dispar-\nities, normalized to the range [0, 1], and the desired output is the\nDP subpixel disparity multiplied by the data mask. The regression\nis conducted using the AdamW optimizer and MSELoss, with an\nadaptive learning rate initialized at lr = 0.00025 and adjusted by\na scheduler with γ = 0.90 and a step size of 25. The final dispar-\nity map is produced after 125 epochs.\n6\n\n\n4. Results and Discussion\nWe evaluated our approach using the Middlebury dataset,\nwhich consists of realistic indoor scenes, and in this sec-\ntion we discuss our quantitative and qualitative findings for\nr256,256,3 resolution images.\nInitially, we compared the disparities estimated by\nDepthPro with the GT using the average error (AvgEr-\nror). We obtained it based on the focal length estimated\nby DepthPro, and baseline provided by GT. Figures\n(8,\n9) demonstrate that monocular estimation produces out of\nscale disparities for all images. Our technique effectively\nleverage monocular gradients, feature matching and stereo\nreasoning about surfaces (B2FS) to obtain accurate solu-\ntions across all Middlebury images with public GT (Addi-\ntional and Training partitions), reducing AvgError by up to\n48 times.\nFigure 8. Quantitative Results: B2FS vs. DepthPro. We eval-\nuated the disparity error from Equation (2), considering the focal\nlength estimated by DepthPro (yellow) and obtained from GT (or-\nange). In this figure, the best error is represented by the solid-\ncolored bar in the front, while the worst error is overlapped and\ntransparent. Since the focal length estimated by DepthPro was\ntypically better, we adopted it as the default.\nWe obtained the publicly available code for state-of-the-\nart techniques such as RAFT-Stereo [16] and Selective-\nIGEV [24] to assess their robustness in extracting stereo\ncues from input images with lower resolutions than those\nused in benchmarks such as Middlebury, ETH3D, and\nKITTI. Our study focused on images r512,512,3, r256,256,3,\nand r128,128,3. As shown in Figure 10, RAFT-Stereo and\nSelective-IGEV struggle as the input resolution decreases,\nwhile B2FS maintains the surface appearance.\nOur evaluation includes standard benchmark metrics:\nAvgError, BadError, and RMSError. While our numerical\nresults (see Figure 12) were not superior to RAFT-Stereo\nand Selective-IGEV, the qualitative results remained con-\nsistent across different resolutions. Additionally, Figure 11\nshows that our approach exhibits more global errors than lo-\ncal errors relative to RAFT-Stereo and Selective-IGEV, in-\nFigure 9.\nQuantitative Results:\nB2FS vs.\nDepthPro\n(Normalized-GT). In this experiment, we normalized DepthPro\ndisparities between the minimum and maximum disparities from\nGT. It is evident that simply transforming DepthPro into the cor-\nrect disparity range is insufficient to achieve optimal results, indi-\ncating that obtaining accurate disparities is not solely dependent\non a scale factor.\nFigure 10. Qualitative Results: A comparison on the impact of\ninput image resolution - more details in Supplementary Material.\ndicating a tendency to preserve local details.\nGiven that our qualitative results preserved detailed\nmonocular surface information while maintaining DP GCs\n(see Figure 1), we questioned the effectiveness of AvgEr-\nror, BadError, and RMSError in assessing the robustness\nof monocular and stereo techniques. To address this, we\nalso evaluated the inverse of the Structural Similarity In-\ndex Measure (SSIMError, where lower is better), as well as\nMutual Information Similarity (MutualInfoSim) and Peak\n7\n\n\nFigure 11. Qualitative Results: Signed Error - red values mean\nthe disparities should be lower (further back), while blue values\nmean the disparities should be higher (further forward).\nSignal-to-Noise Ratio (PSNRSim), where higher values in-\ndicate better performance. Our results (blue) outperformed\nRAFT-Stereo (green) in most cases and were competitive\nwith Selective-IGEV.\nThese findings highlight the limitations of traditional er-\nror metrics in evaluating monocular and stereo depth esti-\nmation techniques. While AvgError, BadError, and RM-\nSError provide numerical accuracy assessments, they do\nnot fully capture the perceptual quality or structural consis-\ntency of disparity maps. Our results demonstrate that B2FS\npreserves finer details and maintains structural coherence,\nparticularly in lower-resolution inputs where stereo-based\nmethods struggle.\nThis suggests that leveraging stereo\nconstraints and monocular gradients can offer a robust al-\nternative, even when input resolutions deviate from those\nused in training. B2FS effectively balances depth accuracy\nwith high-quality visual reconstruction, making it promis-\ning for real-world applications where resolution constraints\nare common.\n5. Conclusion\nBack to the Future Cyclopean Stereo (B2FS) addresses a\ncrucial limitation of current AI systems by emphasizing\nthe importance of understanding and abstraction. In an era\nwhere models are increasingly data-driven, understanding\nthe process of acquiring knowledge is essential.\nOur proposal neither aims to create a foundation model\nfor surfaces nor dismisses the importance of data-driven\nmodels.\nInstead, we advocate for leveraging pre-trained\nmodels as feature extraction and surface prior knowledge\nabout the world and incorporating an additional reasoning\nstep to explore the constraints of specific applications.\nOur results highlight the strengths of state-of-the-art\nmonocular depth estimation techniques, while also reveal-\ning their limitations in scenarios requiring real-scale depth.\nFurthermore, we reaffirm that stereo information remains\nthe most reliable source for 3D scene perception, both for\nAI systems and human vision. By integrating DL with an-\nalytical models, we unlock new possibilities for computer\nvision, not only benefiting from DL’s capabilities but also\nFigure 12. Quantitative Results: B2FS vs Stereo Techniques.\nusing it to deepen our understanding and interpretation of\nStereo Vision and Human Perception. Note that our ap-\nproach, in principle, could be extended straightforwardly\nto transparent surfaces, relaxing on GC2 to allow multiple\ndisparities.\nThis work paves the way for future research on hy-\nbrid AI approaches that balance data-driven learning\nwith structured reasoning. Expanding beyond Stereo Vi-\nsion, this methodology could enhance AI applications in\nrobotics, medical imaging, virtual reality, and autonomous\nnavigation, where understanding the underlying geome-\ntry of the environment is as crucial as pattern recogni-\ntion.\nReferences\n[1] P. N. Belhumeur. A bayesian approach to binocularsteropsis.\nInternational Journal of Computer Vision, 19(3):237 – 260,\n1996. 2, 3, 4\n[2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter\nWonka, and Matthias M¨uller.\nZoedepth: Zero-shot trans-\nfer by combining relative and metric depth. arXiv preprint\narXiv:2302.12288, 2023. 2\n[3] Aleksei Bochkovskii, Ama¨el Delaunoy, Hugo Germain,\nMarcel Santos, Yichao Zhou, Stephan R Richter, and\nVladlen Koltun. Depth pro: Sharp monocular metric depth in\nless than a second. arXiv preprint arXiv:2410.02073, 2024.\n2, 5\n[4] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate en-\nergy minimization via graph cuts. IEEE Transactions on Pat-\n8\n\n\ntern Analysis and Machine Intelligence, 23(11):1222–1239,\n2001. 2\n[5] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao,\nand Chao Dong.\nActivating more pixels in image super-\nresolution transformer. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n22367–22377, 2023. 6\n[6] Leonardo Da Vinci. A treatise on painting. Read Books Ltd,\n2014. 2\n[7] D. Geiger, B. Ladendorf, and A. L. Yuille.\nOcclusions\nand binocular stereo. In Second European Conference on\nComputer Vision, pages 425–433, Berlin, Heidelberg, 1992.\nSpringer Berlin Heidelberg. 2, 3, 4\n[8] D. Geiger, B. Ladendorf, and A. L. Yuille. Occlusions and\nbinocular stereo. International Journal of Computer Vision,\n14(3):211–226, 1995. 2, 3\n[9] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long,\nHao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and\nShaojie Shen. Metric3d v2: A versatile monocular geomet-\nric foundation model for zero-shot metric depth and surface\nnormal estimation. arXiv preprint arXiv:2404.15506, 2024.\n2\n[10] Hiroshi Ishikawa and Davi Geiger. Occlusions, discontinu-\nities, and epipolar lines in stereo. In European Conference\nin Computer Vision (ECCV)’98, pages 232–248. Springer\nBerlin Heidelberg, 1998. 2, 3, 4\n[11] David G. Jones and Jitendra Malik. A computational frame-\nwork for determining stereo correspondence from a set of\nlinear spatial filters.\nIn Second European Conference on\nComputer Vision, pages 395–410. Springer Berlin Heidel-\nberg, 1992. 2\n[12] Bela Julesz. Foundations of cyclopean perception. Univer-\nsity of Chicago Press, 1971. 1, 2\n[13] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Met-\nzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurpos-\ning diffusion-based image generators for monocular depth\nestimation.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9492–\n9502, 2024. 2\n[14] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Zi-\nwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and\nShuaicheng Liu. Practical stereo matching via cascaded re-\ncurrent network with adaptive correlation. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 16263–16272, 2022. 2\n[15] Zhenyu Li, Shariq Farooq Bhat, and Peter Wonka. Patch-\nfusion:\nAn end-to-end tile-based framework for high-\nresolution monocular metric depth estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10016–10025, 2024. 2\n[16] Lahav Lipson, Zachary Teed, and Jia Deng.\nRaft-stereo:\nMultilevel recurrent field transforms for stereo matching. In\n2021 International Conference on 3D Vision (3DV), pages\n218–227. IEEE, 2021. 2, 5, 7\n[17] D. Marr and T. Poggio. A computational theory of human\nstereo vision. In Proc. R. Soc. Lond., page 301–328, 1979. 2\n[18] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia\nSegu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth:\nUniversal monocular metric depth estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10106–10116, 2024. 2\n[19] Ren´e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE transactions on pattern analysis and machine\nintelligence, 44(3):1623–1637, 2020. 2\n[20] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 12179–12188, 2021. 2\n[21] Daniel Scharstein, Heiko Hirschm¨uller, York Kitajima,\nGreg Krathwohl, Nera Neˇsi´c, Xi Wang, and Porter West-\nling. High-resolution stereo datasets with subpixel-accurate\nground truth. In Pattern Recognition: 36th German Confer-\nence, GCPR 2014, M¨unster, Germany, September 2-5, 2014,\nProceedings 36, pages 31–42. Springer, 2014. 3\n[22] Hermann von Helmholtz. Treatise on Physiological Optics,\nVolume III. German edition and, Dover Publications, New\nYork, [1962], 1962. 1, 2\n[23] J. Wang and T. Zickler. Local detection of stereo occlusion\nboundaries. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 3,\n4\n[24] Xianqi Wang, Gangwei Xu, Hao Jia, and Xin Yang.\nSelective-stereo: Adaptive frequency information selection\nfor stereo matching. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n19701–19710, 2024. 2, 7\n[25] Zhenping Xia, Yujie Liu, Yi Bai, Yueyuan Zhang, and Cheng\nCheng. Mismatches between 3d content acquisition and per-\nception cause more visually induced motion sickness. PRES-\nENCE: Virtual and Augmented Reality, 33:481–492, 2024. 3\n[26] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi\nFeng, and Hengshuang Zhao. Depth anything: Unleashing\nthe power of large-scale unlabeled data. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10371–10381, 2024. 2\n[27] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-\ngang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-\nthing v2. arXiv preprint arXiv:2406.09414, 2024. 2, 5\n[28] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,\nKaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\nTowards zero-shot metric 3d prediction from a single image.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 9043–9053, 2023. 2\n[29] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen,\nYitong Yang, and Yong Zhao. High-frequency stereo match-\ning network. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 1327–1336,\n2023. 2\n9\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21280v1.pdf",
    "total_pages": 9,
    "title": "Back to the Future Cyclopean Stereo: a human perception approach unifying deep and geometric constraints",
    "authors": [
      "Sherlon Almeida da Silva",
      "Davi Geiger",
      "Luiz Velho",
      "Moacir Antonelli Ponti"
    ],
    "abstract": "We innovate in stereo vision by explicitly providing analytical 3D surface\nmodels as viewed by a cyclopean eye model that incorporate depth\ndiscontinuities and occlusions. This geometrical foundation combined with\nlearned stereo features allows our system to benefit from the strengths of both\napproaches. We also invoke a prior monocular model of surfaces to fill in\nocclusion regions or texture-less regions where data matching is not\nsufficient. Our results already are on par with the state-of-the-art purely\ndata-driven methods and are of much better visual quality, emphasizing the\nimportance of the 3D geometrical model to capture critical visual information.\nSuch qualitative improvements may find applicability in virtual reality, for a\nbetter human experience, as well as in robotics, for reducing critical errors.\nOur approach aims to demonstrate that understanding and modeling geometrical\nproperties of 3D surfaces is beneficial to computer vision research.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}