{
  "id": "arxiv_2502.21162v1",
  "text": "Parallel-Learning of Invariant and Tempo-variant Attributes of Single-Lead\nCardiac Signals: PLITA\nAdrian Atienza, Jakob E. Bardram, Sadasivan Puthusserypady\nTechnical University of Denmark\n{adar, jakba, sapu}@dtu.dk\nAbstract\nWearable sensing devices, such as Holter monitors, will play a\ncrucial role in the future of digital health. Unsupervised learn-\ning frameworks such as Self-Supervised Learning (SSL) are\nessential to map these single-lead electrocardiogram (ECG)\nsignals with their anticipated clinical outcomes. These signals\nare characterized by a tempo-variant component whose pat-\nterns evolve through the recording and an invariant component\nwith patterns that remain unchanged. However, existing SSL\nmethods only drive the model to encode the invariant attributes,\nleading the model to neglect tempo-variant information which\nreflects subject-state changes through time. In this paper, we\npresent Parallel-Learning of Invariant and Tempo-variant At-\ntributes (PLITA), a novel SSL method designed for capturing\nboth invariant and tempo-variant ECG attributes. The latter\nare captured by mandating closer representations in space for\ncloser inputs on time. We evaluate both the capability of the\nmethod to learn the attributes of these two distinct kinds, as\nwell as PLITA’s performance compared to existing SSL meth-\nods for ECG analysis. PLITA performs significantly better in\nthe set-ups where tempo-variant attributes play a major role.\nIntroduction\nThe wearable sensing field has seen remarkable advance-\nments in recent years. By enabling real-time data collection\non physiological parameters, these devices will play a crucial\nrole in the future of digital health. Among these wearable\nsensors, the Holter monitor captures the cardiac machinery as\nsingle-lead ECG signals. Leveraging the information that is\naccommodated in these signals has the potential of providing\noutstanding benefits: (i) Facilitating the early identification\nof irregular heart rhythms, such as Atrial Fibrillation (AFib),\n(ii) Simplifying the diagnostic process and minimizing\nthe necessity for comprehensive testing (Himmelreich\net al. 2019), and (iii) Enabling users to engage proactively\nin tracking their heart health by offering instant access\nto health data and insights (Abdou and Krishnan 2022).\nGeneric models are mandated to map these data with their\nanticipated clinical outcomes. These models should compute\ninformative single-lead ECG representations applicable to\nseveral downstream tasks and be optimized using large\nvolumes of unlabelled data. This makes Self-Supervised\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nLearning (SSL) framework particularly well-suited for\naddressing this clinical challenge.\nFigure 1: A pair ECG strips from distinct subjects are shown.\nThe signal morphology accommodates a strong stationary\ncomponent with visible differences between the subjects.\nTo ensure blood reaches the entire body, each heart peri-\nodically repeats a sequence of actions, i.e., contractions,\nrelaxations, and repolarizations, involving its different\nchambers with clockwise precision. This unique execution\nof actions results in signals that exhibit a strong stationary\ncomponent which is distinctive among hearts, as shown\nin Figure 1. These stationary attributes accommodate\nmeaningful information such as the subject‘s gender (Attia\net al. 2019b) or tendency to cardiac arrhythmia (Attia et al.\n2019a). In parallel, the functioning of the heart evolves,\nsince it has to adapt to the individual’s temporary needs or it\nmay fall into arrhythmias. This evolution, also captured in\nthe recordings, adds a non-stationary component to the data.\nTherefore, single-lead ECG signals are characterized by two\ncomponents of distinct kinds. The stationary component re-\nmains unchanging over time. Conversely, the non-stationary\ncomponent is time-sensitive and its patterns evolve through\nthe recording. This paper will refer to these components as\ninvariant and tempo-variant attributes, respectively.\nCurrent SSL techniques designed for single-lead ECG\nprocessing (Diamant et al. 2022; Kiyasseh, Zhu, and\nClifton 2021; Wickstrøm et al. 2022) enforce the model\nto understand and encode the signal invariant patterns\nfollowing a Contrastive Learning (Chen et al. 2020)\napproach. Despite each having its uniqueness, they all\narXiv:2502.21162v1  [cs.LG]  28 Feb 2025\n\n\nFigure 2: Considering time-sorted inputs equally spaced in\ntime (X1 . . . X4), representations of nearby inputs in time\nare expected to be closer than time-distant ones.\nconsider non-overlapping signal strips from the same subject\nas positive pairs and enforce similar representations between\nthem. These studies demonstrate that by exploiting the\nnon-stationarity nature of the data, the invariant information\nof the Single-Lead ECG signals are better captured within\nthe representations rather than creating two versions of\nthe same input using data augmentation techniques. This\ncommon strategy is aligned with other time series SSL\nworks, (Qian et al. 2021; Jing et al. 2019; Zhang and Crandall\n2021), where non-overlapping frames from the same video\nare considered as positive pairs during the training procedure.\nHowever, solely focusing on driving the model to capture the\ninvariant attributes only covers a part of the whole picture.\nIn other words, simply mandating similar representations\nfrom inputs belonging to the same recording will lead to the\nmodel neglecting the tempo-variant attributes, and thereby\nthese changes over time. It leads to a loss of meaningful\ninformation that is particularly valuable in specific scenarios\naimed at identifying occasional cardiovascular events that\noccur at irregular intervals throughout the recording, such as\ndetecting AFib or classifying sleep stages.\nTo address this drawback, this paper presents Parallel-\nLearning of Invariant and Tempo-variant Attributes (PLITA),\na novel SSL method designed to represent both the invariant\nand the tempo-variant attributes of single-lead ECG signals.\nThe proposed PLITA approach is consistent with method-\nologies like Split Invariant-Equivariant (SIE) (Garrido,\nNajman, and Lecun 2023), where the model is designed to\ncapture attributes of two different kinds. PLITA compels\nits learning model to integrate tempo-variant attributes\nby ensuring that the representations adhere to a coherent\nprinciple: representations of temporally proximate inputs\nshould be closer than those of temporally distant inputs. This\nconcept is depicted in Figure 2 and is encapsulated in the\ninnovative “Tempo-variant Loss Function” (Ltv), which\nforms an integral part of the training objective.\nIn this study, we hypothesize that: (i) Tempo-variant attributes\ncontain significant information that is distinct from the in-\nformation conveyed by invariant ones, (ii) Simply using the\ntempo-variant attributes as a source of natural variance lim-\nits the potential of the representation in several downstream\ntasks, (iii) These attributes can also be incorporated within\nthe representations by the proposed Ltv loss function, and\n(iv) By encoding these attributes, the model performance\nimproves significantly in set-ups where the tempo-variant\nattributes play an important role. To assess these hypotheses,\nwe have conducted three experiments that require the invari-\nant or/and the tempo-variant attributes to be encoded within\nthe representations:\n1. AFib Classification. AFib episodes can be sporadic in\ntime. Moreover, the susceptibility of an individual to this\ndisease is indicated in the baseline signal (Attia et al.\n2019a). Therefore, both the invariant and tempo-variant\nattributes will play a role in this task.\n2. Sleep Stages Classification. Various sleep stages occur\nthroughout the sleep cycle, regardless of the individual.\nConsequently, it is essential for the model to capture the\ntempo-variant attributes to successfully perform this task.\n3. Gender Identification task. Gender-related information\nwill be persistent within the data throughout the recording.\nTherefore, driving the model to encode the invariant\nattributes will be required in this task.\nWe have evaluated the performance of PLITA against the\nstate-of-the-art (SOTA) methods designed for sinle-lead ECG\nprocessing. The findings indicate a marked enhancement\nin performance on downstream tasks where tempo-variant\ncharacteristics are influential. Furthermore, the model demon-\nstrates robust results in gender classification, confirming also\nthe presence of invariant features within the representations.\nAdditionally, we assess whether both tempo-variant and\ninvariant features are reliably captured in the representations\nand if they fulfill their intended function in downstream tasks.\nIn summary, the contributions of this paper are:\n• We demonstrate through empirical results that the tempo-\nvariant attributes of cardiac signals are not merely a source\nof natural variation for efficient invariant attribute learn-\ning, as assessed in previous studies, but must also be\nintegrated into the representations.\n• We introduce PLITA, a novel SSL method that takes apart\nfrom existing ECG processing SSL methods by driving\nthe model to encode the tempo-variant attributes via the\nnovel Ltv function.\n• We establish a new approach for harnessing tempo-variant\nfeatures. We hypothesize that this may motivate future\nSSL work not only applied to ECG analysis but also\nbroadly to other time series data.\n\n\nFigure 3: PLITA illustrated. Built on top of BYOL, PLITA includes both a student and a teacher network. For the sake of\nclarity, the teacher network is not included in the illustration. The losses are computed between representation triplets from both\nnetworks that process data equally. While Liv is computed between a set of N time series representations belonging to different\nrecords (displayed in black and blue colors), Ltv is computed between representations belonging to the same record. All inputs\nbelong to the same subject. The encoder (F(X)) is saved at the end of the training procedure and used for downstream tasks.\nRelated Work\nTempo-Variant attributes in Time Series\nTime series data represents the evolution of an object of in-\nterest over time. Existing methods (Qian et al. 2021; Jing\net al. 2019; Zhang and Crandall 2021) leverage naturally\noccurring variations in consecutive frames to define posi-\ntive pairs, avoiding heavy reliance on data augmentation\ntechniques. They provide evidence that this approach en-\nhances the model’s effectiveness in handling downstream\ntasks. Another example of how the availability of a sequence\nof inputs can improve model training is Siamese Masked Au-\ntoencoders work (Gupta et al. 2023), which extends Masked\nAutoencoders (He et al. 2021) by optimizing the model to\nreconstruct the subsequent frames instead of the actual one.\nSSL in Single-Lead ECG Signal Processing\nExisting SSL methods tailored for single-lead ECG data\nare aligned with SSL methods for video processing. They\nleverage the data shifts across time for enhancing the learning\nof the invariant attributes. They all utilize the Contrastive\nLearning (Chen et al. 2020) as a common framework, consid-\nering non-overlapping inputs as positive pairs. The details of\nthe positive-pair selection strategy set these methods apart: (i)\nThe Mixing-Up method (Wickstrøm et al. 2022) introduces\na more tailored data augmentation product of two time\nseries from the same recording. (ii) Contrastive Learning of\nCardiac Signals Across Space (CLOCS) (Kiyasseh, Zhu, and\nClifton 2021) utilizes two consecutive ECG time strips as\npositive pairs, and (iii) Patient Contrastive Learning (PCLR)\n(Diamant et al. 2022) which considers two time strips from\nthe same subject but different recordings. PLITA inherits\nthe PCLR strategy by defining positive pairs from distinct\nrecordings to ensure the representation of invariant attributes,\nas it outperforms the other methods (See Evaluation).\nWhile the previous studies only focus on the invariant at-\ntributes, Intra-inter Subject Self-Supervised Learning (ISL)\n(Lan et al. 2022) mandates representing alterations between\nconsecutive beats. PLITA contemplates the tempo-variant\ninformation spotted among temporally sparse inputs, not just\nbetween consecutive beats. The proposed method incorpo-\nrates a novel loss function that drives the model to encode\nthis information by comparing these delayed inputs.\nSSL in 12-Lead ECG Signal Processing\nThe most recent work on the ECG field focuses on 12-Lead\nsignals. Having multiple leads opens up the spatial dimension\nand thus the range of possibilities when designing methods\nto process this kind of data (Na et al. 2024; Wang et al. 2023).\nHowever, it is not possible to adapt them to single-lead\nECG processing. Part of their potential is based precisely\non exploiting the spatial dimension, which is not available\nin the wearable sensing field in which this study is placed.\nTherefore, they are not included as baselines during the\nevaluation of PLITA.\nParallel-Learning of Invariant and\nTempo-variant Attributes (PLITA)\nThe aim of PLITA is to simultaneously drive the model to\nrecognize both the invariant and tempo-variant attributes and\nencode them in the representations. Its workflow is illustrated\nin Figure 3. Here, N inputs equally delayed in time within a\nwindow size W are sampled from two records belonging to\nthe same subject. The inputs are given to the learning model,\ndisplayed as F(X). The model computes the representations\n(denoted as h), which are passed through the invariant and\ntempo-variant projectors (denoted as Giv and Gtv, respec-\ntively). This workflow is mimicked by the teacher network.\nThe invariant distance matrix, denoted as ˆM\niv, is calculated\n\n\nbetween the invariant projections of both recordings (z1\niv and\nz2\niv,). The invariant loss function, Liv, minimizes the values\nof ˆM\niv. Parallel to this, the tempo-variant distance matrix,\nˆM\ntv, is calculated between the tempo-variant projections\nfrom the same recordings (z2\ntv). This matrix is compared with\nthe ideal tempo-variant distance matrix (Mtv) by the tempo-\nvariant loss function, Ltv. The student encoder (F(X)) is\nthe module used when addressing the downstream tasks. The\nother components are discarded after the training.\nProjecting the Invariant and Tempo-variant into\nDistinct Spaces\nPLITA is matched with other existing methods such as\nSIE, which are designed to integrate various types of\nattributes into a singular representation. Analogously, PLITA\nincorporates two projectors (Giv and Gtv) to project the\nrepresentations into two different spaces and avoid some\nconflicting goals during the training procedure. These arise\nfrom the fact that while Liv minimizes the distance between\nthe representations, Ltv encourages these distances between\ntime-sorted inputs to occur and follow a spatial order.\nThe equivariant attributes defined in SIE’s method are not re-\nlated to the tempo-variant ones, nor how they are considered\nin each respective method. PLITA also differs from SIE in\nnot splitting the representations nor incorporating the Lreg\nterm. Variations in baseline ECGs are crucial for identifying\nspontaneous episodes of cardiovascular diseases (Attia et al.\n2019a). We contend that a holistic consideration of the repre-\nsentation will yield more precise tempo-variant projections.\nMoreover, the divergent objectives of Liv and Ltv are antici-\npated to be adequate in preventing mode collapse between the\ntwo projections. Yet the implications of these two decisions\nhave been explored (Refer to Appendix).\nCapturing Invariant and Tempo-variant Attributes\nThis section provides a detailed description of the main tech-\nnical contribution of this work, which is PLITA’s proposed\nparallel learning for driving the model to capture both the\ninvariant and the tempo-variant attributes of cardiac signals.\nCapturing Invariant Attributes:\nSimilarly to PCLR\nmethod, PLITA drives the model to encode invariant at-\ntributes by employing inputs from different recordings from\nthe same subject. Nevertheless, PLITA approaches this ob-\njective in a Non-Contrastive Learning fashion, being built\non top of Bootstrap Your Own Latent (BYOL) framework.\nBoth decisions are supported by superior performance in the\ninvariant downstream task of gender classification (see Eval-\nuation). We use the “cosine similarity” for calculating the\ndistance matrix between the invariant projections, denoted as\nˆM\niv and it defined as the following:\nˆM\niv\ni,j = 1 −\n(ζ1\niv)i · Qiv((z2\niv)j)\nmax\n\u0000∥(ζ1\niv)i∥2 · ∥Qiv((z2\niv)j)∥2 , ϵ\n\u0001,\n(1)\nwhere (ζ1\niv)i and Qiv((z2\niv)j) are the invariant outputs of\nthe teacher and student networks respectively, for the inputs\nwith index i and j drawn from the records 1 and 2. Note that\nthe BYOL framework features both a teacher and a student\nnetwork. These parallel networks and the student projector\nare not illustrated in Figure 3 for the sake of clarity. Yet in\npractice, we calculate each loss function by comparing the\noutput of the student prediction, (Q(z)) with the subsequent\noutput of the teacher projector (ζ). This logic also applies to\nthe Liv introduced below.\nMaking the invariant projections similar implies minimizing\nthe values of ˆM\niv\ni,j, therefore we define the Liv as;\nLiv =\n1\nN 2\nN\nX\ni\nN\nX\nj\nˆM\niv\ni,j.\n(2)\nCapturing Tempo-variant Attributes:\nPLITA requires\nthe model to compute representations in a spatial order\naligned with the chronological sequence of time. In other\nwords, the closer the inputs are on time, the closer the rep-\nresentations should be in space. This desired behavior is\nmodeled by the so-called “Ideal Tempo-variant Distances\nMatrix” (Mtv), which is defined as;\nMtv\ni,j = |i −j|\nN −1,\n(3)\nwhere i and j are the indices of the inputs sorted in time, and\nN is the number of inputs considered in each window size.\nAlthough we consider the “cosine similarity” as the distance\nmetric between two representations from the same recording,\nother choices for distance metrics have been evaluated (Refer\nto Appendix). These pair-wise distances are captured by ˆM\ntv,\nwhich is computed as;\nˆM\ntv\ni,j = 1 −\n(ζ2\ntv)i · Qtv((z2\ntv)j)\nmax (∥(ζ2\ntv)i∥2 · ∥Qtv((z2\ntv)j)∥2 , ϵ),\n(4)\nwhere (ζ2\ntv)i and Qtv((z2\niv)j) are the tempo-variant outputs\nof the teacher and student networks respectively, for the in-\nputs with index i and j drawn from the same record. ˆMtv is\nscaled before calculating the Liv as the following;\nˆM\ntv\n′\n= a + ( ˆM\ntv −min( ˆM\ntv))(b −a)\nmax( ˆM\ntv) −min( ˆM\ntv)\n,\n(5)\nwhere a = 1/(N −1) and b = 1. By scaling ˆM\ntv, we do\nnot only ensure that the values of ˆM\ntv\n′\nand Mtv lie within\nthe same range, but also we alleviate the constraints imposed\nby Ltv. PLITA just mandates the representations to follow a\ntempo-spatial order without imposing a constant distance for\nevery set of inputs. This constant distance would be a problem\nsince the same magnitude of variance can not be expected for\neach set in the batch. The final Ltv that enforces the tempo-\nvariant attributes to be represented into the representations is\ndefined as;\nLtv =\n1\nN(N −1)\nN\nX\ni\nN\nX\nj̸=i\n\u0010\n(Mtv)i,j −ˆM\ntv\n′ )i,j\n\u00112\n.\n(6)\nNote that PLITA does not take into account the diagonal\nterms, since the invariant features are expected to be modeled\nby the Liv loss function. The evaluation of the tempo-variant\nloss term’s integration is presented in (See Ablation).\n\n\nEvaluation Task\nAFIB Classification\nSleep Stage Classification\nGender Identification\nPre-Train Dataset\nSHHS\nIcentia\nSHHS\nIcentia\nSHHS\nIcentia\nMethod / Metric\nAccu. (%)\nF1 Score\nAccu. (%)\nF1 Score\nAccu. (%)\nAUC\nAccu. (%)\nAUC\nAccu. (%)\nAUC\nAccu. (%)\nAUC\nPCLR\n76.4\n73.7\n73.7\n73.6\n71.6\n0.75\n72.3\n0.77\n76.4\n0.84\n66.5\n0.74\nMix-Up\n73.4\n72.3\n62.9\n57.3\n73.8\n0.79\n72.6\n0.75\n70.4\n0.76\n64.2\n0.69\nCLOCS\n75.7\n73.8\n73.6\n72.7\n73.2\n0.78\n72.0\n0.74\n70.4\n0.76\n65.3\n0.7\nBYOL\n76.6\n74.8\n75.3\n72.5\n72.9\n0.77\n73.3\n0.78\n76.7\n0.83\n66.7\n0.72\nTi-MAE\n72.1\n70.9\n52.9\n60.0\n69.3\n0.61\n69.3\n0.66\n60.0\n0.6\n60.0\n0.61\nSiam Auto\n76.5\n73.0\n53.3\n70.1\n72.9\n0.74\n69.3\n0.68\n73.3\n0.8\n55.4\n0.47\nPLITA\n80.7\n78.4\n80.0\n78.2\n75.3\n0.81\n74.8\n0.8\n76.5\n0.83\n66.5\n0.72\nTable 1: Evaluation Results for the three downstream tasks, using the pretrained model trained from both Icentia and SHHS\ndatasets. The bold type indicates the best-performing method for each metric.\nImplementation Details\nTo ensure the replication of the method, we meticulously out-\nline the hyperparameter settings and the model architecture.\nModel Architecture: The Vision Transformer (ViT) (Doso-\nvitskiy et al. 2021) model is used for processing the\nsingle-lead ECG signals. The input consists of a one\ndimensional 10-second signal sampled at 100 Hz. The\npatch size is set to 20. The model counts with 6 regular\ntransformer blocks with 4 heads each and a dimension of 128.\nPLITA Implementation and Optimization: The window\nsize W is set to 10 seconds. N is set to 4, so 4 inputs are\ndrawn from each window. The effect of both W and N is\ndiscussed in Section . Although we do not incorporate any\ndata augmentation, the effect of it is discussed in the Ap-\npendix. The projectors and predictors are implemented as a\ntwo-layer Multilayer Perceptron (MLP) with a dimension-\nality of 512 and 256, respectively. The exponential moving\naverage (EMA) updating factor (τ) is set to 0.995. The train-\ning procedure consists of 35,000 iterations. We use a batch\nsize of 256, Adam (Kingma and Ba 2017) with a learning\nrate of 3e−4, and a weight decay of 1.5e−6 as the optimizer.\nThe training procedure and the evaluations are performed on\na desktop computer, with a Nvidia GeForce RTX 3070 GPU.\nEvaluation\nIn this section, we evaluate the performance of PLITA\ncompared with the most relevant existing SSL methods\nfor single-lead ECG processing. We also have conducted\na study on representations to assess whether the model’s\nlearned representations effectively separate the invariant and\ntempo-variant attributes. Overall, the evaluation involves\nfour distinct databases: MIT-BIH Atrial Fibrillation Database\n(MIT-AFIB) (Moody and Mark 1983), MIT-BIH Polysomno-\ngraphic Database (MIT-PSG), (Ichimaru and Moody 1999),\nPhysionet Challenge 2017 (Cinc2017) (Clifford et al. 2017)\nand Sleep Heart Health Study (SHHS) (Zhang et al. 2018).\nAll databases are publicly available in Physionet (Goldberger\net al. 2000) and National Sleep Research Resource (NSRR).\nComparison against state-of-the-art (SOTA)\nThe performance of the proposed PLITA method has been\ncompared against the three most relevant energy-based\nSOTA methods, namely, (i) PCLR (Diamant et al. 2022), (ii)\nCLOCS (Kiyasseh, Zhu, and Clifton 2021), and (iii) Mixing-\nUp (Wickstrøm et al. 2022). Reconstruction methods such as\n(iv) Ti-MAE (Li et al. 2023) or (v) Siamese Masked Autoen-\ncoderss (Gupta et al. 2023) (For more details about this latter\nimplementation, refer to Appendix). Finally, We have also in-\ncluded (vi) the BYOL method tailored by ECG processing by\nfollowing the PCLR strategy for selecting the positive pairs.\nTo guarantee an equitable assessment, we have optimized\nthe identical model employed in this study, maintaining con-\nsistent settings such as the optimizer, data, batch size, and\niteration count. Each method has been trained in two distinct\ndatasets, SHHS (Zhang et al. 2018; Quan et al. 1998) and\nIcentia (Tan et al. 2019), that are composed of long-term\nsingle-lead ECG recordings.\nAFib Classification:\nTo assess the ability of the method\nto generalize different classes within the same record, given\na limited number of labelled records, we have conducted\na Leave-One-Out (LOO) cross-validation across the 23\nMIT-AFIB subjects. This ensures no subject-overlapping\nbetween the training and validation sets which would sig-\nnificantly simplify AFib identification. A Support Vector\nClassificatier (SVC) (Platt 2000) is fitted on top of the repre-\nsentations. Table 1 reflects the results, where it can be seen\nthat PLITA significantly outperforms the other methods.\nSleep Stage Detection:\nWe have used the MIT-PSG\ndatabase in order to assess the capability of the representa-\ntions to discriminate between Sleep and Wake classes. Since\nthe golden standard classification is performed every 30 sec-\nonds and our model has been optimized for processing 10-\nsecond signals, a Gated Recurrent Unit (GRU) (Cho et al.\n2014) layer is fitted on top of the representations during 5\nepochs for processing 30 seconds of data sequentially, in\nnon-overlapping 10-second chunks. The pre-trained model\nis kept frozen. We have carried out a LOO cross-evaluation\nfor the 18 records contained in the dataset. The outcomes of\nthis analysis are detailed in Table 1. It is evident that PLITA\nachieves a notably higher level of performance.\n\n\n(a) Disentangling Study for invariant features in MIT-AFIB dataset.\n(b) SHAP Analysis in gender classification.\nFigure 4: The features that play an important role in the gender classification task (displayed in Figure 4b), are highlighted in\ngreen in Figure 4a. The feature that accounts for the AFib classification task (Figure 5b) is displayed in purple.\n(a) Disentangling Study for tempo-variant features in MIT-AFIB.\n(b) SHAP Analysis in AFib classification.\nFigure 5: The informative features in the AFib classification are displayed in Figure 5b and highlighted in purple in Figure 5a.\nGender Classification:\nWe conducted a five-fold cross-\nvalidation over 1500 randomly-selected inputs from distinct\nsubjects from the SHHS database. A SVC is fitted on top of\nthe representations. Table 1 shows that despite not achieving\nthe best performance, PLITA reaches competitive results.\nResults conclude that training with Icentia data tends to\nyield worse results, possibly due to increased noise in the\ndata. This decline in performance is more pronounced for\nreconstruction-based methods. PLITA performs best in tasks\nthat involve encoding tempo-variant attributes. The compa-\nrable results achieved in gender classification were also ex-\npected due to the only influence of the invariant attributes\nin this task and the identical manner of BYOL, PCLR and\nPLITA to drive the model to encode them.\nRepresentation Study\nThe purpose of this two-phase experimental analysis is to\nverify that: (i) PLITA prompts the model to encapsulate both\ninvariant and tempo-variant attributes into a suite of discrete\nfeatures that are consistent across different recordings, and (ii)\nThe collection of invariant features is crucial for the gender\nclassification task, as anticipated. Additionally, the tempo-\nvariant attributes are highly relevant for AFib classification\n(for more details on the AFib experiment, see Appendix).\nDisentangling Study:\nIn order to verify that the two non\noverlapping sets of features that express the invariant and\ntempo-variant attributes across the different records are\nconsistent, we have computed the representations of the 23\nrecordings from the MIT-AFIB dataset. Each feature value is\nnormalized to ensure uniform value ranges. Since it can be\nassumed that invariant and tempo-variant features will have\nlow and high intra-record variance respectively, this variance\nis used as a measure of discretization. We cluster the 33% of\nthe features with the least variance as the invariant features\nwhile the 33% of them with the highest variance are clustered\nas the tempo-variant ones. Finally, we tallied the occurrences\nof each feature in both clusters for each recording.\nFigure 4a shows the 20 features that appear most often as\ninvariant features for each recording. The ratio of appearances\ngoes from 95.7% to 56.2% so we can give it a statistical value\nsince in a random baseline, the number of appearances would\nbe around 33.3%. This serves as evidence that the features\nwhich represent the invariant attributes of the ECG signals\nare consistent across recordings. In a similar manner, Figure\n5a represents the 20 tempo-variant features, with a ratio of\nappearances from 90% to 50%. Therefore, it is assessed that\nthe tempo-variant features are also consistent.\nSHapley Additive exPlanations (SHAP) Analysis:\nWe\nhave conducted a SHAP analysis (Lundberg and Lee 2017)\n\n\n(a) AFib Classification.\n(b) Sleep Stage Classification\nFigure 6: Effect of incorporating the tempo-variant attributes in distinct tasks. Blue and Orange bars represent that the model has\nbeen pre-trained on SHHS and Icentia respectively.\nfor the AFib and gender classification tasks. Since the\nDisentangling study has been carried out in the MIT-AFIB\ndatabase, we used the AFib and SR instances from Cinc2017,\nadhering to the training and testing set proposed on it. For the\ngender task, we used the same dataset used in the evaluation.\nFigure 4b and Figure 5b reflect how four and three of the five\nmost important features are included among the invariant and\ntempo-variant sets, respectively. This result is of statistical\nrelevance, since in a random baseline only 6e−3 features\nwould be included in each of the 20-size set of features.\nNotably, the third most important feature for AFib detection\nexhibits an invariant nature (Feature 84). Although at first\nsight, this may seem contradictory, this aligns PLITA with\nthe findings of other studies (Attia et al. 2019a) which claim\nthat invariant attributes present in ECG baselines enable dis-\ncretizing the subjects that are susceptible to suffer episodes.\nDiscussion of the Results\nThroughout this comprehensive evaluation, it has been es-\ntablished that PLITA successfully captures both invariant\nand tempo-variant features within a unified representation.\nMoreover, this capability allows PLITA to achieve markedly\nenhanced results in a variety of downstream tasks, as detailed\nin Table 1. These findings provide robust evidence in favor\nof the hypotheses posited by this study: (i) Tempo-variant\nfeatures hold valuable information that is distinct from that of\ninvariant features, (ii) Merely utilizing tempo-variant features\nas a source of natural variability restricts the representation’s\neffectiveness in numerous downstream tasks, (iii) These fea-\ntures can be integrated into the representations through the\nproposed Ltv loss function, and (iv) The inclusion of these\nfeatures leads to a significant improvement in model perfor-\nmance in scenarios where tempo-variant features are crucial.\nAblation and Sensitivity Studies\nWe have studied both the effect of incorporating the novel\nLtv loss function as well as the role of the hyperparameters\nwhen computing it. Figure 6 demonstrates that the incorpora-\ntion of Ltv leads to a significant and positive impact on the\ntempo-variant related tasks.\nThe impact of the hyperparameters introduced, i.e the number\nof inputs from each recording (N) and the window size (W)\nhave also been evaluated. Table 2 indicates that the selected\nconfiguration, highlighted in bold, achieves the best perfor-\nmance, but also that all configurations yield superior results\ncompared with existing methods.\nDownstream Task\nAFIB Classification\nSleep Stage Classification\nPre-Train Dataset\nSHHS\nIcentia\nSHHS\nIcentia\nN\nW (Seconds)\nAccu. (%)\nAccu. (%)\nAccu. (%)\nAccu. (%)\n3\n120\n79.5\n77.6\n74.3\n73.7\n4\n120\n80.7\n80.0\n75.3\n74.8\n5\n120\n77.3\n78.6\n73.2\n73.9\n4\n90\n76.6\n77.8\n74.1\n74.5\n4\n150\n76.9\n79.4\n73.3\n72.9\nTable 2: Sensitivity Study.\nConclusions\nThis research provides strong evidence that merely using the\ntempo-variant attributes of ECG signals as a source of natu-\nral variability is insufficient. To overcome this, we introduce\nPLITA, a novel SSL technique for ECG analysis. By incor-\nporating the Ltv loss function into the training objective, the\nmodel is directed to efficiently encode these tempo-variant\nattributes. This significantly enhances the model’s ability to\nexcel in various tasks where such attributes are crucial.\nLimitations:\nPLITA has only been evaluated on a single\narchitecture (ViT). In addition, only BYOL has been uti-\nlized to capture the invariant features, leaving aside other\nnon-contrastive frameworks such as Self-Distillation with no\nLabels (DINO) (Caron et al. 2021) or Variance-Invariance-\nCovariance Regularization (VIC-REG) (Bardes, Ponce, and\nLeCun 2022). However, the incorporation of Ltv, is agnostic\nto any of these two we can hypothesize that a similar perfor-\nmance improvement will be obtained for any combination.\nBroader Impact:\nWe believe that the incorporation tempo-\nvariant information as a training objective will inspire not\nonly future ECG but also in general time series SSL methods.\n\n\nReferences\nAbdou, A.; and Krishnan, S. 2022. Horizons in Single-Lead\nECG Analysis From Devices to Data. Frontiers in Signal\nProcessing, 2.\nAttia, Z.; Noseworthy, P.; Lopez-Jimenez, F.; Asirvatham, S.;\nDeshmukh, A.; Gersh, B.; Carter, R.; Yao, X.; Rabinstein,\nA.; Erickson, B.; Kapa, S.; and Friedman, P. 2019a. An\nartificial intelligence-enabled ECG algorithm for the identifi-\ncation of patients with atrial fibrillation during sinus rhythm:\na retrospective analysis of outcome prediction. The Lancet,\n394.\nAttia, Z. I.; Friedman, P. A.; Noseworthy, P. A.; Lopez-\nJimenez, F.; Ladewig, D. J.; Satam, G.; Pellikka, P. A.;\nMunger, T. M.; Asirvatham, S. J.; Scott, C. G.; Carter, R. E.;\nand Kapa, S. 2019b. Age and Sex Estimation Using Artifi-\ncial Intelligence From Standard 12-Lead ECGs. Circulation:\nArrhythmia and Electrophysiology, 12(9): e007284.\nBardes, A.; Ponce, J.; and LeCun, Y. 2022.\nVICReg:\nVariance-Invariance-Covariance Regularization for Self-\nSupervised Learning. arXiv:2105.04906.\nCaron, M.; Touvron, H.; Misra, I.; Jégou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging Properties in\nSelf-Supervised Vision Transformers. arXiv:2104.14294.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.\nA Simple Framework for Contrastive Learning of Visual\nRepresentations. arXiv:2002.05709.\nCho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning\nPhrase Representations using RNN Encoder-Decoder for\nStatistical Machine Translation. arXiv:1406.1078.\nClifford, G. D.; Liu, C.; Moody, B.; Lehman, L.-w. H.; Silva,\nI.; Li, Q.; Johnson, A. E.; and Mark, R. G. 2017. AF clas-\nsification from a short single lead ECG recording: The Phy-\nsioNet/computing in cardiology challenge 2017. In 2017\nComputing in Cardiology (CinC), 1–4.\nDiamant, N.; Reinertsen, E.; Song, S.; Aguirre, A. D.; Stultz,\nC. M.; and Batra, P. 2022. Patient contrastive learning: A\nperformant, expressive, and practical approach to electrocar-\ndiogram modeling. PLOS Computational Biology, 18(2):\n1–16.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. arXiv:2010.11929.\nGarrido, Q.; Najman, L.; and Lecun, Y. 2023.\nSelf-\nsupervised learning of Split Invariant Equivariant representa-\ntions. arXiv:2302.10283.\nGoldberger, A.; Amaral, L.; Glass, L.; Havlin, S.; Hausdorg,\nJ.; Ivanov, P.; Mark, R.; Mietus, J.; Moody, G.; Peng, C.-\nK.; Stanley, H.; and Physiobank, P. 2000. Components of\na new research resource for complex physiologic signals.\nPhysioNet, 101.\nGupta, A.; Wu, J.; Deng, J.; and Fei-Fei, L. 2023. Siamese\nMasked Autoencoders. arXiv:2305.14344.\nHe, K.; Chen, X.; Xie, S.; Li, Y.; Dollár, P.; and Girshick, R.\n2021. Masked Autoencoders Are Scalable Vision Learners.\narXiv:2111.06377.\nHimmelreich, J. C.; Karregat, E. P.; Lucassen, W. A.; van\nWeert, H. C.; de Groot, J. R.; Handoko, M. L.; Nijveldt,\nR.; and Harskamp, R. E. 2019. Diagnostic Accuracy of a\nSmartphone-Operated, Single-Lead Electrocardiography De-\nvice for Detection of Rhythm and Conduction Abnormalities\nin Primary Care. The Annals of Family Medicine, 17(5):\n403–411.\nIchimaru, Y.; and Moody, G. B. 1999. Development of the\npolysomnographic database on CD-ROM. Psychiatry and\nClinical Neurosciences, 53.\nJing, L.; Yang, X.; Liu, J.; and Tian, Y. 2019. Self-Supervised\nSpatiotemporal Feature Learning via Video Rotation Predic-\ntion. arXiv:1811.11387.\nKingma, D. P.; and Ba, J. 2017. Adam: A Method for Stochas-\ntic Optimization. arXiv:1412.6980.\nKiyasseh, D.; Zhu, T.; and Clifton, D. A. 2021. CLOCS:\nContrastive Learning of Cardiac Signals Across Space, Time,\nand Patients. arXiv:2005.13249.\nLan, X.; Ng, D.; Hong, S.; and Feng, M. 2022. Intra-Inter\nSubject Self-Supervised Learning for Multivariate Cardiac\nSignals. Proceedings of the AAAI Conference on Artificial\nIntelligence, 36(4): 4532–4540.\nLi, Z.; Rao, Z.; Pan, L.; Wang, P.; and Xu, Z. 2023. Ti-\nMAE: Self-Supervised Masked Time Series Autoencoders.\narXiv:2301.08871.\nLundberg, S. M.; and Lee, S.-I. 2017. A Unified Approach to\nInterpreting Model Predictions. In Guyon, I.; Luxburg, U. V.;\nBengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and\nGarnett, R., eds., Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc.\nMoody, G.; and Mark, R. 1983. A new method for detecting\natrial fibrillation using R-R intervals. Computers in Cardiol-\nogy, 227–230.\nNa, Y.; Park, M.; Tae, Y.; and Joo, S. 2024. Guiding Masked\nRepresentation Learning to Capture Spatio-Temporal Rela-\ntionship of Electrocardiogram. In International Conference\non Learning Representations.\nPlatt, J. 2000. Probabilistic Outputs for Support Vector Ma-\nchines and Comparisons to Regularized Likelihood Methods.\nAdv. Large Margin Classif., 10.\nQian, R.; Meng, T.; Gong, B.; Yang, M.-H.; Wang, H.; Be-\nlongie, S.; and Cui, Y. 2021. Spatiotemporal Contrastive\nVideo Representation Learning. arXiv:2008.03800.\nQuan, S.; Howard, B.; Iber, C.; Kiley, J.; Nieto, F.; O’Connor,\nG.; Rapoport, D.; Redline, S.; Robbins, J.; Samet, J.; and\nWahl, P. 1998. The Sleep Heart Health Study: Design, Ratio-\nnale, and Methods. Sleep, 20: 1077–85.\nTan, S.; Androz, G.; Chamseddine, A.; Fecteau, P.; Courville,\nA.; Bengio, Y.; and Cohen, J. P. 2019. Icentia11K: An Un-\nsupervised Representation Learning Dataset for Arrhythmia\nSubtype Discovery. arXiv:1910.09570.\nWang, N.; Feng, P.; Ge, Z.; Zhou, Y.; Zhou, B.; and Wang,\nZ. 2023. Adversarial Spatiotemporal Contrastive Learning\n\n\nfor Electrocardiogram Signals. IEEE Transactions on Neural\nNetworks and Learning Systems, 1–15.\nWickstrøm, K.; Kampffmeyer, M.; Mikalsen, K. Ø.; and\nJenssen, R. 2022.\nMixing up contrastive learning: Self-\nsupervised representation learning for time series. Pattern\nRecognition Letters, 155: 54–61.\nZhang, G.-Q.; Cui, L.; Mueller, R.; Tao, S.; Kim, M.;\nRueschman, M.; Mariani, S.; Mobley, D.; and Redline, S.\n2018. The National Sleep Research Resource: Towards a\nSleep Data Commons. Journal of the American Medical\nInformatics Association, 572–572.\nZhang, Z.; and Crandall, D. 2021. Hierarchically Decou-\npled Spatial-Temporal Contrast for Self-supervised Video\nRepresentation Learning. arXiv:2011.11261.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21162v1.pdf",
    "total_pages": 9,
    "title": "Parallel-Learning of Invariant and Tempo-variant Attributes of Single-Lead Cardiac Signals: PLITA",
    "authors": [
      "Adtian Atienza",
      "Jakob E. Bardram",
      "Sadasivan Puthusserypady"
    ],
    "abstract": "Wearable sensing devices, such as Holter monitors, will play a crucial role\nin the future of digital health. Unsupervised learning frameworks such as\nSelf-Supervised Learning (SSL) are essential to map these single-lead\nelectrocardiogram (ECG) signals with their anticipated clinical outcomes. These\nsignals are characterized by a tempo-variant component whose patterns evolve\nthrough the recording and an invariant component with patterns that remain\nunchanged. However, existing SSL methods only drive the model to encode the\ninvariant attributes, leading the model to neglect tempo-variant information\nwhich reflects subject-state changes through time. In this paper, we present\nParallel-Learning of Invariant and Tempo-variant Attributes (PLITA), a novel\nSSL method designed for capturing both invariant and tempo-variant ECG\nattributes. The latter are captured by mandating closer representations in\nspace for closer inputs on time. We evaluate both the capability of the method\nto learn the attributes of these two distinct kinds, as well as PLITA's\nperformance compared to existing SSL methods for ECG analysis. PLITA performs\nsignificantly better in the set-ups where tempo-variant attributes play a major\nrole.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}