{
  "id": "arxiv_2502.21181v1",
  "text": "Reducing Reward Dependence in RL Through Adaptive\nConfidence Discounting\nMuhammed Yusuf Saticia,* and David L. Robertsb\naDepartment of Computer Science, NCSU\nbDepartment of Computer Science, NCSU\nAbstract.\nIn human-in-the-loop reinforcement learning or environ-\nments where calculating a reward is expensive, the costly rewards can\nmake learning efficiency challenging to achieve. The cost of obtain-\ning feedback from humans or calculating expensive rewards means\nalgorithms receiving feedback at every step of long training sessions\nmay be infeasible, which may limit agents’ abilities to efficiently\nimprove performance. Our aim is to reduce the reliance of learning\nagents on humans or expensive rewards, improving the efficiency of\nlearning while maintaining the quality of the learned policy. We offer\na novel reinforcement learning algorithm that requests a reward only\nwhen its knowledge of the value of actions in an environment state\nis low. Our approach uses a reward function model as a proxy for\nhuman-delivered or expensive rewards when confidence is high, and\nasks for those explicit rewards only when there is low confidence in\nthe model’s predicted rewards and/or action selection. By reducing\ndependence on the expensive-to-obtain rewards, we are able to learn\nefficiently in settings where the logistics or expense of obtaining re-\nwards may otherwise prohibit it. In our experiments our approach\nobtains comparable performance to a baseline in terms of return and\nnumber of episodes required to learn, but achieves that performance\nwith as few as 20% of the rewards.\n1\nIntroduction\nHaving the ability to learn from sparse rewards is a crucial aspect\nof real-world applications of reinforcement learning (RL) since, as\nopposed to the proxy reward functions defined in computer simu-\nlations, the real world events do not always return immediate and\naccurate feedback signals to the learner. These sparse rewards present\nchallenges to the RL agents due to their irregular and imperfect distri-\nbution. Many sparse-reward RL algorithms introduce artificial rewards\nto densify the reward function in the hope of facilitating the learning\nprocess and overcoming the sporadic nature of the reward signal. We\ntake the opposite approach in this research. We assume the agent has\nthe ability to request reward signals at will from the environment.\nWe start with relatively dense reward signals at the beginning of the\ntraining and make the reward signals sparser in line with the improve-\nments in the agent’s learning model. In this way, we prevent the agent\nfrom receiving rewards in the parts of the state space where the agent\nalready has a good understanding of its policy and gather reward\nsignals only for the states that the agent has a low confidence in its\naction selection. The agent depends on the external feedback at the\n∗Corresponding Author. Email: msatici@ncsu.edu.\nearly stages of its learning but does not require frequent feedback\nsignals once it obtains a certain amount and variety of experiences.\nOur algorithm, at a high level, measures the learner’s confidence\nusing the output distribution(s) of the agent’s model(s). We assume\nthe agent has the ability to request feedback from the environment for\nthe given state and train a reward function model using the results of\nthose requests. The agent is trained using regular deep RL algorithms,\nbut for the states with no requested feedback value, the agent uses the\noutput of the reward function model as the feedback. In this way, the\nagent skips the feedback from the environment for the states it has\nhigh confidence.\nWe compare two different formulations for measuring confidence.\nThe first uses the entropy derived from the output probability dis-\ntribution of the actor model (or the Q-value distribution for DQN\narchitectures)—low entropy suggests high confidence in action selec-\ntion. The second uses entropy derived from the output distribution\nof the reward function model in addition to the entropy of the actor\nmodel. This combination of reward and action entropies measures\nthe inaccuracies in both the agent’s learning and learning the re-\nward function. We also compare two different regularization terms\nto lessen the reduction in environment rewards. We test all of our\napproaches in three domains: a discrete-space sparse-reward grid-\nworld, a continuous-space sparse-reward robotics environment, and a\ncontinuous-space dense-reward highway environment. We evaluate\nour approach against deep Q-network (DQN) [14], actor-critic net-\nwork (A2C) [15], and hindsight experience replay (HER) [1]. Results\nshow that our approach at worst matches the cumulative reward of the\nbaselines while greatly reducing the number of environment rewards.\n2\nRelated Work\nOne challenge in applying RL to real-world problems is the cost of\nsample collection. Sample efficient RL algorithms have been proposed\nin recent years to lower the sample complexity of deep RL algorithms\nin real world scenarios [3, 13, 24]. These approaches aim to mitigate\nthe challenges posed by the high costs associated with collecting real-\nworld samples for applications of RL. In the case of our algorithm,\ninstead of focusing on the cost of interacting with the environment,\nwe investigate the problem of improving the feedback efficiency by\nmaking the training process as independent as possible from the\nrewards returned by the environment. The feedback efficiency differs\nfrom the sample efficiency in that feedback efficient methods afford\nrequesting and training on many transitions from the environment\nas opposed to sample efficient algorithms but they cannot afford\narXiv:2502.21181v1  [cs.LG]  28 Feb 2025\n\n\nasking guidance from a human expert due to the high cost of feedback\nretrieval associated with it. There have been few deep-RL algorithms\naiming to learn with limited feedback derived from human preferences\n[7, 9, 17] but none of those approaches offer a framework that does\nnot depend on human feedback elicitation. We formulate the feedback\nretrieval problem as an implicit curriculum and try to offer a general\nframework that works on both human-designed and artificial feedback.\nAlgorithms that deal with sparse rewards focus on encouraging\nexploration to increase the agent’s chance of attaining useful reward\nsignals. One common approach defines a curiosity function to measure\nthe novelty of the visited states and provides the agent additional\nreward signals if it visits a novel state of the environment [18]. Another\napproach uses the error of the neural network as an exploration bonus\nadded to the reward function to encourage the agent to visit the areas\nof the state space with high error in the prediction of the observation\nfeatures [4]. A different approach employs a HER buffer to sample\nadditional transitions containing the future state of the agent as the\ngoal for the transition at hand [1]. These methods try to address the\nsparsity of rewards as an issue of limited exploration and aims to\nimprove the agent’s performance by bringing it closer to unknown\nregions of state space. They do not change the number of rewards,\nbut try to put the agent in a position where it could gather the most\nnovel feedback. Our algorithm does not encourage exploration; rather,\nit prevents the agent from receiving unnecessary rewards.\nThere are also human-guided RL algorithms that attempt to model\nthe human feedback using a reward predictor and use this reward\nmodel to train the agent without having the need to design a proxy\nreward function. These algorithms are typically used for complex\nreal-world environments where it would be difficult to craft a re-\nward function that encapsulates all aspects of the RL objective. Deep\nTAMER learns a reward function from human-guided feedback and\nuses this reward function in training the behavioral policy of the\nagent [23]. Similarly, Christiano et al. trains a reward function pre-\ndictor using human-guided and synthetic feedback [6]. Liang et al.\nuses disagreement between reward functions to increase exploration\nin human preference-based rl algorithms [11] and Liu et al. optimizes\nthe reward model, and the agent in parallel in a bi-objective optimiza-\ntion process rather than learning a reward model before the agent\ntraining, which is the case for the majority of the human-guided RL\nalgorithms [12]. These approaches show that it is possible to derive\na useful reward function model through human feedback elicitation\nbut they make no attempt at reducing the agent’s dependency on the\nfeedback coming from the environment. They also only depend on\nthe feedback from the reward predictor in training their agent. On\nthe other hand, we use both the reward from the environment and the\nfeedback from the reward function model in training and show it is\npossible to dramatically reduce the environment rewards used during\ntraining.\nFinally, there are inverse RL (IRL) algorithms that build a model\nof the reward function from demonstrations. These algorithms share\ncommon elements with the sparse-reward RL in that they both aim\nto obtain a reward predictor. However, IRL algorithms address the\nproblem of inferring a good reward function from demonstrations\nwhereas sparse-reward RL uses a reward model for making the reward\nfunction less sparse or more efficient. Although IRL algorithms could\nbe used to deal with sparse-reward learning, they cannot attain that\ngoal without re-purposing their inferred reward model. See Arora et\nal. [2] for a detailed analysis of IRL.\n3\nProblem Formulation\nHere we provide background on the RL method and formally define\nthe feedback diminution problem.\n3.1\nReinforcement Learning\nWe model learning as a MDP M = < S, A, T, R, si, Sg >, a tuple\nconsisting of a set of states S, a set of actions A, a transition function\nT, a reward function R, an initial state si, and a set of terminal states\nSg. The transition function T : S×A×S →[0, 1] corresponds to the\nprobability of transitioning from a state s ∈S to another state s′ ∈S\nusing a valid action a ∈A at state s. All of the environments we use\nare deterministic, so taking action a in state s always transitions into\nthe same resulting state s′—although that is not a requirement for our\nalgorithm. The reward function R : S × A × S →F maps a state,\naction, state tuple to the real-valued reward the learner receives. The\npolicy π : S →A maps states to actions. The cumulative reward G\nat time t is the discounted sum of all rewards the agent receives from\nt until it reaches a terminal state, Gt = Pτ−t\nk=0 δk ∗Rt+k, where δ is\nthe discount factor, τ is the time to reach a terminal state, and Rt+k\nis the feedback received in state st+k [20]. The agent’s objective is to\nlearn the optimal policy π∗\nM that maximizes G.\nWe train using deep Q-networks (DQN) [14] for discrete space\nenvironments. We use two four-layered, fully-connected, feed-forward\nDQNs. The networks take the states as input and output Q-value\nestimates for each available action. One neural network serves as the\nlearned model, and the other provides target Q-value estimates. The\nloss function is\nL(θ) = Es,a,r,trm,s′∼RB[(r + δ∗maxa′Q(s′, a′; θ−)\n−Q(s, a; θ))2],\n(1)\nwhere θ is the learned model weights, θ−is the target model weights,\nand δ is the discount factor [14].\nFor continuous action spaces, we employ an actor-critic architecture\nsimilar to [15]. We use a four-layered, fully-connected, feed-forward\nnetwork for the actor and critic. The critic receives the state as its input\nand outputs the value function estimate. The actor network outputs\ntwo real vectors which we treat as the mean and standard deviation of\nthe multi-dimensional normal distribution that we sample the actions\nfrom. We use the advantage loss defined in [15] to train the actor,\nand we train the critic network using the mean square error given in\nEq. 1. In the case of the robotics environment that we discuss in the\nresults section, in addition to the given actor-critic architecture, we\nalso employ a hindsight experience replay buffer (HER) to up-sample\nthe positive reinforcement transitions. We use the future strategy with\nk = 4 for the HER algorithm since that strategy is shown to produce\nthe best results in [1]. We also use the same loss function as [1] in\nthe robotics environment while retaining the aforementioned actor\nnetwork for the sampling of the action values.\nAt each training step, the agent takes a single action in the envi-\nronment, records the tuple (s, a, r, trm, s′) into a replay buffer RB\n(where trm indicates whether s is a terminal state), randomly samples\ntuples from the RB, and performs a single batch update on the learned\nmodel. The target model weights are updated using the weights of the\nlearned model at the end of each episode.\n3.2\nFeedback Diminution Problem\nIn environments where it is difficult to retrieve rewards but relatively\neasy to take actions and observe new states, there is a need for using\n\n\nfeedback efficient RL algorithms. Real world applications of reinforce-\nment learning such as disease outbreak simulations require extensive\ncomputations to assess the quality of an action being taken where\nallocation of resources to certain locations to mitigate the spread of\nthe disease is necessary [5, 21]. In these scenarios, it becomes trivial\nto take an action in the environment and observe the change in the\nenvironment state for a single timestep but assessing the outcome\nof the action taken involving interactions between many locations\npossibly over the span of a long period of time proves time consum-\ning. Feedback efficient RL agents show potential in addressing the\nshortcomings of designing good reward functions for these complex\nreal world environments by calculating the reward only for certain\nstates of the environment where the agent itself determines what states\nwould require an associated reward value.\nThe feedback diminution problem asks how an agent could learn\nthe optimal policy for a given task while requesting as little feedback\nas possible from the environment. Traditional RL agents receive a\nsingle feedback signal for every environment interaction. In the case\nof feedback diminution, the agent develops a model of the feedback\nfunction and uses this model to sparsify the feedback signals. The\nsparsification of the feedback is accomplished by allowing the agent\nto request a reinforcement only when it deems necessary. By solely\nlooking at its own understanding of the target task, whether that\nknowledge comes from the Q-values or the reward function model,\nthe agent solves a bi-objective optimization problem that reduces\nthe number of reward signals coming from the environment while\nmaximizing the cumulative reward objective as is the case with the\nregular RL agent.\nWe evaluate the performance of the RL agent based on the total\nreward it achieves with and without feedback diminution at a given\ntime in the training (asymptotic performance). We also measure the\nperformance based on the number of feedback signals the agent needs\nto converge to the maximum cumulative reward Gmax (time to con-\nvergence). The second measure does not look at how much training\nthe agent performs to converge since all of our algorithms perform\nthe same amount of training; rather, it attempts to measure the cost\nof receiving rewards during the training process. Our experiments\ncompare feedback diminution algorithms based on these two metrics.\n4\nEntropy Approaches for Feedback Diminution\nWe present confidence measures, pseudo-code for the high-level de-\nscription of our feedback diminution algorithm, and define the regu-\nlarization terms for the entropy calculation.\n4.1\nMeasuring Confidence\nWe construct our confidence criteria using the output distributions of\nthe actor and reward models. The probability distribution here could\nbe defined over a discrete or continuous variable depending on how\nthe neural networks treat output parameters. For the discrete case, we\ndefine the entropy as\nH(π(·|s)) = −\nX\na∈A\nP1(s, a) ∗lg(P1(s, a)),\n(2)\nwhere H(π(·|s)) is the entropy of the action policy π for the state s us-\ning the probabilities P1. We calculate P1 as P1(s, a) =\neQ(s,a)\nP\na∈A eQ(s,a) ,\nfor the DQN architecture where Q(s, a) is the Q-value for the state-\naction pair (a, s), and P1(s, a) is the softmax of Q-values. For the\ncontinuous case, we define the differential entropy as\nH(R(s, a)) = −\nZ\nR(s,a)\nP2(R(s, a)) ∗lg(P2(R(s, a)) ∗dR, (3)\nwhere H(R(s, a)) is the entropy of a single state, action pair w.r.t the\nreward function R modeled by a four-layered fully-connected neural\nnetwork taking state and action as input and outputting the mean\nand standard deviation of the Gaussian distribution for probabilities\nP2. P2 differs from P1 in the sense that it does not use softmax\nto calculate probability values since the output of R is already a\nGaussian distribution that we sample the probability values for P2.\nWe use Eq.3 to calculate the reward entropy of the reward model R\nand use Gaussian negative log likelihood loss in reward model training.\nFor actor models that output continuous actions instead of Q-value\nestimates, we use the entropy calculation given in Eq. 3 for the action\nentropy where R(s, a) is replaced by the output of the action policy π,\nwhich is again a Gaussian distribution described in Section 3.1. Since\ndifferential entropy is not bound to a specific range as opposed to the\ndiscrete entropy, we clip the differential entropy values to [0,10] range\nand then normalize them to obtain a confidence measure that is at the\nsame scale as the discrete entropy, which is always between [0,1]. In\nvast majority of the states, the differential entropy lies in [0, 10] so the\nclipping doesn’t cause a significant change in the entropy calculation.\nWe define the confidence of the agent as Conf(s) = 1 −H(R(s, a))\nfor the reward model and Conf(s) = 1 −H(π(·|s)) for the action\nmodel. We take the harmonic mean of the two confidence values to\nobtain the final confidence value for state s.\n4.2\nFeedback Diminution Algorithm\nOur algorithm calculates the entropy of the action and reward models\nfor the current state, tells the environment whether it wishes to skip\nthe reward based on the agent’s confidence level, and trains the agent\nusing the reward values sampled from the reward model if a transition\ndoes not have a feedback assigned to it. We use two reward models to\nimprove the stability of the feedback prediction. One model serves as\nthe learning model and the other the target. We use the target model\nto predict the missing reward values for the training of the agent. We\ntrain the learning model at every iteration and copy it to the target\nreward model after each episode. We keep two separate buffers: the\nreplay buffer contains all transitions for training the agent and the\nfeedback buffer stores only the transitions containing an associated\nreward value. Algorithm 1 is pseudo-code for the training process.\nAlgorithm 1 first initializes the agent model (Line 4) which is the\nneural network we use for the policy, including the target network.\nThen, Algorithm 1 initializes the reward models (Line 5) and begins\ntraining from an initial state of the environment (Line 7). It performs ϵ-\ngreedy action selection from the agent model (Line 11) and observes\nthe next state (Line 13). It calculates confidence using Eq. 2 & 3\n(Line 12) and if confidence is below the threshold (Line 14), it receives\na reward from the environment (Line 15). It stores (s, a, r, trm, s′)\ntuples in FB for the training of the reward model (Line 16) and in RB\nfor the training of the agent (Line 18). Then, it samples a minibatch of\ntransitions from FB (Line 19) and performs an optimization step on\nthe learning reward model (Line 20). It also samples a minibatch of\ntransitions from RB (Line 21) and replaces the missing reward values\nwith the predictions from the target reward model (Line 24). Finally,\nit performs an optimization step on the agent using the minibatch\nfrom RB with the mixture of actual and predicted rewards values\n(Line 27). It repeats the training process until the agent reaches the\nend of episode (Line 29 &Line 10). Then, it copies the learning reward\n\n\n1: INPUTS: environment: ENV ; convergence criteria: conv.\n2: CONSTANTS: set of terminal states: Sg; initial state: si;\nconfidence threshold: CTHRESH.\n3: VARS: replay buffer: RB; feedback buffer: FB; state: s;\nterminal condition: trm; reward: r.\n4: Initialize agent model: NNagent;\n5: Initialize reward model: NNlearn\nrew\nand NNtarget\nrew\n6: while conv is not satisfied do\n7:\ns ←si of ENV\n8:\ntrm ←s ∈Sg\n9:\nr ←null\n10:\nwhile trm is False do\n11:\nSelect a using ϵ-greedy policy on NNagent(s)\n12:\nCalculate confidence Conf on NNagent and NNtarget\nrew\n13:\nTake action a in ENV and observe s′\n14:\nif Conf ≤CTHRESH then\n15:\nobserve r from ENV\n16:\nStore (s, a, r, trm, s′) in FB\n17:\nend if\n18:\nStore (s, a, r, trm, s′) in RB\n19:\nSample a minibatch B from FB\n20:\nPerform a batch update on NNlearn\nrew\nusing B\n21:\nSample a minibatch B from RB\n22:\nfor (s, a, r, trm, s′) in B do\n23:\nif r == null then\n24:\nr ←NNtarget\nrew\n(s, a)\n25:\nend if\n26:\nend for\n27:\nPerform a batch update on NNagent using B\n28:\ns ←s′\n29:\ntrm ←s ∈Sg\n30:\nend while\n31:\nCopy NNlearn\nrew\nto the NNtarget\nrew\n32: end while\nAlgorithm 1: High-level Algorithm\nmodel to the target reward model (Line 31) and continues until the\nconvergence criteria is met (Line 6).\n1: INPUTS: learning model: NNagent; reward model: NNtarget\nrew\n;\nstate: s; action: a.\n2: VARS: Q-values for state s: Qs; probabilities for agent and\nreward models: P agent & P reward; entropies for agent and\nreward: H(A) & H(R).\n3: OUTPUTS: agent’s confidence for state s: Conf.\n4: Qs\nagent ←get Q-values from NNagent(s)\n5: Pagent ←softmax(Qs\nagent)\n6: Preward ←get µ, σ from NNtarget\nrew\n(s, a)\n7: H(A) ←entropy(P s\nagent)\n8: H(R) ←entropy(P s\nreward)\n9: Conf ←HarmonicMean(1 −H(A), 1 −H(R))\n10: return Conf\nAlgorithm 2: Confidence Calculation with Discrete Action and\nContinuous Reward Entropies\nAlgorithm 2 provides the details for the entropy calculation given\nin Algorithm 1 at Line 12. It takes the agent and reward models, and\nthe current state-action pair as inputs. It takes the Q-values for the\nagent model (Line 4), applies softmax to obtain probability values\n(Line 5), and uses the probability estimates from the reward model\n(Line 6). It then calculates the action entropy (Line 7; H(A)), and the\nreward entropy (Line 8; H(R)), and converts them to confidence values\n(Line 9). It combines these confidence values using the harmonic\nmean (Line 9) since the harmonic mean tends to be closer to the\nlower of the two confidence values making the algorithm less greedy\nin skipping rewards. In our experiments, we refer to this entropy\ncalculation as action entropy (AE) + reward entropy (RE) since it\nattempts to capture the confidence of the agent in both its ability to\nlearn the reward function and its accuracy in the action policy. We\nalso use a simplified version of this algorithm where we only consider\nthe entropy of the agent without any guidance from the reward model.\nWe refer to the second version of the entropy calculation as action\nentropy (AE) since it does not use the harmonic mean to combine two\nentropy values.\n4.3\nRegularization of Confidence\nThe feedback diminution algorithm is a greedy process that skips the\nreward for every state where the agent’s confidence is predicted to be\nhigh based on entropy calculations. During training, the agent does\nnot always produce reliable estimates for its confidence since it has no\naccess to the optimal policy and the entropy only serves as a heuristic\nassessment of confidence. Skipping the reward in states where the\nagent’s confidence is misplaced may lead to suboptimal performance.\nTo prevent this type of phenomena, we regularize the confidence of\nthe agent.\nWe offer two regularization terms, namely, exponential and hyper-\nbolic regularization. We define exponential regularization as e−ν∗n,\nand the hyperbolic regularization as\n1\n1+ν∗n, where ν is the tempera-\nture parameter and n is the number of steps taken without any reward\nfrom the environment (i.e., we reset n to zero when the agent receives\na reward from the environment). We multiply the confidence term\nwith the regularization term at every step to reduce the likelihood\nof the agent not receiving any reward for long periods of training.\nThe exponential regularization performs a steeper reduction in the\nconfidence values. Hyperbolic regularization provides a softer decay\nthan exponential and resembles the human psychological decay for\ndelayed rewards [22]. We use a constant temperature parameter ν\nwhich is 0.5 for exponential and 1 for hyperbolic regularization. We\ndid manual hyperparameter tuning in the range of [0.5, 0.75, 1] and\npicked the best performing value for the temperature parameter.\n5\nEvaluation Methods\nHere we describe the test environments, experimental setup, and state\nrepresentation for each domain.\n5.1\nEnvironments\nWe use a 20 × 20 2D grid and two continuous state-action space\ndomains to test the algorithms.\n5.1.1\nKey-Lock\nThe Key-Lock domain contains keys, locks, pits, and obstacles similar\nto [8] and [16]. The agent’s task is to pick up the key and unlock\nthe lock while avoiding the pits and obstacles. Each key picked up\ngives a reward of 500 and each lock unlocked gives a reward of 1,000.\nFalling into a pit receives -400. All other actions including moving\ninto an obstacle receive -10. Moving into an obstacle results in no\nstate transition. The learner can only move in cardinal directions and\nis assumed to have obtained the key or unlocked the lock if its location\nmatches the location of the key or it has obtained the key and matches\nthe location of the lock. An episode terminates when the agent obtains\nall the keys and unlocks all the locks, falls into a pit, or reaches 100\ntime steps. Since the agent only receives rewards when it is exactly at\nthe same position as the key or the lock, this is a sparse-reward task. A\nstate in the key-lock environment is represented as a vector, including\nthe Euclidean distance from the learner in all cardinal directions to\nthe nearest key and lock, four binary parameters indicating if there is\n\n\nan obstacle in the neighboring cells, four binary parameters indicating\nif there is a key or lock in the neighboring cells, and eight binary\nparameters indicating if there is a pit in the two adjacent cells in all\nfour directions. Lastly, two integers indicate the number of keys and\nlocks captured so far.\n5.1.2\nFetch-Push\nThe Fetch-Push environment from the gymnasium robotics li-\nbrary [19] is a robotics control domain where a mobile manipulator\nrobot must move a block to a target position using its gripper. The\nposition of the robotic gripper and the block are randomly determined\nat the beginning of each episode. The agent takes three continuous\nactions that change the displacement of the gripper in 3D space. All\nof the actions are defined in the range of [-1, 1]. The reward the agent\nreceives is 0 for having the block within the target position and -1\notherwise. Episodes don’t terminate, instead they are truncated after\n50 steps. A state is a vector, including the position, velocity and dis-\nplacement of the gripper and the position, velocity and rotation of the\nblock. The input to the neural network is the concatenation of the state\nand the goal information since the environment uses a goal-aware\nobservation space.\n5.1.3\nParking\nThe parking environment [10] consists of 30 parking spots, an agent\nand a goal item randomly positioned in one of the spots. The agent\nalways starts in the same position but its initial orientation changes\nrandomly and is tasked to reach the goal and orient itself in the right\ndirection. There are two continuous actions: velocity and angular\nvelocity both in [-1,1]. An episode terminates when the agent reaches\nthe goal and orients itself in the correct direction or when the episode\nlength reaches 100 time steps. The environment allows the agent to\nwander outside of the parking lot and does not provide any boundaries.\nThe agent receives a punishment proportional to its distance to the\ngoal w.r.t. position and orientation. A state is a vector, including the\nagent’s position, velocity, angular velocity, and the goal’s position and\norientation. The input to the neural network is the concatenation of the\nstate and goal information since the environment uses a goal-aware\nobservation space.\n5.2\nComparison Algorithm\nWe compare our algorithm to a DQN [14] in the key-lock domain, a\nvariant of advantage actor-critic network (A2C) [15] in the parking\nenvironment and the hindsight experience replay (HER) [1] algorithm\nin the robotics domain. Our approach to reducing environment rewards\nmakes the task harder for the agent to learn as time passes, which in\nreturn creates a soft curriculum without providing a clear sequence of\ncurriculum tasks. HER also serves as an implicit curriculum learning\nalgorithm as it makes reaching the goal simpler for the agent. However,\nour algorithm deals with reducing the feedback input to the agent\nwhereas HER modifies the already existing transitions in the replay\nbuffer. For this reason, we ran our approach on top of HER to get the\nbenefit of both as they do not hinder or interfere with one another.\nFor additional comparison, we also use a randomly generated entropy\nvalue between 0 and 1 as a random feedback diminution method\nand we use the regularization term without any entropy as a constant\nfeedback diminution approach.\n5.3\nExperimental Setup and Hyperparameters\nWe compare performance based on the total reward obtained while\nlearning the task and the convergence times, and include 95% confi-\ndence intervals. All algorithms share the same hyperparameters for\nthe neural network and train using the same environment-specific\nparameters.\nTable 1.\nHyperparameters for the Feedback Diminution Algorithm.\nParam\nValue\nParam\nValue\nϵ\n1\nOptimizer\nAdamax\nEps. Decay\n0.995\nLoss\nMean Squared\nMin. ϵ\n0.01\nBuffer Size\n40,000\nα\n0.005\nBatch Size\n16\nδ\n0.99\nτ\n0.99\nConf. Thresh.\n0.25\nAveraging\nHarmonic Mean\nThere are two types of hyperparameters: 1) neural network (in-\ncluding the convergence criteria) and 2) entropy calculation. Table 1\ncontains the parameters used in our experiments. These parameters\nare not tuned for any specific algorithm and all algorithms we use\nshare these parameters for all of the experiments run in the same do-\nmain. The last row represents the parameters specific for the entropy\ncalculation. All other parameters are for the neural network. ϵ is the\nϵ-greediness of the algorithm, which decays during training. α and δ\nare the learning rate and discount factor respectively. τ is the rate of\nupdate for the target network. Confidence threshold is the value that\ndetermines if an environment reward is obtained. Harmonic Mean\nrefers to the averaging we perform for combining the action entropy\nand reward entropy.\n6\nResults and Analysis\nWe perform multiple runs of each algorithm and report the average.\nEach run uses a different seed for the random initialization of the\nnetwork weights and environment episodes. Using 24 CPU cores,\nexecution times ranged from 5 hours for key-lock to 10 hours for\nfetch-push to 15 hours for parking environment. All experiments\nperform the same number of batch updates for the same number of\nsteps taken.\n6.1\nResults for Key-Lock Domain\nFigure 1(a) contains the number of training steps across 25 runs\nfor our action entropy and reward entropy algorithms in relation to\nDQN and random feedback diminution baselines in the key-lock\ndomain. Constant reg. is the baseline reward diminution algorithm\napplying exponential regularization to a constant entropy value of 1.\nThe random entropy uniformly samples an entropy value between 0\nand 1. Figure 1(b) is a box plot representing the number of rewards\nthe agent required to converge to the highest score. The highest score\nin this case is calculated as the average score of the first five episodes\nthat reach a performance within 5% of the highest possible score in\nthe environment. The vertical box edges represent the 25th and 75th\nquantiles for the highest score while the horizontal box edges denote\nthe 25th and 75th quantiles for the number of rewards required to\nconverge. Similarly, the whiskers display the minimum and maximum\nvalues excluding outliers. The horizontal and vertical lines within\nthe box represent the median highest score and the median number\nof rewards required to converge, respectively. The farther the box\nis to the upper left corner of the plot the better the performance.\nFigure 1(c) compares the performance of AE + RE with Hyperbolic\nRegularization on different sizes of neural networks to get an idea as\n\n\n(a) Asymptotic Performance\n(b) Rewards Required to Learn\n(c) Model Size vs. Performance\nFigure 1.\nPerformance of Our Feedback Diminution Algorithms in Key-Lock Domain.\nto how the performance of the reward model changes depending on\nthe neural network architecture. Finally, Table 2 shows the summary\nstatistics for all algorithms. The median score and the rewards required\nto converge refer to the highest score of the median run and the number\nof rewards needed to learn the task by the median run respectively,\nwhich is equivalent to the vertical median lines of the box plot.\nAE + RE reaches the highest score during training while requiring\nslightly more than 100,000 rewards to converge (Figures 1(a) & 1(b)),\nmaking it the best performing algorithm in this domain. AE + RE with\nhyperbolic regularization reaches a similar highest score as AE + RE\nwhile requiring around 123,000 rewards to converge, making it the sec-\nond best performing method for this domain (Figure 1(b) & Table 2).\nHowever, the asymptotic performance of AE + RE with hyperbolic\nregularization falls slightly behind AE + RE due to one outlier run\ndragging the average down for AE + RE with hyperbolic regulariza-\ntion (Figure 1(a)). The DQN baseline requires the highest number of\nrewards to converge while getting relatively similar performance to\nAE + RE with exponential regularization (Figure 1(b)).\nTable 2.\nSummary Results for Key-Lock Environment.\nAlgorithm\nMedian Score\n# of Rewards\nDQN\n996\n226,000\nRandom Entropy\n847\n280,000\nConstant Reg.\n900\n180,000\nAction Entropy\n659\n233,000\nAE with Exp. Reg.\n961\n197,000\nAE with Hyper. Reg.\n888\n191,000\nAE + RE\n997\n116,000\nAE + RE with Exp. Reg.\n977\n165,000\nAE + RE with Hyper. Reg\n967\n123,000\nThe constant reg. baseline, despite requiring fewer rewards to learn\ncompared to DQN, does not manage to reduce the number of rewards\nas much as AE + RE with hyperbolic regularization, showing that\nthe entropy selection method is necessary to provide better reduction\nin external rewards (Figures 1(a)). AE performs poorly due to not\nhaving access to the confidence of the reward model or any type of\nregularization and gets stuck at a local optima (Figures 1(a)). Using a\nlarge DQN with a small reward model results in the best performance\nwhile the large reward models fail to learn the task (Figures 1(c)).\nThe large reward models seem to be overfitting to the earlier rewards,\nmaking the model overly confident of its policy and not asking for\nenough environment rewards. Our tuning on the model size indicates\nit is necessary to have a reward model that is smaller in size to the\naction model to obtain feedback diminution benefits.\n6.2\nResults for Fetch-Push Domain\nWe perform 5 runs of each algorithm in the fetch-push domain and\nreport the average. We use the best performing neural network archi-\ntecture from (Figures 1(c)). The confidence intervals appear larger\n(a) Success Rate\n(b) Rewards Required to Learn\nFigure 2.\nPerformance in the Fetch-Push Domain.\nin this domain mainly because of the reduction in the number of\nruns we execute. Figure 2(a) shows the asymptotic performance of\nour algorithms excluding AE and AE + RE. Similar to the key-lock\ndomain, AE performs poorly and fails to reach a high success rate\neven after 1 million training steps. To make the graphs more readable,\nwe do not show AE or AE + RE as they remain below 0.2 success rate\nthroughout the training. Figure 2(b) is the box plot for the number of\nrewards required to convergence.\nAE + RE with hyperbolic regularization shows the best asymptotic\nperformance, obtains the highest score, and requires the fewest exter-\nnal rewards, making it the best performing algorithm in this domain\n(Figures 2(a) & 2(b)). It also shows the results we obtain in two dif-\nferent environments are consistent with one another since the same\nalgorithm results in near best performance in both domains. All the\nother algorithms have similar performance to the HER baseline in\nterms of return but they still reduce the number of external rewards\nby at least 50% (Figure 2(b)). AE with exponential regularization\nobtains the highest median score although it is only slightly better\nthan the variants of AE + RE and the difference could be attributed to\n\n\nTable 3.\nSummary Results for Fetch-Push Environment.\nAlgorithm\nMedian Score\n# of Rewards\nHER\n-9.7\n961,000\nAE Exp. Reg.\n-8.85\n450,000\nAE Hyper. Reg.\n-9.5\n306,000\nAE + RE Exp. Reg.\n-9.15\n411,000\nAE + RE Hyper. Reg.\n-9.0\n216,000\nthe variance in the execution (Table 3).\n6.3\nResults for Parking Domain\nWe report the average of 25 runs in the parking domain, using the\nbest performing neural network architecture from (Figure 1(c)). Fig-\nure 3(a) shows the asymptotic performance against advantage actor-\ncritic (A2C). Figure 3(b) is the box plots for the number of rewards\nrequired to converge.\n(a) Asymptotic Performance\n(b) Rewards Required to Learn\nFigure 3.\nPerformance in the Parking Domain.\nAE with exponential reg. gives the best average score (although\nit is only slightly better than the A2C baseline) (Figure 3(a)), but\nrequires more environment rewards to learn the task compared to\nthe variants of AE + RE (Figure 3(b)). AE + RE with hyperbolic\nregularization produces the highest score while needing a very low\nnumber of environment rewards to converge (Figure 3(b)), arguably\nmaking it the best performing algorithm again. AE + RE and AE\nmanage to reduce the number of rewards needed to converge at the\nexpense of performance. They show poor asymptotic performance\nsimilar to prior domains (Figure 3(a)) and show a large variation in\nthe highest score they obtain (Figure 3(b)). AE only requires 24,000\nrewards to learn, providing close to 95% improvement and suggesting\nthat denser external rewards might be more prone to reward skipping\nby the agent (Table 4). The baseline algorithm obtains a similar highest\nscore as the regularized AE + RE but it requires more than four times\nthe number of environment rewards to converge (Figure 3(b)). The\nperformance of AE + RE with hyperbolic regularization and AE with\nexponential reg. remain consistent with the other domains despite the\nparking environment having a denser external reward function. This\nfurther supports reducing the need for environment rewards across\nmultiple network architectures over three domains.\nWe also see that some of the feedback diminution algorithms reach\nhigher scores than the baselines having access to true rewards. Only\nthe feedback diminution algorithms making use of hyperbolic or expo-\nnential regularization manage to outperform the baseline algorithms\nin some domains, which makes us reason that the effect of the regular-\nization allows the algorithm to avoid suboptimal policies by providing\ninterleaved feedback which results in the feedback diminution method\noutperforming the baselines.\nTable 4.\nSummary Results for Parking Environment.\nAlgorithm\nMedian Score\n# of Rewards\nA2C\n-5.53\n431,000\nAE\n-8.88\n24,000\nAE Exp. Reg.\n-5.55\n174,000\nAE Hyper. Reg.\n-7.50\n99,000\nAE + RE\n-9.15\n27,000\nAE + RE Exp. Reg.\n-6.13\n150,000\nAE + RE Hyper. Reg.\n-5.51\n101,000\n6.4\nChoosing the Best Performing Approach\nHaving a feedback diminution method that can provide consistent\nimprovements across multiple domains is crucial since it would be\nimpractical to try all regularization methods and entropy metrics\nevery time the algorithm is adapted to a new domain or framework.\nDue to the variations in performance of the feedback diminution\nalgorithms on different domains, there emerges the need for selecting\nthe proper approach that would be suitable for most domains and real-\nworld applications. If we exclude the non-regularized methods due to\ntheir poor scores in some environments, we see that AE + RE with\nhyperbolic regularization produces the best performance in the key-\nlock and robotics domains in terms of the rewards required to converge\nwhile being close to the best performing algorithm in the parking\ndomain. If the purpose is to reduce the feedback dependence while\nnot compromising the performance of the RL algorithm, AE + RE\nwith hyperbolic regularization presents the best option for applying\nfeedback diminution in RL frameworks.\n7\nConclusion\nWe presented a novel approach to reducing the need to sample environ-\nment rewards using entropy as a heuristic. We detailed two versions of\nthe approach, one using only the entropy of the action model and an-\nother that incorporates both action and reward entropies. We adapted\nvariants of DQN, A2C, and HER algorithms to compare against our\nalgorithm and evaluated performance on the key-lock, robotics, and\nparking domains using discrete and continuous action spaces. Our\nresults show that our approach was able to match the performance\nof a randomly generated entropy baseline, a constant regularization\nbaseline, and the comparison criteria while requiring fewer environ-\nmental rewards to learn in all cases. In the future, we intend to analyze\nin what parts of the state space the agent is more likely to request\nrewards to obtain a better understanding of how the characteristics of\nthe environment affect the feedback diminution process.\n\n\nReferences\n[1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience\nreplay, 2018.\n[2] S. Arora and P. Doshi. A survey of inverse reinforcement learning:\nChallenges, methods and progress, 2018. URL https://arxiv.org/abs/\n1806.06877.\n[3] J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee. Sample-\nefficient reinforcement learning with stochastic ensemble value ex-\npansion.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems, volume 31. Curran Associates, Inc.,\n2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/\nf02208a057804ee16ac72ff4d3cec53b-Paper.pdf.\n[4] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by ran-\ndom network distillation, 2018. URL https://arxiv.org/abs/1810.12894.\n[5] S. Bushaj, X. Yin, A. Beqiri, D. Andrews, and E. Buyuktahtakin. A\nsimulation-deep reinforcement learning (sirl) approach for epidemic\ncontrol optimization. Annals of Operations Research, 328:1–33, 09\n2022. doi: 10.1007/s10479-022-04926-7.\n[6] P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei.\nDeep reinforcement learning from human preferences, 2023.\n[7] D. Kong and L. F. Yang. Provably feedback-efficient reinforcement\nlearning via active reward learning, 2023.\n[8] G. Konidaris and A. Barto. Building portable options: Skill transfer in\nreinforcement learning. In Proceedings of the 20th International Joint\nConference on Artificial Intelligence, pages 895–900, 2007.\n[9] K. Lee, L. Smith, and P. Abbeel. Pebble: Feedback-efficient interactive\nreinforcement learning via relabeling experience and unsupervised pre-\ntraining, 2021.\n[10] E. Leurent. An environment for autonomous driving decision-making.\nhttps://github.com/eleurent/highway-env, 2018.\n[11] X. Liang, K. Shu, K. Lee, and P. Abbeel. Reward uncertainty for explo-\nration in preference-based reinforcement learning, 2022.\n[12] R. Liu, F. Bai, Y. Du, and Y. Yang. Meta-reward-net: Implicitly dif-\nferentiable reward learning for preference-based reinforcement learn-\ning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\nand A. Oh, editors, Advances in Neural Information Processing\nSystems, volume 35, pages 22270–22284. Curran Associates, Inc.,\n2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\n8be9c134bb193d8bd3827d4df8488228-Paper-Conference.pdf.\n[13] V. Mai, K. Mani, and L. Paull. Sample efficient deep reinforcement\nlearning via uncertainty estimation, 2022.\n[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis. Human-level control through\ndeep reinforcement learning. Nature, 518(7540):529–533, Feb. 2015.\nISSN 00280836. URL http://dx.doi.org/10.1038/nature14236.\n[15] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu. Asynchronous methods for deep rein-\nforcement learning, 2016.\n[16] S. Narvekar, J. Sinapov, and P. Stone. Autonomous task sequencing for\ncustomized curriculum design in reinforcement learning. In Proceedings\nof the 26th International Joint Conference on Artificial Intelligence,\nIJCAI’17, pages 2536–2542. AAAI Press, 2017. ISBN 978-0-9992411-\n0-3. URL http://dl.acm.org/citation.cfm?id=3172077.3172241.\n[17] J. Park, Y. Seo, J. Shin, H. Lee, P. Abbeel, and K. Lee. Surf: Semi-\nsupervised reward learning with data augmentation for feedback-efficient\npreference-based reinforcement learning, 2022.\n[18] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven\nexploration by self-supervised prediction, 2017. URL https://arxiv.org/\nabs/1705.05363.\n[19] M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Pow-\nell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, V. Kumar, and\nW. Zaremba. Multi-goal reinforcement learning: Challenging robotics\nenvironments and request for research, 2018.\n[20] R. S. Sutton and A. G. Barto. Reinforcement learning - an introduction.\nAdaptive computation and machine learning. MIT Press, 1998. ISBN\n0262193981. URL http://www.worldcat.org/oclc/37293240.\n[21] A. L. Sykes, J. A. Galvis, K. C. O’Hara, C. Corzo, and G. Machado.\nEstimating the effectiveness of control actions on African swine fever\ntransmission in commercial swine populations in the United States. Pre-\nventive Veterinary Medicine, 217:105962, Aug. 2023. ISSN 01675877.\ndoi: 10.1016/j.prevetmed.2023.105962. URL https://linkinghub.elsevier.\ncom/retrieve/pii/S0167587723001265.\n[22] T. Takahashi, S. Tokuda, M. Nishimura, and R. Kimura.\nThe q-\nexponential decay of subjective probability for future reward: A psy-\nchophysical time approach. Entropy, 16(10):5537–5545, 2014. ISSN\n1099-4300. doi: 10.3390/e16105537. URL https://www.mdpi.com/\n1099-4300/16/10/5537.\n[23] G. Warnell, N. Waytowich, V. Lawhern, and P. Stone. Deep tamer:\nInteractive agent shaping in high-dimensional state spaces, 2018.\n[24] J. Zhang, J. Kim, B. O’Donoghue, and S. Boyd. Sample efficient rein-\nforcement learning with reinforce, 2020.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21181v1.pdf",
    "total_pages": 8,
    "title": "Reducing Reward Dependence in RL Through Adaptive Confidence Discounting",
    "authors": [
      "Muhammed Yusuf Satici",
      "David L. Roberts"
    ],
    "abstract": "In human-in-the-loop reinforcement learning or environments where calculating\na reward is expensive, the costly rewards can make learning efficiency\nchallenging to achieve. The cost of obtaining feedback from humans or\ncalculating expensive rewards means algorithms receiving feedback at every step\nof long training sessions may be infeasible, which may limit agents' abilities\nto efficiently improve performance. Our aim is to reduce the reliance of\nlearning agents on humans or expensive rewards, improving the efficiency of\nlearning while maintaining the quality of the learned policy. We offer a novel\nreinforcement learning algorithm that requests a reward only when its knowledge\nof the value of actions in an environment state is low. Our approach uses a\nreward function model as a proxy for human-delivered or expensive rewards when\nconfidence is high, and asks for those explicit rewards only when there is low\nconfidence in the model's predicted rewards and/or action selection. By\nreducing dependence on the expensive-to-obtain rewards, we are able to learn\nefficiently in settings where the logistics or expense of obtaining rewards may\notherwise prohibit it. In our experiments our approach obtains comparable\nperformance to a baseline in terms of return and number of episodes required to\nlearn, but achieves that performance with as few as 20% of the rewards.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}