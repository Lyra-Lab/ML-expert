{
  "id": "arxiv_2502.21236v1",
  "text": "Transforming Tuberculosis Care: Optimizing Large Language Models for\nEnhanced Clinician-Patient Communication\nDaniil Filienko1, Mahek Nizar1, Javier Roberti2,3, Denise Galdamez4, Haroon Jakher4,\nSarah Iribarren4, Weichao Yuwen5, Martine De Cock1\n1School of Engineering and Technology, University of Washington Tacoma\nTacoma, WA, USA\n2Qualitative Research in Health, Institute for Clinical Effectiveness and Health Policy\n3Centre for Research on Epidemiology and Public Health (CIESP), CONICET\nBuenos Aires, Argentina\n4School of Nursing, University of Washington\nSeattle, WA, USA\n5School of Nursing and Healthcare Leadership, University of Washington Tacoma\nTacoma, WA, USA\nAbstract\nTuberculosis (TB) is the leading cause of death from an in-\nfectious disease globally, with the highest burden in low- and\nmiddle-income countries. In these regions, limited healthcare\naccess and high patient-to-provider ratios impede effective\npatient support, communication, and treatment completion.\nTo bridge this gap, we propose integrating a specialized Large\nLanguage Model into an efficacious digital adherence tech-\nnology to augment interactive communication with treatment\nsupporters. This AI-powered approach, operating within a\nhuman-in-the-loop framework, aims to enhance patient en-\ngagement and improve TB treatment outcomes.\nIntroduction\nTuberculosis (TB) remains the world’s deadliest infec-\ntious disease, despite being preventable and curable (World\nHealth Organization (WHO) 2023). Efforts to meet the\nWHO’s 2030 targets for TB diagnosis and treatment have\nfallen short (Fukunaga et al. 2021), resulting in contin-\nued transmission and loss of life. The burden is dispropor-\ntionately high in low- and middle-income countries, where\nhealthcare systems face significant challenges.\nEffective patient-provider communication and support\nduring the demanding 6- to 9-month treatment period is\ncritical to improving outcomes but is often limited in these\nsettings, contributing to increased treatment non-adherence\n(Tola et al. 2015). Digital Adherence Technologies (DATs)\n- including feature phone-based and smartphone-based\ntechnologies, digital pillboxes, and ingestible sensors-have\nemerged as a promising solution (Subbaraman et al. 2018).\nHowever, DATs still require significant human involvement.\nLarge Language Models (LLMs) offer a promising ad-\nvancement, generating real-time, human-like responses to\nsupport overburdened healthcare workers. They can answer\nmedical questions, provide treatment guidance, and enhance\npatient engagement, potentially transforming TB care deliv-\nery (Moor et al. 2023; Nori et al. 2023; Tu et al. 2024). LLMs\ncan analyze diverse data sources–demographics, socioeco-\nnomic factors and behavior patterns–to create personalized\nCopyright © 2025, GenAI4Health Workshop @ Association for the\nAdvancement of Artificial Intelligence (www.aaai.org). All rights\nreserved.\ntreatment plans tailored to each patient. They can also offer\nmulti-channel communication that helps patients understand\ntheir condition, treatment options, and self-care instructions\nand adapt patient education material to appropriate reading\nlevels, ensuring health information is accessible, and em-\npowering patients to manage their care.\nWhen deployed in human-in-the-loop frameworks, LLMs\ncan suggest responses while maintaining provider oversight.\nThis ensures that healthcare professionals verify all critical\nissues while reducing the cognitive burden on overworked\nhealthcare workers. However, the effectiveness of LLMs as\ncomprehensive tools, combining culturally relevant empathy\nwith accurate and factual medical information, remains un-\nderexplored, particularly in multilingual healthcare settings.\nThis gap is especially relevant for TB treatment, as many\ncountries with the highest TB burden do not use English as\ntheir primary language (Huddart, MacLean, and Pai 2016).\nLLM development in healthcare settings must also ac-\ncount for patient privacy concerns. For TB, a disease bur-\ndened by stigma and discrimination, privacy challenges are\nparticularly acute. Recent studies have highlighted the risk\nof LLMs inadvertently disclosing excerpts of personal data,\nwhich could include medical information about patients\n(Huang et al. 2023; Wang et al. 2023; Zeng et al. 2024). Dif-\nferential Privacy (DP) has been proposed as a mechanism\nto mitigate such information leakage in LLMs (Xie et al.\n2024; Yue et al. 2021). However, its impact on the utility of\nLLMs in healthcare applications, especially in non-English\nlanguages, has yet to be comprehensively investigated. Our\nstudy has two primary objectives:\n1. Develop an LLM-powered TB treatment support tool\nbased on real-world data and patient needs using multiple\nin-context learning techniques.\n2. Evaluate the model based on linguistic appropriateness,\nempathy, medical accuracy, and privacy.\nRelated Work\nConversational AI in Healthcare. Conversational AI has\nbeen increasingly applied to real-time healthcare dialogue\ngeneration. Existing approaches typically fall into two cat-\negories: psychological care (Jo et al. 2023; Kang et al.\narXiv:2502.21236v1  [cs.AI]  28 Feb 2025\n\n\nFigure 1: The user’s query will pass to the LLM-based AI system for processing. The clinical treatment supporter will receive\ntop k suggested responses from the AI system and send the most fitting response to the patient.\n2024; Filienko et al. 2024) or clinical patient-centered care\n(Mukherjee et al. 2024; Tu et al. 2024). High performance is\nachieved by fine-tuning models on curated datasets that re-\nflect desired behaviors (Kang et al. 2024; Tu et al. 2024) or\nby utilizing advanced prompt engineering techniques (Fil-\nienko et al. 2024; Nori et al. 2023). Research in psycho-\nlogical care ensures that conversational agents provide em-\npathetic and relevant responses, adhering to psychologi-\ncal therapy guidelines. In comparison, studies in clinical\npatient-centered care prioritize accurate symptom analysis,\ndiagnosis, and treatment recommendations (Tu et al. 2024;\nMukherjee et al. 2024). These approaches focus on factual-\nity of responses to ensure the delivery of trustworthy medical\ninformation. Our application spans both domains, integrat-\ning elements of psychological and clinical care. Importantly,\nour approach incorporates privacy-preserving mechanisms,\naddressing a critical gap in prior work.\nDigital Adherence Technologies for TB. DATs have\nshown effectiveness in improving TB treatment outcomes\n(Iribarren et al. 2022; Boutilier et al. 2022; Jerene et al.\n2023). These tools, such as mobile applications, support pa-\ntients by providing health education, treatment guidance,\nand emotional support. Building on the intervention TB-\nTreatment Support Tools designed by Iribarren et al. (2022),\nour approach delivers support from treatment supporters –\nsuch as nursesor social workers – via messaging enhanced\nby an LLM-powered conversational agent. This integration\naims to improve communication efficiency by generating\nsuggested responses, reducing the burden on care providers\nwhile maintaining personalized, high-quality support.\nMultilingual LLMs. Research on enhancing LLMs’ mul-\ntilingual capabilities has gained traction, focusing on evalu-\nating their understanding across languages (Zhao et al. 2024)\nand improving performance through innovative methods (Li\net al. 2024). Our work is among the few application-based\nworks that apply a multilingual LLM to a healthcare task.\nWhile there has been some progress in developing Spanish-\nlanguage healthcare tools, such as a suicide prevention chat-\nbot (Ram´ırez et al. 2024), our research is among the first to\nexplore conversational AI for chronic disease management\nin Spanish-speaking populations.\nPrivacy-Preserving In-Context Learning Methods.\nTwo primary paradigms exist for ensuring privacy in in-\ncontext learning: PATE-like (Papernot et al. 2017) privatized\nmodel ensembles and text sanitization methods based on Lo-\ncal Differential Privacy (LDP) (Duchi, Jordan, and Wain-\nwright 2013). The former utilizes an ensemble of privately\nand publicly trained models to generate high-quality, private\noutput. However, they are computationally expensive and\noften restricted to classification tasks, making them unsuit-\nable for the complex textual response generation required in\nhealthcare dialogues (Duan et al. 2023; Tang et al. 2024),\nor they impose a hard limit on the number of questions that\ncan be asked before the datastore is rendered to be unus-\nable because the “privacy budget” has been spent (Wu et al.\n2024). Methods of the LDP kind focus on performing text\nsanitization before model inference, ensuring that the data\nis privatized before being passed to the LLM. Algorithms\nlike UMLDP (Yue et al. 2021) exploit Differential Privacy\n(DP)’s post-processing property, allowing privatized text to\nbe used across multiple models without imposing restric-\ntions on the end task or requiring a privacy budget reset.\nOur study adopts the LDP approach with the UMLDP al-\ngorithm (Yue et al. 2021) for its flexibility and scalability in\ntext-based healthcare applications.\nMethods\nIn this mixed-methods study, we document the iterative de-\nsign process and preliminary evaluation of the models that\nwill power our TB DAT, as shown in Figure 1.\nModel development\nBuilding on prior work (Nori et al. 2023) that adapted\na general-purpose LLM to medical QnA, we developed\na series of GPT-based conversational models designed to\nbe deployed as human-supervised treatment supporters for\nSpanish-speaking individuals with TB. These models were\ndesigned using different prompt engineering techniques\nand Retrieval Augmented Generation (RAG). To enhance\ndomain-specific responses, we integrated publicly available\nTB guidelines and medication suggestions, previous TB trial\nmessages (Iribarren et al. 2022), and manually crafted dia-\nlogue samples mimicking real conversations to be used by\nthe model. To safeguard patient privacy, we applied differen-\ntially private text sanitization (Yue et al. 2021) to trial mes-\nsages used in few-shot prompts.\nLinguistic Performance. To support culturally and\nlinguistically appropriate responses, the models received\nfew-shot examples that reflect local dialect, including\nanonymized messages from a TB trial conducted in Ar-\ngentina (Iribarren et al. 2022) and verified for accuracy and\ndialect suitability by an Argentinian research team member.\n\n\nFigure 2: The system classifies a patient’s query as an “in-\nformational” or “emotional” request. Then, according to the\nclassification result, an LLM is set up with the correspond-\ning prompt and given access to external documents contain-\ning medical knowledge for informational questions.\nEmpathy. Few-shot examples served to model proper\nempathetic responses by simulating prior conversations be-\ntween patients and treatment supporters. These examples\nwere designed to help the model respond in a way that aligns\nwith the emotional and cultural context of the patients.\nMedical Accuracy. To support accurate and factual re-\nsponses to TB-related queries, a RAG pipeline was imple-\nmented (Lewis et al. 2020). The pipeline utilized Spanish-\nlanguage TB resources from reliable sources including\nCDC guidelines,1 Southeastern National Tuberculosis Cen-\nter medication guidelines,2 Mayo Clinic,3 and WHO rec-\nommended resources.4 This approach augmented the mod-\nels’ ability to retrieve and incorporate up-to-date domain-\nspecific information during conversations.\nMulti-Agent Sequence. We also developed a multi-agent\nclassification sequence (see Figure 2). The first LLM agent\nuses a classification prompt to identify whether a user query\nis empathy-seeking or information-heavy based on examples\ncurated by clinical experts. Queries classified as empathy-\nseeking are directed to an empathy-optimized agent, while\ninformation-heavy queries are routed to a fact-focused RAG\nagent. This modular setup enables the system to provide\ncontext-appropriate responses while leveraging the strengths\nof each specialized agent.\nPrompt Engineering\nBuilding on prior work (Nori et al.\n2023), our prompt engineering efforts focused on adapting\nthe LLM for the TB-specific context using a progression of\ntechniques, including zero-shot, few-shot, and RAG meth-\n1https://www.cdc.gov/tb/esp/\n2https://sntc.medicine.ufl.edu/files/products/druginfo/druginfobook.pdf\n3https://www.mayoclinic.org/diseases-\nconditions/tuberculosis/symptoms-causes/syc-20351250\n4https://iris.paho.org/handle/10665.2/55801,\nhttps://iris.paho.org/handle/10665.2/56667,\nhttps://iris.paho.org/handle/10665.2/55926,\nhttps://iris.paho.org/handle/10665.2/55926\nModel\nPrompt Structure\n0\nZero-Shot (English)\n1\nZero-Shot\n2\nFew-Shot\n3\nRAG\n4\nRAG + Few-Shot\n5\nRAG + Few-Shot + Two-Step Classification\nTable 1: Overview of in-context learning methods utilized\nfor each model. All prompts are listed in full in Appendix\nD. They are all in Spanish unless specified otherwise.\nods (see Table 1 for an overview). Full prompts are listed in\nAppendix D.\nZero-Shot. We started with a zero-shot prompt in both\nEnglish and Spanish, designed to elicit responses to TB-\nrelated queries without providing examples. This baseline\nserved as a foundation for more complex approaches.\nFew-Shot. For few-shot (FS) prompting (Brown et al.\n2020), we incorporated sample dialogues between patients\nand treatment supporters.\nRetrieval Augmented Generation. RAG (Lewis et al.\n2020) was implemented to enhance the model’s ability to\nanswer knowledge-intensive questions by integrating exter-\nnal TB-related content, such as symptoms, medications, and\nside effects.\nRetrieval-Augmented Generation + Few-Shot. The\nRAG+FS approach combined curated FS dialogue examples\nwith dynamically retrieved TB information.\nTwo-Step Pipeline for Classification. As depicted in\nFigure 2, we introduced a two-step pipeline to classify pa-\ntient questions and then adjust the response prompt.\nPrivacy and Data Security\nWhen using a third-party\nLLM, such as OpenAI’s GPT3.5 model, examples included\nin the prompt during few-shot learning are disclosed to the\nthird party. This may be problematic in a scenario like ours,\nwhere the examples consist of clinician-patient conversa-\ntions. To protect patient privacy, removing Personally Iden-\ntifiable Information (PII) from these examples is important\nbefore including them in the prompt. Confidential informa-\ntion could also be stolen via natural regurgitation of infor-\nmation by the LLM or by a malicious attacker who crafts a\nprompt to manipulate the LLM into disclosing such infor-\nmation (Zeng et al. 2024). In Appendix B, we document\na prompt-based attack that we implemented (Zeng et al.\n2024), through which an adversary could extract examples\nprovided in our few-shot prompt, showing that our model\ncan leak patient data. These threats can be mitigated if we\nprivatize the messages before passing them to the LLM.\nHere, we examined two approaches for message privati-\nzation. First, we requested medical experts to craft examples\nthat cover various kinds of questions recorded from the TB\npatients’ messages (Iribarren et al. 2022) and do not con-\ntain the real PII. The experts determined the most occurring\nstyles of questions. Second, we simulated a similar process\nthrough RAG (Lewis et al. 2020) with the patient messages\n– instead of TB treatment guideline documents as we de-\nscribed earlier – performing message retrieval. Here, RAG\ndetermines which text messages are most relevant by per-\nforming a semantic similarity search with the Faiss library\n(Douze et al. 2024) and cosine similarity metric. To prevent\nthe patients’ PII leak during RAG, we performed a text sani-\n\n\nCategory\nDescription\nEmpathy\nCategories:\n• The model expressed emotions, such as\nwarmth, compassion, and concern (or similar)\ntowards the patient\n• The model communicated an understanding\nof feelings and experiences inferred from the\npatient’s responses\n• The model explored feelings and experiences\nnot stated in the patient’s response\nRatings:\n0. No empathetic response\n1. Weak expression of empathy\n2. Strong expression of empathy\nMedical\n1. Incorrect Answer\nAccuracy\n2. Mostly Inaccurate Answer\n3. Partially Accurate Answer\n4. Mostly Accurate Answer\n5. Entirely Accurate Answer\nLinguistic\nAccuracy\n• Low: Apparent lack of understanding of\nSpanish language\n• Moderate: Uses neutral Spanish, lacks Ar-\ngentinian variety features\n• High: Model incorporates Argentinian Spa-\nnish features\nTable 2: Descriptions and categories for empathy, medical\naccuracy, and linguistic accuracy assessment.\ntization algorithm (Yue et al. 2021) with Differential Privacy\nguarantees (Dwork and Roth 2014) over full messages, re-\nplacing the English pre-trained BERT (Devlin et al. 2019)\nmodel with a Spanish pre-trained version of BERT, BETO\n(Wu and Dredze 2019). The privatization algorithm works\nby replacing the words in the text with related words ac-\ncording to the Euclidean distance in the embedding space.\nFor each input x, it uses the mechanism M(x) to produce a\nsanitized version y. The probability of selecting y depends\non its similarity to x according to a distance function. Closer\noutputs y are more likely to be chosen, while further ones\nare less likely, controlled by a scaling factor ϵ, a user’ cho-\nsen value. Lower ϵ leads to better privacy while decreasing\nthe quality of the responses.\nEvaluation of models\nThe models were evaluated across three categories: linguis-\ntic appropriateness, medical accuracy, and empathy. We de-\nployed the 6 primary models on a public-facing website and\nasked our evaluation team of 3 clinical experts, including\nan Argentinian resident, a licensed physician, and a nurse\ntrained in empathetic responses, to communicate with the\nmodels for 2 weeks. While being a short time, it was enough\nto collect preliminary results and imitate the setup in Fig-\nure 1, where our set of standardized questions were passed\nto the model as patients’ queries, and the answers given by\nthe model were passed to the evaluation team consisting of\ntreatment supporters, allowing them to evaluate the mod-\nels’ quality in realistic settings. We instructed them to ask\nthe models questions that they thought were the most ap-\npropriate to challenge the models (see Appendix E). At the\nend of 2 weeks, we asked the clinical experts to evaluate\nthe models in their relevant field of expertise and collected\ntheir feedback. We then performed the same procedure for\nour privacy-enhancing models, hosting them for a week. We\nconcluded with a qualitative analysis and summarized their\nfeedback on the areas where models seemed to improve af-\nter inclusion of more complex in-context learning methods\nand areas where they still displayed pitfalls. In the end, we\nasked them to verify the summaries.\nLinguistic Appropriateness. To assess each model’s\nability to respond effectively in Argentinian Spanish, we\nevaluated the communication style and word choices using\nexpert feedback from an Argentinean research team mem-\nber (see Appendix A for more details on its difference from\nother forms of Spanish). This evaluation ensured that the\nmodel’s language use was culturally and contextually appro-\npriate, prioritizing naturalness.\nEmpathy. Empathy, broadly defined as the ability to un-\nderstand, interpret, and respond to another person’s emo-\ntional experience (Nembhard et al. 2023; Sharma et al.\n2020), is essential for tools used in vulnerable, high-risk\npopulations such as TB patients. Our evaluation focused on\nmeasuring the model’s empathy across emotional and cog-\nnitive dimensions. Although there are established empathy\nevaluation algorithms (Sharma et al. 2020), they tend to per-\nform poorly when applied outside their original domain,\noften leading to low-quality ratings (Filienko et al. 2024).\nAs no empathy evaluation tools specific to the cultural and\nlinguistic contexts of Argentina are available, we opted for\nqualitative manual evaluations. Using categories and frame-\nworks from prior research (Sharma et al. 2020), bi-lingual\nresearch team members assessed the model’s empathetic re-\nsponses. The evaluation included questions with emotional\nexperience content in the input.\nMedical Accuracy. Ensuring medical accuracy is critical\nfor building trust in the tools among both patients and clin-\nicians. The factuality of each model’s responses was eval-\nuated by human assessments. Clinical experts reviewed the\nvalidity of responses generated for symptom-heavy queries.\nChallenges arose due to overlapping information in the RAG\ndatabase, where multiple relevant documents sometimes ex-\nisted for a single medical query. In such cases, a definitive\n‘gold standard’ response was not always apparent, further\nhighlighting the importance of human evaluation. The feed-\nback collected from these evaluations also informed iterative\nimprovements to the RAG database and the model’s ability\nto select and synthesize the most relevant information.\nPrivacy. We compared the utility of privatized user mes-\nsages processed using DP techniques from (Yue et al. 2021)\nwith manually curated messages when used for few-shot\nprompting. Privacy was quantified using epsilon (ϵ), a mea-\nsure of added DP noise, to ensure a balance between formal\nprivacy guarantees and model utility. The evaluation con-\nsidered the impact of privacy-preserving transformations on\nlinguistic performance, empathy, and medical accuracy. Ap-\npendix E contains examples of how the messages looked be-\nfore and after perturbation.\nResults\nTable 3 and Table 4 present the models’ linguistic accuracy,\nmedical factuality, and empathy assessment using the cate-\ngories outlined in Table 2.\n\n\nModel\nPrompt Structure\nEmpathy\nMedical Accuracy\nLinguistic Accuracy\nPronouns\n0\nZero-Shot (English)\n0.50, 0.00, 0.00\n3.4\nHigh\nvoseo\n1\nZero-Shot\n0.75, 0.00, 0.00\n3.6\nModerate\nusted\n2\nFew-Shot\n0.25, 0.50, 0.00\n4.4\nModerate\nusted\n3\nRAG\n1.25, 0.00, 0.00\n3.2\nVery Low\nt´u\n4\nRAG + Few-Shot\n0.50, 0.25, 0.00\n4.0\nModerate\nusted\n5\nRAG + Few-Shot + Classification\n0.50, 0.75, 0.00\n4.2\nModerate\nusted\nTable 3: Average scores of 6 primary models for empathy, medical accuracy, linguistic accuracy, and pronoun usage\nEmpathy\nThe models varied in generating empathetic responses\nacross empathy categories and ratings. Models 2 and 5 pro-\nduced empathetic responses in empathy categories one and\ntwo to all four questions (Empathy questions from Appendix\nC). However, Model 5’s responses were rated slightly higher\nin both categories – placing Model 5 as a top performer over-\nall, together with Model 3 which demonstrated strong per-\nformance in category one with empathetic responses for 3\nout of 4 questions, but underperformed in category two.\nRemaining Pitfalls. Misclassification of emotional mes-\nsages: The models misinterpreted some messages as emo-\ntional and provided generic reassurance instead of address-\ning specific concerns. For example, when asked for a time-\nline for when nausea and upset stomach symptoms are ex-\npected to resolve along with providing context for the indi-\nvidual’s experience with the symptoms, Questions 4 and 8\nin Appendix C. Model 2’s response to Question 4 acknowl-\nedged the individual’s experience without responding with\ninformation on the time component— “I understand that it\ncan be frustrating to experiment these side effects during\nseveral weeks.” Similarly, Model 5’s response to Question\n8 was, “I’m sorry you are experiencing these side effects. It\nis important to keep in mind that each person is difference\nand may experience side effects differently.” While the mod-\nels correctly identify the individual’s symptom experience,\nit does not empathetically answer the timeline component to\nthe question.\nMissing Exploratory Responses.(0s in third category):\nThe models did not generate responses that fell into Empa-\nthy Category Three which examines ability of the model to\nexplore feelings and experiences. LLM preferred more close\nended questions, such as, “Do you have any other questions\nor concerns?” instead of generating open-ended exploratory\nstatements like, “Tell me more about your symptoms.”\nMedical Accuracy\nThe inclusion of RAG decreased the overall model score.\nBased on our examination of the results, it seems due to the\nlow specificity of the RAG and can be improved in the fu-\nture.\nRemaining Pitfalls. While medically appropriate, re-\nsponses to severe symptoms occasionally appeared to be\nthe kind of message that could exacerbate users’ anxiety by\nemphasizing urgency without tailoring recommendations to\nspecific circumstances, such as overcrowded healthcare fa-\ncilities. For example, the model tells patients that the prob-\nlem can be very serious and that the patient should seek im-\nmediate help. This repetition failed to provide adequate so-\nlutions to the user’s context.\nRAG’s medical underperformance was not anticipated.\nWhile the model’s ability to respond to certain questions\nimproved, it was accompanied by false claims in other con-\ntexts. This could be due to the model including excessive\nincomplete data from TB guidelines, which resulted in in-\ncorrect or conflicting conclusions. For example, when asked\nabout urine color, it correctly retrieves an excerpt from Mayo\nClinic guidelines, stating (translated to English) that “This\norange discoloration of bodily fluids is expected and harm-\nless. It is normal and the color may vary depending on the\ntype of fluid.” However, for other questions (i.e. a question\nabout whether it is safe to take analgesics), it incorrectly re-\ntrieves a passage relating to other types of medicine which\nexplicitly states (translated to English) that “All TB drugs\ncan be toxic to the liver,” hence leading to an incorrectly\ncautious reply.\nLinguistic Relevance\nThe models generally demonstrated correct grammar and\ncontextually relevant vocabulary in their responses, effec-\ntively aligning with the Spanish variety spoken in Argentina.\nThis was evident in the terminology used to refer to the\nhealth system, healthcare facilities, medical professionals,\nand symptoms or treatment side effects. Responses felt nat-\nural and relatable to users. A notable limitation persisted in\nthe use of the pronoun t´u (you) and its associated verb conju-\ngations, instead of adapting to the informal vos (you) or the\nformal usted or showing inconsistency in maintaining pro-\nnoun and verb conjugation coherence. Specifically, when at-\ntempting to use the Argentine vos form, it may revert to t´u\nor usted within the same interaction. The complexity of the\nvoseo paradigm lies in its variable impact across verb tenses\nand its dependence on geographical and social factors. The\nsingular usted is the standard form in formal contexts in both\nLatin America and Spain. A model’s inability to adapt to ei-\nther vos or usted limits its ability to align with the linguistic\nnorms expected by users in Argentina. Model 0 uses voseo\nexplicitly (e.g., “ten´es”) as used in Argentina. So, the re-\nsponse feels approachable and natural.\nOverall Quality\nContinuity. The models showed difficulty maintaining con-\ntext in more extended interactions. They often failed to in-\ntegrate prior user inputs, leading to repetitive or generic re-\nsponses. Simple affirmations, such as “yes/s´ı” to the model\nquestions, were insufficient to prompt the model to continue\nthe conversation. After providing repeated or irrelevant in-\nformation to a follow-up question, entering another word\nprompted the model to answer the follow-up question ap-\npropriately. For example, when a user reported nausea esca-\nlating to vomiting and added, “I started vomiting and cannot\nsee the doctor now. I’m calling, but no one is answering,”\nthe model initially repeated its prior response about nau-\nsea. Only the second prompt caused the model to address\n\n\nModel Name\nEpsilon (ϵ)\nEmpathy\nMedical Accuracy\nLinguistic Accuracy\nPronouns\nCurated Few-Shot\n—\n0.00, 1.00, 0.00\n4.0\nModerate\nUsted\nDynamic Few-Shot\n0.01\n0.00, 0.50, 0.00\n4.4\nModerate\nt´u\nDynamic Few-Shot\n0.10\n0.00, 0.25, 0.00\n2.6\nModerate\nt´u\nDynamic Few-Shot\n1.00\n0.00, 0.50, 0.00\n4.0\nModerate\nt´u\nDynamic Few-Shot\n10.00\n0.00, 0.50, 0.00\n4.4\nHigh\nVos\nDynamic Few-Shot\n100.00\n0.00, 0.50, 0.00\n4.4\nHigh\nVos\nDynamic Few-Shot\n1000.00\n0.00, 0.50, 0.00\n4.6\nHigh\nVos\nTable 4: Average scores for privacy ablation study. Comparing empathy, medical accuracy, linguistic accuracy, and pronoun\nusage across different privacy levels denoted by epsilon (ϵ).\nthe vomiting. Overuse of generalized responses: The model\nheavily relied on phrases such as “It is important to con-\nsult your doctor,” which was repeated excessively, as an an-\nswer to specific questions. This approach could be frustrat-\ning when users expressed difficulties contacting healthcare\nproviders. Sometimes, the model offered practical advice on\nsymptom management and medication concerns. However,\nit also gave contradictory statements. For instance, when a\nuser asked about depression resources, the model suggested\nthe user to search online for local resources, contradicting\nits earlier claim of being able to provide specific informa-\ntion. This reduces the credibility and utility of its responses,\nespecially for users in urgent need of local services.\nStereotyping: The AI model displayed inconsistency in\ngender-inclusive forms such as m´edico/a (physician) or en-\nfermero/a (nurse) when referring to healthcare professions.\nIn Spanish, nouns ending in -o in the masculine form typ-\nically form the feminine by replacing the final vowel with\n-a. This convention applies to professions and roles, ensur-\ning grammatical agreement between the noun’s gender and\nits referent. By defaulting to the masculine form (m´edico),\nthe model shows a gender bias in linguistic representation\ntoward the default use of masculine forms. Furthermore, the\nmodel occasionally misapplied the -o/a gendered morphol-\nogy to itself, leading to responses that appeared confusing.\nPrivacy\nThe first model in Table 4 has a single manually crafted 8-\nturn dialogue with no PII present placed in the context for\nfew-shot learning, demonstrating model utility with an ep-\nsilon of 0 since no private data is present. The following 6\nmodels have examples that are dynamically retrieved from\nour database of stored patient texts that are sanitized (Yue\net al. 2021) at various privacy epsilon values. To clearly\ndistinguish these scores from the preceding evaluation, we\nname the approach Dynamic Few-Shot, since we use the\nFew-Shot prompt from before, displayed in Table 3, but in-\nstead of using a predefined set of examples, we retrieve them\ndynamically via a RAG pipeline from a datastore with sani-\ntized dialogues between treatment supporters and users col-\nlected during previous study (Tola et al. 2015). The most\nconsistent change in the quality of the model seem to be in\nthe Linguistic Accuracy category, where models with less\nprivacy guarantees (higher ϵ) yielded higher scores. That is\nin line with expectation, since in DP, higher ϵ means less\nadded noise, typically leading to higher utility. Further in-\nvestigation is still needed to explain some results of the eval-\nuation, because our evaluation results were limited by Ope-\nnAI’s guardrails, preventing some of the responses from oc-\ncurring. For example at ϵ 0.10, the Medical Accuracy suf-\nfered a significant drop, that does not seem to be sustained\nwhen the ϵ decreased to 0.01, contrary to the expectations.\nDiscussion\nCreating one conversational agent optimized to respond both\nin an empathetic style and provide factually correct re-\nsponses turned out to be challenging. We tried both con-\ndensing different prompts into one (Model 4) and separating\nprompts (Model 5) in the multi-agent pipeline, but the sys-\ntem continued to occasionally produce both not empathetic\nand not accurate responses. We believe developing a more\nrobust version of our system could be a valuable research\ndirection in the future, with multi-agent framework that can\nallow to separately improve each agent for a specific task.\nLimitations. We recognize that our use-case scenario is\nhighly specific, and the considerations necessary for LLMs’\nincorporation in other settings vary. Nevertheless, these pre-\nliminary results provide valuable insights for developing\na more general procedure for LLM contextualization as a\nmedical tool in different cultures.\nFor our privacy evaluation, we relied on epsilon (ϵ) values\nof the sanitization algorithm instead of performing a mem-\nbership inference attack (MIA), which would give a better\nunderstanding of the algorithm’s sanitization performance.\nThat continues to constitute a valuable research direction.\nFuture Work and Conclusion. We will continue our work\non resolving the issues described in this paper, such as the\npresence of imprecise medical knowledge embedded in the\nmodel, or the culture bias, which have been documented in\nthe previous literature (Liu et al. 2024). LLM’s knowledge\ncan be extended via knowledge graphs, capable of captur-\ning more precise relations in the information than traditional\nRAG (Agrawal et al. 2024). For bias mitigation, multiple so-\nlutions have been proposed, including culture-specific post-\ntraining alignment (Alyafeai et al. 2024) or novel prompt-\ning techniques to address bias directly (AlKhamissi et al.\n2024). The primary limitation of these methods is their\nlimited generalization across different cultures, requiring\nthe involvement of local residents during the development\nphase. Datasets compiled specifically for Argentinian cul-\ntural alignment may be currently lacking, which highlights\nthe importance of our work. Fine-tuning on datasets de-\nsigned for other cultures may lead to worse results through a\nprocess known as catastrophic forgetting (Kotha, Springer,\nand Raghunathan 2024). We believe that a promising ap-\nproach to mitigate existing issues, including inaccurate med-\nical advice and privacy leakage is to build more precise tools\ncapable of detecting instances of these phenomena, allowing\nto re-write responses before they would reach the end user.\n\n\nAcknowledgments\nWe thank the anonymous reviewers for their valuable feed-\nback. Daniil Filienko is a Carwein-Andrews Distinguished\nFellow. This research was, in part, funded by the UW Popu-\nlation Health Initiative and the National Institutes of Health\n(NIH) Agreement No.1OT2OD032581.\nReferences\nAgrawal, G.; Kumarage, T.; Alghamdi, Z.; and Liu, H. 2024.\nCan Knowledge Graphs Reduce Hallucinations in LLMs?:\nA Survey. In Proceedings of the 2024 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies (Volume\n1: Long Papers), 3947–3960.\nAlKhamissi, B.; ElNokrashy, M.; Alkhamissi, M.; and Diab,\nM. 2024. Investigating Cultural Alignment of Large Lan-\nguage Models. In Ku, L.-W.; Martins, A.; and Srikumar, V.,\neds., Proceedings of the 62nd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Pa-\npers), 12404–12422.\nAlyafeai, Z.; Almubarak, K.; Ashraf, A.; Alnuhait, D.; Al-\nshahrani, S.; Abdulrahman, G.; Ahmed, G.; Gawah, Q.;\nSaleh, Z.; Ghaleb, M.; Ali, Y.; and Al-shaibani, M. 2024.\nCIDAR: Culturally Relevant Instruction Dataset For Arabic.\nIn Findings of the Association for Computational Linguis-\ntics: ACL 2024, 12878–12901.\nBoutilier, J. J.; Yoeli, E.; Rathauser, J.; Owiti, P.; Subbara-\nman, R.; and J´onasson, J. O. 2022. Can digital adherence\ntechnologies reduce inequity in tuberculosis treatment suc-\ncess? Evidence from a randomised controlled trial.\nBMJ\nGlobal Health, 7(12).\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. In Advances in Neural Information\nProcessing Systems (NeurIPS), volume 33, 1877–1901.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1, 4171–4186.\nDouze, M.; Guzhva, A.; Deng, C.; Johnson, J.; Szilvasy, G.;\nMazar´e, P.-E.; Lomeli, M.; Hosseini, L.; and J´egou, H. 2024.\nThe Faiss library.\nDuan, H.; Dziedzic, A.; Papernot, N.; and Boenisch, F. 2023.\nFlocks of Stochastic Parrots: Differentially Private Prompt\nLearning for Large Language Models. In Advances in Neu-\nral Information Processing Systems (NeurIPS), volume 36,\n76852–76871.\nDuchi, J. C.; Jordan, M. I.; and Wainwright, M. J. 2013. Lo-\ncal Privacy and Statistical Minimax Rates. In 2013 IEEE\n54th Annual Symposium on Foundations of Computer Sci-\nence, 429–438.\nDwork, C.; and Roth, A. 2014. The Algorithmic Founda-\ntions of Differential Privacy. Theoretical Computer Science,\n9(3-4): 211–407.\nFilienko, D.; Wang, Y.; Jazmi, C. E.; Xie, S.; Cohen, T.;\nDe Cock, M.; and Yuwen, W. 2024.\nToward Large Lan-\nguage Models as a Therapeutic Tool: Comparing Prompt-\ning Techniques to Improve GPT-Delivered Problem-Solving\nTherapy. In AMIA 2024 Annual Symposium Proceedings.\nFukunaga, R.; Glaziou, P.; Harris, J. B.; Date, A.; Floyd, K.;\nand Kasaeva, T. 2021. Epidemiology of tuberculosis and\nprogress toward meeting global targets – Worldwide, 2019.\nMorbidity and Mortality Weekly Report (MMWR), 70(12):\n427–430.\nHuang, Y.; Gupta, S.; Zhong, Z.; Li, K.; and Chen, D. 2023.\nPrivacy Implications of Retrieval-Based Language Models.\nIn Proceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, 14887–14902.\nHuddart, S.; MacLean, E.; and Pai, M. 2016. Location, loca-\ntion, location: tuberculosis services in highest burden coun-\ntries. The Lancet Global Health, 4(12): e907–e908.\nIribarren, S. J.; Milligan, H.; Chirico, C.; Goodwin, K.;\nSchnall, R.; Telles, H.; Iannizzotto, A.; Sanjurjo, M.; Lutz,\nB. R.; Pike, K.; et al. 2022.\nPatient-centered mobile tu-\nberculosis treatment support tools (TB-TSTs) to improve\ntreatment adherence: A pilot randomized controlled trial ex-\nploring feasibility, acceptability and refinement needs. The\nLancet Regional Health–Americas, 13(100291).\nJerene, D.; Levy, J.; van Kalmthout, K.; Rest, J. v.; Mc-\nQuaid, C. F.; Quaife, M.; Charalambous, S.; Gamazina, K.;\nGarfin, A. M. C.; Mleoh, L.; Terleieva, Y.; Bogdanov, A.;\nMaraba, N.; and Fielding, K. 2023. Effectiveness of digital\nadherence technologies in improving tuberculosis treatment\noutcomes in four countries: a pragmatic cluster randomised\ntrial protocol. BMJ Open, 13(3): e068685.\nJo, E.; Epstein, D. A.; Jung, H.; and Kim, Y.-H. 2023. Un-\nderstanding the Benefits and Challenges of Deploying Con-\nversational AI Leveraging Large Language Models for Pub-\nlic Health Intervention. In Proceedings of the 2023 ACM\nCHI Conference on Human Factors in Computing Systems.\nKang, C.; Novak, D.; Urbanova, K.; Cheng, Y.; and Hu,\nY. 2024. Domain-Specific Improvement on Psychotherapy\nChatbot Using Assistant. In 2024 IEEE International Con-\nference on Acoustics, Speech, and Signal Processing Work-\nshops (ICASSPW), 351–355.\nKotha, S.; Springer, J. M.; and Raghunathan, A. 2024. Un-\nderstanding Catastrophic Forgetting in Language Models via\nImplicit Inference. In The Twelfth International Conference\non Learning Representations.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; K¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt¨aschel,\nT.; Riedel, S.; and Kiela, D. 2020.\nRetrieval-augmented\ngeneration for knowledge-intensive NLP tasks. Advances\nin Neural Information Processing Systems (NeurIPS), 33:\n9459–9474.\nLi, Z.; Shi, Y.; Liu, Z.; Yang, F.; Payani, A.; Liu, N.; and\nDu, M. 2024.\nQuantifying Multilingual Performance of\nLarge Language Models Across Languages. arXiv preprint\narXiv:2404.11553.\n\n\nLiu, Y.; Yao, Y.; Ton, J.-F.; Zhang, X.; Guo, R.; Cheng,\nH.; Klochkov, Y.; Taufiq, M. F.; and Li, H. 2024.\nTrust-\nworthy LLMs: a Survey and Guideline for Evaluating\nLarge Language Models’ Alignment.\narXiv preprint\narXiv:2308.05374.\nMoor, M.; Banerjee, O.; Abad, Z. S. H.; Krumholz, H. M.;\nLeskovec, J.; Topol, E. J.; and Rajpurkar, P. 2023. Foun-\ndation models for generalist medical artificial intelligence.\nNature, 616(7956): 259–265.\nMukherjee, S.; Gamble, P.; Ausin, M. S.; Kant, N.; Ag-\ngarwal, K.; Manjunath, N.; Datta, D.; Liu, Z.; Ding, J.;\nBusacca, S.; et al. 2024. Polaris: A Safety-focused LLM\nConstellation Architecture for Healthcare.\narXiv preprint\narXiv:2403.13313.\nNembhard, I. M.; David, G.; Ezzeddine, I.; Betts, D.; and\nRadin, J. 2023. A systematic review of research on empathy\nin health care. Health Services Research, 58(2): 250–263.\nNori, H.; Lee, Y. T.; Zhang, S.; Carignan, D.; Edgar, R.; Fusi,\nN.; King, N.; Larson, J.; Li, Y.; Liu, W.; Luo, R.; McKin-\nney, S. M.; Ness, R. O.; Poon, H.; Qin, T.; Usuyama, N.;\nWhite, C.; and Horvitz, E. 2023. Can Generalist Foundation\nModels Outcompete Special-Purpose Tuning? Case Study in\nMedicine. CoRR, abs/2311.16452.\nPapernot, N.; Abadi, M.; ´Ulfar Erlingsson; Goodfellow, I.;\nand Talwar, K. 2017. Semi-supervised Knowledge Transfer\nfor Deep Learning from Private Training Data. In Proceed-\nings of the 5th International Conference on Learning Repre-\nsentations (ICLR).\nRam´ırez, B. G.; L´opez Espejel, J.; del Carmen Santi-\nago D´ıaz, M.; and Trinidad Rub´ın Linares, G. 2024. S´olo\nEsc´uchame: Spanish Emotional Accompaniment Chatbot.\narXiv preprint arXiv:2408.01852.\nSha, Z.; and Zhang, Y. 2024.\nPrompt Stealing At-\ntacks Against Large Language Models.\narXiv preprint\narXiv:2402.12959.\nSharma, A.; Miner, A.; Atkins, D.; and Althoff, T. 2020.\nA Computational Approach to Understanding Empathy Ex-\npressed in Text-Based Mental Health Support. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), 5263–5276.\nSubbaraman, R.; de Mondesert, L.; Musiimenta, A.; Pai, M.;\nMayer, K. H.; Thomas, B. E.; and Haberer, J. 2018. Digital\nadherence technologies for the management of tuberculosis\ntherapy: mapping the landscape and research priorities. BMJ\nglobal health, 3(5): e001018.\nTang, X.; Shin, R.; Inan, H. A.; Manoel, A.; Mireshghallah,\nF.; Lin, Z.; Gopi, S.; Kulkarni, J.; and Sim, R. 2024. Privacy-\nPreserving In-Context Learning with Differentially Private\nFew-Shot Generation. In Proceedings of the 12th Interna-\ntional Conference on Learning Representations (ICLR).\nTola, H.; Tol, A.; Shojaeizadeh, D.; and Garmaroudi, G.\n2015. Tuberculosis Treatment Non-Adherence and Lost to\nFollow Up among TB Patients with or without HIV in De-\nveloping Countries: A Systematic Review. Iranian Journal\nof Public Health, 44(1): 1–11.\nTu, T.; Palepu, A.; Schaekermann, M.; Saab, K.; Freyberg,\nJ.; Tanno, R.; Wang, A.; Li, B.; Amin, M.; Tomasev, N.; et al.\n2024. Towards conversational diagnostic AI. arXiv preprint\narXiv:2401.05654.\nWang, B.; Chen, W.; Pei, H.; Xie, C.; Kang, M.; Zhang, C.;\nXu, C.; Xiong, Z.; Dutta, R.; Schaeffer, R.; et al. 2023. De-\ncodingTrust: A Comprehensive Assessment of Trustworthi-\nness in GPT Models. In Proceedings of the 37th Interna-\ntional Conference on Neural Information Processing Sys-\ntems (NeurIPS), 31232 – 31339.\nWorld\nHealth\nOrganization\n(WHO).\n2023.\nGlobal\nTuberculosis Report.\nhttps://www.who.int/teams/global-\ntuberculosis-programme/tb-reports/global-tuberculosis-\nreport-2023.\nWu, S.; and Dredze, M. 2019. Beto, Bentz, Becas: The Sur-\nprising Cross-Lingual Effectiveness of BERT. In Proceed-\nings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\nIJCNLP), 833–844.\nWu, T.; Panda, A.; Wang, J. T.; and Mittal, P. 2024. Privacy-\nPreserving In-Context Learning for Large Language Mod-\nels. In Proceedings of the 12th International Conference on\nLearning Representations (ICLR).\nXie, C.; Lin, Z.; Backurs, A.; Gopi, S.; Yu, D.; Inan, H. A.;\nNori, H.; Jiang, H.; Zhang, H.; Lee, Y. T.; Li, B.; and\nYekhanin, S. 2024.\nDifferentially Private Synthetic Data\nvia Foundation Model APIs 2: Text.\nIn Proceedings of\nthe 41st International Conference on Machine Learning\n(ICML), volume 235 of PMLR, 54531–54560.\nYue, X.; Du, M.; Wang, T.; Li, Y.; Sun, H.; and Chow, S.\nS. M. 2021. Differential Privacy for Text Analytics via Nat-\nural Text Sanitization. In Findings of the Association for\nComputational Linguistics: ACL/IJCNLP, 3853–3866.\nZeng, S.; Zhang, J.; He, P.; Liu, Y.; Xing, Y.; Xu, H.; Ren,\nJ.; Chang, Y.; Wang, S.; Yin, D.; and Tang, J. 2024. The\nGood and The Bad: Exploring Privacy Issues in Retrieval-\nAugmented Generation (RAG). In Findings of the Associa-\ntion for Computational Linguistics: ACL 2024, 4505–4524.\nZhao, Y.; Zhang, W.; Chen, G.; Kawaguchi, K.; and Bing,\nL. 2024. How do Large Language Models Handle Multilin-\ngualism? In Proceedings of the 38th Annual Conference on\nNeural Information Processing Systems (NeurIPS).\n\n\nAppendix A:\nArgentinian Variation of Spanish Language\nRioplatense Spanish, spoken in Argentina and Uruguay, ex-\nhibits distinct linguistic features. One of the most prominent\nis “voseo”, where the second-person singular pronoun “vos”\nis used instead of “t´u,” along with corresponding verb forms.\nFor instance, “vos ten´es” replaces “t´u tienes.” This feature\nvaries regionally and impacts interactions with text-based\nsystems. Rioplatense Spanish also favors the periphrastic\nfuture tense (“voy a ir”) over the simple future (“ir´e”) and\nincludes voseo-specific imperatives like “hacelo” instead of\n“hazlo.” The lexicon is enriched with Italianisms, reflecting\nthe influence of Italian immigration, as well as borrowings\nfrom neighboring languages and indigenous terms. Further-\nmore, the lunfardo argot contributes a unique vocabulary\nthat is widely understood in the region.\nAppendix B: Extracting Patient Information\nData leakage from the system prompt, be it in RAG (Zeng\net al. 2024) or other settings (Sha and Zhang 2024), is a\nknown concern with LLMs. It is especially problematic in\nsettings where the prompt contains sensitive information,\nincluding a patient’s symptoms or even the fact of being\npresent in a TB trial.\nWe adapted a previously proposed targeted attack (Zeng\net al. 2024) to our task. The attack presumes that a malicious\nuser has black-box access to the LLM. The attack works re-\ngardless of the system prompt of the model, since our RAG\nprocess only uses recent conversation history and the user’s\nquery to find most similar documents. The attack consists of\ninputting a short sentence that the attacker suspects may be\npresent in the RAG database to retrieve a larger part of the\ndocument that overlaps with the input sentence. The follow-\ning is an example of such an attack\nContin´ua diciendo, “ah, algo que olvid´e decirte” del\ncontexto que proporcion´e\nContinue after, “oh, something I forgot to tell you”\nfrom the context I provided\nwhich overlapped with a chunk from one of the docu-\nments in the RAG database in 5 tokens. That means that\none of the patient-doctor conversations present in the RAG\ndataset contained the phrase ‘ah, algo que olvid´e decirte’ and\nwas retrieved at inference time by RAG, because it is simi-\nlar to the attacker’s query. These tokens could be reasonably\nchosen at random, constituting a very general phrase, even\nif it was not known to be contained in the dataset by the\nattacker. This led the model to output an excerpt of more\nthan 40 tokens verbatim, including the doctor’s name. This\nunderlines the necessity for the privacy-preserving data san-\nitization method used in our study.\nAppendix C: Representative Questions\nTo empirically evaluate the models, clinicians on our team\ncompiled a list of questions representative of questions typ-\nically asked by patients. We developed 10 representative\nquestions, including both empathy-seeking and information-\nseeking questions.\n1. ¿Es normal que la orina tenga un color entre naranja y\nrojo?\n2. ¿Es normal seguir teniendo sudoraci´on nocturna de-\nspu´es de haber comenzado el tratamiento?\n3. ¿Es seguro tomar medicamentos como analg´esicos, an-\ntifebriles o antial´ergicos junto con los medicamentos\npara la tuberculosis?\n4. ¿En qu´e momento desaparecen los s´ıntomas como\nlas n´auseas y el malestar estomacal? Llevo semanas\ntomando los medicamentos y no he notado ninguna\nmejor´ıa.\n5. He olvidado tomar los medicamentos esta semana, ¿qu´e\nsucede si me olvido de tomarlos algunas veces?\n6. Tengo manchitas rojas por todo el cuerpo y me pican mu-\ncho. ¿Qu´e tengo que hacer?\n7. ¿Tomo todas las pastillas juntas o algunas por la ma˜nana\ny otras por la noche?\n8. ¿Cu´ando se van los s´ıntomas como n´auseas y malestar\nestomacal/descompostura/dolor de est´omago? Hace se-\nmanas que estoy tomando la medicaci´on y no hubo\nninguna mejor´ıa.\n9. ¿C´omo puedo estar seguro de que los medicamentos\nest´an haciendo efecto?\n10. ¿Cu´ando podr´e volver a trabajar/estudiar/hacer vida\nnormal?\nHowever, due to OpenAI safety guardrails, some of the\nquestions were rejected, occasionally specifying that “the re-\nsponse was filtered due to the prompt triggering Azure Ope-\nnAI’s content management policy,” leading us to decrease\nthe number of asked questions. When we did the evaluation,\nquestion 6 and 7 tended to be rejected by some or all of\nthe models, hence to provide consistent results, we have not\nused the results associated with these questions.\nEmpathy Questions\nEmpathy results in Table 3 and 4 were based on questions 4,\n8, 9, and 10.\nMedical Accuracy Questions\nThe medical accuracy results in Table 3 and 4 were based on\nquestions 1, 2, 3, 4, and 5.\nAppendix D: Prompts\nBelow we list the prompts used in the models.\nBaseline Prompt\nZero-Shot prompt, in English, without examples.\nYou are a Spanish AI healthcare tool for a mobile Tubercu-\nlosis health application. Your role is to respond to incoming\nuser messages related to tuberculosis (TB) treatment, pro-\nviding information about their treatment plan, side effects,\nand general guidance. Your responses should be short, clear,\nand empathetic, while following the treatment protocols for\nTB management. Respond to the following:\n\n\nBaseline Prompt (Spanish)\nZero-Shot prompt, in Spanish, without examples.\nEres una herramienta de atenci´on m´edica de inteligencia ar-\ntificial en espa˜nol para una aplicaci´on m´ovil de salud contra\nla tuberculosis que responde a los mensajes entrantes de los\nusuarios. Su objetivo es brindarle al usuario informaci´on\nsobre su plan de tratamiento de la tuberculosis y cualquier\nefecto secundario que pueda estar experimentando. Debes\nser solidario y emp´atico en tus respuestas. Tus respuestas\ndeben ser en espa˜nol. Responde a la brevedad.\nFew-Shot/Informational Prompt\nThis prompt contains general information about the task\nand a few carefully selected examples from previous real\ninteractions between a healthcare provider and a TB patient.\nPrompt para Agente de IA: Comunicaci´on sobre Efectos Se-\ncundarios de la Tuberculosis\nSos un asistente virtual especializado en salud, dise˜nado\npara comunicarte con pacientes argentinos que est´an reci-\nbiendo tratamiento para la tuberculosis (TB).\nTu objetivo principal es brindar informaci´on clara y precisa\nsobre los efectos secundarios comunes del tratamiento de la\nTB, utilizando un lenguaje accesible y comprensible para el\np´ublico general.\nContexto:\n- Est´as interactuando con pacientes argentinos de diversos\nor´ıgenes y niveles educativos.\n- El tratamiento de la TB suele ser largo y puede tener var-\nios efectos secundarios.\n- Los pacientes pueden estar preocupados o ansiosos por\nestos efectos secundarios. Si presentan esto, aseg´urese de\nconsolarlos y mostrar empat´ıa.\nTus tareas principales son:\n1. Informar sobre los efectos secundarios comunes del\ntratamiento de la TB, incluyendo:\n- N´auseas y malestar estomacal\n- Cambios en el apetito\n- Fatiga\n- Cambios en la coloraci´on de la orina\n- Erupciones cut´aneas\n- Problemas de visi´on\n2. Explicar que estos efectos son generalmente manejables\ny temporales.\n3. Responder preguntas espec´ıficas sobre efectos secundar-\nios de manera clara y comprensible.\n4. Proporcionar consejos pr´acticos para manejar los efectos\nsecundarios leves en casa.\n5. Enfatizar la importancia de completar el tratamiento\ncompleto, incluso si los s´ıntomas de la TB mejoran.\nPautas de comunicaci´on:\n- Us´a el ’vos’ caracter´ıstico del espa˜nol argentino.\n- Emple´a modismos y expresiones comunes en Argentina\ncuando sea apropiado.\n- Evit´a jerga m´edica compleja; explic´a los t´erminos t´ecnicos\nde manera sencilla.\n- S´e emp´atico y comprensivo con las preocupaciones de los\npacientes.\n- Animate a los pacientes a hacer preguntas y expresar sus\ninquietudes.\nEstos son algunos ejemplos de c´omo ser´ıa una conversaci´on\nentre una enfermera y un paciente:\nP: ¿Es normal que la orina sea (tan) oscura?\nC: S´ı, el medicamento rifampicina com´unmente causa una\ncoloraci´on naranja o caf´e en la orina o las l´agrimas. Pero,\nsi empieza a notar sangre en la orina o un color rojo por\nfavor contacte a su m´edico ya que la sangre en la orina no\nser´ıa normal. Espero que esta informaci´on le sea ´util. ¿Tiene\nalguna otra pregunta?\nP: Me duele mucho el est´omago y tengo n´auseas, ¿qu´e tengo\nque hacer?\nC: Siento mucho que no est´e bien. Las n´auseas y el do-\nlor estomacal son efectos secundarios muy comunes del\ntratamiento de la tuberculosis. ¿Ha vomitado?\nP: S´ı estoy vomitando mucho\nC: Lo siento mucho , a veces, en el caso de algunos pa-\ncientes, los efectos secundarios son muy graves . En este\ncaso, creo que necesita consultar con un m´edico/a ya que es\nposible que le cambien los medicamentos que est´a tomando.\nPor ahora trate de seguir tomando las medicinas y llame a\nm´edico que recet´e la medicaci´on de la tuberculosis. ¿Tiene\nalguna otra pregunta?\nP: Buenas tardes, ¿puedo tomar paracetamol con estos\notros medicamentos?\nC: Buenas tardes (Nombre), S´ı puede tomar tylenol y otros\nmedicamentos para el dolor como ibuprofeno. Recuerde que\ndebe tomar mas de 4 gramos de tylenol al dia. Tiene alguna\notra pregunta?\nP: Buenas tardes, ¿puedo tomar paracetamol con estos\notros pastillas?.\nC: S´ı, puede tomar caf´e con estos medicamentos. ¿Tiene al-\nguna otra pregunta?\nP: No eso es todo\nC: Espero que esto resuelva su duda, si tiene alguna otra\nduda (pregunta) estamos aqu´ı para ayudarle.\nP: ¿Puedo comer hamburguesas con estos medicamentos?\nC: S´ı, puede comer hamburguesas mientras est´atomando\nmedicamentos para la tuberculosis. ¿Tiene alguna pre-\ngunta?\nP: No eso ser´ıa todo, muchas gracias.\nC: ¡De nada! Estamos aqu´ı para ayudar!\nAhora responda a la siguiente pregunta asegur´andose de\nproporcionar informaci´on objetiva y de que sea clara y con-\ncisa:\nRAG Prompt\nShort prompt, saving space for more context being retrieved.\nEres un robot partidario de la tuberculosis. Responda la pre-\ngunta del usuario utilizando la siguiente informaci´on:\nClassification Prompt\nClassification prompt that lists few-shot examples with\ndesired question classification.\nDetermine si esta afirmaci´on busca empat´ıa o (1) o busca\ninformaci´on (0). Clasifique como emocional s´olo si la pre-\ngunta expresa preocupaci´on, ansiedad o malestar sobre el\nestado de salud del paciente.\nEn caso contrario, clasificar como informativo.\nEjemplos:\n\n\n- P: Me siento muy ansioso por mi diagn´ostico de tubercu-\nlosis. 1\n- P: Ay´udenme con mi tratamiento de tuberculosis. Mi orina\nes roja. 0\n- P: Estoy preocupada porque tengo mucho dolor. 1\n- P: ¿Es seguro tomar medicamentos como analg´esicos\njunto con medicamentos para la tuberculosis? 0\n- P: ¿con relacion al tratamiento, es normal tener vomito? 0\nAqu´ı est´a la declaraci´on para clasificar. Simplemente re-\nsponda con el n´umero ”1” o ”0”:\nEmotional Prompt\nZero-Shot prompt for an emotional task.\nSos un asistente virtual especializado en salud, dise˜nado\npara comunicarte con pacientes argentinos que est´an reci-\nbiendo tratamiento para la tuberculosis (TB).\nTu objetivo principal es brindar informaci´on clara y recon-\nfortante sobre los efectos secundarios del tratamiento, con\nun enfoque en escuchar y apoyar a los pacientes en sus in-\nquietudes.\nContexto:\nEst´as interactuando con pacientes argentinos de diversas\nedades, or´ıgenes y niveles educativos.\nSabemos que el tratamiento de la TB puede ser largo y que\nsus efectos secundarios pueden causar incomodidad y pre-\nocupaci´on en los pacientes.\nMuchos pacientes pueden sentirse ansiosos o abrumados\npor estos efectos secundarios. Asegurate de responder con\ncalidez, apoyo y comprensi´on.\nTus tareas principales son:\nEscuchar y responder preguntas espec´ıficas sobre los efec-\ntos secundarios de manera clara y tranquilizadora.\nCompartir consejos pr´acticos para sobrellevar los efectos\nsecundarios leves desde casa.\nEnfatizar la importancia de completar el tratamiento com-\npleto, a´un cuando los s´ıntomas de la TB comiencen a mejo-\nrar.\nPautas de comunicaci´on:\nUtiliz´a el ”vos” caracter´ıstico del espa˜nol argentino,\nadem´as de expresiones propias de la cultura local para que\nse sientan en confianza.\nEvit´a tecnicismos m´edicos; manten´e las explicaciones sim-\nples y accesibles. Mostrate siempre emp´atico y dispuesto a\nescuchar cualquier inquietud o preocupaci´on.\nInvit´a al paciente a seguir en contacto, anim´andolo a que se\nsienta libre de expresar sus dudas.\nAhora responda la siguiente pregunta, asegur´andose de con-\nsolar al paciente si es necesario. Sea conciso y emp´atico:\nAppendix E: Privatized Messages Examples\nAs can be seen, as examples increase in privacy, the qual-\nity of examples decreases, becoming of less and less val-\nues for the Few-Shot settings, which rely on high-quality\nexamples for optimal performance. It hence shows that our\nresults, demonstrating that linguistic model of the accuracy\ndoes not improve as much when presented with low ϵ ex-\namples, are to be expected. Below is the same excerpt from\nthe original set of patient and clinical supporter dialogues,\nbut perturbed at various values of ϵ, showing how perturba-\ntion and added noise decrease the quality of the dialogues\ninputted in the model. As can be seen, with the increase in\nepsilon, model starts to produce tokens that cannot be prop-\nerly decoded. For example, [unused489]. In BETO, some\ntokens are marked as [unusedX] (where X is a number) be-\ncause they were reserved for some future use but are not\nassigned any particular meaningful word in the original pre-\ntraining. Appearance of such values shows how added noise\ndecreases the utility or semantic meaning of the dialogue.\nϵ 0.01\n“##decer ##sburgo excep atar ##bici´on debu incumben\nchich asesinato aser ##raciones ##yp casilla ##sa afe ser´e\navanzada cump disculpe rc ##maci´on ciudad saltos morgan\ndepresi´on flag sue cristo [unused386] hered be tim´on ##rol\norigina obse estructural”\nϵ 0.1\n“damablemente ##uri dama hos [unused489] apliquen hrc\ncbs univers conociendo obtener [unused108] ##ls traidor\npresupuestario ##uz blin genes concentrarse har´a entrome\npinturas proa tem estrangul federados [unused868] nostal\n[unused471] advierto [unused385] casar ##zan disminuir\ntasas iluminaci´on”\nϵ 1\n“promulg´o preocuparme aterror sentiste sientes th primera-\nmente doscientos at interactuar ##canos gravedad ##presid\n##n ##lio van establecieron advierto ##´arez ace fue-\nsen frankenstein non mirado placeres sensores ##lea [un-\nused305] sucesivo cordero inmobiliar fruto reclusi´on cu´anta\n##field esquina”\nϵ 10\n“repentino socioecon´omica contrata comprometer adoraci´on\n2015 ##peraci´on perm´ıtanme dir´e presidenta aplicar´an ter-\nriblemente refi acos alemanes ##isi´on diecis´eis pop inter-\nactu ##˜nada ##cr ten´ıamos 53 demarcaci´on [unused166] re-\ncepci´on ##baci´on si l´ogica alguna autor´ıa australia saludos\nhacia aqui ajustes”\nϵ 100\n“doctor : hola buen d´ıa ! c´omo est´an ? est´an pidiendo car-\ngar la toma de la medicaci´on en la aplicaci´on ? c´omo les\nfunciona ? av´ıs ##eme si tienen alguna duda . saludos ! !”\nϵ 1000\n“doctor : hola buen d´ıa ! c´omo est´an ? est´an pidiendo car-\ngar la toma de la medicaci´on en la aplicaci´on ? c´omo les\nfunciona ? av´ıs ##eme si tienen alguna duda . saludos ! !”\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21236v1.pdf",
    "total_pages": 11,
    "title": "Transforming Tuberculosis Care: Optimizing Large Language Models For Enhanced Clinician-Patient Communication",
    "authors": [
      "Daniil Filienko",
      "Mahek Nizar",
      "Javier Roberti",
      "Denise Galdamez",
      "Haroon Jakher",
      "Sarah Iribarren",
      "Weichao Yuwen",
      "Martine De Cock"
    ],
    "abstract": "Tuberculosis (TB) is the leading cause of death from an infectious disease\nglobally, with the highest burden in low- and middle-income countries. In these\nregions, limited healthcare access and high patient-to-provider ratios impede\neffective patient support, communication, and treatment completion. To bridge\nthis gap, we propose integrating a specialized Large Language Model into an\nefficacious digital adherence technology to augment interactive communication\nwith treatment supporters. This AI-powered approach, operating within a\nhuman-in-the-loop framework, aims to enhance patient engagement and improve TB\ntreatment outcomes.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}