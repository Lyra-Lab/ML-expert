{
  "id": "arxiv_2502.21212v1",
  "text": "Transformers Learn to Implement Multi-step Gradient\nDescent with Chain of Thought\nJianhao Huang1*, Zixuan Wang2∗, Jason D. Lee2\n1Shanghai Jiaotong University, 2Princeton University\nMarch 3, 2025\nAbstract\nChain of Thought (CoT) prompting has been shown to significantly improve the perfor-\nmance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by\ninstructing the model to produce intermediate reasoning steps. Despite the remarkable empir-\nical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms\nunderlying CoT training remain largely unexplored. In this paper, we study the training dy-\nnamics of transformers over a CoT objective on an in-context weight prediction task for linear\nregression. We prove that while a one-layer linear transformer without CoT can only imple-\nment a single step of gradient descent (GD) and fails to recover the ground-truth weight vec-\ntor, a transformer with CoT prompting can learn to perform multi-step GD autoregressively,\nachieving near-exact recovery. Furthermore, we show that the trained transformer effectively\ngeneralizes on the unseen data. With our technique, we also show that looped transformers\nsignificantly improve final performance compared to transformers without looping in the in-\ncontext learning of linear regression. Empirically, we demonstrate that CoT prompting yields\nsubstantial performance improvements.\n1\nIntroduction\nTransformer-based Large Language Models (LLMs) have demonstrated significant success across\nvarious language modeling tasks, achieving state-of-the-art performance in numerous domains\n[38]. Remarkably, these models have also unlocked complex reasoning abilities, particularly in\nmathematical problem-solving and coding tasks [12, 5, 1]. A key method driving this advance-\nment is the Chain of Thought (CoT), which enables LLMs to generate intermediate reasoning\nsteps autoregressively rather than providing a direct answer. This process effectively improves the\nmodel’s capacity to solve complex problems. In practice, CoT reasoning can be elicited either\nby providing few-shot CoT examples or by appending prompts like “let’s think step by step” to\nbootstrap the model’s response [27, 55, 44, 37].\n*Equal Contribution (alphabetical order).\n1\narXiv:2502.21212v1  [cs.LG]  28 Feb 2025\n\n\nTheoretically, CoT enables LLMs to perform multi-step sequential computations by generating\nintermediate results, thereby significantly improving the expressive power of transformers [30,\n17, 34] compared to standard decoder transformers that generate direct outputs without interme-\ndiate reasoning [31, 35]. Despite these theoretical insights, it remains unclear how transformers\nare trained on CoT data to effectively execute multi-step reasoning. Furthermore, it is unknown\nwhether a transformer trained specifically with an auto-regressive objective with multi-step CoT\ncan substantially outperform one trained to directly output answers without CoT.\nThis paper takes an initial step beyond expressiveness to study the training dynamics of trans-\nformers when trained on CoT data. Specifically, following the modified in-context learning (ICL)\nsetting on linear regression proposed by [2, 58], we use it as a testbed to analyze the training\nprocess with the CoT framework implemented. We name the task in-context weight prediction\nwhere the goal is to predict the linear weight vector from the sequence of input prompts. Instead\nof performing direct ICL and outputting a prediction, the transformer with CoT prompting is al-\nlowed to generate multiple intermediate steps before arriving at the final answer. We theoretically\ninvestigate the transformer’s training trajectory on the CoT objective and show the expressiveness\ngap between transformers trained with CoT and those without. Our main results show this separa-\ntion is learnable: gradient-based algorithm can learn the constructed transformer with CoT in the\nexpressivity result.\nWe summarize our contributions as follows:\n• Expressiveness Gap. We characterize the global optimum of the population loss for the in-\ncontext weight prediction task on linear regression using a one-layer transformer without CoT\nprompting. Our results show that, without CoT, the transformer at the global minimizer ef-\nfectively performs a single step of gradient descent (GD)(Theorem 3.1), leading to significant\nerrors in predicting the d-dimensional weight vector w∗∈Rd when the number of examples for\nICL is n = eΘ(d) (Corollary 3.1). In contrast, we demonstrate that a one-layer transformer with\nCoT prompting can achieve near-exact recovery by executing multi-step GD (Theorem 3.2).\n• Convergence. We prove the convergence results of running gradient flow on the population\nCoT loss under mild assumptions (Theorem 4.1). Our analysis uses a novel stage-wise approach\ncombining dynamics analysis and landscape properties: the parameters initially approach the\nglobal minimizer, followed by local convergence toward the final solution. Our proof tech-\nnique involves a novel characterization of the complicated population gradient. Furthermore,\nwe prove that the trained transformer can exhibit both in-distribution and out-of-distribution\ngeneralization (Theorem 4.2) at inference time. We are the first to establish the learnable sep-\naration between transformers with and without CoT under the in-context linear regression set-\nting. We empirically validate that the trained transformer converges to the minimizer predicted\nby our theory, with a distinct performance gap between models trained with and without CoT\nprompting.\nOutline.\nIn Section 2, we formalize the problem setting including the data model, the one-layer\ntransformer architecture, and the CoT prompting format. In Section 3, we theoretically show\nthe performance gap between the transformer with and without CoT. Section 4 consists of our\nmain results, including our dynamics analysis and out-of-distribution (OOD) generalization result.\n2\n\n\nSection 5 empirically validates the advantage of CoT.\n1.1\nRelated works\nTraining dynamics of transformers.\nSeveral works have studied the training process of specific\ntransformer architectures. Jelassi et al. [25], Li et al. [28] examined the training process and sample\ncomplexity of Vision Transformer [15]. Tarzanagh et al. [45], Ataee Tarzanagh et al. [6], Li et al.\n[29] explored the connection between the optimization landscape of self-attention mechanisms\nand the Support Vector Machine problem. Tian et al. [46, 48] provided insights into the training\ndynamics of the self-attention and MLP layers during the training process respectively.\nA related line of research focuses on Markov-like data models. Bietti et al. [8] studied the induction\nhead mechanism from the perspective of associative memory. Nichani et al. [36] demonstrated that\na simplified two-layer transformer provably learns a generalized induction head on latent causal\ngraphs. Chen et al. [11] further proved that a modified two-layer multi-head transformer can learn\nin-context generalized n-gram. Edelman et al. [16] investigated the multi-stage phase transitions\nduring training on bigram and n-gram (n ≥3). Additionally, Makkuva et al. [33] studied the loss\nlandscape of transformers trained on sequences from a Markov Chain.\nAnother growing body of literature aims to understand the training dynamics of in-context learning\n(ICL). Garg et al. [19] first empirically studied the ICL capabilities of transformers over a variety\nof function classes. Aky¨urek et al. [4], Von Oswald et al. [51] investigated the behavior of trans-\nformers on random ICL instances of linear regression. Several works have also established the\nexistence of deep transformers capable of implementing multi-step gradient descent (GD) across\ndifferent domains [18, 7, 21]. Mahankali et al. [32], Ahn et al. [3] analyzed the loss landscape of\nthe linear regression ICL task and Zhang et al. [58] proved global convergence on a one-layer linear\nself-attention layer using gradient flow. Gatmiry et al. [20] demonstrated that a linear looped trans-\nformer with specific update procedures can learn to implement multi-step GD for linear regression.\nFurther analyses of training dynamics under more realistic assumptions about data models and ar-\nchitectures have been conducted by Huang et al. [24], Kim & Suzuki [26], Chen et al. [10]. For a\ndetailed discussion see Appendix A.1.\nCompared to prior works, our study and Huang et al. [24], Ahn et al. [3], Zhang et al. [58],\nTarzanagh et al. [45], Nichani et al. [36], Kim & Suzuki [26], Wang et al. [54], Chen et al. [11], Ren\net al. [41] all use similar reparameterizations that combine key and query matrices to simplify the\ntraining dynamics. Moreover, many previous studies [46, 58, 24, 36, 26, 10, 20] adopted the pop-\nulation loss to facilitate the analysis of these dynamics.\nA closely related work is Gatmiry et al. [20], which shows that a looped transformer can implement\nmulti-step GD on the ICL linear regression task to directly predict the query answer in context. In\ncomparison, the goal of our setting is to predict the weight vector from the input examples using\na realistic CoT autoregressive generation process. Theoretically, we also establish a performance\ngap between transformers with CoT and those without. See Appendix A.2 for a more detailed\ndiscussion.\nChain of Thought and Scratchpad\nThe CoT prompting method was first introduced by Wei\net al. [55] to enhance the multi-step reasoning capability of LLMs. Before the formalization of\n3\n\n\nCoT, Nye et al. [37] demonstrated that allowing language models to generate intermediate re-\nsults on “scratchpads” dramatically boosts the multi-step computation ability of LLMs. Wang\net al. [53], Yao et al. [57], Creswell et al. [13], Zhou et al. [59] further proposed variants of the\nCoT/scratchpad method to improve the efficiency and reliability of generation.\nRecently, several works have attempted to understand CoT from both experimental and theoretical\nperspectives. Wang et al. [52], Saparov & He [42], Shi et al. [43], Paul et al. [40] empirically\nstudied the capability of CoT, providing valuable insights on its reasoning processes. Meanwhile,\nWu et al. [56], Tutunov et al. [49], Hou et al. [22], Cabannes et al. [9] investigated CoT through the\nlens of mechanistic interpretability. On the theoretical side, Liu et al. [31], Merrill & Sabharwal\n[34], Li et al. [30], Feng et al. [17] explored the expressive power of transformers with CoT,\nshowing that CoT can significantly extend the expressivity of transformers in the context of circuit\ncomplexity. Hu et al. [23] investigated the statistical foundations of CoT. However, the training\ndynamics of CoT remain largely unexplored. To the best of our knowledge, this work is among the\nfirst theoretical analyses of training dynamics on CoT/scratchpad objectives.\n2\nPreliminaries\nIn this section, we describe the modified in-context learning linear regression task, i.e. in-context\nweight prediction, the one-layer linear self-attention architecture, and the Chain of Thought (CoT)\nprompting formulation.\nNotation\nWe use [T] to denote the set {1, 2, ..., T}. Scalars are in lower-case unbolded letters\n(y, α, etc.). Matrices and vectors are denoted in upper-case bold letters (W , V , etc.) and lower-\ncase bold letters (x, w, etc.), respectively. W[i,j], W[i,:], W[:,j] respectively denotes the (i, j)-th\nentry, i-th row, and j-th column of the matrix W . W[:,−1] means the last column of the matrix\nW . The notation Wij denotes block matrices/vectors on the i-th row and j-th column according\nto context. For norm, ∥·∥denotes ℓ2 norm and ∥· ∥F denotes the Frobenius norm. We use 1{·} to\ndenote the indicator function. We use ˜O(·) to hide logarithmic factors in the asymptotic notations.\n2.1\nIn-Context Weight Prediction\nPrevious works [58, 2, 3, 4, 32] focus on the in-context learning (ICL) task on linear regression.\nWe suppose the data sequence is sampled from a linear regression task where the ground-truth\nw∗∼N(0, Id)\nxi ∼N(0, Id)\nyi = w∗⊤xi for all i ∈[n].\n(1)\nThe goal of in-context learning is to predict the correct label w∗⊤xquery given a query xquery and\nthe previous example pairs (xi, yi). Most previous works [58, 3, 32] show the transformer predicts\nthe query label yquery by implicitly doing a one-step gradient descent without predicting the linear\nclassifier w∗.\nIn this work, we go one step further: instead of directly outputting the query label, we require the\ntransformers to implement gradient descent to learn the ground-truth weight vector w∗. We call\nthis task in-context weight prediction for linear regression. Specifically, the data sequence is in\n4\n\n\nthe following format:\nZ0 =\n\n\nx1\n· · ·\nxn\n0\ny1\n· · ·\nyn\n0\n0\n· · ·\n0\nw0\n0\n· · ·\n0\n1\n\n:=\n\n\nX\n0\ny\n0\n0d×n\nw0\n01×n\n1\n\n∈Rde×(n+1),\n(2)\nwhere X := [x1, · · · , xn] is the data matrix and w0 is the initialization of the linear parameter ˆw.\nWe assume w0 = 0d for simplicity, and define de = 2d + 2. Our setting is similar to the setting\nin Bai et al. [7] where multi-layer transformers are constructed to do explicit multi-step GD on the\nweight vector ˆw. We separate the input example space and the weight vector space as in Bai et al.\n[7] (the {pi}i∈[N+1]) in order to facilitate training. Moreover, we add a dummy token (an extra 1)\nat the end of each token similar to what Bai et al. [7] did in their input sequence format.\n2.2\nLinear Self-attention Layer\nWe consider a one-layer linear self-attention (LSA) module with residual connection, following\nthe setting in Zhang et al. [58], Ahn et al. [2], Gatmiry et al. [20]: we remove the softmax(·) non-\nlinearity, consolidate the projection and value matrix into a single matrix V ∈Rde×de, and merge\nthe key and query matrices into W ∈Rde×de. We denote\nfLSA(Z; V , W ) = Z + V Z · Z⊤W Z\nn\n(3)\nThe prediction of the transformer will be the last token of the output sequence, namely\nfLSA(Z; V , W )[:,−1] = Z[:,−1] + V Z · Z⊤W Z[:,−1]\nn\n(4)\nSince the first (d+1) entries of the full weight tokens (0, 0, w, 1) are zero, only part of the W and\nV affect the prediction. We can rewrite the parameter V , W into block matrices\nV =\n\n\nV11\nV12\nV13\nV14\nV21\nv22\nV23\nv24\nV31\nV32\nV33\nV34\nV41\nv42\nV43\nv44\n\n, W =\n\n\nW11\nW12\nW13\nW14\nW21\nw22\nW23\nw24\nW31\nW32\nW33\nW34\nW41\nw42\nW43\nw44\n\n∈R(2d+2)×(2d+2)\nwhere the block matrices are in the following shape (i, j ∈{1, 2}):\nV2i−1,2j−1, W2i−1,2j−1 ∈Rd×d; V2i−1,2j, W2i−1,2j, V ⊤\n2i,2j−1, W ⊤\n2i,2j−1 ∈Rd×1; v2i,2j, w2i,2j ∈R.\nIn the following sections, we will show only V31, W13, and w24 affects the prediction. We will\nfurther prove that all other entries are always zero along the training trajectory if initialized at zero.\n2.3\nChain-of-Thought Prompting\nIn language modeling tasks, transformers have been proven to be versatile in various downstream\ntasks. However, transformers struggle to solve mathematical or scientific problems with one single\n5\n\n\ngeneration, where several reasoning steps are required. CoT was then proposed to make transform-\ners learn to generate intermediate results auto-regressively before reaching the answer.\nWith CoT, we allow the transformer to generate k steps before it outputs the final prediction ˆwk\nfor the ground-truth w∗. Specifically, given the generated input sequence ˆZi at the i-th step of\ngeneration, we have fLSA( ˆZi)[:,−1] as the prediction of the next token ((i+1)-th token), and append\nit to the end of the current sequence s.t. ˆZi+1 =\nh\nˆZi, fLSA( ˆZi)[:,−1]\ni\n. After k generation steps, the\nCoT process induces k intermediate sequences { ˆZi}k\ni=1 in the following form:\nˆZi =\n\n\nx1\n· · ·\nxn\n0\n⋆\n· · ·\n⋆\ny1\n· · ·\nyn\n0\n⋆\n· · ·\n⋆\n0\n· · ·\n0\nw0\nˆw1\n· · ·\nˆwi\n0\n· · ·\n0\n1\n1\n· · ·\n1\n\n∈Rde×(n+i+1), i ∈[k]\n(Inference)\nHere, we define ˆwi := fLSA( ˆZi−1)[d+2:2d+1,−1] as the i-th step prediction for the weight vector. The\nother entries in the same column are irrelevant and we denote them as ⋆. Finally, the transformer\ninputs the last generated sequence ˆZk back to the transformer once again to generate the final\noutput ˆwk+1 := fLSA( ˆZk)[d+2:2d+1,−1] as the prediction of the weight vector w∗.\nDifferent from the inference time generation, the training process is similar to pre-training on the\nground-truth sequence to predict the next token. Specifically, we input the transformer with CoT\nground-truth sequences Zi:\nZi =\n\n\nx1\n· · ·\nxn\n0\n0\n· · ·\n0\ny1\n· · ·\nyn\n0\n0\n· · ·\n0\n0\n· · ·\n0\nw0\nw1\n· · ·\nwi\n0\n· · ·\n0\n1\n1\n· · ·\n1\n\n∈Rde×(n+i+1), i ∈[k]\n(Training)\nwhere wi = wi−1−η· X(X⊤wi−1−y⊤)\nn\nis the ground-truth intermediate weight vector after i gradient\nsteps on the linear regression objective. Each gradient step adopts a fixed learning rate η for all\npossible training instances {X, w} when generating the ground-truth sequence Zi. Note that Zi\nis the corresponding ground-truth sequence of ˆZi.\nIn the training objective for the i-th step, the transformer is required to predict the next token\nZi+1[:,−1] := (0d, 0, wi+1, 1) given the i-th ground-truth intermediate sequence Zi. Finally, we\npredict the final ground-truth weight vector w∗with the final intermediate sequence Zk. The CoT\ntraining objective given a sample prompt X, y then becomes:\nℓCoT(X, w∗; V , W ) = 1\n2\nk\nX\ni=0\n\r\rfLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\r\r2\n(5)\nHere we denote wk+1 := w∗for clarity. Following Zhang et al. [58], Nichani et al. [36], Kim &\nSuzuki [26], Tian et al. [47], Chen et al. [10], Gatmiry et al. [20], we consider the gradient flow\ndynamics over the population loss of the CoT objective:\nLCoT(V , W ) = Exi∼N(0,Id),w∗∼N(0,Id)\n\u0002\nℓCoT(X, w∗; V , W )\n\u0003\n(6)\n6\n\n\nFor clarity, we write the expectation as EX,w∗[·]. The following differential equation gives the\ngradient flow dynamics of the parameters:\ndθ\ndt = −∇LCoT(θ),\nθ := (V , W ).\nWhen measuring the performance after training, we apply the CoT inference procedure to generate\nk intermediate sequences { ˆZi}k\ni=1 and consider the final output token f( ˆZk)[:,−1] by inputting the\nlast generated sequence ˆZk. The performance evaluation is measured on the error between the final\noutput f( ˆZk)[:,−1] and the ground-truth w∗:\nLEval(V , W ) = 1\n2EX,w∗\n\u0014\r\r\rfLSA( ˆZk)[:,−1] −(0d, 0, w∗, 1)\n\r\r\r\n2\u0015\n(7)\nWhen CoT prompting is not used (k = 0), the evaluation loss LEval is equivalent to LCoT.\n3\nExpressiveness Improvement with Chain of Thought\nIn this section, we theoretically explore the performance gap on our data model between transform-\ners with CoT and those without. We first prove that a one-layer transformer without CoT can only\nimplement a one-step GD and cannot recover the ground-truth, while it can near-exactly predict\nthe ground-truth parameter with CoT by implementing multi-step GD.\n3.1\nOne-layer Transformer cannot recover ground-truth\nFor the ICL linear regression task, the optimal prediction given by a one-layer linear transformer is\nequivalent to a single step of GD on the MSE objective of linear regression [32]. What about our\ntask on predicting the ground-truth weight vector w∗in context? The following theorem proves\nthat the optimal solution is still a one-step GD solution.\nTheorem 3.1 (Lower bound without CoT). If the global minimizer of LEval(V , W ) is (V ∗, W ∗),\nthe corresponding one-layer transformer fLSA(Z0)[:,−1] implements one step GD on a linear model\nwith some learning rate η∗=\nn\nn+d+1 and the transformer outputs (0d, 0, η∗\nn Xy⊤, 1).\nWe briefly present the high-level intuitions in the proof and the detailed proof is deferred to Ap-\npendix B.1. We use a similar technique in Mahankali et al. [32] when proving the optimality\nof one-step GD in the ICL task. The key strategy of the proof is to replace (0d, 0, w∗, 1) in the\nevaluation loss LEval(V , W ) (Equation (7)) with (0d, 0, η∗\nn Xy⊤, 1) in the following form.\nLEval(V , W ) = 1\n2E\n\"\r\r\r\rfLSA(Z0)[:,−1] −\n\u0012\n0d, 0, η∗\nn Xy⊤, 1\n\u0013\r\r\r\r\n2#\n+ C\nIn order to prove this equation above, we show the gradient of the original loss Equation (7) and\nthis formula are identical. We first obtain the closed-form formula of the expected gradient for\nboth sides with regard to X, w∗. Then we use the symmetric property of the distribution of X, w∗\nto simplify the gradient expressions, and eventually prove them equal.\n7\n\n\nThe equivalent form of loss indicates that the evaluation loss only depends on the ℓ2 distance\nbetween the output of the linear self-attention module and\n\u00000d, 0, η∗\nn Xy⊤, 1\n\u0001\n. Therefore, any\n(V , W ) is a global minimizer of this loss function if and only if the output of fLSA(Zk)[:,−1] is\n(0d, 0, η∗\nn Xy⊤, 1). Meanwhile, one can assign\nV ∗=\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n−η∗I\n0\n0\n0\n0\n0\n0\n0\n\n, W ∗=\n\n\n0\n0\nI\n0\n0\n0\n0\n−1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n(8)\nand the one-layer transformer achieves the optimal solution, which concludes the proof.\nIs a one-step gradient solution good enough? Most of the previous ICL work Zhang et al. [58], Ahn\net al. [2], Gatmiry et al. [20] consider the number of examples n →+∞when d is fixed. In this\ncase, the one-step GD solution can perfectly find the ground-truth weight vector w∗. However,\na simple corollary of this theorem indicates that the one-step solution has a non-negligible error\nwhen there are limited samples, e.g. n = eΘ(d). This number of examples n is required to guarantee\nthe reconstruction of w∗∈Rd.\nCorollary 3.1. For any parameters (V , W ) in the one-layer transformer, LEval(V , W ) ≥Θ\n\u0010\nd2\nn\n\u0011\n.\nMoreover, if n = ˜Θ(d), LEval(V , W ) = ˜Θ(d)\nd→+∞\n−−−−→+∞.\nProof. By Theorem 3.1, we directly calculate the evaluation loss on the global optimum:\nLEval(V , W ) ≥1\n2EX,w∗\n\r\r\r\r\nη∗\nn XX⊤w∗−w∗\n\r\r\r\r\n2\n= 1\n2EX tr\n\u0012\nI −η∗\nn XX⊤\n\u00132\nsince Ew∗\u0002\nw∗w∗⊤\u0003\n= I. Apply E\n\u0002\nXX⊤\u0003\n= nI and E\n\u0002\n(XX⊤)2\u0003\n= n(n + d + 1)I,\n1\n2EX tr\n\u0012\nI −η∗\nn XX⊤\n\u00132\n= 1\n2\n\u0012\nd −2η∗d + η∗2\nn (n + d + 1)d\n\u0013\n= Θ\n\u0012d2\nn\n\u0013\nand we finish the proof by substituting n with ˜Θ(d).\n3.2\nOne-layer Transformer with CoT Can Implement Multi-step GD\nThe previous subsection shows that the one-step solution by the one-layer transformer without\nCoT is not the endgame. Nevertheless, CoT can become the savior for this simple transformer\nbecause it enables the transformer to generate several intermediate computation steps to improve\nthe final performance. The following theorem shows that with the reinforcement of CoT, there\nexists a one-layer transformer that can perform multi-step GD using intermediate generations. We\nshow that Θ(log d) steps of CoT can remarkably improve the performance, reducing the error from\nΘ(\nd\npoly log d) to O(1/ poly d). With constant learning rate, Θ(log d) steps of GD is also necessary to\nreconstruct w∗accurately. The proof is deferred to Appendix B.2.\n8\n\n\nTheorem 3.2 (Informal). There exists V ∗and W ∗s.t. fLSA(Zk)[:,−1] outputs (0d, 0, wk, 1) where\nwk :=\n\u0000I −(I −η\nnXX⊤)k\u0001\nw∗is the k-step GD solution with learning rate η on a linear regres-\nsion model. Moreover, if n = ˜Ω(d), k = Ω(log d), η ∈(0.1, 1), then the evaluation loss\nLEval(V ∗, W ∗) = 1\n2EX,w∗\n\"\r\r\r\r\n\u0010\nI −η\nnXX⊤\u0011k+1\nw∗\n\r\r\r\r\n2#\n≤O\n\u0012\n1\npoly(d)\n\u0013\n(9)\nWith the one-step GD solution in Theorem 3.1, the proof is straightforward: we assign the pa-\nrameters (V , W ) in the same form of Equation (8), with the η∗replaced by η. However, now the\ntransformer is allowed to generate k steps before reaching the final output. We can inductively\ncalculate the i-th step of generation, showing that the output is exactly the i-th gradient step:\nfLSA(Zi−1)[:,−1] = (0d, 0, wi, 1),\ni = 1, 2, ..., k + 1\nAfter k+1 steps, we have the final output\n\u0000I −(I −η\nnXX⊤)k+1\u0001\nw∗by induction and the evalu-\nation loss becomes Equation (9). By Lemma D.4, the final loss is upper bounded by O\n\u0010\n1\npoly(d)\n\u0011\n.\nThis is strictly better than a one-step GD solution by comparing with Corollary 3.1.\nNow we theoretically display the expressivity improvement of transformers brought by CoT. In\nthe following sections, we will further prove that this separation is learnable simply by gradient\nflow.\n4\nGradient Dynamics over Chain of Thought\nIn this section, we go beyond the construction and prove our convergence result on the CoT ob-\njective. We show that the final solution found by gradient flow is approximately our construction\nin Theorem 3.2, which is significantly better than the one-step gradient descent solution without\nCoT.\n4.1\nMain Results\nAccording to our construction in Theorem 3.2, we use the following specific initialization to zero\nout the irrelevant blocks while keeping the essential blocks W13, V31, and w24.\nAssumption 4.1 (Initialization). Let σ > 0 be a parameter. We assume the initialization of the\nparameters satisfies that\nV =\n\n\n0\n0\n0\n0\n0\n0\n0\n0\nV31(0)\n0\n0\n0\n0\n0\n0\n0\n\n, W =\n\n\n0\n0\nW13(0)\n0\n0\n0\n0\nw24\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nHere W13(0) = Pd\ni=1 λW\ni uiu⊤\ni and V31(0) = Pd\ni=1 λV\ni uiu⊤\ni are symmetric and simultaneously\ndiagonalizable, λV\ni ≤−σ, λW\ni\n∈[σ, 1\n2]. Further, we fix w24 = −1 for all t > 0.\nThis initialization follows Chen et al. [10] by assuming V31 and W13 share the same set of eigen-\nvectors. It is close to the particular symmetric random initialization schemes discussed in Zhang\n9\n\n\net al. [58] with a scaling factor σ. We use this specific initialization to zero out the irrelevant\nblocks, and we fix w24 = −1 to break the homogeneity of the model and avoid multiple global\nminimizers. Those simplification facilitates the analysis of the complex dynamical system.\nNow we prove that under appropriate initialization, gradient flow will nearly converge to the global\nminimizer. We provide a proof sketch in the next subsection. See Appendix C.3 for details.\nTheorem 4.1 (Informal, Global Convergence). Suppose n = ˜Ω(d), η ∈(0.1, 0.9), k = Θ(log d).\nUnder Assumption 4.1 with σ = Θ(1), if we run gradient flow on the population loss in Equa-\ntion (5), then after time t = O\n\u0000log d + log 1\nϵ\n\u0001\n, we have LCoT(t) ≤ϵ for any ϵ ∈\n\u0010\n1\npoly(d), 1\n\u0011\n.\n4.2\nProof Ideas\nIn this subsection, we briefly outline the proof of Theorem 4.1.\nBefore analyzing the dynamics, we first prove that under Assumption 4.1, the gradient dynamics\nonly depend on the parameter blocks W13(t), V31(t), w24, while other blocks stay zero (Lemma C.2).\nThis is because the Gaussian data assumption makes sure the gradients of these blocks are zero\nonce initialized at zero, except for W13(t), V31(t), w24. By this lemma, we can simplify the lin-\near self-attention formula and consider the following equivalent yet simplified loss (we denote\nf\nW := W13, eV := V31, and w24 is fixed as −1.):\nLCoT(θ) = 1\n2EX,w∗\nk−1\nX\ni=0\n\r\r\r\r\n1\nn( eV XX⊤f\nW + ηXX⊤)wi −1\nn( eV + ηI)XX⊤w∗\n\r\r\r\r\n2\n2\n+ 1\n2EX,w∗\n\r\r\r\r\n\u0012\nI + 1\nn\neV XX⊤f\nW\n\u0013\nwk −\n\u00121\nn\neV XX⊤+ I\n\u0013\nw∗\n\r\r\r\r\n2\n2\nFor ease of presentation, we denote S :=\n1\nnXX⊤. To analyze the gradient dynamics, we first\nneed to compute the exact closed-form gradient instead of keeping the expectation. However, the\nformula involves the i-th step weight vector wi =\n\u0010\nI −(I −ηS)i\u0011\nw∗, involving the higher order\nmoments of the Wishart matrix S1 whose closed form is hard to obtain. In our paper, we provide\na tighter estimate compared to previous work [20] using the concentration of the Wishart matrix S\n[50] when n = Θ(d poly log d) to estimate the expectation. In particular, we use the exponential\ndecaying tail probability bound for the operator norm of the error δS := S −I. For example,\nwhen estimating the expectation E[(I −ηS)i], we can decompose the expectation into two cases:\nwhen ∥δS∥op is small, (I −ηS)i ≈(1 −η)iI; when ∥δS∥is larger than a threshold, the rest part\nof the expectation can be controlled by integrating the exponential decaying tail probability.2 The\nconcentration lemmas are provided in Appendix D.\nThe motivation behind a better concentration estimation is to ensure nearly independent dynamics\nalong different eigenspaces {ui}d\ni=1 of f\nW and eV . As an extreme case, we consider n →∞\n1To deal with the similar problem, Gatmiry et al. [20] proposed a simple combinatorial method to estimate the\nexpectation. We use the same technique to get a certain form of the expectation (see Appendix D), but the bound is\nnot tight enough to get the desired results. See discussion in Appendix A.2.\n2This method can keep the (1 −η)i factor to prevent introducing unwanted estimation errors when i is large.\n10\n\n\nand S converges to I almost surely. Now the gradient component on the uiu⊤\ni subspace is only\ndependent on λ eV\ni\nand λf\nW\ni\nwithout any other λ eV\nj , λf\nW\nj , j ̸= i involved. That means there is no\ninteraction between two different subspaces, i.e. the dynamics are independent. However, some\ninteractions are introduced since the concentration error δS ̸= 0 when n is finite. Therefore,\nthe improved characterization of the expected gradient is essential to upper bound the interaction\nbetween the dynamics of different eigenspaces {ui}d\ni=1, leading to a nearly independent evolution\nat initialization. This independence property motivates us to conduct the following stage-wise\nanalysis:\nStage 1: f\nW , eV converges to near-optimal.\nIn this stage, the dynamics along each direction\nui stay nearly independent.\nSpecifically, we can expand the gradient flow dynamics for eV ,\nf\nW and project them into the eigenspaces uiu⊤\ni to get the dynamics of the eigenvalues λ eV\ni\n:=\nu⊤\ni eV ui, λf\nW\ni\n:= u⊤\ni f\nW ui.\nThe dynamics of eigenvalues are characterized by the following\nLemma 4.1 where we can prove that the interaction terms between different subspaces are bounded\nby O(1/ log2 d).\nLemma 4.1 (Informal version of Lemma C.6). The dynamics of λ eV\ni\nand λf\nW\ni\nare given by the\nfollowing equations with\n\f\f\fδ eV\nj\n\f\f\f ≤O\n\u0010\n1\nlog2 d\n\u0011\n,\n\f\f\fδ f\nW\nj\n\f\f\f ≤O\n\u0010\n1\nlog2 d\n\u0011\n:\ndλ eV\nj\ndt\n= −\n\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\nλf\nW\nj\n2\nη(2 −η)\n\nλ\neV\nj + 1 −η\n2 −ηλ\nf\nW\nj\n−1 + δ\neV\nj\ndλf\nW\nj\ndt\n=\n\u0014\nk + 1 −1\nη\n\u0015\nλ\neV\nj\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)λ\neV\nj\n2\nλ\nf\nW\nj\n+ 1 −η\n2 −ηλ\neV\nj −δ\nf\nW\nj .\nThis nearly independent evolution along each eigenvector ui enables us to analyze the individual\ndynamics of λ eV\ni\nand λf\nW\ni\nat the beginning of training. Under Assumption 4.1, λ eV\nj , λf\nW\nj\nare ini-\ntialized Θ(1). By Lemma 4.1, we prove by induction that the eigenvalues will go through two\nphases: (1) λ eV\nj increases yet stay smaller than −O\n\u0012\n1\nk(1−λ f\nW\nj\n)\n\u0013\n, while λf\nW\nj\nincreases to 1−o(1). (2)\nλf\nW\nj\nstays o(1)-close to 1, and λ eV\nj also converges to o(1)-close to −η. Here all o(1) terms are some\nO(1/ logc d) terms for some constant c > 0. That implies that the distance between the eigenvalues\nand the target\n\f\f\fλ eV\nj + η\n\f\f\f,\n\f\f\fλf\nW\nj\n−1\n\f\f\f converge to O(1/ logc d) for j ∈[d] at the end of Stage 1.\nStage 2: Local convergence.\nOne may expect that after Stage 1, the transformer can approximate\ngradient steps quite accurately since the parameter eV , f\nW are both o(1)-close to ground-truth along\neach direction ui. Unfortunately, the sum of error in d directions can still be eΘ(d) since we can\nonly reduce the error to O(1/ poly log d) in each direction. So for now, the solution cannot recover\nthe weight vector w∗at this stage. To address this issue, we further consider the exact form of the\ninteraction terms δ f\nW\nj , δ eV\nj and analyze the local convergence. By fine-grained expansion of the error\nterms, we notice that δ f\nW\nj\nand δ eV\nj are always coupled with some individual residual like (1 −λf\nW\nj ),\n11\n\n\n(η + λ eV\nj ), or some weighted average or those individual residuals. Meanwhile, the coefficient of\nthe residual in the interaction terms is still upper bounded by O(1/ poly log d). That enables us to\nderive some gradient lower bound similar to PL-conditions (Lemma C.12) when eV , f\nW are close\nto the ground-truth, leading to local convergence to near-optimal at a linear rate.\nThe final training error is some O(\n1\npoly d), which depends on the inference step k and ground-truth\nη. Note that the optimal loss value is also at least polynomially small in d given Θ(log d) CoT\nsteps. Therefore, now we can conclude that the transformer can learn to implement multi-step GD\nwhen given intermediate ground-truth states after optimizing the CoT loss with gradient flow.\n4.3\nOut-of-distribution Generalization at Inference\nIn this section, we prove that after training, the transformer not only correctly predicts the weight\nvector in context with CoT generation, but also can generalize out-of-distribution (OOD). The\nfollowing theorem shows that the trained transformer obtained from Theorem 4.1 with CoT gener-\nalizes over other problem instances when the input example sequence has an OOD covariance, as\nlong as the covariance is not too ill-conditioned. Here LEval\nΣ\nis defined as the OOD evaluation loss\nin eq. (7) with the in-context examples xi ∼N(0, Σ) and weight vector w∗∼N(0, I):\nLEval\nΣ\n(V , W ) = 1\n2Exi∼N(0,Σ),w∗\n\u0014\r\r\rfLSA( ˆZk)[:,−1] −(0d, 0, w∗, 1)\n\r\r\r\n2\u0015\nTheorem 4.2 (Informal, Theorem C.2). Suppose n = ˜Ω(d), η ∈(0.1, 0.9), k′ = Θ(log d). Assume\nthe out-of-distribution covariance is well-conditioned: δ\nη ≤λmin(Σ) ≤λmax(Σ) ≤2−δ\nη\nfor some\nconstant δ > 0. Then after training in Theorem 4.1, we have LEval\nΣ\n(t) ≤ϵ for any ϵ ∈\n\u0010\n1\npoly(d), 1\n\u0011\n.\nNote that this theorem covers both in-distribution (when η = δ) and OOD tasks at evaluation,\nindicating that the transformer is trained to implement a general iterative optimization algorithm.\nMoreover, the inference step number k′ in this theorem can go beyond the training CoT steps k,\nachieving better estimation for w∗.\nOne may think once the next-token-prediction training loss LCoT converges to zero based on\nground-truth CoT data, the transformer naturally learns to do multi-step reasoning at inference,\ni.e. LEval is small. However, at the i-th generation step, the transformer is predicting the next\nweight token ˆwi+1 based on the previous generation ˆwi instead of the ground-truth intermediate\nstep wi. It is possible that prediction error for each step accumulates or even increases exponen-\ntially. In this theorem, we expand the sum of all the prediction errors at each step and show a\nconverging series of errors throughout the inference process. That ensures we can achieve any\nO(\n1\npoly(d))-small evaluation loss when we have k′ = Θ(log d) reasoning steps. The detailed proof\nis provided in Appendix C.4.\n4.4\nImprovement for the Looped Transformer\nIn this section, we demonstrate that our proof technique improves the optimization result in Gat-\nmiry et al. [20] for looped transformers within the linear regression in-context learning setting. For\ndetails on the linear regression ICL setup, the analysis of training dynamics, and further discussion,\nplease refer to Appendix A.3.\n12\n\n\nNotably, the looped transformer in [20] that implements multi-step gradient descent is shown to\nbe no better than the transformer performing a single gradient descent step. Specifically, Theorem\n4.2 in Gatmiry et al. [20] requires a lower bound on the final loss as d5/2L·4L\n√n\n, where L denotes\nthe number of loops. In contrast, a one-layer transformer without looping, as presented in Ma-\nhankali et al. [32], can achieve a loss of Θ(d2/n) by executing one gradient descent step, which is\nasymptotically better compared to the multi-step approach of [20] for all choices of n, d, and L.\nThe following theorem shows that our techniques enable looped transformers to outperform their\nnon-looped counterparts in the ICL setting described in Gatmiry et al. [20].3\nTheorem 4.3 (Informal, Theorem A.1). Suppose n = ˜Ω(d), L = Θ(log d) and ∥A(0)∥=\nO(1).\nSuppose we run the gradient flow with respect to the loss L(A).\nThen for any ξ >\nΘ\n\u0012\u0010\nL2d log2 d\nn\n\u0011L\u0013\n, after time t ≥Ω\n\u0010\n1\nL2(d\nξ)\nL−1\nL\n\u0011\n, we have L(A(t)) ≤ξ.\nThe theorem implies that as long as the number of loops satisfies L = Ω(log d), the global min-\nimizer of the looped transformer can achieve an arbitrarily small loss that is polynomial in d.\nMoreover, the asymptotic loss is strictly better than that of the non-looped one-layer transformer\nwhen L ≥2, thus establishing the separation between looped and non-looped transformers.\n5\nExperiments\nIn this section, we introduce our experimental setup on our in-context weight vector prediction\ntask to numerically validate our theoretical results. Specifically, we show that parameters of the\ntransformer match the prediction of our theory when optimized over the CoT loss. Furthermore,\nwe present the gap of evaluation loss LEval in eq. (7) between transformers with and without CoT.\nExperimental Setup\nWe train the transformer architecture in Equation (3) on the synthetic data.\nThe data distribution follows our in-context weight prediction task in Equation (1). In particular,\nwe choose the token dimensions d = 10, number of in-context examples n = 20, and GD learning\nrate η = 0.4 for generating the ground-truth intermediate states. We use a batch size B = 1000 and\nrun Adam with learning rate α = 0.001 for τ = 750 iterations. More details refer to Appendix E.\nGlobal convergence\nOur experiments show that the structure that weights of the full model\nexhibit is consistent with Theorem 3.2. At final convergence, all of the entries of W converge to\nzero except the elements on the diagonal in the top-right corner block (the red box in the heatmap\nof W , Figure 1), while all the entries of V are near zero except elements on the diagonal in\nthe bottom-left corner (the red box in the heatmap of V , Figure 1). Also, the pattern shows\nW13 = αI, w24 = −α, and V31 = −η\nαI with some scaling factor α,4 which is equivalent to the\nconstruction stated in Theorem 3.2 and Theorem 4.1. That means the transformer implements one\n3Our focus here is on the simplified setting with Σ = I, as in Gatmiry et al. [20], though our approach readily\nextends to other covariance matrices.\n4In Figure 1, α > 0 while all α ̸= 0 works for the construction. Empirically, the sign of α depends on the random\ninitialization, and both positive and negative solutions exist.\n13\n\n\nstep of gradient descent\n\u00000d, 0, −η\nnXX⊤(wi −w∗), 0\n\u0001\nbefore the residual connection, and the\nautoregressive CoT process enables model to perform multi-step GD.\nPerformance improvement\nWe empirically verify the evaluation loss gap between transform-\ners with and without CoT shown by Theorem 3.1 and Theorem 3.2. Our experiments in Figure 2\ndemonstrate that the evaluation loss of transformers with CoT converges to near zero even when\nk = 10. In comparison, the optimal expected loss that the one-layer linear transformer can achieve\n(the pink dashed line, from Corollary 3.1) is much larger than any of the model that applies mul-\ntiple steps of computation. We also observe that evaluation loss at convergence keeps decreasing\nwhen the number of reasoning steps k increases from 10 to 40, which is consistent with Theo-\nrem C.1 where larger k allows for smaller error ϵ.\nHeatmap of V\nHeatmap of W\nFigure 1:\nModel weights:\nWe present the\nheatmap of the weights of the trained trans-\nformer. We initialize V , W randomly at t = 0,\nwhere n = 20, d = 10 and k = 20. After train-\ning, all entries of V and W converge to zero ex-\ncept the two blocks highlighted in the red box.\nMoreover, the pattern matches the theoretical re-\nsults.\n0\n100\n200\n300\n400\n500\n600\n700\niteration\n0\n1\n2\n3\n4\n5\nEval Loss\nk=10\nk=20\nk=30\nk=40\nwithout CoT\nFigure 2: k-step v.s. 1-step: We plot the\nevaluation loss LEval when n = 20, d = 10.\nWe randomly initialize the transformer. For\ntransformers with CoT, loss converges to\nnear zero while transformers without CoT\ncannot. Moreover, the loss at convergence\ndecreases when k increases.\n6\nConclusion\nThis paper investigates the training dynamics of transformers when the Chain of Thought (CoT)\nprompting is introduced. By focusing on the in-context weight prediction task, our theoretical\nresults demonstrate that transformers can learn to implement iterative algorithms like multi-step\nGD with the enhancement of CoT, highlighting the essential role of CoT in multi-step reasoning\ntasks. Our empirical findings corroborate these theoretical insights, indicating that CoT prompting\nprovides significant performance benefits.\nThere are still many open problems. Can we move beyond population loss on the in-context weight\nprediction task and show a sample complexity guarantee? Can CoT empower the transformer to\nacquire compositional reasoning capability instead of doing the same iterative steps?\n14\n\n\nAcknowledgement\nJDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR\nYoung Investigator Award, and NSF CAREER Award 2144994.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[2] Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra.\nLinear attention is (maybe) all you need (to understand transformer optimization). arXiv\npreprint arXiv:2310.01082, 2023.\n[3] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to\nimplement preconditioned gradient descent for in-context learning. Advances in Neural In-\nformation Processing Systems, 36, 2024.\n[4] Ekin Aky¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.\nWhat\nlearning algorithm is in-context learning? investigations with linear models. arXiv preprint\narXiv:2211.15661, 2022.\n[5] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ra-\nmasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.\nExploring\nlength generalization in large language models. Advances in Neural Information Process-\ning Systems, 35:38546–38556, 2022.\n[6] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin\ntoken selection in attention mechanism. Advances in Neural Information Processing Systems,\n36:48314–48362, 2023.\n[7] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statis-\nticians: Provable in-context learning with in-context algorithm selection.\narXiv preprint\narXiv:2306.04637, 2023.\n[8] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth\nof a transformer: A memory viewpoint. Advances in Neural Information Processing Systems,\n36, 2024.\n[9] Vivien Cabannes, Charles Arnal, Wassim Bouaziz, Alice Yang, Francois Charton, and Ju-\nlia Kempe.\nIteration head: A mechanistic study of chain-of-thought.\narXiv preprint\narXiv:2406.02128, 2024.\n[10] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-\nhead softmax attention for in-context learning: Emergence, convergence, and optimality.\narXiv preprint arXiv:2402.19442, 2024.\n15\n\n\n[11] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang.\nUnveiling induction\nheads: Provable training dynamics and feature learning in transformers.\narXiv preprint\narXiv:2409.10559, 2024.\n[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.\nPalm: Scaling language modeling with pathways. Journal of Machine Learning Research,\n24(240):1–113, 2023.\n[13] Antonia Creswell, Murray Shanahan, and Irina Higgins.\nSelection-inference: Exploiting\nlarge language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712,\n2022.\n[14] Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, and Radu Soricut. Causallm is\nnot optimal for in-context learning. arXiv preprint arXiv:2308.06912, 2023.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\n[16] Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The\nevolution of statistical induction heads: In-context learning markov chains. arXiv preprint\narXiv:2402.11004, 2024.\n[17] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards\nrevealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural\nInformation Processing Systems, 36, 2024.\n[18] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order\noptimization methods for in-context learning: A study with linear models. arXiv preprint\narXiv:2310.17086, 2023.\n[19] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers\nlearn in-context? a case study of simple function classes. Advances in Neural Information\nProcessing Systems, 35:30583–30598, 2022.\n[20] Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, and Sanjiv Ku-\nmar. Can looped transformers learn to implement multi-step gradient descent for in-context\nlearning?\nIn Forty-first International Conference on Machine Learning, 2024.\nURL\nhttps://openreview.net/forum?id=o8AaRKbP9K.\n[21] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dim-\nitris Papailiopoulos.\nLooped transformers as programmable computers.\narXiv preprint\narXiv:2301.13196, 2023.\n[22] Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, An-\ntoine Bosselut, and Mrinmaya Sachan. Towards a mechanistic interpretation of multi-step\nreasoning capabilities of language models. arXiv preprint arXiv:2310.14491, 2023.\n16\n\n\n[23] Xinyang Hu, Fengzhuo Zhang, Siyu Chen, and Zhuoran Yang. Unveiling the statistical foun-\ndations of chain-of-thought prompting methods. arXiv preprint arXiv:2408.14511, 2024.\n[24] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv\npreprint arXiv:2310.05249, 2023.\n[25] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial\nstructure. Advances in Neural Information Processing Systems, 35:37822–37836, 2022.\n[26] Juno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex\nmean-field dynamics on the attention landscape. arXiv preprint arXiv:2402.01258, 2024.\n[27] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. Advances in neural information processing\nsystems, 35:22199–22213, 2022.\n[28] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen.\nA theoretical understanding of\nshallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint\narXiv:2302.06015, 2023.\n[29] Yingcong Li, Yixiao Huang, Muhammed E Ildiz, Ankit Singh Rawat, and Samet Oymak. Me-\nchanics of next token prediction with self-attention. In International Conference on Artificial\nIntelligence and Statistics, pp. 685–693. PMLR, 2024.\n[30] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transform-\ners to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.\n[31] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Trans-\nformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.\n[32] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is\nprovably the optimal in-context learner with one layer of linear self-attention. arXiv preprint\narXiv:2307.03576, 2023.\n[33] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi,\nHyeji Kim, and Michael Gastpar. Attention with markov: A framework for principled analy-\nsis of transformers via markov chains. arXiv preprint arXiv:2402.04161, 2024.\n[34] William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of\nthought. arXiv preprint arXiv:2310.07923, 2023.\n[35] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision\ntransformers. Transactions of the Association for Computational Linguistics, 11:531–545,\n2023.\n[36] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure\nwith gradient descent. arXiv preprint arXiv:2402.14735, 2024.\n17\n\n\n[37] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show\nyour work: Scratchpads for intermediate computation with language models. arXiv preprint\narXiv:2112.00114, 2021.\n[38] OpenAI. Gpt-4 technical report, 2023.\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK¨opf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\nhigh-performance deep learning library, 2019.\n[40] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert\nWest, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv\npreprint arXiv:2304.01904, 2023.\n[41] Yunwei Ren, Zixuan Wang, and Jason D Lee. Learning and transferring sparse contextual\nbigrams with linear transformers. In The Thirty-eighth Annual Conference on Neural Infor-\nmation Processing Systems, 2024.\n[42] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal\nanalysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.\n[43] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al.\nLanguage models are\nmultilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022.\n[44] Mirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-\nbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,\n2022.\n[45] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Trans-\nformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023.\n[46] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du.\nScan and snap:\nUnder-\nstanding training dynamics and token composition in 1-layer transformer. arXiv preprint\narXiv:2305.16380, 2023.\n[47] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and Snap: Understanding\nTraining Dynamics and Token Composition in 1-layer Transformer, July 2023. URL http:\n//arxiv.org/abs/2305.16380. arXiv:2305.16380 [cs].\n[48] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.\nJoma: De-\nmystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint\narXiv:2310.00535, 2023.\n18\n\n\n[49] Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, and Haitham Bou-Ammar.\nWhy can large language models generate correct chain-of-thoughts?\narXiv preprint\narXiv:2310.13571, 2023.\n[50] Roman Vershynin. High-dimensional probability: An introduction with applications in data\nscience, volume 47. Cambridge university press, 2018.\n[51] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo˜ao Sacramento, Alexander\nMordvintsev, Andrey Zhmoginov, and Max Vladymyrov.\nTransformers learn in-context\nby gradient descent. In International Conference on Machine Learning, pp. 35151–35174.\nPMLR, 2023.\n[52] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan\nSun. Towards understanding chain-of-thought prompting: An empirical study of what mat-\nters. arXiv preprint arXiv:2212.10001, 2022.\n[53] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in lan-\nguage models. arXiv preprint arXiv:2203.11171, 2022.\n[54] Zixuan Wang, Stanley Wei, Daniel Hsu, and Jason D Lee. Transformers provably learn sparse\ntoken selection while fully-connected nets cannot. In Forty-first International Conference on\nMachine Learning, 2024.\n[55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:24824–24837, 2022.\n[56] Skyler Wu, Eric Meng Shen, Charumathi Badrinath, Jiaqi Ma, and Himabindu Lakkaraju.\nAnalyzing chain-of-thought prompting in large language models via gradient-based feature\nattributions. arXiv preprint arXiv:2307.13339, 2023.\n[57] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan.\nTree of thoughts: Deliberate problem solving with large language models.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[58] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models\nin-context. arXiv preprint arXiv:2306.09927, 2023.\n[59] Denny Zhou, Nathanael Sch¨arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables\ncomplex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n19\n\n\nA\nDiscussion and limitation\nA.1\nRelated works on Expressiveness\nOur work is closely related to the previous works in multi-step GD using multi-layer attention\nlayers, including [7, 18, 14, 3, 21, 20]. These works guarantee that transformers are expressive\nenough to do in-context learning by implementing gradient descent, and they serve as the founda-\ntion of our work which focuses on optimization. Most of them focus on the in-context learning\nsetup as the testbed so we naturally follow the setup to understand the advantage of CoT.\nMost of the above works on expressiveness focus on those iterative algorithms, e.g. (pre-conditioned)\ngradient descent on various objectives [7, 3, 14], Newton methods/matrix inverse [21], etc. Those\npapers have similar constructive proof techniques using multi-layer transformers: they construct a\nbasic block(s) to represent one step of some iterative algorithm and stack them up to do multi-steps\nof that algorithm. Sometimes the blocks can be even the same, which means a “looped” trans-\nformer, i.e. implementing the same transformer blocks several times as a loop, can express those\nalgorithms. In our warm-up construction for a better understanding of the setup, we use similar\ntechniques to construct the linear transformer that allows auto-regressive generation to iteratively\nimplement the block. However, we require the practical auto-regressive setting, which is novel in\nthe literature.\nMost importantly, despite the close relation between our work and those previous expressiveness\npapers, our work mainly focuses on the optimization perspective. It is a big step beyond expres-\nsiveness because there is no guarantee that one can algorithmically find the constructed solutions in\nthe previous work. Ahn et al. [3], Gatmiry et al. [20] are the only two papers related to optimiza-\ntion of multi-layer transformers over in-context linear regression setup. Ahn et al. [3] analyzed\nthe global optimizer/critical points for multi-layer transformers, but they didn’t prove that any\ngradient-based algorithm can reach those solutions. Compared to all the works above, our proof\ntechniques for the main theorems are completely orthogonal and not straightforward extensions of\nthe previous papers like Bai et al. [7].\nGatmiry et al. [20] is the most related work to us. They also proved some results on learning to\nimplement multi-step GD by looped transformer. We will highlight the differences and our novel\ncontributions of our work in the next section.\nA.2\nDiscussion on Gatmiry et al. [20]\nIn this section, we compare our work with Gatmiry et al. [20]. We begin by outlining the similar-\nities and connections between the two works before highlighting our theoretical contributions in\ncontrast to Gatmiry et al. [20].\nBoth Gatmiry et al. [20] and our study analyze the dynamics of a one-layer linear transformer in\nthe context of a linear regression task, demonstrating that transformers can implement multi-step\ngradient descent. We adopt similar architectural frameworks to those in Zhang et al. [58], Ahn et al.\n[3, 2], Mahankali et al. [32], as well as several other works. The key connection between our work\nand Gatmiry et al. [20] lies in the observation that both looped transformers and transformers with\nCoT prompting through autoregressive generation are capable of naturally implementing iterative\nalgorithms like gradient descent.\n20\n\n\nHowever, our data model and training objective are intrinsically different from those in Gatmiry\net al. [20], leading to distinct insights. While Gatmiry et al. [20] focuses on an ICL setting for linear\nregression tasks involving examples and a query, our task is centered on predicting the ground-\ntruth weight vector w∗within context, i.e. in-context weight prediction. The final converging\nsolutions are totally different, even though they both are equivalent to some type of GD. From the\nperspective of the training objective, Gatmiry et al. [20] uses a standard squared loss over the ICL\nobjective. In contrast, we use a sum of squared losses across all intermediate steps, corresponding\nto the CoT loss defined in Equation (6). Therefore, we highlight the effectiveness in improving\nthe performance of the CoT prompting on a shallow transformer, while Gatmiry et al. [20] stress\na multi-layer transformer with shared weights (looped transformer) can do multi-step GD through\nthe layers.\nFrom a technical perspective, Gatmiry et al. [20] fix the outer layer and train only the matrix A,\nwhich is analogous to our matrix W . In contrast, our work allows for training both layers of\nthe transformer, providing a stronger analysis of training dynamics. Our proof strategy is also\nnovel, given that our training dynamics are more complicated: obtaining our final solution requires\nsolving a challenging d-dimensional dynamical system, whereas prior work in ICL reduces the\nouter layer to a scalar.\nAs a more profound theoretical contribution, we rigorously establish a clear performance\ngap between the one-layer transformer without CoT and the ones with CoT. Specifically,\nthe one-layer transformer without CoT is restricted to a single step of GD, with the final error\nΘ(d/ poly log d), while a one-layer transformer with CoT can achieve a O(1/ poly d) loss with\nonly Θ(log d) steps. On the other hand, Gatmiry et al. [20] do not show their transformer im-\nplementing the multi-step GD can outperform the transformer with one-step GD. According to\nTheorem 4.2, the looped transformer in their setting can only provably get the final loss down to\nd5/2L·4L\n√n\n, where L is the number of loops. However, a one-layer transformer can achieve Θ(d2/n)\nloss by implementing one-step of GD, asymptotically better than the multi-step solution in\nGatmiry et al. [20].\nA.3\nLooped Transformer Learns to Implement Multi-step Gradient Descent\n(Improved Version)\nIn this section, we also discuss how our estimation technique improves the result in Gatmiry et al.\n[20] for looped transformers. The gap between our analysis lies in our different methods of calcu-\nlating the terms in the gradient concerning Wishart matrices. For intuition, we introduce the novel\nexpectation calculation method in Section 4, which asymptotically improves the estimation of\nhigher moments of Wishart matrices in Gatmiry et al. [20]. We adopt the combinatorial technique\nin Gatmiry et al. [20] to compute the form of E\n\u0002\nSΛSkΓSk′\u0003\n, but when we calculate the expected\ngradient we use the concentration tail bound technique to calculate the expectation.\nWe will demonstrate that applying our techniques proves looped transformers outperform their\ncounterparts without looping in the ICL setting. Here, we follow the notations, models and loss\ndefined in Gatmiry et al. [20], where Σ denotes the task covariance of the Gaussian input xi’s, and\nS denotes the empirical covariance matrix S := 1\nnXX⊤.5 We focus on the simple setting with\n5In [20], the covariance of the input is denoted as Σ∗while the empirical covariance matrix is denoted as Σ. To\n21\n\n\nΣ = I, but the same technique should also be applied to cases with other covariance matrices.\nSettings for linear regression ICL and looped transformer\nTo keep this paper self-contained,\nwe briefly go through the definition of the in-context learning on linear regression task, and the\nlooped transformer architecture that solves the problem.\nIn-context Learning for Linear Regression For the in-context learning task, we consider the\nprompt format\nZ =\n\u0012x1\nx2\n· · ·\nxn\nxquery\ny1\ny2\n· · ·\nyn\n0\n\u0013\n∈R(d+1)×(N+1)\nwhere the data sequence is sampled from a linear regression task where the ground-truth\nw∗∼N(0, Id)\nxi ∼N(0, Σ)\nyi = w∗⊤xi for all i ∈[n].\n(10)\nThe goal of in-context learning is to predict the correct label yquery := w∗⊤xquery given a query\nxquery and the previous example pairs (xi, yi).\nLinear attention and looped transformer Recall the linear self-attention architecture is\nAttnV ,W (Z) = V Z · Z⊤W Z.\n(11)\nThe looped transformer inputs the previous output of the ℓ-th layer to the (ℓ+ 1)-th layer:\nZℓ+1 = Zℓ−1\nnAttnV ,W (Zℓ)\nfor\nℓ= 0, 1, . . . , L −1.\nBy reusing the same set of attention parameters V , W for all layers. That is equivalent to recur-\nsively iterating the same transformer block, so it is called looped transformer.\nFollowing the setting in [20], we parameterize the model as follows:\nW :=\n\u0012A\n0\n0\n0\n\u0013\n,\nV :=\n\u00120d×d\n0\n0\n1\n\u0013\n.\nWe consider the final output of the looped transformer as follows:\nTFL(Z0; V , W ) = −ZL\n(d+1,n+1).\nHere L is the loop number of the transformer. The training objective is\nL(A) = Ew∗,X\nh\u0000TFL(Z0; V , W ) −yquery\n\u00012i\nRemark. In the original paper of Gatmiry et al. [20], they also have a parameter u in the bottom-\nleft block of V . However, it is not used in the optimization result, so we ignore that in this section.\nmaintain consistency in our main results, we adhere to our notation.\n22\n\n\nImproved analysis for looped transformer\nIn the following lemmas, we show an improved\nanalysis for looped transformer. We first list some technical lemmas from Gatmiry et al. [20],\ncomputing the equivalent expression of population loss and the gradient expression.\nLemma A.1 (Corollary A.4 in Gatmiry et al. [20]). The loss for loop Transformer is\nL(A) = E\n\u0014\ntr\n\u0010\nI −A\n1\n2SA\n1\n2\n\u00112L\u0015\n.\nLemma A.2 (Equation (24) in Gatmiry et al. [20]). The derivative of the loss can be written as\n∇AL(A) = −\n2L−1\nX\ni=0\nE\nh\n(I −SA)iS(I −SA)2L−1−ii\n.\nBy using our techniques, we strengthen the conclusion and obtain the following theorem.\nTheorem A.1. Suppose n = Ω(dL2 log2 d) and ∥A(0)∥= O(1). Consider the gradient flow with\nrespect to the loss L(A):\nd\ndtA(t) = −∇AL(A(t)).\nThen, for any ξ > Θ\n\u0012\u0010\nL2d log2 d\nn\n\u0011L\u0013\n, after time t ≥Ω\n\u0012\n1\nL2\n\u0010\nd\nξ\n\u0011 L−1\nL \u0013\n, we have L(A(t)) ≤ξ. In\nparticular, given any polynomially small ξ > Θ(\n1\npoly(d)), there exists L = Θ(log d), the final loss\nL(A(t)) ≤ξ.\nRemark. This result also gets arbitrary polynomially small loss as the case in the CoT setting,\nestablishing the separation between looped transformer and the transformer without loop.\nProof. Denote λi to be the i-th eigenvalue of A and ui to be the corresponding eigenvector. Let\nk = arg max\ni\n|1 −λi|,\nλ := λk.\nSuppose u ∈arg maxui,i∈[d] |1−λi| is the corresponding eigenvector. Apply Lemma A.4, we have\ndA\ndt = 2L(I −A)2L−1 + ∥I −A∥2L−1∆2L.\nMultiply u on both sides (note that u is a fixed direction, so the time-differential is zero), we obtain\nd(Au)\ndt\n= 2L(I −A)2L−1u + ∥I −A∥2L−1∆2Lu.\nNote that the gradient of A has the same eigenvectors ui as A. Therefore, ∆2L have the same\neigenvector as A and we have\nd(1 −λ)\ndt\n= du⊤(I −A)u\ndt\n= −2L(1 −λ)2L−1 −(1 −λ)2L−1u⊤∆2Lu.\n23\n\n\nBy Lemma A.4, we have ∥∆2L∥≤O(\nL\nlog d) ≤L. The dynamics of (1 −λ)2 become:\nd(1 −λ)2\ndt\n= −4L(1 −λ)2L −2(1 −λ)2Lu⊤∆2Lu ≤−3L(1 −λ)2L.\nWe can therefore upper bound the difference between λ and 1 by solving the ODE:\n(1 −λ(t))2 ≤\n\u0000(1 −λ(0))2−2L + 3L(L −1)t\n\u0001−\n1\nL−1.\nWhen the training time t ≥Ω\n\u0012\n1\nL2\n\u0010\nd\nξ\n\u0011 L−1\nL \u0013\n, the largest eigenvalue of I −A, i.e.\n∥I −A∥2 = O\n \u0012ξ\nd\n\u00131/L!\n.\nTo satisfy the condition on Lemma A.4, the ξ should be lower bounded by Θ\n\u0012\nd\n\u0010\nL2d log2 d\nn\n\u0011L\u0013\n.\nNow consider the loss expression:\nL(A) = E\n\u0014\ntr\n\u0010\nI −A\n1\n2SA\n1\n2\n\u00112L\u0015\n= E\nh\ntr\n\u0010\n(I −SA)2L\u0011i\n= tr E\n\u0010\n(I −SA)2L\u0011\n= tr\n\u0010\n(I −A)2L + ∥I −A∥2L∆\n\u0011\n(By Lemma A.3)\n≤2d∥I −A∥2L ≤O(ξ).\nBy the computation above, L(A) ≤Θ(ξ). In particular, when L = c log d and n ≥2(L2d log2 d)\nwith c > 1, the loss is smaller than O(dc−1). Hence, only O(log d) steps of looping can achieve\narbitrary polynomially small loss O(\n1\npoly(d)).\nRemark. If we have Θ(log d) steps of GD using this looped transformer, we can get an arbitrary\npolynomially small loss. It is a huge improvement compared to Gatmiry et al. [20], and this result\nsuccessfully establishes a separation between the looped transformer and the ones without the loop.\nLemma A.3. Assume n = ˜Ω(dL2), ∥I −A∥op ≥Θ\n\u0012q\nL2d log2 d\nn\n\u0013\n.\nE\nh\n(I −SA)iS(I −SA)2L−1−ii\n= (I −A)2L−1 + ∥I −A∥2L−1∆\nwhere ∆has O\n\u0010\n1\nlog d\n\u0011\n-operator norm.\n24\n\n\nProof. Denote δS = S −I. Then we expand the term in the expectation:\n(I −SA)iS(I −SA)2L−1−i = (I −A −δS · A)i(I + δS)(I −A −δS · A)2L−1−i\nNow take expectation to both sides. Note that E[δS] = 0, so all terms containing first order δS\nvanish. We denote\n∥I −A∥2L−1 e∆= (I −SA)iS(I −SA)2L−1−i −(I −A)i(I + δS)(I −A)2L−1−i\n−i(I −A)i−1 · δS · A(I −A)2L−1−i −(2L −1 −i)(I −A)2L−2 · δS · A\nto be the sum of all higher order terms (the degree of δS ≥2). We can estimate the expectation\nusing similar technique as in Lemma D.1.\n∥e∆∥op ≤\n2L−1\nX\nk=2\n\u00122L −1\nk\n\u0013\r\r\r(I −A)2L−1−k(δS · A)k\r\r\r\n∥I −A∥2L−1\n(Term 1)\n+\n2L−1−i\nX\nk=1\n\u00122L −1 −i\nk\n\u0013\r\r\r(I −A)iδS(I −A)2L−1−i−k(δS · A)k\r\r\r\n∥I −A∥2L−1\n(Term 2)\n+\ni\nX\nk=1\n2L−1−i\nX\nl=0\n\u0012i\nk\n\u0013\u00122L −1 −i\nl\n\u0013\r\r\r(I −A)i−k(δS · A)kδS(I −A)2L−1−i−l(δS · A)l\r\r\r\n∥I −A∥2L−1\n(Term 3)\nTo get an estimate of the operator norm, we bound each term (Term 1 to Term 3) respectively.\nTerm 1:\n2L−1\nX\nk=2\n\u00122L −1\nk\n\u0013\r\r\r(I −A)2L−1−k(δS · A)k\r\r\r\n∥I −A∥2L−1\n≤\n2L−1\nX\nk=2\n\u0012\n(2L −1)∥δS∥· ∥A∥\n∥I −A∥\n\u0013k\nNote that ∥δS∥is of order O(\nq\nd\nn) with high probability, the term in the middle is less than 1 and\nthe dominating term of the error is\n\u0010\n(2L −1)∥δS∥·∥A∥\n∥I−A∥\n\u00112\n= O(\n1\nlog d).\nTerm 2:\n2L−1−i\nX\nk=1\n\u00122L −1 −i\nk\n\u0013\r\r\r(I −A)iδS(I −A)2L−1−i−k(δS · A)k\r\r\r\n∥I −A∥2L−1\n≤∥I −A∥\n∥A∥\n2L−1−i\nX\nk=1\n\u0012\n(2L −1 −i)∥δS∥· ∥A∥\n∥I −A∥\n\u0013k+1\nTerm 3:\ni\nX\nk=1\n2L−1−i\nX\nl=0\n\u0012i\nk\n\u0013\u00122L −1 −i\nl\n\u0013\r\r\r(I −A)i−k(δS · A)kδS(I −A)2L−1−i−l(δS · A)l\r\r\r\n∥I −A∥2L−1\n25\n\n\n≤∥I −A∥\n∥A∥\ni\nX\nk=1\n\u0012\ni∥δS∥· ∥A∥\n∥I −A∥\n\u0013k\n·\n2L−1−i\nX\nl=0\n\u0012\n(2L −1 −i)∥δS∥· ∥A∥\n∥I −A∥\n\u0013l+1\nNow we upper bound the operator norm of the error term ∆:= E\nh\ne∆\ni\n:\n∥∆∥=\n\r\r\rE\nh\ne∆\ni\r\r\r = E\nh\n∥e∆∥\ni\n= E\n\u0014\n∥e∆∥\n\u0012\n1\n\u001a\n∥e∆∥≤\nC′\nlog d\n\u001b\n+ 1\n\u001a\n∥e∆∥>\nC′\nlog d\n\u001b\u0013\u0015\n≤\nC′\nlog d +\nZ ∞\nC′\nlog d\nPr\nh\n∥e∆∥≥s\ni\nds.\nWhen ∥e∆∥≥s where s ≥\nC′\nlog d, we can first upper bound the ∥e∆∥with ∥δS∥using : there exists\nsome constant C1 > 0 s.t.\n∥e∆∥≤max\n\u0010\n(C1L∥δS∥)2, (C1L∥δS∥)2L+4\u0011\n.\nTherefore, when ∥e∆∥≥s, ∥δS∥≥min{ s1/2\nC1L, s1/(2L+4)\nC1L\n}. To apply the tail bound, we need to\nmake sure we pick some s′ such that max (δ, δ2) ≤min{ s1/2\nC1L, s1/(2L+4)\nC1L\n} to upper bound the integral\nof probability, where δ = C(\nq\nd\nn +\ns′\n√n). Now since s >\nC′\nlog d, min{ s1/2\nC1L, s1/(2L+4)\nC1L\n} ≥Cα\n1\nlog\n3\n2 d\nfor some constant Cα. Therefore, we just need max{ s′\n√n, s′2\nn } ≤min{ s1/2\nC1L, s1/(2L+4)\nC1L\n}, i.e. s′ ≤\nmin\nn\nC2\ns1/(2L+4)√n\nL\n, C3\ns1/(4L+8)√n\n√\nL\n, C4\n√sn\nL , C5\ns1/4√n\n√\nL\no\n.\nApplying the tail bound (20) with s′ = min\nn\nC2\ns1/(2L+4)√n\nL\n, C3\ns1/(4L+8)√n\n√\nL\n, C4\n√sn\nL , C5\ns1/4√n\n√\nL\no\nwhere\nC2, C3, C4, C5 are some constant, we have the error term for the tail expectation,\nZ ∞\nC′\nlog d\nPr\nh\n∥e∆∥≥s\ni\nds ≤\nZ ∞\nC′\nlog d\nPr\n\u0014\n∥δS∥≥min\n\u001a s1/2\nC1L, s1/(2L+4)\nC1L\n\u001b\u0015\nds\n≤2\nZ ∞\nC′\nlog d\nexp\n\b\n−s′2\t\nds.\nNow we estimate the upper bound of error with\ns′2 = min\n\u001a\nC2\n2 · s1/(L+2)\nL2\nn, C2\n3 · s1/(2L+4)\nL\nn, C2\n4 · sn\nL2, C2\n5 ·\n√sn\nL\n\u001b\n.\nFor the first term, let x = C2\n2n\nL2 s1/(L+2):\n2\nZ ∞\nC′\nlog d\nexp\n\u001a\n−C2\n2 · s1/(L+2)\nL2\nn\n\u001b\nds\n= 2(L + 2)\nZ ∞\nC2\n2 n\nL2 ( C′\nlog d)\n1\nL+2\n\u0012 L2\nC2\n2n\n\u0013L+2\nexp{−x}xL+1dx\n26\n\n\n≤2(L + 2) ·\n\u0012 L2\nC2\n2n\n\u0013L+2\n·\n \nC2\n2n\nL2\n\u0012 C′\nlog d\n\u0013\n1\nL+2!L+1\nexp\n(\n−C2\n2n\nL2\n\u0012 C′\nlog d\n\u0013 1\nL)\n≤\n1\nlog d.\nThe second term, let x = C2\n3 · s1/(2L+4)\nL\nn:\n2\nZ ∞\nC′\nlog d\nexp\n\u001a\n−C2\n3 · s1/(2L+4)\nL\nn\n\u001b\nds\n= 4(L + 2)\nZ ∞\nC2\n3 n\nL ( C′\nlog d)\n1\n2L+4\n\u0012 L\nC2\n3n\n\u00132L+4\nexp{−x}x2L+3dx\n≤4(L + 2) ·\n\u0012 L\nC2\n3n\n\u00132L+4\n·\n \nC2\n3n\nL\n\u0012 C′\nlog d\n\u0013\n1\n2L+4!2L+3\nexp\n(\n−C2\n3n\nL\n\u0012 C′\nlog d\n\u0013\n1\n2L+4)\n≤\n1\nlog d.\nFor the third term, let x = C2\n4sn\nL2 :\n2\nZ ∞\nC′\nlog d\nexp\n\u001a\n−C2\n4sn\nk2\n\u001b\nds = 2\nZ ∞\nC′\nlog d·\nC2\n4 n\nL2\nL2\nC2\n4n exp{−x}dx\n≤2L2\nC2\n4n exp\n\u001a\n−C′\nlog d · C2\n4n\nL2\n\u001b\n≤\n1\nlog d.\nThe fourth term, let x = C2\n5 · s1/2\nL n:\n2\nZ ∞\nC′\nlog d\nexp\n\u001a\n−C2\n5 · s1/2\nL n\n\u001b\nds\n= 4L2\nn2C4\n5\nZ ∞\nC2\n5\nn\nL( C′\nlog d)\n1/2 exp{−x}xdx\n≤4L2\nn2C4\n5\n· C2\n5\nn\nL\n\u0012 C′\nlog d\n\u00131/2\nexp\n(\n−C2\n5\nn\nL\n\u0012 C′\nlog d\n\u00131/2)\n≤\n1\nlog d.\nTherefore, we plug this error back into the upper bound of ∥∆∥:\n∥∆∥≤\nC′\nlog d +\nZ ∞\nC′\nlog d\nPr\nh\n∥e∆∥≥s\ni\nds\n≤\nC′\nlog d +\nZ ∞\nC′\nlog d\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(2L+4)\nC1k\n}\n\u0015\nds ≤O\n\u0012\n1\nlog d\n\u0013\n.\nLemma A.4. Assume n = ˜Ω(dL2), for any A that ∥I −A∥≥Θ\n\u0012q\nL2d log2 d\nn\n\u0013\n, we have the\nfollowing gradient estimate:\n∇AL(A) = −2L(I −A)2L−1 −∥I −A∥2L−1∆2L.\nwhere ∥∆2L∥≤O\n\u0010\nL\nlog d\n\u0011\n.\n27\n\n\nProof. We use our technique to estimate the derivative of the loss. By Lemma A.2, we have\n∇AL(A) = −\n2L−1\nX\ni=0\nE\nh\n(I −SA)iS(I −SA)2L−1−ii\n.\nApply Lemma A.3 to each term in the summation, we have\nE\nh\n(I −SA)iS(I −SA)2L−1−ii\n= (I −A)2L−1 + ∥I −A∥2L−1∆i.\nwhere ∆i has O\n\u0010\n1\nlog d\n\u0011\n-operator norm. Denote ∆2L = P2L−1\ni=0\n∆i and add all terms together, we\nobtain\n∇AL(A) = −2L(I −A)2L−1 −∥I −A∥2L−1∆2L.\nwhere ∆2L has O\n\u0010\nL\nlog d\n\u0011\n-operator norm.\nA.4\nLimitation and future directions\nArchitecture and parameterization\nIn this work, we use the single-layer linear transformer\nto analyze the training dynamics. Moreover, we adopt the same reparameterization and similar\ninitialization in previous works [58, 46, 10, 32, 3]. It deviates from the practical softmax attention\nwith Q, K, V parameterization and random initialization, which is a limitation of this work.\nHowever, analyzing the linear counterpart of the model before targeting the more difficult practical\nmodels is common in the development of learning theory. As for linear attention, the connection\nbetween linear attention and softmax attention is also partially justified by the empirical observa-\ntions in Ahn et al. [2]. Analyzing the dynamics using more practical architectures will be a very\nimportant and fundamental future direction.\nPopulation loss and sample complexity\nFollowing most of the previous work, we use popula-\ntion loss when analyzing the training trajectory instead of using finite sample loss. This modifi-\ncation is to simplify the analysis and focus on the population dynamics without noise. A possible\nfuture step is to generalize this analysis to a finite sample setting and train the model with online\nSGD.\nCoT on iterative tasks\nIn this work, we mainly focus on iterative tasks, one of the simplest\nforms where multi-step CoT can help yield better performance. That serves as the initial step\ntowards understanding why CoT helps reasoning following the first principle. As a limitation,\nthough CoT can empower the transformer to acquire compositional reasoning capability instead of\ndoing the same iterative step, it is a much harder question beyond our paper’s scope. It is a very\nimportant future direction and definitely worth further exploring.\nB\nProofs of theorems in Section 3\nIn this section, we prove the expressiveness results of the linear transformers with and without CoT.\nIn Appendix B.1, we prove that a one-layer linear transformer without CoT can only obtain the\n28\n\n\none-step gradient descent solution. In Appendix B.2, we prove that there exists a one-layer linear\ntransformer that implements multi-step gradient descent with the CoT prompting. As corollaries,\nthere exists a separation between the one-step and multi-step solutions.\nB.1\nProof of Theorem 3.1\nWe first restate the theorem:\nTheorem B.1 (Lower bound without CoT). If the global minimizer of LEval(V , W ) is (V ∗, W ∗),\nthe corresponding one-layer transformer fLSA(Z0)[:,−1] implements one step GD on a linear model\nwith some learning rate η =\nn\nn+d+1 and the transformer outputs η\nnXy⊤.\nProof. Recall the loss expression in Equation (5) when k = 0,\nL(V , W ) = 1\n2EX,w∗\n\r\rfLSA(Z0)[:,−1] −(0d, 0, w∗, 1)\n\r\r2\n= 1\n2EX,w∗\n\r\r\r\rV Z0 · Z⊤\n0 W Z0[:,−1]\nn\n−(0d, 0, w∗, 0)⊤\n\r\r\r\r\n2\n(since w0 = 0d.)\nThe key insight of the proof is to replace the w∗with the one-step GD solution η\nnXy⊤,\nL(V , W ) = 1\n2E\n\"\r\r\r\rV Z0 · Z⊤\n0 W Z0[:,−1]\nn\n−\n\u0010\n0d, 0, η\nnXy⊤, 0\n\u0011⊤\r\r\r\r\n2#\n+ C\nAfter proving this property, we can conclude that the optimal solution without CoT is exactly the\none-step solution η\nnXy⊤. We prove this result by showing the gradient of those two loss functions\nare the same.\nFirst, before calculating the gradient, we extract the identical parts of the loss. Notice that the\nground-truth entries are all zero in i = 1, 2, · · · , d, d + 1, 2d + 2 positions in both expressions.\nTherefore, that part of error is the norm of the output fLSA(Z0)[:,−1] in those corresponding entries:\n1\n2E\n\"\r\r\r\rV Z0 · Z⊤\n0 W Z0[1:d+1,−1]\nn\n\r\r\r\r\n2#\n+ 1\n2E\n\"\r\r\r\rV Z0 · Z⊤\n0 W Z0[2d+2,−1]\nn\n\r\r\r\r\n2#\nwhich is the same for both expressions. Therefore, we just need to consider\nfLSA(Z0)[d+2:2d+1,−1] = V[d+2:2d+1,:]Z0 · Z⊤\n0 W Z0[:,−1]\nn\n,\nwhich corresponds to the ground-truth signals. Here V[d+2:2d+1,:] =\n\u0002\nV31, V32, V33, V34\n\u0003\n. We only\nneed to prove that\nE\n\r\rfLSA(Z0)[d+2:2d+1,−1] −w∗\r\r2 = E\n\r\r\rfLSA(Z0)[d+2:2d+1,−1] −η\nnXX⊤w∗\r\r\r\n2\n+ C\nfor some constant C.\n29\n\n\nWe show the gradients of both sides are the same, and equivalently the differential of both sides\nshould be the same. The differential of L.H.S. is\nd\n\u0010\nE\n\r\rfLSA(Z0)[d+2:2d+1,−1] −w∗\r\r2\u0011\n= 2E\n\u0002\n(fLSA(Z0)[d+2:2d+1,−1] −w∗)⊤dfLSA(Z0)[d+2:2d+1,−1]\n\u0003\nand the differential of R.H.S. is\nd\n\u0012\nE\n\r\r\rfLSA(Z0)[d+2:2d+1,−1] −η\nnXX⊤w∗\r\r\r\n2\u0013\n= 2E\nh\n(fLSA(Z0)[d+2:2d+1,−1] −η\nnXX⊤w∗)⊤dfLSA(Z0)[d+2:2d+1,−1]\ni\nTherefore, we only need to prove that\nE\nh\nw∗⊤dfLSA(Z0)[d+2:2d+1,−1]\ni\n= E\n\u0014\u0010η\nnXX⊤w∗\u0011⊤\ndfLSA(Z0)[d+2:2d+1,−1]\n\u0015\n(12)\nWe expand this expression fLSA(Z0)[d+2:2d+1,−1] (Note that now we don’t have the assumption of\ninitialization):\nV[d+2:2d+1,:]Z0 · Z⊤\n0 W Z0[:,−1]\nn\n= 1\nn\n\u0002\nV31\nV32\nV33\nV34\n\u0003\n\n\nX\n0\ny\n0\n0d×n\nw0\n01×n\n1\n\n\n\u0014X⊤\ny⊤\n0n×d\n0n\n01×d\n0\nw⊤\n0\n1\n\u0015\nW\n\n\n0\n0\nw0\n1\n\n\n= 1\nn\n\u0002\nV31\nV32\nV33\nV34\n\u0003\n\n\nXX⊤\nXy⊤\n0d×d\n0d\nyX⊤\nyy⊤\n01×d\n0\n0d×d\n0d\n0d×d\n0d\n01×d\n0\n01×d\n1\n\n\n\n\nW14\nw24\nW34\nw44\n\n\n(since w0 = 0d)\n= 1\nn\n\u0002\nV31\nV32\nV33\nV34\n\u0003\n\n\nXX⊤W14 + w24Xy⊤\nyX⊤W14 + w24yy⊤\n0d\nw44\n\n\n= 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤(W14 + w24w∗) + V34w44\nn\n(y = X⊤w∗.)\nand the differential of fLSA(Z0)[d+2:2d+1,−1] is\ndfLSA(Z0)[d+2:2d+1,−1]\n= d\n\u00121\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤(W14 + w24w∗)\n\u0013\n+ dV34w44\nn\n= 1\nn\n\u0010\ndV31 + dV32w∗⊤\u0011\nXX⊤(W14 + w24w∗) + 1\nn(dV34 · w44 + V34dw44)\n+ 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤(dW14 + dw24w∗)\n30\n\n\nNow, to prove Equation (12), we compare the differential for each parameter on both sides. For all\nparameter, we start from the left side and prove it equal to the right.\nV31: The V31 term of differential in dfLSA(Z0)[d+2:2d+1,−1] is 1\nndV31XX⊤(W14 + w24w∗),\nE\n\u0014\nw∗⊤· 1\nndV31XX⊤(W14 + w24w∗)\n\u0015\n= E\n\u0014\ntr\n\u0012\nw∗⊤· 1\nndV31XX⊤(W14 + w24w∗)\n\u0013\u0015\n(It is a scalar in the trace.)\n= E\n\u0014\ntr\n\u00121\nndV31XX⊤(W14 + w24w∗)w∗⊤\n\u0013\u0015\n= E[tr (dV31w24)]\n(E[XX⊤] = nId, E[w∗] = 0, E[w∗w∗⊤] = Id.)\n= E\nh\ntr\n\u0010 η\nn2 · dV31w24XX⊤XX⊤\u0011i\n(E[\n\u0000XX⊤\u00012] = n(n + d + 1)Id, η =\nn\nn+d+1.)\n= E\nh\ntr\n\u0010 η\nn2 · XX⊤dV31XX⊤(W14 + w24w∗)w∗⊤\u0011i\n(E[w∗] = 0, E[w∗w∗⊤] = Id.)\n= E\n\u0014\n(η\nnXX⊤w∗)⊤· 1\nndV31XX⊤(W14 + w24w∗)\n\u0015\nSo those two dV31 terms are identical.\nV32: The V32 term of differential in dfLSA(Z0)[d+2:2d+1,−1] is dV32\nn w∗⊤XX⊤(W14 + w24w∗),\nE\n\u0014\nw∗⊤· dV32\nn w∗⊤XX⊤(W14 + w24w∗)\n\u0015\n= E\n\u0014\ntr\n\u0012\nw∗⊤· dV32\nn w∗⊤XX⊤(W14 + w24w∗)\n\u0013\u0015\n(It is a scalar in the trace.)\n= E\n\u0014\ntr\n\u0012dV32\nn w∗⊤XX⊤(W14 + w24w∗)w∗⊤\n\u0013\u0015\n= E\n\u0014\ntr\n\u0012dV32\nn w∗⊤XX⊤W14w∗⊤\n\u0013\u0015\n(E[w∗] = 0 and w∗⊤XX⊤w∗w∗⊤is odd)\n= E\n\u0014\ntr\n\u0012dV32\nn W ⊤\n14XX⊤w∗w∗⊤\n\u0013\u0015\n(W ⊤\n14XX⊤w∗is a scalar.)\n= E\n\u0002\ntr\n\u0000dV32W ⊤\n14\n\u0001\u0003\n(E[XX⊤] = nId, E[w∗w∗⊤] = Id.)\n= E\nh\ntr\n\u0010 η\nn2 · dV32W ⊤\n14XX⊤XX⊤\u0011i\n(E[\n\u0000XX⊤\u00012] = n(n + d + 1)Id, η =\nn\nn+d+1.)\n= E\nh\ntr\n\u0010 η\nn2 · XX⊤dV32w∗⊤XX⊤(W14 + w24w∗)w∗⊤\u0011i\n(E[w∗] = 0, E[w∗w∗⊤] = Id.)\n= E\n\u0014\n(η\nnXX⊤w∗)⊤· 1\nndV32w∗⊤XX⊤(W14 + w24w∗)\n\u0015\nSo those two dV32 terms are identical.\nV34: The V34 term of differential in dfLSA(Z0)[d+2:2d+1,−1] is 1\nndV34w44,\nE\n\u0014\nw∗⊤1\nndV34w44\n\u0015\n= 0 = E\n\u0014\n(η\nnXX⊤w∗)⊤1\nndV34w44\n\u0015\n31\n\n\nsince E[w∗] = 0d. Therefore those two are equal.\nW14: The W14 term of differential in dfLSA(Z0)[d+2:2d+1,−1] is 1\nn\n\u0000V31 + V32w∗⊤\u0001\nXX⊤dW14,\nE\n\u0014\nw∗⊤· 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤dW14\n\u0015\n= E\n\u0014\ntr\n\u0012\nw∗⊤· 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤dW14\n\u0013\u0015\n(It is a scalar in the trace.)\n= E\n\u0014\ntr\n\u00121\nn\n\u0010\nw∗⊤V32w∗⊤\u0011\nXX⊤dW14\n\u0013\u0015\n(E[w∗] = 0d.)\n= E\n\u0014\ntr\n\u00121\nn\n\u0010\nV ⊤\n32w∗w∗⊤\u0011\nXX⊤dW14\n\u0013\u0015\n(V ⊤\n32w∗is a scalar.)\n= E\n\u0002\ntr\n\u0000V ⊤\n32dW14\n\u0001\u0003\n(E[XX⊤] = nId, E[w∗w∗⊤] = Id.)\n= E\nh\ntr\n\u0010 η\nn2 · XX⊤V ⊤\n32XX⊤dW14\n\u0011i\n(E[\n\u0000XX⊤\u00012] = n(n + d + 1)Id, η =\nn\nn+d+1.)\n= E\n\u0014\n(η\nnXX⊤w∗)⊤· 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤dW14\n\u0015\nThus the two dW14 terms are the same.\nw24: The w24 term in dfLSA(Z0)[d+2:2d+1,−1] is 1\nn\n\u0000V31 + V32w∗⊤\u0001\nXX⊤dw24w∗,\nE\n\u0014\nw∗⊤· 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤w∗dw24\n\u0015\n= E\n\u0014\ntr\n\u0012\nw∗⊤· 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤w∗dw24\n\u0013\u0015\n(It is a scalar in the trace.)\n= E\n\u0014\ntr\n\u00121\nn\n\u0010\nw∗⊤V31\n\u0011\nXX⊤w∗dw24\n\u0013\u0015\n(E[w∗] = 0d.)\n= E[tr (V31dw24)]\n(E[XX⊤] = nId, E[w∗w∗⊤] = Id.)\n= E\nh\ntr\n\u0010 η\nn2 · XX⊤V31XX⊤dw24\n\u0011i\n(E[\n\u0000XX⊤\u00012] = n(n + d + 1)Id, η =\nn\nn+d+1.)\n= E\n\u0014\n(η\nnXX⊤w∗)⊤· 1\nn\n\u0010\nV31 + V32w∗⊤\u0011\nXX⊤w∗dw24\n\u0015\nTherefore the differential for w24 are the same.\nw44: The w44 term of differential in dfLSA(Z0)[d+2:2d+1,−1] is 1\nnV34dw44,\nE\n\u0014\nw∗⊤1\nnV34dw44\n\u0015\n= 0 = E\n\u0014\n(η\nnXX⊤w∗)⊤1\nnV34dw44\n\u0015\nsince E[w∗] = 0d. Therefore those two are also equal.\nIn conclusion, Equation (12) holds since all the differential terms are equal. Therefore, ∃C\nE\n\r\rfLSA(Z0)[d+2:2d+1,−1] −w∗\r\r2 = E\n\r\r\rfLSA(Z0)[d+2:2d+1,−1] −η\nnXX⊤w∗\r\r\r\n2\n+ C\nwhich finishes our proof.\n32\n\n\nB.2\nProof of Theorem 3.2\nHere we restate the Theorem 3.2 and provide the detailed proof.\nTheorem B.2. Suppose n = Θ(d log5 d), k ≥C log d, η ∈(0.1, 0.9). There exists V ∗and W ∗\ns.t. fLSA(Zk)[:,−1] outputs (0d, 0, wk+1, 1) where wi :=\n\u0000I −(I −η\nnXX⊤)i\u0001\nw∗is the k-step GD\nsolution with learning rate η on a linear regression model. Moreover, the evaluation loss\nLEval(V ∗, W ∗) = 1\n2EX,w∗\n\"\r\r\r\r\n\u0010\nI −η\nnXX⊤\u0011k+1\nw∗\n\r\r\r\r\n2#\n≤\n1\ndC log(\n1\n1−η)\n(13)\nProof. We construct V ∗and W ∗in the following way,\nV ∗=\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n−ηI\n0\n0\n0\n0\n0\n0\n0\n\n, W ∗=\n\n\n0\n0\nI\n0\n0\n0\n0\n−1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n(14)\nNow the transformer is allowed to generate k steps before reaching the final output. We can\ninductively calculate the i-th step of generation, showing that the output is exactly the parameter\nafter i-th gradient step (i = 1, 2, ..., k + 1):\nfLSA(Zi)[:,−1] = (0d, 0, wi, 1) + V Zi · Z⊤\ni W Zi[:,−1]\nn\n= (0d, 0, wi, 1) + 1\nn\n\u00000d, 0, V31(t)XX⊤(W13(t)wi −w∗), 0\n\u0001\n= (0d, 0, wi, 1) + (0d, 0, −η\nnXX⊤(wi −w∗), 0)\n= (0d, 0, wi+1, 1)\nAfter k+1 steps, we have the final output\n\u0000I −(I −η\nnXX⊤)k+1\u0001\nw∗by induction and the evalu-\nation loss becomes Equation (9). By Lemma D.4, the final loss is\n1\n2EX,w∗\n\"\r\r\r\r\n\u0010\nI −η\nnXX⊤\u0011k+1\nw∗\n\r\r\r\r\n2#\n= 1\n2EX,w∗\n\u0014\ntr\n\u0010\nI −η\nnXX⊤\u00112k+2\u0015\n(E[w∗w∗⊤] = I.)\n= 1\n2 tr EX,w∗\n\u0014\u0010\nI −η\nnXX⊤\u00112k+2\u0015\n= 1\n2 tr((1 −η)k(1 + δ)I)\n(By Lemma D.4)\n≤d(1 −η)k ≤d−C log(\n1\n1−η).\n33\n\n\nC\nProof of Theorem 4.1\nC.1\nGradient computation of the full model over the CoT objective\nIn this appendix, we compute the gradient of the full model given the Assumption 4.1 and prove\nthe equivalence between the dynamics of the full model and a simplified model. Throughout the\nappendix, we denote the S = 1\nnXX⊤for simplicity. And recall the i-th step of the linear classifier\nis wi = (I −(I −ηS)i)w∗.\nIn Section 2.2, we have the full attention model\nfLSA(Z; V , W )[:,−1] = Z[:,−1] + V Z · Z⊤W Z[:,−1]\nn\nand the Chain of Thought (CoT) objective\nLCoT(V , W ) = EX,w∗\n\"\n1\n2\nk\nX\ni=0\n\r\rfLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\r\r2\n#\nWe define the error for the i-th step\nLi := 1\n2EX,w∗\n\r\rfLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\r\r2\nBy linearity of expectation, we know the gradient of the CoT objective is the sum of gradients of\nall CoT steps: ∇LCoT = Pk\ni=1 ∇Li. Now we can calculate the gradients of V , W based on the\nloss of each CoT step:\nLemma C.1 (Gradients of the full model). The gradient of V , W are given by the following\nequations:\n∇V L = 1\nnEX,w∗\nk\nX\ni=0\n\u0012\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n−(0d, 0, wi+1 −wi, 0)⊤\n\u0013\nZi\n⊤\n[:,−1]W ⊤ZiZ⊤\ni\n∇W L = 1\nnEX,w∗\nk\nX\ni=0\nZiZ⊤\ni V ⊤\n\u0012\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n−(0d, 0, wi+1 −wi, 0)⊤\n\u0013\nZi\n⊤\n[:,−1]\nProof. The loss is given by eq. (6):\nLCoT(V , W ) = EX,w∗\n\"\n1\n2\nk\nX\ni=0\n\r\rfLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\r\r2\n#\n=\nk\nX\ni=1\nLi\nTake differential of the loss for the i-th step Li and we have\ndLi = EX,w∗\u0000fLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\u0001⊤dfLSA(Zi)[:,−1]\n= EX,w∗\u0000fLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\u0001⊤d\n\u0012\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n\u0013\n34\n\n\n= EX,w∗\u0000fLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\u0001⊤d(V )Zi · Z⊤\ni W Zi[:,−1]\nn\n+ EX,w∗\u0000fLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\u0001⊤V Zi · Z⊤\ni dW Zi[:,−1]\nn\nThen the gradients of W , V of the Li are:\n∇V Li = 1\nnEX,w∗\u0000fLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\u0001\nZi\n⊤\n[:,−1]W ⊤ZiZ⊤\ni\n= 1\nnEX,w∗\n\u0012\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n−(0d, 0, wi+1 −wi, 0)⊤\n\u0013\nZi\n⊤\n[:,−1]W ⊤ZiZ⊤\ni\n∇W Li = 1\nnEX,w∗ZiZ⊤\ni V ⊤\u0000fLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\u0001\nZi\n⊤\n[:,−1]\n= 1\nnEX,w∗ZiZ⊤\ni V ⊤\n\u0012\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n−(0d, 0, wi+1 −wi, 0)⊤\n\u0013\nZi\n⊤\n[:,−1]\nTake the sum of the two equations above from i = 0 to k, and we finish the proof.\nNow we consider the gradient flow (GF) trajectory (note that w24 is fixed under Assumption 4.1):\ndθ\ndt = −∇LCoT(θ),\nθ := (V , W \\{w24}).\nRecall the block matrix form of V , W :\nV =\n\n\nV11\nV12\nV13\nV14\nV21\nv22\nV23\nv24\nV31\nV32\nV33\nV34\nV41\nv42\nV43\nv44\n\n, W =\n\n\nW11\nW12\nW13\nW14\nW21\nw22\nW23\nw24\nW31\nW32\nW33\nW34\nW41\nw42\nW43\nw44\n\n\nAccording to the construction in Theorem 3.2, the blocks W13, V31, w24 are the only relevant pa-\nrameter blocks, while the others should be zeroed out. Next, we prove that if we initialize those\nirrelevant blocks to 0, then they will stay at 0 along the gradient descent trajectory.\nLemma C.2. Under the Assumption 4.1, when the linear transformer is trained under GF, we have\nfor all t > 0, the parameters V (t), W (t) have the following form:\nV (t) =\n\n\n0\n0\n0\n0\n0\n0\n0\n0\nV31(t)\n0\n0\n0\n0\n0\n0\n0\n\n, W (t) =\n\n\n0\n0\nW13(t)\n0\n0\n0\n0\n−1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nProof. To prove this lemma, we prove that when the irrelevant blocks are 0, the gradients ∇V Li, ∇W Li\nfor those blocks are always 0 and they never update the corresponding parameter block. Also, note\nthat w24 = −1 for all t > 0.\n35\n\n\nFirst, we calculate the output of the linear self-attention V Zi ·\nZ⊤\ni W Zi[:,−1]\nn\n:\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n= 1\nn\n\n\n0\n0\n0\n0\n0\n0\n0\n0\nV31(t)\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\nX\n0\n0\n· · ·\n0\ny\n0\n0\n· · ·\n0\n0d×n\nw0\nw1\n· · ·\nwi\n01×n\n1\n1\n· · ·\n1\n\nZ⊤\ni W\n\n\n0\n0\nwi\n1\n\n\n= 1\nn\n\n\n0d×n\n0d\n· · ·\n0d\n01×n\n0\n· · ·\n0\nV31(t)X\n0d\n· · ·\n0d\n01×n\n0\n· · ·\n0\n\n\n\n\nX\n0\n0\n· · ·\n0\ny\n0\n0\n· · ·\n0\n0d×n\nw0\nw1\n· · ·\nwi\n01×n\n1\n1\n· · ·\n1\n\n\n⊤\n\nW13(t)wi\n−1\n0d\n0\n\n\n= 1\nn\n\n\n0d×n\n0d\n· · ·\n0d\n01×n\n0\n· · ·\n0\nV31(t)X\n0d\n· · ·\n0d\n01×n\n0\n· · ·\n0\n\n\n\u0014X⊤W13(t)wi −y⊤\n0i+1\n\u0015\n= 1\nn\n\n\n0d\n0\nV31(t)XX⊤(W13(t)wi −w∗)\n0\n\n\nThe last line is because y⊤= X⊤w∗. Now, we consider the gradient for V :\n∇V Li = 1\nnEX,w∗\n\u0014\u0012\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n−(0d, 0, wi+1 −wi, 0)⊤\n\u0013\nZi\n⊤\n[:,−1]W ⊤ZiZ⊤\ni\n\u0015\n= 1\nn2\n\n\n0d\n0\nV31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n0\n\nZi\n⊤\n[:,−1]W ⊤ZiZ⊤\ni\n= 1\nn2EX,w∗\n\n\n0d\n0\nV31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n0\n\n\n\n\nw⊤\ni W ⊤\n13(t)\n−1\n0d\n0\n\n\n⊤\nZiZ⊤\ni\n= 1\nn2EX,w∗\n\n\n0d\n0\nV31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n0\n\n\n\n\nw⊤\ni W ⊤\n13(t)XX⊤−yX⊤\nw⊤\ni W ⊤\n13(t)Xy⊤−yy⊤\n0d\n0\n\n\n⊤\n=\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n∇V31Li(t)\n∇V32Li(t)\n0\n0\n0\n0\n0\n0\n\n\nTherefore, we know all blocks of the gradient are zero except the positions of V31 and V32.\nNow look at ∇V32Li:\n∇V32Li = 1\nn2EX,w∗\u0002\u0000V31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n\u0001\n\u0000w⊤\ni W ⊤\n13(t)Xy⊤−yy⊤\u0001\u0003\n36\n\n\n= 1\nn2EX,w∗\u0002\u0000V31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n\u0001\n\u0010\nw⊤\ni W ⊤\n13(t)XX⊤w∗−w∗⊤XX⊤w∗\u0011i\nNote that wi = (I −(I −ηS)i)w∗for all i ∈[k], and wk+1 = w∗. Therefore, for all i ∈\n{0, 1, · · · , k + 1} the formula inside the expectation is an odd function of w∗. Since w∗∼\nN(0, Id), the expectation should be 0d.\nSimilarly, we calculate the gradient of the W :\n∇W Li = 1\nnEX,w∗\n\u0014\nZiZ⊤\ni V ⊤\n\u0012\nV Zi · Z⊤\ni W Zi[:,−1]\nn\n−(0d, 0, wi+1 −wi, 0)⊤\n\u0013\nZi\n⊤\n[:,−1]\n\u0015\n= 1\nn2EX,w∗\n\nZiZ⊤\ni V ⊤\n\n\n0d\n0\nV31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n0\n\nZi\n⊤\n[:,−1]\n\n\n= 1\nn2EX,w∗\n\n\n0d×d\n0\nXX⊤V31(t)⊤\n0\n0d×d\n0\nyX⊤V31(t)⊤\n0\n0d×d\n0\n0d×d\n0\n0d×d\n0\n0d×d\n0\n\n\n\n\n0d\n0\nV31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n0\n\nZi\n⊤\n[:,−1]\n= 1\nn2EX,w∗\n\n\n\n\nXX⊤V31(t)⊤V31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\nyX⊤V31(t)⊤V31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n0d\n0\n\n\n\n\n0d\n0\nwi\n1\n\n\n⊤\n\n=\n\n\n0\n0\n∇W13Li(t)\n∇W14Li(t)\n0\n0\n∇W23Li(t)\n∇w24Li(t)\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSince we fix w24, we only consider the remaining three blocks. First, we consider the gradient of\nthe vector block W14:\n∇W14Li(t) = 1\nn2EX,w∗\u0002\nXX⊤V31(t)⊤V31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n\u0003\n.\nNotice that the XX⊤V31(t)⊤V31(t)XX⊤(W13(t)wi −w∗)−n(wi+1 −wi) is odd in w∗. There-\nfore the expectation is 0d. Similarly, we consider the other block W23:\n∇W23Li(t) = 1\nn2EX,w∗\u0002\u0000yX⊤V31(t)⊤V31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n\u0001\nw⊤\ni\n\u0003\n= 1\nn2EX,w∗\nh\u0010\nw∗⊤XX⊤V31(t)⊤V31(t)XX⊤(W13(t)wi −w∗) −n(wi+1 −wi)\n\u0011\nw⊤\ni\ni\n= 01×d.\nIn conclusion, all the blocks have zero gradient except V31, W13 given that they are all zero matri-\nces. Under Assumption 4.1, all the irrelevant blocks remain zero matrices for all t ≥0.\n37\n\n\nBy Lemma C.2, we prove that along the gradient flow trajectory under Assumption 4.1, the objec-\ntive of the linear self-attention model with residual connection can be equivalently transform to the\nfollowing simplified form.\nLemma C.3. Under Assumption 4.1, we have the training objective\nLCoT(V , W ) = 1\n2EX,w∗\n\" k\nX\ni=0\n∥V31(SW13wi −Sw∗) −∆wi∥2\n#\nwhere S = 1\nnXX⊤and ∆wi := wi+1 −wi, i = 0, 1..., k is the residual for each step i.\nProof. Given the following CoT objective,\nLCoT(V , W ) = 1\n2EX,w∗\n\" k\nX\ni=0\n\r\rfLSA(Zi)[:,−1] −(0d, 0, wi+1, 1)\n\r\r2\n#\nBy Lemma C.2, we plug in the V , W expressions and get:\nfLSA(Zi)[:,−1] −(0d, 0, wi+1, 1) = V Zi · Z⊤\ni W Zi[:,−1]\nn\n−(0d, 0, wi+1 −wi, 0)⊤\n=\n\u0012\n0d, 0, 1\nnV31\n\u0000XX⊤W13wi −Xy⊤\u0001\n−∆wi, 0\n\u0013\nSince y⊤= X⊤w∗, we have\nfLSA(Zi)[:,−1] −(0d, 0, wi+1, 1) =\n\u00000d, 0, V31\n\u0000S⊤W13wi −Sw∗\u0001\n−∆wi, 0\n\u0001⊤\nPut it back to the loss expression and we complete the proof.\nNow the chain of thought loss can be rewritten into the form by Lemma C.3, we can directly\ncalculate the gradient update using the simplified loss for clarity. We denote the only relevant\nblocks f\nW := W13 and eV := V31. Moreover, we can further expand the CoT loss with ∆wi =\n−η · XX⊤\nn\n(wi −w∗) for i ∈{0, 1, · · · , k −1}, and ∆wk = w∗−wk. That leads to the following\nexpression of the CoT loss:\nLCoT(θ) = 1\n2EX,w∗\nk−1\nX\ni=0\n\r\r\rwi + eV S\n\u0010\nf\nW wi −w∗\u0011\n−wi+1\n\r\r\r\n2\n2\n+ 1\n2EX,w∗\n\r\r\rwk + eV S\n\u0010\nf\nW wk −w∗\u0011\n−w∗\r\r\r\n2\n2\n(15)\nObserve that the final loss only depends on the (d + 2) to (2d + 2) entries of the transformer’s\noutput, indicating we can simplify the model a bit and prune out the irrelevant part. We can define\na simplified one-layer transformer to get the loss form above, where the dynamics of the equivalent\nmodel is exactly the same with the original dynamics of W13 and V31. Accordingly, the last token\ninput of the transformer for i-th step becomes wi and the label becomes wi+1 since the other entries\nin the original input/label (0, 0, wi, 1) do not affect prediction.\n38\n\n\nDefinition C.1 (Reduced transformer). Let θ = ( eV , f\nW ). Define\nfθ(X, Zi) = wi + eV S\n\u0010\nf\nW wi −w∗\u0011\nto be the reduced model of the one-layer transformer in Equation (3). For ease of presentation, we\ndenote fθ(wi) := fθ(X, Zi).\nIn the following sections, we will consider the equivalent form of transformer. Here we present\nthe gradient with regard to the reduced model. For clarification, throughout this section we will\ndenote wk+1 :=\n\u0010\nI −(I −ηS)k+1\u0011\nw∗as the (k + 1)-th update, and w∗is the ground-truth.\nLemma C.4. The gradient of eV and f\nW are given by the following expectations:\n∂L\n∂eV\n=\nk\nX\ni=0\nE\nh\n(fθ(wi) −wi+1)\n\u0010\nw⊤\ni f\nW ⊤−w∗T\u0011\nS\ni\n+ E\nh\n(wk+1 −w∗)\n\u0010\nw⊤\nk f\nW ⊤−w∗T\u0011\nS\ni\n,\n∂L\n∂f\nW\n=\nk\nX\ni=0\nE\nh\nS eV ⊤(fθ(wi) −wi+1)w⊤\ni\ni\n+ E\nh\nS eV ⊤(wk+1 −w∗)w⊤\nk\ni\n.\nProof. Given the equivalent CoT loss in Equation (15), we take the gradient with regard to eV of\nthe loss and we have\n∂L\n∂eV\n=\nk−1\nX\ni=0\nE\nh\n(fθ(wi) −wi+1)\n\u0010\nw⊤\ni f\nW ⊤−w∗T\u0011\nS\ni\n+ E\nh\n(fθ(wk) −w∗)\n\u0010\nw⊤\nk f\nW ⊤−w∗T\u0011\nS\ni\n=\nk\nX\ni=0\nE\nh\n(fθ(wi) −wi+1)\n\u0010\nw⊤\ni f\nW ⊤−w∗T\u0011\nS\ni\n+ E\nh\n(wk+1 −w∗)\n\u0010\nw⊤\nk f\nW ⊤−w∗T\u0011\nS\ni\nThe second step is because we subtract E\nh\n(fθ(wk) −wk+1)\n\u0010\nw⊤\nk f\nW ⊤−w∗T\u0011\nS\ni\nfrom the second\nterm and put it into the summation. Similarly, the partial derivative of f\nW should be:\n∂L\n∂f\nW\n=\nk−1\nX\ni=0\nE\nh\nS eV ⊤(fθ(wi) −wi+1)w⊤\ni\ni\n+ E\nh\nS eV ⊤(fθ(wk) −w∗)w⊤\nk\ni\n=\nk\nX\ni=0\nE\nh\nS eV ⊤(fθ(wi) −wi+1)w⊤\ni\ni\n+ E\nh\nS eV ⊤(wk+1 −w∗)w⊤\nk\ni\nTherefore we complete the proof.\nC.2\nGradient characterization over the CoT objective\nIn this section, we compute the exact gradient for the reduced model parameters to facilitate\nanalysis on the dynamics.\nFor clarification, throughout this section we will denote wk+1 :=\n\u0010\nI −(I −ηS)k+1\u0011\nw∗as the (k + 1)-th update, and w∗is the ground-truth.\n39\n\n\nWe first compute our gradients for the simplfied model defined in Definition C.1, which is equiv-\nalent to the full model’s dynamics. Recall that under assumption 4.1, we have eV , f\nW are simul-\ntaneously diagonalizable, with the orthonormal basis {ui}d\ni=1. We denote the orthogonal matrix\nformed by the basis as U. We will observe that ui are always the eigenvector of eV , f\nW , so we\ndenote eV = UΛ eV U ⊤, f\nW = UΛf\nW U ⊤. For clarity, we ignore the timestamp when calculating\nthe gradients and dynamics.\nWe present an accurate estimate of the gradient in the following Lemma C.5. We intensively use\nthe concentration lemma in Appendix D to separate the main terms dominating the gradient flow\ndynamics, and some bounded error terms that may complicate the analysis. We also call the error\nterms as ‘interaction terms’, since they contain the interactions between two subspaces uiu⊤\ni and\nuju⊤\nj . The structure of the interaction terms ∆eV , ∆f\nW are further characterized in this lemma,\nwhich is essential for the final local convergence analysis.\nLemma C.5. Suppose n = Θ(d log5 d), η ∈(0.1, 0.9), k = ⌈c log d⌉. Under Assumption 4.1, if\nwe run gradient flow on the population loss in Equation (6), then the gradient of eV and f\nW are\ncharacterized by the following equations:\nU ⊤∂L\n∂eV\nU =\n\u0014\u0012\nk + 1 −2\nη +\n1\nη(2 −η)\n\u0013\nΛ\nf\nW 2\n−2\n\u0012\nk + 1 −1\nη\n\u0013\nΛ\nf\nW + (k + 1)I\n\u0015\nΛ\neV\n−1 −η\n2 −ηΛ\nf\nW + I + ∆\neV ,\nU ⊤∂L\n∂f\nW\nU =\n\u0012\nk + 1 −2\nη +\n1\nη(2 −η)\n\u0013\nΛ\neV 2\nΛ\nf\nW −\n\u0012\nk + 1 −1\nη\n\u0013\nΛ\neV 2\n−1 −η\n2 −ηΛ\neV + ∆\nf\nW .\nwhere the error terms (interaction terms)\n\r\r\r∆eV \r\r\r\nop ≤O\n\u0010\n1\nlog2 d\n\u0011\n,\n\r\r\r∆f\nW \r\r\r\nop ≤O\n\u0010\n1\nlog2 d\n\u0011\n. Moreover,\nthere exist diagonal matrices A eV , B eV , Af\nW , B f\nW with O\n\u0010\n1\nlog2 d\n\u0011\n-operator norm, C eV , D eV , C f\nW , D f\nW , E f\nW\nwith O\n\u0010\n1\nd log2 d\n\u0011\n-operator norm and E eV , F f\nW with O\n\u0010\n(1 −η)k\u0011\n-operator norm s.t. the error\nterms ∆eV , ∆f\nW can be written as\n∆\neV =\n\u0010\nΛ\neV + ηI\n\u0011\nA\neV +\n\u0010\nI −Λ\nf\nW \u0011\nB\neV + tr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nC\neV + tr\n\u0010\nI −Λ\nf\nW \u0011\nD\neV + E\neV ,\n∆\nf\nW =\n\u0010\nΛ\neV + ηI\n\u0011\nA\nf\nW +\n\u0010\nI −Λ\nf\nW \u0011\nB\nf\nW + tr\n\u0010\nI −Λ\nf\nW \u0011\nC\nf\nW + tr\n\u0010\n(Λ\neV + ηI)Λ\neV \u0011\nD\nf\nW\n+ tr\n\u0010\n(I −Λ\nf\nW )Λ\neV 2\u0011\nE\nf\nW + F\nf\nW .\nProof. Recall the gradients formula of eV and f\nW by Lemma C.4:\n∂L\n∂eV\n=\nk\nX\ni=0\nE\nh\n(fθ(wi) −wi+1)\n\u0010\nw⊤\ni f\nW ⊤−w∗T\u0011\nS\ni\n+ E\nh\n(wk+1 −w∗)\n\u0010\nw⊤\nk f\nW ⊤−w∗T\u0011\nS\ni\n∂L\n∂f\nW\n=\nk\nX\ni=0\nE\nh\nS eV ⊤(fθ(wi) −wi+1)w⊤\ni\ni\n+ E\nh\nS eV ⊤(wk+1 −w∗)w⊤\nk\ni\n40\n\n\nWe expand the reduced model fθ(wi) in Definition C.1, and get the residual term\nfθ(wi) −wi+1 = wi + eV S\n\u0010\nf\nW wi −w∗\u0011\n−wi+1\n= eV S\n\u0010\nf\nW wi −w∗\u0011\n+ ηS(wi −w∗)\n=\n\u0010\neV S f\nW + ηS\n\u0011\nwi −\n\u0010\neV + ηI\n\u0011\nSw∗\nSubstitute fθ(wi) −wi+1 term in the dynamics by the equation above, we have\n∂L\n∂eV\n=\nk\nX\ni=0\nE\n\u0014\u0010\neV S f\nW + ηS\n\u0011\u0010\nI −(I −ηS)i\u00112 f\nW ⊤S\n\u0015\n−\nk\nX\ni=0\nE\nh\u0010\neV S f\nW + ηS\n\u0011\u0010\nI −(I −ηS)i\u0011\nS\ni\n−\nk\nX\ni=0\nE\nh\u0010\neV + ηI\n\u0011\nS\n\u0010\nI −(I −ηS)i\u0011\nf\nW ⊤S\ni\n+\nk\nX\ni=0\nE\nh\u0010\neV + ηI\n\u0011\nS2i\n−E\nh\n(I −ηS)k+1\u0010\u0010\nI −(I −ηS)k\u0011\nf\nW ⊤−I\n\u0011i\n=\nk\nX\ni=0\n\u0010\neV + ηI\n\u0011\nE\n\u0014\nS f\nW\n\u0010\nI −(I −ηS)i\u00112 f\nW ⊤S\n\u0015\n(Term 1)\n+ η\nk\nX\ni=0\nE\n\u0014\nS\n\u0010\nI −f\nW\n\u0011\u0010\nI −(I −ηS)i\u00112 f\nW ⊤S\n\u0015\n(Term 2)\n−\nk\nX\ni=0\n\u0010\neV + ηI\n\u0011\nE\nh\nS f\nW\n\u0010\nI −(I −ηS)i\u0011\nS\ni\n(Term 3)\n−η\nk\nX\ni=0\nE\nh\nS\n\u0010\nI −f\nW\n\u0011\u0010\nI −(I −ηS)i\u0011\nS\ni\n(Term 4)\n−\nk\nX\ni=0\n\u0010\neV + ηI\n\u0011\nE\nh\nS\n\u0010\nI −(I −ηS)i\u0011\nf\nW ⊤S\ni\n(Term 5)\n+\nk\nX\ni=0\n\u0010\neV + ηI\n\u0011\nE\n\u0002\nS2\u0003\n(Term 6)\n−E\nh\n(I −ηS)k+1\u0010\u0010\nI −(I −ηS)k\u0011\nf\nW ⊤−I\n\u0011i\n.\n(Term 7)\nTo get an accurate estimate of the gradient, we apply Lemma C.14, Lemma C.15 respectively to\neach of the terms (Term 1 to Term 7) and separate the interaction terms introduced by the moments\nof Wishart matrix, which is bounded by O\n\u0010\n1\nlog3 d\n\u0011\n.\nConsider Term 7 and the i-th term in the summation of Term 1 to Term 6. By Lemma C.14 and\nLemma C.15, there exist diagonal matrices ξj, j ∈[6] satisfying ∥ξj∥op ≤O\n\u0010\n1\nlog3 d\n\u0011\nsuch that\nE\n\u0014\nS f\nW\n\u0010\nI −(I −ηS)i\u00112 f\nW ⊤S\n\u0015\n= U\n\u0014\u0010\n1 −(1 −η)k\u00112\nΛ\nf\nW 2\n+ ξ1\n\u0015\nU ⊤\n41\n\n\nE\n\u0014\nS\n\u0010\nI −f\nW\n\u0011\u0010\nI −(I −ηS)i\u00112 f\nW ⊤S\n\u0015\n= U\n\u0014\u0010\n1 −(1 −η)k\u00112\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW + ξ2\n\u0015\nU ⊤\nE\nh\nS f\nW\n\u0010\nI −(I −ηS)i\u0011\nS\ni\n= U\nh\u0010\n1 −(1 −η)k\u0011\nΛ\nf\nW + ξ3\ni\nU ⊤\nE\nh\nS\n\u0010\nI −f\nW\n\u0011\u0010\nI −(I −ηS)i\u0011\nS\ni\n= U\nh\u0010\n1 −(1 −η)k\u0011\u0010\nI −Λ\nf\nW \u0011\n+ ξ4\ni\nU ⊤\nE\nh\nS\n\u0010\nI −(I −ηS)i\u0011\nf\nW ⊤S\ni\n= U\nh\u0010\n1 −(1 −η)k\u0011\nΛ\nf\nW + ξ5\ni\nU ⊤\nE\n\u0002\nS2\u0003\n= U(I + ξ6)U ⊤\nBy Lemma D.4, there exists diagonal matrix ξ7 satisfying ∥ξ7∥op ≤O\n\u0010\n(1 −η)k\u0011\nsuch that\nE\nh\n(I −ηS)k+1\u0010\u0010\nI −(I −ηS)k\u0011\nf\nW ⊤−I\n\u0011i\n= Uξ7U ⊤.\nMoreover, there exist α1, α2 ≤O\n\u0010\n1\nlog3 d\n\u0011\n, α3, α4, α5 ≤O\n\u0010\n1\nd log3 d\n\u0011\nsuch that\nξ2 =\n\u0010\nα1Λ\nf\nW + α2I\n\u0011\u0010\nI −Λ\nf\nW \u0011\n+ tr\n\u0010\nI −Λ\nf\nW \u0011\u0010\nα3Λ\nf\nW + α4I\n\u0011\n+ α5 tr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nI,\nand exist β1 ≤O\n\u0010\n1\nlog3 d\n\u0011\n, β2 ≤O\n\u0010\n1\nd log3 d\n\u0011\nsuch that\nξ4 = β1\n\u0010\nI −Λ\nf\nW \u0011\n+ β2 tr\n\u0010\nI −Λ\nf\nW \u0011\nI.\nWe define ∆eV\ni as the sum of all the interaction terms\n\u0010\nΛ eV + ηI\n\u0011\n(ξ1 −ξ3 −ξ5 + ξ6)+η(ξ2 −ξ4)\nfor the i-th term in the summation of dynamics of eV . From the analysis above, there exist diagonal\nmatrices A eV\ni , B eV\ni , C eV\ni , D eV\ni with their operator norm O\n\u0010\n1\nlog3 d\n\u0011\n, such that (note every matrix is\ndiagonal, so they commute)\n∆\neV\ni =\n\u0010\nΛ\neV + ηI\n\u0011\nA\neV\ni + O\n\u00121\nd\n\u0013\ntr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nB\neV\ni\n+\n\u0010\nI −Λ\nf\nW \u0011\nC\neV\ni + O\n\u00121\nd\n\u0013\ntr\n\u0010\nI −Λ\nf\nW \u0011\nD\neV\ni .\nWe define ∆eV\n−1 as the interaction term brought by Term 7 since there is no summation in Term 7.\nIt is obvious that\n\r\r\r∆eV\n−1\n\r\r\r\nop ≤O\n\u0010\n(1 −η)k\u0011\n.\nNow we denote\nb∆\neV =\nk\nX\ni=0\n∆\neV\ni −∆\neV\n−1\nto be the sum of all interaction term of the dynamics of Λ eV . From the definition of ∆eV\ni and ∆eV\n−1\nabove, there exist diagonal matrices A eV , B eV , C eV , D eV and E eV\n0 satisfying\n\r\r\rA eV \r\r\r,\n\r\r\rC eV \r\r\r ≤\n42\n\n\nO\n\u0010\n1\nlog2 d\n\u0011\n,\n\r\r\rB eV \r\r\r,\n\r\r\rD eV \r\r\r ≤O\n\u0010\n1\nd log2 d\n\u0011\nand\n\r\r\rE eV\n0\n\r\r\r ≤O\n\u0010\n(1 −η)k\u0011\nsuch that (because k =\nΘ(log d))\nb∆\neV =\n\u0010\nΛ\neV + ηI\n\u0011\nA\neV + tr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nB\neV +\n\u0010\nI −Λ\nf\nW \u0011\nC\neV + tr\n\u0010\nI −Λ\nf\nW \u0011\nD\neV + E\neV\n0\nSum up all the seven terms together and we have\n∂L\n∂eV\n= U\n\n\n\nk + 1 −\n2\n\u0010\n1 −(1 −η)k+1\u0011\nη\n+ 1 −(1 −η)2k+2\nη(2 −η)\n\nΛ\nf\nW \u0010\nΛ\neV Λ\nf\nW + ηI\n\u0011\n\nU ⊤\n−U\n\" \nk + 1 −1 −(1 −η)k+1\nη\n!\u0010\nΛ\neV Λ\nf\nW + ηI\n\u0011#\nU ⊤\n−U\n\" \nk + 1 −1 −(1 −η)k+1\nη\n!\nΛ\nf\nW \u0010\nΛ\neV + ηI\n\u0011#\nU ⊤\n+ U\nh\n(k + 1)\n\u0010\nΛ\neV + ηI\n\u0011i\nU ⊤+ U b∆\neV U ⊤\nDenote E eV\n1 to be the sum of all O\n\u0010\n(1 −η)k\u0011\nterms in the dynamics of eV :\nE\neV\n1 =\n \n2(1 −η)k+1\nη\n−(1 −η)2k+2\nη(2 −η)\n!\nΛ\nf\nW \u0010\nΛ\neV Λ\nf\nW + ηI\n\u0011\n−(1 −η)k+1\nη\n\u0010\n2Λ\neV Λ\nf\nW + ηΛ\nf\nW + ηI\n\u0011\nDenote E eV = E eV\n0 + E eV\n1 and denote ∆eV = b∆eV + E eV\n1 , we have\nU ⊤∂L\n∂eV\nU =\n\u0014\u0012\nk + 1 −2\nη +\n1\nη(2 −η)\n\u0013\nΛ\nf\nW 2\n−2\n\u0012\nk + 1 −1\nη\n\u0013\nΛ\nf\nW + (k + 1)I\n\u0015\nΛ\neV\n−1 −η\n2 −ηΛ\nf\nW + I + ∆\neV\nMoreover, ∆eV has the form\n∆\neV =\n\u0010\nΛ\neV + ηI\n\u0011\nA\neV + tr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nB\neV +\n\u0010\nI −Λ\nf\nW \u0011\nC\neV + tr\n\u0010\nI −Λ\nf\nW \u0011\nD\neV + E\neV\nSimilar to the calculation of the dynamics of eV , we can also have\n∂L\n∂f\nW\n=\nk\nX\ni=0\nE\nh\nS eV ⊤(fθ(wi) −wi+1)w⊤\ni\ni\n+ E\nh\nS eV ⊤(wk+1 −w∗)w⊤\nk\ni\n=\nk\nX\ni=0\nE\n\u0014\nS eV ⊤( eV S f\nW + ηS)\n\u0010\nI −(I −ηS)i\u00112\u0015\n−\nk\nX\ni=0\nE\nh\nS eV ⊤\u0010\neV + ηI\n\u0011\nS\n\u0010\nI −(I −ηS)i\u0011i\n−E\nh\nS eV ⊤(I −ηS)k+1\u0010\nI −(I −ηS)k\u0011i\n43\n\n\n=\nk\nX\ni=0\nE\n\u0014\nS eV ⊤\u0010\neV S\n\u0010\nf\nW −I\n\u0011\n+ (V + ηI)S\n\u0011\u0010\nI −(I −ηS)i\u00112\u0015\n−\nk\nX\ni=0\nE\nh\nS eV ⊤\u0010\neV + ηI\n\u0011\nS\n\u0010\nI −(I −ηS)i\u0011i\n−E\nh\nS eV ⊤(I −ηS)k+1\u0010\nI −(I −ηS)k\u0011i\n=\nk\nX\ni=0\nE\n\u0014\nS eV ⊤eV S\n\u0010\nf\nW −I\n\u0011\u0010\nI −(I −ηS)i\u00112\u0015\n+\nk\nX\ni=0\nE\n\u0014\nS eV ⊤\u0010\neV + ηI\n\u0011\nS\n\u0010\nI −(I −ηS)i\u00112\u0015\n−\nk\nX\ni=0\nE\nh\nS eV ⊤\u0010\neV + ηI\n\u0011\nS\n\u0010\nI −(I −ηS)i\u0011i\n−E\nh\nS eV ⊤(I −ηS)k+1\u0010\nI −(I −ηS)k\u0011i\nWe apply Lemma C.16 and Lemma C.17 to each term, similarly define ∆f\nW\ni\nfor i ∈[k] ∪{0} as\nthe sum of all interaction terms for the i-th term in the smmation of dynamics of f\nW . There exists\ndiagonal matrics Af\nW\ni , B f\nW\ni , C f\nW\ni , D f\nW\ni , E f\nW\ni\nwith their operator norm O\n\u0010\n1\nlog3 d\n\u0011\n, such that\n∆\nf\nW\ni\n=\n\u0010\nΛ\neV + ηI\n\u0011\nA\nf\nW\ni\n+\n\u0010\nI −Λ\nf\nW \u0011\nB\nf\nW\ni\n+ O\n\u00121\nd\n\u0013\ntr\n\u0010\nI −Λ\nf\nW \u0011\nC\nf\nW\ni\n+ O\n\u00121\nd\n\u0013\ntr\n\u0010\n(Λ\neV + ηI)Λ\neV \u0011\nD\nf\nW\ni\n+ O\n\u00121\nd\n\u0013\ntr\n\u0010\n(I −Λ\nf\nW )Λ\neV 2\u0011\nE\nf\nW\ni\nWe define ∆f\nW\n−1 as the interaction term brought by the last term\nE\nh\nS eV ⊤(I −ηS)k+1\u0010\nI −(I −ηS)k\u0011i\n.\nIt is clear that\n\r\r\r∆f\nW\n−1\n\r\r\r\nop ≤O\n\u0010\n(1 −η)k\u0011\n. Similarly denote\nb∆\nf\nW =\nk\nX\ni=0\n∆\nf\nW\ni\n−∆\nf\nW\n−1,\nthen there exist diagonal matrices Af\nW , B f\nW , C f\nW , D f\nW , E f\nW , F f\nW\n0\nsatisfying\n\r\r\rAf\nW \r\r\r,\n\r\r\rB f\nW \r\r\r ≤\nO\n\u0010\n1\nlog2 d\n\u0011\n,\n\r\r\rC f\nW \r\r\r,\n\r\r\rD f\nW \r\r\r,\n\r\r\rE f\nW \r\r\r ≤O\n\u0010\n1\nd log2 d\n\u0011\n,\n\r\r\rF f\nW\n0\n\r\r\r ≤O\n\u0010\n(1 −η)k\u0011\nsuch that\nb∆\nf\nW =\n\u0010\nΛ\neV + ηI\n\u0011\nA\nf\nW +\n\u0010\nI −Λ\nf\nW \u0011\nB\nf\nW + tr\n\u0010\nI −Λ\nf\nW \u0011\nC\nf\nW\n+ tr\n\u0010\n(Λ\neV + ηI)Λ\neV \u0011\nD\nf\nW + tr\n\u0010\n(I −Λ\nf\nW )Λ\neV 2\u0011\nE\nf\nW + F\nf\nW\n0 .\nDenote F f\nW\n1\nto be the sum of all O\n\u0010\n(1 −η)k\u0011\nterms in the dynamics of f\nW , F f\nW = F f\nW\n0\n+ F f\nW\n1\nand ∆f\nW = b∆f\nW + F f\nW\n1 . Thus we have\n∂L\n∂f\nW\n=\nk\nX\ni=0\nU\n\u0010\n1 −(1 −η)i\u00112\nΛ\neV \u0010\nΛ\neV Λ\nf\nW + ηI\n\u0011\nU ⊤−\nk\nX\ni=0\nU\n\u0010\n1 −(1 −η)i\u0011\nΛ\neV \u0010\nΛ\neV + ηI\n\u0011\nU ⊤+ U∆\nf\nW U ⊤\n44\n\n\n= U\n\n\n\nk + 1 −\n2\n\u0010\n1 −(1 −η)k+1\u0011\nη\n+ 1 −(1 −η)2k+2\nη(2 −η)\n\nΛ\neV \u0010\nΛ\neV Λ\nf\nW + ηI\n\u0011\n\nU ⊤\n−U\n\" \nk + 1 −1 −(1 −η)k+1\nη\n!\nΛ\neV \u0010\nΛ\neV + ηI\n\u0011#\nU ⊤+ U b∆\nf\nW U ⊤\n= U\n\u0014\u0012\nk + 1 −2\nη +\n1\nη(2 −η)\n\u0013\nΛ\neV 2\nΛ\nf\nW −\n\u0012\nk + 1 −1\nη\n\u0013\nΛ\neV 2\n−1 −η\n2 −ηΛ\neV + ∆\nf\nW\n\u0015\nU ⊤\nMoreover, ∆f\nW has the form\nb∆\nf\nW =\n\u0010\nΛ\neV + ηI\n\u0011\nA\nf\nW +\n\u0010\nI −Λ\nf\nW \u0011\nB\nf\nW + tr\n\u0010\nI −Λ\nf\nW \u0011\nC\nf\nW\n+ tr\n\u0010\n(Λ\neV + ηI)Λ\neV \u0011\nD\nf\nW + tr\n\u0010\n(I −Λ\nf\nW )Λ\neV 2\u0011\nE\nf\nW + F\nf\nW .\nSince ∥A + B∥op ≤∥A∥op + ∥B∥op and ∥AB∥op ≤∥A∥op∥B∥op, it is obvious that\n\r\r\r∆\neV \r\r\r\nop ≤O\n\u0012\n1\nlog2 d\n\u0013\n,\n\r\r\r∆\nf\nW \r\r\r\nop ≤O\n\u0012\n1\nlog2 d\n\u0013\nAfter obtaining the estimation of the gradient by lemma C.5, we can decompose the gradient\nupdates into the dynamics along each eigenspace ui, which can be characterized by the following\nlemma.\nLemma C.6. Suppose eV = Pd\nj=1 λ eV\nj uju⊤\nj , f\nW = Pd\nj=1 λf\nW\nj uju⊤\nj . The dynamics of the eigen-\nvalues of eV and f\nW are given by the following equations:\ndλ eV\nj\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u0015\nλ\neV\nj + 1 −η\n2 −ηλ\nf\nW\nj\n−1 + δ\neV\nj\ndλf\nW\nj\ndt\n=\n\u0012\nk + 1 −1\nη\n\u0013\nλ\neV\nj\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)λ\neV\nj\n2\nλ\nf\nW\nj\n+ 1 −η\n2 −ηλ\neV\nj −δ\nf\nW\nj\nwhere\n\f\f\fδ eV\nj\n\f\f\f ≤O\n\u0010\n1\nlog2 d\n\u0011\n,\n\f\f\fδ f\nW\nj\n\f\f\f ≤O\n\u0010\n1\nlog2 d\n\u0011\n.\nProof. This is directly obtained from Lemma C.5.\nC.3\nProof of the main Theorem 4.1\nIn this section, we prove Theorem 4.1, which characterizes the CoT loss of the trained transformer.\nFirst, we restate the theorem.\n45\n\n\nTheorem C.1 (Global Convergence). Suppose n = Θ(d log5 d), η ∈(0.1, 0.9), k = ⌈c log d⌉,\nc log\n\u0010\n1\n1−η\n\u0011\n> 2. Under Assumption 4.1 with some constant σ > 3(1−η)\n(2−η)\n1\nk+1, if we run gradient flow\non the population loss in Equation (6), then after time t = O\n\u0000log d + log 1\nϵ\n\u0001\n, we have LCoT(t) ≤ϵ\nfor any ϵ ≥Θ\n\u0012\nlog d\nd\nc log(\n1\n1−η)−2\n\u0013\n.\nProof. According to the previous sections, we can reduce the original optimization problem to\nEquation (15), and consider the equivalent reduced model (Definition C.1). By Lemma C.5, we\nfully characterized the gradient expression, which decomposes the gradient of eV and f\nW into\nmain signal terms with large norm at initialization (terms before ∆eV , ∆f\nW ) and interaction terms\n(∆eV , ∆f\nW ) with bounded norm O(\n1\nlog2 d) for all t > 0.\nThe decomposition motivates us to conduct a stage-wise analysis. We first analyze the dynamics\nin Stage 1 when the distance between the parameters eV , f\nW and the ground-truth is larger than\nO(\n1\nlog2 d). In this stage, the bounded error can be dominated by the signal terms in the gradient,\nleading to nearly independent dynamics along each direction ui. After this stage, we enter Stage\n2 as a local convergence phase. We describe the dynamics below in detail.\nStage 1\nIn the first stage, the dynamics are dominated by the main terms, and the interaction\nterms ∆eV , ∆f\nW can be somehow be ignored. Specifically, by Lemma C.6, given the dynamics of\nλ eV\nj , λf\nW\nj :\ndλ eV\nj\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u0015\nλ\neV\nj + 1 −η\n2 −ηλ\nf\nW\nj\n−1 + δ\neV\nj\ndλf\nW\nj\ndt\n=\n\u0012\nk + 1 −1\nη\n\u0013\nλ\neV\nj\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)λ\neV\nj\n2\nλ\nf\nW\nj\n+ 1 −η\n2 −ηλ\neV\nj −δ\nf\nW\nj\nwe can conclude that the dynamics of the eigenvalue λ eV\nj , λf\nW\nj\nmainly depend on themselves when\nthe main term (terms before δ f\nW\nj , δ eV\nj ) are larger than O(\n1\nlog2 d), which is within the stage 1. That is,\nthe dynamics within the subspace uiu⊤\ni for eV , f\nW are almost independent with other subspaces.\nIn this stage, we focus on the analysis of λ eV\nj , λf\nW\nj\ndepending on their own value.\nThe first stage can be further divided into two phases.\nStage 1, Phase 1.\nAt the beginning of training, we have\nλ\neV\nj (0) + 3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj (0)\n\u0011 < −σ + 3(1 −η)\n(2 −η)\n1\nk + 1 < 0\nthen by Lemma C.8, we can prove an upper bound of λ eV\nj when λf\nW\nj\n≤1 −(k + 1)−7\n12,\nλ\neV\nj < −3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n46\n\n\naccording to the dynamics for both sides. With this upper bound, we prove\ndλ f\nW\nj\ndt\n≥O\n\u0000 1\nk\n\u0001\n. There-\nfore, λf\nW\nj\nwill converge to 1 −(k + 1)−7\n12 in t1 = O(log d) time (Lemma C.9).\nStage 1, Phase 2.\nAfter time t1, we have λf\nW\nj\nvery close to the ground-truth value 1. Meanwhile,\nthe lower bound for λ eV\nj still holds, and it will further decrease. Specifically,\nλ\nf\nW\nj (t1) = 1 −(k + 1)−7\n12\nλ\neV\nj (t1) < −3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj (t1)\n\u0011\nBy Lemma C.10, we can prove that λf\nW\nj\nwill stay close to 1 −o(1):\n1 −2(k + 1)−7\n12 < λ\nf\nW\nj (t) < 1 + (k + 1)−7\n12\nfor any t ≥t1. With this condition, a converging condition for (λ eV\nj + η) can be deducted from\nLemma C.11:\nd\n\u0010\nλ eV\nj + η\n\u00112\ndt\n≤−\n1\n2η(2 −η)\n\u0010\nλ\neV\nj + η\n\u00112\nLemma C.11 shows that\n\f\f\fλ eV\nj + η\n\f\f\f converges to (k + 1)−1\n12 in t2 = O(log log d) time.\nStage 2.\nNow the eigenvalues are already close to ground-truth:\n\f\f\fλ\neV\nj (t1 + t2) + η\n\f\f\f = O\n\u0010\n(k + 1)−1\n12\n\u0011\n,\n\f\f\fλ\nf\nW\nj (t1 + t2) −1\n\f\f\f ≤2(k + 1)−7\n12.\nAccording to the expansion of the error terms in Lemma C.5, we notice that δ f\nW\nj\nand δ eV\nj are always\ncoupled with some individual residual like (Λ eV + ηI), (Λf\nW −I), or some weighted average\n1\nd tr\n\u0010\n(Λ eV + ηI)Λ eV \u0011\n. Meanwhile, the coefficient of this kind of residual in the interaction terms\nis still upper bounded by O(1/ log2 d). That helps us to derive the PL-condition like gradient lower\nbound (Lemma C.12):\nd tr\n\u0014\u0010\nΛ eV + ηI\n\u00112\u0015\ndt\n+\nd tr\n\u0014\u0010\nI −Λf\nW \u00112\u0015\ndt\n≤−\n1\n2η(2 −η) tr\n\u0014\u0010\nΛ\neV + ηI\n\u00112\u0015\n−η2\n2 (k + 1) tr\n\u0014\u0010\nI −Λ\nf\nW \u00112\u0015\n+ α\nwhere α = O\n\u0010\n(1 −η)k\u0011\n≥0.\nBy Lemma C.12, we know\n\f\f\fλ eV\nj + η\n\f\f\f and\n\f\f\f1 −λf\nW\nj\n\f\f\f converge to δ ∈\n\u0010\nΘ\n\u0010\nd\nc\n2 log (1−η)+ 1\n2\n\u0011\n, 1\n\u0011\nin\nt3 = O\n\u0000log 1\nδ\n\u0001\ntime. At this time, there exist diagonal matrices A and B satisfying ∥A∥op ≤Θ(1)\nand ∥B∥op ≤Θ(1) such that\nΛ\neV = −ηI + δ · A\nΛ\nf\nW = I + δ · B.\n47\n\n\nNow we consider the CoT loss given by Lemma C.3\nLCoT(θ) = 1\n2EX,w∗\nk−1\nX\ni=0\n\r\r\r( eV S f\nW + ηS)wi −( eV + ηI)Sw∗\r\r\r\n2\n2\n+ 1\n2EX,w∗\n\r\r\r(I + eV S f\nW )wk −( eV S + I)w∗\r\r\r\n2\n2.\nApply Lemma C.13, we directly obtain that\nLCoT(θ) = O\n\u0000δ2d log d\n\u0001\n.\nSince δ ∈\n\u0010\nΘ\n\u0010\nd\nc\n2 log (1−η)+ 1\n2\n\u0011\n, 1\n\u0011\n, the CoT loss is smaller than ϵ = Θ\n\u0000dc log (1−η)+2 log d\n\u0001\n. The\nlocal convergence takes t3 = O\n\u0000log 1\nδ\n\u0001\n= O\n\u0000log 1\nϵ\n\u0001\n. Considering all stages, at time t = t1 + t2 +\nt3 = O(log d) + O\n\u0000 1\nϵ\n\u0001\n, we have\nLCoT(θ) ≤ϵ.\nC.3.1\nTechnical Lemma in Appendix C.3\nLemma C.7. Assume λf\nW\nj\n≤1 −(k + 1)−7\n12, if −3(1−η)\n2(2−η)\n1\n(k+1)\n\u0010\n1−λ f\nW\nj\n\u0011 ≤λ eV\nj < 0, it holds that\nd\n\u0012\nλ eV\nj + 3(1−η)\n2(2−η)\n1\n(k+1)\n\u0010\n1−λ f\nW\nj\n\u0011\n\u0013\ndt\n< 0\n(16)\nProof. Directly consider the derivative\nd\n\u0012\nλ eV\nj + 3(1−η)\n2(2−η)\n1\n(k+1)\n\u0010\n1−λ f\nW\nj\n\u0011\n\u0013\ndt\n= dλ eV\nj\ndt +\n3(1 −η)\n2(k + 1)(2 −η)\n1\n\u0010\n1 −λf\nW\nj\n\u00112\ndλf\nW\nj\ndt\nSubstitute the derivatives with the equations in Lemma C.6, we have\ndλ eV\nj\ndt +\n3(1 −η)\n2(k + 1)(2 −η)\n1\n\u0010\n1 −λf\nW\nj\n\u00112\ndλf\nW\nj\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u0015\nλ\neV\nj + 1 −η\n2 −ηλ\nf\nW\nj\n−\n\u0010\n1 + δ\neV\nj\n\u0011\n+\n3(1 −η)\n2(k + 1)(2 −η)\n1\n\u0010\n1 −λf\nW\nj\n\u00112\n\u0014\u0012\nk + 1 −1\nη\n\u0013\nλ\neV\nj\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)λ\neV\nj\n2\nλ\nf\nW\nj\n+ 1 −η\n2 −ηλ\neV\nj −δ\nf\nW\nj\n\u0015\n48\n\n\nSince −3(1−η)\n2(2−η)\n1\n(k+1)\n\u0010\n1−λ f\nW\nj\n\u0011 ≤λ eV\nj < 0, we have\ndλ eV\nj\ndt +\n3(1 −η)\n2(k + 1)(2 −η)\n1\n\u0010\n1 −λf\nW\nj\n\u00112\ndλf\nW\nj\ndt\n≤\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u00153(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n+ 1 −η\n2 −ηλ\nf\nW\nj\n−\n\u0010\n1 + δ\neV\nj\n\u0011\n+\n3(1 −η)\n2(k + 1)(2 −η)\n1\n\u0010\n1 −λf\nW\nj\n\u00112\n\"\n(k + 1)\n\n3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n\n\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)\n\n3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n\n\n2\nλ\nf\nW\nj\n−δ\nf\nW\nj\n#\n= 3(1 −η)\n2(2 −η)\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nk + 1\n3(1 −η)\nη(2 −η)λ\nf\nW\nj\n+\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011 3(1 −η)\n2η(2 −η)2λ\nf\nW\nj\n2\n+ 1 −η\n2 −ηλ\nf\nW\nj\n−\n\u0010\n1 + δ\neV\nj\n\u0011\n+\n\u00143(1 −η)\n2(2 −η)\n\u00153\n1\n(k + 1)2\u0010\n1 −λf\nW\nj\n\u00113\n+\n\u00143(1 −η)\n2(2 −η)\n\u00153 1 −η\nη(2 −η)λ\nf\nW\nj\n1\n(k + 1)3\u0010\n1 −λf\nW\nj\n\u00114 −3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u00112δ\nf\nW\nj\n=\n\u0014 1 −η\n2(2 −η)\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n−\n1\n2 −η\n\u0015\n+\n1\nk + 1\n3(1 −η)\n2 −η λ\nf\nW\nj\n+\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011 3(1 −η)\n2η(2 −η)2λ\nf\nW\nj\n2\n−δ\neV\nj +\n\u00143(1 −η)\n2(2 −η)\n\u00153\n1\n(k + 1)2\u0010\n1 −λf\nW\nj\n\u00113\n+\n\u00143(1 −η)\n2(2 −η)\n\u00153 1 −η\nη(2 −η)λ\nf\nW\nj\n1\n(k + 1)3\u0010\n1 −λf\nW\nj\n\u00114 −3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u00112δ\nf\nW\nj\nPut in the assumption on λf\nW\nj\nthat λf\nW\nj\n≤1 −(k + 1)−7\n12, we have\ndλ eV\nj\ndt +\n3(1 −η)\n2(k + 1)(2 −η)\n1\n\u0010\n1 −λf\nW\nj\n\u00112\ndλf\nW\nj\ndt\n≤−\n1 + η\n2(2 −η) +\n1\nk + 1\n3(1 −η)\nη(2 −η) +\n1\n(k + 1)\n5\n12\n3(1 −η)\n2η(2 −η)2 +\n\f\f\fδ\neV\nj\n\f\f\f +\n\u00143(1 −η)\n2(2 −η)\n\u00153\n1\n(k + 1)\n1\n4\n+\n\u00143(1 −η)\n2(2 −η)\n\u00153 1 −η\nη(2 −η)\n1\n(k + 1)\n2\n3 + 3(1 −η)\n2(2 −η)\n\f\f\fδ\nf\nW\nj\n\f\f\f\n49\n\n\n= −\n1 + η\n2(2 −η) + O\n \n1\nlog\n1\n4 d\n!\nLemma C.8 (Upper bound of λ eV\nj ). Under Assumption 4.1, if λf\nW\nj\n≤1 −(k + 1)−7\n12, it holds that\nλ\neV\nj < −3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n(17)\nProof. We prove by induction. First, check the initialization λ eV\nj (0) ≤−σ, σ ≤λf\nW\nj (0) ≤1\n2. If\nσ ≥3(1−η)\n2−η\n1\nk+1, then we have\nλ\neV\nj (0) + 3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj (0)\n\u0011 < −σ + 3(1 −η)\n(2 −η)\n1\nk + 1 ≤0\nIf the inequality holds until some time t1, that is for any t < t1, we have\nλ\neV\nj (t) < −3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj (t)\n\u0011\nbut\nλ\neV\nj (t1) ≥−3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj (t1)\n\u0011\nBy Lemma C.7, we have\nd\n\u0012\nλ eV\nj + 3(1−η)\n2(2−η)\n1\n(k+1)\n\u0010\n1−λ f\nW\nj\n\u0011\n\u0013\ndt\n\f\f\f\f\f\f\f\f\nt=t1\n< 0\nTherefore, there exists some time t′ < t1 such that\nλ\neV\nj (t′) ≥−3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj (t′)\n\u0011\nwhich is a contradiction. Hence, the proof is complete.\nLemma C.9 (λf\nW\nj\nconverges to near optimal). Under Assumption 4.1, it takes O(log d) time for\nλf\nW\nj\nto converge to 1 −(k + 1)−7\n12.\nProof. Recall the gradient of λf\nW\nj\nin Lemma C.6\ndλf\nW\nj\ndt\n=\n\u0012\nk + 1 −1\nη\n\u0013\nλ\neV\nj\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)λ\neV\nj\n2\nλ\nf\nW\nj\n+ 1 −η\n2 −ηλ\neV\nj −δ\nf\nW\nj\n50\n\n\nSubstitute λ eV\nj with Lemma C.8, we have\ndλf\nW\nj\ndt\n≥\n\u0012\nk + 1 −1\nη\n\u0013\n3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n\n\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)\n\n3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n\n\n2\nλ\nf\nW\nj\n−1 −η\n2 −η\n\n3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n\n−δ\nf\nW\nj\n≥4\n5(k + 1)\n\n3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n\n\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n−1 −η\n2 −η\n\n3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n\n−\n\f\f\fδ\nf\nW\nj\n\f\f\f\n= 3\n10\n(1 −η)2\n(2 −η)2\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011 −\n\f\f\fδ\nf\nW\nj\n\f\f\f\n≥1\n5\n(1 −η)2\n(2 −η)2\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj\n\u0011\n≥1\n5\n(1 −η)2\n(2 −η)2\n1\nk + 1\nIn O(log d) time, λf\nW\nj\ncan converge to 1 −(k + 1)−7\n12.\nLemma C.10. Assume λf\nW\nj (t1) = 1 −(k + 1)−7\n12 and λ eV\nj (t1) < −3(1−η)\n2(2−η)\n1\n(k+1)\n\u0010\n1−λ f\nW\nj\n(t1)\n\u0011, for any\nt ≥t1 it holds that\n1 −2(k + 1)−7\n12 < λ\nf\nW\nj (t) < 1 + (k + 1)−7\n12.\nProof. First, it is clear that the inequality holds at time t1. If the inequality doesn’t hold, then there\nexists t′ > t1 such that\n1 −2(k + 1)−7\n12 < λ\nf\nW\nj (t) < 1 + (k + 1)−7\n12\nfor any t1 ≤t < t′\nλ\nf\nW\nj (t′) = 1 −2(k + 1)−7\n12\nor\n1 −2(k + 1)−7\n12 < λ\nf\nW\nj (t) < 1 + (k + 1)−7\n12\nfor any t1 ≤t < t′\nλ\nf\nW\nj (t′) = 1 + (k + 1)−7\n12\n51\n\n\nIn the first case, it suffices to prove\nλ\neV\nj (t′) ≤−3(1 −η)\n2(2 −η)(k + 1)−5\n12 < −3(1 −η)\n2(2 −η)\n1\n(k + 1)\n\u0010\n1 −λf\nW\nj (t′)\n\u0011\nto show\ndλf\nW\nj\ndt\n\f\f\f\f\f\nt=t′\n> 0\nwhich says there exists t1 ≤t′′ < t′ such that\nλ\nf\nW\nj (t′′) ≤1 −2(k + 1)−7\n12\nand leads to a contradiction. Recall the gradient of λeV\nj in Lemma C.6\ndλ eV\nj\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u0015\nλ\neV\nj + 1 −η\n2 −ηλ\nf\nW\nj\n−\n\u0010\n1 + δ\neV\nj\n\u0011\n≤−\n\u0014\n4(k + 1)−1\n6 + 4\nη\nh\n(k + 1)−7\n12 + (k + 1)−7\n6\ni\n+\n1\nη(2 −η)\nh\n1 + 2(k + 1)−7\n12 + (k + 1)−7\n6\ni\u0015\nλ\neV\nj\n−\n1\n2 −η + 1 −η\n2 −η(k + 1)−7\n12 +\n\f\f\fδ\neV\nj\n\f\f\f\n≤−\n2\nη(2 −η)λ\neV\nj −\n1\n2(2 −η)\nand thus we have\nλ\neV\nj (t) ≤Ce−\n2\nη(2−η)(t−t1) −η\n4\nIf C ≤0, then\nλ\neV\nj (t′) ≤−η\n4 ≤−3(1 −η)\n2(2 −η)(k + 1)−5\n12\nelse\nλ\neV\nj (t′) ≤λ\neV\nj (t1) = −3(1 −η)\n2(2 −η)(k + 1)−5\n12\nIn the second case,\nλ\neV\nj (t) ≤−3(1 −η)\n2(2 −η)(k + 1)−5\n12\nstill holds for any t1 ≤t ≤t′. Recall the gradient of λf\nW\nj\nin Lemma C.6\ndλf\nW\nj\ndt\n\f\f\f\f\f\nt=t′\n=\n\u0012\nk + 1 −1\nη\n\u0013\nλ\neV\nj\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)λ\neV\nj\n2\nλ\nf\nW\nj\n+ 1 −η\n2 −ηλ\neV\nj −δ\nf\nW\nj\n= −\n\u0012\nk + 1 −1\nη\n\u0013\nλ\neV\nj\n2\n(k + 1)−7\n12 +\n1 −η\nη(2 −η)λ\neV\nj\n2h\n1 + (k + 1)−7\n12\ni\n+ 1 −η\n2 −ηλ\neV\nj −δ\nf\nW\nj\n≤−(k + 1)\n5\n12\n2\nλ\neV\nj\n2\n+\n1 −η\n2(2 −η)λ\neV\nj +\n\f\f\fδ\nf\nW\nj\n\f\f\f\n52\n\n\n≤−9(1 −η)2\n8(2 −η)2(k + 1)−5\n12 −3(1 −η)2\n4(2 −η)2(k + 1)−5\n12 +\n\f\f\fδ\nf\nW\nj\n\f\f\f\n≤−(1 −η)2\n(2 −η)2(k + 1)−5\n12\nThere exists t1 ≤t′′ < t′ such that\nλ\nf\nW\nj (t′′) ≥1 + (k + 1)−7\n12\nwhich is a contradiction. Hence, the proof is complete.\nLemma C.11 (λ eV\nj converges to near optimal). Assume\n1 −2(k + 1)−7\n12 < λ\nf\nW\nj (t) < 1 + (k + 1)−7\n12\nthen it takes O(log log d) time for\n\f\f\fλ eV\nj + η\n\f\f\f to converge to (k + 1)−1\n12.\nProof. From Lemma C.10, we know\ndλ eV\nj\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u0015\nλ\neV\nj + 1 −η\n2 −ηλ\nf\nW\nj\n−\n\u0010\n1 + δ\neV\nj\n\u0011\n≤−\n\u0014\n4(k + 1)−1\n6 + 4\nη\nh\n(k + 1)−7\n12 + (k + 1)−7\n6\ni\n+\n1\nη(2 −η)\nh\n1 + 2(k + 1)−7\n12 + (k + 1)−7\n6\ni\u0015\nλ\neV\nj\n−\n1\n2 −η + 1 −η\n2 −η(k + 1)−7\n12 +\n\f\f\fδ\neV\nj\n\f\f\f\n= −\n\u0014\n1\nη(2 −η) + O\n\u0010\n(k + 1)−1\n6\n\u0011\u0015\u0010\nλ\neV\nj + η\n\u0011\n+ O\n\u0010\n(k + 1)−1\n6\n\u0011\nand\ndλ eV\nj\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u0015\nλ\neV\nj + 1 −η\n2 −ηλ\nf\nW\nj\n−\n\u0010\n1 + δ\neV\nj\n\u0011\n≥−\n\u0014\n−2\nη\nh\n(k + 1)−7\n12 + (k + 1)−7\n6\ni\n+\n1\nη(2 −η)\nh\n1 −4(k + 1)−7\n12 + 4(k + 1)−7\n6\ni\u0015\nλ\neV\nj\n−\n1\n2 −η −2(1 −η)\n2 −η (k + 1)−7\n12 −\n\f\f\fδ\neV\nj\n\f\f\f\n= −\n\u0014\n1\nη(2 −η) + O\n\u0010\n(k + 1)−1\n6\n\u0011\u0015\u0010\nλ\neV\nj + η\n\u0011\n+ O\n\u0010\n(k + 1)−1\n6\n\u0011\nTherefore,\nd\n\u0010\nλ eV\nj + η\n\u00112\ndt\n= 2\n\u0010\nλ\neV\nj + η\n\u0011dλ eV\nj\ndt\n≤−\n\u0014\n1\nη(2 −η) + O\n\u0010\n(k + 1)−1\n6\n\u0011\u0015\u0010\nλ\neV\nj + η\n\u00112\n+ O\n\u0010\n(k + 1)−1\n6\n\u0011\u0010\nλ\neV\nj + η\n\u0011\n53\n\n\nIf\n\f\f\fλ eV\nj + η\n\f\f\f converges to ϵ = (k + 1)−1\n12, then\nd\n\u0010\nλ eV\nj + η\n\u00112\ndt\n≤−\n1\n2η(2 −η)\n\u0010\nλ\neV\nj + η\n\u00112\nThus, there exists c ≤Θ(1) such that\nϵ2 =\n\u0010\nλ\neV\nj + η\n\u00112\n≤c2 exp\n\u0012\n−\n1\n2η(2 −η)t\n\u0013\nIn O\n\u0000log\n\u0000 1\nϵ\n\u0001\u0001\n= O(log log d) time,\n\f\f\fλ eV\nj + η\n\f\f\f can converge to ϵ.\nLemma C.12 (Local convergence). Suppose k = ⌈c log d⌉. Assume\n\f\f\fλ\nf\nW\nj (t) −1\n\f\f\f ≤2(k + 1)−7\n12\n\f\f\fλ\neV\nj (t) + η\n\f\f\f = O\n\u0010\n(k + 1)−1\n12\n\u0011\n,\nthen there exists α = O\n\u0010\n(1 −η)k\u0011\n≥0 such that Λ eV and Λf\nW comply with\nd tr\n\u0014\u0010\nΛ eV + ηI\n\u00112\u0015\ndt\n+\nd tr\n\u0014\u0010\nI −Λf\nW \u00112\u0015\ndt\n≤−\n1\n2η(2 −η) tr\n\u0014\u0010\nΛ\neV + ηI\n\u00112\u0015\n−η2\n2 (k + 1) tr\n\u0014\u0010\nI −Λ\nf\nW \u00112\u0015\n+ α,\nthus\n\f\f\fλ eV\nj + η\n\f\f\f and\n\f\f\f1 −λf\nW\nj\n\f\f\f can converge to ϵ ∈\n\u0010\nd−c\n2 log(\n1\n1−η)+ 1\n2, 1\n\u0011\nin O\n\u0000log 1\nϵ\n\u0001\ntime.\nProof. Consider the error term in Lemma C.6 more carefully, we have\ndλ eV\nj\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\u0015\u0010\nλ\neV\nj + η\n\u0011\n+ η(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+\n\u00123 −2η\n2 −η λ\nf\nW\nj\n−1\n\u0013\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n\u0010\nλ\neV\nj + η\n\u0011\nO\n\u0012\n1\nlog2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\nO\n\u0012\n1\nlog2 d\n\u0013\n+ tr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ tr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ O\n\u0010\n(1 −η)k\u0011\nand\ndλf\nW\nj\ndt\n=\n\u0012\nk + 1 −2\nη +\n1\nη(2 −η)\n\u0013\nλ\neV\nj\n2\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1 −η\nη(2 −η)λ\neV\nj\n\u0010\nλ\neV\nj + η\n\u0011\n54\n\n\n+\n\u0010\nλ\neV\nj + η\n\u0011\nO\n\u0012\n1\nlog2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\nO\n\u0012\n1\nlog2 d\n\u0013\n+ tr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ tr\n\u0010\n(Λ\neV + ηI)Λ\neV \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ tr\n\u0010\n(I −Λ\nf\nW )Λ\neV 2\u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ O\n\u0010\n(1 −η)k\u0011\nNow we consider the decay rate of the distance between λ eV\nj , λf\nW\nj\nand their ground truth.\nd\n\u0010\nλ eV\nj + η\n\u00112\ndt\n+\nd\n\u0010\nλf\nW\nj\n−1\n\u00112\ndt\n= −\n\u0014\n(k + 1)\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ 2\nηλ\nf\nW\nj\n\u0010\n1 −λ\nf\nW\nj\n\u0011\n+\n1\nη(2 −η)λ\nf\nW\nj\n2\n+ O\n\u0012\n1\nlog2 d\n\u0013\u0015\u0010\nλ\neV\nj + η\n\u00112\n+\n\u00123 −2η\n2 −η λ\nf\nW\nj\n−\n1 −η\nη(2 −η)λ\neV\nj −1 + O\n\u0012\n1\nlog2 d\n\u0013\u0013\u0010\n1 −λ\nf\nW\nj\n\u0011\u0010\nλ\neV\nj + η\n\u0011\n−\n\u0014\u0012\nk + 1 −2\nη +\n1\nη(2 −η)\n\u0013\nλ\neV\nj\n2\n−η(k + 1)\n\u0010\nλ\neV\nj + η\n\u0011\n+ O\n\u0012\n1\nlog2 d\n\u0013\u0015\u0010\n1 −λ\nf\nW\nj\n\u00112\n+\n\u0010\nλ\neV\nj + η\n\u0011\ntr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\nλ\neV\nj + η\n\u0011\ntr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\u0010\nΛ\neV + ηI\n\u0011\nΛ\neV \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\neV 2\u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ O\n\u0010\n(1 −η)k\u0011\n= −\n\"\n1\nη(2 −η) + O\n \n1\nlog\n1\n6 d\n!#\u0010\nλ\neV\nj + η\n\u00112\n+\n \n2(1 −η)\n2 −η\n+ O\n \n1\nlog\n1\n12 d\n!!\u0010\n1 −λ\nf\nW\nj\n\u0011\u0010\nλ\neV\nj + η\n\u0011\n−\nh\nη2(k + 1) + O\n\u0010\nk\n11\n12\n\u0011i\u0010\n1 −λ\nf\nW\nj\n\u00112\n+\n\u0010\nλ\neV\nj + η\n\u0011\ntr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\nλ\neV\nj + η\n\u0011\ntr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\u0010\nΛ\neV + ηI\n\u0011\nΛ\neV \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\neV 2\u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ O\n\u0010\n(1 −η)k\u0011\nUtilizing Mean Inequality, we have\n\f\f\f\n\u0010\n1 −λ\nf\nW\nj\n\u0011\u0010\nλ\neV\nj + η\n\u0011\f\f\f ≤\n1\n2\n√\nk\n\u0010\nλ\neV\nj + η\n\u00112\n+\n√\nk\n2\n\u0010\n1 −λ\nf\nW\nj\n\u00112\n55\n\n\nInsert the inequality into the equation and we have\nd\n\u0010\nλ eV\nj + η\n\u00112\ndt\n+\nd\n\u0010\nλf\nW\nj\n−1\n\u00112\ndt\n≤−\n\"\n1\nη(2 −η) + O\n \n1\nlog\n1\n6 d\n!#\u0010\nλ\neV\nj + η\n\u00112\n−\nh\nη2(k + 1) + O\n\u0010\nk\n11\n12\n\u0011i\u0010\n1 −λ\nf\nW\nj\n\u00112\n+\n\u0010\nλ\neV\nj + η\n\u0011\ntr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\nλ\neV\nj + η\n\u0011\ntr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\nI −Λ\nf\nW \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\u0010\nΛ\neV + ηI\n\u0011\nΛ\neV \u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+\n\u0010\n1 −λ\nf\nW\nj\n\u0011\ntr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\neV 2\u0011\nO\n\u0012\n1\nd log2 d\n\u0013\n+ O\n\u0010\n(1 −η)k\u0011\nThere exist α1, α2, α3, α4, α5 = O\n\u0010\n1\nd log2 d\n\u0011\n≥0 and α6 = O\n\u0010\n(1 −η)k\u0011\n≥0 such that\nd\n\u0010\nλ eV\nj + η\n\u00112\ndt\n+\nd\n\u0010\nλf\nW\nj\n−1\n\u00112\ndt\n≤−\n\"\n1\nη(2 −η) + O\n \n1\nlog\n1\n6 d\n!#\u0010\nλ\neV\nj + η\n\u00112\n−\nh\nη2(k + 1) + O\n\u0010\nk\n11\n12\n\u0011i\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ α1\n\f\f\fλ\neV\nj + η\n\f\f\f tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+ α2\n\f\f\fλ\neV\nj + η\n\f\f\f ·\n\f\f\ftr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\nf\nW \u0011\f\f\f\n+ α3\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+ α4\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f ·\n\f\f\ftr\n\u0010\u0010\nΛ\neV + ηI\n\u0011\nΛ\neV \u0011\f\f\f\n+ α5\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f ·\n\f\f\ftr\n\u0010\u0010\nI −Λ\nf\nW \u0011\nΛ\neV 2\u0011\f\f\f + α6.\nNotice that for diagonal matrices A and B, we have\ntr (AB) ≤|tr (AB)| =\n\f\f\f\f\f\nX\ni\naiibii\n\f\f\f\f\f ≤\nX\ni\n|aii||bii| ≤\nX\ni\n|aii|∥B∥= tr(|A|)∥B∥\nPlug in the inequality and we have\nd\n\u0010\nλ eV\nj + η\n\u00112\ndt\n+\nd\n\u0010\nλf\nW\nj\n−1\n\u00112\ndt\n≤−\n\"\n1\nη(2 −η) + O\n \n1\nlog\n1\n6 d\n!#\u0010\nλ\neV\nj + η\n\u00112\n−\nh\nη2(k + 1) + O\n\u0010\nk\n11\n12\n\u0011i\u0010\n1 −λ\nf\nW\nj\n\u00112\n+ α1\n\f\f\fλ\neV\nj + η\n\f\f\f tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+ α2\n\f\f\fλ\neV\nj + η\n\f\f\f tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n·\n\r\r\rΛ\nf\nW \r\r\r\n+ α3\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+ α4\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f tr\n\u0010\f\f\fΛ\neV + ηI\n\f\f\f\n\u0011\n·\n\r\r\rΛ\neV \r\r\r\n+ α5\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n·\n\r\r\rΛ\neV 2\r\r\r + α6\n56\n\n\n= −\n\"\n1\nη(2 −η) + O\n \n1\nlog\n1\n6 d\n!#\u0010\nλ\neV\nj + η\n\u00112\n−\nh\nη2(k + 1) + O\n\u0010\nk\n11\n12\n\u0011i\u0010\n1 −λ\nf\nW\nj\n\u00112\n+\n\u0010\nα1 + α2\n\r\r\rΛ\nf\nW \r\r\r\n\u0011\n·\n\f\f\fλ\neV\nj + η\n\f\f\f · tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+ α4\n\r\r\rΛ\neV \r\r\r ·\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f · tr\n\u0010\f\f\fΛ\neV + ηI\n\f\f\f\n\u0011\n+\n\u0010\nα3 + α5\n\r\r\rΛ\neV 2\r\r\r\n\u0011\n·\n\f\f\f1 −λ\nf\nW\nj\n\f\f\f · tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+ α6\nTake the sum of both sides separately, we have\nd tr\n\u0014\u0010\nΛ eV + ηI\n\u00112\u0015\ndt\n+\nd tr\n\u0014\u0010\nI −Λf\nW \u00112\u0015\ndt\n≤−\n\"\n1\nη(2 −η) + O\n \n1\nlog\n1\n6 d\n!#\ntr\n\u0014\u0010\nΛ\neV + ηI\n\u00112\u0015\n−\nh\nη2(k + 1) + O\n\u0010\nk\n11\n12\n\u0011i\ntr\n\u0014\u0010\nI −Λ\nf\nW \u00112\u0015\n+\n\u0010\nα1 + α2\n\r\r\rΛ\nf\nW \r\r\r + α4\n\r\r\rΛ\neV \r\r\r\n\u0011\n· tr\n\u0010\f\f\fΛ\neV + ηI\n\f\f\f\n\u0011\n· tr\n\u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+\n\u0010\nα3 + α5\n\r\r\rΛ\neV 2\r\r\r\n\u0011\n· tr2 \u0010\f\f\fI −Λ\nf\nW \f\f\f\n\u0011\n+ α6\nFrom Jensen’s Inequality with f(x) = x2, we have\n Pd\ni=1 λi\nd\n!2\n≤\nPd\ni=1 λ2\ni\nd\n.\nTherefore, it holds for diagonal matrix Λ ∈Rd×d that\ntr2 (Λ) ≤d tr\n\u0000Λ2\u0001\nPlug in the inequality and we have\nd tr\n\u0014\u0010\nΛ eV + ηI\n\u00112\u0015\ndt\n+\nd tr\n\u0014\u0010\nI −Λf\nW \u00112\u0015\ndt\n≤−\n1\n2η(2 −η) tr\n\u0014\u0010\nΛ\neV + ηI\n\u00112\u0015\n−η2\n2 (k + 1) tr\n\u0014\u0010\nI −Λ\nf\nW \u00112\u0015\n+ α6\nBecause k = ⌈c log d⌉, we have O\n\u0010\nd(1 −η)k\u0011\n= d−c log(\n1\n1−η)+1. So in O\n\u0000log 1\nϵ\n\u0001\ntime,\n\f\f\fλ eV\nj + η\n\f\f\f\nand\n\f\f\f1 −λ eV\nj\n\f\f\f converge to ϵ ∈\n\u0010\nΘ\n\u0010\nd\nc\n2 log (1−η)+ 1\n2\n\u0011\n, 1\n\u0011\n.\nLemma C.13. Suppose δ ∈\n\u0010\nΘ\n\u0010\nd\nc\n2 log (1−η)+ 1\n2\n\u0011\n, 1\n\u0011\nand there exist diagonal matrices A and B\nsatisfying ∥A∥op ≤Θ(1) and ∥B∥op ≤Θ(1) such that\nΛ\neV = −ηI + δ · A\nΛ\nf\nW = I + δ · B,\nthen it holds that\nLCoT(θ) = O\n\u0000δ2d log d\n\u0001\n.\n57\n\n\nProof. Now we consider the CoT loss given by Lemma C.3\nLCoT(θ) = 1\n2EX,w∗\nk−1\nX\ni=0\n\r\r\r( eV S f\nW + ηS)wi −( eV + ηI)Sw∗\r\r\r\n2\n2\n+ 1\n2EX,w∗\n\r\r\r(I + eV S f\nW )wk −( eV S + I)w∗\r\r\r\n2\n2.\nPlug in the expression of Λ eV and Λf\nW , we get\nLCoT(θ) = δ2\n2 E\nk−1\nX\ni=0\n\r\r\r(AS −ηSB + δASB)\n\u0010\nI −(I −ηS)i\u0011\n−AS\n\r\r\r\n2\nF\n(18)\n+ 1\n2E\n\r\r\r−(I −ηS)k + Λ\neV S\nh\n−(I −ηS)k + δB\n\u0010\nI −(I −ηS)k\u0011i\r\r\r\n2\nF.\n(19)\nWe first consider the term in the summation:\nE\n\r\r\r(AS −ηSB + δASB)\n\u0010\nI −(I −ηS)i\u0011\n−AS\n\r\r\r\n2\nF\n= E\n\r\r\r(−ηSB + δASB)\n\u0010\nI −(I −ηS)i\u0011\n−AS(I −ηS)i\r\r\r\n2\nF\n= tr E\n\u0014\n(−ηSB + δASB)\n\u0010\nI −(I −ηS)i\u00112\n(−ηBS + δBSA)\n\u0015\n−2 tr E\nh\n(−ηSB + δASB)\n\u0010\nI −(I −ηS)i\u0011\n(I −ηS)iSA\ni\n+ tr E\nh\nAS(I −ηS)2iSA\ni\n= tr\n\u0012\n(−ηI + δA)E\n\u0014\nSB\n\u0010\nI −(I −ηS)i\u00112\nBS\n\u0015\n(−ηI + δA)\n\u0013\n(Term 1)\n−2 tr\n\u0010\n(−ηI + δA)E\nh\nSB\n\u0010\nI −(I −ηS)i\u0011\n(I −ηS)iS\ni\nA\n\u0011\n(Term 2)\n+ tr\n\u0010\nAE\nh\nS(I −ηS)2iS\ni\nA\n\u0011\n(Term 3)\nApple Lemma C.15 to the expectation in Term 1, we have\nE\n\u0014\nSB\n\u0010\nI −(I −ηS)i\u00112\nBS\n\u0015\n=\n\u0010\n1 −(1 −η)i\u00112\nB2 + O\n\u0012\n1\nlog3 d\n\u0013\u0014\nB2 + O\n\u00121\nd\n\u0013\ntr (B)B + O\n\u00121\nd\n\u0013\ntr\n\u0000B2\u0001\nI + O\n\u0012 1\nd2\n\u0013\ntr2 (B)I\n\u0015\n.\nIt is obvious that\n\r\r\r\rE\n\u0014\nSB\n\u0010\nI −(I −ηS)i\u00112\nBS\n\u0015\r\r\r\r\nop\n≤Θ(1).\nTherefore, for Term 1 we have\ntr\n\u0012\n(−ηI + δA)E\n\u0014\nSB\n\u0010\nI −(I −ηS)i\u00112\nBS\n\u0015\n(−ηI + δA)\n\u0013\n≤d∥−ηI + δA∥2\nop ·\n\r\r\r\rE\n\u0014\nSB\n\u0010\nI −(I −ηS)i\u00112\nBS\n\u0015\r\r\r\r\nop\n≤O(d).\n(all matrices in the inequality are diagonal matrices.)\n58\n\n\nSimilarly, for Term 2 and Term 3, we have\n\f\f\ftr\n\u0010\n(−ηI + δA)E\nh\nSB\n\u0010\nI −(I −ηS)i\u0011\n(I −ηS)iS\ni\nA\n\u0011\f\f\f ≤O(d)\ntr\n\u0010\nAE\nh\nS(I −ηS)2iS\ni\nA\n\u0011\n≤O(d).\nAdd Term 1, 2, 3 together and we have\nE\n\r\r\r(AS −ηSB + δASB)\n\u0010\nI −(I −ηS)i\u0011\n−AS\n\r\r\r\n2\nF ≤O(d).\nWe then consider the second term in Equation (19):\nE\n\r\r\r−(I −ηS)k + Λ\neV S\nh\n−(I −ηS)k + δB\n\u0010\nI −(I −ηS)k\u0011i\r\r\r\n2\nF\n= E\n\r\r\r−\n\u0010\nI + Λ\neV S\n\u0011\n(I −ηS)k + δΛ\neV SB\n\u0010\nI −(I −ηS)k\u0011\r\r\r\n2\nF\n= tr\n\u0010\nE\nh\u0010\nI + Λ\neV S\n\u0011\n(I −ηS)2k\u0010\nI + SΛ\neV \u0011i\u0011\n−2δ tr\n\u0010\nE\nh\u0010\nI + Λ\neV S\n\u0011\n(I −ηS)k\u0010\nI −(I −ηS)k\u0011\nBS\ni\nΛ\neV \u0011\n+ δ2 tr\n\u0012\nΛ\neV E\n\u0014\nSB\n\u0010\nI −(I −ηS)k\u00112\nBS\n\u0015\nΛ\neV\n\u0013\n≤O\n\u0000δ2d\n\u0001\n.\n((1 −η)k ≤δ)\nRecall the CoT loss in Equation (18) and Equation (19). By the analysis above, we directly obtain\nthat\nLCoT(θ) = O\n\u0000δ2d log d\n\u0001\n.\nHence, the proof is complete.\nLemma C.14. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1), ∥Γ∥op ≤Θ(1). Then the expectation\nE\nh\nSΛ\n\u0010\nI −(I −ηS)k\u0011\nΓS\ni\n=\n\u0010\n1 −(1 −η)k\u0011\nΛΓ + ∆,\nwhere ∥∆∥op = O(k2d\nn ) ≤O\n\u0010\n1\nlog3 d\n\u0011\n. Moreover, the error is in the form\n∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ + α4 tr(Λ) tr(Γ)I + α5 tr(ΛΓ)I\nwhere α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3, α5 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nProof. We can directly get the lemma by applying Lemma D.2 to E[SΛΓS], E\nh\nSΛ(I −ηS)kΓS\ni\n.\n59\n\n\nLemma C.15. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1), ∥Γ∥op ≤Θ(1). Then the expectation\nE\n\u0014\nSΛ\n\u0010\nI −(I −ηS)k\u00112\nΓS\n\u0015\n=\n\u0010\n1 −(1 −η)k\u00112\nΛΓ + ∆,\nwhere ∥∆∥op = O(k2d\nn ) ≤O\n\u0010\n1\nlog3 d\n\u0011\n. Moreover, the error is in the form\n∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ + α4 tr(Λ) tr(Γ)I + α5 tr(ΛΓ)I\nwhere α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3, α5 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nProof. We can directly get the lemma by applying Lemma D.2 to E[SΛΓS], E\nh\nSΛ(I −ηS)kΓS\ni\nand E\nh\nSΛ(I −ηS)2kΓS\ni\n.\nLemma C.16. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1), ∥Γ∥op ≤Θ(1). Then the expectation\nE\nh\nSΛSΓ\n\u0010\nI −(I −ηS)k\u0011i\n=\n\u0010\n1 −(1 −η)k\u0011\nΛΓ + ∆,\nwhere ∥∆∥op = O(k2d\nn ) ≤O\n\u0010\n1\nlog3 d\n\u0011\n. Moreover, the error is in the form\n∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ + α4 tr(Λ) tr(Γ)I + α5 tr(ΛΓ)I\nwhere α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3, α5 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nProof. We can directly get the lemma by applying Lemma D.3 to E[SΛSΓ], E\nh\nSΛSΓ(I −ηS)ki\n.\nLemma C.17. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1), ∥Γ∥op ≤Θ(1). Then the expectation\nE\n\u0014\nSΛSΓ\n\u0010\nI −(I −ηS)k\u00112\u0015\n=\n\u0010\n1 −(1 −η)k\u00112\nΛΓ + ∆,\nwhere ∥∆∥op = O(k2d\nn ) ≤O\n\u0010\n1\nlog3 d\n\u0011\n. Moreover, the error is in the form\n∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ + α4 tr(Λ) tr(Γ)I + α5 tr(ΛΓ)I\nwhere α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3, α5 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nProof. We can directly get the lemma by applying Lemma D.3 to E[SΛSΓ], E\nh\nSΛSΓ(I −ηS)ki\nand E\nh\nSΛSΓ(I −ηS)2ki\n.\n60\n\n\nC.4\nOut-of-distribution Generalization\nWe restate the formal theorem here. We still denote S := 1\nnXX⊤for simplicity. Note that the\nnumber of steps k can be different/larger compared to the step number in the previous training\ntheorem.\nTheorem C.2. Suppose n = Θ(d log5 d), η ∈(0.1, 0.9), k = C log d.\nAssume the out-of-\ndistribution input data xi ∼N(0d, Σ), i ∈[n] where δ\nη ≤λmin(Σ) ≤λmax(Σ) ≤\n2−δ\nη\nfor\nsome constant δ > 0.1, and w∗∼N(0d, I). Then the trained transformer in Theorem 4.1 satisfies\nthat LEval\nΣ\n(t) ≤ϵ for any ϵ ∈\n\u0010\nd−C log(min{\n1\n1−η ,\n1\n1−δ })+1 log2 d, 1\n\u0011\n.\nProof. Recall the definition of the evaluation loss and our reduced transformer (Definition C.1)\nLEval(V , W ) = 1\n2EX,w∗\n\u0014\r\r\rfLSA( ˆZk)[:,−1] −(0d, 0, w∗, 1)\n\r\r\r\n2\u0015\n= 1\n2E\n\u0002\n∥fθ( ˆwk) −w∗∥2\u0003\nwhere ˆZk is the generated sequence after k steps and ˆwk := fθ( ˆwk−1) is the k-th generated inter-\nmediate weight vector. Note that each step the transformer is inputted with the last step prediction.\nWe define the prediction error at each step i is ∆wi := ˆwi −wi = fθ( ˆwi−1) −wi. We expand the\nterm fθ( ˆwk) −w∗and sum up the error accumulation as follows:\nfθ( ˆwk) −w∗≤(wk+1 −w∗) + (fθ( ˆwk) −wk+1)\n= (wk+1 −w∗) + ˆwk + eV S(f\nW ˆwk −w∗) −wk+1\n≤(wk+1 −w∗) +\n\u0010\nwk + eV S(f\nW wk −w∗) −wk+1\n\u0011\n+\n\u0010\nI + eV S f\nW\n\u0011\n∆wk.\nAfter one step of decomposition, we notice that the error ∆wk+1 can be decomposed into two\nparts: (1) The approximation error predicting wk+1 with ground-truth input wk. We define it\n∆pred\nk+1 := wk + eV S(f\nW wk −w∗) −wk+1\n(2) The accumulated error from the last inference step:\n\u0010\nI + eV S f\nW\n\u0011\n∆wk. Therefore, we can\ninductively calculate the sum of the error:\nfθ( ˆwk) −w∗≤(wk+1 −w∗) +\n\u0010\nwk + eV S(f\nW wk −w∗) −wk+1\n\u0011\n+\n\u0010\nI + eV S f\nW\n\u0011\n∆wk.\n= (wk+1 −w∗) + ∆pred\nk+1 +\n\u0010\nI + eV S f\nW\n\u0011\n∆wk\n= (wk+1 −w∗) + ∆pred\nk+1 +\n\u0010\nI + eV S f\nW\n\u0011\n∆pred\nk\n+\n\u0010\nI + eV S f\nW\n\u00112\n∆wk−1\n= (wk+1 −w∗) +\nk\nX\ni=0\n\u0010\nI + eV S f\nW\n\u0011i\n∆pred\nk−i+1\n(∆w0 = 0 by definition.)\nThen we have our evaluation loss upper bounded:\n1\n2E\n\u0002\n∥fθ( ˆwk) −w∗∥2\u0003\n= 1\n2E\n\r\r\r\r\r(wk+1 −w∗) +\nk\nX\ni=0\n\u0010\nI + eV S f\nW\n\u0011i\n∆pred\nk−i+1\n\r\r\r\r\r\n2\n61\n\n\n≤k + 2\n2\n \nE ∥(wk+1 −w∗)∥2 +\nk\nX\ni=0\nE\n\r\r\r(I + eV S f\nW )i∆pred\nk−i+1\n\r\r\r\n2\n!\n(*)\nWe first consider the first term: E ∥(wk+1 −w∗)∥2:\nE ∥wk+1 −w∗∥2 = E\n\r\r\u0000I −(I −ηS)k+1\u0001\nw∗−w∗\r\r2 = tr\n\u0000E(I −ηS)2k+2\u0001\n≤2d(1 −δ)2k+2 ≤2d−2c log(\n1\n1−δ)+1.\n(Lemma D.5)\nThen we consider the second summation term. Since the parameters of the reduced model eV =\n−ηI + A, f\nW = I + B, where ∥A∥op, ∥B∥op ≤d−1\n2 C log(\n1\n1−η)+ 1\n2 for some constant c > 0, we\nwant to bound the prediction error given the ground-truth input. By Lemma D.6, we have\nE\nk\nX\ni=0\n\r\r\r(I + eV S f\nW )i∆pred\nk−i+1\n\r\r\r\n2\n= E\nk\nX\ni=0\n\r\r\r(I + eV S f\nW )i(wk−i + eV S(f\nW wk−i −w∗) −wk−i+1))\n\r\r\r\n2\n≤O\n\u0010\nd−C log(\n1\n1−η)+1 · k\n\u0011\n.\nTherefore, plug those back to Equation (*), the total evaluation loss should be upper bounded by\nLEval(θ) ≤O\n\u0010\nd−C log(min{\n1\n1−η ,\n1\n1−δ })+1 · k2\u0011\n= O(d−C log(min{\n1\n1−η ,\n1\n1−δ })+1 log2 d)\nD\nSupplementary Lemmas\nD.1\nConcentration lemmas\nIn this appendix, we prove some concentration lemmas to estimate the expected gradient more\naccurately. Throughout the proof, Λ, Γ are both symmetric matrices with orthonormal eigenbasis\n{ui}d\ni=1.\nLemma D.1. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1). Then the expectation\nE\n\u0002\nSΛ(I −ηS)kS\n\u0003\n= (1 −η)k(Λ + ∆),\nwhere ∥∆∥op ≤O(k2d\nn ) = O\n\u0010\n1\nlog3 d\n\u0011\n. Moreover, the error is in the form ∆= α1Λ + α2 tr(Λ)I,\nwhere α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2 = O\n\u0010\nk2\nn\n\u0011\n.\nProof. Denote δS := S −I. Then we expand the term SΛ(I −ηS)kS:\nSΛ(I −ηS)kS\n62\n\n\n= (I + δS)Λ((1 −η)I −ηδS)k(I + δS)\n= (1 −η)k(I + δS)Λ\n\u0012\nI −\nη\n(1 −η)δS\n\u0013k\n(I + δS)\n= (1 −η)k(I + δS)Λ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\n+(1 −η)k(I + δS)Λ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nδS\nNow take expectation to both sides. Note that E[δS] = 0, so all the terms only contain first order\nδS vanish. We denote\n(1 −η)k e∆= SΛ(I −ηS)kS −(1 −η)k\n\u0012\nΛ + δSΛ + ΛδS −\nkη\n1 −ηΛδS\n\u0013\n,\nwhich denotes all the higher order terms (the degree of δS ≥2.)\nSince we have the tail bound for δS in Theorem 4.6.1 Vershynin [50] (In this lemma ∥·∥is operator\nnorm if without specification):\nPr\n\u0000∥δS∥> max\n\u0000δ, δ2\u0001\u0001\n≤2 exp\n\b\n−s2\t\n, where δ = C\n r\nd\nn +\ns\n√n\n!\n(20)\nWe can estimate the expectation using this property. First, given s =\n√\nd and ∥δS∥≤max (δ, δ2) =\nC\nq\nd\nn (since n = Θ(d log5 d)), we can upper bound the operator norm of e∆:\n∥e∆∥op ≤\n\r\r\r\r\rΛ\n \u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\r\r\r\r\r\nop\n+\n\r\r\r\r\rδSΛ\n \n−\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\r\r\r\r\r\nop\n+\n\r\r\r\r\rδSΛ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nδS\n\r\r\r\r\r\nop\n+\n\r\r\r\r\rΛ\n \n−\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nδS\n\r\r\r\r\r\nop\nNow upper bound all matrices with their operator norm and combine all terms with the same degree\nof δS. We have\n∥e∆∥op ≤\nk+2\nX\nj=2\n∥Λ∥\n \u0012k\nj\n\u0013\u0012\nη\n1 −η\n\u0013j\n+ 2\n\u0012\nk\nj −1\n\u0013\u0012\nη\n1 −η\n\u0013j−1\n+\n\u0012\nk\nj −2\n\u0013\u0012\nη\n1 −η\n\u0013j−2!\n∥δS∥j\n63\n\n\n≤\nk+2\nX\nj=2\n∥Λ∥\n\u0000(9k)j + 2(9k)j−1 + (9k)j−2\u0001\n∥δS∥j\n(\nη\n1−η ≤9,\n\u0000k\nj\n\u0001\n≤kj.)\n≤4\nk+2\nX\nj=2\n∥Λ∥· (9k)j\n \nC\nr\nd\nn\n!j\n(∥δS∥≤C\nq\nd\nn.)\n≤4∥Λ∥· 81C2k2d\nn\n·\n1\n1 −(9kd1/2\nn1/2 )\n≤C′k2d\nn\n≤O\n\u0012\n1\nlog3 d\n\u0013\n.\n(*1)\nGiven this upper bound, we can now upper bound the operator norm of the error term ∆:= E[e∆].\nSuppose u := arg maxu:∥u∥=1\n∥∆u∥\n∥u∥, then the operator norm becomes:\n∥∆∥=\n\f\f\fu⊤E[e∆]u\n\f\f\f\n= E\n\u0014\f\f\fu⊤e∆u\n\f\f\f\n\u0012\n1\n\u001a\n∥e∆∥≤C′k2d\nn\n\u001b\n+ 1\n\u001a\n∥e∆∥> C′k2d\nn\n\u001b\u0013\u0015\n≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds\nWhen ∥e∆∥≥s where s ≥C′k2d\nn\n, we can first upper bound the ∥e∆∥with ∥δS∥using the second\nrow of eq. (*1): there exists some constant C1 > 0 s.t.\n∥e∆∥≤4\nk+2\nX\nj=2\n∥Λ∥· (9k∥δS∥)j ≤max\n\u0010\n(C1k∥δS∥)2, (C1k∥δS∥)k+2\u0011\n.\nTherefore, when ∥e∆∥≥s, ∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k }. To apply the tail bound, we need to\nmake sure we pick some s′ such that max (δ, δ2) ≤min{s1/2\nC1k, s1/(k+2)\nC1k } to upper bound the integral\nof probability, where δ = C(\nq\nd\nn +\ns′\n√n). Now since s >\nC′k2d\nn\n, min{s1/2\nC1k, s1/(k+2)\nC1k } ≥Cα\nq\nd\nn\nfor some constant Cα. Therefore, we just need max{ s′\n√n, s′2\nn } ≤min{s1/2\nC1k, s1/(k+2)\nC1k }, i.e. s′ ≤\nmin\nn\nC2\ns1/(k+2)√n\nk\n, C3\ns1/(2k+4)√n\n√\nk\n, C4\n√sn\nk , C5\ns1/4√n\n√\nk\no\n.\nApplying the tail bound (20) with s′ = min\nn\nC2\ns1/(k+2)√n\nk\n, C3\ns1/(2k+4)√n\n√\nk\n, C4\n√sn\nk , C5\ns1/4√n\n√\nk\no\nwhere\nC2, C3, C4, C5 are some constant, we have the error term for the tail expectation,\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds ≤\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min\n\u001as1/2\nC1k, s1/(k+2)\nC1k\n\u001b\u0015\nds\n≤2\nZ ∞\nC′k2d\nn\nexp\n\b\n−s′2\t\nds.\nNow we estimate the upper bound of error with\ns′2 = min\n\u001a\nC2\n2 · s2/(k+2)\nk2\nn, C2\n3 · s1/(k+2)\nk\nn, C2\n4 · sn\nk2 , C2\n5 ·\n√sn\nk\n\u001b\n.\n64\n\n\nFor the first term, let x = C2\n2n\nk2 s2/(k+2):\n2\nZ ∞\nC′k2d\nn\nexp\n\u001a\n−C2\n2 · s2/(k+2)\nk2\nn\n\u001b\nds\n= (k + 2)\nZ ∞\nC2\n2 n\nk2\n\u0010\nC′k2d\nn\n\u0011\n2\nk+2\n\u0012 k2\nC2\n2n\n\u0013(k+2)/2\nexp{−x}xk/2dx\n≤(k + 2) ·\n\u0012 k2\nC2\n2n\n\u0013(k+2)/2\n·\n \nC2\n2n\nk2\n\u0012C′k2d\nn\n\u0013\n2\nk+2!k/2\nexp\n(\n−C2\n2n\nk2\n\u0012C′k2d\nn\n\u0013\n2\nk+2)\n≤k2d\nn .\nThe second term, let x = C2\n3 · s1/(k+2)\nk\nn:\n2\nZ ∞\nC′k2d\nn\nexp\n\u001a\n−C2\n3 · s1/(k+2)\nk\nn\n\u001b\nds\n= 2(k + 2)\nZ ∞\nC3n\nk\n\u0010\nC′k2d\nn\n\u0011\n1\nk+2\n\u0012 k\nC2\n3n\n\u0013k+2\nexp{−x}xk+1dx\n≤2(k + 2) ·\n\u0012 k\nC2\n3n\n\u0013k+2\n·\n \nC2\n3n\nk\n\u0012C′k2d\nn\n\u0013\n1\nk+2!k+1\nexp\n(\n−C2\n3n\nk\n\u0012C′k2d\nn\n\u0013\n1\nk+2)\n≤k2d\nn .\nFor the third term, let x = C2\n4sn\nk2 :\n2\nZ ∞\nC′k2d\nn\nexp\n\u001a\n−C2\n4sn\nk2\n\u001b\nds =\nZ ∞\nC′k2d\nn\n·\nC2\n4 n\nk2\nk2\nC2\n4n exp{−x}dx\n≤k2\nC2\n4n exp\n\u001a\n−C′k2d\nn\n· C2\n4n\nk2\n\u001b\n≤k2d\nn .\nThe fourth term, let x = C2\n5 · s1/2\nk n:\n2\nZ ∞\nC′k2d\nn\nexp\n\u001a\n−C2\n5 · s1/2\nk n\n\u001b\nds\n= 4k2\nn2C4\n5\nZ ∞\nC2\n5\nn\nk\n\u0010\nC′k2d\nn\n\u00111/2 exp{−x}xdx\n≤4k2\nn2C4\n5\n· C2\n5\nn\nk\n\u0012C′k2d\nn\n\u00131/2\nexp\n(\n−C2\n5\nn\nk\n\u0012C′k2d\nn\n\u00131/2)\n≤k2d\nn .\nTherefore, we plug this error back to the upper bound of ∥∆∥:\n∥∆∥≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds\n≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds = O\n\u0012k2d\nn\n\u0013\n≤O\n\u0012\n1\nlog3 d\n\u0013\n.\n65\n\n\nFinally, since by Lemma D.8, we know the error is in the form ∆= α1Λ + α2 tr(Λ)I for all Λ.\nTherefore α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2 = O\n\u0010\nk2\nn\n\u0011\n.\nLemma D.2. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1), ∥Γ∥op ≤Θ(1). Then the expectation\nE\n\u0002\nSΛ(I −ηS)kΓS\n\u0003\n= (1 −η)k(ΛΓ + ∆),\nwhere ∥∆∥op = O(k2d\nn ) ≤O\n\u0010\n1\nlog3 d\n\u0011\n. Moreover, the error is in the form\n∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ + α4 tr(Λ) tr(Γ)I + α5 tr(ΛΓ)I\nwhere α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3, α5 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nProof. Denote δS := S −I. Then we expand the term SΛ(I −ηS)kΓS:\nSΛ(I −ηS)kΓS\n= (I + δS)Λ((1 −η)I −ηδS)kΓ(I + δS)\n= (1 −η)k(I + δS)Λ\n\u0012\nI −\nη\n(1 −η)δS\n\u0013k\nΓ(I + δS)\n= (1 −η)k(I + δS)Λ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nΓ\n+(1 −η)k(I + δS)Λ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nΓδS\nTake expectation to both sides. Note that E[δS] = 0, so all the first order term vanish. We denote\n(1 −η)k e∆= SΛ(I −ηS)kΓS −(1 −η)k\n\u0012\nΛ + δS · ΛΓ + ΛΓ · δS −\nkη\n1 −ηΛ · δSΓ\n\u0013\n,\nwhich denotes all the higher order terms (the degree of δS ≥2.)\nWe can estimate the expectation using similar technique as in Lemma D.1. First, given s =\n√\nd\nand ∥δS∥≤max (δ, δ2) = C\nq\nd\nn (since n = Θ(d log5 d)), we upper bound the operator norm of\ne∆:\n∥e∆∥op ≤\n\r\r\r\r\rΛ\n \u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nΓ\n\r\r\r\r\r\nop\n+\n\r\r\r\r\rδSΛ\n \n−\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nΓ\n\r\r\r\r\r\nop\n+\n\r\r\r\r\rδSΛ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nΓδS\n\r\r\r\r\r\nop\n66\n\n\n+\n\r\r\r\r\rΛ\n \n−\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nΓδS\n\r\r\r\r\r\nop\nNow upper bound all matrices with their operator norm and combine all terms with the same degree\nof δS. We have\n∥e∆∥op ≤\nk+2\nX\nj=2\n∥Γ∥∥Λ∥\n \u0012k\nj\n\u0013\u0012\nη\n1 −η\n\u0013j\n+ 2\n\u0012\nk\nj −1\n\u0013\u0012\nη\n1 −η\n\u0013j−1\n+\n\u0012\nk\nj −2\n\u0013\u0012\nη\n1 −η\n\u0013j−2!\n∥δS∥j\n≤\nk+2\nX\nj=2\n∥Γ∥∥Λ∥\n\u0000(9k)j + 2(9k)j−1 + (9k)j−2\u0001\n∥δS∥j\n(\nη\n1−η ≤9,\n\u0000k\nj\n\u0001\n≤kj.)\n≤4\nk+2\nX\nj=2\n∥Γ∥∥Λ∥· (9k)j\n \nC\nr\nd\nn\n!j\n(∥δS∥≤C\nq\nd\nn.)\n≤4∥Λ∥∥Γ∥· 81C2k2d\nn\n·\n1\n1 −(9kd1/2\nn1/2 )\n≤C′k2d\nn\n≤O\n\u0012\n1\nlog3 d\n\u0013\nNow upper bound the operator norm of the error term ∆:= E[e∆]. Suppose u := arg maxu:∥u∥=1\n∥∆u∥\n∥u∥,\nthen the operator norm becomes:\n∥∆∥=\n\f\f\fu⊤E[e∆]u\n\f\f\f\n= E\n\u0014\f\f\fu⊤e∆u\n\f\f\f\n\u0012\n1\n\u001a\n∥e∆∥≤C′k2d\nn\n\u001b\n+ 1\n\u001a\n∥e∆∥> C′k2d\nn\n\u001b\u0013\u0015\n≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds\nWhen ∥e∆∥≥s where s ≥C′k2d\nn\n, there exists some constant C1 > 0 s.t.\n∥e∆∥≤max\n\u0010\n(C1k∥δS∥)2, (C1k∥δS∥)k+2\u0011\n.\nTherefore, when ∥e∆∥≥s, ∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k }. Like Lemma D.1, applying the tail bound\n(20) with s′ ≤min\nn\nC2\ns1/(k+2)√n\nk\n, C3\ns1/(2k+4)√n\n√\nk\n, C4\n√sn\nk , C5\ns1/4√n\n√\nk\no\nwhere C2, C3, C4, C5 are some\nconstant, we have the error term for the tail expectation\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds ≤\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds\n≤2\nZ ∞\nC′k2d\nn\nexp\n\b\n−s′2\t\nds.\nUse the exact same argument, 2\nR ∞\nC′k2d\nn\nexp{−s′2}ds ≤k2d\nn . Thus, the upper bound of ∥∆∥is:\n∥∆∥≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds\n67\n\n\n≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds = O\n\u0012k2d\nn\n\u0013\n≤O\n\u0012\n1\nlog3 d\n\u0013\n.\nFinally by Lemma D.8, we know the error is in the form ∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ +\nα4 tr(Λ) tr(Γ)I for all Λ, Γ. Therefore α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nLemma D.3. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1), ∥Γ∥op ≤Θ(1). Then the expectation\nE\n\u0002\nSΛSΓ(I −ηS)k\u0003\n= (1 −η)k(ΛΓ + ∆),\nwhere ∥∆∥op ≤O\n\u0010\n1\nlog3 d\n\u0011\n. Moreover, the error is in the form\n∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ + α4 tr(Λ) tr(Γ)I + α5 tr(ΛΓ)I\nwhere α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3, α5 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nProof. Denote δS := S −I. Then we expand the term SΛSΓ(I −ηS)k:\nSΛSΓ(I −ηS)k\n= (1 −η)k(I + δS)Λ(I + δS)Γ\n\u0012\nI −\nη\n(1 −η)δS\n\u0013k\n= (1 −η)k(I + δS)ΛΓ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\n+(1 −η)k(I + δS)ΛδSΓ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\nTake expectation to both sides. Note that E[δS] = 0, so all the first order term vanish. We denote\n(1 −η)k e∆= SΛ(I −ηS)kΓS −(1 −η)k\n\u0012\nΛ + δS · ΛΓ + ΛδSΓ −\nkη\n1 −ηΛΓ · δS\n\u0013\n,\nwhich denotes all the higher order terms (the degree of δS ≥2.)\nWe can estimate the expectation using similar technique as in Lemma D.1. Given s =\n√\nd and\n∥δS∥≤max (δ, δ2) = C\nq\nd\nn (since n = Θ(d log5 d)), we upper bound the operator norm of\ne∆. We directly expand the formula and upper bound all matrices with their operator norm and\ncombine all terms with the same degree of δS. We have\n∥e∆∥op ≤\nk+2\nX\nj=2\n∥Γ∥∥Λ∥\n \u0012k\nj\n\u0013\u0012\nη\n1 −η\n\u0013j\n+ 2\n\u0012\nk\nj −1\n\u0013\u0012\nη\n1 −η\n\u0013j−1\n+\n\u0012\nk\nj −2\n\u0013\u0012\nη\n1 −η\n\u0013j−2!\n∥δS∥j\n≤\nk+2\nX\nj=2\n∥Γ∥∥Λ∥\n\u0000(9k)j + 2(9k)j−1 + (9k)j−2\u0001\n∥δS∥j\n(\nη\n1−η ≤9,\n\u0000k\nj\n\u0001\n≤kj.)\n68\n\n\n≤4\nk+2\nX\nj=2\n∥Γ∥∥Λ∥· (9k)j\n \nC\nr\nd\nn\n!j\n≤C′k2d\nn\n≤O\n\u0012\n1\nlog3 d\n\u0013\n(∥δS∥≤C\nq\nd\nn.)\nNow upper bound the operator norm of ∆:= E[e∆]. Suppose u := arg maxu:∥u∥=1\n∥∆u∥\n∥u∥, then\n∥∆∥= E\n\u0014\f\f\fu⊤e∆u\n\f\f\f\n\u0012\n1\n\u001a\n∥e∆∥≤C′k2d\nn\n\u001b\n+ 1\n\u001a\n∥e∆∥> C′k2d\nn\n\u001b\u0013\u0015\n≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds\nWhen ∥e∆∥≥s where s ≥C′k2d\nn\n, there exists some constant C1 > 0 s.t.\n∥e∆∥≤max\n\u0010\n(C1k∥δS∥)2, (C1k∥δS∥)k+2\u0011\n.\nTherefore, when ∥e∆∥≥s, ∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k }. Like Lemma D.1, applying the tail bound\n(20) with s′ ≤min\nn\nC2\ns1/(k+2)√n\nk\n, C3\ns1/(2k+4)√n\n√\nk\n, C4\n√sn\nk , C5\ns1/4√n\n√\nk\no\nwhere C2, C3, C4, C5 are some\nconstant, we have\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds ≤\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds ≤2\nZ ∞\nC′k2d\nn\nexp\n\b\n−s′2\t\nds.\nUse the exact same argument, 2\nR ∞\nC′k2d\nn\nexp{−s′2}ds ≤k2d\nn . Thus, the upper bound of ∥∆∥is:\n∥∆∥≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds = O\n\u0012k2d\nn\n\u0013\n≤O\n\u0012\n1\nlog3 d\n\u0013\n.\nFinally by Lemma D.8, we know the error is in the form ∆= α1ΛΓ + α2 tr(Λ)Γ + α3 tr(Γ)Λ +\nα4 tr(Λ) tr(Γ)I for all Λ, Γ. Therefore α1 = O\n\u0010\nk2d\nn\n\u0011\n, α2, α3 = O\n\u0010\nk2\nn\n\u0011\n, α4 = O( k2\nnd).\nLemma D.4. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9), ∥Λ∥op ≤\nΘ(1). Then there exists δ = O\n\u0010\nk2d\nn\n\u0011\n≤O\n\u0010\n1\nlog3 d\n\u0011\n, the expectation is\nE\n\u0002\nΛ(I −ηS)k\u0003\n= (1 −η)k(1 + δ)Λ,\nProof. Denote δS := S −I. Then we expand the term Λ(I −ηS)k:\nΛ(I −ηS)k = (1 −η)kΛ\n\u0012\nI −\nη\n(1 −η)δS\n\u0013k\n= (1 −η)kΛ\n \nI −\nkη\n(1 −η)δS +\n\u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\n69\n\n\nTake expectation to both sides. Note that E[δS] = 0, so all the first order term vanish. We denote\n(1 −η)k e∆= Λ(I −ηS)k −(1 −η)k\n\u0012\nΛ −\nkη\n1 −ηΛ · δS\n\u0013\n,\nwhich denotes all the higher order terms (the degree of δS ≥2.)\nWe can estimate the expectation using similar technique as in Lemma D.1. First, given s =\n√\nd\nand ∥δS∥≤max (δ, δ2) = C\nq\nd\nn (since n = Θ(d log5 d)), we upper bound the operator norm of\ne∆:\n∥e∆∥op ≤\n\r\r\r\r\rΛ\n \u0012k\n2\n\u0013\u0012\nη\n1 −η\n\u00132\nδS2 +\nk\nX\nj=3\n\u0012k\nj\n\u0013\u0012 −η\n1 −η\n\u0013j\nδSj\n!\r\r\r\r\r\nop\nNow upper bound all matrices by operator norm and combine all terms with the same degree of\nδS:\n∥e∆∥op ≤\nk\nX\nj=2\n∥Λ∥\n \u0012k\nj\n\u0013\u0012\nη\n1 −η\n\u0013j!\n∥δS∥j ≤\nk+2\nX\nj=2\n∥Λ∥(9k)j∥δS∥j\n(\nη\n1−η ≤9,\n\u0000k\nj\n\u0001\n≤kj.)\n≤∥Λ∥· 81C2k2d\nn\n·\n1\n1 −(9kd1/2\nn1/2 )\n≤C′k2d\nn\n≤O\n\u0012\n1\nlog3 d\n\u0013\n(∥δS∥≤C\nq\nd\nn.)\nNow upper bound the operator norm of the error. Suppose u := arg maxu:∥u∥=1\n∥∆u∥\n∥u∥, we have\n∥∆∥=\n\f\f\fu⊤E[e∆]u\n\f\f\f = E\n\u0014\f\f\fu⊤e∆u\n\f\f\f\n\u0012\n1\n\u001a\n∥e∆∥≤C′k2d\nn\n\u001b\n+ 1\n\u001a\n∥e∆∥> C′k2d\nn\n\u001b\u0013\u0015\n≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds\nWhen ∥e∆∥≥s where s ≥C′k2d\nn\n, there exists some constant C1 > 0 s.t.\n∥e∆∥≤max\n\u0010\n(C1k∥δS∥)2, (C1k∥δS∥)k+2\u0011\n.\nTherefore, when ∥e∆∥≥s, ∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k }. Like Lemma D.1, applying the tail bound\n(20) with s′ ≤min\nn\nC2\ns1/(k+2)√n\nk\n, C3\ns1/(2k+4)√n\n√\nk\n, C4\n√sn\nk , C5\ns1/4√n\n√\nk\no\nwhere C2, C3, C4, C5 are some\nconstant, we have the error term for the tail expectation\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds ≤\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds ≤2\nZ ∞\nC′k2d\nn\nexp\n\b\n−s′2\t\nds.\nUse the exact same argument, 2\nR ∞\nC′k2d\nn\nexp{−s′2}ds ≤k2d\nn . Thus, the upper bound of ∥∆∥is:\n∥∆∥≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds = O\n\u0012k2d\nn\n\u0013\n≤O\n\u0012\n1\nlog3 d\n\u0013\n.\nFinally by Lemma D.8, we know the error is in the form ∆= α1Λ for all Λ. So α1 = O\n\u0010\nk2d\nn\n\u0011\n.\n70\n\n\nD.2\nConcentration lemmas for out-of-distribution data\nFor non-isotropic covariance Gaussian data input, we also have the concentration around the co-\nvariance Σ when n = Θ(d logc d) for c > 0. We still denote S = 1\nnXX⊤. The following lemmas\nare involved in the calculation for the evaluation process, for in-distribution and out-of-distribution\ninput examples X.\nLemma D.5. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni where xi ∼N(0d, Σ), δ\nη ≤λmin(Σ) ≤λmax(Σ) ≤2−δ\nη\nfor some constant δ > 0.1, n = Θ(d log5 d), k = O(log d), η = Θ(1) ∈(0.1, 0.9). Then the\nexpectation\ntr\n\u0000E(I −ηS)k\u0001\n≤2d(1 −δ)k.\nProof. Denote δS := S −Σ. Then we expand the term Λ(I −ηS)k:\n(I −ηS)k = (1 −δ)k\n\u0012I −ηΣ\n1 −δ\n−\nη\n1 −δδS\n\u0013k\n= (1 −δ)k\n \u0012I −ηΣ\n1 −δ\n\u0013k\n−\nkη\n(1 −δ)\n\u0012I −ηΣ\n1 −δ\n\u0013k−1\nδS +\nk\nX\nj=2\n\u0012k\nj\n\u0013\u0012I −ηΣ\n1 −δ\n\u0013k−j\u0012 −η\n1 −δ\n\u0013j\nδSj\n!\nTake expectation to both sides. Note that E[δS] = 0, so all the first order term vanish. We denote\n(1 −δ)k e∆= (I −ηS)k −(1 −δ)k\n \u0012I −ηΣ\n1 −δ\n\u0013k\n−\nkη\n(1 −δ)\n\u0012I −ηΣ\n1 −δ\n\u0013k−1\nδS\n!\n,\nwhich denotes all the higher order terms (the degree of δS ≥2). Note\n\r\rI−ηΣ\n1−δ\n\r\r\nop ≤1.\nWe can estimate the expectation using similar technique as in Lemma D.1. First, given s =\n√\nd\nand ∥δS∥≤max (δ, δ2) = C\nq\nd\nn (since n = Θ(d log5 d)), we upper bound the operator norm of\ne∆:\n∥e∆∥op ≤\n\r\r\r\r\r\nk\nX\nj=2\n\u0012k\nj\n\u0013\u0012I −ηΣ\n1 −δ\n\u0013k−j\u0012 −η\n1 −δ\n\u0013j\nδSj\n\r\r\r\r\r\nop\nNow upper bound all matrices by operator norm and combine all terms with the same degree of\nδS:\n∥e∆∥op ≤\nk\nX\nj=2\n \u0012k\nj\n\u0013\u0012\nη\n1 −δ\n\u0013j!\n∥δS∥j ≤\nk+2\nX\nj=2\n(9k)j∥δS∥j\n(\nη\n1−η ≤9,\n\u0000k\nj\n\u0001\n≤kj.)\n≤81C2k2d\nn\n·\n1\n1 −(9kd1/2\nn1/2 )\n≤C′k2d\nn\n≤O\n\u0012\n1\nlog3 d\n\u0013\n(∥δS∥≤C\nq\nd\nn.)\nNow upper bound the operator norm of the error. Suppose u := arg maxu:∥u∥=1\n∥∆u∥\n∥u∥, we have\n∥∆∥=\n\f\f\fu⊤E[e∆]u\n\f\f\f = E\n\u0014\f\f\fu⊤e∆u\n\f\f\f\n\u0012\n1\n\u001a\n∥e∆∥≤C′k2d\nn\n\u001b\n+ 1\n\u001a\n∥e∆∥> C′k2d\nn\n\u001b\u0013\u0015\n71\n\n\n≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds\nWhen ∥e∆∥≥s where s ≥C′k2d\nn\n, there exists some constant C1 > 0 s.t.\n∥e∆∥≤max\n\u0010\n(C1k∥δS∥)2, (C1k∥δS∥)k+2\u0011\n.\nTherefore, when ∥e∆∥≥s, ∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k }. Like Lemma D.1, applying the tail bound\n(20) with s′ ≤min\nn\nC2\ns1/(k+2)√n\nk\n, C3\ns1/(2k+4)√n\n√\nk\n, C4\n√sn\nk , C5\ns1/4√n\n√\nk\no\nwhere C2, C3, C4, C5 are some\nconstant, we have the error term for the tail expectation\nZ ∞\nC′k2d\nn\nPr\nh\n∥e∆∥≥s\ni\nds ≤\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds ≤2\nZ ∞\nC′k2d\nn\nexp\n\b\n−s′2\t\nds.\nUse the exact same argument, 2\nR ∞\nC′k2d\nn\nexp{−s′2}ds ≤k2d\nn . Thus, the upper bound of ∥∆∥is:\n∥∆∥≤C′k2d\nn +\nZ ∞\nC′k2d\nn\nPr\n\u0014\n∥δS∥≥min{s1/2\nC1k, s1/(k+2)\nC1k\n}\n\u0015\nds = O\n\u0012k2d\nn\n\u0013\n≤O\n\u0012\n1\nlog3 d\n\u0013\n< 1\n2.\nFinally, the absolute value of the trace should be upper bounded by\ntr\n \n(1 −δ)k\n \u0012I −ηΣ\n1 −δ\n\u0013k\n+ ∆\n!!\n≤2d(1 −δ)k.\nThe next lemma deals with the prediction error.\nLemma D.6. Suppose S =\n1\nn\nPn\ni=1 xix⊤\ni where xi ∼N(0d, Σ), and the covariance matrix\nsatisfies δ\nη ≤λmin(Σ) ≤λmax(Σ) ≤2−δ\nη\nfor some constant δ > 0. Assume n = Θ(d log5 d), k =\nO(log d), η = Θ(1) ∈(0.1, 0.9). Denote that A := eV + ηI, B := f\nW −I, ∥A∥op, ∥B∥op ≤\nΘ(d−c). Then for any i < k,\nE\n\r\r\r(I + eV S f\nW )i(wk−i + eV S(f\nW wk−i −w∗) −wk−i+1))\n\r\r\r\n2\n≤O\n\u0012(1 −δ)2i\nd−2c+1\n\u0013\nProof. We will adopt a similar method as we did throughout Lemma D.1 to Lemma D.4.\nFirst, we expand the left hand side loss:\nE\n\r\r\r(I + eV S f\nW )i(wk−i + eV S(f\nW wk−i −w∗) −wk−i+1))\n\r\r\r\n2\n= E\n\r\r\r(I + eV S f\nW )i\u0010\n( eV S f\nW + ηS)(I −(I −ηS)k−i)w∗−( eV + ηI)Sw∗\u0011\r\r\r\n2\n= E\n\r\r\r(I + eV S f\nW )i\u0010\n( eV S f\nW + ηS)(I −(I −ηS)k−i) −( eV + ηI)S\n\u0011\r\r\r\n2\nF\n72\n\n\n≤d · E\n\r\r\r(I + eV S f\nW )i\u0010\n( eV S f\nW + ηS)(I −(I −ηS)k−i) −( eV + ηI)S\n\u0011\r\r\r\n2\nop\nThe second equation is due to wi = (I −(I −ηS)i), and we arranged to stress the error terms.\nThe third line is because E[w∗w∗⊤] = I. The last line is ∥· ∥F ≤\n√\nd∥·∥op.\nNow we expand each term of the expression within the operator norm into A, B, I, and S:\nI + eV S f\nW = I −ηS + ASB −ηSB + AS.\neV S f\nW + ηS = ASB −ηSB + AS, eV + ηI = A.\nTherefore the formula becomes (consider each term separately)\n\u0010\nI + eV S f\nW\n\u0011i\n= (I −ηS + ASB −ηSB + AS)i\n\u0010\n( eV S f\nW + ηS)(I −(I −ηS)k−i) −( eV + ηI)S\n\u0011\n= (−ηSB + ASB)(I −(I −ηS)k−i) −AS(I −ηS)k−i\nWe still denote δS = S −Σ. We first consider when the concentration holds, a.k.a ∥δS∥≤\nC\nq\nd\nn. Since ∥A∥, ∥B∥≤O(d−c), their error are dominated by C\nq\nd\nn. We reduce this case to the\nprevious Lemma D.5. Therefore we can upper bound the expression by\n\r\r\rI + eV S f\nW\n\r\r\r\ni\n= ∥I −ηS + ASB −ηSB + AS∥i ≤3\n2(1 −δ)i\n\r\r(−ηSB + ASB)(I −(I −ηS)k−i) −AS(I −ηS)k−i\r\r ≤O(d−c).\nThat means this part of the expectation is upper bounded by d · 9\n4(1 −δ)2i · O(d−2c) = O\n\u0010\n(1−δ)2i\nd−2c+1\n\u0011\nThen we estimate the tail expectation. We first upper bound the above formula by ∥δS∥:\n\r\r\rI + eV S f\nW\n\r\r\r\ni\n= ∥I −ηS + ASB −ηSB + AS∥i ≤O(k(1 −δ)i min{∥δS∥, 1}i)\n\r\r(−ηSB + ASB)(I −(I −ηS)k−i) −AS(I −ηS)k−i\r\r ≤O(kd−c min{1, ∥δS∥k−i}).\nUse the same argument as in Lemma D.1 to calculate the integral of tail bound, the tail expectation\ncan also be upper bounded by O\n\u0010\n(1−δ)2i\nd−2c+1\n\u0011\n. Combine those two part and we finish the proof.\nD.3\nThe form of expectation\nLemma D.7. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , then the expectation is in the following form for any k:\nE\nh\nSusu⊤\ns Skutu⊤\nt Sk′i\n= α1usu⊤\ns + α2utu⊤\nt + α3I for any s ̸= t.\nE\nh\nSusu⊤\ns Skusu⊤\ns Sk′i\n= α4usu⊤\ns + α5I.\n73\n\n\nProof. We notice that by changing the basis to {us}d\ns=1,\nE\nh\nSusu⊤\ns Skutu⊤\nt Sk′i\n= UE\nh\u0000U ⊤SU\n\u0001\nese⊤\ns\n\u0000U ⊤SU\n\u0001kete⊤\nt\n\u0000U ⊤SU\n\u0001k′i\nU ⊤.\n(21)\nDefine ˆxi = U ⊤xi. Since gaussian is isotropic, we have E[ ˆxi] = U ⊤E[xi] = 0. After we change\nthe basis, the covariance matrix of ˆxi should also be the same:\nCov( ˆxi) = U ⊤Cov(xi)U = I.\nTherefore ˆxi has the same distribution as xi and we have\nUE\nh\u0000U ⊤SU\n\u0001\nese⊤\ns\n\u0000U ⊤SU\n\u0001kete⊤\nt\n\u0000U ⊤SU\n\u0001k′i\nU ⊤= UE\nh\nSese⊤\ns Skete⊤\nt Sk′i\nU ⊤.\nSubsequently, we only need to consider the expectation of Sese⊤\ns Skete⊤\nt Sk′. Decompose xi into\nthe sum of basis vectors and we get xi = Pd\nj=1 xijej.\nPlug in the decomposition into the expectation and we have\nnk+2E\nh\nSese⊤\ns Skete⊤\nt Sk′i\n= E\n\"\n\nn\nX\ni0=1\nX\nj0,j1∈[d]\nxi0j0xi0j1ej0e⊤\nj1\n\nese⊤\ns\nk′\nY\nl=1\n\n\nn\nX\nil=1\nX\nj2l,j2l+1∈[d]\nxilj2lxilj2l+1ej2le⊤\nj2l+1\n\n\nete⊤\nt\nk\nY\nl=1\n\n\nn\nX\nil=1\nX\nj2l,j2l+1∈[d]\nxilj2lxilj2l+1ej2le⊤\nj2l+1\n\n\n#\n= E\n\"\nX\ni0,··· ,ik+k′∈[n]\nX\nj0,··· ,j2(k+k′)+1∈[d]\nxi0j0xi0j1 · · · xik+k′j2(k+k′)xik+k′j2(k+k′)+1\nej0e⊤\nj1ese⊤\ns ej2e⊤\nj3 · · · ej2ke⊤\nj2k+1ete⊤\nt ej2k+2e⊤\nj2k+3 · · · ej2(k+k′)e⊤\nj2(k+k′)+1\n#\n=\nX\ni0,··· ,ik+k′∈[n]\nX\nj0,··· ,j2(k+k′)+1∈[d]\nE\nh\nxi0j0xi0j1 · · · xik+k′j2(k+k′)xik+k′j2(k+k′)+1\ni\nej0e⊤\nj1ese⊤\ns ej2e⊤\nj3 · · · ej2ke⊤\nj2k+1ete⊤\nt ej2k+2e⊤\nj2k+3 · · · ej2(k+k′)e⊤\nj2(k+k′)+1.\nNote that e⊤\na eb ̸= 0 only when a = b, so e⊤\na ese⊤\ns eb ̸= 0 only when a = b = s. Therefore, we\nonly need to consider the case where j2q−1 = j2q for any q ∈[1, k + k′]. By symmetry, we know\nE\n\u0002\nSese⊤\ns Skete⊤\nt Sk′\u0003\nis a diagonal matrix, so we have j0 = j2(k+k′)+1. We denote\nEj0 = ej0e⊤\nj1ese⊤\ns ej1e⊤\nj2 · · · ejke⊤\njk+1ete⊤\nt ejk+1e⊤\njk+2 · · · ejk+k′e⊤\nj0\nto be one of the standard basis in Rd×d space. It is a non-zero matrix when j1 = s and jk+1 = t.\nBy the analysis above, we have\nnk+2E\nh\nSese⊤\ns Skete⊤\nt Sk′i\n=\nX\ni0,··· ,ik+k′∈[n]\nX\nj0,··· ,jk+k′∈[d]\nE\n\u0002\nxi0j0xi0j1 · · · xik+k′jk+k′xik+k′j0\n\u0003\nEj0.\n74\n\n\nLet P(2k + 2) be the set of all distinct ways of partitioning {i0j0, i0j1 · · · , ik+k′jk+k′, ik+k′j0} into\nk + 1 unordered pairs p = ((p1, p2), · · · , (p2k+1, p2k+2)). From Isserlis’ theorem, we have\nE\n\u0002\nxi0j0xi0j1 · · · xik+k′jk+k′xik+k′j0\n\u0003\n=\nX\np∈P(2k+2)\nk+k′\nY\ni=0\nE\n\u0002\nxp2ixp2i+1\n\u0003\n.\nPlug it in the expectation and we have\nnk+2E\nh\nSese⊤\ns Skete⊤\nt Sk′i\n=\nX\ni0,··· ,ik+k′∈[n]\nX\nj0,··· ,jk+k′∈[d]\nX\np∈P(2k+2)\nk+k′\nY\ni=0\nE\n\u0002\nxp2ixp2i+1\n\u0003\nEj0\n=\nX\np∈P(2k+2)\nX\ni0,··· ,ik∈[n]\nX\nj0,··· ,jk∈[d]\nk+k′\nY\ni=0\nE\n\u0002\nxp2ixp2i+1\n\u0003\nEj0.\nTo make sure the term in the summation is non-zero, p2q−1 = p2q should hold for any 1 ≤q ≤k+1.\nNow consider the graph Gp and G′\np with vertices {0, 1, · · · , k + k′}. If iu1jv1 is paired with iu2jv2,\nthen we put an edge between u1 and u2 into Gp and put an edge between v1 and v2 into G′\np, which\nmeans iu1 = iu2 and jv1 = jv2. Therefore, for a cycle C = (u1, u2, · · · , ur) in Gp or G′\np, we have\niu1 = iu2 = · · · = iur or ju1 = ju2 = · · · = jur. Note that we have n or d choices for the value of\nthe circle. Here we use C(·) to denote the set of circles in the graph and use |C(·)| to denote the\nnumber of circles in the graph. Let c∗be the cycle in G′\np which includes the vertex j0.\nCase 1: s ̸= t.\nFor the partition p where j1 ∈c∗and jk+1 ∈c ̸= c∗, there is only one choice\nfor c and c∗to take. So the term in the summation should be n| C(Gp)|d| C(G\n′\np)|−2ese⊤\ns . Similarly,\nfor the partition p where jk+1 ∈c∗and j1 ∈c ̸= c∗, the term in the summation should be\nn| C(Gp)|d| C(G\n′\np)|−2ete⊤\nt . For the partition p where j1 ∈c′ ̸= c∗and jk+1 ∈c′′ ̸= c∗, there is only one\nchoice for c′ and c′′ to take. Therefore, the expectation should be\nnk+2E\nh\nSese⊤\ns Skete⊤\nt Sk′i\n=\nX\nP:j1∈c∗,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ese⊤\ns +\nX\nP:jk+1∈c∗,j1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ete⊤\nt\n+\nX\nP:j1,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ej0e⊤\nj0\n=\nX\nP:j1∈c∗,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ese⊤\ns +\nX\nP:jk+1∈c∗,j1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ete⊤\nt\n+\nX\nP:j1,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−3I.\nRecall Equation (21), we prove that\nE\nh\nSusu⊤\ns Skutu⊤\nt Sk′i\n= α1usu⊤\ns + α2utu⊤\nt + α3I.\n75\n\n\nCase 2: s = t.\nFor the partition p where j1, jk+1 ∈c∗, there is only one choice for c∗to take. So\nthe term in the summation should be n| C(Gp)|d| C(G\n′\np)|−1ese⊤\ns . For the partition p where j1 ∈c∗and\njk+1 ∈c ̸= c∗, there is only one choice for c and c∗to take. So the term in the summation should\nbe n| C(Gp)|d| C(G\n′\np)|−2ese⊤\ns . Similarly, for the partition p where jk+1 ∈c∗and j1 ∈c ̸= c∗, the term\nin the summation should be n| C(Gp)|d| C(G\n′\np)|−2ese⊤\ns . For the partition p where j1 ∈c′ ̸= c∗and\njk+1 ∈c′′ ̸= c∗, there is only one choice for c′ and c′′ to take. Therefore, the expectation should be\nnk+2E\nh\nSese⊤\ns Skese⊤\ns Sk′i\n=\nX\nP:j1,jk+1∈c∗\nn| C(Gp)|d| C(G\n′\np)|−1ese⊤\ns +\nX\nP:j1∈c∗,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ese⊤\ns\n+\nX\nP:jk+1∈c∗,j1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ese⊤\ns +\nX\nP:j1,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2ej0e⊤\nj0\n=\n\"\nX\nP:j1,jk+1∈c∗\nn| C(Gp)|d| C(G\n′\np)|−1 +\nX\nP:j1∈c∗,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2\n+\nX\nP:jk+1∈c∗,j1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−2\n#\nese⊤\ns +\nX\nP:j1,jk+1 /∈c∗\nn| C(Gp)|d| C(G\n′\np)|−3I.\nRecall Equation (21), we prove that\nE\nh\nSusu⊤\ns Skusu⊤\ns Sk′i\n= α4usu⊤\ns + α5I.\nHence, the proof is complete.\nLemma D.8. Suppose S = 1\nn\nPn\ni=1 xix⊤\ni , then the expectation is in the following form for any k:\nE\nh\nSΛSkΓSk′i\n= β1ΛΓ + β2 tr(Λ)Γ + β3 tr(Γ)Λ + β4 tr(Λ) tr(Γ)I + β5 tr(ΛΓ)I.\nwhere Λ = Pd\nj=1 λΛ\nj uju⊤\nj , Γ = Pd\nj=1 λΓ\nj uju⊤\nj .\nProof. By lemma D.7, we have:\nE\nh\nSΛSkΓSk′i\n=\nd\nX\nj=1\nd\nX\ni̸=j\nλΛ\ni λΓ\nj\n\u0000α1uiu⊤\ni + α2uju⊤\nj + α3I\n\u0001\n+\nd\nX\ni=1\nλΛ\ni λΓ\ni\n\u0000α4uiu⊤\ni + α5I\n\u0001\nThe first term here can be expand into the following form:\nd\nX\nj=1\nd\nX\ni̸=j\nλΛ\ni λΓ\nj\n\u0000α1uiu⊤\ni + α2uju⊤\nj + α3I\n\u0001\n= α1 tr(Γ)Λ + α2 tr(Λ)Γ + α3 tr(Λ) tr(Γ)I −(α1 + α2)ΛΓ −α3 tr(ΛΓ)I\nMeanwhile, the second term is directly α4ΛΓ + α5 tr(ΛΓ)I. We pick β2 = α1, β3 = α2, β4 =\nα3, β1 = α4 −α1 −α2, β5 = α5 −α3, and we complete the proof.\n76\n\n\nE\nExperimental details\nFor all our experiments, we use pytorch Paszke et al. [39] and models are trained on an NVIDIA\nRTX A6000. Each experiment takes about 1 hour.\nSetup\nIn all our experiments, we choose d = 10, n = 20 and η = 0.4. The architecture is\nfLSA(Z; V , W )[:,−1] = Z[:,−1] + V Z · Z⊤W Z[:,−1]\nn\nand data is drawn from the distribution in Equation (1). The batch size B is 1000 and the learning\nrate α is 0.001. The total time is τ = 750 iterations. In the first experiment, k is chosen as 20 while\nk = 10, 20, 30, 40 in the second experiment. The baseline (evaluation loss of transformers without\nCoT) is given by Corollary 3.1 where η∗=\nn\nn+d+1:\nLEval(V , W ) ≥1\n2\n\u0012\nd −2η∗d + η∗2\nn (n + d + 1)d\n\u0013\nIn-distribution Generalization\nWe empirically verify the evaluation loss gap between trans-\nformers with and without CoT shown by Theorem 3.1 and Theorem 3.2. Our experiments in\nFigure 2 demonstrate that the evaluation loss of transformers with CoT converges to near zero\neven when k = 10. See Section 5 for details.\nOut-of-distribution Generalization\nIn addition, we empirically verify the OOD generalization\nresult shown by Theorem 4.2. We sample 10 different covariance matrices from the distribution\nwhich complies to\nδ\nη ≤λmin(Σ) ≤λmax(Σ) ≤2 −δ\nη\nwhere η = 0.4 and η = 0.4. 10 experiments are taken to show the generality of our results for\neach set of experiment. Our experiment in Figure 3 exhibits that the OOD loss of transformers\nwith CoT converges to near zero when k = 10, 20, 30, 40 as the training loss/in-distribution loss\nconverges to zero. The final loss also drops when the number of reasoning steps increases.\nGiven all experiments above, we conclude that transformers with CoT can converge to our con-\nstruction (Theorem 4.1), surpass those without CoT (Corollary 3.1, Theorem 3.2) and generalize\nwell to unseen data (Theorem 4.2).\n77\n\n\n0\n50\n100\n150\n200\n250\n300\n350\niteration\n0\n1\n2\n3\n4\n5\nOOD Loss\nk=10\nk=20\nk=30\nk=40\nFigure 3: OOD Generalization: We plot the OOD loss LEval\nΣ\nwhen n = 20, d = 10. Each set of\nexperiments sampled 10 different Σ. The mean results are presented as line charts, with variance\nrepresented by shaded areas. As shown, OOD loss will converge to near zero.\n78\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21212v1.pdf",
    "total_pages": 78,
    "title": "Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought",
    "authors": [
      "Jianhao Huang",
      "Zixuan Wang",
      "Jason D. Lee"
    ],
    "abstract": "Chain of Thought (CoT) prompting has been shown to significantly improve the\nperformance of large language models (LLMs), particularly in arithmetic and\nreasoning tasks, by instructing the model to produce intermediate reasoning\nsteps. Despite the remarkable empirical success of CoT and its theoretical\nadvantages in enhancing expressivity, the mechanisms underlying CoT training\nremain largely unexplored. In this paper, we study the training dynamics of\ntransformers over a CoT objective on an in-context weight prediction task for\nlinear regression. We prove that while a one-layer linear transformer without\nCoT can only implement a single step of gradient descent (GD) and fails to\nrecover the ground-truth weight vector, a transformer with CoT prompting can\nlearn to perform multi-step GD autoregressively, achieving near-exact recovery.\nFurthermore, we show that the trained transformer effectively generalizes on\nthe unseen data. With our technique, we also show that looped transformers\nsignificantly improve final performance compared to transformers without\nlooping in the in-context learning of linear regression. Empirically, we\ndemonstrate that CoT prompting yields substantial performance improvements.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}