{
  "id": "arxiv_2502.20927v1",
  "text": "1\nGoal-Oriented Semantic Communication for\nWireless Video Transmission via Generative AI\nNan Li, Member, IEEE, Yansha Deng, Senior Member, IEEE, Dusit Niyato, Fellow, IEEE\nAbstractâ€”Efficient video transmission is essential for seam-\nless communication and collaboration within the visually-driven\ndigital landscape. To achieve low latency and high-quality video\ntransmission over a bandwidth-constrained noisy wireless chan-\nnel, we propose a stable diffusion (SD)-based goal-oriented\nsemantic communication (GSC) framework. In this framework,\nwe first design a semantic encoder that effectively identify\nthe keyframes from video and extract the relevant semantic\ninformation (SI) to reduce the transmission data size. We then\ndevelop a semantic decoder to reconstruct the keyframes from\nthe received SI and further generate the full video from the\nreconstructed keyframes using frame interpolation to ensure\nhigh-quality reconstruction. Recognizing the impact of wireless\nchannel noise on SI transmission, we also propose an SD-\nbased denoiser for GSC (SD-GSC) condition on an instantaneous\nchannel gain to remove the channel noise from the received noisy\nSI under a known channel. For scenarios with an unknown\nchannel, we further propose a parallel SD denoiser for GSC\n(PSD-GSC) to jointly learn the distribution of channel gains and\ndenoise the received SI. It is shown that, with the known channel,\nour proposed SD-GSC outperforms state-of-the-art ADJSCC,\nLatent-Diff DNSC, DeepWiVe and DVST, improving Peak Signal-\nto-Noise Ratio (PSNR) by 69%, 58%, 33% and 38%, reducing\nmean squared error (MSE) by 52%, 50%, 41% and 45%, and\nreducing FrÂ´echet Video Distance (FVD) by 38%, 32%, 22% and\n24%, respectively. With the unknown channel, our PSD-GSC\nachieves a 17% improvement in PSNR, a 29% reduction in MSE,\nand a 19% reduction in FVD compared to MMSE equalizer-\nenhanced SD-GSC. These significant performance improvements\ndemonstrate the robustness and superiority of our proposed\nmethods in enhancing video transmission quality and efficiency\nunder various channel conditions.\nIndex Termsâ€”Goal-oriented semantic communication, Stable\ndiffusion model, generative AI, and video transmission.\nI. INTRODUCTION\nT\nHE surging popularity of smart devices, coupled with the\ngrowing reliance on smart visual applications, has dra-\nmatically driven the demand for seamless, high-quality video\ntransmission [1]. However, this growing demand of efficient\nand reliable large-scale video transmission capabilities poses\nsignificant challenges to existing communication bandwidth\nresource [2]. Traditional communication systems that transmit\nfull bit streams based on the Shannonâ€™s technical framework,\nare struggling to meet these demands within bandwidth con-\nstraints [3]. This necessitates novel approaches for efficient\nvideo transmission that go beyond traditional methods.\nN. Li, Y. Deng are with the Department of Engineering, Kingâ€™s College\nLondon, strand, London WC2R 2LS, U.K. (email: nan.3.li@kcl.ac.uk; yan-\nsha.deng@kcl.ac.uk).\nD. Niyato is with the School of Computer Science and Engineering,\nNanyang Technological University, Singapore (email: dniyato@ntu.edu.sg).\nTo achieve efficient video transmission, the primary chal-\nlenge lies in substantially reducing transmitted data volume\nwhile maintaining high-quality video reconstruction, particu-\nlarly over bandwidth-limited wireless networks [4]. Semantic\ncommunication (SC) has emerged as a promising approach\nthat is expected to revolutionize the design and development\nof communication systems [5], [6]. Unlike traditional commu-\nnication systems, the core principle of SC is to understand and\nconvey the underlying meaning or intent of the message, rather\nthan transmitting all the bits [7]. This paradigm shift enables\ncommunication systems to reduce redundancy and irrelevant\ninformation, leading to more efficient data transmission. To\nbetter capture semantic information (SI) relevant to specific\ntasks, goal-oriented semantic communication (GSC) frame-\nwork was introduced to incorporate both semantic level and\ntask-specific effectiveness level [8], [9]. The GSC focused on\nthe semantic content of the message as well as its relevance\nand effectiveness in achieving a specific goal by dynamically\nprioritizing the SI transmission based on their importance to\nthe task at hand [10], [11]. A deep neural network (DNN)-\nbased joint source-channel coding (JSCC) scheme was first\ndeveloped in [12] for high-resolution image transmission over\nadditive white Gaussian noise (AWGN) and Rayleigh fad-\ning channels. Building on this foundation, advanced JSCC\nschemes were proposed to optimize bandwidth-agile visual\ndata transmission [13], [14]. To evaluate semantic importance,\n[15] and [16] introduced attention mechanism-based methods\nto allocate varying attention weights to features, enabling the\nextraction of task-relevant SI for achieving the specific goals.\nExtending these advancements to video transmission, recent\nworks have explored strategies to efficiently extract SI while\naddressing the unique challenges of wireless video transmis-\nsion. One such approach is shot boundary detection [17],\nwhich has been widely applied to tasks such as video sum-\nmarization and scene detection. However, while effective for\ncapturing broader temporal context, the SI is not strongly rele-\nvant to the goal of efficient video transmission. Following [15]\nand [16], [18] extended the attention mechanism to a video\nrecognition task by designing a 3D attention map to evaluate\nthe importance of different pixels to achieve high inference ac-\ncuracy. However, this scheme ignored the inter-frame temporal\ncorrelations that are crucial for effective video reconstruction,\npotentially causing redundant information transmission among\nsimilar frames. In [19], the first end-to-end JSCC-based video\ntransmission scheme, DeepWiVe, divides video frames into\nfixed Groups of Pictures (GOPs) with four frames each.\nThe first frame in each GOP is transmitted as a keyframe\nusing JSCC, while the remaining frames transmit motion-\narXiv:2502.20927v1  [eess.IV]  28 Feb 2025\n\n\n2\ncompensated differences relative to adjacent keyframes, also\nusing JSCC with distinct parameters. Similarly, [20] proposed\na deep video semantic transmission (DVST) scheme that\nemploys a comparable GOP structure, integrating a temporal\nadaptive entropy model with an Artificial Neural Network\n(ANN)-based nonlinear transform and conditional coding ar-\nchitecture to extract SI from video frames. Although these\napproaches improve bandwidth efficiency, their static GOP\nstructure lacks flexibility, as the predefined keyframes may\nnot adapt to variations in video content or bandwidth condi-\ntions. For example, the predefined keyframes within GOPs\nmay exhibit only minor differences, resulting in redundant\ntransmissions and suboptimal efficiency. Additionally, the end-\nto-end design of source coding and channel coding in these\nJSCC-based schemes have limitation in its adaptability, due to\nthe fact that the end-to-end training needs to be performed for\neach task/goal with the lack of flexibility in the plug-and-play\nfunctionality of each module.\nIt is also important to note that, the aforementioned works\nshared a common assumption of channel consistency between\ntraining and inference phases, breaking this assumption may\nlead to significant performance degradation under dynamic\nreal-world channel conditions. The impact of channel noise\nis particularly critical in video transmission, where even\nminor distortions can significantly affect the visual quality\nand semantic interpretation of the content [20]. To tackle\nthis issue, [21] proposed an attention mechanism-based deep\nJSCC (ADJSCC) scheme to dynamically adjust the signal-\nto-noise ratio (SNR) during training to adapt to fluctuated\nchannel conditions and mitigate channel noise. However, this\nscheme remains to be end-to-end design of source coding\nand channel coding, limiting its flexibility to independently\noptimize and replace components for better adaptation to\ndynamic wireless channels, and goals/ tasks. Therefore, more\nflexible and robust modular architectures are needed to allow\nindependent optimization and replacement of components to\nadapt to dynamic wireless channel conditions.\nRecent advancements of generative models, such as Genera-\ntive adversarial network (GAN) [22] and Denoising Diffusion\nProbabilistic Models (DDPM) [23], offer new possibilities for\naddressing adaptive denoising challenge. Compared to GAN,\nDDPM has demonstrated its remarkable capability in denois-\ning imperfect input data across various generation tasks, such\nas image generation [24] and video generation [25], consis-\ntently generating high-quality images with complex textures,\nfine details, and sharp edges without the risk of mode collapse\n[23]. Inspired by DDPMs, two novel plug-and-play generative\nAI modules, channel denoising diffusion model (CDDM) [26]\nand Latent diffusion denoising SC (Latent-Diff DNSC) [4],\nwere proposed to characterize noisy channels for efficient\nimage transmission. These methods highlight the effectiveness\nof DDPM in removing noise from received noisy SI. However,\nthe uncontrollable generation process of DDPM makes these\nschemes challenging to perfectly reconstruct the details of\nthe original image, such as appearance, style, and color [24].\nMoreover, these schemes relies on predefined channel mod-\nels (e.g., AWGN and Rayleigh fading) and assumes perfect\nchannel state information (CSI), which limits its robustness in\ndynamic or imperfect channel environments.\nTo fill the gap, we propose a stable diffusion (SD) model-\nbased GSC framework for efficient video transmission over\nwireless fading channels, with the goal of achieving high-\nquality video reconstruction under latency constraints. Our\nmain contributions are summarized as follows:\nâ€¢ We propose a modular SD-based GSC framework with a\nsemantic encoder at the transmitter for extracting SI rele-\nvant to high-quality video reconstruction, and a semantic\ndenoiser and a semantic decoder at the receiver for noise\nmitigation and video reconstruction, respectively.\nâ€¢ We design a DNN-based feature extraction module and\na latency-aware keyframe selection module, to dynami-\ncally identifies keyframes with significant motion changes\nand extracts semantic differences between consecutive\nkeyframes, ensuring efficient transmission of the most\nrelevant information while reducing redundancy.\nâ€¢ For a known channel, we introduce an SD-based semantic\ndenoiser for GSC (SD-GSC) that treats instantaneous\nchannel gain as an input condition of SD to identify and\nremove noise. For an unknown channel, we propose a\nparallel SD-based semantic denoiser for GSC (PSD-GSC)\nthat jointly estimates the instantaneous channel gain and\ndenoises the received noisy SI.\nâ€¢ We design a DNN-based semantic reconstruction module\nto accurately reconstruct keyframes from the denoised SI,\nand a motion-appearance interpolation module to gener-\nate non-keyframes by seamlessly interpolating between\nreconstructed keyframes, enabling efficient and high-\nquality video reconstruction while preserving temporal\nconsistency.\nâ€¢ We conduct extensive experiments to compare our pro-\nposed methods with state-of-the-art schemes, including\nADJSCC [21], Latent-Diff DNSC [4], DeepWiVe [19],\nand DVST [20], in terms of mean squared error (MSE),\npeak signal-to-noise ratio (PSNR), and FrÂ´echet video dis-\ntance (FVD). Under known channel conditions, SD-GSC\ndemonstrates substantial improvements over ADJSCC,\nLatent-Diff DNSC, DeepWiVe, and DVST, achieving\nMSE reductions of approximately 52%, 50%, 41%, and\n45%, respectively; PSNR improvements of 69%, 58%,\n33%, and 38%; and FVD reductions of 38%, 32%, 22%,\nand 24%. Under unknown channel conditions, PSD-GSC\nfurther reduces MSE by approximately 29%, improves\nPSNR by 17%, and lowers FVD by 19% compared to\nMMSE equalizer-enhanced SD-GSC.\nThe remainder of this article is organized as follows. The\nsystem model and problem formulation are presented in Sec-\ntion II. Section III describes the designed semantic encoder\nfor semantic information extraction. Section IV details the\nproposed semantic denoiser for known and unknown channel\nconditions. Section V discusses the developed semantic de-\ncoder for video construction. Section VI introduces the training\npipline of the proposed framework. The simulation results are\npresented and discussed in Section VII, and the conclusions\nare drawn in Section VIII. The notations used in the paper are\nlist in Table I.\n\n\n3\nSemantic Encoder\nFeature \nextraction\nKeyframe \nextraction\nInput video\nğ’™\nSemantic Denoiser\nSD/PSD/MSD\nDenoising\nFeedback\nğ’‰, ğ’\nğ‘§!\n\"\nğ‘§#\", â‹¯ğ‘§$\n\"\nSemantic Decoder\nSemantic \nreconstruction\nFrame \ninterpolation\n!ğ’™\nGen video\nDenoised SI $ğ’›\n#ğ’™\n(a) Overall SD-GSC framework\nInput ğ’™\nğ‘§!\nğ‘§#\nğ‘§$\nLatent \ndata\nğ’›\nWireless \nchannel\nNoisy \nSI\nğ’™â€²\nğ’›â€²\nDenoised \nSI\n$ğ’›\nSI\nMotion \nextraction\nKeyframe \nextraction\nSparse \ncoding\nLatent data ğ’™\" \nğ‘š!,! = 0\nğ‘š!,#\nğ‘š$%!,$\nğ‘š$,$ = 0\nMotion â„³\nğ‘¥!\n\"\nğ‘š!,&\nğ‘š$'!,(\nSI ğ’›\nKeyframe diff.\nğ‘¥!\n\"\nğ‘¥#\n\"\nğ‘¥(\n\"\n(b) Semantic encoder\n(c) Semantic denoiser\nSemantic reconstruction:\nğ‘¥!\"\nğ‘ !,&\nğ‘ $'!,(\nÌƒğ‘§!\nÌƒğ‘§#\nÌƒğ‘§$\nSparse \ndecoding\nCosine diff\nÌ…ğ‘¥!\nÌ…ğ‘¥#\nÌ…ğ‘¥$\nRecovered SI +ğ’›\nReconstructed ,ğ’™ \nFrame interpolation:\n'\nH 8 Ã—\n'\nW 8\nMotion-Appearance\nTransformer\nblock\n'\nH 16 Ã—\n'\nW 16\nÃ—2\nCross-scale\npatch \nembedding\nConv\nConv\nConv\nH\n2 Ã— W\n2\nH\n4 Ã— W\n4\nHÃ—W\nFeature extractor\n$ğ‘¥!, $ğ‘¥\"\n$ğ‘¥#, $ğ‘¥\"$!\nMotion\nestimation\nW\n+\nUnet\n(d) Semantic decoder\nÃ—2\nTransformer\nblock\nFeature extraction\nLinear\nConv+Res +Att.\nÃ—3\nDownsample \nblock\nConv+Res \n+Att.\nğ‘§!\n)\nğ‘§!\n!\nğ‘§!\n*\nğ‘\n|\nğ‘§!\n+ ğ‘§!\n+'!\nğ‘§!\n)\nğ‘§!\n!\nğ‘§!\n*\nğ‘\n|\nğ‘§!\n+ ğ‘§!\n+'!\nâ„)\nğ‘\n|\nâ„+ â„+'!\nâ„!\nâ„*\nSD (Known ğ’‰)\nPSD (Unk. ğ’‰)\nğğœ½, ğğ‘\nğğœ½\nğ‘§!\n*\nğ‘§!\n*'!\nğ‘§!\n)\nğ‘\n|\nğ‘§!\" ğ‘§!+, â„\nMMSE \nequalizer\nSD \ndenoiser\nEstimated ğ’‰\nğ‘§!\n)\nğ‘§!\n!\nğ‘§!\n*\nâ„*\nğ‘\n|\nğ‘§!\n& ğ‘§!\n', â„'\nâ„*'!\nâ„)\nCon. ğ’›ğŸ\n\" , ğ’‰\nSD denoiser\nCon. ğ’›ğŸ\n\"\nPSD denoiser\nSD Denoiser\nMSD Denoiser\nPSD Denoiser\nInput\nInference\nTraining\nSemantic \ninfo ğ’›ğŸ\nChannel \ngain ğ’‰ \nInput\nğ‘\n|\nğ‘§!\n& ğ‘§!\n', â„'\nÃ—3\nLinear\nConvTrans+Res +Att.\nConvTrans\n+Res +Att.\nUpsample \nblock\nFig. 1: Stable diffusion-based GSC framework for wireless video transmission. (a) depicts the overall framework of SD-GSC;\n(b) shows the detailed design of semantic encoder including the feature extraction and keyframe extraction modules; (c)\ndisplays the semantic denoiser design with SD denoiser and PSD/MSD denoisers for known and unknown channel conditions,\nrespectively; and (d) illustrates the semantic decoder design with semantic reconstruction and frame interpolation modules.\nII. SYSTEM MODEL AND PROBLEM FORMULATION\nIn this section, we first present our stable diffusion (SD)-\nbased goal-oriented semantic communication (GSC) frame-\nwork for wireless video transmission. Then, we formulate the\noptimization problem to achieve the goal of efficient video\ntransmission.\nA. SD-based GSC Framework\nWe consider a video transmission task between a transmitter\nand a receiver under Rayleigh fading channels. The commu-\nnication goal is to achieve high-quality video transmission\nunder latency constraints. As such, we introduce the SD-based\nGSC framework for efficient wireless video transmission, as\nillustrated in Fig. 1. The proposed framework comprises a\nsemantic encoder at the transmitter, coupled with a semantic\ndenoiser and semantic decoder at the receiver. The semantic\nencoder extracts semantic information (SI) by identifying\nkeyframes with significant motion changes and capturing their\nrelevant semantic representations from the video; the semantic\ndecoder reconstructs the keyframes from the received SI\naccordingly, and then generates the complete video by inter-\npolating between the reconstructed keyframes; the semantic\ndenoiser removes wireless channel noise from the received SI\nto enhance the reconstruction capacity of the semantic decoder.\nSpecifically, for known channel conditions where the channel\ngain is available, we propose an SD denoiser that effectively\nremoves noise from the received noisy SI by conditioning\non the channel gain. For scenarios with unknown channel\nconditions, we introduce two following denoisers:\nâ€¢ MMSE equalizer-enhanced SD (MSD) denoiser: An\nMMSE equalizer estimates the instantaneous channel\ngain, which is then fed into the SD denoiser.\nâ€¢ Parallel SD (PSD) denoiser: Two parallel SD modules\njointly learn the instantaneous channel gain and mitigate\nchannel noise through a parallel diffusion process that\nsimultaneously estimates channel characteristics and per-\nforms denoising.\nThe details of semantic encoder, semantic denoiser, and\nsemantic decoder are presented one by one in the following.\n1) Transmitter\nThe semantic encoder at the transmitter includes the feature\nextraction and keyframe selection modules. First, the input\nvideo is represented as a 4D tensor x âˆˆRFÃ—HÃ—WÃ—C, where\nF denotes the number of frames, C represents the number of\ncolor channels, and H and W represent the height and width of\neach frame, respectively. Initially, the high-dimensional input\nvideo x is fed into the feature extraction module to extract the\nmost critical and semantically relevant information related to\nthe communication goal from each frame and outputs a lower-\ndimensional latent tensor xâ€² âˆˆRFÃ—d, with d â‰ªH Ã— W Ã—\nC. Mathematically, we express this video feature extraction\nprocess as\nxâ€² = Efe(x),\n(1)\n\n\n4\nTABLE I\nSUMMARY OF NOTATIONS\nNotation\nDescription\nx\nInput video with dimension RFÃ—HÃ—WÃ—C\nxâ€²\nFeature of Video frames with dimension RFÃ—d\nz\nSemantic information of Keyframes\nzâ€²\nNoisy semantic information at the receiver\nËœz\nDenoised semantic information\nË†x\nReconstructed keyframes\nËœx\nGenerated video data\nI\nSet of keyframes with di for the i-th keyframe\nEfe\nFeature extraction operation\nEks\nKeyframe selection operation\nFsd\nSemantic denoising operation\nDsr\nSemantic reconstruction operation\nDsr\nFrame interpolation operation\nh\nChannel gain\nn\nNoise\nB\nTransmission bandwidth\np\nTransmission power\ntcom\nCommunication time\nTexe\nTask execution time\nTmax\nLatency requirement\nzt\n1\nIntermediate latent variable at diffusion step t\nÎ²t\nNoise scheduling coefficient in diffusion model\nË†h\nEstimated channel gain\nÏµÎ¸(zt\n1, t)\nLearned noise estimator of SI z1 at step t\nÏµÏ‘(ht, t)\nLearned noise estimator of channel gain h at step t\nsÎ¸(zt\n1, t)\nScore network of SI z1 at step t\nsÏ‘ (ht, t)\nScore network of channel gain h at step t\nwhere Efe(Â·) denotes the operation of feature extraction.\nThe latent tensor xâ€² is then processed by the keyframe se-\nlection module to identify the semantically significant frames,\nknown as keyframes. The resulting SI of these selected\nkeyframes is represented as a tensor z\n= {zi\n| zi\nâˆˆ\nR1Ã—di, âˆ€i âˆˆI}, where I denotes the set of selected keyframes.\nThe SI z is mathematically defined as\nz = Eks(xâ€²),\n(2)\nwhere Eks(Â·) represents the keyframe selection operation.\n2) Wireless Channel\nThe SI z is then transmitted over a wireless channel under\nRayleigh fading, which introduces impairments such as signal\nattenuation, multipath fading, and additive noise. The received\nnoisy SI of keyframes, zâ€², can be represented as\nzâ€² = h Â· z + n,\n(3)\nwhere h represents the channel gain between the transmitter\nand receiver, and n âˆ¼CN(0, Ïƒ2) is the additive white\nGaussian noise (AWGN) with noise power Ïƒ2.\nLet us denote the transmission bandwidth as B and the\ntransmission power as p, the transmission data rate can be\ncalculated as\nR = B log2\n\u0012\n1 + ph\nn\n\u0013\n.\n(4)\nGiven that the semantic tensor z is typically in float32 data\ntype in machine learning frameworks such as PyTorch and\nTensorFlow [27], the communication time can be calculated\nas\ntcom =\nPI\ni=1 32di\nR\n,\n(5)\nwhere I is the total number of selected keyframes, and di is\nthe dimension of the SI zi for the ith keyframe.\n3) Receiver\nAt the receiver, the primary objective is to reconstruct the\noriginal video from the received SI while mitigating the impact\nof channel-induced noise and distortions. The semantic de-\ncoder at the receiver includes the semantic denoiser, semantic\nreconstruction, and frame interpolation modules.\nThe received noisy SI zâ€² is first processed by the semantic\ndenoiser module. The semantic denoiser progressively de-\nnoises the noisy SI zâ€² to obtain a denoised semantic vector Ëœz\nas\nËœz = Fsd(zâ€²),\n(6)\nwhere Fsd(Â·) is the semantic denosing operation.\nThe denoised semantic representation Ëœz is then passed to the\nsemantic reconstruction module to reconstruct the keyframes\nË†x by learning the inverse mapping from the latent space to the\nhigh-dimensional pixel space. The reconstructed keyframes Ë†x\ncan be denoted as\nË†x = Dsr(Ëœz),\n(7)\nwhere Dsr(Â·) denotes the semantic reconstruction operation.\nFinally, the reconstructed keyframes Ë†x are fed into the\nframe interpolation module to generate the complete video Ëœx\nby interpolating between the reconstructed keyframes Ë†x. The\ngenerated video data Ëœx can be expressed as\nËœx = Dfi(Ë†x),\n(8)\nwhere Dfi(Â·) represents the frame interpolation operation.\nB. Problem Formulation\nTo perform video transmission via a wireless channel, the\nexecution time includes the computation time for semantic\nencoding at the transmitter, the communication time for trans-\nmitting SI, and the computation time for semantic denoising\nand semantic decoding at the receiver. Therefore, the execution\ntime can be expressed as\nTexe = tfe + tks\n|\n{z\n}\nEncoding time\n+tcom + tsd + tsr + tfi\n|\n{z\n}\nDecoding time\n,\n(9)\nwhere tfe and tks represent the measured computation time\nof feature extraction and keyframe selection modules at the\ntransmitter, respectively; tsd, tsr, and tfi denote the measured\ncomputation time of semantic denoising, semantic reconstruc-\ntion, and frame interpolation at the receiver, respectively.\nThe primary objective of the wireless video transmission\nscheme is to accurately reconstruct the original video at the\nreceiver while minimizing the distortion introduced by the\nwireless channel. To quantify the reconstruction quality, we\nintroduce the mean squared error (MSE) as the distortion\nmetric, which measures the average squared difference be-\ntween the transmitted video frames at the transmitter and the\nreconstructed video frames at the receiver. The objective is to\n\n\n5\nInput !\n!!\n!\"\n!#\nLatent data !$ \nSI \"\n\"!\n$\n\"\"\n$\n\"%\n$\n\"!\n$\n#!,'\n##(!,%\nFeature extraction\nLinear\nConv+Res +Att.\nÃ—3\nDownsample \nblock\nConv+Res \n+Att.\nKeyframe extraction\nMotion \nextraction\nKeyframe \nextraction\nSparse \ncoding\n#!,! = 0\n#!,#\n#$%!,$\n#$,$ = 0\nMotion â„³\n\"!\n$\n$!,'\n$#(!,%\nKeyframe diff.\nFig. 2: Semantic encoder with feature extraction and keyframe extraction.\nminimize the average MSE between the original video frames\nand the reconstructed video frames within the time constraints\nas\nmin\nP\n1\nN\nX\nxâˆˆX\n\f\f\f\fx âˆ’Ëœx\n\f\f\f\f2\ns.t. Texe â‰¤Tmax,\n(10)\nwhere N is the size of dataset X; P represents the set\nof learnable parameters in the various components of the\nproposed system, including the feature extraction, keyframe\nselection, semantic denoising, semantic reconstruction, and\nframe interpolation; Tmax is the latency requirement of the\nvideo transmission task.\nIII. SEMANTIC ENCODER\nIn this section, we present the details of the semantic\nencoder at the transmitter, starting with the design of the\nfeature extraction module and then introducing the keyframe\nselection module.\nA. Feature Extraction\nTo\nextract\ndeep\nfeature\ninformation\nfor\nidentifying\nkeyframes while minimizing data transmission requirements\nand optimizing video reconstruction quality at the receiver, we\npropose a deep neural network-based feature extraction mod-\nule, as shown in Fig. 2. This module encodes input video into\na low-dimensional latent space to efficiently preserve the most\ncritical feature information and discard redundant information\nfor keyframe extraction and high-quality reconstruction.\nThe network architecture is designed using a similar down-\nsample framework as U-Net [28], and the details are pre-\nsented as follows. The input video x is fed into an initial\nconvolutional layer (3Ã—3 kernel, stride 1, padding 1),\nfollowed by a residual block and an attention block, both\noperating on the same channel dimension C = 3 (i.e., the\nRGB channels), to capture and preserve essential features.\nThe output tensor is then processed by three successive\ndownsampling stages, each consisting of a convolutional layer\n(3 Ã— 3 kernel, stride 2, padding 1) to halve the spatial\ndimensions, a residual block for feature refinement, and an\nattention block to focus on the most relevant spatial informa-\ntion. These stages operate at progressively increasing channel\ndimensions c âˆˆ{64, 128, 256, 512}, allowing the network to\ncapture more abstract and complex semantic attributes while\nreducing the spatial dimensions. After that, batch normaliza-\ntion is applied to stabilize the learning process, followed by\na ReLU activation function to introduce non-linearity. Finally,\nthe resulting tensor are embedded into a latent representation\nxâ€² âˆˆRFÃ—d, d = 4096 using a linear layer.\nAlgorithm 1: Keyframe Selection\nInput: Video x, latency requirement Tmax, transmission\npower p, bandwidth B, channel gain h, noise power Ïƒ2\nOutput: The set of keyframes I\n1: Initialize the set of keyframes I = âˆ….\n2: Extract the SI from x using (1).\n3: Calculate the cosine difference mi,j between each pair\nof frames using (11).\n4: Apply sparse coding to represent mi,j as si,j.\n5: Add the initial frame 1 and the last frame F to\nI = I âˆª{1, F}.\n6: Calculate the total completion time T0 for I.\n7: Initialize a max-heap H with tuples (P\njâˆˆI si,j, i /âˆˆI).\n8: while T0 < Tmax do\n9:\nSelect frame iâˆ—with the maximum change from H.\n10:\nSet a temporal set of keyframes Iâ€² = I âˆª{iâˆ—}.\n11:\nCalculate the total completion time T â€² for Iâ€².\n12:\nif T â€² < Tmax then\n13:\nI = Iâ€², T0 = T â€².\n14:\nUpdate the max-heap H involving iâˆ—.\n15:\nend if\n16: end while\nB. Keyframes Selection\nVideo sequences frequently encompass redundant frames\ncharacterized by minimal visual variation between consecutive\nimages. For example, in a football match recording, frames\ndisplaying high-speed action, such as a player shooting at the\ngoal or a goalkeeper diving for a save, show considerable vi-\nsual differences compared to frames with little movement like\nplayers walking across the field or remaining stationary. To\nidentify the most informative and representative frames (i.e.,\nkeyframes) in the input video, we propose a keyframe selection\nmodule that analyzes temporal correlation by calculating inter-\nframe differences and detecting substantial visual changes, as\nshown in Fig. 2. Specifically, we quantify these differences\nusing the cosine similarity between the feature representations\nxâ€²\ni and xâ€²\nj of frames xi and xj as\nmi,j = 1 âˆ’\nxâ€²\ni Â· xâ€²\nj\nâˆ¥xâ€²\niâˆ¥âˆ¥xâ€²\njâˆ¥,\n(11)\nwhere a larger mi,j indicates greater visual dissimilarity\nbetween the frame pair, suggesting that one of these frames\ncould be a potential keyframe.\nDuring video transmission, we start by sending the complete\nfeature representation of the first keyframe, xâ€²\n1, to build a\nknowledge base for future frame recovery. For subsequent\n\n\n6\nkeyframes, instead of transmitting entire feature represen-\ntation, we send a sparse tensor representation si,j of the\ncosine differences mi,j between consecutive keyframes. This\napproach leverages the fact that differences between adjacent\nkeyframes are often sparse (i.e., has many zero values), as\nmost of the visual information remains unchanged between\nconsecutive keyframes. By exploiting this sparsity, we can\nremove the redundant information thereby further reducing\nthe bandwidth requirements. The keyframes selection is deter-\nmined by meeting specified latency constraints while ensuing\nhigh quality reconstruction during transmission. The details\nof keyframe selection module are presented in Algorithm 1.\nInitially, since the frame interpolation module in the seman-\ntic decoder requires at least two frames, the first and last\nframes are selected as the keyframes. Then, additional frames\nthat have the maximum cosine difference from the existing\nkeyframes in I are iteratively included to obtain the optimal\nkeyframes set I = {1, 2, . . . , I} until the execution time of the\nvideo task exceeds the latency requirement.\nIV. SEMANTIC DENOISER\nIn this section, we first propose an SD denoiser to effectively\neliminate the effect of wireless channel noise on SI transmis-\nsion under known channel scenario, and then propose a PSD\ndenoiser to handle the unknown channel scenario.\nA. SD Denoiser under Known Channel\nTo handle the wireless channel noise during SI transmission,\nthe uncontrolled image generation process of DDPM, as\napplied in Latent-Diff DNSC [4], potentially degrades the\nreconstruction quality [24]. To overcome this challenge, we\npropose the SD denoiser that uses the received noisy SI and\ninstantaneous channel gain as control conditions to effectively\nremove channel noise, as shown in Fig. 3. Note that, due to\nthe computation-intensive nature of the SD denoiser, we only\napply it to denoise the received noisy SI of the first keyframe,\nand then use the noise information n captured by SD denoiser\nto denoise the subsequent keyframes.\n1) Forward Process of DDPM\nThe forward process (i.e., training process) is applied to\nthe transmitted SI of the first keyframe, z1, generated by the\nsemantic encoder. This involves iteratively adding Gaussian\nnoise to the initial distribution z0\n1 âˆ¼p(z1) over T time steps,\ngradually approaching an isotropic Gaussian distribution zT\n1 âˆ¼\nN(0, I). This process can be viewed as a discrete-time Markov\nchain, where the current state zt\n1 is obtained by adding noise to\nthe previous state ztâˆ’1\n1\n. In DDPM [29], at time step t âˆˆ[0, T],\nthe forward process of zt\n1 is expressed as\nzt\n1 =\np\n1 âˆ’Î²tztâˆ’1\n1\n+\np\nÎ²tÏµ,\n(12)\nwhere Î²t âˆˆ(0, 1) is the noise scheduling function, typically\nmodeled as a monotonically increasing linear function of t,\nand Ïµ âˆ¼N(0, I).\nFrom the score-based perspective (i.e., the gradient of the\nlog probability density with respect to the data zt\n1 at each\n!!\n\"\n!!\n!\n!!\n#\n!\n|\n#!\n$ #!\n$%!\n!!\n\"\n!!\n!\n!!\n#\n!\n|\n#!$ #!$%!\nâ„\"\n!\n|\nâ„$ â„$%!\nâ„!\nâ„#\nSD (Known %)\nPSD (Unk. %)\n&&, &'\n&&\n!!\n#\n!!\n#%!\n!!\n\"\n!\n|\n#!( #!$, â„\nMMSE \nequalizer\nSD \ndenoiser\nEstimated !\n!!\n\"\n!!\n!\n!!\n#\nâ„#\n\"\n|\n$!\n\" $!\n#, â„#\nâ„#%!\nâ„\"\nCon. ()\n( , %\nSD denoiser\nCon. ()\n(\nPSD denoiser\nSD Denoiser\nMSD Denoiser\nPSD Denoiser\nInput\nInference\nTraining\nSemantic \ninfo ()\nChannel \ngain ! \nInput\n\"\n|\n$!\n\" $!\n#, â„#\nFig. 3: Semantic denoiser under known and unkown channels.\nnoise scale Î²t), the forward stochastic differentiable equation\n(SDE) can be expressed as\nd(f)zt\n1 = âˆ’Î²t\n2 zt\n1 dt +\np\nÎ²t dwt\n= f(zt\n1, t) dt + g(t) dwt,\n(13)\nwhere f(zt, t) = âˆ’Î²t\n2 zt is the drift term, g(t) = âˆšÎ²t is the\ndiffusion coefficient, dwt is the standard Wiener process.\n2) Reverse Process of DDPM\nThe reverse process (i.e., inference process) of DDPM aims\nto recover the original SI z1 from the noisy sample zT\n1 âˆ¼\nN(0, I). This process mirrors the marginal distribution of the\nforward process p(zt\n1) but with the drift direction reversed.\nCorrespondingly, the reverse-time SDE becomes\nd(r)zt\n1 =\n\u0002\nf(zt\n1, t) âˆ’g(t)2âˆ‡zt\n1 log p(zt\n1)\n\u0003\ndt + g(t) dwt\n=\n\u0002\nâˆ’Î²t\n2 zt\n1 âˆ’Î²tâˆ‡zt\n1 log p(zt\n1)\n\u0003\ndt +\np\nÎ²t dwt, (14)\nwhere s(zt\n1, t) = âˆ‡zt\n1 log p(zt\n1) is the score function, which\nis intractable and needs to be approximated using a neural\nnetwork sÎ¸(zt\n1, t).\nSince âˆ‡zt\n1 log p(zt\n1) = âˆ‡zt\n1 log p\n\u0000zt\n1 | z0\n0\n\u0001\n, we can approx-\nimate âˆ‡zt\n1 log p(zt\n1) â‰ƒsÎ¸ (zt\n1, t) for the reverse process in\n(14) by solving the following minimization problem during\nthe training in the forward process [30]:\nÎ¸âˆ—= argmin\nÎ¸\nEzt\n1,z0\n1\n\u0014\r\r\rsÎ¸\n\u0000zt\n1, t\n\u0001\nâˆ’âˆ‡zt\n1 log p\n\u0000zt\n1|z0\n0\n\u0001\r\r\r\n2\n2\n\u0015\n,\n(15)\nwhere the trained score network sÎ¸ (zt\n1, t) can be denoted by\nusing Tweedieâ€™s identity [31] as\nsÎ¸\n\u0000zt\n1, t\n\u0001\n= âˆ‡zt\n1 log p(zt\n1) = âˆ’\n1\nâˆš1 âˆ’Â¯Î±t\nÏµÎ¸\n\u0000zt\n1, t\n\u0001\n,\n(16)\nwhere Î±t = 1âˆ’Î²t and Â¯Î±t = Qt\ni=1 (1 âˆ’Î±i), and the parameter\nÏµÎ¸ (zt\n1, t) is the learned noise estimator at time step t.\n3) Reverse Process of Stable Diffusion\nEvidently, the above reverse process of DDPM from a ran-\ndom Gaussian sample zT\n1 cannot ensure the quality of image\nreconstruction. To allow for more controllable and guided\nimage generation, we design an SD denoiser conditional on\nthe received noisy SI zâ€²\n1 and instantaneous channel gain h.\n\n\n7\nHere, the SD denoiser follows the same forward process as\nDDPM [24] .\nLeveraging the diffusion model as the prior, it is straight-\nforward to modify (14) to derive the reverse process of SD\nfrom the posterior distribution\nd(r)zt\n1 = [âˆ’Î²t\n2 zt\n1 âˆ’Î²tâˆ‡zt\n1 log p\n\u0000zt\n1 | zâ€²\n1\n\u0001\n]dt +\np\nÎ²tdwt\n= [âˆ’Î²t\n2 zt\n1 âˆ’Î²t(âˆ‡zt\n1 log p(zt\n1)\n+ âˆ‡zt\n1 log p(zâ€²\n1|zt\n1, h))]dt +\np\nÎ²tdwt,\n(17)\nwhere we use\nâˆ‡zt\n1 log p\n\u0000zt\n1\n\u0001\n= âˆ‡zt\n1 log p\n\u0000zt\n1 | zâ€²\n1\n\u0001\n+ âˆ‡zt\n1 log p (zâ€²\n1)\n|\n{z\n}\n0\n= âˆ‡zt\n1 log p\n\u0000zt\n1 | zâ€²\n1\n\u0001\n,\n(18)\nand\nâˆ‡zt\n1 log p\n\u0000zt\n1 | zâ€²\n1\n\u0001\n=âˆ‡zt\n1 log p\n\u0000zt\n1\n\u0001\n+ âˆ‡zt\n1 log p\n\u0000zâ€²\n1 | zt\n1\n\u0001\n=âˆ‡zt\n1 log p\n\u0000zt\n1\n\u0001\n+ âˆ‡zt\n1 log p\n\u0000zâ€²\n1 | zt\n1, h\n\u0001\n+ âˆ‡zt\n1 log p (h)\n|\n{z\n}\n0\n=âˆ‡zt\n1 log p\n\u0000zt\n1\n\u0001\n+ âˆ‡zt\n1 log p\n\u0000zâ€²\n1 | zt\n1, h\n\u0001\n.\n(19)\nBy discretizing the reverse process in (17), we have\nztâˆ’1\n1\n= 1\nâˆšÎ±t\n(zt\n1 + Î²t[sÎ¸\n\u0000zt\n1, t\n\u0001\n+ âˆ‡zt\n1 log p(zâ€²\n1|zt\n1, h)])\n+\np\nÎ²tN(0, I).\n(20)\nTo solve the reverse process in (20), the main challenge lies\nin the posterior distribution p(zâ€²\n1|zt\n1, h). While the relationship\nbetween the received noisy SI zâ€²\n1 and the transmitted SI z0\n1 is\nknown, the relationship between the intermediate data zt\n1 at\nthe tth step of the forward process and zâ€²\n1 remains unknown.\nTo tackle this issue, we express p (zâ€²\n1 | zt\n1) as\np\n\u0000zâ€²\n1 | zt\n1\n\u0001\n=\nZ\np\n\u0000zâ€²\n1 | z0\n1\n\u0001\np\n\u0000z0\n1 | zt\n1\n\u0001\ndz0\n1,\n(21)\nwhere the mean of p(z0\n1 | zt\n1) can be approximated by a delta\nfunction as\np\n\u0000z0\n1 | zt\n1\n\u0001\nâ‰ƒÎ´E[z0\n1|zt\n1](z0\n1).\n(22)\nTo estimate the E[z0\n1|zt\n1], we can use the well-trained noise\nestimator ÏµÎ¸(zt\n1, t) in the forward process (15) to obtain the\nestimation E[z0\n1|zt\n1] = Ë†zt\n1 as\nË†zt\n1 =\n1\nâˆšÎ±t\n(zt\n1 âˆ’\np\n1 âˆ’Â¯Î±tâˆ’1ÏµÎ¸(zt\n1, t)).\n(23)\nUsing (23), the approximation p(z0\n1|zt\n1) leads to the follow-\ning formula for the gradient of the log-likelihood:\nâˆ‡zt\n1 log p(zâ€²\n1|zt\n1, h) = âˆ’\n1 âˆ’Â¯Î±t\nÎ²t (1 âˆ’Â¯Î±tâˆ’1)âˆ‡zt\n1||zâ€²\n1 âˆ’hzt\n1||2.\n(24)\nB. PSD Denoiser under Unknown Channel\nNotably, the SD denoiser is only applicable when the\ninstantaneous channel gain h is known, and hence cannot be\ndirectly used for the scenarios with imperfect estimation of h\n(e.g., SISO Rayleigh fading channels [32]). To solve this issue,\nwe propose a parallel SD (PSD) denoiser to jointly estimate\nthe channel gain and remove the noise, as shown in Fig. 3.\nSimilar to the SD denoiser, the PSD denoiser is applied only\nto the received noisy SI of the first keyframe.\n1) Forward Process\nGiven that z1 and h are independent, the posterior proba-\nbility can be expressed as:\np(z1, h|zâ€²\n1) âˆp(zâ€²\n1|z1, h)p(z1)p(h),\n(25)\nwhere represents the received noisy SI. Based on this, we train\ntwo separate forward processes for z1 and h, respectively.\nSimilar to equations (16) and (23), the score network of h\nand the estimation E[h0|ht] = Ë†ht, can be derived using\nsÏ‘ (ht, t) = âˆ‡ht log p(ht) = âˆ’\n1\nâˆš1 âˆ’Â¯Î±t\nÏµÏ‘(ht, t),\n(26)\nand\nË†ht =\n1\nâˆšÎ±t\n(ht âˆ’\np\n1 âˆ’Â¯Î±tâˆ’1ÏµÏ‘(ht, t)),\n(27)\nwhere ÏµÏ‘ (ht, t) represents the learned noise estimator for the\nchannel gain at time step t, parameterized by Ï‘.\n2) Reverse Process\nThe reverse processes for PSD follow the framework in (17),\nand the two reverse processes can be expressed as\nd(r)zt\n1 =\n\u0002\nâˆ’Î²t\n2 zt\n1 âˆ’Î²t(âˆ‡zt\n1 log p(zt\n1)\n+ âˆ‡zt\n1 log p(zâ€²\n1|zt\n1, ht))\n\u0003\ndt +\np\nÎ²tdwt,\n(28)\nand\nd(r)ht =\n\u0002\nâˆ’Î²t\n2 ht âˆ’Î²t(âˆ‡ht log p(ht)\n+ âˆ‡ht log p(zâ€²\n1|zt\n1, ht))\n\u0003\ndt +\np\nÎ²tdwt.\n(29)\nCorrespondingly, the above two reverse processes can be\ndiscretized in a similar way to (20) and expressed as\nztâˆ’1\n1\n= 1\nâˆšÎ±t\n\u0000zt\n1 + Î²t\n\u0002\nâˆ‡zt\n1 log p(zâ€²\n1|zt\n1, ht) + sÎ¸\n\u0000zt\n1, t\n\u0001 \u0003\u0001\n+\np\nÎ²tN(0, I),\n(30)\nand\nhtâˆ’1 = 1\nâˆšÎ±t\n\u0000ht + Î²t\n\u0002\nâˆ‡ht log p(zâ€²\n1|zt\n1, ht) + sÏ‘ (ht, t)\n\u0003\u0001\n+\np\nÎ²tN(0, I),\n(31)\nwhere\nâˆ‡zt\n1 log p(zâ€²\n1|zt\n1, ht) = âˆ’\n1 âˆ’Â¯Î±t\nÎ²t (1 âˆ’Â¯Î±tâˆ’1)âˆ‡zt\n1||zâ€²\n1 âˆ’htzt\n1||2,\n(32)\nand\nâˆ‡ht log p(zâ€²\n1|zt\n1, ht) = âˆ’\n1 âˆ’Â¯Î±t\nÎ²t (1 âˆ’Â¯Î±tâˆ’1)âˆ‡ht||zâ€²\n1 âˆ’htzt\n1||2.\n(33)\n\n\n8\nGiven the sparse structure of certain wireless channels, we\nuse â„“1 regularization to sparse the channel gain by augmenting\nthe diffusion prior thereby better stabilizing the reconstruction.\nThe estimated channel gain is then updated as\nhtâˆ’1 = htâˆ’1 âˆ’Î±(||zâ€²\n1 âˆ’htâˆ’1 âˆ—ztâˆ’1\n1\n||2 + Ï•||htâˆ’1||), (34)\nwhere Ï• is the regularization strength.\nV. SEMANTIC DECODER\nThis section provides the details of the semantic recon-\nstruction module and frame interpolation module within the\nsemantic decoder at the receiver.\nA. Semantic Reconstruction\nTo ensure accurate reconstruction of keyframes at the re-\nceiver, the semantic reconstruction module is designed to\nminimize visual artifacts and ensure that the reconstructed\nkeyframes closely resemble the original frames in both content\nand quality. This is achieved by progressively reconstructing\nthe spatial and semantic details of the keyframes from the\ndenoised low-dimensional SI Ëœz, as shown in Fig. 4.\nThe architecture of this network follows an upsampling\nframework similar to that of U-Net [28], and is detailed as\nfollows. It begins with sparse decoding and cosine difference\nto recover the latent representation of keyframes, Â¯z âˆˆRIÃ—d,\nfrom the denoised SI Ëœz. Then, a linear layer is applied to\ntransform the latent representation Â¯z into a higher-dimensional\nfeature space, RIÃ—hÃ—wÃ—c. Subsequently, the upsampling un-\nfolds across three stages, each incorporating a transposed\nconvolutional layer (3Ã—3 kernel, stride 2, padding 1)\nto double the spatial dimensions, followed by a residual\nblock for refining the features and an attention block to\nenhance the most critical spatial features, ensuring that the\nreconstruction captures the essential details accurately. These\nstages progressively reduce the tensorâ€™s channel dimensions\nc âˆˆ{512, 256, 128, 64}, allowing the network to capture\nintricate details while expanding the spatial dimensions of\nthe feature map. The final refinement employs a transposed\nconvolutional layer (3Ã—3 kernel, stride 1, padding 1)\nand a sigmoid activation function to produce the reconstructed\nkeyframes, Ë†x âˆˆRIÃ—HÃ—WÃ—C, initially normalized within [0,\n1]. To achieve the final keyframes, denormalization scales the\nsigmoid output to [0, 255], ensuring the pixel values accurately\nreflect the visual details of the original keyframes.\nB. Frame Interpolation\nOnce keyframes are reconstructed, the frame interpolation\nmodule generates the complete video Ëœx by interpolating in-\ntermediate frames between these keyframes Ë†x. Building upon\n[33], we introduce a lightweight frame interpolation module\nthat involves three stages: low-level feature extraction, motion\nand appearance estimation, and a fusion stage that outputs the\ncolor frames, as shown in Fig. 5. The details are presented\nbelow.\nDenoised SI !\"\nÌƒ$!\nÌƒ$\"\nÌƒ$#\nSparse \ndecoding\nCosine diff\nÌ…&!\nÌ…&\"\nÌ…&#\nRecovered SI '\"\nReconstructed () \nÃ—3\nLinear\nConvTrans+Res +Att.\nConvTrans\n+Res +Att.\nUpsample \nblock\nFig. 4: Semantic reconstruction.\n!\nH 8 Ã—\n!\nW 8\nMotion-Appearance\nTransformer\nblock\n!\nH 16 Ã—\n!\nW 16\nÃ—2\nCross-scale\npatch \nembedding\nConv\nConv\nConv\nH\n2 Ã— W\n2\nH\n4 Ã— W\n4\nHÃ—W\nFeature extractor\n!\"!, !\"\"\n!\"#, !\"\"$!\nMotion\nestimation\nW\n+\nUnet\nÃ—2\nTransformer\nblock\nFig. 5: Frame interpolation.\n1) Low-level Feature Extraction\nTo capture and enhance details at different scales, the low-\nlevel feature extractor uses hierarchical convolutional layers to\nextract multi-scale appearance features (l0\ni , l1\ni , l2\ni ) from each\nkeyframe Ë†xi, where feature lk\ni has dimensions H\n2k Ã— W\n2k Ã— 2kC\nin which C increases as the spatial resolution decreases. These\nmulti-scale features are achieved by using dilated convolutions\nwith strides of 23âˆ’k and dilation rates from 1 to 22âˆ’k.\nTo enhance fine-grained information for subsequent motion-\nappearance estimation, we integrate the multi-scale features lk\ni\nto complement cross-scale information by concatenating and\nfusing them using a linear layer to generate the cross-scale\nappearance feature of the i-th keyframe Ë†xi. Afterward, these\ncross-scale feature representations of keyframes are fed into\nthe hierarchical motion-appearance feature extractor to extract\nboth motion features and inter-frame appearance features.\n2) Motion and Appearance Estimation\nEffective video frame interpolation relies on accurately\ncapturing the motion between input frames and seamlessly\nintegrating inter-frame appearance features, such as color\nand texture. To achieve this, we use inter-frame attention to\nnaturally extract both motion and appearance details from the\nkeyframes.\nGiven two keyframes, Ë†xi and Ë†xj, we denote their appear-\nance features as Ai and Aj âˆˆRbHÃ—c\nWÃ—C, respectively. For\na specific region Am,n\ni\nâˆˆRC in Ë†xi and its corresponding\nspatial neighbors Awm,n\nj\nâˆˆRWÃ—WÃ—C in Ë†xj (with W being\nthe neighborhood window size), we generate the query Qm,n\ni\n,\nkeys Kwm,n\nj\n, and values V wm,n\nj\nusing\nQm,n\ni\n= Am,n\ni\nMQ,\nKwm,n\nj\n= Awm,n\nj\nMK,\nV wm,n\nj\n= Awm,n\nj\nMV ,\n(35)\nwhere MQ, MK, MV\nâˆˆRCÃ—bC are the linear projection\nmatrices.\nTo simultaneously capture appearance information and iden-\ntify motion details between the keyframes, we compute the\n\n\n9\nattention map Sm,n\ni,j\nusing the softmax function to measure\nsimilarity following\nSm,n\ni,j\n= softmax\n \nQm,n\ni\n(Kwm,n\nj\n)T\np\nË†C\n!\n,\n(36)\nwhere Sm,n\ni,j\nis crucial for blending the appearance information\nof the two keyframes, enabling a richer understanding of how\nappearance transforms between frames. Specifically, we refine\nAm,n\ni\nby integrating it with the weighted features from Ë†xj as\nËœ\nAm,n\ni\n= Am,n\ni\n+ Sm,n\ni,j V wm,n\nj\n,\n(37)\nwhere\nËœ\nAm,n\ni\ncombines appearance information from both\nframes, which is essential for generating intermediate frames.\nFor accurate interpolation, we estimate the motion vector\nM m,n\ni,j\nâˆˆR2 by weighting the coordinates as\nM m,n\ni,j\n= Sm,n\ni,j Bwm,n âˆ’Bm,n,\n(38)\nwhere B âˆˆRbHÃ—bWÃ—2 represents a coordinate map.\nBenefiting from the timestep-invariant nature of Ëœ\nAm,n\ni\n, the\nmotion vector M m,n\ni,j\ncan guiding subsequent motion estima-\ntion for arbitrary timestep frame predictions. Assuming local\nlinear motion, the motion vector for an intermediate frame\nË†xi+âˆ†, 0 < âˆ†< j âˆ’i is approximated as\nM m,n\ni+âˆ†=\nâˆ†\nj âˆ’i Ã— M m,n\ni,j ,\n(39)\nIn essence, computing inter-frame attention once facilitates\nefficient and effective motion and appearance estimation across\nmultiple arbitrary timestep frame predictions.\n3) Fusion\nWe begin by estimating bidirectional optical flows F and\nmasks O using the acquired motion and appearance features.\nThese are then used to warp keyframes Ë†xi and Ë†xj to the target\nframe i + âˆ†and fuse them together as\nË†xi+âˆ†=O âŠ™BW (Ë†xi, Fâˆ†â†’0)\n+ (1 âˆ’O) âŠ™BW (Ë†xj, Fâˆ†â†’jâˆ’i) ,\n(40)\nwhere BW(Â·) denotes the backward warp operation.\nSubsequently, we refine the appearance of the fused frame\nË†xi+âˆ†using low-level features and inter-frame appearance as\nË†xi+âˆ†= Ë†xi+âˆ†+ RefineNet (Ë†xi+âˆ†, l, A) ,\n(41)\nwhere RefineNet utilizes three convolution layers for motion\nestimation and a simplified U-Net architecture [28] for achiev-\ning high performance.\nVI. TRAINING AND IMPLEMENTATION\nThe proposed SD-based GSC architecture can be effectively\ntrained via an end-to-end approach; however, this method\nmay suffer from slow convergence. To mitigate this issue,\nwe employ a step-by-step training strategy that systematically\ntrains each component of the SD-based GSC architecture.\nAlgorithm 2: Training Process of Semantic Denoiser\nRequire: Training dataset X, time steps T, hyperparameters\n{Î²1, Â· Â· Â· , Î²T }, Î¶Î¸\nt , Î¶Ï‘\nt , Î± and Ï•.\nEnsure: Semantic denoisers Î¶Î¸\nt , Î¶Ï‘\nt .\n1: Train the semantic extraction and reconstruction modules\nusing the loss function in (42).\n// Training SD denoiser\n2: Freeze parameters of semantic extraction module Ese(Â·).\n3: repeat\n4:\nSample a latent representation z1 âˆ¼p(z1).\n5:\nt âˆ¼Uniform({1, 2, . . . , T}), Ïµ âˆ¼N(0, I).\n6:\nTake gradient descent step on âˆ‡Î¸ âˆ¥ÏµÎ¸(zt\n1, t) âˆ’Ïµâˆ¥2\n2.\n7: until Converged\n// Denoising of SD denoiser\n8: Sample zT\n1 âˆ¼N(0, I)\n9: for denoising step t = T, . . . , 1 do\n10:\nË†Ïµ â†Ïµ(zt\n1, t).\n11:\nEstimate Ë†zt\n1 using (23).\n12:\ng â†âˆ‡zt\n1 log p(zâ€²\n1|zt\n1, h).\n13:\nCompute the conditional score s â†Î¶Î¸\nt g âˆ’\n1\nâˆš1âˆ’Â¯Î±t Ë†Ïµ.\n14:\nSample Ï† âˆ¼N(0, I).\n15:\nCompute ztâˆ’1\n1\n=\n1\nâˆšÎ±t (zt\n1 + Î²ts) + âˆšÎ²tÏ†.\n16: end for\n// Training PSD denoiser\n17: Train denoising estimators ÏµÎ¸ and ÏµÏ‘ for z1 and h\nfollowing steps 3-7, respectively.\n// Denoising of PSD denoiser\n18: Sample zT\n1 , hT âˆ¼N(0, I).\n19: for denoising step t = T, . . . , 1 do\n20:\nË†ÏµÎ¸ â†ÏµÎ¸(zt\n1, t), Ë†ÏµÏ‘ â†ÏµÏ‘(zt\n1, t).\n21:\nEstimate Ë†zt\n1 and Ë†ht using (23) and (27).\n22:\ngÎ¸ â†âˆ‡zt log p(zâ€²\n1|zt\n1, ht),\ngÏ‘ â†âˆ‡ht log p(zâ€²\n1|zt\n1, ht).\n23:\nCompute the conditional score\nsÎ¸ â†Î¶Î¸\nt gÎ¸ âˆ’\n1\nâˆš1âˆ’Â¯Î±t Ë†ÏµÎ¸ and sÏ‘ â†Î¶Ï‘\nt gÏ‘ âˆ’\n1\nâˆš1âˆ’Â¯Î±t Ë†ÏµÏ‘.\n24:\nSample Ï†Î¸, Ï†Ï‘ âˆ¼N(0, I).\n25:\nCompute ztâˆ’1\n1\n=\n1\nâˆšÎ±t (zt\n1 + Î²tsÎ¸) + âˆšÎ²tÏ†Î¸ and\nhtâˆ’1 =\n1\nâˆšÎ±t (ht + Î²tsÏ‘) + âˆšÎ²tÏ†Ï‘.\n26:\nhtâˆ’1 â†htâˆ’1 âˆ’Î±(||zâ€²\n1 âˆ’htâˆ’1ztâˆ’1\n1\n||2 + Ï•||htâˆ’1||).\n27: end for\nA. Training Pipeline\nInitially, we start by jointly training the semantic extraction\nand semantic reconstruction modules. This phase ensures\nthe accurate video reconstruction by integrating MSE and\nKullback-Leibler (KL) divergence into the training process.\nThe MSE facilitates accurate video reconstruction, while the\nKL divergence serves as a regularization term, guiding the\ndata distribution in the latent space towards approximating a\nunit Gaussian distribution. The loss function of the semantic\nextraction and reconstruction modules is defined as the sum\nof MSE and KL divergence via\nL = 1\nN\nX\nxâˆˆX\n\f\f\f\fx âˆ’Ëœx\n\f\f\f\f2 + Î»LKL,\n(42)\n\n\n10\nwhere N is the size of dataset X, and Î» controls the signifi-\ncance of KL divergence relative to MSE. The KL divergence\nis expressed as\nLKL = âˆ’1\n2\nN\nX\ni=1\n(1 + log(Ïƒ2\ni ) âˆ’Âµ2\ni âˆ’Ïƒ2\ni ),\n(43)\nwhere Âµi and Ïƒi denote the mean and standard deviation of\nthe latent space, respectively.\nFollowing (42), we freeze the parameters of the semantic\nextraction module and feed the latent semantic vector z1 of\nthe first frame into the semantic denoiser for training. The\ntraining and inference procedures for the SD denoiser and PSD\ndenoiser under both known and unknown channels are detailed\nin Algorithm 2. For the frame interpolation module, we utilize\nthree consecutive frames from each video sample and optimize\nit with the MSE loss function. This training approach ensures\nefficient convergence and robust performance of the SD-\nbased GSC architecture, particularly enhancing video frame\ninterpolation quality.\nOnce each component is individually trained, we finalize the\nprocess by fine-tuning the entire SD-based GSC architecture\nin an end-to-end manner. This fine-tuning stage optimizes the\ncollaboration between all modules, resolving any discrepancies\nthat may arise from independently trained components. The\ndetails of the fine-tuning process is presented in Algorithm 3.\nB. Complexity Analysis\nTo evaluate the efficiency of our proposed SD-based GSC\narchitecture, we analyze the computational complexity of\neach module by measuring the computational operations at\neach layer, as detailed in [34] and [35]. This comprehensive\nanalysis of time and space complexities provides a detailed\nunderstanding of the computational demands of each module\nwithin our architecture. Specifically, in the semantic encoder,\nboth the time and space complexities of the semantic extrac-\ntion module are O(FH2W2) and that of keyframe selection\nmodule are O(F2); in the semantic denoiser, both the time\nand space complexities are O(TH2W2); in the semantic\ndecoder, both the time and space complexities for the semantic\nreconstruction module are O(IH2W2C2), and that of the\nframe interpolation module are O\n\u0000(F âˆ’I)H2W2\u0001\n. Given that\nthe color channel C = 3 and T â‰«F, the overall time\ncomplexity and space complexity of inference process are the\ndominant term O(TH2W2). Regarding the training process,\nthe forward and backward propagation processes effectively\ndoubles the computational cost, but the overall complexity\nremains O(TH2W2) since backward propagation does not\nexceed the complexity of forward propagation.\nVII. PERFORMANCE EVALUATION\nIn this section, we evaluate our proposed SD-based GSC\nframework for wireless video transmission through various\nsimulations. We consider the end-to-end transmission of\nvideo tasks from a device (i.e., NVIDIA RTX 2080TI) to\na server (i.e., NVIDIA A100 80GB) over fading channels.\nThe X4K1000FPS dataset [36], consisting of 4,408 video\nAlgorithm 3: Training Pipeline\nRequire: Training dataset X, time steps T, hyperparameters\n{Î²1, Â· Â· Â· , Î²T }, Î¶Î¸\nt , Î¶Ï‘\nt , Ï•, latency requirement Tmax,\nbandwidth B, transmission power p, channel gain h,\nnoise power Ïƒ2.\nEnsure: Reconstructed video Ëœx.\n// Training process\n1: Jointly train the semantic extraction and reconstruction\nmodules using the loss function in (42).\n2: Train the SD denoiser and PSD denoiser using Alg. 2.\n3: Sample 3 consecutive frames from each video to train\nthe frame interpolation module using MSE loss.\n// Fine-tuning process\n4: Freeze the parameters of the semantic extraction module\nand semantic denoisers.\n5: repeat\n6:\nfor each sample x âˆˆX do\n7:\nPass through the semantic extraction module to\nobtain the latent representation xâ€².\n8:\nSelect keyframes z using Alg. 1.\n9:\nUse SD denoiser to denoise the received noisy SI\nzâ€²\n1 if h is known; otherwise, use PSD denoiser.\n10:\nDenoise the received noisy SI zâ€²\n2 to zâ€²\nI using the\nchannel information captured by semantic denoiser.\n11:\nend for\n12:\nFed the denoised data Ëœz to jointly finetune the\nsemantic reconstruction module and frame\ninterpolation module using MSE loss.\n13: until Converged\nclips with 65 high-resolution (4096Ã—2160) frames each in\nthe training dataset and 15 video clips with 8 high-resolution\nframes each in the test dataset, is used as the benchmark for\nthis evaluation. The wireless connection between the device\nand server operates with a bandwidth B = 5 MHz and\ntransmission power p = 1, with the signal-to-noise ratio\n(SNR) ranging from 0 to 20 dB. The implementation of\nSD-based GSC framework is built on Ubuntu 22.04 using\nPyTorch. For the semantic denoiser, we set the number of\ntime steps T = 1000 and employ a linear variance scheme\nto determine the hyperparameters {Î²1, Î²2, . . . , Î²T}. To align\nwith the principle of SD models for image-to-image tasks, we\nmaintain consistency between training and inference steps to\nensure high-quality image generation [24]. The learning rate\nis initialized at 0.001, and stochastic gradient descent (SGD)\nis used as the optimizer for the loss function.\nA. Baselines\nTo demonstrate the effectiveness of our SD-based GSC\nframework, we perform a comparative analysis against the\nstate-of-the-art baselines:\nâ€¢ ADJSCC [21]: originally designed for image transmis-\nsion, ADJSCC integrates SNR adaptability into a JSCC\nframework to dynamically adjust to changing channel\nconditions by jointly optimizing encoder and decoder\n\n\n11\n\r\u0019\u000f\u0018\u001a\u0017\u0014\u001b\u001b\u0011\u0019\n\u000b\u0011\u0010\u0011\u0014\u001c\u0011\u0019\n\f\u0007\u0004\b\f\u0006\n\n\f\u0007\u0004\b\f\u0006\n\u0005\u000e\b\t\n\u000b\u000f\u001d\u0016\u0011\u0014\u0012\u0013\n\t\u000f\u0015\u000f\u0012\u000f\u0017\u0014\nFig. 6: Example of SD denoiser-based image transmission.\nparameters. For video transmission, we adapted ADJSCC\nby processing each video frame as an independent image\nand applying the framework on a frame-by-frame basis\nfor encoding and transmission.\nâ€¢ Latent-Diff DNSC [4]: a JSCC-based image transmis-\nsion scheme that incorporates the proposed DDPM-\nbased semantic denoising module into JSCC framework.\nSimilar to ADJSCC, we adapted this method for video\ntransmission by treating each frame independently.\nâ€¢ DeepWiVe [19]: divides video frames into fixed Groups\nof Pictures (GOPs) with four frames each. The first frame\nin each GOP is transmitted as a keyframe using JSCC,\nwhile the remaining frames transmit motion-compensated\ndifferences relative to adjacent keyframes, also using\nJSCC with distinct parameters.\nâ€¢ DVST [20]: employs a comparable GOP structure as\nDeepWiVe, integrating a temporal adaptive entropy model\nwith an Artificial Neural Network (ANN)-based nonlinear\ntransform and conditional coding architecture to extract\nSI from video frames.\nB. Metrics\nWe evaluate performance using the following metrics:\nâ€¢ MSE: Quantifies the average squared difference between\nthe estimated and real values.\nâ€¢ Peak Signal-to-Noise Ratio (PSNR): Evaluates image\nreconstruction quality, with higher values indicating bet-\nter performance.\nâ€¢ FrÂ´echet Inception Distance (FID): Assesses the simi-\nlarity between real and reconstructed image distributions,\nwith lower values signify better quality.\nâ€¢ FrÂ´echet Video Distance (FVD): Measures the overall\nquality of reconstructed videos by measuring the distance\nbetween real and reconstructed video distributions, with\nlower values indicating superior reconstruction.\nC. Denoising capacity of SD Denoiser\nTo evaluate the denoising capacity of our proposed SD\ndenoiser and PSD denoiser, we assessed their performance for\nwireless image transmission under AWGN, Rayleigh fading,\nand Nakagami-m channels, respectively. We present one image\nreceived and recovered at the receiver under three channels for\nboth known channel and unknown channel scenarios in Fig.\n6. We can see that the quality of the denoised image remains\nsimilar across different channels when the channel gain is\nknown. However, there is a slight deterioration observed when\nthe channel gain is unknown, particularly noticeable under the\nNakagami-m channel. This degradation, manifesting as more\nblurred backgrounds and subtle distortions in fine details (e.g.,\nthe babyâ€™s eyes), is due to the increased complexity of channel\nestimation especially in Nakagami-m channel. Despite some\ndistortion, the overall performance remains robust, highlight-\ning the effectiveness of our SD denoiser and PSD denoiser\nunder varying channels.\nFig. 7 evaluates the performance of the first frames from\neach video across various SNRs under both known and\nunknown Rayleigh fading channels. As expected, all three\nmetrics improve as the SNR increases, since better wire-\nless channel conditions facilitate higher-quality image re-\nconstruction. Interestingly, under known channels, SD-GSC\nand Latent-Diff DNSC outperform ADJSCC, DeepWiVe and\nDVST in all metrics, due to the diffusion modelâ€™s ability in\ncapturing wireless channel characteristics and removing noise\neffectively, resulting in more accurate reconstruction. Notably,\nSD-GSC outperforms both baselines in all three metrics.\nSpecifically, SD-GSC achieves approximately 53%, 45%, 49%\nand 52% MSE reduction, 47%, 29%, 36% and 44% PSNR\nimprovement, and 37%, 31%, 33% and 35% FID reduction\nas compared to ADJSCC, Latent-Diff DNSC, DeepWiVe and\nDVST, respectively. This superior performance arises from the\nintegration of the instantaneous channel gain, which enables\nbetter characterization of channel gain and mitigation of dis-\ntortion caused by wirless channel noise, resulting in a more\ncontrolled and guided image generation than the uncontrolled\ngeneration process of DDPM used in Latent-Diff DNSC.\nUnder unknown channels, we compare our proposed PSD-\nGSC with MSD-GSC to validate the effectiveness of our\nproposed PSD-GSC for channel estimation as well as efficient\nimage transmission. We can see both PSD-GSC and MSD-\nGSC experience degradation in all metrics compared to SD-\nGSC with known channel gain h. This is because channel\nestimation errors result in inaccurate channel gain inputs of\nthe diffusion model, which decreases its denoising and high-\nquality image reconstruction capabilities. Importantly, PSD-\nGSC outperforms MSD-GSC in all metrics, reducing MSE by\napproximately 21%, improving PSNR by 10% and reducing\nFID by 18%, respectively. This is due to the probabilistic\nmodeling-based diffusion model used in PSD-GSC can well\nestimate nonlinear channels and capture the complex proper-\nties of wireless channels, for better image reconstruction.\nD. Video Generation\nFig. 8 evaluates the keyframe selection process under vary-\ning latency requirements in a known Rayleigh fading channel\nunder the SNR of 0 dB. Under strict latency constraints,\nthe system selects only the most critical motion changes,\n\n\n12\n0\n5\n10\n15\n20\nSNR (dB)\n5\n10\n15\n20\n25\n30\n35\nMSE\nSD-GSC (Known h)\nLatent-Diff DNSC (Known h)[4]\nADJSCC (Known h)[21]\nDeepWiVe (Known h)[19]\nDVST (Known h)[20]\nPSD-GSC (Unknown h)\nMSD-GSC (Unknown h)\n(a) MSE\n0\n5\n10\n15\n20\nSNR (dB)\n12\n16\n20\n24\n28\nPSNR (dB)\nSD-GSC (Known h)\nLatent-Diff DNSC (Known h)[4]\nADJSCC (Known h)[21]\nDeepWiVe (Known h)[19]\nDVST (Known h)[20]\nPSD-GSC (Unknown h)\nMSD-GSC (Unknown h)\n(b) PSNR\n0\n5\n10\n15\n20\nSNR (dB)\n30\n35\n40\n45\n50\n55\n60\n65\nFID\nSD-GSC (Known h)\nLatent-Diff DNSC (Known h)[4]\nADJSCC (Known h)[21]\nDeepWiVe (Known h)[19]\nDVST (Known h)[20]\nPSD-GSC (Unknown h)\nMSD-GSC (Unknown h)\n(c) FID\nFig. 7: The performance of the generated first frame.\nFig. 8: Keyframe selection under SNR = 0 dB.\nwhile more keyframes with significant motion differences are\nchosen as increased latency constraint. This occurs because\nhigher latency budget enables the transmission of additional\nkeyframes, that can capture finer motion details with improve-\nment in video reconstruction quality. This approach effectively\nbalances the trade-off between transmission delay and video\nquality, enhancing the overall performance of the semantic\nvideo communication system by optimizing bandwidth usage,\nensuring transmission of goal-oriented semantic information.\nFig. 9 evaluates the performance of our proposed frame-\nworks for wireless video transmission over a Rayleigh fading\nchannel under a 1s latency constraint. As shown, DeepWiVe\nand DVST achieve better performance than Latent-Diff DNSC,\ndespite Latent-Diff DNSC outperforming them in the first\nframe. This is because the frame-by-frame processing of\nLatent-Diff DNSC faces frame loss due to strict latency\nconstraints, and the uncontrollable frame generation leads to\ndegraded video generation quality. In comparison, our SD-\nGSC achieves significantly better performance than ADJSCC,\nLatent-Diff DNSC, DeepWiVe, and DVST, achieving MSE\nreductions of approximately 52%, 50%, 41%, and 45%, re-\nspectively; PSNR improvements of 69%, 58%, 33%, and\n38%; and FVD reductions of 38%, 32%, 22%, and 24%.\nAdditionally, PSD-GSC reduces MSE by approximately 29%,\nimproves PSNR by 17%, and reduces FVD by 19% compared\nto MSD-GSC. This superior performance is owing to two\nprimary reasons. First, our framework can dynamically select\nkeyframes and identify the differences between consecutive\nkeyframes to reduce the transmitted data volume, subsequently\nreconstructing the missing frames using the frame interpola-\ntion module to improve video transmission quality. Second,\nthe denoising capability of our proposed semantic denoiser\nimproves the characterization of the wireless channel, thereby\nenhancing video generation quality.\nWe can also see that both SD-GSC and PSD-GSC in Fig.\n9 show some performance degradation in video reconstruc-\ntion compared to the generated initial frame in Fig. 7. This\ndegradation is primarily due to reconstruction loss in the\nframe interpolation process, where the interpolated frames may\nnot perfectly match the original frames. Additional factors\ncause this degradation include cumulative errors in the frame\ninterpolation process, especially for frames further away from\nkeyframes, challenges in accurately capturing complex motion\npatterns between keyframes in scenes with rapid or unpre-\ndictable movements, and potential loss of fine details during\nthe keyframe selection and interpolation processes that may\nnot be fully recovered in the reconstructed video. Nevertheless,\nour proposed framework still outperforms existing approaches,\ndemonstrating their effectiveness in balancing video quality\nand latency constraints in wireless transmission scenarios.\nVIII. CONCLUSION\nIn this paper, we have developed a stable diffusion (SD)-\nbased goal-oriented semantic communication (GSC) frame-\nwork for efficient video transmission under various fading\nchannels. The main goal is to achieve high-quality video\nreconstruction at the receiver under latency constraints. To this\nend, we have first designed a semantic encoder to effectively\nextract the relevant semantic information (SI) by identifying\nthe keyframes and their semantic representation thereby re-\nducing the transmission data size. To accurately reconstruct\nthe video from the received semantic information, we then\ndeveloped a semantic decoder to reconstruct the keyframes\nfrom the received SI and further generated the full video\nfrom the reconstructed keyframes using frame interpolation.\nTo combat the negative impact of wireless channel noise on\nthe transmitted SI, we have proposed an SD-based semantic\ndenoiser for known channel scenario to better characterize and\n\n\n13\n0\n5\n10\n15\n20\nSNR (dB)\n10\n15\n20\n25\n30\n35\n40\nMSE\nSD-GSC (Known h)\nLatent-Diff DNSC (Known h)[4]\nADJSCC (Known h)[21]\nDeepWiVe (Known h)[19]\nDVST (Known h)[20]\nPSD-GSC (Unknown h)\nMSD-GSC (Unknown h)\n(a) MSE\n0\n5\n10\n15\n20\nSNR (dB)\n8\n12\n16\n20\n24\n28\nPSNR (dB)\nSD-GSC (Known h)\nLatent-Diff DNSC (Known h)[4]\nADJSCC (Known h)[21]\nDeepWiVe (Known h)[19]\nDVST (Known h)[20]\nPSD-GSC (Unknown h)\nMSD-GSC (Unknown h)\n(b) PSNR\n0\n5\n10\n15\n20\nSNR (dB)\n35\n40\n45\n50\n55\n60\n65\n70\n75\nFVD\nSD-GSC (Known h)\nLatent-Diff DNSC (Known h)[4]\nADJSCC (Known h)[21]\nDeepWiVe (Known h)[19]\nDVST (Known h)[20]\nPSD-GSC (Unknown h)\nMSD-GSC (Unknown h)\n(c) FVD\nFig. 9: The performance of the generated video frames under latency requirement of 1s.\nmitigate the channel noise, resulting in significant improve-\nments in both MSE, PSNR and FVD metrics compared to the\nstate-of-the-art baselines. For unknown channel scenario, we\nhave further designed a parallel SD-GSC (PSD-GSC) to jointly\nlearn the channel gain and denoise the received SI, showcasing\nthe capability of our proposed PSD-GSC in channel estimation\nand efficient video transmission. This work opens new research\navenues for integrating advanced SD models in GSC to achieve\nhigh-quality transmission of video SI in dynamic and noisy\nwireless channel environments. Future work will focus on\noptimizing the application of SD models in GSC to further\nimprove the robustness of multimodal data transmission.\nREFERENCES\n[1] H. Wang, H. Ning, Y. Lin, W. Wang, S. Dhelim, F. Farha, J. Ding,\nand M. Daneshmand, â€œA survey on the metaverse: The state-of-the-\nart, technologies, applications, and challenges,â€ IEEE Internet Things\nJ., vol. 10, no. 16, pp. 14 671â€“14 688, May 2023.\n[2] Z. Wang, Y. Deng, and A. Hamid Aghvami, â€œGoal-oriented semantic\ncommunications for avatar-centric augmented reality,â€ IEEE Trans.\nCommun., pp. 1â€“1, Jun. 2024.\n[3] Y. Deng, Y. Liu, N. Pappas, J. Zhang, Y. Wang, and Y. Wang, â€œGuest\neditorial: Task-oriented communications and networking for the internet\nof things,â€ IEEE Internet Things Mag., vol. 6, no. 4, pp. 8â€“9, Dec. 2023.\n[4] B. Xu, R. Meng, Y. Chen, X. Xu, C. Dong, and H. Sun, â€œLatent\nsemantic diffusion-based channel adaptive de-noising semcom for future\n6g systems,â€ in Proc. IEEE Global Commun. Conf. (GLOBECOM),\nKuala Lumpur, Malaysia, Dec. 2023, pp. 1229â€“1234.\n[5] X. Luo, H.-H. Chen, and Q. Guo, â€œSemantic communications: Overview,\nopen issues, and future research directions,â€ IEEE Wireless Commun.,\nvol. 29, no. 1, pp. 210â€“219, Jan. 2022.\n[6] M. Kountouris and N. Pappas, â€œSemantics-empowered communication\nfor networked intelligent systems,â€ IEEE Commun. Mag., vol. 59, no. 6,\npp. 96â€“102, Jun. 2021.\n[7] J. Dai, P. Zhang, K. Niu, S. Wang, Z. Si, and X. Qin, â€œCommunication\nbeyond transmitting bits: Semantics-guided source and channel coding,â€\nIEEE Wireless Commun., vol. 30, no. 4, pp. 170â€“177, Aug. 2023.\n[8] E. C. Strinati, P. Di Lorenzo, V. Sciancalepore, A. Aijaz, M. Kountouris,\nD. GÂ¨undÂ¨uz, P. Popovski, M. Sana, P. A. Stavrou, B. Soret et al., â€œGoal-\noriented and semantic communication in 6G AI-native networks: The\n6G-GOALS approach,â€ arXiv preprint arXiv:2402.07573, Feb. 2024.\n[9] E. C. Strinati and S. Barbarossa, â€œ6g networks: Beyond shannon towards\nsemantic and goal-oriented communications,â€ Comput. Netw., vol. 190,\np. 107930, 2021.\n[10] H. Zhou, Y. Deng, X. Liu, N. Pappas, and A. Nallanathan, â€œGoal-\noriented semantic communications for 6g networks,â€ IEEE Internet\nThings Mag., vol. 7, no. 5, pp. 104â€“110, Aug. 2024.\n[11] W. Wu, Y. Yang, Y. Deng, and A. Hamid Aghvami, â€œGoal-oriented\nsemantic communications for robotic waypoint transmission: The value\nand age of information approach,â€ IEEE Trans. Wireless Commun., pp.\n1â€“1, Jul. 2024.\n[12] E. Bourtsoulatze, D. Burth Kurka, and D. GÂ¨undÂ¨uz, â€œDeep joint source-\nchannel coding for wireless image transmission,â€ IEEE Trans. Cogn.\nCommun. Netw., vol. 5, no. 3, pp. 567â€“579, May 2019.\n[13] D. B. Kurka and D. GÂ¨undÂ¨uz, â€œBandwidth-agile image transmission\nwith deep joint source-channel coding,â€ IEEE Trans. Wireless Commun.,\nvol. 20, no. 12, pp. 8081â€“8095, Jun. 2021.\n[14] P. Jiang, C.-K. Wen, S. Jin, and G. Y. Li, â€œWireless semantic communi-\ncations for video conferencing,â€ IEEE J. Sel. Areas Commun., vol. 41,\nno. 1, pp. 230â€“244, Nov. 2023.\n[15] N. Li, A. Iosifidis, and Q. Zhang, â€œAttention-based feature compression\nfor cnn inference offloading in edge computing,â€ in Proc. IEEE Int.\nConf. Commun. (ICC), Rome, Italy, May 2023, pp. 967â€“972.\n[16] D. Huang, F. Gao, X. Tao, Q. Du, and J. Lu, â€œToward semantic\ncommunications: Deep learning-based image semantic coding,â€ IEEE\nJ. Sel. Areas Commun., vol. 41, no. 1, pp. 55â€“71, Nov. 2023.\n[17] W. Zhu, Y. Huang, X. Xie, W. Liu, J. Deng, D. Zhang, Z. Wang,\nand J. Liu, â€œAutoshot: A short video dataset and state-of-the-art shot\nboundary detection,â€ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Vancouver, BC, Canada, Jun. 2023.\n[18] N. Li, M. Bennis, A. Iosifidis, and Q. Zhang, â€œSpatiotemporal attention-\nbased semantic compression for real-time video recognition,â€ in Proc.\nIEEE Global Commun. Conf. Workshops (GC Wkshps), Kuala Lumpur,\nMalaysia, Dec. 2023, pp. 1603â€“1608.\n[19] T.-Y. Tung and D. GÂ¨undÂ¨uz, â€œDeepwive: Deep-learning-aided wireless\nvideo transmission,â€ IEEE J. Sel. Areas Commun., vol. 40, no. 9, pp.\n2570â€“2583, Jul. 2022.\n[20] S. Wang, J. Dai, Z. Liang, K. Niu, Z. Si, C. Dong, X. Qin, and\nP. Zhang, â€œWireless deep video semantic transmission,â€ IEEE J. Sel.\nAreas Commun., vol. 41, no. 1, pp. 214â€“229, Jan 2023.\n[21] J. Xu, B. Ai, W. Chen, A. Yang, P. Sun, and M. Rodrigues, â€œWireless\nimage transmission using deep source channel coding with attention\nmodules,â€ IEEE Trans. Circ. Syst. Video Tech., vol. 32, no. 4, pp. 2315â€“\n2328, May 2022.\n[22] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, â€œGenerative adversarial nets,â€ in\nAdv. Neural Inf. Process. Syst. (NeurIPS), vol. 27, Dec. 2014.\n[23] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, â€œDiffusion\nposterior sampling for general noisy inverse problems,â€ in Proc. Int.\nConf. Learn. Represent. (ICLR), Kigali, Rwanda, May 2023.\n[24] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\nâ€œHigh-resolution image synthesis with latent diffusion models,â€ in Proc.\nIEEE/CVF Comput. Vis. Pattern Recognit. Conf. (CVPR), New Orleans,\nLouisiana, Jun. 2022, pp. 10 684â€“10 695.\n[25] Y. Jiang, S. Yang, T. L. Koh, W. Wu, C. C. Loy, and Z. Liu,\nâ€œText2performer:\nText-driven\nhuman\nvideo\ngeneration,â€\nin\nProc.\nIEEE/CVF Int. Conf. Comp. Vis. (ICCV), Paris, France, Oct. 2023, pp.\n22 747â€“22 757.\n[26] T. Wu, Z. Chen, D. He, L. Qian, Y. Xu, M. Tao, and W. Zhang, â€œCddm:\nChannel denoising diffusion models for wireless communications,â€ IEEE\nTrans. Wireless Commun., vol. 23, no. 9, pp. 11 168â€“11 183, Sept. 2024.\n[27] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,\nE. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, â€œPytorch: An imperative style, high-\n\n\n14\nperformance deep learning library,â€ in Adv. Neural Inf. Process. Syst.\n(NeurIPS), vol. 32, Dec. 2019.\n[28] O. Ronneberger, P. Fischer, and T. Brox, â€œU-net: Convolutional networks\nfor biomedical image segmentation,â€ in Med. Image Comput. Comput.\nAssist. Interv. (MICCAI), Nov. 2015, pp. 234â€“241.\n[29] J. Ho, A. Jain, and P. Abbeel, â€œDenoising diffusion probabilistic models,â€\nin Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 33, Dec. 2020, pp.\n6840â€“6851.\n[30] P. Vincent, â€œA connection between score matching and denoising au-\ntoencoders,â€ Neural Comput., vol. 23, no. 7, pp. 1661â€“1674, Jul. 2011.\n[31] K. Kim and J. C. Ye, â€œNoise2score: Tweedieâ€™s approach to self-\nsupervised image denoising without clean images,â€ in Adv. Neural Inf.\nProcess. Syst. (NeurIPS), vol. 34, Dec. 2021, pp. 864â€“874.\n[32] A. Punchihewa, V. K. Bhargava, and C. Despins, â€œCapacity and power\nallocation for cognitive mac with imperfect channel estimation,â€ IEEE\nTrans. Wireless Commun., vol. 10, no. 12, pp. 4001â€“4007, Nov. 2011.\n[33] G. Zhang, Y. Zhu, H. Wang, Y. Chen, G. Wu, and L. Wang, â€œExtracting\nmotion and appearance via inter-frame attention for efficient video frame\ninterpolation,â€ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Vancouver, BC, Canada, Jun. 2023, pp. 5682â€“5692.\n[34] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, â€œPruning\nconvolutional neural networks for resource efficient inference,â€ in Proc.\nInt. Conf. Learn. Represent. (ICLR), April 2017.\n[35] N. Li, A. Iosifidis, and Q. Zhang, â€œCollaborative edge computing\nfor distributed cnn inference acceleration using receptive field-based\nsegmentation,â€ Comput. Netw., vol. 214, p. 109150, Jul. 2022.\n[36] H. Sim, J. Oh, and M. Kim, â€œXvfi: extreme video frame interpolation,â€\nin Proc. IEEE/CVF Int. Conf. Comp. Vis. (ICCV), October 2021, pp.\n14 489â€“14 498.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20927v1.pdf",
    "total_pages": 14,
    "title": "Goal-Oriented Semantic Communication for Wireless Video Transmission via Generative AI",
    "authors": [
      "Nan Li",
      "Yansha Deng",
      "Dusit Niyato"
    ],
    "abstract": "Efficient video transmission is essential for seamless communication and\ncollaboration within the visually-driven digital landscape. To achieve low\nlatency and high-quality video transmission over a bandwidth-constrained noisy\nwireless channel, we propose a stable diffusion (SD)-based goal-oriented\nsemantic communication (GSC) framework. In this framework, we first design a\nsemantic encoder that effectively identify the keyframes from video and extract\nthe relevant semantic information (SI) to reduce the transmission data size. We\nthen develop a semantic decoder to reconstruct the keyframes from the received\nSI and further generate the full video from the reconstructed keyframes using\nframe interpolation to ensure high-quality reconstruction. Recognizing the\nimpact of wireless channel noise on SI transmission, we also propose an\nSD-based denoiser for GSC (SD-GSC) condition on an instantaneous channel gain\nto remove the channel noise from the received noisy SI under a known channel.\nFor scenarios with an unknown channel, we further propose a parallel SD\ndenoiser for GSC (PSD-GSC) to jointly learn the distribution of channel gains\nand denoise the received SI. It is shown that, with the known channel, our\nproposed SD-GSC outperforms state-of-the-art ADJSCC, Latent-Diff DNSC, DeepWiVe\nand DVST, improving Peak Signal-to-Noise Ratio (PSNR) by 69%, 58%, 33% and 38%,\nreducing mean squared error (MSE) by 52%, 50%, 41% and 45%, and reducing\nFr\\'echet Video Distance (FVD) by 38%, 32%, 22% and 24%, respectively. With the\nunknown channel, our PSD-GSC achieves a 17% improvement in PSNR, a 29%\nreduction in MSE, and a 19% reduction in FVD compared to MMSE\nequalizer-enhanced SD-GSC. These significant performance improvements\ndemonstrate the robustness and superiority of our proposed methods in enhancing\nvideo transmission quality and efficiency under various channel conditions.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}