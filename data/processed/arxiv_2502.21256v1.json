{
  "id": "arxiv_2502.21256v1",
  "text": "ALVI Interface: Towards Full Hand Motion Decoding\nfor Amputees Using sEMG\nAleksandr Kovalev\nAnna Makarova\nPetr Chizhov\nMatvey Antonov\nGleb Duplin\nVladislav Lomtev\nViacheslav Gostevskii\nVladimir Bessonov\nAndrey Tsurkan\nMikhail Korobok\nAleksejs Timˇcenko\nALVI Labs\nkoval.alvi@gmail.com\nAbstract\nWe present a system for decoding hand movements using surface EMG signals.\nThe interface provides real-time (25 Hz) reconstruction of finger joint angles\nacross 20 degrees of freedom, designed for upper limb amputees. Our offline\nanalysis shows 0.8 correlation between predicted and actual hand movements. The\nsystem functions as an integrated pipeline with three key components: (1) a VR-\nbased data collection platform, (2) a transformer-based model for EMG-to-motion\ntransformation, and (3) a real-time calibration and feedback module called ALVI\nInterface. Using eight sEMG sensors and a VR training environment, users can\ncontrol their virtual hand down to finger joint movement precision, as demonstrated\nin our video: youtube link.\n1\nIntroduction\nUpper limb amputation has substantial physical, psychological, and occupational impacts on individ-\nuals (1). Even the most advanced bioelectric prostheses cannot completely solve the problem of low\ndegree of freedom and control flexibility, which is relevant for the users. The main challenge is to\ncreate a universal and convenient control system for the prosthesis, simulating the natural control\nof a real hand. In recent works (2; 3), authors have presented systems that convert muscle electrical\nsignals from the surface of the forearm (sEMG) into precise hand movements in healthy people.\nHowever, for people with amputation, creating such a system remains difficult due to the absence\nof target movements to train the decoders.(4; 5) In this study, we present a system for decoding\nindividual finger movements using sEMG signals for people with hand amputation in real-time. Our\napproach includes:\n• a VR setup for collecting paired datasets of finger movements and forearm muscle activity\nin amputees, which provides essential training data;\n• a transformer-based model for decoding individual finger movements from sEMG signals,\nwhich processes this data;\n• the ALVI Interface, a real-time system that enables adaptive control and visualization of a\nvirtual hand with 20 degrees of freedom.\n2\nMethods\n2.1\nDataset\nWe developed a VR application to collect accurate hand movement data from amputees, crucial for\ntraining our sEMG-based decoding algorithm (6). The system consists of three main components:\narXiv:2502.21256v1  [cs.LG]  28 Feb 2025\n\n\nan experimental environment, a hand reflection module, and a data aggregation module (figure 1).\nThe VR environment features a display screen that shows instructions and guides users through\nspecific movement sequences. This controlled setting enables participants to practice diverse hand\nmotions while receiving real-time visual feedback. The hand reflection module was specifically\ndesigned to precisely capture target finger positions for the amputated hand. This is accomplished by\ntracking the coordinates of the fingers on the participant’s intact hand using the Oculus Quest hand\ntracking system. These coordinates are then mirrored to create a virtual 3D model of the absent hand,\nreflecting the movements of the intact hand in a symmetrical manner. The data aggregation module\nsynchronizes all input data, including finger positions and signals from sEMG sensors (obtained via\nwireless Myo Armband by Thalmic Labs). We used the open-source Lab Streaming Layer (LSL)\nframework (7) to facilitate precise time-synchronized streaming of all data channels in real-time.\nFigure 1: Data collection system. Participant with VR headset and sEMG armband performs\nmovements with intact hand. System tracks movements, mirrors them to create virtual model of\nabsent hand, and records muscle activity from residual limb.\nWe implemented real-time inference for prosthetic hand control. After a brief calibration with simple\ngestures, users can perform any movement and see the virtual hand respond in the real-time.\nExperiment\nOur experiment included 72 daily-life gestures (45 dynamic, 27 static), performed\nsymmetrically with both hands. We had 22 participants (20 non-amputees, 2 amputees), each\ncompleting 3 sessions on different days: 2 for training and 1 for testing. Sessions lasted 1-1.5 hours,\nwith movements repeated for 1 minute each.\nData preprocessing\nRaw EMG activity (200 Hz sampling frequency) is normalized to the [-1, 1]\nrange using min-max scaling. Target movements are encoded as quaternions for 21 joint orientations,\nnormalized relative to the palm position. We extract 4 angles for each finger, following the approach\npresented in NeuroPose3D. The electrode order for the left-hand data is rearranged to match the\nright-hand configuration to enable cross-hand compatibility. Movement data is downsampled from 40\nHz to 25 Hz for real-time processing. Our many-to-many approach uses a 1.28-second input window\n(8 channels, 256 time points) to predict 32 time points of 20 movement-encoding variables.\n2.2\nModel\nWe introduce HandFormer, a transformer-based architecture designed for EMG-to-motion translation.\nThe model consists of an Encoder and a Decoder, optimized for processing EMG signals and\ngenerating hand movements. The EMG data are split into patches, each patch representing one\nelectrode’s activity over 8 time points (input shape: [8, 256], patch size (1, 8), resulting in 256\ntotal tokens). The encoder processes tokenized EMG data to extract relevant features. The Decoder\nemploys a Perceiver-like architecture (8) with 32 learnable queries that match the number of movement\nframes. Notably, we use non-autoregressive prediction, which empirically outperforms autoregressive\n2\n\n\napproaches for our task. HandFormer’s pretraining consists of two sequential stages. The first\nstage, EMG Encoder Pretraining, implements a masked autoencoder architecture (9) with 70% token\nmasking to learn EMG signal patterns. In the second stage, we use the pretrained encoder weights for\nfull model training, optimizing hand pose predictions using L1 loss between the predicted and target\njoint angles.\nFigure 2: Architecture of the model. The HandFormer architecture transforms muscle activity\n(sEMG) into hand movements through a two-stage process. The Encoder (left) tokenizes sEMG\nsignals from 8 channels into patches and extracts relevant features. The Decoder (right) employs a\nPerceiver-like architecture with 32 learnable queries corresponding to predicted movement frames.\nThis non-autoregressive design enables efficient real-time translation of muscle signals into precise\nfinger joint angles across 20 degrees of freedom.\n2.3\nALVI Interface\nBuilding upon our data collection platform, ALVI Interface extends the system from passive recording\nto active bidirectional control. While the initial system focuses on capturing training data, ALVI adds\nreal-time decoding, visual feedback, and adaptive tuning capabilities, enabling users to immediately\nsee their intended movements and participate in refining the model through interactive training.\nThis evolution creates a comprehensive motion decoding system that enables practical, personalized\ncontrol for amputees. (Figure 3).\npredict.py\nClient\nPREDICT\nLSL Quats\nServer\nEMG\nArmband\nemg_buffer.py\nLSL EMG\nVR Server\nVR Client\nLSL get\ncontinous data\nLSL EMG\nDATA\nLSL\nAffected hand\nHealthy hand\nVR\nheadset\nAI Client\nDatasets\nEMG + Hand\nAI Server\nDatasets\nWeights\ntrain.py\nLaptop\nAI Server\nVR Headset\nFigure 3: ALVI Interface system architecture..\n3\n\n\nSystem Architecture: The distributed pipeline connects four key components: sEMG armband\n(input), VR headset (visualization), laptop (processing), and AI server (adaptation). Laptop processes\nsEMG signals to generate real-time hand movement predictions displayed in VR. Simultaneously, data\nis streamed to the AI server, which continuously updates the prediction model and sends improved\nweights back to the laptop, creating an adaptive learning loop between the system and user.\nInteractive Adaptation: ALVI Interface implements a novel approach to model calibration through\ninteractive real-time training. During a 10-minute session, users perform movements while simulta-\nneously observing their virtual hand’s response, allowing them to:\n• Immediately see the quality of movement reconstruction\n• Focus on specific gestures that need improvement\n• Actively guide the training process based on visual feedback\nThe system continuously finetunes the pretrained HandFormer model to the user’s sEMG patterns,\nupdating weights every 10 seconds. This interactive loop combines both new and historical data,\nensuring that each session builds upon previous ones while adapting to current conditions. The\nchallenge of implementing closed-loop adaptive systems with real-time constraints is also being\naddressed in brain-computer interfaces, as demonstrated by the BRAND platform (10).\nReal-time Performance: The system processes sEMG signals through a 256-point sliding window\nat 200 Hz, predicting 32 frames of movement at 25 Hz. It uses the most recent predicted frame for\nimmediate control, with exponential moving average applied to joint angles for smoothness. This\nenables responsive, natural hand control with minimal latency.\nAfter initial adaptation, the Interface runs independently on the laptop, enabling seamless hand control\nin VR without requiring constant server connection. This architecture provides a practical foundation\nfor long-term prosthetic control in virtual environments, significantly enhancing user autonomy and\nexperience.\n3\nResults\nOur system demonstrated strong performance in both quantitative metrics and qualitative assessments.\nThe evaluation was conducted across three key dimensions: offline accuracy, real-time performance,\nand user experience.\nQuantitative Performance\nIn offline testing with 22 participants (20 non-amputees, 2 amputees),\nthe system achieved finger movement reconstruction with a correlation of 0.86 for non-amputee\nparticipants and 0.80 for amputee participants. The mean angular error was 8.09° and 14.50°\nrespectively. Notably, our system is among the first to demonstrate such high performance levels for\namputee users.\nTable 1: Performance comparison between non-amputees and amputees\nSubject Group\nAngle error (◦)\nCorrelation\nNon-amputees\n8.09\n0.86\nAmputees\n14.50\n0.80\nReal-time Performance\nThe system operates in real-time at 25 Hz with a latency of 51.2 ms\n(1.28-second window / 25 frames). After a 10-minute calibration period, users can perform natural\nhand movements in VR with smooth response. The continuous adaptation mechanism maintains\nconsistent performance throughout extended usage sessions, with model weights updating every 10\nseconds based on new sEMG patterns.\nUser Experience\nWe conducted extensive qualitative evaluation with amputee participants (n=2)\nover multiple sessions. Our participants’ interaction with the system revealed an interesting learning\ndynamic. Initially, users performed predefined gestures without system feedback to establish baseline\ndata collection. After implementing real-time feedback through ALVI Interface, we observed\n4\n\n\nsignificant improvements in control quality. Within the first 10 minutes of interactive training, users\ngained precise control over individual finger movements, showing rapid adaptation to the system.\nA particularly interesting finding emerged after 30 minutes of use, when participants reported a\nmutual learning phenomenon - both the system and users adapted their behavior to achieve better\nresults, similar to findings reported in other myoelectric interfaces (11).\nUsers noticed they were unconsciously adjusting their muscle activation patterns to match the model’s\nexpected inputs, while the system continuously refined its predictions based on user behavior.\nThe system’s ability to retain personalized models across sessions proved valuable. Each subsequent\nsession required less adaptation time, as both the system and user retained their learned patterns\nfrom previous interactions. This continuous improvement cycle created an increasingly natural and\nresponsive interface, with users reporting more intuitive control in each session.\nThese observations suggest that our approach of combining initial structured training with interactive\nlearning leads to a more personalized and effective user experience. The decreasing adaptation time\nacross sessions indicates successful long-term learning on both the system and user sides.\n4\nDiscussion\nOur findings indicate that ALVI Interface can provide high-fidelity finger movement decoding in\nreal-time using sEMG signals, even for individuals with upper limb amputation. A key challenge\nremains the inherent variability of sEMG signals—affected by electrode placement, muscle fatigue,\nand day-to-day fluctuations—necessitating regular calibration. Our co-adaptive approach offers a\npractical solution: the system continuously updates the model based on user input while users adapt\ntheir muscle activation patterns, resulting in rapid proficiency gains within each session.\nThis co-adaptation capability is particularly valuable for individuals with amputation, as they can\nquickly learn fine motor control in VR. Over multiple sessions, participants reported a growing sense\nof intuitive control, suggesting that sustained use further refines both the user’s activation strategy\nand the system’s decoding performance. Beyond prosthetic control, the same methodology could\nbenefit stroke or injury rehabilitation by providing real-time visualization of intended movements,\nenhancing both patient motivation and clinical insights.\nWhile the results are promising, larger clinical trials with more amputees are needed to generalize\nthese findings and address known challenges in myoelectric prosthesis control (12). Future work\nwill focus on improving the long-term stability of the interface, integrating position-invariant sEMG\ndecoding, and exploring advanced adaptation techniques to reduce calibration frequency. Further\nresearch should also investigate integrating ALVI with physical prostheses for real-world tasks, as\nwell as exploring how VR-based training might accelerate users’ functional recovery.\nReferences\n[1] H Shahsavari, P Matourypour, S Ghiyasvandian, A Ghorbani, F Bakhshi, M Mahmoudi, et al.\nUpper limb amputation; care needs for reintegration to life: An integrative review. International\njournal of orthopaedic and trauma nursing, 38:100773, 2020.\n[2] Y Liu, S Zhang, and M Gowda. Neuropose: 3d hand pose tracking using emg wearables. In\nThe Web Conference 2021 - Proceedings of the World Wide Web Conference, WWW 2021, pages\n1471–1482. Association for Computing Machinery, Inc, 2021.\n[3] David\nSussillo,\nPatrick\nKaifosh,\nand\nThomas\nReardon.\nA\ngeneric\nnoninva-\nsive neuromotor interface for human-computer interaction.\nbioRxiv, 2024.\ndoi:\nhttps://doi.org/10.1101/2024.02.23.581779.\n[4] D Farina, N Jiang, H Rehbaum, A Holobar, B Graimann, H Dietl, and OC Aszmann. The\nextraction of neural information from the surface emg for the control of upper-limb prostheses:\nemerging avenues and challenges. IEEE Transactions on Neural Systems and Rehabilitation\nEngineering, 22(4):797–809, 2014.\n5\n\n\n[5] F Cordella, AL Ciancio, R Sacchetti, A Davalli, AG Cutti, E Guglielmelli, and L Zollo.\nLiterature review on needs of upper limb prosthesis users. Frontiers in Neuroscience, 10:209,\n2016.\n[6] Alexander Kovalev, Anna Makarova, Matvey Antonov, Petr Chizhov, Vladislav Aksiotis, Andrey\nTsurkan, Alexey Timchenko, Viacheslav Gostevskii, Vladislav Lomtev, Gleb Duplin, and Alex\nOssadtchi. Augmented mirror hand (miranda): Advanced training system for new generation\nprosthesis. In HCI (44), pages 77–83, 2023.\n[7] C Kothe. Lab streaming layer (lsl). GitHub repository, 2014.\n[8] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao\nCarreira. Perceiver: General perception with iterative attention. ArXiv, 2021. /abs/2103.03206\nhttps://doi.org/10.48550/arXiv.2103.03206.\n[9] K He, X Chen, S Xie, Y Li, P Dollár, and R Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16000–16009, 2022.\n[10] Yahia H Ali, Kevin Bodkin, Mattia Rigotti-Thompson, Kushant Patel, Nicholas S Card, Bareesh\nBhaduri, Samuel R Nason-Tomaszewski, Domenick M Mifsud, Xianda Hou, Claire Nicolas,\nShane Allcroft, Leigh R Hochberg, Nicholas Au Yong, Sergey D Stavisky, Lee E Miller,\nDavid M Brandman, and Chethan Pandarinath. Brand: a platform for closed-loop experiments\nwith deep network models. Journal of Neural Engineering, 21(2):026046, apr 2024.\n[11] JM Hahne, M Markovic, and D Farina. User adaptation in myoelectric man-machine interfaces.\nScientific Reports, 7(1):1–10, 2017.\n[12] A Chadwell, L Kenney, S Thies, A Galpin, and J Head. The reality of myoelectric prostheses:\nUnderstanding what makes these devices difficult for some users to control. Frontiers in\nNeurorobotics, 10:7, 2016.\n6\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21256v1.pdf",
    "total_pages": 6,
    "title": "ALVI Interface: Towards Full Hand Motion Decoding for Amputees Using sEMG",
    "authors": [
      "Aleksandr Kovalev",
      "Anna Makarova",
      "Petr Chizhov",
      "Matvey Antonov",
      "Gleb Duplin",
      "Vladislav Lomtev",
      "Viacheslav Gostevskii",
      "Vladimir Bessonov",
      "Andrey Tsurkan",
      "Mikhail Korobok",
      "Aleksejs Timčenko"
    ],
    "abstract": "We present a system for decoding hand movements using surface EMG signals.\nThe interface provides real-time (25 Hz) reconstruction of finger joint angles\nacross 20 degrees of freedom, designed for upper limb amputees. Our offline\nanalysis shows 0.8 correlation between predicted and actual hand movements. The\nsystem functions as an integrated pipeline with three key components: (1) a\nVR-based data collection platform, (2) a transformer-based model for\nEMG-to-motion transformation, and (3) a real-time calibration and feedback\nmodule called ALVI Interface. Using eight sEMG sensors and a VR training\nenvironment, users can control their virtual hand down to finger joint movement\nprecision, as demonstrated in our video: youtube link.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}