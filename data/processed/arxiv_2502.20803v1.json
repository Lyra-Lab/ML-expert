{
  "id": "arxiv_2502.20803v1",
  "text": "Two-Stream Spatial-Temporal Transformer\nFramework for Person Identification via Natural\nConversational Keypoints\nMasoumeh Chapariniya∗, Hossein Ranjbar, Teodora Vukovi´c, Sarah Ebling, Volker Dellwo\n∗Department of Computational Linguistics, University of Zurich, Zurich, Switzerland\nmasoumeh.chapariniya@uzh.ch, hoseinrbr@gmail.com, teodora.vukovic2@uzh.ch,\nebling@cl.uzh.ch, volker.dellwo@uzh.ch\nAbstract—In the age of AI-driven generative technologies,\ntraditional biometric recognition systems face unprecedented\nchallenges, particularly from sophisticated deepfake and face\nreenactment techniques. In this study, we propose a Two-\nStream Spatial-Temporal Transformer Framework for person\nidentification using upper body keypoints visible during online\nconversations, which we term ”conversational keypoints”. Our\nframework processes both spatial relationships between keypoints\nand their temporal evolution through two specialized branches:\na Spatial Transformer (STR) that learns distinctive structural\npatterns in keypoint configurations, and a Temporal Transformer\n(TTR) that captures sequential motion patterns. Using the state-\nof-the-art Sapiens pose estimator, we extract 133 keypoints (based\non COCO-WholeBody format) representing facial features, head\npose, and hand positions. The framework was evaluated on\na dataset of 114 individuals engaged in natural conversations,\nachieving recognition accuracies of 80.12% for the spatial stream,\n63.61% for the temporal stream. We then explored two fusion\nstrategies: a shared loss function approach achieving 82.22%\naccuracy, and a feature-level fusion method that concatenates\nfeature maps from both streams, significantly improving perfor-\nmance to 94.86%. By jointly modeling both static anatomical\nrelationships and dynamic movement patterns, our approach\nlearns comprehensive identity signatures that are more robust\nto spoofing than traditional appearance-based methods.\nIndex Terms—Person identification, conversational keypoints,\nSpatial-temporal transformer, Sapiens pose estimation model.\nI. INTRODUCTION\nThe evolution of person identification systems has seen\na significant shift from reliance on static biometric features\nto the integration of dynamic characteristics. Static features\nencompass physiological traits such as facial geometry, finger-\nprints, and hand measurements [1], which remain consistent\nover time. Dynamic features, on the other hand, pertain to\nbehavioral patterns like gait [2], voice [4], and signature\ndynamics [3], [10], capturing the unique ways individuals\nperform actions. Early research primarily focused on static\nbiometrics; however, the limitations in addressing variations\ndue to pose, expression, and environmental factors led to\nthe exploration of dynamic biometrics. Studies have demon-\nstrated that combining static and dynamic features enhances\nrecognition accuracy. For instance, the fusion of body mea-\nsurements with gait dynamics has been shown to improve\nperson identification performance [14]. Farhadipour et al. [11]\nexplored multimodal approaches for person identification and\nverification by integrating facial and voice features. They\nproposed three fusion strategies: sensor-level, feature-level,\nand score-level fusion. For voice, they employed gamma-\ntonegram representations and x-vectors, while VGGFace2\nwas used for the face modality. While recent studies have\nexplored the recognition of individuals through head, hands,\nand facial movements [5], this area remains underexplored,\nleaving ample opportunities to address numerous unanswered\nquestions. Hill and Johnston [15] explored how head and\nfacial movements independently convey identity-related infor-\nmation beyond static facial features. Using motion capture\nand animation techniques, the authors demonstrate that rigid\nhead movements are particularly effective for distinguishing\nindividuals, while non-rigid facial movements are more useful\nfor categorizing sex. The study highlights the critical role of\ndynamic motion in face-based recognition tasks, providing a\nfoundation for leveraging head and facial movements in person\nidentification systems, especially in contexts where static fea-\ntures are unreliable. Girges et al. [16] investigated the role of\nfacial motion in identity recognition using marker-less motion\ncapture technology to generate realistic facial animations.\nThey captured both rigid (e.g., head movements) and non-\nrigid (e.g., expressions) motions from human actors during\nnatural speech, applying the data to a computer-generated\nface. By eliminating individual facial appearance cues, they\nassessed participants’ ability to discriminate identities solely\nbased on motion patterns. The results demonstrated that\nparticipants could accurately recognize identities using only\nmotion cues, highlighting the significance of dynamic facial\nmotion in human face perception and identity recognition. [20]\ninvestigated the role of different types of facial movements\nin conveying identity information. Using a motion capture\nsystem, they recorded and animated emotional, emotional-in-\nsocial-interaction, and conversational facial movements. Their\nfindings showed that conversational facial movements trans-\nmitted the most identity-related information, while purely\nemotional expressions conveyed the least. They concluded\nthat conversational facial dynamics, with their subtle and\npersonal nuances, offer significant cues for person identifica-\ntion, suggesting their utility for biometric systems in dynamic\nand interactive environments. These studies [15], [16], [20]\nrelied on motion capture techniques and human experiments\narXiv:2502.20803v1  [cs.CV]  28 Feb 2025\n\n\nrather than computational models like deep neural networks.\nPapadopoulos et al. [17] proposed a novel framework for\n3D dynamic face identification using spatio-temporal graph\nconvolutional networks (ST-GCN). Their method utilized 3D\nfacial landmarks to construct graphs capturing both spatial\nand temporal features of dynamic facial expressions. They\nused the BU4DFE [6] dataset which is a 3D dynamic facial\nexpression database consisting of 101 subjects performing 6\nfacial expressions. Kay et al. [18] explored the potential of\nfacial micro-expressions for person recognition using deep\nlearning. Their study demonstrated that subtle and spontaneous\nmicro-expressions provide unique identity-related cues. By\nleveraging the CASME II [8] and SAMM [7] datasets and\nemploying a SlowFast CNN model to capture fine-grained\ntemporal dynamics, they achieved notable improvements in\nrecognition accuracy. Saracbasi et al. [19] introduced the\nMYFED database to analyze six basic emotions for person\nidentification. Their approach leveraged facial landmarks and\ndynamic features across expression phases, such as onset and\napex, to highlight the identity-related significance of emotions\nlike surprise and happiness. The study emphasized the critical\nrole of dynamic emotional expressions in improving biometric\nsystem reliability in real-world conditions. Recent studies\nprimarily rely on datasets collected in controlled laboratory\nsettings when they want the participants to express the six\nuniversal emotion categories. These datasets typically con-\nsist of high-quality videos captured under ideal lighting and\nconsistent camera angles. In contrast, real-world video data,\nthough abundant, is recorded in natural, uncontrolled envi-\nronments, exhibiting significant variations in lighting, camera\nangles, and quality. Our approach utilized a Two-Stream\nSpatial-Temporal Transformer (ST-TR) framework for person\nidentification leveraging both structural and dynamic patterns\nextracted from conversational keypoints. Unlike existing meth-\nods that rely solely on static facial features or simple motion\npatterns, our approach comprehensively analyzes both spatial\nconfigurations and temporal evolution of upper body keypoints\ncaptured during natural conversations from the CANDOR\ncorpus [26]. Our key contributions are as follows:\n• We introduce a holistic framework that leverages 133\nCOCO whole-body keypoints [12] to capture both\nanatomical relationships and motion signatures during\nconversations and combine them to create a robust bio-\nmetric representation.\n• We adopt and adapt the ST-TR architecture [24] origi-\nnally designed for action recognition, as its dual-stream\ndesign is particularly well-suited for our person iden-\ntification task. The architecture’s Spatial Transformer\n(STR) branch excels at capturing distinctive structural\npatterns in keypoint configurations, while its Temporal\nTransformer (TTR) branch effectively models sequential\nmotion patterns. We selected this architecture based on its\nproven ability to disentangle spatial and temporal features\nin skeletal data, which is crucial for distinguishing in-\ndividual behavioral patterns during conversations. While\nprevious applications focused on action classification, we\ndemonstrate that with appropriate modifications to the\nfeature fusion strategy, this architecture can effectively\nlearn person-specific biometric signatures from conversa-\ntional dynamics.\n• We demonstrate the effectiveness of combining spatial\nand temporal information through extensive experiments\non naturalistic conversational data, achieving 82.22% for\nshared loss function method and 94.86% for feature-level\nfusion.\n• We establish conversational keypoints as a promising new\nbiometric modality that is inherently more resilient to\nspoofing compared to traditional appearance-based meth-\nods. By analyzing both spatial structure and temporal\ndynamics, our approach provides a more comprehensive\nidentity signature that is difficult to replicate.\n• By evaluating our framework on the CANDOR corpus\nof natural conversations, we provide insights into how\nindividuals maintain distinctive spatial-temporal patterns\nduring real interactions, opening new directions for be-\nhavioral biometrics that go beyond controlled experimen-\ntal settings.\nThe rest of the paper is organized as follows: Section II\ndescribes our proposed methodology, including the ST-\nTR framework and feature extraction techniques. Sec-\ntion III presents comprehensive experimental results and\nperformance analysis, and Section IV concludes with\ndiscussions and future research directions.\nII. PROPOSED METHOD\nThe block diagram of the proposed method is shown in\nfigure1. This framework consists of the following main sub-\nblocks: a person detection module for identifying and cropping\nthe region of interest specific to the person, a pose estima-\ntion module for extracting keypoints, and a spatial-temporal\ntransformer network for robust identity representation. In the\nrest of this section, we provide a detailed explanation of each\ncomponent. (See Figure 1.)\nA. Person Detection\nThe person detection module forms the foundation of the\nproposed framework. We employed the YOLOv8 model [21],\nan object detection architecture known for its accuracy, speed,\nand efficiency in real-time applications. Its anchor-free design\nand adaptive computation for variable image sizes enable\nreliable detection even in challenging scenarios involving\nocclusions or diverse lighting conditions. In our framework,\nYOLOv8 is utilized to isolate and extract the person’s region,\nensuring high precision in identifying individuals. This step is\ncritical for downstream tasks, as it provides clean, localized in-\nput for subsequent pose estimation and identity representation\nprocesses.\nB. Pose Estimation\nThe pose estimation module leverages the Sapiens model\n[22], a state-of-the-art system trained on over 300 million in-\nthe-wild human images from the Humans-300M dataset. We\n\n\nFig. 1.\nThe block diagram of the proposed method illustrates: 1) input video; 2) person detection and localization; 3) pose estimation using the Sapiens\nmodel; 4) keypoint sequence extraction; and 5) transformer-based identity identification.\nFig. 2. Architecture of the Two-Stream ST-TR framework showing parallel Spatial (STR) and Temporal (TTR) transformer streams with feature extraction,\nself-attention modules, and loss computation.\nutilize the Sapiens-0.3B variant (0.3 billion parameters) to ex-\ntract COCO-WholeBody format keypoints, capturing detailed\nfacial expressions, hand gestures, and body movements. The\nmodel’s robust performance across varying lighting conditions\nand partial occlusions ensures reliable keypoint extraction in\nnatural conversational settings. This high-fidelity pose data is\nthe foundation for our framework’s ability to model distinc-\ntive identity signatures through both structural and temporal\npatterns.\nC. Spatial-Temporal Transformer Network\nThe Spatial-Temporal Transformer Network (ST-TR) serves\nas the core component of our person identification system,\neffectively capturing both spatial relationships and temporal\nmotion patterns of keypoints.\na) Network Architecture: Our implementation processes\nskeleton sequences of dimension (C×T×V×M), where C=3\nrepresents coordinate channels, T=60 frames capture temporal\ndynamics, V=133 represents COCO WholeBody keypoints\n(including face, body, hands, and feet), and M=1 denotes\nsingle-person tracking. The network employs a two-stream\narchitecture as illustrated in figure 2.\nb) Spatial Transformer Stream (STR): The STR stream\nmodels spatial relationships between keypoints within each\nframe through Spatial Self-Attention (SSA). Building on initial\nfeatures extracted by ST-GCN layers [25], SSA computes\ndynamic attention scores across all keypoint pairs:\nAs = softmax\n\u0012(QsKT\ns )\n√\nd\n\u0013\n(1)\nwhere Qs and Ks are learned queries and key matrices. Un-\nlike traditional fixed graph structures, this dynamic attention\nmechanism captures:\n• Distinctive upper body posture configurations\n• Person-specific spatial correlations in keypoint arrange-\nments\nc) Temporal\nTransformer\nStream\n(TTR):\nThe\nTTR\nstream analyzes the temporal evolution of keypoints through\nTemporal Self-Attention (TSA):\nAt = softmax\n\u0012(QtKT\nt )\n√\nd\n\u0013\n(2)\nThis temporal modeling captures dynamic motion signatures\nthrough:\n• Characteristic timing patterns in conversational gestures\n• Distinctive temporal coordination between face and hand\nmovements\n• Long-range dependencies in personal interaction styles\n\n\nD. Training Strategy and Loss Function\nWe employed a four-phase training strategy:\n• Independent STR training: Transfer learning from ac-\ntion recognition on the Kinetics-400 dataset [9] facilitates\nspatial feature extraction, accelerating convergence and\nenhancing performance compared to training from scratch\n• Independent TTR training: Similarly, leveraging pre-\ntrained weights from Kinetics-400 provides a strong\nfoundation for temporal motion understanding, improving\nconvergence and performance by utilizing shared tempo-\nral patterns between action and identity recognition.\n• Joint training with shared loss function: In this sec-\ntion, as shown in Figure 2, we employed a strategy to\ntrain both branches simultaneously, optimizing the entire\ndual-stream architecture with equal weighting of their\nrespective loss functions. Although we tested various\nweighting coefficients, this balanced ratio achieved the\nbest performance.\nLtotal = 0.5 × CELossSTR + 0.5 × CELossTTR\n(3)\nLCross Entropy(ˆy, y) = −\nK\nX\nk=1\ny(k) log ˆy(k)\n(4)\nwhere CELossSTR and CELossTTR are cross-entropy\nlosses for person classification, K denotes the number of\nclasses, y(k) indicates the true label, and ˆy(k) represents\npredicted probabilities.\n• Joint training with feature fusion: We propose a\nfeature-level fusion approach that combines high-level\nfeatures from both streams before classification. Specif-\nically, 256-dimensional L2-normalized feature vectors\nfrom each stream are concatenated channel-wise into a\n512-dimensional representation. This fused vector passes\nthrough a multi-layer fusion classifier with two linear\nlayers (512→512→256) interleaved with batch normal-\nization, ReLU activation, and dropout (0.2), followed by\na final classification layer (256→114). The network is\ntrained end-to-end using a weighted loss combination:\nLtotal = 0.3 × CELossSTR + 0.3 × CELossTTR\n+ 0.4 × CELossFUSION\n(5)\nOur network design is motivated by key insights about person\nidentification from conversational dynamics. The separation\nof spatial and temporal processing through parallel streams\nallows learning both static postural characteristics and dy-\nnamic motion patterns unique to each individual. The attention\nmechanisms adapt to different conversational scenarios, while\nthe architecture’s ability to handle long sequences captures\nsustained behavioral patterns. Our progressive training strategy\nand feature fusion approach ensure complementary informa-\ntion from both streams contributes to identity recognition, with\ndropout and normalization layers preventing overfitting. This\narchitecture thus provides a robust framework for learning\nidentity signatures from natural conversations.\nIII. EXPERIMENTS\nA. Dataset\na) Original Dataset: The emergence of deepfake tech-\nnologies highlights the need to explore dynamic biometric\nfeatures for person identification, as static features become\nincreasingly unreliable in such scenarios. Research by [20]\nhas shown that conversational facial movements carry more\nidentity-specific information than emotional expressions alone.\nTo analyze these dynamic patterns, we utilized the CANDOR\ncorpus, which includes 1,656 natural conversations spanning\nover 850 hours of footage. Unlike databases that focus on\nposed expressions (e.g., BU4DFE, CASME II, MYFED),\nCANDOR captures spontaneous, real-world interactions, mak-\ning it ideal for developing robust biometric features based on\nnatural behavioral patterns. With a diverse participant pool\nrepresenting various demographics, the dataset provides a rich\nsource of conversational data, including vocal, facial, and\ngestural expressions. As one of the largest publicly available\ncollections of naturalistic conversational data, CANDOR is\nuniquely suited for studying dynamic biometric features in\nreal-world conditions.\nb) Our Subset for Person Recognition: To tailor the\nCANDOR dataset for our specific person recognition task, we\nconstructed a subset and processed the CANDOR corpus as\nfollows:\n• Subject Selection: From the complete corpus, we se-\nlected 114 unique individuals who participated in multi-\nple sessions. For each individual, we selected their longest\nrecording session to ensure sufficient data for analysis.\n• Video Segmentation: We segmented the full sessions\ninto shorter utterance clips using the provided transcrip-\ntion timestamps. The resulting clips range from 3.03 to\n4 seconds (average 3.9 seconds). The videos maintain\nframe rate (30 fps) and resolution (320x240 pixels).\n• Data Split: For each subject, we employed an 80:20 train-\ntest split ratio, ensuring the same segments do not appear\nin both sets to avoid potential bias.\n• Temporal Sampling: To optimize computational effi-\nciency while preserving motion dynamics, we imple-\nmented a frame skip parameter of 2, extracting keypoints\nfrom alternating frames.\nB. Implementation Details\nWe implemented our framework using PyTorch and trained\nit on a single NVIDIA GeForce 4080 GPU with 12 GB\nmemory. We employed YOLOv8 for person detection and\nthe Sapiens-0.3B pose estimation model. YOLOv8 detects\nindividuals in each video frame, while Sapiens extracts 133\nkeypoints in the COCO WholeBody format. Video frames\nwere resized to 1024×768 pixels before being processed by\nthe Sapiens model. To reduce computational overhead, we\napplied frame skipping by processing every second frame.\nThe extracted keypoints were stored in structured JSON files\nfor use in our identification framework. We initially train\n\n\neach stem, STR, and TTR, independently with standard cross-\nentropy loss. Subsequently, as illustrated in Figure 2, we train\nthem jointly using the defined shared loss function and feature\nfusion methods. These two transformer streams share several\nkey architectural and training characteristics. Both streams\nprocess sequences of 60 frames, utilize 8 attention heads with\ndimension ratios dk=0.25 and dv=0.25, and are trained for 120\nepochs with learning rate adjustments. For STR, we employed\nthe Adam optimizer with an initial learning rate of 0.01 and\nweight decay of 0.0001. The TTR used SGD optimizer with\nmomentum, featuring an initial learning rate of 0.001 and\nweight decay of 0.0001 In the joint training with the shared\nloss function and feature fusion, we maintained a batch size\nof 32 for training and 8 for testing. The Adam optimizer with\nan initial learning rate of 0.01 and weight decay of 0.0001 is\nutilized for both methods. We used equal weights (0.5) for both\nSTR and TTR streams in the shared loss computation to ensure\nbalanced learning between spatial and temporal features. (See\nEquation 3.) For feature fusion training, the loss function\nformulated in Equation 5 assigns weights to each cross entropy\nloss.\nTABLE I\nPERFORMANCE COMPARISON OF DIFFERENT MODEL ARCHITECTURES\nModel\nName\nNumber of Parameters\nAccuracy (%)\nmAP (%)\nSTR\n3,502,490\n80.12\n79.43\nTTR\n2,543,998\n63.61\n63.14\nSTTR\n+\nShared\nLoss\n6,046,488\n82.22\n82.10\nSTTR\n+\nFeature\nFusion\n6,460,512\n94.86\n94.81\nC. Performance Evaluation\nIn this study, we conducted extensive experiments to eval-\nuate three transformer-based architectures for person identi-\nfication: Spatial Transformer (STR), Temporal Transformer\n(TTR), STTR with shared loss function and feature-level fu-\nsion. Our experimental results show the following progression\nin performance (see Tabel I):\na) Individual Stream Analysis:\n• The STR achieved 80.12% accuracy, demonstrating that\nspatial relationships between facial, hand, and body key-\npoints contain significant identity-discriminative informa-\ntion.\n• The TTR achieves 63.61% accuracy, demonstrating that\ntemporal patterns in conversational dynamics contain\nimportant identity-specific information. This performance\nindicates that individuals exhibit characteristic temporal\nsignatures in their movements during natural interactions..\n• The relatively lower TTR performance compared to STR\nlikely stems from the inherent complexity of modeling\ntemporal dependencies across longer sequences and the\nnatural variation in conversational dynamics.\nb) Joint Stream Performance:\n• The shared loss function approach improved accuracy to\n82.22%, showing modest gains from joint optimization.\n• The feature-level fusion strategy significantly boosted\nperformance to 94.86%, indicating that the careful com-\nbination of spatial and temporal features is crucial for\nrobust identification.\nThe substantial improvement from feature-level fusion stems\nfrom the concatenation of L2-normalized 256-dimensional\nfeatures from each stream, preserving their distinct char-\nacteristics and ensuring balanced contributions. Additional\nfusion layers optimize feature combinations while maintaining\ndiscriminative power, and a weighted loss function ensures\nbalanced learning, preventing dominance by either stream.\nOur analysis reveals key insights about identity information\nin conversational dynamics. The strong performance of the\nspatial stream shows that individuals maintain characteristic\nspatial configurations in their facial and upper body key-\npoints, while temporal features provide crucial complementary\ninformation for resolving ambiguous cases. The significant\nimprovement achieved through feature-level fusion (94.86% vs\n82.22%) demonstrates that spatial and temporal patterns offer\ncomplementary information about identity, suggesting that\nrobust person identification requires considering both static\nconfigurations and dynamic evolution of keypoints during\nconversations.\nc) Comparisons with Other techniques: Table II presents\na comparison of our methods with other research. While direct\ncomparison is challenging due to the unique nature of our\ntask, which focuses on conversational keypoints and natural-\nistic data, our model achieves promising results. Unlike most\nexisting works that concentrate on controlled expressions or\nspecific gestures, our approach addresses the complexities of\nunconstrained conversational behavior, highlighting its novelty\nand effectiveness.\nd) Suggestions and Feature Work: Our framework re-\nveals several promising avenues for future research and im-\nprovement. A critical next step is expanding the dataset\nsize and increasing utterance length from the current 3-4\nseconds to 8-10 seconds, which would allow more robust\ntemporal feature extraction and potentially reduce the impact\nof short-term behavioral variations. The current performance\nvariability, particularly in the temporal stream, suggests the\nneed for more sophisticated sequence modeling approaches\nand advanced feature fusion strategies. Exploring additional\nmodalities like audio cues and developing more sophisticated\nattention mechanisms could further enhance the framework’s\nperson identification capabilities. Future work should also\nfocus on improving generalizability by incorporating more\ndiverse conversational contexts and demographic variations,\nultimately creating a more comprehensive and reliable bio-\nmetric identification system.\nIV. CONCLUSION\nThis work introduces a Two-Stream Spatial-Temporal\nTransformer Framework that achieves 94.86% accuracy in\n\n\nTABLE II\nCOMPARISON OF METHODS FOR PERSON IDENTIFICATION\nRef.\nDataset\nFeatures\nClassifier\nAccuracy\n[18]\nCASME II, SMIC, SAMM,\nSlowFast CNN\nFully connected layer\n94.95%, 89.61%, 87.4%\n[19]\nMYFED\nStatistical facial dynamics fea-\ntures\nKNN, LSTM\nKNN: 88.1%, LSTM: 87.8%\n[17]\nBU4DFE\nSpatio-temporal graph features\nSpatio-Temporal Graph Convolu-\ntional Network (ST-GCN)\n88.45%\n[13]\nSelf-collected\nVGG-face CNN and geometric\nfeatures\nLSTM\n96.2%\nOurs\nCandor’s subset\nSpatial temporal features from\n133 keypoints\nTwo fully-connected layers\n94.86%\nperson identification by leveraging both spatial relationships\nand temporal dynamics of conversational keypoints. Our\nframework’s success on natural conversational data from the\nCANDOR corpus demonstrates the effectiveness of combining\nstructural patterns through STR and dynamic motion through\nTTR, creating robust biometric signatures that are inherently\nresistant to spoofing. The significant improvement achieved\nthrough feature-level fusion highlights the complementary\nnature of spatial and temporal information in conversational\nbehavior. This research establishes conversational keypoints\nas a promising new biometric modality, particularly valuable\nin an era where traditional appearance-based methods face\nincreasing challenges from deepfake technologies.\nREFERENCES\n[1] N. L. Baisa, ”Joint Person Identity, Gender, and Age Estimation from\nHand Images using Deep Multi-Task Representation Learning,” in Proc.\n2024 12th Int. Workshop Biometrics Forensics (IWBF), pp. 01–06, 2024,\nIEEE.\n[2] V. Rani and M. Kumar, “Human gait recognition: A systematic review,”\nMultimedia Tools and Applications, vol. 82, no. 24, pp. 37003–37037,\n2023.\n[3] R. Alrawili, A. A. S. AlQahtani, and M. K. Khan, Comprehensive\nsurvey: Biometric user authentication application, evaluation, and dis-\ncussion,” Comput. Electr. Eng., vol. 119, p. 109485, 2024.\n[4] H. Kheddar, M. Hemis, and Y. Himeur, Automatic speech recognition\nusing advanced deep learning approaches: A survey,” Information Fu-\nsion, vol. –, Article no. 102422, 2024, Elsevier.\n[5] C. Rack, A. Hotho, and M. E. Latoschik, ”Comparison of data encodings\nand machine learning architectures for user identification on arbitrary\nmotion sequences,” in Proc. IEEE Int. Conf. Artificial Intelligence and\nVirtual Reality (AIVR), 2022, pp. 11–19.\n[6] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale, A. Horowitz, and\nP. Liu, “A high-resolution spontaneous 3D dynamic facial expression\ndatabase,” in Proc. 10th IEEE Int. Conf. Workshops on Automatic Face\nand Gesture Recognition (FG), Shanghai, China, 2013, pp. 1–6.\n[7] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap, “SAMM:\nA spontaneous micro-facial movement dataset,” IEEE Trans. Affective\nComput., vol. 9, no. 1, pp. 116–129, 2016.\n[8] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen, and X. Fu,\nCASME II: An improved spontaneous micro-expression database and\nthe baseline evaluation,” PLoS One, vol. 9, no. 1, pp. e86041, 2014.\n[9] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-\nnarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al., “The kinetics\nhuman action video dataset,” arXiv preprint arXiv:1705.06950, 2017.\n[10] A. Battisti, E. van den Bold, A. G¨ohring, F. Holzknecht, and S. Ebling,\nPerson identification from pose estimates in sign language,” University\nof Zurich, 2024.\n[11] A. Farhadipour, M. Chapariniya, T. Vukovic, and V. Dellwo, ”Compar-\native Analysis of Modality Fusion Approaches for Audio-visual Person\nIdentification and Verification,” arXiv preprint arXiv:2409.00562, 2024.\n[12] S. Jin, L. Xu, J. Xu, C. Wang, W. Liu, C. Qian, W. Ouyang, and P. Luo,\n”Whole-Body Human Pose Estimation in the Wild,” in Proc. European\nConf. Computer Vision (ECCV), 2020.\n[13] R. E. Haamer, K. Kulkarni, N. Imanpour, M. A. Haque, E. Avots,\nM. Breisch, K. Nasrollahi, S. Escalera, C. Ozcinar, X. Baro, et al.,\n”Changes in facial expression as biometric: a database and benchmarks\nof identification,” in Proc. 2018 13th IEEE Int. Conf. Automatic Face\n& Gesture Recognition (FG 2018), pp. 621–628, 2018, IEEE.\n[14] L. Wang, H. Ning, T. Tan, and W. Hu, “Fusion of static and dynamic\nbody biometrics for gait recognition,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 14, no. 2, pp. 149–158, Feb.\n2004. DOI: 10.1109/TCSVT.2003.821972.\n[15] H. Hill and A. Johnston, “Categorizing sex and identity from the\nbiological motion of faces,” Current Biol., vol. 11, no. 11, pp. 880–\n885, 2001.\n[16] C. Girges, J. Spencer, and J. O’Brien, “Categorizing identity from facial\nmotion,” Quarterly Journal of Experimental Psychology, vol. 68, no. 9,\npp. 1832–1843, 2015.\n[17] K. Papadopoulos, A. Kacem, D. Aouada, and others, ‘Face-GCN: A\ngraph convolutional network for 3D dynamic face recognition,” in *Proc.\n8th Int. Conf. Virtual Reality (ICVR)*, Apr. 2022, pp. 454–458.\n[18] T. Kay, Y. Ringel, K. Cohen, M.-A. Azulay, and D. Mendlovic, ‘Person\nrecognition using facial micro-expressions with deep learning,” arXiv\npreprint, arXiv:2306.13907, 2023.\n[19] Z. N. Saracbasi, C. E. Erdem, M. Taskiran, and N. Kahraman, ‘MYFED:\na dataset of affective face videos for investigation of emotional facial\ndynamics as a soft biometric for person identification,” Machine Vision\nand Applications, vol. 36, no. 1, pp. 8, 2025, Springer.\n[20] K. Dobs, I. B¨ultthoff, and J. Schultz, ‘Identity information content\ndepends on the type of facial movement,” Scientific Reports, vol. 6,\nno. 1, pp. 34301, 2016.\n[21] G. Jocher, A. Chaurasia, and J. Qiu, “Ultralytics YOLOv8,” version\n8.0.0, 2023. [Online]. Available: https://github.com/ultralytics/ultralytics\n[22] R. Khirodkar, T. Bagautdinov, J. Martinez, Z. Su, A. James, P. Selednik,\nS. Anderson, and S. Saito, “Sapiens: Foundation for human vision\nmodels,” in *European Conference on Computer Vision*, Springer,\n2025, pp. 206–228.\n[23] L. Xu, S. Jin, W. Liu, C. Qian, W. Ouyang, P. Luo, and X. Wang,\nZoomNAS: Searching for whole-body human pose estimation in the\nwild,” IEEE Trans. Pattern Anal. Mach. Intell., 2022.\n[24] C. Plizzari, M. Cannici, and M. Matteucci, “Skeleton-based action\nrecognition via spatial and temporal transformer networks,” Comput.\nVis. Image Underst., vol. 208, p. 103219, 2021.\n[25] S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional\nnetworks for skeleton-based action recognition,” in *Proceedings of the\nAAAI Conference on Artificial Intelligence*, vol. 32, no. 1, 2018.\n[26] A. Reece, G. Cooney, P. Bull, C. Chung, B. Dawson, C. Fitzpatrick, T.\nGlazer, D. Knox, A. Liebscher, and S. Marin, “The CANDOR corpus:\nInsights from a large multimodal dataset of naturalistic conversation,”\nScience Advances, vol. 9, no. 13, pp. eadf3197, 2023.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20803v1.pdf",
    "total_pages": 6,
    "title": "Two-Stream Spatial-Temporal Transformer Framework for Person Identification via Natural Conversational Keypoints",
    "authors": [
      "Masoumeh Chapariniya",
      "Hossein Ranjbar",
      "Teodora Vukovic",
      "Sarah Ebling",
      "Volker Dellwo"
    ],
    "abstract": "In the age of AI-driven generative technologies, traditional biometric\nrecognition systems face unprecedented challenges, particularly from\nsophisticated deepfake and face reenactment techniques. In this study, we\npropose a Two-Stream Spatial-Temporal Transformer Framework for person\nidentification using upper body keypoints visible during online conversations,\nwhich we term conversational keypoints. Our framework processes both spatial\nrelationships between keypoints and their temporal evolution through two\nspecialized branches: a Spatial Transformer (STR) that learns distinctive\nstructural patterns in keypoint configurations, and a Temporal Transformer\n(TTR) that captures sequential motion patterns. Using the state-of-the-art\nSapiens pose estimator, we extract 133 keypoints (based on COCO-WholeBody\nformat) representing facial features, head pose, and hand positions. The\nframework was evaluated on a dataset of 114 individuals engaged in natural\nconversations, achieving recognition accuracies of 80.12% for the spatial\nstream, 63.61% for the temporal stream. We then explored two fusion strategies:\na shared loss function approach achieving 82.22% accuracy, and a feature-level\nfusion method that concatenates feature maps from both streams, significantly\nimproving performance to 94.86%. By jointly modeling both static anatomical\nrelationships and dynamic movement patterns, our approach learns comprehensive\nidentity signatures that are more robust to spoofing than traditional\nappearance-based methods.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}