{
  "id": "arxiv_2502.20249v1",
  "text": "Enhancing 3D Gaze Estimation in the Wild using\nWeak Supervision with Gaze Following Labels\nPierre Vuillecard\nIDIAP, EPFL\npierre.vuillecard@idiap.ch\nJean-Marc Odobez\nIDIAP, EPFL\nodobez@idiap.ch\nVideo\nImage\nGT\nSupervised (Gaze360)\nimage inference\nST-WSGE (Gaze360+GF)\nvideo inference\nST-WSGE (Gaze360+GF)\nimage inference\nFigure 1. Significance of ST-WSGE. Our self-training based weakly-supervised framework for robust 3D gaze estimation in real-world\nconditions (e.g., varying appearance, extreme poses, resolution, and occlusion). All predictions used our image and video agnostic Gaze\nTransformer (GaT) model. Top row: importance of the training diversity using ST-WSGE and GazeFollow (GF) for generalization com-\npared to standard supervised methods. Bottom row: influence of temporal context between image and video inference. Circles in images\nrepresent unit disks where 3D gaze vectors are projected onto the image plane (x, y in yellow) and a top-down view (x, z in blue). Images\nfrom VideoAttentionTarget, GFIE, and MPIIFaceGaze datasets.\nAbstract\nAccurate 3D gaze estimation in unconstrained real-world\nenvironments remains a significant challenge due to vari-\nations in appearance, head pose, occlusion, and the lim-\nited availability of in-the-wild 3D gaze datasets.\nTo\naddress these challenges, we introduce a novel Self-\nTraining Weakly-Supervised Gaze Estimation framework\n(ST-WSGE). This two-stage learning framework leverages\ndiverse 2D gaze datasets, such as gaze-following data,\nwhich offer rich variations in appearances, natural scenes,\nand gaze distributions, and proposes an approach to gen-\nerate 3D pseudo-labels and enhance model generaliza-\ntion. Furthermore, traditional modality-specific models, de-\nsigned separately for images or videos, limit the effective\nuse of available training data. To overcome this, we propose\nthe Gaze Transformer (GaT), a modality-agnostic architec-\nture capable of simultaneously learning static and dynamic\ngaze information from both image and video datasets. By\ncombining 3D video datasets with 2D gaze target labels\nfrom gaze following tasks, our approach achieves the fol-\nlowing key contributions: (i) Significant state-of-the-art im-\nprovements in within-domain and cross-domain generaliza-\ntion on unconstrained benchmarks like Gaze360 and GFIE,\nwith notable cross-modal gains in video gaze estimation;\n(ii) Superior cross-domain performance on datasets such\nas MPIIFaceGaze and Gaze360 compared to frontal face\nmethods. Code and pre-trained models will be released to\nthe community.\n1. Introduction\nNon-verbal behaviors play a crucial role in human com-\nmunication, often conveying more information than words\nalone. Among the various forms of non-verbal cues, eye\ngaze stands out as an important signal for understanding\nhuman behavior, including attention, communication, in-\ntents, and mental state. Consequently, gaze signals have\nbeen used in many applications. Some applications require\naccurate gaze for frontal head pose such as AR/VR [5], 3D\navatar animation [41], human-computer interaction [3], and\ndriver behavior monitoring [26]. While other applications\nfocus on robust 3D gaze estimation from a wide range of\nhead poses such as medical and psychological analysis [28]\nor human-robot interaction [2].\n1\narXiv:2502.20249v1  [cs.CV]  27 Feb 2025\n\n\nIn this paper, our goal is to develop a robust 3D gaze\nestimation for in-the-wild applications with unconstrained\nhead pose and real-world environments. In the literature,\nthis refers to the less explored and highly challenging prob-\nlem of “physically unconstrained gaze estimation” [27].\nMotivations. Estimating gaze in unconstrained, real-world\nsettings poses unique challenges not fully addressed by\ncurrent lab-based datasets, which are primarily collected\nin controlled screen-target setups [18, 19, 30, 39, 50, 53].\nWhile these datasets have enabled recent approaches to\nachieve high accuracy in frontal 3D gaze estimation from\nmonocular images [1, 7, 9–11, 18, 47, 51], their effec-\ntiveness declines in real-world scenarios. This limitation\nstems from restricted gaze distributions, lab-specific condi-\ntions, limited subject diversity, and reliance on potentially\nnoisy head-pose estimates for normalizing eye and face im-\nages [52].\nTo address the lack of data for unconstrained gaze es-\ntimation, Gaze360 [27] and GFIE [24] were developed.\nAlthough these datasets have advanced the field, models\ntrained on them continue to struggle with challenging, real-\nworld conditions (see Fig. 1), particularly when facing ex-\ntreme head poses, partial eye occlusions, varying resolu-\ntions from diverse camera-to-subject distances, and a wide\nrange of appearances (e.g., skin tones, hairstyles, facial ex-\npressions).\nThis limitation is largely due to insufficient\ndiversity in the training data, as collecting high-quality,\nnaturally occurring, and diverse 3D gaze data is complex,\nresource-intensive, and not easily scalable.\nTo overcome these limitations, researchers have ex-\nplored using “secondary” labels that are easier to obtain,\ne.g. by relying on internet data. For example, Kothari et\nal. [29] leveraged 2D gaze direction labels from the “Look-\ning at Each Other” (LAEO) dataset [35]. By applying ge-\nometric constraints and head-size heuristics, they generated\npseudo-3D gaze data.\nWhile this approach showed some generalization im-\nprovement, the authors noted that LAEO’s gaze distribu-\ntion is primarily horizontal and requires images containing\nat least two people with mutual gaze, which limits sample\ndiversity and availability.\nHere we aim to utilize more general 2D gaze annotations\nfrom the gaze following task [14, 40]. Although the ground\ntruth for gaze following is defined as the 2D pixel location\na person in the scene is looking at, we can repurpose it as\n2D gaze direction ground truth. Compared to LAEO, gaze\nfollowing datasets offer greater diversity, with broader gaze\ndistributions and a wider variety of natural scenes.\nIn addition, in contrast to [29], we propose a Self-\nTraining Weakly-Supervised Gaze Estimation (ST-WSGE)\nframework using a two-stage training approach without re-\nlying on heuristics or relative depth estimation to generate\npseudo-3D gaze labels. First, we train a gaze network on\nexisting 3D gaze datasets. We then use this network’s pre-\ndictions on gaze-following data, combined with 2D gaze\nground truth, to create 3D gaze pseudo-labels. In the sec-\nond stage, we retrain a similar gaze network using both\n3D gaze datasets and gaze-following datasets with these\npseudo-labels. Our approach minimizes the need for labor-\nintensive, unconstrained 3D gaze labeling and demonstrates\nsignificant improvements over state-of-the-art methods in\nboth within-domain and cross-domain generalization on\nGaze360, GFIE, and MPIIFaceGaze [50].\nGiven the scarcity of in-the-wild 3D gaze datasets, an-\nother challenge lies in how to leverage both image and video\ndata effectively. Modality-specific models restrict the train-\ning set to modality-specific datasets, limiting their ability\nto benefit from all available resources. While static models\ncan draw on large image datasets such as GazeFollow [14],\ntemporal dynamics are also essential for robust 3D gaze es-\ntimation in unconstrained environments [27, 37], and is par-\nticularly valuable when the eye region is obscured, whether\ndue to occlusions, low resolution, or blinking (see Fig. 1).\nTo address this, one approach is to pre-train a model on\nimages and transfer the weights to a temporal model us-\ning techniques like filter inflation, where 2D filters are ex-\ntended to 3D models, as done in prior adaptations for video\ntasks [6].\nHowever, this method is more suited to fine-\ntuning and risks catastrophic forgetting, where the model\nloses pre-trained knowledge [36]. Alternatively, images can\nbe duplicated to simulate fixed-length video clips, allowing\nfor training on both image and video datasets in a temporal\nmodel. However, this can impair the learned gaze dynam-\nics, as synthetic clips lack genuine motion information.\nTransformers offer a promising solution for handling\nmultiple modalities. Inspired by recent work [20, 21], we\npropose a Gaze Transformer (GaT) designed to encode\nboth image and video inputs into a shared representation.\nThis allows us to leverage labeled datasets more effectively,\nby training jointly on image and video data. We demon-\nstrate better cross-modal generalization, and that using im-\nage datasets enhances video gaze prediction, thus enabling\na more versatile and robust 3D gaze estimation model.\nContributions. They can be summarized as:\n• ST-WSGE, a novel learning framework enhancing\ngeneralization. To address the lack of diverse, natural-\nistic 3D gaze datasets, we leverage 2D gaze-following\ndatasets using 3D pseudo labels. Combining these with\n3D gaze datasets in a two-stage manner, we demonstrate\nimproved 3D generalization on several benchmarks.\n• A visual modality agnostic Gaze Transformer (GaT)\narchitecture making efficient use of existing gaze\ndatasets. By allowing simultaneous learning from 3D\ngaze image and video datasets, it outperforms modality-\nspecific models, resulting in better static and dynamic\ngaze representations, better capturing spatiotemporal pat-\n2\n\n\nterns in head sequences compared to the state-of-the-art.\n• State-of-the-art results. Our approach surpasses exist-\ning methods in both unconstrained (Gaze360, GFIE) and\nconstrained (MPIIFaceGaze) environments, achieving su-\nperior results in within- and cross-dataset evaluations.\nThese contributions position our approach as ideal for real-\nworld unconstrained 3D gaze estimation applications.\n2. Related Work\nOur research pertains to 3 main aspects: unconstrained gaze\nestimation, temporal gaze modeling, and generalization\nusing additional data and labels to bridge the domain gap\nbetween controlled setups and real-world data.\nUnconstrained Gaze Estimation. Most 3D gaze estima-\ntion models address the frontal face gaze prediction task\n[1, 9–11, 18, 47, 51], relying on normalized frontal face\ncrop as input. These methods tend to fail under partial oc-\nclusion of the eyes due to extreme head pose. Nevertheless,\nat 90-135 head pose yaw, a significant part of one eyeball is\nstill often visible and informative for gaze estimation [27].\nFor this reason, few works tackle the most challenging\nsetting of “physically unconstrained gaze estimation”\nwithout constraint on the head pose. Kellnhofer et al. [27]\nare the first to collect a physically unconstrained 3D gaze\ndataset Gaze360 and develop a method that used head\ncrop as input. Then, combining different head crop scales\nproved to be beneficial [8] since more resolution helps on\nthe frontal face while more context is beneficial for profiles\nand back heads. Following this idea, MCGaze [22] used\na spatiotemporal interaction module between head, face,\nand eye features in an end-to-end manner to extract local\neyes and global head features.\nThese approaches focus\non within-data performance, while in this work we aim to\nimprove both within and the generalization as discussed in\nthe following section.\nGeneralization in the Wild.\nBridging the dataset’s\ndomain gap challenge is crucial for 3D gaze estimation in\nreal-world applications.\nTwo trends have been explored\nto adapt to specific target domains effectively:\nOne\nleverages few labeled samples, while the other uses only\nunlabeled samples [13, 25, 27, 31, 45, 46]. In contrast, gaze\ngeneralization models focus on enhancing cross-domain\nperformance without any prior knowledge of the target\ndomain. For instance, the methods proposed in [4, 13, 46]\ndemonstrate improved generalization by learning robust\ngeneral features (e.g. via image rotation consistency) for\ngaze estimation across varying conditions. Even if those\nmethods focus on constrained settings with face crop as\ninput, we compare our approach with them to show the\neffectiveness in frontal face generalization.\nFurthermore, to improve in-the-wild generalization, re-\nsearchers seek to exploit diverse weak gaze labels that can\nbe easily or automatically generated on in-the-wild data.\nIn this direction, Zang et al. [49] automatically generates\na new 3D gaze dataset, MPSGaze, by blending on images\nof people from the Widerface datasets [48], eyes from\nimages of the ETH-Xgaze dataset with known 3D gaze and\nsimilar head pose. While this greatly improves diversity\nwith more than 10000 new identities, this method generates\nonly near frontal faces and might impact the appearance of\nthe face. In another study, Ververas et al. [45] used eyeball\nfitting techniques to create pseudo-3D gaze on new face\ndatasets. They improved generalization, but their work is\nalso restricted to frontal faces. Finally, Kothari et al. [29]\nused a weakly-supervised learning framework for improved\ngeneralization using pseudo 3D gaze labels from 2D gaze\nLAEO labeled datasets.\nHowever, as acknowledged by\nthe authors, the 2D gaze distribution of LAEO is limited\nhorizontally. In our work, we follow this idea but leverage\na more diverse gaze distribution and natural scene 2D gaze\nlabel obtained from the annotation of where people look\nin the scene. Using a self-training learning approach with\ngenerated 3D pseudo labels via geometric projection, we\nshow improved within and cross-dataset generalization\non unconstrained Gaze360 and GFIE [24] and frontal\nMPIIFaceGaze [50] datasets.\nDynamic 3D Gaze Estimation has not been extensively\nexplored due to the lack of available datasets.\nEYE-\nDIAP [19] and EVE [39] are video datasets collected in\nconstrained settings, resulting in mostly frontal poses. In\nthis particular context, Park et al. [39] and Palmero et\nal. [38] estimated the gaze from face crop image sequences\nbut only showed marginal improvement compared to static\nmethods.\nIndeed, it is questionable if eyeball dynamics\nhave temporal dependencies besides the ones due to\nspecific tasks or scenarios (e.g.\nreading).\nNevertheless,\nin unconstrained settings with low resolution and head\npose dynamic scenarios, temporal methods show benefits\nin encoding the head and eye dynamics [8, 22, 27, 37].\nFor instance, seen from a far distance, head and body\norientation dynamic revealed to be an important prior\nfor gaze estimation when eyes are barely visible [37].\nUnconstrained video gaze data is challenging to collect.\nBeyond Gaze360, GFIE [24] is the only other 3D gaze\ndataset for gaze following, using complex laser setups, yet\nit is limited to indoor settings and lacks natural scene and\ngaze dynamics.\nThe scarcity of video 3D gaze datasets\nhampers the development of video-based methods that\ngeneralize to real-world data. To address this, we propose\na unified model trained on both image and video datasets,\ndemonstrating improved video prediction through diverse,\nlarge-scale data.\nFurthermore, current video-based gaze estimation methods\ntypically employ a backbone to extract features from\nimage sequences, followed by a Recurrent Neural Network\n(RNN) to capture temporal dynamics [8, 25, 27, 29, 38, 39].\n3\n\n\nEncoder\nSwin3D\nT>1\nImages\n3D Gaze\nVideos\n3D Gaze\nTm x 7 x 7 x dout\nTm x dout\nInterp.\nGaze \nMLP\nT x dout\nGaze Decoder\nMean\nSpatial\nPool\n2D Gaze\nData\n1. Supervised Training \nGaT\n2. Pseudo 3D Gaze Generation \nv\nT x 3\n3. Training with Pseudo Label \nGaze Decoder Module\nImages\n3D Gaze\nVideos\n3D Gaze\nPseudo 3D\nGaze Data\nGaT\nPretrain\nModel\nPseudo\nLabel\n3DGP\nGeometric Projection (GP)\n3DPred\n2D GT\nPseudo 3D\nGaze Data\nLinear\nProjection\n&\nLayerNorm\nPatchifier\nTm x 7 x 7 x dout\n  1 x 56 x 56 x din\n  Tm x 56 x 56 x din\nT=1\nGaze Transformer (GaT)\nFigure 2. Our ST-WSGE training framework. 1. In the first stage, we train a Gaze Transformer (GaT) on both image and video 3D gaze\ndatasets. 2. Using the trained network, 3D gaze is inferred on 2D gaze dataset. Then, a geometric rotation is applied to generate a pseudo\n3D gaze label from the inferred 3D gaze that is aligned to the 2D ground truth gaze label in the image plane. 3. In the second stage, we\ntrain a similar gaze network as in 1. using available 3D gaze datasets and gaze following datasets with 3D pseudo labels.\nHowever, these approaches do not explicitly model the\nspatiotemporal interactions in the input sequence.\nTo\naddress this limitation, we investigate a spatio-temporal\nmodel to encode subtle eye motion or head pose changes in\nthe input sequence directly.\n3. ST-WSGE Method\n3.1. Self-Training Pipeline\nWe propose a two-stage training pipeline for gaze estima-\ntion to leverage any 2D gaze datasets, as presented in Fig. 2.\nIn the first stage, a gaze network is trained on image and\nvideo 3D gaze datasets in a supervised manner. Next, the\nnetwork is used to infer 3D gaze on 2D gaze datasets.\nSince only the gaze’s depth is missing in the 2D gaze la-\nbel, we employ a geometric transformation to generate a\nrobust pseudo-3D gaze label from the inferred 3D gaze that\nis aligned with the 2D gaze label. We assume that a model\npre-trained on unconstrained 3D gaze datasets provides a\ngood prior z-estimation. In the second stage, a similar gaze\nnetwork is trained in a supervised regime using both gaze\nfollowing data with 3D pseudo labels and 3D gaze datasets.\n3.2. Gaze Transformer (GaT)\nModel Architecture Motivation. Accurate and robust 3D\ngaze estimation in the wild requires three key capabilities:\ncapturing fine local details from the eye region when vis-\nible; extracting global information from head orientation\nwhich is particularly useful when the eyes are occluded or\npartially obfuscated; capturing small motion of head pose\nand eyes in the temporal domain to capture subtle gaze\nshifts.\nA model capable of training on both image and\nvideo data is especially valuable, as it broadens the range\nof available training datasets. Convolutional Neural Net-\nworks (CNNs) excel in frontal gaze estimation due to their\nability to extract local eye features [1, 27, 47] but may en-\ncounter more difficulty in global reasoning (i.e. merging\npose and eye information), and extending CNNs to handle\ntemporal data within a single modality-agnostic model is\nchallenging. Vision Transformers (ViTs) [16], as noted by\nCheng et al. [10], are less suited for gaze estimation since\nthey may miss critical local details, especially when the eye\nregion is split across multiple patches. In contrast, hier-\narchical transformer architecture [32, 42] offers a flexible\narchitecture to capture both local and global features. For\ninstance, the Swin Transformer, which uses smaller patches\n(typically 4×4 vs 16×16 in standard ViTs), is better able to\ncapture fine local details. Its ”shifted window” mechanism,\nwhich applies self-attention within local windows that shift\nat regular intervals, effectively aggregates local and global\ncontext. Extending the Swin Transformer to the temporal\ndimension has proven successful for temporal tasks on sev-\neral benchmarks [33]. Additionally, transformers are versa-\n4\n\n\ntile, recent work has demonstrated their effectiveness when\ntrained on both image and video datasets within a single\nmodel [20, 21]. Inspired by these approaches, we introduce\nour Gaze Transformer, GaT, with several modifications for\n3D gaze estimation, as illustrated in Fig. 2 and detailed in\nthe following sections.\nPatchifier. The model needs a common representation for-\nmat to encode both image and video input.\nFollowing\n[16, 17, 20, 32, 44], images and videos are represented\nas 4D tensors X ∈RT ×H×W ×3, with T\n= 1 for an\nimage I, and T > 1 for a video clip V.\nThen, the in-\nput X is divided into a collection {xi}N of 4D sub-tensor\n(patches) xi ∈Rt×h×w×3, as presented in Fig. 2. Follow-\ning [20, 21, 35, 44], we use t = 2. When working with\nimage only, we duplicate the image instead of zero-padding\nbecause we find better cross modalities generalization from\nvideo to image. Then, a shared linear layer and LayerNorm\nare applied to project the patches to a token representation.\nEncoder. The tokens from the patchifier are then fed into\na tiny Swin3D hierarchical spatiotemporal encoder. It re-\nlies on self-attention within nearby tokens in a spatiotem-\nporal window that is shifted every time. In addition, it uses\ntwo sets of relative positional encoding: one spatial and one\ntemporal. Because of the hierarchical representation, the\nnumber of tokens is reduced by patch merging layers as the\nnetwork gets deeper. The temporal output dimension is re-\nduced by a factor of two. The output tokens are then fed to\na gaze decoder module.\nGaze Decoder. We first apply a mean spatial pooling on the\noutput tokens, followed by an interpolation function to dou-\nble the temporal dimension to match the input length (for\nimages, interpolation is skipped). Finally, a shared MLP\nwith a single hidden layer is applied to each token to pre-\ndict a normalized 3D gaze vector.\nBaseline Networks. Different approaches exist to process\nimage and video in a single model. To compare the per-\nformance of our GaT model, we develop in addition two\nbaselines. Given that the static Swin(2D) transformer gives\ngood performance on gaze estimation. We add a temporal\nencoder to model the gaze dynamic. Therefore, we develop\nSwin(2D)-LSTM which first processes a set of images using\nSwin and outputs a set of embedding for each image. Then,\nit is fed to a bidirectional LSTM followed by a shared gaze\nMLP on each output to produce a gaze vector. Similarly, the\nsecond baseline called Swin(2D)-Tr replaces the LSTM by\na transformer. The output tokens from each image are pro-\njected to a lower dimension followed by a LayerNorm and\nabsolute spatiotemporal encoding. Then, the transformer is\napplied to the spatiotemporal output token. Finally, a spatial\nmean pooling is applied followed by a similar gaze MLP.\nBoth architectures are input agnostic and are compared in\nan ablation study.\n3.3. Pseudo 3D Gaze Generation\nCreating 2D gaze datasets is easier than creating 3D gaze\ndatasets, as annotation can be completed after the images\nhave been collected, unlike 3D gaze which can not be an-\nnotated by humans and require a special setup. As a result,\n2D datasets like GazeFollow offer a broad gaze, head pose\ndistribution, and head/face appearance diversity. There are\ndifferent possibilities to leverage such a dataset which will\nbe discussed in the ablation sections. For instance, Kothari\net al. [29] used 2D gaze from LAEO labels and 3D fitted\nhead models for z-direction estimation. In contrast, as pre-\nsented in Fig. 2, our 3D pseudo gaze generation method as-\nsumes that a pre-trained model trained on unconstrained 3D\ngaze datasets can provide a good prior z-estimation, which\nis confirmed by our experiments. Combining the z com-\nponent of predicted 3D gaze with 2D gaze ground truth\nprovides a robust pseudo 3D gaze label. Then, using this\npseudo label as an additional label during training, we re-\nport improvement in unconstrained generalization.\nGeometric Projection. The predicted 3D Gaze (3DPred)\nˆg and the 2D ground truth (2D GT) v = (vx, vy) are com-\nbined such that the image plane projection of the pseudo\n3D gaze (3DGP) gps is aligned with the 2D ground truth v.\nTherefore, a rotation is applied to ˆg around the z-axis such\nthat gps has the same x,y direction as v:\ngps = (vx∥(ˆgx, ˆgy)∥2, vy∥(ˆgx, ˆgy)∥2, ˆgz)\n(1)\n3.4. Training Strategy\nIn both training stages, the objective is to train our GaT\nmodel on a collection of both image and video datasets with\ngaze label {(X, g)j} where g ∈RT ×3 with T=1 for images,\nwhich creates different training challenges to be addressed.\nVideo Training Data. As our model is modality agnostic, a\nvideo dataset can be considered both as a set of video clips\nor as a collection of images. These views of the data are not\nequivalent, as, typically, considering the data as video-clip\nat training will impact more inference on videos at test time\nrather than on images. Hence, a video dataset can be used\ntwice as an image or video training dataset1. We will see\nin ablations that it can impact the modality generalization\ncapability.\nMini-batch Strategy. Different mini-batch strategies have\nbeen proposed in the literature to handle multiple datasets.\nOne approach mixes samples from each of the datasets, but\nin our case, this requires careful implementation because\nimages and videos don’t have the same dimensions.\nAnother strategy creates batches from one dataset at a\ntime and alternates between them.\nThis approach has\nproven effective in previous work [20, 23] and we followed\n1By convention, when reporting experiments, for video datasets, we\nwill add the suffix I when it is considered as an image dataset, V in the\nvideo case, and I&V when it is used twice as image and as video dataset.\n5\n\n\nGF Additional Label\nG360 Full\nGFIE\nMPII\nEDIAP\nMethod\nTraining Data\nNone\n2D\n3DPred\n3DGP\nImg\nVid\nImg\nVid\nImg\nVid\nSupervised\nG360I&V\n✓\n13.6\n12.6\n21.9\n20.9\n7.4\n8.3\nWeakly-Sup (WS)\nG360I&V+GF\n✓\n13.1\n12.1\n16.1\n15.7\n6.5\n9.2\nSelf-Training (ST)\nG360I&V+GF\n✓\n13.6\n12.7\n20.2\n19.7\n7.4\n7.7\nST-WSGE\nG360I&V+GF\n✓\n13.2\n12.2\n15.9\n15.5\n6.4\n8.2\nSupervised\nGFIEI&V\n✓\n30.6\n29.9\n15.7\n15.4\n23.8\n37.8\nWeakly-Sup (WS)\nGFIEI&V+GF\n✓\n22.9\n22.1\n12.5\n12.2\n24.4\n33.0\nSelf-Training (ST)\nGFIEI&V+GF\n✓\n29.7\n29.4\n14.9\n14.9\n21.2\n34.6\nST-WSGE\nGFIEI&V+GF\n✓\n21.5\n21.1\n13.0\n12.7\n17.3\n16.7\nTable 1. Ablation study for the self-training weakly-supervised\nlearning framework. We experiment with our GaT model, two\n3D gaze datasets Gaze360 and GFIE, and three ways to include\nGazeFollowing labels (GF). The best and the second best scores\nare in bold and underlined, respectively.\nTraining modality\nG360 Full\nG360 180\nG360 40\nModel\nImg\nVid\nImg\nVid\nImg\nVid\nImg\nVid\nSwin(2D)-LSTM\n✓\n14.33\n13.97\n12.17\n11.89\n9.73\n9.47\nSwin(2D)-LSTM\n✓\n14.75\n13.05\n12.58\n10.98\n9.86\n8.67\nSwin(2D)-LSTM\n✓\n✓\n13.93\n13.02\n11.76\n10.94\n8.88\n8.27\nSwin(2D)-Tr\n✓\n14.05\n14.53\n12.02\n12.78\n9.63\n9.32\nSwin(2D)-Tr\n✓\n14.05\n12.63\n12.05\n10.54\n9.46\n8.14\nSwin(2D)-Tr\n✓\n✓\n13.81\n12.91\n11.96\n11.09\n9.43\n8.67\nGaT\n✓\n13.95\n13.82\n11.95\n11.78\n9.58\n8.89\nGaT\n✓\n13.87\n12.31\n11.89\n10.39\n9.29\n7.95\nGaT\n✓\n✓\n13.64\n12.60\n11.66\n10.67\n9.10\n8.23\nTable 2. Ablation study for the gaze model network. Since\ndifferent models are image and video training agnostic, we exper-\niment with three models on three training modalities dataset com-\nbinations using Gaze360 as the training set.\nG360 Full\nG360 180\nG360 40\nGFIE\nTraining Data\nImg\nVid\nImg\nVid\nImg\nVid\nImg\nVid\nG360V+GF\n13.5\n12.1\n11.6\n10.2\n8.3\n7.7\n15.7\n17.9\nG360I&V+GF\n13.2\n12.2\n11.3\n10.3\n8.6\n7.7\n15.9\n15.5\nGFIEV+GF\n22.8\n24.2\n22.4\n23.9\n29.9\n31.8\n13.4\n13.0\nGFIEI&V+GF\n21.5\n21.1\n20.6\n20.3\n26.6\n26.7\n13.0\n12.7\nTable 3. Impact of the training datasets modalities on cross-\nmodal generalization.\nWe experiment with GaT model, ST-\nWSGE framework, and different training dataset modalities.\nthis approach here.\nIn addition, dataset size imbalance\nis another challenge, as dataset sizes range from 30k to\n120k samples. To address this, we balance the datasets by\noversampling smaller ones and undersampling larger ones\nso that each dataset contributes equally during an epoch.\nLoss.\nFor training the model, we utilize a temporal\nweighted average of the angular loss, which represents the\nangular difference between the predicted gaze vector ˆg and\nthe ground truth g in degree:\nLgaze(ˆg, g) = 1\nT\nT\nX\nt=1\n180\nπ arccos(\nˆgT\nt gt\n∥ˆgt∥∥gt∥)\n(2)\n4. Experiments\n4.1. Datasets\nIn this work, we employ five 3D gaze datasets: two video\nunconstrained datasets for training and evaluation: Gaze360\n(G360) [27], GFIE [24], and three constrained only\nfor generalization MPSGaze (MPS) [49], MPIIFaceGaze\n(MPII) [50] and EYEDIAP (EDIAP) [19] (EDIAP), with\nonly EDIAP being a video dataset. As shown in Fig. 3,\nFigure 3. Dataset gaze distribution. Gaze in polar coordinates.\nG360 and GFIE differ considerably in their gaze distribu-\ntion, which makes cross-dataset evaluations challenging. In\naddition, we consider the 2D gaze following dataset Gaze-\nFollow [40] (GF), which contains more than 100k images\nwith gaze target annotations.\nThe details of the six datasets are presented in the supple-\nmentary materials. Nevertheless, as authors have been us-\ning many subsets of G360 for evaluation, we clarify the\ntest splits to avoid any confusion. We followed the split of\n[27]: G360 Full corresponds to ”All 360°” (all the test set);\nG360 180 corresponds to ”Front 180°” (gaze within 90°);\nand G360 40 to ”Front Facing” (gaze within 20°). Addition-\nally, we consider G360 Back (gaze above 90°) [8] and G360\nFace (all detected faces), used in many studies [1, 7, 9–\n11, 18, 47, 51]. G360 Face 180 or 40 corresponds to the\ndetected face with a gaze within 90° or 20°.\n4.2. Implementation Details\nEach dataset has different head bounding boxes ground\ntruth. To avoid discrepancies in cropping, we standardize\nthe input by using a robust pre-trained head detector2 train\non the CrowdHuman dataset [43]. We match the detected\nand ground truth bounding boxes to get the final head crop\nbounding box. Furthermore, we downscale the head bound-\ning box by 10% and resize it to 224 × 224 pixels. We show\nthe impact of the head bounding box size in the supplemen-\ntary materials. An 8-frame head crop clip is used for the\nvideo modalities, and the frame rate is unified across all\nvideo datasets. All the backbones used in this work are pre-\ntrained on Imagnet for static backbones and ImageNet-1K,\nKinetics-400, and SUN RGB-D for Swin3D. Please refer to\nthe supplementary materials for training and data augmen-\ntation details.\n4.3. Ablation Study\nDoes gaze following label improve 3D gaze estimation?\nIn Tab. 1, we evaluate various methods for learning from 2D\ngaze following labels (GF). We find that, with few excep-\ntions, incorporating GF consistently improves 3D gaze es-\ntimation. This underscores the importance of broader train-\ning diversity for robust 3D gaze estimation. The specific\ndetails and advantages of each approach are discussed in\nthe following section.\nSelf-Training Weakly-Supervised learning framework.\nIn Tab. 1, we perform ablation experiments related to the\n2https://github.com/zhangda1018/yolov5-crowdhuman\n6\n\n\nlearning framework.\nIn our experiments, we train with\nour model GaT on two 3D gaze datasets namely G360 and\nGFIE. The first baseline experiment is to train on a 3D gaze\ndataset in a standard supervised manner. Then, there are\nthree possibilities to leverage additional 2D datasets such\nas GF. The first one is the Weakly-Supervised (WS) method\ndefined by a specific loss applied only on the x, y 3D gaze\nprediction coordinate for GF batch samples supervised by\nthe 2D GT label. The second approach Self-Training (ST)\nis similar to our ST-WSGE approach described in Fig. 2 but\nin the second stage, 3DPred is used to supervise the train-\ning. Finally, the last approach ST-WSGE is our proposed\napproach described in Fig. 2 and Sec. 3.1. Compared to\nST, ST-WSGE achieves higher accuracy across all evalua-\ntions except when trained on G360 and tested on EDIAP.\nThis indicates that relying solely on 3DPred lacks diversity\nin gaze distribution, as it mostly follows the training data\ndistribution. By incorporating our 3DGP label, we obtain\na more robust gaze vector that enhances accuracy. Further-\nmore, when compared to using only 2D labels in the WS\nmethod, ST-WSGE performs slightly better overall, partic-\nularly on the frontal EDIAP and MPII benchmarks. This\nsuggests that datasets, where the z component is signifi-\ncant (e.g., frontal views), require more than 2D supervision.\nOverall, our method is either the best or the second best (by\na small difference) demonstrating the effectiveness of our\nST-WSGE learning framework.\nGaze Model Network. In Tab. 2, we present the results\nconcerning the model architecture. In our experiments, we\ntrain on G360 in a supervised manner with different train-\ning modalities combinations ( I, V, and I&V). We compare\nour model GaT with baseline models (see Sec. 3.2) that can\nalso handle different training data modalities. First, we no-\ntice that when models are trained on both image and video,\nour GaT model is the best model on G360.\nIt suggests\nthat spatiotemporal learning from the input is beneficial for\ngaze estimation, especially in non-frontal scenarios. Addi-\ntionally, within each model, training on both modalities im-\nproves image evaluation but slightly reduces video evalua-\ntion. However, modality-specific models are limited to their\nown data type which limits the available training dataset.\nIndeed, in the next section, we show that combining modal-\nities can result in cross-modal generalization. Given these\nfindings, our model stands out as a reliable and versatile op-\ntion for robust 3D gaze estimation.\nDoes training on additional image datasets help video\ngeneralization? As observed in Tab. 1, training our GaT\nmodel with ST-WSGE, which includes a diverse image\ndataset (GF), not only improves image generalization but,\nmore notably, enhances generalization on videos.\nCom-\npared to a supervised method, our approach improves image\nGFIE evaluation by 38% and video GFIE by 34%. A simi-\nlar trend is observed when training on GFIE and evaluating\non G360 Full. Furthermore, the modality of the training\ndata plays an important role, as observed in Tab. 3. Indeed,\nwhen training with both an image and a video dataset such\nas G360V+GF, at test time, evaluation on images (respec-\ntively videos) will be dominantly affected by the character-\nistics of the image (respectively video) training data. When\nwe train on G3360V+GF, the results on GFIE image have\na 15.7° angular error against 17.9° on video. Interestingly,\nwhen trained on G360I&V+GF, the generalization modality\ngap is reduced with 15.9° on image and 15.5° on video. A\nSimilar trend is observed when training on GFIE.\nTemporal Context. Temporal dynamic plays a crucial role\nin unconstrained gaze estimation, as evidenced in Tab. 1\nwith our GaT model trained on both modalities (I&V). In\nall configurations, video predictions consistently outper-\nform image-based predictions. Other important observa-\ntions emerged from visual and quantitative analyses and are\ndiscussed in the supplementary material.\n4.4. Comparison with State-of-the-art (SOTA).\nWithin-Dataset Experiments.\nIn the following, we fo-\ncus on within-dataset experiments. In Tab. 4, we compare\nour results with the state-of-the-art methods on G360 and\nGFIE. We report results using our model GaT in a super-\nvised manner and using our ST-WSGE learning framework.\nOur model trained in a supervised manner is SOTA on im-\nage G360 Full and GFIE with 2% and 13% relative im-\nprovement, respectively. On video inference, MSA+Seq is\nslightly better (12.6° vs 12.5° ours) since it uses an average\nof multiple input scales. More importantly, when trained\nwith gaze following labels like GF using our ST-WSGE\nlearning framework, we outperform all the SOTA on im-\nage and video by 5% on G360 Full image, 3% on G360\nFull video, and 36% on GFIE image. In contrast, Kothari et\nal. [29] don’t improve when using the LAEO label (AVA)\nin a weakly-supervised framework. Additionally, in Tab. 5,\nwe compared our method trained on G360 Full to meth-\nods trained on detected face subset G360 Face. Given that\nthe state-of-the-art methods are specifically designed for\nnear-frontal faces, our supervised model GaT is not SOTA\nbut demonstrates competitive performance. When includ-\ning gaze following label using our ST-WSGE framework, it\nshows very competitive results and SOTA performance on\nG360 Face video (9.92° vs 10.05°), G360 Face 180 video\n(9.84° vs 9.75°), and G360 Face 40 (8.62° vs 8.30°). There-\nfore, compared to methods using tight face crops (increas-\ning eye resolution), our ST-WSGE approach proved to be\ncompetitive on near frontal view. We include a comparison\nwith SOTA trained on G360 and evaluated on constrained\nbenchmarks MPII and EDIAP in supplementary materials.\nCross-Dataset Experiments.\nIn this section, we empha-\nsize on cross dataset experiments. In Tab. 4, we compare\nour method with SOTA methods on generalization on G360\n7\n\n\nG360 Full\nG360 180\nG360 40\nG360 Face\nG360 Back\nGFIE\nMethod\nTraining Data\nImg\nVid\nImg\nVid\nImg\nVid\nImg\nVid\nImg\nVid\nImg\nVid\nGaze360 [27]\nG360I\n15.6\n-\n13.4\n-\n13.2\n-\n-\n-\n-\n-\n-\n-\nKothari et al. [29]\nG360I\n15.07\n-\n-\n-\n10.94\n-\n-\n-\n-\n-\n-\n-\nMSA [8]\nG360I\n13.9\n-\n12.2\n-\n-\n-\n-\n-\n23.5\n-\n-\n-\nGaze360 [27]\nG360V\n-\n13.5\n-\n11.4\n-\n11.1\n-\n-\n-\n-\n-\n-\nKothari et al. [29]\nG360V\n-\n13.2\n-\n-\n-\n10.1\n-\n-\n-\n-\n-\n-\nKothari et al. [29]\nG360V+AVA\n-\n13.2\n-\n-\n-\n10.2\n-\n-\n-\n-\n-\n-\nMCGaze [22]\nG360V\n-\n12.96\n-\n10.74\n-\n10.02\n-\n-\n-\n-\n-\n-\nMSA+Seq [8]\nG360V\n-\n12.5\n-\n10.7\n-\n-\n-\n-\n-\n19.0\n-\n-\nSupervised (GaT)\nG360I&V\n13.64\n12.60\n11.66\n10.67\n9.10\n8.23\n11.20\n10.25\n20.74\n19.53\n21.86\n20.89\nST-WSGE (GaT)\nG360I&V+GF\n13.19\n12.17\n11.34\n10.35\n8.58\n7.67\n10.84\n9.92\n19.82\n18.72\n15.90\n15.51\nGFIE [24]\nGFIEI\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n17.7\n-\nKothari et al. [29]\nETH\n52.6\n-\n-\n-\n20.5\n-\n-\n-\n-\n-\n-\n-\nKothari et al. [29]\nETH+AVA\n-\n25.0\n-\n-\n-\n16.9\n-\n-\n-\n-\n-\n-\n3DGazeNet [45]†\nETH\n-\n-\n-\n-\n-\n-\n22.1†\n-\n-\n-\n-\n-\n3DGazeNet [45]†\nETH+AVA+CMU\n-\n-\n-\n-\n-\n-\n17†\n-\n-\n-\n-\n-\n3DGazeNet [45]†\nETH+ITWG-MV\n-\n-\n-\n-\n-\n-\n15.4†\n-\n-\n-\n-\n-\nSupervised (GaT)\nGFIEI&V\n30.57\n29.90\n29.08\n28.65\n33.43\n32.94\n28.87\n28.56\n35.95\n34.40\n15.70\n15.44\nST-WSGE (GaT)\nGFIEI&V+GF\n21.48\n21.06\n20.61\n20.32\n26.55\n26.73\n20.46\n20.23\n24.61\n23.75\n12.99\n12.68\nSupervised (GaT)\nGFIEI&V+MPS\n25.75\n24.29\n20.35\n19.42\n16.35\n15.49\n19.07\n18.28\n45.19\n41.81\n15.61\n15.38\nST-WSGE (GaT)\nGFIEI&V+MPS+GF\n21.59\n20.02\n17.02\n15.67\n13.90\n12.66\n16.00\n14.75\n38.02\n35.69\n12.82\n12.49\nTable 4. Comparison with the state-of-the-art on physically unconstrained benchmark Gaze360 and GFIE test set. We report both\nwithin and cross-dataset evaluation trained using GaT model with and without our ST-WSGE framework. The top table presents methods\ntrained with Gaze360, while at the bottom, methods are trained using GFIE. The method with † is restricted with only frontal pose with\nface and eye crop as input. Moreover, the method is evaluated on a new subset (head pose yaw ∈[-90,90]), which is close to, but not the\nsame as, the Gaze360 Face subset.\nG360 Face\nG360 Face 180\nG360 Face 40\nMethod\nTraining Data\nImg\nVid\nImg\nVid\nImg\nVid\nFullFace [51]\nG360I Face\n14.99\n-\n-\n-\n-\n-\nDilated [9]\nG360I Face\n13.73\n-\n-\n-\n-\n-\nRT-Gene [18]\nG360I Face\n12.26\n-\n-\n-\n-\n-\nCA-Net [11]\nG360I Face\n11.20\n-\n-\n-\n-\n-\nGaze360 [12]\nG360V Face\n-\n11.04\n-\n-\n-\n-\nResNet50 [10]\nG360I Face\n10.73\n-\n-\n-\n-\n-\nGazeTR [10]\nG360I Face\n10.62\n-\n-\n-\n-\n-\nL2CS [1]\nG360I Face\n-\n-\n10.4\n-\n9.0\n-\nSPMCCA [47]\nG360I Face\n-\n-\n10.16\n-\n8.62\n-\nSAM-LSTM [25]\nG360V Face\n-\n10.05\n-\n9.84\n-\n6.92\nSupervised (GaT)\nG360I&V\n11.20\n10.25\n11.01\n10.09\n8.81\n8.02\nST-WSGE (GaT)\nG360I&V+GF\n10.84\n9.92\n10.65\n9.75\n8.30\n7.48\nTable 5.\nComparison with the state-of-the-art constrained\nmethods tested on Gaze360 detected face. All the state-of-the-\nart methods use a face crop as input and are trained on the de-\ntected face subset of Gaze360. We report results trained on G360\nFull using GaT and with or without ST-WSGE. Note that the other\nmethods are constrained to face input therefore our method is more\ngeneral and can be applied to any head pose orientation.\n(bottom part). Among the few approaches that explore gen-\neralization on Gaze360, Kothari et al. [29] provides the\nmost relevant comparison.\nIn contrast, 3DGazeNet [45]\nprovides cross-dataset generalization on G360 Face but\nonly works on frontal faces requiring face and eye crop\nas input. Our results show that our ST-WSGE framework\ntrained with various available 3D gaze datasets (GFIE, or\nGFIE+MPS) always improves generalization. For instance,\nwhen tested on G360 Full image and video, it always out-\nperforms our supervised approach by 40% and 20% when\ntrained with GFIE or GFIE+MPS, respectively. A similar\ntrend is observed when trained on G360 and tested on GFIE.\nTherefore, it confirms that our framework using gaze follow\nlabels is effective for improved generalization.\nWhen compared to Kothari et al. [29] trained on LAEO\nlabels (AVA), our ST-WSGE approach trained using\nGFIE+GF shows better performance on G360 Full but is\nbehind on G360 40 because GFIE doesn’t contain frontal\nsamples. In contrast, when trained using GFIE+MPS+GF,\nit improves over Kothari on both G360 Full and 40.\nLimitations.\nAs expected when using our framework,\nthe generalization improvement is tight to the training di-\nversity used in the pre-training stage. In cross-dataset ex-\nperiments in Tab. 4, compared to our supervised model, we\nobserve that when trained using GFIE our framework im-\nproves more on G360 Back and less on G360 40 because of\nthe non-frontal distribution of GFIE.\n5. Conclusion\nIn this work, we introduced the ST-WSGE learning frame-\nwork, which leverages weakly annotated images with 2D\ngaze datasets, such as gaze-follow labels, to enhance ap-\npearance diversity and broaden gaze distributions across\nnatural scenes. We also presented our Gaze Transformer,\nGaT, which improves performance and supports both image\nand video training. By combining ST-WSGE and GaT, we\nachieve significant gains in both within- and cross-dataset\nexperiments, reaching state-of-the-art results on GFIE and\nGaze360.\nAdditionally, we demonstrate effective cross-\nmodal generalization, a critical capability given the scarcity\nof video datasets. We believe our approach is a promising\nsolution for robust 3D gaze tracking in the wild, suitable for\na range of challenging applications.\n8\n\n\nReferences\n[1] Ahmed A Abdelrahman, Thorsten Hempel, Aly Khalifa, Ay-\noub Al-Hamadi, and Laslo Dinges. L2cs-net: Fine-grained\ngaze estimation in unconstrained environments. In 2023 8th\nInternational Conference on Frontiers of Signal Processing\n(ICFSP), pages 98–102. IEEE, 2023. 2, 3, 4, 6, 8, 12\n[2] Henny Admoni and Brian Scassellati.\nSocial eye gaze in\nhuman-robot interaction: a review. Journal of Human-Robot\nInteraction, 6(1):25–63, 2017. 1\n[3] Sean Andrist, Xiang Zhi Tan, Michael Gleicher, and Bilge\nMutlu. Conversational gaze aversion for humanlike robots.\nIn Proceedings of the 2014 ACM/IEEE international confer-\nence on Human-robot interaction, pages 25–32, 2014. 1\n[4] Yiwei Bao, Yunfei Liu, Haofei Wang, and Feng Lu. Gen-\neralizing gaze estimation with rotation consistency. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4207–4216, 2022. 3, 13\n[5] Alisa Burova, John M¨akel¨a, Jaakko Hakulinen, Tuuli Keski-\nnen, Hanna Heinonen, Sanni Siltanen, and Markku Turunen.\nUtilizing vr and gaze tracking to develop ar solutions for in-\ndustrial maintenance. In Proceedings of the 2020 CHI con-\nference on human factors in computing systems, pages 1–13,\n2020. 1\n[6] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299–6308, 2017. 2\n[7] Andy\nCatruna,\nAdrian\nCosma,\nand\nEmilian\nRadoi.\nCrossgaze: A strong method for 3d gaze estimation in the\nwild. In International Conference on Automatic Face and\nGesture Recognition (FG), 2024. 2, 6, 12\n[8] Chu-Song Chen, Hsuan-Tien Lin, et al. 360-degree gaze es-\ntimation in the wild using multiple zoom scales. In British\nMachine Vision Conference (BMVC), 2021. 3, 6, 8, 12, 13\n[9] Zhaokang Chen and Bertram E Shi. Appearance-based gaze\nestimation using dilated-convolutions. In Asian Conference\non Computer Vision, pages 309–324. Springer, 2018. 2, 3, 6,\n8, 12\n[10] Yihua Cheng and Feng Lu.\nGaze estimation using trans-\nformer. In 2022 26th International Conference on Pattern\nRecognition (ICPR), pages 3341–3347. IEEE, 2022. 4, 8\n[11] Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and\nFeng Lu. A coarse-to-fine adaptive network for appearance-\nbased gaze estimation. In Proceedings of the AAAI confer-\nence on artificial intelligence, pages 10623–10630, 2020. 2,\n3, 6, 8, 12\n[12] Yihua Cheng, Haofei Wang, Yiwei Bao, and Feng Lu.\nAppearance-based gaze estimation with deep learning: A\nreview and benchmark. arXiv preprint arXiv:2104.12668,\n2021. 8, 12, 13\n[13] Yihua Cheng, Yiwei Bao, and Feng Lu. Puregaze: Purifying\ngaze feature for generalizable gaze estimation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, pages\n436–443, 2022. 3, 13\n[14] Eunji Chong, Nataniel Ruiz, Yongxin Wang, Yun Zhang,\nAgata Rozga, and James M. Rehg. Connecting gaze, scene,\nand attention:\nGeneralized attention estimation via joint\nmodeling of gaze and scene saliency. In The European Con-\nference on Computer Vision (ECCV), 2018. 2\n[15] Eunji Chong, Yongxin Wang, Nataniel Ruiz, and James M.\nRehg. Detecting attended visual targets in video. In The\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2020. 15, 17, 18\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 4, 5\n[17] Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al.\nMasked autoencoders as spatiotemporal learners. Advances\nin neural information processing systems, 35:35946–35958,\n2022. 5\n[18] Tobias Fischer, Hyung Jin Chang, and Yiannis Demiris. Rt-\ngene: Real-time eye gaze estimation in natural environments.\nIn Proceedings of the European conference on computer vi-\nsion (ECCV), pages 334–352, 2018. 2, 3, 6, 8, 12\n[19] Kenneth Alberto Funes Mora, Florent Monay, and Jean-\nMarc Odobez. Eyediap: A database for the development and\nevaluation of gaze estimation algorithms from rgb and rgb-d\ncameras. In Proceedings of the symposium on eye tracking\nresearch and applications, pages 255–258, 2014. 2, 3, 6, 12,\n13\n[20] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens Van\nDer Maaten, Armand Joulin, and Ishan Misra. Omnivore:\nA single model for many visual modalities. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 16102–16112, 2022. 2, 5\n[21] Rohit\nGirdhar,\nAlaaeldin\nEl-Nouby,\nMannat\nSingh,\nKalyan Vasudev Alwala, Armand Joulin, and Ishan Misra.\nOmnimae: Single model masked pretraining on images and\nvideos. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 10406–10417,\n2023. 2, 5\n[22] Yiran Guan, Zhuoguang Chen, Wenzheng Zeng, Zhiguo\nCao, and Yang Xiao. End-to-end video gaze estimation via\ncapturing head-face-eye spatial-temporal interaction context.\nIEEE Signal Processing Letters, 30:1687–1691, 2023. 3, 8\n[23] Ronghang Hu and Amanpreet Singh. Unit: Multimodal mul-\ntitask learning with a unified transformer. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 1439–1449, 2021. 5\n[24] Zhengxi Hu, Yuxue Yang, Xiaolin Zhai, Dingye Yang, Bo-\nhan Zhou, and Jingtai Liu. Gfie: A dataset and baseline for\ngaze-following from 2d to 3d in indoor environments. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 8907–8916, 2023. 2, 3,\n6, 8, 12\n[25] Swati Jindal, Mohit Yadav, and Roberto Manduchi. Spatio-\ntemporal attention and gaussian processes for personalized\nvideo gaze estimation.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 604–614, 2024. 3, 8\n9\n\n\n[26] Isaac Kasahara, Simon Stent, and Hyun Soo Park.\nLook\nboth ways: Self-supervising driver gaze estimation and road\nscene saliency. In European Conference on Computer Vision,\npages 126–142. Springer, 2022. 1\n[27] Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-\ntusik, and Antonio Torralba. Gaze360: Physically uncon-\nstrained gaze estimation in the wild.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 6912–6921, 2019. 2, 3, 4, 6, 8, 12, 13\n[28] Chris L Kleinke. Gaze and eye contact: a research review.\nPsychological bulletin, 100(1):78, 1986. 1\n[29] Rakshit Kothari, Shalini De Mello, Umar Iqbal, Wonmin\nByeon, Seonwook Park, and Jan Kautz. Weakly-supervised\nphysically unconstrained gaze estimation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 9980–9989, 2021. 2, 3, 5, 7, 8\n[30] Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kan-\nnan, Suchendra Bhandarkar, Wojciech Matusik, and Anto-\nnio Torralba. Eye tracking for everyone. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 2176–2184, 2016. 2\n[31] Yunfei Liu, Ruicong Liu, Haofei Wang, and Feng Lu. Gen-\neralizing gaze estimation with outlier-guided collaborative\nadaptation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 3835–3844, 2021. 3,\n13\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012–10022, 2021. 4, 5\n[33] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 3202–3211, 2022. 4\n[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In In International Conference on Learning\nRepresentations, 2018. 13\n[35] Manuel J Marin-Jimenez, Vicky Kalogeiton, Pablo Medina-\nSuarez, and Andrew Zisserman. Laeo-net: revisiting peo-\nple looking at each other in videos.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3477–3485, 2019. 2, 5\n[36] Michael McCloskey and Neal J Cohen. Catastrophic inter-\nference in connectionist networks: The sequential learning\nproblem. In Psychology of learning and motivation, pages\n109–165. Elsevier, 1989. 2\n[37] Soma Nonaka, Shohei Nobuhara, and Ko Nishino. Dynamic\n3d gaze from afar: Deep gaze estimation from temporal eye-\nhead-body coordination. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2192–2201, 2022. 2, 3\n[38] Cristina Palmero, Javier Selva, Mohammad Ali Bagheri, and\nSergio Escalera. Recurrent cnn for 3d gaze estimation using\nappearance and shape cues.\nIn In British Machine Vision\nConference (BMVC), 2018. 3\n[39] Seonwook Park, Emre Aksan, Xucong Zhang, and Otmar\nHilliges. Towards end-to-end video-based eye-tracking. In\nComputer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part XII\n16, pages 747–763. Springer, 2020. 2, 3\n[40] Adria Recasens, Aditya Khosla, Carl Vondrick, and Anto-\nnio Torralba. Where are they looking? Advances in neural\ninformation processing systems, 28, 2015. 2, 6, 12\n[41] Alexander Richard, Colin Lea, Shugao Ma, Jurgen Gall, Fer-\nnando De la Torre, and Yaser Sheikh. Audio-and gaze-driven\nfacial animation of codec avatars.\nIn Proceedings of the\nIEEE/CVF winter conference on applications of computer\nvision, pages 41–50, 2021. 1\n[42] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei,\nHaoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu\nChowdhury, Omid Poursaeed, Judy Hoffman, et al.\nHi-\nera: A hierarchical vision transformer without the bells-and-\nwhistles. In International Conference on Machine Learning,\npages 29441–29454. PMLR, 2023. 4\n[43] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu,\nXiangyu Zhang, and Jian Sun.\nCrowdhuman: A bench-\nmark for detecting human in a crowd.\narXiv preprint\narXiv:1805.00123, 2018. 6\n[44] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learners\nfor self-supervised video pre-training. Advances in neural\ninformation processing systems, 35:10078–10093, 2022. 5\n[45] Evangelos Ververas, Polydefkis Gkagkos, Jiankang Deng,\nMichail Christos Doukas, Jia Guo, and Stefanos Zafeiriou.\n3dgazenet:\nGeneralizing gaze estimation with weak-\nsupervision from synthetic views.\nIn ECCV, 2024.\n3, 8,\n13\n[46] Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wen-\nrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Con-\ntrastive regression for domain adaptation on gaze estimation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 19376–19385, 2022. 3,\n12, 13\n[47] Chao Yan, Weiguo Pan, Cheng Xu, Songyin Dai, and\nXuewei Li.\nGaze estimation via strip pooling and multi-\ncriss-cross attention networks.\nApplied Sciences, 13(10):\n5901, 2023. 2, 3, 4, 6, 8, 12\n[48] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang.\nWider face: A face detection benchmark. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 5525–5533, 2016. 3, 12\n[49] Mingfang Zhang, Yunfei Liu, and Feng Lu.\nGazeonce:\nReal-time multi-person gaze estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4197–4206, 2022. 3, 6, 12\n[50] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas\nBulling. Appearance-based gaze estimation in the wild. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 4511–4520, 2015. 2, 3, 6, 12, 13\n[51] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas\nBulling. It’s written all over your face: Full-face appearance-\nbased gaze estimation. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition workshops,\npages 51–60, 2017. 2, 3, 6, 8, 12\n10\n\n\n[52] Xucong Zhang, Yusuke Sugano, and Andreas Bulling. Re-\nvisiting data normalization for appearance-based gaze esti-\nmation. In Proceedings of the 2018 ACM symposium on eye\ntracking research & applications, pages 1–9, 2018. 2\n[53] Xucong Zhang, Seonwook Park, Thabo Beeler, Derek\nBradley, Siyu Tang, and Otmar Hilliges. Eth-xgaze: A large\nscale dataset for gaze estimation under extreme head pose\nand gaze variation. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part V 16, pages 365–381. Springer, 2020. 2,\n12, 13\n11\n\n\nEnhancing 3D Gaze Estimation in the Wild using\nWeak Supervision with Gaze Following Labels\nSupplementary Material\nWhat is expected? The supplementary material consists of\ndatasets details, experiments details, and extended experi-\nments analysis mentioned in the main paper. In addition,\nvideos of qualitative examples of our method on VideoAt-\ntentionTarget further demonstrate the robustness in chal-\nlenging real-world scenarios.\nA. Datasets Details\nA.1. Datasets\nGaze360 (G360). [27] is video 3D gaze datasets. It is col-\nlected in both indoor and outdoor environments in uncon-\nstrained setting, which contains 3D gaze of 238 subjects\nwith a wide-range head pose and gaze direction. G360 is\nrecorded at 8FPS. In all of our experiments, we always used\nthe same training set as [27] with 126928 samples. For the\ntest set, we followed the split of [27] where G360 Full cor-\nresponds to ”All 360°” (the entire test set) with 25969 sam-\nples, G360 180 corresponds to ”Front 180°” (gaze within\n90°) with 20322 samples, and G360 40 to ”Front Facing”\n(gaze within 20°) with 3995 samples. In addition to those\nsplits, we consider G360 Back (gaze above 90°) [8] with\n5647 samples and finally G360 Face (all detected faces)\nwith 16031 samples, which is used in many constrained\ngaze studies [1, 7, 9–11, 18, 47, 51]. When we refer to\nG360 Face 180 (15895 samples), it corresponds to the de-\ntected face with a gaze within 90°, a subset of G360 180,\nthe same for G360 Face 40 with 3687 samples. We used the\nvalidation set described in [27] with 17038 samples.\nGFIE. [24] is a video 3D gaze dataset collected indoors\nwith 71799 frames from 61 subjects (27 male and 34 fe-\nmale). It is an unconstrained dataset with a wide range of\nhead poses. It was collected for gaze following task; using\na complex calibrated laser setup, they can infer the 3D gaze\nfrom the eye to the visual target direction. They recorded\npeople doing various indoor activities at 30 fps. We follow\nthe data splits described in [24], 59217 for training, 6281\nfor validation, and 6281 for testing.\nMPSGaze (MPS). [49] is a modified 3D gaze datasets that\nhas been automatically generated using ETH-Xgaze [53]\neyes. They apply a blending technique on people from the\nWiderface [48] dataset to put eyes with a known 3D gaze\nfrom ETH on heads with similar head poses. This dataset\nis diverse, with more than 10k identities and challenging\nposes, appearances, and lighting conditions. However, the\nblending process reduces the quality of the visual appear-\nance, and it contains only near frontal head poses and no\nback view. We used the same training and test split with\n24282 samples in training and 6277 samples in testing. No\nvalidation is defined in this work.\nEYEDIAP (EDIAP). [19] is a 3D gaze video dataset. It in-\ncludes videos from 16 subjects (30 fps), using either screen\ntargets (CS, DS subset EDIAP) or 3D floating balls ( FT\nsubset EDIAP-FT) as gaze targets. It is a constrained setup\nwith mainly frontal head poses. Following [12, 46], we\nused the evaluation set under screen target session (CS, DS,\nnamely EDIAP) with 16674 samples from 14 subjects.\nMPIIFaceGaze (MPII). [50] is a 3D gaze image dataset\ncollected from 15 subjects in a screen-based gaze target\nsetup, resulting in a constrained dataset with mostly frontal\nhead pose.\nWe follow the standard evaluation protocol\n[12, 46, 50], which selects 3000 images from each subject\nto form an evaluation set for a total of 45000 samples.\nGazeFollow (GF). [40] is a 2D gaze image dataset anno-\ntated on in the wild dataset for the gaze the following task.\nThe 2D target label corresponds to where a given person is\nlooking at in the image. It is a diverse dataset that includes\nvarious head poses, appearances, scenes, and lighting con-\nditions. Overall, it has around 130K annotated person-target\ninstances in 122K images.\nA.2. Video Processing\nAs mentioned in the main section, for video clip input, our\napproach predicts the 3D gaze from an 8-frame video clip.\nHowever, video datasets have different frame rates, which\ncan impact the gaze prediction. In this work, since G360\nhas a lower frame rate, we resample EYEDIAP and GFIE\nto match G360’s frame rate of 8 fps.\nA.3. Gaze Representation\nWorking with different 3D gaze datasets requires a unified\nway to define and represent the 3D gaze vector. Usually, in\nconstrained gaze estimation, studies use data normalization\nto map the input image to a normalized space where a vir-\ntual camera is used to warp the face patch out of the original\ninput image according to the 3D head pose [53]. Thus, the\ngaze is expressed in this virtual camera coordinate defined\nby the 3D head pose.\nHowever, in unconstrained settings, it is not possible to get\naccess to a robust and reliable 3D head pose; thus, we fol-\nlow the gaze representation of Gaze360 [27] in the “Eye\n12\n\n\nFigure S1. Input head crop using different scales. In our work, a\nscale of -0.1 is used and proved to be effective in both constrained\nand frontal face setting Sec. C.1\ncoordinate system”. The practical interpretation of the eye\ncoordinate system is that the positive x-axis points to the\nleft, the positive y-axis points up, and the positive z-axis\npoints away from the camera, i.e. [-1,0,0] is a gaze look-\ning to the right or [0,0,-1] straight into the camera from the\ncamera’s point of view, irrespective of subjects position in\nthe world. The origin of the gaze vector is the middle of\nthe eyes, except for MPS and MPII, where the gaze origin\nis the average of 3D eyes and mouth landmarks resulting in\nan origin located at the middle of the nose, and for GF, we\nused the center of the head bounding box as the origin.\nB. Experiments Details\nMetric. We follow the test split described in the state-of-\nthe-art method and explained in Sec. A.1. As a metric, we\nuse the standard angular error in degrees between the pre-\ndicted and ground truth gaze prediction [19, 27, 50, 53].\nPrevious methods reporting video evaluation used a 7-frame\nvideo clip and predict the middle frame gaze direction.\nSince our approach outputs eight gaze directions from an 8-\nframe video clip, for a fair comparison, we use the 4th gaze\nprediction of an 8-frame video clip to compute the evalua-\ntion metric.\nTraining. We used the same setup in all the experiments\nto be as fair as possible. All the models are trained for a\nminimum of 20 epochs. We used an early stopping on the\nvalidation set with a patience of 10 epochs. We use the\nAdamW optimizer [34] with a learning rate of 1e-4 and a\ncosine annealing schedule with a 5 epochs linear warmup\n(from 2e-5 to 1e-4). For evaluation, we report the perfor-\nmance of the best model defined by the best angular error\non the validation set.\nData augmentation. Data augmentation is crucial for ro-\nbust gaze estimation in the wild. In this work, we used stan-\ndard data augmentation techniques. First, we applied jit-\ntering during the head crop to introduce slight variations in\nscale and aspect ratio, which reduces the model’s sensitivity\nto noisy or imprecise head bounding boxes. Next, color jit-\ntering was applied by adjusting brightness, contrast, and sat-\nuration, making the model more resilient to diverse lighting\nMPII\nEDIAP\nMethod\nTraining Dataset\nImg\nImg\nVid\nPureGaze [13] (Res18)\nG360I Face\n9.3\n9.2\n-\nLiu et al. [31] (Res18)\nG360I Face\n7.7\n9.0\n-\nLiu et al. [31] (Res50)\nG360I Face\n8.3\n7.5\n-\nRAT [4] (Res18)\nG360I Face\n7.6\n7.1∗\n-\nRAT [4] (Res50)\nG360I Face\n7.7\n7.1∗\n-\nCDG [46] (Res50)\nG360I Face\n7.0\n7.3\n-\nSupervised (GaT)\nG360I&V\n7.43\n8.88\n8.28\nST-WSGE (GaT)\nG360I&V+GF\n6.43\n8.87\n8.19\nTable S1. Comparision with state-of-the-art on constrained do-\nmain generalization benchmarks. All these methods [4, 13, 31,\n45, 46] use a face crop as input and are trained on the detected face\nsubset of Gaze360. Our method is trained and tested on head crop\nwhich makes it more general but more challenging for frontal gaze\nestimation. ∗In [4] they used only 6400 sample for EDIAP but we\nfollow [12, 13, 46] with 16674 samples.\nconditions commonly encountered in real-world scenarios.\nSince gaze labels, such as those in the GF 2D dataset, may\nexhibit bias toward one side, we applied horizontal flipping\nto the images while appropriately adjusting the gaze direc-\ntion, ensuring more balanced training data in the yaw gaze\ndirection. These augmentations collectively improved the\nmodel’s ability to handle variations in data and enhance its\ngeneralization to unseen environments.\nC. Additional Experiments\nC.1. Effect of Head Crop Size\nAs mentioned by Chen et al. [8], the input head crop scale\nimpacts the 3D gaze estimation. We find that the effect on\nthe prediction depends on the head orientation. Fig. S1 il-\nlustrates the different inputs with different head crop scales.\nAs shown in Fig. S2b, a smaller head crop tighter to the face\nimproves 3D gaze estimation on frontal head poses, while\na larger head crop improves gaze on the non-frontal head\npose. Indeed, as shown in Fig. S1, a tighter crop increases\nthe eye resolution in the image and a larger crop provides\nmore context about the head orientation and upper body ori-\nentation, which gives a strong prior for the gaze direction\nwhen eyes are not visible. In the context of gaze estimation\nin the wild, a scale of -10% is part of the Pareto front as\nillustrated in Fig. S2b and is also the best on the G360 Full\nimage as shown in Fig. S2a. Therefore, it is a reasonable\ntrade-off between frontal and back view performance. We\nuse it for all our experiments.\nC.2. Constrain Gaze Evaluation\nThe objective of this work is to improve unconstrained gaze\nestimation in the wild. As seen in Sec. C.1, compared to a\ntight face crop a larger crop improves gaze in challenging\nhead pose. Therefore, a larger crop is more suited to our\nobjective. In contrast, some methods specialize in frontal\n13\n\n\n(a) Effect of head bounding box scale as input on the 3D gaze angular error\non G360 Full test set. A scale ratio of 0.1 corresponds to a 10% bounding\nbox scale.\n(b) Effect of head bounding box scale on the angular error with respect to\nG360 Back and G360 40 test subset.\nFigure S2. Effect of head crop size.\ngaze estimation and rely on tight face crops, which provide\nbetter resolution for the eye regions. While this is not a\nfully fair comparison, we compare our approach to these\nconstrained methods for generalization on constraint bench-\nmarks. Note that for the constrained methods, models are\ntrained and tested only on a subset of detected faces (G360\nFace), while in our approach the model is trained on G360\nFull.\nAs shown in Tab. S1, on MPII, the supervised GaT lags\nbehind the best method by 6%. On EDIAP, GaT is 21%\nbehind the best method in image evaluation but narrows the\ngap to 13% when evaluated on videos. Then, when using\nour ST-WSGE learning framework including GF labels, we\nobserve an important improvement on MPII with state-of-\nthe-art angular error of 6.43 compared to 7 from CDG. On\nEDIAP the improvement is marginal. Compared to EDIAP,\nMPII has more diversity in lighting conditions and environ-\nment. GF doesn’t contain a lot of frontal gaze direction but\nhas a broad diversity of environments. Therefore, the im-\nprovement on MPII should come from the additional diver-\nFigure S3. Image vs video predictions, where does it help?. GaT\ntrained on G360I&V and tested on G360 Full image and video.\nThe difference between image and video angular error with re-\nspect to the ground truth gaze directions from the camera ([0,0,-\n1]). The mean and standard deviation are displayed for each 10°\nbin. Positive values indicate better performance in video predic-\ntion compared to image prediction.\nsity that GF brings but this is not useful for EDIAP predic-\ntion. While constrained methods excel in frontal settings,\nthey fail in unconstrained scenarios. Our approach, which\nachieves state-of-the-art performance in unconstrained en-\nvironments (G360, GFIE) while remaining competitive in\nconstrained settings (MPII, EDIAP), proves to be a versa-\ntile and robust solution for gaze estimation in the wild.\nC.3. Qualitative Analysis\nWhen does temporal context contribute most effec-\ntively? As seen in the main paper, video prediction con-\nsistently outperforms image prediction. To understand the\nsignificance of temporal context in gaze estimation, we ex-\namined cases with large angular errors between image and\nvideo predictions. Several key observations emerged. As\nillustrated in Fig. S4 in the first two rows, temporal con-\ntext proves valuable during blinks, as it allows the model to\ninterpolate gaze direction when the eyes are closed. If the\nhead pose is not informative, temporal context helps disam-\nbiguate between blinking and looking down since the eyes\nare not visible, as shown in row 1. Additionally, when in-\ndividuals are viewed entirely from behind (rows 6-7), video\ninferences provide a more consistent gaze direction in re-\nlation to time. Thus, there is less jittering and it might im-\nprove the prediction accuracy. In rows 4-5, the head and eye\nmotion can be used in video prediction to improve the gaze\ndirection. Finally, it can help in case of occlusion, as seen\nin row 3.\n14\n\n\nFurthermore, we explore the impact of image- and video-\nbased prediction with respect to gaze direction. Indeed, we\nexpected more improvement when people are from the back\nsince additional head motion cues can be useful for gaze\nestimation. In the results, video prediction on G360 Back\nclearly improves image prediction. In addition, in Fig. S3,\nwe plot the difference between image and video prediction\nangular error for different gaze directions. If we look at\nthe trend, video prediction seems to be better, especially for\ngaze over 150°, but given the standard deviation, it might\nnot be a statistically significant observation. A more de-\ntailed analysis by considering only cases where there is a\nhead motion can better highlight the impact of video pre-\ndiction.\nWhat are the limitations of temporal context for gaze?\nWe investigate prediction made on the VideoAttentionTar-\nget [15] (VAT) videos using our ST-WSGE framework and\nGaT model. VAT is a challenging dataset with real-world\nscenarios, various appearances, and diverse gaze distribu-\ntion, making it well-suited for assessing our approach. Our\nqualitative analysis reveals two limitations of video-based\ninference compared to image-based inference using our\nmodel. The first limitation arises in cases of rapid head ro-\ntation, as illustrated in Fig. S5, temporal context may be\nmisused, leading to predictions that do not align with the\nactual gaze. It might be because no rapid head motion is\npresent in the G360 training sets. The second aspect in-\nvolves cases of “gaze recentering”, where the gaze direc-\ntion returns to its initial position following a shift. This be-\nhavior can occur very rapidly, within just 3-4 frames. Due\nto the smoothing effect in the temporal modeling, the pre-\ndicted gaze may not exhibit the same amplitude as the actual\nmovement. Indeed, this behavior is not present in the G360\ndataset, and the use of videos sampled at 8 frames per sec-\nond may limit the ability to capture fine-grained gaze dy-\nnamics. However, such behavior is better captured during\nimage-based inference. This highlights a trade-off: while\nvideo-based inference provides smoother and more robust\npredictions, image-based inference offers greater accuracy\nbut can result in jittery outputs. To mitigate the lack of natu-\nral gaze behavior we apply our ST-WSGE framework using\n2D gaze video data from VAT. Unfortunately, since current\nbenchmarks don’t contain natural gaze behavior, the results\ndon’t show quantitative improvement. Further research to\nevaluate this aspect is needed.\nIn which scenarios does ST-WSGE with GazeFollow la-\nbels provide the most benefit? We demonstrated the ad-\nvantages of ST-WSGE with GazeFollow labels across var-\nious benchmarks, both within- and cross-datasets. But in\nwhich scenarios does it outperform supervised methods\ntrained solely on G360? To address this question, we an-\nalyze predictions made in real-world scenarios using the\nVideoAttentionTarget (VAT) dataset [15]. Our findings re-\nveal that ST-WSGE achieves the most notable improve-\nments in cases of extreme head poses, particularly when\nthe head is facing downward, as shown in Fig. S6. It is\nalso more robust to appearance diversity like hair partially\noccluding the face or varying skin tones. It also helps in dif-\nficult lighting conditions and low-resolution inputs. Addi-\ntionally, we include a video (provided in the supplementary\nmaterials) displaying predictions on VAT with an explana-\ntion, enabling a direct comparison between the two methods\nand a clearer visualization of our approach’s performance\non real-world data.\n15\n\n\nFrame 0\nFrame -3\nFrame -2\nFrame -1\nFrame 0\nFrame 1\nFrame 2\nFrame 3\nFrame 4\nGT\nST-WSGE (Gaze360+GF)\nvideo inference\nST-WSGE (Gaze360+GF)\nimage inference\nFigure S4. Illustration of image against video prediction. Comparison between single-image (frame 0) and video predictions (frame\n-3 to 4). We use our ST-WSGE learning framework with GaT trained on G360 and GF. All examples are from G360 test set. Rows 1-2\nillustrate eye blinks, Row 3 shows an example of occlusion, Rows 4-5 demonstrate frontal head/eyes motion, and Rows 6-7 depict back\nview prediction. In the last row, the first two frames are not part of the test subset. Arrows in red represent image predictions, and arrows in\nmagenta are video predictions. The angular error between groundtruth and prediction is displayed in red at the top right corner. The circles\nin the images represent unit disks where 3D gaze vectors are projected onto the image plane (x,y in yellow) and a top view (x,z in blue)\n16\n\n\nFrame -9\nFrame -6\nFrame -3\nFrame 0\nFrame 3\nFrame 6\nFrame 9\nFrame 12\nGT\nST-WSGE (Gaze360+GF)\nvideo inference\nST-WSGE (Gaze360+GF)\nimage inference\nFigure S5. Illustration of image and video prediction in case of rapid head motion. We use our ST-WSGE learning framework with\nGaT trained on G360 and GF. All examples are from VideoAttentionTarget [15] (VAT). Arrows in red represent image predictions, and\narrows in magenta are video predictions. The circles in the images represent unit disks where 3D gaze vectors are projected onto the image\nplane (x,y in yellow) and a top view (x,z in blue). Note that since VAT has a frame per second (fps) of 24 and G360 has a fps of 8, we show\nthe temporal context used for video inference corresponding to 8 fps.\n17\n\n\nST-WSGE (Gaze360+GF)\nimage inference\nSupervised (Gaze360)\nimage inference\n \nFigure S6. Illustration of supervised against ST-WSGE learning framework with GazeFollow label. We use in both experiments our\nGaT model. All examples are from VideoAttentionTarget [15] (VAT). Arrows in blue represent image predictions with supervised GaT\ntrained on G360, and arrows in red are image predictions with ST-WSGE GaT trained on G360 and GF. The circles in the images represent\nunit disks where 3D gaze vectors are projected onto the image plane (x,y in yellow) and a top view (x,z in blue).\n18\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20249v1.pdf",
    "total_pages": 18,
    "title": "Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels",
    "authors": [
      "Pierre Vuillecard",
      "Jean-Marc Odobez"
    ],
    "abstract": "Accurate 3D gaze estimation in unconstrained real-world environments remains\na significant challenge due to variations in appearance, head pose, occlusion,\nand the limited availability of in-the-wild 3D gaze datasets. To address these\nchallenges, we introduce a novel Self-Training Weakly-Supervised Gaze\nEstimation framework (ST-WSGE). This two-stage learning framework leverages\ndiverse 2D gaze datasets, such as gaze-following data, which offer rich\nvariations in appearances, natural scenes, and gaze distributions, and proposes\nan approach to generate 3D pseudo-labels and enhance model generalization.\nFurthermore, traditional modality-specific models, designed separately for\nimages or videos, limit the effective use of available training data. To\novercome this, we propose the Gaze Transformer (GaT), a modality-agnostic\narchitecture capable of simultaneously learning static and dynamic gaze\ninformation from both image and video datasets. By combining 3D video datasets\nwith 2D gaze target labels from gaze following tasks, our approach achieves the\nfollowing key contributions: (i) Significant state-of-the-art improvements in\nwithin-domain and cross-domain generalization on unconstrained benchmarks like\nGaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii)\nSuperior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360\ncompared to frontal face methods. Code and pre-trained models will be released\nto the community.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}