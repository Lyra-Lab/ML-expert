{
  "id": "arxiv_2502.21151v1",
  "text": " \n \nA REVIEW ON GENERATIVE AI FOR TEXT-TO-IMAGE AND \nIMAGE-TO-IMAGE GENERATION AND IMPLICATIONS TO \nSCIENTIFIC IMAGES \n \n \n \n \nZineb Sordo \nApplied Math and Computational Research \nLawrence Berkeley National Laboratory \nBerkeley, CA 94720 \nzsordo@lbl.gov \n \nEric Chagnon \nApplied Math and Computational Research \nLawrence Berkeley National Laboratory \nBerkeley, CA 94720 \nechagnon@lbl.gov \n \nDaniela Ushizima ∗ \nApplied Math and Computational Research \nLawrence Berkeley National Laboratory \nBerkeley, CA 94720 \ndushizima@lbl.gov \n \n \nFebruary 28, 2025 \n \nABSTRACT \nThis review surveys the state-of-the-art in text-to-image and image-to-image generation within \nthe scope of generative AI. We provide a comparative analysis of three prominent architectures: \nVariational Autoencoders, Generative Adversarial Networks and Diffusion Models. For each, we \nelucidate core concepts, architectural innovations, and practical strengths and limitations, particularly \nfor scientific image understanding. Finally, we discuss critical open challenges and potential future \nresearch directions in this rapidly evolving field. \n \n1 Introduction \n \nGenerative AI (genAI) has emerged as a powerful tool with the ability to create novel digital content, including images, \ntext, and music [5]. However, using generative AI to create scientific images of phenomena unseen by the model \ncontinues to be challenging, and prone to hallucination [43] and misrepresentation of scientific principles. If the model \nextrapolates beyond its training data, it can generate images that, while visually plausible, are physically or biologically \nimpossible [37]. This can lead to the propagation of inaccurate scientific concepts and hinder genuine discovery [19, 20]. \nThis paper overviews the major milestones in the last few years, then describes how Variational Autoencoders (VAEs), \nGenerative Adversarial Networks (GANs) and Diffusion Models have revolutionized these areas. Finally, we delineate \npotential avenues for verification and validation. \nOverall, this paper focuses on two key subdomains: text-to-image and image-to-image generation. \nThis review aims to: \n• Analyze their applications in text-to-image and image-to-image generation. \n• Compare and contrast the strengths and weaknesses of these approaches for scientific data generation. \n• Discuss the challenges and future directions of research in this field. \n \n∗Corresponding Author \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n2 \n \n \n2 Background \nThis section provides an overview of the key advancements in text-to-image and image-to-image generation technologies, \nparticularly considering the technologies from major tech companies, such as Google, Meta, Microsoft and OpenAI. \nWe highlight significant software releases and the underlying algorithms that have shaped the landscape of generative \nAI from 2021 to 2024. \nIn 2021, OpenAI introduced DALL-E, a groundbreaking model that utilized a variant of the GPT-3 architecture, which \nis a large language model (LLM), to generate images from textual descriptions. DALL-E employed a transformer-based \narchitecture, leveraging the principles of attention mechanisms to understand and synthesize complex relationships \nbetween text and visual elements. The model was trained on a diverse dataset of text-image pairs, enabling it to create \nnovel images that often combined disparate concepts in coherent and imaginative ways. This marked a significant \nadvancement in generative models, setting the stage for future developments in text-to-image synthesis[30, 25]. \n \n \n \nFigure 1: Major highlights of language and multimodal models, with less focus on text-to-image generation models \n[38]. \n \nThe year 2022 saw significant advancements in text-to-image generation technologies. Google introduced Imagen, a \ndiffusion-based model that generates high-quality images from textual prompts. Imagen utilized a two-step process \ninvolving a denoising diffusion probabilistic model (DDPM), which iteratively refined a random noise image into a \ncoherent visual representation based on the input text. While Imagen did not directly employ Variational Autoencoders \n(VAEs) or Generative Adversarial Networks (GANs) [8], its diffusion approach presented an alternative to these \ntraditional generative frameworks. Meanwhile, OpenAI released DALL-E 2, which improved upon its predecessor by \nincorporating CLIP (Contrastive Language-Image Pretraining) [25] to better align the generated images with textual \ndescriptions[28]. CLIP itself is a multi-modal vision and language model that understands and relates textual and \nvisual information. CLIP has since become the benchmark for text-to-image and multi modal text+image-to-image \ngeneration. Meta’s Make-A-Scene also emerged, allowing users to have more control over the composition of generated \nimages through scene graphs, enhancing the interactivity of the image creation process[6]. Additionally, Microsoft \nintegrated OpenAI’s models into its Azure platform, making these advanced capabilities more accessible to developers \nand businesses. \nIn 2023, the landscape of generative AI continued to evolve with notable releases and innovations. Google unveiled \nImagen Video, extending the capabilities of its diffusion models to generate videos from text prompts, thus introducing \ntemporal coherence and movement to the generative process[10]. OpenAI launched DALL-E 3, which featured enhanced \ntext comprehension and image generation capabilities, further refining the alignment between textual input and visual \noutput through improved training techniques and larger datasets[1]. This model continued to leverage the principles of \nLLMs to enhance its generative abilities. Anthropic’s Claude AI also began to incorporate multimodal functionalities, \nallowing for a richer interaction between text and images, while not strictly focused on image generation, it illustrated \nthe growing trend of integrating LLMs with visual understanding. Meta’s Segment Anything Model (SAM), a zero-shot \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n3 \n \n \nalgorithm, has facilitated advanced image segmentation, providing tools for image-to-image manipulation by enabling \nusers to specify regions of interest within images[16]. Finally, Microsoft’s Copilot integrated these generative AI \ntechnologies into design workflows, allowing users to seamlessly leverage AI-driven image generation and manipulation \ntools, thus democratizing access to sophisticated design capabilities[36]. \nIn 2024, major tech players release improved technologies for text-to-image and image-to-image generation. Google, \nwith its ongoing development of Imagen and related models, focused on enhanced photorealism and semantic under- \nstanding, leveraging diffusion models and potentially incorporating LLMs for improved prompt interpretation. Meta \nexpanded its offerings, building upon its Emu architecture, emphasizing both speed and quality, exploring variations \nof diffusion models potentially incorporating VAEs for efficient latent space manipulation [17][7][33]. OpenAI con- \ntinued refining DALL-E, focusing on higher resolution and more consistent image generation, further optimizing its \ndiffusion-based pipeline and potentially integrating LLM-driven refinement steps. Anthropic, while primarily known \nfor LLMs, began exploring visual generation in conjunction with its Claude model, potentially integrating diffusion \nmodels with their sophisticated contextual understanding capabilities. Microsoft further solidified its position with \nupdates to its Designer and Image Creator powered by DALL-E 3, focusing on user accessibility and integration within \nits ecosystem. Underpinning these advancements are diffusion models, which iteratively denoise images from Gaussian \nnoise, often operating within a latent space defined by VAEs for computational efficiency. Some models, though less \nprevalent in 2024 for high-fidelity image generation, may still utilize GANs for specific tasks like upscaling or style \ntransfer, though diffusion models have become the dominant approach for general text-to-image and image-to-image \ntasks. The integration of LLMs, especially in prompt understanding and refinement, became a key trend, ensuring \ngenerated images more accurately reflect the intent of the user. \n \n3 Key Generative Architectures \n3.1 Variational Auto-Encoder (VAE) \nFirst introduced in 2013 [15], the Variational Auto-Encoder (VAE) is a type of generative neural network capable of \nlearning a probability distribution over a set of data points without labels. A VAE learns to encode input data into a \nlower-dimensional latent space and then decode it back to the original space by sampling latents, while ensuring the \nlatent representations follow a known probability distribution. \nVAE is categorized as a model with an explicit intractable density function (tractable models allow for explicit likelihood \ncomputation) that learns a probability distribution using variational inference and latent variables. Intuitively, latent \nvariables (LV) “explain” the data in a “simpler” way, and more rigorously, LV result from a transformation of the data \npoints into a continuous lower-dimensional space. Mathematically, we define x as a data point that follows a probability \ndistribution p(x) and z, to be a latent variable that follows a probability distribution p(z), then: \n• p(x) is the marginal distribution (and goal of the model) \n• p(z) is the prior distribution \n• p(x|z) is the likelihood mapping latents z to data points x \n• p(x, z) = p(x|z) ∗ p(z) is the joint distribution of data points and latent variables \n• p(z|x) is the posterior distribution that describes z that can be produced by x \n \n \n \nFigure 2: Variational Inference \n \nThe generative process in VAEs consists of computing x given z (computed by p(x|z)) by sampling z ∼ p(z) and \nx ∼ p(x), while inference consists of finding z given x (computed by p(z|x)) by sampling x ∼ p(x) and then sampling \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n4 \n \n \n| \n| \n∼ \n| \n| \nx  p(z x) (see Figure 2). To find the parameters of the marginal distribution p(x), we can apply gradient descent \nwhich translates to compute the following: \n \n∇logpθ(x) =\n  \npθ(z|x)∇θlogpθ(x, z)dz \n(1) \nThe goal of variational inference is to approximate the posterior distribution p(z x) with an explicit tractable probability \ndistribution and allow its computation as an optimization problem. We can call this distribution the variational posterior \nq(z x). During training, the goal is to minimize the Kullback-Leibler (KL) divergence, which expresses the difference \nbetween the true posterior and the variational posterior and is given by (with θ being the model parameters): \n \n  ∞ \np(x) \n p(x)  \nKL(P ||Q) = \np(x)log( \n)dx = Ex∼p(x)[log( \n] \n(2) \n−∞ \nq(x) \nq(x, θ) \n \nTo enable backpropagation in the decoder part of the VAE, this method considers a reparametrization trick where \ninstead of just sampling from the latent distribution, they add a random noise to the mean and standard deviation to \nmake gradient computation possible. \n \n \n \n \nFigure 3: VAE Encode - Decoder architecture \n \n \nThe final loss function is given by the following equation: \n \nLθ,ϕ(x) = Eqϕ(z|x) [logpϕ(x|z)] − KL(qϕ(z|x)||pθ(z)) \n(3) \nThe first part of the previous equation (also called negative reconstruction error associated to the Decoder part of \nthe VAE) controls how well the model reconstructs x given z of the variational posterior whereas the second part \n(corresponding the Encoder part of the VAE) controls how close the variational posterior q(z x) is to the prior p(x) i.e. \nhow well the dimensionality reduction of the Encoder captures the data features within the latent space. \n \n3.2 Generative Adversarial Networks (GANs) \nThe first Generative Adversarial Network (GAN) was introduced in 2014 [9, 8] and represents a major advancement in \ngenerative learning. GANs are a class of machine learning models that consists of two key components: a generator \nand a discriminator, where the generator aims to produce synthetic data, and the discriminator attempts to distinguish \nbetween the real data and synthetic data (see Figure 4. \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n5 \n \n \n∼ \n− \nG \nD \nG \nD \n \n \n \nFigure 4: Architecture of the Vanilla GAN \n \n• The generator G(z) maps random noise z  p(z) (also called latents) to the data distribution and outputs the \nsynthetic image in the shape of a 1D-vector. The stochasticity given by this random sampling will provide a \nnon-deterministic output, which is how the model creates diversity in the generation process. The goal here is \nto fool the discriminator and minimize log(1 D(G(z))), which amounts to maximizing the discriminator’s \nmistakes. \n• The discriminator D(x) takes as input a real and synthetic image (generated by the generator) and outputs the \nprobability that the image corresponds to the real data distribution or not. The goal here is to maximize the \nloss function or the probability that it correctly classifies real and fake images. \nThis adversarial process drives both the generator and the discriminator to improve, resulting in high-quality synthetic \ndata. In addition, the fact that the generator is only trained to fool the discriminator makes this Vanilla GAN model \nunsupervised. \nThe goal of the GAN is to solve the min-max game or adversarial game between the generator and the discriminator \nwith the following objective function and optimization problem: \n \nmin max V (G, D) = Ex∼pdata(x)[logD(x)] + Ez∼pfake(z)[log(1 − D(G(z)))] \n(4) \n \nwhere D(x) is the probability that x is real, G(z) is the generated sample, and thus D(G(z)) is the probability that the \ngenerated image given latent z is real. One of the most common limitations of GANs is the so-called mode collapse \nproblem where the generator fails to represent accurately the pixel space of all possible outputs. This issue is common \nin high-resolution images, where too many fine-scale features must be captured. In that case, the generator gets stuck in \na parameter setting with a similar level of noise that can consistently fool the discriminator and only captures a subset \nof the real data distribution. It then fails to produce diversity in its outputs and collapses to producing only a few types \nof synthetic samples. \n \n3.2.1 Conditional GAN (CGAN) \nAs an extension of the Vanilla GAN, the Conditional GAN was introduced in 2014 [21] and uses conditional information \n(image or text) to guide the generation process. The CGAN performs conditioning generation by feeding information \nto both the generator and the discriminator. The generator G(z, y) takes as input random noise z, and the conditional \nembedding y and learns to generate data given this condition, whereas, the discriminator D(x, y) learns to classify real \nand fake images by checking that the condition y is met. The min-max optimization function becomes: \nmin max V (G, D) = Ex∼pdata(x)[logD(x|y)] + Ez∼pfake(z)[log(1 − D(G(z|y), y))] \n(5) \nStackGAN [41] and Attentional GAN (AttnGAN) [40] are influential CGAN architectures that advanced text-to-image \ngeneration. StackGAN introduced a hierarchical approach, generating low-resolution images and iteratively refining \nthem to high-resolution outputs. AttnGAN innovated with attention mechanisms, allowing the model to selectively \nattend to specific words or phrases in the text description when generating corresponding image regions. \n \n3.2.2 Deep Convolutional GAN (DCGAN) \nFollowing the initial development of GANs, various architectures emerged, notably Deep Convolutional Generative \nAdversarial Networks (DCGANs) introduced by Radford et al. in 2015 [29], which extended the foundational GAN \nframework. While the Vanilla GAN’s architecture contains simple downsampling and upsampling layers with ReLU \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n6 \n \n \nrr \ns=0 \nN \n| \nrr \nX\nX\nactivations and a Sigmoid activation for the discriminator, this variant of the GAN is made of strided convolution layers, \nbatch norm layers, and LeakyReLU activation functions. This architecture is adapted to small size images such as RGB \ninputs of shape (3,64,64). \n \n3.3 Diffusion Models \nDiffusion models, now producing state-of-the-art high-fidelity and diverse images, have evolved from the initial work \nof Sohl-Dickstein et al. in 2015 [34] to the significantly impactful Denoising Diffusion Probabilistic Models (DDPM) \nby Ho et al. in 2020 [11]. These models differ from the previous generative models as they decompose the image \ngeneration process through small denoising steps. In fact, the idea behind diffusion models is that they take an input \nimage x0 and gradually add Gaussian noise in what is called the forward process. The second part of the network is the \nreverse process, or sampling process, which consists of removing the noise to obtain new data (see Figure 5). \nThe forward process in the diffusion network consists of a Markov chain of T steps. Given an input image x0 sampled \nfrom the true data distribution x0 ∼ q(x0), at each step t < T , Gaussian noise is added to xt−1, according to a variance \nschedule β1, ..., βt to obtain xt ∼ q(xt|xt−1) where the x1, ..., xT are latents of the same dimensionality as x0: \nq(xt|xt−1) = N(xt; µt = \n✓\n1 − βtxt−1, Σt = βtI) \n(6) \nWhere I is the identity matrix and the variances βt can be learned or kept constant as hyperparameters. In the case of \nthe DDPM paper, the authors used a linear schedule increasing from β1 = 10−4 to βT = 0.02. The scheduler, however, \ncan be linear, quadratic, cosine [24] etc. The posterior probability can be defined as: \n \nT \nq(x1:T |x0) = \nq(xt|xt−1) \n(7) \nt=1 \n And using the reparametrization trick to obtain a tractable closed-form sampling at any timestep, we define αt = 1 − βt, \nα¯ =  t \nαs where ϵ0, ..., ϵt−1 ∼ N(0, I) and finally have: \nxt ∼ q(xt|x0) = N(xt; α¯tx0, (1 − α¯)I) \n(8) \nGiven that βt is a hyperparameter, it is possible to compute αt and α¯ t  for all timesteps. We can therefore sample the \nnoise at any timestep t and get the latent variables xt. \nThe second part of diffusion models is the reverse process which is also a Markov chain with learned Gaussian transitions \nstarting at p(xT ) =  (xT ; 0, I). The goal of the reverse process is to learn the reverse distribution q(xt−1 xt) by \napproximating it with a parametrized model pθ (where pθ is Gaussian and the mean and variance will be parametrized \nand learned by a neural network): \n \npθ(xt−1|xt) = N(xt−1; µθ(xt, t), Σθ(xt, t)) \n(9) \nT \npθ(x0:T ) = pθ(xT ) \npθ(xt−1|xt) \n(10) \nt=1 \n \nTo train diffusion models, we optimize the negative log-likelihood of the training data. This optimization, similar to the \napproach used in Variational Autoencoders (VAEs), is achieved by maximizing the Evidence Lower Bound (ELBO), a \ntractable approximation. The following expression represents the ELBO after a series of computational steps: \n \nT \nlog p(x) ≥ Eq(x1|x0) [log pθ(x0|x1)] − DKL (q(xT |x0)∥p(xT )) − \nEq(xt|x0) [DKL (q(xt−1|xt, x0)∥pθ(xt−1|xt))] \nt=2 \n(11) \nT \nlog p(x) ≥ L0 − LT − \nLt−1 \n(12) \nt=2 \nWhere: \n• Eq(x1|x0) [log pθ(x0|x1)] is the reconstruction term \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n7 \n \n \nL\n• \n0 \n| \n| \nrr \n \n \nFigure 5: Denoising diffusion probabilistic models (DDPMs). Source: [18] \n \n• DKL (q(xT |x0)∥p(xT )) basically shows how close xT is to the Standard Gaussian distribution. \nT \nt=2 Lt−1 = Lt represents the difference between the denoising steps pθ(xt−1|xt) and the approximated \nones qθ(xt−1 xt, x0). The KL divergence compares pθ(xt−1 xt) against forward process posteriors, which \nare tractable when conditioned on x0. \nTherefore maximizing the likelihood amounts to learning the denoising steps Lt. Based on further calculations, the \nDDPM paper shows that instead of predicting the mean of the distribution during the training of the reverse process, the \nmodel will predict the noise ϵ at each timestep t using the following simplified formula of the denoising term in the \nELBO (also the loss function of the reverse process network): \nLsimple(θ) := Et,x ,ϵ\n ∥ϵ − ϵθ(√α¯tx0 + √1 − α¯tϵ, t)∥2 \n(13) \nThe model associated to the loss function of the reverse process is a U-Net architecture with residual blocks, group \nnormalization as well as self-attention blocks. The timestep t is concatenated to the input image using a cosine positional \nembedding into each residual block. This denoising U-Net is trained to predict the noise at each timestep of the process. \n \n3.3.1 Conditional Image Generation with Guided Diffusion \nClassifier Guidance \nSimilarly to the CGAN, an important extension of the diffusion model is the Guided diffusion model that includes \nconditional image generation in the network. In that scenario, the model adds conditioning information y at each \ndiffusion step: \nT \npθ(x0:T |y) = pθ(xT ) \npθ(xt−1|xt, y) \n(14) \nt=1 \n \nUsing Bayes rule with some computations and more importantly by adding the guidance scalar term s, we can show \nthat guided diffusion models aim to learn ∇logpθ(xt|y) such that: \n∇logpθ(xt|y) = ∇logpθ(xt) + s.∇logpθ(y|xt) \n(15) \nIt was also shown in [34] and [3] that a classifier guidance model defined by fΦ(y|xt, t) can guide the diffusion \ntowards the target class y by training fΦ(y|xt, t) on a noisy image xt to predict class y. To do so, we build a class- \nconditional diffusion model with mean µ(xt|y) and variance Σθ(xt|y) and perturb the mean by the gradients of \nlogfΦ(y|xt, t) of class y, resulting in: \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n8 \n \n \n∇ \n∇ \ni \nx \n2 \n2 \ni \ni \nσi \ni \n2 \ni=1 \n \nµˆ(xt|y) = µθ(xt|y) + s · Σθ(xt|y)∇logfΦ(y|xt, t) \n(16) \n \n \nFigure 6: Algorithm of classifier guided diffusion. Source: [3] \n \nClassifier Free-Guidance \nClassifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating \nthe need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and \nintroduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional \nand unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional \ndiffusion model ϵθ(xt|y) and an unconditional model ϵθ(xt|y = 0) as a single neural network as follows: \nϵˆθ(xt|y) = ϵθ(xt|0) + s · (ϵθ(xt|y) − ϵθ(xt|0)) \n(17) \nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process \nand can take different types of conditional data such as text embeddings. We will see that many models rely on classifier \nfree-guidance especially when training on multi-modal data. \n \n3.3.2 Score-based generative models \nScore-Based Diffusion Models (SBDMs) are a class of diffusion models proposed by [35] that use score functions \n(gradient of the log probability density function) and Lagevin dynamics (iterative process where we draw samples from \na distribution based only on its score function). Like GANs, SBDMs use adversarial training and try to generate images \nthat are indistinguishable from real images. \nInstead of learning a probability density p(x), the neural network sθ estimates the score function  xlogp(x) directly, \nand the training objective can be as follows: \nEp\n  ∥∇x log p(x) − sθ(x) − ∥2 =\n  \np(x∥∇x log p(x) − sθ(x)∥2dx \n(18) \n \nWhile Langevin dynamics can sample p(x) using the approximated score function, directly estimating xlogp(x is \ndifficult and imprecise. To address this, diffusion models learn score functions at various noise levels, achieved by \nperturbing the data with multiple scales of Gaussian noise. \nSo given the data distribution p(x), we perturb it with Gaussian noise N (0, σ2I) where i = 1, 2, . . . , L to obtain a \nnoise-perturbed distribution: \npσ (x) =\n  \np(y)N (x; y, σ2I) dy \n(19) \n \nThen we train a network sθ(x, i), known as the Noise Conditional Score-Based Network (NCSN), to estimate the score \nfunction ∇x log pσi (x) . The training objective is a weighted sum of Fisher divergences for all noise scales: \nL \n   \nλ(i)Ep  (x) ∥∇x log pσ (x) − sθ(x, i)∥2 \n(20) \n \n \nThe authors of [35] combine components of NSCNs and DDPMs into one generative model, based on Stochastic \nDifferential Equations (SDE) that does not depend on the data and no trainable parameters. Rather than perturbing \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n9 \n \n \n{  } \ndata with a finite set of noise distributions, we utilize a continuous range of distributions that evolve over time through \na diffusion process xt t∈[0,T ]. This process is governed by a predefined SDE. Then, it is possible to generate new \nsamples by reversing this process. The forward process going from an input image x0 to random noise xT is defined \nsuch that: \n \ndx = f (x, t)dt + g(t)dw \n(21) \n \n \nWhere: \n \n• dw is a Wiener process (random noise), \n \n• f (x, t) is the drift term and a vector valued function \n \n• g(t) is the diffusion term and a scalar function \n \nAfter adding noise to the original data distribution for enough time steps, the perturbed distribution becomes close \nto a tractable noise distribution. Then it is possible to generate new samples by reversing the diffusion process and \ncomputing the reverse SDE given that the SDE was chosen to have a corresponding reverse SDE in closed form (see \nFigure 7): \n \ndx = f (x, t) − g2(t)∇x log pt(x) dt + g(t)dw \n(22) \n \n \n \n \n \n \nFigure 7: Score-based generative modeling through SDE \n \n \n3.4 Stable Diffusion and Latent diffusion models \n \nLatent diffusion models (LDMs) are yet another innovative extension of diffusion models [32]. Instead of applying the \ndiffusion on a high-dimensional input (pixel space), we project the input image into a smaller latent space and apply \ndiffusion with latents as inputs. The authors of [32] propose to use an encoder network g to downsample the input \ninto a latent representation zt = g(xt) and apply the forward process to zt. Then the reverse process is the same as \na standard diffusion process with a U-Net to generate new data that are then upsampled by a decoder network (see \nFigure 8). Therefore, given an encoder ε (Stable diffusion uses a pre-trained VAE encoder network), then the loss can \nbe formulated as: \n \nLLDM = Eε(x),t,ϵ[∥ϵ − ϵθ(zt, t)2∥] \n(23) \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n10 \n \n \n \n \n \nFigure 8: Latent diffusion architecture. Source: [18] \n \n \nStable diffusion can also be conditioned, in particular, using classifier-free guidance by adding conditional embeddings \nsuch as image features or text descriptions using a text encoder (e.g. CLIP’s text encoder) to steer the generation \nprocess. \n \n3.4.1 Diffusion Transformers (DiT) \nOne of the most recent diffusion-based models is the Diffusion Transformer (DiT) proposed in [26] which is an \narchitecture that combines the principles of diffusion models and transformer models and that generates high-quality \nsynthetic images. It leverages the iterative denoising process inherent in diffusion models while utilizing the powerful \nrepresentation learning capabilities of transformers for improved sample generation. The authors in [26] replace the \nU-Net backbone, in the LDM model, by a neural network called a Transformer [39]. Transformers are a class of \nmodels based on self-attention mechanisms, and they have been proven to excel in tasks involving sequential data (like \nlanguage processing). They work by attending to all input tokens at once and using multi-head self-attention to process \nthe input efficiently. Mathematically, the attention mechanism can be formulated as follows: \n \n QKT  \n \n \nWhere: \nAttention(Q, K, V ) = softmax \n√d \nV \n(24) \n \n• Q (query), K (key), and V (value) are input representations. \n• dk is the dimensionality of the key vectors, and the softmax function normalizes the attention scores. \n \nIn the context of a Diffusion Transformer (see Figure 9), the input to the transformer is typically a set of tokens or \nfeatures (e.g., image patches, sequence tokens), and self-attention helps the model attend to dependencies across all \ntokens to capture long-range relationships. In the reverse process of the diffusion model, the transformer network is \nresponsible for predicting the noise at each step, conditioned on the noisy data. For example, given the noisy image at \ntime step t, the transformer can model long-range spatial dependencies across the image patches (or sequence tokens) \nand generate a clean image at the next step: \n \nxt−1 = Transformer(µθ(xt, t), context) \n(25) \nWhere: \n \n• µθ(xt, t) is the predicted noise (as described in the reverse diffusion equation), \n• context could be a conditioning input, such as a text prompt (in the case of text-to-image generation). \nk \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n11 \n \n \n \n \n \n \nFigure 9: Architecture of the Diffusion Transformer \n \n4 Comparative Analysis \nWe can define a set of metrics to evaluate a model family’s general performance in image generation. Image Quality \nrefers to the level of detail in the generated image. A model with high Image Quality strictly adheres to the imposed \nrestrictions placed on it while maintaining a high level of detail, and absence of artifacts. A model with low Image \nQuality consistently generates images with large amounts of noise and/or artifacts and incoherent features[14]. A \nmodel’s Diversity refers to its range of potential outputs. A model with high diversity can produce a wide spectrum of \nimages while maintaining a constant image quality. A model with low diversity can only generate images in a narrow \nrange with a constant image quality[23]. Leaving this narrow range can lead to significant and rapid decreases in image \nquality. Controllability refers to how easy it is to guide the image generation process with some additional input. For \nexample, if you wanted to generate variations of an image you could condition the model with an input image to help \nshape the generated image. A highly controllable model can take into account additional user input, understand the \nunderlying features, and apply those features to the generated image. Training stability refers to the model’s ability to \nreliably and smoothly converge over the training process. \nWithin the scope of generative models for image synthesis, Diffusion Models stand out for their ability to produce the \nhighest quality images, often surpassing GANs, which also generate sharp visuals but may not achieve the same level \nof detail as diffusion-based approaches. VAEs, on the other hand, tend to yield blurrier images, indicating a trade-off \nin image fidelity. When it comes to diversity, both GANs and Diffusion Models excel at generating a wide variety of \noutputs, while VAEs can struggle with high variability, limiting their effectiveness in certain applications. In terms \nof controllability, Diffusion Models offer the most significant level of control over the generation process, allowing \nfor precise adjustments, whereas GANs provide moderate to high control that can vary based on specific architectural \nchoices. VAEs, however, exhibit limited controllability, making them less suitable for applications requiring fine-tuned \nimage generation. Lastly, in terms of training stability, VAEs and Diffusion Models are generally more stable during \nthe training process, reducing the likelihood of issues, while GANs often face challenges related to instability and \nmode collapse, which can hinder their performance and diversity[44]. Table 1 summarizes aspects about image quality, \ndiversity, controllability and training stability. \nScientific images can be detailed and high-resolution as many of them are acquired using advanced instruments, e.g., \nmicroscopes. In order to generate valuable synthetic images to augment scientific datasets, image quality is expected \nto be higher than in other domains, such as art. For example, MRI scans of human brains must both be detailed and \nexpressly go through the HIPAA guidelines. Being able to generate synthetic MRI brains scans represent an invaluable \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n12 \n \n \nTable 1: Comparison of VAEs, GANs, and Diffusion Models for Text-to-Image Generation \n \nModel Type \nImage Quality \nDiversity \nControllability \nTraining Stability \nVariational \nModerate  to  High: \nModerate: Capable of \nModerate: Can con- \nHigh: \nMore stable \nAutoencoders \nGenerally  produces \ngenerating diverse im- \ndition on text embed- \nduring training com- \n(VAEs) \nimages  with  good \nages but may struggle \ndings but lacks fine- \npared to GANs, but \nquality but can be \nwith high variability \ngrained control over \ncan suffer from is- \n \n \nblurry due to the loss \nfunction used. \nin complex datasets. \nimage features. \nsues like posterior \ncollapse. \nGenerative \nHigh: \nKnown for \nHigh: Capable of pro- \nModerate  to  High: \nModerate:  Training \nAdversarial \ngenerating sharp and \nducing a wide variety \nCan \nimplement \ncan be unstable and \nNetworks \ndetailed images, es- \nof images, especially \nvarious \ncondition- \nsensitive to hyperpa- \n(GANs) \npecially  with  tech- \nwith diverse training \ning  methods  (e.g., \nrameters; mode col- \nniques like Progres- \nsive Growing GANs. \ndata. \ntext-to-image) \nbut \nmay require complex \nlapse can occur, lead- \ning to reduced diver- \n \n \narchitectures \nfor \nprecise control. \nsity. \nDiffusion \nVery High: Achieves \nHigh: Generates di- \nHigh: \nAllows for \nHigh: \nGenerally \nModels \nstate-of-the-art image \nverse images effec- \nmore explicit control \nmore \nstable \nthan \nquality, often surpass- \ntively, with the poten- \nover the generation \nGANs during train- \n \n \ning GANs and VAEs \ntial for high variabil- \nprocess through iter- \ning, with well-defined \n \nin realism and detail. \nity. \native denoising steps \nand conditioning. \ntraining \nobjectives \nthat reduce issues \nlike mode collapse. \n \n \nopportunity to create a more diverse datasets from a few “approved” images, which could be used by researchers to \ntrain models[4][27][22]. The challenge with scientific image generation lies in the Controllability or controlling their \ngeneration since pre-existent models are typically trained on data dissimilar to specialized imagery like microscopy data. \nSo if you tried to just condition on a single cross-section on a standard GAN or Diffusion Model the results are likely \nlackluster. Alternatively, training a model from scratch would require a large dataset, which is actually the motivation \nfor using image generation in the first place. Gathering sufficient amounts of data coming from experimental settings is \noften difficulty, and sometimes impossible, but without the sufficient quantity to reach convergence during training, the \nmodels can be useless. Considering the aforementioned strenghts, Diffusion Models are expected to exhibit optima \nperformance in the synthesis of scientific imagery, as they address each of these criteria. \n \n \n5 Verification & Validation \n \nHallucinations and unexpected outcomes are some of the issues associated with GenAI. Other problems include inherent \nbiases within training datasets can skew the generated images, reinforcing existing misconceptions [19] or overlooking \nimportant, yet underrepresented, scientific phenomena. Validation becomes exceptionally difficult when dealing with \ncompletely novel scenarios, as there may be no existing experimental or observational data for comparison. This \nlack of ground truth poses risks of generating misleading visualizations that could inadvertently guide research down \nunproductive paths, therefore only rigorous scrutiny and expert validation could potentially mitigate these risks [20] \nVerification and validation (V&V) are essential for establishing the reliability and accuracy of AI generative models, \nand several efforts have focused on creating standardized benchmarks [13], however curated datasets using scientific \nimaging are either extremely narrow [31] or sparse [42]. Verification assesses a model’s adherence to specified \nrequirements and its performance under defined conditions. This includes unit testing for component correctness and \nperformance evaluation against benchmark datasets. Cross-validation further examines the predictive performance \nacross data subsets, indicating robustness. Validation determines whether the model accurately reflects real-world \nphenomena. In scientific imaging, validation involves qualitative expert (domain scientist) evaluations of generated \nimage realism and quantitative metrics such as structural similarity index (SSIM) or peak signal-to-noise ratio (PSNR) \nfor image quality. Incorporating domain-specific knowledge strengthens reliability. For example, in biological or \nmaterial sciences imaging, comparisons against existing scientific models and datasets ensure generated outputs are \nboth visually and scientifically sound. Through rigorous V&V, researchers can avoid major pitfalls of generative AI \nmodels, and potentially model utilization in critical scientific applications. \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n13 \n \n \n6 Conclusion & Future Directions \nThe future of text-to-image and image-to-image technologies promises significant advancements, with profound \nimplications across diverse fields, notably scientific data analysis. We can anticipate continuous refinements in diffusion \nmodels, leading to hyper-realistic image generation coupled with increasingly granular control over specific attributes \nand detail. AI models will develop a deeper comprehension of contextual relationships, enabling the production of \nmore nuanced and precise visual representations. Additionally, ongoing optimization of algorithms and hardware will \nyield faster generation times and reduced computational costs, while cloud-based platforms and mobile applications \ncould democratize access to these technologies. A significant trend is the rapid progression of light-weight multimodal \nmodels [13, 2], with expectations of substantial improvements in quality and coherence, particularly taking advantage \nof high-performance computer systems. Finally, AI will increasingly personalize image generation, learning individual \nuser preferences to produce highly tailored visual outputs. \nThe impact of these technologies on scientific data analysis, particularly with scarce image sets from specialized \ninstruments, will be transformative. AI-driven data augmentation promises to enable the generation of synthetic \ndata to supplement limited datasets, enhancing the training of machine learning models for critical tasks like image \nsegmentation and object detection. Moreover, AI will translate abstract scientific data into intuitive visual representations, \nfacilitating the identification of patterns and trends in fields like genomics and materials science. By generating visual \nrepresentations of potential scenarios, AI will assist scientists in formulating hypotheses and designing experiments, \nsuch as simulating molecular interactions or astronomical phenomena. AI can also be employed to identify and rectify \nerrors in scientific images, improving the accuracy and reliability of data analysis. Furthermore, AI will foster increased \ncollaboration by creating easily understandable visual representations of data for diverse scientific audiences. \nDespite the immense potential, challenges remain. AI models can inherit biases from training data, leading to inaccurate \nresults, which requires careful attention to dataset representativeness. The “black box” nature of some AI models poses \nchallenges to interpretability, requiring efforts to develop more transparent models for scientific applications. Crucially, \nvalidation of AI-generated results against experimental data and established scientific principles is essential, especially \nwhen dealing with scarce datasets, to ensure the responsible and effective application of these powerful tools. \n \n7 Disclosure \nThis article incorporates text and table generation facilitated by generative artificial intelligence. Although these tools \naided in the organization and presentation of information, the authors retain full responsibility for the accuracy and \nscientific validity of the content. This article is a working in progress and further version will be shared shortly. \n \n8 Acknowledgments \nThis work was supported by the US Department of Energy (DOE) Office of Science Advanced Scientific Computing \nResearch (ASCR) and Basic Energy Sciences (BES) under Contract No. DE-AC02-05CH11231 to the Center for \nAdvanced Mathematics for Energy Research Applications (CAMERA) program. It also included partial support from \nthe DOE ASCR-funded project Autonomous Solutions for Computational Research with Immersive Browsing & \nExploration (ASCRIBE) and Laboratory Directed Research & Development (LDRD) Program and the project Analytics \nthrough Diffusion Transformer Models (ADTM) for Scientific Image and Text. \n \nCompeting interests \nThe authors declare that they have no competing interests. \n \nAdditional information \nCorrespondence and material requests should be addressed to D.U. \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n14 \n \n \nReferences \n[1] James Betker, Gabriel Goh, Li Jing, † TimBrooks, Jianfeng Wang, Linjie Li, † LongOuyang, † JuntangZhuang, † \nJoyceLee, † YufeiGuo, † WesamManassra, † PrafullaDhariwal, † CaseyChu, † YunxinJiao, and Aditya Ramesh. \nImproving image generation with better captions. 2023. \n[2] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Moham- \nmad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint \narXiv:2401.03382, 2024. \n[3] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In M. Ranzato, \nA. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information \nProcessing Systems, volume 34, pages 8780–8794. Curran Associates, Inc., 2021. \n[4] Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, and Furen Xiao. Conditional diffusion models for \nsemantic 3d brain mri synthesis. IEEE Journal of Biomedical and Health Informatics, 28(7):4084–4093, 2024. \n[5] David Foster. Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play. O’Reilly \nMedia, 2nd edition, 2023. \n[6] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Greater creative \ncontrol for ai image generation, Jul 2022. \n[7] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar \nShah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image \nconditioning, 2024. \n[8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. \n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, \nand Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and \nK.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, \nInc., 2014. \n[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben \nPoole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation \nwith diffusion models, 2022. \n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, \nR. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, \npages 6840–6851. Curran Associates, Inc., 2020. \n[12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. \n[13] Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj \nJoshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, \nUdi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph \nAuer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, \nLuis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Nikolaos Livathinos, \nNimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira de Lima, Rameswar Panda, Sivan Doveh, Shubham \nGupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, \nDerek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Shila Ofek- \nkoifman, Sriram Raghavan, Tanveer Syeda-Mahmood, Peter Staar, Tal Drory, and Rogerio Feris. Granite \nvision: a lightweight, open-source multimodal model for enterprise intelligence. Preprint, 2024. Available at: \nhttps://arxiv.org/abs/2502.09927. \n[14] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and \nimproving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition (CVPR), June 2020. \n[15] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022. \n[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, \nSpencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. \narXiv:2304.02643, 2023. \n[17] Narine Kokhlikyan, Bargav Jayaraman, Florian Bordes, Chuan Guo, and Kamalika Chaudhuri. Emu: Enhancing \nimage generation models using photogenic needles in a haystack, Sep 2023. \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n15 \n \n \n[18] Akshay Kulkarni, Adarsha Shivananda, Anoosh Kulkarni, and Dilip Gudivada. Diffusion Model and Generative \nAI for Images, pages 155–177. Apress, 2023. \n[19] Jason S. Lucas, Barani Maung Maung, Maryam Tabar, Keegan McBride, and Dongwon Lee. The longtail impact \nof generative ai on disinformation: Harmonizing dichotomous perspectives. IEEE Intelligent Systems, 39(5):12–19, \n2024. \n[20] Negar Maleki, Balaji Padmanabhan, and Kaushik Dutta. Ai hallucinations: A misnomer worth clarifying. In 2024 \nIEEE Conference on Artificial Intelligence (CAI), pages 133–138, 2024. \n[21] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets, 2014. \n[22] Puria Azadi Moghadam, Sanne Van Dalen, Karina C. Martin, Jochen Lennerz, Stephen Yip, Hossein Farahani, \nand Ali Bashashati. A morphology focused diffusion probabilistic model for synthesis of histopathology images, \n2022. \n[23] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and \ndiversity metrics for generative models. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th \nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages \n7176–7185. PMLR, 13–18 Jul 2020. \n[24] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Proceedings of the \n38th International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning \nResearch, pages 8162–8171. PMLR, 2021. \n[25] OpenAI. Clip: Connecting text and images. https://openai.com/index/clip/, 2021. Accessed: 02/27/2025. \n[26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF \nInternational Conference on Computer Vision (ICCV), pages 4195–4205, October 2023. \n[27] Walter H. L. Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F. Da Costa, Virginia Fernandez, Parashkev \nNachev, Sebastien Ourselin, and M. Jorge Cardoso. Brain imaging generation with latent diffusion models. In \nAnirban Mukhopadhyay, Ilkay Oksuz, Sandy Engelhardt, Dajiang Zhu, and Yixuan Yuan, editors, Deep Generative \nModels, pages 117–126, Cham, 2022. Springer Nature Switzerland. \n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, \nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual \nmodels from natural language supervision, 2021. \n[29] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional \ngenerative adversarial networks, 2016. \n[30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya \nSutskever. Zero-shot text-to-image generation, 2021. \n[31] Mariana T. Rezende, Raniere Silva, Fagner de O. Bernardo, Alessandra H. G. Tobias, Paulo H. C. Oliveira, \nTales M. Machado, Caio S. Costa, Fatima N. S. Medeiros, Daniela M. Ushizima, Claudia M. Carneiro, and Andrea \nG. C. Bianchi. Cric searchable image database as a public platform for conventional pap smear cytology data. \nNature Scientific Data, 8(1):151, 2021. \n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution \nImage Synthesis with Latent Diffusion Models . In 2022 IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR), pages 10674–10685, Los Alamitos, CA, USA, June 2022. IEEE Computer Society. \n[33] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv \nTaigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition (CVPR), pages 8871–8879, June 2024. \n[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using \nnonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International \nConference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256–2265, \nLille, France, 07–09 Jul 2015. PMLR. \n[35] Yang Song and Stefano Ermon. Score-based generative modeling through stochastic differential equations. In \nProceedings of the International Conference on Learning Representations (ICLR), 2021. \n[36] Jared Spataro. Introducing microsoft 365 copilot – your copilot for work, Mar 2023. \n[37] Yujie Sun, Dongfang Sheng, Zihan Zhou, and Yifei Wu. Ai hallucination: towards a comprehensive classification of \ndistorted information in artificial intelligence-generated content. Humanities and Social Sciences Communications, \n11(1):1278, 2024. \n\n\nA PREPRINT - FEBRUARY 28, 2025 \n16 \n \n \n[38] Alan D. Thompson. Life.architect, 2024. \n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, \nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, \nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran \nAssociates, Inc., 2017. \n[40] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: \nFine-grained text to image generation with attentional generative adversarial networks. In 2018 IEEE/CVF \nConference on Computer Vision and Pattern Recognition, pages 1316–1324, 2018. \n[41] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. \nStackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In 2017 IEEE \nInternational Conference on Computer Vision (ICCV), pages 5908–5916, 2017. \n[42] Kai Zhao, Sheng Di, Xin Lian, Sihuan Li, Dingwen Tao, Julie Bessac, Zizhong Chen, and Franck Cappello. \nSdrbench: Scientific data reduction benchmark for lossy compressors. In 2020 IEEE International Conference on \nBig Data (Big Data), pages 2716–2724, 2020. \n[43] Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, Cèsar Ferri, and José Hernández- \nOrallo. Larger and more instructable language models become less reliable. Nature, 634(8032):61–68, 2024. \n[44] Jingyuan Zhu, Huimin Ma, Jiansheng Chen, and Jian Yuan. High-quality and diverse few-shot image generation \nvia masked discrimination. IEEE Transactions on Image Processing, 33:2950–2965, 2024. \n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21151v1.pdf",
    "total_pages": 16,
    "title": "A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images",
    "authors": [
      "Zineb Sordo",
      "Eric Chagnon",
      "Daniela Ushizima"
    ],
    "abstract": "This review surveys the state-of-the-art in text-to-image and image-to-image\ngeneration within the scope of generative AI. We provide a comparative analysis\nof three prominent architectures: Variational Autoencoders, Generative\nAdversarial Networks and Diffusion Models. For each, we elucidate core\nconcepts, architectural innovations, and practical strengths and limitations,\nparticularly for scientific image understanding. Finally, we discuss critical\nopen challenges and potential future research directions in this rapidly\nevolving field.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}