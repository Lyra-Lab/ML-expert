{
  "id": "arxiv_2502.21274v1",
  "text": "BAnG: Bidirectional Anchored Generation for Conditional RNA Design\nRoman Klypa 1 Alberto Bietti 2 Sergei Grudinin 1\nAbstract\nDesigning RNA molecules that interact with spe-\ncific proteins is a critical challenge in experimen-\ntal and computational biology. Existing compu-\ntational approaches require a substantial amount\nof experimentally determined RNA sequences for\neach specific protein or a detailed knowledge of\nRNA structure, restricting their utility in prac-\ntice. To address this limitation, we develop RNA-\nBAnG, a deep learning-based model designed\nto generate RNA sequences for protein interac-\ntions without these requirements. Central to our\napproach is a novel generative method, Bidirec-\ntional Anchored Generation (BAnG), which lever-\nages the observation that protein-binding RNA\nsequences often contain functional binding mo-\ntifs embedded within broader sequence contexts.\nWe first validate our method on generic synthetic\ntasks involving similar localized motifs to those\nappearing in RNAs, demonstrating its benefits\nover existing generative approaches. We then eval-\nuate our model on biological sequences, showing\nits effectiveness for conditional RNA sequence\ndesign given a binding protein.\n1. Introduction\nDeep learning has significantly advanced bioinformatics\nand structural biology, particularly in predicting the struc-\ntures, interactions, and functions of biomolecules (Callaway,\n2024). It has also improved the efficiency of macromolec-\nular design, facilitating applications in drug discovery and\nsynthetic biology. In particular, significant progress has\nbeen made in protein sequence design. Some illustrative ex-\namples include ESM3 (Hayes et al., 2024) and Chroma (In-\ngraham et al., 2023). This remarkable progress has not only\nrevolutionized protein design but also opened new opportu-\n1 Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000\nGrenoble, France. 2Center for Computational Mathematics, Flat-\niron Institute, 162 5th Ave, New York, NY 10010, USA. Correspon-\ndence to: Roman Klypa <roman.klypa@univ-grenoble-alpes.fr>,\nAlberto Bietti <abietti@flatironinstitute.org>, Sergei Grudinin\n<sergei.grudinin@univ-grenoble-alpes.fr>.\nnities for addressing other complex biomolecular challenges.\nOne such area is RNA generation, where similar principles\nof leveraging deep learning can be applied to advance our\nunderstanding and design of functional RNA sequences.\nAmong the many challenges in RNA design, generating\nRNA sequences capable of binding to specific proteins\nstands out as a critical task with significant implications\nfor understanding RNA-protein interactions (Li et al., 2024;\nFasogbon et al., 2024). These interactions, central to essen-\ntial biological processes such as gene regulation, splicing,\nand translation (Hentze et al., 2018), highlight the need for\nprecisely engineered RNA molecules. A notable example\nof such molecules are aptamers, short single-stranded RNA\nsequences that bind to specific proteins with high affinity\nand specificity. By acting as molecular inhibitors, probes,\nor delivery agents, aptamers offer versatile applications in\ntherapeutics and diagnostics (Guo et al., 2010; Thavarajah\net al., 2021). Traditionally, aptamers are identified through\nSELEX (Systematic Evolution of Ligands by Exponential\nEnrichment), a labor-intensive experimental process. De-\nveloping computational methods to design aptamers could\nsignificantly accelerate and simplify their discovery, expand-\ning their potential in biomedical applications.\nSeveral studies have explored RNA generation in this do-\nmain. More classical approaches exploited evolutionary\nsignals and statistical models (Kim et al., 2007a;b; Aita &\nHusimi, 2010; Tseng et al., 2011; Zhang et al., 2023), molec-\nular modeling (Torkamanian-Afshar et al., 2021), and Monte\nCarlo tree search (Lee et al., 2021; Wang et al., 2022; Shin\net al., 2023; Obonyo et al., 2024). More recent works used\nconditional variation autoencoders (Chen et al., 2022; Iwano\net al., 2022; Andress et al., 2023), long short-term memory\nmodels (Im et al., 2019; Park & Han, 2020), transformer-\nbased architectures (Zhao et al., 2024; Zhang et al., 2024),\nand adversarial approach (Ozden et al., 2023). Most recent\nstudies, e.g., AptaDiff (Wang et al., 2024), or RNAFLOW\n(Nori & Jin, 2024) also explored diffusion processes and\nflow matching. Almost all of the aforementioned approaches\ndepend on a vast collection of nucleotide sequences known\nto interact with proteins to generate new ones, limiting their\napplicability to proteins for which extensive experimental\ndata is available. To the best of our knowledge, RNAFLOW\nstands out as the only method that has not been trained on\nRNA affinity experimental data. However, it relies on RNA\n1\narXiv:2502.21274v1  [cs.LG]  28 Feb 2025\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nstructure prediction tools to guide RNA design. Since these\ntools often lack the accuracy needed for precise RNA struc-\ntures (Rhiju et al., 2024), they ultimately reduce the model’s\neffectiveness in many practical scenarios.\nIn this work, we present a novel generative method com-\nbined with a deep-learning model that operates without rely-\ning on specific experimental data for the target protein and\ndoes not depend on RNA structural information. This ap-\nproach enables a broader applicability and greater efficiency\nin RNA sequence generation than the ones mentioned above.\nThe motivation for the design of the proposed generative\nmethod stems from two key observations. First, the total\nlength of the RNA sequence to be generated is often un-\nknown. Therefore, the method adopts an autoregressive\napproach. Second, RNA sequences that interact with pro-\nteins typically contain functional binding motifs —- specific\nregions that mediate interaction by forming molecular con-\ntacts with the protein. These binding motifs are embedded\nwithin larger sequence contexts, where the surrounding non-\nbinding regions exert lesser influence on binding specificity.\nThis makes it more effective to initiate sequence generation\nfrom the binding motif, rather than from the sequence’s\nends, as is commonly done in current state-of-the-art NLP\nautoregressive models. These are the core ideas behind our\nmethod, Bidirectional ANchored Generation (BAnG).\nThe model, named RNA-BAnG, is based on a transformer\narchitecture with geometric attention. The latter allows for\nthe incorporation of protein structural information, which is\ncrucial for predicting RNA-protein interactions. By utilizing\nAlphaFold2 (Jumper et al., 2021), a state-of-the-art protein\nstructure prediction tool, we can obtain highly accurate\nstructural data, even if the target protein is not solved ex-\nperimentally. The resulting combination of the RNA-BAnG\nmodel and the generative method, schematically illustrated\nin Figure 1, produces RNA sequences that interact with\na given protein, utilizing both its sequence and structural\ninformation. Our main contributions can be summarized\nas follows:\n1. We propose a new bidirectional generation method,\nBAnG, along with a transformer-based architecture,\nRNA-BAnG, that are well-suited for RNA generation\nconditioned on a binding protein.\n2. We thoroughly validate the effectiveness of our method\non relevant synthetic tasks and compare it with other\nwidely used sequence generation methods.\n3. We evaluate our approach on experimental RNA-\nprotein interaction data, showing promising results that\noutperform previous methods.\nTarget Protein\nRNA-BAnG\nRNA Sequences\nFigure 1. Schematic illustration of the RNA-BAnG generative pro-\ncess and its conditioning on the input protein 3D structure. The\nprotein model was generated by AlphaFold2 and is colored by the\nsecondary structure. RNA sequences are colored by nucleotides.\n<ancl><ancr>\nG<ancl><ancr>A\nUG<ancl><ancr>A<eos>\n<eos>UG<ancl><ancr>A<eos>\nUGA\n<ancl><ancr>A\nG<ancl><ancr>A<eos>\n1\n2\n3\n4\n5\n=\nFigure 2. A step-by-step example process of RNA sequence ’UGA’\ngeneration.\n2. Bidirectional anchored generation for RNA\nIn this section, we present our generative modeling ap-\nproach, BAnG, and its application to conditional RNA se-\nquence prediction through the RNA-BAnG model.\n2.1. Description of the BAnG generative approach\nIn the BAnG framework, we leverage the following fac-\ntorization of the joint distribution over a sequence x =\n(x−n−1, ..., x0, ..., xn):\nP(x) = P(x0)\n×\nn\nY\ni=0\nP(x−i−1|x−i...i)\n|\n{z\n}\nleft tokens\nn\nY\ni=1\nP(xi|x−i...i−1)\n|\n{z\n}\nright tokens\n, (1)\nwhere x0 is the first generated token. Concretely, sequence\ngeneration begins with two special anchor tokens, <ancl>\nand <ancr>, representing the left and right boundaries of\nthe sequence, respectively. Tokens are then generated one\nat a time, alternating between directions: we first sample a\ntoken on the right, then a token on the left, and repeat this\n2\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\n-3\n-2\n-1\n-1\n0\n0\n1\nToken index\n<eos>\nU\nG\n<ancl>\n<ancr>\nA\n<eos>\nToken\n<eos>\nU\nG\nA\n<eos>\n<ancl><ancr>\nFigure 3. BAnG attention mask. Anchor tokens are indexed in\na way to preserve relative distances in a sequence after anchors\ninsertion.\nprocess to progressively extend the sequence outward. At\neach step, the distribution of the next token is conditioned on\nalready generated ones, following (1). If an end-of-sequence\n(<eos>) token is generated in either direction, no further\ntokens are produced along that axis. The generation process\nstops when <eos> tokens are produced for both boundaries\nor when a predefined maximum sequence length is reached\n(see Figure 2).\nBAnG enables training a deep learning model to estimate the\nconditional distributions in (1) in order to perform bidirec-\ntional generation in the described manner, using a process\nanalogous to autoregressive training. The key difference\nin the architecture compared to the standard autoregressive\ncase is the replacement of the conventional lower triangular\nattention mask with a specifically designed bidirectional at-\ntention mask, shown in Figure 3. This custom mask ensures\nthat any representation of a given token cannot depend on\ntokens beyond those appearing in the corresponding condi-\ntional in (1). This simple modification allows the model to\nlearn the dependencies needed for our generation strategy,\nwhile ensuring efficient parallelization of the forward and\nbackward passes across all tokens in a sequence during train-\ning. During model training and inference, the probabilities\nfor the next token in each direction are derived from the\nembedding of the most recently generated token in the same\ndirection, as shown schematically in Figure 4.\nWe shall emphasize that the single pass training with the\nconditional factorization in (1) is only possible thanks to\nthe introduction of two anchor tokens. Indeed, if only a\nsingle anchor token was used, it would be responsible for\npredicting the first tokens in both directions, right and left.\nTo prevent information leakage, its attention would be re-\n<eos>\nU\nG\n<ancl>\n<ancr>\nA\n<eos>\nFigure 4. Schematic illustration of the masked attention mecha-\nnism and token probabilities derivation. Bold lines indicate the\ntoken embeddings from which the probabilities for each token are\nderived, while the dotted lines represent the tokens to which the\ngiven token’s attention is directed.\nstricted to itself, which would make the prediction of the\nleft token independent of the right one. This lack of condi-\ntioning could lead to the generation of incompatible token\npairs.\n2.2. Model\nRNA-BAnG architecture consists of two main components:\na protein module and a nucleotide module. The protein\nmodule derives a representation from the protein’s sequence\nand structure, while the nucleotide module generates a nu-\ncleotide sequence conditioned on this representation.\nSelf Attention\nGeometric Attention\nFeedforward\nSelf Attention\nCross Attention\nFeedforward\nNucleotide embedder\nProtein embedder\nPrediction head\ntimes\nFigure 5. Schematic illustration of the RNA-BAnG architecture.\nThe protein component is on the left, the nucleotide component is\non the right. More details can be found in Appendix A.\nOur model’s modules comprise several main blocks - Em-\nbedder, Self Attention, Geometric Attention, and Cross\nAttention, schematically illustrated in Figure 5. The Em-\nbedder block generates token embeddings from protein,\nRNA, and DNA sequences. The inclusion of RNA and\nDNA sequences in the training data serves to augment the\ndataset, as it can help the model learn the shared patterns.\n3\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nThe Embedder block processes amino acids with one-hot\nencoding residue types (20 canonical and a padding token),\nfollowed by a linear transformation and normalization of\nthe resulting embeddings. For nucleic acids, it encodes\nsequence information in two steps. First, it one-hot en-\ncodes residue types, using tokens for the four standard RNA\nnucleotides, along with two anchor tokens, <eos> token,\nand a padding (<pad>) token. DNA residues are treated\nas RNA equivalents, with thymine (T) replaced by uracil\n(U). Second, the sequence type (DNA or RNA) is one-hot\nencoded, linearly transformed, and concatenated with the\nresidue embeddings. This combined representation is then\npassed through a normalization layer.\nSelf-attention in the model was implemented in a classical\nway (Vaswani et al., 2023) using Rotary Position Embed-\nding (RoPE) (Su et al., 2023). While RoPE has recently\nbeen predominantly used in autoregressive models, it was\noriginally introduced for bidirectional transformers, making\nit particularly suitable for our case. Self-attention for protein\nsequences uses unmasked attention, while for nucleotide\nsequences, the BAnG mask is applied.\nCross-attention is implemented similarly to the self-\nattention mechanism, with one key difference: instead of\nusing RoPE, sinusoidal positional embeddings (Vaswani\net al., 2023) are applied to the nucleotide sequence. Specifi-\ncally, we encoded the positions of nucleotide tokens relative\nto the anchor ones, where the anchor token <ancl> is as-\nsigned an index of zero. This choice was made because\nresidues in different chains do not have relative sequential\ndistances, but we still want to include nucleotide position\ninformation in the calculation of its attention to the protein.\nGeometric Attention aims to incorporate protein structural\ninformation. To achieve this, we adopted the protein repre-\nsentation introduced in AlphaFold2 (Jumper et al., 2021).\nSpecifically, for each protein residue, a rigid frame T is\nconstructed based on the coordinates of its C, Cα, and N\nbackbone atoms. Attention between these frames is then\ncalculated using the geometric part from AlphaFold2’s In-\nvariant Point Attention (IPA) mechanism (more details in\nAppendix A). The importance of this module is demon-\nstrated by the fact that RNA-BAnG fails to converge in\nits absence.\nWe use GELU (Hendrycks & Gimpel, 2016) as the activa-\ntion function in the Feed Forward layers. For normalization,\nwe employed RMSNorm (Zhang & Sennrich, 2019). The\nnumber of attention heads is consistent across all attention\nblocks, and the dimension of each head is set independently\nof other model parameters. In both self-attention and cross-\nattention mechanisms, keys and queries are normalized prior\nto multiplication to prevent the attention-logit growth in-\nstability (Dehghani et al., 2023). The specific choice of\nhyperparameters is described in Appendix A.\n3. Validating BAnG on a synthetic task\nIn this section, we evaluate the effectiveness of our BAnG\nstrategy through a thorough analysis on a relevant synthetic\ntask involving distinct subsequences.\nDescription of the task.\nTo evaluate the BAnG method,\nwe first consider a synthetic task that emulates a real-world\nconditional generation scenario. As mentioned above, RNA\nsequences often include a functional binding motif within a\nbroader sequence context, where non-binding regions con-\ntribute minimally to binding specificity (Ray et al., 2013).\nBased on this observation, the task’s objective is to gener-\nate sequences that contain a predefined short subsequence,\nsynthetic motif.\nSynthetic data.\nThe synthetic data consists of nucleotide\nsequences, each 50 residues long, with a synthetic motif\nplaced at a random position. The remaining residues are\nuniformly distributed. We fixed two different random sub-\nsequences of length 6 as the synthetic motifs. The exact\ncontent of these motifs is not crucial to the task, as any\npossible subsequences of this length are equally likely to be\nuniformly sampled. The length of six was chosen because it\nclosely matches the size of real binding motifs (Ray et al.,\n2013) and reduces the likelihood of random occurrences\nof such subsequences. As the anchor point for BAnG, we\nchose the center of the synthetic motif.\nIn the SingleBind training setup, sequences contained only\nthe first synthetic motif, while in the DoubleBind setup,\nsequences could contain either the first or the second syn-\nthetic motif with equal probability. The objective of the\nSingleBind setup was to compare generative methods on a\nsimpler task, while the goal of DoubleBind was to assess\ntheir performance under more realistic, thus more uncer-\ntain conditions.\nReference methods.\nWe compare our method to exist-\ning generative approaches, including autoregressive genera-\ntion, which is commonly used in natural language process-\ning (Bengio et al., 2000), and iterative generation methods\nbased on masked language modeling, as implemented in\nESM3 (Hayes et al., 2024). Also, as the simplest base-\nline, we include in the comparison set of random sequences,\nwhere each token is sampled from a uniform distribution.\nFor the autoregressive approach, we trained the model using\na lower triangular attention mask. In the iterative approach,\nthe model was trained with a demasking objective and a\nhigh masking rate of 50%, which, according to the ESM3\nauthors, has been shown to yield effective generation results.\nAdditional model training details for each tested method\ncan be found in Appendix B.1.\nFor the autoregressive approach, tokens are generated se-\n4\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nTable 1. Comparison of generative methods on the SingleBind and\nDoubleBind tasks. Values represent the proportion of sequences\nthat contain the correct synthetic motif.\nGENERATIVE METHOD\nSINGLEBIND ↑\nDOUBLEBIND ↑\nBANG\n0.98\n0.97\nAUTOREGRESSIVE\n0.94\n0.53\nIANG ENTROPY\n0.91\n0.54\nIANG LOGIT MAX\n0.89\n0.54\nITERATIVE ENTROPY\n0.06\n0.04\nITERATIVE LOGIT MAX\n0.04\n0.05\nRANDOM SEQUENCES\n0.01\n0.02\nquentially, one at a time, starting from the start-of-sequence\n(<sos>) token and continuing until the <eos> token is\nproduced. In the iterative approach, however, all tokens but\n<sos> and <eos> are initially masked. Tokens are then\nunmasked one by one, with the next token to unmask chosen\nbased on either the largest logit value (max logit decoding)\nor the smallest entropy (entropy decoding). Sampling details\nmay be found in Appendix B.2.\nWe also introduced a modification of iterative methods,\nbetter suited for the task - Iterative Anchored Generation\n(IAnG). This approach merges BAnG with iterative methods\nby incorporating an anchor token (<anc>) placed in the\nmiddle of the synthetic motif. In this method, the anchor\ntoken remains unmasked during training. At the start of the\ninference, the anchor token is positioned at a random loca-\ntion within the sequence and remains unmasked throughout\nthe process.\nEvaluation results.\nWith each tested approach we gener-\nated 1,000 sequences. Table 1 summarizes the performance\nof each method. Additional statistics on the frequency of\neach synthetic motif in the generated data for DoubleBind\ntask are provided in Table 2. Examples of generated se-\nquences can be found in Appendix B.3.\nThe tables show that BAnG outperforms other methods,\nwith a particularly notable margin on the DoubleBind task.\nBAnG also generates fewer sequences containing mixed\nsynthetic motifs. The very low performance of ESM3’s iter-\native methods is expected, as the lack of absolute positional\ndependencies in the data causes the model to assign nearly\nuniform probabilities across tokens. This reasoning is fur-\nther supported by the significant performance improvement\nobserved when positional information is introduced through\nthe anchor token in IAnG. Nonetheless, both methods suffer\nfrom a key limitation: a discrepancy between the demasking\nprocess during training and inference, which undermines\ntheir overall effectiveness.\nIterative methods may demonstrate improved performance\nTable 2. Detailed statistics for the DoubleBind task: proportion of\nsequences containing either one of the synthetic motifs or both.\nGENERATIVE METHOD\nFIRST ↑\nSECOND ↑\nBOTH ↓\nBANG\n0.43\n0.53\n0.01\nAUTOREGRESSIVE\n0.17\n0.25\n0.11\nIANG LOGIT MAX\n0.09\n0.44\n0.01\nIANG ENTROPY\n0.10\n0.44\n0.01\nITERATIVE ENTROPY\n0.03\n0.02\n0.01\nITERATIVE LOGIT MAX\n0.01\n0.04\n0.01\nRANDOM SEQUENCES\n0.01\n0.01\n0\nin modality translation scenarios, such as structure-to-\nsequence generation, which is a key focus of ESM3. How-\never, they are unlikely to match the effectiveness of regres-\nsive approaches in purely generative tasks.\n4. Conditional RNA generation\nIn this section, we evaluate our RNA-BAnG modeling strat-\negy for protein-conditioned generation of RNA sequences\nbased on experimental biological data.\n4.1. Data\nWe collected our protein-nucleotide interaction data from\nthe Protein Data Bank (PDB) (Berman, 2000), utilizing in-\nformation provided in the PPI3D database (Dapk¯unas et al.,\n2024). However, we conducted independent postprocessing\nof the data, distinct from PPI3D, as they focus on structural\nRNA and DNA information, while we are concerned solely\nwith their sequences. The postprocessing steps involved\nverifying chain interactions, discarding ambiguous protein\nstructures, and excluding chains containing non-standard\nresidues, as explained in more detail in Appendix C.1. Dur-\ning training and validation, each time a sample was encoun-\ntered, its anchor point was randomly sampled from the in-\nteracting nucleotides. The anchor tokens were then inserted\nright after the selected nucleotide. Additionally, to diver-\nsify RNA sequence information, we collected non-coding\nsequences from RNAcentral (release 24), a comprehensive\ndatabase integrating RNA sequences from multiple expert\nsources (The RNAcentral Consortium et al., 2019) (more\ndetails in Appendix C.2). Since we lack information about\ntheir interactions, the anchor points for solo RNA sequences\nwere selected randomly.\n4.2. RNA-BAnG training\nWe designed the training process to consist of two steps:\nthe first for the model to learn general information about\nRNA sequences, and the second for the model to learn con-\nditioning on proteins. In both steps, the training objective\nis the cross-entropy loss between the predicted and ground\n5\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\ntruth nucleotide token probabilities (Bengio et al., 2000).\nAs the first step we train RNA-BAnG nucleotide module\nwithout cross-attention block on standalone RNA sequence\ndata from RNAcentral. During training, the loss values\nfor the four tokens closest to each anchor on either side\nare weighted at 0.01. This weighting scheme is applied\nbecause the model lacks the context needed to accurately\npredict these residues. Next, we train the full model, using\nweights from the previous step, on the combined protein-\nnucleotide sequence data, this time without applying any\nloss weighting. Additional training details can be found in\nAppendix D.\n4.3. Evaluation protocol\nTo evaluate the performance of our model, we adopted the\nscoring approach proposed by the authors of GenerRNA\n(Zhao et al., 2024) and others (Im et al., 2019). This method\nleverages DeepCLIP (Grønning et al., 2020), a state-of-the-\nart predictive model that, after being trained on examples\nof interacting and non-interacting RNA sequences for a\ngiven protein, can assign binding probabilities to any RNA\nsequence with the same protein. These probabilities serve\nas a proxy for evaluating the quality of sequences generated\nby RNA-BAnG and other methods.\nTo obtain suitable training and testing data for DeepCLIP,\nwe utilized RNAcompete experiments (Ray et al., 2009),\nconducted by the authors of the RNA Compendium (Ray\net al., 2013). Using this data, we trained DeepCLIP models\nfor each sample and selected only those that met our per-\nformance criteria (see Appendix E.1). Separately, for each\nsample, we derived 1,000 sequences identified by RNA-\ncompete as highly likely to bind to the protein (positive\nexamples) and another 1,000 sequences as highly unlikely\nto bind (negative examples), based on their experimental\naffinity scores (see Appendix E.1). We excluded these se-\nquences from the DeepCLIP model training, ensuring that\nthey served as an independent benchmark for testing the\nquality of both the DeepCLIP predictions and the RNA se-\nquences generated by RNA-BAnG. Below, we refer to them\nas to the positive and negative experimental sets.\nDue to the lack of experimentally solved protein structures\nfor the test set, and aiming for a more robust method, we\nonly used AlphaFold2 protein models for RNA-BAnG in-\nference during evaluation. Concretely, for each RNA Com-\npendium sample, we generated the corresponding three-\ndimensional protein model, and retained for further test-\ning only those with a predicted local distance difference\ntest (pLDDT) score greater than 70%. This threshold is\nwell-accepted in the community and guarantees structural\nreliability of the produced protein structures, at least at the\ndomain level. The final test set consisted of 71 samples,\nrepresenting 67 unique protein sequences.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nThreshold Value\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion Above Threshold\nFigure 6. Proportion of sequences above the threshold: generated\nby RNA-BAnG (green) and randomly (yellow), the positive (blue)\nand negative (red) experimental sets. The values here represent the\naverages for the entire test set.\nTo match the amount of RNA sequences in experimental\nsets, we generated 1,000 of them with RNA-BAnG for each\ntest sample. We also generated 1,000 random sequences to\nserve as the simplest baseline (see Appendix E.2).\n4.4. Experiment results\nTo estimate RNA-BAnG’s performance, we use the pro-\nportion of sequences with DeepCLIP scores above a given\nthreshold, rather than relying on the mean score for each\ntest sample. This approach aligns better with practical ap-\nplications, where the goal is often to maximize the number\nof sequences that meet a specific performance criterion,\nrather than optimizing for average performance across the\nentire set.\nStatistics of generated sequences.\nFigure 6 presents the\nthreshold-dependent performance curve, showing the rela-\ntionship between sequences with DeepCLIP scores above a\ncertain threshold and the threshold value. The area under the\nthreshold-dependent performance curves in Figure 6 cap-\ntures the method’s ability to generate affine sequences. A\nlarger area indicates better performance, reflecting a higher\nproportion of sequences that exceed the threshold at dif-\nferent points. A high area value of 0.88 for the positive\nexperimental set indicates that DeepCLIP is good at distin-\nguishing high-affinity sequences from others. RNA-BAnG\nshowed the value of 0.57, indicating moderate success in\ngenerating sequences that somewhat align with the positive\nset. The random set, with the area value of 0.39, suggests\nthat our model is outperforming random generation. The\nnegative set’s very low area value of 0.11 further under-\nscores the DeepCLIP good performance, also suggesting\nthat RNA-BAnG has some ability to avoid producing low-\naffinity outputs.\nAveraging across samples conceals important details about\n6\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nthe statistics of the compared sequences. For a more de-\ntailed, sample-by-sample analysis, shown in Figure 7, we\nselected a threshold value of 0.75, as DeepCLIP scores\naround 0.5 can be ambiguous in interpretation. We con-\nsider sequences with DeepCLIP scores above this value\nas high-affinity RNA sequences (haRNA). As one can see\nfrom Figure 7, RNA-BAnG generates more high-affinity\nsequences than a random generator for 56 out of 71 test\nsamples. For 33 of the test samples, more than half of the\nsequences generated by RNA-BAnG are the high-affinity\nones. Additionally, our model generated less than 20% high-\naffinity sequences for only 14 out of 71 test samples. The\nhigh success rate of randomly generated sequences for some\nsamples may be attributed to the low complexity of captured\nprotein binding motifs (Ray et al., 2013). The examples of\ngenerated sequences for some representative samples and\nindividual DeepCLIP score distributions can be found in\nAppendix F.\nThe performance of our model shows no significant corre-\nlation with the predicted quality of the protein 3D model\n(pLDDT) or with the sequence similarity of a protein target\nto the training data (Figure F.1 in Appendix F). This anal-\nysis suggests that the model does not only memorize the\ninformation, but exhibits a degree of generalization. We can\nalso conclude that the performance accuracy of RNA-BAnG\ncan not be attributed to protein structure prediction quality,\nif its pLDDT score is higher than 70%.\nNovelty and diversity of generated sequences.\nFor our\nmodel’s goal of accelerating experimental RNA design, di-\nversity and novelty are essential metrics. By generating a\nwide range of diverse sequences, the model increases the\nchances of identifying optimal binders with varying affini-\nties for the target protein, ensuring more effective candidates\nfor experimental validation. Furthermore, the generation\nof novel sequences allows the discovery of unique RNA-\nprotein interactions, which can lead to innovative therapeu-\ntic applications.\nTo quantify the diversity metric, we compute the ratio of the\nnumber of distinct clusters at a threshold of 0.9 sequence\nidentity to the total number of generated sequences. A se-\nlected threshold of 0.9 is often considered as one of the lower\nlimits used in RNA sequence clustering, especially when\naiming to identify highly similar sequences or gene families\n(Edgar, 2018). The resulting average diversity across the test\nset is 0.93 ± 0.13, indicating that the generated sequences\nare highly varied. The novelty is defined by the proportion\nof RNA sequences in the generated set that are not similar\nto the training data. We identify sequences for each test\nsample that have no similarity (see Appendix G for more\ndetails) to the training set, and the novelty is calculated as\nthe ratio of these sequences to the total number of generated\nsequences. The resulting average novelty across the test set\nTable 3. High-affinity RNA sequence proportion generated by Gen-\nerRNA and RNA-BAnG for each RNA Compendium sample. The\nprotein sequences are identical for the same genes. Although un-\nderlined samples do not meet our test set selection criteria, they\nare included to enhance the diversity of the comparison. Sample\nIDs mapping to RNAcompete IDs mentioned in Appendix E.1.\nSAMPLE ID\nGENE\nGENERRNA\nRNA-BANG\n106\nSRSF1\n0.42\n0.49\n107\nSRSF1\n0.59\n0.74\n108\nSRSF1\n0.56\n0.77\n109\nSRSF1\n0.62\n0.85\n110\nSRSF1\n0.43\n0.66\n121\nELAVL1\n0.64\n0.91\nis 0.99 ± 0.01, indicating that the generated sequences are\nhighly novel and distinct from the training data, which is a\npositive outcome for our model.\nComparison with similar models.\nWe compare our\nmodel, RNA-BAnG, with two other methods, GenerRNA\n(Zhao et al., 2024) and RNAFLOW (Nori & Jin, 2024),\nas they are the most relevant existing methods for gener-\nating RNA sequences for a diverse set of proteins. The\nfirst model, GenerRNA, leverages a substantial collection\nof RNA sequences known to interact with a specific pro-\ntein during its fine-tuning process, allowing it to generate\nadditional sequences with similar binding properties. Conse-\nquently, our comparison is limited to the proteins for which\nGenerRNA was fine-tuned by its authors. We used their\npublished inference results, removing generated RNA larger\nthan 50 nucleotides, leaving 921 and 909 sequences for the\nELAVL1 and SRSF1 proteins, respectively. These proteins\nare already present in the RNA Compendium samples. As\nshown in Table 3, our model generates more high-affinity\nRNA sequences than GenerRNA. It is important to note that\nRNA-BAnG generates these sequences without relying on\nany additional information, whereas GenerRNA required\nextensive data mining from multiple experimental studies.\nThe second baseline model is RNAFLOW, which uses RNA\nstructure prediction tools. Due to its much lower speed,\napproximately 50 times slower than RNA-BAnG, we only\ngenerated RNA sequences for proteins that had zero se-\nquence similarity with our training set, resulting in 100\nsequences of length 50 for each protein. Unfortunately,\nRNAFLOW generated sequences with an unusually high\nfrequences of G and C nucleotides — 62% and 32%, re-\nspectively — regardless of protein structure or sequence.\nAs a result, it produced high-affinity sequences for only a\ncouple of proteins, and none for the others. Consequently,\nRNA-BAnG outperformed RNAFLOW on most of the test\nsamples (Figure 8).\nThe low performance of RNAFLOW can be explained by\n7\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\n133\n167\n241\n157\n158\n236\n156\n139\n052\n178\n283\n205\n076\n263\n071\n024\n072\n069\n184\n285\n032\n008\n249\n146\n096\n043\n171\n117\n095\n029\n255\n257\n219\n274\n226\n252\n256\n258\n022\n033\n087\n246\n023\n027\n288\n136\n245\n232\n037\n049\n062\n169\n261\n253\n268\n107\n206\n269\n108\n109\n005\n165\n229\n075\n036\n091\n141\n217\n197\n173\n077\nSample ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of haRNA\nFigure 7. Proportions of high-affinity RNA sequences for each test sample, generated by RNA-BAnG (green) and randomly (yellow).\nThese are compared with the proportions in the positive (blue) and negative (red) experimental sets. Test samples are ordered by\nRNA-BAnG performance. Sample IDs mapping to RNAcompete IDs mentioned in Appendix E.1.\n158\n156\n178\n069\n008\n226\n252\n258\n027\n037\n206\n091\nSample ID\n0.0\n0.2\n0.4\n0.6\n0.8\nProportion of haRNA\nFigure 8. Proportion of high affinity sequences generated by RNA-\nBAnG (green) and RNAFLOW (yellow). Samples are sorted by\nthe performance of RNA-BAnG. Sample IDs mapping to RNA-\ncompete IDs mentioned in Appendix E.1.\na discrepancy between its training process and the infer-\nence we conducted. Concretely, RNAFLOW was trained on\nprotein sequences and structures truncated to 50 residues,\nensuring the inclusion of the protein binding site. However,\nwe tested it on proteins of varying lengths, ranging from 177\nto 589 amino acids, without highlighting their binding sites\nthrough truncation (since this information is unavailable).\nAdditionally, the proteins we tested had zero sequence simi-\nlarity to those in RNAFLOW’s training set (PDB), making\nit more challenging to predict their structure in complex\nwith RNA. These limitations significantly restrict the appli-\ncability of RNAFLOW to RNA sequence design, especially\nwhen compared to RNA-BAnG.\n5. Conclusion\nThis study introduces a novel deep-learning model, RNA-\nBAnG, and a sequence generation method, BAnG, for de-\nsigning RNA sequences that bind to a given protein. Unlike\nprevious approaches, our model demonstrates remarkable\nflexibility by eliminating the need for extensive structural\nor interaction data. Although our model relies on protein\nstructure, AlphaFold predictions make sequence informa-\ntion sufficient. This innovation significantly broadens the\napplicability of our method, making it a versatile tool for\nRNA-protein interaction studies.\nOur method is based on the observation that RNA sequences\ncontain functional binding motifs, while the surrounding\nsequence context is less critical for interaction. When evalu-\nated on a synthetic task against other sequence generation\napproaches, BAnG demonstrated superior performance. Im-\nportantly, the method’s design makes it applicable beyond\nRNA-protein interactions, extending to any scenario where\nthe focus is on optimizing functional subsequences within\na larger sequence. According to the state-of-the-art Deep-\nCLIP scoring, RNA-BAnG outperforms existing methods,\ngenerating a higher proportion of sequences with strong\npredicted binding affinity. The generated sequences exhibit\nboth diversity and novelty, expanding the range of potential\nRNA candidates for further experimental validation.\nFuture work could focus on integrating experimental feed-\nback to further refine the model, optimizing its architecture\nfor enhanced performance, and improving its usability for\nbroader applications. Additionally, experimental validation\nof the generated sequences would provide further insights\ninto the practical applicability of our method. In summary,\nour work represents a significant step forward in RNA se-\nquence generation, offering a powerful and flexible tool for\nresearchers in the field of RNA-protein interactions.\nSoftware and Data.\nThe code and the model, along with\nthe model weights, will be available upon publication.\n8\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nAcknowledgments\nThis work was granted access to the HPC resources of\nIDRIS under the allocation 2024-AD011015647 made by\nGENCI. A substantial part of the computations presented in\nthis paper was performed using the GRICAD infrastructure\n(https://gricad.univ-grenoble-alpes.fr), which is supported\nby Grenoble research communities.\nReferences\nAita, T. and Husimi, Y. Biomolecular information gained\nthrough in vitro evolution. Biophysical reviews, 2:1–11,\n2010. Publisher: Springer.\nAltschul, S. F., Gish, W., Miller, W., Myers, E. W., and\nLipman, D. J. Basic local alignment search tool. Jour-\nnal of Molecular Biology, 215(3):403–410, October\n1990. ISSN 00222836. doi: 10.1016/S0022-2836(05)\n80360-2. URL https://linkinghub.elsevier.\ncom/retrieve/pii/S0022283605803602.\nAndress, C., Kappel, K., Villena, M. E., Cuperlovic-\nCulf, M., Yan, H., and Li, Y.\nDAPTEV: Deep ap-\ntamer evolutionary modelling for COVID-19 drug de-\nsign. PLOS Computational Biology, 19(7):e1010774,\nJuly 2023.\nISSN 1553-7358.\ndoi: 10.1371/journal.\npcbi.1010774.\nURL https://dx.plos.org/10.\n1371/journal.pcbi.1010774.\nBengio, Y., Ducharme, R., and Vincent, P.\nA Neural\nProbabilistic Language Model. In Leen, T., Dietterich,\nT., and Tresp, V. (eds.), Advances in Neural Infor-\nmation Processing Systems, volume 13. MIT Press,\n2000.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2000/file/\n728f206c2a01bf572b5940d7d9a8fa4c-Paper.\npdf.\nBerman, H. M.\nThe Protein Data Bank.\nNucleic\nAcids Research, 28(1):235–242, January 2000. ISSN\n13624962. doi: 10.1093/nar/28.1.235. URL https://\nacademic.oup.com/nar/article-lookup/\ndoi/10.1093/nar/28.1.235.\nCallaway,\nE.\nChemistry Nobel goes to develop-\ners of AlphaFold AI that predicts protein struc-\ntures.\nNature, 634(8034):525–526, October 2024.\nISSN\n0028-0836,\n1476-4687.\ndoi:\n10.1038/\nd41586-024-03214-7. URL https://www.nature.\ncom/articles/d41586-024-03214-7.\nChen, J. C., Chen, J. P., Shen, M. W., Wornow, M., Bae,\nM., Yeh, W.-H., Hsu, A., and Liu, D. R.\nGenerat-\ning experimentally unrelated target molecule-binding\nhighly functionalized nucleic-acid polymers using ma-\nchine learning.\nNature Communications, 13(1):4541,\nAugust 2022.\nISSN 2041-1723.\ndoi:\n10.1038/\ns41467-022-31955-4. URL https://www.nature.\ncom/articles/s41467-022-31955-4.\nDapk¯unas, J., Timinskas, A., Olechnoviˇc, K., Tomku-\nvien˙e, M., and Venclovas, C.\nPPI3D: a web server\nfor searching, analyzing and modeling protein–protein,\nprotein–peptide and protein–nucleic acid interactions.\nNucleic Acids Research, 52(W1):W264–W271, July\n2024.\nISSN 0305-1048, 1362-4962.\ndoi: 10.1093/\nnar/gkae278. URL https://academic.oup.com/\nnar/article/52/W1/W264/7645776.\nDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,\nHeek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R.,\nAlabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen,\nM., Arnab, A., Wang, X., Riquelme, C., Minderer, M.,\nPuigcerver, J., Evci, U., Kumar, M., Steenkiste, S. v.,\nElsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot,\nF., Bastings, J., Collier, M. P., Gritsenko, A., Birodkar,\nV., Vasconcelos, C., Tay, Y., Mensink, T., Kolesnikov,\nA., Paveti´c, F., Tran, D., Kipf, T., Luˇci´c, M., Zhai, X.,\nKeysers, D., Harmsen, J., and Houlsby, N.\nScaling\nVision Transformers to 22 Billion Parameters, Febru-\nary 2023.\nURL http://arxiv.org/abs/2302.\n05442. arXiv:2302.05442 [cs].\nEdgar, R. C.\nUpdating the 97% identity threshold for\n16S ribosomal RNA OTUs.\nBioinformatics, 34(14):\n2371–2375, July 2018. ISSN 1367-4803, 1367-4811.\ndoi:\n10.1093/bioinformatics/bty113.\nURL https:\n//academic.oup.com/bioinformatics/\narticle/34/14/2371/4913809.\nFasogbon, I. V., Ondari, E. N., Tusubira, D., Rangasamy, L.,\nVenkatesan, J., Musyoka, A. M., and Aja, P. M. Recent\nfocus in non-SELEX-computational approach for de novo\naptamer design: A mini review. Analytical Biochemistry,\npp. 115756, 2024. Publisher: Elsevier.\nGrønning, A., Doktor, T. K., Larsen, S., Petersen, U.,\nHolm, L. L., Bruun, G., Hansen, M. B., Hartung,\nA.-M., Baumbach, J., and Andresen, B. S. DeepCLIP:\npredicting the effect of mutations on protein–RNA\nbinding with deep learning. Nucleic Acids Research, pp.\ngkaa530, June 2020. ISSN 0305-1048, 1362-4962. doi:\n10.1093/nar/gkaa530.\nURL https://academic.\noup.com/nar/advance-article/doi/10.\n1093/nar/gkaa530/5859960.\nGuo, P., Coban, O., Snead, N. M., Trebley, J., Hoeprich,\nS., Guo, S., and Shu, Y.\nEngineering RNA for Tar-\ngeted siRNA Delivery and Medical Application. Ad-\nvanced Drug Delivery Reviews, 62(6):650–666, April\n2010.\nISSN 0169409X.\ndoi: 10.1016/j.addr.2010.\n03.008. URL https://linkinghub.elsevier.\ncom/retrieve/pii/S0169409X10000773.\n9\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nHayes, T., Rao, R., Akin, H., Sofroniew, N. J., Oktay,\nD., Lin, Z., Verkuil, R., Tran, V. Q., Deaton, J., Wig-\ngert, M., Badkundri, R., Shafkat, I., Gong, J., Derry,\nA., Molina, R. S., Thomas, N., Khan, Y., Mishra, C.,\nKim, C., Bartie, L. J., Nemeth, M., Hsu, P. D., Sercu,\nT., Candido, S., and Rives, A.\nSimulating 500 mil-\nlion years of evolution with a language model, July\n2024. URL http://biorxiv.org/lookup/doi/\n10.1101/2024.07.01.600583.\nHendrycks, D. and Gimpel, K. Gaussian Error Linear Units\n(GELUs), 2016. URL https://arxiv.org/abs/\n1606.08415. Version Number: 5.\nHentze, M. W., Castello, A., Schwarzl, T., and Preiss, T.\nA brave new world of RNA-binding proteins. Nature\nReviews Molecular Cell Biology, 19(5):327–341, May\n2018.\nISSN 1471-0072, 1471-0080.\ndoi: 10.1038/\nnrm.2017.130. URL https://www.nature.com/\narticles/nrm.2017.130.\nIm, J., Park, B., and Han, K.\nA generative model for\nconstructing nucleic acid sequences binding to a pro-\ntein. BMC genomics, 20(Suppl 13):967, 2019. Publisher:\nSpringer.\nIngraham, J. B., Baranov, M., Costello, Z., Barber, K. W.,\nWang, W., Ismail, A., Frappier, V., Lord, D. M., Ng-\nThow-Hing, C., Van Vlack, E. R., Tie, S., Xue, V.,\nCowles, S. C., Leung, A., Rodrigues, J. V., Morales-\nPerez, C. L., Ayoub, A. M., Green, R., Puentes, K.,\nOplinger, F., Panwar, N. V., Obermeyer, F., Root, A. R.,\nBeam, A. L., Poelwijk, F. J., and Grigoryan, G.\nIl-\nluminating protein space with a programmable gener-\native model.\nNature, 623(7989):1070–1078, Novem-\nber 2023. ISSN 0028-0836, 1476-4687. doi: 10.1038/\ns41586-023-06728-8. URL https://www.nature.\ncom/articles/s41586-023-06728-8.\nIwano, N., Adachi, T., Aoki, K., Nakamura, Y., and\nHamada, M.\nGenerative aptamer discovery using\nRaptGen.\nNature Computational Science, 2(6):378–\n386, June 2022.\nISSN 2662-8457.\ndoi: 10.1038/\ns43588-022-00249-6. URL https://www.nature.\ncom/articles/s43588-022-00249-6.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M.,\nRonneberger, O., Tunyasuvunakool, K., Bates, R., ˇZ´ıdek,\nA., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S.\nA. A., Ballard, A. J., Cowie, A., Romera-Paredes, B.,\nNikolov, S., Jain, R., Adler, J., Back, T., Petersen, S.,\nReiman, D., Clancy, E., Zielinski, M., Steinegger, M.,\nPacholska, M., Berghammer, T., Bodenstein, S., Silver,\nD., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli,\nP., and Hassabis, D. Highly accurate protein structure\nprediction with AlphaFold. Nature, 596(7873):583–589,\nAugust 2021. ISSN 0028-0836, 1476-4687. doi: 10.1038/\ns41586-021-03819-2. URL https://www.nature.\ncom/articles/s41586-021-03819-2.\nKim, N., Gan, H. H., and Schlick, T. A computational\nproposal for designing structured RNA pools for in vitro\nselection of RNAs. RNA, 13(4):478–492, 2007a. Pub-\nlisher: Cold Spring Harbor Lab.\nKim, N., Shin, J. S., Elmetwaly, S., Gan, H. H., and Schlick,\nT. RagPools: RNA-As-Graph-Pools: a web server for as-\nsisting the design of structured RNA pools for in vitro se-\nlection. Bioinformatics, 23(21):2959–2960, 2007b. Pub-\nlisher: Oxford University Press.\nKingma, D. P. and Ba, J. Adam: A Method for Stochastic\nOptimization, January 2017. URL http://arxiv.\norg/abs/1412.6980. arXiv:1412.6980 [cs].\nLee, G., Jang, G. H., Kang, H. Y., and Song, G. Predicting\naptamer sequences that interact with target proteins us-\ning an aptamer-protein interaction classifier and a Monte\nCarlo tree search approach. PloS one, 16(6):e0253760,\n2021. Publisher: Public Library of Science San Francisco,\nCA USA.\nLi, D., Huang, R., Cui, C., Towey, D., Zhou, L., Tian, J., and\nZou, B. RNA-Protein Interaction Prediction Based on\nDeep Learning: A Comprehensive Survey. arXiv preprint\narXiv:2410.00077, 2024.\nLi, W. and Godzik, A.\nCd-hit:\na fast program for\nclustering and comparing large sets of protein or\nnucleotide sequences.\nBioinformatics, 22(13):1658–\n1659,\nJuly 2006.\nISSN 1367-4811,\n1367-4803.\ndoi:\n10.1093/bioinformatics/btl158.\nURL https:\n//academic.oup.com/bioinformatics/\narticle/22/13/1658/194225.\nNori, D. and Jin, W. RNAFlow: RNA Structure & Se-\nquence Design via Inverse Folding-Based Flow Matching,\nJune 2024. URL http://arxiv.org/abs/2405.\n18768. arXiv:2405.18768 [q-bio].\nObonyo, S., Jouandeau, N., and Owuor, D. RNA Generative\nModeling With Tree Search. In 2024 IEEE Conference on\nComputational Intelligence in Bioinformatics and Com-\nputational Biology (CIBCB), pp. 1–9. IEEE, 2024.\nOlechnoviˇc, K. and Venclovas, C.\nVoronota: A fast\nand reliable tool for computing the vertices of the\nVoronoi diagram of atomic balls.\nJournal of Com-\nputational Chemistry, 35(8):672–681, March 2014.\nISSN 0192-8651, 1096-987X.\ndoi:\n10.1002/jcc.\n23538.\nURL https://onlinelibrary.wiley.\ncom/doi/10.1002/jcc.23538.\n10\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nOlechnoviˇc, K. and Venclovas, C. VoroContacts: a tool\nfor the analysis of interatomic contacts in macromolec-\nular structures.\nBioinformatics, 37(24):4873–4875,\nDecember 2021.\nISSN 1367-4803,\n1367-4811.\ndoi: 10.1093/bioinformatics/btab448.\nURL https:\n//academic.oup.com/bioinformatics/\narticle/37/24/4873/6300513.\nOzden, F., Barazandeh, S., Akboga, D., Tabrizi, S. S.,\nSeker, U. O. S., and Cicek, A. E.\nRNAGEN: A\ngenerative adversarial network-based model to gener-\nate synthetic RNA sequences to target proteins, July\n2023. URL http://biorxiv.org/lookup/doi/\n10.1101/2023.07.11.548246.\nPark, B. and Han, K. Discovering protein-binding RNA mo-\ntifs with a generative model of RNA sequences. Computa-\ntional Biology and Chemistry, 84:107171, February 2020.\nISSN 14769271. doi: 10.1016/j.compbiolchem.2019.\n107171. URL https://linkinghub.elsevier.\ncom/retrieve/pii/S1476927119305365.\nRay, D., Kazan, H., Chan, E. T., Castillo, L. P., Chaudhry,\nS., Talukder, S., Blencowe, B. J., Morris, Q., and Hughes,\nT. R. Rapid and systematic analysis of the RNA recog-\nnition specificities of RNA-binding proteins.\nNature\nBiotechnology, 27(7):667–670, July 2009. ISSN 1087-\n0156, 1546-1696. doi: 10.1038/nbt.1550. URL https:\n//www.nature.com/articles/nbt.1550.\nRay, D., Kazan, H., Cook, K. B., Weirauch, M. T., Na-\njafabadi, H. S., Li, X., Gueroussov, S., Albu, M., Zheng,\nH., Yang, A., Na, H., Irimia, M., Matzat, L. H., Dale,\nR. K., Smith, S. A., Yarosh, C. A., Kelly, S. M., Na-\nbet, B., Mecenas, D., Li, W., Laishram, R. S., Qiao,\nM., Lipshitz, H. D., Piano, F., Corbett, A. H., Carstens,\nR. P., Frey, B. J., Anderson, R. A., Lynch, K. W., Pe-\nnalva, L. O. F., Lei, E. P., Fraser, A. G., Blencowe, B. J.,\nMorris, Q. D., and Hughes, T. R. A compendium of\nRNA-binding motifs for decoding gene regulation. Na-\nture, 499(7457):172–177, July 2013. ISSN 0028-0836,\n1476-4687. doi: 10.1038/nature12311. URL https:\n//www.nature.com/articles/nature12311.\nReddi, S. J., Kale, S., and Kumar, S. On the Convergence of\nAdam and Beyond, April 2019. URL http://arxiv.\norg/abs/1904.09237. arXiv:1904.09237 [cs].\nRhiju, D., Shujun, H., Alissa, H., and Rachael, K.\nNucleic\nAcid\nAssessment\nCASP16,\nDecember\n2024.\nURL\nhttps://predictioncenter.\norg/casp16/doc/presentations/\nDay-3/Day3-01-Kretsch_CASP16_NA_\nAssessement_PuntaCana_RCK_v1.pptx.\nShin, I., Kang, K., Kim, J., Sel, S., Choi, J., Lee, J.-W.,\nKang, H. Y., and Song, G. AptaTrans: a deep neural\nnetwork for predicting aptamer-protein interaction using\npretrained encoders. BMC bioinformatics, 24(1):447,\n2023. Publisher: Springer.\nSu, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.\nRoFormer: Enhanced Transformer with Rotary Position\nEmbedding, November 2023. URL http://arxiv.\norg/abs/2104.09864. arXiv:2104.09864 [cs].\nThavarajah, W., Hertz, L. M., Bushhouse, D. Z., Archuleta,\nC. M., and Lucks, J. B.\nRNA Engineering for Pub-\nlic Health:\nInnovations in RNA-Based Diagnostics\nand Therapeutics.\nAnnual Review of Chemical and\nBiomolecular\nEngineering,\n12(1):263–286,\nJune\n2021.\nISSN 1947-5438, 1947-5446.\ndoi: 10.1146/\nannurev-chembioeng-101420-014055.\nURL https:\n//www.annualreviews.org/doi/10.1146/\nannurev-chembioeng-101420-014055.\nThe RNAcentral Consortium, Sweeney, B. A., Petrov, A. I.,\nBurkov, B., Finn, R. D., Bateman, A., Szymanski, M.,\nKarlowski, W. M., Gorodkin, J., Seemann, S. E., Can-\nnone, J. J., Gutell, R. R., Fey, P., Basu, S., Kay, S.,\nCochrane, G., Billis, K., Emmert, D., Marygold, S. J.,\nHuntley, R. P., Lovering, R. C., Frankish, A., Chan, P. P.,\nLowe, T. M., Bruford, E., Seal, R., Vandesompele, J.,\nVolders, P.-J., Paraskevopoulou, M., Ma, L., Zhang, Z.,\nGriffiths-Jones, S., Bujnicki, J. M., Boccaletto, P., Blake,\nJ. A., Bult, C. J., Chen, R., Zhao, Y., Wood, V., Ruther-\nford, K., Rivas, E., Cole, J., Laulederkind, S. J. F., Shi-\nmoyama, M., Gillespie, M. E., Orlic-Milacic, M., Kalvari,\nI., Nawrocki, E., Engel, S. R., Cherry, J. M., Team, S., Be-\nrardini, T. Z., Hatzigeorgiou, A., Karagkouni, D., Howe,\nK., Davis, P., Dinger, M., He, S., Yoshihama, M., Ken-\nmochi, N., Stadler, P. F., and Williams, K. P.\nRNA-\ncentral: a hub of information for non-coding RNA se-\nquences. Nucleic Acids Research, 47(D1):D221–D229,\nJanuary 2019. ISSN 0305-1048, 1362-4962. doi: 10.\n1093/nar/gky1034. URL https://academic.oup.\ncom/nar/article/47/D1/D221/5160993.\nTorkamanian-Afshar, M., Nematzadeh, S., Tabarzad, M.,\nNajafi, A., Lanjanian, H., and Masoudi-Nejad, A. In\nsilico design of novel aptamers utilizing a hybrid method\nof machine learning and genetic algorithm. Molecular\ndiversity, 25:1395–1407, 2021. Publisher: Springer.\nTseng, C.-Y., Ashrafuzzaman, M., Mane, J. Y., Kapty, J.,\nMercer, J. R., and Tuszynski, J. A. Entropic Fragment-\nBased Approach to Aptamer Design. Chemical Biology &\nDrug Design, 78(1):1–13, 2011. Publisher: Wiley Online\nLibrary.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nIs All You Need, August 2023. URL http://arxiv.\norg/abs/1706.03762. arXiv:1706.03762 [cs].\n11\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nWang, Y., Mistry, B. A., and Chou, T. Discrete stochastic\nmodels of SELEX: Aptamer capture probabilities and\nprotocol optimization. The Journal of Chemical Physics,\n156(24), 2022. Publisher: AIP Publishing.\nWang, Z., Liu, Z., Zhang, W., Li, Y., Feng, Y., Lv, S.,\nDiao, H., Luo, Z., Yan, P., He, M., and others. AptaDiff:\nde novo design and optimization of aptamers based on\ndiffusion models.\nBriefings in Bioinformatics, 25(6):\nbbae517, 2024. Publisher: Oxford University Press.\nZhang, B. and Sennrich, R.\nRoot Mean Square Layer\nNormalization, October 2019. URL http://arxiv.\norg/abs/1910.07467. arXiv:1910.07467 [cs].\nZhang, Y., Jiang, Y., Kuster, D., Ye, Q., Huang, W.,\nF¨urbacher, S., Zhang, J., Tang, Z., Ibberson, D., Wild, K.,\nand others. Single-step discovery of high-affinity RNA\nligands by UltraSelex. 2023.\nZhang, Z., Chao, L., Jin, R., Zhang, Y., Zhou, G., Yang, Y.,\nYang, Y., Huang, K., Yang, Q., Xu, Z., and others. RNA-\nGenesis: Foundation Model for Enhanced RNA Sequence\nGeneration and Structural Insights. bioRxiv, pp. 2024–12,\n2024. Publisher: Cold Spring Harbor Laboratory.\nZhao, Y., Oono, K., Takizawa, H., and Kotera, M. Gen-\nerRNA: A generative pre-trained language model for de\nnovo RNA design. PLOS ONE, 19(10):e0310814, Oc-\ntober 2024.\nISSN 1932-6203.\ndoi: 10.1371/journal.\npone.0310814. URL https://dx.plos.org/10.\n1371/journal.pone.0310814.\n12\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nAppendix\nA. RNA-BAnG architecture\nRNA-BAnG architecture is schematically presented in Figure 5 of the main text and is composed of protein and nucleotide\nmodules, detailed in Figure A.1 and Figure A.2. The latent dimension of the model is cs = 128, all heads dimensions are set\nto ch = 64. Feedforward blocks have a scaling factor n = 2. We set the number of protein and nucleotide modules to 10\neach. While reducing this number led to poorer performance, increasing it did not yield any noticeable improvements. We\nadjusted the rest of the hyperparameters by selecting the smallest model size combination that ensured stable convergence.\nResulting model contains 14,5 million parameters.\nProtein sequence\nProtein indices\nProtein frames\nOne-hot + Linear\nRMSNorm\nRoPE Self Attention\nGeometric Self Attention\nMLP+GELU\nProtein representation\nSequence embedding\nEmbedder\nRepeatable protein module\nFigure A.1. Schematic illustration of the protein module in the RNA-BAnG architecture. Here, raa is the number of protein residues, N\nstands for concatenation.\nAlgorithm A.1 describes the Geometric Attention block. It takes frames Ti and single representation si of every protein\nresidue in the chain as inputs. The number of attention heads is h = 12, the number of query points is Nquery points = 4, the\nnumber of value points is Nvalue points = 8. The weight per head γh ∈R is the softplus of a learnable scalar. ω is a weighting\nfactor. We adjusted the block’s hyperparameters to align with the AlphaFold2 IPA choices.\nB. Synthetic task\nB.1. Toy model\nTo avoid direct memorization on the synthetic task, we opted for a compact model, same for each tested method. Its final\nconfiguration is a two-block transformer (with a hidden dimension of 64) with the RoPE positional attention and two\nattention heads, resulting in 17k parameters. We trained the model for each method for 80k steps with a batch size of 8,\nusing ADAM optimizer (Kingma & Ba, 2017) with the learning rate of 0.0001. During the inference, the length was fixed to\n50 tokens for iterative methods.\n13\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nNucleotide sequence\nNucleotide indices\nRoPE Self Attention\nCross Attention\nMLP+GELU\nNucleotide type\nRMSNorm\nOne-hot + Linear\nOne-hot + Linear\nSequence representation\nProtein representation\nNucleotide sequence representation\nRepeatable nucleotide module\nder\nFigure A.2. Schematic illustration of the nucleotide module in the RNA-BAnG architecture. Here, rna is the number of nucleotides, raa\nis the number of protein residues, N stands for concatenation.\nB.2. Synthetic sampling\nDuring the inference, the sequence length was fixed to 50 tokens for iterative methods. For all the methods tested on the\nsynthetic task, we sample tokens from their distributions using the top-k strategy with k = 4.\nB.3. Generated synthetic sequences\nTable B.1 lists examples of sequences generated for the DoubleBind task with each tested approach.\nTable B.1. Example of generated sequences for the synthetic task. Expected synthetic motifs are underlined.\nGENERATIVE METHOD\nSEQUENCES\nBANG\nUCCCGGCUGGUUCCGAUCGGAACUGACUCGCACCUGGUUCCGACUUAUAU\nUAUCGGUACGCUACAGGCGUCUCAAUUGAGAGCGGCUGGCAUGUAUUGCU\nAUTOREGRESSIVE\nGGACCAAUUGUUGAUUGACUCGGACUAAUUGCUCCACCGCUAACGAUUGC\nIANG ENTROPY\nCCUCAAUUGGGGAGCCGGCUGCCGUGCUGCGAGUGACAUUUGAACGUGAU\nCUCGCCGCCCGGGGACCAUUGCACAAAUUGACUCUAGCGCAGAAUGGUAC\nITERATIVE ENTROPY\nAUAGAAUUUACCCACCUGAUGAUGCCCCACUUAGCGGAUAUCUGCUUUCG\nAAAAUAUUCGUGGUUUUACUCCACCACUCCUAAACGCGAACUGAACCUAC\n14\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nAlgorithm A.1 Geometric Attention\ninput si, Ti\nqhp\ni , khp\ni\n←LinearNoBias(si)\np ∈[1, Nquery points]\nvhp\ni\n←LinearNoBias(si)\np ∈[1, Npoint values]\nw ←\nq\n2\n9Nquery points\nah\nij ←softmax\n\u0010\n−γhw\n2\nP\np ∥Ti ◦qhp\ni\n−Tj ◦khp\nj ∥2\u0011\nohp\ni\n←T −1\ni\n◦P\nj ah\nij\n\u0010\nTj ◦vhp\nj\n\u0011\noutput Linear\n\u0010\nconcath,p(ohp\ni , ∥ohp\ni ∥)\n\u0011\nC. Data processing\nC.1. Protein coupled nucleotide sequences\nWe defined two protein-RNA or protein-DNA chains as interacting if at least one interaction occurred between their residues.\nWe defined an interaction between two residues if they share at least one atom-atom contact, as calculated using the\nVoroContacts software (Olechnoviˇc & Venclovas, 2021). The VoroContacts method identifies contacts based on the Voronoi\ntessellation of atomic balls, constrained within the solvent-accessible surface (Olechnoviˇc & Venclovas, 2014).\nWe excluded all samples where protein atom coordinates are ambiguous (had alternative locations), where the protein chain\ncontains non-canonical residues (though we substitute ’SCE’ with ’CYS’ and ’MSE’ with ’MET’ beforehand), or where\nthe nucleotide chain contains non-standard residues or a mixture of standard RNA and DNA residues. We also excluded\nsamples with the nucleotide sequence length of less than 10 residues. To avoid potential computational resource problems,\nwe included in the training only samples with a protein length of fewer than 500 residues.\nDuring training, we split the data by protein sequence homology. We measured it using the clustering provided by PPI3D\n(Dapk¯unas et al., 2024), where proteins within the same cluster share less than 40% sequence similarity with those in other\nclusters. We allocated samples from 95% of randomly selected clusters to the train set and the rest we used for validation.\nThis approach allows tracking the model generalization across the protein space. To enhance our potential test set, we\nremoved from the training data clusters containing proteins from the PDB samples 5ITH (chain A), 7CRE (chain A), 6QW6\n(chain R), 8OPS (chain B), and 1CVJ (chain A).\nSome proteins and nucleotide sequences are overrepresented in the data, which may lead to training imbalance. To address\nthis, we introduced an additional level of sample clustering. First, we clustered nucleotide sequences (DNAs and RNAs\nseparately) based on sequence similarity, grouping those with 90% identity using CD-HIT-EST (Li & Godzik, 2006). Then,\nwe clustered the samples based on the combination of protein and nucleotide sequences clusters. At each epoch during\ntraining, we selected eight random samples (the mean cluster population) to represent each of the latter clusters, repeating\nthe samples when necessary.\nThe resulting dataset consisted of 123,043 samples, distributed across 3,580 protein sequence clusters, 2,807 nucleotide\nsequence clusters (915 RNA and 1,892 DNA), and 12,667 combined clusters. The protein sequences in the dataset have\na mean length of 155 residues with a standard deviation of 90. For nucleotide sequences, RNA lengths average 1,834\nnucleotides (± 1,564), while DNA lengths average 76 nucleotides (± 78).\nTo prevent potential computational resource issues and to focus the model on the binding motifs, we truncated nucleotide\nsequences exceeding 300 residues during training and validation. This truncation limited sequences to 300 residues centered\naround the anchor point, which was selected as described prior to the truncation.\nC.2. Standalone RNA sequences\nFrom RNAcentral database we selected sequences containing only standard residues and with lengths between 10 and 500\nnucleotides. The selection was then deduplicated using CD-HIT-EST with a 90% similarity threshold, resulting in a final set\nof 3 million sequences.\n15\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nTable F.1. Example of sequences generated by RNA-BAnG. Samples are ordered by decreasing proportion of high-affinity sequences.\nFirst two generated nucleotides are in bold and marked by red color.\nRNACOMPETE ID\nSEQUENCES\nRNCMPT00077\nAUUUUUAAAUAUUUUUAAAAAAAA\nUAUUAUUUAUUUUAAAA\nGUAUGUAUUUAUUUU\nRNCMPT00173\nGUGAACGUAAAACUUUUAACUUAAAUUCCUCA\nAUUGAAAGCUUUUAUGCCUUUUACAAUAAA\nCGACUCAAAAGACAAUCUAAUACUCAAAAACCGGAUUAAACUUAAAAAUA\nRNCMPT00167\nCUUGUCUGACACG\nGCCCCUUGACCUUGAGUCCCAUGUGGCAGAGCAGUACAGGCUGAGUCGCU\nUGAGGUACACCA\nRNCMPT00133\nGCGCAGUGCCCAUAGACUCUGCAUAAUGGGACUCCAAGGAGCCGUCGGUU\nCCUGCGAACUUAUCAUUUCUAUAGUGAUGCAAAUAUGUACUUAAUUUUUA\nCGGAACGGAUUAUUUGUUUUAUAAAUAAUUAUGAAAAGUAUUUUAUUAUA\nD. Training details\nAs explained in the main text, we conducted the RNA-BAnG training in two steps. In the first step, we used a batch size\nof 64 and trained for 255k steps. In the second step, we used a batch size of 8 and trained for 216k steps. We stopped\ntraining when the validation loss decline became negligible. In both training steps, we used the ASMGrad (Reddi et al.,\n2019) variation of the ADAM optimizer (Kingma & Ba, 2017) with default parameters and the learning rate of 0.0001.\nLearning rate was warmed up linearly for the first 1k steps and then decayed exponentially with γ = 0.99 and a period of 1k\nsteps. Complete training took 4 days on a single MI120 AMD GPU.\nE. Evaluation details\nE.1. Test data\nThe RNA Compendium study provides 244 samples, each comprising a protein sequence paired with approximately 200,000\nRNA sequences and their corresponding experimental binding scores. These samples are designated by the authors as\nRNCMPT00XXX, where XXX represents a numerical identifier. For simplicity, throughout this paper, we refer to these\nsamples solely by their numerical identifiers XXX.\nWe processed the data as follows. For each sample, RNA sequences were ranked by their binding scores. The top 2,000\nsequences were labeled as the positive (interacting) class, while the bottom 2,000 were labeled as the negative (non-\ninteracting) class. To eliminate sequence redundancy and prevent data leakage, duplicate RNA sequences were removed\nusing CD-HIT-EST with a sequence identity threshold of 90%. These sequences were then randomly split into training\nand testing sets, so that each test set would have a 1,000 sequences. We then trained individual DeepCLIP models for\neach sample, following the protocol outlined by the authors. Only models achieving an area under the receiver operating\ncharacteristic curve (AUROC) of 0.95 or higher on the corresponding test set were selected for further analysis.\nE.2. RNA-BAnG sampling\nFor RNA-BAnG, we sample tokens from their distributions using the top-k strategy with k = 4. Maximum sequence length\nis set to 50 nucleotides. Average sampling time is 20 minutes for 1,000 RNA sequences. To sample random sequences,\nwe first randomly select a sequence length between 40 and 50, then uniformly sample that many nucleotides. This length\nchoice ensures that the mean sequence length of random samples matches that of RNA-BAnG-generated sequences (45.6\nnucleotides).\nF. Additional results\nTable F.1 lists randomly selected examples of generated RNA-BAnG sequences for the two best and two worst performance\ntest samples.\n16\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\nFigure F.1 shows the proportion of high-affinity generated sequences as a function of AlphaFold pLDDT scores and protein\nsequence similarity with the train set. Appendix G details protein sequence similarity calculations. Figure F.2 shows kernel\ndensity plots calculated over the whole test set of 71 samples. We assessed four different DeepCLIP score threshold values\n(0.65, 0.75, 0.85, 0.95, from left to right in the Figure). We can conclude that the value of 0.95 is too stringent, as the mean\nof the positive experimental sets approaches the value of 0.5. Subjectively, the most visually appealing threshold value\nis 0.75. Nonetheless, in the main text (Figure 6) we continuously assess all the values by plotting the mean proportions\nabove a certain threshold value. Distributions of DeepCLIP scores for each of 71 test samples are depicted in Figure F.3 and\nFigure F.4.\n0\n20\n40\n60\n80\n100\nProtein Sequence Similarity\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of haRNA\n75\n80\n85\n90\nProtein pLDDT\nFigure F.1. Proportion of generated high-affinity sequences as a function of protein sequence similarity to the training data and the\nAlphaFold pLDDT scores of the predicted protein structures.\n0.0\n0.5\n1.0\n0\n1\n2\n3\n4\n5\n0.65\n0.0\n0.5\n1.0\n0.75\n0.0\n0.5\n1.0\n0.85\n0.0\n0.5\n1.0\n0.95\nProportions of haRNA\nDensity\nFigure F.2. Density plots of the proportion of high-affinity sequences for several threshold values (0.65, 0.75, 0.85, and 0.95, listed on\ntop) across the whole test set. RNA-BAnG is shown in green, random sequences in yellow, positive experimental sequences in blue and\nnegative experimental sequences in red.\nG. Tools parameters\nClustering of nucleotide sequences was always performed using CD-HIT-EST (Li & Godzik, 2006) with a sequence identity\nthreshold of 90%. We used CD-HIT-EST with these parameters: -c 0.9 -n 9 -d 0 -T 10 -U 10 -l 9. For\nprotein sequence identity to the train set calculations we used blastp (Altschul et al., 1990) with default parameters. For the\nnucleotide sequences similarity to the train set calculations we used blastp (Altschul et al., 1990) with default parameters.\n17\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\n0\n5\n10\n15\n20\nRNCMPT00146\nRNCMPT00008\nRNCMPT00156\nRNCMPT00158\nRNCMPT00133\nRNCMPT00022\n0\n5\n10\n15\n20\nRNCMPT00023\nRNCMPT00024\nRNCMPT00245\nRNCMPT00167\nRNCMPT00027\nRNCMPT00091\n0\n5\n10\n15\n20\nRNCMPT00288\nRNCMPT00178\nRNCMPT00029\nRNCMPT00095\nRNCMPT00096\nRNCMPT00117\n0\n5\n10\n15\n20\nRNCMPT00136\nRNCMPT00032\nRNCMPT00274\nRNCMPT00033\nRNCMPT00169\nRNCMPT00062\n0\n5\n10\n15\n20\nRNCMPT00036\nRNCMPT00255\nRNCMPT00197\nRNCMPT00037\nRNCMPT00261\nRNCMPT00263\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n5\n10\n15\n20\nRNCMPT00139\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00043\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00171\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00157\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00246\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00206\nScores\nDensity\nFigure F.3. Distribution of DeepCLIP scores for the first part of the test samples (RNAcompete sample IDs on top). Scores of RNA-BAnG\nare in green. Scores of positive and negative experimental sequences are in blue and red, respectively.\n18\n\n\nBAnG: Bidirectional Anchored Generation for Conditional RNA Design\n0\n5\n10\n15\n20\nRNCMPT00229\nRNCMPT00249\nRNCMPT00268\nRNCMPT00269\nRNCMPT00141\nRNCMPT00184\n0\n5\n10\n15\n20\nRNCMPT00285\nRNCMPT00049\nRNCMPT00283\nRNCMPT00052\nRNCMPT00241\nRNCMPT00173\n0\n5\n10\n15\n20\nRNCMPT00205\nRNCMPT00069\nRNCMPT00232\nRNCMPT00071\nRNCMPT00107\nRNCMPT00108\n0\n5\n10\n15\n20\nRNCMPT00109\nRNCMPT00072\nRNCMPT00076\nRNCMPT00217\nRNCMPT00219\nRNCMPT00252\n0\n5\n10\n15\n20\nRNCMPT00253\nRNCMPT00165\nRNCMPT00077\nRNCMPT00256\nRNCMPT00005\nRNCMPT00075\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n5\n10\n15\n20\nRNCMPT00226\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00236\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00257\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00258\n0.00\n0.25\n0.50\n0.75\n1.00\nRNCMPT00087\nScores\nDensity\nFigure F.4. Distribution of DeepCLIP scores for the second part of test samples (RNAcompete sample IDs on top). Scores of RNA-BAnG\nare in green. Scores of positive and negative experimental sequences are in blue and red, respectively.\n19\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21274v1.pdf",
    "total_pages": 19,
    "title": "BAnG: Bidirectional Anchored Generation for Conditional RNA Design",
    "authors": [
      "Roman Klypa",
      "Alberto Bietti",
      "Sergei Grudinin"
    ],
    "abstract": "Designing RNA molecules that interact with specific proteins is a critical\nchallenge in experimental and computational biology. Existing computational\napproaches require a substantial amount of experimentally determined RNA\nsequences for each specific protein or a detailed knowledge of RNA structure,\nrestricting their utility in practice. To address this limitation, we develop\nRNA-BAnG, a deep learning-based model designed to generate RNA sequences for\nprotein interactions without these requirements. Central to our approach is a\nnovel generative method, Bidirectional Anchored Generation (BAnG), which\nleverages the observation that protein-binding RNA sequences often contain\nfunctional binding motifs embedded within broader sequence contexts. We first\nvalidate our method on generic synthetic tasks involving similar localized\nmotifs to those appearing in RNAs, demonstrating its benefits over existing\ngenerative approaches. We then evaluate our model on biological sequences,\nshowing its effectiveness for conditional RNA sequence design given a binding\nprotein.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}