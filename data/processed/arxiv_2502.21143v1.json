{
  "id": "arxiv_2502.21143v1",
  "text": "Published as a conference paper at ICLR 2025\nVARIATIONAL BAYESIAN PSEUDO-CORESET\nHyungi Lee\nKAIST\nlhk2708@kaist.ac.kr\nSeungyoo Lee\nKAIST\npunctuate@kaist.ac.kr\nJuho Lee\nKAIST\njuholee@kaist.ac.kr\nABSTRACT\nThe success of deep learning requires large datasets and extensive training, which\ncan create significant computational challenges.\nTo address these challenges,\npseudo-coresets, small learnable datasets that mimic the entire data, have been\nproposed.\nBayesian Neural Networks, which offer predictive uncertainty and\nprobabilistic interpretation for deep neural networks, also face issues with large-\nscale datasets due to their high-dimensional parameter space. Prior works on\nBayesian Pseudo-Coresets (BPC) attempt to reduce the computational load for\ncomputing weight posterior distribution by a small number of pseudo-coresets\nbut suffer from memory inefficiency during BPC training and sub-optimal results.\nTo overcome these limitations, we propose Variational Bayesian Pseudo-Coreset\n(VBPC), a novel approach that utilizes variational inference to efficiently approx-\nimate the posterior distribution, reducing memory usage and computational costs\nwhile improving performance across benchmark datasets.\n1\nINTRODUCTION\nWhile deep learning has shown remarkable performance across various fields, its success requires\nlarge amounts of data storage and extensive training. However, handling such large datasets can\nimpose a significant computational burden, especially when training new models or updating ex-\nisting ones with new data. In settings like continual learning, where the model must be trained\ncontinuously on new data, this challenge becomes more pronounced due to the risk of catastrophic\nforgetting. To mitigate this, a small subset of representative data, called a coreset, is needed to pre-\nserve knowledge from previously learned data. Instead of creating a small dataset as a subset of the\nentire data to represent it, the approach of treating the small dataset itself as learnable parameters\nand training it to mimic the entire dataset is known as dataset distillation or pseudo-coreset (Nguyen\net al., 2020; 2021; Zhou et al., 2022; Loo et al., 2023).\nOn the other hand, Bayesian Neural Networks (BNNs) have gained attention in fields like health-\ncare (Abdullah et al., 2022; Lopez et al., 2023) and climate analysis (Vandal et al., 2018) because\nthey provide a posterior distribution over the weights of a deep neural network, enabling the mea-\nsurement of predictive uncertainty and allowing for a probabilistic interpretation of parameters (Pa-\npamarkou et al., 2024). While this method is promising for enabling various types of statistical\nanalysis, BNNs face significant challenges when applied to real-world scenarios that involve large-\nscale datasets. The high-dimensional parameter space and structure of BNNs often lead to posterior\nlandscapes with multiple modes, which complicates efficient and straightforward computation of\npredictive uncertainty. To overcome this, BNNs typically rely on indirect methods such as Stochas-\ntic Gradient Markov Chain Monte Carlo (SGMCMC; Welling & Teh, 2011; Chen et al., 2014; Ma\net al., 2015) or variational inference (VI; Blei et al., 2017; Fiedler & Lucia, 2023; Harrison et al.,\n2024b) instead of directly calculating the posterior distribution in closed form. However, these\napproaches still depend on gradient-based updates of model weights for large-scale datasets. In par-\nticular, SGMCMC-based methods face the challenge of increased computational load, as the amount\nof training grows linearly with the number of weight samples needed.\nTo overcome these issues, prior works on Bayesian Pseudo-Coreset (BPC; Manousakas et al., 2020;\nKim et al., 2022; 2023; Tiwary et al., 2024) aim to learn a small synthetic dataset that helps effi-\nciently compute the posterior distribution of BNNs’ weights. These studies train the pseudo-coreset\nby minimizing the divergence between the posterior obtained using the full dataset and the posterior\nobtained using the pseudo-coreset. However, these studies face three major problems: 1) require\n1\narXiv:2502.21143v1  [cs.LG]  28 Feb 2025\n\n\nPublished as a conference paper at ICLR 2025\nexpert trajectories for training, 2) use stop-gradient during training, and 3) still rely on SGMCMC\nsampling for weight space posterior computation. First, expert trajectories refer to the trajectories of\nmodel weights trained using the full dataset. In previous studies, these trajectories are saved for every\nepoch with multiple different seeds, and they are used to approximate and match the posterior dis-\ntribution. This creates the problem of needing to store the model weights for the number of epochs\nmultiplied by the number of seeds in order to train the pseudo-coreset. Secondly, when training\nBPC, the posterior distribution is computed using the BPC for loss computation via gradient-based\nmethods. As the updates progress, the computational graph required to update the pseudo-coreset\nbased on the loss becomes significantly larger, resulting in increased memory demands. To address\nthis memory issue, prior works have used the stop-gradient method to reduce memory consumption.\nHowever, this approach leads to sub-optimal results because it prevents accurate updates. Finally,\neven after training the pseudo-coreset, the weight posterior distribution remains multi-modal, mean-\ning that while the training cost is reduced, sequential training through SGMCMC sampling is still\nrequired for each sample. Additionally, after obtaining the samples, forward computation is needed\nfor each sample to calculate the predictive distribution during Bayesian inference.\nTo address these issues, we propose a novel BPC approach called Variational Bayesian Pseudo-\nCoreset (VBPC). In learning VBPC, unlike previous works, we employ VI, specifically last-layer\nVI (Fiedler & Lucia, 2023; Harrison et al., 2024b), to approximate the posterior distribution. During\nthe VBPC training and inference process, we demonstrate that this variational formulation allows\nus to obtain the closed-form posterior distribution of the last layer weights, which frees our method\nfrom relying on stop-gradient. This resolves the issue of suboptimal performance seen in previous\napproaches. And, we propose a memory-efficient method to approximate the predictive distribution\nwith only a single forward pass instead of multiple forwards, making the approach computationally\nand memory-efficient. Furthermore, we empirically show that VBPC achieves better performance\ncompared to other baselines on various benchmark datasets.\n2\nPRELIMINARIES\n2.1\nBAYESIAN NEURAL NETWORKS AND BAYESIAN MODEL AVERAGING\nIn Bayesian Neural Network frameworks (Papamarkou et al., 2024; Lee et al., 2024), the main\nobjective is to compute the predictive distribution for a given input x, while accounting for model\nuncertainty (i.e., epistemic uncertainty), as shown below:\np(y|x, D) =\nZ\np(y|x, θ)p(θ|D)dθ,\n(1)\nwhere D represents the observed data, and θ denotes the model parameters. This process is known\nas Bayesian Model Averaging (BMA). To perform BMA, we need to compute the posterior distri-\nbution p(θ|D) and evaluate the integral. However, due to the complexity of the model and the high-\ndimensional parameter space, directly computing a closed-form solution for p(θ|D) is impractical.\nTherefore, in practice, we typically rely on posterior sampling methods such as SGMCMC (Welling\n& Teh, 2011; Chen et al., 2014; Ma et al., 2015) or VI (Blei et al., 2017; Fiedler & Lucia, 2023) to\napproximate the posterior distribution.\n2.2\nBAYESIAN PSEUDO-CORESET\nAs mentioned in Section 1, the large size of the training dataset makes it computationally intensive\nto perform SGMCMC or VI for approximating the posterior distribution of BNNs. To address\nthese challenges and efficiently compute the posterior distribution in terms of both computation and\nmemory, previous works (Kim et al., 2022; 2023; Tiwary et al., 2024) introduced BPC within the\nSGMCMC framework. Specifically, BPC S is optimized using the following objective:\nS∗= arg min\nS\nD(p(θ|D), p(θ|S)),\n(2)\nwhere D can be various divergences between the two distributions (Kim et al., 2022). The optimiza-\ntion poses a challenge, as the posteriors p(θ|D) and p(θ|S) are intractable for most of the cases.\nPrevious works (Kim et al., 2022; 2023; Tiwary et al., 2024) attempt to approximate them using\nweight checkpoints obtained from training trajectories based on the dataset D (i.e., expert trajecto-\nries) which requires expensive computation and memory cost.\n2\n\n\nPublished as a conference paper at ICLR 2025\n2.3\nNATURAL GRADIENT VARIATIONAL INFERENCE WITH EXPONENTIAL FAMILIES\nAlthough several methods exist for approximating the posterior p(θ|D), in this paper, we focus on\nVI (Bishop, 2006; Blundell et al., 2015; Blei et al., 2017). In VI, we approximate the target posterior\nwith a variational distribution that is easier to handle and optimize the parameters of the variational\ndistribution to minimize the Kullback-Leibler (KL) divergence between the approximate and target\nposterior distributions. Among the many possible choices for variational distributions, we focus on\nthe exponential family. We assume that both the prior pλ0(θ) and the variational distribution qλ(θ)\nbelong to the same class of exponential family distributions:\nqλ(θ) ∝exp(⟨ψ(θ), λ⟩−A(λ)),\npλ0(θ) ∝exp(⟨ψ(θ), λ0⟩−A(λ0)),\n(3)\nwhere ψ(·) represents the sufficient statistics, A(·) is the log partition function, and λ and λ0 are\nthe natural parameters for qλ and pλ0, respectively. We further assume that the exponential family\nis minimal, meaning that there is no non-zero vector x such that ⟨x, ψ(θ)⟩evaluates to a constant.\nUnder this setting, we can optimize the variational parameter λ by minimizing the following loss:\nLD(λ) := Eqλ[−log p(D|θ)] + βDKL[qλ(θ)∥pλ0(θ)],\n(4)\nwhere β > 0 is a temperature controlling the strength of the KL regularization (Blundell et al.,\n2015; Wenzel et al., 2020).\nWhen β\n= 1, minimizing Eq. 4 is equivalent to minimizing\nDKL[qλ(θ)∥p(θ|D)]. Optimizing equation 4 with natural gradient descent (Amari, 1998) has been\nshown to be effective, especially for large-scale deep neural networks (Khan et al., 2018; Osawa\net al., 2019; Shen et al., 2024). The optimal solution of Eq. 4 must satisfy the following equation,\nλ∗= λ0 + β−1∇µEqλ∗[log p(D|θ)],\n(5)\nwhere µ = Eqλ[ψ(θ)] = ∇λA(λ) is the mean parameter corresponding to the natural parameter λ.\nExcept for some cases, Eq. 5 does not admit a closed-form expression for λ∗. Therefore, one must\nrely on iterative algorithms to obtain it. This approach, which solves the variational inference using\niterative natural gradient descent steps, covers a broad spectrum of machine learning algorithms and\nis commonly referred to as the Bayesian Learning Rule (BLR) (Khan & Rue, 2023).\n3\nVARIATIONAL BAYESIAN PSEUDOCORESET\nIn this section, we propose a novel method called Variational Bayesian Pseudo-Coreset (VBPC)\nwhich effectively learns S and thereby well approximates the variational posterior distribution with\nfull dataset distribution. Several recent studies (Fiedler & Lucia, 2023; Harrison et al., 2024b) have\nshown that using only a last layer for variational inference is simple and computationally cheap,\nyet it performs comparably to more complex methods. Motivated by these findings, we seek to\nlearn a pseudo-coreset S that effectively approximates the last layer variational posterior for the\nclassification task, all while ensuring computational and memory efficiency.\n3.1\nPROBLEM SETUP\nConsider a supervised learning problem with a dataset D = (xi, yi)n\ni=1. While our discussion\ncan be easily extended to more general problems, in this paper, we focus on k-way classification\ntasks, where yi ∈{0, 1}k is a one-hot vector representing a category. Given D and a model fθ\nparameterized by θ, we aim to learn a synthetic dataset (pseudocoreset) S := (ˆxi, ˆyi)ˆn\ni=1 solving\nEq. 2 under a constraint ˆn ≪n. We approximate the pseudocoreset posterior p(θ|S) by solving the\nfollowing variational inference problem,\nLS(λ) := ℓS(λ) + βSDKL[qλ(θ)∥pλ0(θ)],\nλ∗\nS = arg min\nλ\nLS(λ),\n(6)\nwhere ℓS(λ) := −Eqλ[Pˆn\ni=1 log pS(yi|xi, θ)] is the expected sum of negative log-likelihoods over\nS given a choice of likelihood pS(y|x, θ). Throughout the paper, we call Eq. 6 as coreset VI prob-\nlem. Ideally, we would like to match the optimal solution of the coreset VI problem to the optimal\nvariational distribution computed with the original dataset D,\nLD(λ) := ℓD(λ) + βDDKL[qλ(θ)∥pλ0(θ)],\nλ∗\nD = arg min\nλ\nLD(λ),\n(7)\nwhere ℓD(λ) := −Eqλ [Pn\ni=1 log pD(yi|xi, θ)] for a likelihood pD(y|x, θ). We call Eq. 7 as dataset\nVI problem. After obtaining λ∗\nS and λ∗\nD, to learn S, we can minimize D(qλ∗\nS, qλ∗\nD) for some pre-\ndefined divergence D.\n3\n\n\nPublished as a conference paper at ICLR 2025\n3.2\nBILEVEL OPTIMIZATION\nIt is often challenging to first compute the approximate solutions of Eqs. 6 and 7 and then backprop-\nagate through the divergence D(qλ∗\nS, qλ∗\nD). Instead, considering the optimization nature of the VI,\nwe cast the problem of coreset learning as a bilevel optimization as follows:\nS∗= arg min\nS\nLD(λ∗\nS) where λ∗\nS = arg min\nλ\nLS(λ).\n(8)\nNote that similar approaches have also been considered in the dataset distillation literature (Loo\net al., 2023). Under the bilevel optimization formulation, learning S requires the derivative\n∇SLD(λ∗\nS) = (∇Sµ∗\nS)∇µLD(λ∗\nS),\n(9)\nwhere µ∗\nS = ∇λA(λ∗\nS) is the mean parameter corresponding to λ∗\nS. To obtain ∇Sµ∗\nS, we may apply\nthe implicit function theorem (Bengio, 2000; Krantz & Parks, 2002) to Eq. 5. Specifically, if we let:\nF(S, µ) := λ −λ0 + β−1\nS ∇µℓS(λ)\n(10)\nWith F(S, µ∗\nS) = 0, applying the implicit function theorem,\n∇SF(S, µ∗\nS) + (∇Sµ∗\nS)∇µF(S, µ∗\nS) = 0 ⇒∇Sµ∗\nS = −∇SF(S, µ∗\nS)∇µF(S, µ∗\nS)−1,\n∇Sµ∗\nS = −β−1\nS (∇S∇µℓS(λ∗\nS))\n\u0000∇µλ∗\nS + β−1\nS ∇2\nµℓS(λ∗\nS)\n\u0001−1 .\n(11)\nPlugging this back into the above equation, we get the expression for the gradient\n∇SLD(λ∗\nS) = −β−1\nS (∇S∇µℓS(λ∗\nS))\n\u0000∇µλ∗\nS + β−1\nS ∇2\nµℓS(λ∗\nS)\n\u0001−1 ∇µLD(λ∗\nS).\n(12)\nUnfortunately, the term involving the inverse is usually intractable, so one needs an approximation\n(e.g., Lorraine et al. (2020)). In the next section, we describe a case where the derivatives can be\ncomputed in closed form, and develop Bayesian pseudo-coreset algorithm based on it.\n3.3\nLAST LAYER VARIATIONAL BAYESIAN PSEUDOCORESET\nRecently, there has been growing interest in subspace Bayesian neural networks (BNNs), where only\na subset of the network’s parameters are treated as random, while the remaining parameters are kept\ndeterministic (Sharm et al., 2023; Shen et al., 2024). An extreme form of a subspace BNN would\nbe the last layer randomization, where a neural network fθ(x) ∈Rk is decomposed as a feature\nextractor ϕ(x) ∈Rh followed by a linear layer W ∈Rh×k. Denoting the jth column of W as wj\nand the jth output from fθ(x) as [fθ(x)]j, we have [fθ(x)]j = ϕ(x)⊤wj for j ∈[k]. Adapting the\nlast layer randomization scheme, we treat only the parameter W of the linear layer as random while\nkeeping the feature extractor ϕ(x) deterministic. From below, we describe our model more in detail.\nVariational distributions.\nWe assume the Gaussian priors and variational posteriors for W,\npλ0(W) =\nK\nY\nj=1\nN(wj|0, ρ−1Ih),\nqλ(W) =\nk\nY\nj=1\nN(wj|mj, Vj),\n(13)\nwith the natural parameters and the corresponding mean parameters are given as,\nλ0 = concat(\n\u0002\n0 −(ρ−1/2)Ih\n\u0003k\nj=1),\nµ0 = concat(\n\u0002\n0, ρ−1Ih\n\u0003k\nj=1),\nλ = concat((λj)k\nj=1),\nµ = concat((µj)k\nj=1),\n(14)\nwhere λj =\n\u0002\nV −1\nj\nmj −(1/2)Vj\n\u0003\n, and µj =\n\u0002\nmj, Vj + mjm⊤\nj\n\u0003\n. Here, we denote Id as the d × d\nidentity matrix and ρ is a pre-defined precision hyperparameter of the prior. Note that the block-wise\napproximation qλ(W) reduces the space complexity of the variance parameter V := (Vj)k\nj=1 from\nO(k2h2) to O(kh2) while keeping flexibility compare to mean field approximation.\n4\n\n\nPublished as a conference paper at ICLR 2025\nLikelihoods.\nFor a classification problem, it is common to use a softmax categorical likelihood,\nand we follow that convention for the dataset VI problem with pD. However, for the coreset VI\nproblem, the softmax categorical likelihoods would not allow a closed-form solution, which would\nnecessitate approximations involving iterative computations to solve the bilevel optimization Eq. 8.\nThis would, for instance, require storing the unrolled computation graph (Vicol et al., 2021) of the\niterative updates and performing backpropagation through it, leading to significant computational\nand memory overhead (Werbos, 1990). As a detour, we use the Gaussian likelihood for the pS, as\nit allows us to obtain a closed-form solution. While using Gaussian likelihoods may seem coun-\nterintuitive for a classification problem, it is widely used in the literature on infinitely-wide neural\nnetworks (Lee et al., 2017; 2019; 2022), and one can also interpret it as solving the classifica-\ntion problem as a regression, using one-hot labels as the target vector. More specifically, we set\npS(y|x, θ) = N(y|W ⊤ϕ(x), γ−1Ik) where γ−1 is the precision hyperparameter for the likelihood.\nWith our choices for pD and pS we can expand the bilevel optimization problem as follows.\nλ∗\nS = arg min\nλ\n−Eqλ\n\" ˆn\nX\ni=1\nlog N(ˆyi|W ⊤ϕ(ˆxi), γ−1Ik)\n#\n+ βSDKL[qλ∥pλ0],\n(15)\nS∗= arg min\nS\nEqλ∗\nS\n\n−\nn\nX\ni=1\nk\nX\nj=1\nyi,j log\nexp(ϕ(xi)⊤wj)\nPk\nl=1 exp(ϕ(x)⊤wl)\n\n+ βDDKL[qλ∗\nS∥pλ0].\n(16)\n3.4\nSOLVING CORESET VI PROBLEM\nBased on our choices described in the previous section, we show how we can obtain closed-form\nexpressions for the coreset VI problem. The likelihood term for the coreset VI problem is\nEqλ\n\"\n−\nˆn\nX\ni=1\nlog N(ˆyi|W ⊤ϕ(ˆxi), γ−1Ik)\n#\nc= γ\n2\nˆn\nX\ni=1\nk\nX\nj=1\nEqλ\nh\u0000ˆyi,j −ϕ(ˆxi)⊤wj\n\u00012i\n,\n(17)\nwhere ˆyi,j indicates jth element of ˆyi for all i ∈[ˆn] and\nc= denotes equality up to a constant. Then\nwe can further elaborate Eq. 17 as follows:\nγ\n2\nˆn\nX\ni=1\nk\nX\nj=1\nEqλ\nh\u0000ˆyi,j −ϕ(ˆxi)⊤wj\n\u00012i c= γ\n2\nk\nX\nj=1\n\u0010\n−2ˆy⊤\n:,jΦµ(1)\nj\n+ Tr\n\u0010\nΦ⊤Φµ(2)\nj\n\u0011\u0011\n,\n(18)\nwhere ˆy:,j := [ˆy1,j, . . . , ˆyˆn,j]⊤, Φ := [ϕ(ˆx1), . . . , ϕ(ˆxˆn)]⊤, µ(1)\nj\n= mj, and µ(2)\nj\n= Vj +mjm⊤\nj for\nall j ∈[k]. Then by Eq. 18, the gradient of the likelihood with respect to µj can be computed as:\n∇µ(1)\nj ℓS(λ) = −γΦ⊤ˆy:,j,\n∇µ(2)\nj ℓS(λ) = γ\n2 Φ⊤Φ,\n(19)\nThen from Eq. 5, we obtain the closed-form solution for the coreset VI problem as follows:\nλ∗\nS,j =\n\u0014 γ\nβS\nΦ⊤ˆy:.j −ρ\n2Ih −\nγ\n2βS\nΦ⊤Φ\n\u0015\n,\n∀j ∈[k],\n(20)\nwith Woodbury formula (Woodbury, 1950) which leads to\nm∗\nj = Φ⊤\n\u0012ρβS\nγ Iˆn + ΦΦ⊤\n\u0013−1\nˆy:,j,\nV ∗\nj = 1\nρIh −\nγ\nρ2βS\nΦ⊤\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦ.\n(21)\nFor all j ∈[k], the values V ∗\nj are identical, meaning the full covariance calculation, though O(kh2),\nonly requires computing and storing the variance once, O(h2). We will refer to this shared variance\nas V ∗. See Appendix A.1 and Appendix A.2 for detailed derivations in this section.\nBilevel optimization as an influence maximization.\nBefore proceeding to the dataset VI prob-\nlem, let us describe how the last-layer variational model simplifies the coreset gradient Eq. 12. From\nEq. 19, we have ∇2\nµℓS(λ∗\nS) = 0, leading to ∇2\nµLS(λ∗\nS) = ∇µλ∗\nS. Using this, we can show that\n∇SLD(λ∗\nS) = ∇S\n\u0010\n−∇µLS(λ∗\nS)⊤\u0000∇2\nµLS(λ∗\nS)\n\u0001−1 ∇µLD(λ∗\nS)\n\u0011\n.\n(22)\nHere, −∇µLS(λ∗\nS)⊤\u0000∇2\nµLS(λ∗\nS)\n\u0001−1 ∇µLD(λ∗\nS) is the variant (in a sense that it is defined w.r.t.\nthe gradient of the variational objective by the mean parameters) of the influence function (Koh &\nLiang, 2017), measuring the influence of the coreset S on the dataset VI loss computed with D.\n5\n\n\nPublished as a conference paper at ICLR 2025\n3.5\nCOMPUTATION FOR DATASET VI PROBLEM\nNow with these coreset VI problem solutions, we have to find the optimal S∗by solving Eq. 16.\nHowever, unlike the coreset VI problem, since we use a categorical likelihood with a softmax output,\na closed-form solution cannot be obtained from Eq. 16. Thus we have to use iterative updates,\nsuch as Stochastic Gradient Descent (SGD), for the outer optimization problem. Then because\nϕ(x)⊤wj ∼N(ϕ(x)⊤m∗\nj, ϕ(x)⊤V ∗ϕ(x)) for all j ∈[k], the dataset VI problem changed into\nLD(λ∗\nS) = −\nn\nX\ni=1\nk\nX\nj=1\nyi,jEz∼N( ¯\nm∗(xi),¯Σ∗(xi))\n\"\nlog\nexp zj\nPk\ni=1 exp zi\n#\n+ βDDKL\n\u0002\nqλ∗\nS||pλ0\n\u0003\n,\n(23)\nwhere z = [z1, . . . , zk], ¯m∗(x) = [ ¯m∗\n1(x), . . . , ¯m∗\nk(x)], ¯Σ∗(x) = diag([Σ∗\n1(x), . . . , Σ∗\nk(x)]) and\n( ¯m∗\ni (x), Σ∗\ni (x)) = (ϕ(x)⊤m∗\ni , ϕ(x)⊤V ∗ϕ(x)) for all j ∈[k] and x. For a simpler notation, we will\ndenote ( ¯m∗\ni (x), ¯Σ∗\ni (x)) as ( ¯m∗\ni , ¯Σ∗\ni ). Then we have to approximate Ez∼N( ¯\nm∗,¯Σ∗)\nh\nlog\nexp zj\nPk\ni=1 exp zi\ni\nto compute the loss LD(λ∗\nS) analytically. To compute approximate expectation for the likelihood,\nwe first change the form as follows:\nEz\n\"\nlog\nexp(zj)\nPk\ni=1 exp zi\n#\n=\nZ\nlog\n\n2 −K +\nX\ni̸=j\n1\nσ(zj −zi)\n\n\n−1\nN(z| ¯m∗, ¯Σ∗)dz,\n(24)\nwhere σ(·) is the sigmoid function. Then we utilize mean-field approximation (Lu et al., 2020) to\nthe zis to approximately compute the Eq. 24:\nEz∼N( ¯\nm∗,¯Σ∗)\n\"\nlog\nexp (zj)\nPt\ni=1 exp(zi)\n#\n≈\n\u0014\nlog softmax\n\u0012\n¯m∗\n√\n1 + αΣ∗\n\u0013\u0015\nj\n,\n(25)\nwhere α =\nπ\n8 and Σ∗= ϕ(x)⊤V ∗ϕ(x). Refer to Appendix A.3 for the complete derivation of\nEq. 23, Eq. 24, and Eq. 25. By Eq. 25, our outer optimization loss has changed form as follows:\nLD(λ∗\nS) = −\nn\nX\ni=1\nk\nX\nj=1\nyi,j log softmax\n\" \n¯m∗(xi)\np\n1 + αΣ∗(xi)\n!#\nj\n+ βDDKL[qλ∗\nS||pλ0].\n(26)\nHere, since n is large, we need to employ the SGD method to optimize S. Thus, using the training\nbatch B ⊂{(x1, y1), . . . , (xn, yn)}, we compute approximate loss ˜LD for the batch and update S\nusing stochastic loss as follows:\n˜LD(λ∗\nS) = −n\n|B|\nX\ni∈B\nk\nX\nj=1\nyi,j log softmax\n\" \n¯m∗(xi)\np\n1 + αΣ∗(xi)\n!#\nj\n+ βDDKL[qλ∗\nS||pλ0].\n(27)\n3.6\nTRAINING AND INFERENCE\nMemory Efficient Loss computing\nIf we naïvely compute the gradient of S by directly eval-\nuating Eq. 27, calculating Σ∗and DKL[qλ∗\nS||pλ0] will require computations involving V ∗, which\ndemands h2 memory. However, the quadratic memory requirements with respect to the feature di-\nmension pose a challenge when training S for large-scale models. To address this issue, we propose\na memory-efficient approach for computing loss during training in this paragraph. We will address\nthe efficient computation of Σ∗in the below paragraph Variational Inference and Memory Effi-\ncient Bayesian Model Averaging. Here, we will focus on efficiently computing the KL divergence.\nSince both qλ∗\nS and pλ0 are Gaussian distributions, the KL divergence can be expressed as follows:\nDKL[qλ∗\nS||pλ0]\nc= 1\n2\n\u0002\n−k log | det(V ∗)| + kρ Tr(V ∗) + ρ∥m∗∥2\u0003\n.\n(28)\nThus we have to efficiently compute det V ∗and Tr(V ∗).\nFor the det V ∗, we use Weinstein-\nAronszajn identity (Pozrikidis, 2014) which results as follows:\ndet V ∗=\n1\nρh det(Iˆn +\nγ\nρβS ΦΦ⊤).\n(29)\n6\n\n\nPublished as a conference paper at ICLR 2025\nAnd for the Tr(V ∗), we can easily change the form with a property of matrix trace computation:\nTr(V ∗) = βS\nγ\n \nγh\nρβS\n−\n\u0012 γ\nρβS\n\u00132\nTr\n \u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦΦ⊤\n!!\n.\n(30)\nBy these formula, we can calculate the KL divergence without directly computing V ∗, reducing the\nmemory from O(h2) to O(ˆn2). Refer to Appendix A.4 for the derivation of Eq. 28 and Eq. 29.\nModel Pool\nIf we train S based on only one ϕ, it may overfit to that single ϕ, resulting in an\ninability to properly generate the variational posterior for other ϕ’s. This overfitting issue is common\nnot only in Bayesian pseudo-coresets but also in the field of dataset distillation (Zhou et al., 2022).\nWhile several prior studies (Wang et al., 2018; 2022) tackle this overfitting problem, we address it\nby employing a model pool during training, following the approach of Zhou et al. (2022); Loo et al.\n(2023). This model pool method involves generating P different θi’s through random initialization\nduring the training of S and storing them in a set M = {θi}P\ni=1. At each step, one θ is sampled\nfrom M, and ϕ is constructed using this θ. Then, S is trained for one step using SGD with this\nϕ. Afterward, θ is updated by training it for one step using S and the Gaussian likelihood, and\nthe original θ in M is replaced with this updated version. Once each θi has been trained for a\npre-defined number of T steps, it is replaced with a new θ generated through random initialization.\nThrough this process, S is trained with a new ϕ at every step, allowing it to generalize better across\ndifferent ϕ’s and become more robust to various initialization. See Algorithm 1 for a summary of\nthe whole VBPC training procedure.\nVariational Inference and Memory Efficient Bayesian Model Averaging\nAfter training S, we\nuse it for variational inference. During variational inference, to improve the quality of the model’s\nfeature map ϕ, we first train the randomly initialized θ using data sampled from S for a small\nnumber of steps T ′ with a Gaussian likelihood. Then, using the trained feature map ϕ, we compute\nthe variational posterior by finding the optimal mean m∗\nj and variance V ∗for each θL\nj as determined\nin the inner optimization. However, the variance V ∗we computed corresponds to a full covariance\nmatrix, leading to a memory cost of h2. To address this, rather than calculating V ∗explicitly, we\nneed a memory-efficient approach for conducting BMA on test points. This can be done easily by :\nΣ∗= βS\nγ\n \nγ\nρβS\nΦteΦ⊤\nte −\n\u0012 γ\nρβS\n\u00132\nΦteΦ⊤\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦΦ⊤\nte\n!\n,\n(31)\nwhere Φte ∈Rnte×h denotes the feature matrix of nte number of test points. Then by storing Φ ∈\nRˆn×h and (Iˆn +\nγ\nρβS ΦΦ⊤)−1 ∈Rˆn×ˆn instead of V ∗, we can reduce the memory requirements\nto ˆnh + ˆn2, which is much smaller than h2. Refer to Algorithm 2 for an overview of variational\ninference and BMA. This procedure does not require multiple forwards for BMA.\n4\nRELATED WORKS\nBayesian Pseudo-Coreset\nAs discussed in Section 1 and Section 2, the large scale of modern\nreal-world datasets leads to significant computational costs when performing SGMCMC or varia-\ntional inference to approximate posterior distributions. To address this issue, previous works, such\nas Bayesian Coreset (BC; Campbell & Broderick, 2018; 2019; Campbell & Beronov, 2019), have\nproposed selecting a small subset from the full training dataset so that the posterior distribution built\nfrom this subset closely approximates the posterior from the full dataset. However, Manousakas\net al. (2020) highlighted that simply selecting a subset of the training data is insufficient to accu-\nrately approximate high-dimensional posterior distributions, and introduced BPC for simple logistic\nregression tasks. Later, Kim et al. (2022) extended BPC to BNNs, using reverse KL divergence,\nforward KL divergence, and Wasserstein distance as measures for D in Eq. 2 to assess the difference\nbetween the full posterior and the BPC posterior. Subsequent works have used contrastive diver-\ngence (Tiwary et al., 2024) or calculated divergence in function space (Kim et al., 2023). However,\nas discussed in Section 1, computational and memory overhead remains an issue when training BPC\nand during inference using BMA. For the additional related works, refer to Appendix C.\n7\n\n\nPublished as a conference paper at ICLR 2025\nFigure 1: Learned VBPC images for the Fashion-MNIST (ipc=10; left), CIFAR10 (ipc=10; middle)\nand CIFAR100 (ipc=1; right) cases. These images construct trained mean for the distribution S∗.\nTable 1: Comparison of the VBPC with BPC baselines for the benchmark datasets. We report ACC\nand NLL for the VBPC and BPC baselines.\nBPC-rKL\nBPC-fKL\nFBPC\nBPC-CD\nVBPC (Ours)\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nMNIST\n1\n74.8±1.2 1.90±0.01 83.0±2.2 1.87±0.03 92.5±0.1 1.68±0.01 93.4±0.1 1.53±0.01 96.7±0.4 0.11±0.02\n10 95.3±0.2 1.53±0.01 92.1±0.4 1.51±0.02 97.1±0.2 1.31±0.01 97.7±0.2 1.57±0.02 99.1±0.1 0.03±0.01\n50 94.2±0.3 1.36±0.02 93.6±1.8 1.36±0.02 98.6±0.1 1.39±0.02 98.9±0.2 1.36±0.01 99.4±0.1 0.02±0.01\nFMNIST\n1\n70.5±1.1 2.47±0.02 72.5±2.5 2.30±0.02 74.7±1.4 1.81±0.03 77.3±0.5 1.90±0.03 82.9±0.6 0.47±0.03\n10 78.8±0.2 1.64±0.01 83.3±0.6 1.54±0.03 85.2±0.1 1.61±0.02 88.4±0.2 1.56±0.01 89.4±0.2 0.30±0.01\n50 77.0±0.6 1.48±0.02 74.8±0.5 1.47±0.02 76.7±0.4 1.46±0.02 89.5±0.1 1.30±0.02 91.0±0.2 0.25±0.01\nCIFAR10\n1\n21.6±0.8 2.57±0.01 29.3±1.1 2.10±0.03 35.5±0.3 3.79±0.04 46.9±0.2 1.87±0.02 55.1±0.3 1.34±0.08\n10 37.9±1.5 2.13±0.02 49.9±1.4 1.73±0.01 62.3±0.3 1.31±0.02 56.4±0.7 1.72±0.03 69.8±0.7 0.89±0.02\n50 37.5±1.3 1.93±0.03 42.3±2.9 1.54±0.01 71.2±0.2 1.03±0.05 71.9±0.2 1.57±0.03 76.7±0.5 0.71±0.03\nCIFAR100\n1\n3.6±0.1 4.69±0.02 14.7±0.2 4.20±0.10 21.0±0.8 3.76±0.11 24.0±0.1 4.01±0.02 38.4±0.2 2.47±0.04\n10 23.6±0.7 3.99±0.03 28.1±0.6 3.53±0.05 39.7±0.3 2.67±0.02 28.4±0.2 3.14±0.02 49.4±0.1 2.07±0.02\n50 30.8±0.5 3.57±0.17 37.1±0.3 3.28±0.24 44.5±0.4 2.63±0.01 39.6±0.2 3.02±0.01 52.4±0.4 2.02±0.02\nTiny-ImageNet 1\n3.2±0.1 5.91±0.07 4.0±0.1 5.63±0.03 10.1±0.7 4.69±0.05 8.4±0.1 4.72±0.01 23.1±0.2 3.65±0.01\n10\n9.8±0.6 5.26±0.05 11.4±0.5 5.08±0.05 19.4±0.5 4.14±0.02 17.8±0.4 3.64±0.05 25.8±0.3 3.45±0.02\n5\nEXPERIMENT\nIn this section, we present empirical results that demonstrate the effectiveness of posterior approx-\nimation using VBPC across various datasets and scenarios. We compare VBPC with four BPC\nalgorithms that use SGMCMC to perform Bayesian Model Averaging (BMA) with posterior sam-\nples: BPC-rKL (Kim et al., 2022), BPC-fKL (Kim et al., 2022), FBPC (Kim et al., 2023), and\nBPC-CD (Tiwary et al., 2024). BPC-rKL and BPC-fKL employ reverse KL divergence and forward\nKL divergence, respectively, for the divergence term in Eq. 2. BPC-CD uses a more complex diver-\ngence called contrastive divergence, while FBPC also applies forward KL divergence but matches\nthe posterior distribution in function space rather than weight space. Following all other prior works,\nwe adopted a three-layer convolutional network with Batch Normalization (BN; Ioffe, 2015) as the\nbase model architecture. For the target dataset, we used the MNIST (LeCun et al., 1998), Fashion-\nMNIST (Xiao et al., 2017), CIFAR10/100 (Krizhevsky, 2009), and Tiny-ImageNet (Le & Yang,\n2015). Additionally, we used image-per-class (ipc) as the unit to count the number of pseudo-\ncoresets. For a k-way classification task, m ipc signifies that a total of mk pseudo-coresets are\ntrained. Along with evaluating classification accuracy (ACC) for each methods, we assess the per-\nformance of the resulting predictive distributions using negative log-likelihood (NLL).\nIn all tables, the best performance is indicated with boldfaced underline, while the second-best value\nis represented with underline in each row. See Appendix E for the additional experimental details.\n5.1\nBAYESIAN MODEL AVERAGING COMPARISON\nWe begin by evaluating the effectiveness of VBPC on five benchmark datasets by comparing the\nBMA performance across different methods. Table 1 clearly demonstrates that VBPC surpasses\nother BPC baselines across all benchmark datasets and ipcs in terms of ACC and NLL. Notably,\nVBPC achieves significantly better NLL, with large margins, while requiring only a single forward\n8\n\n\nPublished as a conference paper at ICLR 2025\nTable 2: Comparison with dataset distillation baselines in terms of ACC. Here, ↓indicates the\nperformance drop compare to original method.\nDataset\nipc FRePo\nFRePo VI\nRCIG\nRCIG VI\nVBPC\nAVBPC\nCIFAR10\n1\n46.8±0.7 28.2(18.6↓)±0.9 53.9±1.0 27.8(24.1↓)±0.7 55.1±0.3 39.7(15.4↓)±1.5\n10 65.5±0.4 55.7(9.8↓)±0.5 69.1±0.4 55.6(13.8↓)±1.5 69.8±0.7 67.8(2.0↓)±0.8\nCIFAR100 1\n28.7±0.1 19.9(8.8↓)±0.4 39.3±0.4 2.1(37.2↓)±0.1 38.4±0.2 31.3(7.1↓)±1.0\n10 42.5±0.2 34.8(7.7↓)±0.4 44.1±0.4 2.5(41.6↓)±0.4 49.4±0.1 44.0(5.4↓)±0.8\nTable 3: Comparison of the VBPC with BPC baselines on the OOD setting with CIFAR10-C dataset.\nThe +A in the first column indicates that A type corruption is applied to the CIFAR10 test dataset.\nBPC-rKL\nBPC-fKL\nFBPC\nBPC-CD\nVBPC (Ours)\nCorruption\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR10\n37.9±1.5 2.13±0.02 49.9±1.4 1.73±0.01 62.3±0.3 1.31±0.02 56.4±0.7 1.72±0.03 69.8±0.7 0.89±0.02\n+Gaussian Blur\n31.0±2.7 2.13±0.77 39.7±2.7 1.94±0.05 35.8±0.2 1.85±0.08 41.4±0.7 1.73±0.83 59.3±0.9 1.20±0.03\n+JPEG Compression 30.4±0.9 2.13±0.02 37.3±2.9 1.95±0.06 40.1±0.1 1.73±0.02 37.3±0.2 1.71±0.03 61.9±0.8 1.12±0.02\n+Snow\n26.9±1.7 2.20±0.07 35.7±2.7 2.00±0.07 38.6±0.4 1.78±0.16 37.8±0.6 1.91±0.05 59.0±0.1 1.20±0.02\n+Zoom Blur\n31.7±1.2 2.09±0.04 35.1±2.9 2.04±0.07 28.9±0.2 2.19±0.11 38.3±0.8 1.93±0.13 58.1±0.8 1.23±0.04\n+Pixelate\n29.0±2.3 2.19±0.07 39.1±3.2 1.93±0.06 38.0±0.3 1.77±0.04 39.0±1.5 1.92±0.07 58.8±0.9 1.26±0.04\n+Defocus Blur\n27.6±1.3 2.20±0.05 36.7±3.7 1.99±0.08 31.7±0.4 2.07±0.19 37.2±1.0 1.87±0.04 63.0±0.7 1.08±0.02\n+Motion Blur\n17.4±2.5 2.73±0.14 35.2±3.3 2.01±0.05 27.9±0.2 2.29±0.15 37.1±0.5 1.92±0.04 55.9±0.5 1.32±0.03\npass for BMA. These results empirically validate that the variational distribution trained by VBPC\neffectively captures epistemic uncertainty with a small amount of synthetic data, while keeping\nperformance. Refer to Fig. 1 for examples of VBPC-trained images from the Fashion-MNIST, CI-\nFAR10, and CIFAR100 datasets. For more trained VBPC images for other settings, see Appendix G.\nComparison with dataset distillation baselines\nIn addition to the BPC baselines, we compared\nVBPC with two notable dataset distillation baselines, FRePo (Zhou et al., 2022) and RCIG (Loo\net al., 2023), which are recognized for their strong accuracy performance. Since FRePo and RCIG\ndo not employ cross-entropy loss for training, we only report ACC, as comparing NLL would be\nunfair. As shown in Table 2, although VBPC is designed to learn pseudo-coresets to approximate\nthe variational distribution from the training data, it outperforms these dataset distillation baselines,\nfocused mainly on ACC, in nearly all tasks except for CIFAR100 with 1 ipc. The results for each\nmethods (i.e., FRePo, RCIG, and VBPC) in Table 2 were evaluated based on each baseline’s evalu-\nation methods. However, one might question whether the significant performance of VBPC is due to\nthe trained pseudo-coreset itself or the VI method. To verify that VBPC’s performance isn’t solely\ndue to the VI method, we applied our VI method to the baselines’ pseudo-coresets (i.e., FRePo VI\nand RCIG VI) and used FRePo’s method to evaluate VBPC’s pseudo-coresets (i.e., AVBPC). Al-\nthough all methods saw some performance decline, VBPC exhibited a smaller drop, indicating that\nits performance is not solely due to the VI method, but to its ability to effectively learn the varia-\ntional distribution. Full comparisons across all benchmark datasets, available in Appendix F.1, show\nthat VBPC maintains a consistent trend over dataset distillation baselines across all the datasets.\n5.2\nRESULTS ON OUT OF DISTRIBUTION SCENARIOS\nTo further demonstrate that the predictive distribution derived from the VBPC dataset enhances\nrobustness to distributional shifts and out-of-distribution (OOD) data, we assess the performance\nof VBPC and BPC baselines on a corrupted version of the CIFAR10 dataset, known as CIFAR10-\nC (Hendrycks & Dietterich, 2019). In this case, we use the CIFAR10 10ipc BPC data trained in\nSection 5.1 for all methods and evaluate their performance on the corrupted dataset across 7 different\ntypes of corruption. We assess performance using all 5 levels of severity provided in the dataset.\nTable 3 clearly illustrates that VBPC shows strong robustness against various types of corruption and\nconsistently outperforms other baselines across all corruption types in terms of both ACC and NLL.\nThese findings highlight that the predictive distribution obtained from the VBPC dataset improves\nrobustness to distributional shifts and OOD scenarios.\n9\n\n\nPublished as a conference paper at ICLR 2025\nTable 4: Comparison of the VBPC with BPC baselines on the architecture generalization. The\nA −B in the first column indicates that B type normalization layer is used for the A model.\nBPC-rKL\nBPC-fKL\nFBPC\nBPC-CD\nVBPC (Ours)\nModel\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nConv-BN\n37.9±1.5 2.13±0.02 49.9±1.4 1.73±0.01 62.3±0.3 1.31±0.02 56.4±0.7 1.72±0.03 69.8±0.7 0.89±0.02\nConv-NN\n23.1±3.8 2.22±0.02 22.9±4.4 2.12±0.04 28.6±4.8 2.17±0.02 30.1±4.4 2.05±0.19 58.4±0.8 1.46±0.05\nConv-GN\n28.5±4.5 2.85±0.23 29.1±4.4 2.81±0.24 31.5±5.2 1.93±0.01 23.8±4.2 3.07±0.42 66.8±0.6 0.95±0.04\nConv-IN\n26.7±4.3 2.81±0.22 27.7±4.7 2.82±0.25 31.7±5.3 1.96±0.09 26.9±4.4 3.29±0.27 58.1±0.8 1.22±0.12\nAlexNet-NN 24.2±3.8 2.23±0.01 21.4±4.3 2.82±0.24 32.1±0.9 2.91±0.05 30.8±1.4 2.24±0.11 48.0±0.4 1.94±0.05\nResNet18-BN 9.6±2.6 3.27±0.15 10.5±4.5 3.16±0.14 46.7±1.2 1.81±0.08 41.7±1.1 2.05±0.27 54.9±0.5 1.36±0.05\nVGG11-GN\n10.0±2.9 2.94±0.11 10.1±3.0 2.85±0.11 37.2±0.9 1.40±0.05 44.5±1.2 1.78±0.12 52.4±1.1 1.44±0.15\nTable 5: Ablation results on memory allocation and time requirements on CIFAR10 10ipc.\nNaïve Training\nTraining (Ours)\nNaïve BMA\nBMA (Ours)\nMemory (MB) sec/100 steps Memory (MB) sec/100 steps Memory (MB) Memory (MB)\n542.9\n54.0\n272.9\n9.9\n542.9\n268.9\n5.3\nARCHITECTURE GENERALIZATION\nTo demonstrate that VBPC can be applied when performing BMA on unseen architectures, we\nconduct BMA using different model structures with various normalization layers.\nSpecifically,\nwe include the identity layer (NN), Group Normalization (GN; Wu & He, 2018), and Instance\nNormalization (IN; Ulyanov, 2016) as additional normalization methods.\nWe also incorporate\nAlexNet (Krizhevsky et al., 2012), ResNet18 (He et al., 2016), and VGG11 (Simonyan & Zis-\nserman, 2014) as new model architectures. Similar to Section 5.2, we use the CIFAR10 10ipc BPC\ndata. As shown in Table 4, VBPC successfully performs VI across various architectures and effec-\ntively constructs predictive distributions through BMA. Notably, while other baselines are sensitive\nto changes in normalization layers, VBPC demonstrates robust learning over diverse feature maps\nthrough the model pool, resulting in strong ACC and NLL performance.\n5.4\nMEMORY ALLOCATION AND TIME REQUIREMENTS\nIn this section, we perform an ablation study to compare memory usage and time requirements be-\ntween the naive computation and the efficient computation for the variance V ∗, Σ∗, and the loss\nterms during both training and inference. As we discussed in Section 3.6, naive loss computa-\ntion requires O(h2) space complexity and O(h3) computational complexity. However, our com-\nputationally efficient loss computation method only requires O(ˆn2) space complexity and O(ˆn3)\ncomputational complexity. Therefore, in the BPC setting where ˆn ≪h typically holds, we can sig-\nnificantly reduce the space and computational complexity required for training. This difference can\nbe observed during the actual training process. As shown in Table 5, our computationally efficient\ntraining reduces the memory requirements for loss computation by nearly half and decreases the\ntraining time to under 20%. Also, we can see the similar results during the BMA procedure. Refer\nto Appendix F to see the various additional ablation studies including ablation on hyperparameters,\npseudo-coreset initialization, and augmentations.\n6\nCONCLUSION\nIn this paper, we present a novel BPC method for VI, referred to as VBPC. By utilizing the Gaussian\nlikelihood, we enable the computation of a closed-form solution for coreset VI, thereby removing the\nneed to unroll the computation graph or use stop gradients. Leveraging this closed-form solution, we\npropose a method to approximate dataset VI without weight sampling during the training of VBPC.\nAdditionally, we introduce a computationally efficient training and BMA inference method that sig-\nnificantly reduces both computational and space complexity. Finally, we empirically show that the\nvariational distribution obtained from VBPC substantially outperforms the predictive distributions\nderived from other BPC baselines in BMA performance across various scenarios.\n10\n\n\nPublished as a conference paper at ICLR 2025\nReproducibility Statement.\nWe present comprehensive derivations of all equations in the paper\nin Appendix A. The overall algorithms can be found in Appendix B. Details regarding the datasets,\nmodel architecture, data preprocessing, and hyperparameters are provided in Appendix E.\nEthics Statement.\nWe propose a method that improves the computational and memory efficiency\nof the variational inference method for posterior approximation in Bayesian Neural Networks. Thus\nalthough our approach does not have a direct positive or negative impact on ethical or societal\naspects, it can enhance privacy preservation. Specifically, our method facilitates Bayesian inference\nusing private training data in neural network models by generating synthetic datasets, allowing for\nthe computation of the posterior distribution while maintaining privacy.\nAcknowledgements\nThis work was partly supported by Institute of Information & communi-\ncations Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT)\n(No.RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST)), the National\nResearch Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (RS-2021-\nNR056917), Institute of Information & communications Technology Planning & Evaluation(IITP)\ngrant funded by the Korea government(MSIT) (No.RS-2024-00509279, Global AI Frontier Lab),\nand Institute of Information & communications Technology Planning & Evaluation(IITP) grant\nfunded by the Korea government(MSIT) (No.RS-2022-II220713, Meta-learning Applicable to Real-\nworld Problems).\nREFERENCES\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris\nOlah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\nVincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wat-\ntenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learn-\ning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software\navailable from tensorflow.org. 22\nAbdullah A Abdullah, Masoud M Hassan, and Yaseen T Mustafa. A review on bayesian deep\nlearning in healthcare: Applications and challenges. IEEE Access, 10:36538–36562, 2022. 1\nAF Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375,\n2018. 23\nSungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via stochastic\ngradient fisher scoring. arXiv preprint arXiv:1206.6380, 2012. 20\nS. Amari. Natural gradient works efficiently in learning. Neural Computation, 10:251–276, 1998. 3\nIgor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky,\nDavid Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci,\nJonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou,\nSteven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena\nMartens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan,\nRoman Ring, Francisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer,\nSrivatsan Srinivasan, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The\nDeepMind JAX Ecosystem, 2020. URL http://github.com/deepmind. 22\nYoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889–\n1900, 2000. 4\nChristopher M Bishop. Pattern recognition and machine learning, volume 4. Springer, 2006. 3, 21\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statis-\nticians. Journal of the American statistical Association, 112(518):859–877, 2017. 1, 2, 3, 20,\n21\n11\n\n\nPublished as a conference paper at ICLR 2025\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty\nin neural network. In Proceedings of The 32nd International Conference on Machine Learning\n(ICML 2015), 2015. 3, 21\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:\n//github.com/google/jax. 22\nTrevor Campbell and Boyan Beronov. Sparse variational inference: Bayesian coresets from scratch.\nIn Advances in Neural Information Processing Systems 32 (NeurIPS 2019), 2019. 7, 20\nTrevor Campbell and Tamara Broderick. Bayesian coreset construction via greedy iterative geodesic\nascent. In Proceedings of The 35th International Conference on Machine Learning (ICML 2018),\n2018. 7, 20\nTrevor Campbell and Tamara Broderick. Automated scalable bayesian inference via hilbert coresets.\nJournal of Machine Learning Research, 20(15):1–38, 2019. 7, 20\nGeorge Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset\ndistillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 4750–4759, 2022. 21\nTianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In\nProceedings of The 31st International Conference on Machine Learning (ICML 2014), 2014. 1,\n2, 20\nYL Cun, L Bottou, G Orr, and K Muller. Efficient backprop, neural networks: tricks of the trade.\nLecture notes in computer sciences, 1524:5–50, 1998. 23\nMichael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji\nLakshminarayanan, and Dustin Tran. Efficient and scalable bayesian neural nets with rank-1\nfactors. In International conference on machine learning, pp. 2782–2792. PMLR, 2020. 21\nFelix Fiedler and Sergio Lucia.\nImproved uncertainty quantification for neural networks with\nbayesian last layer. IEEE Access, 2023. 1, 2, 3, 20, 21\nJ. Harrison, J. Willes, and J. Snoek. Variational Bayesian last layers. In International Conference\non Learning Representations (ICLR), 2024a. 21\nJames Harrison, John Willes, and Jasper Snoek. Variational bayesian last layers. arXiv preprint\narXiv:2404.11599, 2024b. 1, 2, 3, 22\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016. 10, 23\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. In International Conference on Learning Representations (ICLR),\n2019. 9, 23\nJeremy Howard. A smaller subset of 10 easily classified classes from imagenet, and a little more\nfrench, 2020. URL https://github.com/fastai/imagenette/. 25\nSergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covari-\nate shift. arXiv preprint arXiv:1502.03167, 2015. 8, 23\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and gener-\nalization in neural networks. In Advances in Neural Information Processing Systems 31 (NeurIPS\n2018), 2018. 21\nAgnan Kessy, Alex Lewin, and Korbinian Strimmer. Optimal whitening and decorrelation. The\nAmerican Statistician, 72(4):309–314, 2018. 24\n12\n\n\nPublished as a conference paper at ICLR 2025\nM. E. Khan and H. Rue. The Bayesian learning rule. Journal of Machine Learning Research, 24:\n1–46, 2023. 3\nM. E. Khan, D. Nielsen, V. Tangkaratt, W. Lin, Y. Gal, and A. Srivastava. Fast and scalable bayesian\ndeep learning by weight-perturbation in Adam. In Proceedings of The 35th International Confer-\nence on Machine Learning (ICML 2018), 2018. 3\nBalhae Kim, Jungwon Choi, Seanie Lee, Yoonho Lee, Jung-Woo Ha, and Juho Lee. On divergence\nmeasures for bayesian pseudocoresets. In Advances in Neural Information Processing Systems 35\n(NeurIPS 2022), 2022. 1, 2, 7, 8, 20, 23, 24, 27, 29\nBalhae Kim, Hyungi Lee, and Juho Lee. Function space bayesian pseudocoreset for bayesian neural\nnetworks. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023. 1,\n2, 7, 8, 20, 23, 24, 27, 29\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014. 24\nP. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In Proceedings\nof The 34th International Conference on Machine Learning (ICML 2017), 2017. 5\nSteven George Krantz and Harold R Parks. The implicit function theorem: history, theory, and\napplications. Springer Science & Business Media, 2002. 4\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University\nof Tront, 2009. 8, 22\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\nlutional neural networks. In Advances in Neural Information Processing Systems 25 (NIPS 2012),\n2012. 10, 23\nYa Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 8, 22\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 8, 22\nHyungi Lee, Eunggu Yun, Hongseok Yang, and Juho Lee. Scale mixtures of neural network gaussian\nprocesses. In International Conference on Learning Representations (ICLR), 2022. 5\nHyungi Lee, Giung Nam, Edwin Fong, and Juho Lee. Enhancing transfer learning with flexible non-\nparametric posterior sampling. In International Conference on Learning Representations (ICLR),\n2024. 2\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha\nSohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,\n2017. 5, 21\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear mod-\nels under gradient descent. In Advances in Neural Information Processing Systems 32 (NeurIPS\n2019), 2019. 5\nNoel Loo, Ramin Hasani, Mathias Lechner, and Daniela Rus. Dataset distillation with convexified\nimplicit gradients. In Proceedings of The 39th International Conference on Machine Learning\n(ICML 2023), 2023. 1, 4, 7, 9, 21\nL Lopez, Tim GJ Rudner, and Farah E Shamout.\nInformative priors improve the reliability of\nmultimodal clinical data classification. arXiv preprint arXiv:2312.00794, 2023. 1\nJ. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit dif-\nferentiation. In Proceedings of The 23rd International Conference on Artificial Intelligence and\nStatistics (AISTATS 2020), 2020. 4\nZhiyun Lu, Eugene Ie, and Fei Sha. Mean-field approximation to gaussian-softmax integral with\napplication to uncertainty estimation. arXiv preprint arXiv:2006.07584, 2020. 6, 18\n13\n\n\nPublished as a conference paper at ICLR 2025\nYi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc. In\nAdvances in Neural Information Processing Systems 28 (NIPS 2015), 2015. 1, 2, 20\nDionysis Manousakas, Zuheng Xu, Cecilia Mascolo, and Trevor Campbell. Bayesian pseudocore-\nsets. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), 2020. 1, 7,\n20\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning. Granada, 2011. 26\nTimothy Nguyen, Zhourong Chen, and Jaehoon Lee.\nDataset meta-learning from kernel ridge-\nregression. arXiv preprint arXiv:2011.00050, 2020. 1, 21\nTimothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely\nwide convolutional networks. In Advances in Neural Information Processing Systems 34 (NeurIPS\n2021), 2021. 1\nK. Osawa, S. Swaroop, A. Jain, R. Eschenhagen, R. E. Turner, R. Yokota, and M. E. Khan. Practical\ndeep learning with Bayesian principles. In Advances in Neural Information Processing Systems\n32 (NeurIPS 2019), 2019. 3\nTheodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel,\nDavid Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, José Miguel Hernández-\nLobato, et al. Position: Bayesian deep learning is needed in the age of large-scale ai. In Interna-\ntional Conference on Learning Representations (ICLR), 2024. 1, 2\nConstantine Pozrikidis. An introduction to grids, graphs, and networks. Oxford University Press,\nUSA, 2014. 6, 19\nTim GJ Rudner, Zonghao Chen, and Yarin Gal. Rethinking function-space variational inference in\nbayesian neural networks. In Third Symposium on Advances in Approximate Bayesian Inference,\n2021. 20\nTim GJ Rudner, Freddie Bickford Smith, Qixuan Feng, Yee Whye Teh, and Yarin Gal. Contin-\nual learning via sequential function-space variational inference. In International Conference on\nMachine Learning, pp. 18871–18887. PMLR, 2022. 20\nEvgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann,\nMatthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against di-\nverse image corruptions. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part III 16, pp. 53–69. Springer, 2020. 30\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115:211–252, 2015. 25\nS. Sharm, M. nd Farquhr, E. Nalisnick, and T. Rainforth. Do Bayesian neural networks need to be\nfully stochastic? In Proceedings of The 26th International Conference on Artificial Intelligence\nand Statistics (AISTATS 2023), 2023. 4\nY. Shen, N. Daheim, B. Cong, P. Nickl, G. M. Marconi, C. Bazan, R. Yokota, I. Gurevych, D. Cre-\nmers, M. E. Khan, and T. Möller. Variational learning is effective for large deep networks. In\nProceedings of The 40th International Conference on Machine Learning (ICML 2024), 2024. 3,\n4, 21\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. In International Conference on Learning Representations (ICLR), 2014. 10, 23\nPiyush Tiwary, Kumar Shubham, Vivek V Kashyap, and AP Prathosh. Bayesian pseudo-coresets via\ncontrastive divergence. In The 40th Conference on Uncertainty in Artificial Intelligence, 2024. 1,\n2, 7, 8, 20, 23, 24, 27, 29\n14\n\n\nPublished as a conference paper at ICLR 2025\nAntonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data set for\nnonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 30(11):1958–1970, 2008. doi: 10.1109/TPAMI.2008.128. 22\nD Ulyanov. Instance normalization: The missing ingredient for fast stylization. arXiv preprint\narXiv:1607.08022, 2016. 10, 23\nThomas Vandal, Evan Kodra, Jennifer Dy, Sangram Ganguly, Ramakrishna Nemani, and Auroop R\nGanguly. Quantifying uncertainty in discrete-continuous and skewed data with bayesian deep\nlearning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pp. 2377–2386, 2018. 1\nPaul Vicol, Luke Metz, and Jascha Sohl-Dickstein. Unbiased gradient estimation in unrolled com-\nputation graphs with persistent evolution strategies. In Proceedings of The 38th International\nConference on Machine Learning (ICML 2021), 2021. 5\nChong Wang and David M Blei. Variational inference in nonconjugate models. Journal of Machine\nLearning Research, 2013. 21\nKai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan\nBilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n12196–12205, 2022. 7\nTongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv\npreprint arXiv:1811.10959, 2018. 7, 20\nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In\nProceedings of The 28th International Conference on Machine Learning (ICML 2011), 2011. 1,\n2, 20\nFlorian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub ´Swi ˛atkowski, Linh Tran, Stephan Mandt,\nJasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes\nposterior in deep neural networks really? arXiv preprint arXiv:2002.02405, 2020. 3\nPaul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the\nIEEE, 78(10):1550–1560, 1990. 5\nMax A Woodbury. Inverting modified matrices. Department of Statistics, Princeton University,\n1950. 5, 16, 17\nYuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on\ncomputer vision (ECCV), pp. 3–19, 2018. 10, 23\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-\ning machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. 8, 22\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan\nSong, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\nLarge batch optimization for deep\nlearning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019. 28\nBo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In\nProceedings of The 38th International Conference on Machine Learning (ICML 2021), 2021. 21,\n26\nBo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision, pp. 6514–6523, 2023. 21\nYongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regres-\nsion. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022. 1, 7, 9,\n21, 22, 23, 24, 26, 27, 28, 29\n15\n\n\nPublished as a conference paper at ICLR 2025\nA\nFULL DERIVATIONS\nA.1\nFULL DERIVATION FOR THE INNER OPTIMIZATION\nIn this section, we present the full derivation calculation for the inner optimization in Section 3.4.\nLet us first examine the term Eqλ\nh\n−Pˆn\ni=1 log N(ˆyi|W ⊤ϕ(ˆxi), γ−1Ik)\ni\n, which can be computed\nas follows:\nEqλ\n\"\n−\nˆn\nX\ni=1\nlog N(ˆyi|W ⊤ϕ(ˆxi), γ−1Ik)\n#\n= −\nˆn\nX\ni=1\nEqλ\n\u0002\nlog N(ˆyi|W ⊤ϕ(ˆxi), γ−1Ik)\n\u0003\n(32)\n=\nˆn\nX\ni=1\nEqλ\nhγ\n2 ||ˆyi −ϕ(ˆxi)⊤W||2i\n+ constant\n(33)\n= γ\n2\nˆn\nX\ni=1\nk\nX\nj=1\nEqλ\nh\u0000ˆyi,j −ϕ(ˆxi)⊤wj\n\u00012i\n+ constant\n(34)\nc= γ\n2\nˆn\nX\ni=1\nk\nX\nj=1\nEqλ\nh\u0000ˆyi,j −ϕ(ˆxi)⊤wj\n\u00012i\n,\n(35)\nwhere ˆyi,j indicates jth element of ˆyi for all i ∈[ˆn]. With this approximation, now we can compute\nEqλ\nh\n−Pˆn\ni=1 log N(ˆyi|W ⊤ϕ(ˆxi), γ−1Ik)\ni\nas follows:\nEqλ\n\"\n−\nˆn\nX\ni=1\nlog N(ˆyi|W ⊤ϕ(ˆxi), γ−1Ik)\n#\nc= γ\n2\nˆn\nX\ni=1\nk\nX\nj=1\nEqλ\nh\u0000ˆyi,j −ϕ(˜xi)⊤wj\n\u00012i\n(36)\n= γ\n2\nk\nX\nj=1\nEqλ\n\u0002\nˆy⊤\nj ˆyj −2ˆy⊤\nj Φwj + w⊤\nj Φ⊤Φwj\n\u0003\n(37)\nc= γ\n2\nk\nX\nj=1\n\u0000−2ˆy⊤\nj Φmj + Eqλ\n\u0002\nw⊤\nj Φ⊤Φwj\n\u0003\u0001\n(38)\n= γ\n2\nk\nX\nj=1\n\u0000−2ˆy⊤\nj Φmj + Tr\n\u0000Φ⊤ΦEqλ\n\u0002\nwjw⊤\nj\n\u0003\u0001\u0001\n(39)\n= γ\n2\nk\nX\nj=1\n\u0000−2ˆy⊤\nj Φmj + Tr\n\u0000Φ⊤Φ\n\u0002\nVj + mjm⊤\nj\n\u0003\u0001\u0001\n(40)\n= γ\n2\nk\nX\nj=1\n\u0010\n−2ˆy⊤\nj Φµ(1)\nj\n+ Tr\n\u0010\nΦ⊤Φµ(2)\nj\n\u0011\u0011\n,\n(41)\nwhere ˆyj := [ˆy1,j, . . . , ˆyˆn,j]⊤, Φ := [ϕ(ˆx1), . . . , ϕ(ˆxˆn)], µ(1)\nj\n= mj, and µ(2)\nj\n= Vj + mjm⊤\nj\nfor all j ∈[k]. Here,\nc= denotes equality up to a constant. Eq. 39 derived from the fact that\nEqλ\n\u0002\nw⊤\nj Φ⊤Φwj\n\u0003\nis scalar value and the property of the Tr function.\nA.2\nNUMERICALLY STABLE MEAN AND VARIANCE\nIn this section, we present the full derivation calculation for the numerically stable mean and vari-\nance in Section 3.4. Due to the dimension of Φ is ˆn × h and usually ˆn ≪h, naïve computation of\nm∗\nj and V ∗\nj lead numerically unstable results. To address this issue, we transformed the formulas\nfor m∗\nj and V ∗\nj into equivalent but more numerically stable forms. Specifically, when calculating\nV ∗\nj , we applied the Woodbury formula (Woodbury, 1950). First, we utilize the kernel trick to make\n16\n\n\nPublished as a conference paper at ICLR 2025\nmean m∗\nj more numerically stable. The derivation is as follows:\nm∗\nj = −1\n2λ(2)∗−1\nj\nλ(1)∗\nj\n(42)\n= −1\n2(−ρ\n2Ih −\nγ\n2βS\nΦ⊤Φ)−1 γ\nβS\nΦ⊤ˆyj\n(43)\n= 1\n2(ρ\n2Ih +\nγ\n2βS\nΦ⊤Φ)−1 γ\nβS\nΦ⊤ˆyj\n(44)\n= (ρIh + γ\nβS\nΦ⊤Φ)−1 γ\nβS\nΦ⊤(ρIˆn + γ\nβS\nΦΦ⊤)(ρIˆn + γ\nβS\nΦΦ⊤)−1ˆyj\n(45)\n= γ\nβS\n(ρIh + γ\nβS\nΦ⊤Φ)−1(ρΦ⊤+ γ\nβS\nΦ⊤ΦΦ⊤)(ρIˆn + γ\nβS\nΦΦ⊤)−1ˆyj\n(46)\n= γ\nβS\nΦ⊤(ρIˆn + γ\nβS\nΦΦ⊤)−1ˆyj\n(47)\n= Φ⊤(ρβS\nγ Iˆn + ΦΦ⊤)−1ˆyj.\n(48)\nNext, we utilize the Woodbury formula (Woodbury, 1950) to make variance V ∗\nj more numerically\nstable. The derivation is as follows:\nV ∗\nj = βS\nγ\n\u0012ρβS\nγ Ih + Φ⊤Φ\n\u0013−1\n(49)\n= βS\nγ\n\n\n\u0012ρβS\nγ Ih\n\u0013−1\n−\n\u0012ρβS\nγ Ih\n\u0013−1\nΦ⊤\n \nI−1\nˆn\n+ Φ\n\u0012ρβS\nγ Ih\n\u0013−1\nΦ⊤\n!−1\nΦ\n\u0012ρβS\nγ Ih\n\u0013−1\n\n\n(50)\n= βS\nγ\n \nγ\nρβS\nIh −\n\u0012 γ\nρβS\n\u00132\nΦ⊤\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦ\n!\n(51)\n= 1\nρIh −\nγ\nρ2βS\nΦ⊤\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦ.\n(52)\nIt is important to note that for all j ∈[k], the V ∗\nj values are identical. This implies that while\ncalculating the full covariance for all j ∈[k] can be computationally intensive (i.e. O(kh2)), we\nonly need to compute and store the variance once (i.e. O(h2)).\nA.3\nFULL DERIVATION FOR OUTER OPTIMIZATION PROBLEM\nIn this section, we present the full derivation for the outer optimization problem. Here, we first\nchange LD(λ∗\nS) as follows:\nLD(λ∗\nS) = EθL∼qλ∗\nS [−\nn\nX\ni=1\nlog pD(yi|xi, θL)] + βDDKL[qλ∗\nS||pλ0]\n(53)\n= Eqλ∗\nS\n\n−\nn\nX\ni=1\nk\nX\nj=1\nyi,j log\nexp(ϕ(xi)⊤wj)\nPk\nl=1 exp(ϕ(x)⊤wl)\n\n+ βDDKL[qλ∗\nS(W)∥pλ0(W)]\n(54)\n= −\nn\nX\ni=1\nk\nX\nj=1\nyi,jEqλ∗\nS\n\"\nlog\nexp(ϕ(xi)⊤wj)\nPk\nl=1 exp(ϕ(x)⊤wl)\n#\n+ βDDKL[qλ∗\nS(W)∥pλ0(W)].\n(55)\n17\n\n\nPublished as a conference paper at ICLR 2025\nNext, in order to compute approximate expectation Ez∼N( ¯\nm∗,¯Σ∗)\nh\nlog\nexp zj\nPk\ni=1 exp zi\ni\n, we first change\nthe form as follows:\nEz∼N( ¯\nm∗,¯Σ∗)\n\"\nlog\nexp(zj)\nPk\ni=1 exp zi\n#\n=\nZ\nlog\nexp zj\nPk\ni=1 exp zi\nN(z| ¯m∗, ¯Σ∗)dz\n(56)\n=\nZ\nlog\n1\n1 + P\ni̸=j exp(−(zj −zi))N(z| ¯m∗, ¯Σ∗)dz\n(57)\n=\nZ\nlog(1 +\nX\ni̸=j\nexp(−(zj −zi)))−1N(z| ¯m∗, ¯Σ∗)dz\n(58)\n=\nZ\nlog(2 −K +\nX\ni̸=j\n(1 + exp(−(zj −zi))))−1N(z| ¯m∗, ¯Σ∗)dz\n(59)\n=\nZ\nlog(2 −K +\nX\ni̸=j\n1\nσ(zj −zi))−1N(z| ¯m∗, ¯Σ∗)dz,\n(60)\nwhere σ(·) is the sigmoid function. Then we utilize mean-field approximation (Lu et al., 2020) to\nthe zis to approximately compute the Eq. 24:\nEz∼N(m∗,¯Σ∗)\n\"\nlog\nexp (zj)\nPt\ni=1 exp(zi)\n#\n≈log\n\n2 −k +\nX\ni̸=j\n1\nE(zj,zi)∼N( ¯m∗\nj,i,¯Σ∗\nj,i)[σ(zj −zi)]\n\n\n−1\n(61)\n≈log\n\n\n\n2 −K +\nX\ni̸=j\n1\nσ\n\u0012\n¯m∗\nj −¯m∗\ni\n√\n1+α¯Σ∗\nj\n\u0013\n\n\n\n\n−1\n(62)\n= log\n1\n1 + P\ni̸=j exp(−\n¯m∗\nj −¯m∗\ni\n√\n1+α¯Σ∗\nj )\n(63)\n=\n\nlog softmax\n\n\n¯m∗\nq\n1 + α¯Σ∗\nj\n\n\n\n\nj\n(64)\n=\n\u0014\nlog softmax\n\u0012\n¯m∗\n√\n1 + αΣ∗\n\u0013\u0015\nj\n,\n(65)\nwhere α = π\n8 and Σ∗= ϕ(x)⊤V ∗ϕ(x).\nA.4\nFULL DERIVATION FOR TRAINING AND INFERENCE\nIn this section, we present the full derivation for the training and inference. Since both qλ∗\nS and pλ0\nare Gaussian distributions, the KL divergence can be expressed as follows:\nDKL[qλ∗\nS||pλ0] = 1\n2[k log | det(ρ−1Ih)|\n| det(V ∗)|\n−kh + kTr(ρI−1\nh V ∗) +\nk\nX\nj=1\n(m∗\nj)⊤(ρI−1\nh )m∗\nj]\n(66)\n= 1\n2[k log | det(ρ−1Ih)|\n| det(V ∗)|\n−kh + kTr(ρV ∗) + ρ∥m∗∥2]\n(67)\nc= 1\n2[−k log | det(V ∗)| + kρTr(V ∗) + ρ∥m∗∥2].\n(68)\nHere, we have to reduce the memory requirements for the det(V ∗) and the Tr(V ∗) as they require\nO(h2) memory to compute directly from V ∗. For the det V ∗, we used Weinstein-Aronszajn iden-\n18\n\n\nPublished as a conference paper at ICLR 2025\ntity (Pozrikidis, 2014) which results as follows:\ndet V ∗= det(ρIh + γ\nβS\nΦ⊤Φ)−1\n(69)\n=\n1\ndet(ρIh +\nγ\nβS Φ⊤Φ)\n(70)\n=\n1\nρh det(Ih +\nγ\nρβS Φ⊤Φ)\n(71)\n=\n1\nρh det(Iˆn +\nγ\nρβS ΦΦ⊤).\n(72)\nThus we have:\nlog det V ∗= −h log ρ −log det\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013\n(73)\nc= −log det\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013\n.\n(74)\nAlso we can compute trace as follows:\nTr(V ∗) = Tr\n \nβS\nγ\n \nγ\nρβS\nIh −\n\u0012 γ\nρβS\n\u00132\nΦ⊤\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦ\n!!\n(75)\n= βS\nγ\n \nγh\nρβS\n−\n\u0012 γ\nρβS\n\u00132\nTr\n \nΦ⊤\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦ\n!!\n(76)\n= βS\nγ\n \nγh\nρβS\n−\n\u0012 γ\nρβS\n\u00132\nTr\n \u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦΦ⊤\n!!\n(77)\nc= −\nγ\nβSρ2 Tr\n \u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦΦ⊤\n!\n.\n(78)\nThese computations allow us to reduce memory requirements during training from O(h2) to O(ˆn2),\nwhich represents a significant reduction when dealing with a high-dimensional feature space h.\nMemory Efficient Bayesian Model Averaging\nFor the variance V ∗we computed corresponds to\na full covariance matrix, leading to a memory cost of h2. To address this, rather than calculating V ∗\nexplicitly, we need a memory-efficient approach for conducting BMA on test points. This can be\ndone easily by calculating Σ∗as follows:\nΣ∗= ΦteV ∗Φ⊤\nte\n(79)\n= βS\nγ\n \nγ\nρβS\nΦteΦ⊤\nte −\n\u0012 γ\nρβS\n\u00132\nΦteΦ⊤\n\u0012\nIˆn +\nγ\nρβS\nΦΦ⊤\n\u0013−1\nΦΦ⊤\nte\n!\n,\n(80)\nwhere Φte ∈Rnte×h denotes the feature matrix of nte number of test points. Then by storing Φ ∈\nRˆn×h and (Iˆn +\nγ\nρβS ΦΦ⊤)−1 ∈Rˆn×ˆn instead of V ∗, we can reduce the memory requirements to\nˆnh + ˆn2, which is much smaller than h2.\nB\nALGORITHM FOR TRAINING AND INFERENCE\nIn this section, we present algorithms for training and inference. In Algorithm 1, the overall training\nprocedures are presented, and note that we utilize the model pool M to prevent overfitting. We\nalso use the Gaussian likelihood to update the weights contained in the model pool. Additionally,\nin Algorithm 2, we present computationally and memory-efficient variational inference and BMA\nmethods. Here, we store Φ and (Iˆn +\nγ\nρβS ΦΦ⊤)−1 instead of directly computing V ∗.\n19\n\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 1 Training Variational Bayesian Pseudo-Coreset (VBPC).\nRequire: Training dataset D, learning rate δ.\nEnsure: Learned synthetic dataset S∗.\n1: Initialize:\nInitialize synthetic dataset distribution S with ˆn pairs of (ˆxi, ˆyi).\n2: Initialize:\nRandomly initialize P different θis and construct a model pool M.\n3: while not converged do\n4:\nSample training batch B from training distribution D.\n5:\nUniformly sample θi from model pool M and construct feature map ϕ.\n6:\nEfficiently compute loss Eq. 27 with Eq. 29, Eq. 30 and Eq. 31.\n7:\nUpdate ˆxis and ˆyis using gradient descent: ˆxi ←ˆxi −δ∇ˆxi ˜LD, ˆyi ←ˆyi −δ∇ˆyi ˜LD\n8:\nUpdate θi with S and the Gaussian likelihood.\n9:\nReplace θi in M with updated θi.\n10:\nIf θi ∈M has been updated T times, reinitialize θi and replace θi in M.\n11: end while\nAlgorithm 2 Variational inference and Bayesian Model Averaging using VBPC.\nRequire: Learned synthetic dataset S∗, MODE which is VI or BMA, and test dataset T .\nEnsure: Variational posterior or Bayesian Model Averaged output prediction.\n1: Initialize:\nRandomly initialize θ.\n2: while not converged do\n3:\nUpdate θ with Gaussian likelihood and S∗.\n4: end while\n5: Compute m∗\nj, Φ, and (Iˆn +\nγ\nρβS ΦΦ⊤)−1 with (ˆxi, ˆyi)s.\n6: if MODE == VI then\n7:\nCompute V ∗with Eq. 21.\n8: else if MODE==BMA then\n9:\nCompute Σ∗with Eq. 31.\n10:\nCompute approximate expected predictive distribution for T similar to Eq. 25.\n11: end if\nC\nADDITIONAL RELATED WORKS\nBayesian Pseudo-Coreset\nAs discussed in Section 1 and Section 2, the large scale of modern\nreal-world datasets leads to significant computational costs when performing SGMCMC (Welling\n& Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ma et al., 2015) or variational inference (Blei\net al., 2017; Fiedler & Lucia, 2023) to approximate posterior distributions. To address this issue,\nprevious works, such as Bayesian Coreset (BC; Campbell & Broderick, 2018; 2019; Campbell &\nBeronov, 2019), have proposed selecting a small subset from the full training dataset so that the\nposterior distribution built from this subset closely approximates the posterior from the full dataset.\nHowever, Manousakas et al. (2020) highlighted that simply selecting a subset of the training data\nis insufficient to accurately approximate high-dimensional posterior distributions, and introduced\nBPC for simple logistic regression tasks. Later, Kim et al. (2022) extended BPC to BNNs, using\nreverse KL divergence, forward KL divergence, and Wasserstein distance as measures for D in Eq. 2\nto assess the difference between the full posterior and the BPC posterior. Subsequent works have\nused contrastive divergence (Tiwary et al., 2024) or calculated divergence in function space (Kim\net al., 2023) using Function-space Bayesian Neural Network (FBNN; Rudner et al., 2021; 2022).\nHowever, as discussed in Section 1, computational and memory overhead remains an issue when\ntraining BPC and during inference using BMA.\nDataset Distillation\nSimilar to but distinct from BPC, dataset distillation (Wang et al., 2018) meth-\nods aim to train a pseudo-coreset that preserves the essential information contained in the full train-\ning dataset. These methods ensure that the model trained on the pseudo-coreset learns information\nthat allows it to perform similarly to a model trained on the full dataset. This approach enables com-\nputationally efficient training of new models using the pseudo-coreset and helps prevent catastrophic\nforgetting in continual learning scenarios, leading to more stable learning.\n20\n\n\nPublished as a conference paper at ICLR 2025\nTo train these dataset distillation methods, a bilevel optimization problem must be solved, requiring\nthe computation of meta-gradients through unrolled inner optimization to find the solution to the\nouter optimization problem. To address this challenge, various learning methods have been pro-\nposed in the dataset distillation field, which can be broadly categorized into three approaches: 1)\nusing surrogate objectives, 2) closed-form approximations, and 3) employing the implicit function\ntheorem.\nExamples of works in the first category include Zhao & Bilen (2021), Zhao & Bilen (2023), and\nCazenavette et al. (2022), where Zhao & Bilen (2021) uses gradient matching, Zhao & Bilen (2023)\nfocuses on feature distribution alignment, and Cazenavette et al. (2022) employs a trajectory match-\ning objective. Papers in the second category, Nguyen et al. (2020) and Zhou et al. (2022), calculate\nclosed-form solutions by using the Neural Tangent Kernel (Jacot et al., 2018) and Neural Network\nGaussian Process Kernel (Lee et al., 2017), respectively. Lastly, Loo et al. (2023), representing the\nthird category, uses the implicit function theorem to compute gradients for unrolled inner optimiza-\ntion, allowing for the updating of the pseudo-coreset.\nVariational Inference\nVariational inference (Bishop, 2006; Blundell et al., 2015; Blei et al.,\n2017), one of the most general methods for approximating most posterior distributions, is a tech-\nnique that approximates the target posterior distribution using a variational distribution, which has\na well-known and manageable form. The parameters of the variational distribution are learned by\nminimizing the KL divergence between the target posterior distribution and the variational distribu-\ntion. Although using all the parameters of the variational distribution can enhance its expressiveness,\nallowing for more accurate approximations, two common approaches are typically employed to ad-\ndress the computational and memory challenges that arise when handling the large scale of BNN\nweights: 1) mean-field approximation (Blundell et al., 2015; Shen et al., 2024), and 2) computing the\nposterior distribution for only a subset of the network parameters (Dusenberry et al., 2020; Fiedler\n& Lucia, 2023; Harrison et al., 2024a). In both of these cases, the parameters of the variational\ndistribution are optimized either directly using gradient descent methods to minimize the KL diver-\ngence (Blundell et al., 2015; Dusenberry et al., 2020; Shen et al., 2024), or a closed-form solution is\nfound (Wang & Blei, 2013).\nD\nADDITIONAL DISCUSSION ON VBPC\nFuture work direction\nHere, we would like to discuss some concerns and challenges we fore-\nsee in adopting the Laplace approximation on the softmax likelihood instead of using variational\ninference with Gaussian likelihood.\nSpecifically, if we switch from using a Gaussian likelihood to employing a softmax likelihood with\nLaplace approximation for variational inference, there are two cases to consider: (1) using Laplace\napproximation on the last-layer weights without any updates, and (2) updating the last-layer weights\nwith some gradient descent steps before applying Laplace approximation.\nIn the first case—applying Laplace approximation to weights without updating the last layer—two\nmain issues may arise. First, the Laplace approximation assumes that the weights are near a mini-\nmum, allowing for the approximation of the first-order term in Taylor expansion as zero. However,\nthis assumption may not hold for untrained weights, leading to significant approximation error.\nAdditionally, the computational burden of calculating the Hessian for Laplace approximation is\nsubstantial, and the need to compute gradients through this Hessian during pseudo-coreset updates\nincreases the computational load further.\nIn the second case—updating the last layer weights with gradient steps before applying Laplace\napproximation—there’s the advantage of reducing Taylor expansion error. However, this approach\ninvolves a large computational graph, which can be problematic due to the computational expense\ntypical in bilevel optimization settings. Additionally, the need to compute gradients through the\nHessian remains a challenge.\nOverall, we believe that solving these issues could lead to new meaningful future work for VBPC.\nLimitations of the Last-Layer Approximation\nThere might be concerns that considering the\nposterior distribution of only the last layer weights, rather than the entire parameter set, could limit\n21\n\n\nPublished as a conference paper at ICLR 2025\nthe model’s ability to capture uncertainty effectively, especially as the model size increases and\ntasks become more complex. We fully agree that this is a valid concern and would like to provide a\ndiscussion based on related findings.\nSpecifically, Harrison et al. (2024b) provides extensive empirical evidence on the effectiveness of\nlast-layer variational inference. Their experiments span diverse tasks, including regression with UCI\ndatasets, image classification using a Wide ResNet model, and sentiment classification leveraging\nLLM features from the OPT-175B model. They compared their method with other Bayesian in-\nference approaches such as Dropout, Ensemble methods, and Laplace approximation for the full\nmodel. Their results demonstrate that even though last-layer variational inference focuses solely\non the final layer weights, it achieves performance comparable to other comprehensive Bayesian\ninference techniques across various tasks.\nThese findings indicate that while conducting Bayesian inference on the full set of weights in a\nneural network could potentially lead to more precise uncertainty estimation, employing last-layer\nvariational inference is still effective in capturing meaningful uncertainty.\nWe believe that extending VBPC to incorporate full-weight variational inference could be a promis-\ning direction for future work, offering the potential to further enhance the method’s uncertainty\nestimation capabilities. We will include this discussion in the final manuscript to provide a balanced\nperspective and acknowledge possible avenues for improvement.\nE\nEXPERIMENTAL DETAILS\nOur VBPC code implementation is built on the official FRePo (Zhou et al., 2022)1 codebase. The im-\nplementation utilizes the following libraries, all available under the Apache-2.0 license2: JAX (Brad-\nbury et al., 2018), Flax (Babuschkin et al., 2020), Optax (Babuschkin et al., 2020), TensorFlow\nDatasets (Abadi et al., 2015), and Augmax3. For the baseline methods, we used the official code\nimplementations provided for each. All experiments, except those on the Tiny-ImageNet (Le &\nYang, 2015) dataset, were performed on NVIDIA RTX 3090 GPU machines, while Tiny-ImageNet\nexperiments were conducted on NVIDIA RTX A6000 GPUs.\nE.1\nDATASETS\nDatasets for the Bayesian Model Averaging comparison\nFor the BMA comparison experi-\nments, we utilize 5 different datasets: 1) MNIST (LeCun et al., 1998), 2) Fashion-MNIST (Xiao\net al., 2017), 3) CIFAR10 (Krizhevsky, 2009), 4) CIFAR100 (Krizhevsky, 2009), and 5) Tiny-\nImageNet (Le & Yang, 2015).\n• MNIST: The MNIST dataset4 contains 10 classes of handwritten digits with 60,000 train-\ning images and 10,000 test images, each with dimensions of 28 × 28 × 1. All images were\nnormalized using a mean of [0.1307] and a standard deviation of [0.3081].\n• Fashion-MNIST: The Fashion-MNIST dataset5 consists of 10 classes of fashion article\nimages, with 60,000 training images and 10,000 test images, each with dimensions of 28×\n28 × 1. Images were normalized using a mean of [0.2861] and a standard deviation of\n[0.3530].\n• CIFAR-10/100:\nThe CIFAR-10/100 dataset6 contains 10/100 classes, with 50,000\ntraining images and 10,000 test images sourced from the 80 Million Tiny Images\ndataset (Torralba et al., 2008).\nEach image has dimensions of 32 × 32 × 3.\nFor\nCIFAR-10, images were normalized with a mean of [0.4914, 0.4822, 0.4465] and a stan-\ndard deviation of [0.2470, 0.2435, 0.2616], while CIFAR-100 images used a mean of\n[0.5071, 0.4866, 0.4409] and a standard deviation of [0.2673, 0.2564, 0.2762].\n1https://github.com/yongchaoz/FRePo\n2https://www.apache.org/licenses/LICENSE-2.0\n3https://github.com/khdlr/augmax\n4https://yann.lecun.com/exdb/mnist/\n5https://github.com/zalandoresearch/fashion-mnist\n6https://www.cs.toronto.edu/˜kriz/cifar.html\n22\n\n\nPublished as a conference paper at ICLR 2025\n• Tiny-ImageNet: The Tiny-ImageNet dataset7 contains 200 classes, with 100,000 train-\ning images and 10,000 test images. Each image has dimensions of 64 × 64 × 3. Im-\nages were normalized using a mean of [0.4759, 0.4481, 0.3926] and a standard deviation of\n[0.2763, 0.2687, 0.2813].\nDatasets for the Out of Distribution scenarios\nFor the distribution shift and OOD scenarios, we\nuse CIFAR10-C (Hendrycks & Dietterich, 2019), which includes seven corruption types with five\nseverity for each corruption type: 1) Gaussian Blur, 2) JPEG Compression, 3) Snow, 4) Zoom Blur,\n5) Pixelate, 6) Defocus Blur, and 7) Motion Blur.\n• CIFAR10-C: The CIFAR10-C dataset8 consists of 10 classes, with 50,000 test images\nfor each corruption type.\nIt applies various corruptions to 10,000 test images from\nCIFAR10, with five levels of severity, each containing 10,000 images.\nThe images\nare normalized using the same mean [0.4914, 0.4822, 0.4465] and standard deviation\n[0.2470, 0.2435, 0.2616] as the CIFAR10 dataset.\nE.2\nMODEL ARCHITECTURE\nModel architecture utilized for the Bayesian Model Averaging and Out of Distribution tasks\nFollowing previous works (Kim et al., 2022; 2023; Tiwary et al., 2024; Zhou et al., 2022), we used\na convolutional neural network (CNN) for the Bayesian Model Averaging comparison experiment\nand the Out of Distribution experiment. This model is composed of several blocks, each consisting\nof a 3×3 convolution kernel, pre-defined normalization layer, Rectified Linear Unit (ReLU; Agarap,\n2018) activation, and a 2 × 2 average pooling layer with a stride of 2. For datasets with resolutions\nof 28 × 28 × 1 and 32 × 32 × 3, we used 3 blocks, and for datasets with a resolution of 64 ×\n64 × 3, we used 4 blocks. Following Zhou et al. (2022), we increase twice the number of filters\nwhen the feature dimension was halved, to prevent the feature dimensions from becoming too small.\nAdditionally, by default, we used the Batch Normalization (Ioffe, 2015) layer for normalization\nunless stated otherwise. For initializing model weights, we conducted experiments using the Lecun\nInitialization (Cun et al., 1998) method, which is the default initialization method of the Flax library.\nThis configuration was applied both during the model pool in the VBPC training process and in the\nevaluation phase.\nModel architecture utilized for the Architecture Generalization task\nFor the Architecture\ngeneralization experiments, we incorporate three additional normalization layers and three ad-\nditional model architectures. The normalization layers include Instance Normalization (Ulyanov,\n2016), Identity map, and Group Normalization (Wu & He, 2018). For the model architectures, we\ninclude AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014), and ResNet (He\net al., 2016). Initially, we evaluate all baselines by replacing Batch Normalization in the convolution\nlayers with the three alternative normalization methods, referring to these as CNN-IN, CNN-NN,\nand CNN-GN, respectively. Next, we use the three additional model architectures for evaluation.\nSince AlexNet does not have normalization layers in its original design, we retain this structure and\nrefer to it as AlexNet-NN. For VGG and ResNet, we use VGG11 with Group Normalization and\nResNet18 with Batch Normalization. These models are denoted as VGG11-GN and ResNet18-BN.\nE.3\nPSEUDO-CORESET INITIALIZATION, PREPROCESSING, AND AUGMENTATION\nInitialization\nBuilding on prior works (Kim et al., 2022; 2023; Tiwary et al., 2024; Zhou et al.,\n2022), we initialize the pseudo-coreset by randomly sampling images and labels from the original\ntraining dataset using a fixed sampling seed. For the labels, following Zhou et al. (2022), we initial-\nize them with scaled, mean-centered one-hot vectors corresponding to each image, where the scaling\nfactor is determined by the number of classes k, specifically\n1\n√\nk/10. Here, we train both images and\nlabels during training.\n7https://tiny-imagenet.herokuapp.com/\n8https://github.com/hendrycks/robustness?tab=readme-ov-file\n23\n\n\nPublished as a conference paper at ICLR 2025\nData preprocessing and Augmentation\nFollowing previous works (Kim et al., 2022; Tiwary\net al., 2024; Zhou et al., 2022), we perform standard preprocessing on each dataset, with the ad-\ndition of ZCA (Kessy et al., 2018) transformations for all datasets with 3 channels. Consistent with\nZhou et al. (2022), we apply a regularization strength of λ = 0.1 across all datasets. Similar to\nprevious works (Kim et al., 2022; 2023; Tiwary et al., 2024; Zhou et al., 2022), we apply the fol-\nlowing augmentations to the MNIST and Fashion-MNIST datasets: ‘Gaussian noise’, ‘brightness’,\n‘crop’, ‘rotate’, ‘translate’, and ‘cutout’. For all other datasets, we use ‘flip’, ‘Gaussian noise’,\n‘color’, ‘crop’, ‘rotate’, ‘translate’, and ‘cutout’ augmentations. These augmentations are applied\nboth during the training of the Bayesian pseudo-coreset and during evaluation with them.\nE.4\nHYPERPARAMTERS\nHyperparameters during training VBPC\nFollowing previous works (Kim et al., 2022; 2023;\nTiwary et al., 2024), we select 1, 10, or 50 images per class for all datasets when training VBPC\nfor evaluation. For βS, we use ˆn, which corresponds to the number of pseudo-coresets in each\nexperiment. This setup is designed to control the gradient magnitude by averaging, rather than\nsumming, the expected likelihood, while maintaining the influence of the KL divergence for stable\ntraining. For βD, we used 1e-8 as the default value, and when adjusted, it was selected from the\nrange [1e-6, 1e-7, 1e-8] across all experiments. For ρ and γ, we set the default values to ρ = 1.0 and\nγ = 100.0 for the ipc 1 and ipc 10 settings, and ρ = 10.0 and γ = 100.0 for the ipc 50 settings.\nExcept for the CIFAR100 ipc 10 setting where we utilize ρ = 10.0 and γ = 100.0 for the default\nsetting. When tuning these parameters, we adjusted them on a log scale in steps of 10 within the\nrange of [-5, 5]. Following the default settings in Zhou et al. (2022), we set the number of models\nstored in the model pool, P, to 10. Additionally, as per Zhou et al. (2022), we set the number of\ntraining steps, T, for each model in the model pool to 100. For the model pool optimizer, we used\nthe Adam (Kingma, 2014) optimizer with a fixed learning rate of 0.0003 across all experiments. For\nthe pseudo-coreset optimizer, we also used the Adam optimizer by default, with a cosine learning\nrate schedule starting at 0.003 for both images and labels. Lastly, we used a batch size of 1024 and\ntrained for 0.5 million steps to ensure sufficient convergence.\nHyperparameters during variational inference and Bayesian Model Averaging\nFor all exper-\niments, the hyperparameters γ, ρ, and βS used during evaluation were the same as those used for\npseudo-coreset training in the corresponding experiment. The optimizer used for training the models\nduring evaluation was the Adam optimizer with a constant learning rate of 0.0003. The number of\ntraining steps for each model was as follows: for MNIST and Fashion-MNIST, 100 steps for 1 ipc,\n500 steps for 10 ipc, and 1000 steps for 50 ipc. For CIFAR10, 200 steps for 1 ipc, 2000 steps for 10\nipc, and 5000 steps for 50 ipc. For CIFAR100, 2000 steps for both 1 ipc and 10 ipc, and 5000 steps\nfor 50 ipc. Lastly, for Tiny-ImageNet, 1000 steps were used for 1 ipc and 2000 steps for 10 ipc.\nF\nADDITIONAL EXPERIMENT\nF.1\nFULL EXPERIMENTAL RESULTS ON BAYESIAN MODEL AVERAGING COMPARISON\nHere, we report the full experimental results for Section 5.1. We report results for FRePo and\nRCIG across the entire benchmark dataset and varying IPC settings additional to Table 1. Table 6\nclearly demonstrates that VBPC surpasses other BPC baselines across all benchmark datasets and\nIPC settings in terms of ACC and NLL. Notably, VBPC achieves significantly better NLL, with large\nmargins, while requiring only a single forward pass to conduct BMA. Although VBPC is designed to\nlearn pseudo-coresets that approximate the variational distribution derived from the training dataset,\nit outperforms dataset distillation baselines, which primarily focus on achieving high ACC, in nearly\nall tasks, except for CIFAR100 with 1 IPC and Tiny-ImageNet. These results empirically validate\nthat the variational distribution trained by VBPC effectively captures epistemic uncertainty with a\nsmall amount of synthetic data, while maintaining high performance.\nComparison with dataset distillation baselines\nIn Section 5.1, the performance was evaluated\nbased on the training and evaluation methods proposed by each baseline’s original papers. However,\none might question whether the significant performance of VBPC is due to the trained pseudo-\ncoreset itself or the VI method. To address this, and to validate that the significant performance of\n24\n\n\nPublished as a conference paper at ICLR 2025\nTable 6: Comparison of the VBPC with BPC and additional dataset distillation baselines for the\nbenchmark datasets. We report ACC and NLL for the BPC baselines, and ACC for the dataset dis-\ntillation baselines. Boldfaced blue color indicates when the performance of the dataset distillation\nbaseline surpasses that of VBPC.\nFRePo\nRCIG\nBPC-rKL\nBPC-fKL\nFBPC\nBPC-CD\nVBPC (Ours)\nDataset\nipc ACC(↑) ACC(↑) ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nMNIST\n1\n93.0±0.4 94.7±0.5 74.8±1.2 1.90±0.01 83.0±2.2 1.87±0.03 92.5±0.1 1.68±0.01 93.4±0.1 1.53±0.01 96.7±0.4 0.11±0.02\n10 98.6±0.1 98.9±0.0 95.3±0.2 1.53±0.01 92.1±0.4 1.51±0.02 97.1±0.2 1.31±0.01 97.7±0.2 1.57±0.02 99.1±0.1 0.03±0.01\n50 99.2±0.1 99.2±0.0 94.2±0.3 1.36±0.02 93.6±1.8 1.36±0.02 98.6±0.1 1.39±0.02 98.9±0.2 1.36±0.01 99.4±0.1 0.02±0.01\nFMNIST\n1\n75.6±0.3 79.8±1.1 70.5±1.1 2.47±0.02 72.5±2.5 2.30±0.02 74.7±1.4 1.81±0.03 77.3±0.5 1.90±0.03 82.9±0.6 0.47±0.03\n10 86.2±0.2 88.5±0.2 78.8±0.2 1.64±0.01 83.3±0.6 1.54±0.03 85.2±0.1 1.61±0.02 88.4±0.2 1.56±0.01 89.4±0.2 0.30±0.01\n50 89.6±0.1 90.2±0.2 77.0±0.6 1.48±0.02 74.8±0.5 1.47±0.02 76.7±0.4 1.46±0.02 89.5±0.1 1.30±0.02 91.0±0.2 0.25±0.01\nCIFAR10\n1\n46.8±0.7 53.9±1.0 21.6±0.8 2.57±0.01 29.3±1.1 2.10±0.03 35.5±0.3 3.79±0.04 46.9±0.2 1.87±0.02 55.1±0.3 1.34±0.08\n10 65.5±0.4 69.1±0.4 37.9±1.5 2.13±0.02 49.9±1.4 1.73±0.01 62.3±0.3 1.31±0.02 56.4±0.7 1.72±0.03 69.8±0.7 0.89±0.02\n50 71.7±0.2 73.5±0.3 37.5±1.3 1.93±0.03 42.3±2.9 1.54±0.01 71.2±0.2 1.03±0.05 71.9±0.2 1.57±0.03 76.7±0.5 0.71±0.03\nCIFAR100\n1\n28.7±0.1 39.3±0.4 3.6±0.1 4.69±0.02 14.7±0.2 4.20±0.10 21.0±0.8 3.76±0.11 24.0±0.1 4.01±0.02 38.4±0.2 2.47±0.04\n10 42.5±0.2 44.1±0.4 23.6±0.7 3.99±0.03 28.1±0.6 3.53±0.05 39.7±0.3 2.67±0.02 28.4±0.2 3.14±0.02 49.4±0.1 2.07±0.02\n50 44.3±0.2 46.7±0.3 30.8±0.5 3.57±0.17 37.1±0.3 3.28±0.24 44.5±0.4 2.63±0.01 39.6±0.2 3.02±0.01 52.4±0.4 2.02±0.02\nTiny-ImageNet 1\n15.4±0.3 25.6±0.3 3.2±0.1 5.91±0.07 4.0±0.1 5.63±0.03 10.1±0.7 4.69±0.05 8.4±0.1 4.72±0.01 23.1±0.2 3.65±0.01\n10 25.4±0.2 29.4±0.2 9.8±0.6 5.26±0.05 11.4±0.5 5.08±0.05 19.4±0.5 4.14±0.02 17.8±0.4 3.64±0.05 25.8±0.3 3.45±0.02\nTable 7: Ablation experiment on BMA method. Here, we conduct our variational inference method\nutilizing datasets trained with other baselines.\nFRePo VI\nRCIG VI\nBPC-rKL VI\nBPC-fKL VI\nFBPC VI\nBPC-CD VI\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR10\n1\n28.2±0.9 2.22±0.02 27.8±0.7 2.20±0.01 10.1±0.1 2.30±0.01 10.1±0.1 2.32±0.01 10.0±0.1 2.37±0.02 10.4±0.8 2.35±0.02\n10 55.7±0.5 2.07±0.02 55.6±1.5 2.05±0.02 12.0±0.5 2.25±0.02 20.1±1.9 2.21±0.01 10.0±0.0 2.37±0.02 10.5±0.7 2.32±0.01\nCIFAR100 1\n19.9±0.4 4.55±0.02 2.1±0.1 5.02±0.05 1.2±0.1 4.60±0.01 1.4±0.3 4.60±0.01 1.2±0.2 4.60±0.01 1.2±0.2 4.60±0.01\n10 34.8±0.4 4.50±0.01 2.5±0.4 5.45±0.12 2.6±0.2 4.59±0.02 4.0±0.2 4.59±0.02 1.6±0.3 4.59±0.02 11.6±0.4 4.54±0.02\nVBPC is not solely attributable to the VI method, we collected the pseudo-coresets trained on all\nbaselines used in Section 5.1 for the CIFAR10 and CIFAR100 datasets in the 1ipc and 10ipc settings.\nWe then applied our proposed VI method to these baseline pseudo-coresets to measure their BMA\nperformance and compared the results with those reported in Table 6. Results in Table 7 and Table 6\nclearly show that the performance significantly drops for all baselines compared to their original\nperformance. This validates that the performance is not solely attributable to the VI method, and\ndemonstrates that VBPC successfully learns to approximate the variational distribution effectively.\nF.2\nADDITIONAL EXPERIMENT RESULTS ON LARGE DATASET AND CONTINUAL LEARNING\nTo further highlight the ability of VBPC to handle tasks that pose challenges for other BPC baselines,\nwe conduct additional experiments on more large datasets and the continual learning setting.\nLarge Datasets\nFirst, to show that our method is uniquely scalable to large datasets compared\nto other BPC methods, we conducted additional experiments on the ImageNetWoof (128x128x3)\ndataset (Howard, 2020) and the ImageNet1k (64x64x3) dataset(Russakovsky et al., 2015). Addi-\ntionally, we included an experiment in a continual learning scenario to validate that our method\nperforms better in practical scenarios.\nWe conducted experiments on the ImageWoof (128x128x3) dataset with ipc 1 and ipc 10 settings,\nas well as the resized ImageNet1k (64x64x3) dataset with ipc 1 and ipc 2 settings, to demonstrate\nthe scalability of our method to high-resolution images and larger datasets. Unlike existing BPC\nbaselines, which encountered memory issues and failed to train due to out-of-memory errors on an\nRTX 3090 GPU as the image resolution and number of classes increased, our method successfully\ncompleted training. Table 8 clearly shows that VBPC significantly outperforms other baselines with\na large margin for both the ImageWoof and resized ImageNet1k datasets.\nContinual Learning\nNext, we validated the practical effectiveness of our method through con-\ntinual learning experiments using pseudo-coreset images learned by each method. We followed the\n25\n\n\nPublished as a conference paper at ICLR 2025\nTable 8: Experiments on the scalability utilizing ImageWoof and resized ImageNet datasets. Here\n‘-’ indicates the training fails due to the out-of-memory problems.\nImageWoof ipc 1 ImageWoof ipc 10\nImageNet ipc 1\nImageNet ipc 2\nMethod\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nRandom 14.2±0.9 3.84±0.25 27.0±1.9 2.83±0.33 1.1±0.1 8.32±0.05 1.4±0.1 8.10±0.05\nBPC-CD 18.5±0.1 2.76±0.05\n-\n-\n-\n-\n-\n-\nFBPC\n14.8±0.1 3.73±0.02 28.1±0.3 2.69±0.09\n-\n-\n-\n-\nBPC-fKL 14.9±0.9 3.74±0.23 25.0±0.8 2.90±0.27\n-\n-\n-\n-\nBPC-rKL 12.0±0.5 6.07±0.31\n-\n-\n-\n-\n-\n-\nVBPC\n31.2±0.1 2.13±0.04 39.0±0.1 1.84±0.1 10.1±0.1 5.33±0.04 11.5±0.2 5.25±0.05\nTable 9: Experiments on the continual learning setting. Here, we utilize the CIFAR100 dataset with\nipc 20 setting. We assume 5 steps during training and each step contains data from new 20 classes\nin the CIFAR100 dataset. Here we only report accuracy due to the variant of the number of classes\nduring the steps.\nNumber of Classes\n20\n40\n60\n80\n100\nBPC-CD\n52.5±2.4 40.4±1.3 35.2±0.8 33.4±0.5 29.4±0.2\nFBPC\n61.4±1.8 53.2±1.5 48.8±0.7 43.9±0.4 41.2±0.3\nBPC-fKL\n51.8±2.2 39.8±1.1 35.5±0.7 33.1±0.5 29.5±0.3\nBPC-rKL\n48.2±2.7 35.5±1.8 32.0±1.0 29.8±0.6 25.5±0.3\nVBPC\n75.3±2.0 65.8±1.5 57.1±0.9 53.3±0.5 50.3±0.2\ncontinual learning setup described in Zhou et al. (2022); Zhao & Bilen (2021), where class-balanced\ntraining examples are greedily stored in memory, and the model is trained from scratch using only\nthe latest memory. Specifically, we performed a 5-step class incremental learning experiment on\nCIFAR100 with an ipc 20 setting, following the class splits proposed in Zhou et al. (2022); Zhao &\nBilen (2021). Table 9 demonstrates that VBPC consistently outperforms other baselines across all\nsteps, confirming its superior practicality and effectiveness in real-world continual learning scenar-\nios.\nF.3\nADDITIONAL EXPERIMENTS ON OUT-OF-DISTRIBUTION DATA\nTo further validate the effectiveness of VBPC, We have conducted additional Out-of-Distribution\n(OOD) detection experiments and reported the results. The metrics we evaluate include AUROC,\nAUPR-In, and AUPR-Out, where higher values indicate better performance.\nWe used models\ntrained with the CIFAR10 IPC 10 setting and evaluated them on CIFAR100, TinyImageNet, and\nSVHN (Netzer et al., 2011) datasets as OOD datasets.\nThe results, presented in Table 10, demonstrate that the pseudo-coreset learned by VBPC performs\nrobustly in OOD detection scenarios. These findings, combined with the corruption experiments in\nthe main paper, validate the effectiveness and robustness of VBPC under diverse and challenging\nevaluation conditions.\nF.4\nANALYSIS ON COMPUTATIONAL COSTS AND TRAINING TIME\nIn this section, we performed analyses focusing on two aspects of computational cost.\nCost of training the pseudo-coreset\nAs mentioned in the Section 1, conventional BPC methods\nrelying on SGMCMC require the creation of expert trajectories, which are training trajectories de-\nrived from the full dataset. Each dataset typically involves training with 10 different random seeds\nfor these trajectories, making this step computationally expensive. Since all BPC baselines share\nand utilize these precomputed trajectories, their associated computational cost can be considered a\nshared overhead.\nTo isolate the computational cost of training the pseudo-coreset itself, we measured the wall-clock\ntime required for pseudo-coreset optimization by each method. The results of this comparison are\n26\n\n\nPublished as a conference paper at ICLR 2025\nTable 10: AUROC, AUPR-In, and AUPR-Out results for the OOD detection task with a model\ntrained with the learned pseudo-coresets. Note that we used the same model structure which is\nutilized when training pseudo-coresets.\nDataset\nModel\nAUROC(↑) AUPR-In(↑) AUPR-Out(↑)\nTinyImageNet\nBPC-CD\n49.09\n52.79\n45.88\nBPC-fKL\n48.95\n51.72\n47.00\nBPC-rKL\n48.34\n52.71\n44.49\nFBPC\n45.39\n49.70\n43.14\nVBPC\n52.85\n56.22\n49.64\nSVHN\nBPC-CD\n55.09\n35.64\n73.88\nBPC-fKL\n54.26\n34.78\n75.47\nBPC-rKL\n42.61\n28.29\n67.15\nFBPC\n41.34\n30.12\n62.18\nVBPC\n68.50\n48.49\n82.91\nTable 11: Wall clock time results for training pseudo-coresets with each BPC method using CI-\nFAR10 ipc 10 settings. We used RTX3090 GPU to measure the exact training time. Here, all\nmethods except for VBPC share the training time for expert trajectories.\nMethod\nBPC-CD BPC-rKL FBPC BPC-fKL VBPC\nTimes (hr)\n5+8.5\n5+9\n5+10.5\n5+12\n5.5\nsummarized in Table 11, providing insights into how VBPC reduces training costs compared to other\nbaselines.\nCost of inference\nWhen performing inference, VBPC requires training only a single model,\nwhereas other BPC baselines rely on multiple SGMCMC samples. Each sample incurs significant\ntraining and inference costs, which grow linearly with the number of samples.\nTo quantify this difference, we measured the wall-clock time for inference across methods, with\nresults presented in Table 12. These results highlight how VBPC achieves superior efficiency during\ninference by avoiding the high computational costs associated with sampling-based approaches.\nThese analyses demonstrate VBPC’s ability to perform Bayesian inference efficiently, both in terms\nof pseudo-coreset training and inference, and further reinforce the computational advantages of our\nmethod.\nF.5\nABLATION ON RANDOM INITIALIZATION\nSince our method initializes the pseudo-coreset by randomly sampling images and labels from the\noriginal training dataset, following previous works (Kim et al., 2022; 2023; Tiwary et al., 2024;\nZhou et al., 2022), we conducted an ablation experiment using random initialization for the pseudo-\ncoreset. In this experiment, we first randomly initialized the pseudo-coreset by sampling pixel values\nfrom a uniform distribution Unif[0, 1]. We then trained the images after normalizing them with the\npredefined mean and variance for each dataset reported in Appendix E.1. We conducted this ablation\nexperiment on the CIFAR10/100 1 ipc and 10 ipc settings. Fig. 2 and Fig. 3 clearly illustrate that\nVBPC can effectively learn semantic information even when initialized randomly. Specifically, in\nthe CIFAR10 ipc 1 case shown in the top figures of both Fig. 2 and Fig. 3, the images after training\nappear similar, whether they were initialized randomly or sampled from the training dataset. Also\nTable 13 shows that randomly initialized VBPC shows comparable performance compared to the\nVBPC.\nF.6\nABLATION ON PSEUDO-CORESET OPTIMIZER\nSince we use the Adam optimizer for training VBPC, which differs from the default choice in pre-\nvious work (Zhou et al., 2022), we conducted an ablation experiment on the optimizer. Following\n27\n\n\nPublished as a conference paper at ICLR 2025\nTable 12: Wall clock time results for inference using learned pseudo-coresets. We measure the in-\nference time for evaluating all the test data from the CIFAR10 test dataset. After finishing training\nthe pseudo-coresets, the inference cost for the all baselines are same because they only need SGM-\nCMC and BMA with same number of datasets and weight samples.\nMethod\nBPC-CD BPC-rKL FBPC BPC-fKL VBPC\nTimes (s)\n165\n165\n165\n165\n20\nFigure 2: Learned VBPC images from the random initialization for the CIFAR10 ipc 1 (above) and\nipc 10 (below) cases. The left figure shows the random images sampled from the uniform distribution\nand the right figure shows the trained VBPC images starting from the left images. Training from\nrandom initialization successfully learns semantic information from the full dataset.\nZhou et al. (2022), we used the LAMB (You et al., 2019) optimizer with a cosine learning rate\nschedule for this ablation. We conduct this ablation experiment on the CIFAR10 1 ipc and 10 ipc\nsettings. As seen in Fig. 4, although there are minor differences, the images trained with the LAMB\nand Adam optimizers are largely similar when starting from the same pseudo-coreset initial images.\nAdditionally, Table 14 demonstrates that our method effectively learns pseudo-coreset with varying\noptimizers, closely approximating the variational distribution of the full training dataset.\nF.7\nABLATION ON MODEL POOL MAXIMUM UPDATE STEPS\nAs mentioned in Appendix E.4, we set T = 100 as the maximum update step for the weights in\nthe model pool M across all experiments. The model pool was introduced to address VBPC’s\noverfitting issue, as the weights in the model pool are trained for T steps, leading to a variety of\nfeature maps. This prevents VBPC from learning based on a single feature map. To investigate\nthe effect of T, we plan to conduct an ablation study to examine how changes in T impact image\nquality and performance. We conducted an ablation experiment on the CIFAR100 ipc 10 task with\nTable 13: Comparison between the random initialization and initialization with randomly sampled\nimages. Random Initialization denotes the VBPC learned starting from the uniform random initial-\nization. Here, we report ACC and NLL for both initializations.\nRandom Initialization\nVBPC\nRandom Initialization\nVBPC\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR10 1\n54.2±0.5\n1.37±0.02\n55.1±0.3 1.34±0.08 CIFAR100 1\n37.5±0.4\n2.51±0.06\n38.4±0.2 2.47±0.04\n10 68.9±0.4\n0.98±0.01\n69.8±0.7 0.89±0.02\n10 48.4±0.4\n2.20±0.03\n49.4±0.1 2.07±0.02\n28\n\n\nPublished as a conference paper at ICLR 2025\nFigure 3: Learned VBPC images from the randomly sampled image from the original training\ndataset for the CIFAR10 ipc 1 (above) and ipc 10 (below) cases. The left figure shows the initial\nimages sampled from the original dataset and the right figure show the final learned VBPC starting\nfrom the left images.\nFigure 4: Visualization of learned VBPC images utilizing different optimizers for the CIFAR10 ipc\n1 (above) and ipc 10 (below). The left figure shows the learned VBPC images with LAMB optimizer\nand the right figure shows the learned VBPC images with Adam optimizer.\nT = 200 and T = 400. As shown in Fig. 5, the images learned with different maximum update\nsteps appear visually similar. However, Table 15 quantitatively shows that excessive updates to the\nmodel pool weights can reduce feature diversity, potentially leading to a decline in performance for\nunseen feature maps.\nF.8\nABLATION ON LABEL LEARNING\nFollowing the previous works (Kim et al., 2022; 2023; Tiwary et al., 2024; Zhou et al., 2022), we\nlearned the labels when training the pseudo-coreset. This can be crucial and effective for learning\na more informative BPC, as the mean of the pseudo-coreset variational distribution depends on the\nlabel. This dependency also impacts the loss function used in the outer optimization process. Fig. 6\nshows that without label learning, trained VBPC images significantly lost the semantic informa-\n29\n\n\nPublished as a conference paper at ICLR 2025\nTable 14: Comparison between the VBPC learned with LAMB optimizer and Adam optimizer.\nLAMB denotes the VBPC trained with LAMB optimizer. Here, we report ACC and NLL for both\noptimizers.\nLAMB\nVBPC\nLAMB\nVBPC\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR10 1\n54.4±0.8 1.36±0.03 55.1±0.3 1.34±0.08 CIFAR100 1\n38.5±0.5 2.46±0.03 38.4±0.2 2.47±0.04\n10 69.4±0.5 0.90±0.02 69.8±0.7 0.89±0.02\n10 49.4±0.2 2.25±0.10 49.4±0.1 2.07±0.02\nFigure 5: Learned VBPC images utilizing different maximum updates steps for the model pool\nelements in the CIFAR100 ipc 10 experiment. The left figure shows the T = 100 case which is\nthe default setting for the all experiments. The middle and the right figures show the T = 200 and\nT = 400 cases. The learned images show minor difference in visual.\ntion for each image. Also, results presented in Table 16 clearly shows that the BMA performance\nwith VBPC variational distribution largely drops without label learning. These results validate that\nlearning the label is important for the successful VBPC training.\nF.9\nABLATION ON GAUSSIAN NOISE AUGMENTATION\nDuring VBPC training, we apply Gaussian noise augmentation. Based on previous findings that\nadding Gaussian noise to images during neural network training improves robustness to various\nimage corruptions (Rusak et al., 2020), we incorporate Gaussian noise during VBPC training. This\nhelps the learned pseudo-coreset dataset produce a variational posterior that is robust to unseen\nmodel structures and corrupted test datasets. Specifically, we add Gaussian noise sampled from\nN(0, 0.01) after normalizing the images using the predefined mean and standard deviation for all\ntasks. We conduct the ablation experiment on the existence of this Gaussian Noise during training\nVBPC utilizing CIFAR10/100 1 ipc and 10 ipc settings. As clearly seen in the CIFAR10 1 ipc case in\nFig. 7, training with Gaussian noise results in much clearer and brighter images. In contrast, without\nGaussian noise, the model tends to learn visually similar features in the background, unlike the cases\nwhere noise is applied. Table 17 confirms that, as expected, the overall performance decreases when\nGaussian noise augmentation is not applied, compared to VBPC with Gaussian noise augmentation.\nTable 15: Ablation results on the model pool maximum update steps. Here, we used CIFAR100 ipc\n10 setting for the ablation. T = 200 and T = 400 indicate the maximum updates for the model pool\nis 200 and 400, respectively. Here, we report ACC and NLL for all the update steps.\nVBPC\nT = 200\nT = 400\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR100 10 49.4±0.1 2.07±0.02 48.7±0.2 2.16±0.03 48.0±0.2 2.22±0.04\n30\n\n\nPublished as a conference paper at ICLR 2025\nFigure 6: Visualization of learned VBPC images with and without label learning for the CIFAR10\nipc 1 (above) and ipc 10 (below). The left figure shows the learned VBPC images without label\nlearning and the right figure shows the learned VBPC images with label learning.\nTable 16: Comparison between the VBPC learned with and without label learning. No Label de-\nnotes the VBPC trained without label learing. Here, we report ACC and NLL for both results.\nNo Label\nVBPC\nNo Label\nVBPC\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR10 1\n40.6±1.3 1.83±0.02 55.1±0.3 1.34±0.08 CIFAR100 1\n10.1±0.0 5.12±0.05 38.4±0.2 2.47±0.04\n10 56.6±0.3 1.54±0.03 69.8±0.7 0.89±0.02\n10 23.5±1.2 4.92±0.06 49.4±0.1 2.07±0.02\nF.10\nABLATION ON HYPERPARAMETER\nIn this section, we conduct ablation experiments on ρ and γ, which are the hyperparameters newly\nproposed in our work. We set the default values to ρ = 10 and γ = 100.0 for the CIFAR100 ipc 10\nsettings. And, we conduct ablation experiment with the CIFAR100 ipc 10 setting. And the results\npresented in Table 18 and Table 19 show that even when we varied our hyperparameters by orders of\nmagnitude (in log scale, with changes up to 10-fold), the performance remains consistently similar.\nAnd this concludes that our method works robustly with respect to hyperparameter changes.\nF.11\nABLATION ON TRAINING STEPS DURING INFERENCE\nIn this section, we conduct ablation experiments on the number of training steps T ′ during inference.\nWhen learning the pseudo-coreset using the VBPC method, we leverage a model pool to allow the\ndata to observe various feature maps, ensuring diverse learning. Therefore, even during inference,\nalthough the model may not perfectly fit the pseudo-coreset that was trained with the feature map,\nit can still approximate the best variational distribution for the current last-layer weights based on\nTable 17: Comparison between the VBPC learned with and without Gaussian Noise augmentation.\nNo Noise denotes the VBPC trained without Gaussian Noise augmentation. Here, we report ACC\nand NLL for both results.\nNo Noise\nVBPC\nNo Noise\nVBPC\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR10 1\n53.9±0.8 1.41±0.04 55.1±0.3 1.34±0.08 CIFAR100 1\n35.4±0.3 2.62±0.05 38.4±0.2 2.47±0.04\n10 68.8±0.7 0.92±0.04 69.8±0.7 0.89±0.02\n10 48.5±0.4 2.22±0.06 49.4±0.1 2.07±0.02\n31\n\n\nPublished as a conference paper at ICLR 2025\nFigure 7: Visualization of learned VBPC images utilizing Gaussian noise during training for the\nCIFAR10 ipc 1 (above) and ipc 10 (below). The left figure shows the learned VBPC images without\nthe Gaussian noise and the right figure shows the learned VBPC images with the Gaussian noise.\nTable 18: Ablation results on the hyperparamer γ. Here, we used CIFAR100 ipc 10 setting for\nthe ablation. γ = 10, γ = 1000, and γ = 10000 indicate that we set γ as 10, 1000, and 10000,\nrespectively. Our default setting is γ = 1. Here, we report ACC and NLL for all the update steps.\nγ = 10\nVBPC\nγ = 1000\nγ = 10000\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR100 10 48.5±0.2 2.30±0.03 49.4±0.1 2.07±0.02 49.4±0.4 2.14±0.04 49.0±0.5 2.10±0.02\nthe available feature map. This enables the model to achieve sufficient BMA performance even\nbefore the pseudo-coreset learning is fully completed. As shown in Table 20, the model exhibits\nslightly lower performance during initial steps, such as at 400 or 800 steps, compared to the best\nperformance. However, after these early stages, the performance becomes nearly identical to the\nfinal convergence step at 2000 steps. These results further demonstrate that our VBPC approach\nallows for fast and efficient posterior approximation.\nG\nTRAINED VBPC IMAGES\nIn this section, we present the images learned under the ipc 1, 10, and 50 settings for MNIST,\nFashion-MNIST, CIFAR10, CIFAR100, and Tiny-ImageNet. To avoid overwhelming the report\nwith too many images, we have limited the number of reported images to a maximum of 100 per\ntask.\nG.1\nTAKE-HOME MESSAGE FROM LEARNED IMAGES\nRegarding the learned pseudo-coreset images for CIFAR10, the results can be found in Fig. 12 and\nleft figure of Fig. 13, showing the outcomes for ipc values of 1 and 10. These images reveal several\ninteresting aspects of how VBPC captures information.\nFirst, both ipc 1 and ipc 10 images show that VBPC effectively learns features associated with\nspecific classes, such as “horse\" or “automobile,\" as can be visually confirmed. This indicates that\nthe pseudo-coreset images retain class-relevant information necessary for approximating the original\ndataset’s posterior distribution. When comparing ipc 1 and ipc 10, there are notable differences. In\nthe case of ipc 1, where only a single image per class is available, VBPC attempts to encode as\nmany class-specific features as possible into a single image. As a result, the learned image appears\nto incorporate multiple discriminative features from the class symmetrically. In contrast, with ipc\n32\n\n\nPublished as a conference paper at ICLR 2025\nTable 19: Ablation results on the hyperparamer ρ. Here, we used CIFAR100 ipc 10 setting for the\nablation. ρ = 1, ρ = 100, and ρ = 1000 indicate that we set ρ as 1, 100, and 1000, respectively.\nOur default setting is ρ = 10. Here, we report ACC and NLL for all the update steps.\nρ = 1\nVBPC\nρ = 100\nρ = 1000\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR100 10 49.0±0.3 2.10±0.02 49.4±0.1 2.07±0.02 49.0±0.2 2.20±0.03 47.5±0.4 2.35±0.04\nTable 20: Ablation results on the training step T ′ during inference. Here, we used CIFAR100 ipc\n10 setting for the ablation. T ′ = 400, T ′ = 800, T ′ = 1200, and T ′ = 1600 indicate that the\nintermediate performance at step 400, 800, 1200, and 1600, respectively. Our default setting is\nT ′ = 2000. Here, we report ACC and NLL for all the update steps.\nT ′ = 400\nT ′ = 800\nT ′ = 1200\nT ′ = 1600\nVBPC\nDataset\nipc ACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nACC(↑)\nNLL(↓)\nCIFAR100 10 48.9±0.3 2.16±0.01 49.2±0.3 2.09±0.01 49.4±0.3 2.07±0.01 49.5±0.2 2.07±0.01 49.4±0.1 2.07±0.02\n10, where more images per class are available, VBPC distributes the class-relevant features across\nmultiple images. This leads to a greater diversity of features being captured across the pseudo-\ncoreset, enabling a more comprehensive representation of the class.\nAdditionally, both ipc 1 and ipc 10 images often include low-level features beyond the main class-\nrelevant ones. These features likely help capture the dataset’s variability and ensure the learned\npseudo-coreset maintains a close approximation of the original data distribution.\nThese observations suggest that VBPC is effective in compressing the dataset while retaining essen-\ntial information. The learned images illustrate how VBPC balances feature extraction and informa-\ntion retention to ensure that the variational posterior distribution learned using the pseudo-coreset\nclosely approximates the one learned using the full dataset. This further validates the interpretability\nand utility of VBPC in various tasks.\n33\n\n\nPublished as a conference paper at ICLR 2025\nFigure 8: Visualization of learned VBPC images for the MNIST ipc 1.\nFigure 9: Visualization of learned VBPC images for the MNIST ipc10 (left) and ipc50 (right).\nFigure 10: Visualization of learned VBPC images for the Fashion-MNIST ipc1.\nFigure 11: Visualization of learned VBPC images for the Fashion-MNIST ipc10 (left) and ipc50\n(right).\nFigure 12: Learned VBPC images for the CIFAR10 ipc 1 case.\n34\n\n\nPublished as a conference paper at ICLR 2025\nFigure 13: Visualization of learned VBPC images for the CIFAR10 ipc10 (left) and ipc50 (right).\nFigure 14: Visualization of learned VBPC images for the CIFAR100 ipc1.\n35\n\n\nPublished as a conference paper at ICLR 2025\nFigure 15: Visualization of learned VBPC images for the CIFAR100 ipc10 (left) and ipc50 (right).\nFigure 16: Visualization of learned VBPC images for the Tiny-ImageNet ipc1 (left) and ipc10\n(right).\n36\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21143v1.pdf",
    "total_pages": 36,
    "title": "Variational Bayesian Pseudo-Coreset",
    "authors": [
      "Hyungi Lee",
      "Seungyoo Lee",
      "Juho Lee"
    ],
    "abstract": "The success of deep learning requires large datasets and extensive training,\nwhich can create significant computational challenges. To address these\nchallenges, pseudo-coresets, small learnable datasets that mimic the entire\ndata, have been proposed. Bayesian Neural Networks, which offer predictive\nuncertainty and probabilistic interpretation for deep neural networks, also\nface issues with large-scale datasets due to their high-dimensional parameter\nspace. Prior works on Bayesian Pseudo-Coresets (BPC) attempt to reduce the\ncomputational load for computing weight posterior distribution by a small\nnumber of pseudo-coresets but suffer from memory inefficiency during BPC\ntraining and sub-optimal results. To overcome these limitations, we propose\nVariational Bayesian Pseudo-Coreset (VBPC), a novel approach that utilizes\nvariational inference to efficiently approximate the posterior distribution,\nreducing memory usage and computational costs while improving performance\nacross benchmark datasets.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}