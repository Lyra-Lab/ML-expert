{
  "id": "arxiv_2502.20685v1",
  "text": "EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching\nDongki Jung 1,2\nJaehoon Choi 2\nYonghan Lee 2\nSomi Jeong 1\nTaejae Lee 1\nDinesh Manocha 2 Suyong Yeon 1\n1NAVER LABS\n2University of Maryland\njdk9405@umd.edu\nInput\n(c) EDM (ours)\nWarp\n(b) Prev. SotA with Cubemap Proj.\nWarp\nMultiple \nPredictions\n(a) Prev. SotA\nWarp\nFigure 1. (a) Previous state-of-the-art [15] struggles to achieve accurate dense matching in equirectangular projection (ERP) images due to\ninherent distortions. (b) The ERP image can be transformed into a cubemap image, which consists of six perspective images. However, this\napproach demands multiple independent iterations of inference for each pair of perspective images, increasing computational complexity\nand losing the global information in the ERP image. (c) Our proposed method, EDM, leverages the spherical camera model, rendering it\nrobust against distortions. Warp refers to results obtained by multiplying the warped image with the predicted certainty map, demonstrating\nthat our method yields more accurate dense matches.\nAbstract\nWe introduce the first learning-based dense matching algo-\nrithm, termed Equirectangular Projection-Oriented Dense\nKernelized Feature Matching (EDM), specifically designed\nfor omnidirectional images.\nEquirectangular projection\n(ERP) images, with their large fields of view, are partic-\nularly suited for dense matching techniques that aim to\nestablish comprehensive correspondences across images.\nHowever, ERP images are subject to significant distortions,\nwhich we address by leveraging the spherical camera model\nand geodesic flow refinement in the dense matching method.\nTo further mitigate these distortions, we propose spherical\npositional embeddings based on 3D Cartesian coordinates\nof the feature grid. Additionally, our method incorporates\nbidirectional transformations between spherical and Carte-\nsian coordinate systems during refinement, utilizing a unit\nsphere to improve matching performance. We demonstrate\nthat our proposed method achieves notable performance en-\nhancements, with improvements of +26.72 and +42.62 in\nAUC@5° on the Matterport3D and Stanford2D3D datasets.\nProject Page: https://jdk9405.github.io/EDM\n1. Introduction\nOmnidirectional images, also known as 360° images, pro-\nvide significant advantages owing to their expansive fields\nof view, offering more contextual information and versatil-\nity [12, 21, 38, 63, 67]. These spherical images enable a\ncomprehensive representation of environments, facilitating\na deeper understanding of spatial information. Their utility\nextends to aiding robot navigation [40, 61] and autonomous\nvehicle driving [43] by minimizing blind spots. 360° im-\nages also can be utilized in a diverse range of applications,\nfrom creating immersive AR/VR experiences to practical\nuses in interior design [1], tourism [48], and real estate pho-\ntography [5]. Integrating omnidirectional images into vir-\ntual house tours allows customers to experience an immer-\nsive view, enabling them to fully engage themselves in the\nservice. Moreover, the adoption of omnidirectional images\ncontributes to more efficient data collection. By replacing\nthe need for multiple perspective images, omnidirectional\nimages can reduce both the cost and time associated with\ndata scanning. The large field of view provided by 360° im-\nages has also demonstrated superiority over narrower views\nin 3D motion estimation [18, 27, 42].\nFeature matching plays a critical role in numerous 3D\ncomputer vision tasks, including mapping and localiza-\ntion.\nTraditionally, Structure from Motion (SfM) [49]\n1\narXiv:2502.20685v1  [cs.CV]  28 Feb 2025\n\n\nleverages feature matching to estimate relative poses. Re-\ncent advancements have introduced semi-dense or dense\napproaches for feature matching such as LoFTR [55]\nand DKM [15], which demonstrate superior performance\nin repetitive or textureless environments compared to\nkeypoint-based methods [13, 28, 36, 46, 47]. These meth-\nods have been mainly developed for perspective 2D im-\nages and videos, but encounter challenges when applied to\nomnidirectional images. For example, to adapt matching\nmethods for spherical images, two prevalent approaches for\nsphere-to-plane projections are the equirectangular projec-\ntion (ERP) and the cubemap projection [63]. ERP images\nexhibit significant distortions, particularly near the pole re-\ngions, which hinder the effective application of perspective\nmethods. On the other hand, the cubemap format, consist-\ning of six perspective images, can be processed indepen-\ndently without such distortions.\nHowever, this approach\ninvolves the costly computation of multiple inferences for\neach pair of perspective images, resulting in the loss of\nglobal information from a single spherical image and di-\nminishing feature matching capabilities due to the reduced\nfield of view in each perspective image. These challenges\nare shown in Fig. 1 (a) and (b).\nMain Results\nIn this paper,\nwe propose EDM, a\ndistortion-aware dense feature matching method for om-\nnidirectional images, addressing challenges that existing\ndetector-free approaches [15, 16, 55] struggle to overcome.\nTo the best of our knowledge, EDM is the first learning-\nbased method designed for dense matching and relative\npose estimation between two omnidirectional images. As\nseen in Fig. 1, our method defines feature matching in 3D\ncoordinates, specifically addressing the challenges posed\nby distortions of ERP images. We accomplish this based\non the integration of two novel steps: a Spherical Spatial\nAlignment Module (SSAM) and specific enhancements in\nGeodesic Flow Refinement. The SSAM leverages spher-\nical positional embeddings for ERP images and incorpo-\nrates a decoder to generate the global matches. Further-\nmore, the Geodesic Flow Refinement step employs coor-\ndinate transformation to refine the residuals of correspon-\ndences. Compared to both recent sparse and dense feature\nmatching methods [15, 16, 19, 69], our approach results in\nsignificant performance improvement of +26.72 and +42.62\nAUC@5° in relative pose estimation for spherical images\non the Matterport3D [5] and Stanford2D3D [2] datasets.\nAdditionally, we evaluate our method qualitatively on the\nEgoNeRF [7] and OmniPhotos [4] datasets, demonstrating\nrobust performance across diverse environments. The main\ncontributions of this paper are summarized as follows:\n• We introduce a novel approach for estimating dense\nmatching across ERP images using geodesic flow on a\nunit sphere.\n• We propose a Spherical Spatial Alignment Module that\nutilizes Gaussian Process regression and spherical posi-\ntional embeddings to establish 3D correspondences be-\ntween omnidirectional images.\nIn addition, we use\nGeodesic Flow Refinement by enabling conversions be-\ntween coordinates to refine the displacement on the sur-\nface of the sphere.\n• With azimuth rotation for data augmentation, we achieve\nstate-of-the-art performance in dense matching and rela-\ntive pose estimation between two omnidirectional images.\n2. Related Work\nOmnidirectional Images\nThe popularity of consumer-\nlevel 360° cameras has led to increased interest in spherical\nimages, which offer comprehensive coverage of the field\nof view from a single vantage point.\nThese images are\noften represented using equirectangular projection (ERP)\n[63], facilitating their utilization in various computer vision\ntasks. Recent advancements in computer vision have lever-\naged ERP images for diverse tasks such as object detection\n[11, 53], semantic segmentation [24, 66], depth estimation\n[25, 32, 33, 45, 50, 60, 65], omnidirectional Simultaneous\nLocalization and Mapping [62], scene understanding [54],\nand neural rendering [8, 26, 29, 37].\nDespite the utility of ERP images, their unique geom-\netry presents several challenges in visual representation.\nAs ERP images are obtained through projecting a sphere\nonto a plane, a single spherical image can be expressed\nby multiple distinct ERP images. Additionally, ensuring\nperfect alignment of their left and right extremities is es-\nsential.\nWhile some research methods have introduced\nrotation-equivariant convolutions [9, 17] to address these is-\nsues, their implementation often demands increased compu-\ntational resources. To mitigate this constraint, we propose\nan azimuth rotation approach for data augmentation, under\nthe assumption that maintaining the downward orientation\nof scanned omnidirectional images parallel to gravity offers\nbenefits [3].\nFeature Matching\nLocal feature matching has relied\non detector-based methods, encompassing both traditional\nhand-crafted techniques [36, 46] and learning-based ap-\nproaches [13, 28, 34, 44, 59]. These methods typically in-\nvolve detecting keypoints, computing descriptor distances\nbetween paired keypoints, and performing matching via\nmutual nearest neighbor search. SuperGlue [47] introduces\na learning-based paradigm, optimizing visual descriptors\nusing an attentional graph neural network and an optimal\nmatching layer. However, detector-based methods face lim-\nitations in terms of accurately detecting keypoints, particu-\nlarly in repetitive or indiscriminative regions. In contrast,\ndetector-free or dense methods [15, 16, 39, 55, 57, 58] offer\n2\n\n\na solution to the keypoint detection issue, providing dense\nfeature matches at the pixel level.\nWhile the aforementioned methods are tailored for per-\nspective images, they often fail to address the unique chal-\nlenges of spherical cameras. SPHORB [69], an extension\nof ORB [46], mitigates distortion in ERP images using a\ngeodesic grid and local planar approximation [14]. Simi-\nlarly, learning-based matching methods such as SphereGlue\n[19, 20] and PanoPoint [68] adapt keypoint matching tech-\nniques for spherical imagery. CoVisPose [23, 41] explores\nlayout features for estimating camera poses over large base-\nlines yet remains constrained by detected feature infor-\nmation.\nTherefore, we propose a novel dense matching\nmethod that extracts all matches without keypoint detection\nin spherical images.\n3. Preliminaries\n3.1. Spherical and Cartesian Coordinate\nERP\nSphere\n(𝑆!, 𝑆\", 𝑆#)\n𝑥\n𝑦\n𝑧\n𝜃\nϕ\n𝒖= (𝜃, 𝜙)\nFigure 2. Coordinate system.\n\n\n\n\n\nSx = sin(θ) cos(ϕ)\nSy = sin(ϕ)\nSz = cos(θ) cos(ϕ)\n\n\n\n\n\n\n\nθ = arctan(Sx\nSz )\nϕ = arcsin(Sy\n|S|)\n(1)\nAlthough ERP images are displayed in 2D space, they actu-\nally represent a collection of flattened rays normalized to\na unit scale within a spherical camera model. Thus, we\ncan express the coordinate conversion equation u = π(S)\nbetween the spherical coordinates u = (θ, ϕ) and the 3D\nCartesian coordinates S = (Sx, Sy, Sz) as shown in Fig.\n2. Each value of θ ∈[−π, π] and ϕ ∈[−π\n2 , π\n2 ] indicates\nthe longitude and latitude. We utilize this coordinate trans-\nformation π(·) in Section 4.1 and Section 4.2 to handle the\nspherical camera model effectively.\n3.2. Dense Kernelized Feature Matching\nDense matching is the task of finding dense correspondence\nand estimating 3D geometry from two images (IA, IB). Re-\ncently, DKM [15] introduced a kernelized global matcher\nand warp refinement, formulating this problem as finding\na mapping f →u where u are 2D spatial coordinates.\nFirst, DKM extracts multi-scale features using a ResNet50\nencoder [22],\n{f l\nA}L\nl=1 = Encoder(IA),\n{f l\nB}L\nl=1 = Encoder(IB),\n(2)\nwhere the strides are defined as elements of the set l ∈\n{20, ..., 2L−1}. Coarse features are associated with stride\n{32, 16}, and fine features correspond to {8, 4, 2, 1}.\nAt the coarse level, it consists of a kernelized regres-\nsion to estimate the posterior mean µA|B using a Gaus-\nsian Process (GP) formulation. GP regression generates a\nprobabilistic distribution using the feature information con-\nditioned on frame B to estimate coarse global matches. The\nnormalized 2D feature grid f grid\nB\n∈Rh×w×2, where h and w\ndenote the resolution of the feature grid, is embedded into\nχB with an additional cosine embedding [51] to induce mul-\ntimodality in GP. The embedded coordinates are processed\nby an exponential cosine similarity kernel K to calculate\nµA|B,\nµA|B = KAB(KBB + σ2\nnI)−1χcoarse\nB\n,\n(3)\n\n\n\n\n\n\n\nKmn = exp\n \nτ\n \nfm · fn\np\n(fm · fm)(fn · fn) + ε\n−1\n!!\n,\nχcoarse\nB\n= cos(Wf grid\nB\n+ b),\n(4)\nwhere τ = 5, ϵ = 10−6, and the standard deviation of\nthe measurement noise σn = 0.1 in the experiments. W\nand b are the weights and biases of a 1 × 1 convolution\nlayer. Then, CNN embedding decoder [64] yields the ini-\ntial global matches ˆucoarse\nA→B and confidence of matches ˆc coarse\nA→B\nfrom the concatenation of the reshaped estimated posterior\nmean µgrid\nA|B and the coarse features,\n(ˆucoarse\nA→B, ˆc coarse\nA→B) = Decoder(µgrid\nA|B ⊕f coarse\nA\n).\n(5)\nAt the fine level, the warp refiners estimate the residual dis-\nplacement using the previous matches and feature informa-\ntion. The process is described as follows,\n\u0000△ˆul+1\nA→B, △ˆc l+1\nA→B\n\u0001\n= Refinerl+1\u0010\nf l+1\nA\n⊕f l+1\nB→A⊕Corrl+1\nΩk ⊕ˆul+1\nA→B−ul+1\nA\n\u0011\n,\n(6)\n\n\n\n\n\n\n\n\n\nf l+1\nB→A = fB⟨ˆul+1\nA→B⟩,\nf l+1\nB→A, Ωk = fB⟨Ωk, (ˆul+1\nA→B)⟩,\nCorrl+1\nΩk =\nX\nchannel\nf l+1\nA\nf l+1\nB→A, Ωk,\n(7)\nwhere Ωk(u) = u + p (∥p∥∞≤k) is the patch sized\nk, ⟨·⟩means the bilinear interpolation function, Corrl+1\nΩk\nrepresents local correlation between the features, and ul+1\nA\nindicates the grid in f l+1\nA . Finally, it recursively updates the\nmatching points and confidence by adding the residuals to\n3\n\n\nInput\nWarp\nEncoder\nGaussian \nProcess\nDecoder\nRefiner\nGeodesic Flow \nRefinement\nSpherical Spatial \nAlignment Module\nMulti-scale \nFeature Extraction\nPositional\nEmbedding\nSpherical\nCorrespondence\n&\nCertainty\nOutput\n𝑨\n𝑩\n𝑩→𝑨\n𝑨→𝑩\n𝝅\nCartesian\nTo\nSpherical\nSpherical\nTo\nCartesian\n𝝅!𝟏\nFigure 3.\nOverview of our approach. It consists of three steps:\nMulti-scale Feature Extraction, Spherical Spatial Alignment Mod-\nule (Sec. 4.1), and Geodesic Flow Refinement (Sec. 4.2).\nthe previous information and upsampling until reaching the\nsame resolution as the input images,\nˆul\nA→B = ˆul+1\nA→B + △ˆul+1\nA→B,\nˆc l\nA→B = ˆc l+1\nA→B + △ˆc l+1\nA→B.\n(8)\n4. Our Proposed Method\nThe overall process is illustrated in Fig. 3. Following the\napproach outlined in Section 3.2, we first utilize ERP im-\nages IA and IB as input and extract multi-scale features fA\nand fB. Different from [15], we reformulate the problem as\nfinding a mapping f →S using 3D Cartesian coordinates.\nWe introduce the Spherical Spatial Alignment Module, a\nglobal matcher utilizing a spherical camera system to com-\npensate for distortions caused by sphere-to-plane projection\nin ERP images. We then formalize the geodesic flow on\na unit sphere and establish projections between equirectan-\ngular and spherical spaces to refine matches. In addition,\nto enhance the robust accuracy of our method, we leverage\nrandomized azimuth rotation during the training process.\n4.1. Spherical Spatial Alignment Module\nOur Spherical Spatial Alignment Module (SSAM) conducts\nglobal matching at a coarse level through Gaussian Process\n(GP) regression, depicted in Fig. 4. GP predicts the poste-\nrior mean µA|B from the embeddings as in Eq. 3. Due to\nthe pronounced distortions in the polar regions of ERP im-\nages, spherical positional embedding/encoding is frequently\nemployed to mitigate this challenge [6, 30, 31]. Here, we\nexplicitly apply positional embeddings with 3D Cartesian\ncoordinates, derived from the 2D spherical feature grid and\nthe inverse transformation function π−1(·),\nχcoarse\nB\n= cos(Wπ−1(f grid\nB ) + b).\n(9)\nOur proposed positional embedding facilitates the utiliza-\ntion of embedded coordinates χcoarse\nB\nto promote distortion\nawareness within the ERP images. Additionally, this em-\nbedding ensures structural consistency along the boundaries\n𝜒!\n1x1 Conv\nSpherical Positional Embedding\n𝑓\"\n𝑓!\n⊕\n1x1 Conv\n⊗\n⊗\nembedding\nHW x HW\nC x H x W\nHW x 3\n𝜇\"|!\n𝐆𝐏( ⋅)\n𝑲𝑨𝑩\n𝑲𝑩𝑩\nGrid 𝑺𝑩\nHW x C’\n1 x H x W\nshared \nweights\nHW x HW\nC x H x W\nHW x C’\n3 x H x W\nFeatures\nPositional \nEmbedding\nPosterior \nMean\nDecoder\n(Eq. 9)\nCertainty *𝒄𝑨→𝑩\n𝐜𝐨𝐚𝐫𝐬𝐞\nGlobal Match ,𝑺𝑨→𝑩\n𝐜𝐨𝐚𝐫𝐬𝐞\nFigure 4. Our Spherical Spatial Alignment Module. We present\nSpherical Positional Embedding (red dotted box). The embed-\nding decoder generates the global matches ˆScoarse\nA→B. Here, the gray\ncurved lines represent the geodesic flow between SA and SB. ⊕\ndenotes concatenation, ⊗means reshape and matrix multiplica-\ntion. We provide the matrix dimensions of intermediate features\nfor reference.\n⊕\n⟨⋅⟩\n$𝒖!→#\n$%&\n△$𝒖!→#\n$%&\n$𝒖!→#\n$\n'𝑺!→#\n$\n𝝅\n𝝅\"𝟏\n'𝑺!→#\n$%&\n$𝒖!→#\n$%& −𝒖!\n$%&\n△̂𝑐!→#\n$%&\nK x H x W\n2 x H x W\n𝒇𝑨\n𝒍\n𝒇𝑩\n𝒍\n𝑪𝒐𝒓𝒓𝛀𝒌\n𝒍\n𝒇𝑩→𝑨\n𝒍\nC x H x W\n3 x H x W\n1x1 Conv\nC’ x H x W\n2 x H x W\n1 x H x W\n2 x H x W\n3 x H x W\nBilinear \nInterpolation\nFeatures\n3D Cartesian To  2D Spherical\n2D Spherical To 3D Cartesian\n(Eq. 11)\n(Eq. 12)\nRefiner𝒍+1\nFigure 5. Our proposed Geodesic Flow Refinement. Refining the\ndisplacement along curved lines on the spherical surface presents\nsignificant challenges. To address this, we project the displace-\nment into the ERP space for refinement (Cartesian to spherical)\nand subsequently unproject it back onto the spherical surface for\nfurther refinement (spherical to Cartesian).\nof ERP images by leveraging relative spatial information\nwithin the 3D Cartesian grid. The outputs of the subse-\nquent embedding decoder provide the initial global matches\nˆScoarse\nA→B on the unit sphere and the ERP certainty map ˆccoarse\nA→B,\n\u0010\nˆScoarse\nA→B, ˆccoarse\nA→B\n\u0011\n= Decoder(µA|B ⊕f coarse\nA\n).\n(10)\n4.2. Geodesic Flow Refinement\nIn our SSAM approach, as the geodesic flow must reside\non the unit sphere, directly defining warp refinement on\nthe surface of the sphere makes it impossible to update the\nresiduals linearly. Thus, we circumvent this problem by en-\nabling a conversion between the 3D Cartesian coordinates\nand the 2D equirectangular space, as illustrated in Fig. 5,\nˆul+1\nA→B = π(ˆSl+1\nA→B).\n(11)\nAfter following all the processes outlined in Eq. 6 for re-\nfinement, we update the residuals as described in Eq. 8.\n4\n\n\n𝑾𝒐𝒓𝒍𝒅\n𝑻\n𝑻𝒂𝒖𝒈\n𝜽𝒂𝒖𝒈\n𝐼𝐴←𝐼𝐴⟨𝜋𝑇𝐴\n𝑎𝑢𝑔𝜋−1 𝐼𝐴\n𝑔𝑟𝑖𝑑\n⟩\n𝐷𝐴←𝐷𝐴⟨𝜋𝑇𝐴\n𝑎𝑢𝑔𝜋−1 𝐷𝐴\n𝑔𝑟𝑖𝑑\n⟩\n𝑇𝐴←𝑇𝐴𝑇𝐴\n𝑎𝑢𝑔\nFigure 6. Maintaining consistent geometry, ERP can produce mul-\ntiple visual representations based on θaug.\nAs this refinement stage iterates repeatedly, the predicted\nˆul\nA→B is back-projected into 3D Cartesian coordinates,\nˆSl\nA→B = π−1(ˆul\nA→B).\n(12)\n4.3. Augmentation\nA single omnidirectional image can be transformed into\nmultiple distinct ERP images, as shown in Fig. 6. This\ntransformation is feasible by capturing the full spectrum of\nrays and ensuring a seamless representation in the spherical\ninput image, which facilitates the generation of diverse ERP\nimages while maintaining consistent geometric properties\nin the world space. Consequently, we define a horizontal\nrotation matrix T aug\nA\nwith a randomly selected azimuth an-\ngle θaug\nA ∈[0, 2π] during training. Based on T aug\nA , we rotate\nand redefine the ERP image IA, the depth map DA, and the\npose TA. Notably, this transformation adjusts TA and DA\ntogether, ensuring consistent geometry in the world space.\nThe same process is applied to the counterpart frame B.\n4.4. Loss\nUtilizing dense ground truth depth maps and aligned camera\nposes, we can derive ERP depth DA→B and matches SA→B\nduring the warping process from frame A to B within the\nspherical coordinate system. We adopt the certainty estima-\ntion method proposed by Edstedt et al. [15], which involves\nfinding consistent matches using relative depth consistency\nbetween frames A and B,\ncA→B =\n\f\f\f\f\nDA→B −DB\nDB\n\f\f\f\f < α,\n(13)\nwhere α is 0.05. The binary mask cA→B represents the\nground truth certainty map. Diverging from the approach\noutlined in Edstedt et al. [15], our method constrains the\npredicted matches ˆSl\nA→B, composed of 3D Cartesian co-\nordinates, to reside on the surface of the unit sphere. This\nimplies that the predicted matches can be interpreted as the\nray directions of the spherical camera. Instead of defining\nthe loss function based on the Euclidean distance between\nthe predicted matches ˆSl\nA→B and the ground truth matches\nSl\nA→B, we use the angular difference between the ray di-\nrections. Consequently, this approach ensures that ˆSl\nA→B\nis optimized along the surface of the unit sphere. We define\nour regression loss Ll\nr using cosine similarity to measure the\nangular difference. For the certainty loss Ll\nc, we employ the\nbinary cross-entropy function, as utilized in Edstedt et al.\n[15],\nLl\nr =\nX\ngrid\ncl\nA→B ⊙(1 −∥Sl\nA→B · ˆSl\nA→B∥\n∥Sl\nA→B∥∥ˆSl\nA→B∥\n),\n(14)\nLl\nc =\nX\ngrid\ncl\nA→Blogˆcl\nA→B + (1 −cl\nA→B)log(1 −ˆcl\nA→B).\n(15)\nThe total loss function comprises a weighted sum of the re-\ngression loss and the certainty loss, as detailed in Edstedt\net al. [15], Melekhov et al. [39], Tan et al. [56], Zhou et al.\n[70], with λ set at 0.01,\nLtotal =\nL\nX\nl=1\nLl\nr + λLl\nc.\n(16)\n5. Experiments\n5.1. Experiments Settings\nMatterport3D Dataset\nTraining our method requires\nERP input images, ground truth depth maps, and aligned\nposes. The Matterport3D dataset [5] encompasses 90 in-\ndoor scenes represented by 10,800 panoramas reconstructed\nas textured meshes. However, the dataset lacks pose and\ndepth information for skybox images, which are essential\nfor creating ERP images. Previous works have addressed\nthis limitation by rendering both images and depth maps\nfrom the textured mesh [71] or by employing 360° SfM\nto estimate poses [45]. In our approach, we generate the\nposes for skybox images directly from the originally pro-\nposed camera poses in Matterport3D. Through experimen-\ntation, we found that treating the 12th camera pose, out of\nthe 18 viewpoints (comprising 6 rotations and 3 tilt angles)\nin each panorama, identically to the second skybox image\ndid not result in any issues. We define the remaining poses\nfor the skybox images by rotating 90° in each direction from\nthe second pose. We adhere to the official benchmark split,\nutilizing 61 scenes for training, 11 for validation, and 18\nfor testing. For two-view pose estimation, it is necessary\nto create pairs of overlapped images. We achieve this by\ntransforming ERP depth maps between frames within the\nspherical coordinate system. Pixels where the depth differ-\nence is below a specified threshold, e.g. 0.1, are classified as\ninliers. Subsequently, we compare the ratio of these inliers\nto the total number of pixels. We organize both the train-\ning and testing datasets based on the overlap ratio of image\npairs and the benchmark split. Specifically, images with the\noverlap ratio exceeding 30% are distributed into respective\ntraining and testing splits. As a result, the training set con-\ntains 44,700 pairs, while the test set comprises 4,575 pairs.\n5\n\n\nWe resize the resolution of ERP images and depth maps to\n640 × 320.\nStanford2D3D Dataset\nStanford2D3D [2] consists of\ndata scanned from six large-scale indoor spaces collected\nfrom three distinct buildings. This dataset contains a rela-\ntively small number of 1,413 panorama images and, there-\nfore, is utilized exclusively for testing purposes. We assess\nthe overlap ratio between frames and include them in the\ntest split if their ratio exceeds 50%. A total of 3,460 pairs\nare incorporated into the test set. During testing, we resize\nthe resolution to 640 × 320.\nEgoNeRF and OmniPhotos Dataset\nEgoNeRF [7] intro-\nduces 11 synthetic scenes created with Blender [10] and\n11 real scenes captured with a RICOH THETA V cam-\nera. OmniPhotos [4] provides a dataset captured with an\nInsta360 ONE X camera. Both datasets contain egocentric\nscenes captured with a casually rotating camera stick. Con-\nsequently, their rotation axes, pole regions, or camera height\nchange, resulting in different distortions compared to Mat-\nterport3D or Stanford2D3D. We present additional qualita-\ntive results from these datasets to validate our method.\nImplementation Details\nWe employ the AdamW [35]\noptimizer with a weight-decay factor of 10−2, a learning\nrate of 5·10−6 for multiscale feature extractor, and 10−4 for\nthe SSAM and the Geodesic Flow Refiner. EDM is trained\nfor 300,000 steps with a batch size of 4 in a single RTX 3090\nGPU, which takes approximately two days to complete.\nDuring evaluation, the balanced sampling approach using\nkernel density estimation [15] tends to establish correspon-\ndences primarily in concentrated areas with high probability\ndistributions, making it unsuitable for omnidirectional im-\nages. Thus, we randomly sample up to 5,000 matches after\ncertainty filtering with a threshold of 0.8 to ensure corre-\nspondences cover the entire area.\n5.2. Experimental Results\nWe compare our proposed method EDM with four differ-\nent methods: 1) SPHORB [69] is a hand-crafted keypoint-\nbased feature matching algorithm. 2) SphereGlue [19] is a\nlearning-based keypoint matching method. Both SPHORB\n[69] and SphereGlue [19] are specifically designed for\nspherical images.\n3) DKM [15] and 4) RoMa [16] are\nstate-of-the-art dense matching algorithms for perspective\nimages. To estimate the essential matrix and the relative\npose for spherical cameras, Solarte et al. [52] proposed a\nnormalization strategy and non-linear optimization within\nthe classic 8-point algorithm. We adopt this for two-view\npose estimation in all quantitative comparisons.\nTable 1 shows the quantitative results of the pose esti-\nmation in Matterport3D. Despite SPHORB and SphereGlue\nMethod\nImage\nFeature\nAUC\n@5°\n@10°\n@20°\nSPHORB [69]\nERP\nsparse\n0.38\n1.41\n3.99\nSphereGlue [19]\nERP\nsparse\n11.29\n19.95\n31.10\nDKM [15]\npersepctive\ndense\n18.43\n28.50\n38.44\nRoMa [16]\nperspective\ndense\n12.45\n22.37\n34.24\nEDM (ours)\nERP\ndense\n45.15\n60.99\n73.60\nTable 1. Quantitative comparison on Matterport3D with recent\nalgorithms. EDM improves AUC@5° by 26.72.\nMethod\nImage\nFeature\nAUC\n@5°\n@10°\n@20°\nSPHORB [69]\nERP\nsparse\n0.14\n1.01\n4.08\nSphereGlue [19]\nERP\nsparse\n11.25\n22.41\n36.57\nDKM [15]\nperspective\ndense\n12.46\n22.18\n34.13\nRoMa [16]\nperspective\ndense\n11.48\n22.52\n37.07\nEDM (ours)\nERP\ndense\n55.08\n71.65\n82.72\nTable 2. Quantitative comparison on Stanford2D3D with recent\nalgorithms. EDM improve AUC@5° by 42.62.\nbeing designed for the ERP images, the presence of tex-\ntureless or repetitive regions, which are common in indoor\nenvironments of Matterport3D, leads to performance degra-\ndation in the keypoint-based methods. SPHORB fails to es-\ntimate the essential matrix correctly due to the limited num-\nber of matching points. EDM demonstrates significantly\nhigher performance than all the other methods.\nFigure 7 illustrates the qualitative results in Matter-\nport3D. The previous methods designed for perspective im-\nages, such as DKM and RoMa, exhibit good matching abil-\nity but encounter challenges when confronted with the dis-\ntortions of ERP. While SphereGlue and SPHORB perform\nwell in discriminative regions, their performance deterio-\nrates as the overlap ratio decreases, resulting in numerous\nfalse positive matches. In contrast, EDM can estimate dense\ncorrespondences regardless of occlusion and textureless ar-\neas.\nDue to the similarity in results between DKM and\nRoMa, we have only included the former to maintain a con-\ncise visualization. Experimental results in Fig.8 depict the\nrelationship between image overlap ratio and AUC@20°\nperformance. As expected, a decrease in the overlap ra-\ntio leads to severe performance degradation in the previous\nworks. On the other hand, our proposed method demon-\nstrates robustness in more challenging scenes, maintaining\nsimilar performance levels until the overlap decreases to\n60%, compared to other methods.\nFor a fair comparison, we use another benchmark\ndataset, Stanford2D3D. We validate EDM using a model\ntrained on Matterport3D without additional training on\nStanford2D3D. In Table 2, EDM outperforms the previous\n6\n\n\n(a) Keypoint-based\n(b) DKM\n(c) EDM (ours)\n(a) Keypoint-based\n(b) DKM\n(c) EDM (ours)\n90% ⎼\n80% ⎼90%\n70% ⎼80%\n60% ⎼70%\n50% ⎼60%\n40% ⎼50%\n30% ⎼40%\n60% ⎼70%\nFigure 7. Qualitative results on Matterport3D. (a) The blue lines represent the results of matching points from SPHORB [69]; the green\nlines correspond to SphereGlue [19]. Both (b) DKM [15] and (c) EDM depict the outcomes of multiplying the warped image with the\ncertainty map. EDM can estimate dense and accurate matches even in the presence of distortions and severe occlusions. The numbers\nbeside the images represent the overlap ratio, reflecting the difficulty of matching. Smaller numbers indicate more challenging scenes.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n30 ⎼40% 40 ⎼50% 50 ⎼60% 60 ⎼70% 70 ⎼80% 80 ⎼90%\n90% ⎼\nSphereGlue\nDKM\nEDM (ours)\nOverlap Ratio\nAUC @ 20°\nFigure 8. Performance with respect to the overlap ratio. This high-\nlights the robustness of EDM in scenarios with varying levels of\noverlap, particularly in challenging conditions where the overlap\nratio is limited.\nworks by a significant margin, especially in scenes with se-\nvere occlusion. The certainty map demonstrates EDM’s ro-\nbustness, particularly in handling occluded scenes. Addi-\ntionally, although the panorama images in Stanford2D3D\nKeypoint-based\nEDM (ours)\nKeypoint-based\nEDM (ours)\nFigure 9. Qualitative results on Stanford2D3D. The blue and green\nlines correspond to SPHORB and SphereGlue.\ncontain missing regions in the upper and lower parts of\nthe sphere, the proposed spherical positional embedding en-\nables the network to predict matching correspondences ac-\ncurately, as shown in Fig. 9.\n5.3. Additional Qualitative Results\nTo demonstrate the robust performance of our method\nacross diverse environments, we qualitatively validate EDM\nusing additional datasets such as EgoNeRF and OmniPho-\ntos. As it is primarily trained on indoor environments [5]\nwhere the camera is oriented parallel to gravity, severely\nslanted image pairs of rotational scenes or outdoor envi-\n7\n\n\nImages\nWarp\nImage\nWarp\nFigure 10. Qualitative results on EgoNeRF [7] and OmniPhotos\n[4]. Despite being primarily trained on indoor scenes, EDM effec-\ntively estimates dense matching on these datasets, demonstrating\nits generalization capability across diverse environments.\nronments may cause EDM to fail in accurately estimating\ncorrespondences. However, despite these differences in set-\ntings, EDM demonstrates the ability to conduct dense fea-\nture matching robustly, as shown in Fig. 10.\nFurthermore, we demonstrate the applicability of our\nmethod to various omnidirectional downstream tasks. As\nshown in Fig. 11, our approach successfully performs trian-\ngulation from pairs of omnidirectional images. By leverag-\ning EDM’s capability to predict dense correspondences, the\ntriangulated points yield a dense 3D reconstruction. For a\nmore comprehensive discussion, please refer to the supple-\nmentary materials.\n5.4. Ablation Study\nDKM’s dependence on the pinhole camera model makes it\ninherently unsuitable for learning with ERP images. To en-\nsure the fair comparison, we modified the warping process\nin the loss function of DKM to support spherical cameras,\nresulting in DKM∗. As shown in Table 3, this demonstrates\nthe structural effectiveness of our proposed bidirectional co-\nordinate transformation. The proposed positional embed-\ndings result in improvements based on the coordinate sys-\ntem of the spherical camera model. We observe that utiliz-\ning a 3D grid input of Cartesian coordinates yields better\nperformance than 2D spherical ones. Additionally, in our\nmethod, positional embedding with a linear layer slightly\noutperforms spherical positional encoding with sinusoidal\n[31]. Table 3 also confirms the advantage of our rotational\naugmentation. Through this augmentation technique, we\ncan effectively address the challenge of a limited number\nof datasets for omnidirectional images in dense matching\ntasks.\n6. Conclusion, Limitations, and Future Work\nIn this paper, we present, for the first time, a novel dense\nfeature matching method tailored for omnidirectional im-\nMethod\nPositional\nBidirectional\nRotational\nAUC\nEmbedding\nTransformation\nAugmentation\n@5°\n@10°\n@20°\nDKM∗\n2D linear\n-\n-\n19.83\n33.06\n46.24\nOurs\n2D linear\n✓\n-\n29.67\n45.90\n60.82\nOurs\n2D linear\n✓\n✓\n35.03\n51.14\n65.07\nOurs\n3D linear\n✓\n-\n34.64\n50.82\n65.16\nOurs\n3D linear\n✓\n✓\n45.15\n60.99\n73.60\nOurs\n3D sinusoidal\n✓\n✓\n42.39\n58.27\n70.98\nTable 3. Ablation study for the proposed method. DKM∗indi-\ncates the DKM model trained on Matterport3D with a modified\nloss function for ERP images. Compared to DKM∗, our method\nenhances performance through the proposed spherical positional\nembedding in SSAM, bidirectional transformation via Geodesic\nFlow Refinement, and rotational augmentation.\nFigure 11.\nTriangulation results on Matterport3D and Stan-\nford2D3D. These point clouds are generated through spherical tri-\nangulation using the estimated poses between two omnidirectional\nimages. Our method can reconstruct dense point clouds in texture-\nless regions, which are particularly challenging in indoor environ-\nments.\nages. Leveraging the foundational principles of DKM, we\nintegrate the inherent characteristics of the spherical camera\nmodel into our dense matching process using geodesic flow\nfields. This integration instills distortion awareness within\nthe network, thereby enhancing its performance specifically\nfor ERP images. However, it is important to note that our\nmethod is predominantly trained on indoor datasets where\nthe camera is vertically oriented, rendering it somewhat vul-\nnerable to extreme rotations or outdoor environments. To\naddress this limitation, future endeavors will focus on di-\nversifying the training data and data augmentation to en-\ncompass a wider range of environments, fortifying the ro-\nbustness of our network. Furthermore, we aim to extend\nour method into downstream tasks, particularly for visual\nlocalization and mapping applications for omnidirectional\nimages.\n8\n\n\nReferences\n[1] Friska Amalia and Ahmad Fitriyansah. Case study of 360\nimage viewer software utilization in interior design presenta-\ntion to improve product immersion. In ICCED. IEEE, 2023.\n1\n[2] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.\nJoint 2d-3d-semantic data for indoor scene understanding.\narXiv preprint arXiv:1702.01105, 2017. 2, 6\n[3] Matheus A Bergmann, Paulo GL Pinto, Thiago LT da Sil-\nveira, and Cl´audio R Jung.\nGravity alignment for single\npanorama depth inference. In SIBGRAPI. IEEE, 2021. 2\n[4] Tobias Bertel, Mingze Yuan, Reuben Lindroos, and Christian\nRichardt. Omniphotos: casual 360 vr photography. ACM\nTransactions on Graphics (TOG), 39(6):1–12, 2020. 2, 6, 8\n[5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang.\nMatterport3d: Learning\nfrom rgb-d data in indoor environments.\narXiv preprint\narXiv:1709.06158, 2017. 1, 2, 5, 7\n[6] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light:\nZero-shot text-driven hdr panorama generation. ACM Trans-\nactions on Graphics (TOG), 41(6):1–16, 2022. 4\n[7] Changwoon Choi, Sang Min Kim, and Young Min Kim. Bal-\nanced spherical grid for egocentric view synthesis. In CVPR,\n2023. 2, 6, 8\n[8] Dongyoung Choi, Hyeonjoong Jang, and Min H Kim. Om-\nnilocalrf: Omnidirectional local radiance fields from dy-\nnamic videos. arXiv preprint arXiv:2404.00676, 2024. 2\n[9] Taco S Cohen, Mario Geiger, Jonas K¨ohler, and Max\nWelling. Spherical cnns. arXiv preprint arXiv:1801.10130,\n2018. 2\n[10] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 6\n[11] Benjamin Coors, Alexandru Paul Condurache, and Andreas\nGeiger. Spherenet: Learning spherical representations for\ndetection and classification in omnidirectional images.\nIn\nECCV, 2018. 2\n[12] Thiago LT da Silveira, Paulo GL Pinto, Jeffri Murrugarra-\nLlerena, and Cl´audio R Jung. 3d scene geometry estimation\nfrom 360 imagery: A survey. ACM Computing Surveys, 55\n(4):1–39, 2022. 1\n[13] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. Superpoint: Self-supervised interest point detection\nand description. In CVPR Workshops, 2018. 2\n[14] Marc Eder, Mykhailo Shvets, John Lim, and Jan-Michael\nFrahm. Tangent images for mitigating spherical distortion.\nIn CVPR, 2020. 3\n[15] Johan Edstedt, Ioannis Athanasiadis, M˚arten Wadenb¨ack,\nand Michael Felsberg.\nDKM: Dense kernelized feature\nmatching for geometry estimation. In CVPR, 2023. 1, 2,\n3, 4, 5, 6, 7\n[16] Johan\nEdstedt,\nQiyu\nSun,\nGeorg\nB¨okman,\nM˚arten\nWadenb¨ack, and Michael Felsberg. Roma: Revisiting ro-\nbust losses for dense feature matching.\narXiv preprint\narXiv:2305.15404, 2023. 2, 6\n[17] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-\ndia, and Kostas Daniilidis. Learning so (3) equivariant rep-\nresentations with spherical cnns. In ECCV, 2018. 2\n[18] Cornelia Ferm¨uller and Yiannis Aloimonos. Geometry of\neye design: Biology and technology. In Multi-Image Anal-\nysis: 10th International Workshop on Theoretical Founda-\ntions of Computer Vision Dagstuhl Castle, Germany, March\n12–17, 2000 Revised Papers, pages 22–38. Springer, 2001. 1\n[19] Christiano Gava, Vishal Mukunda, Tewodros Habtegebrial,\nFederico Raue, Sebastian Palacio, and Andreas Dengel.\nSphereglue: Learning keypoint matching on high resolution\nspherical images. In CVPR Workshops, 2023. 2, 3, 6, 7\n[20] Christiano Gava, Yunmin Cho, Federico Raue, Sebastian\nPalacio, Alain Pagani, and Andreas Dengel. Spherecraft: A\ndataset for spherical keypoint detection, matching and cam-\nera pose estimation. In WACV, 2024. 3\n[21] Julia Guerrero-Viu, Clara Fernandez-Labrador, C´edric De-\nmonceaux, and Jose J Guerrero. What’s in my room? object\nrecognition on indoor panoramic images. In ICRA. IEEE,\n2020. 1\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 3\n[23] Will Hutchcroft, Yuguang Li, Ivaylo Boyadzhiev, Zhiqiang\nWan, Haiyan Wang, and Sing Bing Kang. Covispose: Co-\nvisibility pose transformer for wide-baseline relative pose\nestimation in 360° indoor panoramas. In ECCV. Springer,\n2022. 3\n[24] Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Philip Mar-\ncus, Matthias Niessner, et al. Spherical cnns on unstructured\ngrids. arXiv preprint arXiv:1901.02039, 2019. 2\n[25] Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui\nHuang. Unifuse: Unidirectional fusion for 360 panorama\ndepth estimation. IEEE Robotics and Automation Letters, 6\n(2):1519–1526, 2021. 2\n[26] Hakyeong Kim, Andreas Meuleman, Hyeonjoong Jang,\nJames Tompkin, and Min H Kim.\nOmnisdf: Scene re-\nconstruction using omnidirectional signed distance functions\nand adaptive binoctrees. arXiv preprint arXiv:2404.00678,\n2024. 2\n[27] Jong Weon Lee, Suya You, and Ulrich Neumann.\nLarge\nmotion estimation for omnidirectional vision. In Proceed-\nings IEEE Workshop on Omnidirectional Vision (Cat. No.\nPR00704), pages 161–168. IEEE, 2000. 1\n[28] Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,\nand Yulan Guo. Decoupling makes weakly supervised local\nfeature better. In CVPR, 2022. 2\n[29] Longwei Li, Huajian Huang, Sai-Kit Yeung, and Hui Cheng.\nOmnigs: Omnidirectional gaussian splatting for fast radiance\nfield reconstruction using omnidirectional images.\narXiv\npreprint arXiv:2404.03202, 2024. 2\n[30] Meng Li, Senbo Wang, Weihao Yuan, Weichao Shen, Zhe\nSheng, and Zilong Dong. S2Net: Accurate panorama depth\nestimation on spherical surface. IEEE Robotics and Automa-\ntion Letters, 8(2):1053–1060, 2023. 4\n[31] Xiang Li, Haoyuan Cao, Shijie Zhao, Junlin Li, Li Zhang,\nand Bhiksha Raj. Panoramic video salient object detection\nwith ambisonic audio guidance. In AAAI, 2023. 4, 8\n9\n\n\n[32] Yuyan Li, Zhixin Yan, Ye Duan, and Liu Ren. Panodepth:\nA two-stage approach for monocular omnidirectional depth\nestimation. In 3DV. IEEE, 2021. 2\n[33] Yuyan Li, Yuliang Guo, Zhixin Yan, Xinyu Huang, Ye Duan,\nand Liu Ren. Omnifusion: 360 monocular depth estimation\nvia geometry-aware fusion. In CVPR, 2022. 2\n[34] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao,\nand Xiaowei Zhou. Gift: Learning transformation-invariant\ndense visual descriptors via group cnns. Advances in Neural\nInformation Processing Systems, 32, 2019. 2\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[36] David G Lowe.\nDistinctive image features from scale-\ninvariant keypoints. International journal of computer vi-\nsion, 60:91–110, 2004. 2\n[37] Yikun Ma, Dandan Zhan, and Zhi Jin. Fastscene: Text-driven\nfast 3d indoor scene generation via panoramic gaussian splat-\nting. arXiv preprint arXiv:2405.05768, 2024. 2\n[38] Kevin Matzen, Michael F Cohen, Bryce Evans, Johannes\nKopf, and Richard Szeliski. Low-cost 360 stereo photog-\nraphy and video capture. ACM Transactions on Graphics\n(TOG), 36(4):1–12, 2017. 1\n[39] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc\nPollefeys, Esa Rahtu, and Juho Kannala. Dgc-net: Dense\ngeometric correspondence network. In WACV. IEEE, 2019.\n2, 5\n[40] Emanuele Menegatti, Takeshi Maeda, and Hiroshi Ishiguro.\nImage-based memory for robot navigation using properties\nof omnidirectional images. Robotics and Autonomous Sys-\ntems, 47(4):251–267, 2004. 1\n[41] Negar\nNejatishahidin,\nWill\nHutchcroft,\nManjunath\nNarayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khos-\nravan, Jana Koˇseck´a, and Sing Bing Kang.\nGraph-covis:\nGnn-based multi-view panorama global pose estimation. In\nCVPR, 2023. 3\n[42] Randal C Nelson and John Aloimonos. Finding motion pa-\nrameters from spherical motion fields (or the advantages of\nhaving eyes in the back of your head). Biological cybernet-\nics, 58(4):261–273, 1988. 1\n[43] Gaurav Pandey, James R McBride, and Ryan M Eustice.\nFord campus vision and lidar data set.\nThe International\nJournal of Robotics Research, 30(13):1543–1552, 2011. 1\n[44] Jerome Revaud, Cesar De Souza, Martin Humenberger, and\nPhilippe Weinzaepfel. R2d2: Reliable and repeatable detec-\ntor and descriptor. Advances in neural information process-\ning systems, 32, 2019. 2\n[45] Manuel Rey-Area, Mingze Yuan, and Christian Richardt.\n360monodepth: High-resolution 360deg monocular depth\nestimation. In CVPR, 2022. 2, 5\n[46] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary\nBradski. Orb: An efficient alternative to sift or surf. In ICCV.\nIeee, 2011. 2, 3\n[47] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich.\nSuperglue:\nLearning feature\nmatching with graph neural networks. In CVPR, pages 4938–\n4947, 2020. 2\n[48] Olivier Saurer, Friedrich Fraundorfer, and Marc Pollefeys.\nOmnitour: Semi-automatic generation of interactive virtual\ntours from omnidirectional video. In 3DPVT, 2010. 1\n[49] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In CVPR, 2016. 1\n[50] Zhijie Shen, Chunyu Lin, Kang Liao, Lang Nie, Zishuo\nZheng, and Yao Zhao. Panoformer: Panorama transformer\nfor indoor 360° depth estimation. In ECCV. Springer, 2022.\n2\n[51] Herman P Snippe and Jan J Koenderink.\nDiscrimination\nthresholds for channel-coded systems. Biological cybernet-\nics, 66(6):543–551, 1992. 3\n[52] Bolivar Solarte, Chin-Hsuan Wu, Kuan-Wei Lu, Yi-Hsuan\nTsai, Wei-Chen Chiu, and Min Sun. Robust 360-8pa: Re-\ndesigning the normalized 8-point algorithm for 360-fov im-\nages. In ICRA. IEEE, 2021. 6\n[53] Yu-Chuan Su and Kristen Grauman. Learning spherical con-\nvolution for fast features from 360 imagery.\nAdvances in\nneural information processing systems, 30, 2017. 2\n[54] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet:\n360 indoor holistic understanding with latent horizontal fea-\ntures. In CVPR, 2021. 2\n[55] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. Loftr: Detector-free local feature matching\nwith transformers. In CVPR, 2021. 2\n[56] Dongli Tan, Jiang-Jiang Liu, Xingyu Chen, Chao Chen,\nRuixin Zhang, Yunhang Shen, Shouhong Ding, and Ron-\ngrong Ji.\nEco-tr:\nEfficient correspondences finding via\ncoarse-to-fine refinement. In ECCV. Springer, 2022. 5\n[57] Prune Truong, Martin Danelljan, and Radu Timofte. Glu-\nnet: Global-local universal network for dense flow and cor-\nrespondences. In CVPR, 2020. 2\n[58] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Learning accurate dense correspondences and when\nto trust them. In CVPR, 2021. 2\n[59] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk:\nLearning local features with policy gradient. Advances in\nNeural Information Processing Systems, 33:14254–14265,\n2020. 2\n[60] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and\nYi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via\nbi-projection fusion. In CVPR, 2020. 2\n[61] Niall Winters, Jos´e Gaspar, Gerard Lacey, and Jos´e Santos-\nVictor. Omni-directional vision for robot navigation. In Pro-\nceedings IEEE Workshop on Omnidirectional Vision (Cat.\nNo. PR00704), pages 21–28. IEEE, 2000. 1\n[62] Changhee Won, Hochang Seok, Zhaopeng Cui, Marc Polle-\nfeys, and Jongwoo Lim. Omnislam: Omnidirectional local-\nization and dense mapping for wide-baseline multi-camera\nsystems. In ICRA. IEEE, 2020. 2\n[63] Mai Xu, Chen Li, Shanyi Zhang, and Patrick Le Callet.\nState-of-the-art in 360 video/image processing: Perception,\nassessment and compression. IEEE Journal of Selected Top-\nics in Signal Processing, 14(1):5–26, 2020. 1, 2\n[64] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. Learning a discriminative feature\nnetwork for semantic segmentation. In CVPR, 2018. 3\n10\n\n\n[65] Ilwi Yun, Hyuk-Jae Lee, and Chae Eun Rhee. Improving 360\nmonocular depth estimation via non-local dense prediction\ntransformer and joint supervised and self-supervised learn-\ning. In AAAI, 2022. 2\n[66] Chao Zhang, Stephan Liwicki, William Smith, and Roberto\nCipolla. Orientation-aware semantic segmentation on icosa-\nhedron spheres. In ICCV, 2019. 2\n[67] Fanglue Zhang, Junhong Zhao, Yun Zhang, and Stefanie\nZollmann. A survey on 360° images and videos in mixed\nreality: Algorithms and applications. Journal of Computer\nScience and Technology, 38(3):473–491, 2023. 1\n[68] Hengzhi Zhang, Hong Yi, Haijing Jia, Wei Wang, and\nMakoto Odamaki. Panopoint: Self-supervised feature points\ndetection and description for 360deg panorama. In CVPR\nWorkshops, 2023. 3\n[69] Qiang Zhao, Wei Feng, Liang Wan, and Jiawan Zhang.\nSphorb: A fast and robust binary feature on the sphere. In-\nternational journal of computer vision, 113:143–159, 2015.\n2, 3, 6, 7\n[70] Qunjie Zhou,\nTorsten Sattler,\nand Laura Leal-Taixe.\nPatch2pix: Epipolar-guided pixel-level correspondences. In\nCVPR, 2021. 5\n[71] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,\nand Petros Daras. Omnidepth: Dense depth estimation for\nindoors spherical panoramas. In ECCV, 2018. 5\n11\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20685v1.pdf",
    "total_pages": 11,
    "title": "EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching",
    "authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Yonghan Lee",
      "Somi Jeong",
      "Taejae Lee",
      "Dinesh Manocha",
      "Suyong Yeon"
    ],
    "abstract": "We introduce the first learning-based dense matching algorithm, termed\nEquirectangular Projection-Oriented Dense Kernelized Feature Matching (EDM),\nspecifically designed for omnidirectional images. Equirectangular projection\n(ERP) images, with their large fields of view, are particularly suited for\ndense matching techniques that aim to establish comprehensive correspondences\nacross images. However, ERP images are subject to significant distortions,\nwhich we address by leveraging the spherical camera model and geodesic flow\nrefinement in the dense matching method. To further mitigate these distortions,\nwe propose spherical positional embeddings based on 3D Cartesian coordinates of\nthe feature grid. Additionally, our method incorporates bidirectional\ntransformations between spherical and Cartesian coordinate systems during\nrefinement, utilizing a unit sphere to improve matching performance. We\ndemonstrate that our proposed method achieves notable performance enhancements,\nwith improvements of +26.72 and +42.62 in AUC@5{\\deg} on the Matterport3D and\nStanford2D3D datasets.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}