{
  "id": "arxiv_2502.21264v2",
  "text": " \nFoundation Models - A Panacea for Artificial Intelligence in\nPathology?\nNita Mulliqi1, Anders Blilie2,3, Xiaoyi Ji1, Kelvin Szolnoky1, Henrik Olsson1, Sol Erika\nBoman1,4, Matteo Titus1, Geraldine Martinez Gonzalez1, Julia Anna Mielcarz1, Masi\nValkonen5, Einar Gudlaugsson2, Svein R. Kjosavik6,7, José Asenjo8, Marcello Gambacorta9,\nPaolo Libretti9, Marcin Braun10, Radzislaw Kordek10, Roman Łowicki11, Kristina\nHotakainen12,13, Päivi Väre14, Bodil Ginnerup Pedersen15,16, Karina Dalsgaard Sørensen16,17,\nBenedicte Parm Ulhøi18, Pekka Ruusuvuori5,19,20, Brett Delahunt21,22, Hemamali\nSamaratunga23, Toyonori Tsuzuki24, Emilius A.M. Janssen2,25,26, Lars Egevad22, \nMartin Eklund1, Kimmo Kartasalo27\n1.\nDepartment of Medical Epidemiology and Biostatistics, Karolinska Institutet, Stockholm, Sweden\n2.\nDepartment of Pathology, Stavanger University Hospital, Stavanger, Norway\n3.\nFaculty of Health Sciences, University of Stavanger, Stavanger, Norway\n4.\nDepartment of Molecular Medicine and Surgery, Karolinska Institutet, Stockholm, Sweden\n5.\nInstitute of Biomedicine, University of Turku, Turku, Finland\n6.\nThe General Practice and Care Coordination Research Group, Stavanger University Hospital, Norway\n7.\nDepartment of Global Public Health and Primary Care, Faculty of Medicine, University of Bergen,\nNorway\n8.\nDepartment of Pathology, Synlab, Madrid, Spain\n9.\nDepartment of Pathology, Synlab, Brescia, Italy\n10. Department of Pathology, Chair of Oncology, Medical University of Lodz, Lodz, Poland\n11. 1st Department of Urology, Medical University of Lodz, Lodz, Poland\n12. Department of Clinical Chemistry and Hematology, University of Helsinki, Helsinki, Finland\n13. Laboratory Services, Mehiläinen Oy, Helsinki, Finland\n14. Department of Pathology, Mehiläinen Länsi-Pohja Hospital, Kemi, Finland\n15. Department of Radiology, Aarhus University Hospital, Aarhus, Denmark\n16. Department of Clinical Medicine, Aarhus University, Aarhus, Denmark\n17. Department of Molecular Medicine, Aarhus University Hospital, Aarhus, Denmark\n18. Department of Pathology, Aarhus University Hospital, Aarhus, Denmark\n19. InFLAMES Research Flagship, University of Turku, Turku, Finland\n20. Faculty of Medicine and Health Technology, Tampere University, Tampere, Finland\n21. Malaghan Institute of Medical Research, Wellington, New Zealand\n22. Department of Oncology and Pathology, Karolinska Institutet, Stockholm, Sweden\n23. Aquesta Uropathology and University of Queensland, QLD, Brisbane, Australia\n24. Department of Surgical Pathology, School of Medicine, Aichi Medical University, Nagoya, Japan\n25. Department  of  Chemistry,  Bioscience  and  Environmental  Engineering,  University  of  Stavanger,\nStavanger, Norway\n26. Institute for Biomedicine and Glycomics, Griffith University, Queensland, Australia\n27. Department of Medical Epidemiology and Biostatistics, SciLifeLab, Karolinska Institutet, Stockholm,\nSweden\nCorresponding author: Kimmo Kartasalo, kimmo.kartasalo@ki.se.\n1\n\n\n \nAbstract\nThe role of artificial intelligence (AI) in pathology has evolved from aiding diagnostics to uncovering\npredictive morphological patterns in whole slide images (WSIs). Recently, foundation models (FMs)\nleveraging self-supervised pre-training have been widely advocated as a universal solution for diverse\ndownstream  tasks.  However,  open  questions  remain  about  their  clinical  applicability  and\ngeneralization advantages over end-to-end learning using task-specific (TS) models.\nHere, we focused on AI with clinical-grade performance for prostate cancer diagnosis and Gleason\ngrading. We present the largest validation of AI for this task, using over 100,000 core needle biopsies\nfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with a fully end-to-end TS\nmodel in a multiple instance learning framework. Our findings challenge assumptions that FMs\nuniversally outperform TS models. While FMs demonstrated utility in data-scarce scenarios, their\nperformance converged with—and was in some cases surpassed by—TS models when sufficient\nlabeled training data were available. Notably, extensive task-specific training markedly reduced\nclinically significant misgrading, misdiagnosis of challenging morphologies, and variability across\ndifferent WSI scanners. Additionally, FMs used up to 35 times more energy than the TS model,\nraising concerns about their sustainability.\nOur results underscore that while FMs offer clear advantages for rapid prototyping and research, their\nrole as a universal solution for clinically applicable medical AI remains uncertain. For high-stakes\nclinical applications, rigorous validation and consideration of task-specific training remain critically\nimportant. We advocate for integrating the strengths of FMs and end-to-end learning to achieve robust\nand resource-efficient AI pathology solutions fit for clinical use.\nKeywords: Artificial  Intelligence,  Clinical  Validation,  Computational  Pathology,  End-to-End\nLearning, Foundation Models, Generalization, Gleason Grading, Model Efficiency, Prostate Cancer,\nWhole Slide Imaging\n2\n\n\n \nIntroduction\nThe pathological assessment of tissue specimens remains a cornerstone of diagnostic and therapeutic\ndecision  making.  The  ongoing  digitization  of  pathology  has  been  closely  followed  by  the\ndevelopment of computational methods for analyzing whole slide image (WSI) data1. Initially,\ncomputational pathology mainly targeted improved diagnostic efficiency and accuracy by aiding\npathologists with decision support and automation. Recently, the focus has increasingly shifted\ntowards also discovering novel morphological patterns that are currently unknown to pathologists but\nare predictive of clinical outcomes, therapy response, or underlying molecular biomarkers. Besides\nthe growing amount of digitized specimens, progress in computational pathology has been fueled by\ndevelopments in artificial intelligence (AI).\nThe first breakthroughs in pathology image analysis were triggered by abandoning traditional image\nprocessing and feature engineering in favor of end-to-end learning2,3. In the former, researchers\nhandcraft quantitative descriptors of tissue morphology to feed machine learning models, whereas the\nlatter involves training deep neural networks to discover relevant patterns without human intervention.\nEnd-to-end learning generally produces AI models with superior performance but requires large\namounts of labeled training data4,5. Collecting sufficient annotated data to cover all pathology tasks is\ninfeasible, particularly for rare diseases or infrequently observed molecular biomarkers. Additionally,\nin contrast to the earlier analysis of micrographs or small image patches, applying end-to-end learning\nto the more clinically relevant high-resolution WSIs has been technically challenging and has forced\ndevelopers to resort to multi-step approaches featuring a chain of separately trained AI models.\nTypically, WSIs are split into patches and the outputs of a patch-level model are aggregated by\nanother AI model to obtain slide- or patient-level predictions6,7.\nA string of seminal papers on foundation models (FM)8–14 has arguably marked the beginning of a\nsecond period of rapid breakthroughs in computational pathology. This new paradigm in medical AI 15\ncircumvents the collection of vast datasets for supervised training of ‘narrow’ task-specific (TS)\nmodels. Instead, ‘broad’ generalist FMs are pre-trained in an unsupervised manner using an unlabeled\nmixture of tissue and specimen types to try and capture a generally useful quantitative representation,\nor feature embedding, of the data. An off-the-shelf FM can then be used as a feature encoder with\nvery little or no task-specific training to extract descriptors of patches or WSIs for downstream\napplications. In this sense, FMs represent a step back from end-to-end learning to a two-stage\napproach, where the data are first summarized using predefined features, which are then used for\ntraining relatively simple task-specific models.\n3\n\n\n \nThe primary focus of most studies on FMs has been their applicability as a one-size-fits-all transfer\nlearning platform to accelerate the development of AI models for a plethora of tasks, including those\nwith very limited training data available. This viewpoint is supported by conclusive evidence from\nbroad (but often superficial) evaluations across a range of tissue types and pathology tasks. However,\nthe role envisioned for FMs in medical AI extends far beyond serving as an upgrade to transfer\nlearning models pre-trained on non-medical data (e.g. ImageNet16). A more controversial hypothesis\nunderlying the ‘paradigm shift’ expectations placed on FMs is that the unprecedented scale of these\nmodels and their unlabeled training data will give rise to generalist AI models, which exhibit\nunexpected skills, or ‘emergent abilities’, and outperform TS models even in tasks that the ‘narrow’\nmodels specialize in8,17,18. For research and rapid prototyping, the prospect of satisfactory baseline\nperformance in most tasks even without any task-specific training is highly appealing. However, from\na clinical standpoint, the difference between an off-the-shelf prototype and a clinical-grade model\nwith validated robust performance in a specific task can be a matter of life and death. From this\nperspective, the main promise of FMs lies in their hypothesized ability to ‘understand’ the data, which\nshould translate to more consistent performance in real-world clinical settings where overfitting, batch\neffects and various biases are significant challenges for AI19.\nCurrently, there is a knowledge gap regarding the expected universal generalization advantage of FMs\nover end-to-end trained TS models for WSI analysis, potentially due to the sparsity of large-scale\nlabeled datasets and the computational challenges of analyzing full WSIs. In the absence of direct\ncomparisons, the superiority of FMs’ feature embeddings compared to those tailored for a given task\nvia end-to-end learning remains a conjecture. Handling entire WSIs is computationally demanding,\nbut evolving graphics processing units (GPU) and algorithmic innovations are overcoming this\nobstacle and making end-to-end learning a feasible option for modern computational pathology20–22.\nDue to the large number of model parameters that need to be optimized jointly, end-to-end training\nrequires a large and diverse training cohort to avoid overfitting and capturing biases due to e.g.\nconfounding between task labels and scanner-dependent variations in WSI appearance. Collecting\nsuch a dataset is understandably not feasible in the case of e.g. rare diseases, but vast amounts of\nWSIs are already generated from tissue types frequently analyzed in pathology laboratories, such as\nprostate specimens.\nProstate cancer is the most common cancer in men globally, and more than 2 million people undergo\nprostate biopsy annually in Europe and the US23,24. Pathological assessment for diagnosing and\nGleason grading is crucial for the clinical management of the disease, but considerable inter-observer\nvariation and the limited prognostic and treatment-predictive ability of the Gleason grading system\ncomplicate decision making25. Applying AI to harmonize grading of prostate biopsies has long been a\nwidely studied topic in computational pathology4,6,7,26–28, and recent studies have also aimed at\n4\n\n\n \nimproved prognostication of patient outcomes29 or treatment response30. Some prostate AI models are\nalso already commercially available and validated to varying degrees for clinical use31–36. Also FMs\nhave been applied to prostate cancer detection8,12 and Gleason grading9, but no extensive validation\ncovering different patient populations, laboratories, and scanners has been reported.\nHere, we approach the problem of AI-based prostate cancer diagnosis and Gleason grading with a\nweakly supervised, task-specific model trained in true end-to-end fashion on entire WSIs using slide-\nlevel labels. The model was trained and retrospectively validated on cohorts that represent the most\nextensive external validation of AI for prostate cancer grading to date, including approximately\n100,000 core needle biopsies from 7,342 patients, collected across 15 clinical sites and trials spanning\n11 countries (Figure 1). We address the challenges associated with validating AI models—namely,\nthe absence of standardized study designs, pre-registered protocols, and appropriate external cohort\nsampling37,38—through  rigorously  following  a  pre-specified  study  protocol,  which  also  includes\ndetailed descriptions of the patient cohorts included in this study (Supplementary Appendix 1)39.\nUsing varying amounts of task-specific training data, we apply two recent FMs to this task and\npresent the first direct comparison between foundation models and supervised WSI-level end-to-end\nlearning in the context of a clinically relevant, large-scale retrospective validation of AI diagnostics.\nResults\nFoundation models’ Gleason grading is improved with task-specific\ntraining\nPrevious studies have advocated using FMs as feature encoders in a few-shot learning setup without\nextensive training data for a specific downstream task8,9. To investigate whether FMs require large\ntask-specific training datasets to reach clinically optimal performance in the diagnosis and Gleason\ngrading of prostate cancer, we trained models with increasing amounts of WSIs of prostate core\nneedle biopsies ranging between 1%-100% of the full training set (Extended Data Figure 1E)\ncollected in the Swedish STHLM3 trial40 and at Stavanger University Hospital, Norway and Capio S:t\nGöran Hospital, Sweden, and evaluated the models on 12 international cohorts (Figure 1). The slides\nwere sampled randomly and added cumulatively. We trained models based on the UNI (UFM)9 and\nVirchow2 (VFM)8,41 foundation models as frozen feature encoders alongside an in-house developed,\nfully end-to-end TS model relying on a trainable EfficientNet-V2S encoder (initialized with ImageNet\nweights)42. All three models used identical pre-processing for extracting tissue patches from the WSIs,\nthe  well-established  attention-based  multiple  instance  learning  (ABMIL)  architecture20 for\naggregating  patch-level  feature  embeddings  into  WSI-level  feature  vectors,  and  additional\n5\n\n\n \nclassification layers for predicting the primary and secondary Gleason patterns for a WSI (Figure\n2A). While having been trained in weakly supervised fashion using only slide-level Gleason score\n(GS) labels, the model architecture still allows for efficiently capturing and visually presenting the\nspatial distribution of distinct Gleason patterns (Figure 2B-2D).\nThe models were evaluated on held-out validation datasets representing both internal cohorts, i.e. data\noriginating from the same laboratory and/or WSI scanner as training data but independent patients\n(n=14,808 WSIs), and fully external cohorts, i.e. data from different laboratories, scanners, and\npatients than the training data (n=10,801 WSIs). We quantified the models’ concordance with\npathologists in terms of Cohen’s quadratically weighted kappa (QWK) for the International Society of\nUrological Pathology (ISUP) grade, a five-level grouping of Gleason scores also known as the WHO\ngrade or grade group43. All models benefited from increased amounts of data and reached their\nmaximal performance in internal (Figure 2E) and external (Figure 2F) validation when using 100%\nof the training set (n=55,798 WSIs). Considering the expected ability of FMs to function well even\nwith limited task-specific training, it is surprising that with 1% of the training set (n=524 WSIs), the\nUFM and TS models performed comparably. In contrast, VFM met the expectation of performing\nrelatively well already with 1% of the data but still exhibited markedly improved performance with\nmore training. From a clinical standpoint, the improvements in QWK observed for UFM (0.672 to\n0.915 on internal data, 0.637 to 0.856 on external data) and for VFM (0.862 to 0.911 on internal data,\n0.821 to 0.849 on external data) are considerable and suggest FMs may still need relatively large task-\nspecific training datasets for optimal performance.\nIn view of the assumed superior generalization performance of FMs, their advantage appears to be\nlimited to the 1%-15% training data regime, with the TS model slightly outperforming UFM and\nVFM in terms of overall QWK on the external cohorts when more than 20% of the training data\n(n=10,326 WSIs) became available (Figure 2F). The differences in performance between 1% (Figure\n2G) and 100% (Figure 2H) training data were to a large degree consistent across different cohorts.\nCorresponding results in terms of GS are provided in Extended Data Figure 1 and full per-cohort\nresults are presented in Extended Data Figure 2. In the remainder of the paper, we focus on the three\nmodels trained on 100% of the task-specific training data and additionally evaluate the feasibility of\nthe FM-based few-shot learning approach relying on 1% of the data.\nGleason grading by AI models is comparable to pathologists\nWe further analyzed the maximum performance of the models (using 100% training data) on a cohort-\nby-cohort basis. All models exhibited high concordance relative to each cohort’s original reference\nstandard on all tuning and internal validation datasets, but more pronounced variation could be\n6\n\n\n \nobserved across the external validation cohorts with QWK varying between 0.48-0.90 for GS and\n0.62-0.90 for ISUP grade (Figure 3A). The TS model exhibited a minor but relatively consistent\nadvantage over the FMs on the external data, outperforming both of them on 5/7 and 6/10 cohorts in\nterms of QWK for GS and ISUP, respectively. The sensitivity and specificity for cancer detection\nwere fairly consistent across all cohorts for all models, ranging between 87% and 100%, but UFM and\nVFM tend to have somewhat higher sensitivity than the TS model at the expense of lower specificity\n(Figure 3B).\nMeasuring AI generalization across patient populations, laboratories, and scanners is complicated by\ninter-observer  variability  between  site-specific  reference  standards  provided  by  different  local\npathologists and by variations in the granularity of the reporting (i.e. grading per slide, per a set of\nslides from the same anatomical region, or per patient). To directly measure the models’ capabilities\nto generalize across cohorts without the confounding effect of varying reference standards, we\nestablished a uniform reference standard based on a per-slide re-assessment of a randomly selected set\nof slides (stratified by ISUP grade according to the original grading) from each cohort by the lead\nstudy pathologist (L.E.). Measured against this uniform reference, all models exhibited QWK > 0.75\nin ISUP grading consistently across all external cohorts with the exception of the FMs on the\nSPROB20 cohort (ISUP QWK 0.580 and 0.680 for UFM and VFM, respectively) (Figure 3C).\nOn the cohorts where the original grading was conducted per slide, a direct comparison between the\noriginal and uniform reference standards was possible. The concordance between the models and the\nuniform reference standard (mean QWK across all cohorts for ISUP) was higher at 0.883 (TS), 0.850\n(UFM), and 0.870 (VFM) than the concordance between the original grading by local pathologists\nand  the  uniform  reference  standard  at  0.801.  The  lead  study  pathologist  is  an  experienced\nuropathology specialist and has been shown to be highly concordant with other specialists in earlier\nstudies6,26,44, but relying on a single reader as the sole reference is still problematic in view of the\nsubjective nature of Gleason grading. To place AI grading variability in the context of inter-observer\nvariability  between  pathologists, we  evaluated pairwise grading  concordance on  subsets of the\nSTHLM3 internal validation set (ImageBase45) and the RUMC external validation set26, as well as on\nthe full UKK and WNS external validation sets46, all graded independently by panels of pathologists\nand the AI models (Figure 4). The mean pairwise ISUP QWK between pathologists ranged from 0.67\nto 0.93 in the four cohorts. The corresponding values achieved by the three models were comparable\nto the pathologists, with the TS model showing a minimal advantage over the FMs in all four cohorts.\nWe additionally analyzed model performance in the following subgroups: malignant samples only\n(Extended Data Figure 3), across patient age groups (Extended Data Figure 4A), and among\npatients treated for benign prostatic hyperplasia with 5-ɑ-reductase inhibitors or alpha-blockers\n7\n\n\n \n(which has been hypothesized to influence prostate morphology47,48) (Extended Data Figure 4B).\nResults in terms of Cohen’s linearly weighted kappa (LWK) for GS and ISUP are shown in Extended\nData Table 1 (original reference standard) and Extended Data Table 2 (uniform reference standard).\nResults in terms of Area Under the Receiver Operating Characteristic Curve (AUROC) are further\nshown in Extended Data Figure 5.\nDiagnosing difficult and rare cases requires task-specific training\nAfter confirming that the AI models achieve overall pathologist-level performance across diverse\npatient cohorts, we focused on difficult and rare cases. The most severe consequences of misgrading\nare associated with high-grade cancers, leading to under- or overtreatment of patients, whereas errors\nbetween benign and low-grade cancers have less clinical significance due to the indolence of ISUP\ngrade 1 prostate cancer49. We adopted the following pre-specified definition for clinically significant\nerrors: a cancer of at least ISUP grade 2 predicted as benign or a benign sample predicted as a cancer\nof at least ISUP grade 2. Following this definition, we quantified significant errors committed by the\nmodels across all validation cohorts where the original reference standard was provided per slide\n(n=11,406 WSIs). When trained with 100% of training data, all models arrived at a similar rate of\nerrors (110 WSIs, 0.96% for UFM and VFM, 111 WSIs, 0.97% for TS). Of the slides with errors, 49\nwere common to all models. Importantly, extensive task-specific training considerably decreased the\nnumber of clinically significant errors committed by the FMs trained with 1% vs. 100% of the data:\nfrom 139 to 110 WSIs (-20.9%) for UFM, and from 136 to 110 (-19.1%) for VFM. For the TS model,\nthis result (from 366 to 111 WSIs, -69.7% decrease) was naturally expected.\nTo further evaluate the nature of the significant errors, the lead study pathologist re-graded all the\nslides (n=111) with significant errors committed by one of the models (TS), blinded to the original\nreference standard and the AI grading. Compared to this re-assessment, errors in 63/111 slides (57%)\nwere resolved, meaning they could be attributed to database issues (e.g. mistyped information in the\nreference standard, mixed-up slide identifiers, and WSI scanning issues in cases where the original\ngrading was done through microscope, etc.). Out of the 49 slides common to the three models, 42 fell\ninto  this  category.  Assuming  the  remaining  48  slides  represent  true  AI  errors,  the  TS  model\ncommitted clinically significant errors in 0.42% of the validation slides (see Extended Data Figure 6\nfor GS and ISUP QWK relative to the refined reference standard). These slides were characterized by\nthe presence of minimal lesions, crush artifacts, overstained sections, out of focus regions, and\nunusual morphologies. Furthermore, many of these slides would require immunohistochemistry (IHC)\nin clinical routine to confirm primary prostatic adenocarcinoma.\n8\n\n\n \nWe additionally evaluated the models on biopsies from three validation cohorts (AQ, KUH-2,\nSTHLM3)  representing  challenging  morphologies  (n=304  WSIs), such  as  benign  mimickers of\nprostate cancer and rare prostate cancer subtypes, which are difficult to diagnose but will inevitably\nbe encountered by AI models in clinical use50 (Figure 5A). Model performance was quantified in\nterms of sensitivity (rare malignant morphologies) and specificity (benign mimickers), as Gleason\ngrading is not applicable to the majority of these morphologies. All models exhibited high sensitivity\n(92-100%)  for  detecting  cancers  with  rare  morphologies  (Figure  5B)  but  struggled  with  a\nconsiderable number of false positives when assessing unusual benign tissue (Figure 5C). The\nresulting specificity varied dramatically depending on the model and the amount of task-specific\ntraining. The specificity of the TS model pooled across the cohorts was 0.690. For UFM and VFM,\nthe pooled specificity improved from 0.261 to 0.584 and from 0.632 to 0.710, respectively, when\nincreasing the fraction of training data used from 1% to 100%.\nCross-scanner reproducibility of AI Gleason grading varies with task-\nspecific training\nGiven that our tuning and validation cohorts include data collected with 14 different individual\nscanner instruments, representing 9 models from 5 vendors (Figure 6A), the overall results provide an\nindirect  evaluation  of  AI  generalization  across  scanners.  However,  differences  in  patient\ndemographics  and  sample  preparation  contribute  to  the  overall  variation.  To  directly  measure\nreproducibility across scanners, we quantified the models’ cross-scanner concordance in terms of\nQWK for ISUP grading between subsets of slides re-scanned with multiple scanner models in the\nMUL (n=481 slides, 2 scanners) and STHLM3 (n=48 slides, 5 scanners) cohorts (Figure 6B-C).\nIn line with the other experiments, UNI trained with 1% of training data exhibited very low cross-\nscanner concordance but improved considerably with the full training dataset (mean pairwise QWK\nbetween scanners improved from 0.622 to 0.937 for STHLM3 and from 0.577 to 0.908 for MUL). In\ncontrast, VFM exhibited high cross-scanner concordance overall but the effect of additional training\ndata was cohort-dependent (mean pairwise QWK between scanners degraded from 0.952 to 0.904 for\nSTHLM3 but improved from 0.870 to 0.912 for MUL). The TS model (trained fully with 100% task-\nspecific data) exhibited mean cross-scanner pairwise QWK of 0.963 (STHLM3) and 0.918 (MUL)\nand outperformed both FMs on 9 of the 11 scanner pairs. Corresponding results in terms of cross-\nscanner QWK for GS are presented in Extended Data Figure 7.\n9\n\n\n \nFoundation models consume up to 35x more energy than a task-\nspecific model\nFoundation  models  typically  rely  on  more  complex  neural  network  architectures  (e.g.  Vision\nTransformers  (ViT)51)  than  task-specific  models:  UFM  (ViT-L/16)  and  VFM  (ViT-H/14)  have\napproximately 300 million and 632 million parameters, respectively, compared to the approximately\n22 million parameters of TS (EfficientNet-V2S architecture)52. The size of a model influences its\nenergy  consumption  and  runtime,  which  has  important  implications  for  the  financial  and\nenvironmental sustainability of clinically deployed AI solutions.\nTo compare the relative efficiency of the three models in this study, we recorded the total amount of\npower consumed by the GPU to produce the Gleason grading predictions for our tuning set (n=801\nslides). The models consumed 0.51 kWh (TS), 5.40 kWh (UFM) and 17.70 kWh (VFM), with\nruntimes of 2.50 GPUh, 15.25 GPUh, and 45.57 GPUh, respectively. The models spent 0.63 Wh (TS),\n6.74 Wh (UFM), and 22.09 Wh (VFM) per prostate biopsy.\nDiscussion\nPrevious studies on FMs have demonstrated their ability to operate effectively across a variety of\ntasks by relying on few-shot learning with limited training data. This is particularly advantageous in\nthe medical domain, where labeled data are often scarce. At the same time, medical applications\ninvolving high-stakes decisions pose stringent requirements for AI accuracy53. Regulatory frameworks\nmay need to be adapted in view of generalist AI models54, but it will remain essential that all AI\nmodels be thoroughly evaluated with clinically relevant performance metrics before being deployed\nfor a specific task. Here, we present the most comprehensive validation study to date on AI-based\nassessment of biopsies for diagnosis and Gleason grading of prostate cancer, relying on both FMs and\nstate-of-the-art end-to-end learning. Our first key result is that to reach AI performance optimal for\nclinical deployment, even FMs require substantial amounts of task-specific training data. Moreover,\nwith sufficient training data available, FMs do not possess any intrinsic generalization advantage over\nan end-to-end trained task-specific model. On the contrary, while all models exhibited nearly identical\nperformance in internal validation, the overall performance of the FMs was slightly lower than the TS\nmodel  on  fully  external  validation  cohorts.  While  the  value  proposition  of  FMs  is  clear  for\napplications  where  data  are  scarce  (e.g.  rare  diseases),  these  results  suggest  against  assuming\nuniversal  superiority  of  FMs  over  end-to-end  models  and  advocate  for  caution  with  clinical\ndeployment of FMs relying on limited task-specific training, which may risk committing clinically\ncritical errors.\n10\n\n\n \nSensitivity of AI models to variability introduced by different WSI scanners is a well-known problem\nin computational pathology, greatly hampering the clinical applicability of AI55–59. Despite hopes of\nFMs possessing inherent tolerance for scanner variation and solving the generalization problem once\nand for all, limited evidence suggests that the composition of FM pretraining data does influence\ndownstream task performance on different tissue types60 and that FMs are not immune to batch effects\nand biases present during the self-supervised pre-training59,61–63. Our second key result, handling the\nmodels’  cross-scanner  concordance,  supports  these  earlier  findings,  demonstrating  that  UNI  in\nparticular suffered from poor diagnostic reproducibility when the same slides were imaged using\ndifferent scanners. Interestingly, the cross-scanner concordance of the model was greatly improved\nwith extensive task-specific training. This seems to suggest that the feature representation learned by\nthe FM captures scanner-specific data characteristics, and a considerable number of task-specific\nexamples are needed to adapt the auxiliary aggregation and classification layers to learn to correctly\ndisregard these biases. Virchow2 exhibited more consistent performance even with limited task-\nspecific training, but the highest cross-scanner concordance was still obtained by the end-to-end TS\nmodel.\nOur third key result handles the energy consumption of AI in pathology. One thus far largely\nneglected consequence of increasing model complexity is the spiraling energy consumption. While\nthe  absolute  power consumption per sample depends on  various factors such as  the type and\nefficiency of hardware (also including other components than the GPU) and the amount of tissue per\nslide, our experiments provide an estimate of the relative energy demands of analyzing prostate\nbiopsies using FMs in comparison to a task-specific model (approx. 11x for UFM, 35x for VFM).\nThis result is in line with the earlier reported directly proportional relationship between the number of\nmodel parameters and the resulting CO2  emissions64. Increased demands for computing are also\nassociated with growing use of freshwater resources for cooling data centers 65. Amidst the global\nclimate crisis, the increased environmental toll of FMs and other large models has to be weighed\nagainst the benefits they can provide in a given task compared to more streamlined, task-specific\nmodels that consume a fraction of the energy. Besides environmental sustainability, laboratories also\nneed to consider the operational costs of AI computing from a financial perspective. In the case of AI-\nbased prostate cancer diagnosis and grading, our results do not support the use of FMs in this task in\nplace of simpler, task-specific models, provided that extensive training data are available and the aim\nis clinical-level diagnostic performance.\nOur fourth key result is the confirmation of earlier findings6,7,26,27 indicating that AI can diagnose and\nGleason grade prostate cancer comparably to experienced pathologists, marking the most extensive\nvalidation of AI for this task to date. In this study, FM-based models did not demonstrate any clear\nadvantages over the end-to-end trained TS model. However, when trained with extensive task-specific\n11\n\n\n \ndata, all the models in this study reached performance comparable to each other, to pathologists, and\nto earlier studies. It seems likely that performance in Gleason grading has reached a plateau that\nprobably cannot be considerably improved on with further advances in AI. However, the picture may\nbe markedly different in other tasks. For example, for direct prognostication of patient outcomes or\ntreatment response from tissue morphology, novel (potentially subtle) patterns need to be discovered,\nwhich has led some authors to advocate for further research into end-to-end approaches30. Despite\nvarying definitions for “end-to-end” training, current state-of-the-art models for prognosticating\npatient outcomes66 or molecular biomarkers67 from WSIs do not represent true end-to-end learning,\nwhere all parameters of the model are optimized jointly during training. Abandoning this key\ningredient of the first wave of success in AI pathology precludes supervised data-driven discovery of\npatterns that are most informative for the task at hand.\nOur study has some limitations. Firstly, while the validation cohorts were fully external to the\nevaluated models and captured a broad spectrum of clinical sites, laboratories and scanners, they\nrepresent populations with primarily Caucasian ancestry. Second, all scanners included in the cross-\nscanner analysis except for the Grundium device were present in the models’ training data. Even more\ndrastic cross-scanner variation in AI performance is thus likely to be seen if a similar evaluation is\nrepeated on additional, unseen scanners. We plan to address these limitations in future validation\nstudies representing more diversity in terms of patient ethnicity and scanner models.\nDespite the concerns and shortcomings raised by us and others, the value of FMs is clear for\naccelerating research and development, and as a solution when labeled data are scarce. In addition,\nwhen moving from more fine-grained tasks (e.g. detection of nuclei) towards larger scales (e.g.\nprognostication of patient survival), the number of labeled data points (e.g. patients vs. pixels)\nshrinks, which complicates fully supervised end-to-end learning. Addressing complex multimodal\napplications may also be infeasible without FMs that are capable of integrating visual data with\ntextual10 or molecular information68. However, in its current form, the accelerating proliferation of\nFMs resembles the era of feature engineering, where machine learning practitioners solving a given\ntask were faced with a semi-empirical  ad hoc process of picking their favorites from a vast and\nexpanding pool of various morphological descriptors. Our results clearly demonstrate that for a given\ntask, not all FMs are created equal, and evidence is mounting on the existence of different biases\nencoded in their feature representations.\nWe believe future research will lead to optimized, data-driven ways of selecting, fusing, and pruning\nFMs60 to dissect the relevant components of their embedded representations for a given task, while at\nthe same time mitigating their growing environmental footprint. Another exciting opportunity in this\ndirection are AI agents capable of autonomously selecting and using the best tools for each task 69. For\n12\n\n\n \nclinical applicability, stringent validation studies will still be needed to ascertain that biases and batch\neffects arising e.g. from the variability between WSI scanners are controlled for. Despite their\nunquestionable potential to revolutionize medical AI, without comprehensive studies addressing these\npoints, FMs risk being prematurely positioned as a universal replacement for established approaches\nand discouraging the continued development of end-to-end learning for computational pathology.\nEthical considerations\nThis study included data gathered in one or more collection rounds at participating international sites\nbetween 2012 and 2024. All datasets were de-identified at their respective sites and subsequently\ntransferred to Karolinska Institutet in an anonymized format. This study complies with the Helsinki\nDeclaration.  The  patient  sample  collection  was  approved  by  the  Stockholm  Regional  Ethics\nCommittee (permits 2012/572-31/1, 2012/438-31/3, and 2018/845-32), the Swedish Ethical Review\nAuthority (permit 2019-05220), and the Regional Committee for Medical and Health Research Ethics\nin Western Norway (permits REC/Vest 80924, REK 2017/71). Informed consent was obtained from\npatients in the Swedish dataset and was waived for other data cohorts due to the use of de-identified\nprostate specimens in a retrospective setting. Patient involvement in this study was supported by the\nSwedish Prostate Cancer Society.\nAcknowledgments\nA.B. received a grant from the Health Faculty at the University of Stavanger, Norway. B.G.P and\nK.D.S received funding from Innovation Fund Denmark and Nordforsk (Grant no. 8114-00014B) for\nthe Danish branch of the NordCaP project. K.H. received funding from Business Finland (BF) for the\nFinnish branch of the NordCaP project. P.R. received funding from the Research Council of Finland\n(Grant no. 341967) and the Cancer Foundation Finland. M.E. received funding from the Swedish\nResearch Council, Swedish Cancer Society, Swedish Prostate Cancer Society, Nordic Cancer Union,\nKarolinska  Institutet,  and  Region  Stockholm.  K.K.  received  funding  from  the  SciLifeLab  &\nWallenberg  Data  Driven  Life  Science  Program  (KAW  2024.0159),  David  and  Astrid  Hägelen\nFoundation, Instrumentarium Science Foundation, KAUTE Foundation, Karolinska Institute Research\nFoundation, Orion Research Foundation and Oskar Huttunen Foundation. We want to thank Carin\nCavalli-Björkman, Astrid Björklund and Britt-Marie Hune for assistance with scanning and database\nsupport. We would also like to thank Simone Weiss for assistance with scanning in Aarhus, and Silja\nKavlie Fykse and Desmond Mfua Abono for scanning in Stavanger. We would like to acknowledge\nthe patients who participated in the STHLM3 diagnostic study and the OncoWatch and NordCaP\nprojects and contributed the clinical information that made this study possible. High-performance\ncomputing was supported by the National Academic Infrastructure for Supercomputing in Sweden\n(NAISS) and the Swedish National Infrastructure for Computing (SNIC) at C3SE partially funded by\n13\n\n\n \nthe Swedish Research Council through grant agreement no. 2022-06725 and no. 2018-05973, by the\nsupercomputing resource Berzelius provided by the National Supercomputer Centre at Linköping\nUniversity and the Knut and Alice Wallenberg Foundation, and by CSC - IT Center for Science,\nFinland.\nCompeting interests\nN.M., L.E., K.K. and M.E. are shareholders of Clinsight AB.\nReferences\n1.\nvan der Laak, J., Litjens, G. & Ciompi, F. Deep learning in histopathology: the path to the clinic. \nNat. Med. 27, 775–784 (2021).\n2.\nEhteshami Bejnordi, B. et al. Diagnostic Assessment of Deep Learning Algorithms for Detection\nof Lymph Node Metastases in Women With Breast Cancer. JAMA 318, 2199–2210 (2017).\n3.\nLitjens, G. et al. A survey on deep learning in medical image analysis. Med. Image Anal. 42, 60–\n88 (2017).\n4.\nLitjens, G. et al. Deep learning as a tool for increased accuracy and efficiency of \nhistopathological diagnosis. Sci. Rep. 6, 26286 (2016).\n5.\nJanowczyk, A. & Madabhushi, A. Deep learning for digital pathology image analysis: A \ncomprehensive tutorial with selected use cases. J. Pathol. Inform. 7, 29 (2016).\n6.\nStröm, P. et al. Artificial intelligence for diagnosis and grading of prostate cancer in biopsies: a \npopulation-based, diagnostic study. Lancet Oncol. 21, 222–232 (2020).\n7.\nBulten, W. et al. Automated deep-learning system for Gleason grading of prostate cancer using \nbiopsies: a diagnostic study. Lancet Oncol. 21, 233–241 (2020).\n8.\nVorontsov, E. et al. A foundation model for clinical-grade computational pathology and rare \ncancers detection. Nat. Med. 30, 2924–2935 (2024).\n9.\nChen, R. J. et al. Towards a general-purpose foundation model for computational pathology. Nat.\nMed. 30, 850–862 (2024).\n10. Lu, M. Y. et al. A visual-language foundation model for computational pathology. Nat. Med. 30, \n14\n\n\n \n863–874 (2024).\n11. Xu, H. et al. A whole-slide foundation model for digital pathology from real-world data. Nature \n(2024) doi:10.1038/s41586-024-07441-w.\n12. Wang, X. et al. A pathology foundation model for cancer diagnosis and prognosis prediction. \nNature 634, 970–978 (2024).\n13. Kondepudi, A. et al. Foundation models for fast, label-free detection of glioma infiltration. \nNature (2024) doi:10.1038/s41586-024-08169-3.\n14. Zhang, K. et al. A generalist vision-language foundation model for diverse biomedical tasks. \nNat. Med. 30, 3129–3141 (2024).\n15. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, \n259–265 (2023).\n16. Russakovsky, O. et al. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. \n115, 211–252 (2015).\n17. Wei, J. et al. Emergent abilities of large language models. arXiv [cs.CL] (2022).\n18. Tu, T. et al. Towards generalist biomedical AI. NEJM AI 1, (2024).\n19. Schömig-Markiefka, B. et al. Quality control stress test for deep learning-based diagnostic model\nin digital pathology. Mod. Pathol. 34, 2098–2108 (2021).\n20. Ilse, M., Tomczak, J. & Welling, M. Attention-based Deep Multiple Instance Learning. in \nProceedings of the 35th International Conference on Machine Learning (eds. Dy, J. & Krause, \nA.) vol. 80 2127–2136 (PMLR, 10--15 Jul 2018).\n21. Pinckaers, H., van Ginneken, B. & Litjens, G. Streaming convolutional neural networks for end-\nto-end learning with multi-megapixel images. IEEE Trans. Pattern Anal. Mach. Intell. 44, 1581–\n1590 (2022).\n22. Campanella, G., Fluder, E., Zeng, J., Vanderbilt, C. & Fuchs, T. J. Beyond multiple instance \nlearning: Full resolution all-in-memory end-to-end pathology slide modeling. arXiv [eess.IV] \n(2024).\n23. Loeb, S. et al. Systematic review of complications of prostate biopsy. Eur. Urol. 64, 876–892 \n(2013).\n15\n\n\n \n24. Loeb, S., Carter, H. B., Berndt, S. I., Ricker, W. & Schaeffer, E. M. Complications after prostate \nbiopsy: data from SEER-Medicare. J. Urol. 186, 1830–1834 (2011).\n25. Egevad, L. et al. Standardization of Gleason grading among 337 European pathologists. \nHistopathology 62, 247–256 (2013).\n26. Bulten, W. et al. Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the \nPANDA challenge. Nat. Med. 28, 154–163 (2022).\n27. Nagpal, K. et al. Development and validation of a deep learning algorithm for Gleason grading \nof prostate cancer from biopsy specimens. JAMA Oncol. 6, 1372–1380 (2020).\n28. Tabesh, A. et al. Multifeature prostate cancer diagnosis and Gleason grading of histological \nimages. IEEE Trans. Med. Imaging 26, 1366–1378 (2007).\n29. Wulczyn, E. et al. Predicting prostate cancer specific-mortality with artificial intelligence-based \nGleason grading. Commun. Med. 1, 10 (2021).\n30. Esteva, A. et al. Prostate cancer therapy personalization via multi-modal deep learning on \nrandomized phase III clinical trials. NPJ Digit. Med. 5, 71 (2022).\n31. Eloy, C. et al. Artificial intelligence-assisted cancer diagnosis improves the efficiency of \npathologists in prostatic biopsies. Virchows Arch. 482, 595–604 (2023).\n32. Pantanowitz, L. et al. An artificial intelligence algorithm for prostate cancer diagnosis in whole \nslide images of core needle biopsies: a blinded clinical validation and deployment study. Lancet \nDigit. Health 2, e407–e416 (2020).\n33. Santa-Rosario, J. C., Gustafson, E. A., Sanabria Bellassai, D. E., Gustafson, P. E. & de Socarraz, \nM. Validation and three years of clinical experience in using an artificial intelligence algorithm \nas a second read system for prostate cancer diagnosis-real-world experience. J. Pathol. Inform. \n15, 100378 (2024).\n34. Vazzano, J. et al. Evaluation of A computer-aided detection software for prostate cancer \nprediction: Excellent diagnostic accuracy independent of preanalytical factors. Lab. Invest. 103, \n100257 (2023).\n35. Sandeman, K. et al. AI model for prostate biopsies predicts cancer survival. Diagnostics (Basel) \n12, 1031 (2022).\n16\n\n\n \n36. Tolkach, Y., Dohmgörgen, T., Toma, M. & Kristiansen, G. High-accuracy prostate cancer \npathology using deep learning. Nat. Mach. Intell. 2, 411–418 (2020).\n37. Nagendran, M. et al. Artificial intelligence versus clinicians: systematic review of design, \nreporting standards, and claims of deep learning studies. BMJ 368, m689 (2020).\n38. McGenity, C., Bossuyt, P. & Treanor, D. Reporting of Artificial Intelligence Diagnostic \nAccuracy Studies in Pathology Abstracts: Compliance with STARD for Abstracts Guidelines. J. \nPathol. Inform. 13, 100091 (2022).\n39. Mulliqi, N. et al. Study Protocol: Development and Retrospective Validation of an Artificial \nIntelligence System for Diagnostic Assessment of Prostate Biopsies. medRxiv (2024).\n40. Grönberg, H. et al. Prostate cancer screening in men aged 50–69 years (STHLM3): a prospective\npopulation-based diagnostic study. Lancet Oncol. 16, 1667–1676 (2015).\n41. Zimmermann, E. et al. Virchow2: Scaling self-supervised mixed magnification models in \npathology. arXiv [cs.CV] (2024).\n42. Tan, M. & Le, Q. V. EfficientNet: Rethinking model scaling for convolutional Neural Networks. \narXiv [cs.LG] (2019).\n43. Epstein, J. I. et al. The 2014 international society of urological pathology (ISUP) consensus \nconference on Gleason grading of prostatic carcinoma: Definition of grading patterns and \nproposal for a new grading system. Am. J. Surg. Pathol. 40, 244–252 (2016).\n44. Egevad, L. et al. Utility of Pathology Imagebase for standardisation of prostate cancer grading. \nHistopathology 73, 8–18 (2018).\n45. Egevad, L. et al. Pathology Imagebase-a reference image database for standardization of \npathology. Histopathology vol. 71 677–685 Preprint at https://doi.org/10.1111/his.13313 (2017).\n46. Tolkach, Y. et al. An international multi-institutional validation study of the algorithm for \nprostate cancer detection and Gleason grading. NPJ Precis Oncol 7, 77 (2023).\n47. Lucia, M. S. et al. Finasteride and high-grade prostate cancer in the Prostate Cancer Prevention \nTrial. J. Natl. Cancer Inst. 99, 1375–1383 (2007).\n48. Evans, A. J. Treatment effects in prostate cancer. Mod. Pathol. 31, S110–121 (2018).\n49. Eggener, S. E. et al. Low-grade prostate cancer: Time to stop calling it cancer. J. Clin. Oncol. 40,\n17\n\n\n \n3110–3114 (2022).\n50. Olsson, H. et al. Estimating diagnostic uncertainty in artificial intelligence assisted pathology \nusing conformal prediction. Nat. Commun. 13, 7761 (2022).\n51. Dosovitskiy, A. An image is worth 16×16 words: transformers for image recognition at scale. in \nInternational Conference on Learning Representations (2021).\n52. Tan, M. & Le, Q. EfficientNetV2: Smaller Models and Faster Training. in Proceedings of the \n38th International Conference on Machine Learning (eds. Meila, M. & Zhang, T.) vol. 139 \n10096–10106 (PMLR, 18--24 Jul 2021).\n53. Longoni, C., Bonezzi, A. & Morewedge, C. K. Resistance to medical Artificial intelligence. J. \nConsum. Res. 46, 629–650 (2019).\n54. Gilbert, S. & Kather, J. N. Guardrails for the use of generalist AI in cancer care. Nat. Rev. \nCancer 24, 357–358 (2024).\n55. Chen, R. J. et al. Algorithmic fairness in artificial intelligence for medicine and healthcare. Nat. \nBiomed. Eng. 7, 719–742 (2023).\n56. Swiderska-Chadaj, Z. et al. Impact of rescanning and normalization on convolutional neural \nnetwork performance in multi-center, whole-slide classification of prostate cancer. Sci. Rep. 10, \n14398 (2020).\n57. Howard, F. M. et al. The impact of site-specific digital histology signatures on deep learning \nmodel accuracy and bias. Nat. Commun. 12, 4423 (2021).\n58. Dehkharghanian, T. et al. Biased data, biased AI: deep networks predict the acquisition site of \nTCGA images. Diagn. Pathol. 18, 67 (2023).\n59. Ji, X. et al. Physical color calibration of digital pathology scanners for robust artificial \nintelligence-assisted cancer diagnosis. Mod. Pathol. 38, 100715 (2025).\n60. Neidlinger, P. et al. Benchmarking foundation models as feature extractors for weakly-\nsupervised computational pathology. arXiv [eess.IV] (2024).\n61. Kömen, J., Marienwald, H., Dippel, J. & Hense, J. Do histopathological foundation models \neliminate batch effects? A comparative study. arXiv [cs.LG] (2024).\n62. Yun, J., Hu, Y., Kim, J., Jang, J. & Lee, S. EXAONEPath 1.0 patch-level foundation model for \n18\n\n\n \npathology. arXiv [cs.LG] (2024).\n63. de Jong, E. D., Marcus, E. & Teuwen, J. Current Pathology Foundation Models are unrobust to \nMedical Center Differences. arXiv [cs.LG] (2025).\n64. Vafaei Sadr, A. et al. Operational greenhouse-gas emissions of deep learning in digital \npathology: a modelling study. Lancet Digit. Health 6, e58–e69 (2024).\n65. Li, P., Yang, J., Islam, M. A. & Ren, S. Making AI less ‘thirsty’: Uncovering and addressing the \nsecret water footprint of AI models. arXiv [cs.LG] (2023).\n66. Jiang, X. et al. End-to-end prognostication in colorectal cancer by deep learning: a retrospective, \nmulticentre study. Lancet Digit. Health 6, e33–e43 (2024).\n67. El Nahhas, O. S. M. et al. From whole-slide image to biomarker prediction: end-to-end weakly \nsupervised deep learning in computational pathology. Nat. Protoc. 20, 293–316 (2025).\n68. Vaidya, A. et al. Molecular-driven foundation model for oncologic pathology. arXiv [cs.CV] \n(2025).\n69. Szolnoky, K., Nordström, T. & Eklund, M. Tomorrow’s patient management: LLMs empowered \nby external tools. Nat. Rev. Urol. 1–4 (2024).\n70. Mongan, J., Moy, L. & Kahn, C. E., Jr. Checklist for Artificial Intelligence in Medical Imaging \n(CLAIM): A Guide for Authors and Reviewers. Radiol Artif Intell 2, e200029 (2020).\n71. Kleppe, A. et al. Designing deep learning studies in cancer diagnostics. Nat. Rev. Cancer 21, \n199–211 (2021).\n72. Marée, R. et al. Collaborative analysis of multi-gigapixel imaging data using Cytomine. \nBioinformatics 32, 1395–1401 (2016).\n73. Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N. & Liang, J. UNet++: A Nested U-Net Architecture\nfor Medical Image Segmentation. arXiv [cs.CV] (2018).\n74. Xie, S., Girshick, R., Dollár, P., Tu, Z. & He, K. Aggregated residual transformations for deep \nneural networks. arXiv [cs.CV] (2016).\n75. Abadi, M. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed \nSystems. arXiv [cs.DC] (2016).\n76. Oquab, M. et al. DINOv2: Learning robust visual features without supervision. arXiv [cs.CV] \n19\n\n\n \n(2023).\n77. Glorot, X. & Bengio, Y. Understanding the difficulty of training deep feedforward neural \nnetworks. AISTATS 9, 249–256 (13--15 May 2010).\n78. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. arXiv [cs.LG] (2017).\n79. Li, S. et al. PyTorch distributed. Proceedings VLDB Endowment 13, 3005–3018 (2020).\n80. Micikevicius, P. et al. Mixed Precision Training. arXiv [cs.AI] (2017).\n81. Egevad, L. et al. Prognosis of Gleason score 8 prostatic adenocarcinoma in needle biopsies: a \nnationwide population-based study. Virchows Arch. (2024) doi:10.1007/s00428-024-03810-y.\n82. Egevad, L. et al. Prognosis of Gleason Score 9–10 Prostatic Adenocarcinoma in Needle \nBiopsies: A Nationwide Population-based Study. European Urology Oncology 7, 213–221 \n(2024).\n83. Jung, M. et al. Artificial intelligence system shows performance at the level of uropathologists \nfor the detection and grading of prostate cancer in core needle biopsy: an independent external \nvalidation study. Mod. Pathol. 35, 1449–1457 (2022).\n84. Otálora, S. et al. stainlib: a python library for augmentation and normalization of histopathology \nH&E images. bioRxiv (2022) doi:10.1101/2022.05.17.492245.\n85. Cohen, J. A coefficient of agreement for nominal scales. Educ. Psychol. Meas. 20, 37–46 (1960).\n86. Artstein, R. & Poesio, M. Inter-coder agreement for computational linguistics. Comput. Linguist.\nAssoc. Comput. Linguist. 34, 555–596 (2008).\n87. Lapuschkin, S. et al. Unmasking Clever Hans predictors and assessing what machines really \nlearn. Nat. Commun. 10, 1096 (2019).\n88. Schmitt, M. et al. Hidden Variables in Deep Learning Digital Pathology and Their Potential to \nCause Batch Effects: Prediction Model Study. J. Med. Internet Res. 23, e23436 (2021).\n20\n\n\n \nFigures and Tables\n21\n\n\n \nFigure 1: Overview of patient cohorts. (a) In total, 7,243 patients who underwent prostate biopsy\nwere included in the study, resulting in approximately 100,000 biopsy cores from 15 clinical sites in\n11 countries. The dataset contains 58,744 glass slides, digitized into 82,584 WSIs and tiled into\napproximately 60 million tissue patches (256 x 256 pixel images). The number of WSIs differs from\nthe number of glass slides due to repeated scanning of subsets of the slides on multiple scanners. The\npatients were divided into development (training and tuning) and validation (internal and external)\npartitions, such that there was no overlap between the patients, clinical sites or scanners included in\nthe external validation and the development partitions. See Supplementary Appendix 1 for detailed\npatient  characteristics,  data  collection  processes,  reference  standard  protocols  and  CONSORT\ndiagrams for each cohort. (b) The ISUP grade and Gleason score distributions of the glass slides\nincluded in each partition. For the SPROB20, UKK and WNS cohorts, only ISUP grades were\navailable. (c) Geographical distribution of the included clinical sites and the numbers of patients, glass\nslides  and  WSIs  per  cohort.  *The  approximate  total  number  of  biopsy  cores  was  estimated\nconsidering the total number of glass slides and the typical number of cores per glass slide for each\ncohort. ISUP=International Society of Urological Pathology, WSI = Whole slide image.1\n1  Blilie, A. (2025) https://BioRender.com/e66o459\n22\n\n\n \nFigure  2: Model  design  and overall performance  as  a  function  of  increased  task-specific\ntraining. (a) A schematic of the model design featuring attention-based aggregation of patch-level\nfeature embeddings, followed by a multitask classification layer for predicting the primary and\nsecondary Gleason patterns of the entire WSI. The model is trainable with either foundation model\nbased patch encoders or in a full end-to-end fashion with an EfficientNet encoder. (b) Heatmap\nshowing predictions per patch for an example WSI demonstrates that weakly supervised training with\nWSI-level Gleason score labels has still allowed the model to learn to recognize individual Gleason\npatterns  on  the  patch  level. (c) A  pathologist’s  annotations  of  Gleason  patterns  (conducted\nindependently and blinded to the AI models). (d) Zoomed-in views of the annotated cancer foci (C1-\n23\n\n\n \nC5). (e-f) Overall ISUP grading concordance between the models (TS, UFM, VFM) and the original\nreference standard measured in terms of QWK as a function of increasing fractions of downstream\ntask-specific training data used, evaluated on the pooled internal (e) and external (f) validation cohorts\n(nw indicates the number of validation WSIs). (g-h) Per-cohort ISUP grading QWK for the models\ntrained with 1% of the training data (n=524 WSIs) (g) and 100% of the training data (n=55,789 WSIs)\n(h).  ABMIL=Attention-based  multiple  instance  learning,  FM=Foundation  model,  GP=Gleason\npattern, ISUP=International Society of Urological Pathology, QWK=Quadratically weighted Cohen’s\nkappa, TS=Task-specific model, UFM=UNI foundation model, VFM=Virchow2 foundation model,\nWSI=Whole slide image.2\n2  Mulliqi, N. (2025) https://BioRender.com/j00e590\n24\n\n\n \nFigure 3: Gleason grading performance of AI models across international validation cohorts. (a)\nGleason grading concordance between the models (TS, UFM, VFM; all trained with 100% of the\ntask-specific data) and the original reference standards defined by cohort-specific local pathologists,\n25\n\n\n \nmeasured in terms of QWK for Gleason score (left; reference Gleason scores not available for\nSPROB20, UKK, WNS) and for ISUP grade (right).  (b) Cancer detection performance of the models\nmeasured by sensitivity and specificity (omitted for the UKK and WNS cohorts, which only include\nmalignant cases) relative to the original reference standards. (c) The Gleason grading concordance of\nthe models and the local pathologists relative to a uniform slide-level reference standard by the lead\nstudy pathologist, measured in terms of QWK for Gleason score (top) and ISUP grade (bottom). For\nthe MLP, SCH and SPROB20 cohorts, the original reference standard was not reported on slide-level\nand the comparison between the local pathologists and the lead study pathologist is omitted. The\ngranularity of the reporting of the original reference standards varied by cohort. The  ns,  nl and  np\nindicate the number of glass slides, anatomical locations or patients, respectively, included in each\nanalysis. The values indicated by the plots for QWK, sensitivity and specificity represent point\nestimates on the full cohorts, while the whiskers and error bars represent 95% confidence intervals\nestimated  by  bootstrapping.  ISUP=International  Society  of  Urological  Pathology,\nQWK=Quadratically  weighted  Cohen’s  kappa,  TS=Task-specific  model,  UFM=UNI  foundation\nmodel, VFM=Virchow2 foundation model, WSI=Whole slide image.3\n3  Blilie, A. (2025) https://BioRender.com/w12k266\n26\n\n\n \nFigure 4: Inter-observer variability among panels of pathologists and AI models. (a-d)  Mean\npairwise concordance between the models (TS, UFM, VFM; all trained with 100% of the task-specific\ndata) and pathologists, compared to mean pairwise concordance between pathologists, measured in\nterms of QWK for Gleason score (only available for the RUMC cohort) and for ISUP grade, and\nevaluated in (a) ImageBase (part of STHLM3 internal validation cohort) and in the (b) RUMC, (c)\nUKK, and  (d) WNS external validation cohorts. The panelists (pathologists and AI models) are\nranked according to their mean pairwise QWK, but the AI models were not included in the calculation\nof the pathologists’ or other models’ mean QWK to avoid biasing the panel’s consensus towards the\nmodels (i.e. for each pathologist or model, the mean QWK against all other pathologists is shown).\nThe number of glass slides included in each analysis is indicated by  ns. The dots indicate point\nestimates  and  the  error  bars  represent  95%  confidence  intervals  estimated  by  bootstrapping.\nISUP=International Society of Urological Pathology, QWK=Quadratically weighted Cohen’s kappa,\nTS=Task-specific model, UFM=UNI foundation model, VFM=Virchow2 foundation model.4\n4  Mulliqi, N. (2025) https://BioRender.com/v76j626\n27\n\n\n \nFigure 5: Diagnostic performance of AI models on challenging morphologies. (a) The distribution\nof challenging morphological subtypes, including benign (e.g., atrophy, adenosis, Cowper’s glands)\nand malignant (e.g., pseudohyperplastic cancer, PIN-like cancer) types, in the AQ, KUH-2 and\nSTHLM3 cohorts.  (b) Diagnostic sensitivity and  (c) specificity for the models (TS, UFM, VFM;\ntrained with 1% or 100% of the task-specific data), relative to the reference standard by the lead study\npathologist. Sensitivity is omitted for the AQ cohort, which only contains benign mimickers of\ncancer. Subsets of the AQ cohort were scanned on different scanners (Grundium, fully external;\nPhilips, included in development cohorts). The  ns and  np indicate the number of glass slides and\npatients, respectively, included in each analysis. The bars indicate point estimates and the error bars\nrepresent 95% confidence intervals estimated by bootstrapping. TS=Task-specific model, UFM=UNI\nfoundation model, VFM=Virchow2 foundation model.5\n5  Blilie, A. (2025) https://BioRender.com/m31o854\n28\n\n\n \nFigure 6: Cross-scanner performance of AI models. (a) The distributions of individual slide\nscanners used to digitize the study cohorts. The scanners used for digitizing external cohorts for the\n29\n\n\n \nprimary validation (see  Figures 2-4) were not involved in the development, tuning or internal\nvalidation cohorts. The Philips Intellisite UFS #1 scanner, used to collect training and internal\nvalidation data, was also used to re-scan a subset of the MUL external validation cohort, but these\nWSIs were used only for the cross-scanner analysis presented here. This scanner was also used to\ndigitize a subset of the AQ cohort for the analysis on challenging morphologies (see Figure 5). (b)\nThe cross-scanner concordance of each model (TS, UFM, VFM; trained with 1% or 100% of the task-\nspecific data) measured in terms of QWK for ISUP grade, comparing each model’s grading for the\nsame slides digitized on pairs of different scanners (incl. 5 scanners for the STHLM3 cohort and 2\nscanners for the MUL cohort). In addition, the mean of the 10 pairwise comparisons on the STHLM3\ncohort is indicated (see legend). (c) Heatmaps showing the ISUP grade predictions by each model\nacross glass slides (columns) digitized on different scanners (rows). For each model, the slides are\nsorted for visual readability based on the model’s average ISUP prediction across scanners i.e. the\ncolumn ordering is different between models. The number of glass slides included from the internal\n(STHLM3) and external  (MUL) validation cohorts is  indicated  by  ns. The bars  indicate  point\nestimates  and  the  error  bars  represent  95%  confidence  intervals  estimated  by  bootstrapping.\nISUP=International Society of Urological Pathology, QWK=Quadratically weighted Cohen’s kappa,\nTS=Task-specific model, UFM=UNI foundation model, VFM=Virchow2 foundation model.\n30\n\n\n \nExtended Data Figures\nExtended Data Figure 1: Overall Gleason scoring performance of AI models as a function of\nincreased task-specific training. (a-b) Overall Gleason score concordance between the models (TS,\nUFM, VFM) and the original reference standard measured in terms of QWK as a function of\nincreasing fractions of downstream task-specific training data used, evaluated on the pooled internal\n31\n\n\n \n(a) and external (b) validation cohorts. (c-d) Per-cohort Gleason scoring QWK for the models trained\nwith 1% of the training data (n=524 WSIs) (c) and 100% of the training data (n=55,789 WSIs) (d). (e)\nNumber  of  slides  and  WSIs  corresponding  to  each  training  data  fraction.  GS=Gleason  score,\nQWK=Quadratically  weighted  Cohen’s  kappa,  TS=Task-specific  model,  UFM=UNI  foundation\nmodel, VFM=Virchow2 foundation model, WSI=Whole slide image.\n32\n\n\n \nExtended Data Figure 2: Gleason grading performance of AI models across validation cohorts\nas a function of increased task-specific training. (a-b) Per-cohort concordance between the models\n(TS, UFM, VFM) and the original reference standard measured in terms of QWK as a function of\nincreasing fractions of downstream task-specific training data for Gleason score (a) and ISUP grade\n(b). ISUP=International Society of Urological Pathology, QWK=Quadratically weighted Cohen’s\nkappa, TS=Task-specific model, UFM=UNI foundation model, VFM=Virchow2 foundation model.\n33\n\n\n \nExtended Data Figure 3. Gleason grading performance of AI models across international\nvalidation cohorts excluding benign samples. (a) Gleason grading concordance between the models\n(TS, UFM, VFM; all trained with 100% of the task-specific data) and the original reference standards\ndefined by cohort-specific local pathologists, measured in terms of QWK for Gleason score (left;\nreference Gleason scores not available for SPROB20, UKK, WNS) and for ISUP grade (right).  (b)\nGleason grading concordance of the models and the local pathologists relative to a uniform slide-level\nreference standard by the lead study pathologist, measured in terms of QWK for Gleason score (left)\nand ISUP grade (right). For the MLP, SCH and SPROB20 cohorts, the original reference standard\nwas not reported on slide-level and the comparison between the local pathologists and the lead study\npathologist is omitted. The granularity of the reporting of the original reference standards varied by\ncohort. The  ns,  nl and  np indicate the number of glass slides, anatomical locations or patients,\nrespectively, included in each analysis. Cases diagnosed as benign in the original reference standard\nwere excluded from these analyses. The values indicated by the plots represent point estimates on the\nfull cohorts, while the whiskers and error bars represent 95% confidence intervals estimated by\n34\n\n\n \nbootstrapping. ISUP=International Society of Urological Pathology, QWK=Quadratically weighted\nCohen’s kappa, TS=Task-specific model, UFM=UNI foundation model, VFM=Virchow2 foundation\nmodel, WSI=Whole slide image.6\n6  Blilie, A. (2025) https://BioRender.com/w87s307\n35\n\n\n \nExtended Data Figure 4: Grading performance of AI models across patient age groups and for\npatients treated for benign prostatic hyperplasia. (a) Gleason grading concordance between the\nmodels (TS, UFM, VFM; all trained with 100% of the task-specific data) and the original reference\nstandards defined by cohort-specific local pathologists, measured in terms of QWK for Gleason score\n(left) and for ISUP  grade (right) across patient  age groups  (for  cohorts  with age information\navailable). The SUH and SFR cohorts had an insufficient number of patients < 50 years to measure\nthe concordance. (b-c) Cancer detection performance measured by sensitivity (b) and specificity (c)\nagainst original reference standards on patients treated for benign prostatic hyperplasia with 5-ARI or\nalpha-blockers prior to biopsy. For the STHLM3 cohort, we included patients who were treated for at\nleast 3 months and who were biopsied within 6 or 12 months after treatment ended. The time of the\nbiopsy in relation to treatment was not reported for SFI patients and we therefore included all patients\nwith a mention of 5-ARI or alpha-blocker treatment in their pathology reports. The ns and np indicate\nthe number of glass slides and patients, respectively, included in each analysis. The values indicated\nby the plots for QWK, sensitivity and specificity represent point estimates on the full cohorts, while\nthe whiskers and error bars represent 95% confidence intervals estimated by bootstrapping. 5-ARI=5-\nɑ-reductase  inhibitor,  ISUP=International  Society  of  Urological  Pathology,  QWK=Quadratically\n36\n\n\n \nweighted Cohen’s kappa, TS=Task-specific model, UFM=UNI foundation model, VFM=Virchow2\nfoundation model.7\n7  Blilie, A. (2025) https://BioRender.com/v83h860\n37\n\n\n \nExtended Data Figure 5: Discriminatory capacity of AI models in cancer detection across\ninternational cohorts. The models’ (TS, UFM, VFM; all trained with 100% of the task-specific data)\ncancer detection performance against the original reference standard on each cohort, measured by\nAUC using ROC analysis. In addition, the mean AUC over all cohorts is indicated (see legend). The\nns,  nl and  np indicate the number of glass slides, anatomical locations or patients, respectively,\nincluded in each cohort. The values indicated by the bars represent point estimates and the error bars\nrepresent  95%  confidence  intervals estimated by  bootstrapping.   AUC=Area  Under the Curve,\nROC=Receiver  operating  characteristic,  TS=Task-specific  model,  UFM=UNI  foundation  model,\nVFM=Virchow2 foundation model.8\nExtended Data Figure 6: Gleason grading performance of the end-to-end model relative to the\nrefined  reference  standard.  (a-b)  Concordance  of  the  TS  model  with  the  original  reference\nstandards defined by cohort-specific local pathologists vs. the updated reference standards followed\nby re-assessment of significant errors by the lead study pathologist quantified in terms of QWK for\nGleason score (a) and for ISUP grade (b). To allow for a direct comparison, only cohorts with a slide-\nlevel original reference standard were included. The ns indicate the number of glass slides included in\neach cohort. The values indicated by the bars represent point estimates and the error bars represent\n95% confidence intervals estimated by bootstrapping. ISUP=International Society of Urological\nPathology, QWK=Quadratically weighted Cohen’s kappa, TS=Task-specific model.9\n8  Blilie, A. (2025) https://BioRender.com/n50j237\n9  Mulliqi, N. (2025) https://BioRender.com/m66s151\n38\n\n\n \nExtended Data Figure  7:  Cross-scanner Gleason scoring performance of AI models. (a) The\ncross-scanner concordance of each model (TS, UFM, VFM; trained with 1% or 100% of the task-\nspecific data) measured in terms of QWK for Gleason score, comparing each model’s grading for the\nsame slides digitized on pairs of different scanners (incl. 5 scanners for the STHLM3 cohort and 2\nscanners for the MUL cohort). In addition, the mean of the 10 pairwise comparisons on the STHLM3\ncohort is indicated (see legend). (b) Heatmaps showing the Gleason score predictions by each model\nacross glass slides (columns) digitized on different scanners (rows). For each model, the slides are\nsorted for visual readability based on the model’s average Gleason score prediction across scanners\ni.e. the column ordering is different between models. The number of glass slides included from the\ninternal (STHLM3) and external (MUL) validation cohorts is indicated by ns. The bars indicate point\nestimates  and  the  error  bars  represent  95%  confidence  intervals  estimated  by  bootstrapping.\n39\n\n\n \nQWK=Quadratically  weighted  Cohen’s  kappa,  TS=Task-specific  model,  UFM=UNI  foundation\nmodel, VFM=Virchow2 foundation model.\n40\n\n\n \nExtended Data Table 1. Gleason grading performance of AI models relative to the original\nreference standard in terms of Cohen’s linearly weighted kappa (LWK). Models’ (TS, UFM,\nVFM; all trained with 100% of the task-specific data) concordance with the original reference\nstandard measured with LWK for Gleason score and ISUP grade. The UKK, WNS, SPROB20 lack\nreference Gleason scoring. Additionally, LWK was evaluated on malignant slides only. The ns, nl and\nnp indicate the number of glass slides, anatomical locations or patients, respectively, included in each\ncohort. Corresponding numbers of malignant cases are indicated with  ns+,  nl+ and  np+. The LWK\nvalues indicate point estimates, with 95% CI estimated by bootstrapping given in parentheses.\nGS=Gleason score, ISUP=International Society of Urological Pathology, LWK=Linearly weighted\nCohen’s kappa, TS=Task-specific model, UFM=UNI foundation model, VFM=Virchow2 foundation\nmodel.\n41\n\n\n \nExtended  Data  Table  2.  Gleason  grading  performance  of  AI  models  relative  to  uniform\nreference standard in terms of Cohen’s linearly weighted kappa (LWK).  Models’ (TS, UFM,\nVFM;  all  trained  with  100%  of  the  task-specific  data)  and  local  cohort-specific  pathologists’\n(“Pathologist”) concordance with the uniform reference standard by the lead study pathologist,\nmeasured with LWK for Gleason score and ISUP grade. For the MLP, SCH and SPROB20 cohorts,\nthe original reference standard was not reported on slide-level and the comparison between the local\npathologists and the lead study pathologist is omitted. Additionally, LWK was evaluated on malignant\nslides only. The ns, nl and np indicate the number of glass slides, anatomical locations or patients,\nrespectively, included in each cohort. Corresponding numbers of malignant cases are indicated with\nns+, nl+ and np+. The LWK values indicate point estimates, with 95% CI estimated by bootstrapping\ngiven  in  parentheses.  GS=Gleason  score,  ISUP=International  Society  of  Urological  Pathology,\nLWK=Linearly weighted Cohen’s kappa, TS=Task-specific model, UFM=UNI foundation model,\nVFM=Virchow2 foundation model.\n42\n\n\n \nMethods\nStudy design and datasets\nThe study followed a pre-specified design (see Supplementary Appendix 1 for the study protocol).\nDetails on the included patient cohorts are provided in the study protocol and an overview is shown in\nFigure 1. All patients from the participating clinical sites who underwent prostate core needle biopsy\nwere initially eligible for inclusion. The exclusion criteria were based on problems with information\nretrieval (e.g. missing, mismatched, or ambiguous identifiers or pathology information), staining and\nslide preparation (e.g., non-prostate tissue, slides not stained with hematoxylin and eosin (HE), pen\nmarkings on tissue), or slide digitization (e.g. corrupt files).\nThe international prostate cancer digital pathology dataset includes 7,342 patients who underwent\nprostate biopsies between 2012 and 2023, resulting in approximately 100,000 core needle biopsies\nand 82,000 WSIs. Samples originate from 15 clinical sites or clinical trials across 11 countries\n(Austria, Australia, Denmark, Finland, France, Germany, the Netherlands, Norway, Poland, Sweden\nand Switzerland). All slides represent formalin-fixed, paraffin-embedded (FFPE) HE-stained prostate\ncore needle biopsy specimens with a varying number of cores and/or tissue sections per slide. The\ndata were partitioned into a model development set containing training (n=55,798 WSIs) and tuning\n(n=1,177 WSIs) cohorts and into a validation set containing internal (n=14,808 WSIs) and external\n(n=10,801 WSIs) validation cohorts. The data were split at patient-level, i.e. all WSIs representing a\ngiven patient were randomized together to avoid information leakage between development and\nvalidation data70,71.\nThe study involved two phases: a development phase and a validation phase. During the development\nphase, 10-fold cross-validation on the development data and separate evaluations on the tuning cohort\nwere used to evaluate the effects of different design choices on model performance. After the model\ndesign was completed, the fixed AI models were evaluated in the validation phase. Validation data\nwere held-out and not analyzed in any way prior to the design freeze. The internal validation data\nrepresent laboratories and/or scanners that are also present in the development set, whereas the\nexternal validation data are fully independent of the development or tuning cohorts in terms of\npatients, laboratories and scanners. There are three deviations from these rules: 1) The AQ validation\ncohort is not fully external due to a subset of it being scanned on the same Philips scanner that was\ninvolved in the development set (we present results separately for the partly external and fully\nexternal subsets of AQ), 2) A subset of the MUL validation cohort was re-scanned using the same\nPhilips scanner that was involved in the development set (we present results on these WSIs only in a\ncross-scanner consistency experiment, and used a fully external Grundium scanner for the main\nanalysis of this cohort), 3) The RUMC cohort was allocated for development and internal validation\n43\n\n\n \nin the study protocol, but we chose not to use it for model training (that is, the RUMC validation\ncohort is fully external to the model, but RUMC development data were accessible prior to design\nfreeze).\nWhole slide scanning\nSlides were scanned using 14 WSI scanner instruments (9 different models from 5 vendors) including\nPhilips IntelliSite UFS, Hamamatsu NanoZoomer (XR,  2.0-HT, S60, S360), Aperio (AT2 DX),\n3DHISTECH Pannoramic (250 Flash III, Scan ll), and Grundium Ocus40. The distribution of scanned\nWSIs across scanners is presented in Figure 6A. In the STHLM3 and MUL cohorts, subsets of slides\nwere re-scanned using 5 and 2 scanners, respectively. During the model design phase, multiple WSIs\nper slide in the STHLM3 development cohort were used as a data augmentation technique. During the\nmodel  validation  phase,  one  WSI  per  slide  was  picked  randomly  from  the  STHLM3  internal\nvalidation cohort for evaluation, and only WSIs captured with the fully external Grundium scanner\nwere used from the MUL external validation cohort. The other WSIs of these slides were reserved\nonly for cross-scanner reproducibility analysis. Importantly—all external validation cohorts (except\nfor a subset of AQ, see “Study design and datasets”) consist of slides digitized with scanners different\nfrom those used during model development. For details, see Supplementary Appendix 1: Table 2.\nData management\nOur data collection, management and verification process generally followed the process: patient\nidentifiers were pseudonymized at extraction at each site, slides were scanned and identifiers were\nstored in filenames or metadata. Linking slide data to clinical/pathology information involved parsing\nfilenames,  resolving  inconsistencies,  and  employing  semi-automated  OCR  systems  to  extract\nidentifiers. To ensure unique labeling and an added layer of centralized pseudonymization, MD5\nhashes were generated based on filenames, scanner serial numbers, scanning time and the original\npatient/slide identifiers. The clinical and pathology data for the cohorts were retrieved through various\nmethods: directly from registries, provided in tabular form by data providers, or manually tabulated\nfrom scanned pathology reports. Comprehensive unit testing was implemented with Python’s unittest\nframework to verify dataset integrity. For details, see Supplementary Appendix 1.\nReference standard protocols\nReference standards in the form of a pathologist’s Gleason grading were provided by the lead study\npathologist (L.E.) for the KUH-1, KUH-2, STG, and STHLM3 cohorts. Local pathologists at each site\ngraded  the  AQ,  AUH,  MLP,  MUL,  SCH,  SFI,  SFR,  SPROB20  and  SUH  cohorts.  Panels  of\npathologists  graded  the  following  validation  cohorts:  the  ImageBase45 subset  of  STHLM3  (23\npathologists), the subsets of RUMC serving as test sets in PANDA26 (4 pathologists), UKK (10\n44\n\n\n \npathologists), and WNS (11 pathologists). The granularity of reference standards differed across sites,\nincluding assessment per slide (AQ, AUH, KUH-1, KUH-2, MUL, RUMC, SFR, STG, STHLM3,\nSUH, UKK, WNS), per anatomical prostate location (MLP, SCH, SFI), and per patient (SPROB20).\nOnly the ISUP grade was provided for the SPROB20, UKK, and WNS cohorts, whereas all other\ncohorts have both ISUP grade and Gleason score reported. Reference standards across all cohorts\nwere obtained conventionally using a microscope, except for digital assessment for the UKK and\nWNS cohorts. For details, see Supplementary Appendix 1: Table 3.\nA uniform reference standard was additionally established by the lead study pathologist (L.E.) on\nsubsets of slides from all internal and external validation cohorts originally assessed by other\npathologists (except for cohorts with a panelist reference standard: UKK, WNS). Slides for re-\nassessment were selected randomly, and stratified on ISUP grade based on the original grading.\nAdditionally, the lead study pathologist re-assessed cases with clinically significant errors. All re-\nassessments were conducted blinded to the original grading and AI predictions, with grading reported\nper slide using the Cytomine platform72. For details, see Supplementary Appendix 1.\nTissue detection and tiling\nTissue was detected from WSIs using an in-house developed tissue segmentation model based on a\nUNet++ architecture73 with a ResNeXt-101 (32 x 4d)74 encoder. Patches of 512×512 pixels were first\nextracted across the entire WSI area at a resolution of 8.0 μm/px with an overlap of 128 px,\nsegmented pixel-wise to detect tissue, and then stitched into a single binary segmentation mask per\nWSI. Subsequently, high-resolution tissue patches of 256×256 px were extracted at 1.0 µm/px from\nthe WSIs using the segmented tissue masks to select only patches with minimally 10% of pixels\ndetected as tissue. Tissue patches were extracted either without overlap (for model training, to limit\nGPU memory footprint) or with 128 px overlap (for predictions, to improve diagnostic performance).\nPatches were downsampled from the closest, higher resolution in the resolution pyramid to 1.0 µm/px\nusing Lancsoz resampling. Patches were stored in the disk-friendly TFRecord data format75, with one\nfile per WSI.\nModel architectures\nAll models were built using an attention-based multiple instance learning (ABMIL)20 architecture with\nweak slide-level supervision, using either an end-to-end trained task-specific patch encoder or frozen\nfoundation model encoders. The encoders extract feature embeddings from patches, which are then\nattention-aggregated  into  slide-level  feature  vectors  for  further  classification.  The  TS  model\narchitecture  used  an  EfficientNet-V2-S  encoder42 to  extract  1280-dimensional  patch-level\nembeddings. The FMs were trained by their developers using the DINO v2 self-supervised learning\n45\n\n\n \nalgorithm76 which employs a student–teacher paradigm with both student and teacher networks\nutilizing the same architecture (ViT-L/16 for UFM, and ViT-H/14 for VFM)9,41 and produce 1024-\ndimensional (UNI) and 1280-dimensional (VFM) patch embeddings.\nAfter the patch encoder, the feature vectors first undergo average pooling and a fully connected (FC)\nlayer to reduce them to 1x1000 dimensions. Subsequently, they are passed through a gated-variant\nABMIL aggregator, first transformed into 512-dimensional representations through a linear layer and\nthen into hidden 384-dimensional representations in intermediate layers. Outputs from these layers are\ncombined through element-wise multiplication, and a final linear layer is used to compute attention\nweights for each patch. The attention weights are normalized using the softmax function and used as\nweights to the weighted mean of all patch embeddings that produce a final 512-dimensional slide-\nlevel feature vector. The slide-level feature vector is processed with classification layers for predicting\nprimary and secondary Gleason patterns for the set of input patches passed through the model (for\nslide-level grading: all patches from a WSI). The final classification layers contain FC layers and\nrectified linear unit (ReLU) activations, finally outputting two vectors (the primary and secondary\nGleason patterns) of 4-class classification logits (i.e. benign, Gleason pattern 3, pattern 4 or pattern 5),\nfollowed by softmax. We applied dropout (p=0.2) to the input embeddings as well as after each\nintermediate layer in the aggregator and classifier networks for regularisation.\nModel training\nThe TS model was trained with end-to-end learning, where all model parameters (incl. the encoder)\nwere jointly optimized with regard to a single loss function. The training of the FM-based models\nconsisted in keeping the FM encoders frozen with the weights from their self-supervised pre-training,\nand training only the attention-aggregator and classification layers. The EfficientNet-V2-S encoder\nwas initialized with ImageNet-pretrained weights, and the attention module and classification network\nwere initialized using Xavier initialisation77. All models were trained using cross-entropy loss and\nAdamW optimizer78 with a base learning rate (lr=0.0001).\nAt each iteration, up to 1,800 patches were randomly sampled without replacement per WSI to serve\nas a single minibatch associated with a WSI-level label. Training was run on multiple GPUs using the\ndistributed data-parallel (DDP) PyTorch framework79 with the NVIDIA collective communication\nlibrary (nccl) backend. Gradient accumulation was performed by averaging gradients over 4 iterations\nacross 8 GPUs to obtain an effective minibatch size of 32 WSIs. The number of tissue patches per\nminibatch varied, as not all WSIs had 1,800 patches. Each minibatch was sampled by selecting one\nWSI per GPU process, accumulating gradients independently on each GPU, and averaging them\nacross processes before the optimizer step. We used PyTorch's automatic mixed precision (AMP) 80\n46\n\n\n \nand gradient scaler for optimized memory use. In addition, to reduce the GPU memory footprint of\nend-to-end training, we applied activation checkpointing (i.e. recomputing activations during the\nbackward passes instead of storing them in GPU memory from the forward pass) for the TS encoder.\nAdditionally, memory pre-allocation was performed at the start of training to decrease GPU memory\nfragmentation due to variable-sized input WSIs having different patch numbers. \nThe model has two output heads both using cross-entropy losses for predicting the primary and\nsecondary Gleason patterns of a WSI. The overall loss is the summed loss of the two heads,\nnormalized by the gradient accumulation interval. The labels (i.e. 0 for benign slides and Gleason\npatterns 3, 4 and 5 for cancer) are mapped to an ordinal evenly spaced scale (i.e. 0, 1, 2, 3) before loss\ncalculations. After having obtained the raw output values from the models’ classification heads, we\napply the argmax rule to get Gleason patterns (i.e. 0, 1, 2, or 3). Given that there is no in-built\nregularization in the model to avoid invalid combinations of zero and non-zero Gleason patterns, we\nduplicate the values of single non-zero Gleason patterns (e.g. 0+3 will be corrected to 3+3). To\ncompute metrics during training, the Gleason scores are encoded onto an ordinal scale: benign (0),\n3+3 (1), 3+4 (2), 4+3 (3), 3+5 (4), 4+4 (5), 5+3 (6), 4+5 (7), 5+4 (8), 5+5 (9) as defined in other\nstudies81–83\nTo improve the robustness of the models to scanner and staining variation, we employ several types of\ndata augmentations. First, we use the subsets of training slides re-scanned on multiple instruments for\nscanner augmentation where on each epoch, one WSI per slide is randomly picked. To simulate slide-\nlevel staining and scanning variation, we apply stain augmentation by generating simulated variability\nin  stain  intensity  and  distribution84 together  with  Gaussian  blur,  unsharp  masking  and  color\naugmentations  to  all  patches  of  a  WSI.  Furthermore,  we  use  Sierra  color  calibration 59 as  an\naugmentation technique, where color calibration is applied to all patches of a WSI during training\nwith  a  probability  of  50%.  Each  patch  is  additionally  independently  processed  with  simple\ngeometrical  transformations  (i.e.  random  flipping  and  90°  rotations)  and  noise-simulating\naugmentations (such as Gaussian noise, ISO noise, decreasing image quality by JPEG and WebP\ncompression).\nTo reduce spurious correlations between image characteristics and the target labels during training,\nnamely variation of ISUP grade distribution across laboratories and/or scanners, we apply a sampling\nscheme. At the beginning of each epoch, we sample slides such that a uniform distribution of ISUP\ngrades is obtained for each scanner (that is, the ISUP grade distributions between scanners will also be\nidentical). The models were trained using 10-fold cross-validation, with folds stratified by patients,\nISUP grade and cohorts (STHLM3, SUH, STG). That is, each model was trained on 90% of the\ntraining data, and the remaining 10% in each CV fold was used to measure performance for early\n47\n\n\n \nstopping. After every epoch, QWK for ISUP was measured on these test data, and training was\nstopped if no improvement took place in 200 epochs. From each CV fold, the best model was kept,\nresulting in an ensemble of 10 models.\nModel prediction\nWe applied test-time augmentation (TTA) with three iterations per model, using simple geometric\ntransformations (i.e. flips and 90° rotations) applied randomly and independently to each patch. Due\nto some validation cohorts having labels assigned per prostate location or patient, we grouped the\nrespective tissue patches and obtained predictions for the pooled set of WSIs with a shared reference\nstandard label. All patches of each WSI were included to obtain predictions and processed stepwise in\nbatches of 64 to limit memory usage in the patch encoder part of the models. The final model\npredictions are a slide-level classification predicting the final Gleason score and ISUP grade for a\nWSI (or multiple WSIs per location or patient) and patch-level classifications predicting the Gleason\npatterns per patch. Encoding the raw output values from the model to Gleason scores was done in the\nsame way as during training. Model ensembling and TTA was done by obtaining the majority vote of\nthe 30 predicted Gleason scores (10 models x 3 TTA runs), and further translated into an ISUP grade.\nThe tile-level classification was performed by bypassing the ABMIL module and performing a\nclassification task iteratively on each patch obtaining the Gleason pattern probabilities per patch. This\napproach allows for e.g. visualizing Gleason patterns in the WSI.\nComputational efficiency measurements\nComputational efficiency  in  terms  of  time  and energy consumption  was  measured  by running\npredictions on the tuning set (n=801 slides) for the three models while logging power usage and\nruntime. To ensure that any observed differences in energy consumption were solely attributed to\nmodel complexity, each model was evaluated using identical hardware configurations (e.g., GPU\nmodels, memory allocation) and standard input settings. Experiments were run on 1 x NVIDIA A100\n40GB GPU for one trained model and one TTA run. The final values were multiplied by 30 to obtain\nthe total consumption for the 10-model ensemble with 3xTTA. The GPU power consumption was\nlogged using NVIDIA System Management Interface (nvidia-smi) monitoring real-time GPU power\ndraw at a frequency of 1 sample per second in Watts. The cumulative energy usage (kWh) was\nobtained by adding power draw values over the total runtime (GPUh) for each model. Additionally,\nper-biopsy energy consumption (Wh/biopsy) was calculated by normalizing total energy usage by the\nnumber of slides processed.\n48\n\n\n \nStatistical analysis\nTo quantify the concordance between the models and the reference standards in terms of ISUP grade\nand  Gleason  score  we  used  QWK and  LWK,  as  implemented  in  scikit-learn85,86.  The  average\nconcordance across models and pathologists was computed with the mean kappa values. To quantify\nthe concordance of negative/positive diagnosis for prostate cancer with the reference standard we used\nsensitivity (true positive rate), specificity (true negative rate), and the AUROC. We calculated the\n95% confidence intervals for the models’ and pathologists’ performance using bootstrapping over\n1000 replicates. We addressed statistical confounders in the training and validation data which can\nvery often cause models to exploit unintended correlations57,87,88, leading to unrealistically optimistic\nestimates of performance as long as such correlations are available to the model in validation sets. For\nexample, variations of the Gleason score and ISUP grade distributions across subsets of the data\ndigitized with different scanners (and/or prepared in different labs) can introduce such bias linking\nscanner type or clinical site to diagnostic outcomes. To mitigate this, we use external validation\ncohorts and ISUP sampling strategies during training. Another potential confounder is pen marks\nplaced by pathologists on the slides during diagnosis, which may lead models to associate pen\nmarkings with malignancy. We tackled this by segmenting only tissue for analysis and washing\naffected slides before scanning or excluding them if washing was not possible.\nComputing hardware and software\nWe used Python (v3.8.10), PyTorch (v2.0.0, CUDA 12.2) (https:// pytorch.org) and PyTorch DDP for\nmulti-GPU training for all experiments across all models. We used the pre-trained weights for UNI\nand  Virchow2  FMs  from  their  official  releases  on  the  HuggingFace  hub\n(https://huggingface.co/MahmoodLab/UNI; https://huggingface.co/paige-ai/Virchow2) and integrated\nthem with the ViT implementations provided by timm library (v0.9.8). All experiments were done on\ntwo  high-performance  clusters:  Alvis  (part  of  the  National  Academic  Infrastructure  for\nSupercomputing in Sweden) and Berzelius (part of the National Supercomputer Centre). On Alvis,\ntraining was done on 4 x 80GB NVIDIA A100 GPUs (256 GB system memory, 16 CPU cores per\nGPU). On Berzelius, training was done on 8 x 80 GB NVIDIA A100 GPUs (127 GB system memory,\n16 CPU cores per GPU). Predictions were run on the clusters on a single 40 GB A100 NVIDIA GPU.\nInitial model development and prototyping were done locally on 2 x 24 GB NVIDIA GeForce RTX\n3090  GPUs  (127  GB  system  memory,  32  CPU  cores).  Docker  (v20.10.21)  was  used  locally,\nSingularity and Apptainer were used on the computing clusters. OpenSlide (v4.0.0), openslide-python\n(v1.3.1),  and  OpenPhi  (v2.1.0)  were  used  to  access  WSIs.  DareBlopy  (v0.0.5)  was  used  for\ncompatibility between the TFRecord data format (.tfrecord) and PyTorch. Albumentations (v1.3.1)\nand Stainlib (v0.6.1) were used for image augmentations. For implementing the tissue segmentation\nmodel PyTorch segmentation_models_pytorch library (v0.3.3) was used. NumPy (v1.24.0), scikit-\n49\n\n\n \nlearn (v1.2.2), and Pandas (v1.5.3) were used for numerical operations, model evaluation, and data\nmanagement. Pillow (v9.4.0) and OpenCV-python were used for basic image processing tasks.\nMatplotlib (v3.7.1) and Seaborn (v0.12.2) were used for plots and figures and Biorender was used to\nassemble figure panels.\n50\n\n\nSupplementary Appendix 1 \n\n\nStudy Protocol: Development and Retrospective Validation\nof an Artificial Intelligence System for Diagnostic\nAssessment of Prostate Biopsies\nVersion 1.0\nNita Mulliqi1, Anders Blilie2,3, Xiaoyi Ji1, Kelvin Szolnoky1, Henrik Olsson1, Matteo Titus1,\nGeraldine Martinez Gonzalez1, Sol Erika Boman1,4, Masi Valkonen5, Einar Gudlaugsson2, Svein\nR. Kjosavik3,6, José Asenjo7, Marcello Gambacorta8, Paolo Libretti8, Marcin Braun9, Radzislaw\nKordek9, Roman Łowicki10, Kristina Hotakainen11,12, Päivi Väre13, Bodil Ginnerup Pedersen14,15,\nKarina\nDalsgaard\nSørensen15,16,\nBenedicte\nParm\nUlhøi17,\nMattias\nRantalainen1,\nPekka\nRuusuvuori4,18, Brett Delahunt19, Hemamali Samaratunga20, Toyonori Tsuzuki21, Emilius A.M.\nJanssen2,22, Lars Egevad23, Kimmo Kartasalo1, Martin Eklund1\n1.\nDepartment of Medical Epidemiology and Biostatistics, Karolinska Institutet, Stockholm, Sweden\n2.\nDepartment of Pathology, Stavanger University Hospital, Stavanger, Norway\n3.\nFaculty of Health Sciences, University of Stavanger, Stavanger, Norway\n4.\nDepartment of Molecular Medicine and Surgery, Karolinska Institutet, Stockholm, Sweden\n5.\nInstitute of Biomedicine, University of Turku, Turku, Finland\n6.\nThe General Practice and Care Coordination Research Group, Stavanger University Hospital,\nNorway\n7.\nDepartment of Pathology, Synlab, Madrid, Spain\n8.\nDepartment of Pathology, Synlab, Brescia, Italy\n9.\nDepartment of Pathology, Chair of Oncology, Medical University of Lodz, Lodz, Poland\n10. 1st Department of Urology, Medical University of Lodz, Lodz, Poland\n11. Department of Clinical Chemistry, University of Helsinki, Helsinki, Finland\n12. Laboratory Services, Mehiläinen Oy, Helsinki, Finland\n13. Mehiläinen Länsi-Pohja Hospital, Kemi, Finland\n14. Department of Radiology, Aarhus University Hospital, Aarhus, Denmark\n15. Department of Clinical Medicine, Aarhus University, Aarhus, Denmark\n16. Department of Molecular Medicine, Aarhus University Hospital, Aarhus, Denmark\n17. Department of Pathology, Aarhus University Hospital, Aarhus, Denmark\n18. Faculty of Medicine and Health Technology, Tampere University, Tampere, Finland\n19. Department of Pathology and Molecular Medicine, Wellington School of Medicine and Health\nSciences, University of Otago, Wellington, New Zealand\n20. Aquesta Uropathology and University of Queensland, QLD, Brisbane, Australia\n21. Department of Surgical Pathology, School of Medicine, Aichi Medical University, Nagoya, Japan\n22. Faculty of Science and Technology, University of Stavanger, Stavanger, Norway\n23. Department of Oncology and Pathology, Karolinska Institutet, Stockholm, Sweden\nCorresponding author: Martin Eklund, martin.eklund@ki.se.\n1\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n\n\nRevision history\nVersion and date\nStatus\nMain changes\n1.0 (2024-07-04)\nInitial publication.\nInitial publication.\n2\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nAbstract\nHistopathological evaluation of prostate biopsies using the Gleason scoring system is critical for\nprostate cancer diagnosis and treatment selection. However, grading variability among\npathologists can lead to inconsistent assessments, risking inappropriate treatment. Similar\nchallenges complicate the assessment of other prognostic features like cribriform cancer\nmorphology and perineural invasion. Many pathology departments are also facing an\nincreasingly unsustainable workload due to rising prostate cancer incidence and a decreasing\npathologist workforce coinciding with increasing requirements for more complex assessments\nand reporting.\nDigital pathology and artificial intelligence (AI) algorithms for analysing whole slide images\n(WSI) show promise in improving the accuracy and efficiency of histopathological assessments.\nStudies have demonstrated AI's capability to diagnose and grade prostate cancer comparably to\nexpert pathologists. However, external validations on diverse data sets have been limited and\noften show reduced performance. Historically, there have been no well-established guidelines for\nAI study designs and validation methods. Diagnostic assessments of AI systems often lack\npre-registered protocols and rigorous external cohort sampling, essential for reliable evidence of\ntheir safety and accuracy.\nThis study protocol covers the retrospective validation of an AI system for prostate biopsy\nassessment. The primary objective of the study is to develop a high-performing and robust AI\nmodel for diagnosis and Gleason scoring of prostate cancer in core needle biopsies, and at scale\nevaluate whether it can generalise to fully external data from independent patients, pathology\nlaboratories, and digitalisation platforms. The secondary objectives cover AI performance in\nestimating cancer extent and in detecting cribriform prostate cancer and perineural invasion. This\nprotocol outlines the steps for data collection, predefined partitioning of data cohorts for AI\nmodel training and validation, model development, and predetermined statistical analyses,\nensuring systematic development and comprehensive validation of the system. The protocol\nadheres to TRIPOD+AI, PIECES, CLAIM, and other relevant best practices.\n3\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nTable of contents\n1. Introduction................................................................................................................................6\n2. Study objectives..........................................................................................................................8\n2.1. Primary objective................................................................................................................8\n2.2. Secondary objectives.......................................................................................................... 8\n3. Artificial intelligence system.....................................................................................................8\n4. Study design................................................................................................................................9\n5. Inclusion and exclusion criteria..............................................................................................11\n5.1. Inclusion criterion............................................................................................................. 11\n5.2. Exclusion criteria.............................................................................................................. 12\n6. Data partitions..........................................................................................................................13\n6.1. Requirements for data partition........................................................................................ 13\n6.2. Predefined data partitions................................................................................................. 14\n7. Data cohorts..............................................................................................................................16\n7.1. Development, tuning and internal validation data cohorts............................................... 16\n7.1.1. Karolinska University Hospital (KUH-1)................................................................16\n7.1.2. Radboud University Medical Center (RUMC)........................................................17\n7.1.3. Capio S:t Göran Hospital (STG)..............................................................................18\n7.1.4. Stockholm3 (STHLM3)...........................................................................................18\n7.1.5. Stavanger University Hospital (SUH)..................................................................... 20\n7.2. External validation cohorts............................................................................................... 22\n7.2.1. Aichi Medical University (AMU)............................................................................22\n7.2.2. Aquesta Uropathology morphological subtypes (AQ)............................................ 22\n7.2.3. Aarhus University Hospital (AUH)......................................................................... 22\n7.2.4. Karolinska University Hospital morphological subtypes (KUH-2).........................23\n7.2.5. Mehiläinen Länsi-Pohja (MLP)...............................................................................23\n7.2.6. Medical University of Lodz (MUL)........................................................................ 24\n7.2.7. Synlab Switzerland (SCH).......................................................................................25\n7.2.8. Synlab Finland (SFI)................................................................................................26\n7.2.9. Synlab France (SFR)................................................................................................26\n7.2.10. Spear Prostate Biopsy 2020 (SPROB20)...............................................................27\n7.2.11. University Hospital Cologne (UKK)..................................................................... 27\n7.2.12. Hospital Wiener Neustadt (WNS)..........................................................................28\n4\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n8. Statistical analyses................................................................................................................... 28\n8.1. Overview of statistical analyses........................................................................................28\n8.1.1. Primary analysis: Diagnosis and Gleason scoring...................................................28\n8.1.2. Secondary analysis: Cancer extent prediction......................................................... 29\n8.1.3. Secondary analysis: Cribriform cancer detection.................................................... 29\n8.1.4. Secondary analysis: Perineural invasion detection..................................................29\n8.1.5. Exploratory analyses................................................................................................30\n8.2. Details of statistical analyses............................................................................................ 30\n8.3. Confounding factors..........................................................................................................36\n8.4. Representative sampling...................................................................................................37\n8.5. Power................................................................................................................................ 39\n8.6. Data quality and label noise..............................................................................................39\n9. Discussion................................................................................................................................. 41\n10. Ethical considerations............................................................................................................43\n11. Acknowledgements.................................................................................................................44\n12. Competing interests............................................................................................................... 44\n13. References...............................................................................................................................44\n14. Figures and tables.................................................................................................................. 50\n5\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n1.\nIntroduction\nHistopathological evaluation of prostate core needle biopsies is an important factor for prostate\ncancer diagnosis and treatment. Pathologists examine biopsies using the Gleason scoring system\n(Gleason, 1992) assigning primary and secondary grades based on the relative quantities of tissue\nrepresenting different Gleason patterns (e.g. a Gleason score of 3 + 4 = 7 indicating primary\nGleason pattern 3 and secondary Gleason pattern 4) (Epstein et al., 2005). Grading is however\ninherently subjective and associated with high intra- and inter-pathologist variability placing\npatients at risk of inappropriate treatment selection (Melia et al., 2006; Egevad et al., 2013;\nOzkan et al., 2016). With the aim of standardisation, the International Society of Urological\nPathology (ISUP) updated grading guidelines such that Gleason scores (GS) are pooled into five\nordinal categories (i.e. 1 to 5) referred to as the ISUP grades (also called grade groups or WHO\ngrade) (Ji, 2005; Epstein et al., 2016; WHO Classification of Tumours Editorial Board and\nInternational Agency for Research on Cancer, 2022). Besides Gleason scoring, similar issues\nalso hamper the reliable and repeatable assessment of other histopathological entities relevant to\nthe clinical management of prostate cancer, such as cribriform cancer morphology (Egevad et al.,\n2023) or perineural invasion (PNI) (Egevad et al., 2021), both of which are associated with a\npoor prognosis.\nDigital pathology (Pantanowitz et al., 2018) and the application of artificial intelligence (AI)\nalgorithms to analyse whole slide images (WSIs) hold promise for reducing variability and\nimproving\nthe accuracy of histopathological assessments. Many previous studies have\ndemonstrated that AI can diagnose and grade prostate cancer on par with expert pathologists\n(Campanella et al., 2019; Bulten et al., 2020, 2022; Ström et al., 2020). However, external\nvalidations demonstrating the generalisation capacity of these models on data spanning across\nscanning devices, laboratories, and patient populations not involved in the model development\nhave been limited. Moreover, results from the validation studies have often shown deteriorated\nperformance on the external data (Campanella et al., 2019; Swiderska-Chadaj et al., 2020; Ji et\nal., 2023). These complications are not specific to prostate pathology, as there are several\nexamples of scanner-induced variability and bias posing challenges for AI models across\ndifferent tasks and tissue types (Howard et al., 2021; Schmitt et al., 2021; Duenweg et al., 2023).\nThe unresolved issues with generalisation limit the widespread application of AI in clinical\npractice, including histopathology. The field has historically lacked well-established guidelines\non AI study designs and standardised methods for the proper evaluation and reporting of AI\nvalidation studies. Generally, diagnostic assessments of AI systems lack pre-registered study\n6\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nprotocols with predefined analysis plans and rigorous sampling of external cohorts, which are\nkey factors for generating reliable evidence of the safety and diagnostic accuracy of these\nsystems in view of further prospective evaluations in clinical trials (Nagendran et al., 2020;\nMcGenity, Bossuyt and Treanor, 2022). Here, we present a comprehensive study protocol for\nretrospective validation of an AI system for diagnostic assessment of prostate biopsies. This\nprotocol outlines study objectives, analysis and experimental pipelines, as well as data cohorts\nfor evaluating the generalisability and robustness of the AI system. The AI system is ultimately\nintended to be used as part of computer-aided diagnosis (CAD) software to provide\ndecision-making support for pathologists, but the focus of the current study is on the standalone\ndiagnostic performance of the system. Aspects relating to the clinical implementation of the\nsystem, user interaction, and analysis of the diagnostic performance of the system in combination\nwith the supervision of a human pathologist are outside of the scope of this protocol.\nSeveral guidelines have recently been proposed or are under development for reporting clinical\nvalidation\nstudies\nof\nAI-based\nmethods\ne.g.\nSPIRIT-AI\n(Standard\nProtocol\nItems:\nRecommendations for Interventional Trials) and its companion statement CONSORT-AI\n(Consolidated Standards of Reporting Trials), which are intended for protocols and reporting of\nrandomised clinical trials involving an AI intervention component (Cruz Rivera et al., 2020; Liu\net al., 2020), or the DECIDE-AI (Developmental and Exploratory Clinical Investigations of\nDEcision support systems driven by Artificial Intelligence) guideline which applies specifically\nto early, small-scale evaluation of AI interventions, with a focus on clinical utility, safety and\nhuman factors (Vasey et al., 2022).\nIn terms of guidelines applicable to pre-clinical and offline evaluation of AI prediction models,\nthe TRIPOD+AI (Transparent Reporting of a multivariable prediction model of Individual\nPrognosis Or Diagnosis) (Collins et al., 2021) guideline on developing or reporting performance\nof AI prediction models has recently been released (Collins et al., 2024), while the STARD-AI\n(Standards for Reporting of Diagnostic Accuracy Study) (Sounderajah et al., 2021) guideline is\nstill under development. This protocol incorporates guidelines by the TRIPOD+AI (Collins et\nal., 2024), applicable parts of the best practice checklists proposed in PIECES (Protocol Items\nfor External Cohort Evaluation of a Deep Learning System in Cancer Diagnostics) (Kleppe et al.,\n2021), CLAIM (Checklist for AI in Medical Imaging) (Mongan, Moy and Kahn, 2020; Tejani et\nal., 2023) and methodological checklists with a focus on radiology due to absence of such\nguidelines in the field of pathology (Park and Han, 2018). This AI study protocol covers the\nsteps of data collection, prespecified partitioning of data cohorts, model development, and\nprespecified statistical analyses, ensuring systematic development and thorough validation of the\nsystem.\n7\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n2.\nStudy objectives\nThe objective of the study is to develop a high-performing and robust AI model for diagnosis and\nGleason scoring of prostate cancer in core needle biopsies, and at scale demonstrate that it can\ngeneralise to fully external data from independent patients, pathology laboratories, and\ndigitalisation platforms.\n2.1.\nPrimary objective\nThe primary objective is to assess the concordance between the AI model and pathologists in\ndiagnosing and Gleason scoring prostate cancer in core needle biopsies.\n2.2.\nSecondary objectives\nThere are three secondary objectives which this study accommodates:\n-\nAssess the concordance between the AI model and pathologists in measuring cancer\nextent (in millimetres) in prostate core needle biopsies.\n-\nAssess the concordance between the AI model and pathologists in detecting perineural\ninvasion in prostate core needle biopsies.\n-\nAssess the concordance between the AI model and pathologists in detecting cribriform\ncancer in prostate core needle biopsies.\n3.\nArtificial intelligence system\nThe AI system to be developed and evaluated in this study is intended for the histopathological\nassessment of digitised prostate core needle biopsies. The system will be based on deep neural\nnetworks and its specific design (e.g. image preprocessing steps, model architecture and training\napproach) will be optimised during the study (see Section 4 for further description of the design\nchoices and hyperparameters that will be evaluated). This study comprises multiple AI models,\neach tailored for the specific objectives i.e. grading, perineural invasion, cribriform cancer and\ncancer length and together these models integrate to form an AI system.\nSystem input: A WSI stored in a supported vendor-specific format, depicting a formalin-fixed,\nparaffin-embedded (FFPE) haematoxylin & eosin (HE) stained prostate core needle biopsy\nspecimen with one or several tissue cuts of one or several biopsy cores.\n8\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nSystem output:\n-\nGleason score: the system will output GS, such as 4 + 3 = 7, indicating the primary and\nsecondary patterns observed within the input WSI. The GS ranges from 3 + 3 = 6 to 5 + 5\n= 10, with lower scores representing less aggressive cancer and higher scores indicating\nmore aggressive cancer. Benign samples are encoded as 0 + 0.\n-\nISUP grade: the system will output an ISUP grade which groups GS into ordinal\ncategories, ranging from 1 to 5. The GS are expressed as ISUP grades as follows: ISUP 1\n(GS 6), ISUP 2 (GS 3 + 4 = 7), ISUP 3 (GS 4 + 3 = 7), ISUP 4 (GS 8), ISUP 5 (GS 9 -\n10). Benign samples are encoded as 0.\n-\nCancer extent: the system will quantify the extent of cancer within the provided WSI in\nmillimetres. This measurement indicates the size of the cancerous area within the tissue\nspecimen.\n-\nCribriform cancer: the system will output the predicted probability of cribriform prostate\ncancer morphology being present within the input WSI.\n-\nPerineural invasion: the system will output the predicted probability of perineural\ninvasion being present within the input WSI.\n-\nVisualisation: the system will provide a visualisation of its predictions including areas of\ndifferent Gleason patterns, PNI and cribriform cancer, which can be examined in a WSI\nviewer software overlaid on the digital slide. The exact format of the visualisation will\nvary depending on the viewer software.\n4.\nStudy design\nIn this study, the aim is to develop the AI system described above and validate its diagnostic\nperformance on retrospectively collected cohorts. To carry out the study, historical data,\nincluding medical records, pathology reports, and digitised images have been collected for cases\nwhere both the AI system and human pathologists make diagnostic assessments. The study\ndesign involves two independent phases: AI system development and AI system validation as\nshown in Fig. 1. The development phase involves an iterative cycle of refining the model design\nand hyperparameters using predefined development and tuning cohorts for model training and\nestimation of the effects of design choices on diagnostic performance. Once the overall\nperformance on the development and tuning sets is deemed to have reached a plateau and further\nchanges to the model design no longer yield meaningful improvements, a design freeze will take\nplace and the final AI model will be graduated to the validation phase. This design achieves\ncomplete isolation between the model development and the retrospective validation to avoid any\ninformation leakage, which could lead to overly optimistic validation results. All model\nparameters and hyperparameters, including selection of any classifier thresholds, will be set\n9\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nbased on the development and tuning cohorts and no adjustments or tweaking will be conducted\non the validation cohorts, which will remain entirely untouched during the development phase.\nThe development cohorts provide a wide representation of tissue morphologies, scanning\ndevices, laboratories, and clinical characteristics of patients, allowing for the training of a robust\nmodel. The tuning cohorts enable assessing model generalisation (i.e. performance on data from\ndifferent laboratories and scanners than the development cohorts) on each development iteration,\nand direct performance comparison with state-of-the-art models evaluated on these same datasets\nin earlier studies (Ström et al., 2020; Bulten et al., 2022). Sequential experiments will be\nconducted one modification at a time to evaluate e.g. different preprocessing approaches for\nextracting\nimage\ndata\nfrom\nthe\nWSI,\ndeep\nneural\nnetwork\narchitectures,\noptimiser\nhyperparameters etc. (see Supplementary Appendix Section 1). Model performance at each step\nwill be measured using cross-validation on the development cohorts and independent evaluation\non the tuning cohorts. To accelerate the development process by reducing runtime for early\nmodel designs and to simplify troubleshooting, we will initially only use one of the development\ncohorts for model training and gradually introduce the other development cohorts one by one.\nThis approach to model development allows:\n-\nEffective troubleshooting: systematic experiments facilitate easier debugging and\nidentification of error root causes.\n-\nTraceability and accountability: transparency and traceability of how the model evolved\nduring development, and accountability in cases of improvements or issues.\n-\nIsolation of changes: the impact of each modification is assessed independently without\nthe confounding effects of simultaneous changes (e.g. changing multiple hyperparameters\nat once).\n-\nOptimal model tuning: controlled and sequential modifications allow for optimal tuning\nof the model and achieving the best possible model performance.\nThe validation phase will employ a blinded approach, wherein neither the pathologists nor the AI\nmodel have access to each other’s assessments. The validation cohorts consist of samples\nrepresenting a range of heterogeneous clinical settings and were collected from patients not\nincluded in the development or tuning cohorts. They are categorised as internal (scanner and\nlaboratory included in the development set), partly external (scanner included in the development\nset) or fully external (neither scanner nor laboratory included in the development set) depending\non the slide scanners and clinical laboratories involved. Internal validation can be expected to\nprovide an optimistic estimate of the diagnostic performance of the AI model in the absence of\nlaboratory or scanner variation. The generalisation performance of the model will ultimately be\n10\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nevaluated on the external validation cohorts, which avoids any optimistic bias. The design also\nallows for additional validation cohorts to be added at any point after the development phase.\nDue to inter-observer variability among pathologists, reference standards established by\npathologists vary across different validation cohorts. This complicates the assessment of the AI\nmodel for generalisation across cohorts, as any differences in observed performance can be\npartly attributed to differences in reference standards and partly attributed to imperfect AI\ngeneralisation to data originating from different clinical sites. In the case of the primary study\nobjective of Gleason scoring, we have addressed this issue by having a representative subset of\nslides from each cohort be re-assessed by the lead pathologist (L.E.). The lead pathologist is\nhighly experienced in urological pathology and has shown high concordance relative to other\nexperienced uropathologists in several studies (Kweldam et al., 2016; Egevad et al., 2017;\nBulten et al., 2022). For the secondary study objectives of cribriform cancer and perineural\ninvasion detection, the assessments were conducted either by the lead pathologist or by other\nexperienced (>25 years of clinical experience after residency) uropathologists (B.D., H.S.) whose\nconcordance with the lead pathologist has been quantified in earlier studies (Egevad et al., 2021,\n2023). This provides a consistent reference standard which will allow us to assess the technical\ngeneralisation performance of the model (without complete confounding between laboratory,\nscanner, and pathologist reference standards), in addition to large-scale evaluation relying on the\nvarying reference standards provided by the local pathologists for each cohort.\nClinical and pathological characteristics of the included patients are summarised in Table 1 and\ndetailed information on the slide scanning is provided in Table 2. Details on reference standards\nfor each cohort with respect to grading are given in Table 3, and with respect to cribriform cancer\nand PNI are given in Table 4. Information on slides representing morphological subtypes is given\nin Table 5, and the number of slides for which immunohistochemistry (IHC) staining was\nperformed in order to confirm the diagnosis is tabulated in Table 6. See Supplementary\nAppendix Section 3 for CONSORT diagrams summarising the data cohorts.\n5.\nInclusion and exclusion criteria\nProvided below are the detailed criteria used to assess the eligibility of patients, individual\nbiopsy slides, or WSIs for inclusion in this study.\n5.1.\nInclusion criterion\n-\nPatients who underwent a prostate core needle biopsy were eligible.\n11\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n5.2.\nExclusion criteria\n-\nClinical information:\na) Patients with either slides or associated pathology information unavailable.\nb) Slides lacking identifiers (IDs) preventing linkage to the pathology data.\nc) Slides with identical IDs preventing unambiguous linkage to the pathology data.\nd) Slides with mismatching GS and ISUP grade information.\ne) Slides with mismatching information concerning malignancy and GS or ISUP\ngrade (e.g. indicated benign but a GS is provided).\nf)\nSlides with partial or erroneous GS reporting (e.g. <6, 4 + 0 or 1 + 1 etc.).\n-\nStaining and slide preparation:\na) Samples not containing prostate tissue e.g. bladder biopsies, testicular biopsies.\nb) Samples not stained with HE (e.g. IHC stains).\nc) Initial cuts of tissue blocks deemed unsuitable by the pathologist for providing a\ndiagnosis and requiring a recut.\nd) Empty biopsy slides with no tissue on the glass.\n-\nSlide integrity and annotation:\na) Slides with pen mark annotations that cover a vast amount of the tissue, obscuring\nthe underlying morphology.\nb) Slides with pen mark annotations conflicting with the pathology diagnosis (e.g.\nthere exists a pen mark annotation on the slide, but the slide is diagnosed as\nbenign or vice versa). This only applies to the STHLM3 cohort (see Section 7.1),\nwhere the pen mark annotation process is known to be consistent for all samples.\nc) Slides with pen mark annotations that result in the majority of the tissue being out\nof focus during scanning.\n-\nSlide digitisation:\na) Earlier scans of the same slide on the same scanner instrument, assuming the\nlatest WSI represents a successful rescanning due to e.g. earlier focus issues.\nb) Corrupt WSI files which cannot be accessed with Openslide (Goode et al., 2013)\nor OpenPhi (Mulliqi et al., 2021).\n12\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n6.\nData partitions\n6.1.\nRequirements for data partition\nWe established a number of requirements to guide the inclusion, exclusion and partitioning of\ndata into development, tuning and validation sets to account for several sources of potential bias\nin the training and validation of the model. We followed available guidelines and criteria for\nbalanced and representative data partitions (Park and Han, 2018; Mongan, Moy and Kahn, 2020;\nWillemink et al., 2020; Varoquaux and Cheplygina, 2022) and arrived at the following set of\nrequirements:\n1. Representative sample selection: Ensure the data are representative of the diversity\nencountered in clinical practice by including multi-site cohorts with variations in\nscanning equipment (e.g. vendors, models, image formats), biopsy preparation (e.g.\nstaining, tissue cutting), morphological heterogeneity (e.g. different Gleason scores and\nrare cancer subtypes) and patient demographics.\n2. Representative sample size: Include a sufficiently large sample for development and\nvalidation to increase the probability of generalisability in the larger population.\n3. Mitigate overfitting due to observer bias: Alleviate the possibility of overfitting or “over\ntweaking” of the model, which may be caused by excessive refinement of the model\ndesign aimed at maximising cross-validation performance in development data, since that\ncan jeopardise generalisation outside the development cohorts. The issue can be mitigated\nby additional (external) tuning data cohorts serving as a less biased performance indicator\nduring model development. It should be further ensured that the tuning cohorts are\nindependent of model training (for example, criteria for early stopping of model training\nshould be assessed only on the development data).\n4. Ensure independence of specimens between data partitions: Each data partition\n(development, tuning, internal or external validation sets) should be independent of the\nothers with no overlap of biopsies or patients.\n5. Ensure independence of sample preparation process between data partitions: Sample\nexternal cohorts such that there is no overlap with respect to the clinical laboratories that\nprepared these cohorts and the development cohorts.\n6. Ensure independence of the digitisation process between data partitions: Sample external\ncohorts such that there is no overlap with respect to the scanning device used for these\ncohorts and the development cohorts.\n13\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n6.2.\nPredefined data partitions\nThe process of splitting the data cohorts into development, tuning, and internal and external\nvalidation sets was conducted adhering to the requirements for data partitions and is described\nbelow (see Fig. 1 for an overview). The characteristics of the data cohorts included in this study\nare summarised in Table 1 and described in detail in Section 7.\nThe development set was sampled from the following cohorts: Capio S:t Göran Hospital (STG),\nRadboud University Medical Center (RUMC), Stavanger University Hospital (SUH) and\nStockholm3 (STHLM3). From the RUMC, STHLM3 and SUH cohorts, the patients who were\nnot allocated to tuning or validation sets (see below) were assigned to the development set\n(approximately 80% of patients). Given the limited size and skewed grade distribution of the\nSTG cohort, it was fully allocated into the development set. The development set covers several\nclinical laboratories and scanner devices as well as a large degree of variation in tissue\nmorphology and the clinical characteristics of patients, in part due to the largest cohort,\nSTHLM3, originating from a population-based screening trial (Requirements 1-2). Each of the\ndevelopment cohorts was further split into 10 cross-validation folds by randomly allocating\npatients to folds, stratified by the maximum slide level ISUP grade of each patient.\nThe tuning set was sampled from the following cohorts: Karolinska University Hospital\n(KUH-1), RUMC and STHLM3. The entire KUH-1 cohort was assigned to tuning and represents\na fully external cohort relative to the development set (i.e. different patients, laboratory and\nscanner). This set also corresponds to the European external validation cohort of the PANDA\nchallenge (Bulten et al., 2022). The subsets of the RUMC and STHLM3 cohorts assigned to the\ntuning set represent internal data relative to the development set (i.e. different patients but the\nsame laboratories and scanners) and correspond to the PANDA public test sets in Kaggle (i.e. the\nPANDA tuning sets). The tuning sets allow for evaluating the effects of model design changes on\ndata that is independent of the development set, direct comparison with state-of-the-art models\nfrom PANDA, and in the case of KUH-1, assessing the generalisation performance of the model\nprior to design freeze (i.e. performance on data coming from different patients, laboratories, and\nscanners compared to the development data) (Requirement 3). A subset of slides belonging to the\nPANDA Swedish tuning set was allocated to the internal validation set for reasons related to\npatient stratification and the inclusion of specific subsets of interest in the internal validation (see\nbelow).\n14\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nThe internal validation set was sampled from the following cohorts: RUMC, STHLM3 and\nSUH, consisting of patients who were not part of the development or tuning sets but whose\nbiopsies were obtained from the same clinical laboratories and scanned with the same scanners\nas the development and tuning set samples. The STHLM3 internal validation set includes the\nfollowing subsets, supplemented with randomly sampled patients to achieve a total 20% fraction\nof patients assigned to tuning and validation: ImageBase (Egevad et al., 2017), Swedish private\ntest set in Kaggle (i.e. PANDA Swedish internal validation set) (Bulten et al., 2022), perineural\ninvasion multi-observer validation set (Kartasalo et al., 2022), and rare morphological subtypes\nset (Olsson et al., 2022). Including these samples as subsets of the internal validation set will\nfacilitate (internal) comparisons with results obtained in the papers referenced in the preceding\nsentence. The SUH internal validation set includes the following subsets, supplemented with\nrandomly sampled patients to achieve a 20% fraction of patients assigned to validation: all\npatients (n=25) with multiple recuts of their biopsy tissue blocks, and patients (n=81)\ncorresponding to a random selection of 119 slides stratified on ISUP grade (to be rescanned\nrepeatedly over time for an AI temporal stability study). The STHLM3 subsets allocated into the\ninternal validation set were selected based on being particularly valuable for the evaluation phase\nof the study, while the SUH subsets will be used as validation sets in upcoming follow-up studies\ninvolving the AI model developed here, hence cannot be assigned to the development set. The\nRUMC internal validation set includes the RUMC private test set in Kaggle (i.e. PANDA RUMC\ninternal validation set) (Bulten et al., 2022), supplemented with randomly sampled patients to\nachieve a total 20% fraction of patients assigned to tuning and validation.\nExternal validation cohorts are fully external relative to the development set (no overlap with\nrespect to patients, laboratory, or scanner) or partly external (no overlap with respect to patients\nor laboratory, but digitisation performed using a scanner that is present also in the development\nset). Fully external validation set cohorts include Aarhus University Hospital (AUH), Karolinska\nUniversity Hospital morphological subtypes (KUH-2), Mehiläinen Länsi-Pohja (MLP), Medical\nUniversity of Lodz (MUL), Synlab Switzerland (SCH), Synlab Finland (SFI), Synlab France\n(SFR), Spear Prostate Biopsy 2020 (SPROB20), University Hospital Cologne (UKK), Hospital\nWiener Neustadt (WNS). Partly external validation set cohorts include: Aquesta Uropathology\nmorphological subtypes (AQ), partially scanned on a scanner present in the development set and\npartially scanned on an external scanner. The external nature of the validation set cohorts fulfils\nRequirements 4-6.\nAll data splits were performed on patient level, that is, all slides and resulting WSIs from a given\npatient were allocated to the same data partition in order to avoid information leakage between\ndevelopment and validation sets. If a patient was biopsied on several occasions, all biopsies were\n15\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nincluded and allocated together. Any samples lacking patient identifiers were assigned to\ndevelopment data to avoid the risk of slides from any patients ending up in both development\nand evaluation cohorts.\nSubsets of the slides included in this study have been scanned multiple times. If the same slide\nhad been rescanned multiple times on the same individual scanner (i.e. the same physical\ndevice), we only kept the WSI with the latest scanning date, assuming the rescanning was due to\ne.g. initially poor focus or other scanning issues. Subsets of the STG, STHLM3 and MUL\ncohorts were rescanned with multiple different scanners (see Table 2). To avoid biasing the\nevaluation towards these slides that appear in the dataset multiple times, we will only include one\nWSI per slide in the validation sets. For STHLM3, we will randomly select one WSI for each\nslide to be evaluated, and for MUL, we will utilise WSIs from the Grundium Ocus40 scanner,\nexcluding those on the Philips UFS scanner. This ensures that the MUL cohort remains entirely\nexternal relative to the development data, considering that the STHLM3 cohort was partly\ndigitised on the same Philips UFS instrument. The repeated scans will, however, be used during\nAI model development as an augmentation technique (except for the Grundium Ocus40 which is\nkept as an external scanner for validation), and for a separate cross-scanner reproducibility\nanalysis (see Section 8).\n7.\nData cohorts\n7.1.\nDevelopment, tuning and internal validation data cohorts\n7.1.1.\nKarolinska University Hospital (KUH-1)\nThe KUH-1 samples were collected at the Department of Pathology, Karolinska University\nHospital in Solna, Sweden in 2018. Among the cases assessed by L.E. during 2018, we included\nall positive slides of all patients diagnosed with ISUP grade 2-5 cancer, all positive slides from a\nrandom selection of patients diagnosed with ISUP grade 1 cancer, and all slides from a random\nselection of patients with a negative diagnosis. Patients underwent systematic transrectal biopsies\nin approximately 1/3 of the cases, and magnetic resonance imaging (MRI) targeted or combined\nbiopsies in approximately 2/3 of cases. Slides typically contain one core, sectioned at two levels.\nThis cohort has been used as an external validation set in previous studies (Ström et al., 2020;\nBulten et al., 2022).\n16\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n7.1.1.1.\nReference standard protocol\nAll cases were assessed by the lead pathologist (L.E.) using a microscope to determine the GS\nand cancer extent per slide, as well as the ISUP grade per slide and per patient. The linear cancer\nextent was generally measured from end to end in cases with discontinuous cancer and it was\nreported on a per-cut level.\n7.1.2.\nRadboud University Medical Center (RUMC)\nThe RUMC samples were collected at the Radboud University Medical Center in Nijmegen, the\nNetherlands from January 2012 to December 2017 (Bulten et al., 2020). Patients were sampled\nrandomly, stratified by the highest reported GS in the pathology reports, and the slide with the\nmost aggressive part of the tumour was included for each patient. Additionally, a group of\npatients with only benign biopsies were randomly sampled. Patients generally underwent\nMRI-targeted transrectal biopsy. The data underwent additional refinement in preparation for the\nPANDA Kaggle challenge (Bulten et al., 2022): only one core, sectioned at one level was\nretained per WSI, the background was masked to hide most of the markings made on the glass,\nand the images were converted into .tiff format (JPEG compression, quality 70). For the\npurposes of PANDA, the cohort was partitioned into three sets—development, tuning, and\ninternal validation, stratified by patient and the highest Gleason pattern in the biopsy.\n7.1.2.1.\nReference standard protocol\nThe reference standard for all cases on the RUMC development set was determined based on the\noriginal pathology reports. Due to each slide containing multiple biopsy cores, trained\nnon-experts digitally outlined the individual cores, allowing them to be partitioned into separate\nWSIs, and assigned core-level GS based on the pathology reports. Inconclusive pathology\nreports were assigned for a second review, and if no match could be made these cases were\ndiscarded (Bulten et al., 2020).\nSubsets of the cohort underwent additional re-assessments as follows:\n●\nThe PANDA RUMC tuning set (n=195, corresponds to our RUMC tuning set) and the\nPANDA RUMC internal validation set (n=333, part of our RUMC internal validation set)\nwere assessed in three rounds. In the first round, three uropathologists individually\ngraded the cases digitally, providing a GS per slide. A majority vote was taken for cases\nwhere an agreement was reached on the ISUP grade but there was a discrepancy in the\nGleason patterns, and cases where two uropathologists agreed and the third one had a\nmaximum deviation of one ISUP grade. In the second round, all the cases that did not\nachieve consensus were re-graded by the uropathologist whose grade differed from the\n17\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nothers, followed by pooling of all the assessments and discussion in a consensus meeting\nin the third round. The GS was reported per slide.\n●\nA subset of slides (n=66) from the RUMC internal validation cohort was randomly\nselected, stratified by the ISUP grade, for re-assessment by the lead pathologist (L.E.).\nThis re-assessment was conducted digitally on Cytomine (Marée et al., 2016) using\n3DHISTECH WSIs (.mrxs converted to .tiff) to report the GS per slide.\n7.1.3.\nCapio S:t Göran Hospital (STG)\nThe STG samples were collected at Capio S:t Göran Hospital in Stockholm, Sweden from 2016\nto 2017. We included a random selection of slides with an enrichment for high-grade cancer.\nPatients underwent transrectal biopsy, and slides typically contain one core, sectioned at two\nlevels. This cohort was also part of the development set in a previous study (Ström et al., 2020).\n7.1.3.1.\nReference standard protocol\nAll cases were assessed by the lead pathologist (L.E.) using a microscope to provide GS, ISUP\ngrade, and cancer extent on a per-slide level. The linear cancer extent was generally measured\nfrom end to end in cases with discontinuous cancer and it was reported on a per-cut level.\n7.1.4.\nStockholm3 (STHLM3)\nThe STHLM3 samples were collected in a population-based clinical trial (ISRCTN84445406)\n(Grönberg et al., 2015) from 2012 to 2015 in Stockholm, Sweden. Histological sample\npreparation was performed at Histocenter, Gothenburg, Sweden, and the samples were assessed\nat the Department of Pathology, Karolinska University Hospital in Stockholm. Patients\nunderwent 10-12 core systematic transrectal biopsies and slides usually contain one core,\nsectioned at two levels. Subsets of the digitised samples have been used as development and\ninternal validation sets in previous studies (Ström et al., 2020; Bulten et al., 2022; Ji et al., 2022;\nKartasalo et al., 2022; Olsson et al., 2022). Patient and slide selection, retrieval and digitisation\ntook place on five occasions between 2014 and 2023 (see Table 2), as below:\n●\n2014: All cores from the first 500 patients diagnosed with prostate cancer in the\nSTHLM3 trial were scanned on a Hamamatsu NanoZoomer 2.0-HT.\n●\n2017-2019: All patients with at least one core graded as GS 4 + 4 or 5 + 5 and 497\nrandomly selected patients with at least one core graded as 3 + 3 were considered. From\neach of these patients, we included all positive cores and a randomly selected negative\ncore. Finally, we randomly selected 139 cancer-free patients from whom we included one\nrandomly selected core. Additionally, we added all cores which were indicated to have\nPNI and had not been scanned earlier. The cores were scanned on an Aperio AT2.\n18\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n●\n2018-2019: The cores of a random selection of patients were scanned on a Hamamatsu\nNanoZoomer XR.\n●\n2019-2020: The cores of a random selection of patients were scanned on the Philips\nIntelliSite Ultra Fast Scanner (UFS).\n●\n2023: Patients belonging to the PANDA challenge Swedish public and private validation\nsets were scanned on the Grundium Ocus40.\n●\n2023: Initially, cores with < 4 millimetres of cancer were excluded to have sufficient\ncancer tissue for future molecular profiling of the samples. Among the remaining\npatients, 50% of those with ISUP 1 or ISUP 2 (patient level ISUP) were randomly\nselected for inclusion, while all patients with ISUP 3-5 were included for scanning on the\nGrundium Ocus40.\n7.1.4.1.\nReference standard protocol\nAll cases were assessed by the lead pathologist (L.E.) using a microscope to obtain the GS, the\nISUP grade, cancer extent and PNI on a per-slide level. The linear cancer extent was generally\nmeasured from end to end in cases with discontinuous cancer and reported on a per-cut level.\nHowever, in cases with 1 or 2 cores infiltrated by low-grade discontinuous cancer with a benign\ngap exceeding 3 millimetres, the benign tissue was subtracted in the reporting of total cancer\nextent.\nSubsets of the cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=212) from the STHLM3 internal validation cohort underwent a\nsecond review to construct a reference standard for the PANDA Swedish internal\nvalidation set. Slides initially indicated as benign according to the original reference\nstandard were not re-reviewed, while cases indicated as malignant were divided between\ntwo uropathologists (B.D. and H.S.), each reviewing 100 slides blinded to the original\nreview. In the case of agreement between the initial and the second review, the consensus\nISUP grade was assigned to the case. In case of disagreement, a third uropathologist\n(T.T.) reviewed the case. For cases that were indicated as malignant by all pathologists,\nthe final ISUP grade was assigned according to 2/3 consensus. If all three reviews were in\ndisagreement, the case was excluded from the internal validation set. Any cases indicated\nas benign in the second or third review were excluded from the PANDA Swedish internal\nvalidation set. The re-assessment was conducted digitally on Cytomine using Hamamatsu\nand Aperio WSIs (.ndpi and .svs converted to .tiff) as described in (Bulten et al., 2022).\n●\nA subset of slides (n=24) from the STHLM3 internal validation cohort was additionally\nassessed by the lead pathologist (L.E.) for specific rare morphologies (see Table 5) using\n19\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\na microscope. This set has been used as validation data in a previous study (Olsson et al.,\n2022).\n●\nA subset of slides (n=87) from the STHLM3 internal validation cohort, representing the\nImageBase set (Egevad et al., 2017) was additionally assessed by an expert panel of\nuropathologists (n=23). The assessment was conducted using digital micrographs. This\nset has been previously used in the study (Ström et al., 2020) as an internal validation set.\n●\nA subset of slides (n=702) from the STHLM3 development and internal validation\ncohorts was digitally assessed for cribriform cancer as described in (Egevad et al., 2023).\nTo arrive at this selection, we first enriched Gleason pattern 4 tissue by randomly\nselecting one core per combination of patient and ISUP grade among all cores with ISUP\ngrades 3-5. To maintain some representation of GS 3+4 biopsies, we randomly selected\n86 additional cores with one core per patient from the set of all cores with ISUP grade 2.\nThe slides were assessed by the lead pathologist (L.E.) on Cytomine using Hamamatsu\n(.ndpi) and Aperio (.svs) WSIs to create pixel-wise annotations of areas with cribriform\ncancer. The pathologist could also indicate uncertain cases with a borderline category.\n●\nA subset of slides positive for cribriform cancer (n=152) and a random selection of slides\nnegative for cribriform cancer (n=152) according to the assessment by L.E. were\nadditionally assessed by an expert panel of uropathologists (n=9) as described in (Egevad\net al., 2023). The pathologists assessed the presence of cribriform cancer on slide level on\nCytomine using Hamamatsu (.ndpi) and Aperio (.svs) WSIs. The pathologists were\nblinded to the distribution of positive or negative slides and to each other’s assessments.\n●\nAll slides positive for PNI (n=485) in the STHLM3 development and internal validation\ncohorts were digitally re-assessed as described in (Kartasalo et al., 2022). The slides were\nassessed by the lead pathologist (L.E.) in QuPath (Bankhead et al., 2017) using\nHamamatsu (.ndpi) and Aperio (.svs) WSIs to create pixel-wise annotations of areas of\nPNI.\n●\nA subset of slides positive for PNI (n=106) and a random selection of slides negative for\nPNI (n=106) according to the assessment by L.E. was additionally assessed by an expert\npanel of uropathologists (n=4) as described in (Egevad et al., 2021). The pathologists\nassessed the presence of PNI on slide level on Cytomine using Hamamatsu (.ndpi) and\nAperio (.svs) WSIs. The pathologists were blinded to the distribution of positive or\nnegative slides and to each other’s assessments. The pathologists could also indicate\nuncertain cases with borderline categories.\n7.1.5.\nStavanger University Hospital (SUH)\nThe SUH samples represent consecutive cases collected from routine diagnostics at the\nDepartment of Pathology, Stavanger University Hospital in Stavanger, Norway from December\n20\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n2016 to March 2018. Biopsies were taken at the Department of Urology in Stavanger University\nHospital and other private urological clinics at the Stavanger Urological Center. Patients\nprimarily underwent systematic transrectal biopsies, although some received MRI-targeted\nbiopsies, either alone or combined with systematic biopsy. Slides typically contain two cores\nfrom the same anatomical location, sectioned at two levels. A subset of the SUH cohort has been\nused as an external validation set in previous studies (Ji et al., 2022; Olsson et al., 2022).\n7.1.5.1.\nReference standard protocol\nThe reference standard was obtained from the original pathology reports from the clinical\nroutine. Seven uropathologists and seven general pathologists assessed the slides microscopically\nreporting the GS, ISUP grade, Gleason pattern 4 percentage, cancer extent, biopsy length, PNI,\nfatty tissue infiltration (FTI), and additional stainings (e.g. IHC) on the slide level. The linear\ncancer extent was generally measured from end to end in cases with discontinuous cancer and it\nwas reported on a per-cut level.\nSubsets of the SUH cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=66) from the SUH internal validation cohort was randomly selected\nand stratified by ISUP grade for re-assessment by the lead pathologist (L.E.). This\nre-assessment was conducted digitally on Cytomine using Hamamatsu WSIs (.ndpi) to\nreport the GS per slide.\n●\nA subset of slides (n=332) with Gleason pattern 4 tissue from the SUH development and\ninternal validation cohorts was initially assessed by a uropathologist (A.B.) for potential\ncribriform cancer using QuPath. We then randomly selected at most 90 positive, 30\nborderline and 30 negative slides from the development cohort and at most 30 positive,\n10 borderline and 10 negative slides from the internal validation cohort to be re-assessed\nby the lead pathologist (L.E.), resulting in 200 slides. This re-assessment was conducted\ndigitally on Cytomine using Hamamatsu (.ndpi) WSIs to report cribriform cancer per\nslide. The pathologist could also indicate uncertain cases with a borderline category.\n●\nAll slides from cases reported as positive for PNI in the SUH development and internal\nvalidation cohorts were initially assessed by a uropathologist (A.B.) for potential PNI\nusing a microscope. We then randomly selected at most 25 positive and 5 negative slides\nper ISUP grade from the development cohort, and at most 8 positive and 2 negative slides\nper ISUP grade from the internal validation cohort to be re-assessed by the lead\npathologist (L.E.), resulting in 185 slides. This re-assessment was conducted digitally on\nCytomine using Hamamatsu (.ndpi) WSIs to report PNI per slide. The pathologist could\nalso indicate uncertain cases with a borderline category.\n21\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n7.2.\nExternal validation cohorts\n7.2.1.\nAichi Medical University (AMU)\nThe AMU samples were collected at the Aichi Medical University in Nagakute, Japan from 2020\nto 2023. Samples were selected to include cribriform prostate cancer cases and non-cribriform\ncases. Cribriform cases were chosen sequentially, while non-cribriform cases were selected\namong cases containing Gleason pattern 4 and age-adjusted to match the cribriform cases.\nPatients generally underwent systematic transrectal biopsy, with only a few undergoing\nMRI-targeted biopsy. Slides typically contain several cores, sectioned at several levels.\n7.2.1.1.\nReference standard protocol\nAll cases were assessed by a uropathologist (T.T.) initially using a microscope and then\nconfirmed digitally with the NDP.View software using Hamamatsu WSIs (.ndpi). The presence\nor absence of cribriform prostate cancer was reported on slide level and GS was reported on\npatient level.\n7.2.2.\nAquesta Uropathology morphological subtypes (AQ)\nThe AQ cases were collected at the Aquesta Specialised Uropathology laboratory in Toowong,\nAustralia from 2009 to 2023. The biopsies were performed in private hospitals and urology\nclinics in Queensland state, Australia. Slides were specifically selected to represent rare\nmorphologies such as benign mimickers of prostate cancer which are typically hard to diagnose\nin routine pathology. Patients generally underwent MRI-targeted transrectal biopsies, and each\nslide has two cores, sectioned at two levels.\n7.2.2.1.\nReference standard protocol\nA uropathologist (H.S.) assessed the slides microscopically and reported the GS, ISUP grade,\nadditional stainings (e.g. IHC), and the presence or absence of specific morphological subtype\ncategories\non\nslide\nlevel\n(see\nTable\n5).\nSlides\nrepresenting\nbenign\nmimickers\nwere\nmicroscopically re-assessed by the lead pathologist (L.E.).\n7.2.3.\nAarhus University Hospital (AUH)\nThe AUH samples were part of the PRIMA clinical trial conducted at the Aarhus University\nHospital in Aarhus, Denmark from January 2018 to December 2021 (Fredsøe et al., 2023).\nHistopathology assessment was conducted at the Department of Pathology, Aarhus University\nHospital, Aarhus, Denmark. In this trial, men aged 50-59 years with elevated prostate-specific\n22\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nantigen (PSA) (3-10 ng/ml) and/or positive STHLM3 test (defined as STHLM3 score equal to or\nabove 11%) and MRI of PIRADS 3-5 were referred to MRI-targeted transrectal biopsy. Out of\n117 patients who underwent the biopsy procedure, the pathologist selected slides based on\nhistopathological features with the aim of a uniform distribution of ISUP grades. Slides typically\ncontain two cores, sectioned at three levels. This cohort was used as an external validation set in\na previous study (Ji et al., 2022).\n7.2.3.1.\nReference standard protocol\nAll cases were assessed by a uropathologist (B.P.U.) microscopically and the GS, the ISUP\ngrade, cancer extent and biopsy length were reported on the slide level.\nSubsets of the AUH cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=41) was randomly selected, stratified by the ISUP grade, for\nre-assessment by the lead pathologist (L.E.). This re-assessment was conducted digitally\non Cytomine using Hamamatsu WSIs (.ndpi) to report the GS per slide.\n7.2.4.\nKarolinska University Hospital morphological subtypes (KUH-2)\nThe KUH-2 samples were collected at the Department of Pathology, Karolinska University\nHospital in Solna, Sweden in 2022. The biopsy procedure and number of tissue sections per slide\nadhere to the KUH-1 cohort. Similarly to the AQ cohort, these samples were specifically selected\nto represent cases that are typically challenging to diagnose in clinical practice, such as rare\ndisease morphologies and benign mimickers. This cohort was used as an external validation set\nin a previous study (Olsson et al., 2022).\n7.2.4.1.\nReference standard protocol\nThe reference standard protocol for the KUH-2 cohort adheres to KUH-1, except for additional\nreporting of the presence or absence of specific morphological subtype categories, assessed by\nthe lead pathologist (L.E.) on slide level (see Table 5).\n7.2.5.\nMehiläinen Länsi-Pohja (MLP)\nThe MLP samples represent consecutive cases from routine pathology at the Mehiläinen\nLänsi-Pohja Hospital in Kemi, Finland from 2016 to 2019. Patients underwent systematic\ntransrectal biopsies, and biopsies were sampled based on anatomical location: left and right\ntypically consisting of six cores per location. Slides typically contain one core, sectioned at two\nto three levels.\n23\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n7.2.5.1.\nReference standard protocol\nThe reference standard was obtained from routine assessments done by several pathologists\nusing a microscope to determine the GS, the ISUP grade, cancer extent and biopsy length per\npatient or per anatomical location (i.e. a set of biopsy cores assessed together).\nSubsets of the MLP cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=66) was randomly selected, stratified by the ISUP grade, for\nre-assessment by the lead pathologist (L.E.). The patient level ISUP grade was used for\nstratification, due to missing slide level grading. This re-assessment was conducted\ndigitally on Cytomine using 3DHISTECH WSIs (.mrxs) to report the GS per slide.\n7.2.6.\nMedical University of Lodz (MUL)\nThe MUL samples represent consecutive cases from routine pathology at the 1st Department of\nUrology, University Clinical Hospital of the Military Academy of Medicine - Central Veterans\nHospital, Medical University of Lodz, Lodz, Poland from January 2018 to March 2019.\nHistopathological assessment was conducted at the Department of Pathology, Department of\nOncology, Medical University of Lodz, Lodz, Poland. Patients underwent systematic transrectal\nbiopsy and slides typically contain one core, sectioned at four to seven levels.\n7.2.6.1.\nReference standard protocol\nThe reference standard was determined based on an initial assessment by a single pathologist\n(M.B.) and a second review by a more experienced pathologist (R.K.). Both pathologists have a\nspecialisation in surgical pathology and are currently specialising in uropathology. The\npathologists assessed the cases using a microscope and reported the GS, the ISUP grade, total\ncancer percentage and Gleason pattern 4 and 5 percentages on the slide level.\nSubsets of the MUL cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=66) was randomly selected, stratified by ISUP grade, for\nre-assessment by the lead pathologist (L.E.). This re-assessment was conducted digitally\non Cytomine using Grundium WSIs (.svs) to report the GS per slide.\n●\nAll slides containing Gleason pattern 4 (n=276) were initially assessed for potential\ncribriform cancer by a uropathologist (A.B.). The assessment was conducted digitally on\nCytomine using Grundium WSIs (.svs) to report cribriform cancer per slide and mark the\npositive and borderline foci. All foci were then re-assessed on Cytomine by the lead\npathologist (L.E.).\n24\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n●\nThe slides (n=276) assessed for cribriform cancer were also initially assessed for\npotential PNI by a uropathologist (A.B.). The assessment was conducted on Cytomine\nusing Grundium WSIs (.svs) to report PNI per slide and mark the positive and borderline\nfoci. All foci were then re-assessed on Cytomine by the lead pathologist (L.E.).\n7.2.7.\nSynlab Switzerland (SCH)\nThe SCH samples represent consecutive cases from routine diagnoses at the Argot Laboratory in\nLausanne, Switzerland from January 2020 to December 2020. Patients underwent systematic,\nMRI-targeted or combined transrectal biopsies. Slides typically contain one core, sectioned at\ntwo levels. A varying number of cores were typically obtained from a varying number of\nanatomical locations.\n7.2.7.1.\nReference standard protocol\nThe reference standard was determined based on the pathology reports from routine diagnostics.\nUsing the microscope the pathologists reported the GS, the ISUP grade, cancer extent, biopsy\nlength, Gleason pattern 4 percentage, cribriform cancer, PNI, high-grade prostatic intraepithelial\nneoplasia (HGPIN) and possible IHC staining per anatomical location (i.e. a set of biopsy cores\nassessed together) and per patient.\nSubsets of the SCH cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=72) was randomly selected, stratified by the ISUP grade and\nanatomical location for re-assessment by the lead pathologist (L.E.). This re-assessment\nwas conducted digitally on Cytomine using Philips WSIs (.isyntax converted to .tiff) to\nreport the GS per slide.\n●\nA subset of slides (n=56) were digitally re-assessed for cribriform cancer by a\nuropathologist (H.S.). We selected all positive anatomical locations and a random\nselection of 6 negative anatomical locations with Gleason pattern 4 tissue and included\nall slides from these locations. This re-assessment was conducted digitally on Cytomine\nusing Philips WSIs (.isyntax converted to .tiff) to report cribriform cancer per slide. The\npathologist could also indicate uncertain cases with a borderline category.\n●\nA subset of slides (n=94) were digitally re-assessed for PNI by a uropathologist (B.D.).\nWe randomly selected 12 positive and 5 negative anatomical locations per ISUP grade\nand included all slides from these locations. This re-assessment was conducted digitally\non Cytomine using Philips WSIs (.isyntax converted to .tiff) to report PNI per slide. The\npathologist could also indicate uncertain cases with a borderline category.\n25\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n7.2.8.\nSynlab Finland (SFI)\nThe SFI samples represent consecutive cases from routine diagnostics at the Synlab Laboratory\nin Helsinki, Finland from January 2020 to February 2021. Patients underwent systematic,\nMRI-targeted or combined transrectal biopsies. Slides typically contain two cores, sectioned at\nfive to six levels. A varying number of cores were typically obtained from a varying number of\nanatomical locations.\n7.2.8.1.\nReference standard protocol\nThe reference standard was determined based on the pathology reports from routine diagnostics.\nUsing the microscope the pathologists reported the GS, the ISUP grade, cancer extent, biopsy\nlength, Gleason pattern 4 percentage, cribriform cancer, PNI, HGPIN and possible IHC staining\nper anatomical location (i.e. a set of biopsy cores assessed together) and in some cases per\npatient.\nSubsets of the SFI cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=67) was randomly selected, stratified by the ISUP grade and\nanatomical location for re-assessment by the lead pathologist (L.E.). This re-assessment\nwas conducted digitally on Cytomine using Philips WSIs (.isyntax converted to .tiff) to\nreport the GS per slide.\n7.2.9.\nSynlab France (SFR)\nThe SFR samples represent consecutive cases from routine diagnostics at the Technipath-Synlab\nMedical Laboratory in Dommartin, Rhône, France from September 2020 to December 2020.\nPatients underwent systematic, MRI-targeted or combined transrectal biopsies. Slides usually\ncontain two to three cores from the same anatomical location, sectioned at two levels.\n7.2.9.1.\nReference standard protocol\nThe reference standard was determined based on the pathology reports from routine diagnostics.\nPathologists using a microscope reported the GS, the ISUP grade, cancer extent, biopsy length,\nGleason pattern 4 percentage, cribriform cancer, PNI, HGPIN and possible IHC staining per\nanatomical location (i.e. slide) and in some cases per patient.\nSubsets of the SFR cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=49) was randomly selected, stratified by the ISUP grade and\nanatomical location for re-assessment by the lead pathologist (L.E.). This re-assessment\n26\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nwas conducted digitally on Cytomine using Philips WSIs (.isyntax converted to .tiff) to\nreport the GS per slide.\n7.2.10.\nSpear Prostate Biopsy 2020 (SPROB20)\nThe SPROB20 samples were collected at Uppsala University Hospital, Uppsala, Sweden from\n2015 to 2018. Patients underwent targeted transrectal biopsies. Slides typically contain one core,\nsectioned at one level. This cohort is publicly available at the AIDA Data Hub (Walhagen et al.,\n2020).\n7.2.10.1.\nReference standard protocol\nThe reference standard was obtained from the clinical routine. The pathologists assessed the\nslides microscopically and reported the ISUP grade at the patient level in two ways: as the\nmaximum and as the average of the slide level ISUP grades. The underlying slide-level ISUP\ngrades were not provided on the AIDA Data Hub.\nSubsets of the SPROB20 cohort underwent additional re-assessments as follows:\n●\nA subset of slides (n=50) was randomly selected, stratified by ISUP grade and patient, for\nre-assessment by the lead pathologist (L.E.). This re-assessment was conducted digitally\non Cytomine using Hamamatsu WSIs (.ndpi converted to .tiff) to report the GS per slide.\n7.2.11.\nUniversity Hospital Cologne (UKK)\nThe UKK samples represent consecutive cases from the Institute of Pathology at the University\nHospital\nCologne\nin Cologne, Germany. Patients underwent combined systematic and\nMRI-targeted transrectal biopsies. Slides typically contain one core, sectioned at three levels.\nThe publicly available subset of samples was randomly selected and stratified by the ISUP grade,\nincluding ten samples per ISUP grade. This cohort was obtained from a publicly available\ndataset which was part of the development and validation sets in an earlier study (Tolkach et al.,\n2023). The WSIs were converted from JPEG2000 compressed OME-TIFF format via an\nintermediate raw Zarr format to JPEG compressed (quality 80) generic pyramidal TIFF format\nfor OpenSlide compatibility using the bioformats2raw (v. 0.9.3), raw2ometiff (v. 0.7.1) and\nlibvips (v. 8.9.1) converters.\n7.2.11.1.\nReference standard protocol\nThe reference standard was determined digitally by a panel of 10 different pathologists from\nAustria, Germany, Israel, Japan, the Netherlands, Russia and the United States. All pathologists\nreported the ISUP grade per slide and the final grade was obtained as the majority vote. A\nconsensus was considered reached in cases where the majority ISUP grade had at least six votes.\n27\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n7.2.12.\nHospital Wiener Neustadt (WNS)\nThe WNS samples represent consecutive cases from the Hospital Wiener Neustadt in Wiener\nNeustadt, Austria. Patients underwent combined systematic and MRI-targeted transrectal\nbiopsies. Slides typically contain one core, sectioned at one level. The publicly available subset\nof samples was randomly selected and stratified by the ISUP grade, including ten samples per\nISUP grade. This cohort was obtained from a publicly available dataset which was part of the\ndevelopment and validation sets in an earlier study (Tolkach et al., 2023). The WSIs were\nconverted from JPEG2000 compressed OME-TIFF format via an intermediate raw Zarr format to\nJPEG compressed (quality 80) generic pyramidal TIFF format for OpenSlide compatibility using\nthe bioformats2raw (v. 0.9.3), raw2ometiff (v. 0.7.1) and libvips (v. 8.9.1) converters.\n7.2.12.1.\nReference standard protocol\nThe reference standard was determined digitally by a panel of 11 different pathologists from\nAustria, Germany, Israel, Japan, the Netherlands, Russia and the United States. All pathologists\nreported the ISUP grade per slide and the final grade was obtained as the majority vote. A\nconsensus was considered reached in cases where the majority ISUP grade had at least six votes.\n8.\nStatistical analyses\n8.1.\nOverview of statistical analyses\n8.1.1.\nPrimary analysis: Diagnosis and Gleason scoring\nI.\nInternal and external validation against the original cohort-specific reference standard\nII.\nSubgroup analyses\nA. Evaluate performance across different age groups\nB. Evaluate performance on systematic vs. targeted biopsies\nC. Evaluate performance on non-treated patients vs. patients treated for benign\nprostatic hyperplasia prior to biopsy\nD. Evaluate performance on morphological subtypes\nE. Evaluate performance on cases requiring vs. not requiring IHC staining\nF. Evaluate performance compared to the current state-of-the-art AI systems\nIII.\nSensitivity analyses\nA. Cross-scanner consistency analyses\nB. Compare the AI system vs. individual pathologist panel members\n28\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nC. Internal and external validation against uniform reference standard by the lead\npathologist\nD. Blinded re-assessment of slides with marked errors\n8.1.2.\nSecondary analysis: Cancer extent prediction\nI.\nInternal and external validation against the original cohort-specific reference standards\nII.\nSubgroup analyses\nA. Evaluate performance across different age groups\nB. Evaluate performance on systematic vs. targeted biopsies\nC. Evaluate performance on non-treated patients vs. patients treated for benign\nprostatic hyperplasia prior to biopsy\nIII.\nSensitivity analyses\nA. Cross-scanner consistency analyses\n8.1.3.\nSecondary analysis: Cribriform cancer detection\nI.\nInternal and external validation against the original cohort-specific reference standards\nII.\nSubgroup analyses\nA. Evaluate performance across different age groups\nB. Evaluate performance on systematic vs. targeted biopsies\nC. Evaluate performance on non-treated patients vs. patients treated for benign\nprostatic hyperplasia prior to biopsy\nIII.\nSensitivity analyses\nA. Cross-scanner consistency analyses\nB. Compare the AI system vs. individual pathologist panel members\nE. Re-assessment excluding borderline slides\n8.1.4.\nSecondary analysis: Perineural invasion detection\nI.\nInternal and external validation against the original cohort-specific reference standards\nII.\nSubgroup analyses\nA. Evaluate performance across different age groups\nB. Evaluate performance on systematic vs. targeted biopsies\nC. Evaluate performance on non-treated patients vs. patients treated for benign\nprostatic hyperplasia prior to biopsy\nIII.\nSensitivity analyses\nA. Cross-scanner consistency analyses\nB. Compare the AI system vs. individual pathologist panel members\n29\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nE. Re-assessment excluding borderline slides\n8.1.5.\nExploratory analyses\nI.\nEvaluate visualisations of the AI output\nII.\nEvaluate the impact of tissue segmentation algorithms\nIII.\nEvaluate end-to-end vs. transfer-learning-based models\nIV.\nEvaluate the impact of physical colour calibration\n8.2.\nDetails of statistical analyses\nPrimary analysis: Diagnosis and Gleason scoring\nWe will quantify the concordance of the AI system’s cancer diagnosis (positive/negative),\nGleason score and ISUP grade with the reference standards in the tuning, internal validation and\nexternal validation cohorts using the metrics described below. The analysis will be conducted on\nslide level (AQ, AUH, KUH-1, KUH-2, MUL, RUMC, SFR, STHLM3, SUH, UKK, WNS),\nanatomical location level (MLP, SFI, SCH) and/or patient level (KUH-1, SCH, SFI, SFR,\nSPROB20) depending on the granularity of the available reference standards.\nCancer diagnosis: Sensitivity (true positive rate) and specificity (true negative rate) will be used\nto quantify the agreement of negative/positive diagnosis for prostate cancer with the reference\nstandard. Confidence intervals for sensitivity and specificity will be computed using the\nnon-parametric bootstrap over cases. We will additionally report the Area Under the Receiver\nOperating Characteristics Curve (AUROC) and confusion matrices.\nGleason score: Quadratically weighted Cohen’s kappa (QWK) will be used to quantify the\nagreement of Gleason scoring with the reference standard. In addition, we will also report\nlinearly weighted Cohen’s kappa (LWK) and confusion matrices. To allow calculating weighted\nkappas, Gleason patterns (e.g. 3+4) will be encoded into ordinal variables following earlier\nstudies (Jung et al., 2022; Egevad, Micoli, Delahunt, et al., 2024; Egevad, Micoli, Samaratunga,\net al., 2024) as follows: benign (0), 3+3 (1), 3+4 (2), 4+3 (3), 3+5 (4), 4+4 (5), 5+3 (6), 4+5 (7),\n5+4 (8), 5+5 (9). Confidence intervals will be computed using the non-parametric bootstrap over\ncases.\nISUP grade: Quadratically weighted Cohen’s kappa (QWK) will be used to quantify the\nagreement of the ISUP grade with the reference standard. In addition, we will also report linearly\nweighted Cohen’s kappa (LWK) and confusion matrices. To allow calculating weighted kappas,\n30\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nISUP grades will be treated as ordinal variables (0-5), with benign encoded as 0. Confidence\nintervals will be computed using the non-parametric bootstrap over cases.\nSecondary analysis: Cancer extent prediction\nWe will quantify the concordance of the AI system’s prediction of linear cancer extent expressed\nin millimetres with the reference standards in those tuning, internal validation and external\nvalidation cohorts where a reference standard is available (AUH, KUH-1, STHLM3, SUH, STG,\nMLP, SCH, SFI, SFR). The concordance will be quantified using root mean squared error\n(RMSE). In addition, we will also report Pearson’s linear correlation coefficient, and show\nscatter plots of predicted millimetre cancer length vs. millimetre cancer length reported by the\nreference standard. The analysis will be conducted on slide level (AUH, KUH-1, STHLM3,\nSUH, STG, SFR), anatomical location level (MLP, SFI, SCH) and/or patient level (MLP, SCH,\nSFI, SFR) depending on the granularity of the available reference standards (see Table 3).\nConfidence intervals will be computed using the non-parametric bootstrap over cases.\nSecondary analysis: Cribriform cancer detection\nWe will quantify the concordance of the AI system’s prediction of the presence of cribriform\ncancer with the reference standards in those internal and external validation cohorts where a\nreference standard is available (MUL, SCH, STHLM3, SUH). The tuning set has an insufficient\nnumber of cribriform samples for evaluation and will be included in the training. The\nconcordance will be quantified using unweighted Cohen’s kappa. In addition, we will also report\nAUROC, sensitivity (true positive rate), specificity (true negative rate) and confusion matrices.\nSlides reported as borderline for cribriform cancer will be considered negative. The analysis will\nbe conducted on slide level. Confidence intervals will be computed using the non-parametric\nbootstrap over cases.\nSecondary analysis: Perineural invasion detection\nWe will quantify the concordance of the AI system’s prediction of the presence of perineural\ninvasion with the reference standards in those internal and external validation cohorts where a\nreference standard is available (MUL, SCH, STHLM3, SUH). The tuning set has an insufficient\nnumber of PNI samples for evaluation and will be included in the training. The concordance will\nbe quantified using unweighted Cohen’s kappa. In addition, we will also report AUROC,\nsensitivity (true positive rate), specificity (true negative rate) and confusion matrices. Slides\nreported as borderline for perineural invasion will be considered negative. The analysis will be\nconducted on slide level. Confidence intervals will be computed using the non-parametric\nbootstrap over cases.\n31\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nSubgroup analyses\nSubgroup analysis A: We will measure the performance of the AI system in terms of the\nprimary and secondary objectives across subgroups of patients divided by age. Analysis will be\nconducted on the cohorts where age information can be retrieved (see Table 1) according to the\nage groups: <50, 50 - 59, 60 - 69, and ≥70.\nSubgroup analysis B: We will measure the performance of the AI system in terms of the\nprimary and secondary objectives across subgroups of patients divided by biopsy sampling\ntechnique (systematic vs. targeted vs. combined). The analysis will be conducted on the cohorts\nwhere biopsy sampling technique information can be retrieved.\nSubgroup analysis C: We will measure the performance of the AI system in terms of the\nprimary and secondary objectives across subgroups of patients who were treatment-naive or had\nreceived treatment for benign prostatic hyperplasia (BPH) (using e.g. 5-alpha reductase\ninhibitors) before the biopsy procedure. The analysis will be conducted on the cohorts where\ntreatment information can be retrieved. Some (very few) individuals included in the patient\ncohorts may also have undergone prior prostate cancer treatment (e.g. radiation therapy), but the\nnumber of cases is insufficient for a subgroup analysis.\nSubgroup analysis D: We will measure the performance of the AI system in terms of the\nprimary objective on subgroups of slides representing morphological subtypes of benign and\nmalignant tissue that are usually hard for pathologists to diagnose. We evaluate the performance\nof the AI system in the STHLM3 morphological subtypes internal validation cohort, the KUH-2\nexternal validation cohort and the AQ external and partly external validation cohorts. See Table 5\nfor the distribution of morphological subtypes reported in each cohort. We will evaluate\nperformance in terms of cancer diagnosis and additionally, Gleason scoring, where applicable to\nthe subtype.\nSubgroup analysis E: We will measure the performance of the AI system in terms of the\nprimary objective across subgroups of slides which required IHC staining for confirming the\ndiagnosis and slides which the pathologists could assess without IHC. The analysis will be\nconducted on the cohorts where information on IHC can be retrieved (see Table 6).\nSubgroup analysis F: We will measure the performance of the AI system in terms of the\nprimary objective in comparison to the state-of-the-art algorithms developed in the PANDA\nchallenge (Bulten et al., 2022). The analysis will be conducted on the subgroups of the KUH-1,\n32\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nRUMC and STHLM3 cohorts representing the internal and external validation sets of PANDA.\nFor a fair comparison, we will apply the AI system on the WSIs provided to the challenge\nparticipants, which differ in terms of preprocessing and file format from the underlying original\nWSIs of the KUH-1 and STHLM3 cohorts, which are used in our primary analysis.\nA. We evaluate the performance in the tuning cohort KUH-1 (i.e. PANDA European external\nvalidation set) and compare the AI system with the PANDA challenge algorithms.\nB. We evaluate the performance in the combined PANDA subset of the RUMC and\nSTHLM3 internal validation cohorts (i.e. PANDA internal validation set) and compare\nthe AI system with the PANDA challenge algorithms.\nSensitivity analyses\nSensitivity analysis A: We will evaluate the reproducibility of the AI system’s output in terms of\nthe primary and secondary objectives on WSIs obtained from the same slides on multiple\nscanners. The analysis will be conducted on the STHLM3 tuning and internal validation cohorts\nand the MUL external validation cohort, which contain WSIs rescanned on different scanners\n(see Table 2). In the STHLM3 cohort, a subset of slides (n=287) have been rescanned on five\nscanners: Aperio AT2 DX, Grundium Ocus40, Hamamatsu NanoZoomer 2.0-HT C9600-12,\nHamamatsu NanoZoomer XR C12000-02 and Philips IntelliSite UFS. In the MUL cohort, a\nsubset of slides (n=503) have been rescanned on two scanners: Grundium Ocus40 and Philips\nIntelliSite UFS. We will quantify the reproducibility of the AI predictions across scanners using\nQWK, and LWK and the percentage of slides with discordant predictions for each objective and\neach pair of scanners. We will additionally report confusion matrices.\nSensitivity analysis B: To put the discrepancies between the AI system and the reference\nstandards in the context of inter-observer variation between pathologists, we will quantify\nall-against-all pairwise agreements in panels consisting of pathologists and the AI system.\nFor the primary objective, the analysis will be conducted on subsets of the STHLM3\n(ImageBase) and RUMC (PANDA Radboud) internal validation cohorts and on the full UKK\nand WNS external validation cohorts, which were assessed by a panel of pathologists and have\nper-pathologist grades available in addition to their consensus (see Table 3). For the secondary\nobjectives of cribriform cancer and PNI detection, the analysis will be conducted on subsets of\nthe STHLM3 internal validation cohort, assessed by panels of pathologists (see Table 4).\nWe will calculate the average pairwise agreement (QWK and LWK for the primary objective,\nunweighted Cohen’s kappa for the secondary objectives) for all the pathologists in the panels,\nincluding the AI system, and compare the average AI-pathologist agreement to the average\n33\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\npathologist-pathologist agreement. Confidence intervals will be computed using bootstrapping,\nas detailed before (Egevad et al., 2018).\nSensitivity analysis C: To assess the sensitivity of the results to different pathologists providing\nthe cohort-specific reference standards and to isolate differences in observed AI performance due\nto varying reference standards from those due to imperfect generalisation to different labs and\nscanners, we will repeat the primary analysis using a consistent reference standard. We will\nmeasure the agreement between the AI system and the uniform reference standard set by the lead\npathologist (L.E.) on subsets of the SUH and RUMC internal validation cohorts and the AUH,\nMLP, MUL, SCH, SFI, SFR, and SPROB20 external validation cohorts (see Table 3 for a\nsummary of the re-assessed subsets and Section 7 for details on the case selection for each\ncohort). While the original reference standards were varyingly reported either on the level of\nslides, anatomical locations, or patients, L.E.'s re-assessments are consistently reported on slide\nlevel.\nFurthermore, we will measure the agreement in ISUP grades (QWK and LWK) between the\noriginal reference standards and the lead pathologist on the re-assessed subsets of each cohort. To\nfacilitate this comparison for cohorts with original reference standards provided on anatomical\nlocation or patient level (whereas the grading by L.E. is on slide level), the location or patient\nlevel grading by L.E. will be obtained as the maximum ISUP grade over all slides belonging to a\nlocation or patient.\nSensitivity analysis D: We will perform a sensitivity analysis that involves a re-assessment of\nslides where the AI system committed clinically significant errors by repeating the primary\nanalysis against the updated reference standard. This analysis aims to evaluate what portion of\nclinically significant errors can be attributed to data quality issues, such as mistyped information\nin the reference standard tables, mixed-up slide identifiers, or WSI scanning issues in cases\nwhere the original reference standard was set using a microscope. Significant errors are defined\nas cases where the AI model predicts a slide as benign, but the reference standard indicates ISUP\ngrade ≥2, or conversely the AI predicts a slide as ISUP grade ≥2, but the reference standard\nindicates benign. These slides will be re-assessed by the lead pathologist (L.E.) and/or other\nexperienced uropathologists, blinded to the original reference standard and the AI output. If a\nslide cannot be assessed due to e.g. poor focus, it will be excluded. The evaluation will be\nconducted on the internal and external validation cohorts, on both the full cohorts after updating\nthe reference standards, and on only the updated subsets. Additionally, during this analysis,\npathologists will report whether any of the cases with clinically significant errors represent\nductal adenocarcinoma (DAC). Despite being the second most common subtype of prostate\n34\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\ncancer after acinar adenocarcinoma, DAC only accounts for 0.17% of prostate cancers\n(Ranasinha et al., 2021) and may therefore be challenging for AI to detect due to the limited\namount of training data.\nSensitivity analysis E: We will perform a sensitivity analysis that involves the exclusion of\nsamples reported by the pathologists as “borderline” for cribriform cancer or PNI, followed by\nrepeating the secondary analyses concerning these objectives. Conducting the analysis only on\nsamples indicated as negative or positive will provide an estimate of the AI system’s\nperformance in detecting cribriform cancer and PNI less affected by the uncertainty and\nsubjectivity in the definition of these entities. We will additionally quantify the prevalence of\nborderline diagnoses among slides initially classified as false positives vs. true negatives to\nquantify whether borderline cases are overrepresented among false positives. This would indicate\nthat false positives mainly arise due to uncertainty of the reference standard.\nExploratory analysis: Evaluate visualisations of the AI output\nWe will output visualisations of the AI system’s predictions to highlight areas on each slide\ncontaining different Gleason patterns, cribriform cancer or PNI. The visualisations will be\nassessed qualitatively by the lead pathologist (L.E.) and/or other experienced uropathologists for\nconcordance with their assessments. We may additionally quantify the rate of agreement between\nthe AI system and the pathologists by collecting region annotations to serve as a reference\nstandard, and by calculating the pixel-wise sensitivity, specificity, intersection over union or\nother suitable metrics.\nExploratory analysis: Evaluate the impact of tissue segmentation algorithms\nDetecting tissue from the background to only apply the rest of the analysis on tissue pixels is a\ncommon preprocessing step for most computational pathology algorithms. While this task of\ntissue segmentation may seem trivial, many modern AI algorithms reach such low error rates in\ntheir main task, that any errors in tissue detection can contribute to the overall model\nperformance in a considerable way. In particular, missed tissue poses a risk of false negative\ndiagnoses, if this leads to the exclusion of malignant tissue from the analysis. We will evaluate\nthe effect of tissue segmentation on the overall performance of the AI system in terms of the\nprimary and secondary objectives by comparing two different tissue segmentation algorithms.\nOne of the algorithms represents classical image processing and relies on filtering and\nthresholding the image (Ström et al., 2020). The other algorithm is a trained deep learning based\nsegmentation model. We will apply both algorithms to perform the tissue segmentation during\nmodel training and validation and compare the results on the internal and external validation\ncohorts.\n35\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nExploratory analysis: Evaluate end-to-end vs. transfer-learning-based models\nRecently, so-called foundation models trained in a self-supervised manner on large and\nheterogeneous datasets, have been proposed as generally applicable solutions to diverse tasks in\ncomputational pathology as an alternative to tissue type or task specific models (Chen et al.,\n2024). We aim to compare our end-to-end trained prostate cancer specific model to\ntransfer-learning-based models relying on state-of-the-art foundation models for histopathology.\nWe will apply a suitable foundation model as a feature extractor and train an additional classifier\nto adapt the model to the task of diagnosis and Gleason scoring of prostate biopsies. For this\ntransfer learning step, we will use the same development cohorts as for the end-to-end trained\nmodel. We will then evaluate the model on the same internal and external validation cohorts as\nthe end-to-end trained model for a direct comparison.\nExploratory analysis: Evaluate the impact of physical colour calibration\nVariations in the reproduction of colour across different digital pathology scanners may pose a\nproblem for AI, leading to inconsistent model outputs depending on the scanner used for slide\ndigitisation. A physical calibrant in the form of a spectrophotometrically characterised slide has\nbeen proposed as a means for standardising the colour characteristics of WSIs acquired with\ndifferent scanners (Clarke et al., 2018). We will evaluate the impact of applying physical colour\ncalibration on the performance of the AI model on those internal and external validation cohorts\nwhere the calibrant slide could be scanned on the same scanner as the prostate biopsies to allow\ncalibration.\n8.3.\nConfounding factors\nStatistical confounding, or spurious correlations, in the training and validation data of predictive\nmodels, may lead to “shortcut learning” or so-called “Clever Hans predictors” (Lapuschkin et al.,\n2019), where overly optimistic performance on validation data is seen as the result of the model\ntaking advantage of unintended correlations between some attributes of the data and the correct\nlabels. Such biases are also common in digital pathology datasets (Howard et al., 2021; Schmitt\net al., 2021). We have carefully considered the potential presence of such biases in our cohorts\nand taken the steps described below to mitigate the issue.\nAn important confounding factor is the scanner instruments used for digitising various subsets of\nour data cohorts. Patients in different cohorts and subsets of cohorts have been sampled in\nvarying ways, leading to differences in the compositions of these groups in terms of GS and\nISUP grade distribution. These correlations between specific clinical sites or scanner instruments\n36\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nand the target labels can create biases during training since the model could learn to associate the\nappearance of WSIs obtained from a specific site or with a specific scanner with a higher or\nlower likelihood of a particular diagnostic or grading outcome. If the same bias is present in\nvalidation data, this will lead to overly optimistic results. Conversely, if the bias present in\ntraining data is not present in the validation data, a model relying on these spurious correlations\nwill perform poorly. The main approach we have taken to mitigate the risk of overly optimistic\nvalidation results is relying on fully external validation data. The external validation cohorts\nrepresent patients, clinical sites, laboratories and scanners not present in the training data. This\nminimises the risk of the same spurious correlations appearing in both training and external\nvalidation data. When it comes to discouraging the model from learning any spurious\ncorrelations between laboratories or scanners and the target labels, which could result in\nsuboptimal performance in the absence of these correlations, we will apply a sampling scheme\nwhich removes the correlations between these variables during model training.\nAnother common confounding factor we have identified is markings on the slides. Pathologists\noften place pen marks on the glass slides to indicate cancerous regions. These can lead the AI\nmodel to directly associate the presence of markings with the presence of cancer, or indirectly to\nassociate image quality artefacts such as poor focus caused by the pen marks with a higher\nlikelihood of cancer being present. We have mitigated these issues by 1) Applying tissue\ndetection and masking of background pixels as an image preprocessing step, ensuring that pen\nmarkings adjacent to tissue will not be shown to the model, 2) Washing and rescanning of slides\nwhere pen markings are placed on top of tissue or caused focusing issues, or 3) Excluding slides\nwhere neither of the first two options was possible. The first approach of background masking is\napplied to all the WSIs included in the study. The second approach of washing slides was applied\nto the development cohorts where we had control over the scanning process, namely STHLM3\nand SUH. In the RUMC cohort, we excluded slides with pen marks on the tissue based on the\nfindings of the participants in the PANDA challenge.\n8.4.\nRepresentative sampling\nA key issue in the evaluation of diagnostic tests is how disease prevalence influences estimates\nof statistical measures used to assess the diagnostic performance of the tests. Prevalence is\ngenerally defined as the proportion of individuals in a population who have a particular disease\nat a given time. However, more specifically, the prevalence relates to the datasets used for\nevaluating a diagnostic test.\n37\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nThe positive predictive value (PPV; i.e. the probability that individuals with a positive test result\ntruly have the disease), negative predictive value (NPV; i.e. the probability that individuals with\na negative test result truly do not have the disease), and the Cohen’s kappa statistics are\ninfluenced by the disease prevalence in the datasets used for evaluating the performance of\ndiagnostic tests. As prevalence increases, the PPV of a test also increases; and conversely, NPV\ndecreases with increasing prevalence. This relationship means that in datasets where a disease (or\ndisease subtype) is more common, the test's ability to identify true positives increases and true\nnegatives decreases. Similarly, the disease prevalence and case mix will impact estimates of\nCohen’s kappa.\nIn contrast to PPV, NPV and Cohen’s kappa, sensitivity (also known as true positive rate i.e. the\nability of a test to correctly identify patients with the disease) and specificity (also known as true\nnegative rate i.e. the ability to correctly identify those without the disease) are not affected by\nchanges in prevalence. These measures are intrinsic properties of the test and do not depend on\nhow common the disease is in a population or dataset.\nThe sampling scheme or experimental design impacts the estimated prevalence in a study,\nthereby affecting the diagnostic performance statistics that are sensitive to prevalence. For\nexample, in case-control studies, the prevalence is artificially set by the researcher. In datasets\ncollected for the development of diagnostic AI systems (such as the one described in this\nprotocol), it is common to upsample patients with a disease or disease subtype. If a consecutive\ncase series were used for training an AI system to perform Gleason scoring, a very large set\nwould be required in order to ensure a sufficiently large subsample of e.g. Gleason score 9 and\n10 samples for efficient training. Similarly, convenience sampling, where subjects are selected\nbased on their availability rather than at random or according to a defined study design, can lead\nto a sample with a prevalence rate that does not match the general population. These types of\nexperimental designs and sampling schemes can lead to assessments of PPV, NPV, and Cohen’s\nkappa that do not reflect estimates that would be obtained in a consecutive case series in the\ngeneral population.\nThe impact of prevalence on performance estimates underlines the importance of carefully\nconsidering the design of diagnostic studies. When prevalence is expected to differ, adjustments\nor different interpretations of PPV and NPV may be necessary to avoid misinformative\nconclusions. The data we use for training and evaluation of the AI system is a mixture of\nconvenience samples (AMU, AQ, KUH-2, RUMC, SPROB20, STG) and data representing\nconsecutive clinical cases or another well defined and controlled sampling scheme (AUH,\nKUH-1, MLP, MUL, SCH, SFI, SFR, STHLM3, SUH, UKK, WNS). For the datasets with a\n38\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nknown sampling scheme and experimental design, we can use prior probability shift corrections\nto achieve estimates of PPV, NPV, and Cohen’s kappa on a well defined base population\n(Schölkopf et al., 2012; Heiser, Allikivi and Kull, 2020).\n8.5.\nPower\nWe have not performed formal power (or sample size) calculations. The reason for this is as\nfollows:\n●\nThe central objective of this study is to calculate point estimates of performance (using\nstatistical measures as described above) and their confidence intervals, rather than\nemphasising power to detect a specific effect size (which is more relevant when\ncomparing interventions or diagnoses).\n●\nThis is a retrospective evaluation of AI for prostate pathology. This means that the\nsample size is fixed based on the datasets at hand.\n8.6.\nData quality and label noise\nCollecting and pseudonymising or anonymising clinical and pathology data and associating these\nrecords with the correct WSIs requires a number of steps, each introducing potential sources for\nerror. Our data collection, management and verification process generally followed these steps:\nRetrieval and digitisation of clinical/pathology data: Depending on the data cohort, the\nclinical and pathology data were extracted from existing databases/registries (STHLM3) in\ntabular form, provided in tabular form by the data providing sites (AMU, AQ, AUH, MLP,\nMUL, RUMC, SPROB20, SUH, UKK, WNS) or tabulated manually in-house from pathology\nreports scanned into PDF files (KUH-1, KUH-2, SCH, SFI, SFR, STG). The manual tabulation\nin-house involved human translation of the reports from Finnish (SFI), French (SCH, SFR) and\nSwedish (KUH-1, KUH-2, STG) by trained non-experts fluent in the respective languages.\nPatient identifiers were pseudonymised during the data extraction or tabulation process by each\ndata provider.\nRetrieval and digitisation of slides: Slides were retrieved from the respective archives at each\nsite and scanned with the instruments tabulated in Table 2. Each slide had a label with an\nidentifier and depending on the scanning site, the identifiers were stored either in the form of\nmacro/label images as part of the WSI metadata, automatically detected from QR codes and\nstored as WSI metadata, or manually typed in by the scanner operator when naming the resulting\nWSI files.\n39\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nLinking slides to clinical/pathology data: Depending on the manner in which slide identifiers\nwere stored for each WSI, the linking step involved one of the following approaches. For WSIs,\nwhere the identifier was manually typed into the filename, customised scripts were written in\nPython for each data cohort to parse the filename strings. This involved comparing the parsed\nidentifiers to those present in the clinical/pathology data, and iterative refinements to rectify\nissues such as missing or additional zeros, missing or additional whitespace or other delimiters,\nand discrepancies with the representation of characters not belonging to the Basic Latin (standard\nASCII) set, e.g. Ä or Ö. For WSIs, where the identifier was stored in the form of WSI metadata,\nwe used an in-house developed optical character recognition (OCR) system to extract identifiers\nin a semi-automated manner from the QR-code based metadata items and the macro/label images\nembedded in the WSIs. The system first extracted the QR-code based identifier, if available, or\nperformed OCR using the pytesseract (version 0.3.2) implementation of the Tesseract OCR\nengine (Smith, 2007). The system featured a simple user interface, which presented the\nautomatically detected identifier pre-filled into a text box, alongside the macro/label image of the\nslide. The human operator then had the option of accepting the proposed identifier or correcting\nit manually based on the label image. All identifiers were assessed by trained non-experts using\nthis semi-automated approach.\nRelabeling: Slides and patients were initially labelled independently by each data provider using\npseudonymised identifiers. This poses a risk that the same identifier (e.g. Patient_01) is used by\nmultiple data providers, which would cause ambiguous matches in the final combined dataset. In\norder to minimise this risk and to obtain unique identifiers for each WSI, each slide and each\npatient, we calculated unique MD5 hashes based on the variables below. This step additionally\nprovided another round of pseudonymisation to minimise the risk of any non-pseudonymised\nidentifiers being accidentally used by the data providing sites.\n●\nWSI ID: Filename + scanner serial number + scanning time stamp\n●\nSlide ID: Cohort name + original slide ID\n●\nPatient ID: Cohort name + original patient ID\nVerification: The final dataset covering all the data cohorts is managed internally as a CSV\nspreadsheet, generated and maintained using scripts written in Python relying on pandas\n(Creators The pandas development team; McKinney, 2010). Upon generation and any\nmodifications, the dataset undergoes comprehensive unit testing to ensure correctness,\nimplemented in Python using the unittest framework. A version history of the dataset is retained\nto allow tracing back errors. In summary, the tests used for verification cover the following\naspects. The uniqueness and unambiguity of matches based on the identifiers described above are\nverified. Patient-level variables are tested for consistency across all slides and WSIs from the\n40\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nsame patient, and slide level variables are checked for consistency across multiple WSIs\nrepresenting the same slide. We verify that all variables have valid values, with specific tests for\ncategorical, quantitative, and Boolean variables and test for logical mismatches between\nvariables (e.g. a slide negative for cancer cannot be positive for PNI). We ensure there is no\noverlap\nbetween\npatients\nin\ndifferent\ndevelopment\nvs.\nvalidation\nsplits\nor\nbetween\ncross-validation folds in the development data. Please refer to the Supplementary Appendix\nSection 2 for an extensive list of all the tests.\n9.\nDiscussion\nThis study protocol underscores our dedication to transparency and scientific rigour in\ndeveloping\nAI\nsystems\nfor\nmedical\ndiagnostics.\nThe\nprotocol\noutlines\ndata\ncohorts,\ndevelopment-validation\npartitions,\nperformance\nmetrics\nand\nan\nexperimental\npipeline\nprespecified before any investigations or experiments on the validation datasets have taken place.\nFor each data cohort, we report information on patient characteristics and selection, biopsy\nacquisition, histopathological sample preparation, digitisation, and previous utilisation of the\ncohorts in earlier studies on other AI systems. Furthermore, we report reference standard\nprotocols detailing the variables assessed by pathologists, the level of assessment (pixels, slides,\nanatomical locations or patients), and any additional re-assessments. This comprehensive\ndocumentation of data cohorts facilitates transparency and reproducibility of the research,\ninterpretation of data diversity and representativeness, as well as reliability and integrity of\ndeveloping and validating the AI system. The study results will be submitted for publication\nregardless of whether they are positive, negative or inconclusive in relation to the study\nhypothesis.\nDespite the rigorous design, the study has a number of limitations, which we aim to address in\nfuture revisions of the protocol and in follow-up studies. Firstly, many AI systems, including\nthose developed for diagnostic purposes, often suffer from the under-representation of certain\ndemographic groups in the data used for their development and validation (Garin et al., 2023). In\nthis study as well, we recognise potential biases in patient demographic representation and are\ncommitted to addressing them through additional data collection and subsequent validation\nprocesses. Importantly, while all data cohorts and partitions are predefined, the protocol is\ndesigned to accommodate the addition of new cohorts for development (up until the model\ndesign freeze and initiation of the validation phase) or for validation without altering the initial\npartitions. For example, we are currently collecting validation data from ethnically diverse North\nAmerican (Vigneswaran et al., 2024) and Middle Eastern cohorts. This protocol will be extended\n41\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\naccordingly to support additional retrospective evaluation of the AI system across these and\nother patient populations on a global scale.\nSecondly, reproducible AI performance across different digital pathology scanners would greatly\nfacilitate scalable clinical deployment of AI systems, and we address this question in a\nprespecified cross-scanner consistency analysis, which currently has some limitations. The\nmajority of the scanners used for rescanning slides from the STHLM3 and MUL validation\ncohorts for this analysis were also involved in the digitisation of the development data (except\nthe Grundium Ocus40 scanner). This can potentially lead to optimistic results due to the AI\nmodel having been exposed to the variation seen between these scanners during training.\nNevertheless, it should be noted that all the external validation cohorts described in this protocol\nhave been digitised on scanners not involved in the collection of the AI development data, which\nwill allow us to assess cross-scanner generalisation indirectly. For a direct comparison using the\nexact same set of slides digitised on multiple external scanners (i.e. corresponding to a paired\nstudy design), we are in the process of rescanning slides on additional scanners. This will allow\nus to repeat the analysis using scanners fully external to the AI system in a follow-up study.\nThirdly, the criteria for distinguishing between uropathologists and general pathologists are often\nvague and lack standardised definitions across different countries and hospitals. This may\nintroduce differences when comparing agreement rates between general pathologists and\nuropathologists across different cohorts. Furthermore, there are varying practices in the reporting\nof prostate pathology, for example in terms of measuring cancer extent and summarising Gleason\nscoring results on the patient level. This might introduce additional systematic differences when\nevaluating\nthe\nperformance\nacross\ncohorts,\nwhich\nwe\nhave\nmitigated\nby\nadditional\nre-assessments performed in a consistent manner by the lead pathologist (L.E.). Still, prostate\npathology assessment remains a subjective process and inter- and intra-observer variability\ncannot be fully eliminated from the reference standards.\nThis protocol covers retrospective validation of an AI system for assessing prostate core needle\nbiopsies for four main objectives i.e. prostate cancer diagnosis and grading, cancer extent,\ncribriform cancer and perineural invasion. These objectives are crucial for predicting disease\nprognosis and guiding treatment for prostate cancer patients. However, additional objectives of\nour work on AI for prostate cancer will be added. For example, the diagnostic AI system\ndescribed in this protocol can serve as a foundation model for developing models for direct\nprognostication (based on relevant oncological outcomes, such as time to biochemical recurrence\n(BCR), metastatic disease or prostate cancer death) and treatment prediction, and with further\nrefinements can be adapted to predict additional objectives based on prostate morphology or\n42\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nother use cases in prostate pathology, such as reducing the need for IHC staining (Table 6).\nMoreover, we will perform molecular characterisation (genomic and transcriptomic profiling) of\ntissue samples from diagnostic biopsies, following the same protocol as we use in the ProBio\ntrial for metastatic prostate cancer (Crippa et al., 2020; De Laere et al., 2022). Linked imaging\nand genomic data will be used to develop models to predict clinically important genomic\nalterations and mutations from the morphological data in the WSIs. For example, we will\ndevelop AI models for the prediction of alterations in the BRCA genes; patients with alterations\nin these genes tend to respond well to poly ADP-ribose polymerase (PARP) inhibitors (de Bono\net al., 2020; Chi et al., 2023; Fizazi et al., 2023). Such AI models could in a clinical setting help\nto triage tissue samples for genomic analysis to verify AI predictions, which would reduce costs\nand improve chances of detecting clinically actionable genetic information. We will also use the\ndata presented in this protocol to further develop conformal predictors to detect unreliable AI\npredictions (Olsson et al., 2022). Additional information regarding these objectives will be added\nin future revisions of this protocol (and then noted in the revision history of the document).\nThe importance of relating performance to a well defined population (see Section 8.4) motivates\nprospective evaluation in a clinical trial, which we are currently planning. (The prospective trial\nwill be described and detailed in its own protocol.) Prospective evaluation also enables assessing\naspects relevant to the clinical implementation of AI systems that are not possible to evaluate on\nretrospective data, e.g. user interaction, pathologist-in-the-loop approaches, etc. This planned\nclinical trial will thus evaluate the AI system performance in a real-world clinical setting against\ngold-standard diagnostic practices and provide evidence of its efficacy and reliability for guiding\nclinical decision-making in prostate cancer diagnosis.\n10.\nEthical considerations\nThe study is conducted in agreement with the Helsinki Declaration. The collection of patient\nsamples was approved by the Stockholm regional ethics committee (permits 2012/572-31/1,\n2012/438-31/3, and 2018/845-32), the Swedish Ethical Review Authority (permit 2019-05220),\nand the Regional Committee for Medical and Health Research Ethics (REC) in Western Norway\n(permits REC/Vest 80924, REK 2017/71). Informed consent was provided by the participants in\nthe Swedish dataset. For the other datasets, informed consent was waived by the institutional\nreview board due to the usage of de-identified prostate specimens in a retrospective setting.\n43\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n11.\nAcknowledgements\nA.B. received a grant from the Health Faculty at the University of Stavanger, Norway. B.G.P and\nK.D.S received funding from Innovation Fund Denmark (Grant no. 8114-00014B) for the Danish\nbranch of the NordCaP project. M.R. received funding from the Swedish Research Council and\nthe Swedish Cancer Society. P.R. received funding from the Research Council of Finland (Grant\nno. 341967) and the Cancer Foundation Finland. M.E. received funding from the Swedish\nResearch Council, Swedish Cancer Society, Swedish Prostate Cancer Society, Nordic Cancer\nUnion, Karolinska Institutet, and Region Stockholm. K.K. received funding from the David and\nAstrid\nHägelen\nFoundation,\nInstrumentarium\nScience\nFoundation, KAUTE Foundation,\nKarolinska Institute Research Foundation, Orion Research Foundation and Oskar Huttunen\nFoundation.\nWe want to thank Carin Cavalli-Björkman, Astrid Björklund and Britt-Marie Hune for assistance\nwith scanning and database support. We would also like to thank Simone Weiss for assistance\nwith scanning in Aarhus, and Silja Kavlie Fykse and Desmond Mfua Abono for scanning in\nStavanger. We would like to acknowledge the patients who participated in the STHLM3\ndiagnostic study and the OncoWatch and NordCaP projects and contributed the clinical\ninformation that made this study possible.\nThe computations are possible through the National Academic Infrastructure for Supercomputing\nin Sweden (NAISS) and the Swedish National Infrastructure for Computing (SNIC) at C3SE\npartially funded by the Swedish Research Council through grant agreement no. 2022-06725 and\nno.\n2018-05973, by the supercomputing resource Berzelius provided by the National\nSupercomputer Centre at Linköping University and the Knut and Alice Wallenberg Foundation,\nand by CSC - IT Center for Science, Finland.\n12.\nCompeting interests\nN.M., L.E., K.K. and M.E. are shareholders of Clinsight AB, and M.R. is a co-founder and\nshareholder of Stratipath AB.\n13.\nReferences\n1.\nBankhead, P. et al. (2017) ‘QuPath: Open source software for digital pathology image\nanalysis’, Scientific reports, 7(1), p. 16878.\n44\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n2.\nde Bono, J. et al. (2020) ‘Olaparib for Metastatic Castration-Resistant Prostate Cancer’,\nThe New England journal of medicine, 382(22), pp. 2091–2102.\n3.\nBulten, W. et al. (2020) ‘Automated deep-learning system for Gleason grading of prostate\ncancer using biopsies: a diagnostic study’, The lancet oncology, 21(2), pp. 233–241.\n4.\nBulten, W. et al. (2022) ‘Artificial intelligence for diagnosis and Gleason grading of\nprostate cancer: the PANDA challenge’, Nature medicine, 28(1), pp. 154–163.\n5.\nCampanella, G. et al. (2019) ‘Clinical-grade computational pathology using weakly\nsupervised deep learning on whole slide images’, Nature medicine, 25(8), pp. 1301–1309.\n6.\nChen, R.J. et al. (2024) ‘Towards a general-purpose foundation model for computational\npathology’, Nature medicine, 30(3), pp. 850–862.\n7.\nChi, K.N. et al. (2023) ‘Niraparib and Abiraterone Acetate for Metastatic\nCastration-Resistant Prostate Cancer’, Journal of clinical oncology: official journal of the\nAmerican Society of Clinical Oncology, 41(18), pp. 3339–3351.\n8.\nClarke, E.L. et al. (2018) ‘Development of a novel tissue‐mimicking color calibration\nslide for digital microscopy’, Color research and application, 43(2), pp. 184–197.\n9.\nCollins, G.S. et al. (2021) ‘Protocol for development of a reporting guideline\n(TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic\nprediction model studies based on artificial intelligence’, BMJ open, 11(7), p. e048008.\n10.\nCollins, G.S. et al. (2024) ‘TRIPOD+AI statement: updated guidance for reporting\nclinical prediction models that use regression or machine learning methods’, BMJ , 385.\nAvailable at: https://doi.org/10.1136/bmj-2023-078378.\n11.\nCreators The pandas development team pandas-dev/pandas: Pandas. Available at:\nhttps://doi.org/10.5281/zenodo.10957263.\n12.\nCrippa, A. et al. (2020) ‘The ProBio trial: molecular biomarkers for advancing\npersonalized treatment decision in patients with metastatic castration-resistant prostate\ncancer’, Trials, 21(1), p. 579.\n13.\nCruz Rivera, S. et al. (2020) ‘Guidelines for clinical trial protocols for interventions\ninvolving artificial intelligence: the SPIRIT-AI extension’, The Lancet. Digital health,\n2(10), pp. e549–e560.\n14.\nDe Laere, B. et al. (2022) ‘Clinical Trial Protocol for ProBio: An Outcome-adaptive and\nRandomised Multiarm Biomarker-driven Study in Patients with Metastatic Prostate\nCancer’, European urology focus, 8(6), pp. 1617–1621.\n15.\nDuenweg, S.R. et al. (2023) ‘Whole slide imaging (WSI) scanner differences influence\noptical and computed properties of digitized prostate cancer histology’, Journal of\npathology informatics, 14, p. 100321.\n16.\nEgevad, L. et al. (2013) ‘Standardization of Gleason grading among 337 European\npathologists’, Histopathology, 62(2), pp. 247–256.\n17.\nEgevad, L. et al. (2017) ‘Pathology Imagebase-a reference image database for\nstandardization of pathology’, Histopathology, 71(5), pp. 677–685.\n18.\nEgevad, L. et al. (2018) ‘Utility of Pathology Imagebase for standardisation of prostate\ncancer grading’, Histopathology, 73(1), pp. 8–18.\n19.\nEgevad, L. et al. (2021) ‘Interobserver reproducibility of perineural invasion of prostatic\nadenocarcinoma in needle biopsies’, Virchows Archiv: an international journal of\n45\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\npathology, 478(6), pp. 1109–1116.\n20.\nEgevad, L. et al. (2023) ‘Interobserver reproducibility of cribriform cancer in prostate\nneedle biopsies and validation of International Society of Urological Pathology criteria’,\nHistopathology, 82(6), pp. 837–845.\n21.\nEgevad, L., Micoli, C., Delahunt, B., et al. (2024) ‘Prognosis of Gleason score 8 prostatic\nadenocarcinoma in needle biopsies: a nationwide population-based study’, Virchows\nArchiv: an international journal of pathology [Preprint]. Available at:\nhttps://doi.org/10.1007/s00428-024-03810-y.\n22.\nEgevad, L., Micoli, C., Samaratunga, H., et al. (2024) ‘Prognosis of Gleason Score 9–10\nProstatic Adenocarcinoma in Needle Biopsies: A Nationwide Population-based Study’,\nEuropean Urology Oncology, 7(2), pp. 213–221.\n23.\nEpstein, J.I. et al. (2005) ‘The 2005 International Society of Urological Pathology (ISUP)\nConsensus Conference on Gleason Grading of Prostatic Carcinoma’, The American\njournal of surgical pathology, 29(9), pp. 1228–1242.\n24.\nEpstein, J.I. et al. (2016) ‘The 2014 International Society of Urological Pathology (ISUP)\nConsensus Conference on Gleason Grading of Prostatic Carcinoma: Definition of\nGrading Patterns and Proposal for a New Grading System’, The American journal of\nsurgical pathology, 40(2), pp. 244–252.\n25.\nFizazi, K. et al. (2023) ‘Rucaparib or Physician’s Choice in Metastatic Prostate Cancer’,\nThe New England journal of medicine, 388(8), pp. 719–732.\n26.\nFredsøe, J. et al. (2023) ‘Results from the PRIMA Trial: Comparison of the STHLM3\nTest and Prostate-specific Antigen in General Practice for Detection of Prostate Cancer in\na Biopsy-naïve Population’, European Urology Oncology, 6(5), pp. 484–492.\n27.\nGarin, S.P. et al. (2023) ‘Medical imaging data science competitions should report dataset\ndemographics and evaluate for bias’, Nature medicine, 29(5), pp. 1038–1039.\n28.\nGleason, D.F. (1992) ‘Histologic grading of prostate cancer: a perspective’, Human\npathology, 23(3), pp. 273–279.\n29.\nGoode, A. et al. (2013) ‘OpenSlide: A vendor-neutral software foundation for digital\npathology’, Journal of pathology informatics, 4. Available at:\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/pmc3815078/.\n30.\nGrönberg, H. et al. (2015) ‘Prostate cancer screening in men aged 50-69 years\n(STHLM3): a prospective population-based diagnostic study’, The lancet oncology,\n16(16), pp. 1667–1676.\n31.\nHeiser, T.J.T., Allikivi, M.-L. and Kull, M. (2020) ‘Shift Happens: Adjusting Classifiers’,\nin Machine Learning and Knowledge Discovery in Databases. Springer International\nPublishing, pp. 55–70.\n32.\nHoward, F.M. et al. (2021) ‘The impact of site-specific digital histology signatures on\ndeep learning model accuracy and bias’, Nature communications, 12(1), p. 4423.\n33.\nJi, E. (2005) ‘ISUP Grading Committee. The 2005 International Society of Urological\nPathology (ISUP) Consensus Conference on Gleason Grading of Prostatic Carcinoma’,\nThe American journal of surgical pathology, 29, pp. 1228–1242.\n34.\nJi, X. et al. (2023) ‘Physical Color Calibration of Digital Pathology Scanners for Robust\nArtificial Intelligence Assisted Cancer Diagnosis’, arXiv [q-bio.QM]. Available at:\n46\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nhttp://arxiv.org/abs/2307.05519.\n35.\nJung, M. et al. (2022) ‘Artificial intelligence system shows performance at the level of\nuropathologists for the detection and grading of prostate cancer in core needle biopsy: an\nindependent external validation study’, Modern pathology: an official journal of the\nUnited States and Canadian Academy of Pathology, Inc, 35(10), pp. 1449–1457.\n36.\nKartasalo, K. et al. (2022) ‘Detection of perineural invasion in prostate needle biopsies\nwith deep neural networks’, Virchows Archiv: an international journal of pathology,\n481(1), pp. 73–82.\n37.\nKleppe, A. et al. (2021) ‘Designing deep learning studies in cancer diagnostics’, Nature\nreviews. Cancer, 21(3), pp. 199–211.\n38.\nKweldam, C.F. et al. (2016) ‘Gleason grade 4 prostate adenocarcinoma patterns: an\ninterobserver agreement study among genitourinary pathologists’, Histopathology, 69(3),\npp. 441–449.\n39.\nLapuschkin, S. et al. (2019) ‘Unmasking Clever Hans predictors and assessing what\nmachines really learn’, Nature communications, 10(1), p. 1096.\n40.\nLiu, X. et al. (2020) ‘Reporting guidelines for clinical trial reports for interventions\ninvolving artificial intelligence: the CONSORT-AI extension’, The Lancet. Digital\nhealth, 2(10), pp. e537–e548.\n41.\nMarée, R. et al. (2016) ‘Collaborative analysis of multi-gigapixel imaging data using\nCytomine’, Bioinformatics , 32(9), pp. 1395–1401.\n42.\nMcGenity, C., Bossuyt, P. and Treanor, D. (2022) ‘Reporting of Artificial Intelligence\nDiagnostic Accuracy Studies in Pathology Abstracts: Compliance with STARD for\nAbstracts Guidelines’, Journal of pathology informatics, 13, p. 100091.\n43.\nMcKinney, W. (2010) ‘Data Structures for Statistical Computing in Python’, in\nProceedings of the 9th Python in Science Conference. Python in Science Conference,\nSciPy. Available at: https://doi.org/10.25080/majora-92bf1922-00a.\n44.\nMelia, J. et al. (2006) ‘A UK-based investigation of inter- and intra-observer\nreproducibility of Gleason grading of prostatic biopsies’, Histopathology, 48(6), pp.\n644–654.\n45.\nMongan, J., Moy, L. and Kahn, C.E., Jr (2020) ‘Checklist for Artificial Intelligence in\nMedical Imaging (CLAIM): A Guide for Authors and Reviewers’, Radiology. Artificial\nintelligence, 2(2), p. e200029.\n46.\nMulliqi, N. et al. (2021) ‘OpenPhi: An interface to access Philips iSyntax whole slide\nimages for computational pathology’, Bioinformatics [Preprint]. Available at:\nhttps://doi.org/10.1093/bioinformatics/btab578.\n47.\nNagendran, M. et al. (2020) ‘Artificial intelligence versus clinicians: systematic review\nof design, reporting standards, and claims of deep learning studies’, BMJ , 368, p. m689.\n48.\nOlsson, H. et al. (2022) ‘Estimating diagnostic uncertainty in artificial intelligence\nassisted pathology using conformal prediction’, Nature communications, 13(1), p. 7761.\n49.\nOzkan, T.A. et al. (2016) ‘Interobserver variability in Gleason histological grading of\nprostate cancer’, Scandinavian journal of urology, 50(6), pp. 420–424.\n50.\nPantanowitz, L. et al. (2018) ‘Twenty years of digital pathology: An overview of the road\ntravelled, what is on the horizon, and the emergence of vendor-neutral archives’, Journal\n47\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nof pathology informatics, 9(1), p. 40.\n51.\nPark, S.H. and Han, K. (2018) ‘Methodologic Guide for Evaluating Clinical Performance\nand Effect of Artificial Intelligence Technology for Medical Diagnosis and Prediction’,\nRadiology, 286(3), pp. 800–809.\n52.\nRanasinha, N. et al. (2021) ‘Ductal adenocarcinoma of the prostate: A systematic review\nand meta-analysis of incidence, presentation, prognosis, and management’, BJUI\ncompass, 2(1), pp. 13–23.\n53.\nSchmitt, M. et al. (2021) ‘Hidden Variables in Deep Learning Digital Pathology and\nTheir Potential to Cause Batch Effects: Prediction Model Study’, Journal of medical\nInternet research, 23(2), p. e23436.\n54.\nSchölkopf, B. et al. (2012) ‘On causal and anticausal learning’, in Proceedings of the\n29th International Coference on International Conference on Machine Learning.\nMadison, WI, USA: Omnipress (ICML’12), pp. 459–466.\n55.\nSmith, R. (2007) ‘An Overview of the Tesseract OCR Engine’, in Ninth International\nConference on Document Analysis and Recognition (ICDAR 2007). IEEE, pp. 629–633.\n56.\nSounderajah, V. et al. (2021) ‘Developing a reporting guideline for artificial\nintelligence-centred diagnostic test accuracy studies: the STARD-AI protocol’, BMJ\nopen, 11(6), p. e047709.\n57.\nStröm, P. et al. (2020) ‘Artificial intelligence for diagnosis and grading of prostate cancer\nin biopsies: a population-based, diagnostic study’, The lancet oncology, 21(2), pp.\n222–232.\n58.\nSwiderska-Chadaj, Z. et al. (2020) ‘Impact of rescanning and normalization on\nconvolutional neural network performance in multi-center, whole-slide classification of\nprostate cancer’, Scientific reports, 10(1), p. 14398.\n59.\nTejani, A.S. et al. (2023) ‘Updating the Checklist for Artificial Intelligence in Medical\nImaging (CLAIM) for reporting AI research’, Nature Machine Intelligence, 5(9), pp.\n950–951.\n60.\nTolkach, Y. et al. (2023) ‘An international multi-institutional validation study of the\nalgorithm for prostate cancer detection and Gleason grading’, NPJ precision oncology,\n7(1), p. 77.\n61.\nVaroquaux, G. and Cheplygina, V. (2022) ‘Machine learning for medical imaging:\nmethodological failures and recommendations for the future’, NPJ digital medicine, 5(1),\np. 48.\n62.\nVasey, B. et al. (2022) ‘Reporting guideline for the early-stage clinical evaluation of\ndecision support systems driven by artificial intelligence: DECIDE-AI’, Nature medicine,\n28(5), pp. 924–933.\n63.\nVigneswaran, H.T. et al. (2024) ‘Stockholm3 validation in a multi-ethnic cohort for\nprostate cancer (SEPTA) detection: A multicentered, prospective trial’, Journal of\nclinical orthodontics: JCO, 42(4_suppl), pp. 262–262.\n64.\nWalhagen, P. et al. (2020) ‘Spear Prostate Biopsy 2020 (SPROB20)’. AIDA. Available at:\nhttps://datahub.aida.scilifelab.se/10.23698/aida/sprob20 (Accessed: 4 March 2024).\n65.\nWHO Classification of Tumours Editorial Board and International Agency for Research\non Cancer (2022) Urinary and Male Genital Tumours. WHO Classification of Tumours.\n48\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n66.\nWillemink, M.J. et al. (2020) ‘Preparing Medical Imaging Data for Machine Learning’,\nRadiology, 295(1), pp. 4–15.\n49\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n14.\nFigures and tables\n50\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nFigure 1. Overview of the study design. The study design has two main steps: (top) The\ndevelopment phase involves model design optimisation through an iterative process of\nexperiments. In each experiment, the model is trained and its performance is evaluated on the\ndevelopment set using cross-validation and on a separate tuning set. (bottom) The validation\nphase is initiated with a design freeze, after which no further changes to the model take place.\nValidation comprises the assessment on the internal data (i.e. collected from the same laboratory\nand/or using the same scanner as development data) and the external data (i.e. collected from\nother laboratories using other scanners than any of the development data). This figure was\ncreated with BioRender.\n51\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nTable 1. Patient clinical and pathological characteristics. Patient and slide level information\nfor the development, tuning, internal, and external validation cohorts including age, PSA, ISUP\ngrade and cancer length distributions. Averaged age and PSA are shown for patients who\nunderwent multiple biopsies. The ISUP distributions are based on the initial, original grading\nexcluding any re-assessments. For the AMU, MLP, SCH, SFI, SFR and SPROB20 cohorts,\nwhere pathology reporting was performed on anatomical location or patient level, the total\nsummed numbers of slides associated with a given ISUP grade or cancer length are shown. The\nAUH cohort has an age range of 50.4 to 69.9 yrs (mean 63.2 yrs, median 64.0 yrs) and a PSA\nrange of 1.5 ng/mL to 9.8 ng/mL (mean 4.6 ng/mL, median 4.2 ng/mL). The SPROB20 cohort\nhas an age range of 39 to 79 yrs (median 67 yrs). Slides in the AQ, KUH-2 and SUH cohorts\nmissing\nISUP\ngrade\ninformation\nrepresent\nnon-gradable\nmorphological\nvariants.\nPSA=prostate-specific\nantigen,\nISUP=International\nSociety\nof\nUrological\nPathology,\nSTHLM3=Stockholm3, SUH=Stavanger University Hospital, RUMC=Radboud University\nMedical Center, STG=Capio S:t Göran Hospital, KUH-1=Karolinska University Hospital,\nAMU=Aichi\nMedical\nUniversity,\nAQ=Aquesta\nUropathology,\nAUH=Aarhus\nUniversity\nHospital, KUH-2=Karolinska University Hospital morphological subtypes, MLP=Mehiläinen\nLänsi-Pohja, MUL=Medical University of Lodz, SCH=Synlab Switzerland, SFI=Synlab Finland,\nSFR=Synlab France, SPROB20=Spear Prostate Biopsy 2020, UKK=University Hospital\nCologne, WNS=Hospital Wiener Neustadt.\nDevelopment\nSTHLM3\nSUH\nRUMC\nSTG\nNo. participants (%)\nn=2,711\nn=710\nn=976\nn=70\nAge, years\n<=49 yrs\n4 (0.14)\n13 (1.83)\n/\n0 (0.0)\n50 - 54 yrs\n216 (7.96)\n35 (4.92)\n1 (1.42)\n55 - 59 yrs\n429 (15.82)\n94 (13.23)\n2 (2.85)\n60 - 64 yrs\n702 (25.89)\n137 (19.29)\n4 (5.71)\n65 - 69 yrs\n1,207 (44.52)\n191 (26.90)\n6 (8.57)\n>= 70 yrs\n153 (5.64)\n240 (33.80)\n37 (52.85)\nMissing\n0 (0.0)\n0 (0.0)\n20 (28.6)\nProstate-specific antigen\n<3 ng/mL\n611 (22.53)\n60 (8.45)\n/\n2 (2.85)\n3 - <5 ng/mL\n1,306 (48.17)\n135 (19.01)\n1 (1.42)\n5 - <10 ng/mL\n592 (21.83)\n350 (49.29)\n6 (8.57)\n>= 10 ng/mL\n202 (7.45)\n163 (22.95)\n38 (54.28)\n52\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nMissing\n0 (0.0)\n2 (0.28)\n23 (32.85)\nNo. slides (%)\nn=29,536\nn=4,606\nn=4,564\nn=247\nCancer length\nNo cancer\n23,530 (79.67)\n3,435 (74.57)\n/\n1 (0.40)\n>0 - 1 mm\n2,021 (6.84)\n238 (5.16)\n7 (2.83)\n>1 - 5 mm\n2,577 (8.72)\n405 (8.78)\n42 (17.00)\n>5 - 10 mm\n1,054 (3.56)\n226 (4.90)\n86 (34.81)\n>10 mm\n354 (1.19)\n300 (6.51)\n111 (44.93)\nMissing\n0 (0.0)\n2 (0.04)\n0 (0.0)\nCancer grade\nBenign\n23,530 (79.67)\n3,435 (74.57)\n912 (19.98)\n1 (0.40)\nISUP 1 (3+3)\n3,571 (12.09)\n683 (14.82)\n731 (16.01)\n1 (0.40)\nISUP 2 (3+4)\n1,265 (4.28)\n240 (5.20)\n594 (13.01)\n1 (0.40)\nISUP 3 (4+3)\n494 (1.67)\n129 (2.79)\n800 (17.52)\n2 (0.80)\nISUP 4 (4+4, 3+5, 5+3)\n377 (1.28)\n54 (1.17)\n668 (14.63)\n32 (12.95)\nISUP 5 (4+5, 5+4, 5+5)\n299 (1.01)\n63 (1.36)\n859 (18.82)\n210 (85.02)\nMissing\n0 (0.0)\n2 (0.04)\n0 (0.0)\n0 (0.0)\nInternal validation\nSTHLM3\nSUH\nRUMC\nNo. participants (%)\nn=654\nn=178\nn=172\nAge, years\n<=49 yrs\n3 (0.45)\n1 (0.56)\n/\n50 - 54 yrs\n58 (8.86)\n6 (3.37)\n55 - 59 yrs\n96 (14.67)\n15 (8.42)\n60 - 64 yrs\n182 (27.82)\n39 (21.91)\n65 - 69 yrs\n289 (44.18)\n46 (25.84)\n>= 70 yrs\n26 (3.97)\n71 (39.88)\nMissing\n0 (0.0)\n0 (0.0)\nProstate-specific antigen\n<3 ng/mL\n123 (18.80)\n12 (6.74)\n/\n3 - <5 ng/mL\n321 (49.08)\n23 (12.92)\n53\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n5 - <10 ng/mL\n153 (23.39)\n91 (51.12)\n>= 10 ng/mL\n57 (8.71)\n52 (29.21)\nMissing\n0 (0.0)\n0 (0.0)\nNo. slides (%)\nn=7,036\nn=1,156\nn=516\nCancer length\nNo cancer\n5,098 (72.45)\n736 (63.70)\n/\n>0 - 1 mm\n583 (8.28)\n52 (4.48)\n>1 - 5 mm\n767 (10.90)\n109 (9.48)\n>5 - 10 mm\n434 (6.16)\n87 (7.50)\n>10 mm\n154 (2.18)\n172 (14.82)\nMissing\n0 (0.0)\n0 (0.0)\nCancer grade\nBenign\n5,098 (72.46)\n736 (63.66)\n195 (37.79)\nISUP 1 (3+3)\n958 (13.62)\n153 (13.23)\n87 (16.86)\nISUP 2 (3+4)\n380 (5.40)\n76 (6.55)\n45 (8.72)\nISUP 3 (4+3)\n240 (3.41)\n74 (6.37)\n77 (14.92)\nISUP 4 (4+4, 3+5, 5+3)\n203 (2.89)\n53 (4.56)\n54 (10.46)\nISUP 5 (4+5, 5+4, 5+5)\n157 (2.23)\n64 (5.51)\n58 (11.24)\nMissing\n0 (0.0)\n0 (0.0)\n0 (0.0)\nTuning\nSTHLM3\nKUH-1\nRUMC\nNo. participants (%)\nn=24\nn=73\nn=72\nAge, years\n<=49 yrs\n0 (0.0)\n2 (2.73)\n/\n50 - 54 yrs\n1 (4.16)\n5 (6.84)\n55 - 59 yrs\n2 (8.33)\n10 (13.69)\n60 - 64 yrs\n8 (33.33)\n12 (16.43)\n65 - 69 yrs\n13 (54.16)\n15 (20.54)\n>= 70 yrs\n0 (0.0)\n29 (39.72)\nMissing\n0 (0.0)\n0 (0.0)\nProstate-specific antigen\n54\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n<3 ng/mL\n3 (12.50)\n/\n/\n3 - <5 ng/mL\n12 (50.00)\n5 - <10 ng/mL\n5 (20.83)\n>= 10 ng/mL\n4 (16.66)\nMissing\n0 (0.0)\nNo. slides (%)\nn=276\nn=330\nn=195\nCancer length\nNo cancer\n192 (69.57)\n108 (32.72)\n/\n>0 - 1 mm\n32 (11.59)\n33 (10.00)\n>1 - 5 mm\n27 (9.67)\n77 (23.33)\n>5 - 10 mm\n16 (5.73)\n75 (22.72)\n>10 mm\n9 (3.22)\n37 (11.21)\nMissing\n0 (0.0)\n0 (0.0)\nCancer grade\nBenign\n192 (69.57)\n108 (32.72)\n95 (48.72)\nISUP 1 (3+3)\n28 (10.14)\n65 (19.70)\n24 (12.31)\nISUP 2 (3+4)\n18 (6.52)\n63 (19.09)\n15 (7.69)\nISUP 3 (4+3)\n13 (4.71)\n49 (14.85)\n15 (7.69)\nISUP 4 (4+4, 3+5, 5+3)\n13 (4.71)\n19 (5.76)\n19 (9.74)\nISUP 5 (4+5, 5+4, 5+5)\n12 (4.35)\n26 (7.88)\n27 (13.85)\nMissing\n0 (0.0)\n0 (0.0)\n0 (0.0)\nExternal validation\nAMU\nAQ\nAUH\nNo. participants (%)\nn=43\nn=135\nn=42\nAge, years\n<= 49 yrs\n/\n/\n/\n50 - 54 yrs\n55 - 59 yrs\n60 - 64 yrs\n65 - 69 yrs\n>= 70 yrs\n55\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nMissing\nProstate-specific antigen\n<3 ng/mL\n1 (2.32)\n/\n/\n3 - <5 ng/mL\n1 (2.32)\n5 - <10 ng/mL\n11 (25.58)\n>= 10 ng/mL\n30 (69.76)\nMissing\n0 (0.0)\nNo. slides (%)\nn=73\nn=136\nn=102\nCancer length\nNo cancer\n/\n/\n43 (42.15)\n>0 - 1 mm\n5 (4.90)\n>1 - 5 mm\n18 (17.64)\n>5 - 10 mm\n24 (23.52)\n>10 mm\n12 (11.76)\nMissing\n0 (0.0)\nCancer grade\nBenign\n0 (0.0)\n122 (89.70)\n43 (42.15)\nISUP 1 (3+3)\n0 (0.0)\n1 (0.73)\n26 (25.49)\nISUP 2 (3+4)\n0 (0.0)\n1 (0.73)\n25 (24.50)\nISUP 3 (4+3)\n6 (8.21)\n0 (0.00)\n1 (0.98)\nISUP 4 (4+4, 3+5, 5+3)\n22 (28.76)\n0 (0.00)\n7 (6.86)\nISUP 5 (4+5, 5+4, 5+5)\n45 (60.27)\n1 (0.73)\n0 (0.0)\nMissing\n0 (0.0)\n11 (8.08)\n0 (0.0)\nExternal validation\nKUH-2\nMLP\nMUL\nNo. participants (%)\nn=89\nn=199\nn=207\nAge, years\n<=49 yrs\n/\n/\n2 (0.96)\n50 - 54 yrs\n4 (1.93)\n55 - 59 yrs\n10 (4.83)\n60 - 64 yrs\n29 (14.00)\n56\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n65 - 69 yrs\n50 (24.15)\n>= 70 yrs\n108 (52.17)\nMissing\n4 (1.96)\nProstate-specific antigen\n<3 ng/mL\n/\n19 (9.54)\n/\n3 - <5 ng/mL\n26 (13.06)\n5 - <10 ng/mL\n65 (32.66)\n>= 10 ng/mL\n85 (42.71)\nMissing\n4 (2.03)\nNo. slides (%)\nn=146\nn=1,964\nn=1,959\nCancer length\nNo cancer\n/\n302 (15.37)\n/\n>0 - 1 mm\n24 (1.22)\n>1 - 5 mm\n207 (10.53)\n>5 - 10 mm\n191 (9.72)\n>10 mm\n1,189 (60.53)\nMissing\n54 (2.63)\nCancer grade\nBenign\n103 (70.54)\n323 (16.44)\n1,483 (75.70)\nISUP 1 (3+3)\n34 (23.28)\n433 (22.04)\n161 (8.21)\nISUP 2 (3+4)\n5 (3.42)\n506 (25.76)\n58 (2.96)\nISUP 3 (4+3)\n0 (0.0)\n216 (10.99)\n74 (3.77)\nISUP 4 (4+4, 3+5, 5+3)\n0 (0.0)\n133 (6.77)\n65 (3.31)\nISUP 5 (4+5, 5+4, 5+5)\n0 (0.0)\n353 (17.97)\n118 (6.02)\nMissing\n4 (2.73)\n0 (0.0)\n0 (0.0)\nExternal validation\nSCH\nSFI\nSFR\nNo. participants (%)\nn=199\nn=99\nn=84\nAge, years\n<=49 yrs\n3 (1.50)\n/\n1 (1.19)\n50 - 54 yrs\n3 (1.50)\n5 (5.95)\n57\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n55 - 59 yrs\n22 (11.05)\n11 (13.09)\n60 - 64 yrs\n27 (13.56)\n11 (13.09)\n65 - 69 yrs\n46 (23.11)\n2 (2.0)\n21 (25.00)\n>= 70 yrs\n98 (49.24)\n3 (3.03)\n35 (41.66)\nMissing\n0 (0.0)\n94 (94.97)\n0 (0.0)\nProstate-specific antigen\nLow\n0 (0.0)\n2 (2.02)\n0 (0.0)\nNormal\n0 (0.0)\n2 (2.02)\n0 (0.0)\nElevated\n19 (9.54)\n8 (8.08)\n0 (0.0)\n<3 ng/mL\n3 (1.50)\n2 (2.02)\n1 (1.35)\n3 - <5 ng/mL\n21 (10.55)\n8 (8.08)\n6 (7.14)\n5 - <10 ng/mL\n45 (22.61)\n39 (39.39)\n51 (60.71)\n>= 10 ng/mL\n39 (19.59)\n32 (32.32)\n16 (19.04)\nMissing\n72 (36.18)\n6 (6.06)\n10 (11.75)\nNo. slides (%)\nn=2,434\nn=537\nn=515\nCancer length\nNo cancer\n1,580 (64.91)\n311 (57.91)\n373 (72.42)\n>0 - 1 mm\n22 (0.90)\n16 (2.97)\n1 (0.19)\n>1 - 5 mm\n156 (6.39)\n39 (7.26)\n34 (6.60)\n>5 - 10 mm\n88 (3.60)\n30 (5.58)\n32 (6.21)\n>10 mm\n565 (23.27)\n54 (10.05)\n69 (13.39)\nMissing\n23 (0.94)\n87 (16.42)\n6 (0.97)\nCancer grade\nBenign\n1,580 (64.91)\n311 (57.91)\n373 (72.42)\nISUP 1 (3+3)\n325 (13.31)\n61 (11.35)\n87 (16.89)\nISUP 2 (3+4)\n201 (8.25)\n51 (9.49)\n28 (5.43)\nISUP 3 (4+3)\n183 (7.51)\n50 (9.31)\n3 (0.58)\nISUP 4 (4+4, 3+5, 5+3)\n94 (3.86)\n16 (2.97)\n10 (1.94)\nISUP 5 (4+5, 5+4, 5+5)\n47 (1.93)\n30 (5.58)\n6 (1.16)\nMissing\n4 (0.16)\n18 (3.39)\n8 (1.55)\n58\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nExternal validation\nSPROB20\nUKK\nWNS\nNo. participants (%)\nn=452\nn=50\nn=50\nAge, years\n<=49 yrs\n/\n/\n/\n50 - 54 yrs\n55 - 59 yrs\n60 - 64 yrs\n65 - 69 yrs\n>= 70 yrs\nMissing\nProstate-specific antigen\n<3 ng/mL\n13 (2.87)\n/\n/\n3 - <5 ng/mL\n14 (3.09)\n5 - <10 ng/mL\n30 (6.63)\n>= 10 ng/mL\n190 (42.03)\nMissing\n205 (45.35)\nNo. slides (%)\nn=2,570\nn=50\nn=50\nNo cancer\n/\n/\n/\n>0 - 1 mm\n>1 - 5 mm\n>5 - 10 mm\n>10 mm\nMissing\nCancer grade\nBenign\n950 (36.96)\n0 (0.0)\n0 (0.0)\nISUP 1 (3+3)\n543 (21.12)\n12 (24.0)\n10 (20.0)\nISUP 2 (3+4)\n700 (27.23)\n8 (16.0)\n10 (20.0)\nISUP 3 (4+3)\n186 (7.23)\n12 (24.0)\n12 (24.0)\nISUP 4 (4+4, 3+5, 5+3)\n103 (4.00)\n8 (16.0)\n8 (16.0)\nISUP 5 (4+5, 5+4, 5+5)\n88 (3.42)\n10 (20.0)\n10 (20.0)\nMissing\n0 (0.0)\n0 (0.0)\n0 (0.0)\n59\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nTable 2. Overview of image acquisition attributes and WSIs. Cohorts marked with (*) (i.e.\nSTHLM3, STG, and MUL) contain overlapping subsets of slides digitised with different\nscanners. Other cohorts were either digitised with a single scanner or contain non-overlapping\nsubsets\nof\nslides\ndigitised\nwith\ndifferent\nscanners.\nWSI=whole\nslide\nimage,\nSTHLM3=Stockholm3, SUH=Stavanger University Hospital, RUMC=Radboud University\nMedical Center, STG=Capio S:t Göran Hospital, KUH-1=Karolinska University Hospital,\nAMU=Aichi\nMedical\nUniversity,\nAQ=Aquesta\nUropathology,\nAUH=Aarhus\nUniversity\nHospital, KUH-2=Karolinska University Hospital morphological subtypes, MLP=Mehiläinen\nLänsi-Pohja, MUL=Medical University of Lodz, SCH=Synlab Switzerland, SFI=Synlab Finland,\nSFR=Synlab France, SPROB20=Spear Prostate Biopsy 2020, UKK=University Hospital\nCologne, WNS=Hospital Wiener Neustadt.\nSplit\nCohort\nScanning location\nScanning period\nScanner\nMagnification\n(Pixel size)\nWSI\nformat\nWSI\nnumber\nVendor\nModel\nSerial no.\nDevelopment,\ntuning and\ninternal\nvalidation\ncohorts\nSTHLM3*\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n07/2014 - 11/2014\nHamamatsu\nNanoZoomer\n2.0-HT\nC9600-12\n760347\n20x (0.4520 μm)\n.ndpi\n5,726\n.tiff\n3,417\nSciLifeLab, Uppsala, Sweden\n09/2017 - 06/2019\nAperio\nAT2 DX\nRUD-D10971\n20x (0.5032 μm)\n.svs\n3,667\n.tiff\n2,445\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n03/2018 - 06/2019\nHamamatsu\nNanoZoomer\nXR\nC12000-02\n870003\n20x (0.4536 μm)\n.ndpi\n17,973\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n10/2019 - 06/2020\nPhilips\nIntelliSite\nUFS\nFMT0047\n40x (0.2500 μm)\n.isyntax\n32,078\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n02/2023 - 03/2023\nGrundium\nOcus40\nMGU-00003-\n000184\n40x (0.2505 μm)\n.svs\n2,289\nSUH\nDepartment of Pathology,\nStavanger University Hospital,\nStavanger, Norway\n02/2022 - 03/2023\nHamamatsu\nNanoZoomer\nS60\nC13210-01\n000266\n40x (0.2199 μm)\n.ndpi\n5,762\nRUMC\nRadboud University Medical\nCenter, Nijmegen, The\nNetherlands\n01/2019 - 12/2019\n3DHISTECH\nPannoramic\nScan ll\nN/A\n20x (0.4861 μm)\n.tiff\n5,275\nSTG*\nDepartment of Immunology,\nGenetics, and Pathology,\nUppsala University, Uppsala,\nSweden\n09/2018 - 10/2018\nHamamatsu\nC13210\n000058\n20x (0.4405 μm)\n.ndpi\n74\nDepartment of Immunology,\nGenetics, and Pathology,\nUppsala University, Uppsala,\nSweden\n10/2018\nHamamatsu\nC13210\n000044\n20x (0.4409 μm)\n.ndpi\n67\n60\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nSciLifeLab, Uppsala, Sweden\n12/2018\nAperio\nAT2 DX\nRUD-D10971\n20x (0.5032 μm)\n.svs\n247\nKUH-1\nDepartment of Pathology,\nKarolinska University\nHospital, Solna, Sweden\n07/2019 - 08/2019\nHamamatsu\nNanoZoomer\nS360\nC13220-01\n000077\n20x (0.4604 μm)\n.ndpi\n330\nExternal and\npartly\nexternal\nvalidation\ncohorts\nAMU\nAichi Medical University,\nNagakute, Japan\n01/2023 - 12/2023\nHamamatsu\nC13210\n000218\n40x (0.2211 μm)\n.ndpi\n73\nAQ\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n10/2019 - 06/2020\nPhilips\nIntelliSite\nUFS\nFMT0047\n40x (0.2500 μm)\n.isyntax\n58\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n01/2024 - 02/2024\nGrundium\nOcus40\nMGU-00003-\n000184\n40x (0.2505 μm)\n.svs\n78\nAUH\nDepartment of Pathology,\nAarhus University Hospital,\nAarhus, Denmark\n11/2019 - 06/2020\nHamamatsu\nNanoZoomer\n2.0-HT\nC9600-12\n1Z0209\n20x (0.4545 μm)\n.ndpi\n102\nKUH-2\nDepartment of Pathology,\nKarolinska University\nHospital, Solna, Sweden\n07/2022\nAperio\nAT2 DX\nSS7033\n20x (0.5032 μm)\n.svs\n146\nMLP\nFinnish Institute of Molecular\nMedicine, Helsinki, Finland\n10/2019 - 03/2020\n3DHISTECH\nPannoramic\n250 Flash III\n01702\n40x (0.2427 μm)\n.mrxs\n1,964\nMUL*\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n12/2019 - 01/2020\nPhilips\nIntelliSite\nUFS\nFMT0047\n40x (0.2500 μm)\n.isyntax\n503\nDepartment of Medical\nEpidemiology and\nBiostatistics, Karolinska\nInstitutet, Solna, Sweden\n01/2023 - 03/2023\nGrundium\nOcus40\nMGU-00003-\n000184\n40x (0.2505 μm)\n.svs\n1,945\nSCH & SFI\n& SFR\nSynlab italia srl, Monza, Italy\n06/2022 - 02/2023\nPhilips\nIntelliSite\nUFS\nN/A\n40x (0.2500 μm)\n.isyntax\n3,486\nSPROB20\nUppsala University Hospital,\nUppsala, Sweden\n2020\nHamamatsu\nNanoZoomer\nS360 C13210\nN/A\n40x (0.2204 μm)\n.tif\n2,570\nUKK\nInstitute of Pathology,\nUniversity Hospital Cologne,\nCologne, Germany\nN/A\nHamamatsu\nNanoZoomer\nS360\nN/A\n40x (0.2305 μm)\n.ome.tiff\n50\nWNS\nHospital Wiener Neustadt,\nWiener Neustadt, Austria\nN/A\nHamamatsu\nNanoZoomer\nS360\nN/A\n40x (0.2305 μm)\n.ome.tiff\n50\n61\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nTable 3. Reference standard protocols with respect to grading. Reference standard protocols\nare divided into three categories: single reader, consensus and panel. In the single reader\ncategory, a sole reader assessed each slide. In the consensus category, assessments from multiple\nreaders were combined based on site-specific criteria for consensus. In the panel category,\nreaders provided independent assessments in a blinded manner. STHLM3=Stockholm3,\nSUH=Stavanger University Hospital, RUMC=Radboud University Medical Center, STG=Capio\nS:t Göran Hospital, KUH-1=Karolinska University Hospital, AMU=Aichi Medical University,\nAQ=Aquesta Uropathology, AUH=Aarhus University Hospital, KUH-2=Karolinska University\nHospital morphological subtypes, MLP=Mehiläinen Länsi-Pohja, MUL=Medical University of\nLodz, SCH=Synlab Switzerland, SFI=Synlab Finland, SFR=Synlab France, SPROB20=Spear\nProstate Biopsy 2020, UKK=University Hospital Cologne, WNS=Hospital Wiener Neustadt.\nCohorts\nReference standard protocol\nSplit\nCohort\nCohort subset\nSlide number\nType\nTotal number\nof readers\nLevel\nDevelopment, tuning\nand internal\nvalidation cohorts\nSTHLM3\nSTHLM3 full cohort\n36,848\nSingle reader (L.E.)\n1\nSlide\nPatient\nImageBase\n90\nPanel\n23\nSlide\nPANDA Swedish private validation set\n212\nConsensus\n3\nSTHLM3 morphological subtypes\n24\nSingle reader (L.E.)\n1\nSUH\nSUH full cohort\n5,762\nSingle reader\n14\nSlide\nRe-graded\n66\nSingle reader (L.E.)\n1\nRUMC\nRUMC full cohort\n5,275\nSingle reader\nmultiple\nSlide\nPANDA RUMC tuning set\n195\nPanel\n3\nPANDA RUMC private validation set\n333\nPanel\n3\nRe-graded\n66\nSingle reader (L.E.)\n1\nSTG\nSTG full cohort\n247\nSingle reader (L.E.)\n1\nSlide\nKUH-1\nKUH-1 full cohort\n330\nSingle reader (L.E.)\n1\nSlide\nPatient\nExternal and partly\nexternal validation\ncohorts\nAMU\nAMU full cohort\n73\nSingle reader\n1\nPatient\nAQ\nAQ full cohort\n136\nSingle reader\n1\nSlide\nAUH\nAUH full cohort\n102\nSingle reader\n1\nSlide\nRe-graded\n41\nSingle reader (L.E.)\n1\nKUH-2\nKUH-2 full cohort\n146\nSingle reader (L.E.)\n1\nSlide\nMLP\nMLP full cohort\n1,964\nSingle reader\nmultiple\nLocation\nRe-graded\n66\nSingle reader (L.E.)\n1\nSlide\n62\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nMUL\nMUL full cohort\n1,959\nConsensus\n2\nSlide\nRe-graded\n66\nSingle reader (L.E.)\n1\nSCH\nSCH full cohort\n2,434\nSingle reader\nmultiple\nLocation\nRe-graded\n72\nSingle reader (L.E.)\n1\nSlide\nSFI\nSFI full cohort\n537\nSingle reader\nmultiple\nLocation\nRe-graded\n67\nSingle reader (LE)\n1\nSlide\nSFR\nSFR full cohort\n515\nSingle reader\nmultiple\nLocation\nRe-graded\n49\nSingle reader (LE)\n1\nSlide\nSPROB20\nSPROB20 full cohort\n2,570\nSingle reader\nmultiple\nPatient\nRe-graded\n50\nSingle reader (LE)\n1\nSlide\nUKK\nUKK full cohort\n50\nPanel\n11\nSlide\nWNS\nWNS full cohort\n50\nPanel\n10\nSlide\n63\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nTable 4. Reference standard protocols with respect to PNI and cribriform cancer. Reference\nstandard protocols are divided into three categories: single reader, consensus and panel. In the\nsingle reader category, a sole reader assessed each slide. In the consensus category, assessments\nfrom multiple readers were combined based on site-specific criteria for consensus. In the panel\ncategory, readers provided independent assessments in a blinded manner. The SUH, MUL and\nSCH cohorts do not have consistent original reporting on cribriform cancer and PNI.\nPNI=perineural invasion, WSI=whole slide image, STHLM3=Stockholm3, SUH=Stavanger\nUniversity Hospital, AMU=Aichi Medical University, MUL=Medical University of Lodz,\nSCH=Synlab Switzerland.\nCohorts\nReference standard protocol\nSplit\nCohort\nCohort subset\nSlide number\nType\nTotal number\nof readers\nLevel\nDevelopment, tuning\nand internal\nvalidation cohorts\nSTHLM3\nSTHLM3 full cohort\n36,848\nSingle reader (L.E.)\n1\nSlide\nPatient\nRe-assessed cribriform cancer (round 1)\n702\nSingle reader (L.E.)\n1\nSlide\nPixel\nRe-assessed cribriform cancer (round 2)\n304\nPanel\n9\nSlide\nRe-assessed PNI (round 1)\n485\nSingle reader (L.E.)\n1\nSlide\nPixel\nRe-assessed PNI (round 2)\n212\nPanel\n4\nSlide\nSUH\nSUH full cohort\nN/A\nN/A\nN/A\nN/A\nRe-assessed cribriform cancer (round 1)\n332\nSingle reader (A.B.)\n1\nSlide\nRe-assessed cribriform cancer (round 2)\n200\nSingle reader (L.E.)\n1\nSlide\nRe-assessed PNI (round 1)\n509\nSingle reader (A.B.)\n1\nSlide\nRe-assessed PNI (round 2)\n185\nSingle reader (L.E.)\n1\nSlide\nExternal validation\ncohorts\nAMU\nAMU full cohort\n73\nSingle reader\n1\nSlide\nMUL\nMUL full cohort\nN/A\nN/A\nN/A\nN/A\nRe-assessed cribriform cancer\n276\nConsensus\n2\nSlide\nRe-assessed PNI\n276\nConsensus\n2\nSlide\nSCH\nSCH full cohort\nN/A\nN/A\nN/A\nN/A\nRe-assessed cribriform cancer\n56\nSingle reader (H.S.)\n1\nSlide\nRe-assessed PNI\n94\nSingle reader (B.D.)\n1\nSlide\n64\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nTable 5. Summary of slides representing various morphological subtypes. The AQ cohort\ncontains partly external validation data (scanner was used in the development) and fully external\nvalidation data (scanner was not used in the development). A single slide can be associated with\nmultiple subtypes. Instead of morphological subtypes, the samples denoted with (*) represent\nother types of specimens than core needle biopsies. Besides assessing performance on unusual\nand potentially challenging morphologies, we will assess how the AI system intended for needle\nbiopsies will respond to other specimen types and evaluate frameworks for automatically\nflagging outlier cases (Olsson et al., 2022). STHLM3=Stockholm3, AQ=Aquesta Uropathology,\nKUH-2=Karolinska University Hospital morphological subtypes, PIN=prostatic intraepithelial\nneoplasia, TUR-P=transurethral resection of the prostate.\nMorphological subtype\nInternal validation cohort\nPartly external validation cohort\nExternal validation cohort\nSTHLM3 (n=24)\nAQ (n=58)\nAQ (n=78)\nKUH-2 (n=146)\nAdenosis\n4\n18\n7\n34\nAtrophy\n0\n0\n0\n38\nPartial atrophy\n0\n7\n9\n0\nSimple atrophy\n0\n1\n15\n0\nBasal cell hyperplasia\n0\n10\n7\n20\nCancer of atrophic type\n7\n1\n3\n2\nClear cell cribriform hyperplasia\n0\n0\n5\n3\nCowper's glands\n0\n2\n14\n6\nFoamy gland cancer\n0\n4\n2\n13\nIncreased number of glands\n0\n0\n5\n0\nPostatrophic hyperplasia\n0\n2\n2\n4\nProstatectomy*\n0\n1\n4\n0\nPIN-like cancer\n3\n0\n0\n0\nPseudohyperplastic cancer\n9\n4\n0\n24\nSclerosing adenosis\n0\n4\n2\n0\nSeminal vesicle\n0\n5\n7\n0\nSmall cell cancer\n0\n0\n0\n4\nTUR-P*\n0\n13\n4\n0\n65\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\nTable 6. Summary of slides with IHC staining confirming the diagnosis. Number of slides\nstratified by ISUP grade with/without IHC staining performed for confirming the diagnosis. For\nthe SCH and SFR cohorts, where pathology reporting was performed on anatomical location or\npatient level, the total summed numbers of slides associated with an IHC-supported diagnosis are\nshown. IHC=immunohistochemistry, ISUP=International Society of Urological Pathology,\nSUH=Stavanger University Hospital, SCH=Synlab Switzerland, SFR=Synlab France.\nSplit\nCohort\nIHC performed\nNumber of slides\nAll\nBenign\nISUP 1\n(3+3)\nISUP 2\n(3+4)\nISUP 3\n(4+3)\nISUP 4 (4+4,\n3+5, 5+3)\nISUP 5 (4+5,\n5+4, 5+5)\nInternal\nvalidation\nSUH\nYes\n247\n132\n60\n16\n10\n9\n20\nNo\n909\n604\n93\n60\n64\n44\n44\nExternal\nvalidation\nSCH\nYes\n365\n120\n131\n47\n46\n9\n12\nNo\n2,064\n1,455\n194\n154\n137\n85\n35\nMissing\n5\n5\n0\n0\n0\n0\n0\nSFR\nYes\n116\n66\n41\n4\n1\n1\n0\nNo\n398\n306\n46\n24\n2\n9\n6\nMissing\n1\n1\n0\n0\n0\n0\n0\n66\n . \nCC-BY 4.0 International license\nIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \n(which was not certified by peer review)\nThe copyright holder for this preprint \nthis version posted July 7, 2024. \n; \nhttps://doi.org/10.1101/2024.07.04.24309948\ndoi: \nmedRxiv preprint \n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21264v2.pdf",
    "total_pages": 117,
    "title": "Foundation Models -- A Panacea for Artificial Intelligence in Pathology?",
    "authors": [
      "Nita Mulliqi",
      "Anders Blilie",
      "Xiaoyi Ji",
      "Kelvin Szolnoky",
      "Henrik Olsson",
      "Sol Erika Boman",
      "Matteo Titus",
      "Geraldine Martinez Gonzalez",
      "Julia Anna Mielcarz",
      "Masi Valkonen",
      "Einar Gudlaugsson",
      "Svein R. Kjosavik",
      "José Asenjo",
      "Marcello Gambacorta",
      "Paolo Libretti",
      "Marcin Braun",
      "Radzislaw Kordek",
      "Roman Łowicki",
      "Kristina Hotakainen",
      "Päivi Väre",
      "Bodil Ginnerup Pedersen",
      "Karina Dalsgaard Sørensen",
      "Benedicte Parm Ulhøi",
      "Pekka Ruusuvuori",
      "Brett Delahunt",
      "Hemamali Samaratunga",
      "Toyonori Tsuzuki",
      "Emilius A. M. Janssen",
      "Lars Egevad",
      "Martin Eklund",
      "Kimmo Kartasalo"
    ],
    "abstract": "The role of artificial intelligence (AI) in pathology has evolved from aiding\ndiagnostics to uncovering predictive morphological patterns in whole slide\nimages (WSIs). Recently, foundation models (FMs) leveraging self-supervised\npre-training have been widely advocated as a universal solution for diverse\ndownstream tasks. However, open questions remain about their clinical\napplicability and generalization advantages over end-to-end learning using\ntask-specific (TS) models. Here, we focused on AI with clinical-grade\nperformance for prostate cancer diagnosis and Gleason grading. We present the\nlargest validation of AI for this task, using over 100,000 core needle biopsies\nfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with a\nfully end-to-end TS model in a multiple instance learning framework. Our\nfindings challenge assumptions that FMs universally outperform TS models. While\nFMs demonstrated utility in data-scarce scenarios, their performance converged\nwith - and was in some cases surpassed by - TS models when sufficient labeled\ntraining data were available. Notably, extensive task-specific training\nmarkedly reduced clinically significant misgrading, misdiagnosis of challenging\nmorphologies, and variability across different WSI scanners. Additionally, FMs\nused up to 35 times more energy than the TS model, raising concerns about their\nsustainability. Our results underscore that while FMs offer clear advantages\nfor rapid prototyping and research, their role as a universal solution for\nclinically applicable medical AI remains uncertain. For high-stakes clinical\napplications, rigorous validation and consideration of task-specific training\nremain critically important. We advocate for integrating the strengths of FMs\nand end-to-end learning to achieve robust and resource-efficient AI pathology\nsolutions fit for clinical use.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}