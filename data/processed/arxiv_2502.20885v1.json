{
  "id": "arxiv_2502.20885v1",
  "text": "Published in Transactions on Machine Learning Research (03/2025)\nA Fused Gromov-Wasserstein Approach to Subgraph Con-\ntrastive Learning\nAmadou S. Sangare\namadou.sangare@telecom-paris.fr\nLTCI, Télécom Paris\nInstitut Polytechnique de Paris, France\nNicolas Dunou\nnicolas.dunou@mines-saint-etienne.org\nÉcole des Mines de Saint-Étienne\nUniversité Paris Dauphine-PSL, France\nJhony H. Giraldo\njhony.giraldo@telecom-paris.fr\nLTCI, Télécom Paris\nInstitut Polytechnique de Paris, France\nFragkiskos D. Malliaros\nfragkiskos.malliaros@centralesupelec.fr\nUniversité Paris-Saclay\nCentraleSupélec, Inria, France\nReviewed on OpenReview: https: // openreview. net/ forum? id= J7cY9Jr9WM\nAbstract\nSelf-supervised learning has become a key method for training deep learning models when\nlabeled data is scarce or unavailable. While graph machine learning holds great promise\nacross various domains, the design of effective pretext tasks for self-supervised graph repre-\nsentation learning remains challenging. Contrastive learning, a popular approach in graph\nself-supervised learning, leverages positive and negative pairs to compute a contrastive loss\nfunction. However, current graph contrastive learning methods often struggle to fully use\nstructural patterns and node similarities. To address these issues, we present a new method\ncalled Fused Gromov Wasserstein Subgraph Contrastive Learning (FOSSIL). Our model in-\ntegrates node-level and subgraph-level contrastive learning, seamlessly combining a standard\nnode-level contrastive loss with the Fused Gromov-Wasserstein distance. This combination\nhelps our method capture both node features and graph structure together. Importantly,\nour approach works well with both homophilic and heterophilic graphs and can dynamically\ncreate views for generating positive and negative pairs.\nThrough extensive experiments\non benchmark graph datasets, we show that FOSSIL outperforms or achieves competitive\nperformance compared to current state-of-the-art methods.\n1\nIntroduction\nSelf-Supervised Learning (SSL) is a methodology employed in machine learning to extract meaningful and\ntransferable representations from unlabeled data. SSL has been successfully used in many areas, like com-\nputer vision (Tomasev et al., 2022), speech processing (Baevski et al., 2020), and natural language processing\n(Devlin et al., 2019; Lewis et al., 2019). SSL is particularly important when labeled data is impractical or\ncostly to obtain. Graph Representation Learning (GRL) stands out as a domain where SSL excels due to\nthe complexity involved in annotating graphs, which often demands specialized knowledge and cannot be\neasily crowd-sourced. For instance, in biochemistry (Zang et al., 2023), annotating molecular graphs requires\nspecialized knowledge, significantly increasing the labeling effort compared to other data modalities.\n1\narXiv:2502.20885v1  [cs.LG]  28 Feb 2025\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nIn SSL, Contrastive Learning (CL) is a prominent approach (Chen et al., 2020b). CL methods typically\ninvolve sampling an anchor from the dataset, generating an augmentation (positive pair), and contrasting\nit with negative pairs using a CL loss function. Unlike certain modalities, such as images, the generation\nof positive pairs in GRL presents unique challenges. Various methods have been proposed, including those\ninvolving node-level or subgraph-level contrastive loss functions (Zhu et al., 2020b; Yuan et al., 2023; Han\net al., 2022; Jiao et al., 2020). Recent studies have shown that subgraph-level approaches outperform node-\nlevel ones, as they can capture the structural similarities between the anchor and positive graph pair (Han\net al., 2022).\nPrior subgraph-level CL methods often use readout functions to extract subgraph representations (Jiao et al.,\n2020; Veličković et al., 2019). Alternatively, some approaches rely on Optimal Transport (OT) distances for\ncomputing the CL loss (Han et al., 2022). OT-based techniques exhibit superior performance compared to\nreadout-based methods in subgraph-level CL. Notably, the Generative Subgraph Contrast (GSC) method\n(Han et al., 2022) leverages a combination of the well-established Wasserstein Distance (WD) (Villani et al.,\n2009) and Gromov-Wasserstein Distance (GWD) (Mémoli, 2011) from OT. However, existing OT-based\nmethodologies face two challenges: separately computing the WD and GWD does not capture both structural\nand feature information simultaneously when comparing subgraphs, and the Graph Neural Network (GNN)\nencoder struggles with heterophilic datasets.\nMotivated by the limitations of previous subgraph CL methodologies, we introduce a new GCL approach\nemploying the Fused Gromov-Wasserstein Distance (FGWD) (Vayer et al., 2018; Brogat-Motte et al., 2022)\nfrom OT. The FGWD captures both feature and structural similarities jointly. Our model, termed Fused\nGromov Wasserstein Subgraph Contrastive Learning (FOSSIL), is designed to be robust against variations\nin homophily levels within the graph dataset. The proposed method uses a decoupled scheme to process\nheterophilic data more effectively. Furthermore, FOSSIL leverages the FGWD to simultaneously encode\nthe structural and feature characteristics of our subgraphs for calculating the CL loss. We conduct a com-\nprehensive comparative analysis of FOSSIL against state-of-the-art models across various homophilic and\nheterophilic datasets for self-supervised node classification tasks. Our findings demonstrate that FOSSIL ei-\nther outperforms existing methodologies or achieves competitive results, thereby showcasing its efficacy in\ngraph SSL tasks. The main contributions of our paper can be summarized as follows:\n• We introduce a novel approach, named FOSSIL, that effectively captures both feature and structural\nsimilarities simultaneously with the FGWD, addressing the limitations of previous OT-based SSL meth-\nods in GRL.\n• We design a robust architecture capable of accommodating varying levels of homophily within graph\ndatasets. By using a decoupled scheme for the GNN encoder, FOSSIL performs consistently well across\nhomophilic and heterophilic datasets. This robustness enhances the applicability of the proposed method\nin real-world scenarios.\n• We conduct an extensive empirical evaluation to validate the effectiveness of FOSSIL against state-of-\nthe-art models in self-supervised node classification tasks across diverse datasets. We also thoroughly\nvalidate each design choice of FOSSIL through a series of ablation studies.\n2\nRelated Work\n2.1\nGraph Neural Networks\nGNNs are neural architectures designed for graphs (Wu et al., 2021). They usually extract information via\nmessage-passing mechanisms between nodes. This process can be divided into two steps: message aggrega-\ntion from the neighborhood and updating of the node embedding (Gilmer et al., 2017). The key difference\nbetween state-of-the-art GNN architectures lies in the aggregation functions. For example, Graph Convo-\nlutional Networks (GCNs) (Kipf & Welling, 2017) perform a weighted average of the messages inside the\n1-hop neighborhood, where weights are fixed and defined with respect to the node degrees. Graph Attention\nNetworks (GATs) (Veličković et al., 2018) compute learnable attention weights thanks to interactions inside\nthe 1-hop neighborhood. Nevertheless, most of the GNN architectures are built under the homophily as-\n2\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nsumption. This assumption is useful in graph datasets where nodes have similar characteristics. Therefore,\nthese GNNs have poor node classification performance in heterophilic graph datasets.\nPrevious research works have tried to address the heterophilic limitation of classical message-passing neural\nnetworks. For example, GPR-GNN (Chien et al., 2021) uses a polynomial graph filter with coefficients learned\naltogether with the linear projection layer used in GNNs. H2GCN (Zhu et al., 2020a) proposes a set of designs\nto improve the node classification performance of GNN on heterophilic graphs: feature-structure embedding\nseparation, aggregating in higher-order neighborhoods, and a combination of intermediate representations\n(which is equivalent to adding skip connections between GNN layers).\nFrom a graph signal processing\nperspective (Isufi et al., 2024), GCNs are considered low-pass filters (NT & Maehara, 2019). Therefore,\nACM (Luan et al., 2022) adaptively combines information extracted by a graph low-pass filter, a high-pass\nfilter, and an MLP to address heterophilic networks.\nIn FOSSIL we adopt a decoupled encoder design similar to ACM (Luan et al., 2022), making our encoder\nagnostic to the input graph’s level of homophily. Specifically, we use a GNN with a normalized adjacency\nmatrix without self-loops and the identity, which implements a vanilla message-passing function and an\nMLP. These two representations are then adaptively combined with a fusion module. The message-passing\nmodule of our GNN is equivalent to the low-pass channel of ACM, while the MLP is the identity channel.\nIntuitively, we make our encoder more robust to different levels of homophily with the two representations.\n2.2\nSelf-supervised Learning on Graphs\nThe key idea of SSL on graphs is to design and solve some pretext tasks that do not require label information.\nDepending on how the pretext task is defined, SSL methods for graphs can be divided into two categories:\npredictive and contrastive methods. Predictive methods aim to learn meaningful representations where the\noutput is typically a perturbed version of the input graph. For example, BGRL (Thakoor et al., 2021) learns\nnode representations by encoding two perturbed versions of the input graph with an online encoder and\na target encoder. The online encoder is trained to predict the representation of the target encoder, and\nthe target encoder is updated as an exponential moving average of the online encoder. BNLL (Liu et al.,\n2024) improves the BGRL model by introducing a few noisy-ground truth positive node pairs. Based on the\nhomophily assumption that neighboring nodes often share the same label, they include additional candidate\npositive pairs to the objective of BGRL. Specifically, for each node, BNLL adds the cosine similarity of its\nonline representation and the target representations of its neighbors weighted by its attention weights with\nits neighbors. VGAE (Kipf & Welling, 2016) uses a variational autoencoder approach in graphs to predict\nthe same graph and feature as in the input. Finally, GPT-GNN (Hu et al., 2020b) performs an autoregressive\nreconstruction of the graph.\nContrastive methods, which are also the focus of this paper, have shown better performance than predictive\nmethods for SSL on graphs in general. Such methods can be divided into three categories depending on\nhow the data pairs are defined: node-to-node, graph-to-graph, and node-to-graph. For example, GRACE\n(Zhu et al., 2020b) generates two perturbed views of the original graph and performs contrastive learning\nbetween the nodes of these views. MUSE (Yuan et al., 2023) extracts node-wise semantic, contextual, and\nfused embeddings to perform node-to-node contrastive learning for each of these embeddings. Nevertheless,\nnode-level contrastive learning approaches are suboptimal since they cannot easily capture global information\nabout the structure of the graphs.\nRegarding subgraph level contrast, DGI (Veličković et al., 2019) performs node-to-graph contrast by extract-\ning the node embeddings of the original graph and a perturbed version, then it increases or decreases the\nagreement between the original or perturbed node embeddings using a readout function. Though spectral\npolynomial filters such as GPR-GNN (Chien et al., 2021) and ChebNetII (He et al., 2022) are more expressive\nthan GCNs and can adapt to arbitrary homophily levels, they underperform GCNs when used as an encoder\nfor traditional SSL methods. PolyGCL (Chen et al., 2024) tries to solve this problem by restricting the\nexpressiveness of the polynomial filters from a spectral perspective to construct the low-pass and high-pass\nviews and introduces a simple linear combination strategy to construct the optimization objective. Specif-\nically, they follow a similar approach to DGI (Veličković et al., 2019) with the difference that they do it\nwith both high/low-frequency embeddings extracted with high-pass and low-pass polynomial filters sharing\n3\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nweights. Subg-Con (Jiao et al., 2020) follows up the work of DGI by doing the same contrast but at a sub-\ngraph level. It starts by sampling a set of anchor nodes, followed by the extraction of subgraphs utilizing the\npersonalized PageRank algorithm. Overall, the model aims to adjust the agreement level between an anchor\nnode and the sampled subgraph for the positive and negative pairs. Methods such as DGI and Subg-Con,\nhowever, use a readout embedding to characterize graphs, which ignores the structure of the graph. GSC\n(Han et al., 2022) addresses this issue by performing subgraph-level contrast using WD and GWD from OT\nas the measure of subgraph similarity.\nFOSSIL is a combined subgraph-level and node-level contrastive learning approach. Our model uses OT\ndistance metrics to measure the dissimilarity between subgraphs while further incorporating node-level con-\ntrast to capture differences among nodes. While sharing common features, our method differs from GSC in\nthree key aspects: (i) we jointly capture structural and feature information in the OT distance using the\nFGWD, (ii) we perform node-level contrast to emphasize the node’s difference, and (iii) we have a specialized\narchitecture that is robust to both homophilic and heterophilic datasets.\n3\nPreliminaries\n3.1\nMathematical Notation\nIn this paper, matrices are represented with uppercase bold letters like A, while lowercase bold letters like\nx denote vectors. The symbol ⟨·, ·⟩denotes the matrix scalar product associated with the Frobenius norm.\nFor an element x of a measured space, δx represents the Dirac measure at x. The vector 1n ∈Rn denotes\nthe vector of all ones of dimension n. We denote the simplex histogram with n bins as Σn = {h ∈Rn\n+ |\nPn\ni=1 hi = 1}. Let L be a 4-way tensor and B be a matrix. The tensor-matrix multiplication between L\nand B is denoted as the matrix L ⊗B, where (L ⊗B)i,j = P\nk,l Li,j,k,lBk,l. The element-wise (Hadamard)\nproduct operator between two matrices is denoted as ⊙. The fraction u\nv denotes the element-wise division\nof vectors u and v.\n3.2\nGraph Neural Networks\nWe consider node-attributed and undirected graphs, denoted as G = (A, X), where A ∈RN×N represents\nthe adjacency matrix, X ∈RN×F denotes the feature matrix, and N and F respectively denote the number of\nnodes and the dimensionality of each node feature vector. A GNN is a function f : RN×N ×RN×F →RN×D,\nwhere H = f(A, X) ∈RN×D represents the node embeddings matrix of G, with each row Hi corresponding\nto the embedding of node vi (Wu et al., 2021).\nAdditionally, for a vector v ∈RN, diag(v) ∈RN×N\ndenotes the diagonal matrix with diagonal elements from v. Let S ⊆{1, . . . , N} be a set of indices; A[S; S]\nrepresents the adjacency matrix of the subgraph of G restricted to nodes indexed by S, and X[S] denotes its\ncorresponding feature matrix.\n3.3\nFused Gromov-Wasserstein Distance\nThe main purpose of OT is to find the optimal way to shift objects from one location to another in a metric\nspace. This is done by finding a minimum cost soft matching between two probability distributions, one on\nobjects of each location. Let µ ∈Σn and ν ∈Σm be two probability distributions, respectively, over the\nobjects of the first and the second location. The WD (Villani et al., 2009) uses a matrix M ∈Rn×m of\ntransportation costs (or pair-wise distances) between location 1 and 2 to find a soft matching that minimizes\nthe overall transportation cost.\nHowever, locations 1 and 2 may not lie in the same space, preventing\nus from defining the transportation cost M. GWD (Mémoli, 2011) solves this problem by using internal\ntransportation cost matrices C1 and C2 in each space separately to find the soft matching. The purpose is\nto match objects that have similar structural behaviors inside their metric spaces.\nThe main downside of WD and GWD is that they fail to capture both structural and feature information\n(Vayer et al., 2018) when used to compare graphs. In fact, WD only uses node features to define its cost\nmatrix M, while GWD only uses each graph’s adjacency matrix to define its internal costs matrices C1 and\nC2. To alleviate this issue, Vayer et al. (2018) proposed the Fused Gromov-Wasserstein Distance (FGWD).\n4\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nThe idea is to use a coefficient α ∈[0, 1] that trades off between WD and GWD. The FGWD between the\ntwo locations is defined as follows:\nFGWDα,M,C1,C2(µ, ν) =\nmin\nP∈Π(µ,ν)⟨αM + (1 −α)L(C1, C2) ⊗P, P⟩,\n(1)\nwhere P ∈Rn×m is a soft matching (or joint probability distribution) between objects of location 1 and 2,\nL(C1, C2)i,j,k,l = |C1[i; k]−C2[j; l]|2, and Π(µ, ν) is the set of joint probability distributions with marginals\nµ and ν. FGWD is a generalization of WD and GWD. We recover WD for α = 1 and GWD for α = 0.\n3.4\nGraph Contrastive Learning (GCL)\nGCL (You et al., 2020; Xie et al., 2021) is built upon the Mutual Information Maximization principle (MIM)\n(Linsker, 1988). Given a graph G sampled from a graph distribution PG, the goal is to learn an encoder f(·)\nby maximizing the mutual information between G and its encoded version f(G). The encoder f should be\nable to distinguish different graphs. Hence, we solve:\nmax MI(G; f(G)), where G ∼PG.\n(2)\nFor computational efficiency, in practice, we maximize an estimated lower bound c\nMI of the mutual infor-\nmation. These lower bounds are used to maximize the similarity between positive pairs and minimize the\nsimilarity between negative ones. Formally, given a graph G ∼PG, and a graph view generation operator\nT(G) that creates different but semantically similar versions t(G) of G, the problem writes:\nmax c\nMI(f(G), f(t(G))), where G ∼PG and t(G) ∼T(G).\n(3)\nIn this work, we use the Jensen-Shannon estimator (Nowozin et al., 2016) for our subgraph-level contrastive\nlearning and the InfoNCE (van den Oord et al., 2018) for our node-level contrastive learning.\nThe learned representation f(G) can then be transferred to a graph-level or node-level downstream task by\nplugging in a carefully designed decoder:\nmin\nθds Ldownstream(Dec(f(G)), Y),\n(4)\nwhere Y, Dec(·), θds, and Ldownstream are respectively the labels, the decoder (or prediction head), the\nparameters, and the loss for the downstream task.\n4\nFused Gromov-Wasserstein Subgraph Contrastive Learning (FOSSIL)\n4.1\nOverview\nFigure 1 illustrates the pipeline of our proposed method, referred to as FOSSIL. FOSSIL comprises five\nprimary stages: (i) encoding, (ii) view generation, (iii) fusion layer, (iv) subgraph sampling, and (v) SSL\nloss computation based on OT for subgraph-level plus a node-level contrastive loss. FOSSIL takes an input\ngraph G and employs two neural networks with shared weights as encoders. The first neural network performs\nlinear projection, while the second implements a GNN (Kipf & Welling, 2017), both equipped with the same\nlearnable parameters W. Subsequently, view generation follows a decoupled strategy similar to the encoder,\nutilizing a GAT (Veličković et al., 2018).\nThe fusion layer combines the outputs from the encoder and\ngenerator using a linear combination. Finally, subgraphs are sampled at the encoder and generator levels to\ncompute the subgraph-level CL loss using the FGWD as the distance metric, and a node-level contrastive\nloss is additionally computed.\n4.2\nEncoder\nSeveral GNNs for self-supervised node classification are built under the homophily assumption, meaning\nthat they assume the nodes in the neighborhood share similar labels (Han et al., 2022). As a consequence,\n5\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nENC.\nENC.\nShared\nWeights\nGAT\nGAT\nShared\nWeights\nInput Graph\nEncoder\nGenerator\nFusion\nSampling\nSSL\nLoss\nFigure 1: Pipeline of FOSSIL for self-supervised node classification tasks. The proposed method integrates\nencoding, view generation, fusion, subgraph sampling, and CL in attributed graphs. FOSSIL employs a\nshared-weight encoder and generator with GNNs to extract informative representations, subsequently fused\nfor enhanced learning.\nSubgraph sampling facilitates subgraph-level CL loss computation based on the\nFGWD.\nthey typically perform badly when evaluated on heterophilic graph datasets. Several works (Zhu et al.,\n2020a; Hamilton et al., 2017; Defferrard et al., 2016; Abu-El-Haija et al., 2019; Chien et al., 2021) proposed\nnew architectures to tackle this issue in the supervised domain. Surprisingly, these more powerful encoders\nunderperform a simple GCN in current GCL methods (Chen et al., 2024). Based on these observations and\nfollowing (Zhu et al., 2020a), we propose a decoupled encoding, where we use a GNN and an MLP with\nshared weights W ∈RF ×D. Nodes are encoded as follows:\nH = σ(IXW) + diag(λ)σ(˜AXW) = Hf + diag(λ)Hs,\n(5)\nwhere σ(·) is a non-linearity, I is the identity matrix, ˜A = D−1/2AD−1/2 is the normalized adjacency matrix,\nD = diag(A1N) ∈RN×N is the diagonal matrix of node degrees and λ ∈[0, 1]N a vector that dictates how\nthe two terms are combined. We explain how to compute λ in more detail in Sec. 4.4.\nIn (5), we adaptively combine an MLP embedding, termed feature embedding, and a GNN embedding, termed\nstructural embedding. Self-loops are not considered in the adjacency matrix, so the GNN only aggregates\ninformation from its neighbors. Intuitively, the decoupling strategy proves effective due to the presence of\ntwo distinct sources of information in an attributed graph: node features and graph structure. The relevance\nof each information source can vary depending on the downstream task at hand. For instance, in the context\nof a social network, individual user features often play a primary role in defining users, while friendship\nconnections offer supplementary insights into their interactions. Conversely, in a citation network, it could\nbe more crucial to examine the documents cited by an article to predict its category effectively. In essence, on\nhomophilic graphs, traditional GNN encoding typically outperforms MLPs, whereas on heterophilic graphs,\nMLPs tend to perform better (Zhu et al., 2020a). The decoupled approach enables us to better adapt the\nSSL model to the homophily level of the graph.\n4.3\nGenerator\nGenerating positive pairs in SSL for graphs is still an open problem of research.\nA common approach\nhere involves perturbing the original graph to preserve the underlying semantics (You et al., 2020; Qiu et al.,\n2020). However, determining these perturbations for graphs is not straightforward. State-of-the-art methods\nusually apply hand-crafted perturbations like random node feature masking and random edge dropping. Such\nperturbations lack control and can degrade the intrinsic properties of the original graph—hence increasing the\nrisk of false positives (Wang et al., 2022). Moreover, the performance of these hand-crafted perturbations\nheavily depends on the characteristics of the particular graph dataset.\nTherefore, an additional step is\nrequired to manually identify the appropriate perturbation method tailored to each dataset.\n6\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nIn FOSSIL, we adopt an end-to-end perturbation strategy, meaning that we learn the correct perturbation\nfrom the data. We use a similar strategy as in the encoder to separately generate node feature perturbations\nand graph structure perturbations. Given the feature embeddings Hf and structure embeddings Hs from the\nencoder, we generate perturbations as ˆHf = g(I, Hf), ˆHs = g(˜A, Hs), where g : RN×N × RN×F →RN×D\ncorresponds to a GAT model (Veličković et al., 2018). Intuitively, we leverage the attention weights of the\nGAT to adaptively generate perturbations. At the feature level, this is equivalent to applying adaptive\nscaling to the node features, while at the graph structure level, this is equivalent to adaptively dropping\nsome edges.\n4.4\nFusion Layer\nWe combine the feature and structure information from the encoder and generator to obtain the final node\nembeddings. To this end, we apply adaptive node-wise fusion layers with shared weights θ. This stems from\nthe fact that nodes do not necessarily require the same amount of structural information. At the encoder\nlevel, for each node vi we combine the semantic and structural information as follows:\nλi = ψ(Hf\ni , Hs\ni, (A1N)i; θ),\n(6)\nHi = h(Hf\ni , Hs\ni) = Hf\ni + λiHs\ni,\n(7)\nwhere ψ : RD × RD × R →R is an MLP (with sigmoid as the last activation function) computing how much\nstructural information each node needs, and λ ∈[0, 1]N is the vector of the fusion coefficients of the graph\nnodes. Similarly, for the generator, we have the following:\nˆλi = ψ( ˆHf\ni , ˆHs\ni, (A1N)i; θ),\n(8)\nˆHi = h( ˆHf\ni , ˆHs\ni) = ˆHf\ni + ˆλi ˆHs\ni.\n(9)\nFollowing (Yuan et al., 2023), we incorporate the degrees of the nodes A1N into the fusion module. The\nrationale behind this choice is to leverage the node degree centralities to account for the connectivity infor-\nmation of each node inside the graph. Therefore, we help function ψ(·) in computing the fusion coefficients\nλ that are more sensitive to the structure of the graph.\n4.5\nSubgraph Sampling\nUsing the original node embeddings H and the perturbed node embeddings ˆH, we sample the subgraphs\nthat will be subsequently used as contrastive samples. To this end, we first sample without replacement a\nset of anchor-node indices S ⊆{1, . . . , N}. From each anchor node vi ∈S, we sample a set of node indices\nSi of size k (including vi) using breadth-first sampling in the 2-hop neighborhood of vi. If we do not get the\nrequired size k, then the subgraph is not included in the subgraph-level contrast. Therefore, we create two\nsubgraphs using each Si. The first subgraph is given by Gs\ni = (A[Si; Si], H[Si]). The second subgraph is\ngiven by ˆGs\ni = (ˆA[Si; Si], ˆH[Si]), where ˆAi,j = s(ˆhi, ˆhj) =\nˆh⊤\nsi ˆhsj\n∥ˆhsi∥2∥ˆhsj ∥2 . These subgraphs are used in our CL\nloss function using the FGWD. A perturbed subgraph corresponds to the original subgraph contextualized\nby the GAT we are using for adaptive graph perturbation. The intuition is that this contextualized subgraph\nfurther captures the similarities between nodes inside the original subgraph, hence, it should be similar to the\noriginal subgraph while adding additional context information to the nodes. In this work, for simplicity, we\nchoose to randomly sample subgraphs. Some studies have proposed more adaptive and data-driven subgraph\nsampling techniques, which present an interesting direction for future work in subgraph contrastive learning\n(Niepert et al., 2021; Bevilacqua et al., 2024; Kong et al., 2023). This subgraph sampling approach breaks\nthe equivariance of our encoder with respect to the contrastive loss. However, this does not negatively impact\nthe performance of FOSSIL in the datasets we use (please see Sec. 5).\n4.6\nSelf-supervised Loss Function\nThe SSL loss function in FOSSIL is given by two terms: a contrastive loss Lcontrast, and a regularization\nterm Lθ to guide the learning of the fusion layer. The contrastive loss is at the same time divided into an\n7\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nOT loss Lot and a node-level contrastive loss Lnode, so that Lcontrast = Lot + Lnode. Our final loss is thus\ngiven by L = Lcontrast + Lθ.\n4.6.1\nOT distance-based subgraph-level contrastive loss Lot\nIn FOSSIL, we consider graphs as measured spaces by endowing them with probability distributions over\ntheir nodes. The probability mass on a given node represents the node’s relative importance in the graph.\nWith no prior knowledge of the graphs, we assume a uniform probability distribution over its nodes.\nLet Gs\n1 = (As\n1, Hs\n1) and Gs\n2 = (As\n2, Hs\n2) be two subgraphs. We measure the difference between Gs\n1 and Gs\n2\nusing the FGWD as follows:\nDfgw(Gs\n1, Gs\n2) = FGWDα,C1,2,C1,C2(µ, ν),\n(10)\nwhere µ is the uniform distribution over nodes of Gs\n1, ν is the uniform distribution on the nodes of Gs\n2, C1 =\nexp(−As\n1/τ), C2 = exp(−As\n2/τ) and C1,2 = exp(−Hs\n1(Hs\n2)⊤/τ) is the matrix of pairwise transportation\ncosts between their nodes (the exponentials on matrices are applied element-wise). We use FGWD to jointly\nexploit both features and structure information inside the two graphs to compute their distances. In this\nwork, we solve the optimization problem associated with FGWD using the Bregman Alternated Projected\nGradient (BAPG) method (Li et al., 2023) given in Alg. 1, App. A. Our subgraph-level contrastive loss can\nbe thus written as:\nLot = −\n1\n|S|(M + 1)\nX\ni∈S\n\u0014\nlog\n\u0010\nσ(exp(−Dfgw(Gs\ni , ˆGs\ni )/τ))\n\u0011\n+\nM\nX\nj=1\nlog\n\u00001 −σ(exp(−Dfgw(Gs\ni , Gsnj\ni\n)/τ))\n\u0001\u0015\n.\n(11)\nwhere S is the set of sampled anchor-node indices defined in Sec. 4.5, Gs\ni is a sampled subgraph, and Gsnj\ni\nis its j-th negative view used in the contrastive loss. For each node, we define M negative views in (11). In\npractice, we set M = 2 and define Gsn1\ni\n= Gs\nj and Gsn2\ni\n= ˆGs\nj with j randomly picked in {j ∈S : j ̸= i}.\nIn the contrastive loss Lot, Lalign aims to increase the agreement between positive pairs while Lreg aims to\ndecrease the agreement between negative pairs.\n4.6.2\nNode-level contrastive loss Lnode\nWe also incorporate node-level contrastive loss. This improves our downstream node classification perfor-\nmance, as demonstrated later in our experiments. The node-level loss writes:\nl(hi, ˆhi) = −log\nexp(s(hi, ˆhi)/τ)\nP\nj̸=i exp(s(hi, hj)/τ) + PN\nj=1 exp(s(hi, ˆhj)/τ)\n(12)\nLnode =\n1\n2N\nN\nX\ni=1\n\u0010\nl(hi, ˆhi) + l(ˆhi, hi)\n\u0011\n.\n(13)\nOne may notice that (12) presents a memory bottleneck coming from the contrastive loss at the node level\nLnode. This loss requires the storage of a N × N matrix for the pairwise cosine similarities between all pairs\nof nodes within the graph, hence O(N 2) of memory complexity. This limits the scalability of FOSSIL to\nlarge-scale graphs. To avoid this issue, we propose a FOSSILv2 model that considers only the union of nodes\nwithin the sampled subgraphs when computing the contrastive loss at the node level. Hence, the alternative\nnode-level contrastive loss becomes:\nLnode-v2 =\n1\n2\n\f\f\fS\nj∈S Sj\n\f\f\f\nX\ni∈S\nj∈S Sj\n\u0010\nl(hi, ˆhi) + l(ˆhi, hi)\n\u0011\n.\n(14)\nWith this expression, we have a memory complexity of O((|S|k)2D), which scales constantly with respect to\nN for FOSSILv2.\n8\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\n4.6.3\nEmbedding fusion loss Lθ\nThe embedding fusion loss guides the fusion layer learning behavior. The main motivation for this loss is to\nprevent the structure and feature embeddings from the encoder from being too similar. To this end, when\nthe structure and feature embedding are very similar, we assume that we should emphasize more the feature\nembedding. Therefore, we promote low values in the vector λ for the concerned nodes. We also couple the Lθ\nand the OT loss through the parameter α. Notice that 1−α in the definition of the FGWD in (1) represents\nthe relative weight of structure similarity when comparing subgraphs with OT. Therefore, we promote the\nvalues of λ to align with 1−α on average so that λ aligns with the OT distances. Additionally, we constrain\nthe variance of λ with L2 regularization to put more control on the way we combine embeddings. Our fusion\nlayer loss function is thus given as:\nLθ =\nN\nX\ni=1\nλis(hs\ni, hc\ni) + β1∥λ∥2 + β2\n\f\f\f\f\f\n1\nN\nN\nX\ni=1\nλi −(1 −α)\n\f\f\f\f\f ,\n(15)\nwhere β1 and β2 are regularization parameters.\n4.7\nComputational Complexity of the Framework\n4.7.1\nTime complexity\nHere, we analyze the computational complexity of our subgraph-level contrastive loss in (10). To compute\nFGWD, we use the BAPG method described in Alg.\n1 of App.\nA, which is a single-loop method for\nsolving the non-convex quadratic optimization problem of FGWD. The tensor product L(C1, C2) ⊗P can\nbe computed with complexity O(n2m + nm2). So the gradients computed at lines 4 and 8 of Alg. 1 can be\ncomputed with complexity O(n2m + nm2). The other operations of Alg. 1 have complexity O(nm). Hence,\nthe time complexity of BAPG is O(T(n2m + nm2)) overall, where n and m are, respectively, the order of\nthe first and second graph, and T is the number of iterations that have been performed. Now we need to\ncompute the cost matrices M, C1, and C2 that are inputs of the BAPG algorithm. Those are computed\nwith an overall complexity O(nmD), where D is the dimension of the embedding vectors. Finally solving\nthe optimization problem in (1) with Alg. 1 has complexity O(T(n2m + nm2) + nmD).\n0\n10,000\n20,000\n30,000\nNumber of nodes N\n10−2\n10−1\n100\nTime (s)\nTime per iteration\nOT loss\nNode-level loss\nOthers\nFigure 2: Time per iteration of the different compo-\nnents of FOSSIL in a GPU A40 48G.\nAt each iteration of our method, we sample |S| sub-\ngraphs, each with k nodes (so we are in the case where\nn = m = k). Then, for each subgraph, we compute\nits FWGDs with its positive view and its M nega-\ntive views, which costs O((M + 1)(Tk3 + k2D)) in\ntime.\nHence, solving all the optimization problems\ncosts O(|S|(M + 1)(Tk3 + k2D)) in time. Finally, we\nneed to compute the contrastive loss with the FGWDs,\nwhich is done in O(|S|(M +1)). Overall the time com-\nplexity of computing O(|S|(M + 1)(Tk3 + k2D)).\nIt is worth noting that this time complexity only de-\npends on the hyperparameters of our method. In par-\nticular, the cubic term k3 is not a real concern in prac-\ntice, as k is chosen such that k ≪N, with N the num-\nber of nodes in the graph. Therefore, the complexity of computing Lot does not have scalability issues with\nthe size of the input graph as it remains constant w.r.t. N.\nIn our framework, we also compute a node-level contrastive loss Lnode. This loss is computed with complexity\nO(N 2D) theoretically. However, in practice, as computing the pair-wise cosine similarities between nodes\ninside the graph is parallelized thanks to matrix multiplication on the GPU, the complexity is roughly\nconstant depending on the number of cores and memory available on the GPU.\nTo experimentally validate this theoretical analysis, we evaluate the time required for computing each loss\nwith synthetic graphs generated with the contextual stochastic block model (cSBM) (Chien et al., 2021).\n9\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nTable 1: Statistics of the datasets used in this work.\nCora\nCiteSeer\nPubMed\nCoAuthor-CS\nActor\nChameleon\nSquirrel\nH(G)\n0.81\n0.74\n0.80\n0.81\n0.22\n0.24\n0.22\nNodes\n2, 708\n3, 327\n19, 717\n18, 333\n7, 600\n2, 277\n5, 201\nEdges\n10, 556\n9, 104\n88, 648\n163, 788\n30, 019\n36, 101\n217, 073\nFeatures\n1, 433\n3, 703\n500\n6, 805\n932\n2, 325\n2, 089\nClasses\n7\n6\n3\n15\n5\n5\n5\nWe vary the number of nodes N from 1, 000 to 30, 000, and for each value of N, we measure the times on\n5 synthetic graphs with arbitrary homophily levels. We fix k = 12, D = 512, |S| = 300, and M = 2 for\nthis experiment. Fig. 2 shows that the time required to compute the OT loss and node-level contrastive\nlosses remains roughly constant for any value of N. Overall, in Fig. 2, we observe that our method scales\nreasonably well with the size of the graph. The empirical time complexity depicted by the green bars does\nnot significantly increase from a graph with 1, 000 nodes to one with 30, 000 nodes.\n4.7.2\nMemory complexity\nComputing the FGWD with Alg. 1 requires storing all the cost matrices and the intermediate matrices used\ninside the algorithm. Hence, this entails a memory complexity of O(n2 + m2 + nm) for two graphs of order\nn and m. In our case, n = m = k, and we compute the FGWD of all the |S|(M +1) subgraph pairs together\ninside the same batch. Therefore, we have a memory complexity of O(k2|S|(M + 1)) for Lot. The memory\ncomplexity of the optimal transport component of FOSSIL is thus constant w.r.t. the size of the graph N.\n5\nExperimental Framework and Results\nWe perform a set of experiments to compare our model with several approaches in the literature. We compare\nFOSSIL with nine state-of-the-art methods including: GSC (Han et al., 2022), DGI (Veličković et al., 2019),\nDSSL (Xiao et al., 2022), BGRL (Thakoor et al., 2021), GRACE (Zhu et al., 2020b), MUSE (Yuan et al.,\n2023), Subg-Con (Jiao et al., 2020), PolyGCL (Chen et al., 2024), and BNLL (Liu et al., 2024). In addition,\nwe compare FOSSIL with GSC using as the encoder the Chebyshev graph convolution (Defferrard et al.,\n2016) (GSC+Cheb) instead of the regular GCN to make the GSC framework more robust to heterophilic\ndatasets.\nWe evaluate all methods in four homophilic datasets Cora (McCallum et al., 2000), CiteSeer\n(Sen et al., 2008), PubMed (Namata et al., 2012), and CoAuthor-CS (Shchur et al., 2018); and in three\nheterophilic datasets Actor (Tang et al., 2009), Chameleon, and Squirrel (Rozemberczki et al., 2021). We\nperform additional experiments in large-scale datasets in App. B. Table 1 shows the statistics of the datasets\ntested in this work, where H(G) is the homophily of the graph as defined in (Pei et al., 2020). Finally, we\nalso run a set of ablation studies to analyze different aspects of FOSSIL.\nWe use a standard downstream logistic regression accuracy with the self-supervised node embeddings to\nevaluate all compared methods. We split the data into a development set (80%) and a test set (20%) once,\nensuring that the test set is not used during the hyperparameter optimization process. Therefore, we split\nthe development set into training and validation sets for hyperparameter tuning. Following (Klicpera et al.,\n2019; Giraldo et al., 2023), we take 20 nodes per class for the training set and the rest into the validation\nset for Cora, CiteSeer, and PubMed. For the other datasets, the training set gets 60% of the nodes, and\nthe validation set gets the other 40% in the development set.\nFinally, we test each method using 100\ntest seeds to randomly split the dataset into the train and validation, while keeping the original test set.\nWe train and evaluate the encoder on those splits and report the average performances accompanied by\n95% confidence intervals calculated by bootstrapping with 1, 000 samples. Unlike other works in SSL for\ngraphs, we do not perform model selection during SSL training by looking at the downstream test/validation\nperformance. Finally, we take the encoder at the end of the SSL training to extract the node embeddings\nfor the downstream evaluation. All experiments are conducted on A40 48GB and P100 16GB GPUs.\n10\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nTable 2: Test accuracy on the node classification task for different baseline models and FOSSIL. The best\nresults are highlighted in bold, while the second best-performing methods are underlined.\nMethod\nCora\nCiteSeer\nPubMed\nCoAuthor CS\nActor\nChameleon\nSquirrel\nFS-GCN\n80.04±0.26\n69.05±0.26\n77.33±0.41\n92.50±0.02\n29.69±0.14\n45.48±0.25\n30.26±0.18\nFS-MLP\n57.28±0.39\n56.37±0.37\n69.20±0.43\n92.05±0.06\n30.08±0.20\n48.68±0.33\n37.14±0.16\nGSC\n79.29±0.20\n54.56±0.74\n76.98±0.50\n90.16±0.06\n28.43±0.16\n42.90±0.40\n28.98±0.22\nGSC+Cheb\n76.84±0.26\n68.11±0.26\n75.75±0.45\n89.20±0.14\n32.46±0.13\n43.99±0.34\n35.03±0.24\nMUSE\n62.52±0.38\n58.05±0.42\n79.08±0.31\n54.32±1.76\n36.61±0.15\n52.11±0.50\n33.57±0.17\nDGI\n79.90±0.23\n70.22±0.32\n77.80±0.47\n87.38±0.06\n29.11±0.14\n36.24±0.24\n29.58±0.16\nDSSL\n72.39±0.56\n54.16±0.64\n69.98±0.69\n87.73±0.08\n26.60±0.14\n48.38±0.35\n34.99±0.21\nBGRL\n62.95±0.87\n38.02±1.46\n72.93±0.44\n91.81±0.22\n27.93±0.17\n36.90±0.49\n28.91±0.25\nGRACE\n80.86±0.29\n65.86±0.40\n79.84±0.33\n93.01±0.03\n28.91±0.18\n43.87±0.34\n30.60±0.22\nSubg-Con\n44.32±0.02\n30.07±0.91\n67.35±0.85\n82.29±0.54\n27.60±0.18\n45.60±0.44\n37.16±0.43\nPolyGCL\n80.95±0.26\n71.38±0.21\n76.39±0.41\nOOM\n26.52±0.07\n32.75±0.19\n34.25±0.13\nBNLL\n66.46±0.83\n41.57±1.17\n70.45±0.92\n93.60±0.03\n25.33±0.34\n50.18±0.90\n36.63±0.50\nFOSSIL\n80.02±0.26\n68.24±0.34\n79.76±0.48\n94.49±0.04\n35.61±0.15\n53.03±0.33\n37.13±0.24\nFOSSILv2\n78.94±0.26\n68.16±0.23\n79.09±0.39\n95.04±0.03\n35.41±0.18\n53.14±1.24\n34.79±0.21\n5.1\nImplementation Details\nWe implement all methods using PyTorch (Ketkar, 2017) and PyG (Fey & Lenssen, 2019). Our encoder\nis a two-layer GCN with hidden dimension 1, 024 and output dimension 512, and a PReLU (He et al.,\n2015) activation function between them. In all experiments, our method is trained for 300 epochs with\nthe Adam (Kingma & Ba, 2015) optimizer. We optimize the other hyperparameters with the framework\nOptuna (Akiba et al., 2019). The search spaces for hyperparameters are defined as follows: 1) learning rate\nlr ∈{10−4, 5 × 10−4, 10−3, 5 × 10−3, 10−2}; 2) learning rate of fusion module lrf ∈{10−4, 5 × 10−4, 10−3, 5 ×\n10−3, 10−2}; 3) α ∈{[0 : 0.1 : 1]}; 4) the FGWD regularizer β ∈{10−3, 5 × 10−3, 10−2, 5 × 10−2, 10−1, 5 ×\n10−1, 1, 1.5, 2}; 5) the number of nodes in each sampled subgraph k ∈[10, 30]; 6) the temperature parameter\nτ ∈{0.2, 0.5, 0.8, 1.0, 1.5, 2.0, 2.5, 3.0}; 7) the GNN dropout parameter p ∈{0.1, 0.2, 0.3, 0.4}, and 8) the\nfusion MLP dropout pf ∈{0.1, 0.2, 0.3, 0.4}. The value of β2 is fixed to 1. We tune the hyperparameters\nby performing 100 trials in the development set.\nFinally, for each trial, we sample the values from the\nsearch space using the Tree-structured Parzen estimator (Watanabe, 2023). We study the sensibility of some\nimportant hyperparameters of FOSSIL in App. C. The source code of FOSSIL is publicly available 1.\n5.2\nResults and Discussion\nTable 2 shows the results of the comparison of FOSSIL against previous state-of-the-art models for self-\nsupervised node classification. We evaluate all methods with our setting to have a fair comparison. Our\nencoder is a decoupled GCN without self-loops; thus, we also evaluate GCN in a fully supervised setting,\nwhich we refer to as FS-GCN. We also evaluate a fully-supervised MLP (FS-MLP) for reference.\nIn Table 2, we observe that FOSSIL outperforms FS-MLP and FS-GCN, suggesting that pre-trained models\nusing self-supervision and then fine-tuned on a downstream task can outperform end-to-end supervised\nmodels. In general, FOSSIL ranks either as the best or second best method among the baseline models used\nacross both homophilic and heterophilic datasets. We also observe that FOSSIL is better than MUSE and\nGRACE on average, showing that node-level only contrast is suboptimal compared to including subgraph-\nlevel contrast for node classification.\nIt is also important to highlight that FOSSIL outperforms DGI and Subg-Con, which use a readout function\nto characterize a graph. This suggests that a simple readout function does not properly capture the structure\nof the graph. Besides, these methods perform node-graph contrast. Maximizing the agreement of an anchor\nnode and its sampled subgraph is not optimal for node classification since this subgraph can contain a\n1https://github.com/sangaram/FOSSIL\n11\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nTable 3: Impact of subgraph similarity metric in node classification.\nCosim\nWD\nGWD\nFGWD\nLnode\nCora\nCiteSeer\nPubMed\nCoAuthor CS\nActor\nChameleon\nSquirrel\n✓\n✗\n✗\n✗\n✗\n66.04±0.54\n61.32±0.44\n70.13±0.63\n84.33±0.28\n34.27±0.17\n42.43±0.29\n33.35±0.29\n✗\n✓\n✗\n✗\n✗\n61.82±0.44\n59.06±0.39\n71.94±0.68\n91.22±1.83\n35.64±0.18\n44.20±0.36\n33.09±0.26\n✗\n✗\n✓\n✗\n✗\n60.73±0.89\n50.88±0.50\n68.56±0.76\n90.98±0.10\n33.57±1.45\n52.42±1.05\n31.07±0.82\n✗\n✗\n✗\n✓\n✗\n75.19±0.42\n65.22±0.29\n77.59±0.43\n94.61±0.04\n35.79±0.18\n48.95±0.60\n33.61±0.22\n✓\n✗\n✗\n✗\n✓\n78.00±0.35\n69.14±0.30\n78.58±0.43\n94.80±0.03\n35.11±0.17\n55.42±1.08\n35.66±0.41\n✗\n✓\n✗\n✗\n✓\n78.23±0.29\n69.95±0.23\n74.56±0.56\n94.16±0.03\n31.22±0.15\n60.37±0.49\n37.48±0.25\n✗\n✗\n✓\n✗\n✓\n79.11±0.31\n67.72±0.29\n77.30±0.41\n94.01±0.04\n34.29±1.13\n50.51±0.36\n35.25±0.20\n✗\n✗\n✗\n✓\n✓\n80.02±0.26\n68.24±0.34\n79.76±0.48\n94.49±0.04\n35.61±0.15\n53.03±0.33\n37.13±0.24\nmajority of dissimilar nodes (particularly in heterophilic graphs)—hence, its readout representation can be\na false positive. This can be observed by the performance of DGI on heterophilic graphs.\nFOSSIL works better than the previous OT-based method GSC, showing that a joint feature-structure\nsubgraph similarity metric together with a decoupled embedding extraction benefits from the structural\npatterns inside the graph. Therefore, FOSSIL induces more relevant perturbations. Resulting in a more\naccurate contrastive training. Interestingly, FOSSILv2 has competitive performance in all compared datasets\nwhile being more scalable than many other methods of the literature. Therefore, FOSSILv2 offers a good\ncompromise between performance and scalability. This suggests that our proposed model is a promising\ndirection when scalability is a key factor.\n5.3\nAblation Studies\nWe validate the effectiveness of each component of FOSSIL through a series of ablation studies. First, we\nvalidate the use of the FGWD that captures both feature and graph structure similarities between subgraphs.\nNotably, we demonstrate that adding the node-level contrastive loss consistently improves our downstream\nnode-classification performance. Then, we validate the design of our encoder by testing other alternatives.\nWe also explore other strategies to generate graph perturbations within our framework. Finally, we validate\nthe usage of using node connectivity information for the fusion module ψ(·) to assist the computation of λ.\n5.3.1\nImpact of the subgraph similarity metric\nWe replace the FGWD with other subgraph similarity metrics to evaluate its effectiveness. More precisely,\nwe use other OT distances like WD and GWD. We also use a simple average pooling of the subgraphs’\nnode features followed by a cosine similarity (referred to as Cosim). To properly assess the effect of every\nsimilarity function, we evaluate their performance with and without adding the node-level contrastive loss\nLnode. Table 3 shows the results of this ablation study in all datasets. We first notice that node features are\nmore important than pure graph connectivity for the node classification tasks. Indeed, GWD, which relies\nsolely on subgraph structure similarity, performs worse than WD and Cosim. Secondly, we observe that\nWD performs better than Cosim. This suggests that OT distances are better for self-supervised learning\nin estimating graph similarities than a Readout function followed by a cosine similarity. Nevertheless, both\nWD and Cosim ignore the underlying graph structure. Interestingly, we observe the superior performance\nof using the FGWD that jointly captures the feature and structure similarities between subgraphs. This\naligns with the observations of (Vayer et al., 2018). Finally, we notice that adding a node-level contrastive\nloss consistently improves our node classification results.\n5.3.2\nImpact of encoder design\nWe analyze the effectiveness of our encoder by replacing it with a vanilla GCN and a GCNII (Chen et al.,\n2020a) architecture.\nAs shown in Table 4, the decoupled approach outperforms the standard GCN on\nthe heterophilic datasets. Moreover, GCNII also underperforms our simple encoder. This aligns with the\nobservation of Chen et al. (2024) that encoders specifically designed to work on arbitrary homophily levels\ndo not perform well on current GCL methods. We also note a significant improvement in the performance of\nthe homophilic datasets with the decoupled approach, emphasizing its importance in the FOSSIL framework.\n12\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nTable 4: Ablation study on the encoder method.\nEncoder\nCora\nCiteSeer\nPubMed\nCoAuthor CS\nActor\nChameleon\nSquirrel\nGCN\n74.72±0.49\n61.35±0.45\n74.11±0.60\n90.12±0.06\n13.54±0.94\n48.68±0.52\n25.34±0.28\nGCNII\n50.18±0.44\n46.45±0.40\n68.59±0.51\n88.17±0.09\n36.10±0.16\n44.75±0.33\n33.71±0.22\nDecoupled encoder\n80.02±0.26\n68.24±0.34\n79.76±0.48\n94.49±0.04\n35.61±0.15\n53.03±0.33\n37.13±0.24\nTable 5: Graph perturbation ablation study results on node classification.\nGenerator\nCora\nCiteSeer\nPubMed\nCoAuthor CS\nActor\nChameleon\nSquirrel\nRandom\n78.28±0.35\n65.85±0.39\n76.54±0.42\n89.40±0.27\n28.95±1.58\n57.29±1.34\n38.22±0.96\nGAT-F\n74.47±0.31\n66.84±0.35\n70.25±0.51\n92.72±0.08\n34.68±0.17\n53.69±0.30\n34.53±0.54\nF-GAT-E\n75.53±0.45\n62.72±0.48\n78.33±0.42\n90.60±0.32\n33.68±0.18\n52.68±0.82\n33.82±0.48\nGCN-E\n76.95±0.32\n67.70±0.34\n79.44±0.50\n93.49±1.93\n34.33±0.62\n46.83±0.77\n39.43±0.23\nGAT-E\n80.02±0.26\n68.24±0.34\n79.76±0.48\n94.49±0.04\n35.61±0.15\n53.03±0.33\n37.13±0.24\nTable 6: Ablation study on fusion the design of the fusion\nDesign\nCora\nCiteSeer\nPubMed\nCoAuthor CS\nActor\nChameleon\nSquirrel\nw/o centrality\n80.24±0.25\n67.96±0.29\n79.95±0.37\n95.04±0.04\n35.76±0.60\n54.24±0.48\n36.43±0.27\nw/ degree\n80.02±0.26\n68.24±0.34\n79.76±0.48\n94.49±0.04\n35.61±0.15\n53.03±0.33\n37.13±0.24\nw/ degree normalized-1\n79.44±0.29\n68.77±0.27\n79.36±0.40\n94.78±0.03\n30.48±2.00\n53.00±0.94\n36.79±0.21\nw/ degree normalized-2\n79.68±0.27\n69.77±0.29\n79.70±0.35\n94.39±0.05\n35.16±0.18\n53.94±0.31\n36.97±0.20\nw/ pagerank\n79.14±0.30\n69.63±0.30\n79.45±0.42\n94.34±0.08\n35.94±0.15\n54.84±0.40\n36.77±0.22\nw/ eigenscore\n79.45±0.28\n69.89±0.27\n79.83±0.37\n93.90±2.66\n36.08±0.15\n53.67±0.79\n37.38±0.61\n5.3.3\nImpact of the view generation process\nWe validate the design of the generator of FOSSIL by testing other alternatives.\nWe first replace the\ngenerator with a traditional non-parametric random perturbation scheme (denoted as Random). Specifically,\nwe perform random edge dropping (∼15%) to assess the benefit of a learnable view generation. Moreover, we\ninvestigate whether to apply the GAT on embeddings (GAT-E), initial features (GAT-F), or an intermediate\napproach in which we compute attention weights with the features and directly apply them on the embeddings\n(F-GAT-E). Finally, we replace the GAT with a GCN that is applied to the embeddings (GCN-E). Table 5\nshows the results of this ablation study. First, we notice that both the GCN and GAT models outperform\nthe random perturbation. Even though random perturbation presents competitive results, it requires careful\nfine-tuning of the hyperparameters according to the graph (You et al., 2020).\nSecondly, we notice that\napplying the GAT on initial features does not work well compared to applying it on the node embeddings.\nWe believe that applying the GAT on node features in our framework is equivalent to learning a different\nencoder, which is not aligned with the intended view generation. The intermediate approach (F-GAT-E)\nalso yields competitive results but still underperforms (GAT-E), suggesting that this approach is less aligned\nwith the intended view generation compared to (GAT-E). However, applying the GAT on node embeddings\nis more aligned with the initial intuition to generate similar views. Finally, we observe that GAT-E provides\nbetter results than GCN-E since the attention mechanism provides flexibility during contrastive learning.\n5.3.4\nAblation on the design of the fusion module\nWe incorporate the degree information of nodes in our fusion module. Intuitively, this brings useful structural\ninformation on nodes inside the graph. To validate this intuition, we explore different alternatives. More\nprecisely, we remove the degree information and test other node centralities: scaling degrees with N −1\n(normalized-1) or by the sum of degrees (normalized-2), PageRank (Page et al., 1999) and Eigenvector\n(Burr, 1982) centralities. Table 6 shows the results of this study. We observe that removing the node degree\noutperforms the degree centrality in some datasets.\nNevertheless, we further observe that other global\ncentrality measures like Eigenscore or PageRank outperform the baseline in several cases. Therefore, the\ndegree of centrality offers a good trade-off between complexity and performance.\n13\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\n6\nConclusion\nIn this paper, we introduced a novel subgraph-based contrastive learning framework utilizing the Fused\nGromov-Wasserstein distance to measure subgraph similarities. Drawing inspiration from recent advance-\nments in addressing heterophily with GNNs, we implemented a decoupled embedding extraction strategy,\nrendering our method agnostic to the graph’s homophily level. This approach allowed us to independently\ngenerate feature and structure perturbations adaptively.\nOur extensive experimental evaluation demon-\nstrated that our method outperforms or has competitive performance against state-of-the-art frameworks\nacross multiple benchmark datasets with varying levels of homophily. Our findings also highlight the chal-\nlenges posed by heterophilic datasets. We suggest future research avenues, such as integrating a high-pass\nfeature extractor to enhance performance in such scenarios. Similarly, extending FOSSIL to other down-\nstream tasks such as link prediction or graph classification deserves further research.\nAcknowledgments\nWe thank the anonymous reviewers for their suggestions and feedback. This work was supported in part by\nANR (French National Research Agency) under the JCJC projects GraphIA (ANR-20-CE23-0009-01), and\nDeSNAP (ANR-24-CE23-1895-01).\nReferences\nSami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina Ler-\nman, Greg Ver Steeg, and A. G. Galstyan. MixHop: Higher-order graph convolutional architectures via\nsparsified neighborhood mixing. In International Conference on Machine Learning, 2019.\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-\ngeneration hyperparameter optimization framework. ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, 2019.\nAlexei Baevski, Henry Zhou, Abdel rahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for\nself-supervised learning of speech representations. In Advances in Neural Information Processing Systems,\n2020.\nBeatrice Bevilacqua, Moshe Eliasof, Eli A. Meirom, Bruno Ribeiro, and Haggai Maron. Efficient subgraph\nGNNs by learning effective selection policies. In International Conference on Learning Representations,\n2024.\nLuc Brogat-Motte, Rémi Flamary, Céline Brouard, Juho Rousu, and Florence d’Alché Buc. Learning to\npredict graphs with fused Gromov-Wasserstein barycenters.\nIn International Conference on Machine\nLearning, 2022.\nStefan A. Burr. The mmthematics of networks. 1982.\nJingyu Chen, Runlin Lei, and Zhewei Wei. PolyGCL: Graph contrastive learning via learnable spectral\npolynomial filters. In International Conference on Learning Representations, 2024.\nMing Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional\nnetworks. In International Conference on Machine Learning, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. In International Conference on Machine Learning, 2020b.\nEli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized PageRank graph\nneural network. In International Conference on Learning Representations, 2021.\nMichaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs\nwith fast localized spectral filtering. In Advances in Neural Information Processing Systems, 2016.\n14\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In North American Chapter of the Association for\nComputational Linguistics, 2019.\nMatthias Fey and Jan Eric Lenssen. Fast graph representation learning with PyTorch Geometric. In Inter-\nnational Conference on Learning Representations - Workshops, 2019.\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message\npassing for quantum chemistry. In International Conference on Machine Learning, 2017.\nJhony H. Giraldo, Konstantinos Skianis, Thierry Bouwmans, and Fragkiskos D. Malliaros. On the trade-off\nbetween over-smoothing and over-squashing in deep graph neural networks. ACM International Conference\non Information and Knowledge Management, 2023.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\nAdvances in Neural Information Processing Systems, 2017.\nYuehui Han, Le Hui, Haobo Jiang, Jianjun Qian, and Jin Xie.\nGenerative subgraph contrast for self-\nsupervised graph representation learning. In European Conference on Computer Vision, 2022.\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification. IEEE/CVF International Conference on Computer Vision, pp.\n1026–1034, 2015.\nMingguo He, Zhewei Wei, and Ji-Rong Wen. Convolutional neural networks on graphs with Chebyshev\napproximation, revisited. In Advances in Neural Information Processing Systems, 2022.\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and\nJure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Advances in Neural\nInformation Processing Systems, 2020a.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. GPT-GNN: Generative pre-\ntraining of graph neural networks. ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, 2020b.\nElvin Isufi, Fernando Gama, David I Shuman, and Santiago Segarra. Graph filters for signal processing and\nmachine learning on graphs. IEEE Transactions on Signal Processing, 2024.\nYizhu Jiao, Yun Xiong, Jiawei Zhang, Yao Zhang, Tianqi Zhang, and Yangyong Zhu. Sub-graph contrast for\nscalable self-supervised graph representation learning. IEEE International Conference on Data Mining,\n2020.\nNikhil S. Ketkar. Deep learning with Python. In Apress, 2017.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Confer-\nence on Learning Representations, 2015.\nThomas Kipf and Max Welling.\nVariational graph auto-encoders.\nIn Advances in Neural Information\nProcessing Systems - Workshops, 2016.\nThomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Inter-\nnational Conference on Learning Representations, 2017.\nJohannes Klicpera, Stefan Weißenberger, and Stephan Günnemann. Diffusion improves graph learning. In\nAdvances in Neural Information Processing Systems, 2019.\nLecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, and Muhan Zhang. MAG-GNN: Rein-\nforcement learning boosted graph neural network. In Advances in Neural Information Processing Systems,\n2023.\n15\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman Mohamed, Omer Levy, Veselin\nStoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In Association for Computational Linguistics, 2019.\nJiajin Li, Jianheng Tang, Lemin Kong, Huikang Liu, Jia Li, Anthony Man-Cho So, and José H. Blanchet.\nA convergent single-loop algorithm for relaxation of Gromov-Wasserstein in graph data. In International\nConference on Learning Representations, 2023.\nRalph Linsker. Self-organization in a perceptual network. Computer, 21:105–117, 1988.\nYunhui Liu, Huaisong Zhang, Tieke He, Tao Zheng, and Jianhua Zhao. Bootstrap latents of nodes and\nneighbors for graph self-supervised learning. In European Conference on Machine Learning and Knowledge\nDiscovery in Databases, 2024.\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiaoming Chang, and\nDoina Precup.\nRevisiting heterophily for graph neural networks.\nIn Advances in Neural Information\nProcessing Systems, 2022.\nAndrew McCallum, Kamal Nigam, Jason D. M. Rennie, and Kristie Seymore. Automating the construction\nof internet portals with machine learning. Information Retrieval, 3:127–163, 2000.\nFacundo Mémoli. Gromov–Wasserstein distances and the metric approach to object matching. Foundations\nof Computational Mathematics, 11:417–487, 2011.\nGalileo Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven active surveying for collective\nclassification. In International Workshop on Mining and Learning with Graphs, 2012.\nMathias Niepert, Pasquale Minervini, and Luca Franceschi. Implicit MLE: Backpropagating through discrete\nexponential family distributions. In Advances in Neural Information Processing Systems, 2021.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using\nvariational divergence minimization. In Advances in Neural Information Processing Systems, 2016.\nHoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters. ArXiv,\nabs/1905.09550, 2019.\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank citation ranking: Bring-\ning order to the web. In The Web Conference, 1999.\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-GCN: Geometric graph\nconvolutional networks. In International Conference on Learning Representations, 2020.\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie\nTang. GCC: Graph contrastive coding for graph neural network pre-training. ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 2020.\nBenedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal of\nComplex Networks, 9(2):cnab014, 2021.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collec-\ntive classification in network data. volume 29, pp. 93–93, 2008.\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph\nneural network evaluation. In Advances in Neural Information Processing Systems - Workshops, 2018.\nJie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks. In ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, 2009.\nShantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L. Dyer, Rémi Munos,\nPetar Veličković, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In\nInternational Conference on Learning Representations, 2021.\n16\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nNenad Tomasev, Ioana Bica, Brian McWilliams, Lars Buesing, Razvan Pascanu, Charles Blundell, and\nJovana Mitrovic. Pushing the limits of self-supervised ResNets: Can we outperform supervised learning\nwithout labels on ImageNet? In International Conference on Machine Learning - Workshops, 2022.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals.\nRepresentation learning with contrastive predictive\ncoding. ArXiv, abs/1807.03748, 2018.\nTitouan Vayer, Nicolas Courty, Romain Tavenard, Laetitia Chapel, and Rémi Flamary. Optimal transport\nfor structured data with application on graphs. In International Conference on Machine Learning, 2018.\nPetar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.\nGraph attention networks. In International Conference on Learning Representations, 2018.\nPetar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R. Devon Hjelm.\nDeep graph infomax. In International Conference on Learning Representations, 2019.\nCédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.\nHaonan Wang, Jieyu Zhang, Qi Zhu, and Weitao Huang. Augmentation-free graph contrastive learning.\nArXiv, abs/2204.04874, 2022.\nShuhei Watanabe. Tree-structured parzen estimator: Understanding its algorithm components and their\nroles for better empirical performance. ArXiv, abs/2304.11127, 2023.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive\nsurvey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):\n4–24, 2021.\nTeng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang. Decoupled self-supervised\nlearning for non-homophilous graphs. In Advances in Neural Information Processing Systems, 2022.\nYaochen Xie, Zhao Xu, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks:\nA unified review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45:2412–2429, 2021.\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive\nlearning with augmentations. In Advances in Neural Information Processing Systems, 2020.\nMengyi Yuan, Minjie Chen, and Xiangci Li. MUSE: Multi-view contrastive learning for heterophilic graphs.\nACM International Conference on Information and Knowledge Management, 2023.\nXuan Zang, Xianbing Zhao, and Buzhou Tang. Hierarchical molecular graph self-supervised learning for\nproperty prediction. Communications Chemistry, 6(1):34, 2023.\nJiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily\nin graph neural networks: Current limitations and effective designs. In Advances in Neural Information\nProcessing Systems, 2020a.\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive represen-\ntation learning. In International Conference on Machine Learning - Workshops, 2020b.\n17\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\nA\nAlgorithm for FGWD\nAlgorithm 1 Bregman Alternated Projected Gradient (BAPG) for FGWD\nP(0) ←µν⊤\n∆P ←∞\ni ←1\nwhile (i < T) And (∆P > ϵ) do\n// Row updating\nG ←αM + 2(1 −α)L(C1, C2) ⊗P(i−1) // Gradient of the objective function of Eq. (1) in P(i−1)\n˜P ←P(i−1) ⊙exp(−G/β)\n˜P ←diag(\nµ\n˜P1m )˜P\n// Column updating\nG ←αM + 2(1 −α)L(C1, C2) ⊗˜P // Gradient of the objective function of Eq. (1) in ˜P\nP(i) ←˜P ⊙exp(−G/β)\nP(i) ←P(i) diag(\nµ\nP(i)⊤1n )\n∆P ←∥P(i) −P(i−1)∥\ni ←i + 1\nend while\nB\nEvaluation on Larger Graphs\nWe compare FOSSILv2 with some baselines on two large-scale graphs: OGBN-Arxiv and OGBN-Proteins,\ncontaining 1M and 30M edges, respectively (Hu et al., 2020a). For OGBN-Proteins, we extract a subgraph\nof 1M edges due to computational resource limitations. To this end, we use the NeigbhorLoader loader of\nPyG to sample, for some nodes, 18 neighbors in each layer of its 3-hop neighborhood (Hamilton et al., 2017).\nAny number above 18 results in a number of edges that lead to a memory overflow with our method and\nall baselines. We show the results in Table 7. Some baselines present in Table 2 result in memory overflow,\ntherefore, they are not included in this experiment. We do not show results for DSSL in OGBN-Proteins\nbecause it does not work for graphs where we have disconnected regions. We observe that our method is\ncompetitive against the baselines.\nTable 7: Test accuracy on the node classification on large-scale graphs. The best results are highlighted in\nbold, while the second best-performing methods are underlined.\nMethod\nOGBN-Arxiv\nOGBN-Proteins\nFS-GCN\n30.75±0.32\n88.67±0.01\nFS-MLP\n23.16±0.58\n88.70±0.01\nGSC\n35.19±0.36\n88.87±0.01\nGSC+Cheb\n34.97±0.53\n88.64±0.01\nDSSL\n58.36±2.69\n−\nBGRL\n54.16±0.30\n89.32±0.01\nSubg-Con\n60.82±0.08\n89.04±0.01\nFOSSILv2\n56.22±0.12\n88.63±0.01\nC\nSensibility Analysis\nOne of the most important hyperparameters of FOSSIL is α, which dictates how much our model should\nfocus on each source of information of the input graph, namely, node features and graph structure. To assess\nhow sensitive our framework is to the value of α, we vary its value and see how the performance changes.\n18\n\n\nPublished in Transactions on Machine Learning Research (03/2025)\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n74\n76\n78\n80\nAccuracy\nCora\n(a) Cora\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n40\n50\n60\nAccuracy\nCiteSeer\n(b) Citeseer\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n55\n60\n65\n70\n75\nAccuracy\nPubMed\n(c) PubMed\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n40\n60\n80\n100\nAccuracy\nCoAuthor CS\n(d) CoAuthor-CS\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n15\n20\n25\n30\n35\nAccuracy\nActor\n(e) Actor\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n48\n50\n52\n54\nAccuracy\nChameleon\n(f) Chameleon\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n31\n32\n33\n34\n35\nAccuracy\nSquirrel\n(g) Squirrel\nFigure 3: Variation of the accuracy for several datasets w.r.t. α.\nSpecifically for each real-world dataset in our experiments, we fix the other hyperparameters and vary α\nwithin the set {[0 : 0.1 : 1]}. For each value of α, we evaluate our method on 5 seeds and report the mean and\nstandard deviation. Figure 3 shows the sensibility analysis results. We notice that the performance of our\nmethod can significantly change depending on the value of α. Overall, for homophilic graphs, the best value\nof α tends to be close to 0.5. This suggests that node features and graph structure are equally important for\nthe encoder to characterize the graph. However, for heterophilic graphs, the best value of α tends to be close\nto 1. A plausible explanation is that even though the graph structure is important, the GCN encoder is not\nable to correctly exploit the structure of heterophilic graphs due to its intrinsic homophily assumption; our\nframework will rely more on node features. Hence, our encoder will converge to an MLP with a value of α\nclose to 1. This further supports our idea to include a feature extractor that is more suited to heterophilic\ngraphs for future research.\n19\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20885v1.pdf",
    "total_pages": 19,
    "title": "A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning",
    "authors": [
      "Amadou S. Sangare",
      "Nicolas Dunou",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ],
    "abstract": "Self-supervised learning has become a key method for training deep learning\nmodels when labeled data is scarce or unavailable. While graph machine learning\nholds great promise across various domains, the design of effective pretext\ntasks for self-supervised graph representation learning remains challenging.\nContrastive learning, a popular approach in graph self-supervised learning,\nleverages positive and negative pairs to compute a contrastive loss function.\nHowever, current graph contrastive learning methods often struggle to fully use\nstructural patterns and node similarities. To address these issues, we present\na new method called Fused Gromov Wasserstein Subgraph Contrastive Learning\n(FOSSIL). Our model integrates node-level and subgraph-level contrastive\nlearning, seamlessly combining a standard node-level contrastive loss with the\nFused Gromov-Wasserstein distance. This combination helps our method capture\nboth node features and graph structure together. Importantly, our approach\nworks well with both homophilic and heterophilic graphs and can dynamically\ncreate views for generating positive and negative pairs. Through extensive\nexperiments on benchmark graph datasets, we show that FOSSIL outperforms or\nachieves competitive performance compared to current state-of-the-art methods.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}