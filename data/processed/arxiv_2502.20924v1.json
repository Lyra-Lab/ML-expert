{
  "id": "arxiv_2502.20924v1",
  "text": "Decoder Gradient Shield: Provable and High-Fidelity Prevention of\nGradient-Based Box-Free Watermark Removal\nHaonan An1\nGuang Hua2*\nZhengru Fang1\nGuowen Xu3\nSusanto Rahardja2\nYuguang Fang1\n1Department of Computer Science, City University of Hong Kong, Hong Kong\n2Infocomm Technology (ICT) Cluster, Singapore Institute of Technology (SIT), Singapore 138683\n3School of Computer Science and Engineering, University of Electronic Science and Technology of China\nAbstract\nThe intellectual property of deep image-to-image models can\nbe protected by the so-called box-free watermarking. It uses\nan encoder and a decoder, respectively, to embed into and\nextract from the model’s output images invisible copyright\nmarks. Prior works have improved watermark robustness,\nfocusing on the design of better watermark encoders. In this\npaper, we reveal an overlooked vulnerability of the unpro-\ntected watermark decoder which is jointly trained with the\nencoder and can be exploited to train a watermark removal\nnetwork. To defend against such an attack, we propose the\ndecoder gradient shield (DGS) as a protection layer in the\ndecoder API to prevent gradient-based watermark removal\nwith a closed-form solution. The fundamental idea is in-\nspired by the classical adversarial attack, but is utilized for\nthe first time as a defensive mechanism in the box-free model\nwatermarking. We then demonstrate that DGS can reorient\nand rescale the gradient directions of watermarked queries\nand stop the watermark remover’s training loss from con-\nverging to the level without DGS, while retaining decoder\noutput image quality. Experimental results verify the effec-\ntiveness of proposed method. Code of paper will be made\navailable upon acceptance.\n1. Introduction\nToday’s deep learning models can provide exceptional per-\nformance across a wide range of tasks, even surpassing hu-\nman capability [2, 6, 7, 10, 39]. However, these resource-\nintensive models are also subject to the risk of intellectual\nproperty infringement. To address this problem, model wa-\ntermarking has been developed to verify model ownership\nor detect model theft.\nAccording to how the watermark is extracted, model wa-\ntermarking can be classified into white-box, black-box, and\n*Equal Contribution\nWatermark Encoder 𝔼\nImage-to-Image \nModel 𝕄\n𝑋0\n𝑊\n𝑋\n𝑌\n𝑊\n𝑊0\nBlack Box API with 𝕄 Protected \n+\nWatermark Decoder 𝔻\nBlack Box API\nFigure 1. Flowchart of box-free model watermarking for image-\nto-image models. The thin black arrows represent the black-box\nquerying flow (processing and watermarking), while the thick col-\nored arrows represent potential watermark extraction, and each\ncolored arrow pair corresponds to a single input-output pair for D.\nbox-free methods. White-box methods [20, 30, 31] require\naccess to the protected model’s internal content in which the\nwatermark is encoded. Black-box methods [1, 11, 15], also\nknown as backdoor watermarking, require querying the pro-\ntected model for watermark extraction since they encode the\nwatermark into the model’s input-output mapping. Box-free\nmethods, however, extract the watermark directly from the\nprotected model’s outputs, which are more flexible and are\nspecially suitable for models that generate high-entropic con-\ntent, e.g., encoder-decoder image models [33, 37, 38] and\ngenerative adversarial networks (GANs) [8, 17]. Since only\nbox-free methods embed watermarks into generated outputs,\nthey present a viable solution to the growing demand for\nAI-generated content attribution.\nThe typical workflow of box-free watermarking for image-\nto-image models is depicted in Figure 1 [37]. Considering\na model M that takes an input X0 ∈X0 and generates an\narXiv:2502.20924v1  [cs.CV]  28 Feb 2025\n\n\n𝑋𝑋0\n𝑌𝑌\nℝ(𝑌𝑌)\n𝑍𝑍\nRemoval Loss\nℒR =\n𝑍𝑍−𝑊𝑊0 2\n2\nRemover ℝ \nQuery\nLoss Calculation\nGradient\nEstimation\nImage-to-Image\nBlack Box API\nWatermark\nDecoder 𝔻𝔻\nBlack Box API\nQuery\nFigure 2. Flowchart of gradient-based removal attack. The gradient\nbackpropagated from D can be estimated by leveraging black-box\nadversarial attacks. In our setting, however, we assume the attacker\ncan directly obtain the gradient without estimation.\noutput X ∈X, box-free watermarking creates a watermark\nencoder E which embeds a copyright mark image W into\nX and yields the watermarked image Y ∈Y. A dedicated\nwatermark decoder D is jointly created and can extract the\nmark W from the watermarked set Y or a null-mark W0\nfrom the non-watermarked complement set Y∁. According\nto the specific image task, (X0, X) can be (noisy, denoised),\n(original style, transferred style), etc. In the above process,\nthe model M is protected by only providing Y , instead of X,\nto the user. It has been verified that if the attacker uses the\ncollection of X0 and Y to train a surrogate model, W can\nstill be extracted by D from the surrogate output images [37].\nWe refer to M as either the protected model or the victim\nmodel where applicable.\nThe attacker has the freedom to alter Y with the hope\nof removing the watermark while preserving image qual-\nity, prior to surrogate training. Intuitively, the alteration\ncan be compression, noise addition, flipping, cropping, etc.\nAlthough this vulnerability can be mitigated by adding an\naugmentation layer between Y and D when training E and D\n[38], the attacker can launch a more advanced removal attack\nby training a removal network R, as shown in Figure 2. Ad-\nversarial attack literature [4, 13] has shown that the gradient\nof D can be estimated via black-box queries, and the gradient\ncan then be used to train R. Such a gradient-based attack is\nfeasible because D is jointly trained with E and contains the\nwatermarking mechanism that can be compensated.\nIn this paper, we first show that the above gradient-\nbased attack can remove state-of-the-art box-free water-\nmarks. Then, under the practical threat model with the\ngradient information of D assumed to be observable to the\nattacker, we propose a more advanced defense mechanism\ncalled decoder gradient shield (DGS). For non-watermarked\nqueries, the black-box API of D returns the null watermark\noutput (close to the all-white image W0) to the users. For\nwatermarked queries, DGS reorients and rescales the corre-\nsponding gradient so that when the gradient is backpropa-\ngated and used to train R, the training loss will not be able\nto converge to the level without DGS protection. Such reori-\nentation is realized by adding specially crafted perturbations\non the output of D while retaining output image quality. No-\ntably, our approach could yield a neat closed-form solution,\nwhich is distinct from existing defense or related solutions.\nOur contributions are summarized as follows.\n• We reveal the vulnerability of the unprotected watermark\ndecoder in existing box-free watermarking methods, that\nis, the decoder can be exploited via black-box access to\ntrain a watermark removal network to remove existing\nbox-free watermarks.\n• We propose a novel DGS framework to prevent model\nextraction from exploiting the watermark decoder. This is\ndifferent from the existing post-hoc methods that extract\nwatermarks from successful surrogates.\n• We provide both the closed-form solution and extensive\nexperimental results to verify and demonstrate the effec-\ntiveness of our proposed method.\n2. Related Work\n2.1. Model Extraction\nModel extraction, also known as model stealing or surro-\ngate attack, aims to replicate a victim model’s functionality,\nin which the attacker curates a set of query data, and with\nthe victim model returned query outputs, trains a surrogate\nmodel. Previous research mainly focused on stealing deep\nclassification models, in which the surrogate query data can\nbe public data [24], evolutionary proxy data [3], or even syn-\nthetic noise [14, 29], and the attack can be successful even\nif the victim model only returns hard labels [27]. Model ex-\ntraction can also be launched against self-supervised image\nencoder [28] and ensemble models [21].\n2.2. Box-Free Watermarking\nBox-free watermarking is so named because the watermark\nis extracted from the model outputs using a dedicated wa-\ntermark decoder, not requiring the protected model. The\nwatermark decoder can be (1) pretrained and frozen when\nfine-tuning the protected model for watermark embedding\n[8, 17], (2) jointly trained with the protected model [19, 33],\nor (3) a post-hoc model not coupled with the protected model\n[37, 38]. Since the watermarks are embedded in the pro-\ntected model outputs, box-free methods are commonly ap-\nplied to generative models with high entropic image outputs.\n2.3. Watermark Removal\nWatermark removal in the context of box-free watermarking\nis similar yet different from the conventional image water-\nmarking. Through the alteration of victim model returned\n\n\noutputs before surrogate training, the removal attack aims to\nensure that the watermark cannot be extracted from surro-\ngate model generated images. Such alteration can be either\nnormal image augmentation [37] or a specially designed pro-\ncess such as inpainting [18] or an image-to-image watermark\nremover. These removers are also constrained to preserve\nthe image quality for effective surrogate training.\n2.4. Gradient Attack and Defense\nThe attack on neural network gradient has been researched\nfor a decade and is mainly for generating adversarial exam-\nples. It can be a white-box attack such as the classic fast\ngradient sign method (FGSM) [12] and projected gradient\ndescent (PGD) [22]. Under the more practical black-box set-\nting, the gradient to be attacked can be estimated via query-\ning [13], while the required query times can be substantially\nreduced [4]. These results serve as the foundation of our\nthreat model assuming observable gradients of the black-box\ndecoder to the attacker. In model watermarking, gradient\nalteration has instead been utilized as a defense approach to\nprotect the extraction of classification models [23, 25], but it\ncannot withstand hard-label based extraction [27]. We note\nthat our work is the first to incorporate gradient alteration in\nprotecting image-to-image models.\n3. Problem Formulation\n3.1. Box-free Model Watermarking\nSince all box-free watermarking methods share the same\nextraction process using D, without loss of generality, we\nconsider the post-hoc type [37, 38] as our watermarking\nmodel, whose workflow is depicted in Figure 1. It contains\n• Image Processing: X ≜M(X0),\n• Watermark Embedding: Y ≜E(Concat(X, W)),\n• Watermark Extraction: D : Y →W and Y∁→W0,\nwhere Concat(·) denotes the channel-wise concatenation\nand X0, X ⊂Y∁. With M untouched, E and D are jointly\ntrained by minimizing\nLVictim = α1LEmbed + α2LFidelity,\n(1)\nwhere α1 and α2 are weighting parameters,\nLEmbed =\nX\nY ∈Y\n∥D (Y ) −W∥2\n2 +\nX\nS∈Y∁\n∥D (S) −W0∥2\n2,\n(2)\nLFidelity =\nX\nY ∈Y,X∈X\n∥Y −X∥2\n2,\n(3)\nand S denotes an arbitrary image. Note that E is indirectly\nexpressed by its output Y in (2) and (3), which facilitates our\nsubsequent attack formulation in which E is inaccessible.\n3.2. Threat Model\nDefender. We consider the owner of M as the defender,\nwho not only trains M but also implements box-free water-\nmarking and owns E and D. The defender aims to extract\nwatermarks in surrogate model generated images or prevent\nsurrogate training, i.e., incurring non-negligible performance\ndegradation in surrogate models. As shown in Figure 1, the\ndefender only provides the black-box API which accepts\nquery X0 and only returns Y , while M and E are strictly\nprivate. The black-box API of D is also provided to accept\nwatermark verification queries.\nAttacker. On the other side, the attacker aims to ex-\ntract M with a watermark-removed surrogate model. To\nachieve so, the attacker curates a set of X0 to query the vic-\ntim model and obtains a set of Y , which is identical to how\nnormal users behave. Then, prior to surrogate training, the\nattacker alters Y for watermark removal while preserving\nimage quality. Meanwhile, the attacker can query D to check\nif an image contains the defender-embedded watermark. In\naddition, we assume that the attacker can observe the gra-\ndient backpropagated from the output of D, thanks to the\nadversarial attack literature. Furthermore, if the watermark\nremains intact despite attempts by a removal-loss-minimized\nremover, the attacker might reasonably infer the presence\nof defender-imposed gradient perturbations and adjust the\nreturned gradient as a countermeasure.\n3.3. Gradient-based Removal Attack\nWe now formulate the gradient-based box-free watermark\nremoval attack depicted in Figure 2. The rationale behind\nis that the attacker can deploy an inverse of E, i.e., another\nimage-to-image network R, which takes in Y and undoes\nwatermark embedding. This can be achieved by using D as\na watermark verifier and an all-white null watermark W0 as\nthe supervision signal, leading to the minimization problem\nwith the loss function\nLAttack = β1LRemoval + β2LA\nFidelity,\n(4)\nwhere β1 and β2 are the weighting parameters,\nLRemoval = ∥D[R(Y )] −W0∥2\n2,\n(5)\nLA\nFidelity = ∥R(Y ) −Y ∥2\n2.\n(6)\nThe removal loss ensures that the altered Y , i.e., R(Y ), does\nnot contain the watermark, while the fidelity loss ensures the\npreserved image quality for subsequent surrogate training.\nTo minimize (5), it holds that\n∇LRemoval ∝∂LRemoval\n∂D [R(Y )]\n∂D [R(Y )]\n∂R(Y )\n,\n(7)\nwhere ∇LRemoval is the gradient component backpropagated\nto update R parameters. The ℓ2-norm loss function is used\n\n\nin (5) in this paper, although other loss functions can also\nbe possible. Under our threat model, the gradient of D in\n(7), ∂D [R(Y )]/∂R(Y ), is observable, and all other gradient\ncomponents are determined (known) by the attacker. There-\nfore, R can be effectively trained in the black-box setting.\nWe note that the above attack resembles the classical adap-\ntive filter for model inversion [9] where R is the inverse of\nE. Given the black-box access of D and without protection,\nthis attack has a theoretical guarantee of convergence.\n4. The Proposed Decoder Gradient Shield\nThe black-box setting of D enables the defender to alter its\noutput before returning to the user for protection purposes,\nwhile the alteration is subject to the constraint of not affect-\ning the watermark extraction functionality. This is similar\nto API poisoning based defense for black-box (backdoor)\nwatermarking [35], but it is formulated herein for box-free\nwatermarking. The proposed DGS is derived as follows.\n4.1. Gradient Reorientation\nFor the ease of presentation, let Z ≜D[R(Y )] (see Figure 2)\nand let Z∗be the altered output that is returned to the user.\nAccording to (5) and (7), we have\n∇LRemoval ∝∂LRemoval\n∂R(Y )\n= 2(Z −W0)T\n∂Z\n∂R(Y ),\n(8)\nwhere {·}T is the transpose operator. The training of R\nrequires the curation of a set of Y obtained from querying\nthe encapsulated M and E using a set of X0. During the\ninitial training stage, R is not able to remove the watermark,\nyielding Z ≈W. To prevent Z from eventually converging\nto W0, we can reorient Z into Z∗when Z ≈W, so the true\ngradient direction can be protected. According to (8), the\nperturbed gradient component can be expressed as\n\u0012∂LRemoval\n∂R(Y )\n\u0013∗\n= 2(Z∗−W0)T\n∂Z∗\n∂R(Y ),\n(9)\nwhich can be designed in such a way that the direction\nchange is between 90 and 180 degree, i.e.,\n(Z∗−W0)T\n∂Z∗\n∂R(Y ) = −(Z −W0)T P\n∂Z\n∂R(Y )\n⇒(Z∗−W0)T ∂Z∗\n∂Z = −(Z −W0)T P,\n(10)\nwhere the chain rule is applied to cancel ∂Z/∂R(Y ) and P\nis a positive definite matrix. Note that if P is the identity\nmatrix I, then the above reorientation is simply gradient sign\nflipping (180 degree). Meanwhile, to preserve the output\nimage quality, it is required that\nZ∗≈W.\n(11)\nNote that (10) is a first-order differential equation, and the\nsolution for Z∗has the following form\nZ∗= −PZ + C,\n(12)\nwhere C is independent of Z. To solve for C, substitute (12)\ninto (11), then we have\n−PZ + C ≈W ⇒C ≈(P + I)W,\n(13)\nand substituting (13) into (12) yields\nZ∗= −PZ + (P + I)W,\n(14)\nwhere the approximation is replaced by equality for imple-\nmentation. The gradient reorientation in (14) is the essential\ncomponent in the proposed DGS.\n4.2. DGS Protected Decoder API\nWe now describe how the proposed gradient reorientation is\nincorporated in the API of decoder D, as depicted in Figure 3.\nSince D serves as a black-box watermark verifier to end users,\nwe first discuss possible situations of its query denoted by S\nwithout deploying DGS.\nSituation 1: S = Y = E(Concat(M(X0), W)) ∈Y.\nThis means that S is a processed and watermarked image\nfrom the black-box API of M and it corresponds to the\nbenign query for watermark extraction. It then follows from\nSection 3.1 that it returns D(S) = D(Y ) ≈W.\nSituation 2: S = R(Y ). This is the malicious query\nfor gradient-based removal attack, but it is indistinguishable\nfrom Situation 1 because (6) ensures that R(Y ) is semanti-\ncally identical to Y . However, since the true gradient has not\nbeen returned for R to learn watermark removal, the initial\nmalicious query follows that S = R(Y ) ∈Y, and according\nto Section 4.1, it returns D(S) = D[R(Y )] = Z ≈W.\nSituation 3: S ∈Y∁. This corresponds to the benign\nquery with a non-watermarked image, and it follows from\nSection 3.1 that D(Y∁) ≈W0 is returned.\nAccording to the above situations and incorporating (14),\nwe can summarize the response mechanism of DGS pro-\ntected D API, denoted by D∗(S), as\nD∗(S) =\n\u001a\n−PD(S) + (P + I)W,\nif NC(D(S), W) > 0.96,\nD(S),\notherwise,\n(15)\nwhere NC(·, ·) is the normalized cross-correlation function,\nand we determine D(S) ≈W using the threshold of 0.96\n[37]. The above mechanism means that for Situations 1\nand 2, since they are indistinguishable, the API returns the\ngradient reoriented output, while for Situation 3, the API\nsimply returns the original output. Note that the gradient\nreorientation in (15) only adds imperceptible perturbations\non the extracted watermark W, which does not affect the\n\n\nWatermark Decoder 𝔻𝔻\nReorientation\nIf ≈𝑊𝑊\nOtherwise\nDecoder Gradient Shield\n𝑊𝑊\n𝑃𝑃\n𝑆𝑆\n𝔻𝔻(𝑆𝑆)\nBlack-Box API\n𝔻𝔻\n∗(𝑆𝑆)\nFigure 3. Flowchart of the proposed DGS in the black-box API of D.\nwatermark verification, so its influence on normal users is\nnegligible. However, such perturbation can effectively pre-\nvent the attacker from training R for watermark removal.\nWe note that (15) is the first solution to protecting the\ndecoder D in box-free watermarking so as to prevent black-\nbox model extraction. It is also a neat closed-form solution\ncompared to gradient-based defense in other contexts, e.g.,\nthe recursive methods called prediction poisoning [25] and\ngradient redirection [23], for protecting deep classification\nmodels.\n4.3. Choice of P\nThe positive definite matrix P introduced in (10) is an essen-\ntial component in the proposed reorientation process. The\nrisk of omitting P, or equivalently setting P = I, is that\nthe attacker can simply flip the gradient sign back to obtain\nthe true gradient. In the situation where P ̸= I, let the\neigendecomposition be P = QT ΛQ, where Λ is the diag-\nonal eigenvalue matrix and Q the eigenvector orthonormal\nmatrix, then for a vector multiplied at its right-hand side, it\nfirst rotates the vector via Q, followed by scaling the vector\nelements by the eigenvalues in Λ and finally reverting the\nrotation via QT . The rotation imposed by Q is compensated\nin this process, but the rotation indirectly caused by scaling\nusing Λ is not. For example, as just one use case, we consider\nQ = I and thus P = Λ is a diagonal matrix with all-positive\nelements. As long as these elements are unequal, −P in\n(10) can incur a 90 to 180 degree rotation. Note that the\nattacker can still flip the gradient sign, while this will result\nin a gradient deviated by 0 to 90 degree. To further prevent\nthe learning of R, we can reduce the gradient norm by setting\n0 < Λi ≪1, where Λi is the ith diagonal element, so that\neven the attacker succeeds in recovering the true gradient\ndirection, the rate of learning becomes negligibly small.\n5. Experimental Results\nIn this section, we present the experimental results to verify\nthe effectiveness of our proposed DGS. We select the state-\nof-the-art box-free watermarking model [38] to implement\nour defense. Notably, [38] is an extended version of [37],\naddressing the vulnerabilities of their watermarking scheme\nto image augmentation attacks, while both share the same\nwatermark encoder and decoder.\n5.1. Experimental Setting\n5.1.1. Datasets\nWe consider two representative image-to-image tasks, i.e.,\nimage deraining (classic low-level image processing) and\nstyle transfer (a high-level computer vision task). For both\ntasks, we use the PASCAL VOC dataset [5] with different\ndata splits. For image deraining, the data corresponds to X\nin Figure 1. We uniformly split the 12, 000 training images\ninto two equal parts, each containing 6, 000 for victim model\ntraining and remover training, respectively. Rainy images\ncorresponding to X0 in Figure 1 are generated using the algo-\nrithm in [34]. For style transfer, the PASCAL VOC data are\ntreated as X0 and similarly divided for victim and remover\ntraining. The style transfer algorithm in [16] is employed\nto generate X. Additionally, to reduce the computational\ncomplexity, we resize all images to 256 × 256 grayscale.\n5.1.2. Metric\nFor fidelity evaluation, we use peak signal-to-noise ratio\n(PSNR) and multi-scale structural similarity index (MS-\nSSIM) [32] to measure the similarity between two images.\nFor robustness evaluation, we use the success rate of defense,\ndenoted by SR, which is the ratio of the number of images\nwith embedded watermarks successfully extracted over the\ntotal number of watermarked images under going through\nremoval attacks.\n5.1.3. Implementation Detail\nWe follow the model architecture, hyperparameters, and\ntraining process of victim model in [38], including M, E,\nand D. The gradient-based remover R is implemented using\na UNet architecture [26]. Both models are trained from\nscratch for 100 epochs using the Adam optimizer with a\n\n\n(a) ℓ1 loss\n(b) ℓ2 loss\n(c) ℓ2 and consistent losses\n(d) ℓ1 loss\n(e) ℓ2 loss\n(f) ℓ2 and consistent loss\nFigure 4. Demonstration of the convergence behavior of attacker’s removal loss functions when training R, under different choices of P.\nThe 1st row is deraining and the 2nd row is style transfer. The ℓ1 loss is ∥Z∗−W0∥2\n1, the ℓ2 loss is ∥Z∗−W0∥2\n2, while the consistent loss\nis incorporated from [36]. The loss corresponding to no defense is ∥Z −W0∥2\n2.\nlearning rate of 0.0002. Weighting parameters α1, α2, β1,\nand β2 in (1) and (4) are set equally to 1.\n5.2. Convergence of Removal Loss\nWe first verify the effectiveness of the proposed DGS in\nterms of preventing R from learning to remove the water-\nmark, reflected by the loss behavior during the training of R.\nDue to DGS, the actual removal loss obtained by the attacker\nis modified from (5) to\nLA∗\nRemoval = ∥Z∗−W0∥2\n2,\n(16)\nwhen the ℓ2-norm is used. It is worth noting that the pro-\nposed DGS is derived based on the ℓ2-norm removal loss\nfunction. In fact, the attacker can use other removal loss\nfunctions, such as the ℓ1-norm and ℓ2-norm plus consistent\nloss [36]. This reflects the real-world situation of removal\nloss mismatch, which is also considered in our experiments,\nwhile the fidelity loss is consistently the ℓ2-norm in (6). The\nresults for deraining and style transfer tasks are presented in\nthe first and second rows, respectively, in Figure 4.\nFor both tasks, when no defense is deployed, all loss\nvalues converge to the zero level, 10−8 and 10−14, respec-\ntively, which verifies the effectiveness of the gradient-based\nremoval attack. It is also observed that the use of the consis-\ntent loss leads to a smoother convergence curve. In contrast,\nwith DGS deployed, none of the loss functions can be re-\nduced. It can be seen from the zoom-in versions that all\nthe loss values show a decreasing trend but in a negligibly\nsmall range, indicating the successful prevention of R from\nFigure 5. Demonstration of the convergence behavior of the true\nloss function ∥Z −W0∥2\n2 after deploying the proposed DGS, under\ndifferent choices of P, and deraining is considered as an example.\nlearning to remove the watermark. The results also verify\nthat the proposed defense based on the ℓ2-norm removal loss\ncan be well generalized against other loss functions.\nTo further verify the ineffectiveness of the training of R,\nwe present the loss between the raw output of D, i.e., Z, and\nW0 when R is trained in presence of DGS, and the results\nare shown in Figure 5, where deraining is considered as an\nexample. It can be seen that for all choices of P, the loss\nvalues increase and converge to a high level corresponding\nto failure of watermark removal.\n\n\n𝑋0            𝑋              𝑌          ℝ𝑌          𝑊          𝔻𝑌    𝔻[ℝ𝑌]\n(a)\n(b)\nFigure 6. Demonstration of the gradient-based watermark removal\nattack without defense. (a) Deraining. (b) Style transfer.\n5.3. Effectiveness\nIllustrative image and watermark examples for both derain-\ning and style transfer tasks before and after the proposed\nDGS are presented in Figures 6 and 7, respectively. The\nimages from left to right are the to-be-processed X0, pro-\ncessed non-watermarked X, processed and watermarked\nY , watermarked attacked by remover R(Y ), original water-\nmark W, decoded watermark without attack D(Y ), decoded\nwatermark after attack D[R](Y ) (or equivalently Z), and\nDGS perturbed result D∗[R](Y ) (or equivalently Z∗). It can\nbe observed that without defense, the gradient-based attack\nsuccessfully removes the watermark from Y in both tasks,\nleading to nearly all-white images at the output of D. In\ncontrast, with the proposed DGS, the attacker cannot remove\nthe watermark, not only with matched loss (Figure 7 (b)\nand (e)) but also with mismatched losses (Figure 7 other\nsubfigures). Additionally, comparing the last two columns,\nDGS preserves decoder output image quality with impercep-\ntible difference between D[R(Y )] and D∗[R(Y )], allowing\nlegitimate queries while preventing the training of R.\n5.4. Robustness\nDespite that the perturbed output D∗[R(Y )] is returned, the\nattacker has the freedom to further process it before using it\nto update R parameters with the hope of rendering potential\ndefenses ineffective. Here, we consider three types of further\nprocessing imposed by an attacker:\n• JPEG compression and noise addition.\n• Lattice attack [18].\n• Gradient sign flipping.\nAmong them, JPEG compression and noise addition (us-\ning i.i.d. white Gaussian noise) are intuitive image quality\ndegradation operations, the lattice attack replaces pixels by\nrandom values according to a fixed step for watermark re-\nmoval, while gradient sign flipping can be launched if the\nattacker observes no reduction of the removal loss and thus\nbelieve that the gradient has been flipped.\nResults for the first two attacks are presented in\nTables 1–3, where PSNR and MS-SSIM are used to measure\nthe similarity between the API returned gradient-reoriented\noutput D∗[R(Y )] and the attacked version. It can be seen\nthat even with a JPEG compression factor of 10% or a lattice\n𝑋0            𝑋              𝑌          ℝ𝑌 \n𝔻𝑌\n𝔻ℝ𝑌 𝔻∗ℝ𝑌\n(b)\n(a)\n(c)\n(d)\n(e)\n(f)\nFigure 7. Demonstration of the gradient-based watermark removal\nattack with the proposed DGS defense, where (a)–(c) are deraining,\n(d)–(f) are style transfer, and W is the same as in Figures 6. The\nremoval loss functions used by the attacker are ℓ1 loss in (a) and\n(d), ℓ2 loss in (b) and (e), and ℓ2 plus consistent in (c) and (f).\nTable 1. Robustness test of DGS against JPEG compression, where\n10−5 < Λi < 10−4, PSNR is in dB, and 0 ≤MS-SSIM, SR ≤1.\nFactor\nDeraining\nStyle Transfer\nPSNR↑MS-SSIM↑SR↑PSNR↑MS-SSIM↑SR↑\n10% 28.5748\n0.9672\n1.00 28.5776\n0.9673\n1.00\n20% 30.8380\n0.9855\n1.00 30.8364\n0.9855\n1.00\n30% 32.0850\n0.9902\n1.00 32.0845\n0.9902\n1.00\n40% 33.0321\n0.9927\n1.00 33.0278\n0.9927\n1.00\nTable 2. Robustness test of DGS against WGN addition, where\n10−5 < Λi < 10−4, noise level and PSNR are in dB, and 0 ≤\nMS-SSIM, SR ≤1.\nNoise\nLevel\nDeraining\nStyle Transfer\nPSNR↑MS-SSIM↑SR↑PSNR↑MS-SSIM↑SR↑\n0\n1.7791\n0.3042\n1.00 1.7768\n0.3041\n0.58\n10\n11.7799\n0.5613\n1.00 11.7809\n0.5612\n1.00\n20\n21.7774\n0.8022\n1.00 21.7801\n0.8022\n1.00\n30\n31.7781\n0.9573\n1.00 31.7784\n0.9573\n1.00\nattack that randomly alters every 1 out of 3 pixels (step is 2),\nthe success rate of watermark extraction is still 100%. Guar-\nanteed success is preserved with 10 dB noise addition, while\nthe rate reduces to 58% under 0 dB noise for style transfer\nonly, and this is with substantial image quality degradation.\nGenerally, the proposed DGS demonstrates promising ro-\nbustness against both normal and advanced attacks.\nTo provide more insights, let F be a generic additive\n\n\nTable 3. Robustness test of DGS against lattice attack [18], where\n10−5 < Λi < 10−4, PSNR is in dB, and 0 < MS-SSIM, SR < 1.\nStep\nDeraining\nStyle Transfer\nPSNR↑MS-SSIM↑SR↑PSNR↑MS-SSIM↑SR↑\n2\n12.3766\n0.6391\n1.00 12.3766\n0.6391\n1.00\n6\n21.9368\n0.8275\n1.00 21.9370\n0.8275\n1.00\n11 26.8371\n0.9263\n1.00 26.8373\n0.9263\n1.00\n16 30.6777\n0.9655\n1.00 30.6777\n0.9655\n1.00\ninterference resulted from the attacker’s further operation,\nand F is independent of Z, then (14) is modified to\nZ∗\nInterf = −PZ + (P + I)W + F,\n(17)\nand the attacker’s gradient component is modified from (9)\nto\n2(Z∗\nInterf −W0)T ∂Z∗\nInterf\n∂R(Y )\n= −2[Z −(W0 −F)]T P\n∂Z\n∂R(Y ).\n(18)\nThis means that with the deployment of DGS, the additive in-\nterference forces the remover output to diverge from W0 −F\ninstead of diverging from the original W0, which still cannot\nundermine the defense. Additionally, similar performance\nbetween deraining and style transfer tasks indicates that DGS\nis insensitive to data distribution and can be generalized to\nother image-to-image tasks.\nFor gradient sign flipping, the reorientation can be par-\ntially compensated, not fully, because it is not strictly 90\ndegree. Due to this, the attacker-flipped gradient will contain\nthe true gradient component and enable R to learn watermark\nremoval, though not most efficiently. However, the proposed\nsmall values of Λi can effectively reduce the learning rate.\nThis is illustrated in Figure 8. It can be seen that when Λi\nis within the interval [10−7, 10−6] or smaller, DGS remains\nrobust to gradient-based removal attacks.\n5.5. Limitation\nWe have discussed and experimented with attacker’s intuitive\nand practical countermeasure of flipping the gradient sign\nbefore updating R parameters, if it is observed that the re-\nmoval loss cannot be reduced. It is shown that the proposed\nDGS can still withstand such gradient flipping thanks to the\nsmall diagonal values of P, while here we provide a further\ndiscussion about the the potential weakness of DGS.\nAccording to (14), the attacker can fully overcome DGS\nif the hidden Z can be recovered from the observed Z∗. To\nachieve so, the inverse of (14) is given by\nZ = −P −1Z∗+\n\u0000I + P −1\u0001\nW,\n(19)\n(a)\n(b)\n(c)\n(d)\n𝑋0 \n𝑋\n𝑌\nℝ𝑌 \n𝔻𝑌\n𝔻ℝ𝑌\n𝔻∗ℝ𝑌\n(e)\nFigure 8. Demonstration of the robustness of the proposed DGS\nwhen the attacker applies gradient sign flipping, where W is the\nsame as in Figures 6, deraining is used as a example, and Λi is\nrandomly sampled within (a) [10−4, 10−3], (b) [10−5, 10−4], (c)\n[10−6, 10−5], (d) [10−7, 10−6], and (e) [10−8, 10−7].\nrequiring the knowledge of W and P. While W can be\nestimated by querying D using a watermarked image Y ∈\nY, it is difficult to guess P. However, the attacker may\nsimply set P = I and replace W by D∗(Y ) in (19), which\nyields an estimate of Z given by ˜Z = −Z∗+ 2D∗(Y ).\nAccording to our experiments, such an approximation suffers\nfrom performance degradation, but it remains open for the\nattacker to develop more advanced attacks to improve the\nestimation of Z. We note that the existing gradient-based\ndefense methods, e.g., [25] and [23], are also vulnerable to\ngradient sign flipping, indicating that incurring a gradient\nrotation between 90 to 180 degree may not be sufficient for\nprotection, which calls for further investigation.\n6. Conclusion\nExisting box-free watermarking methods for image-to-image\nmodels use a dedicated decoder D for watermark extraction\ndirectly from watermarked images. Since D is coupled with\nthe protected watermark encoder E, its black-box querying\nprocess makes the watermarking mechanism vulnerable and\nthus be exploited for watermark removal. Motivated by this\nobservation, we have developed a gradient-based removal at-\ntack which can remove state-of-the-art box-free watermarks.\nTo address this vulnerability, we have then proposed the\n\n\ndecoder gradient shield (DGS) framework in the black-box\nAPI of D and derived a closed-form solution. DGS can ef-\nfectively reorient and rescale the gradient of watermarked\nqueries through a newly introduced positive definite matrix\nP. With proper choices of the eigenvalues of P, it is shown\nthat DGS can prevent R from learning to remove watermarks.\nWe have conducted extensive experiments and verified the\neffectiveness of our proposed DGS on both image deraining\nand style transfer tasks. Further investigation on improv-\ning the robustness of DGS in terms of preventing reverse\nengineering of the true gradient is still needed.\nReferences\n[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas,\nand Joseph Keshet. Turning your weakness into a strength:\nWatermarking deep neural networks by backdooring. In 27th\nUSENIX Security Symposium (USENIX Security 18), pages\n1615–1631, 2018. 1\n[2] Haonan An, Zhengru Fang, Yuang Zhang, Senkang Hu, Xian-\nhao Chen, Guowen Xu, and Yuguang Fang. Channel-aware\nthroughput maximization for cooperative data fusion in cav.\narXiv preprint arXiv:2410.04320, 2024. 1\n[3] Antonio Barbalau, Adrian Cosma, Radu Tudor Ionescu, and\nMarius Popescu. Black-box ripper: Copying black-box mod-\nels using generative evolutionary algorithms. In Advances in\nNeural Information Processing Systems, pages 20120–20129,\n2020. 2\n[4] Yinpeng Dong, Shuyu Cheng, Tianyu Pang, Hang Su, and Jun\nZhu. Query-efficient black-box adversarial attacks guided by\na transfer-based prior. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 44(12):9536–9548, 2022. 2, 3\n[5] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88:303–338, 2010. 5\n[6] Zhengru Fang, Senkang Hu, Haonan An, Yuang Zhang,\nJingjing Wang, Hangcheng Cao, Xianhao Chen, and Yuguang\nFang. PACP: Priority-aware collaborative perception for con-\nnected and autonomous vehicles. IEEE Transaction of Mobile\nComputing (DOI: 10.1109/TMC.2024.3449371), Aug. 2024.\n1\n[7] Z. Fang, S. Hu, J. Wang, Y. Deng, X. Chen, and Y.\nFang. Prioritized information bottleneck theoretic frame-\nwork with distributed online learning for edge video analytics.\nIEEE/ACM Transactions on Networking, Jan. 2025. DOI:\n10.1109/TON.2025.3526148. 1\n[8] Jianwei Fei, Zhihua Xia, Benedetta Tondi, and Mauro Barni.\nWide flat minimum watermarking for robust ownership verifi-\ncation of gans. IEEE Transactions on Information Forensics\nand Security, 19:8322–8337, 2024. 1, 2\n[9] Simon Haykin. Adaptive filter theory. Prentice Hall, 4th\nedition, 2002. 4\n[10] X. Hou, J. Wang, C. Jiang, Z. Meng, J. Chen, and Y. Ren.\nEfficient federated learning for metaverse via dynamic user\nselection, gradient quantization and resource allocation. IEEE\nJournal on Selected Areas in Communications, 2023. 1\n[11] Guang Hua, Andrew Beng Jin Teoh, Yong Xiang, and Hao\nJiang. Unambiguous and high-fidelity backdoor watermarking\nfor deep neural networks.\nIEEE Transactions on Neural\nNetworks and Learning Systems, Early Access:1–14, 2023. 1\n[12] Christian Szegedy Ian J. Goodfellow, Jonathon Shlens. Ex-\nplaining and harnessing adversarial examples. In Proc. ICLR,\n2014. 3\n[13] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy\nLin. Black-box adversarial attacks with limited queries and\ninformation. In International conference on machine learning,\npages 2137–2146, 2018. 2, 3\n[14] Sanjay Kariyappa, Atul Prakash, and Moinuddin K Qureshi.\nMaze: Data-free model stealing attack using zeroth-order gra-\ndient estimation. In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 13809–13818,\n2021. 2\n[15] Li Li, Weiming Zhang, and Mauro Barni. Universal black-\nmarks: Key-image-free blackbox multi-bit watermarking of\ndeep neural networks. IEEE Signal Processing Letters, 30:\n36–40, 2023. 1\n[16] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang.\nLearning linear transformations for fast arbitrary style trans-\nfer. In IEEE Conference on Computer Vision and Pattern\nRecognition, 2019. 5\n[17] Dongdong Lin, Benedetta Tondi, Bin Li, and Mauro Barni.\nA cyclegan watermarking method for ownership verification.\nIEEE Transactions on Dependable and Secure Computing,\nEarly Access:1–15, 2024. 1, 2\n[18] Hangcheng Liu, Tao Xiang, Shangwei Guo, Han Li, Tianwei\nZhang, and Xiaofeng Liao. Erase and repair: An efficient\nbox-free removal attack on high-capacity deep hiding. IEEE\nTransactions on Information Forensics and Security, 18:5229–\n5242, 2023. 3, 7, 8\n[19] Nils Lukas and Florian Kerschbaum. PTW: Pivotal tuning\nwatermarking for Pre-Trained image generators. In 32nd\nUSENIX Security Symposium (USENIX Security 23), pages\n2241–2258, Anaheim, CA, 2023. 2\n[20] Peizhuo Lv, Pan Li, Shengzhi Zhang, Kai Chen, Ruigang\nLiang, Hualong Ma, Yue Zhao, and Yingjiu Li. A robustness-\nassured white-box watermark in neural networks. IEEE Trans-\nactions on Dependable and Secure Computing, 20(6):5214–\n5229, 2023. 1\n[21] Zhuo Ma, Xinjing Liu, Yang Liu, Ximeng Liu, Zhan Qin, and\nKui Ren. Divtheft: An ensemble model stealing attack by\ndivide-and-conquer. IEEE Transactions on Dependable and\nSecure Computing, 20(6):4810–4822, 2023. 2\n[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learn-\ning models resistant to adversarial attacks. In International\nConference on Learning Representations, 2018. 3\n[23] Mantas Mazeika, Bo Li, and David A. Forsyth. How to steer\nyour adversary: Targeted and efficient model stealing defenses\nwith gradient redirection. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA, pages 15241–15254, 2022. 3, 5, 8\n[24] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.\nKnockoff nets: Stealing functionality of black-box models. In\n\n\n2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4949–4958, 2019. 2\n[25] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Pre-\ndiction poisoning: Towards defenses against dnn model steal-\ning attacks. arXiv preprint arXiv:1906.10908, 2019. 3, 5,\n8\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234–241. Springer, 2015. 5\n[27] Sunandini Sanyal, Sravanti Addepalli, and R. Venkatesh Babu.\nTowards data-free model stealing in a hard label setting. In\n2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 15263–15272, 2022. 2, 3\n[28] Zeyang Sha, Xinlei He, Ning Yu, Michael Backes, and Yang\nZhang. Can’t steal? cont-steal! contrastive stealing attacks\nagainst image encoders. In 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n16373–16383, Los Alamitos, CA, USA, 2023. 2\n[29] Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls, and\nNicolas Papernot.\nData-free model extraction.\nIn 2021\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4769–4778, Los Alamitos, CA,\nUSA, 2021. 2\n[30] Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and\nShin’ichi Satoh. Embedding watermarks into deep neural\nnetworks. In Proceedings of the 2017 ACM on international\nconference on multimedia retrieval, pages 269–277, 2017. 1\n[31] Run Wang, Haoxuan Li, Lingzhou Mu, Jixing Ren, Shang-\nwei Guo, Li Liu, Liming Fang, Jing Chen, and Lina Wang.\nRethinking the vulnerability of dnn watermarking: Are wa-\ntermarks robust against naturalness-aware perturbations? In\nProceedings of the 30th ACM International Conference on\nMultimedia, pages 1808–1818, 2022. 1\n[32] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale\nstructural similarity for image quality assessment. In The\nThrity-Seventh Asilomar Conference on Signals, Systems &\nComputers, pages 1398–1402, 2003. 5\n[33] Hanzhou Wu, Gen Liu, Yuwei Yao, and Xinpeng Zhang. Wa-\ntermarking neural networks with watermarked images. IEEE\nTransactions on Circuits and Systems for Video Technology,\n31(7):2591–2601, 2020. 1, 2\n[34] He Zhang and Vishal M Patel. Density-aware single image\nde-raining using a multi-stream dense network. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 695–704, 2018. 5\n[35] Haitian Zhang, Guang Hua, Xinya Wang, Hao Jiang, and Wen\nYang. Categorical inference poisoning: Verifiable defense\nagainst black-box dnn model stealing without constraining\nsurrogate data and query times. IEEE Transactions on Infor-\nmation Forensics and Security, 18:1473–1486, 2023. 4\n[36] Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming\nZhang, Wenbo Zhou, Hao Cui, and Nenghai Yu. Model\nwatermarking for image processing networks. In Proceedings\nof the AAAI conference on artificial intelligence, pages 12805–\n12812, 2020. 6\n[37] Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang,\nHuamin Feng, Gang Hua, and Nenghai Yu. Deep model\nintellectual property protection via deep watermarking. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n44(8):4005–4020, 2022. 1, 2, 3, 4, 5\n[38] Jie Zhang, Dongdong Chen, Jing Liao, Zehua Ma, Han Fang,\nWeiming Zhang, Huamin Feng, Gang Hua, and Nenghai Yu.\nRobust model watermarking for image processing networks\nvia structure consistency. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 46(10):6985–6992, 2024. 1, 2,\n3, 5\n[39] Yuang Zhang, Haonan An, Zhengru Fang, Guowen Xu, Yuan\nZhou, Xianhao Chen, and Yuguang Fang. Smartcooper: Ve-\nhicular collaborative perception with adaptive fusion and\njudger mechanism. In 2024 IEEE International Conference\non Robotics and Automation (ICRA), pages 4450–4456. IEEE,\n2024. 1\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20924v1.pdf",
    "total_pages": 10,
    "title": "Decoder Gradient Shield: Provable and High-Fidelity Prevention of Gradient-Based Box-Free Watermark Removal",
    "authors": [
      "Haonan An",
      "Guang Hua",
      "Zhengru Fang",
      "Guowen Xu",
      "Susanto Rahardja",
      "Yuguang Fang"
    ],
    "abstract": "The intellectual property of deep image-to-image models can be protected by\nthe so-called box-free watermarking. It uses an encoder and a decoder,\nrespectively, to embed into and extract from the model's output images\ninvisible copyright marks. Prior works have improved watermark robustness,\nfocusing on the design of better watermark encoders. In this paper, we reveal\nan overlooked vulnerability of the unprotected watermark decoder which is\njointly trained with the encoder and can be exploited to train a watermark\nremoval network. To defend against such an attack, we propose the decoder\ngradient shield (DGS) as a protection layer in the decoder API to prevent\ngradient-based watermark removal with a closed-form solution. The fundamental\nidea is inspired by the classical adversarial attack, but is utilized for the\nfirst time as a defensive mechanism in the box-free model watermarking. We then\ndemonstrate that DGS can reorient and rescale the gradient directions of\nwatermarked queries and stop the watermark remover's training loss from\nconverging to the level without DGS, while retaining decoder output image\nquality. Experimental results verify the effectiveness of proposed method. Code\nof paper will be made available upon acceptance.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}