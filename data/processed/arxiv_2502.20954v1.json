{
  "id": "arxiv_2502.20954v1",
  "text": "Robust and Efficient Writer-Independent\nIMU-Based Handwriting Recognization\nJindong Li1[0000−0002−3550−1660], Tim Hamann2[0000−0003−3562−6882], Jens\nBarth2[0000−0003−3967−9578], Peter Kaempf2, Dario Zanca1[0000−0001−5886−0597],\nand Bjoern Eskofier1[0000−0002−0417−0336]\n1 Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität\nErlangen-Nürnberg, Germany\n2 STABILO International GmbH, Germany\nAbstract. Online handwriting recognition (HWR) using data from in-\nertial measurement units (IMUs) remains challenging due to variations\nin writing styles and the limited availability of high-quality annotated\ndatasets. Traditional models often struggle to recognize handwriting from\nunseen writers, making writer-independent (WI) recognition a crucial\nbut difficult problem. This paper presents an HWR model with an encoder-\ndecoder structure for IMU data, featuring a CNN-based encoder for fea-\nture extraction and a BiLSTM decoder for sequence modeling, which\nsupports inputs of varying lengths. Our approach demonstrates strong\nrobustness and data efficiency, outperforming existing methods on WI\ndatasets, including the WI split of the OnHW dataset and our own\ndataset. Extensive evaluations show that our model maintains high accu-\nracy across different age groups and writing conditions while effectively\nlearning from limited data. Through comprehensive ablation studies, we\nanalyze key design choices, achieving a balance between accuracy and\nefficiency. These findings contribute to the development of more adapt-\nable and scalable HWR systems for real-world applications. The code is\navailable at https://github.com/jindongli24/REWI.\nKeywords: Online Handwriting Recognition · Time-series Analysis ·\nInertial Measurement Unit\n1\nIntroduction\nHandwriting has been an essential way of recording and sharing information\nthroughout human history. With advancements in technology, the demand for\ndigitizing handwriting has increased. Handwriting recognition (HWR), a method\nfor turning handwritten symbols into computer-readable text, has become an\nimportant area of research.\nHWR is generally divided into two types: offline HWR and online HWR.\nOffline HWR, also known as optical character recognition, identifies handwriting\nfrom static images of handwritten text. This approach is widely used in various\nfields, including historical research [18] and healthcare [7]. On the other hand,\narXiv:2502.20954v1  [cs.LG]  28 Feb 2025\n\n\n2\nJ. Li et al.\nonline HWR works with time-series data that captures dynamic handwriting\nfeatures such as strokes, positions, directions, and speeds. This data is usually\ncollected using touch screens and styluses on mobile devices [3], which limits the\nwriting surface users can write on.\nAnother approach to online HWR uses pens equipped with inertial mea-\nsurement units (IMUs) [21,1,14]. These sensors, including accelerometers and\ngyroscopes, capture pen movement without relying on the exact position of the\ntip, allowing the pen to function independently of external devices and on any\nsurface. As IMU sensor costs continue to decrease, this method shows great po-\ntential for the application in online HWR. However, variations in handwriting\nstyles can impact recognition accuracy, and sensor noise from rough surfaces,\ntemperature changes, and digitization artifacts add further challenges. More sig-\nnificantly, gravitational acceleration also introduces noise, making motion track-\ning less accurate and complicating reliable HWR.\nThis paper introduces a sequence-to-sequence model for IMU-based online\nHWR that addresses challenges related to handwriting style variations and sen-\nsor noise. The model uses an encoder-decoder structure that combines a con-\nvolutional neural network (CNN) with a bidirectional long short-term memory\n(BiLSTM) network and integrates recent advancements in deep learning. We\nevaluate the proposed model by comparing it with several mainstream models\nand existing IMU-based HWR methods. Experimental results on datasets col-\nlected with an IMU-based pen show that our model outperforms others in terms\nof accuracy, data efficiency, robustness, and flexibility in WI HWR.\nSection 2 reviews related work on IMU-based HWR and advanced main-\nstream models. Section 3 describes the datasets, the CNN-BiLSTM model, and\nthe data augmentation methods that we used. Section 4 explains the training\nprocess and presents the experimental results in detail. Section 5 discusses key\nfactors that improve IMU-based HWR. Finally, Section 6 provides the conclu-\nsions and suggests directions for future research.\n2\nRelated Works\n2.1\nIMU-based HWR\nIMU-based HWR has been an area of research for decades. Early studies, such\nas [4,9], used dynamic-time warping-based algorithms to recognize digit data\ncollected with IMU-based pens, achieving recognition rates above 90%. Later\nworks, including [13,15], investigated LSTM-based models for recognizing indi-\nvidual English characters, reaching recognition accuracies of up to 99.68% and\n79.01% on their respective datasets. While these methods achieved high accu-\nracy, they rely on isolated character recognition, processing entire input signals\nto classify single characters. This character-by-character approach disrupts natu-\nral writing flow, making it less practical for real-world applications where people\ntypically write word-by-word.\nTo handle more complex tasks, [22,16] employed CNN-LSTM models with\nconnectionist temporal classification (CTC) [6] to recognize English and Ger-\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n3\nman characters in a sequence-to-sequence format. These models were tested on\ndatasets collected using the IMU-based pen developed by STABILO [21]. Al-\nthough the methods achieved character error rates (CERs) of 27.8% and 17.97%,\nwhich are worse than earlier approaches, they paved the way for recognizing full\nwords and even sentences, taking a step closer to practical applications.\n2.2\nAdvancements in Deep Learning Architectures\nIn recent years, the introduction of ResNet [8] and Transformers [20] has signif-\nicantly advanced the development of deep learning models. ResNet tackled the\nvanishing gradient problem using skip connections, making it possible to train\nvery deep neural networks effectively. Transformers introduced self-attention\nmechanisms, which improved parallelization and enhanced the ability to model\nlong-range dependencies compared to convolutional or recurrent neural networks.\nBuilding on these innovations, Vision Transformer [5] applied a transformer-\nbased architecture to image recognition by treating images as sequences of\npatches and leveraging self-attention to achieve state-of-the-art results. MLP-\nMixer [19] showed that strong performance in vision tasks could be achieved\nwithout convolutions or self-attention, relying solely on multi-layer perceptrons\n(MLPs) to mix spatial and channel information. Swin Transformers [11,10] intro-\nduced a hierarchical architecture with shifted window-based self-attention, en-\nabling efficient multi-scale modeling for various vision tasks. ConvNeXt [12] com-\nbined ideas from CNNs and Transformers, achieving state-of-the-art performance\nwhile maintaining the simplicity and efficiency of traditional CNNs. xLSTM [2]\nenhanced conventional LSTMs with memory augmentation and cross-layer pa-\nrameter sharing, improving their ability to handle long-term dependencies and\nprocess sequential data effectively.\nAlthough these models were originally designed for different tasks, their core\nprinciples e.g. self-attention, hierarchical structures, and memory augmentation,\nare well-suited for time-series data. Applying these techniques to IMU-based\nHWR could potentially enhance performance and open new research opportuni-\nties in the field.\n3\nMethods\n3.1\nDatasets\nThis paper uses a dataset collected with the IMU-based pen developed by\nSTABILO [21]. The pen is equipped with two accelerometers, one at each end,\nalong with a gyroscope, a magnetometer, and a force sensor, generating 13 out-\nput channels at a sampling rate of 100 Hz. Data collection included 984 recording\nsessions with participants of different ages and handednesses, resulting in 54,666\nsamples of English and German words of varying lengths. These words cover\n59 character categories, including both upper- and lowercase letters for both\nlanguages.\n\n\n4\nJ. Li et al.\nThe datasets are evaluated using two configurations: writer-independent (WI)\nand random splits, both following a 5-fold cross-validation approach. In the WI\nsplit, data are divided so that no writer appears in both the training and test-\ning sets, ensuring handwriting styles remain independent. In the random split,\nsamples are assigned randomly without considering writer identity.\nTo evaluate model robustness, we analyze subsets of the WI dataset based\non participants’ ages. Since children are still developing their handwriting skills,\ntheir handwriting patterns differ significantly from those of adults, particularly\nin areas such as writing speed and discontinuity due to high cognitive load. These\npatterns typically stabilize around 14 years old [17]. Based on this, we classify\nparticipants aged 12 and under as children to ensure that most writers represent\ntypical children’s handwriting patterns, and those aged 18 and older as adults.\nWe assess model performance on both groups. Participants aged 13 to 17 were\nexcluded because some had already developed mature and fluent handwriting,\nmaking them less representative of either the children or adult groups. Notably,\nthe adult subset (47,992 samples) is significantly larger than the children subset\n(6,070 samples).\nTo evaluate data efficiency, we reduce the training sets to 50% and 25%\nof their original size while keeping the testing sets unchanged. Due to the sig-\nnificant imbalance between right- and left-handed samples (51,854 vs. 2,812,\nrespectively), we exclude handedness-related comparisons from our analysis.\nFor commercial reasons, this dataset will not be publicly available. How-\never, we also use the writer-dependent (WD) subset, where the WD subset is\ndivided by words, and the WI subset only from the right-handed portion of the\nOnHW-words500 dataset [16] to further benchmark our model against various\nmainstream models. Due to the significant imbalance between right-handed and\nleft-handed samples (25,218 vs. 1,000, respectively), left-handed portion of the\nOnHW dataset are excluded from our experiments.\n3.2\nHWR Model\nThe sequence-to-sequence model uses an encoder-decoder structure, as shown in\nFig 1. The encoder is a CNN that efficiently extracts and embeds input features,\nwhile the decoder is a BiLSTM network that captures contextual information\nin both directions. Both the CNN and BiLSTM can process inputs of varying\nlengths, providing flexibility for different handwriting data.\nThe encoder has three stages, each with a patch embedding layer followed by\nthree convolutional blocks. The patch embedding layer uses a 1-D convolutional\nlayer with a kernel size of 2 and a stride of 2, followed by a 1-D instance normal-\nization layer. The first patch embedding layer converts the 13-channel input data\ninto 128 channels, and each subsequent layer doubles the number of channels.\nThe convolutional blocks are based on the ConvNeXt block, which improves\nefficiency using grouped convolution and inverted bottlenecks. To reduce compu-\ntational cost without sacrificing performance, each block includes a depth-dilated\n1-D separable convolution. This layer uses a 1-D depthwise convolution with a\nkernel size of 5 that doubles the input dimension, followed by a 1-D pixelwise\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n5\n×3\nInstanceNorm\nConv1D\nDepth dilated \n1-D separable \nconvolution \nInstanceNorm\nGELU\nDropout\nBi-LSTM\n×3\n×3\nGGGGGG|ooooo|ooooo|ddddddd\nIMU signal\nDecoder\nEncoder\nText\nFig. 1. Model architecture\nconvolution that reduces it back. This design creates a larger hidden space for\nbetter feature representation compared to standard depthwise separable con-\nvolutions while reducing complexity by omitting an extra layer found in the\nConvNeXt block. Each depth-dilated 1-D separable convolution is followed by\na 1-D instance normalization layer, a GELU activation function, and a dropout\nlayer with a drop rate of 0.2.\nThe decoder has three BiLSTM layers, each with a hidden size of 128 and\na dropout rate of 0.2. These layers are followed by a fully connected layer and\na Softmax layer for pixelwise classification. The Softmax outputs are then pro-\ncessed by a greedy CTC decoder to produce the final results.\n3.3\nData Augmentation\nTo improve the model’s resistance to noise, we used four data augmentation\ntechniques: AddNoise, Drift, Dropout, and TimeWarp. Each technique has a\n25% chance of being applied to a given signal. TimeWarp only affects the time\ndimension, while the other methods are applied multiplicatively to keep the\naugmented signals’ magnitude similar to the original signals. Examples of these\naugmentations are shown in Fig 2. After augmentation, the signals are also\nnormalized.\n– AddNoise: Adds Gaussian noise to the original signals.\n– Drift: Divides the original signals into sections and applies random drift\nwithin each section.\n\n\n6\nJ. Li et al.\n– Dropout: Randomly replaces small segments of the signal with the last\nvalue preceding each segment.\n– TimeWarp: Randomly adjusts the speed of sections of the original signal.\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAddNoise\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDrift\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDropout\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimeWarp\nFig. 2. Data augmentations\n4\nExperiments\nThe experiments are divided into five parts: comparison with previous methods,\nbenchmarking against mainstream models, robustness evaluation, data efficiency\nevaluation, and an ablation study. In each part, models are trained using 5-fold\ncross-validation, and the best results from each fold’s test set are used to calculate\nthe final results.\nExcept for the CLDNN [22], which is trained using the method described\nin its original paper, all other models, including those from previous studies\nand mainstream models, are trained using the same following approach. We use\nthe AdamW optimizer with a learning rate of 0.001 for 300 epochs. To improve\nconvergence, a linear learning rate warm-up starts at 0.0001 for the first 30\nepochs, followed by a cosine annealing schedule for the remaining epochs. The\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n7\ntraining process uses the CTC loss function and a batch size of 64. Models\nare saved and evaluated every 5 epochs during training. All experiments are\nconducted on a computer with an AMD Ryzen 9 5950X processor and a GeForce\nRTX 3090 graphics card, using the PyTorch library for model implementation.\nTo assess model performance, we use two metrics: character error rate (CER)\nand word error rate (WER). These measure the proportion of errors, including\nsubstitutions, deletions, and insertions, relative to the total number of characters\nin the reference text at the character and word levels, respectively. Additionally,\nwe evaluate model size and computational requirements by measuring the num-\nber of parameters (number of Params) and multiply and accumulate operations\n(MACs).\n4.1\nComparison with Previous Works\nIn this section, we compare our model with previous works using both our dataset\nand the OnHW dataset [16]. For our dataset, we evaluate performance on the\nrandom and WI splits, while for the OnHW dataset, we use the WD and WI\nsplits of the right-handed words500 subset. The comparison includes three mod-\nels: CNN+BiLSTM [16], CLDNN, and our proposed model. Since the code for\nthe previous models is not publicly available, we re-implemented them based on\ntheir published descriptions using PyTorch. We replicated the training and pre-\nprocessing strategy for CLDNN as described. However, the training details for\nCNN+BiLSTM were not clearly specified, making it difficult to reproduce the\nreported performance. Therefore, we trained CNN+BiLSTM using our training\npipeline and data augmentation strategy, which may result in differences from\nthe original reported results. Additionally, we include the previously published\nresults for CNN+BiLSTM on the OnHW dataset for reference.\nTable 1. Comparison with previous works on our dataset.\nModels\nRandom\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nCNN+BiLSTM\n0.0710\n0.3142\n0.1512\n0.5213\n0.40M\n153M\nCLDNN\n0.0784\n0.3291\n0.1533\n0.5112\n0.75M\n291M\nOurs\n0.0367\n0.1703\n0.0692\n0.2676\n3.89M\n600M\nAs shown in Table 1, our model outperforms previous works, achieving the\nlowest CER and WER on both the random and WI splits of our dataset. Specif-\nically, it achieves a CER of 0.0367 and a WER of 0.1703 on the random split,\nand a CER of 0.0692 and a WER of 0.2676 on the WI split. In comparison, the\nerror rates of the CNN+BiLSTM and CLDNN models are approximately twice\nas high. However, our model has a larger number of parameters and higher\ncomputational costs than these models.\n\n\n8\nJ. Li et al.\nTable 2. Comparison with previous works on OnHW dataset [16].\nModels\nWD\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nCNN+BiLSTM (orig.)\n0.1716\n0.5195\n0.2780\n0.6091\n0.40M\n153M\nCNN+BiLSTM\n0.1597\n0.5243\n0.1716\n0.4240\n0.40M\n153M\nCLDNN\n0.1696\n0.5404\n0.1715\n0.3948\n0.75M\n291M\nOurs\n0.1546\n0.4551\n0.0746\n0.1559\n3.89M\n600M\nAs shown in Table 2, on the OnHW dataset, our model achieves the best\nperformance compared to previous works. It achieves the lowest CER and WER\nacross both the WD and WI splits, with a CER of 0.1546 and a WER of 0.4551\nfor the WD split, and a CER of 0.0746 and a WER of 0.1559 for the WI split.\nCompared to the original CNN+BiLSTM results reported in the paper, our reim-\nplementation using the same model architecture but our training pipeline and\ndata augmentation strategy leads to better convergence and lower error rates.\nIn the WI split, our model reduces the error rates by more than half, highlight-\ning its effectiveness in handling challenges associated with unseen handwriting\nstyles.\n4.2\nBenchmark against Mainstream Models\nIn this section, we benchmark our proposed model against various models using\nthe same datasets as in the previous experiments. To efficiently extract high-\nlevel features while reducing output size and computational cost for the decoder,\nwe use ResNet, ConvNeXt, MLP-Mixer, and Swin Transformer V2 as encoders\ncombined with our BiLSTM decoder. To capture long-range dependencies, we\nuse the mLSTM module of xLSTM as a decoder alongside our CNN encoder. For\na fair comparison, we implement the mLSTM module in a bidirectional manner,\nallowing it to access both past and future information. Since the Transformer\narchitecture can capture high-level information and has a global receptive field\nover the input, we evaluate it as an encoder, a decoder, and as a standalone\nHWR model. Since these models were not originally designed for HWR and\nvary in size, we adjust their hyperparameters to better suit the HWR task while\nmaintaining a similar number of parameters across models. Additionally, because\nTransformer, MLP-Mixer, Swin Transformer V2, and Bi-mLSTM require a fixed\ninput length, all inputs for these models are zero-padded to a length of 1024.\nTable 3 shows that our model achieves the best CER and WER on the WI\nsplit, with values of 0.0692 and 0.2676, respectively, demonstrating its strong\nability to generalize to unseen handwriting styles. It also delivers competitive\nperformance on the random split, achieving a CER of 0.0367 and a WER of\n0.1703. Compared to other models, our approach provides well-balanced per-\nformance across both splits, outperforming most other models. While the MLP-\nMixer achieves the lowest error rates on the random split, it requires significantly\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n9\nTable 3. Benchmark against mainstream models on our dataset.\nModels\nRandom\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nTransformer\n0.0443\n0.2082\n0.1163\n0.4042\n3.96M\n509M\nResNet (enc.)\n0.0306\n0.1456\n0.0805\n0.3026\n3.97M\n591M\nConvNeXt (enc.)\n0.0355\n0.1696\n0.0834\n0.3205\n3.86M\n600M\nMLP-Mixer (enc.)\n0.0274\n0.1339\n0.1025\n0.3624\n3.90M\n802M\nTransformer (enc.)\n0.0422\n0.1975\n0.1002\n0.3572\n3.71M\n477M\nSwinV2 (enc.)\n0.0303\n0.1491\n0.0746\n0.2924\n3.88M\n601M\nTransformer (dec)\n0.0387\n0.1822\n0.0828\n0.3183\n3.82M\n590M\nBi-mLSTM (dec.)\n0.0476\n0.2192\n0.0914\n0.3484\n4.10M\n625M\nOurs\n0.0367\n0.1703\n0.0692\n0.2676\n3.89M\n600M\nhigher computational resources, with the highest MACs among all models. In-\nterestingly, the MLP-Mixer ranks second worst on the WI split. This highlights\nthe efficiency and robustness of our model in achieving strong performance while\nmaintaining a favorable balance between accuracy and computational cost.\nTable 4. Benchmark against mainstream models on OnHW dataset.\nModels\nWD\nWI\n#Params\nMACs\nCER\nWER\nCER\nWER\nTransformer\n0.2615\n0.6283\n0.1139\n0.2573\n3.96M\n509M\nResNet (enc.)\n0.1294\n0.4164\n0.0850\n0.1846\n3.97M\n591M\nConvNeXt (enc.)\n0.1515\n0.4657\n0.0812\n0.1791\n3.86M\n600M\nMLP-Mixer (enc.)\n0.1438\n0.4659\n0.0964\n0.2149\n3.90M\n802M\nTransformer (enc.)\n0.1817\n0.5242\n0.1060\n0.2303\n3.71M\n477M\nSwinV2 (enc.)\n0.1750\n0.4983\n0.0820\n0.1814\n3.88M\n601M\nTransformer (dec)\n0.1396\n0.4407\n0.0864\n0.1881\n3.82M\n590M\nBi-mLSTM (dec.)\n0.2267\n0.5749\n0.0841\n0.1803\n4.10M\n625M\nOurs\n0.1546\n0.4551\n0.0746\n0.1559\n3.89M\n600M\nTable 4 compares the performance of different models on the OnHW dataset.\nOur model delivers strong results, with a CER of 0.1546 and a WER of 0.4551\non the WD split, and the lowest CER of 0.0746 and WER of 0.1559 on the\nWI split. CNN-based models, such as ResNet and ConvNeXt, also perform well,\nwith ResNet achieving the lowest CER and WER on the WD split. These models\nshow better performance on the unseen WI split compared to Transformer-based\nmodels, which generally have higher CER and WER. These results highlight the\nadvantage of CNN-based models in handling unseen handwriting styles while\nmaintaining a good balance between performance and efficiency.\n\n\n10\nJ. Li et al.\n4.3\nRobustness Evaluation\nIn this section, we assess the robustness of the models on the adult (Adult) and\nchildren (Children) subsets of our WI dataset. Additionally, we evaluate how\nmodels trained on the adult subset perform when tested on the children’s subset\n(Adult2Child). All models are tested using the same configurations as in the\nprevious experiments.\nTable 5. Robustness evaluation.\nModels\nAdults\nChildren\nAdult2Child\nCER\nWER\nCER\nWER\nCER\nWER\nCNN+BiLSTM\n0.1525\n0.5086\n0.6545\n0.9669\n0.6678\n0.9982\nCLDNN\n0.1479\n0.4793\n0.4704\n0.8331\n0.3716\n0.7783\nTransformer\n0.1247\n0.4199\n0.7584\n0.9881\n0.3350\n0.7466\nResNet (enc.)\n0.0891\n0.3194\n0.1775\n0.4934\n0.2841\n0.6607\nConvNeXt (enc.)\n0.0910\n0.3330\n0.3102\n0.6557\n0.2801\n0.6612\nMLP-Mixer (enc.)\n0.1118\n0.3715\n0.7732\n0.9835\n0.3044\n0.6824\nTransformer (enc.)\n0.1115\n0.3727\n0.4559\n0.8390\n0.2888\n0.6681\nSwinV2 (enc.)\n0.0842\n0.3109\n0.5400\n0.8634\n0.2637\n0.6368\nTransformer (dec.)\n0.0891\n0.3252\n0.2462\n0.5431\n0.2807\n0.6630\nBi-mLSTM (dec.)\n0.0974\n0.3524\n0.3079\n0.6769\n0.2995\n0.7018\nOurs\n0.0752\n0.2772\n0.1748\n0.4493\n0.2678\n0.6273\nTable 5 shows the robustness of our proposed model on the adult and children\nsubsets of the WI split of our dataset. Our model achieves the best results on\nboth subsets, with a CER of 0.0752 and a WER of 0.2772 for adults, and a CER\nof 0.1748 and a WER of 0.4493 for children. When trained on the adult subset,\nalthough our model is slightly behind the Swin Transformer V2, it still achieves\nthe lowest WER and ranks among the top performers on the children subset.\nThis demonstrates its ability to learn robust features and adapt to different\nhandwriting styles across age groups. Notably, models like the Transformer and\nMLP-Mixer struggle with the children subset, likely due to the challenge of\ngeneralizing to different handwriting styles when trained on the adult dataset\nand the smaller size of the children subset.\n4.4\nData Efficiency Evaluation\nIn this section, we assess the data efficiency of the models by training them on\ndifferent portions of the training set. Using 5-fold cross-validation, each training\nset consists of four groups of data. To evaluate performance under varying data\nconditions, we train all models on 100% (four groups), 50% (two groups), and\n25% (one group) of the original training set from our WI dataset.\nTable 6 shows the data efficiency of various models when trained on different\nproportions of the WI dataset. Our model consistently outperforms all others\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n11\nTable 6. Data efficiency evaluation.\nModels\n100%\n50%\n25%\nCER\nWER\nCER\nWER\nCER\nWER\nCNN+BiLSTM\n0.1512\n0.5213\n0.2733\n0.6833\n0.3877\n0.8208\nCLDNN\n0.1533\n0.5112\n0.1868\n0.5665\n0.2688\n0.7096\nTransformer\n0.1163\n0.4042\n0.1765\n0.5318\n0.1910\n0.5615\nResNet (enc.)\n0.0805\n0.3026\n0.1256\n0.4135\n0.1978\n0.5730\nConvNeXt (enc.)\n0.0834\n0.3205\n0.1297\n0.4336\n0.2005\n0.5869\nMLP-Mixer (enc.)\n0.1025\n0.3624\n0.1471\n0.4622\n0.2223\n0.6213\nTransformer (enc.)\n0.1002\n0.3572\n0.1476\n0.4692\n0.2272\n0.6295\nSwinV2 (enc.)\n0.0746\n0.2924\n0.1191\n0.4081\n0.1929\n0.5728\nTransformer (dec.)\n0.0828\n0.3183\n0.1253\n0.4244\n0.1910\n0.5615\nBi-mLSTM (dec.)\n0.0914\n0.3484\n0.1356\n0.4526\n0.2077\n0.5935\nOurs\n0.0692\n0.2676\n0.1102\n0.3763\n0.1802\n0.5235\nacross all training set sizes, achieving a CER of 0.0692 and WER of 0.2676 on the\nfull dataset (100%), 0.1102 and 0.3763 on 50%, and 0.1802 and 0.5235 on 25%.\nThese results highlight our model’s ability to maintain high accuracy even with\nless training data. While ResNet and SwinV2 perform well on larger training\nsets, their accuracy drops more noticeably as the dataset size decreases. Other\nmodels, such as the Transformer and MLP-Mixer, show even larger performance\ngaps, emphasizing the robustness and efficiency of our model in making effective\nuse of training data.\n4.5\nAblation Study\nIn this section, we conduct an ablation study on our WI dataset using CLDNN\nas the baseline, as it has a similar architecture to our model. We evaluate the\nperformance improvements achieved through incremental changes, including op-\ntimized training strategies, architectural enhancements, hyperparameter tuning,\nand data augmentation.\nTable 7 summarizes the ablation study results, showing performance im-\nprovements from incremental modifications. Starting from the CLDNN baseline\nwith a CER of 0.1533 and WER of 0.5112, each enhancement improves accuracy.\nScaling the CNN and BiLSTM reduces error rates at the cost of more parameters\nand MACs. Optimized training strategies, instance normalization, GELU activa-\ntion, and dropout rate adjustment further boost performance without increasing\ncomputational overhead. Notably, the dilated-depth 1-D separable convolution\nimproves accuracy while reducing parameters and MACs. Data augmentation\ntechniques, such as add dropout and time warping, also play a key role, lowering\nCER from 0.0839 to 0.0745 (-11.2%) and WER from 0.3098 to 0.2810 (-9.3%),\nachieving significant gains despite limited room for improvement. With all en-\nhancements and fine-tuned dropout, the final model achieves a CER of 0.0692\nand a WER of 0.2676, demonstrating the effectiveness of these modifications. In\n\n\n12\nJ. Li et al.\nTable 7. Ablation study.\nModels\nCER\nWER\n#Params\nMACs\nCLDNN\n0.1533\n0.5112\n0.75M\n291M\n+ Training strategy\n0.1305\n0.4500\n0.75M\n291M\n+ Reverse dimension order\n0.1261\n0.4470\n0.92M\n214M\n+ 3× deeper CNN\n0.1070\n0.3924\n3.05M\n989M\n+ Standalone embedding layer\n0.1064\n0.3851\n3.95M\n687M\n+ Dilated-depth separable convolution\n0.0997\n0.3630\n2.85M\n467M\n+ Instance normalization\n0.0960\n0.3635\n2.85M\n465M\n+ GELU\n0.0934\n0.3526\n2.85M\n465M\n+ 2× wider BiLSTM\n0.0860\n0.3203\n3.52M\n551M\n+ 3-layer BiLSTM\n0.0830\n0.3048\n3.91M\n602M\n+ Remove hidden layer\n0.0839\n0.3098\n3.89M\n600M\n+ Data augmentation\n0.0745\n0.2810\n3.89M\n600M\n+ Dropout rate 0.2\n0.0692\n0.2676\n3.89M\n600M\nsummary, our final model reduces CER by 55% and WER by 48% compared to\nthe baseline CLDNN. To achieve this performance improvement, it uses approx-\nimately 5.2 times more parameters (increasing from 0.75M to 3.89M) and 2.1\ntimes more MACs (rising from 291M to 600M).\n5\nDiscussion\nThis section examines the key factors behind our model’s strong performance,\nwith a focus on robustness, data efficiency, and flexibility.\n5.1\nRobustness\nHandwriting styles vary widely among individuals, creating significant challenges\nfor HWR. Since it is impossible to train an HWR model on data that includes\nevery handwriting style in the world, evaluating models on WI datasets provides\na more realistic measure of performance than WD or random-split datasets. WI\ndatasets ensure that the handwriting styles in the training set do not overlap\nwith those in the test set, better simulating real-world scenarios where a model\nmust generalize to unseen writers.\nAs children are beginners in handwriting, their handwriting styles differ from\nthose of adults, making HWR more challenging. However, they should not be\nexcluded as potential users of HWR systems. Therefore, developing a solution\nthat works well for both adults and children is essential for the success of an\nHWR system.\nAdditionally, while noise is common in IMU data and poses a challenge for\nHWR, this issue can be mitigated by manually introducing noise during training\nto improve the model’s resistance to noise.\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n13\n5.2\nData Efficiency\nMore data generally improves the performance of deep learning models, including\nHWR. However, supervising contributors and removing faulty samples, such as\ntypographical mistakes, makes collecting handwriting IMU data time-consuming\nand costly. Therefore, the ability to extract features efficiently from smaller\ndatasets is crucial for real-world applications. This can be achieved through\ndata augmentation, regularization, normalization, and learning rate scheduling,\nwhich help models converge better and perform well with limited training data.\n5.3\nFlexibility\nIn real-world deployment, handwriting inputs vary in size, posing challenges\nfor models that require fixed input dimensions. Transformer-based models, such\nas Swin Transformer V2 and xLSTM, rely on predefined input sizes, requiring\npadding or compression to meet these constraints. However, padding increases\ncomputational overhead, while compression can lead to information loss. In con-\ntrast, the CNN-BiLSTM design processes inputs of any size without resizing,\nimproving computational efficiency and adaptability, making it more suitable\nfor real-world HWR.\n6\nConclusion & Outlook\nIn conclusion, our experiments show that our model consistently outperforms\ncompetitors on WI datasets, demonstrating strong robustness, data efficiency,\nand flexibility in HWR. These strengths make it well-suited for real-world de-\nployment.\nHowever, our evaluation is limited by the dataset, as we have not compared\nperformance on left- and right-handed data and have observed significant im-\nprovements with larger datasets. Additionally, we have not investigated whether\nthere are patterns of errors that could provide insights for further reducing the\nerror rate. We also have yet to explore performance on sentence-level handwrit-\ning, which better reflects real-world use and could offer valuable directions for\nfuture research.\nSince HWR is closely related to language, incorporating natural language pro-\ncessing techniques, such as multimodal pretraining, could potentially encourage\nmodels to learn more semantic features and further enhance performance. Ex-\nploring these approaches should lead to even more robust and intelligent HWR\nsystems.\nReferences\n1. Alemayoh, T.T., Shintani, M., Lee, J.H., Okamoto, S.: Deep-learning-based char-\nacter recognition from handwriting motion data captured using imu and force\nsensors. Sensors 22(20) (2022). https://doi.org/10.3390/s22207840, https://www.\nmdpi.com/1424-8220/22/20/7840\n\n\n14\nJ. Li et al.\n2. Beck, M., Pöppel, K., Spanring, M., Auer, A., Prudnikova, O., Kopp, M., Klam-\nbauer, G., Brandstetter, J., Hochreiter, S.: xlstm: Extended long short-term\nmemory. In: Thirty-eighth Conference on Neural Information Processing Systems\n(2024), https://arxiv.org/abs/2405.04517\n3. Carbune, V., Gonnet, P., Deselaers, T., Rowley, H.A., Daryin, A., Calvo, M., Wang,\nL.L., Keysers, D., Feuz, S., Gervais, P.: Fast multi-language lstm-based online hand-\nwriting recognition. International Journal on Document Analysis and Recognition\n(IJDAR) 23(2), 89–102 (Jun 2020). https://doi.org/10.1007/s10032-020-00350-4,\nhttps://doi.org/10.1007/s10032-020-00350-4\n4. Choi, S.d., Lee, A.S., Lee, S.y.: On-line handwritten character recognition with 3d\naccelerometer. In: 2006 IEEE International Conference on Information Acquisition.\npp. 845–850 (2006). https://doi.org/10.1109/ICIA.2006.305842\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInternational Conference on Learning Representations (2021), https://openreview.\nnet/forum?id=YicbFdNTTy\n6. Graves, A., Fernández, S., Gomez, F., Schmidhuber, J.: Connectionist tempo-\nral classification: labelling unsegmented sequence data with recurrent neural net-\nworks. In: Proceedings of the 23rd International Conference on Machine Learning.\npp. 369–376. ICML ’06, Association for Computing Machinery, New York, NY,\nUSA (2006). https://doi.org/10.1145/1143844.1143891, https://doi.org/10.1145/\n1143844.1143891\n7. Hassan, E., Tarek, H., Hazem, M., Bahnacy, S., Shaheen, L., Elashmwai, W.H.:\nMedical prescription recognition using machine learning. In: 2021 IEEE 11th An-\nnual Computing and Communication Workshop and Conference (CCWC). pp.\n0973–0979 (2021). https://doi.org/10.1109/CCWC51732.2021.9376141\n8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (June 2016)\n9. Jeen-Shing, W., Yu-Liang, H., Cheng-Ling, C.: Online handwriting recognition\nusing an accelerometer-based pen device. In: Proceedings of the 2nd International\nConference on Advances in Computer Science and Engineering (CSE 2013). pp.\n231–234. Atlantis Press (2013/07). https://doi.org/10.2991/cse.2013.52, https://\ndoi.org/10.2991/cse.2013.52\n10. Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong,\nL., Wei, F., Guo, B.: Swin transformer v2: Scaling up capacity and resolution. In:\nInternational Conference on Computer Vision and Pattern Recognition (CVPR)\n(2022)\n11. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n12. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for\nthe 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) (2022)\n13. Lopez-Rodriguez, P., Avina-Cervantes, J.G., Contreras-Hernandez, J.L., Cor-\nrea, R., Ruiz-Pinales, J.: Handwriting recognition based on 3d accelerometer\ndata by deep learning. Applied Sciences 12(13) (2022). https://doi.org/10.3390/\napp12136707, https://www.mdpi.com/2076-3417/12/13/6707\n\n\nRobust and Efficient Writer-Independent IMU-based HWR\n15\n14. Meißl, F., Eibensteiner, F., Petz, P., Langer, J.: Online handwriting recognition\nusing lstm on microcontroller and imu sensors. In: 2022 21st IEEE International\nConference on Machine Learning and Applications (ICMLA). pp. 999–1004 (2022).\nhttps://doi.org/10.1109/ICMLA55696.2022.00167\n15. Meißl, F., Eibensteiner, F., Petz, P., Langer, J.: Online handwriting recognition\nusing lstm on microcontroller and imu sensors. In: 2022 21st IEEE International\nConference on Machine Learning and Applications (ICMLA). pp. 999–1004 (2022).\nhttps://doi.org/10.1109/ICMLA55696.2022.00167\n16. Ott, F., Rügamer, D., Heublein, L., Hamann, T., Barth, J., Bischl, B., Mutschler,\nC.: Benchmarking online sequence-to-sequence and character-based handwriting\nrecognition from imu-enhanced pens. Int. J. Doc. Anal. Recognit. 25(4), 385–\n414 (Dec 2022). https://doi.org/10.1007/s10032-022-00415-6, https://doi.org/10.\n1007/s10032-022-00415-6\n17. Pontart, V., Bidet-Ildei, C., Lambert, E., Morisset, P., Flouret, L., ALA-\nMARGOT, D.: Influence of handwriting skills during spelling in primary\nand lower secondary grades. Frontiers in Psychology 4 (2013). https://doi.\norg/10.3389/fpsyg.2013.00818, https://www.frontiersin.org/journals/psychology/\narticles/10.3389/fpsyg.2013.00818\n18. Seuret, M., van der Loop, J., Weichselbaumer, N., Mayr, M., Molnar, J., Hass, T.,\nChristlein, V.: Combining ocr models for reading early modern books. In: Fink,\nG.A., Jain, R., Kise, K., Zanibbi, R. (eds.) Document Analysis and Recognition -\nICDAR 2023. pp. 342–357. Springer Nature Switzerland, Cham (2023)\n19. Tolstikhin, I.O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner,\nT., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., Dosovitskiy, A.:\nMlp-mixer: An all-mlp architecture for vision. In: Ranzato, M., Beygelzimer, A.,\nDauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information\nProcessing Systems. vol. 34, pp. 24261–24272. Curran Associates, Inc. (2021)\n20. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V., Bengio,\nS., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in\nNeural Information Processing Systems. vol. 30. Curran Associates, Inc. (2017)\n21. Wehbi, M., Hamann, T., Barth, J., Eskofier, B.: Digitizing handwriting with a\nsensor pen: A writer-independent recognizer. In: 2020 17th International Con-\nference on Frontiers in Handwriting Recognition (ICFHR). pp. 295–300 (2020).\nhttps://doi.org/10.1109/ICFHR2020.2020.00061\n22. Wehbi, M., Hamann, T., Barth, J., Kaempf, P., Zanca, D., Eskofier, B.: Towards an\nimu-based pen online handwriting recognizer. In: Lladós, J., Lopresti, D., Uchida,\nS. (eds.) Document Analysis and Recognition – ICDAR 2021. pp. 289–303. Springer\nInternational Publishing, Cham (2021)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20954v1.pdf",
    "total_pages": 15,
    "title": "Robust and Efficient Writer-Independent IMU-Based Handwriting Recognization",
    "authors": [
      "Jindong Li",
      "Tim Hamann",
      "Jens Barth",
      "Peter Kaempf",
      "Dario Zanca",
      "Bjoern Eskofier"
    ],
    "abstract": "Online handwriting recognition (HWR) using data from inertial measurement\nunits (IMUs) remains challenging due to variations in writing styles and the\nlimited availability of high-quality annotated datasets. Traditional models\noften struggle to recognize handwriting from unseen writers, making\nwriter-independent (WI) recognition a crucial but difficult problem. This paper\npresents an HWR model with an encoder-decoder structure for IMU data, featuring\na CNN-based encoder for feature extraction and a BiLSTM decoder for sequence\nmodeling, which supports inputs of varying lengths. Our approach demonstrates\nstrong robustness and data efficiency, outperforming existing methods on WI\ndatasets, including the WI split of the OnHW dataset and our own dataset.\nExtensive evaluations show that our model maintains high accuracy across\ndifferent age groups and writing conditions while effectively learning from\nlimited data. Through comprehensive ablation studies, we analyze key design\nchoices, achieving a balance between accuracy and efficiency. These findings\ncontribute to the development of more adaptable and scalable HWR systems for\nreal-world applications.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}