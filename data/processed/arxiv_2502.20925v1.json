{
  "id": "arxiv_2502.20925v1",
  "text": "Amortized Conditional Independence Testing\nBao Duong, Nu Hoang⋆, Thin Nguyen\nApplied Artificial Intelligence Institute (A2I2), Deakin University, Australia\n{duongng, nu.hoang, thin.nguyen}@deadkin.edu.au\nAbstract. Testing for the conditional independence structure in data is\na fundamental and critical task in statistics and machine learning, which\nfinds natural applications in causal discovery–a highly relevant problem\nto many scientific disciplines. Existing methods seek to design explicit\ntest statistics that quantify the degree of conditional dependence, which\nis highly challenging yet cannot capture nor utilize prior knowledge in\na data-driven manner. In this study, an entirely new approach is intro-\nduced, where we instead propose to amortize conditional independence\ntesting and devise ACID (Amortized Conditional InDependence test)–\na novel transformer-based neural network architecture that learns to test\nfor conditional independence. ACID can be trained on synthetic data\nin a supervised learning fashion, and the learned model can then be ap-\nplied to any dataset of similar natures or adapted to new domains by\nfine-tuning with a negligible computational cost. Our extensive empirical\nevaluations on both synthetic and real data reveal that ACID consis-\ntently achieves state-of-the-art performance against existing baselines\nunder multiple metrics, and is able to generalize robustly to unseen sam-\nple sizes, dimensionalities, as well as non-linearities with a remarkably\nlow inference time.\nKeywords: Conditional independence testing · Transformer · Amor-\ntization\n1\nIntroduction\nConditional independence testing is a cornerstone of statistical and machine\nlearning research, with critical applications in causal discovery [9,26,16], which\nhas broad implications across scientific fields. The Conditional Independence\n(CI) testing problem is formulated as follows: given empirical data of random\nvariables X, Y , and Z, we seek to answer whether or not X and Y are statistically\nindependent after all the influences from Z to X and Y have been accounted for.\nMore formally, the problem can be described in the hypothesis testing framework\nwith:\nH0 : X ⊥⊥Y | Z\nand\nH1 : X ̸⊥⊥Y | Z\n(1)\nwhere H0 and H1 are termed as the Null hypothesis and the Alternative hypoth-\nesis, respectively. Following [3,5], we refer to X and Y as the target variables\nand Z as the conditioning variable.\nIdeally, the conditional independence X ⊥⊥Y | Z can be checked using\nthe probability density/mass function p (·) of the underlying distribution, i.e.,\n⋆Corresponding author\narXiv:2502.20925v1  [stat.ML]  28 Feb 2025\n\n\n2\nBao Duong, Nu Hoang, Thin Nguyen\nX ⊥⊥Y | Z if and only if p (x, y | z) = p (x | z) p (y | z) for all x, y, z in the sup-\nports. This condition is intractable to verify for continuous and high-dimensional\nempirical data, thus many methods must resort to discretization [4,27], which\nsuffers from the curse of dimensionality. This intrinsic challenge of CI testing and\nits significant impacts motivate us to tackle the problem under the most chal-\nlenging setting, where all variables are continuous and may be multi-dimensional.\nTraditionally, there is a common strategy to design a CI testing method,\nwhich formulates a so-called test statistics to quantify the conditional depen-\ndence between the target variables given the conditioning variable. Then, the\ndistribution of the test statistics under H0, i.e., the Null distribution, is derived.\nNext, given a test statistics value t inferred from input data, the Null distribu-\ntion is used to compute the probability that any dataset coming from H0 has\na test statistics more “extreme” than t, which is called the test’s p-value. Fi-\nnally, if the p-value is less than a pre-specified significance level α then H0 is\nrejected, otherwise we fail to reject H0. While being mathematically justified,\nthis strategy entails two challenges. First, the test statistics must well reflect\nthe degree of conditional dependence and should be efficiently estimated from\ndata, which is technically challenging and may involves restrictive assumptions.\nFor example, while partial correlation is a simple measure of conditional depen-\ndence that can be easily computed, it is only valid when data follows a jointly\nmultivariate Gaussian or similar distribution [2]; meanwhile, conditional mutual\ninformation is a powerful measure, but it is intractable to compute exactly, thus\nrequires approximations [22,14,6]. Second, to estimate the p-value, the null dis-\ntribution must be easily derived from data, either analytically or empirically. As\nan example, the partial correlation is not used as-is for CI testing because its\nnull distribution is complex even for multivariate Gaussian data, so the Fisher’s\nz-transformation was devised to transform it to an approximately Gaussian vari-\nable [8]. Moreover, when no analytical transformation can be found to obtain a\nsimple null distribution, bootstrapping is adopted to simulate the null distribu-\ntion from empirical data [20,17], in which the user must trade-off between the\ncomputational expense and accuracy via the number of bootstraps.\nPresent study. Given the obstacles discussed above in designing a high-quality\ntest statistic, this study offers an entirely different approach to test for condi-\ntional independence, motivated by the transition of learning algorithms from\nmachine learning to deep learning. Particularly, deep learning (DL) exhibits a\nhuge technological leap from classical machine learning, where features are hand-\ncrafted, to the stage where features are learned by effectively utilizing data. This\ncapability has fueled a surge in developing heuristic models for diverse tasks,\nsuch as causal structure learning [15,10] and combinatorial optimization [11,23].\nBuilding on this momentum, we propose to innovate CI testing by learning to\ntest for conditional independence. In other words, unlike existing methods that\ndemand designing specific test statistics, we amortize conditional independence\ntesting via a neural architecture that analyzes the entire dataset at once. This\nnetwork is trained in a supervised manner to determine whether the null hy-\npothesis H0 is rejected or not. Designing such a network is tricky, requiring to\nbe flexible enough to handle datasets with varying numbers of variables and ro-\n\n\nAmortized Conditional Independence Testing\n3\n(d) Cross-attention \nover dimensions\nMLP\nSummary\n(a) Input data\n(b) Embeddings\n(c) Self-attention \nover dimensions\n(e) Self-attention \nover samples\n(f) Per-sample inter-dependences \nbetween dimensions\nTest statistics\n(g) Null distribution\nFig. 1. Overview of the proposed ACID architecture.\nbust to the order of the data, across all both samples and variables. To this end,\nwe leverage the transformer architecture [25], a breakthrough invention in deep\nlearning, to design a new architecture for CI testing, coined ACID (Amortized\nConditional Independence testing), which can be trained end-to-end on simu-\nlated data, where the label is available, to reach arbitrary levels of competence\nin CI testing. Notably, the trained model demonstrates impressive performance\nwhen tested with unseen datasets, especially generalizing to various data char-\nacteristics like sample size, dimensionality, and non-linearity, coupled with a\nnear-zero inference expense, effectively amortizing its training cost.\nThe main contributions in this study include:\n1. We introduce a novel approach for conditional independence testing, by\namortizing the procedure with a neural network that can learn to perform\nconditional independence tests directly from data. To the best of our knowl-\nedge, this is the first study exploring this promising territory, contributing\na significantly new perspective to the literature of the field.\n2. We present ACID, a tailor-made neural architecture that can can operate\nwith variable-sized datasets and is invariant to permutations in both the\nsample and variable dimensions, maximizing its statistical efficacy.\n3. We demonstrate the effectiveness of the proposed ACID architecture in\ncomparison with state-of-the-art CI tests on a wide range of experiments\nover synthetic datasets and real data.\n2\nProposed Method\nIn this section we describe in details the proposed ACID architecture for CI\ntesting, depicted in Figure 1.\n2.1\nA supervised learning approach for CI testing\nWe denote an i.i.d. dataset as D =\n\b\u0000x(i) ∈RdX, y(i) ∈RdY , z(i) ∈RdZ\u0001\tn\ni=1 and its label\nG ∈{0, 1} corresponds to two classes H0 and H1, respectively. The innovation\nin this study is seeded from the switch of perspective, from seeing D as a mere\ndataset to seeing D as a random variable in the context of CI testing. Thus, there\nmust exist a data generating distribution p (D, G) between datasets and labels.\nOnce both are observed, we can lean to predict a dataset’s label by estimating\nthe conditional distribution of the label given the dataset p (G | D). Towards\nlearning this conditional, we model it as pθ (G | D) := Bernouli (G; F (D, θ)),\n\n\n4\nBao Duong, Nu Hoang, Thin Nguyen\nwhere F (·, θ) is a neural network parametrized by θ that maps a dataset to the\nparameter (typically the real-valued logit) of the Bernoulli distribution. Then,\nthe neural network can be trained via minimizing the binary cross entropy (BCE)\nloss commonly employed in binary classification:\nθ∗:= arg min\nθ\nL (θ) , where\n(2)\nL (θ) := −Ep(D,G)\n\u0002\nG ln pθ (G | D) + (1 −G) ln pθ\n\u0000 ¯G | D\u0001\u0003\n(3)\nTo accommodate the fact that G is usually not observed, we leverage synthetic\ndata where an unlimited amount of labeled data is available, allowing us to train\nthe neural network F to the fullest extent possible.\nThe remaining challenge for us now is to design a sophisticate neural net-\nwork that can take as input a whole dataset, which is significantly different from\ntypical applications where a neural network only takes a sample as input. Not\nonly so, the CI problem also requires the neural network to be agnostic of the\ndataset’s size and arrangement, since shuffling data samples or dimensions must\nnot change the CI outcome. To achieve this goal, the Transformer architecture\n[25] is the most well-fit device for two reasons. First, all of its learnable parame-\nters does not depend on the dataset’s size, hence the learned model can be applied\nto arbitrarily sized datasets, enabling us to design a size-independent represen-\ntation of a whole dataset. The second reason involves the inherent permutation-\ninvariance property of attention mechanisms, allowing us to encode the dataset\nin a way that shuffling the dimensions or samples does not change the prediction.\n2.2\nModel architecture\nMulti-head attention Attention mechanism [25] was introduced to construct\na representation for one data sequence by attending and aggregating information\nfrom another data sequence. In traditional applications, attention mechanisms\nare adopted for modeling sequentially ordered data, such as sentence or temporal\ndata in general. However, in our application the order must not be accounted for\ndue to the i.i.d. nature of the samples and the irrelevance between the dimen-\nsions arrangement w.r.t. the CI label. Following [15,10], we detail below how the\nattention is customized for the CI testing problem.\nAssuming we have two data matrices A ∈Rn×dA and B ∈Rm×dB where we\nwish to exploit information from B to produce a new representation for A. One\nof the most common attention mechanisms–the scaled dot-product attention\n[25]–computes the attention weights, i.e., how much to attend to each position,\nby comparing the queries Q = AW Q ∈Rn×e with the keys K = BW K ∈Rm×e\nusing their dot-product, scaled by the squared root of the embedding size e, then\nthe updated representation is the aggregation of the values V = BW V ∈Rm×e\nweighted by the softmax-activated attention weights:\nattn (A, B) := softmax\n\u0012QK⊤\n√e\n\u0013\nV ∈Rn×e\n(4)\nwhere W Q ∈RdA×e, W K ∈RdB×e, and W V ∈RdB×e are learnable projection\nmatrices that linearly map the inputs A and B to the query, key, and value\n\n\nAmortized Conditional Independence Testing\n5\nspaces, respectively. Self-attention is a special form of attention where A = B.\nConversely, cross-attention is the attention specifically for two different entities\nA ̸= B. Multi-head attention is an extension of the ordinal attention mechanism,\nwhich attends to different aspects of the reference sequence, empowering a richer\nrepresentation for the target sequence. To achieve this, multi-head attention\nperforms a series of h independent attentions, then concatenate and mix them\naltogether:\nmhah (A, B) := concat (H1, . . . , Hh) W H ∈Rn×e\n(5)\nwhere Hi := attn\n\u0000AW A\ni , BW B\ni\n\u0001\nis an attention head with the inputs linearly\nprojected to another space via the learnable weight matrices W A\ni\n∈RdA×e and\nW B\ni\n∈RdB×e, and W H ∈R(e·h)×e is another learnable weight matrix that mixes\nthe outputs of different attention heads.\nIn addition, following the best practices in Transformer-based models [25,28,13],\nwe first add a skip connection [7], then a Dropout [24], followed by Layer Nor-\nmalization [1] to create an intermediate representation:\nIR (A, B) := LN \u0000AW Q + Dropout (mhah (A, B))\u0001\n(6)\nThen, we apply a position-wise feed-forward network (FFN) [25], followed by\nDropout and another skip connection, and obtain the full multi-head attention\nlayer for our architecture:\nMHAttn (A, B) := AW Q + Dropout (FFN (IR (A, B)))\n(7)\nWhen multi-head self-attention of A is used, we simply denote the output as\nMHAttn (A).\nWhole-dataset Encoder Given a dataset D =\n\b\u0000x(i), y(i), z(i)\u0001\tn\ni=1, we first stack\nthe samples of each random variables to create data matrices X ∈Rn×dX,\nY ∈Rn×dY , and Z ∈Rn×dZ. The goal of the dataset encoder is to discover the\nintrinsic statistical properties of the given dataset to infer what makes it con-\nditionally independent of dependent. With the help of attention mechanisms,\nour data encoder exploits the intra-dimensional, inter-dimensional, as well as\nintra-sample relationships presented in the dataset. These are performed in mul-\ntiple layers, where each layer is expected to produce a finer representation of\nthe dataset. In our implementation, L = 4 layers of three types of attentions\nare used. Additionally, it is important to note that X and Y share the same\nrole in the CI condition, because X ⊥⊥Y | Z is equivalent with Y ⊥⊥X | Z.\nTo preserve this symmetry in our architecture, every operator applied on X is\napplied identically on Y with the same set of parameters. Hence, for brevity, in\nthe remaining part of this section we omit the descriptions of the operators on\nY , which are similar to X.\nData entry embedding Before we can encode the dataset, we first need to embed\neach entry in the data matrices from a scalar to a vector of size e. The embeddings\nare obtained by passing each individual data entry to a shared position-wise feed-\nforward network (FFN). More particularly, we use two FFNs namely EmbXY (·) :\n\n\n6\nBao Duong, Nu Hoang, Thin Nguyen\nR →Re shared between X and Y , and EmbZ (·) : R →Re for Z solely. This\nprocess results in the initial representations of X, Y , and Z, which we denote as\nX (0) := EmbXY (X) ∈Rn×dX×e\n(8)\nZ(0) := EmbZ (Z) ∈Rn×dZ×e\n(9)\nSelf-attention over dimensions (SoD) We are now in a position to encode the\ndataset. We proceed by applying (multi-head) self-attention (Equation 7) to each\nvariable to capture their intra-dimensional relationships. Particularly, for the l-\nth layer (l = 1 . . . L), we perform the following independently and identically for\nevery row i = 1 . . . n:\nSoD(l)\nXY\n\u0010\nX (l−1)\ni\n\u0011\n:= MHAttn\n\u0010\nX (l−1)\ni\n\u0011\n(10)\nSoD(l)\nZ\n\u0010\nZ(l−1)\ni\n\u0011\n:= MHAttn\n\u0010\nZ(l−1)\ni\n\u0011\n(11)\nwhere the SoD modules are shared across all samples, therefore they are unaf-\nfected by the arrangement of the samples.\nCross-attention over dimensions (CoD) from Z to X and Y We next fuse the\ninformation of Z into X and Y with the end goal being the representations of\nX and Y that contain minimal information about Z. While this may sound\ncontradictory, it is inspired from the observation that X ⊥⊥Y\n| Z if and\nonly if after removing all information of Z from X and Y , the remaining of\nX and Y are marginally independent. This is mathematically supported by [5],\nwhere it is proved that if there exists invertible functions fX and fY such that\n(fX (X, Z) , fY (Y, Z)) ⊥⊥Z, i.e., they remove all information of Z from X and\nY , then X ⊥⊥Y | Z ⇔fX (X, Z) ⊥⊥fY (Y, Z). Our CoD module mimics this\nfeature of these functions. Specifically, at layer l we distribute the information\nof Z into X and Y by applying the following identically for each row i = 1 . . . n\nof X and Y :\nCoD(l) \u0010\nX (l−1)\ni\n, Z(l−1)\ni\n\u0011\n:= MHAttn\n\u0010\nX (l−1)\ni\n, Z(l−1)\ni\n\u0011\n(12)\nSelf-attention over samples (SoS) While the first two attention modules exploit\nthe dependence mechanism across data dimensions, our last attention module\ndetects the distributional characteristics of the dataset by attending over the\ndatapoints, as suggested in [13]. To be more specific, the l-th layer of SoS is\ndefined identically for each column j of the data matrices as:\nSoS(l)\nXY\n\u0010\nX (l−1)\n:,j\n\u0011\n:= MHAttn\n\u0010\nX (l−1)\n:,j\n\u0011\n(13)\nSoS(l)\nZ\n\u0010\nZ(l−1)\n:,j\n\u0011\n:= MHAttn\n\u0010\nZ(l−1)\n:,j\n\u0011\n(14)\nIn summary, the final output of each attention layer is the sum of the three\nkinds of attention above with a skip connection, followed by an FFN for increased\ncapacity:\nX (l) := FFN(l)\nXY\n\u0000X (l−1) + SoDXY (·) + CoD (·) + SoSXY (·)\u0001\n(15)\nZ(l) := FFN(l)\nZ\n\u0000Z(l−1) + SoDZ (·) + SoSZ (·)\u0001\n(16)\nwhere we omit the input arguments to avoid notational clutter.\n\n\nAmortized Conditional Independence Testing\n7\nEncoder summary After a series of attention layers, as argued above, we expect\nthe dependence between X (L) and Y(L) can expose the conditional dependence of\nthe original variables, which motivates us to measure their dependence. Towards\nthis end, we begin by extending the embedding dimension by h times, which\ncorrespond to h inter-dependence heads that focus on different aspects of the\ndata, inspired by multi-head attentions. This is done via a single FFN Re →Rh×e\nas follows:\n˜\nX := FFN \u0000X (L)\u0001\n∈Rn×dX×h×e\n(17)\n˜Y := FFN \u0000Y(L)\u0001\n∈Rn×dY ×h×e\n(18)\nThen, we measure the sample-level inter-dependence magnitudes between the\ndimensions of ˜\nX and ˜Y using the squared dot-product of the embeddings:\nS := \u0000 ˜\nX ⊗˜Y\u00012 ∈Rn×dX×dY ×h\n(19)\nNext, we average this over the sample axis and take the maximum magnitude\nover every pairs of dimensions as the final h-dimensional vector representation\nof the dataset, where the r-th value corresponds to the r-th inter-dependence\nhead:\nRr := max\ni,j\n1\nn\nn\nX\no=1\nSo,i,j,r\n(20)\nSince P is invariant to samples permutations and max is invariant to dimen-\nsions permutations, R is permutation-invariant to both samples and dimensions\nof D. Additionally, in our experiments, the number of attention heads and inter-\ndependence heads are the same and equal to h = 8.\nClassification The final representation vector of the dataset is used as input\nfor a Multiple-layer Perceptron (MLP) T : Rh →R that outputs the logit of\nthe conditional distribution pθ (G | D). In other words, pθ (G | D) is a Bernoulli\ndistribution with mean equals to sigmoid (T (R (D))). Plugging this into Equa-\ntion 3 we obtain a fully differentiable system that is end-to-end trainable with\nany gradient-based optimization method. In our implementation, the popular\nmomentum-based Adam optimizer [12] is employed for this purpose.\n2.3\nTest statistic and Null distribution\nOnce trained, the predicted logit is used as our test statistics since they are\npredictive of the CI label by design. As for the null distribution, we collect the\npredicted logits of training datasets with label G = 0 to construct an empirical\nnull distribution. Our experiments (Section 3.2) show that this distribution is\nsimple with a normal-like shape. Therefore, we fit it with a slightly more flexible\ndistribution, i.e., the skewed normal distribution [18], and use it to calculate\nthe p-value for all datasets. Here, higher values of the test statistics are consid-\nered more extreme since datasets from H0 should have the logits, i.e., the test\nstatistics, as low as possible. Hence, the p-value in our framework is the extreme\nregion on the right tail of the null distribution:\n\n\n8\nBao Duong, Nu Hoang, Thin Nguyen\nFig. 2.\nThe\nlearning\nprocess of\nACID. Val-\nidation\ndataset\nis\nnot\nneeded since no dataset is\nseen twice during train-\ning. Shaded areas depict\nstandard deviations.\nblank\np-value (D) := 1 −Φ (T (R (D)))\n(21)\nwhere Φ is the cumulative distribution function of the fitted skewed normal\ndistribution.\n3\nExperiments\n3.1\nExperimental Setting\nMethods and evaluation criterion We compare the CI testing performance\nof the proposed ACID test in comparison with both popular and recent state-\nof-the-arts. Specifically, we consider KCIT [30]–a well known and widely adopted\nkernel-based method, SCIT [29]–a recent development from regression-based\nmethods, and LCIT [5]–a recent DL approach based on representation learn-\ning. Regarding evaluation metrics, we employ Area Under the Receiver Operat-\ning Characteristic Curve (ROC AUC, or AUC), F1 score, Type I, and Type II\nerrors.\nSynthetic data For the synthetic data generation process to be as generic as\npossible so models trained on this scheme can have robust generalizability, we\ngenerate data following six data models of conditional (in)dependences based on\nthree canonical types of graphical d-separation [19]. To incorporate functional\nmechanisms of varying non-linearity, we employ MLPs as the function class for\nthe links between the variables. We denote MLP-k as the class of MLPs with\none hidden layer of size k and we control the level of non-linearity via the choice\nof k. In all comparative experiments, we generate 200 i.i.d. datasets from the\nsimulated distribution with 100 datasets belonging to class H0 and 100 datasets\nbelonging to H1. To account for uncertainty, we randomly divide them into five\n\n\nAmortized Conditional Independence Testing\n9\n100\n200\n300\n400\n500\nSample size\n0\n20\n40\n60\n80\n100\n%\nROC AUC ↑\n100\n200\n300\n400\n500\nSample size\nF1 score ↑\n100\n200\n300\n400\n500\nSample size\nType I error ↓\n100\n200\n300\n400\n500\nSample size\nType II error ↓\nKCIT\nLCIT\nSCIT\nACID (Ours)\n(a) Conditional Independence Testing performance w.r.t. different sample sizes\n25\n50\n75\n100\nDimension of Z\n0\n20\n40\n60\n80\n100\n%\nROC AUC ↑\n25\n50\n75\n100\nDimension of Z\nF1 score ↑\n25\n50\n75\n100\nDimension of Z\nType I error ↓\n25\n50\n75\n100\nDimension of Z\nType II error ↓\nKCIT\nLCIT\nSCIT\nACID (Ours)\n(b) Conditional Independence Testing performance w.r.t. different dimensionalities of Z\nFig. 3. Conditional Independence Testing performance on synthetic data. The evalua-\ntion metrics are AUC, F1 score (higher is better), Type I and Type II errors (lower is\nbetter). Error bars are 95% confidence intervals.\nFig. 4.\nConditional\nIndependence\nTesting\nperformance\nin\nOut-of-\ndistribution settings. The\nperformance metrics are\nAUC,\nF1\nscore\n(higher\nis better), Type I and\nType II errors (lower is\nbetter).\nError\nbars\nare\n95% confidence intervals.\nblank\nMLP-08\nMLP-08\nMLP-16\nMLP-16\nMLP-32\nMLP-32\nMLP-64\nMLP-64\nTested on\nTrained on\nseparate folds for estimating the metrics’ variation. Moreover, to make sure no\ntest data is seen during training, we use separate random seed ranges for the\ntraining and test data.\n3.2\nResults on Synthetic Data\nTraining of ACID We first train ACID on datasets with n ∈{50, 200}, dX =\ndY = 1, dZ ∈{5, 10, 20}, and k = 16. Batches of fresh synthetic datasets are\ncontinuously generated for training ACID so that a dataset is very rarely seen\ntwice, maximizing the model’s generalizability on the training domain and avoid\noverfitting. This model is trained strictly on small-scale datasets to demonstrate\nthe generalizability to larger-scale data (Section 3.2 and Section 3.2). Figure 2\npresents the training summary. The training process converges after 50,000 steps\nat a loss of 0.1. However, the model clearly separates the H0 and H1 logits and\nachieves an AUC of nearly 100% after just 10,000 steps, which takes only about\n\n\n10\nBao Duong, Nu Hoang, Thin Nguyen\nFig. 5. Conditional Independence\nTesting performance on Real data\n(the Sachs dataset [21]). Time is\nmeasured on an Apple M1 CPU\nwith 8 GB of RAM.\nblank\n0\n10\n20\nTime (s)\n60\n65\n70\n75\nAUC (%)\nKCIT\nLCIT\nSCIT\nACID (Ours)\n50 minutes on a system with an Nvidia V100 32GB GPU. This confirms that\nthe ACID architecture is indeed able to test for conditional independence given\nsufficient training data, even if the datasets possess a limited number of samples.\nGeneralization to Sample sizes We next test ACID’s ability to generalize\nto datasets with different sample sizes than what it was trained on. Specifically,\nwe fix dX = dY = 1, dZ = 10 and compare the CI testing performance of ACID\nagainst state-of-the-arts in MLP-16 with sample sizes varying from 5 to 500. The\nresult is reported in Figure 3(a), which shows that ACID is far more effective\nthan its baseline counterparts for all sample sizes. More particularly, from 100\nsamples to 500 samples, our method always achieves the highest AUC scores of\nnearly 100%, vanishing Type II errors, highest F1 scores of at least 80%, and\nType I errors competitive with baseline methods. At the extreme case of only 5\nsamples per dataset, ACID still achieves the highest AUC of over 80%, while\nother competitors struggle at 50%. Additionally, even though with just 5 samples\nour Type II error is up to 70% due to the lack of information, the situation is\nquickly mitigated as the sample size increases to 100, where the Type II error of\nACID reaches below 20% but other state-of-the-arts still struggle at over 70%.\nGeneralization to Dimensionalities To evaluate the generalizability of the\nproposed ACID architecture over different dimensionalities of the conditioning\nvariable Z, we fix n = 300, dX = dY = 1, and vary dZ from 5 to 100 dimensions.\nThe result depicted in Figure 3(b) demonstrates the remarkable extendability of\nACID, which was trained on data of only 20 dimensions but still consistently\nsurpasses all baselines in 100 dimensions, as evidenced by the highest AUC and\nF1 scores, as well as lowest Type II errors in all cases, while Type I error is\ncomparable to other competitors.\nOut-of-distribution Generalization to Non-linearity In the following, we\nexamine ACID’s ability to generalize beyond the training distribution. Particu-\nlarly, we consider four different data domains with exponentially increasing dif-\nficulties, namely MLP-08, MLP-16, MLP-32, and MLP-64. Each environment is\nused to train an ACID model, which is subsequently cross-validated on the other\ndomains. In Figure 4 we display the out-of-distribution generalization results.\nThe hardness of the considering environments is reflected in the in-distribution\nnumbers, where the easiest environment MLP-08 can be learned to the fullest\n\n\nAmortized Conditional Independence Testing\n11\nextent, with near 100% AUC and F1 scores, as well as vanishing Type I and\nType II errors. Meanwhile, models trained on the most challenging environment\nMLP-64 can only reach 90% in AUC and F1, with 20% of Type I error.\n3.3\nPerformance and Scalability on Real Data\nTo validate the effectiveness of the proposed ACID test, we demonstrate evalu-\nate it against state-of-the-arts in CI testing on the Sachs dataset [21], a common\nbenchmarking real dataset. We adapt our model to the real data by fine-tuning\na model in Section 3.2 for only 500 steps on really small down-sampled datasets\nof merely 50 samples. The process takes no more than three minutes on a system\nwith an Nvidia V100 32GB GPU. Then, in Figure 5 we compare the performance\nof all methods along with their runtime. The time is measured on an Apple M1\nCPU with 8 GB of RAM. It can be seen that our method achieves the best\nperformance both in terms of AUC and inference time. With under one second\nof inference time for each dataset of 853 samples and dZ = 3, we achieve 78%\nof AUC, while the second-best method, which is KCIT, can only achieve 74%,\nwith an average inference time of 10 seconds per dataset. Meanwhile, SCIT is\nthe fastest method but obtains the lowest AUC score among all.\n4\nConclusions\nIn this study we propose a novel supervised learning approach to conditional\nindependence testing called ACID, which amortizes the conditional indepen-\ndence testing process via a neural network that can learn to test for conditional\nindependence. The performance of our ACID architecture is demonstrated via\na wide range of experiments on both synthetic and real datasets, which show\nthat ACID can generalize well beyond the trained data and consistently out-\nperform existing state-of-the-arts. Our architecture has potential to be extended\nfurthermore to handle more challenging data scenarios, such as mixed-type or\nmissing value, which will be the subject of future development.\nReferences\n1. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv:1607.06450 (2016)\n2. Baba, K., Shibata, R., Sibuya, M.: Partial correlation and conditional correlation\nas measures of conditional independence. Aust. Nz. J. Stat. 46, 657–664 (2004)\n3. Bellot, A., van der Schaar, M.: Conditional independence testing using generative\nadversarial networks. In: NeurIPS. vol. 32 (2019)\n4. Diakonikolas, I., Kane, D.M.: A new approach for testing properties of discrete\ndistributions. In: FOCS (2016)\n5. Duong, B., Nguyen, T.: Conditional independence testing via latent representation\nlearning. In: ICDM (2022)\n6. Duong, B., Nguyen, T.: Diffeomorphic information neural estimation. In: AAAI\n(2023)\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n8. Hotelling, H.: New light on the correlation coefficient and its transforms. J. R.\nStat. Soc. 15(2), 193–232 (1953)\n\n\n12\nBao Duong, Nu Hoang, Thin Nguyen\n9. Jaber, A., Kocaoglu, M., Shanmugam, K., Bareinboim, E.: Causal discovery\nfrom soft interventions with unknown targets: Characterization and learning. In:\nNeurIPS. vol. 33 (2020)\n10. Ke, N.R., Chiappa, S., Wang, J.X., Bornschein, J., et al.: Learning to induce causal\nstructure. In: ICLR (2023)\n11. Khalil, E., Dai, H., Zhang, Y., Dilkina, B., Song, L.: Learning combinatorial opti-\nmization algorithms over graphs. In: NeurIPS. vol. 30 (2017)\n12. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR\n(2015)\n13. Kossen, J., Band, N., Lyle, C., Gomez, A.N., Rainforth, T., Gal, Y.: Self-attention\nbetween datapoints: Going beyond individual input-output pairs in deep learning.\nIn: NeurIPS. vol. 34, pp. 28742–28756 (2021)\n14. Kubkowski, M., Mielniczuk, J., Teisseyre, P.: How to gain on power: Novel condi-\ntional independence tests based on short expansion of conditional mutual informa-\ntion. JMLR 22(62), 1–57 (2021)\n15. Lorch, L., Sussex, S., Rothfuss, J., Krause, A., Schölkopf, B.: Amortized inference\nfor causal structure learning. In: NeurIPS. vol. 35, pp. 13104–13118 (2022)\n16. Mooij, J.M., Claassen, T.: Constraint-based causal discovery using partial ancestral\ngraphs in the presence of cycles. In: UAI (2020)\n17. Mukherjee, S., Asnani, H., Kannan, S.: CCMI: Classifier based conditional mutual\ninformation estimation. In: UAI (2020)\n18. O’Hagan, A., Leonard, T.: Bayes estimation subject to uncertainty about param-\neter constraints. Biometrika 63(1), 201–203 (1976)\n19. Pearl, J.: Causality: Models, Reasoning and Inference. Cambridge University Press\n(2009)\n20. Runge, J.: Conditional independence testing based on a nearest-neighbor estimator\nof conditional mutual information. In: AISTATS (2018)\n21. Sachs, K., Perez, O., Pe’er, D., Lauffenburger, D.A., Nolan, G.P.: Causal protein-\nsignaling networks derived from multiparameter single-cell data. Science 308, 523–\n529 (2005)\n22. Samo, Y.L.K.: Inductive mutual information estimation: A convex maximum-\nentropy copula approach. In: AISTATS (2021)\n23. Shrivastava, H., Chen, X., Chen, B., Lan, G., Aluru, S., Liu, H., Song, L.: GLAD:\nLearning sparse graph recovery. In: ICLR (2020)\n24. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\nDropout: a simple way to prevent neural networks from overfitting. JMLR 15(1),\n1929–1958 (2014)\n25. Vaswani, A., Shazeer, N., Parmar, N., et al.: Attention is all you need. In: NeurIPS.\nvol. 30 (2017)\n26. Versteeg, P., Mooij, J., Zhang, C.: Local constraint-based causal discovery under\nselection bias. In: CLeaR (2022)\n27. Warren, A.: Wasserstein conditional independence testing. arXiv:2107.14184 (2021)\n28. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan,\nY., Wang, L., Liu, T.: On layer normalization in the transformer architecture. In:\nICML (2020)\n29. Zhang, H., Zhang, K., Zhou, S., Guan, J.: Residual similarity based conditional\nindependence test and its application in causal discovery. In: AAAI (2022)\n30. Zhang, K., Peters, J., Janzing, D., Schölkopf, B.: Kernel-based conditional inde-\npendence test and application in causal discovery. In: UAI (2011)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20925v1.pdf",
    "total_pages": 12,
    "title": "Amortized Conditional Independence Testing",
    "authors": [
      "Bao Duong",
      "Nu Hoang",
      "Thin Nguyen"
    ],
    "abstract": "Testing for the conditional independence structure in data is a fundamental\nand critical task in statistics and machine learning, which finds natural\napplications in causal discovery - a highly relevant problem to many scientific\ndisciplines. Existing methods seek to design explicit test statistics that\nquantify the degree of conditional dependence, which is highly challenging yet\ncannot capture nor utilize prior knowledge in a data-driven manner. In this\nstudy, an entirely new approach is introduced, where we instead propose to\namortize conditional independence testing and devise ACID - a novel\ntransformer-based neural network architecture that learns to test for\nconditional independence. ACID can be trained on synthetic data in a supervised\nlearning fashion, and the learned model can then be applied to any dataset of\nsimilar natures or adapted to new domains by fine-tuning with a negligible\ncomputational cost. Our extensive empirical evaluations on both synthetic and\nreal data reveal that ACID consistently achieves state-of-the-art performance\nagainst existing baselines under multiple metrics, and is able to generalize\nrobustly to unseen sample sizes, dimensionalities, as well as non-linearities\nwith a remarkably low inference time.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}