{
  "id": "arxiv_2502.21112v1",
  "text": "IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n1\nOptimizing Large Language Models for ESG Activity Detection\nin Financial Texts\nMattia Birti1, Francesco Osborne2,3, and Andrea Maurino1\n1Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy\n2Department of Business and Law, University of Milano-Bicocca, Milan, Italy\n3KMi, The Open University, Milton Keynes, UK\nThe integration of Environmental, Social, and Governance (ESG) factors into corporate decision-making is a fundamental aspect\nof sustainable finance. However, ensuring that business practices align with evolving regulatory frameworks remains a persistent\nchallenge. AI-driven solutions for automatically assessing the alignment of sustainability reports and non-financial disclosures with\nspecific ESG activities could greatly support this process. Yet, this task remains complex due to the limitations of general-purpose\nLarge Language Models (LLMs) in domain-specific contexts and the scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to environmental activities. Furthermore, we demonstrate\nthat their performance can be significantly enhanced through fine-tuning on a combination of original and synthetically generated\ndata. To this end, we introduce ESG-Activities, a benchmark dataset containing 1,325 labeled text segments classified according to the\nEU ESG taxonomy. Our experimental results show that fine-tuning on ESG-Activities significantly enhances classification accuracy,\nwith open models such as Llama 7B and Gemma 7B outperforming large proprietary solutions in specific configurations. These\nfindings have important implications for financial analysts, policymakers, and AI researchers seeking to enhance ESG transparency\nand compliance through advanced natural language processing techniques.\nIndex Terms—Deep learning, Environmental management, Financial technology, Generative AI, Large Language Models, Machine\nlearning, Natural language processing, Sustainability, Text classification.\nI. INTRODUCTION\nI\nN recent years, driven by the widespread adoption of\nthe Sustainable Development Goals (SDGs), the Euro-\npean Union has introduced principles and regulations aimed\nat helping organizations integrate environmental, social, and\ngovernance (ESG) factors into their operations and strategic\ndecision-making. These initiatives encourage businesses and\ninvestors to assess and improve their environmental impact,\nfostering a more sustainable approach to economic activity [1].\nAs part of this effort, the ESG taxonomy1 has been established,\noffering a standardized framework to support sustainable prac-\ntices across industries [2]. This resource enables companies to\nevaluate their activities in alignment with its criteria and report\ntheir performance in non-financial disclosures and sustainabil-\nity reports.\nHowever, aligning business practices with regulatory frame-\nworks continues to be a challenge. Financial investors and\nother stakeholders often struggle to determine which aspects of\ncorporate social responsibility correspond to specific activities\nregulated in the ESG taxonomy, making it difficult to accu-\nrately interpret and effectively utilize these reports. Indeed, a\ncompany can be associated with a vast amount of text that,\nin theory, can be mapped to various environmental activities\ndescribed in the taxonomy. This text may include non-financial\ndisclosures, marketing materials, website descriptions, product\nand service descriptions, corporate sustainability reports, and\nManuscript received December xx, 20xx; revised xxx , xxxx. Corresponding\nauthor: F. Osborne (email: francesco.osborne@unimib.it).\n1https://finance.ec.europa.eu/sustainable-finance/tools-and-standards/\neu-taxonomy-sustainable-activities en\nmore. Thoroughly analyzing all this material is a complex and\ncostly endeavour.\nThe rise of social fintech technologies, which increas-\ningly support intelligent solutions for social impact invest-\ning, presents a promising opportunity to leverage advanced\ncomputational techniques for analyzing financial texts and ex-\ntracting sustainability-related insights. For instance, several AI\nmodels have been developed to classify financial documents\nbased on relevant SDGs [3], [4]. The rapid advancement of\nlarge language models (LLMs) in recent years has further\nimproved automated solutions for the analysis of financial\nand sustainability-related texts [5]–[9]. However, categorizing\ntext based on specific environmental activities, such as those\ndefined in the ESG taxonomy, remains a significant challenge.\nExamples of these activities include promoting low-carbon rail\ntransport, ensuring green port operations with zero direct CO2\nemissions, or expanding infrastructure for cycling and personal\nelectric mobility. This task is considerably more complex than\nlinking text to broad SDG categories, as it requires a fine-\ngrained understanding of the actions and activities described\nin a document. General-purpose LLMs struggle with this\nlevel of specificity. Overcoming these limitations, therefore,\nrequires fine-tuning such models on high-quality, domain-\nspecific datasets – an effort often hindered by the scarcity\nof training data in this domain.\nIn this paper, we explore the ability of current-generation\nLLMs to identify text related to environmental actions. Fur-\nthermore, we demonstrate how their performance can be\nsignificantly enhanced by fine-tuning them on a combination\nof original and synthetically generated data. Specifically, our\nstudy focuses on classifying textual segments extracted from\narXiv:2502.21112v1  [cs.AI]  28 Feb 2025\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n2\nNon-Financial Disclosures (NFDs) according to the activi-\nties defined in the ESG taxonomy. These activities include\nmeasures such as reducing emissions through energy-efficient\nproduction processes, transitioning to renewable energy, and\nimplementing water conservation strategies, such as wastewa-\nter treatment and reuse in manufacturing plants.\nTo fine-tune and evaluate a range of models on their task,\nwe introduce ESG-Activities, a novel benchmark consisting of\n1,325 texts classified according to ESG activities. Since we\nintended assess the capability of synthetic data in supporting\nAI systems in this domain, the training set of ESG-Activities\nincludes a mixture of manually curated data from human\nexperts and synthetic data generated by LLMs. In contrast,\nthe test set consists exclusively of items curated by human\nexperts to ensure a high-quality evaluation.\nThe evaluation demonstrates that fine-tuning the system\nwith a combination of manually crafted and synthetically\ngenerated data leads to significant improvements, not only\ncompared to zero-shot learning but also over models fine-tuned\nexclusively on the manually annotated dataset. Furthermore,\nwe show that relatively small open-source models, such as\nLlama 7B, can achieve excellent performance when fine-tuned\non ESG-Activities, even surpassing large proprietary models\nin certain configurations.\nIn summary, the contributions of this paper are as follows:\n• We analyze the performance of state-of-the-art LLMs on\nthe challenging task of associating textual descriptions\nwith specific environmental activities.\n• We introduce ESG-Activities2, a novel benchmark that\ncombines both original and synthetic data.\n• We demonstrate that augmenting a high-quality, manu-\nally curated dataset for fine-tuning with synthetic data\nenhances the performance of the resulting models on this\ntask.\n• We provide the complete codebase for our analysis to\nensure reproducibility3.\nThe remainder of this paper is structured as follows. Section\nII discusses the use case and provides a formal definition of\nthe task. Section III introduces the new benchmark. Section\nIV describes the methodology adopted in our analysis. Section\nV presents the evaluation and discusses key insights. Section\nVI reviews related work. Finally, Section VII concludes the\npaper and outlines future research directions.\nII. TASK DEFINITION AND USE CASE\nA. The role of ESG taxonomy for social fintech companies\nIn the European landscape, the analysis of social and\nenvironmental impact investments and the assessment of the\nsustainability of companies, including crowdfunding platforms\nor fintech, are becoming increasingly crucial. In 2021, the Eu-\nropean Commission defined a set of ambitious policies related\nto the so-called “European Green Deal” with the overarching\ngoal of making Europe climate-neutral by 2050. Among\n2ESG-Activities\n-\nhttps://github.com/Mattia-Brt/Fine tuning LLM/tree/\nmain/data\n3Codebase - https://github.com/Mattia-Brt/Fine tuning LLM\nother initiatives, the European Green Deal has established a\ntaxonomy that defines the activities companies must undertake\nto enhance their contributions to ESG aspects. This taxonomy\nis becoming a crucial benchmark for European investors,\nwho utilize it to assess the sustainability of their investment\ndecisions. Investors require a thorough understanding of the\nextent to which companies comply with the European ESG\ntaxonomy. This is essential for making informed investment\ndecisions that align with their values and contribute to sus-\ntainable development. However, the complexity of the ESG\ntaxonomy and the vast amount of financial documentation can\nmake it challenging for investors to independently assess a\ncompany’s compliance. As a result, there is an increasing need\nfor tools that can streamline this process by automatically\nidentifying and annotating relevant sections within financial\ndocuments. These tools represent sustainable applications of\nSocial Fintech, showcasing how AI can enhance corporate so-\ncial responsibility. By enabling investors to identify companies\ngenuinely committed to sustainability, these technologies help\ndirect capital toward initiatives aligned with SDGs, such as\nthose outlined in the European Green Deal.\nFurthermore, semi-automatic annotation tools can be seam-\nlessly integrated into Social Fintech platforms, including\ncrowdfunding platforms, reinforcing their dedication to sus-\ntainability and social impact. Automating ESG analysis with\nAI enhances transparency and promotes responsible invest-\nment opportunities, ultimately fostering a more sustainable and\nequitable financial system.\nB. Task definition\nIn this paper, we describe the process of developing an\nLLM tailored to verify if a text portion from NFD relates to\na particular item in the ESG taxonomy. The task is framed as\na binary classification problem, where the goal is to train the\nLLM to answer the question: Does c pertain to i?. Specifically,\nfor a text segment c and an item i – in this case, the description\nof an ESG activity – the formulated fine-tuned LLM function\nf(c, i) is expressed as:\nf(c, i) =\n(\n1\nif c pertains to i,\n0\notherwise.\nThe aim of the fine-tuned LLM is to learn a mapping function\nf that accurately assigns each pair (c, i) to the correct class.\nThis fine-tuning process is performed using a dataset of\ntriples (c, i, class), where c is a text chunk, i is an activity\ndescription in the ESG taxonomy, and class ∈{0, 1} indicates\nwhether c is related to i. The goal is to minimize the binary\ncross-entropy loss:\nL = −1\nN\nN\nX\nj=1\n[yj log ˆyj + (1 −yj) log(1 −ˆyj)] ,\nwhere N is the number of samples in the dataset, yj is the\ntrue label (class) for the j-th sample, and ˆyj = f(cj, ij) is the\npredicted probability that cj pertains to ij.\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n3\nThe fine-tuning process was performed using the Low-Rank\nAdaptation (LoRA) technique [10], which updates only a low-\nrank subspace of the model’s pre-trained weight matrices,\nreducing the number of trainable parameters while maintaining\nefficiency. Let W be a weight matrix from the pre-trained\nmodel. During fine-tuning, the adaptation introduces a low-\nrank update ∆W:\n∆W = AB,\nwhere A ∈Rm×r and B ∈Rr×n, r ≪min(m, n), ensuring\nefficiency in storage and computation.\nThe updated weight matrix becomes:\nW ′ = W + ∆W = W + AB.\nBy freezing the original weights W and only optimizing\nA and B, LoRA enables efficient fine-tuning with minimal\ncomputational overhead.\nIII. THE ESG-ACTIVITIES BENCHMARK\nTo the best of our knowledge, no existing dataset has been\nexplicitly designed to enhance an LLM’s ability to classify\ntext based on precise and granular environmental activities,\nsuch as those defined in the ESG taxonomy. To address this\ngap, we introduce ESG-Activities, a novel benchmark dataset.\nESG-Activities links textual segments from NFDs to specific\nactivities defined in the ESG taxonomy. The dataset was\nconstructed following the process outlined below.\nFirst, we selected the NFDs of four major companies within\nthe transportation industry. The selected companies were Fer-\nrovie dello Stato, the Italian State Railways; Autostrade per\nl’Italia, Italy’s largest company for motorway management\nand maintenance; Maersk, a well-known shipping and logistics\ncompany; and Mundys, a multinational company specializing\nin motorway and airport infrastructure. Next, we selected\n12 activities related to the transport industry from the ESG\ntaxonomy. These activities represent various actions that com-\npanies can take to promote sustainability in the transportation\nsector, such as developing zero-emission vessel infrastructure\nwith electric charging and hydrogen refuelling, implementing\nsmart mobility systems to enhance traffic efficiency, promoting\nlow-carbon rail transport with CO2-free or bimodal trains,\nexpanding infrastructure for walking, cycling, and personal\nelectric mobility, and ensuring green port operations with zero\ndirect CO2 emissions.\nSince the original descriptions were too long, we generated\nshorter versions using GPT-4 as a rephraser. We then indexed\nthe NFDs in a vector database and employed a Retrieval-\nAugmented Generation (RAG) [11] pipeline to retrieve the\nmost relevant text chunks for each activity description. This\nprocess yielded 265 candidate mappings between text seg-\nments and ESG actions.\nThe candidate mappings were evaluated by three domain\nexperts – professors and postdoctoral researchers with exper-\ntise in the transport industry. Each annotator independently\nassessed whether to confirm or reject each candidate mapping.\nA mapping was deemed valid if it received at least two positive\nvotes out of three, following a majority rule. The resulting\ndataset consisted of 265 entries, each containing a textual\ndescription, an activity description, and a binary flag indicating\na match. Among these, 78 text-activity pairs were classified\nas true matches. The dataset was then split into a training set\nof 212 instances and a test set of 53 instances.\nSince we aimed to verify the effectiveness of syntactic\ndata in this domain, we then expanded the training set with\nartificial data. Specifically, we used ChatGPT-4o to generate\nfive alternative formulations for each of the original 212\nsentences in the training set, ensuring they conveyed the\nsame meaning while using different wording. This process\nresulted in 1,060 additional sentences, which were labelled\nas ’synthetic data’ and incorporated into the training set. This\nsetup allows users of the benchmark to fine-tune their models\nusing only the original data (212 items) or a combination of\noriginal and syntactic data (1272 items).\nIV. METHODOLOGY\nIn this section, we first introduce ESGQuest, the prototype\nwe developed for analysing documents and mapping them\nto ESG activities using an LLM fine-tuned for this task.\nWe then provide an overview of the LLMs evaluated in this\nstudy and describe the experimental setup used to assess their\nperformance on the ESG-Activities benchmark.\nA. System Architecture\nThe study presented in this paper is part of a broader project\naimed at developing ESGQuest, a research prototype designed\nto support the annotation of NFD with ESG-related activities.\nThis system is built on a RAG pipeline. Given a document\nabout a company and a set of relevant NACE codes, ESGQuest\naims to identify text segments that align with ESG activities\nassociated with the specified NACE codes. The results are\npresented as an annotated PDF, which can be further refined\nby human users.4 The ESGQuest pipeline operates as follows.\nFirst, ESGQuest selects ESG activities based on the given\nNACE codes using a predefined mapping schema. NACE codes\n(Nomenclature statistique des Activit´es ´economiques dans la\nCommunaut´e Europ´eenne) are a European industry standard\nclassification system that groups organizations based on their\neconomic activities.5 This step is optional but generally ben-\neficial, as many ESG activities described in the taxonomy\nare relevant only to specific industrial sectors (e.g., transport,\nenergy). Including irrelevant activities in the analysis would\nnot only waste computational resources but also introduce\nunnecessary complexity and potential confusion.\nSecond, the input documents are divided into smaller chunks\nand stored in Pinecone6, a vector database optimized for\nsimilarity search. Pinecone facilitates the efficient retrieval of\nthe most semantically relevant segments for a given query by\n4A demo of this system is available at https://esgquest.datai.disco.unimib.it.\n5These codes are used in the EU Taxonomy to assess the environmental\nsustainability of economic activities. For more details, visit https://ec.europa.\neu/eurostat/web/nace\n6Pinecone - https://www.pinecone.io/\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n4\nleveraging high-dimensional vector embeddings. It is specif-\nically designed for approximate nearest neighbour search,\nensuring fast and scalable retrieval even for large datasets.\nThird, for each relevant ESG activity, the system queries\nthe vector database with the activity description to retrieve the\nmost pertinent chunks from the input documents.\nFinally, an LLM optimized for this purpose analyzed each\nchunk to assess whether it aligned with the description of the\naction. If a chunk is assigned to an action, the corresponding\ntext segment in the original PDF is annotated accordingly.\nB. Large Language Models\nIn this paper, we examine a range of LLMs, including sev-\neral open-source models, which offer a cost-effective solution\nand can be deployed locally to ensure the privacy of sensitive\ndocuments, as well as a large proprietary model (GPT-4o\nMini). Below, we provide a description of each LLM evaluated\nin this paper, including their sizes and key characteristics.\n1) Llama.\nLlama, introduced by Hugo Touvron et al. in 2023 [5], is\na family of decoder-only language models designed to deliver\nhigh performance and computational efficiency. The Llama\nmodels range from 1 to 90 billion parameters, making them\nsuitable for a wide variety of natural language processing\napplications. In this study, we evaluate three versions: Llama\n3B, Llama 2 7B, and Llama 3 8B, all available on official\nplatforms such as Hugging Face.\nLlama 3B7 is an open-source model with 3 billion parame-\nters. The model incorporates architectural enhancements such\nas pre-normalization and the SwiGLU activation function [12],\nwhich contribute to training stability and efficiency.\nLlama 2 7B8, represents an evolution of the first generation\nof Llama [13]. It was trained using additional data and\noptimized techniques to improve generalization. This model\nexcels in zero-shot and few-shot learning tasks and directly\ncompetes with larger models like GPT-3 and Chinchilla [14].\nLlama 3 8B9, is the latest iteration in the series. This\nmodel, featuring 8 billion parameters, was trained on a dataset\nexceeding 15 trillion tokens [15]. It incorporates significant\nadvancements such as odds ratio preference optimization [16],\nwhich optimizes performance and alignment.\nThe Llama series was trained on diverse datasets, compris-\ning publicly available data such as CommonCrawl, C4 [17],\nGitHub, Wikipedia, Books3 [18], ArXiv, and Stack Exchange.\nThese models have demonstrated excellent performance in\nbenchmarks such as NaturalQuestions [19] and TriviaQA [20],\noften surpassing larger models like Gopher (280B) [21] and\nChinchilla (70B) [14]. For instance, the Llama 33B model out-\nperformed both Gopher and Chinchilla in question-answering\ntasks on the NaturalQuestions benchmark, demonstrating that\nthe Llama architecture effectively balances size and perfor-\nmance.\n7https://huggingface.co/openlm-research/open Llama 3b\n8https://huggingface.co/meta-Llama/Llama-2-7b-hf\n9https://huggingface.co/meta-llama/Meta-Llama-3-8B\n2) Gemma.\nIntroduced by Google in 2024, Gemma [6] is a family\nof open-source language models derived from the Gemini\nseries [7], designed to achieve an optimal balance between\ncomputational efficiency and performance across various nat-\nural language processing tasks. The models are available in\nthree configurations – 2B, 7B, and 27B parameters – each\ntailored for specific applications. Gemma employs a decoder-\nonly architecture with advanced features such as multi-query\nattention to reduce memory overhead [22], Rotary Position\nEmbeddings (RoPE) [17] to improve positional representation,\nGeGLU activations for better convergence during training,\nand RMSNorm to enhance stability on large-scale datasets.\nThe models were trained on diverse publicly available data,\nincluding CommonCrawl, C4 [17], GitHub repositories, ArXiv\npapers, Wikipedia, and Books3 [18], ensuring broad linguistic\ncoverage and domain-specific expertise. The 2B model10 is op-\ntimized for on-device deployments and CPU-based inference,\nmaking it efficient for general-purpose applications. The 7B\nvariant11 is fine-tuned for complex tasks like advanced coding,\nscientific reasoning, and mathematical problem-solving, out-\nperforming comparable models such as Llama 7B [5] and often\nrivalling Mistral 7B [8]. The Recurrent Gemma 2B model12\nintroduces a recurrent mechanism that enhances performance\non sequential reasoning tasks and long-context processing,\nmaking it particularly effective for multi-turn dialogues and\nstructured data analysis [23]. Across multiple benchmarks,\nincluding coding, reasoning, and question answering, Gemma\nmodels have demonstrated competitive performance.\n3) Mistral.\nMistral, introduced by Albert Q. Jiang et al. in 2023 [8], is\na highly efficient decoder-only language model designed for\na wide array of natural language processing tasks, excelling\nin areas such as reasoning, mathematics, code generation,\nand commonsense reasoning. With 7.3 billion parameters,\nMistral was specifically engineered to achieve a balance be-\ntween computational efficiency and performance. The model\nincorporates several advanced techniques that make it highly\neffective for tasks that demand long-context processing and\nmemory optimization. One such technique is the sliding win-\ndow attention (SWA) [24], which enables the model to process\nlong sequences of input in overlapping chunks, allowing it to\nhandle inputs of arbitrary length with minimal computational\noverhead. In addition, Mistral utilizes grouped-query attention\n(GQA) [25], a method that optimizes inference speed by\nreducing memory usage during the attention computation\nphase.\nMistral’s ability to maintain performance despite having a\nsmaller parameter count than other models makes it a powerful\ntool for practical applications, especially where computational\nresources may be limited. It consistently outperforms larger\nmodels like Llama 2 13B [5], Llama 1 34B [26], and other\nestablished benchmarks across a variety of metrics. This is\n10https://huggingface.co/google/gemma-2b\n11https://huggingface.co/google/gemma-7b\n12https://huggingface.co/google/recurrentgemma-2b\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n5\nespecially evident in reasoning-intensive tasks, where Mistral\nexhibits superior generalization capabilities.\nThe 7B variant of Mistral13 has been fine-tuned to support\ndiverse use cases, including scientific research, where accurate\ninterpretation and generation of complex texts are critical. The\nmodel is also capable of achieving state-of-the-art results on\ntasks requiring a deep understanding of specialized knowl-\nedge, such as those found in academic papers or technical\ndocuments.\n4) Gpt-4o Mini.\nGPT-4o Mini is a decoder-only language model developed\nby OpenAI as a faster and more cost-effective alternative\nto GPT-4o. GPT-4o is a comprehensive multimodal model\ncapable of processing and generating text, images, and au-\ndio, designed for complex tasks requiring high computational\npower and precision. It features a 128K-token context window\nand supports up to 16K output tokens per request, with a\nknowledge cutoff in October 2023 [9]. Despite its smaller\nsize, GPT-4o Mini outperforms previous models, such as GPT-\n3.5 Turbo, in academic benchmarks, demonstrating superior\nperformance in textual intelligence and multimodal reason-\ning [27]. It retains the same context window and output token\nlimits as GPT-4o, ensuring robust performance across various\napplications. Additionally, GPT-4o Mini incorporates an in-\nstruction hierarchy method to enhance resistance to jailbreaks\nand prompt injections, improving security for deployment in\ndiverse settings [28].\nC. Experimental Setup\nIn order to study the advantages of fine-tuning LLMs for\nESG activity classification, we evaluated the models in two\ndifferent modes: zero-shot and fine-tuning. The fine-tuning\nwas performed on the two alternative training sets within ESG-\nactivities: the manually curated dataset of 212 instances and a\nlarger dataset of 1,272 instances, which combines original and\nsynthetic data. In the following sections, we provide a detailed\ndiscussion of both approaches.\n1) Zero-shot\nZero-shot learning is a method in which a LLM is prompted\nto perform a task based solely on provided instructions,\nwithout the need for task-specific examples. This approach\nleverages the extensive pretraining of LLMs, allowing them to\ngeneralize to new, previously unseen tasks. It is particularly\nvaluable because it eliminates the need for additional task-\nspecific training data, making it a cost-effective and scalable\nsolution across various applications.\nIn this study, we designed and iteratively refined a prompt\ntemplate for zero-shot classification. The template takes two\ninputs: 1) the text to be classified and 2) a description of an\nESG action. Based on these inputs, the prompt instructs the\nLLM to return a value of 1 if the text aligns with the ESG\naction and 0 otherwise.\n2) Fine-tuning\nFine-tuning is a supervised learning technique that involves\nadjusting the weights of a pre-trained LLM using a labelled\n13https://huggingface.co/mistralai/Mistral-7B-v0.1\ndataset. This process refines the model’s broad language\nunderstanding and knowledge acquired during its initial pre-\ntraining phase, enabling it to achieve superior performance on\nspecific tasks or within particular domains.\nIn this study, we fine-tuned eight LLMs for the proposed\ntask using the training and validation datasets from the ESG-\nAction benchmark. Specifically, each model was fine-tuned\ntwice: first using the manually curated training and validation\ndatasets, and then using augmented datasets that included\nsynthetic data. This approach enables a comparative analysis\nof the two model versions, allowing us to evaluate the impact\nof synthetic data on models of different sizes.\nDue to the limited size of our training dataset, we employed a\n10-fold cross-validation approach to mitigate the risk of over-\nfitting. This technique systematically partitions the dataset into\nten subsets, ensuring that the model is trained and evaluated\non different data portions in each iteration. By rotating the\nvalidation set across all folds, cross-validation reduces bias\nand variance, leading to a more reliable assessment of the\nmodel’s generalization capability and performance metrics.\nModels were then saved after each epoch, and validation was\nconducted at regular intervals, referred to as steps. Specifically,\nthe models were evaluated and saved every 10% of the total\noptimization steps. This strategy enables the selection of\nthe best-performing model based on validation metrics and\nmitigates issues like overfitting or underfitting.\nThe optimization process was handled using the AdamW opti-\nmizer [29], a variant of the Adam optimizer that incorporates\ndecoupled weight decay regularization. To optimize the fine-\ntuning process, we employed LoRA techniques from the\nPEFT library, as previously mentioned. This approach reduces\nthe number of trainable parameters while preserving model\nperformance. For instance, when fine-tuning Llama 3B, which\nhas over 3.3 billion parameters, LoRA required modifying only\n0.08% of the total parameter space. Similarly, for Llama 7B,\nonly 0.12% of its parameters were updated.\nThe only exception to this procedure was GPT-4o Mini,\nwhich we fine-tuned using Azure OpenAI14, a cloud-based\nservice provided by Microsoft that enables access to OpenAI’s\nlanguage models. We formatted the training dataset in the\nrequired JSON structure, uploaded it to Azure, and executed\nthe fine-tuning process with the default hyperparameters.\nV. EVALUATION\nIn this section, we present the results of our experiments on\nthe ESG-Activities benchmark. Specifically, we evaluated the\neight LLMs described in Section IV using three alternative\nconfigurations: 1) zero-shot learning, 2) fine-tuning on the\noriginal data from ESG-Activities, and 3) fine-tuning on a\ncombination of the original and synthetic data. All models\nwere tested on the test set of ESG-Activities.\nAs the task is framed as a binary classification problem, we\nused precision, recall, and F1 score as evaluation metrics.\n14Azure OpenAI - https://azure.microsoft.com/en-us/products/ai-services/\nopenai-service\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n6\nA. Zero-Shot Setting\nTable I reports the performance of the models in a zero-\nshot setting. GPT4o_mini yields the best result (72.7%),\nfollowed by GEMMA_7B (70.5%), and Llama3_8B (70.1%).\nGEMMA_2B and Llama_3B performed fairly well, exceeding\n68% F1. Conversely, RecurrentGEMMA_2B and MISTRAL\nshowed relatively poor performance in this task. This suggests\nthat the recurrent architecture of RecurrentGEMMA_2B may\nnot offer significant benefits in this specific task. Overall, there\nis considerable room for improvement.\nModel\nPrecision\nRecall\nF1-Score\nLlama_3B\n0.6565\n0.7358\n0.6816\nLlama2_7B\n0.7145\n0.5472\n0.5828\nLlama3_8B\n0.6845\n0.7358\n0.7008\nGEMMA_2B\n0.6696\n0.6792\n0.6742\nGEMMA_7B\n0.8361\n0.6792\n0.7050\nrecurrentGEMMA_2B\n0.7455\n0.4717\n0.4945\nMISTRAL\n0.5574\n0.3207\n0.3396\nGPT4o_mini\n0.7202\n0.7358\n0.7270\nTABLE I\nPERFORMANCE OF THE MODELS IN A ZERO-SHOT SETTING.\nB. Fine-tuning\nTable II presents the results of the models fine-tuned on the\nmanually curated dataset. The results demonstrate a significant\nimprovement across all models, confirming the efficacy of the\nESG-Action benchmark in optimizing performance for this\ntask.\nThe top-performing model is again GPT4o_mini, which\nattains an F1 score of 80.5%, followed by Llama_3B (78.9%)\nand GEMMA_2B (78.1%). It is interesting that the smaller\nmodel achieved the best performance after fine-tuning. This is\nlikely due to the fact that the manually curated training set is\nrelatively small. As a result, models tend to overfit quickly, and\nthis effect is even more pronounced in larger models, which\noverfit earlier and thus experience a drop in performance.\nModel\nPrecision\nRecall\nF1-Score\nLlama_3B\n0.7866\n0.7925\n0.7892\nLlama2_7B\n0.8361\n0.6792\n0.7050\nLlama3_8B\n0.7420\n0.7170\n0.7273\nGEMMA_2B\n0.7763\n0.7925\n0.7813\nGEMMA_7B\n0.8146\n0.6981\n0.7226\nrecurrentGEMMA_2B\n0.7763\n0.4340\n0.4396\nMISTRAL\n0.6701\n0.5472\n0.5838\nGPT4o_mini\n0.8015\n0.8113\n0.8050\nTABLE II\nPERFORMANCE OF THE FINE-TUNED MODELS (ORIGINAL DATA).\nTable III presents the performance of models fine-tuned\non the second training set of ESG-activities, which includes\na combination of original and synthetic data. This exper-\niment aimed to determine whether incorporating synthetic\ndata during fine-tuning provides a performance advantage.\nThe results clearly indicate that it does, with most models\nnow achieving F1 scores exceeding 80%. The best overall\nresult is achieved by Llama2_7B, with an F1 score of\n84.9%, followed by Llama_3B at 84.4% and GEMMA_7B\nModel\nPrecision\nRecall\nF1-Score\nLlama_3B\n0.8421\n0.8491\n0.8440\nLlama2_7B\n0.8491\n0.8491\n0.8491\nLlama3_8B\n0.8290\n0.7925\n0.8036\nGEMMA_2B\n0.8176\n0.8302\n0.8127\nGEMMA_7B\n0.8190\n0.8302\n0.8211\nrecurrentGEMMA_2B\n0.8470\n0.5283\n0.5452\nMISTRAL\n0.8032\n0.7736\n0.7840\nGPT4o_mini\n0.7684\n0.7925\n0.7711\nTABLE III\nPERFORMANCE OF THE FINE-TUNED MODELS (ORIGINAL AND SYNTHETIC\nDATA).\nat 82.1%. GPT4o_mini, which was fine-tuned using the\nOpenAI service, did not perform as well in this setting.\nIn conclusion, our experiments demonstrate that LLMs can\nachieve high performance in classifying text based on ESG\nactivities when fine-tuned on high-quality datasets. These\nfindings also provide further evidence supporting the effec-\ntiveness of synthetic data in specific domains. Notably, the\nmost comprehensive version of the ESG-Activities benchmark\ntraining set, which combines both manually annotated and syn-\nthetic data, further improves classification performance. For\nexample, Llama2_7B, the best-performing model, achieved\na remarkable 14.8% increase in F1 score, rising from 70.1%\nto 84.9% with the inclusion of synthetic data.\nC. Training and Inference Time\nTraining times varied significantly across models, reflecting\ndifferences in architecture, size, and the complexity of the\nfine-tuning\nprocesses.\nAnalyzing\nthese\nvariations\noffers\nvaluable insights into the computational demands of each\nmodel.\nTable IV provides the training and inference times for the\nvarious models. It was not possible to report the fine-tuning\ntime for GPT4o_mini, as this process was managed by the\nOpenAI service.\nModel\nInference Time\nTraining Time\nTraining Time\n(s)\nOriginal (s)\nOriginal+Synthetic (s)\nGemma_2B\n3.24\n115\n828\nGemma_7B\n2.18\n120\n976\nRecGemma_2B\n2.51\n130\n1011\nLlama3_8B\n3.74\n226\n1096\nLlama2_7B\n2.14\n132\n1046\nLlama_3B\n3.61\n181\n1534\nMistral_7B\n10.89\n703\n5888\nGPT4o_mini\n4.46\nNA\nNA\nTABLE IV\nTRAINING AND INFERENCE TIME FOR THE EIGHT MODELS.\nNotably, models from the Gemma family exhibit faster\ntraining times compared to the Llama models. Among the\nmodels analyzed, MISTRAL_7B stands out with a signif-\nicantly longer training time of about 96 minutes, nearly\nseven times that of GEMMA_2B. Regarding inference times,\nLlama2_7B is the fastest model, followed by Gemma_7B\nand RecurrentGemma_2B.\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n7\nD. Cost Analysis\nThe computational demands of training and inference pro-\ncesses can have a significant impact on the overall cost of a\nproject.\nModel\noriginal\norig+syntetic\n($)\n($)\nGPT4o_mini\n0.14\n0.86\nGemma_2B\n0.29\n2.07\nGemma_7B\n0.30\n2.44\nRecGemma_2B\n0.33\n2.52\nLlama2_7B\n0.33\n2.62\nLlama3_8B\n0.56\n2.74\nLlama_3B\n0.45\n3.84\nMistral_7B\n1.76\n14.72\nTABLE V\nFINE-TUNING COSTS\nTable V reports the costs associated with fine-tuning the\neight models on both the original dataset and the full\nversion, which includes synthetic data. All models, except\nGPT4o_mini, were trained on an NVIDIA A100 GPU. The\nhourly rental rate for an A100 machine was approximately\n$9 at the time of this study. Differently, the fine-tuning of\nGPT4o_mini was conducted using OpenAI’s standard API\noffering, which, at the time of writing, allowed fine-tuning at a\ncost of $3.00 per 1 million tokens. For these reasons, although\nGPT4o_mini is the largest model in terms of the number of\nparameters, it is the cheapest to train using OpenAI’s services.\nHowever, this training cost must be combined with the fees\nrequired to deploy and maintain the model on OpenAI’s\ninfrastructure, which is currently $1.62 an hour. Among the\nopen models, the most cost-effective to fine-tune with our\nconfiguration are Gemma_2B and Gemma_7B.\nVI. RELATED WORK\nThis section examines key advancements in the development\nand application of LLMs in the ESG domain. LLMs have\ndemonstrated remarkable capabilities in processing textual\ndata at scale, enabling the accurate extraction and synthesis\nof information from diverse sources, including research pa-\npers [30], patents [31], medical records [32], social media\nposts [33], news articles [34], technical documentation [35],\nlegal texts [36], and financial documents [37]–[39].\nRecent LLM families, including GPT [9], Llama [5], Mis-\ntral [8], and Gemma [6], have demonstrated outstanding\ncapabilities in analyzing and classifying ESG and financial\ndata. Their proficiency in processing vast amounts of un-\nstructured information positions them as powerful tools for\nenhancing ESG reporting, regulatory compliance, and data-\ndriven decision-making. However, the standard versions of\nthese models, when used with simple prompting, do not always\nachieve optimal performance. Consequently, many researchers\nfocus on developing domain-specific adaptations by fine-\ntuning open models on ESG and financial data. For instance,\nYang et al. [40] introduced FinGPT, an open-source LLM\ndesigned for the financial sector. Initially trained on a general-\npurpose corpus, FinGPT was then fine-tuned using domain-\nspecific datasets, including financial news, earnings reports,\nand real-time market data. This process enables the model\nto capture the nuanced complexities of financial language,\nimproving its performance in tasks such as market forecast-\ning, sentiment analysis, and news classification. As a result,\nFinGPT can be applied to various use cases, including robo-\nadvising, algorithmic trading, and low-code development.\nFinBERT [41] is a domain-specific adaptation of BERT [42]\ndesigned for financial text analysis. It is pre-trained on a\nvast corpus of financial documents, including regulatory fil-\nings, corporate announcements, and financial articles, before\nundergoing fine-tuning for specific tasks. This specialized\ntraining enables FinBERT to develop a deep understanding\nof financial terminology and contextual nuances. Notably,\nit has demonstrated exceptional performance in sentiment\nclassification of financial analyst reports. Similarly, ESG-\nBERT [43] is a specialized variant of BERT that has been pre-\ntrained on a large corpus of sustainability-related texts. This\nmodel has demonstrated strong performance in various natural\nlanguage processing tasks within the ESG domain, particularly\nin text classification and sentiment analysis [44]. We initially\nexplored adapting these models to the task analyzed in this\npaper; however, this approach proved unfeasible. These models\nare trained for specific classification tasks with predefined\nlabels, making it difficult to adapt them for matching a text\nwith an arbitrary activity description.\nSeveral approaches have been proposed for ESG sentiment\nclassification, which involves categorizing text as either pos-\nitive/opportunity or negative/risk from an ESG perspective.\nNotably, the ML-ESG-2 shared task [45], co-located with\nthe FinNLP workshop at IJCNLP-AACL 2023, has played a\nkey role in advancing methodologies in this field. To tackle\nthis challenge, researchers have leveraged both encoder-only\nand decoder-only transformer models, including fine-tuned\nversions of FinBERT [46] and LLMs such as Llama 2 and\nDolly [47]. Similar to previous approaches, we leverage and\nevaluate a variety of fine-tuned language models. However, the\ntask addressed in this paper is more fine-grained than merely\ndetecting a positive or negative sentiment.\nTo tackle the challenges of long contexts and data recency\nin LLMs, researchers have started incorporating the RAG\nparadigm [11], which improves response accuracy by dynami-\ncally retrieving relevant information. For instance, ESGReveal\n[47] is a recent method for extracting and analyzing ESG\ndata from corporate reports that utilizes an LLM augmented\nwith RAG. The system includes an ESG metadata module\nfor precise querying, a preprocessing module for database\nconstruction, and an LLM agent for extracting information.\nLike our approach, ESGReveal leverages an RAG architecture.\nHowever, its primary focus is on extracting quantitative data\nfrom documents, such as Scope 1 and Scope 2 emissions.\nIn contrast, our work is centred on identifying portions of\ndocuments that align with the activities described in the ESG\ntaxonomy. Furthermore, ESGReveal utilizes zero-shot learning\nLLMs with basic prompting, whereas we fine-tune LLMs to\noptimize their performance for our specific task.\nEfforts have also been made to develop a question-\nanswering system in the ESG domain using a pipeline based\non Knowledge Graph-based Retrieval Augmented Generation\n(KG-RAG) [48]. Unlike traditional RAG, which retrieves in-\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n8\nformation from textual sources, KG-RAG leverages knowledge\ngraphs [49], which are structured representations of infor-\nmation that capture relationships between entities in a graph\nformat. Similarly, Angioni et al. [50] employ a combination\nof linguistic pattern analysis and transformer models to extract\na knowledge graph from a corpus of news articles, aiming to\ncapture key trends related to ESG aspects. In this work, we\nfocus on the more specific task of classifying text based on\nESG activities. Consequently, we rely exclusively on action\ndescriptions from the ESG taxonomy rather than employing a\nmore complex knowledge representation.\nVII. CONCUSION\nIn this paper, we investigate the ability of current-generation\nLLMs to identify text related to ESG activities. Specifically,\nwe assess strategies for optimizing these models using high-\nquality data and explore the feasibility of leveraging a combi-\nnation of original and synthetically generated data. To support\nthis study, we introduce ESG-Activities, a novel benchmark\ncomprising 1,325 labelled text segments classified according\nto the EU ESG taxonomy.\nOur results demonstrate that fine-tuned models significantly\noutperform zero-shot approaches, confirming that domain\nadaptation plays a crucial role in enhancing ESG text clas-\nsification. Moreover, the inclusion of synthetic data proved to\nbe a valuable strategy, yielding substantial performance gains\nacross multiple model architectures. This finding suggests that\nsynthetic data augmentation can serve as an effective method\nto overcome data scarcity issues, particularly in specialized\ndomains such as ESG analysis.\nAnother key takeaway from our experiments is that rel-\natively small, open-source models can achieve competitive\nperformance levels when fine-tuned with high-quality, domain-\nspecific data. This insight has significant implications for\norganizations looking to deploy cost-effective AI solutions for\nESG analysis or those that need to run models locally due\nto privacy and data security concerns, which are particularly\ncommon in the financial domain.\nThis study opens several promising avenues for future\nresearch. First, we plan to explore alternative data genera-\ntion techniques, including human-in-the-loop approaches, to\nfurther refine synthetic data for ESG applications. Second,\nwhile our work focused on ESG-related sentence classification,\nfurther research is needed to evaluate the models’ perfor-\nmance on other NLP tasks, such as ESG sentiment analysis\nand risk assessment. Finally, future studies should examine\nhow these models can be effectively deployed in real-world\nfinancial decision-making processes, ensuring that AI-driven\nESG analysis aligns with regulatory requirements and industry\nstandards.\nACKNOWLEDGMENT\nThis work is supported by the Italian Ministry of University\nand Research (MUR) within the PRIN2022—ISALDI: Inter-\npretable Stock Analysis Leveraging Deep multImodal models\n(CUP: E53D23008150006).\nREFERENCES\n[1] M. Kostetckaia and M. Hametner, “How sustainable development goals\ninterlinkages influence european union countries’ progress towards the\n2030 agenda,” Sustainable Development, vol. 30, no. 5, pp. 916–926,\n2022.\n[2] M. Dumrose, S. Rink, and J. Eckert, “Disaggregating confusion? the\neu taxonomy and its relation to esg rating,” Finance research letters,\nvol. 48, p. 102928, 2022.\n[3] D. Guariso, O. A. Guerrero, and G. Casta˜neda, “Automatic sdg budget\ntagging: Building public financial management capacity through natural\nlanguage processing,” Data & Policy, vol. 5, p. e31, 2023.\n[4] A. Hajikhani and A. Suominen, “Mapping the sustainable development\ngoals (sdgs) in science, technology and innovation: application of\nmachine learning in sdg-oriented artefact detection,” Scientometrics, vol.\n127, no. 11, pp. 6661–6693, 2022.\n[5] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, S. Azhar et al.,\n“Llama: Open and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[6] P. Mesnard et al., “Gemma: Google’s open models for advanced\nlanguage tasks,” Google AI Blog, 2024. [Online]. Available: https:\n//ai.google.dev/gemma\n[7] G. Team, “Gemini: A family of highly capable multimodal models,”\n2024.\n[8] A. Q. Jiang et al., “Mistral: Efficient language models with advanced\narchitectures,”\narXiv\npreprint\narXiv:2310.09700,\n2023.\n[Online].\nAvailable: https://arxiv.org/abs/2310.09700\n[9] OpenAI, “Gpt-4o: Advancing large-scale multimodal ai,” OpenAI\nTechnical Report, 2023. [Online]. Available: https://openai.com/index/\ngpt-4o\n[10] N.\nHeidloff,\n“Efficient\nfine-tuning\nwith\npeft\nand\nlora,”\n2023,\naccessed: 2024-07-30. [Online]. Available: https://heidloff.net/article/\nefficient-fine-tuning-lora/\n[11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\n[12] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,\nA. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient\nfoundation\nlanguage\nmodels,”\n2023.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/2302.13971\n[13] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n2: Open foundation and fine-tuned chat models,” arXiv preprint\narXiv:2307.09288, 2023.\n[14] J. Hoffmann, S. Borgeaud, A. Mensch, et al., “An empirical study\nof training compute-optimal large language models,” arXiv preprint\narXiv:2203.15556, 2022.\n[15] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nA. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of\nmodels,” arXiv preprint arXiv:2407.21783, 2024.\n[16] J. Hong, N. Lee, and J. Thorne, “Orpo: Monolithic preference\noptimization without reference model,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2403.07691\n[17] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, “Exploring the Limits of Transfer Learning\nwith a Unified Text-to-Text Transformer,” arXiv, 2019.\n[18] S. Presser, “Books3 dataset: A collection of text for large language\nmodel training,” 2021. [Online]. Available: https://the-eye.eu/public/AI/\npile/Books3/\n[19] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\nquestions: A benchmark for question answering research,” Transactions\nof the Association for Computational Linguistics, vol. 7, pp. 453–466,\n2019.\n[20] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “Triviaqa: A large scale\ndistantly supervised challenge dataset for reading comprehension,” arXiv\npreprint arXiv:1705.03551, 2017.\n[21] J. Rae et al., “Gopher: A 280 billion parameter language model,”\narXiv preprint arXiv:2112.11446, 2022. [Online]. Available: https:\n//arxiv.org/abs/2112.11446\n[22] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak,\nL. Sifre, M. Rivi`ere, M. S. Kale, J. Love et al., “Gemma: Open\nmodels based on gemini research and technology,” arXiv preprint\narXiv:2403.08295, 2024.\n\n\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. X, NO. X, XX 20XX\n9\n[23] A. Botev, S. De, S. L. Smith, A. Fernando, G.-C. Muraru, R. Haroun,\nL. Berrada, R. Pascanu, P. G. Sessa, R. Dadashi et al., “Recurrentgemma:\nMoving past transformers for efficient open language models,” arXiv\npreprint arXiv:2404.07839, 2024.\n[24] B. Anderson et al., “Sliding window attention: Efficiently handling long\ncontexts,” arXiv preprint arXiv:2204.05160, 2022. [Online]. Available:\nhttps://arxiv.org/abs/2204.05160\n[25] W. Zhang et al., “Grouped-query attention for memory-efficient\ninference,” arXiv preprint arXiv:2303.12943, 2023. [Online]. Available:\nhttps://arxiv.org/abs/2303.12943\n[26] H. Touvron et al., “Llama 1: Scaling transformer-based models to 34\nbillion parameters,” arXiv preprint arXiv:2203.01953, 2022. [Online].\nAvailable: https://arxiv.org/abs/2203.01953\n[27] OpenAI, “Gpt-4o mini: Cost-efficient language modeling,” OpenAI\nTechnical Report, 2023. [Online]. Available: https://openai.com/index/\ngpt-4o-mini\n[28] ——, “Safety enhancements in gpt-4o models,” OpenAI Technical Re-\nport, 2023. [Online]. Available: https://openai.com/index/gpt-4o-safety\n[29] L. Guan, “Weight prediction boosts the convergence of adamw,” 2023.\n[Online]. Available: https://arxiv.org/abs/2302.00195\n[30] F. Bolanos, A. Salatino, F. Osborne, and E. Motta, “Artificial intelli-\ngence for literature reviews: Opportunities and challenges,” Artificial\nIntelligence Review, vol. 57, no. 10, p. 259, 2024.\n[31] C. W. Kosonocky, C. O. Wilke, E. M. Marcotte, and A. D. Ellington,\n“Mining patents with large language models elucidates the chemical\nfunction landscape,” Digital Discovery, vol. 3, no. 6, pp. 1150–1159,\n2024.\n[32] J. A. Omiye, H. Gui, S. J. Rezaei, J. Zou, and R. Daneshjou, “Large\nlanguage models in medicine: the potentials and pitfalls: a narrative\nreview,” Annals of internal medicine, vol. 177, no. 2, pp. 210–220, 2024.\n[33] K. Yang, T. Zhang, Z. Kuang, Q. Xie, J. Huang, and S. Ananiadou,\n“Mentallama: interpretable mental health analysis on social media with\nlarge language models,” in Proceedings of the ACM Web Conference\n2024, 2024, pp. 4489–4500.\n[34] E. Motta, F. Osborne, M. M. Pulici, A. Salatino, and I. Naja, “Cap-\nturing the viewpoint dynamics in the news domain,” in International\nConference on Knowledge Engineering and Knowledge Management.\nSpringer, 2024, pp. 18–34.\n[35] G. Cascini, A. Fantechi, and E. Spinicci, “Natural language processing\nof patents and technical documentation,” in International Workshop on\nDocument Analysis Systems.\nSpringer, 2004, pp. 508–520.\n[36] J. Savelka and K. D. Ashley, “The unreasonable effectiveness of large\nlanguage models in zero-shot semantic annotation of legal texts,” Fron-\ntiers in Artificial Intelligence, vol. 6, p. 1279794, 2023.\n[37] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\nP. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large\nlanguage model for finance,” arXiv preprint arXiv:2303.17564, 2023.\n[38] K. S. Phogat, C. Harsha, S. Dasaratha, S. Ramakrishna, and S. A.\nPuranam, “Zero-shot question answering over financial documents using\nlarge language models,” arXiv preprint arXiv:2311.14722, 2023.\n[39] Y. Li, S. Wang, H. Ding, and H. Chen, “Large language models in\nfinance: A survey,” in Proceedings of the fourth ACM international\nconference on AI in finance, 2023, pp. 374–382.\n[40] H. Yang, X.-Y. Liu, and C. D. Wang, “Fingpt: Open-source financial\nlarge language models,” Preprint at arXiv, 2023.\n[41] A. H. Huang, H. Wang, and Y. Yang, “Finbert: A large language\nmodel for extracting information from financial text,” Contemporary\nAccounting Research, vol. 40, no. 2, pp. 806–841, 2023.\n[42] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep\nbidirectional transformers for language understanding,” in Proceedings\nof naacL-HLT, vol. 1, no. 2.\nMinneapolis, Minnesota, 2019.\n[43] M. Mukherjee, “Esg-bert: Nlp meets sustainable investing,” Towards\nData Science Blog, 2020.\n[44] S. Pasch and D. Ehnes, “Nlp for responsible finance: Fine-tuning\ntransformer-based models for esg,” in 2022 IEEE International Con-\nference on Big Data (Big Data).\nIEEE, 2022, pp. 3532–3536.\n[45] N. Kannan and Y. Seki, “Textual evidence extraction for esg scores,” in\nProceedings of the Fifth Workshop on Financial Technology and Natural\nLanguage Processing and the Second Multimodal AI For Financial\nForecasting, 2023, pp. 45–54.\n[46] S. Mishra, “Esg impact type classification: Leveraging strategic prompt\nengineering and llm fine-tuning,” in Proceedings of the Sixth Workshop\non Financial Technology and Natural Language Processing, 2023, pp.\n72–78.\n[47] Y. Zou, M. Shi, Z. Chen, Z. Deng, Z. Lei, Z. Zeng, S. Yang,\nH. Tong, L. Xiao, and W. Zhou, “Esgreveal: An llm-based approach for\nextracting structured data from esg reports,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2312.17264\n[48] T. K. Gupta, T. Goel, I. Verma, L. Dey, and S. Bhardwaj, “Knowledge\ngraph aided llm based esg question-answering from news,” 2024.\n[49] C. Peng, F. Xia, M. Naseriparsa, and F. Osborne, “Knowledge graphs:\nOpportunities and challenges,” Artificial Intelligence Review, pp. 1–32,\n2023.\n[50] S. Angioni, S. Consoli, D. Dess´ı, F. Osborne, D. R. Recupero, and\nA. Salatino, “Exploring environmental, social, and governance (esg)\ndiscourse in news: An ai-powered investigation through knowledge\ngraph analysis,” IEEE Access, 2024.\nMattia Birti is a Data Scientist and Researcher\nat the University of Milano-Bicocca (Italy) with a\nbackground in Computer Science and Data Science.\nHis research focuses on fine-tuning large language\nmodels (LLMs) for ESG sentence evaluation in sus-\ntainability reports. He has contributed to developing\nGold Standard datasets and optimizing LLMs using\ntechniques such as LoRA and QLoRA, achieving\nhigh accuracy in ESG data extraction. He also has\nexperience in machine learning, deep learning, and\nbackend development with FastAPI and Docker.\nFrancesco Osborne is Senior Research Fellow at\nThe Open University (UK) and Assistant Professor\nat the University of Milano-Bicocca (Italy). His\nresearch interests cover artificial intelligence, infor-\nmation extraction, knowledge graphs, and natural\nlanguage processing. Application domains include\nscientific research, media analysis, sustainability,\nfinance, big data, tourism, and astronomy. He has\nauthored more than 150 peer-reviewed publications\nin leading journals and conferences, including Ar-\ntificial Intelligence Review, Journal of Big Data,\nNeurocomputing, Future Generation Computer Systems, and Technological\nForecasting and Social Change.\nAndrea Maurino is Full Professor at the University\nof Milano-Bicocca after an initial activity at the\nPolitecnico di Milano. His main research interests\nare in the field of data quality and data management,\nhe is the author of more than 100 publications\nin international peer review journals, conferences,\nand proceedings including VLDB, WWW, Semantic\nWeb Journal, ESWC, and ISWC. He was the project\ncoordinator for the COMOSODE FP7 project related\nto the development of a methodology and a tool for\nopen data publication. The project was considered\nexcellent by the EU. Currently he is the director of the master degree in Data\nScience at University of Milano-Bicocca\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21112v1.pdf",
    "total_pages": 9,
    "title": "Optimizing Large Language Models for ESG Activity Detection in Financial Texts",
    "authors": [
      "Mattia Birti",
      "Francesco Osborne",
      "Andrea Maurino"
    ],
    "abstract": "The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}