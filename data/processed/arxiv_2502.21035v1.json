{
  "id": "arxiv_2502.21035v1",
  "text": "S4ConvD: Adaptive Scaling and Frequency Adjustment for\nEnergy-Efficient Sensor Networks in Smart Buildings\nMELANIE SCHALLER and BODO ROSENHAHN, Institute for Information Processing (tnt), Leibniz\nUniversity Hannover, Germany and L3S Research Center, Germany\nLinear Layer\nS4D Block\nTime-series Input: \nmeter reading, air temperature, cloud \ncoverage, dew temperature\nLinear Layer\nOutput:\npredicted energy consumption \nat the next time step\nS4D-Conv Block\nS4D Conv Kernel\nKernel Value K(t)\nFig. 1. Processing pipeline: Meter-readings and temperature inputs are encoded linearly. The S4ConvD layer’s\nkernel visualization shows: x-Axis: Temporal development. y-Axis: State-space components. z-Axis: Kernel\nvalues. Concludes with a Decoder predicting energy consumption (best viewed in color and zoomed in.)\nPredicting energy consumption in smart buildings is challenging due to dependencies in sensor data and the\nvariability of environmental conditions. We introduce S4ConvD, a novel convolutional variant of Deep State\nSpace Models (Deep-SSMs), that minimizes reliance on extensive preprocessing steps. S4ConvD is designed to\noptimize runtime in resource-constrained environments. By implementing adaptive scaling and frequency\nadjustments, this model shows to capture complex temporal patterns in building energy dynamics. Experiments\non the ASHRAE Great Energy Predictor III dataset reveal that S4ConvD outperforms current benchmarks.\nAdditionally, S4ConvD benefits from significant improvements in GPU runtime through the use of Block\nTiling optimization techniques. Thus, S4ConvD has the potential for practical deployment in real-time energy\nmodeling. Furthermore, the complete codebase and dataset are accessible on GitHub, fostering open-source\ncontributions and facilitating further research. Our method also promotes resource-efficient model execution,\nenhancing both energy forecasting and the potential integration of renewable energy sources into smart grid\nsystems.\nAuthors’ Contact Information: Melanie Schaller, schaller@tnt.uni-hannover.de; Bodo Rosenhahn, rosenhahn@tnt.uni-\nhannover.de, Institute for Information Processing (tnt), Leibniz University Hannover, Hannover, Lower Saxony, Germany\nand L3S Research Center, Hannover, Lower Saxony, Germany.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 1557-735X/2018/8-ART111\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\narXiv:2502.21035v1  [cs.LG]  28 Feb 2025\n\n\n111:2\nSchaller et al.\nCCS Concepts: • Computing methodologies →Spectral methods.\nAdditional Key Words and Phrases: Machine Learning, State Space Models, Enhancing energy efficiency,\nSensor Networks, urban infrastructure, Smart buildings\nACM Reference Format:\nMelanie Schaller and Bodo Rosenhahn. 2018. S4ConvD: Adaptive Scaling and Frequency Adjustment for\nEnergy-Efficient Sensor Networks in Smart Buildings. J. ACM 37, 4, Article 111 (August 2018), 17 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nThe field of building energy consumption prediction with sensor networks [11, 27] has made\nsignificant advancements over the past decades with the improvement of data-driven approaches [1,\n30, 33] and smart metering infrastructure [47]. These have enabled the collection of vast amounts\nof data and the generation of valuable insights through machine learning [10, 39]. Initially, research\nin the 1990s utilized statistical models and machine learning techniques [24, 31] such as artificial\nneural networks (ANN) [38] and support vector machines (SVM) [22], which have later evolved into\nmore complex ensemble methods that reduce errors through model diversity. Despite numerous\nstudies, comparing different prediction methods is challenging due to their customization for\nspecific contexts [24]. While numerous building energy prediction methods have been devised over\nthe past thirty years [2], there remains no widespread agreement on the most effective methods for\ndifferent types of buildings. Moreover, many of the developed techniques are not readily accessible\nto the broader research community. It has been observed, that a majority with 33 out of 42 of\nprediction studies using measured data implemented and tested the models on a single building [31].\nTo address this issue, the ASHRAE Great Energy Predictor III (GEPIII) Competition [24, 31, 32],\nheld in 2019, sought to enhance building energy consumption forecasting using machine learning\nmodels for different types of buildings. This competition series, which began in 1993, has served\nas a cornerstone for crowdsourced benchmarking of time-series data in the building industry\nand is therefore still used. The competition aimed to identify the best methods for hourly energy\nprediction in commercial buildings and disseminate the accumulated modeling knowledge to the\nwider academic and practitioner communities [24].\nIn recent years, integrating physical process representations into neural networks has enhanced\nthe modeling of dynamic systems by capturing temporal dependencies in real-world data [36, 37].\nState Space Models (SSMs) have emerged as a powerful alternative to Transformer architectures [16,\n17, 19, 40, 44], offering efficient handling of long-range dependencies while maintaining lower\ncomputational complexity. As first Deep SSM, the Structured State Space Models (S4) [17] and its\nderivative, S4D [19] have been introduced. The S4 framework introduced the handling of sequential\ndata by utilizing the structured state space representation’s using HiPPO (High-order Polynomial\nProjection Operator) matrices’ inherent ability to capture long-range dependencies [17]. Building\nupon S4, S4D further refines this approach by introducing diagonal matrices that enhance the\nmodel’s numerical robustness [19]. This foundational work in S4 and S4D sets the basis for our\nproposed S4ConvD model, a novel extension of S4D for energy consumption prediction. Unlike the\nstandard S4D convolution with its Cauchy Kernel approach, S4ConvD makes use of a convolutional\nkernel, to integrate adaptive state-space modeling and thus to dynamically adjust to changing\nenergy patterns, enhancing both accuracy and runtime.\nThe main contributions of this research are:\n(1) Adaptive State-Space Convolution: S4ConvD extends the S4D framework by introducing\na dynamically parameterized state matrix and an adaptive input transformation.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:3\n(2) Efficient Frequency Sensitivity: Through optimized handling of frequency components,\nS4ConvD selectively emphasizes relevant temporal patterns, improving performance over\nstatic convolutional methods without excessive computational overhead.\n(3) Benchmark Validation: Evaluations on the ASHRAE Great Energy Predictor III dataset\nshow that S4ConvD outperforms competing models, demonstrating superior generalization\nacross diverse building types.\n(4) CUDA Memory Optimization: We demonstrate that the CUDA optimization technique\nBlock Tiling, specifically tailored for the S4ConvD methodology on modern GPU architectures,\ncan improve runtime by 36 %.\n(5) Reproducibility and Open Science: To encourage further research and transparency, we\nprovide the full code and dataset on GitHub1.\n2\nRelated Work\nWe first list up the observations of the Great Energy Predictor III dataset challenge [16, 17, 19, 40, 44],\nthat were made by the winning teams. Then we summarize the methods used by the three winning\nteams of the challenge as benchmarking results. As last point we introduce the related work on\ndeep SSMs. One of the key differences between the top-performing approaches and approaches\nwith worse performance was the used preprocessing methods to filter the data before modeling and\nthe corrections they applied after prediction and before submission e.g for removing anomalous\nbehavior from the dataset as well as various methods to apply weightings and the creation of\ncomplex ensembling frameworks [24].\nThe first-place implemented comprehensive data preprocessing to remove anomalies, impute\nmissing values, correct time zones and employed feature engineering to extract features, which\nincluded raw data, categorical interactions, temporal attributes, various weather features, and target\nencoding. An ensemble of CatBoost [8], LightGBM [28], and Multi-Layer Perceptrons (MLP) [14]\nwas used. The final predictions were obtained by combining individual model outputs using a\ngeneralized weighted mean approach [24].\nThe second-place approach involved manually removing outliers through visual inspection and\nfiltering of each building’s data. For feature selection, simple statistical and temporal features\nfrom weather and building metadata were computed, without relying on complex lag features. XG-\nBoost [6], LightGBM [28], CatBoost [8] and Feed-forward Neural Networks (FFNN) [41], specifically\nused for the electrical meter [47], were used as ensemble [24].\nThe third-place incorporated a log transformation on the target variable. The feature engineering\ninvolved weather-related features like heat, windchill, and lagged weather features, along with\ntemporal data and building metadata. Models including CatBoost [8], neural networks [38] and\nLightGBM [28] were trained. The final predictions were achieved by ensembling [7] these models\nusing a weighted average method, with weights derived from publicly available datasets [24].\nIn building energy prediction, issues of generalizability and scalability remain significant chal-\nlenges [31]. Traditional machine learning models often struggle to adapt across various building\ntypes and contexts. This necessitates exploring deep learning approaches, which offer the potential\nto capture intricate patterns and temporal behaviors. Additionally, integrating data-driven models\nwith physics-based approaches can provide a more holistic understanding of energy consumption\ndynamics [31]. Motivated by these challenges and opportunities, the development of S4ConvD\naims to bridge these gaps.\nWhile Transformer-based architectures have achieved remarkable success in sequence model-\ning [43], their quadratic complexity [26] limits scalability. Recent advancements in State Space\n1https://github.com/MilanShao/S4ConvD\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n111:4\nSchaller et al.\nModels (SSMs) [16, 17, 19, 40, 44] provide an efficient alternative by utilizing structured state\ntransitions to capture temporal dynamics with lower computational overhead. SSMs originally\nderive from control theory [20]. In control engineering and system identification, state-space repre-\nsentation models a physical system through input, output, and state variables linked by first-order\ndifferential or difference equations. State variables evolve over time based on their current values\nand externally applied inputs. Output variables depend on the state variables, and potentially the\ninput variables [4, 21]. However, existing SSM approaches often rely on fixed parameterization,\nlimiting adaptability to rapidly changing data distributions. S4ConvD addresses this gap by intro-\nducing an adaptive state-space convolution, dynamically parameterized state matrices, enabling\nenhanced frequency sensitivity. Our results demonstrate that these refinements lead to superior\nperformance in energy consumption forecasting, particularly in urban-scale datasets such as the\nASHRAE Great Energy Predictor III dataset.\n3\nDataset\nFig. 2. Example of an anomaly in the dataset.\nDue to limitations in scalability observed in\nmany current approaches, which often cater\nto specific or limited datasets [31], the Great\nEnergy Predictor III dataset [24, 31, 32] is used.\nThis dataset is compiled from approximately\n61,910,200 energy measurements acquired from\ndiverse building types across 16 geographical\nlocations worldwide. ASHRAE also hosted the\nGreat Energy Predictor III (GEPIII) machine\nlearning competition on the Kaggle platform\n2019. This dataset captures hourly energy con-\nsumption metrics for 2,380 energy meters situ-\nated within 1,448 buildings. It contains various\ntypes of anomalies, such as abrupt changes in meter readings or outlier temperature values, which\ncould be due to sensor malfunctions, data entry errors, or unusual operational days. See, for instance,\nFig. 2, which depicts an example of such measurement anomalies where there is an unexpected\nspike in energy consumption not aligned with typical patterns and external conditions.\nThese hourly measurements come from different building types. The t-SNE (t-Distributed Sto-\nchastic Neighbor Embedding) distribution [45] in Fig. 3 visualizes high-dimensional building data\nby reducing its dimensionality to two dimensions while preserving the local structure of the data.\nEach point in the plot represents a building, and its color indicates its primary use category. The\nlegend provides a mapping between colors and categories, including lodging/residential, enter-\ntainment/public assembly, office, public services, education, parking, food sales and service, retail,\nwarehouse/storage, other, healthcare, utility, technology/science, manufacturing/industrial, and\nreligious worship. The distribution of points suggests that these building categories don’t form\ndense clusters. The lack of dense clustering for certain categories could suggest variations in\nbuilding design and operational efficiency which are not strictly bound by building type similarity.\nTo support analysis, coincident weather data was provided for each site. The dataset underwent\nminimal cleaning and processing to reflect real-world conditions accurately. It covers four different\ntypes of energy consumption: electricity, chilled water, steam, and hot water. The modeling process\ninvolved examining historical usage patterns alongside corresponding weather data.\nFig. 4 contains multiple time-series, each representing the mean meter readings of different\nbuildings over time. The x-axis in all plots corresponds to the time period from March 2016 to\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:5\nJuly 2016, while the y-axis represents the mean meter reading. Each plot is labeled with a specific\nbuilding_id, indicating the building for which the data is displayed.\nIn each graph, two different aggregations are shown: the blue lines represent the readings\nrecorded on an hourly basis, capturing short-term fluctuations, whereas the orange lines depict\ndaily averages, providing a smoothed representation of the overall trend. Most buildings exhibit\nan increasing trend in energy consumption over time, though the degree of variability differs.\nSome buildings, such as those with building_id: 60, display sudden spikes, suggesting anomalies or\npossible data recording issues. In contrast, buildings such as building_id: 76 show high fluctuations,\nbut with a clear increasing trend. The daily aggregation follows the overall shape of the hourly\ndata while reducing noise, making it easier to observe long-term trends.\nFig. 3. t-SNE (t-Distributed Stochastic Neighbor Embedding) plot of distributions along building types.\n3.1\nDatasplit\nThe dataset was segmented into three disjunct separate sets. The dataset is divided into a training\nset consisting of 11,637,272 rows, a validation set comprising 3,415,636 rows, and a test set con-\ntaining 5,108,845 rows. This should prevent from data leakage like it is caused in cross validation\nsettings [35].\n4\nEvaluation Metric\nTo evaluate the submissions in the ASHRAE Great Energy Predictor III competition [24, 31], the Root\nMean Squared Logarithmic Error (RMSLE) [23] was used as the primary metric. RMSLE was chosen\ndue to its capability to mitigate the disproportionate influence of meters with larger consumption\nvalues [15], thereby offering a balanced scoring approach compared to the conventional Root Mean\nSquare Error (RMSE) [5]. This choice was particularly relevant since significant variations in meter\nreadings could skew results if not addressed properly. Given the nonprofit nature of the competition,\nthe team opted for this established metric over a custom-designed one.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n111:6\nSchaller et al.\nFig. 4. Energy meter-readings for different exemplary buildings with mean values per hour and per day sorted\nby building-ID.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:7\nThe RMSLE is defined by the equation [23]:\nRMSLE =\nv\nt\n1\n𝑛\n𝑛\n∑︁\n𝑖=1\n\u0000log(pred𝑖+ 1) −log(actual𝑖+ 1)\u00012\n(1)\nwhere pred𝑖represents the predicted value, actual𝑖the actual value, and 𝑛is the number of\nobservations [23]. This formulation ensures that relative errors are emphasized, making prediction\nerrors for low values just as significant as those for high values. As a result, it provides a more\nbalanced evaluation, particularly in cases where the data spans multiple orders of magnitude.\n4.1\nInput Definition\nOur input data is derived from the ASHRAE Great Energy Predictor III dataset [24]. For each\nbuilding 𝑏𝑖with building index 𝑖, the input at each time step 𝑡𝑗at time 𝑗is defined by the feature\nvector:\nx(𝑖)\n𝑡𝑗=\nh\n𝐸(𝑖)\n𝑡𝑗,𝐶(𝑖)\n𝑡𝑗,𝑆(𝑖)\n𝑡𝑗, 𝐻(𝑖)\n𝑡𝑗,𝑇(𝑖)\n𝑎,𝑡𝑗,𝐶𝐶(𝑖)\n𝑡𝑗,𝑇(𝑖)\n𝑑,𝑡𝑗,𝜙(𝑡𝑗)\ni𝑇\n(2)\nIn this vector, 𝐸(𝑖)\n𝑡𝑗, 𝐶(𝑖)\n𝑡𝑗, 𝑆(𝑖)\n𝑡𝑗, and 𝐻(𝑖)\n𝑡𝑗\nrepresent the meter readings for electricity, chilled water,\nsteam, and hot water, respectively. Weather features include 𝑇(𝑖)\n𝑎,𝑡𝑗(air temperature), 𝐶𝐶(𝑖)\n𝑡𝑗(cloud\ncoverage), and 𝑇(𝑖)\n𝑑,𝑡𝑗(dew temperature). Additionally, 𝜙(𝑡𝑗) captures timestamp-derived features\nsuch as the hour of the day, day of the week, and holiday indicators.\nThe comprehensive feature vector x(𝑖)\n𝑡𝑗encapsulates all relevant data needed for our model to\ncapture both consumption patterns and influencing environmental factors.\nFig. 5. Heatmap of Input Correlations (same order on the x- and y-axis, thus x-axis not labeled due to\nrepetition).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n111:8\nSchaller et al.\nThe heatmap depicted in Fig. 5 provides a visual representation of the correlations among the\ninput features used for building energy consumption prediction in the ASHRAE Great Energy\nPredictor III dataset [24]. Through this heatmap [48], the strength and direction of linear rela-\ntionships between pairs of variables is discerned, indicated by correlation coefficients ranging\nfrom -1 to 1. A coefficient near 1 denotes a strong positive correlation, where an increase in one\nvariable is associated with an increase in another, whereas a coefficient near -1 suggests a strong\nnegative correlation, indicating an inverse relationship [18]. It uses a color gradient to represent the\ncorrelation coefficients between variables, where red indicates a strong positive correlation close to\n1.0, meaning that the two variables tend to increase or decrease together. Blue represents a negative\ncorrelation close to -1.0, meaning that as one variable increases, the other tends to decrease. White\nor light colors indicate little to no correlation close to 0.0, suggesting no strong linear relationship\nbetween the variables. The diagonal elements are all equal to 1.0, as they represent the correlation\nof each feature with itself. Some notable observations from the heatmap include that air tempera-\nture and east-west temperature show a strong positive correlation of 0.72, which is expected as\nthese two temperature-based features are closely related. Wind direction and wind speed have a\nmoderate positive correlation of 0.43, indicating some relationship between wind movement and\nspeed. Atmospheric pressure at sea level and air temperature show a negative correlation of -0.3,\nsuggesting that as air temperature increases, atmospheric pressure decreases. Other features such as\n𝑠𝑞𝑢𝑎𝑟𝑒𝑓𝑒𝑒𝑡and 𝑦𝑒𝑎𝑟𝑏𝑢𝑖𝑙𝑡have weak correlations with most variables, meaning their relationships\nare less significant, just to explain some correlations.\n5\nS4ConvD Method\nOur proposed method builds upon the existing S4D framework [19] by introducing a novel method,\nS4ConvD, which reformulates the convolution operation within the state-space model to improve\nperformance and computational efficiency (for SSMs in general see next section). While conven-\ntional CNNs [34] or Transformers [43] struggle with long context windows [26], SSMs retain past\ninformation efficiently through their internal state update structures [4, 21]. S4D Conv combines\nthe S4D framework [19] with convolutional mechanisms [34] to handle long time-series data. The\nmatrix structure of SSMs is further utilized to enable parallel and spectral processing. In contrast\nto S4D, the kernel generation is facilitated by the SSM matrix structure [19] and subsequently\napplied as a convolution on the input time-series signal via Fast Fourier Transform (FFT) [9]. The\nparameters log𝐴and 𝐴𝑖𝑚determine the behavior of system dynamics and influence how quickly\nor slowly states are updated. By integrating with convolutions, the model focus is further directed\ntoward relevant frequency domains. While classical SSMs [4, 21] require recursive updates, which\ncan become numerically unstable and lead to vanishing or exploding gradients, S4 [17] introduced\nthe convolutional representation of SSMs for inference. In S4D the same Cauchy Kernel is used [19].\nFor the proposed S4ConvD, we instead use a convolutional kernel [34].\n5.1\nFoundation in S4D\nState Space Models (SSMs) provide a systematic and mathematical framework to model the dynamics\nof physical systems by capturing the intrinsic relationships between input, output, and internal\nstate variables. The block diagram representation of SSMs in Fig. 6 displays the interaction and\nflow of signals within the system’s structure. The primary components include input nodes u(𝑡),\nstate blocks x(𝑡), output nodes y(𝑡), transfer function elements represented by the matrices A, B,\nC, and D, as well as summation nodes, an integrator 1\n𝑠, and feedback loops.\nInput nodes represent the external inputs applied to the system, which are essential for driving\nthe system dynamics. These inputs influence the state variables, which are depicted in state blocks as\ncentral elements responsible for holding the current condition of the system, where x(0) represents\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:9\nTable 1. Comparison between S4D and S4D Conv\nProperty\nS4D (Structured State Space for Se-\nquences) [19]\nS4D\nConv\n(Convolutional\nVari-\nant)=ours\nCore Principle\nSolves the recursive form of state-space\nequations directly in the Fourier domain\nfor long sequences.\nPerforms convolution in the frequency\ndomain, combining SSMs with CNNs\nfor efficient processing.\nComputation\nExplicitly computes the state transition\nmatrix 𝐴and output matrix 𝐶via dis-\ncrete approximation.\nConducts discrete convolution over the\nentire sequence with FFT, similar to a\nCNN.\nMemory Requirement\nCan have high memory demands if not\nimplemented efficiently.\nReduces memory requirements through\nFFT-based convolution and its possibilty\nto use CUDA optimization.\nthe initial state [13, 29, 46]. The evolution of the state is determined by the contributions of Bu(𝑡)\nand Ax(𝑡), as indicated by the left summation node in the diagram. The result of this summation\nis then processed through the integration block 1\n𝑠, which represents the system’s memory and\naccumulates state changes over time.\nD\nB\nC\nA\n1/s\nu(t)\nx(t)\ny(t)\nx(0)\nx(t)\nFig. 6. Block diagram representation of SSMs.\nThe output is derived from the internal state\nthrough a transformation governed by Cx(𝑡), which\nis computed before reaching the right summation\nnode. This node also considers the direct contribu-\ntion of Du(𝑡), ensuring that both state-dependent\nand input-dependent components contribute to the\nfinal output y(𝑡).\nA key feature of the model is the feedback loop,\nwhich connects the state x(𝑡) back to the summation\nnode through the transformation by A. This feed-\nback mechanism highlights how the current state\ninfluences its own rate of change, a crucial aspect\nof capturing dynamic system behavior. By combining these elements, the block diagram pro-\nvides a clear visualization of how state-space equations operate within a system, emphasizing the\ncontinuous interaction between input, state, and output variables.\nState blocks are intricately linked to transfer functions, which mathematically define how system\ninputs transform into state changes over time (¤x(𝑡) = Ax(𝑡) + Bu(𝑡)). These transformations\ntypically involve linear operations such as multiplications by matrices, representing the system’s\ntime-invariant characteristics. Output nodes follow from the state blocks and are responsible for\nproducing the observable outputs of the system. The output values depend not only on the state\nvariables but may also be directly influenced by the inputs (y(𝑡) = Cx(𝑡) + Du(𝑡)). This dual\ndependency is integral to the logic of SSMs, where the output is a function of the current state and\nthe input, encapsulated by the output matrix. Feedback loops in the block diagram symbolize the\ncontrol actions that adjust the input based on the output, forming a closed-loop system [13, 29, 46].\nThe S4D model [19] is a state-space approach commonly represented by the following equa-\ntions [17, 19]:\nx(𝑖)\n𝑡𝑗+1 = Ax(𝑖)\n𝑡𝑗+ Bu(𝑖)\n𝑡𝑗,\n(3)\ny(𝑖)\n𝑡𝑗= Cx(𝑖)\n𝑡𝑗,\n(4)\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n111:10\nSchaller et al.\nwhere x(𝑖)\n𝑡𝑗∈R𝑛𝑥is the state vector at time step 𝑡𝑗for building 𝑏𝑖, u(𝑖)\n𝑡𝑗∈R𝑛𝑢represents the input\nvector, and y(𝑖)\n𝑡𝑗∈R𝑛𝑦is the output vector. The matrices A ∈R𝑛𝑥×𝑛𝑥, B ∈R𝑛𝑥×𝑛𝑢, and C ∈R𝑛𝑦×𝑛𝑥\ndefine the system dynamics, where A governs state transitions, B captures input influences, and C\nmaps states to outputs. To improve computational efficiency, A is chosen to be diagonal, simplifying\nmatrix operations and accelerating inference [19].\n5.2\nNovelty of the S4ConvD Kernel\nIn the original S4 model, the computation of the convolution kernel posed significant computational\nchallenges, especially for general state matrices 𝐴. The S4 model introduced a complex algorithm\nfor Diagonal Plus Low Rank (DPLR) state matrices [17].\nFor diagonal state matrices 𝐴, however, the computation becomes numerical more stable [19].\nThe convolution kernel K is described by:\n𝐾ℓ=\n𝑁−1\n∑︁\n𝑛=0\n𝐶𝑛𝐴ℓ\n𝑛𝐵𝑛=⇒K = (B ⊙C) · VL(𝐴)\n(5)\nwhere ⊙denotes the Hadamard product, · represents matrix multiplication, and VL(𝐴) is the\nVandermonde matrix defined by:\nVL(𝐴)𝑛,ℓ= 𝐴ℓ\n𝑛\n(6)\nUnpacking this, K can be expressed using the Vandermonde matrix-vector multiplication:\nK =\n\u0002𝐵0𝐶0\n· · ·\n𝐵𝑁−1𝐶𝑁−1\n\u0003\n\n1\n𝐴0\n𝐴2\n0\n· · ·\n𝐴𝐿−1\n0\n1\n𝐴1\n𝐴2\n1\n· · ·\n𝐴𝐿−1\n1\n...\n...\n...\n...\n...\n1\n𝐴𝑁−1\n𝐴2\n𝑁−1\n· · ·\n𝐴𝐿−1\n𝑁−1\n\nThe approach involves materializing the Vandermonde matrix VL(𝐴) and performing matrix\nmultiplication, which requires O(𝑁𝐿) time and space.\nHowever, Vandermonde matrices are well-studied, and their multiplication can be theoretically\ncomputed in O(𝑁+ 𝐿) operations and O(𝑁+ 𝐿) space. Notably, Vandermonde matrices have close\nties with Cauchy matrices, forming the computational core of S4’s DPLR algorithm with similar\ncomplexity characteristics.\nFor our novel S4ConvD method, the convolution operation is formalized differently to better\nintegrate rapid variations in the building energy datasets. This is particularly achieved through the\nuse of adaptive scaling and frequency adjustment that aligns with dynamic input patterns.\nTo effectively capture rapid variations in building energy datasets, the convolution operation in\nthe S4ConvD method is structured to incorporate adaptive scaling and frequency adjustment. This\nallows the model to dynamically respond to changes in input patterns.\nThe convolution kernel in the S4ConvD framework is defined as:\nKS4ConvD(𝑡) = 𝐶× 𝜎(𝑒𝑡𝐴adaptive · 𝐵adaptive)\nHere, the components are detailed as follows: 𝐴adaptive: A dynamically parameterized state matrix\nthat adjusts during training to reflect real-time changes in the input data’s temporal dynamics.\n𝐵adaptive is an input matrix similarly parameterized to ensure that the model can incorporate\nvariations in the input data, adapting its influence on the state transformations. 𝜎(·) is a selective\nnon-linear transformation applied to the product 𝑒𝑡𝐴adaptive · 𝐵adaptive. This function enhances the\nkernel’s sensitivity to changes, differing from the constant linear scaling prevalent in traditional\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:11\nS4D. The non-linear transformation 𝜎(𝑥) could represent a Sigmoid function or another suitable\nnon-linear function, such as:\n𝜎(𝑥) =\n1\n1 + 𝑒−𝑥\nThis adjustment amplifies pertinent signals while suppressing noise, ensuring the kernel can\ndynamically reflect and adapt to new data patterns. Here, the 𝑠𝑡𝑎𝑡𝑒−𝑑𝑖𝑚is 64, the 𝑚𝑒𝑎𝑠𝑢𝑟𝑒𝑚𝑒𝑛𝑡−\n𝑑𝑖𝑚is 128, the 𝑖𝑛𝑝𝑢𝑡−𝑑𝑖𝑚is 4, the 𝑜𝑢𝑡𝑝𝑢𝑡−𝑑𝑖𝑚is 1, 𝑑𝑟𝑜𝑝𝑜𝑢𝑡has been set to 0.01, 𝑏𝑎𝑡𝑐ℎ−𝑠𝑖𝑧𝑒\nhas been set to 16, the 𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔−𝑟𝑎𝑡𝑒of the Stochastic Gradient Descent(SGD) has been set to\n0.001, the 𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚for the SGD-optimizer has been set to 0.9, the 𝑙𝑜𝑔−𝑖𝑛𝑡𝑒𝑟𝑣𝑎𝑙has been set to\n200 and the 𝑛𝑢𝑚−𝑒𝑝𝑜𝑐ℎ𝑠has been set to 100, equally for all models in the benchmarking. More\ndetails are available in the provided code on Github.\n6\nExperimental Results\nIn the following section, we present an analysis of the experimental results, demonstrating the ca-\npabilities and performance of our developed method. We begin with an ablation study that explores\nthe impact of different kernel functions on the system’s runtime and accuracy. Following this, we\nevaluate the memory footprint of our approach, highlighting its resource efficiency compared to\nexisting methodologies. Furthermore, we conduct a robustness validation to assess the resilience\nand reliability of the proposed solution. Finally, we present benchmarking results that compare\nour method against state-of-the-art techniques on the same dataset. Through these experimental\nvalidations, we substantiate the efficacy of our proposed method in addressing contemporary\nchallenges in energy consumption prediction throughout different building types.\n6.1\nAblation study with different kernels\nIn our ablation study, we explored the performance impact of deploying different convolutional\nkernels within the sequence modeling framework for energy consumption prediction. Our primary\nfocus, the S4ConvD model, incorporates convolutional adjustments to improve sequence data\ninterpretation. We compared our approach against the original S4D approach, which utilizes a\nCauchy kernel renowned for its stability and capacity to smoothly model long-range dependencies.\nTo ensure a fair comparison all hyperparameters have been set on the same value like described in\nsection 5.2.\nTable 2. Test RMSLE Comparison between S4ConvD and S4D with Cauchy Kernel\nModell\nTest RMSLE (1% of dataset)\nS4D Cauchy\n4.6702\nS4ConvD (ours)\n4.6676↓\n6.2\nMemory Evaluation\nIn our comparative memory analysis between S4D and S4ConvD over 5 epochs in Fig. 7 (a),\nwe identified a reduction in process memory usage when utilizing the S4ConvD method. This\nefficiency was consistently observed during both the training and inference phases. By integrating\nmemory tracking capabilities through Weights and Biases [25], we were able to monitor memory\nallocation and usage dynamically. The streamlined architecture of S4ConvD, which leverages\nFFT-based convolutions, contributes to this reduction by minimizing the storage demands typically\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n111:12\nSchaller et al.\nassociated with state-space computations. As can be observed from the figure, the process memory\navailable in MB is lower for S4ConvD, which means that the RAM usage is lower. The observed\npattern, where process memory usage increases at the start of each epoch and gradually decreases\ntowards the end, can be explained by the dynamic nature of memory allocation during model\ntraining. At the beginning of each epoch, the computational demands increase as the model loads\nnew batches of data and associated parameters into memory. This initialization phase requires\nsubstantial memory resources to accommodate the data structures involved in processing new\nbatches, gradient accumulation, and other temporary computations. As the epoch progresses,\nmemory optimization mechanisms such as memory recycling and garbage collection come into\nplay, freeing up resources by deallocating memory associated with completed computations and\nintermediate variables no longer in use. This resource management strategy leads to a gradual\ndecline in memory usage as the epoch advances.\n(a) Process Memory Available (MB) for S4D with\nCauchy Kernel and S4ConvD. Time in minutes on\nthe x-axis and Process Memory Available in MB on\nthe y-axis.\n(b) Disk Utilization (GB) for S4D with Cauchy Kernel\nand S4ConvD. Time in minutes on the x-axis and\nDisk Utilization in GB on the y-axis.\nFig. 7. Comparison of Process Memory and Disk Utilization between S4D and S4ConvD.\nContinuing from the analysis of memory usage, our evaluation extends to the comparison of\ndisk utilization between the S4D and S4ConvD models as illustrated in Fig. 7 (b). The S4ConvD\nmodel demonstrates an improvement in disk utilization efficiency. This is largely attributable\nto its FFT-based convolutional strategy, which minimizes the frequency and volume of disk I/O\noperations. During both training and inference, the reduced disk access requirements of S4ConvD\nlessen the model’s dependency on persistent storage, thereby decreasing the disk utilization. Lower\ndisk utilization also indicates that S4ConvD processes data more efficiently in-memory, relying less\non disk storage for intermediary computational tasks. Consequently, this translates to a decrease in\nlatency that can arise from disk read/write operations, providing a smoother and faster execution\nflow. The observed temporary drop to zero in disk utilization at the 30-minute mark for the S4ConvD\nmodel can be attributed to its reliance on in-memory computations and the high-speed processing\ncapabilities inherently tied to Fast Fourier Transforms (FFT). These architectures are designed\nto maximize memory utilization, thereby minimizing dependency on slower disk operations. By\nprioritizing in-memory computations, the model reduces latency and enhances processing efficiency,\nwhich may lead to temporary pauses in disk I/O operations during intensive data processing phases.\nAs a result, the system can manage data more effectively without frequent disk access, explaining\nthe brief drop during this period.\n6.3\nRobustness Validation\nIn order to ensure the robustness of the results and validate the consistency of our findings, we\nconducted repetitive experiments with both the S4D Cauchy Kernel and the S4ConvD approach.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:13\nEach model was trained and evaluated on subsets of the dataset, specifically using 1% and 5% of\nthe data. These experiments were repeated ten times for each kernel. The results demonstrated a\nremarkable consistency, as no significant deviations were observed in the performance metrics for\neither kernel across the repeated runs (e.g. the RMSLE remained 4.6676 for S4ConvD for all 10 runs\nwith 1% of the data, see Table 2). This demonstrates, that the observed performance is stable and\nnot an artifact of random sampling variability.\n6.4\nBenchmarking Results\nBased on the defined data split, we re-implemented the top three submissions and some of the\nbest other used models from the challenge on the ASHRAE Great Energy Predictor III competition\nleaderboard to serve as a benchmark against our proposed S4ConvD.\nTable 3. Top submissions in the ASHRAE Great Energy Predictor III Competition\nModel\nRMSLE\nS4DConv (ours)\n0.395\nS4D\n0.425\nDecision Tree\n0.607\nEnsemble LGB-CatBoost-XGBoost\n1.232\nLight GBM\n1.292\nLinear Regression\n1.381\nLasso Estimator\n1.381\nSGD Regression\n1.381\nRidge\n1.384\nElastic Net\n1.473\nSVR\n2.298\nAs Table 3 shows, our proposed S4ConvD method uses an adaptive scaling and frequency\nadjustment within its convolution kernel. This allows it to respond dynamically to variations in\ninput data, hence capturing real-time changes in energy consumption patterns more effectively than\nstatic methods. Although our method did not incorporate extensive data preprocessing techniques,\nsuch as robust feature engineering or anomalous data filtering, it nonetheless achieved superior\nperformance in comparison to the ASHRAE Great Energy Predictor III competition submissions.\nBy employing an adaptive convolution kernel that dynamically adjusts to rapidly changing input\nsequences, our approach inherently captures the essential patterns and dependencies within the\ndataset without requiring labor-intensive preprocessing steps. It also shows to be slightly better than\nthe original S4D approach with its Cauchy kernel. The Decision Tree model had an RMSLE of 0.607,\nwhich was better than the winning submissions in the challenge. The ensemble model combining\nLightGBM, CatBoost, and XGBoost, despite incorporating a variety of predictors performed notably\nlower than our S4ConvD, with an RMSLE of 1.232. LightGBM and other linear regression-based\napproaches like Lasso, Ridge, and Elastic Net exhibited limitations in inherently capturing nonlinear\ndependencies even with feature engineering. Their RMSLE scores, ranging from 1.292 to 1.473,\nreach middle scores. On the extreme, SVR’s high RMSLE of 2.298 indicates challenges in scalability\nand sensitivity to variations in the data set that the S4ConvD efficiently managed.\n7\nCUDA Optimization\nIn CUDA optimization, several key techniques are employed to enhance performance by improving\nmemory access patterns and computational efficiency [12]. Block Tiling [3, 42] further refines\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n111:14\nSchaller et al.\nefficiency by organizing data into chunks, or tiles, that are processed by thread blocks. Such a\nscheme optimizes data parallelism and minimizes cache misses. In order to achieve a high usage, we\nset the batch-size to 8192 for all experiments on this GPU. For our experiments, we used a NVIDIA\nTesla P100 GPU, with the specifications displayed in Table 4.\nTable 4. Specifications of the NVIDIA Tesla P100 GPU\nMetric\nValue\nName\nNVIDIA Tesla P100\nCompute Capability\n6.0\nMax Threads per Block\n1024\nMax Threads per Multiprocessor\n2048\nThreads per Warp\n32\nWarp Allocation Granularity\n4\nMax Registers per Block\n65536\nMax Registers per Multiprocessor\n65536\nRegister Allocation Unit Size\n64\nRegister Allocation Granularity\nWarp\nTotal Global Memory\n16280 MB\nMax Shared Memory per Block\n48 KB\nCUDA Runtime Shared Memory Overhead per Block\n512 B\nShared Memory per Multiprocessor\n65536 B\nMultiprocessor Count\n56\nMax Warps per Multiprocessor\n64\nIn CUDA, a warp consists of 32 threads, so the tile size should be a multiple of 32 to ensure full\nwarp utilization. Each block can have up to 1024 threads, aligning with potential block configurations\nfor efficiency. The maximum shared memory per block is 48 KB, which limits the tile configuration\nto not exceed this capacity. For a block using 8192 bytes of shared memory and 512 bytes for\nruntime overhead, the total is 8704 bytes per block, allowing up to 7 blocks in shared memory.\nWith 1024 threads per block and a max of 2048 threads per multiprocessor, two blocks can fit\nin shared memory. Each thread uses 37 registers, totaling 38848 registers per block. Given 65536\nregisters per multiprocessor, this is the limiting factor as only one block fits per shared memory. This\nresults in approximately 50% occupancy, with 32 out of 64 possible active warps per multiprocessor.\nConsequently, a tile size of 32 is set for Block Tiling on the NVIDIA Tesla P100 GPU to optimize\nutilization.\nTable 5. Performance Impact of Various CUDA Optimizations on S4ConvD method\nUsed Technique\nTime/epoch (s)\nTimered.(%)\nGPU Memory(%)\nUsage Improv.(%)\nNaive Kernel\n01:05\n0\n9.8 GiB (63%)\n0\nBlock Tiling (32)\n00:42↓\n35.38↑\n9,8 GiB (99%)\n36↑\nThis CUDA optimization process exemplifies the importance of algorithmic structure in deriving\nbenefits from the GPU’s architecture. Using S4ConvD allowed an easier transition and application\nof CUDA optimization techniques. In contrast, a Cauchy Kernels, due to their matrix structure\nand specialized computational needs, typically cause scattered memory accesses. This scattering\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:15\ncomplicates efforts to implement efficient tiling strategies. In Table 5 we show, that optimizing the\nkernel for higher arithmetic intensity (fewer memory operations, more computation) and ensuring\nefficient shared memory usage can maximize performance within these resource constraints about\n36 %.\n8\nConclusion\nIn this paper, we introduced S4ConvD, an convolutional variant of Deep State Space Models, to\naddress the challenges inherent in predicting energy consumption in smart buildings. By utilizing\nadaptive scaling and frequency adjustments, S4ConvD captures changing patterns in the multivari-\nate time-series data of the smart meter sensor network. Our evaluations on the ASHRAE Great\nEnergy Predictor III dataset demonstrate that S4ConvD surpasses existing benchmarks in predictive\naccuracy, showcasing its capability to generalize across diverse urban energy infrastructures.\nThe efficiency of S4ConvD is highlighted by its reduced memory and disk utilization, making it\nan option for deployment in resource-constrained real-world environments. These efficiencies are\nprimarily due to the use of FFT-based convolutions, which streamlined computational processes\nwithout sacrificing model performance in our task.\nFurthermore, the integration of CUDA optimization techniques, such as Global Memory Coalesc-\ning and Block Tiling, further amplifies the performance of S4ConvD. By leveraging the architectural\nstrengths of the NVIDIA Tesla P100 GPU, we achieved a significant reduction of 35.38% in com-\nputational time per epoch, alongside maximal memory utilization at 99%. Its design principles\npromote resource-efficient model execution, enhancing both energy forecasting and the potential\nintegration of renewable energy sources into smart grid systems.\nBy minimizing reliance on extensive preprocessing and emphasizing dynamic adaptability,\nS4ConvD presents a scalable solution for energy-efficient sensor networks. In future work we will\nexplore the integration of S4ConvD into broader smart city frameworks, further validating its\nimpact on energy management systems. Our work also demonstrates significant synergies between\nmachine learning and energy modeling, particularly in enhancing prediction accuracy and response\ntimes in smart grids. In future research we will also focus on the integration of S4ConvD in real-time\nwith IoT platforms, enabling seamless data flow and analysis for instant decision-making in urban\ninfrastructures.\nAcknowledgments\nWe thank the German Federal Ministry of Education and Research (BMBF) under the grant number\n13GW0586F for funding research about state space models for time-series forecasting.\nReferences\n[1] Kadir Amasyali and Nora M El-Gohary. 2018. A review of data-driven building energy consumption prediction studies.\nRenewable and Sustainable Energy Reviews 81 (2018), 1192–1205.\n[2] Sheraz Aslam, Herodotos Herodotou, Syed Muhammad Mohsin, Nadeem Javaid, Nouman Ashraf, and Shahzad Aslam.\n2021. A survey on deep learning methods for power load and renewable energy forecasting in smart microgrids.\nRenewable and Sustainable Energy Reviews 144 (2021), 110992. doi:10.1016/j.rser.2021.110992\n[3] Burak Bastem. 2019. Tiling-based programming model for GPU clusters targeting structured grids. Ph. D. Dissertation.\nKoç University.\n[4] W.L. Brogan. 1974. Modern Control Theory. Quantum Publishers. https://books.google.de/books?id=Vu9QAAAAMAAJ\n[5] Tianfeng Chai, Roland R Draxler, et al. 2014. Root mean square error (RMSE) or mean absolute error (MAE). Geoscientific\nmodel development discussions 7, 1 (2014), 1525–1534.\n[6] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, Rory Mitchell,\nIgnacio Cano, Tianyi Zhou, et al. 2015. Xgboost: extreme gradient boosting. R package version 0.4-2 1, 4 (2015), 1–4.\n[7] Thomas G Dietterich. 2000. Ensemble methods in machine learning. In International workshop on multiple classifier\nsystems. Springer, 1–15.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n111:16\nSchaller et al.\n[8] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost: gradient boosting with categorical features\nsupport. arXiv preprint arXiv:1810.11363 (2018).\n[9] Pierre Duhamel and Martin Vetterli. 1990. Fast Fourier transforms: a tutorial review and a state of the art. Signal\nprocessing 19, 4 (1990), 259–299.\n[10] Mahmoud Elsisi, Karar Mahmoud, Matti Lehtonen, and Mohamed MF Darwish. 2021. Reliable industry 4.0 based on\nmachine learning and IOT for analyzing, monitoring, and securing smart meters. Sensors 21, 2 (2021), 487.\n[11] Varick L Erickson, Miguel Á Carreira-Perpiñán, and Alberto E Cerpa. 2014. Occupancy modeling and prediction for\nbuilding energy management. ACM Transactions on Sensor Networks (TOSN) 10, 3 (2014), 1–28.\n[12] Naznin Fauzia, Louis-Noël Pouchet, and P Sadayappan. 2015. Characterizing and enhancing global memory data\ncoalescing on GPUs. In 2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO). IEEE,\n12–22.\n[13] Bernard Friedland. 2012. Control system design: an introduction to state-space methods. Courier Corporation.\n[14] Matt W Gardner and Stephen R Dorling. 1998. Artificial neural networks (the multilayer perceptron)—a review of\napplications in the atmospheric sciences. Atmospheric environment 32, 14-15 (1998), 2627–2636.\n[15] EJ Gilroy, RM Hirsch, and TA Cohn. 1990. Mean square error of regression-based constituent transport estimates.\nWater Resources Research 26, 9 (1990), 2069–2077.\n[16] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\narXiv:2312.00752 (2023).\n[17] Albert Gu, Karan Goel, and Christopher Ré. 2021. Efficiently modeling long sequences with structured state spaces.\narXiv preprint arXiv:2111.00396 (2021).\n[18] Zuguang Gu. 2022. Complex heatmap visualization. Imeta 1, 3 (2022), e43.\n[19] Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal State Spaces are as Effective as Structured State Spaces.\narXiv:2203.14343 [cs.LG]\n[20] James D Hamilton. 1994. State-space models. Handbook of econometrics 4 (1994), 3039–3080.\n[21] Andrew C. Harvey. 1990. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge University Press.\n[22] M.A. Hearst, S.T. Dumais, E. Osuna, J. Platt, and B. Scholkopf. 1998. Support vector machines. IEEE Intelligent Systems\nand their Applications 13, 4 (1998), 18–28. doi:10.1109/5254.708428\n[23] Timothy O Hodson. 2022. Root mean square error (RMSE) or mean absolute error (MAE): When to use them or not.\nGeoscientific Model Development Discussions 2022 (2022), 1–10.\n[24] Addison Howard, Chris Balbach, Clayton Miller, Jeff Haberl, Krishnan Gowri, and Sohier Dane. 2019. ASHRAE - Great\nEnergy Predictor III. https://kaggle.com/competitions/ashrae-energy-prediction. Kaggle.\n[25] Glenn Jocher, Alex Stoken, Jirka Borovec, Liu Changyu, Adam Hogan, Ayush Chaurasia, Laurentiu Diaconu, Francisco\nIngham, Adrien Colmagro, Hu Ye, et al. 2021. ultralytics/yolov5: v4. 0-nn. SiLU () activations, Weights & Biases logging,\nPyTorch Hub integration. Zenodo (2021).\n[26] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast\nautoregressive transformers with linear attention. In International conference on machine learning. PMLR, 5156–5165.\n[27] Aqeel H Kazmi, Michael J O’grady, Declan T Delaney, Antonio G Ruzzelli, and Gregory MP O’hare. 2014. A review of\nwireless-sensor-network-enabled building energy management systems. ACM Transactions on Sensor Networks (TOSN)\n10, 4 (2014), 1–43.\n[28] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm:\nA highly efficient gradient boosting decision tree. Advances in neural information processing systems 30 (2017).\n[29] Gregory Levitin. 2007. Block diagram method for analyzing multi-state systems with uncovered failures. Reliability\nEngineering & System Safety 92, 6 (2007), 727–734.\n[30] Chengdong Li, Zixiang Ding, Dongbin Zhao, Jianqiang Yi, and Guiqing Zhang. 2017. Building energy consumption\nprediction: An extreme deep learning approach. Energies 10, 10 (2017), 1525.\n[31] Clayton Miller. 2019. More Buildings Make More Generalizable Models—Benchmarking Prediction Methods on Open\nElectrical Meter Data. Machine Learning and Knowledge Extraction 1, 3 (2019), 974–993. doi:10.3390/make1030056\n[32] Clayton Miller, Liu Hao, and Chun Fu. 2022. Gradient boosting machines and careful pre-processing work best: Ashrae\ngreat energy predictor iii lessons learned. arXiv preprint arXiv:2202.02898 (2022).\n[33] Razak Olu-Ajayi, Hafiz Alaka, Ismail Sulaimon, Funlade Sunmola, and Saheed Ajayi. 2022. Building energy consumption\nprediction for residential buildings using deep learning and other machine learning techniques. Journal of Building\nEngineering 45 (2022), 103406.\n[34] Keiron O’shea and Ryan Nash. 2015. An introduction to convolutional neural networks. arXiv preprint arXiv:1511.08458\n(2015).\n[35] Melanie Schaller, Mathis Kruse, Antonio Ortega, Marius Lindauer, and Bodo Rosenhahn. 2025. AutoML for Multi-Class\nAnomaly Compensation of Sensor Drift. arXiv:2502.19180 [cs.LG] https://arxiv.org/abs/2502.19180\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\nS4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings\n111:17\n[36] Melanie Schaller, Daniel Schlör, and Andreas Hotho. 2024. Modeconv: A novel convolution for distinguishing anomalous\nand normal structural behavior. arXiv preprint arXiv:2407.00140 (2024).\n[37] Melanie Schaller, Michael Steininger, Andrzej Dulny, Daniel Schlör, and Andreas Hotho. 2023. Liquor-HGNN: A\nheterogeneous graph neural network for leakage detection in water distribution networks.. In LWDA. 454–469.\n[38] Jürgen Schmidhuber. 2014.\nDeep Learning in Neural Networks: An Overview.\nCoRR abs/1404.7828 (2014).\narXiv:1404.7828 http://arxiv.org/abs/1404.7828\n[39] Joseph Siryani, Bereket Tanju, and Timothy J Eveleigh. 2017. A machine learning decision-support system improves\nthe internet of things’ smart meter operations. IEEE Internet of Things Journal 4, 4 (2017), 1056–1066.\n[40] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. 2023. Simplified State Space Layers for Sequence\nModeling. arXiv:2208.04933 [cs.LG]\n[41] Daniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal. 1997. Introduction to multi-layer feed-forward neural networks.\nChemometrics and intelligent laboratory systems 39, 1 (1997), 43–62.\n[42] Ben Van Werkhoven, Jason Maassen, and Frank J Seinstra. 2011. Optimizing convolution operations in cuda with\nadaptive tiling. In A4MMC’11: Proc. Workshop on Applications for Multi and Many Core Processors.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. 2023. Attention Is All You Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[44] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. 2024. Graph-mamba: Towards long-range graph sequence modeling\nwith selective state spaces. arXiv preprint arXiv:2402.00789 (2024).\n[45] Martin Wattenberg, Fernanda Viégas, and Ian Johnson. 2016. How to use t-SNE effectively. Distill 1, 10 (2016), e2.\n[46] Robert L Williams, Douglas A Lawrence, et al. 2007. Linear state-space control systems. John Wiley & Sons.\n[47] Baran Yildiz, Jose I Bilbao, Jonathon Dore, and Alistair B Sproul. 2017. Recent advances in the analysis of residential\nelectricity consumption and applications of smart meter data. Applied Energy 208 (2017), 402–427.\n[48] Shilin Zhao, Yan Guo, Quanhu Sheng, and Yu Shyr. 2014. Advanced heat map and clustering analysis using heatmap3.\nBioMed research international 2014, 1 (2014), 986048.\nReceived 28 February 2025; revised 12 March 2025; accepted 5 June 2025\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21035v1.pdf",
    "total_pages": 17,
    "title": "S4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient Sensor Networks in Smart Buildings",
    "authors": [
      "Melanie Schaller",
      "Bodo Rosenhahn"
    ],
    "abstract": "Predicting energy consumption in smart buildings is challenging due to\ndependencies in sensor data and the variability of environmental conditions. We\nintroduce S4ConvD, a novel convolutional variant of Deep State Space Models\n(Deep-SSMs), that minimizes reliance on extensive preprocessing steps. S4ConvD\nis designed to optimize runtime in resource-constrained environments. By\nimplementing adaptive scaling and frequency adjustments, this model shows to\ncapture complex temporal patterns in building energy dynamics. Experiments on\nthe ASHRAE Great Energy Predictor III dataset reveal that S4ConvD outperforms\ncurrent benchmarks. Additionally, S4ConvD benefits from significant\nimprovements in GPU runtime through the use of Block Tiling optimization\ntechniques. Thus, S4ConvD has the potential for practical deployment in\nreal-time energy modeling. Furthermore, the complete codebase and dataset are\naccessible on GitHub, fostering open-source contributions and facilitating\nfurther research. Our method also promotes resource-efficient model execution,\nenhancing both energy forecasting and the potential integration of renewable\nenergy sources into smart grid systems.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}