{
  "id": "arxiv_2502.21187v1",
  "text": "SYN-LUNGS: Towards Simulating Lung Nodules\nwith Anatomy-Informed Digital Twins for AI\nTraining\nFakrul Islam Tushar, Lavsen Dahal, Cindy McCabe, Fong Chi Ho, Paul Segars,\nEhsan Abadi, Kyle J. Lafata1, Ehsan Samei , Joseph Y. Lo\nCenter for Virtual Imaging trials, Dept. of Radiology, Duke University Medical Center\nDept. of Electrical and Computer Engineering, Pratt School of Engineering, Duke\nUniversity\nAbstract. AI models for lung cancer screening struggle with data scarcity,\nlimiting generalizability and clinical applicability. Generative models for\nimage synthesis are constrained by training data variability. We introduce\nSYN-LUNGS, a framework for generating high-quality 3D CT images\nwith detailed annotations. SYN-LUNGS integrates XCAT3 phantoms for\ndigital twin generation, X-Lesions for nodule simulation (varying size, lo-\ncation, appearance), and DukeSim for CT image formation with vendor\nand parameter variability. The dataset includes 3,072 nodule images from\n1,044 simulated CT scans, 512 lesions, and 174 digital twins. Effective-\nness was shown for nodule detection, segmentation, cancer classification,\nand synthesis. Models trained on clinical + simulated data outperform\nclinical-only models, with 10% improvement in detection, 2–9% in seg-\nmentation, 2–9% in classification and enhanced synthesis. By integrating\nanatomy-informed simulations, this work facilitates future AI model de-\nvelopment such as for rare disease representation and model reliability.\nKeywords: Digital Human Twin · Segmentation · Computed Tomogra-\nphy · Simulated Nodule · Detection · classification · Synthesis.\n1\nIntroduction\nComputer-aided detection and diagnosis systems powered by deep learning are\ncrucial for lung cancer screening, yet they remain limited by data scarcity, espe-\ncially for rare cases [1,2,3]. Lung nodules vary widely in size, shape, and type, yet\nreal-world datasets lack rare cases, segmentation masks, and diagnostic labels,\nlimiting AI training [4,5,6,7,8]. Expert annotation is costly and time-consuming,\nwhile privacy concerns further restrict data sharing. To address these challenges,\ndiffusion-based generative models have been explored for medical image syn-\nthesis [3,9]. While promising, they often lack anatomical and pathological con-\nstraints, failing to capture the full diversity of lung nodules or rare disease cases.\nAdditionally, most image synthesis studies do not address the need to constrain\nfor clinically relevant imaging scenarios.\narXiv:2502.21187v1  [cs.LG]  28 Feb 2025\n\n\n2\nTushar, F.I., et al.\nTo overcome the limitations of existing datasets and generative models, we\nintroduce SYN-LUNGS, a physics- and anatomy-informed framework for gener-\nating simulated lung CT images to enhance AI model training. Unlike traditional\ngenerative models, SYN-LUNGS integrates digital human twins, procedural le-\nsion modeling, and physics-based imaging simulation to improve anatomical and\npathological fidelity. Using publicly available XCAT3 phantoms [10], we gener-\nate digital twins, embedding synthetic lung nodules with controlled size (4-30\nmm), location, and appearance through X-Lesions [11,12]. DukeSim [13] ensures\nimaging realism by introducing scanner-specific variations, replicating vendor\nand acquisition differences. We evaluated SYN-LUNGS in nodule detection, seg-\nmentation, classification, and AI-based synthesis, demonstrating its impact on\nmodel generalization and robustness. In related work, phantom modeling has\nadvanced from single-organ simulations such as for breast imaging to whole-body\nmodels, expanding their applications in CT for imaging protocol optimization,\nperformance evaluation, disease quantification, and in-silico trials [15,16]. This\nevolution has enabled broader use in imaging research, including comparative\nanalysis of imaging tools, AI-driven diagnostics evaluation [15,12,17,16]. To the\nbest of our knowledge, this is among the first works integrating a digital human\nmodel with simulated lung nodules and physics-based CT simulation to enhance\nAI performance across multiple lung cancer diagnosis tasks. To summarize, this\npaper makes the following contributions:\n1. We introduce SYN-LUNGS, a framework that synthesizes lung nodules by in-\ntegrating anatomy-informed digital twins with imaging simulations. We gen-\nerated anatomically and pathologically diverse digital human twins using the\nXCAT3 phantom family, incorporating controlled anatomical, pathological,\nand demographic variations. We employed the X-Lesions tool to procedurally\ngenerate synthetic lung nodules while ensuring realistic lesion characteristics\nbefore embedding them into digital twins.We applied DukeSim, a physics-\nbased imaging simulator, to model realistic CT acquisition physics, ensuring\nsynthetic nodules maintain consistency with real-world imaging.\n2. We evaluate SYN-LUNGS across multiple downstream tasks, demonstrating\nthat models trained on clinical + simulated data outperform clinical-only\nmodels, achieving better generalization and robustness in AI model training.\n2\nMethod\nFig. 1 provides an overview of the SYN-LUNGS workflow, including dataset gen-\neration, downstream tasks, and evaluation metrics. The following sections pro-\nvide detailed descriptions of digital human modeling, nodule simulation, imaging\nsimulation, and the final dataset composition.\n2.1\nDigital Human Twins\nWe used the XCAT3 phantoms [10] to develop digital human twins, curat-\ning anatomical models from clinical chest CT scans. Instead of direct image\n\n\nTitle Suppressed Due to Excessive Length\n3\nFig. 1. Overview of the SYN-LUNGS workflow, integrating XCAT3, X-Lesion,\nand DukeSim for digital twin and nodule simulation. Clinical and simulated\ndatasets are used for nodule detection, segmentation, classification, and syn-\nthesis, with external evaluations using FROC, DICE, AUC, and qualitative\nanalysis.\nFig. 2. Simulated lung nodules with varying sizes and imaging conditions. Top:\ndigital human twins model slices with embedded nodules. Middle: simulated\nCT with different scanner settings. Bottom: zoomed-in nodule views.\n\n\n4\nTushar, F.I., et al.\nsynthesis, this structured pipeline integrates AI-driven segmentation and pro-\ncedural texture modeling to generate anatomy-informed representations. The\nprocess began with nnU-Net segmentation using DukeSeg to delineate over 200\nanatomical structures. A multi-step quality control process ensured anatomical\nfidelity, including statistical validation, anomaly detection, and physician review.\nFollowing segmentation, procedural modeling generated tissue textures such as\nlung vasculature, airways, and trabecular bone structures [18,19,20]. Each tis-\nsue type was mapped to its material properties to facilitate the next step for\nphysics-based simulations. The axial slices of the digital human twins are shown\nin Fig. 2.\n2.2\nNodule Simulation and Embedding\nWe utilized the X-Lesions tool [11,12] to generate and embed simulated lung\nnodules into digital human twins. Nodules were created at 0.1 mm voxel resolu-\ntion, with controlled size, shape, and density. Internal heterogeneity of texture\nwas introduced by the 3D clustered lumpy background (CLB) model [12]. Lesion\nsizes were drawn from a gamma distribution based on NLST [4] nodule data,\nf(ℓs|a, b) =\nba\nΓ(a)ℓa−1\ns\ne−bℓs,\nℓs > 0\n(1)\nwhere ℓs represents lesion size, and a, b are shape and scale parameters. This\npipeline ensures a robust, enriched distribution. Fig. 2 shows axial slices of digital\ntwins with embedded nodules.\n2.3\nCT Simulation\nAfter embedding simulated nodules into digital human twins, CT imaging was\nperformed using DukeSim [13], a validated simulation tool employing ray tracing\nfor primary signals and Monte Carlo simulation for scatter and radiation dose.\nCT projections were reconstructed using a vendor-neutral MCR toolkit, repli-\ncating two representative scanners. The W12 scanner had 12 mm collimation,\n570 mm source-to-isocenter distance (SIsoD), 1040 mm source-to-detector dis-\ntance (SID), 16-channel detectors (1.5 mm width), and 7° anode angle. The W20\nscanner featured 20 mm collimation, 541 mm SIsoD, 949 mm SID, 16-channel\ndetectors (2.19 mm width), and 8° anode angle. Both scanners acquired 1,000\nprojections per scan using Hann 0.5, 0.6, and 1.2 reconstruction filters. Fig. 2\npresents examples of simulated CT images.\n2.4\nSimulated dataset\nThe SYN-LUNGS dataset consists of 174 digital human twins with 512 simulated\nlung nodules, generating 1,044 CT scans and 3,072 nodule images across\nthe two scanners. Each scanner contributed 522 CT scans and 1,536 nodule im-\nages, ensuring coverage of diverse imaging conditions. The dataset includes 95\n\n\nTitle Suppressed Due to Excessive Length\n5\nmales (54.6%), ages 59 ± 15 years, with a mean BMI of 26 ± 6. The cohort\nis 73.6% White, 20.7% Black, and 5.8% Other/Unknown, with 98.3% identify-\ning as non-Hispanic. Unlike LUNA16 [5] and DLCSD24 [7,6], which provide 3D\nbounding boxes, only MSD Task06 [8] and SYN-LUNGS include 3D segmenta-\ntion masks. While NLST [4] lacks segmentation, it provides malignancy labels.\nSYN-LUNGS incorporates statistically derived malignancy labels, adapted from\nprior studies [25]. Among the 174 digital twins, 124 were used for AI model train-\ning, while 50 were reserved as a test set. Performance was evaluated on internal\nand external datasets to assess how clinical + simulated training improves model\ngeneralization and robustness.\n3\nDownstream Tasks and Experiments\nFig. 3. Dataset Distribution and Detection Performance. (a) CT scan and nodule dis-\ntribution. (b) Nodule size density plots. (c) FROC curve: clinical (green) vs. clinical +\nsimulated data (brown). (d) Detection results with axial CT slices (top) and zoomed-in\nviews (bottom), showing nodule size and confidence scores.\n\n\n6\nTushar, F.I., et al.\n3.1\nNodule Detection\nNodule detection is a key step in lung cancer screening. To evaluate the impact of\nsimulated data on AI model performance, we integrated clinical and simulated\ndatasets into an open-source nodule detection pipeline. For training, we used\nLUNA16 [5] , which includes 600 CT scans with over 1,000 annotated nodules,\nand 744 simulated CT images with varying imaging configurations to enhance\ndataset diversity. Fig. 3 shows the datasets and the nodule size distribution.\nAdding simulated dataset significantly enriches the number of images and nod-\nule varing sizes. Two models were trained: one on clinical data only and another\non a clinical + simulated dataset to assess the effect of synthetic augmenta-\ntion. Performance was reported on the DLCSD24 open-access test set [7,6]. We\nemployed the MONAI detection model [21], a widely used two-stage pipeline\nwith region proposal networks (RPNs) and refined classification. The clinical +\nsimulated model shows higher sensitivity across all false-positive rates, with an\naverage sensitivity improvement from 0.45 to 0.55. Highlighted points indicate\nsensitivity at selected false-positive thresholds (Fig. 3(c)). Fig. 3(d) shows the\nnodule detection results of the model across different sizes.\n3.2\nNodule Segmentation\nNodule segmentation is crucial in lung cancer screening, providing precise lesion\nboundaries for diagnosis. To evaluate the impact of simulated data, we integrated\nclinical and simulated datasets into an open-source segmentation pipeline, com-\nparing models trained on clinical only and clinical + simulated data to as-\nsess segmentation accuracy and generalization. Fig. 4(a) shown the datasets and\nnoduel size distribution associated to this experiment. We fine-tuned Vista3D [1]\nusing MSD Task06 [8] as the primary clinical dataset and evaluated its gener-\nalizability on DLSC (validation dataset) [7], where voxel-wise annotations were\nunavailable. Performance was assessed within bounding box regions of detected\nnodules. Additionally, we trained nnUNet(v2) [24] on the same datasets, but\nunlike Vista3D, it lacked pretrained weights, requiring training from scratch.\nThe segmentation performance was evaluated on both the internal MSD\nTask06 test dataset [1](Fig. 4(b,c)) and the external DLCSD24 dataset(Fig. 4(d,e)).\nAs shown in Fig. 4, models trained with clinical + simulated data demon-\nstrated improved segmentation compared to those trained on clinical data alone.\nVista3D and nnUNet models fine-tuned and trained with clinical + simulated\ndatasets consistently outperformed their clinical-only counterparts across differ-\nent nodule sizes. Performance gains were particularly notable for smaller nod-\nules (<10 mm), where the inclusion of simulated data enhanced Dice scores.\nQualitative results in Fig. 4(f) further illustrate the segmentation improvements,\nwith better alignment between predicted and ground truth boundaries.\n3.3\nCancer Classification\nClassifying lung nodules is challenging due to overlapping features. We trained\nmodels on clinical-only and clinical + simulated datasets, evaluating them on\n\n\nTitle Suppressed Due to Excessive Length\n7\nFig. 4. Nodule Segmentation dataset(a) and performance(b-f). (a) Histograms of nod-\nule size distributions across datasets, with mean (red), median (blue), and 5-mm thresh-\nold (black) dashed lines for reference.(a-d) Box plots compare Dice scores across models\nand datasets: (b, c) MSD Task06 (internal) and (d, e) DLCSD24 (external). Left: all\nnodules; right: clinically significant sizes. Legends show segmented nodule counts. (f)\nQualitative examples with nodule diameter and Dice scores.\nan external clinical dataset (Fig.5). The DLCSD24 dataset included 1,452 be-\nnign and 166 malignant nodules for training, and 510 benign and 65 malignant\nfor validation [7,6]. From the simulated dataset, 366 nodules (1/4 of the clinical\ntraining set) were randomly selected from 290 synthetic CTs. Malignancy labels\nwere assigned using a statistical model adapted from prior work [25], incorpo-\nrating additional nodule features. Fig.5(a) illustrates the labeling process, where\na logistic regression model trained on NLST [4] CT Set-1 predicted malignancy\nbased on patient (age, sex) and nodule (size, margin, location, type) features.\nFig.5(b) shows its AUC and 95% CI, ensuring reliability across datasets. For\nexternal evaluation, 677 LUNA16 nodules were used, adopting the Radiologist-\nVisual Assessed Malignancy Index (RVAMI) diagnoses [22,7] (Fig. 5(c)). We\ntrained 3D ResNet50 [7] from scratch and fine-tuned pre-trained state-of-the-\nart MedicalNet’s ResNet50 (MedNet3D) [23]. All CT volumes were resampled\n\n\n8\nTushar, F.I., et al.\nFig. 5. Cancer Classification experiment workflow.(a)Statistical labeling, (b)Statistical\nlabeling evaluation,(c)Dataset distribution,(d)Classification performance AUC and\n(e)score.\nto 0.7 × 0.7 × 1.25 mm, with intensity values clipped (-1000 to 500 HU) and\nstandardized. Nodules were extracted as 64 × 64 × 64 patches, and all mod-\nels were trained for 200 epochs, selecting the best-performing model based on\nvalidation AUC. Fig.5(d) compares AUC-ROC curves, showing improved per-\nformance with simulated data. The ResNet3D clinical model achieved 0.78 (95%\nCI: 0.74–0.82), improving to 0.80 (0.77–0.84) with simulated dataset. MedNet3D\nsaw greater gains, increasing from 0.78 (0.75–0.82) to 0.87 (0.85–0.90). Fig.5(e)\nhighlights classification score distributions, showing better prediction agreement\nwith clinical + simulated training.\n3.4\nTargeted Synthesis\nGenerative models have shown promising results in medical imaging, but their\neffectiveness is often limited by training data and the ability to control imaging\nfeatures. The MAISI foundational generative model introduced Control-\nNet [3], allowing control over body region and size without requiring explicit\nmasks. However, for targeted abnormality generation, such models are restricted\nby the availability of diverse pathological data. SYN-LUNGS addresses this gap\nby providing simulated images, facilitating controlled lesion synthesis. In this\nstudy, we developed SYN-ControlNet, an adaptation of MAISI’s Control-\nNet [3], fine-tuned exclusively on simulated solid nodules from SYN-LUNGS. By\nleveraging MAISI’s pretrained VAE and diffusion models, SYN-ControlNet en-\nhances targeted type of nodule synthesis. This fine-tuned model enables control\nover nodule size, and anatomical placement, supporting both clinical and aug-\nmented nodule masks (Fig. 6). Fig. 6 illustrates the targeted synthesis approach\nleveraging MAISI and our fine-tuned SYN-ControlNet. Fig. 6(a) represents the\nfinetuning of the original MAISI framework. Fig. 6(b) demonstrates the infer-\n\n\nTitle Suppressed Due to Excessive Length\n9\nence pipeline ustilizing developed SYN-ControlNet, where clinical or augmented\nnodule masks are used to generate synthetic CT scans, enhancing control over\nnodule type and placement. Fig. 6 (c) provides a comparative visualization, syn-\nthetic CTs generated by MAISI-ControlNet (2nd column) and SYN-ControlNet\n(3rd column) respectively. These results highlight the role of simulated data in\nadvancing generative models for medical imaging, enhancing their control, and\napplicability in clinical AI research.\nFig. 6. Targeted nodule synthesis using simulated data. The first column shows the\ninput body mask with a nodule for ControlNet, while the second and third columns\ndisplay synthetic CT outputs from MAISI-ControlNet and SYN-ControlNet, respec-\ntively. The first row presents axial slices, and the second row zooms into the generated\nnodules. SYN-ControlNet enables nodule synthesis with specific nodule type.\n4\nAdvantages, Limitations and Future Directions\nSYN-LUNGS provides a structured approach to AI model training by integrating\nanatomy-informed simulated data with real-world clinical imaging characteris-\ntics. Our results demonstrate that clinical + simulated training consistently\nimproves performance across multiple tasks. Nodule detection generalizes bet-\nter to external dataset (DLCSD24), and segmentation models fine-tuned with\nSYN-LUNGS achieve higher Dice scores, especially for small nodules [1,3]. In\nmalignancy classification, statistically labeled simulated nodules boosted perfor-\nmance through controlled augmentation. SYN-LUNGS also fine-tunes genera-\ntive models, as shown by SYN-ControlNet [3], enabling anatomically consistent\nnodule generation. The digital human twins and dataset can be requested at\nhttps://cvit.duke.edu/resources/, and model weights, training, and eval-\nuation scripts will be available at https://github.com/fitushar/SYN-LUNGS.\n\n\n10\nTushar, F.I., et al.\nThis study has limitations. The dataset includes 1000+ CT scans and 3000+\nnodules, but unique digital twins remain limited due to full-chest modeling\ncomplexity. A new pipeline is in development to streamline this process. Sta-\ntistical malignancy labeling, based on NLST[4], lacks biological modeling, and\nnodules are currently restricted to solid types, requiring expansion to semi-solid\nand ground-glass. The simulation pipeline mimics two NLST-era vendor con-\nfigurations, and can be expanded with newer, vendor-specific imaging systems\nincluding photon-counting CT. Expanding SYN-ControlNet [3] to generate di-\nverse lung abnormalities and increasing scanner diversity will further enhance\nits clinical relevance.\n5\nConclusion\nThe SYN-LUNGS framework demonstrates improvements in detection, segmen-\ntation, and classification, establishing a foundation for future research aimed\nat enhancing rare disease representation and model reliability in clinical lung\nnodule assessment.\n6\nAcknowledgment\nThis work was funded in part by the National Institutes of Health (P41-EB028744,\nR01EB001838, R01HL155293). We also thank MONAI and NVIDIA for provid-\ning open-access code resources that supported this research.\nReferences\n1. He, Y., et al.: VISTA3D: A Unified Segmentation Foundation Model For 3D Med-\nical Imaging. arXiv preprint arXiv:2406.05285 (2024), https://arxiv.org/abs/\n2406.05285\n2. Kiraly, A.P.,et al.: Assistive AI in Lung Cancer Screening: A Retrospective Multi-\nnational Study in the United States and Japan. Radiology: Artificial Intelligence\n6(3), e230079 (2024). https://doi.org/10.1148/ryai.230079\n3. Guo, P., Zhao, C., Yang, D., Xu, Z., Nath, V., et al.: MAISI: Medical AI for Syn-\nthetic Imaging. arXiv preprint arXiv:2409.11169 (2024), https://arxiv.org/\nabs/2409.11169\n4. National Lung Screening Trial Research Team, Aberle, D.R., et al.: Reduced\nlung-cancer mortality with low-dose computed tomographic screening. The New\nEngland Journal of Medicine 365(5), 395–409 (2011). https://doi.org/10.1056/\nNEJMoa1102873\n5. Setio, A.A.A., et al.: Validation, comparison, and combination of algorithms for\nautomatic detection of pulmonary nodules in computed tomography images: The\nLUNA16 challenge. Medical Image Analysis 42, 1–13 (2017). https://doi.org/10.\n1016/j.media.2017.06.015\n6. Wang, A., Tushar, F.I., Harowicz, M.R., Lafata, K.J., Tailor, T.D., Lo, J.Y.: Duke\nLung Cancer Screening Dataset 2024 (1.1) [Data set]. Zenodo (2024). https://doi.\norg/10.5281/zenodo.13799069\n\n\nTitle Suppressed Due to Excessive Length\n11\n7. Tushar, F.I., et al.: AI in Lung Health: Benchmarking Detection and Diagnos-\ntic Models Across Multiple CT Scan Datasets. arXiv preprint arXiv:2405.04605\n(2024), https://arxiv.org/abs/2405.04605\n8. Antonelli, M., Reinke, A., Bakas, S., et al.: The Medical Segmentation De-\ncathlon. Nature Communications 13(1), 4128 (2022). https://doi.org/10.1038/\ns41467-022-31801-3\n9. Chen, Q., Chen, X., Song, H., Xiong, Z., Yuille, A., Wei, C., Zhou, Z.: Towards\nGeneralizable Tumor Synthesis. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 11147–11158 (2024)\n10. Dahal, L., et al.: XCAT-3.0: A Comprehensive Library of Personalized Digital\nTwins Derived from CT Scans. arXiv preprint arXiv:2405.11133 (2024), https:\n//arxiv.org/abs/2405.11133\n11. Sauer, T.J., Samei, E.: Modeling dynamic, nutrient-access-based lesion progres-\nsion using stochastic processes. In: Schmidt, T.G., Chen, G.-H., Bosmans, H. (eds.)\nMedical Imaging 2019: Physics of Medical Imaging, vol. 10948, 1094850. SPIE, In-\nternational Society for Optics and Photonics (2019). https://doi.org/10.1117/\n12.2513201\n12. McCabe, C., Solomon, J., Segars, W.P., Abadi, E., Samei, E.: Synthesizing het-\nerogeneous lung lesions for virtual imaging trials. In: Fahrig, R., Sabol, J.M., Li,\nK. (eds.) Medical Imaging 2024: Physics of Medical Imaging, vol. 12925, 129251N.\nSPIE, International Society for Optics and Photonics (2024). https://doi.org/10.\n1117/12.3006199\n13. Abadi, E., Harrawood, B., Sharma, S., Kapadia, A., Segars, W.P., Samei, E.:\nDukeSim: A Realistic, Rapid, and Scanner-Specific Simulation Framework in Com-\nputed Tomography. IEEE Transactions on Medical Imaging 38(6), 1457–1465\n(2019). https://doi.org/10.1109/TMI.2018.2886530\n14. Badano, A., et al.: Evaluation of digital breast tomosynthesis as replacement of full-\nfield digital mammography using an in silico imaging trial. JAMA Network Open\n1(7), e185474 (2018). https://doi.org/10.1001/jamanetworkopen.2018.5474\n15. Abadi, E., et al.: Virtual clinical trials in medical imaging: a review. Journal of Med-\nical Imaging 7(4), 042805 (2020). https://doi.org/10.1117/1.JMI.7.4.042805\n16. Tushar, F.I., et al.: Virtual Lung Screening Trial (VLST): An In Silico Replica\nof the National Lung Screening Trial for Lung Cancer Detection. arXiv preprint\narXiv:2404 (2024), https://arxiv.org/abs/2404\n17. Tushar, F.I., et al.: Virtual vs. reality: external validation of COVID-19 clas-\nsifiers using XCAT phantoms for chest computed tomography. In: Drukker, K.,\nIftekharuddin, K.M. (eds.) Medical Imaging 2022: Computer-Aided Diagnosis, vol.\n12033, 1203305. SPIE, International Society for Optics and Photonics (2022).\nhttps://doi.org/10.1117/12.2613010\n18. Abadi, E., Segars, W.P., Sturgeon, G.M., Roos, J.E., Ravin, C.E., Samei, E.: Mod-\neling lung architecture in the XCAT series of phantoms: physiologically based air-\nways, arteries and veins. IEEE Transactions on Medical Imaging 37(3), 693–702\n(2018). https://doi.org/10.1109/TMI.2017.2769640\n19. Abadi, E., et al.: Airways, vasculature, and interstitial tissue: anatomically in-\nformed computational modeling of human lungs for virtual clinical trials. In: Flohr,\nT.G., Lo, J.Y., Schmidt, T.G. (eds.) Medical Imaging 2017: Physics of Medical\nImaging, vol. 10132, 101321Q. SPIE, International Society for Optics and Photon-\nics (2017). https://doi.org/10.1117/12.2254739\n20. Sauer, T.J., McCabe, C., Abadi, E., Samei, E., Segars, W.P.: Surface-based anthro-\npomorphic bone structures for use in high-resolution simulated medical imaging.\n\n\n12\nTushar, F.I., et al.\nPhysics in Medicine & Biology 69(1), 015023 (2023). https://doi.org/10.1088/\n1361-6560/acfc69\n21. MONAI\nProject:\nMONAI\nDetection\nTutorial.\nhttps://github.com/\nProject-MONAI/tutorials/tree/main/detection, last accessed 2024/02/25.\n22. Pai, S., et al.: Foundation model for cancer imaging biomarkers. Nature Machine\nIntelligence 6(3), 354–367 (2024). https://doi.org/10.1038/s42256-024-00707-y\n23. Chen, S., Ma, K., Zheng, Y.: Med3D: Transfer Learning for 3D Medical Im-\nage Analysis. arXiv preprint arXiv:1904.00625 (2019), https://arxiv.org/abs/\n1904.00625\n24. Isensee, F., et al.: nnU-Net: a self-configuring method for deep learning-based\nbiomedical image segmentation. Nature Methods 18(2), 203–211 (2021). https:\n//doi.org/10.1038/s41592-020-01008-z\n25. Tushar, F.I., Vancoillie, L., Ghosh, D., Lafata, J.K., Lo, J.: Beyond Detection:\nBridging the Gap between Virtual Imaging Trials and Clinical Impact. In: Proc.\nVirtual Imaging Trials in Medicine 2024, 2024/05/08 (2024).\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21187v1.pdf",
    "total_pages": 12,
    "title": "SYN-LUNGS: Towards Simulating Lung Nodules with Anatomy-Informed Digital Twins for AI Training",
    "authors": [
      "Fakrul Islam Tushar",
      "Lavsen Dahal",
      "Cindy McCabe",
      "Fong Chi Ho",
      "Paul Segars",
      "Ehsan Abadi",
      "Kyle J. Lafata",
      "Ehsan Samei",
      "Joseph Y. Lo"
    ],
    "abstract": "AI models for lung cancer screening are limited by data scarcity, impacting\ngeneralizability and clinical applicability. Generative models address this\nissue but are constrained by training data variability. We introduce SYN-LUNGS,\na framework for generating high-quality 3D CT images with detailed annotations.\nSYN-LUNGS integrates XCAT3 phantoms for digital twin generation, X-Lesions for\nnodule simulation (varying size, location, and appearance), and DukeSim for CT\nimage formation with vendor and parameter variability. The dataset includes\n3,072 nodule images from 1,044 simulated CT scans, with 512 lesions and 174\ndigital twins. Models trained on clinical + simulated data outperform clinical\nonly models, achieving 10% improvement in detection, 2-9% in segmentation and\nclassification, and enhanced synthesis.By incorporating anatomy-informed\nsimulations, SYN-LUNGS provides a scalable approach for AI model development,\nparticularly in rare disease representation and improving model reliability.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}