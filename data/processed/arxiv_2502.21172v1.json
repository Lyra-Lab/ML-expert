{
  "id": "arxiv_2502.21172v1",
  "text": "Modeling discrete common-shock risks through\nmatrix distributions\nMartin Bladt, Eric C. K. Cheung, Oscar Peralta and Jae-Kyung Woo\nAbstract\nWe introduce a novel class of bivariate common-shock discrete phase-type (CDPH)\ndistributions to describe dependencies in loss modeling, with an emphasis on those\ninduced by common shocks. By constructing two jointly evolving terminating Markov\nchains that share a common evolution up to a random time corresponding to the\ncommon shock component, and then proceed independently, we capture the essential\nfeatures of risk events influenced by shared and individual-specific factors.\nWe de-\nrive explicit expressions for the joint distribution of the termination times and prove\nvarious class and distributional properties, facilitating tractable analysis of the risks.\nExtending this framework, we model random sums where aggregate claims are sums of\ncontinuous phase-type random variables with counts determined by these termination\ntimes, and show that their joint distribution belongs to the multivariate phase-type or\nmatrix-exponential class. We develop estimation procedures for the CDPH distribu-\ntions using the expectation-maximization algorithm and demonstrate the applicability\nof our models through simulation studies and an application to bivariate insurance\nclaim frequency data.\nKeywords: Phase-type distribution; Discrete bivariate distribution; Common shocks;\nExpectation-maximization algorithm.\n1\nIntroduction\nEver since their conception in Jensen (1954), phase-type distributions, which represent\nthe time until absorption in a finite state-space Markov chain, have been extensively used\ndue to their flexibility in approximating a wide range of distributions and their analytical\ntractability (see Bladt and Nielsen (2017) for a review). Such a class of distributions has also\ngained popularity in actuarial science in the last two decades. In insurance ruin problems,\nphase-type distributions can be used to model the claim amounts (e.g. Drekic et al. (2004);\nFrostig et al. (2012)) and/or the inter-arrival times (an assumption implicitly embedded in\nthe class of risk processes with Markovian claim arrivals; see e.g. Cheung and Landriault\n(2010)). Interested readers are referred to Asmussen and Albrecher (2010) and Badescu\n1\narXiv:2502.21172v1  [math.ST]  28 Feb 2025\n\n\nand Landriault (2009) for comprehensive reviews. Outside ruin theory, Hassan Zadeh and\nStanford (2016) developed Bayesian and B¨uhlmann credibility theories under phase-type\nclaims whereas Wang et al. (2018) derived the distribution of the discounted aggregate\nclaims under phase-type renewal process. Generalizations of phase-type distributions have\nalso been proved to be successful for fitting insurance loss data. In particular, Ahn et al.\n(2012) utilized log phase-type distribution to fit the well-known Danish fire data, and\nBladt and Yslas (2023a) proposed a phase-type mixture-of-experts regression model to fit\nthe severities from the French Motor Third Party Liability dataset. Apart from modeling\ninsurance losses and interclaim times, phase-type mortality law was studied by Lin and\nLiu (2007), and a review of the use of phase-type distributions in health care systems can\nbe found in Fackrell (2009). While all the above works are concerned with continuous\nphase-type distributions, discrete phase-type (DPH) distributions have also been applied\nto model claim frequencies. For example, Panjer-type recursions for the calculations of\ncompound distributions under DPH claim counts were developed by Eisele (2006); Wu and\nLi (2010); Ren (2010).\nIn modern risk theory and insurance mathematics, there is increasing need for re-\nsearchers and practitioners to accurately model the dependencies among multiple risk fac-\ntors or risk events, as this is crucial for effective risk management. Such dependence mod-\neling is particularly important in scenarios where different risks are influenced by shared\nfactors or catastrophic events—commonly referred to as common shocks, leading to real-\nization of random variables that are strongly dependent. For example, an accident or a\nnatural disaster can cause claims in both personal injury and property damage, resulting\nin correlated claim counts and dependent losses in different insurance contracts or business\nlines. Capturing dependencies is essential for a realistic assessment of joint behavior of\nrisks. While the use of copulas is popular (see e.g. Frees and Valdez (1998) for a review),\nspecific models for multivariate claim counts (e.g. Shi and Valdez (2014); Pechon et al.\n(2018); Bolanc´e and Vernic (2019); Fung et al. (2019)) and multivariate losses (e.g. Lee\nand Lin (2012); Willmot and Woo (2015)) have been developed as well. The presence of\ndependency has also led to related research topics in actuarial science, such as multivariate\nrisk measures (see e.g. Cossette et al. (2016); Landsman et al. (2016)) and multivariate\nruin problems (see e.g. Badila et al. (2015); Albrecher et al. (2022)).\nDependencies also commonly arise in the applied probability literature, and several ap-\nproaches to constructing continuous multivariate phase-type distributions have been pro-\nposed in the field. These models often introduce dependencies by coupling the underlying\nMarkov processes in specific ways, namely, by employing the exit times from different sets\nas in Assaf et al. (1984), or by using accumulating reward systems as in Kulkarni (1989).\nIn Bladt and Nielsen (2010), a class of multivariate matrix-exponential (MME) distribu-\ntions, characterized by rational multivariate Laplace transforms, was introduced. However,\nthe multivariate matrix distributions in Kulkarni (1989) and Bladt and Nielsen (2010) do\nnot necessarily admit closed-form probability density function in general, possibly limiting\ntheir applications. With an emphasis on explicit results and ease of applications in an\n2\n\n\nactuarial context, recent efforts have also been made in Cheung et al. (2022); Bladt (2023);\nAlbrecher et al. (2023); Bladt and Yslas (2023b) to introduce new classes of distributions\nwhich are conditionally independent mixtures of matrix distributions. Specifically, in Bladt\n(2023), the author proposed to employ a system of Markov jump processes that agree on\nthe initial state and evolve independently from that point onward.\nDespite the increasing interest in continuous multivariate matrix distributions men-\ntioned above, extensions of the DPH class to a multivariate setting, particularly in the\ncontext of matrix distributions, are relatively scarce in the literature.\nOne of the first\nattempts is the work by He and Ren (2016a), who constructed a multivariate DPH distri-\nbution via the numbers of different types of batches that have arrived before absorption of a\ndiscrete-time Markov chain, with an accompanying estimation scheme discussed in He and\nRen (2016b). A discrete analog of Kulkarni (1989)’s continuous multivariate phase-type\ndistributions was considered in Navarro (2019); however, such a class, like its continuous\ncounterpart, does not admit closed-form expressions for the joint probability mass function\n(pmf). A more recent development was presented by Bladt and Yslas (2023b) which is a\ndiscrete version of Bladt (2023)’s work, where the authors investigated the termination\ntime of discrete-time Markov chains that begin in the same initial state but otherwise\nbehave independently. However, many of these models impose dependencies that do not\nnecessarily explicitly capture the common-shock scenarios that arise.\nIn this paper, we introduce a novel class of bivariate common-shock discrete phase-type\n(CDPH) distributions. Our approach extends the DPH framework to a bivariate setting by\nconstructing two temporarily jointly evolving Markov chains that share a common evolu-\ntion up to a random time—the occurrence of the common shock (or the number of common\nshocks)—after which they evolve independently. This construction effectively models the\ndependency induced by common shocks while retaining the tractability of DPH distribu-\ntions. More precisely, we consider two discrete-time Markovian processes, M1 and M2,\nwhich evolve identically until a decoupling time τ1,2, after which they proceed indepen-\ndently and terminate at possibly different times τ1 and τ2. We derive explicit expressions\nfor the joint distribution of (τ1, τ2), which characterize the CDPH distributions. This frame-\nwork allows us to model scenarios where two types of risks are affected by shared events\nbefore diverging due to independent factors. We can show that, in the bivariate case, our\nproposed class of CDPH distributions is larger than the class of multivariate DPH distri-\nbutions in Bladt and Yslas (2023b) and is a subclass of that considered by Navarro (2019).\nBuilding on this foundation, we explore applications of the CDPH distributions in risk\nmodeling involving random sums with common shocks. In particular, we consider aggre-\ngate risks represented as sums of random variables whose counts are determined by τ1 and\nτ2. This setup is particularly relevant in insurance, where aggregate claims may depend\non both shared catastrophic events and independent claim occurrences. We characterize\nthe joint distributions of these random sums, demonstrating the practicality and analytical\nconvenience of our models.\nOur contributions can be summarized as follows: We introduce the CDPH distributions,\n3\n\n\na new class of bivariate DPH distributions designed to effectively model dependencies in-\nduced by common shocks. To facilitate their applications in loss modeling, we provide\nexplicit formulas for the joint pmf and probability generating function (pgf) of the CDPH\nclass and prove various closure properties.\nCharacterizations of the associated random\nsums are also provided. Furthermore, we develop estimation procedures for the CDPH\ndistributions using the expectation-maximization (EM) algorithm, enabling their statisti-\ncal implementation. Finally, we illustrate the applicability and effectiveness of our models\nthrough simulation studies, highlighting their potential in practical risk assessment scenar-\nios. In essence, our work extends the versatility of phase-type distributions in modeling\ncomplex dependencies in arising in actuarial science, offering both theoretical insights and\npractical tools for actuaries and risk managers.\nThe remainder of the paper is organized as follows. In Section 2, we define the CDPH\ndistributions and derive their fundamental properties. Section 3 applies these distribu-\ntions to analyze random sums.\nIn Section 4, we address the estimation of the CDPH\ndistributions from data, proposing an EM algorithm tailored to our models. Section 5\npresents simulation studies that demonstrate the implementation and performance of our\nestimation procedures, as well as an application to a dataset comprising bivariate insurance\nclaim frequencies. Finally, Section 6 concludes the paper and suggests directions for future\nresearch.\n2\nA new class of CDPH distributions\nThis section provides some of the essential properties of CDPH distributions, starting from\nbasic principles and then treating more intricate class properties. Throughout, we link our\nclass to related constructions in the literature.\n2.1\nBackground on DPH\nConsider a Markov chain M = {M(n)}n∈N evolving in a finite state-space E = {1, 2, . . . , d}\naccording to some initial probability vector α = (α1, α2, . . . , αd) and subprobability matrix\nP = {pij}i,j∈E, where N is the set of non-negative integers. Here, M is assumed to be\n(almost surely) terminating, meaning that while in state i, it gets absorbed in the next\nstep with probability 1 −Pd\nj=1 pij. We note that if M has no superfluous states, then the\nproperty of M terminating is equivalent to the eigenvalues of P being strictly contained\nin the complex unit circle (see e.g. Theorem 1.2.63 in Bladt and Nielsen (2017)).\nAn alternative setup is to consider the augmented state-space E ∪∆, where ∆is an\nabsorbing cemetery state used to indicate the termination of M. In this setup, M evolves\naccording to the probability vector (α1, α2, . . . , αd, 0) and a block-partitioned transition\nmatrix\n\u0012P\n1 −P 1\n0\n1\n\u0013\n,\n4\n\n\nwhere 1 denotes a column vector of ones of appropriate dimension (d in this case), P 1\ncorresponds to the sum of probabilities from each state to all others in E, and 0 denotes\na zero matrix of appropriate dimension (a zero row vector of dimension d in this case).\nThe final ‘1’ is a scalar representing the probability of remaining in the state ∆once\nentered. Both setups—the terminating and the absorbing ones—are interchangeable for\nour purposes, and we switch back and forth depending on the situation.\nDefine τ to be the step in which M gets terminated, that is, let\nτ := inf{n ∈N+ : M(n) /∈E},\nwhere N+ is the set of positive integers. Then τ follows what is known as a DPH distri-\nbution, which we denote by DPH(α, P ). The class of DPH distributions is particularly\ntractable (see e.g. Chapter 1.2.6 in Bladt and Nielsen (2017)). For instance, its pmf over\nN+ is explicitly given by\nfτ(ℓ) := P(τ = ℓ) = αP ℓ−1 (1 −P 1) .\nLikewise, the pgf takes the tractable form\nE(xτ) = xα(I −xP )−1 (1 −P 1) ,\nwhere I denotes the identity matrix. Note that the above formula is valid for all x ∈R\non the real line such that the eigenvalues of xP lie within the complex unit circle (the set\nthat contains the interval [−1, 1]).\n2.2\nDefinition, joint probability and joint moments of CDPH\nAs mentioned in the introduction, literature on the construction of multivariate DPH\ndistributions is rather limited. Our paper is inspired by the approach in Bladt and Yslas\n(2023b), adding the flexibility that the Markov chains evolve synchronously for some period\nof time (rather than just starting in the same initial state) before proceeding independently.\nThe exact details behind our novel construction are as follows. Consider two jointly evolving\nterminating processes, M1 = {M1(n)}n∈N and M2 = {M2(n)}n∈N, each taking values\nin the state space E ∪S, where E ∩S = ∅and |E| ∧|S| ≥1. We construct the joint\nprocess (M1, M2) such that each marginal process is a Markov chain, with a specific joint\ndependence structure: we let M1(n) = M2(n) for all n up to the first time they exit the set\nE. After this point, they evolve independently within S and eventually terminate, possibly\nat different times.\nMore explicitly, we let M1 be a terminating Markov chain that evolves according to the\nblock-partitioned initial probability vector (α, 0) and subprobability matrix\nP1 :=\n\u0012P\nU\n0\nQ1\n\u0013\n,\n5\n\n\nwhere P is a subprobability matrix over E (like before), U denotes the transition probabil-\nities from E to S, Q1 is a subprobability matrix over S. It is further assumed that each row\nof the submatrix (P U) sums to one, so that the Markov chain M1 (and also M2 below)\nmust enter S before termination. For M2, we define M2(n) = M1(n) for all n ≤τ1,2, where\nτ1,2 := inf{n ∈N+ : M1(n) /∈E}.\nObviously, τ1,2 follows a DPH(α, P ) distribution. After time τ1,2, we let M2 evolve inde-\npendently as a terminating Markov chain within S according to the subprobability matrix\nQ2. This construction yields M2 as a Markov chain in E ∪S that evolves according to the\nblock-partitioned initial probability vector (α, 0) and subprobability matrix\nP2 :=\n\u0012P\nU\n0\nQ2\n\u0013\n.\nWe now define the bivariate random variable (τ1, τ2) via the termination times of M1 and\nM2 so that, for k ∈{1, 2},\nτk := inf{n ∈N+ : Mk(n) /∈E ∪S}.\nFor each k ∈{1, 2}, the marginal distribution of τk is clearly DPH((α, 0), Pk). Moreover,\nτ1 and τ2 are greater than τ1,2. Specifically, by writing τ1 = τ1,2 + (τ1 −τ1,2) and τ2 =\nτ1,2 + (τ2 −τ1,2), one observes that τ1 and τ2 are dependent for two reasons:\n• Both τ1 and τ2 share the term τ1,2, the time until which the Markov chains M1 and\nM2 evolve identically.\nThe random variable τ1,2 can be regarded as the common\nshock component.\n• Although the Markov chains M1 and M2 evolve independently after time τ1,2, the\nremaining times until they get absorbed (namely τ1 −τ1,2 and τ2 −τ1,2) are only\nconditionally independent given the state M1(τ1,2) of the Markov chain M1 at the\ntime τ1,2. Unconditionally, the variables τ1 −τ1,2 and τ2 −τ1,2 are dependent.\nThe joint distribution of (τ1, τ2) is provided in the following proposition.\nProposition 2.1. For n1, n2 ≥2, m, z1, z2 ≥1, and i ∈E, define the unconditional and\nconditional joint pmf’s of (τ1, τ2) by\nfτ1,τ2(n1, n2) := P(τ1 = n1, τ2 = n2),\nfτ1,τ2|M1,M2(n1, n2|i) := P(τ1 = n1, τ2 = n2 | M1(0) = M2(0) = i),\nand define the auxiliary functions\nψ(m, z1, z2) := P(τ1,2 = m, τ1 −τ1,2 = z1, τ2 −τ1,2 = z2),\n6\n\n\nψi(m, z1, z2) := P(τ1,2 = m, τ1 −τ1,2 = z1, τ2 −τ1,2 = z2 | M1(0) = M2(0) = i).\nThen, we have\nfτ1,τ2(n1, n2) =\nmin{n1,n2}−1\nX\nm=1\nψ(m, n1 −m, n2 −m),\n(2.1)\nfτ1,τ2|M1,M2(n1, n2|i) =\nmin{n1,n2}−1\nX\nm=1\nψi(m, n1 −m, n2 −m),\n(2.2)\nand\nψ(m, z1, z2) = αP m−1U\n\u0010\n(Qz1−1\n1\nq1) ⊙(Qz2−1\n2\nq2)\n\u0011\n,\n(2.3)\nψi(m, z1, z2) = e⊺\ni P m−1U\n\u0010\n(Qz1−1\n1\nq1) ⊙(Qz2−1\n2\nq2)\n\u0011\n,\n(2.4)\nwhere q1 = 1 −Q11 and q2 = 1 −Q21 denote the one-step probabilities of termination\nfrom the states in S for M1 and M2, respectively; ei denotes the i-th canonical column\nvector, and ⊙denotes the Hadamard (entrywise) product of two vectors.\nProof. Equations (2.1) and (2.2) follow by straightforward application of the law of total\nprobability. Next,\nP(τ1,2 = m, τ1 −τ1,2 = z1, τ2 −τ1,2 = z2)\n=\nX\nj∈S\nP(τ1,2 = m, M1(m) = M2(m) = j)\n× P(τ1 −τ1,2 = z1, τ2 −τ1,2 = z2 | τ1,2 = m, M1(m) = M2(m) = j)\n=\nX\nj∈S\nP(τ1,2 = m, M1(m) = M2(m) = j)\n× P(τ1 −τ1,2 = z1 | τ1,2 = m, M1(m) = j)P(τ2 −τ1,2 = z2 | τ1,2 = m, M2(m) = j)\n=\nX\nj∈S\n(αP m−1U)j(Qz1−1\n1\nq1)j(Qz2−1\n2\nq2)j\n= αP m−1U\n\u0010\n(Qz1−1\n1\nq1) ⊙(Qz2−1\n2\nq2)\n\u0011\n,\nwhich establishes (2.3). Finally, (2.4) follows by replacing the initial distribution α with\ne⊺\ni .\nUsing Proposition 2.1, one can readily show the following corollary.\n7\n\n\nCorollary 2.2. For ζ0, ζ1, ζ2 ∈[0, 1], the joint pgf of (τ1,2, τ1 −τ1,2, τ2 −τ1,2) is given by\nE\n\u0010\nζτ1,2\n0\nζτ1−τ1,2\n1\nζτ2−τ1,2\n2\n\u0011\n=\nX\nj∈S\n(α(Iζ−1\n0\n−P )−1U)j((Iζ−1\n1\n−Q1)−1q1)j((Iζ−1\n2\n−Q2)−1q2)j.\n(2.5)\nMoreover, the joint pgf of (τ1, τ2) is given by\nE (ζτ1\n1 ζτ2\n2 ) =\nX\nj∈S\n(α(I(ζ1ζ2)−1 −P )−1U)j((Iζ−1\n1\n−Q1)−1q1)j((Iζ−1\n2\n−Q2)−1q2)j.\n(2.6)\nProof. As ψ is the joint pmf of (τ1,2, τ1 −τ1,2, τ2 −τ1,2), we have\nE\n\u0010\nζτ1,2\n0\nζτ1−τ1,2\n1\nζτ2−τ1,2\n2\n\u0011\n=\nX\nz0,z1,z2≥1\nψ(z0, z1, z2)ζz0\n0 ζz1\n1 ζz2\n2\n=\nX\nz0,z1,z2≥1\nX\nj∈S\n(αP z0−1U)j(Qz1−1\n1\nq1)j(Qz2−1\n2\nq2)jζz0\n0 ζz1\n1 ζz2\n2\n= ζ0ζ1ζ2\nX\nj∈S\nX\nz0,z1,z2≥1\n(α(ζ0P )z0−1U)j((ζ1Q1)z1−1q1)j((ζ2Q2)z2−1q2)j\n= ζ0ζ1ζ2\nX\nj∈S\n(α(I −ζ0P )−1U)j((I −ζ1Q1)−1q1)j((I −ζ2Q2)−1q2)j,\nwhich yields (2.5). Substituting ζ0 = ζ1ζ2 into (2.5) gives (2.6).\nEmploying Corollary 2.2, we are able to compute the cross moments of (τ1, τ2) via the\nfollowing result regarding falling factorial moments. Here, for n ∈N and a generic random\nvariable X, we let X[n] = X(X −1)(X −2) · · · (X −n + 1) denote the falling factorial of\nX.\nCorollary 2.3. For n0, n1, n2 ∈N, the joint factorial moments of (τ1,2, τ1 −τ1,2, τ2 −τ1,2)\nare given by\nE\n\u0000τ [n0]\n1,2 (τ1 −τ1,2)[n1] (τ2 −τ1,2)[n2]\u0001\n=\nX\nj∈S\n(α e\nP (n0)U)j( e\nQ1(n1)q1)j( e\nQ2(n2)q2)j,\n(2.7)\nwhere\ne\nP (n0) := ∂n0\n∂ζn0\n1\n(Iζ−1\n0\n−P )−1\n\f\f\f\f\nζ0=1\n= n0!P n0−1(I −P )−n0−1,\nn0 ∈N+,\ne\nQ1(n1) := ∂n1\n∂ζn1\n1\n(Iζ−1\n1\n−Q1)−1\n\f\f\f\f\nζ1=1\n= n1!Qn1−1\n1\n(I −Q1)−n1−1,\nn1 ∈N+,\ne\nQ2(n2) := ∂n2\n∂ζn2\n2\n(Iζ−1\n2\n−Q2)−1\n\f\f\f\f\nζ2=1\n= n2!Qn2−1\n2\n(I −Q2)−n2−1,\nn2 ∈N+,\nwith e\nP (0) = (I −P )−1, e\nQ1(0) = (I −Q1)−1 and e\nQ2(0) = (I −Q2)−1.\n8\n\n\nProof. It is standard that the (n0, n1, n2)-th falling factorial moment is obtained from\nthe joint pgf (2.5) by taking ni derivatives with respect to ζi for all i = 0, 1, 2 and then\nevaluating at ζ0 = ζ1 = ζ2 = 1 so that\nE\n\u0000τ [n0]\n1,2 (τ1 −τ1,2)[n1] (τ2 −τ1,2)[n2]\u0001\n=\n∂n0+n1+n2\n∂ζn0\n0 ∂ζn1\n1 ∂ζn2\n2\nE(ζτ1,2\n0\nζτ1−τ1,2\n1\nζτ2−τ1,2\n2\n)\n\f\f\f\f\nζ0=1,ζ1=1,ζ2=1\n.\nExploiting the linearity of differentiation yields (2.7) as long as one can prove the formula\n∂r\n∂ζr\n0\nζ0(I −T ζ0)−1 = r!T r−1(I −T ζ0)−r−1,\nr ∈N+,\nwhere T can be P , Q1 or Q2. It can be seen from the proof of Theorem 1.2.69 of Bladt\nand Nielsen (2017) that the above equation is indeed valid, concluding our proof.\nRemark 2.1. While Corollary 2.3 does not explicitly provide the cross moments of τ1 and\nτ2, by writing τ1 = τ1,2 +(τ1 −τ1,2) and τ2 = τ1,2 +(τ2 −τ1,2) followed by applying binomial\nexpansions of their powers, it can be readily seen that the afore-mentioned moments can be\nrepresented as linear combinations of the falling factorial moments in (2.7). For instance,\none finds\nE\n\u0000τ 2\n1 τ2\n\u0001\n= E(τ 3\n1,2) + 2E(τ 2\n1,2 (τ1 −τ1,2)) + E(τ1,2 (τ1 −τ1,2)2) + E(τ 2\n1,2 (τ2 −τ1,2))\n+ 2E(τ1,2 (τ1 −τ1,2)(τ2 −τ1,2)) + E((τ1 −τ1,2)2(τ2 −τ1,2)).\nNote that the support of (τ1, τ2) is (N + 2) × (N + 2). For modeling purposes, it is often\nuseful to consider discrete random variables that have a lattice different from N + 2. Thus,\nfor k1, k2 ∈R and c1, c2 > 0, one can define the vector (N1, N2) where\nN1 := c1(τ1 −2) + k1,\nN2 := c2(τ2 −2) + k2.\n(2.8)\nWe then say in a broad sense (see Remark 2.2) that the bivariate random variable (N1, N2)\nfollows a bivariate CDPH distribution with support on (c1N + k1) × (c2N + k2) and param-\neters (α, P , U, Q1, Q2). Employing Proposition 2.1, we obtain the following immediate\nresult.\nCorollary 2.4. Let (N1, N2) follow a bivariate CDPH distribution with support (c1N +\nk1) × (c2N + k2) and parameters (α, P , U, Q1, Q2). Then, the joint pmf of (N1, N2) for\nx1 ∈c1N + k1 and x2 ∈c2N + k2, namely fN1,N2(x1, x2) := P(N1 = x1, N2 = x2), is given\nby\nfN1,N2(x1, x2) = fτ1,τ2\n\u0012x1 −k1\nc1\n+ 2, x2 −k2\nc2\n+ 2\n\u0013\n,\nwhere fτ1,τ2(n1, n2) = P(τ1 = n1, τ2 = n2).\n9\n\n\n2.3\nRelationships with other bivariate DPH distributions\nLet us first note that the (bivariate version of) the so-called mDPH class considered in\nBladt and Yslas (2023b) is a subclass of our proposed CDPH.\nIndeed, if we let β be\na probability row vector and Q a subprobability matrix, then choosing the parameters\nα = (1), P = (0), U = β, and Q1 = Q2 = Q for a CDPH over the lattice (N+1)×(N+1)\n(i.e. c1 = c2 = 1 and k1 = k2 = −1) yields an mDPH distribution. This distribution\narises from starting two terminating processes in the same state according to the initial\nprobability vector β then and evolving them independently from there onwards according\nto the subprobability matrix Q.\nIn turn, the CDPH with support in (N + 2) × (N + 2) forms a subclass of the MDPH∗\nclass introduced in Chapter 5.2 of Navarro (2019). These distributions accumulate integer-\nvalued rewards across each occupation epoch within a shared Markov chain, rather than\nbifurcating into independent processes after a common shock component concludes. Specif-\nically, let us consider a partitioned state-space E ∪(S × S) ∪({∂1} × S) ∪(S × {∂2}) and a\nMarkov chain γ = {γ(n)}n∈N defined over it. The chain evolves within E while the common\nshock is ongoing, then jumps to S ×S (here we take the lexicographic order of S ×S) while\nthe independent components are active, and finally either terminates, jumps to {∂1} × S,\nor jumps to S × {∂2}. These transitions occur depending on whether the independent\ncomponents terminate at the same time, the first coordinate terminates before the second,\nor the second coordinate terminates before the first. Thus, we define the bivariate random\nvector (τ1, τ2) as\nτ1 :=\nσ−1\nX\nℓ=0\n1γ(ℓ)∈E∪(S×S)∪(S×{∂2}),\nτ2 :=\nσ−1\nX\nℓ=0\n1γ(ℓ)∈E∪(S×S)∪({∂1}×S),\n(2.9)\nwhere σ corresponds to the termination time of γ so that σ := inf{n ∈N+ : γ(n) /∈\nE ∪(S × S) ∪({∂1} × S) ∪(S × {∂2})}. The above representation of the bivariate vector\n(τ1, τ2) is already in the form of Equation (5.5) of Navarro (2019). It remains to specify\nthe transition probability of the terminating Markov chain γ.\nTo mimic the distributional properties of the original processes M1 and M2 embedded\nin the Markov chain γ, we take the block-partitioned parameters (α, 0, 0, 0), and the\nsubprobability matrix\nPmax :=\n\n\n\n\nP\nU ∗\n0\n0\n0\nQ1 ⊗Q2\nq1 ⊗Q2\nQ1 ⊗q2\n0\n0\nQ2\n0\n0\n0\n0\nQ1\n\n\n\n,\n(2.10)\nwhere U ∗= {u∗\ni,(j1,j2)}i∈E,(j1,j2)∈S×S is defined by u∗\ni,(j1,j2) := uij1δj1,j2, with δj1,j2 denoting\nthe Kronecker delta and uij the (i, j)-th element of U, and ⊗represents the Kronecker\nproduct between matrices. Then, σ is DPH((α, 0, 0, 0), Pmax). Note that the resulting\n10\n\n\nparameters are similar to those used to compute the distribution of the maximum between\ntwo discrete phase-type distributions (Bladt and Nielsen, 2017, Theorem 1.2.67).\nThe\nderivation of these parameters for the process γ is as follows:\n• The process evolves in E according to P , mimicking the common shock component\nτ1,2, eventually jumping from a state i ∈E to a state (j, j) ∈S × S with probability\nuij.\n• It then evolves in S×S according to Q1⊗Q2 such that a transition from (i1, i2) ∈S×S\nto (j1, j2) ∈S × S occurs with probability q1,i1j1 q2,i2j2 (where, for k = 1, 2, we use\nqk,ij to denote the (i, j)-th element of Qk). Then, either both components terminate\nsimultaneously according to the exit column vector q1 ⊗q2; the first terminates while\nthe second remains active, moving to {∂1} × S; or the second terminates while the\nfirst remains active, moving to S × {∂2}.\n• If the process jumps to {∂1} × S or S × {∂2}, it continues evolving according to the\nmatrix Q2 or Q1, respectively.\nThe above results are summarized in the following proposition.\nProposition 2.5. Concerning bivariate DPH distributions, our proposed CDPH class sat-\nisfies the relationships\nmDPH ⊂CDPH (with support (N + 1) × (N + 1)),\nCDPH (with support (N + 2) × (N + 2)) ⊂MDPH∗,\nwhere mDPH is the class introduced by Bladt and Yslas (2023b) and MDPH∗is the class\nintroduced by Navarro (2019).\nIt is important to emphasize that, although the MDPH∗class in Navarro (2019) is a\nlarger class than our CDPH class on (N + 2) × (N + 2), a CDPH distributed bivariate\nrandom vector admits explicit joint pmf as in Proposition 2.1 while the same is not true\nfor the MDPH∗class.\nRemark 2.2. Since the general CDPH distributed random vector (N1, N2) in (2.8) with\nsupport on (c1N + k1) × (c2N + k2) is merely linear transformation of its basic version\n(τ1, τ2) which has support on (N + 2) × (N + 2), from now we shall simply use CDPH\nto denote the class with support on (N + 2) × (N + 2), and say that (τ1, τ2) follows a\nCDPH(ααα, P , U, Q1, Q2) distribution.\n2.4\nFurther distributional and closure properties\nBelow, we explore further properties of the CDPH class.\nIn particular, we investigate\ncertain operations on the vector (τ1, τ2) that belong to the class of DPH or CDPH distri-\nbutions.\n11\n\n\nMinimum and maximum between τ1 and τ2\nConsider again the Markov chain γ driven by (2.10), with initial vector (α, 0, 0, 0), state-\nspace E ∪(S × S) ∪({∂1} × S) ∪(S × {∂2}) and termination time σ. One can easily see\nthat σ indeed corresponds to the maximum τmax := max{τ1, τ2} of τ1 and τ2. Moreover,\nthe minimum τmin := min{τ1, τ2} is simply the exit time of the Markov chain γ from the\nset E ∪(S × S). Defining the matrix\nPmin :=\n\u0012P\nU ∗\n0\nQ1 ⊗Q2\n\u0013\n,\n(2.11)\nthe next proposition follows as a direct consequence.\nProposition 2.6. Given a CDPH(ααα, P , U, Q1, Q2) random vector (τ1, τ2), the maximum\nτmax follows a DPH((α, 0, 0, 0), Pmax) distribution and the minimum τmin follows a\nDPH((α, 0), Pmin) distribution, where Pmax and Pmin are given by (2.10) and (2.11), re-\nspectively.\nAggregation of coordinates τ1 + τ2\nWhen considering the sum of the coordinates of (τ1, τ2), it is helpful to represent it as\nτ1 +τ2 = 2 min{τ1, τ2}+(max{τ1, τ2}−min{τ1, τ2}). That is, τ1 +τ2 counts twice the steps\nin which both τ1 and τ2 have not terminated, and then adds the steps during which the\nlarger value exceeds the smaller one. Consequently, we have\nτ1 + τ2 =\nσ−1\nX\nℓ=0\n\u00002 · 1γ(ℓ)∈E∪(S×S) + 1γ(ℓ)∈({∂1}×S)∪(S×{∂2})\n\u0001\n.\n(2.12)\nWe have the following proposition.\nProposition 2.7. Given a CDPH(ααα, P , U, Q1, Q2) random vector (τ1, τ2), the sum τ1+τ2\nfollows a DPH distribution.\nProof. With the representation (2.12) for the random variable τ1 + τ2, it follows from\nTheorem 5.2 in Navarro (2019) that the distribution of τ1 +τ2 is a mixture of a probability\nmass at zero and a DPH distribution.\nHowever, both τ1 and τ2 are at least one from\ntheir definitions, implying that the afore-mentioned probability mass at zero must be non-\nexistent and hence τ1 + τ2 follows a DPH distribution. The construction of the parameters\nof such a DPH distribution can be found in Chapter 5.1.1.2 in Navarro (2019) and is\nomitted for brevity.\n12\n\n\nMixture of CDPH random vectors\nFor each i ∈{1, 2, . . . , m}, suppose that the random vector (τ1(i), τ2(i)) is CDPH dis-\ntributed. Let a mixture be formed where the component (τ1(i), τ2(i)) is chosen with prob-\nability wi > 0. The following proposition shows that the class of CDPH distributions is\nclosed under finite mixture.\nProposition 2.8. Suppose that a mixture is formed such that a bivariate random vector\n(τ1,mix, τ2,mix) is chosen to be (τ1(i), τ2(i)) with probability wi > 0, such that (τ1(i), τ2(i))\nis CDPH(ααα(i), P (i), U(i), Q1(i), Q2(i)) distributed and Pm\ni=1 wi = 1. Then, (τ1,mix, τ2,mix)\nfollows a CDPH(αααmix, Pmix, Umix, Q1,mix, Q2,mix) distribution, where the parameters are\ngiven by\nαααmix :=\n\u0000w1ααα(1)\nw2ααα(2)\n· · ·\nwmααα(m)\n\u0001\n,\nPmix :=\n\n\n\n\n\nP (1)\n0\n· · ·\n0\n0\nP (2)\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nP (m)\n\n\n\n\n,\nUmix :=\n\n\n\n\n\nU(1)\n0\n· · ·\n0\n0\nU(2)\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nU(m)\n\n\n\n\n,\nQ1,mix :=\n\n\n\n\n\nQ1(1)\n0\n· · ·\n0\n0\nQ1(2)\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nQ1(m)\n\n\n\n\n,\nQ2,mix :=\n\n\n\n\n\nQ2(1)\n0\n· · ·\n0\n0\nQ2(2)\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nQ2(m)\n\n\n\n\n.\nProof. Indeed, the initial distribution αααmix is simply a weighted combination of the initial\nprobabilities {ααα(i)}m\ni=1 of the component CDPH distributions, with weight wi assigned to\nαααi. The transition matrix Pmix is a block-diagonal matrix, where each block corresponds\nto the transition matrix P (i) of the i-th component. Similarly, Umix, Q1,mix, and Q2,mix\nare block-diagonal matrices, where each block corresponds to U(i), Q1(i), and Q2(i),\nrespectively.\nSuch block-diagonal structure ensures that there is no interaction among\nthe components (i.e. (τ1(i), τ2(i)) across different i’s) once a specific component is chosen\naccording to the mixing weights {wi}m\ni=1. This construction guarantees that the resulting\nparameters describe a valid CDPH distribution for (τ1,mix, τ2,mix).\nSum of CDPH random vectors\nSuppose that the random vector (τ1(i), τ2(i)) is CDPH distributed for each i ∈{1, 2, . . . , m},\nand the random vectors {(τ1(i), τ2(i))}m\ni=1 are independent. Then one could be interested\nin the joint distribution of the sum (τ1,sum, τ2,sum) := (Pm\ni=1 τ1(i), Pm\ni=1 τ2(i)). Indeed, this\nfollows a CDPH distribution. In the next proposition, we provide a proof for the case\nm = 2 and explicitly state the resulting parameters.\n13\n\n\nProposition 2.9. Consider the bivariate random vector (τ1,sum, τ2,sum) = (τ1(1)+τ1(2), τ2(1)+\nτ2(2)), where (τ1(i), τ2(i)) is CDPH(ααα(i), P (i), U(i), Q1(i), Q2(i)) distributed for i ∈{1, 2},\nand the random vectors (τ1(1), τ2(1)) and (τ1(2), τ2(2)) are independent. Then, (τ1,sum, τ2,sum)\nfollows a CDPH(αααsum, Psum, Usum, Q1,sum, Q2,sum) distribution, where the parameters are\ngiven by\nαααsum := (ααα(1), 0),\n(2.13)\nPsum :=\n\u0012P (1)\nU(1) ⊗ααα(2)\n0\nI ⊗P (2)\n\u0013\n,\nUsum :=\n\u0012\n0\n0\nI ⊗U(2)\n0\n\u0013\n,\n(2.14)\nQ1,sum :=\n\u0012Q1(1) ⊗I\nq1(1) ⊗I\n0\nQ1(2)\n\u0013\n,\nQ2,sum :=\n\u0012I ⊗Q2(1)\nI ⊗q2(1)\n0\nQ2(2)\n\u0013\n,\n(2.15)\nwith q1(1) = 1 −Q1(1)1 and q2(1) = 1 −Q2(1)1. The state space corresponding to the\ntransitions specified by Psum is Esum := E(1) ∪(S(1) × E(2)), and that for the transitions\nspecified by Q1,sum and Q2,sum is (S(1) × S(2)) ∪S(2). Here, with the obvious notation,\nE(i) ∪S(i) is the state space of the underlying terminating Markov chains that define the\nvector (τ1(i), τ2(i)).\nProof. In order to explain the key ideas behind the proof, for i, k ∈{1, 2} it will be helpful\nto write τk(i) = τ1,2(i) + (τk(i) −τ1,2(i)). Here the random variable τ1,2(i) is the common\nshock component for the pair (τ1(i), τ2(i)). We can thus decompose each coordinate of the\nrandom vector (τ1,sum, τ2,sum) = (τ1(1) + τ1(2), τ2(1) + τ2(2)) as\nτ1,sum = (τ1,2(1) + τ1,2(2)) + (τ1(1) −τ1,2(1)) + (τ1(2) −τ1,2(2)),\nτ2,sum = (τ1,2(1) + τ1,2(2)) + (τ2(1) −τ1,2(1)) + (τ2(2) −τ1,2(2)).\nTo show that (τ1,sum, τ2,sum) follows a CDPH distribution, we note from the above represen-\ntation that τ1,2(1)+τ1,2(2) can be regarded as the common shock component from which we\ncan construct Psum. After τ1,2(1) and τ1,2(2) are realized and given the states in S(1) and\nS(2) entered upon exiting E(1) and E(2), the variables (τ1(1) −τ1,2(1)) + (τ1(2) −τ1,2(2))\nand (τ2(1)−τ1,2(1))+(τ2(2)−τ1,2(2)) are conditionally independent: such observation can\nbe used to construct Q1,sum and Q2,sum. The construction of the matrix parameters of the\nCDPH representation of (τ1,sum, τ2,sum) is described as follows:\n• We start by considering the part τ1,2(1) that belongs to the common shock τ1,2(1) +\nτ1,2(2). The process underlying (τ1,sum, τ2,sum) starts in E(1) with the initial prob-\nability ααα(1) (which explains (2.13)) and then transitions within E(1) according to\nP (1). Upon exiting E(1), the variable τ1,2(1) is realized and the process underlying\n(τ1(1), τ2(1)) enters S(1) according to U(1). While one can start counting the remain-\ning common shock component τ1,2(2) using the initial probability ααα(2) and transition\n14\n\n\nprobability matrix P (2) within E(2), it is important to also keep track of the state\nin S(1) because this will have an impact on the distributions of τ1(1) −τ1,2(1) and\nτ2(1) −τ1,2(1). These explain the block matrix representation of Psum in (2.14). The\nsubmatrix I ⊗U(2) of Usum in (2.14) subsequently takes care of the termination of\nτ1,2(2) as the process exits S(1) × E(2) and enters S(1) × S(2).\n• Once the underlying process of (τ1,sum, τ2,sum) has entered S(1) × S(2), the clocks\nfor τ1(1) −τ1,2(1) and τ2(1) −τ1,2(1) start ticking independently within the set S(1)\naccording to the transition matrices Q1(1) and Q2(1), respectively, while keeping\nthe state in S(2) fixed.\nWhen τ1(1) −τ1,2(1) (resp.\nτ2(1) −τ1,2(1)) terminates\naccording to the probability vector q1(1) (resp. q2(1)), one no longer needs to keep\ntrack of the state space S(1), and then we move on to consider τ1(2) −τ1,2(2) (resp.\nτ2(2) −τ1,2(2)) as the process evolves within S(2) according to Q1(2) (resp. Q2(2))\nuntil it gets absorbed. Such arguments lead to the specification of the matrices Q1,sum\nand Q2,sum in (2.15).\nThe above construction ensures that (τ1,sum, τ2,sum) is CDPH distributed, and the proof is\ncomplete.\nIt is clear that (τ1,sum, τ2,sum) = (Pm\ni=1 τ1(i), Pm\ni=1 τ2(i)) for values of m greater than\ntwo still follows a CDPH distribution by summing the CDPH bivariate random vectors one\nat a time and using Proposition 2.9 repeatedly.\n3\nCompound sums under bivariate CDPH claim count\nHaving defined the class of bivariate CDPH distributions, we now turn our attention to a\nclass of bivariate random variables (Y1, Y2) with a random sum representation\nY1 :=\nτ1\nX\nℓ=1\nX1,ℓ,\nY2 :=\nτ2\nX\nℓ=1\nX2,ℓ,\n(3.1)\nwhere, for each coordinate k ∈{1, 2}, we aggregate a random number of positive variables\nXk,1, Xk,2, Xk,3, . . ., with the total number τk ∈N drawn from a bivariate CDPH distribu-\ntion. In actuarial science, (τ1, τ2) can be interpreted as bivariate claim count for correlated\nbusiness lines.\nFor k ∈{1, 2}, if Xk,1, Xk,2, Xk,3, . . . are the individual claim amounts\nspecific to line k, then the compound sum Yk corresponds to the aggregate loss for line k.\nBefore analyzing the distribution of (Y1, Y2) in detail, we briefly outline the assumptions we\nmake regarding the sequence {(X1,ℓ, X2,ℓ)}ℓ∈N+. Firstly, we assume that {(X1,ℓ, X2,ℓ)}ℓ∈N+\nis a sequence of independent and identically distributed bivariate random vectors follow-\ning the same distribution as a reference random vector (X1, X2). Secondly, the sequence\n{(X1,ℓ, X2,ℓ)}ℓ∈N+ is independent of (τ1, τ2). While the CDPH distributed (τ1, τ2) has sup-\nport on (N + 2) × (N + 2), the cases where the counting variables are replaced by more\n15\n\n\ngeneral CDPH distributed (N1, N2) with support on (N + k1) × (N + k2) for k1, k2 ∈N can\nbe addressed by modifying our arguments with minimal technical effort (see Remark 2.2).\nWe shall consider two scenarios for X1 and X2: the case where they are independent,\ndiscussed in Subsection 3.1, and the case where they are dependent, addressed in Subsection\n3.2.\n3.1\nIndependent summands\nSuppose that X1 is independent of X2. Below, we provide information about the distribu-\ntion of (Y1, Y2) under this assumption.\nProposition 3.1. For θ1, θ2 ≥0, the joint Laplace transform LY1,Y2(θ1, θ2) := E (exp(−θ1Y1 −θ2Y2))\nof the bivariate random sum in (3.1) is given by\nLY1,Y2(θ1, θ2) = α\n\u0010\nIa−1\n1,θ1a−1\n2,θ2 −P\n\u0011−1\nU\n\u0012\u0012\u0010\nIa−1\n1,θ1 −Q1\n\u0011−1\nq1\n\u0013\n⊙\n\u0012\u0010\nIa−1\n2,θ2 −Q2\n\u0011−1\nq2\n\u0013\u0013\n,\nwhere ak,θk := E\n\u0000e−θkXk\u0001\nis the Laplace transform of Xk for k ∈{1, 2}.\nProof. By conditioning on (τ1, τ2), we have\nLY1,Y2(θ1, θ2) = E\n \nexp\n \n−θ1\nτ1\nX\nℓ=1\nX1,ℓ−θ2\nτ2\nX\nℓ=1\nX2,ℓ\n!!\n= E\n \nE\n \nexp\n \n−θ1\nτ1\nX\nℓ=1\nX1,ℓ−θ2\nτ2\nX\nℓ=1\nX2,ℓ\n!\f\f\f\f\f τ1, τ2\n!!\n= E\n \" τ1\nY\nℓ=1\nE\n\u0010\ne−θ1X1,ℓ\n\u0011# \" τ2\nY\nℓ=1\nE\n\u0010\ne−θ2X2,ℓ\n\u0011#!\n= E\n\u0010\naτ1\n1,θ1 aτ2\n2,θ2\n\u0011\n.\nThe above expression is simply the joint pgf in (2.6) evaluated at the arguments\n(ζ1, ζ2) = (a1,θ1, a2,θ2). Since a1,θ1 and a2,θ2 are the Laplace transforms of positive ran-\ndom variables, one must have a1,θ1, a2,θ2 ∈[0, 1] for θ1, θ2 ≥0. This yields the desired\nresult.\nProposition 3.1 provides a characterization of the distribution of (Y1, Y2) in terms of\nthe joint Laplace transform while leaving the distributions of X1 and X2 unspecified. It is\neasy to see that if a1,θ1 and a2,θ2 are rational in θ1 and θ2 respectively, then LY1,Y2(θ1, θ2)\nis rational in θ1 and θ2. Since univariate rational Laplace transform characterizes matrix-\nexponential distributions (see e.g. Chapter 4 in Bladt and Nielsen (2017)) while multivari-\nate rational Laplace transform characterizes the class of MME distributions proposed by\nBladt and Nielsen (2010), we have the following corollary as an immediate consequence.\n16\n\n\nCorollary 3.2. If X1 and X2 follow independent matrix-exponential distributions, then the\nbivariate random vector (Y1, Y2) defined in (3.1) belongs to the class of MME distributions\nintroduced in Bladt and Nielsen (2010).\nIn the following subsection, we establish a related result by showing that if (X1, X2) be-\nlongs to the subclass of multivariate phase-type distributions (MPH∗) proposed in Kulkarni\n(1989) then so does (Y1, Y2). The main advantage of MPH∗distributions is that, unlike\nthe MME class, they possess a simple probabilistic interpretation.\n3.2\nDependent summands\nHere we assume that (X1, X2) follows a bivariate version of Kulkarni’s MPH∗class which we\nbriefly describe next. Such an MPH∗distribution is constructed using a single terminating\ncontinuous-time Markov jump process J = {J(t)}t≥0 with finite state space C and a system\nof state-dependent rewards. Specifically, let (π, S, R) be the parameters, where:\n• π is the initial probability vector;\n• S is the subintensity matrix governing the continuous-time transitions between tran-\nsient states; and\n• R = {rkj} is the reward matrix, where rkj represents the instantaneous non-negative\nreward associated to the j-th state in C and k-th coordinate for k ∈{1, 2}.\nWith these elements, J evolves according to (π, S) and the coordinates X1 and X2 are\ndefined via\nXk :=\nZ δ\n0\nrk,J(s) ds,\nk ∈{1, 2},\nwhere δ := inf{t ≥0 : J(t) /∈C} is the termination time of J. The random vector (X1, X2)\nobtained through this construction is said to follow an MPH∗(π, S, R) distribution.\nThis construction allows for capturing dependencies between X1 and X2 through the\nshared underlying Markov process J.\nApart from including independence as a special\ncase, the class of MPH∗distributions is known to be dense within the class of bivariate\ndistributions with support on the positive quadrant. Unfortunately, the MPH∗class is\nnot particularly tractable in the general case as the joint distribution function cannot be\nexpressed in closed form. However, its joint Laplace transform is known to be\nE\n\u0010\ne−θ1X1−θ2X2\u0011\n= π (∆θ1,θ2,R −S)−1 (−S1),\nfor θ1, θ2 ≥0, where\n∆θ1,θ2,R = diag{θ1r1j + θ2r2j : j ∈C}.\n17\n\n\nOur aim is to prove that, under this setup, the bivariate random vector (Y1, Y2) defined\nin (3.1) with each (X1,ℓ, X2,ℓ) distributed as the generic bivariate random vector (X1, X2)\nfollows an MPH∗distribution as well.\nProposition 3.3. Suppose that each bivariate random vector in the sequence {(X1,ℓ, X2,ℓ)}ℓ∈N+\nfollows an MPH∗distribution with parameters (π, S, R). Then the pair (Y1, Y2) defined in\n(3.1) follows an MPH∗distribution with an underlying continuous-time Markov jump pro-\ncess on the state-space\n\u0000E ∪(S × S) ∪({∂1} × S) ∪(S × {∂2})\n\u0001\n× C, initial probability vector\n(α, 0, 0, 0) ⊗π, subintensity matrix\n\n\n\n\nI\n0\n0\n0\n0\nI\n0\n0\n0\n0\nI\n0\n0\n0\n0\nI\n\n\n\n⊗S +\n\n\n\n\nP\nU ∗\n0\n0\n0\nQ1 ⊗Q2\nq1 ⊗Q2\nQ1 ⊗q2\n0\n0\nQ2\n0\n0\n0\n0\nQ1\n\n\n\n⊗(−S1π),\nand reward matrix\n\u00121⊺⊗r1\n1⊺⊗r1\n0\n1⊺⊗r1\n1⊺⊗r2\n1⊺⊗r2\n1⊺⊗r2\n0\n\u0013\n.\nHere, for k ∈{1, 2}, the quantity rk represents the row vector (rk1, rk2, . . . , rk|C|) such that\nR =\n\u0012r1\nr2\n\u0013\n.\nProof. We follow a probabilistic approach similar to that in Theorem 3.1.28 of Bladt and\nNielsen (2017). The key idea is to construct a continuous-time Markov jump process whose\naccumulated rewards correspond to the random variables Y1 and Y2.\nRecall γ is the discrete-time Markov chain that leads to the MDPH∗representation\n(2.9). That is, γ is a Markov chain on the state space E ∪(S ×S)∪({∂1}×S)∪(S ×{∂2}).\nFor each ℓ∈N+, suppose that Jℓis the process underlying the MPH∗pair (X1,ℓ, X2,ℓ).\nThen, we concatenate independent realizations of Jℓ’s to match the number of steps of γ\nprior to termination. Thus, we construct a process that keeps track of the state of γ while\nsimultaneously evolving within C. After starting γ ×J1 according to the initial distribution\n(α, 0, 0, 0) ⊗π, the constructed process evolves as follows:\n• While in E × C (representing the common-shock phase), γ evolves according to the\ntransition matrix P and Jℓevolves independently according to the subintensity ma-\ntrix S.\nIn this phase, both Y1 and Y2 accumulate rewards, reflecting that both\ncomponents are active.\n• When γ leaves E, it transitions to S × S with transition probabilities given by U ∗,\nand the constructed process moves into (S × S) × C representing the phase after\nthe common shocks. Here, γ evolves according to the Kronecker product Q1 ⊗Q2,\nwhile Jℓcontinues to evolve according to S. Both Y1 and Y2 continue to accumulate\nrewards.\n18\n\n\n• If one of the components reaches absorption (i.e., transitions to ∂1 or ∂2), the pro-\ncess Jℓmoves to {∂1} × S or S × {∂2}, respectively, representing single-coordinate\ncontinuation in ({∂1} × S) × C or (S × {∂2}) × C for the constructed process. In\n({∂1} × S) × C , only Y2 continues to accumulate rewards, while the reward accumu-\nlation for Y1 ceases. Conversely, in (S × {∂2}) × C, only Y1 continues to accumulate\nrewards.\n• The process terminates when both components have reached absorption.\nBy constructing the Markov jump process as described and defining the rewards accord-\ningly, we ensure that the accumulated rewards Y1 and Y2 correspond to the total occupation\ntimes of their respective components before absorption. This setup mirrors the dynamics\nof the original MPH∗distribution for the pair (Y1, Y2), thus completing the proof.\nWith the random vector (Y1, Y2) fully characterized as an MPH∗distribution, the joint\nmoments of the pair follow directly from Theorem 8.1.5 in Bladt and Nielsen (2017).\n4\nEstimation for bivariate CDPH random variables\nWe shall perform parameter estimation given the data {(n(m)\n1\n, n(m)\n2\n)}n\nm=1, where each\n(n(m)\n1\n, n(m)\n2\n) for m = 1, 2, . . . , n is an independent realization from our proposed bivariate\nCDPH distribution. For later use, we denote (M(m)\n1\n, M(m)\n2\n) as the joint process underlying\nthe m-th CDPH random vector with realization (n(m)\n1\n, n(m)\n2\n). We first review the fully\nobserved case, and then proceed to provide maximum likelihood estimation through the\nuse of the EM algorithm.\n4.1\nThe fully observed case\nLet the parameters be collected in Θ := (ααα, P , U, Q1, Q2). The likelihood is given by\nLO(Θ | {(n(m)\n1\n, n(m)\n2\n)}n\nm=1) =\nn\nY\nm=1\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n; Θ),\nwhere we place emphasis on the dependence of the pmf on Θ by writing fτ1,τ2(·, ·; Θ)\ninstead of fτ1,τ2(·, ·) (see Proposition 2.1).\nWhile direct optimization via gradient methods is possible and relatively quick for very\nsmall dimensions, the EM algorithm is more precise and efficient for moderate matrix\ndimensions |E| and |S|. Thus, we introduce the following latent variables:\n1. Ai: the number of times the processes {M(m)\n1\n}n\nm=1 start in state i ∈E.\n2. NA\nij : the total number of jumps of {M(m)\n1\n}n\nm=1 from state i ∈E to j ∈E.\n19\n\n\n3. NT\nij: the total number of jumps of {M(m)\n1\n}n\nm=1 from state i ∈E to j ∈S.\n4. NB,k\nij\n: the total number of jumps of {M(m)\nk\n}n\nm=1 from state i ∈S to j ∈S, where\nk ∈{1, 2}.\n5. NB,k\ni\n: the total number of jumps of {M(m)\nk\n}n\nm=1 from state i ∈S to the absorbing\nstate, where k ∈{1, 2}.\nHaving specified the above sufficient statistics, we are able to reconstruct the paths of each\nof the Markov jump processes. Recall that αi is the i-th element of the row vector α for\ni ∈E as well as the notation P = {pij}i,j∈E, Qk = {qk,ij}i,j∈S and U = {uij}i∈E,j∈S. For\nk ∈{1, 2}, we further define qk,i to be the i-th element of the column vector qk for i ∈S.\nThen, we can write the complete likelihood as\nLC(Θ | {M(m)\n1\n}n\nm=1, {M(m)\n2\n}n\nm=1)\n=\n Y\ni∈E\nαAi\ni\n! \nY\ni,j∈E\np\nNA\nij\nij\n\n\n\nY\ni∈E\nY\nj∈S\nu\nNT\nij\nij\n\n\n\n\n2\nY\nk=1\nY\ni,j∈S\nq\nNB,k\nij\nk,ij\n\n\n 2\nY\nk=1\nY\ni∈S\nq\nNB,k\ni\nk,i\n!\n.\nThe above specification is particularly convenient since it belongs to the exponential dis-\npersion family of distributions, and thus has fully explicit maximum likelihood estimators\n(with a ‘ˆ’ placed on top of the corresponding parameter) given by\nˆαi = Ai\nn ,\nˆpij =\nNA\nij\nP\nℓ∈E NA\niℓ+ P\nℓ∈S NT\niℓ\n,\nˆuij =\nNT\nij\nP\nℓ∈E NA\niℓ+ P\nℓ∈S NT\niℓ\n,\nˆqk,ij =\nNB,k\nij\nP\nℓ∈S NB,k\niℓ\n+ NB,k\ni\n,\nˆqk,i =\nNB,k\ni\nP\nℓ∈S NB,k\niℓ\n+ NB,k\ni\n.\n4.2\nThe EM algorithm\nSince the full-trajectory data is not observed, we employ the EM algorithm to find the\nMaximum Likelihood Estimate iteratively. This implies that at each iteration, the condi-\ntional expectations of the sufficient statistics, given the absorption times, are computed,\ncorresponding to the E-step. Subsequently such conditional expectations are replaced into\nLC instead of the statistics themselves, and so upon maximization, we obtain updated\nparameters (Θ), commonly referred to as the M-step.\nThe conditional expectations required in the E-step are given as follows:\nLemma 4.1. Let ξ := {(N(m)\n1\n, N(m)\n2\n) = (n(m)\n1\n, n(m)\n2\n)}n\nm=1 be the observed data. The con-\nditional expectations required in the E-step are given as follows:\nE[Ai | ξ] =\nn\nX\nm=1\nαifτ1,τ2|M1,M2(n(m)\n1\n, n(m)\n2\n|i)\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n)\n.\n20\n\n\nE[NA\nij | ξ] =\nn\nX\nm=1\nX\nt≥1\n(αααP t−1)i pijfτ1,τ2|M1,M2(n(m)\n1\n−t, n(m)\n2\n−t|j)\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n)\n.\nE[NT\nij | ξ] =\nn\nX\nm=1\nX\nt≥1\n(αααP t−1)i uij(Qn(m)\n1\n−t−1\n1\nqqq1)j(Qn(m)\n2\n−t−1\n2\nqqq2)j\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n)\n.\nE[NB,1\nij\n| ξ] =\nn\nX\nm=1\nX\nt≥2\nt−1\nX\nℓ=1\nX\nr∈S\n(αααP ℓ−1U)r(Qt−ℓ−1\n1\n)ri q1,ij(Qn(m)\n1\n−t−1\n1\nqqq1)j(Qn(m)\n2\n−ℓ−1\n2\nqqq2)r\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n)\n.\nE[NB,2\nij\n| ξ] =\nn\nX\nm=1\nX\nt≥2\nt−1\nX\nℓ=1\nX\nr∈S\n(αααP ℓ−1U)r(Qt−ℓ−1\n2\n)ri q2,ij(Qn(m)\n2\n−t−1\n2\nqqq2)j(Qn(m)\n1\n−ℓ−1\n1\nqqq1)r\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n)\n.\nE[NB,1\ni\n| ξ] =\nn\nX\nm=1\nX\nt≥2\nt−1\nX\nℓ=1\nX\nr∈S\n(αααP ℓ−1U)r(Qt−ℓ−1\n1\n)ri q1,i(Qn(m)\n2\n−ℓ−1\n2\nqqq2)r\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n)\n.\nE[NB,2\ni\n| ξ] =\nn\nX\nm=1\nX\nt≥2\nt−1\nX\nℓ=1\nX\nr∈S\n(αααP ℓ−1U)r(Qt−ℓ−1\n2\n)ri q2,i(Qn(m)\n1\n−ℓ−1\n1\nqqq1)r\nfτ1,τ2(n(m)\n1\n, n(m)\n2\n)\n.\nHere fτ1,τ2 and fτ1,τ2|M1,M2 are given by (2.1) and (2.2) with the help of (2.3) and (2.4).\nProof. The key in all expressions is to apply the disintegration formula by conditioning on\nthe common shock. All the remaining calculations then follow along similar lines as in the\nunivariate case, since the paths evolve independently thereafter.\n5\nSimulations and real data analysis\nIn this section, we investigate the flexibility and applicability of our proposed CDPH model\non count data. Specifically, we conduct simulations involving bivariate Poisson and Poisson-\nLindley distributions, followed by an application to real-world insurance claims data. Our\nfocus is on assessing how well CDPH can capture or mimic these distributions.\n21\n\n\n5.1\nBivariate Poisson distribution with common shocks\nWe begin by demonstrating that our proposed CDPH model can effectively fit a bivariate\nPoisson distribution. Although the Poisson distribution itself is not a special case of the\nDPH distribution, we aim to show that the CDPH model provides a good approximation.\nTo define the bivariate Poisson distribution, we let (N∗\n1 , N∗\n2 ) be a bivariate Poisson dis-\ntributed pair such that N∗\n1 := V1+Z and N∗\n2 := V2+Z, where V1, V2, and Z are independent\nPoisson random variables with parameters λV1, λV2, and λZ, respectively. Under this con-\nstruction, the marginal distributions are again Poisson with intensities λN∗\n1 := λV1 + λZ\nand λN∗\n2 := λV2 + λZ. The joint pmf of (N∗\n1 , N∗\n2 ) is provided in Equation (2) of Holgate\n(1964).\nTo evaluate the effectiveness of our model, we simulate data from (N∗\n1 , N∗\n2 ) and then fit\n(N∗\n1 +2, N∗\n2 +2) using our CDPH approach1. An interesting aspect of the study is to observe\nhow the proposed model performs as the value of the common shock rate λZ increases, while\nkeeping λN∗\n1 and λN∗\n2 fixed. Specifically, we fix λN∗\n1 = 5 and λN∗\n2 = 4 and consider three\ndifferent values of λZ, namely 1, 2, and 3. The results are presented in Figures 5.1, 5.2, and\n5.3. For each value of λZ, we explore three different model configurations with dimensions\n(|E|, |S|) = (2, 1), (|E|, |S|) = (3, 2), and (|E|, |S|) = (4, 3). Since matrix representations\nare not unique and hence not identifiable, the use of Akaike Information Criterion (AIC)\nand Bayesian Information Criterion (BIC) is not appropriate in this context. Instead, we\ntrack the evolution of the likelihood as a function of the EM iteration step to assess model\nperformance.\nThe simulations suggest that as the common shock component becomes more pro-\nnounced, our model captures it more effectively.\nNevertheless, in all cases, the model\nprovides a satisfactory fit, which is quantified through heatmap plots, showing the best\nfitted pmf among the three considered models (which is the most complex one with dimen-\nsions (|E|, |S|) = (4, 3)), as well as the absolute difference between the empirical pmf and\nthe best fitted one.\n5.2\nBivariate Poisson-Lindley distribution\nIn addition to the bivariate Poisson case, we now investigate the ability of our model to\ncapture overdispersion in claim counts, where the variance is not necessarily equal to the\nmean. The Poisson-Lindley is a mixture distribution that serves as a natural candidate\nfor this setting. Specifically, the bivariate Poisson-Lindley distribution is constructed by\nconsidering conditionally independent Poisson random variables, where the Poisson param-\neter is mixed over a Lindley distribution that depends on a parameter θ (see Definition\n2 in G´omez-D´eniz et al. (2012)). The joint pmf in the bivariate case is also provided in\nEquation (17) of G´omez-D´eniz et al. (2012).\n1In this paper, the initial parameters for estimation are randomly generated (independent uniform\nrandom variables), suitably transformed to comply with the matrix constraints.\n22\n\n\nFigure 5.1: Bivariate Poisson distribution with common shock intensity λZ = 1. In the top\npanels we show the empirical pmf (black circles) versus the fitted pmf (by increasing matrix\ndimension: red triangles, blue crosses, green diamonds), for the distribution of the common\nshock (left), and the two marginals (center and right). In the middle panel we have two\nheatmaps, corresponding to the best fitted bivariate pmf (left), and the absolute difference\nbetween the best fitted and empirical bivariate pmf (right). The bottom panel shows the\nevolution of the likelihood for the three models as the number of EM steps increases to\n500.\n23\n\n\nFigure 5.2: Bivariate Poisson distribution with common shock intensity λZ = 2. In the top\npanels we show the empirical pmf (black circles) versus the fitted pmf (by increasing matrix\ndimension: red triangles, blue crosses, green diamonds), for the distribution of the common\nshock (left), and the two marginals (center and right). In the middle panel we have two\nheatmaps, corresponding to the best fitted bivariate pmf (left), and the absolute difference\nbetween the best fitted and empirical bivariate pmf (right). The bottom panel shows the\nevolution of the likelihood for the three models as the number of EM steps increases to\n500.\n24\n\n\nFigure 5.3: Bivariate Poisson distribution with common shock intensity λZ = 3. In the top\npanels we show the empirical pmf (black circles) versus the fitted pmf (by increasing matrix\ndimension: red triangles, blue crosses, green diamonds), for the distribution of the common\nshock (left), and the two marginals (center and right). In the middle panel we have two\nheatmaps, corresponding to the best fitted bivariate pmf (left), and the absolute difference\nbetween the best fitted and empirical bivariate pmf (right). The bottom panel shows the\nevolution of the likelihood for the three models as the number of EM steps increases to\n500.\n25\n\n\nTo examine the performance of our model, we simulate (N∗\n1 , N∗\n2 ) from this distribution\nand apply our CDPH model to the transformed data (N∗\n1 +2, N∗\n2 +2). Concretely, we sim-\nulate a Lindley random variable L with θ = 2, and subsequently construct N∗\n1 and N∗\n2 as\nindependent Poisson with parameters 2L and 3L, respectively. This is then repeated 10, 000\ntimes. We again consider three model configurations with dimensions (|E|, |S|) = (2, 1),\n(|E|, |S|) = (3, 2), and (|E|, |S|) = (4, 3). The evolution of the likelihood across EM it-\nerations is tracked, as in the previous study, to assess convergence and fitting accuracy.\nThe results are provided in Figure 5.4, and they further illustrate the adaptability of our\nCDPH approach in modeling overdispersed count data. Unlike the bivariate Poisson set-\nting, the Poisson-Lindley mixture does not inherently possess a common shock component.\nRemarkably, however, our model is still able to effectively capture and mimic dependencies\nbetween the two components, demonstrating its practical modeling flexibility.\n5.3\nApplication to insurance data\nWe now apply our model to real-world insurance data, previously analyzed in Vernic (2000).\nThe dataset can be found in Table 1 of Vernic (2000), and was originally modeled using\nthe bivariate generalized Poisson distribution (BGPD). It consists of claim frequencies from\ntwo related insurance categories, with a total of 708 claims.\nWe fit this dataset to our proposed CDPH distribution, applying the same shift trans-\nformation as before. The plots in Figure 5.5 illustrate the results for the same three different\nmodel configurations: (|E|, |S|) = (2, 1), (|E|, |S|) = (3, 2), and (|E|, |S|) = (4, 3). As with\nthe previous cases, we track the likelihood evolution over EM iterations and evaluate the fit\nusing probability plots. The CDPH model in general provides a good fit to the insurance\ndataset, even in the case of low dimensions. It is interesting to note that a non-negligible\ncommon shock component is estimated quite robustly by the three models, which by the\nprevious simulation studies suggests that there could either be a true underlying common\nshock, or that the bivariate distribution is well approximated by the CDPH class. Despite\nthe structural differences between BGPD and CDPH, our approach is also able to capture\ndependencies effectively.\n6\nConclusion\nIn this paper, we have introduced a novel class of common-shock bivariate discrete phase-\ntype distributions, abbreviated as CDPH, to model dependencies in risk models induced\nby common shocks. The key idea is that the two random variables (τ1, τ2) of the CDPH\ndistribution are defined as the termination times of two Markov chains that first evolve\njointly (resembling common shocks influenced by shared risk factors) and then proceed\nindependently (following individual-specific dynamics).\nIn addition to deriving explicit\nand tractable formulas for the joint pmf and the joint pgf, we have also established clear\nrelationships between our CDPH class and other classes of bivariate DPH distributions\n26\n\n\nFigure 5.4: Poisson-Lindley distribution with parameter θ = 2. In the top panels we show\nthe empirical pmf (black circles) versus the fitted pmf (by increasing matrix dimension:\nred triangles, blue crosses, green diamonds) for the two marginals (center and right), and\nthe fitted values for a suggested common shock (left). In the middle panel we have two\nheatmaps, corresponding to the best fitted bivariate pmf (left), and the absolute difference\nbetween the best fitted and empirical bivariate pmf (right). The bottom panel shows the\nevolution of the likelihood for the three models as the number of EM steps increases to\n500.\n27\n\n\nFigure 5.5: Bivariate claim frequency insurance data. In the top panels we show the fitted\npmf (by increasing matrix dimension: red triangles, blue crosses, green diamonds), for\nthe distribution of the suggested common shock (left), and the two marginals (center and\nright). In the middle panel we have two heatmaps, corresponding to the best fitted bivariate\npmf (left), and the absolute difference between the best fitted and empirical bivariate pmf\n(right). The bottom panel shows the evolution of the likelihood for the three models as\nthe number of EM steps increases to 500.\n28\n\n\nproposed by Bladt and Yslas (2023b) and Navarro (2019). We have proved a number of\nclosure properties of the CDPH class as well. In particular, min{τ1, τ2}, max{τ1, τ2} and\nτ1 + τ2 are shown to be DPH whereas mixtures and sums of independent and identically\ndistributed copies of (τ1, τ2) belong to the CDPH class. Further properties of compound\nsums with CDPH claim counts are also discussed, and an EM algorithm for the CDPH\nclass is developed for parameter estimation and subsequently implemented in our numerical\nstudies.\nOur proposed class of bivariate CDPH distributions opens up new research directions\nfor modeling dependencies in multivariate risk modeling, particularly in situations where\ncommon shocks play a significant role. Future research could explore extensions to higher\ndimensions, incorporate covariate information, or consider the tail behavior through the\nintroduction of inhomogeneity functions. We leave these as open questions.\nAcknowledgement.\nMB would like to acknowledge financial support from the Swiss\nNational Science Foundation Project 200021 191984, as well as from the Carlsberg Foun-\ndation, grant CF23-1096.\nEC and JKW acknowledge the support from the Australian\nResearch Council’s Discovery Project DP200100615.\n29\n\n\nReferences\nAhn, S., Kim, J. H. T., and Ramaswami, V. (2012). A new class of models for heavy tailed\ndistributions in finance and insurance risk. Insurance: Mathematics and Economics,\n51(1):43–52.\nAlbrecher, H., Bladt, M., and M¨uller, A. J. (2023). Joint lifetime modeling with matrix\ndistributions. Dependence Modeling, 11(1):20220153.\nAlbrecher, H., Cheung, E. C. K., Liu, H., and Woo, J.-K. (2022). A bivariate Laguerre ex-\npansions approach for joint ruin probabilities in a two-dimensional insurance risk process.\nInsurance: Mathematics and Economics, 103:96–118.\nAsmussen, S. and Albrecher, H. (2010). Ruin Probabilities (2nd edition). World Scientific.\nAssaf, D., Langberg, N. A., Savits, T. H., and Shaked, M. (1984). Multivariate phase-type\ndistributions. Operations Research, 32(3):688–702.\nBadescu, A. L. and Landriault, D. (2009).\nApplications of fluid flow matrix analytic\nmethods in ruin theory - a review. RACSAM - Revista de la Real Academia de Ciencias\nExactas, Fisicas y Naturales. Serie A. Matematicas, 103(2):353–372.\nBadila, E. S., Boxma, O. J., and Resing, J. A. C. (2015). Two parallel insurance lines with\nsimultaneous arrivals and risks correlated with inter-arrival times. Insurance: Mathe-\nmatics and Economics, 61:48–61.\nBladt, M. (2023). A tractable class of multivariate phase-type distributions for loss mod-\neling. North American Actuarial Journal, 27(4):710–730.\nBladt, M. and Nielsen, B. F. (2010).\nMultivariate matrix-exponential distributions.\nStochastic Models, 26(1):1–26.\nBladt, M. and Nielsen, B. F. (2017). Matrix-Exponential Distributions in Applied Proba-\nbility. Springer.\nBladt, M. and Yslas, J. (2023a). Phase-type mixture-of-experts regression for loss severities.\nScandinavian Actuarial Journal, 2023(4):303–329.\nBladt, M. and Yslas, J. (2023b). Robust claim frequency modeling through phase-type\nmixture-of-experts regression. Insurance: Mathematics and Economics, 111:1–22.\nBolanc´e, C. and Vernic, R. (2019).\nMultivariate count data generalized linear models:\nThree approaches based on the Sarmanov distribution. Insurance: Mathematics and\nEconomics, 85:89–103.\n30\n\n\nCheung, E. C. K. and Landriault, D. (2010). A generalized penalty function with the\nmaximum surplus prior to ruin in a MAP risk model.\nInsurance: Mathematics and\nEconomics, 46(1):127–134.\nCheung, E. C. K., Peralta, O., and Woo, J.-K. (2022). Multivariate matrix-exponential\naffine mixtures and their applications in risk theory. Insurance: Mathematics and Eco-\nnomics, 106:364–389.\nCossette, H., Mailhot, M., Marceau, E., and Mesfioui, M. (2016).\nVector-valued Tail\nValue-at-Risk and capital allocation. Methodology and Computing in Applied Probability,\n18(3):653–674.\nDrekic, S., Dickson, D. C. M., Stanford, D. A., and Willmot, G. E. (2004).\nOn the\ndistribution of the deficit at ruin when claims are phase-type. Scandinavian Actuarial\nJournal, 2004(2):105–120.\nEisele, K. T. (2006). Recursions for compound phase distributions. Insurance: Mathematics\nand Economics, 38(1):149–156.\nFackrell, M. (2009). Modelling healthcare systems with phase-type distributions. Health\nCare Management Science, 12(1):11–26.\nFrees, E. W. and Valdez, E. A. (1998). Understanding relationships using copulas. North\nAmerican Actuarial Journal, 2(1):1–25.\nFrostig, E., Pitts, S. M., and Politis, K. (2012). The time to ruin and the number of claims\nuntil ruin for phase-type claims. Insurance: Mathematics and Economics, 51(1):19–25.\nFung, T. C., Badescu, A. L., and Lin, X. S. (2019). A class of mixture of experts models\nfor general insurance: Application to correlated claim frequencies.\nASTIN Bulletin,\n49(3):647–688.\nG´omez-D´eniz, P., Sarabia, J. M., and Balakrishnan, N. (2012). A multivariate discrete\nPoisson-Lindley distribution: Extensions and actuarial applications. ASTIN Bulletin,\n42(2):655–678.\nHassan Zadeh, A. and Stanford, D. (2016). Bayesian and B¨uhlmann credibility for phase-\ntype distributions with a univariate risk parameter. Scandinavian Actuarial Journal,\n2016(4):338–355.\nHe, Q.-M. and Ren, J. (2016a). Analysis of a multivariate claim process. Methodology and\nComputing in Applied Probability, 18:257–273.\nHe, Q.-M. and Ren, J. (2016b). Parameter estimation of discrete multivariate phase-type\ndistributions. Methodology and Computing in Applied Probability, 18(3):629–651.\n31\n\n\nHolgate, P. (1964).\nEstimation for the bivariate Poisson distribution.\nBiometrika,\n51(1/2):241–245.\nJensen, A. (1954). A Distribution Model Applicable to Economics. Thesis Dissertation,\nMunksgaard, Copenhagen.\nKulkarni, V. G. (1989). A new class of multivariate phase type distributions. Operations\nResearch, 37(1):151–158.\nLandsman, Z., Makov, U., and Shushi, T. (2016). Multivariate tail conditional expectation\nfor elliptical distributions. Insurance: Mathematics and Economics, 70:216–223.\nLee, S. and Lin, X. S. (2012). Modeling dependent risks with multivariate Erlang mixtures.\nASTIN Bulletin, 42(1):153–180.\nLin, X. S. and Liu, X. (2007). Markov aging process and phase-type law of mortality. North\nAmerican Actuarial Journal, 11(4):92–109.\nNavarro, A. C. (2019). Order Statistics and Multivariate Discrete Phase-Type Distributions.\nThesis Dissertation, Technical University of Denmark.\nPechon, F., Trufin, J., and Denuit, M. (2018). Multivariate modelling of household claim\nfrequencies in motor third-party liability insurance. ASTIN Bulletin, 48(3):969 –993.\nRen, J. (2010). Recursive formulas for compound phase distributions–univariate and bi-\nvariate cases. ASTIN Bulletin, 40(2):615–629.\nShi, P. and Valdez, E. A. (2014). Multivariate negative binomial models for insurance claim\ncounts. Insurance: Mathematics and Economics, 55:18–29.\nVernic, R. (2000). A multivariate generalization of the generalized Poisson distribution.\nASTIN Bulletin, 30(1):57–67.\nWang, Y. F., Garrido, J., and L´eveill´e, G. (2018). The distribution of discounted compound\nPH-renewal processes. Methodology and Computing in Applied Probability, 20(1):69–96.\nWillmot, G. E. and Woo, J.-K. (2015). On some properties of a class of multivariate Erlang\nmixtures with insurance applications. ASTIN Bulletin, 45(1):151–173.\nWu, X. and Li, S. (2010). Matrix-form recursions for a family of compound distributions.\nASTIN Bulletin, 40(1):351–368.\n32\n\n\nMartin Bladt\nDepartment of Mathematical Sciences, University of Copenhagen, Copenhagen, Denmark\nEmail address: martinbladt@math.ku.dk\nEric C. K. Cheung\nSchool of Risk and Actuarial Studies, UNSW Business School, University of New South\nWales, Sydney, Australia\nEmail address: eric.cheung@unsw.edu.au\nOscar Peralta\nDepartment of Actuarial and Insurance Sciences, Autonomous Technological Institute of\nM´exico, M´exico City, M´exico\nEmail address: oscar.peralta@itam.mx\nJae-Kyung Woo\nSchool of Risk and Actuarial Studies, UNSW Business School, University of New South\nWales, Sydney, Australia\nEmail address: j.k.woo@unsw.edu.au\n33\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21172v1.pdf",
    "total_pages": 33,
    "title": "Modeling discrete common-shock risks through matrix distributions",
    "authors": [
      "Martin Bladt",
      "Eric C. K. Cheung",
      "Oscar Peralta",
      "Jae-Kyung Woo"
    ],
    "abstract": "We introduce a novel class of bivariate common-shock discrete phase-type\n(CDPH) distributions to describe dependencies in loss modeling, with an\nemphasis on those induced by common shocks. By constructing two jointly\nevolving terminating Markov chains that share a common evolution up to a random\ntime corresponding to the common shock component, and then proceed\nindependently, we capture the essential features of risk events influenced by\nshared and individual-specific factors. We derive explicit expressions for the\njoint distribution of the termination times and prove various class and\ndistributional properties, facilitating tractable analysis of the risks.\nExtending this framework, we model random sums where aggregate claims are sums\nof continuous phase-type random variables with counts determined by these\ntermination times, and show that their joint distribution belongs to the\nmultivariate phase-type or matrix-exponential class. We develop estimation\nprocedures for the CDPH distributions using the expectation-maximization\nalgorithm and demonstrate the applicability of our models through simulation\nstudies and an application to bivariate insurance claim frequency data.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}