{
  "id": "arxiv_2502.21202v1",
  "text": "1\nAn Adaptive Multiparameter Penalty Selection\nMethod for Multiconstraint and Multiblock ADMM\nLuke Lozenski, Graduate Student Member, IEEE, Michael T. McCann, Member, IEEE and\nBrendt Wohlberg, Fellow, IEEE\nAbstract—This work presents a new method for online selec-\ntion of multiple penalty parameters for the alternating direction\nmethod of multipliers (ADMM) algorithm applied to optimization\nproblems with multiple constraints or functionals with block ma-\ntrix components. ADMM is widely used for solving constrained\noptimization problems in a variety of fields, including signal\nand image processing. Implementations of ADMM often utilize\na single hyperparameter, referred to as the penalty parameter,\nwhich needs to be tuned to control the rate of convergence.\nHowever, in problems with multiple constraints, ADMM may\ndemonstrate slow convergence regardless of penalty parameter\nselection due to scale differences between constraints. Accounting\nfor scale differences between constraints to improve convergence\nin these cases requires introducing a penalty parameter for\neach constraint. The proposed method is able to adaptively\naccount for differences in scale between constraints, providing\nrobustness with respect to problem transformations and initial\nselection of penalty parameters. It is also simple to understand\nand implement. Our numerical experiments demonstrate that the\nproposed method performs favorably compared to a variety of\nexisting penalty parameter selection methods.\nIndex Terms—convex optimization, ADMM, adaptive ADMM,\nmultiparameter ADMM, parameter selection\nI. INTRODUCTION\nThe alternating direction method of multipliers (ADMM)\nis a proximal splitting algorithm [1] for solving constrained\noptimization problems [2], [3]. This work focuses on an\nADMM variant for solving optimization problems with mul-\ntiple constraints, of the form\nargmin\nx,z\nf(x) + g(z)\ns.t.\nAjx + Bjz = cj\nj = 1, . . . , J,\n(1)\nwith variables x ∈RM, z ∈RN, constraint vectors cj ∈RPj,\nconstraint matrices Aj ∈RPj×M, Bj ∈RPj×N, and convex\nobjective functions f : RM →R, g : RN →R.\nLuke Lozenski is with the Oden Institute for Computational Engineering\nand Sciences, The University of Texas at Austin, Austin, TX 78712, USA and\nthe Department of Electrical and Systems Engineering, Washington University\nin St. Louis, St. Louis, MO 63130, USA. This work began when L. Lozenski\nwas an intern with the Theoretical Division, Los Alamos National Laboratory,\nLos Alamos, NM 87545\nMichael McCann is with the Theoretical Division, Los Alamos National\nLaboratory, Los Alamos, NM 87545\nBrendt Wohlberg is with the Computer, Computational, and Statistical\nSciences Division, Los Alamos National Laboratory, Los Alamos, NM 87545\nFurther author information: (Send correspondence to Michael T. McCann.)\nEmail: mccann@lanl.gov.\nThis research was supported by the Center for Nonlinear Studies, and\nthe Laboratory Directed Research and Development program of Los Alamos\nNational Laboratory under project number 20230771DI. LL acknowledges\nsupport from the Imaging Science Pathway fellowship funded by the NIH\nunder grant T32 EB014855.\nThe multiparameter ADMM iterates for multiple constraints\nare expressed\nx(k+1)\n= argmin\nx\nf(x)\n+\nJ\nX\nj=1\nρj\n2\n\r\r\r\r\rAjx+Bjz(k)−cj +\ny(k)\nj\nρj\n\r\r\r\r\r\n2\n(2)\nz(k+1)\n= argmin\nz\ng(z)\n+\nJ\nX\nj=1\nρj\n2\n\r\r\r\r\rAjx(k+1)+Bjz−cj +\ny(k)\nj\nρj\n\r\r\r\r\r\n2\n(3)\ny(k+1)\nj\n= y(k)\nj\n+ ρj\n\u0000Ajx(k+1) + Bjz(k+1) −cj\n\u0001\n,\n(4)\nwhere {ρj}J\nj=1 ⊂R is a set of positive scalars penalty\nparameters, ∥· ∥denotes the ℓ2 norm, and yj ∈RPj is\nknown as the j-th dual variable or the j-th Lagrange multiplier\nassociated with the j-th constraint in (1). To simplify notation,\nwe let argmin f denote a single minimizer of f, even when\nf does not have a unique minimizer.\nNote that grouping all constraints into a single constraint,\nby vertical matrix concatenation, and utilizing a single penalty\nparameter recovers standard ADMM, which is known to\nconverge under a wide variety of conditions [4, §3.2], [5],\n[6], [7] 1. The iteration (2)-(4) can be shown to converge to\na solution of (1) based on the equivalence of multiparameter\nADMM and standard ADMM outlined in Appendix A.\nFor ease of notation, the dual variable can be expressed in\na stacked vector form\ny(k+1) = y(k) + Dρ(Ax(k+1) + Bz(k+1) −c),\nwhere y(k)\n=\n\u0010\n(y(k)\n1 )T\n. . .\n(y(k)\nJ )T\n\u0011T\n∈RP is the\nvectorized stack of multipliers.\nB =\n\n\n\nB1\n...\nBJ\n\n\n,\nA =\n\n\n\nA1\n...\nAJ\n\n\n,\nand\nc =\n\n\n\nc1\n...\ncJ\n\n\n\nare the grouped constraint matrices, ρ =\n\u0000ρ1\n. . .\nρJ\n\u0001T ∈\nRJ is the vectorized stack of penalty parameters, and\nDρ = D(ρ) = D(ρ1, . . . , ρJ)\n= diag(\nρ1\n|{z}\nP1-times\n, . . . ,\nρj\n|{z}\nPj-times\n, . . . ,\nρJ\n|{z}\nPJ-times\n) ∈RP ×P\n(5)\n1While this formulation uses the Euclidean ℓ2 norm, ADMM can be\ngeneralized to a wide variety of Hilbert spaces [8], [9], [10].\narXiv:2502.21202v1  [eess.IV]  28 Feb 2025\n\n\n2\nis a diagonal matrix operator.\nA very important subclass of multiconstraint optimization\nproblems, as in (1), is multiblock optimization problems,\nwhich involve a separable objective of several variables with\none of these variables being a consensus variable. This multi-\nblock problem then has several constraints with each constraint\ninvolving the consensus variable and one other variable2. A\nmultiblock problem is formulated as\nargmin\nx,z1,...,zJ\nf(x) +\nJ\nX\nj=1\ngj(zj)\ns.t.\nAjx + ˜\nBjzj = cj\nj = 1, . . . , J,\n(6)\nwith variables x ∈RM, zj ∈RNj; vector cj ∈RPj; matrices\nAj ∈RPj×M and\n˜\nBj ∈RPj×Nj; convex functionals f :\nRM →R and gj : RNj →R.\nDefining z ∈RN for N = PJ\nj=1 Nj as\nz =\n\n\n\nz1\n...\nzJ\n\n\n, g(z) =\nJ\nX\nj=1\ngj(zj),\nand Bj ∈RPj×N is the matrix such that Bjz =\n˜\nBjzj,\nthen the multiblock form in (6) reduces to a form identical to\none in (1). These multiblock optimization problems naturally\nemerge in several computational imaging applications, such as\ncases with multiple regularization and data fidelity terms [13],\n[14] or applying separation of variables for proximal based\noptimization [15], [16].\nIn the multiblock case, the ADMM constraints defined in\n(7)-(9) can equivalently be defined as\nx(k+1)\n= argmin\nx\nf(x)\n+\nJ\nX\nj=1\nρj\n2\n\r\r\r\r\rAjx+ ˜\nBjz(k)\nj\n−cj +\ny(k)\nj\nρj\n\r\r\r\r\r\n2\n(7)\nz(k+1)\nj\n= argmin\nzj\ngj(zj)\n+ρj\n2\n\r\r\r\rAjx(k+1)+ ˜\nBjzj−cj +\ny(k)\nj\nρj\n\r\r\r\r\n2\n(8)\ny(k+1)\nj\n= y(k)\nj\n+ ρj\n\u0000Ajx(k+1) + ˜\nBjz(k+1)\nj\n−cj\n\u0001\n.\n(9)\nAn advantage of this multiblock formulation of ADMM is\nthat the updates of each zj variable can be performed inde-\npendently of each other.\nA further subset of multiblock ADMM of note is consensus\nADMM in which cj = 0 and Bj = −I, and often Aj = I,\nfor all j. Consensus ADMM can be applied for distributed and\nasynchronous optimization of large-scale functions [17], [18],\n[19], [20], [21], [22].\nIn practice, the rate of convergence of ADMM algorithms is\nstrongly dependent on the choice of penalty parameters [23].\nMoreover, whereas the standard ADMM algorithm requires\nselection of a single penalty parameter, many problems with\nmultiple constraints, such as multiblock problems in Eq.\n2Note that this definition is distinct from those of [11], [12], which refer to\nany ADMM implementation with more than two variables as “multiblock”.\n(6), will require some form of preconditioning applied to\nthe constraint for fast convergence [24]. As demonstrated in\nAppendix A, utilizing a distinct penalty parameter for each\nconstraint is equivalent to applying diagonal preconditioning,\nand can similarly accelerate convergence.\nIn general, there are no analytic methods for determining the\noptimal selection of penalty parameters. Furthermore, brute-\nforce methods that run the ADMM with multiple penalty\nparameters are computationally expensive, scaling exponen-\ntially in the number of penalty parameters, and are therefore\nimpractical for large-scale or time-sensitive problems.\nAlternatively, an ADMM implementation can utilize an\nadaptive penalty parameter selection criterion. In an adaptive\nADMM implementation, the penalty parameter ρ or penalty\nparameters {ρj}J\nj=1 are replaced with iteration dependent\nversions, ρ(k) or {ρ(k)\nj }J\nj=1, via a chosen selection criterion.\nA number of previous works have explored selection criterion\nfor the single parameter cases [25], [26], [27], [28], [29]. This\nwork proposes an extension of the single parameter criterion\nproposed by [29] to an adaptive multiparameter rule. To our\nknowledge, the only other proposed multiparameter rule is\na generalization of the BBS method [27], discussed in [30].\nAdapting notation from [29], an adaptive criterion for multiple\npenalty parameters can be defined as a function\nρ(k+1) = ϕ\n\u0010\u0000ρ(i), x(i+1), z(i+1), y(u+1)\u0001k\ni=0\n\u0011\n,\n(10)\nϕ : RJ ×RM ×RN ×RP ×· · · →RJ that selects new penalty\nparameters based on all current and past penalty parameters,\nall current and past variables, and, implicitly, the problem\ndefinition f, g, {Aj}J\nj=1, {Bj}J\nj=1, and {cj}J\nj=1.\nThe remainder of this paper is structured as follows. In\nSection II, a framework for analyzing multiparameter ADMM\nfor problems with multiple constraints as an affine fixed-point\niteration on the dual variable is formulated. In Section III,\nthis framework is used to derive the proposed multiparameter\nselection method by minimizing the spectral radius of the\naffine iteration matrix. In Section IV, a brief survey of existing\nadaptive single-parameter methods and an additional multipa-\nrameter method is conducted, with these methods serving as\ncomparison methods. Section V identifies an important class\nof problem transformations and characterizes the behavior\nof the proposed method and the comparison methods with\nrespect to these problem transformations. In Section VI, three\nnumerical studies are conducted to demonstrate the benefits of\nthe proposed method and compare it to the reference methods.\nSection VII presents the conclusions drawn from these results\nand possibilities for future extensions.\nII. PENALTY PARAMETER SELECTION FRAMEWORK\nThis section presents a framework for selection of multiple\nADMM penalty parameters. The presented framework is a\nmultiparameter generalization of the single parameter spectral\nradius approximation (SRA) method [29]. The fundamental\nidea of this framework is to formulate ADMM iterations lo-\ncally as an affine fixed-point iteration, y(k+1) = Hρy(k)+hρ,\nwhere Hρ ∈RP ×P and hρ ∈RP are dependent on penalty\nparameter selection. The theory of affine fixed-point iteration\n\n\n3\ndictates that the fastest convergence is achieved when the\nspectral radius of Hρ is minimized. Based on this analysis,\nthe proposed method attempts to minimize the spectral radius\nof Hρ. (While similar concepts underpin other approaches\nfor ADMM parameter selection [27], [28], the resulting algo-\nrithms are different.)\nIn addition to extending the derivation of [29] to the\nmultiparameter case, this work seeks to address one of its\nweakness. Specifically, [29] assumes that the iteration matrix\nHρ has real eigenvalues for every ρ and that y(k+1)−y(k) will\nconverge to a single dominant eigenvector. This assumption is\nexplicitly shown to be false via counter example in Section VI.\nThis work corrects the derivation resulting from this incorrect\nassumption and demonstrates that the original single parameter\nSRA method and proposed multiparameter method roughly\nachieve optimal performance.\nA. Iteration on y\nThis section provides a brief formulation of the ADMM\niterations in Eq. (2)-(4) as an affine fixed-point iteration solely\nin terms of the dual variable or multiplier y. This adapts the\nin-depth derivation in [29] for the multiparameter case, and\nis often expressed in literature as Douglas Rachford splitting\n(DRS) [31] on the dual problem [32], [27], [28].\nApplying first-order optimality conditions in terms of sub-\ngradients on the z-update [33], [4]\n0 ∈∂zg(z(k+1)) = ∂zg(z(k+1)) + BT y(k+1),\nwhere ∂zg(z(k+1)) represents the subgradient of g evaluated\nat z(k+1). This then allows z(k) to be expressed as a function\nof y(k)\nz(k) = G\n\u0000y(k)\u0001\nG(w) = argmin\nz\ng(z) +\n\u0000BT w\n\u0001T z, (11)\nwhere w ∈RP .\nSimilarly, first-order optimality conditions in terms of sub-\ngradients can be applied for the x-updates\n0 ∈∂xf(x(k+1)) = ∂xf(x(k+1)) + AT ˜y(k+1),\nwhere ∂xf(x(k+1) represents the subgradient of f evaluated\nat x(k+1) and\n˜y(k+1) = y(k) + Dρ(Ax(k+1) + Bz(k) −c),\n(12)\nis an introduced synthetic multiplier or intermediate multiplier.\nThen x(k) can be expressed as a function of ˜y(k)\nx(k) = F\n\u0000˜y(k)\u0001\n,\nF(w) = argmin\nx\nf(x)+\n\u0000AT w\n\u0001Tx. (13)\nUsing these definitions of F and G, the ADMM updates can\nbe expressed solely in terms of the implicit multiplier updates.\n(I −DρAF)(˜y(k+1)) = (I + DρBG)(y(k)) −Dρc. (14)\n(I −DρBG)(y(k+1)) = ˜y(k+1) −DρBG(y(k)).\n(15)\nAssuming (I −DρAF) and (I −DρBG) are locally in-\nvertible, the implicit updates in Eq. (14) and Eq. (15) can be\nsummarized as\n˜y(k+1) = (I −DρAF)−1 \u0010\n(I + DρBG)(y(k)) −Dρc\n\u0011\n,\ny(k+1) = (I −DρBG)−1 \u0010\n˜y(k+1) −DρBG(y(k))\n\u0011\n.\nNote that ˜y(k+1) can be written purely in terms of y(k) and\nmerely acts as a placeholder variable, to improve readability.\nB. Affine Fixed-Point Iteration\nThis section considers the case when ADMM is applied to\na quadratic optimization problem. Using the logic applied in\nSection II-A, this then allows ADMM to be expressed as an\naffine fixed-point iteration and is a critical component to derive\nthe proposed penalty parameter selection rule. Furthermore,\nit is assumed that convex functions can be locally well-\napproximated with a series of quadratics [34], [35], [36],\n[37], such as the local approximations utilized in Newton\nmethods, [38], [39], thereby allowing the proposed method\nto be generalized to the broader class of convex optimization\nproblems.\nConsider the case when f and g are of the forms\nf(x) = 1\n2xT Qx + qT x\ng(z) = 1\n2zT Rz + rT z,\nwhere Q is a positive definite M × M matrix, q ∈RM, R is\na positive definite N × N matrix, and r ∈RN.\nThese definitions lead to a fixed point iteration in terms of\nthe multiplier variable\ny(k+1) =\n((I + DρG)−1(I + DρF)−1(I + DρFDρG)\n|\n{z\n}\nHρ\ny(k) + hρ\nwhere F = AQ−1AT , G = BR−1BT , Hρ ∈RP ×P is the\nupdate matrix that is dependent on ρ, and hρ ∈RP is an affine\ncomponent. This fixed point iteration will be convergent when\nthe spectral radius rad(Hρ) < 1.\nSupposing that these iterations converge to a unique fixed\npoint y∗, denote the error term ϵ(k) = y(k) −y∗. In the cases\nwhen Hρ only has real eigenvalues, ϵ(k) will converge to a\ndominant eigenvector of Hρ, as the components correspond-\ning to smaller eigenvalue will rapidly vanish. Therefore, for\nsufficiently large k and ∆k > 0\ny(k+∆k) −y(k) ≈\n\u0000r(Hρ)∆k −1\n\u0001\nϵ(k),\n(16)\nthat is y(k+∆k) −y(k) will also be a maximal eigenvector.\nHowever, in the cases when Hρ has dominant eigenval-\nues that are complex, ϵ(k) will instead converge to a real\ncombination of the dominant eigenvectors vρ and vρ, since\nthe other eigenvector components will vanish rapidly. Let λρ\nbe one of the dominant eigenvalues, vρ its corresponding\neigenvector, and (λρ, vρ) their corresponding conjugate pair.\nThen for some complex coefficient ζ\nϵ(k) ≈ζ\n2vρ + ζ\n2vρ.\nTherefore, for sufficiently large k and ∆k > 0\ny(k+∆k) −y(k) ≈ϵ(k+∆k) −ϵ(k)\n\n\n4\n= ζ\n2(λ∆k\nρ\n−1)vρ + ζ\n2(λ∆k\nρ\n−1)vρ.\nThis means that y(k+∆k) −y(k) may not be a dominant\neigenvalue, but is contained within plane spanned by the real\nand imaginary components of the dominant eigenvector.\nIII. PROPOSED PENALTY PARAMETER SELECTION\nMETHOD\nThis section presents the proposed selection method for\nmultiple adaptive penalty parameter methods. The proposed\nselection rule is derived based on the affine fixed-point it-\neration introduced in the previous section, and attempts to\nminimize the spectral radius of the iteration matrix by avoiding\ntwo limiting cases that are shown not to include the optimal\npenalty parameters.\nLet (λρ, vρ) be a dominant eigenpair of Hρ. Then\n(I + DρG)−1(I + DρF)−1(I + DρFDρG)vρ = λρvρ\n=⇒(I + DρFDρG)vρ = λρ(I + DρF)(I + DρG)vρ.\nThen\n|λρ|2 =\n∥(I + DρFDρG)vρ∥2\n∥(I + DρF)(I + DρG)vρ∥2 .\nA. Dominating Cases\nConsider the two cases when either vρ dominates DρGvρ\nor vice versa. This section demonstrates that the optimal ρ\ndoes not satisfy either of these cases, and then proposes\na simple and robust adaptive selection method for multiple\npenalty parameters by avoiding these two worst cases. This is\ndemonstrated via a bounding argument that removes the need\nto linearize about the maximal eigenvector, as required in [29].\nCase 1: ∥vρ∥≫∥DρGvρ∥. Then\n|λρ|2 ≈\n∥vρ∥2\n∥(I + DρF)vρ∥2 .\nThe eigenvalue norm can then be bounded above and below\nas\n1\n(1 + σmax(DρF))2 ≤|λρ|2 ≤\n1\n(1 + σmin(DρF))2 ,\nwhere σmin(DρF) and σmax(DρF) are the minimum and\nmaximum singular values of DρF. Both of these functions are\nstrictly elementwise decreasing with respect to ρ. This implies\nthat the global min for the eigenvalue norm is not achieved in\nCase 1.\nNote that in Case 1,\n(I + DρFDρG)vρ = λρ(I + DρF)(I + DρG)vρ\n=⇒vρ ≈(I + DρF)λρvρ\nwhich implies that (λρ, vρ) approaches an eigenpair of (I +\nDρF)−1 and the eigenvalue λρ and eigenvector vρ are real.\nCase 2: ∥vρ∥≪∥DρGvρ∥. Then\n|λρ|2 ≈\n∥DρFDρGvρ∥2\n∥(I + DρF)DρGvρ∥2 .\nThe eigenvalue norm can then be bounded above and below\nby\n\u0012\nσmin(FDρG)\n(σmax(G) + σmin(FDρG)\n\u00132\n≤|λρ|2\n≤\n\u0012\nσmax(FDρG)\nσmin(G) + σmax(FDρG)\n\u00132\n.\nBoth of these functions are strictly elementwise increasing\nwith respect to ρ. This implies that the global minimum for\nthe eigenvalue norm is not achieved in Case 2.\nNote that in Case 2,\n(I + DρFDρG)vρ = λρ(I + DρF)(I + DρG)vρ\n=⇒DρFDρGvρ ≈(I + DρF)λρDρGvρ\nwhich implies that (λρ, DρGvρ) approaches an eigenpair of\n(I +DρF)−1DρF and the eigenvalue λρ and DρGvρ are real.\nDρGvρ being real similarly implies vρ is because Dρ and\nGvρ are real.\nB. Proposed Penalty Parameter Selection Method\nThe proposed method is based on avoiding either of the\npreviously mentioned Case 1 and Case 2, that is\n∥vρ∥≫∥DρGvρ∥\nor\n∥vρ∥≪∥DρGvρ∥.\nAs noted, Case 1 and Case 2 both imply that both the\neigenvalues and eigenvectors are real, or dominated by their\nreal component. This means both cases can be described in\nterms of inequalities that only consider real components of\nthe eigenvector\n∥Real(ζvρ)∥≫∥DρG Real(ζvρ)∥\nor\n∥Real(ζvρ)∥≪∥DρG Real(ζvρ)∥\n,\nfor any chosen complex coefficient ζ.\nThis can easily be avoided when the left and right sides are\nset to be equal\n∥Real(ζvρ)∥2 = PJ\nj=1 ∥Real(ζvρ)j∥2\n= ∥DρG Real(ζvρ)∥2 = PJ\nj=1 ρ2\nj∥(G Real(ζvρ))j∥2 ,\nwhere (·)j selects the components of vρ and Gvρ correspond-\ning to yj. This above equality is achieved when\n∥Real(ζvρ)j∥= ρj∥(G Real(ζvρ))j∥\nholds for a chosen ζ.\nChoosing ζ such that y(k+1) −y(k) ≈\nζ\n2vρ + ζ\n2vρ =\nReal(ζvρ), gives rise to the proposed multiparameter spectral\nradius approximation (MpSRA) rule\n\u0010\nρ(k+1)\nj\n\u0011\nMpSRA =\n\r\ry(k+1)\nj\n−y(k)\nj\n\r\r\n\r\rBj(z(k+1) −z(k))\n\r\r.\n(17)\nNote that the resulting adaptive rule for ρ does not explicitly\nrequire that the problem be quadratic, and only requires values\nfor y, B, and z. In practice, it is then possible to apply\nthe proposed method to any ADMM algorithm for solving\narbitrary convex problems.\n\n\n5\nFurthermore, the adaptive penalty parameters proposed in\n(17) can be formulated for the multiblock case as\n\u0010\nρ(k+1)\nj\n\u0011\nSRA =\n\r\ry(k+1)\nj\n−y(k)\nj\n\r\r\n\r\r ˜\nBj(z(k+1)\nj\n−z(k)\nj\n)\n\r\r.\n(18)\nIn this form the proposed penalty parameters are defined in-\ndependently for each multiblock subproblem. For applications\nof consensus ADMM, the proposed multiparameter method\nis easily adapted and creates separate penalty parameters for\neach subproblem and is fully compatible with with distributed\nand asynchronous approaches.\nC. Implementation of Proposed Method\nTwo key points must be addressed for practical implemen-\ntation of the proposed MpSRA rule in Eq. (17).\nFirst, the proposed rule requires that y(k+1) −y(k) approx-\nimates the largest eigenvector of Hρ, or is approximately in\nthe plane determined by the dominant eigenvector pair in the\ncomplex eigenvalue case. This approximation requires that k\nbe sufficiently large. In practice, this can be achieved by only\napplying the proposed rule every T iterations. Based on trial\nand error, T = 5 demonstrates sufficient performance.\nSecond, the rule presented in Eq. (17) does not directly\naccount for the cases when y(k+1)\nj\n= y(k)\nj\nor Bjz(k+1) =\nBjz(k). Standard ADMM requires a finite and positive penalty\nparameter and, via the equivalence of standard ADMM and\nmultiparameter ADMM as demonstrated in Appendix A,\neach element of the multiparameter ρj must also be finite\npositive. In these cases, multiplicative scaling is applied in\na manner similar to residual balancing method [26]. That\nis ∥y(k+1)\nj\n−y(k)\nj\n∥= 0 and ∥Bj(z(k+1) −z(k))∥> 0\nindicates that the ADMM method is weighted too much\ntowards the constraint and a greater emphasis can be put\non the objective by decreasing ρj by a chosen factor τ decr.\nSimilarly, ∥y(k+1)\nj\n−y(k)\nj\n∥> 0 and ∥Bj(z(k+1) −z(k))∥= 0\nindicates that the ADMM method is weighted too much\ntowards the objective and a greater emphasis can be put on\nthe constraint by increasing ρj by a chosen factor τ incr. If\n∥y(k+1)\nj\n−y(k)\nj\n∥= 0 and ∥Bj(z(k+1) −z(k))∥= 0, both\nthe constraint and objective are equally weighted and no\nadjustment to ρj is needed. In practice, τ decr = τ incr = 10\ndemonstrates sufficient performance.\nThis practical implementation of the proposed rule in Eq.\n(17) is described in Algorithm 1.\nIV. EXISTING PENALTY PARAMETER METHODS\nThis section briefly outlines and describes other state-of-\nthe-art adaptive penalty parameter methods proposed in other\nworks.\nA. Single-Parameter Methods\nSeveral works have proposed adaptive single-parameter\nmethods for the single-constraint version of ADMM. We\nfocus here on the same four methods analyzed in [29], which\nrepresent the state-of the-art methods in practice. These single-\nparameter methods are the residual balancing (RB) method,\nAlgorithm 1: Proposed multiparameter spectral radius\napproximation (MpSRA) ρ selection method\nInput: k, ρ(k), z(k), z(k+1), y(k), y(k+1)\nParameters: T = 5, τ incr = τ decr = 10\nOutput: ρ(k+1)\nif k mod T ̸= 0 :\nreturn ρ(k)\nfor j = 1, . . . , J do\npj ←\n\r\ry(k+1)\nj\n−y(k)\nj\n\r\r\nqj ←\n\r\rBj\n\u0000z(k+1) −z(k)\u0001\r\r\nif pj = 0 and qj > 0 :\nρ(k+1)\nj\n←ρ(k)\nj /τ decr\nelse if pj > 0 and qj = 0 :\nρ(k+1)\nj\n←τ incrρ(k)\nelse if pj = 0 and qj = 0 :\nρ(k+1)\nj\n←ρ(k)\nj\nelse\nρ(k+1)\nj\n←pj/qj\nend\nρ(k+1) ←(ρ(k+1)\n1\n, . . . , ρ(k+1)\nJ\n)T\nreturn ρ(k+1)\nproposed in [25] and applied in [40], [41], [42], [43], [44],\n[45], the spectral radius bound (SRB) method [28], the single-\nparameter Barzilai-Borwein spectral (BBS) method [27], [46],\n[47], and the single-parameter spectral radius approximation\n(SRA) method [29].\nB. Multiparameter Barzilai-Borwein Spectral Method\nAlthough many single-parameter methods can sometimes be\nadapted to multiparameter analogs for multiconstraint prob-\nlems, few works have implemented multiparameter versions\nof ADMM. One exception is the multiparameter version of\nthe Barzilai-Borwein spectral (MpBBS) method [30]. The\nmultiparameter BBS method serves as the main point of\ncomparison for the proposed multiparameter method.\nThe multiparameter BBS method [30, Section 5.3.3] is the\nnatural extension of the single-parameter BBS method and is\nderived in a similar manner. The multiparameter BBS method\nis expressed.\n\u0010\nρ(k+1)\nj\n\u0011\nMpBBS =\ns\n\r\r∆˜y(k)\nj\n\r\r2\r\r∆y(k)\nj\n\r\r2\n⟨Aj(∆x(k)),∆˜y(k)\nj\n⟩⟨Bj(∆z(k)),∆y(k)\nj\n⟩.\n(19)\nwhere ∆x(k)\n=\nx(k) −x(k−∆k), ∆z(k)\n=\nz(k) −\nz(k−∆k), ∆y(k)\nj\n= y(k)\nj\n−y(k−∆k)\nj\n, ˜y(k)\nj\nis defined the same\nas in (12), ∆˜y(k)\nj\n= ˜y(k)\nj\n−˜y(∆k)\nj\nand ∆k is some positive\ninteger for delay in iterations. In practice, utilizing the BBS\nmethod requires a variety of additional safeguards based on a\n\n\n6\ncomplicated heuristic assessment of curvature values derived\nin the formulation of the BBS method.\nV. PROBLEM MULTISCALING\nThis section identifies an important type of problem trans-\nformation for optimization problems with multiple constraints.\nThis problem transformation, multiscaling, introduces a family\nof optimization problems whose solutions are related via\na corresponding transformation in the solution space. This\nsection addresses how multiscaling alters the behaviors of\neach adaptive penalty parameter method and introduces the\nconcept of multiscaling covariance, which is a generalization\nof scaling covariance [26], [29], intended to classify a family\nof optimization problems that share constraints at different\nscales.\nConsider using ADMM to solve a member of a family of\noptimization problems\nmin\nx,z\nαf(γx) + αg(δz)\ns.t.\nβjAjγx + βjBjδz = βjcj\nj = 1, . . . , J\n(20)\nwhere the family is parameterized by the scalars α, {βj}J\nj=1, γ,\nand δ. We refer to the problem with α = β1 = . . . = βJ =\nγ = δ = 1 as the unscaled problem. It is important to note\nthat there is no definitive choice of the unscaled problem\nand it merely serves as a reference member of the family of\nproblems.\nDenote x∗, z∗as the solution to the unscaled problem. Then\nthe solution to the problem with scaling α, {βj}J\nj=1, γ, δ is\n¯x∗= x∗\nγ ,\n¯z∗= z∗\nδ ,\n(21)\nand is independent of choice of α or {βj}J\nj=1.\nHowever, note that when multiconstraint ADMM is imple-\nmented for the unscaled problem with a penalty parameter\nρ = (ρ1, . . . , ρJ)T , equivalent behavior in ADMM for the\n(α, {βj}J\nj=1, γ, δ)-scaled problem is demonstrated with the\nscaled penalty parameter ρ = ( α\nβ2\n1 ρ1, . . . , α\nβ2\nJ ρJ)T . This means\nthat although scaling the constraints does not affect the solu-\ntion to the optimization problem, it affects penalty parameter\nselection and thereby convergence behavior of ADMM.\nIdeally, then a multiconstraint ADMM penalty parameter\nmethod should scale properly with scaling applied to the\noptimization problem, motivating the following definition.\nDefinition 5.1 (Multiscaling Covariant): A multiconstraint\nADMM penalty parameter selection method, ϕ, is multiscaling\ncovariant if\nϕ′\n \u0012\nαD−2\nβ ρ(ℓ), x(ℓ+1)\nγ\n, z(ℓ+1)\nδ\n, αD−1\nβ y(ℓ+1)\n\u0013k\nℓ=0\n!\n= αD−2\nβ ϕ\n\u0012\u0010\nρ(ℓ), x(ℓ+1), z(ℓ+1), y(ℓ+1)\u0011k\nℓ=0\n\u0013\n,\n(22)\nwhere β = (β1, . . . , βJ)T and D is the diagonal matrix\noperator defined in Eq. (5) and ϕ′ is selection criteria for the\nscaled problem, i.e., where ϕ depends on f, g, A, B, and c,\nϕ′ depends on αf(γ·), αg(δ·), γDβA, δDβB, and Dβc.\nThat is, if a method is multiscaling covariant it will select\nρ(k) at iteration k of the unscaled problem, it will select\nαD−2\nβ ρ(k) for each corresponding scaled problem. Parameter\nselection methods being multiscaling covariant is critical be-\ncause problem scaling is unavoidable in practice. For example,\nin many engineering problems, problem structure is deter-\nmined by the choice of units of measurement, or scale, within\nthe objective and constraints and an effective optimization\nneeds to be independent of this choice.\nNote that the characterized family of problems in Eq. (20)\nand Definition 5.1 can be further generalized by replacing the\nscalars γ and δ with invertible matrices Γ ∈RM×M and\n∆∈RN×N. In practice, members of this generalized family\nof problems can exhibit different behaviors in numerical\nsolutions, potentially in cases when Γ or ∆are ill-conditioned.\nReference [29] provides additional analysis on adaptive meth-\nods being translation invariant. Note that MpSRA inherits\ntranslation invariance from the single parameter SRA.\nA. Single Parameter Selection Methods\nConsider an ADMM single penalty parameter selection\nmethod\nρ(k+1) = ψ\n\u0010\u0000ρ(ℓ), x(ℓ+1), z(ℓ+1), y(ℓ+1)\u0001k\nℓ=0\n\u0011\n.\n(23)\nThis update is defined in terms of a function ψ : R×RM ×\nRN × RP × · · · →R. This can be viewed as an equivalent\nadaptive multiparameter rule that only inputs and outputs ρ(k)\nsuch that ρ(k)\n1\n= . . . = ρ(k)\nJ .\nA single-parameter method can only scale according to\nEq. (22) in the β1 = . . . = βJ case. This limitation means\nthat a single-parameter method may be scaling covariant with\nrespect to scaling by a single scalar but cannot be multiscaling\ncovariant.\nB. Multiparameter BBS\nThe multiparameter BBS method is multiscaling covariant,\nsince for each j\nϕMpBBS′\nj\n \u0012\nαD−2\nβ ρ(ℓ), x(ℓ+1)\nγ\n, z(ℓ+1)\nδ\n, αD−1\nβ y(ℓ+1)\n\u0013k\nℓ=0\n!\n=\nv\nu\nu\nu\nt\n\r\r α\nβj ∆˜y(k)\nj\n\r\r2\r\r α\nβj ∆y(k)\nj\n\r\r2\n⟨βjAjγ( ∆x(k)\nγ\n), α\nβj ∆˜y(k)\nj\n⟩⟨βjBjδ(∆z(k)\nδ ), α\nβj ∆y(k)\nj\n⟩\n.\n= α\nβ2\nj\nv\nu\nu\nt\n\r\r∆˜y(k)\nj\n\r\r2\r\r∆y(k)\nj\n\r\r2\n⟨Aj(∆x(k)), ∆˜y(k)\nj\n⟩⟨Bj(∆z(k)), ∆y(k)\nj\n⟩\n.\n= α\nβ2\nj\nϕMpBBS\nj\n \u0012\nρ(ℓ), x(ℓ+1), z(ℓ+1)\nδ\n, y(ℓ+1)\n\u0013k\nℓ=0\n!\n.\n\n\n7\nC. Multiparameter SRA\nThe proposed MpSRA method is multiscaling covariant\nsince for each j\nϕMpSRA′\nj\n \u0012\nαD−2\nβ ρ(ℓ), x(ℓ+1)\nγ\n, z(ℓ+1)\nδ\n, αD−1\nβ y(ℓ+1)\n\u0013k\nℓ=0\n!\n=\nα\n|βj|∥y(k+1)\nj\n−y(k)\nj\n∥\n|βj|∥Bjδ( z(k+1)\nδ\n−z(k)\nδ )∥\n= α\nβ2\nj\n∥y(k+1)\nj\n−y(k)\nj\n∥\n∥B(z(k+1) −z(k))∥\n= α\nβ2\nj\nϕMpSRA\nj\n \u0012\nρ(ℓ), x(ℓ+1), z(ℓ+1)\nδ\n, y(ℓ+1)\n\u0013k\nℓ=0\n!\n.\nVI. EXPERIMENTS AND RESULTS\nWe performed three numerical experiments to demonstrate\nthe benefits of the proposed multiparameter method and\ncompare it to other adaptive penalty parameter approaches.\nIn each of these experiments, the proposed and comparison\nmethods were applied to a different optimization. The first\nexperiment applied these methods for solving a constrained\nsum of quadratics optimization problem that resulted in an\niteration matrix with complex eigenvalues for some penalty\nparameter selections and assessed our analysis of the itera-\ntion matrix and eigenvalue behavior. The second experiment\napplied these methods for solving a constrained sum of\nquadratics optimization problem with multiple scales between\nconstraints and assessed the ability of the proposed method\nto adjust to scales between constraints. The third experiment\napplied these methods to solving a multiblock formulation of\nan image reconstruction problem with ℓ1 data fidelity and TV\nregularization and assessed each method for a computationally\nexpensive, non-quadratic problem.\nA. Sum of Quadratics Resulting in an Iteration Matrix with\nComplex Eigenvalues\nConsider the constrained optimization problem\nargmin\nx,z\n1\n2xT Qx + qT x + 1\n2zT Rz + rT x\ns.t.\nx + z = c\n,\n(24)\nwhere x, z ∈R2, R = diag(0.1, 10), Q = UθRU T\nθ\nUθ =\n\u0012\ncos(θ)\n−sin(θ)\nsin(θ)\ncos(θ)\n\u0013\n,\nθ =\nπ\n4 , q = (1, 1)T , r = (1, −1)T , and c = (2, 1)T . In\nthis setting the constraint can be split into two subconstraints,\nx1+z1 = c1 and x2+z2 = c2, and a multiconstraint ADMM\nformulation can be applied. The corresponding ADMM itera-\ntion matrix is\nHρ = (I + DρR−1)−1(I + DρQ−1)−1(I + DρQ−1DρR−1).\nNotably, Hρ will have complex eigenvalues for some values\nof ρ = (ρ1, ρ2)T . Figure 1 displays surfaces for the magnitude\nand angle of the maximum eigenvalues of Hρ. Similarly, the\nmagnitude and angles of the maximum eigenvalues in the cases\nwhen ρ = ρ1 = ρ2, corresponding to the diagonals in Figure\n1, are plotted in Figure 2.\nThe relative residual of the multiconstraint algorithms for\nan array of starting (ρ1, ρ2) after 20 and 50 iterations is\ndisplayed as a surface in Fig. 3. The relative residuals of the\nsingle-constraint and multiconstraint algorithms for a range of\nstarting ρ = ρ1 = ρ2 after 20 and 50 iterations are plotted in\nFig. 4.\nB. Scaled Constraint Sum of Squares\nConsider the constrained optimization problem\nargmin\nx,z\n1\n2xT Qx + qT x + 1\n2zT Rz + rT x\ns.t.\njm(aT\nj x + bT\nj z −cj) = 0, j = 1, . . . , J,\n(25)\nwhere each constraint is scaled by it index j raised to a chosen\npower m, x ∈RM, z ∈RN, aj and q are randomly sampled\nfrom an M-dimensional standard normal distribution, bj and\nr are randomly sampled from an N-dimensional standard\nnormal distribution, Q = QT\n1 Q1, Q1 is randomly sampled\nfrom an M × M standard normal distribution, R = RT\n1 R,\nR1 is drawn from an N × N standard normal distribution, cj\nare scalar variables drawn from a random normal distribution,\nand m is a scaling parameter between the different constraints.\nThe relative residuals of the single-constraint and multicon-\nstraint algorithms for a range of starting ρ = ρ1 = ρ2 after 50\niterations for the m = 0, 1, 2 cases are plotted in Fig. 5.\nC. Multiblock ℓ1 Data Fidelity Total Variation Regularization,\nImage Reconstruction\nConsider the unconstrained optimization problem for image\nreconstruction\nargmin\nx\n∥F(x) −d∥1 + δ∥∇x∥2,1 ,\n(26)\nwhere x ∈RM×M is the space of images to reconstruct over,\nF : RM×M →Rm represents a linear imaging operator that\nmaps from an image to a set of m measurements, d ∈Rm\nrepresents a set of noisy measurements, ∥· ∥1 represents the\nℓ1 norm over the M × M image, ∇: RM×M →R2×M×M\nis the finite difference gradient operator, ∥· ∥2,1 is the 2-\ndimensional ℓ2,1 norm, and δ > 0 is a regularization parameter.\nThe regularization term ∥∇·∥2,1 is referred to as total variation\n(TV) which is known to promote piecewise constant images\nwhile preserving sharp edges [48]. Both the ℓ1-norm and TV\nfunctional are not differentiable and the TV norm in particular\nis not well suited to gradient-based optimization due to the\ninstability of the gradient operator.\nAlternatively, this problem can be reformulated into a con-\nstrained optimization problem of the form\nargmin\nx,z1,z2\n∥z1∥1 + δ∥z2∥2,1\ns.t.\nF(x) −z1 = d\n∇x −z2 = 0\n,\n(27)\nwhich can be solved utilizing multiblock ADMM.\nThis multiblock ADMM algorithm was applied to an image\nreconstruction problem where the imaging operator F was a\n\n\n8\nFig. 1: Magnitude and angle (radians) of maximum eigenvalues of complex iteration matrix Hρ plotted as a function of ρ1\nand ρ2. The maximum eigenvalues become real when either ρ1 or ρ2 are very large or very small. The the optimal fixed\nρ = (ρ1, ρ2)T corresponds to the location of the smallest magnitude, which occurs for a complex maximal eigenvalue.\nFig. 2: Magnitude and angle (radians) of maximum eigenvalues of complex iteration matrix Hρ plotted as a function of\nρ = ρ1 = ρ2.\nsparse computed tomography (CT) imaging operator based\non the Radon transform, with 20 view angles equispaced\nover a semicircle and 363 projections per view angle, and\nwas implemented in Python using the SCICO package [49].\nMeasurements were then computed via the relationship\nd = F(xgt) + η,\nwhere xgt denotes the ground truth object, a Siemens Star with\n8 spokes on a M ×M = 256×256 pixel grid, and η represents\nan additive noise term corresponding to salt-and-pepper noise\ncorrupting a quarter of the measurements.\nUpdating the x variable required solving an ill-conditioned\nlinear system using conjugate gradient determined by the\ngradient and imaging operator. Solving this linear system\ncomprises most of the computational burden for solving this\nproblem.\nThe relative residual of the multiblock algorithms for an\narray of starting (ρ1, ρ2) after 50 iterations is displayed as a\nsurface in Fig. 6. The relative residuals of the single-constraint\nand multiconstraint algorithms for a range of starting ρ =\nρ1 = ρ2 after 50 iterations are plotted in Fig. 7. The resulting\nreconstructed images from each method after 50 iterations are\ndisplayed in Fig. 8.\nD. Run Times\nThe run time of each ADMM method was recorded,\ncompared, and presented in Table I. Because the proposed\nmethod does not involve expensive computations, it does not\nsignificantly increase run time compared to the fixed method.\nIn the image reconstruction problem, the proposed method\nlead to the shortest run time, most likely due to providing\nautomatic conditioning on the linear system and requiring\nfewer conjugate gradient iterations.\nE. Summary\nTable II depicts the relative residual at k = 50 with\nρ(0) = ρ(0)\n1\n= ρ(0)\n2\n= 1.0 for each method across each\nproblem. Table III displays the median relative residual at\nk = 50 for each method across each problem. The results\npresented in this work are consistent with empirical obser-\nvations that penalty parameter selection has a large impact\n\n\n9\nFig. 3: Relative residual of ADMM solutions corresponding to an iteration matrix with complex eigenvalues for fixed penalty\nparameter, multiparameter BBS, and multiparameter SRA methods after 20 and 50 iterations plotted on a surface as a function\nof initial ρ1 and ρ2. Note that the structure of the fixed method’s residual plots mimics the eigenvalue structure in Fig. 1 and\nonly converges by 50 iterations for a specific set of ρ. At 20 iterations the multiparameter SRA method has converged to a\nrelative residual close to zero for the entire ρ search space, while the multiparameter BBS method requires 50 iterations.\nﬁxed\nRB\nBBS\nMpBBS\nSRB\nSRA\nMpSRA\nFig. 4: Relative residual of ADMM solutions corresponding to an iteration matrix with complex eigenvalues for single-parameter\nand multiparameter adaptive penalty parameter rules after 20 and 50 iterations plotted as a function of initial ρ = ρ1 = ρ2.\nNote that the structure of the fixed method’s residual plots mimics the eigenvalue structure in Fig. 2. Both the multiparameter\nBBS and SRA method outperforms and converge quicker than the single-parameter methods, with the proposed multiparameter\nSRA method converging the quickest (20 iterations instead of 50).\nTABLE I: Run time, mean ± standard deviation [s]\nmethod\nproblem\nfixed\nRB\nBBS\nMpBBS\nSRB\nSRA\nMpSRA\nComplex Quads\n5.9e-2 ± 8.9e-3\n6.5e-2 ± 9.8e-3\n6.7e-2 ± 9.4e-3\n6.6e-2 ± 9.7e-3\n6.3e-2 ± 9.3e-3\n6.3e-2 ± 1.0e-2\n6.6e-2 ± 1.0e-2\nScaled Quads\n1.3e-1 ± 1.4e-2\n1.4e-1 ± 1.3e-2\n1.4e-1 ± 1.3e-2\n1.4e-1 ± 1.4e-2\n1.4e-1 ± 1.3e-2\n1.4e-1 ± 1.3e-2\n1.4e-1 ± 1.4e-2\nℓ1 fidelity TV reg\n1.1e+2 ± 6.0e+1\n1.5e+2 ± 3.8e+1\n1.0e+2 ± 5.0e+1\n9.7e+1 ± 5.6e+1\n1.5e+2 ± 1.5e+1\n1.2e+2 ± 1.6e+1\n7.3e+1 ± 1.0e+1\n\n\n10\nﬁxed\nRB\nBBS\nMpBBS\nSRB\nSRA\nMpSRA\nFig. 5: Relative residual after 50 iterations for ADMM solutions of m = 0 (left), m = 1 (middle), and m = 2 (right) sum of\nquadratics problem for each adaptive penalty parameter method plotted as a function of initial ρ = ρ1 = ρ2. The optimal ρ\nshifts for each case of m and the single ρ methods perform worse as the scaling between constraints grows. The multiparameter\nmethods do not perform the best at the m = 0 case when the constraints are scaled evenly. However, the performance of the\nmultiparameter methods demonstrate stable performance as the scaling between constraints grows.\nFig. 6: Relative residual of ADMM solutions for ℓ1 fidelity, total variation regularization sparse CT reconstruction problem\nutilizing fixed, multiparameter BBS, and multiparameter SRA methods after 50 iterations plotted on a surface as a function\nof initial ρ1 and ρ2. Note that the fixed method converges only in a region off of the ρ1 = ρ2 diagonal. The multiparameter\nBSS method demonstrates very poor performance and converges almost nowhere. The proposed multiparameter SRA method\nconverges almost everywhere.\non convergence and optimal selection method varies between\noptimization problems. In the sum of quadratics problem with\ncomplex eigenvalues in the iteration matrix, the proposed\nmethod converged before the other methods. In the scaled sum\nof quadratics problem, the proposed and BBS multiparameter\nmethods did not demonstrate the best performance in the\nunscaled case, but they demonstrated stable performance as\nthe scale between constraints increased while the performance\nof the single-parameter methods significantly decreased. This\nindicates that multiparameter methods that are multiscaling\ncovariant are needed to ensure quick convergence as the scale\nbetween constraints becomes uneven. The results of the ℓ1\ndata fidelity TV regularization image reconstruction problem\ndemonstrated how each method performs on a non-quadratic,\nconvex problem. This experiment indicated that the proposed\nmultiparameter method demonstrated similar behavior as it\ndid in a quadratic problem. However, the multiparameter BBS\nmethod exhibited work performance compared to the quadratic\nproblem. In this example, multiple parameters were needed for\nquick and accurate reconstruction. The multiparameter BBS\nmethod did not lead to an accurate reconstruction despite\na fixed optimal ρ existing. The proposed MpSRA method\ndemonstrated the best performance (by two orders of mag-\nnitude) and quickly converged regardless of initial choice of\npenalty parameter.\nVII. CONCLUSIONS\nThis work proposes an adaptive multiparameter selection\nmethod for ADMM applied to multiconstraint and multiblock\noptimization problems. This method was developed via a\ntheoretical framework that analyzes ADMM as an affine fixed-\npoint iteration problem and attempts to minimize the spectral\nradius of the iteration matrix involved. The proposed multipa-\nrameter method is referred to as the multiparameter spectral\nradius approximation (MpSRA) method and is an extension\nof the single-parameter SRA method\n[29] and is derived\nby extending and correcting the analysis of [29] for spectral\napproximation and optimization with multiple parameters. The\nMpSRA method is intended to be simple to understand and\nimplement while providing robust performance with respect to\n\n\n11\nTABLE II: Relative residual at k = 50 with ρ(0) = ρ(0)\n1\n= ρ(0)\n2\n= 1.0\nmethod\nproblem\nfixed\nRB\nBBS\nMpBBS\nSRB\nSRA\nMpSRA\nComplex Quads\n2.14e-12\n2.14e-12\n8.29e-9\n1.20e-15\n1.41e-11\n2.41e-10\n5.72e-16\nScaled Quads(m = 0)\n5.13e-4\n1.96e-7\n2.39e-5\n1.94e-7\n8.61e-9\n1.24e-9\n1.03e-6\nScaled Quads(m = 1)\n2.81e-1\n2.02e-4\n7.37e-3\n4.75e-8\n1.17e-3\n9.36e-4\n3.90e-6\nScaled Quads(m = 2)\n7.97e-1\n3.22e-1\n4.20e-1\n7.10e-5\n6.40e-1\n5.03e-1\n1.68e-5\nℓ1 fidelity TV reg\n4.96e-1\n1.09e-0\n4.96e-1\n4.46e-1\n1.23e-0\n4.92e-1\n2.31e-3\nTABLE III: Median relative residual at k = 50\nmethod\nproblem\nfixed\nRB\nBBS\nMpBBS\nSRB\nSRA\nMpSRA\nComplex Quads\n1.32e-2\n8.34e-11\n8.29e-9\n9.81e-16\n9.71e-11\n4.31e-10\n1.10e-15\nScaled Quads(m = 0)\n1.40e-1\n1.36e-7\n1.52e-5\n1.24e-6\n3.66e-8\n3.00e-9\n3.97e-6\nScaled Quads(m = 1)\n3.55e-1\n3.79e-4\n5.12e-3\n2.78e-6\n1.17e-3\n1.01e-3\n6.76e-6\nScaled Quads(m = 2)\n7.97e-1\n2.85e-1\n4.09e-1\n1.25e-5\n6.40e-1\n4.80e-1\n1.39e-5\nℓ1 fidelity TV reg\n2.50e+00\n8.85e-1\n2.50e+00\n1.97e+00\n1.25e+00\n6.69e-1\n3.86e-3\nﬁxed\nRB\nBBS\nMpBBS\nSRB\nSRA\nMpSRA\nFig. 7: Relative residual of ADMM solutions for ℓ1 fidelity,\ntotal variation regularization sparse CT reconstruction problem\nfor single-parameter and multiparameter adaptive penalty pa-\nrameter rules after 50 iterations plotted as a function of initial\nρ = ρ1 = ρ2. Note that none of the single-parameter methods\nconverge, corresponding to the optimal ρ being off-diagonal in\nFig. 6. Meanwhile, the proposed multiparameter SRA method\nconverges for all initial ρ.\ninitial parameters and preventing further complexity associated\nwith multiple parameters.\nThe efficacy of the proposed method was demonstrated and\ncompared to several single-parameter ADMM approaches and\nanother multiparameter method, the multiparameter Barzilai-\nBorwein spectral (BBS) method, in three numerical experi-\nments. In each of these experiments, each adaptive penalty\nparameter method for ADMM was applied to an optimization\nproblem. The first optimization problem was a constrained\nsum of quadratics optimization problem which resulted in an\niteration matrix with complex eigenvalues. This experiment\ndemonstrated that the proposed method assumptions around\ncomplex eigenvalues held and that the proposed method con-\nverges quicker than the other methods regardless of initial\npenalty parameter. The second optimization problem was a\nsum of quadratics with multiple constraints, and the associated\nexperiment was repeated for constraints at three different\nscales. This experiment demonstrates that a multiparameter\nmethod may not lead to the quickest convergence when\nconstraints are scaled equally, but, as the scales between\nconstraints grow, a multiparameter method is necessary for\nquick convergence. The third optimization problem was a\nmultiblock optimization formulation of image reconstruction\nfor sparse computed tomography using an ℓ1 data fidelity and\nTV regularization. This problem was an example of an ADMM\nproblem in which the quickest convergence could not be\nachieved with a single-parameter and a multiparameter method\nwas needed. The proposed method was the only method\nthat achieved satisfactory convergence within 50 iterations\nand presented little dependence on initial penalty parameter.\nThese experiments highlight the empirical performance of the\nproposed method and that is competitive or superior to state-\nof-the-art methods.\nAPPENDIX A\nEQUIVALENCE OF MULTICONSTRAINT ADMM AND\nSTANDARD ADMM VIA DIAGONAL PRECONDITIONING\nConsider the constrained optimization problem equivalent\nto the one in (1) with scaled constraints\nargmin\nx,z1,...,zJ\nf(x) + g(z)\ns.t.\nβjAjx + βjBjz = βjcj\nj = 1, . . . , J,\n(28)\nfor scalar parameters {βj}J\nj=1. Letting β = (β1, . . . , βJ)T and\nD be the diagonal operator in (5), then (28) can be expressed\nin a form with a single constraint\nargmin\nx,z1,...,zJ\nf(x) + g(z)\ns.t.\nDβAx + DβBz = Dβc.\n(29)\n\n\n12\nGround T\nruth\nRRMSE=0.847\nfixed\nRRMSE=0.816\nRB\nRRMSE=0.847\nBBS\nRRMSE=1.089\nMpBBS\nRRMSE=0.973\nSRB\nRRMSE=0.739\nSRA\nRRMSE=0.247\nMpSRA\nFig. 8: Example reconstructions from each penalty parameter method initialized at ρ = ρ1 = ρ2 = 1. Only the multiparameter\nmethod leads to a high-fidelity reconstruction with low relative root mean square error (RRMSE).\nThe standard implementation of ADMM can then be ap-\nplied to this single-constraint problem with a scalar penalty\nparameter ρ0 > 0\nx(k+1)\n= argmin\nx\nf(x)\n+ρ0\n2\n\r\r\rDβAx+DβBz(k)−Dβc + ˜y(k)\nρ0\n\r\r\r\n2\n(30)\nz(k+1)\n= argmin\nz\ng(z)\n+ρ0\n2\n\r\r\rDβAx(k+1)+DβBz−Dβc + ˜y(k)\nρ0\n\r\r\r\n2\n(31)\n˜y(k+1)\n= ˜y(k)\n+ρ0\n\u0000DβAx(k+1) + DβBz(k+1) −Dβc\n\u0001\n.\n(32)\nLetting y(k) = Dβ˜y(k) and splitting the constraints reformu-\nlates Eqs. (30)-(32) to\nx(k+1)\n= argmin\nx\nf(x)\n+\nJ\nX\nj=1\nρ0β2\nj\n2\n\r\r\r\r\rAjx+Bjz(k)−cj +\ny(k)\nj\nρ0β2\nj\n\r\r\r\r\r\n2\n(33)\nz(k+1)\n= argmin\nz\ng(z)\n+\nJ\nX\nj=1\nρ0β2\nj\n2\n\r\r\r\r\rAjx(k+1)+Bjz−cj + yj(k)\nρ0β2\nj\n\r\r\r\r\r\n2\n(34)\ny(k+1)\nj\n= yj(k) + ρ0β2\nj\n\u0000Ajx(k+1) + Bjz(k+1) −cj\n\u0001\n.\n(35)\nLetting ρj\n= ρ0β2\nj recovers the multiparameter ADMM\nimplementation in Eqs. (2)-(4).\nThus the multiparameter ADMM for multiconstraint prob-\nlems is equivalent to the standard implementation of ADMM\nwith scaled constraints. This multiparameter method then\ninherits the convergence properties of standard ADMM. Fur-\nthermore, this formulation implies that finding the optimal set\nof penalty parameters ρ = (ρ1, . . . , ρJ)T in Eqs. (2)-(4) is\nequivalent to finding a single optimal penalty parameter ρ0\nand diagonal scaling or conditioning between constraints [24]\nβ = (β1, . . . , βJ)T in Eqs. (30)-(32). Note that this work\nfocuses on scaling between constraints, but these same ideas\ncould be expanded to scaling constraints with a matrix and\nintroducing a formulation of ADMM based on norms with\npositive definite matrices.\nREFERENCES\n[1] P. L. Combettes and J.-C. Pesquet, “Proximal splitting methods in signal\nprocessing,” Fixed-point algorithms for inverse problems in science and\nengineering, pp. 185–212, 2011.\n[2] R. Glowinski and A. Marroco, “Sur l’approximation, par ´el´ements\nfinis d’ordre un, et la r´esolution, par p´enalisation-dualit´e d’une classe\nde probl`emes de dirichlet non lin´eaires,” ESAIM: Mathematical Mod-\nelling and Numerical Analysis - Mod´elisation Math´ematique et Analyse\nNum´erique, vol. 9, no. R2, pp. 41–76, Aug. 1975.\n[3] D. Gabay and B. Mercier, “A dual algorithm for the solution of nonlin-\near variational problems via finite element approximation,” Computers\n& Mathematics with Applications, vol. 2, no. 1, pp. 17–40, 1976.\ndoi:10.1016/0898-1221(76)90003-1\n[4] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed\noptimization and statistical learning via the alternating direction method\nof multipliers,” Foundations and Trends in Machine Learning, vol. 3,\nno. 1, pp. 1–122, Jan. 2011. doi:10.1561/2200000016\n[5] W. Deng and W. Yin, “On the global and linear convergence of the gen-\neralized alternating direction method of multipliers,” Journal of Scientific\nComputing, vol. 66, no. 3, pp. 889–916, May 2015. doi:10.1007/s10915-\n015-0048-x\n[6] P. Giselsson and S. Boyd, “Linear convergence and metric selection for\nDouglas-Rachford splitting and ADMM,” arXiv, 2014, arXiv:1410.8479.\n[7] Y. Wang, W. Yin, and J. Zeng, “Global convergence of ADMM in\nnonconvex nonsmooth optimization,” Journal of Scientific Computing,\nvol. 78, pp. 29–63, 2019. doi:10.1007/s10915-018-0757-z\n\n\n13\n[8] R. I. Bot, and E. R. Csetnek, “ADMM for monotone operators: con-\nvergence analysis and rates,” Advances in Computational Mathematics,\nvol. 45, pp. 327–359, 2019.\n[9] L. Lozenski and U. Villa, “Consensus ADMM for inverse problems\ngoverned by multiple PDE models,” arXiv preprint arXiv:2104.13899,\n2021.\n[10] E. B¨orgens and C. Kanzow, “ADMM-type methods for generalized Nash\nequilibrium problems in Hilbert spaces,” SIAM Journal on Optimization,\nvol. 31, no. 1, pp. 377–403, 2021.\n[11] W. Deng, M.-J. Lai, Z. Peng, and W. Yin, “Parallel multi-block ADMM\nwith O(1/k) convergence,” Journal of Scientific Computing, vol. 71, pp.\n712–736, 2017.\n[12] T.-Y. Lin, S.-Q. Ma, and S.-Z. Zhang, “On the sublinear convergence\nrate of multi-block ADMM,” Journal of the Operations Research Society\nof China, vol. 3, pp. 251–274, 2015.\n[13] S. Ramani and J. A. Fessler, “A splitting-based iterative algorithm\nfor accelerated statistical x-ray ct reconstruction,” IEEE Transac-\ntions on Medical Imaging, vol. 31, no. 3, pp. 677–688, 2012.\ndoi:10.1109/TMI.2011.2175233\n[14] A. et. al., “First m87 event horizon telescope results. iv. imaging the\ncentral supermassive black hole,” The Astrophysical Journal Letters, vol.\n875, no. 1, p. L4, 2019.\n[15] B. Wahlberg, S. Boyd, M. Annergren, and Y. Wang, “An admm\nalgorithm for a class of total variation regularized estimation problems,”\nIFAC Proceedings Volumes, vol. 45, no. 16, pp. 83–88, 2012.\n[16] S. V. Venkatakrishnan, C. A. Bouman, and B. Wohlberg, “Plug-and-play\npriors for model based reconstruction,” in 2013 IEEE global conference\non signal and information processing.\nIEEE, 2013, pp. 945–948.\n[17] R. Zhang and J. Kwok, “Asynchronous distributed ADMM for consensus\noptimization,” in International conference on machine learning. PMLR,\n2014, pp. 1701–1709.\n[18] A. Makhdoumi and A. Ozdaglar, “Convergence rate of distributed\nADMM over networks,” IEEE Transactions on Automatic Control,\nvol. 62, no. 10, pp. 5082–5095, 2017.\n[19] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, “Asynchronous\ndistributed ADMM for large-scale optimization - part I: Algorithm and\nconvergence analysis,” IEEE Transactions on Signal Processing, vol. 64,\nno. 12, pp. 3118–3130, 2016.\n[20] J. F. Mota, J. M. Xavier, P. M. Aguiar, and M. P¨uschel, “D-ADMM:\nA communication-efficient distributed algorithm for separable optimiza-\ntion,” IEEE Transactions on Signal processing, vol. 61, no. 10, pp. 2718–\n2723, 2013.\n[21] T.-H. Chang, M. Hong, and X. Wang, “Multi-agent distributed opti-\nmization via inexact consensus ADMM,” IEEE Transactions on Signal\nProcessing, vol. 63, no. 2, pp. 482–497, 2014.\n[22] W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin, “On the linear\nconvergence of the ADMM in decentralized consensus optimization,”\nIEEE Transactions on Signal Processing, vol. 62, no. 7, pp. 1750–1761,\n2014.\n[23] R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. Jordan, “A\ngeneral analysis of the convergence of ADMM,” in Proceedings of the\n32nd International Conference on Machine Learning, ser. Proceedings\nof Machine Learning Research, vol. 37, Lille, France, 07–09 Jul 2015,\npp. 343–352.\n[24] P. Giselsson and S. Boyd, “Diagonal scaling in douglas-rachford splitting\nand admm,” in 53rd IEEE Conference on Decision and Control, Dec.\n2014, pp. 5033–5039. doi:10.1109/cdc.2014.7040175\n[25] B. He, H. Yang, and S. Wang, “Alternating direction method with\nself-adaptive penalty parameters for monotone variational inequalities,”\nJournal of Optimization Theory and Applications, vol. 106, pp. 337–356,\n2000. doi:10.1023/a:1004603514434\n[26] B. Wohlberg, “ADMM penalty parameter selection by residual balanc-\ning,” 2017,” arXiv:1704.06209.\n[27] Z. Xu, M. Figueiredo, and T. Goldstein, “Adaptive ADMM with spectral\npenalty parameter selection,” in Proceedings of the 20th International\nConference on Artificial Intelligence and Statistics, ser. Proceedings of\nMachine Learning Research, vol. 54, Apr. 2017, pp. 718–727.\n[28] D. A. Lorenz and Q. Tran-Dinh, “Non-stationary Douglas-Rachford\nand alternating direction method of multipliers: adaptive step-sizes and\nconvergence,” Computational Optimization and Applications, vol. 74,\nno. 1, pp. 67–92, May 2019. doi:10.1007/s10589-019-00106-9\n[29] M. T. McCann and B. Wohlberg, “Robust and simple ADMM penalty\nparameter selection,” IEEE Open Journal of Signal Processing, vol. 5,\npp. 402–420, Jan. 2024. doi:10.1109/OJSP.2023.3349115\n[30] Z. Xu, “Alternating optimization: Constrained problems, adversarial net-\nworks, and robust models,” Ph.D. dissertation, University of Maryland,\nCollege Park, 2019.\n[31] J. Eckstein and D. P. Bertsekas, “On the Douglas-Rachford splitting\nmethod and the proximal point algorithm for maximal monotone oper-\nators,” Mathematical Programming, vol. 55, no. 1-3, pp. 293–318, Apr.\n1992. doi:10.1007/bf01581204\n[32] M. Yan and W. Yin, “Self equivalence of the alternating direction method\nof multipliers,” Splitting Methods in Communication, Imaging, Science,\nand Engineering, pp. 165–194, 2016. doi:10.1007/978-3-319-41589-5 5\n[33] Y. Nesterov, Introductory Lectures on Convex Optimization.\nSpringer\nUS, 2004.\n[34] P. T. Boggs and J. W. Tolle, “Sequential quadratic programming,” Acta\nnumerica, vol. 4, pp. 1–51, 1995.\n[35] J. B. Rosen and R. F. Marcia, “Convex quadratic approximation,”\nComputational Optimization and Applications, vol. 28, pp. 173–184,\n2004.\n[36] S. Lee, S. Kwon, and Y. Kim, “A modified local quadratic approxi-\nmation algorithm for penalized optimization problems,” Computational\nStatistics & Data Analysis, vol. 94, pp. 275–286, 2016.\n[37] D. Azagra, “Global and fine approximation of convex functions,” Pro-\nceedings of the London Mathematical Society, vol. 107, no. 4, pp. 799–\n824, 2013.\n[38] J. E. Dennis, Jr and J. J. Mor´e, “Quasi-Newton methods, motivation and\ntheory,” SIAM review, vol. 19, no. 1, pp. 46–89, 1977.\n[39] A. S. Lewis and M. L. Overton, “Nonsmooth optimization via quasi-\nNewton methods,” Mathematical Programming, vol. 141, pp. 135–163,\n2013.\n[40] A.\nHansson,\nZ.\nLiu,\nand\nL.\nVandenberghe,\n“Subspace\nsys-\ntem\nidentification\nvia\nweighted\nnuclear\nnorm\noptimization,”\nin\nIEEE Conference on Decision and Control (CDC), Dec. 2012.\ndoi:10.1109/CDC.2012.6426980\n[41] Z. Liu, A. Hansson, and L. Vandenberghe, “Nuclear norm system iden-\ntification with missing inputs and outputs,” Systems & Control Letters,\nvol. 62, no. 8, pp. 605 – 612, 2013. doi:10.1016/j.sysconle.2013.04.005\n[42] V. Q. Vu, J. Cho, J. Lei, and K. Rohe, “Fantope projection and selection:\nA near-optimal convex relaxation of sparse PCA,” in Advances in Neural\nInformation Processing Systems 26, 2013, pp. 2670–2678.\n[43] M.-D. Iordache, J. M. Bioucas-Dias, and A. Plaza, “Collaborative\nsparse regression for hyperspectral unmixing,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 52, no. 1, pp. 341–354, Jan. 2014.\ndoi:10.1109/TGRS.2013.2240001\n[44] D. S. Weller, A. Pnueli, O. Radzyner, G. Divon, Y. C. Eldar, and J. A.\nFessler, “Phase retrieval of sparse signals using optimization transfer\nand ADMM,” in IEEE International Conference on Image Processing\n(ICIP), Oct. 2014, pp. 1342–1346.\n[45] B.\nWohlberg,\n“Efficient\nconvolutional\nsparse\ncoding,”\nin\nIEEE\nInternational Conference on Acoustics, Speech, and Signal Pro-\ncessing\n(ICASSP),\nFlorence,\nItaly,\nMay\n2014,\npp.\n7173–7177.\ndoi:10.1109/ICASSP.2014.6854992\n[46] Z. Xu, M. A. T. Figueiredo, X. Yuan, C. Studer, and T. Goldstein, “Adap-\ntive relaxed ADMM: Convergence theory and practical implementation,”\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), Jul. 2017. doi:10.1109/cvpr.2017.765\n[47] J. Barzilai and J. M. Borwein, “Two-point step size gradient methods,”\nIMA Journal of Numerical Analysis, vol. 8, no. 1, pp. 141–148, 1988.\n[48] L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based\nnoise removal algorithms,” Physica D: nonlinear phenomena, vol. 60,\nno. 1-4, pp. 259–268, 1992.\n[49] T. Balke, F. Davis, C. Garcia-Cardona, S. Majee, M. McCann, L. Pfister,\nand B. Wohlberg, “Scientific computational imaging code (SCICO),”\nJournal of Open Source Software, vol. 7, no. 78, p. 4722, 2022.\ndoi:10.21105/joss.04722\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21202v1.pdf",
    "total_pages": 13,
    "title": "An Adaptive Multiparameter Penalty Selection Method for Multiconstraint and Multiblock ADMM",
    "authors": [
      "Luke Lozenski",
      "Michael T. McCann",
      "Brendt Wohlberg"
    ],
    "abstract": "This work presents a new method for online selection of multiple penalty\nparameters for the alternating direction method of multipliers (ADMM) algorithm\napplied to optimization problems with multiple constraints or functionals with\nblock matrix components. ADMM is widely used for solving constrained\noptimization problems in a variety of fields, including signal and image\nprocessing. Implementations of ADMM often utilize a single hyperparameter,\nreferred to as the penalty parameter, which needs to be tuned to control the\nrate of convergence. However, in problems with multiple constraints, ADMM may\ndemonstrate slow convergence regardless of penalty parameter selection due to\nscale differences between constraints. Accounting for scale differences between\nconstraints to improve convergence in these cases requires introducing a\npenalty parameter for each constraint. The proposed method is able to\nadaptively account for differences in scale between constraints, providing\nrobustness with respect to problem transformations and initial selection of\npenalty parameters. It is also simple to understand and implement. Our\nnumerical experiments demonstrate that the proposed method performs favorably\ncompared to a variety of existing penalty parameter selection methods.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}