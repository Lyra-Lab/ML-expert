{
  "id": "arxiv_2502.21163v1",
  "text": "Adaptive Illumination-Invariant Synergistic Feature Integration in a Stratified\nGranular Framework for Visible-Infrared Re-Identification\nYuheng Jia\nOxford e-Research Centre\nDepartment of Engineering Science\nUniversity of Oxford\nOxford, UK\nyuheng.jia@eng.ox.ac.uk\nWesley Armour\nOxford e-Research Centre\nDepartment of Engineering Science\nUniversity of Oxford\nOxford, UK\nwes.armour@oerc.ox.ac.uk\nAbstract\nVisible-Infrared\nPerson\nRe-Identification\n(VI-ReID)\nplays a crucial role in applications such as search and res-\ncue, infrastructure protection, and nighttime surveillance.\nHowever, it faces significant challenges due to modality\ndiscrepancies, varying illumination, and frequent occlu-\nsions. To overcome these obstacles, we propose AMINet, an\nAdaptive Modality Interaction Network. AMINet employs\nmulti-granularity feature extraction to capture comprehen-\nsive identity attributes from both full-body and upper-body\nimages, improving robustness against occlusions and back-\nground clutter. The model integrates an interactive feature\nfusion strategy for deep intra-modal and cross-modal align-\nment, enhancing generalization and effectively bridging the\nRGB-IR modality gap. Furthermore, AMINet utilizes phase\ncongruency for robust, illumination-invariant feature ex-\ntraction and incorporates an adaptive multi-scale kernel\nMMD to align feature distributions across varying scales.\nExtensive experiments on benchmark datasets demonstrate\nthe effectiveness of our approach, achieving a Rank-1 ac-\ncuracy of 74.75% on SYSU-MM01, surpassing the baseline\nby 7.93% and outperforming the current state-of-the-art by\n3.95%.\n1. Introduction\nVisible-Infrared Person Re-identification (VI-ReID) is crit-\nical for modern security and surveillance, enabling the\nmatching of individuals across spectral modalities in low-\nlight conditions. Applications like nighttime surveillance,\nsearch and rescue, and infrastructure protection rely on IR\nimaging to complement RGB, capturing thermal radiation\nwhere visible light fails [15, 29, 30]. However, the modality\ngap between RGB and IR images presents significant chal-\nlenges [3, 13, 19, 28]. RGB relies on visible light reflection,\nwhile IR captures thermal emission, leading to substantial\ndifferences in appearance, texture, and color [10, 34]. These\ndiscrepancies complicate the extraction of robust, modality-\ninvariant features, and real-world factors like variable illu-\nmination, occlusions, and background clutter further add to\nthe complexity of reliable VI-ReID [1, 11, 12, 18].\nTo address the modality gap in Visible-Infrared Per-\nson Re-Identification (VI-ReID), several methods have been\nproposed.\nGenerative Adversarial Networks (GANs) are\ncommonly used to generate consistent RGB-IR images, but\nthey often suffer from artifacts that degrade image quality\nand impair recognition [7, 22, 30]. Shared-specific feature\ntransfer methods, like Cross-Modal Shared-Specific Fea-\nture Transfer (cm-SSFT), integrate shared and modality-\nspecific features for better cross-modal fusion [17, 26]. Hy-\nbrid Dual-Path Networks enhance alignment by leverag-\ning shared and specific parameter layers with consistency\nconstraints [3, 5, 6, 12]. Graph Neural Networks (GNNs)\nmodel complex inter-modal relationships, using node-level\ninformation propagation to strengthen feature representa-\ntion [13, 16]. Semantic enhancements, including human\npose estimation and attribute integration, improve discrim-\ninative capability [10, 11]. Lastly, domain adaptation tech-\nniques such as adversarial training align features across\nmodalities but still face challenges in effectively capturing\nidentity-specific details [28].\nIn this paper,\nwe propose a novel framework to\ntackle the complex challenges of Visible-Infrared Per-\nson Re-Identification (VI-ReID), emphasizing comprehen-\nsive multi-granularity feature extraction and robust cross-\nmodality alignment.\nOur framework, the Hierarchical\nMulti-Granular Dual-Branch Network (HMG-DBNet), uti-\nlizes a dual-branch architecture to independently process\nfull-body and upper-body images.\nThis design captures\nboth high-level semantic information and detailed fine-\ngrained features, enabling the model to effectively learn\n1\narXiv:2502.21163v1  [cs.CV]  28 Feb 2025\n\n\ndistinctive appearance attributes even under partial occlu-\nsion. By integrating representations from both full-body\nand upper-body inputs, HMG-DBNet extracts comprehen-\nsive global features and critical upper-body details, such as\nshoulder posture and clothing texture, enhancing its capac-\nity to differentiate individual identities.\nTo effectively bridge the modality gap between RGB\nand infrared (IR) images, we introduce the Interactive Fea-\nture Fusion Strategy (IFFS). ICMIAS addresses the com-\nmon issue of fragmented feature representation by integrat-\ning intra-modality fusion with cross-modality alignment,\nensuring a cohesive combination of global and local fea-\ntures while simultaneously aligning data across RGB and\nIR modalities. The strategy leverages a nonlinear fusion\nmechanism, enhancing the interaction between modalities\nand facilitating the learning of robust, modality-invariant\nrepresentations. To further mitigate challenges posed by\nlighting variations, we incorporate a phase congruency-\nbased feature extraction method, Phase-Enhanced Struc-\ntural Attention Module (PESAM). This approach captures\nillumination-invariant structural features, providing a con-\nsistent foundation for effective cross-modality alignment.\nComplementing this, the PESAM selectively focuses on\nsalient structural regions, refining the alignment process by\nprioritizing critical features. Together, these innovations en-\nhance the model’s ability to achieve precise feature align-\nment and robust generalization, offering a comprehensive\nsolution for cross-modality person re-identification tasks.\nTraditional Maximum Mean Discrepancy (MMD) meth-\nods rely on a fixed single-kernel, limiting their adaptability\nto complex, multi-scale data patterns. To address this, we\nintroduce the Adaptive Multi-Scale Kernel MMD (AMK-\nMMD) module, which uses multi-scale Gaussian kernels\nwith adaptive bandwidth selection and learnable weights for\ngreater flexibility in feature alignment. To improve scala-\nbility, we also optimize computational efficiency through\nmini-batch processing and vectorized computations, re-\nducing complexity and making AMK-MMD suitable for\nlarge datasets. These integrated innovations achieve pre-\ncise feature alignment and robust generalization, providing\na scalable and state-of-the-art solution for real-world cross-\nmodality person re-identification.\nThe main contributions of this paper are summarized as\nfollows:\n• HMG-DBNet processes full-body and half-body im-\nages separately, capturing complementary global and\ndetailed features for better RGB-IR alignment and re-\nidentification robustness.\n• IFFS integrates intra-modality fusion with cross-modality\nalignment, reducing feature-level discrepancies and en-\nhancing recognition accuracy by combining global and\nlocal features.\n• PESAM leverages phase congruency for illumination-\ninvariant feature extraction and uses edge-guided atten-\ntion to focus on key structural regions for precise align-\nment.\n• AMK-MMD employs multi-scale kernel fusion with\nadaptive bandwidth selection, capturing fine-grained dis-\ncrepancies and improving alignment efficiency for scal-\nable cross-modality tasks.\n2. Related Work\nCross-Modal Person Re-Identification. Cross-modal Re-\nID (VI-ReID and VT Re-ID) tackles the challenge of\nmatching identities between visible and infrared images.\nSupervised approaches, such as DTRM and CAJ, align\nmodality-independent features at both feature and pixel lev-\nels, achieving strong cross-modality performance [17, 27].\nUnsupervised methods, including Cluster Contrast, ICE,\nADCA, and PGM, utilize clustering and graph-based strate-\ngies to establish cross-modal correspondence without re-\nlying on labeled data [23, 28, 29, 34]. GAN-based mod-\nels, like Align-GAN, enhance alignment by fusing features\nand performing pixel-level transformations. Auxiliary tech-\nniques, such as X-modality generation and grayscale trans-\nformation, further reduce the modality gap, improving fea-\nture consistency across domains [6, 8, 16, 30]. Despite their\neffectiveness, these methods often face scalability issues,\nhigh annotation costs, and sensitivity to noise, limiting their\napplicability in large-scale settings [4, 9, 14, 19].\nData Augmentation. Data augmentation is crucial for mit-\nigating modality discrepancies and enhancing sample di-\nversity in person re-identification. Traditional methods in-\nclude modality transformation, color space conversion, and\nnoise addition, which help improve generalization [24, 32].\nAdvanced techniques like Modality Mixed Augmentation\n(M-CutMix) create mixed samples to facilitate robust fea-\nture learning across visible and infrared modalities [20, 25].\nSemantic-guided pixel sampling selectively swaps clothing\nregions (e.g., shirts, pants) using human parsing models,\ngenerating structurally consistent samples for better gener-\nalization [2, 33]. Inter-modality transformations and GAN-\nbased synthetic generation further expand datasets by con-\nverting visible to infrared images, boosting cross-modality\nalignment [21, 31].\nOverall, these augmentation strate-\ngies significantly enhance model performance by increasing\nsample diversity and addressing modality gaps.\n3. Proposed Method\nIn cross-modality person re-identification, aligning RGB\nand IR features presents significant challenges due to dif-\nferences in illumination and spectral characteristics.\nTo\naddress this, we propose a Hierarchical Multi-Granular\nDual-Branch Network (HMG-DBNet), which systemati-\n2\n\n\ncally captures comprehensive identity features by extracting\nand fusing multi-granularity information from both global\nand part-based views.\n3.1. Multi-Granular Feature Extraction and Fusion\nHMG-DBNet leverages two branches to handle full-body\nand half-body images in both RGB and IR modalities, en-\nabling multi-scale identity feature extraction. Let Xglobal\nrgb\n∈\nRH×W ×C and Xglobal\nir\n∈RH×W ×C represent full-body\nRGB and IR images, while Xpart\nrgb ∈R\nH\n2 ×W ×C and Xpart\nir\n∈\nR\nH\n2 ×W ×C are half-body images.\nThe branches extract\nglobal and part-based features as:\nF m\nglobal = ϕglobal(Xglobal\nrgb\n),\nF i\nglobal = ϕglobal(Xglobal\nir\n)\n(1)\nF m\npart = ϕpart(Xpart\nrgb ),\nF i\npart = ϕpart(Xpart\nir )\n(2)\nThe extracted features are hierarchically fused to form a\nunified identity representation:\nHfused = ϕshared(F m\nglobal ⊕F i\nglobal ⊕F m\npart ⊕F i\npart),\n(3)\nwhere ⊕denotes concatenation, and ϕshared refines the\nconcatenated features into a modality-invariant representa-\ntion. Generalized Mean Pooling (GeM) then distills this\nrepresentation for improved compactness and robustness.\nLoss Function. HMG-DBNet utilizes identity loss, triplet\nloss, and MMD loss to achieve robust feature alignment\nand discrimination. The identity loss enhances separabil-\nity across labels, while triplet loss optimizes intra-class and\ninter-class distinctions. MMD loss further reduces distribu-\ntion gaps between RGB and IR features, collectively boost-\ning cross-modality re-identification accuracy.\n3.2. Interactive Feature Fusion Strategy (IFFS)\nThe IFFS framework addresses the inherent discrepancies\nbetween RGB and IR modalities by synergistically com-\nbining intra-modality fusion and cross-modality alignment.\nBy capturing identity cues at both global and local levels,\nthis dual strategy effectively enhances feature representa-\ntion, alignment, and robustness in cross-modal person re-\nidentification tasks.\nIntra-Modality Feature Fusion.\nThe goal of intra-\nmodality fusion is to refine identity-specific representations\nwithin each modality (RGB and IR) by combining comple-\nmentary global and part-based features. This process in-\ntegrates full-body and half-body views, ensuring that each\nmodality captures both broad structural attributes and de-\ntailed local cues effectively. Formally, intra-modality fusion\ncan be expressed as:\nF RGB\nintra = F RGB\nglobal ⊕F RGB\npart ,\nF IR\nintra = F IR\nglobal ⊕F IR\npart,\n(4)\nwhere ⊕denotes the concatenation operation. By merging\nexpansive global features (e.g., body shape, overall appear-\nance) with fine-grained local details (e.g., texture, distinc-\ntive identity markers), this fusion enhances the discrimina-\ntive power of each modality. Specifically, it helps RGB fea-\ntures retain crucial spatial patterns that may be influenced\nby varying illumination, while IR features leverage unique\nthermal signatures to maintain robust identity representa-\ntion under challenging environmental conditions.\nCross-Modality Feature Fusion. While intra-modality\nfusion strengthens individual modality features, cross-\nmodality fusion focuses on bridging the inherent distribu-\ntion gap between RGB and IR data.\nTraditional fusion\nmethods typically align full-body RGB features with full-\nbody IR features, which often leads to loss of critical local\ninformation unique to each modality. To address this issue,\nwe propose a novel fusion strategy that combines global\nfeatures from full-body RGB images with localized features\nfrom half-body IR images, and vice versa:\nF RGB-IR\ncross\n= F RGB\nglobal⊕F IR\npart,\nF IR-RGB\ncross\n= F IR\nglobal⊕F RGB\npart . (5)\nThis cross-modality fusion leverages the complementary\nnature of RGB and IR features: full-body RGB features\nprovide comprehensive structural information such as body\nposture and overall silhouette, while half-body IR features\noffer enhanced local details (e.g., facial contours, shoul-\nder outlines) due to their sensitivity to thermal variations.\nBy effectively aligning complementary characteristics from\nboth modalities, this approach reduces the feature distri-\nbution gap, enhances feature alignment, and improves the\nmodel’s generalization capability.\nThe result is a robust\nidentity representation that performs well across diverse\ncross-modal re-identification scenarios.\n3.3. Adaptive Multi-Scale Kernel Maximum Mean\nDiscrepancy (AMK-MMD)\nThe AMK-MMD is designed to address complex feature\ndistribution variations between RGB and IR modalities by\nleveraging multi-scale kernel fusion, adaptive bandwidth\nselection, and learnable kernel weights. Unlike traditional\nMMD, which typically employs a single kernel, AMK-\nMMD integrates multiple Gaussian kernels, each with dis-\ntinct bandwidths, providing the flexibility to capture both\ncoarse and fine-grained discrepancies across modalities.\nThe multi-scale kernel fusion, which captures diverse\nfeature granularity, is defined as:\nK(x, y) =\nM\nX\nm=1\nαm exp\n\u0012\n−∥x −y∥2\n2\n2σ2m\n\u0013\n,\n(6)\nwhere {σm}M\nm=1 are bandwidths and αm are learnable\nweights that dynamically adjust during training. This adapt-\nability enables AMK-MMD to capture essential distribu-\n3\n\n\nFigure 1. The proposed framework employs an Interactive Feature Fusion Strategy(IFFS) within a dual-stream network (HMG-DBNet) to\ncapture full-body and partial-body features effectively. PESAM utilizes phase congruency and edge-guided attention for robust RGB-IR\nalignment. AMK-MMD adaptively aligns feature distributions to reduce modality discrepancies. The multi-branch design, supervised by\nID, MMD and Triplet losses, enhances the overall accuracy of cross-modality person re-identification.\ntional nuances, emphasizing the kernel scales that con-\ntribute most to feature alignment across heterogeneous data.\nTo improve computational efficiency,\nAMK-MMD\nleverages batch processing, symmetry, and vectorized com-\nputation, optimizing resource usage for larger datasets. The\npairwise squared Euclidean distance matrix D for features\nfrom source and target domains, combined into a single ma-\ntrix Z, is calculated as:\nDij = ∥Zi −Zj∥2\n2.\n(7)\nThis formulation minimizes redundant calculations, facili-\ntating scalability even for high-dimensional embeddings.\nAMK-MMD further takes advantage of matrix symme-\ntry, which significantly reduces computational load. Given\nthe symmetrical property of the kernel matrix K (i.e.,\nKij = Kji), only the upper triangular part of K is com-\nputed, effectively halving the computational cost.\nThe AMK-MMD discrepancy loss, which quantifies fea-\nture alignment between modalities, is computed as:\nAMK-MMD2 =\n1\nns(ns −1)\nns\nX\ni,j=1\ni̸=j\nKss(i, j)\n+\n1\nnt(nt −1)\nnt\nX\ni,j=1\ni̸=j\nKtt(i, j)\n−\n2\nnsnt\nns\nX\ni=1\nnt\nX\nj=1\nKst(i, j),\n(8)\nwhere Kss, Ktt, and Kst are kernel submatrices repre-\nsenting source-source, target-target, and source-target com-\nparisons. By leveraging batch processing and symmetry,\nAMK-MMD scales efficiently with large datasets while\nmaintaining high alignment precision across modalities.\nThese features allow AMK-MMD to dynamically adapt\nto varying data distributions, effectively enhancing cross-\nmodality re-identification through a balanced and computa-\ntionally efficient approach to feature alignment.\n4\n\n\nSYSU-MM01\nRegDB\nComponents\nAll Search\nIndoor Search\nThermal to Visible\nVisible to Thermal\nIndex\nBase\nUBF\nIMDAL\nIDAL\nR-1\nmAP\nmINP\nR-1\nmAP\nmINP\nR-1\nmAP\nmINP\nR-1\nmAP\nmINP\nM0\n✓\n66.82\n62.61\n47.90\n70.34\n75.52\n70.86\n78.41\n73.41\n61.36\n80.78\n75.74\n62.53\nM1\n✓\n✓\n71.65\n64.97\n49.58\n75.87\n78.49\n73.47\n83.77\n76.92\n64.05\n84.86\n78.95\n65.33\nM2\n✓\n✓\n✓\n72.62\n67.70\n51.19\n77.94\n80.03\n75.72\n87.02\n79.86\n66.65\n87.89\n81.74\n67.46\nM3\n✓\n✓\n✓\n72.23\n67.04\n51.52\n77.35\n79.55\n76.15\n87.69\n80.04\n66.79\n88.37\n82.57\n67.70\nM4\n✓\n✓\n✓\n✓\n74.75\n69.71\n53.32\n79.18\n82.39\n77.43\n89.51\n82.41\n68.04\n91.29\n84.69\n69.96\nTable 1. Ablation study results showing the impact of different components (UBF, IMDAL, and IDAL) on Rank-1 accuracy (R-1), mean\nAverage Precision (mAP), and mean Inverse Negative Penalty (mINP) across SYSU-MM01 and RegDB datasets.\nWeights\nSYSU\nRegDB\nIntra\nInter\nRank-1\nmAP\nmINP\nRank-1\nmAP\nmINP\n0.0\n1.0\n71.05\n65.15\n49.15\n89.13\n81.46\n66.80\n0.2\n0.8\n73.94\n68.29\n52.87\n91.29\n83.49\n68.69\n0.4\n0.6\n74.75\n69.71\n53.32\n89.66\n81.89\n67.30\n0.6\n0.4\n73.07\n67.01\n50.77\n87.52\n81.18\n66.75\n0.8\n0.2\n71.47\n65.35\n49.44\n87.33\n81.61\n68.58\n1.0\n0.0\n71.60\n65.16\n48.80\n88.16\n81.80\n67.38\nTable 2. Results with different Intra and Inter weights on SYSU\nand RegDB datasets.\n3.4. Phase-Enhanced Structural Attention Module\n(PESAM)\nThe PESAM leverages the unique properties of phase con-\ngruency to extract consistent, modality-invariant features\nacross RGB and IR images, focusing on essential struc-\ntural details such as edges and contours that remain unaf-\nfected by changes in lighting and contrast. This approach\nis particularly advantageous in cross-modality person re-\nidentification, as phase congruency enables the network to\nidentify structural features reliably across both RGB and IR\nmodalities, providing a stable basis for alignment.\n(a) Effect on SYSU\n(b) Effect on RegDB\nFigure 2. Impact of Upper Body Proportion (UBP) on Model Ac-\ncuracy for SYSU (left) and RegDB (right) datasets.\nTo compute the phase congruency map, we use the fol-\nlowing formula:\nPC(x, y) =\nPS\ns=1 (Es(x, y) −T)\nPS\ns=1 As(x, y) + ϵ\n,\n(9)\nwhere Es(x, y) represents the phase congruency energy at\nscale s, T is a noise threshold to filter out low-amplitude\nresponses, and ϵ prevents division by zero. This phase con-\ngruency map PC(x, y) highlights illumination-invariant\nedges and contours, allowing it to serve as a robust feature\nfoundation across different lighting conditions.\nEdge-Guided Attention Mechanism (EGAM) further\nenhances the model’s focus on modality-invariant features\nby generating an attention map from the phase congruency\noutput. This attention map A(x, y), computed as:\nA(x, y) = σ(We ∗PC(x, y)),\n(10)\nwhere σ(·) is the sigmoid activation function, directs the\nnetwork to concentrate on structurally relevant regions that\nare robust across RGB and IR domains. The EGAM effec-\ntively guides feature extraction, strengthening phase con-\ngruency’s role in aligning features from both modalities.\nThen, we employ an adaptive weighting scheme to bal-\nance the contributions of RGB, IR, and phase congruency\nfeatures in the final fused representation. This scheme dy-\nnamically adjusts each modality’s contribution to ensure\nbalanced and robust feature fusion:\nFfinal = αvisF ′\nvis + αirF ′\nir + αphaseFphase,\n(11)\nwhere αvis, αir, and αphase are learned weights for each\nmodality.\n4. Experiments\n4.1. Experimental Setups\nEvaluation Metrics and Datasets.\nWe evaluate cross-\nmodality Re-ID using three metrics: CMC, mAP, and mINP,\nto measure retrieval accuracy and robustness on two stan-\ndard VI-ReID datasets:\nSYSU-MM01: A large-scale dataset with 491 iden-\ntities from six cameras (four RGB, two IR). The train-\ning set includes 395 identities; testing uses 96 identities\nwith 3,803 IR queries and a gallery of 301 (single-shot) or\n3,010 (multi-shot) RGB images, evaluated in all-search and\nindoor-search modes.\n5\n\n\nRegDB: Consists of 412 identities, each with 10 RGB\nand 10 IR images from one RGB and one thermal cam-\nera. Evaluations cover Thermal-to-Visible and Visible-to-\nThermal modes, providing a comprehensive bidirectional\nassessment.\nThermal to Visible\nVisible to Thermal\nMethod\nVenue\nR-1\nR-10\nmAP\nR-1\nR-10\nmAP\nZero-Pad [35]\nICCV 17\n16.63\n34.68\n17.82\n17.75\n34.21\n18.90\neDBTR [42]\nTIFS 20\n34.21\n58.74\n32.49\n34.62\n58.96\n33.46\nD2RL [33]\nCVPR 19\n43.40\n66.10\n44.10\n43.40\n66.10\n44.10\nAlignGAN [30]\nICCV 19\n56.30\n-\n53.40\n57.90\n-\n53.60\nDDAG [44]\nECCV 20\n68.08\n85.15\n61.80\n69.34\n86.19\n63.46\ncm-SSFT [21]\nCVPR 20\n71.00\n-\n71.70\n72.30\n-\n72.90\nIMT [38]\nNeuro 21\n56.30\n71.33\n88.11\n75.49\n87.48\n69.64\nFBP-AL [34]\nTNNLS 21\n70.05\n89.22\n66.61\n73.98\n89.71\n68.24\nVSD [27]\nCVPR 21\n71.80\n-\n70.10\n73.20\n-\n71.60\nSMCL [59]\nICCV 21\n83.05\n-\n78.57\n83.93\n-\n79.83\nMPANet [8]\nCVPR 21\n83.70\n-\n80.90\n82.80\n-\n80.70\nSPOT [2]\nTIP 22\n79.37\n92.79\n72.26\n80.35\n93.48\n72.46\nMAUM G [22]\nCVPR 22\n81.07\n-\n78.89\n83.39\n-\n78.75\nDART [33]\nCVPR 22\n81.97\n-\n73.78\n83.60\n-\n75.67\nSCFNet [24]\nACCV 22\n86.33\n99.41\n82.10\n85.97\n99.80\n81.91\nTSME [34]\nTCSVT 22\n86.41\n96.39\n75.70\n87.35\n97.10\n76.94\nGDA [27]\nPR 23\n69.67\n86.41\n61.98\n73.95\n89.47\n65.49\nTOPLight [39]\nCVPR 23\n80.65\n92.81\n75.91\n85.51\n94.99\n79.95\nCMTR [63]\nTMM 23\n81.06\n-\n83.75\n80.62\n-\n74.42\nMTMFE [11]\nPR 23\n81.11\n92.35\n79.59\n85.04\n94.38\n82.52\nAMINet (Ours)\nThis work\n89.51\n97.48\n82.41\n91.29\n97.88\n84.69\nTable 3. Performance comparisons on the RegDB dataset in both\nThermal to Visible and Visible to Thermal modes.\nImplementation Details. Our method, implemented in Py-\nTorch on a 24 GB RTX4090 GPU, resizes input images to\n388×144 (global) and 194×144 (head-shoulder). Data aug-\nmentations include Random Cropping and Random Eras-\ning. The model, based on ResNet-50 with Non-local blocks,\nis trained for 80 epochs with a batch size of 64. Feature\nextraction combines global and part-based modules aligned\nvia a Contrastive Modal Aligner. GEM pooling and Batch\nNormalization stabilize training, while feature mixing en-\nhances cross-modal discrimination. Optimization uses SGD\nwith weight decay 5 × 10−4, momentum 0.9, and a staged\nlearning rate starting at 0.01, peaking at 0.1, and reducing\nto 0.001.\n4.2. Ablation Study\nAs shown in Tab. 1, to evaluate the impact of each mod-\nule in our model on cross-modality person re-identification\n(Re-ID) performance, we conducted a series of abla-\ntion experiments involving three key components: Upper\nBody Feature extraction (UBF), Intra-Modality Distribution\nAlignment Loss (IMDAL), and Inter-Modality Distribution\nAlignment Loss (IDAL).\nBaseline Model.\nThe baseline model (M0), which only\nincludes full-body feature extraction (Base) without any\nadditional modules, achieved 66.82% Rank-1 accuracy in\nthe SYSU-MM01 all-search mode and 70.34% in indoor-\nsearch. On the RegDB dataset, it scored 78.41% Rank-1\naccuracy in the Thermal-to-Visible mode and 80.78% in the\nVisible-to-Thermal mode. These results indicate that us-\ning whole-body features alone limits the model’s ability to\nhandle cross-modality discrepancies, particularly under oc-\nclusion and varying illumination.\n(a) Rank-1 (SYSU)\n(b) Rank-1 (RegDB)\n(c) mAP (SYSU)\n(d) mAP (RegDB)\nFigure 3. Rank-1 and mAP results on SYSU and RegDB datasets.\nUpper Body Feature Extraction (UBF). Introducing the\nUBF module (Model M1) improved the Rank-1 accuracy\nfrom 66.82% to 71.65% in SYSU-MM01 all-search and\nfrom 78.41% to 83.77% in RegDB Thermal-to-Visible. The\nUBF module captures fine-grained details from the upper\nbody, such as head, shoulders, and chest, which enhances\nthe model’s robustness, particularly in cases with occlusion\nor pose variation. This module highlights the importance of\nlocal features for maintaining discriminative power across\ndifferent modalities.\nIntra-Modality\nDistribution\nAlignment\n(IMDAL).\nBuilding on M1, the addition of IMDAL (Model M2)\nfurther increased the Rank-1 accuracy to 72.62% in\nSYSU-MM01\nall-search\nand\nto\n87.02%\nin\nRegDB\nThermal-to-Visible.\nIMDAL\nreduces\nintra-modality\nvariability by aligning feature distributions within each\nmodality, addressing inconsistencies caused by lighting,\npose, and other variations. This enhanced intra-modality\nalignment improves feature stability and robustness, laying\na solid foundation for effective cross-modality alignment.\nInter-Modality Distribution Alignment (IDAL). Adding\nIDAL to M1 (Model M3) improved the Rank-1 accuracy\n6\n\n\nfrom 71.65% to 72.23% in SYSU-MM01 all-search and\nfrom 83.77% to 87.69% in RegDB Thermal-to-Visible.\nIDAL minimizes discrepancies between RGB and infrared\nfeature distributions, effectively aligning them within a\nshared feature space. By reducing cross-modality matching\nerrors, IDAL enables better generalization across modali-\nties, capturing intrinsic correlations and enhancing model\nrobustness under diverse modality conditions.\n4.3. Effect of Loss Weights\nAs shown in Tab. 2, the results indicate clear differences in\nthe optimal weights for intra-modality and inter-modality\nalignment under MMD constraints across datasets.\nThis\nhighlights the distinct modality characteristics of each\ndataset. In the SYSU, the RGB and IR images exhibit a\ncertain degree of consistency in lighting conditions, view-\npoints, and environmental changes, resulting in relatively\nsmaller modality discrepancies. The best performance is\nachieved with an intra-modality weight of 0.4 and an inter-\nmodality weight of 0.6. Conversely, the RegDB has greater\nmodality differences, necessitating a higher inter-modality\nweight of 0.8. This indicates that stronger cross-modality\nalignment is required to bridge the larger distribution gap.\n4.4. Effect of Upper-Body Proportion\nAs shown in Fig. 2, experiments on SYSU and RegDB\ndatasets assess the impact of upper-body data propor-\ntion on model performance in cross-modality person re-\nidentification. Results show a strong correlation between\nupper-body proportion and accuracy. For SYSU, peak ac-\ncuracy (74.15%) occurs at 50% upper-body input, while\nRegDB reaches its highest (91.29%) at 60%.\nThis sug-\ngests an optimal range of 50%-60%, where key features\nlike shoulders and torso enhance RGB-IR alignment. No-\ntably, SYSU accuracy declines beyond 50%, indicating re-\ndundancy from excessive upper-body data, which dilutes\ncritical full-body cues. In contrast, RegDB shows stability\nat 60%, benefiting from consistent scene conditions. These\nfindings underscore the importance of a balanced upper-\nbody ratio for effective feature alignment and robust identi-\nfication.\n4.5. State-of-the-Art Performance Comparison\nEvaluation on SYSU-MM01. Our method surpasses state-\nof-the-art models on the challenging SYSU-MM01 dataset,\ndemonstrating superior performance in both All Search and\nIndoor Search modes, as shown in Tab. 4. In All Search, it\nreaches a Rank-1 accuracy of 74.75%, mAP of 66.11%, and\nmINP of 51.32%, surpassing SCFNet by 1.79% in Rank-1\nand 11.78% in mINP. This superior alignment reflects the\nmethod’s robustness in unifying RGB and IR features effec-\ntively. In Indoor Search, our model further demonstrates its\nadaptability under controlled settings, achieving 79.18% in\nRank-1 accuracy and 82.39% in mAP, surpassing MPANet\nby 2.44% and 1.44%, respectively. These consistent gains\nhighlight the method’s effectiveness in handling intra- and\ncross-modality challenges.\nEvaluation on RegDB. On the RegDB dataset, which en-\ncompasses both Thermal-to-Visible and Visible-to-Thermal\nmodes, our model once again achieves state-of-the-art re-\nsults, as shown in Tab. 3. In Thermal-to-Visible mode, it\nrecords 89.51% in Rank-1 accuracy and 82.41% in mAP,\nimproving upon MAUM P by 2.56% and 3.07%, respec-\ntively.\nIn Visible-to-Thermal mode, it reaches a Rank-1\naccuracy of 91.29% and mAP of 84.69%, with a 3.42%\ngain in Rank-1 over MCLNet.\nThese results reflect the\nmodel’s advanced feature alignment and fusion strategies,\neffectively bridging RGB and IR modalities to achieve high\nre-identification accuracy across varied conditions.\n4.6. Visual Analysis\nt-SNE Visualization of Extracted Embeddings. Fig. 4\npresents the t-SNE visualizations of feature embeddings\nfrom RGB and IR modalities. In Figure 4(a), the initial\nmodel shows dispersed identity clusters with clear separa-\ntion between RGB (circles) and IR (triangles), indicating\npoor cross-modality alignment. Figure 4(b) illustrates our\nnewly integrated baseline model, which achieves improved\nclustering but still exhibits inconsistent modality alignment.\nIn contrast, Figure 4(c) shows proposed AMINet model,\nwhere identity clusters are distinct and tightly grouped, in-\ncluding both RGB and IR samples, demonstrating superior\nalignment and identity discrimination, and highlighting the\nmodel’s strong performance.\nCosine Similarity Analysis. Figures 4(d), 4(e), and 4(f)\nillustrate the intra- and inter-class feature distance distri-\nbutions to assess model discrimination. The x-axis shows\nEuclidean distances, and the y-axis shows frequency. Blue\nlines represent intra-class (same identity), while green lines\nrepresent inter-class (different identities). In Figure 4(d),\nthe initial model shows significant overlap with a small\nmean gap of 0.26, reflecting weak discrimination. Figure\n4(e) shows the baseline model increasing this gap, improv-\ning separation. Our final model in Figure 4(f) further ex-\npands this gap to 0.56, with minimal overlap, indicating en-\nhanced clustering of intra-class samples and clear separa-\ntion of inter-class samples. This confirms the superior per-\nformance of our model in cross-modality feature alignment\nand identity discrimination.\n5. Conclusion\nIn\nthis\npaper,\nwe\npropose\nAMINet,\nan\nAdaptive\nModality Interaction Network for Visible-Infrared Per-\nson Re-Identification (VI-ReID). AMINet integrates multi-\ngranularity feature extraction (HMG-DBNet), interactive\nfeature fusion (IFFS), phase-enhanced attention (PESAM),\n7\n\n\nAll Search\nIndoor Search\nMethod\nVenue\nR-1\nR-10\nR-20\nmAP\nmINP\nR-1\nR-10\nR-20\nmAP\nmINP\neDBTR [42]\nTIFS 20\n27.82\n67.34\n81.34\n28.42\n-\n32.46\n77.42\n89.62\n42.46\n-\nCoSiGAN [56]\nICMR 20\n35.55\n81.54\n90.43\n38.33\n-\n-\n-\n-\n-\n-\nMSR [5]\nTIP 20\n37.35\n83.40\n93.34\n38.11\n-\n39.64\n89.29\n97.66\n50.88\n-\nX-Modal [12]\nAAAI 20\n49.92\n89.79\n95.96\n50.73\n-\n-\n-\n-\n-\n-\nDDAG [44]\nECCV 20\n54.75\n90.36\n95.81\n53.02\n39.62\n61.02\n94.06\n98.41\n67.98\n62.61\nLLM [4]\nECCV 20\n55.25\n86.09\n92.69\n52.96\n-\n59.65\n90.85\n95.02\n65.46\n-\nIMT [38]\nNeuro 21\n56.52\n90.26\n95.59\n57.47\n38.75\n68.72\n94.61\n97.42\n75.22\n64.22\nNFS [1]\nCVPR 21\n56.91\n91.34\n96.52\n55.45\n-\n62.79\n96.53\n99.07\n69.79\n-\nVSD [27]\nCVPR 21\n60.02\n94.18\n98.14\n58.80\n-\n66.05\n96.59\n99.38\n72.98\n-\nGLMC [48]\nTNNLS 21\n64.37\n93.90\n97.53\n63.43\n-\n67.35\n98.10\n99.77\n74.02\n-\nMCLNet [8]\nICCV 21\n65.40\n93.33\n97.14\n61.98\n47.39\n72.56\n96.98\n99.20\n76.58\n72.10\nSMCL [59]\nICCV 21\n67.39\n92.87\n96.76\n61.78\n-\n68.84\n96.55\n98.77\n75.56\n-\nDML [42]\nTCSVT 22\n58.40\n91.20\n96.90\n56.10\n-\n62.40\n95.20\n98.70\n69.50\n-\nMAUM G [22]\nCVPR 22\n61.59\n-\n-\n59.96\n-\n67.07\n-\n-\n73.58\n-\nTSME [34]\nTCSVT 22\n64.23\n95.19\n98.73\n61.21\n-\n64.8\n96.92\n99.31\n71.53\n-\nSPOT [2]\nTIP 22\n65.34\n92.73\n97.04\n62.25\n-\n69.42\n96.22\n99.12\n74.63\n-\nFMCNet [43]\nCVPR 22\n66.34\n-\n-\n62.51\n-\n68.15\n-\n-\n74.09\n-\nDART [33]\nCVPR 22\n68.72\n96.39\n98.96\n66.29\n-\n72.52\n97.84\n99.46\n78.17\n-\nGDA [27]\nPR 23\n63.94\n93.34\n97.29\n60.73\n-\n71.06\n97.31\n99.47\n76.01\n-\nCMTR [63]\nTMM 23\n65.45\n94.47\n98.16\n62.90\n-\n71.64\n97.16\n99.22\n76.67\n-\nTOPLight [39]\nCVPR 23\n66.76\n96.23\n98.70\n64.01\n-\n72.89\n97.93\n99.28\n76.70\n-\nMRCN [61]\nAAAI 23\n68.90\n95.20\n98.40\n65.50\n-\n76.00\n98.30\n99.70\n79.80\n-\nMTMFE [11]\nPR 23\n69.47\n96.42\n99.11\n66.41\n-\n71.72\n97.19\n98.97\n76.38\n-\nMRCN-P [61]\nAAAI 23\n70.80\n96.50\n99.10\n67.30\n-\n76.40\n98.50\n99.90\n80.00\n-\nAMINet (Ours)\nThis work\n74.75\n97.87\n99.32\n66.11\n51.32\n79.18\n98.78\n99.67\n82.39\n77.43\nTable 4. Performance comparisons on SYSU-MM01 dataset in both All Search and Indoor Search modes. Our method, denoted as AMINet\n(Ours), significantly outperforms existing methods across key evaluation metrics, including Rank-1, Rank-10, Rank-20, mAP, and mINP.\nand adaptive alignment (AMK-MMD) to bridge the RGB-\nIR modality gap effectively.\nExtensive experiments on\nSYSU-MM01 and RegDB datasets validate our approach,\nachieving a new state-of-the-art Rank-1 accuracy of 74.75%\non SYSU-MM01 and 91.29% on RegDB. These results\ndemonstrate the robustness and generalization of our model,\noffering a strong baseline for future VI-ReID research.\n8\n\n\nFigure 4. t-SNE Visualizations and Feature Distance Distributions for Different Models, Demonstrating Cross-Modality Feature Alignment\nand Intra/Inter-Class Separation.\n9\n\n\nReferences\n[1] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical\nevaluation of gated recurrent neural networks on sequence\nmodeling.\nIn Proc. 2014 Adv. Neural Inf. Process. Syst.,\npages 1725–1732, 2014. 1\n[2] M. Cutmix. M-cutmix: Data augmentation using cut and\nmix. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\npages 6006–6015, 2019. 2\n[3] W. Deng, L. Zheng, and S. Wang.\nImage-image transla-\ntion with identity information for person re-identification.\nIn Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages\n4167–4176, 2018. 1\n[4] Chaoyou Fu, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei,\nand Ran He. Cm-nas: Cross-modality neural architecture\nsearch for visible-infrared person re-identification. In Proc.\nIEEE/CVF Int. Conf. Comput. Vis., pages 11803–11812,\n2021. 2\n[5] Xiaowei Fu, Fuxiang Huang, Yuhang Zhou, Huimin Ma, Xin\nXu, and Lei Zhang. Cross-modal cross-domain dual align-\nment network for rgb-infrared person re-identification.\nIn\nIEEE Trans. Circuits Syst. Video Technol., pages 6874–6887,\n2022. 1\n[6] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\nadaptation by backpropagation. In Proc. Int. Conf. Mach.\nLearn., pages 1180–1189, 2015. 1, 2\n[7] D. Gray and H. Tao. Viewpoint invariant pedestrian recog-\nnition with an ensemble of localized features. In Proc. 2008\nEur. Conf. Comput. Vis., pages 262–275, 2008. 1\n[8] Xin Hao, Sanyuan Zhao, Mang Ye, and Jianbing Shen.\nCross-modality person re-identification via modality confu-\nsion and center aggregation. In Proc. IEEE/CVF Int. Conf.\nComput. Vis., pages 16383–16392, 2021. 2\n[9] Yi Hao, Nannan Wang, Jie Li, and Xinbo Gao. Hsme: Hy-\npersphere manifold embedding for visible thermal person re-\nidentification. In Proc. AAAI Conf. Artif. Intell., pages 8385–\n8392, 2019. 2\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition.\nIn Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 770–778, 2016. 1\n[11] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang.\nTransreid: Transformer-based object re-\nidentification. In Proc. IEEE/CVF Int. Conf. Comput. Vis.,\npages 14993–15002, 2021. 1\n[12] K. Hidaka and T. Ogawa. Color-spaces conversion for per-\nson re-identification in video-surveillance environments. In\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., pages\n1245–1254, 2018. 1\n[13] E. Hoffer and N. Ailon. Deep metric learning using triplet\nnetwork. In Proc. IEEE Conf. Comput. Vis. Pattern Recog-\nnit., pages 84–92, 2015. 1\n[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\npages 7132–7141, 2018. 2\n[15] G. Huang, Z. Liu, and L. van der Maaten. Densely connected\nconvolutional networks. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 4700–4708, 2017. 1\n[16] Xun Huang and Serge Belongie.\nArbitrary style transfer\nin real-time with adaptive instance normalization. In Proc.\nIEEE Int. Conf. Comput. Vis., pages 1501–1510, 2017. 1, 2\n[17] X. Huang and S. Belongie. Modality-adaptive feature trans-\nformation for cross-modality re-identification. In Proc. IEEE\nInt. Conf. Comput. Vis., pages 1538–1546, 2020. 1, 2\n[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nProc. 2015 Int. Conf. Mach. Learn., pages 448–456, 2015. 1\n[19] Jieru Jia, Qiuqi Ruan, and Timothy M Hospedales. Frus-\ntratingly easy person re-identification.\nIn arXiv preprint\narXiv:1905.03422, pages 1–10, 2019. 1, 2\n[20] H. Li, Y. Ge, and X. Wang. Joint visual-semantic reasoning\nfor image-text matching. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 5186–5195, 2020. 2\n[21] P. Li, Z. Yang, and X. Li. Pose guided data augmentation for\nperson re-identification. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 5406–5415, 2018. 2\n[22] S. Paisitkriangkrai, C. Shen, and A. van den Hengel. Learn-\ning to rank in person re-identification with metric ensembles.\nIn Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages\n1846–1855, 2015. 1\n[23] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D.\nParikh, and D. Batra. Grad-cam: Visual explanations from\ndeep networks via gradient-based localization. In Proc. IEEE\nInt. Conf. Comput. Vis., pages 618–626, 2017. 2\n[24] C. Shorten and T. M. Khoshgoftaar. A survey on image data\naugmentation for deep learning. J. Big Data, 6(1):60, 2019.\n2\n[25] L. Shu, S. Li, and Q. Zhao. Semantic guided pixel sampling\nfor data augmentation. In Proc. IEEE Int. Conf. Comput.\nVis., pages 1000–1009, 2020. 2\n[26] X. Wang, L. Zhang, and L. Zhang. Manifold alignment us-\ning procrustes analysis. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 500–510, 2019. 1\n[27] Y. Yang, G. Wang, and T. Zhang. Cluster contrast for un-\nsupervised cross-modality person re-identification. In Proc.\nIEEE Int. Conf. Comput. Vis., pages 1536–1544, 2019. 2\n[28] M. Ye, B. Du, and J. Shen. Joint learning for visible-infrared\nperson re-identification.\nIn IEEE Trans. Image Process.,\npages 9387–9399, 2020. 1, 2\n[29] Y. Zhang and H. Wang.\nDiverse embedding expansion\nnetwork for visible-infrared person re-identification. IEEE\nTrans. Image Process., 32:2153–2162, 2022. 1, 2\n[30] H. Zhao, Z. Zhang, and S. Wang.\nCross-modality fea-\nture alignment for visible-infrared person re-identification.\nIn Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages\n8432–8440, 2021. 1, 2\n[31] X. Zhao, M. Jiang, and Z. Zhang. Random erasing data aug-\nmentation. In Proc. AAAI Conf. Artif. Intell., pages 13001–\n13007, 2020. 2\n[32] Z. Zhao, H. Shao, and Z. Shi. Person re-identification with\nfused multi-channel representation.\nIn Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., pages 452–460, 2019. 2\n[33] J. Zhong, S. Zhang, and J. Li. Jaccard distance re-ranking\nof k-reciprocal encoding. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 5941–5949, 2017. 2\n10\n\n\n[34] K. Zhu, H. Guo, T. Yan, Y. Zhu, J. Wang, and M. Tang.\nPass: Part-aware self-supervised pre-training for person re-\nidentification. In Proc. Eur. Conf. Comput. Vis., pages 198–\n214, 2023. 1, 2\n11\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21163v1.pdf",
    "total_pages": 11,
    "title": "Adaptive Illumination-Invariant Synergistic Feature Integration in a Stratified Granular Framework for Visible-Infrared Re-Identification",
    "authors": [
      "Yuheng Jia",
      "Wesley Armour"
    ],
    "abstract": "Visible-Infrared Person Re-Identification (VI-ReID) plays a crucial role in\napplications such as search and rescue, infrastructure protection, and\nnighttime surveillance. However, it faces significant challenges due to\nmodality discrepancies, varying illumination, and frequent occlusions. To\novercome these obstacles, we propose \\textbf{AMINet}, an Adaptive Modality\nInteraction Network. AMINet employs multi-granularity feature extraction to\ncapture comprehensive identity attributes from both full-body and upper-body\nimages, improving robustness against occlusions and background clutter. The\nmodel integrates an interactive feature fusion strategy for deep intra-modal\nand cross-modal alignment, enhancing generalization and effectively bridging\nthe RGB-IR modality gap. Furthermore, AMINet utilizes phase congruency for\nrobust, illumination-invariant feature extraction and incorporates an adaptive\nmulti-scale kernel MMD to align feature distributions across varying scales.\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\nour approach, achieving a Rank-1 accuracy of $74.75\\%$ on SYSU-MM01, surpassing\nthe baseline by $7.93\\%$ and outperforming the current state-of-the-art by\n$3.95\\%$.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}