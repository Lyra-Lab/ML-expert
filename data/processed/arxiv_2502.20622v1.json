{
  "id": "arxiv_2502.20622v1",
  "text": "RTGen: Real-Time Generative Detection Transformer\nChi Ruan\nUniversity of Ottawa\nAbstract\nWhile open-vocabulary object detectors require prede-\nfined categories during inference, generative object detec-\ntors overcome this limitation by endowing the model with\ntext generation capabilities. However, existing generative\nobject detection methods directly append an autoregressive\nlanguage model to an object detector to generate texts for\neach detected object.\nThis straightforward design leads\nto structural redundancy and increased processing time.\nIn this paper, we propose a Real-Time GENerative De-\ntection Transformer (RTGen), a real-time generative ob-\nject detector with a succinct encoder-decoder architecture.\nSpecifically, we introduce a novel Region-Language De-\ncoder (RL-Decoder), which innovatively integrates a non-\nautoregressive language model into the detection decoder,\nenabling concurrent processing of object and text informa-\ntion.\nWith these efficient designs, RTGen achieves a re-\nmarkable inference speed of 60.41 FPS. Moreover, RTGen\nobtains 18.6 mAP on the LVIS dataset, outperforming the\nprevious SOTA method by 3.5 mAP.\n1. Introduction\nObject detection [33, 21, 32] has traditionally been con-\nstrained to fixed categories, driving researchers to explore\nopen-vocabulary detection techniques. Despite significant\nadvances [19, 8, 40, 44, 27], existing open-vocabulary de-\ntectors still rely on predefined categories during inference,\nconstraining their applicability in real-world scenarios. To\ntackle this issue, generative object detection [13] has been\ndemonstrated to be an effective solution, offering a more\ndynamic and flexible approach to object detection.\nVarious approaches [38, 20, 39, 24, 13] have been pro-\nposed for generative object detection, often employing rel-\natively simple methods. For instance, in GRiT [38], a text\ndecoder is positioned after a foreground object extractor to\nreceive object features and generate descriptive captions.\nSimilarly, in GenerateU [20], object queries generated by an\nobject detector are transmitted into a language model to pro-\n*Corresponding Author.\n44× speedup\n5× improvement\nFigure 1: Speed and performance comparison.\nRTGen\nachieves state-of-the-art performance in terms of both accu-\nracy and speed. All evaluation experiments are conducted\non one NVIDIA RTX 4090 GPU. The size of the circle rep-\nresents the model parameters.\nduce object names. These methods generally involve feed-\ning object features extracted by a detector into an autore-\ngressive language model to generate related texts. While\nthese approaches are effective, they tend to exhibit inade-\nquate integration between the language and detection mod-\nels, leading to redundant frameworks and increased run-\ntime.\nTo address the challenges mentioned above, we explore\nhow to develop a more efficient approach for integrating a\nlanguage model into a detector. Inspired by extensive mul-\ntimodal research [17, 2, 18, 26] that utilizes a shared trans-\nformer architecture to process information across various\nmodalities, we attempt to employ a unified decoder to pro-\ncess both object and text data. However, most language\nmodels rely on an autoregressive architecture [31, 35, 4].\nThis approach consumes a significant amount of time, hin-\ndering real-time implementation. Additionally, it requires\nground truth inputs during training, which can only be\nobtained once the detected objects have been accurately\nmatched with their corresponding annotations, thus prevent-\n1\narXiv:2502.20622v1  [cs.CV]  28 Feb 2025\n\n\ning the concurrent processing of object and text informa-\ntion. Therefore, we search for a non-autoregressive lan-\nguage model [7] that does not require ground truth inputs\nduring training for the integration with a detector.\nIn this paper, we propose the Real-Time GENerative de-\ntection transformer (RTGen), a real-time open-ended gen-\nerative object detector.\nAs illustrated in Fig.\n2, open-\nvocabulary detectors [3, 19, 22, 41] require predefined\ncategories as input, whereas previous generative detectors\n[38, 20, 39, 24] first detect objects and then generate texts.\nBy comparison, our proposed RTGen integrated with a text\ngenerator produces objects and corresponding texts con-\ncurrently.\nOur model is adapted from the RT-DETR ar-\nchitecture [42], inheriting its core strength in real- time\ntransformer-based object detection. Based on the preceding\nwork, we introduce a novel Region-Language Decoder (RL-\nDecoder) to replace the original decoder. Specifically, we\ninnovatively integrate a non-autoregressive language model,\nthe Directed Acyclic Transformer (DA-Transformer) [11],\ninto the detection decoder, empowering it to iteratively re-\nfine both object and text data. Furthermore, we leverage\nan open-vocabulary pre-training to enhance the semantic\nunderstanding of the model, facilitating cross-modal align-\nment between visual and textual domains.\nIn summary, our contributions are summarized as fol-\nlows:\n• We introduce a Real-Time GENerative Detection\nTransformer (RTGen), which achieves real-time per-\nformance while eliminating the reliance on predefined\ncategories, making it practical in real-world scenarios.\n• We propose a novel Region-Language Decoder (RL-\nDecoder) to concurrently process object and text infor-\nmation and open-vocabulary pre-training to enhance\ncross-modal alignment.\n• In comparison to other generative models, RTGen not\nonly achieves impressive accuracy but also maintains\nsuperior speed, as shown in Fig. 1, thereby expanding\nthe scope of potential applications.\n2. Related work\n2.1. Open-Vocabulary Object Detection\nOpen-vocabulary object detection (OVD), which enables\nthe identification of objects beyond a predefined set, has re-\ncently drawn significant attention. These OVD approaches\n[19, 44, 27, 3, 22, 41] incorporate a pre-trained language\nmodel with object detectors, enabling the classification of\nregions by calculating the similarity between image re-\ngions and categories. Several methods [40, 43, 8] have fo-\ncused on distilling knowledge from Vision-Language Mod-\nels (VLMs) with richer semantic understanding. For ex-\nObject\nDetector\nText\nGenerator\nObject\nDetector\nText Encoder\nContrastive\nHead\nRTGen\n(a) Open-Vocabulary Object Detector\n(b) Previous Generative Detector\n(c) Our Proposed Generative Detector\nPredefined\nCategories\ndog, chair,\nelephant\nelephant\nFigure 2: Comparison of different object detection frame-\nworks.\n(a) Predefined categories are required for open-\nvocabulary detectors as they classify objects by measuring\nregion-text similarity through a contrastive head. (b) Pre-\nvious generative detectors typically consist of two stages:\nfirst, an object detector detects objects, followed by a text\ngenerator producing corresponding texts. (c) Our proposed\nRTGen incorporates a text generator into a detector, en-\nabling the concurrent generation of detected objects and\ntheir corresponding texts.\nample, OVR-CNN [40] is the first OVD method that incor-\nporates grounded image-caption pre-training to align visual\nand textual features, whereas ViLD [8] distills knowledge\nfrom the pre-trained CLIP model [30] into a two-stage de-\ntector. Another way to improve the capability of detectors\nin open scenarios involves expanding the training dataset.\nFor instance, GLIP [19] unifies object detection and phrase\ngrounding during pre-training, while Grounding DINO [22]\nintegrates the grounded pre-training directly into detection\ntransformers, leveraging cross-modality fusions to enhance\nthe interaction between visual and textual features. How-\never, OVD models still need categories predefined by hu-\nmans during inference, which limits their ability in various\nscenarios.\n2.2. Dense Captioning\nDense captioning aims to generate detailed descriptions\nfor specific areas of an image. The concept was first in-\ntroduced by Johnson et al., who proposed FCLN [13], a\nfully convolutional localization network. The model uses\na CNN and localization layer to extract regions, then pro-\ncesses them with a recognition network and generates de-\nscriptions with an RNN. Subsequently, CapDet [13] applies\nthis concept within an open-world detection setting [14],\nwhere unlabeled objects are passed into the dense caption-\ning head to generate captions. Both GRiT [38] and Det-\n2\n\n\nSelection\nEncoder\nBackbone\nN × d\nN × K × d\nski\npark\n##a\nCross Attention\nRegion-Language Decoder\n× L\nFeed Forward\nSelf Attention\nFeed Forward\nSelf Attention\nDAG Text Head\nski parka\nDetection\nHead\nPositional Text\nEmbedding\nObject\nQueries\nFigure 3: Overview of RTGen. In the model, the input image is first processed by the backbone and encoder to generate image\nfeatures, from which a fixed number of object queries is selected. Then, the selected object queries with initialized positional\ntext embeddings are fed into the Region-Language Decoder (RL-Decoder), where both are iteratively refined. Finally, the\ndetection head generates boxes, while the DAG Text Head organizes the text embedding into a directed acyclic graph (DAG)\nstructure, calculates the transition probabilities, and samples an optimal path to generate texts.\nCLIPv3 [39] attach a text generator to an object detector to\nproduce object descriptions. The difference between dense\ncaptioning and our approach is that our method predicts cat-\negory names, while dense captioning generates descriptive\ncaptions. The evaluation of dense captioning involves both\nAP from object detection and METEOR [1] from machine\ntranslation.\n2.3. Non-autoregressive Translation\nNon-autoregressive translation (NAT) models are de-\nsigned to generate entire sequences in parallel without re-\nlying on previous tokens to increase speed.\nThe NAT\nmodel was first proposed by Gu et al.\n[7], significantly\nenhancing inference speed. However, due to the lack of\ntoken-wise dependencies, its accuracy falls behind the AT\nmodel by a considerable margin. To mitigate this trade-\noff, the Semi-Autoregressive Transformer (SAT) [36] was\nintroduced, which strategically incorporates limited autore-\ngressive steps during decoding to partially model dependen-\ncies while retaining some parallelism. Afterward, CMLM\n[6], an iteration-based method, progressively predicts and\nrefines masked tokens through multiple decoding passes.\nBuilding on these efforts, Directed Acyclic Transformer\n(DA-Transformer) [11] was proposed as a fully NAT model\nthat organizes token generation through a directed acyclic\ngraph (DAG), dynamically balancing parallel computation\nand dependency-aware decoding to optimize both speed and\naccuracy. In our work, we adopt the key technique from\nDA-Transformer, Directed Acyclic Graph (DAG), which al-\nlows us to maintain fast inference speeds without requiring\nground truth inputs during training.\n3. Method\nIn this section, we provide a comprehensive description\nof our proposed RTGen, a real-time generative object de-\ntection model. To improve the integration of a language\nmodel with a detector, we incorporate a non-autoregressive\nlanguage model, the Directed Acyclic Transformer (DA-\nTransformer) [11] into the decoder of our object detector,\nenabling the concurrent processing and generation of de-\ntected objects and corresponding texts. The overview of\nRTGen is shown in Fig. 3. We will present a data formula-\ntion for generative object detection in Sec. 3.1, the overview\nof model architecture in Sec. 3.2, and the training scheme\nin Sec. 3.3.\n3.1. Data Formulation\nIn traditional object detection tasks, the data is formu-\nlated as (x, {bi, ci}N\ni=1), where x ∈R3×H×W denotes\nthe input image, {bi|bi ∈R4}N\ni=1 represents the bound-\ning boxes, and {ci}N\ni=1 corresponds to the category la-\nbels. In this paper, we reformulate the data formulation as\n(x, {bi, yi}N\ni=1), where {yi}N\ni=1 refers to the corresponding\ntexts instead of the labels. Different from the open vocab-\nulary model which needs both image and predefined cate-\ngories as input, RTGen only takes image x and generates\npredicted boxes {ˆbi}K\ni=1 and corresponding texts {ˆyi}K\ni=1.\n3.2. Model Architecture\nThe architecture of our proposed RTGen is illustrated\nin Fig. 3, which consists of a backbone, an encoder, and\na novel Region-Language Decoder (RL-Decoder) with a\nDAG Text Head. The backbone extracts hierarchical feature\nmaps from an input image, and the encoder further refines\n3\n\n\nthese features into context-aware object queries. Then, we\nemploy the RL-Decoder to enhance object queries, while\ninnovatively aggregating object query information into cor-\nresponding positional text embeddings. Finally, a detection\nhead predicts objectness scores and bounding box coordi-\nnates, while a DAG Text Head organizes a directed acyclic\ngraph from text embeddings, calculates token dependencies\nthrough attention mechanisms, and selects the optimal path\nbased on the relationships between tokens.\nRTGen is adapted from RT-DETR [42], inheriting its\nspeed and efficiency. The original RT-DETR consists of\nfour key modules: a pre-trained ResNet [10] backbone for\nmulti-scale feature extraction, an efficient hybrid encoder\nfusing convolutional and attention operations, a decoder\nwith deformable attention [45] for content-aware sampling,\nand auxiliary prediction heads enabling multi-task joint\ntraining through intermediate supervision.\nBased on the\nprevious model, we retain the original backbone and en-\ncoder while integrating the decoder with DA-Trans [11] to\nform the RL-Decoder. Additionally, we replace the default\nprediction heads with class-agnostic detection heads.\nThe decoder is designed as a critical component for\ncross-modal fusion, where object queries propagate visual\nsemantics into the corresponding text embeddings. In each\ndecoder layer, the object queries Ql ∈RN×d are first re-\nfined through self-attention for contextual modeling, cross-\nattention with image features M ∈RM×d for visual feature\naggregation, and a feed-forward network (FFN) for nonlin-\near transformation, as described in Eq. 1. Next, as shown\nin Eq. 2, the refined object queries ˜\nQl are concatenated\nwith the text embeddings Tl ∈RN×K×d, and the result\nis transposed along the first and second dimensions to form\nHl ∈RN×(K+1)×d. Then, to fuse the information from ob-\nject queries into positional text embeddings, self-attention\nand FFN are applied, as demonstrated in Eq. 3. Addition-\nally, we employ a mask to prevent the object queries from\naccessing the image embeddings. Finally, the fused features\n˜\nHl are transposed along the first and second dimensions and\nthen split along the second dimension into object queries\nQl+1 with size 1 and text embeddings Tl+1 with size K, as\nshown in Eq. 4.\n˜Ql = FFN (CrossAttn (SelfAttn (Ql) , M))\n(1)\nHl = Concatdim=2\n\u0010\n˜Ql, Tl\n\u0011⊤(1,2)\n(2)\n˜Hl = FFN (SelfAttn(Hl))\n(3)\nQl+1, Tl+1 = Split\n\u0010\n˜H\n⊤(1,2)\nl\n, dim=2, size=[1, K]\n\u0011\n(4)\nAfter the text embeddings aggregate information from\nobject queries in the decoder, they are subsequently fed into\nthe DAG Text Head, which leverages a non-autoregressive\ntext generation method, Directed Acyclic Graph (DAG)\n[11], enabling parallel text generation. The DAG Text Head\nfirst decomposes the output text embeddings from the de-\ncoder, TL, into N components, {T(n)\nL }N\nn=1, as described in\nEq. 5, where L denotes the layer number of the decoder.\nThen, it models the text embeddings T(n)\nL\nin each compo-\nnent as a directed acyclic graph and computes the transition\nprobability matrix E between each directed acyclic graph\nthrough an attention mechanism. This is formalized in Eq.\n6, adapted from [11], where Q and K are query and key\nprojections of T(n)\nL , obtained through the learnable projec-\ntions WQ and WK, respectively. Finally, we utilize Looka-\nhead decoding from [11] to sample a path from the DAG\nbased on these probabilities, and the target tokens are pre-\ndicted from the text embeddings along the selected path.\nOne significant advantage of this DAG method is its ability\nto be trained without requiring ground truth input. In ob-\nject detection tasks, assigning ground truth texts to detected\nobjects depends on accurately matching detected objects\nwith their corresponding ground truth annotations, which\nprevents obtaining valid text associations before successful\nobject detection. Therefore, to enable the parallel process-\ning of objects and text information, rather than first obtain-\ning object results and then processing the text, the language\nmethods used in the model must be trainable without requir-\ning ground truth input.\nTL ∈RN×K×d\n⇒\nn\nT(n)\nL\n∈RK×d oN\nn=1\n(5)\nE = softmax\n\u0012QK⊤\n√\nd\n\u0013\nQ = T(n)\nL WQ, K = T(n)\nL WK.\n(6)\nTo ensure that text embeddings acquire sufficient object-\nrelated semantic information, object queries must establish\ncross-modal alignment between visual patterns and textual\nrepresentations. To achieve that, we leverage text-to-region\ncontrastive learning. Specifically, we utilize a text encoder\nfrom the pre-trained CLIP model [30] to generate seman-\ntic features for textual inputs.\nEach reference text yi in\nthe input sequence {yi}N\ni=1 is encoded into a semantic fea-\nture vector, dynamically mapping texts to a shared vision-\nlanguage latent space. Then, we employ a text contrastive\nhead from [3] to compute the similarity score between the\nencoded text feature vectors and the object queries. The\ncontrastive head optimizes similarity scores to distinguish\npositive text-query pairs from negatives. By aligning object\n4\n\n\nMethod\nBackbone\nParams (M)\nGFLOPs\nFPS\nmAP\nFCLN [13]\nGeneralized R-CNN\n105\n115\n82.70\n3.7\nGRiT-B [38]\nViT-B\n205\n15638\n0.67\n8.9\nGenerateU-T [20]\nSwin-T\n329\n4581\n1.37\n13.9\nGenerateU-L [20]\nSwin-L\n499\n5331\n1.26\n15.1\nRTGen\nR50\n54\n130\n60.41\n18.6\nTable 1: Evaluation on LVIS minimal [9]. All models were retrained on the LVIS dataset. For FCLN [13], we used open-\nsource code available online, which modified the original Convolutional Network into a Generalized R-CNN. GFLOPs and\nFPS are evaluated on the LVIS minimal set with one batch size. All retraining and testing experiments were conducted on\none NVIDIA RTX 4090 GPU.\nqueries with text feature vectors through contrastive learn-\ning, the model learns to associate visual regions with se-\nmantic concepts, enabling object queries generated by the\ndetector to be infused with adequate visual semantic infor-\nmation. The CLIP text encoder and text contrastive head\nare applied exclusively during the training stage.\n3.3. Training Scheme\nIn this section, we introduce the training scheme of our\nmodel, which consists of two stages: open-vocabulary pre-\ntraining and text-generation fine-tuning.\nOpen-vocabulary pre-training.\nTo enhance cross-\nmodal alignment between visual and textual representa-\ntions, we conduct open-vocabulary pre-training on large-\nscale detection and grounding datasets. In this process, we\nexclusively activate the classification loss Lcls from the text\ncontrastive head for semantic alignment, box regression loss\nLreg for coordinate prediction, and IoU localization loss Liou\nfor spatial accuracy calibration. In the matching stage, the\nHungarian matching cost [16] for proposal assignments is\ncomputed based on classification scores, bounding box re-\ngression, and spatial calibration. The large scale of the pre-\ntraining data ensures exposure to diverse visual and textual\nvariations, enabling the model to learn generalized map-\npings between object appearances and textual concepts.\nText-generation fine-tuning.\nDuring the fine-tuning\nphase stage, we utilize five distinct loss functions. Aside\nfrom the classification loss Lcls, box regression loss Lreg,\nand IoU localization loss Liou, we also incorporate a DAG\ntext loss LDAG from DAG Text Head for text genera-\ntion and objectness classification loss Lobj for foreground-\nbackground discrimination. The classification scores in the\nHungarian matching cost are changed to objectness scores.\nDuring text generation training in DAG Text Head, we\ndisabled glancing training [29] due to its dependency on\nground-truth text, which is unavailable when jointly pro-\ncessing object queries and text embeddings, as detailed in\nSec. 3.2. Additionally, we scale the DAG text loss LDAG\nby an IoU coefficient between predicted and ground-truth\nbounding boxes. This scaling encourages the model to pri-\noritize optimizing the text generation of well-localized pre-\ndictions while suppressing interference from poorly local-\nized samples.\n4. Experiments\nIn this section, we present a comprehensive evaluation\nof the proposed RTGen. We first describe the experimental\nsettings, including datasets, the evaluation metric, and im-\nplementation details. Subsequently, we conduct extensive\ncomparisons with state-of-the-art approaches across multi-\nple metrics to demonstrate the superiority of our method.\n4.1. Experimental Settings\nDatasets.\nIn the open-vocabulary pre-training stage,\nwe adopt detection and grounding datasets, including Ob-\njects365 (V1) [34], GQA [12], and Flickr30k [28]. Follow-\ning the method proposed in [19], we exclude images from\nthe COCO dataset in both GQA and Flickr30k datasets. In\nthe text-generation fine-tune stage, our model is trained on\nthe LVIS [9] training set and evaluated on the LVIS minimal\nvalidation set. The LVIS dataset contains 1,203 categories,\nproviding a comprehensive and diverse database.\nEvaluation Metric.\nTo evaluate the performance of\ngenerative detection, we adopt an evaluation metric from\ndense captioning [13], similar to those in object detection.\nThis metric calculates an overall performance score at var-\nious thresholds, evaluating both the precision of object lo-\ncalization and the accuracy of the corresponding descrip-\ntions. Following [13], the mean Average Precision (mAP)\nis computed by averaging the Average Precision (AP) val-\nues across all pairwise combinations of localization and de-\nscription thresholds. Specifically, localization is evaluated\nusing box IoU thresholds of .3, .4, .5, .6, and 0.7, while de-\nscription accuracy is measured using the METEOR score\n[1] with thresholds of 0, .05, .1, .15, .2, and .25.\nImplementation Details. We utilize the Hugging Face\nTransformers library [37] for model implementation. Fol-\nlowing RT-DETR-R50 [42], we adopt ResNet-50 [10] as the\n5\n\n\nbackbone, an efficient hybrid encoder as the encoder, and\nconfigure our decoder with 6 layers and 8 attention heads.\nThe number of object queries selected from encoder fea-\ntures is set to 300, and the number of text embeddings for\neach object query is set to 16. According to [11], where the\ngraph size is set to 8 times the source length, we assume the\nsource length is 2. The pre-trained CLIP text encoder [30]\nis employed in a frozen state exclusively during training.\nIn the open-vocabulary pre-training stage, an AdamW op-\ntimizer [25] with a learning rate and weight decay are both\nset to 0.0001. In the text-generation fine-tuning stage, the\nlearning rate is adjusted to 0.00005. We pre-train the model\nby 62k iterations with a batch size of 8 and fine-tune the\nmodel by 240000 iterations with the same batch size. The\nloss weight factors for Lreg, Liou, Lobj, Lcls, LDAG are set to\n5.0, 2.0, 2.0, 1.0, 1.0. To balance the influence across the\ndifferent losses, LDAG is not normalized by the number of\ntarget boxes. Our model is pre-trained on 4 NVIDIA A100\nGPUs and fine-tuned on one NVIDIA RTX 4090 GPU.\n4.2. Generative Object Detection\nIn GenerateU [20], the model is trained on the Visual\nGenome (VG) dataset [15] with object annotations and eval-\nuated on LVIS [9] in a zero-shot setting. We argue that the\nVG dataset covers a broader range of objects than LVIS, and\nwith the absence of input detection texts in open-vocabulary\ndetection to control the detected categories, models trained\non VG tend to detect objects outside the intended evalua-\ntion scope, leading to a negative impact on the evaluation\nresults. Thus, we utilize the LVIS dataset for both training\nand evaluation. After training, we evaluate our model on\nthe minimal LVIS dataset with the metric from dense cap-\ntioning [13]. We calculate the average GFLOPs and FPS for\nall evaluation models during the evaluation phase to assess\ntheir computational efficiency and inference speed.\nOur proposed RTGen demonstrates state-of-the-art per-\nformance compared to other methods in both accuracy and\nspeed. Despite employing a simple R50 backbone [10], RT-\nGen outperforms models with heavy Swin [23] and ViT\n[5] backbones. Notably, RTGen achieves a 3.5 mAP im-\nprovement over GenerateU-L [20], the second-place model.\nMoreover, compared to GRiT-B [38] and GenerateU with T\nand L backbones, RTGen exhibits tremendous advantages\nin speed, outperforming them by factors of 90, 48, and 44\ntimes, respectively. Similarly, RTGen also shows signifi-\ncant advantages in GFLOPs. While RTGen is slightly be-\nhind FLCN [13] in speed, it significantly outperforms in re-\nsults for 14.9 mAP. Remarkably, our model has the fewest\nparameters, with a total of 54M, compared to all other mod-\nels. These advantages can be attributed to integrating the\nnon-autoregressive language model into the detection de-\ncoder with shared parameters in self-attention layers.\nPre-training\nBox mAP\nGeneration mAP\n✗\n18.3\n7.2\n✓\n29.9\n18.6\nTable 2: Ablations on incorporating open-vocabulary pre-\ntrain.\nBox mAP represents the mean average precision\ncalculated based on the accuracy of predicted bounding\nboxes. Generation mAP represents the evaluation metric\nfrom dense captioning [13].\nContrastive loss\nBox mAP\nGeneration mAP\n✗\n29.3\n17.5\n✓\n29.9\n18.6\nTable 3: Ablations on incorporating a contrastive loss gen-\nerated by text-contrastive head during fin-tuning stage.\n4.3. Ablation Study\nIn order to analyze RTGen, we conduct extensive abla-\ntion studies focusing on two primary aspects: pre-training\nand contrastive loss.\nOpen-vocabulary pre-training.\nAs shown in Table\n2, we investigate the advantages of open-vocabulary pre-\ntraining.\nWith open-vocabulary pre-training, the model\nshows a significant performance improvement, leading to\nan +11.4 mAP improvement in generation mAP. During the\npretraining phase, the model effectively establishes cross-\nmodal alignment between the visual and textual domains.\nIn addition, RTGen achieves a significant improvement of\n+11.6 percentage points in box mAP. The pre-training on\nlarge detection and grounding datasets significantly boosts\nRTGen performance.\nContrastive Learning during fine-tuning.\nIn Table\n3, we compare the effects of ablation on contrastive loss\nduring the text-generation fine-tuning stage. Although the\ncross-model alignment has been well-established during the\nopen-vocabulary pre-training, our results show that RTGen\ncan still benefit from contrastive loss during fine-tuning.\nBoth Box mAP and Generation mAP gain +0.6 and +1.1,\nrespectively.\n4.4. Visualization\nIn Fig. 4, we provide visualization of RTGen perfor-\nmance on the LVIS dataset. The figure displays sample im-\nages with detected objects highlighted by bounding boxes,\neach accompanied by a generated relative text description,\nwhich accurately describes the object category. This visual-\nization demonstrates the capability of RTGen to accurately\ndetect objects, comprehend their semantic properties, and\ngenerate precise textual descriptions that reflect their cate-\n6\n\n\nFigure 4: Visualization results from RTGen on LVIS.\ngories.\n5. Conclusion\nIn this paper, we introduce a real-time transformer-based\ngenerative object detector, called RTGen, which only takes\nimage inputs and generates detected objects and related\ntexts, completely eliminating the dependency on predefined\ncategories. Our proposed RTGen comprises a novel RL-\nDecoder to concurrently process object and text informa-\ntion by integrating a non-autoregressive language model,\nDA-Trans [11], into the detection decoder.\nIn addition,\nwe leverage open-vocabulary pre-training to enhance cross-\nmodel alignment between visual and textual domains. Ex-\ntensive experiments demonstrate the remarkable results of\nour method, achieving state-of-the-art results in both accu-\nracy and speed.\nReferences\n[1] Satanjeev Banerjee and Alon Lavie. Meteor: An auto-\nmatic metric for mt evaluation with improved correla-\ntion with human judgments. In Proceedings of the acl\nworkshop on intrinsic and extrinsic evaluation mea-\nsures for machine translation and/or summarization,\npages 65–72, 2005. 3, 5\n[2] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,\nOwais Khan Mohammed, Kriti Aggarwal, Subho-\njit Som, Songhao Piao, and Furu Wei.\nVlmo:\nUnified vision-language pre-training with mixture-of-\nmodality-experts.\nAdvances in Neural Information\nProcessing Systems, 35:32897–32912, 2022. 1\n[3] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu,\nXinggang Wang, and Ying Shan. Yolo-world: Real-\ntime open-vocabulary object detection. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16901–16911, 2024.\n2, 4\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. Palm: Scaling language mod-\neling with pathways.\nJournal of Machine Learning\nResearch, 24(240):1–113, 2023. 1\n[5] Alexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk\nWeissenborn,\nXiaohua\nZhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer,\nGeorg Heigold,\nSylvain Gelly,\net al.\nAn image is worth 16x16 words:\nTransformers\nfor image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 6\n7\n\n\n[6] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. Mask-predict: Parallel decoding of\nconditional masked language models. arXiv preprint\narXiv:1904.09324, 2019. 3\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor OK Li, and Richard Socher. Non-autoregressive\nneural\nmachine\ntranslation.\narXiv\npreprint\narXiv:1711.02281, 2017. 2, 3\n[8] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin\nCui.\nOpen-vocabulary object detection via vision\nand language knowledge distillation. arXiv preprint\narXiv:2104.13921, 2021. 1, 2\n[9] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis:\nA dataset for large vocabulary instance segmentation.\nIn Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 5356–\n5364, 2019. 5, 6\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun.\nDeep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\n4, 5, 6\n[11] Fei Huang, Hao Zhou, Yang Liu, Hang Li, and Min-\nlie Huang.\nDirected acyclic transformer for non-\nautoregressive machine translation. In International\nConference on Machine Learning, pages 9410–9428.\nPMLR, 2022. 2, 3, 4, 6, 7\n[12] Drew A Hudson and Christopher D Manning. Gqa: A\nnew dataset for real-world visual reasoning and com-\npositional question answering. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 6700–6709, 2019. 5\n[13] Justin Johnson, Andrej Karpathy, and Li Fei-Fei.\nDensecap: Fully convolutional localization networks\nfor dense captioning. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 4565–4574, 2016. 1, 2, 5, 6\n[14] KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and\nVineeth N Balasubramanian. Towards open world ob-\nject detection. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 5830–5840, 2021. 2\n[15] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yan-\nnis Kalantidis, Li-Jia Li, David A Shamma, et al. Vi-\nsual genome: Connecting language and vision using\ncrowdsourced dense image annotations. International\njournal of computer vision, 123:32–73, 2017. 6\n[16] Harold W Kuhn. The hungarian method for the as-\nsignment problem. Naval research logistics quarterly,\n2(1-2):83–97, 1955. 5\n[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven\nHoi.\nBlip-2:\nBootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In International conference on machine\nlearning, pages 19730–19742. PMLR, 2023. 1\n[18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and genera-\ntion. In International conference on machine learning,\npages 12888–12900. PMLR, 2022. 1\n[19] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang,\nJianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan\nWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded language-image pre-training. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10965–10975, 2022. 1,\n2, 5\n[20] Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, and\nJianfei Cai.\nGenerative region-language pretraining\nfor open-ended object detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 13958–13968, 2024. 1, 2, 5,\n6\n[21] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming\nHe, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pages 2117–2125, 2017. 1\n[22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li,\nHao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jian-\nwei Yang, Hang Su, et al. Grounding dino: Marry-\ning dino with grounded pre-training for open-set ob-\nject detection. In European Conference on Computer\nVision, pages 38–55. Springer, 2025. 2\n[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer:\nHierarchical vision transformer using\nshifted windows.\nIn Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages\n10012–10022, 2021. 6\n[24] Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu,\nPengzhen Ren, Wei Zhang, Shen Zhao, and Xiaodan\nLiang. Capdet: Unifying dense captioning and open-\nworld detection pretraining.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 15233–15243, 2023. 1, 2\n[25] Ilya\nLoshchilov\nand\nFrank\nHutter.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017. 6\n[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\nVilbert: Pretraining task-agnostic visiolinguistic rep-\n8\n\n\nresentations for vision-and-language tasks. Advances\nin neural information processing systems, 32, 2019. 1\n[27] Matthias Minderer, Alexey Gritsenko, Austin Stone,\nMaxim Neumann, Dirk Weissenborn, Alexey Doso-\nvitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa\nDehghani, Zhuoran Shen, et al.\nSimple open-\nvocabulary object detection. In European conference\non computer vision, pages 728–755. Springer, 2022.\n1, 2\n[28] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik.\nFlickr30k entities: Collecting region-to-\nphrase correspondences for richer image-to-sentence\nmodels. In Proceedings of the IEEE international con-\nference on computer vision, pages 2641–2649, 2015.\n5\n[29] Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin\nQiu, Weinan Zhang, Yong Yu, and Lei Li.\nGlanc-\ning transformer for non-autoregressive neural machine\ntranslation. arXiv preprint arXiv:2008.07905, 2020. 5\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pages 8748–8763. PmLR, 2021. 2,\n4, 6\n[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language mod-\nels are unsupervised multitask learners. OpenAI blog,\n1(8):9, 2019. 1\n[32] Joseph Redmon, Santosh Divvala, Ross Girshick, and\nAli Farhadi. You only look once: Unified, real-time\nobject detection.\nIn Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 779–788, 2016. 1\n[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun.\nFaster r-cnn: Towards real-time object detec-\ntion with region proposal networks.\nIEEE transac-\ntions on pattern analysis and machine intelligence,\n39(6):1137–1149, 2016. 1\n[34] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng,\nGang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Ob-\njects365: A large-scale, high-quality dataset for ob-\nject detection. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 8430–\n8439, 2019. 5\n[35] Hugo Touvron,\nThibaut Lavril,\nGautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al.\nLlama:\nOpen and effi-\ncient foundation language models.\narXiv preprint\narXiv:2302.13971, 2023. 1\n[36] Chunqi Wang, Ji Zhang, and Haiqing Chen. Semi-\nautoregressive neural machine translation.\narXiv\npreprint arXiv:1808.08583, 2018. 3\n[37] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz,\net al. Transformers: State-of-the-art natural language\nprocessing.\nIn Proceedings of the 2020 conference\non empirical methods in natural language processing:\nsystem demonstrations, pages 38–45, 2020. 5\n[38] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe\nGan, Zicheng Liu, Junsong Yuan, and Lijuan Wang.\nGrit: A generative region-to-text transformer for ob-\nject understanding. In European Conference on Com-\nputer Vision, pages 207–224. Springer, 2025. 1, 2, 5,\n6\n[39] Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang,\nHang Xu, Wei Zhang, Zhenguo Li, and Dan Xu. Det-\nclipv3: Towards versatile generative open-vocabulary\nobject detection.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 27391–27401, 2024. 1, 2, 3\n[40] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and\nShih-Fu Chang. Open-vocabulary object detection us-\ning captions. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 14393–14402, 2021. 1, 2\n[41] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-\nChun Chen, Liunian Li, Xiyang Dai, Lijuan Wang,\nLu Yuan, Jenq-Neng Hwang, and Jianfeng Gao.\nGlipv2: Unifying localization and vision-language un-\nderstanding. Advances in Neural Information Process-\ning Systems, 35:36067–36080, 2022. 2\n[42] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei,\nGuanzhong Wang, Qingqing Dang, Yi Liu, and Jie\nChen. Detrs beat yolos on real-time object detection.\nIn Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 16965–\n16974, 2024. 2, 4, 5\n[43] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-\nyuan Li, Noel Codella, Liunian Harold Li, Luowei\nZhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip:\nRegion-based language-image pretraining.\nIn Pro-\nceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 16793–16803,\n2022. 2\n[44] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKr¨ahenb¨uhl, and Ishan Misra.\nDetecting twenty-\nthousand classes using image-level supervision.\nIn\n9\n\n\nEuropean conference on computer vision, pages 350–\n368. Springer, 2022. 1, 2\n[45] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection.\narXiv\npreprint arXiv:2010.04159, 2020. 4\n10\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20622v1.pdf",
    "total_pages": 10,
    "title": "RTGen: Real-Time Generative Detection Transformer",
    "authors": [
      "Chi Ruan"
    ],
    "abstract": "While open-vocabulary object detectors require predefined categories during\ninference, generative object detectors overcome this limitation by endowing the\nmodel with text generation capabilities. However, existing generative object\ndetection methods directly append an autoregressive language model to an object\ndetector to generate texts for each detected object. This straightforward\ndesign leads to structural redundancy and increased processing time. In this\npaper, we propose a Real-Time GENerative Detection Transformer (RTGen), a\nreal-time generative object detector with a succinct encoder-decoder\narchitecture. Specifically, we introduce a novel Region-Language Decoder\n(RL-Decoder), which innovatively integrates a non-autoregressive language model\ninto the detection decoder, enabling concurrent processing of object and text\ninformation. With these efficient designs, RTGen achieves a remarkable\ninference speed of 60.41 FPS. Moreover, RTGen obtains 18.6 mAP on the LVIS\ndataset, outperforming the previous SOTA method by 3.5 mAP.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}