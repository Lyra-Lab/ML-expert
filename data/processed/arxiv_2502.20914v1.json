{
  "id": "arxiv_2502.20914v1",
  "text": "Published as a conference paper at ICLR 2025\nEVERYTHING, EVERYWHERE, ALL AT ONCE:\nIS MECHANISTIC INTERPRETABILITY IDENTIFIABLE?\nMaxime M´eloux, Franc¸ois Portet, Silviu Maniu, Maxime Peyrard\nUniversit´e Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n{melouxm,portetf,manius,peyrardm}@univ-grenoble-alpes.fr\nABSTRACT\nAs AI systems are increasingly deployed in high-stakes applications, ensur-\ning their interpretability is essential. Mechanistic Interpretability (MI) aims to\nreverse-engineer neural networks by extracting human-understandable algorithms\nembedded within their structures to explain their behavior. This work systemati-\ncally examines a fundamental question: for a fixed behavior to explain, and under\nthe criteria that MI sets for itself, are we guaranteed a unique explanation? Draw-\ning an analogy with the concept of identifiability in statistics, which ensures the\nuniqueness of parameters inferred from data under specific modeling assumptions,\nwe speak about the identifiability of explanations produced by MI. We identify\ntwo broad strategies to produce MI explanations: (i) “where-then-what”, which\nfirst detects a subset of the network (a circuit) that replicates the model’s behavior\nbefore deriving its interpretation, and (ii) “what-then-where”, which begins with\ncandidate explanatory algorithms and searches in the activation subspaces of the\nneural model where the candidate algorithm may be implemented, relying on no-\ntions of causal alignment between the states of the candidate algorithm and the\nneural network. We systematically test the identifiability of both strategies us-\ning simple tasks (learning Boolean functions) and multi-layer perceptrons small\nenough to allow a complete enumeration of candidate explanations. Our experi-\nments reveal overwhelming evidence of non-identifiability in all cases: multiple\ncircuits can replicate model behavior, multiple interpretations can exist for a cir-\ncuit, several algorithms can be causally aligned with the neural network, and a\nsingle algorithm can be causally aligned with different subspaces of the network.\nWe discuss whether the unicity intuition is necessary. One could adopt a prag-\nmatic stance, requiring explanations only to meet predictive and/or manipulability\nstandards. However, if unicity is considered essential, e.g., to provide a sense of\nunderstanding, we also discuss less permissive criteria. Finally, we also refer to\nthe inner interpretability framework that demands explanation to be validated by\nmultiple complementary criteria. This work aims to contribute constructively to\nthe ongoing effort to formalize what we expect from explanations in AI.\n1\nINTRODUCTION\nInterpretability in machine learning spans diverse goals and methods (Molnar, 2022; Carvalho et al.,\n2019; Teney et al., 2022), from creating inherently interpretable models to applying post hoc tech-\nniques to explain model decisions. Mechanistic interpretability (MI) aims to reverse-engineer mod-\nels to reveal simple, human-interpretable algorithms embedded in neural network structure (Olah\net al., 2020). MI is focused on generating what we call computational abstractions, where complex\nneural networks’ behaviors are explained by simpler algorithms that track the internal computations\n(Olah et al., 2020). A computational abstraction – a mechanistic explanation – has two components:\n(a) what is the explanatory algorithm, and (b) where in the computational structure is this algo-\nrithm embedded? Given the intractability of exhaustively searching all possible algorithms across\nall subsets of a neural network, researchers have developed methods with different assumptions\nand trade-offs. We categorize these methods into two broad strategies. The first, which we call\nwhere-then-what, focuses on finding a subset of the network – a circuit – that captures most of the\ninformation flow from inputs to outputs. Once this circuit is identified, typically using heuristics,\n1\narXiv:2502.20914v1  [cs.LG]  28 Feb 2025\n\n\nPublished as a conference paper at ICLR 2025\nthe next step is to interpret its components (features) to derive the explanatory algorithm (Dunefsky\net al., 2024; Davies and Khakzar, 2024; Conmy et al., 2023). The second approach, which we name\nwhat-then-where, starts by identifying candidate algorithms and then searches subspaces in the neu-\nral network where the algorithm may be implemented. This is performed using causal alignment\nbetween the explanatory algorithm’s states and the network’s internal states and typically requires\napproximation algorithms (Geiger et al., 2022a;b). Each strategy relies on specific criteria to assess\ncandidate explanations. For instance, circuits can be evaluated by their circuit error, which quan-\ntifies how closely the circuit’s predictions match the full model ones (Conmy et al., 2023). In the\nwhat-then-where strategy, candidate algorithms are compared based on causal alignment measures\nlike intervention interchange accuracy (IIA), which assesses how well the algorithm’s states remain\naligned with the network’s internal states after counterfactual manipulations of the states.\nIn this work, we question a property of explanation that appears to be tacitly taken for granted: do\nMI criteria guarantee a unique explanation of a fixed behavior? The concept of identifiability is\nwell-established in statistics, where a model is identifiable if its parameters can be uniquely inferred\nfrom data under a given set of modeling assumptions (e.g., Rothenberg, 1971). By analogy, we\nextend this terminology to interpretability, defining the identifiability of explanation as the property\nwhere, under fixed assumptions of validity, a unique explanatory algorithm satisfies the criteria.\nSpecifically, we ask the following questions: In the where-then-what strategy, (i) is the circuit (the\n“where”) unique? (ii) Is a given circuit’s grounding interpretation (the “what”) unique? In the what-\nthen-where strategy, (iii) is the causally-aligned algorithm (the “what”) unique? (iv) For a given\nalgorithm, is there a unique subspace of the neural network (the “where”) that is causally aligned?\nWe stress-test the identifiability properties of current MI criteria by conducting experiments in a\ncontrolled, small-scale setting. Using simple tasks like learning Boolean functions and very small\nmulti-layer perceptrons (MLPs), we search for Boolean circuit explanations – aiming to discover\nwhich succession of logic gates is implemented by the MLPs. This setup allows us to exhaustively\nenumerate incompatible candidate explanations and test them with existing criteria. Our experiments\nreveal non-identifiability at every stage of the MI process. Specifically, we find that: (i) Multiple\ncircuits can perfectly replicate the model’s behavior (with a circuit error of zero), (ii) for a given\ncircuit, multiple valid interpretations exist, (iii) several algorithms can be perfectly causally aligned\nwith the neural computation (IIA of one), and (iv) for a given causally aligned algorithm, multiple\nsubspaces of the neural network can be equally aligned (IIA = 1).\nIn the discussion, we revisit whether unicity is necessary. We discuss alternative criteria and per-\nspectives that do not require modifying existing criteria, such as requiring explanations only to meet\npredictive and/or manipulability standards. However, if unicity is considered essential, e.g., to pro-\nvide a sense of understanding, we also discuss less permissive criteria. Finally, we also refer to the\ninner interpretability framework (Vilas et al., 2024) that requires an explanation to be validated by\nmultiple complementary criteria. We hope our work contributes constructively to the ongoing effort\nto develop rigorous definitions for what it means to explain a complex neural network.\n2\nBACKGROUND\n2.1\nMECHANISTIC INTERPRETABILITY\nMechanistic interpretability rests on the key assumptions that a neural network’s behavior can be\nexplained by a simpler algorithm than the full network, and that a sparse subset of the network\nexecutes this algorithm. Previous research has given support to these assumptions: pruning studies\n(Gale et al., 2019; Ma et al., 2023; Sun et al., 2024) and the lottery ticket hypothesis (Frankle and\nCarbin, 2019; Liu et al., 2024) suggest that networks are often overparameterized, and only a fraction\nof neurons and connections are critical to the final performance. Training sub-networks (Yuan et al.,\n2019) to approximate the full model (Liao and Kyrillidis, 2022), similar to dropout (Srivastava et al.,\n2014), supports the idea that sub-networks can approximate the full network’s behavior well.\nThis search for interpretable circuits is inspired by neuroscience, which has long sought to uncover\nneural circuits that explain observed behaviors (Yuste, 2008). Once the neural circuit is discovered,\nresearchers focus on interpreting the functional roles of each component in the brain (Yuste, 2008).\nResearch in computer vision has already shown that some nodes within neural networks compute\n2\n\n\nPublished as a conference paper at ICLR 2025\ninterpretable features (Olah et al., 2017). Connections between such features, also called circuits,\ncan be compact explanations of model behavior (Olah et al., 2020; Carter et al., 2019; Dreyer et al.,\n2024). Finally, recent work has applied mechanistic interpretability to LLMs (Elhage et al., 2021),\nespecially in transformer models (Templeton et al., 2024; Bricken et al., 2023; Vilas et al., 2023). For\nexample, Wang et al. (2022) identified a circuit responsible for Indirect Object Identification (IOI)\nin transformers, highlighting the potential for mechanistic explanations of complex LLM behaviors.\n2.2\nDEFINITIONS\nF\n0\n0\n1\nF\n1\n0\n1\nF\n2\na\nb\nc\nBase \nNeural Network\n1. Circuit\n0 1\n0 1\na\nb\nc\nI\nF0\nF1\nF2\n0\nComputational\nAbstraction\n2. Mapping 𝝉\nneural activations ↔feature values\nFigure 1: Illustration of the computational abstraction\ncomponents within a neural network. The circuit rep-\nresents a subgraph, and the mapping specifies the high-\nlevel features computed by the circuit, detailing how\ntheir values arise from low-level neural activations. To-\ngether, these form the computational abstraction (ex-\nplanation of the neural network). Here, feature F2 has\nthree possible values and is defined within the 2D ac-\ntivation space of two neurons. Features F0 and F1 are\nbinary variables, each assigned to a single neuron. F0\ncovers the entire activation space and F1 only maps\nspecific intervals, leaving some activations unassigned.\nA satisfactory mechanistic explanation of\na model’s behavior consists of two com-\nponents: the what, a high-level algorithm\nthat closely approximates the model’s be-\nhavior and tracks its internal computation,\nand the where, specifying how and where\nthis algorithm is embedded in the model’s\nlow-level neural computation.\nWe refer to the combination of an explana-\ntory algorithm and the mapping between\nthe high- and low-level states as a compu-\ntational abstraction. This is an abstraction\nas it simplifies the neural network’s com-\nputation, focusing on a subset of the com-\nputational graph and abstracting neural ac-\ntivations into simpler, high-level features.\nFor example, consider the mechanistic ex-\nplanation of how a vision algorithm recog-\nnizes rectangles. We might identify a com-\nputational abstraction where certain mod-\nules perform edge detection, others detect\nright angles, and a final component applies\nan AND logic gate to confirm the pres-\nence of four right angles. This abstraction\nspecifies the algorithm and how and where\nlow-level neural activations correspond to\nhigh-level features of the algorithm.\nIn\nthis work, we interchange the terms expla-\nnation and computational abstraction.\nFormally, we define a computational ab-\nstraction A as a tuple (S, τ), where S is the circuit, the subset of the neural network’s computational\ngraph responsible for the behavior of interest, and τ is the mapping between the states of the circuit\nand the states of the variables of the algorithm, which specifies how to interpret the computational\nfunction of the circuit’s components. We now proceed to define the circuit and mapping formally.\nDefinition 1 (Circuit). Let G = (V, E) represent the computational graph of a neural network,\nwhere V is the set of nodes (neurons) and E ⊆V × V is the set of edges (connections between\nneurons). A circuit S = (VS, ES) is a subgraph of G that contains at least one path from a subset\nof input nodes to a subset of output nodes.\nDefinition 2 (Mapping (τ)). A mapping between low-level values taken by neurons and high-level\nvalues taken by the variables of the explanatory algorithm consists of a set of K surjective maps,\none for each high-level variable. Each associates the neural network activations with the values of\nthe corresponding high-level variable. For a group of neurons Vj in the neural network, mapped to a\nhigh-level variable Aj with possible values {f0, . . . , fm}, the mapping τj : R|Vj| →{f0, . . . , fm}\nassigns a vector of activations to one of the possible values of Aj. Each mapping should be surjective\n\u0000∀fi, ∃h ∈R|Vj| : τj(h) = fi\n\u0001\nand with a non-empty pre-image\n\u0000∀fi, τ −1\nj\n(fi) ̸= ∅\n\u0001\nThese conditions ensure that all high-level values can be realized by some set of low-level activa-\ntions.\n3\n\n\nPublished as a conference paper at ICLR 2025\nIn practice, we are interested in mappings that satisfy a consistency requirement. Intuitively, consis-\ntency means that if we first perform part of the computation using the neural network and then apply\nthe mapping to get the state of a high-level variable, the outcome should be identical to applying the\nmapping and then performing the computation of the high-level algorithm. The computations in the\nneural network and the high-level algorithm should align consistently according to the mapping.\nDefinition 3 (Consistent Mapping). Let τ be a mapping between groups of low-level neurons {Vj}\nand their corresponding high-level variables {Aj}. The mapping τ is said to be consistent if for any\nhigh-level variable Aj, with parents PAj, the following diagram commutes:\nPAj\nAj\nR|VP Aj |\nR|Vj|\nAlg.\nNN\nτP Ai\nτj\nHere: τP Aj represents the application of τ to each variable in PAj; NN refers to the computation\nbetween the low-level neural network states; and Alg. refers to the computation between high-level\nvariables governed by the explanatory algorithm.\nPrevious works have explored various types of high-level features and representational abstractions,\nincluding mappings based on “directions in activation space” or specific points within activation\nsubspaces (Olah et al., 2020; 2018; Bereska and Gavves, 2024). This work focuses on explana-\ntory algorithms represented as Boolean circuits, where high-level features are binary (0 or 1). The\nmappings specify which activations correspond to 0 and 1. Boolean circuits are computationally\nuniversal and thus sufficient to demonstrate identifiability issues in existing MI criteria.\n2.3\nAPPROACHES TO CIRCUIT DISCOVERY\nWe identify and describe two strategies for reverse-engineering neural networks: the where-then-\nwhat and what-then-where approaches.\nWHERE-THEN-WHAT\nMethods from this strategy first aim to identify a circuit that replicates the behavior of the full model\nwell. Once a circuit is found, the next step is to interpret its components to uncover the high-level\nalgorithm being implemented (Dunefsky et al., 2024; Davies and Khakzar, 2024). The evaluation\ncriteria for circuits is how well they replicate the full model’s behavior for the input of interest.\nDefinition 4 (Circuit Error). Let S be the function computed by a circuit and g the function computed\nby the model on which the circuit is defined. For the input set x, the error of the circuit S is:\n1 −\n1\n|x|\nP\nx∈x 1[S(x) = g(x)]\nIn the case of perturbed inputs, it can also be defined via the KL divergence between the logits of\nthe circuit and the model (Conmy et al., 2023).\nIn practice, circuit search relies on causal mediation analysis, which seeks to isolate the subset of\nthe network that carries the information from the inputs to the output. Since it is computationally\nintractable to enumerate all possible circuits in complex models (Adolfi et al., 2024a), existing meth-\nods focus on computing mediation formulas for individual components to decide their inclusion in\nthe circuit (Vig et al., 2020; Meng et al., 2022; Monea et al., 2024; Kram´ar et al., 2024; Conmy et al.,\n2023; Geva et al., 2023; Syed et al., 2023).\nA combination of data analysis and human input is typically used to interpret candidate circuits.\nFor example, activation maximization identifies inputs that maximally activate a component, which\nhelps clarify its function (Zhou et al., 2016; Zeiler and Fergus, 2014; Simonyan et al., 2014). This\ntechnique has been extended to modern LLMs (Peyrard et al., 2021; Jawahar et al., 2019; Dai et al.,\n2022). However, polysemantic neurons, which encode multiple concepts simultaneously (Templeton\net al., 2024; Bricken et al., 2023), complicate the interpretation of LLMs components. For a broader\noverview of these challenges, we refer readers to the following surveys: Sajjad et al. (2022); Khakzar\net al. (2021). In this work, we use the concept of consistent mapping as the objective evaluation of\nthe quality of an interpretation.\n4\n\n\nPublished as a conference paper at ICLR 2025\nWHAT-THEN-WHERE\nMethods from this strategy first hypothesize a candidate high-level algorithm and then search for\nmappings between the states of this algorithm and subspaces of the neural activations. The goal is\nto identify mappings where the high-level and low-level states are causally aligned, meaning they\nrespond similarly under interventions.\nGiven a candidate high-level algorithm A, neural activations H, and a mapping τ defined between\nthem, counterfactual interventions are performed on the inner variables of A, and corresponding\ninterventions are applied to H via τ. Intervention interchange accuracy (IIA) (Geiger et al., 2022b)\nis then defined for each high-level variable and measures the similarity of outputs in A and H after\nintervening (metric for causal alignment). We give in Appendix A a complete, formal definition.\nA perfect IIA score (1) for all variables indicates that all possible interventions produce the same ef-\nfect in low-level and high-level models. In practice, exhaustive enumeration is often impractical, and\nIIA is approximated using randomly sampled inputs (Geiger et al., 2022b). Similarly to the map-\nping consistency defined above, perfect causal alignment requires diagram commutation between\nlow- and high-level models under interventions.\nSearching for causal alignment between high-level models and neural activations can be computa-\ntionally expensive, as it often requires testing many potential mappings. The Distributed Align-\nment Search method (Geiger et al., 2024) addresses this challenge by employing gradient de-\nscent to search for alignments efficiently. This approach also allows for distributed representa-\ntions, where multiple neurons represent a single high-level variable. Indeed, an underlying assump-\ntion of IIA is that the neural activations corresponding to distinct high-level variables are disjoint:\n∀x, y ∈V, τ −1(x) ∩τ −1(y) = ∅, which may not occur in real-world examples (Olah et al., 2020).\nCurrently, no systematic method exists for choosing which candidate algorithms to test. Previous\nwork (Wu et al., 2023) has manually proposed a few candidates, but the vast space of possible\nalgorithms makes this an open challenge.\n2.4\nEXPLANATION “IDENTIFIABILITY”\nThe assumption of explanatory unicity – the idea that there exists a single, unique explanation for\na given phenomenon – is not only implicit in the practice of mechanistic interpretability (see rel-\nevant citations in Appendix C) but also rooted in human cognitive and psychological tendencies\n(Trout, 2007; Waskan, 2024; Gopnik, 2000). Humans demonstrate a cognitive preference for coher-\nent explanations that integrate disparate observations into a unified narrative (e.g., Friedman, 1974;\nKitcher, 1962; 1981; Schurz, 1999; Kveraga et al., 2007). This preference aligns with the psy-\nchological need for cognitive closure, defined as the desire for a definitive conclusion (Kruglanski,\n1989). Multiple incompatible explanations disrupt coherence, leading to ambiguity and a sense of\nunresolved understanding.\nConversely, in the philosophy of science, explanatory pluralism acknowledges that the world is\ntoo complex to be fully described by a single comprehensive explanation (Kellert et al., 2006; Po-\ntochnik, 2017). Multiple explanations often coexist without conflict because they address different\nexplanatory goals (e.g., explaining distinct behaviors) or employ different simplification strategies\n(e.g., differing levels of abstraction Marr and Poggio, 1976). However, in this work, we deliber-\nately search for conflicting explanations by fixing both the explanatory goal and the simplification\nstrategies, as the ones defined by MI criteria.\nIdentifiability and incompatible explanations.\nAs mentioned in Section 1, we borrow the term of identifiability from the field of statistics (Rothen-\nberg, 1971), defining identifiability of explanation as the property where a unique explanation is\nvalid under fixed standards of validity. An MI strategy is not identifiable if its standards of validity\ndo not discriminate between two incompatible explanations.\nWe define two explanations as incompatible or conflicting if they share the same explanatory goal\nand simplification strategy, but posit different computational abstractions. In our context, the ex-\nplanatory goal is fixed: explaining the specific input-output behavior of a trained MLP. The simplifi-\ncation strategy is also fixed, corresponding to one of two predefined strategies to find computational\nabstractions: the what-then-where or the where-then-what defined above.\n5\n\n\nPublished as a conference paper at ICLR 2025\nBase Neural Network\nTrain an MLP to implement XOR\n𝐴= 0|1 + 𝒩(0, ℰ) \n𝐵= 0|1 + 𝒩(0, ℰ) \n𝐶= 𝑟𝑜𝑢𝑛𝑑𝐴⊕ 𝑟𝑜𝑢𝑛𝑑(𝐵) \n𝐴\n𝐵\n𝐶\nWhere-then-what: searching circuits \nthat support behavior and then \ninterpreting their features via grounding\n…\nCircuits\nComputational Abstractions\n…\n85 unique circuits with perfect accuracy\n25 unique abstractions with exact grounding \nbetween neural activations and gate definitions\nWhat-then-where: searching for different \ncandidate algorithms causally aligned in the \nactivations of the base neural network\nCandidate Algorithms\nA\nB\n𝐀𝐍𝐃\n𝐍𝐀𝐍𝐃\n𝐎𝐑\n𝐎𝐑\nA\nB\n¬𝐀 𝐀𝐍𝐃 𝐁\n𝐀 𝐀𝐍𝐃 ¬𝐁\n2 candidates are tested\nComputational Abstractions\n159 perfect mappings (IIA=1)\n…\nExample of 2 perfect mappings for one algorithm\nFigure 2: Illustration of identifiability problems using the XOR example. We train a small\nMLP with two hidden layers of size 3 to compute the XOR function perfectly. The figure shows\nthe outcome of stress-testing the two reverse-engineering strategies: Top: For the what-then-where\nstrategy, we enumerate all subsets of neurons searching for subsets causally aligned with interme-\ndiate variables of candidate algorithms, with alignment measured by IIA. Even testing only two\ncandidate algorithms, we find perfect implementations of both in the model. Multiple mappings\n(localizations) for each algorithm were identified, showing that neither the algorithm (what) nor its\nlocation in the network (where) is unique. Bottom: For the where-then-what strategy, we enumerate\ncircuits (sub-networks) and test whether each computes the XOR independently. For each circuit,\nwe search for possible feature interpretations of the selected neurons, identifying intermediate logic\ngates whose values can be mapped consistently with the neurons’ activations. Consistency is defined\nas in 3. We find many different perfect circuits (the where is not unique) and for any given circuit,\nwe find multiple valid interpretations (the what is not unique).\nIncompatibility of two explanations can occur if (1) the two explanations posit different algorithms\nfor the same behavior, or (2) the same algorithm is embedded in different subspaces of the neural\nnetwork. Both scenarios entail different internal representations and causal pathways linking inputs\nto outputs. In Figure 2, we report examples of incompatible explanations in trained MLPs.\nOur experiments show that even the strict causal criteria of MI allow many incompatible computa-\ntional abstractions. In the discussion (Section 5), we revisit whether this expectation of unicity is\nnecessary or even achievable.\n3\nILLUSTRATING POTENTIAL IDENTIFIABILITY ISSUES\nThis section highlights identifiability counter-examples for a small MLP trained to compute the XOR\nfunction. It is well-known that an MLP requires at least two layers to compute the XOR function.\nOnce the network can do so, the interpretability exercise becomes: how is the XOR implemented?\nFor a mechanistic explanation, the answer must have two components: what algorithm is being used,\nsuch as which combination of logic gates transforms the inputs into the XOR truth table, and where\nthese intermediate logic gates are located within the neural network’s computation—i.e., where the\nalgorithm is executed within the MLP.\nTo stress-test the two main MI strategies (where-then-what and what-then-where), we chose an MLP\nsmall enough to allow exhaustive enumeration of all circuits and extensive search over mappings.\nThe MLP is trained on binary inputs with a single logit output to produce the XOR behavior. The\ninputs are 0 or 1 and can have a randomly sampled Gaussian noise of a fixed standard deviation.\n6\n\n\nPublished as a conference paper at ICLR 2025\nOur methods to test the different criteria defined in the previous section are as follows:\nCircuit search: We enumerate all possible circuits, and then execute the validation data of the XOR\non each circuit as if it were a standalone neural network, effectively removing from the computation\neach node and edge that is not part of the circuit. If a circuit achieves perfect accuracy (zero circuit\nerror), we label it a perfect circuit, as it exactly replicates the model’s behavior. This search tests the\nidentifiability property of the circuit error criteria.\nInterpretation search: For each perfect circuit, we attempt to interpret the activations of the in-\ncluded neurons based on XOR validation data. As the scope of interpretations is limited to logic\ngates, we search, for each neuron, a logic gate whose values are consistent with that neuron’s acti-\nvation. The method proceeds recursively, layer by layer, based on a given neuron’s relationship with\nits parents in the circuit. The parents already have an interpretation (mapping their activations to 0\nor 1). We enumerate all possible inputs from the parents and examine how they are mapped into the\nneuron’s activation by the model. We then list all possible ways to separate these inputs and label\nthe resulting logic gate. If we find no valid interpretation for a given neuron (e.g., all inputs overlap\nin the output activation and no separation is possible), we end this candidate interpretation of the cir-\ncuit. If we find multiple, we expand the tree of possible candidate interpretations for the circuit. To\navoid trivial over-counting, we ignore value relabeling (e.g., swapping 0 and 1) and, by convention,\nassign 1 to the larger intervals and 0 to the smaller ones. The outcome is a computational abstrac-\ntion, a Boolean circuit computing the XOR function whose internal logic gates are mapped to some\nneural network components. Note that this method undercounts possible interpretations because it\ndoes not consider cases where the high-level logic gates are mapped on multiple low-level neurons.\nThis search tests the identifiability property of the mapping consistency criteria.\nAlgorithm search: We enumerate all algorithms implementing the XOR gate by considering\nBoolean formula parse trees. We assume that each neuron’s activations encode either the iden-\ntity, AND, or OR gate, a network of depth d can implement only formulas with a parse tree of depth\n≤d. We therefore recursively enumerate all unique formulas of depth d or less using AND, OR,\nand negation. For d = 3, this results in 56 XOR-equivalent algorithms.\nMapping search: For a given candidate algorithm with specified intermediate logic gates, we ex-\nplore all possible neuron subsets and mappings between these subsets and the algorithm’s interme-\ndiate gates. We then measure the causal alignment of the mapping using IIA. If a mapping achieves\nperfect IIA, we call it a perfect mapping. If there is no other mapping with larger images (set\ninclusion-wise), we also call this mapping minimal. In the example described in this section, we\nmanually test two candidate algorithms, while the next section enumerates algorithms that imple-\nment the target function, excluding trivial variations (e.g., negating gates). This search tests the\nidentifiability property of the IIA criteria.\nWe depict in Figure 2 counter-examples for each criterion in one small MLP. In this example, for the\nwhat-then-where strategy, we only test two candidate algorithms but find 159 perfect minimal map-\npings within the neural network activations, with perfect mappings for both algorithms. Therefore,\nthe algorithm is not unique and, for a given algorithm, its localization is not unique. For the where-\nthen-what strategy, we find 85 unique circuits with perfect accuracy, with an average of 535.8 logic\ngate interpretations (consistent mappings) per circuit. Therefore, the localization is not unique, and\nfor a given circuit, the interpreted algorithm is not unique. Overall, in this example, we obtain 159 +\n45,543 computational abstractions, most of which are incompatible. This is a serious identifiability\nproblem as there is no clear and consensual criterion to decide among all these explanations.\n4\nEXPERIMENTS\n4.1\nQUANTITATIVE ANALYSIS\nWe now repeat the experiment used for the XOR example with different seeds, while varying the\narchitecture size and the complexity of the global behavior.\nThe basic setup is consistent across all experiments. We choose n 2-input logic gates L1, . . . , Ln,\ngenerate a multilayer perceptron (MLP) N with layer sizes (2, k, k, n), and train N to implement\nthe gates L1, . . . Ln. Similarly to the previous section, training is performed on binary samples with\nadded Gaussian noise and continues until the network’ mean squared loss is lower than n × 10−3.\n7\n\n\nPublished as a conference paper at ICLR 2025\nWe then quantify the identifiability issues again. For the circuit-first search (where-then-what strat-\negy), we count perfect circuits for L in N and valid interpretations for each circuit.\nFor the\nalgorithm-first search (what-then-where strategy), we count perfectly aligned algorithms for L in\nN and perfect minimal mappings for each algorithm.\nIn the circuit-first search, we only search for circuits containing two inputs and one output for each\ntarget gate. Furthermore, due to the combinatorial explosion in circuit enumeration, we only test\ncircuits with a sparsity greater than 0.3, where sparsity is measured as the fraction of components\nexcluded from the circuit. This number is chosen as the smallest sparsity that remains manageable\nfor the size of MLPs that we consider. As a result, the reported number of circuits should be consid-\nered a lower bound. Furthermore, the number of potential interpretations for a single circuit grows\nexponentially with its size. Since we count interpretations for only the sparser circuits, the reported\nnumber of interpretations is also significantly lower than an exhaustive search would yield.\n4.1.1\nARCHITECTURE SIZE\nIncreasing the neural network’s size may impact the number of computational abstractions found\nin networks. Although a larger architecture may create more computational abstractions due to the\nincreased search space, it could also lead to greater overparameterization, meaning a smaller subset\nof the network may suffice to implement the target gate. This, in turn, could reduce the number of\nvalid abstractions if most of the network is inactive during inference.\nIn the left side of Figure 3, we report the total number of explanations found when the architecture\nsize ranges from k = 2 to k = 5. We exclude from this figure the networks for which no valid\nmapping or interpretation was found, which we give in Appendix D.2 along with additional plots.\nFigure 3: Number of computational abstractions found in the circuit-first approach (circuit interpre-\ntations) and the algorithm-first approach (perfect minimal mappings), as a function of architecture\nsize k (left) or of the number n of gates the model is trained on (right, averaged over all target gates).\nOne point per neural network.\nIn both cases, we observe that the number of computational abstractions found significantly in-\ncreases with network size, with median values growing from 38 to 910,000 in the circuit-first method\nand from 8 to 3,700 in the algorithm-first approach. Less than 2% of the trained networks contain\nexactly one valid minimal mapping, and no network contains exactly one circuit interpretation.\n4.1.2\nMULTIPLE TASKS\nWe also investigate the effect of global behavior complexity. The model is trained to implement\na single logic gate in the basic setup. What happens when the network is trained in a multi-task\nsetting? As the number of target tasks increases, we expect the network to use its activations more\nefficiently, possibly relying on a smaller subset of its structure for each task. To explore this, we fix\nk = 3 and vary n from 1 to 6, sampling n logic gates (without replacement) from the same list as\nabove, extended to include the negation of the gates (NOR, NAND, NIMP, and XNOR). The neural\nnetwork N is then trained to implement these gates in parallel, using two shared input neurons and\nn output neurons (one per gate). We repeat this procedure using different random seeds.\n8\n\n\nPublished as a conference paper at ICLR 2025\nThe right side of Figure 3 contains the total number of computational abstractions obtained when\ntraining each neural network on a different number of logic gates in parallel, ranging from 1 to 6.\nMore detailed plots are available in Appendix D.3.\nIn both approaches, the number of interpretations significantly decreases with the number of training\ntasks (p = 0.05), up to 4 tasks. Past that point, the variation is no longer statistically significant.\n4.2\nTRAINING DYNAMICS\nA possible explanation for the high number of computational abstractions we find in trained net-\nworks is that our networks do not perfectly implement the target logic gates, since training is stopped\nwhen a low but non-zero loss value is reached. We explored this effect by varying the loss cutoff in\nthe basic setup. The results are given in Appendix D.4. More generally, the influence of the training\ndistribution on the number of valid computational abstractions is investigated in Appendix D.6. In\naddition, we find that adding noise to binary samples during training has no significant effect on the\nresults obtained in the algorithm-first method, but decreases the number of circuits while increas-\ning the overall number of interpretations found in the circuit-first method (results are described in\nAppendix D.5). Training dynamics and generalization abilities of a model may therefore reduce the\nnumber of available abstractions, but this effect alone is unlikely to mitigate the issue entirely.\n4.3\nTOWARDS LARGER MODELS\nWhile enumerating circuits or mappings is infeasible in large networks, it is still possible to find\ncounterexamples in which multiple circuits exist. For example, we trained a larger MLP on a subset\nof the MNIST dataset (Deng, 2012), filtered to contain only the digits 0 and 1. We obtained a\nregression model with layer sizes (784, 128, 128, 3, 3, 3, 1). After training, we extracted the last\nlayers of the model to form two sub-networks: one of size (784, 128, 128, 3) and one of size (3, 3, 3,\n1). We fed the training samples through the larger sub-network, generating a new dataset comprised\nof partial computations of the overall model.\nApplying the circuit search method to the smaller sub-network using this new dataset yielded 3,209\nvalid circuits. While we cannot enumerate circuits in the first half of the network, any such valid\ncircuit can include one of the circuits of the second half as its continuation. Two situations may\narise: If the first half of the MLP does not contain any valid circuits, then no valid circuit exists for\nthe full network; if valid circuits exist in the first half of the MLP, then a minimum of 3,209 valid\ncircuits exist in the full network. This shows, at least in the case of circuits, that the problem does\nnot seem to disappear with significantly larger scale and more complex data distributions.\n5\nWHAT DOES IT MEAN FOR INTERPRETABILITY?\nOur findings challenge the strong intuition that a unique mechanistic explanation exists for a given\nbehavior under fixed explanatory goals and validity criteria. Even when employing the strict causal\nrequirements of MI, we find that many incompatible explanations can coexist. While predictive\nof behavior and causally aligned with the neural network’s states, these explanations differ in the\ncomputational algorithms they postulate or how they are embedded in the network’s subspaces. We\nnow discuss several ways to move forward from this striking observation.\n5.1\nDOES LACK OF UNICITY MATTER?\nWhether multiple “valid” explanations pose a real problem is worth considering. From a pragmatic\nstance, one could argue that unicity is not essential if the explanations meet functional goals such\nas predictivity, controllability, or utility in decision-making (Van Fraassen, 1988; Achinstein, 1984).\nThis perspective emphasizes crafting practical criteria to evaluate explanations based on their utility,\nrather than their ontological closeness to the truth. Stating explicitly the pragmatic goals of an\nexplanation can also clarify what is expected of an explanation (Woodward and Ross, 2021). For\nexample, in the recent debate about interpretability illusion, Makelov et al. (2023) mention problems\nabout interventions that can potentially activate dormant pathways leading the resulting explanation\nto misrepresent the mechanisms at play. In their response, one of the arguments advanced by Wu\net al. (2024) is to point out that the explanation produced by their method (DAS), still meets the\n9\n\n\nPublished as a conference paper at ICLR 2025\npragmatic goals of predictivity and manipulability, ensuring its usefulness. The debate is resolved\nby clarifying the epistemic goals of the explanation.\n5.2\nIF YES, HOW CAN WE AIM TO RESOLVE IT?\nIf we decide that identifiability of explanations is important, our work demonstrates that current\nMI criteria are insufficient to guarantee it. One potential approach to resolving this issue involves\nintroducing additional heuristics, such as prioritizing the sparsest circuits. However, Occam’s razor\nalone is unlikely to solve the problem. Should we dismiss an entirely different candidate explanation\nsimply because it involves one additional node than another? In our experiments, simplicity or\nsparsity cannot single out one explanation.\nTo address these challenges, we believe that ideas from causal abstraction (Beckers and Halpern,\n2019; Beckers et al., 2020; Rubenstein et al., 2017) can be helpful (Geiger et al., 2022a). Although\nIIA is directly inspired by causal abstraction, it does not fully implement it in its current form. Unlike\ncurrent MI frameworks, causal abstraction requires that all lower-level model states are accounted\nfor in higher-level representations. Furthermore, if components are excluded from an explanation,\ntheir absence must be justified causally. This intuition has been formalized recently through the\nconcept of faithfulness, which evaluates how well a circuit replicates the model’s behavior and the\n(lack of) impact of excluded elements (Hanna et al., 2024).\nAlternatively, one can look at broader approaches and not focus on searching for explanations that\noptimize a single criterion. Interestingly, Vilas et al. (2024) propose an inner interpretability frame-\nwork based on lessons from cognitive neuroscience. In this framework, the authors emphasize the\nimportance of building multi-level mechanistic explanations and stress-testing these explanations\nwith proper hypotheses testing. An explanation validated by many criteria and which exhibits differ-\nent properties (e.g., invariances) becomes more trustworthy. This framework provides a promising\npath to establish MI as a natural science akin to neuroscience or biology.\n5.3\nIS IDENTIFIABILITY EVEN ACHIEVABLE?\nIn some domains of science, competing theories coexist despite being ontologically incompatible.\nFor instance, the Lagrangian and Hamiltonian formulations of classical mechanics posit different\nunderlying entities but yield identical experimental predictions. Such scenarios are examples of con-\ntrastive underdetermination, a philosophical debate about whether empirical evidence can or should\nuniquely determine scientific explanations (Stanford, 2023). The intrinsic computational hardness\nof interpretability queries (Adolfi et al., 2024a;b) suggests that MI may have fundamental limits,\nleaving it possibly underdetermined. More broadly, the search for MI explanations can be seen as\na type of causal representation learning, where identifiability may depend on strong constraints that\nmight not be realistic Morioka and Hyv¨arinen (2024).\n5.4\nFROM TOY MODELS TO REAL MODELS\nOur experiments focus on toy MLPs trained on toy tasks, which differ drastically from large lan-\nguage models (LLMs) trained on vast, complex datasets using Transformer architectures. This raises\nthe possibility that the issues in small models may not apply to larger, more sophisticated models.\nHowever, if this is true, why the problems would disappear at larger scales must be demonstrated.\nUnderstanding why current criteria function well in some regimes but not in others would also lead\nto refined criteria and definitions.\n5.5\nCONCLUSION\nOur results should encourage the community to reflect on the role of unicity when searching for and\ncommunicating about mechanistic explanations found in neural networks. We believe that exploring\nstricter criteria based on causal abstraction, explicitly formulating pragmatic goals of explanations\nand embracing broader frameworks such as the inner interpretability one are all promising directions.\nThe code and parameters used to conduct this paper’s experiments can be found on GitHub.\n10\n\n\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGEMENTS\nThis work was conducted within French research unit UMR 5217 and was partially supported by\nCNRS (grant ANR-22-CPJ2-0036-01) and by MIAI@Grenoble-Alpes (grant ANR-19-P3IA-0003).\nIt was granted access to the HPC resources of IDRIS under the allocation 2025-AD011014834\nmade by GENCI.\n11\n\n\nPublished as a conference paper at ICLR 2025\nREFERENCES\nPete Achinstein. 1984. The pragmatic character of explanation. In PSA: Proceedings of the Bien-\nnial Meeting of the Philosophy of Science Association, volume 1984, pages 274–292. Cambridge\nUniversity Press.\nFederico Adolfi, Martina G. Vilas, and Todd Wareham. 2024a. Complexity-Theoretic Limits on the\nPromises of Artificial Neural Network Reverse-Engineering. Proceedings of the Annual Meeting\nof the Cognitive Science Society, 46(0).\nFederico Adolfi, Martina G. Vilas, and Todd Wareham. 2024b. The computational complexity of\ncircuit discovery for inner interpretability.\nSander Beckers, Frederick Eberhardt, and Joseph Y. Halpern. 2020. Approximate causal abstrac-\ntions. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115\nof Proceedings of Machine Learning Research, pages 606–615. PMLR.\nSander Beckers and Joseph Y. Halpern. 2019. Abstracting causal models. Proceedings of the AAAI\nConference on Artificial Intelligence, 33(01):2678–2685.\nLeonard Bereska and Efstratios Gavves. 2024. Mechanistic interpretability for ai safety – a review.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-\nerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,\nShauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex\nTamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,\nTom Henighan, and Christopher Olah. 2023.\nTowards monosemanticity: Decomposing lan-\nguage models with dictionary learning.\nTransformer Circuits Thread.\nHttps://transformer-\ncircuits.pub/2023/monosemantic-features/index.html.\nNick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and Chris Olah. 2021.\nCurve Circuits. Distill, 6(1):e00024.006.\nShan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah. 2019. Activation atlas.\nDistill. Https://distill.pub/2019/activation-atlas.\nDiogo V. Carvalho, Eduardo M. Pereira, and Jaime S. Cardoso. 2019.\nMachine learning inter-\npretability: A survey on methods and metrics. Electronics, 8(8).\nArthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a Garriga-\nAlonso. 2023. Towards automated circuit discovery for mechanistic interpretability. In Advances\nin Neural Information Processing Systems, volume 36, pages 16318–16352. Curran Associates,\nInc.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge\nneurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland. Association for\nComputational Linguistics.\nAdam Davies and Ashkan Khakzar. 2024. The cognitive revolution in interpretability: From ex-\nplaining behavior to interpreting representations and algorithms.\nLi Deng. 2012. The mnist database of handwritten digit images for machine learning research. IEEE\nSignal Processing Magazine, 29(6):141–142.\nMaximilian Dreyer, Erblina Purelku, Johanna Vielhaben, Wojciech Samek, and Sebastian La-\npuschkin. 2024. Pure: Turning polysemantic neurons into pure features by identifying relevant\ncircuits.\nJacob Dunefsky, Philippe Chlenski, and Neel Nanda. 2024. Transcoders find interpretable llm fea-\nture circuits.\n12\n\n\nPublished as a conference paper at ICLR 2025\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\nGanguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\nKamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\nChris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits\nThread. Https://transformer-circuits.pub/2021/framework/index.html.\nJonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable\nneural networks. In ICLR. OpenReview.net.\nMichael Friedman. 1974. Explanation and scientific understanding. Journal of Philosophy, 71(1):5–\n19.\nTrevor Gale, Erich Elsen, and Sara Hooker. 2019. The state of sparsity in deep neural networks.\nAtticus Geiger, Zhengxuan Wu, Karel D’Oosterlinck, Elisa Kreiss, Noah D. Goodman, Thomas\nIcard, and Christopher Potts. 2022a. Faithful, interpretable model explanations via causal ab-\nstraction. Stanford AI Lab Blog.\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good-\nman, and Christopher Potts. 2022b. Inducing causal structure for interpretable neural networks.\nIn Proceedings of the 39th International Conference on Machine Learning, volume 162 of Pro-\nceedings of Machine Learning Research, pages 7324–7338. PMLR.\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman. 2024. Find-\ning alignments between interpretable causal variables and distributed neural representations. In\nProceedings of the Third Conference on Causal Learning and Reasoning, volume 236 of Pro-\nceedings of Machine Learning Research, pages 160–187. PMLR.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of fac-\ntual associations in auto-regressive language models. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pages 12216–12235, Singapore. Associa-\ntion for Computational Linguistics.\nAlison Gopnik. 2000. Explanation as orgasm and the drive for causal knowledge: The function,\nevolution, and phenomenology of the theory formation system. In Explanation and cognition.,\npages 299–323. The MIT Press, Cambridge, MA, US.\nMichael Hanna, Sandro Pezzelle, and Yonatan Belinkov. 2024. Have faith in faithfulness: Going\nbeyond circuit overlap when finding model mechanisms.\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. 2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, Florence, Italy. Association for Computational Linguistics.\nStephen H. Kellert, Helen Longino, and C. Kenneth Waters. 2006. Introduction: The pluralist stance.\nIn Stephen H. Kellert, Helen Longino, and C. Kenneth Waters, editors, Scientific Pluralism, pages\nvii–xxix. University of Minnesota Press.\nAshkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim,\nand Nassir Navab. 2021. Neural response interpretation through the lens of critical pathways. In\n2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13523–\n13533.\nPhilip Kitcher. 1962. Explanatory unification and the causal structure of the world. In Philip Kitcher\nand Wesley C. Salmon, editors, Scientific Explanation, pages 410–505. Univ of Minnesota Pr.\nPhilip Kitcher. 1981. Explanatory unification. Philosophy of Science, 48(4):507–531.\nJ´anos Kram´ar, Tom Lieberum, Rohin Shah, and Neel Nanda. 2024. Atp*: An efficient and scalable\nmethod for localizing llm behaviour to components.\n13\n\n\nPublished as a conference paper at ICLR 2025\nArie W. Kruglanski. 1989.\nLay epistemics and human knowledge: Cognitive and motivational\nbases. Lay epistemics and human knowledge: Cognitive and motivational bases. Plenum Press,\nNew York, NY, US.\nKestutis Kveraga, Avniel S. Ghuman, and Moshe Bar. 2007. Top-down predictions in the cognitive\nbrain. Brain and Cognition, 65(2):145–168.\nFangshuo Liao and Anastasios Kyrillidis. 2022. On the convergence of shallow neural network\ntraining with randomly masked neurons.\nBohan Liu, Zijie Zhang, Peixiong He, Zhensen Wang, Yang Xiao, Ruimeng Ye, Yang Zhou, Wei-\nShinn Ku, and Bo Hui. 2024. A survey of lottery ticket hypothesis.\nXinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of\nlarge language models. In Advances in Neural Information Processing Systems.\nAleksandar Makelov, Georg Lange, and Neel Nanda. 2023. Is this the subspace you are looking for?\nan interpretability illusion for subspace activation patching.\nSamuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\n2024. Sparse feature circuits: Discovering and editing interpretable causal graphs in language\nmodels.\nD. Marr and T. Poggio. 1976. From understanding computation to understanding neural circuitry.\nTechnical report, USA.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022.\nLocating and edit-\ning factual associations in GPT.\nAdvances in Neural Information Processing Systems, 36.\nArXiv:2202.05262.\nChristoph Molnar. 2022. Interpretable Machine Learning, 2 edition.\nGiovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre\nKıcıman, Hamid Palangi, Barun Patra, and Robert West. 2024. A glitch in the matrix? locat-\ning and detecting language model grounding with fakepedia.\nHiroshi Morioka and Aapo Hyv¨arinen. 2024. Causal representation learning made identifiable by\ngrouping of observational variables.\nIn Proceedings of the 41st International Conference on\nMachine Learning, ICML’24. JMLR.org.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n2020. Zoom in: An introduction to circuits. Distill. Https://distill.pub/2020/circuits/zoom-in.\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill.\nHttps://distill.pub/2017/feature-visualization.\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine\nYe, and Alexander Mordvintsev. 2018.\nThe building blocks of interpretability.\nDistill.\nHttps://distill.pub/2018/building-blocks.\nJudea Pearl. 2009. Causality: Models, Reasoning and Inference, 2nd edition. Cambridge University\nPress, USA.\nMaxime Peyrard, Beatriz Borges, Kristina Gligori´c, and Robert West. 2021. Laughing heads: Can\ntransformers detect what makes a sentence funny? In Proceedings of the Thirtieth International\nJoint Conference on Artificial Intelligence, IJCAI-21, pages 3899–3905. International Joint Con-\nferences on Artificial Intelligence Organization. Main Track.\nAngela Potochnik. 2017.\nIdealization and the Aims of Science.\nUniversity of Chicago Press,\nChicago.\nThomas J. Rothenberg. 1971. Identification in parametric models. Econometrica, 39(3):577–591.\n14\n\n\nPublished as a conference paper at ICLR 2025\nP. K. Rubenstein, S. Weichwald, S. Bongers, J. M. Mooij, D. Janzing, M. Grosse-Wentrup, and\nB. Sch¨olkopf. 2017. Causal consistency of structural equation models. In Proceedings of the 33rd\nConference on Uncertainty in Artificial Intelligence (UAI), page ID 11. *equal contribution.\nHassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022. Neuron-level Interpretation of Deep NLP\nModels: A Survey. Transactions of the Association for Computational Linguistics, 10:1285–\n1303.\nGerhard Schurz. 1999. Explanation as unification. Synthese, 120(1):95–114.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional net-\nworks: Visualising image classification models and saliency maps.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(56):1929–1958.\nKyle Stanford. 2023. Underdetermination of Scientific Theory. In Edward N. Zalta and Uri Nodel-\nman, editors, The Stanford Encyclopedia of Philosophy, Summer 2023 edition. Metaphysics Re-\nsearch Lab, Stanford University.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2024. A simple and effective pruning\napproach for large language models.\nAaquib Syed, Can Rager, and Arthur Conmy. 2023. Attribution patching outperforms automated\ncircuit discovery.\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen,\nAdam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L\nTurner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Ed-\nward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. 2024.\nScaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer\nCircuits Thread.\nDamien Teney, Maxime Peyrard, and Ehsan Abbasnejad. 2022. Predicting is not understanding:\nRecognizing and addressing underspecification in machine learning. In European Conference on\nComputer Vision, pages 458–476. Springer.\nJ. D. Trout. 2007. The psychology of scientific explanation. Philosophy Compass, 2(3):564–591.\nBas Van Fraassen. 1988. The pragmatic theory of explanation. Theories of explanation, 8:135–155.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\nStuart Shieber. 2020. Investigating gender bias in language models using causal mediation anal-\nysis. In Advances in Neural Information Processing Systems, volume 33, pages 12388–12401.\nCurran Associates, Inc.\nMartina G. Vilas, Federico Adolfi, David Poeppel, and Gemma Roig. 2024. Position: An inner\ninterpretability framework for ai inspired by lessons from cognitive neuroscience.\nMartina G. Vilas, Timothy Schauml¨offel, and Gemma Roig. 2023. Analyzing vision transformers\nfor image classification in class embedding space. In Advances in Neural Information Processing\nSystems, volume 36, pages 40030–40041.\nKevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022.\nInterpretability in the wild: a circuit for indirect object identification in gpt-2 small.\nJonathan Waskan. 2024. Experimental Philosophy of Science: Scientific Explanation, pages 237–\n262. De Gruyter, Berlin, Boston.\nJames Woodward and Lauren Ross. 2021. Scientific Explanation. In Edward N. Zalta, editor, The\nStanford Encyclopedia of Philosophy, Summer 2021 edition. Metaphysics Research Lab, Stanford\nUniversity.\n15\n\n\nPublished as a conference paper at ICLR 2025\nZhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, and\nNoah D. Goodman. 2024. A reply to makelov et al. (2023)’s ”interpretability illusion” arguments.\nZhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. 2023. Inter-\npretability at scale: Identifying causal mechanisms in alpaca. In Advances in Neural Information\nProcessing Systems, volume 36, pages 78205–78226. Curran Associates, Inc.\nBinhang Yuan, Anastasios Kyrillidis, and Christopher M. Jermaine. 2019. Distributed learning of\ndeep neural networks using independent subnet training. CoRR, abs/1910.02120.\nRafael Yuste. 2008. Circuit neuroscience: the road ahead. Frontiers in neuroscience, 2:1038.\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks.\nIn Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September\n6-12, 2014, Proceedings, Part I, volume 8689 of Lecture Notes in Computer Science, pages 818–\n833. Springer.\nB. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. 2016. Learning deep features for dis-\ncriminative localization. In 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 2921–2929, Los Alamitos, CA, USA. IEEE Computer Society.\n16\n\n\nPublished as a conference paper at ICLR 2025\nA\nFORMAL DEFINITION OF IIA\nThe definitions in this section are adapted from Geiger et al. (2022b). We begin by setting notation\nconventions.\nLet N be a neural network, and A be a high-level algorithm.\nLet τ be a mapping between low-level neuron groups {Vj} of N and the values of their correspond-\ning high-level variables {Aj} in A.\nLet Vin (resp. Vout) be the neuron groups corresponding to the inputs (resp. outputs) of N.\nLet Ain (resp. Aout) be the variables in A with no parents (resp. no children).\nNotation (Value reading). Let vin ∈R|Vin| be some possible input values of the network N. We note\nN[vin, Vj] the values of the activations of Vj that are obtained when setting the values of Vin to vin\nand running the computation graph of N.\nSimilarly, let ain ∈R|Ain| be some possible values of the variables of A with no parents. We note\nA[ain, Aj] the values of the variable Aj that are obtained when setting the values of Ain to ain and\nrunning the algorithm A.\nNotation (Intervention). Let vj ∈R|Vj| be some possible activations of the variable Vj in the\nnetwork N. We note NVj←vj a copy of N, in which the activations of Vj are forcibly set to the\nvalue vj during the computation.\nSimilarly, let aj ∈R|aj| be a possible value of the variable Aj in the algorithm A. We note AAj←aj\na copy of A, in which the value of Aj is forcibly set to the value aj when the algorithm is run.\nThis notion is aligned with the do-operator (Pearl, 2009).\nDefinition 5 (Intervention interchange). Let basel,in, sourcel,in ∈R|Vj| be some possible input values\nof the network N. We call low-level intervention interchange the quantity:\nIIlow(N, basel,in, sourcel,in, Vk) = (NVk←N[sourcel,in,Vk])[basel,in, Vout]\nSimilarly, let baseh,in, sourceh,in ∈R|Aj| be some possible values of the variables in A with no\nparent. We call high-level intervention interchange the quantity:\nIIhigh(N, baseh,in, sourceh,in, Ak) = (AAk←N[sourceh,in,Ak])[baseh,in, Aout]\nThis corresponds to the notion of counterfactual intervention: After running the algorithm (or net-\nwork) on a set of inputs (source) and recording the value of a given variable, we execute the algo-\nrithm again on a different set of inputs (base) but restore the value of the variable from the first run\nduring the computation. The system is now in a counterfactual state, and we measure its new output.\nDefinition 6 (IIA). Let Val(Aj) be the set of possible values of Aj, and Val(Ain) = Q\nV ∈Vin Val(V )\nbe the set of possible combinations of values of the variables in A with no parents. Let Ak be a\nhigh-level variable of A.\nThe intervention interchange accuracy of the mapping τ for the variable Ak is the quantity:\nIIA(N, A, Ak, τ) =\n1\n|Val(Ain)|2\nX\nb,s∈Val(Ain)\n1 [IIhigh(A, b, s, Ak) = IIlow(N, b, s, Vk)]\nB\nOUTPUT EXAMPLES\nThis section contains additional examples of computational abstractions found by both strategies.\nSpecifically, we report abstractions found in a single neural network trained on the XOR gate with\nk = 3 and n = 1 with a loss cutoff of 10−3.\n17\n\n\nPublished as a conference paper at ICLR 2025\nB.1\nCIRCUIT-FIRST APPROACH\nAn exhaustive pass of the circuit-first approach (with no minimal sparsity threshold) yielded 59\ncircuits (*where*). We depict in Figure 4 the 12 most sparse circuits found.\nFigure 4: The 12 most sparse circuits found in the example network.\nWhen searching for interpretations (*what*) in all 59 circuits, we find 114,230 interpretations. We\nthen focus on circuit 8 (second row, last column in Figure 4, chosen for illustration purposes as it\nyields 24 valid interpretations, which we fully list in Table 1. Each of these interpretations leads\nto a different explanation of the network; for example, interpretation 1 corresponds to the formula\n¬(¬(A ∧B) →¬(A ∨B)), while interpretation 2 corresponds to ¬((¬(A ∧B) ∨¬(A ∨B)) →\n(¬(A ∧B) →¬(A ∨B))).\nB.2\nALGORITHM-FIRST APPROACH\nIn the algorithm-first approach, exhaustive enumeration yields 56 possible logic formulas for the\nXOR gate with a depth of 3 (excluding commutative-invariant formulas). Four of these formulas\nproduce valid mappings for the neural network. Those mappings are listed in Table 2.\nC\nIDENTIFIABILITY IN CIRCUIT LITERATURE\nIdentifiability is, to the best of our knowledge, never stated as an explicit assumption in existing\nworks about circuits. In this section, we list examples from the literature that indicate that it is\nnonetheless typically taken for granted:\n• Cammarata et al. (2021): ”the curve circuit” (multiple occurrences)\n• Wang et al. (2022): ”discover the circuit”, ”discovering the circuit”, ”uncover the circuit”\n• Kram´ar et al. (2024): ”we investigate the circuit underlying multiple-choice question-\nanswering”\n• Conmy et al. (2023): ”Choosing a clearly defined behavior means that the circuit will be\neasier to interpret than a mix of circuits corresponding to a vague behavior”, ”ACDC [...]\nfully recovers the circuit of toy model”.\n• Hanna et al. (2024): ”We next search for the circuit responsible for computing this task”\n• Marks et al. (2024): ”The circuit for agreement across a prepositional phrase (Figure 12)”\n18\n\n\nPublished as a conference paper at ICLR 2025\nNeuron (1, 0)\nNeuron (1, 1)\nNeuron (2, 0)\nNeuron (2, 1)\nNeuron (3, 0)\n#\nGate\nSep.\nGate\nSep.\nGate\nSep.\nGate\nSep.\nGate\nSep.\n1\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nOR\n0.5\nNOT A\n2\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nOR\n0.5\nRNIMP\n3\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nA\n0.5\nNOT A\n4\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.777\nA\n0.5\nRNIMP\n5\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nIMP\n6\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nNOT A\n7\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nB\n8\n-0.003\nNAND\n0.693\nNOR\n0.151\nIMP\n0.987\nNIMP\n0.5\nRNIMP\n9\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOT A\n0.987\nNIMP\n0.5\nB\n10\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOT A\n0.987\nNIMP\n0.5\nRNIMP\n11\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOR\n0.987\nNIMP\n0.5\nB\n12\n-0.003\nNAND\n0.693\nNOR\n0.397\nNOR\n0.987\nNIMP\n0.5\nRNIMP\n13\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nOR\n0.5\nNOT A\n14\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nOR\n0.5\nRNIMP\n15\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nB\n0.5\nNOT A\n16\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.777\nB\n0.5\nRNIMP\n17\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nIMP\n18\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nNOT A\n19\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nB\n20\n0.321\nNOR\n0.230\nNAND\n0.151\nRIMP\n0.987\nRNIMP\n0.5\nRNIMP\n21\n0.321\nNOR\n0.230\nNAND\n0.397\nNOT B\n0.987\nRNIMP\n0.5\nB\n22\n0.321\nNOR\n0.230\nNAND\n0.397\nNOT B\n0.987\nRNIMP\n0.5\nRNIMP\n23\n0.321\nNOR\n0.230\nNAND\n0.397\nNOR\n0.987\nRNIMP\n0.5\nB\n24\n0.321\nNOR\n0.230\nNAND\n0.397\nNOR\n0.987\nRNIMP\n0.5\nRNIMP\nTable 1: The list of interpretations found for circuit 8 of 59 in the example network. For each\ninterpretation, the four intermediate neurons and the output one are assigned a logic gate and a\nseparation boundary. Each neuron is represented by its layer and position in the layer (indexed from\n0), as read from left to right and from top to bottom in Figure 4. IMP refers to the implication gate\n(A implies B), NIMP to its negation, and RIMP and RNIMP refer to the reversed implication gate\n(B implies A) and its negation.\nFormula 1: ¬(A ∧B) ∧(A ∨B)\nMapping\nA ∨B\n¬(A ∧B)\n1\nNeuron (1, 2)\nNeuron (1, 1)\n2\nNeuron (1, 0)\nNeuron (1, 1)\nFormula 2: ¬((A ∧B) ∨¬(A ∨B))\nMapping\nA ∧B\n¬(A ∨B)\n1\nNeuron (1, 1)\nNeuron (1, 2)\n2\nNeuron (1, 1)\nNeuron (1, 0)\nFormula 39: ¬((A ∧B) ∨¬(A ∨B)) ∧(A ∨B)\nMapping\nA ∧B\nA ∨B\n¬((A ∧B) ∨¬(A ∨B))\n¬(A ∨B)\n1\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (2, 0)\nNeuron (1, 0)\n2\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (2, 1)\nNeuron (1, 0)\n3\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (2, 0)\nNeuron (1, 2)\n4\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (2, 1)\nNeuron (1, 2)\nFormula 40: ¬(((A ∧B) ∨¬(A ∨B)) ∨¬(A ∨B))\nMapping\n(A ∧B) ∨¬(A ∨B)\nA ∧B\n¬(A ∨B) (left)\n¬(A ∨B) (left)\n1\nNeuron (2, 1)\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (1, 2)\n2\nNeuron (2, 1)\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (1, 0)\n3\nNeuron (2, 0)\nNeuron (1, 1)\nNeuron (1, 0)\nNeuron (1, 2)\n4\nNeuron (2, 0)\nNeuron (1, 1)\nNeuron (1, 2)\nNeuron (1, 0)\nTable 2: The list of valid minimal mappings for the example network. For each intermediate node\nof each formula, we specify which neuron corresponds to that node.\n19\n\n\nPublished as a conference paper at ICLR 2025\nD\nADDITIONAL PLOTS\nD.1\nTARGET GATE VARIATION\nFigure 5 contains the total number of computational abstractions obtained after fixing k = 3 and\nn = 1 and sampling the target gate from the following list: AND, OR, XOR, IMP.\nFigure 5: Total number of interpretations found in the circuit-first approach and of mappings found\nin the algorithm-first approach, grouped by target gate.\nFigure 6 contains the results of the same experiment but displays separate plots for the number\nof circuits and interpretations per circuit (resp. algorithms and mappings per algorithm) for each\nnetwork.\nFigure 6: Left: Number of circuits and average interpretations per circuit found in the circuit-first\napproach. Right: Number of algorithms and average mappings per algorithm found in the algorithm-\nfirst approach (right). Results are grouped by target gate over 100 experiments.\nD.2\nARCHITECTURE SIZE\nFigure 7 contains additional plots for the experiment described in 4.1.1, in which we vary the archi-\ntecture size.\nD.3\nMULTI-TASK TRAINING\nWe report in Figure 8 additional plots for the experiment in which we vary the number of gates the\nmodel is being trained on (described in 4.1.2).\n20\n\n\nPublished as a conference paper at ICLR 2025\nFigure 7: Number of abstractions found in the circuit-first approach (left) and the algorithm-first\napproach (right) as a function of the architecture size.\nFigure 8: Number of abstractions found in the circuit-first approach (left) and the algorithm-first\napproach (right) as a function of the number of training tasks.\n21\n\n\nPublished as a conference paper at ICLR 2025\nD.4\nLOSS CUTOFF\nWe report in Figure 9 plots for the experiment in which we apply the basic setup with n = 1 and k3\non a set of networks, trained while varying the loss cutoff from 10−1 to 10−6. For the algorithm-first\napproach, a two-sample t-test indicates a modest but significant decrease in the number of algorithms\nfound when the loss cutoff is lower or equal to 10−5. In contrast, the number of mappings per\nalgorithm does not statistically vary. For the circuit-first approach, significantly fewer circuits and\ninterpretations per circuit are found when the loss cutoff is high (0.1), but values do not otherwise\nvary for lower loss values. In addition, we found that multiple computational abstractions can still\nbe identified in randomly initialized networks that happen to implement a logic gate (i.e. without\ntraining).\nFigure 9: Number of abstractions found in the circuit-first approach (left) and the algorithm-first\napproach (right) as a function of the neural network’s training loss cutoff.\nD.5\nEFFECT OF NOISE\nWe report in Table 3 the number of abstractions found in the basic setup (without noisy inputs) for\nboth approaches, and compare it to the results found when repeating the same setup while adding\nGaussian noise from N(0, 0.1) to the inputs at training time. We also report the p-value obtained\nwith Welch’s t-test.\nCriterion\nBasic setup\nNoisy setup\np-value\nAlgorithms\n2.70\n2.56\n0.766\nMinimal mappings/algorithm (avg)\n2.84\n3.05\n0.617\nTotal mappings\n9.86\n9.89\n0.959\nCircuits\n52.3\n28.6\n< 0.001\nInterpretations/circuit (avg)\n2,270\n16,400\n< 0.001\nTotal interpretations\n107,000\n285,000\n< 0.001\nTable 3: Number of abstractions found for both approaches in each setup (averaged over all net-\nworks).\nD.6\nTRAINING DISTRIBUTION\nThe influence of the training distribution was investigated through the following procedure:\n1. Sample a random neural network NN and target gate with n = 1 and k = 3\n2. Draw x1, . . . , x4 from U[0,1]\n3. Train NN on a skewed input distribution, with weights\nxi\nP\ni xi for each input i, and a loss\ncutoff of 10−3.\n4. Exhaustively enumerate all circuits, interpretations, algorithms, and mappings as in the\nbasic setup.\n22\n\n\nPublished as a conference paper at ICLR 2025\n5. Repeat from step 1.\nWe repeated those steps 100 times, resulting in varying training distributions with joint entropy\nvarying from 0.8 to 2.0 bits. We then performed a linear regression of the resulting counts as a\nfunction of the distribution’s joint entropy. We give in Table 4 the results for this experiment, which\nshow that the results of the algorithm-first approach do not significantly depend on the training\ndistribution. On the other hand, having an unbalanced distribution causes an increase in the number\nof circuits and total interpretations found in the circuit-first approach, but a decrease in the number\nof interpretations per circuit.\nCriterion\nSlope\nIntercept\np-value\nAlgorithms\n-0.21\n2.86\n0.857\nMinimal mappings/algorithm (avg)\n-0.14\n1.42\n0.807\nTotal mappings\n-0.17\n8.82\n0.974\nCircuits\n-326\n556\n0.001\nInterpretations/circuit (avg)\n1,235\n139\n0.029\nTotal interpretations\n-174,000\n367,000\n0.018\nTable 4: Linear regression performed on the number of computational abstractions as a function of\nthe training distribution’s joint entropy (in bits).\n23\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20914v1.pdf",
    "total_pages": 23,
    "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?",
    "authors": [
      "Maxime Méloux",
      "Silviu Maniu",
      "François Portet",
      "Maxime Peyrard"
    ],
    "abstract": "As AI systems are used in high-stakes applications, ensuring interpretability\nis crucial. Mechanistic Interpretability (MI) aims to reverse-engineer neural\nnetworks by extracting human-understandable algorithms to explain their\nbehavior. This work examines a key question: for a given behavior, and under\nMI's criteria, does a unique explanation exist? Drawing on identifiability in\nstatistics, where parameters are uniquely inferred under specific assumptions,\nwe explore the identifiability of MI explanations.\n  We identify two main MI strategies: (1) \"where-then-what,\" which isolates a\ncircuit replicating model behavior before interpreting it, and (2)\n\"what-then-where,\" which starts with candidate algorithms and searches for\nneural activation subspaces implementing them, using causal alignment.\n  We test both strategies on Boolean functions and small multi-layer\nperceptrons, fully enumerating candidate explanations. Our experiments reveal\nsystematic non-identifiability: multiple circuits can replicate behavior, a\ncircuit can have multiple interpretations, several algorithms can align with\nthe network, and one algorithm can align with different subspaces.\n  Is uniqueness necessary? A pragmatic approach may require only predictive and\nmanipulability standards. If uniqueness is essential for understanding,\nstricter criteria may be needed. We also reference the inner interpretability\nframework, which validates explanations through multiple criteria. This work\ncontributes to defining explanation standards in AI.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}