{
  "id": "arxiv_2502.20973v1",
  "text": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?\nPerla Al Almaoui1 Pierrette Bouillon1 Simon Hengchen1,2\n1Faculté de traduction et d’interprétation, Université de Genève 2iguanodon.ai\nCorrespondence: almaoui.perla@outlook.com\nAbstract\nIn this era of rapid technological advancements,\ncommunication continues to evolve as new lin-\nguistic phenomena emerge. Among these is\nArabizi, a hybrid form of Arabic that incorpo-\nrates Latin characters and numbers to represent\nthe spoken dialects of Arab communities. Ara-\nbizi is widely used on social media and allows\npeople to communicate in an informal and dy-\nnamic way, but it poses significant challenges\nfor machine translation due to its lack of for-\nmal structure and deeply embedded cultural\nnuances. This case study arises from a growing\nneed to translate Arabizi for gisting purpose.\nIt evaluates the capacity of different LLMs to\ndecode and translate Arabizi, focusing on multi-\nple Arabic dialects that have rarely been studied\nup until now. Using a combination of human\nevaluators and automatic metrics, this research\nproject investigates the model’s performance in\ntranslating Arabizi into both Modern Standard\nArabic and English. Key questions explored\ninclude which dialects are translated most ef-\nfectively and whether translations into English\nsurpass those into Arabic.\n1\nIntroduction\nAlthough there are approximately 420 million Ara-\nbic speakers worldwide, an intriguing linguistic\nparadox emerges: Modern Standard Arabic (MSA),\nthe standardized form of the language, is the mother\ntongue of none.\nInstead, Arabs communicate\nthrough their regional dialects, which are vibrant\nlinguistic hybrids influenced by Arabic and the\nhistorical languages of each region.\nThese di-\nalects have been honed by geographic, cultural,\nand historical factors, and can vary significantly\neven within a single country, resulting in a mo-\nsaic of over 60 distinct varieties. Arabizi (a fusion\nof \"Arabic\" and Englizi, the Arabic word for En-\nglish) is an informal, non-standard writing system\nthat emerged in the 1990s when Arabic keyboards\nwere not widely available. It uses Latin charac-\nters and numbers, combining both transliteration\nand transcription mappings. Primarily used in on-\nline communication—such as short messages and\ncomments on social media—Arabizi varies signif-\nicantly across dialects and even within the same\ndialect (Harrat et al., 2019).\nThe idea of romanizing the Arabic language is\nnot a new concept, as there have already been sev-\neral attempts to do so over the last century. How-\never, these efforts largely failed, as they were per-\nceived as colonialist initiatives aimed at suppress-\ning cultural and religious identity. More recently,\nthe International Organization for Standardization\n(ISO) introduced two norms, ISO 233 in 1984 and\nISO 233-2 in 1993, to standardize the romanization\nof Arabic. These standards aimed to facilitate the\ninternational exchange of information. Neverthe-\nless, their adoption remained limited due to their\nimpracticality, with usage restricted primarily to\nofficial contexts (Al Almaoui, 2024).\nConversely, Arabizi has become the dominant\nwritten form of communication among Arabic\nspeakers in informal settings. Its rise reflects a\ncrucial sociolinguistic reality: while MSA remains\nthe language reserved for academic, religious, and\nformal settings, it is often perceived as inacces-\nsible or overly formal for daily use. Arabizi, by\ncontrast, offers a dynamic and flexible medium for\nself-expression that aligns with the fluidity of Ara-\nbic dialects (Allehaiby, 2013; Yushmanov, 1961).\nDespite its widespread use across digital plat-\nforms, and the recent focus on informal language\nand low-resource languages, Arabizi remains an\nunexplored area in natural language processing\n(NLP). It poses particular challenges due to its col-\nloquial nature, variation across dialects and lack\nof standardization, as well as the scarcity of dig-\nital resources. In NLP, research on Arabizi has\nmainly focused on transliteration into Arabic (dero-\nmanization) at the character or word level, using\n1\narXiv:2502.20973v1  [cs.CL]  28 Feb 2025\n\n\ndifferent approaches (Guellil et al., 2017; Shazal\net al., 2020). Some studies explore interlinguis-\ntic machine translation (MT) techniques to and\nfrom Arabizi, employing various architectures and\npipelines, mainly between English and Egyptian\ndialects (see Harrat et al. (2019) for a summary up\nuntil 2017). While some recent datasets for low\nresource language translation include romanization,\nthey are not specifically focused on Arabizi. Flores\nbenchmark (Goyal et al., 2022), for example, is lim-\nited to the romanized transcription of MSA or Ara-\nbic dialects in Arabic scripts. The TerjamaBench\ndataset (Momayiz et al., 2024) is an exception and\nincludes entries in Darija, the Moroccan Arabic\ndialect, written in both Latin alphabet (Arabizi)\nand Arabic script, and their corresponding English\ntranslations.\nSince there is a growing need to translate Arabizi\ninto resource-rich languages on social media and\nother digital platforms, we conducted a case study\nto evaluate the feasibility of using large language\nmodels (LLMs) for out-of-the-box machine trans-\nlation. The project began when the Brussels-based\nlanguage technology company iguano-don.ai re-\nceived a request from a client who wanted to know\nif short Arabizi texts could be translated for gisting\npurposes. The study involves a collaboration be-\ntween the start-up and a professional translator with\nprevious experience in Arabizi. Our contribution\nincludes an authentic dataset in Arabizi for three\ndialects and a comparative evaluation of five LLMs\nusing different prompting strategies. To our knowl-\nedge, this is the first study that has explored the\ndirect translation from different dialects in Arabizi\nto MSA or English without prior deromanization.\nThe rest of this paper is organized as follows.\nWe describe the data production methodology and\nresulting dataset in Section 2, followed by the ex-\nperimental setup in Section 3 and the results in\nSection 4. Finally, discussion of results and the\nlimitations of this study are presented in Section 5\nand Section 6.\n2\nData Collection and Dataset\n2.1\nDialects\nWe decided to focus on the translation of\nthree Arabic dialects from three distinct coun-\ntries—Lebanon, Egypt, and Algeria—into two tar-\nget languages: MSA, a less-resourced language,\nand English. These three countries were selected\nbecause their dialects represent distinct linguistic\nvarieties. The Lebanese dialect aligns with the\nLevantine group and the Algerian dialect with the\nMaghrebi group, while the Egyptian dialect is ex-\nceptionally prominent due to its widespread use\nand cultural influence.\nThe Lebanese dialect reflects a rich history and\nvarious cultural influences. Ancient languages such\nas Aramaic and Syriac, once dominant in north-\nern Lebanon, had a notable impact on the dialect,\nparticularly when it comes to phonological fea-\ntures like the use of silent vowels. Other regions,\ncloser to major coastal cities, feature dialects more\naligned with Classical Arabic, with fewer phono-\nlogical deviations. Lebanon’s Ottoman past also\nshaped its linguistic landscape, with Turkish loan-\nwords becoming integral to Lebanese lexicon after\nfour centuries of Ottoman rule (Iskandar, 2022;\nAl Almaoui, 2024).\nEgyptian Arabic evolved through layers of his-\ntorical migrations, demographic shifts, and ancient\nlinguistic roots. It was heavily influenced by Cop-\ntic, the language of ancient Egypt, and later by\nArabic after the Islamic conquest in the 7th cen-\ntury. Over time, Egyptian Arabic absorbed linguis-\ntic elements from Greek, Turkish, Italian, French,\nand English during various periods of occupation\nand cultural exchange. Regional variations within\nEgypt further enrich its linguistic diversity: north-\nern regions, including the Delta and Cairo, feature\nsubtle dialectal differences, while Upper Egypt’s\nSa’idi Arabic retains more conservative features.\nAdditionally, Bedouin communities in the Western\nDesert speak Arabic varieties that are distinct from\nurban Egyptian Arabic (Souag, 2009; Bettega et al.,\n2022).\nAlgerian Arabic is a product of extensive histor-\nical and cultural interactions. Indigenous Berber\nlanguages, particularly Tamazight, form its linguis-\ntic foundation, while successive occupations intro-\nduced other influences. The Roman era brought\nLatin, especially in administration and religion;\nthe influence of this language was then further re-\ninforced by Christian scholars such as Saint Au-\ngustine. The Arab conquest in the 7th century\nmade Arabic the language of faith and the elite.\nTamazight continued to be used in day-to-day life.\nSubsequent occupations by the Spanish, the Ot-\ntomans, and the French contributed lexical and\nstructural elements to the dialect. French, in par-\nticular, had a profound impact during colonial rule,\nshaping Algeria’s modern plurilingual society. Al-\ngerian Arabic is marked by significant regional\n2\n\n\nvariation. Western regions display a strong Spanish\ninfluence, while central areas, including Algiers,\nare heavily influenced by French. Eastern regions,\nsuch as Constantine, retain more Classical Arabic\nfeatures. Southern regions, including the Sahara,\nexhibit notable Berber linguistic characteristics, re-\nflecting the enduring presence of Berber-speaking\npopulations (Saadane and Habash, 2015; Chami,\n2009).\n2.2\nParticipants\nThirty-one participants were recruited for the study\nthrough LinkedIn and targeted recruitment mes-\nsages, with at least four participants per sub-dialect\nto ensure balanced representation. All participants\nwere native Arabic speakers who represent the spe-\ncific regional varieties outlined in the previous sec-\ntion. Lebanese participants were selected from\nboth southern and northern regions of Lebanon.\nSimilarly, Algerian participants were drawn from\nAlgiers, the capital, and Constantine to reflect dis-\ntinct linguistic traits within the country. For Egypt,\nparticipants were recruited from Cairo in the north\nand Luxor in the south.\nParticipants were asked to share WhatsApp con-\nversations they had engaged in with peers of a sim-\nilar age group (20–35 years) and from the same\nregions as them. These conversations revolved\naround a range of everyday topics, in order to re-\nflect natural and spontaneous interactions. The\nfocus on this age demographic provided a degree\nof consistency in communication styles, as partici-\npants shared a common digital literacy and texting\nculture.\nAfter\nthe\ncorpus\nwas\ncollected,\nit\nwas\nanonymized to ensure privacy. Subsequently, a\nprofessional Arabic-speaking translator translated\nthe corpus into MSA and English, with these trans-\nlations serving as reference texts for automatic met-\nrics. The translation into Arabic represents an in-\ntralingual transformation from a dialectal and infor-\nmal variety of Arabic to a formal and standardized\nform.\nTable 1 presents the collected corpora, including\nthe number of segments and tokens, the average\nnumber of tokens per sentence and the percentage\nof foreign and mixed words (code-switching).\n3\nExperimental Setup\nWe carried out a systematic evaluation of transla-\ntion quality using an automated protocol. For each\ndialect, we created a combination of parameters\ndefined as follows:\n• target language ∈[EN, MSA]\n• prompt_language ∈[EN, MSA]\n• prompt_strategy ∈[no-shot, one-shot, two-shot]\n• prompt_variation ∈[Lebanon, Egypt, Algeria]1\n• model ∈[GPT4o, Llama 3, Claude, Gemini, Gemma,\nMistral, Jais].\nAll models were prompted with a temperature\nof 0.5. A discussion of the chosen models and\nprompts is available in 3.1 and in 3.2. Evaluation\nmetrics are presented in 3.3. Our code will be\nshared upon acceptance.\n3.1\nModels\nThe models used in the experiments are all decoder-\nonly transformer (Vaswani et al., 2017) models\ngenerally called “generative LLMs”. We used a\nrange of instruction-tuned LLMs of different pa-\nrameter sizes (from 27B for Gemma to at least 70B\nfor Llama3, while proprietary models are expected\nto be much larger) to cover various models, from\nopen weights to proprietary, general purpose or, in\nthe case of Jais, ones that specifically target the\nEnglish-Arabic pair (Sengupta et al., 2023).\nIn this paper, “Llama” refers to Llama\n3.3\n70B-Instruct (Dubey et al., 2024),\n“GPT-\n4o” to gpt-4o-2024-08-06 (OpenAI, 2024),2\n“Claude” to Anthropic’s 3.5 Sonnet,3 “Gemma”\nto gemma-2-27b-it (Gemma Team et al., 2024),\nand, also from Google, “Gemini” to the latest Gem-\nini 1.5 Pro version (Gemini Team et al., 2024).4\n“Mistral” is Mistral\nLarge\n24.11 from the\neponymous company and, finally, “Jais” refers\nto jais-family-30b-16k-chat (Sengupta et al.,\n2023).\n3.2\nPrompts\nIn order to achieve the best translation results, we\nbuilt on He (2024)’s findings by assigning the role\nof a professional translator to our LLM. This ap-\nproach outperformed both simpler prompts and\nthose with excessive context. Furthermore, for each\nof the three main dialects, we used three prompt\nstrategies: no-shot, one-shot, and two-shot, all writ-\nten in English. These prompts were the same across\nregions, except for the specific mention of each di-\nalect in the corresponding prompts. The examples\n1More on this in Subsection 3.2.\n2https://platform.openai.com/docs/models/\ngpt-4o#gpt-4o\n3anthropic.claude-3-5-sonnet-20240620-v1:0\n4December 2024 release.\n3\n\n\nCountry\nRegion\nNumber of\nNumber of\nTotal\nTokens per\nEnglish\nFrench\nMixed\n% code-switching\nsegments\ntokens\ntokens\nsegment\nwords\nwords\nwords\nin corpus\nLebanon\nNorth\n127\n508\n1075\n3.1\n55\n12\n0\n13.19%\nSouth\n141\n567\n33\n11\n0\n7.76%\nEgypt\nCairo\n117\n601\n1159\n8.1\n28\n0\n0\n4.65%\nLuxor\n42\n558\n3\n0\n0\n0.5%\nAlgeria\nAlgiers\n145\n639\n1164\n3.8\n59\n3\n8\n10.95%\nConstantine\n99\n525\n52\n1\n5\n11%\nTable 1: Summary of segment, token, and foreign word counts by region\nused in the one- and two-shot configurations are\nnot part of the evaluated set, and are from the Al-\ngerian and Lebanese dialects. We further refined\nand duplicated these prompt variations to cover\ntwo target languages: one set asked for translation\ninto English and the other into MSA. Finally, all\nprompts were translated into Arabic by a native\nArabic speaker who is also a professional transla-\ntor. In total, we ran experiments with 36 unique\nprompts (3 regions * 3 strategies * 2 target lan-\nguages * 2 prompt languages), or 18 per target lan-\nguage, which we used on all models. The prompts\nin English are available in Appendix A, while their\nequivalents in Arabic will be released as part of the\ncode release upon acceptance.\n3.3\nMT Evaluation\nWe used automatic metrics and evaluated the poten-\ntial of using LLM-as-a-judge for direct assessment\nevaluation.\n3.3.1\nAutomatic Evaluation\nWe used several metrics to quantify the quality\nof the generated translations. On the more clas-\nsical side we use BLEU (Papineni et al., 2002),\nchrF (Popovi´c, 2015) and TER (Snover et al.,\n2006). All scores were calculated using Sacre-\nBLEU (Post, 2018).5 In order to avoid the usual\npitfalls of word- and character-based metrics, espe-\ncially since we were studying dialects without for-\nmal orthography, we further investigated the qual-\nity of the translations using techniques based on\nsub-word embeddings: BERTScore (Zhang et al.,\n2019) and two versions of COMET: COMET-22\n(Rei et al., 2022a, Unbabel/wmt22-comet-da) and\nits reference-free version CometKiwi (Rei et al.,\n2022b, Unbabel/wmt22-cometkiwi-da). The lat-\nter three methods help alleviate two limitations of\nour work: the fact that only one reference transla-\n5The relevant signatures are provided in Appendix D.\ntion is available for each sentence and the extremely\nshort length of certain sentences.\n3.3.2\nHuman Evaluation\nSince no Arabizi-specific metric or resource ex-\nists for our dialect selection, we assessed whether\nLLMs in an “LLM-as-a-judge” setting (Zheng\net al., 2023) can be used to mimic human evalua-\ntion to reduce the reliance on hard-to-source users\nof Arabizi.\nFor human evaluation, we adopted the direct\nassessment method, which evaluates translations\nbased on a Likert scale ranging from 1 to 5 (higher\nis better) according to two key criteria: fluency\nand adequacy (See Appendix B). Due to time and\nhuman resource constraints, we did not manually\nannotate all translations. We instead sampled a ran-\ndom machine translation for both target languages\nand for each source sentence of our dataset. These\nmachine translation outputs were sampled across\nall our variables, i.e. models, prompt languages,\nand prompt strategies. The resulting set, consisting\nof 671 segments (268 for Lebanon, 159 for Egypt\nand 244 for Algeria, see Table 1) for each target\nlanguage, was then manually rated by two native\nspeakers of Arabic who are professional translators,\none of them being the first author of this study and\nthe original translator of the dataset. We then calcu-\nlated Cohen (1960) κ to measure their agreement\nin terms of fluency and accuracy (see Table 2 for\nresults). Cohen (1960) κ results indicate moder-\nate agreement for adequacy and lower agreement\nfor fluency, with some variations across language\npairs.\nThe set, which not only consisted of the refer-\nence and machine translation but also of the source\nsentence, was then iteratively fed into gpt4o in\nan “LLM-as-a-judge” setting (Zheng et al., 2023),\nwith a prompt in English tasking the LLM to fol-\n4\n\n\nlow the human annotation guidelines.6 The LLM\nshowed strong correlation with both human anno-\ntators, with Spearman (1904)’s ρs comprised in the\nrange from 0.457 (annotator 2, fluency, Egypt to\nEN) to 0.844 (annotator 2, adequacy, Egypt to AR).\nThese results indicate that an LLM can be used\nas an easy way to gauge translation quality during\nmodel development. The different correlations as\nwell as all the data for direct assessment can be\nfound in Appendix E.\n4\nResults\n4.1\nQualitative Error Analysis\nDue to space constraints, this section will only pro-\nvide some examples of the main errors (full annota-\ntion will be provided upon acceptance). Most mod-\nels tend to mistranslate, especially when figurative\nlanguage is used. A larger issue lies with Llama3\nwhich tends to output words in another script when\ntranslating to MSA. An obvious example is the\ntranslation of the segment Almatar da (PA¢ÖÏ@ @ \tYë,\n“this airport”) as Q¢ÖÏ@ да – transforming the “da”\nin Arabic to Cyrillic. The problem is not limited to\nCyrillic, as characters in Latin and Chinese scripts\ncan also be found in the output. Another type of\nfailure specific to a model is Jais’ re-occuring hal-\nlucinations. The model often associates feelings of\nanger to an otherwise neutral message, leading to\ntranslations that are irrelevant and contain violent\ninformation.\n4.2\nQuantitative Analysis\nSee Appendix F for the complete set of results.\n4.2.1\nEffect of Prompting Techniques\nOn one hand, one-shot prompting for translations\ninto English increased BLEU scores across all mod-\nels. For example, in GPT-4o, the BLEU score\nimproved from 17.386 for no-shot to 20.158 for\none-shot, a 16% increase. Two-shot prompting,\nhowever, provided only a marginal gain or even\nslight variation. For instance, in GPT-4o, the BLEU\nscore slightly dropped from 20.158 for one-shot to\n19.771 for two-shot. On the other hand, the im-\nprovements to translations into Arabic were less\npronounced, suggesting that few-shot prompting is\nless effective. In GPT-4o, BLEU increased from\n8.395 for no-shot to 10.099 for one-shot, a 20%\nincrease, but the shift from one-shot to two-shot\n6The prompt is shared in Appendix C.\n(10.150) was minimal. Similarly, in the case of\nClaude-3, the BLEU score improved from 2.982\nfor no-shot to 4.009 for one-shot, a 34% increase,\nbut two-shot promting (4.016) provided almost no\nadditional benefit.\n4.2.2\nEffect of Target Language\nEnglish translations consistently outperformed Ara-\nbic translations across all metrics, indicating that\nmodels handle English more effectively. For in-\nstance, GPT-4o achieved higher BLEU scores in\nEnglish (17.39 to 20.16) than in Arabic (8.40 to\n10.15), with chrF scores following a similar trend\n(43.08 to 45.50 for English vs. 36.64 to 38.09 for\nArabic). TER also confirmed that English transla-\ntions required fewer edits, with scores of 70.29 for\none-shot compared to 78.70 for Arabic. Other mod-\nels, such as Claude-3 and LLaMA-3, exhibited sim-\nilar disparities, with English BLEU scores nearly\ndoubling those of Arabic. Both COMET metrics\nand BERTScore further highlighted this gap, al-\nthough BERTScore pointed to different alignment\ncharacteristics between languages. While GPT-4o\nand Gemini were the strongest models for Arabic,\ntheir scores still lagged behind their English per-\nformance, reinforcing the overall trend of English\ntranslations being more accurate and consistent.\n4.2.3\nEffect of Source Dialect\nThe evaluation of translation performance across\ndifferent dialects revealed notable variations in\nquality, as measured by the different translation\nmetrics (cf Appendix 7). The Egyptian dialect\ndemonstrated the highest translation quality, with\nan average BLEU score of 9.65 and a chrF score of\n34.64, indicating the highest word- and character-\nlevel accuracy. Additionally, Egyptian achieved a\nBERTScore of 0.37 and a COMET score of 0.67,\nsuggesting higher semantic similarity to reference\ntranslations. The Lebanese dialect followed with\na BLEU score of 7.52 and a chrF score of 26.59,\nwith a comparable COMET Kiwi score of 0.48\nbut a slightly lower COMET score of 0.65. The\nAlgerian dialect ranked third, with a significantly\nlower BLEU score of 4.24 and a chrF score of\n23.21, along with the lowest BERTScore of 0.33\nand COMET score of 0.63.\nThe disparity in translation quality among the\ndialects could be explained by linguistic, sociocul-\ntural, and technological factors. Egyptian Arabic,\nthe most widely spoken and documented dialect,\naligns closely with MSA and is predominant in\n5\n\n\nLB-AR\nLB-EN\nEG-AR\nEG-EN\nALG-AR\nALG-EN\nAdequacy\n0.56\n0.39\n0.34\n0.33\n0.49\n0.45\nFluency\n0.24\n0.31\n0.21\n0.52\n0.28\n0.31\nTable 2: Agreement scores (Cohen (1960)’s κ) for different language pairs for Adequacy and Fluency (Koehn and\nMonz, 2006). LB = Lebanon, EG = Egypt, ALG = Algeria, {AR, EN} = Translation to {Arabic, English}\nthe media, ensuring better representation in train-\ning datasets. By contrast, Algerian Arabic’s heavy\ncode-switching (cf Table 1) with Berber, French,\nand Spanish, along with figurative word mean-\nings, make translation more challenging. Its lack\nof representation in digital corpora further limits\nLLMs training, resulting in poorer translation per-\nformance.\n4.2.4\nEffect of Prompt Language\nAs seen in a prior article (Zhang et al., 2023), our\nresults confirm that prompting in English generally\nyields better results across all models.\n4.3\nMetrics Correlation\nBecause traditional metrics such as BLEU and chrF\nquantify n-gram overlap with the reference, thereby\nrewarding surface-level similarity and penalizing\ndeviations, they tend to produce correlated scores\nand inversely correlate with TER.\nMeanwhile, embedding-based metrics such as\nBERT Score and COMET rely on learned contex-\ntual representations to gauge semantic similarity,\nthus capturing deeper nuances in meaning and tol-\nerating surface-level variations, which often leads\nthem to yield patterns that are distinct from n-gram-\nfocused measures.\nAcross the different combinations, BLEU and\nchrF scores typically fluctuated in parallel. How-\never, certain model-prompt settings revealed in-\nconsistencies, where BLEU increased while BERT\nScore or COMET remained unchanged or declined,\nindicating improved n-gram overlap but not neces-\nsarily better semantic accuracy or fluency. Despite\nthese inconsistencies, higher BLEU generally cor-\nrelated with good embedding-based metrics scores.\n5\nConclusion and Discussion\nModels struggle significantly with Arabizi. GPT-\n4o is the best-performing translation model, fol-\nlowed by Gemini. Mistral Large and Gemma per-\nform moderately well, while Llama 3 and Jais are\nthe weakest models (see Appendix H). Interest-\ningly, Gemma performed surprisingly well in trans-\nlation tasks despite being a 27B parameter model.\nIts results, particularly in English, were compet-\nitive with larger models, suggesting that model\nsize is not the only determinant of translation qual-\nity—architectural optimizations and training data\nalso play a crucial role.\nFor model prompting, few-shot approaches im-\nproved performance but was more effective for En-\nglish than for Arabic. English prompts worked bet-\nter overall and in all prompting scenarios, though\nthe difference was much less stark for GPT-4o and\nGemini and, to a lesser extent, for Gemma.\nDespite a large variation in average segment\nlength between different dialects, no clear pattern\nemerged in terms of automatic scores. This hints\nthat translation quality does not directly depend on\nsegment length.\nThe LLM-as-a-Judge scenario aligned with ex-\npert human raters, making it a relevant tool in\nthis setting. This study further shows that while\nfar from perfect, using “out-of-the-box” LLMs to\ntranslate Arabizi is a viable solution for gisting, es-\npecially when combined with an LLM-as-a-judge.\n6\nLimitations\nThis study has several limitations. First, the dataset\ndoes not fully capture the diversity of Arabizi us-\nage across different regions and social contexts.\nSecond, it relies on translators who are non-native\nspeakers of English. Third, the variety of text\nlengths may affect performance, as shorter or\nlonger texts might yield varying results. Further-\nmore, no Arabizi-specific evaluation metric was\nused, which can affect the accuracy of the assess-\nments. Lastly, the study was constrained by a rel-\natively small corpus, which may limit the applica-\nbility of its findings.\n7\nCO2 Emission Related to Experiments\nIt is difficult to estimate the energy usage of models\nthat were run in an “inference-as-a-service” setting,\nespecially when the details of such models are pro-\nprietary. Using the tool provided by Lannelongue\net al. (2021)7 and basing our calcuations on model\n7https://calculator.green-algorithms.org/ai\n6\n\n\nsizes of around 400B parameters for the proprietary\nmodels, we estimate that the energy usage of our\nexperiments amounted to 6.99 kWh in a US data-\ncenter, which corresponds to a carbon footprint of\n2.97 kgCO2e.\nReferences\nPerla Al Almaoui. 2024. ChatGPT comme outil de\ntraduction automatique des discussions WhatsApp :\névaluation de la traduction de différents dialectes en\nArabizi. Master’s thesis, Université de Genève.\nWid H Allehaiby. 2013. Arabizi: An analysis of the ro-\nmanization of the arabic script from a sociolinguistic\nperspective. Arab World English Journal, 4(3).\nAnthropic. The Claude 3 model family: Opus, Sonnet,\nHaiku.\nSimone Bettega, Roberta Morano, et al. 2022. The\nclassification of Arabic dialects: Traditional ap-\nproaches, new proposals, and methodological prob-\nlems. MDPI.\nAbdelkarim Chami. 2009. A historical background of\nthe linguistic situation in algeria.\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and psychological mea-\nsurement, 20(1):37–46.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nGemini Gemini Team, Petko Georgiev, Ving Ian Lei,\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang,\net al. 2024.\nGemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530.\nGemma Gemma Team, Thomas Mesnard, Cassidy\nHardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivière, Mihir San-\njay Kale, Juliette Love, et al. 2024. Gemma: Open\nmodels based on gemini research and technology.\narXiv preprint arXiv:2403.08295.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nImane Guellil, Faiçal Azouaou, Mourad Abbas, and\nSadat Fatiha. 2017. Arabizi transliteration of Alge-\nrian Arabic dialect into Modern Standard Arabic. In\nSocial MT 2017/ First workshop on Social Media\nand User Generated Content Machine Translation,\nPrague, Czech Republic.\nSalima Harrat, Karima Meftouh, and Kamel Smaili.\n2019. Machine translation for arabic dialects (sur-\nvey).\nInformation Processing & Management,\n56(2):262–273. Advance Arabic Natural Language\nProcessing (ANLP) and its Applications.\nSui He. 2024. Prompting ChatGPT for translation: A\ncomparative analysis of translation brief and persona\nprompts. In Proceedings of the 25th Annual Con-\nference of the European Association for Machine\nTranslation (Volume 1), pages 316–326, Sheffield,\nUK. European Association for Machine Translation\n(EAMT).\nAmine Jules Iskandar. 2022.\nÀ l’origine du parler\nlibanais (parts 1 and 2).\nPart 1 available at:\nhttps://terredecompassion.com/2022/03/\n01/a-lorigine-du-parler-libanais-1-2/,\nPart\n2\navailable\nat:\nhttps://\nterredecompassion.com/2022/03/08/\na-lorigine-du-parler-libanais-2-2/.\nAc-\ncessed: 2025-01-01.\nPhilipp Koehn and Christof Monz. 2006. Manual and\nautomatic evaluation of machine translation between\nEuropean languages. In Proceedings on the Work-\nshop on Statistical Machine Translation, pages 102–\n121, New York City. Association for Computational\nLinguistics.\nLoïc Lannelongue, Jason Grealey, and Michael In-\nouye. 2021. Green algorithms: quantifying the car-\nbon footprint of computation. Advanced science,\n8(12):2100707.\nImane\nMomayiz,\nAissam\nOutchakoucht,\nOmar\nChoukrani, and Ali Nirheche. 2024. Terjamabench:\nA culturally specific dataset for evaluating translation\nmodels for moroccan darija.\nOpenAI. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMaja Popovi´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\n7\n\n\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022a. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Marcos Treviso, Nuno M. Guerreiro,\nChrysoula Zerva, Ana C Farinha, Christine Maroti,\nJosé G. C. de Souza, Taisiya Glushkova, Duarte\nAlves, Luisa Coheur, Alon Lavie, and André F. T.\nMartins. 2022b. CometKiwi: IST-unbabel 2022 sub-\nmission for the quality estimation shared task. In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 634–645, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nHouda Saadane and Nizar Habash. 2015. A conven-\ntional orthography for Algerian Arabic. In Proceed-\nings of the Second Workshop on Arabic Natural Lan-\nguage Processing, pages 69–79, Beijing, China. As-\nsociation for Computational Linguistics.\nNeha Sengupta, Sunil Kumar Sahu, Bokang Jia,\nSatheesh Katipomu, Haonan Li, Fajri Koto, William\nMarshall, Gurpreet Gosal, Cynthia Liu, Zhiming\nChen, et al. 2023.\nJais and jais-chat:\nArabic-\ncentric foundation and instruction-tuned open gen-\nerative large language models.\narXiv preprint\narXiv:2308.16149.\nAli Shazal, Aiza Usman, and Nizar Habash. 2020. A\nunified model for Arabizi detection and translitera-\ntion using sequence-to-sequence models. In Proceed-\nings of the Fifth Arabic Natural Language Processing\nWorkshop, pages 167–177, Barcelona, Spain (Online).\nAssociation for Computational Linguistics.\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea\nMicciulla, and John Makhoul. 2006. A study of trans-\nlation edit rate with targeted human annotation. In\nProceedings of the 7th Conference of the Association\nfor Machine Translation in the Americas: Technical\nPapers, pages 223–231, Cambridge, Massachusetts,\nUSA. Association for Machine Translation in the\nAmericas.\nLameen Souag. 2009. Siwa and its significance for\nArabic dialectology. Zeitschrift für arabische Lin-\nguistik = Journal of Arabic linguistics = Journal de\nlinguistique arabe, 51:51–75.\nC. Spearman. 1904. The proof and measurement of as-\nsociation between two things. The American Journal\nof Psychology, 15(1):72–101.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Neural Information Processing Systems.\nNikolai Vladimirovich Yushmanov. 1961. The structure\nof the arabic language.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine transla-\ntion: A case study. Preprint, arXiv:2301.07069.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019.\nBERTScore:\nEvaluating text generation with bert. arXiv preprint\narXiv:1904.09675.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36:46595–46623.\n8\n\n\nA\nPrompts in English for all Dialects, Target Languages, and Prompt Strategies\n“ALG” stands for Algeria, “EG” for Egypt, and “LB” for Lebanon. For the experiments with a prompt in\nArabic, all prompts were translated into Modern Standard Arabic by the first author of the study, who is a\nnative speaker of Arabic and a professional translator.\n{\n\"ALG_AR\": {\n\"no-shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nTranslate the following text from the Algerian dialect to Modern\nStandard Arabic.\",\n\"one -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in Arabic: \"éËñ\u0010®\u0010K AÓ Ñê\t¯\r@ B\"\nBased on the example above , translate the following text from the Algerian\ndialect to Modern Standard Arabic.\",\n\"two -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in Arabic: \"éËñ\u0010®\u0010K AÓ Ñê\t¯\r@ B\"\nSource text: \"M t7kilich 7yetk kho\"\nTarget text in Arabic: \"ú\n \tk\r@ ½\u0010KAJ\nk \u0010é \u000f\u0010¯ \u000fú\nÎ« \u0010®\u0010K B\"\nBased on the examples above , translate the following text from the Algerian\ndialect to Modern Standard Arabic.\"\n},\n\"ALG_EN\": {\n\"no-shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nTranslate the following text from the Algerian dialect to English.\",\n\"one -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in English: \"I don 't understand what you 're saying.\"\nBased on the example above , translate the following text from the Algerian\ndialect to English.\",\n\"two -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in English: \"I don 't understand what you 're saying.\"\nSource text: \"M t7kilich 7yetk kho\"\nTarget text in English: \"Don 't tell me your life story, bro\"\nBased on the examples above , translate the following text from the Algerian\ndialect to English.\"\n},\n\"EG_AR\": {\n\"no-shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nTranslate the following text from the Egyptian dialect to Modern\nStandard Arabic.\",\n9\n\n\n\"one -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in Arabic: \"éËñ\u0010®\u0010K AÓ Ñê\t¯\r@ B\"\nBased on the example above , translate the following text from the Egyptian\ndialect to Modern Standard Arabic.\",\n\"two -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in Arabic: \"éËñ\u0010®\u0010K AÓ Ñê\t¯\r@ B\"\nSource text: \"M t7kilich 7yetk kho\"\nTarget text in Arabic: \"ú\n \tk\r@ ½\u0010KAJ\nk \u0010é \u000f\u0010¯ \u000fú\nÎ« \u0010®\u0010K B\"\nBased on the examples above , translate the following text from the Egyptian\ndialect to Modern Standard Arabic.\"\n},\n\"EG_EN\": {\n\"no-shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nTranslate the following text from the Egyptian dialect to English.\",\n\"one -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in English: \"I don 't understand what you 're saying.\"\nBased on the example above , translate the following text from the Egyptian\ndialect to English.\",\n\"two -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in English: \"I don 't understand what you 're saying.\"\nSource text: \"M t7kilich 7yetk kho\"\nTarget text in English: \"Don 't tell me your life story, bro\"\nBased on the examples above , translate the following text from the Egyptian\ndialect to English.\"\n},\n\"LB_AR\": {\n\"no-shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nTranslate the following text from the Lebanese dialect to Modern\nStandard Arabic.\",\n\"one -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in Arabic: \"éËñ\u0010®\u0010K AÓ Ñê\t¯\r@ B\"\nBased on the example above , translate the following text from the Lebanese\ndialect to Modern Standard Arabic.\",\n\"two -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\n10\n\n\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in Arabic: \"éËñ\u0010®\u0010K AÓ Ñê\t¯\r@ B\"\nSource text: \"M t7kilich 7yetk kho\"\nTarget text in Arabic: \"ú\n \tk\r@ ½\u0010KAJ\nk \u0010é \u000f\u0010¯ \u000fú\nÎ« \u0010®\u0010K B\"\nBased on the examples above , translate the following text from the Lebanese\ndialect to Modern Standard Arabic.\"\n},\n\"LB_EN\": {\n\"no-shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nTranslate the following text from the Lebanese dialect to English.\",\n\"one -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in English: \"I don 't understand what you 're saying.\"\nBased on the example above , translate the following text from the Lebanese\ndialect to English.\",\n\"two -shot\": \"You are a professional Arabic translator with years of\nexperience translating spoken language from various Arabic dialects.\nSource text: \"Ma 3am efham chu 3am te7ke\"\nTarget text in English: \"I don 't understand what you 're saying.\"\nSource text: \"M t7kilich 7yetk kho\"\nTarget text in English: \"Don 't tell me your life story, bro\"\nBased on the examples above , translate the following text from the Lebanese\ndialect to English.\"\n}\n}\nB\nAdequacy and Fluency\nScore\nAdequacy\nFluency\n5\nAll Meaning\nFlawless Language\n4\nMost Meaning\nGood Language\n3\nMuch Meaning\nNon-native Language\n2\nLittle Meaning\nDisfluent Language\n1\nNone\nIncomprehensible Language\nTable 3: Adequacy and Fluency Evaluation Scale (Koehn and Monz, 2006)\nC\nLLM-as-a-judge\nThe system prompt was the following:\nYou are a professional translator, expert in Arabic, English, and Arabic dialects. Your role here\nis to evaluate the quality of a translation using two dimensions: ‘Adequacy’ (scale of 1 to 5,\nhigher is better) and ‘Fluency’ (scale of 1 to 5, higher is better). You will be given a source\ntext in Arabic dialect, a reference translation into {target_lang}, and a machine translation.\nReturn in this format, and NOTHING ELSE:\nAdequacy:[your_score]\nFluency:[your_score]\n11\n\n\nI trust and count on you.\nThe prompt was the following:\nSource from {country}: {source}\nReference translation: {ref}\nMachine translation: {hyp}\nGive scores from 1 to 5 for both Adequacy and Fluency using the template:\nAdequacy:[your_score]\nFluency:[your_score]\nReturn nothing else.\nD\nMetrics Signatures\nThe BLEU signature is nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.5.1.\nThe chrF signature is nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.5.1.\nThe TER signature is nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.5.1.\nE\nHuman-LLM-as-a-judge Correlation and Direct Assessment\nCountry\nFluency\nAdequacy\nLLM-Rater 1\nLLM-Rater 2\nLLM-Rater 1\nLLM-Rater 2\nLebanon - EN\n0.602\n0.495\n0.653\n0.824\nLebanon - AR\n0.685\n0.601\n0.820\n0.795\nEgypt - EN\n0.631\n0.457\n0.667\n0.781\nEgypt - AR\n0.637\n0.611\n0.677\n0.844\nAlgeria - EN\n0.642\n0.536\n0.683\n0.800\nAlgeria - AR\n0.678\n0.485\n0.760\n0.770\nTable 4: Correlation Scores (Spearman (1904)’s ρ) Between Human Annotators and LLM-as-a-Judge in Direct\nAssessment Scores per Country and Target Language.\nCountry\nFluency\nAdequacy\nRater 1\nRater 2\nRater 1\nRater 2\nLebanon - EN\n2.494\n3.822\n2.203\n2.431\nLebanon - AR\n2.782\n3.430\n2.362\n2.662\nEgypt - EN\n2.560\n3.340\n2.082\n2.679\nEgypt - AR\n2.956\n3.538\n2.497\n2.887\nAlgeria - EN\n2.534\n3.773\n1.853\n2.315\nAlgeria - AR\n2.721\n3.500\n2.225\n2.335\nTable 5: Average Direct Assessment Scores (1-5) for Both Human Raters, per Country and Target Language.\n12\n\n\nF\nAutomatic Evaluation Results for all Models\nDue to space constraints, we are presenting the results averaged over the two prompt languages. The full\nset will be shared upon acceptance.\nModel\nTL\nPrompt Tech\nBLEU\nchrF\nTER\nBERT\nKIWI\nCOMET\ngpt4o\nEN\nno-shot\n17.386\n43.081\n77.038\n0.478\n0.434\n0.733\ngpt4o\nEN\none-shot\n20.158\n45.232\n70.287\n0.529\n0.439\n0.757\ngpt4o\nEN\ntwo-shot\n19.771\n45.496\n70.294\n0.521\n0.439\n0.755\ngpt4o\nAR\nno-shot\n8.395\n36.637\n86.501\n0.558\n0.422\n0.757\ngpt4o\nAR\none-shot\n10.099\n38.221\n78.701\n0.586\n0.423\n0.776\ngpt4o\nAR\ntwo-shot\n10.150\n38.090\n79.775\n0.585\n0.421\n0.774\nclaude3\nEN\nno-shot\n5.603\n25.565\n150.074\n0.270\n0.540\n0.620\nclaude3\nEN\none-shot\n8.795\n30.356\n97.400\n0.191\n0.536\n0.605\nclaude3\nEN\ntwo-shot\n9.433\n32.360\n96.325\n0.225\n0.541\n0.625\nclaude3\nAR\nno-shot\n2.982\n22.957\n122.794\n0.367\n0.428\n0.657\nclaude3\nAR\none-shot\n4.009\n28.169\n97.126\n0.420\n0.431\n0.686\nclaude3\nAR\ntwo-shot\n4.016\n28.473\n98.488\n0.425\n0.427\n0.686\nllama3\nEN\nno-shot\n6.972\n27.982\n107.160\n0.293\n0.526\n0.620\nllama3\nEN\none-shot\n8.234\n28.510\n101.095\n0.290\n0.518\n0.618\nllama3\nEN\ntwo-shot\n7.709\n27.988\n99.623\n0.274\n0.521\n0.613\nllama3\nAR\nno-shot\n2.196\n17.748\n160.334\n0.243\n0.412\n0.587\nllama3\nAR\none-shot\n2.862\n20.763\n118.675\n0.343\n0.425\n0.623\nllama3\nAR\ntwo-shot\n1.139\n17.728\n218.553\n0.286\n0.425\n0.610\ngemma2\nEN\nno-shot\n8.523\n27.979\n89.761\n0.330\n0.548\n0.634\ngemma2\nEN\none-shot\n9.866\n28.977\n88.589\n0.337\n0.543\n0.639\ngemma2\nEN\ntwo-shot\n9.925\n29.258\n87.319\n0.333\n0.545\n0.640\ngemma2\nAR\nno-shot\n3.583\n23.098\n99.113\n0.358\n0.429\n0.644\ngemma2\nAR\none-shot\n4.070\n24.175\n94.968\n0.388\n0.424\n0.660\ngemma2\nAR\ntwo-shot\n3.959\n24.400\n96.123\n0.389\n0.426\n0.664\nmistrallarge\nEN\nno-shot\n7.919\n29.002\n101.704\n0.285\n0.524\n0.627\nmistrallarge\nEN\none-shot\n9.409\n28.445\n91.376\n0.263\n0.518\n0.610\nmistrallarge\nEN\ntwo-shot\n9.259\n28.347\n91.020\n0.268\n0.521\n0.612\nmistrallarge\nAR\nno-shot\n4.019\n25.413\n103.042\n0.434\n0.493\n0.673\nmistrallarge\nAR\none-shot\n4.230\n24.663\n95.517\n0.428\n0.482\n0.672\nmistrallarge\nAR\ntwo-shot\n4.372\n25.162\n94.433\n0.426\n0.481\n0.669\njais\nEN\nno-shot\n1.518\n15.273\n344.316\n0.065\n0.470\n0.501\njais\nEN\none-shot\n2.157\n15.667\n160.450\n0.016\n0.511\n0.507\njais\nEN\ntwo-shot\n1.735\n15.958\n192.409\n0.042\n0.499\n0.508\njais\nAR\nno-shot\n0.750\n14.486\n208.837\n0.241\n0.420\n0.583\njais\nAR\none-shot\n0.847\n14.120\n196.811\n0.258\n0.418\n0.578\njais\nAR\ntwo-shot\n0.704\n14.418\n202.979\n0.270\n0.418\n0.579\ngemini\nEN\nno-shot\n11.317\n36.950\n94.146\n0.379\n0.493\n0.672\ngemini\nEN\none-shot\n16.119\n41.023\n78.911\n0.451\n0.493\n0.713\ngemini\nEN\ntwo-shot\n16.187\n41.182\n77.605\n0.455\n0.495\n0.720\ngemini\nAR\nno-shot\n5.174\n31.319\n90.174\n0.502\n0.470\n0.729\ngemini\nAR\none-shot\n6.636\n33.513\n84.945\n0.536\n0.468\n0.752\ngemini\nAR\ntwo-shot\n7.585\n33.987\n84.428\n0.541\n0.470\n0.753\nTable 6: Automatic Evaluation Results for all Models, Averaged over Prompt Languages. TL = Target Language,\nBERT = BERTScore, KIWI = wmt22-cometkiwi-da, COMET = wmt22-comet-da. Best scores for every model,\ntarget language, and prompt strategy are indicated in bold.\n13\n\n\nG\nAverage Metric Scores for Dialects\nCountry\nBLEU\nchrF\nTER\nBERTScore\nKIWI\nCOMET\nLebanon\n7.5212\n26.5935\n123.5180\n0.3604\n0.4799\n0.6547\nEgypt\n9.6466\n34.6404\n102.8187\n0.3699\n0.4749\n0.6749\nAlgeria\n4.2445\n23.2068\n122.1789\n0.3322\n0.4642\n0.6303\nTable 7: Translation Quality Scores for the Arabic Dialects\nH\nModel Ranking\nModel\nBLEU\nchrF\nTER\nBERTScore\nKIWI\nCOMET\ngpt4o\n14.326\n41.126\n77.099\n0.542\n0.429\n0.758\ngemini\n10.503\n36.329\n85.034\n0.477\n0.481\n0.723\ngemma2\n6.654\n26.314\n92.645\n0.355\n0.485\n0.646\nmistrallarge\n6.534\n26.838\n96.181\n0.350\n0.503\n0.643\nclaude3\n5.806\n27.980\n110.368\n0.316\n0.483\n0.646\nllama3\n4.851\n23.453\n134.240\n0.287\n0.471\n0.611\njais\n1.285\n14.986\n217.633\n0.148\n0.456\n0.542\nTable 8: Ranking of Translation Models from Best to Worst Based on Average Automatic Metric Scores\n14\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20973v1.pdf",
    "total_pages": 14,
    "title": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?",
    "authors": [
      "Perla Al Almaoui",
      "Pierrette Bouillon",
      "Simon Hengchen"
    ],
    "abstract": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}