{
  "id": "arxiv_2502.20948v1",
  "text": "Concealed Adversarial attacks on neural networks\nfor sequential data\nPetr Sokerin1*, Dmitry Anikin1, Sofia Krehova1, Alexey Zaytsev1\n1*LARSS laboratory, AI Center, Skoltech, Bolshoy Boulevard, 30, bld. 1,\nMoscow, 121205, Russia.\n*Corresponding author(s). E-mail(s): sokerinpo@mail.ru;\nContributing authors: Dmitry.Anikin@skoltech.ru;\nkrehovasofia@gmail.com; A.Zaytsev@skoltech.ru;\nAbstract\nThe emergence of deep learning led to the broad usage of neural networks in\nthe time series domain for various applications, including finance and medicine.\nWhile powerful, these models are prone to adversarial attacks: a benign targeted\nperturbation of input data leads to significant changes in a classifier’s output.\nHowever, formally small attacks in the time series domain become easily detected\nby the human eye or a simple detector model.\nWe develop a concealed adversarial attack for different time-series models: it pro-\nvides more realistic perturbations, being hard to detect by a human or model\ndiscriminator. To achieve this goal, the proposed adversarial attack maximizes\nan aggregation of a classifier and a trained discriminator loss. To make the\nattack stronger, we also propose a training procedure for a discriminator that\nprovides broader coverage of possible attacks. Extensive benchmarking on six\nUCR time series datasets across four diverse architectures — including recur-\nrent, convolutional, state-space, and transformer-based models — demonstrates\nthe superiority of our attack for a concealability-efficiency trade-off. Our find-\nings highlight the growing challenge of designing robust time series models,\nemphasizing the need for improved defenses against realistic and effective attacks.\nKeywords: adversarial attacks, concealed adversarial attacks, sequential data, neural\nnetworks\n1\narXiv:2502.20948v1  [cs.LG]  28 Feb 2025\n\n\n1 Introduction\nTime series data appear in a wide range of applications, from healthcare and finance\nto energy systems and environmental monitoring [1, 2]. Time series classification\nhas emerged as a critical task, enabling predictive and diagnostic insights that drive\ndecision-making in these domains [3, 4]. Neural networks have become a first choice in\nvarious such problems, being a powerful solution in this domain [5, 6] and one of the\nbest non-ensemble approaches. However, neural networks are known to be more sus-\nceptible to adversarial attacks — subtle data perturbations crafted to deceive models\nbeing undetectable to human observers [7]. This vulnerability poses significant risks,\nespecially in high-stakes applications like medicine, where incorrect predictions lead\nto severe consequences.\nAdversarial attacks have been studied extensively in image [8] and natural language\nclassification [9, 10]. Their numerous studies have laid the foundation for understand-\ning adversarial attacks and defenses. However, the time series modality presents unique\nchallenges. Time series data exhibit complex temporal dependencies, noise character-\nistics, and domain-specific variations that distinguish it from other modalities [11].\nThe key challenge in this domain is ensuring that adversarial perturbations remain\nconcealed, as anomalies in time series data are often easier for humans or machine\nlearning models to detect than in other modalities.\nConsequently, we need a more intricate constraint for adversarial perturbation that\nis wider than common l2 or l∞norm differences between the initial time series and\nits attacked counterpart. The common constraints lead to examples that are easy to\ndetect even by human eyes, as Figure 1 demonstrates. Given this definition, we should\ndefine a proper attack that resolves the issues with limited concealability.\nOriginal data\nAttacked data\n(a) An attack of an image classifier\n0\n10\n20\n30\n40\n50\nTime\n1\n0\n1\n2\n3\nOriginal data\nVanilla attack\nRegularized attack (ours)\n(b) An attack of a time-series classifier\nFig. 1: Examples of common adversarial attacks [12] in the computer vision domain\n(a) and the time series domain (b). In computer vision, adversarially perturbed\nand original images appear nearly identical to the human eye. In the time series\ndomain, standard adversarial attacks introduce noticeable artefacts, making them eas-\nily detectable. Our regularized approach generates more natural-looking perturbations,\nenhancing attack concealment.\n2\n\n\nThis work proposes a concealed adversarial attack properly defined for the time\nseries domain. In contrast to previous approaches, we can produce realistically look-\ning adversarial examples that are, by design, hard to detect with an additional\ndiscriminator. In more detail, our contributions are the follows:\n• A novel concealed adversarial attack. We find out that for the time series domain,\nthe attack should be more realistic to be undetectable to the human eye. So, the\nproposed method simultaneously looks at the initial model’s score and an additional\ndiscriminator constructed for attack detection. In time series classification, it is\nboth effective and concealed from human observers and domain-specific anomaly\ndetection.\n• An approach to train a discriminator that detects attacks of various strengths.\nThe training starts with strongly perturbed examples; then perturbations gradually\ndecrease to improve the ability to detect small attacks.\n• A detailed analysis of the inherent characteristics of time series data that make tra-\nditional adversarial attacks easily detectable, contrasting this with the challenges\nobserved in image-based attacks. We consider four different models, three attacks\nand six diverse datasets, e.g. Coffee, FreezerRegularTrain, GunPoint, GunPoint-\nMalevsFemale, PowerCons, and Strawberry. The model types include Transformers,\nConvolutional Neural Networks, Recurrent Neural Networks and State-Space mod-\nels. The attacks are two white-box attacks: an iterative Fast Gradient Sign Method\n(iFGSM), Projected Gradient Descent (PGD), and a black-box attack, SimBA. We\ncompare the proposed attack to reasonable alternatives, showing that it has the\nbest empirical quality. These findings also present a more detailed view of how\nadversarial attacks should look in the time series domain.\nTo facilitate reproducibility and further research, we provide full implementation\ndetails, including code and datasets, on our GitHub page.\n2 Related works\nTime series classification, a critical task within sequential data processing, has made\nnotable progress in part due to the emergence of deep learning. However, adversarial\nattacks discovered in computer vision (CV) have extended into the deep time series\nmodels. This section overviews neural time series classifiers and adversarial attacks\nand defenses, from their origins in computer vision to their extension into sequential\ndata. The conclusion highlights the specific challenges for concealed attacks in the\ntime series domain.\nTime series classification\nTime series classification (TSC) is a core task within time series analysis [13], crucial\nfor various diverse applications, including finance and healthcare.\nTraditional TSC approaches historically dominated the field, relying on feature\nengineering and classical machine learning. Such methods as One Nearest Neighbor\nwith Dynamic Time Warping [14], as well as more sophisticated methods, including\nCOTE [15] and Weasel [16], were the SOTA sequential classification models in the\n3\n\n\nlast decade. Building on these foundations, more recent advancements, such as HIVE-\nCOTE 2.0 (HC2) [17], have further pushed the boundaries of time series classification.\nThis ensemble approach consists of four main components: Temporal Dictionary\nEnsemble (TDE), Diverse Representation Canonical Interval Forest (DrCIF), The\nArsenal, and Shapelet Transform Classifier (STC).\nWhile HC2 remains one of the most advanced hybrid methods for time series clas-\nsification [5], deep learning approaches have gained traction for their ability to extract\ncomplex features and scale effectively [18], working well for many real-world appli-\ncations. However, no single deep learning architecture has emerged as the leading\napproach, with different models excelling in different scenarios. Convolutional neural\nnetworks (CNNs) quickly became popular, with models like ResCNN [19], which inte-\ngrated residual connections from ResNet [20], improving gradient flow and multi-scale\nfeature extraction. Recurrent architectures, including RNNs and LSTMs [21], were\nalso widely adopted, particularly for capturing long-term dependencies, with hybrid\napproaches like RNNAttention [22] combining recurrence with attention mechanisms.\nMore recently, transformer-based models have become popular, with PatchTST [23]\nemerging as a state-of-the-art approach. It segments time series into patches and\napplies self-attention to capture long-range dependencies efficiently. State space models\n(SSMs) have also emerged as a promising direction, with S4 [24] being a key example.\nS4 reparameterizes structured state matrices, enabling efficient handling of lengthy\nsequences and flexible switching between recurrent and convolutional representations.\nThese models demonstrate strong performance across diverse problems while excelling\nunder specific architectural constraints.\nBenchmark repositories like the UCR Time Series Classification Archive [11] and\nthe UEA Multivariate Time Series Archive [25] have greatly facilitated TSC model\ndevelopment. These repositories offer diverse datasets across multiple domains. The\nUCR archive, for instance, includes over 100 datasets covering tasks like human activ-\nity recognition, ECG classification, and sensor data analysis. Based on the current\nbenchmarking landscape, we selected six diverse datasets for univariate time series\nclassification.\nAdversarial attacks\nDeep neural networks, despite their ability to handle complex patterns, are suscepti-\nble to small data perturbations that result in a wrong prediction [7], called adversarial\nattacks. Szegedy et al. [7] were among the first to demonstrate that adding sub-\ntle, imperceptible noise to images could drastically alter predictions. This discovery\nprompted extensive research into attack strategies and defense mechanisms [26, 27].\nAdversarial attacks generally fall into white-box or black-box categories. White-\nbox attacks assume full access to the model’s parameters and architecture [28]. Early\nwork introduced the Fast Gradient Sign Method (FGSM) [12], which uses a single\ngradient step to perturb inputs. Its iterative variant (iFGSM) [29] repeats the process\nmultiple times for an attack. Projected Gradient Descent (PGD) [30] further enhances\niFGSM by projecting each perturbation into an lp-norm ball, targeting model corrup-\ntion more successfully. Black-box attacks have no access to model internals and rely\non query-based strategies. A strong Simple Black-box Adversarial Attacks (SimBA)\n4\n\n\n[31] iteratively add random vectors from a predefined orthonormal basis, guided solely\nby model output. This attack has proven highly successful in real-world environments.\nWe adopt iFGSM, PGD, and SimBA because they are straightforward, compet-\nitive, and readily adaptable to time series. While domain-specific methods continue\nto evolve, these established attacks remain competitive baselines for a broad range of\napplications.\nAdversarial attacks in time-series domain\nAdversarial attacks have received considerable attention in computer vision (CV) for\ntheir capacity to manipulate image classifiers. However, despite the growing use of\ndeep learning models in TSC, adversarial attacks remain relatively underexplored in\nthis field [32].\nWhite-box adversarial attacks, which use gradients [8] [28], expose model vulnera-\nbilities across domains. In [29], Fawaz demonstrates the effectiveness of straightforward\niFGSM and the Basic Iterative Method (BIM) on time series. However, iterative meth-\nods create perturbations that are easily noticeable, highlighting concealability as a key\nchallenge in time-series adversarial attacks.\nBeyond gradient-based approaches, researchers have begun developing attack\nmethods explicitly tailored to time series data. Karim et al. [33] introduced adver-\nsarial transformation networks, which train a neural network to generate adversarial\nexamples. Later, Harford et al. [34] extended this approach to multivariate time series,\nshowing its broader applicability.\nWhile adapting existing CV-based white-box methods like FGSM and BIM to time\nseries data has proven effective, the detectability of perturbations and developing time\nseries-specific attacks remain active research challenges.\nDetecting attacks in time-series domain and hidden attacks\nIn [35], authors present a method targeting FGSM and BIM attacks by treating iden-\ntifying adversarial samples as an outlier detection problem. The approach builds a\nnormality model using information-theoretic and chaos-theoretic measures, achieving\nup to 97% detection accuracy on datasets from the UCR Time Series Archive.\nThis highlights a central challenge in the field: attack concealment. Designing\nadversarial perturbations that not only mislead models but also remain impercepti-\nble to human observers or detection mechanisms is particularly difficult in time series\ndata, where minor modifications often result in abrupt, easily noticeable changes [36].\nTo address this, [37] proposes a multi-objective optimization approach incorpo-\nrating a “Camouflage Coefficient,” which balances attack efficiency with the sample’s\nproximity to its original class, improving concealability. Similarly, [38] introduces\na fused LASSO-type regularization to smooth adversarial perturbations and com-\nbines it with a Kullback-Leibler divergence loss to retain attack efficacy. While the\nauthors highlight that neural network models can detect adversarial perturbations,\ntheir approach does not use a discriminator to optimize concealability.\n5\n\n\nResearch gap\nThe adversarial attacks on neural time series classifiers remain detectable and require\nfurther research on their concealment. Introducing an additional discriminator can lead\nto more realistically-looking attacks without compromising their efficiency. Moreover,\nadditional research on discriminators as a defense is required to better understand\nhow protected existing models could be.\n3 Methodology\n3.1 Problem statement\nWe consider a time series classification model f(x) that takes a multivariate time\nseries x as an input. Our objective is to design a concealed adversarial attack h that\nmanipulates the predictions of a target model f(x) while being difficult to detect. To\nachieve the second goal, our method uses an additional discriminator D that aims to\ndetect adversarial examples. More formally:\n• Target Model f: The model that the adversarial attack aims to deceive, to generate\nperturbations that mislead its predictions.\n• Discriminator Model D: A model trained to distinguish adversarial data from\noriginal data. The aim is to create perturbations that are hard for D to detect.\n• Concealed Adversarial Attack h: A function that generates perturbations on the\ninput x to deceive the target model f while ensuring that the perturbations remain\nsubtle and undetectable by the discriminator D. To account for both components,\nthe attack uses an aggregation function g for outputs of f and D.\n3.2 Concealed attack construction\nWe design a pipeline for constructing a concealed adversarial attack depicted in\nFigure 2a. It consists of the following steps:\n1. Generate an adversarial dataset by applying an adversarial attack with low magni-\ntude. Objects from this adversarial dataset, labelled with 1, and the original data,\nlabelled with 0, are merged in a single dataset BD for the discriminator model D\ntraining.\n2. Train a discriminator model D on adversarial samples and samples from original\ndataset B.\n3. Define a discriminator-based adversarial attack.\nThe subsequent subsections discuss in detail all steps of the pipeline. We begin by\ndescribing a target classifier and a vanilla adversarial attack. Then, we present our\nmethodological contributions: the training scheme for a discriminator model and our\ndiscriminator-regularized concealed adversarial attack.\nClassification models for an attack\nThe classification target model f is trained on dataset B = {(xi, yi)}n\ni=1, including\nfeatures xi and target labels yi for an object i. The entire dataset includes feature\n6\n\n\n Our attack                                \n \n Vanilla attack    \n \n \nTraining data\n \nTarget model\nAttacked training\ndata\n2. Craft an\nadversarial\nsample\n1. Train a\ntarget model\nDiscriminator\nmodel\n3. Train a\ndiscriminator\n4. Define a reg. attack\n(a) Attack design scheme\n Our attack                                \n \nTest data\n \nTarget model\nAttacked test\ndata\nDiscriminator\nmodel\nTarget model\nDiscriminator\nmodel\nEfficiency\nmetric\nConcealability\nmetric\nSuccessfullness\nmetric\n1. Perform an reg. attack\n2. Сalculate metrics\n(b) Attack validation scheme\nFig. 2: Pipeline of definition (a) and validation (b) for an adversarial attack with the\ndiscriminator regularization. Firstly, we train the target model for an attack. Then, we\ngenerate adversarial data with a vanilla attack to train discriminator models to classify\nwhether the data object has been attacked or not. The next step is to apply a final\nattack on the original data to generate perturbed data objects to compromise both\nthe target model and the discriminator. Finally, we get Efficiency and Concealability\nmetrics to estimate the results of our final attack.\nmatrix X = {xi}n\ni=1 and label vector y = {yi}n\ni=1. The dataset is split into the train\npart (Xtrain, ytrain) and the test part (Xtest, ytest). The model predictions f(Xtest) are\ncompared with ground true labels ytest to estimate quality metrics. We train models\nusing a common cross-entropy loss L(f(Xtrain), ytrain).\nIn the field of time-series classification, there is no universally dominant model.\nWhile Transformer models, which excel in NLP, can be applied to time-series classifi-\ncation tasks, their effective training can be challenging due to the limited availability of\nlarge pre-training datasets. Convolutional Neural Networks (CNNs), Recurrent Neural\nNetworks (RNNs) and State Space Models also remain popular choices. In our study,\nwe employ the convolutional ResCNN model [19], the recurrent RNNAttention model\n[22], the Transformer-based model PatchTST [23], and a state-space models S4 [24].\n3.3 Adversarial attacks\nAdversarial attacks are methods for compromising target model f predictions. Mainly,\nadversarial attacks h add special adversarial perturbations h(x) to original data object\nfeatures x to get perturbed data object x′ = x + h(x). If an attack runs over several\niterations, we start with x0 = x and iteratively update it by generating xt+1 =\nxt + a(xt). We obtain our adversarial example at the last T-th iteration: x′ = xT .\nBelow, we consider different options for defining T and xT .\n7\n\n\nOur paper covers several frequently used attacks. The main requirements for an\nattack are Efficiency in other domains and simplicity in potential regularization. We\nselect two gradient-based attacks, iFGSM and PGD, and a black box attack, SimBA.\nThe detailed definitions for them are in Appendix A. As an example of the attack, we\nuse an iterative attack, iFSGM. It has the following form at t-th iteration:\nxt+1 = xt + ϵ sign\n\u0000∇xtL\n\u0000f(xt), y\n\u0001\u0001\n,\nwhere ϵ is the attack step size.\n3.4 Discriminator model\nDiscriminator for adversarial attacks\nOur discriminator D is trained to distinguish between normal x and adversarially\nperturbed objects x′ = x + h(x).\nTo train a discriminator, we need training data BD. It is composed of two sub-\nsamples: a subsample of initial training sample B0 = {(xi, 0)}n\ni=1 with normal x and\na sample B1\nθ = {(x′\ni, 1)}n\ni=1 of adversarially perturbed objects x′ = xi + hη(xi). The\nparameters θ define a specific attack and constitute, in most cases, the number of iter-\nations T and the attack step size ε. The labels 0 and 1 mark if an attack perturbed\nan object.\nThe discriminator model is used for two aims:\n• Metric of Concealability. The errors of the discriminator during an attack signify\nthe quality of an attack.\n• The tool to make the attack more hidden. The discriminator can serve as a regularizer\nto increase the concealability of an attack.\nRobust discriminator training\nVanilla training of a discriminator typically results in a weak classifier because we\ndon’t learn it to recognize the diverse attacked options with varying attack power\nand the number of steps. To overcome this limitation, we provide a more diverse and\ncomprehensive training set of adversarial examples. Additionally, following the cur-\nriculum learning, the attack amplitude gradually decreases during training, increasing\nthe problem’s complexity.\nThe steps for the discriminator training are in Algorithm 1. The thresholds for the\nminimal accuracy of the discriminator D(x) and reduction of the attack strength in\nthe algorithm are 0.9 and 0.8 correspondingly. They suit all considered samples and\nmodels. For some models and datasets, we need only 3−4 iterations to achieve chosen\ncriteria, for others, we train the discriminator with 7 −8 iterations.\n3.5 Concealed adversarial attacks\nTo make the attack more concealed, we modify a single perturbation step. Here is an\nexample of a modification for an iFGSM attack:\nxt+1 = xt + ϵ sign\n\u0002\n∇xtg\n\u0000Ltarget\n\u0000f(xt), y\n\u0001\n, Ldisc\n\u0000D(xt), ydisc\n\u0001\u0001\u0003\n.\n8\n\n\nAlgorithm 1 Iterative Robust Discriminator Training\nRequire: Clean data B0, initial attack strength εinit, total attack iterations T\n1: initialize ε = εinit\n▷εinit is typically large\n2: generate adversarial sample: Apply an attack with the strength ε and T\niterations to B0 to obtain B1\ngen = B1\nε.\n3: train a discriminator D(x): Use B0 and B1\ngen\n▷The initial discriminator\naccuracy is typically ∼1.\n4: while accuracy of D(x) > 0.9 do\n5:\nreduce the attack strength ε = 0.8 · ε\n6:\ngenerate a new adversarial sample B1\ngen = B1\nε\n7:\nfinetune the discriminator using B0 and B1\ngen\n8: end while\n9: return Robust discriminator model D(x)\nUsing an aggregation function g(·, ·) we take into account both Ltarget and Ldisc, that\nare loss functions of the target model and discriminator respectively. Inside them y\nand ydisc are true labels for the target task and the discriminator.\nSimilarly to GANs, the second loss is a log of the discriminator score that we want\nto minimize. But, unlikely in GAN, we use freezed discriminator model. So, our step\ntakes the form:\nxt+1 = xt + ϵ sign\n\u0002\n∇xtg\n\u0000Ltarget\n\u0000f(xt), ytarget\n\u0001\n, −log D(xt)\n\u0001\u0003\n.\nWe consider a simple sum aggregation function:\ng(x) = a(x) + αd(x) = Ltarget (f(x), y) −α log D(x).\nIt has a weight for the discriminator α and, according to it, takes into account both\nscores. Alternatively, our approach can use other aggregation functions, like a harmonic\naggregation function that has no hyperparameters to tune:\ng(x) = 2 Ltarget (f(x), y) · (−log D(xt))\nLtarget (f(x), y) −log D(x) + γ ,\nwhere γ is a small value to avoid the zero division problems.\nIn our work, we propose other variants of aggregation functions for target and\ndiscriminator losses. Namely, we would also consider a hypercone aggregation. Details\non it, as well as a more detailed examination of the properties of possible aggregation\nfunctions, are available in Appendix B.\n4 Results\nIn this section, we describe the results of experiments on applying adversarial attacks\nfor different models and datasets, starting with a proper validation scheme for the\nconsidered approaches.\n9\n\n\n4.1 Attack Efficiency and Concealability validation\nDuring validation, we measure Efficiency of an attack, how concelable it is, and overall\naggregated quality, a harmonic mean for them.\nEfficiency of the attack we defend is the ability to compromise the target model.\nSo we can get the attack efficiency as 1 minus F1 score for attack examples x′:\nE = 1 −F1(ytrue, f(x′)).\nConcealability of the attack is the discriminator model error:\nC = 1 −F1(ytrue\ndisc , D(x)).\nFinally, Successfulness S of the attack as the harmonic mean of Efficiency and\nConcealability:\nS = 2 C · E\nC + E .\nThe harmonic mean sharply decreases with an increase of each term C and E. All\nthree proposed metrics are the greater, the better.\nGiven the definition of the quality metrics, we obtain the validation scheme in\nFigure 2b:\n1. Perform an attack for the test part of the dataset to generate perturbed data.\n2. Estimate Successfulness, Efficiency, and Concealability metrics for perturbed data.\nFor the estimation, we use the target and discriminator models.\n4.2 Experiments setup and data\nDatasets\nTo compare vanilla and concealed adversarial attacks, we attacked models trained\non six datasets from the UCR package [11]: PowerCons, Strawberry, GunPoint, Gun-\nPointMalevsFemale, Coffee, and FreezerRegularTrain. These datasets are diverse in\ndata domains and number of data samples. All datasets contain univariate time-series\ndata labelled for the classification task. Some datasets are imbalanced, so this is the\nreason why we take F1 as a basic metric for Efficiency and Concealability calculation.\nThe data for all datasets was divided into training and testing sets as per the original\nsplits. All datasets do not have any time feature and were split by data objects.\nTarget model and discriminator training\nAll models, ResCNN, RNNAttantion, S4, and PatchTST, were trained on all six\ndatasets. We use different model hyperparameters for training models, and F1 mea-\nsures greater than 0.75 for all model-dataset combinations. For almost all models, we\nachieve target metrics higher than 0.9.\nFor discriminator training, we use the same type as the target model: if the target\nmodel is ResCNN, the discriminator model is also ResCNN. We trained discriminator\nmodels on adversarial training data that was weakly perturbed compared to the Suc-\ncessfullness. Mostly, we decrease the number of attack iterations but not the step of\n10\n\n\nattack size or radius for the PGD attack. We varied different model hyperparameters\nto achieve better results for discriminator training. In particular, we tuned dropout\nand learning rates to avoid overfitting. Besides, for the PGD attack, which was hardly\ndetectable for the discriminator, we used smaller models with fewer hyperparameters\nthan in the target model. As was mentioned in 3, we use a special procedure of training\nthe discriminator, decreasing the strength of attack five to eight times at 20% every\ntime to make the discriminator model more sensitive to small data perturbations. All\ndiscriminator models were trained with an accuracy score greater than 0.85 on small\nperturbations. The accuracy metric is correct in this case because we have an equal\nnumber of perturbed and original data samples. For all attacks, iFGSM, SimBA, PGD,\nand SGM, we train separate discriminator models using vanilla or non-regularized\nattack versions.\nExperiments setup\nAll discriminators were trained on the vanilla attack without regularization. We set\nthe number of steps to 10 for the iFGSM and PGD attack and 500 steps for the SimBA\nAttack. We begin training the discriminator with the next parameters of the attacks:\nthe ϵ was set to 0.03 for the iFGSM attack, e was set to 300 for SimBA, and η was\nset to 0.3for the PDG attack. We decrease these parameters 5-8 times on 20% every\ntime to provide iterative discriminator training.\nFor various types of attacks, hyperparameters were optimized through grid search.\nSpecifically, the iFGSM attack with regularization and sum aggregation was applied\nusing different ϵ values: 0.005, 0.01, 0.03, and 0.05, along with regularization α val-\nues of 0.001, 0.01, 0.1, 1, 10, and 100. For hypercone aggregation, the ∆parameter was\nvaried across the values −0.5, −0.3, −0.1, 0, 0.1, 0.3, 0.5, and 1.0. The Projected Gradi-\nent Descent (PGD) attack was applied with different η: 0.05, 0.1, 0.25, 0.5, and 1. The\nPGD attack with regularization employed the same α values as the iFGSM attack\nwith regularization.\nWe also add an SGM attack using KL divergence and L2 distance from original\ndata for regularization [38]. This method is SOTA concealed adversarial attack for time\nseries data. To train the discriminator, a modified version of the attack was used, with\nthe regularization term coefficients for attack magnitude (L2 distance) and smoothness\nset to zero while maintaining the noise clipping coefficient, representing the maximum\nattack magnitude, ϵ = 0.1, as in the original paper. The attack was then performed\nwith various ϵ values: 0.05, 0.1, 0.25, 0.5, and 1.0. Additionally, the smoothness and L2\ncoefficients were assigned equal values, including 0.0, 0.1, 0.5, and 1.0.\nIn estimating attack results, we use the following procedure of results validation:\nwe take the best iteration according to the Successfulness among all attack hyperpa-\nrameter combinations. For fair metrics estimation, we calculate take metrics after at\nleast 40 attack iterations for the iFGSM and PGD attacks, 1300 iterations for the\nSimBA attacks, and 400 iterations for the SGM attack or with an Efficiency of more\nthan 0.9 for all attacks. Otherwise, an attack at the first iteration can be taken because\nof the discriminator’s inability to detect small perturbations.\n11\n\n\n4.3 Main results\nAblation Study\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVanilla iFGSM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\niFGSM with Sum Regularization (ours)\n(a) Vanilla vs regularized iFGSM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSGM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\niFGSM with Sum Regularization (ours)\nDataset\nCoffee\nFreezerRegularTrain\nGunPoint\nGunPointMaleVersusFemale\nPowerCons\nStrawberry\nModel\nPatchTST\nRNNAttention\nResCNN\nS4\n(b) SGM vs regularized iFGSM\nFig. 3: The successfulness of iFGSM with sum regularization versus two baseline\napproaches for different pairs of models and datasets: for almost all experiments, our\napproach outperforms vanilla attack (a) and SGM baseline (b).\nThe aggregated results for all our datasets and models with iFGSM attack are\npresented in table 1. We computed the mean metrics and mean ranks from them for the\nbest iteration of each attack, considering various hyperparameters across all described\ndatasets and models. The best iteration was selected based on Successfulness metrics,\nwhile Efficiency and Concealability for this iteration were also reported, following the\nselection rule outlined in the previous subsection. Additionally, we included the SGM\nattack for comparison.\nTable 1: Different iFGSM attack regularization mean rank for Efficiency, Conceal-\nability, and Successfullness. Experiments were run on all presented datasets, and\nResCNN, RNNAttention, S4, PatchTST models\nRegularization method\nMean metric (↑)\nMean rank (↓)\nSuccess.\nEfficiency\nConceal.\nSuccess.\nEfficiency\nConceal.\nVanilla attack\n0.300\n0.861\n0.239\n3.750\n2.042\n3.792\nSGM\n0.570\n0.618\n0.641\n2.750\n3.562\n2.479\nHarmonic reg. (ours)\n0.789\n0.819\n0.800\n1.875\n1.958\n2.021\nSum reg. (ours)\n0.786\n0.744\n0.878\n1.625\n2.438\n1.708\nAs we can see, a sum regularization performs the best according to the Conceala-\nbility mean metric, Successfulness and Concealability mean ranks. The Concealability\nof the sum regularization grows up to 0.878 compared with 0.239 for unregularised\nattacks, while Efficiency drops only to 0.744 from 0.861. Harmonic regularization also\nworks well and has better Efficiency than sum regularization but worse Concealability.\n12\n\n\nThe Harmonic regularization method is the best according to the mean Successfullness\nbut the second best according to the mean rank. Both harmonic and sum regulariza-\ntion outperform an SGM adversarial attack for all three metrics. More detailed results\nare presented in Appendix C.\nWe also compare our iFGSM regularised attack side by side with vanilla iFGSM\nattack and SGM baseline. We visualize Successfulness of iFGSM regularization on the\ny-axis and vanilla iFGSM (a) and SGM attacks (b) on the x-axis in Figure 3. As\nwe can see, all points are located higher than the diagonal line. Our sum regularised\nattack often performs better than vanilla iFGSM in most experiments. For a small\nnumber of cases, regularized iFGSM is worse than SGM, but for most experiments,\nour method outperforms SGM.\nSensitivity Study\nOur next step was to compare attack regularization across various datasets. We con-\nducted experiments using all three types of regularization for the iFGSM attack across\nfour models on the PowerCons dataset. As shown in figure 4, both the transformer\nmodel PatchTST and the state-space model S4 were successfully attacked using har-\nmonic and sum regularization but remained resilient to hypercones regularization.\nHowever, in recurrent architecture RNNAttention, hypercones regularization outper-\nformed both harmonic and sum regularization. Additionally, harmonic regularization\nproved to be more effective than sum regularization for the convolutional model Ress-\nCNN. Notably, hypercones regularization performed poorly across all models except\nfor the RNNAttention model. Furthermore, in all our experiments, both harmonic\nand sum regularization consistently outperformed the SGM attack. Based on these\nfindings, we will focus solely on harmonic and sum regularization in subsequent exper-\niments. These results also highlight that different regularization techniques are more\neffective for specific model architectures.\nPatchTST\nRNNAttention\nResCNN\nS4\nModel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccessfullness\nVanilla attack\nSGM\nHyperCones Regularization (ours)\nHarmonic Regularization (ours)\nSum Regularization (ours)\nFig. 4: Successfullness for different models (the greater, the better). The performance\nof different regularization types varies depending on the model’s architecture.\n13\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVanilla PGD\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPGD with Sum Regularization (ours)\nDataset\nCoffee\nFreezerRegularTrain\nGunPoint\nGunPointMaleVSFemale\nPowerCons\nStrawberry\nModel\nPatchTST\nRNNAttention\nResCNN\nS4\n(a) PGD attack\nResCNN\nPatchTST\nRNNAttention\nModel\n0.0\n0.1\n0.2\n0.3\n0.4\nSuccessfullness\nSuccessfullness\nResCNN\nPatchTST\nRNNAttention\nModel\n0.0\n0.1\n0.2\n0.3\n0.4\nConcealability\nConcealability\nVanilla SimBA\nSimBA with Sum Regularization (ours)\n(b) SimBA attack\nFig. 5: The iFGSM with sum regularization for almost all experiments outperforms\nvanilla attack version (a), and SGM baseline (b).\nOur method performs well against other gradient-based attacks. As provided in\nFigure 5a, the PGD attack with Sum regularization outperforms vanilla PGD, similar\nto our findings with FGM. However, iFGSM with Sum or Harmonic regularization\nyields even better results compared to PGD, as shown in Table C1.\nOn the other hand, our adversarial attack regularization method performs poorly\nwith the SimBA attack, as shown in figure 5b. The non-gradient nature of SimBA\nmakes training the discriminator for effective attack detection and regularization sig-\nnificantly more difficult. It appears that because SimBA perturbs the input in a highly\nrandom manner—unlike gradient-based methods—it prevents the discriminator from\nidentifying distinguishable patterns. Additionally, for some models, SimBA requires an\nexcessive number of iterations to achieve an effective attack, which further undermines\nthe results of the experiments.\nLast but not least, we visualize data attacked with vanilla iFGSM adversarial and\nregularized attacks. The plots are in Figure 1b from the Appendix , as well as technical\ndetails on experiments. Visually, the regularized attack results in fewer artefacts, with\nthe object closely resembling the original.\n5 Conclusion\nIn this study, we address the challenge of concealed adversarial attacks on sequential\ndata models. Our novel approach utilizes a discriminator model trained on a binary\nclassification task to determine whether input data has been adversarially perturbed.\nThis discriminator serves a dual function: it operates as both an adversarial perturba-\ntion detector and a regularization mechanism for the attack. The principal contribution\nof this work is the development of advanced adversarial attack regularization tech-\nniques that enhance the concealment of attacks while preserving their Efficiency.\nBy incorporating discriminator loss into conventional adversarial mechanisms, our\napproach achieves a crucial balance between attack efficiency and detectability, thereby\naddressing a significant gap in the existing research. Furthermore, we illustrate the\n14\n\n\ninherent visibility of adversarial attacks in time series models and introduce an effec-\ntive methodology for training discriminator models. Our analysis reveals that standard\nadversarial attacks, such as iFGSM and PGD, introduce noticeable perturbation arte-\nfacts that can be easily detected. In contrast, our regularization techniques significantly\nsuppress these artefacts, making attacks harder to distinguish from original data.\nThe proposed experimental framework integrates the training of target models, the\ngeneration of adversarial datasets, and the use of discriminator-based regularization.\nIt provides a comprehensive approach to assess both the Efficiency and Concealabil-\nity of adversarial attacks. As part of this framework, we explore various strategies\nfor discriminator regularization and conduct an in-depth comparison of their perfor-\nmance across different models and datasets. Specifically, we assess three regularization\ntechniques: Sum Regularization, Harmonic Regularization, and Hypercones Regular-\nization — using four deep learning architectures: ResCNN, RNNAttention, S4, and\nPatchTST.\nOur experimental results demonstrate that Sum and Harmonic Regularization\nconsistently yield the most favourable outcomes in terms of both attack Efficiency\nand Concealability. Notably, both techniques outperform the SGM adversarial attack,\nwhich is also designed to be less detectable across all three key evaluation metrics —\nSuccess, Efficiency, and Concealability - further validating their Efficiency. However,\nwhile our approach performs strongly against multiple gradient-based attacks, it does\nnot generalize well to blackbox methods, such as SimBA.\nLooking ahead, the insights derived from this study can inform the development\nof more resilient time series classification models and foster further exploration into\nadvanced adversarial defense mechanisms. Future research should also focus on refining\nadaptive regularization strategies that dynamically adjust based on the characteristics\nof the target model and dataset. Also, extending our approach to non-gradient-based\nattacks remains a critical challenge. Finally, investigating methods that integrate gra-\ndients from multiple models — whether sharing the same architecture or not —\ncould significantly enhance concealability while maintaining attack Efficiency due to\nimproved generalization capabilities.\nReferences\n[1] Marusov, A.E., Zaytsev, A.: Noncontrastive representation learning for intervals\nfrom well logs. IEEE Geoscience and Remote Sensing Letters 20, 1–5 (2023)\n[2] Zhang, C., Sjarif, N.N.A., Ibrahim, R.: Deep learning models for price forecast-\ning of financial time series: A review of recent advancements: 2020–2022. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery 14(1), 1519\n(2024)\n[3] Yue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., Xu, B.: TS2Vec:\nTowards universal representation of time series. In: Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 36, pp. 8980–8987 (2022)\n[4] Dempster, A., Petitjean, F., Webb, G.I.: Rocket: exceptionally fast and accurate\n15\n\n\ntime series classification using random convolutional kernels. Data Mining and\nKnowledge Discovery 34(5), 1454–1495 (2020)\n[5] Middlehurst, M., Sch¨afer, P., Bagnall, A.: Bake off redux: a review and experi-\nmental evaluation of recent time series classification algorithms. Data Mining and\nKnowledge Discovery, 1–74 (2024)\n[6] Ismail Fawaz, H., Forestier, G., Weber, J., Idoumghar, L., Muller, P.-A.: Deep\nlearning for time series classification: a review. Data mining and knowledge\ndiscovery 33(4), 917–963 (2019)\n[7] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.J.,\nFergus, R.: Intriguing properties of neural networks. In: Bengio, Y., LeCun,\nY. (eds.) International Conference on Learning Representations, ICLR 2014,\nConference Track Proceedings (2014). http://arxiv.org/abs/1312.6199\n[8] Akhtar, N., Mian, A.: Threat of adversarial attacks on deep learning in computer\nvision: A survey. IEEE Access 6, 14410–14430 (2018)\n[9] Zhang, W.E., Sheng, Q.Z., Alhazmi, A., Li, C.: Adversarial attacks on deep-\nlearning models in natural language processing: A survey. ACM Transactions on\nIntelligent Systems and Technology (TIST) 11(3), 1–41 (2020)\n[10] Goyal, S., Doddapaneni, S., Khapra, M.M., Ravindran, B.: A survey of adversarial\ndefenses and robustness in NLP. ACM Computing Surveys 55(14s), 1–39 (2023)\n[11] Dau, H.A., Keogh, E., Kamgar, K., Yeh, C.-C.M., Zhu, Y., Gharghabi, S.,\nRatanamahatana, C.A., Yanping, Hu, B., Begum, N., Bagnall, A., Mueen, A.,\nBatista, G., Hexagon-ML: The UCR Time Series Classification Archive (2018)\n[12] Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial\nexamples. In: Bengio, Y., LeCun, Y. (eds.) International Conference on Learning\nRepresentations, ICLR 2015, Conference Track Proceedings (2015). http://arxiv.\norg/abs/1412.6572\n[13] Yang,\nQ.,\nWu,\nX.:\n10\nchallenging\nproblems\nin\ndata\nmining\nresearch.\nInternational\nJournal\nof\nInformation\nTechnology\n&\nDecision\nMak-\ning\n05(04),\n597–604\n(2006)\nhttps://doi.org/10.1142/S0219622006002258\nhttps://doi.org/10.1142/S0219622006002258\n[14] Keogh, E., Ratanamahatana, C.A.: Exact indexing of dynamic time warping.\nKnowledge and information systems 7, 358–386 (2005)\n[15] Bagnall, A., Lines, J., Hills, J., Bostrom, A.: Time-series classification with\ncote: the collective of transformation-based ensembles. IEEE Transactions on\nKnowledge and Data Engineering 27(9), 2522–2535 (2015)\n16\n\n\n[16] Sch¨afer, P., Leser, U.: Fast and accurate time series classification with weasel.\nIn: Proceedings of the 2017 ACM on Conference on Information and Knowledge\nManagement, pp. 637–646 (2017)\n[17] Middlehurst, M., Large, J., Flynn, M., Lines, J., Bostrom, A., Bagnall, A.: Hive-\ncote 2.0: a new meta ensemble for time series classification. Machine Learning\n110(11), 3211–3243 (2021)\n[18] Mohammadi Foumani, N., Miller, L., Tan, C.W., Webb, G.I., Forestier, G., Salehi,\nM.: Deep learning for time series classification and extrinsic regression: A current\nsurvey. ACM Computing Surveys 56(9), 1–45 (2024)\n[19] Zou, X., Wang, Z., Li, Q., Sheng, W.: Integration of residual network and con-\nvolutional neural network along with various activation functions and global\npooling for time series classification. Neurocomputing 367, 39–45 (2019) https:\n//doi.org/10.1016/j.neucom.2019.08.023\n[20] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recogni-\ntion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 770–778 (2016)\n[21] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),\n1735–1780 (1997) https://doi.org/10.1162/neco.1997.9.8.1735\n[22] Oguiza, I.: tsai - A state-of-the-art deep learning library for time series and\nsequential data. Github (2023). https://github.com/timeseriesAI/tsai\n[23] Nie, Y., H. Nguyen, N., Sinthong, P., Kalagnanam, J.: A time series is worth 64\nwords: Long-term forecasting with transformers. In: International Conference on\nLearning Representations, ICLR 2023 (2023)\n[24] Gu, A., Goel, K., R´e, C.: Efficiently modeling long sequences with structured\nstate spaces. In: The International Conference on Learning Representations ICLR\n2022 (2022)\n[25] Bagnall, A., Dau, H.A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P.,\nKeogh, E.: The UEA multivariate time series classification archive, 2018. arXiv\npreprint arXiv:1811.00075 (2018)\n[26] Wang, Y., Sun, T., Li, S., Yuan, X., Ni, W., Hossain, E., Poor, H.V.: Adversarial\nattacks and defenses in machine learning-empowered communication systems and\nnetworks: A contemporary survey. IEEE Communications Surveys & Tutorials\n(2023)\n[27] Zaytsev, A., Kovaleva, M., Natekin, A., Vorsin, E., Smirnov, V., Smirnov,\nG., Sidorshin, O., Senin, A., Dudin, A., Berestnev, D.: Designing an attack-\ndefense game: how to increase robustness of financial transaction models via a\n17\n\n\ncompetition. arXiv preprint arXiv:2308.11406 (2023)\n[28] Tram`er, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., McDaniel, P.:\nEnsemble adversarial training: Attacks and defenses. In: International Conference\non Learning Representations, ICLR 2018 (2018)\n[29] Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.-A.: Adver-\nsarial attacks on deep neural networks for time series classification. In: 2019\nInternational Joint Conference on Neural Networks (IJCNN), pp. 1–8 (2019).\nIEEE\n[30] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep\nlearning models resistant to adversarial attacks. In: International Conference on\nLearning Representations, ICLR 2018, Conference Track Proceedings (2018)\n[31] Guo, C., Gardner, J., You, Y., Wilson, A.G., Weinberger, K.: Simple black-box\nadversarial attacks. In: International Conference on Machine Learning, pp. 2484–\n2493 (2019). PMLR\n[32] Ding, D., Zhang, M., Feng, F., Huang, Y., Jiang, E., Yang, M.: Black-box adver-\nsarial attack on time series classification. Proceedings of the AAAI Conference\non Artificial Intelligence 37(6), 7358–7368 (2023) https://doi.org/10.1609/aaai.\nv37i6.25896\n[33] Karim, F., Majumdar, S., Darabi, H.: Adversarial attacks on time series. IEEE\nTransactions on pattern analysis and machine intelligence 43(10), 3309–3320\n(2020)\n[34] Harford, S., Karim, F., Darabi, H.: Adversarial attacks on multivariate time series.\narXiv preprint arXiv:2004.00410 (2020)\n[35] Abdu-Aguye, M.G., Gomaa, W., Makihara, Y., Yagi, Y.: Detecting adversarial\nattacks in time-series data. In: ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pp. 3092–3096 (2020).\nIEEE\n[36] Pialla, G., Fawaz, H.I., Devanne, M., Weber, J., Idoumghar, L., Muller, P.-A.,\nBergmeir, C., Schmidt, D., Webb, G., Forestier, G.: Smooth perturbations for time\nseries adversarial attacks. In: Pacific-Asia Conference on Knowledge Discovery\nand Data Mining, pp. 485–496 (2022). Springer\n[37] Wang, Y., Du, D., Liu, Y.: TSFool: Crafting Highly-imperceptible Adversarial\nTime Series through Multi-objective Black-box Attack to Fool RNN Classifiers.\narXiv preprint arXiv:2209.06388 (2022)\n[38] Pialla, G., Ismail Fawaz, H., Devanne, M., Weber, J., Idoumghar, L., Muller,\nP., Bergmeir, C., Schmidt, D., Webb, G., Forestier, G.: Time series adversarial\n18\n\n\nattacks: an investigation of smooth perturbations and defense approaches. Inter-\nnational Journal of Data Science and Analytics (2023) https://doi.org/10.1007/\ns41060-023-00438-0\n[39] Tercan, A., Prabhu, V.S.: Thresholded lexicographic ordered multiobjective\nreinforcement learning. arXiv preprint arXiv:2408.13493 (2024)\n19\n\n\nAppendix A\nAdversarial attacks algorithms\nThis section contains a detailed description of adversarial attack algorithms used in\nour research. These attacks are described below.\n• iFGSM attack. Several papers demonstrate the Efficiency of this attack in the CV\ndomain. Besides, the iFGSM attack was also applied to sequential data successfully.\nThe basic attack approach for this method is the fast gradient sign method (FGSM).\nThe idea is to make perturbations with the sign of the model loss L gradient. The\nsign of gradient for every data object x is multiplied by the strength of an attack\nhyperparameter ϵ to get perturbed data object x′.\nx′ = x + ϵ sign (∇xL(f(x), y)) ,\nwhere ∇xL(f(x), y) — the gradient of the model.\nHowever, the main disadvantage of a vanilla FGSM attack is that it transforms\ndata only once. It cannot be enough to change the predictions of the model. So,\nthe iterative modification of this approach was proposed [29]. The key idea of an\niterative adversarial attack is to apply the FGSM perturbation n times, changing\nthe result of the previous iteration in every step t:\nxt+1 = xt + ϵ sign\n\u0000∇xtL\n\u0000f(xt), y\n\u0001\u0001\n.\nThis method can help provide a more precise attack on the model by changing the\ndirection of the attack for different data periods that shouldn’t be transformed more\nfor model compromising.\n• PGD attack. The Projected Gradient Descent (PGD) [30] attack is a white-box\nuniversal “first-order adversary” method of attack. Unlike the iFGSM attack, the\nauthor proposes the idea of the limitation difference between adversarial data and\noriginal data objects. Perturbations generated in the linf-ball S of radius η T times\nwith step size λ. In other words, if our perturbation reaches the border of the\nlinf-ball, we clip it. It helps to increase the robustness of the attack.\nxt+1 = Πx+S\n\u0000xt + λ sign\n\u0000∇xtL\n\u0000f(xt), y\n\u0001\u0001\u0001\n,\nwhere ΠA is the projection operator for a set A. As in the original paper, we fix the\nstep size λ = 2.5η\nT\nto decrease the number of hyperparameters.\n• SimBA attack. The SimBA is a simple but effective black box attack algorithm\nthat modifies different parts of original objects. In the case of computer vision, it\niteratively modifies different randomly chosen picture pixels. The algorithm was\nused for Cartesian coordinates. For computer vision objects, it means they take a\nrandomly chosen colour of one pixel to transform it. Our variant of SimBA has two\nhyperparameters: the number of maximum iterations Tmax and the step size ϵ. The\nalgorithm of the attack, as well as its hyperparameters, are presented in Algorithm 2.\nFor a time series object, we will choose a one-time point and one feature in this\ntime point if the data object is multivariate.\n20\n\n\nAlgorithm 2 SimBA attack\nprocedure SimBA(x, y, Q, ϵ, Tmax,)\nδ = 0\np = pf(y | x)\nt = 0\nwhile py = maxy′ py′ or t < Tmax do\nPick randomly without replacement: q ∈Q\nfor α ∈{ϵ, −ϵ} do\np′ = pf(y | x + δ + αq)\nif p′\ny < py then\nδ = δ + αq\np′ = p\nbreak\nend if\nt = t + 1\nend for\nend while\nreturn δ\nend procedure\nAppendix B\nConsidered aggregation functions\nWe consider three reasonable aggregation functions: sum, harmonic, and hypercone.\nThe details for them are provided below. For all methods, hyperparameter tuning was\nprovided by a grid search of possible values for every hyperparameter with fixed attack\nnumber steps.\n1. Sum aggregation. This aggregation method was inspired by classical machine\nlearning Ridge and LASSO regression loss when we take one loss as it is and add\na second part with an extra α hyperparameter. An example of this loss for the\niFGSM attack:\ng(x) = a(x) + αd(x) = Ltarget (f(x), y) −α log D(x)\n2. Harmonic aggregation. Harmonic aggregation doesn’t take some extra hyper-\nparameters for tuning. The idea is to take the harmonic mean between the loss of\nthe discriminator and the target model.\ng(x) = meanharmonic(a(x), d(x)) = 2 a(x) · d(x)\na(x) + d(x) =\n= 2 Ltarget (f(x), y) · (−log D(xt))\nLtarget (f(x), y) −log D(x) + γ ,\nwhere γ is a small value to avoid the zero division problems.\n21\n\n\n3. Hypercone aggregation. The idea behind this aggregation function is to project\nthe discriminator loss gradient to the hypercone around the target attack gradient\n[39]. The width of the hypercone depends on hyperparameter ∆, which is the angel\nhypercone and axes. Increasing ∆brings a greater role of regularization in the\nfinal vector. The main difference between this and other methods is that hypercone\naggregation mixes final gradient vectors but not the loss values. So, the limitation\nof this method is the inability to apply hypercone regularization to the black box\nattacks.\n∇xg(x) = cos ∆\nsin ϕ sin(∆+ ϕ)\n\u0010\n∇xLtarget+\n+ ∇xLdisc\n∥∇xLtarget∥\n∥∇xLdisc∥(sin ϕ tan ∆−cos ϕ)\n\u0011\n,\nwhere ϕ is the angle between the gradient of the target model loss ∇xLtarget and\nthe gradient of discriminator model loss ∇xLdisc.\nAppendix C\nComprehensive experiments results\nTable C1 presents the results of all main experiments. For most experiments, our\nregularised attacks outperform vanilla attacks and SGM attacks.\nAppendix D\nExamples of attacked data samples\nFigure D1 presents examples of both the vanilla iFGSM attack and the iFGSM attack\nwith sum regularization. These examples illustrate scenarios in which both attack\nvariants successfully altered the target model’s predictions. However, the regularized\nattack also remained undetected, as demonstrated by misclassification from the dis-\ncriminator model. Both attack variants were applied with equal strength, while the\nregularization parameter α was tuned to ensure concealment.\nFor better visual clarity, the time-series data has been truncated to display only\nthe first 50 or 100 points, depending on the dataset.\nWhile both attacks resulted in a change to the model’s class predictions, the dis-\ncriminator scores for the vanilla attack were close to 1.0, indicating detection. In\ncontrast, the regularized attack consistently produced discriminator scores below 0.2,\nindicating that the discriminator failed to detect any perturbations. The regularized\nattack produces fewer artifacts and generates samples that more closely resemble the\noriginal data, as evidenced by its ability to deceive the discriminator model.\n22\n\n\nTable C1: The Successfulness metric (↑) of all experiments. The best attack for every\ndataset-model experiment is highlighted bold text style. Values marked with ∗are calcu-\nlated with less strict rules for discriminator training. The reason is that it is impossible to\ntrain a discriminator with a quality above the specified threshold.\nDataset\nAttack\nModel\nPatchTST\nRNNA\nResCNN\nS4\nCoffee\nVanilla iFGSM\n0.464\n0.662\n1.0\n0.893\nVanilla PGD\n0.133\n0.339\n0.036\n0.018∗\nSGM\n0.422\n1.0\n0.964\n0.895\nPGD Sum Reg (our)\n0.557\n1.0\n1.0\n0.861∗\niFGSM Harmonic Reg (our)\n1.0\n1.0\n1.0\n1.0\niFGSM Sum Reg (our)\n1.0\n1.0\n1.0\n1.0\nFreezerRegularTrain\nVanilla iFGSM\n0.051\n0.076\n0.996\n0.09\nVanilla PGD\n0.537\n0.464\n0.648\n0.742\nSGM\n0.457\n0.243\n0.992\n0.46\nPGD Sum Reg (our)\n1.0\n0.537\n0.979\n0.935\niFGSM Harmonic Reg (our)\n0.995\n0.469\n0.994\n0.85\niFGSM Sum Reg (our)\n1.0\n0.944\n0.997\n0.951\nGunPoint\nVanilla iFGSM\n0.02\n0.013\n0.275\n0.294\nVanilla PGD\n0.361∗\n0.179\n0.304\n0.517∗\nSGM\n0.503\n0.927∗\n0.538\n0.475\nPGD Sum Reg (our)\n0.814∗\n0.469\n0.575\n0.704∗\niFGSM Harmonic Reg (our)\n0.767\n0.46\n0.859\n0.864\niFGSM Sum Reg (our)\n0.802\n0.513\n0.736\n0.886\nGunPointMaleVSFemale\nVanilla iFGSM\n0.034\n0.294\n0.335\n0.156\nVanilla PGD\n0.132\n0.382\n0.162\n0.323∗\nSGM\n0.334\n0.606∗\n0.928∗\n0.600∗\nPGD Sum Reg (our)\n0.541\n0.705\n0.579\n0.441∗\niFGSM Harmonic Reg (our)\n0.501\n0.523\n0.778\n0.36\niFGSM Sum Reg (our)\n0.673\n0.415\n1.0\n0.581\nPowerCons\nVanilla iFGSM\n0.187\n0.376\n0.328\n0.404\nVanilla PGD\n0.187\n0.265\n0.575\n0.277\nSGM\n0.199\n0.118\n0.401\n0.412∗\nPGD Sum Reg (our)\n0.543\n0.522\n0.66\n0.281\niFGSM Harmonic Reg (our)\n0.869\n0.46\n0.931\n0.689\niFGSM Sum Reg (our)\n0.97\n0.379\n0.636\n0.815\nStrawberry\nVanilla iFGSM\n0.053\n0.0\n0.078\n0.12\nVanilla PGD\n0.027\n0.003\n0.037\n0.172\nSGM\n0.654\n0.576\n0.544\n0.427\nPGD Sum Reg (our)\n0.665\n0.577\n0.651\n0.522\niFGSM Harmonic Reg (our)\n0.921\n0.943\n0.965\n0.745\niFGSM Sum Reg (our)\n0.769\n0.537\n0.45\n0.799\n23\n\n\n0\n10\n20\n30\n40\n50\n1\n0\n1\n2\n3\nOriginal data\nVanilla attack\nRegularized attack\n(a) Strawberry, ResCNN\n0\n10\n20\n30\n40\n50\n1\n0\n1\n2\n3\nOriginal data\nVanilla attack\nRegularized attack\n(b) Strawberry, ResCNN\n0\n20\n40\n60\n80\n100\n10\n5\n0\n5\n10\n15\nOriginal data\nVanilla attack\nRegularized attack\n(c) FreezerRegularTrain, RNNAttention\n0\n20\n40\n60\n80\n100\n5\n0\n5\n10\n15\nOriginal data\nVanilla attack\nRegularized attack\n(d) FreezerRegularTrain, RNNAttention\n0\n10\n20\n30\n40\n50\n1\n0\n1\n2\n3\nOriginal data\nVanilla attack\nRegularized attack\n(e) Strawberry, PatchTST\n0\n10\n20\n30\n40\n50\n1\n0\n1\n2\n3\nOriginal data\nVanilla attack\nRegularized attack\n(f) Strawberry, PatchTST\n0\n20\n40\n60\n80\n100\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nOriginal data\nVanilla attack\nRegularized attack\n(g) FreezerRegularTrain, ResCNN\n0\n20\n40\n60\n80\n100\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nOriginal data\nVanilla attack\nRegularized attack\n(h) FreezerRegularTrain, ResCNN\nFig. D1: Examples of iFGSM attacks on various models and datasets, both with and\nwithout regularization.\n24\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20948v1.pdf",
    "total_pages": 24,
    "title": "Concealed Adversarial attacks on neural networks for sequential data",
    "authors": [
      "Petr Sokerin",
      "Dmitry Anikin",
      "Sofia Krehova",
      "Alexey Zaytsev"
    ],
    "abstract": "The emergence of deep learning led to the broad usage of neural networks in\nthe time series domain for various applications, including finance and\nmedicine. While powerful, these models are prone to adversarial attacks: a\nbenign targeted perturbation of input data leads to significant changes in a\nclassifier's output. However, formally small attacks in the time series domain\nbecome easily detected by the human eye or a simple detector model.\n  We develop a concealed adversarial attack for different time-series models:\nit provides more realistic perturbations, being hard to detect by a human or\nmodel discriminator. To achieve this goal, the proposed adversarial attack\nmaximizes an aggregation of a classifier and a trained discriminator loss. To\nmake the attack stronger, we also propose a training procedure for a\ndiscriminator that provides broader coverage of possible attacks. Extensive\nbenchmarking on six UCR time series datasets across four diverse architectures\n- including recurrent, convolutional, state-space, and transformer-based models\n- demonstrates the superiority of our attack for a concealability-efficiency\ntrade-off. Our findings highlight the growing challenge of designing robust\ntime series models, emphasizing the need for improved defenses against\nrealistic and effective attacks.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}