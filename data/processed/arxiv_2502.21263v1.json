{
  "id": "arxiv_2502.21263v1",
  "text": "RuCCoD: Towards Automated ICD Coding in Russian\nAleksandr Nesterov1, Andrey Sakhovskiy2, Ivan Sviridov3, Airat Valiev4\nVladimir Makharev1, Petr Anokhin1, Galina Zubkova3, Elena Tutubalina1,2,5\n1 AIRI, Moscow, Russia\n2 Sber AI, Moscow, Russia\n3 Sber AI Lab, Moscow, Russia\n4 HSE University, Moscow, Russia\n5 ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia\nAbstract\nThis study investigates the feasibility of au-\ntomating clinical coding in Russian, a language\nwith limited biomedical resources. We present\na new dataset for ICD coding, which includes\ndiagnosis fields from electronic health records\n(EHRs) annotated with over 10,000 entities\nand more than 1,500 unique ICD codes. This\ndataset serves as a benchmark for several state-\nof-the-art models, including BERT, LLaMA\nwith LoRA, and RAG, with additional exper-\niments examining transfer learning across do-\nmains (from PubMed abstracts to medical di-\nagnosis) and terminologies (from UMLS con-\ncepts to ICD codes). We then apply the best-\nperforming model to label an in-house EHR\ndataset containing patient histories from 2017\nto 2021. Our experiments, conducted on a care-\nfully curated test set, demonstrate that training\nwith the automated predicted codes leads to a\nsignificant improvement in accuracy compared\nto manually annotated data from physicians.\nWe believe our findings offer valuable insights\ninto the potential for automating clinical cod-\ning in resource-limited languages like Russian,\nwhich could enhance clinical efficiency and\ndata accuracy in these contexts.\n1\nIntroduction\nThe explosion of medical data driven by technology\nand digitalization presents a unique opportunity\nto enhance healthcare quality. With the adoption\nand implementation of electronic health records\n(EHRs), accurate and timely data utilization is cru-\ncial for effective treatment and disease manage-\nment. Central to this process is the assignment\nof International Classification of Diseases (ICD)\ncodes, which is essential for medical documen-\ntation, billing (Sonabend et al., 2020), insurance\n(Park et al., 2000), and research (Bai et al., 2018;\nLu et al., 2022; Shang et al., 2019).\nWhile ICD codes assignment is crucial for EHRs,\nit poses significant challenges. Human coders must\nFigure 1: Examples of ICD code assignments by anno-\ntators: each entity in green is annotated with its ICD\ncode above and its English translation (in yellow).\nnavigate a wide array of medical terminology, sub-\njective interpretations, and time pressures, all while\nstaying updated with constantly changing classifica-\ntion standards (Burns et al., 2012; O’Malley et al.,\n2005; Cheng et al., 2009). Coding errors can lead\nto misdiagnosis, ineffective treatment, diminished\ntrust in the healthcare system, and negative public\nhealth outcomes. Furthermore, errors in manual\ncoding, according to the ICD system, result in fi-\nnancial repercussions, accounting for 6.8% of the\ntotal payments (Manchikanti, 2002).\nDespite extensive research on ICD coding us-\ning neural networks (Li and Yu, 2020; Zhou et al.,\n2021; Yuan et al., 2022a; Baksi et al., 2024; Boyle\net al., 2023; Mullenbach et al., 2018a; Cao et al.,\n2020; Yuan et al., 2022a; Yang et al., 2022), signif-\nicant challenges persist for non-English languages.\nThese include low inter-coder agreement, limited\nlabeled data, variability in clinical notes, the hier-\narchy of ICD codes, and reliance on incomplete\ninput data. To address these issues, we introduce a\nnovel dataset for automatic ICD coding in Russian.\nPrevious studies have primarily focused on\nEnglish-language datasets, specifically MIMIC-\nIII/IV (Goldberger et al., 2000; Johnson et al.,\n2023). Russian is considered underdeveloped in the\nclinical domain, despite being one of the top ten lan-\nguages. The Russian segment of the Unified Medi-\ncal Language System (UMLS) (Bodenreider, 2004)\ncomprises only 1.96% of the vocabulary and 1.62%\nof the source counts found in the English UMLS\n1\narXiv:2502.21263v1  [cs.CL]  28 Feb 2025\n\n\n(NIH). Recent corpora, such as RuCCoN (Nesterov\net al., 2022a) and NEREL-BIO (Loukachevitch\net al., 2023), focus on concepts within the Russian\nUMLS. Our study shows that UMLS may not fully\nmeet the structured requirements of ICD coding,\nparticularly in semantic representation, limiting the\ntransferability of models dependent on UMLS for\nclinical tasks (Sec. 4).\nIn this paper, we present RuCCoD (Russian\nICD Coding Dataset), a new dataset for ICD cod-\ning in Russian1, labeled by medical professionals\nbased on concepts from the ICD-10 CM (Clini-\ncal Modification) system (Sec. 3.1). Second, we\nestablish a comprehensive benchmark for state-\nof-the-art models, including a BERT-based (Devlin\net al., 2019) pipeline for information extraction, a\nLLaMa-based (Touvron et al., 2023) model with\nParameter Efficient Fine-Tuning (PEFT) and with\nretrieval-augmented generation (RAG). Addition-\nally, we evaluate the transfer of model performance\nfrom UMLS codes and similar datasets, such as\nPubMed abstracts (Loukachevitch et al., 2023) and\nclinical notes (Nesterov et al., 2022a) (Sec. 4).\nFinally, we perform a set of experiments on a\nlarge in-house dataset of 865k EHRs from 164k pa-\ntients, divided into training and testing sets (further\nnamed RuCCoD-DP). Using the best-performing\nmodel on RuCCoD, we generated ICD codes for\nthe training set and evaluated how performance\nvaries when employing our automatic labels for a\ndiagnosis prediction task, compared to those as-\nsigned by doctors during patient visits (Sec. 5). Ex-\npriments have shown a great potential of automati-\ncally labeled EHRs for training a disease diagnostic\nmodel. Specifically, pre-training on automatically\nassigned ICD codes gives a huge macro-averaged\nF1-score growth of 28% for diagnosis prediction\ncompared to physician-assigned ICD codes. The\nfinding indicates a great complexity of diagnosis\nformalization within the ICD system for doctors\nas well as the great perspective for AI-guided diag-\nnosis prediction. We hope our work to provide a\nfoundation and guidance for researchers working\nin low-resource clinical languages.\n2\nICD-Related Tasks\nWe explore two complementary and interconnected\ntasks: ICD coding, which involves normalizing\na physician’s diagnostic conclusion into an appro-\npriate set of ICD codes, and diagnosis prediction,\n1We will release this dataset upon acceptance.\nFigure 2: Schematic description of ICD coding (in blue)\nand diagnosis prediction tasks (in yellow). Diagnosis\nprediction uses prior EHR data and current visit details,\nexcluding the doctor’s conclusion, which is used for\nICD coding to generate AI-assigned ICD codes. Both\noriginal and AI ICD code lists are then used as targets\nto train different diagnosis prediction models.\nwhere we aim to identify multiple diagnoses based\non a patient’s complete medical history without\nrelying on doctor-assigned diagnoses (Fig. 2).\nTask: ICD coding\nis akin to medical concept\nnormalization (or entity linking), where the objec-\ntive is to assign a set of unique ICD codes to the\nlatest patient appointment based on textual diag-\nnosis conclusion written by a doctor. The task is\naimed at helping a doctor normalize diagnosed dis-\neases to a set of codes from the complex formal\nICD hierarchy. We formulate the ICD Coding task\nas a modification of traditional information extrac-\ntion pipeline with three components: (1) Nested\nEntity Recognition (NER) and (2) Entity Linking\n(EL) followed by (3) EHR-level code aggregation.\nStep (3) aims to alleviate the influence of NER\ncomponent on the resulting quality metrics in an\nNER+EL pipeline by omitting NER spans. The\napproach aligns with real-world ICD applications,\nwhere the primary objective is accurate assignment\nof ICD codes (i.e., disease recognition), and impre-\ncise NER outputs are not impactful.\nICD Coding:\nEHR-level Code Aggregation\nGiven an EHR, we perform regular EL over NER\npredictions. Let Lp = (cp\n1, cp\n2, . . . , cp\nn) and Lt =\n(ct\n1, ct\n2, . . . , ct\nm) denote the lists of predicted and\nground truth ICD codes, respectively. In standard\nEL, each list may contain multiple mentions of\nthe same disease (i.e., ct\ni = ct\nj for i ̸= j). Next,\nwe remove duplicates from both lists resulting in\n2\n\n\nTrain\nTest\n# of records\n3000\n500\n# of assigned entities\n8769\n1557\n# of unique ICD codes\n1455\n548\nAvg. # of codes per record\n3\n3\nTable 1: Statistics for the RuCCoD training and testing\nsets on ICD coding of diagnosis.\nunique code sets Sp and St such that cp\ni ̸= cp\nj and\nct\ni ̸= ct\nj for ∀i̸=j. Finally, micro-averaged classi-\nfication metrics can be computed using the inter-\nsection of Sp and St being True Positives (TP),\nand sets differences being False Positives (FP)\nand False Negatives (FN) cases: TP = Sp ∩St;\nFP = St \\ Sp; FN = St \\ Sp.\nTask: Diagnosis Prediction\nis a multi-label clas-\nsification task also known as automated ICD predic-\ntion that outputs likely diagnoses (ICD codes) from\na patient’s past medical history, including com-\nplaints, test and examination results from previous\nappointments. In our study, each EHR contains a\ndoctor’s diagnosis conclusion. A major challenge\nfor ICD-grounded applications is that this conclu-\nsion is a free-form text, and its normalization to\nICD might introduce sensitive errors. Conversely,\nautomatic Diagnosis Prediction is constrained to\noutput ICD-compliant diagnoses by task design.\nICD Coding vs. Diagnosis Prediction\nWhile\nICD Coding only observes the current appoint-\nment’s diagnosis conclusion written by a doctor,\nthe goal of Diagnosis Prediction is to actually write\nthe diagnosis conclusion (i.e., make an AI diagno-\nsis conclusion). Here, the motivation is to offer a\ndoctor an independent, AI-driven opinion, poten-\ntially beneficial for decision-making in complex\ncases. Hence, the two tasks are complementary\nby design, using non-overlaping EHR parts: ICD\nCoding leverages the latest diagnosis while Diag-\nnosis Prediction observes an entire patient’s history\nexcept for the latest diagnosis conclusion.\n3\nICD Datasets\n3.1\nRuCCoD: ICD Coding Dataset\nFor ICD coding, we release RuCCoD, the first\ndataset of Russian EHRs with disease entities man-\nually linked to ICD-10. In this section, we describe\nthe data collection and annotation pipeline and pro-\nvide important statistics.\nData Collection\nAs a source for RuCCoD, we\nutilize diagnosis conclusions from the records of a\nFigure 3: Distribution of ICD code frequencies in the\nRuCCoD train set.\nmajor European city’s Medical Information System.\nBefore starting the annotation process, we imple-\nmented a meticulous de-identification protocol to\nprotect data privacy. Medical professionals invited\nto annotate the dataset first conducted a comprehen-\nsive manual review of all diagnoses. Their task was\nto identify and remove any personal or identifiable\ninformation manually. This thorough process guar-\nantees compliance with privacy regulations and\nensures the dataset is suitable for research use.\nAnnotation Process and Principles\nThe label-\ning team consisted of three highly qualified ex-\nperts with advanced education in different fields of\nmedicine, two of whom hold Ph.D. degrees, with\nevery annotation further validated by a fourth ex-\npert, a Ph.D. holder in medicine. Grounded in the\nICD-10 CM (Clinical Modification) system, the\nteam aimed to identify all nosological units in a\ndiagnosis conclusion and assign the most accurate\nICD code to each. An annotation example is shown\nin Fig. 1. The dataset was randomly split into 3,000\ntraining and 500 testing records. Each expert in-\ndependently annotated 1,000 training records for\ndiverse labeling, while all three annotated the same\n500 test records for consistency. An ICD code was\naccepted if at least two annotators agreed. Annota-\ntion guidelines are in Appx. A.\nInter-Annotator Agreement\nWe assessed anno-\ntation consistency among experts using the Inter-\nAnnotator Agreement (IAA) metric, defined as the\nratio of accepted codes to the total unique codes\nassigned per record (Luo et al., 2019). Among ICD\ncodes, the IAA value was 50%, indicating moderate\nagreement. We note that the inherent subjectivity in\nmanual ICD coding is evident in the low inter-coder\nagreement among human experts. Existing studies\nhave reported fair to moderate agreement on ter-\nminal ICD codes, with Kappa values ranging from\n3\n\n\nOriginal Dataset\nLinked Dataset\nManual Test Set\nNumber of records\n865539\n865539\n494\nNumber of unique patients\n164527\n164527\n450\nNumber of unique ICD codes\n3546\n3546\n394\nAvg. number of ICD codes per patient\n3 ± 2\n5 ± 2\n4 ± 2\nAvg. number of EHR records before current appointment\n(15, 36, 73)\n(15, 36, 73)\n(17, 36, 77)\nAvg. length of EHR records per one appointment\n(77, 167, 316)\n(77, 167, 316)\n(86, 176, 320)\nPatient’s age\n(59, 67, 74)\n(59, 67, 74)\n(60, 67, 75)\nPercentage of male patients\n69\n69\n71\nTable 2: Statistics for the randomly split training and testing sets of RuCCoD-DP for diagnosis prediction. Values in\nbrackets show the 25th, 50th, and 75th percentiles.\n27% to 42%, corresponding to agreement rates of\n29.2% and 46.8%, respectively (Stausberg et al.,\n2008). The reported accuracy of coding exhibits\nsignificant variability, ranging from 53% to 98%\n(Campbell et al., 2001) and from 41.8% to 88.87%\n(Hosseini et al., 2021). To sum up, the evaluation\nof inter-annotator agreement using the IAA metric\nhighlights both the challenges and strengths of our\nannotation process.\nDataset Statistics\nStatistics of train and test\nsplits of the RuCCoD dataset are provided in Tab. 1.\nDespite the large number of ICD codes, especially\nin the training set, their distribution is uneven.\nFig. 3 shows the distribution of ICD codes within\nthe RuCCoD train set. While a small number of\ncodes dominate the dataset, appearing from 50 to\n250 occurrences, most codes are rare, with 1,087\ncodes occurring fewer than 5 times. This stark dis-\nparity underscores the challenges of dealing with\nreal-world medical data, where frequent diagnoses\nare well-represented, but rare conditions remain\nsignificantly under-sampled.\n3.2\nRuCCoD-DP: Diagnosis Prediction\nDataset\nTo explore AI-guided Diagnosis Prediction, we\ncollect RuCCoD-DP (RuCCoD for Diagnosis\nPrediction), a corpus of real-world EHRs.\nDataset Construction\nRuCCoD-DP includes\ndoctor appointments from 2017 to 2021, divided\ninto four parts: (i) patient complaints and anamne-\nsis, (ii) lab test results, (iii) appointment summary\n(including assigned ICD codes), and (iv) appoint-\nment history. Although RuCCoD and RuCCoD-DP\nshare a common source, we ensure both sets to have\nno overlapping appointments and patients.\nPaired Human-AI ICD Codes\nICD has a fine-\ngrained disease hierarchy introducing a significant\nchallenge even for a qualified doctor to formal-\nize a correct general diagnosis to a specific ICD\ncode. For instance, a H10 Conjunctivitis disease\ngroup has 8 specifications including: H10.0 mu-\ncopurulent, H10.1 acute atopic, H10.2 other acute,\nand H10.3 unspecified acute conjunctivitis. Thus,\ndoctor-assigned ICD codes in real-world EHRs can\nexpose substantial errors even if a disease is diag-\nnosed correctly. Our work is the first to explore\nthe mismatch between verbose diagnosis conclu-\nsion and the assigned ICD codes by modeling a\nvirtual AI-based medical expert. For each EHR,\nwe consider two ICD code sets: (i) real-world ICD\ncodes originally written by physicians within the\nEHR (doctor-assigned codes); (ii) automatically\nassigned ICD codes predicted by a neural model\ntrained on RuCCoD (AI codes). To obtain AI codes,\nwe applied an automated ICD coding for each ap-\npointment to relabel the ICD codes based on a\ndoctor’s appointment summary. The AI-generated\ncodes capture medical entities explicitly mentioned\nin an appointment report.\nOriginal and Linked RuCCoD-DP\nWe will re-\nfer to RuCCoD-DP variations sharing the same\nappointments yet different in ICD code assignment\nmethod (either doctor-assigned or AI-based) as\noriginal and linked datasets, respectively. In other\nwords, a single textual appointment entry has two\ndistinct labels sets. To prevent ICD codes distri-\nbution shift between original and linked data, we\nretained the ICD codes overlapping between these\ntwo sets. For each appointment sample, its textual\ninput included the concatenation of chronologically\nsorted all prior appointments.\nDiagnosis Prediction Test Set\nThe collection of\ntwo sets of labels allows exploration of whether\nmanual or generated ICD labels are more reliable\nfor model training. For a fair comparison of the la-\nbeling approaches, we manually labeled a common\n4\n\n\ntest set from a subset of the original appointment\ndataset’s test set. We formed it by selecting a sub-\nset from the test part of the original appointment\ndataset. For annotation, we adopted the same an-\nnotation methodology as for the RuCCoD dataset\n(Sec. 3.1). The IAA between the doctors was 50%\nfor exact ICD codes and 74% for ICD groups. The\nfinal statistics for original, linked datasets as well\nas the common manual test is summarized in Tab. 2.\n4\nICD Coding Evaluation\nFor ICD coding experiments, we experiment with\nthe following approaches: 1) a fine-tuned BERT-\nbased pipeline for information extraction, 2) a large\nlanguage model (LLM) with Parameter-Efficient\nFine-Tuning (PEFT), and 3) LLM with retrieval-\naugmented generation (RAG). All three systems\nuse the same dictionary, with 17,762 pairs of codes\nand diagnoses (refered to as ICD dict). This was\ncompiled using data from the Ministry of Health.\nIn addition, LLM-based systems used a train set as\na dictionary as well. See the Appx. G for a list of\nthe LLMs used. See related work in Appx. B.\n4.1\nModels\nBERT-based IE Pipeline\nWe developed our in-\nformation extraction (IE) system with two sequen-\ntial modules for each task.\nThe NER module\nwith the softmax layer extracts relevant entities,\nwhile the EL module links these extracted en-\ntities to corresponding ICD codes.\nFor NER,\nwe utilize the pre-trained RuBioBERT (Yalunin\net al., 2022), and for EL, we employ the multilin-\ngual state-of-the-art models SapBERT (Liu et al.,\n2021a,b), CODER (Yuan et al., 2022b), and BERG-\nAMOT (Sakhovskiy et al., 2024). We fine-tuned\nmodels on training EL sets via synonym marginal-\nization as suggested by the authors of BioSyn (Sung\net al., 2020). For more details, see Appx G.\nLLMs with PEFT\nWe explored the capabilities\nof LLMs for clinical coding using PEFT with Low-\nRank Adaptation (LoRA) (Hu et al., 2021). The\npipeline comprised two stages: NER and EL, fol-\nlowing the same structure as the BERT-based IE\npipeline described earlier. The NER stage involved\nfine-tuning models on RuCCoD using task-specific\nprompts (Appx. H). Predictions on the test set were\nvalidated via exact string matching or Levenshtein\ndistance (threshold ≤2) to accommodate spelling\nvariations. For the EL stage, a RAG approach was\nemployed to link extracted entities to ICD codes.\nThis approach involved three strategies for build-\ning the retrieval component: (i) Using only the ICD\ndict with embeddings from the widely recognized\nBGE model (Chen et al., 2024), (ii) Combining the\nICD dict and RuCCoD training entities, also with\nBGE embeddings, and (iii) Fine-tuning BERG-\nAMOT embeddings (Sakhovskiy et al., 2024) on\nRuCCoD while using the ICD dict.\nThe retrieval process used a FAISS index (Douze\net al., 2024) for each strategy. For each entity ex-\ntracted in the NER stage, the top-15 most similar\nentries were retrieved from the index. The final\nICD code was assigned using an LLM to select\nthe closest match from the retrieved candidates\n(prompt in Appx. H). To address class imbalance,\ndiagnosis lists were shuffled during training, forc-\ning models to learn contextual code-discrimination.\nFine-tuning parameters followed standard LoRA\nconfigurations (Tab. 4, Appx. G).\nZero-shot LLM with RAG\nAs an ablation study,\nwe evaluated the same pipeline as in the PEFT\nstage but without fine-tuning to isolate the LLMs’\ninherent capabilities. We used only the fine-tuned\nBERGAMOT embeddings from strategy (iii) for\nretrieval, retaining the FAISS index and prompts\n(Appx. H). The LLM selected ICD codes from\nretrieved candidates if no direct match was found,\nreplicating the EL process from the PEFT stage.\nThis setup allowed us to quantify the contribution\nof fine-tuning versus zero-shot inference.\n4.2\nEvaluation Methodology\nOn RuCCoD, our evaluation includes the newly in-\ntroduced ICD Coding task as well as conventional\nNER and EL. To recall, ICD Coding can be seen\nas an entity position-agnostic NER+EL task com-\nposition with explicitly removed EHR-level ICD\ncode duplicates. For instance, a language model\nsuccessfully diagnoses a patient by assigning the\ncorrect ICD code when it finds at least one of three\nmentions of the corresponding ICD disease within\nan EHR. For all three tasks, we evaluate the stan-\ndard classification micro-averaged metrics, namely\nprecision, recall, F1-score, and accuracy.\nFor EL, we use a retrieval-based approach (Liu\net al., 2024; Yuan et al., 2022b; Sakhovskiy et al.,\n2024) and evaluate retrieval accuracy with acc@k,\nwhere acc@k = 1 if a correct ICD code is retrieved\nat rank ≤k. We consider two evaluation scenarios:\n(i) strict score assessing exact match between a\npredicted and a ground truth codes and (ii) relaxed\n5\n\n\nModel\nPrecision\nRecall\nF-score\nAccuracy\nSupervised with various corpora for NER and EL\nBERT, NER: NEREL-BIO + RuCCoD, EL: RuCCoD\n0.512\n0.529\n0.520\n0.352\nBERT, NER: RuCCoN + RuCCoD, EL: RuCCoD\n0.471\n0.543\n0.504\n0.337\nBERT, NER: RuCCoD, EL: RuCCoD\n0.510\n0.542\n0.525\n0.356\nLLM with RAG (zero-shot with dictionaries)\nLLaMA3-8b-Instruct, NEREL-BIO\n0.059\n0.053\n0.056\n0.029\nLLaMA3-8b-Instruct, RuCCoN\n0.164\n0.15\n0.157\n0.085\nLLaMA3-8b-Instruct, ICD dict.\n0.379\n0.363\n0.371\n0.228\nLLaMA3-8b-Instruct, ICD dict. + RuCCoD\n0.465\n0.451\n0.458\n0.297\nLLM with tuning\nPhi3_5_mini, ICD dict.\n0.394\n0.39\n0.392\n0.244\nPhi3_5_mini, ICD dict. + RuCCoD\n0.483\n0.477\n0.48\n0.316\nPhi3_5_mini, ICD dict. + BERGAMOT\n0.454\n0.448\n0.451\n0.291\nTable 3: Entity-level code assignment metrics on RuCCoD’s test set. The best results are highlighted in bold. We\nalso refer to Appx. D, E, F on more experiments with different LMs, corpora, and terminologies.\nscore with each code being truncated to its higher-\nlevel disease group (e.g., H10.0 mucopurulent is\ntruncated to H10 Conjunctivitis).\n4.3\nResults\n4.3.1\nTransfer Learning\nFirst, we performed cross-domain experiments on\nthe entity linking task (with gold entities provided)\nto see how variability in entities and terminology\naffect performance. Since UMLS includes the ICD\nsystem, we automatically map UMLS CUIs to ICD\ncodes for evaluation. Cross-domain transfer results\nwith entity linking models on RuCCoD, RuCCoN,\nNEREL-BIO as well as their union are presented\nin detail in Appx. D. Our evaluation revealed the\nfollowing key observations.\nMaleficent Cross-Domain Vocabulary Extension\nWhile extension of ICD vocabulary consistently\ngives a slightly improved acc@1 in a zero-shot set-\nting, additional synonyms introduce severe noise\nin a supervised setting. Specifically, a significant\ndrop of 8.1%, 8.4%, and 14.3% acc@1 is observed\nfor SapBERT, CODER, and BERGAMOT, respec-\ntively. Even in an unsupervised setting, vocabulary\nextension drops acc@5 by 5.2% and 6.8% for Sap-\nBERT and BERGAMOT, respectively.\nComplicated\nCross-Terminology\nTransfer\nBoth training on RuCCoN and NEREL-BIO as\nwell merge of these corpora with RuCCoD do not\nlead to improvement over zero-shot coding. The\nfinding indicates that RuCCoN and NEREL-BIO\ndatasets do not easily transfer to our dataset as\nwell as the specificity and high complexity of\nhierarchical ICD coding within the EL task.\nComplexity of Fine-Grained ICD Coding\nThe\n15% gap in acc@1 between strict and relaxed evalu-\nations shows the challenging nature of semantically\nsimilar diseases within the same therapeutic group.\nTransfer learning for NER is Feasible\nRegard-\ning the quality of NER across different training sets\nfor disease-related entities, the model trained on the\nNEREL-BIO dataset and tested on the RuCCoD\ntest set yielded an F1 score of 0.62. The model\ntrained on a combined dataset of NEREL-BIO and\nRuCCoD achieved scores of 0.72. Similar results\nwere observed with RuCCoN. We also evaluated\nBINDER, which uses a RuBioBERT backbone and\ntreats NER as a representation learning problem by\nmaximizing similarity between vector representa-\ntions (Zhang et al.). However, BINDER’s perfor-\nmance was 1.5% lower than that of RuBioBERT,\nwhich achieved the best F1 score of 0.77 using a\nsoftmax classifier. We conclude that the transfer\nof NER for disease entities is significantly better\nthan for entity linking (EL), with the best results\nobtained from RuCCoD (full results in Appx. C).\n4.3.2\nEnd-to-end ICD coding\nIn the next experiments, we evaluated an end-to-\nend ICD coding quality on raw texts, in which\nmodels were fine-tuned on either RuCCoN or\nNEREL-BIO or utilized entity dictionaries from\n6\n\n\nthese datasets, are presented in Tab. 3. As seen\nfrom the results, training on datasets from other do-\nmains gives limited performance and the best ICD\ncoding results are observed for models trained with\nICD data from RuCCoD data on all three set-ups.\nExtended RAG results are in Appx. E. Fine-\ntuning LLMs improves performance across all\ntasks, exceeding LLM + RAG results in zero-shot\nsettings. Use of RuCCoD significantly enhances\nmetrics compared to approaches that rely solely on\nthe ICD dictionary or embeddings. Llama3-Med42-\n8B and Phi3_5_mini are the most effective models\nafter PEFT tuning (see Appx. F).\n5\nDiagnosis Prediction Evaluation\n5.1\nExperimental Set-up\nModel\nWe chose the Longformer architec-\nture (Beltagy et al., 2020) due to its strong perfor-\nmance in clinical tasks (Edin et al., 2023). Our\nLongformer model is initialized from a BERT\nmodel pre-trained using private EHRs from mul-\ntiple clinics and further pre-trained on extended\nsequences. Training details are in Appx. G.\nEvaluation\nDue to highly unbalanced ICD codes\ndistribution, we adopted the F1-score to assess\nmodel performance for each class and the weighted\nF1-score to evaluate overall performance across all\nclasses for the Diagnosis Prediction task. These\nmetrics are well-suited for handling class imbal-\nance, a known challenge in medicine tasks (John-\nson and Khoshgoftaar, 2019). For the weighted\nF1-score, the weight of each class is calculated as\nthe proportion of appointment documents sharing\nthe given ICD code in the union of both training\ndatasets. In our experiments, we evaluate the qual-\nity of the models trained on original and linked\ndatasets on the manual test set.\n5.2\nResults\n5.2.1\nDiagnosis Prediction Learning\nWe fine-tuned two Longformer instances: one on\nthe original dataset and one on the linked dataset,\nfor predicting ICD codes from doctor’s appoint-\nments. Fig. 4 presents the dependences of the\nweighted F1-scores on training step number for\nthese two models.\nAI-based ICD Coding Improves Diagnosing\nAs seen from Fig. 4, AI-guided ICD coding (linked\ndata) significantly outperforms manual coding\n(original data) with the peak weighted F1-score\nFigure 4: Comparison of weighted F1 scores on the\nmanual diagnosis prediction test set for models trained\non original and linked datasets at different training steps.\nFigure 5: F1 score distribution for top and bottom 10%\nfrequent ICD codes in the common test set.\nof 0.48. The latter quickly reaches its F1-score\nplateau at 0.2. The huge performance gap of 0.28\nhighlights the effectiveness of automatic data anno-\ntation for model training. Yet, the finding reveals\nthe complexity of ICD-agreed diagnosis prediction\ntask for professional physicians indicating the ne-\ncessity of AI-driven assistance.\n5.3\nDiagnosing Stability to Disease Frequency\nNext, we study the diagnosis prediction model’s\nability to generalize to both frequent and rare dis-\nease when trained on original and linked datasets.\nFrequency-Based ICD Test Set Split\nThe test\ndataset was split into two parts: the 10% most\nfrequent ICD codes and the 10% least frequent ICD\ncodes, with a minimum frequency threshold of 15\ninstances in the manual test set for the less frequent\ngroup. The stratification approach is designed to\nalign with the distribution of real-world diagnoses\nassigned and carefully verified by clinicians.\nDiagnosing Improvement is Frequency-Robust\nFig. 5 presents the F1 scores spread for individ-\nual ICD codes (diseases) grouped by frequency\ngroups. The model trained on linked data outper-\nforms the one trained on original data for both rare\n7\n\n\nFigure 6: Dependency between differences in the num-\nber of codes in original and linked train sets and corre-\nsponding F1 scores differences on the common test.\nand frequent codes. The ∼6x median F1 score im-\nprovement for the bottom 10% codes (0.6 vs. 0.1)\nunderscores the difficulty of manually assigning\nICD codes for infrequent diseases. For frequent\ncodes, the training on linked data gives about 0.3\nmedian F1 growth over original data (∼0.7 vs 0.4)\nwith a significantly lower score deviation (indicated\nby smaller interquartile distance). Thus, pretrain-\ning on automatically labeled data enhances diagno-\nsis prediction for both rare and common diseases,\nreducing variability for the latter.\n5.4\nDisease-Wise Quality Shift Analysis\nLinked Data’s Improvement Stability\nFig. 6\ndisplays how changes in appointment counts from\noriginal to linked data affect the diagnosis predic-\ntion F1 score. Notably, F1 scores generally im-\nproved across the majority of ICD codes regard-\nless of whether automatic linking increased or de-\ncreased the number of appointments. This suggests\nimproved class balance in the linked dataset, al-\nthough the effect varies.\nCase Study: Diagnosing Degradation\nIn Fig. 6,\na sharp F1 score drop is observed for I25.2 Past\nmyocardial infarction. Apparently, the disease has\nbeen mistakingly re-linked to other errouness by\nour ICD Coding system. We studied the case by an-\nalyzing which ICD codes has I25.2 been replaced\nwith. As shown in Fig. 7, the most frequent tran-\nsition, from I25.2 to I11.9 (Hypertensive heart\ndisease, 29,639 cases), resulted in a minimal F1\nimprovement of 0.02, likely due to overlapping\nsymptoms. The transition to E11.9 (Type 2 dia-\nbetes mellitus, 11,413 cases) produced the high-\nest F1 gain of 0.48, reflecting clearer distinctions.\nTransitions to I25.1 (Atherosclerotic heart disease,\n12,932 cases) and I20.9 (Angina pectoris, 11,819\ncases) led to significant F1 increases of 0.38 and\n0.47, while I67.9 (Unspecified cerebrovascular dis-\nFigure 7: The relationship between transitions from\nI25.2 and F1 improvements: numbers on the arrows\nindicate transition frequency, while node color intensity\nrepresents the magnitude of F1 metric change.\nease, 10,573 cases) showed a moderate gain of\n0.21. These results suggest that diagnoses with\nclearer distinctions improve F1 more than those\nwith overlapping symptoms.\n6\nConclusion\nIn this paper, we presented the first models for\nmulti-label ICD-10 coding of electronic health\nrecords (EHRs) in Russian. Our study focuses\non two key tasks: information extraction from the\ndiagnosis field of EHRs and diagnosis prediction\nbased on a patient’s medical history. The NLP\npipeline developed for the first task was utilized\nto re-annotate EHRs in the training set for the sec-\nond task. The results demonstrate that fine-tuned\nLMs significantly enhance performance in predict-\ning ICD codes from past medical history. Specif-\nically, the model trained on automatically linked\ndata exhibited faster learning and better general-\nization compared to the original dataset, achieving\nhigher weighted F1 scores early in training, while\nthe original model plateaued with minimal improve-\nments. Notably, the linked data model consistently\noutperformed the original across both frequent and\nrare ICD classes, achieving higher F1 scores with\nreduced variability. This suggests that the linked\ndataset enables effective handling of both common\nand rare ICD codes. Overall, our findings highlight\nthe importance of a neural pipeline for automat-\ning ICD coding and improving the accuracy and\ninformativeness of medical text labeling.\nFuture research will focus on the integration of\nadditional external medical sources like knowledge\ngraphs to improve ICD code prediction. We plan\nto study the generalization of LLMs on rare codes.\n8\n\n\nLimitations\nOther biomedical corpora in Russian\nThe\nmost relevant corpora to our study are RuC-\nCoN (Nesterov et al., 2022b) and NEREL-BIO\n(Loukachevitch et al., 2023, 2024), which link en-\ntities from clinical records or PubMed abstracts to\nthe Russian segment of UMLS. We conducted pre-\nliminary transfer learning experiments using these\ntwo corpora; however, a detailed analysis of the\nsemantic differences among the three corpora has\nyet to be performed.\nModerate Inter-Annotator Agreement\nAmong\nICD codes, the IAA value was 50%, indicating\nmoderate agreement, while for ICD groups, the\nIAA increased to 74%, reflecting higher consis-\ntency at a group-wise level. This disparity suggests\nthat annotators were generally aligned when cate-\ngorizing broader ICD groups but faced challenges\nin granular code assignment. This pattern mirrors\ntrends observed in clinical practice, possibly due to\nambiguities in documentation and coding guide-\nlines (cf. §3.1).\nWhile our terminal code IAA\n(50%) aligns with the upper bounds of reported\nexpert agreement (29.2%–46.8%) (Stausberg et al.,\n2008), the residual variability underscores the need\nfor standardized annotation protocols or ensemble\napproaches to mitigate subjectivity in fine-grained\ncoding.\nClinical Diversity\nWhile our dataset is substan-\ntial, it may not fully capture the diversity of clinical\nscenarios and patient demographics. A more varied\ndataset could improve the robustness and general-\nizability of the models. Clinical language can vary\nsignificantly across different medical specialties\nand institutions. This variability may impact the\nmodel’s ability to generalize across various clinical\ncontexts.\nData Imbalance\nThe dataset may suffer from\nclass imbalance, with certain ICD codes being un-\nderrepresented. This could affect the model’s abil-\nity to generalize and accurately predict less com-\nmon diagnoses.\nEthics Statement\nNo Personal Patient Data in RuCCoD\nRuC-\nCoD does not contain any personally identifiable\npatient information. The dataset consists solely\nof diagnosis conclusions written by medical pro-\nfessionals, which were manually labeled based on\nthe ICD-10 CM (Clinical Modification) system.\nPrior to the annotation process, annotators were\ninstructed to ensure that no personal information\nwas included in the conclusions. Their task was\nto identify and remove any personal or identifiable\ninformation manually from these texts. Overall, no\npatient-related information will be disclosed upon\nthe dataset’s release.\nPrivate in-house EHR data in RuCCoD-DP\nDi-\nagnosis prediction leverages prior EHR data along\nwith details from the current visit. As a source for\nRuCCoD-DP, we utilize records from the Medical\nInformation System of a major European city. All\npatients, prior to visiting a doctor, sign a special\nconsent form for the processing of their data. The\nEHR data, which forms the foundation of RuCCoD,\nis an in-house dataset that will not be released.\nHuman Annotations\nThe dataset introduced in\nthis paper involved only new annotations. Dataset\nannotation was conducted by annotators, and there\nare no associated concerns (e.g. regarding compen-\nsation). Each annotator received a compensation\nof approximately $12 per hour for their contribu-\ntions. An estimated 85 hours of annotation work\nper expert resulted in a total payment of $1,020\nper annotator. For context, the minimum monthly\nwage in Russia for full-time employment is un-\nder $200, highlighting the substantial effort and\ninvestment in creating this high-quality resource.\nAll annotators were aware of potential annotation\nusage for research purposes. As discussed in lim-\nitations, we believe these new annotated datasets\nserve as a starting point for the evaluation of LMs\non ICD coding in Russian. Our annotations, code,\nand annotation guidelines will be released upon\nacceptance of this paper.\nInference Costs\nRunning the complete evalua-\ntion experiment on a single V100 GPU takes ap-\nproximately 7.5h and 11h for a decoder-only and\nencoder-only LM, respectively, while the LLM\nwith RAG evaluation experiment on a single A100\nGPU takes approximately 5.5h.\nPotential Misuse\nThe RuCCoD dataset, intended\nfor ICD coding in Russian, may be misused if not\nhandled correctly. Potential issues include inac-\ncurate applications leading to incorrect code as-\nsignments and overreliance on automated systems\nwithout proper validation. To prevent these prob-\nlems, it is crucial to provide clear guidelines and\n9\n\n\nadequate training for doctors on using AI assis-\ntants, ensuring compliance with ethical and legal\nstandards in research and healthcare.\nTransparency\nThe RuCCoD and all associated\nannotation materials are being released under the\nCC BY 4.0 license. It should be noted that the\ndataset contains only diagnosis codes and no medi-\ncal histories or personal patient data. Furthermore,\nall diagnoses have been rigorously verified to en-\nsure complete anonymity, in accordance with the\nprevailing norms of open research practice. Our\nGitHub repository and HuggingFace dataset card\nwill include comprehensive documentation on the\ncodebase, the methodology for creating bench-\nmarks, and the human annotation process. The\nsource code for our experiments will be freely\navailable at this anonymized repository: https:\n//github.com/auto-icd-coding/ruccod.\nUse of AI Assistants\nWe utilize Grammarly to\nenhance and proofread the text of this paper, cor-\nrecting grammatical, spelling, and stylistic errors,\nas well as rephrasing sentences. Consequently, cer-\ntain sections of our publication may be identified\nas AI-generated, AI-edited, or a combination of\nhuman and AI contributions.\nReferences\nLlama-3.1-8b-instruct.\nhttps://ollama.com/\nlibrary/llama3.1:8b-instruct-fp16.\nMed42-8b. https://huggingface.co/m42-health/\nLlama3-Med42-8B.\nMistral-nemo-instruct-2407. https://huggingface.\nco/mistralai/Mistral-Nemo-Instruct-2407.\nPhi-3.5-mini-instruct.\nhttps://huggingface.co/\nmicrosoft/Phi-3.5-mini-instruct.\nQwen2.5-7b-instruct.\nhttps://huggingface.co/\nQwen/Qwen2.5-7B-Instruct.\nAkhila Abdulnazar, Roland Roller, Stefan Schulz, and\nMarkus Kreuzthaler. 2024. Large language models\nfor clinical text cleansing enhance medical concept\nnormalization. IEEE Access, 12:147981–147990.\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nB. A. McDermott. 2019. Publicly available clinical\nbert embeddings. ArXiv, abs/1904.03323.\nTian Bai and Slobodan Vucetic. 2019. Improving medi-\ncal code prediction from clinical text via incorporat-\ning online knowledge sources. The World Wide Web\nConference.\nTian Bai, Shanshan Zhang, Brian L Egleston, and Slo-\nbodan Vucetic. 2018. Interpretable representation\nlearning for healthcare via capturing disease progres-\nsion through time. In Proceedings of the 24th ACM\nSIGKDD international conference on knowledge dis-\ncovery & data mining, pages 43–51.\nKrishanu Das Baksi, Elijah Soba, John J. Higgins, Ravi\nSaini, Jaden Wood, Jane Cook, Jack Scott, Nirmala\nPudota, Tim Weninger, Edward Bowen, and Sanmitra\nBhattacharya. 2024. Medcoder: A generative ai assis-\ntant for medical coding. Preprint, arXiv:2409.15368.\nWeidong Bao, Hongfei Lin, Yijia Zhang, Jian Wang, and\nShaowu Zhang. 2021. Medical code prediction via\ncapsule networks and icd knowledge. BMC Medical\nInformatics and Decision Making, 21.\nTal Baumel, Jumana Nassour-Kassis, Raphael Cohen,\nMichael Elhadad, and Noémie Elhadad. 2018. Multi-\nlabel classification of patient notes: Case study on\nicd code assignment. In AAAI Workshops.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. CoRR,\nabs/2004.05150.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (UMLS): integrating biomedical termi-\nnology. Nucleic Acids Res., 32(Database-Issue):267–\n270.\nZeyd Boukhers, Prantik Goswami, and Jan Jürjens.\n2023. Knowledge guided multi-filter residual con-\nvolutional neural network for icd coding from clin-\nical text.\nNeural Computing and Applications,\n35(24):17633–17644.\nJoseph S. Boyle, Antanas Kascenas, Pat Lok, Maria Li-\nakata, and Alison Q. O’Neil. 2023. Automated clini-\ncal coding using off-the-shelf large language models.\nPreprint, arXiv:2310.06552.\nE M Burns, Emily Rigby, Ravikrishna Mamidanna, Alex\nBottle, Paul P Aylin, Paul Ziprin, and Omar Faiz.\n2012. Systematic review of discharge coding accu-\nracy. Journal of public health, 34 1:138–48.\nSean E Campbell, Mark K Campbell, Jeremy M\nGrimshaw, and Angela E Walker. 2001. A system-\natic review of discharge coding accuracy. Journal of\nPublic Health Medicine, 23(3):205–211.\nPengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Sheng-\nping Liu, and Weifeng Chong. 2020. HyperCore: Hy-\nperbolic and co-graph representation for automatic\nICD coding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3105–3114.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu\nLian, and Zheng Liu. 2024. Bge m3-embedding:\nMulti-lingual, multi-functionality, multi-granularity\ntext embeddings through self-knowledge distillation.\nPreprint, arXiv:2402.03216.\n10\n\n\nPing Cheng, Annette Gilchrist, Kerin Robinson, and\nLindsay Paul. 2009. The risk and consequences of\nclinical miscoding due to inadequate medical doc-\numentation: A case study of the impact on health\nservices funding. Health Information Management\nJournal, 38:35 – 46.\nEdward Choi, Mohammad Taha Bahadori, Andy\nSchuetz, Walter F Stewart, and Jimeng Sun. 2016.\nDoctor ai: Predicting clinical events via recurrent\nneural networks. JMLR Workshop and Conference\nProceedings, 56:301–318. PMID: 28286600, PMC:\nPMC5341604.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXiaolin Diao, Yanni Huo, Shuai Zhao, Jing Yuan, Meng\nCui, Yuxin Wang, Xiaodan Lian, and Wei Zhao. 2021.\nAutomated icd coding for primary diagnosis via clin-\nically interpretable machine learning. International\njournal of medical informatics, 153:104543.\nHang Dong, Víctor Suárez-Paniagua, Huayu Zhang,\nMinhong Wang, Emma Whitfield, and Honghan Wu.\n2021. Rare disease identification from clinical notes\nwith ontologies and weak supervision. In 2021 43rd\nAnnual International Conference of the IEEE En-\ngineering in Medicine & Biology Society (EMBC),\npages 2294–2298. IEEE.\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff\nJohnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré,\nMaria Lomeli, Lucas Hosseini, and Hervé Jégou.\n2024. The faiss library.\nJoakim Edin, Alexander Junge, Jakob Drachmann Hav-\ntorn, Lasse Borgholt, Maria Maistro, Tuukka Ruot-\nsalo, and Lars Maaløe. 2023. Automated medical\ncoding on mimic-iii and mimic-iv: A critical review\nand replicability study. Proceedings of the 46th In-\nternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval.\nFumitoshi\nFukuzawa,\nYasutaka\nYanagita,\nDaiki\nYokokawa, Shun Uchida, Shiho Yamashita, Yu Li,\nKiyoshi Shikino, Tomoko Tsukamoto, Kazutaka\nNoda, Takanori Uehara, and Masatomi Ikusaka.\n2024.\nImportance of patient history in artificial\nIntelligence-Assisted medical diagnosis: Compari-\nson study. JMIR Med Educ, 10:e52674.\nChufan Gao, Mononito Goswami, Jieshi Chen, and Ar-\ntur Dubrawski. 2022. Classifying unstructured clin-\nical notes via automatic weak supervision. In Ma-\nchine Learning for Healthcare Conference, pages\n673–690. PMLR.\nA. L. Goldberger, L. A. Amaral, L. Glass, J. M. Haus-\ndorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B.\nMoody, C. K. Peng, and H. E. Stanley. 2000. Phys-\nioBank, PhysioToolkit, and PhysioNet: Components\nof a new research resource for complex physiologic\nsignals. Circulation, 101(23):E215–220.\nRobert Grout, Rishab Gupta, Ruby Bryant, Mawada A\nElmahgoub, Yijie Li, Khushbakht Irfanullah, Rahul F\nPatel, Jake Fawkes, and Catherine Inness. 2024. Pre-\ndicting disease onset from electronic health records\nfor population health management: a scalable and ex-\nplainable deep learning approach. Front Artif Intell,\n6:1287541.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nNafiseh Hosseini, Khalil Kimiafar, Sayyed Mostafa\nMostafavi, Behzad Kiani, Kazem Zendehdel, Armin\nZareiyan, and Saeid Eslami. 2021. Factors affect-\ning the quality of diagnosis coding data with a tri-\nangulation view: A qualitative study. The Interna-\ntional Journal of Health Planning and Management,\n36(5):1666–1684.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nShaoxiong Ji, Matti Hölttä, and Pekka Marttinen. 2021.\nDoes the magic of bert apply to medical code assign-\nment? a quantitative study. Computers in biology\nand medicine, 139:104998.\nAlistair E. W. Johnson, Lucas Bulgarelli, Lu Shen,\nAlvin Gayles, Ayad Shammout, Steven Horng, Tom J.\nPollard, Benjamin Moody, Brian Gow, Li-wei H.\nLehman, Leo A. Celi, and Roger G. Mark. 2023.\nMIMIC-IV, a freely accessible electronic health\nrecord dataset. Scientific Data, 10(1):1.\nJustin M. Johnson and Taghi M. Khoshgoftaar. 2019.\nSurvey on deep learning with class imbalance. Jour-\nnal of Big Data, 6(1):27.\nByung-Hak Kim and Varun Ganapathi. 2021. Read,\nattend, and code: Pushing the limits of medical codes\nprediction from clinical notes by machines. In Ma-\nchine Learning for Healthcare Conference, pages\n196–208. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization.\nIn 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nEyal Klang, Idit Tessler, Donald U Apakama, Ethan Ab-\nbott, Benjamin S Glicksberg, Monique Arnold, Akini\n11\n\n\nMoses, Ankit Sakhuja, Ali Soroush, Alexander W\nCharney, David L. Reich, Jolion McGreevy, Nicholas\nGavin, Brendan Carr, Robert Freeman, and Girish N\nNadkarni. 2024. Assessing retrieval-augmented large\nlanguage model performance in emergency depart-\nment icd-10-cm coding compared to human coders.\nmedRxiv.\nHeejoon Koo. 2024. Next visit diagnosis prediction via\nmedical code-centric multimodal contrastive EHR\nmodelling with hierarchical regularisation. CoRR,\nabs/2401.11648.\nKeith Kwan. 2024. Large language models are good\nmedical coders, if provided with tools.\nPreprint,\narXiv:2407.12849.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nFei Li and Hong Yu. 2020. ICD coding from clinical\ntext using multi-filter residual convolutional neural\nnetwork. In Proceedings of the AAAI conference on\nartificial intelligence, volume 34, pages 8180–8187.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021a. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4228–4238, Online. Association for Computa-\ntional Linguistics.\nFangyu Liu, Ivan Vuli´c, Anna Korhonen, and Nigel\nCollier. 2021b. Learning domain-specialised repre-\nsentations for cross-lingual biomedical entity linking.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 565–574,\nOnline. Association for Computational Linguistics.\nHaochen Liu, Sai Rallabandi, Yijing Wu, Parag Dakle,\nand Preethi Raghavan. 2024. Self-training strategies\nfor sentiment analysis: An empirical study. In Find-\nings of the Association for Computational Linguistics:\nEACL 2024, pages 1944–1954, St. Julian’s, Malta.\nAssociation for Computational Linguistics.\nYang Liu, Hua Cheng, Russell Klopfer, Matthew R\nGormley, and Thomas Schaaf. 2021c. Effective con-\nvolutional attention network for multi-label clinical\ndocument classification. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5941–5953.\nIlya Loshchilov and Frank Hutter. 2017. Sgdr: Stochas-\ntic gradient descent with warm restarts. Preprint,\narXiv:1608.03983.\nNatalia Loukachevitch, Suresh Manandhar, Elina Baral,\nIgor Rozhkov, Pavel Braslavski, Vladimir Ivanov,\nTatiana Batura, and Elena Tutubalina. 2023. NEREL-\nBIO: A Dataset of Biomedical Abstracts Anno-\ntated with Nested Named Entities. Bioinformatics.\nBtad161.\nNatalia Loukachevitch, Andrey Sakhovskiy, and Elena\nTutubalina. 2024. Biomedical concept normaliza-\ntion over nested entities with partial UMLS termi-\nnology in Russian. In Proceedings of the 2024 Joint\nInternational Conference on Computational Linguis-\ntics, Language Resources and Evaluation (LREC-\nCOLING 2024), pages 2383–2389, Torino, Italia.\nELRA and ICCL.\nChang Lu, Tian Han, and Yue Ning. 2022. Context-\naware health event prediction via transition func-\ntions on dynamic disease graphs. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 36, pages 4567–4574.\nChang Lu, Chandan Reddy, Ping Wang, and Yue Ning.\n2023. Towards semi-structured automatic icd cod-\ning via tree-based contrastive learning. Advances in\nNeural Information Processing Systems, 36:68300–\n68315.\nYen-Fu Luo, Weiyi Sun, and Anna Rumshisky. 2019.\nMcn: A comprehensive corpus for medical concept\nnormalization. Journal of Biomedical Informatics,\n92:103132.\nMingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, An-\nthony Cuturrufo, Vijay S Nori, Eran Halperin, and\nWei Wang. 2025. Memorize and rank: Elevating\nlarge language models for clinical diagnosis predic-\ntion. Preprint, arXiv:2501.17326.\nYubo Ma, Yixin Cao, Yong Hong, and Aixin Sun. 2023.\nLarge language model is not a good few-shot informa-\ntion extractor, but a good reranker for hard samples!\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023. Association for Computa-\ntional Linguistics.\nLaxmaiah Manchikanti. 2002. Implications of fraud\nand abuse in interventional pain management. Pain\nPhysician, 5(3):320.\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng\nSun, and Jacob Eisenstein. 2018a. Explainable pre-\ndiction of medical codes from clinical text. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1101–1111.\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng\nSun, and Jacob Eisenstein. 2018b. Explainable pre-\ndiction of medical codes from clinical text. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1101–1111, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\n12\n\n\nAlexandr Nesterov, Galina Zubkova, Zulfat Miftahutdi-\nnov, Vladimir Kokh, Elena Tutubalina, Artem Shel-\nmanov, Anton Alekseev, Manvel Avetisian, Andrey\nChertok, and Sergey Nikolenko. 2022a. RuCCoN:\nClinical concept normalization in Russian. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, pages 239–245, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nAlexandr Nesterov, Galina Zubkova, Zulfat Miftahutdi-\nnov, Vladimir Kokh, Elena Tutubalina, Artem Shel-\nmanov, Anton Alekseev, Manvel Avetisian, Andrey\nChertok, and Sergey Nikolenko. 2022b. RuCCoN:\nClinical concept normalization in Russian. pages\n239–245.\nNIH. 2023. Nih umls statistics.\nKimberly O’Malley, Karon F. Cook, Matt D. Price, Kim-\nberly Raiford Wildes, John F. Hurdle, and Carol M.\nAshton. 2005. Measuring diagnoses: Icd code accu-\nracy. Health services research, 40 5 Pt 2:1620–39.\nJong-Ku Park, Ki-Soon Kim, Tae-Yong Lee, Kang-Sook\nLee, Duk-Hee Lee, Sun-Hee Lee, Sun-Ha Jee, Il Suh,\nKwang-Wook Koh, So-Yeon Ryu, et al. 2000. The\naccuracy of ICD codes for cerebrovascular diseases\nin medical insurance claims. Journal of Preventive\nMedicine and Public Health, 33(1):76–82.\nDamian Pascual, Sandro Luck, and Roger Watten-\nhofer. 2021. Towards bert-based automatic icd cod-\ning: Limitations and opportunities. arXiv preprint\narXiv:2104.06709.\nAdler Perotte, Rimma Pivovarov, Karthik Natarajan,\nNicole Weiskopf, Frank Wood, and Noémie Elhadad.\n2014. Diagnosis code assignment: models and eval-\nuation metrics. Journal of the American Medical\nInformatics Association, 21(2):231–237.\nAndrey Sakhovskiy, Natalia Semenova, Artur Kadurin,\nand Elena Tutubalina. 2024. Biomedical entity rep-\nresentation with graph-augmented multi-objective\ntransformer. In Findings of the Association for Com-\nputational Linguistics: NAACL 2024, pages 4626–\n4643, Mexico City, Mexico. Association for Compu-\ntational Linguistics.\nJunyuan Shang, Tengfei Ma, Cao Xiao, and Jimeng Sun.\n2019. Pre-training of graph augmented transformers\nfor medication recommendation. In Proceedings of\nthe Twenty-Eighth International Joint Conference on\nArtificial Intelligence, IJCAI-19, pages 5953–5959.\nInternational Joint Conferences on Artificial Intelli-\ngence Organization.\nHaoran Shi, Pengtao Xie, Zhiting Hu, Ming Zhang, and\nEric P. Xing. 2017. Towards automated ICD coding\nusing deep learning. CoRR, abs/1711.04075.\nAaron Sonabend, Winston Cai, Yuri Ahuja, Ashwin\nAnanthakrishnan, Zongqi Xia, Sheng Yu, and Chuan\nHong. 2020. Automated ICD coding via unsuper-\nvised knowledge integration (unite). International\njournal of medical informatics, 139:104135.\nJürgen Stausberg, Nils Lehmann, Dirk Kaczmarek, and\nMarkus Stein. 2008. Reliability of diagnoses cod-\ning with icd-10. International Journal of Medical\nInformatics, 77(1):50–57.\nMujeen Sung, Hwisang Jeon, Jinhyuk Lee, and Jaewoo\nKang. 2020. Biomedical entity representations with\nsynonym marginalization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3641–3650, Online. Association\nfor Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nThanh Vu, Dat Quoc Nguyen, and Anthony Nguyen.\n2020. A label attention model for icd coding from\nclinical text. arXiv preprint arXiv:2007.06351.\nPengtao Xie and Eric Xing. 2018. A neural architecture\nfor automated ICD coding. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1066–1076, Melbourne, Australia. Association for\nComputational Linguistics.\nXiancheng Xie, Yun Xiong, Philip S. Yu, and Yangyong\nZhu. 2019. Ehr coding with multi-scale feature at-\ntention and structured knowledge graph propagation.\nProceedings of the 28th ACM International Confer-\nence on Information and Knowledge Management.\nKeyang Xu, Mike Lam, Jingzhi Pang, Xin Gao, Char-\nlotte Band, Piyush Mathur, Frank Papay, Ashish K.\nKhanna, Jacek B. Cywinski, Kamal Maheshwari,\nPengtao Xie, and Eric P. Xing. 2018. Multimodal\nmachine learning for automated icd coding. ArXiv,\nabs/1810.13348.\nAlexander Yalunin, Alexander Nesterov, and Dmitriy\nUmerenkov. 2022.\nRubioroberta: a pre-trained\nbiomedical\nlanguage\nmodel\nfor\nrussian\nlan-\nguage biomedical text mining.\narXiv preprint\narXiv:2204.03951.\nZhichao Yang, Shufan Wang, Bhanu Pratap Singh\nRawat, Avijit Mitra, and Hong Yu. 2022. Knowledge\ninjected prompt based fine-tuning for multi-label few-\nshot ICD coding. arXiv preprint arXiv:2210.03304.\nZheng Yuan, Chuanqi Tan, and Songfang Huang. 2022a.\nCode synonyms do matter:\nMultiple synonyms\nmatching network for automatic ICD coding.\nIn\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 808–814.\nZheng Yuan, Zhengyun Zhao, Haixia Sun, Jiao\nLi, Fei Wang, and Sheng Yu. 2022b.\nCODER:\nknowledge-infused cross-lingual medical term em-\nbedding for term normalization. J. Biomed. Informat-\nics, 126:103983.\n13\n\n\nSheng Zhang, Hao Cheng, Jianfeng Gao, and Hoifung\nPoon. Optimizing bi-encoder for named entity recog-\nnition via contrastive learning. In The Eleventh Inter-\nnational Conference on Learning Representations.\nTianran Zhang, Muhao Chen, and Alex A T Bui. 2020a.\nDiagnostic prediction with sequence-of-sets represen-\ntation learning for clinical events. Artif. Intell. Med.\nConf. Artif. Intell. Med., 12299:348–358.\nZachariah Zhang, Jingshu Liu, and Narges Raza-\nvian. 2020b.\nBert-xml: Large scale automated\nicd coding using bert pretraining. arXiv preprint\narXiv:2006.03685.\nLingling Zhou, Cheng Cheng, Dong Ou, and Hao\nHuang. 2020. Construction of a semi-automatic icd-\n10 coding system. BMC medical informatics and\ndecision making, 20:1–12.\nTong Zhou, Pengfei Cao, Yubo Chen, Kang Liu, Jun\nZhao, Kun Niu, Weifeng Chong, and Shengping Liu.\n2021. Automatic ICD coding via interactive shared\nrepresentation networks with self-distillation mech-\nanism.\nIn Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 5948–5957.\nA\nAppendix: Annotation guidelines.\nA.1\nTask Overview\nThe task is to review the diagnoses in the BRAT\nmarkup system, categorize them into separate en-\ntities corresponding to individual nosologic units,\nand assign each of the selected entities an identifier\nin the form of an ICD code from the provided clin-\nical modification of the ICD-10-CM classifier. The\npurpose of the annotation is to assign the correct,\nmost private (to the extent possible from the limited\nanamnesis cotext) identifier to each nosologic unit\nrepresented in the diagnosis.\nA.2\nData and resources\nData. The documents you will be annotating are\nanonymized diagnoses. To facilitate and speed up\nthe annotation process, most nosologic units are\nhighlighted and pre-labeled with an ICD code.\nVocabulary. Each phrase identified in the text\nas a nosological unit or not highlighted but being\nsuch must be associated with a code from the ICD-\n10. This markup will use the clinical modification\nof the ICD-10-CM, which includes about 17762\ndifferent medical diagnoses.\nAdditional Resources. Although the markup sys-\ntem is already loaded with the ICD-10, you can\nuse the following additional resources to help you\ncorrectly identify the most appropriate ICD code:\n• The ICD Code Clinical Modification Version\n10 is a Russian-language web service for\nsearching and determining the optimal ICD\ncode, available at: www.mkb-10.com. Regis-\ntration is not required to access this resource.\n• Google - You can use Google if you are un-\nfamiliar with a clinical diagnosis or if you\nencounter a previously unknown abbreviation\nor acronym.\n• Wikipedia - You can also use Wikipedia to\nfind additional information.\nA.3\nTask Description\nFor each selected or unselected piece of text cor-\nresponding to a nosological unit, you need to as-\nsign an ICD code. Example: “Atopic dermatitis\nin partial remission disseminated form”. The se-\nlected text fragment “Atopic dermatitis in partial\nremission” should be associated with the diagnosis\n\"Other atopic dermatitis\" (L20.8). Make sure that\nno text fragment representing a nosological unit is\nleft without an assigned ICD code, thus ensuring\nthe completeness of the markup.\nHowever, each nosologic unit should correspond\nto only one code. However, in many cases, the se-\nlected nosologic units may correspond to more than\none ICD code, in which case you should follow the\nfollowing rules:\n• 1. Select an ICD code that maximizes the\nspecificity of the diagnosis up to subsection\nX.00.\n• 2. If the nosological unit includes modifiers\nsuch as “mild”, “severe”, “acute”, “chronic”,\nindication of degree, stage, etc., the modifier\nshould be taken into account when searching\nfor the appropriate ICD code. However, it\nis often the case that the classifier will only\nhave a more general diagnosis that does not\ninclude the above modifier. In this case, se-\nlect the optimal ICD code by ignoring the\nmodifier. However, modifiers that are insepa-\nrable in meaning from the underlying concept\nshould always be considered when selecting\nthe optimal ICD code (e.g., “Acute myocardial\nischemia”).\nThe following rules should also be followed\nwhen marking up:\n• If the selected nosological unit is written in\nthe plural and the corresponding ICD code\nexists in the classifier in the plural, you should\nselect it. Otherwise, you should search for the\nICD code in the singular.\n14\n\n\n• Sometimes in the classifier there are diagnoses\nthat at first glance seem to be absolutely iden-\ntical, which can be differentiated only by the\ncontext of the electronic medical record.\nA.4\nAnnotation Tool\nThe\nannotation\nprocess\nis\nconducted\nus-\ning\na\nspecialized\nweb\nservice\ncalled\nbrat\n(https://brat.nlplab.org/).\nYou will be provided\nwith a customized login and password.\nAll\nnecessary information from clinical diagnoses and\npreliminary markup with ICD codes are entered\ninto the annotation tool. Each document in the brat\nweb service leads to a separate clinical diagnosis.\nEach selected text fragment is a nosological unit\nto be associated with the corresponding ICD code.\nIn order to call the ICD code selection menu, you\nneed to highlight the section of text you are go-\ning to mark up or double-click on the green label\n“icd_code” located above the selected text frag-\nment. If you think that a section of the diagnosis\nis selected incorrectly or redundantly, you need to\ncorrect or delete the corresponding selection.\nThe window may or may not have a pre-selected\nICD code on the Ref line. If specified, compare\nthe correctness of the ICD code specified in the\n“Ref” line with the selected text fragment specified\nin the “Text” field. If the ICD code is correct, press\nthe “OK” button and move to the next selected text\nfragment. If the ICD-code is not specified or is\nspecified incorrectly, double-click the “Ref” line\nin the “Normalization” field, and the ICD-code\nsearch window will open. In the opened window\ncheck the correctness of the diagnosis selection for\nsearch in the “Query” line and click on the “Search\nICD_codes” button. The system will search in the\nICD codes classifier and list them. If the system\ndoes not find the codes by the specified text frag-\nment, try to change it.\nSelect the appropriate ICD code and its decoding\nfrom the list and press the “OK” button (or double-\nclick on the required ICD code). The system will\nsave your selection and return to the previous win-\ndow, where you should also click on the “OK” but-\nton. The system will remember your selection and\nyou can proceed to annotate the next selected text\nsection.\nIf you did not find a suitable ICD code in the\nlist of ICD codes found by the system, you can try\nto change the search phrase in the “Query” field,\nby which the search is performed, and perform the\nsearch again. In most cases, the correct selection\nof the search phrase allows one to find the most\nappropriate ICD code in the classifier.\nIf the built-in search system does not yield re-\nsults, you can switch to the external directory of\nICD codes specified in A2. To do this, click on\nthe magnifying glass icon in the “Normalization”\nfield. You can also go to the Google search engine\nand Wikipedia web encyclopedia by clicking on\nthe corresponding link in the “Search” field.\nIf even after changing the search phrase and\nsearching in external resources you cannot find\na suitable ICD code, return to the previous menu\nby clicking on the “cancel” button and delete the\nidentifier located in the ID line in the “Normaliza-\ntion” field in the opened window. The same should\nbe done if a text section that is not a nosological\nunit is selected. Deleting the identifier will clear\nthe “Ref” line; this will serve as an indicator that\nthe selected text fragment could not be matched\nwith a suitable ICD code.\nB\nRelated Work\nICD coding\nICD coding has traditionally relied\non established machine learning techniques. Early\napproaches employed methods such as Support\nVector Machines (SVM) with TF-IDF features to\nrepresent clinical notes (Perotte et al., 2014). Fea-\nture engineering, including gradient boosting for\nlarge datasets, also played a significant role in en-\nhancing ICD coding accuracy (Diao et al., 2021).\nRegular expression-based mapping and adaptive\ndata processing further improved efficiency in spe-\ncific healthcare settings (Zhou et al., 2020).\nThe advent of neural networks marked a\nparadigm shift in ICD coding. Recurrent Neural\nNetworks (RNNs), including LSTMs and GRUs,\nwere utilized to encode EHR data and capture\ntemporal dependencies within clinical notes (Choi\net al., 2016; Baumel et al., 2018). Convolutional\nNeural Networks (CNNs) offered alternative archi-\ntectures for extracting features from clinical text,\nwith models like CAML demonstrating their effec-\ntiveness (Mullenbach et al., 2018b). Subsequent\nadvancements introduced multi-filter CNNs (Li and\nYu, 2020) and squeeze-and-excitation networks in\nCNN (Liu et al., 2021c) to enhance feature extrac-\ntion. Addressing the challenge of imbalanced code\ndistribution, researchers introduced focal loss (Liu\net al., 2021c) and self-distillation mechanisms to\nimprove prediction accuracy for rare codes (Zhou\net al., 2021). Other models, like HA-GRUs used the\n15\n\n\ncharachter-level information (Baumel et al., 2018).\nEnsemble models used CNN, LSTM, and decision\ntrees to improve accuracy (Xu et al., 2018).\nA crucial line of research has focused on inte-\ngrating external medical knowledge and the in-\nherent hierarchical structure of ICD codes. Ap-\nproaches have incorporated medical definitions\n(Shi et al., 2017), Wikipedia data for rare dis-\neases (Bai and Vucetic, 2019) and medical ontolo-\ngies (Bao et al., 2021) to enrich term embeddings.\nTree-of-sequences LSTMs (Xie and Xing, 2018)\nand graph neural networks (Cao et al., 2020; Xie\net al., 2019) were developed to capture relation-\nships between codes, either through hierarchical\nstructures or co-occurrence patterns. Models like\nKG-MultiResCNN leveraged external knowledge\nfor relations understanding (Boukhers et al., 2023).\nWeak supervision was used to overcome the lack of\ntraining data (Dong et al., 2021; Gao et al., 2022).\nFurthermore, domain-specific pre-trained language\nmodels (PLMs) such as BioBERT (Lee et al., 2019),\nClinicalBERT (Alsentzer et al., 2019), and Pub-\nMedBERT (Gu et al., 2021) have shown promise\nin improving performance on various biomedical\ntasks. However, adapting these models to the large-\nscale, multi-label nature of ICD coding presents\nunique challenges, particularly regarding long in-\nput sequences (Pascual et al., 2021; Ji et al., 2021).\nRecent efforts, such as BERT-XML (Zhang et al.,\n2020b), have addressed this through input splitting\nand label attention mechanisms. Read, Attend, and\nCode (RAC) was proposed by Kim and Ganapathi\n(Kim and Ganapathi, 2021) and achieved state-of-\nthe-art results. Despite these developments, chal-\nlenges remain in handling semi-structured text and\nvariability of notes (Lu et al., 2023).\nRecent studies have increasingly focused on\nleveraging attention mechanisms and improving\nthe interaction between clinical note representa-\ntions and ICD code representations. Models such\nas LAAT (Vu et al., 2020) and EffectiveCAN (Liu\net al., 2021c) have incorporated refined label-aware\nattention mechanisms. However, the effective ap-\nplication of PLMs to ICD coding requires careful\nconsideration of input length constraints and the\ndevelopment of robust mechanisms for capturing\nlong-range dependencies. Also, the models need to\nbetter understand relationships between different\nsections of clinical notes (Lu et al., 2023).\nDiagnosis prediction\nDiagnosis prediction us-\ning structured EHR data has been extensively stud-\nied with deep learning approaches. NECHO (Koo,\n2024) improves next-visit diagnosis prediction by\ncentering learning on medical codes and incorporat-\ning hierarchical regularization to capture structured\ndependencies in EHR data. DPSS (Zhang et al.,\n2020a) enhances predictive robustness by modeling\npatient records as sequences of unordered clinical\nevents, preserving temporal patterns while mitigat-\ning biases introduced by the artificial ordering of\nmedical records. The importance of patient history\nin EHR-based diagnosis prediction demonstrates\nthat historical records alone can achieve 76.6% ac-\ncuracy, which increases to 93.3% when structured\nphysical examination and laboratory data are inte-\ngrated (Fukuzawa et al., 2024). At the population\nlevel, applying a Bi-GRU model trained on struc-\ntured EHR data with SNOMED embeddings to pre-\ndict chronic disease onset demonstrates the utility\nof structured clinical histories in early disease iden-\ntification (Grout et al., 2024). To optimize the use\nof structured medical codes for diagnosis predic-\ntion, MERA (Ma et al., 2025) introduces hierarchi-\ncal contrastive learning and ranking mechanisms\nto refine diagnosis classification within large ICD\ncode spaces. These studies collectively illustrate\nthe evolution of EHR-based diagnosis prediction\nfrom sequence modeling to hierarchical represen-\ntation learning, highlighting the role of structured\nclinical history in improving predictive accuracy.\nRAG\nLLMs face challenges as standalone sys-\ntems for high-precision tasks such as ICD-linking,\nprimarily due to their limited accuracy in extract-\ning detailed, domain-specific information.\nMa\net al.(Ma et al., 2023) demonstrated that while\nLLMs lag behind fine-tuned SLMs in informa-\ntion extraction tasks, they excel in understand-\ning and reorganizing semantic content, making\nthem effective at reranking retrieved information.\nTo overcome the limitations of accuracy and do-\nmain specificity, recent approaches have incorpo-\nrated Retrieval-Augmented Generation (RAG) tech-\nniques. RAG combines the structured knowledge\nof external databases for retrieval with the semantic\nreasoning strengths of LLMs for reranking, result-\ning in improved precision and overall task perfor-\nmance.\nKlang et al. (Klang et al., 2024) demonstrated\nthe effectiveness of RAG in enhancing LLMs\nfor ICD-10-CM medical coding. Their study re-\nvealed that RAG-enhanced LLMs outperform hu-\nman coders in accuracy and specificity, emphasiz-\n16\n\n\ning the potential of retrieval mechanisms in im-\nproving clinical documentation. Similarly, Kwan\n(Kwan, 2024) proposed a two-stage Retrieve-Rank\nsystem for medical coding, achieving a perfect\nmatch rate for ICD-10-CM codes and significantly\nsurpassing vanilla LLMs. The MedCodER frame-\nwork (Baksi et al., 2024) leverages a pipeline of\nextraction, retrieval, and reranking, to improve au-\ntomation and interpretability in ICD-10 coding. It\ndemonstrates SOTA performance on ACI-BENCH\nby integrating LLMs with semantic search and\nevidence-based reasoning. Boyle et al. (Boyle\net al., 2023) presented a zero-shot ICD coding\napproach using LLMs and a tree-search strategy,\nachieving a SOTA on the CodiEsp dataset, par-\nticularly excelling in rare code prediction without\ntask-specific training. Abdulnazar et al. (Abdul-\nnazar et al., 2024) applied GPT-4 for clinical text\ncleansing to enhance MCN. By combining text\nstandardization with RAG, their method improved\nmapping precision to SNOMED CT in the German\nlanguage.\nC\nBERT-based NER Results\nTab. 5 presents evaluation results for NER task on\nthe RuCCoD dataset. In the context of NER, Ru-\nBioBERT employs a softmax activation function\nin its output layer. BINDER utilizes RuBioBERT\nbackbone and approaches NER as a representa-\ntion learning problem by maximizing the similar-\nity between the vector representations of an en-\ntity mention and its corresponding type (Zhang\net al.). RuBioBERT achieves the highest F1-score\nof 0.756 when trained on the RuCCoD, suggest-\ning that this dataset is particularly effective for the\nmodel. BINDER trained on RuCCoD achieves an\nF1-score of 0.71, slightly lower than RuBioBERT\ntrained on the same dataset.\nD\nEntity Linking Results\nSince there are many datasets for entity linking\nin the biomedical domain, including corpora in\nRussian, we explored whether these corpora can\nbe helpful for ICD coding. Additionally, we at-\ntempted to enrich the ICD normalization vocabu-\nlary with concept names from the Unified Medical\nLanguage System (UMLS) metathesaurus which\nincludes the ICD-10 vocabulary. Specifically, for\neach ICD code, we find its Concept Unique Identi-\nfier (CUI) in UMLS and retrieve all concept names\nthat share the same CUI but are adopted from the\nsource vocabularies different from ICD-10. We\nemploy the following Russian biomedical corpora\nfor experiments on cross-terminology transfer:\nRuCCoN (Nesterov et al., 2022a) is a manu-\nally annotated corpus of clinical records in Rus-\nsian. It contains 16,028 mentions linked to 2,409\nunique concepts from the Russian subset of UMLS\nmetathesaurus (Bodenreider, 2004).\nNEREL-BIO (Loukachevitch et al., 2023, 2024)\nis a corpus of 756 PubMed abstracts in Russian\nmanually linked to 4,544 unique UMLS concepts.\nThe corpus is specifically focused on two main\nproblems: (i) entity nestedness and (ii) cross-\nlingual Russian-to-English normalization for the\nincomplete Russian UMLS terminology. In to-\ntal, NEREL-BIO provides 23,641 entity mentions\nmanually linked to 4,544 unique UMLS concepts.\n4,424 mentions have no concept name representa-\ntion in the Russian UMLS subset and are linked\nto 1,535 unique concepts present in the English\nUMLS only.\nWe experiment with three state-of-the-art spe-\ncialized biomedical entity linking models:\nSapBERT is a metric learning framework that\nlearns from synonymous UMLS concept names by\ngenerating hard triplets for pre-training (Liu et al.,\n2021a,b).\nCODER is a contrastive learning model inspired\nby semantic matching methods that use both syn-\nonyms and relations from the UMLS (Yuan et al.,\n2022b) to learn concept representations.\nBERGAMOT is an extension of SapBERT which\nlearns concept name-based and graph-based con-\ncept representations simultaneously and introduces\na cross-modal alignment loss to transfer knowledge\nfrom a graph encoder to a BERT-based language\nencoder (Sakhovskiy et al., 2024). The graph en-\ncoder is discarded after the pretraining stage and\nonly a BERT encoder is used for inference.\nFor\nsupervised\nentity\nlinking,\nwe\nadopt\nBioSyn (Sung et al., 2020), a BERT-based frame-\nwork that iteratively updates entity representations\nusing synonym marginalization. For each dataset,\nwe trained BioSyn with default hyperparameters\nfor 20 epochs.\nRelaxed EL Evaluation\nWe assess two entity\nlinking set-ups: (i) strict evaluation which implies\nan exact match between predicted and ground truth\ncodes and (ii) relaxed evaluation with all codes\nbeing truncated to 3-symbols codes (corresponding\nto the second level of hierarchy).\n17\n\n\nTask\nModel or Approach\nLR\n# Epochs\nBS\nScheduler\nWD\nNER\nRuBioBERT\n1e-5\n20\n32\nCosine (Loshchilov and Hutter, 2017)\n0.01\nEL\nBERGAMOT+BioSyn\n2e-5\n20\n32\nAdam (Kingma and Ba, 2015)\n0.01\nLLM tuning\nLoRA\n5e-5\n33\n2\nLinear with Warmup\n0.01\nICD code prediction\nLongformer\n5e-5\n2\n4\nLinear with Warmup\n0.01\nTable 4: Models and training hyperparameters. LR stands for learning rate, BS for batch size, WD for weight decay\nModel\nTrain Data\nF1-score\nPrecision\nRecall\nRuBioBERT\nRuCCoD train\n0.756\n0.75\n0.77\nRuBioBERT\nBIO-NNE train\n0.62\n0.57\n0.67\nRuBioBERT\nRuCCoD + BioNNE train\n0.72\n0.75\n0.70\nBINDER + RuBioBERT\nRuCCoD train\n0.71\n0.72\n0.71\nTable 5: Evaluation results for NER task on RuCCoD dataset.\nTrain set\nSapBERT\nCODER\nBERGAMOT\n@1\n@5\n@1\n@5\n@1\n@5\nZero-shot evaluation, strict\nICD dict\n0.3327\n0.5712\n0.2631\n0.4687\n0.3495\n0.6170\nICD dict+UMLS synonyms\n0.3546\n0.5197\n0.3237\n0.4765\n0.3559\n0.5487\nSupervised evaluation, strict\nICD\n0.6132\n0.8182\n0.6202\n0.8169\n0.6415\n0.8459\nICD+UMLS sumonyms\n0.5326\n0.7382\n0.5358\n0.7318\n0.4984\n0.7253\nRuCCoN\n0.3591\n0.5345\n0.3598\n0.5732\n0.3643\n0.5313\nRuCCoN+ICD\n0.3952\n0.5732\n0.3888\n0.6570\n0.3817\n0.5983\nNEREL-BIO\n0.3443\n0.4913\n0.3378\n0.5274\n0.3353\n0.5113\nNEREL-BIO+ICD\n0.3804\n0.5596\n0.3804\n0.6325\n0.3598\n0.5525\nZero-shot evaluation, relaxed\nICD dict\n0.4842\n0.6886\n0.3752\n0.6190\n0.5035\n0.7286\nICD dict+UMLS synonyms\n0.5551\n0.6867\n0.5055\n0.6293\n0.5603\n0.7073\nSupervised evaluation, relaxed\nICD\n0.7763\n0.8839\n0.7872\n0.8743\n0.7917\n0.8943\nICD+UMLS sumonyms\n0.7788\n0.8616\n0.7714\n0.8860\n0.7449\n0.8738\nRuCCoN\n0.5235\n0.6531\n0.5429\n0.7208\n0.5132\n0.6564\nRuCCoN+ICD\n0.5493\n0.6602\n0.5770\n0.7485\n0.5571\n0.6873\nNEREL-BIO\n0.4803\n0.6067\n0.4958\n0.6634\n0.4778\n0.6170\nNEREL-BIO+ICD\n0.5455\n0.6447\n0.5474\n0.7292\n0.5384\n0.6505\nTable 6: Cross-domain transfer results for biomedical linking models. Evaluation results for linking models trained\non RuCOD, RuCCoN, NEREL-BIO as well as their union. ICD+UMLS synonyms stands for ICD train set with the\nvocabulary enriched with ICD disease name synonyms from the UMLS knowledge base. The best results for each\nmodel and set-up are highlighted in bold.\nThe results of cross-terminology entity linking\ntransfer presented in Tab. 8 reveal a few insightful\nfindings related to linking ICD codes.\nVocabulary Extension is not a Cure\nWhile ex-\ntension of ICD vocabulary consistently gives a\nslightly improved Accuracy@1 in a zero-shot set-\nting, additional synonyms introduce severe noise in\n18\n\n\na supervised setting. Specifically, a significant drop\nof 8.1%, 8.4%, 14.3% Accuracy@1 is observed\nfor SapBERT, CODER, and BERGAMOT, respec-\ntively. Even in an unsupervised setting, vocabulary\nextension drops Accuracy@5 by 5.2% and 6.8%\nfor SapBERT and BERGAMOT, respectively.\nComplicated\nCross-Terminology\nTransfer\nBoth training on RuCCoN and NEREL-BIO as\nwell merge of these corpora with RuCCoD do\nnot lead to improvement over zero-shot coding.\nThe finding indicates the specificity and high\ncomplexity of ICD coding within the entity linking\ntask.\nComplexity of Fine-Grained ICD coding\nThe\nhigh gap between the strict and supervised evalu-\nation of around 15% Accuracy@1 indicates that\ndistinguishing between semantically similar dis-\neases sharing the same therapeutic group is a major\nchallenge.\nE\nLLM with RAG results\nAll LLM with RAG experiments were conducted\nwith a temperature setting of 0 for all LLMs and\na top-k value of 15 for the number of retrieved en-\ntities from similarity search. The LLMs used are\nspecified in Appx. G. For the embedding model,\nwe utilized BERGAMOT. To construct the vec-\ntor database, we used dictionaries extracted from\nNEREL-BIO, RUCCON, the ICD dictionary, and\nthe ICD dictionary combined with RuCCoD. The\nresults are presented in Tables 9 and 10 for strict\nevaluation, and in Tables 11 and 12 for relaxed\nevaluation.\nFor the NER task, the ICD dict.+RuCCoD\ndataset yielded the best results. The Llama3.1:8b-\ninstruct-fp16 model achieved the highest F-score\n(0.511), precision (0.580), recall (0.456), and ac-\ncuracy (0.343). Qwen2.5-7B-Instruct and Llama3-\nMed42-8B followed with F-scores of 0.495 and\n0.491, respectively. In contrast, NEREL-BIO and\nRUCCON datasets showed significantly lower per-\nformance, with F-scores below 0.13 and accuracies\nunder 0.07.\nFor NER+ICD Linking, the same dataset and\nmodel led again, with Llama3.1:8b-instruct-fp16\nachieving an F-score of 0.268 and accuracy of\n0.155. Qwen2.5-7B-Instruct and Llama3-Med42-\n8B followed closely with F-scores around 0.245.\nPerformance on NEREL-BIO and RuCCon was\nmuch lower, with F-scores under 0.022 and accura-\ncies below 0.011.\nFor ICD Code assignment, Llama3.1:8b-instruct-\nfp16 also performed best, with an F-score of 0.458\nand accuracy of 0.297. Qwen2.5-7B-Instruct and\nLlama3-Med42-8B also performed well, with F-\nscores of 0.463 and 0.457. Again, NEREL-BIO and\nRUCCON datasets exhibited weaker results, with\nF-scores below 0.15 and accuracies under 0.09.\nIn summary, the ICD dict.+RuCCoD dataset con-\nsistently outperformed others with Llama3.1:8b-\ninstruct-fp16 being the best model. Relaxed evalu-\nation settings produced similar trends.\nF\nLLM with tuning results\nThe LLM tuning results are in Tab. 7.\nFor the NER task, Llama3-Med42-8B achieved\nthe highest F-score of 0.642, which corresponds to\nthe highest Precision and Recall among the mod-\nels. Phi3_5_mini and Mistral-Nemo demonstrated\nsimilar performance (F-scores of 0.627 and 0.614,\nrespectively), but slightly lag behind the leader.\nThe Qwen2.5-7B-Instruct model showed the low-\nest scores across all metrics, with an F-score of\n0.565 and an Accuracy of 0.393.\nIn the NER + ICD linking task, the use of\nthe RuCCoD or BERGAMOT approach signifi-\ncantly improved the linking performance. For in-\nstance, Phi3_5_mini achieved the highest F-score\nof 0.333 when using RuCCoD, and Llama3-Med42-\n8B reached an F-score of 0.299. Notably, for all\nmodels, the use of RuCCoD proved to be more\nbeneficial than the BERGAMOT approach.\nIn the ICD code assignment task, results also\nimproved significantly with the use of the RuC-\nCoD dataset. Once again, Phi3_5_mini emerged\nas the top-performing model, attaining an F-score\nof 0.480 when using RuCCoD. Llama3-Med42-\n8B and Mistral-Nemo also demonstrated strong\nresults, with F-scores of 0.435 and 0.446, respec-\ntively, when using RuCCoD. It is noteworthy that\nthe inclusion of RuCCoD consistently improved\nPrecision and Recall across all models.\nBased on the presented results, it can be con-\ncluded that for all tasks (NER, NER+Linking, and\nICD code assignment), the use of RuCCoD sig-\nnificantly enhances model performance compared\nto relying solely on the dictionary or embeddings.\nThe top-performing models across all tasks are\nLlama3-Med42-8B and Phi3_5_mini, indicating\ntheir high efficiency in medical tasks following\nPEFT tuning.\n19\n\n\nModel\nPrecision\nRecall\nF-score\nAccuracy\nNER\nLlama3-Med42-8B, RuCCoD\n0.642\n0.642\n0.642\n0.473\nQwen2.5-7B-Instruct, RuCCoD\n0.567\n0.562\n0.565\n0.393\nPhi3_5_mini, RuCCoD\n0.632\n0.623\n0.627\n0.457\nMistral-Nemo, RuCCoD\n0.631\n0.598\n0.614\n0.443\nNER+Linking\nLlama3-Med42-8B, ICD dict.\n0.149\n0.149\n0.149\n0.08\nLlama3-Med42-8B, ICD dict. + RuCCoD\n0.299\n0.299\n0.299\n0.176\nLlama3-Med42-8B, ICD dict. + BERGAMOT\n0.286\n0.286\n0.286\n0.167\nQwen2.5-7B-Instruct, ICD dict.\n0.188\n0.186\n0.187\n0.103\nQwen2.5-7B-Instruct, ICD dict. + RuCCoD\n0.281\n0.279\n0.28\n0.163\nQwen2.5-7B-Instruct, ICD dict. + BERGAMOT\n0.2\n0.198\n0.199\n0.11\nPhi3_5_mini, ICD dict.\n0.272\n0.268\n0.27\n0.156\nPhi3_5_mini, ICD dict. + RuCCoD\n0.335\n0.33\n0.333\n0.199\nPhi3_5_mini, ICD dict. + BERGAMOT\n0.322\n0.317\n0.32\n0.19\nMistral-Nemo, ICD dict.\n0.231\n0.219\n0.224\n0.126\nMistral-Nemo, ICD dict. + RuCCoD\n0.303\n0.287\n0.295\n0.173\nMistral-Nemo, ICD dict. + BERGAMOT\n0.267\n0.253\n0.26\n0.149\nCode assignment\nLlama3-Med42-8B, ICD dict.\n0.229\n0.231\n0.23\n0.13\nLlama3-Med42-8B, ICD dict. + RuCCoD\n0.434\n0.435\n0.435\n0.278\nLlama3-Med42-8B, ICD dict. + BERGAMOT\n0.403\n0.405\n0.404\n0.253\nQwen2.5-7B-Instruct, ICD dict.\n0.296\n0.295\n0.295\n0.173\nQwen2.5-7B-Instruct, ICD dict. + RuCCoD\n0.456\n0.449\n0.452\n0.292\nQwen2.5-7B-Instruct, ICD dict. + BERGAMOT\n0.305\n0.303\n0.304\n0.179\nPhi3_5_mini, ICD dict.\n0.394\n0.39\n0.392\n0.244\nPhi3_5_mini, ICD dict. + RuCCoD\n0.483\n0.477\n0.48\n0.316\nPhi3_5_mini, ICD dict. + BERGAMOT\n0.454\n0.448\n0.451\n0.291\nMistral-Nemo, ICD dict.\n0.326\n0.311\n0.319\n0.189\nMistral-Nemo, ICD dict. + RuCCoD\n0.458\n0.435\n0.446\n0.287\nMistral-Nemo, ICD dict. + BERGAMOT\n0.394\n0.372\n0.383\n0.237\nTable 7: ICD coding results for finetuned LLMs on RuCCoD. The best results are highlighted in bold.\nG\nImplementation Details\nUtilized LLMs:\n• Phi-3.5-mini-instruct (Phi)\n• Qwen2.5-7B-Instruct (Qwe)\n• Llama3-Med42-8B (Med)\n• Mistral-Nemo-Instruct-2407 (Mis)\n• llama3.1:8b-instruct-fp16 (Lla)\nDiagnosis prediction\nEach Longformer was\ntrained for two epochs on separate NVidia A100\nGPUs, with the fine-tuning process taking approxi-\nmately one week per model. We provide hyperpa-\nrameters for these models training in Tab. 4.\nHyperparameters\nA detailed overview, includ-\ning parameter values and configurations, is pro-\nvided in Tab. 4.\nH\nPrompts\nThe original prompts were in Russian. Below are\ntheir translations to English.\n20\n\n\nModel\nPrecision\nRecall\nF-score\nAccuracy\nNER\nBioBERT, Biosyn, RuCCoD\n0.649\n0.655\n0.653\n0.485\nBioBERT, RuCCoD\n0.721\n0.769\n0.744\n0.592\nBioBERT, NEREL-BIO\n0.588\n0.675\n0.628\n0.458\nBioBERT, NEREL-BIO, RuCCoD\n0.689\n0.713\n0.701\n0.54\nBioBERT, RuCCoN\n0.637\n0.613\n0.625\n0.454\nBioBERT, RuCCoN + RuCCoD\n0.609\n0.709\n0.655\n0.487\nNER+Linking\nBioBERT, Biosyn, RuCCoD\n0.392\n0.396\n0.394\n0.245\nBioBERT, RuCCoD\n0.427\n0.455\n0.441\n0.283\nBioBERT, NEREL-BIO\n0.353\n0.406\n0.377\n0.233\nBioBERT, NEREL-BIO, RuCCoD\n0.406\n0.42\n0.413\n0.26\nBioBERT, RuCCoN\n0.387\n0.372\n0.379\n0.234\nBioBERT, RuCCoN + RuCCoD\n0.351\n0.409\n0.378\n0.233\nCode assignment\nBioBERT, Biosyn, RuCCoD\n0.507\n0.508\n0.507\n0.340\nBioBERT, RuCCoD\n0.51\n0.542\n0.525\n0.356\nBioBERT, NEREL-BIO\n0.466\n0.531\n0.497\n0.33\nBioBERT, NEREL-BIO, RuCCoD\n0.512\n0.529\n0.52\n0.352\nBioBERT, RuCCoN\n0.508\n0.485\n0.496\n0.33\nBioBERT, RuCCoN + RuCCoD\n0.471\n0.543\n0.504\n0.337\nTable 8: Evaluation results for entity-level tasks for BERT-based IE pipeline on RuCCoD corpus. The best results\nare highlighted in bold.\nNER prompt\nYou will be provided with a text\ncontaining diagnoses. Extract the\ndiagnoses from this text. Do not\nalter the spelling of the\ndiagnoses in the text. Respond\nonly in the format of a list: [’\ndiagnosis1 ’, ’diagnosis2 ’, ...]\nText: {text}\nDiagnosis selection prompt\nYou will be given a reference\ndiagnosis and a list of diagnoses\nfrom a database.\nYour task is to determine which\ndiagnosis from the database best\nmatches the reference diagnosis.\nTry to select the diagnosis\naccurately , paying attention to\ndetails. Choose the diagnosis with\nthe highest match in terms of\nwords and meaning.\nYou can only choose from the\ndiagnoses in the list.\nPay more attention to the\ndiagnoses at the beginning of the\nlist , as they are more likely to\nbe a better match.\nIt ’s better to choose a shorter\ndiagnosis than one that includes\ninformation not present in the\nreference diagnosis.\nIn your response , write only the\ndiagnosis number and nothing else.\nReference diagnosis: {diagnosis}\nList of diagnoses from a database:\n{list}\n21\n\n\nModel\nPrecision\nRecall\nF-score\nAccuracy\nNER: ICD dict.\nLlama3.1:8b-instruct\n0.208\n0.088\n0.124\n0.066\nLlama3-Med42-8B\n0.202\n0.084\n0.118\n0.063\nPhi-3.5-mini-instruct\n0.211\n0.093\n0.129\n0.069\nMistral-Nemo-Instruct-2407\n0.198\n0.072\n0.105\n0.055\nQwen2.5-7B-Instruct\n0.206\n0.087\n0.122\n0.065\nNER: ICD dict. + RuCCoD\nLlama3.1:8b-instruct\n0.581\n0.456\n0.511\n0.343\nLlama3-Med42-8B\n0.556\n0.441\n0.492\n0.326\nPhi-3.5-mini-instruct\n0.543\n0.450\n0.492\n0.326\nMistral-Nemo-Instruct-2407\n0.541\n0.372\n0.441\n0.283\nQwen2.5-7B-Instruct\n0.566\n0.440\n0.495\n0.329\nNER+Linking: ICD dict.\nLlama3.1:8b-instruct\n0.071\n0.067\n0.069\n0.036\nLlama3-Med42-8B\n0.058\n0.063\n0.060\n0.031\nPhi-3.5-mini-instruct\n0.062\n0.069\n0.065\n0.034\nMistral-Nemo-Instruct-2407\n0.066\n0.056\n0.060\n0.031\nQwen2.5-7B-Instruct\n0.065\n0.065\n0.065\n0.033\nNER+Linking: ICD dict. + RuCCoD\nLlama3.1:8b-instruct\n0.272\n0.264\n0.268\n0.155\nLlama3-Med42-8B\n0.235\n0.261\n0.247\n0.141\nPhi-3.5-mini-instruct\n0.228\n0.257\n0.242\n0.137\nMistral-Nemo-Instruct-2407\n0.247\n0.215\n0.230\n0.130\nQwen2.5-7B-Instruct\n0.244\n0.246\n0.245\n0.140\nCode assignment: ICD dict.\nLlama3.1:8b-instruct\n0.379\n0.363\n0.371\n0.228\nLlama3-Med42-8B\n0.310\n0.345\n0.327\n0.195\nPhi-3.5-mini-instruct\n0.260\n0.294\n0.276\n0.160\nMistral-Nemo-Instruct-2407\n0.413\n0.360\n0.385\n0.238\nQwen2.5-7B-Instruct\n0.401\n0.411\n0.406\n0.255\nCode assignment: ICD dict. + RuCCoD\nLlama3.1:8b-instruct\n0.465\n0.451\n0.458\n0.297\nLlama3-Med42-8B\n0.434\n0.483\n0.457\n0.296\nPhi-3.5-mini-instruct\n0.409\n0.458\n0.432\n0.276\nMistral-Nemo-Instruct-2407\n0.462\n0.401\n0.429\n0.273\nQwen2.5-7B-Instruct\n0.461\n0.465\n0.463\n0.301\nTable 9: Evaluation results for NER, Code assignment, and end-to-end entity linking task on RuCCoD for\nLLM+RAG pipeline.\n22\n\n\nModel\nPrecision\nRecall\nF-score\nAccuracy\nNER: NEREL-BIO\nLlama3.1:8b-instruct\n0.100\n0.042\n0.059\n0.030\nLlama3-Med42-8B\n0.104\n0.043\n0.060\n0.031\nPhi-3.5-mini-instruct\n0.098\n0.043\n0.059\n0.031\nMistral-Nemo-Instruct-2407\n0.115\n0.044\n0.063\n0.033\nQwen2.5-7B-Instruct\n0.099\n0.043\n0.060\n0.031\nNER: RuCCoN\nLlama3.1:8b-instruct\n0.188\n0.088\n0.120\n0.064\nLlama3-Med42-8B\n0.174\n0.079\n0.108\n0.057\nPhi-3.5-mini-instruct\n0.172\n0.085\n0.114\n0.060\nMistral-Nemo-Instruct-2407\n0.197\n0.082\n0.116\n0.061\nQwen2.5-7B-Instruct\n0.185\n0.091\n0.122\n0.065\nNER+Linking: NEREL-BIO\nLlama3.1:8b-instruct\n0.023\n0.020\n0.021\n0.011\nLlama3-Med42-8B\n0.018\n0.019\n0.018\n0.009\nPhi-3.5-mini-instruct\n0.019\n0.020\n0.019\n0.010\nMistral-Nemo-Instruct-2407\n0.025\n0.020\n0.022\n0.011\nQwen2.5-7B-Instruct\n0.021\n0.020\n0.020\n0.010\nNER+Linking: RuCCoN\nLlama3.1:8b-instruct\n0.050\n0.046\n0.048\n0.025\nLlama3-Med42-8B\n0.042\n0.044\n0.043\n0.022\nPhi-3.5-mini-instruct\n0.038\n0.041\n0.040\n0.020\nMistral-Nemo-Instruct-2407\n0.053\n0.044\n0.048\n0.025\nQwen2.5-7B-Instruct\n0.048\n0.046\n0.047\n0.024\nCode assignment: NEREL-BIO\nLlama3.1:8b-instruct\n0.059\n0.053\n0.056\n0.029\nLlama3-Med42-8B\n0.045\n0.047\n0.046\n0.024\nPhi-3.5-mini-instruct\n0.046\n0.049\n0.047\n0.024\nMistral-Nemo-Instruct-2407\n0.062\n0.051\n0.056\n0.029\nQwen2.5-7B-Instruct\n0.058\n0.056\n0.057\n0.029\nCode assignment: RuCCoN\nLlama3.1:8b-instruct\n0.164\n0.150\n0.157\n0.085\nLlama3-Med42-8B\n0.125\n0.131\n0.128\n0.068\nPhi-3.5-mini-instruct\n0.125\n0.134\n0.129\n0.069\nMistral-Nemo-Instruct-2407\n0.156\n0.129\n0.141\n0.076\nQwen2.5-7B-Instruct\n0.156\n0.152\n0.154\n0.084\nTable 10: Evaluation results for NER, Code assignment, and end-to-end entity linking task on RuCCoD for\nLLM+RAG pipeline using NEREL-BIO and RuCCoN for vectorstore.\n23\n\n\nModel\nPrecision\nRecall\nF-score\nAccuracy\nNER: ICD dict.\nLlama3.1:8b-instruct\n0.208\n0.088\n0.124\n0.066\nLlama3-Med42-8B\n0.202\n0.084\n0.118\n0.063\nPhi-3.5-mini-instruct\n0.211\n0.093\n0.129\n0.069\nMistral-Nemo-Instruct-2407\n0.198\n0.072\n0.105\n0.055\nQwen2.5-7B-Instruct\n0.206\n0.087\n0.122\n0.065\nNER: ICD dict. + RuCCoD\nLlama3.1:8b-instruct\n0.581\n0.456\n0.511\n0.343\nLlama3-Med42-8B\n0.556\n0.441\n0.492\n0.326\nPhi-3.5-mini-instruct\n0.543\n0.450\n0.492\n0.326\nMistral-Nemo-Instruct-2407\n0.541\n0.372\n0.441\n0.283\nQwen2.5-7B-Instruct\n0.566\n0.440\n0.495\n0.329\nNER+Linking: ICD dict.\nLlama3.1:8b-instruct\n0.095\n0.088\n0.091\n0.048\nLlama3-Med42-8B\n0.077\n0.083\n0.080\n0.042\nPhi-3.5-mini-instruct\n0.083\n0.092\n0.087\n0.046\nMistral-Nemo-Instruct-2407\n0.083\n0.070\n0.076\n0.040\nQwen2.5-7B-Instruct\n0.087\n0.086\n0.087\n0.045\nNER+Linking: ICD dict. + RuCCoD\nLlama3.1:8b-instruct\n0.378\n0.362\n0.369\n0.227\nLlama3-Med42-8B\n0.324\n0.354\n0.338\n0.203\nPhi-3.5-mini-instruct\n0.323\n0.357\n0.339\n0.204\nMistral-Nemo-Instruct-2407\n0.342\n0.295\n0.317\n0.188\nQwen2.5-7B-Instruct\n0.343\n0.340\n0.342\n0.206\nCode assignment: ICD dict.\nLlama3.1:8b-instruct\n0.575\n0.561\n0.568\n0.396\nLlama3-Med42-8B\n0.523\n0.594\n0.556\n0.385\nPhi-3.5-mini-instruct\n0.437\n0.510\n0.471\n0.308\nMistral-Nemo-Instruct-2407\n0.598\n0.533\n0.564\n0.392\nQwen2.5-7B-Instruct\n0.595\n0.618\n0.607\n0.435\nCode assignment: ICD dict. + RuCCoD\nLlama3.1:8b-instruct\n0.701\n0.684\n0.692\n0.529\nLlama3-Med42-8B\n0.644\n0.720\n0.680\n0.515\nPhi-3.5-mini-instruct\n0.627\n0.703\n0.663\n0.496\nMistral-Nemo-Instruct-2407\n0.691\n0.605\n0.645\n0.476\nQwen2.5-7B-Instruct\n0.700\n0.704\n0.702\n0.541\nTable 11: Relaxed evaluation results for NER, Code assignment, and end-to-end entity linking task on RuCCoD for\nLLM+RAG pipeline.\n24\n\n\nModel\nPrecision\nRecall\nF-score\nAccuracy\nNER: NEREL-BIO\nLlama3.1:8b-instruct-fp16\n0.100\n0.042\n0.059\n0.030\nLlama3-Med42-8B\n0.104\n0.043\n0.060\n0.031\nPhi-3.5-mini-instruct\n0.098\n0.043\n0.059\n0.031\nMistral-Nemo-Instruct-2407\n0.115\n0.044\n0.063\n0.033\nQwen2.5-7B-Instruct\n0.099\n0.043\n0.060\n0.031\nNER: RuCCoN\nLlama3.1:8b-instruct-fp16\n0.188\n0.088\n0.120\n0.064\nLlama3-Med42-8B\n0.174\n0.079\n0.108\n0.057\nPhi-3.5-mini-instruct\n0.172\n0.085\n0.114\n0.060\nMistral-Nemo-Instruct-2407\n0.197\n0.082\n0.116\n0.061\nQwen2.5-7B-Instruct\n0.185\n0.091\n0.122\n0.065\nNER+Linking: NEREL-BIO\nLlama3.1:8b-instruct\n0.033\n0.029\n0.031\n0.016\nLlama3-Med42-8B\n0.024\n0.025\n0.025\n0.013\nPhi-3.5-mini-instruct\n0.026\n0.028\n0.027\n0.014\nMistral-Nemo-Instruct-2407\n0.033\n0.027\n0.030\n0.015\nQwen2.5-7B-Instruct\n0.030\n0.029\n0.030\n0.015\nNER+Linking: RuCCoN\nLlama3.1:8b-instruct\n0.076\n0.069\n0.072\n0.038\nLlama3-Med42-8B\n0.061\n0.063\n0.062\n0.032\nPhi-3.5-mini-instruct\n0.060\n0.064\n0.062\n0.032\nMistral-Nemo-Instruct-2407\n0.076\n0.062\n0.068\n0.035\nQwen2.5-7B-Instruct\n0.073\n0.070\n0.072\n0.037\nCode assignment: NEREL-BIO\nLlama3.1:8b-instruct\n0.114\n0.107\n0.110\n0.058\nLlama3-Med42-8B\n0.088\n0.096\n0.092\n0.048\nPhi-3.5-mini-instruct\n0.098\n0.110\n0.104\n0.055\nMistral-Nemo-Instruct-2407\n0.121\n0.105\n0.112\n0.059\nQwen2.5-7B-Instruct\n0.125\n0.126\n0.125\n0.067\nCode assignment: RuCCoN\nLlama3.1:8b-instruct\n0.295\n0.282\n0.288\n0.168\nLlama3-Med42-8B\n0.254\n0.275\n0.264\n0.152\nPhi-3.5-mini-instruct\n0.248\n0.273\n0.260\n0.149\nMistral-Nemo-Instruct-2407\n0.284\n0.244\n0.263\n0.151\nQwen2.5-7B-Instruct\n0.292\n0.294\n0.293\n0.172\nTable 12: Relaxed evaluation results for NER, Code assignment, and end-to-end entity linking task on RuCCoD for\nLLM+RAG pipeline using NEREL-BIO and RuCCoN for vectorstore.\n25\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21263v1.pdf",
    "total_pages": 25,
    "title": "RuCCoD: Towards Automated ICD Coding in Russian",
    "authors": [
      "Aleksandr Nesterov",
      "Andrey Sakhovskiy",
      "Ivan Sviridov",
      "Airat Valiev",
      "Vladimir Makharev",
      "Petr Anokhin",
      "Galina Zubkova",
      "Elena Tutubalina"
    ],
    "abstract": "This study investigates the feasibility of automating clinical coding in\nRussian, a language with limited biomedical resources. We present a new dataset\nfor ICD coding, which includes diagnosis fields from electronic health records\n(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD\ncodes. This dataset serves as a benchmark for several state-of-the-art models,\nincluding BERT, LLaMA with LoRA, and RAG, with additional experiments examining\ntransfer learning across domains (from PubMed abstracts to medical diagnosis)\nand terminologies (from UMLS concepts to ICD codes). We then apply the\nbest-performing model to label an in-house EHR dataset containing patient\nhistories from 2017 to 2021. Our experiments, conducted on a carefully curated\ntest set, demonstrate that training with the automated predicted codes leads to\na significant improvement in accuracy compared to manually annotated data from\nphysicians. We believe our findings offer valuable insights into the potential\nfor automating clinical coding in resource-limited languages like Russian,\nwhich could enhance clinical efficiency and data accuracy in these contexts.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}