{
  "id": "arxiv_2502.21011v1",
  "text": "MagNet: Multi-Level Attention Graph Network for\nPredicting High-Resolution Spatial Transcriptomics\nJunchao Zhua, Ruining Dengb, Tianyuan Yaoa, Juming Xionga, Chongyu Qua, Junlin Guoa,\nSiqi Lua, Yucheng Tangc, Daguang Xuc, Mengmeng Yind, Yu Wangd, Shilin Zhaod, Yaohong\nWange, Haichun Yangd, and Yuankai Huo*a,d\naVanderbilt University, Nashville, TN, USA\nbWeill Cornell Medicine, NY, USA\ncNVIDIA, WA, USA\ndVanderbilt University Medical Center, Nashville, TN, USA\neUT MD Anderson Cancer Center, TX, USA\nABSTRACT\nThe rapid development of spatial transcriptomics (ST) offers new opportunities to explore the gene expression\npatterns within the spatial microenvironment. Current research integrates pathological images to infer gene\nexpression, addressing the high costs and time-consuming processes to generate spatial transcriptomics data.\nHowever, as spatial transcriptomics resolution continues to improve, existing methods remain primarily focused\non gene expression prediction at low-resolution (55 µm) spot levels. These methods face significant challenges,\nespecially the information bottleneck, when they are applied to high-resolution (8 µm) HD data. To bridge\nthis gap, this paper introduces MagNet, a multi-level attention graph network designed for accurate prediction\nof high-resolution HD data. MagNet employs cross-attention layers to integrate features from multi-resolution\nimage patches hierarchically and utilizes a GAT-Transformer module to aggregate neighborhood information. By\nintegrating multilevel features, MagNet overcomes the limitations posed by low-resolution inputs in predicting\nhigh-resolution gene expression. We systematically evaluated MagNet and existing ST prediction models on both\na private spatial transcriptomics dataset and a public dataset at three different resolution levels. The results\ndemonstrate that MagNet achieves state-of-the-art performance at both spot level and high-resolution bin levels,\nproviding a novel methodology and benchmark for future research and applications in high-resolution HD-level\nspatial transcriptomics. Code is available at https://github.com/Junchao-Zhu/MagNet.\nKeywords: Spatial Transcriptomics, Computational Pathology, Medical Image Analysis\n1. INTRODUCTION\nSpatial transcriptomics (ST) provides a novel view for correlating pathological tissue structures with their spatial\ngene expression patterns.1–3 This approach advances the development of effective treatment strategies.4 Studies\nhave demonstrated a strong correlation between features of pathological images and their gene expression pat-\nterns.5 In recent years, the widespread application of deep learning methods in medical image analysis3,6 has\nmade it possible to predict gene expression from broadly accessible and affordable whole-slide images (WSIs).\nCurrently, several studies have employed methods such as convolutional neural networks (CNNs)3,7,8 and\ngraph neural networks (GNNs)9–11 to predict spatial transcriptomic expression at the spot level with low resolu-\ntion. These approaches exploit spatial dependencies9,10 and image similarities7,8 inherent in pathological images,\nthus integrating information to optimize the fusion of image features. Such advances address the challenges of\nscarce high-quality spatial transcriptomic data and the high cost of acquisition.\nContinuous advancements in ST sequencing technology12–14 have significantly improved the resolution of\nexisting ST data, as is shown in Figure 1, which has progressed from the initial 55 µm spots to higher resolutions,\nsuch as HD data with bin diameters of 8 µm or even 2 µm. Such advancement enables a more comprehensive\nCorresponding author: Yuankai Huo: E-mail: yuankai.huo@vanderbilt.edu\narXiv:2502.21011v1  [cs.CV]  28 Feb 2025\n\n\nFigure 1: Spatial transcriptomics data at different resolutions. (A) Traditional low-resolution 10X Visium\nv2 barcoded spots, where spots are discretely distributed with a diameter of 55 µm. (B) Current high-resolution\n10X Visium HD barcoded squares, where bins are densely distributed with a diameter of 8 µm.\nanalysis of the relationship between pathological tissues and gene expression at the single-cell level.15–18 However,\ncurrent deep-learning methods face an information bottleneck when dealing with high-resolution HD data.19\nSpecifically, the limited information from low-resolution input images is insufficient to effectively support the\nprediction of high-dimensional gene expression. The features extracted by these models may lack the complexity\nrequired to represent the intricate details of high-resolution, high-dimensional gene expression data.\nTo address this issue, this paper proposes MagNet, a Multi-Level Attention Graph Network designed for accu-\nrate prediction of high-resolution HD data. MagNet integrates information across multiple resolutions, including\nthe bin, spot, and region levels, through cross-attention layers. MagNet also extracts and combines features\nfrom neighboring regions with Graph Attention Network (GAT) and Transformer layers. Thus, our proposed\nframework overcomes the information bottleneck posed by low-resolution inputs when predicting high-resolution,\nhigh-dimensional gene expression by efficient extraction and integration of multisource and multilevel features.\nFurthermore, the model incorporates cross-resolution constraints on gene expression within the same region,\nfurther enhancing its performance in HD gene expression prediction. Our contributions can be summarized in\nthree aspects:\n• We present MagNet, a Multi-Level Attention Graph Network designed for accurate prediction of high-\nresolution HD data. To our knowledge, it is the first model dedicated to HD-level gene expression prediction.\n• Our proposed framework leverages cross-attention layers and GAT-Transformer blocks to effectively extract\nand integrate multi-source and multi-level features, tackling the information bottleneck of low-resolution inputs\nin predicting high-resolution ST expression.\n• We provide our model as an open-source tool, benchmarking and providing a systematic evaluation on a\nprivately-collected kidney HD ST dataset and a public colorectal cancer HD ST dataset.\n2. METHOD\n2.1 Unified Cross-Resolution Feature Aggregation\nWe cropped patches at the bin, spot, and region levels for each bin i, denoted as ib, is and ir. Features of\nthese patches, represented as fb, fs and fr, are extracted by a pre-trained ResNet50.20 We adopt the strategy\nproposed by TRIPLEX21 that freezes the encoder parameters for the spot and region levels while updating only\nthe bin-level encoder to minimize computational overhead.\nTo refine the representation of fb, the features of other resolutions are treated as the key matrix (K) and the\nvalue matrix (V), with fb acting as the query matrix (Q). A cross-attention layer is used to effectively merge the\nfeatures of fs and fr into fb. Thus, the fused feature f ′\nb is formulated as:\nf ′\nb = softmax\n\u0012fbf T\ni\n√\nd\n\u0013\nfi,\ni = s, r\n(1)\n\n\nFigure 2: The network structure of the proposed MagNet. MagNet utilizes cross-attention layers to\nintegrate features extracted from multi-resolution patches. Additionally, it incorporates a GAT-Transformer\nblock to aggregate neighborhood information while leveraging spatial relationships. The predictions for each\nresolution level are then independently generated by a regression head.\nwhere\n√\nd is a scaling factor. Finally, by concatenating the features from all three levels, the fused multi-level\nfeature F is obtained for use in subsequent processes.\n2.2 Spatial-Guided Graph Integration Block\nTo exploit the spatial relationship of pathological images, we propose a spatially-guided graph integration block\nthat integrates GAT and transformer layers. The connections between bins are first established by calculating the\nweight eij between any two nodes i and j using the Euclidean distance. The top-k lowest eij values are selected\nto establish connections within the whole-slide image. The constructed graph is then fed into the spatial-guided\ngraph integration block for further processing.\nSubsequently, after rounds of graph attention convolution, the processed feature F i\nm for each ib, is and ir is\nformulated as follows:\nFi\nm =\n\r\r\r\r\nK\nk=1\nσ\n\nX\nj∈N(i)\nαk\nijWkf j\nm\n\n, {m|b, s, r}\n(2)\nwhere N(i) denotes the set of adjacent nodes,\n\r\r\r\r represents concatenation operation, σ is the activation function,\nαk\nij is the weight of the k-th attention head, and Wk is a linear transformation matrix determined by the\nconnections between nodes.\nA Transformer layer is used for adaptive aggregation of neighborhood information from each round, thus\nenhancing the representation of features. Finally, the regression head generates gene expression predictions for\neach level separately, denoted as pb, ps, and pr.\n\n\n2.3 Loss Function\nTo exploit the mutual consistency among multilevel information, we designed a hybrid loss function comprising\nprediction loss Lp and consistency loss Lc to optimize the model learning process. The prediction loss primarily\nfocuses on minimizing the discrepancies between the model’s predictions and the ground truth at each resolution\nlevel. For the prediction task at bin level, we employ Mean Squared Error (MSE) and Pearson Correlation\nCoefficient loss (P) to evaluate the model’s performance. To avoid introducing additional noise, only PCC loss is\nutilized to assess the model’s performance at the spot and region levels. Hence, the prediction loss is formulated\nas:\nLp = MSE(pb, yb) +\nX\ni=b,s,r\nλi · P(pi, yi)\n(3)\nHere, b, s, and r represent the bin, spot, and region levels, respectively. pi and yi denote the prediction of the\nmodel and its corresponding ground truth, while λi is a hyperparameter used to balance the PCC loss at different\nresolution levels.\nSince patches at different resolutions within the same region exhibit similar trends in gene expression, we\nemploy PCC loss to constrain the differences between bin-level predictions and those at other levels.\nThe\nconsistency loss Lc is defined as:\nLc = λ1 · P(pb, ps) + λ2 · P(pb, pr)\n(4)\nThus, the overall loss of the model L is defined as:\nL = γ1 · Lp + γ2 · Lc\n(5)\nHere, γ1 and γ2 are hyperparameters used to balance the two types of losses, and they are set to 1 and 0.25 in\nthe subsequent experiments.\n3. DATA AND EXPERIMENT\nDataset. We benchmarked our MagNet and other baseline models on a privately collected kidney pathology\ndataset (VUMC) and a publicly available colorectal cancer (CRC) dataset.17\nWe conducted four-fold cross-\nvalidation at the WSI level. Our in-house dataset contains 12 HD ST samples with three resolutions: 2 µm,\n8 µm, and 16 µm, where 1px in the WSI corresponds to 0.25 µm of real tissue. The CRC dataset consists of four\nsamples with a single-layer section, including two CRC tissues and two adjacent normal tissues. The process has\nbeen approved by Institutional Review Board (IRB).\nData Preprocessing. 6,000 bins were randomly selected for each WSI, and 112×112 pixel patches centered\nat 8 µm and 16 µm bins were cropped. At the spot and region levels, patches with diameters of 224 and 512\npixels were extracted across the WSI, with their gene expressions aggregated from bin-level data. 2,500 spot-level\npatches per WSI were selected for training and testing. Patch pairing across levels was based on the distance\nbetween the coordinates in different resolutions. We follow the method proposed in ST-Net20 and select the top\n250 genes with the highest average expression levels of more than 20,000 original genes for prediction. Gene\nexpression values were normalized using the approach introduced in TRIPLEX,21 which involves proportional\nnormalization followed by a log transformation.\nCompared Methods and Evaluation Metrics. MagNet was benchmarked against current ST counterparts,\nincluding spatial-aware methods HisToGene9 and His2ST,10 similarity-based strategy BLEEP7 and EGN,8 and\nthe classic approach ST-Net.3 We used the officially released code published along with the papers for all of\nthe methods. The Pearson correlation coefficient (PCC), mean squared error (MSE), and mean absolute error\n(MAE) are used to evaluate the performance of the models comprehensively.\nExperiment Setting and Implementation. Experiments were conducted on NVIDIA RTX A6000 GPU\ncards. The SGD optimizer was utilized, with momentum set to 0.9 and a weight decay of 10−4. An initial\nlearning rate of 10−4 was applied, which followed a cosine decay schedule, reducing it to 0.01 of its starting\nvalue throughout training. All models are trained to converge. We employed a batch size of 256 for training and\nfine-tuned the hyperparameters λ1, λ2, λb, λs, and λr in our hybrid loss function to values of 0.1, 0.1, 0.8, 0.25,\nand 0.25, respectively. For graph construction, the top-k value was fixed at 8. We select 8 µm and 16 µm bins\n\n\nTable 1: Quantitative comparisons across different datasets. The best performance is highlighted in\nbold, where we can observe that MagNet outperforms the state-of-the-art in multiple resolutions.\nResolution\nModel\nVUMC (in-house dataset)\nCRC17\nMSE\nMAE\nPCC\nMSE\nMAE\nPCC\n8µm /112px\nST-Net\n0.193±0.004\n0.388±0.009\n0.226±0.040\n0.292±0.076\n0.402±0.084\n0.527±0.155\nEGN\n0.048±0.011\n0.134±0.020\n0.157±0.024\n0.409±0.164\n0.508±0.139\n0.511±0.152\nHisToGene\n0.105±0.007\n0.241±0.006\n0.109±0.018\n0.311±0.088\n0.419±0.075\n0.451±0.128\nBLEEP\n0.063±0.006\n0.163±0.009\n0.199±0.052\n0.348±0.041\n0.440±0.0361\n0.475±0.1379\nHis2ST\n0.140±0.019\n0.358±0.026\n0.175±0.033\n0.287±0.113\n0.4041±0.109\n0.537±0.165\nMagNet(Ours)\n0.048±0.008\n0.109±0.008\n0.278±0.042\n0.271±0.054\n0.375±0.053\n0.541±0.167\n16µm /112px\nST-Net\n0.288±0.007\n0.420±0.027\n0.364±0.0539\n0.661±0.239\n0.632±0.146\n0.560±0.151\nEGN\n0.149±0.037\n0.302±0.06\n0.308±0.037\n0.740±0.0241\n0.677±0.013\n0.552±0.014\nHisToGene\n0.204±0.045\n0.380±0.052\n0.243±0.035\n0.660±0.176\n0.6368±0.099\n0.522±0.136\nBLEEP\n0.174±0.029\n0.290±0.031\n0.317±0.058\n0.673±0.161\n0.625±0.088\n0.504±0.123\nHis2ST\n0.224±0.044\n0.427±0.049\n0.330±0.046\n0.610±0.168\n0.611±0.103\n0.562±0.152\nMagNet(Ours)\n0.127±0.024\n0.228±0.034\n0.378±0.057\n0.564±0.184\n0.581±0.114\n0.574±0.154\n55µm /224px\nST-Net\n0.442±0.036\n0.549±0.019\n0.609±0.059\n0.767±0.203\n0.652±0.086\n0.649±0.080\nEGN\n0.355±0.030\n0.471±0.010\n0.601±0.0561\n0.778±0.229\n0.651±0.105\n0.674±0.071\nHisToGene\n0.403±0.028\n0.517±0.017\n0.596±0.058\n0.702±0.173\n0.622±0.074\n0.663±0.067\nBLEEP\n0.339±0.026\n0.467±0.017\n0.576±0.049\n0.717±0.112\n0.623±0.044\n0.667±0.043\nHis2ST\n0.327±0.021\n0.459±0.013\n0.601±0.058\n0.813±0.199\n0.673±0.089\n0.673±0.065\nMagNet(Ours)\n0.324±0.044\n0.458±0.030\n0.611±0.082\n0.688±0.149\n0.612±0.069\n0.670±0.059\nas the target HD resolution to predict, due to the extremely low gene expression amount in 2 µm bins. During\nspot-level experiments, we freeze the encoder parameters of the bin and region levels and update the spot level\ninstead.\n4. RESULTS\n4.1 Cross-Validation Evaluation\nWe conducted four-fold validation on the WSI level to validate and benchmark MagNet and SOTAs on the two\nHD datasets. Table 1 summarizes quantitative comparisons of various baselines across different datasets and\nresolutions. Our proposed MagNet consistently outperforms existing methods in almost all metrics, with its su-\nperiority particularly evident at HD high-resolution levels. Taking the 8 µm prediction task in our VUMC dataset\nas an example, MagNet achieved MSE, MAE, and PCC values of 0.048±0.008, 0.109±0.008, and 0.278±0.042,\nrespectively, significantly surpassing the results of other methods, such as BLEEP, which reported values of\n0.063±0.006, 0.163±0.009, and 0.199±0.052.\nThese findings demonstrate the capability of MagNet to effectively address the information bottleneck inher-\nent in high-resolution gene prediction tasks. By efficiently integrating and leveraging multi-source and multi-level\ninformation, MagNet overcomes the performance limitations caused by constrained data and substantially en-\nhances prediction accuracy for high-resolution HD data. Furthermore, the relatively low standard deviation\nobserved among all metrics during cross-validation highlights the method’s robustness and stability, underscor-\ning its reliability for practical clinical applications.\n4.2 Pivotal Gene Expression Prediction\nWe evaluated the clinical applicability of various baselines by analyzing the predictive performance of the key\nbiomarker SGPP1 in our kidney dataset. SGPP1 and its associated pathways play a critical role in kidney health\nand disease, with direct implications for conditions such as acute kidney injury and fibrotic kidney diseases.22–24\nFigure 3 illustrates the predictive performance of different models for the SGPP1 gene. Compared with other\nbaseline models, our proposed MagNet achieved the best MSE of 0.051. By deeply integrating and leveraging\n\n\nFigure 3: Qualitative comparison for pivotal gene expression prediction.\nTable 2: Ablation study for functional blocks in MagNet. The benefits from each designed block are\northonormal, while MagNet achieves optimal results when integrating all modules.\nFunctional Blocks\nVUMC (in-house dataset) /16µm\nCRC17/16 µm\nMSE\nMAE\nPCC\nMSE\nMAE\nPCC\nw.o. GAT & Multi-resolution\n0.148±0.042\n0.281±0.069\n0.299±0.028\n0.799±0.259\n0.709±0.146\n0.548±0.146\nw.o. GAT block\n0.135±0.030\n0.266±0.048\n0.306±0.043\n0.632±0.170\n0.624±0.096\n0.550±0.147\nw.o. Multi-resolution\n0.133±0.030\n0.260±0.051\n0.323±0.044\n0.634±0.175\n0.628±0.111\n0.563±0.152\nw.o. Consistency Loss\n0.130±0.023\n0.235±0.040\n0.369±0.054\n0.624±0.187\n0.619±0.117\n0.559±0.146\nw. All blocks\n0.127±0.024\n0.228±0.034\n0.378±0.057\n0.564±0.184\n0.581±0.114\n0.574±0.154\nmulti-level information, MagNet captures the spatial distribution of key gene expressions in pathological tissues\nwith higher resolution, providing more detailed predictions for subsequent diagnoses and demonstrating strong\npotential for clinical applications.\n4.3 Ablation Study\nWe conducted a detailed ablation study to evaluate the effectiveness of each functional block, as is summarized\nin Table 2. The results demonstrate that incorporation of the GAT-Transformer block and multi-resolution\ninformation effectively compensates for the limited information in the original bin-level data, thus significantly\nincreasing the PCC by 0.079 in our dataset and by 0.026 in the CRC dataset.\nAdditionally, introducing a consistency loss enhances the synergy of multi-resolution information by leveraging\nthe mutual constraints of gene expression across different resolutions within the same region, thereby facilitating\nmore effective learning of high-resolution features and further improving the model’s performance. To conclude,\nthe benefits of each block are mutually exclusive and synergistic, allowing MagNet to achieve optimal results\nwhen integrating all modules.\n\n\n5. CONCLUSION\nWe introduce a novel framework specifically tailored for high-resolution gene expression tasks. Our MagNet\nmodel integrates multi-level information and leverages spatial relationships derived from pathological images, ef-\nfectively overcoming the input-information bottleneck in HD gene expression prediction. Consequently, MagNet\ncan accurately capture gene expression patterns at an 8 µm single-cell resolution. In addition, we present the\nfirst systematic and comprehensive evaluation of HD-level spatial transcriptomics datasets. We benchmarked\nMagNet against current state-of-the-art methods on two HD datasets under three different resolution settings.\nExperimental results demonstrate that MagNet consistently achieves top-tier predictive performance across mul-\ntiple resolutions in both datasets. By extending gene prediction from the spot level to the cellular scale, MagNet\nestablishes a new paradigm and benchmark for future research in spatial transcriptomics.\nACKNOWLEDGMENTS\nThis research was supported by NIH R01DK135597(Huo), DoD HT9425-23-1-0003(HCY), NIH NIDDK DK56942\n(ABF). This work was also supported by Vanderbilt Seed Success Grant, Vanderbilt Discovery Grant, and VISE\nSeed Grant. This project was supported by The Leona M. and Harry B. Helmsley Charitable Trust grant G-\n1903-03793 and G-2103-05128. This research was also supported by NIH grants R01EB033385, R01DK132338,\nREB017230, R01MH125931, and NSF 2040462. We extend gratitude to NVIDIA for their support by means of\nthe NVIDIA hardware grant.\nREFERENCES\n[1] Burgess, D. J., “Spatial transcriptomics coming of age,” Nature Reviews Genetics 20(6), 317–317 (2019).\n[2] Asp, M., Giacomello, S., Larsson, L., Wu, C., F¨urth, D., Qian, X., W¨ardell, E., Custodio, J., Reimeg˚ard,\nJ., Salm´en, F., et al., “A spatiotemporal organ-wide gene expression and cell atlas of the developing human\nheart,” Cell 179(7), 1647–1660 (2019).\n[3] He, B., Bergenstr˚ahle, L., Stenbeck, L., Abid, A., Andersson, A., Borg, ˚A., Maaskola, J., Lundeberg, J.,\nand Zou, J., “Integrating spatial gene expression and breast tumour morphology via deep learning,” Nature\nbiomedical engineering 4(8), 827–834 (2020).\n[4] Asp, M., Bergenstr˚ahle, J., and Lundeberg, J., “Spatially resolved transcriptomes—next generation tools\nfor tissue exploration,” Bioessays 42(10), 1900221 (2020).\n[5] Badea, L. and St˘anescu, E., “Identifying transcriptomic correlates of histology using deep learning,” PloS\none 15(11), e0242858 (2020).\n[6] Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N., and Liang, J., “Unet++: A nested u-net architecture\nfor medical image segmentation,” in [Deep Learning in Medical Image Analysis and Multimodal Learning\nfor Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop,\nML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings\n4], 3–11, Springer (2018).\n[7] Xie, R., Pang, K., Chung, S., Perciani, C., MacParland, S., Wang, B., and Bader, G., “Spatially resolved\ngene expression prediction from histology images via bi-modal contrastive learning,” Advances in Neural\nInformation Processing Systems 36 (2024).\n[8] Yang, Y., Hossain, M. Z., Stone, E. A., and Rahman, S., “Exemplar guided deep neural network for spatial\ntranscriptomics analysis of gene expression prediction,” in [Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision], 5039–5048 (2023).\n[9] Pang, M., Su, K., and Li, M., “Leveraging information in spatial transcriptomics to predict super-resolution\ngene expression from histology images in tumors,” BioRxiv , 2021–11 (2021).\n[10] Zeng, Y., Wei, Z., Yu, W., Yin, R., Yuan, Y., Li, B., Tang, Z., Lu, Y., and Yang, Y., “Spatial tran-\nscriptomics prediction from histology jointly through transformer and graph neural networks,” Briefings in\nBioinformatics 23(5), bbac297 (2022).\n[11] Jia, Y., Liu, J., Chen, L., Zhao, T., and Wang, Y., “Thitogene: a deep learning method for predicting\nspatial transcriptomics from histological images,” Briefings in Bioinformatics 25(1), bbad464 (2024).\n\n\n[12] St˚ahl, P. L., Salm´en, F., Vickovic, S., Lundmark, A., Navarro, J. F., Magnusson, J., Giacomello, S., Asp,\nM., Westholm, J. O., Huss, M., et al., “Visualization and analysis of gene expression in tissue sections by\nspatial transcriptomics,” Science 353(6294), 78–82 (2016).\n[13] Wang, X., Allen, W. E., Wright, M. A., Sylwestrak, E. L., Samusik, N., Vesuna, S., Evans, K., Liu, C.,\nRamakrishnan, C., Liu, J., et al., “Three-dimensional intact-tissue sequencing of single-cell transcriptional\nstates,” Science 361(6400), eaat5691 (2018).\n[14] Eng, C.-H. L., Lawson, M., Zhu, Q., Dries, R., Koulena, N., Takei, Y., Yun, J., Cronin, C., Karp, C., Yuan,\nG.-C., et al., “Transcriptome-scale super-resolved imaging in tissues by rna seqfish+,” Nature 568(7751),\n235–239 (2019).\n[15] Zhu, S., Kubota, N., Wang, S., Wang, T., Xiao, G., and Hoshida, Y., “Stie: Single-cell level deconvolu-\ntion, convolution, and clustering in in situ capturing-based spatial transcriptomics,” Nature communica-\ntions 15(1), 7559 (2024).\n[16] Benjamin, K., Bhandari, A., Kepple, J. D., Qi, R., Shang, Z., Xing, Y., An, Y., Zhang, N., Hou, Y.,\nCrockford, T. L., et al., “Multiscale topology classifies cells in subcellular spatial transcriptomics,” Nature\n, 1–7 (2024).\n[17] Oliveira, M. F., Romero, J. P., Chung, M., Williams, S., Gottscho, A. D., Gupta, A., Pilipauskas, S. E.,\nMohabbat, S., Raman, N., Sukovich, D., et al., “Characterization of immune cell populations in the tumor\nmicroenvironment of colorectal cancer using high definition spatial profiling,” bioRxiv , 2024–06 (2024).\n[18] Janesick, A., Shelansky, R., Gottscho, A. D., Wagner, F., Williams, S. R., Rouault, M., Beliakoff, G.,\nMorrison, C. A., Oliveira, M. F., Sicherman, J. T., et al., “High resolution mapping of the tumor microen-\nvironment using integrated single-cell, spatial and in situ analysis,” Nature Communications 14(1), 8353\n(2023).\n[19] Tishby, N. and Zaslavsky, N., “Deep learning and the information bottleneck principle,” in [2015 ieee\ninformation theory workshop (itw)], 1–5, IEEE (2015).\n[20] He, K., Zhang, X., Ren, S., and Sun, J., “Deep residual learning for image recognition,” in [Proceedings of\nthe IEEE conference on computer vision and pattern recognition], 770–778 (2016).\n[21] Chung, Y., Ha, J. H., Im, K. C., and Lee, J. S., “Accurate spatial gene expression prediction by integrating\nmulti-resolution features,” in [Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition], 11591–11600 (2024).\n[22] Drexler, Y., Molina, J., Mitrofanova, A., Fornoni, A., and Merscher, S., “Sphingosine-1-phosphate\nmetabolism and signaling in kidney diseases,” Journal of the American Society of Nephrology 32(1), 9–\n31 (2021).\n[23] Keller, N., Midgley, J., Khalid, E., Lesmana, H., Mathew, G., Mincham, C., Teig, N., Khan, Z., Khosla,\nI., Mehr, S., et al., “Factors influencing survival in sphingosine phosphate lyase insufficiency syndrome: a\nretrospective cross-sectional natural history study of 76 patients,” Orphanet journal of rare diseases 19(1),\n355 (2024).\n[24] Lovric, S., Goncalves, S., Gee, H. Y., Oskouian, B., Srinivas, H., Choi, W.-I., Shril, S., Ashraf, S., Tan, W.,\nRao, J., et al., “Mutations in sphingosine-1-phosphate lyase cause nephrosis with ichthyosis and adrenal\ninsufficiency,” The Journal of clinical investigation 127(3), 912–928 (2017).\nAPPENDIX A. GENE SELECTION AND ESTIMATION\nTo estimate the gene expression at the spot level and the region level, we aggregated the value of gene expression\nof 16 µm bins within their respective spot and region areas. This process can be defined as:\nys =\nX\ni∈S\nyi,\nyr =\nX\ni∈R\nyi\n(6)\nHere, yi denotes the gene expression value at the i-th bin, S represents the set of bins within a specific spot,\nand R denotes the set of bins within a certain area, thus ensuring the consistency of gene expression across\nmultiple resolutions. The selected genes with the highest average expression for each dataset and resolution are\npresented in Figure. 4\n\n\nFigure 4: Gene selection in each dataset and resolution.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21011v1.pdf",
    "total_pages": 9,
    "title": "MagNet: Multi-Level Attention Graph Network for Predicting High-Resolution Spatial Transcriptomics",
    "authors": [
      "Junchao Zhu",
      "Ruining Deng",
      "Tianyuan Yao",
      "Juming Xiong",
      "Chongyu Qu",
      "Junlin Guo",
      "Siqi Lu",
      "Yucheng Tang",
      "Daguang Xu",
      "Mengmeng Yin",
      "Yu Wang",
      "Shilin Zhao",
      "Yaohong Wang",
      "Haichun Yang",
      "Yuankai Huo"
    ],
    "abstract": "The rapid development of spatial transcriptomics (ST) offers new\nopportunities to explore the gene expression patterns within the spatial\nmicroenvironment. Current research integrates pathological images to infer gene\nexpression, addressing the high costs and time-consuming processes to generate\nspatial transcriptomics data. However, as spatial transcriptomics resolution\ncontinues to improve, existing methods remain primarily focused on gene\nexpression prediction at low-resolution spot levels. These methods face\nsignificant challenges, especially the information bottleneck, when they are\napplied to high-resolution HD data. To bridge this gap, this paper introduces\nMagNet, a multi-level attention graph network designed for accurate prediction\nof high-resolution HD data. MagNet employs cross-attention layers to integrate\nfeatures from multi-resolution image patches hierarchically and utilizes a\nGAT-Transformer module to aggregate neighborhood information. By integrating\nmultilevel features, MagNet overcomes the limitations posed by low-resolution\ninputs in predicting high-resolution gene expression. We systematically\nevaluated MagNet and existing ST prediction models on both a private spatial\ntranscriptomics dataset and a public dataset at three different resolution\nlevels. The results demonstrate that MagNet achieves state-of-the-art\nperformance at both spot level and high-resolution bin levels, providing a\nnovel methodology and benchmark for future research and applications in\nhigh-resolution HD-level spatial transcriptomics. Code is available at\nhttps://github.com/Junchao-Zhu/MagNet.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}