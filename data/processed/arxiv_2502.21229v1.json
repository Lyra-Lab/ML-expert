{
  "id": "arxiv_2502.21229v1",
  "text": "A Method of Selective Attention for Reservoir Based Agents\nKevin McKee\nAstera Institute\nAbstract\nTraining of deep reinforcement learning agents is slowed considerably by\nthe presence of input dimensions that do not usefully condition the reward\nfunction. Existing modules such as layer normalization can be trained with\nweight decay to act as a form of selective attention, i.e. an input mask, that\nshrinks the scale of unnecessary inputs, which in turn accelerates training\nof the policy. However, we find a surprising result that adding numerous\nparameters to the computation of the input mask results in much faster\ntraining.\nA simple, high dimensional masking module is compared with\nlayer normalization and a model without any input suppression. The high\ndimensional mask resulted in a four-fold speedup in training over the null\nhypothesis and a two-fold speedup in training over the layer normalization\nmethod.\nIntroduction\nNatural intelligence relies on selective attention, the cognitive process of excluding un-\nnecessary distractions from consideration and enhancing the most important information.\nSeveral influential works in cognitive science have previously espoused the theory that at-\ntention arises from the brain’s need to conserve resources1,2,3. However, selective attention\nis likely more than just a matter of resource constraints, and instead concerns the efficiency\nof learning by flexibly prioritizing information.4 Four functions of selective attention in ani-\nmals were identified through a review of empirical results:5 “(1) restriction of the number of\ninputs analyzed, (2) restriction of the dimensions analyzed, (3) the items (defined by sets of\ncritical features) for which [the subject] looks or listens, and (4) selections of which results of\nperceptual analysis will control behavior and be stored in memory.” The implication of this\ntheory is that there may exist general methods of intelligently suppressing uninformative\ninputs to improve the efficiency of machine learning. In the reinforcement learning (RL)\ncontext, reward signals present a sufficiently general objective for such a filter.\nIn this study, we consider the model of selective attention as suppression of unim-\nportant inputs to improve the efficiency of learning. We test for changes in reinforcement\nlearning efficiency as a result of both existing method and novel methods.\nOur results\ndemonstrate, first, that input suppression controlled by reward signals greatly accelerates\nlearning, and second, that greatly over-parameterizing the input suppression mask results\narXiv:2502.21229v1  [cs.LG]  28 Feb 2025\n\n\nSELECTIVE ATTENTION FOR RESERVOIR BASED AGENTS\n2\nin the largest speedups in training. We give this new method the nickname Excessively\nParameterized Input Concealment, or EPIC.\nSpecifically, we develop this method for use with reinforcement learning agents that\nincorporate reservoir computing.\nIn previous work, it was found that a reservoir com-\nputer formulated as a recurrent neural network (RNN) with fixed, random, and normalized\nweights, called the Echo State Network (ESN), outperforms other memory architectures on\nexperimental tasks that require meta-learning and non-Markovian time dependence.6 How-\never, reservoir computers do not typically allow for the training of input weights to the RNN.\nTraining a multi-layer decoder with back-propagation therefore scales poorly as number of\ninputs dimensions with non-trivial variances increases. By incorporating reward-driven se-\nlective attention, inputs can be either suppressed or amplified before they are registered\nin the RNN, resulting in more efficiently decoded memory representations with a larger\nsignal-to-noise ratio with respect to the final rewards.\nTask\nDistracting Multi-armed Bandit.\nThe primary task involved a standard Multi-\narmed bandit as detailed in both ours and others’ previous experiments with meta-\nlearning,6,7 with the addition of a large observation vector of random, normally distributed\nnoise. Half of the observation vector was updated with new noise every step, while the other\nhalf updated only once per episode. The noise vector served to artificially complicate the\nloss function and imitate the case of high-dimensional real-world distractions to the agent.\nModels\nAll agents used the actor critic algorithm regularized with entropy maximization. The\ndesign was the same as used in previous related work.6 In short, inputs to the agent and\naction-reward feedbacks are passed to the Echo State Network (ESN), which then feeds\nforward through two-layer MLPs for each the actor and the critic outputs. The Echo State\nNetwork incorporated a dense-local and sparse-global connection pattern. The MLPs had\nhidden layers with 256 nodes each. Hyperparameters are given in Appendix A.\nLayer Normalization.\nLayer normalization with element-wise affine transforma-\ntion (LayerNorm)8 provides a common sense solution for suppressing inputs.\ny =\nE[X]\np\nVar(X) + ϵ ⊙γ + β\n(1)\nTo test the effectiveness of layer normalization as an input suppressant, one agent included a\nstandard layer normalization step between the inputs and the ESN. The initial scale values\nwere multiplied by 2.5, to allow a broader range of inputs to the ESN. This minor step\nwas found through manual search to improve training time. To use layer normalization to\nsuppress inputs, a relatively large weight decay coefficient (1e-4) was included in the loss\nfunction only with respect to the parameters γ and β.\nSimple vector filter.\nA simple alternative to layer normalization is to use a simple\nreal valued vector of parameters b. The parameters are transformed into a mask by bounding\n\n\nSELECTIVE ATTENTION FOR RESERVOIR BASED AGENTS\n3\nin the [min, max] interval as follows:\nm = (max −min)σ(b) + min,\n(2)\ny = x ⊙m.\n(3)\nThis represents the simplest filtering strategy tested as it uses the fewest parameters and\nunlike layer normalization, involves no normalization step.\nExcessively Parameterized Input Concealment.\nA less common sense solution\nto suppressing inputs is to start with a fixed vector of random numbers u several times length\nof the inputs x, then applying a trained affine transformation of those random numbers to\nproduce the mask values m. The mask values are bound using a scaled and shifted sigmoid\nfunction:\nu ∼N(0, 1),\n(4)\nm = (max −min)σ(Wu + b) + min,\n(5)\ny = x ⊙m,\n(6)\nwhere W is a fully connected weight matrix and b is the bias term. This strategy arbitrarily\nover-parameterizes the mask so that during training, there are many equivalent solutions,\nand many that will, by chance, be close to the parameter’s starting vector. By making the\nmask’s training as speedy as possible, the search space for the actor and critic parameters\nis greatly reduced for the remaining majority of training.\nThis strategy requires the actor and critic modules backpropagate error gradients to\nthe mask generating parameters. Those gradients must be larger than the gradients of a\nregularizing penalty that shrinks the mask. That penalty is nothing more than the mean\nvalue of the final mask, weighted by a small scaler. Among values manually tried for these\nexperimental tasks, the best value for that scaler was found to be 1e-5. For larger input\ntasks, it may be necessary to tune this coefficient.\nResults\nThe results of this experiment are shown in Figure 1a. The distraction vector resulted\nin the ESN based agent reaching a high score in about 7000-8000 episodes, rather than the\nexpected 500-1500 episodes.6 Adding either the vector filter or layer normalization with\nweight decay halved the number of episodes to convergence. Replacing either with EPIC\nhalved the number of episodes again. Increasing the size of the random vector in EPIC\nonly made a negligible improvement in training time. When the task had 64 (Figure 1b)\ndistraction inputs instead of 32 (Figure 1a), the ratios of performance were about the same\nwith a slightly more pronounced improvement in EPIC in the former.\nBesides training time itself, the reliability of the training curves also appears to im-\nprove with better input masking. The model without any input mask varied in its conver-\ngence, ranging from nearly the same rate as that with layer normalization, to taking twice\nas many episodes.\nDiscussion\nOur results demonstrate first that intelligently masking the inputs results in much\nfaster learning in the presence of many extra, uninformative input dimensions. This was\n\n\nSELECTIVE ATTENTION FOR RESERVOIR BASED AGENTS\n4\n(a) Results for multi-armed bandit with 32-\nlength distraction noise.\n(b) Results for multi-armed bandit with 64-\nlength distraction noise.\nFigure 1. Results on multi-armed bandit with 32 and 64-length distraction noise. Trained\ninput masking by regularized layer normalization methods doubled training efficiency. Over-\nparameterizing the mask quadrupled training efficiency. For EPIC, the length of random\nvector u is given in parentheses.\ndone in both of the examined methods by balancing reward gradients with an objective to\nminimize the scale of the inputs. Second, we demonstrate a surprising principle that the\nseemingly unnecessary over-parameterization of the input mask causes significant further\nimprovements to training efficiency. We hypothesize that this effect results from provid-\ning the input mask-generating function with many equivalent but randomly distributed\nsolutions such that one is likely to exist close to the network’s starting parameters at the\nbeginning of training. By making such solutions convenient for the solver, the mask can\nreduce the search space for the policy and critic networks early in training, leading to the\noverall speedups observed.\nLimitations.\nOur hypothesis and findings arose in the context of developing an\nefficient reservoir-based RL agent, so our study was limited to that context. We do not\nexamine here whether the input masking method is superior to alternatives in general, or\nsuperior to trainable input weights in general. We only justify our result in light of the\nprinciples by which reservoir-based agents work.\nIn this paper, we do not examine the more general goal of conditional input masking,\nwhich better reflects naturalistic usage of selective attention. While we leave conditional\nmasking for future work, the current findings regarding over-parameterization were in fact\na consequence of repeatedly finding that conditioning the mask on the recurrent (ESN)\nstate via multiple, fully connected layers produced the improvements over fixed masks\nshown here. When conditioning on the ESN state, the resulting mask nonetheless tended to\ndemonstrate a fixed set of scalars over the input vector, leading to the current hypothesis\nthat the excessive parameter dimensionality was more the cause of improvement than was\nconditioning on the ESN. Furthermore, the task used here, and perhaps most familiar\ndiagnostic tasks, tend to entail a fixed degree of importance per input element. To study\nconditional selection of inputs, more work is required to conceptualize a set of tasks that\n\n\nSELECTIVE ATTENTION FOR RESERVOIR BASED AGENTS\n5\nrequire or emphasize the necessity to shift the mask’s values with changing context.\nConclusion.\nCombining reward signal backpropagation with regularization can re-\nsult in learned, intelligent masking of inputs reflecting their importance to the task. This\nmasking reduces the search space for the policy and critic networks, resulting in signifi-\ncant improvements to training efficiency in agents that use reservoir based memory. Over-\nparameterizing the input mask-generating function further increases the training efficiency\nby as much as double. Hence the latter method represents an easy, generic, and low-cost\nstrategy for improving RL performance.\nReferences\n[1] Donald Eric Broadbent. Perception and communication. Elsevier, 2013.\n[2] Anne M Treisman and Garry Gelade. A feature-integration theory of attention. Cogni-\ntive psychology, 12(1):97–136, 1980.\n[3] Daniel Kahneman. Attention and effort, volume 1063. Citeseer, 1973.\n[4] Peter Dayan, Sham Kakade, and P Read Montague. Learning and selective attention.\nNature neuroscience, 3(11):1218–1223, 2000.\n[5] Anne M Treisman. Strategies and models of selective attention. Psychological review,\n76(3):282, 1969.\n[6] Kevin McKee. Reservoir computing for fast, simplified reinforcement learning on mem-\nory tasks. arXiv preprint arXiv:2412.13093, 2024.\n[7] Jane X Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer,\nJoel Z Leibo, Demis Hassabis, and Matthew Botvinick. Prefrontal cortex as a meta-\nreinforcement learning system. Nature neuroscience, 21(6):860–868, 2018.\n[8] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv\npreprint arXiv:1607.06450, 2016.\n\n\nSELECTIVE ATTENTION FOR RESERVOIR BASED AGENTS\n6\nAppendix A: Hyperparameters\nThe hyperparameters for the ESNs are given in Table 1.\nTable 1\nHyperparameters for all models, including fixed ESN weight matrices.\nParameter\nDescription\nValue\nActor-Critic\nLR\nLearning rate\n0.0001\nβe\nEntropy regularization coefficient\n0.001\nNHidden\nHidden units per MLP layer\n256\nK\nNumber of hidden layers per MLP\n2\nESNLG\nNUnique\nUnique hidden nodes per input\n40\nNShared\nOverlapping nodes per neighboring inputs\n20\nϕ\nSpectral radius\n1.0\nPL(W)\nLocal connection probability\n50%\nPG(W)\nGlobal connection probability\n1%\nPI(W)\nInput connection probability\n50%\nR\nMax local connection radius\n10\nEPIC\nNu\nLength of random normal vector\nInput dimension ×4, 8\nRegularizer coefficient\n1e −5\nmin\nMinimum mask value\n0.25\nmax\nMaximum mask value\n5.0\nVector filter\nRegularizer coefficient\n1e −5\nmin\nMinimum mask value\n0.25\nmax\nMaximum mask value\n5.0\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21229v1.pdf",
    "total_pages": 6,
    "title": "A Method of Selective Attention for Reservoir Based Agents",
    "authors": [
      "Kevin McKee"
    ],
    "abstract": "Training of deep reinforcement learning agents is slowed considerably by the\npresence of input dimensions that do not usefully condition the reward\nfunction. Existing modules such as layer normalization can be trained with\nweight decay to act as a form of selective attention, i.e. an input mask, that\nshrinks the scale of unnecessary inputs, which in turn accelerates training of\nthe policy. However, we find a surprising result that adding numerous\nparameters to the computation of the input mask results in much faster\ntraining. A simple, high dimensional masking module is compared with layer\nnormalization and a model without any input suppression. The high dimensional\nmask resulted in a four-fold speedup in training over the null hypothesis and a\ntwo-fold speedup in training over the layer normalization method.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}