{
  "id": "arxiv_2502.20969v1",
  "text": "TELERAG: Efficient Retrieval-Augmented Generation Inference\nwith Lookahead Retrieval\nChien-Yu Lin1,∗Keisuke Kamahori1,∗Yiyu Liu2,† Xiaoxiang Shi2,† Madhav Kashyap1 Yile Gu1\nRulin Shao1 Zihao Ye1 Kan Zhu1 Stephanie Wang1 Arvind Krishnamurthy1 Rohan Kadekodi1\nLuis Ceze1 Baris Kasikci1\n1University of Washington 2Shanghai Jiao Tong University\nAbstract\nRetrieval-augmented generation (RAG) extends large lan-\nguage models (LLMs) with external data sources to en-\nhance factual correctness and domain coverage. Modern RAG\npipelines rely on large datastores, leading to system chal-\nlenges in latency-sensitive deployments, especially when lim-\nited GPU memory is available. To address these challenges,\nwe propose TELERAG, an efficient inference system that re-\nduces RAG latency with minimal GPU memory requirements.\nThe core innovation of TELERAG is lookahead retrieval,\na prefetching mechanism that anticipates required data and\ntransfers it from CPU to GPU in parallel with LLM gener-\nation. By leveraging the modularity of RAG pipelines, the\ninverted file index (IVF) search algorithm and similarities be-\ntween queries, TELERAG optimally overlaps data movement\nand computation. Experimental results show that TELERAG\nreduces end-to-end RAG inference latency by up to 1.72×\non average compared to state-of-the-art systems, enabling\nfaster, more memory-efficient deployments of advanced RAG\napplications.\n1\nIntroduction\nRetrieval-augmented generation (RAG) has emerged as a pow-\nerful technique to enhance large language models (LLMs) by\nintegrating them with external databases [13, 29, 57]. Dur-\ning inference, RAG retrieves relevant content from external\ndata sources, usually indexed as vector datastores, to miti-\ngate issues such as hallucinations [50,63,74] and incorporate\nup-to-date or private information [36,64].\nModern RAG applications share two key characteristics.\n(1) RAG applications are built as modular pipelines as shown\nin Figure 1a, where a single query undergoes multiple rounds\nof LLM calls and retrievals, each of which serves different\n∗Equal contribution. †Work done during internship at UW. Correspon-\ndence to: Chien-Yu Lin <cyulin@cs.washington.edu>.\nPre-retrieval\ngeneration\nRetrieval\nPost-retrieval\ngeneration\n(a) Typical pipeline stages of a RAG application.\nPre-retrieval\ngeneration\nGPU\nCPU\nGPU\nRetrieval\nPost-retrieval\ngeneration\nIdeal Scenario (requires substantial GPU memory)\nPre-retrieval\ngeneration\nGPU\nCPU\nCPU Retrieval\nPost-retrieval\ngeneration\nTypical CPU-offload\nPre-retrieval\ngeneration\nGPU\nCPU\nCPU\nRetrieval\nPost-retrieval\ngeneration\nTeleRAG: Lookahead Retrieval\nPrefetch\nCPU → GPU\nLatency:\nCost:\nLow\nHigh\nLatency:\nCost:\nLatency:\nCost:\nHigh\nLow\nLow\nLow\nGPU\nRetrieval\n(b) Different scenarios for RAG and the illustration of the proposed\nlookahead retrieval mechanism. It prefetches relevant data for re-\ntrieval from CPU to GPU, overlaps data transfer with the pre-retrieval\nstage, and accelerates retrieval with GPU-CPU cooperation.\nFigure 1: (a) Illustrations of RAG pipeline stages. (b)\nOverview of TELERAG and comparison to baseline systems.\nfunctions to improve overall output quality. For example,\nquery transformation rounds [10, 27, 71, 100] generally oc-\ncur before retrieval to refine the user’s query with LLMs\n(pre-retrieval generation). (2) RAG’s datastores are typically\nlarge, supported by recent works demonstrating that increas-\ning the size of the datastore positively affects the performance\nof RAG applications [15,33,50,64,77].\nThese characteristics create significant challenges for ef-\nficient RAG inference, especially in latency-sensitive appli-\ncations such as customer chatbots [7,18,83], financial analy-\nsis [62,65], and emergency medical diagnosis [31,53]. First,\nthe latency of RAG systems is inherently increased due to\n1\narXiv:2502.20969v1  [cs.DC]  28 Feb 2025\n\n\nmultiple rounds of LLM generation and retrieval. Second,\nalthough GPU can speed up both LLM and retrieval, the com-\nbined memory demand of these processes often exceeds avail-\nable GPU resources, making it expensive or infeasible to fully\nhost both operations on GPU. Consequently, in local or small-\nscale deployments, which are common for RAG applications\nhandling private or sensitive data, the retrieval datastore is\nfrequently off-loaded to CPU memory to alleviate GPU con-\nstraints. However, while CPU off-loading resolves memory\nlimitations, it significantly increases retrieval latency, dimin-\nishing overall system efficiency.\nAddressing the latency challenge, several recent works\nhave been proposed to accelerate RAG inference. These\napproaches include FPGA-based retrieval acceleration [41],\nreuse of KV-cache [44, 60, 92], and speculative document\nretrieval [87,98]. However, none of these works directly ad-\ndress the substantial memory demands associated with re-\ntrieval, which remains a significant bottleneck for efficient\nRAG deployments.\nIn this paper, we identify a key opportunity in the widely\nadopted inverted file index (IVF) [80] retrieval method. IVF\nreduces retrieval latency by pre-clustering the datastore into\nsub-clusters, limiting the search space to relevant clusters of\nthe retrieval query during runtime. This approach also offers\na way to reduces GPU memory usage, as only a subset of\nclusters are needed for each retrieval query. For example, after\nidentifying relevant clusters, their data can be dynamically\ntransferred from CPU to GPU memory and accelerate the\nsimilarity searches on GPU. However, this approach incurs\nhigh CPU-GPU data transfer overhead, keeping the overall\nretrieval latency still high [49] (see §3).\nOur proposal. To tackle latency and memory bottlenecks\nin RAG inference, we introduce a novel mechanism called\nlookahead retrieval. Lookahead retrieval predicts which sub-\nsets of IVF clusters will likely be accessed during the retrieval\nphase and proactively transfers them from CPU to GPU dur-\ning the pre-retrieval generation stage.\nThe idea behind lookahead retrieval is based on the observa-\ntion that queries before and after the pre-retrieval generation\nstage share substantial semantic overlap. Consequently, IVF\nclusters relevant to the initial input query are also likely to\nbe relevant to the refined query. To validate this intuition, we\nrigorously analyze IVF cluster assignments across original\nqueries and refined queries produced by the pre-retrieval gen-\neration step, evaluating on six representative RAG pipelines\nand three datasets. Our analysis confirms a strong similarity\nbetween these two query forms (see §3).\nLeveraging this insight, we propose TELERAG, an efficient\ninference system designed to optimize RAG latency while\nminimizing GPU memory consumption. TELERAG employs\nlookahead retrieval to preemptively load relevant IVF clusters\nonto the GPU, effectively hiding CPU-GPU data transfer\noverhead during concurrent LLM generation. As illustrated\nin Figure 1b, this approach significantly reduces retrieval\nlatency without exceeding GPU memory constraints, enabling\nefficient execution of RAG applications in resource-limited\nenvironments.\nAlthough TELERAG significantly reduces latency by pro-\ncessing prefetched IVF clusters on the GPU, some relevant\nclusters might still be missed during prefetching. To maintain\nretrieval accuracy, TELERAG concurrently executes simi-\nlarity searches for these missed clusters on the CPU. The\nresults from CPU searches are seamlessly merged with GPU\nresults, ensuring complete retrieval precision. Thanks to the\nhigh cluster similarity between the initial and refined queries,\nthe number of missed clusters is typically small, allowing\nTELERAG to achieve low retrieval latency through efficient\nCPU-GPU coordination.\nA key challenge in deploying lookahead retrieval is de-\ntermining the optimal number of IVF clusters to prefetch.\nPrefetching too many clusters increases data-transfer over-\nhead, whereas fetching too few clusters could result in higher\nretrieval latency due to increased CPU processing. To address\nthis, we propose a profile-guided approach coupled with an an-\nalytical model that dynamically determines the ideal prefetch\namount based on pipeline characteristics and hardware config-\nurations. Our adaptive method consistently outperforms static\nconfigurations, delivering substantial latency improvements\nacross diverse RAG setups and hardware environments.\nResults summary. We evaluate TELERAG using a 61GB\nWikipedia-based datastore [5] across six popular RAG\npipelines built with the Llama model family [82] (3B, 8B,\nand 13B). Remarkably, TELERAG supports retrieval from\na 61GB datastore alongside a Llama3-8B (16GB) LLM\non a single RTX 4090 GPU (24GB memory), significantly\noutperforming the CPU retrieval baseline. Experiments con-\nducted on an RTX 4090 and an H100 GPU [68] demonstrate\nspeedups of up to 2.68x and 2.49x, with average speedups\nof 1.70x and 1.75x, respectively. These results highlight the\ncapability of TELERAG to efficiently handle large-scale RAG\ninference tasks within tight GPU memory constraints, con-\nfirming its practical value for resource-limited deployments.\nIn summary, we make the following key contributions:\n• Analyzing the correlation of the queries between the\npre-retrieval generation and retrieval stages, revealing\nsignificant overlap in their corresponding IVF clusters.\n• Proposing lookahead retrieval, which prefetches likely\nIVF clusters to the GPU, and hides CPU-GPU data trans-\nfer time during pre-retrieval generation.\n• Developing TELERAG, an efficient RAG inference sys-\ntem that integrates lookahead retrieval, resulting in sig-\nnificant acceleration of RAG with minimal GPU memory\nusage.\n2\n\n\nQuery\nLLM\nQuery\n(Refined)\nPre-retrieval\nGeneration\nLLM\nOutput\nPost-retrieval\nGeneration\nRetrieval\nJudgment\nDocuments\nQuery\nOutput\nRAG Application\nUser\nEmbedding Vectors\n(Clustered)\nCentroids\nVector Index (IVF)\nEmbed & Cluster\nIVF\nIndex\nRules/LLM\nFigure 2: Overview of RAG.\n2\nBackground\n2.1\nRAG\nRAG is a technique that enhances the capabilities of LLMs\nby integrating them with information retrieval to generate\nmore accurate and relevant text [13, 29, 57]. The core idea\nbehind RAG is to augment the LLM with relevant information\nretrieved from a large corpus of documents, improving the\nLLM’s ability to answer questions without hallucinations [50,\n63,74] and generate contents based on up-to-date or private\ninformation [36,64].\nRAG workflow. A basic RAG workflow involves several\nphases, including data store building, retrieval, and interac-\ntions with LLMs [23,24,28,34,70,84,88]. In order to build a\ndata store, raw data in various formats is cleaned, converted\nto plain text, and divided into chunks. These chunks are then\nencoded into vectors using an embedding model and stored\nin a vector index, enabling efficient searching based on sim-\nilarity. When a user provides a query, it is converted into a\nvector using the same embedding model, and the most similar\nchunks from the indexed database are retrieved. For a large-\nscale index, approximate algorithms such as the inverted file\nindex (IVF) [80] (see §2.2 for detail) are commonly used to\naccelerate the search process. The retrieved chunks, along\nwith the original user query, are combined and given as a\nprompt to the LLM. The LLM then generates a response that\nrelies on the information provided in the retrieved documents.\nThis approach enables dynamic and informative responses in\nboth single- and multi-turn dialogues [57].\nModularity in modern RAG applications. However,\nnaively integrating document retrieval into LLM generation\ncan cause several issues. For example, the retrieval often\nstruggles to find relevant content and may select irrelevant or\nmismatched chunks [27,61,100], and a single retrieval may\nnot provide sufficient context, necessitating multiple retrieval\nrounds [29].\nTo solve these issues, most state-of-the-art RAG models\nadopt a modular approach that employs multiple rounds of\nLLM calls and retrievals for a single query [23,26,28,29,34,\n88]. Typically, they have the following types of steps (shown\nin Figure 2):\n• Pre-retrieval generation, used to assess whether retrieval\nis needed or to generate queries for retrieval. An example\nof a pre-retrieval technique is query transformation [27,\n37,58,61,72,73,93,100,102], which reformulates the\noriginal query to make it clearer and more suitable for\nthe retrieval task.\n• Retrieval, used to identify relevant documents from the\nvector data store. This stage takes the output of pre-\nretrieval generation and generates data for the next stage.\n• Post-retrieval generation, generates response based on\nuser query and retrieved documents. It can also perform\nadditional process such as summarization [39, 51] or\nreranking [81,105] on the retrieved documents.\n• Judgment, dynamically determines the execution flow.\nFor example, it decides if more iteration is needed to\nenhance the response. Heuristics or LLMs can be used\nfor judgement stage.\nBy proceeding through these functions, RAG applications\ncan deliver more precise and contextually appropriate re-\nsponses, significantly enhancing the capabilities of LLMs\nfor various applications [29,45].\n2.2\nVector Index and Inverted File (IVF)\nThe vector index is a crucial component of RAG applica-\ntions that retrieves similar items from large datasets of high-\ndimensional vectors. Given the query vector x ∈RD and vec-\ntor database Y = {y0,...,yN−1} ⊂RD, which comprises N\nvectors, the vector index aims to find the k nearest neighbors\nof x from the database:\nk-argmini=0:Nd(x,yi),\nwhere D is the dimensionality of the vector determined by\nthe embedding model, and d denotes the distance function,\nwhich is typically the L2-norm or inner product [46].\n3\n\n\nTo improve search efficiency, the inverted file index (IVF)\nalgorithm is widely used for large-scale vector databases due\nto its simplicity and effectiveness. IVF partitions the data\nstore into clusters and restricts searches to the most relevant\nclusters, reducing the computational cost. To obtain the clus-\nter assignment of each stored vector, it performs clustering\nalgorithm such as k-means [66] and partitions the database\ninto Nc clusters:\n{C1,C2,...,CNc} = k-means(Y,Nc),\nwhere Cj is the set of vectors assigned to the j-th cluster, and\nthe cluster centroids {c1,c2,...,cNc} are obtained by taking\nthe mean of each vectors in {C1,C2,...,CNc}. Then, each\ndatabase vector yi ∈Y is assigned to the nearest cluster center:\nCluster(yi) = argmin j=1:Ncd(yi,c j).\nWith the trained centroids and cluster assignment, we can\nperform a more efficient index search. There are two steps\ninvolved in the IVF index searching. First, it will identify the\nnearest L cluster centroids of a query vector x:\n{cj1,c j2,...,cjL} = L-argminj=1:C d(x,cj).\nThis step is also referred to as the coarse-grained search in\nIVF, as it identifies candidates at the cluster level. Second, the\nfine-grained search is performed in the L nearest clusters and\nidentify k closest vectors of x:\nk-argminyi∈∪L\nl=1Cjl d(x,yi).\nwhich involves sorting. In this way, IVF reduces search space\nand accelerates the retrieval process. Here, the hyperparameter\nL from the first step is also referred as nprobe [25]. When\nnprobe is larger, the IVF retrieval will be more accurate as it\nsearch more clusters. However, the retrieve latency is longer\nas more computation and data are needed.\nSince the search process is highly parallelizable among\neach cluster and each vector, this search algorithm can be\nhighly accelerated by GPUs. Open-source libraries offer ef-\nficient GPU implementations [46, 75]. However, IVF does\nnot reduce the memory requirement for the index since the\ndata for all clusters must be stored. As a result, IVF still re-\nquires a substantial GPU memory footprint, which becomes a\nbottleneck when GPU memory capacity is the constraint of\nRAG systems. Moreover, recent work reported that increasing\nthe size of the data store positively affects RAG application\nperformance [15,33,50,64,77], exacerbating this issue.\n2.3\nLatency-Sensitive RAG Use Cases\nIn this paper, we focus on reducing inference latency for a\nsingle query, a critical objective in latency-sensitive RAG ap-\nplications. These applications span a range of real-world sce-\nnarios, such as customer chatbots [7,18,83], financial analy-\nsis [62,65], autonomous driving [17,94], and emergency med-\nical diagnosis [31,53], where users expect near-instantaneous\nHyDE\nSubQ\nIter\nIRG\nFLARE\nS-RAG\n0\n1\n2\n3\n4\nRatio w.r.t LLM time\nLLM\nCPU retrieval\nGPU retrieval\nFigure 3: Latency breakdown of six RAG pipelines on NQ\ndataset [54].\nresponses. In each of these contexts, maintaining low latency\nper query is paramount for providing timely and accurate\nresults.\nBatching multiple queries, a common strategy for im-\nproving throughput, is not considered in this work. It is\nbecause many RAG applications involve user-specific pri-\nvate data [21, 30, 55, 86, 89] and serve relatively small user\nbases, making batching infeasible. For large-scale RAG ser-\nvices [1,2,10], although batching is feasible, it increases the\nlatency for each individual queries [79,97,103] and therefore,\nis not ideal for the latency sensitive applications.\nA straightforward way to reduce inference latency is to\nscale up GPU resources, allowing both the LLM and retrieval\ndatastore to reside in GPU memory. However, this approach is\noften cost-prohibitive and could lead to significant hardware\nunderutilization, particularly in the single-query scenarios that\nthis work targets. Therefore, in this paper, we aim to improve\nsingle-query RAG latency without significantly increasing\nGPU memory usage, ensuring that both cloud-based and local\ndeployments can satisfy stringent performance requirements\nwithout incurring excessive costs.\n3\nAnalyzing Latency of RAG Pipelines\nIn this section, we analyze state-of-the-art RAG applications\nand identify their underlying systems challenges in achieving\nlow latency. To conduct the analysis, we construct a 61GB\nvector index with the FAISS library [25], and set the IVF\nclusters to 4096 following common practice [11]. We use\nLlama-3-8B [6] as the LLM, and run our analysis on an H100\nGPU with 80GB memory. §5.2 provides details about our\nexperimental setup.\n3.1\nEnd-to-end Latency of RAG Pipelines\nWe begin by analyzing the end-to-end latency of six represen-\ntative RAG pipelines (see §5.1 for details) in two scenarios. In\nthe first, the LLM runs on the GPU while retrieval is offloaded\nto the CPU, minimizing GPU memory usage. In the second,\nboth the LLM and the vector index reside in GPU memory,\nenabling GPU-based retrieval for lower latency. If the GPU’s\n4\n\n\n0\n20\n40\n60\n80\n100\nMemory (GB)\nGPU\nCPU\nGPU\nCPU\nRTX4090\nCPU-offload\nGPU-based\nLLM\nIndex\nMisc\nFigure 4: The breakdown of memory consumption at GPU\nand CPU for two different strategies: CPU offloading and\nGPU-based retrieval. The dotted line indicates the memory\ncapacity of a RTX4090 GPU, which is a common used GPU\nfor local deployment.\nmemory is insufficient, then CPU offloading is necessary.\nWe set the nprobe to 512, a common used setting under\nthis index’s configuration (see §5.2 for details), to measure\nretrieval latency. Figure 3 shows the breakdown of end-to-end\nlatency into LLM computation and retrieval. The results are\nthe average of 512 random data from the NQ dataset [54].\nWe observe that CPU-based retrieval (first scenario) is the\nprimary bottleneck, consuming about 56% of the total exe-\ncution time on average, while GPU-based retrieval (second\nscenario) accounts for just 12%. On average, GPU retrieval is\n12.5x faster than CPU retrieval, reducing overall latency by\napproximately 2.2x. Thus, accelerating retrieval on GPU is\ncrucial for improving end-to-end latency.\nHowever, GPU acceleration comes with a significant mem-\nory cost. Figure 4 shows the memory requirements for both\nGPU and CPU. As Figure 4 shows, putting both LLM weights\nand the retrieval index on the GPU requires around 77GB of\nmemory, which exceeds the capacity of consumer GPUs like\nthe RTX 4090 with 24GB. Thus, GPU acceleration on re-\ntrieval is often not feasible when running on a lower-end GPU\nor indexing with a large datastore. Note that the memory foot-\nprint of the LLM and the index can be even larger and further\nexacerbate the memory constraint.\nIn the rest of this section, we try to answer the following\nquestion: Is it possible to achieve the latency of GPU-based\nretrieval while using much less GPU memory?\n3.2\nOpportunities of GPU-accelerated Re-\ntrieval with Limited Memory\nA straightforward approach to enable GPU retrieval with lim-\nited GPU memory is to fetch the necessary data from CPU\nto GPU on-demand for each query, leveraging the IVF in-\ndex structure that narrows the target search clusters (§2.2).\nWhile such a method enables faster search on the GPU, the\nbottleneck of retrieval is shifted to the data fetching.\nTo examine the performance, we reconstruct the IVF index\nin PyTorch [8], leveraging its high-performance GPU oper-\nCPU\noffload\nGPU\nfetch\nCPU\noffload\nGPU\nfetch\nCPU\noffload\nGPU\nfetch\nNprobe\n0\n100\n200\n300\nTime (ms)\n128\n256\n512\nCPU Retrieval\nGPU Retrieval\nData Transmission\nFigure 5: Latency breakdown of CPU-offload and runtime-\nfetch GPU retrieval, averaged over 512 random NQ queries.\nDataset\nHyDE\nSubQ\nIter\nIRG\nFLARE\nS-RAG\n[27]\n[58]\n[58]\n[78]\n[43]\n[11]\nNQ [54]\n75.7%\n66.2%\n93.4%\n85.7%\n80.3%\n100.0%\nHotpotQA [91]\n78.6%\n66.6%\n86.9%\n91.2%\n82.2%\n100.0%\nTriviaQA [47]\n76.8%\n65.3%\n86.8%\n87.5%\n82.9%\n100.0%\nTable 1: IVF cluster overlapping rate between the input and\noutput of the pre-retrieval generation. The nprobe is set to\n512. Since Self-RAG does not incorporate query transform,\nits coverage is always 100%.\nator, and implement such an on-demand fetching retrieval\nsystem. We measure the retrieval latency with an RTX 4090\nGPU and compare it with the baseline of CPU-based retrieval.\nFigure 5 shows the breakdown of retrieval time into data\nfetch and search time. We show the performance for different\namounts of data fetched (indicated by the nprobe parame-\nter). We observe that the fetch time dominates the latency\ndue to limited PCIe bandwidth between the CPU and GPU\n(32 GB/s). Despite the fact that GPU retrieval is significantly\nfaster than CPU retrieval, due to the fetch overhead, the ef-\nfective latency is slower than CPU retrieval (around 3% slow\ndown on average across nprobe’s). Therefore, we must hide\nthe data fetching latency in order to achieve a meaningful\nspeedup to following such approach.\nHiding the data fetch cost. Hiding the data fetch cost\nimplies that CPU-to-GPU data fetch must be done before the\nretrieval operation is performed. However, predicting which\ndata to retrieve in advance is challenging since the exact data\nrequirements only become clear at retrieval time once we have\nthe query produced by the pre-retrieval process (§2.1).\nFortunately, as we show next, the original form of a query\nbefore pre-retrieval can serve as a valuable hint for the clus-\nters required to be retrieved at the end of the pre-retrieval\nstage. This hint can guide us in predicting the clusters that\nshould be fetched, and the data transfer can be initiated in the\nbackground along with pre-retrieval to hide its latency.\n5\n\n\n3.3\nOverlapping of IVF Clusters\nWhile the exact data to be retrieved is only known after the\npre-retrieval generation is done, we observe that there are\nhigh similarities of the IVF clusters assignments between the\nqueries at different stages.\nSimilarity of queries at different stages. During the pre-\nretrieval process of RAG pipelines (e.g., query transforma-\ntion [27,37,58,61,72,73,93,100,102]), an LLM call is issued\nto refine an initial user query qin into a transformed query qout,\nwhich is then used for retrieval. This process often rewrites\nthe query into a different format [27] or simplifies its com-\nplexities [58], which intuitively preserves the query’s core\nsemantic content. Hence, the embedding vectors of qin and\nqout are likely to be similar. This similarity, in turn, suggests\nthat the IVF clusters to which these queries would be assigned\nwill overlap. Therefore, qin can serve as a valuable hint for\npredicting subsequent memory accesses.\nPrediction coverage. To verify this hypothesis, we evalu-\nate the average cluster coverage rate between prefetched clus-\nters and clusters used for retrieval in three popular question-\nanswer datasets (NQ [54], HotpotQA [91], and TriviaQA [47])\nand six RAG pipelines. Table 1 shows the coverage when we\nprefetch 512 clusters. From the table, we observe that IVF\ncluster overlap rates are consistently high across a range of\ndatasets and pipelines. For instance, even the lowest reported\nvalues remain above 65% (for SubQ).\nOpportunity. This data shows an opportunity for predict-\ning required clusters, which can make it possible to hide data\ntransfer overhead during LLM generation. In this paper, we\naim to leverage this observation to accelerate the inference\nlatency for RAG.\n4\nDesign of TELERAG\nBased on the high IVF cluster overlapping observation pre-\nsented in § 3.3, we present TELERAG, an efficient inference\nsystem for RAG that incorporates the idea of lookahead re-\ntrieval, which predicts and prefetches likely IVF cluster data\nto GPU and significantly accelerates the retrieval process with\nminimal GPU memory requirement. In this section, we de-\nscribe our design overview (§4.1), an analytical model on\nfinding the optimal prefetch data amount (§4.3), and the im-\nplementation details (§4.4).\n4.1\nOverview\nFigure 6 presents the overview of TELERAG with lookahead\nretrieval. In Figure 6, the input to the pre-retrieval stage is\ndefined as qin, and the output is denoted as qout. In the subse-\nquent retrieval stage, qout serves as the input query to perform\nthe retrieval. We highlight the IVF clusters corresponding to\nqin with a red background (Cin), and those corresponding to\nqout with a blue background (Cout). Due to the semantic simi-\nlarity of qin and qout, there is significant overlap between Cin\nand Cout (§3.3), marked with a purple background (Coverlap).\nGiven this, the lookahead retrieval technique in TELERAG\noperates in three key steps. First, Cin is computed, and their\ndata is transferred to GPU memory during pre-retrieval gener-\nation, beginning with those clusters whose centroids are clos-\nest to qin. This is possible because GPUs are equipped with\nDirect Memory Access (DMA) engines, which can handle\nmemory transfers concurrently with compute operations. Ad-\nditionally, LLM generation is typically a compute-intensive\ntask that involves minimal data transfer between the device\nand host memory. Hence, lookahead retrieval can run seam-\nlessly alongside LLM generation.\nSecond, at retrieval stage, the GPU performs a fast similar-\nity search on the accurately predicted clusters (Coverlap) after\ndetermining Cout. Since similarity search essentially involves\ndot-product computations with high dimension embeddings,\nthis is highly efficient on the GPU.\nThird, simultaneously to the second step, the CPU pro-\ncesses the similarity search for remaining clusters of Cout\nthat were not prefetched and is stored on CPU memory. Al-\nthough CPUs are generally slower at performing similarity\nsearch, lookahead retrieval significantly reduces the workload\nassigned to the CPU, thereby accelerating the overall retrieval\nprocess.\nAdditionally, TELERAG optimizes the sorting of vector\ndistances, which is required for the k-argmin operation, the\nfinal step of the IVF search where the top-k indices are iden-\ntified. Since each cluster could have more than thousands of\nembedding vectors, sorting is much faster on the GPU than\non the CPU [9]. In order to perform the sorting on GPU, TEL-\nERAG passes the CPU computed distances of Cmiss to GPU\nfirst and then perform the global sorting on the combined\ndistances of Coverlap and Cmiss. Unlike the original vector data,\nthe distance values (such as L2-norm or inner product [46])\nare scalars per data point and thus, are much smaller in size.\nHence, transferring them to the GPU incurs minimal over-\nhead, allowing the GPU to achieve end-to-end speedup on\nthe sorting process. This GPU-based sorting optimization is\ntypically absent in CPU-based retrieval implementations [25]\nas they assume there is no GPU in the systems.\nIn summary, TELERAG optimizes retrieval by leverag-\ning lookahead retrieval to hide the data movement time for\nprefetching, GPU computation for efficient similarity search\nand sorting, and CPU processing for non-overlapping clusters,\nsignificantly accelerating the retrieval process.\n4.2\nPrefetching Design\nPrefetch target. In the early design of TELERAG, we use\nnumber of clusters as the prefetch target as it aligns with\nthe nprobe setting of the IVF retrieval process. With this tar-\nget, Despite we can configure the number clusters to prefetch\n6\n\n\nGPU\nCPU\nBaseline (CPU retrieval)\nTeleRAG (Lookahead Retrieval)\nPre-retrieval\ngeneration\nSearch in\nCout\nGPU\nCPU\nPre-retrieval\ngeneration\nSearch in\nPrefetch\nCout\nCin\nCoverlap\nqin\nPre-retrieval\ngeneration\nqout\nGPU\nCPU\nPost-retrieval\ngeneration\nVector\nIndex\nIdentify\nOverlap\nRetrieval\n(a)\n(b)\nPrefetch\nCmiss\nCin\nSearch in\nRetrieval\nRetrieval\nRetrieval\nFigure 6: (a) The overview of TELERAG’s lookahead retrieval and comparison to the baseline CPU-offloaded retrieval. (b)\nSystem design of TELERAG. It first loads the entire index data into CPU memory and identifies the clusters (Cin) for the\nquery before the transformation (qin). It then transfers the data of Cin to GPU memory while running the pre-retrieval stage and\ngenerating the retrieval query qout. During the search, the overlapped clusters are searched on GPU, and the missed clusters are\nsearched on CPU. The retrieval results are then merged and passed to the next stage.\nto adapt to each pipeline’s characteristics, we found this target\nbrings an unstable inference performance. It’s because the\nsize of each cluster is often heavily skewed [44,59], making\ndata loading time for some clusters much longer than the\nothers. To solve this, we use number of bytes, bp, as the\nprefetch target. With bp as the prefeth target, it provides an\nupper bound on prefetching time at bp\nB , where B stands for\nthe CPU-GPU memory bandwidth.\nTo perform prefetching, we first sort the clusters by the\ndistance between their centroids and the query embedding\nvector. Starting with the nearest cluster, we include clusters\none by one until adding another would exceed the target bp.\nWe do not split clusters to completely fill bp, ensuring a clear\nseparation between GPU and CPU searches.\nPrefetch for Multi-Round RAG. As discussed in §2.1,\nRAG pipelines often run through multiple rounds. Because\neach round still processes for the same input query, the IVF\nclusters used in retrieval are highly similar across rounds. In\nTELERAG, we therefore perform a full prefetch only for the\nfirst round, then fetch only the additional IVF clusters needed\nfor subsequent rounds, until reaching the memory budget.\nWe currently flush the prefetched data for each new query,\nassuming no correlation across queries. We leave more ad-\nvanced memory management and identification of hot clusters\nbetween queries for future work.\n4.3\nFinding the Optimal Prefetch Amount\nA key challenge in TELERAG is to balance the benefit of\nreducing retrieval latency by prefetching data against the over-\nhead of CPU-GPU transfers. Prefetching more clusters re-\nduces the subsequent retrieval time but can also extend the\ntransfer phase; if it extends beyond the LLM generation win-\ndow, we lose the advantage of overlap and potentially intro-\nduce additional delay. Nevertheless, even additional delay in\ntransferring data can be worthwhile if it substantially reduces\nretrieval latency.\nMathematical model. We develop a mathematical model\nto guide the choice of the optimal amount of data to prefetch.\nHere, we follow the notations from §4.2 where bp is bytes to\nprefetch and B is the CPU-GPU bandwidth. Optimal amount\nof data to prefetch is denoted as b∗\np.\nTo start, we let t1 represent the combined time of prefetch-\ning and pre-retrieval LLM generation, and t2 represents the\nretrieval time. We have:\nt1 = max(tLLM,tp),\n(1)\nt2 = max(tc,tg),\n(2)\nwhere tLLM is the time for LLM generation, tp is prefetching\ntime, tc is the CPU retrieval time, and tg is the GPU retrieval\ntime. The objective is to minimize t1 +t2.\nSince prefetching time tp is proportional to bp, we express\nt1 as a piecewise function:\nt1 =\n\n\n\ntLLM,\nif bp ≤B·tLLM,\nbp\nB ,\nif bp > B·tLLM,\n(3)\nFrom Eq. 3, if we prefetch fewer bytes than can be transferred\nduring LLM generation, t1 is effectively just tLLM because the\ntransfer overlaps completely with LLM execution.\n7\n\n\nAs shown in §3.1, GPU retrieval (tg) is generally much\nfaster than CPU retrieval (tc). Thus, we assume tc ≫tg. As\nCPU has a limited parallelism, tc usually grows proportionally\nwith the number of clusters to be processed [40], such that:\nt2 = tc = rmiss ×nprobe ×tcc,\n(4)\nwhere rmiss is the miss rate (percentage of IVF clusters are\nnot caught on GPU), nprobe is the total number of clusters\nto search, and tcc is the CPU time to search a single cluster.\nIncreasing bp can only decrease or maintain the current miss\nrate, i.e., drmiss\ndbp ≤0. Moreover, because clusters are prefetched\nin order of descending likelihood, we assume rmiss is either\nlinear or a concave up function2 of bp, i.e., d2rmiss\ndb2p\n≥0.\nWe now examine two cases:\n• Case 1: b∗\np ≤B ·tLLM. Here, t1 = tLLM is constant be-\ncause prefetching is fully overlapped with LLM genera-\ntion. Since increasing bp in this regime will not increase\nt1 and cannot worsen the miss rate, pushing bp to the\nboundary B·tLLM minimizes t1 +t2. Hence,\nb∗\np = B·tLLM.\n(5)\n• Case 2: b∗\np > B·tLLM. In this region, t1 grows linearly\nwith bp, and we have: d2\ndb2p (t1 +t2) = d2rmiss\ndb2p\n·nprobe ·tcc ≥\n0. Therefore, t1 +t2 is concave up, allowing at most one\nminimum. At the minimum point, we have:\nd\ndbp\n(t1 +t2) = 0 =⇒1\nB + drmiss\ndbp\n·nprobe ·tcc = 0. (6)\nFrom this, we obtain:\nb∗\np = B×nprobe ×tcc ×∆rmiss,\n(7)\nwhere ∆rmiss is the decrement of the miss rate for this\nround. If b∗\np is indeed larger than B ·tLLM, it becomes\nthe global minimum; otherwise, the solution reverts to\nCase 1.\nIn summary, our analysis shows that the optimal prefetch\namount b∗\np can only lie at one of two points: (1) Prefetch\nuntil LLM generation completes (i.e. bp = B ·tLLM). (2) A\npoint determined by Eq. 7. However, under typical CPU-GPU\nbandwidth (e.g., 55 GB/s on PCIe 5), the time spent loading\nadditional clusters often outweighs any retrieval latency reduc-\ntion from lowering rmiss. Consequently, the second scenario\nin Eq. 7 becomes nearly infeasible in practice. Therefore, on\ncurrent hardware, prefetching exactly until LLM execution\nends is generally the most effective choice.\nProfiling-guided approach. Although b∗\np can be derived\nfrom the analysis above, it depends on knowing tLLM for\n2An upward U-shaped function whose second derivative is positive.\neach query, which cannot be obtained ahead of time. To ad-\ndress this, we use a profiling-guided approach, leveraging\nthe observation that, despite differences in query content, the\noutput length (and thus generation time) for a RAG pipeline\noften remains similar across most queries. Accordingly, for\neach RAG pipeline, we can measure tLLM on a calibration set\ncontaining n queries, and get estimated ˆb∗\np = B·tLLM, where\ntLLM = mean{tLLM,1,tLLM,2,...,tLLM,n}. This estimated ˆb∗\np\ncan then be used for incoming queries, ensuring a near-\noptimal prefetch amount.\n4.4\nImplementation\nTELERAG is implemented in Python and leverages Py-\nTorch’s [8] operator ecosystem for efficient computation. The\ndatastore index is initially constructed using FAISS [25], and\nits data structures, such as IVF centroids and cluster data, are\nconverted to PyTorch tensors. These tensors are stored on\ndisk to eliminate the need for repeated conversions.\nAt runtime, cluster data is loaded into a contiguous pinned\nmemory region on the CPU, enabling non-blocking memory\ncopies to the GPU. A fixed-size contiguous buffer on the GPU\nis allocated based on the user’s configuration or GPU memory\ncapacity during runtime.\nTo\nenable\nconcurrent\nCPU-GPU\ndata\ntransfer-\nring\nand\nLLM\ngeneration, we\nutilize\nPyTorch’s\n_copy(non_blocking=True)\nAPI\nand\nuse\nseparate\nCUDA streams for prefetching and LLM. In this way, the\ndata copy operations will not block the GPU to perform\ncomputation, and thus the TELERAG can do prefetching in\nparallel to the pre-retrieval LLM generation.\nTo implement the index search with GPU-CPU cooperation,\nfor the GPU part, we use a single matrix-vector multiplica-\ntion that computes distances for all prefetched vectors; for\nthe CPU part, we utilize multithreading in Python to par-\nallelize similarity searches across clusters. We then move the\ndistances computed from CPU to GPU, merge with distances\non GPU and perform global sorting on GPU.\n5\nEvaluation\nWe conduct extensive experiments to evaluate the effective-\nness of TELERAG. In this section, we describe the necessary\ndetails on how we set up the evaluations, present experiments\nresults and provide in-depth analysis and discussions.\n5.1\nDatasets and RAG Pipelines\nDatastore. We build a datastore based on the wiki_dpr\ndataset [48], which is a popular dataset contains 2.1 billions\ntokens from Wikipedia. Following previous works [11, 48,\n64], we chunk the passages by every 100 tokens, and use\n3Original embedding without compression for the best retrieval precision.\n8\n\n\nSpecification\nValue\nDataset\nwiki_dpr [48]\nDataset size\n2.1 billion tokens\n# of chunks\n21 million\n# of IVF cluster\n4096\nEmbed model\nContriever [35]\nEmbed dimension\n768\nIndex type\nFLAT3\nDistance metric\nInner Product\nIndex size\n61GB\nTable 2: Detailed configurations of our retrieval index.\nContriever [35] to generate embedding for each chunk. The\nembeddings have a hidden dimension of 768.\nVector index. For the baseline retrieval, we build an IVF\nvector index using FAISS [25] from the datastore, following\nsetups from [11]. See Table 2 for the detailed configurations\nof our vector index. As described in §4.4, we then convert the\nFAISS index to a customized index in PyTorch for TELERAG.\nLLMs. We evaluate TELERAG on the Llama model fam-\nily [82] in three different sizes (Llama-3.2-3B, Llama-3-8B,\nLlama-2-13B) to represent different use cases.\nRAG pipelines. We evaluate TELERAG with six popular\nRAG pipelines. Figure 7 shows the overview of each pipeline.\nNote that even though some pipelines lack pre-retrieval gen-\neration, the post-retrieval generation serves similar function-\nality for the retrieval of the next iteration. Below are brief\ndescriptions to evaluated RAG pipelines. For iterative-based\npipelines, we set the maximum iterations to 3.\n1. HyDE [27] prompts LLM to generate a hypothetical\nparagraph and perform retrieval based on the embedding\nof the generated paragraph.\n2. SubQestion (SubQ)4 [58] prompts LLM to generate\nmultiple sub-questions and performs retrievals for each\ngenerated sub-question.\n3. Iterative (Iter)5 [58] prompts LLM to generate narrower\nquestions first and iteratively refine them based on pre-\nvious answers. At the end of each iteration, it prompts\nLLM to judge if the answer is good enough.\n4. Iter-RetGen (IRG) [78] iteratively do retrieval and\nLLM generation for 3 iterations.\n5. FLARE [43] iteratively issues retrievals based on the\nconfidence (probability score) of predicted tokens for\nthe upcoming sentence.\n4Implemented in LlamaIndex as SubQuestionQueryEngine\n5Implemented in LlamaIndex as MultiStepQueryEngine\nPre-retrieval\nGeneration\nRetrieval\nPost-retrieval\nGeneration\nPre-retrieval\nGeneration\nRetrieval\nPost-retrieval\nGeneration\n(1) HyDE Pipeline\n(3) Iterative Pipeline\nPre-retrieval\nGeneration\nRetrieval\nPost-retrieval\nGeneration\n(2) SubQuestion Pipeline\nRetrieval\n(5) FLARE Pipeline\n(6) Self-RAG Pipeline\nRetrieval\nPost-retrieval\nGeneration\nCritique\nPre-retrieval\nGeneration\nRetrieval\nPost-retrieval\nGeneration\n(4) Iter-RetGen Pipeline\nRetrieval\nPost-retrieval\nGeneration\nPre-retrieval\nGeneration\nPost-retrieval\nGeneration\nNon-LLM\nOperation\nRetrieval\nJudgment\nJudgment\nJudgment\nJudgment\nMax\nIteration?\nMax\niteration?\nMax\niteration?\nFigure 7: Overview of six RAG pipelines that we evaluate.\n6. Self-RAG (S-RAG)6 [11] decides whether to retrieve\ndocuments and then answers for each query. A critique\nstage selects the most confident answer. We use fine-\ntuned models based on Llama2-7B and Llama2-13B\nfrom their official implementation [12].\nEvaluation Datasets. We evaluate on three commonly\nused question-answering datasets, NQ [54], HotpotQA [91]\nand TriviaQA [47]. For each dataset, we randomly sample\n512 queries and report the average unless otherwise specified.\n5.2\nExperiment Setups\nHardware setups. We evaluate TELERAG on two hard-\nware environments, Desktop and Server, which equiped\nwith represent the settings for the desktop and cloud instance\nuse case. For Desktop, it’s paired with a RTX 4090 GPU\n(24GB memory) and we consider the 3B and 8B models. For\nServer, it’s paired with a RTX 4090 GPU (24GB memory)\nand we consider the 8B and 13B models. These sizes are\ncommon model size for RAG applications [10, 11]. We do\nnot evaluate the 13B model on Desktop as its model size (26\nGB) exceeds RTX4090’s memory space. Table 3 summaries\nthe hardware configurations.\nNprobe and top-k. We evaluate latency with three nprobe\nvalues, 128, 256, and 512, which represent 3.1%, 6.3%, and\n12.5% of the total clusters in our evaluated index. This range\nfalls into the commonly used settings of IVF index [25,106].\n6We evaluate on the short-form version which only has one iteration.\n9\n\n\nSetup\nDesktop\nServer\nCPU\nTR 5975\nEPYC 9554\nCPU memory size\n512GB\n1.5TB\nGPU\nRTX4090 [67]\nH100 [68]\nGPU memory size\n24GB\n80GB\nCPU-GPU Bus\nPCIe 4\nPCIe 5\nBandwidth\n32 (24) GB/s\n64 (51) GB/s\nTable 3: Hardware specifications for our setups. In bandwidth,\nthe number in the parentheses is the actual bandwidth we\nmeasured from our system.\nWe use a top-k number of 3 for retrieval, which returns 3 most\nrelevant passages based on the retrieval query.\nRAG pipeline implementation. To collect the input and\noutput texts for each stage and have consistent results across\nall pipelines, we implement the RAG pipelines with the\nFlashRAG framework [45]. For IRG, FLARE, and S-RAG,\nwe use the framework’s default implementations; for the other\npipelines, we re-implement them using FlashRAG’s APIs.\nBenchmark methodology. We follow a benchmarking\nmethodology from [101] and use GPT-3.5-Turbo [69] to run\nthrough each pipeline and record the input and output text of\neach step. During latency evaluation, we set LLM’s output\nlength based on the recorded text. This way, we ensure a fair\nlatency comparison across different LLM models.\nBaseline systems. To evaluate the latency of each pipeline,\nwe construct a clean execution flow in Python that only con-\ntains LLM generation, datastore retrieval, and other necessary\nlogical operations to fulfill each pipeline. For LLM genera-\ntion, we use SGLang [101], which is a state-of-the-art LLM\ninference engine.\nPrefetching budget setups. Based on the methodology\nwe described in §4.3, we profile each RAG pipeline with 64\nrandom samples from NQ [54] and define the prefetch budget\nof each pipeline.\nMax prefetching memory. We set a maximum GPU mem-\nory limit for prefetching in each configuration. For Server\nwith H100, we allocate up to 16GB, sufficient for a high\ncluster hit rate at the largest tested nprobe value (512). For\nDesktop with RTX4090, we allocate up to 10GB and 3.75GB\nfor the 3B and 8B models, respectively. These settings demon-\nstrate that TELERAG can efficiently operate using only a\nsmall fraction (up to 40% for RTX4090 and 20% for H100)\nof total GPU memory.\n5.3\nLatency Evaluation\nBaseline retrieval. Table 4 reports the retrieval precision\nand the baseline CPU retrieval latency of different nprobe.\nTo conduct this test, we use 1024 random queries from NQ.\nFor retrieval precision, we compare against the top-10 results\nMetric\nHardware\nnprobe\n128\n256\n512\nPrecision\n-\n95.9%\n97.6%\n98.9%\nCPU Latency (ms)\nDesktop\n89\n176\n349\nServer\n64\n126\n251\nTable 4: Retrieval precision and baseline latency of FAISS for\ndifferent nprobe values. Precision is measured by comparing\ntop-10 results against greedy search without IVF.\nfrom the greedy search. For the latency, we report the average\nretrieval time. Due to the limited scalability of the CPU, a\nhigher nprobe directly leads to a longer retrieval latency even\nwhen using the state-of-the-art implementation from FAISS.\nEnd-to-end latency. We evaluate the end-to-end latency\nof all the RAG pipelines and datasets under all the hardware\nsettings described in §5.2. Our comprehensive end-to-end\nspeedup results are presented in Figure 8.\nWe can observe that TELERAG improves the execution\nspeed consistently with various configurations. Across differ-\nent pipelines, datasets, and model/hardware setups, the aver-\nage speedup is 1.21x, 1.41x, and 1.72x for nprobe values of\n128, 256, and 512, respectively. We observe a more significant\nspeedup for larger nprobe, as increased nprobe settings incur\nlonger retrieval latencies.\nIn the best case, we observe around 2.6x speedup (for Iter-\nRetGen, HotpotQA, Llama-3.2-3B, on RTX4090). This is\nbecause the Iter-RetGen pipeline employs a retrieval step\nin every iteration and has a short LLM output in general,\nproviding a larger end-to-end speedup margin for accelerating\nthe retrieval part.\nAnother pipeline that has a particularly high speedup is the\nSubQuestion pipeline, for which TELERAG achieves around\n2.5x speedup across all the datasets with Llama-3.2-3B on\nRTX4090. This pipeline uses LLM to generate multiple sub-\nquestions and perform a batched retrieval with 3 to 4 gen-\nerated queries at one time. The CPU retrieval performance\nof batched retrieval is significantly worse than that of GPU,\nas it has limited parallelism. In TELERAG, we handle the\nbatched retrieval efficiently by leveraging GPU’s high parallel\ncomputing capacity and, therefore, achieve a high speedup\nfor the SubQuestion pipeline.\nOverall, the performance gain is stable across different\ndatasets, with less than 3% of variance across datasets on\naverage. For a given GPU, TELERAG has a higher speedup\nwith a smaller LLM because retrieval time will account for\na larger fraction of the end-to-end latency. Still, we have\ntangible speedup in RTX4090 with Llama-3-8B model (e.g.,\nmore than 1.5x when nprobe is 512, on average), even though\nprefetching memory budget is only 3.75GB (space left after\nholding LLM, embedding model, IVF cluster centroids and\nmisc tensors on GPU) in this case.\n10\n\n\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\nNQ\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nHotpotQA\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTriviaQA\nNprobe 128\nNprobe 256\nNprobe 512\n(a) End-to-end latency speedup with Llama-3.2-3B with a single RTX4090 GPU.\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\nNQ\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nHotpotQA\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTriviaQA\nNprobe 128\nNprobe 256\nNprobe 512\n(b) End-to-end latency speedup with Llama-3-8B with a single RTX4090 GPU.\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\nNQ\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nHotpotQA\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTriviaQA\nNprobe 128\nNprobe 256\nNprobe 512\n(c) End-to-end latency speedup with Llama-3-8B with a single H100 GPU.\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\nNQ\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nHotpotQA\nHyDE SubQ\nIter\nIRG\nFLARE S-RAG\nAvg\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTriviaQA\nNprobe 128\nNprobe 256\nNprobe 512\n(d) End-to-end latency speedup with Llama-2-13B with a single H100 GPU.\nFigure 8: End-to-end latency speedup of TELERAG and baseline on six RAG pipelines, three datasets, with H100 and RTX4090\nGPU.\nSpeedups on retrieval. Figure 9 shows the latency\nspeedups on retrieval on NQ. We observe a consistent\nspeedups for all the nprobe numbers but notice the greatest\nspeedups of retrieval often occur at nprobe 256, with average\nspeedups of 7.21x and 7.41x on RTX4090 and H100. This is\nbecause, with a larger nprobe, the retrieval performance of\nTELERAG will start to be bounded by the missed clusters on\nthe CPU (we have a fixed prefetch budget of different nprobe\nnumbers). Nonetheless, TELERAG still has a large latency\nimprovement against the pure CPU baseline.\n5.4\nAnalysis\nLatency breakdown and GPU compute interference. We\nfurther show the latency breakdown of running RAG pipelines\nwith Llama-3-8B and Llama-2-13B on Server with H100,\nexamining the case when the GPU’s memory capacity is\nenough for a good amount of prefetching. Figure 10 shows\nthe breakdown of latency into LLM time and retrieval time.\nWe compare the baseline’s retrieval latency (FAISS CPU)\nwith TELERAG’s retrieval latency in three different nprobe\nnumbers.\nThe first thing we observe is that, although we set the\nprefetching budget to the amount that the data loading time\ncan overlap with the pre-retrieval generation time, there is\na slight slowdown in LLM generation time for TELERAG.\nThis is potentially because GPU’s compute performance will\nbe impacted when there is a memory-loading workload, as\nnoticed in [103]. However, such impact is minimal, with less\n11\n\n\nHyDE\nSubQ\nIter\nIRG\nFLARE\nS-RAG\nAvg\n0\n2\n4\n6\n8\n10\n12\nSpeedup\nNprobe 128\nNprobe 256\nNprobe 512\n(a) On Desktop with a RTX4090 GPU.\nHyDE\nSubQ\nIter\nIRG\nFLARE\nS-RAG\nAvg\n0\n2\n4\n6\n8\n10\n12\nSpeedup\n(b) On Server with a H100 GPU.\nFigure 9: Retrieval speedup on NQ.\nPipeline\nH100 (Llm3-8B)\n4090 (Llm3-3B)\nBudget\nHit Rate\nBudget\nHit Rate\nHyDE\n9 GB\n77.23%\n7 GB\n67.16%\nSubQ\n8 GB\n63.26%\n7 GB\n58.38%\nIter\n5 GB\n80.79%\n3 GB\n61.46%\nIRG\n4 GB\n44.49%\n2.5 GB\n29.39%\nFLARE\n6 GB\n58.17%\n3 GB\n33.01%\nS-RAG\n3 GB\n36.01%\n1.25 GB\n14.98%\n0%\n20%\n40%\n60%\n80%\n100%\nTable 5: The prefetch budget and corresponding averaged\ncluster hit rate for each pipeline and hardware setup on NQ\ndataset. The target retrieval nprobe is 512.\nthan 5% slowdown in general. We observe higher impacts\nfor Iter-RetGen and Self-RAG with Llama-3-8B, which TEL-\nERAG’s LLM geeration time is 28% and 20% longer than\nthe baseline. It’s because these two pipelines have a relatively\nshorter LLM generation time, and thus, are more vulnerable\nto interference on GPU compute. However, with the signif-\nicant amount of speedups on retrieval, TELERAG achieves\nconsistent speedups across all the pipelines despite having\nthis small impact on the LLM generation.\nPrefetch budgets and cluster hit rates. Table 5 shows the\nprefetch budgets we set with the profiling-guided approach\non RTX4090 and H100 for NQ. It also presents the averaged\ncluster hit rate achieved with this prefetch budget. From the\ntable, we can see that it can generally attain a high cluster hit\nrate (> 50%) for the cases where it can hold a larger amount of\ndata for prefetching. For pipelines or scenarios that can hold\nfor less than 3GB prefetching amount, we generally observe\nbase ours\nbase ours\nbase ours\nbase ours\nbase ours\nbase ours\nPipeline\n0\n500\n1000\n1500\n2000\nTime (ms)\nHyDE\nSubQ\nIter\nIRG\nFLARE\nS-RAG\nFaiss nprobe 128\nFaiss nprobe 256\nFaiss nprobe 512\nLLM\nTeleRAG nprobe 128\nTeleRAG nprobe 256\nTeleRAG nprobe 512\nMisc\n(a) With Llama-3-8B.\nbase ours\nbase ours\nbase ours\nbase ours\nbase ours\nbase ours\nPipeline\n0\n500\n1000\n1500\n2000\nTime (ms)\nHyDE\nSubQ\nIter\nIRG\nFLARE\nS-RAG\nFaiss nprobe 128\nFaiss nprobe 256\nFaiss nprobe 512\nLLM\nTeleRAG nprobe 128\nTeleRAG nprobe 256\nTeleRAG nprobe 512\nMisc\n(b) With Llama-2-13B.\nFigure 10: Latency breakdown for NQ on Server with a\nH100 GPU.\na relatively lower hit rate (< 35%), limiting our benefits of\nreducing CPU’s search workloads. However, as observed from\nFigure 8, TELERAG achieves from 1.2x to 1.5x end-to-end\nspeedups for these pipelines. This is the combined benefit of\nreducing CPU workload and utilizing GPU to perform sorting\non similarity distances, demonstrating the strong performance\nof TELERAG across a wide range of scenarios.\nAnalysis on the cluster hit rate. We measure cluster hit\nrates across a wide range of prefetch amounts, examining sce-\nnarios with higher CPU-GPU bandwidth and sufficient GPU\nmemory to accommodate larger prefetches. Our experiments\nare conducted on the HyDE pipeline with nprobe set to 512\nand prefetching from 32 to 1024 clusters. We use nprobe as\nthe prefetch metric (instead of bytes) to facilitate comparisons\nwith optimal hit rates. We evaluate on NQ, HotpotQA, and\nTriviaQA, reporting average results in Figure 11.\nWe have three observations from Figure 11. (1) From 0%\nto 50% of nprobe, accuracy remains high, illustrating that the\nprobability of hitting relevant clusters decreases by order of\nprefetching. (2) Between 50% and 150% of nprobe, coverage\nstill increases rapidly but is slower than the first range. (3)\nAfter 150% of nprobe, coverage plateaus at roughly 95%.\nThese results suggest that prefetching the amount beyond the\ntarget nprobe is likely unnecessary, as the coverage increase\nis limited.\n12\n\n\n64\n128\n256\n512\n768\n1024\nNumber of Prefetched Clusters\n0%\n20%\n40%\n60%\n80%\n100%\nCluster Coverage\nOptimal\nPrediction All Correct\nFigure 11: The cluster coverage rate for HyDE with different\nnumbers of prefetched clusters. The target retrieval nprobe is\n512. Results are the average of NQ, HotpotQA and TriviaQA.\n6\nRelated Work\n6.1\nSystems for RAG\nSeveral prior works have attempted to build systems for RAG.\nRAGCache [44] proposes a caching system that stores KV\ncaches from datastore chunks in an order-aware manner to\nimprove the time to the first token. However, this approach\nonly reduces prefill latency, leaving retrieval and decode times\nunaffected, despite the fact they typically dominate total la-\ntency [4]. Moreover, it assumes repeated use of the same doc-\nument across multiple requests, limiting its scalability over\nlarge data stores. Similarly, TurboRAG [60] precomputes KV\ncaches from the data store, but it also only optimizes prefill\nlatency.\nRaLMSpec [98] proposes speculative retrieval and batched\nverification, Chameleon [41] proposes a CPU-GPU-FPGA\nheterogeneous architecture for accelerating the retrieval pro-\ncess, and PipeRAG [42] proposes an algorithm-system co-\ndesign technique to overlap the retrieval and generation pro-\ncess by modifying the algorithms. However, these works fo-\ncus on the paradigm of RAG that retrieves documents once\nevery few tokens and are not applicable to modular RAG\napplications that are widely used now, as discussed in §2.1.\nSpeculative RAG [87] introduces drafting by smaller LLMs\nto reduce RAG latency, but it does not target the retrieval\nlatency. APIServe (InferCept) [3] proposes a novel KV cache\nmanagement strategy that can support the interception of LLM\ngeneration by other workloads, including retrieval. However,\nthis work again does not focus on optimizing retrieval latency.\nUnlike all these prior works, we tackle system challenges\nof long retrieval latency and large memory requirement for\nmodular RAG pipelines with large-scale datastores.\n6.2\nSystems for Compound LLM Applications\nApart from RAG, there is a growing interest in compound\nor agentic LLM applications, where multiple LLM calls and\nother applications are combined to serve complex functionali-\nties [14,85,95]. LLMCompiler [52] is a framework that opti-\nmizes the execution of multiple functions in large language\nmodels by enabling parallel function calling. AI Metropo-\nlis [90] accelerates LLM-based multi-agent simulations with\nout-of-order execution. ALTO [76] improves the performance\nof complex AI systems by streaming partial results. Some\nopen-source libraries provide graph-based representation for\nprogramming this kind of application [22,56]. RAG is a spe-\ncific type of application in this broader direction, and we\npropose systems techniques to optimize its execution latency,\nfocusing on the characteristics of retrieval workload.\n6.3\nVector Index\nVector index is gaining growing attention as an essential com-\nponent of RAG [16,29], and many works have been proposed\nto improve the efficiency with system or algorithm techniques.\nScaNN proposes an anisotropic quantization method for bet-\nter efficiency [32]. DiskANN and SPANN propose memory-\ndisk hybrid indexing systems that work beyond the limita-\ntion of memory capacity [19,38]. Other prior works propose\nhardware acceleration of vector index with GPUs [46], FP-\nGAs [40, 96], TPU [20], or ray tracing hardware [59, 104].\nBANG proposes a method to scale graph-based ANN be-\nyond GPU memory [49], and Rummy allows the index to\nscale beyond GPU memory capacity with reordered pipelin-\ning [99]. However, these methods either require algorithm\nmodifications or are bottlenecked by CPU-GPU bandwidth.\nOur proposal, focuses on the context of modular RAG ap-\nplications, where queries for retrieval are usually generated\nby LLMs, and optimizes the latency and the GPU memory\nconsumption without altering the algorithm of the IVF index.\n7\nConclusion\nIn this paper, we introduced TELERAG, an inference sys-\ntem that tackles the system challenges of RAG pipelins\nunder latency-sensitive scenarios. By using lookahead re-\ntrieval, which overlaps data transfer and GPU computation\nfor faster retrieval, a profile-guided approach to determine\noptimal prefetching data amount, and GPU-CPU coopera-\ntion, TELERAG speeds up the end-to-end latency with min-\nimal GPU memory requirement. Our evaluation shows that\nTELERAG significantly improves performance compared to\nexisting state-of-the-art solutions.\nReferences\n[1] Genspark. https://www.genspark.ai/.\n[2] Perplexity. https://www.perplexity.ai/.\n[3] Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao\nZhang, and Yiying Zhang.\nApiserve: Efficient api\n13\n\n\nsupport for large-language model inferencing. arXiv\npreprint arXiv:2402.01869, 2024.\n[4] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree\nMohan, Nipun Kwatra, Bhargav S Gulavani, Alexey\nTumanov, and Ramachandran Ramjee.\nTaming\nthroughput-latency tradeoff in llm inference with\nsarathi-serve. arXiv preprint arXiv:2403.02310, 2024.\n[5] AI@Meta. Dataset card for “wiki_dp”. https://\nhuggingface.co/datasets/facebook/wiki_dpr,\n2020.\n[6] AI@Meta.\nLlama 3 model card.\nhttps:\n//github.com/meta-llama/llama3/blob/main/\nMODEL_CARD.md, 2024.\n[7] Rama Akkiraju, Anbang Xu, Deepak Bora, Tan Yu,\nLu An, Vishal Seth, Aaditya Shukla, Pritam Gundecha,\nHridhay Mehta, Ashwin Jha, et al. Facts about building\nretrieval augmented generation-based chatbots. arXiv\npreprint arXiv:2407.07858, 2024.\n[8] Jason Ansel, Edward Yang, Horace He, Natalia\nGimelshein, Animesh Jain, Michael Voznesensky, Bin\nBao, Peter Bell, David Berard, Evgeni Burovski, Geeta\nChauhan, Anjali Chourdia, Will Constable, Alban Des-\nmaison, Zachary DeVito, Elias Ellison, Will Feng,\nJiong Gong, Michael Gschwind, Brian Hirsh, Sher-\nlock Huang, Kshiteej Kalambarkar, Laurent Kirsch,\nMichael Lazos, Mario Lezcano, Yanbo Liang, Jason\nLiang, Yinghai Lu, C. K. Luk, Bert Maher, Yunjie\nPan, Christian Puhrsch, Matthias Reso, Mark Saroufim,\nMarcos Yukio Siraichi, Helen Suk, Shunting Zhang,\nMichael Suo, Phil Tillet, Xu Zhao, Eikan Wang, Keren\nZhou, Richard Zou, Xiaodong Wang, Ajit Mathews,\nWilliam Wen, Gregory Chanan, Peng Wu, and Soumith\nChintala. Pytorch 2: Faster machine learning through\ndynamic python bytecode transformation and graph\ncompilation. In Proceedings of the 29th ACM Inter-\nnational Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems, Volume\n2, ASPLOS ’24, page 929–947, New York, NY, USA,\n2024.\n[9] Dmitri I Arkhipov, Di Wu, Keqin Li, and Amelia C\nRegan. Sorting with gpus: A survey. arXiv preprint\narXiv:1709.02520, 2017.\n[10] Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi,\nAmanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca\nSoldaini, Sergey Feldman, Mike D’arcy, et al. Open-\nscholar: Synthesizing scientific literature with retrieval-\naugmented lms.\narXiv preprint arXiv:2411.14199,\n2024.\n[11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection. In The\nTwelfth International Conference on Learning Repre-\nsentations, 2023.\n[12] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. Original implementation of self-\nrag: Learning to retrieve, generate and critique through\nself-reflection.\nhttps://github.com/AkariAsai/\nself-rag, 2024.\n[13] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei\nKoh, Luke Zettlemoyer, Hannaneh Hajishirzi, and\nWen-tau Yih.\nReliable, adaptable, and attributable\nlanguage models with retrieval.\narXiv preprint\narXiv:2403.03187, 2024.\n[14] Emery\nBerger and\nBen\nZorn.\nAi\nsoft-\nware should be more like plain old software.\nhttps://www.sigarch.org/ai-software-\nshould-be-more-like-plain-old-software/,\n2024.\n[15] Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. Improving lan-\nguage models by retrieving from trillions of tokens. In\nInternational conference on machine learning, pages\n2206–2240. PMLR, 2022.\n[16] James Briggs, Gibbs Cullen, and Greg Kogan. Vec-\ntor search in the wild. https://www.pinecone.io/\nlearn/series/wild/.\n[17] Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma,\nSeth Z Zhao, Zhiwen Wu, and Jiaqi Ma.\nDriving\nwith regulation: Interpretable decision-making for au-\ntonomous vehicles with retrieval-augmented reasoning\nvia llm. arXiv preprint arXiv:2410.04759, 2024.\n[18] Binoy Chemmagate. Reducing rag pipeline latency for\nreal-time voice conversations. https://developer.\nvonage.com/en/blog/reducing-rag-pipeline-\nlatency-for-real-time-voice-conversations,\n2024.\n[19] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li,\nChuanjie Liu, Zengzhong Li, Mao Yang, and Jing-\ndong Wang. Spann: Highly-efficient billion-scale ap-\nproximate nearest neighbor search.\narXiv preprint\narXiv:2111.08566, 2021.\n[20] Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo,\nDavid Majnemer, and Sanjiv Kumar. Tpu-knn: K near-\nest neighbor search at peak flop/s.\nIn Advances in\n14\n\n\nNeural Information Processing Systems, volume 35,\npages 15489–15501, 2022.\n[21] Neo Christopher Chung, George Dyer, and Lennart\nBrocki.\nChallenges of large language models\nfor mental health counseling.\narXiv preprint\narXiv:2311.13857, 2023.\n[22] Together Computer.\nGraphAI.\nhttps://github.\ncom/receptron/graphai, 2024.\n[23] Databricks.\nRag (retrieval augmented genera-\ntion) on databricks.\nhttps://docs.databricks.\ncom/en/generative-ai/retrieval-augmented-\ngeneration.html, 2024.\n[24] Divyanshu Dixit. Advanced rag series: Generation\nand evaluation.\nhttps://div.beehiiv.com/p/\nadvanced-rag-series-generation-evaluation,\n2024.\n[25] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff\nJohnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré,\nMaria Lomeli, Lucas Hosseini, and Hervé Jégou. The\nfaiss library. arXiv preprint arXiv:2401.08281, 2024.\n[26] Wenqi Fan, Yujuan Ding, Liang bo Ning, Shijie Wang,\nHengyun Li, Dawei Yin, Tat-Seng Chua, and Qing\nLi. A survey on rag meeting llms: Towards retrieval-\naugmented large language models. In Knowledge Dis-\ncovery and Data Mining, 2024.\n[27] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie\nCallan. Precise zero-shot dense retrieval without rele-\nvance labels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1762–1777, 2023.\n[28] Yunfan Gao.\nModular rag and rag flow: Part ii.\nhttps://medium.com/@yufan1602/modular-rag-\nand-rag-flow-part-ii-77b62bf8a5d3, 2024.\n[29] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang\nJia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for\nlarge language models: A survey.\narXiv preprint\narXiv:2312.10997, 2023.\n[30] Samira Ghodratnama and Mehrdad Zakershahrak.\nAdapting llms for efficient, personalized information\nretrieval: Methods and implications. In International\nConference on Service-Oriented Computing, pages 17–\n26. Springer, 2023.\n[31] Abdussamad GM and Gopala Dhar.\nHow apollo\n24|7\nleverages\nmedlm\nwith rag\nto\nrevolution-\nize\nhealthcare.\nhttps://cloud.google.com/\nblog/products/ai-machine-learning/how-\napollo-247-leverages-medlm-with-rag-to-\nrevolutionize-healthcare, 2024.\n[32] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar. Accel-\nerating large-scale inference with anisotropic vector\nquantization. In International Conference on Machine\nLearning, 2020.\n[33] Moritz Hardt and Yu Sun. Test-time training on nearest\nneighbors for large language models. arXiv preprint\narXiv:2305.18466, 2023.\n[34] Ivan Ilin.\nAdvanced rag techniques: an illus-\ntrated overview.\nhttps://pub.towardsai.net/\nadvanced-rag-techniques-an-illustrated-\noverview-04d193d8fec6, 2023.\n[35] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave.\nUnsupervised dense information\nretrieval with contrastive learning.\narXiv preprint\narXiv:2112.09118, 2021.\n[36] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. Atlas: Few-shot learning with retrieval aug-\nmented language models. Journal of Machine Learn-\ning Research, 24(251):1–43, 2023.\n[37] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui\nWang, and Michael Bendersky. Query expansion by\nprompting large language models.\narXiv preprint\narXiv:2305.03653, 2023.\n[38] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vard-\nhan Simhadri, Ravishankar Krishnawamy, and Rohan\nKadekodi. Diskann: Fast accurate billion-point nearest\nneighbor search on a single node. Advances in Neural\nInformation Processing Systems, 32, 2019.\n[39] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng\nLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllm-\nlingua: Accelerating and enhancing llms in long con-\ntext scenarios via prompt compression. arXiv preprint\narXiv:2310.06839, 2023.\n[40] Wenqi\nJiang, Shigang\nLi, Yu\nZhu, Johannes\nde Fine Licht, Zhenhao He, Runbin Shi, Cedric\nRenggli, Shuai Zhang, Theodoros Rekatsinas, Torsten\nHoefler, et al. Co-design hardware and algorithm for\nvector search. In Proceedings of the International Con-\nference for High Performance Computing, Networking,\nStorage and Analysis, pages 1–15, 2023.\n15\n\n\n[41] Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten\nHoefler, and Gustavo Alonso.\nChameleon: a het-\nerogeneous and disaggregated accelerator system for\nretrieval-augmented language models. arXiv preprint\narXiv:2310.09949, 2023.\n[42] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang,\nBernie Wang, and Tim Kraska. Piperag: Fast retrieval-\naugmented generation via algorithm-system co-design.\narXiv preprint arXiv:2403.05676, 2024.\n[43] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig.\nActive retrieval aug-\nmented generation. 2023.\n[44] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin\nLiu, Xuanzhe Liu, and Xin Jin. Ragcache: Efficient\nknowledge caching for retrieval-augmented generation.\narXiv preprint arXiv:2404.12457, 2024.\n[45] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang,\nand Zhicheng Dou. Flashrag: A modular toolkit for ef-\nficient retrieval-augmented generation research. arXiv\npreprint arXiv:2405.13576, 2024.\n[46] Jeff Johnson, Matthijs Douze, and Hervé Jégou.\nBillion-scale similarity search with GPUs. IEEE Trans-\nactions on Big Data, 7(3):535–547, 2019.\n[47] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. Triviaqa: A large scale distantly super-\nvised challenge dataset for reading comprehension. In\nProceedings of the 55th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long\nPapers), pages 1601–1611, 2017.\n[48] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. Dense passage retrieval for open-domain\nquestion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2020.\n[49] Saim Khan, Somesh Singh, Harsha Vardhan Simhadri,\nJyothi Vedurada, et al. Bang: Billion-scale approximate\nnearest neighbor search using a single gpu.\narXiv\npreprint arXiv:2401.11324, 2024.\n[50] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. Generalization through\nmemorization: Nearest neighbor language models. In\nInternational Conference on Learning Representations,\n2019.\n[51] Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin\nPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question\nanswering of llms via summarized retrieval. In The\nTwelfth International Conference on Learning Repre-\nsentations, 2023.\n[52] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas\nLee, Michael W Mahoney, Kurt Keutzer, and Amir\nGholami. An llm compiler for parallel function calling.\narXiv preprint arXiv:2312.04511, 2023.\n[53] Eyal Klang, Idit Tessler, Donald U Apakama, Ethan Ab-\nbott, Benjamin S Glicksberg, Monique Arnold, Akini\nMoses, Ankit Sakhuja, Ali Soroush, Alexander W\nCharney, et al. Assessing retrieval-augmented large\nlanguage model performance in emergency depart-\nment icd-10-cm coding compared to human coders.\nmedRxiv, 2024.\n[54] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. Natural questions: a benchmark for\nquestion answering research. Transactions of the As-\nsociation for Computational Linguistics, 7:453–466,\n2019.\n[55] Maximilian Lam, Jeff Johnson, Wenjie Xiong, Kiwan\nMaeng, Udit Gupta, Yang Li, Liangzhen Lai, Ilias Leon-\ntiadis, Minsoo Rhu, Hsien-Hsin S. Lee, Vijay Janapa\nReddi, Gu-Yeon Wei, David Brooks, and G. Edward\nSuh. Gpu-based private information retrieval for on-\ndevice machine learning inference, 2023.\n[56] LangChain, Inc. LangGraph. https://github.com/\nlangchain-ai/langgraph, 2024.\n[57] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\net al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Advances in Neural Information\nProcessing Systems, 33:9459–9474, 2020.\n[58] Jerry Liu.\nLlamaIndex.\nhttps://github.com/\njerryjliu/llama_index, 11 2022.\n[59] Zihan Liu, Wentao Ni, Jingwen Leng, Yu Feng, Cong\nGuo, Quan Chen, Chao Li, Minyi Guo, and Yuhao\nZhu.\nJuno: Optimizing high-dimensional approxi-\nmate nearest neighbour search with sparsity-aware al-\ngorithm and ray-tracing core mapping. arXiv preprint\narXiv:2312.01712, 2023.\n[60] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen,\nand Yaohua Tang. Turborag: Accelerating retrieval-\naugmented generation with precomputed kv caches for\nchunked text. arXiv preprint arXiv:2410.07590, 2024.\n16\n\n\n[61] Xinbei Ma, Yeyun Gong, Pengcheng He, Nan Duan,\net al. Query rewriting in retrieval-augmented large lan-\nguage models. In The 2023 Conference on Empirical\nMethods in Natural Language Processing, 2023.\n[62] Melissa Malec. Rag in financial services: Use-cases,\nimpact, & solutions.\nhttps://hatchworks.com/\nblog/gen-ai/rag-for-financial-services/,\n2024.\n[63] Alex Troy Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effective-\nness of parametric and non-parametric memories. In\nThe 61st Annual Meeting Of The Association For Com-\nputational Linguistics, 2023.\n[64] Sewon Min, Suchin Gururangan, Eric Wallace, Weijia\nShi, Hannaneh Hajishirzi, Noah A Smith, and Luke\nZettlemoyer. Silo language models: Isolating legal\nrisk in a nonparametric datastore. In The Twelfth In-\nternational Conference on Learning Representations,\n2023.\n[65] MyScale. 4 key benefits of rag algorithmic trading\nin financial markets.\nhttps://myscale.com/\nblog/benefits-rag-algorithmic-trading-\nfinancial-markets/, 2024.\n[66] Mohammad Norouzi and David J Fleet. Cartesian k-\nmeans. In Proceedings of the IEEE Conference on\ncomputer Vision and Pattern Recognition, pages 3017–\n3024, 2013.\n[67] NVIDIA.\nGeforce\nrtx\n4090.\nhttps:\n//www.nvidia.com/en-us/geforce/graphics-\ncards/40-series/rtx-4090/, 2024.\n[68] NVIDIA.\nNvidia\nh100\ntensor core\ngpu.\nhttps://resources.nvidia.com/en-us-tensor-\ncore/nvidia-tensor-core-gpu-datasheet,\n2024.\n[69] OpenAI. Gpt-3.5 turbo. https://platform.openai.\ncom/docs/models/gpt-3-5-turbo.\n[70] Pathway. Adaptive rag: How we cut llm costs with-\nout sacrificing accuracy.\nhttps://pathway.com/\ndevelopers/showcases/adaptive-rag, 2024.\n[71] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang,\nDan Ou, Xiaoyi Zeng, Tongxu, and Enhong Chen.\nLarge language model based long-tail query rewrit-\ning in taobao search. Companion Proceedings of the\nACM on Web Conference 2024, 2023.\n[72] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang,\nDan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and En-\nhong Chen. Large language model based long-tail\nquery rewriting in taobao search. In Companion Pro-\nceedings of the ACM on Web Conference 2024, 2024.\n[73] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. Measuring and nar-\nrowing the compositionality gap in language models.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 5687–5711, 2023.\n[74] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhl-\ngay, Amnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham.\nIn-context retrieval-augmented language\nmodels. Transactions of the Association for Computa-\ntional Linguistics, 11:1316–1331, 2023.\n[75] Rapidsai. Rapidsai/raft: Raft contains fundamental\nwidely-used algorithms and primitives for data science,\ngraph and machine learning. https://github.com/\nrapidsai/raft, 2022.\n[76] Keshav\nSanthanam, Deepti\nRaghavan, Muham-\nmad Shahir Rahman, Thejas Venkatesh, Neha Kunjal,\nPratiksha Thaker, Philip Levis, and Matei Zaharia.\nAlto: An efficient network orchestrator for compound\nai systems. In Proceedings of the 4th Workshop on\nMachine Learning and Systems, pages 117–125, 2024.\n[77] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi,\nTim Dettmers, Sewon Min, Luke Zettlemoyer, and\nPang Wei Koh. Scaling retrieval-based language mod-\nels with a trillion-token datastore. In The Thirty-eighth\nAnnual Conference on Neural Information Processing\nSystems.\n[78] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen.\nEnhancing\nretrieval-augmented large language models with iter-\native retrieval-generation synergy. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 9248–9274, 2023.\n[79] Susav Shrestha, Narasimha Reddy, and Zongwang Li.\nEspn: Memory-efficient multi-vector information re-\ntrieval. In Proceedings of the 2024 ACM SIGPLAN\nInternational Symposium on Memory Management,\npages 95–107, 2024.\n[80] Josef Sivic and Andrew Zisserman. Video google: A\ntext retrieval approach to object matching in videos.\nIn ICCV, pages 1470–1477. IEEE Computer Society,\n2003.\n[81] David Stewart and Jamie Linsdell. Say hello to pre-\ncision: How rerankers and embeddings boost search.\n17\n\n\nhttps://cohere.com/blog/say-hello-to-\nprecision-how-rerankers-and-embeddings-\nboost-search, 2024.\n[82] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023.\n[83] Kuan Tung. Enhancing user experience by overcoming\nlatency in the ion iq chatbot. https://www.ontinue.\ncom/resource/enhancing-user-experience-by-\novercoming-latency-in-the-ion-iq-chatbot/,\n2024.\n[84] Niithiyn Vijeaswaran, AJ Dhimine, Armando Diaz,\nSebastian Bustillo, Farooq Sabir, and Marco Punio.\nAdvanced rag\npatterns\non\namazon\nsagemaker.\nhttps://aws.amazon.com/blogs/machine-\nlearning/advanced-rag-patterns-on-amazon-\nsagemaker/, 2024.\n[85] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang,\nHao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, et al. A survey on large lan-\nguage model based autonomous agents. Frontiers of\nComputer Science, 18(6):186345, 2024.\n[86] Zijie J. Wang and Duen Horng Chau. Mememo: On-\ndevice retrieval augmentation for private and person-\nalized text generation. In Proceedings of the 47th In-\nternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, SIGIR ’24, page\n2765–2770, New York, NY, USA, 2024. Association\nfor Computing Machinery.\n[87] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven\nZheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang,\nAnush Mattapalli, Ankur Taly, Jingbo Shang, et al.\nSpeculative rag:\nEnhancing retrieval augmented\ngeneration\nthrough drafting.\narXiv preprint\narXiv:2407.08223, 2024.\n[88] Khye Wei.\nAdvanced rag with azure ai search\nand\nllamaindex.\nhttps://techcommunity.\nmicrosoft.com/t5/ai-azure-ai-services-\nblog/advanced-rag-with-azure-ai-search-\nand-llamaindex/ba-p/4115007, 2024.\n[89] Lukas Wutschitz, Boris Köpf, Andrew Paverd, Sara-\nvan Rajmohan, Ahmed Salem, Shruti Tople, Santiago\nZanella-Béguelin, Menglin Xia, and Victor Rühle. Re-\nthinking privacy in machine learning pipelines from an\ninformation flow control perspective, 2023.\n[90] Zhiqiang Xie, Hao Kang, Ying Sheng, Tushar Kr-\nishna, Kayvon Fatahalian, and Christos Kozyrakis. Ai\nmetropolis: Scaling large language model-based multi-\nagent simulation with out-of-order execution. arXiv\npreprint arXiv:2411.03519, 2024.\n[91] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W. Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. HotpotQA: A dataset for diverse,\nexplainable multi-hop question answering. In Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\n[92] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yi-\nhua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, and\nJunchen Jiang. Cacheblend: Fast large language model\nserving for rag with cached knowledge fusion, 2024.\n[93] Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yil-\nmaz.\nEnhancing conversational search: Large lan-\nguage model-aided informative query rewriting. In\nThe 2023 Conference on Empirical Methods in Natu-\nral Language Processing, 2023.\n[94] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao,\nPaul Newman, Lars Kunze, and Matthew Gadd.\nRag-driver: Generalisable driving explanations with\nretrieval-augmented in-context learning in multi-\nmodal large language model.\narXiv preprint\narXiv:2402.10828, 2024.\n[95] Matei\nZaharia, Omar Khattab, Lingjiao\nChen,\nJared Quincy Davis, Heather Miller, Chris Potts, James\nZou, Michael Carbin, Jonathan Frankle, Naveen Rao,\nand Ali Ghodsi. The shift from models to compound\nai systems.\nhttps://bair.berkeley.edu/blog/\n2024/02/18/compound-ai-systems/, 2024.\n[96] Chaoliang\nZeng, Layong\nLuo, Qingsong\nNing,\nYaodong Han, Yuhang Jiang, Ding Tang, Zilong Wang,\nKai Chen, and Chuanxiong Guo. FAERY: An FPGA-\naccelerated embedding-based retrieval system. In 16th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 22), pages 841–856, 2022.\n[97] Hengrui Zhang, August Ning, Rohan Baskar Prabhakar,\nand David Wentzlaff. Llmcompass: Enabling efficient\nhardware design for large language model inference. In\n2024 ACM/IEEE 51st Annual International Symposium\non Computer Architecture (ISCA), pages 1080–1096,\n2024.\n[98] Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lant-\ning Li, Phitchaya Mangpo Phothilimthana, and Zhi-\nhao Jia. Accelerating retrieval-augmented language\nmodel serving with speculation.\narXiv preprint\narXiv:2401.14021, 2024.\n18\n\n\n[99] Zili Zhang, Fangyue Liu, Gang Huang, Xuanzhe Liu,\nand Xin Jin. Fast vector query processing for large\ndatasets beyond GPU memory with reordered pipelin-\ning. In 21st USENIX Symposium on Networked Sys-\ntems Design and Implementation (NSDI 24), pages\n23–40, 2024.\n[100] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,\nHeng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny\nZhou. Take a step back: Evoking reasoning via ab-\nstraction in large language models. arXiv preprint\narXiv:2310.06117, 2023.\n[101] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie,\nChuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,\nChristos Kozyrakis, Ion Stoica, Joseph E. Gonzalez,\nClark Barrett, and Ying Sheng. Sglang: Efficient exe-\ncution of structured language model programs, 2024.\n[102] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\nCui, Olivier Bousquet, Quoc V Le, et al. Least-to-\nmost prompting enables complex reasoning in large\nlanguage models. In The Eleventh International Con-\nference on Learning Representations, 2022.\n[103] Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile\nGu, Dedong Xie, Yufei Gao, Qinyu Xu, Tian Tang,\nZihao Ye, et al.\nNanoflow: Towards optimal large\nlanguage model serving throughput. arXiv preprint\narXiv:2408.12757, 2024.\n[104] Yuhao Zhu. RTNN: Accelerating neighbor search us-\ning hardware ray tracing. In Proceedings of the 27th\nACM SIGPLAN Symposium on Principles and Prac-\ntice of Parallel Programming, PPoPP ’22, pages 76–89,\nApril 2022.\n[105] Shengyao Zhuang, Bing Liu, Bevan Koopman, and\nGuido Zuccon. Open-source large language models are\nstrong zero-shot query likelihood models for document\nranking. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, pages 8807–8817,\n2023.\n[106] Zilliz.\nHow to select index parameters for ivf in-\ndex.\nhttps://zilliz.com/blog/select-index-\nparameters-ivf-index, 2020.\n19\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20969v1.pdf",
    "total_pages": 19,
    "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval",
    "authors": [
      "Chien-Yu Lin",
      "Keisuke Kamahori",
      "Yiyu Liu",
      "Xiaoxiang Shi",
      "Madhav Kashyap",
      "Yile Gu",
      "Rulin Shao",
      "Zihao Ye",
      "Kan Zhu",
      "Stephanie Wang",
      "Arvind Krishnamurthy",
      "Rohan Kadekodi",
      "Luis Ceze",
      "Baris Kasikci"
    ],
    "abstract": "Retrieval-augmented generation (RAG) extends large language models (LLMs)\nwith external data sources to enhance factual correctness and domain coverage.\nModern RAG pipelines rely on large datastores, leading to system challenges in\nlatency-sensitive deployments, especially when limited GPU memory is available.\nTo address these challenges, we propose TeleRAG, an efficient inference system\nthat reduces RAG latency with minimal GPU memory requirements. The core\ninnovation of TeleRAG is lookahead retrieval, a prefetching mechanism that\nanticipates required data and transfers it from CPU to GPU in parallel with LLM\ngeneration. By leveraging the modularity of RAG pipelines, the inverted file\nindex (IVF) search algorithm and similarities between queries, TeleRAG\noptimally overlaps data movement and computation. Experimental results show\nthat TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average\ncompared to state-of-the-art systems, enabling faster, more memory-efficient\ndeployments of advanced RAG applications.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}