{
  "id": "arxiv_2502.20871v1",
  "text": "arXiv:2502.20871v1  [math.AP]  28 Feb 2025\nTime-optimal problem in the\nspace of probabilities measures\nYurii Averboukh\nEkaterina Kolpakova\nThis paper focuses on the value function in the time-optimal problem\nfor a continuity equation in the space of probability measures. We derive\nthe dynamic programming principle for this problem. In particular, we\nprove that the Kruzhkov transform of the value function coincides with\na unique discontinuous viscosity solution to the corresponding Dirichlet\nproblem for the Hamilton–Jacobi equation. Finally, we establish the Γ-\nconvergence of the value function in a perturbed problem to the value\nfunction in the unperturbed problem.\nKeywords:\ntime-optimal problem, Hamilton—Jacobi equation, con-\ntrolled continuity equation, viscosity solution, Dirichlet problem, non-\nsmooth analysis in the Wasserstein space..\nMSC Classiﬁcation (2020): 49J20, 49J52, 49L25, 49N70, 91A23,\n49K20, 82C22.\n1. Introduction\nIn this paper, we study a control system in the space of probability measures, where\nthe dynamics are governed by a nonlocal continuity equation driven by an external\nforce. This equation models the evolution of a system comprising inﬁnitely many\nidentical particles, with each particle inﬂuenced by the collective ﬁeld generated by\nall other particles.\nThe nonlocal continuity equation traces its origins to the pioneering work of Vlasov\non plasma dynamics. Nowadays, it ﬁnds widespread application in various ﬁelds,\nincluding opinion dynamics models [16,32,38], crowd behavior analysis [21,23,26,34]\nand swarm dynamics [24,31].\nThe objective of the control is to steer the system to a target in minimal time.\nThe target here is assumed to be an arbitrary closed set of probability measures.\nIn this work, we develop the dynamic programming approach and investigate the\nproperties of the value function.\n1\n\n\nThe main results of the paper are as follows. First, we establish the existence\nof an optimal relaxed control. Next, we derive the dynamic programming princi-\nple for the value function in this setting. We then implement a framework based\non the Dirichlet problem for the Hamilton-Jacobi equation, which, in this context,\nis a partial diﬀerential equation in the space of measures.\nWe demonstrate that\nthe Kruzhkov transform of the value function satisﬁes the aforementioned Dirichlet\nproblem in the viscosity sense. Furthermore, by deriving a comparison principle,\nwe prove the existence and uniqueness of a solution to the Dirichlet problem associ-\nated with the time-optimal problem for a nonlocal continuity equation. In general,\nthe value function is discontinuous; however, we identify a suﬃcient condition that\nensures its continuity. Finally, we show that the values of perturbed time-optimal\nproblems Γ-converge to the solution of the limiting problem.\nNow, let us give a brief overview of the existing literature and compare our results\nwith what is already known. The study of control problems for nonlocal continu-\nity began with papers [13, 36]. Necessary optimality conditions in the form of the\nPontryagin maximum principle for ﬁnite-horizon problems were derived in [8–11,35].\nWe should also highlight the paper [5], where the authors develop the dynamic pro-\ngramming principle for the optimal control problem of the continuity equation on\na ﬁnite time interval. In that work, it is shown that the value function restricted\nto the space of compactly supported measures is a solution to the Cauchy problem\nfor the Hamilton-Jacobi equation in the space of probability measures. Moreover,\nthe uniqueness of the solution is also established on the set of compactly supported\nprobability measures.\nNotice that the problem studied in this paper assumes the system is inﬂuenced by\nan external force. Consequently, the velocity ﬁeld is a continuous function of a point.\nAn alternative approach considers the possibility of a discontinuous velocity ﬁeld.\nThis framework known as the mean ﬁeld type control theory is particularly relevant\nfor systems involving intellectual agents striving to achieve a common goal. For the\ncurrent state of research in this direction, we refer to the works of [3,27,28]. Time-\noptimal problems for mean ﬁeld type control systems were investigated in [17–19],\nwhere the target is assumed to be a hyperplane in the space of probability measures.\nIn those papers, the value function was characterized as a solution to a Dirichlet\nproblem for the Hamilton-Jacobi equation in the space of measures. Furthermore,\nsuﬃcient conditions for the regularity of the value function were established. How-\never, these works do not provide a uniqueness result.\nAs previously mentioned, our approach to analyzing time-optimal problems re-\nlies on viscosity solutions to the Hamilton–Jacobi equation. This methodology was\ninitially developed for ﬁnite-dimensional equations (see [6,22,37]).\nNowadays, the concept of viscosity solutions has been extended to spaces of proba-\nbility measures (see [1,5,18,19,25,27,33]). It is worth to notice that the deﬁnition of a\nviscosity solution hinges on the concepts of subdiﬀerentials and superdiﬀerentials. In\nthe context of probability measure spaces, there are multiple approaches to deﬁning\nthese objects. The central question revolves around determining which variations are\nadmissible and what constitutes sub- and superdiﬀerentials. In our framework, we\n2\n\n\nassume that elements of the sub- and superdiﬀerentials are square-integrable func-\ntions, and we permit variations deﬁned by directions that are also square-integrable\nfunctions. This approach was ﬁrst introduced in [15]. Alternatively, one can con-\nsider varying a measure near an element speciﬁed by a given direction. This method\nwas employed in [5] for ﬁnite-horizon optimal control problems involving nonlo-\ncal continuity equations and their corresponding Hamilton-Jacobi equations. There\nare also approaches that rely on directions deﬁned by plans (see [1, 18, 19, 25, 27]).\nMoreover, in [2] elements of subdiﬀerentials are assumed to be distributions. It is\nimportant to note that the aforementioned approaches are intrinsic, as they utilize\nonly measures and directions within the space of measures. However, a probabil-\nity measure with ﬁnite second moment can also be represented as a push-forward\nmeasure via some square-integrable random variable. This leads to an external ap-\nproach to nonsmooth analysis in the space of probability measures (see [14,25,33]).\nConnections between the external approach to nonsmooth analysis and certain def-\ninitions of sub- and superdiﬀerentials within the intrinsic framework are explored\nin [14,25,28]. Additionally, the papers [27,28] oﬀer a comparative analysis of various\nintrinsic approaches.\nIn our view, the approach to nonsmooth analysis employed in this paper is par-\nticularly well-suited for the control problem associated with the nonlocal continuity\nequation. However, in Section 9, we discuss an alternative approach that involves\nvarying a measure near an element speciﬁed by a given direction, as introduced\nin [5], alongside deﬁnitions based on directions provided by plans (see [18,19,27]).\nWe show that these approaches yield the same notion of a viscosity solution to the\nDirichlet problem when the Hamiltonian arises from the control problem for the\nnonlocal continuity equation.\nThe paper is organized as follows. In Section 2, we introduce the general notation\nused throughout the paper. The formulation of the time-optimal control problem\nfor the nonlocal continuity equation is presented in Section 3. In this section, we\nalso prove the existence of an optimal control and show that the value function is\nlower semicontinuous. Section 4 introduces the statement of the Dirichlet problem\nand deﬁnes the notion of a viscosity solution corresponding to the examined prob-\nlem. The comparison principle for this problem is established in Section 5, which,\nin turn, implies the uniqueness of the viscosity solution. In Section 6, we prove that\nthe Kruzhkov transform of the value function is a viscosity solution to the aforemen-\ntioned Dirichlet problem. Additionally, we derive a suﬃcient condition ensuring the\ncontinuity of the value function and show that the values of perturbed time-optimal\ncontrol problems for the nonlocal continuity equation Γ-converge to the original\nproblem (see Section 7). An example of a time-optimal control problem for the con-\ntrolled continuity equation is provided in Section 8. Finally, in Section 9, we discuss\nequivalent deﬁnitions of the viscosity solution in the context of the examined case.\n3\n\n\n2. Preliminaries\nIf X1, . . . , Xn are sets, i1 . . . , ik ∈{1, . . . , n}, then pi1,...,ik denotes a natural pro-\njection of X1 × . . . × Xn onto Xi1 × . . . × Xik.\nIf (Ω′, F ′) and (Ω′′, F ′′) are measurable spaces, m is a measure on F ′, a map\nh : Ω′ →Ω′′ is F ′/F ′′-measurable, then we denote by h♯m the push-forward measure\n[2] deﬁned by the rule: for each Γ ∈F ′′\n(h♯m)(Γ) = m(h−1(Γ)).\nIf (X, ρX) is a Polish space, x ∈X, r > 0, then Br(x) stands for the ball of radius\nr centered at x.\nIf (X, ρ) is a Polish space, then M(X) denotes the space of ﬁnite Borel measures\non X, while P(X) stands for the space of Borel probabilities on X. Recall that\nP(X) = {m ∈M(X) : m(X) = 1}.\nWe consider on M(X) the narrow convergence.\nThis means that {mn}∞\nn=1 ⊂\nM(X) converges to m ∈M(X) if, for every φ ∈Cb(X),\nZ\nX\nφ(x)mn(dx) →\nZ\nX\nφ(x)m(dx) as n →∞.\nObviously, P(X) is closed w.r.t. the narrow convergence.\nWe denote the space of probability measures with the ﬁnite second moment\nby P2(X), i.e., m ∈P(X) belongs to P2(X) provided that, for some x∗∈X,\nR\nX ρ2(x, x∗)m(dx) < ∞. If X is a Banach space, then we denote\nς(m) ≜\n\" Z\nX\n∥x∥2m(dx)\n#1/2\n.\nWe endow P2(X) with the second Wasserstein distance deﬁned by the rule:\nW2(m1, m2) =\n\"\ninf\nπ∈Π(m1,m2)\nZ\nRd×Rd ρ(x1, x2)2π(d(x1, x2))\n#1/2\n,\nwhere Π(m1, m2) stands for the set of probabilities π ∈P2(Rd × Rd) such that\npi ♯π = mi.\nIf (X, ρ) is a Polish space, Y is a Banach space, m is a measure on X, then\nL2(X, m; Y ) denotes the space of functions ϕ : X 7→Y such that\nZ\nX\n∥ϕ(x)∥2\nY m(dx) < ∞.\nIf X = Y = Rd, we will shorten the notation and write L2(m) instead of L2(Rd, m; Rd).\nGiven s1, s2 ∈L2(m), the inner product of these functions in L2(m) is denoted by\n⟨s1, s2⟩m, i.e.,\n⟨s1, s2⟩m ≜\nZ\nRd⟨s1(x), s2(x)⟩m(dx).\n4\n\n\nAdditionally, L0(X; Y ) denotes the set of all measurable functions from X to Y .\nIf (X, ρX), (Y, ρY ) are Polish spaces, m is a ﬁnite measure on X, then we denote\nby Λ(X, m; Y ) the set of measures α on X × Y such that p1 ♯α = m. Notice that\ndue to the disintegration theorem [7, Theorem 10.4.14], given α ∈Λ(X, m; Y ) one\ncan ﬁnd a family of probability measures {α(·|x)}x∈X ⊂P(Y ) such that, for every\nfunction φ ∈Cb(X × Y ),\nZ\nX×Y\nφ(x, y)α(d(x, y)) =\nZ\nX\nZ\nY\nφ(x, y)α(dy|x)m(dx).\n(1)\nMoreover, this family is m-a.e. unique. If {α(·|x)}x∈X is a family of probabilities on\nX that is weakly measurable, one can uniquely construct a measure α ∈Λ(X, m; Y )\nsuch that (1) holds true.\nNotice that each Borel function h : X →Y produces a weakly measurable family\nof probabilities by the rule\nαh(·|x) ≜δh(x).\n(2)\nHereinafter, δz stands for the Dirac measure concentrated at z. So, (2) produces\nthe embedding of the space of L1(X, m; Y ) into Λ(X, m; Y ). Notice that, if Y is a\ncompact, then the set {αh : h ∈L0(X; Y )} is dense in Λ(X, m; Y ).\nIn the following, given a metric compact U, we will consider measures on [0, +∞)×\nU those marginal distributions on [0, +∞) are equal to the Lebesgue measure on\n[0, +∞). The set of such measures is denoted by U. Elements of U are regarded as a\nrelaxed (generalized) controls. If ξ ∈U, T > 0, then ξ|T stands for its restriction on\n[0, T] ×U. Notice that ξ|T lies in UT ≜Λ([0, T], λ; U), where λ denotes the standard\nLebesgue measures.\nDeﬁnition 2.1. We say that a sequence {ξn}∞\nn=1 ⊂U converges to ξ ∈U provided\nthat, for each T > 0, {ξn|T}∞\nn=1 converges to {ξ|T} narrowly.\nNotice that in this deﬁnition it suﬃces to consider only natural T. Thus, the space\nU is compact. Moreover, each measurable function u : [0, +∞) →U generates an\nelement of U such that ξ(·|t) ≜δu(t). The set of such elements is dense in U.\n3. Statement of the problem\nThe main object of the paper is the following controlled nonlocal continuity equa-\ntion\n∂tm(t) + div(f(x, m(t), u(t))m(t)) = 0,\nt ∈[0, +∞).\n(3)\nHere, x ∈Rd, m(t) ∈P2(Rd), u(t) ∈U, the set U is a metric compact set. We\ninterpret u(t) is an instantaneous control. Thus, U is the space of admissible controls.\nEquation (3) describes an evolution of the distribution of inﬁnitely many similar\nparticles obeying the following equation\nd\ndtx(t) = f(x(t), m(t), u(t)), x ∈Rd, u ∈U.\n5\n\n\nHere, the distribution of agents at the instant t is denoted m(t) ∈P2(Rd), while u(t)\nstands for an external control that inﬂuences the whole system.\nWe put the following assumptions.\nHypothesis 3.1. The function f is continuous in all arguments.\nHypothesis 3.2. There exists a constant L > 0 such that, for every x1, x2 ∈Rd,\nm1, m2 ∈P2(Rd), u ∈U, one has the inequality\n∥f(x1, m1, u) −f(x2, m2, u)∥≤L(∥x1 −x2∥+ W2(m1, m2)).\nWithin the paper, we use relaxed controls. As we mentioned above, each relaxed\ncontrol is a measure on [0, +∞) × U with the marginal distribution on [0, +∞)\ncoinciding with the Lebesgue measure. Furthermore, given ξ ∈U, there exists its\ndisintegration w.r.t. the Lebesgue measure denoted by ξ(·|t). Formally, the usage of\nrelaxed controls means that we replace continuity equation (3) with\n∂tm(t) + div\n Z\nU\nf(x, m(t), u)ξ(du|t) · m(t)\n!\n= 0,\nt ∈[0, +∞).\n(4)\nWe will consider this equation in the distributional sense.\nDeﬁnition 3.3. Let T > 0, ξ ∈UT . A measure-valued function m(·) : [0, T] →\nP2(Rd) is called a solution of equation (4) on [0, T] if, for every ϕ ∈C∞\n0 ((0, T); Rd),\none has that\nZ\n[0,T]×U\nZ\nRd ∂tϕ(t, x) + ⟨∇ϕ(t, x), f(x, m(t), u)⟩m(t, dx)ξ(d(t, u)) = 0.\nIf ξ ∈U, then a measure-valued function m(·) : [0, +∞) →P2(Rd) is a solution of\n(4) on [0, +∞) if, for every T > 0, the restriction of m(·) on [0, T] is a solution of (4)\non [0, T].\nGiven µ ∈P2(Rd), ξ ∈U, we denote by m(·; µ, ξ) the unique solution of (4) on\n[0, +∞) satisfying m(0) = µ. If ξ ∈UT, then without loss of generality, we will keep\nthe designation for the solution of (4) satisfying the initial condition m(·) = µ.\nAn alternative deﬁnition of the solution to continuity equation (4) relies on the\nparticle interpretation. To introduce it, for given y ∈Rd, a continuous function\nm(·) : [0, T] →P2(Rd), ξ ∈UT, we denote by X(·; y, m(·), ξ) a solution of the initial\nvalue problem\nd\ndtx(t) =\nZ\nU\nf(x(t), m(t), u)ξ(du|t), x(0) = y.\nThe function X(·; y, m(·), ξ) describes the motion of a particle on the time interval\n[0, T] in the case where the distribution of all particles is given by m(·), while the\nsystem is aﬀected by a relaxed control ξ. The motion on [0, +∞) can be deﬁned in\nthe same way.\nFurthermore, for T > 0, t ∈[0, T], the evaluation operator et assigns to x(·) ∈\nC([0, T]; Rd) the value x(t).\n6\n\n\nProposition 3.4. Given T > 0, ξ ∈UT , a measure-valued function m(·) : [0, T] →\nP2(Rd) solves (4) if and only if there exists a probability χ ∈P(C([0, T]; Rd)) such\nthat the following properties holds true:\n(M1) m(t) ≜et♯χ;\n(M2) χ-a.e. curves x(·) are such that\nx(·) = X(·; x(0), m(·), ξ).\nThis statement directly follows from the seminal superposition principle (see [2,\nTheorem 8.2.1].\nFurthermore, we have the following equivalent deﬁnition of the\nsolution of (4) on [0, +∞).\nCorollary 3.5. Assume that ξ ∈U. Then, a measure-valued function m(·) : [0, T] →\nP2(Rd) solves (4) if and only if there exists a probability χ ∈P(C([0, +∞); Rd)) such\nthat conditions (M1), (M2) hold true.\nNow, let us describe the time-optimal problem examined in the paper. We assume\nthat we are given with a closed set M ⊂P2(Rd). Furthermore, we denote G ≜\nP2(Rd) \\ M.\nAdditionally, ∂G stands for the border of G.\nLet us consider the\nfunctional τ deﬁned on C([0, +∞); P2(Rd)) such that\nτ(m(·)) ≜inf{t ∈[0; +∞) : m(t) ∈M}.\n(5)\nNotice that, if, for every t > 0, m(t) ̸∈M, then τ(m(·)) = +∞. The time-optimal\nproblem means that, given an initial distribution µ ∈P2(Rd), one wishes to minimize\nthe quantity τ(m(·; µ, ξ)). Thus, the value function of time-optimal problem (3), (5)\nis deﬁned by the rule:\nVal(µ) = inf{τ(m(·; µ, ξ)) :\nξ ∈U}.\n(6)\n3.1. Properties of the value function\nThe properties of the value function in a time-optimal problem in Rd are well\nstudied [6]. It is proved that the value function is lower semicontinuous and satisﬁes\nthe dynamical programming principle. In this section, we derive the analogous prop-\nerties for the examined time–optimal control in the space of probability measures.\nFirst, let us recall the continuity of the trajectories on each ﬁnite interval proved\nin [4].\nProposition 3.6. Assume that, for each n, fn is a function from Rd × P2(Rd) × U\nto Rd; µn ∈P2(Rd), ξn ∈U are such that\n• each function fn is continuous w.r.t. all variables and Lipschitz continuous\nw.r.t. x and m for the constant L;\n7\n\n\n• there exists a continuous function f : Rd × P2(Rd) × U →Rd that is Lipschitz\ncontinuous w.r.t. x and m such that, for each c > 0,\nsup\nn\n∥fn(x,m, u) −f(x, m, u)∥:\nx ∈Rd, m ∈P2(Rd), ς(m) ≤c, u ∈U\no\n→0 as n →∞;\n• the sequences {µn}∞\nn=1 and {ξn}∞\nn=1 converge to µ ∈P2(Rd) and ξ ∈U respec-\ntively;\n• for each natural n, mn(·) is the solution of (4) for the dynamics equal to fn,\nrelaxed control ξn such that mn(0) = µn.\nThen, for every T > 0, the sequence of {mn(·)}∞\nn=1 converges to m(·; µ, ξ) in C([0, T]; P2(Rd)).\nRemark 3.7. This proposition and the fact that the set of relaxed controls corre-\nsponding to usual ones is dense in U imply that, given ξ ∈U and µ ∈P2(Rd), there\nexists a sequence {mn}∞\nn=1 such that, for each n, mn(·) satisﬁes\n∂tmn(t) + div(f(x, mn(t), un(t))mn(t)) = 0,\nmn(0) = µ\nwhereas mn(·) →m(·; µ, ξ) as n →∞. In particular,\nVal(µ) = inf\nn\nτ(m(·) : m(·) satisﬁes ∂tm(t) + div(f(x, m(t), u(t))m(t)) = 0,\nm(0) =µ,\nu(·) ∈L0([0, +∞); U)\no\n.\nNow, we establish the existence of an optimal control whenever the value function\nis ﬁnite.\nTheorem 3.8. Assume that µ ∈cl G satisﬁes Val(µ) < +∞. Then, there exists a\nrelaxed control ξ∗∈U such that\nVal(µ) = τ(m(·; µ, ξ∗)).\nRemark 3.9. We say that a relaxed control ξ∗∈U is optimal if\nVal(µ) = τ(m(·; µ, ξ∗)).\nProof of Theorem 3.8. By the deﬁnition of the value function, there exists a sequence\n{ξn}∞\nn=1 ⊂U such that\nVal(µ) = τ∗≜lim\nn→∞τ(m(·; µ, ξn)).\nFurthermore, we denote τn ≜τ(m(·; µ, ξn)). Since U is compact, without loss of\ngenerality, one may assume that {ξn}∞\nn=1 converges to some control ξ∗∈U. Proposi-\ntion 3.6 implies that {m(·; µ, ξn)}∞\nn=1 converges to m(·; µ, ξ∗). Since m(τn; µ, ξn) ∈M,\npassing to the limit and using the closeness of the target set M, we conclude that\nm(τ∗; µ, ξ∗) ∈M.\nTherefore, Val(µ) = τ(m(·; µ, ξ∗)).\n8\n\n\nProposition 3.10. The value function for problem (3), (5) is lower semicontinuous.\nProof. Let {µk}∞\nk=1 ⊂cl G be a sequence of initial conditions for problem (3) converg-\ning to µ0 ∈cl G. Denote Tk ≜Val(µk). First, we consider the case where {Tk}∞\nk=1 is\nbounded. Without loss of generality, we assume that the sequence {Tk}∞\nk=1 converges\nto some T0 ∈[0, +∞). Furthermore, due to Theorem 3.8, for each k there exists a\nrelaxed control ξk such that, Tk = τ(m(·; µk, ξk)). Again, without loss of generality,\nwe assume that the whole sequence {ξk}∞\nk=1 converges to ξ0 ∈U. Proposition 3.6\ngives that the probabilities m(Tk; µk, ξk) tend to m(T0; µ0, ξ0) as k →∞. Since the\nset M is closed, while m(Tk; µk, ξk) ∈M, we have that m(T0; µ0, ξ0) ∈M. Thus,\nusing the choice of Tk, we conclude that\nlim\nk→∞Val(µk) = T0 ≥τ(m(·; µ0, ξ0)) ≥Val(µ0).\nIf\nlim\nk→∞Tk = +∞,\none obviously has that\nlim\nk→∞Val(µk) ≥Val(µ0).\n3.2. Dynamic programming principle\nTheorem 3.11. The value function for problem (3), (5) satisﬁes the dynamical\nprogramming principle on cl G, i.e., for every µ ∈cl G and h ∈[0, Val(µ)],\nVal(µ) −h = inf\nξ∈Uh Val(m(h; µ, ξ)).\nProof. If ξ ∈Uh, η ∈U, then we denote by ξ ⋄h η the relaxed control determined by\nits disintegration as follows:\n(ξ ⋄h η)(·|t) ≜\n(\nξ(·|t),\nt ∈[0, h),\nη(·|t −h),\nt ∈[h, +∞).\nWe have that\ninf\nξ∈U Val(m(h; µ, ξ)) = inf\nξ∈Uh inf\nη∈U τ(m(·; m(h; µ, ξ), η))\n= inf\nξ∈Uh inf\nη∈U τ(m(·; µ, ξ ⋄h η) −h\n= inf\nξ∈U τ(m(·; µ, ξ) −h = Val(µ) −h.\n9\n\n\n4. The Dirichlet problem for the Hamilton—Jacobi\nequation\nFirst, we deﬁne the Hamiltonian by the following rule: for every m ∈P2(Rd) and\ns(·) ∈L2(m)\nH(m, s) = inf\nu∈U\nZ\nRd ⟨s(x), f(x, m, u)⟩m(dx).\nThe value function of problem (3), (5) can have inﬁnite value. So, we apply the\nKruzhkov transform and deﬁne the function φ : cl G →R by the rule:\nφ(m) = 1 −e−Val(m).\n(7)\nNotice that φ takes values in [0, 1]. Thus, we arrive at the following Dirichlet problem\nfor the Hamilton—Jacobi equation and the transformed value function:\nH(m, Dφ(m)) + 1 −φ(m) = 0, m ∈G; φ(m) = 0, m ∈∂G.\n(8)\nHere, we formally use Dφ(m) to designate the derivative of the function φ. Below,\nwe will develop the viscosity solution approach for this problem that comes back to\npapers [5,20]. To this end, we need some concepts of the nonsmooth analysis in the\nWasserstein space.\nLet m ∈P2(Rd), r > 0, and let ψ be a real-valued function deﬁned on Br(m).\nDeﬁnition 4.1. A subdiﬀerential of the function ψ at the measure m is deﬁned by\nthe rule:\n∂−ψ(m) ≜\nn\ns ∈L2(m) : for every F ∈L2(m) and h > 0,\nψ((Id +hF)♯m) −ψ(m) ≥h⟨s, F⟩m + o(h)\no\n.\n(9)\nSimilarly, the superdiﬀerential of the ψ at the measure m is equal to\n∂+ψ(m) ≜\nn\ns ∈L2(m) : for every F ∈L2(m) and h > 0,\nψ((Id +hF)♯m) −ψ(m) ≤h⟨s, F⟩m + o(h)\no\n.\n(10)\nRemark 4.2. An alternative deﬁnition of the sub-/superdiﬀerentials involves the con-\ncept of Hadamard directional derivatives. Below, we endow directions determined\nby functions from L2(m). Let a measure m, a radius r and a function ψ be as above.\nA Hadamard lower directional derivative of the function ψ at the probability m in\na direction F ∈L2(m) is\nd−\nHψ(m; F) ≜\nlim inf\nh→0\n∥F ′−F ∥L2(m)=O(h)\nψ((Id +hF ′)♯m) −ψ(m)\nh\n.\n10\n\n\nSimilarly, a Hadamard upper directional derivative of the function ψ at a probability\nm ∈G in a direction F ∈L2(m) is equal to\nd+\nHψ(m; F) ≜\nlim sup\nh→0,\n∥F ′−F ∥L2(m)=O(h)\nψ((Id +hF ′)♯m) −ψ(m)\nh\n.\nOne can easily show that\n∂−ψ(m) =\n\b\nF ∈L2(m) : ⟨s, F⟩m ≤d−\nHψ(m; F)\n\t\n,\n∂+ψ(m) =\n\b\nF ∈L2(m) : ⟨s, F⟩m ≥d+\nHψ(m; F)\n\t\n.\nNotice that from the very deﬁnition of the subdiﬀerential (see\n(9)) it directly\nfollows that, if ψ1, ψ2 ∈Br(m) →R for some positive number r, then\n∂−ψ1(m) + ∂−ψ2(m) ⊂∂−(ψ1 + ψ2)(m).\n(11)\nAnalogously, (10) implies the inclusion\n∂+ψ1(m) + ∂+ψ2(m) ⊂∂+(ψ1 + ψ2)(m).\nOur approach to the deﬁnition of the viscosity solution to the Dirichlet problem\nis close to one proposed in [37, Deﬁnition 18.3, Theorem 18.6] and [6, §IV.3] for\nthe ﬁnite dimensional case. In particular, we postulate that the viscosity solution\ncoincides with the supersolution. Notice that in [6] this notion is called an envelope\nviscosity solution, while in [37] the words ‘minimax solution’ are used.\nDeﬁnition 4.3. A bounded lower semicontinuous function φ1 : cl G →R is called a\nviscosity supersolution of problem (8) provided that\n• for each m ∈∂G, φ1(m) = 0;\n• for every m ∈G and s ∈∂−φ1(m)\nH(m, s) + 1 −φ1(m) ≤0.\nDeﬁnition 4.4. A bounded upper semicontinuous function φ2 : cl G →R is called\na viscosity subsolution of problem (8) if\n• for each m ∈∂G, φ2(m) = 0;\n• φ2 is continuous on ∂G;\n• for every m ∈G and s ∈∂+φ2(m) one has that\nH(m, s) + 1 −φ2(m) ≥0.\n11\n\n\nDeﬁnition 4.5. A bounded lower semicontinuos function φ : cl G →R is called a\nviscosity solution of problem (8) if\n• φ is a supersolution of problem (8);\n• there exists a sequence of functions {φk}∞\nk=1 deﬁned on cl G with values in R\nsuch that each function φk is a subsolution of (8) while, for every m ∈G, one\nhas that\nφ(m) = lim\nk→∞φk(m).\n5. Comparison principle\nThe proof of the comparison principle relies on the diﬀerentiability properties of\nthe squared Wasserstein distance [2, Theorem 10.2.2]. It states that, given µ, µ′, ν ∈\nP2(Rd), π ∈Π(µ, µ′), ϑ ∈Π(µ, ν) that is an optimal plan between µ and ν, and\n̟ ∈P((Rd)3) such that p1,2 ♯̟ = π, while p1,3 ♯̟ = ϑ, one has that\n1\n2W 2\n2 (µ′, ν) −1\n2W 2\n2 (µ, ν) −\nZ\n(Rd)3⟨x −y, x′−x⟩̟(d(x, x′, y))\n≤\nZ\n(Rd)2 ∥x′ −x∥2π(d(x, x′)).\nLetting in this inequality µ′ = (Id +hF)♯µ, where F ∈L2(µ), we have that\n1\n2W 2\n2 ((Id +hF)♯µ, ν) −1\n2W 2\n2 (µ, ν) −h\nZ\nRd⟨ˆϑ, F⟩µ ≤h2∥F∥2\nL2(µ).\n(12)\nHere, ˆϑ ∈L2(µ) is a barycenter of the plan ϑ deﬁned by the rule:\nˆϑ(x) ≜\nZ\nRd(x −y)ϑ(dy|x).\n(13)\nTheorem 5.1. Let φ1 : cl G →[0, 1] be a supersolution of problem (8) and let\nφ2 : cl G →[0, 1] be a subsolution of problem (8). Then, for each m ∈cl G φ1(m) ≥\nφ2(m).\nProof. Below, we duplicate the space and work with G2. The distance on this space\nbetween two pairs (m1, m2) and (µ1, µ2) is assumed to be equal to\nd((m1, m2), (µ1, µ2)) ≜W2(m1, µ1) + W2(m2, µ2).\nAdditionally, without loss of generality, we assume that the Lipschitz constant of the\nfunction f is greater than 1/4.\nFurthermore, we introduce the functions: Φ : G2 →R and ρ : (G2)2 →R by the\nfollowing rules:\nΦ(m1, m2) ≜φ2(m2) −φ1(m1) −1\n2W 2\n2 (m1, m2),\n12\n\n\nρ(m1, m2; µ1, µ2) ≜(4L −1)\n\u0002\nW 2\n2 (m1, µ1) + W 2\n2 (m2, µ2)\n\u0003\n.\nNotice that ρ is a gauge-type function in the sense of [30, Deﬁnition 2], i.e.,\n• ρ(m1, m2; m1, m1) = 0 for every m1, m2 ∈P2(Rd);\n• given ǫ > 0 there exists β > 0 such that, for all (m1, m2) ∈G2, (µ1, µ2) ∈G2\ninequality ρ(m1, m2; µ1, µ2) ≤β implies d((m1, m2), (µ1, µ2)) ≤ǫ.\nLet ˆm be a point in G. By Borwein—Preiss Variational Principle [30, Corollary\n5], there exists a pair (m∗\n1, m∗\n2) ∈G2 such that\nΦ(m∗\n1, m∗\n2) −ρ( ˆm, ˆm; m∗\n1, m∗\n2) ≥Φ( ˆm, ˆm),\n(14)\nand, for every pair (m1, m2) ∈G2,\nΦ(m∗\n1, m∗\n2) ≥Φ(m1, m2) −ρ(m1, m2; m∗\n1, m∗\n2).\n(15)\nNow, let us introduce the following functions:\nΨ1(m1) = φ1(m1) −φ2(m∗\n2) + 1\n2W 2\n2 (m1, m∗\n2),\nΨ2(m2) = φ2(m2) −φ1(m∗\n1) −1\n2W 2\n2 (m∗\n1, m2).\nWe will show that 0(·) ∈∂−Ψ1(m∗\n1) and 0(·) ∈∂+Ψ2(m∗\n2). Hereinafter, 0(·) is a\nfunction that is equal to zero everywhere. To this end, given F ∈L2(m∗\n2) and h > 0,\nwe compute\nΨ1((Id +hF)♯m∗\n1) −Ψ1(m∗\n1) = −Φ((Id +hF)♯m∗\n1, m∗\n2) + Φ(m∗\n1, m∗\n2)\n≥−ρ((Id +hF)♯m∗\n1, m∗\n2; m∗\n1, m∗\n2)\n= −(4L −1)W 2\n2 ((Id +hF)♯m∗\n1, m∗\n1) ≥−(4L −1)h2∥F∥2\nL2(m∗\n1).\n(16)\nHere, we ﬁrst used the deﬁnition of the function Ψ2 and, then, (15).\nInequal-\nity (16) and the equivalent deﬁnition of the subdiﬀerential (see (9)) imply that\n0(·) ∈∂−Ψ1(m∗\n1). The inclusion 0(·) ∈∂+Ψ2(m∗\n2) is proved in the similar way.\nNow, let ϑ be an optimal plan between m∗\n1 and m∗\n1. We put\ns1(x1) ≜\nZ\nRd(x1 −x2)ϑ(dx2|x1),\ns2(x2) ≜\nZ\nRd(x2 −x1)ϑ(dx1|x2),\n(17)\nwhere (ϑ(·|x1))x1∈Rd (respectively, (ϑ(·|x2))x2∈Rd) denotes the disintegration of ϑ\nw.r.t. m∗\n1 (respectively, w.r.t. m∗\n2). If\nw1(m) ≜1\n2W2(m, m∗\n2),\nw2(m) ≜1\n2W2(m∗\n1, m),\n(18)\nthen inequality (12) implies that\ns1 ∈∂+w1(m∗\n1),\ns2 ∈∂+w2(m∗\n2).\n(19)\n13\n\n\nThanks to equality φ1(m1) = Ψ1(m1)+φ2(m∗\n2)−w1(m1), inclusions (11), (19) and\nthe fact that 0(·) ∈∂−Ψ1(m∗\n1) give that\n−s1 ∈∂−φ1(m∗\n1).\nSimilarly, since 0(·) ∈∂+Ψ2(m∗\n2), we conclude that\ns2 ∈∂+φ2(m∗\n2).\nNow, let us recall that φ1 is a supersolution of (8). By substituting the measure\nm∗\n1 and the element of the subdiﬀerential −s1 into the deﬁnition of the supersolution,\nwe conclude that\nH(m∗\n1, −s1) + 1 −φ1(m∗\n1) ≤0.\nAnalogously, since φ2 is a subsolution of (8) and s2 ∈∂+φ2(m∗\n2), one has\nH(m∗\n2, s2) + 1 −φ2(m∗\n2) ≥0.\nThus,\nH(m∗\n1, −s1) −H(m∗\n2, s2) −φ1(m∗\n1) + φ2(m∗\n2) ≤0.\nBy the deﬁnition of the Hamiltonian, we have\ninf\nu∈U\nZ\nRd\n\n−s1(x), f(x, m∗\n1, u)\n\u000b\nm∗\n1(dx) −inf\nu∈U\nZ\nRd\n\ns2(y), f(y, m∗\n2, u)\n\u000b\nm∗\n2(dy)\n−φ1(m∗\n1) + φ2(m∗\n2) ≤0.\nLet u∗\n1 be such that\nZ\nRd\n\n−s1(x), f(x, m∗\n1, u∗\n1)\n\u000b\nm∗\n1(dx) = inf\nu∈U\nZ\nRd\n\n−s1(x), f(x, m∗\n1, u)\n\u000b\nm∗\n1(dx).\nUsing this, the deﬁnitions of the functions s1 and s2 (see (17)) and the fact that\nϑ is a plan between m∗\n1 and m∗\n1, we conclude the following:\nφ2(m∗\n2) −φ1(m∗\n1)\n≤\nZ\n(Rd)2⟨x2 −x1, f(x2, m∗\n2, u∗\n1) −f(x1, m∗\n1, u∗\n1)⟩ϑ(d(x1, x2)).\n(20)\nSince f is Lipschitz continuous with the constant L, we have that\nZ\n(Rd)2⟨x2 −x1, f(x2, m∗\n2, u∗\n1) −f(x1, m∗\n1, u∗\n1)⟩ϑ(d(x1, x2))\n≤\nZ\n(Rd)2 L\n\u0000∥x1 −x2∥2 + ∥x1 −x2∥· W2(m∗\n1, m∗\n2)\n\u0001\nϑ(d(x1, x2)\n≤2LW 2\n2 (m∗\n1, m∗\n2).\n14\n\n\nHere, we used the choice of ϑ as an optimal plan between m∗\n1 and m∗\n1. Therefore, we\nestimate the right-hand side in (20) and obtain\nφ2(m∗\n2) −φ1(m∗\n1) ≤2LW 2\n2 (m∗\n1, m∗\n2).\nThis and the deﬁnition of the function Φ yield that\nΦ(m∗\n1, m∗\n2) + 1\n2W 2\n2 (m∗\n1, m∗\n2) ≤2LW 2\n2 (m∗\n1, m∗\n2).\nFrom this and (14), it follows that\nΦ( ˆm, ˆm) + ρ( ˆm, ˆm; m∗\n1, m∗\n2) + 1\n2W 2\n2 (m∗\n1, m∗\n2) ≤2LW 2\n2 (m∗\n1, m∗\n2).\n(21)\nNow we use the deﬁnition of gauge function ρ and notice that\nW 2\n2 (m∗\n1, m∗\n2) ≤\n\u0002\nW2( ˆm, m∗\n1) + W2( ˆm, m∗\n2)\n\u00032 ≤\n2\n4L −1ρ( ˆm, ˆm; m∗\n1, m∗\n2).\nTherefore, (21) directly implies the following inequality:\nφ2( ˆm) −φ1( ˆm) = Φ( ˆm, ˆm) ≤0.\nSince ˆm was arbitrarily, we have that φ2 ≤φ1 on the whole domain G.\nTheorem 5.1 and Deﬁnition 4.5 imply the following.\nCorollary 5.2. There exists at most one solution of Dirichlet problem (8).\n6. Characterization of the value function\nThe aim of this section is to show that the Kruzhkov transform of the value\nfunction deﬁned by (7) is a viscosity solution of (8). This, in particular, gives the\nexistence result for (8).\nOur proof will use the relaxations of the time-optimal problem in the space of\nmeasures. To deﬁne them, for every ε > 0, we put\nMε ≜cl\n\u0010\b\nm ∈P2(Rd) : dist(m, M) ≤ε\n\t\u0011\n.\nHere,\ndist(m, M) ≜inf\n\b\nW2(m, µ) : µ ∈M\n\t\n.\nNotice that\nMε1 ⊂Mε2 whenever ε1 ≤ε2\n(22)\nand\nM =\n\\\nε>0\nMε.\n15\n\n\nWe now consider the time-optimal problem for the target set Mε. As above, given\nm(·) ∈C([0, +∞); P2(Rd)), we set\nτ ε(m(·)) ≜inf\n\b\nt ∈[0, +∞) : m(t) ∈Mε\t\n.\nBy (22), we have that, if ε1 ≤ε2,\nτ ε1(m(·)) ≥τ ε2(m(·)).\n(23)\nWe deﬁne the value function for the time-optimal problem with the target set Mε\nby the rule:\nValε(µ) ≜inf\nn\nτ ε(m(·; µ, ξ) : ξ ∈U\no\n.\nAs for the original value function we have the following properties.\nProposition 6.1. The function Valε is lower semicontinuous. Moreover, Valε(µ) =\n0 whenever µ ∈Mε.\nProposition 6.2. If ε > 0, µ ∈G is such that Valε(µ) < +∞. Then, there exists\nξε,µ satisfying\nValε(µ) = τ ε(m(·; µ, ξε,µ)).\nProposition 6.3. For every µ ∈cl G and h ∈[0, Valε(µ)],\nValε(µ) −h = inf\nξ∈Uh Val(m(h; µ, ξ)).\nProposition 6.4. If ε1 ≤ε2, then\nValε1(µ) ≥Valε2(µ).\nProof. If Valε(µ) = +∞, there is nothing to prove. We will consider the case where\nValε(µ) is ﬁnite. Let ξε1,µ be such that Valε1(µ) = τ ε1(m(·; µ, ξε1,µ)). By (23), we\nhave that\nτ ε1(m(·; µ, ξε1,µ)) ≥τ ε2(m(·; µ, ξε1,µ)).\nThis, together with the deﬁnition of the function Val yield the statement of the\nproposition.\nRecall that above we introduced the function φ that is the Kruzhkov transform of\nthe value function Val (see (7)).\nTheorem 6.5. The function φ is a viscosity solution of Dirichlet problem for Hamilton-\nJacobi equation (8).\nProof. We split the proof into four steps.\nStep 1. Here, we prove that the function φ is a supersolution to (8). First, notice\nthat φ is bounded and lower semicontinuous, while φ(µ) = 0 when µ ∈∂G.\n16\n\n\nFurthermore, we choose a probability µ ∈G and a function s ∈∂−φ∗(µ). By\nRemark 4.2, we have that, for every F ∈L2(µ),\n⟨s, F⟩µ ≤d−\nHφ(µ; F).\n(24)\nThe dynamic programmin principle (see Theorem 3.11) can be rewritten as follows:\nfor each h ∈[0, −ln(1 −φ(x)],\n(1 −φ(µ))eh = sup\nξ∈Uh\n(1 −φ(m(h; µ, ξ))).\nDue to Theorem 3.8, there exists ξh ∈Uh such that\n(1 −φ(µ))eh = (1 −φ(m(h; µ, ξh))).\nLet\nFh(y) ≜1\nh\nZ h\n0\nZ\nU\nf(X(t; y, µ, ξh), m(t; µ, ξh), u)ξh(du|t)dt,\nwhere X(·; y, µ, ξh) is a trajectory such that\nd\ndtX(t; y, µ, ξh) =\nZ\nU\nf(X(t; y, µ, ξ), m(t; µ, ξh), u)ξh(du|t),\nX(0; y, µ, ξ) = y.\nNotice that m(h; µ, ξh) = (Id +hFh)♯µ. Thus, we have that\nφ((Id +hFh)♯µ) −φ(µ) = (φ(µ) −1) · (eh −1).\n(25)\nFurthermore, Proposition A.3 gives the existence of a sequence {hk}∞\nk=1 and a prob-\nability ζ ∈P(U) such that hk →0, ∥Fhk −F∥L2(µ) →0 as k →∞for F(x) ≜\nR\nU f(x, µ, u)ζ(du). Therefore, from (25), we conclude that\nd−\nHφ(µ; F) ≤φ(µ) −1.\nThis and (24) imply that\nZ\nU\nZ\nRd ⟨s(x), f(x, µ, u)⟩µ(dx)ζ(du) ≤φ(µ) −1.\nUsing the deﬁnition of the Hamiltonian, we obtain the following inequality, for every\nµ ∈G and s ∈∂−φ(µ),\nH(µ, s) + 1 −φ(µ) ≤0.\nThus, φ is a supersolution of problem (8).\nStep 2. Let ψε be the Kruzhkov transform of the function Valε, i.e.,\nψε(µ) ≜1 −eValε(µ).\nMoreover, we put φε to be an upper envelope of the function ψε. This means that,\nfor every µ ∈G,\nφε(µ) ≜\nlim sup\nµ′→µ,\nµ′∈cl G\nψε(µ′) = lim\nδ↓0 sup\n\b\nψε(µ′) : W2(µ, µ′) ≤δ,\nµ′ ∈G \\ Mε\t\n.\n17\n\n\nStep 3. Here we prove that each function φε is a subsolution of (8). Notice that,\nby the construction, ψε(µ) = 0 whenever µ ∈int Mε. Thus, φε = 0 on ∂M and φε is\ncontinuous on ∂M.\nMoreover, the very deﬁnition of the function φε gives that it is bounded and upper\nsemicontinuous.\nFurthermore, let us show that the function φε satisﬁes the following condition: for\nevery µ ∈G and s ∈∂+φε(µ),\nH(µ, s) + 1 −φε(µ) ≥0\n(26)\nFirst, we consider the case when Valε(µ) > 0.\nBy the construction of the function φε, there exists a {µk}∞\nk=1 ⊂G\\Mε converging\nto µ, such that\nlim\nk→∞ψε(µk) = φε(µ).\nWithout loss of generality, we assume that ψε(µk) ≥ˆh ≜φε(µ)/2 > 0. By the\ndeﬁnition of the superdiﬀerential, for every F ∈L2(µ), we have\n⟨s, F⟩µ ≥d+\nHφε(µ; F).\nWe choose f 0(x) ≜f(x, µ, u0), where u0 is such that\nH(µ, s) = ⟨s, f(·, µ, u0)⟩µ .\n(27)\nHence,\nH(µ, s) =\n\ns, f 0\u000b\nµ ≥d+\nHφε(µ; f 0).\n(28)\nFurthermore, we evaluate d+\nHφε(µ; f 0) in the following way.\nSince ψε is the Kruzhkov transform of the function Valε, the dynamical program-\nming principle for problem with the target Mε (see Proposition 6.3) implies that,\nfor h ∈[0, −ln(1 −ψε(µk))],\n(1 −ψε(µk))eh = sup\nξ∈Uh\n(1 −ψε(m(h; µk, ξ))).\n(29)\nWe consider h ∈(0, ˆh) and the constant relaxed control on Uh deﬁned by the rule\nξ0(·|t) ≜δu0. Inequality (29) gives that\n(1 −ψε(µk))eh ≥1 −ψε(m(h; µk, ξ0)).\nUsing the deﬁnition of the function φε, the choice of the sequence {µk}∞\nk=1 and\nProposition 3.6, we conclude that\n(1 −φε(µ))eh ≥1 −φε(m(h; µ, ξ0)).\nNow, as above, let X(·; y, µ, ξ0) satisfy\nd\ndtX(t; y, µ, ξ0) = f(X(t; y, µ, ξ0), m(t; µ, ξ0), u0),\nX(0; y, µ, ξ0) = y.\n18\n\n\nSet\nFh(y) ≜1\nh\nZ h\n0\nf(X(t; y, µ, ξ0), m(t), u0)dt.\nProposition A.3 yields that\n∥Fh −f 0∥L2(µ) →0 as h →0.\nAdditionally, notice that X(h; y, µ, ξ0) = y + Fh(y) · h. Thus,\nm(h; µ, ξ0) = (Id +hFh)♯µ.\nTherefore,\nφε((Id +hFh)♯µ) −φε(µ) ≥(φ(µ) −1) · (eh −1).\nDividing both parts on h and using the deﬁnition of the Hadamard upper derivative,\nwe conclude that\nd+\nHφε(µ, f0) ≥φ∗(µ) −1.\nThis and (28) imply (26) for each µ ∈G satisfying φε(µ) > 0 and every s ∈∂+φε(µ).\nNow, let φε(µ) = 0. As above, we choose s ∈∂+φε(µ). Since φε(µ) = 0, we have\nthat d+\nHφε(µ; f 0) ≥0. Here, f 0(·) = f(·, µ, u0) with u0 satisfying (27). Thus,\nH(µ, s) ≥d+\nHφε(µ; f 0) ≥0.\nThis implies (26) for each µ satisfying φε(µ) = 0 and every s ∈∂+φε(µ).\nStep 4. We claim that, for each µ ∈G,\nlim\nε↓0 φε(µ) = φ(µ).\n(30)\nThis equality directly follows from the inequality\nlim\nε↓0 Valε(µ) ≥Val(µ).\n(31)\nthe monotonicity of the Kruzhkov transform and the comparison principle (see The-\norem 5.1). Equality (30) and Steps 1–3 give that the function φ is a viscosity solution\nof (8).\nSo, it remains to prove (31). First, recall that Proposition 6.4 means that ε 7→\nValε(µ) decreases. Therefore, there exists a limit limε↓0 Valε(µ).\nNow, we consider the case where\nT ≜lim\nε↓0 Valε(µ) < ∞.\n(32)\nBy Proposition 6.2, for each ε > 0, there exists a relaxed control ξε,µ such that\nValε(µ) = τ ε(m(·; µ, ξε,µ))\nand m(Valε(µ); µ, ξε,µ) ∈Mε.\nFurthermore, the fact that U is compact implies\nthat there exist a sequence {εk}∞\nk=1 converging to zero and a relaxed control ξ such\n19\n\n\nthat ξεk,µ tend to ξ.\nThus, by Proposition 3.6, we have that {m(·; µ, ξεk,µ)}∞\nk=1\nconverges to m(·; µ, ξ). Using the deﬁnition of the sets Mε, the functions Valε and\nthe convergence (32), we conclude that m(T; µ, ξ) ∈M. Hence,\nVal(µ) ≤T = lim\nε↓0 Valε(µ).\nThe case where\nlim\nε↓0 Valε(µ) = ∞\nis obvious.\n7. Topological properties of the value function\nFirst, we consider a suﬃcient condition assuring the continuity of the value func-\ntion.\nTheorem 7.1. Assume that the viscosity solution of Dirichlet problem (8) is con-\ntinuous on ∂G. Then, it is continuous at every point of cl G.\nRemark 7.2. Theorem 7.1 means that, if the value function is continuous on ∂G, then\nit is continuous everywhere on cl G. Moreover, the continuity of the value function\non ∂G directly follows from the small time local attainability property introduced\nin [29]. Indeed, for the examined case, it takes the following form: given ε > 0,\none can ﬁnd δ > 0 such that Val(µ) ≤ε whenever µ satisﬁes infµ′∈M W2(µ, µ′) < δ.\nObviously, under the small time local attainability assumption, the value function\nis continuous at every point of ∂G. If, additionally, M is compact, the small time\nlocal attainability property is equivalent to the continuity of the value function on\n∂G. For the space of probability measures, this property was considered in [17,20].\nIn these studies, the regularity of the value function for the time-optimal problem\nin the space of probability measures was investigated under the assumption that the\nproblem satisﬁes the small-time local attainability property, with the target being a\nhyperplane in the space of measures.\nProof of Theorem 7.1. Let φ denote the aforementioned viscosity solution. For every\nµ ∈cl G, we put\nφ∗(µ) ≜lim sup\nµ′→µ\nφ(µ).\nNotice that the function φ∗: cl G →R is upper semicontinuous. By following the\nsame reasoning as in Step 3 of the proof of Theorem 6.5, one can show that\nH(µ, s) + 1 −φ∗(µ) ≤0\nfor every µ ∈G and s ∈∂+φ∗(µ). Moreover, the continuity of the function φ on\n∂G gives that φ∗(µ) = 0 for every µ ∈∂G. Thus, φ∗is a subsolution of (8). The\ncomparison principle (see Theorem 5.1) gives that φ∗(µ) ≤φ(µ) on cl G.\nSince\nthe opposite inequality directly follows from the deﬁnition of the function φ∗, we\nconclude that φ = φ∗and the function φ is continuous.\n20\n\n\nNow, let us consider the perturbation of the dynamics of the time-optimal problem.\nSince the value function is generally only lower semicontinuous, we need a concept of\nconvergence. In our opinion, the most natural convergence here is the Γ-convergence\ndeﬁned as follows (see [12, Deﬁnition 1.5]).\nDeﬁnition 7.3. For each natural n, let Fn be a functional on deﬁned on a topological\nspace X with values in R ∪{+∞}. The sequence {Fn}∞\nn=1 is said to Γ-converge to\na functional F : X →R ∪{+∞} if following two conditions hold:\n1. for every sequence {xn}∞\nn=1 ∈X satisfying lim\nn→∞xn = x,\nF(x) ≤lim inf\nn→∞Fn(xn);\n2. for each x ∈X, there is a sequence {xn}∞\nn=1 ∈X such that\nlim\nn→∞xn = x and F(x) ≥lim sup\nn→∞Fn(xn).\nIf {Fn}∞\nn=1 Γ-converges to F, then we write\nF = Γ- lim\nn→∞Fn.\nNow we consider a perturbation of the dynamics of (3). For each natural n, let a\nfunction fn : Rd × P2(Rd) × U →Rd, n ∈N be continuous and Lipschitz continuous\nw.r.t. x and m. If µ ∈P2(Rd), ξ ∈U, then there exists a unique distributional\nsolution to\n∂tmn(t) + div\n Z\nU\nfn(x, m(t), u)m(t)ξ(du|t)\n!\n= 0,\nm(0) = µ.\nWe denote this solution by m(·; µ, ξ). The value function of the perturbed time-\noptimal is deﬁned by the rule:\nValn(µ) ≜inf\n\b\nτ(mn(·; µ, ξ) : ξ ∈U\n\t\n.\nTheorem 7.4. Assume the following convergence of the dynamics fn to f: for each\nc > 0,\nsup\nn\n∥fn(x,m, u) −f(x, m, u)∥:\nx ∈Rd, m ∈P2(Rd), ς(m) ≤c, u ∈U\no\n→0 as n →∞.\nThen,\nΓ- lim\nn→∞Valn = Val .\n21\n\n\nProof. Let us prove the ﬁrst property in Deﬁnition 7.3. Let {µn}∞\nn=1 ∈P2(Rd) be a\nsequence converging to µ. If Valn(µn) →+∞as n →∞, then, obviously,\nVal(µ) ≤lim\nn→∞Valn(µn).\nIf the limit is ﬁnite, we, without loss of generality, can assume that the whole se-\nquence {Valn(µn)}∞\nn=1 converges to some number θ. By Theorem 3.8, for each natural\nn, there exists a control ξn ∈U such that\nθn ≜τ(mn(·; µn, ξn)) = Valn(µn).\nAdditionally, for suﬃciently large n one has that τ(mn(·; µn, ξn)) < 2θ. Since U\nis compact, we passing, if necessary, to a subsequence, have that ξn →ξ as n →\n∞. Thus, Proposition 3.6 gives that {mn(·; µn, ξn)}∞\nn=1 converges to m(·; µ, ξ) on\nC([0, 2θ], P2(Rd)). Since, m(θn; µn, ξn) ∈M, whereas M is closed, we have that\nm(θ; µ, ξ) ∈M. Hence,\nVal(µ) ≤τ(m(·; µ, ξ)) ≤θ = lim inf\nn→∞Valn(µn).\nThis proves the ﬁrst property of the Γ-convergence.\nTo prove the second property, we choose a measure µ ∈P2(Rd). First, we assume\nthe case when Val(µ) < ∞. As above, by Theorem 3.8, one can ﬁnd a control ξ ∈U,\nsuch that Val(µ) = τ(m(·; µ, ξ)). For shortness, we put θ ≜Val(µ), ν ≜m(θ, µ, ξ).\nNotice that ν ∈M. Now, let m′\nn(·) : [0, θ] →P2(Rd) satisfy\n∂tm′\nn(t) + div\n Z\nU\nfn(·, m′\nn(t), u)ξ(du|t)m′\nn(t)\n!\n= 0,\nm′\nn(θ) = ν.\nOne can use the time reversing and prove the existence of such motion. Additionally,\nProposition 3.6 applied in the reverse time gives that\nsup\nt∈[0,θ]\nW2(m′\nn(t), m(t, µ, ξ)) →0 as n →∞.\nWe put µn ≜m′\nn(0). By construction, mn(·, µn, ξ) = m′\nn(·). Moreover, W2(µn, µ)\ntends to 0 when n →∞. In particular, µn ∈G for suﬃciently large n. Thus, due to\nthe fact that m′\nn(θ) = ν ∈M, τ(mn(·, µn, ξ)) ≤θ. Hence,\nValn(µn) ≤τ(mn(·; µn, ξ)) ≤τ(m(·; µ, ξ)) = Val(µ).\nThis means that lim sup\nn→∞Valn(µn) ≤Val(µ).\nIf Val(µ) = +∞, then, for an arbitrary sequence {µn}∞\nn=1 ∈G converging to\nµ ∈G, one has that lim sup\nn→∞Valn(µn) ≤Val(µ).\n22\n\n\n8. An example\nWe consider a time-optimal problem for the nonlocal continuity equation assuming\nthat d = 1, U = [−1, 0],\nf(x, m, u) = u −\nZ\nR\nym(dy),\nwhile the target set is\nM =\n(\nm ∈P2(R) :\nZ\nR\nym(dy) = 0\n)\n.\nTherefore, nonlocal continuity equation (3) takes the form:\n∂tm(t) + div\n \n(u(t) −\nZ\nR\nym(t, dy))m(t)\n!\n= 0, m(0) = µ ∈P2(R).\nThe mean of the measure m(t) deﬁned by the rule:\n¯m(t) ≜\nZ\nR\nym(t, dy)\nobeys the following equation:\nd\ndt ¯m(t) = u −¯m(t),\n¯m(0) = ¯µ.\n(33)\nHere, given a measure µ, we put ¯µ ≜\nR\nR yµ(dy). Notice that the target set also is\ndetermined by the mean, i.e, the aim of the control is to ﬁnd the ﬁrst time τ such\nthat ¯m(τ) = 0. Given an open-loop control u(·), we have that the corresponding\nevaluation of the mean is\n¯m(t) =\n \n¯µ +\nZ t\n0\nu(t′)et′dt′\n!\ne−t.\nThus, the optimal control is as follows:\n1. u(·) ≡−1 if ¯µ ≥0;\n2. no optimal control if ¯µ < 0;\nwhereas the value function is\nVal(µ) =\n\u001a ln(¯µ + 1),\n¯µ ≥0,\n+∞,\notherwise.\nNotice that the value function is only lower semicontinuous.\n23\n\n\nNow, let us write down the Hamiltonian corresponding to the examined problem.\nIt has the form:\nH(m, s) =\ninf\nu∈[−1;0]\nZ\nR\ns(x)\n\u0010\nu −\nZ\nR\nym(dy)\n\u0011\nm(dx)\n=\n(\u0010\n−1 −¯m\n\u0011 R\nR s(x)m(dx),\nR\nR s(x)m(dx) > 0,\n−¯m\nR\nR s(x)m(dx),\nR\nR s(x)m(dx) ≤0.\nThus, the Dirichlet problem is\nH(m, ∇φ(m)) + 1 −φ(m) = 0,\nZ\nR\nym(dy) ̸= 0,\nφ(m) = 0, when\nZ\nR\nym(dy) = 0.\nDue to the Kruzhkov transform of the function Val, the viscosity solution is equal\nto\nφ(µ) =\n(\n1 −e−ln(1+¯µ),\n¯µ ≥0,\n1,\n¯µ < 0.\n9. Equivalent deﬁnitions of the viscosity solution\nAs we mentioned in the Introduction, there are several approaches to the notion\nof a viscosity solution of the Hamilton-Jacobi equation in the space of probability\nmeasures. In this paper, we do not touch the approaches relying on the L2 represen-\ntation and restrict our attention only to some concepts involving intrinsic sub- and\nsuperdiﬀerentials. We consider two approaches coming back to papers [5] and [27].\n9.1. Measure-varying viscosity solution\nThis approach relies on deﬁnitions proposed in [5]. There, the Hadamard lower\nderivative of a function ψ : Br(m) →R at m in the direction F is deﬁned by the\nrule:\n˜d−\nHψ(m; F) ≜\nlim inf\nh↓0,W2(µ,(Id +hF )=o(h)\n1\nh[ψ(µ) −ψ(m)].\nThe corresponding subdiﬀerential is deﬁned by the rule\n˜∂−ψ(m) ≜\nn\ns ∈L2(m) : ⟨s, F⟩m ≤˜d−\nHψ(m; F) for every F ∈L2(m)\no\n.\nBelow, we will call these subdiﬀerentials a measure-varying subdiﬀerential (MV-\nsubdiﬀerential for shortness). Notice that\n˜d−\nHψ(m; F) ≤d−\nHψ(m; F).\nThus,\n˜∂−ψ(m) ⊂∂−ψ(m).\n(34)\n24\n\n\nOne can also deﬁne the MV-superdiﬀerential letting\n˜∂+ψ(m) ≜−˜∂−(−ψ)(m).\nAnalogously,\n˜∂+ψ(m) ⊂∂+ψ(m).\n(35)\nNotice that, for every ψ1, ψ2 : Br(m) →R, one has that\n˜∂−ψ1(m) + ˜∂−ψ2(m) ⊂˜∂−(ψ1 + ψ2)(m).\n(36)\nIndeed, given s1 ∈˜∂−ψ1(m), s2 ∈˜∂−ψ2(m), F ∈L2(m), let {hk}∞\nk=1 ⊂(0, +∞)\nconverge to zero and let {µ′\nk}∞\nk=1 ⊂Br(m) be such that W2(µ′\nk, (Id +hkF)♯m) = o(hk)\nwhile\n˜d−\nH(ψ1 + ψ2)(m; F) = lim\nk→∞\n1\nhk\n[(ψ1(µ′\nk) + ψ2(µ′\nk)) −(ψ1(m) −ψ2(m))].\nBy construction, for i = 1, 2,\n⟨si, F⟩m ≤lim inf\nk→∞\n1\nhk\n[ψi(µ′\nk) −ψi(m)].\nThrefore,\n⟨s1 + s2, F⟩m ≤˜d−\nH(ψ1 + ψ2)(m).\nSince F was an arbitrary element of L2(m), s1 + s2 ∈˜∂(ψ1 + ψ2)(m).\nAnalogously,\n˜∂+ψ1(m) + ˜∂+ψ2(m) ⊂˜∂+(ψ1 + ψ2)(m).\nReplacing the sub- and superdiﬀerentials in the deﬁnitions of super- and subso-\nlutions with MV-sup- and MV-superdiﬀerentials, we arrive at the concepts of MV-\nviscosity supersolutions, subsolutions and MV-viscosity solutions of problem (8).\nProposition 9.1. Each viscosity solution of (8) is a MV-viscosity solution. More-\nover, the MV -viscosity solution of (8) is unique.\nProof. The fact that each viscosity solution of (8) is a MV-viscosity solution directly\nfollows from (34) and (35).\nTo show the uniqueness of the MV-viscosity solution, one should prove the com-\nparison principle for MV-viscosity sub- and supersolution. It mimics the proof of\nTheorem 5.1 with two diﬀerences.\nFirst, we are to prove that 0(·) ∈˜∂−Ψ1(m∗\n1). To this end, we choose F ∈L2(m∗\n1),\nh > 0 and m′\n1 ∈G such that W2((Id +hF)♯m∗\n1, m′\n1) = o(h). We have that\nΨ1(m′\n1) −Ψ1(m∗\n1) = −Φ(m′\n1, m∗\n2) + Φ(m∗\n1, m∗\n2)\n≥−ρ((Id +hF)♯m∗\n1, m∗\n2; m∗\n1, m∗\n2)\n= −(4L −1)W 2\n2 (m′\n1, m∗\n1)\n≥−(4L −1)[h2∥F∥2\nL2(m∗\n1) + W2(m′\n1, (Id +hF)♯m∗\n1).\n25\n\n\nThus, ˜d−\nHφ(m; F) ≥0 for every F ∈L2(m). This as in the proof of Theorem 5.1\ngives that 0(·) ∈˜∂Ψ1(m∗\n1).\nSecond diﬀerence is to show that the functions s1 and s2 deﬁned by (17) lie\nin ˜∂+w1(m∗\n1) and ˜∂+w2(m∗\n2) respectively (here the functions w1 and w2 are given\nby (18)). This directly follows from the deﬁnition of the MV-superdiﬀerential and (12).\n9.2. Plan determined optimal displacement viscosity solution\nNotice that the deﬁnition of the sub-/superdiﬀerentials used in the paper and the\nMV-sub- superdiﬀerentails rely directions given by a square integrable functions.\nOne can also consider more general directions those are determined by plans. This\napproach is natural for mean ﬁeld type optimal control problems where the agents\nare aﬀected by individual controls. The study of a viscosity solution for Hamilton-\nJacobi equations corresponding to ﬁnite horizon mean ﬁled type control problems is\npresented in [27]. We also refer to papers [18, 19] where the viscosity solutions for\nDirichlet problem corresponding to time-optimal mean ﬁeld type control problem\nwere introduced. Additionally, the approach of [18,19,27] implies that the elements\nof the subdiﬀerential are optimal displacements. Recall that a function s ∈L2(m)\nis an optimal displacement from m provided that s = Id −T, where T is an optimal\ntransportion map between m and T♯m. We denote the set of all optimal displacement\nfrom m by Dis(m). Equivalent deﬁnitions of optimal displacement are derived in [27,\nLemmas 4 and 5]. Now let us introduce the concept of sub- and superdiﬀerentials\ngiven in [27]. We will call them PDOD-sub- and superdiﬀerentials due to the fact\nthat this concept uses directions determined by plans and optimal displacements.\nLet φ : Br(m) →R.\nDeﬁnition 9.2. Let δ ≥0, a δ-PDOD-subdiﬀerential is a set ˆ∂−\nδ φ(m) consisting of\nall s ∈L2(m) such that\n• −s ∈Dis(m);\n• for every µ ∈Br(m), and π ∈Π(m, µ), one has that\nφ(µ) −φ(m) ≤\nZ\nRd⟨s(x), y −x⟩π(d(x, y))+δ\n\u0014 Z\nRd ∥y −x∥2π(d(x, y)\n\u00151/2\n+o\n \u0014 Z\nRd ∥y −x∥2π(d(x, y)\n\u00151/2!\n.\nA δ-PDOD-subdiﬀerential is a set\nˆ∂+\nδ φ(m) ≜−ˆ∂−\nδ (−φ)(m).\nIf δ = 0, then we will use words PDOD-subdiﬀerentials and PDOD-superdiﬀerentials.\n26\n\n\nNotice that\nˆ∂−\n0 φ(m) =\n\\\nδ>0\nˆ∂−\nδ φ(m),\nˆ∂+\n0 φ(m) =\n\\\nδ>0\nˆ∂+\nδ φ(m).\n(37)\nMoreover, the deﬁnitions 4.1, 9.2 yield that\nˆ∂−\n0 φ(m) ⊂∂−φ(m).\n(38)\nSimilarly,\nˆ∂+\n0 φ(m) ⊂∂+φ(m).\n(39)\nUsing notions of δ-PDOD-sub- and superdiﬀerentials, one can deﬁne analogs of\nthe viscosity solution as follows.\nDeﬁnition 9.3. A bounded lower semicontinuous function φ1 : cl G →R is called a\nrelaxed PDOD-viscosity supersolution of problem (8) provided that\n• for each m ∈∂G, φ1(m) = 0;\n• for every δ > 0, m ∈G and s ∈ˆ∂−\nδ φ1(m)\nH(m, s) + 1 −φ1(m) ≤Cδ,\nwhere C is a constant depending only on H and G.\nDeﬁnition 9.4. A bounded upper semicontinuous function φ2 : cl G →R is called\na relaxed PDOD-viscosity subsolution of problem (8) if\n• for each m ∈∂G, φ2(m) = 0;\n• φ2 is continuous on ∂G;\n• for every δ > 0, m ∈G and s ∈∂+φ2(m) one has that\nH(m, s) + 1 −φ2(m) ≥−Cδ,\nwhere C is a constant depending only H and G.\nDeﬁnition 9.5. A bounded function φ : cl G →R is called a PDOD-viscosity\nsolution if\n• φ is a PDOD-supersolution of (8);\n• there exists a sequence {φk}∞\nk=1, where, for each k, the function φk : cl G →R\nis a PDOD-subsolution of (8), while\nφ(m) = lim\nk→∞φk(m).\n27\n\n\nFurthermore, we consider the limiting case where only pure PDOD-sub- and\nPDOD-superdiﬀerentials are used.\nThe replacement of words viscosity sub- and\nsuperdiﬀerentials with the words PDOD-sub- and PDOD-superdiﬀerentials leads to\nthe notion of PDOD-viscosity solution. The link between the introduced deﬁnitions\nis as follows.\nProposition 9.6. Each relaxed PDOD-viscosity solution is a PDOD-viscosity solu-\ntion.\nProof. The fact that each relaxed PDOD-viscosity subsolution is a PDOD-viscosity\nsubsolution directly follows from equality (37) and the deﬁnitions of these notions.\nAnalogously, every relaxed PDOD-viscosity supersolution is a PDOD-viscosity su-\npersolution. These two statements prove the proposition.\nNow we discuss the link between PDOD-viscosity solutions and viscosity solutions\nin the sense of Deﬁnition 4.5.\nProposition 9.7. Each viscosity solution in the sense of Deﬁnition 4.5 is a PDOD-\nviscosity solution. A PDOD-viscosity solution is unique.\nProof. First notice that due to (38), one has that each viscosity subsolution is a\nPDOD-viscosity subsolution. Analogously, due to (39), we have that each viscosity\nsupersolution is a PDOD-viscosity supersolution. This proves the ﬁrst statement of\nthe proposition.\nTo obtain the uniquenss of the PDOD-viscosity solution, it suﬃces to prove the\ncomparison principle for PDOD-viscosity sub- and supersolutions. It mimics the\nproof of Theorem 5.1 with two diﬀerences those concern the sub- and superdiﬀeren-\ntials. First, we are to show that 0(·) ∈ˆ∂−\n0 Ψ1(m∗\n1). To this end, consider m′ ∈G and\na plan π ∈Π(m′, m∗\n1) and replace (16) with the following estimate\nΨ1(m′)−Ψ1(m∗\n1) = −Φ(m′, m∗\n2) + Φ(m∗\n1, m∗\n2)\n≥−ρ(m′, m∗\n2; m∗\n1, m∗\n2) = −(4L −1)W 2\n2 (m′, m∗\n1)\n≥−(4L −1)\nZ\n(Rd)2 ∥x′ −x∥2π(d(x′, x).\nThis and the fact that 0(·) is an optimal displacement yield the inclusion 0(·) ∈\nˆ∂−\n0 Ψ1(m∗\n1). Analogously, 0(·) ∈ˆ∂+\n0 Ψ2(m∗\n2).\nThe second diﬀerence as in the case of MV-viscosity solution is that s1 ∈ˆ∂+\n0 w1(m∗\n1),\ns2 ∈ˆ∂−\n0 w2(m∗\n2). This directly follows from (12) and the fact that s1 (respectively,\ns2) is an optimal displacement from m∗\n1 (respectively, from m∗\n2). Furthermore, we\nuse the deﬁnition of PDOD-sub and PDOD-superdiﬀerentials and conclude that\n−s1 ∈ˆ∂−\n0 φ1(m∗\n1), while s2 ∈ˆ∂+\n0 φ1(m∗\n2). Inclusion 36 gives that −s1 ∈ˆ∂−\n0 φ1(m∗\n1) and\ns2 ∈ˆ∂+\n0 φ2(m∗\n1).\nPropositions 9.1, 9.6, 9.7 imply the following.\nTheorem 9.8. The notion of the viscosity solution presented in Deﬁnition 4.5 is\nequivalent to the notions of MV-, relaxed PDOD-, and PDOD-viscosity solutions.\n28\n\n\nA. Properties of relaxed controls\nRecall that, if µ ∈P2(Rd),\nς(µ) ≜\n\" Z\nRd ∥x∥2µ(dx)\n#1/2\n.\nFurthermore, for given T > 0, µ ∈P2(Rd), ξ ∈UT and m(·) ≜m(·; µ, ξ), we denote\nby X(·; y, µ, ξ) a solution of the following initial value problem:\nd\ndtx(t) =\nZ\nU\nf(x(t), m(t), u)ξ(du|t), x(0) = y.\n(40)\nProposition A.1. The following estimates hold true, for every t ∈[0, T], y ∈Rd\nand some constants dependent only on ς(µ) and T:\n1. ς(m(t)) ≤c0;\n2. W2(m(t), µ) ≤c1t;\n3. ∥X(t; y, µ, ξ)∥≤c3(1 + ∥y∥);\n4. ∥X(t; y, µ, ξ) −y∥≤c4(1 + ∥y∥)t.\nProof. The proof relies on the integral representation of (40)\nX(·; y, µ, ξ) = y +\nZ t\n0\nZ\nU\nf(X(t′; y, µ, ξ), m(t′), u)ξ(du|t′)dt′\n(41)\nand the upper bound on f that directly follows from Hypothesis 3.2\n∥f(x, m, u)∥≤C′(1 + ∥x∥+ ς(m)).\n(42)\nHere C′ is a constant.\nUsing this, the fact that ς(m(t)) = ∥X(t; ·, µ, ξ)∥L2(µ) and the Minkowski inequal-\nities, we conclude that\nς(m(t)) ≤ς(µ) + C′\nZ t\n0\n(1 + 2ς(m(t′))dt′.\nThis and the Gronwall’s inequality imply the ﬁrst statement of the proposition.\nTo show the second statement, it suﬃces to notice that W2(m(t), µ) ≤∥X(t; ·, µ, ξ)−\nId ∥L2(µ), use estimate (42) and the ﬁrst statement.\nThe third and the fourth statements directly follow from integral representation of\nsolution (41), estimate (42), the ﬁrst statement of this proposition and the Gronwall’s\ninequality.\n29\n\n\nBelow, we will prove that the set of averaged velocities over the time interval [0, h]\nis precompact in L2. This result is based on the following.\nLemma A.2. Let\n• {ζk}∞\nk=1 ⊂P(U) narrowly converges to ζ ∈P(U);\n• a function g : Rd × U →Rd be continuous and satisﬁes the sublinear growth\nconditions: ∥g(x, u)∥≤C1(1 + ∥x∥);\n• µ ∈P2(Rd);\n• for each k, Gk ∈L2(µ) be deﬁned by the rule: Gk(x) ≜\nR\nU g(x, u)ζk(du);\n• G ∈L2(µ) be such that G(x) ≜\nR\nU g(x, u)ζ(du).\nThen the sequence {Gk}∞\nk=1 converges to G in L2(µ).\nProof. Let ε > 0. For each R > 0, we have that\nZ\nRd ∥Gk(x) −G(x)∥2µ(dx) =\nZ\n∥x∥≤R\n∥Gk(x) −G(x)∥2µ(dx)\n+\nZ\n∥x∥>R\n∥Gk(x) −G(x)∥2µ(dx).\n(43)\nUsing the deﬁnitions of the functions Gk, G and the sublinear growth condition we\nhave that\nZ\n∥x∥>R\n∥Gk(x) −G(x)∥2µ(dx) ≤8\nZ\n∥x∥>R\nC2\n1(1 + ∥x∥2)µ(dx).\n(44)\nSince µ ∈P2(Rd), one can choose R such that\n8\nZ\n∥x∥>R\nC2\n1(1 + ∥x∥2)µ(dx) ≤ε.\nSince the function g is continuous, there exists a ﬁnite set {yj}J\nj=1 ⊂BR(0) such that\nmax\n∥x∥≤R min\nj=1,...,J max\nu∈U ∥g(x, u) −g(yj, u)∥≤√ε/3.\nFurthermore, we use the assumption that {ζk}∞\nk=1 narrowly converges to ζ. Thus,\nthere exists a natural number K such that for every k ≥K and j = 1, . . . , J,\n\f\f\f\f\f\nZ\nU\ng(yj, u)ζk(du) −\nZ\nU\ng(yj, u)ζ(du)\n\f\f\f\f\f ≤√ε/3.\nMoreover, for every x satisfying ∥x∥≤R, there exists a number j ∈{1, . . . , J} such\nthat\nmax\nu∈U ∥g(x, u) −g(yj, u)∥≤√ε/3.\n30\n\n\nHence, if ∥x∥≤R and k ≥K, then\n|Gk(x) −G(x)| =\n\f\f\f\f\f\nZ\nU\ng(x, u)ζk(du) −\nZ\nU\ng(x, u)ζ(du)\n\f\f\f\f\f\n≤\n\f\f\f\f\f\nZ\nU\ng(yj, u)ζk(du) −\nZ\nU\ng(yj, u)ζ(du)\n\f\f\f\f\f + 2√ε/3 ≤√ε.\n(45)\nFrom (45) it directly follows that\nZ\n∥x∥≤R\n∥Gk(x) −G(x)∥2µ(dx) ≤ε.\nEvaluating the right-hand side of (43) using this estimate and (44), we have that,\nthere exists a natural number K such that, for every k ≥K, one has that\nZ\nRd ∥Gk(x) −G(x)∥2µ(dx) ≤2ε.\nProposition A.3. Let\n• µ ∈P2(Rd);\n• T > 0;\n• for each h ∈[0, T] ξh ∈Uh;\n• Fh(y) ≜h−1 R\n[0,h]×U f(X(t; y, µ, ξh), m(t; µ, ξh), u)ξ(d(t, u)).\nThen, there exists a sequence {hk}∞\nk=1 such that hk →0, ∥Fhk −F∥L2(µ) →0 as\nk →∞, where F ∈L2(µ) with F(y) ≜\nR\nU f(y, µ, u)ζ(du) for some ζ ∈P(U). If,\nadditionally, one can ﬁnd a probability ζ0 ∈P(U) such that ξh(·|t) = ζ0, then ζ = ζ0\nand ∥Fh −F∥L2(µ) →0 as h →0.\nProof. Let F ′\nh(y) ≜h−1 R\n[0,h]×U f(y, µ, u)ξh(d(t, u)). From Proposition A.1, we have\nthat\n∥F ′\nh −Fh∥≤C2h.\n(46)\nFurthermore, we deﬁne the measure ζh ∈P(U) by the rule: for every Borel set\nΓ ⊂U,\nζh(Γ) ≜h−1ξh([0, h] × Γ).\nThus, F ′\nh(y) =\nR\nU f(y, µ, u)ζh(du).\nSince U is compact, there exists a sequence\n{hk}∞\nk=1 converging to zero such that {ζhk}∞\nk=1 narrowly converges to some ζ ∈P(U).\nLetting F(y) ≜\nR\nU f(y, µ, u)ζ(du) and using Proposition A.2, we conclude that\n∥F ′\nhk−F∥L2(µ) →0 as k →∞. This and (46) imply the convergence ∥Fhk−F∥L2(µ) →\n0 as k →∞.\nIn the case where ξh(·|t) = ζ0, we have that F ′\nh(y) =\nR\nU f(y, µ, u)ζ0(du) = F(y).\nThus, Fh converges itself to F in L2(µ).\n31\n\n\nReferences\n[1] L. Ambrosio and W. Gangbo. Hamiltonian ODEs in the Wasserstein space of\nprobability measures. Comm. Pure Appl. Math., 61(1):18–53, 2008.\n[2] L. Ambrosio, N. Gigli, and G. Savaré.\nGradient ﬂows in metric spaces and\nin the space of probability measures.\nLectures in Mathematics ETH Zürich.\nBirkhäuser, 2. ed edition, 2008.\n[3] Y. Averboukh.\nKrasovskii-Subbotin approach to mean ﬁeld type diﬀerential\ngames. Dyn. Games Appl., 9:573–593, 2019.\n[4] Y. Averboukh. Stability analysis of mean ﬁeld type control system with major\nagent. COT, 2023:1–22, 2023.\n[5] Z. Badreddine and H. Frankowska. Hamilton-Jacobi inequalities on a metric\nspace. J. Diﬀer. Equ., 271:1058–1091, 2021.\n[6] M. Bardi and I. Capuzzo-Dolcetta. Optimal control and viscosity solutions of\nHamilton-Jacobi-Bellman equations. Birkhäuser Boston, Boston, 1997.\n[7] V. I. Bogachev. Measure theory. Vol. I and II. Berlin: Springer, 2007.\n[8] M. Bongini, M. Fornasier, F. Rossi, and F. Solombrino. Mean-ﬁeld Pontryagin\nmaximum principle. J. Optim. Theory Appl., 175(1):1–38, 2017.\n[9] B. Bonnet. A Pontryagin maximum principle in Wasserstein spaces for con-\nstrained optimal control problems. ESAIM: COCV, 25:38, 2019. Id/No 52.\n[10] B. Bonnet and H. Frankowska.\nNecessary optimality conditions for optimal\ncontrol problems in Wasserstein spaces.\nAppl. Math. Optim., 84:1281–1330,\n2021.\n[11] B. Bonnet and F. Rossi. The Pontryagin maximum principle in the Wasserstein\nspace. Calc. Var. Partial Diﬀer. Equ., 58(1):36, 2019. Id/No 11.\n[12] A. Braides. Gamma-convergence for beginners. Oxford University Press, 2002.\n[13] R. W. Brockett. Notes on the control of the Liouville equation. In Control of\nPartial Diﬀerential Equations, pages 101–129, 2012.\n[14] P. Cardaliaguet, F. Delarue, J.-M. Lasry, and P.-L. Lions. The master equation\nand the convergence problem in mean ﬁeld games, 2019.\n[15] P. Cardaliaguet and M. Quincampoix. Deterministic diﬀerential games under\nprobability knowledge of initial condition. Int. Game Theory Rev., 10(01):1–16,\n2008.\n32\n\n\n[16] J. A. Carrillo, M. Fornasier, G. Toscani, and F. Vecil. Particle, kinetic, and hy-\ndrodynamic models of swarming. In Mathematical modeling of collective behav-\nior in socio-economic and life sciences, pages 297–336. Boston, MA: Birkhäuser,\n2010.\n[17] G. Cavagnari. Regularity results for a time-optimal control problem in the space\nof probability measures. Math. Control Relat. Fields, 7(2):213–233, 2017.\n[18] G. Cavagnari and A. Marigonda. Time-pptimal control problem in the space of\nprobability measures, 2015.\n[19] G. Cavagnari and A. Marigonda. Attainability property for a probabilistic target\nin Wasserstein spaces. Discrete Continuous Dyn. Syst. - A, 41(2):777–812, 2021.\n[20] G. Cavagnari, A. Marigonda, and M. Quincampoix. Compatibility of state con-\nstraints and dynamics for multiagent control systems. J. Evol. Equ., 21(4):4491–\n4537, 2021.\n[21] R. M. Colombo, M. Garavello, M. Lécureux-Mercier, and N. Pogodaev. Con-\nservation laws in the modeling of moving crowds. Preprint, arXiv:1211.0408\n[math.AP] (2012), 2012.\n[22] M. G. Crandall and P.-L. Lions. Viscosity solutions of Hamilton-Jacobi equa-\ntions. Trans. Amer. Math. Soc., 277(1):1, 1983.\n[23] C. Dogbe. On the modelling of crowd dynamics by generalized kinetic models.\nJ. Math. Anal. Appl., 387(2):512–532, 2012.\n[24] R. C. Fetecau, Y. Huang, and T. Kolokolnikov. Swarm dynamics and equilibria\nfor a nonlocal aggregation model. Nonlinearity, 24(10):2681–2716, 2011.\n[25] W. Gangbo and A. Tudorascu. On diﬀerentiability in the Wasserstein space and\nwell-posedness for Hamilton-Jacobi equations. J. Math. Pures Appl., 125:119–\n174, 2019.\n[26] R. L. Hughes. A continuum theory for the ﬂow of pedestrians. Transp. Res.\nPart B, 36(6):507–535, 2002.\n[27] C. Jimenez, A. Marigonda, and M. Quincampoix. Optimal control of multiagent\nsystems in the Wasserstein space. Calc. Var. Partial Diﬀer. Equ., 59(2):45, 2020.\nId/No 58.\n[28] C. Jimenez, A. Marigonda, and M. Quincampoix.\nDynamical systems\nand Hamilton–Jacobi–Bellman equations on the Wasserstein space and their\nl<sup>2</sup> representations. SIAM J. Math. Anal., 55(5):5919–5966, 2023.\n[29] M. Krastanov and M. Quincampoix. Local small time controllability and at-\ntainability of a set for nonlinear control system. ESAIM Control Optim. Calc.\nVar., 6:499–516, 2001.\n33\n\n\n[30] A. Kruger, S. Plubtieng, and T. Seangwattana.\nBorwein–Preiss variational\nprinciple revisited. J. Math. Anal. Appl., 435:1183–1193, 2016.\n[31] A. Mogilner and L. Edelstein-Keshet. A non-local model for a swarm. J. Math.\nBiol., 38(6):534–570, 1999.\n[32] G. Naldi, L. Pareschi, and G. Toscani, editors. Mathematical modeling of col-\nlective behavior in socio-economic and life sciences. Model. Simul. Sci. Eng.\nTechnol. Boston, MA: Birkhäuser, 2010.\n[33] H. Pham and X. Wei. Bellman equation and viscosity solutions for mean-ﬁeld\nstochastic control problem. ESAIM Control Optim. Calc. Var., 24(1):437–461,\n2018.\n[34] B. Piccoli and F. Rossi.\nMeasure-theoretic models for crowd dynamics.\nIn\nCrowd dynamics, Volume 1. Theory, models, and safety problems, pages 137–\n165. Cham: Birkhäuser, 2018.\n[35] N. Pogodaev. Optimal control of continuity equations. Nonlinear Diﬀer. Equ.\nAppl., 23(2):24, 2016. Id/No 21.\n[36] M. H. Rinaldo M. Colombo and M. Mercier. Control of the continuity equation\nwith a non local ﬂow. ESAIM Control Optim. Calc. Var., 17(2):353–379, 2011.\n[37] A. I. Subbotin. Generalized solutions of ﬁrst-order PDEs: The dynamical ppti-\nmization perspective. Birkhäuser Boston, Boston, 1995.\n[38] B.-C. Wang and Y. Liang.\nRobust mean ﬁeld social control problems with\napplications in analysis of opinion dynamics. Int. J. Control, 95(12):3309–3325,\n2022.\nYurii Averboukh:\nHSE University, Moscow, Russia\nKrasovskii Institute of Mathematics and Mechanics,\nYekaterinburg, Russia\ne-mail: averboukh@gmail.com\nEkaterina Kolpakova:\nKrasovskii Institute of Mathematics and Mechanics,\nYekaterinburg, Russia\ne-mail: eakolpakova@gmail.com\n34\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20871v1.pdf",
    "total_pages": 34,
    "title": "Time-optimal problem in the space of probabilities measures",
    "authors": [
      "Yurii Averboukh",
      "Ekaterina Kolpakova"
    ],
    "abstract": "This paper focuses on the value function in the time-optimal problem for a\ncontinuity equation in the space of probability measures. We derive the dynamic\nprogramming principle for this problem. In particular, we prove that the\nKruzhkov transform of the value function coincides with a unique discontinuous\nviscosity solution to the corresponding Dirichlet problem for the\nHamilton-Jacobi equation. Finally, we establish the $\\Gamma$-convergence of the\nvalue function in a perturbed problem to the value function in the unperturbed\nproblem.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}