{
  "id": "arxiv_2502.21194v1",
  "text": "Class prior estimation for positive-unlabeled\nlearning when label shift occurs\nJan Mielniczuk1,2, Wojciech Rejchel3, and Paweł Teisseyre1,2 (\u0000)\n1 Warsaw University of Technology, Warsaw, Poland\n2 Polish Academy of Sciences, Warsaw, Poland\n3 Nicolaus Copernicus University, Toruń, Poland\nPawel.Teisseyre@ipipan.waw.pl\nAbstract. We study estimation of class prior for unlabeled target sam-\nples which is possibly different from that of source population. It is as-\nsumed that for the source data only samples from positive class and from\nthe whole population are available (PU learning scenario). We introduce\na novel direct estimator of class prior which avoids estimation of poste-\nrior probabilities and has a simple geometric interpretation. It is based\non a distribution matching technique together with kernel embedding\nand is obtained as an explicit solution to an optimisation task. We es-\ntablish its asymptotic consistency as well as a non-asymptotic bound on\nits deviation from the unknown prior, which is calculable in practice. We\nstudy finite sample behaviour for synthetic and real data and show that\nthe proposal, together with a suitably modified version for large values\nof source prior, works on par or better than its competitors.\nKeywords: positive-unlabeled learning · label shift · Reproducing Ker-\nnel Hilbert Space (RKHS) · class prior estimation· Maximum Mean Dis-\ncrepancy (MMD)\n1\nIntroduction\nPositive-unlabeled learning [6,2] is an active research topic that has attracted\na lot of interest in the machine learning community in recent years. The goal\nis to build a binary classification model based on training data that contains\nonly positive cases and unlabeled cases, which can be either positive or nega-\ntive. For example, patients with a confirmed diagnosis of a disease are treated\nas positive cases, while patients with no diagnosis are considered as unlabeled\nobservations, since this group may include both sick and healthy individuals.\nPU data occurs frequently in many fields, such as bioinformatics [17], image and\ntext classification [18,9], and survey research [28].\nMost state-of-the-art learning algorithms for PU data, such as uPU [24],\nnnPU [16] and others [4,33,20] require knowledge of the class prior, i.e. the\nprobability of the positive class. Knowledge of the class prior can be used to\neither modify a risk function or to change a threshold value of a classification\nrule learned on source data. Since the class prior is usually unknown, there is an\narXiv:2502.21194v1  [stat.ML]  28 Feb 2025\n\n\n2\nJ. Mielniczuk et al.\nimportant line of research aimed at developing methods for estimating the class\nprior from PU data, see [15,25,1] for representative examples. The task is non-\ntrivial, because in the case of PU data we do not have direct access to negative\nobservations, but only to an unlabeled sample which is a mixture of positive and\nnegative observations. Moreover, most of the existing methods assume that the\nclass prior remains constant for both the source (training) data and the target\n(test) data on which we want to perform classification or make an inference.\nThis assumption is not fulfilled in many situations. For example, imagine that\nthe source data is collected in the period before the outbreak of an epidemic,\nwhere the percentage of people with the considered disease is small, while the\ntarget data is collected during an epidemic, where the prevalence of the disease\nmay be much higher [26]. The source and target data may also be collected in\ndifferent climatic zones, which naturally differ in the prevalence of diseases. In\nsuch situations, it is necessary to estimate the class prior probability not only\nfor the source PU data but also for new unlabeled target data, for which we\nonly observe features whereas the labels remain unknown. Figure 1 illustrates\nthe discussed situation. In the example, the source class prior π = 0.7 whereas\nthe target class prior is π′ = 0.3.\nThe problem of inference under class prior shift, known in the literature as\nlabel shift, has been extensively studied and several methods have been devel-\noped to estimate the probability of the class prior for the target data as well as\nto modify the classifier to take the shift into account [27,19,10,14,30]. We note\nthat in business applications evaluation of proportion of each label on unlabeled\ndata set (known as quantification task) is frequently more needed than classifi-\ncation itself. This is particularly important for applications tracking trends (see\n[7] and [12] for the review). However, methods developed for label-shift problem\nrequire fully labeled source data and thus cannot be directly applied to the prob-\nlem considered here. Thus the important endeavour is to estimate target class\nprior directly, possibly avoiding label prediction for target samples. The main\ncontribution of this work is the proposal of a new estimator of target class prior\nπ′ having this property, called TCPU. Our approach is based on employing\nthe distributions matching technique and the kernel method. In the theoretical\nanalysis, we prove the consistency of the proposed estimator, i.e. convergence\nin probability to the true target class prior. Even more importantly, we provide\na non-asymptotic bound on the approximation error. Additionally, in the paper\nwe show how to adapt the popular KM estimator [25], being a state-of-the-art\nclass prior estimator for PU data, to the case of class prior shift. Our experi-\nments, conducted on artificial and real data for different class prior shift schemes,\nconfirm the effectiveness of the proposed method.\n2\nLabel shift for positive unlabeled learning\nWe first introduce relevant notation. Let X ∈X be a random variable corre-\nsponding to a feature vector, Y ∈{−1, 1} be a true class indicator and PXY\ntheir joint distribution (called a source distribution). We consider a problem of\n\n\nClass prior estimation for positive-unlabeled learning\n3\nSOURCE PU data\n= 0.7\nnegative (unlabeled)\npositive (unlabeled)\npositive (labeled)\n0.00\n0.01\n0.02\n0.03\n0.04\n0.06\n0.07\n0.08\n0.09\n0.10\nTARGET UNLABELED data\n′ = 0.3\nnegative (unlabeled)\npositive (unlabeled)\n0.00\n0.01\n0.02\n0.03\n0.04\n0.06\n0.07\n0.08\n0.09\n0.10\nFig. 1: Label shift visualization for PU data. Source (training) data contains\npositive (blue) and unlabeled (grey) observations. Target (test) data contains\nonly unlabeled observations. Class priors differ between target (π = 0.7) and\nsource data (π′ = 0.3). The goal is to estimate target class prior π′ using source\nPU data and target unlabeled data.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n = 0.2  \n′ = 0.5\n( )\navg. \n′\ntrue \n′\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n = 0.8  \n′ = 0.5\n( )\navg. \n′\ntrue \n′\nFig. 2: Visualization of the objective function behavior for π=0.2, 0.8 and\nπ′ = 0.5. The TCPU estimator is defined as bπ′ = arg minγ ˆL(γ). The grey area\nindicates the range of estimator’s values for 20 runs. For larger π, the objective\nfunction is flat and has a less pronounced minimum.\nmodeling Positive Unlabeled data in case-control setting, which means that only\nsamples coming from the positive class and samples coming from the overall\npopulation X are available. More formally, let PX be the distribution of X and\nP+ = PX|Y =1, P−= PX|Y =−1 the distributions of samples from the positive\nclass and the negative class, respectively. We denote by X1, . . . , Xn independent\nsamples generated according to PX and X+\n1 , . . . , X+\nm generated according to P+.\nMoreover, we consider the second vector (X′, Y ′) such that its distribution\nP ′\nX′Y ′ (called a target distribution) is a label shifted distribution of (X, Y ), which\nmeans that the marginal distribution of Y ′ is different from that of Y , i.e.\nπ′ = P(Y ′ = 1) ̸= P(Y = 1) = π,\nhowever, the covariate distributions in both the positive and the negative class\nremain the same:\nP ′\nX′|Y ′=i = PX|Y =i\nfor\ni = ±1.\n\n\n4\nJ. Mielniczuk et al.\nThus, we have that a distribution of X′ satisfies P ′\nX′ = π′P+ + (1 −π′)P−and\nis different from PX = πP+ + (1 −π)P−.\nDenote by X′\n1, . . . , X′\nn′ independent samples generated from P ′\nX′. In the pa-\nper we consider the problem of estimation of label-shifted probability π′. Note\nthat that the problem is nontrivial as the class indicators corresponding to shifted\nsamples are not available. However, we have at our disposal data from the posi-\ntive class and unlabeled X observations corresponding to a different prior π.\nWe note that if π is known the distribution P−is uniquely determined and the\nproblem is well defined in general. When π is unknown, the specific assumptions\nare needed under which it is identifiable, e.g. an assumption that P−is not a\nconvex combination of P+ and other probability measure (see e.g. [25]). We stress\nthat identifiability is necessary to investigate theoretical properties of considered\nestimate.\nFinally, it is worth mentioning that the estimation of π′ is crucial to define\na classification rule for the target set which involves a modified threshold base\non π′, see e.g. [19].\n3\nClass prior estimation for target data\n3.1\nTCPU: a novel kernel-based estimator of class prior\nIn this section we introduce a novel method of estimating π′, which will be called\nTCPU (Target Class prior estimator for Positive-Unlabled data under label\nshift).\nLet K(·, ·) be a kernel function (i.e. symmetric, continuous and semi-positive\nfunction defined on X × X) and let H be a Reproducing Kernel Hilbert Space\n(RHKS) induced by K(·, ·) (see e.g. [8]). We denote an associated kernel trans-\nform by ϕ(x) = K(x, ·) ∈H. From the reproducing property < f, ϕ(x) >H= f(x)\nfor each x ∈X, f ∈H and a scalar product in H is defined by a kernel as\n< ϕ(x1), ϕ(x2) >H= K(x1, x2) for each x1, x2 ∈X and extending it for general\nelements of H. Next, let Φ(PX) = Eϕ(X) =\nR\nX ϕ(s) PX(ds) ∈H be a mean\nfunctional of PX with a norm ||Φ(PX)||2\nH = EK(X1, X2), where X1 and X2 are\ntwo independent random vectors following a distribution PX. The mean func-\ntionals Φ(P+) and Φ(P ′\nX′) are defined analogously. Recall that when kernel is\nuniversal we have that Φ(P) = Φ(Q) is equivalent to the fact that distributions\nP and Q coincide [8].\nLet ˆPX, ˆP+ and ˆP ′X′ be empirical distributions corresponding to observable\nsamples. In the following we will omit indices X and X′ in PX and P ′\nX′, re-\nspectively, and the same convention is applied to their empirical counterparts.\nWe note that due to the fact that ˆP is a discrete distribution with mass n−1\nat each observation Xi we have that Φ( ˆP) = n−1 Pn\ni=1 ϕ(Xi) and analogous\nrepresentations hold for Φ( ˆP+) and Φ( ˆP ′).\nIn the above setup we have that\n(1 −π′)(P −πP+) = (1 −π′)(1 −π)P−= (1 −π)(P ′ −π′P+).\n\n\nClass prior estimation for positive-unlabeled learning\n5\nTherefore, in order to determine π′, it is natural to substitute γ for π′ and\nminimize the following objective function\nL(γ) = ||(1 −γ)[Φ(P) −πΦ(P+)] −(1 −π)[Φ(P ′) −γΦ(P+)]||2\nH.\n(1)\nLemma 1. Suppose that a kernel K is universal, P+ ̸= P−and π < 1. Then π′\nis unique minimizer of L(γ).\nProof. Denote by Crit(γ) the function given by\nCrit(γ) = |π′ −γ| × (1 −π)||Φ(P−) −Φ(P+)||H.\n(2)\nThen by noting that\nΦ(P)−πΦ(P+) = (1−π)Φ(P−),\nΦ(P ′)−γΦ(P+) = (π′−γ)Φ(P+)+(1−π′)Φ(P−)\nwe have that L(γ) = [Crit(γ)]2 and the minimiser of Crit(γ) is obviously π′.\nIn the proposed method, we consider the empirical version of (1) given as\nbL(γ) = ||(1 −γ)[Φ( ˆP) −πΦ( ˆP+)] −(1 −π)[Φ( ˆP ′) −γΦ( ˆP+)]||2\nH\n(3)\nand the estimator of π′ is defined as its minimizer\nbπ′ = argminγ bL(γ).\n(4)\nThe estimator defined above will be called TCPU further on. For theoretical\nresults it will be assumed that π is known. This assumption is plausible when\nthe large data base corresponding to source distribution is available. In the ex-\nperiments we estimate π using well-known KM2 method [25] and then plug-in\nit into (3). Note that the slope of Crit(γ) is proportional to 1 −π and close to\n0 for π ≈1. It will make estimation of π′ more difficult for large π.\nIn the following lemma we show that the proposed estimator can be explicitly\ncalculated using a simple algebraic formula.\nLemma 2. Let bπ′ be defined by (4). Then we have\nbπ′ = < Φ( ˆP) −Φ( ˆP+), ∆>H\n||Φ( ˆP) −Φ( ˆP+)||2\nH\n= 1−(1 −π) < Φ( ˆP) −Φ( ˆP+), Φ( ˆP ′) −Φ( ˆP+) >H\n||Φ( ˆP) −Φ( ˆP+)||2\nH\n,\n(5)\nwhere ∆= Φ( ˆP) −πΦ( ˆP+) −(1 −π)Φ( ˆP ′).\nRemark 1. In view of the first equality in (5), bπ′ has a simple geometric inter-\npretation: it is a coefficient of projection of ∆on Φ( ˆP) −Φ( ˆP+) in H.\n\n\n6\nJ. Mielniczuk et al.\nProof. Simple calculations show that the expression under the norm in (3) equals\n−γ[Φ( ˆP) −Φ( ˆP+)] + Φ( ˆP) −πΦ( ˆP+) −(1 −π)Φ( ˆP ′) = −γ[Φ( ˆP) −Φ( ˆP+)] + ∆.\nThus, the squared norm equals\nγ2||Φ( ˆP) −Φ( ˆP+))||2\nH + 2γ < Φ( ˆP+) −Φ( ˆP), ∆>H +||∆||2\nH\nand the first form of bπ′ follows by calculating a minimizer of the above function.\nThe second formula follows by simple algebraic manipulations.\nThe kernel approach is a core of Maximum Mean Discrepancy (MMD) method\nwhich matches the distributions based on the features in RHKS induced by\nkernel K. It is frequently used in machine learning and statistics to compare\na data distribution with a specific distribution, in a two sample problem and\ncovariate shift detection (see [13]). It has been also applied for the label shift\nproblem in classical framework. Namely, for the case when distribution PXY is\nobservable the approach relies on the equality (cf [31])\nπ′ = argminλ∈[0,1]\n||Φ(PX′) −[λΦ(P+) + (1 −λ)Φ(P−)]||2\nH\n(6)\n(see also [14] and [5]). In the considered scenario, a direct application of (6) to\nestimate π′ is clearly infeasible, as observations from the negative class are not\navailable. The estimator (4) can be considered as a modification for the MMD\napproach applied for label shift in PU setting.\nIn numerical experiments we will also consider a modified TCPU estimator,\ncalled TCPU+, which switches from TCPU to KM2-LS defined in Section 4.2\nbelow, when denominator in (5) falls below a predefined threshold. Note that\n||Φ(P) −Φ(P+)||H is proportional to (1 −π) and it is expected, in view of\nthe discussion above, that performance of TCPU will deteriorate for large π.\nIn Figure 2 we visualise the essence of the problem. For large π the objective\nfunction ˆL(γ) is flat around its minimum and thus bπ′ is less accurate than for\nsmaller π.\n3.2\nAsymptotic consistency and non-asymptotic error bounds for\nthe proposed estimator\nWe let N = min(n, m, n′). In the next result we establish asymptotic and\nnonasymptotic error bounds for bπ′. We also state conditions, which guarantee\nthat ||Φ( ˆP) −Φ( ˆP+)||H > 0, which is implicitly assumed in (5).\nTheorem 1. Suppose that a kernel K is universal, P+ ̸= P−and π < 1.\n(i) Moreover, assume EK(X, X) < ∞for X ∼P and analogous conditions\nhold for X+ ∼PX|Y =1 and X′ ∼P ′. Then for N →∞we have ˆπ′ →π′ in\nprobability.\n(ii) Assume that M = supx K(x, x) < ∞. Moreover, fix α ∈(0, 1) and δ ≤\nexp(−(\n√\n2 + 1)2/2) and let\nN ≥\n16M log(1/δ)\n(1 −α)2(1 −π)2||Φ(P−) −Φ(P+)||2\nH\n.\n(7)\n\n\nClass prior estimation for positive-unlabeled learning\n7\nThen we have\nP\n\n|bπ′ −π′| ≤\n4\nq\nM\nN log(1/δ)\nα(1 −π)||Φ(P−) −Φ(P+)||H\n\n≥1 −3δ.\n(8)\nWe note that consistency of bπ′ is proved in Theorem 1(i) under weak condi-\ntions. Indeed, dropping the conditions P+ ̸= P−and π ̸= 1 makes the problem\nill-posed. Finally, the restrictions imposed on a kernel are satisfied, for instance,\nfor a gaussian kernel.\nThe claim of Theorem 1 (ii) is stronger (it is an nonasymptotic result im-\nplying asymptotic consistency), so it needs more restrictive assumptions as well.\nHowever, these conditions are reasonable as in (i). For instance, a gaussian ker-\nnel satisfies the assumption in (ii) with M = 1. The dependence of an error\nbound on N, δ, π, ||Φ(P−) −Φ(P+)||H is stated explicitly in (8). Clearly, an error\nof bπ′ decreases as N increases. Also, as it should be expected, the bound for\n|bπ′ −π′| increases when P+ gets closer to P−or π increases. Finally, note that\nthe condition on N in (7) becomes more restrictive for increasing π.\nTheorem 2. Assume that M = supx K(x, x) < ∞. Then for any δ ≤exp(−(\n√\n2+\n1)2/2) we have that\nP\n\n|bπ′ −π′| ≤\n4\nq\nM\nN log(1/δ)\n||Φ( ˆP) −Φ( ˆP+)||H\n\n≥1 −3δ.\n(9)\nWe stress the differences between Theorems 2 and 1. Probability inequality\n(9) does not require assumption (7) and it yields a bound on an estimation error\nwhich depends on ||Φ( ˆP)−Φ( ˆP+)||H. Notice that this bound can be calculated in\npractice. On the other hand, the error bound in (8) is nonrandom and explicitly\nestablishes the dependence on π and a distance between P−and P+.\nProof (of Theorem 1). The proof of (i) follows from Chebyshev’s inequality ap-\nplied for Φ( ˆP) = n−1 Pn\ni=1 Φ(Xi) as well as Φ( ˆP+) and Φ( ˆP ′): fix ε > 0, then\nP(||Φ( ˆP) −Φ(P)||H > ε) ≤E||Φ( ˆP) −Φ(P)||2\nH\nε2\n.\n(10)\nNow we focus on bounding the numerator in (10). First, we obtain\nE||Φ( ˆP) −Φ(P)||2 = E||Φ( ˆP)||2\nH −2E < Φ( ˆP), Φ(P) >H +||Φ(P)||2\nH.\n(11)\n\n\n8\nJ. Mielniczuk et al.\nNext, we consider the two first terms on the right-hand side of (11). For the first\none, we have:\nE\n\f\f\f\f\f\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nϕ(Xi)\n\f\f\f\f\f\n\f\f\f\f\f\n2\nH\n= 1\nn2\nn\nX\ni=1\nn\nX\nj=1\n< ϕ(Xi), ϕ(Xj) >H= 1\nn2\nn\nX\ni=1\nn\nX\nj=1\nK(Xi, Xj)\n= 1\nn2\nn\nX\ni=1\nEK(Xi, Xi) + 1\nn2\nX\n1≤i̸=j≤n\nEK(Xi, Xj)\n= 1\nnEK(X1, X1) +\n\u0012\n1 −1\nn\n\u0013\n||Φ(P)||2\nH.\nMoreover, we have\nE < Φ( ˆP), Φ(P) >H= 1\nn\nn\nX\ni=1\nE < ϕ(Xi), Φ(P) >H= E < ϕ(X1), Φ(P) >H .\n(12)\nFrom the reproducing property we obtain < ϕ(x1), Φ(P) >H= [Φ(P)](x1) =\nEK(X, x1). Therefore, the right-hand side of (12) equals ||Φ(P)||2\nH. Finally, (11)\nis 1\nn[EK(X, X)−||Φ(P)||2\nH], so the right-hand side of (10) tends to zero as n goes\nto infinity. This fact implies that Φ( ˆP) tends to Φ(P) in probability. Obviously,\nthe analogous properties hold for Φ( ˆP+) and Φ( ˆP ′). In particular, we have that\n||Φ( ˆP) −Φ( ˆP+)||H is positive with probability tending to one as N →∞. It\nfollows from continuity of a norm and the assumptions\n||Φ( ˆP) −Φ( ˆP+)||H →P ||Φ(P) −Φ(P+)||H = (1 −π)||Φ(P−) −Φ(P+)||H.\nThus, the second equality in (5) and continuity of a scalar product give\nˆπ′ →P\n1 −(1 −π) < Φ(P) −Φ(P+), Φ(P ′) −Φ(P+) >H\n||Φ(P) −Φ(P+)||2\nH\n= π′.\nNext, we focus on the proof of (ii). From the first equality in (5) and the\nCauchy–Schwarz inequality we have\n|bπ′ −π′| = < Φ( ˆP) −Φ( ˆP+), ∆−π′[Φ( ˆP) −Φ( ˆP+)] >H\n||Φ( ˆP) −Φ( ˆP+)||2\nH\n(13)\n≤||∆−π′[Φ( ˆP) −Φ( ˆP+)]||H\n||Φ( ˆP) −Φ( ˆP+)||H\n.\nNotice that\n∆−π′[Φ( ˆP) −Φ( ˆP+)] = (1 −π′)Φ( ˆP) −(1 −π)Φ( ˆP ′) + (π′ −π)Φ( ˆP+)\nand\n(1 −π′)Φ(P) −(1 −π)Φ(P ′) + (π′ −π)Φ(P+) = 0,\n\n\nClass prior estimation for positive-unlabeled learning\n9\nwhich imply that\n||∆−π′[Φ( ˆP) −Φ( ˆP+)]||H ≤(1 −π′)||Φ( ˆP) −Φ(P)||H\n+ (1 −π)||Φ( ˆP ′) −Φ(P ′)||H + |π′ −π| × ||Φ( ˆP+) −Φ(P+)||H.\nNow we use Lemma 3 (given below) and consider the event on which all three\ninequalities in this lemma hold. In this case\n||∆−π′[Φ( ˆP) −Φ( ˆP+)]||H ≤4\nr\nM\nN log(1/δ)\n(14)\nand\n||Φ( ˆP)−Φ( ˆP+)||H ≥||Φ(P)−Φ(P+)||H−||Φ( ˆP)−Φ(P)||H−||Φ( ˆP+)−Φ(P+)||H,\nwhich again can be bounded by\n(1 −π)||Φ(P−) −Φ(P+)||H −4\nr\nM\nN log(1/δ).\nFrom the assumption (7) the above expression is not smaller than α(1−π)||Φ(P−)−\nΦ(P+)||H. In particular, we have that ||Φ( ˆP) −Φ( ˆP+)||H is positive with high\nprobability. Since the numerator and the denominator on the right-hand side of\n(13) are bounded, the proof of (ii) is finished.\nProof (of Theorem 2). The proof is even simpler than the proof of Theorem 1\nand involves bounding the numerator on the RHS of (13) done in (14).\nLemma 3. Suppose that M = supx K(x, x) < ∞and δ ≤exp(−(\n√\n2 + 1)2/2)\nis arbitrary. Then with probability at least 1 −3δ the following three inequalities\nsimultaneously hold\n||Φ( ˆP) −Φ(P)||H ≤2\nr\nM\nn log(1/δ) ,\n||Φ( ˆP ′) −Φ(P ′)||H ≤2\nr\nM\nn′ log(1/δ) ,\n||Φ( ˆP+) −Φ(P+)||H ≤2\nr\nM\nm log(1/δ) .\nThe similar concentration inequalities are proven, for instance, in [29, Proposi-\ntion A.1 and Remark A.2]. For completeness we provide the proof in supplemen-\ntary material.\n4\nRelated work\n4.1\nMethod DRPU\nThe most related method is DRPU [22], which in the label-shift setting con-\nsidered here, constructs empirical Bayes classifier for samples from label shifted\n\n\n10\nJ. Mielniczuk et al.\npopulation. It involves estimator of ratio r of densities of distributions P+ and\nP for training data and of both prior probabilities π and π′. Estimator ˆr of r is a\nminimiser of expected Bregman divergence functional (cf Section 2.5 in [22]) and\nboth prior estimators are based on an observation (cf [3]) that π and π′ can be\nrecovered in PU setting by minimising P(A)/P+(A) (respectively P ′(A)/P+(A))\nover all sets A for which P+(A) > 0. In [22] ratio ˆP ′(A)/ ˆP+(A) is minimised\nover sets being the level sets of ˆr. The estimator based on the above method will\nbe denoted simply as DRPU.\n4.2\nAdapting the KM method to label shift\nThe popular KM [25] method of estimating the prior in PU setting can be\nstraightforwardly modified in order to account for label shift. [25] is based on\nthe observation that distribution of negative class P−can be written as P−=\nλP + (1 −λ)P+, where λ = 1/(1 −π) and thus, analogously to (6), estimator of\nλ can be constructed by projecting γΦ(P) + (1 −γ)Φ(P+) on a convex C hull of\nϕ(X1), . . . , ϕ(Xn), ϕ(X+\n1 ), . . . , ϕ(X+\nm) and defining ˆλ as γ yielding the smallest\ndistance. This leads to two estimators of π, KM1 and KM2 investigated in [25].\nAs unavailable positive samples for test population have the same distribution\nas positive observations from training distribution, we can apply approach from\n[25] to samples X′\n1, . . . , X′\nn′, X+\n1 , . . . , X+\nm and obtain KM2 estimator of π′ also\ninvestigated below. The adaptation will be called KM2-LS in the following.\n5\nExperiments\n5.1\nMethods\nWe empirically evaluate the effectiveness of TCPU and TCPU+ to recover the\ntrue target class prior π′. As baselines we used DRPU [22] and KM2-LS methods\ndescribed in Section 4. In the case of DRPU, we used the implementation made\npublicly available by the authors. In the case of KM2-LS, we used the code\nof the standard KM2 method [25] and adopted it to our setting as described in\nSection 4. Note that TCPU and TCPU+ require knowledge of π. Since typically,\nπ remains unknown, we estimate it using KM2 estimator [25] for both methods.\nImportantly, the KM2-LS method does not require the π estimation. The DRPU\nmethod is the only one that requires learning a parametric model. As in the\noriginal work, we used MLP to this end. Due to space constraints, technical\ndetails about the model used in DRPU and the selection of hyperparameters are\ndiscussed in the supplement (Section 5).\nFor TCPU, we used Gaussian kernel K(x, y) = exp(−τ||x −y||2) with the\ndefault value of parameter τ = 1/p, where p is the number of features. We also\nconsider TCPU+ discussed at the end of Section 3.1, which switches from TCPU\nto KM2-LS if the ||Φ( ˆP) −Φ( ˆP+)||H < s, where s is a threshold. The value of\ns = 0.02 was empirically chosen based on examination of performance of TCPU.\n\n\nClass prior estimation for positive-unlabeled learning\n11\n5.2\nDatasets\nThe experiments were conducted on 10 datasets, including one synthetic dataset,\n6 tabular datasets from the UCI repository (Diabetes, Spambase, Segment,\nWaveform, Vehicle, Yeast), and 3 image datasets: CIFAR-10, MNIST, and Fash-\nionMNIST [23]. For the synthetic dataset, negative observations are generated\nfrom a 10-dimensional normal distribution N(0, I), and positive observations\nare generated from N(a, I), where a = (1, . . . , 1). The characteristics of the UCI\nand image datasets are provided in the supplement. Tabular datasets with mul-\ntiple classes were transformed into binary classification datasets, where the most\ncommon class is treated as the positive class, and the remaining classes are com-\nbined into the negative class. For image datasets, the binary class variable is\ndefined according to the specific dataset, following methods used in other PU\nlearning papers [16,11,22]. For MNIST, even digits form the positive class, and\nodd digits form the negative class. In CIFAR-10, vehicles form the positive class,\nand animals form the negative class. For FashionMNIST, clothing items worn\non the upper body are marked as positive cases, and the remaining items are\nassigned to the negative class. For image data, we use a pre-trained deep neural\nnetwork, ResNet18, to extract the feature vector. For each image, the feature\nvector, with a dimension of 512, is the output of the average pooling layer. From\nthe extracted 512-dimensional feature vector, we select the 30 most correlated\nfeatures with the class variable to reduce the dimensionality of the problem.\n5.3\nExperimental settings\nFirst, each dataset is split into a source and a target dataset. For image data,\nwe use the splits defined in the PyTorch library [23]. For synthetic datasets,\nwe generate observations for fixed values of π and π′. For real datasets, we\nsimulate a label shift scenario using the downsampling technique. Specifically,\nwe randomly remove observations from one of the classes in both the source\nand target datasets to control the class priors π and π′, respectively. Finally,\nbased on the source data, we artificially create a PU dataset by selecting some\npositive observations for the labeled subset, while the unlabeled subset consists\nof a mixture of positive and negative observations. We follow the procedure\ndescribed in [21] to control the size of the source data and the labeling frequency\nc, which represents the percentage of labeled observations among all positive\nobservations. In the experiments, we set c = 0.5. The entire target dataset is\ntreated as unlabeled. The considered methods take as input the source PU data\nand unlabeled target data. For each method, we calculate the absolute estimation\nerror |π′ −bπ′|, where bπ′ is the estimator returned by the method. We perform 20\nrepetitions of the above procedure and analyze the distributions of the errors.\n5.4\nDiscussion\nFigures 3, 4, 5, and 6 show the distributions of estimators and estimation errors\nfor the analyzed datasets. Additional results for synthetic and image data are\n\n\n12\nJ. Mielniczuk et al.\n500\n1000\n1500\n2000\nSize of source and target data\n0.0\n0.2\n0.4\n0.6\nEstimation error\nSynthetic = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n500\n1000\n1500\n2000\nSize of source and target data\n0.0\n0.2\n0.4\n0.6\nEstimation error\nSynthetic = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n500\n1000\n1500\n2000\nSize of source and target data\n0.0\n0.2\n0.4\n0.6\nEstimation error\nSynthetic = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n500\n1000\n1500\n2000\nSize of source and target data\n0.0\n0.2\n0.4\n0.6\nEstimation error\nSynthetic = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\nFig. 3: Estimation errors wrt size of data.\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nEstimation error\nSynthetic = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nEstimation error\nSynthetic = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nEstimation error\nSynthetic = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nEstimation error\nSynthetic = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEstimator\nSynthetic = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEstimator\nSynthetic = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEstimator\nSynthetic = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEstimator\nSynthetic = 0.8  \n′ = 0.2\nFig. 4: Rows 1, 2: distribution of estimation errors and distribution of estimators\n(blue line indicates the true π′). Size of the source data and the target data is\n1000.\nalso provided in the supplement (Sections 3-5). Since our primary interest lies in\nanalyzing estimation errors for small or moderate data samples, we present the\nboxplots for the case where the source and target datasets consist of randomly\nchosen samples of 1000 observations. When the total number of observations in\nthe original dataset is less than 2000, we split the data into source and target\ndatasets in equal proportions. Experiments indicate that the values of π and π′\nsignificantly influence the quality of the π′ estimation, with the impact differing\nacross methods. For the TCPU method, we observe small estimation errors for\nsmall or moderate values of π = 0.2, 0.4, 0.6, regardless of the value of π′. How-\never, for π = 0.8, the errors for TCPU increase. This is likely due to the issue of\n\n\nClass prior estimation for positive-unlabeled learning\n13\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCIFAR10 = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCIFAR10 = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCIFAR10 = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCIFAR10 = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMNIST = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMNIST = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMNIST = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMNIST = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFashion = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFashion = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFashion = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFashion = 0.8  \n′ = 0.2\nFig. 5: Distribution of estimators for image datasets. Blue line indicates true π′.\nthe lack of a well-pronounced minimum for the objective function, as discussed\nat the end of Section 3.1 and illustrated in Figure 2. In this case, TCPU under-\nestimates the true π′. This problem is avoided in the TCPU+ method, which\nis highly effective for all values of π. In the problematic situation occurring\nfor large π, TCPU+ switches from TCPU to KM2-LS. However, the KM2-LS\nmethod itself performs significantly worse than competing methods for small π,\ndue to the underestimation of the true value of π′. The DRPU method produces\nlarge estimation errors for π = 0.8, which is related to the overestimation of π′.\nThe effect of dataset size on the quality of the estimation is shown in Figure 4\n(row 1). Additional figures in the supplement illustrate the individual effects of\nvariable source and target dataset sizes. For π = 0.2, the estimation error for\nthe DRPU method diminishes much more slowly than for the other methods. In\nthis case, DRPU yields large errors for small sample sizes, which decrease as the\nsample size increases. On the other hand, for π = 0.8, large errors are observed\nfor DRPU, regardless of dataset size. TCPU+ can be recommended as a method\nthat performs consistently well across all considered combinations of π and π′\nvalues.\nIt is worth noting that DRPU is the only method considered that requires\nmodel training, which leads to longer computation times (see Section 7 in the\nsupplement). For example, for image datasets, TCPU is about 4–6 times faster\nthan DRPU for π = 0.2 and π = 0.8, respectively. The computational times for\nTCPU and TCPU+ are the same for π = 0.2, whereas for π = 0.8, TCPU+\nruns slower than TCPU due to the additional time required to determine the\nthreshold for the switch between TCPU and KM2-LS, as well as the execution\ntime of the KM2-LS method itself.\n\n\n14\nJ. Mielniczuk et al.\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDiabetes = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDiabetes = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDiabetes = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDiabetes = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Spambase = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Spambase = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Spambase = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Spambase = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSegment = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Segment = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Segment = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSegment = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Waveform = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Waveform = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Waveform = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Waveform = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVehicle = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVehicle = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVehicle = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVehicle = 0.8  \n′ = 0.2\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nYeast = 0.2  \n′ = 0.8\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nYeast = 0.4  \n′ = 0.6\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nYeast = 0.6  \n′ = 0.4\nTCPU+\nTCPU\nDRPU\nKM2-LS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nYeast = 0.8  \n′ = 0.2\nFig. 6: Distribution of estimators for benchmark datasets.\n6\nConclusions and future work\nIn this paper, we introduced a novel estimator, TCPU, for estimating the target\nclass prior π′ under label shift in the context of positive-unlabeled (PU) learn-\ning. Our approach, which leverages distribution matching and kernel embedding,\nprovides an explicitly expressed estimate of the target class prior without esti-\nmating posterior probabilities via classifier training. We proved that the TCPU\nestimator is both asymptotically consistent and established a calculable non-\n\n\nClass prior estimation for positive-unlabeled learning\n15\nasymptotic error bound. Experimental results on synthetic and real datasets\nshow that TCPU outperforms or performs on par with existing methods, partic-\nularly in cases of moderate source class prior π values. In situations where larger\nvalues of π cause TCPU to underestimate the target class prior, the TCPU+\nvariant effectively mitigates this issue by switching to the KM2-LS method when\nnecessary. A natural direction for future research is to further investigate the ef-\nfect of π and π′ estimation on classification accuracy and to develop methods\nchecking whether label-shift occurs. Also, generalisation of the proposed method\nto not necessarily binary nominal response Y by considering extension of PU\nscenario to noisy data model ([32]) is of interest.\n\n\n16\nJ. Mielniczuk et al.\nReferences\n1. Bekker, J., Davis, J.: Estimating the class prior in positive and unlabeled data\nthrough decision tree induction. In: Proceedings of the 32th AAAI Conference on\nArtificial Intelligence. pp. 1–8 (2018)\n2. Bekker, J., Davis, J.: Learning from positive and unlabeled data: a survey. Machine\nLearning 109, 719–760 (2020)\n3. Blanchard, G., Lee, G., Scott, C.: Semi-supervised novelty detection. Journal of\nMachine Learning Research 11, 2973–3009 (2010)\n4. Chen, X., Chen, W., Chen, T., Yuan, Y., Gong, C., Chen, K., Wang, Z.: Self-PU:\nSelf boosted and calibrated positive-unlabeled training. In: Proceedings of the 37th\nInternational Conference on Machine Learning. ICML’20 (2020)\n5. Dussap, B., Blanchard, G., Chérif-Abdellatif, B.E.: Label shift quantification with\nrobustness guarantees via distribution feature matching. In: Proceedings of the\nEuropean Conferencce on Machine Learning (2023)\n6. Elkan, C., Noto, K.: Learning classifiers from only positive and unlabeled data. In:\nProceedings of the 14th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining. pp. 213–220. KDD ’08 (2008)\n7. Forman, G.: Quantifying counts and costs via classification. Data Mining and\nKnowledge Discovery 17, 164–206 (2008)\n8. Fukumizu, K., Gretton, A., Sun, X., Schölkopf, B.: Kernel measures of conditional\ndependence. In: Advances in Neural Information Processing Systems. vol. 20 (2007)\n9. Fung, G.P.C., Yu, J.X., Lu, H., Yu, P.S.: Text classification without negative exam-\nples revisit. IEEE Transactions on Knowledge and Data Engineering 18(1), 6–20\n(2006)\n10. Garg, S., Wu, Y., Balakrishnan, S., Lipton, Z.C.: A unified view of label shift\nestimation. In: Proceedings of the 34th International Conference on Neural Infor-\nmation Processing Systems. pp. 1–11. NIPS’ 20 (2020)\n11. Gong, C., Wang, Q., Liu, T., Han, B., You, J., Yang, J., Tao, D.: Instance-\ndependent positive and unlabeled learning with labeling bias estimation. IEEE\nTrans Pattern Anal Mach Intell pp. 1–16 (2021)\n12. González, P., Castaño, A., Chawla, N., Coz, J.: A review on quantification learning.\nACM Comput. Surv. 50(5) (2017)\n13. Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B., Smola, A.: A kernel two-\nsample test. Journal of Machine Learning Research 13, 723–773 (2012)\n14. Iyer, A., Nath, S., Sarawagi, S.: Maximum mean discrepancy for class ratio es-\ntimation: convergence bounds and kernel selection. In: Proceedings of the 31th\nInternational Conferencce on Machine Learning. IMLR W & CP vol. 32 (2014)\n15. Jain, S., White, M., Radivojac, P.: Estimating the class prior and posterior from\nnoisy positives and unlabeled data. In: Proceedings of the 30th International Con-\nference on Neural Information Processing Systems. p. 2693–2701 (2016)\n16. Kiryo, R., Niu, G., du Plessis, M.C., Sugiyama, M.: Positive-unlabeled learning\nwith non-negative risk estimator. In: Proceedings of the International Conference\non Neural Information Processing Systems. pp. 1674–1684. NIPS’17 (2017)\n17. Li, F., Dong, S., Leier, A., Han, M., Guo, X., Xu, J., Wang, X., Pan, S., Jia, C.,\nZhang, Y., Webb, G., Coin, L.J.M., Li, C., Song, J.: Positive-unlabeled learning\nin bioinformatics and computational biology: a brief review. Briefings in Bioinfor-\nmatics 23(1) (2021)\n18. Li, X., Liu, B.: Learning to classify texts using positive and unlabeled data. In:\nProceedings of the 18th International Joint Conference on Artificial Intelligence.\np. 587–592. IJCAI’03 (2003)\n\n\nClass prior estimation for positive-unlabeled learning\n17\n19. Lipton, Z.C., Wang, Y., Smola, A.J.: Detecting and correcting for label shift with\nblack box predictors. In: Proceedings of the 35th International Conference on Ma-\nchine Learning. pp. 3128–3136. ICML’ 18 (2018)\n20. Luo, C., Zhao, P., Chen, C., Qiao, B., Du, C., Zhang, H., Wu, W., Cai, S., He, B.,\nRajmohan, S., Lin, Q.: Pulns: Positive-unlabeled learning with effective negative\nsample selector. In: Proceedings of the AAAI Conference on Artificial Intelligence.\nAAAI’21, vol. 35, pp. 8784–8792 (2021)\n21. Mielniczuk, J., Wawrzeńczyk, A.: Single-sample versus case-control sampling\nscheme for Positive Unlabeled data: the story of two scenarios. Fundamenta Infor-\nmaticae 191, 1–17 (2024)\n22. Nakajima, S., Siguyama, M.: Positive-unlabeled classification under class-prior\nshift: a prior-invariant approach based on density ratio estimation. Machine Learn-\ning 112, 889–919 (2023)\n23. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,\nLin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,\nRaison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:\nPytorch: An imperative style, high-performance deep learning library. In: Advances\nin Neural Information Processing Systems. pp. 8024–8035. NIPS’19 (2019)\n24. du Plessis, M.C., Niu, G., Sugiyama, M.: Analysis of learning from positive and\nunlabeled data. In: Proceedings of the International Conference on Neural Infor-\nmation Processing Systems. pp. 703–711. NIPS’14 (2014)\n25. Ramaswamy, H., Scott, C., Tewari, A.: Mixture proportion estimation via kernel\nembeddings of distributions. In: Proceedings of The 33rd International Conference\non Machine Learning. vol. 48, pp. 2052–2060 (2016)\n26. Roland, T., Bock, C., Tschoellitsch, T., Maletzky, A., Hochreiter, S., Meier, J.,\nKlambauer, G.: Domain shifts in machine learning based covid-19 diagnosis from\nblood tests. Journal of Medical Systems 46(5), 1–12 (2022)\n27. Saerens, M., Latinne, P., Decaestecker, C.: Adjusting the outputs of a classifier to\nnew a priori probabilities: a simple procedure. Neural Comput. 14(1), 21–41 (2002)\n28. Sechidis, K., Sperrin, M., Petherick, E.S., Luján, M., Brown, G.: Dealing with\nunder-reported variables: An information theoretic solution. International Journal\nof Approximate Reasoning 85, 159 – 177 (2017)\n29. Tolstikhin, I., Sriperumbudur, B.K., Muandet, K.: Minimax estimation of kernel\nmean embeddings. Journal of Machine Learning Research 18(86), 1–47 (2017)\n30. Vaz, A., Izbicki, R., Stern, R.: Quantification under prior probability shift: the\nratio estimator and its extensions. Journal of Machine Learning Research 20, 1–33\n(2019)\n31. Zhang, K., Schölkopf, B., Muandet, K., Wang, Z.: Domain adaptation under target\nand conditional shift. In: Proceedings of the 30th International Conferencce on\nMachine Learning (2014)\n32. Zhang, Z., Sabuncu, M.: Generalizec cross entropy loss for training neural networks\nwith noisy labels. In: NIPS’18. pp. 8792 – 8802 (2018)\n33. Zhao, Y., Xu, Q., Jiang, Y., Wen, P., Huang, Q.: Dist-pu: Positive-unlabeled learn-\ning from a label distribution perspective. In: Proceedings of the Conference on\nComputer Vision and Pattern Recognition. pp. 14461–14470. CVPR’22 (2022)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21194v1.pdf",
    "total_pages": 17,
    "title": "Class prior estimation for positive-unlabeled learning when label shift occurs",
    "authors": [
      "Jan Mielniczuk",
      "Wojciech Rejchel",
      "Paweł Teisseyre"
    ],
    "abstract": "We study estimation of class prior for unlabeled target samples which is\npossibly different from that of source population. It is assumed that for the\nsource data only samples from positive class and from the whole population are\navailable (PU learning scenario). We introduce a novel direct estimator of\nclass prior which avoids estimation of posterior probabilities and has a simple\ngeometric interpretation. It is based on a distribution matching technique\ntogether with kernel embedding and is obtained as an explicit solution to an\noptimisation task. We establish its asymptotic consistency as well as a\nnon-asymptotic bound on its deviation from the unknown prior, which is\ncalculable in practice. We study finite sample behaviour for synthetic and real\ndata and show that the proposal, together with a suitably modified version for\nlarge values of source prior, works on par or better than its competitors.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}