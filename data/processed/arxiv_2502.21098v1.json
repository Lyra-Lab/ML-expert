{
  "id": "arxiv_2502.21098v1",
  "text": "Re-evaluating Theory of Mind evaluation in large language models\nJennifer Hu*1, Felix Sosa*2, Tomer Ullman1,2\n1Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University\n2Department of Psychology, Harvard University\n‚àóThese authors contributed equally.\nCorresponding authors\nJennifer Hu (jenniferhu@fas.harvard.edu)\nFelix Sosa (fsosa@fas.harvard.edu)\narXiv:2502.21098v1  [cs.AI]  28 Feb 2025\n\n\nAbstract\nThe question of whether large language models (LLMs) possess Theory of Mind (ToM) ‚Äì often defined as the ability\nto reason about others‚Äô mental states ‚Äì has sparked significant scientific and public interest. However, the evidence as\nto whether LLMs possess ToM is mixed, and the recent growth in evaluations has not resulted in a convergence. Here,\nwe take inspiration from cognitive science to re-evaluate the state of ToM evaluation in LLMs. We argue that a major\nreason for the disagreement on whether LLMs have ToM is a lack of clarity on whether models should be expected\nto match human behaviors, or the computations underlying those behaviors. We also highlight ways in which current\nevaluations may be deviating from ‚Äúpure‚Äù measurements of ToM abilities, which also contributes to the confusion. We\nconclude by discussing several directions for future research, including the relationship between ToM and pragmatic\ncommunication, which could advance our understanding of artificial systems as well as human cognition.\n2\n\n\n1\nIntroduction\nHumans are mindreaders. We reckon what others feel, want, and believe based on how they act or what they say.\nThe ability to reason about the mental states of others is generally referred to as Theory of Mind (ToM) (Premack and\nWoodruff, 1978; Apperly, 2011), and is considered a core ability at the heart of a wide variety of social interactions,\nincluding reference (Clark and Marshall, 1981; Clark and Wilkes-Gibbs, 1986), rational communication (Brennan\net al., 2010; Frank and Goodman, 2012; Goodman and Frank, 2016), non-literal (Spotorno et al., 2012; Hsu and\nCheung, 2013; Kline Struhl et al., 2018; Bischetti et al., 2019) or discourse-level (Jacoby and Fedorenko, 2020)\nlanguage understanding, collaboration and cooperation (Sally and Hill, 2006; Krych-Appelbaum et al., 2007; Stacy\net al., 2024), moral judgment (Leslie et al., 2006; Young et al., 2007; Moran et al., 2011; Fu et al., 2014; Sosa et al.,\n2021), and planning in multi-agent contexts (Baker et al., 2009; Baker and Tenenbaum, 2014; Baker et al., 2017; Ho\net al., 2021, 2022). Beyond mediating social interactions, ToM may also support learning and cultural change, such as\nimitation learning (Gopnik and Meltzoff, 1993; Tomasello et al., 1993; Tomasello, 1996; Meltzoff, 2010) and language\nevolution (Woensdregt and Smith, 2017; Smith, 2018). The ability to seemingly read minds is early-developing, cross-\ncultural, and continues to develop in its complexity throughout the early childhood years (Masangkay et al., 1974;\nFlavell et al., 1981; Kiley Hamlin et al., 2013; Tomasello, 2018). Basic aspects of Theory of Mind are likely shared\nwith non-human primates (Hare et al., 2001; Kaminski et al., 2008; Call and Tomasello, 2011; Krupenye et al., 2016),\nwith ongoing arguments about the degree to which even higher-order Theory of Mind may be present in non-human\nprimates (Kano et al., 2019; Royka and Santos, 2022; Heyes, 1998).\nConverging lines of research suggest then that ToM is a core component of social and linguistic interaction in\nhumans. So, it seems reasonable to expect any agent that can socialize with others at a human level to have ToM. This\nissue has recently seen significant interest due to the successes of large language models (LLMs). LLMs have been\nshown to solve complex tasks and cooperate with people with unprecedented sophistication, sparking both scientific\nand public interest in whether these models can reason about the mental states of the people they interact with (Whang,\n2023; Eliot, 2023; Gent, 2023). As LLMs are increasingly deployed in real-world applications, the question of whether\nthese models have the ability to engage in reliable social interactions, and do so in a human-like way, has taken on an\nadditional practical weight (Street, 2024).\nWhile it is uncontroversial to believe that ToM is important for social agents, there are conflicting claims as to\nwhether LLMs possess ToM. Some researchers argue that LLMs achieve human-level performance on signature ToM\nevaluation tasks (Kosinski, 2024; Bubeck et al., 2023; Strachan et al., 2024; Street et al., 2024); others claim that\nmodels are highly sensitive to low-level heuristics or adversarial alterations to task examples that humans would likely\nnot be sensitive to, such as changing the material of an object in a vignette (Ullman, 2023; Shapira et al., 2024). New\nmodels are released at a fast pace, each seemingly more capable than the ones that came before. Alongside these\nmodels, benchmarks for evaluating ToM are also being released at an increasing rate. The growth in interest and\nevaluations has not yet resulted in a convergence in the assessment of LLM‚Äôs ability to perform ToM reasoning. And\nwithout a clear standard as to how to define the abilities we seek to measure, or how to properly evaluate those abilities,\nreleasing more models and benchmarks is not an obvious solution.\n3\n\n\nHere, we take inspiration from cognitive science to re-evaluate Theory of Mind evaluation in large language mod-\nels, and highlight two specific issues. The first issue is with the definition of ToM: we argue that a major reason for\nthe disagreement on whether LLMs have ToM is a lack of clarity on what it means to ‚Äúhave‚Äù ToM. One implicit defi-\nnition of ToM is the ability to match people‚Äôs behavior in ToM evaluations (i.e. matching their input/output mapping;\n‚Äúbehavior-matching‚Äù). Another definition of ToM is formally about the mental computations or algorithm that people\nuse to carry out this mapping in ToM evaluations (i.e. matching how people perform their input/output mapping;\n‚Äúcomputation matching‚Äù). We discuss the implications of both views in Section 2, and suggest paths for going beyond\nthe behavior-matching approach, focusing more on computation-matching. The second issue is the validity of ToM\nevaluations: ToM evaluations may fail to measure the underlying psychological construct that they are designed to\nmeasure (Cronbach and Meehl, 1955; Quesque and Rossetti, 2020). In particular, models could succeed or fail on a\nparticular evaluation for unintended reasons. For example, closed-API models (such as GPT-4) are prone to ‚Äútraining\naway‚Äù, whereby novel test items are continually used to update the model, making them appear more sophisticated\nwhile the underlying computations remain the same. Also, evaluating LLMs using adversarially-constructed exam-\nples introduces complexities that may shift the target of evaluation away from ‚Äúpure‚Äù ToM, and toward more general\nreasoning capacities (Hu and Frank, 2024). By providing clarity on these issues, we hope to inspire more precise\nmeasurements of ToM ability grounded in best principles of cognitive evaluation.\nThe paper is structured as follows. We first sketch out definitions of the term ‚ÄúTheory of Mind‚Äù in Section 2 to\nprovide clarity and scaffolding for the paper. We then give a brief overview of empirical support for and against ToM\nabilities in LLMs in Section 3. Building upon the definitions and empirical evidence for ToM in LLMs, we highlight\ntwo issues with current ToM evaluation paradigms in Section 4. We discuss suggested directions for future work in\nSection 5, including the relationship between ToM and pragmatic communication, and conclude in Section 6.\n2\nDefinitions of Theory of Mind\nA major reason for the disagreement on whether LLMs (or other models) have Theory of Mind is a lack of agreement\non what it means to ‚Äúhave‚Äù a Theory of Mind. This problem plagues cognitive science as well, but is especially in force\nin current research in LLMs. To get across what we see as the primary source of confusion, we need to distinguish\nbetween (1) the empirical fact that people can and do attribute mental states to other entities based on the observed\nbehavior of those entities, and (2) the mental computation(s) that people use to carry out this attribution. Both (1) and\n(2) are targets of research in cognitive science and cognitive development, but they are not the same thing.\nTurning first to (1), it has been empirically established that people connect the actions of others to statements\nabout the mental states that lead to those actions. There‚Äôs hardly any arguing that such a thing exists, and researchers\nhave examined when children start to make such attributions (Gergely et al., 1995; Saxe et al., 2005), how fast people\nmake them (Apperly et al., 2011; Malle and Holbrook, 2012), whether they agree with ground truth (where applicable)\n(Apperly et al., 2011), and so on. Specifically, we can think of this ability as a mapping from the observed actions ùê¥\nof an agent to the mental states ùëÄof that agent that caused those actions, ùê¥‚ÜíùëÄ. If this is our focus, then our LLM\n4\n\n\nüßë\nü§ñ\nHuman\nModel\nM\nM‚Äô\nf‚Äô\nf\nA\n‚ÄúAnne secretly takes \nJohn's ball out of his bag \nand puts it in her pocket.‚Äù\n‚ÄúJohn will look for his \nball in the last place \nhe had it: the bag.‚Äù\n‚ÄúJohn will look for \nthe ball in the bag.‚Äù\nQ2: Same computation?\nQ1: Same output?\nFigure 1: What does it mean for a model to ‚Äúhave‚Äù Theory of Mind? Given an input observation (action ùê¥), there is a\ndistinction between asking whether humans and models arrive at the same output (beliefs or predictions about latent\nmental states ùëÄ), versus asking whether humans and models use the same kinds of computations to map from ùê¥to ùëÄ.\nThe first question (Q1) is concerned with whether ùëÄ== ùëÄ‚Ä≤. The second question (Q2) is concerned with whether\nùëì== ùëì‚Ä≤.\nevaluation can be conceptualized in the following way (Figure 1): given a certain observation ùê¥, will a model‚Äôs output\n(inferred mental state ùëÄ‚Ä≤) match a human‚Äôs output (inferred mental state ùëÄ)?\nIn cognitive science, ‚ÄúTheory of Mind‚Äù is sometimes used as shorthand for (1) ‚Äì i.e., the fact that people do\ncarry out mental state attribution) ‚Äì but also as shorthand for a specific claim about the way that people carry out the\nattribution, corresponding to (2). Classically, Theory of Mind refers to mapping observed actions to mental states\nby positing a theory-like structure of how people‚Äôs actions are driven by their mental states, and then inverting that\ntheory to infer the most likely mental states from observed actions (Dennett, 1989; Gergely et al., 1995). In recent\ndecades, such an inversion has also also been formalized in a rational Bayesian setting (Baker et al., 2009, 2017; Jara-\nEttinger, 2019). This is not the only model proposed for how people attribute mental states to others, however, and\nother proposals exist for connecting observable actions to the attribution of mental states. These alternative proposals\ninclude simulation through one‚Äôs own decision-making process (Saxe, 2012), or directly mapping observable features\nto mental attributes in a more bottom-up way (Scholl and Gao, 2013).\nWhether ToM is about behavior or a specific algorithm determines the evaluations one should use, and what results\nqualify as ‚Äúpositive‚Äù. For example, if computation, or the algorithm used to attribute mental states to agents, is our\nfocus, then we should ask what is the specific mapping ùëìthat connects ùê¥to ùëÄ, such that ùëì(ùê¥) = ùëÄ. Returning to\nFigure 1, our LLM evaluation should then be conceptualized as: do humans and models share a similar underlying set\nof computations that map from actions to beliefs about mental states?1\n1There is ambiguity about what it would mean for ùëìto be ‚Äúthe same‚Äù as ùëì‚Ä≤: if we are to view LLMs as models of mental phenomena (beyond\njust word sequences or behavioral outputs), then we face the issue of comparing the distributed activity patterns within the network to potentially\nsymbolic descriptions of the mind‚Äôs computations (Bechtel and Abrahamsen, 1991; Blank, 2023). For our purposes here, all that matters is that ùëì\nmatters. To illustrate this in a simpler domain, imagine if someone was interested in whether LLMs have learned to multiply. This question could\nbe operationalized in at least two different ways, akin to our Q1 and Q2 in Figure 1: ‚ÄúGiven ùëãand ùëå, does the LLM output the same ùëã‚àóùëå= ùëças\npeople?‚Äù, or ‚ÄúHave LLMs learned the underlying multiplication algorithm that people use to compute ùëã‚àóùëå?‚Äù Under the first question, one might\nnot care about how the LLM arrives at the answer, whereas using, e.g., a lookup table to produce the correct answer would not count as evidence\n5\n\n\nFrom this starting point, it becomes clear that we have a definition issue when we ask (or state) whether LLMs\n‚Äúhave‚Äù Theory of Mind. When researchers say ‚Äúthe LLM has ToM‚Äù, they may mean that (i) the model matches\npeople‚Äôs behavior on mental-state-attribution tasks (ùëÄ= ùëÄ‚Ä≤), or that (ii) the model uses the same computations that\npeople use to connect observed behavior to mental states ( ùëì= ùëì‚Ä≤, whatever ùëìactually is), or that (iii) the model uses\nthe same computation and that this computation is specifically theory-like, in the sense of a generative world-model\nthat is then inverted. Note that these claims are not independent, but nested.\nThe situation becomes more complicated when arguments about the algorithms people use to make mental state\nattributions become ossified as specific empirical tasks. For example, consider the classic Sally-Anne task (Baron-\nCohen et al., 1985), which was originally proposed as a litmus test for whether someone is able to attribute ‚Äúfalse\nbelief‚Äù. In this task, participants are given vignettes describing two sisters, Sally and Anne. Sally places her sandwich\non the table for both her and Anne to see. Sally then leaves the room, and Anne places the sandwich underneath the\ncouch. Participants are then asked both where the sandwich actually is, and where Sally will look for the sandwich\nwhen she comes back into the room. The reasoning goes that if the participant can correctly say the sandwich is\nactually underneath the couch, but that Sally will incorrectly look for the sandwich where she last saw it (on the table),\nthen they can attribute false belief to Sally. The attribution of false belief is in turn taken as a strong indication that\nan agent ‚Äúhas‚Äù Theory of Mind (in the sense of a specific computation). The identity then becomes ‚Äúpass Sally-Anne\ntask‚Äù = ‚Äúcan attribute false belief‚Äù = ‚Äúhas Theory of Mind‚Äù, which then leads to the task being used as a ToM evaluation\nin LLM research (Kosinski, 2024; Bubeck et al., 2023; Kim et al., 2023).\nBut, this identity need not strictly hold, and can easily be abused in LLMs. For a comparison, consider the ‚Äúmirror\ntest‚Äù (Gallup Jr, 1970; Amsterdam, 1972): suppose a participant (adult or human child, or non-human animal) has a\nred dot marked on their forehead and, then shown their reflection in a mirror. If the participant reaches up to their\nown forehead to touch the dot rather than towards their reflection, the test concludes that the participant recognizes the\nimage in the mirror as their own reflection, rather than someone else. Putting aside for a moment the many questions\nsurrounding this test, suppose an engineer heard of the test and programmed a robot along the following lines: ‚ÄúIF\nred dot in sensory field, THEN move arm up to forehead‚Äù. The engineer then shows that the robot acts like a human\nchild when seeing its own reflection in a mirror with a red dot on it. The engineer concludes that either the robot has\nlearned to recognize its own reflection in the mirror, or the test is not a valid test of reflection recognition in people.\nObviously, neither option has to be true: the test was abused and passed in an uninteresting way, and one is still\njustified in supposing people do not use the same algorithm as the robot. A similar situation may exist for LLMs, such\nthat scoring well on a ToM benchmark need not imply that the model uses the same computation as people.\nThe distinction between behavior- vs computation-centric evaluation (Q1 vs Q2) could explain the conflicting\nfindings of prior studies. Before turning to the potential issues of the evaluation landscape itself, we discuss the\nfindings of multiple ToM evaluations in the next section, and highlight that the positive claims of LLMs ‚Äúhaving‚Äù\nToM are mostly supported by the success of LLM‚Äôs ability to match people‚Äôs input/output behavior, while the negative\nfor success under the second question, though in some cases people may use lookup tables. This distinction does not rely on commitments as to\nhow the lookup table is implemented within the network.\n6\n\n\nclaims of LLMs not having ToM use adversarial examples to argue that the computation used by the LLMs is not the\nsame as the one used by people. Both can be true, depending on the definition.\n3\nClaims for and against Theory of Mind in large language models\nIn this section, we briefly discuss the findings of several recent evaluations of ToM abilities in LLMs in light of the\ndefinitions provided in Section 2. Our goal is not to provide a comprehensive survey, but to illustrate key points of\nevidence on both sides of the debate, and how this evidence is contextualized by our working definition of ToM. We\nrefer readers to Ma et al. (2023) and Shapira et al. (2024) for more in-depth surveys of the ToM evaluation landscape.\n3.1\nPositive claims\nOver the past two years, a body of work has claimed that LLMs can succeed on tasks designed to measure ToM,\nsometimes reaching or even exceeding human-level performance. In a now well-known study (Kosinski, 2024), the\nperformance of 11 LLMs was compared to past studies that examined the behavior of children on two types of false-\nbelief tasks: the ‚Äúunexpected contents‚Äù and ‚Äúunexpected transfer‚Äù tasks. Both tasks involve scenarios in which a\ncharacter‚Äôs belief does not match the ground-truth state of the world. For example, the character may observe an\nopaque container with a misleading label (e.g., ‚Äúchocolate‚Äù written on a bag that actually contains popcorn), or may\nnot observe an action that swaps the location of two objects (e.g., another character moving a cup from the table to\nthe shelf after the protagonist leaves the kitchen). The study reported that GPT-4 solved 75% of the tasks, which is\ncomparable to the performance of six-year-old children. A controversial conclusion from this study was that either\nToM spontaneously emerged in LLMs, or that the classic tasks designed to evaluate ToM in humans are not actually\nmeasuring ToM.\nAlso using tests inspired by classic ToM tasks, Bubeck et al. (2023) concluded that GPT-4 has ‚Äúa very advanced\ntheory of mind‚Äù. Moghaddam and Honey (2023) found that with in-context learning, RLHF-trained LLMs performed\nnear human-level, and GPT-4 reached 100% accuracy on materials previously used for performing functional local-\nization of ToM in human brains. Using a synthetically generated dataset, Gandhi et al. (2023) evaluated LLMs on a\nvariety of inference tasks on the full causal graph that links actions, beliefs, and percepts, and reported that GPT-4\nbehaviors mirror human inference patterns.\nSuccessful cases also extend beyond the classic suite of false-belief tasks. van Duijn et al. (2023) compared 11\nLLMs to 7- to 10-year-old children on higher-order false-belief tasks, as well as non-literal language understanding\nand recursive rationality. They found that LLMs with instruction fine-tuning can outperform children, suggesting that\nthis training paradigm can induce aspects of ToM by rewarding cooperative communication. Street et al. (2024) found\nthat LLMs reach human-level performance on higher-order inferences, and GPT-4 even exceeds humans on 6-order\ninferences. Also recently, Strachan et al. (2024) found that GPT-4 performed at or above human-level for phenomena\ninvolving ToM such as indirect requests, false beliefs, and misdirection.\nAt the representational level, Jamali et al. (2023) reported that LLM embeddings encoded behaviorally-relevant\n7\n\n\ninformation about false- and true-belief, using materials designed for single-neuron recordings in humans. This could\npotentially be taken as suggestive evidence that ToM-relevant representations emerge in LLMs, in a way that mirrors\nToM-associated neuronal activity in the human brain.\nThese findings collectively suggest that LLMs are capable of succeeding on tasks that were designed to evaluate\nToM in humans. In addition, there are clear patterns across studies: for example, size (i.e., parameter count), fine-\ntuning, and few-shot prompting seem to significantly affect models‚Äô performance (Moghaddam and Honey, 2023;\nZhou et al., 2023).\n3.2\nNegative claims\nDespite the success cases, an opposing body of work has argued that LLMs in their current form do not possess robust\nToM abilities in the generalizable, flexible manner that humans do. For example, LLMs appear to be brittle to basic\nmodifications of the ‚Äúunexpected contents‚Äù task that would presumably be trivial for humans (Ullman, 2023). Sap et al.\n(2022) found that LLMs perform poorly on QA-based tests of social commonsense, and Trott et al. (2023) found that\nGPT-3 struggled to match or explain human behavior in false-belief tasks. Kim et al. (2023) tested LLMs in interactive\nsettings with information-asymmetric contexts, and found that LLMs perform poorly, even with fine-tuning or chain-\nof-thought prompting. Shapira et al. (2024) tested LLMs on a wide set of ToM tasks and also found that LLMs fail\non adversarial examples, suggesting their apparent ToM abilities can be explained by shallow heuristics. Zhou et al.\n(2023) showed that GPT-4 and PALM 2 could track beliefs in social scenarios, but struggled to translate these into\nresulting actions. He et al. (2023) demonstrated that LLMs struggle to perform recursive reasoning about agent beliefs,\nwith performance dropping as a function of the order of the task (e.g., LLMs find it harder to reason about what agent\nA believes agent B believes compared to reasoning about what agent B believes).\nA common theme of these failures is that models can succeed on ‚Äústandard‚Äù examples while failing on adversarial\nexamples, or minimal alterations to existing tasks that reveal where models are surprisingly brittle. These results also\nshow that fine-tuning or structured prompting strategies are not a cure-all for guaranteeing robust ToM performance.\n3.3\nWhat now?\nThe mixed evidence as to whether LLMs have ToM makes sense through the lens of our definitions in Section 2. Most\nof the positive evidence for ToM is based on an assumption that to ‚Äúhave‚Äù ToM is to match the input/output behavior\nof humans. Most of the evidence against ToM is designed with the intention of understanding whether the algorithm\nby which LLMs match human behavior is itself human-like, and generalizable in a human-like way.\nEven when LLMs fail at ToM tasks, their performance can be boosted via prompting, to hopefully adopt algo-\nrithmic biases that enable the LLM to reason about ToM tasks in ways that are similar to the (hypothesized) ways\npeople might be solving these tasks. Such recent prompting methods involve perspective taking (Wilf et al., 2024)\nand structured reasoning (Zhou et al., 2023). Taking a different route, Sclar et al. (2023) demonstrated that base-\nLLM performance can be boosted with a set of decoding-time symbolic reasoning components. They concluded that\n8\n\n\nLLMs might struggle with ToM because reasoning about the mental states of others often involves symbolic and im-\nplicit reasoning. Similarly, Tang and Belle (2024) improved LLM performance by externalizing the LLM‚Äôs reasoning\nabout beliefs via a symbolic executor designed for epistemic logic problems and fine-tuning the model on generating\nexpressions for this executor.\nThe evidence as to whether LLMs have ToM is mixed. More bleakly, the empirical landscape continues to change\nin a way that isn‚Äôt clearly converging, and instead resembles more a game of Whac-a-Model with changing hammers,\nin which new LLM models keep popping up and getting smacked with new, seemingly independent ToM evaluations.\nCurrently, we expect the research question of ‚ÄúDo language models have Theory of Mind?‚Äù to produce different\nanswers every time a new LLM is released because the definitions keep changing for what is meant by ‚ÄúLLM‚Äù and\n‚ÄúTheory of Mind‚Äù. Every LLM will likely fail at some cases, and succeed at others, and without a commitment\nto a standard definition of and approach to evaluating ToM, we fail to move the needle on our broader theoretical\nunderstanding of ToM abilities in large neural models.\nWhile we take the stance that ToM evaluations should be about the computations that LLMs use to map observable\nbehaviors to mental states (i.e., Q2), both the behavior- and computation-centric approaches can lead to potential issues\nin evaluation. We discuss these issues in the next section.\n4\nCurrent issues with Theory of Mind evaluations\nHaving seen the conflicting claims about LLMs‚Äô ToM abilities, as well as definitions of ToM in Section 2, we now\nre-evaluate existing evaluation approaches in more depth. As we already stated, a central question in evaluations is\nwhat it would mean for an LLM to ‚Äúhave‚Äù ToM. The conclusions that one draws about an LLM‚Äôs cognitive ability\nbased on evaluation results depend on how one defines the underlying ability, as well as whether the evaluation actually\nmeasures the ability as defined. We argue that much of the confusion surrounding ToM in LLMs is due to a lack of\nclarity on both of these fronts.\nIn this section, we now highlight two issues with existing ToM evaluations: an over-emphasis of matching the\nbehaviors of humans in limited ToM evaluations (cf. a computation-matching view) (Section 4.1), and threats to the\nvalidity of the evaluation materials (Section 4.2).\n4.1\nIssue 1: ToM evaluations focus on matching behavior\nBuilding on the distinctions made in Section 2, we believe much of the effort in ToM evaluations for LLMs has been\nspent on matching the expected behavior of people on various ToM-related tasks, such as false-belief attribution and\nrecognizing faux pas, without much concern for the computations that generate those behaviors. While matching\npeople‚Äôs behavior on specific data-sets as a target metric is by no means a bad idea, it makes the evaluation of highly\ngeneral abilities such as ToM more difficult than it should be. And while any evaluation must in some sense measure\nobservable behavior, the focus on purely matching human behavior without concern for the underlying computations\ncan force us to mindlessly catalog all of the possible behaviors we might expect from ToM, akin to cataloging all\n9\n\n\npossible multiplication problems rather than more general consideration of the underlying multiplication algorithm.\nThis line of thought has been taken seriously by some and has inspired people to move beyond tasks such as false\nbelief and faux pas, and toward broader taxonomies of ToM behaviors, such as the ‚ÄúAbilities in Theory of Mind\nSpace‚Äù (ATOMS) framework (Beaudoin et al., 2020), which some have advocated for in LLM evaluation (Ma et al.,\n2023), and the ‚ÄúExperimental Protocol Inventory for Theory of Mind Evaluation‚Äù (EPITOME) framework (Jones et al.,\n2024). Other approaches have argued that we should reorganize the study of ToM not according to isolated abilities,\nbut instead based on the kind of information sources needed (Achim et al., 2013). The intention of these frameworks\nmirrors earlier efforts in computer vision to break down high-level cognitive abilities such as visual perception into\nsmaller, well-defined tasks (Zamir et al., 2018).\nWhile some progress will be made by cataloguing the space of behaviors we want LLMs to have with respect\nto ToM, we believe that progress will be limited simply by the breadth of ToM alone. We will likely never have an\nevaluation for every possible behavior enabled by ToM. Given this, we believe that proposing generic frameworks\nfor the underlying computations of ToM (and evaluations developed with those target computations in mind) holds\npromise for both developing and evaluating ToM in artificial agents. Such frameworks have already been suggested in\ncognitive science ‚Äì for example, defining ToM as a generic inverse planning engine or inverse reinforcement-learning\nproblem (Baker et al., 2009; Baker and Tenenbaum, 2014; Baker et al., 2017; Jara-Ettinger, 2019). While we do\nnot comment here about whether these proposals are the ‚Äúright‚Äù way of describing how humans map from actions to\nmental states, we do believe that these efforts have already led significant progress on understanding and evaluating\nToM in humans and machines over behavior-focused counterparts.\nTo concretely illustrate how a computation-focused benchmark might look, we highlight the AGENT benchmark\n(Shu et al., 2021) and the BigToM framework (Gandhi et al., 2023). In both of these benchmarks, rather than focusing\ncompletely on one or two tasks that have been defined in developmental psychology, such as the Sally-Anne task for\nfalse-belief attribution (Baron-Cohen et al., 1985), here the emphasis is on building out a minimal set of evaluations\nthat target the core computational abilities of any agent that implements a generic Bayesian inverse-planning engine\nto perform ToM. For AGENT, this includes evaluations for goal preferences, action efficiency, unobserved constraints,\nand cost-reward tradeoffs. For BigToM, this includes a prompting scheme that uses LLMs to generate instances of\ncausal graphs that reflect the expected causal trace of human-like social reasoning, such as inferring the actions of an\nagent from their inferred beliefs, observations, and desires. These evaluations are all motivated by core concepts in\ncognitive science that have been investigated at length over years, such as rationality assumptions and goal-directed\naction.\nImportantly, these benchmarks evaluate model generalization, allowing models to be trained on one situation and\nthen tested on another, while keeping the underlying ToM principle the same. For example, in the case of AGENT, a\nmodel may be trained on situations in which an agent minimizes effort by going over a bridge (rather than around a\npit), but then tested on a situations in which the agent goes through a hole in a barrier (rather than around the barrier).\nIn the case of BigToM, LLMs can be tasked with inferring the actions of an agent under different task conditions such\nas the presence or absence of certain observations, goals, or beliefs. This allows for finer-grained controls on model\n10\n\n\nanalysis, as opposed to being trained on random selections of pit/barrier situations that are independent and identically\ndistributed.\nDepending on how we define ToM ‚Äì either as a set of behaviors that manifest in social interactions, or as a specific\nset of computation(s) ‚Äì we will come to different philosophies regarding how to evaluate this ability. While we believe\nthe latter will bear more fruit for understanding ToM, evaluating it in machines, and developing more socially capable\nAI systems, there is still much progress to be made on this front. A concerted effort toward building evaluations\ngrounded in cognitive theory, such as the AGENT benchmark (Shu et al., 2021), BigToM (Gandhi et al., 2023), or\nthe EWOK benchmark (Ivanova et al., 2024), can neatly define the space of minimal computations needed to perform\nToM at a human level and the set of evaluations for determining when a machine has such capabilities.\n4.2\nIssue 2: ToM evaluations might not be testing ToM\nRegardless of what definition of ToM we adopt, as discussed in Section 4.1, there is the independent issue of whether\nour evaluation actually measures the agreed-on latent construct. An evaluation might fail to measure ToM ‚Äì whatever\nwe take ToM to mean ‚Äì in two ways: overestimating models‚Äô ToM abilities (models being right for the wrong reasons),\nor underestimating models‚Äô ToM abilities (models being wrong for the wrong reasons).\n4.2.1\nRight for the wrong reasons\nTraining away evaluations.\nA potential failure mode of current evaluation paradigms is what we refer to as ‚Äútraining\naway‚Äù, or training on the testing data involved in the evaluation (Jacovi et al., 2023). This phenomenon involves\nupdating a model with respect to specific instances where the model seems to fail (e.g., adversarial stimuli) after the\ndemonstration of failure, without fundamentally changing the model‚Äôs underlying computations. These updates could\nbe implemented in the traditional sense of the word ‚Äútraining‚Äù ‚Äì i.e., by updating the parameters of the model (through\ncontinual pre-training or fine-tuning) ‚Äì or in-context learning without parameter updates. Regardless how training\naway is implemented, the underlying issue is the same.\nAs an analogy, suppose we are interested in evaluating whether a model has ‚Äúlearned‚Äù multiplication. We can\nstart with a ‚Äúmultiplication model‚Äù that only updates a lookup table with input pairs of numbers (the multiplicand and\nthe multiplier) and their corresponding output answers (the product). Whenever this model fails on a new problem, it\nsimply adds the input pair and correct output product to the lookup table so it never fails again on that input. Obviously,\nover time the model will answer more and more questions correctly, but the underlying mode of algorithm it uses to\nsolve multiplication is not changing -‚Äì it always simply looks up the corresponding output for any input. Naturally,\nas we iterate on this learning process, the space of relevant input numbers will shrink,2 but not for interesting reasons,\nand certainly not because the model has learned a human-like way to perform multiplication, despite the increase in\nevaluation performance.\nThe issue of training away can be seen as a special type of data contamination (Magar and Schwartz, 2022; Dodge\net al., 2021; Carlini et al., 2021), which refers to the more general phenomenon of test items being present in the\n2The set of numbers is infinite, but in this thought experiment we imagine a world where only a small set of numbers are relevant.\n11\n\n\ntraining data of the model being evaluated. We believe that training away warrants special attention for several reasons.\nFirst, as has been previously noted (Jacovi et al., 2023), it only applies to closed-API models which offer no guarantees\nabout how models are ‚Äúoriginally‚Äù trained (i.e., before public release), nor how they may be continually updated based\non test items input through the API. And second, training away can give the illusion of progress within a model,\nwhereas other types of data contamination can give the illusion of progress across models. This directly conflicts with\nthe effort to create adversarial ToM evaluations. If models keep ‚Äúeating up‚Äù examples that were previously designed\nto be adversarial, then there is an illusion that they are becoming more sophisticated, without actually employing more\nrobust reasoning or computational strategies. This is distinct from other types of data contamination, as we cannot\naddress this particular test-on-train issue by simply creating novel or out-of-distribution evaluation items.\nTo avoid the issue of training away, we suggest using openly accessible models, which can be seen as static artifacts\nwhich are frozen with respect to any given benchmark (Frank, 2023b). Instead of claiming ‚ÄúChatGPT can do X‚Äù or\n‚ÄúGPT-4 can do Y‚Äù, we need to recognize that these models keep changing with more and more tricks and more and\nmore data, but in a way that makes it impossible to assess whether new successes are the result of actual better ToM\nreasoning, or simply putting the test into the training.\nAlternate strategies for performing ToM tasks.\nA recurring concern with benchmarking is that models may use\nheuristics or shallow strategies to correctly perform a task without necessarily using the ability that is being tested\n(McCoy et al., 2019; Pacchiardi et al., 2024). Sometimes it might be the case that a model may exploit unintended\nstatistical associations in the test items ‚Äì for example, if the correct answer options in a multiple-choice setting con-\nsistently have higher word overlap with the question than the incorrect answer options, then the correct answer will\npresumably have higher probability than the other answers conditioned on the question. This is a general concern not\nrestricted to ToM (Ivanova, 2025), and can be addressed by removing spurious confounds from test items and adding\ncontrols.\nIt could also be that models succeed on ToM evaluations not by relying on simple surface-level heuristics (related\nto the test items themselves), but by appealing to deeper heuristics (learned during training). For example, imagine\na false-belief scenario where Anne puts her fork in the kitchen, but Sally moves the fork to the bedroom after Anne\nleaves. A model could plausibly correctly predict that Anne would then look for her fork in the kitchen, simply because\nforks are more likely a priori to be found in kitchens than bedrooms. Indeed, recent studies have shown that models\nare highly sensitive to content effects (Lampinen et al., 2024) and the statistical regularities of pretraining data (McCoy\net al., 2024). A reasonable and popular strategy for testing the robustness of a model‚Äôs ToM abilities is to construct\nadversarial test cases, which might violate a model‚Äôs expectations or introduce settings that are beyond the distribution\nseen in training. While this is an important endeavor, it could also introduce other complications, which we elaborate\non below.\n12\n\n\n4.2.2\nWrong for the wrong reasons\nAdversarial tests increase auxiliary task demands.\nAs discussed above, we take the stance that ToM evaluation\nin LLMs should move toward the computation-centric view instead of focusing on behavior-matching. This general\napproach has been growing in popularity, as recent ToM benchmarks have started using adversarial tests to probe for\nways in which models and humans appear to use different computations (Shapira et al., 2024; Holterman and Deemter,\n2023; Aru et al., 2023; Ullman, 2023). In many cases, models succeed at straightforward versions of a task but fail\nunder adversarial conditions (where a human would presumably succeed), implying that their earlier success reflected\nan ability to match a human-like input/output mapping, without using the kinds of computations a human would use.\nWhile we believe the adversarial approach is on the right track, we also note that it can introduce complications\nfor measuring ToM. As an example, consider the ‚Äúunexpected contents‚Äù scenario, which was used to evaluate LLMs\nby (Kosinski, 2024):\nHere is a bag filled with popcorn. There is no chocolate in the bag. Yet, the label on the bag says ‚Äúchocolate‚Äù\nand not ‚Äúpopcorn.‚Äù Sam finds the bag. She had never seen the bag before. She cannot see what is inside the\nbag. She reads the label.\nGPT-3.5 and several other LLMs correctly predict that Sam will see the bag is full of popcorn if she opens the bag,\nand yet she believes it contains chocolate. However, consider the following simple modification:\nHere is a bag filled with popcorn. There is no chocolate in the bag. The bag is made of transparent plastic, so\nyou can see what is inside. Yet, the label on the bag says ‚Äòchocolate‚Äô and not ‚Äòpopcorn.‚Äô Sam finds the bag.\nShe had never seen the bag before. Sam reads the label.\nIn this scenario, previously successful LLMs still predict that an agent looking at a bag of popcorn labeled as ‚Äúchoco-\nlate‚Äù would believe that the bag contains chocolate, even though the bag is transparent (Shapira et al., 2024; Ullman,\n2023). It has been implicitly assumed that humans, who have robust ToM, would not fail on these ‚Äútrivial alterations‚Äù\nto the task (Ullman, 2023). However, a recent experiment demonstrated that human participants also perform worse\nunder these perturbations (Strachan et al., 2024).3 Because these items are more complex, there are many potential\nreasons for why LLMs (or humans) might fail ‚Äì for example, because they truly lack some ability to integrate mental\nstates and attribute updated beliefs, or because they are failing to integrate agent models with the relevant physical\nprinciples described in the scenario (e.g., transparency), or because the scenarios cause more mental load which intro-\nduces error. If a model (or human) were to fail on this test because they didn‚Äôt know what ‚Äútransparency‚Äù meant, or\nwere unable to integrate this concept into the social context, then we should hesitate before attributing this failure to a\nlack of ToM.\nMore broadly, as LLMs become more sophisticated, test items will need to become more adversarial in order to\n3Complicating the issue, LLMs performed significantly worse on these tasks than people, raising the question of whether the focus should be:\n‚Äúboth humans and LLMs are not perfect in these cases‚Äù, or ‚Äúwhile imperfect, humans are better than LLMs in these cases‚Äù.\n13\n\n\npoke holes in their apparent abilities. And as test items become more adversarial, they will inevitably also become\nmore complex. As this happens, we run the risk of no longer primarily testing ToM abilities, but instead introducing\nunintended tests of the ability of models to overcome auxiliary demands associated with the task (e.g., longer context\nwindows, keeping track of more agents, keeping track of unfamiliar vocabulary items, or performing physical reason-\ning) (Hu and Frank, 2024). Indeed, developmental psychologists have widely debated the age at which ToM ‚Äúemerges‚Äù\nin children, and tasks that reduce auxiliary demands have revealed evidence for some ToM abilities in young children\nwho would otherwise fail similar tests (Lewis and Osborne, 1990; Carlson et al., 1998; Surian and Leslie, 1999; Setoh\net al., 2016; Fu et al., 2023). To design ToM evaluations that more directly measure ToM while minimizing auxiliary\ndemands, we need to develop a deeper understanding of the kinds of resource constraints that LLMs face, as well as\nbest practices for performing ‚Äúspecies-fair‚Äù evaluations (McCoy et al., 2024; Lampinen, 2023; Firestone, 2020).\nThere may be cases where a researcher may want to evaluate a model according to the strictest adversarial standard:\nfor example, if a model is being developed for a user-facing application in a setting with potential for emotional or\nphysical harm. In these cases, our goal might be for models to demonstrate ToM abilities, no matter how complex the\nenvironment or context might be. But if our goal is to understand LLMs scientifically, then our tests should be ‚Äúpure‚Äù,\nin the sense that they should isolate the targeted cognitive capability of interest. This distinction is closely related\nto the divide between ‚Äúcompetence‚Äù and ‚Äúperformance‚Äù in LLM evaluation (and cognitive science), and remains an\nimportant design choice for LLM evaluations more broadly (Firestone, 2020; Lampinen, 2023; Hu and Frank, 2024).\nConcretely, then, we recommend that future ToM evaluations explicitly describe the auxiliary demands that are\nassociated with performing the tested task, and design control conditions that test whether models can overcome these\ndemands. We also recommend comparing model performance to empirically measured human performance whenever\npossible, instead of implicitly assuming that humans will be at ceiling (see also Ivanova 2025).\nText representations introduce pragmatic artifacts.\nThe typical approach to ToM or other types of cognitive\nevaluation is to take evaluations designed for testing these abilities in humans, and then adapt them for testing LLMs.\nThe benefits of this approach are clear: these stimuli have been created by domain experts, and have often been subject\nto careful statistical controls and empirical validation.4 However, translating existing assays of ToM (or other types of\ncommonsense knowledge) into an LLM-appropriate text format may introduce unintended artifacts, which have been\nunder-studied in existing ToM evaluations.\nTraditionally, ToM has been evaluated in children using embodied settings ‚Äì for example, by having dolls or\npuppets act out a scene (Baron-Cohen et al., 1985). When this ‚Äúacting out‚Äù paradigm is translated into text input for a\nlanguage model, it creates a potential mismatch between the salience (i.e., markedness) of the linguistic stimulus and\nthe salience of the corresponding actions or events in the scenario. This could happen for several reasons. First, text\ncorpora may underestimate the prevalence of frequently-occurring concepts or events due to reporting biases (Gordon\nand Van Durme, 2013), and as a result, LLMs may assign relatively low probabilities to strings describing events that\nare actually highly predictable in the real world. Second, the filtering of the scenario through language comprehension\n4Here we put aside the issues of data contamination and ‚Äútraining away‚Äù; see Section 4.2.1 for more detailed discussion of these phenomena.\n14\n\n\nmay interact with pragmatic inference. People generally expect their interlocutors to say things that are informative\nand relevant ‚Äì i.e., things that are worth the effort to say (Grice, 1975; Sperber and Wilson, 1986). Conversely, as\ncomprehenders, we try to impute meaning beyond what is literally expressed by inferring that speakers had a reason to\nsay what they did. In the original experimental settings, where human subjects watch a scene unfold, their observations\nare coming from their own perception, instead of being filtered through the lens of a presumably cooperative, rational\nspeaker. Therefore, while in a visual scene people can choose to attend to certain pieces of information, when the\nscene is described in language everything becomes somewhat relevant.\nAs an example, consider a simple modification of the false-belief scenario discussed in Section 4.2, where the\nitalicized portions mark the differences from the original example:\nHere is a bag filled with popcorn. There is no chocolate in the bag. The bag is made of transparent plastic, so\nyou can see what is inside. Yet, the label on the bag says ‚Äòchocolate‚Äô and not ‚Äòpopcorn.‚Äô Sam finds the bag.\nShe had never seen the bag before. She has no prior knowledge of the bag‚Äôs contents. She cannot smell what\nis in the bag. She cannot taste what is in the bag. Sam reads the label.\nIf this scenario were ‚Äúacted out‚Äù in a grounded setting, the content of the italicized portions would likely not contribute\nmuch new information ‚Äì i.e., the fact that Sam cannot taste the contents of the bag is trivial based on the observer‚Äôs past\nexperience with plastic bags. When this content is described in text, however, it might lead an observer to ‚Äúread into‚Äù\nwhy a speaker has chosen to phrase things in this way, and bias an observer to think that Sam will rely on the label to\nidentify the contents of the bag. This is a bit of a contrived example, but it raises the broader issue of whether specific\ntypes of linguistic content may be introducing artifacts by directing models‚Äô (or humans‚Äô) attention in unintended\nways.\nAs another example of linguistic cues that may introduce unintended biases, consider the following item from the\nAdversarial Commonsense with False-Belief dataset (Shapira et al., 2024):\nOn the shelf in the company‚Äôs headquarters, there is a hard drive that contains only audio files and no video\nfiles. Yet, confusingly, its label clearly states ‚Äúvideo files‚Äù and not ‚Äúaudio files‚Äù. The newly hired computer\nengineer finds the hard drive on the shelf. She has never seen this hard drive before. Her boss comes over and\nsays ‚Äúthe hard drive contains audio, ignore the label‚Äù. She reads its label.\nIn the second sentence, the use of the words ‚Äúyet‚Äù and ‚Äúconfusingly‚Äù imply a value judgment on the part of the speaker\n(i.e., the producer of the text). Without even reading the rest of the scenario, a comprehender may already be primed\nto expect that some character will be confused or hold a false belief. This might bias a comprehender to infer that the\ncomputer engineer will believe the hard drive will contain video files, despite the trusted testimony from her boss. This\ninference would lead to an incorrect answer according to the benchmark, which assumes that the trusted testimony\nwill override the misleading label.\nWhile humans will likely also be sensitive to these kinds of inferences, they may be better than LLMs at sup-\n15\n\n\npressing or disregarding this information when they are aware they are being tested, especially in an adversarial or\nchallenging setting. Indeed, Shapira et al. (2024) cite this as a potential explanation of why models are failing on ad-\nversarial examples. The authors speculate that the fine-tuning training phase may encourage models to be cooperative,\ncausing them to ‚Äúpay too much attention to the mention of the false label in the unexpected contents task‚Äù. This issue\nmight be especially heightened if the mention of the false label contains value-coded words such as ‚Äúconfusingly‚Äù, as\nmodels are strongly regularized to be helpful (Ouyang et al., 2022; Bai et al., 2022). In a sense, the improvements in\nbeing ‚Äúpragmatic‚Äù that are gained during the fine-tuning process may actually work against models in text-based ToM\nscenarios, by potentially rewarding a model for over-attending or imputing meaning to irrelevant details.\nNote that moving toward multimodal evaluations (Jin et al., 2024) may alleviate some of these specific limitations,\nbut is not necessarily a solution to the broader issues that we have highlighted in the current section (Section 4). If\nthe fundamental problem is that we want to identify the function ùëìthat connects observed actions ùê¥to posteriors over\nmental states ùëÄ, rather than just getting a better score on behaviorist tests, then focusing on getting a better score on\na static benchmark (as is done in much of AI evaluation) will create problems regardless of whether the stimuli are\nmultimodal or text-only.\n5\nFuture directions for LLM ToM evaluation\nIn closing, we discuss various considerations and desiderata for LLM ToM evaluation that have been underexplored in\nthe current literature. We believe these topics suggest exciting directions for future work, with the potential to advance\nour understanding of artificial systems as well as human cognition.\nThe relationship between pragmatic communication and ToM.\nThe relationship between pragmatic (or non-\nliteral) communication and ToM has been a major topic of debate in cognitive science (Bosco et al., 2018; Enrici\net al., 2019; Rubio-Fern√°ndez, 2019). Some researchers have argued that pragmatics is highly linked to ToM or\nsocial reasoning (Milligan et al., 2007; Spotorno et al., 2012; Kline Struhl et al., 2018; Enrici et al., 2019; Jacoby and\nFedorenko, 2020), while others have argued that pragmatics and ToM constitute distinct, dissociable abilities (Bosco\net al., 2018; Babarczy et al., 2024). LLM evaluations offer a potentially interesting angle to this debate. If LLMs appear\nto have pragmatic abilities but fail at ToM, then that would go against the idea that ToM is strictly necessary to do\npragmatics. And conversely, if LLMs pragmatic abilities are highly tied to their ToM abilities, then this would provide\na new type of evidence that pragmatic and ToM abilities are intertwined, potentially involving similar computations.\nWhile ToM and pragmatics have both been the topic of LLM evaluation, they have primarily been investigated\nusing separate tasks and evaluation settings. In both cases, the investigations have tended to focus on behavior-\nmatching. As discussed earlier, ToM benchmarks tend to focus on tasks such as false-belief attribution and faux\npas (see Section 3 for examples). Pragmatics benchmarks have focused on phenomena such as indirect responses,\nconversational implicatures, and presupposition (e.g., Sravanthi et al. 2024; Zheng et al. 2021; Hu et al. 2023; Ruis et al.\n2023; Jeretic et al. 2020). An interesting direction for future work is to explicitly study what abilities tend to co-occur\n16\n\n\nin models: e.g., whether their ability in certain pragmatic tasks (like irony interpretation) predicts their ability in certain\nToM tasks (like false-belief inference). These relationships could then be compared to relationships that have been\ndiscovered in humans, both in adults (Floyd et al., 2023) as well as across development (e.g., Babarczy et al. 2024).\nWhether the relationships observed in models mirror or diverge from those attested in humans, the outcome would be\ninteresting. If the relationship between pragmatics and ToM abilities looks similar across models and humans, this\nwould suggest that LLMs‚Äô learning paradigms lead to the co-emergence of certain kinds of abilities. If the relationship\ndiffers across models and humans, this would suggest that models and humans acquire the tested abilities in different\nways, or use different kinds of information to perform the tested tasks. Indeed, some recent studies have begun to\ninvestigate the relationship between pragmatics and mentalizing in LLMs. For example, Barattieri di San Pietro et al.\n(2023) find that LLMs exhibit ‚Äúmostly human-like‚Äù pragmatic skills with exception to aspects of pragmatics that\nrequire representations of mental states, and Hu et al. (2023) find that LLMs struggle most with phenomena that rely\non violations of social expectations (such as humor and irony). These studies suggest that pragmatic behaviors can\nemerge in LLMs, but primarily when these behaviors are not hypothesized to involve mental state inference.\nBeyond analyzing the relationship between pragmatics and ToM at the task level (e.g., correlating false-belief\nabilities with irony interpretation abilities), we believe that studying LLMs‚Äô pragmatic abilities with a computation-\nmatching approach can also reveal information about LLMs‚Äô ToM abilities. The experimental pragmatics literature\nhas shown that many human pragmatic behaviors can be explained with ToM-like inference frameworks, such as the\nRational Speech Act (RSA) model (e.g., Frank and Goodman 2012; Goodman and Frank 2016; Degen 2023). RSA\nproposes that a speaker and listener communicate by performing Bayesian reasoning about the other‚Äôs mental states:\nthe speaker chooses an utterance based on how likely it will get the listener to recover the intended meaning, and the\nlistener infers a meaning based on the alternative utterances the speaker could have used. Recently, some evaluations\nof LLMs‚Äô pragmatic abilities have begun to analyze whether LLMs‚Äô outputs can be characterized by a pragmatic\nspeaker/listener predicted by RSA (Carenini et al., 2023; Jian and N, 2024). If LLMs‚Äô behaviors do conform to the\nnormative predictions of ToM-like reasoning frameworks such as RSA, this would be informative in two ways: (1)\nit would provide a potential computational explanation for LLMs‚Äô pragmatic behaviors, and (2) it would provide\nevidence for models‚Äô ToM abilities that is complementary to the standard behavioral inventory (e.g., false-belief and\nfaux pas).\nLearning ToM.\nEven if LLMs have learned ToM, which is debatable, it leaves open the question of what kind of\ntraining objectives and linguistic input support the emergence of ToM abilities. An interesting direction for future work\nis to leverage the control we have over LLMs to test specific hypotheses about how ToM is learned. Such controlled\nlearning experiments could contribute to the interpretability of LLMs, and have also shown promise for providing new\ninsights into theories about human cognition, such as the acquisition of syntactic generalizations (McCoy et al., 2018;\nYedetore et al., 2023; Misra and Mahowald, 2024).\nAny ability in an LLM, including ToM reasoning, must come from either the pre-training phase, the fine-tuning\nphase, or some interaction of the two. A potential experiment would be to test whether ToM abilities can emerge\n17\n\n\nduring a model‚Äôs pre-training phase, or if ToM reasoning requires some form of fine-tuning/alignment (most often a\nform of reinforcement learning from human feedback, or RLHF; Ouyang et al. 2022). An indirect consequence of the\nfine-tuning phase may be that models‚Äô outputs become more pragmatically appropriate, conforming to the cooperative\nprinciples that govern human conversation (Grice, 1975). In fact, recent work has shown that any language model\ncan be seen as a bounded pragmatic speaker, or a speaker that tries to communicate pragmatically but is limited in its\ncomputational capacity, and RLHF is equivalent to applying variational inference on such a speaker (Nguyen, 2023).\nIndeed, past studies reported that models with instruction fine-tuning, but not base models, were able to outperform\nchildren on a series of ToM tasks (van Duijn et al., 2023). However, they did not perform a controlled comparison\nwithin model families (with the exception of Falcon and Falcon-Instruct), leaving open the question of whether fine-\ntuning is causally driving performance improvements aside from other differences in size or architecture.\nRelatedly, another experiment would be to test what kind of linguistic data in the pretraining phase can boost ToM.\nThere are links between language development and ToM development in children (de Villiers and de Villiers, 2014) ‚Äì\nfor example, through exposure to words expressing propositional attitudes such as ‚Äúknow‚Äù and ‚Äúbelieve‚Äù (Brown et al.,\n1996), as well as other syntactic and conversational structures (Ruffman et al., 2002; Hale and Tager-Flusberg, 2003;\nAstington and Baird, 2005; Milligan et al., 2007; Pyers and Senghas, 2009; Slaughter and Peterson, 2011). As has\nbeen previously suggested (Frank, 2023a), one approach could be to train LLMs on corpora with and without certain\nlinguistic markers such as ‚Äúknow‚Äù and ‚Äúbelieve‚Äù and test what kind of effect this manipulation has on downstream\nToM behaviors.\nSpontaneous vs. prompted ToM.\nHumans have a strong tendency to attribute mental states to things, referred to\nas ‚Äúhyperactive agency detection‚Äù by (Barrett, 2004). These tendencies are difficult to suppress, as demonstrated\nby the classic experiments of (Heider and Simmel, 1944): when watching simple animations of shapes moving in a\ntwo-dimensional environment, we attribute goals, intents, and even emotions to the shapes. While it remains debated\nwhether ToM is automatic (Apperly, 2011, 2018; Rubio-Fern√°ndez et al., 2019), humans are clearly predisposed to\nperform mentalizing in some way. Furthermore, the tendency to reason about agents and intentionality is present even\nin the earliest stages of life (Gergely et al., 1995; Saxe et al., 2005).\nBy contrast, ToM-like behaviors in LLMs often need to be explicitly prompted or cued, either through strategies\nsuch as chain-of-thought or few-shot learning (Moghaddam and Honey, 2023), or through bespoke structured frame-\nworks (Wilf et al., 2024; Zhou et al., 2023; Guo et al., 2024). Accordingly, Gurney et al. (2024) call for ‚Äúspontaneous\nToM‚Äù in LLMs: that is, ToM-like behaviors that do not need to be explicitly prompted or cued, and instead fall out of\nmore general principles or cognitive functions. For example, models could have a general bias toward paying attention\nto information about agents. How this would be implemented remains an open question, but this is a desideratum of\nsocially capable artificial agents that deserves further study.\nMechanistic interpretability and ToM.\nThe effort to understand the intermediate computations of LLMs relates\nto a broader, ongoing discussion regarding mechanistic interpretability. A lot of recent work has attempted to under-\nstand the internal computations, representations, and algorithms learned by neural networks, for example by projecting\n18\n\n\nvarious features of the model into a lower-dimensional space (Wang et al., 2022; Merullo et al., 2023). More recent\nmethods have considered the use of LLMs as mappings that project such features of neural models into natural lan-\nguage space, allowing for easier interpretation (Singh et al., 2024). One potential avenue for future work is exploring\nthe use of mechanistic interpretability methods to better understand the computations that generate model behavior in\nToM evaluations.\nImportantly, mechanistic interpretability is not a cure-all for the issues we‚Äôve outlined here. The problems of\ndefinition and validity that we point out for ToM evaluations still stand: all mechanistic interpretability methods\nincorporate assumptions about what model features are relevant for a given task, how these features are mapped to\nhigher-level interpretations. Whether we decide to perform readouts from various layers of the model as in the case\nof early decoding (nostalgebraist, 2020), feature visualization (Olah et al., 2017), conceptual activations (Kim et al.,\n2018), or perform causal circuit analysis on the forward pass of the model in question, our choices of what ‚Äúcounts‚Äù\nas a relevant feature, concept, or circuit is dependent on our definition of what ToM is and what counts as a valid use\nof ToM.\nExisting cognitive models of ToM ‚Äì in particular, inverse-planning and RSA models ‚Äì can serve as a benchmark of\ncomputation for interpretability, above and beyond general projection methods. For example, if we think the variables\npeople are using in ToM are ‚Äúcognitive‚Äù variables that determine observed behavior, such as beliefs, desires, and\ngoals, this gives us a better target for asking, did the LLM learn to infer and represent these variables when making\nsense of observed behavior. Along these lines, Jamali et al. (2023) have observed LLM embeddings that encode\nbehaviorally-relevant information about false- and true-belief, suggesting the feasibility of this style of approach. In\nother words, regardless of the exact means by which we interpret the algorithms used by models, we must ground\nthese interpretations in a normative framing of the process we‚Äôre intending to interpret. Cognitive science offers\nthese normative framings, which can be productively used in conjunction with mechanistic interpretability methods to\nimprove our understanding of how these models solve ToM tasks.\n6\nConclusion\nWe argued that the lack of clarity on how ToM is defined is a major contributor to the disagreement surrounding\nwhether LLMs have ToM, and discussed two definitions of what it means for an LLM to ‚Äúhave‚Äù Theory of Mind.\nBuilding on these concepts, we then highlighted two prevailing issues with current ToM evaluations: a focus on the\nbehavior-matching definition of ToM, and threats to the validity of ToM evaluations. Our recommendations are to (1)\nmove toward comparing the computations used by humans and machines to arrive at mental state inferences, instead\nof focusing on behaviorist input-output matching; (2) use clearer construct validity in evaluations and specify the\nauxiliary task demands that might be imposed by their tests; and (3) use frozen (and ideally open) models that are not\ncontinually updated on adversarial examples. While there are no simple solutions, we hope that this will help enable\nmore precise, valid measurements of ToM ability that are grounded in what we know about human cognition.\n19\n\n\nAcknowledgments\nThis work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the\nKempner Institute for the Study of Natural and Artificial Intelligence.\nReferences\nA. M. Achim, M. Guitton, P. L. Jackson, A. Boutin, and L. Monetta. On what ground do we mentalize? Characteristics\nof current tasks and sources of information that contribute to mentalizing judgments. Psychological Assessment, 25\n(1):117‚Äì126, 2013. ISSN 1939-134X(Electronic),1040-3590(Print). doi: 10.1037/a0029137.\nB. Amsterdam.\nMirror self-image reactions before age two.\nDevelopmental Psychobiology: The journal of the\ninternational society for developmental psychobiology, 5(4):297‚Äì305, 1972.\nI. Apperly. Mindreaders: The cognitive basis of \"Theory of Mind\". Psychology Press, New York, 2011. ISBN\n978-1-84169-697-3 (Hardcover).\nI. Apperly. Mindreading and Psycholinguistic Approaches to Perspective Taking: Establishing Common Ground.\nTopics in Cognitive Science, 10(1):133‚Äì139, Jan. 2018. ISSN 1756-8757. doi: 10.1111/tops.12308. URL https:\n//doi.org/10.1111/tops.12308.\nI. A. Apperly, F. Warren, B. J. Andrews, J. Grant, and S. Todd.\nDevelopmental Continuity in Theory of Mind:\nSpeed and Accuracy of Belief‚ÄìDesire Reasoning in Children and Adults. Child Development, 82(5):1691‚Äì1703,\nSept. 2011. ISSN 0009-3920. doi: 10.1111/j.1467-8624.2011.01635.x. URL https://doi.org/10.1111/j.\n1467-8624.2011.01635.x.\nJ. Aru, A. Labash, O. Corcoll, and R. Vicente. Mind the gap: challenges of deep learning approaches to Theory\nof Mind. Artificial Intelligence Review, Jan. 2023. ISSN 1573-7462. doi: 10.1007/s10462-023-10401-x. URL\nhttps://doi.org/10.1007/s10462-023-10401-x.\nJ. W. Astington and J. A. Baird, editors. Why Language Matters for Theory of Mind. Oxford University Press, Apr.\n2005. ISBN 978-0-19-515991-2. doi: 10.1093/acprof:oso/9780195159912.001.0001. URL https://doi.org/\n10.1093/acprof:oso/9780195159912.001.0001.\nA. Babarczy, D. Dob√≥, P. Nagy, A. M√©sz√°ros, and √Å. Luk√°cs. Variability of theory of mind versus pragmatic ability\nin typical and atypical development. Journal of Communication Disorders, 112:106466, Nov. 2024. ISSN 0021-\n9924. doi: 10.1016/j.jcomdis.2024.106466. URL https://www.sciencedirect.com/science/article/pii/\nS0021992424000625.\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph,\nS. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. John-\nston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann,\n20\n\n\nand J. Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,\n2022. URL https://arxiv.org/abs/2204.05862.\nC. L. Baker and J. B. Tenenbaum.\nModeling Human Plan Recognition Using Bayesian Theory of Mind.\nIn\nG. Sukthankar, C. Geib, H. H. Bui, D. V. Pynadath, and R. P. Goldman, editors, Plan, Activity, and Intent\nRecognition, pages 177‚Äì204. Morgan Kaufmann, Boston, Jan. 2014. ISBN 978-0-12-398532-3. URL https:\n//www.sciencedirect.com/science/article/pii/B9780123985323000075.\nC. L. Baker, R. Saxe, and J. B. Tenenbaum. Action understanding as inverse planning. Cognition, 113(3):329 ‚Äì 349,\n2009. ISSN 0010-0277. doi: https://doi.org/10.1016/j.cognition.2009.07.005. URL http://www.sciencedirect.\ncom/science/article/pii/S0010027709001607.\nC. L. Baker, J. Jara-Ettinger, R. Saxe, and J. B. Tenenbaum.\nRational quantitative attribution of beliefs, desires\nand percepts in human mentalizing. Nature Human Behaviour, 1(4):0064, Mar. 2017. ISSN 2397-3374. doi:\n10.1038/s41562-017-0064. URL https://doi.org/10.1038/s41562-017-0064.\nC. Barattieri di San Pietro, F. Frau, V. Mangiaterra, and V. Bambini. The pragmatic profile of chatgpt: Assessing the\ncommunicative skills of a conversational agent. Sistemi intelligenti, 35(2):379‚Äì400, 2023.\nS. Baron-Cohen, A. M. Leslie, and U. Frith. Does the autistic child have a ‚Äútheory of mind‚Äù ?\nCognition, 21(1):\n37‚Äì46, Oct. 1985. ISSN 0010-0277. doi: 10.1016/0010-0277(85)90022-8. URL https://www.sciencedirect.\ncom/science/article/pii/0010027785900228.\nJ. Barrett. Finding Agents Everywhere. In Why Would Anyone Believe in God? 2004.\nC. Beaudoin, E. Leblanc, C. Gagner, and M. H. Beauchamp.\nSystematic Review and Inventory of Theory of\nMind Measures for Young Children.\nFrontiers in Psychology, 10, 2020.\nISSN 1664-1078.\nURL https:\n//www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.02905.\nW. Bechtel and A. Abrahamsen. Connectionism and the mind: An introduction to parallel processing in networks.\n1991.\nL. Bischetti, I. Ceccato, S. Lecce, E. Cavallini, and V. Bambini. Pragmatics and theory of mind in older adults‚Äô humor\ncomprehension. Current Psychology, June 2019. ISSN 1936-4733. doi: 10.1007/s12144-019-00295-w. URL\nhttps://doi.org/10.1007/s12144-019-00295-w.\nI. A. Blank. What are large language models supposed to model? Trends in Cognitive Sciences, 2023. ISSN 1364-\n6613. doi: 10.1016/j.tics.2023.08.006. URL https://doi.org/10.1016/j.tics.2023.08.006.\nF. M. Bosco, M. Tirassa, and I. Gabbatore. Why Pragmatics and Theory of Mind Do Not (Completely) Overlap.\nFrontiers in Psychology, 9:1453, 2018. ISSN 1664-1078. doi: 10.3389/fpsyg.2018.01453. URL https://www.\nfrontiersin.org/article/10.3389/fpsyg.2018.01453.\n21\n\n\nS. E. Brennan, A. Galati, and A. K. Kuhlen. Two minds, one dialog: Coordinating speaking and understanding. In\nPsychology of learning and motivation, pages 301‚Äì344. Elsevier, 2010.\nJ. R. Brown, N. Donelan-McCall, and J. Dunn. Why Talk about Mental States? The Significance of Children‚Äôs Con-\nversations with Friends, Siblings, and Mothers. Child Development, 67(3):836‚Äì849, June 1996. ISSN 0009-3920.\ndoi: 10.1111/j.1467-8624.1996.tb01767.x. URL https://doi.org/10.1111/j.1467-8624.1996.tb01767.x.\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg,\nH. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of Artificial General Intelligence: Early experiments with\nGPT-4, 2023. URL https://arxiv.org/abs/2303.12712.\nJ. Call and M. Tomasello. Does the Chimpanzee have a Theory of Mind? 30 years later. In S. Schleidgen, M. Jungert,\nR. Bauer, and V. Sandow, editors, Human Nature and Self Design, pages 83‚Äì96. Brill, Leiden, The Netherlands,\n2011. URL https://doi.org/10.30965/9783957438843_008.\nG. Carenini, L. Bodot, L. Bischetti, W. Schaeken, and V. Bambini. Large Language Models Behave (Almost) As Ra-\ntional Speech Actors: Insights From Metaphor Understanding. In NeurIPS 2023 workshop: Information-Theoretic\nPrinciples in Cognitive Systems, 2023. URL https://openreview.net/forum?id=SosbRhZLBV.\nN. Carlini, F. Tram√®r, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlings-\nson, A. Oprea, and C. Raffel. Extracting Training Data from Large Language Models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633‚Äì2650. USENIX Association, Aug. 2021. ISBN 978-1-939133-24-3.\nURL https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.\nS. M. Carlson, L. J. Moses, and H. R. Hix. The Role of Inhibitory Processes in Young Children‚Äôs Difficulties with\nDeception and False Belief. Child Development, 69(3):672‚Äì691, 1998. ISSN 00093920, 14678624. doi: 10.2307/\n1132197. URL http://www.jstor.org/stable/1132197.\nH. H. Clark and C. R. Marshall. Definite reference and mutual knowledge. In A. Joshi, B. Webber, and I. Sag, editors,\nElements of Discourse Understanding, pages 10‚Äì63. Cambridge University Press, 1981.\nH. H. Clark and D. Wilkes-Gibbs. Referring as a collaborative process. Cognition, 22(1):1‚Äì39, Feb. 1986. ISSN\n0010-0277. doi: 10.1016/0010-0277(86)90010-7. URL https://www.sciencedirect.com/science/article/\npii/0010027786900107.\nL. J. Cronbach and P. E. Meehl. Construct validity in psychological tests. Psychological Bulletin, 52(4), 1955.\nJ. G. de Villiers and P. A. de Villiers. The Role of Language in Theory of Mind Development. Topics in Language\nDisorders, 34(4), 2014. ISSN 0271-8294. URL https://journals.lww.com/topicsinlanguagedisorders/\nfulltext/2014/10000/the_role_of_language_in_theory_of_mind_development.5.aspx.\n22\n\n\nJ.\nDegen.\nThe\nRational\nSpeech\nAct\nFramework.\nAnnual\nReview\nof\nLinguistics,\n9(1):519‚Äì540,\n2023.\ndoi:\n10.1146/annurev-linguistics-031220-010811.\nURL\nhttps://doi.org/10.1146/\nannurev-linguistics-031220-010811.\nD. C. Dennett. The intentional stance. MIT press, 1989.\nJ. Dodge, M. Sap, A. Marasovi¬¥c, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner. Documenting\nLarge Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. In M.-F. Moens, X. Huang, L. Specia,\nand S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-\ning, pages 1286‚Äì1305, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.emnlp-main.98. URL https://aclanthology.org/2021.emnlp-main.98.\nL. Eliot. Theory Of Mind As An Emergent Property Of Generative AI Could Be A Linchpin For AI-Powered Mental\nHealth Advisement Apps.\nForbes, 2023.\nURL https://www.forbes.com/sites/lanceeliot/2023/12/20/\ntheory-of-mind-as-an-emergent-property-of-generative-ai-could-be-a-linchpin-for-ai-powered-mental-health-\nI. Enrici, B. G. Bara, and M. Adenzato. Theory of Mind, pragmatics and the brain: Converging evidence for the role\nof intention processing as a core feature of human communication. Pragmatics & Cognition, 26(1):5‚Äì38, 2019.\nISSN 0929-0907. doi: https://doi.org/10.1075/pc.19010.enr. URL https://www.jbe-platform.com/content/\njournals/10.1075/pc.19010.enr.\nC. Firestone. Performance vs. competence in human‚Äìmachine comparisons. Proceedings of the National Academy of\nSciences, 117(43):26562‚Äì26571, Oct. 2020. doi: 10.1073/pnas.1905334117. URL https://doi.org/10.1073/\npnas.1905334117.\nJ. H. Flavell, B. A. Everett, K. Croft, and E. R. Flavell. Young children‚Äôs knowledge about visual perception: Fur-\nther evidence for the Level 1‚ÄìLevel 2 distinction. Developmental Psychology, 17(1):99‚Äì103, 1981. ISSN 1939-\n0599(Electronic),0012-1649(Print). doi: 10.1037/0012-1649.17.1.99.\nS. Floyd, O. Jouravlev, Z. Mineroff, E. Gibson, and E. Fedorenko.\nA tripartite structure of pragmatic language\nabilities: Comprehension of social conventions, intonation processing, and causal reasoning, Sept. 2023. URL\nosf.io/preprints/psyarxiv/e2xta.\nM. C. Frank. Baby steps in evaluating the capacities of large language models. Nature Reviews Psychology, 2(8):\n451‚Äì452, Aug. 2023a. ISSN 2731-0574. doi: 10.1038/s44159-023-00211-x. URL https://doi.org/10.1038/\ns44159-023-00211-x.\nM. C. Frank. Openly accessible LLMs can help us to understand human cognition. Nature Human Behaviour, 7(11):\n1825‚Äì1827, Nov. 2023b. ISSN 2397-3374. doi: 10.1038/s41562-023-01732-4. URL https://doi.org/10.1038/\ns41562-023-01732-4.\nM. C. Frank and N. D. Goodman. Predicting Pragmatic Reasoning in Language Games. Science, 336(6084):998‚Äì998,\nMay 2012. doi: 10.1126/science.1218633. URL https://doi.org/10.1126/science.1218633.\n23\n\n\nG. Fu, W. S. Xiao, M. Killen, and K. Lee. Moral judgment and its relation to second-order theory of mind. Develop-\nmental Psychology, 50(8):2085‚Äì2092, Aug. 2014. ISSN 1939-0599 0012-1649. doi: 10.1037/a0037077.\nI.-N. Fu, K.-L. Chen, M.-R. Liu, D.-R. Jiang, C.-L. Hsieh, and S.-C. Lee. A systematic review of measures of theory of\nmind for children. Developmental Review, 67:101061, Mar. 2023. ISSN 0273-2297. doi: 10.1016/j.dr.2022.101061.\nURL https://www.sciencedirect.com/science/article/pii/S027322972200051X.\nG. G. Gallup Jr. Chimpanzees: self-recognition. Science, 167(3914):86‚Äì87, 1970.\nK. Gandhi, J.-P. Fr√§nken, T. Gerstenberg, and N. D. Goodman. Understanding Social Reasoning in Language Models\nwith Language Models, 2023. URL https://arxiv.org/abs/2306.15448. _eprint: 2306.15448.\nE.\nGent.\nUnderstanding\nhuman\nintentions\nwill\nbe\nthe\nnext\nbig\nbreakthrough\nin\nAI.\nNewScientist,\n2023.\nURL\nhttps://www.newscientist.com/article/\nmg25734260-500-understanding-human-intentions-will-be-the-next-big-breakthrough-in-ai/.\nG. Gergely, Z. N√°dasdy, G. Csibra, and S. B√≠r√≥.\nTaking the intentional stance at 12 months of age.\nCog-\nnition, 56(2):165‚Äì193, Aug. 1995.\nISSN 0010-0277.\ndoi: 10.1016/0010-0277(95)00661-H.\nURL https:\n//www.sciencedirect.com/science/article/pii/001002779500661H.\nN. D. Goodman and M. C. Frank. Pragmatic Language Interpretation as Probabilistic Inference. Trends in Cognitive\nSciences, 20(11):818‚Äì829, 2016. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2016.08.005. URL https:\n//www.sciencedirect.com/science/article/pii/S136466131630122X.\nA. Gopnik and A. Meltzoff. Imitation, cultural learning and the origins of ‚Äútheory of mind‚Äù. Behavioral and Brain\nSciences, 16(3):521‚Äì523, 1993.\nISSN 0140-525X.\ndoi: 10.1017/S0140525X00031368.\nURL https://www.\ncambridge.org/core/product/7BDD2376D6069A959D47E03B9B9DEE0B.\nJ. Gordon and B. Van Durme. Reporting bias and knowledge acquisition. In Proceedings of the 2013 Workshop on\nAutomated Knowledge Base Construction, AKBC ‚Äô13, pages 25‚Äì30, New York, NY, USA, 2013. Association for\nComputing Machinery. ISBN 978-1-4503-2411-3. doi: 10.1145/2509558.2509563. URL https://doi.org/10.\n1145/2509558.2509563.\nH. P. Grice. Logic and conversation. In Speech acts, pages 41‚Äì58. Brill, 1975.\nJ. Guo, B. Yang, P. Yoo, B. Y. Lin, Y. Iwasawa, and Y. Matsuo. Suspicion agent: Playing imperfect information games\nwith theory of mind aware GPT-4. In First Conference on Language Modeling, 2024. URL https://openreview.\nnet/forum?id=F2yGbwXJAi.\nN. Gurney, D. V. Pynadath, and V. Ustun. Spontaneous Theory of Mind for Artificial Intelligence, 2024.\nURL\nhttps://arxiv.org/abs/2402.13272.\n24\n\n\nC. M. Hale and H. Tager-Flusberg. The influence of language on theory of mind: A training study. Developmental\nScience, 6(3):346‚Äì359, June 2003. ISSN 1363-755X. doi: 10.1111/1467-7687.00289. URL https://doi.org/\n10.1111/1467-7687.00289. Publisher: John Wiley & Sons, Ltd.\nB. Hare, J. Call, and M. Tomasello. Do chimpanzees know what conspecifics know?\nAnimal Behaviour, 61(1):\n139‚Äì151, Jan. 2001. ISSN 0003-3472. doi: 10.1006/anbe.2000.1518. URL https://www.sciencedirect.com/\nscience/article/pii/S0003347200915185.\nY. He, Y. Wu, Y. Jia, R. Mihalcea, Y. Chen, and N. Deng. Hi-tom: A benchmark for evaluating higher-order theory of\nmind reasoning in large language models. arXiv preprint arXiv:2310.16755, 2023.\nF. Heider and M. Simmel. An Experimental Study of Apparent Behavior. The American Journal of Psychology, 57\n(2):243‚Äì259, 1944. ISSN 00029556. doi: 10.2307/1416950. URL http://www.jstor.org/stable/1416950.\nPublisher: University of Illinois Press.\nC. M. Heyes. Theory of mind in nonhuman primates. Behavioral and brain sciences, 21(1):101‚Äì114, 1998.\nM. K. Ho, F. Cushman, M. L. Littman, and J. L. Austerweil. Communication in action: Planning and interpreting\ncommunicative demonstrations. Journal of experimental psychology. General, 150(11):2246‚Äì2272, Nov. 2021.\nISSN 1939-2222 0022-1015. doi: 10.1037/xge0001035.\nM. K. Ho, R. Saxe, and F. Cushman. Planning with Theory of Mind. Trends in Cognitive Sciences, 2022. ISSN\n1364-6613. doi: https://doi.org/10.1016/j.tics.2022.08.003. URL https://www.sciencedirect.com/science/\narticle/pii/S1364661322001851.\nB. Holterman and K. v. Deemter. Does ChatGPT have Theory of Mind?, 2023. URL https://arxiv.org/abs/\n2305.14020.\nY. K. Hsu and H. Cheung. Two mentalizing capacities and the understanding of two types of lie telling in children.\nDevelopmental Psychology, 49:1650‚Äì1659, 2013. doi: 10.1037/a0031128. URL https://doi.org/10.1037/\na0031128.\nJ. Hu and M. C. Frank. Auxiliary task demands mask the capabilities of smaller language models, 2024. URL\nhttps://arxiv.org/abs/2404.02418.\nJ. Hu, S. Floyd, O. Jouravlev, E. Fedorenko, and E. Gibson. A fine-grained comparison of pragmatic language un-\nderstanding in humans and language models. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 4194‚Äì4213, Toronto, Canada, July 2023. Association\nfor Computational Linguistics. URL https://aclanthology.org/2023.acl-long.230.\nA. A. Ivanova. How to evaluate the cognitive abilities of LLMs. Nature Human Behaviour, 9(2):230‚Äì233, Feb. 2025.\nISSN 2397-3374. doi: 10.1038/s41562-024-02096-z. URL https://doi.org/10.1038/s41562-024-02096-z.\n25\n\n\nA. A. Ivanova, A. Sathe, B. Lipkin, U. Kumar, S. Radkani, T. H. Clark, C. Kauf, J. Hu, R. T. Pramod, G. Grand,\nV. Paulun, M. Ryskina, E. Akyurek, E. Wilcox, N. Rashid, L. Choshen, R. Levy, E. Fedorenko, J. Tenenbaum, and\nJ. Andreas. Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world\nknowledge in language models, 2024. URL https://arxiv.org/abs/2405.09605.\nN. Jacoby and E. Fedorenko. Discourse-level comprehension engages medial frontal Theory of Mind brain regions\neven for expository texts. Language, Cognition and Neuroscience, 35(6):780‚Äì796, 2020. doi: 10.1080/23273798.\n2018.1525494. URL https://doi.org/10.1080/23273798.2018.1525494.\nA. Jacovi, A. Caciularu, O. Goldman, and Y. Goldberg. Stop Uploading Test Data in Plain Text: Practical Strategies\nfor Mitigating Data Contamination by Evaluation Benchmarks.\nIn H. Bouamor, J. Pino, and K. Bali, editors,\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5075‚Äì5084,\nSingapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.308. URL\nhttps://aclanthology.org/2023.emnlp-main.308.\nM. Jamali, Z. M. Williams, and J. Cai. Unveiling Theory of Mind in Large Language Models: A Parallel to Single\nNeurons in the Human Brain, 2023. URL https://arxiv.org/abs/2309.01660.\nJ. Jara-Ettinger.\nTheory of mind as inverse reinforcement learning.\nCurrent Opinion in Behavioral Sciences,\n29:105‚Äì110, 2019. ISSN 2352-1546. doi: https://doi.org/10.1016/j.cobeha.2019.04.010. URL https://www.\nsciencedirect.com/science/article/pii/S2352154618302055.\nP. Jeretic, A. Warstadt, S. Bhooshan, and A. Williams. Are Natural Language Inference Models IMPPRESsive?\nLearning IMPlicature and PRESupposition. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 8690‚Äì8705, Online, July 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.acl-main.768. URL https://aclanthology.org/2020.acl-main.768.\nM. Jian and S. N. Are LLMs good pragmatic speakers? In NeurIPS 2024 Workshop on Behavioral Machine Learning,\n2024. URL https://openreview.net/forum?id=htFtNdgWdf.\nC. Jin, Y. Wu, J. Cao, J. Xiang, Y.-L. Kuo, Z. Hu, T. Ullman, A. Torralba, J. B. Tenenbaum, and T. Shu. MMToM-QA:\nMultimodal Theory of Mind Question Answering. In Proceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics, 2024. URL https://arxiv.org/abs/2401.08743.\nC. R. Jones, S. Trott, and B. Bergen. Comparing humans and large language models on an experimental protocol\ninventory for theory of mind evaluation (EPITOME). Transactions of the Association for Computational Linguistics,\n12:803‚Äì819, 2024.\nJ. Kaminski, J. Call, and M. Tomasello. Chimpanzees know what others know, but not what they believe. Cog-\nnition, 109(2):224‚Äì234, Nov. 2008.\nISSN 0010-0277.\ndoi: 10.1016/j.cognition.2008.08.010.\nURL https:\n//www.sciencedirect.com/science/article/pii/S0010027708001881.\n26\n\n\nF. Kano, C. Krupenye, S. Hirata, M. Tomonaga, and J. Call. Great apes use self-experience to anticipate an agent‚Äôs\naction in a false-belief test. Proceedings of the National Academy of Sciences, 116(42):20904‚Äì20909, 2019.\nJ. Kiley Hamlin, T. Ullman, J. Tenenbaum, N. Goodman, and C. Baker. The mentalistic basis of core social cognition:\nExperiments in preverbal infants and a computational model. Developmental Science, 16(2):209‚Äì226, Mar. 2013.\nISSN 1363-755X. doi: 10.1111/desc.12017. URL https://doi.org/10.1111/desc.12017.\nB. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, and R. Sayres. Interpretability beyond feature attri-\nbution: Quantitative testing with concept activation vectors (tcav), 2018. URL https://arxiv.org/abs/1711.\n11279.\nH. Kim, M. Sclar, X. Zhou, R. L. Bras, G. Kim, Y. Choi, and M. Sap. FANToM: A Benchmark for Stress-testing\nMachine Theory of Mind in Interactions, 2023. URL https://arxiv.org/abs/2310.15421.\nM. Kline Struhl, J. Gall√©e, Z. Balewski, and E. Fedorenko. Understanding jokes draws most heavily on the Theory of\nMind brain network, 2018. URL psyarxiv.com/h2nyx.\nM. Kosinski. Evaluating large language models in theory of mind tasks. Proceedings of the National Academy of\nSciences, 121(45):e2405460121, Nov. 2024. doi: 10.1073/pnas.2405460121. URL https://doi.org/10.1073/\npnas.2405460121. Publisher: Proceedings of the National Academy of Sciences.\nC. Krupenye, F. Kano, S. Hirata, J. Call, and M. Tomasello. Great apes anticipate that other individuals will act\naccording to false beliefs. Science, 354(6308):110‚Äì114, Oct. 2016. doi: 10.1126/science.aaf8110. URL https:\n//doi.org/10.1126/science.aaf8110.\nM. Krych-Appelbaum, J. B. Law, D. Jones, A. Barnacz, A. Johnson, and J. P. Keenan. ‚ÄúI think I know what you mean‚Äù:\nThe role of theory of mind in collaborative communication. Interaction Studies, 8(2):267‚Äì280, 2007. ISSN 1572-\n0373. doi: https://doi.org/10.1075/is.8.2.05kry. URL https://www.jbe-platform.com/content/journals/10.\n1075/is.8.2.05kry. Publisher: John Benjamins Type: Journal Article.\nA. K. Lampinen. Can language models handle recursively nested grammatical structures? A case study on comparing\nmodels and humans, 2023. URL https://arxiv.org/abs/2210.15303.\nA. K. Lampinen, I. Dasgupta, S. C. Y. Chan, H. R. Sheahan, A. Creswell, D. Kumaran, J. L. McClelland, and F. Hill.\nLanguage models, like humans, show content effects on reasoning tasks. PNAS Nexus, 3(7):pgae233, July 2024.\nISSN 2752-6542. doi: 10.1093/pnasnexus/pgae233. URL https://doi.org/10.1093/pnasnexus/pgae233.\nA. M. Leslie, J. Knobe, and A. Cohen. Acting Intentionally and the Side-Effect Effect: Theory of Mind and Moral\nJudgment. Psychological Science, 17(5):421‚Äì427, May 2006. ISSN 0956-7976. doi: 10.1111/j.1467-9280.2006.\n01722.x. URL https://doi.org/10.1111/j.1467-9280.2006.01722.x.\n27\n\n\nC. Lewis and A. Osborne. Three-Year-Olds‚Äô Problems with False Belief: Conceptual Deficit or Linguistic Artifact?\nChild Development, 61(5):1514‚Äì1519, 1990. ISSN 00093920, 14678624. doi: 10.2307/1130760. URL http:\n//www.jstor.org/stable/1130760.\nZ. Ma, J. Sansom, R. Peng, and J. Chai. Towards A Holistic Landscape of Situated Theory of Mind in Large Language\nModels. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2023, pages 1011‚Äì1031, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.findings-emnlp.72. URL https://aclanthology.org/2023.findings-emnlp.72.\nI. Magar and R. Schwartz. Data contamination: From memorization to exploitation. In S. Muresan, P. Nakov, and\nA. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 157‚Äì165, Dublin, Ireland, May 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-short.18. URL https://aclanthology.org/2022.acl-short.18.\nB. F. Malle and J. Holbrook. Is there a hierarchy of social inferences? The likelihood and speed of inferring inten-\ntionality, mind, and personality. Journal of Personality and Social Psychology, 102(4):661‚Äì684, Apr. 2012. ISSN\n1939-1315 0022-3514. doi: 10.1037/a0026790.\nZ. S. Masangkay, K. A. McCluskey, C. W. McIntyre, J. Sims-Knight, B. E. Vaughn, and J. H. Flavell. The Early\nDevelopment of Inferences about the Visual Percepts of Others. Child Development, 45(2):357‚Äì366, 1974. ISSN\n00093920, 14678624. doi: 10.2307/1127956. URL http://www.jstor.org/stable/1127956.\nR. T. McCoy, R. Frank, and T. Linzen. Revisiting the poverty of the stimulus: hierarchical generalization without\na hierarchical bias in recurrent neural networks. In Proceedings of the Annual Meeting of the Cognitive Science\nSociety, 2018. URL https://arxiv.org/abs/1802.09091.\nR. T. McCoy, S. Yao, D. Friedman, M. D. Hardy, and T. L. Griffiths. Embers of autoregression show how large\nlanguage models are shaped by the problem they are trained to solve. Proceedings of the National Academy of\nSciences, 121(41):e2322420121, Oct. 2024. doi: 10.1073/pnas.2322420121. URL https://doi.org/10.1073/\npnas.2322420121. Publisher: Proceedings of the National Academy of Sciences.\nT. McCoy, E. Pavlick, and T. Linzen. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Lan-\nguage Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n3428‚Äì3448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1334.\nURL https://aclanthology.org/P19-1334.\nA. N. Meltzoff. Social Cognition and the Origins of Imitation, Empathy, and Theory of Mind. In The Wiley-Blackwell\nHandbook of Childhood Cognitive Development, pages 49‚Äì75. Aug. 2010. ISBN 978-1-4443-2548-5. doi: 10.\n1002/9781444325485.ch2. URL https://doi.org/10.1002/9781444325485.ch2.\nJ. Merullo, C. Eickhoff, and E. Pavlick. Circuit component reuse across tasks in transformer language models. arXiv\npreprint arXiv:2310.08744, 2023.\n28\n\n\nK. Milligan, J. W. Astington, and L. A. Dack. Language and Theory of Mind: Meta-Analysis of the Relation Between\nLanguage Ability and False-belief Understanding. Child Development, 78(2):622‚Äì646, Mar. 2007. ISSN 0009-\n3920. doi: 10.1111/j.1467-8624.2007.01018.x. URL https://doi.org/10.1111/j.1467-8624.2007.01018.x.\nPublisher: John Wiley & Sons, Ltd.\nK. Misra and K. Mahowald. Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the\nMissing AANNs, 2024. URL https://arxiv.org/abs/2403.19827.\nS. R. Moghaddam and C. J. Honey. Boosting Theory-of-Mind Performance in Large Language Models via Prompting,\n2023. URL https://arxiv.org/abs/2304.11490.\nJ. M. Moran, L. L. Young, R. Saxe, S. M. Lee, D. O‚ÄôYoung, P. L. Mavros, and J. D. Gabrieli. Impaired theory of\nmind for moral judgment in high-functioning autism. Proceedings of the National Academy of Sciences, 108(7):\n2688‚Äì2692, Feb. 2011. doi: 10.1073/pnas.1011734108. URL https://doi.org/10.1073/pnas.1011734108.\nK. X. Nguyen. Language Models are Bounded Pragmatic Speakers. In First Workshop on Theory of Mind in Commu-\nnicating Agents, 2023. URL https://openreview.net/forum?id=kEdFOtiZ5C.\nnostalgebraist.\ninterpreting gpt:\nthe logit\nlens,\n2020.\nURL https://www.lesswrong.com/posts/\nAcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.\nC. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. Distill, 2(11):e7, 2017.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\nJ. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and\nR. Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, vol-\nume 35, pages 27730‚Äì27744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\nL. Pacchiardi, M. Tesic, L. G. Cheke, and J. Hern√°ndez-Orallo. Leaving the barn door open for Clever Hans: Simple\nfeatures predict LLM benchmark answers, 2024. URL https://arxiv.org/abs/2410.11672.\nD. Premack and G. Woodruff. Does the chimpanzee have a theory of mind? Behavioral and Brain Sciences, 1(4):\n515‚Äì526, 1978. ISSN 0140-525X. doi: 10.1017/S0140525X00076512. URL https://www.cambridge.org/\ncore/product/1E96B02CD9850016B7C93BC6D2FEF1D0.\nJ. E. Pyers and A. Senghas. Language promotes false-belief understanding: Evidence from learners of a new sign lan-\nguage. Psychological Science, 20(7):805‚Äì812, July 2009. ISSN 1467-9280 0956-7976. doi: 10.1111/j.1467-9280.\n2009.02377.x. Place: United States.\n29\n\n\nF. Quesque and Y. Rossetti. What Do Theory-of-Mind Tasks Actually Measure? Theory and Practice. Perspectives\non Psychological Science, 15(2):384‚Äì396, Mar. 2020. ISSN 1745-6916. doi: 10.1177/1745691619896607. URL\nhttps://doi.org/10.1177/1745691619896607. Publisher: SAGE Publications Inc.\nA. Royka and L. R. Santos. Theory of mind in the wild. Current Opinion in Behavioral Sciences, 45:101137, 2022.\nP. Rubio-Fern√°ndez. Theory of Mind. In C. Cummins and N. Katsos, editors, The Oxford Handbook of Experimental\nSemantics and Pragmatics, page 0. Oxford University Press, Mar. 2019. ISBN 978-0-19-879176-8. doi: 10.1093/\noxfordhb/9780198791768.013.23. URL https://doi.org/10.1093/oxfordhb/9780198791768.013.23.\nP. Rubio-Fern√°ndez, F. Mollica, M. Oraa Ali, and E. Gibson. How do you know that? Automatic belief inferences in\npassing conversation. Cognition, 193:104011, Dec. 2019. ISSN 0010-0277. doi: 10.1016/j.cognition.2019.104011.\nURL https://www.sciencedirect.com/science/article/pii/S0010027719301842.\nT. Ruffman, L. Slade, and E. Crowe. The Relation between Children‚Äôs and Mothers‚Äô Mental State Language and\nTheory-of-Mind Understanding. Child Development, 73(3):734‚Äì751, May 2002. ISSN 0009-3920. doi: 10.1111/\n1467-8624.00435. URL https://doi.org/10.1111/1467-8624.00435.\nL. E. Ruis, A. Khan, S. Biderman, S. Hooker, T. Rockt√§schel, and E. Grefenstette. The Goldilocks of Pragmatic\nUnderstanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=5bWW9Eop7l.\nD. Sally and E. Hill. The development of interpersonal strategy: Autism, theory-of-mind, cooperation and fairness.\nEconomic Socialization, 27(1):73‚Äì97, Feb. 2006. ISSN 0167-4870. doi: 10.1016/j.joep.2005.06.015. URL https:\n//www.sciencedirect.com/science/article/pii/S0167487005000565.\nM. Sap, R. Le Bras, D. Fried, and Y. Choi. Neural Theory-of-Mind? On the Limits of Social Intelligence in Large\nLMs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762‚Äì\n3780, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https:\n//aclanthology.org/2022.emnlp-main.248.\nR. Saxe. The happiness of the fish: Evidence for a common theory of one‚Äôs own and others‚Äô actions. In Handbook of\nimagination and mental simulation, pages 257‚Äì266. Psychology Press, 2012.\nR. Saxe, J. Tenenbaum, and S. Carey. Secret Agents: Inferences About Hidden Causes by 10- and 12-Month-Old\nInfants. Psychological Science, 16(12):995‚Äì1001, Dec. 2005. ISSN 0956-7976. doi: 10.1111/j.1467-9280.2005.\n01649.x. URL https://doi.org/10.1111/j.1467-9280.2005.01649.x.\nB. J. Scholl and T. Gao. Perceiving animacy and intentionality: Visual processing or higher-level judgment. Social\nperception: Detection and interpretation of animacy, agency, and intention, 4629:197‚Äì229, 2013.\nM. Sclar, S. Kumar, P. West, A. Suhr, Y. Choi, and Y. Tsvetkov. Minding Language Models‚Äô (Lack of) Theory of\nMind: A Plug-and-Play Multi-Character Belief Tracker. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors,\n30\n\n\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 13960‚Äì13980, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.acl-long.780. URL https://aclanthology.org/2023.acl-long.780.\nP. Setoh, R. M. Scott, and R. Baillargeon. Two-and-a-half-year-olds succeed at a traditional false-belief task with\nreduced processing demands. Proceedings of the National Academy of Sciences, 113(47):13360‚Äì13365, Nov. 2016.\ndoi: 10.1073/pnas.1609203113. URL https://doi.org/10.1073/pnas.1609203113.\nN. Shapira, M. Levy, S. H. Alavi, X. Zhou, Y. Choi, Y. Goldberg, M. Sap, and V. Shwartz. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models. In Y. Graham and M. Purver, editors,\nProceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2257‚Äì2273, St. Julian‚Äôs, Malta, Mar. 2024. Association for Computational Linguistics.\nURL https://aclanthology.org/2024.eacl-long.138.\nT. Shu, A. Bhandwaldar, C. Gan, K. A. Smith, S. Liu, D. Gutfreund, E. Spelke, J. B. Tenenbaum, and T. D. Ullman.\nAGENT: A Benchmark for Core Psychological Reasoning. In Proceedings of the 38th International Conference on\nMachine Learning, 2021. URL https://arxiv.org/abs/2102.12321.\nC. Singh, J. P. Inala, M. Galley, R. Caruana, and J. Gao. Rethinking interpretability in the era of large language models.\narXiv preprint arXiv:2402.01761, 2024.\nV. Slaughter and C. C. Peterson. How conversational input shapes theory of mind development in infancy and early\nchildhood. In M. Siegal and L. Surian, editors, Access to Language and Cognitive Development, page 0. Oxford\nUniversity Press, Dec. 2011. ISBN 978-0-19-959272-2. doi: 10.1093/acprof:oso/9780199592722.003.0001. URL\nhttps://doi.org/10.1093/acprof:oso/9780199592722.003.0001.\nK. Smith. The cognitive prerequisites for language: Insights from iterated learning. The Evolution of Language, 21:\n154‚Äì160, June 2018. ISSN 2352-1546. doi: 10.1016/j.cobeha.2018.05.003. URL https://www.sciencedirect.\ncom/science/article/pii/S235215461730178X.\nF. A. Sosa, T. Ullman, J. B. Tenenbaum, S. J. Gershman, and T. Gerstenberg.\nMoral dynamics: Grounding\nmoral judgment in intuitive physics and intuitive psychology. Cognition, 217:104890, Dec. 2021. ISSN 0010-\n0277. doi: 10.1016/j.cognition.2021.104890. URL https://www.sciencedirect.com/science/article/pii/\nS0010027721003139.\nD. Sperber and D. Wilson.\nRelevance:\nCommunication and Cognition.\nWiley-Blackwell, 1986.\nISBN\n978-0-631-19878-9. URL https://monoskop.org/images/e/e6/Sperber_Dan_Wilson_Deirdre_Relevance_\nCommunica_and_Cognition_2nd_edition_1996.pdf.\nN. Spotorno, E. Koun, J. Prado, J.-B. Van Der Henst, and I. A. Noveck.\nNeural evidence that utterance-\nprocessing entails mentalizing:\nThe case of irony.\nNeuroImage, 63(1):25‚Äì39, Oct. 2012.\nISSN 1053-\n31\n\n\n8119.\ndoi: 10.1016/j.neuroimage.2012.06.046.\nURL https://www.sciencedirect.com/science/article/\npii/S1053811912006611.\nS. Sravanthi, M. Doshi, P. Tankala, R. Murthy, R. Dabre, and P. Bhattacharyya. PUB: A Pragmatics Understanding\nBenchmark for Assessing LLMs‚Äô Pragmatics Capabilities. In L.-W. Ku, A. Martins, and V. Srikumar, editors,\nFindings of the Association for Computational Linguistics: ACL 2024, pages 12075‚Äì12097, Bangkok, Thailand,\nAug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.719. URL https:\n//aclanthology.org/2024.findings-acl.719.\nS. Stacy, S. Gong, A. Parab, M. Zhao, K. Jiang, and T. Gao. A Bayesian theory of mind approach to modeling\ncooperation and communication. WIREs Computational Statistics, 16(1):e1631, Jan. 2024. ISSN 1939-5108. doi:\n10.1002/wics.1631. URL https://doi.org/10.1002/wics.1631.\nJ. W. A. Strachan, D. Albergo, G. Borghini, O. Pansardi, E. Scaliti, S. Gupta, K. Saxena, A. Rufo, S. Panzeri, G. Manzi,\nM. S. A. Graziano, and C. Becchio. Testing theory of mind in large language models and humans. Nature Human\nBehaviour, May 2024. ISSN 2397-3374. doi: 10.1038/s41562-024-01882-z. URL https://doi.org/10.1038/\ns41562-024-01882-z.\nW. Street. Llm theory of mind and alignment: Opportunities and risks. arXiv preprint arXiv:2405.08154, 2024.\nW. Street, J. O. Siy, G. Keeling, A. Baranes, B. Barnett, M. McKibben, T. Kanyere, A. Lentz, B. A. y. Arcas, and\nR. I. M. Dunbar. LLMs achieve adult human performance on higher-order theory of mind tasks, 2024. URL\nhttps://arxiv.org/abs/2405.18870.\nL. Surian and A. M. Leslie. Competence and performance in false belief understanding: A comparison of autistic\nand normal 3-year-old children. British Journal of Developmental Psychology, 17(1):141‚Äì155, Mar. 1999. ISSN\n0261-510X. doi: 10.1348/026151099165203. URL https://doi.org/10.1348/026151099165203.\nW. Tang and V. Belle. Tom-lm: Delegating theory of mind reasoning to external symbolic executors in large language\nmodels, 2024. URL https://arxiv.org/abs/2404.15515.\nM. Tomasello. Do Apes Ape?\nIn C. M. Heyes and B. G. Galef, editors, Social Learning in Animals, pages 319‚Äì\n346. Academic Press, San Diego, Jan. 1996. ISBN 978-0-12-273965-1. URL https://www.sciencedirect.com/\nscience/article/pii/B9780122739651500169.\nM. Tomasello. How children come to understand false beliefs: A shared intentionality account. Proceedings of the\nNational Academy of Sciences, 115(34):8491‚Äì8498, Aug. 2018. doi: 10.1073/pnas.1804761115. URL https:\n//doi.org/10.1073/pnas.1804761115.\nM. Tomasello, S. Savage-Rumbaugh, and A. C. Kruger. Imitative Learning of Actions on Objects by Children, Chim-\npanzees, and Enculturated Chimpanzees. Child Development, 64(6):1688‚Äì1705, 1993. ISSN 00093920, 14678624.\ndoi: 10.2307/1131463. URL http://www.jstor.org/stable/1131463.\n32\n\n\nS. Trott, C. Jones, T. Chang, J. Michaelov, and B. Bergen. Do Large Language Models Know What Humans Know?\nCognitive Science, 47(7):e13309, July 2023. ISSN 0364-0213. doi: 10.1111/cogs.13309. URL https://doi.org/\n10.1111/cogs.13309. Publisher: John Wiley & Sons, Ltd.\nT. Ullman. Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks, 2023. URL https://arxiv.\norg/abs/2302.08399.\nM. van Duijn, B. van Dijk, T. Kouwenhoven, W. de Valk, M. Spruit, and P. van der Putten. Theory of Mind in Large\nLanguage Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced\nTests. In J. Jiang, D. Reitter, and S. Deng, editors, Proceedings of the 27th Conference on Computational Natural\nLanguage Learning (CoNLL), pages 389‚Äì402, Singapore, Dec. 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.conll-1.25. URL https://aclanthology.org/2023.conll-1.25.\nK. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect\nobject identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.\nO. Whang. Can a Machine Know That We Know What It Knows?\nThe New York Times, 2023. URL https:\n//www.nytimes.com/2023/03/27/science/ai-machine-learning-chatbots.html.\nA. Wilf, S. Lee, P. P. Liang, and L.-P. Morency. Think Twice: Perspective-Taking Improves Large Language Mod-\nels‚Äô Theory-of-Mind Capabilities. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8292‚Äì8308,\nBangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.451.\nURL https://aclanthology.org/2024.acl-long.451.\nM. Woensdregt and K. Smith. Pragmatics and Language Evolution. Mar. 2017. doi: 10.1093/acrefore/9780199384655.\n013.321.\nURL https://oxfordre.com/linguistics/view/10.1093/acrefore/9780199384655.001.0001/\nacrefore-9780199384655-e-321.\nA. Yedetore, T. Linzen, R. Frank, and R. T. McCoy. How poor is the stimulus? Evaluating hierarchical generalization\nin neural networks trained on child-directed speech, 2023. URL https://arxiv.org/abs/2301.11462.\nL. Young, F. Cushman, M. Hauser, and R. Saxe. The neural basis of the interaction between theory of mind and moral\njudgment. Proceedings of the National Academy of Sciences, 104(20):8235‚Äì8240, May 2007. doi: 10.1073/pnas.\n0701408104. URL https://doi.org/10.1073/pnas.0701408104.\nA. Zamir, A. Sax, W. Shen, L. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling task transfer learning,\n2018.\nZ. Zheng, S. Qiu, L. Fan, Y. Zhu, and S.-C. Zhu. GRICE: A Grammar-based Dataset for Recovering Implicature and\nConversational rEasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages\n2074‚Äì2085, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.\n182. URL https://aclanthology.org/2021.findings-acl.182.\n33\n\n\nP. Zhou, A. Madaan, S. P. Potharaju, A. Gupta, K. R. McKee, A. Holtzman, J. Pujara, X. Ren, S. Mishra, A. Ne-\nmatzadeh, S. Upadhyay, and M. Faruqui. How FaR Are Large Language Models From Agents with Theory-of-\nMind?, 2023. URL https://arxiv.org/abs/2310.03051.\n34\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21098v1.pdf",
    "total_pages": 34,
    "title": "Re-evaluating Theory of Mind evaluation in large language models",
    "authors": [
      "Jennifer Hu",
      "Felix Sosa",
      "Tomer Ullman"
    ],
    "abstract": "The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}