{
  "id": "arxiv_2502.20084v1",
  "text": "JOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n1\nMinds on the Move: Decoding Trajectory Prediction\nin Autonomous Driving with Cognitive Insights\nHaicheng Liao∗, Chengyue Wang∗, Kaiqun Zhu, Yilong Ren, Bolin Gao, Shengbo Eben Li, Senior Member,\nIEEE, Chengzhong Xu, Fellow, IEEE and Zhenning Li†\nAbstract—In mixed autonomous driving environments, accu-\nrately predicting the future trajectories of surrounding vehicles\nis crucial for the safe operation of autonomous vehicles (AVs).\nIn driving scenarios, a vehicle’s trajectory is determined by the\ndecision-making process of human drivers. However, existing\nmodels primarily focus on the inherent statistical patterns in\nthe data, often neglecting the critical aspect of understanding\nthe decision-making processes of human drivers. This oversight\nresults in models that fail to capture the true intentions of\nhuman drivers, leading to suboptimal performance in long-term\ntrajectory prediction. To address this limitation, we introduce\na Cognitive-Informed Transformer (CITF) that incorporates a\ncognitive concept, Perceived Safety, to interpret drivers’ decision-\nmaking mechanisms. Perceived Safety encapsulates the varying\nrisk tolerances across drivers with different driving behaviors.\nSpecifically, we develop a Perceived Safety-aware Module that\nincludes a Quantitative Safety Assessment for measuring the\nsubject risk levels within scenarios, and Driver Behavior Profiling\nfor characterizing driver behaviors. Furthermore, we present a\nnovel module, Leanformer, designed to capture social interactions\namong vehicles. CITF demonstrates significant performance\nimprovements on three well-established datasets. In terms of\nlong-term prediction, it surpasses existing benchmarks by 12.0%\non the NGSIM, 28.2% on the HighD, and 20.8% on the MoCAD\ndataset. Additionally, its robustness in scenarios with limited or\nmissing data is evident, surpassing most state-of-the-art (SOTA)\nbaselines, and paving the way for real-world applications.\nIndex Terms—Autonomous Driving, Trajectory Prediction,\nPerceived Safety, Mixed Autonomy Traffic, Cognitive Modeling\nI. INTRODUCTION\nI\nN the evolving landscape of autonomous driving (AD)\nsystems, the complex interactions between autonomous\nvehicles (AVs) and human-driven vehicles (HVs) present a\nsignificant challenge to achieving accurate trajectory predic-\ntion [1], [2]. The future trajectory of human-driven vehicles\nis essentially the result of the human driver’s decision-making\nprocess [3], [4]. Since human drivers require reaction time to\nadjust their behavior when facing changes in the external envi-\nronment [5]–[7], the dynamics of the vehicle will not change\n† Corresponding author; * Authors contributed equally.\nHaicheng Liao, Chengyue Wang, Kaiqun Zhu, Chengzhong Xu, and\nZhenning Li are with the State Key Laboratory of Internet of Things for\nSmart City, University of Macau, Macau. Yilong Ren is with the School of\nTransportation Science and Engineering, Beihang University, Beijing, China.\nBolin Gao and Shengbo Eben Li are with the School of Vehicle and Mobility,\nTsinghua University, Beijing, China. E-mails: zhenningli@um.edu.mo. This\nresearch is supported by the State Key Lab of Intelligent Transportation\nSystem under Project (2024-B001), Science and Technology Development\nFund of Macau SAR (File no. 0021/2022/ITP, 0081/2022/A2, 001/2024/SKL),\nShenzhen-Hong Kong-Macau Science and Technology Program Category C\n(SGDX20230821095159012), and University of Macau (SRG2023-00037-\nIOTSC).\ndrastically in the short term, making short-term (≤2 seconds)\npredictions relatively straightforward. Nevertheless, long-term\nprediction necessitates models that accurately estimate the\nimpact of numerous factors on the decision-making process of\nhuman drivers, a feat that is particularly challenging to achieve\n[8]. Recent advancements in algorithms and the availability\nof driving datasets have led to significant breakthroughs in\ntrajectory prediction [9]. However, the accuracy of long-term\npredictions (i.e., >2 seconds) remains a persistent challenge,\nprimarily due to the inherent complexity of real-world driving\nscenarios. These challenges stem from the complex inter-\nactions between traffic agents, the impact of environmental\nfactors like weather and road conditions, and the unpredictable\nnature of human driver behavior. These factors introduce\nsignificant uncertainty, making reliable long-term forecasts a\npersistent struggle for researchers in the field.\nThis backdrop prompts us to ask critical questions about the\nfuture trajectory of AD: Is the key to advancing AD not just in\naccumulating more data or refining algorithms, but in gaining a\ndeeper understanding of the driving environment itself? How\ncan we reshape our models to interpret and respond to the\nintricate human dynamics that underpin driving? Motivated\nby these questions, our research embarks on an innovative\npath. We propose a paradigm shift, extending beyond conven-\ntional data-driven approaches to embrace a critical yet often-\nneglected aspect of driving – the concept of perceived safety.\nThis concept, pivotal in shaping driving behaviors and\ndecisions, is deeply rooted in psychological constructs, as\ndetailed in [10]. According to the Theory of Planned Behav-\nior, individual actions in driving are influenced by attitudes\n(driving behaviors towards others), subjective norms (personal\nevaluation of safety), and perceived behavioral control (con-\nfidence in driving ability) [11]. Further depth is added by\nneuroscientific research, such as studies by [12], [13] and\n[14], which unveil that perceived safety is an intricate blend\nof both conscious and instinctive responses, involving the\namygdala’s emotional processing and the prefrontal cortex’s\nrational decision-making. Notably, this nuanced understanding\nof perceived safety is exemplified in diverse driving scenarios.\nFor instance, when encountering a close car ahead, differ-\nent drivers exhibit markedly varied responses. An aggressive\ndriver, possibly influenced by sensation-seeking tendencies\n[15], might quickly swerve, perceiving lower risk. Conversely,\na cautious driver, perhaps more risk-averse [16], might opt for\na complete stop. These behaviors, far from being random, are\nintricately linked to each driver’s psychological profile and\npast experiences, revealing a significant limitation in current\narXiv:2502.20084v1  [cs.RO]  27 Feb 2025\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n2\nAD systems: their inability to account for these complex,\ncognitive behavioral patterns. Overall, perceived safety and its\ninfluence on the decisions of drivers with different behaviors.\nIn response, our research introduces the Cognitive-Informed\nTransformer (CITF) that integrates the concept of perceived\nsafety into trajectory prediction for AVs. This integration does\nmore than add a new variable; it injects a human-centric\nperspective into the heart of these systems. By doing so,\nwe aim to enhance the models’ ability to interpret driving\nbehaviors, leading to optimal long-term predictions. This ap-\nproach, promises a transformative impact on the predictive\ncapabilities of AD systems, aligning them more closely with\nthe multifaceted nature of human driving behavior.\nOverall, the key contributions of this study include:\n• We introduce the Quantitative Safety Assessment (QSA)\nas a cornerstone component for objectively evaluating\nthe safety of driving scenarios. In addition, we establish\nDriver Behavior Profiling (DBP) upon the QSA frame-\nwork to differentiate between distinct driver profiles.\nThis DBP effectively captures and interprets continuous\nnuances in driving behavior, while eliminating the depen-\ndence on manual labeling or predefined time windows.\n• We introduce an innovative module, named Leanformer,\nthat represents a significant advancement in understand-\ning social interactions on the road. This lightweight\ntransformer-based framework is adept at capturing the\nsubtle and complex inter-vehicular interactions that occur\nin everyday traffic. This development reflects a paradigm\nshift in AD research, aligning with the latest advance-\nments and understanding of vehicular social dynamics.\n• CITF significantly outperforms the SOTA baseline mod-\nels when tested on the NGSIM, MoCAD, and HighD\ndatasets. It maintains impressive performance even when\ntrained on only 25% of the dataset and with a much\nsmaller number of model parameters, demonstrating its\nefficiency and adaptability in various traffic scenes, in-\ncluding highways, campuses, and busy urban locales.\nImportantly, in a significant stride towards practical appli-\ncability, CITF shows unparalleled resilience in scenarios\nwith incomplete or inconsistent data.\nII. RELATED WORK\nTrajectory Prediction For Autonomous Driving.\nIn\nthe field of trajectory prediction, the analysis of prediction\nperformance is often categorized into short-term and long-term\nhorizons [3], [17]. Early research employed physical models to\nrepresent vehicle motion dynamics, thereby estimating future\ntrajectories. In [18], a trajectory prediction model based on\nthe bicycle model was proposed and successfully applied to\nan accident warning system. While physical models achieved\nsignificant progress in short-term prediction horizons, their\ninherent simplicity limited their performance in long-term\npredictions [19]. The complexity of human driving behavior,\ninfluenced by numerous factors such as cognitive processes,\ninteractions with surrounding vehicles, and environmental con-\nditions, renders long-term prediction a particularly challenging\ntask [17], [20]. In response to these challenges, researchers\nhave begun integrating deep learning models to account for\nthese factors in the trajectory prediction process. Notable\nprior efforts [8], [21], [22] have explored the complex social\ndynamics among traffic participants, revealing crucial latent\ninsights that enhance predictive accuracy. Transformer-based\nmodels [23], [24] have been increasingly employed for their\nability to predict future trajectory distributions effectively.\nGraph Neural Networks (GNNs) are also gaining traction\nfor capturing dynamic interactions in complex traffic scenes\n[25], [26]. These approaches primarily focus on understanding\nthe temporal and spatial interplays between traffic agents\nfrom historical data to optimize accuracy. Generative models\n[27], including Variational Auto Encoders (VAEs), Diffusion\nmodels, and Generative Adversarial Networks (GANs), are\nalso being explored for their potential to generate multiple\nfuture trajectory possibilities from latent distributions, offering\na probabilistic perspective of future paths in this field.\nPerceived Safety Concept. The notion of perceived safety\nhas been a focal point in psychology and physical human-\nrobot interaction (pHRI) studies [28]. In pHRI, it is crucial for\nassessing and representing individuals’ perceptions of danger\nand comfort during interactions with autonomous systems like\nmobile robots [29], industrial manipulators [30], humanoid\nrobots [31] and AVs [32]. Despite its relevance, perceived\nsafety remains a challenging concept to quantify due to its sub-\njective nature [33]. Our study breaks new ground in this area\nby proposing a novel quantitative criterion for perceived safety\nin self-driving trajectory prediction, drawing from Safety State\nMetrics (SSMs) and human decision-making processes. This\ninnovation enables our model to more accurately interpret\ndriving behavior and traffic conditions, thereby enhancing\nprediction accuracy in mixed autonomy environments.\nDriving Behavior Understanding. Existing studies in driv-\ning behavior have formulated various criteria and metrics for\ndetecting and representing driving patterns, using scales like\nthe Social Value Orientation (SVO) [34], Driving Anger Scale\n(DAS) [35], among others [36]. While these methods have\nbeen successful, as noted by [1] and [37], they typically de-\npend on manually annotated labels and predetermined sliding\ntime windows for analysis. Our research diverges from these\ntraditional approaches by proposing a dynamic, adaptive set of\nbehavior-aware criteria. This model captures driving behavior\nin real-time through continuous behavioral data representation,\neliminating the reliance on manual labeling in the training\nphase. This novel approach not only offers enhanced flexibility\nover fixed-category methods but also effectively addresses\nthe challenges of label shifts and time window selection,\nleading to a more accurate and fluid representation of driving\nbehavior. This advancement significantly contributes to the\ndevelopment of more refined and effective behavior prediction\nmethodologies in autonomous driving systems.\nIII. PROBLEM FORMULATION\nIn mixed autonomy traffic scenarios, trajectory prediction\nmodels within AVs are tasked with forecasting the future\ntrajectories of all surrounding vehicles within their perception\nrange. According to surveys by Mozaffari et al. [38] and\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n3\nTABLE I: Primary notations and their meanings.\nNotation\nMeaning\nXt−th:t\n0\nHistorical states of the target vehicle within a defined duration\nth\nXt−th:t\n1:n\nHistorical states of surrounding agents 1 to n within the\nduration th\nY\nt:t+tf\n0\nPredicted future trajectory of target vehicle over the ensuing\ntf time intervals\npt−th:t\n0:n\nx- and y- coordinates of the target vehicle and surrounding\nagents from time horizon t −th to t\nvt−th:t\n0:n\nVelocity of the target vehicle and its surrounding agents from\nt −th to t\nth\nAcceleration of the target vehicle and its surrounding agents\nfrom t −th to t\nn\nNumber of surrounding vehicles\nM\nTotal number of the predicted potential trajectories\nˆxi\nOriginal longitudinal coordinates of vehicle i\nM\nProbability of different maneuvers\nτsc\nGiven time threshold i\nai\nAcceleration of the traffic agent i\nvt\nx\nLateral velocity\nvt\ny\nLongitudinal position velocity at time t\npt\nx\nLongitudinal position coordinates at time t\npt\ny\nHorizontal coordinate\nSt\ni\nSafe Magnitude Index for the i-th traffic agent at time t\nH\nSet of safety indices\nIN\nIdentity Matrix\nA\nAdjacency matrix\n˜A\nDegree matrix for normalizing the graph structure\nZk+1\ni\nLearned feature matrix the i-th agent from GCNs\nαbehavior\nOutput generated by the multi-head self-attention mechanism\nfor DBP\nαpriority\nOutput of the multi-head self-attention mechanism for the\nPriority-Aware Module\nNotation\nMeaning\nhs\nTotal number of attention heads for QSA\nαsafety\nOutput of the multi-head self-attention mechanism for QSA\nGt\nDynamic geometric graph at time step t\nV t\nNode set of the DGG at time step t\nvt\ni\ni-th node of the DGG at time step t\nvt\ni\nEdge of the i-th node at time step t\nd(vt\ni, vt\nj)\nShortest distance between i-th and jth node\nN t\ni\nNeighborhood set of i-th node at time step t\nJ t\ni\nBehavior-awre criteria of i-th node at time step t\nJt\ni (D)\nDegree centrality of i-th node at time step t\nJt\ni (C)\nCloseness centrality of i-th node at time step t\nJt\ni (E)\nEigenvector centrality of i-th node at time step t\nJt\ni (B)\nBetweenness centrality of the i-th node at time step t\nJt\ni (P)\nPower centrality of i-th node at time step t\nJt\ni (K)\nKatz centrality of i-th node at time step t\n\f\fN t\ni\n\f\f\nTotal elements in Neighborhood set N t\ni\nσj,k\nTotal number of shortest paths between vt\nj and vt\nk at time\nstep t\nσj,k(vi)\nNumber of the paths traversing vt\ni at time step t\nAk\nii\ni-th diagonal element of the adjacency matrix to the k-th\npower\nk!\nFactorial of k\nαk\nDecay factor\nβk\nWeight for immediate neighbor nodes\ndk\nDimensionality of the projected key vectors for the Lean-\nformer framework\n¯Ot−th:t\nsafety\nSafety feature output from the QSA\n¯Obehavior\nBehavior feature output from the DBP\n¯O\nOutput from the Interaction-Aware Module\nOt−th:t\npriority\nBehavior feature output from the Priority-Aware Module\nYpred(T)\nPredicted trajectories at the prediction horizon T\nYgt(T)\nGround-truth trajectory at the horizon T\nDing et al. [39], the single-agent prediction setting remains a\nprevalent approach in the field of trajectory prediction. In this\nsetting, the model is developed by selecting one vehicle from\nthe surrounding vehicles as the prediction target. During the\nevaluation phase, the model’s predictive capability is assessed\nin a traversal manner, which treats each vehicle in the scene\nas the prediction target once. Adhering to this setting, we can\ndefine the terminology used in our study as follows:\n• Target vehicle: The vehicle is designated as the subject\nof the trajectory prediction task.\n• Surrounding agents: The AV and all of its perceived\ntraffic agents, excluding the target vehicle.\nIn summary, our problem could be formulated as developing\na trajectory prediction model that could utilize the historical\nstates (position, velocity, etc.) of both the target vehicle\nXt−th:t\n0\nand the surrounding vehicles Xt−th:t\n1:n\nspanning from\ntime t−th to present moment t, to predict the future trajectory\nY t:t+tf\n0\nof the target vehicle over the ensuing tf time intervals.\nA. Discretized Inputs and Outputs\nTheoretically, the inputs (historical states) and outputs (fu-\nture trajectories) should be represented in a continuous form.\nHowever, in practical deployment, the sensors on AVs collect\ndata at fixed intervals. Therefore, to maintain consistency with\nthe collected data, it is widely accepted in both academia [40],\n[41] and industry [42] to use discretized inputs and outputs\nwhen developing trajectory prediction models. Specifically, we\ndefine the inputs and outputs as follows:\n• Inputs: The historical states Xt−th:t\n0:n\nof the target vehi-\ncle and its surrounding agents, consists of a sequence\nof historical states {Xt−th\n0:n , Xt−th+1\n0:n\n, ..., Xt\n0:n}. At any\ntime t, the historical states Xt\n0:n comprise 2D position\ncoordinates pt\n0:n, velocity vt\n0:n, and acceleration at\n0:n.\n• Outputs: The predicted trajectory of the target vehicle,\ndenoted as Y t:t+tf\n0\n, consists of a sequence of predicted\npositions {pt+1\n0\n, pt+2\n0\n, . . . , pt+tf −1\n0\n, pt+tf\n0\n}.\nFor brevity, we also list the primary notations and their\nmeanings in Table I.\nB. Multi-modal Probabilistic Maneuver Prediction\nwe adopt a multimodal prediction framework to tackle the\ninherent uncertainty and variability in predictions. By evaluat-\ning different possible maneuvers that the target vehicle might\nperform, the framework computes the probability of each\nmaneuver based on historical states Xt−th:t\n0:n\n, which include 2D\nposition coordinates, velocity, and acceleration over a defined\ntime horizon th. This approach generates multiple predictions\nwhile also quantifying the confidence level associated with\neach prediction. This allows AVs to account for and respond\nto the uncertainty inherent in prediction outcomes, providing\na valuable advantage for decision-making processes.\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n4\nFig. 1: Pipeline of CITF. It is an encoder-decoder model (a) and includes four essential parts: Perceived Safety-Aware\nModule generates both safety and behavior features through driver behavior profiling (b) and quantitative safety assessment (c)\ncomponents, respectively. These features, along with the priority feature derived from the Priority-Aware Module, are integrated\ninto the Interaction-Aware Module, embedded by the Leanformer framework. Finally, this integration results in a high-level\nfusion, which is then fed into the Multimodal Decoder to produce a multimodal prediction distribution for the target vehicle.\nIn this study, we employ a hierarchical Bayesian framework\nto predict future trajectories. At each time step, we evaluate the\nprobability distribution over the possible maneuvers M of the\ntarget vehicle. To capture the driver’s behavioral nuances, we\ndecompose the vehicle’s potential maneuvers into two distinct\nsub-maneuvers: M = (Mp, Mv). Here, Mp represents the\nposition sub-maneuver, encompassing three discrete driver\ndecisions: left lane change ml, right lane change mr, and\nlane keeping mk. Similarly, Mv denotes the speed sub-\nmaneuver, with three options: accelerating ma, braking mb,\nand maintaining constant speed mc.\nFollowing this categorization, the framework generates de-\ntailed trajectories for the vehicle conditioned on each maneu-\nver within a predefined distributional form. At each current\ntime t, we extend the trajectory prediction task to compute the\nplausible trajectory distribution P\n\u0010\nY t:t+tf\n0\n| M, Xt−th:t\n0:n\n\u0011\n.\nIn particular, given the estimated maneuvers M, the prob-\nability distribution of the multimodal trajectory predictions\nY t:t+tf\n0\nis parameterized as a bivariate Gaussian distribution\nwith the estimable parameters Ω:\nP\n\u0010\nY\nt:t+tf\n0\n| M, Xt−th:t\n0:n\n\u0011\n= PΩ(Y\nt:t+tf\n0\n| M, Xt−th:t\n0:n\n)\n(1)\n= N(Y\nt:t+tf\n0\n|µ(Xt−th:t\n0:n\n), Σ(Xt−th:t\n0:n\n))\nHere, Ω=\n\u0002\nΩt+1, . . . , Ωt+tf \u0003\n, and Ωt = [µt, Σt] repre-\nsents the mean and variance of the distribution of predicted\ntrajectory point at time t. Correspondingly, the multi-modal\npredictions are then formulated as a Gaussian Mixture Model:\nP\n\u0010\nY\nt:t+tf\n0\n| M, Xt−th:t\n0:n\n\u0011\n(2)\n=\nX\n∀i\nP\n\u0010\nMi | Xt−th:t\n0:n\n\u0011\nPΩ\n\u0010\nY\nt:t+tf\n0\n| Mi, Xt−th:t\n0:n\n\u0011\nwhere Mi denote the i-th element in possible maneuvers M.\nIV. TRAJECTORY PREDICTION MODEL\nFigure 1 shows the hierarchical framework of CITF. Rooted\nin the encoder-decoder paradigm, the model seamlessly in-\ncorporates four novel modules: the Perceived Safety-Aware\nModule, the Priority-Aware Module, the Interaction-Aware\nModule, and the Multimodal Decoder. Collectively, these\nmodules are designed to capture human-machine interactions\nbetween the target vehicle and its surrounding agents and\nemulate the human decision-making process during driving.\nDetailed overviews of these modules follow.\nA. Perceived Safety-Aware Module\nAs mentioned before, perceived safety [43] plays a critical\nrole in human decision-making during driving. The nuances in\nperceived safety can significantly affect human driver behavior\nand further impact AV’s inability to account for the complex,\ncognitive behavioral patterns in mixed autonomy environ-\nments. Recognizing this, the Perceived Safety-Aware Module\nis introduced to establish precise criteria and quantify specific\ncriteria for evaluating perceived safety. It consists of two in-\ntegral components: 1) Quantitative Safety Assessment: This\ncomponent focuses on the development of physically based,\nmeasurable criteria that can accurately reflect how humans\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n5\nTABLE II: Perceived-Safety criteria and their interpretations.\nIndies\nSub-indicators\nDefinition\nSafe Magnitude Index (SMI)\nTime-to-Collision (TTC)\nTime until a potential collision occurs between the agent\nand another agent moving at the current velocity\nTime Exposed Time-to-Collision (TET)\nCumulative sum of instances in which a driver approaches\na leading vehicle with a TTC below a predefined threshold value\nTime Integrated Time-to-Collision (TIT)\nIntegral of the TTC profile over time if it remains below a specified threshold\nRisk Tendency Index (RTI)\nSubjective Risk Perception (SRP)\nCongestion level of each agent in complex traffic environment\nDynamic Risk Volatility (DRV)\nRate of change of congestion level for each agent in a complex traffic environment\nsubjectively assess the level of danger; 2) Driver Behavior\nProfiling: This component aims to provide in-depth, real-time\nanalysis and profiling of the continuous driving behavior of\nhuman drivers especially those influenced by their perceived\nsafety. Together, as shown in Table II, these components\nare meticulously designed to enhance AVs’ understanding\nof perceived safety in driving contexts, allowing them to\nbetter understand and anticipate human driver responses. By\nincorporating this type of valuable prior knowledge, we facil-\nitate the synthesis of human-like contextual patterns for the\nproposed model. This enhancement, along with the Priority-\nAware Module, allows our Interaction-Aware Module to better\ndecipher and assimilate the intentions of traffic agents and\nmore closely match the intricacies of human cognition and\ndecision-making in driving scenarios, resulting in improved\noverall model performance.\n1) Quantitative Safety Assessment: As shown in Figure\n1 (c), this component H includes two safety indices: the\nSafe Magnitude Index (SMI) and the Risk Tendency Index\n(RTI). In a nutshell, the SMI focuses primarily on quantifying\nthe spatio-temporal distance between different agents and the\npossibility of collision to evaluate the absolute safety level\nin real-time scenarios. Conversely, RTI tends toward a more\nsubjective analysis, capturing dynamic shifts in safety trends\nand congestion conditions that reveal potential escalation or\nmitigation of risk over time.\nSafe Magnitude Index. In the traffic safety domain, three\nmetrics in SSMs have gained prominence for their comprehen-\nsive portrayal of on-road risks: Time-to-Collision, Time Ex-\nposed Time-to-Collision (TET), and Time Integrated Time-to-\nCollision (TIT) [44]. Originating from traffic conflict studies,\nthese metrics are essential tools in microscopic traffic simula-\ntions to assess traffic safety. Correspondingly, we synthesize\nTTC, TIT, and TET into a ternary composite structure within\nSMI. This composite structure is introduced to evaluate the\ndynamics of interaction between traffic agents and to estimate\nthe likelihood of potential collisions for each vehicle in real-\ntime scenarios. Specifically, the SMI for an agent at a specific\ntime t can be expressed as St\ni\n= [TTCt\ni, TET t\ni , TIT t\ni ].\nTo align these with the traffic scenarios, we made slight\nmodifications. Using the 2D position coordinates pt\ni, pt\nj and\nvelocity vt\ni, vt\nj for vehicles i and j at time t.\n1) Time-to-Collision: TTC is a widely accepted measure\nused to evaluate the time available before two vehicles collide\nif they continue on their current trajectories. It offers insights\ninto imminent collision risks and serves as an early warn-\ning indicator. The TTC for the i-th vehicle is computed as\nTTCt\ni = −\ndt\ni,j\n˙dt\ni,j , where di,j represents the distance between\nvehicles i and j, and ˙di,j is its rate of change:\n\n\n\ndt\ni,j =\nq\u0000pt\ni −pt\nj\n\u0001⊤\u0000pt\ni −pt\nj\n\u0001\n˙dt\ni,j =\n1\ndt\ni,j\n\u0000pt\ni −pt\nj\n\u0001⊤\u0000vt\ni −vt\nj\n\u0001\n(3)\nAccordingly, the higher the TTC value, the lower the risk of\ncollision for the vehicle in this case.\n2) Time Exposed Time-to-Collision: TET measures the\nexposure duration to critical TTC values within th. It is the\nsum product of a switching variable and a time threshold\nτsc (set at 0.1s): TET tk\ni\n= Pt\ntk=t−th δi(tk) · τsc with the\nswitching variable given by:\nδi(tk) =\n\u001a\n1\n∀\n0 ≤TTCtk\ni ≤TTC∗\n0\notherwise\n(4)\nIn our study, TTC∗= 3.0s, delineating safety threshold.\n3) Time Integrated Time-to-Collision: An adaptation of\nTET, TIT integrates the TTC profile to evaluate safety levels.\nIt factors in the evolution of each vehicle’s TET temporally:\nTIT tk\ni\n=\nth\nX\ntk=t−tf\n[TTC∗−TTCi(tk)] · τsc\n(5)\nElevated values of TTC, TET, and TIT imply sustained expo-\nsure to potential collision risks, underscoring a deterioration\nin perceived safety. Overall, the SMI provides both real-time\ncrash risk assessment and an aggregated risk evaluation over a\ndefined period, eliminating the need for historical crash data.\nIt also takes into account the fluctuation and rate of change\nof these risks, assessing the safety benefits of AVs in mixed\nautonomy environments, and offering a comprehensive safety\nevaluation for each agent.\nRisk Tendency Index. To further capture congestion pat-\nterns in complex traffic environments, we propose an index\nbetween the i-th and j-th vehicles at time t, denoted as sub-\njective risk perception indicator (SPR), i.e. Rt\ni, and dynamic\nrisk volatility indicator (DRV), i.e. Rt\ni,j, respectively:\nRt\ni = Rt\nj =\nh\nlog(Rt\ni,j), log( ˙Rt\ni,j)\niT\n, ∀i, j ∈[0, n], i ̸= j (6)\nIn this context, the vector Rt\ni,j with larger values indi-\ncates an increased risk of collision, while the vector\n˙Rti,j\ncharacterizes the dynamic congestion conditions in complex\ntraffic scenarios. Then, the set of the safety indices H =\n{St−th:t\n0\n, Rt−th:t\n0\n, . . . , St−th:t\ni\n, Rt−th:t\ni\n∀i ∈[1, n]} serve as\ncontextual cues and are then fed into the safety encoder for\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n6\nembedding into high-level safety features. The definitions of\nSPR and DRV are defined as follows:\nRt\ni,j = Rt\nj,i =\n\u001a\n1/eqt\ni,j, qt\ni,j > 0\n0, qt\ni,j = 0\n(7)\nwhere the DRV ˙Rt\ni,j represents the gradient to evaluate fluc-\ntuations in SPR Rt\ni,j and can be expressed as follows:\n˙Rti,j = ˙Rt\nj,i =\n\u001a\n1/e ˙qt\ni,j, ˙qt\ni,j > 0\n0, ˙qt\ni,j = 0\n(8)\nThe quantities qt\ni,j and ˙qt\ni,j are calculated based on several crit-\nical parameters related to the dynamics of two traffic agents.\nThese parameters include the lateral velocity vt\nx, longitudinal\nvelocity vt\ny, 2D position coordinate pt\nx and pt\ny, as well as\nthe lateral at\nxand longitudinal at\ny. Mathematically, it can be\nrepresented as follows:\nqt\ni,j = max\n\u0012\n−∆i,jvt\nx × ∆i,jpt\nx + ∆i,jvt\ny × ∆i,jpt\ny\n∆i,jv2x + ∆i,jv2y\n, 0\n\u0013\n(9)\n˙qt\ni,j = −∆i,jat\nx × ∆i,jpt\nx + ∆i,jat\ny × ∆i,jpt\ny\n∆i,ja2x + ∆i,ja2y\n(10)\nwhere the ∆i,j(·) denotes the difference between quantities\nof the i-th and j-th vehicles. A larger vector Rt\ni,j indicates a\nhigher risk of collision, while the vector\n˙Rti,j describes the\ndynamic congestion conditions in complex traffic scene.\nSafety Encoder. This encoder applies the GCNs [45] to\nanalyze the spatial layouts of traffic agents and their environ-\nmental context. Next, it enhances the scaled dot-product multi-\nhead self-attention mechanism [46] for a nuanced analysis of\ntemporal relationships within safety indices.\nSpecifically, for GCN, we employ a convolutional neural\nnetwork on a fully connected interaction multigraph to capture\nthe dynamic geometric relationships among traffic agents. This\nmultigraph operational layer sequentially incorporates the set\nof safety indices H as nodes. These nodes represent various\nsecurity-related properties and states of the traffic agents over\ntime. To establish the connections between these nodes, we\nuse an adjacency matrix A, which is detailed in the following\nsubsection. This matrix represents the edges of the graph and\nis critical in defining the interactions and relationships between\ndifferent nodes (agents) within the graph. Formally,\nZk+1\ni\n= ϕReLU\n\u0010\n˜D−1\n2 ˜\nA ˜D−1\n2 Zk\ni W k\ni\n\u0011\n(11)\nwhere the matrix ˜D serves as the scale factor of ˜A, is the\ndegree matrix for normalizing the graph structure. It helps to\nbalance the influence of each node based on its connectivity.\nThe W k\ni represents the trainable weight matrix of the GCN\nfor the k-th layer, while ϕReLU is the Rectified Linear Unit\n(ReLU) activation function. Consequently, the matrix ˜A can be\ndefined as ˜\nA = A +λAIN, where λA is the weight and IN is\nthe identity matrix. The output of the k-th convolutional layer,\ndenoted as Zk+1\ni\n, represents the learned feature matrix of the i-\nth agent. Moreover, the initial feature Z0\ni = ϕMLP(H), where\nϕMLP denotes a Multi-Layer Perceptron (MLP). The MLP\nserves as a fully connected layer to embed the safety indices\nH into a feature space suitable for graph convolution. In\naddition, we employ a tri-layer convolutional neural network\nthat incorporates scatter and gathers operations to parallelize\nthe learning of contextual information and spatio-temporal\nagent interdependencies.\nNext, the feature matrix Zk+1\ni\n, Zk\ni and Zk−1\ni\noutput from\nthe (k + 1), k and (k −1)-th GCNs is then converted to the\nquery, key, value vectors, respectively, by the multi-head self-\nattention mechanism within the encoder to produce the high-\nlevel safety features. Formally,\n\n\n\n\n\nQsafety\ni\n= W Qs ϕMLP\n\u0000Zk+1\ni\n\u0001\nKsafety\ni\n= W Ks ϕMLP\n\u0000Zk\ni\n\u0001\nV safety\ni\n= W V s ϕMLP\n\u0000Zk−1\ni\n\u0001\n(12)\nwhere the W Qs , W Ks , W V s are learnable weights that can\nbe optimized via gradient descent. For the i-th self-attention\nhead headi, the formulation is as follows:\nhead s\ni\n= ϕsoftmax\n \nQsafety\ni\n(Ksafety\ni\n)⊤\n√ds\n!\nV safety\ni\n(13)\nIn the equation provided, ϕsoftmax(·) denotes the softmax\nactivation function, while ds represents the dimensionality\nof the projected key vectors. The output generated by the\nself-attention mechanism can be expressed as αsafety\n=\nPhs\ni=1 head s\ni , where hs is the total number of attention heads.\nTo increase training stability and efficiency, our model takes\ninspiration from ResNet [47] and incorporates Gated Linear\nUnits (GLUs) [48] along with Layer Normalization (LN) [49]\nfor the output of multi-head attention mechanism αsafety to\nefficiently manage features. Formally,\n¯Ot−th:t = ϕLN (ϕMLP(ϕGLUs(α)))\n(14)\nIn particular, GLUs provide a mechanism to control the flow\nof information through the network, making the model more\nadaptable, which can be defined as:\nϕGLUs(α) = (αW1 + b1) ⊙ϕsigmoid(αW2 + b2)\n(15)\nwhere α represents the safe attention coefficient from the\nmulti-head attention mechanism, W1 and W2 are the learnable\nweight parameters associated with the GLUs layer, b1 and b2\nare the corresponding biases, ⊙denotes element-wise multi-\nplication, ϕsigmoid is the sigmoid activation function, and ϕLN(·)\nstands for Layer Normalization. Correspondingly, the output\nof the encoder within the Quantitative Safety Assessment is\nthe high-level safety features, denoted as ¯Ot−th:t\nsafety .\n2) Driver Behavior Profiling: As shown in Figure 1 (b), we\nrepresent vehicles and their interactions as nodes and edges,\nrespectively, thereby constructing a Dynamic Geometric Graph\n(DGG). Leveraging this graph-based framework, we employ\ncentrality measures from graph theory to profile continuum\ndriver behavior in an unsupervised manner.\nDynamic Geometric Graph. Due to the dynamic nature of\ntraffic scenarios, the structure of the DGG evolves over time.\nAt any given moment t, we define the DGG Gt = {V t, Et}.\nSpecifically, the node set V t = {vt\n0, vt\n1, ..., vt\nn}, where node\nvt\ni represents vehicle i. The adjacency matrix At illustrates\nwhether edges exist between nodes, signifying the presence\nof interactions between vehicles. The establishment of this\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n7\nmatrix is based on the distances between vehicles, which can\nbe mathematically represented as follows:\nAt(i, j) =\n(\nd(vt\ni, vt\nj)\nif d(vt\ni, vt\nj) ≤r and i ̸= j\n0\notherwise\n(16)\nwhere d(vt\ni, vt\nj) denotes the distance between vehicle i and\nvehicle j, and r is a predefined threshold. The number of\nvehicles interacting with vehicle i is represented as N t\ni\nWith these configurations in place, we then apply centrality\nmeasures to assess agent behavior, identify key agents, and\nevaluate the overall connectivity within the traffic graph.\nCentrality Measures. Driver behavior significantly shapes\nthe interaction patterns between the driver and surrounding\nagents, resulting in distinct spatiotemporal dynamics. There-\nfore, we posit that spatiotemporal dynamics can effectively\ndifferentiate between various driver behaviors. Given that\ncentrality measures in graph theory provide a comprehensive\ndescription of the properties of nodes within a graph [50],\n[51], we employ centrality indicators such as degree Jt\ni (D),\ncloseness Jt\ni (C), eigenvector Jt\ni (E), betweenness Jt\ni (B),\npower Jt\ni (P), and Katz Jt\ni (K) centrality to characterize the\nspatial interaction dynamics of agent i at each moment t. To\naccount for both the temporal and spatial dimensions of these\ndynamics, we further analyze the temporal evolution of these\nindicators and establish the Behavior-aware Criteria, enabling\nthe continuous differentiation of diverse driving behaviors.\n1) Degree Centrality: The number of agents a vehicle\ncan influence reflects its significance within the traffic scene.\nDegree centrality Jt\ni (D), a metric that measures the number of\nconnections a node has, is thus naturally employed to describe\nthe importance of each vehicle i. Formally,\nJ t\ni (D) =\n\f\fN t\ni\n\f\f + J t−1\ni\n(D)\n(17)\nwhere |N t\ni | denotes the total agents in N t\ni .\n2) Closeness Centrality: The position of a vehicle within\na scene also reflects its significance. It is well-recognized that\nvehicles located centrally exert greater influence than those at\nthe periphery. Consequently, closeness centrality Jt\ni (C), which\nmeasures the proximity of a node to the center of the graph,\nis employed to characterize the importance of a vehicle as:\nJ t\ni (C) =\n|N t\ni | −1\nP\n∀vt\nj∈N t\ni d\n\u0000vt\ni, vt\nj\n\u0001\n(18)\n3) Eigenvector Centrality: The vehicle’s behavior can\ninfluence a broader set of agents through those it directly\ninteracts with, meaning that the importance of the directly\nconnected agents also reflects the vehicle’s significance. Eigen-\nvector centrality, which considers the importance of connected\nnodes, is used to assess the vehicle’s importance. The eigen-\nvector centrality of the vehicle i can be formulated as follows:\nJ t\ni (E) =\nP\n∀vt\nj∈N t\ni d\n\u0000vt\ni, vt\nj\n\u0001\nλ\n(19)\nwhere λ is the eigenvalue [52].\n4) Betweenness Centrality: The vehicle’s influence can\nbe transmitted to distant agents through intermediary agents,\nimplying that vehicles frequently acting as intermediaries play\na more crucial role in the network. Betweenness centrality\nJ t\ni (B), a metric that measures the extent to which a node\nserves as an intermediary within the shortest path between any\ntwo nodes, is naturally used to assess the vehicle’s importance.\nJ t\ni (B) =\nX\n∀vts,vt\nk∈V t\nσj,k(vt\ni)\nσj,k\n(20)\nwhere V t denotes the set of all agents present in the scene,\nσj,k signifies the total number of shortest paths between agent\nvt\nj and agent vt\nk, and σj,k(vi) represents the number of those\npaths traversing the agent vt\ni.\n5) Power Centrality: An interaction loop is a closed\nloop formed by a group of agents through direct or indirect\ninteractions. A vehicle’s participation in more interaction loops\nindicates greater influence within the overall traffic network.\nPower centrality J t\ni (P), which measures the frequency with\nwhich a node is part of closed cycles formed by edges, is used\nto describe the vehicle’s influence.\nJ t\ni (P) =\nX\nk\nAk\nii\nk!\n(21)\nwhere Ak\nii denotes the i-th diagonal element of the adjacency\nmatrix raised to the k-the power, k! signifies the factorial.\n6) Katz Centrality: To address the limitation of degree\ncentrality, which considers only direct interactions, we employ\nKatz centrality to emphasize both direct and distant inter-\nactions of the vehicle. Mathematically, the Katz centrality\nJ t\ni (K) of an agent vt\ni at time t can be formulated as:\nJ t\ni (K) =\nX\nk\nX\nj\nαkAk\nij+βk, ∀i, j ∈[0, n], where αk <\n1\nλmax\n(22)\nwhere n is the number of agents in the traffic scenario, αk\ndenotes the decay factor, βk represents weight for immediate\nneighbors, and Ak\nij is the i,j-th element of the k-th power of\nthe adjacency matrix. And λmax denotes the largest eigenvalue\nof the adjacency matrix. By carefully selecting the value of the\ndecay factor, Katz centrality can underscore the importance of\ncloser interactions while discounting more distant connections.\nBehavior-aware Criteria. Given the centrality metrics that\ncapture the spatial interaction dynamics of traffic agents,\nwe establish Behavior-aware Criteria that identify driving\nbehavior not only based on the magnitude of these metrics\nbut also on their temporal variation. Numerous studies have\ndemonstrated the feasibility of this approach, showing that\ndriving behavior can be identified using not only the instanta-\nneous magnitude of features like speed but also their temporal\nderivatives, such as acceleration and jerk [53]. This approach\nalso aligns with human intuition, as driving behaviors charac-\nterized by large and fluctuating centrality measures over short\nperiods are more likely to be relevant to driving behavior as\nsudden changes in acceleration within short intervals. Inspired\nby the established triadic relationship between velocity, ac-\nceleration, and jerk, we introduce three continuous criteria:\nBehavior Magnitude Index (BMI) Ct\ni, which measures the\ninfluence of driving behaviors by evaluating their centrality;\nBehavior Tendency Index (BTI) Lt\ni, which quantifies behavior\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n8\npropensity by calculating temporal derivatives, larger deriva-\ntives suggesting higher probabilities of specific behaviors;\nand Behavior Curvature Index (BCI) It\ni, which uses the jerk\nconcept to measure the intensity of driving behaviors by cal-\nculating the second-order derivatives of continuous centrality\nmeasures. At time t, the behavior J t\ni for vi can be defined as\nJ t\ni = [Ct\ni, Lt\ni, It\ni]T . Each component meticulously evaluates\nthe magnitude, probability, and intensity of diverse driving\nbehaviors exhibited by the target vehicle and its surrounding\nagents. This assessment is conducted through the computation\nof threshold rates, gradients, and concavities associated with\ncentrality measures, which capture behaviors such as lane\nchanges, acceleration, and deceleration, as well as aggressive,\nneutral, or conservative driving tendencies. The underlying\nrationale is that driving behaviors characterized by substantial\nand volatile centrality measure values over short time intervals\nare more likely to exert a significant influence on nearby\nagents, emphasizing the temporal dynamics that are integral\nto human drivers’ decision-making processes.\n1) Behavior Magnitude Index. The BMI is designed to\nquantify the scale and interconnectedness of various driving\nbehaviors by assessing their centrality measures. The BMI\nencapsulates the absolute values of these measures, providing a\nquantitative representation of a behavior’s influence on the sur-\nrounding traffic agents. Specifically, the BMI focuses on each\nagent’s centrality measures, with a higher index indicating that\na particular driving behavior exerts a more significant impact\non the current traffic dynamics. Formally, we first formulate\nthe BMI C for vehicle i as follows:\nCt\ni =\n\u0002\f\fJ t\ni (D)\n\f\f ,\n\f\fJ t\ni (C)\n\f\f ,\n\f\fJ t\ni (E)\n\f\f ,\n\f\fJ t\ni (B)\n\f\f ,\n\f\fJ t\ni (P)\n\f\f ,\n\f\fJ t\ni (K)\n\f\f\u0003T\n(23)\nwhere |·| denotes the absolute value operator.\n2) Behavior Tendency Index. Building on the BMI, the\nBTI incorporates human factors in driving, particularly those\nbehaviors that may cause significant fluctuations in centrality\nmeasures, such as aggressive driving, sudden lane changes,\nor abrupt braking. Specifically, the BTI aims to quantify the\npropensity for various driving behaviors by calculating their\ntemporal derivatives. By capturing the temporal interaction\ndynamics of driving behavior, the BTI can identify instances\nwhere large gradients and local extrema suggest a higher\nprobability of particular behaviors. This approach enables the\nmodel to estimate the likelihood of specific behaviors, even in\nthe absence of explicit behavior classification. Mathematically,\nLt\ni =\n\f\f\f\f\n∂Ct\ni\n∂t\n\f\f\f\f =\n\u0014\f\f\f\f\n∂J t\ni (D)\n∂t\n\f\f\f\f ,\n\f\f\f\f\n∂J t\ni (C)\n∂t\n\f\f\f\f , · · · ,\n\f\f\f\f\n∂J t\ni (K)\n∂t\n\f\f\f\f\n\u0015T\n(24)\n3) Behavior Curvature Index. BCI introduces the concept\nof jerk to quantify the potential impact of driving behavior\non surrounding agents. Building upon the BTI, the BCI\ncaptures driving behavior by calculating the second derivative\nof sequential centrality measures. The motivation behind BCI\nstems from the observation that abrupt changes in BTI over\nshort periods, such as during braking or acceleration, result\nin peaks in the BCI curve. Additionally, BCI considers the\nduration of behavior fluctuations, positing that behaviors with\nprolonged fluctuations have a greater impact on the traffic\nenvironment than short-term variations. For instance, a driver\nwho frequently changes lanes or adjusts speed over an ex-\ntended period may confuse and pressure other drivers, thereby\nsignificantly disrupting the dynamic traffic environment.\nIt\ni =\n\f\f\f\f\n∂Lt\ni\n∂t\n\f\f\f\f =\n\u0014\f\f\f\f\n∂2J t\ni (D)\n∂2t\n\f\f\f\f ,\n\f\f\f\f\n∂2J t\ni (C)\n∂2t\n\f\f\f\f , · · · ,\n\f\f\f\f\n∂J t\ni (K)\n∂2t\n\f\f\f\f\n\u0015T\n(25)\nThe introduction of the BMI, BTI, and BCI provides a\nholistic understanding of individual driving behaviors. Addi-\ntionally, our proposed behavior-aware criteria eliminate the\nneed for manual labeling during the training phase, effectively\nmitigating challenges associated with dynamic behavior labels\nand the selection of appropriate time windows.\nBehavior Encoder. To leverage the high-level prior knowl-\nedge embedded in the Behavior-aware Criteria, we introduce\nthe behavior encoder to extract behavior features. The Behav-\nior encoder comprises two main components: the LSTM and\nthe multi-head self-attention mechanism. The behavior J t−th:t\ni\nare first processed by the LSTM, yielding temporal vectors:\n˜Jt−th:t\ni\n= ϕLSTM\n\u0010\nht−th:t\ni\n, ϕMLP(Jt−th:t\ni\n), ϕMLP( ¯Ot−th:t\nsafety\n)\n\u0011\n(26)\nHere, the LSTM incrementally updates the hidden state of\nagent vi on a frame-by-frame basis using shared weights.\nTo enhance this process, we incorporate a multi-head self-\nattention mechanism and GLUs to calculate attention weights\nacross diverse agent behaviors. This approach yields precise\nsequential behavioral features ¯Obehavior, akin to the QSA:\n¯Ot−th:t\nbehavior = ϕLN\n\u0000ϕMLP(ϕGLUs(αbehavior))\n\u0001\n(27)\nwhere αbehavior is the output of the multi-head self-attention\nmechanism within the behavior encoder.\nB. Priority-Aware Module\n1) Pooling Mechanism: In light of recent advances in\ncognitive studies [8], [54], it has become evident that the\nspatial positioning of vehicles within a scene can variably\ninfluence the behavior and decisions of a target vehicle. For\ninstance, vehicles located directly in the anticipated trajectory\npath tend to exert greater influence relative to those situated\nbehind. Furthermore, during overtaking maneuvers, vehicles\npositioned on the left may carry augmented significance. Rec-\nognizing these spatial intricacies, we introduce the Priority-\nAware Module. This sophisticated module adeptly transforms\nthe spatial coordinates of agents, encoding them into high-\ndimensional positional vectors, producing positional features.\nOur pooling mechanism adeptly amalgamates dynamic po-\nsitional information from the encompassing traffic scenario,\neffectively capturing both individual and multi-agent position\nvectors. This mechanism emphasizes the dynamic nuances of\nposition data, accommodating historical agent states, denoted\nas Stk\ni , as well as the intricate spatial interplay symbolized by\nP tk\ni,j. Mathematically, these relationships are represented as:\nStk\ni\n= {ptk\ni −ptk−1\ni\n, vtk\ni −vtk−1\ni\n, atk\ni −atk−1\ni\n}\n(28)\nCorrespondingly,\nP tk\ni,j = {ptk\ni −ptk\nj , vtk\ni −vtk\nj , atk\ni −atk\nj }\n(29)\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n9\nFig. 2: Architecture of the proposed Leanformer and the scaled dot-product linear attention mechanism.\nBy converting the position point sets into sequential vectors,\nthis pooling mechanism effectively indicates the potential spa-\ntial relationship between the target agent and its neighboring\nagents at each scene and time step. This allows the module to\naccurately represent the necessary interactions over dynamic\nand spatial position characteristics of each agent.\n2) Priority Encoder: Within this module, the encoder,\nwhich combines both LSTM and multi-head attention mecha-\nnisms, processes the dynamic position vectors. This processing\ninvolves transforming discrete position vectors into a more\ncontinuous spatio-temporal domain, thereby improving the\nrepresentation of temporal and spatial dynamics. At each\ndiscrete temporal instance t, the encoder assimilates recent\nhistorical position vectors via an LSTM network:\nOt−th:t\npriority = ϕLSTM\n\u0000¯ht−th:t−1\ni\n, St−th:t−1\ni\n, P t−th:t−1\ni,j\n\u0001\n(30)\nThen, the output of the LSTM is then channeled through\na multi-head attention mechanism and GLUs, similar to the\nquantitative safety assessment, culminating in the synthesis of\nrefined priority features:\n¯Ot−th:t\npriority = ϕLN\n\u0000ϕMLP(ϕGLUs(α priority))\n\u0001\n(31)\nwhere α priority is the output of the multi-head attention mech-\nanism in the priority encoder.\nC. Interaction-Aware Module\nTo better understand the synergistic influence of surrounding\nvehicles’ risk levels, positions, and one’s behavior on the target\nvehicle’s future trajectory, we introduce an Interaction-Aware\nModule. This Module is based on a novel lightweight Trans-\nformer framework, i.e. Leanformer, which is an adaptation\nof the Linformer architecture [55]. Departing from traditional\nTransformer models that rely on fully connected weight matri-\nces, our framework refines the Transformer design, especially\nin its attention mechanism. By adopting a low-rank matrix\napproximation, we significantly increase the computational\nefficiency. This approach effectively reduces the computational\ncomplexity of self-attention from O(n2) to O(n × d), where\nn represents the sequence length and d denotes a much\nsmaller projected dimension. Mathematically, the interaction\nis represented as follows:\nOt−th:t = ϕMLP( ¯Ot−th:t\nsafety )∥ϕMLP( ¯Ot−th:t\nbehavior)∥ϕMLP( ¯Ot−th:t\npriority)\n(32)\nThis equation captures the integrated effect of safety per-\nception, behavioral tendencies, and spatial positioning on the\ntarget vehicle’s trajectory.\nAs shown in Figure 2, the sequence Ot−th:t serves as the\ninput to the Transformer-based framework. The queries Q,\nkeys K, and values V are obtained by performing linear trans-\nformations on the input sequence using low-rank projection\nmatrices, as illustrated below:\n\n\n\nQ = W Q(ϕMLP(Ot−th:t + Lq))\nK = W K(ϕMLP(Ot−th:t + Lk))\nV = W V (ϕMLP(Ot−th:t))\n(33)\nHere, W Q, W K, and W V represent low-rank projection matri-\nces for the queries, keys, and values, respectively. Notably, we\nintroduce additional tokens Lq and Lk to the end of the queries\nQ and keys K to enhance feature representation and ensure\ntraining stability. This augmentation can be mathematically\nrepresented as follows:\n\n\n\nLq = ϕGRU\n\u0010\nϕMLP( ¯Ot−th:t\nsafety )∥ϕMLP( ¯Ot−th:t\nbehavior)\n\u0011\nLk = ϕGRU\n\u0010\nϕMLP( ¯Ot−th:t\nsafety )∥ϕMLP( ¯Ot−th:t\npriority )\n\u0011\n(34)\nwhere ϕGRU denotes the GRU framework. The output matrix ¯O\nis computed as the sum of the outputs from all attention heads,\ndenoted as headi, where i ranges from 1 to h, representing the\ntotal number of attention heads. Each attention head has its\nown set of projection matrices W Q\ni , W K\ni , and W V\ni , while skip\nconnections are also used in this framework. This is expressed\nmathematically as follows:\n¯O =\nh\nX\ni=1\nheadi + (ϕMLP(Q)∥ϕMLP(V ))\n(35)\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n10\nIn this equation, headi refers to the output of the i-th attention\nhead. The query matrix Q ∈Rn×dz, key matrix K ∈Rn×dz,\nand value matrix V ∈Rn×dz are all of dimensionality n×dz.\nThe attention mechanism in each attention head computes\na context mapping matrix ¯P of size (k × d) using scaled dot-\nproduct attention. It involves the following calculations:\n¯P = ϕsoftmax\n \nQW Q\ni\n\u0000UiKW K\ni\n\u0001T\n√dk\n!\n(36)\nwhere Ui ∈Rn×k denotes a fixed linear projection matrix,\nwhile dk is the dimensionality of the projected key vectors.\nFinally, the output matrix headi is obtained by multiplying\n¯P with the projected value matrix FV W V\ni\nusing a linear\nprojection matrix Fi ∈Rn×k:\nheadi = ¯PFV W V\ni\n= ϕAttention\n\u0010\nQW Q\ni , UiKW K\ni , FiV W V\ni\n\u0011\n(37)\nThis can also be expressed as follows:\nheadi = ϕsoftmax\n \nQW Q\ni (UiKW K\ni )T\n√dk\n!\n|\n{z\n}\n¯\nP :n×k\n· FiV W V\ni\n|\n{z\n}\nk×d\n(38)\nwhere the attention weights are obtained by calculating the\nscaled dot product of the query and key projection matrices,\nfollowed by the softmax activation function. The resulting\nweights are then used to compute the weighted sum of\nthe value projection matrix, which represents the composite\ninteractive vectors ¯O fed into the Multimodal Decoder to\ngenerate the future trajectories for the target vehicle.\nD. Multimodal Decoder\nThe decoder, rooted in a Gaussian Mixture Model with\nmultimodality, employs a dedicated LSTM and a fully con-\nnected layer. It processes the composite interactive vectors\n¯O to forecast the target vehicle’s trajectory. The predicted\ntrajectory, Y t:t+tf\n0\n, is determined by:\nY t:t+tf\n0\n= Fθ\n\u0000Fθ( ¯O)\n\u0001\n(39)\nsuch that,\nFθ(·) = ϕReLU (ϕMLP [ϕGN (ϕLSTM(·))])\n(40)\nHere, ϕGN is Group Normalization, used for improved\ntraining stability. The decoder’s output comprises multiple\nfuture trajectories for the vehicle.\nV. EXPERIMENTS\nA. Experimental Setups\nTo validate the prediction capability of our model across\ndifferent scenarios, we conduct a series of experiments on\nthree widely used traffic datasets: NGSIM, MoCAD, and\nHighD. The experiments on NGSIM and HighD primarily\nassess the model’s performance in highway scenarios with\nvarying traffic densities, while the experiments on MoCAD\nfocus on its ability to predict in urban and campus-like,\nunstructured environments. To ensure a fair comparison with\nexisting models [21], [56], we adopt the same training and\nevaluation protocols as those used in prior benchmarks. Specif-\nically, we define a reference time point, utilizing the preceding\nthree seconds of data as input to the model and the subsequent\nfive seconds as ground truth for supervising model training.\nIn addition, we provide a detailed analysis of our model’s\nperformance across both short-term (≤2 seconds) and long-\nterm prediction horizons (>2 seconds).\nAcknowledging a gap in existing research regarding data\nomissions in prediction, we develop an innovative approach to\ntackle the issue of missing data. We establish the missing test\nset, which is further categorized into three subsets based on\nthe duration of data omissions: drop 3-frames, drop 5-frames,\nand drop 8-frames. Omissions are purposefully made around\nthe midpoint of the historical trajectory. For instance, in the\ndrop 5-frames subset, data ranging from the (t −8)-th to the\n(t−12)-th frame is excluded. To manage these omissions, we\nemploy simple linear interpolation. The evaluation results on\nthese subsets are reported as CITF (drop 3-frames), CITF\n(drop 5-frames), and CITF (drop 8-frames), respectively.\nFurthermore, to demonstrate the adaptability and efficiency of\nour model, we train it on a limited training set, which contains\nonly 25% of the available training datasets. This model variant\nis denoted as CITF (25%) in this study.\nB. Evaluation Metric\nTo evaluate the performance of our model and compare it\nto the baselines, we utilize the commonly used performance\nmetric: Root Mean Square Error (RMSE), which can be\ndefined as follows:\nRMSE =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(Ypred(T) −Ygt(T))2\n(41)\nwhere Ypred(T) and Ygt(T) represent the predicted and\nground-truth trajectories, respectively, at a given horizon T.\nC. Training and Implementation Details\nOur model was trained on an NVIDIA A40 GPU with 48GB\nof memory. The training process utilized a batch size of 64 and\nwas conducted over 20 epochs. We adopted a dynamic learning\nrate strategy, initially set at 10−3 and gradually reducing\nto 10−5. The Adam optimizer was employed, coupled with\nthe CosineAnnealingWarmRestarts scheduler to manage the\nlearning rate adjustments. Following the multi-task learning\nframework [57], our loss function combines the RMSE and the\nNegative Log-Likelihood (NLL) metrics. The RMSE metric\nquantifies the average Euclidean distance between predicted\nand ground truth trajectories, serving as a general measure of\npredictive accuracy. The NLL metric is particularly valuable\nfor assessing the fidelity of trajectory predictions relative\nto expected maneuvers, ensuring the reliability of trajectory\nforecasting within action-based models.\nD. Experiment Results\n1) Performance Comparison on Short-term Horizon: We\npresent comparative results of our model’s prediction perfor-\nmance on the short-term prediction horizon (≤2 seconds)\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n11\nTABLE III: Evaluation results of the proposed model and baseline methods on the short-term prediction horizon (≤2 seconds).\nThe metric is RMSE (m), with lower values indicating better performance. Cells marked with (‘-’) indicate data not available.\nBold and underlined values indicate the best and the second-best performance in each category, respectively.\nModel\nNGSIM\nMoCAD\nHighD\n1\n2\n1\n2\n1\n2\nCS-LSTM [58]\n0.61\n1.27\n1.45\n1.98\n0.22\n0.61\nNLS-LSTM [59]\n0.56\n1.22\n0.96\n1.27\n0.20\n0.57\nCF-LSTM [60]\n0.55\n1.10\n0.72\n0.91\n0.18\n0.42\niNATran [61]\n0.39\n0.96\n-\n-\n0.04\n0.05\nBAT [8]\n0.23\n0.81\n0.35\n0.74\n0.08\n0.14\nMHA-LSTM [62]\n0.41\n1.01\n1.25\n1.48\n0.19\n0.55\nSTDAN [3]\n0.39\n0.96\n0.62\n0.85\n0.19\n0.27\nHLTP [12]\n0.41\n0.91\n0.55\n0.76\n0.09\n0.16\nHLTP++ [13]\n0.46\n0.98\n0.64\n0.86\n0.12\n0.18\nWSiP [21]\n0.56\n1.23\n0.70\n0.87\n0.20\n0.60\nCITF\n0.30\n0.81\n0.28\n0.63\n0.04\n0.09\nCITF (drop 3-frames)\n0.38\n0.86\n0.35\n0.80\n0.05\n0.11\nCITF (drop 5-frames)\n0.41\n0.90\n0.45\n0.94\n0.16\n0.30\nCITF (drop 8-frames)\n0.42\n0.94\n0.65\n1.03\n0.17\n0.44\nCITF (25%)\n0.42\n0.93\n0.55\n0.96\n0.08\n0.21\nTABLE IV: Comparative evaluation of our model on the short-term prediction horizon against selected baselines on the\nmissing test set of the NGSIM dataset. RMSE (m) is used as the evaluation metric. Bold values indicate the best performance,\nwhile underlined values indicate the second-best performance in each category.\nModel\nDrop 3-frames\nDrop 5-frames\nDrop 8-frames\n1s\n2s\n1s\n2s\n1s\n2s\nCS-LSTM [58]\n0.67\n1.47\n0.75\n1.52\n0.84\n1.72\nCF-LSTM [60]\n0.59\n1.14\n0.64\n1.37\n0.70\n1.46\nWSiP [21]\n0.60\n1.29\n0.69\n1.37\n0.76\n1.58\nSTDAN [3]\n0.42\n1.00\n0.47\n1.12\n0.57\n1.37\nBAT [8]\n0.28\n0.88\n0.48\n0.99\n0.52\n1.15\nHLTP [12]\n0.49\n1.21\n0.67\n1.34\n0.75\n1.46\nHLTP++ [13]\n0.48\n1.09\n0.63\n1.26\n0.70\n1.34\nCITF\n0.38\n0.86\n0.41\n0.90\n0.42\n0.94\nagainst existing baseline models on the NGSIM, MoCAD,\nand HighD datasets in Table III. Our model achieves the\nbest or second-best performance across all three datasets.\nAmong existing approaches, the BAT model demonstrates\nthe best results on the NGSIM and MoCAD datasets, while\nthe iNATran model performs best on the HighD dataset.\nTherefore, a comparative analysis of these two models is both\nessential and representative. Compared to the BAT model,\nalthough our model slightly underperforms in the 1-second\nprediction horizon on the NGSIM dataset, it surpasses BAT\non all other datasets. Specifically, on the MoCAD dataset, our\nmodel achieves a minimum improvement of 14.8% in short-\nterm prediction horizons, while on the HighD dataset, the\nimprovement is at least 35.8%. Similarly, when compared to\nthe iNATran model, our model falls slightly behind only in the\n2-second prediction horizon on the HighD dataset. However, it\ndemonstrates significant improvements on the NGSIM dataset,\nachieving gains of 23.1% and 15.6% in the 1-second and 2-\nsecond prediction horizons, respectively. We further conducted\na comprehensive evaluation of the robustness of our proposed\nmodel using the missing test set and compared its perfor-\nmance with SOTA baselines. As shown in Table III, CITF\noutperforms most models, including CS-LSTM and WSiP, on\nthe missing test set, even surpassing their performance on\nthe complete test set. Additionally, we assessed the ability\nof various baselines to handle data missingness challenges\nusing the NGSIM missing dataset, with results presented\nin Table IV. A clear trend emerges: as the proportion of\nmissing data increases, the advantages of CITF become even\nmore pronounced. Specifically, CITF outperforms the previous\nbest-performing BAT model by at least 9.0% and 20.0% in\nthe drop 5-frames and drop 8-frames scenarios, respectively.\nOverall, while CITF does not show a distinct advantage over\nprevious baseline models in short-term prediction horizons,\nit significantly outperforms them in the presence of data\nomissions—a common challenge in real-world applications\ndue to observational constraints.\n2) Performance Comparison on Long-term Horizon: As\nshown in Table V, our model achieves the best performance\nacross all datasets for long-term prediction horizons (>2\nseconds). On the NGSIM dataset, our model significantly\nsurpasses all baselines from 2018 to 2024, achieving im-\nprovements of 7.8%, 15.7%, and 17.1% for the 3-second, 4-\nsecond, and 5-second prediction horizons, respectively. Sim-\nilarly, results on the MoCAD dataset highlight the strong\nperformance of our model in busy urban traffic scenarios, with\nimprovements of 21.6%, 23.7%, and 20.8% over the same\nhorizons. Furthermore, our model demonstrates substantial\nperformance gains on the HighD dataset, outperforming the\nBAT and DACR-AMTP models with remarkable improve-\nments of up to 31.8% and 57.4% in RMSE, respectively. In the\n5-second prediction horizon, our model surpasses iNATran by\nan impressive 60.9%. We also investigated the impact of data\nomissions on the model’s long-term prediction performance.\nAs shown in Table V, even when faced with the drop 3-frames\nand drop 5-frames scenarios, CITF outperforms existing base-\nlines on the majority of prediction horizons across all datasets.\nSpecifically, at the 5-second prediction horizon, CITF (drop 3-\nframes) achieved improvements of 13.2%, 16.7%, and 24.2%\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n12\nTABLE V: Evaluation results of the proposed model and baseline methods on the long-term prediction horizon (>2 seconds).\nThe accuracy is measured using RMSE (m), with lower values indicating better performance. Cells marked with (‘-’) indicate\ndata not available. underlined values indicate the best and the second-best performance in each category, respectively.\nModel\nNGSIM\nMoCAD\nHighD\n3\n4\n5\n3\n4\n5\n3\n4\n5\nCS-LSTM [58]\n2.09\n3.10\n4.37\n2.94\n3.56\n4.49\n1.24\n2.10\n3.27\nNLS-LSTM [59]\n2.02\n3.03\n4.30\n2.08\n2.86\n3.93\n1.14\n1.90\n2.91\nCF-LSTM [60]\n1.78\n2.73\n3.82\n1.73\n2.59\n3.44\n1.07\n1.72\n2.44\niNATran [61]\n1.61\n2.42\n3.43\n-\n-\n-\n0.21\n0.54\n1.10\nBAT [8]\n1.54\n2.52\n3.62\n1.39\n2.19\n2.88\n0.20\n0.44\n0.62\nMHA-LSTM [62]\n1.74\n2.67\n3.83\n2.57\n3.22\n4.20\n1.10\n1.84\n2.78\nSTDAN [3]\n1.61\n2.56\n3.67\n1.62\n2.51\n3.32\n0.48\n0.91\n1.66\nWSiP [21]\n2.05\n3.08\n4.34\n1.70\n2.56\n3.47\n1.21\n2.07\n3.14\nHLTP++ [13]\n1.52\n2.17\n3.02\n1.56\n2.40\n3.19\n0.30\n0.47\n0.75\nCITF\n1.42\n2.04\n2.82\n1.09\n1.67\n2.28\n0.18\n0.30\n0.43\nCITF (drop 3-frames)\n1.51\n2.32\n2.95\n1.24\n1.75\n2.40\n0.23\n0.35\n0.47\nCITF (drop 5-frames)\n1.52\n2.40\n3.31\n1.30\n1.80\n2.72\n0.43\n0.64\n0.92\nCITF (drop 8-frames)\n1.65\n2.45\n3.51\n1.63\n2.13\n2.98\n0.83\n1.25\n1.72\nCITF (25%)\n1.55\n2.54\n3.30\n1.35\n2.22\n3.10\n0.41\n0.62\n0.92\nTABLE VI: Comparative evaluation of our model on the long-term prediction horizon against selected baselines on the missing\ntest set of the NGSIM dataset. RMSE (m) is used as the evaluation metric. Bold values indicate the best performance, while\nunderlined values indicate the second-best performance in each category.\nModel\nDrop 3-frames\nDrop 5-frames\nDrop 8-frames\n3s\n4s\n5s\n3s\n4s\n5s\n3s\n4s\n5s\nCS-LSTM [58]\n2.34\n3.60\n4.71\n2.47\n3.82\n4.97\n2.64\n3.97\n5.34\nCF-LSTM [60]\n1.82\n2.77\n3.91\n1.94\n2.83\n3.98\n2.21\n3.10\n4.47\nWSiP [21]\n2.10\n3.17\n4.42\n2.19\n3.41\n4.77\n2.37\n3.64\n5.10\nSTDAN [3]\n1.68\n2.64\n3.72\n1.81\n2.75\n3.88\n2.14\n3.04\n4.19\nBAT [8]\n1.59\n2.59\n3.67\n1.74\n2.61\n3.84\n1.82\n2.69\n3.98\nCITF\n1.51\n2.32\n2.95\n1.52\n2.40\n3.31\n1.65\n2.45\n3.51\non the NGSIM, MoCAD, and HighD datasets, respectively.\nTable VI further illustrates CITF’s performance against other\nmodels on the missing test set of the NGSIM dataset. CITF\noutperforms all other models across all prediction horizons,\nand this advantage increases as the proportion of data omis-\nsions grows. On the drop 8-frames test set, CITF surpasses the\nbest existing models by 9.3%, 8.9%, and 11.8% at the 3, 4,\nand 5-second prediction horizons, respectively. Overall, CITF\nexhibits better performance in long-term prediction horizons\ncompared to short-term horizons. These results underscore the\nmodel’s ability to capture long-term intentions and deliver\naccurate predictions over long-term horizons.\n3) Performance Comparison on Limited 25% Training Set:\nTo challenge our model’s adaptability, we trained it using only\na quarter of the available training set from the NGSIM, HighD,\nand MoCAD datasets, yet evaluated its performance on the\ncomplete test set. Impressively, as shown in Tables III and\nV, even with this limited training data, our model delivered\nRMSE values that were notably lower than most baseline\nmodels. Such results underscore our model’s efficiency and\nrobustness in trajectory prediction. This performance indicates\na promising potential: our model might substantially cut down\non the data demands typically associated with training AVs,\nparticularly in scenarios that are data-scarce. In summary, our\nfindings attest to the model’s reliability, resource efficiency,\nand precision in forecasting vehicle trajectories.\n4) Comparative Analysis of Model Performance and Com-\nplexity: As shown in Table VII, our model is benchmarked\nagainst several top baselines across three real-world datasets.\nA notable challenge in this field is the limited availability\nof efficiency metrics, compounded by restricted access to the\nsource code of various models. Consequently, our comparison\nmainly focuses on open-source models. Although our model is\nnot the most parameter-efficient, it surpasses all competitors\nby achieving the lowest average RMSE across all datasets.\nRemarkably, this high level of accuracy is attained with\nsubstantially reduced model complexity—using 18.3% fewer\nparameters than Gava. Additionally, we assess the inference\nspeed of CITF on the NGSIM dataset. As presented in Table\nVIII, while our model’s inference speed is slightly slower than\nthat of the MHA-LSTM model, it ranks among the most ac-\ncurate. Specifically, CITF outpaces the previous SOTA model\nSTDAN, with inference speeds 31.8% faster, respectively.\nThese findings highlight CITF’s ability to balance speed and\naccuracy, further emphasizing its lightweight, efficient, and\nprecise performance in predicting future vehicle trajectories.\nTABLE VII: Comparative evaluation of CITF with selected\nbaselines. Highlighting the accuracy metric (Average RMSE\n(m)) and complexity measured by the number of parameters\n#Param. (K). Purple indicates the performance of our model.\nModel\nAverage RMSE (m)\n#Param. (K)\nNGSIM\nHighD\nMoCAD\nCS-LSTM [58]\n2.29\n1.49\n2.88\n194.92\nCF-LSTM [60]\n1.99\n1.17\n1.88\n387.10\nWSiP [21]\n2.25\n1.44\n1.86\n300.76\nGaVa [63]\n1.65\n0.39\n1.63\n360.75\nCITF\n1.48\n0.21\n1.00\n294.61\nE. Ablation Studies\nWe perform a detailed ablation study to assess the spe-\ncific contributions of each component within our trajectory\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n13\nTABLE VIII: Inference time comparison of CITF with the\nSOTA baselines on NGSIM. The inference time is for 10\nbatches with a size of 128 on two Nvidia A40 48G GPUs.\nPurple indicates the performance of our model.\nModel\nAverage RMSE (m)\nInference time (s)\nCS-LSTM [58]\n2.29\n0.22\nMHA-LSTM [64]\n1.93\n0.11\nTS-GAN [65]\n2.06\n0.23\nWSiP [21]\n2.25\n0.25\nSTDAN [3]\n1.87\n0.22\nCITF\n1.48\n0.15\nTABLE IX: Different components of ablation study.\nComponents\nAblation methods\nA\nB\nC\nD\nE\nF\nDriver Behavior Profiling\n✘\n✔\n✔\n✔\n✔\n✔\nPerceived Safety-Aware Module\n✔\n✘\n✔\n✔\n✔\n✔\nPriority-Aware Module\n✔\n✔\n✘\n✔\n✔\n✔\nInteraction-Aware Module\n✔\n✔\n✔\n✘\n✔\n✔\nMultimodal Decoder\n✔\n✔\n✔\n✔\n✘\n✔\nprediction model. The summarized results are presented in\nTable X. Notably, Model F, which integrates all components,\nconsistently outperforms other variations across all evaluation\nmetrics, underscoring the combined value of these components\nin optimizing performance. In contrast, Model A, which omits\nthe Driver Behavior Profiling within the Perceived Safety-\nAware Module, experiences a substantial decline in perfor-\nmance, particularly in short-term predictions, with reductions\nof at least 19.8% on the NGSIM dataset and 30.7% on the\nHighD dataset. This underscores the critical role of Driver\nBehavior Profiling in improving trajectory prediction accuracy.\nModel B, a reduced version of Model F without the Perceived\nSafety-Aware Module shows a significant reduction in RMSE,\nespecially for long-term predictions, with improvements of at\nleast 16.0% and 28.0% on the NGSIM and HighD datasets,\nrespectively. This highlights the importance of considering\nperceived safety factors in trajectory prediction, especially\nfor long-term prediction. Model C, which uses absolute co-\nordinates instead of relative positions in the Priority-Aware\nModule, displays non-negligible reductions in prediction met-\nrics, highlighting the importance of spatial relationships in\nachieving accuracy. Model D, which lacks the Interaction-\nAware Module, shows performance losses of at least 14.8%\nand 25% on the NGSIM and HighD datasets for short-term\nprediction, and at least 8.5% and 26.7% for long-term pre-\ndiction, respectively. Finally, Model E reduces the multimodal\nprobabilistic maneuver prediction in the Decoder, resulting in\na performance degradation of at least 5.7% and 13.9% in the\nNGSIM and HighD datasets, respectively. This suggests their\nimportance in improving prediction accuracy.\nF. Qualitative Results\nFigure 3 compares the performance of CITF with the top\nbaselines, BAT and WSiP, in complex highway scenarios.\nAll three models perform well in short-term predictions, with\npredicted trajectories closely aligning with the ground truth.\nHowever, as shown in Figure 3 (a), in the long-term pre-\ndiction horizon, CITF (ours) successfully identifies the target\nTABLE X: Evaluation results of the ablation analysis for\ndifferent models on the NGSIM and HighD datasets, with\nRMSE (m) as the evaluation metric.\nDataset\nTime (s)\nModel\nA\nB\nC\nD\nE\nF\nNGSIM\n1\n0.47\n0.45\n0.41\n0.43\n0.39\n0.30\n2\n1.01\n0.97\n0.89\n0.93\n0.88\n0.81\n3\n1.50\n1.69\n1.77\n1.54\n1.59\n1.42\n4\n2.58\n2.70\n2.39\n2.47\n2.27\n2.04\n5\n3.32\n3.41\n3.10\n3.26\n2.98\n2.82\nHighD\n1\n0.06\n0.05\n0.04\n0.05\n0.05\n0.04\n2\n0.13\n0.14\n0.10\n0.12\n0.11\n0.09\n3\n0.25\n0.25\n0.21\n0.24\n0.22\n0.18\n4\n0.40\n0.42\n0.36\n0.38\n0.36\n0.30\n5\n0.58\n0.67\n0.51\n0.55\n0.49\n0.43\nFig. 3: Visual insights from CITF and top baselines on the\nNGSIM dataset, illustrating short-term and long-term predic-\ntions for three complex driving scenarios: (a) merging, and\n(b-c) rightward lane change. A darker blue shade indicates an\nincreased risk to the target vehicle, and vice versa.\nvehicle’s intent to merge into an adjacent lane and predicts\nthe overtaking maneuver, producing an accurate forecast. In\ncontrast, both the BAT and WSiP baselines incorrectly predict\nthat the vehicle will continue driving straight. Moreover, in\nFigures 3 (b-c), facing the intricate dynamics and interactions\ntypical of long-term predictions, such as sudden lane changes,\nsurrounding traffic congestion, and potential collisions, CITF\nexcels at capturing the subtle influences of driver behavior.\nIt effectively models the traffic scene, recognizing the im-\nportance of surrounding vehicles and their impact on the\ntarget vehicle’s trajectory, thus maintaining high prediction\naccuracy. These visual results reveal that other models struggle\nto accurately discern vehicle intentions in complex long-\nterm scenarios. This also further underscores the competitive\nperformance of CITF in short-term predictions, as well as its\noutstanding capabilities in long-term prediction tasks.\nVI. CONCLUSION\nOn the journey to fully autonomous driving, long-term\ntrajectory prediction remains a complex challenge. This study\nintroduces an innovative approach rooted in cognitive insights,\nemphasizing the critical role of perceived safety in driver\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n14\ndecision-making. Our Perceived Safety-Aware Module har-\nmoniously merges Quantitative Safety Assessment and Driver\nBehavior Profiling, offering a detailed perspective on safety\nperceptions in driving. Rigorous evaluations on real-world\ndriving datasets demonstrated the adaptability of our model,\neven under data constraints and missing data. In long-term\nprediction horizons, our proposed CITF significantly outper-\nforms existing models, highlighting the promise of combining\ncomputational advantages with human cognitive processes\nto enhance both the safety and efficiency of autonomous\ndriving. Despite significant progress in prediction accuracy\nand efficiency, the limitations of our model in short-term low-\ncomplexity scenarios warrant further investigation. In such\nscenes, minimal interactions between the target and surround-\ning vehicles make short-term dynamics largely governed by\nbasic kinematic principles. A promising avenue for future\nresearch is to integrate physical models with deep learning\ntechniques to better account for varying traffic scenes and\nimprove the robustness of the model.\nREFERENCES\n[1] W. Schwarting, A. Pierson, J. Alonso-Mora, S. Karaman, and D. Rus,\n“Social behavior for autonomous vehicles,” Proceedings of the National\nAcademy of Sciences, vol. 116, no. 50, pp. 24 972–24 978, 2019.\n[2] H. Liao, H. Shen, Z. Li, C. Wang, G. Li, Y. Bie, and C. Xu, “Gpt-\n4 enhanced multimodal grounding for autonomous driving: Leveraging\ncross-modal attention with large language models,” Communications in\nTransportation Research, vol. 4, p. 100116, 2024.\n[3] X. Chen, H. Zhang, F. Zhao, Y. Hu, C. Tan, and J. Yang, “Intention-\naware vehicle trajectory prediction based on spatial-temporal dynamic\nattention network for internet of vehicles,” IEEE TITS, vol. 23, no. 10,\npp. 19 471–19 483, 2022.\n[4] H. Liao, Y. Li, Z. Li, Z. Bian, J. Lee, Z. Cui, G. Zhang, and C. Xu, “Real-\ntime accident anticipation for autonomous driving through monocular\ndepth-enhanced 3d modeling,” Accident Analysis & Prevention, vol. 207,\np. 107760, 2024.\n[5] N. Arbabzadeh, M. Jafari, M. Jalayer, S. Jiang, and M. Kharbeche, “A\nhybrid approach for identifying factors affecting driver reaction time\nusing naturalistic driving data,” TR Part C, vol. 100, pp. 107–124, 2019.\n[6] Z. Li, Z. Cui, H. Liao, J. Ash, G. Zhang, C. Xu, and Y. Wang, “Steering\nthe future: Redefining intelligent transportation systems with foundation\nmodels,” CHAIN, vol. 1, no. 1, pp. 46–53, 2024.\n[7] H. Liao, Y. Li, C. Wang, Y. Guan, K. Tam, C. Tian, L. Li, C. Xu, and\nZ. Li, “When, where, and what? a benchmark for accident anticipation\nand localization with large language models,” in Proceedings of the 32nd\nACM International Conference on Multimedia, 2024, pp. 8–17.\n[8] H. Liao, Z. Li, H. Shen, W. Zeng, D. Liao, G. Li, and C. Xu,\n“Bat: Behavior-aware human-like trajectory prediction for autonomous\ndriving,” in AAAI, vol. 38, no. 9, 2024, pp. 10 332–10 340.\n[9] S. Casas, C. Gulino, S. Suo, K. Luo, R. Liao, and R. Urtasun,\n“Implicit latent variable model for scene-consistent motion forecasting,”\nin Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XXIII 16.\nSpringer, 2020,\npp. 624–641.\n[10] M.\nRubagotti,\nI.\nTusseyeva,\nS.\nBaltabayeva,\nD.\nSummers,\nand\nA. Sandygulova, “Perceived safety in physical human–robot interac-\ntion—a survey,” Robotics and Autonomous Systems, vol. 151, p. 104047,\n2022.\n[11] I. Ajzen, “The theory of planned behavior,” Organizational behavior\nand human decision processes, vol. 50, no. 2, pp. 179–211, 1991.\n[12] H. Liao, Y. Li, Z. Li, C. Wang, Z. Cui, S. E. Li, and C. Xu, “A cognitive-\nbased trajectory prediction approach for autonomous driving,” IEEE TIV,\nvol. 9, no. 4, pp. 4632–4643, 2024.\n[13] H. Liao, Y. Li, Z. Li, C. Wang, G. Li, C. Tian, Z. Bian, K. Zhu,\nZ. Cui, and J. Hu, “Less is more: Efficient brain-inspired learning for\nautonomous driving trajectory prediction,” in ECAI 2024.\nIOS Press,\n2024, pp. 4361–4368.\n[14] S. I. Kronemer, M. Aksen, J. Z. Ding, J. H. Ryu, Q. Xin, Z. Ding, J. S.\nPrince, H. Kwon, A. Khalaf, S. Forman et al., “Human visual conscious-\nness involves large scale cortical and subcortical networks independent\nof task report and eye movement activity,” Nature Communications,\nvol. 13, no. 1, p. 7342, 2022.\n[15] M. Zuckerman, “The psychophysiology of sensation seeking,” Journal\nof personality, vol. 58, no. 1, pp. 313–345, 1990.\n[16] M. Rabin, “Risk aversion and expected-utility theory: A calibration\ntheorem,” in Handbook of the fundamentals of financial decision making:\nPart I.\nWorld Scientific, 2013, pp. 241–252.\n[17] C. Wang, H. Liao, Z. Li, and C. Xu, “Wake: Towards robust and\nphysically feasible trajectory prediction for autonomous vehicles with\nwavelet and kinematics synergy,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2025.\n[18] M. Br¨annstr¨om, E. Coelingh, and J. Sj¨oberg, “Model-based threat\nassessment for avoiding arbitrary vehicle collisions,” IEEE TITS, vol. 11,\nno. 3, pp. 658–669, 2010.\n[19] C. Wang, H. Liao, K. Zhu, G. Zhang, and Z. Li, “A dynamics-enhanced\nlearning model for multi-horizon trajectory prediction in autonomous\nvehicles,” arXiv preprint arXiv:2412.20784, 2024.\n[20] H. Liao, S. Liu, Y. Li, Z. Li, C. Wang, Y. Li, S. E. Li, and C. Xu, “Human\nobservation-inspired trajectory prediction for autonomous driving in\nmixed-autonomy traffic environments,” in ICRA.\nIEEE, 2024, pp.\n14 212–14 219.\n[21] R. Wang, S. Wang, H. Yan, and X. Wang, “Wsip: Wave superposition\ninspired pooling for dynamic interactions-aware trajectory prediction,”\nin AAAI, vol. 37, 2023, pp. 4685–4692.\n[22] H. Liao, Z. Li, C. Wang, B. Wang, H. Kong, Y. Guan, G. Li, and\nZ. Cui, “A cognitive-driven trajectory prediction model for autonomous\ndriving in mixed autonomy environments,” in Proceedings of the IJCAI-\n24.\nIJCAI, 8 2024, pp. 5936–5944.\n[23] H. Liao, C. Wang, Z. Li, Y. Li, B. Wang, G. Li, and C. Xu, “Physics-\ninformed trajectory prediction for autonomous driving under missing\nobservation,” in Proceedings of the IJCAI-24, 8 2024, pp. 6841–6849.\n[24] C. Wang, H. Liao, B. Wang, Y. Guan, B. Rao, Z. Pu, Z. Cui,\nC. Xu, and Z. Li, “Nest: A neuromodulated small-world hypergraph\ntrajectory prediction model for autonomous driving,” arXiv preprint\narXiv:2412.11682, 2024.\n[25] L. Rowe, M. Ethier, E.-H. Dykhne, and K. Czarnecki, “Fjmp: Factor-\nized joint multi-agent motion prediction over learned directed acyclic\ninteraction graphs,” in CVPR, 2023, pp. 13 745–13 755.\n[26] H. Liao, Z. Li, C. Wang, H. Shen, D. Liao, B. Wang, G. Li, and C. Xu,\n“Mftraj: Map-free, behavior-driven trajectory prediction for autonomous\ndriving,” in Proceedings of IJCAI, 2024, pp. 5945–5953.\n[27] H. Liao, X. Li, Y. Li, H. Kong, C. Wang, B. Wang, Y. Guan, K. Tam, and\nZ. Li, “Cdstraj: Characterized diffusion and spatial-temporal interaction\nnetwork for trajectory prediction in autonomous driving,” in Proceedings\nof the IJCAI-24.\nIJCAI, 8 2024, pp. 7331–7339, aI for Good.\n[28] J. Guiochet, M. Machin, and H. Waeselynck, “Safety-critical advanced\nrobots: A survey,” Robotics and Autonomous Systems, vol. 94, pp. 43–\n52, 2017.\n[29] X. Chen, S. He, Y. Zhang, L. C. Tong, P. Shang, and X. Zhou, “Yard\ncrane and agv scheduling in automated container terminal: A multi-robot\ntask allocation framework,” TR Part C, vol. 114, pp. 241–271, 2020.\n[30] J. L. Davis, “Role-taking and robotic form: An exploratory study of\nsocial connection in human-robot interaction,” International Journal of\nHuman-Computer Studies, p. 103094, 2023.\n[31] B. Busch, G. Cotugno, M. Khoramshahi, G. Skaltsas, D. Turchi, L. Ur-\nbano, M. W¨achter, Y. Zhou, T. Asfour, G. Deacon et al., “Evaluation\nof an industrial robotic assistant in an ecological environment,” in IEEE\nRO-MAN.\nIEEE, 2019, pp. 1–8.\n[32] J. Sun and J. Kim, “Joint prediction of next location and travel time\nfrom urban vehicle trajectories using long short-term memory neural\nnetworks,” TR Part C, vol. 128, p. 103114, 2021.\n[33] C. Bartneck, D. Kuli´c, E. Croft, and S. Zoghbi, “Measurement in-\nstruments for the anthropomorphism, animacy, likeability, perceived\nintelligence, and perceived safety of robots,” International journal of\nsocial robotics, vol. 1, pp. 71–81, 2009.\n[34] R. O. Murphy, K. A. Ackermann, and M. J. Handgraaf, “Measuring\nsocial value orientation,” Judgment and Decision making, vol. 6, no. 8,\npp. 771–781, 2011.\n[35] J. L. Deffenbacher, E. R. Oetting, and R. S. Lynch, “Development of a\ndriving anger scale,” Psychological reports, vol. 74, no. 1, pp. 83–91,\n1994.\n[36] O. Taubman-Ben-Ari, M. Mikulincer, and O. Gillath, “The multidimen-\nsional driving style inventory—scale construct and validation,” Accident\nAnalysis & Prevention, vol. 36, no. 3, pp. 323–332, 2004.\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n15\n[37] R. Chandra, U. Bhattacharya, T. Mittal, A. Bera, and D. Manocha,\n“Cmetric: A driving behavior measure using centrality functions,” in\n2020 IROS.\nIEEE, 2020, pp. 2035–2042.\n[38] S. Mozaffari, O. Y. Al-Jarrah, M. Dianati, P. Jennings, and A. Mouza-\nkitis, “Deep learning-based vehicle behavior prediction for autonomous\ndriving applications: A review,” IEEE TITS, vol. 23, no. 1, pp. 33–47,\n2020.\n[39] Z. Ding and H. Zhao, “Incorporating driving knowledge in deep learning\nbased vehicle trajectory prediction: A survey,” IEEE TIV, vol. 8, no. 8,\npp. 3996–4015, 2023.\n[40] X. Chen, H. Zhang, F. Deng, J. Liang, and J. Yang, “Stochastic\nnon-autoregressive transformer-based multi-modal pedestrian trajectory\nprediction for intelligent vehicles,” IEEE TITS, 2023.\n[41] H. Zhou, X. Yang, D. Ren, H. Huang, and M. Fan, “Csir: Cascaded\nsliding cvaes with iterative socially-aware rethinking for trajectory\nprediction,” IEEE TITS, 2023.\n[42] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,\nD. Wang, P. Carr, S. Lucey, D. Ramanan et al., “Argoverse: 3d tracking\nand forecasting with rich maps,” in CVPR, 2019, pp. 8748–8757.\n[43] G. S. Nair and C. R. Bhat, “Sharing the road with autonomous vehicles:\nPerceived safety and regulatory preferences,” TR Part C, vol. 122, p.\n102885, 2021.\n[44] M. M. Minderhoud and P. H. Bovy, “Extended time-to-collision mea-\nsures for road traffic safety assessment,” Accident Analysis & Prevention,\nvol. 33, no. 1, pp. 89–97, 2001.\n[45] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation\nlearning on large graphs,” Advances in neural information processing\nsystems, vol. 30, 2017.\n[46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in CVPR, 2016, pp. 770–778.\n[48] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling\nwith gated convolutional networks,” in International conference on\nmachine learning.\nPMLR, 2017, pp. 933–941.\n[49] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[50] S. P. Borgatti, M. G. Everett, and J. C. Johnson, Analyzing Social\nNetworks.\nSAGE Publications Limited, 2018.\n[51] J. Zhang and Y. Luo, “Degree centrality, betweenness centrality, and\ncloseness centrality in social network,” in 2017 2nd international confer-\nence on modelling, simulation and applied mathematics (MSAM2017).\nAtlantis press, 2017, pp. 300–303.\n[52] U. N. Pillai and L.-H. Lim, “Perron-frobenius theorem for nonnegative\ntensors,” Communications in Mathematical Sciences, vol. 3, no. 3, pp.\n531–539, 2005.\n[53] Y. L. Murphey, R. Milton, and L. Kiliaris, “Driver’s style classification\nusing jerk analysis,” in 2009 IEEE workshop on computational intelli-\ngence in vehicles and vehicular systems.\nIEEE, 2009, pp. 23–28.\n[54] D. P. Broadbent, G. D’Innocenzo, T. J. Ellmers, J. Parsler, A. J.\nSzameitat, and D. T. Bishop, “Cognitive load, working memory capacity\nand driving performance: A preliminary fnirs and eye tracking study,”\nTR Part F, vol. 92, pp. 121–132, 2023.\n[55] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer:\nSelf-attention with linear complexity,” arXiv preprint arXiv:2006.04768,\n2020.\n[56] K. Gao, X. Li, B. Chen, L. Hu, J. Liu, R. Du, and Y. Li, “Dual\ntransformer based prediction for lane change intentions and trajectories\nin mixed traffic environment,” IEEE TITS, 2023.\n[57] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using uncer-\ntainty to weigh losses for scene geometry and semantics,” in CVPR,\n2018, pp. 7482–7491.\n[58] N. Deo and M. M. Trivedi, “Convolutional social pooling for vehicle\ntrajectory prediction,” in CVPR Workshops, 2018, pp. 1468–1476.\n[59] K. Messaoud, I. Yahiaoui, A. Verroust-Blondet, and F. Nashashibi,\n“Non-local social pooling for vehicle trajectory prediction,” in 2019\nIEEE IV.\nIEEE, 2019, pp. 975–980.\n[60] X. Xie, C. Zhang, Y. Zhu, Y. N. Wu, and S.-C. Zhu, “Congestion-aware\nmulti-agent trajectory prediction for collision avoidance,” in 2021 ICRA.\nIEEE, 2021, pp. 13 693–13 700.\n[61] X. Chen, H. Zhang, F. Zhao, Y. Cai, H. Wang, and Q. Ye, “Vehicle\ntrajectory prediction based on intention-aware non-autoregressive trans-\nformer with multi-attention learning for internet of vehicles,” IEEE TIM,\nvol. 71, pp. 1–12, 2022.\n[62] K. Messaoud, I. Yahiaoui, A. Verroust-Blondet, and F. Nashashibi,\n“Attention based vehicle trajectory prediction,” IEEE TIV, vol. 6, no. 1,\npp. 175–185, 2021.\n[63] H. Liao, S. Liu, Y. Li, Z. Li, C. Wang, B. Wang, Y. Guan, and C. Xu,\n“Human observation-inspired trajectory prediction for autonomous driv-\ning in mixed-autonomy traffic environments,” ICRA, 2024.\n[64] K. Messaoud, I. Yahiaoui, A. Verroust-Blondet, and F. Nashashibi,\n“Attention based vehicle trajectory prediction,” IEEE TIV, vol. 6, no. 1,\npp. 175–185, 2020.\n[65] Y. Wang, S. Zhao, R. Zhang, X. Cheng, and L. Yang, “Multi-vehicle\ncollaborative learning for trajectory prediction with spatio-temporal\ntensor fusion,” IEEE TITS, vol. 23, no. 1, pp. 236–248, 2020.\nHaicheng Liao (Student Member, IEEE) received\nthe B.S. degree in software engineering from the\nUniversity of Electronic Science and Technology of\nChina (UESTC) in 2022. He is currently pursuing\nthe Ph.D. degree at the State Key Laboratory of In-\nternet of Things for Smart City and the Department\nof Computer and Information Science, University of\nMacau. Over his academic career, he has published\nover 20 papers. His research interests include con-\nnected autonomous vehicles and the application of\ndeep reinforcement learning to autonomous driving.\nChengyue Wang is currently pursuing a Ph.D.\ndegree at the State Key Laboratory of Internet of\nThings for Smart City and the Department of Civil\nEngineering, University of Macau. He received his\nM.S. degree in civil engineering from the University\nof Illinois Urbana-Champaign (UIUC) in 2022. He\nreceived his B.E. degree in transportation engineer-\ning from Chang’an University in 2021. His research\ninterests include connected autonomous vehicles and\nthe application of deep reinforcement learning to\nautonomous driving.\nKaiqun Zhu received the Ph.D. degree in con-\ntrol science and engineering from the University\nof Shanghai for Science and Technology, Shanghai,\nChina, in 2022. From 2020 to 2022, he was a visiting\nPh.D. student with the Department of Computer Sci-\nence, Brunel University London, Uxbridge, U.K. He\nis currently a Postdoctoral Fellow with the Univer-\nsity of Macau, Macau, China. His research interests\ninclude set-membership filtering, model predictive\ncontrol, neural networks, privacy preserving, and\ntheir applications in autonomous vehicles.\nYilong Ren (Member, IEEE) received the B.S. and\nPh.D. degrees from Beihang University, Beijing,\nChina, in 2010 and 2017, respectively. He is cur-\nrently an Associate Professor with the Research In-\nstitute for Frontier Science, Beihang University. His\nresearch interests include vehicular communications,\nvehicular crowd sensing, and traffic Big Data.\n\n\nJOURNAL OF IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 2025\n16\nBolin Gao received the B.S. and M.S. degrees in Ve-\nhicle Engineering from Jilin University, Changchun,\nChina, in 2007 and 2009, respectively, and the\nPh.D. degree in Vehicle Engineering from Tongji\nUniversity, Shanghai, China, in 2013. He is now\nan associate research professor at the School of\nVehicle and Mobility, Tsinghua University. His re-\nsearch interests include the theoretical research and\nengineering application of the dynamic design and\ncontrol of intelligent and connected vehicles, espe-\ncially about the collaborative perception and tracking\nmethod in cloud control system, intelligent predictive cruise control system on\nCommercial trucks with cloud control mode, as well as the test and evaluation\nof intelligent vehicle driving system.\nShengbo Li (Senior Member, IEEE) received his\nM.S. and Ph.D. degrees from Tsinghua University\nin 2006 and 2009, respectively. Before joining Ts-\ninghua University, he had worked at Stanford Uni-\nversity, University of Michigan, and UC Berkeley.\nHis active research interests include intelligent vehi-\ncles and driver assistance, deep reinforcement learn-\ning, optimal control and estimation, etc. He is the\nauthor of over 130 peer-reviewed journal/conference\npapers and the co-inventor of over 30 patents. He is\nthe recipient of the best (student) paper awards of\nIEEE ITSC, ICCAS, IEEE ICUS, CCCC, etc. His important awards include\nthe National Award for Technological Invention of China (2013), the Excellent\nYoung Scholar of NSF China (2016), the Young Professor of ChangJiang\nScholar Program (2016), the National Award for Progress in Sci & Tech of\nChina (2018), Distinguished Young Scholar of Beijing NSF (2018), Youth\nSci & Tech Innovation Leader from MOST (2020), etc. He also serves as the\nBoard of Governor of the IEEE ITS Society, Senior AE of IEEE OJ ITS, and\nAEs of IEEE ITSM, IEEE Trans ITS, Automotive Innovation, etc.\nChengzhong Xu (Fellow, IEEE) received the Ph.D.\ndegree from The University of Hong Kong, in 1993.\nHe is currently the chair professor of computer\nscience and the dean with the Faculty of Science\nand Technology, University of Macau. Prior to this,\nhe was with the faculty at Wayne State University,\nUSA, and the Shenzhen Institutes of Advanced\nTechnology, Chinese Academy of Sciences, China.\nHe has published more than 400 papers and more\nthan 100 patents. His research interests include cloud\ncomputing and data-driven intelligent applications.\nHe was the Best Paper awardee or the Nominee of ICPP2005, HPCA2013,\nHPDC2013, Cluster2015, GPC2018, UIC2018, and AIMS2019. He also won\nthe Best Paper award of SoCC2021. He was the Chair of the IEEE Technical\nCommittee on Distributed Processing from 2015 to 2019.\nZhenning Li (Member, IEEE) received his Ph.D. in\nCivil Engineering from the University of Hawaii at\nManoa, Honolulu, Hawaii, USA, in 2019. Currently,\nhe holds the position of Assistant Professor at the\nState Key Laboratory of Internet of Things for Smart\nCity, as well as the Department of Computer and\nInformation Science at the University of Macau,\nMacau. His main areas of research focus on the inter-\nsection of connected autonomous vehicles and Big\nData applications in urban transportation systems.\nHe has been honored with several awards, including\nthe Macau Science and Technology Award, Chinese Government Award for\nOutstanding Self-financed Students Abroad, TRB best young researcher award\nand the CICTP best paper award, amongst others.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20084v1.pdf",
    "total_pages": 16,
    "title": "Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights",
    "authors": [
      "Haicheng Liao",
      "Chengyue Wang",
      "Kaiqun Zhu",
      "Yilong Ren",
      "Bolin Gao",
      "Shengbo Eben Li",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "abstract": "In mixed autonomous driving environments, accurately predicting the future\ntrajectories of surrounding vehicles is crucial for the safe operation of\nautonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory is\ndetermined by the decision-making process of human drivers. However, existing\nmodels primarily focus on the inherent statistical patterns in the data, often\nneglecting the critical aspect of understanding the decision-making processes\nof human drivers. This oversight results in models that fail to capture the\ntrue intentions of human drivers, leading to suboptimal performance in\nlong-term trajectory prediction. To address this limitation, we introduce a\nCognitive-Informed Transformer (CITF) that incorporates a cognitive concept,\nPerceived Safety, to interpret drivers' decision-making mechanisms. Perceived\nSafety encapsulates the varying risk tolerances across drivers with different\ndriving behaviors. Specifically, we develop a Perceived Safety-aware Module\nthat includes a Quantitative Safety Assessment for measuring the subject risk\nlevels within scenarios, and Driver Behavior Profiling for characterizing\ndriver behaviors. Furthermore, we present a novel module, Leanformer, designed\nto capture social interactions among vehicles. CITF demonstrates significant\nperformance improvements on three well-established datasets. In terms of\nlong-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM,\n28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, its\nrobustness in scenarios with limited or missing data is evident, surpassing\nmost state-of-the-art (SOTA) baselines, and paving the way for real-world\napplications.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}