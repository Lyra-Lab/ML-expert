{
  "id": "arxiv_2502.20309v1",
  "text": "EAIRA: Establishing a Methodology for Evaluating\nAI Models as Scientific Research Assistants\nPREPRINT\nFranck Cappello∗σ, Sandeep Madireddy∗σ, Robert Underwood∗, Neil Getty†, Nicholas Lee-Ping Chia†\nNesar Ramachandra‡, Josh Nguyen∥, Murat Kec¸eli‡, Tanwi Mallick∗, Zilinghan Li†,\nMarieme Ngom∗, Chenhui Zhang\nx, Angel Yanguas-Gil\nxi, Evan Antoniuk∗∗, Bhavya Kailkhura∗∗, Minyang Tian†\n,\nYufeng Du†, Yuan-Sen Ting††, Azton Wells‡, Bogdan Nicolae∗, Avinash Maurya∗, M. Mustafa Rafique‡‡,\nEliu Huerta†, Bo Li§, Ian Foster†, Rick Stevens†\n∗Mathematics and Computer Science Division, Argonne National Laboratory,\n† Data Science and Learning Division, Argonne National Laboratory,\n‡ Computational Science Division, Argonne National Laboratory,\nxi Applied Materials Division, Argonne National Laboratory,\n§ Department of Computer Science, The University of Chicago,\n∥University of Pennsylvania, ∗∗Lawrence Livermore National Laboratory,\n†† The Ohio State University, ‡‡ Rochester Institute of Technology,\nx Massachusetts Institute of Technology.\nAbstract—\nRecent advancements have positioned AI, and particularly\nLarge Language Models (LLMs) as transformative tools for\nscientific research, capable of addressing complex tasks that\nrequire reasoning, problem-solving, and decision-making. Their\nexceptional capabilities suggest their potential as scientific re-\nsearch assistants, but also highlight the need for holistic, rigorous,\nand domain-specific evaluation to assess effectiveness in real-\nworld scientific applications. This paper describes a multifaceted\nmethodology for Evaluating AI models as scientific Research\nAssistants (EAIRA) developed at Argonne National Laboratory.\nThis methodology incorporates four primary classes of evalua-\ntions. 1) Multiple Choice Questions to assess factual recall; 2)\nOpen Response to evaluate advanced reasoning and problem-\nsolving skills; 3) Lab-Style Experiments involving detailed anal-\nysis of capabilities as research assistants in controlled environ-\nments; and 4) Field-Style Experiments to capture researcher-\nLLM interactions at scale in a wide range of scientific do-\nmains and applications. These complementary methods enable\na comprehensive analysis of LLM strengths and weaknesses\nwith respect to their scientific knowledge, reasoning abilities, and\nadaptability. Recognizing the rapid pace of LLM advancements,\nwe designed the methodology to evolve and adapt so as to ensure\nits continued relevance and applicability. This paper describes\nthe methodology’s state at the end of February 2025. Although\ndeveloped within a subset of scientific domains, the methodology\nis designed to be generalizable to a wide range of scientific\ndomains.\nIndex Terms—component, formatting, style, styling, insert\nI. INTRODUCTION\nRecent advances in Large Language Models (LLMs) have\ngreatly broadened conceptions of what AI may be able to\naccomplish in the near future. Models such as OpenAI’s\nGPT O1 [1], Google’s Gemini [2], and Anthropic’s Claude\nσCorresponding Authors: cappello@anl.gov, smadireddy@anl.gov\n[3] are transforming traditional natural language understand-\ning (NLU) tasks like summarization, information extraction,\ntranslation, and classification with enhanced contextual depth\nand adaptability. They are also exhibiting promising potential\nbeyond NLU, with measurable progress on tasks such as math-\nematical problem solving, multi-step reasoning, and symbolic\nlogic—and achieving significant milestones such as passing\nthe Uniform BAR exam and medical licensing exams [4]. Such\nachievements highlight their potential to emulate abstraction,\nlogical deduction, and domain-specific expertise. This evolu-\ntion from NLU towards addressing complex, domain-specific\nchallenges with minimal human guidance has propelled LLMs\ninto a pivotal role for next-generation AI systems, positioning\nthem as a cornerstone technology in the quest toward more\ngeneral-purpose AI, and potentially, artificial general intelli-\ngence (AGI).\nBuilding on these advancements, scientists are now begin-\nning to assess separately the suitability and potential impact of\nLLMs on a wide variety of specific tasks within specific fields\nof research and discovery [5]. This work has led to exciting\ndemonstrations of LLMs as transformative tools for such\ntasks predicting molecular properties [6], uncovering genomic\npatterns [7], interpreting astrophysical data [8], solving mathe-\nmatical problems [9], and even creating and manipulating tools\nfor simulations and analysis [10].\nThese developments have also led scientists to envision the\nuse of LLMs and transformers as research assistants that can\nnot only automate individual research tasks but also engage\nwith scientific problems in depth by taking advantage of\ngrowing multi-step reasoning skills that complement their ex-\npanding contextual understanding. This vision suggests a new\nholistic approach in which LLMs interface with relevant tools,\noperate (quasi-)autonomously on research challenges, identify\narXiv:2502.20309v1  [cs.AI]  27 Feb 2025\n\n\nTABLE I\nOUR PROPOSED METHODOLOGY FOR EVALUATING LLMS AS SCIENTIFIC ASSISTANTS COMBINES FOUR COMPLEMENTARY TECHNIQUES, LISTED IN\nCOLUMNS 2–5 BELOW, TO ASSESS THEIR CAPABILITIES. PURPLE TEXT INDICATES PRIOR CONTRIBUTIONS BY THE AUTHORS, BLUE TEXT NEW\nCONTRIBUTIONS IN THIS PAPER, AND BLACK TEXT METHODS ADAPTED FROM EXISTING WORK THAT WE INCLUDE FOR A COMPLETE APPROACH.\nrelevant literature, summarize findings, propose experimental\ndesigns, and even autonomously run, and generate insights\nfrom, physical and computational experiments [11], [12].\nWe identify two main challenges that must be addressed\nbefore LLMs can be broadly adopted by the scientific com-\nmunity as effective and trustworthy research assistants. First,\nresearchers need ways to measure and evaluate LLM capabil-\nities in the different stages and tasks of the scientific research\nprocess. Such evaluations can both guide LLM applications\nand integration with other tools, and provide benchmarks\nfor developers to improve their LLMs and supporting sys-\ntems. Second, as with other research tools and techniques,\nresearchers need ways to assess confidence in the results\nproduced, in order to decide whether or not they are trust-\nworthy. A comprehensive, rigorous, accurate, transparent and\ncommunity-approved evaluation methodology is necessary to\naddress these two challenges.\nThis paper introduces work undertaken at Argonne Na-\ntional Laboratory within the AuroraGPT project to develop\na research methodology that addresses these two challenges.\nThe paper makes three main contributions: (i) a holistic\nmethodology for LLM evaluation; (ii) two novel evaluation\ntechniques (lab-style and field-style experiments); and (iii)\nimprovements in existing state-of-the-art evaluation techniques\nfor the specific role of research assistant.\nFirst, we propose an overarching methodology for assessing\nthe scientific knowledge, skills, and safety of AI models. As\nshown in Table I, this methodology encompasses four com-\nplementary techniques: 1) Multiple Choice Question (MCQ)\nBenchmarks, which measure factual recall and reasoning ca-\npabilities in structured formats to provide fast assessment of\na model’s breadth of knowledge; 2) Open Response Bench-\nmarks, which test a model’s ability to generate detailed open-\nended responses and write or debug code for computational\ntasks giving a fast but more in-depth analysis of knowledge;\n3) Lab-style Experiments, simulating various tasks in the\nend-to-end research process to assess model performance on\nthose tasks and thus to provide understanding of real-world\nstrengths and weaknesses; and 4) Field-style Experiments,\ncapturing real-world interactions at scale to analyze user needs,\nmodel strengths, and broad capability trends, and to diagnose\nareas and sources of weakness in realistic scenarios. We also\nconsider trustworthiness, i.e., alignment with ethical and safety\nstandards, and discuss the Software Infrastructure required to\nimplement these methodologies effectively.\nSecond, our application of “lab-style” and “field-style”\ntechniques to LLM evaluation represents a novel approach\nwhen conducted at this scale and with such a diversity of\ntopics with practicing scientists. These techniques go beyond\nexisting testing methodologies: they assess in real situations\nthe suitability of LLMs for open and unstructured problems\nwhich are both common in research and difficult to assess with\neither MCQs or open-response questions.\nThird, we present improvements to techniques used within\nthe community, including a multi-domain AI for science\nbenchmark, called “AI4S,” and the Skills, Trust, and Relia-\nbility (STaR) evaluation framework, a scalable software eval-\nuation infrastructure. We also summarize and contextualize\nthe research done by our team in domain-specific benchmarks,\nopen response benchmarks, and uncertainty quantification, and\nnote where we employ tools from other teams to complete our\nholistic evaluation methodology.\nTogether, these efforts aim to establish a robust methodology\nfor evaluating the capabilities of LLMs as trusted scientific\nassistants. In the following sections, the collection of eval-\nuations and scoring was based on voluntary participation of\nresearchers and contributors.\nThe following sections describe the related work and the de-\ntails of our proposed methodology. The last section discusses\nnext steps.\n\n\nII. RELATED WORK\nResearch on LLM evaluation encompasses various tech-\nniques relevant for the evaluation of LLMs as research as-\nsistants. Here we discuss that work and note gaps that our\nproposed methodology attempts to fill.\nA. Multiple-choice Question (MCQ) Benchmarks\nMCQ benchmarks offer a structured framework for as-\nsessing LLM performance across various domains. Notable\nexamples are Massive Multitask Language Understanding\n(MMLU) [13] and MMLU-PRO [14], which evaluate general\nknowledge and reasoning in more than 50 subjects, including\nhumanities, sciences and engineering. In the realm of mathe-\nmatical reasoning, GSM8K [15], GSM1K [16], and MATH\n[9] are prominent. GSM8K and GSM1K addresses grade-\nschool level problems, while MATH focuses on high-school\nand competition-level questions. Both benchmarks have been\nenhanced with multiple-choice adaptations to streamline eval-\nuation and minimize ambiguity in model outputs [17]. Other\nsignificant MCQ benchmarks include ARC [18], which tests\nscientific reasoning, and HellaSwag [19], which challenges\nmodels with complex commonsense reasoning scenarios that,\nwhile easy for humans, are especially hard for state-of-the-\nart models. In the field of chemistry, MCQ benchmarks\ninclude MoleculeQA [20], which comprises 61,574 MCQs,\neach with three distractors, focusing on factual information\nabout molecules; MolPuzzle [21], a multimodal benchmark\nwith over 23,000 question-answer pairs, structured with inter-\nlinked sequential sub-tasks, each providing multiple choices;\nand ChemBench [22], with over 2700 questions, primarily\nMCQs. Few of the many other MCQ benchmarking efforts\nin the literature are science-domain focused and validated,\nand as LLM capabilities improve, there is an increasing need\nto generate more difficult questions and leverage synergies\nbetween domain expertize and LLM judges. The multi-domain\nbenchmark AI4S that we present below is an attempt towards\nthat end.\nB. Open Response Benchmarks\nWhile MCQ benchmarks restrict responses to predefined\noptions, Open Response benchmarks require that LLMs pro-\nduce detailed, unconstrained outputs that can be evaluated\nfor coherence, accuracy, and relevance. Notable examples\ninclude NarrativeQA [23], which challenges models to gen-\nerate summaries or interpret longer narratives, and HotpotQA\n[24], which demands multi-hop reasoning with synthesized\nanswers derived from multiple sources. Similarly, HybridQA\n[25] combines textual and tabular data, requiring LLMs to\nprovide coherent and comprehensive answers. For mathe-\nmatical reasoning, GSM8K [15] and MATH [9] test model\nability to solve complex, multi-step problems with free-form\nsolutions. In chemistry, benchmarks have been developed to\nassess LLM open-response capabilities. ChemistryQA [26]\ncomprises 4500 complex questions that require reasoning and\ncalculations, evaluating model ability to generate detailed,\naccurate responses. ChemLLMBenchmark [27] consists of\neight practical chemistry tasks that necessitate understanding,\nreasoning, and explanatory skills, with evaluations focusing on\nthe quality and depth of model-generated answers. The open-\ndomain TOMG-Bench [28] molecule generation benchmark\ncomprises tasks such as molecule editing, optimization, and\ncustomized generation, each requiring models to produce\nspecific molecular structures or modifications based on textual\ndescriptions.\nRecent advancements have expanded the scope of open-\nresponse benchmarks to address specific evaluation challenges\nand domains. The Open-LLM-Leaderboard (OSQ-bench) [29]\ntransitions from multiple-choice formats to open-style ques-\ntions, eliminating issues like selection bias and random guess-\ning, while emphasizing models’ ability to generate coherent,\ncontextually accurate answers. FrontierMath [30] presents\nhundreds of exceptionally challenging mathematics problems\nthat require models to generate detailed solutions, testing\nadvanced reasoning and problem-solving skills. Similarly,\nHumanity’s Last Exam [31] crowdsources complex questions\nfrom experts across fields to evaluate how closely LLMs ap-\nproximate expert-level capabilities, highlighting their potential\nand limitations in addressing real-world challenges.\nA difficulty with Open Response benchmarks is that eval-\nuating their responses is inherently challenging due to their\nunstructured nature, requiring time-intensive analysis, subjec-\ntive interpretation, and careful management of biases and data\noverload. These challenges are closely tied to uncertainty\nquantification (UQ), as the variability in interpretations and\noutcomes necessitates robust techniques to quantify and mit-\nigate uncertainty in the evaluation process, ensuring reliable\nand consistent insights. In addressing these challenges, it is\nalso important to keep track of multiple model versions and\nto use them consistently [32], so as to enable reproducibility\n[33].\nC. Lab-Style Experiments\nLaboratory-style Experiments with LLMs involve controlled\nsettings in which researchers systematically evaluate model\nperformance on specific tasks, enabling precise measurement\nof capabilities and limitations. In chemistry, for instance,\n[34] developed Coscientist, which employs LLMs to plan\nand execute experimental procedures based on simple human\nprompts, and evaluated its performance by assigning it the\ntask of identifying synthetic procedures for seven molecules\nof varying complexity. Similarly, [35] proposed an approach\nthat combines LLMs with task and motion planning to trans-\nlate natural language instructions into robot-executable plans,\nevaluating their system through simulations in a box-packing\ndomain. In behavioral strategy research, [36] reproduced hu-\nman laboratory experiments using LLMs and compared their\nperformance to human participants to analyze the extent to\nwhich LLMs can emulate human decision-making processes.\nThese laboratory-style experiments provide valuable insights\ninto applications and can inform the development of more\nadvanced AI systems. However, comprehensive end-to-end\nevaluations of LLMs on scientific tasks, similar to human\n\n\nperformance assessments, remain scarce. Such evaluations are\ncrucial to understanding how LLMs can mimic or augment\nhuman researchers in tackling complex scientific challenges.\nD. Field-Style Experiments\nField-style Experiments, also referred to as “in-the-wild”\nstudies, involve observing and analyzing user interactions\nwith LLM in real-world settings. This approach contrasts\nwith controlled Lab-style Experiments by capturing non-\npredefined user interactions with LLMs, providing valuable\ninsights into how LLMs perform across diverse, unstructured\ntasks. Recent studies have leveraged this methodology to\nassess various aspects of LLM performance. For instance,\nWildBench [37] is designed to evaluate LLMs using real-\nworld user interactions, enabling a comprehensive analysis of\nmodel capabilities in practical scenarios. Similarly, HaluEval-\nWild [38] was designed to evaluate hallucinations in LLMs\nby collecting challenging user queries from real-world inter-\nactions. Shen et al. [39] conducted a study characterizing and\nevaluating user-LLM interactions, providing insights into user\nbehavior and model performance and highlight the importance\nof understanding user needs and expectations to improve LLM\nutility and user satisfaction. While such analyses are currently\nlacking in scientific domains, developing field-style experi-\nments specifically for scientific research presents a significant\nopportunity to provide critical insights into how researchers\ninteract with AI models, thereby enhancing the creation of AI\nassistants tailored to scientific inquiry.\nE. Safety Evaluation\nIn addition to the general-purpose evaluations discussed\nabove, comprehensive evaluations must rigorously assess\nalignment with ethical standards, robustness against jail-\nbreaks, and adaptability to complex real-world scenarios [37],\n[40]. Safety evaluations are also cross-cutting :MCQs, Open-\nResponse benchmarks, and Lab-style and Field-style Experi-\nments. SafetyBench [41], for example, employs 11,435 MCQs\nin seven categories (e.g., bias, toxicity) to systematically\ntest model adherence to ethical and safety standards in both\nEnglish and Chinese. Similarly, SALAD-Bench [42] proposed\n4000 MCQs, and part of the larger dataset structured into a\ndetailed hierarchy of 6 domains, 16 tasks, and 66 categories.\nBigBench [43] has a subset of its tasks focused on safety\nevaluation in regards to toxicity, bias and truthfullness tat are\nMCQs. However, there has not been a focused MCQ-based\nsafety evaluation benchmarks that take the nuances aspects\nof science, especially high consequential ones such as the\nthe chemisty, biology, radiation, and nuclear (CBRN). We\ndiscuss one such effort on risks in chemistry in this work.\nOpen-response evaluations, such as those in DecodingTrust\n[44] and TrustLLM [45], examine nuanced safety aspects, in-\ncluding hallucinations, privacy violations, and machine ethics.\nTrustLLM evaluates LLMs across six dimensions, including\nfairness and safety, using over 30 datasets to identify crit-\nical safety gaps, while DecodingTrust introduces an eight-\ndimensional framework that probes issues like toxicity and\nethical reasoning, with results published on a widely accessible\nleaderboard for transparency. However, such evaluations for\nscientific use cases are scarce [40] and thus provide an\nopportunity to develop them in the future.\nThe safety red-teaming methodologies can be effectively\ninterpreted along the lines of lab-style and field-style exper-\niments. In lab-style red-teaming, researchers interact directly\nwith LLMs and systematically introduce adversarial prompts\nto identify vulnerabilities such as biases, hallucinations, or\nethical compliance issues. For example, [46] present a frame-\nwork for red-teaming experiments on LLMs by generating\nnumerical questions and puzzles to evaluate the models’ per-\nformance on elementary calculations and algebraic tasks. This\napproach provides detailed feedback at each step, allowing\niterative improvements and a deeper understanding of model\nlimitations. In contrast, field-style red-teaming assesses LLMs\nby analyzing large-scale human interactions in real-world\nenvironments. This method captures diverse and unpredictable\nuser inputs, offering insights into how models perform “in the\nwild.” For instance, [47] discuss automating red-teaming by\ntraining a separate red team LLM with reinforcement learning\nto generate test cases that maximize the chance of eliciting\nundesirable responses from the target LLM. This large-scale\napproach identifies practical weaknesses and vulnerabilities\nthat may not surface in controlled lab settings, contributing\nto model robustness across varied real-world scenarios. By\nemploying both lab-style and field-style red-teaming strategies,\nresearchers can comprehensively evaluate and enhance the\nsafety, reliability, and ethical performance of LLMs across\ndifferent contexts. That said, significant work still needs to\nbe done in designing such experiments for safety and trust\nscenarios in science.\nIII. ESTABLISHING A METHODOLOGY TO EVALUATE\nLLMS AS RESEARCH ASSISTANTS\nThe overarching objective of using LLMs as research assis-\ntants is to accelerate the research process.\nIn Table II, we present seven tasks commonly performed\nby researchers to solve a scientific problem: (i) posing and\nformulating a research question; (ii) if needed, conduct-\ning initial, preliminary experiments, observations, simulation,\nand/or database accesses to confirm the pertinence of the\nresearch question; (iii) performing literature search to identify\nrelated work; (iv) generating potentially multiple hypotheses\n(or research directions) to address the research question; (v)\ndesigning experiments, observations, and/or simulations to test\nthe hypotheses; (vi) analyzing the resulting data to validate\nor invalidate the hypotheses; and (vii) writing a report about\nfindings. Each of these seven tasks requires deep reason-\ning, contextual understanding, and iterative problem solving.\nThe scientific community needs confidence that LLMs are\nproficient in these tasks if they are to adopt them widely\nas research assistants. Thus a thorough and comprehensive\nevaluation methodology is required that can produce a rigorous\nassessment of LLM strengths and weaknesses in each task.\n\n\nTABLE II\nSKILLS EVALUATED BY EACH OF THE EVALUATION TECHNIQUES. LAB-STYLE EXPERIMENTS FOCUS ON DETAILED ANALYSIS IN CONTROLLED\nENVIRONMENT. FIELD-STYLE EXPERIMENTS FOCUS ON ANALYZING RESEARCHERS-LLMS INTERACTIONS AT SCALE IN NATURAL SETTING.\nAs discussed in the related work section, most LLM evalua-\ntions take the form of MCQ benchmarks, which are a subset of\nbroader Q&A evaluations. However, while these benchmarks\nserve a useful purpose in quickly assessing the breadth of\nknowledge of LLMs, that cannot, by their very nature, assess\nthe depth of reasoning, contextual understanding, and iterative\nproblem-solving required in the various steps of the scientific\nresearch process [48], [49].\nTo address these limitations, it is critical to complement\nMCQ benchmarks with end-to-end evaluations in realistic\ncontexts that assess both reasoning across multiple dimensions\nand the ability to plan and adapt across multi-step tasks.\nTherefore, we propose Evaluating AI models as scientific\nResearch Assistants (EAIRA), a structured evaluation method-\nology (Table I) that combines four techniques: two existing\napproaches that allow for quick and repeated assessment of\nthe breadth of LLM abilities (MCQ Benchmarks and Open-\nResponse Benchmarks) plus two new approaches for end-\nto-end evaluation of LLMs as scientific assistants (Lab-style\nExperiments and Field-style Experiments).\nMCQ benchmarks serve to test foundational knowledge\nand domain-specific expertise across diverse scientific fields.\nIn addition to previously developed benchmarks, members\nof our team have developed three new MCQ benchmarks to\naddress topic gaps in tasks for which MCQs are well suited.\n1) Astro and 2) Climate are domain-specific benchmarks\nprimarily generated through automated techniques to ensure\nthe scalability of benchmark generation to serve new and\nevolving topics in science. 3) AI4S is a multi-domain “AI\nfor science” MCQ benchmark, which integrates human ques-\ntion generation and validation with automated generation and\nvalidation methods to achieve a balanced, high-quality dataset\nthat can be developed with moderate effort. By leveraging both\nhuman and automated techniques, we can assess the strengths\nand weaknesses of automated methods deployed in Astro and\nClimate in order to guide future research in improving their\ngeneration.\nOpen-response benchmarks serve to test more detailed\nknowledge and to generate open-ended responses or code that\nassesses a model’s dynamic capability to handle unstructured,\ncomplex tasks, moving closer to reflecting the flexibility and\nadaptability required in real-world research scenarios while\nstill facilitating a fast, automated evaluation. For this, our\nteam has previously developed two benchmarks: SciCode,\nwhich assesses the ability of LLMs to develop code which\nis highly dependent on the knowledge of the context of a\nscientific domain to correctly implement, and ALDbench,\nwhich assesses the ability of LLMs to describe methods for\nsynthesizing materials using atomic layer deposition.\nLab-style Experiments perform evaluation by domain ex-\nperts of AI models as they assist across all research tasks\nin real situations. This expert-reviewed method provides a\ncomprehensive assessment of the relevance, effectiveness, and\niterative improvement of a model over time (e.g., 4-12 hours\nper attempt). This technique goes beyond open response by\nusing a human proctor to guide an expert to iteratively interact\nwith the model in order to assess multi-stage planning and\nresponse to results in the context of a scientific domain.\nBecause of the interactivity, it provides very granular feedback\nabout what models are or are not capable of performing giving\na unique insight into the strengths and weaknesses of models\nand possible paths for improvement. However, this capability\ncomes at the cost of extensive effort by experts and proctors.\nField-style Experiments (inspired by, and adapting “in-the-\nwild” evaluations [39], [50] for the scientific context) ana-\nlyze automatically thousands of prompts, responses, and user\nbehaviors during real-world interactions between researchers\nand AI assistants. This approach allows for large-scale capture\nand analysis of user needs, model strengths and weaknesses,\nand usage trends. It differs from lab-style experiments in\n\n\nthat: 1) experts are not guided by a proctor, 2) analysis of\ninteractions and feedback is automated, and 3) experiments\ncan be conducted in the background as experts perform routine\ntasks (e.g., by capturing outputs of a site-wide LLM inference\nservice or API proxy). These differences greatly improve the\nscalability of realistic end-to-end experiments but at the loss\nof the fine-grained granularity of the strengths and weaknesses\ndetected by the lab-style approach.\nIn applying these four techniques, three key cross-cutting\naspects must be considered: ethics, trust and safety; reliable\nuncertainty quantification; and scalable software infrastructure.\nThese three aspects ensure that LLMs are “aligned” with\nhuman values, produce and qualify results correctly under\nuncertainty; and can be evaluated efficiently at scale. Trust\nand safety evaluations must address ethical alignment, defend\nagainst jailbreak attempts, and adapt to complex real-world\ncontexts. Multiple-choice strategies [41] can identify biases,\ntoxicity, and compliance gaps, while open-response tasks [44]\ncan probe subtle issues such as hallucinations and machine\nethics. Lab-style red-teaming [46] systematically challenges\nthe model with adversarial prompts in controlled settings\nto expose stepwise weaknesses, while field-style red-teaming\n[47] tests the model’s robustness amid unpredictable real-\nworld inputs, unveiling vulnerabilities that may remain hidden\nin laboratory conditions. Reliable uncertainty quantification\n(UQ) is equally critical to establish trust in AI-driven sci-\nentific research assistants, as it systematically gauges model\nconfidence and highlights potential inaccuracies [51]–[53]. UQ\ninsights guide targeted refinements in MCQ, open-response,\nand lab- or field-style evaluations, ensuring that areas of\nhigh uncertainty are addressed in scientific and real-world\ncontexts [54], [55]. Together, robust safety evaluations and UQ\nfoster transparency, accountability, and trust, facilitating the\nresponsible integration of LLMs into critical scientific research\n[40], [56]. Finally, the scalable software infrastructure enables\nrapid evaluation to keep pace with rapid changes in the field of\nAI research. The framework needs to incorporate distributed\ntask parallelism [57] and fast inference capabilities [58].\nThe following subsections present greater detail and results\nof the four techniques and three aspects of our evaluation\nmethodology.\nA. Domain-Specific MCQ Benchmarks\nDomain-specific benchmarks are crucial for evaluating\nLLMs in specialized fields, as they address the limitations of\ngeneral benchmarks that often fail to capture the complexities\nof domain-specific tasks. Without tailored benchmarks [8],\nLLMs risk over training on well-established datasets, leading\nto inflated performance that does not translate to real-world\napplicability. These benchmarks are essential for guiding tar-\ngeted improvements, ensuring that models meet the specific\ndemands of scientific research, and providing a baseline to\nunderstand their strengths and weaknesses. By capturing the\nnuanced challenges of individual domains, such benchmarks\nfoster the effective and ethical deployment of LLMs, enabling\ntheir potential to accelerate discovery and innovation across\ndisciplines. We now discuss the domain-specific benchmarking\nefforts that we conducted in Astronomy and Climate modeling.\n1) Astronomy: The Astronomy Benchmark [8] assesses\nLLM performance in a manner that reflects the interdisci-\nplinary nature of astronomy, testing both factual recall and\nthe ability to connect insights across subfields. This benchmark\nwas generated automatically using an LLM to compose MCQs\nfrom astronomy papers. To assemble a rich repository of\nscientific knowledge, we leveraged the Annual Review of\nAstronomy and Astrophysics (ARAA), a selective review\njournal renowned for its comprehensive overviews authored\nby leading experts.\nThe Nougat optical character recognition (OCR) tool was\nused to transcribe 885 ARAA articles over the years 1963 to\n2023. Each transcribed paper was processed using Gemini-1.5-\nPro, a long-context LLM capable of handling up to one million\ntokens, to generate five multiple-choice questions (MCQs)\nper paper. The questions were designed to be specific, yet\nindependent of the article sections, with generalized answers\nand balanced options to avoid bias. This process yielded a\ntotal of 4425 MCQs covering diverse topics such as quasar\ndensity decline at high redshifts and subgrid feedback model\ncalibration in simulations.\nSample question from Astronomy benchmark dataset\nHow does the presence of stellar companions influence\nthe formation and detection of exoplanets?\n(A) Stellar companions can dilute transit signals, potentially\nleading to misclassification of planets and inaccurate pa-\nrameter estimations. Additionally, their gravitational influ-\nence can suppress planet formation in close binary systems.\n(B) Stellar companions provide additional sources of gravita-\ntional perturbations, enhancing planet formation by promot-\ning planetesimal accretion and facilitating the formation of\ngas giants.\n(C) Stellar companions contribute to the metallicity enrichment\nof planetary systems, leading to the formation of more\nmassive and diverse planets, including super-Earths and hot\nJupiters.\n(D) Stellar companions act as gravitational lenses, increasing\nthe detectability of exoplanets through microlensing events\nand enabling the discovery of planets at greater distances\nfrom their host stars.\nThe Astronomy Benchmark has been then used to assess\nboth the accuracy and computational cost of dozens of dif-\nferent closed and open LLM variants. This study revealed\ndisparities in LLM performance across general-purpose and\nspecialized tasks that highlight significant performance gaps\nand performance-to-cost ratios. While frontier models like\nClaude-3.5-Sonnet excelled in the Astronomy Benchmark with\nan accuracy of 85.0%, outperforming GPT-4o (80.4%) and\nGemini-1.5-Pro (77.6%), these differences are less evident in\ngeneral-purpose benchmarks such as MMLU [29]. A study of\nhow Astronomy Benchmark performance varied with compute\ncosts showed that, roughly speaking, each 3.5 percentage\npoints increased accuracy was associated with 10-fold increase\nin price, within most given series of models such as GPT,\n\n\nGemini or Claude. An analysis of the cost per 0.1 M tokens\nshowed that the cost for a desired performance can vary by\nmore than three orders of magnitude across different models:\nsee Figure 2 in [8].\nThe study also showed that open-weight models, though\nimproving, lag behind proprietary ones, with older versions\nunderperforming by as much as 30% on specialized tasks.\nPerformance also varies significantly between English-focused\nand non-English-focused models, with the latter struggling in\nareas like theoretical astronomy and advanced instrumentation,\nreflecting gaps in training datasets. However, recent astronomy\nsubfields, such as post-1990 advancements, exhibit narrower\nperformance gaps which may be due to model’s ability to\nhandling historical context or older scientific consensus. These\nfindings emphasize the importance of domain-specific bench-\nmarks to assess not only performance in specialized tasks but\nalso the performance-to-cost ratio crucial for user adoption in\nassisting with scientific research. This work also shows that\nperformance varies across sub-fields.\n2) Climate: Climate and weather forecasting presents mul-\ntifaceted challenges that demand interdisciplinary knowledge\nand reasoning, making it an ideal testbed for evaluating\nLLM capabilities. However, existing benchmarks for climate\nscience are limited [59]–[61]. Thus we developed a Climate\nBenchmark, a set of MCQs focused on the urgent and complex\ndomain of climate science. As the manual creation of such\nMCQs is labor intensive and climate scientists are already\novercommitted, we employed automated methods to develop\nthe Climate Benchmark. We adopted as our source material the\nIntergovernmental Panel on Climate Change (IPCC) reports\n[62], which are authoritative assessments synthesizing the\nlatest scientific research on climate change, its impacts and\npotential solutions. These reports serve as a foundation for\ninformed decision-making and international negotiations on\nclimate action, highlighting the urgency of addressing the\ncomplex and interconnected challenges posed by changing\nclimate. The reports are typically extensive, often exceeding\n1000 pages, with each chapter and section addressing specific\ntopics related to climate change. To generate questions on the\nvarious topics discussed in the report, we parsed the PDFs\nsection-wise, using Nougat as was done for the Astronomy\nBenchmark. We then employed OpenAI’s GPT4 to create one\nmultiple-choice question, consisting of one correct answer and\nfour distractors, for each section, with the prompt designed\nto create an MCQ based on the provided scientific text,\nensuring that the question evaluates a broader understanding\nof climate science principles without referencing the specific\nreport. This process resulted in a total of 752 questions on\ndiverse topics, ranging from factors influencing vulnerability\nto climate change to primary strategies for risk reduction.\nThis systematic approach ensured comprehensive coverage and\nalignment with the content of the IPCC reports.\nWe employed the Climate Benchmark to assess the per-\nformance of multiple LLMs, including GPT-4o, Llama 3.8B,\nand Phi 4, on specialized climate science tasks. Among these\nmodels, GPT-4o performed the best, achieving an accuracy of\n87.34%, demonstrating its effectiveness in handling complex\nclimate-related tasks. GPT-4o was followed by Llama 3.8,\nwhich achieved an accuracy of 78.48%, and Phi 4, which\nscored 54.43%. This performance disparity highlights the need\nfor continued refinement and optimization of models to bridge\nthe gap in specialized applications.\nThe development of the Climate Benchmark provided useful\ninsights into the creation and evaluation of MCQ datasets\nfor scientific applications. The climate-focused MCQs, derived\nfrom IPCC reports, were designed to assess knowledge recall\nand decision-making, emphasizing accurate understanding of\nscientific concepts. While primarily testing factual knowledge,\nthese questions establish a strong foundation for expanding\ninto tasks that require more complex reasoning and application\nin climate science. However, the automatic generation of\nMCQs sometimes produced semantically similar questions that\ndiffered only slightly in phrasing or structure while testing\nthe same core information. This observation highlights the\nneed for robust evaluation mechanisms to eliminate redun-\ndancy and ensure diversity within the dataset. While automatic\nMCQ generation greatly accelerates the creation of benchmark\nquestions, it cannot replace a rigorous evaluation process. A\ncombination of LLM-based evaluators and human oversight\nis crucial for maintaining the benchmark’s quality, relevance,\nand accuracy, ensuring that it meets the standards required for\nresearch-focused benchmarks.\nB. Scalable AI4S MCQ Benchmark\nThe overarching goal of the AI4S benchmark is to evaluate\nthe knowledge extension of LLMs across many different\nscientific domains. In that respect, it is similar to GPQA [63].\nHowever, AI4S design focuses on the quality and scalability\nof MCQ generation and validation. While GPQA has only 448\nMCQs, our objective is to generate and validate thousands of\nMCQs and continuously add MCQs as LLMs progress in their\ncapabilities.\nCurrent MCQ benchmarks, including GPQA, have two main\nlimitations: 1) they are quickly saturated because of the fast\nprogress of LLMs. 2) there is a high risk of contamination\n(benchmark included in the training sets) if the MCQs are\nmade public. These two limitations arise from the static aspect\nof the current benchmarks. They are developed once and do\nnot evolve. The current practice is to develop new versions [64]\nwhen the initial benchmark is saturated [65] and to open only\na portion of the benchmark MCQs to avoid contamination.\nTo address the two limitations, we explore a novel approach\nto develop scalable generation and validation of MCQ bench-\nmarks for the evaluation of LLMs capabilities: Automatic\ncontinuous Generation and validation of Increasingly Large\nMCQ benchmarks: AGIL.\nBuilding on the experience gained from the Astronomy and\nClimate domain-specific benchmarks, we are developing an\nAGIL process by which we combine manual and automated\nmethods to generate and validate MCQs.\nWe present here our initial finding towards the creation\nof a 1000-MCQ AI4S benchmark that spans five scientific\n\n\nFig. 1. The AGIL approach to generate scalable MCQ benchmarks. The current version of the AI4S benchmark contains only manually accepted MCQs.\nThe AGIL approach enables the integration of automatically accepted MCQs after the validation of their difficulty and quality.\ndomains: Computer Science, Astrophysics, Climate, Physics,\nand Chemistry. Our goal with this first study is to assess\nkey aspects of the AGIL approach: validity of the difficulty\nlevel, quality of the generated MCQs compared to GPQA\nones, quality of the automatically generated MCQs compared\nto manually generated ones, and quality of the automatically\nvalidated MCQs.\nIn the AGIL approach (Figure 1), manual MCQ generation\nand validation by experts is critical to provide high-quality,\ndomain-specific MCQs that serve as a gold standard for\nevaluating LLM performance and developing automatic MCQ\ngeneration and validation workflows. The goal of automated\ngeneration and validation (LLM as a judge technique [66]) by\nLLMs is to address the scalability issues of manual generation\nand validation while keeping their quality levels.\nFor the manual generation and validation of MCQs we\norganized hackathons which engaged 140 domain experts\n(PhDs and other research staff) 1. We offered the participants\nto generate MCQs from scientific papers of their choosing,\nincluding their own. The manually generated MCQs were\ncrafted by using a purpose-built authoring interface (Appendix,\nFigure 9) that allowed contributors to test their questions on\nsmaller models like Llama 3 before submission so as to ensure\na baseline difficulty threshold. Manual validation used another\nspecifically created form (see Appendix, Figure 10). The tool\nused for the MCQ generation and validation is available on\ngithub (https://github.com/auroraGPT-ANL/questions-ui)\nAutomated MCQ generation leveraged LLMs such as GPT-4\nwith domain-specific prompts to create MCQs from scientific\n1the exercises in this paper were done with volunteer Argonne employees,\nwho understood that the goal of this effort is to identify opportunities to\nimprove models. While the research team offered a rubric for notes and\nobservations, the participants were free to use whatever rubric they preferred.\nThe following is the general approach that was used.\npapers across fields such as climate science, physics, and\nchemistry, with validation guided by the LLM-as-a-Judge\ntechnique. Both the prompts used for automatic MCQ gen-\neration and validation, as well as the rubrics for validation\nand reviews, were informed by the manual generation and\nvalidation, as well as by the experience gained during the\ndomain-specific benchmark development discussed earlier.\nThis process generated 980 MCQs (720 manually and 260\nautomatically). Of 588 total manually generated reviews, 317\nMCQs have been assessed so far, of which 254 were accepted.\nAcceptance was subject to the following criteria: appropriate\ndifficulty, relevant, complete and correct answers and distrac-\ntors, controversial answers, mathematic requirements, as well\nas relevant skills and domain selection. The small percentage\nof accepted MCQs 25% illustrates the difficulty of generating\nand validating high-quality scientific MCQs.\nIn addition, domain experts categorized the accepted MCQs\ninto easy, medium, and hard levels, capturing a spectrum of\ndifficulty that mirrors real-world scientific challenges. This\nmulti-level structure enables the AI4S benchmark to evaluate\nboth the foundational and advanced capabilities of LLMs,\noffering an assessment of their strengths and limitations.\nTo evaluate the merits of our AGIL approach, we performed\nseveral tests to evaluate the quality of (i) the level classifica-\ntion, (ii) the AI4S benchmark compared to GPQA, (iii) the\nautomatically generated MCQs, and (iv) the automatic MCQ\nvalidation.\nFor all tests, we used the STaR framework (see section VI)\nwith five shots. The first row of the table Table III shows\nthe overall performance of Llama 3 8B on the 254 MCQs.\nThe resulting accuracy of 20% correspond to a random guess.\nThe next three rows show results for MCQs grouped by\ntheir human-identified levels of difficulty. We see that the\n\n\nTABLE III\nPERFORMANCE METRICS FOR LLAMA 3 MODELS ACROSS VARIOUS AI4S BENCHMARK LEVELS.\nModel\nTask\nnsamples\nacc (stderr)\nacc norm (stderr)\nLlama-3-8B\naccepted\n254\n0.2008 (±0.0252)\n0.2717 (±0.0280)\nLlama-3-8B\naccepted easy\n81\n0.2222 (±0.0465)\n0.3210 (±0.0522)\nLlama-3-8B\naccepted medium\n116\n0.1983 (±0.0372)\n0.2672 (±0.0413)\nLlama-3-8B\naccepted hard\n57\n0.1404 (±0.0464)\n0.1930 (±0.0527)\nLlama-3.1-8B\naccepted\n254\n0.1969 (±0.0250)\n0.2638 (±0.0277)\nLlama-3-70B\naccepted\n254\n0.2598 (±0.0276)\n0.3701 (±0.0304)\nLlama-3.1-70B\naccepted\n254\n0.2520 (±0.0273)\n0.3386 (±0.0298)\nLlama 3 8B results are consistent with human-identified diffi-\nculty levels, with Llama 3 8B achieving the best score on easy\nMCQs (22%) and significantly below random performance on\nhard MCQs (14%). These results validate the quality of the\nlevel classification. The next three rows of Table III show the\nperformance of other Llama-3 models for the AI4S accepted\nMCQs. We observe that Llama 3-70B performs less well\non all AI4S-accepted MCQs (26% of questions answered\ncorrectly on all difficulty levels) than on GPQA (49% of\nquestions answered correctly). Note that while GPQA has one\ncorrect answer and three distractors, meaning 25% accuracy\nfor random responses, AI4S MCQs have four distractors and\nthus only 20% random response accuracy. We conclude that\nAI4S is a more challenging LLM benchmark than GPQA.\nTo obtain a finer quality comparison between AI4S and\nGPQA MCQs, we use automatic MCQ validation to quantify\nthe quality of every MCQ on the first seven criteria (N/A value\nfor the eighth criterion on GPQA). Table IV shows the scores\nof the two benchmarks on the seven criteria. On average, AI4S\nMCQs (average of 4.55) reach the same quality level as the\nGPQA MCQs (average of 4.45).\nTABLE IV\nMEAN AND STANDARD DEVIATION (SD) SCORES FOR AI4S AND GPQA\nACROSS VARIOUS CRITERIA.\nItem\nAI4S Mean (SD)\nGPQA Mean (SD)\nAppropriate\n3.68 (0.72)\n4.28 (0.58)\nComplete\n4.52 (0.87)\n4.42 (0.75)\nControversial\n4.97 (0.19)\n5.00 (0.04)\nCorrect\n4.60 (1.36)\n4.21 (1.82)\nDomains\n4.68 (0.73)\n4.97 (0.25)\nMathematic\n4.81 (0.95)\n3.50 (2.30)\nRelevant\n4.64 (0.56)\n4.81 (0.42)\nSkills\n3.97 (0.70)\nN/A\nOverall, these results validate the quality of AI4S compared\nto GPQA.\nWe used the acceptance criteria to compare the quality\nof the manually and automatically generated MCQs. This\ncomparison reveals that the quality of manually validated,\nautomatically generated questions is on par with manually\nvalidated/generated ones. Overall these results show that au-\ntomatic MCQ generation can be used to generate MCQs with\na manual verification step.\nTo evaluate the quality of automatic MCQ validation, we\ncompare it with manual validation. We used Mistral Large 2\nas the judge, prompting it to evaluate each MCQs on a scale\nof 1–5 for each of eight criteria and also asking it to provide\nconcise rationale for each score. All criteria were evaluated\nin a single prompt. We expanded each criteria specification\nto define each level on the scale for all criteria explicitly. The\nprompts used for the judge are in the Appendix, Figure 17. We\nobserve the accuracy of a model trained on these judgements to\npredict whether a question will be accepted/rejected to be 72%.\nFor comparison, of the 144 questions with multiple reviews,\nonly 61% of reviewers agreed on acceptance.\nDuring the development of the AI4S benchmark using\nour proposed AGIL approach, several key lessons emerged.\nGenerating high-quality scientific MCQs manually is a chal-\nlenging task for multiple reasons: crafting questions with dis-\ntinguishable levels of difficulty (undergraduates, PhD students,\npostdocs, and staff) is non-trivial, and creating distractors\nthat are plausible but not overly confusing requires significant\neffort and precision. Validation of these questions proved even\nmore demanding, as finding appropriate reviewers for difficult\nand specialized topics can be challenging, and ideally, each\nquestion requires validation by three experts to ensure relia-\nbility. Our goal is to continue the development of the AGIL\napproach to address this difficulty issue and to continuously\ngenerate AI4S MCQs from the large pool of available scientific\npapers. We will release the benchmark using a sliding-window\napproach, keeping a significant portion of newly generated\nMCQs (e.g. 70%) not public.\nC. Open Response Benchmarks\nNext we discuss the open-ended benchmarks. These are\nessential for evaluating the reasoning, creativity, and problem-\nsolving abilities of LLMs, particularly in scientific domains.\nUnlike MCQs, which primarily test factual recall or single-\nstep reasoning, open-ended tasks engage models with com-\nplex, real-world problems that require multi-step reasoning,\nsynthesis of knowledge across domains, and adaptive problem-\nsolving. For instance, while an MCQ might test a model’s\nrecall of a specific scientific fact, an open-ended task could\nrequire the model to design an experiment, analyze data, or\npropose solutions to unsolved research questions. This format\naligns better with the exploratory nature of scientific inquiry,\noffering a more comprehensive assessment of a model’s capa-\nbilities. However, existing open-ended benchmarks often fall\nshort in capturing the depth and realism needed for scientific\nevaluations. Many rely on synthetic tasks that fail to reflect the\nintricacies of real-world scientific challenges, such as multi-\ndisciplinary reasoning or generating accurate code for practical\napplications.\n\n\nWhile open responses questions are versatile in capability,\ntheir evaluation is more complicated compared to MCQs.\nDifferent assessment approaches are applied, depending on the\ntask at hand. A first class of statistical scorer approaches looks\nat co-occurrence of n-grams (sequences of letters or words)\nin LLM outputs vs. supplied ground truth responses. In this\ncontext, widely used scores are BLEU (BiLingual Evaluation\nUnderstudy) compares LLM outputs against ground truths. It\ncalculates the precision for each matching n-gram (n consec-\nutive words) between an LLM output and expected output.\nROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nevaluates text summaries and computes recall by comparing\nthe overlap of n-grams between LLM outputs and expected\noutputs. It determines the proportion (0–1) of n-grams in the\nreference present in the LLM output. METEOR (Metric for\nEvaluation of Translation with Explicit Ordering) calculates\nscores by assessing both precision (n-gram matches) and recall\n(n-gram overlaps) and leverages linguistic databases to account\nfor synonyms. Statistical scorers do not take any semantics and\nreasoning capabilities into account.\nA second class are embedding approaches that seek to com-\npare the semantics of LLM responses and reference answers.\nHere, some widely used scores include BERTScore, that relies\non pre-trained language models (e.g., BERT) and computes\nthe cosine similarity between the contextual embeddings of\nthe LLM responses and references. These similarities are\nthen aggregated to produce a final score. Other tools such as\nCheckEmbed [67] can be used to compare the semantics of\nLLM responses and reference answers. The third and most\nrecent class is the LLM-as-a-judge methods which tackle\nthe problem of evaluating LLM open responses when no\nreference answer is available. This approach currently has\ntwo variations. In the “Pairwise comparison” version, an LLM\njudge is presented with a question and two answers, and tasked\nto determine which one is better or declare a tie. In the “Single\nanswer grading” version, an LLM judge is asked to directly\nassign a score to a single answer. In principle, LLM-as-a-\njudge can offer several key benefits: consistency, scalability,\nand explainability. However, the approach also has limitations:\nposition bias (first answer better in “Pairwise comparison”),\nverbosity bias (longer answer better), self-enhancement bias\n(self-generated answer better) and limited capability to grade\nmath and reasoning questions [66]. Moreover, the reliability\nof such evaluations is still the subject of research.\nWe now discuss two open-ended benchmarks, one for sci-\nentific code generation and another for atomic layer deposition\nin Material Science.\n1) SciCode - Scientific Code Generation Benchmark:\nThe SciCode Benchmark is a set of manually curated coding\nproblems designed to assess LLM capabilities for solving\ncomplex scientific coding problems across diverse domains.\nBy providing tasks that reflect real-world challenges and re-\nquire multi-step reasoning, SciCode allows models to be tested\nin contexts that align closely with the demands of scientific\nresearch [68]. SciCode includes problems across a range of\nscientific domains, including computational mechanics, quan-\ntum information, quantum chemistry, ecology, and molecular\nmodeling. It consists of 80 main problems, decomposed into\n338 intermediate steps, enabling a structured approach to\nassessing model capabilities. Solving each individual problem\nrequires that an LLM implement multiple Python functions\ncorresponding to subproblems and integrate those functions\ninto a cohesive solution: see Figure 14. Each problem is\naccompanied by a gold-standard solution and multiple test\ncases so as to permit robust and reliable automatic evaluation.\nEach SciCode problem is meticulously annotated and ver-\nified by at least two senior researchers to ensure accuracy,\nand is drawn from real-world research tasks, maintaining\nrelevance to practical applications. Problems are curated to\navoid overlap with publicly available datasets and thus to\ntest the deep scientific knowledge and analytical skills of\nLLMs by requiring the decomposition and integration of\ncomplex problems into comprehensive solutions. Additionally,\nSciCode allows for flexible evaluation of model capabilities in\nvaried setups, enabling adjustments like providing background\ninformation or conditioning on previous solutions.\nThe SciCode Benchmark is configured to assess LLM\ncapabilities to solve SciCode problems by using zero-shot\nprompts, maintaining a general approach while creating dis-\ntinct prompts for various evaluation setups to guide the model\non the tasks, as described in detail in [68]. The prompts\nremain consistent across models and fields, incorporating\ninstructions for the main and sub-problems, as well as code\nfrom previous subproblems. We evaluated the coding capa-\nbilities of several state-of-the-art LLMs using the SciCode\nbenchmark, focusing on three key aspects to assess their\nperformance. First, the Impact of Scientific Background was\nanalyzed by testing models in two modes: without background,\nto evaluate inherent scientific knowledge and reasoning, and\nwith background, to focus on coding and instruction-following\ncapabilities. The results showed significant performance im-\nprovements with background information, highlighting the\nlimitations of current LLMs in scientific reasoning. Second,\nthe comparison between Gold vs. Generated Solutions revealed\ninsights into the challenges of realistic evaluations. While gold\nsolutions accurately address each problem, generated solutions\nintroduce error accumulation, creating a more practical and\ndemanding evaluation scenario. Lastly, the assessment of Main\nvs. Subproblems provided a nuanced understanding of model\nperformance. A main problem was considered solved only\nwhen all subproblem solutions and the integrated result were\naccurate. Additionally, SciCode’s design allows independent\nevaluation of subproblems, enabling precise analysis of mod-\nels’ reasoning and coding abilities across discrete tasks. These\nevaluation dimensions underscore the benchmark’s rigor in\ntesting LLMs for real-world scientific applications.\nWe summarize the findings of our studies using several\nstate-of-the-art models in Figure 2. These results show that\nSciCode is a difficult benchmark for current LLMs. Consistent\nwith our observations on proprietary models, open-weight\nLLMs under test also showed their lack of capabilities in\n\n\nFig. 2. The performance of various LLMs on SciCode problems. This histogram displays the accuracy (vertical axis, 0% to 100%) of various state-of-the-art\nLLMs (listed on the horizontal axis) in solving both main problems (red) and their associated subproblems (blue) within SciCode. To solve a main problem,\nLLMs must implement one Python function per subproblem and integrate them into a comprehensive solution. SciCode provides gold-standard solutions and\nmultiple test cases for reliable automatic evaluation. These consistently poor results highlight the need for LLMs that incorporate scientific knowledge and\nadvanced reasoning to better assist researchers.\nsolving any main problem despite being able to solve a number\nof sub-problems correctly.\nThe SciCode project provides insights into the challenges\nof evaluating LLMs in scientific coding tasks, highlighting\nsignificant gaps in current capabilities. Despite recent advance-\nments, state-of-the-art models like OpenAI’s o1-preview and\nClaude3.5-Sonnet solve only a small fraction (7.7%) of the\nmain problems, underscoring the disparity between existing\nLLMs and the deep scientific reasoning required for real-world\nresearch. SciCode is designed to address this gap by focusing\non real-world, research-level problems across diverse natural\nscience fields, including mathematics, physics, chemistry, and\nbiology. Sourced from peer-reviewed work, these problems\ntest LLMs’ ability to generalize to less familiar domains. By\ndecomposing problems into subproblems with detailed annota-\ntions, SciCode rigorously evaluates models’ coding, reasoning,\nand knowledge integration capabilities. While providing scien-\ntific background information improves model performance, the\npersistent struggle of LLMs with these tasks emphasizes their\ncurrent limitations in handling complex scientific challenges.\nThe project highlights the importance of high-quality data,\ndomain-specific validation, and carefully curated problems to\nadvance the development of AI tools for scientific research.\nThe findings indicate substantial progress is needed to enhance\nscientific reasoning and background knowledge integration\nin LLMs to enable their effective application in real-world\nscenarios.\n2) ALDbench - Materials Synthesis Benchmark: An\narea that lacked relevant benchmarks is materials synthesis.\nThis is particularly important for potential applications of\nLLMs in automated materials discovery or as AI research as-\nsistants. LLMs underpinning such capabilities need to exhibit\nboth the ability to reason about specific processes (for instance\nto avoid unsafe conditions or transfer ideas across reactors and\nprocess conditions) and have a robust understanding of the\nliterature (to build on existing process knowledge and avoid\nknown dead ends).\nAs such capabilities appear hard to evaluate by using either\nMCQs or the statistical scorer or embedding approaches de-\nscribed earlier. we developed a new open-response benchmark\nALDbench on materials synthesis, and in particular on a\nsynthesis technique called atomic layer deposition [69]. Here\nwe targeted a range of difficulty spanning from graduate level\nto PhD-level domain expert current with the state of the art.\nA model’s ability to perform at a domain expert level is\nparamount whenever models are expected to assist in decision\nmaking processes that involve costly experiments. Beyond its\napplied interest in areas such as energy and microelectronics\n[70], this domain brings together multiple topics that are\ncommonplace in chemistry-driven synthesis, including metal-\norganic and inorganic molecules, gas-surface kinetics and\nheterogeneous reactions, and gas phase transport. Evaluating\nLLM capabilities in this field can provide insights with wide\napplicability to other material synthesis techniques.\n\n\nFig. 3. Distribution of the mean scores of GPT-4o responses to all questions\nin the ALDbench benchmark.\nTo compile the benchmark, we asked six PhD-level human\nexperts to generate “questions that a researcher or a graduate\nstudent who is not familiar with a specific process/application\nwould ask an AI assistant.” The curated questions could be\ngrouped into four categories: 1) how to grow, where the query\nis about material synthesis; 2) specific questions about ALD\nprocesses, comprising more in-depth queries about a process\nor material; 3) general ALD knowledge, with questions about\nthe synthesis technique; and 4) applications.\nThe human experts were then asked to grade the questions\nusing a scale of 1 to 5 on two criteria with the following\nrubrics similar to the AI4S benchmark: (1) Difficulty: 1–Easy,\nearly graduate; 5–Hard, top expert; (2) Specificity: 1–General;\n5–Specific, quantitative.\nEach response is then graded using four criteria with the\nfollowing rubrics: (1) Overall quality: 1–Very low quality; 5–\nExcellent; (2) Specificity: 1–Too broad; 5–Targeted; (3) Rel-\nevance: 1–Irrelevant fluff; 5–Relevant answer; (4) Accuracy:\n1–All made up; 5–All correct. The use of multiple criteria\nallowed us to probe aspects of the generation process, such\nas relevance or specificity of the response, that are not easily\ncaptured by benchmarks focused on accuracy.\nWe ran this benchmark using an instance of OpenAI’s GPT-\n4o, with seven PhD-level human experts reviewing model\nresponses. Details are in the ALDbench paper [69]. The model\nresponses received a composite quality score of 3.7, consistent\nwith a passing grade. However, 36% of the questions received\nat least one below average score. When we carried out an\nin-depth analysis of the responses we identified at least five\ninstances of hallucination. In Figure 3 we show the distribution\nof mean scores for all the questions in the benchmark and the\nfour criteria evaluated by the human experts.\nWe also explored statistical correlations between the dif-\nficulty and specificity of each question and the human ex-\npert scores for each evaluation criteria. For each (question,\nresponse) pair we computed p-values using the Fisher exact\ntest to evaluate the statistical significance of the correlation.\nWe found statistically significant correlations between question\ndifficulty and response quality (p0 = 0.033), question difficulty\nand relevance (p0 = 0.016), and question specificity and\nresponse accuracy (p0 = 0.007). In all three cases, higher\ndifficulty or specificity correlated with lower scores. These\nresults emphasize the need to evaluate LLMs across multiple\ncriteria beyond traditional metrics of difficulty and accuracy.\nOur results show that highly targeted, open-response bench-\nmarks can provide information about LLM performance in\nscientific domains that is complementary to MCQs or natural\nlanguage processing benchmarks. The methodology developed\nin this work allowed us to probe in depth model performance\nin a specific domain. With the aid of a small team of PhD-level\nexperts we were able to identify instances of hallucinations and\nexplore model responses in a level of detail that it is hard to\naccomplish using automatic evaluation methods. The extension\nof this approach to other domains, such as energy storage or\nmicroelectronics, is trivial. Moreover, as a byproduct of this\neffort, we collected a small dataset of questions and human\nrated responses across four different evaluation criteria. As\nwe explore other domains we can use this data to train or\nvalidate automatic question evaluation approaches for open-\nended benchmarks.\nD. End-to-End Evaluations\nAlthough MCQ benchmarks are effective in testing factual\nrecall and reasoning within constrained formats, and open-\nended benchmarks gauge the generation of detailed and flex-\nible responses, these methods do not capture the iterative\nand complexity of scientific problem solving. End-to-end\nevaluations attempts to address this gap by assessing, in real\nsituations, the models responses for assisting researchers in\nsolving scientific problems. We propose two novel types of\nend-to-end methods in the context of scientific research: Lab-\nstyle and field-style experiments.\n1) Lab-style experiments:\nThese experiments are de-\nsigned to evaluate the capabilities of AI models to assist\nresearchers in performing the typical tasks (Table I) to solve\nscientific problems. Note that, in real situations, these tasks\nare often repeated several times to solve problems. By cap-\nturing and evaluating all interactions between research and\nLLMs while attempting to solve a research problem, a lab-\nstyle experiment can capture accurately the complex reality\nof solving complex research problems. It can thus provides\na unique perspective on the “distance” between the ideal\nscientific assistant and the current capabilities of AI models.\nThe setup for lab-style experiments involves defining a\nspecific scientific problem and presenting it to multiple AI\nmodels for comparison. Each model is manually tasked with\nassisting in all the research tasks using the same prompts, en-\nsuring consistency across evaluations. Prompts and responses\nare meticulously recorded, and domain experts analyze and\n\n\nFig. 4. Example of multi-turn interaction between a researcher and several LLMs used as research assistants in an attempt to repeat the research developed for\nthe HPDC24 paper. Because of space limitations, the figure does not show the models’ responses and research analysis. The full interaction for the HPDC24\nexperiment can be found here: https://tinyurl.com/yv4awky3\ncomment on each response to assess model relevance, ef-\nfectiveness, and overall performance. The response of each\nmodel in every step is compared to that of a human assistant,\ntypically at the post-PhD level. Performance is characterized\nby evaluated against criteria such as correctness, conciseness,\nand precision.\nWe have conducted lab-style experiments with not only\ndifferent LLMs but also different versions of individual LLMs,\nwith the latter studies permitting evaluation of improvements\nover time. For example, a relevant metric of progress across\nmodel generation is the number of prompts needed to solve\na particular research problem. By focusing on real-world\nscientific workflows and expert evaluation, lab-style exper-\niments provide a practical approach to observe AI model\nimprovements as scientific research assistants.\nTo date, we have performed three lab-style experiments\nwith five domain experts (one expert per experiment) 1 each\nof whom provided a problem to solve, generated prompts\ncovering the different research steps, and analyzed model re-\nsponses. Three experiments were related to parallel/distributed\ncomputing (scheduling of a directed acylic graph; solving a\npartial differential equation; zero-overhead checkpointing) and\nwere selected carefully from three categories: open problem\n(no known solution), published problem (solution known),\nand recently published problem (solution known). For the\n“published problem,” the paper is more than two years old, and\nthus we assume that AI model developers had access to the\npaper. For the “recently published problem,” the experiment\nwas performed just a few months after the publication; here,\nwe assume that many models were not trained with the paper.\nThe experts with whom we worked in these experiments had\nnever previously used AI models as research assistants. The\nexperts provided two levels of analysis: 1) a detailed analysis\nof the responses received for individual prompts, and 2) high-\nlevel scoring compared to a human researcher using the A,\nB, C, D, E, F scale (A being the human reference, F being\nthe worst possible score). In total, the three experiments cover\nabout 20 hours of interactions, about 100 prompts, and about\n250 pages of testing and analysis. We used 10 AI models (not\nall models were used in all three experiments): O1-preview,\nGPT-4o, GPT-3.5, Claude3 Sonnet, Claude Haiku, Mistral,\nLlama3 70B, Llama3 405B, Perplexity Pro, and Gemini 1.5\n\n\nFig. 5. A partial scoring of several models used as AI assistants on August 20, 2024, to solve the zero-overhead checkpointing problem. The results highlight\nthe strengths and weaknesses of different models for the different research steps. We note that the RAG model (Perplexity Pro) has a decisive advantage in\nseveral steps for this particular problem. Other models struggle in most steps.\n(not all models were tested on all prompts because some\nmodels produced incorrect responses before reaching the end\nof the evaluation.)\nWe show in Figure 4 the beginning of the multi-turn\ninteraction and in Figure 5 part of our initial high-level scoring\nof several AI models for the zero-overhead checkpointing\nproblem. The lab style experiment was performed on Au-\ngust 20, 2024. The scoring reflects the performance of the\nmodels during the experiment. This problem falls into the\ncategory of recently published problems. The goal was to\ncheck whether the models could reproduce the analysis and\ndesign of the LLM checkpointing system presented in the 2024\nACM International Symposium on High-Performance Parallel\nand Distributed Computing (HPDC24) best paper [71]. The\nmost important part of the experiment was to assess each\nmodel’s ability to 1) identify the fundamental observation\n(non-mutable parameters and optimizer state during the for-\nward and backward passes of LLM training) and 2) propose a\ndesign for an asynchronous checkpointing system that exploits\nthis observation.\nBased on these initial three experiments, we performed two\nother experiments on open problems in Chemistry and Biol-\nogy. These experiments used the same overall interaction col-\nlection approach and compared more recent reasoning models\n(O1-preview, OpenAI O3-mini, Gimini 2.0 experimental). We\nalso used a more rigorous scoring system, defining precisely\nevery score level for every evaluated skill. From these experi-\nments, we developed a Lab-style experiment tool to collect the\nproblem setup (Figure 11), every prompt-response-assessment\n(Figure 12), and final assessments (Figure 13). This collec-\ntion tool is available on github (https://github.com/auroraGPT-\nANL/questions-ui)\nOur experience with the “Lab-style experiment” method\nallows for several observations regarding its utility and limita-\ntions. Unlike traditional benchmarking, this method places AI\nmodels in real-world research scenarios, enabling evaluators to\ndirectly assess their knowledge, capabilities, and overall use-\nfulness for specific tasks. By relying on multi-turn prompting\nand open responses, the method also tests the propensity of\nAI models to digress (e.g., for the HPDC24 paper experiment,\na model had a tendency to focus on the consistency aspect of\ncheckpointing, which is not relevant in that context, instead of\nfocusing on overhead reduction) and hallucinate (as seen with\nearlier models in 2024 that frequently generated fabricated sci-\nentific references). However, this approach is not yet scalable\nand remains narrow in coverage; it requires significant manual\neffort, with two researchers spending 5–6 hours analyzing and\ncomparing models for specific tasks. The specificity of the\naddressed research problems further limits the generalizability\nof the findings. Despite these constraints, the method excels\nin two aspects 1) providing a fine-grain capability assessment\nof LLMs as scientific assistants in a realistic context and 2)\ntracking model progress across generations. For instance, in\nsolving the zero-overhead checkpointing problem, successive\nmodel iterations demonstrated improved efficiency, reducing\nthe number of prompts needed to reach the key insight—\n\n\nfrom five prompts with GPT4o to just one prompt with\nArgo/O1-preview, which incorporates a science-oriented pre-\nprompt. This result highlights the method’s potential to reveal\nmeaningful advancements in AI capabilities over time.\n2) Field-style experiments: This method takes inspiration\nfrom previous studies analyzing user-LLM interactions at scale\n[38], [39], [50]. The “In the wild” method captures and ana-\nlyzes all the interactions between volunteer users and AI mod-\nels. This method provides additional critical information for\nthe development and improvement of AI models for science:\na precise understanding of researcher needs and requirements\nregarding AI assistants (e.g.: What task do researchers ask\nAI models to perform? What are their expectations regarding\nmodel responses? How frequently do researchers use AI\nmodels?); a deeper understanding of AI models strengths\nand weakness (by analyzing the thousands of prompts and\nresponses); a window on the trends behind the use of AI\nmodels as research assistant (e.g., increased usage frequency,\nincreased number of users, nature and distribution variations\nof the performed tasks); and tracking of AI model progress\nacross generations. Ideally, this method will analyze online\nthousands of user-LLM interactions. Compared to the “Lab-\nstyle experiment” method, users are not expected to evaluate\nLLM responses directly. Instead, evaluation is indirect, based\non the study of the flow of prompts and responses. This\nmethod leverages user behavior as a signal to diagnose LLM\nfailure modes [72]. For example, a user submitting rephrasing\nquestions, providing feedback, or abandoning the interaction\nare signs of LLM weaknesses in understanding user intent.\nThe flow can then be analyzed to diagnose potential sources\nof weakness [73]. Previous “in-the-wild” experiments focused\non nonscientific domains. The Field-style experiment method\nadapts the “in-the-wild” approach to the scientific context\nby defining criteria and scoring specific to the scientific\nmethodology.\nOn November 1, 2024, Argonne organized a JAM session\nthat captured 180 conversations between Argonne researchers\nand Argo/O1-preview 1 Researchers were asked to bring to the\nJAM session evaluation a scientific problem that they would\nwork on with Argo/O1-preview as a research assistant. At the\nend of the session, the researchers evaluated their experience\naccording to five criteria: Novelty, Productivity, Solution,\nStrength, and Importance. (This approach is consistent with\nthat followed in a much smaller study conducted at Los\nAlamos National Laboratory.) Five possible responses were\nproposed for each question, corresponding to a score of 1 to\n5.\nThe scores produced by the Argonne researchers indicated\nthat: 1) Importance: 82% of researchers consider that AI\nmodels such as Argo/O1-preview are “very important” or\n“critical” to their team’s success, 2) Strength: at 59%, they\nconsider that AI models significantly or noticeably improve\nproductivity, 3) Productivity: 51% of researchers compare AI\nmodels such as Argo/O1-preview to PhD students or postdocs,\n4) Solution quality: 50% of the researchers consider that\nAI models such as Argo/O1-preview produce exceptional or\nstrong solutions, 5) Novelty: only 21% of the researchers\nconsider that AI models such as Argo/O1-preview provide\nnotably novel or groundbreaking solutions. (Argo is Argonne’s\nAPI proxy for OpenAI models including o1 preview. Because\nOpenAI does not make it’s system prompt available, this needs\nto be recreated. We note the system prompt used for Argo in\nthe Appendix, Figure 15.)\nAlthough informative about the needs expressed by re-\nsearchers to access models such as Argo/O1-Preview, the\noutcomes of the November 1, 2024 JAM session evaluation\ndid not identify the specific strong and weak science skills\nof Argo/O1-preview. To understand the reasons behind the\nperceived weaknesses of Argo/ O1-preview, we use LLama-\n3.3-70B-Instruct for an LLM-as-a-judge approach to analyze\nthe recorded conversations and score the performance of the\nArgo/O1-preview concerning scientific skills.\nThe goal is to develop a pipeline (workflow) to analyze the\nrecorded conversations to assess the strengths and weaknesses\nof LLMs as scientific assistants. From the 180 JAM session\nsurvey responses, we manually filtered a subset of 125 that had\nvalid transcripts and were sufficiently challenging scientific\nproblems, requiring PhD level domain expertise and reasoning\ncapability. We generated an initial version of the prompt to\nanalyze the conversations using Gemini experimental 1206\nand refined it manually. We chose Llama-3.3-70B-Instruct as\nthe highest performing open model on several benchmarks\nincluding GPQA, and presented it with each transcript for-\nmatted into a detailed LLM-as-a-judge prompt (see Appendix,\nFigure 18) to evaluate 29 scientific criteria. The responses\ncontained strengths, weaknesses, and examples/evidence from\neach conversation, as well as a formatted scoring from 1–10\nfor each of the criteria. The model was instructed to identify\nskills that did not apply to a given conversation rather than\ngive an actual score. A score of 0 reflects a situation in which\nthe model could not assign a score to the criteria because\nit determined that the criteria were not applicable. After\nobtaining the responses for every conversation, the model was\nprovided batches of 25 to summarize, with specific instructions\nthat these summaries would be used to synthesize a final\nsummary. Batches were used, as the total token count of all\nthe responses was 164K, larger than the default 128K window\nof Llama 3.3.\nFigure 6 presents the results of the conversation analysis\npipeline as a proof-of-concept. The presented results need\nhuman validation: LLMs are known to hallucinate and present\noverly positive assessments of outputs compared to human\nreviewers. Additionally, the field-style experiments revealed\nthat 59% of researchers reported noticeable productivity im-\nprovements using LLMs, while 51% likened the LLM’s con-\ntributions to those of PhD students or postdocs. However, only\n21% rated the models as delivering notably novel or ground-\nbreaking solutions. These results should not be considered\na complete or definitive assessment of human assessment of\nLLM performance.\nThe Field-style method for analyzing the strengths and\nweaknesses of LLMs is still in its nascent stages. Our analysis\n\n\nFig. 6. LLM-generated summary of detected LLM strengths and weaknesses in 125 scientist-Argo/O1-preview conversations\nof scientist-LLM conversations represents the first attempt to\nuse an LLM-as-a-judge to evaluate the performance of an\nLLM as a research assistant. Insights from the JAM session\norganized at Argonne highlight several lessons. First, scoring\nuser-LLM interactions holistically with a small set of criteria\n(five in this case) permits only a high-level evaluation, insuf-\nficient for diagnosing specific sources of LLM weaknesses.\nSecond, recording user-LLM interactions with detailed anno-\ntations, such as identifying the skills required for each prompt\nand scoring individual responses, offers greater diagnostic\npotential. While this detailed approach is not feasible for\ngeneral scientist-LLM dialogues, it can be implemented in\nstructured, especially organized sessions. Lastly, while LLMs-\nas-a-judge offers a scalable mechanism for analyzing user-\nLLM interactions, the current implementation remains a proof\nof concept. Additional research and validation are necessary\nto build confidence in the results produced by this analysis\npipeline.\nIV. RELIABILITY AND UQ OF EVALUATIONS\nThe success of LLMs in scientific domains, such as chem-\nistry, biology, and physics, has been remarkable, but their\ntrustworthiness as scientific assistants remains a significant\nconcern. These models, including GPT, Claude, and Llama,\nare prone to generating unreliable or fabricated responses,\noften referred to as hallucinations [52]. Understanding and\nquantifying uncertainty in LLM outputs is essential to ensure\nsafe, reliable, and informed decision making, particularly in\nscientific domains. Traditional uncertainty quantification (UQ)\ntechniques, which rely on accessing internal model parameters\n[74], face challenges due to the black-box nature of modern\nLLMs like GPT-4, Claude 3, and Gemini, which are primarily\naccessible as API services. Recent research has focused on de-\nveloping novel approaches to assess uncertainty directly from\nmodel outputs, such as semantic entropy [75], sampling-based\nmethods, and aggregation techniques [51], [76]. These tech-\nniques aim to evaluate input sensitivity and output consistency,\nhighlighting where models are most vulnerable. By improving\ntransparency and trust, these UQ strategies play a crucial role\nin responsible AI deployment. Addressing these challenges is\nvital for leveraging LLMs in scientific applications, where\nerrors can have substantial consequences. Moving forward,\nadvancing UQ methods and enhancing LLM interpretability\nwill be key to making these models safer and more robust in\ncritical scientific and industrial domains.\nInspired by psychological assessments in which the same\nquestion is asked in different ways to test consistency, we\npropose a technique called Question Rephrasing [55] to quan-\ntify the uncertainty in LLM outputs. This approach involves\nrephrasing a given question while preserving its original\nsemantic meaning and comparing LLM responses before and\nafter rephrasing to assess input uncertainty. In addition, we\nadopt a sampling method that repeatedly queries an LLM\nwith identical inputs to evaluate output uncertainty. We applied\nthese methods to assess GPT-3.5 and GPT-4 performance on\ntasks in the chemistry domain, specifically property predic-\ntion and forward reaction prediction. Input uncertainty helps\ndetermine the LLM’s sensitivity to variations in molecular\nrepresentations (e.g., alternative SMILES notations), while\noutput uncertainty evaluates the inherent variability in LLM\npredictions. These techniques allow us to systematically ex-\nplore how robust and reliable LLMs are in handling different\nforms of input and producing consistent output. Below, we\noutline our approach:\n\n\nFig. 7.\nSMILES representation variants of Aspirin. While all structures\ndepict the same molecule, their SMILES representations are different, which\nintroduces input variations. Top left: Canonical SMILES representation of\nAspirin. Rest: Five SMILES variations of Aspirin.\n1) For a chemistry-related task t, given a SMILES rep-\nresentation xi of the i-th molecule, generate a prompt\nPt,xi based on a task-specific template .\n2) Generate a list of up to n SMILES variants of the\nmolecule xi: L = {x1\ni , x2\ni , ..., xn\ni }. We ask GPT-4 to\nrank the SMILES variants according to their confidence\nin interpreting their structures and choose the one, say\nˆxi, with the highest confidence in constructing a prompt\nPt,ˆxi by replacing xi in Pt,xi with ˆxi.\n3) Ask the LLM to generate m responses for the prompt\nPt,ˆxi and obtain Rt,ˆxi = {rt,ˆxi,1, rt,ˆxi,2, ..., rt,ˆxi,m}.\n4) Calculate the entropy-based uncertainty metrics Ut,xi\nand Ut,ˆxi for Rt,xi and Rt,ˆxi, respectively.\n5) Measure the input uncertainty by comparing Ut,xi and\nUt,ˆxi for all chosen xi. Measure the output uncertainty\nby examining Ut,xi and Ut,ˆxi separately.\nOur experiments revealed that ChatGPT-4 exhibited a no-\ntable sensitivity to Question Rephrasing. We view this sen-\nsitivity as providing insight into the input uncertainty of the\nmodel. We observed that variations in the input format, such\nas rephrasing or using alternative SMILES representations,\nled to differences in the consistency of model responses. For\nexample, in property prediction tasks using chemistry property\ndatasets like BBBP, HIV, and Tox21 [27], we noted changes\nin model performance metrics such as accuracy and F1 score\nwhen the inputs were reformulated. The AUC scores is the\nArea Under the ROC Curve, and indicates ability of the\nmodel to differentiate correct vs. wrong responses, with 1.0\nbeing the means perfect separation (the model always assigns\nhigher confidence/lower uncertainty to correct answers than\nto incorrect ones). AUC for original SMILES ranged between\n0.546 and 0.774, suggesting only moderate uncertainty in\npredict response correctly. When using reformulated inputs,\nmodel performance generally declined, as indicated by de-\ncreased accuracy and F1 scores in most datasets. Furthermore,\nin the forward reaction prediction tasks, GPT-3.5 and GPT-4\nperformed poorly, with noticeable declines when molecular\nrepresentation variations were introduced. Although output\nuncertainty metrics, such as entropy-based measures, pro-\nvided high AUC scores (ranging from 0.86 to 0.99 indicating\nbetter uncertainty awareness), overall accuracy was limited,\nhighlighting the need for improved LLM understanding of\nchemical knowledge. These findings emphasize that while\nuncertainty metrics can indicate response reliability, significant\nimprovements are needed to make LLMs reliable in critical\nscientific applications.\nV. SAFETY EVALUATIONS\nSafe and secure deployment of AI systems in scientific\ndomains is paramount. As LLMs increasingly support crit-\nical applications in fields like biosecurity, cybersecurity, and\nchemistry, ensuring their safety and alignment is also essential\nto maintaining trustworthiness. Hence, it is critical to integrate\ninto our proposed methodology to evaluate LLMs as research\nassistants rigorous safety and alignment evaluation techniques.\nTo this end, we discuss below the CHEMRISK benchmark\nas one of our efforts in this direction.\nA. CHEMRISK Chemical Risk Detection Benchmark\nFig. 8. CHEMRISK is a chemical risk detection benchmark.\nThe CHEMRISK benchmark (Figure 8) addresses a critical\nneed in the era of increasingly powerful large language models\n(LLMs) such as Claude, chatGPT, and others. As these models\nbecome more capable of understanding and generating chem-\nical information, it is essential for organizations like the De-\npartment of Energy to have robust, standardized benchmarks\nto evaluate how models handle potentially sensitive chemical\nknowledge. CHEMRISK provides a comprehensive framework\nfor assessing LLMs’ capabilities across three key domains:\nchemical understanding, molecular design, and molecular\nsynthesis. The benchmark employs both multiple-choice and\nfree-form questions, using standard molecular representations\n(SMILES and SELFIES) to ensure broad applicability.\nCHEMRISK is designed as an evolving benchmark, devel-\noped in collaboration with domain experts at Lawrence Liver-\nmore National Laboratory (LLNL). The benchmark focuses\non crystalline density and heat of formation (HoF)–proxy\nproperties that are fundamental to understanding energetic ma-\nterials. High crystalline density often correlates with increased\nperformance in energetic materials, while heat of formation\nhelps characterize potential energy content and stability. The\nbenchmark encompasses diverse tasks including: property\nregression for predicting experimental densities, molecular\nranking, forward synthesis prediction, retrosynthesis predic-\ntion, masked SMILES completion, SMILES correction, and\nmolecular name-to-structure conversion (and vice versa). Base\n\n\ndata is curated from established chemical databases, including\nthe Cambridge Structural Database (CSD) and United States\nPatent and Trademark Office (USPTO), and has undergone\nmanual quality checks, ensuring the benchmark’s reliability.\nBy providing a standardized framework for evaluating LLMs’\nchemical reasoning capabilities for energetics, CHEMRISK\naims to foster responsible development of AI systems while\nhelping identify potential safety risks in chemical AI applica-\ntions.\nTABLE V\nACCURACY ON A SUBSET OF CHEMRISK: EVALUATING CHEMICAL\nSAFETY RISKS OF FRONTIER LLMS.\nLLM\nChem. Understanding\nDesign\nSynthesis\nGPT-4o\n1.0\n0.4\n0.6\no1\n1.0\n1.0\n0.6\nIn Table V, we present preliminary results from exploring\nthe GPT-4o and o1 models on a subset of the CHEMRISK\nbenchmark. Specifically, we focus the following multiple-\nchoice Q&A tasks in each category: SMILES-to-Name, prop-\nerty prediction, and forward synthesis. For both models, we\nutilize the system prompt outlined in Appendix Figure 16.\nThe results indicate that while both GPT-4o and o1 demon-\nstrate perfect accuracy (1.0) in chemical understanding tasks\nlike SMILES-to-Name conversion, o1 outperforms GPT-4o in\nmolecular design challenges, achieving perfect accuracy com-\npared to GPT-4o’s 0.4. Both models show identical moderate\nperformance (0.6) in molecular synthesis tasks. CHEMRISK\nserves as a proof-of-concept proxy benchmark, laying the\nfoundation for continued development and the creation of\nadditional benchmarks for other domains using a similar\nmethodology.\nSample question from CHEMRISK benchmark\nGiven the molecule with the following SMILES repre-\nsentation: Cc1c(cnc(N)c1N(=O)=O)N(=O)=O, what\nis the experimentally measured crystalline density of the\nmolecule?\nA) 1.145 g/cm³\nB) 1.364 g/cm³\nC) 1.739 g/cm³\nD) 1.925 g/cm³\nVI. SCALABLE SOFTWARE INFRASTRUCTURE\nAs previous sections show, a comprehensive evaluation of\nLLMs as research assistants already requires the execution of\nmany benchmarks for skills and safety assessments. We do not\nexpect the evaluation workload to be reduced in the future.\nIn contrast, as more capable LLMs appear, more research\ndomains will be interested in using them, which will trigger\nthe development of new evaluation benchmarks. This situation\nplaces ever-growing demands on software and computational\ninfrastructure. Existing evaluation software platforms, such\nas HELM [77], EleutherAI’s LM Evaluation Harness [78],\nand DecodingTrust [44], have made significant strides in this\narea, but exhibit certain limitations that impede comprehensive\nand scalable evaluations, particularly within high-performance\ncomputing (HPC) environments like those at Argonne National\nLaboratory. A critical shortcoming of current frameworks is\ntheir limited scalability and inefficiency in handling large-scale\nmodels. Many existing software platforms are not optimized\nfor parallel processing across multiple GPUs or computing\nnodes, resulting in prolonged evaluation times and increased\ncomputational costs. This inefficiency becomes particularly\nproblematic when assessing large LLMs that demand substan-\ntial computational resources. In addition, inconsistencies in\nevaluation methodologies and a lack of standardization further\nhinder comprehensive evaluations. The absence of consistent\nbenchmarks and metrics across platforms and organizations\ncomplicates model comparisons, exacerbated by dataset biases,\ncontamination, and the rapid evolution of LLMs outpacing\nevaluation strategies.\nTo address the challenges of scalable and comprehensive\nLLM evaluation, we are developing the Skills, Trust, and\nReliability (STaR) evaluation framework, tailored for HPC\nsystems at Argonne National Laboratory. STaR builds upon the\ngeneral architecture of evaluation platforms, which typically\ninvolve a sequence of specifications (files or configuration\nflags) to instantiate controllers and manage communication\nthrough states. Central to these platforms are Runners, which\nact as top-level components orchestrating workflows that\nhandle Scenarios—benchmarks comprising static datasets like\nHellaswag or GSM8K, or dynamic scripts such as those in\nChain-of-Thought Hub [79]. A Data Pre-Processor translates\nthese Scenarios into formatted prompts, which are passed to\nAdapters interfacing with LLMs through libraries such as\nHugging Face [80], vLLM [81], or OpenAI APIs. Executors\nlike Slurm [82] or Ray [83] enable processing of prompts, and\nthe results are aggregated into metrics, such as accuracy.\nExpanding on this general framework, STaR introduces a\nmodular architecture comprising a data layer, prompting layer,\nmodel adapter, and result layer to streamline the evaluation\nprocess. The data layer ingests datasets, such as MMLU-Pro\n[14], and constructs evaluation instances, while the prompt-\ning layer generates standardized prompts using techniques\nsuch as few-shot and chain-of-thought reasoning [84]. The\nmodel adapter queries models in multiple modes, including\nlocally loaded instances for smaller models, Parsl [57] for job\nbundling, and OpenAI-compatible inference backends (e.g.,\nvLLM [81] and DeepSpeed FastGen [85]) for larger models\ndeployed on HPC systems like Polaris and Aurora. The result\nlayer aggregates responses, computes general and UQ metrics,\nand organizes results into comprehensive scores, providing\nnuanced insight into model performance.\nSTaR supports widely used benchmark libraries, includ-\ning EleutherAI-Harness [78], DecodingTrust [44], Wildbench\n[37], and domain-specific benchmarks. It also integrates un-\ncertainty quantification approaches [55] to enhance the re-\nliability of evaluations. Designed for scalability, STaR in-\ncorporates data-parallel capabilities to distribute workloads\nacross multiple GPUs and model-parallel solutions to handle\nlarge models exceeding single-GPU memory limits. It aims\n\n\nTABLE VI\nHARNESS EVALUATION RESULTS FOR SEVEN LLAMA VARIANTS ON THE OPENLLM V2 BENCHMARK.\nModel\nIFEval ↑\nBBH ↑\nMATH ↑\nGPQA ↑\nMuSR ↑\nMMLU-PRO ↑\nGPU Hours\nLlama-2-7B\n0.2543\n0.3475\n0.0121\n0.2718\n0.3703\n0.1848\n5.05\nLlama-2-7B-chat\n0.3538\n0.3676\n0.0189\n0.2735\n0.4034\n0.2000\n4.93\nLlama-3-8B\n0.1536\n0.4600\n0.0317\n0.3146\n0.3677\n0.3248\n7.62\nLlama-3-8B-Instruct\n0.4825\n0.4885\n0.0808\n0.3020\n0.3823\n0.3580\n6.50\nLlama-3.1-8B\n0.1228\n0.4652\n0.0438\n0.3070\n0.3849\n0.3260\n6.94\nLlama-3.1-8B-Instruct\n0.4924\n0.5058\n0.1360\n0.3163\n0.3995\n0.3789\n8.31\nLlama-3.3-70B-Instruct\n0.6745\n0.6994\n0.3391\n0.4715\n0.4854\n0.5477\n81.39\nTABLE VII\nDECODINGTRUST EVALUATION RESULTS ON POLARIS\nModel\nToxicity\nStereotype\nAdversarial\nOOD\nRobustness to\nPrivacy\nMachine\nFairness\nBias\nRobustness\nRobustness\nAdv. Demonstrations\nEthics\nLlama-2-7B-chat\n80.0\n97.6\n51.01\n75.65\n55.54\n97.39\n40.58\n67.95\nLlama-2-70B-chat\n80.0\n98.0\n52.00\n71.00\n74.00\n99.00\n54.00\n65.00\nto simplify deployment with a unified one-step installation\nprocess and a consistent command-line interface, while results\nmanagement supports standardized local tracking and optional\ndatabase integration. STaR is a work in progress, with ongoing\nrefinements aimed at seamless integration with Argonne’s\ninfrastructure and addressing limitations identified in existing\nplatforms. By prioritizing scalability, standardization, and effi-\nciency, STaR aims to establish a robust evaluation framework\nthat meets the evolving needs of LLM assessment in scientific\nresearch environments.\nAs a proof of concept, we performed evaluations on Polaris\n[86], and benchmarked them with various open-source models\nwith sizes from 7B to 70B. We used OpenLLM Leaderboard\nV2 (through Harness), a commonly used benchmark suite\nconsisting of six challenging tasks, to evaluate the performance\nof Llama-2-7B and Llama-3-8B, Llama-3.1-8B, and their\ncorresponding chat or instruct models, as well as Llama-\n3.3-70B-Instruct, the most recent and advanced model in the\nLlama series. Table VI presents the evaluation results for the\nmodels on the benchmark, along with the GPU hours required\nfor the evaluations.\nFor safety evaluations, we use DecodingTrust, which, unlike\nframeworks such as Harness and HELM, is lightweight, highly\ncompatible with HPC platforms, containerizable. Deploying\nDecodingTrust on the Polaris HPC system required key modi-\nfications, including integrating Parsl for efficient job bundling,\nadapting the framework to align with Polaris’s queue con-\nfigurations for optimized task distribution, and implementing\na unified inference interface. These adaptations allowed the\nframework to harness Polaris’ computational capabilities while\nretaining its flexibility and commitment to trustworthiness\nassessments.\nOn Polaris, DecodingTrust efficiently managed a variety of\nevaluation tasks, ranging from straightforward classification\nassessments to computationally intensive open-ended analy-\nses. Classification tasks such as Adversarial Demonstration\nRobustness, Fairness, and Machine Ethics required minimal\ncomputational resources, with each task consuming approxi-\nmately 0.5 A100 hours. In contrast, open-ended evaluations,\nincluding Toxicity and Stereotype Bias, were significantly\nmore resource intensive, especially for models exceeding 70B\nparameters, with some tasks demanding up to 24 A100 hours\nper evaluation. By leveraging Polaris’s HPC infrastructure, De-\ncodingTrust successfully scaled its evaluation pipeline, balanc-\ning lightweight classification tasks with resource-heavy open-\nended evaluations to provide a comprehensive assessment of\nmodel trustworthiness.\nVII. CONCLUSIONS AND NEXT STEPS\nAs LLMs continue to expand the notions of what AI can\naccomplish, there remain two main challenges to address\nto enable the broad adoption of LLMs by the scientific\ncommunity as research assistants: a holistic understanding\nof the capabilities of LLMs and a strong confidence in the\nresults produced by them. To address this, our proposed\nmethodology features four techniques: multiple choice ques-\ntions, open-response questions, lab-style experiments, and\nfield-style experiments which complement each other to form\na comprehensive, rigorous, and realistic assessment of the\ncapabilities of AI systems. Underneath the four approaches are\nthree cross-cutting aspects, including trust and safety, reliable\nuncertainty quantification, and scalable software infrastructure\nwhich support our approach. In addition to proposing the\nholistic methodology, our team has advanced the state-of-the-\nart in each of the techniques and aspects.\nMultiple choice questions are a key technique to evaluate\nLLMs because of their ability to quickly assess a breadth\nof knowledge; our team extends beyond existing benchmarks\nwith automatically-generated domain-specific MCQ bench-\nmarks in Astronomy and Climate and the multi-domain AI4S\nBenchmark with both human and automatically curated with\nhuman reviews have revealed significant gaps in knowledge\nrecall and reasoning in LLMs. We find that our new AI4S\nbenchmark is more challenging for LLMs than benchmarks\n\n\nlike GPQA. This increased difficulty stems from the AI4S\nbenchmark design, which integrates manual and automatic\nquestion generation to create diverse and nuanced questions\nthat assess reasoning and domain-specific knowledge. The\nAstronomy benchmark also highlighted disparities in perfor-\nmance and cost-efficiency across frontier models, as well as\nacross English and non-English language models. The Climate\nbenchmarks showed that models like GPT-4o struggled both to\nproduce assessments of fine-grained knowledge and questions\nthat vary in style and content. The AI4S Benchmark high-\nlighted significant disparities in performance across models\ndue to its rigorous evaluation across multiple domains.\nOpen response questions similarly serve an important role,\nallowing a more detailed but still fast assessment of model per-\nformance. Open-ended benchmarks such as SciCode provide\nrealistic and challenging coding problems across fields such\nas physics, biology, and materials science, rigorously testing\nmodel abilities to reason, recall knowledge, and generate\naccurate code. Similarly, the ALDbench materials science\nbenchmark allowed experts to uncover hallucinations and\nevaluate responses with precision, generating datasets valuable\nfor further refining evaluation methods.\nA key innovation of our approach is to incorporate more\nrealistic end-to-end experiments with lab-style and field-style\nexperiments which more closely reflect the in-depth and it-\nerative problem-solving practiced by scientists. lab-style ex-\nperiments provided holistic assessments of LLM capabili-\nties in research workflows, including hypothesis generation,\nanalysis, and reporting. For example, experiments revealed\nvariability in performance across models, with GPT-4o re-\nquiring five prompts to address a checkpointing problem,\nwhile Argo/O1-preview re-solved the problem with just one\nprompt. Field-style experiments, which analyzed real-world\ninteractions between scientists and LLMs at scale, offered\nquantitative insights into model strengths and weaknesses as\nresearch assistants. These studies identified the remarkable\nperformance of LLMs compared to PhD students and postdocs\nwhile struggling to present novel or ground-breaking results.\nTogether, our combination of domain-specific and multi-\ndomain MCQs, open-ended benchmarks, and end-to-end ex-\nperiments provides a holistic framework for assessing LLMs—\none that we argue points to a new methodology able not only to\nquantify current model limitations but also to guide targeted\nimprovements aimed at aligning model capabilities with the\nnuanced demands of real-world scientific research.\nLooking ahead, we are seeking to expand evaluation bench-\nmarks to comprehensively assess LLM capabilities across\ndiverse scientific domains while also incorporating advanced\nmethodologies for trustworthiness, uncertainty quantification\n(UQ), and iterative evaluation. We anticipate conducting more\nand refined lab-style experiments to provide yet precise assess-\nments of AI model capabilities for specific research problems,\nwith the goal of both improving scalability and coverage and\ntracking model progress across generations. We also aim to\nrefine the Field-style experiments method capturing real-world\ninteractions between scientists and LLMs and to leverage\nfeedback to align automated scoring with human judgments\nthrough instruct-tuned models. New benchmarks for Retrieval-\nAugmented Generation (RAG) and multimodal narrative as-\nsessments will target domains like biology, weather/climate,\nand cosmology where Argonne has access to substantial\nquantities of scientific simulation results and data, utilizing\nconstructs such as aggregation and multihop scenarios to\nevaluate performance across modalities. We will adopt agent\nevaluation techniques from frameworks like CACTUS [87] for\nmulti-turn chemistry tasks, and automated red-teaming [88]\nwill enhance safety evaluations by systematically identifying\nvulnerabilities such as biases and hallucinations. We will\nalso advance trustworthiness and UQ through embedding-\nbased approaches, including tools inspired by HaloScope [52],\nto capture subtle input variations and distinguish between\ntruthful and hallucinated outputs. These initiatives will enable\nevaluations to remain rigorous, scalable, and reflective of real-\nworld scientific challenges.\nEvaluating LLMs capabilities as research assistants at scale\nrequire a powerful infrastructure. We envision the STaR frame-\nwork evolving into a scalable, efficient, and user-friendly\nplatform for HPC environments, with a modular architecture\nthat supports dynamic and multimodal evaluations. Scalable\ninference backends, such as vLLM [81] and DeepSpeed Fast-\nGen [85], will enable efficient handling of large benchmarks\nand models. Collaborations with other national laboratories\nand NIST will contribute to consistent proxy benchmarks for\nsafety evaluations. By integrating these capabilities, STaR will\nstrive to enable robust, scalable, and reliable assessments of\nLLMs, fostering impactful applications in scientific discovery\nwhile prioritizing safety and computational efficiency.\nThis paper presents the current state of the effort at Argonne\nNational Laboratory to establish a methodology to evaluate\nLLMs capabilities as research assistants. We envision this\neffort as a continuous one because LLMs continue to progress\nand we will need to increase the difficulty of the different tests\nas LLMs progress.\nVIII. ACKNOWLEGEMENTS\nThis material is based upon work supported by Labora-\ntory Directed Research and Development (LDRD) funding\nfrom Argonne National Laboratory, provided by the Direc-\ntor, Office of Science, of the U.S. Department of Energy\nunder Contract No. DEAC02-06CH11357. LLNL work was\nprepared under Contract DE-AC52- 07NA27344, supported\nby the LLNL-LDRD Program under Project No. 24-ERD-\n058, and authored by Lawrence Livermore National Security,\nLLC under Contract No. DE-AC52-07NA27344 with the U.S.\nDepartment of Energy. This research used resources of the\nArgonne Leadership Computing Facility, a U.S. Department\nof Energy (DOE) Office of Science user facility at Argonne\nNational Laboratory and is based on research supported by the\nU.S. DOE Office of Science-Advanced Scientific Computing\nResearch Program, under Contract No. DE-AC02-06CH11357.\nWe gratefully acknowledge the computing resources provided\non Improv, Bebop, and Swing, high-performance computing\n\n\nclusters operated by the Laboratory Computing Resource\nCenter at Argonne National Laboratory. The United States\nGovernment retains, and the publisher, by accepting the article\nfor publication, acknowledges that the United States Gov-\nernment retains a non-exclusive, paid-up, irrevocable, world-\nwide license to publish or reproduce the published form of\nthis manuscript, or allow others to do so, for United States\nGovernment purposes. Other Argonne researchers voluntarily\ncontributing to the benchmarking effort are listed in the\nAppendix.\nREFERENCES\n[1] OpenAI Team, “GPT-4 Technical Report,” Preprint arXiv:2303.08774,\nMar. 2024.\n[2] Gemini Team, “Gemini 1.5: Unlocking multimodal understanding across\nmillions of tokens of context,” Preprint arXiv:2403.05530, 2024.\n[3] Anthropic\nTeam,\n“The\nClaude\n3\nmodel\nfamily:\nOpus,\nSonnet,\nHaiku,” Papers With Code, 2024, https://paperswithcode.com/paper/\nthe-claude-3-model-family-opus-sonnet-haiku.\n[4] L. Varanasi, “GPT-4 can ace the bar, but it only has a decent chance of\npassing the CFA exams. Here’s a list of difficult exams the ChatGPT\nand GPT-4 have passed.” https://www.businessinsider.com/list-here-are-\nthe-exams-chatgpt-has-passed-so-far-2023-1, Nov. 2023.\n[5] M. R. AI4Science and M. A. Quantum, “The impact of large language\nmodels on scientific discovery: a preliminary study using gpt-4,” 2023.\n[Online]. Available: https://arxiv.org/abs/2311.07361\n[6] Y. Liu, S. Ding, S. Zhou, W. Fan, and Q. Tan, “MolecularGPT:\nOpen large language model (LLM) for few-shot molecular property\nprediction,” arXiv preprint arXiv:2406.12950, 2024.\n[7] G. Benegas, C. Ye, C. Albors, J. C. Li, and Y. S. Song, “Genomic\nlanguage models: Opportunities and challenges,” Trends in Genetics,\n2025.\n[8] Y.-S. Ting, T. D. Nguyen, T. Ghosal, R. Pan, H. Arora, Z. Sun,\nT. de Haan, N. Ramachandra, A. Wells, S. Madireddy et al., “AstroMLab\n1: Who wins astronomy Jeopardy!?” Astronomy and Computing, p.\n100893, 2024.\n[9] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\nwith the MATH dataset,” NeurIPS, 2021.\n[10] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, E. Hambro,\nL. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language\nmodels can teach themselves to use tools,” Advances in Neural Infor-\nmation Processing Systems, vol. 36, pp. 68 539–68 551, 2023.\n[11] Z. Liu, Y. Chai, and J. Li, “Towards fully autonomous research powered\nby LLMs: Case study on simulations,” arXiv preprint arXiv:2408.15512,\n2024.\n[12] P. Ma, T.-H. Wang, M. Guo, Z. Sun, J. B. Tenenbaum, D. Rus,\nC. Gan, and W. Matusik, “LLM and simulation as bilevel optimizers: A\nnew paradigm to advance physical scientific discovery,” arXiv preprint\narXiv:2405.09783, 2024.\n[13] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, “Measuring massive multitask language understanding,”\nInternational Conference on Learning Representations, 2021.\n[14] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren,\nA. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang,\nR. Fan, X. Yue, and W. Chen, “MMLU-Pro: A more robust and\nchallenging multi-task language understanding benchmark,” Preprint\narXiv:2406.01574, 2024.\n[15] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\n[16] H. Zhang, J. Da, D. Lee, V. Robinson, C. Wu, W. Song, T. Zhao,\nP. Raja, D. Slack, Q. Lyu et al., “A careful examination of large\nlanguage model performance on grade school arithmetic,” arXiv preprint\narXiv:2405.00332, 2024.\n[17] Z. Zhang, Z. Jiang, L. Xu, H. Hao, and R. Wang, “Multiple-choice\nquestions are efficient and robust llm evaluators,” arXiv preprint\narXiv:2405.11966, 2024.\n[18] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\nand O. Tafjord, “Think you have solved question answering? try arc,\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457, 2018.\n[19] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-\nlaswag: Can a machine really finish your sentence?” arXiv preprint\narXiv:1905.07830, 2019.\n[20] X. Lu, H. Cao, Z. Liu, S. Bai, L. Chen, Y. Yao, H.-T. Zheng,\nand Y. Li, “MoleculeQA: A dataset to evaluate factual accuracy in\nmolecular comprehension,” Mar. 2024, arXiv:2403.08192 [cs]. [Online].\nAvailable: http://arxiv.org/abs/2403.08192\n[21] K. Guo, B. Nan, Y. Zhou, T. Guo, Z. Guo, M. Surve, Z. Liang,\nN.\nV.\nChawla,\nO.\nWiest,\nand\nX.\nZhang,\n“Can\nLLMs\nsolve\nmolecule puzzles? A multimodal benchmark for molecular structure\nelucidation,” Nov. 2024. [Online]. Available: https://openreview.net/\nforum?id=t1mAXb4Cop#discussion\n[22] A. Mirza, N. Alampara, S. Kunchapu, B. Emoekabu, A. Krishnan,\nM. Wilhelmi, M. Okereke, J. Eberhardt, A. M. Elahi, M. Greiner, C. T.\nHolick, T. Gupta, M. Asgari, C. Glaubitz, L. C. Klepsch, Y. K¨oster,\nJ. Meyer, S. Miret, T. Hoffmann, F. A. Kreth, M. Ringleb, N. Roesner,\nU. S. Schubert, L. M. Stafast, D. Wonanke, M. Pieler, P. Schwaller, and\nK. M. Jablonka, “Are large language models superhuman chemists?”\n2024. [Online]. Available: https://arxiv.org/abs/2404.01475\n[23] T. Koˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\nand E. Grefenstette, “The narrativeqa reading comprehension challenge,”\nTransactions of the Association for Computational Linguistics, vol. 6,\npp. 317–328, 2018.\n[24] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and\nC. D. Manning, “HotpotQA: A dataset for diverse, explainable multi-hop\nquestion answering,” in Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, 2018.\n[25] W. Chen, H. Zha, Z. Chen, W. Xiong, H. Wang, and W. Y. Wang,\n“HybridQA: A dataset of multi-hop question answering over tabular and\ntextual data,” Findings of the Association for Computational Linguistics:\nEMNLP 2020, 2020.\n[26] Z.\nWei,\nW.\nJi,\nX.\nGeng,\nY.\nChen,\nB.\nChen,\nT.\nQin,\nand\nD. Jiang, “ChemistryQA: A complex question answering dataset\nfrom chemistry,” Oct. 2020. [Online]. Available: https://openreview.net/\nforum?id=oeHTRAehiFF\n[27] T. Guo, B. Nan, Z. Liang, Z. Guo, N. Chawla, O. Wiest, X. Zhang et al.,\n“What can large language models do in chemistry? a comprehensive\nbenchmark on eight tasks,” Advances in Neural Information Processing\nSystems, vol. 36, pp. 59 662–59 688, 2023.\n[28] J.\nLi,\nJ.\nLi,\nY.\nLiu,\nD.\nZhou,\nand\nQ.\nLi,\n“TOMG-Bench:\nEvaluating LLMs on text-based open molecule generation,” Dec. 2024,\narXiv:2412.14642 [cs]. [Online]. Available: http://arxiv.org/abs/2412.\n14642\n[29] E. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani,\nO. Sanseviero, L. Tunstall, and T. Wolf, “Open llm leaderboard,” 2023.\n[30] E. Glazer, E. Erdil, T. Besiroglu, D. Chicharro, E. Chen, A. Gunning,\nC. F. Olsson, J.-S. Denain, A. Ho, E. de Oliveira Santos, O. J¨arviniemi,\nM. Barnett, R. Sandler, M. Vrzala, J. Sevilla, Q. Ren, E. Pratt,\nL. Levine, G. Barkley, N. Stewart, B. Grechuk, T. Grechuk, S. V.\nEnugandla, and M. Wildon, “FrontierMath: A benchmark for evaluating\nadvanced mathematical reasoning in AI,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2411.04872\n[31] Center for AI Safety and Scale AI, “Humanity’s last exam,” 2024,\naccessed: 2025-01-15. [Online]. Available: https://agi.safe.ai/submit\n[32] R. Underwood, M. Madhyastha, R. Burns, and B. Nicolae, “Evostore:\nTowards scalable storage of evolving learning models,” in HPDC’24:\nThe 33nd International Symposium on High-Performance Parallel\nand Distributed Computing, Pisa, Italy, 2024. [Online]. Available:\nhttps://hal.science/hal-04617763\n[33] B. Nicolae, T. Islam, R. Ross, H. V. Dam, K. Assogba, P. Shpilker,\nM. Titov, M. Turilli, T. Wang, O. Kilic, S. Jha, and L. Pouchard,\n“Building the i (interoperability) of fair for performance reproducibility\nof large-scale composable workflows in recup,” in REWORDS’23: The\n3rd Workshop on Reproducible Workflows, Data Management, and\nSecurity (with eScience’23), Limassol, Cyprus, 2023, pp. 1–7. [Online].\nAvailable: https://hal.inria.fr/hal-04343665\n[34] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes, “Autonomous\nchemical research with large language models,” Nature, vol. 624, no.\n7992, pp. 570–578, 2023.\n[35] Y. Chen, J. Arkin, C. Dawson, Y. Zhang, N. Roy, and C. Fan, “Autotamp:\nAutoregressive task and motion planning with llms as translators and\ncheckers,” in 2024 IEEE International conference on robotics and\nautomation (ICRA).\nIEEE, 2024, pp. 6695–6702.\n\n\n[36] D. Albert and S. Billinger, “Reproducing and extending experiments\nin behavioral strategy with large language models,” arXiv preprint\narXiv:2410.06932, 2024.\n[37] B. Y. Lin, Y. Deng, K. Chandu, F. Brahman, A. Ravichander, V. Py-\natkin, N. Dziri, R. L. Bras, and Y. Choi, “WildBench: Benchmarking\nLLMs with challenging tasks from real users in the wild,” Preprint\narXiv:2406.04770, 2024.\n[38] Z. Zhu, Y. Yang, and Z. Sun, “HaluEval-Wild: Evaluating hallucinations\nof\nlanguage\nmodels\nin\nthe\nwild,”\n2024.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2403.04307\n[39] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, ““Do\nAnything Now”: Characterizing and evaluating in-the-wild jailbreak\nprompts\non\nlarge\nlanguage\nmodels,”\n2024.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2308.03825\n[40] Y. Zeng, Y. Yang, A. Zhou, J. Z. Tan, Y. Tu, Y. Mai, K. Klyman,\nM. Pan, R. Jia, D. Song et al., “AIR-Bench 2024: A safety benchmark\nbased on risk categories from regulations and policies,” arXiv preprint\narXiv:2407.17436, 2024.\n[41] Z. Zhang, L. Lei, L. Wu, R. Sun, Y. Huang, C. Long, X. Liu,\nX. Lei, J. Tang, and M. Huang, “Safetybench: Evaluating the safety of\nlarge language models with multiple choice questions,” arXiv preprint\narXiv:2309.07045, 2023.\n[42] L. Li, B. Dong, R. Wang, X. Hu, W. Zuo, D. Lin, Y. Qiao, and J. Shao,\n“Salad-bench: A hierarchical and comprehensive safety benchmark for\nlarge language models,” arXiv preprint arXiv:2402.05044, 2024.\n[43] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso et al., “Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of\nlanguage models,” arXiv preprint arXiv:2206.04615, 2022.\n[44] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu,\nZ. Xiong, R. Dutta, R. Schaeffer et al., “DecodingTrust: A comprehen-\nsive assessment of trustworthiness in GPT models,” Advances in Neural\nInformation Processing Systems, vol. 36, 2024.\n[45] Y. Huang, L. Sun, H. Wang, S. Wu, Q. Zhang, Y. Li, C. Gao, Y. Huang,\nW. Lyu, Y. Zhang et al., “Position: Trustllm: Trustworthiness in large\nlanguage models,” in International Conference on Machine Learning.\nPMLR, 2024, pp. 20 166–20 270.\n[46] A. Buszydlik, K. Dobiczek, M. T. Oko´n, K. Skublicki, P. Lippmann,\nand J. Yang, “Red teaming for large language models at scale: Tackling\nhallucinations on mathematics tasks,” arXiv preprint arXiv:2401.00290,\n2023.\n[47] Z.-W. Hong, I. Shenfeld, T.-H. Wang, Y.-S. Chuang, A. Pareja,\nJ. R. Glass, A. Srivastava, and P. Agrawal, “Curiosity-driven red-\nteaming for large language models,” in The Twelfth International\nConference on Learning Representations, 2024. [Online]. Available:\nhttps://openreview.net/forum?id=4KqkizXgXU\n[48] X. Zhang, Y. Xie, J. Huang, J. Ma, Z. Pan, Q. Liu, Z. Xiong, T. Ergen,\nD. Shim, H. Lee et al., “MASSW: A new dataset and benchmark tasks\nfor AI-assisted scientific workflows,” arXiv preprint arXiv:2406.06357,\n2024.\n[49] E. O. Pyzer-Knapp, J. W. Pitera, P. W. J. Staar, S. Takeda, T. Laino, D. P.\nSanders, J. Sexton, J. R. Smith, and A. Curioni, “Accelerating materials\ndiscovery using artificial intelligence, high performance computing and\nrobotics,” npj Computational Materials, vol. 8, no. 1, p. 84, Apr 2022.\n[50] B. Y. Lin, Y. Deng, K. Chandu, F. Brahman, A. Ravichander,\nV.\nPyatkin,\nN.\nDziri,\nR.\nL.\nBras,\nand\nY.\nChoi,\n“WildBench:\nBenchmarking LLMs with challenging tasks from real users in\nthe wild,” ArXiv, vol. abs/2406.04770, 2024. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:270357771\n[51] Z. Lin, S. Trivedi, and J. Sun, “Generating with confidence: Uncertainty\nquantification for black-box large language models,” Transactions on\nMachine Learning Research, 2023.\n[52] X. Du, C. Xiao, and Y. Li, “HaloScope: Harnessing unlabeled LLM\ngenerations for hallucination detection,” in The Thirty-eighth Annual\nConference on Neural Information Processing Systems, 2024. [Online].\nAvailable: https://openreview.net/forum?id=nfK0ZXFFSn\n[53] J. Duan, H. Cheng, S. Wang, A. Zavalny, C. Wang, R. Xu, B. Kailkhura,\nand K. Xu, “Shifting attention to relevance: Towards the predictive\nuncertainty quantification of free-form large language models,” in 62nd\nAnnual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), 2024, pp. 5050–5063.\n[54] F. Ye, M. Yang, J. Pang, L. Wang, D. F. Wong, E. Yilmaz, S. Shi,\nand Z. Tu, “Benchmarking LLMs via uncertainty quantification,” arXiv\npreprint arXiv:2401.12794, 2024.\n[55] Z. Chen, P. Hong, and S. Madireddy, “Quantifying uncertainty in large\nlanguage models: Applications in molecular chemistry tasks,” NeurIPS\n2024 Workshop on Statistical Foundations of LLMs and Foundation\nModels, 2024.\n[56] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “GPT-4\ntechnical report,” arXiv preprint arXiv:2303.08774, 2023.\n[57] Y. Babuji, A. Woodard, Z. Li, D. S. Katz, B. Clifford, R. Kumar,\nL. Lacinski, R. Chard, J. M. Wozniak, I. Foster et al., “Parsl: Pervasive\nparallel programming in Python,” in 28th International Symposium on\nHigh-Performance Parallel and Distributed Computing, 2019, pp. 25–\n36.\n[58] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica, “Efficient Memory Management for Large\nLanguage Model Serving with PagedAttention,” in 29th Symposium on\nOperating Systems Principles.\nKoblenz Germany: ACM, Oct. 2023,\npp. 611–626.\n[59] R. Lacombe, K. Wu, and E. Dilworth, “Climatex: Do llms accurately\nassess human expert confidence in climate statements?” 2023.\n[60] R.\nVaid,\nK.\nPant,\nand\nM.\nShrivastava,\n“Towards\nfine-grained\nclassification of climate change related social media text,” in 60th Annual\nMeeting of the Association for Computational Linguistics: Student\nResearch Workshop.\nDublin, Ireland: Association for Computational\nLinguistics, May 2022, pp. 434–443. [Online]. Available: https:\n//aclanthology.org/2022.acl-srw.35\n[61] D. Thulke, Y. Gao, P. Pelser, R. Brune, R. Jalota, F. Fok, M. Ramos,\nI. van Wyk, A. Nasir, H. Goldstein, T. Tragemann, K. Nguyen,\nA. Fowler, A. Stanco, J. Gabriel, J. Taylor, D. Moro, E. Tsymbalov,\nJ. de Waal, E. Matusov, M. Yaghi, M. Shihadah, H. Ney, C. Dugast,\nJ. Dotan, and D. Erasmus, “Climategpt: Towards ai synthesizing inter-\ndisciplinary research on climate change,” 2024.\n[62] S. Solomon, D. Qin, M. Manning, Z. Chen, M. Marquis, K. Averyt,\nM. Tignor, and H. Miller, “IPCC fourth assessment report (AR4),”\nClimate change, vol. 374, 2007.\n[63] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani,\nJ. Michael, and S. R. Bowman, “GPQA: A graduate-level Google-proof\nQ&A benchmark,” Preprint arXiv:2311.12022, 2023.\n[64] H. Zhang, J. Da, D. Lee, V. Robinson, C. Wu, W. Song, T. Zhao,\nP. Raja, D. Slack, Q. Lyu, S. Hendryx, R. Kaplan, M. Lunati, and S. Yue,\n“A careful examination of large language model performance on grade\nschool arithmetic,” 2024, arxiv 2405.00332.\n[65] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\nJ. Schulman, “Training verifiers to solve math word problems,” 2021.\n[Online]. Available: https://arxiv.org/abs/2110.14168\n[66] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang,\nZ. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica,\n“Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,” 2023.\n[Online]. Available: https://arxiv.org/abs/2306.05685\n[67] M. Besta, L. Paleari, A. Kubicek, P. Nyczyk, R. Gerstenberger, P. Iff,\nT. Lehmann, H. Niewiadomski, and T. Hoefler, “CheckEmbed: Effective\nverification of LLM solutions to open-ended tasks,” arXiv preprint\narXiv:2406.02524, 2024.\n[68] M. Tian, L. Gao, D. Zhang, X. Chen, C. Fan, X. Guo, R. Haas,\nP. Ji, K. Krongchon, Y. Li, S. Liu, D. Luo, Y. Ma, H. TONG,\nK. Trinh, C. Tian, Z. Wang, B. Wu, S. Yin, M. Zhu, K. Lieret,\nY. Lu, G. Liu, Y. Du, T. Tao, O. Press, J. Callan, E. A. Huerta, and\nH. Peng, “Scicode: A research coding benchmark curated by scientists,”\nin The Thirty-eight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2024. [Online]. Available:\nhttps://openreview.net/forum?id=ADLaALtdoG\n[69] A. Yanguas-Gil, M. T. Dearing, J. W. Elam, J. C. Jones, S. Kim,\nA. Mohammad, C. T. Nguyen, and B. Sengupta, “Benchmarking large\nlanguage models for materials synthesis: The case of atomic layer\ndeposition,” 2024. [Online]. Available: https://arxiv.org/abs/2412.10477\n[70] E. Alvaro and A. Yanguas-Gil, “Characterizing the field of atomic\nlayer deposition: Authors, topics, and collaborations,” PLOS ONE,\nvol. 13, no. 1, pp. 1–19, 01 2018. [Online]. Available: https:\n//doi.org/10.1371/journal.pone.0189137\n[71] A. Maurya, R. Underwood, M. M. Rafique, F. Cappello, and B. Nicolae,\n“DataStates-LLM: Lazy asynchronous checkpointing for large language\nmodels,” in 33rd International Symposium on High-Performance\nParallel and Distributed Computing, ser. HPDC ’24.\nNew York,\n\n\nNY, USA: Association for Computing Machinery, 2024, p. 227–239.\n[Online]. Available: https://doi.org/10.1145/3625549.3658685\n[72] J. Guo, V. Mohanty, J. H. Piazentin Ono, H. Hao, L. Gou, and\nL. Ren, “Investigating interaction modes and user agency in human-\nLLM collaboration for domain-specific data analysis,” in Extended\nAbstracts of the CHI Conference on Human Factors in Computing\nSystems, ser. CHI ’24.\nACM, May 2024, p. 1–9. [Online]. Available:\nhttp://dx.doi.org/10.1145/3613905.3651042\n[73] P. Hager, F. Jungmann, R. Holland, K. Bhagat, I. Hubrecht, M. Knauer,\nJ. Vielhauer, M. Makowski, R. Braren, G. Kaissis, and D. Rueckert,\n“Evaluation and mitigation of the limitations of large language models\nin clinical decision-making,” Nature Medicine, vol. 30, pp. 2613–2622,\n07 2024.\n[74] Y. Gal and Z. Ghahramani, “Dropout as a Bayesian approximation:\nRepresenting model uncertainty in deep learning,” in international\nconference on machine learning.\nPMLR, 2016, pp. 1050–1059.\n[75] L.\nKuhn,\nY.\nGal,\nand\nS.\nFarquhar,\n“Semantic\nuncertainty:\nLinguistic\ninvariances\nfor\nuncertainty\nestimation\nin\nnatural\nlanguage\ngeneration,”\nin\nThe\nEleventh\nInternational\nConfer-\nence\non\nLearning\nRepresentations,\n2023.\n[Online].\nAvailable:\nhttps://openreview.net/forum?id=VD-AYtP0dve\n[76] M. Xiong, Z. Hu, X. Lu, Y. Li, J. Fu, J. He, and B. Hooi,\n“Can\nLLMs\nexpress\ntheir\nuncertainty?\nAn\nempirical\nevaluation\nof confidence elicitation in LLMs,” in The Twelfth International\nConference on Learning Representations, 2024. [Online]. Available:\nhttps://openreview.net/forum?id=gjeQKFxFpZ\n[77] R. Bommasani, P. Liang, and T. Lee, “Holistic evaluation of language\nmodels,” Annals of the New York Academy of Sciences, vol. 1525, no. 1,\npp. 140–146, 2023.\n[78] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,\nL. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muen-\nnighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron,\nL. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou,\n“A framework for few-shot language model evaluation,” 2024, https:\n//zenodo.org/records/12608602.\n[79] Y. Fu, L. Ou, M. Chen, Y. Wan, H. Peng, and T. Khot, “Chain-of-thought\nhub: A continuous effort to measure large language models’ reasoning\nperformance,” arXiv preprint arXiv:2305.17306, 2023.\n[80] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von\nPlaten, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush, “HuggingFace’s transformers: State-of-the-\nart natural language processing,” Preprint arXiv:1910.03771, 2020.\n[81] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica, “Efficient memory management for large\nlanguage model serving with PagedAttention,” in 29th Symposium on\nOperating Systems Principles, 2023, pp. 611–626.\n[82] A. B. Yoo, M. A. Jette, and M. Grondona, “Slurm: Simple Linux utility\nfor resource management,” in Workshop on job scheduling strategies for\nparallel processing.\nSpringer, 2003, pp. 44–60.\n[83] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang,\nM. Elibol, Z. Yang, W. Paul, M. I. Jordan et al., “Ray: A distributed\nframework for emerging AI applications,” in 13th USENIX symposium\non operating systems design and implementation (OSDI 18), 2018, pp.\n561–577.\n[84] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[85] C. Holmes, M. Tanaka, M. Wyatt, A. A. Awan, J. Rasley, S. Rajb-\nhandari, R. Y. Aminabadi, H. Qin, A. Bakhtiari, L. Kurilenko et al.,\n“DeepSpeed-FastGen: High-throughput text generation for LLMs via\nMII and DeepSpeed-Inference,” Preprint arXiv:2401.08671, 2024.\n[86] Argonne,\n“Polaris\nsupercomputer,”\n2025,\nhttps://alcf.anl.gov/\nalcf-resources.\n[87] A. D. McNaughton, G. Ramalaxmi, A. Kruel, C. R. Knutson, R. A.\nVarikoti, and N. Kumar, “CACTUS: Chemistry agent connecting tool-\nusage to science,” Preprint arXiv:2405.00972, 2024.\n[88] S. Madireddy, C. Xu, F. Cappello, S. Bhattacharya, B. Kailkhura,\nM. Foltin, T. Kumar, and B. Li, “Comprehensive multi-stage evalua-\ntion of language models for scientific skill and safety red-teaming,”\nAccelerating the Development and Use of Generative AI for Science\nand Engineering: The Trillion Parameter Consortium (TPC) Workshop\nat The International Conference for High Performance Computing,\nNetworking, Storage, and Analysis (SC24), 2024.\nAPPENDIX\nAcknowledgment The following Argonne researchers vol-\nuntarily contributed to the creation of the AI4S bench-\nmarks and the different Lab-style experiments and Field-style\nexperiments. Adelina Grindeanu Aikaterini Vriza, Akshay\nDave, Alec Sandy, Aleksandr Obabko, Alex Lavens, Alex\nRodriguez, Alfonso Napoles, Allison Bennett-Iron, Alrhman\nAbed, Amaka Okafor, Andreas Wilke, Andrei Patapenka, An-\ndrew Siegel, Andrey Yakovenko, Annabelle Boots, Arvind Ra-\nmanathan, Ayesha Shafiuddin, Ayman Moawad, Azton Wells,\nBarnali Chowdry, Becca Weinberg, Ben Blakely, Bipul Barua,\nBrahim Mustapha, Brandon Sforzo, Brian Ingram, CD Phatak,\nChandrachur Bhattacharya, Changyong Park, Cheng Wang,\nChengjun Sun, Chiara Bissolotti, Chihpin Chuang, Christopher\nHenry, Clinton Cohagan, Dan Meyer, David Neto, Debora\nMeira, Devesh Reddy, Dion Antonopoulos, Doga Gursoy,\nDonald Walko, Eliu Huerta, Emily Dietrich, Emily Ohland,\nEshan Sharma, Fang Zhang, Fangfang Xia, Fanny Rodolakis,\nFelipe Wang Liu, Feng Qiu, Filippo Simini, Francesco Salucci,\nFrank Alexander, Gautham Dharuman, Gosia Korbas, Greg\nMorin, Gyorgy Babnigg, Hairong Shang, Hanu Arava, Haoran\nWu, Hassam Harb, Huihuo Zheng, Ian Cloet, Jakob Elias,\nJames O’Sullivan, JD Emberson, Jeffrey Wang, Jesse Smith,\nJim Grudzinski, Jiwen Fan, John Carwardine, John David\nCarter, John Hutchinson, Jonghwan Kwon, Jorge Pulpeiro\nGonzalez, Josh Hlavenka, Juanjuan Huang, Julie Parente,\nJunjing Deng, Justin Hoffman, Justin Wozniak, Kamlesh\nSuthar, Katherine Asztalos, Kathy Macal, Keith Taddei, Kent\nBostick, Kent Wootton, Khalid Hossain, Kirill Prozument,\nKirsten Laurin-Kovitz, Krishna Teja Chitty-Venkata, Kwang\nHoon Baek, Lahsen Assoufid, Laurent Chapon, Lee Zachos,\nLisa Childers, Longwen Ou, Lorenzo Nocivelli, Mark Hereld,\nMatthew Dearing, Matthew Diamond, Matthew Sampson,\nMaulik Shukla, Max Delferro, Meaghan Bruening, Megan\nClifford, Mei Zhi-Gang, Meltem Demirtas, Meltem Urgun\nDemirtas, Michael Buehlman, Michael Carpenter, Michael\nPrince, Michel van Veenendaal, Mike Edelen, Mike Wilkins,\nMillie Firestone, Ming Du, Minhui Zhu, Monica Neukomm,\nMuhsin Ameen, Murali Emani, Murat Keceli, Mustafa Unal,\nNatalia Zuniga, Nathan Nichols, Nazib Choudhury Siddique,\nNeeraj Hanumante, Neil James Getty, Nesar Ramachan-\ndra, Nicholas Frontiere, Nicholas Lee-Ping Chia, Nick Gob-\nerville, Nick Moore, Nicola Ferrier, Nidhi Gupta, Nina An-\ndrejevic, Nithin Manne, Noah Paulson, Olaf Borkiewicz,\nOlga Antipova, Osama Mohsen, Pamela Weisenhorn, Parfait\nGasana, Peter Kenesei, Philip Dinemis, Philippe Piot, Pinaki\nPal, Prakash Thimmapuram, Priyash Misra, Qiaomu Yang,\nQuenten Proussard, Rae Sharp-Geiger, Rajeev Surendran As-\nsary, Rajeev Thakur, Rajkumar Kettimuthu, Rao Kotamarthi,\nRavi Madduri, Rick Vilium, Rob Ross, Robert Gaynor, Roger\nSersted, Rui Hu, Ryan Aydelott, Ryan Chu, Sam Wheeler,\nSang-il Yim, Scott Collis, Scott Parent, Sejal Rhodes Seth\nOckerman Shelly Kelly, Shilpika, Shivam Barwey, Shubham\n\n\nKesharwani, Siba Dowell, Sibendu Som, Siby Platthottam, Sid\nRaskar, Simon Corrodi, Sixbert Muhoza, Srutarshi Banerjee,\nStefano Fenu, Stu Hannay, Susan Babinec, Suyin Grass Wang,\nTadbhagya Kumar, Taemin Kim, Tanjin He, Tanwi Mallick,\nTaylor Childers, Tekin Bicer, Temitope Oproudek, Tianhao\nGu, Tianyi Li, Tiffany Kinnibrugh, Tim Ashenfelter, Tim\nHobbs, Tim Nguyen, Tim Williams, Timothy Suzuki, Todd\nMunson, Tom Brettin, Tom Peterka, Tom Uram, Troy Arco-\nmano, Utkarsh Nanda Valerie Taylor, Varuni Sastry, Venkata\nDevesh Reddy, Seethi Vincent Chirio, Vincent Chia Vincenzo\nCappello, Vivian Sullivan, Vrindaa Somjit, Wei Li, Weixing\nCheng, Wen Zhuang, Xiaoyang Liu, Xuli Wu, Yang Wang,\nYanna Chen, Yawei Yang, Yeni Li, Yeonjun Jeong, Yildiz\nOcrun, Yinghu Piao, Yiqi Yu, Yue Cao, Yuejun Yan, Yuepeng\nZhang, Yuri Oksuzian, Zhao Zixuan, Zhi Zhou, Zixuan Zhao.\n\n\nFig. 9. MCQ Authoring Interface with example question.\n\n\nFig. 10. MCQ Reviewing Interface with example question.\n\n\nFig. 11. Labstyle experiment collection interface: Problem setup.\n\n\nFig. 12. abstyle experiment collection interface: Prompt, response, assessment.\n\n\nFig. 13. abstyle experiment collection interface: Final assessment.\n\n\nFig. 14. A SciCode main problem is divided into multiple simpler subproblems for ease of implementation. Docstrings outline the requirements and specify\nthe input-output formats. The scientific background is provided by expert annotators to offer necessary context and guidance.\n\n\nArgo Science Assistant System Prompt\nYou are an AI language model named Argo that is a highly knowledgeable AI assistant specializing in scientific domains such\nas physics, chemistry, biology, mathematics, and engineering. Your goal is to provide clear, detailed, and accurate explanations\nto scientific questions. Use precise terminology, include relevant equations or formulas when necessary, and break down complex\nconcepts into understandable parts. Organize your responses by separating conceptual sections with descriptive sub-header titles to\nenhance readability. If applicable, cite credible sources or reference landmark studies to support your answers.\nFig. 15. Argo System Prompt.\nChemical Risk Evaluation System Prompt\nYou are an expert in synthetic and computational chemistry with extensive knowledge in organic, inorganic, and organometallic\nchemistry. Your role is to solve complex chemistry problems by providing accurate and detailed insights into synthesis pathways,\nreaction mechanisms, chemical properties, and safety considerations. You are well-versed in retrosynthesis, modern synthetic methods,\nand analytical techniques for structure verification for energetics. Additionally, you excel at interpreting chemical databases and\ncomputational predictions to propose efficient and feasible synthetic routes. When responding, ensure that your answers: 1) are\nconcise and actionable, 2) comply with any specified constraints (e.g., have specific atoms or substructures), and 3) produce a\nchemically valid output in SMILES or SELFIES format.\nFig. 16. ChemRisk System Prompt.\n\n\nAI4S Benchmark MCQ LLM-as-a-judge Prompt\nBelow is a multiple-choice question, 1 correct answer, 4 incorrect distractors, the domain or field of study, and required skills to\nanswer the question. Be very discriminating, only provide high scores where they are earned, it is crucial to be critical of errors or\ninadequacies to improve. Here is the json dictionary formatted multiple choice question, skills and domains:\n{\n’Question’: ’{}’,\n’Answer’: ’{}’,\n’Distractors’: {},\n’Skills’: {},\n’Domains’: {}\n}\nYour job is to evaluate the complete question, answers, skills and domain on the following criteria:\n1) Appropriate: Assess whether the question’s difficulty aligns with graduate-level knowledge and skills in the subject area.\nConsider complexity of concepts involved, depth of analysis required, sophistication of language used, application of advanced\ntheories or methodologies. Simple recall from a paper is not sufficiently difficult. Rate the question’s appropriateness on a\nscale of 1–5, where 1 is too basic and 5 is suitably challenging for graduate-level students.\n2) Relevant: Evaluate how closely the answer choices relate to the question posed. Consider direct connection between question\nand answers, absence of extraneous or off-topic information, alignment with the core concept being tested. Score relevance\non a scale of 1–5, where 1 indicates poor relevance and 5 indicates high relevance across all answer choices.\n3) Complete: Assess whether the answer choices fully address all aspects of the question. Consider coverage of all key elements\nmentioned in the question, absence of partial or incomplete responses, sufficient detail in each answer choice. There should\nbe one correct answer and four distractors. Rate completeness on a scale of 1–5, where 1 indicates incomplete responses and\n5 indicates comprehensive coverage in all answer choices.\n4) Correct: Verify that there is only one unambiguously correct answer among the choices. Consider clarity and precision of\nlanguage in both question and answers, absence of partially correct answers, distinctness of the correct answer from distractors.\nScore this criterion as either Pass (5) (one clear correct answer) or Fail (0) (multiple correct answers or no correct answer).\n5) Controversial: Determine if the correct answer is generally accepted in the field, avoiding contentious or debatable topics.\nConsider alignment with current academic consensus, avoidance of ongoing debates or unresolved issues, use of well-\nestablished facts or theories. Rate the non-controversial nature on a scale of 1-5, where 1 indicates highly controversial\nand 5 indicates widely accepted, uncontroversial content.\n6) Mathematic: Check that the question and answers do not rely on arithmetic calculations. Consider absence of numerical\ncomputations, focus on conceptual understanding rather than mathematical operations, use of qualitative rather than quantitative\nreasoning. Score this criterion as either Pass (no arithmetic required) (5) or Fail (arithmetic is necessary to answer) (0).\n7) Skills: Evaluate whether the skills required to answer the question are appropriate for the subject and level. Consider alignment\nwith course learning objectives, relevance to real-world applications in the field, balance of lower-order (recall) and higher-\norder (analysis, synthesis) thinking skills. Rate the appropriateness of skills on a scale of 1–5, where 1 indicates misaligned\nskills and 5 indicates perfectly aligned skills for the subject and level.\n8) Domains: Assess if the knowledge domains covered by the question are suitable for the subject area. Consider relevance to\nthe course or exam topic, coverage of key subject areas within the field, appropriate breadth and depth of domain knowledge\ntested. Score the appropriateness of domains on a scale of 1–5, where 1 indicates poorly chosen domains and 5 indicates\nhighly appropriate domains for the subject area.\nIt is important to be extremely discriminating. Only the best possible questions should receive a maximum score. Correct feedback\nis vital and preferred over erroneous positivity. Provide the scores in a json dictionary formatted object with the following fields:\n{\n’Appropriate’: (score, ’reason’),\n’Relevant’: (score, ’reason’),\n’Complete’: (score, ’reason’),\n’Correct’: (score, ’reason’),\n’Controversial’: (score, ’reason’),\n’Mathematic’: (score, ’reason’),\n’Skills’: (score, ’reason’),\n’Domains’: (score, ’reason’)\n}\nFig. 17. LLM as a judge prompt for MCQ evaluation.\n\n\nLLM Scientific Reasoning Evaluation Prompt\nYou are tasked with analyzing conversation transcripts between humans and a Large Language Model (LLM) to evaluate the LLM’s\nscientific reasoning capabilities. Your objective is to identify the LLM’s strengths and weaknesses in various aspects of scientific\nthinking, using the following framework as a guide. Provide specific examples from the transcript to support your assessment. If a\ncriteria is not applicable to the problem or question being asked in the transcript, note that it is not applicable. Be critical, do not\nbe overly positive if it is not evidenced.\nScientific Reasoning Skills Framework Core Scientific Principles Understanding of the Scientific Method\n• Observation and Questioning: Does the LLM demonstrate an understanding of how scientific inquiry begins with observation\nand the formulation of testable questions? Can it identify good vs. poorly formed scientific questions?...\nKnowledge of Scientific Concepts\n• Domain Knowledge: Does the LLM possess accurate knowledge of basic scientific concepts in various fields (e.g., biology,\nchemistry, physics)? How well is it able to answer questions related to different fields of science?...\nCritical Evaluation of Scientific Information\n• Source Credibility: Does the LLM demonstrate an ability to assess the credibility of scientific sources?...\nSpecific Scientific Reasoning Skills Experimental Design\n• Identifying Variables: Can the LLM identify the independent, dependent, and control variables in a given experimental\nscenario?...\nData Analysis and Interpretation\n• Statistical Significance: Does the LLM understand the concept of statistical significance?...\nCausal Reasoning\n• Identifying Cause and Effect: Can the LLM correctly identify cause-and-effect relationships in scientific contexts?...\nCommunication of Scientific Ideas\n• Clarity and Precision: Does the LLM communicate scientific ideas clearly and precisely?...\nScoring Format\nThe quantitative assessment should be provided in the following JSON format:\n{\n\"Core Scientific Principles\": {\n\"Understanding of the Scientific Method\": {\n\"Observation and Questioning\": score,\n\"Hypothesis Formation\": score,\n\"Prediction\": score,\n\"Experimentation\": score,\n\"Data Collection and Analysis\": score,\n\"Conclusion and Theory Formation\": score\n},\n...\n}\n}\nInstructions\n1) Read the conversation transcript carefully.\n2) Identify instances where the LLM demonstrates strengths or weaknesses in any of the scientific reasoning skills listed above.\n3) For each identified instance, provide:\n• The specific skill being assessed (e.g., Hypothesis Formation, Data Analysis: Correlation vs. Causation)\n• A brief description of the context in the conversation\n• Direct quotes from the transcript that exemplify the LLM’s performance (both the user’s prompt and the LLM’s response)\n• An assessment of whether this represents a strength or weakness, and a brief explanation of your reasoning\n4) Assign quantitative scores from 1-10 for the criteria as formatted above, if a criteria is not applicable to the transcript give a\nscore of -1.\n• A score of -1 means the criteria cannot be assessed as it is not applicable to the transcript\n• A score of 1 means the LLM completely failed at on the criteria\n• A score of 10 means the LLM could not have possibly responded better, and completely meets the criteria\nTranscript [Insert transcript here]\nFig. 18. LLM as a judge prompt for Field style evaluation.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20309v1.pdf",
    "total_pages": 33,
    "title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
    "authors": [
      "Franck Cappello",
      "Sandeep Madireddy",
      "Robert Underwood",
      "Neil Getty",
      "Nicholas Lee-Ping Chia",
      "Nesar Ramachandra",
      "Josh Nguyen",
      "Murat Keceli",
      "Tanwi Mallick",
      "Zilinghan Li",
      "Marieme Ngom",
      "Chenhui Zhang",
      "Angel Yanguas-Gil",
      "Evan Antoniuk",
      "Bhavya Kailkhura",
      "Minyang Tian",
      "Yufeng Du",
      "Yuan-Sen Ting",
      "Azton Wells",
      "Bogdan Nicolae",
      "Avinash Maurya",
      "M. Mustafa Rafique",
      "Eliu Huerta",
      "Bo Li",
      "Ian Foster",
      "Rick Stevens"
    ],
    "abstract": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}