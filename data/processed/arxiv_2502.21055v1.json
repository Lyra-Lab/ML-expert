{
  "id": "arxiv_2502.21055v1",
  "text": "Quantum-aware Transformer model for state\nclassification\nPrzemysław Sekuła1,2[0000-0002-4599-1077], Michał\nRomaszewski1[0000-0002-8227-929X], Przemysław\nGłomb1[0000-0002-0215-4674], Michał Cholewa1[0000-0001-6549-1590], and\nŁukasz Pawela1[0000-0002-0476-7132]\n1 Institute of Theoretical and Applied Informatics, Polish Academy of Sciences,\nBałtycka 5, 44-100 Gliwice, Poland\n2 University of Maryland, College Park, Department of Civil and Environmental\nEngineering, MD, USA\n{psekula,mromaszewski,przemg,mcholewa,lpawela}@iitis.pl\nAbstract. Entanglement is a fundamental feature of quantum mechan-\nics, playing a crucial role in quantum information processing. However,\nclassifying entangled states, particularly in the mixed-state regime, re-\nmains a challenging problem, especially as system dimensions increase.\nIn this work, we focus on bipartite quantum states and present a data-\ndriven approach to entanglement classification using transformer-based\nneural networks. Our dataset consists of a diverse set of bipartite states,\nincluding pure separable states, Werner entangled states, general entan-\ngled states, and maximally entangled states. We pretrain the transformer\nin an unsupervised fashion by masking elements of vectorized Hermi-\ntian matrix representations of quantum states, allowing the model to\nlearn structural properties of quantum density matrices. This approach\nenables the model to generalize entanglement characteristics across dif-\nferent classes of states. Once trained, our method achieves near-perfect\nclassification accuracy, effectively distinguishing between separable and\nentangled states. Compared to previous Machine Learning, our method\nsuccessfully adapts transformers for quantum state analysis, demonstrat-\ning their ability to systematically identify entanglement in bipartite sys-\ntems. These results highlight the potential of modern machine learn-\ning techniques in automating entanglement detection and classification,\nbridging the gap between quantum information theory and artificial in-\ntelligence.\nKeywords: Quantum entanglement · State classification · Transform-\ners · Large language models\n1\nIntroduction\nThe entanglement phenomenon is at the heart of quantum information, en-\nabling its key applications such as quantum computing, secure communication,\narXiv:2502.21055v1  [quant-ph]  28 Feb 2025\n\n\n2\nAuthors Suppressed Due to Excessive Length\nand enhanced metrology. Unlike classical correlations, entanglement represents\na uniquely quantum feature where the state of a system cannot be described\nindependently of its subsystems. This fundamental property underlies protocols\nlike quantum teleportation, superdense coding, and quantum key distribution,\nas well as quantum computational speedups [25,17]. However, not all quantum\nstates exhibit entanglement, and distinguishing entangled states from separable\nones is a crucial yet challenging problem in quantum information science. The\nability to efficiently classify quantum states has direct implications for the prac-\ntical implementation of quantum technologies, motivating the development of\nreliable entanglement detection and classification methods.\nBipartite quantum states, which describe systems naturally partitioned into\ntwo subsystems, are fundamental in quantum information science and serve as\na basis for studying quantum correlations and computational advantages. These\nstates reside in a tensor product space L(Cd1 ⊗Cd2), where entanglement emerges\nas a crucial resource for various quantum information applications [14,6], such as\nsecure communication, quantum-enhanced computation, and efficient informa-\ntion transfer. For pure bipartite states, i.e., vectors in Cd1 ⊗Cd2, entanglement\ncan be clearly identified: a state is separable if and only if it can be expressed as\na tensor product of subsystem states. Any departure from this structure signi-\nfies entanglement, which directly influences quantum nonlocality, measurement-\nbased quantum computing, and the efficiency of entanglement-assisted protocols.\nThe identification of entanglement for bipartite states is straightforward in\nthe case of pure states but becomes significantly more challenging when con-\nsidering mixed bipartite states. A variety of analytical and numerical methods\nhave been developed for the characterization and detection of mixed-state en-\ntanglement. One of the most celebrated approaches is based on the positivity\nof the partial transpose (PPT), introduced by Peres [19] and Horodecki [11].\nWhile the PPT criterion is both necessary and sufficient for separability in low-\ndimensional cases (C2 ⊗C2 and C2 ⊗C3), in higher dimensions, a state can be\nPPT and still entangled. Such states are known as bound entangled states [12]\n(see Fig. 1 for sketch). They cannot be distilled into pure entangled states using\nlocal operations and classical communication (LOCC), rendering them entan-\ngled yet practically “inaccessible” for certain quantum information protocols [1].\nThis discovery highlighted the limitations of the PPT criterion and the intricate\nnature of entanglement in higher-dimensional systems. To emphasize this differ-\nence, entangled states which are not bound entangled are sometimes called free\nentangled states.\nTo address these limitations, other techniques—particularly entanglement\nwitnesses—have been introduced [14,10]. An entanglement witness is a Hermi-\ntian operator W with the property that Tr(W ρsep) ≥0 for all separable states\nρsep, but Tr(W ρent) < 0 for at least one entangled state ρent. Finding and\noptimizing entanglement witnesses can often be formulated via semidefinite pro-\ngramming techniques, and in many cases, witnesses can be tailored to detect\nspecific classes of entangled states, including those exhibiting bound entangle-\nment. Additional approaches to mixed-state entanglement include various entan-\n\n\nQLLM\n3\nglement measures (e.g., negativity, entanglement of formation), which attempts\nto quantify the degree of entanglement in a given density operator [24,20].\nAnother class of approaches relies on machine learning (ML) to identify en-\ntanglement directly from the data. In [8], authors use automated ML for state\nclassification. Instead of directly measuring entanglement properties, the state\nis reconstructed, and entanglement is inferred from the data itself. Recently,\nTransformers have also been applied to quantum random number validation [7],\nshowcasing their ability to handle large input sequences efficiently and perform\nmultiple statistical tests in parallel. The same self-attention mechanism that cap-\ntures subtle global dependencies in random bit streams can likewise model the\nintricate correlations of bipartite quantum states, suggesting that Transform-\ners are a promising architecture for entanglement classification under partial or\nnoisy data.\nDespite significant progress, a complete classification of bipartite entangled\nmixed states remains an open challenge, particularly as system dimensions grow.\nIn this work, we take a data-driven approach to this problem by generating a\ndiverse dataset of pure and mixed states through multiple methods and employ-\ning transformer-based neural networks to analyze their properties and classify\nthem. We demonstrate that entanglement identification can be performed effec-\ntively from the data itself, extending the range of successful classification beyond\nprevious studies. Furthermore, we validate the application of transformer archi-\ntectures in this domain, achieving a breakthrough where prior deep learning\napproaches [8] have struggled. By integrating machine learning with established\ntheoretical criteria, we provide a scalable framework for systematically detecting\nand categorizing entanglement, bridging the gap between quantum information\ntheory and modern Artificial Intelligence techniques.\nΩ\nSEP\nNPT\nBOUND\nFig. 1. Schematic representation of the set of mixed quantum states Ωdepicting sepa-\nrable states (SEP) ρsep, bound entangled states (BOUND) σbound and negative partial\ntranspose states (NPT) ξnpt.\n\n\n4\nAuthors Suppressed Due to Excessive Length\nThis paper is organized as follows. In Section 2, we explain the methodol-\nogy for constructing our dataset. In Section 3, we detail our proposition of the\nQuantum-aware Transformer model and its training scheme. In Section 4, we\npresent the results of validation experiments and discussion.\nThe code for this paper is publicly available on GitHub3 under an open license\nto facilitate result reproducibility and transparency and the data can be shared\nupon a reasonable request.\n2\nDataset generation\nQuantum states, the basic objects of quantum mechanics, can be broadly classi-\nfied as pure or mixed. A pure state is described by a single d-dimensional vector\n|ψ⟩in a complex Euclidean space Cd, and a corresponding density operator\nρ = |ψ⟩⟨ψ|. This operator satisfies ρ2 = ρ and Tr(ρ) = 1. When d = 2, the\ncorresponding system is called a qubit; for d = 3, it is called a qutrit.\nIn contrast, a mixed state is represented by a statistical ensemble of pure\nstates, described by a density operator L(Cd) ∋ρ = P\ni pi |ψi⟩⟨ψi|, where pi ∈\n[0, 1] and P\ni pi = 1 [16]. Such states often emerge from partial traces of larger\nsystems or incomplete information about the quantum system under study. Note\nthat ρ is a positive semidefinite matrix.\nIn many quantum information scenarios, one focuses on bipartite states.\nThese describe a physical system that can be naturally partitioned into two\nsubsystems, |ψ⟩⊗|ϕ⟩, associated with complex Euclidean spaces Cd1 and Cd2.\nThe total state then lives in the tensor product space Cd1 ⊗Cd2. Bipartite quan-\ntum systems are not only a cornerstone for foundational studies of quantum\ncorrelations but also play a central role in key quantum information tasks such\nas quantum teleportation, quantum key distribution, and superdense coding.\nFor pure bipartite states, there exists a straightforward way to distinguish\nentangled from separable states: a pure bipartite state |ψ⟩∈Cd1 ⊗Cd2 is sepa-\nrable if and only if it can be written as |ξ⟩= |ψ⟩⊗|ϕ⟩. Any deviation from this\nproduct structure indicates entanglement.\nIn this work, we study the following cases of discrimination between separable\nand entangled states:\n1. two-qubit states, C2 ⊗C2,\n2. qubit-qutrit systems, C2 ⊗C3,\n3. qutrit-qutrit systems, C3 ⊗C3,\nFor each of these, we generate a dataset consisting of:\n1. pure separable states,\n2. general entangled states,\n3. Werner entangled states (for qubit-qubit and qutrit-qutrit systems),\n4. maximally entangled states (for qubit-qubit and qutrit-qutrit systems),\n3 https://github.com/iitis/LQM\n\n\nQLLM\n5\n5. bound entangled states from the family by Horodecki [13] (for qutrit-qutrit\nsystems).\nFor this work we assume we have access to the full tomography [3] of each ρ,\nhence we encode each state as a vector of 2d1d2 real variables.\nThe details of the sampling are described in the following subsections. Uni-\nform sampling of quantum states (either pure or mixed) was conducted utilizing\nthe QuantumInformation.jl package [5].\n2.1\nSampling pure separable states\nIn this case, we need to sample uniformly normalized vectors of the form Cd1 ⊗\nCd2 ∋|ψ⟩= |ϕ1⟩⊗|ϕ2⟩. This is done in the following steps:\n1. Sample a non-normalized |x⟩∈Cd1 with each element xi such that Re(xi) ∼\nN(0, 1) and Im(xi) ∼N(0, 1).\n2. Normalize |x⟩: |ϕ1⟩=\n|x⟩\n∥|x⟩∥.\n3. Repeat steps 1 and 2 to obtain |ϕ2⟩.\n4. Put |ψ⟩= |ϕ1⟩⊗|ϕ2⟩.\nThis procedure ensures that each |ϕi⟩is sampled from the Haar measure.\n2.2\nSampling Werner entangled states\nA Werner state is a mixed state having the form\nρwer = (1 −p) |ψ⟩⟨ψ| + pρ∗,\n(1)\nwhere rho∗is the maximally mixed state\nρ∗= 1\nd2\n(2)\nand\n|ψ⟩=\n1\n√\nd\nd−1\nX\ni=0\n|ii⟩.\n(3)\nIt can be shown that the state remain entangled for\np <\nd\nd + 1.\n(4)\nWe sample p uniformly in this interval and construct ρwer.\n\n\n6\nAuthors Suppressed Due to Excessive Length\n2.3\nSampling general entangled states\nWe sample general entangled states by uniformly sampling the state of all mixed\nquantum states and only accepting the state as entangled when it is NPT. The\nsteps are:\n1. Sample a square Ginibre matrix, G, of dimension d1d2. Elements Gij per-\ntain to the complex normal distribution Re(Gij) ∼N(0, 1) and Im(Gij) ∼\nN(0, 1).\n2. Calculate W = GG†.\n3. Normalize the trace, ρ =\nW\nTr W .\n4. Check the Peres-Horodecki criterion. If ρ is NPT, accept it into the set;\notherwise, repeat the procedure.\nNote that this procedure is quite efficient, especially as the dimension increases,\nas the relative volume of the separable states diminishes [26,27,28].\n2.4\nSampling maximally entangled states\nWe sample maximally entangled states by sampling unitary matrices, vectorizing\nthem, and renormalizing them [21]. The procedure is:\n1. Sample a square Ginibre matrix G of dimension d as described in Section 2.3.\n2. Calculate its QR decomposition G = QR, where Q is a unitary matrix and\nR is upper triangular.\n3. Multiply i-th column of Q, Qi, by the phase of the corresponding diagonal\nelement of R, Rii, thus obtaining the i-th column of a unitary matrix U.\nThis step is necessary to ensure the proper distribution of eigenvalues of\nU [15,18].\n4. Vectorize the matrix U and normalize by\n1\n√\nd.\n2.5\nSampling bound entangled states\nThis procedure is based on a family of bound entangled states described in [13].\nFirst, let us introduce\nσ+ = 1\n3 (|01⟩⟨01| + |12⟩⟨12| + |20⟩⟨20|)\nσ−= 1\n3 (|10⟩⟨10| + |21⟩⟨21| + |02⟩⟨02|)\n|ψ⟩=\n1\n√\n3 (|00⟩+ |11⟩+ |22⟩) .\n(5)\nWe construct a state ρ as follows\nρα = 2\n7 |ψ⟩⟨ψ| + α\n7 σ+ + 5 −α\n7\nσ−.\n(6)\nDepending on parameter α this state can be separable (2 ≤α ≤3), bound\nentangled (3 < α ≤4) or free entangled (4 < α ≤5). Hence, we sample α\nuniformly in the range (3, 4].\n\n\nQLLM\n7\n3\nQuantum-aware Transformer model\nWe use a Transformer [23] based model for processing quantum state matri-\nces, leveraging its self-attention mechanism for structured data reconstruction.\nTransformers, originally designed for language sequence modeling, operate on\nsets of input tokens with global context awareness. Here, we treat quantum\nstate matrices as tokenized inputs, where each matrix element (real and imag-\ninary parts) forms a structured sequence. By applying a masked autoencoding\nstrategy, we train the model to reconstruct missing matrix elements, allowing\nit to learn intrinsic quantum state properties. This approach enables the Trans-\nformer to capture patterns in quantum data and improve downstream tasks like\nentanglement classification.\n3.1\nModel Description\nWe employ a masked Transformer architecture, referred to as the MaskedTrans-\nformer, to reconstruct partially masked 2D quantum data. The input is first\nflattened from\n\u0002\nB, 2N 2\u0003\ninto\n\u0002\nB, N 2, 2\n\u0003\n, where B stands for the size of data\nbatch, N is the size of the original square matrix (each entry contains real and\nimaginary parts). Tokens in this representation are randomly masked and re-\nplaced by a mask token, and the partially masked sequence is passed through\na Transformer encoder composed of multi-head self-attention layers and feed-\nforward networks.\nEach token embedding is further augmented with a trainable positional vector\npi, ensuring that the Transformer retains the relative location of each matrix\nelement in the spatial grid. Concretely, we added pi ∈Rd to the embedded token\nxi, yielding\n˜xi = Embed\n\u0000xi\n\u0001\n+ pi,\n(7)\nwhere Embed is the token embedding function. This positional encoding is cru-\ncial to provide the necessary positional information for the self-attention mech-\nanism.\nA simple linear decoder projects the final encoded representations back to real\nand imaginary components, reconstructing both the originally masked and un-\nmasked portions. This masked-reconstruction approach is analogous to masked\nautoencoders or BERT [4] like masking methods, thereby encouraging the model\nto infer missing entries from the surrounding context.\nIn our experiments, we tested three configurations corresponding to n = 4\n(for C2⊗C2), n = 6 (for C2⊗C3), and n = 9 (for C3⊗C3). All other Transformer\nhyperparameters (embedding dimension, number of attention heads, number of\nlayers, and dropout) were selected during the initial research phase by trail-and-\nerror method, and remained the same across these experiments.\nWe used separate datasets for each experiment for pretraining and classifi-\ncation, as presented in Table 1. In every experiment, the dataset was divided\ninto training, validation, and test subsets. The validation evaluation was per-\nformed every epoch, and the test evaluation was performed after the training\nwas completed.\n\n\n8\nAuthors Suppressed Due to Excessive Length\nPretraining\nClassification\nGroup name\nC2 ⊗C2\nC2 ⊗C3\nC3 ⊗C3\nC2 ⊗C2\nC2 ⊗C3\nC3 ⊗C3\nsep\n4,000,000\n8,000,000\n6,000,000\n1,000,000\n1,000,000\n1,000,000\ngeneral-ent\n2,000,000\n8,000,000\n2,000,000\n300,000\n1,000,000\n500,000\nwerner-ent\n2,000,000\n2,000,000\n300,000\n500,000\nmax-ent\n2,000,000\n2,000,000\n300,000\n500,000\nhorodecki-bound\n2,000,000\n500,000\nhorodecki-ent\n2,000,000\n500,000\nTable 1. Training data sizes for Pretraining and Classifier tasks. Numbers indicate\nsamples used for each model configuration and data type. Group names correspond to\nthe type of data (pure separable, general entangled etc.) described in Section 2.\n3.2\nPretraining\nIn this stage, we train the MaskedTransformer from scratch as an autoencoder.\nSpecifically, we:\n– Use the datasets of 10 mln (for C2 ⊗C2) and 16 mln (C2 ⊗C3 and C3 ⊗C3),\ndescribed in detail in Table 1 in the Pretraining columns.\n– Split the dataset it into training (90%), validation (5%), and test (5%) par-\ntitions.\n– Randomly mask a given fraction (15%) of tokens, replacing them with a\nlearned mask token.\n– Pass the masked tokens through the Transformer encoder and reconstruct\nthem via a linear decoder head.\n– Optimize the full reconstruction loss using mean squared error (MSE).\nThe training uses a well-known cosine-annealing learning rate schedule and stan-\ndard PyTorch Lightning callbacks for logging and checkpointing. Upon comple-\ntion, the final checkpoints were saved for subsequent classification.\nTo evaluate the pretraining performace we introduce a metric called Her-\nmitian distance, that measures the deviation of a matrix from being perfectly\nHermitian by computing the average Frobenius norm of the difference between\nthe matrix and its conjugate transpose:\nh = 1\nb\nb\nX\nk=1\nq\n||Ak −A†\nk||F ,\n(8)\nwhere b is the number of matrices in the batch, and Ak denotes the k-th complex\nmatrix. The notation A†\nk refers to its conjugate transpose, and ∥· ∥F indicates\nthe Frobenius norm. For a Hermitian matrix, Ak = A†\nk, which makes the norm\nvanish. When the matrix is split into real and imaginary parts, Ak = Rk +\ni Ik, Hermiticity requires Rk to be symmetric\n\u0000Rk = RT\nk\n\u0001\nand Ik to be anti-\nsymmetric\n\u0000Ik = −IT\nk\n\u0001\n. Consequently, the quantity Ak −A†\nk captures deviations\nfrom these symmetries, and its Frobenius norm measures how far the matrix is\n\n\nQLLM\n9\nfrom being perfectly Hermitian. Averaging over all matrices in the batch yields\nthe final distance h.\nWe used this metric to evaluate our pretraining process by assessing how\nwell the pretrained model preserves the Hermitian structure of the data when\n15% of the matrix is reconstructed by a network. A lower Hermitian distance\nindicates that the intrinsic mathematical properties are maintained, serving as\na meaningful indicator of the quality of pretraining.\n3.3\nClassifier Training\nAfter pretraining, we fine-tune or adapt the learned Transformer weights for a\ndownstream binary classification task. The core steps are:\n– Load the pre-trained Transformer weights into a new model that augments\nthe Transformer encoder with a feed-forward classification head (two-class\noutput).\n– Use the datasets of 1.9 mln (for C2 ⊗C2), 2 mln (for C2 ⊗C3), and 3.5 mln\n(for C3 ⊗C3), described in detail in Table 1 in the Classification subsection.\nThis data is separate from the pretraining data.\n– Split the dataset into training (90%), validation (5%), and test (5%) parti-\ntions.\n– Train the network using cross-entropy loss and the same PyTorch Light-\nning setup, with logging, checkpointing, and cosine-annealing learning rate\nschedule.\nThis two-stage approach leverages the pre-trained Transformer’s learned rep-\nresentation of the quantum matrices, enhancing the performance of the down-\nstream classification task.\n4\nResults and discussion\nTo determine the final results of the pretraining and classification, we used the\nseparate data subset that was not used during the training phase neither for\ntraining nor for validation and testing. This subset comprises of 100,000 samples\nfor each of the classes. The final evaluation was carried out for both pretraining\nand classification after the entire training process was completed.\nPretraining During the pretraining phase, both the loss function and the Her-\nmitian distance improved significantly. Table 2 shows the averaged results of the\nHermitian distance metric for the pretraining phase. The results are consistent\nand show that, overall, the pretraining process ended up generating models that\nare able to reconstruct the Hermitian structure of the data. Surprisingly, the\nHermitian distances achieved very low values during the early pretraining. We\ndid not observe any significant improvement in this metric after the first few\nepochs, whereas the loss function continued to decrease. This suggests that the\nmodel learned the Hermitian structure of the data very quickly, and the loss\nfunction was optimized to a greater extent.\n\n\n10\nAuthors Suppressed Due to Excessive Length\nUntrained\nPretrained\nGroup name\nC2 ⊗C2\nC2 ⊗C3\nC3 ⊗C3\nC2 ⊗C2\nC2 ⊗C3\nC3 ⊗C3\nsep\n6.686\n3.718\n8.101\n0.265\n0.364\n0.419\ngeneral-ent\n6.704\n3.716\n8.098\n0.189\n0.187\n0.167\nwerner-ent\n6.645\n8.131\n0.183\n0.222\nmax-ent\n6.693\n8.102\n0.296\n0.449\nhorodecki-bound\n8.104\n0.120\nhorodecki-ent\n8.102\n0.122\nTable 2. Averaged Hermitian distances for untrained, and fully pretrained models.\nClassification The classification results are presented in Table 3. It is impor-\ntant to emphasize that, while results are presented for each type of state, the\nclassification was deliberately kept binary–entangled vs. separable–as this is the\nrelevant distinction for practical applications. The results show that the model\nwas able to classify the states with very high accuracy. The results are consistent\nacross all dimensions and classes. The only errors appear for the pure separable\nstates C2 ⊗C2 and C2 ⊗C3 groups, where a few of states were misclassified as\nentangled. This confirms the validity of our approach, as the model effectively\ncaptures the structural properties of quantum states and generalizes well across\ndifferent state classes and dimensions, with only minimal misclassification in\nspecific cases.\nGroup name\nC2 ⊗C2\nC2 ⊗C3\nC3 ⊗C3\nsep\n99.995%\n99.998%\n100%\ngeneral-ent\n100%\n100%\n100%\nwerner-ent\n100%\n100%\nmax-ent\n100%\n100%\nhorodecki-bound\n100%\nhorodecki-ent\n100%\nTable 3. Accuracy of binary classification (entangled vs separated) for each data group.\nTo further verify our results, we tested whether the deep fine-tuning during\nclassification training provides an additional learning benefit beyond the pre-\ntraining phase. Specifically, we took a pretrained model and froze all its layers\nexcept for the final classification layer, which was then fine-tuned on labeled data.\nThis approach aimed to determine whether the pretrained representations alone\nwere sufficient for entanglement classification or if further adaptation was nec-\nessary. The fine-tuned model performed nearly perfectly on C2 ⊗C2 and C2 ⊗C3\nstates but struggled with C3 ⊗C3 states, achieving around 85% accuracy. A de-\ntailed breakdown revealed that while entangled states were consistently classified\ncorrectly, the model frequently misclassified separable states. This suggests that\n\n\nQLLM\n11\nwhile the pretrained model captures general entanglement patterns, adapting\ndeeper layers during training may be crucial for distinguishing subtle features in\nhigher-dimensional separable states.\nDiscussion Our work is closely related to [8], which also explores automated en-\ntanglement classification. However, we achieve significantly better results, with\nnear-perfect accuracy compared to their reported range of 62–88%. While their\napproach struggled with deep learning, we successfully adapted transformers\ninto a highly effective classification method. An important distinction is that\nthe presence of bound entangled states in our dataset does not degrade perfor-\nmance. Additionally, our dataset is substantially larger, containing millions of\nstates, whereas theirs consisted of only 3,254. Their dataset generation method,\nin principle, allows for bound entangled states in any dimension, while our ap-\nproach focuses on a specific family of states for the C3 ⊗C3 case. However, their\ngeneration method is much more computationally expensive. Overall, our results\nare consistent with theirs, but we extend the approach significantly, demonstrat-\ning the feasibility of deep learning for entanglement classification at a much larger\nscale.\nOur work serves as an example of the successful integration of machine learn-\ning techniques with quantum information science. Similar approaches–whether\nin developing quantum information models [2] or solving quantum computing\nproblems [22]–represent a promising direction for the field. We expect that this\nfusion of computational methods and quantum theory will gain increasing promi-\nnence, much like the impact of machine learning in computational chemistry.\nOur approach addresses a different but related problem compared to [9].\nTheir work focuses on generating entanglement witnesses for specific types of\nquantum states and specially structured witnesses. While their method is well-\nexecuted, it is inherently limited, as reflected in their choice of states. In contrast,\nour approach is applied to a significantly larger dataset, allowing for a broader\nand more flexible classification of entanglement. However, their method extends\nto multipartite entanglement, an area we have not yet explored.\nConclusions We have demonstrated that transformer-based neural networks can\neffectively classify bipartite quantum states as entangled or separable by learning\ndirectly from quantum state matrices. By leveraging a masked autoencoding\npretraining strategy, our model captures the structural properties of density\nmatrices, achieving near-perfect classification accuracy across various state types\nand dimensions. These results highlight the potential of modern deep learning\narchitectures for quantum information processing, paving the way for scalable,\ndata-driven approaches to entanglement detection and beyond.\nAcknowledgments. This project was supported by the National Science Center\n(NCN), Poland, under Projects: Sonata Bis 10, No. 2020/38/E/ST3/00269 (L.P.)\nDisclosure of Interests. The authors have no competing interests to declare that\nare relevant to the content of this article.\n\n\n12\nAuthors Suppressed Due to Excessive Length\nReferences\n1. Bennett, C.H., DiVincenzo, D.P., Mor, T., Shor, P.W., Smolin, J.A., Terhal, B.M.:\nUnextendible product bases and bound entanglement. Physical Review Letters\n82(26), 5385–5388 (1999). https://doi.org/10.1103/PhysRevLett.82.5385\n2. Cholewa, M., Gawron, P., Głomb, P., Kurzyk, D.: Quantum hidden Markov models\nbased on transition operation matrices. Quantum Information Processing 16(4),\n101 (Mar 2017). https://doi.org/10.1007/s11128-017-1544-8\n3. Cramer, M., Plenio, M.B., Flammia, S.T., Somma, R., Gross, D., Bartlett, S.D.,\nLandon-Cardinal, O., Poulin, D., Liu, Y.K.: Efficient quantum state tomography.\nNature Communications 1(1), 149 (2010). https://doi.org/10.1038/ncomms1147\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-\nrectional transformers for language understanding (2019), https://arxiv.org/abs/\n1810.04805\n5. Gawron, P., Kurzyk, D., Pawela, Ł.: QuantumInformation.jl—a julia package\nfor numerical computation in quantum information theory. PLOS ONE 13(12),\ne0209358 (dec 2018). https://doi.org/10.1371/journal.pone.0209358\n6. Gisin, N., Thew, R.: Quantum communication. Nature Photonics 1, 165–171\n(2007). https://doi.org/10.1038/nphoton.2007.22\n7. Goel, R., Xiao, Y., Ramezani, R.: Transformer models as an efficient replacement\nfor statistical test suites to evaluate the quality of random numbers. In: 2024 In-\nternational Symposium on Networks, Computers and Communications (ISNCC).\npp. 1–6 (2024). https://doi.org/10.1109/ISNCC62547.2024.10758985\n8. Goes, C.B.D., Canabarro, A., Duzzioni, E.I., Maciel, T.O.: Automated ma-\nchine learning can classify bound entangled states with tomograms. Quan-\ntum Information Processing 20(3),\n99 (Mar 2021). https://doi.org/10.1007/\ns11128-021-03037-9\n9. Greenwood, A.C., Wu, L.T., Zhu, E.Y., Kirby, B.T., Qian, L.: Machine-learning-\nderived entanglement witnesses. Physical Review Applied 19(3), 034058 (2023).\nhttps://doi.org/10.1103/PhysRevApplied.19.034058\n10. Gühne, O., Tóth, G.: Entanglement detection. Physics Reports 474(1-6), 1–75\n(2009). https://doi.org/10.1016/j.physrep.2009.02.004\n11. Horodecki, M., Horodecki, P., Horodecki, R.: Separability of mixed states: necessary\nand sufficient conditions. Physics Letters A 223(1-2), 1–8 (1996). https://doi.org/\n10.1016/S0375-9601(96)00706-2\n12. Horodecki, P., Horodecki, M., Horodecki, R.: Mixed-state entanglement and distil-\nlation: is there a “bound” entanglement in nature? Physical Review Letters 80(24),\n5239–5242 (1998). https://doi.org/10.1103/PhysRevLett.80.5239\n13. Horodecki, P., Horodecki, M., Horodecki, R.: Bound entanglement can be ac-\ntivated. Physical Review Letters 82(5),\n1056 (1999). https://doi.org/10.1103/\nPhysRevLett.82.1056\n14. Horodecki, R., Horodecki, P., Horodecki, M., Horodecki, K.: Quantum entangle-\nment. Reviews of Modern Physics 81(2), 865–942 (2009). https://doi.org/10.1103/\nRevModPhys.81.865\n15. Kukulski, R., Nechita, I., Pawela, Ł., Puchała, Z., Życzkowski, K.: Generating\nrandom quantum channels. Journal of Mathematical Physics 62(6) (2021). https:\n//doi.org/10.1063/5.0038838\n16. Nielsen, M.A., Chuang, I.L.: Quantum Computation and Quantum Information.\nCambridge University Press, 10th anniversary ed. edn. (2010)\n\n\nQLLM\n13\n17. Nielsen, M.A., Chuang, I.L.: Quantum computation and quantum information.\nCambridge university press (2010)\n18. Ozols, M.: How to generate a random unitary matrix. http://home.lu.lv/sd20008\n(2009), accessed: 2025-02-12\n19. Peres, A.: Separability criterion for density matrices. Physical Review Letters 77,\n1413–1415 (1996). https://doi.org/10.1103/PhysRevLett.77.1413\n20. Plenio, M.B., Virmani, S.: An introduction to entanglement measures. Quantum\nInformation & Computation 7(1-2), 1–51 (2007). https://doi.org/10.5555/2011706.\n2011707\n21. Puchała, Z., Jenčová, A., Sedlák, M., Ziman, M.: Exploring boundaries of quantum\nconvex structures: Special role of unitary processes. Physical Review A 92, 012304\n(2015). https://doi.org/10.1103/PhysRevA.92.012304\n22. Śmierzchalski, T., Pawela, Ł., Puchała, Z., Trzciński, T., Gardas, B.: Post-error cor-\nrection for quantum annealing processor using reinforcement learning. In: Groen,\nD., de Mulatier, C., Paszynski, M., Krzhizhanovskaya, V.V., Dongarra, J.J., Sloot,\nP.M.A. (eds.) Computational Science – ICCS 2022. pp. 261–268. Springer Interna-\ntional Publishing (2022)\n23. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,\nU.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.)\nAdvances in Neural Information Processing Systems. vol. 30. Curran Asso-\nciates, Inc. (2017), https://proceedings.neurips.cc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n24. Vidal, G., Werner, R.F.: A computable measure of entanglement. Physical Review\nA 65(3), 032314 (2002). https://doi.org/10.1103/PhysRevA.65.032314\n25. Watrous, J.: The theory of quantum information. Cambridge university press\n(2018)\n26. Życzkowski, K.: Volume of the set of separable states. ii. Physical Review A 60(5),\n3496 (1999). https://doi.org/10.1103/PhysRevA.60.3496\n27. Zyczkowski, K., Horodecki, P., Sanpera, A., Lewenstein, M.: On the volume of the\nset of mixed entangled states. Physical Review A 58(arXiv: quant-ph/9804024),\n883 (1998). https://doi.org/10.1103/PhysRevA.58.883\n28. Życzkowski, K., Horodecki, P., Sanpera, A., Lewenstein, M.: Volume of the set of\nseparable states. Physical Review A 58(2), 883 (1998). https://doi.org/10.1103/\nPhysRevA.58.883\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21055v1.pdf",
    "total_pages": 13,
    "title": "Quantum-aware Transformer model for state classification",
    "authors": [
      "Przemysław Sekuła",
      "Michał Romaszewski",
      "Przemysław Głomb",
      "Michał Cholewa",
      "Łukasz Pawela"
    ],
    "abstract": "Entanglement is a fundamental feature of quantum mechanics, playing a crucial\nrole in quantum information processing. However, classifying entangled states,\nparticularly in the mixed-state regime, remains a challenging problem,\nespecially as system dimensions increase. In this work, we focus on bipartite\nquantum states and present a data-driven approach to entanglement\nclassification using transformer-based neural networks. Our dataset consists of\na diverse set of bipartite states, including pure separable states, Werner\nentangled states, general entangled states, and maximally entangled states. We\npretrain the transformer in an unsupervised fashion by masking elements of\nvectorized Hermitian matrix representations of quantum states, allowing the\nmodel to learn structural properties of quantum density matrices. This approach\nenables the model to generalize entanglement characteristics across different\nclasses of states. Once trained, our method achieves near-perfect\nclassification accuracy, effectively distinguishing between separable and\nentangled states. Compared to previous Machine Learning, our method\nsuccessfully adapts transformers for quantum state analysis, demonstrating\ntheir ability to systematically identify entanglement in bipartite systems.\nThese results highlight the potential of modern machine learning techniques in\nautomating entanglement detection and classification, bridging the gap between\nquantum information theory and artificial intelligence.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}