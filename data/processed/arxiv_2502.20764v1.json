{
  "id": "arxiv_2502.20764v1",
  "text": "Visual Attention Exploration in Vision-Based Mamba Models\nJunpeng Wang*\nVisa Research\nChin-Chia Michael Yehâ€ \nVisa Research\nUday Singh Sainiâ€¡\nVisa Research\nMahashweta DasÂ§\nVisa Research\nğ‘\nğ‘\nğ‘\nğ‘‘\nFigure 1: Our visual exploration tool contains two visualization components. The Scatterplot view on the left shows the dimensionality\nreduction results. The Patch view on the right shows the patch layout and highlights the patches of interest.\nABSTRACT\nState space models (SSMs) have emerged as an efficient alterna-\ntive to transformer-based models, offering linear complexity that\nscales better than transformers. One of the latest advances in SSMs,\nMamba, introduces a selective scan mechanism that assigns trainable\nweights to input tokens, effectively mimicking the attention mech-\nanism. Mamba has also been successfully extended to the vision\ndomain by decomposing 2D images into smaller patches and arrang-\ning them as 1D sequences. However, it remains unclear how these\npatches interact with (or attend to) each other in relation to their\noriginal 2D spatial location. Additionally, the order used to arrange\nthe patches into a sequence also significantly impacts their attention\ndistribution. To better understand the attention between patches and\nexplore the attention patterns, we introduce a visual analytics tool\nspecifically designed for vision-based Mamba models. This tool\nenables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout\na Mamba model. Using the tool, we also investigate the impact of\ndifferent patch-ordering strategies on the learned attention, offering\nfurther insights into the modelâ€™s behavior.\n1\nINTRODUCTION\nState space models (SSMs) use state variables to mathematically\ndescribe the state of a dynamic system. They have a long history of\nmodeling time series problems, where the state variables are time-\ndependent. Recent advances [4,5,12] have shown that SSMs achieve\nperformance on par with state-of-the-art transformer models [3,13].\nAdditionally, their linear time complexity allows them to outperform\ntransformers in latency-critical applications.\n*e-mail: junpenwa@visa.com\nâ€ e-mail: miyeh@visa.com\nâ€¡e-mail: udasaini@visa.com\nÂ§e-mail: mahdas@visa.com\nMamba [4], also known as S6, is a cutting-edge SSM that en-\nhances its predecessor S4 [5] with a selective scan mechanism. This\nmechanism enables the model to assign trainable weights to input\ntokens, allowing it to filter out less relevant information and empha-\nsize more relevant details. These trainable weights are similar to the\nattention mechanism in transformers, which determines how much\nfocus a token should place on other tokens. Since its release in late\n2023, vision-based applications of Mamba have been rapidly devel-\noped. Similar to how transformers are adapted for vision tasks [3],\nvision-based Mamba models also decompose images into smaller\npatches and arrange them into sequences as inputs.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11\n12 13 14 15\nroute 1\nroute 1 order\nroute 2\nroute 3\nroute 4\nFigure 2: The four-way cross-scan in VMamba [10].\nIn vision transformers [3], all image patches are fed into the\nmodels simultaneously, with positional encoding used to differenti-\nate patches at different positions. As a result, the order of patches\nhas a relatively minor impact. However, in vision-based Mamba\nmodels, the patches are fed into the models sequentially one af-\nter another (similar to RNNs). This means that the order of the\npatches is crucial, as each patch can only collect information from\nits preceding ones. To ensure that each patch has access to all other\npatches, both forward and backward scans are often performed. For\nexample, vision-Mamba (Vim) [17] uses routes 1 and 3 in Fig. 2 to\nallow each patch to â€œseeâ€ both its preceding and succeeding patches.\nVMamba [10] uses all four routes in Fig. 2 to preserve spatial locality\nbetween patches within the same row and column.\nA vision-based Mamba model consists of a hierarchy of Mamba\nblocks, each of which learns the attention between image patches.\nHowever, it remains unclear (1) if the attention pattern is fixed within\na block, (2) how attention patterns evolve across blocks, and (3) how\narXiv:2502.20764v1  [cs.LG]  28 Feb 2025\n\n\npatch-ordering strategies impact the attention pattern. This paper\nseeks to address these questions using a visual analytics approach.\nSpecifically, we extract the attention learned at each stage of the\nvision-based Mamba model and apply dimensionality reduction tech-\nniques to reveal attention patterns across stages. We also profile the\nattention patterns at each patch to disclose how attention is spatially\ndistributed relative to the patchâ€™s position. Lastly, we introduce dif-\nferent patch-ordering strategies and compare their resulting attention\npatterns. In summary, our contributions are twofold:\n1. We design and develop a visual analytics tool to explore and\nsummarize attention patterns in vision-based Mamba models.\n2. We introduce multiple patch-ordering strategies and investigate\ntheir impact on patch attention in vision-based Mamba models.\n2\nBACKGROUND AND RELATED WORK\nOur work focuses on interpreting the attention mechanisms in vision-\nbased Mamba models. Here, we briefly review the history of SSMs\nand summarize early work on attention interpretation.\n2.1\nState Space Models (SSMs)\nState space models (SSMs) are sequence modeling techniques that\ndate back to the 1960s. They gained significant attention after Gu et\nal. [5] integrated the HiPPO matrix into them. The resulting model,\nknown as S4, demonstrated substantial improvements in efficiency\nfor modeling long sequences. However, the performance of S4\nis not comparable to that of transformers, as it cannot effectively\ndistinguish different input tokens. To overcome this, Gu and Dao [4]\nintroduced the selective scan mechanism into S4, giving rise to S6,\nalso known as Mamba. The selective scan allows Mamba to assign\ndifferent weights to input tokens, enabling the model to learn the\nrelative importance of each token. This allows Mamba to focus more\non important tokens and selectively ignore less relevant ones. The\nlearned weights are similar to the attention weights in transformers.\nMamba demonstrates performance on par with transformers across a\nwide range of applications. More importantly, its linear complexity\nmakes it a superior choice for many latency-critical applications,\nwhere the quadratic complexity of transformers would be prohibitive.\nsÃ—ğ‘ Ã—3\nÃ—2\ns/8Ã—ğ‘ /8Ã—2â„\ndown   sample\ns/16Ã—ğ‘ /16Ã—4â„\ns/32Ã—ğ‘ /32Ã—8â„\ns/32Ã—ğ‘ /32Ã—8â„\navg_pool\nlinear\n# class\ns/8Ã—ğ‘ /8Ã—2â„\ns/16Ã—ğ‘ /16Ã—4â„\ndecompose\nğ‘ğ‘™ğ‘œğ‘ğ‘˜\nğ‘ğ‘™ğ‘œğ‘ğ‘˜\nğ‘ğ‘™ğ‘œğ‘ğ‘˜\nğ‘ğ‘™ğ‘œğ‘ğ‘˜\ndown   sample\ndown   sample\nÃ—2\nÃ—8\nÃ—2\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 0\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 1\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 2\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 3\nğ‘Ã—ğ‘Ã—â„\nğ‘Ã—ğ‘Ã—â„\nğ‘ ğ‘ğ‘ğ‘›\nğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’\ns/4Ã—ğ‘ /4Ã—â„\ns/4Ã—ğ‘ /4Ã—â„\nğ‘\nFigure 3: The architecture of VMamba for image classification [10].\nIn the vision domain, vision-Mamba (Vim) [17] and VMamba\n[10] are two seminal works that extend Mamba from NLP to vision\ntasks. Fig. 3 illustrates the architecture of VMamba for image clas-\nsification. Given an RGB input image of size sÃ—s, the model first\ndecomposes it into pÃ—p smaller patches. Each patch is considered\nas a token, and all patches are arranged into a sequence of length\npÃ—p. This sequence is then treated similarly to textual data and fed\ninto Mamba for learning. Specifically, the VMamba block in Fig. 3a\nemploys four different scan orders (Fig. 2) to process the patches:\n(1) leftâ†’right then topâ†’down, (2) topâ†’down then leftâ†’right, (3)\nrightâ†’left then bottomâ†’up, and (4) bottomâ†’up then rightâ†’left.\nEach scan order introduces a unique dependency between patches,\nand consequently, the learned attention between patches also varies.\nThe merge operation aggregates the attention learned for each patch\nacross the four different scan orders. The output of this process\nis a p2Ã—p2 attention matrix, where each element at position (i, j)\nrepresents the attention strength between patch i and patch j. Stack-\ning multiple scan-merge blocks forms a stage, and Fig. 3 shows\nfour such stages, each containing 2, 2, 8, and 2 blocks, respectively.\nBetween stages, the latent representations are downsampled to distill\nessential features. Finally, a fully-connected layer transforms the\nlatent representation into a vector, the length of which corresponds\nto the number of possible classes. Each element in this vector deter-\nmines the probability for the corresponding class. Vim [17] follows\na similar process, but uses only a two-way scan (routes 1 and 3 in\nFig. 2). Therefore, we focus on VMamba in this work.\n2.2\nAttention Analysis and Visualization\nInterpreting and diagnosing machine learning models is an important\ntopic in the visualization literature [6,9,15], and there are multiple\nworks focusing on interpreting the attention mechanisms of these\nmodels. For example, Abnar and Zuidema [1], Park et al. [11], and\nLi et al. [7] all used heatmaps to externalize the attention strengths in\ntransformer-based NLP models. Vig [14] employed multiple parallel\ncoordinate plots to visualize the attention patterns in BERT and GPT-\n2 models. DeRose et al. [2] introduced a radial layout for visualizing\nattention in BERT, layer by layer, which also facilitates comparisons\nbetween the attentions of two models. For vision transformers, Li et\nal. [8] used dimensionality reduction and scatterplots to summarize\nattention patterns within self-attention heads and across attention\nlayers. Yeh et al. [16] employed a similar approach to visualize\nthe joint q/k embedding space, providing a global view of attention\npatterns across a transformer model.\nTo the best of our knowledge, no studies have yet focused on\nanalyzing the attention patterns in Mamba models. In particular, we\nare interested in whether regular attention patterns emerge within a\nMamba block and whether hierarchical attention patterns develop\nacross different stages of a Mamba model. To explore these ques-\ntions, we have developed a visual analytics tool specifically designed\nto analyze attention patterns in vision-based Mamba models.\n3\nPROBLEM AND METHODOLOGY\nIn this study, we aim to investigate the attention patterns in Mamba,\nfocusing on two key aspects based on insights from existing literature\nand the interests of domain experts working with Mamba:\nâ€¢ Inter-Block Attention Pattern: Are the attention patterns con-\nsistent across different Mamba blocks within the same stage?\nâ€¢ Intra-Block Attention Pattern: How is the attention spatially\ndistributed across image patches within a single Mamba block?\nWe provide solutions to answer these two questions in Sec. 3.1\nand Sec. 3.2, respectively. For our study, we focus on the VMamba\nmodel [10], whose architecture is illustrated in Fig. 3. The model is\ntrained for 400 epochs on the ImageNet dataset (1,281,167 training\nimages). After training, we use 1000 test images to investigate the\nattention patterns. The input images are of size 224Ã—224Ã—3 (i.e.,\ns=224 in Fig. 3) and the number of patches at the four stages is\n56Ã—56, 28Ã—28, 14Ã—14, and 7Ã—7, respectively.\n3.1\nInter-Block Attention Pattern\nThe attention matrices for different Mamba blocks at the same stage\nhave the same size. For example, in Fig. 3, the attention matrices\nfrom the two blocks at stage 0 are of the same size, which is 562Ã—562\nwhen s=224. This allows us to compare the matrices and assess\nhow the attention patterns differ across blocks at the same stage.\nHowever, attention matrices from blocks across different stages have\nvarying sizes, and thus, these blocks are not directly comparable.\nWhen an image is fed through a Mamba block, it produces an\nattention matrix of size p2Ã—p2, where the element at position (i, j)\nrepresents the attention strength from patch i to patch j. For example,\n\n\nAlgorithm 1 Generating cluster pattern across blocks of a stage.\nRequire: n,m\nâ–·number of images and blocks\nRequire: stage\nâ–·the focused stage\nRequire: image\nâ–·the array of images\nRequire: VMamba,DR â–·the VMamba model and DR algorithm\n1: attentions = []\n2: for i â†1 to n do\n3:\nfor j â†1 to m do\n4:\nattention = VMamba(image[i],stage, j)\n5:\nattentions.append(attention)\n6:\nend for\n7: end for\n8: points = DR(attentions)\nâ–·points âˆˆRmÃ—nÃ—2\nat stage 0 in Fig. 3, the attention matrix will be of shape 562Ã—562. If\nthe focused stage contains m blocks, we will obtain m such p2Ã—p2\nmatrices. By feeding all n=1000 test images through the VMamba\nmodel and collecting their attention matrices from the m blocks at\nthe focused stage, we compile the attention data into a large matrix\nof shape (mÃ—n)Ã—(p2Ã—p2). This process is outlined in lines 1 to 7\nof the pseudo-code in Algorithm 1.\nWe then apply dimensionality reduction (DR) techniques to map\nthe mÃ—n points, originally in the p2Ã—p2 dimensional space, to a 2D\nspace for visualization as a scatterplot. If the n points from different\nblocks form isolated clusters, this indicates that the attention patterns\nbetween the blocks are significantly different. The details of the\nvisualization and interactions are described later in Sec. 4.1.\n3.2\nIntra-Block Attention Pattern\nAccording to the formulation in Sec. 3.1, the attention matrix for\na single block and a single image has the shape p2Ã—p2 (line 4 of\nAlgorithm 1). The i-th row of this matrix represents the attention\nstrength from patch i to all p2 patches. To identify patches with\nsimilar attention patterns, we can apply dimensionality reduction\n(DR) techniques to reduce the p2Ã—p2 matrix to a p2Ã—2 matrix,\nwhich can then be analyzed through a scatterplot visualization to\nassess the similarity between patches.\nHowever, the approach described above can be significantly influ-\nenced by the image content, making it difficult to extract common,\ncontent-agnostic patterns. To enhance the identification of common\nattention patterns, we generate the p2Ã—p2 attention matrix for all n\nimages and aggregate the resulting n matrices into a single p2Ã—p2\nmatrix for pattern augmentation. The details of this process are\noutlined in Algorithm 2 (lines 1 to 6).\nWe employ a scatterplot to visualize the p2 2D points. The color\nand size of each point correspond to the column and row of the\nrespective patch. By examining the clustering patterns of these\npoints, we can identify patches with similar attention patterns and\nrelate these similarities to their spatial position in the image space\n(detailed in Sec. 4.1).\nAlgorithm 2 Generating cluster pattern within a block.\nRequire: n\nâ–·number of images\nRequire: stage,block\nâ–·the focused stage and block\nRequire: image\nâ–·the array of images\nRequire: VMamba,DR â–·the VMamba model and DR algorithm\n1: attentions = []\n2: for i â†1 to n do\n3:\nattention = VMamba(image[i],stage,block)\n4:\nattentions.append(attention)\n5: end for\n6: avg attn = np.mean(np.array(attentions),axis = 0)\n7: points = DR(avg attn)\nâ–·points âˆˆRp2Ã—2\n4\nVISUAL ANALYTICS SYSTEM\nWe have developed a visual analytics tool to effectively visualize\nthe outputs of Algorithm 1 and Algorithm 2. The tool features two\ndistinct visualization views: the Scatterplot view (Fig. 1, left) and\nthe Patch view (Fig. 1, right).\n4.1\nThe Scatterplot View\nThe Scatterplot view provides two visualization modes for exploring\nthe inter-block and intra-block attention patterns.\nMode 1:\nUsers can select a stage and a block from the header\nof the Scatterplot view (Fig. 1, left). If the block ID is not specified,\nthe Scatterplot will display mÃ—n points based on the output of\nAlgorithm 1, where m represents the number of blocks in the selected\nstage, and n=1000 is the number of images. Each point corresponds\nto a p2Ã—p2 attention matrix for a given image from a specific block.\nFor example, in the VMamba architecture depicted in Fig. 3, there\nare 4 stages, with 2, 2, 8, and 2 blocks per stage, respectively. The\ndimensionality reduction results for the four stages are shown in\nFig. 4, where each figure represents the output of a single stage.\nThe color in each figure corresponds to the block ID, and distinct\nclusters of points are clearly visible, with different colors separating\nthe blocks. This indicates that blocks within the same stage exhibit\nnoticeably different attention patterns.\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 0\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 1\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 2\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 3\nğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0\nğ‘ğ‘™ğ‘œğ‘ğ‘˜ 1\nğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0\nğ‘ğ‘™ğ‘œğ‘ğ‘˜ 1\nğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0\nğ‘ğ‘™ğ‘œğ‘ğ‘˜ 1\n0\n1\n2\n3\n4\n5\n6\n7\nFigure 4: Attention pattern similarity between blocks from the four\nstages of the VMamba model in Fig. 3.\nMode 2:\nWhen a block ID is specified, the Scatterplot will show\np2 points based on the output of Algorithm 2. In this case, each point\nrepresents the averaged attention across n=1000 test images for a\nspecific patch position. The color and size of each point correspond\nto the column and row of the corresponding patch, respectively. As\nillustrated by the clustering pattern in Fig. 1, patches from the same\nrow (points in Fig. 1a with the same size) or the same column (points\nin Fig. 1b with the same color) exhibit similar attention patterns.\nFigs. 1c and 1d reveal further clusters that share similar attention\npatterns to those in Figs. 1a and 1b, respectively.\nFor both modes, we provide three popular dimensionality reduc-\ntion techniques: PCA, tSNE, and UMAP. Each technique has its\nunique strengths and limitations, so offering multiple options allows\nusers to explore different perspectives and uncover subtle clustering\npatterns. When processing very high-dimensional data using tSNE\nand UMAP, we first use PCA to project them to a relatively lower\ndimension (i.e., 100D), then apply tSNE and UMAP for efficiency.\n\n\nThe Scatterplot view also supports zooming, enabling users to ex-\namine cluster details at various levels of granularity. In Mode 2,\nusers can perform lasso selection to highlight a group of points. The\ncorresponding patches for the selected points will then be visualized\nin the Patch view (described in the following section).\n4.2\nThe Patch View\nThe Patch view also offers two visualization modes, allowing users\nto explore patch-level details from the selected stage.\nMode 1:\nIn this mode, the patches from the selected stage are\ndisplayed as gray squares. For example, in Fig. 1 (right), the 28Ã—28\npatches from stage 1 are visualized as 28Ã—28 squares. Meanwhile,\nthe selected patches from the Scatterplot view (Fig. 1, left) are\nhighlighted as red squares in the Patch view, visually indicating\ntheir spatial location within the image. This highlighting directly\ncorresponds to the cluster patterns observed in the Scatterplot view\n(Fig. 1, left), which helps reveal the spatial relationships between\npatches that exhibit similar attention patterns. This coordinated\nvisualization provides a clear understanding of how attention patterns\nare distributed across different regions of the image.\nğ‘\nğ‘\nğ‘\nrow: 5, col:4\nrow: 6, col:4\nrow: 7, col:4\nsmall\nlarge\nFigure 5: Patches in the same column exhibit similar attention.\nMode 2:\nWhen a square/patch is clicked in the Patch view, all\nsquares/patches will be colored according to their attention strength\nto the clicked one. Attention strengths from small to large are\nmapped to colors from light-yellow to dark-red. For example, in\nFig. 5a, the clicked patch is located at row 5, column 4. This patch\nshows strong attention to: (1) itself, (2) the patches in the same\nrow to its left, and (3) the patches to its right. From the Scatterplot\nview, we noticed that patches in the same column as the selected\npatch have similar attention patterns. Based on this, we select two\nadditional patches for further exploration. As shown in Figs. 5b\nand 5c, these two patches, which are in the same column as the one\nin Fig. 5a, exhibit very similar attention patterns.\n5\nFINDINGS AND PATTERN SUMMARY\nUsing the developed visual analytics tool, we conducted a detailed\nexploration of the four stages and their respective blocks in the\nVMamba model (Fig. 3). This section summarizes the key findings\nfrom our analysis.\nFinding 1:\nBlocks within the same stage exhibit significantly\ndifferent attention patterns. This finding is clearly illustrated in\nFig. 4. To examine the differences further, we focus on the two\nblocks at stage 0 for a detailed exploration. As shown in Figs. 6-a1\n(stage 0, block 0) and b1 (stage 0, block 1), the color and size of\nthe points (representing individual patches) change progressively\nalong both the row and column directions. This indicates a gradual\ntransition of the attention pattern for patches along rows and columns.\nWhen individual patches are clicked in the Patch view, their attention\npatterns are displayed on the right. It is evident that the same patch\nfrom the two blocks exhibits very different attention patterns. In\nblock 0 (Fig. 6-a2), the selected patch strongly attends to (1) itself,\n(2) patches in the first row with a larger column ID, (3) patches in\nthe first column with a larger row ID, and (4) patches that have both\nlarger row and column IDs than the selected patch. In contrast, the\npatch in block 1 (Fig. 6-b2) pays noticeably weaker attention to itself\nand patches along similar rows and columns, but stronger attention\nto patches in other areas of the image. We checked multiple patches\nfrom these two blocks, and the observation is consistent across them.\nFinding 2:\nSome blocks exhibit complementary attention pat-\nterns. For instance, the selected patch at stage 2, block 1 (Fig. 6-e2)\nshows stronger attention to itself as well as to patches in the same\nrow and column. In contrast, the same patch at stage 2, block 3\n(Fig. 6-f2) and block 5 (Fig. 6-g2) exhibits weaker attention to the\npatch itself and to patches along the same row and column. This\ncomplementary attention behavior across blocks enables VMamba\nto selectively focus on relevant patches, contributing to its flexibility\nin attending to different regions of the input images.\nFinding 3:\nThere is a hierarchy regarding the attention pattern\nlearned from early to later stages. At early stages, patches that are\nspatially closer tend to exhibit similar attention patterns, while at\nlater stages, the attention is more influenced by the image content.\nAt stage 0 (Fig. 6, a1-b1), attention patterns show smooth and pro-\ngressive changes for patches in the same row and column, indicating\na strong spatial correlation. At stage 1 (Fig. 6, c1-d1), clear clusters\nform for patches in the same row and columnâ€”represented by points\nof the same color but varying sizes or the same size but different\ncolors. This pattern persists at stage 2 (Fig. 6, e1-h1), though clus-\ntering is less pronounced for some blocks at this stage. By stage\n3, the clustering structure becomes less obvious in the Scatterplot\nview (Fig. 6, i1-j1), suggesting that the attention patterns have be-\ncome more diverse and content-dependent. This phenomenon aligns\nwith the behavior observed in CNNs and vision transformers, where\nlower layers tend to focus on local, content-agnostic features, while\nhigher layers capture more complex, content-relevant patterns.\n6\nIMPACT OF PATCH ORDER ON ATTENTION PATTERNS\nOne of our key findings is that patches within the same row or\ncolumn often exhibit similar attention patterns. We hypothesize that\nthis is influenced by the order in which patches are arranged into\nsequences. To test this hypothesis, we introduce three alternative\npatch orders, shown in Fig. 7, and examine how the attention patterns\nchange when these new orders are applied.\nFig. 7a shows a patch order that scans patches along the diagonal.\nFig. 7b employs the Morton order (z-order curve), a well-known\nspace-filling curve that is particularly effective at preserving spatial\nlocality. Fig. 7c arranges patches in a spiral layout, where the\ninnermost patch retains the highest spatial locality. We modified the\nVMamba code to implement each of these patch orders and trained\nthe model from scratch for 400 epochs. All three scanning methods\nachieved accuracy levels similar to the original VMamba model, i.e.,\nachieving accuracy greater than or equal to 82.6% on ImageNet as\nreported in the original VMamba paper [10].\nFigs. 8-a1, a2, and a3 show the results of exploring attention\npatterns using the diagonal order. In Fig. 8-a1, all patches are\nprojected onto a continuous curve. By selecting points in a local\nregion, we observe that the corresponding patches are adjacent along\nthe diagonal, as shown in Fig. 8-a2. When clicking on any patch\nin the Patch view, we see that its attention behavior mirrors what\nwas observed in Fig. 6 but along the diagonals instead of the rows\nor columns. For example, the attention pattern of a patch at stage 1,\nblock 0 is displayed in Fig. 8-a3. This patch strongly attends to its\npreceding patches along the diagonal, which is very similar to what\nwas observed in Fig. 6-c2. However, in Fig. 6-c2, the preceding\npatches are those based on the cross-scan order in Fig. 2.\nNext, we conduct similar explorations with the VMamba model\ntrained using the Morton order. As shown in Fig. 8-b1, patches at\nstage 1, block 0 are grouped into clusters. Selecting a cluster high-\nlights the patches within a local region in Fig. 8-b2. These patches\nare actually in the same z-order curve block. The strong spatial lo-\ncality among them results in similar attention patterns. Clicking on a\n\n\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 0, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 1, ğ‘ˆğ‘€ğ´ğ‘ƒ\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 2 ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 1, ğ‘ƒğ¶ğ´\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 2 ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 5, ğ‘¡ğ‘†ğ‘ğ¸\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 2 ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 7, ğ‘¡ğ‘†ğ‘ğ¸\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 3 ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0, ğ‘ƒğ¶ğ´\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 3 ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 1, ğ‘ƒğ¶ğ´\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 2 ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 3, ğ‘ƒğ¶ğ´\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 0, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0, ğ‘ˆğ‘€ğ´ğ‘ƒ\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 1, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 1, ğ‘¡ğ‘†ğ‘ğ¸\nğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 1, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0, ğ‘¡ğ‘†ğ‘ğ¸\n# ğ‘ğ‘ğ‘¡ğ‘â„: 56 Ã— 56\n# ğ‘ğ‘ğ‘¡ğ‘â„: 56 Ã— 56\n# ğ‘ğ‘ğ‘¡ğ‘â„: 28 Ã— 28\n# ğ‘ğ‘ğ‘¡ğ‘â„: 28 Ã— 28\n# ğ‘ğ‘ğ‘¡ğ‘â„: 14 Ã— 14\n# ğ‘ğ‘ğ‘¡ğ‘â„: 14 Ã— 14\n# ğ‘ğ‘ğ‘¡ğ‘â„: 14 Ã— 14\n# ğ‘ğ‘ğ‘¡ğ‘â„: 14 Ã— 14\n# ğ‘ğ‘ğ‘¡ğ‘â„: 7 Ã— 7\n# ğ‘ğ‘ğ‘¡ğ‘â„: 7 Ã— 7\nğ‘œğ‘›ğ‘’ ğ‘Ÿğ‘œğ‘¤\nğ‘1\nğ‘2\nğ‘1\nğ‘2\nğ‘1\nğ‘2\nğ‘‘1\nğ‘‘2\nğ‘’1\nğ‘’2\nğ‘“1\nğ‘“2\nğ‘”1\nğ‘”2\nâ„1\nâ„2\nğ‘–1\nğ‘–2\nğ‘—1\nğ‘—2\nFigure 6: The attention pattern similarity between patches from blocks of different stages. (a1-j1) Two, two, four, and two blocks from stages 0, 1,\n2, and 3 are shown, respectively. (a2-j2) Selecting a patch to inspect its attention pattern.\n\n\nğ·ğ‘–ğ‘ğ‘”ğ‘›ğ‘ğ‘™\nğ‘€ğ‘œğ‘Ÿğ‘¡ğ‘œğ‘›\nğ‘\nğ‘\nğ‘†ğ‘ğ‘–ğ‘Ÿğ‘ğ‘™\nğ‘\nğ‘Ÿğ‘œğ‘¢ğ‘¡ğ‘’ 1\nğ‘Ÿğ‘œğ‘¢ğ‘¡ğ‘’ 2\nğ‘Ÿğ‘œğ‘¢ğ‘¡ğ‘’ 3\nğ‘Ÿğ‘œğ‘¢ğ‘¡ğ‘’ 4\nFigure 7: Arranging patches in VMamba following different orders.\npatch reveals that it also strongly attends to its preceding patches, as\nshown in Fig. 8-b3. The results for the spiral patch order, shown in\nFigs. 8-c1, c2, and c3, are consistent with those observed from other\npatch orders. These findings confirm that stage 1, block 0 of the\nVMamba model exhibits a fixed attention pattern: any patch in this\nblock consistently attends strongly to its preceding patches. How-\never, the definition of â€œpreceding patchesâ€ depends on the specific\npatch order used.\nğ·ğ‘–ğ‘ğ‘”ğ‘œğ‘›ğ‘ğ‘™:  ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 1, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0, ğ‘¡ğ‘†ğ‘ğ¸\nğ‘1\nğ‘2\nğ‘3\nğ‘1\nğ‘2\nğ‘3\nğ‘€ğ‘œğ‘Ÿğ‘¡ğ‘œğ‘›:  ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 1, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0, ğ‘¡ğ‘†ğ‘ğ¸\nğ‘†ğ‘ğ‘–ğ‘Ÿğ‘ğ‘™:  ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ 1, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 0, ğ‘¡ğ‘†ğ‘ğ¸\nğ‘1\nğ‘2\nğ‘3\nFigure 8: Exploring attention patterns when patches are arranged\nfollowing the three new orders in Fig. 7: Diagonal, Morton, and Spiral.\n7\nCONCLUSION AND FUTURE WORK\nIn this paper, we introduced a visual analytics tool to explore\nand compare attention patterns within and across VMamba blocks.\nThrough our exploration, we discovered several key insights: (1)\nVMamba blocks within the same stage exhibit distinct attention\npatterns; (2) the order of patches significantly influences the result-\ning attention patterns; and (3) patches that are close in the input\nsequence generally exhibit similar attention patterns. Given the\nsignificant impact of patch arrangement on attention patterns, we\nproposed multiple new patch orders that better preserve the patchesâ€™\nspatial locality. Using our tool, we further investigated VMamba\nmodels trained with these new orders, and found similar attention\nbehaviors to those observed in the original patch order.\nLooking ahead, we aim to extend our tool in several directions.\nFirst, we plan to explore more complex VMamba models with addi-\ntional stages and blocks per stage. We hypothesize that the general\ntrends observed in this study will hold as the architecture becomes\nmore intricate. Second, our current work focuses on identifying\ncontent-agnostic attention patterns by averaging attention over n\nimages. In the future, we intend to integrate additional views into\nthe tool to present content-relevant attention patterns, which will be\nparticularly useful for diagnosing specific images of interest.\nREFERENCES\n[1] S. Abnar and W. Zuidema. Quantifying attention flow in transformers.\nIn Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 4190â€“4197, July 2020.\n[2] J. F. DeRose, J. Wang, and M. Berger. Attention flows: Analyzing and\ncomparing attention mechanisms in language models. IEEE Trans. Vis.\nComput. Graphics, 27(2):1160â€“1170, 2020.\n[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In 9th International Conference\non Learning Representations (ICLR), 2021.\n[4] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with\nselective state spaces. arXiv preprint arXiv:2312.00752, 2023.\n[5] A. Gu, K. Goel, and C. RÂ´e. Efficiently modeling long sequences\nwith structured state spaces. In International Conference on Learning\nRepresentations, 2022.\n[6] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau. Visual analytics\nin deep learning: An interrogative survey for the next frontiers. IEEE\nTransactions on Visualization and Computer Graphics, 25(8):2674â€“\n2693, 2018.\n[7] R. Li, W. Xiao, L. Wang, H. Jang, and G. Carenini. T3-vis: visual ana-\nlytic for training and fine-tuning transformers in NLP. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pp. 220â€“230, 2021.\n[8] Y. Li, J. Wang, X. Dai, L. Wang, C.-C. M. Yeh, Y. Zheng, W. Zhang,\nand K.-L. Ma. How does attention work in vision transformers? a visual\nanalytics attempt. IEEE Transactions on Visualization and Computer\nGraphics, 29(6):2888â€“2900, 2023.\n[9] S. Liu, W. Yang, J. Wang, and J. Yuan. Visualization for Artificial\nIntelligence. Springer Cham, 2025. doi: 10.1007/978-3-031-75340-4\n[10] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, J. Jiao, and\nY. Liu. VMamba: Visual state space model. In The Thirty-eighth\nAnnual Conference on Neural Information Processing Systems, 2024.\n[11] C. Park, I. Na, Y. Jo, S. Shin, J. Yoo, B. C. Kwon, J. Zhao, H. Noh,\nY. Lee, and J. Choo. Sanvis: Visual analytics for understanding self-\nattention networks. In 2019 IEEE Visualization Conference (VIS), pp.\n146â€“150. IEEE, 2019.\n[12] J. T. Smith, A. Warrington, and S. W. Linderman. Simplified state\nspace layers for sequence modeling. arXiv preprint arXiv:2208.04933,\n2022.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin. Attention is all you need. Ad-\nvances in Neural Information Processing Systems, 2017.\n[14] J. Vig. A multiscale visualization of attention in the transformer model.\narXiv preprint arXiv:1906.05714, 2019.\n[15] J. Wang, S. Liu, and W. Zhang. Visual analytics for machine learning:\nA data perspective survey. IEEE Transactions on Visualization and\nComputer Graphics, 30(12):7637â€“7656, 2024.\n[16] C. Yeh, Y. Chen, A. Wu, C. Chen, F. ViÂ´egas, and M. Wattenberg. At-\ntentionviz: A global view of transformer attention. IEEE Transactions\non Visualization and Computer Graphics, 30(01):262â€“272, 2024.\n[17] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision\nmamba: Efficient visual representation learning with bidirectional state\nspace model. arXiv preprint arXiv:2401.09417, 2024.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20764v1.pdf",
    "total_pages": 6,
    "title": "Visual Attention Exploration in Vision-Based Mamba Models",
    "authors": [
      "Junpeng Wang",
      "Chin-Chia Michael Yeh",
      "Uday Singh Saini",
      "Mahashweta Das"
    ],
    "abstract": "State space models (SSMs) have emerged as an efficient alternative to\ntransformer-based models, offering linear complexity that scales better than\ntransformers. One of the latest advances in SSMs, Mamba, introduces a selective\nscan mechanism that assigns trainable weights to input tokens, effectively\nmimicking the attention mechanism. Mamba has also been successfully extended to\nthe vision domain by decomposing 2D images into smaller patches and arranging\nthem as 1D sequences. However, it remains unclear how these patches interact\nwith (or attend to) each other in relation to their original 2D spatial\nlocation. Additionally, the order used to arrange the patches into a sequence\nalso significantly impacts their attention distribution. To better understand\nthe attention between patches and explore the attention patterns, we introduce\na visual analytics tool specifically designed for vision-based Mamba models.\nThis tool enables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout a Mamba model.\nUsing the tool, we also investigate the impact of different patch-ordering\nstrategies on the learned attention, offering further insights into the model's\nbehavior.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}