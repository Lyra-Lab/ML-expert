{
  "id": "arxiv_2502.21262v1",
  "text": "arXiv:2502.21262v1  [cs.AI]  28 Feb 2025\nModeling Human Beliefs about AI Behavior\nfor Scalable Oversight\nLeon Lang\nl.lang@uva.nl\nUniversity of Amsterdam\nPatrick Forré\np.d.forre@uva.nl\nUniversity of Amsterdam\nAbstract\nContemporary work in AI alignment often relies on human feedback to teach AI systems\nhuman preferences and values.\nYet as AI systems grow more capable, human feedback\nbecomes increasingly unreliable. This raises the problem of scalable oversight: How can\nwe supervise AI systems that exceed human capabilities?\nIn this work, we propose to\nmodel the human evaluator’s beliefs about the AI system’s behavior to better interpret the\nhuman’s feedback. We formalize human belief models and theoretically analyze their role in\ninferring human values. We then characterize the remaining ambiguity in this inference and\nconditions for which the ambiguity disappears. To mitigate reliance on exact belief models,\nwe then introduce the relaxation of human belief model covering. Finally, we propose using\nfoundation models to construct covering belief models, providing a new potential approach\nto scalable oversight.\nContents\n1\nIntroduction\n2\n2\nHuman belief models\n3\n2.1\nConventions and preliminaries for linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nPreliminaries on Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nThe human’s ontology and reward object\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.4\nThe human’s feature belief and observation return function\n. . . . . . . . . . . . . . . . . . .\n6\n2.5\nThe deﬁnition of human belief models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.6\nComplete belief models and the ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.7\nFaithful belief models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.8\nConceptual examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3\nHuman belief model covering\n15\n3.1\nHuman belief model covering and its implications . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.2\nMorphisms of human belief models and ontology translations . . . . . . . . . . . . . . . . . .\n17\n3.3\nAn example of symmetry-invariant features and reward functions . . . . . . . . . . . . . . . .\n19\n3.4\nA proposal for belief model covering in practice . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n1\n\n\n4\nDiscussion\n26\n4.1\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.2\nRelated work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.3\nFuture work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.4\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nReferences\n30\nA Preliminary results on linear algebra\n38\nB Balanced human belief models and choices\n41\nC A diagram in the category of human belief models\n43\nD Details on the example with invariant features\n48\nE Mathematical interpretations of related work in our framework\n51\n1\nIntroduction\nIn recent years, reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) and variations\nlike direct preference optimization (Rafailov et al., 2023) have become a practical approach for aligning lan-\nguage models (Ziegler et al., 2020; Stiennon et al., 2022; Bai et al., 2022; Ouyang et al., 2022). These tech-\nniques have then been used in the alignment of large-scale foundation models like ChatGPT (OpenAI,\n2022) and GPT-4 (OpenAI et al., 2024), Gemini (Gemini Team, 2023), Llama (Touvron et al., 2023;\nGrattaﬁori et al., 2024), and Claude (Bai et al., 2022; Anthropic, 2023a;b).\nRLHF uses feedback of human evaluators on AI behavior to give information about desired behavior. But as\nAI systems become more capable, they may eventually outstrip our ability to evaluate them. The problem of\nscalable oversight therefore asks how to eﬀectively teach our preferences to AI systems when they become\nmore capable than ourselves (Amodei et al., 2016; Christiano et al., 2018; Irving et al., 2018; Leike et al.,\n2018; Bowman et al., 2022). Conceptually, if human evaluators lack the capacity to fully understand the\nAI’s behavior, then this may incentivize the AI to perform behavior that the human evaluator believes to\nbe good, possibly at the expense of actual value.\nRecent work has in fact revealed problems with RLHF that stem from erroneous human beliefs about AI\nbehavior. In an early example (Amodei et al., 2017), an AI was supposed to grasp a ball with a robot hand\nin a simulation. The evaluators, who looked at a video of the behavior, sometimes believed that the hand was\ngrasping the ball when it was in fact only in front of it, leading to positive feedback for the wrong behavior.\nIn Cloud et al. (2024), a synthetic overseer in a baseline setting is unaware of whether a reached goal is a\ndiamond (positive) or ghost (negative), leading the policy to learn to approach the ghost. Denison et al.\n(2024) shows a language model modifying a ﬁle unbeknownst to a synthetic evaluator, successfully deceiving\nit into believing a checklist has been accomplished. Similar models have also been shown to mislead humans\nwho are limited in their time or competence to evaluate question answering or programming tasks, leading\nthem to believe that the performance is better than it is (Wen et al., 2024).\nGiven the crucial role of evaluator beliefs in erroneous feedback, in this theoretical work, we are exploring\nthe idea to model human beliefs about AI behavior. Our idea is that if we knew what the human believes\nthe AI has done in its environment, then we could properly assign the human’s feedback to that believed\nbehavior instead of to the actual behavior. For example, imagine the robot hand from Amodei et al. (2017)\nis in front of the ball and the human evaluator gives positive feedback. Additionally, assume we know the\n2\n\n\nhuman believes the ball was actually grasped. If so, then we know that the human thinks grasping the ball\nis positive, and can incentivize this behavior from now on.\nBut what are beliefs? Prior work (Lang et al., 2024) takes the view that a belief is a probability distribution\nover state sequences in the AI’s environment. They show in theoretical toy examples that modeling such\nbeliefs can help to learn eﬀectively from feedback. However, in reality it is unrealistic that humans form\nprobability distributions over entire state sequences, and it would be prohibitive to specify such a belief\nexplicitly. We think it is more realistic that humans think in terms of features, which we conceptualize as\nhigher-level and independent properties of trajectories in an environment. We then model a human belief\nabout an AI’s behavior as a vector of feature strengths. This leads to our notion of a human belief model,\nwhich we introduce in Section 2. This model, together with feedback on observations, then theoretically\nallows to infer the return function over trajectories up to an inherent ambiguity, which we characterize\nin terms of the human belief model. The ambiguity disappears when the model is complete, which holds,\nin particular, when the human’s beliefs of all observations linearly cover the feature combinations that are\npossible in the environment (Theorem 2.11). We then analyze conceptual toy examples of non-complete and\ncomplete models and consequences for the resulting return function inference.\nWe would like to use human belief modeling in practice to make concrete progress on scalable oversight.\nHowever, realistically, we cannot model human’ beliefs precisely; after all, we do not even have an explicit\nspeciﬁcation of the human’s feature set! In Section 3, we thus relax the requirement of exact modeling\nby investigating belief models that can represent all return functions and feedback functions that are\ncompatible with the true belief model. We then say that these models cover the true belief model. When\nsuch a model is complete, it can replace the true model for the return function inference (Theorem 3.2). In\na conceptual example, we analyze a human with symmetry-invariant features, which we can cover with a\ncomplete model that assumes symmetry-invariant reward functions.\nWe also characterize model coverage by the existence of what we call model morphisms and linear\nbelief-compatible ontology translations (Theorem 3.5). Intuitively, this means that if one can linearly\ntranslate from the speciﬁed model’s features to the human’s features in a way that is compatible with their\nbeliefs about observations, then coverage holds. The notion of a linear ontology translation is strikingly\nsimilar to work in the ﬁeld of mechanistic interpretability on sparse autoencoders (Cunningham et al., 2023;\nBricken et al., 2023), which linearly map from the internal representation space of language models to a\nspace of human-interpretable features. Inspired by this connection, in Section 3.4, we make a preliminary\nproposal to use adapted foundation models to construct a belief model that covers the true human belief\nmodel, which we hope motivates future work that makes empirical progress on scalable oversight.\nFinally, in our discussion in Section 4, we summarize our work, survey related work, motivate theoretical\nand empirical future work, and conclude.\n2\nHuman belief models\nIn this section we deﬁne our generalized notion of human belief models that can help to infer return functions\nfrom the human’s feedback.\nIn Section 2.1 we start by explaining our conventions and some preliminaries for linear algebra. We recom-\nmend also experienced readers to brieﬂy skim this section to understand our notation. In Section 2.2, we\nbrieﬂy deﬁne Markov decision processes and slightly adapt them: We do not make the assumption that the\nreturn function over trajectories comes from a reward function. This allows us to be slightly more general.\nIn Section 2.3, we then introduce the notion of the human’s ontology, which is a map that assigns a vector of\nfeature strengths to each trajectory. Our crucial assumption is that the return function evaluates a trajectory\nby summing up the rewards of each feature, weighted by the feature strengths. This allows us to recover\nclassical reward functions as a special case.\nIn reinforcement learning, the goal is to maximize the return function, but we assume to be this function\nunknown: It represents the implicit values of a human evaluator and needs to be inferred from the human’s\nfeedback. In Section 2.4, we explain the human to give feedback based on forming a belief over features based\non observations. We call the resulting feedback the observation return function. The fundamental question\n3\n\n\nis how to use it to infer the true return function. To do this, we assume that the whole human belief model,\nwhich we introduce in Section 2.5, is known; in other words, all aspects of the human’s feedback process\nare known except the return parameters. Under this assumption, one can infer the return function from the\nhuman’s feedback up to an ambiguity, which we characterize in Section 2.6. We also deﬁne complete human\nbelief models for which the ambiguity vanishes and characterize them in Theorem 2.11. In Section 2.7, we\nintroduce the dual notion of faithfulness. In Section 2.8, we study conceptual examples that give an intuition\nfor when the ambiguity does, or does not, vanish.\n2.1\nConventions and preliminaries for linear algebra\nLet X be a set. For x ∈X, we deﬁne the delta function δx : X →R as usual by\nδx(x′) =\n(\n1, x′ = x,\n0, else .\nLet R be the real numbers. Then RX denotes the vector space of functions from X to R, which one can also\nview as column-vectors indexed by x ∈X. For a function v ∈RX, we write v(x) for the entry of v at position\nx ∈X. The standard basis functions of RX are given by {ex}x∈X with ex = δx. Then every function v ∈RX\ncan be written as v = P\nx∈X v(x) · ex. We deﬁne the standard scalar product ⟨·, ·⟩: RX × RX →R by\n⟨v, w⟩:=\nX\nx∈X\nv(x)w(x).\nFor a family of functions vi ∈RX indexed by indices i ∈I, we denote the span by\nR\n\nvi | i ∈I\n\u000b :=\n(X\ni∈I\nλivi \f\f\f λi ∈R for all i ∈R\n)\n.\nLet A : RX →RY be a linear map.\nThen we view A also as a matrix with matrix elements Ayx :=\n\u0002\nA(ex)\n\u0003\n(y) ∈R for x ∈X, y ∈Y . We write Ay ∈RX for the row of A at index y ∈Y , which is the function\nwith entries Ay(x) = Ayx. Consequently, if v ∈RX and y ∈Y , then we obtain\n\u0002\nA(v)\n\u0003\n(y) =\nX\nx∈X\nv(x)\n\u0002\nA(ex)\n\u0003\n(y) =\nX\nx∈X\nAyxv(x) = ⟨Ay, v⟩.\nThis corresponds to the typical way that linear functions can be represented as matrix-vector products. If\nV ⊆RX is a vector subspace, then we write A|V : V →RY for the restriction of A to V, which is simply\ngiven by (A|V)(v) = A(v) for all v ∈V. We denote the kernel and image of A : RX →RY by\nker(A) :=\nn\nv ∈RX \f\f A(v) = 0\no\n⊆RX,\nim(A) :=\nn\nw ∈RY\n\f\f ∃v ∈RX : A(v) = w\no\n⊆RY .\nI.e., they are the set of functions in RX that are sent to zero by A, and the set of functions in RY that are\nhit by A, respectively. Both are vector subspaces of their respective surrounding vector spaces. We explain\nbasic properties of kernels and images in Appendix A.2 and will refer to those results when needed.\nSometimes, our linear functions “come from” a deterministic function in the other direction. I.e., if h : Y →X\nis a function, then we deﬁne the linear dual function h∗: RX →RY for v ∈RX and y ∈Y by\n\u0002\nh∗(v)\n\u0003\n(y) := v\n\u0000h(y)\n\u0001\n.\n(1)\nThe matrix elements are then given by\nh∗\nyx =\n\u0002\nh∗(ex)\n\u0003\n(y) = ex\n\u0000h(y)\n\u0001\n=\n(\n1, h(y) = x,\n0, else.\n(2)\n4\n\n\nFor two linear maps A : RX →RY and B : RY →RZ, we write their composition as B ◦A : RX →RZ,\nwhich has matrix elements (B ◦A)zx = P\ny∈Y BzyAyx for x ∈X, z ∈Z. This also implies\n(B ◦A)z =\nX\ny∈Y\nBzyAy.\n(3)\nFinally, whenever we draw a diagram of (usually linear) functions in this paper, the diagram commutes,\nmeaning that all directed pathways from one node to another node are the same function. For example, in\na diagram of the form\nRY\nRX\nRZ\nB\nC\nA\nwe have C = B ◦A. The only exceptions to this convention are Equations (20) and (26) in the appendix.\n2.2\nPreliminaries on Markov Decision Processes\nWe work in the following setting: We have an MDP (S, A, T , P0, T, G), with a ﬁnite set of states S and\nactions A, a transition kernel T : S × A →∆(S), an initial state distribution P0 ∈∆(S), a ﬁnite\ntime horizon T ∈{0, 1, 2, 3, . . .}, and the human’s implicit return function G, which we clarify after\ndeﬁning trajectories below. We ﬁx this generic MDP for the rest of the paper.\nWe now deﬁne trajectories. In the rest of the paper, whenever we have a state-action sequence ξ ∈(S ×\nA)T × S present in some context and then write about states and actions s0, a0, . . . , sT −1, aT −1, sT , then\nthey implicitly refer to the states and actions in ξ. Then the set of trajectories be given by\nΞ =\nn\nξ ∈(S × A)T × S\n\f\f ξ is possible\no\n⊆(S × A)T × S,\nwhere we call a state-action sequence ξ possible if P0(s0) > 0 and T (st | st−1, at−1) > 0 for all t ≥1. Then\nthe return function is a function G ∈RΞ.\nNote that this formalism does not assume that G decomposes into a reward function R : S × A × S →R.\nIn other words, the formalism allows for human preferences that do not satisfy the reward hypothe-\nsis (Bowling et al., 2022) and is thus slightly more general than typical reinforcement learning settings.\nPolicies are functions π : S →∆(A). A policy π together with the MDP induces a distribution P π ∈∆(Ξ)\nover trajectories by sampling initial states from P0, actions from π, and transitions from T . The policy\nevaluation function is then given by\nJ(π) :=\nE\nξ∼P π(·)\n\u0002\nG(ξ)\n\u0003\n.\n(4)\nThe goal in reinforcement learning is to ﬁnd a policy π that maximizes this evaluation function.\n2.3\nThe human’s ontology and reward object\nWe assume that the return function G : Ξ →R encodes what a given human cares about. We imagine that\nG measures the quality of each trajectory linearly in certain feature strengths associated to each trajectory;\nhere, features are high-level return-relevant concepts.\nMore precisely, we assume the human comes equipped with a ﬁnite feature set Ω. The human’s ontology\nis a function λ : Ξ →RΩthat encodes for each ξ ∈Ξ and ω ∈Ωthe extent\n\u0002\nλ(ξ)\n\u0003\n(ω) to which the feature\nω is present in the trajectory ξ. As we discuss in greater detail in Section 2.4, these feature strengths λ(ξ)\n5\n\n\nare idealized, i.e., the human may not be able to compute them. In the general theory, we allow them to\nbe negative, though it may help to imagine them to be non-negative. The human’s reward object is a\nﬁxed function RΩ∈RΩthat assigns to each feature ω ∈Ωthe degree RΩ(ω) to which the human likes this\nfeature. The return function G : Ξ →R evaluates a trajectory by summing up the quality of all features,\nweighted by the extent to which they appear in the trajectory:\nG(ξ) =\nX\nω∈Ω\n\u0002\nλ(ξ)\n\u0003\n(ω) · RΩ(ω) =\n\nλ(ξ), RΩ\n\u000b\n.\nLet Λ : RΩ→RΞ be given by\n\u0002\nΛ( ˜RΩ)\n\u0003\n(ξ) :=\n\nλ(ξ), ˜RΩ\n\u000b\n. Then Λ is a linear function from which we\ncan recover λ using the matrix elements of Λ:\n\u0002\nλ(ξ)\n\u0003\n(ω) = Λξω (Proposition A.1). We slightly abuse the\nterminology by referring to both λ : Ξ →RΩand Λ : RΩ→RΞ as the human’s ontology. This representation\nof the ontology satisﬁes Λ(RΩ) = G.\nExample 2.1 (Classical reward functions). Assume that the human’s feature set is Ω= S × A × S, i.e., the\nset of all state-action-state transitions. Then the reward object RΩ= R ∈RS×A×S is a reward function in\nthe classical sense. Let γ ∈[0, 1] be a discount factor and deﬁne the ontology Λ = Γ : RS×A×S →RΞ by\n\u0002\nΓ( ˜R)\n\u0003\n(ξ) =\nT −1\nX\nt=0\nγt ˜R(st, at, st+1).\nG = Γ(R) is then given by the discounted sum of rewards of individual transitions in a trajectory, as is\ntypical in reinforcement learning. The matrix elements of the ontology Γ are given by\nΓξ,(s,a,s′) =\n\u0002\nΓ(e(s,a,s′))\n\u0003\n(ξ) =\nT −1\nX\nt=0\nγte(s,a,s′)(st, at, at+1) =\nT −1\nX\nt=0\nγtδ(s,a,s′)(st, at, st+1),\nIn other words, the extent to which the “feature” (s, a, s′) is present in the trajectory ξ is simply the discounted\nnumber of times that it appears.\n2.4\nThe human’s feature belief and observation return function\nIn standard reinforcement learning, it is typically assumed that we know the return function G to be max-\nimized by a policy. However, G is the implicit return function of a given human. We must thus rely on\nfeedback by the human to infer information about G. A naive idea would be to show the human each tra-\njectory ξ, ask them to evaluate it by computing G(ξ), and to use these returns to train a policy. There are\nthree challenges to this plan:\n(i) The human might not have access to the full trajectory ξ or all trajectories ξ ∈Ξ. For example, in\ncomplex environments, they would usually only receive partial observations.\n(ii) Even if ξ were fully accessible, the human might not have the capacity to compute the features λ(ξ).\nFor example, if ξ encodes a proof-attempt of a mathematical conjecture and ω encodes “correctness”,\nthen the human may not have the competence to determine the extent\n\u0002\nλ(ξ)\n\u0003\n(ω) to which ξ is correct.\n(iii) Even if the human could fully compute λ(ξ), they may not have full access to their reward object RΩ\nin order to then compute G(ξ) as an explicit number.\nTo address (i), we assume a set of observations O that the human can receive. They may be come from\nan observation function O: Ξ →O, as in some examples from Amodei et al. (2017); Lang et al. (2024);\nCloud et al. (2024); Denison et al. (2024). For example, O(ξ) could be a sequence of observation segments,\none for each time-step. Alternatively, O could be a set of simulations to probe the human’s opinion on\nspeciﬁc situations that may be present in real trajectories. We can also imagine O to be a subset of Ξ. For\nexample, it can make sense to only show trajectories to the human that they are able to correctly evaluate,\nas in easy-to-hard generalization (Sun et al., 2024; Hase et al., 2024; Ding et al., 2024). In any case, we\n6\n\n\nassume O to be a ﬁxed set of observations where, for the most part, we are agnostic about the process that\ngenerates them.\nTo address (ii), we assume the human then has a feature belief function, i.e. a function ǫ : O →RΩthat\nencodes for each o ∈O the extent\n\u0002\nǫ(o)\n\u0003\n(ω) to which the human believes the feature ω to be present in the\nobservation o or an associated unknown trajectory ξ. Even if O = Ξ, we can have ǫ ̸= λ, which reﬂects that\nthe human may believe the features of a trajectory to be diﬀerent from what they actually are.\nThe human then judges observations o ∈O according to the observation return function GO ∈RO. It\nevaluates a trajectory by summing up the quality of all features, weighted by the extent to which the human\nbelieves them to be present:\nGO(o) :=\nX\nω∈Ω\n\u0002\nǫ(o)\n\u0003\n(ω) · RΩ(ω) =\n\nǫ(ξ), RΩ\n\u000b\n.\nLet E : RΩ→RΞ be given by\n\u0002\nE( ˜RΩ)\n\u0003\n(ξ) :=\n\nǫ(o), ˜RΩ\n\u000b\n. Then as for Λ, E is a linear function from which we\ncan recover ǫ using the matrix elements of E:\n\u0002\nǫ(o)\n\u0003\n(ω) = Eoω (Proposition A.1). We again slightly abuse\nthe terminology by referring to both ǫ : O →RΩand E : RΩ→RO as the human’s feature belief function.\nThis representation of the featre belief function satisﬁes E(RΩ) = GO.\nTo address (iii), in RLHF it is typically assumed that the human can make choices between observa-\ntions (Christiano et al., 2017) that implicitly reﬂect the underlying return function (or, on our case, ob-\nservation return function). We discuss this setting in Appendix B for the special case that all relevant linear\nfunctions are row-constant. However, in the main paper, we do not want to overcomplicate the theory and\nassume that the human can directly compute GO. In principle, if we would show each o ∈O to the human,\nwe could then record the entire observation return function GO. Thus, we assume GO, which represents the\nhuman’s feedback, to be fully known.\n2.5\nThe deﬁnition of human belief models\nHow can we infer the return function G from the human’s feedback, represented by GO? In order to do this,\nwe assume the features Ω, ontology Λ, and feature belief function E are all known; additionally, we assume\nwe may have a priori knowledge of a vector subspace of valid reward objects V ⊆RΩin which RΩresides:\nRΩ∈V. This may come from any a priori knowledge, e.g. of certain symmetries in the environment that\nleave rewards invariant (cf. Example 2.17 and Section 3.3). This leads to the following notion:\nDeﬁnition 2.2 (Human belief model, representing). Let Ξ be the set of trajectories in an MDP and O the\nﬁxed set of observations that a human evaluator receives. Then a human belief model (or belief model,\nor model, for short) is a tuple M = (Ω, Λ, E, V), where:\n• Ωis a set called feature set;\n• Λ : RΩ→RΞ is a linear function, called ontology;\n• E : RΩ→RO is a linear function, called feature belief function;\n• and V ⊆RΩis a vector subspace, called space of valid reward objects.\nA human belief model M represents the true return function G : Ξ →R and observation return function\nGO : O →R if there is a reward object RΩ∈V with G = Λ(RΩ) and GO = E(RΩ).\nWhen appropriate, we will also use the representation λ : Ξ →RΩwith\n\u0002\nλ(ξ)\n\u0003\n(ω) = Λξω and ǫ : O →RΩ\nwith\n\u0002\nǫ(o)\n\u0003\n(ω) = Eoω of the ontology and feature belief function, respectively (cf. Proposition A.1). We\n7\n\n\nvisualize a model M = (Ω, Λ, E, V) as\nRΞ\nRΞ\nRΩ\n⊆\nV\nor\nRΩ\nRO\nRO,\nΛ\nE\nΛ\nE\nwhere we use the latter version in the special case that V = RΩ.\nRemark 2.3 (Which human belief model?). A priori, there can be many human belief models c\nM =\n(bΩ, bΛ, bE, bV) that can represent the true return function G and the human’s feedback GO. For example, in Sec-\ntion 3.3 we will discuss a situation with three diﬀerent models, and in Appendix C one with many more. In\nfact, there is always a trivial belief model that can represent G and GO: View R as the R-vectorspace R{⋆}\nof functions from a one-element set {⋆} to R. Associate to G : Ξ →R = R{⋆} and GO : O →R = R{⋆}\nvia Proposition A.1 the linear functions lin(G) : R{⋆} →RΞ and lin(GO) : R{⋆} →RO given by\n\u0002\nlin(G)\n\u0003\n(r · e⋆) := r · G,\n\u0002\nlin(GO)\n\u0003\n(r · e⋆) := r · GO\nfor r ∈R and the only e⋆. Then, deﬁne the human belief model\nM :=\n\u0000{⋆}, lin(G), lin(GO), R{⋆}\u0001\nwith the feature set {⋆}. Then this represents G and GO with the reward object e⋆∈R{⋆} since\n\u0002\nlin(G)\n\u0003\n(e⋆) =\nG and\n\u0002\nlin(GO)\n\u0003\n(e⋆) = GO. Intuitively, the feature ⋆then means “goodness”, the “ontology” G : Ξ →R{⋆}\nmeasures the extent to which “goodness” is present in a trajectory, and the “feature belief function” GO :\nO →R{⋆} measures how much goodness the human believes to be present in an observation. This trivial\nmodel is not very interesting for our purposes since assuming that the human belief model is known would\nthen presuppose knowledge of G itself — which is the return function that we want to infer based on feedback\nGO.\nA belief model M = (Ω, Λ, E, V) is more interesting if Ωconsists of a variety of meaningful concepts such\nthat G and GO decompose linearly over these features, in such a way that:\n(i) It is easier to know the model M than to know the return function G.\n(ii) Given M and GO, it is easy to determine G.\nFor the rest of this section, we imagine to know the “true” belief model M, which we assume to have these\nproperties.\nNote that these are philosophical assumptions that guide how we talk about M and how we\nimagine its application. But mathematically, all of our claims here and in the upcoming sections are correct\nwhenever M represents G and GO.\nIn Section 3, we then relax the requirement of “knowledge” of M, and speciﬁcally in Section 3.4 we make\na proposal for model speciﬁcation using foundation models. We then also show how G could in principle be\nlearned by learning GO via supervised learning. We think that section will make it plausible that properties\n(i) and (ii) can hold in practice.\nExample 2.4. Assume the setting from Example 2.1, in which Ω= S ×A×S and Λ = Γ : RS×A×S →RΞ.\nWe now explain the belief model from Lang et al. (2024) in our framework, thus showing that we generalize\ntheir work.\nNamely, assume the human comes equipped with a belief function b : O →RΞ that assigns a probability\ndistribution over trajectories b(o) ∈∆(Ξ) ⊆RΞ to each observation o ∈O. For example, this could be\n8\n\n\na posterior belief if o is sampled by ﬁrst sampling a trajectory ξ according to some distribution and then\ncomputing the observation as o = O(ξ) for an obervation function O : Ξ →O.\nThen we assume that ǫ : O →RS×A×S computes, for each o ∈O and (s, a, s′) ∈S × A × S, the expected\ndiscounted number of times that the transition (s, a, s′) appears in the trajectory ξ expected probabilistically\nfrom observing o:\n\u0002\nǫ(o)\n\u0003\n(s, a, s′) :=\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ) · Γξ,(s,a,s′) .\nLet B : RΞ →RO be the linear function given by\n\u0002\nB(G)\n\u0003\n(o) :=\n\nb(o), G\n\u000b\n, which has matrix elements\nBoξ =\n\u0002\nb(o)\n\u0003\n(ξ) by Proposition A.1. We obtain\nEo,(s,a,s′) =\n\u0002\nǫ(o)\n\u0003\n(s, a, s′) =\nX\nξ∈Ξ\nBoξ Γξ,(s,a,s′) = (B ◦Γ)o,(s,a,s′).\nThus, E = B ◦Γ.\nWe ﬁnally assume that the set of valid reward objects is simply given by V = RS×A×S. Thus, in total, our\nmodel is given by (Ω, Λ, E, V) = (S × A × S, Γ, B ◦Γ, RS×A×S):\nRΞ\nRS×A×S\nRO.\nΓ\nB ◦Γ\n2.6\nComplete belief models and the ambiguity\nIn this whole subsection, we ﬁx an MDP with trajectories Ξ, observations O, and corresponding return\nfunction G ∈RΞ and observation return function GO ∈RO.\nWe also ﬁx a human belief model M =\n(Ω, Λ, E, V) that represents G and GO with an implicit reward object RΩ∈V: Λ(RΩ) = G and E(RΩ) = GO.\nWe can visualize the model together with the reward object and return functions as follows:\nG\nRΞ\nRΩ\n∈\nRΩ\n⊆\nV\nGO\nRO.\nΛ\nE\nΛ\nE\nWe are concerned with the following question:\nQuestion 2.5. Assume the human belief model M and the human feedback, in the form of the observation\nreturn function GO, are known. Under what conditions, or to what extent, can we infer the return function\nG?\nIn Section 3, we will relax the assumption that the belief model is known precisely.\nThe idea for the answer is as follows: When trying to infer G from GO, we make use of the knowledge that\nthey are connected by the unknown reward object RΩ∈V. Thus, one can ﬁrst try to determine a reward\nobject ˜RΩ∈V with the correct observation return function E( ˜RΩ) = GO. Then the question is whether the\ncorresponding return function Λ( ˜RΩ) equals the true return function G. They diﬀer by the return function\nG′ = Λ( ˜RΩ) −G. The ambiguity will then be deﬁned as the set of all these diﬀerences, and the question will\nbe how to characterize it and when it vanishes, leading to the notion of a complete human belief model.\n9\n\n\nDeﬁnition 2.6 (Feedback-compatible, ambiguity). We deﬁne the set of return functions that are feedback-\ncompatible with GO as\nFCM(GO) :=\nn\n˜G ∈RΞ \f\f ∃˜RΩ∈V : E( ˜RΩ) = GO and Λ( ˜RΩ) = ˜G\no\n.\nWe deﬁne the ambiguity left in the return function G after the observation return function GO is known by\nAmbM(G, GO) :=\nn\nG′ ∈RΞ \f\f G′ = ˜G −G for ˜G ∈FCM(GO)\no\n.\nThen clearly, we have\nFCM(GO) = G + AmbM(G, GO).\nThis leaves open the question of how to characterize the ambiguity and when it vanishes.\nProposition 2.7 (Ambiguity characterization). We have\nAmbM(G, GO) = Λ(ker(E) ∩V).\nProof. For one direction, let G′ ∈AmbM(G, GO). Then G′ = Λ( ˜RΩ) −G for ˜RΩ∈V with E( ˜RΩ) = GO.\nBy the linearity of E we obtain\nE( ˜RΩ−RΩ) = E( ˜RΩ) −GO = 0\nand thus ˜RΩ−RΩ∈ker(E) ∩V. Since Λ is linear, we obtain\nG′ = Λ( ˜RΩ) −G = Λ( ˜RΩ) −Λ(RΩ) = Λ( ˜RΩ−RΩ) ∈Λ(ker(E) ∩V).\nFor the other direction, let G′ ∈Λ(ker(E) ∩V). Then G′ = Λ(R′\nΩ) for R′\nΩ∈ker(E) ∩V. Deﬁne ˜RΩ:=\nRΩ+ R′\nΩ∈V. By the linearity of E and by R′\nΩ∈ker(E) we obtain\nE( ˜RΩ) = E(RΩ) + E(R′\nΩ) = GO + E(R′\nΩ) = GO.\nFinally, the linearity of Λ shows\nG′ = Λ(R′\nΩ) = Λ( ˜RΩ−RΩ) = Λ( ˜RΩ) −Λ(RΩ) = Λ( ˜RΩ) −G.\nThis shows G′ ∈AmbM(G, GO).\nSee Proposition B.6 for a version of the preceding proposition where the feedback is given by a choice\nprobability function instead of GO. See Appendix C.4 for the ambiguities of many human belief models.\nRemark 2.8. In light of the previous proposition, it turns out that the ambiguity does not depend on the\ntrue return function and observation return function, and we can thus simply write it as AmbM.\nExample 2.9. We continue Example 2.4, with the model given by M = (S × A × S, Γ, B ◦Γ, RS×A×S).\nProposition 2.7 implies that the ambiguity is given by AmbM = Γ(ker(B ◦Γ)) = ker(B)∩im(Γ). Lang et al.\n(2024) contains characterizations of this ambiguity in examples and special cases.\nIt is important to know when the ambiguity disappears, which is captured as completeness in the following\ndeﬁnition.\nDeﬁnition 2.10 (Completeness). We call the human belief model M = (Ω, Λ, E, V) complete if ker(E)∩V ⊆\nker(Λ) ∩V, i.e. if AmbM = Λ(ker(E) ∩V) = 0 (cf. Proposition 2.7).\nWe will explain in Interpretation 2.12 why we call this property completeness.\nWe ﬁnd equivalent and\nsuﬃcient conditions of completeness in the following proposition:\nTheorem 2.11 (Completeness characterization). Remember the representations λ : Ξ →RΩand ǫ : O →\nRΩof the ontology Λ : RΩ→RΞ and feature belief function E : RΩ→RO, respectively. Consider the\nfollowing four statements:\n10\n\n\n1. M is complete.\n2. There exists a linear function Z : RO →RΞ with Λ|V = Z ◦E|V.\n3. There exists a linear function Z : RO →RΞ with Λ = Z ◦E.\n4. For all ξ ∈Ξ, λ(ξ) ∈RΩis contained in the span of the image of ǫ: λ(ξ) ∈R\n\nǫ(o) | o ∈O\n\u000b\n.\nThen 1 and 2 are equivalent, and 3 and 4 are equivalent and imply 1 and 2:\n1\n2\n3\n4\nFurthermore, the linear function Z from 2 and 3 satisﬁes G = Z(GO). Finally, if V = RΩ, then all four\nstatements are equivalent.\nProof. Note that ker(Λ) ∩V = ker(Λ|V) and ker(E) ∩V = ker(E|V). Thus, the equivalence of statements 1\nand 2 immediately follow from Proposition A.3. That 3 implies 2 is obvious.\nNow assume statement 4. For all ξ ∈Ξ and o ∈O, let Zξo ∈R be coeﬃcients with\nλ(ξ) =\nX\no∈O\nZξoǫ(o).\nThis implies\nΛξω =\nX\no∈O\nZξoEoω.\nDeﬁne the linear function Z : RO →RΞ to have the matrix elements Zξo. Then the previous equation\nimplies Λ = Z ◦E, proving statement 3. That 3 implies 4 follows by reversing these arguments.\nAssume statement 2. Then\nG = Λ|V(RΩ) =\n\u0000Z ◦E|V\n\u0001\n(RΩ) = Z\n\u0000E(RΩ)\n\u0001\n= Z(GO).\nThat all statements are equivalent if V = RΩis clear.\nInterestingly, for complete models, the preceding proposition shows that G = Z(GO) for a linear function Z\nthat only depends on the human belief model M, thus showing once more that we can infer G from GO if\nthe model is complete. Since Z may be impractical to ﬁnd, it may however be more useful to determine G\nas the unique feedback-compatible return function by ﬁrst ﬁnding ˜RΩwith E( ˜RΩ) = GO.\nInterpretation 2.12. Overall, we have thus answered Question 2.5: When the human belief model is known,\nthen G can be inferred from GO up to the ambiguity AmbM = Λ(ker(E) ∩V), which vanishes if the model\nM is complete. Looking back at the deﬁnition of feedback-compatible return functions, G is then determined\nas G = Λ( ˜RΩ) for any ˜RΩ∈V that gives rise to the human’s feedback: E( ˜RΩ) = GO. For completeness, we\nthen found further equivalent and suﬃcient conditions in Theorem 2.11.\nWe now explain why we chose the name completeness. By Theorem 2.11, statement 3, a suﬃcient condition\nfor completeness is the existence of coeﬃcients Zξo ∈R such that for all ξ ∈Ξ, we can write λ(ξ) as a linear\ncombination of the ǫ(o):\nλ(ξ) =\nX\no∈O\nZξoǫ(o).\nThis means that the observation-dependent feature beliefs ǫ(o) span the true feature strengths of trajectories.\nIn this sense, the belief model is “complete”: The human’s interpretations of observations suﬃciently entail\nwhat happens in the MDP. It is thus not a surprise that completeness implies that the ambiguity disappears,\nand thus that we can infer the return function from the observation return function GO.\n11\n\n\n2.7\nFaithful belief models\nLet again M = (Ω, Λ, E, V) be a human belief model for an MDP together with trajectories Ξ and observa-\ntions O, and let RΩ∈V with G = Λ(RΩ), GO = E(RΩ). Here, we brieﬂy study the notion of faithfulness of\nhuman belief models, which is dual to completeness:\nDeﬁnition 2.13 (Faithfulness). We call the human belief model M faithful if ker(Λ) ∩V ⊆ker(E) ∩V.\nMany human belief models we study in this paper are faithful. The model from Example 2.9 is faithful\nsince ker(Γ) ⊆ker(B ◦Γ). All models from Section 3.3 are faithful. In Appendix C.2 we study several\nmore faithful human belief models. We can ﬁnd a characterization of faithfulness that is completely dual\nto Theorem 2.11:\nProposition 2.14 (Faithfulness characterization). Remember the representations λ : Ξ →RΩand ǫ : O →\nRΩof the ontology Λ : RΩ→RΞ and feature belief function E : RΩ→RO, respectively. Consider the\nfollowing four statements:\n1. M is faithful.\n2. There exists a linear function Y : RΞ →RO with E|V = Y ◦Λ|V.\n3. There exists a linear function Y : RΞ →RO with E = Y ◦Λ.\n4. For all o ∈O, ǫ(o) ∈RΩis contained in the span of the image of λ: ǫ(o) ∈R\n\nλ(ξ) | ξ ∈Ξ\n\u000b\n.\nThen 1 and 2 are equivalent, and 3 and 4 are equivalent and imply 1 and 2:\n1\n2\n3\n4\nFurthermore, the linear function Y from 2 and 3 satisﬁes GO = Y (G). Finally, if V = RΩ, then all four\nstatements are equivalent.\nProof. The proof is analogous to the one for Theorem 2.11.\nInterpretation 2.15. By Proposition 2.14, a suﬃcient condition for faithfulness is the existence of coeﬃ-\ncient Yoξ ∈R such that for all o ∈O, we can write ǫ(o) as a linear combination of the λ(ξ):\nǫ(o) =\nX\nξ∈Ξ\nYoξλ(ξ).\nWe can suggestively interpret Yoξ as the human’s “belief” that the true trajectory ξ was responsible for the\nobservation o.1\nUnder this interpretation, the feature strengths ǫ(o) are like an “expected value” of true\nfeature strengths, given the human’s beliefs. Thus, the feature beliefs encoded by ǫ are “faithful” to a belief\nover the actual MDP. This interpretation is especially valid for Example 2.4, where Y is given by B, and\nthus Yoξ = Boξ =\n\u0002\nb(o)\n\u0003\n(ξ) is the probability of the trajectory ξ, given o.\nFaithfulness is also philosophically reasonable for a related reason. Assume RΩ, ˜RΩ∈V are two reward objects\nwith Λ(RΩ) = Λ( ˜RΩ). Then the resulting return functions coincide in their evaluation of all trajectories.\nIt would then be reasonable to assume that they also coincide in their evaluation of observations, insofar as\nobservations give information about a state of aﬀairs in the actual MDP: E(RΩ) = E( ˜RΩ). This requires\nthat Λ(RΩ−˜RΩ) = 0 implies E(RΩ−˜RΩ) = 0, which exactly means that ker(Λ) ∩V ⊆ker(E) ∩V, i.e. the\nfaithfulness of the model. However, since human evaluators do not necessarily have feature beliefs that are\n“this rational”, and since this property is not needed in the rest of our theory, we do not assume faithfulness\nof our human belief models in the general theory.\n1Though note that Y is usually not unique and Yo ∈RΞ is usually not an actual probability distribution over ξ.\n12\n\n\n2.8\nConceptual examples\nWe now present some simple conceptual examples to illustrate the theory with respect to the ambiguity.\nWe will not deﬁne entire MDPs but only those parts of the formalism that are necessary to reason about\nthe ambiguity. In all our examples, we assume O ⊆Ξ, i.e., observations are entire trajectories: They are\nfully observed. This brings other factors than observability into the focus, like the human’s understanding\nof the trajectories, and whether there is enough data. We refer to Section 3.3 and Lang et al. (2024) for\nconcrete examples where the MDP is deﬁned entirely and a possibly remaining ambiguity stems from partial\nobservability.\nIn all examples, we have a feature set Ω= {ω1, ω2, ω3, ω4} whose meaning will vary in each scenario. We\nalso have observations o1, o2, o3, o4, o5, though the observation set O ⊆Ξ is sometimes only a subset of\n{o1, o2, o3, o4, o5}. We will make use of the feature belief function ǫ : O →RΩ∼= R4. We represent each ǫ(o)\nas a row-vector with entries ǫ(o)i =\n\u0002\nǫ(o)\n\u0003\n(ωi) as follows:\nǫ(o1) = (1, 0, 0, 1)\nǫ(o2) = (0, 1, 0, 1)\nǫ(o3) = (0, 0, 1, 1)\nǫ(o4) = (0, 3, 0, 2)\nǫ(o5) = (0, 0, 0, 1).\n(5)\nIn all examples, we assume Ξ, V, and Λ (and thus overall M = (Ω, Λ, E, V)) to also be known, though we\ndo not make them explicit.\nExample 2.16 (Simple Completeness). The goal is to create children stories ξ ∈Ξ that appeal to a speciﬁc\nchild Alice. We assume that it is known that children care about the presence or absence of exactly the\nfollowing four features Ω= {ω1, ω2, ω3, ω4}:\n• ω1: Rule-breaking.\n• ω2: Non-linear story-telling.\n• ω3: Scariness.\n• ω4: Moral lessons.\nWe show Alice four stories O = {o1, o2, o3, o4} ⊆Ξ that give rise to this feature belief function (cf. Equa-\ntion (5)):\nE : RΩ→RO,\nE =\n\n\n\n\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n3\n0\n2\n\n\n\n.\n(6)\nConceptually, we assume these are the true feature strengths, meaning that λ(o) = ǫ(o) for all o ∈O, though\nthis detail does not matter for the ambiguity analysis. The matrix elements mean that o1 contains rule-\nbreaking and moral lessons, but the story-telling is linear and the story is not scary. o4 is very non-linear and\ncontains quite some moral lessons, but shows no rule-breaking or scariness. Since the rows of Equation (6)\nform a basis of RΩ, we obtain\nR\n\nǫ(o) | o ∈O\n\u000b\n= RΩ,\nand so by Theorem 2.11 it follows that the model is complete. Thus, the ambiguity vanishes and we can infer\nthe return function from Alice’s feedback GO on the stories in O (cf. Interpretation 2.12).\nWe demonstrate this now explicitly. Assume RΩis Alice’s (a priori unknown) reward object, and that the\nresulting observation return function GO = E(RΩ) ∈RO is given as follows:\nGO(o1) = −3,\nGO(o2) = 1,\nGO(o3) = 0,\nGO(o4) = 4.\n(7)\n13\n\n\nRepresenting RΩand GO as column-vectors, this means:\n\n\n\n\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n3\n0\n2\n\n\n\n· RΩ= E(RΩ) = GO =\n\n\n\n\n−3\n1\n0\n4\n\n\n\n.\nThis results in\nRΩ=\n\n\n\n\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n3\n0\n2\n\n\n\n\n−1\n·\n\n\n\n\n−3\n1\n0\n4\n\n\n\n=\n\n\n\n\n1\n−3\n0\n1\n0\n−2\n0\n1\n0\n−3\n1\n1\n0\n3\n0\n−1\n\n\n\n·\n\n\n\n\n−3\n1\n0\n4\n\n\n\n=\n\n\n\n\n−2\n2\n1\n−1\n\n\n\n.\nThus, we recovered Alice’s reward object: She positively values non-linear story-telling (2) and scariness (1)\nbut does not like moral lessons (−1) or rule-breaking (−2). Together with the knowledge of the ontology Λ\nto associate features to stories, we can create more enjoyable stories for Alice.\nThis example shows that knowledge of return-relevant features and a successful modeling of the feature belief\nfunction can go a long way to determine the correct return function: We essentially need only four “data\npoints” (in the form of observation returns GO(o)) to determine G.\nExample 2.17 (Completeness from symmetries). We assume the same situation as in Example 2.16 except\nthis time, we assume that we only have three observations O = {o1, o2, o4}:\nE : RΩ→RO,\nE =\n\n\n1\n0\n0\n1\n0\n1\n0\n1\n0\n3\n0\n2\n\n.\n(8)\nNote that the third row now represents ǫ(o4) since o3 is not in O. Assume we have a priori information\nthat children who like scariness do not like moral lessons and vice versa; i.e., it is known that Alice’s reward\nobject satisﬁes the following equation:\nRΩ(ω3) = −RΩ(ω4).\nThe set of all reward functions with this property forms a vector subspace V ⊆RΩ. It turns out that this\nimplies ker(E) ∩V = {0}. Indeed, let R′\nΩ∈ker(E) ∩V. Then E(R′\nΩ) = 0 implies R′\nΩ(ω1) = R′\nΩ(ω2) =\nR′\nΩ(ω4) = 0, and R′\nΩ∈V implies R′\nΩ(ω3) = −R′\nΩ(ω4) = 0. With ker(E) ∩V = 0, the ambiguity vanishes:\nAmbM = Λ(ker(E) ∩V) = 0. Thus, the model is complete and the return function can be inferred from\nAlice’s feedback GO on O = {o1, o2, o4} (cf. Interpretation 2.12).\nExample 2.18 (Ambiguity from undetected vulnerabilities). The goal is to correctly evaluate coding-agents\nto produce valid and safe code blocks ξ ∈Ξ. We assume that exactly the following four features are relevant\nfor evaluating code:\n• ω1: Code-vulnerability.\n• ω2: Syntax error.\n• ω3: Simplicity.\n• ω4: Validity.\nWe show the human evaluator four code-blocks O = {o2, o3, o4, o5} ⊆Ξ that give rise to this feature belief\nfunction (cf. Equation (5)):\nE : RΩ→RO,\nE =\n\n\n\n\n0\n1\n0\n1\n0\n0\n1\n1\n0\n3\n0\n2\n0\n0\n0\n1\n\n\n\n.\n14\n\n\nCrucially, we can assume that o5 does contain a code-vulnerability, but the human does not detect it. In other\nwords, the ontology λ : Ξ →RΩassigns the feature strengths λ(o5) = (1, 0, 0, 1) ̸= (0, 0, 0, 1) = ǫ(o5). If the\nhuman had detected the code-vulnerability, then o5 would be mathematically equivalent to o1 in Example 2.16,\nthe model would be complete, and the ambiguity would disappear.\nHowever, this is not the case. Assuming V = RΩ, we obtain that ker(E) ∩V ̸= {0} contains the reward\nobject R′\nΩ∈RΩwith R′\nΩ(ω1) = 1 and R′\nΩ(ω) = 0 for all ω ̸= ω1. With RΩbeing the unknown true reward\nobject and ˜RΩ:= RΩ+ R′\nΩwe then have E( ˜RΩ) = E(RΩ) = GO, and so we cannot distinguish between the\nreward object ˜RΩand the true reward object RΩfrom the human’s feedback. Let G = Λ(RΩ) be the true\nreturn function. Then the return function ˜G = Λ( ˜RΩ) = G + Λ(R′\nΩ) ∈FCM(G) is feetback-compatible and\nsatisﬁes:\n˜G(ξ) = G(ξ) +\n\u0002\nΛ(R′\nΩ)\n\u0003\n(ξ)\n= G(ξ) +\n\nλ(ξ), R′\nΩ\n\u000b\n= G(ξ) +\n\u0002\nλ(ξ)\n\u0003\n(ω1).\nThis return function positively rewards code-vulnerabilities and can result from an attempt to infer G from\nGO.\nExample 2.19 (Rescuing the return inference). Consider Example 2.18, but with the code block o1 added\nto the observations, leading to all ﬁve observations O = {o1, . . . , o5}. This results in the following feature\nbelief function (cf. Equation (5)):\nE : RΩ→RO,\nE =\n\n\n\n\n\n\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n3\n0\n2\n0\n0\n0\n1\n\n\n\n\n\n\n.\nThen ker(E) = 0 and thus the ambiguity disappears: Λ(ker(E) ∩V) = 0, leading to a correct return function\ninference (cf. Interpretation 2.12). This example highlights that even when the human misinterprets some\nobservations (in our example: o5), the correct return function can sometimes be inferred as long as the\nhuman’s belief model including the feature belief function is known.\n3\nHuman belief model covering\nSo far, we assumed that the true belief model is known precisely, and studied when this allows to recover\nthe return function on trajectories from the human’s feedback. Knowing the true belief model might not be\nrealistic, and so we need to relax this condition. One possibility is to only require that we specify a belief\nmodel that can cover all return functions and observation return functions that are represented by the true\nmodel. If it does, and if it is complete (meaning the ambiguity disappears), then we can intuitively use such\na model for a correct return function inference. In Section 3.1, we deﬁne this notion of belief model covering.\nWe show that the ambiguity of a covering model is at least as large as the ambiguity of the true model.\nIf the ambiguity disappears, the covering model can be used to infer the correct return function from the\nhuman’s feedback (Theorem 3.2).\nIn Section 3.2, we then ﬁnd an equivalent condition of belief model covering, based on our notion of a\nmorphism between belief models. In many examples in this paper we have a natural morphism that accounts\nfor a model covering. In the same theorem, we also ﬁnd a suﬃcient condition for the existence of a morphism\nin terms of a linear ontology translation from the covering model to the covered model that also respects\nfeature beliefs (Theorem 3.5). If such an ontology translation exists, then the covering model has the capacity\nto simultaneously linearly represent the covered model’s concepts and beliefs.\nIn Section 3.3, we study a detailed example of belief model covering: We consider a human with an ontology\nthat is invariant under symmetry transformations in the environment, which implies that we can cover the\nhuman’s model with a model that assumes symmetry-invariant reward functions. We also demonstrate that\n15\n\n\nthis covering model has a vanishing ambiguity, improving upon the ambiguity of a model that considers\ngeneral reward functions. In Section 3.4, we conclude with a practical proposal for how to ﬁnd a belief\nmodel that covers the true human model, based on using foundation models for the ontology and feature\nbelief function. That the resulting belief model might cover the true human model is motivated by research\non sparse autoencoders (Cunningham et al., 2023; Bricken et al., 2023), which we will interpret as linear\nontology translations.\nIn this whole section, we assume an MDP together with trajectories Ξ and observations O as given.\n3.1\nHuman belief model covering and its implications\nDeﬁnition 3.1 (Belief model covering). Let M = (Ω, Λ, E, V) and c\nM = (bΩ, bΛ, bE, bV) be two human belief\nmodels. Then we say that c\nM covers M if for all v ∈V there exists bv ∈bV with bΛ(bv) = Λ(v) and bE(bv) = E(v).\nThis means that c\nM can represent all return functions Λ(v) and observation return functions E(v) that are\nrepresented by M. We can visualize this as follows:\nΛ(v) = bΛ(bv)\nRΞ\n∀v\n∃bv\n∈\nRΩ\n⊆\nV\nRb\nΩ\n⊆\nb\nV\nE(v) = bE(bv)\nRO\nΛ\nE\nb\nΛ\nb\nE\nΛ\nE\nb\nΛ\nb\nE\n(9)\nTheorem 3.2. Let M = (Ω, Λ, E, V) and c\nM = (bΩ, bΛ, bE, bV) be two human belief models, and assume that\nc\nM covers M. We think of M as the “true” human belief model representing G = Λ(RΩ) and GO = E(RΩ)\nwith a reward object RΩ∈V. Then we have:\n1. AmbM ⊆Amb b\nM.\n2. If M also covers c\nM, then AmbM = Amb b\nM.\n3. There is an Rb\nΩ∈bV with bE(Rb\nΩ) = GO and bΛ(Rb\nΩ) = G, and so c\nM also represents G and GO.\n4. Assume c\nM is complete. Then every reward object ˜Rb\nΩ∈bV with bE( ˜Rb\nΩ) = GO also satisﬁes bΛ( ˜Rb\nΩ) =\nG. In other words, the set of feedback compatible return functions is given by FC b\nM(GO) = {G}.\nProof. By Proposition 2.7, we have AmbM = Λ(ker(E) ∩V) and Amb b\nM = bΛ\n\u0000ker(bE) ∩bV\n\u0001\n. To prove claim\n1, let G′ = Λ(R′\nΩ) ∈AmbM, where R′\nΩ∈ker(E) ∩V. Then by the deﬁnition of c\nM covering M, there exists\na R′\nb\nΩ∈bV with bΛ(R′\nb\nΩ) = Λ(R′\nΩ) = G′ and bE(R′\nb\nΩ) = E(R′\nΩ) = 0. The latter implies R′\nb\nΩ∈ker(bE) ∩bV, and so\nG′ = bΛ(R′\nb\nΩ) ∈bΛ(ker(bE) ∩bV) = Amb b\nM. This proves claim 1.\nClaim 2 then immediately follows from claim 1. Claim 3 is also immediate by the deﬁnition of c\nM covering\nM and using that GO = E(RΩ) and G = Λ(RΩ).\nNow we prove claim 4. Thus, assume c\nM is complete and let ˜Rb\nΩ∈bV be a reward object with bE( ˜Rb\nΩ) = GO.\nBy claim 3, there exists an Rb\nΩ∈bV with bE(Rb\nΩ) = GO and bΛ(Rb\nΩ) = G. It follows ˜Rb\nΩ−Rb\nΩ∈ker(bE) ∩bV ⊆\nker(bΛ) ∩bV, where we use that c\nM is complete in the last step. Consequently, we have\nbΛ( ˜Rb\nΩ) = bΛ(Rb\nΩ) = G,\n16\n\n\nthus proving the claim.\nIn Theorem B.8 we present a version of the preceding theorem for the case that feedback is given by a choice\nprobability function instead of GO. In Appendix C.4, we see several applications of the ﬁrst two statements\nof the theorem to determine inclusions and equalities of ambiguities.\nWe can interpret Theorem 3.2 as follows: Given a “true” model M, but using a covering model c\nM, we lose\nsomething since the ambiguity of c\nM is possibly larger. However, c\nM is able to represent the true return\nfunction and observation return function, and if the ambiguity of c\nM disappears, then the set of return\nfunctions compatible with the human’s feedback that can be inferred using c\nM is exactly {G}. In other\nwords, we can then infer G from the human’s feedback and c\nM without knowing the true human belief model\nM, thus relaxing the assumptions baked into Question 2.5. In Section 3.4 we discuss hypotheses for ﬁnding\na covering belief model c\nM in practice.\n3.2\nMorphisms of human belief models and ontology translations\nDeﬁnition 3.3 (Morphism of human belief models). Let M = (Ω, Λ, E, V) and c\nM = (bΩ, bΛ, bE, bV) be human\nbelief models. Then a linear function Φ : RΩ→Rb\nΩis called a morphism of human belief models if the\nfollowing holds:\n1. Φ(V) ⊆bV.\n2. Λ|V = bΛ ◦Φ|V.\n3. E|V = bE ◦Φ|V.\nWe write the morphism also as Φ : M →c\nM.\nThe following visualization, an adaptation of Equation (9), makes intuitive that the existence of a morphism\nis equivalent to belief model covering:\nΛ(v) = bΛ\n\u0000Φ(v)\n\u0001\nRΞ\nv\nΦ(v)\n∈\nRΩ\n⊆\nV\nRb\nΩ\n⊆\nb\nV\nE(v) = bE\n\u0000Φ(v)\n\u0001\nRO\nΦ\nΛ\nE\nb\nΛ\nb\nE\nΛ\nE\nΦ\nb\nΛ\nb\nE\nHuman belief model morphisms are closely related to ontology translations. For this relationship, recall the\nform λ : Ξ →RΩ, ǫ : O →RΩof the ontology and feature belief function of a belief model M = (Ω, Λ, E, V),\nrespectively. We deﬁne:\nDeﬁnition 3.4 (Linear ontology translation). Let λ : Ξ →RΩ, ǫ : O →RΩand bλ : Ξ →Rb\nΩ, bǫ : O →Rb\nΩ\nbe two pairs of an ontology and a feature belief function. A linear function Ψ : Rb\nΩ→RΩwith Ψ ◦bλ = λ is\ncalled a linear ontology translation from bλ to λ. Furthermore, we call it belief-compatible with bǫ and\nǫ if we also have Ψ ◦bǫ = ǫ.\nThese notions are connected in the following Theorem:\nTheorem 3.5. Let M = (Ω, Λ, E, V) and c\nM = (bΩ, bΛ, bE, bV) be two human belief models. Let λ, ǫ, bλ,bǫ be the\nalternative representations of the ontologies and feature belief functions. Consider the following statements:\n17\n\n\n1. c\nM covers M.\n2. There exists a morphism of belief models Φ : M →c\nM.\n3. There exists a function Φ : RΩ→Rb\nΩwith bΛ ◦Φ = Λ and bE ◦Φ = E.\n4. There is a function Ψ : Rb\nΩ→RΩthat is a linear ontology translation from bλ to λ that is also\nbelief-compatible with bǫ and ǫ.\nThen 1 and 2 are equivalent, and 3 and 4 are equivalent and both imply 1 and 2:\n1\n2\n3\n4\nΨ from 4 can be deﬁned as Ψ = ΦT for Φ from 3. If V = RΩ, then all four statements are equivalent.\nFinally, if 2 holds and Φ(V) = bV, then M also covers c\nM and AmbM = Amb b\nM.\nProof. The implication from the second to the ﬁrst statement follows immediately from setting bv := Φ(v) in\nthe deﬁnition of belief model covering. For the other direction, consider the following diagram:\nbV\nV\nRΞ ⊕RO.\ng\nf\nIn this diagram, we deﬁne f := (Λ|V, E|V) and g := (bΛ|b\nV, bE|b\nV). Then statement 1 is equivalent to im(f) ⊆\nim(g). By Proposition A.4, there is thus a linear function φ : V →bV with g ◦φ = f:\nbV\nV\nRΞ ⊕RO.\ng\nf\nφ\nExtend φ arbitrarily to a linear function Φ : RΩ→Rb\nΩwith Φ|V = φ, e.g., by extending a basis on V to a\nbasis on all of RΩ. Then for all v ∈V, the diagram shows that we have\n\u0000Λ(v), E(v)\n\u0001\n=\n\u0000bΛ(Φ(v)), bE(Φ(v))\n\u0001\n.\nConsequently, Λ|V = bΛ ◦Φ|V and E|V = bE ◦Φ|V. Thus, Φ is a morphism of belief models.\nThat statements 3 and 4 are equialent and Ψ can be chosen as ΦT follows from Proposition A.2. That 3\nimplies 2 is clear. That all statements are equivalent if V = RΩis also clear.\nFor the ﬁnal statement, assume that 2 holds and that Φ(V) = bV. Then for all bv ∈bV there is v ∈V with\nΦ(v) = bv. Since Φ is a morphism, this results in\nΛ(v) = bΛ\n\u0000Φ(v)\n\u0001\n= bΛ(bv),\nE(v) = bE\n\u0000Φ(v)\n\u0001\n= bE(bv),\nshowing that M covers c\nM. AmbM = Amb b\nM then follows from statement 2 in Theorem 3.2.\n18\n\n\nIn Appendix C, we show that human belief models, together with their morphisms, form a category (Lane,\n1998).\nIn Appendix C.4 we then construct a large diagram of human belief models together with their\nnatural morphisms, which then also allows for an analysis of their ambiguities.\nThe preceding theorem shows that a suﬃcient condition for Φ : M →c\nM to be a human belief model\nmorphism is for ΦT : Rb\nΩ→RΩto be a belief-compatible ontology translation: ΦT ◦bλ = λ and ΦT ◦bǫ = ǫ.\nIntuitively, this means that our model c\nM is “expressive” enough to allow for the true model’s concepts and\nbeliefs to be linearly represented. We will draw more connections to linearly represented beliefs and concepts\nin Section 3.4.\n3.3\nAn example of symmetry-invariant features and reward functions\nWe now study an MDP with natural symmetries in the environment. We can then reasonably assume the\nhuman’s ontology to be invariant under these symmetries. We explain how one can cover the resulting human\nbelief model with a model that distinguishes between symmetry-related states, but compensates for it by\nassuming that the valid reward functions are symmetry-invariant (van der Pol et al., 2021). The ambiguity\nin this covering model will disappear, while the ambiguity of a third model that allows for generic reward\nfunctions does not. This demonstrates the usefulness of both covering belief models and a careful choice of\nthe vector space of valid reward objects. In particular, the analysis shows that encoding a priori knowledge\nof symmetries into the human belief model can be fruitful for inferring the correct return function from the\nhuman’s feedback.\nWe proceed by ﬁrst deﬁning the MDP, set of trajectories, and observations. Afterward, we deﬁne all three\nbelief models together with their morphisms, demonstrating model coverage. Finally, we conclude with the\nambiguity analysis.\n3.3.1\nSpeciﬁcation of the MDP, Ξ, and O\nOur MDP is a 2x2 gridworld with a movable hand H and a ﬁxed button B, inspired by the robot-hand\nexample from Amodei et al. (2017). States look like this:\nH\nB\n(10)\nIn total, the set of states S has sixteen elements, one for each combination of the position of H and B. The\nset of action is given by A = {L, R, U, D, P}, where the ﬁrst four actions move the hand: L to the left, R to\nthe right, U upward, and D downward. P does not change the state, and is conceptually meant to “press”\nthe button if the hand and button are in the same position. If a movement goes toward an adjacent wall in\nthe gridworld, then the state also does not change. This speciﬁes the transition function T : S × A →S.\nP0, the initial state distribution, is a uniform distribution over the following four states:\nH\nB\nH\nB\nH\nB\nH\nB\n(11)\nThe time horizon is T = 3. The unknown true return function is given by\nG(ξ) =\n2\nX\nt=0\nR(st, at),\nwhere R(st, at) = 1 if the hand H and button B are in the same position in st and if at = P, and R(st, at) = 0\notherwise. In other words, the return function rewards pressing the button. This completely speciﬁes the\nMDP (S, A, T , P0, T, G).\n19\n\n\nThe Trajectories Ξ are given by all sequences of four states and three actions that start with a state sampled\nfrom P0, and where each transition is compatible with the description above. The observations O of the\nhuman evaluator is given by views “from below”. We assume the human does not observe movement actions\n(but may be able to infer them if the hand visibly changes position), but does observe whether the button\nwas pressed. Formally, O = O(Ξ) for a surjective function O : Ξ →O that projects the view and removes\nmovement information, as we suggestively depict for an example trajectory ξ and its observation O(ξ) in\nthis ﬁgure:\nH\nB\nR\nH\nB\nD\nHB\nP\nHB\n7→\nO\nH\nB\nHB\nHB\nP\nHB\nSee Appendix D.1 for some mathematical details.\n3.3.2\nThree human belief models and their morphisms\nCrucially, we assume that the human evaluator does not use state-action pairs as features in the ontology,\nbut instead representatives of symmetry-equivalence classes of state-action pairs. This is reasonable since\nwe can a priori assume that the human evaluator does not care about the orientation of the scene. In other\nwords, we consider the symmetry group G = D4 of the square, which identiﬁes two state-action pairs if they\nare related by a rotation of 0◦, 90◦, 180◦, or 270◦, or a reﬂection along the vertical, horizontal, or one of the\ndiagonal axes. This leads to just three representative states\ns0 =\nHB\n,\ns1 =\nH\nB\n,\ns2 =\nH\nB\n.\n(12)\nOverall, the set of representative state-action pairs is given by S × A = S2\ni=0{si} × Ai with A0 = A2 =\n{L, D, P} and A1 = {L, R, U, D, P}.2 Let h : S × A →S × A map each state-action pair to the unique\nrepresentative. Details on everything discussed so far can be found in Appendix D.2.\nWe deﬁne the human’s ontology λ : Ξ →RS×A via\n\u0002\nλ(ξ)\n\u0003\n(s, a) :=\n2\nX\nt=0\nδ(s,a)(h(st, at)),\nwhich is the number of types that, up to symmetry, the state-action pair (s, a) appears in the trajectory\nξ. Interestingly, this ontology is invariant under transforming trajectories ξ via symmetries since h is in-\nvariant under transforming state-action pairs. Set Λ : RS×A →RΞ as the linear function correponding\nto λ via Proposition A.1, and let Γ : RS×A →RΞ be the function from Example 2.1 without discount-\ning (γ = 1).3\nFinally, let h∗: RS×A →RS×A be the function h∗(R) := R ◦h (see also the discussion\nsurrounding Equation (1)). Then in Equation (22) we show\nΛ = Γ ◦h∗.\n(13)\n2s0 and s2 have fewer representative actions since L and U, and also R and D, are related by the reﬂection along the diagonal\naxis from top left to bottom right. This transformation leaves the state invariant and maps between the actions.\n3Note the small diﬀerence that now we consider reward functions that only depend on state-action pairs instead of state-\naction-state transitions.\n20\n\n\nLet b : O →∆(Ξ) ⊆RΞ be the human’s trajectory belief function, where we deﬁne b(o) as the uniform\ndistribution over all ξ ∈Ξ that get observed as o: O(ξ) = o (cf. Equation (23)). We then deﬁne the human’s\nfeature belief function ǫ : O →RS×A by\n\u0002\nǫ(o)\n\u0003\n(s, a) :=\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ) ·\n\u0002\nλ(ξ)\n\u0003\n(s, a).\nThis is the expected number of times that, up to symmetry, the state-action pair (s, a) occurs in a trajectory\nthat led to observation o. Set E : RS×A →RO and B : RΞ →RO as the linear functions correponding to ǫ\nand b via Proposition A.1. Then as a consequence of Equation (13), we obtain\nE = B ◦Γ ◦h∗,\n(14)\nas we show in Equation (24).\nFinally, we assume that we have a priori knowledge that the human’s reward object lies in the subvectorspace\nV ⊆RS×A given by reward objects R with R(s, a) = 0 whenever a ̸= P. In other words, we know that only\nthe pressing-action can be rewarded, but we do not know a priori that it is only rewarded when the hand\nis over the button. Furthermore, deﬁne V′ ⊆RS×A likewise as reward functions with R(s, a) = 0 whenever\na ̸= P. Consider the commutative diagram\nRΞ\nRS×A\n⊆\nV\nRS×A\n⊆\nh∗(V)\nRS×A\n⊆\nV′\nRO.\nΓ ◦h∗\nB ◦Γ ◦h∗\nh∗\nΓ\nB ◦Γ\nidRS×A\nΓ\nB ◦Γ\nThis establishes three human belief models\nM1 = (S × A, Γ ◦h∗, B ◦Γ ◦h∗, V),\nM2 = (S × A, Γ, B ◦Γ, h∗(V)),\nM3 = (S × A, Γ, B ◦Γ, V′),\ntogether with the morphisms\nM1\nM2\nM3.\nh∗\nidRS×A\nOur aim will be to show that the ambiguities of M1 and M2 will vanish since these models leverage a priori\nknowledge of symmetries, while the ambiguity of M3 will not vanish. Intuitively, M1 is the true belief model\nof a human evaluator with symmetry-invariant features. M2 is the model that we “know” and with which\nwe “cover” the true belief model. That it indeed covers M1 follows from Theorem 3.5 and the existence of\nthe morphism h∗: M1 →M2. Write g.(s, a) for the action of a symmetry-transformation g ∈G = D4 of\nthe square on a state-action pair (s, a) (cf Appendix D.2). Then the set of valid reward objects of M2 is\ngiven by\nh∗(V) =\nn\nR ∈RS×A \f\f ∀g ∈G: R(g.(s, a)) = R(s, a) and ∀a ̸= P : R(s, a) = 0\no\n.\n(15)\nIn other words, it is the set of symmetry-invariant reward functions that don’t reward actions unequal to P.\nSuch reward functions play a role in symmetry-invariant reinforcement learning (van der Pol et al., 2021).\nFinally, M3 is the same model, but with a larger set of valid reward functions that are not necessarily\nsymmetry-invariant.\n21\n\n\nMathematically, all three models are faithful by Proposition 2.14. They are also balanced (Deﬁnition B.2)\nby Lemma B.3, which essentially means that the ontology and feature belief functions are row-constant. The\nthree models and their morphisms are also closely related to the three models MF\nF, MS×A×S\nF\n, and MS×A×S\nS×A×S\nthat we study in Appendix C.4.\n3.3.3\nThe ambiguity analysis\nWe now analyze the ambiguities of the three models M1, M2, and M3. Since the morphism h∗maps the\nset of valid reward objects V of M1 precisely to the valid reward objects h∗(V) of M2, by Theorem 3.5, M1\nand M2 have the same ambiguity. Thus, let us analyze the ambiguities of M1 and M3.\nFor M1, Proposition 2.7 shows that the ambiguity is given by (Γ ◦h∗)\n\u0002\nker(B ◦Γ ◦h∗)∩V\n\u0003\n. For showing that\nit vanishes, we simply show ker(B ◦Γ ◦h∗) ∩V = ker(E) ∩V = 0. Thus, let R ∈RS×A be a reward object\nin V with E(R) = 0. We need to show R = 0. Recall the representative states s0, s1, s2 from Equation (12).\nSince R ∈V, we have R(s, a) = 0 for all s ∈{s0, s1, s2} and all a ̸= P, and so we simply need to show\nR(si, P) = 0 for all i = 0, 1, 2. Consider the following three observations:\no0 =\nH\nB\nH\nB\nHB\nP\nHB\no1 =\nH\nB\nH\nB\nP\nH\nB\nP\nH\nB\no2 =\nH\nB\nP\nH\nB\nP\nH\nB\nP\nH\nB\nThen it is easy to show that\n\u0002\nE(R)\n\u0003\n(o2) = 0 implies R(s2, P) = 0 since (s2, P) is, up to symmetry, the only\nstate-action pair that is compatible with the observation o2. Then,\n\u0002\nE(R)\n\u0003\n(o1) = 0 implies R(s1, P) = 0\nsince (s1, P) is the only state-action pair other than (s2, P) that is compatible with the observation o1 and\ncould a priori have a non-zero contribution to the reward. Finally,\n\u0002\nE(R)\n\u0003\n(o0) = 0 implies R(s0, P) = 0 for\nsimilar reasons. We present details of these arguments in Appendix D.3. Overall, we have thus showed that\nR = 0, and thus ker(E) ∩V = 0, which implies AmbM1 = Λ(ker(E) ∩V) = 0 (cf. Proposition 2.7). Since M1\nand M2 have the same ambiguities, also our covering model M2 has vanishing ambiguity: AmbM2 = 0.\nNow we show that the ambiguity of M3 does not vanish. Consider the following two states, which are\nsymmetry-transformed versions of state s1 from Equation (12):\ns′\n1 =\nH\nB\n,\ns′′\n1 =\nH\nB\n.\nThen, let R′ : S × A →R be the reward function with\nR′(s, a) =\n\n\n\n\n\n1, s = s′\n1 and a = P\n−1, s = s′′\n1 and a = P\n0, else.\nClearly, we have R′ ∈V′. Then note that for all observations o, we have (B ◦Γ)o,(s′\n1,P ) = (B ◦Γ)o,(s′′\n1 ,P ), for\nsymmetry reasons.4 Thus, we have\n\u0002\n(B ◦Γ)(R′)\n\u0003\n(o) = 0 for all o ∈O, as we detail in Equation (25). This\nshows 0 ̸= R′ ∈ker(B ◦Γ)∩V′, and consequently 0 ̸= Γ(R′) ∈Γ(ker(B ◦Γ)∩V′) = AmbM3 (Proposition 2.7),\nproving the claim that the ambiguity is nontrivial. Crucially, in order to construct R′, we needed to allow\nthat the symmetry-related state-action pairs (s′\n1, P) and (s′′\n1, P) have diﬀerent rewards.\n4For this, recall that (B ◦Γ)o,(s,a) is simply the expected number of times that state-action pair (s, a) appears in a trajectory\nthat gives rise to observation o. For each trajectory, the up-down mirrored trajectory creates the same observation, leading to\nthe aforementioned symmetry.\n22\n\n\n3.3.4\nConclusion of the example\nThis example highlights that a priori knowledge of symmetry-invariant reward functions (via model M2\nwith its ambiguity h∗(V), see Equation (15)) or symmetry-invariant features (via model M1) can help to\ninfer the correct return function from the human’s feedback. We highlight again that one could in practice\nwork with model M2 even if M1 were the “true” model, the reason being that M2 covers M1 and has\na vanishing ambiguity. Future work could analyze this case in more detail, by developing a more general\ntheory of symmetry-invariant human belief models.\n3.4\nA proposal for belief model covering in practice\nSo far, our discussion has been purely theoretical: We showed that if a model c\nM =\n\u0000bΩ, bΛ, bE, bV\n\u0001\nis complete\nand covers the true belief model M = (Ω, Λ, E, V), then the true return function G can can be inferred from\nthe human’s feedback GO (Theorem 3.2, statement 4). This raises the following two questions:\n1. How can c\nM be speciﬁed?\n2. How can G be determined in practice, using c\nM and GO?\nWe now give preliminary answers to these questions in Sections 3.4.1 and 3.4.2, based on using foundation\nmodels for both the ontology bΛ and the feature belief function bE. We hope our ideas can inspire future\nempirical work.\n3.4.1\nDeﬁning c\nM for answering question 1\nTo answer question 1, ﬁrst, one needs to choose an MDP together with trajectories Ξ, and observations O.\nIdeally, whole trajectories ξ ∈Ξ or parts of them, and all observations, can be “tokenized” so that one can\nfeed them into foundation models. Let bλ : Ξ →Rb\nΩbe a foundation model, which we interpret as a function\nfrom trajectories to an internal representation space with |bΩ|-many neurons.5 Then, deﬁne bΛ : Rb\nΩ→RΞ as\nthe linear function corresponding to bλ according to Proposition A.1.\nFor bλ to be a valid ontology that is part of a belief model that covers the true model, by Theorem 3.5, we need\nthere to exist an (implicit) linear ontology translation Ψ : Rb\nΩ→RΩ. There is substantial prior work showing\nthat human concepts are represented linearly in foundation model’s representation spaces (Mikolov et al.,\n2013; Park et al., 2024b; Turner et al., 2024; Nanda et al., 2023; Wang et al., 2023; Gurnee & Tegmark,\n2024). Most relevant to our claims, sparse autoencoders (Cunningham et al., 2023; Bricken et al., 2023)\ndirectly construct a linear transformation that maps from a foundation model’s representation space to a\nspace of human-interpretable features, thus constructing a function akin to our (only implicitly needed)\nlinear ontology translation Ψ.\nNotably, bλ needs to be a capable foundation model for such a linear function to have any hope to be an exact\nontology translation. After all, imagine we show bλ the Riemann hypothesis: If it is not vastly superhuman,\nthen it cannot determine whether this hypothesis is true, which we consider an important feature that likely\nappears in the human’s ontology λ. Since we are concerned with scalable oversight, which is about the\nproblem of ensuring alignment of future, powerful AI systems, we assume bλ to be very capable, and thus\nthat Ψ is an exact ontology translation. Overall, this gives conﬁdence that for a capable foundation model\nbλ : Ξ →Rb\nΩ, there will exist a linear ontology translation Ψ : Rb\nΩ→RΩ, such that Ψ ◦bλ = λ.\nNow, let bǫ : O →Rb\nΩbe another foundation model (in the proposals below it will be an adaptation of bλ).\nThen, deﬁne bE : Rb\nΩ→RO as the linear functions corresponding to bǫ according to Proposition A.1. Set\nc\nM =\n\u0000bΩ, bΛ, bE, bV\n\u0001\n.\n5This means that we remove the output functionality from this model.\n23\n\n\nWhy or when would this be a useful speciﬁcation? As discussed before, we need c\nM to be complete and\nto cover the (implicit) “true” belief model M. Based on suﬃcient conditions we found in Theorem 3.5\nand Theorem 2.11, we thus need to ensure the following two properties:\n(i) The ontology translation Ψ : Rb\nΩ→RΩis belief-respecting: Ψ ◦bǫ = ǫ.\n(ii) For all ξ ∈Ξ, bλ(ξ) is contained in the span of the image of bǫ: bλ(ξ) ∈R\n\nbǫ(o) | o ∈O\n\u000b\n.\nEnsuring (i).\nThis is the most speculative part of our proposal. Crucially, if the human does not recognize\nthe presence of a feature in o ∈O due to limited capabilities, then bǫ should ideally also not recognize this\nfeature so that translating from one ontology into the other respects beliefs. Conceptually, this means that\nbǫ should simulate, in the feature space bΩ, the human’s beliefs and understanding. We make three proposals:\n• For research prototyping, one could restrict to problem with “pure partial observability”. In other\nwords, choose a setting where observations are given as o = O(ξ) for trajectories ξ ∈Ξ, and such\nthat superhuman capabilities do not make it easier to infer further return-relevant aspects of ξ from\nO(ξ), e.g., if information is simply entirely “missing” from observations. Then, simply choose bǫ := bλ,\napplied to observations instead of trajectories. In this case, the fact that crucial information is\nequally obstructed to the human with feature belief function ǫ and to the AI with bǫ should ensure\nthat the ontology translation also correctly translates feature beliefs: Ψ ◦bǫ = ǫ.\n• Now consider a setting that may go beyond “pure partial observability”. One speculative idea for\nhow to construct bǫ is to prepend a “belief prompt” bp to inputs of bλ that nudges the model to think\nmore in the way a human evaluator would think (Park et al., 2024a):\nbǫ(o) := bλbp(o) := bλ(bp, o).\nAn example of such a prompt would be\nbp = “Think about the following input like a typical human evaluator:”\nAlternatively, one could potentially achieve this by ﬁnetuning bλ to obtain bǫ. Unfortunately, while\nfoundation models can simulate the behavior of speciﬁc people in their outputs, it is unclear whether\nthis also reﬂects in their internal representations. For example, prior work shows that the truth-\nvalue of statements can sometimes be linearly predicted from internal representations even when the\nmodel outputs the non-truth, leading to the potential for AI lie detectors (Azaria & Mitchell, 2023;\nBurns et al., 2023b). However, other work trains a foundation model to predict human behavior\nin experiments and ﬁnds it to have internal representations that can predict human neural activity\nwhen engaging in the same task (Binz et al., 2024).\n• Alternatively, one could also consider deﬁning bǫ := bλearly as an earlier training checkpoint of bλ.\nBeing an earlier training checkpoint, bǫ would then be less capable than bλ, leading to the potential\nthat it has the same blindspots in understanding observations o ∈O as the human evaluator.\nEnsuring (ii).\nTo ensure that for all ξ ∈Ξ we have bλ(ξ) ∈R\n\nbǫ(o) | o ∈O\n\u000b\n, it is important to ensure\nthat the image of bǫ : O →Rb\nΩis “large”, i.e., spans as much as possible of the representation space. To\nform more intuitions on this, note that by Theorem 3.2, for c\nM to be complete (which would be implied by\nproperty (ii)) requires also the true belief model M to be complete. Again, by Theorem 2.11, a suﬃcient\ncondition for this is that for all ξ ∈Ξ, λ(ξ) ∈R\n\nǫ(o) | o ∈O\n\u000b\n. In particular, any “bad” feature ω ∈Ω\nthat can ever be present in a trajectory (meaning\n\u0002\nλ(ξ)\n\u0003\n(ω) ̸= 0) needs to also sometimes be believed to be\npresent by the human (meaning there exists an o ∈O with\n\u0002\nǫ(o)\n\u0003\n(ω) ̸= 0). This is a relaxation from the\nrequirement that the human understands all observations perfectly, but it does mean that there needs to\nbe a variety of observations o ∈O that is understandable enough for the human to sometimes detect any\npossible problem. It will depend on the speciﬁc MDP and setup of an experiment to reason about how to\nensure or purposefully violate this property.\n24\n\n\n3.4.2\nLearning G using c\nM and GO for answering question 2\nNow that we have discussed preliminary approaches for how to specify a complete model c\nM =\n\u0000bΩ, bΛ, bE, bV\n\u0001\nthat covers the true belief model M, we can turn to the question of how to use M and the human’s feedback\nGO : O →R to determine the true return function G. By statement 4 in Theorem 3.2, we can determine G\nas G = bΛ(Rb\nΩ) for Rb\nΩ∈Rb\nΩwith bE(Rb\nΩ) = GO.\nWe now unpack that. Remembering the relation between bΛ and the foundation model bλ, and bE and the\nfoundation model bǫ via Proposition A.1, we want to determine Rb\nΩsuch that for all o ∈O, we have\nGO(o) =\n\u0002bE(Rb\nΩ)\n\u0003\n(o) =\n\nbǫ(o), Rb\nΩ\n\u000b\n.\n(16)\nThis can be achieved by attaching Rb\nΩas a linear reward probe to the representation space of bǫ and learning\nthe function GO by supervised learning.\nIn the case that GO cannot be directly evaluated but that it is indirectly accessible in the form of choice\nprobabilities between observations, one can learn Rb\nΩby logistic regression as in (Christiano et al., 2017).\nSee Appendix B.2 for a possible correspondence between bE(Rb\nΩ) and the resulting choice probabilities. No-\ntably, the approach via logistic regression, however, loses a theoretical guarantee: Namely, GO can at best\nbe learned up to an additive constant. If bǫ and bλ were balanced (meaning that the total weight of feature\nstrengths is constant over all observations and trajectories, respectively), this would lead to the inferred G\nalso being correct up to an additive constant by Proposition B.6. Since additive constants in return func-\ntions are inconsequential for policy optimization, this would be ﬁne. However, typically the representation\nspaces of foundation models are not normalized, the balancedness property does not hold, and this guarantee\nbreaks. It is then an empirical question to what extent this breakage is an issue or how to resolve it.\nAfter successful learning, we can then compute the true return function G for ξ ∈Ξ as:\nG(ξ) =\n\u0002bΛ(Rb\nΩ)\n\u0003\n(ξ) =\n\nbλ(ξ), Rb\nΩ\n\u000b\n.\n(17)\nIn other words, we attach the learned linear reward probe Rb\nΩto the representation space of bλ and use it to\ncompute returns. These can then be used to train a policy to maximize the policy evaluation function Equa-\ntion (4) via standard reinforcement learning techniques.\n3.4.3\nFurther remarks\nLooking at the deﬁnition of the human belief model c\nM, we see that it includes the ontology bΛ : Rb\nΩ→RΞ\nand feature belief function bE : Rb\nΩ→RO. These can be extremely large matrices. However, since in the\nprocess of training Rb\nΩand computing G, we only need to be able to query the resulting observation return\nfunction and return function on speciﬁc observations and trajectories as in Equations (16) and (17), the\nmatrices never need to be stored or used in their entirety. Thus, the size of the matrices is not a concern.\nWe also remark on a special case we mentioned before: If we are in a case of “pure” partial observability\nwhere observations o ∈O contain no information that bλ understands better than the human, then we\nproposed to simply set bǫ := bλ, applied to observations o ∈O. In that case, the procedure we describe\nis essentially classical RLHF, with the only — crucial — diﬀerence that during training of Rb\nΩ, we only\nshow observations, instead of full trajectories, to the model. This prevents a model misspeciﬁcation where\nthe data the model reads diﬀers from what the human sees, and could theoretically allow generalization\nto data ξ ∈Ξ.\nLang et al. (2024) consider naive RLHF, where the initialized return function reads entire\ntrajectories during training time while the human’s view is obstructed, leading to issues of deceptive inﬂation\nand overjustiﬁcation after policy optimization.\n3.4.4\nConclusions for the proposal\nWe have thus explained how to specify c\nM using (possibly adapted) foundation models, and how to learn G\nby supervised learning of GO with an attached reward probe. We note again that these ideas are preliminary\nand leave open many choices to be made, and that empirical research would need to determine whether the\n25\n\n\nproposal can be instantiated in a compelling way. In Section 4.3.2, we motivate further future work related\nto this proposal.\n4\nDiscussion\n4.1\nSummary\nIn this work, we have introduced the notion of a human belief model, based on modeling a human’s ontology\nand feature belief function. The goal of such a model is to aid the inference of the human’s implicit return\nfunction from feedback. In our framework, the feedback, in the form of an observation return function, is\nviewed as carrying information about the reward of features that the human believes to be associated with\nan observation. Once the feature rewards, in the form of a reward object within a valid set, are inferred, they\ncan be used together with the human’s ontology to infer a return function. We deﬁned and characterized\nthe resulting ambiguity in the return function in terms of the belief model and showed that for complete\nmodels, the ambiguity disappears. Complete models have an important suﬃcient condition in terms of a\nlinear coverage of the features of any trajectory by feature beliefs of observations (Theorem 2.11). This\nshows that for observations that are varied enough and provide ample information in total, a correct return\nfunction inference is possible, which we demonstrated in simple conceptual examples in Section 2.8.\nSince the human belief model is not known in practice, we then introduced the notion of belief model coverage.\nHere, model c\nM covers another model M if c\nM can represent all return functions and observation return\nfunctions that can be expressed in M. If a complete model covers the true belief model, then it can be used\nfor the return function inference just as well (Theorem 3.2). We then characterized belief model coverage\nin terms of belief model morphisms, and found an important suﬃcient condition given by the existence of\na linear translation from the covering model’s ontology to the true model’s ontology that is also compatible\nwith the feature belief functions (Theorem 3.5). We then studied a conceptual example of a human with\nsymmetry-invariant feature beliefs, which we could cover with a model whose completeness stems from the\nvalid reward functions being symmetry-invariant.\nFinally, in Section 3.4 we sketched a proposal for how to ﬁnd covering human belief models in practice,\nby using foundation models for both the human’s ontology and feature belief function. For the latter, it is\nimportant to ensure that the foundation model has a similar understanding of the observtions as the human\nevaluator, for which we sketched out three proposals. That the resulting belief model might cover the true\nbelief model relies on prior research on the linear representation hypothesis, which provides evidence that a\nbelief-respecting linear ontology translation could indeed exist.6 Our hope is that our proposal can help to\nﬁnd covering models that are easier to determine than the return function itself, which might subsequently\nbe trained with modest eﬀort as an approach to scalable oversight (Remark 2.3).\n4.2\nRelated work\nOther human modeling approaches.\nThe human belief models we introduce in this work are largely\nabout modeling a human’s ontology and feature belief function for learning from the human’s feedback. There\nis also other work that models aspects of humans for better goal inference. For example, reward-rational\nchoice (Jeon et al., 2020) requires modeling human choice probabilities for choices over various options like\ntrajectory-pairs, language-utterances, or initial environment states.\nThis framework can be regarded a\nspecial case of assistance problems (Fern et al., 2014; Hadﬁeld-Menell et al., 2016; Shah et al., 2021), which\nrequires a model of the human’s action selection in a cooperative two-player game. This framework has\nrecently been generalized to a partially observable setting (Emmons et al., 2024). Much of this work makes\nspeciﬁc assumptions about the human’s model, like a Boltzmann-rational or optimal selection of choices.\n(Zhi-Xuan et al., 2024) instantiates a version of assistance games in which a human’s utterance is modeled\nusing a language model. Finally, Hatgis-Kessell et al. (2025) researches how to inﬂuence human evaluators\nto conform with a theoretical model of human choices. Compared to all this work, we instead model human\n6We emphasize again that our theory only requires its existence and no explicit speciﬁcation of this ontology translation.\n26\n\n\nbeliefs about AI behavior instead of human actions or choices.7 In Section 4.3.1 we propose research directions\nto combine these types of work.\nDeception in AI.\nOur work generalizes the human belief modeling from Lang et al. (2024), which is\nmeant to address deceptive AI behavior that also surfaces in recent empirical work for cases where humans\nlack evaluative capacity (Cloud et al., 2024; Denison et al., 2024; Wen et al., 2024; Williams et al., 2024).\nDeception in AI systems can also occur for various other reasons (Park et al., 2024c), e.g., when language\nagents are put under pressure (Scheurer et al., 2023). An important theoretical concern is deceptive align-\nment, in which an AI system follows the given goals while actually planning a later takeover (Hubinger et al.,\n2019). Recent work (Greenblatt et al., 2024) substantiates this concern by showing that Claude plays along\nwith a new harmful goal for the purpose of preventing that the learning process updates its safety behavior\nin the long-term. Finally, deception has also been formalized for structural causal games (Ward et al., 2023).\nSurfacing\nlatent\nknowledge.\nIn\nSection\n3.4,\nwe\nalready\nmentioned\nsparse\nautoen-\ncoders (Cunningham et al., 2023; Bricken et al., 2023), which construct an explicit linear transformation\nfrom a foundation model’s representation space to human-interpretable features, which is in the spirit\nof a linear ontology translation (Deﬁnition 3.4).\nInstead of decoding the AI’s entire ontology in a\nhuman-interpretable way, other paradigms seek to construct a reporter that can be queried for speciﬁc\ninformation (Christiano et al., 2021). In this direction, recent work builds toward AI lie detectors by ﬁnding\ninternal linear representations of truth (Burns et al., 2023b; Azaria & Mitchell, 2023; Marks & Tegmark,\n2024), with alternative interpretations of such ﬁndings discussed in Liu et al. (2023). Other work linearly\npredicts concepts like harmfulness (Zou et al., 2024), theft advice (Roger, 2023), the activation of a harmful\nbackdoor (MacDiarmid et al., 2024), and concepts related to honesty and power, among others (Zou et al.,\n2023).\nAmpliﬁed oversight.\nWhile our work aims to learn from feedback of humans who potentially lack capabil-\nities, work on ampliﬁed oversight tries to increase the human’s evaluation capabilities through AI assistance\nto achieve scalable oversight. Recursive reward modeling is the general proposal to train AI models by\nreward modeling and then using their assistance to evaluate the next generation of AIs (Leike et al., 2018).\nSaunders et al. (2022) shows that model-written critiques of summaries can help humans ﬁnd ﬂaws that they\nwould have missed on their own. This raises the question why to trust the critiquing AI, which leads to the\nidea to, in turn, criticize the critic. Recursively, this leads to a debate in which the debaters are trained to\nproduce arguments that are persuasive to a human judge (Irving et al., 2018). This requires for debates to\nsurface true and useful information to the human judges, which has found support for reading comprehen-\nsion tasks (Michael et al., 2023). Optimizing debaters to be persuasive then increases the human’s ability\nto identify the truth (Khan et al., 2024; Kenton et al., 2024). Finally, some work uses AI to directly give\nfeedback based on a constitution (Bai et al., 2022) or model spec (Guan et al., 2025), thus omitting humans\nfrom the evaluation process.\nEasy-to-hard and weak-to-strong generalization.\nInstead of amplifying the evaluator capabilities,\nother work for scalable oversight relies on easy examples that humans can reliably evaluate, which then\nrequires the reward model to generalize to data that is harder to evaluate (Sun et al., 2024). Language\nmodels have also shown to generalize tasks like STEM questions from easy to hard data (Hase et al.,\n2024). Weak-to-strong generalization diﬀers by trying to generalize from weak evaluation on potentially\nhard data (Burns et al., 2023a). Our proposal from Section 3.4 can be considered an approach to weak-to-\nstrong generalization since we aim to learn a correct return function G from feedback of an evaluator with\npotentially faulty beliefs. Since weak supervision is plausibly cheaper to obtain than strong supervision,\nthere is also work that investigates tradeoﬀs to ﬁnd the correct allocation of a ﬁxed budget to label data\nwith diﬀerent data labeling quality (Mallen & Belrose, 2024).\nIn Appendix E, we brieﬂy interpret ampliﬁed oversight, easy-to-hard generalization, and classical RLHF\ntogether with weak-to-strong generalization in terms of our theoretical framework by interpreting their\n7Only in Appendix B do we consider human choices.\n27\n\n\nunderlying evaluator belief modeling assumptions and reasoning about their ambiguities and learned return\nfunctions.\n4.3\nFuture work\n4.3.1\nTheory extensions\nSeveral extensions and generalizations of our theory could be studied in future work. We assumed that we\ncan exactly specify a belief model that covers the true human belief model. Future work could develop an\napproximate theory, akin to Lang et al. (2024, Theorem 5.4). Furthermore, we assumed that the human’s\nreturn function is linear in the features of trajectories. One could study non-linear models, which would also\ntheoretically ground the use of non-linear reward probes instead of the linear probes we propose in Section 3.4.\nWe also assumed that learned return functions can read entire trajectories, which might be unrealistic for\nvery complex environments.\nIt would thus be interesting to develop a theory based on a second set of\nobservations O′ for the learned return function and the trained policy. In the practical proposal, this would\nalso mean that we cannot assume to have a foundation model bλ that translates to the human’s true ontology\n— instead, it would represent another feature belief function. Relaxing the capabilities of the foundation\nmodel could also help to model a case in which the human evaluator has information that is hidden from\nthe learned return function or resulting policy.\nThe ambiguity is a measure of the information that is available in the feedback, given a belief model, for\ndetermining the human’s return function. Future work could theoretically study concrete learning proce-\ndures, which are about extracting said information. One could, for example, study training distributions over\nthe observations O to determine sample complexity bounds for the error of the resulting return function,\nor the regret of the resulting policy. This is theoretically interesting since the usage of the learned return\nfunction in policy optimization involves two shifts compared to the return function’s learning process: First,\nit needs to evaluate trajectories instead of observations, making use of an ontology instead of a feature belief\nfunction; and second, the policy optimization leads to a further distribution shift over trajectories, which\ncan in turn lead to increased regret even if the return function seemed accurate before (Fluri et al., 2024).\nFinally, if there is remaining ambiguity, it would also be desirabe to study protocols for deciding for a return\nfunction within the ambiguity, possibly using a priori knowledge about “human-like” return functions that\nis not captured by our notion of valid reward objects.\nGoing beyond our framework, one could attempt a synthesis with work that models the human’s action selec-\ntion, which we discussed at the start of Section 4.2. For example, one could consider reward-rational choice\nor assistance games under the assumption that humans form beliefs based on observations. Emmons et al.\n(2024) does a ﬁrst step in this direction by assuming humans form rational beliefs based on knowledge of a\nprior of the agent’s policy. In that framework, they then study the notion of information interference, which\nleads to an increase in the human’s uncertainty about the world state. If future work were to study more\ngeneral, possibly faulty, belief models for humans in partially observable assistance games, this could make\nit possible to go beyond information interference to study deception.\n4.3.2\nEmpirical work\nWe would be interested in attempts to instantiate our proposal from Section 3.4. One could test the proposal\nin settings with synthetic humans with a known ontology and feature belief function, which can for small\nMDPs allow to compute the ambiguity explicitly. This should make it possible to make concrete predictions\nabout experimental results. It would also be interesting to study settings with partial observability in which\ncapable AI does not have an advantage over humans in understanding the meaning of the observations.\nAs we discussed in our proposal, that should allow to use the same foundation model for the ontology and\nfeature belief function, with the latter only applied to observations. This could then be compared with\na baseline of “naive” RLHF in which the initialized return function reads entire trajectories during the\nlearning process, similar to the conceptual examples from Lang et al. (2024). It would be interesting to\nstudy diﬀerent levels of observability to dial the ambiguity up or down. We also encourage to break some of\nour theoretical assumptions, e.g., by using non-linear reward probes. Finally, it would be desirable to learn\nhow our approach can be combined with approaches in the direction of ampliﬁed oversight, easy-to-hard\n28\n\n\ngeneralization, or weak-to-strong generalization discussed in Section 4.2. For example, a combination of our\nwork with easy-to-hard generalization could seek to only train the reward pobe on data where the human\nbeliefs are modeled correctly by bǫ, which should be a superset of easy data.\nInstead of studying the whole pipeline, one could also empirically assess the underlying assumptions. The\nmost precarious assumption is that it is possible to construct a feature belief function bǫ, for which we\ndiscuss proposals in Section 3.4.1.\nIn situations that are not purely based on partial observability, we\nproposed to prompt the model to step “into the shoes” of a human evaluator, or to use earlier training\ncheckpoints of a model to decrease its capabilities to that of a human evaluator (assuming future models\nthat would otherwise be superhuman). For this to work, there needs to be a linear ontology translation\nthat is compatible with bǫ and the human’s true feature belief function ǫ. One could test this empirically\nby using sparse autoencoders (Cunningham et al., 2023; Bricken et al., 2023) trained on an unobstructed\ncapable foundation model, and evaluating the resulting human-readable features when applied to bǫ.\n4.4\nConclusion\nIn conclusion, in this work we theoretically studied models of human beliefs about AI behavior for the goal\nof scalable oversight, thus contributing to the theory of how to align advanced AI. We hope that future work\nwill build on our theory, empirically study our practical proposal, or engage in other research on how to\nmake AI systems safe and aligned with the goal of decreasing the risks of advanced AI.\nAuthor Contributions\nLeon Lang developed the ideas, derived all theoretical results, and wrote the paper. Patrick Forré gave\ngeneral guidance and feedback.\nAcknowledgments and disclosure of funding\nWe thank Scott Emmons, Davis Foote, Erik Jenner, Micah Carroll, and Alex Cloud for discussions that\nshaped how we think about human evaluators with limited capabilities. We thank Open Philanthropy for\nﬁnancial support.\n29\n\n\nReferences\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete\nProblems in AI Safety, 2016. URL https://arxiv.org/abs/1606.06565.\nDario\nAmodei,\nPaul\nChristiano,\nand\nAlex\nRay.\nLearning\nfrom\nhuman\npreferences.\nhttps://openai.com/research/learning-from-human-preferences, 2017. Accessed: 2023-12-13.\nAnthropic. Introducing Claude. https://www.anthropic.com/index/introducing-claude, 2023a. Ac-\ncessed: 2023-09-05.\nAnthropic. Claude’s Constitution. https://www.anthropic.com/index/claudes-constitution, 2023b.\nAccessed: 2023-09-05.\nAmos Azaria and Tom Mitchell.\nThe Internal State of an LLM Knows When It’s Lying, 2023.\nURL\nhttps://arxiv.org/abs/2304.13734.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom\nConerly, Sheer El-Showk, Nelson Elhage, Zac Hatﬁeld-Dodds, Danny Hernandez, Tristan Hume, Scott\nJohnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack\nClark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless\nAssistant with Reinforcement Learning from Human Feedback, 2022.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher\nOlah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie\nKerr, Jared Mueller, Jeﬀrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,\nMichael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby,\nRobin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera\nLanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman,\nZac Hatﬁeld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. Constitutional AI: Harmlessness from AI Feedback. arXiv e-prints, art. arXiv:2212.08073,\nDecember 2022. doi: 10.48550/arXiv.2212.08073.\nMarcel Binz, Elif Akata, Matthias Bethge, Franziska Brändle, Fred Callaway, Julian Coda-Forno, Peter\nDayan, Can Demircan, Maria K. Eckstein, Noémi Éltető, Thomas L. Griﬃths, Susanne Haridi, Akshay K.\nJagadish, Li Ji-An, Alexander Kipnis, Sreejan Kumar, Tobias Ludwig, Marvin Mathony, Marcelo Mattar,\nAlireza Modirshanechi, Surabhi S. Nath, Joshua C. Peterson, Milena Rmus, Evan M. Russek, Tankred\nSaanum, Natalia Scharfenberg, Johannes A. Schubert, Luca M. Schulze Buschoﬀ, Nishad Singhi, Xin\nSui, Mirko Thalmann, Fabian Theis, Vuong Truong, Vishaal Udandarao, Konstantinos Voudouris, Robert\nWilson, Kristin Witte, Shuchen Wu, Dirk Wulﬀ, Huadong Xiong, and Eric Schulz. Centaur: a foundation\nmodel of human cognition, 2024. URL https://arxiv.org/abs/2410.20268.\nMichael Bowling, John D. Martin, David Abel, and Will Dabney. Settling the Reward Hypothesis, 2022.\nSamuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil˙e Lukoši¯ut˙e,\nAmanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christo-\npher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion,\nJamie Kerr, Jared Mueller, Jeﬀrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage,\nNicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish,\nSandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-\nLawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatﬁeld-Dodds, Ben Mann, and\nJared Kaplan. Measuring Progress on Scalable Oversight for Large Language Models, 2022.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer,\nTim Maxwell, Nicholas Joseph, Zac Hatﬁeld-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,\n30\n\n\nJosiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards Monose-\nmanticity: Decomposing Language Models With Dictionary Learning. Transformer Circuits Thread, 2023.\nhttps://transformer-circuits.pub/2023/monosemantic-features/index.html.\nCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbren-\nner,\nYining Chen,\nAdrien Ecoﬀet,\nManas Joglekar,\nJan Leike,\nIlya Sutskever,\nand JeﬀWu.\nWeak-to-Strong Generalization:\nEliciting Strong Capabilities With Weak Supervision, 2023a.\nURL\nhttps://arxiv.org/abs/2312.09390.\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering Latent Knowledge in Language\nModels Without Supervision. In The Eleventh International Conference on Learning Representations,\n2023b. URL https://openreview.net/forum?id=ETKGuby0hcs.\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep Re-\ninforcement Learning from Human Preferences. arXiv e-prints, art. arXiv:1706.03741, June 2017. doi:\n10.48550/arXiv.1706.03741.\nPaul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts,\n2018. URL https://arxiv.org/abs/1810.08575.\nPaul\nChristiano,\nAjeya\nCotra,\nand\nMark\nXu.\nEliciting\nLatent\nKnowledge.\nhttps://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit,\n2021. Accessed: 2023-04-25.\nAlex Cloud, Jacob Goldman-Wetzler, Evžen Wybitul, Joseph Miller, and Alexander Matt Turner.\nGradient Routing:\nMasking Gradients to Localize Computation in Neural Networks, 2024.\nURL\nhttps://arxiv.org/abs/2410.04332.\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse Autoencoders Find\nHighly Interpretable Features in Language Models, 2023. URL https://arxiv.org/abs/2309.08600.\nCarson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas\nSchiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez,\nand Evan Hubinger.\nSycophancy to Subterfuge: Investigating Reward-Tampering in Large Language\nModels, 2024. URL https://arxiv.org/abs/2406.10162.\nMucong Ding, Chenghao Deng, Jocelyn Choo, Zichu Wu, Aakriti Agrawal, Avi Schwarzschild, Tianyi\nZhou, Tom Goldstein, John Langford, Anima Anandkumar, and Furong Huang.\nEasy2Hard-Bench:\nStandardized Diﬃculty Labels for Proﬁling LLM Performance and Generalization,\n2024.\nURL\nhttps://arxiv.org/abs/2409.18433.\nScott Emmons, Caspar Oesterheld, Vincent Conitzer, and Stuart Russell. Observation Interference in Par-\ntially Observable Assistance Games, 2024. URL https://arxiv.org/abs/2412.17797.\nAlan Fern, Sriraam Natarajan, Kshitij Judah, and Prasad Tadepalli. A Decision-Theoretic Model of Assis-\ntance. J. Artif. Int. Res., 50(1):71–104, may 2014. ISSN 1076-9757.\nLukas Fluri, Leon Lang, Alessandro Abate, Patrick Forré, David Krueger, and Joar Skalse. The Perils of\nOptimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret, 2024. URL\nhttps://arxiv.org/abs/2406.15753.\nGoogle\nGemini\nTeam.\nGemini:\nA\nFamily\nof\nHighly\nCapable\nMultimodal\nModels.\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf, 2023.\nAccessed:\n2023-12-11.\nAaron Grattaﬁori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere,\n31\n\n\nBethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,\nChris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton\nFerrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic,\nFrancisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind\nThattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar,\nHu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evti-\nmov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geﬀert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet\nShah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu\nHuang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua\nJohnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heaﬁeld, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley\nChiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen,\nLiang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke\nde Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria\nTsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si,\nMitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev,\nNiladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Peng-\nwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,\nPuxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Sil-\nveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain\nSauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hos-\nseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang\nNie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon\nVandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Syd-\nney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias\nSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ra-\nmanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic,\nWeiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang\nWang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh\nGaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre\nCoudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain,\nAdam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay\nSharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo,\nAnam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton,\nAndrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arka-\nbandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James,\nBen Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing\nLiu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\nBritt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim,\nChao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia\nGao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine,\nDelia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Ed-\nward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan\nLe, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian,\nFilippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Flo-\nrez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi,\nZhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen\nZha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan,\nIbrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weiss-\nman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, JeﬀMarcus, JeﬀTang,\nJennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang,\n32\n\n\nKai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang,\nLailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu,\nLiron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus,\nMatan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan\nKeneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov,\nMikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Moham-\nmad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa,\nNayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Nor-\nman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh,\nPaul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag-\nina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, RaﬁAyub,\nRaghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah\nHogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh\nBondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh\nMahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng\nFeng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang,\nSinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve\nKehoe, Steve Satterﬁeld, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny\nVirk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo\nKoehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked,\nVarun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla,\nVlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen\nJiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao,\nYaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary\nDeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of\nModels, 2024. URL https://arxiv.org/abs/2407.21783.\nRyan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks,\nJohannes Treutlein,\nTim Belonax,\nJack Chen,\nDavid Duvenaud,\nAkbir Khan,\nJulian Michael,\nSören Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris,\nSamuel R. Bowman, and Evan Hubinger.\nAlignment faking in large language models, 2024.\nURL\nhttps://arxiv.org/abs/2412.14093.\nMelody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, An-\ndrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel,\nand Amelia Glaese. Deliberative Alignment: Reasoning Enables Safer Language Models, 2025. URL\nhttps://arxiv.org/abs/2412.16339.\nWes Gurnee and Max Tegmark. Language Models Represent Space and Time. In The Twelfth International\nConference on Learning Representations, 2024. URL https://openreview.net/forum?id=jE8xbmvFin.\nDylan Hadﬁeld-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. Cooperative Inverse Reinforcement\nLearning. arXiv e-prints, art. arXiv:1606.03137, June 2016. doi: 10.48550/arXiv.1606.03137.\nPeter Hase, Mohit Bansal, Peter Clark, and Sarah Wiegreﬀe.\nThe Unreasonable Eﬀectiveness of Easy\nTraining Data for Hard Tasks, 2024. URL https://arxiv.org/abs/2401.06751.\nStephane Hatgis-Kessell, W. Bradley Knox, Serena Booth, Scott Niekum, and Peter Stone.\nInﬂuencing\nHumans to Conform to Preference Models for RLHF, 2025. URL https://arxiv.org/abs/2501.06416.\nEvan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from Learned\nOptimization in Advanced Machine Learning Systems. arXiv e-prints, art. arXiv:1906.01820, June 2019.\ndoi: 10.48550/arXiv.1906.01820.\nGeoﬀrey\nIrving,\nPaul\nChristiano,\nand\nDario\nAmodei.\nAI\nsafety\nvia\ndebate,\n2018.\nURL\nhttps://arxiv.org/abs/1805.00899.\n33\n\n\nHong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying formalism\nfor reward learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances\nin Neural Information Processing Systems, volume 33, pp. 4415–4426. Curran Associates, Inc., 2020. URL\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/2f10c1578a0706e06b6d7db6f0b4a6af-Paper.pdf.\nZachary Kenton, Noah Y. Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian,\nRishabh Agarwal, David Lindner, Yunhao Tang, Noah D. Goodman, and Rohin Shah.\nOn scalable\noversight with weak LLMs judging strong LLMs, 2024. URL https://arxiv.org/abs/2407.04622.\nAkbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefen-\nstette, Samuel R. Bowman, Tim Rocktäschel, and Ethan Perez. Debating with More Persuasive LLMs\nLeads to More Truthful Answers, 2024. URL https://arxiv.org/abs/2402.06782.\nS.M. Lane. Categories for the Working Mathematician. Graduate Texts in Mathematics. Springer, 1998.\nISBN 9780387984032. URL https://books.google.nl/books?id=MXboNPdTv7QC.\nLeon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, and Scott Emmons. When Your AIs\nDeceive You: Challenges of Partial Observability in Reinforcement Learning from Human Feedback. In\nAdvances in Neural Information Processing Systems, 2024.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg.\nScalable agent\nalignment via reward modeling: a research direction, 2018. URL https://arxiv.org/abs/1811.07871.\nKevin Liu, Stephen Casper, Dylan Hadﬁeld-Menell, and Jacob Andreas.\nCognitive Dissonance:\nWhy\nDo Language Model Outputs Disagree with Internal Representations of Truthfulness?, 2023.\nURL\nhttps://arxiv.org/abs/2312.03729.\nMonte\nMacDiarmid,\nTimothy\nMaxwell,\nNicholas\nSchiefer,\nJesse\nMu,\nJared\nKaplan,\nDavid\nDuvenaud,\nSam\nBowman,\nAlex\nTamkin,\nEthan\nPerez,\nMrinank\nSharma,\nCarson\nDeni-\nson,\nand\nEvan\nHubinger.\nSimple\nprobes\ncan\ncatch\nsleeper\nagents,\n2024.\nURL\nhttps://www.anthropic.com/news/probes-catch-sleeper-agents.\nAlex Mallen and Nora Belrose. Balancing Label Quantity and Quality for Scalable Elicitation, 2024. URL\nhttps://arxiv.org/abs/2410.13215.\nSamuel Marks and Max Tegmark. The Geometry of Truth: Emergent Linear Structure in Large Language\nModel Representations of True/False Datasets, 2024. URL https://arxiv.org/abs/2310.06824.\nJulian Michael,\nSalsabila Mahdi,\nDavid Rein,\nJackson Petty,\nJulien Dirani,\nVishakh Padmaku-\nmar,\nand\nSamuel\nR.\nBowman.\nDebate\nHelps\nSupervise\nUnreliable\nExperts,\n2023.\nURL\nhttps://arxiv.org/abs/2311.08702.\nTomas Mikolov, Wen-tau Yih, and Geoﬀrey Zweig.\nLinguistic Regularities in Continuous Space Word\nRepresentations.\nIn Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoﬀ(eds.), Proceedings of\nthe 2013 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 746–751, Atlanta, Georgia, June 2013. Association for Computational\nLinguistics. URL https://aclanthology.org/N13-1090/.\nNeel Nanda, Andrew Lee, and Martin Wattenberg. Emergent Linear Representations in World Models of\nSelf-Supervised Sequence Models, 2023. URL https://arxiv.org/abs/2309.00941.\nOpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022. Accessed: 2024-02-06.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir\nBalaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, JeﬀBelgum, Irwan Bello,\nJake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoﬀ, Oleg Boiko, Madelaine\nBoyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai,\nRosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che\n34\n\n\nChang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester\nCho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,\nThomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoﬀet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman,\nJuston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel\nGoh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, JeﬀHarris, Yuchen He, Mike Heaton, Johannes\nHeidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu,\nShengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\nKim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo,\nMichael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim,\nMolly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju,\nKim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco,\nEvan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro\nNakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe,\nJakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish,\nEmy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong,\nTolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya\nRamesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,\nNick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\nPranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Ben-\njamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\nston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\nCJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoﬀ, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, JeﬀWu, Michael Wu,\nKai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,\nMarvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4\nTechnical Report, 2024. URL https://arxiv.org/abs/2303.08774.\nLong Ouyang, JeﬀWu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback, 2022.\nJoon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris,\nRobb Willer, Percy Liang, and Michael S. Bernstein. Generative Agent Simulations of 1,000 People, 2024a.\nURL https://arxiv.org/abs/2411.10109.\nKiho Park, Yo Joong Choe, and Victor Veitch. The Linear Representation Hypothesis and the Geometry of\nLarge Language Models, 2024b. URL https://arxiv.org/abs/2311.03658.\nPeter S Park, Simon Goldstein, Aidan O’Gara, Michael Chen, and Dan Hendrycks. Ai deception: A survey\nof examples, risks, and potential solutions. Patterns, 5(5), 2024c.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn.\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model. arxiv e-prints, 2023.\n35\n\n\nFabien\nRoger.\nCoup\nprobes:\nCatching\ncatastrophes\nwith\nprobes\ntrained\noﬀ-policy.\nhttps://www.alignmentforum.org/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-\n2023. Accessed: 2023-04-25.\nWilliam Saunders, Catherine Yeh, JeﬀWu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike.\nSelf-critiquing models for assisting human evaluators, 2022. URL https://arxiv.org/abs/2206.05802.\nJérémy Scheurer, Mikita Balesni, and Marius Hobbhahn. Technical Report: Large Language Models can\nStrategically Deceive their Users when Put Under Pressure. arxiv e-prints, 2023.\nRohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, Dmitrii Krasheninnikov, Lawrence Chan, Michael D\nDennis, Pieter Abbeel, Anca Dragan, and Stuart Russell. Beneﬁts of Assistance over Reward Learning,\n2021. URL https://openreview.net/forum?id=DFIoGDZejIB.\nNisan Stiennon, Long Ouyang, JeﬀWu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano.\nLearning to summarize from human feedback, 2022.\nURL\nhttps://arxiv.org/abs/2009.01325.\nZhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang\nGan.\nEasy-to-Hard Generalization:\nScalable Alignment Beyond Human Supervision, 2024.\nURL\nhttps://arxiv.org/abs/2403.09472.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-\nrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh\nKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subra-\nmanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat\nModels. arxiv e-prints, 2023.\nAlexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini,\nand Monte MacDiarmid.\nSteering Language Models With Activation Engineering, 2024.\nURL\nhttps://arxiv.org/abs/2308.10248.\nElise van der Pol,\nDaniel E. Worrall,\nHerke van Hoof,\nFrans A. Oliehoek,\nand Max Welling.\nMDP\nHomomorphic\nNetworks:\nGroup\nSymmetries\nin\nReinforcement\nLearning,\n2021.\nURL\nhttps://arxiv.org/abs/2006.16908.\nZihao\nWang,\nLin\nGui,\nJeﬀrey\nNegrea,\nand\nVictor\nVeitch.\nConcept\nAlgebra\nfor\n(Score-\nBased)\nText-Controlled\nGenerative\nModels.\nIn\nA.\nOh,\nT.\nNaumann,\nA.\nGloberson,\nK.\nSaenko,\nM.\nHardt,\nand\nS.\nLevine\n(eds.),\nAdvances\nin\nNeural\nInformation\nPro-\ncessing\nSystems,\nvolume\n36,\npp.\n35331–35349.\nCurran\nAssociates,\nInc.,\n2023.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/6f125214c86439d107ccb58e549e828f-Paper-Con\nFrancis Rhys Ward, Francesco Belardinelli, Francesca Toni, and Tom Everitt. Honesty Is the Best Policy:\nDeﬁning and Mitigating AI Deception. arxiv e-prints, 2023.\nJiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Bow-\nman, He He, and Shi Feng.\nLanguage Models Learn to Mislead Humans via RLHF, 2024.\nURL\nhttps://arxiv.org/abs/2409.12822.\nMarcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, and Anca Dra-\ngan. On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback, 2024. URL\nhttps://arxiv.org/abs/2411.02306.\n36\n\n\nTan Zhi-Xuan, Lance Ying, Vikash Mansinghka, and Joshua B. Tenenbaum.\nPragmatic Instruc-\ntion Following and Goal Assistance via Cooperative Language-Guided Inverse Planning, 2024.\nURL\nhttps://arxiv.org/abs/2402.17930.\nDaniel M. Ziegler, Nisan Stiennon, Jeﬀrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano,\nand Geoﬀrey Irving. Fine-Tuning Language Models from Human Preferences, 2020.\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan\nWang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and\nDan Hendrycks. Representation Engineering: A Top-Down Approach to AI Transparency, 2023. URL\nhttps://arxiv.org/abs/2310.01405.\nAndy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang,\nZico Kolter, Matt Fredrikson, and Dan Hendrycks. Improving Alignment and Robustness with Circuit\nBreakers, 2024. URL https://arxiv.org/abs/2406.04313.\n37\n\n\nAppendices\nIn the appendices, we present mathematical details and content that goes beyond the main text. Appendix A\nlists preliminary results on linear algebra together with their proofs. In Appendix B, we present a theory of\nbalanced belief models and the ambiguity for feedback that is given by binary choices between observations.\nThis complements the core theory from Sections 2 and 3, where the feedback is given by an observation\nreturn function. In Appendix C, we complement Section 3 by showing that human belief models and their\nmorphisms form a category, and we construct a variety of natural models composing a diagram. Appendix D\ncontains further mathematical details for the example in Section 3.3 on symmetry-invariant belief models\nand reward functions. Finally, Appendix E interprets some of the related work from Section 4.2 in our\nframework.\nA\nPreliminary results on linear algebra\nFor general notation and conventions on linear algebra, see Section 2.1.\nA.1\nDiﬀerent representations of linear functions\nLet X, Y be two sets. Then by Lin(RX, RY ) we denote the set of all linear maps F : RX →RY . By\nMaps(Y, RX), we denote the set of all (simple) maps, or functions, f : Y →RX. Intuitively, these encode\nthe same information: A linear function F, when represented as a matrix, is a collection of rows indexed\nby y ∈Y , which are “picked out” by a function f : Y →RX. This correspondence is made precise in the\nfollowing proposition, which is a “transposed” version of the classical statement that linear functions on\nvector spaces correspond to functions on a basis:\nProposition A.1. Deﬁne the two functions\nMaps(Y, RX)\nLin(RX, RY )\nlin\nmap\nas follows: For f ∈Maps(Y, RX), v ∈RX and y ∈Y , we deﬁne\nh\u0002\nlin(f)\n\u0003\n(v)\ni\n(y) := ⟨f(y), v⟩.\nFor F ∈Lin(RX, RY ), y ∈Y and x ∈X, we deﬁne\nh\u0002\nmap(F)\n\u0003\n(y)\ni\n(x) := Fyx.\nThen lin(f) has matrix elements\nlin(f)yx =\n\u0002\nf(y)\n\u0003\n(x)\nfor x ∈X and y ∈Y . Furthermore, lin and map are mutually inverse bijections.\nProof. First, note that for each f ∈Maps(Y, RX), lin(f) : RX →RY is indeed a linear function since the\nscalar product is linear in the second component. Its matrix elements are given by\nlin(f)yx =\nh\u0002\nlin(f)\n\u0003\n(ex)\ni\n(y) = ⟨f(y), ex⟩=\n\u0002\nf(y)\n\u0003\n(x).\nNow we show that lin and map are mutually inverse bijections, i.e., lin ◦map = idLin(RX,RY ) and map ◦lin =\nidMaps(Y,RX). Indeed, for F ∈Lin(RX, RY ), x ∈X, and y ∈Y , we have\n\u0002\n(lin ◦map)(F)\n\u0003\nyx =\n\u0002\nlin(map(F))\n\u0003\nyx\n38\n\n\n=\nh\u0002\nlin(map(F))\n\u0003\n(ex)\ni\n(y)\n=\nD\u0002\nmap(F)\n\u0003\n(y), ex\nE\n=\nh\u0002\nmap(F)\n\u0003\n(y)\ni\n(x)\n= Fyx\nSince linear functions are fully characterized by their matrix elements, this shows (lin ◦map)(F) = F, and\nthus lin ◦map is the identity.\nFor the other direction, for f ∈Maps(Y, RX), y ∈Y , and x ∈X, we have\nh\u0002\n(map ◦lin)(f)\n\u0003\n(y)\ni\n(x) =\nh\u0002\nmap(lin(f))\n\u0003\n(y)\ni\n(x)\n= lin(f)yx\n=\n\u0002\nf(y)\n\u0003\n(x).\nThis shows that (map ◦lin)(f) = f, and so map ◦lin is also the identity.\nProposition A.2. Let X, b\nX, Y be sets and F : RX →RY , bF : Rb\nX →RY , and Φ : RX →Rb\nX be linear\nfunctions. Let f = map(F) : Y →RX and bf = map( bF ) : Y →Rb\nX be the functions corresponding to F and\nbF by Proposition A.1. Let ΦT : Rb\nX →RX be the transpose of Φ, with matrix elements ΦT\nxbx = Φbxx. Then\nF = bF ◦Φ if and only if f = ΦT ◦bf:\nRb\nX\nRb\nX\n⇐⇒\nRX\nRY\nRX\nY\nb\nF\nΦT\nΦ\nF\nf\nb\nf\nProof. We have\nF = bF ◦Φ\n⇐⇒\n∀y ∈Y, x ∈X : Fyx = ( bF ◦Φ)yx =\nX\nbx∈b\nX\nbFybxΦbxx\n⇐⇒\n∀y ∈Y, x ∈X :\n\u0002\nf(y)\n\u0003\n(x) =\nX\nbx∈b\nX\nΦT\nxbx\n\u0002 bf(y)\n\u0003\n(bx) =\n\nΦT\nx , bf(y)\n\u000b\n=\nh\nΦT \u0000 bf(y)\n\u0001i\n(x) =\n\u0002\n(ΦT ◦bf)(y)\n\u0003\n(x)\n⇐⇒\nf = ΦT ◦bf.\nThat was to show.\nA.2\nProperties of kernels and images of linear functions\nThe following two propositions list basic and well-known properties of kernels and images that we use in the\npaper:\nProposition A.3. Let A : V →U and B : V →W be linear functions. Then the following statements are\nequivalent:\n1. ker(A) ⊆ker(B);\n39\n\n\n2. There exists a linear function C : U →W with C ◦A = B:\nV\nW\nU\nA\nB\nC\nProof. Clearly, the second claim implies the ﬁrst. So now assume 1. Let {u1, . . . , um} be a basis for im(A)\nand complement it to a basis {u1, . . . , un} for all of U, where n ≥m. For each i ∈{1, . . . , m}, let vi ∈V be\nany element with A(vi) = ui. Deﬁne C(ui) := B(vi) for i ∈{1, . . . , m} and C(ui) = 0 if i > m. Linearly\nextend C to a linear function C : U →W.\nTo show that C ◦A = B, let v ∈V be arbitrary. Then A(v) ∈im(A), and thus there exist coeﬃcients λi ∈R\nfor i ∈{1, . . . , m} with\nA(v) =\nm\nX\ni=1\nλiui.\nNote that\nA\n \nv −\nm\nX\ni=1\nλivi\n!\n= A(v) −\nm\nX\ni=1\nλiA(vi) = A(v) −\nm\nX\ni=1\nλiui = 0\nThus, v −Pm\ni=1 λivi ∈ker(A) ⊆ker(B). Consequently, we obtain\nB(v) = B\n m\nX\ni=1\nλivi\n!\n=\nm\nX\ni=1\nλiB(vi) =\nm\nX\ni=1\nλiC(ui)\n= C\n m\nX\ni=1\nλiui\n!\n= C\n\u0000A(v)\n\u0001\n= (C ◦A)(v).\nThis shows C ◦A = B, and thus the claim.\nProposition A.4. Let A : U →W and B : V →W be linear functions. Then the following statements are\nequivalent:\n1. im(A) ⊆im(B).\n2. There exists a “lift”, i.e., a linear map C : U →V with B ◦C = A:\nV\nU\nW.\nB\nA\nC\nProof. It can easily be checked that the second claim implies the ﬁrst. For the other direction, let {u1, . . . , un}\nbe a basis for U. For i ∈{1, . . ., n}, choose vi ∈V with B(vi) = A(ui), which exists since im(A) ⊆im(B).\nDeﬁne C : U →V as the unique linear function with C(ui) = vi for i ∈{1, . . ., n}. We obtain\nA(ui) = B(vi) = B\n\u0000C(ui)\n\u0001\n= (B ◦C)(ui).\nSince linear functions are determined on a basis, it follows A = B ◦C, proving the claim.\n40\n\n\nB\nBalanced human belief models and choices\nIn this appendix, we present the core theory from Section 2 and Section 3 for the case that the feedback is\nin the form of choices instead of the observation return function GO. To still get a useful theory, we will\nthen need to assume our human belief models to be balanced to ensure that constants are “propagated”\nappropriately through the model. In this whole appendix, we ﬁx an MDP with a set of trajectories Ξ and\nobservations O.\nB.1\nBalanced belief models\nDeﬁnition B.1 (Row-constant). Let X, and Y be sets. We call a linear function A: RX →RY row-\nconstant if for all y, y′ ∈Y we have 0 ̸= P\nx∈X Xyx = P\nx∈X Xy′x.\nDeﬁnition B.2 (Balanced). Let M = (Ω, Λ, E, V) be a human belief model. We call M balanced if Λ and\nE are row-constant, and if V ⊆RΩcontains all constant functions.\nΛ and E being row-constant means that the corresponding functions λ : Ξ →RΩand ǫ : O →RΩ(cf. Propo-\nsition A.1) map to vectors of feature strengths with a constant total weighting, as is for example the case for\nprobability distributions. That V contains all constant functions is often naturally the case. To demonstrate\nthis in a simple example we ﬁrst prove the following lemma:\nLemma B.3. Let X, Y, Z be sets and A : RX →RY , B : RY →RZ be row-constant linear functions. Then\nthe composition B ◦A : RX →RZ is also row-constant.\nProof. Let a, b be the row-sums of A and B, respectively. Then for all z ∈Z, we obtain\nX\nx∈X\n(B ◦A)zx =\nX\nx∈X\nX\ny∈Y\nBzyAyx =\nX\ny∈Y\nBzy\nX\nx∈X\nAyx = b · a ̸= 0,\nwhich shows the claim.\nExample B.4. We continue Example 2.4 and show that the model is balanced. Note that for all ξ ∈Ξ, we\nhave\nX\n(s,a,s′)∈S×A×S\nΓξ,(s,a,s′) =\nX\n(s,a,s′)∈S×A×S\nT −1\nX\nt=0\nγtδ(s,a,s′)(st, at, st+1)\n=\nT −1\nX\nt=0\nγt\nX\n(s,a,s′)∈S×A×S\nδ(s,a,s′)(st, at, st+1)\n=\nT −1\nX\nt=0\nγt\n̸= 0.8\nThus, Γ is row-constant. B is also row-constant since all rows are probability distributions, and so Lemma B.3\nimplies that also E = B ◦Γ is row-constant. V = RS×A×S also clearly contains all constant functions.\nOverall, this means the model (S × A × S, Γ, B ◦Γ, RS×A×S) is balanced.\nFor more examples of balanced human belief models, see Appendix C.2.\nB.2\nThe ambiguity for balanced belief models and choices\nLet σ : R →(0, 1) be any known bijective function with σ(r) + σ(−r) = 1 for all r ∈R (e.g., the sigmoid\nfunction). For o, o′ ∈O, we deﬁne the probability that a human with feature belief function E : RΩ→RO\nand reward object ˜RΩ∈RΩprefers o over o′ by\nP\n˜\nRΩ\nE\n(o ≻o′) := σ\n\u0010\u0002\nE( ˜RΩ)\n\u0003\n(o) −\n\u0002\nE( ˜RΩ)\n\u0003\n(o′)\n\u0011\n.\n(18)\n41\n\n\nFor the rest of the section, we ﬁx a human belief model M = (Ω, Λ, E, V). Furthermore, we ﬁx the implicit\ntrue reward object RΩ∈V together with the return function G = Λ(RΩ), the observation return function\nGO = E(RΩ) and the choice probability function PO := P RΩ\nE\nthat serves as our operationalization of\n“feedback”.\nWe use the following adaptation of Deﬁnition 2.6:\nDeﬁnition B.5. We deﬁne the set of return functions that are feedback-compatible with PO as\nFCM(PO) :=\nn\n˜G ∈RΞ \f\f ∃˜RΩ∈V : P\n˜\nRΩ\nE\n= PO and Λ( ˜RΩ) = ˜G\no\n.\nWe deﬁne the ambiguity left in the return function G after the choice probability function PO is known by\nAmbM(G, PO) :=\nn\nG′ ∈RΞ \f\f G′ = ˜G −G for ˜G ∈FCM(PO)\no\n.\nClearly, we have\nFCM(GO) = G + AmbM(G, PO).\nRecall the ambiguity AmbM(G, GO) deﬁned in Deﬁnition 2.6. We obtain:\nProposition B.6. Assume M = (Ω, Λ, E, V) is balanced. Let 1 ∈RΞ denote the function that is constant\n1. Then:\nAmbM(G, PO) = AmbM(G, GO) + R⟨1⟩= Λ\n\u0000ker(E) ∩V\n\u0001\n+ R⟨1⟩.\nProof. The second equality follows from Proposition 2.7, so we are left with proving the ﬁrst. By abuse of\nnotation, we will write 1 for the three functions that are constant 1 on Ξ, O, or Ω.\nLet G′ ∈AmbM(G, PO). Then G′ = Λ( ˜RΩ)−G for ˜RΩ∈V with P\n˜\nRΩ\nE\n= PO. The latter means the following\nfor all o, o′ ∈O:\nσ\n\u0010\u0002\nE( ˜RΩ)\n\u0003\n(o) −\n\u0002\nE( ˜RΩ)\n\u0003\n(o′)\n\u0011\n= σ\n\u0010\u0002\nE(RΩ)\n\u0003\n(o) −\n\u0002\nE(RΩ)\n\u0003\n(o′)\n\u0011\n.\nSince σ is invertible, we obtain\n\u0002\nE( ˜RΩ)\n\u0003\n(o) −\n\u0002\nE( ˜RΩ)\n\u0003\n(o′) =\n\u0002\nE(RΩ)\n\u0003\n(o) −\n\u0002\nE(RΩ)\n\u0003\n(o′)\nFix any o′ ∈O and set cO :=\n\u0002\nE( ˜RΩ)\n\u0003\n(o′) −\n\u0002\nE(RΩ)\n\u0003\n(o′). Then for all o ∈O, we have\n\u0002\nE( ˜RΩ)\n\u0003\n(o) =\n\u0002\nE(RΩ)\n\u0003\n(o) + cO.\nOr, equivalently:\nE( ˜RΩ) = E(RΩ) + cO · 1 = GO + cO · 1.\nSince E is row-constant, there exists a cΩ∈R with E(cΩ· 1) = cO · 1. This implies\nE( ˜RΩ−cΩ· 1) = GO.\nWe also have ˜RΩ−cΩ· 1 ∈V since V contains all constant functions. Furthermore, Λ(cΩ· 1) = cΞ · 1 for\nanother constant cΞ ∈R since Λ is row-constant. Overall, we thus obtain\nG′ = Λ( ˜RΩ) −G = Λ( ˜RΩ−cΩ· 1) −G + cΞ · 1 ∈AmbM(G, GO) + R⟨1⟩.\nFor the other direction, let G′ ∈AmbM(G, GO) + R⟨1⟩. Then G′ = ˜G −G + cΞ · 1 for ˜G = Λ( ˜RΩ) with\n˜RΩ∈V with E( ˜RΩ) = GO. We have\n˜G + cΞ · 1 = Λ( ˜RΩ+ cΩ· 1)\nfor a constant cΩ∈R with Λ(cΩ·1) = cΞ·1. Since V contains all constant functions, we have ˜RΩ+cΩ·1 ∈V.\nWe also have\nP\n˜\nRΩ+cΩ·1\nE\n= P\n˜\nRΩ\nE\n= PO\nsince the constant gets cancelled out in the deﬁnition of the choice probabilities, and since E( ˜RΩ) = GO. All\nof this implies\nG′ =\n\u0000 ˜G + cΞ · 1\n\u0001\n−G ∈AmbM(G, PO).\nThat proves the claim.\n42\n\n\nNote that for the purpose of policy optimization it is not an issue that the ambiguity has an “irreducible”\nconstant term since this does not change the ordering of policies under the policy evaluation function J(π) =\nEξ∈P π(·)[G(ξ)].\nRemark B.7. In light of the previous proposition, it turns out that the ambiguity does not depend on the\ntrue return function and choice probabilities, and we can thus write it as AmbM\nP = AmbM(G, PO). The “P”\nis added to distinguish from the ambiguity AmbM = AmbM(G, GO) that we study in the main paper.\nUsing this result, we also obtain a version of Theorem 3.2, with the ambiguity replaced by the one we use in\nthis appendix:\nTheorem B.8. Let M = (Ω, Λ, E, V) and c\nM = (bΩ, bΛ, bE, bV) be two balanced human belief models and\nassume that c\nM covers M. We think of M as the “true” human belief model with reward object RΩ∈V and\ncorresponding return function G = Λ(RΩ) and choice probability function PO = P RΩ\nE\n. Then we have:\n1. AmbM\nP ⊆Amb b\nM\nP .\n2. If M also covers c\nM, then AmbM\nP = Amb b\nM\nP .\n3. There is an Rb\nΩ∈bV with P\nRb\nΩ\nb\nE\n= PO and bΛ(Rb\nΩ) = G.\n4. Assume c\nM is complete. Then every reward object ˜Rb\nΩ∈bV with P\n˜\nRb\nΩ\nb\nE\n= PO also satisﬁes bΛ( ˜Rb\nΩ) =\nG + cΞ · 1 for a constant cΞ ∈R.\nProof. Statements 1 and 2 follow from the ambiguity characterization in Proposition B.6 and the analogous\nstatements in Theorem 3.2. Statements 3 and 4 can be proved with similar arguments as the corresponding\nstatements in Theorem 3.2.\nC\nA diagram in the category of human belief models\nLet us consider an MDP together with a ﬁxed set of trajectories Ξ and observations O. Then in Deﬁnition 2.2,\nwe deﬁned the notion of a human belief model M = (Ω, Λ, E, V). In Deﬁnition 3.3, we then introduced the\nnotion of a morphism Φ : M →c\nM between human belief models, which is deﬁned as a linear function\nΦ : RΩ→Rb\nΩsuch that Φ(V) ⊆V′, bΛ ◦Φ|V = Λ and bE ◦Φ|V = E. This notion turned out important since it\nis equivalent to model covering (Deﬁnition 3.1), which implies that the covering model can be used for the\nreturn function inference from human feedback (Theorem 3.2), especially if its ambiguity disappears.\nBelief models for ﬁxed sets of trajectories Ξ and observations O, together with their morphisms, form a\ncategory (Lane, 1998), meaning that they satisfy the following simple properties:\n• Composition: Assume M1, M2, M3 are three human belief models and Φ : M1 →M2, Φ′ :\nM2 →M3 morphisms between them. Then also the composition Φ′ ◦Φ : M1 →M3 is a morphism.\n• Identities: For any human belief model M = (Ω, Λ, E, V), the identity idRΩ: M →M is a\nmorphism.\n• Associativity: (Φ′′ ◦Φ′) ◦Φ = Φ′′ ◦(Φ′ ◦Φ) for any three morphisms that can be composed in the\nspeciﬁed order.\nAll of these properties can be trivially checked, and so human belief models and their morphisms indeed\nform a category.\nIn this appendix, we want to write down a simple commutative diagram of morphisms in this category. Here,\na diagram means a graph of human belief models and morphisms between them. For this to be commutative\nmeans that any pathway from one human belief model to another is the same morphism.\nWe prepare\n43\n\n\nthis in Appendix C.1 by writing down all linear functions from which the functions Λ, E, and Φ will be\nconstructed. In Appendix C.2 we then specify the resulting human belief models and brieﬂy consider their\nproperties. In Appendix C.3 we interpret the matrix elements that appear in the feature belief functions of\nall models. Finally, in Appendix C.4, we write down the resulting commutative diagram and the resulting\nrelations for the ambiguities.\nC.1\nPreparing the models\nWe build on Example 2.4.\nThe idea is that we consider reward objects at four diﬀerent levels: Return\nfunctions, classical reward functions, and return- and reward functions of abstractions of trajectories and\ntransitions that the human might care about. By modeling the human as having features at all four of these\ndiﬀerent levels, we can create a multitude of human belief models.\nLet B : RΞ →RO be the matrix corresponding to a trajectory-belief function b : O →∆(Ξ) ⊆RΞ\nvia Proposition A.1. Let Γ : RS×A×S →RΞ be the linear function mapping reward functions to their\ncorresponding return functions.\nLet F be a set of “abstractions of transitions” and h : S × A × S →F a function mapping each transition\nto its abstraction. Write reward objects over abstractions as RF ∈RF. Then we obtain the induced map\nh∗: RF →RS×A×S,\nRF 7→RF ◦h.\nh∗(RF) measures a transition (s, a, s′) by evaluating RF at the transition’s abstraction: RF(h(s, a, s′)).\nThus, h∗(RF) is guaranteed to give the same reward to transitions with the same abstraction.\nWe can then also consider the space of abstraction sequences FT together with the function hT : Ξ →FT\ngiven by\nhT (s0, a0, . . . , sT −1, aT −1, sT ) :=\n\u0000h(s0, a0, s1), . . . , h(sT −1, aT −1, sT )\n\u0001\n.\nWrite return functions over abstraction sequences as GF T ∈RF T . hT then gives rise to the dual function\nhT ∗: RF T →RΞ,\nGF T 7→GF T ◦hT .\nThus, hT ∗(GF T ) evaluates a trajectory by evaluating the sequence of abstractions using GF T . As before,\ntwo trajectories with the same sequences of abstractions then obtain the same return.\nRecall the function Γ : RS×A×S →RΞ mapping a reward function to the corresponding return function.\nThen we obtain an analogous function for reward objects on abstractions:\nΓF : RF →RF T ,\n\u0002\nΓF(RF)\n\u0003\n(f1, . . . , fT ) :=\nT −1\nX\nt=0\nγtRF(ft).\nProposition C.1. The diagram\nRS×A×S\nRΞ\nRO\nRF\nRF T\nΓ\nB\nΓF\nh∗\nhT ∗\n(19)\nof linear functions commutes, meaning that all pathways with the same start and end are the same function.\nProof. We have\n\u0002\n(Γ ◦h∗)(RF)\n\u0003\n(s0, a0, . . . , aT −1, sT ) =\nh\nΓ\n\u0000h∗(RF)\n\u0001i\n(s0, a0, . . . , aT −1, sT )\n44\n\n\n=\nT −1\nX\nt=0\nγt\u0002\nh∗(RF)\n\u0003\n(st, at, st+1)\n=\nT −1\nX\nt=0\nγtRF\n\u0000h(st, at, st+1)\n\u0001\n=\n\u0002\nΓF(RF)\n\u0003\u0000h(s0, a0, s1), . . . , h(sT −1, aT −1, sT )\n\u0001\n=\n\u0002\nΓF(RF)\n\u0003\u0000hT (s0, a0, . . . , sT −1, aT −1, sT )\n\u0001\n=\nh\nhT ∗\u0000ΓF(RF)\n\u0001i\n(s0, a0, . . . , aT −1, sT )\n=\n\u0002\n(hT ∗◦ΓF)(RF)\n\u0003\n(s0, a0, . . . , aT −1, sT ).\nThis shows Γ ◦h∗= hT ∗◦ΓF. Consequently, the diagram commutes.\nThe idea will be that the rows of B, B ◦Γ, B ◦hT ∗and B ◦Γ ◦h∗= B ◦hT ∗◦ΓF all correspond (via Proposi-\ntion A.1) to feature beliefs over trajectories, transitions, trajectory abstractions, and transition abstractions,\nrespectively. We explain this interpretation in detail in Appendix C.3. All of these functions “factorize” over\ntrajectories, but of course this need not be the case in reality: A realistic human could have an intrinsic belief\nover state transitions, sequences of abstractions, or single abstractions, without this belief “factorizing” in a\nrational way over state sequences.\nThus, let the following be an extended version of the diagram from Proposition C.1, with new linear functions\nB′, B′′, B′′′. This extension is now not necessarily commutative anymore:\nRS×A×S\nRΞ\nRO\nRF\nRF T\nΓ\nB′\nB\nΓF\nB′′′\nh∗\nhT ∗\nB′′\n(20)\nTo interpret B′, B′′, B′′′ on similar grounds as B, it makes sense to assume that they are row-constant\n(Deﬁnition B.1), but otherwise they can be arbitrary.\nC.2\nVarious human belief models\nWe take the previous diagrams as the starting point to construct human belief models Remember that a belief\nmodel is of the form M = (Ω, Λ, E, V). The sets Ξ, S × A × S, F, and FT are four diﬀerent possible feature\nsets Ω. Λ is given by a composition of linear functions that maps to RΞ. E is given by a composition mapping\nto RO. The space V is either given by the full vector space RΩ, or by images of functions mapping to RΩ.\nOverall, using the diagram from Proposition C.1, this leads to the following 9 models, with the superscript\ndenoting the feature space, and the subscript indicating where the valid reward objects “originate from”:\nMF\nF :=\n\u0000F, Γ ◦h∗, B ◦Γ ◦h∗, RF\u0001\nMS×A×S\nS×A×S :=\n\u0000S × A × S, Γ, B ◦Γ, RS×A×S\u0001\nMS×A×S\nF\n:=\n\u0000S × A × S, Γ, B ◦Γ, im(h∗)\n\u0001\nMF T\nF T :=\n\u0000FT , hT ∗, B ◦hT ∗, RF T \u0001\nMF T\nF\n:=\n\u0000FT , hT ∗, B ◦hT ∗, im(ΓF)\n\u0001\n45\n\n\nMΞ\nΞ :=\n\u0000Ξ, idRΞ, B, RΞ\u0001\nMΞ\nS×A×S :=\n\u0000Ξ, idRΞ, B, im(Γ)\n\u0001\nMΞ\nF T :=\n\u0000Ξ, idRΞ, B, im(hT ∗)\n\u0001\nMΞ\nF :=\n\u0000Ξ, idRΞ, B, im(Γ ◦h∗)\n\u0001\nFor example, MS×A×S\nS×A×S is the model from Example 2.4; MS×A×S\nF\nis the same model, but with valid reward\nfunctions restricted to those that only “care about” abstractions; MΞ\nΞ is a model in which the features are\ngiven by full trajectories, and there are no restrictions on the valid return functions; etc.\nNow, B′ naturally gives rise to the following three models, which we color diﬀerently to distinguish them\nmore easily:\nM′F\nF =\n\u0000F, Γ ◦h∗, B′ ◦h∗, RF\u0001\nM′S×A×S\nF\n=\n\u0000S × A × S, Γ, B′, im(h∗)\n\u0001\nM′S×A×S\nS×A×S =\n\u0000S × A × S, Γ, B′, RS×A×S\u0001\n.\nSimilarly, B′′ gives rise to the following three models:\nM′′F\nF\n=\n\u0000F, hT ∗◦ΓF, B′′ ◦ΓF, RF\u0001\nM′′F T\nF\n=\n\u0000FT , hT ∗, B′′, im(ΓF)\n\u0001\nM′′F T\nF T\n=\n\u0000FT , hT ∗, B′′, RF T \u0001\nFinally, B′′′ gives rise to a single model:\nM′′′F\nF\n=\n\u0000F, Γ ◦h∗, B′′′, RF\u0001\nNote\nthat\nall\ncomponent\nlinear\nfunctions\nappearing\nin\nany\nof\nthese\nmodels\n(identities,\nh∗, Γ, ΓF, hT ∗, B, . . . , B′′′) are row-constant.\nBy Lemma B.3 then, also all compositions are row-\nconstant, which then implies that all 16 models are balanced, as deﬁned in Deﬁnition B.2.\nThe ﬁrst 9\nmodels are also faithful (Deﬁnition 2.13) since all feature belief functions factorize as in Proposition 2.14,\nwith Y given by B in all cases. The other 7 models will typically not be faithful.\nC.3\nInterpreting the matrix elements\nWe now interpret the diﬀerent feature belief functions that appeared in the nine ﬁrst models of the previous\nsubsection. Recall that the linear function B : RΞ →RO “comes from” a function b : O →∆(Ξ) ⊆RΞ.\nThus, all matrix elements Boξ can be interpreted as a probability\n\u0002\nb(o)\n\u0003\n(ξ) for the trajectory ξ when viewing\nobservation o. We now explain similar interpretations for the matrix elements of all the other feature belief\nfunctions:\nB ◦Γ: It contains matrix elements\n(B ◦Γ)o,(s,a,s′) =\nX\nξ\n\u0002\nb(o)\n\u0003\n(ξ)\nT −1\nX\nt=0\nγtδ(s,a,s′)(st, at, st+1),\nthe expected discounted number of times the transition (s, a, s′) is present in the trajectory.\nB ◦hT ∗: Write f for (f1, . . . , fT ). Then this contains matrix elements\n(B ◦hT ∗)of =\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ)hT ∗\nξf\n=\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ)δf(hT (ξ))\n46\n\n\n=\nX\nξ : hT (ξ)=f\n\u0002\nb(o)\n\u0003\n(ξ)\n=\n\u0002\nb(o)\n\u0003\u0000(hT )−1(f)\n\u0001\n=\n\u0002\nb(o)hT\n\u0003\n(f).\nIn the second to last step, we view b(o) as a probability distribution that, when evaluated on a set, evaluates to\nthe sum of the probabilities of the set’s elements. In the last step, we use the deﬁnition of the distributional\nlaw of a random variable X with respect to a probability distribution P on the sample space: PX(x) =\nP(X−1(x)).\nThe result is the believed probability, after observing o, of a trajectory with sequence of\nabstractions f.\nFinally, we look at the matrix B ◦Γ ◦h∗(for which we give two slightly diﬀerent formulas): The matrix\nelements are given as\n(B ◦Γ ◦h∗)of =\nX\n(s,a,s′)∈S×A×S\n(B ◦Γ)o,(s,a,s′) · h∗\n(s,a,s′),f\n=\nX\n(s,a,s′): h(s,a,s′)=f\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ)\nT −1\nX\nt=0\nγtδ(s,a,s′)(st, at, st+1)\n=\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ)\nT −1\nX\nt=0\nγt\nX\n(s,a,s′): h(s,a,s′)=f\nδ(s,a,s′)(st, at, st+1)\n=\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ)\nT −1\nX\nt=0\nγtδf(h(st, at, st+1)).\nThis is the expected discounted number of times that one encounters the abstraction f. Using that Γ ◦h∗=\nhT ∗◦ΓF by Proposition C.1, we can also write this as\n\u0000B ◦hT ∗◦ΓF\n\u0001\nof =\nX\nf∈F T\n(B ◦hT ∗)of · (ΓF)ff\n=\nX\nf∈F T\n\u0002\nb(o)hT\n\u0003\n(f)\nT\nX\nt=0\nγtδf(ft).\nThis can also be described as the expected discounted number of times that one encounters the abstraction\nf.\nC.4\nThe resulting commutative diagram\nBuilding on Appendix C.2, the following is a commutative diagram of belief models and model morphisms,\nwith four diﬀerently colored “connected components”; one can sometimes use Proposition C.1 in the process\nof showing that every linear function in the diagram is a morphism of belief models, and that the ﬁnal\ndiagram commutes:\nM′S×A×S\nS×A×S\nMS×A×S\nS×A×S\nMΞ\nS×A×S\nMΞ\nΞ\nM′S×A×S\nF\nMS×A×S\nF\nMΞ\nF\nMΞ\nF T\nM′F\nF\nMF\nF\nMF T\nF\nMF T\nF T\nM′′′F\nF\nM′′F\nF\nM′′F T\nF\nM′′F T\nF T\nΓ\nidRΞ\nidRS×A×S\nidRS×A×S\nΓ\nidRΞ\nidRΞ\nidRΞ\nh∗\nh∗\nΓF\nhT ∗\nidRFT\nhT ∗\nΓF\nidRFT\n47\n\n\nFor example, the following diagram visualizes the fact that h∗: MF\nF →MS×A×S\nF\nis a morphism:\nRΞ\nRF\n⊆\nRF\nRS×A×S\n⊆\nim(h∗)\nRO.\nΓ ◦h∗\nh∗\nB ◦Γ ◦h∗\nΓ\nB ◦Γ\nThis gives rise to the following diagram of ambiguities:\nΓ(ker(B′))\nΓ\n\u0000ker(B ◦Γ)\n\u0001\n=\nker(B) ∩im(Γ)\n⊆\nker(B)\nΓ\n\u0000ker(B′) ∩im(h∗)\n\u0001\nΓ\n\u0000ker(B ◦Γ) ∩im(h∗)\n\u0001\n=\nker(B) ∩im(Γ ◦h∗)\n⊆\nker(B) ∩im\n\u0000hT ∗\u0001\n(Γ ◦h∗)\n\u0000ker(B′ ◦h∗)\n\u0001\n(Γ ◦h∗)\n\u0000ker(B ◦Γ ◦h∗)\n\u0001\n=\nhT ∗\u0000ker(B ◦hT ∗) ∩im(ΓF)\n\u0001\n⊆\nhT ∗\u0000ker(B ◦hT ∗)\n\u0001\n(Γ ◦h∗)\n\u0000ker(B′′′)\n\u0001\n\u0000hT ∗◦ΓF\n\u0001\u0000ker(B′′ ◦ΓF)\n\u0001\n=\nhT ∗\u0000ker(B′′) ∩im(ΓF)\n\u0001\n⊆\nhT ∗\u0000ker(B′′)\n\u0001\n⊆\n⊆\n⊆\n⊆\n=\n=\n=\n=\nThe ambiguities are computed using Proposition 2.7, and the inclusions and equalities of ambiguities follow\nfrom Theorem 3.2 and Theorem 3.5. Here, the ambiguity Γ\n\u0000ker(B ◦Γ)\n\u0001\n= ker(B) ∩im(Γ) is the special\ncase discussed in depth in Lang et al. (2024). Note that the models MF\nF, MS×A×S\nF\nand MS×A×S\nS×A×S are closely\nrelated to the models M1, M2 and M3 from Section 3.3.\nAssume we would use one of these models in practice. The further right or up it is in the diagram, the more\nambiguity there is, but it is then also more likely that the model covers the true belief model (should it\nappear in the diagram in the ﬁrst place). Thus, there is a trade-oﬀbetween covering the true belief model,\nand keeping the ambiguity small.\nD\nDetails on the example with invariant features\nHere, we present more mathematical details for Section 3.3. This appendix is not self-contained and we\nrecommend reading it alongside the section in the main paper.\nD.1\nDetails on the MDP and observations\nFormally, the states are given by S = ({L, R} × {U, D})2, with the ﬁrst component being the hand-position,\nand the second component being the button position. For example, the state in Equation (10) is given by\n((L, U), (R, D)).\nFurthermore, we deﬁne functions PosH : S →{L, R} × {U, D} and PosB : S →{L, R} × {U, D} as the ﬁrst\nand second projection. These are the position of a “hand” H and a “button” B, in a 2x2 gridworld. Then\nthe state s from Equation (10) satisﬁes PosH(s) = (L, U) and PosB(s) = (R, D).\nThe set of trajectories is formally given by Ξ = (S × A)3 × S. The set of observations is formally given by\nO =\nh\n{L, R}2 × {P, P}\ni3\n× {L, R}2.\nHere, P means that it was not observed that a button was pressed.\n48\n\n\nD.2\nDetails on the belief models and symmetries\nLet G = D4 be the dihedral group of order 8, i.e., the symmetry group of the square. It is given by\nG = D4 =\n\b\ne, r, r2, r3, f, rf, r2f, r3f\n\t\n,\nwhere r is a clockwise rotations by 90◦and f is a ﬂip over the horizontal axis. In compositions, we apply f\nﬁrst. G acts on S × A by individually acting on states and actions:\ng.(s, a) := (g.s, g.a),\nwhere g.s = g.(PosH(s), PosB(s)) := (g. PosH(s), g. PosB(s)), where on the generators g = r and g = f we\nhave\nr.(L, U) = (R, U),\nr.(R, U) = (R, D),\nr.(R, D) = (L, D),\nr.(L, D) = (L, U)\nf.(L, U) = (L, D),\nf.(R, U) = (R, D),\nf.(R, D) = (R, U),\nf.(L, D) = (L, U).\nThis speciﬁes the action on states. On actions, we specify\nr.L = U,\nr.U = R,\nr.R = D,\nr.D = L,\nr.P = P.\nf.L = L,\nf.U = D,\nf.R = R,\nf.D = U,\nf.P = P.\nThus, the “pressing” action remains invariant.\nWith this group action, we obtain a set of equivalence classes of state-action pairs, given by S × A. A set of\nrepresentatives for the equivalence classes is given by\n\u0000{s0} × As0\u0001\n∪\n\u0000{s1} × As1\u0001\n∪\n\u0000{s2} × As2\u0001\n,\n(21)\nwhere\ns0 = ((R, D), (R, D)),\ns1 = ((L, D), (R, D)),\ns2 = ((L, U), (R, D)),\nand where the (state-dependent) set of actions are given by\nAs0 = As2 = {L, D, P},\nAs1 = {L, R, U, D, P}.\nWe then have a function\nh : S × A →\n[\ni∈{0,1,2}\n{si} × Asi\nthat maps each state-action pair to a representative, given by h(s, a) = g.(s, a) for the unique g ∈D4\nfor which g.(s, a) is in the set of representatives from Equation (21). Via h, we now identify S × A with\nS\ni∈{0,1,2}{si} × Asi.\nEquation (13) can be showed by\nΛξ,(s,a) =\n\u0002\nλ(ξ)\n\u0003\n(s, a)\n=\n2\nX\nt=0\nδ(s,a)(h(st, at))\n=\nX\n(s′,a′)∈S×A\nδ(s,a)(h(s′, a′))\n2\nX\nt=0\nδ(s′,a′)(st, at)\n=\nX\n(s′,a′)∈S×A\nh∗\n(s′,a′),(s,a) Γξ,(s′,a′)\n=\nX\n(s′,a′)∈S×A\nΓξ,(s′,a′) h∗\n(s′,a′),(s,a)\n= (Γ ◦h∗)ξ,(s,a).\n(22)\n49\n\n\nFor the matrix elements of h∗, we used Equation (1).\nThe human’s belief E(s, a | o) for (s, a) ∈S × A and o ∈O is then given as follows: The human has a\nuniform prior B(s) = P0 over possible start-states sampled from P0, and a uniform prior over possibly next\nactions given the current state, leading to a prior distribution over B(ξ) ∈∆(Ξ). Then, upon seeing o, the\nhuman implicitly computes a posterior belief over trajectories compatible with the observation, simply given\nby\nB(ξ | o) ∝δo(O(ξ)) · B(ξ).\n(23)\nEquation (14) can be showed by\nEo,(s,a) =\n\u0002\nǫ(o)\n\u0003\n(s, a)\n=\nX\nξ∈Ξ\n\u0002\nb(o)\n\u0003\n(ξ) ·\n\u0002\nλ(ξ)\n\u0003\n(s, a)\n=\nX\nξ∈Ξ\nBoξ ·Λξ,(s,a)\n= (B ◦Λ)o,(s,a)\n= (B ◦Γ ◦h∗)o,(s,a)\n(24)\nD.3\nDetails on the ambiguity analysis for M2\nRecall the observation o2:\no2 =\nH\nB\nP\nH\nB\nP\nH\nB\nP\nH\nB\nSince we assumed that the starting state is one of the states in Equation (11), the human has the belief\n\u0002\nǫ(o2)\n\u0003\n(s2, P) = 3, i.e., the human is certain that, up to symmetry, the hand performed a pressing action\nthree times in s2. Thus,\n0 =\n\u0002\nE(R)\n\u0003\n(o2) =\n\u0002\nǫ(o2)\n\u0003\n(s2, P) · R(s2, P) = 3 · R(s2, P)\nThis implies R(s2, P) = 0.\nRecall observation o1:\no1 =\nH\nB\nH\nB\nP\nH\nB\nP\nH\nB\nNow, the ﬁrst action could either not change anything, or horizontally align H and B. An action that does\nnot change anything is more likely (chance 2/3 since there are two actions, in the direction of two diﬀerent\nadjacent walls, that achieve this, which both correspond to action L up to symmetry), and so we obtain\n\u0002\nǫ(o1)\n\u0003\n(s2, L) = 2/3,\n\u0002\nǫ(o1)\n\u0003\n(s2, P) = 4/3,\n\u0002\nǫ(o1)\n\u0003\n(s2, D) = 1/3,\n\u0002\nǫ(o1)\n\u0003\n(s1, P) = 2/3.\nCompare also with (24). Thus, we obtain\n0 =\n\u0002\nE(R)\n\u0003\n(o1)\n= 2/3 · R(s2, L) + 4/3 · R(s2, P) + 1/3 · R(s2, D) + 2/3 · R(s1, P)\n= 2/3 · R(s1, P).\nHere, we used that R(s2, P) = 0 by what we showed before, and R(s2, L) = R(s2, D) = 0 since R(s, a) = 0\nwhenever a ̸= P (i.e., since R ∈V). Thus, we have R(s1, P) = 0 as well.\n50\n\n\nFinally, we look at the observation sequence o0 given as follows:\no0 =\nH\nB\nH\nB\nHB\nP\nHB\nAgain, there is a chance of 2/3 that the ﬁrst action does not change anything. Given the ﬁrst step, everything\nwhich follows is deterministic, leading to these feature beliefs:\n\u0002\nǫ(o0)\n\u0003\n(s2, L) = 2/3,\n\u0002\nǫ(o0)\n\u0003\n(s2, R) = 2/3,\n\u0002\nǫ(o0)\n\u0003\n(s1, P) = 2/3\n\u0002\nǫ(o0)\n\u0003\n(s2, D) = 1/3,\n\u0002\nǫ(o0)\n\u0003\n(s1, R) = 1/3,\n\u0002\nǫ(o0)\n\u0003\n(s0, P) = 1/3.\nCompare again with (24). This means that\n0 =\n\u0002\nE(R)\n\u0003\n(o0)\n= 2/3 · R(s2, L) + 2/3 · R(s2, R) + 2/3 · R(s1, P)+\n+ 1/3 · R(s2, D) + 1/3 · R(s1, R) + 1/3 · R(s0, P)\n= 1/3 · R(s0, P).\nHere, we used that R(s1, P) by what we showed before, together, again, with the fact that R(s, a) = 0 for\nall a ̸= P. That shows R(s0, P) = 0.\nD.4\nDetails on the ambiguity analysis for M3\nWe have\n\u0002\n(B ◦Γ)(R′)\n\u0003\n(o) = (B ◦Γ)o,(s′\n1,P ) · R′(s′\n1, P) + (B ◦Γ)o,(s′′\n1 ,P ) · R′(s′′\n1, P)\n= (B ◦Γ)o,(s′\n1,P ) −(B ◦Γ)o,(s′′\n1 ,P )\n= 0\n(25)\nIn the computation, the second step follows from the deﬁnition of R′. The last step follows from the symmetry\nremarked on before.\nE\nMathematical interpretations of related work in our framework\nIn this appendix, we brieﬂy interpret some of the related work from Section 4.2 in our framework for\nthe special case that they learn linear reward probes. Note that these interpretations are not meant to\ncapture everything there is to say about that work — the summaries we provide are quite coarse. In all\nexamples below, we assume access to a very capable foundation model bλ : Ξ →Rb\nΩthat allows for a linear\nontology translation Ψ : Rb\nΩ→RΩto the human’s ontology λ, as in Section 3.4.1: Ψ ◦bλ = λ. Deﬁne\nΦ := ΨT : RΩ→Rb\nΩ, which then satisﬁes bΛ ◦Φ = Λ by Proposition A.2. In all approaches below, we deﬁne\nbǫ and assume that the return function is learned with the same method as in Section 3.4.2. Notably, in all\nof the approaches one essentially just deﬁnes bǫ := bλ, i.e., no explicit modeling of humans is performed.\nE.1\nAmpliﬁed oversight and eliciting latent knowledge\nIn ampliﬁed oversight, one ampliﬁes the human to give accurate feedback, which means we can assume\nO = Ξ and ǫ = λ. One approach to achieve this would be to essentially deﬁne ǫ := Ψ ◦bλ by giving the\nhuman access to the linear ontology translation Ψ for understanding the foundation model’s thoughts. This\nwould roughly be in the spirit of eliciting latent knowledge (Christiano et al., 2021), where the human can\nquery a reporter to give information about arbitrary latent knowledge of an AI.\nAccordingly, one can also choose bǫ = bλ, leading to the following coverage diagram:\n51\n\n\nRΞ\nRΩ\nRb\nΩ\nRΞ\nΛ\nΛ\nΦ\nb\nΛ\nb\nΛ\nThe ontologies and feature belief functions are then the same, which automatically means that the ambiguity\ndisappears: Λ\n\u0000ker(Λ)\n\u0001\n= 0.\nE.2\nEasy-to-hard generalization\nIn this setting, O ⊆Ξ is a subset of trajectories that the human correctly understands. Thus, for ξ ∈O,\none has ǫ(ξ) = λ(ξ), and so ǫ = λ|O is simply a restriction. In this setting, one can also set bǫ = bλ|O. Now,\nlet Λ| and bΛ| be the linear functions corresponding to λ|O and bλ|O, respectively, via Proposition A.1. One\nobtains the following diagram:\nRΞ\nRΩ\nRb\nΩ\nRO\nΛ\nΛ|\nΦ\nb\nΛ\nb\nΛ|\nFor Φ in this diagram to be a morphism, we need that the lower diagram commutes. With Proposition A.2,\nthis follows from the assumption that ΦT = Ψ is an ontology translation: Ψ ◦bλ = λ implies Ψ ◦bλ|O = λ|O.\nThe ambiguity is now given by bΛ\n\u0000ker(bΛ|)\n\u0001\n. By using Theorem 2.11, this ambiguity disappears if and only\nif for all ξ ∈Ξ, we have bλ(ξ) ∈R\n\nbλ(ξ) | ξ ∈O\n\u000b\n. Thus, the ambiguity vanishes if the trajectories that the\nhuman understands have enough variety in the vector space of feature strengths.\nE.3\nClassical RLHF and weak-to-strong generalization\nIn classical RLHF, without any safeguards, one just uses the model bǫ = bλ as the feature belief function even\nthough ǫ ̸= λ and hopes for the best:\nRΞ\nRΩ\nRb\nΩ\nRΞ\nΛ\nE\nΦ\nb\nΛ\nb\nΛ\n(26)\nIn this case, the lower triangle does not commute since bΛ ◦Φ ̸= E, which, using Proposition A.2, is due to\nΨ ◦bλ = λ ̸= ǫ. This means that the second model does not cover the true belief model, and so the guarantee\nfrom Theorem 3.2 breaks. In fact, the return function that would be inferred using this model is GO, the\nobservation return function itself (cf. the deﬁnition of feedback-compatible return functions, Deﬁnition 2.6,\napplied to this faulty model).\nLang et al. (2024) extensively discuss failure modes in this case, called\ndeceptive inﬂation and overjustiﬁcation. We note that weak-to-strong generalization (Burns et al., 2023a),\n52\n\n\nwhen used without additional techniques, also considers this setting, but tries to ensure that the learning\nprocess or bλ contains inductive biases that steer the learning process to learn G anyway.\n53\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21262v1.pdf",
    "total_pages": 53,
    "title": "Modeling Human Beliefs about AI Behavior for Scalable Oversight",
    "authors": [
      "Leon Lang",
      "Patrick Forré"
    ],
    "abstract": "Contemporary work in AI alignment often relies on human feedback to teach AI\nsystems human preferences and values. Yet as AI systems grow more capable,\nhuman feedback becomes increasingly unreliable. This raises the problem of\nscalable oversight: How can we supervise AI systems that exceed human\ncapabilities? In this work, we propose to model the human evaluator's beliefs\nabout the AI system's behavior to better interpret the human's feedback. We\nformalize human belief models and theoretically analyze their role in inferring\nhuman values. We then characterize the remaining ambiguity in this inference\nand conditions for which the ambiguity disappears. To mitigate reliance on\nexact belief models, we then introduce the relaxation of human belief model\ncovering. Finally, we propose using foundation models to construct covering\nbelief models, providing a new potential approach to scalable oversight.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}