{
  "id": "arxiv_2502.20908v1",
  "text": "PRECONDITIONED BLOCK ENCODINGS\nFOR QUANTUM LINEAR SYSTEMS\nLeigh Lapworth∗1 and Christoph Sünderhauf†2\n1Rolls-Royce plc, P.O. Box 31, Derby, DE24 8BJ, UK\n2Riverlane, St. Andrews House, 59 St. Andrews Street, Cambridge CB2 3BZ, UK\nABSTRACT\nQuantum linear system solvers like the Quantum Singular Value Transformation (QSVT) require a\nblock encoding of the system matrix A within a unitary operator UA. Unfortunately, block encoding\noften results in significant subnormalisation and increase in the matrix’s effective condition number κ,\naffecting the efficiency of solvers. Matrix preconditioning is a well-established classical technique to\nreduce κ by multiplying A by a preconditioner P. Here, we study quantum preconditioning for block\nencodings. We consider four preconditioners and two encoding approaches: (a) separately encoding\nA and its preconditioner P, followed by quantum multiplication, and (b) classically multiplying A\nand P before encoding the product in UP A. Their impact on subnormalisation factors and condition\nnumber κ are analysed using practical matrices from Computational Fluid Dynamics (CFD). Our\nresults show that (a) quantum multiplication introduces excessive subnormalisation factors, negating\nimprovements in κ. We introduce preamplified quantum multiplication to reduce subnormalisation,\nwhich is of independent interest. Conversely, we see that (b) encoding of the classical product\ncan significantly improve the effective condition number using the Sparse Approximate Inverse\npreconditioner with infill. Further, we introduce a new matrix filtering technique that reduces the\ncircuit depth without adversely affecting the matrix solution. We apply these methods to reduce the\nnumber of QSVT phase factors by a factor of 25 for an example CFD matrix of size 1024x1024.\nContents\n1\nIntroduction\n2\n1.1\nPrevious work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nThis work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2\nMatrices and their block encodings\n4\n2.1\nCondition numbers of encoded matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nDiagonal scaling and encoding the original matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nToeplitz matrix encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.4\nBanded diagonal matrix encoding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.5\nCirculant matrix encoding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.6\nMatrix infill . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n∗leigh.lapworth@rolls-royce.com\n†christoph.sunderhauf@riverlane.com\n1\narXiv:2502.20908v1  [quant-ph]  28 Feb 2025\n\n\nPreconditioned block encodings for quantum linear systems\n3\nPreconditioning\n7\n3.1\nClassical preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2\nSubnormalisation factors of preconditioners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.3\nPreconditioning by quantum multiplication\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.3.1\nPreamplified quantum multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.4\nPreconditioning by classical multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n4\nQuery oracle circuit trimming\n11\n5\nCFD Applications using QSVT\n13\n6\nConclusions\n15\n7\nAcknowledgements\n15\nA Diagonal preconditioning\n18\nB\nSparse Approximate Inverse (SPAI)\n18\nB.1\nSparse-Sparse Iteration Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.2\nColumn Minimisation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nC Toeplitz Approximate Inverse (TPAI)\n20\nD Circulant Approximate Inverse\n21\nE\nDouble pass filtering to create equal values\n21\nE.1\nDigitising the matrix entries\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nE.2\nCollapsing equal angled rotation gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nE.3\nPerformance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n1\nIntroduction\nMany engineering applications, such as Computational Fluid Dynamics (CFD), adopt a predictor-corrector approach,\nwhere the predictor step involves the solution of a large system of linear equations. This step is generally the most\ntime-consuming part of the solver, where a quantum speed-up could be impactful. The system is expressed as the matrix\nequation\nAx = b\n(1)\nwhere A ∈RN×N and x, b ∈RN. Typically, A has sparsity, s, where s does not depend on N.\nDirect classical solvers, such as Lower-Upper decomposition, have time complexity O(N 3). These follow a prescribed\nsequence of steps that do not depend on the matrix condition number, κ, of the matrix. Classical iterative methods,\nsuch as the family of Krylov subspace methods [1, 2] are far more efficient. For example, the Conjugate Gradient (CG)\nmethod has time complexity O(Ns√κ log(1/ϵ)) where ϵ is the precision of the solution. For non-symmetric matrices,\nthe CG-Squared method scales with O(κ) and later variants, such as Bi-CGSTAB [3], can recover the O(\np\nκ) scaling\nwith a higher prefactor.\nQuantum Linear Equation Solvers (QLES) can offer complexity advantages over classical solvers. The first QLES\nis the HHL algorithm [4] with time complexity O(log(N)s2κ2/ϵ)). Later, Linear Combination of Unitaries [5] and\nQuantum Singular Value Transform (QSVT) [6, 7] techniques improved the query complexity to O(κ log(κ/ϵ)), an\nexponential improvement in precision, ϵ. Finally, the discrete adiabatic (DA) method [8] achieves provably optimal\n2\n\n\nPreconditioned block encodings for quantum linear systems\nquery complexity of O(κ log(1/ϵ)). It is important to note that these quantum algorithms require assumptions on I/O to\nbe effective [9], such as efficient block encoding circuits for loading matrices in QSVT.\nFor second-order elliptic problems, such as the pressure-correction equations considered here, κ scales with O(N 2/d)\nwhere d is the dimension of the domain [10]. Hence, in 2-dimensions, CG scales with O(N 3/2s log(1/ϵ)) and, the\nquery complexity of QSVT is O(N log(N/ϵ)). Hence, although the quantum solvers have worse scaling with κ than,\nsay, CG they can deliver a quantum speed-up, provided sufficiently efficient block encodings are available.\nClassical CG methods are prone to loss of precision that can lead to divisions by near-zero numbers. The GMRES\nmethod [11] addresses this, but needs to store search vectors of length N over which it can minimise the search error.\nThis consumes computer memory and most implementations of GMRES truncate the search to a small number of\nvectors. However, most iterative classical methods solve the preconditioned system\nPAx = Pb\n(2)\nwhere P is the preconditioning matrix such that, ideally, κ(PA) ≪κ(A). This reduces the dependence on the condition\nnumber and also makes the solver more stable.\nGiven that block encoding oracles of O(log(N)) for general purpose matrices remain elusive, it is essential to improve\nQLES as much as possible to gain a speed-up. In this light, the present paper considers preconditioning for QLES, and\nimproves block encodings with a circuit trimming procedure.\n1.1\nPrevious work\nThe first investigation of preconditioning a quantum linear equation solver [12] adopted the Sparse Approximate\nInverse (SPAI) technique that is popular in classical supercomputing as it solves an independent set of small n × d\nleast squares problems where n and d depend on the sparsity of the matrix and its approximate inverse. The rows of\nthe preconditioner were calculated independently and a single unitary was used to calculate the elements of PA. The\npreconditioning was implemented within the HHL algorithm and applied to electromagnetic scattering.\nA quantum algorithm for solving a circulant matrix system using a modified HHL solver [13] demonstrated the benefit\nof being able to express circulant matrices in terms of Quantum Fourier Transforms. Separately, [14] showed how to\nimplement a circulant preconditioner using a modified quantum singular value estimation scheme.\nIncomplete Lower Upper (ILU) preconditioning was used by [15] in conjunction with the Variational Quantum Linear\nSolver. The ILU preconditioner was computed and applied as a classical preprocessing step with VQLS used to solve\nthe preconditioned system.\nA fast inversion scheme for preconditioning [16] writes the matrix system as (A + B) |x⟩= |b⟩and assumes\n||A|| >> ||B||. Rewriting gives:\n(I + A−1B) |x⟩= A−1 |b⟩\n(3)\nA is fast-invertible if, after rescaling A so that ||A−1|| = 1, there is a block-encoding of A−1 where the number of\noracle queries for A is not dependent on its condition number. Diagonal and 1-sparse matrices can be fast-inverted.\nAlthough for general matrices, the scheme requires a classical Singular Value Decomposition to give A = UDV †. For\nnormal matrices, U = V and in certain cases V is easily computed, e.g. when V is the Quantum Fourier Transform. A\ncomparison of the SPAI, circulant and fast inversion preconditioners [17] showed significant scaling benefits for fast\ninversion using an inverse Laplacian preconditioner. The test case focussed on fracture mechanics and split the matrix\ninto A = ∆+ AF , where ∆is the Laplacian of the flow field in the absence of fractures and AF is the fracture matrix.\nFor the cases studied, the Laplacian has a known eigenvalue decomposition. Classical preconditioning using an inverse\nLaplacian [18, 19, 20, 21] has demonstrated similar benefits using Krylov subspace iterative solvers.\nWhilst previous work has provided some practical details on preconditioning quantum linear solvers, none has considered\nthe impact of the subnormalisation factors arising from block encoding. Nor has the difference between the circuit\nimplementations of classical and quantum multiplication of P and A been considered.\n1.2\nThis work\nThis work investigates matrix preconditioning within the context of block encoding for quantum solvers such as QSVT.\nWe consider the SPAI and circulant preconditioners that have previously been reported and an approximate Toeplitz\npreconditioner. For the SPAI and Toeplitz preconditioners we also evaluate the effect of infill. Separately, we apply\ndiagonal scaling as a precursor step. This can be cast in the form of a fast inverse but is a O(N) operation that is\nclassically efficient.\n3\n\n\nPreconditioned block encodings for quantum linear systems\nFirst, the matrices in questions (preconditioners P, CFD matrix A, or products PA) must be implemented as block\nencodings amenable to quantum computation, these are shown in Section 2.\nNext, in Section 3 we study preconditioning. Many classical methods do not explicitly form the matrices A and P,\ninstead subroutines are called to perform the matrix-vector multiplication. For QLES, the classical overhead of forming\nPA suggests the analogous approach of multiplying the encoded matrices for A and P rather than encoding PA. For\ncompactness, we use the terms quantum multiplication for the separate encoding of P and A and classical multiplication\nfor pre-multiplication of P and A prior to encoding the product PA. We introduce preamplified quantum multiplication\n(Section 3.3.1), which improves the state of the art for multiplying block encodings and is of independent interest. Both\napproaches (classical multiplication and quantum multiplication) are studied and compared: We investigate how the\nsubnormalisation factors associated with the block-encoding of A, P and PA affect the condition number for each of\nthe preconditioners considered.\nIn Section 4, we introduce a circuit trimming technique that reduces the depth of query oracles used in the block\nencodings. Although we have used efficient encoding techniques [22] that have low subnormalisation factors and a low\nnumber of ancilla qubits, the depth of the query oracles scales with the number of non-zero entries in the matrix. We\nintroduce a double-pass filtering technique that places the entries of the matrix into bins of equal value and show how\nthis can reduce the depth of the query oracle with a minimal effect on the solution accuracy.\nFinally, in Section 5 we turn to CFD applications of the methodology presented. Using emulated circuits for CFD\nmatrices ranging from 16×16 to 4, 096×4, 096 we provide the first practical circuit level evaluations of preconditioning\nwith QSVT.\n2\nMatrices and their block encodings\nFor this study four preconditioners are considered:\n• Diagonal scaling, see Appendix A.\n• Sparse Approximate Inverse (SPAI), see Appendix B.\n• Toeplitz Approximate Inverse (TPAI), see Appendix C.\n• Circulant Approximate Inverse (CLAI), see Appendix D.\nWhether the matrix A and its preconditioner P are being encoded separately or as a classically computed product,\nthe key factors of an efficient block encoding are the reduction in subnormalisation and circuit depth. In the next\nSection 2.1, we review the impact of subnormalisation on the effective condition number. Then (Sections 2.2–2.5)\nwe present the different encodings used for the matrices appearing in this work, following those of [22]. Finally, in\nSection 2.6 we discuss the sparsity/infill of the required matrices, and how they are affected by preconditioning.\n2.1\nCondition numbers of encoded matrices\nBlock encoding a matrix A ∈RN×N with N = 2n entails creating a unitary operator such that:\nUA =\n\u0012\nA/s\n∗\n∗\n∗\n\u0013\n(4)\nwhere s is the subnormalisation factor [23, 24].\nWhen inverting a matrix using QSVT, it is the subnormalised condition number, κs, that dictates the number of phase\nrotations, where:\nκs =\n1\n|λmin\ns\n|\n(5)\nand λmin\ns\nis the smallest eigenvalue of A/s. If A/s is not diagonalisable, then λmin\ns\nis the smallest singular value.\nNote that due to the unitary embedding, it is not the case that κs = λmax\ns\n/λmin\ns\n. Note, also, that while λmin\ns\n= λmin/s\nthis does not, in general, mean that κs = sκ. However, it does give:\nκs =\ns\n|λmin|\n(6)\n4\n\n\nPreconditioned block encodings for quantum linear systems\n2.2\nDiagonal scaling and encoding the original matrix\nWhilst we have labelled diagonal scaling as a preconditioner, it is best considered as an O(N) classical pre-processing\noperation. It is a necessary step for the Toeplitz and circulant preconditioners. It is also beneficial for encoding A since\nit sets all the entries along the main diagonal to 1. Whilst this has a marginal effect on the condition number, it usually\nincreases |λmin\ns\n|.\nThe diagonal scaled matrix DA provides the reference values for measuring the improvements provided by precon-\nditioning. Since D−1 is not a preconditioner in the same sense as the others, we will use A to refer to D−1A in the\nfollowing unless necessary to avoid confusion.\nSince A is a banded diagonal matrix, it uses the encoding method described in Section 2.4. However, we first describe\nthe encoding of Toeplitz matrices as the banded diagonal encoding is a generalisation thereof.\n2.3\nToeplitz matrix encoding\nThe encoding of a Toeplitz matrix has the shortest circuit depth and follows directly from the PREP/UNPREP scheme of\n[22]. We set their parameter p to 1\n2 which means that the UNPREP operator is the adjoint of the PREP operator, except\nfor negative-valued diagonals where there is a sign difference.\n|del⟩\n|j0⟩\n|j1⟩\n|j2⟩\n|j3⟩\n|s0⟩\n|s1⟩\n|s2⟩\nPREP\n+3\n+4\n+5\n+8\nUNPREP\n−4\nFigure 1: Circuit for encoding a 16x16 pentadiagonal Toeplitz matrix with diagonals at offsets: −4, −1, 0, 1, 4.\nNegative offsets are super-diagonals, positive offsets are sub-diagonals and 0 is the main diagonal. Strictly, the above\ncircuit has a controlled addition of 0, but this is omitted as it has no effect.\nFigure 1 gives an overview of the circuit to encode a 16x16 pentadiagonal Toeplitz matrix based on a 4x4 CFD pressure\ncorrection equation. Each diagonal has a constant value; these are loaded using the PREP operator. The multiplexed\ncontrols then offset each diagonal by a positive amount. Note that there is no offset for the −4 diagonal as a controlled\naddition of 0 has no effect. After the controlled additions, the final arithmetic operator moves the diagonals to their\ncorrect location.\nThe qubit labels follow [22] where those labelled |j⟩designate the columns of the matrix, and those labelled |s⟩\ndesignate the diagonals. The |del⟩qubit is needed so that the encoding creates a 32x32 matrix. This is because when a\ndiagonal is offset, values that move out of the range of the matrix wrap round to the other side. This contaminates the\n32x32 matrix but not the upper left 16x16 block.\nThe subnormalisation factor for PREP/UNPREP encoding is the sum of absolute entries on the diagonal, which is\ngenerally lower than the number of diagonals.\n2.4\nBanded diagonal matrix encoding\nEncoding a band diagonal matrix is a straightforward extension of the Toeplitz scheme and is used to encode A and the\nSPAI preconditioner. Instead of loading a constant value for each diagonal, we must use separate oracles to load the\nvalues along each diagonal. To keep the subnormalisation factor as low as possible, the values along each diagonal are\nscaled to have a maximum value of 1.0. The PREP operator loads the factors needed to scale all the diagonals back to\ntheir original values. Since PREP must load a normalised state, the diagonals are not returned to their original values.\nFigure 2 shows a schematic of the circuit to encode a 16x16 pentadiagonal matrix. As before, the qubit labels follow\n[22] where the data loading controlled rotations are applied to the |d0⟩qubit. The encoding oracle follows full\nmatrix encoding [24] but instead of using row-column indexing, diagonal-column indexing is used. Importantly, the\nsubnormalisation factor scales with the number of diagonals rather than with the dimension of the matrix. The operators\nD0, D−1, ... contain both the data loading oracles and the arithmetic operators to offset the diagonals to the correct\nposition. A negative offset creates a super-diagonal and a positive offset creates a sub-diagonal.\n5\n\n\nPreconditioned block encodings for quantum linear systems\n|j0⟩\n|j1⟩\n|j2⟩\n|j3⟩\n|d0⟩\n|s0⟩\n|s1⟩\n|s2⟩\nPREP\nD0\nD−1\nD+1\nD−4\nD+4\nPREP †\nFigure 2: Circuit for encoding a 16x16 pentadiagonal matrix using an LCU to add separately encoded diagonals. Each\ndiagonal encoding operator Di includes the addition or subtraction of i to give the correct offset from the main diagonal.\n|j0⟩\n|j1⟩\n|j2⟩\n|s0⟩\n|s1⟩\n|d0⟩\nPREP\nRy\nRy\nRy\nRy\nRy\nRy\nRy\nRy\n−1\nRy\nRy\nRy\nRy\nRy\nRy\nRy\nRy\nRy\nRy\nRy\n+4\nX\nPREP †\nD0\nD−1\nD+4\nFigure 3: Circuit segment for encoding a banded 8x8 tridiagonal matrix with one super-diagonal offset by 1 from the\nmain diagonal and one sub-diagonal offset by 4 from the main diagonal. Note only 2 PREP qubits are needed to load the\n3 diagonals.\nFigure 3 shows the circuit segment for loading a banded tridiagonal matrix. The controls on the |j⟩qubits select the\ncolumn and the controls on the |s⟩qubits select the diagonal. By themselves, the controlled rotations create a direct sum\nof the three diagonal matrices. The controlled subtraction offsets the part of the direct sum containing the super-diagonal\nby 1. Similarly, the controlled addition offsets the sub-diagonal by 4. The PREP-UNPREP operations scale and mix the\ndiagonals to give the full encoded matrix in the top left corner of the resulting unitary. The X gate after the rotations is\nrequired for ARCSIN encoding [25]. Since only the non-zero entries of the matrix are encoded, there is no out of range\nwrap-around as occurs in the Toeplitz encoding.\nIn this study, banded diagonal encoding is used to load A and the SPAI preconditioner, and all the classically computed\nproducts PA.\n2.5\nCirculant matrix encoding\nFrom Equation (36) in Appendix D, the circulant preconditioner must load FΛ−1F † where F is the Fourier transform.\nThe diagonal matrix of inverse eigenvalues, Λ−1, can be computed from Equation (35) and can be encoded using\ndiagonal encoding. The resulting circuit including the product with A is shown in Figure 4\n|c0⟩\n|c1⟩\n|c2⟩\n|s0⟩\n|s1⟩\n|s2⟩\n|d0⟩\n|a0⟩\nEncode A\nX\nQFT †\nEncode Λ−1\nQFT\nFigure 4: Circuit for encoding C−1A for an 8x8 pentadiagonal matrix.\n2.6\nMatrix infill\nAlthough the inverse of a sparse matrix is not, itself, sparse, the default for many preconditioners is to impose the same\nsparsity pattern on P as the original matrix A. This limits the effectiveness of the preconditioning but is often necessary\ndue to computational limits. An alternative is for the preconditioner to use infill where P is allowed to have more\n6\n\n\nPreconditioned block encodings for quantum linear systems\nnon-zeros than A. This is discussed in Appendix B and illustrated in Figure 12. For banded diagonal matrices, the infill\nprocedure introduces additional diagonals. The infill algorithm can be repeated with each repetition giving a better\napproximation to A−1 at the expense of adding more diagonals.\nTPAI\nSPAI\nInfill levels\ndiag(P)\ndiag(PA)\ndiag(P)\ndiag(PA)\nnon-zero diag(PA)\n0\n5\n13\n5\n13\n9\n1\n11\n23\n13\n25\n13\n2\n17\n33\n25\n41\n17\n3\n23\n43\n41\n61\n21\nTable 1: Number of banded diagonals in P and the classical product PA for different infill levels with the TPAI and\nSPAI preconditioners. The final column shows the number of diagonals that have non-zero entries.\nTable 1 shows the number of diagonals with infill levels from zero to three for P and the classical product PA using the\nTPAI and SPAI preconditioners. Note that CLAI generates a dense matrix for which infill is not relevant. As described\nin Appendix B and Appendix C, the preconditioners use different infill strategies with SPAI adding more diagonals per\ninfill operation.\nAn unexpected feature of SPAI is that a number of diagonals in the product PA consist entirely of zeros to within the\nprecision of the arithmetic. The is a result of exactly solving each reduced system as described in Appendix B.1. For\nSPAI with three levels of infill, performing quantum multiplication requires encoding two banded matrices with 5 and\n41 diagonals. Whereas, classical multiplication encodes a single banded matrix with 21 diagonals. Of course, the latter\nhas a high classical pre-processing cost to compute PA prior to encoding.\nAlthough the TPAI reduced system is also solved exactly, the initial approximation of A as a Toeplitz matrix means PA\nhas no diagonals with zero values.\n3\nPreconditioning\nAs a test case for preconditioning, we use CFD pressure correction matrices for the flow in a lid-driven cavity taken\nfrom the open source qc-cfd 3 repository. The CFD meshes range from 4 × 4 to 64 × 64 cells. The corresponding\npressure correction matrices have dimensions ranging from 16 × 16 to 4, 096 × 4, 096 and are each extracted from the\nnon-linear solver after 100 outer iterations.\nDS\nCLAI\nTPAI\nSPAI\nSection\nFigure\nClassical\n✓\n✓\n✓\n✓\n§3.1\n§5\nQuantum multiplication\n✓\n✓\n✓\n§3.2, §3.3\n§6, §7\nClassical multiplication\n✓\n✓\n§3.4\n§8\nTable 2: Results presented in this section. DS = Diagonal scaling. CLAI, TPAI, SPAI = Circulant, Toeplitz and Sparse\nApproximate Inverse respectively.\nWe study the impact of preconditioning on subnormalisation and subnormalised condition number, where Table 2 gives\nan overview of the scenarios considered. Firstly, the preconditioners are evaluated exactly as they would be used by a\nclassical solver (Section 3.1). Then we study the subnormalisation factors for each preconditioner (Section 3.2), required\nfor preconditioning by quantum multiplication. In Sections 3.3 and 3.4, we study preconditioning with the quantum\nmultiplication and classical multiplication approaches, respectively. For preconditioning by classical multiplication,\nonly TPAI and SPAI are considered as the CLAI preconditioner produces a dense matrix that would require excessive\nclassical computing resources. The effects of preconditioning on both subnormalisation factors and condition numbers\nare considered.\nAs mentioned above diagonal scaling is a preprocessing method used by all the preconditioners. It is included in the\nclassical section only to show the small effect it has. The SPAI and TPAI results include infill up to three levels. All\nmatrices are scaled to have ||A||max = 1 prior to block encoding.\n3https://github.com/rolls-royce/qc-cfd/tree/main/2D-Cavity-Matrices\n7\n\n\nPreconditioned block encodings for quantum linear systems\n3.1\nClassical preconditioning\nFor comparison, we present the purely classical effect that preconditioning has on the matrix condition number. Note\nthat this does not require any encoding, it is a simple matrix-matrix multiplication, and there is no equivalent of a\nsubnormalisation factor.\n23\n25\n27\n29\n211\n213\nMatrix rank\n101\n102\n103\n104\n105\nCondition number\nOriginal\nDiag\n(a) Diagonal preconditioning.\n23\n25\n27\n29\n211\n213\nMatrix rank\n100\n101\n102\n103\n104\n105\nCondition number\nOriginal\nSPAI(0)\nSPAI(1)\nSPAI(2)\nSPAI(3)\nSPAI(5)\n(b) Sparse Approximate Inverse.\n23\n25\n27\n29\n211\n213\nMatrix rank\n100\n101\n102\n103\n104\n105\nCondition number\nOriginal\nTPAI(0)\nTPAI(2)\nTPAI(3)\n(c) Toeplitz Approximate Inverse.\n23\n25\n27\n29\n211\n213\nMatrix rank\n100\n101\n102\n103\n104\n105\nCondition number\nOriginal\nCirculant\n(d) Circulant Approximate Inverse.\nFigure 5: Classical preconditioning - condition numbers for PA as used by a classical algorithm. Computed using\nsimple matrix-matrix multiplication. Numbers in brackets indicate levels of infill.\nFigure 5 shows the effect of the four preconditioners on the condition numbers of the CFD matrix. As anticipated,\ndiagonal scaling has little benefit by itself. There is some erratic behaviour for SPAI and TPAI with higher levels of\ninfill on the coarser meshes which is due to the small matrix size. For larger meshes, there is an increasing benefit of\nmore levels of infill. TPAI shows an initial improvement without infill but only a marginal improvement thereafter.\nThere is no option to use infill with CLAI. However, it performs as well as SPAI with 3 levels of infill for the larger\nmatrices.\n3.2\nSubnormalisation factors of preconditioners\nWe first consider the subnormalisation factors for encoding the preconditioner P. From Figure 6a we can see that the\nsubnormalisation factor for A is approximately s = 2 for all meshes. This is due to the mass conservation equation\nbeing used to construct the pressure correction matrix, where for each row the diagonal entry is the negative of the sum\nof the off diagonal entries. The small difference from 2 is due to the fact that each diagonal is scaled by 1/||D||max\nprior to encoding. The locations of the maximal entries can occur on different rows of the matrix and, hence, s = 2 is\nlower bound if all the maxima occur on the same row. However, this is small effect that reduces on the finer meshes.\nThe first observation is the striking difference between the values of s for the SPAI and TPAI preconditioners, Figure 6b\nand Figure 6c, as the level of infill is increased. This is largely due to the difference in the way each preconditioner adds\ninfill. The numbers of diagonals added for each level of infill are compared in Table 1. These are independent of the\n8\n\n\nPreconditioned block encodings for quantum linear systems\n23\n25\n27\n29\n211\n213\nMatrix rank\n0\n1\n2\n3\n4\n5\nSubnormalisation factor\nDiagonal\n(a) Diagonal preconditioning, s.\n23\n25\n27\n29\n211\n213\nMatrix rank\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nSubnormalisation factor\nSPAI(0)\nSPAI(1)\nSPAI(2)\nSPAI(3)\n(b) Sparse Approximate Inverse, s.\n23\n25\n27\n29\n211\n213\nMatrix rank\n0\n2\n4\n6\n8\n10\nSubnormalisation factor\nTPAI(0)\nTPAI(1)\nTPAI(2)\nTPAI(3)\n(c) Toeplitz Approximate Inverse, s.\n23\n25\n27\n29\n211\n213\nMatrix rank\n0\n100\n200\n300\n400\n500\n600\nSubnormalisation factor\nCirculant\n(d) Circulant Approximate Inverse, s.\nFigure 6: Subnormalisation factors for encoding each preconditioner for CFD pressure correction matrices on meshes\nranging from 4 × 4 to 64 × 64 cells. Numbers in brackets indicate levels of infill.\nCFD mesh size except for the two smallest cases. The difference in subnormalisation factors is largely attributable to\nthe difference in the number of diagonals. This is not a pro-rata increase as it is the maximum entry along each diagonal\nthat sets the PREP coefficients.\nAnother difference is that for each level of infill, the subnormalisation factors for TPAI reduce as the mesh size increases.\nWhereas, for SPAI, ignoring the smaller meshes, s is broadly constant as the mesh size increases. This is due to the\nneed to approximate A as a Toeplitz matrix in order to compute the TPAI preconditioner. This adds infill to A which is\nsignificant on the smaller meshes but has a diminishing effect as the mesh size increases.\nThe second observation is that for the circulant preconditioner, s rises rapidly with the size of the matrix. This is due to\nthe fact that as the mesh size increases, the cell dimensions become smaller and the lowest eigenvalue reduces. The\nincrease in s is driven entirely by the need to scale Λ−1 so that ||Λ−1||max = 1. There is no subnormalisation factor\nassociated with the quantum Fourier transforms.\n3.3\nPreconditioning by quantum multiplication\nThe first approach to preconditioning is to separately encode A and its preconditioner, P, and then multiply the\nblock encoded matrices on the quantum computer. From a circuit depth perspective, this is attractive as the Toeplitz\nand circulant preconditioners have efficient circuit implementations. However, the usual method [26] to quantum\nmultiplication of matrices does result in the subnormalisation factors of A and P being multiplied.\nFigure 7 shows the resulting values of κs for QSVT from Equation (5). All figures have two reference lines. The blue\nline labelled ’Original’ is the classically computed condition number after diagonal scaling. Results below this line are\n9\n\n\nPreconditioned block encodings for quantum linear systems\n23\n25\n27\n29\n211\n213\nMatrix rank\n101\n102\n103\n104\n105\n106\nCondition number\nOriginal\nDiagonal\n(a) Diagonal preconditioning, κs.\n23\n25\n27\n29\n211\n213\nMatrix rank\n101\n102\n103\n104\n105\n106\nCondition number\nOriginal\nDiagonal\nSPAI(0)\nSPAI(1)\nSPAI(2)\nSPAI(3)\n(b) Sparse Approximate Inverse, κs.\n23\n25\n27\n29\n211\n213\nMatrix rank\n101\n102\n103\n104\n105\n106\nCondition number\nOriginal\nDiagonal\nTPAI(0)\nTPAI(1)\nTPAI(2)\nTPAI(3)\n(c) Toeplitz Approximate Inverse, κs.\n23\n25\n27\n29\n211\n213\nMatrix rank\n101\n102\n103\n104\n105\n106\nCondition number\nOriginal\nDiagonal\nCirculant\n(d) Circulant Approximate Inverse, κs.\nFigure 7: Preconditioning by quantum multiplication - condition numbers using all preconditioners. The blue line\n(Original) gives the classical condition numbers (κ) of A without preconditioning. The orange line (Diagonal) gives the\nquantum condition numbers (κs) for the encoded matrix UA without preconditioning. Numbers in brackets indicate\nlevels of infill.\nbetter than a classical solver without preconditioning. The orange line labelled ’Diagonal’ is the block encoding of the\noriginal matrix. Results below this are better that a quantum solver without preconditioning.\nAs shown, none of the preconditioners reduce κs below the unpreconditioned value. For SPAI, Figure 7b, the benefit of\nincreasing infill cannot overcome the increase in subnormalisation factor. All of the SPAI results are worse than not\ndoing preconditioning. TPAI, Figure 7c, fares a little better due to the lower subnormalisation factors but it, too, offers\nno benefit over not doing preconditioning. CLAI, Figure 7d, performs the worst except for the smallest matrices due to\nthe rapid increase in subnormalisation factor.\n3.3.1\nPreamplified quantum multiplication\nLet UA and UP be block encoding circuits of A and P with subnormalisations α and β, respectively. Then [26]\nconstructs a block encoding of AP with subnormalisation αβ and 1 query to UA and UP each. The figure of merit [22]\nof the resulting block encoding is the product of subnormalisation and circuit cost\nαβ · (gates(UA) + gates(UP )).\n(7)\nThis figure of merit, and not the gate count of the block encoding alone, determines the total cost of matrix inversion\nand other quantum algorithms (up to logarithmic factors), due to the appearance of the subnormalisation s in κs in the\nnumber of queries to the block encoding, see Equation (6).\nHere, we introduce the new method of preamplified quantum multiplication. In the spirit of methods constructing\npreamplified block encodings [26, 22], separate parts of the quantum circuit are amplified individually. Singular value\n10\n\n\nPreconditioned block encodings for quantum linear systems\namplification allows to improve the subnormalisation of a block encoding by a factor of γ with approximately\nγ 3\nδ log γ\nϵ\n(8)\nqueries to the block encoding [22]. The parameters ϵ and δ determine the accuracy and range of singular value\namplification.\nFor preamplified quantum multiplication of block encodings, UA and UP are first amplified by amplification factors\nγ1 < α, γ2 < β, and then multiplied with the usual method for multiplying block encodings. The resulting block\nencoding has subnormalisation αβ/(γ1γ2) and requires γ1/2\n3\nδ log\nγ1/2\nϵ\nqueries to UA and UP , respectively. The figure\nof merit is\nαβ\n\u0012 1\nγ2\n3\nδ log γ1\nϵ · gates(UA) + 1\nγ1\n3\nδ log γ2\nϵ · gates(UP )\n\u0013\n.\n(9)\nPreamplified multiplication is advantageous to regular multiplication (with figure of merit (8)) if\nγ1 > 3\nδ log γ2\nϵ and γ2 > 3\nδ log γ1\nϵ ,\n(10)\nwhich can be achieved if the original subnormalisations α, β allow for strong amplification. For the example matrices\nand matrix sizes considered in this work, we find that for preamplification of UA, γ1 ≃1.25 is the best that can be\nachieved. For UP , γ2 ≃1.25 −2.0 can be achieved with more levels of infill allowing higher values of γ2. However,\nthese are not sufficient to reduce the figure of merit, and we use regular quantum multiplication [26]. However, in larger\nsystems or if the matrices involved have worse subnormalisations, our preamplified multiplication method will provide\nan advantage.\n3.4\nPreconditioning by classical multiplication\nThe second approach is to perform the product of P and A as a classical preprocessing step and the encode PA. This is\nonly feasible for TPAI and SPAI as PA retains some degree of sparsity. Figure 8 shows the resulting values of s and κs\nfor encoding PA.\nThe subnormalisation factors, Figure 8a and Figure 8b, for TPAI and SPAI have similar values and are relatively\ninsensitive to the size of the mesh and the number of levels of infill. When encoding the preconditioner, P, by itself,\nthe subnormalisation factors for SPAI were significantly larger than TPAI due to the fact that the infill created more\ndiagonals, see Table 1. However, when encoding PA, SPAI creates more diagonals than TPAI, but many of the\ndiagonals contain only zeros to within the precision of the matrix product as discussed in Section 2.6. The presence of\nthe zero-valued diagonals in SPAI does not influence the subnormalisation factor as the PREP operator just loads a zero\ncoefficient. However, removing them from the encoding reduces the number of qubits needed for the PREP register. The\nnet result is that SPAI with 3 levels of infill reduces the QSVT condition number, κs, by almost an order of magnitude.\nThis, in turn, leads to a reduction of over 20 in the number of QSVT phase factors.\nAlthough the reduction in κs is significant this comes with two costs:\n• The product PA must be computed on the classical computer.\n• The number of diagonals to be encoded is significantly higher than for the original matrix.\nCounter-intuitively, the number of diagonals to encode PA is less than the number to encode P and A separately. For 3\nlevels of infill, separate encoding requires 41+5 diagonals, encoding PA after removing the zero diagonals requires\nonly 21 diagonals, see Table 1.\nThe classical product PA can be parallelised and since each row-column multiplication leads to one multi-controlled\nrotation, the encoding circuit can be written directly without the need to form an intermediate matrix. Since the rotations\nfor each diagonal commute, the parallel circuit generation does not create any loop-order dependencies.\nReducing the depth of the encoding circuit is described in the next section and relies on the fact that, as well as creating\nzero valued diagonals, SPAI also tends to equalise the values along each of the non-zero diagonals.\n4\nQuery oracle circuit trimming\nThe circuit trimming approach reduces the circuit size required for block encoding a band diagonal matrix (see\nSection 2.4. It relies on finding multiplexed rotations where the rotation angles are the same and the multiplexed\ncontrols differ by a Hamming distance of 1. The effectiveness of this is enhanced if the matrix is filtered to increase the\n11\n\n\nPreconditioned block encodings for quantum linear systems\n23\n25\n27\n29\n211\n213\nMatrix rank\n0\n2\n4\n6\n8\n10\nSubnormalisation factor\nTPAI(0)\nTPAI(1)\nTPAI(2)\nTPAI(3)\n(a) Toeplitz Approximate Inverse, s.\n23\n25\n27\n29\n211\n213\nMatrix rank\n0\n2\n4\n6\n8\n10\nSubnormalisation factor\nSPAI(0)\nSPAI(1)\nSPAI(2)\nSPAI(3)\n(b) Sparse Approximate Inverse, s.\n23\n25\n27\n29\n211\n213\nMatrix rank\n101\n102\n103\n104\n105\n106\nCondition number\nOriginal\nDiagonal\nTPAI(0)\nTPAI(1)\nTPAI(2)\nTPAI(3)\n(c) Toeplitz Approximate Inverse, κs.\n23\n25\n27\n29\n211\n213\nMatrix rank\n101\n102\n103\n104\n105\n106\nCondition number\nOriginal\nDiagonal\nSPAI(0)\nSPAI(1)\nSPAI(2)\nSPAI(3)\n(d) Sparse Approximate Inverse, κs.\nFigure 8: Preconditioning by classical multiplication - subnormalisation factors and condition numbers for the product\nPA using the TPAI and SPAI preconditioners. The blue and orange lines in Figures (c) and (d) are the\nnon-preconditioned reference lines as in Figure 7. Numbers in brackets indicate levels of infill.\nnumber of equal-valued entries along each diagonal. We take a digitisation approach where values that are close to each\nother are placed in bins of equal value. However, the filtering needs to take account of the physical characteristics of\nthe matrix and, for example, just because an entry is close to zero does not mean it can be rounded to zero. We use\nthe double-pass filter approach described in Appendix E. The filter involves a free parameter, f that controls the size\nof the bins. This is a relative factor that sets the bin size to\nh\n(1 −f\n2 ) ¯di, (1 + f\n2 ) ¯di\ni\nwhere ¯di is the average value of\nthe ith bin. The first pass creates a list of overlapping bins that all meet the size criterion. The second pass selects\nnon-overlapping bins giving preference to bins with a large number of entries.\nFigure 9 shows the influence of the smoothing factor, f, on the encoding of PA for the 16x16, 32x32 and 64x64 CFD\nmeshes. The preconditioner is SPAI with 3 levels of infill. The overall trends are as expected: increasing f reduces the\nnumber of unique angles (i.e. fewer bin with more entries in each bin) and rotation gates, but increases the L2 error of\nthe solution vector.\nThere are two other notable features in Figure 9. The first is that the reduction in the number of unique angles is\nfar greater than the reduction in the number of rotation gates. This is because not all multiplexed controls meet the\nrequirement to be coalesced. The second feature is that the percentage reduction in the number of unique values and\nrotations increases with the size of the mesh. Taking the circuits for which the L2 error is ≤10−2, the number of\nrotations relative to the unsmoothed case is 70.7%, 49.5%, 30.7% for the 16x16, 32x32 and 64x64 meshes respectively.\nThese features are discussed in Appendix E.3.\n12\n\n\nPreconditioned block encodings for quantum linear systems\n1%\n10%\n100%\n0\n0.005\n0.01\n0.015\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nPercentage of full circuit\nSmoothing Parameter\n256x256\n1024x1024\n4096x4096\n(a) Number of unique rotation angles.\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0\n0.005\n0.01\n0.015\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nPercentage of full circuit\nSmoothing Parameter\n256x256\n1024x1024\n4096x4096\n(b) Number of multiplexed gates.\n1.E-04\n1.E-03\n1.E-02\n1.E-01\n1.E+00\n0\n0.005\n0.01\n0.015\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nL2 error\nSmoothing Parameter\n256x256\n1024x1024\n4096x4096\n(c) L2 norm of effect of smoothing\nFigure 9: Effect of the smoothing parameter on the encoding circuit for PA where P uses SPAI with 3 levels of infill.\nThe numbers of angles and rotations are shown as percentages of the unsmoothed values. Legends refer to the matrix\ndimensions not the CFD mesh dimensions.\n5\nCFD Applications using QSVT\nIn this section we consider two test cases that have not been modelled previously due to the number of emulated qubits\nrequired. The QSVT phase factors are calculated with the Remez method [27, 28] using the open-sourced QSPPACK 4\nsoftware package.\n0\n200\n400\n600\n800\n1000\nState vector index\n0.0015\n0.0010\n0.0005\n0.0000\n0.0005\nState vector amplitude\nClassical\nQSVT\nFigure 10: Comparison of QSVT and classical solutions from the pressure correction matrix of a 32x32 CFD mesh.\nThe solution contains corrections to the pressure field.\nThe first case is the pressure correction matrix for a 2D cavity with a 32x32 mesh sampled after 100 outer non-linear\niterations. Figure 10 compares the QSVT and classical state vector solutions. These are stored at every point in the 2D\nmesh and are ordered in sweeps across the horizontal grid lines starting at the lower left corner of the cavity and ending\nat the top right. The results show good agreement with small differences due to the application of oracle trimming. The\ndetails of the QSVT calculation are:\n4https://github.com/qsppack/QSPPACK\n13\n\n\nPreconditioned block encodings for quantum linear systems\n• SPAI with 3 levels of infill.\n• Diagonal smoothing factor, f = 0.015\n• Number of unique angles reduced from 17374 to 1077.\n• Number of nonzero rotations reduced from 18,378 to 8,928.\n• Subnormalisation factor from classical multiplication, PA = 4.81\n• 14,011 QSVT phase factors based on κs = 2, 500 and ϵ = 0.01\n• L2 norm of difference with the classical solution = 2.22 × 10−2\n• Total number qubits is 17 of which 16 are used for encoding PA, (with reference to Figure 2, there are 10 |j⟩\nqubits, 5 |s⟩qubits and 1 |d⟩qubit) and 1 for signal processing.\nAs shown in Figure 9 the large reduction in the number of unique values does not translate into an equivalent reduction\nin the number of rotation gates. For comparison, previous HHL investigations estimated that this case would require 29\nqubits [29]. Without preconditioning, QVST would require of the order 350,000 phase factors, a reduction of a factor of\n25. Without preconditioning the number of rotation gates to block encode A is 4,290. Hence, the overall circuit depth is\nreduced by a factor of 12.5 with preconditioning.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ny-coordinate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nz-coordinate\nScalar Field\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nScalar values\n(a) Classical solution.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ny-coordinate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nz-coordinate\nScalar Field\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nScalar values\n(b) QSVT solution.\nFigure 11: Solution of a 16x16x16 Laplacian matrix system for a scalar field representative of fully developed flow in a\nsquare sided channel. Contours are taken at a slice mid-way along the channel.\nThe second testcase is a 3D 16x16x16 Laplacian matrix system, set up using the open source L-QLES 5 framework\n[30]. The boundary conditions are set up to model fully developed Stokes flow in a square cross-section channel. The\nsubnormalised condition number for encoding A is κs = 123.6. Using SPAI preconditioning with no infill reduces this\nto κs = 65.2. SPAI with one level of infill, which is used for this calculation, further reduces the condition number to\nκs = 36.7.\nFigure 11 compares the classical and QSVT solutions at a cross-section midway along the channel. There is very good\nagreement between the classical and QSVT solutions. The details of the QSVT calculation are:\n• SPAI with one level of infill. The resulting product matrix, PA contains 63 diagonals of which 43 have\nnon-zero entries\n• Diagonal smoothing factor, f = 0.1\n• Number of unique angles reduced from 21,262 to 1,852.\n• Number of non-zero rotations reduced from 103,872 to 41,181.\n• Subnormalisation factor from classical multiplication, PA = 5.48\n5https://github.com/rolls-royce/qc-cfd/tree/main/L-QLES\n14\n\n\nPreconditioned block encodings for quantum linear systems\n• 249 QSVT phase factors based on κs = 40 and ϵ = 0.01\n• L2 norm of difference with the classical solution = 8.2 × 10−3\n• Total number qubits is 20 of which 19 are used for encoding PA, (with reference to Figure 2, there are 12 |j⟩\nqubits, 6 |s⟩qubits and 1 |d⟩qubit) and 1 for signal processing.\nFor comparison, 879 QSVT phase factors are needed to solve the system without preconditioning. However, the\nencoding of A requires only 8,348 rotation gates. Hence, in this case, the factor of 3.5 increase in phase factors is\nmore than offset by the 4.9x saving in the number of rotation gates. The larger saving in rotation gates is due to the\nfact that the Laplacian matrix has a regular structure with many repeated entries, even before smoothing. This is not\nrepresentative of 3D CFD testcases and savings similar to the 2D testcase are expected.\n6\nConclusions\nA thorough analysis of preconditioning has been performed using emulated circuits. Banded diagonal encoding using\nPREP-SELECT to load the diagonals of the matrix produces a low subnormalisation factor and a low qubit overhead.\nBlock encoding P and A separately as a quantum multiplication circuit and encoding the classically multiplied product\nPA have been evaluated. Disappointingly, the subnormalisation factors for the quantum multiplication of P and A\nmean there is no reduction in the effective condition number κs for QSVT. Preamplified quantum multiplication has\nbeen considered but did not offer an advantage for the matrices considered here. This may be different for larger cases\nor non-banded matrices. The circulant preconditioner was beset by the need to encode a diagonal matrix of inverse\neigenvalues with a maximum value over 500 on the largest test case. This has implications for other encodings based on\nsingular value decomposition.\nFor classical multiplication, encoding PA with either TPAI or SPAI results in low subnormalisation factors that are\nlargely independent of the matrix size and the number of levels of infill. SPAI with infill gives the best reduction in κs.\nAn unexpected feature of SPAI with infill is that while the matrix PA has an increasing number of diagonals, over half\nof them contain only zeros. This has the beneficial effect that PA can be encoded with fewer diagonals than separately\nencoding P and A.\nA diagonal filtering technique has been developed with allows circuit operations to be coalesced if certain criteria are\nmet. The percentage reductions in circuit increase with mesh size giving some indication that sub-linear circuit depths\nmay be achievable.\nGiven the initial concerns over subnormalisation factors, it was not obvious that encoding a classical matrix product\nwould lead to a reduction in condition number. Indeed, encoding PA using SPAI with no infill gives only a marginal\nimprovement over the classical condition number. Although SPAI with 3 levels of infill gives an order of magnitude\nreduction, it comes with large classical pre-processing cost. Fortunately, when forming PA in parallel, since the\nencoding operations commute, each parallel process can directly store its chunk of the circuit. This avoids the need to\nperform expensive sparse matrix-matrix products during the classical preconditioning step.\nTwo new emulation results have been achieved that were not previously possible. A 32x32 CFD matrix was emulated\nusing only 17 qubits instead of 29 for HHL. A first 3D case using a Laplacian to approximate fully developed Stokes\nflow has been run on a 16x16x16 mesh.\nAlthough this work has exploited the diagonal matrix structure of lattice-based meshes, for unstructured meshes the\nbenefits of SPAI with infill are expected to be repeated.\n7\nAcknowledgements\nThis work was completed under funding received from the UK’s Commercialising Quantum Technologies Programme.\nThe matrix encoding was developed under Grant reference 10004857. The preconditioning was developed under Grant\nreference 10071684.\nThe permission of Rolls-Royce and Riverlane to publish this work is gratefully acknowledged.\n15\n\n\nPreconditioned block encodings for quantum linear systems\nReferences\n[1] G. H. Golub and C. F. Van Loan, Matrix computations. JHU press, 2013.\n[2] Y. Saad, Iterative methods for sparse linear systems. SIAM, 2003.\n[3] H. A. Van der Vorst, “Bi-cgstab: A fast and smoothly converging variant of bi-cg for the solution of nonsymmetric\nlinear systems,” SIAM Journal on scientific and Statistical Computing, vol. 13, no. 2, pp. 631–644, 1992.\n[4] A. W. Harrow, A. Hassidim, and S. Lloyd, “Quantum algorithm for linear systems of equations,” Physical review\nletters, vol. 103, no. 15, p. 150502, 2009.\n[5] A. M. Childs, R. Kothari, and R. D. Somma, “Quantum algorithm for systems of linear equations with exponentially\nimproved dependence on precision,” SIAM Journal on Computing, vol. 46, no. 6, pp. 1920–1950, 2017.\n[6] A. Gilyén, Y. Su, G. H. Low, and N. Wiebe, “Quantum singular value transformation and beyond: exponential\nimprovements for quantum matrix arithmetics,” in Proceedings of the 51st Annual ACM SIGACT Symposium on\nTheory of Computing, pp. 193–204, 2019.\n[7] J. M. Martyn, Z. M. Rossi, A. K. Tan, and I. L. Chuang, “Grand unification of quantum algorithms,” PRX Quantum,\nvol. 2, p. 040203, Dec 2021.\n[8] P. C. Costa, D. An, Y. R. Sanders, Y. Su, R. Babbush, and D. W. Berry, “Optimal scaling quantum linear-systems\nsolver via discrete adiabatic theorem,” PRX quantum, vol. 3, no. 4, p. 040303, 2022.\n[9] S. Aaronson, “Read the fine print,” Nature Physics, vol. 11, no. 4, pp. 291–293, 2015.\n[10] J. R. Shewchuk et al., An introduction to the conjugate gradient method without the agonizing pain. Carnegie-\nMellon University. Department of Computer Science Pittsburgh, 1994.\n[11] Y. Saad and M. H. Schultz, “Gmres: A generalized minimal residual algorithm for solving nonsymmetric linear\nsystems,” SIAM Journal on scientific and statistical computing, vol. 7, no. 3, pp. 856–869, 1986.\n[12] B. D. Clader, B. C. Jacobs, and C. R. Sprouse, “Preconditioned quantum linear system algorithm,” Physical review\nletters, vol. 110, no. 25, p. 250504, 2013.\n[13] L.-C. Wan, C.-H. Yu, S.-J. Pan, F. Gao, Q.-Y. Wen, and S.-J. Qin, “Asymptotic quantum algorithm for the toeplitz\nsystems,” Physical Review A, vol. 97, no. 6, p. 062322, 2018.\n[14] C. Shao and H. Xiang, “Quantum circulant preconditioner for a linear system of equations,” Physical Review A,\nvol. 98, no. 6, p. 062321, 2018.\n[15] A. Hosaka, K. Yanagisawa, S. Koshikawa, I. Kudo, X. Alifu, and T. Yoshida, “Preconditioning for a variational\nquantum linear solver,” arXiv preprint arXiv:2312.15657, 2023.\n[16] Y. Tong, D. An, N. Wiebe, and L. Lin, “Fast inversion, preconditioned quantum linear system solvers, fast green’s-\nfunction computation, and fast evaluation of matrix functions,” Physical Review A, vol. 104, no. 3, p. 032422,\n2021.\n[17] J. Golden, D. O’Malley, and H. Viswanathan, “Quantum computing and preconditioners for hydrological linear\nsystems,” Scientific Reports, vol. 12, no. 1, p. 22285, 2022.\n[18] B. F. Nielsen, A. Tveito, and W. Hackbusch, “Preconditioning by inverting the laplacian: an analysis of the\neigenvalues,” IMA journal of numerical analysis, vol. 29, no. 1, pp. 24–42, 2009.\n[19] T. Gergelits, K.-A. Mardal, B. F. Nielsen, and Z. Strakos, “Laplacian preconditioning of elliptic pdes: Localization\nof the eigenvalues of the discretized operator,” SIAM Journal on Numerical Analysis, vol. 57, no. 3, pp. 1369–1394,\n2019.\n[20] I. Gutman and W. Xiao, “Generalized inverse of the laplacian matrix and some applications,” Bulletin (Académie\nserbe des sciences et des arts. Classe des sciences mathématiques et naturelles. Sciences mathématiques), pp. 15–\n23, 2004.\n[21] J. Chanzy, “Inverse du laplacien discret dans le problème de poisson-dirichlet à deux dimensions sur un rectangle,”\nin Annales de la Faculté des sciences de Toulouse: Mathématiques, vol. 15, pp. 485–552, 2006.\n[22] C. Sünderhauf, E. Campbell, and J. Camps, “Block-encoding structured matrices for data input in quantum\ncomputing,” Quantum, vol. 8, p. 1226, 2024.\n[23] B. D. Clader, A. M. Dalzell, N. Stamatopoulos, G. Salton, M. Berta, and W. J. Zeng, “Quantum resources required\nto block-encode a matrix of classical data,” IEEE Transactions on Quantum Engineering, vol. 3, pp. 1–23, 2022.\n[24] L. Lin, “Lecture notes on quantum algorithms for scientific computation,” arXiv preprint arXiv:2201.08309, 2022.\n16\n\n\nPreconditioned block encodings for quantum linear systems\n[25] L. Lapworth, “Evaluation of block encoding for sparse matrix inversion using qsvt,” arXiv preprint\narXiv:2402.17529, 2024.\n[26] A. Gilyén, Y. Su, G. H. Low, and N. Wiebe, “Quantum singular value transformation and beyond: exponential\nimprovements for quantum matrix arithmetics,” arXiv preprint arXiv:1806.01838, 2018.\n[27] Y. Dong, X. Meng, K. B. Whaley, and L. Lin, “Efficient phase-factor evaluation in quantum signal processing,”\nPhysical Review A, vol. 103, no. 4, p. 042419, 2021.\n[28] Y. Dong, L. Lin, H. Ni, and J. Wang, “Robust iterative method for symmetric quantum signal processing in all\nparameter regimes,” arXiv preprint arXiv:2307.12468, 2023.\n[29] L. Lapworth, “A hybrid quantum-classical cfd methodology with benchmark hhl solutions,” arXiv preprint\narXiv:2206.00419, 2022.\n[30] L. Lapworth, “L-qles: Sparse laplacian generator for evaluating quantum linear equation solvers,” arXiv preprint\narXiv:2402.12266, 2024.\n[31] M. Benson, J. Krettmann, and M. Wright, “Parallel algorithms for the solution of certain large sparse linear\nsystems,” International journal of computer mathematics, vol. 16, no. 4, pp. 245–260, 1984.\n[32] E. Chow and Y. Saad, “Approximate inverse preconditioners via sparse-sparse iterations,” SIAM Journal on\nScientific Computing, vol. 19, no. 3, pp. 995–1023, 1998.\n[33] M. J. Grote and T. Huckle, “Parallel preconditioning with sparse approximate inverses,” SIAM Journal on Scientific\nComputing, vol. 18, no. 3, pp. 838–853, 1997.\n[34] S. Suresh and K. Suresh, “Computing a sparse approximate inverse on quantum annealing machines,” arXiv\npreprint arXiv:2310.02388, 2023.\n[35] G. Strang, “A proposal for toeplitz matrix calculations,” Studies in Applied Mathematics, vol. 74, no. 2, pp. 171–\n176, 1986.\n[36] T. F. Chan, “An optimal circulant preconditioner for toeplitz systems,” SIAM journal on scientific and statistical\ncomputing, vol. 9, no. 4, pp. 766–771, 1988.\n17\n\n\nPreconditioned block encodings for quantum linear systems\nA\nDiagonal preconditioning\nDiagonal preconditioning is based on scaling the rows and columns of a matrix by the inverse of the diagonal. If D is\nthe matrix formed by the diagonal entries in A, then preconditioning is applied by:\n\u0000D−1A\n\u0001\n|x⟩= D−1 |b⟩\n(11)\nAn alternative is to take Ds as the diagonal matrix of the square root of the diagonal entries which gives:\n\u0000D−1\ns AD−1\ns\n\u0001\nDs |x⟩= D−1\ns\n|b⟩\n(12)\nThe difference between the two formulations is that Equation (11) scales the rows of A and Equation (12) scales both\nthe rows and columns. The former is preferred as the solution vector, |x⟩, is left unchanged. Classically, this is also\nreferred to as Jacobi preconditioning but is rarely used as there are better performing preconditioners which yield\na much greater reduction in the condition number. However, for QLES, the benefit is that as well as setting all the\ndiagonal entries to unity, diagonal scaling tends to equalise other entries in the matrix. This is a necessary step prior to\nthe Toeplitz and circulant approximate inverses discussed in Appendix C and Appendix D.\nNote that diagonal preconditioning is not the same as diagonal encoding.\nB\nSparse Approximate Inverse (SPAI)\nThe objective of SPAI [31, 32, 2] is to find a preconditioning matrix M that minimises a function F(M) such that:\nF(M) = ||I −AM||2\nF\n(13)\nWhere the subscript F the Frobenius norm of a matrix A defined by:\n||A||F =\nsX\ni,j\n|ai,j|2 =\nq\nTr(AA†)\n(14)\nF(M) is minimised when M is close to A−1. There are two approaches to this described below.\nB.1\nSparse-Sparse Iteration Method\nThe first method uses the global minimal residual descent method [32]. After an appropriate M0 has been selected, the\nfollowing iterations are performed.\nGk\n=\nI −AMk\nαk\n=\ntr(G†\nkAGK)/||AGk||2\nF\nMk+1\n=\nMk + αkGk\n(15)\nEach iteration of the algorithm adds new non-zero entries and reduces the sparsity of Mk as shown in Figure 12. This is\ncalled infill and the degree of infill is controlled via numerical dropping, i.e. ignoring any entries that fall outside the\ninfill sparsity pattern. This can be applied to either Mk or Gk [32]. A common choice is to retain the same sparsity\npattern for Mk as Ak.\nThe recommended starting approximation [32] is M0 = α0AT , where\nα0 =\n||A||F\n||AA†||F\n(16)\nB.2\nColumn Minimisation Method\nThe second approach separates the minimisation into a set of independent column based operations that can be easily\nparallelised on a classical computer [33].\n18\n\n\nPreconditioned block encodings for quantum linear systems\n130\n140\n150\n160\n170\n180\n130\n140\n150\n160\n170\nOriginal matrix, k = 3440.64\n120\n130\n140\n150\n160\n170\n180\n130\n140\n150\n160\n170\nPreconditioning matrix\n120\n130\n140\n150\n160\n170\n180\n120\n130\n140\n150\n160\n170\n180\nPreconditioned matrix, k = 1640.58\nSPAI with 1 levels\nFigure 12: Zoomed in sparsity patterns for SPAI applied to 16x16 pressure correction matrix with 1 level of infill. Left:\noriginal matrix (A), centre: preconditioning matrix (P), right: result of preconditioning (PA)\nSolving AA−1 = I for A−1 can be expressed in terms of the column vectors yj of A−1 as:\nAyj = ej\n(j = 0, N −1)\n(17)\nwhere ej is the jth column of the identity matrix. If M is an approximation to A−1 then:\nAmj ≃ej\n(j = 0, N −1)\n(18)\nFor a given sparsity pattern, the entries in mj can be reordered to place all s non-zero entries, ˆmj first:\nmj =\n\u0012\nˆmj\n0\n\u0013\n(19)\nEquation (19) can be achieved by a permutation operator, Pj that can be applied to A to give:\nPjAP −1\nj\n=\n\u0012 ˆAj\n∗\n∗\n∗\n\u0013\n(20)\nAnd to ej to give:\nPjej =\n\u0012\nˆej\n0\n\u0013\n(21)\nFrom which we have N small s × s systems to solve:\nˆAj ˆmj = ˆej\n(j = 0, N −1)\n(22)\nThe size of s depends on the level of infill. For small values of s, Equation (22) can be directly inverted. For larger\nvalues, it can be solved via a least squares minimisation [12] or by finding the minimum of an energy functional [34].\nThe key to performance is that the N equations are independent and can be solved in parallel. Here, we use a direct\nLower-Upper decomposition to solve Equation (22) exactly to within machine precision.\nThe infill pattern for the column minimisation method is the same as the sparse-sparse method and is computed by\ntaking successive products with itself. This is an expensive task but can be performed as a one-off calculation. Since\nonly the sparisty pattern is needed, it is sufficient to know whether each row-column multiplication has a common entry.\nFinding a preconditioner of the form MA follows the same procedure except that mj and ej are row vectors and we\nsolve:\nˆmj ˆAj = ˆej\n(j = 0, N −1)\n(23)\n19\n\n\nPreconditioned block encodings for quantum linear systems\nC\nToeplitz Approximate Inverse (TPAI)\nA Toeplitz matrix has the form:\n\n\n\n\n\n\n\n\nt0\nt−1\n. . .\nt2−n\nt1−n\nt1\nt0\nt−1\n...\nt2−n\n...\nt1\nt0\n...\n...\ntn−2\n...\n...\n...\nt−1\ntn−1\ntn−2\n. . .\nt1\nt0\n\n\n\n\n\n\n\n\n(24)\nThe Toeplitz Approximate Inverse (TPAI) is inspired by the SPAI column minimisation method from Appendix B.2.\nHowever, it makes the further approximation that the inverse is itself a Toeplitz matrix. This means that only one s × s\nsystem needs to be inverted instead of N. More importantly from a quantum perspective, the TPAI can be encoded with\ncircuit depth of O(d) where d is the number of diagonals, rather than O(dN) for SPAI.\nThe TPAI method begins by approximating A with a Toeplitz matrix:\nˆA = Sdiag(D−1A)\n(25)\nWhere D is the diagonal scaling matrix described in Appendix A. For the CFD matrices considered here, this creates\nones along the main diagonal and tends to equalise the entries along the off-diagonals. Sdiag is a function that takes the\naverage value along each diagonal and uses the result for the value along the corresponding Toeplitz diagonal. This\napproach is only appropriate to matrices that have a predominantly diagonal sparsity pattern.\nIt is further assumed that ˆA and its TPAI have infinite dimension. To illustrate consider the case when ˆA is tridiagonal\nwith entries a, b, c and the approximate inverse is also tridiagonal with entries ai, bi, ci. Multiplying a row of the inverse\nwith a 3x3 block of ˆA gives a row of the identity matrix:\n(ai\nbi\nci)\n b\nc\n0\na\nb\nc\n0\na\nb\n!\n= (0\n1\n0)\n(26)\nThe inverse of the 3x3 block matrix is:\n1\nb3 −2abc\n\n\nb2 −ac\n−bc\nc2\n−ab\nb2\n−bc\na2\n−ab\nb2 −ac\n\n\n(27)\nSolving for the approximate inverse gives:\nbi\n=\nb/(b2 −2ac)\nai\n=\n−abi/b\nci\n=\n−cbi/b\n(28)\nInfill is treated the same as SPAI and is illustrated with a pentadiagonal preconditioner for a tridiagonal ˆA where a 5 × 5\nsystem is solved:\n(ai\nbi\nci\ndi\nei)\n\n\n\n\n\nb\nc\n0\n0\n0\na\nb\nc\n0\n0\n0\na\nb\nc\n0\n0\n0\na\nb\nc\n0\n0\n0\na\nb\n\n\n\n\n= (0\n0\n1\n0\n0)\n(29)\nThe inverse of the 5x5 block matrix is:\n20\n\n\nPreconditioned block encodings for quantum linear systems\n1\nD\n\n\n\n\n\nb4 −3ab2c + a2c2\n−b3c + 2abc2\nb2c2 −ac3\n−bc3\nc4\n−ab3 + 2a2bc\nb4 −2ab2c\n−b3c + abc2\nb2c2\n−bc3\na2b2 −a3c\n−ab3 + a2bc\nb4 −2ab2c + a2c2\n−b3c + abc2\nb2c2 −ac3\n−a3b\na2b2\n−ab3 + a2bc\nb4 −2ab2c\n−b3c + 2abc2\na4\n−a3b\na2b2 −a3c\n−ab3 + 2a2bc\nb4 −3ab2c + a2c2\n\n\n\n\n\n(30)\nwhere, the determinant D is:\nD = b5 −4ab3c + 3a2bc2\n(31)\nSolving for the approximate inverse gives:\nai\n=\n(a2b2 −a3c)/D\nbi\n=\n(−ab3 + a2bc)/D\nci\n=\n(b4 −2ab2c + a2c2)/D\ndi\n=\n(−b3c + abc2)/D\nei\n=\n(b2c2 −ac3)/D\n(32)\nFor greater levels of infill, the s×s system is solved using a Lower-Upper decomposition [1]. Note that the preconditioner\nis applied to D−1A and not ˆA.\nThe infill for TPAI simply adds new diagonals adjacent to the existing diagonals. This results in lower levels of infill as\nSPAI. As reported in the main text, this leads to trade-off between the level of reduction in the condition number and\nthe subnormalisation factor for encoding the preconditioning matrix.\nD\nCirculant Approximate Inverse\nA circulant matrix is a special case of a Toeplitz matrix and has the form:\n\n\n\n\n\n\n\n\nc0\ncn−1\n. . .\nc2\nc1\nc1\nc0\ncn−1\n...\nc2\n...\nc1\nc0\n...\n...\ncn−2\n...\n...\n...\ncn−1\ncn−1\ncn−2\n. . .\nc1\nc0\n\n\n\n\n\n\n\n\n(33)\nFor an arbitrary matrix A, the circulant preconditioner has the form [35, 36]:\nC(A) = F †diag(FAF †)F\n(34)\nwhere Fjk = ωjk\n√\nN and ω = e−2πi/N. F is effectively the Quantum Fourier Transform (QFT) operator. The diagonal\nterm can be computed from:\nΛk = diag(FAF †)k = 1\nn\nX\np,q\nω(p−q)kAp,q\n(35)\nThe preconditioned system is C−1Ax = C−1b. Hence, an efficient way to compute C−1 is needed. From Equation (34)\nand Equation (35):\nC−1 = FΛ−1F †\n(36)\nThe benefit of the circulant preconditioner is that C−1 can be efficiently implemented on a quantum computer.\nE\nDouble pass filtering to create equal values\nWhen filtering or smoothing entries in the matrix, it is important to respect the underlying discretisation on which the\nmatrix is based. Non-uniform meshes can create mesh volumes that vary by several orders of magnitude. Variations in\nflow properties, e.g. between the free-stream and near the walls can also be significant. Hence, matrix entries that are\nclose to zero cannot be arbitrarily rounded to zero.\n21\n\n\nPreconditioned block encodings for quantum linear systems\nE.1\nDigitising the matrix entries\nTo avoid rounding non-zero values to zero, the digitisation algorithm creates bins based on percentages of the mean\nvalue within each bin. The principle is illustrated in Figure 13.\n+5%\n-5%\nMid-point\nState vector index\nAmplitude\nFigure 13: Bin selection for digitising the matrix entries: blue = entries in ascending order, yellow = +5%, grey = −5%.\nBox indicates the extent of the bin. - all blue values in the bin are reset to the mid-point value.\nThe algorithm initially applies a convolution operator to the ordered state amplitudes. This creates a number of potential\nbins, all of which meet the criteria that the values in each bin differ by no more than a given percentage, e.g. 10%\nas shown in Figure 13. At this stage, there are a number of overlapping bins. A marching algorithm then eliminates\nany overlaps by retaining bins with the largest number of entries. As with most marching algorithms, it is a greedy\nalgorithm and is not guaranteed to produce the minimum number of bins and/or the maximum number of repeats.\nE.2\nCollapsing equal angled rotation gates\nHaving created a circuit with a large number of equal angle rotations, the final step is to collapse as many pairs of\ncontrolled rotations as possible [25]. The requirements to coalesce two controlled rotations:\n• Both rotations must belong to the same matrix diagonal.\n• Both rotations must have the same angle. In practice, this is relaxed by a small amount to allow for rounding\nerror.\n• The bit patterns of the controls on each rotation must have a Hamming distance of 1.\nThe collapsing of equal-valued rotations is shown in Figure 14.\n|q0⟩\n|q1⟩\n|q2⟩\n|q2⟩\nRy\nRy\n=\nRy\nFigure 14: Coalescing multi-qubit controlled gates with a Hamming distance of 1 where the Ry gates have the same\nrotation angle.\nWithin each bin, the coalescing process is as follows:\n• Collect all operations that are a Hamming distance of 1 away from any other operation.\n• Create a binary tree based on the multiplexed controls.\n• Sweep through the levels of the tree, coalescing branches that have a Hamming distance of 1.\nE.3\nPerformance\nThe performance of filtering and collapsing algorithm is illustrated in Figure 15. In this example, the bin contains 56\nequal-valued multiplexed rotations, all of which are a Hamming distance of 1 away from at least one other rotation. The\n22\n\n\nPreconditioned block encodings for quantum linear systems\ncollapsing algorithm reduces this to 34 rotations. Examination of Figure 15a shows that there are 16 rotations that have\nnot been modified and none are within a Hamming distance of 1 of the other. Note that while the matrix entries have\nbeen reordered to be monotonic, this does not mean that controls correspond to a contiguous set of integers.\nFor larger bins, the relative performance is seen to improve. For a bin of 145 equal-valued rotations, the collapsing\nalgorithm reduces this to 45. As the size of the matrix increases, so does the size of the bins resulting in the greater\npercentage reductions shown in Figure 9.\n0 40\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n(a) Binary tree for untrimmed circuit. All paths in the tree are a Hamming distance of 1 away from at least one other path.\n33\n0\n1 2 3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 23\n24\n25\n26\n27\n28\n29 30\n31\n32\n(b) Binary tree for trimmed circuit. Grey nodes correspond to controls that have been removed.\nFigure 15: Performance of the circuit trimming algorithm, displayed as a binary tree. Each path from the lowest leaves\nto the root represents a controlled rotation.\nFigure 16 shows the circuit with the multiplexed rotations trimmed according to the tree in Figure 15b. The labels on\nthe circuit gates correspcond to paths through the tree.\n|q0⟩\n|q1⟩\n|q2⟩\n|q3⟩\n|q4⟩\n|q5⟩\n|q6⟩\n|q7⟩\n|q8⟩\n|q9⟩\n|q10⟩\n0\nRy\n1\nRy\n2\nRy\n3\nRy\n4\nRy\n5\nRy\n6\nRy\n7\nRy\n8\nRy\n9\nRy\n10\nRy\n11\nRy\n12\nRy\n13\nRy\n14\nRy\n15\nRy\n16\nRy\n17\nRy\n18\nRy\n19\nRy\n20\nRy\n21\nRy\n22\nRy\n23\nRy\n24\nRy\n25\nRy\n26\nRy\n27\nRy\n28\nRy\n29\nRy\n30\nRy\n31\nRy\n32\nRy\n33\nRy\nFigure 16: Section of matrix loading circuit after trimming. All Ry gates have the same rotation angle. The gate indices\ncorrespond to the path labels in Figure 15b\n23\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20908v1.pdf",
    "total_pages": 23,
    "title": "Preconditioned Block Encodings for Quantum Linear Systems",
    "authors": [
      "Leigh Lapworth",
      "Christoph Sünderhauf"
    ],
    "abstract": "Quantum linear system solvers like the Quantum Singular Value Transformation\n(QSVT) require a block encoding of the system matrix $A$ within a unitary\noperator $U_A$. Unfortunately, block encoding often results in significant\nsubnormalisation and increase in the matrix's effective condition number\n$\\kappa$, affecting the efficiency of solvers. Matrix preconditioning is a\nwell-established classical technique to reduce $\\kappa$ by multiplying $A$ by a\npreconditioner $P$. Here, we study quantum preconditioning for block encodings.\nWe consider four preconditioners and two encoding approaches: (a) separately\nencoding $A$ and its preconditioner $P$, followed by quantum multiplication,\nand (b) classically multiplying $A$ and $P$ before encoding the product in\n$U_{PA}$. Their impact on subnormalisation factors and condition number\n$\\kappa$ are analysed using practical matrices from Computational Fluid\nDynamics (CFD). Our results show that (a) quantum multiplication introduces\nexcessive subnormalisation factors, negating improvements in $\\kappa$. We\nintroduce preamplified quantum multiplication to reduce subnormalisation, which\nis of independent interest. Conversely, we see that (b) encoding of the\nclassical product can significantly improve the effective condition number\nusing the Sparse Approximate Inverse preconditioner with infill. Further, we\nintroduce a new matrix filtering technique that reduces the circuit depth\nwithout adversely affecting the matrix solution. We apply these methods to\nreduce the number of QSVT phase factors by a factor of 25 for an example CFD\nmatrix of size 1024x1024.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}