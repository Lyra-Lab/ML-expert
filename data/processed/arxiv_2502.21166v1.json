{
  "id": "arxiv_2502.21166v1",
  "text": "Autonomous Curriculum Design via Relative\nEntropy Based Task Modifications\nMuhammed Yusuf Satici1*, Jianxun Wang1 and David L. Roberts1\n1*Department of Computer Science, NC State University, Raleigh, North\nCarolina, USA.\n*Corresponding author(s). E-mail(s): msatici@ncsu.edu ;\nContributing authors: jwang75@ncsu.edu; robertsd@csc.ncsu.edu;\nAbstract\nCurriculum learning is a training method in which an agent is first trained on\na curriculum of relatively simple tasks related to a target task in an effort to\nshorten the time required to train on the target task. Autonomous curriculum\ndesign involves the design of such curriculum with no reliance on human knowl-\nedge and/or expertise. Finding an efficient and effective way of autonomously\ndesigning curricula remains an open problem. We propose a novel approach for\nautomatically designing curricula by leveraging the learner’s uncertainty to select\ncurricula tasks. Our approach measures the uncertainty in the learner’s policy\nusing relative entropy, and guides the agent to states of high uncertainty to facil-\nitate learning. Our algorithm supports the generation of autonomous curricula in\na self-assessed manner by leveraging the learner’s past and current policies but it\nalso allows the use of teacher guided design in an instructive setting. We provide\ntheoretical guarantees for the convergence of our algorithm using two time-scale\noptimization processes. Results show that our algorithm outperforms randomly\ngenerated curriculum, and learning directly on the target task as well as the\ncurriculum-learning criteria existing in literature. We also present two additional\nheuristic distance measures that could be combined with our relative-entropy\napproach for further performance improvements.\nKeywords: Curriculum Learning, Autonomous Curriculum Design, Relative Entropy,\nTransfer Learning, Deep Reinforcement Learning\n1\narXiv:2502.21166v1  [cs.LG]  28 Feb 2025\n\n\n1 Introduction\nCurriculum Learning tries to simplify the learning process of an agent on a difficult-\nto-learn target task by leveraging the knowledge the agent obtains from learning on a\nsequence of source tasks related to the target task. Curriculum design for a reinforce-\nment learning problem can be performed either by manually selecting the tasks or by\nautomated curriculum design algorithms. Automated curriculum design algorithms\nare more efficient in choosing tasks as they eliminate the time spent on manual task\ngeneration. On the other hand, automated curriculum generation requires the craft-\ning of a selection criteria that can leverage what the agent currently knows and what\nneeds to be learned to converge to the optimal policy faster. By employing autonomous\ncurricula design techniques, the agent aims to attain improved learning efficiency and\nreduced dependence on human expertise in generation of curricula and achieves this\npurpose by adaptively modifying the curriculum based on its level of knowledge dur-\ning training. We present Relative Entropy based Autonomous Design of Curricula\n(READ-C), a autonomous-curriculum-design framework using the divergence between\npolicies to identify states with high epistemic uncertainty where additional learning\ncould improve the performance of the agent.\nInspiration for READ-C comes from psychology learning theory. Studies have\nshown that using curriculum-design strategies that reduce students’ uncertainty\nimproves learning efficiency and knowledge acquisition [1–4], and it is often advanta-\ngeous for students to successively learn concepts in accordance with some guidance\nto keep them in the zone of proximal development [5, 6]. Furthermore, entropy-based\nsampling approaches have already been used, mostly in the context of supervised\nlearning and active learning, to query data points about which the agent has the most\nuncertainty [7]. We hypothesized that a curriculum-design strategy that reduces the\nlearner’s epistemic uncertainty using the relative entropy (KL divergence) between the\nagent policy and the true policy could provide a significant performance improvement\nto the agent without resulting in a high overhead of curriculum generation.\nREAD-C, at a high level, measures the relative entropy of the regions in the agent’s\nstate space using probability values derived from the agent’s policy representation and\nprobability values derived from the true policy, and selects the states of high uncer-\ntainty for the curriculum generation using heuristics based on the relative entropy\ncriteria. It assumes the agent has the ability to change its starting state in the given\nenvironment and modifies the Markov decision process (MDP) by moving the start\nstate to a discovered state of high uncertainty for the generation of each curriculum\nstep. In this setting, relative entropy encapsulates, for each part of the agent’s state\nspace, how much the learner’s current policy diverges from a true policy standing for\nan optimal or near-optimal solution for the domain at hand. The higher the diver-\ngence for a region or state of the MDP, the more likely the learner does not have an\noptimal policy for that region. READ-C then modifies the target task to encourage\nlearning in the selected areas of high uncertainty.\nWe offer two different realizations of the true policy for READ-C, namely READ-\nC-TD for teacher dependent entropy calculation and READ-C-SA for self-assessed\nentropy calculation. READ-C-TD assumes the existence of a teacher model having\nlearnt the optimal policy and works in a similar fashion as uncertainty-aware active\n2\n\n\nlearning approaches where the agent picks samples for a teacher to label based on\nthe uncertainty of its learning process w.r.t. various entropy metrics including relative\nentropy [8]. READ-C-TD uses a student-teacher architecture where the teacher pro-\nvides the optimal policy for the relative entropy calculation and the student obtains\nthe uncertainty estimates based on the relative entropy between the student and\nteacher policies. READ-C-SA, on the other hand, solely relies on the information gen-\nerated during the agent’s training process and estimates the relative entropy using a\nregression model with information from the current and past policies of the agent to\nmitigate the reliance of the algorithm on already existing policies. Since true policy is\noften unknown during the training process, using a regressor to estimate the uncer-\ntainty values allows the agent to not require the existence of a teacher model for the\ngeneration of curricula. The regression model learns from a simpler environment that\nresembles the target task using the relative entropy between the agent policy and the\ntrue policy of the simpler environment as ground truth values in calculation of the\nuncertainty.\nFurthermore, READ-C affords different heuristic criteria for prioritizing the\nregions of uncertainty, and we present three such criteria: 1) directly using relative\nentropy, 2) relative entropy filtered by proximity to a goal state, and 3) relative\nentropy filtered by distance to other low-relative-entropy regions. We provide a proof\nof convergence for the curriculum learning algorithm and explain how the selection\nof curricula does not change the convergence of the reinforcement learning agent. We\nevaluate READ-C against randomly selected curricula, learning directly on the tar-\nget task, and a curriculum-generation criteria from the literature. Results show that\nREAD-C outperforms the baseline algorithms and the existing curriculum-selection\ncriteria in many cases, and at worst performs similarly to existing algorithms while\nreducing curriculum-generation overhead.\nThe main contributions of our research are\n• We present a novel entropy-based curriculum generation algorithm for the purpose\nof facilitating the learning process of a reinforcement learning agent.\n• We provide two realizations of this curriculum learning algorithm, a teacher-student\nframework and a self-assessed approach, to reduce the dependence of the curriculum\nlearning methods on already learnt policies.\n• We provide a novel convergence proof for our curriculum generation algorithm\nto have a theoretical guarantee for the convergence of our reinforcement learning\nmodel.\n• We provide extensive evaluations of our algorithm against curriculum learning and\nreinforcement learning methods from the literature in three different domains with\nvarying degrees of difficulty.\n2 Related Work\nCurriculum-design techniques are either automated or use a human-in-the-loop\nparadigm. Human-designed approaches focus on understanding how humans teach\nusing curricula [9], and use human knowledge to exploit aspects of the target task\nthat might improve training efficiency [10, 11]. Automated approaches generate a\n3\n\n\nsequence of tasks, samples, or states without human intervention. We can further\ndivide autonomous approaches into subgroups based on the type of curricula they\ngenerate. Some algorithms provide learners a meaningful sequence of data samples\nor demonstrations [12, 13], while others generate new initial/goal/terminal states at\nevery curriculum step [14, 15]. Other approaches search a parameterized task space by\nsampling environment parameters based on the agent’s current knowledge using con-\ntextual reinforcement learning [16, 17]. Some further approaches attempt to remove\nthe dependence on the parameterized task distributions by using optimal transport\nto generate a curriculum in a contextual RL setting [18, 19]. Some lifelong learn-\ning approaches similar to these curriculum learning techniques decompose the task\nspace into subregions based on diverse model primitives and learn subpolicies for each\nregion of the task space in a bottom-up manner to sequentially solve multiple tasks\n[20]. Task-sequencing algorithms either heuristically pre-generate a sequence of tasks\nbefore training [21, 22] or adaptively generate curricula by leveraging the learner’s\nknowledge at various points during training [23–25]. READ-C uses adaptive modifica-\ntions of the agent’s start state, so our focus will be on adaptive curriculum-generation\nalgorithms that perform modifications to the target task.\nAdaptive curriculum-generation algorithms train on the target task and use the\nknowledge they obtain to determine which tasks and/or states benefit the learner. An\napproach similar to READ-C is reversed-curriculum learning that generates a curricu-\nlum of start states by sequentially expanding backwards from the goal state [14]. In\ncontrast, READ-C uses heuristic selection of start states based on the learner’s rela-\ntive uncertainty. Another approach samples start states proportional to the Euclidean\nnorm of the gradient of a performance measure defined on the value function, guiding\nthe learner towards regions of the environment where nearby states have wildly-\ndifferent value function estimates [26]. It requires learning a start state selection policy\nin addition to the RL policy, which increases the training time. Finally, Narvekar et al.\n[25] present an adaptive task sequencing algorithm that selects curriculum tasks based\non the maximal change in the learner’s policy, guiding the agent towards tasks that\nare expected to improve the policy most. This approach requires the learner to train\non the target task and every source task for every curriculum step, which results in\nsignificant curriculum-generation overhead. See Narvekar et al. [27] for more analysis\nof curriculum learning approaches.\nREAD-C-TD also relates to active learning in that it allows a student to query\na teacher’s policy for the purpose of identifying high uncertainty regions. Settles and\nCraven [8] use KL Divergence of an ensemble of models to identify the samples that\nthe model is least confident in how to label. Madhawa and Murata [28] test multi-\nple uncertainty measures including information gain between the model predictions\nand model posterior distribution for graph neural networks. Our algorithm shows\nsimilarities to these approaches in the way it uses relative entropy to determine the\nuncertainty of the agent regarding its learning process. However, we adapt this active\nlearning process to a reinforcement learning problem and measure the agent’s uncer-\ntainty in its state space rather than data instances. Bougie and Ichise [29] offers\nan active learning framework using goal-driven demonstrations where the RL agent\n4\n\n\nidentifies in which states the feedback is most needed based on Bayesian or quan-\ntile confidence and asks for guidance from an expert only for the states it has low\nconfidence of reaching its goal. In contrast, we offer a curriculum learning framework\nthat generates a curriculum for the RL agent without using demonstrations from an\nexpert through a self-assessed learning approach. Settles [7] explains more about the\ntraditional active learning approaches.\nREAD-C relates to options learning literature due to the interleaved learning pro-\ncess it performs based on the uncertainty of the agent over different regions of the\nstate space. Options in the context of options learning represent higher-level actions or\nsub-policies that the agent learns to execute. By using these options, the agent man-\nages to skip parts of the training process where it already has a good understanding\nof optimal policy. The traditional options learning algorithms use tabular Q-learning\nand planning methods to explore the option space [30, 31]. Adaptive Skills, Adaptive\nPartitions framework simultaneously learns options for sub-regions of the state space\nusing a modified gradient calculations based on the hyperplanes that partition the\nstate space into subtasks [32]. Another sparse sampling approach uses a generative\nmodel to sparsely sample a series of states that would construct a small MDP to be\nused for planning the optimal path [33]. In more recent deep-RL research, actor-critic\narchitectures have been used to model the agent and the options policies [34–36].\nBacon et al. offers an option-critic architecture where they train the option networks\nusing stochastic gradient descent similar to how an actor would learn and they use a\nplanner to determine which option policy to run at the current step of training [35].\nHarb et al. improves upon the option-critic by using a deliberation cost model instead\nof a planner to select good options [34]. Harutyunyan et al. adds a termination critic\nto the option-critic architecture to optimize the termination condition of the options\nlearning framework [36]. Although these options learning algorithms show similari-\nties to READ-C, their focus lies on skipping the actions that do not offer additional\ninformation to the agent whereas the curriculum learning algorithm tries to bring the\nagent close to the regions of the state space where the learning could improve the\npolicy the most.\n3 Problem Formulation\nHere we provide background on the RL method and formally define the curriculum-\nlearning problem in this context.\n3.1 Reinforcement Learning\nWe model the learning process as a Markov decision process M = < S, A, T, R,\nsi, Sg, γ >, a tuple consisting of a set of states S, a set of actions A, a transition\nfunction T, a reward function R, an initial state si, a set of terminal states Sg and a\ndiscount factor γ. The transition function T : S × A × S →[0, 1] corresponds to the\nprobability of transitioning from a state s ∈S to another state s′ ∈S using a valid\naction a ∈A at state s. All of the environments we use are deterministic, so taking\naction a in state s always transitions into the same resulting state s′—although that\nis not a requirement for READ-C. The reward function R : S ×A×S →F maps from\n5\n\n\na state, action, state tuple to a real number corresponding to the reward signal the\nlearner receives. The policy π : S →A maps states to actions. The cumulative reward\nG at time t is the discounted sum of all feedback the agent receives from t until it\nreaches a terminal state,\nGt =\nτ−t\nX\nk=0\nγkRt+k\nwhere γ is the discount factor, τ is the time to reach a terminal state, and Rt+k is\nthe feedback received at state st+k [37]. The agent’s objective is to learn the optimal\npolicy π∗\nM that maximizes G.\nFor discrete action spaces, we train using dual deep Q-networks (Dual-DQN) [38].\nWe use two four-layered, fully-connected, feed-forward networks. The networks take\nthe states as input and output Q-value estimates for each available action. One neural\nnetwork serves as the learned model, and the other provides target Q-value estimates\nfor batch updates. The loss function is\nL(θ) = Es,a,r,trm,s′∼RB\n\u00141\n2(r + γ maxa′Q(s′, a′; θ−) −Q(s, a; θ))2\n\u0015\n(1)\nwhere θ is the weights of the learned model, θ−denotes the weights of the target\nmodel, γ is the discount factor, s is the current state, a is the action taken at state s,\ns′ is the next state and a′ is the action taken at state s′ [38].\nFor continuous action spaces, we employ an actor-critic architecture similar to [39].\nWe use a four-layered, fully-connected, feed-forward network for the actor and critic.\nWe allocate 256 nodes at every layer and employ rectified linear unit (reLU) activation\nfunction between each layer. The critic receives the state of the environment as its\ninput and outputs the value function estimate. The actor network outputs two real\nvectors which we treat as the mean and standard deviation of the multi-dimensional\nnormal distribution that we sample the actions from. We use the advantage loss to\ntrain the actor which is given as\nLa(θ, ω) = Es,a,r,trm,s′∼RB\n\u0014\nlog(π(a; s, θ))\n\u0000r + γ maxa′V (s′; ω−) −V (s; ω)\n\u0001 \u0015\nwhere ω is the learned critic weights, ω−is the target critic weights, θ is the actor\nweights, π is the actor policy that is modeled as a normal distribution and γ is the\ndiscount factor [39]. We train the critic network using the mean square error which is\ngiven as,\nLc(θ, ω) = Es,a,r,trm,s′∼RB\n\u00141\n2(r + γ maxa′V (s′; ω) −V (s; ω))2\n\u0015\n(2)\nwhere ω is the weights of the critic, ω−denotes the weights of the target critic model,\nand γ is the discount factor. We use the epsilon-greedy policy same as the DQN\nmentioned above.\n6\n\n\nAt each training step the agent takes a single action in the environment, records\nthe tuple (s, a, r, trm, s′) into a replay buffer (RB) (where trm indicates whether s is\na terminal state), randomly samples a batch of tuples from the buffer, and performs\na single batch update on the learned model. The target model weights are updated\nusing the weights of the learned model at the end of each episode.\n3.2 Defining Curricula\nA READ-C curriculum of length d consists of a sequence of start states C = (s1 . . . sd)\nsuch that si ∈S for all i ∈[1, d]. Let πi be the agent’s policy after training on MDPs\nM1 to Mi employing the start states s1 to si. Then, a curriculum step i produces the\ntransition πi →πi+1 where policy πi is the starting policy for the curriculum step and\nπi+1 is the policy attained by training on MDP Mi+1. Hence, each curriculum step is\na modification to the agent’s policy based on the current MDP Mi+1. Note that all of\nthe MDPs in our formulation use the same state space, action space, dynamics, and\nreward function. We compare performance using either the total reward the agent\nachieves during training (asymptotic) or the total time spent on the curriculum and\ntarget task to obtain the maximum cumulative reward Gmax (time to convergence).\nThe objective of the curriculum learning algorithm is to improve the sample effi-\nciency and the learning speed of the reinforcement learning process. In environments\nwhere it is often costly to retrieve samples, it is advantageous to design guidance meth-\nods that could direct the agent towards the parts of the state space that the training\nwould create improvements in the learnt policy of the agent. READ-C aims to modify\nthe start state of the environment with the purpose of bringing the agent closer to the\nstates it is struggling to learn to facilitate the learning process and reduce the number\nof samples needed from the environment to reach the optimal policy. Since all of our\nalgorithms perform one batch update after receiving each sample from the environ-\nment, reducing the sample complexity, in this case, also reduces the number of batch\nupdates made during the learning process, speeding up the training time of the agent.\n3.3 Transfer Learning\nThe learner retains the same model throughout training, using the model obtained\nin the prior curriculum step as the starting point for the next step. Generating new\nmodels or transferring weights is not required. The learner also retains the contents\nof the replay buffer across curriculum steps.\n4 Curriculum Learning\nWe first describe our uncertainty measures and provide pseudo-code for the high-\nlevel description of READ-C. Then, we define two main realizations of READ-C,\nnamely READ-C-SA and READ-C-TD, and describe two distance-metric variants of\nthe relative-entropy heuristic.\n7\n\n\n4.1 Measuring Uncertainty\nWe measure the uncertainty between two policies, namely the learnt policy and the\ntrue policy, through relative entropy. The relative entropy is defined as\nDKL(Ptrue||Plearnt) =\nX\na∈A\nPtrue(s, a)log\n \nPtrue(s, a)\nPlearnt(s, a)\n!\n(3)\nwhere Ptrue denotes the action probabilities obtained from a reference policy and\nPlearnt represents the probabilities of a learnt policy at a certain checkpoint during\nthe training. We take the relative entropy of the reference probabilities with respect\nto the learnt probabilities to measure the reduction in uncertainty the agent would\nhave in its policy if it were to use the correct policy instead of its own. In this manner,\nour uncertainty metric reflects how inaccurate the agent is in its action selection for\na given state of MDP. Since the true policy already has good performance on the\ntarget task, the relative entropy is expected to drop as the agent improves its policy\nand comes closer to the estimate of the true policy. The probability distribution here\ncould be defined over a discrete or continuous variable depending on how the neural\nnetworks treat the output parameters.\nSince the policy relies on the Q-value estimates of the agent, we require a transfer\nfunction that could convert the Q-value estimates to probability values for the relative-\nentropy calculation. For this purpose, we use softmax and calculate the probability of\naction selection as\nP(s, a) =\ne\nQ(s,a)\n||Q(s)||2\nP\na∈A e\nQ(s,a)\n||Q(s)||2\nwhere Q(s, a) is the Q-value for the state, action pair (a, s), Q(s) is the list of Q-values\nfor state s.\n4.2 READ-C\nGiven the definition of uncertainty in Eq. 3, we construct a high-level curriculum-\nlearning algorithm that affords the use of different RL algorithms for the training of\nthe agent. Algorithm 1 is the high-level pseudo-code for READ-C. It initializes the\ntarget environment and the agent model (Line 4). It starts with an empty curriculum\n(Line 5) and trains the agent model for η iterations using the train function defined\nin Algorithm 2 to generate an initial agent policy (Line 7)— the starting point for\nrelative-entropy calculation. Each iteration of the loop (Line 8) creates a new curricu-\nlum step by selecting a new start state and the agent trains on MDP with the new\nstart state. The function named uncertainty defined in Algorithm 4 (Line 9) uses Eq.\n3 to calculate relative entropy values of current policy for each state in the set of vis-\nited states (SB), and returns the state with the highest uncertainty to Algorithm 1.\nThen, Algorithm 1 changes the start state to the state of highest uncertainty (Line 10)\nand trains on the modified MDP until the convergence criteria—if the entropy has not\nreduced for the last 10 episodes—is met (Line 12). This convergence method receives\n8\n\n\nthe relative entropy of the selected state from the agent before and after each train-\ning episode, which is not computationally expensive since the algorithm calculates\nentropy for only a single state—not all regions. The process repeats until the curricu-\nlum reaches a defined length, at which point the student trains on the original target\ntask until an overall convergence criterion—if the agent has not reached the highest\ncumulative reward for the last 10 episodes—is met (Line 17).\n1: Inputs: target MDP: Mtar.\n2: Consts: # of training iterations: η; curriculum length threshold: MAX LENGTH.\n3: Vars: learning model: NN; environment: ENV ; curriculum: C; state buffer: SB;\nconvergence criteria: conv.\n4: NNagent, ENVtar ←initialize(Mtar)\n5: C ←{}\n6: Set conv to η training iterations\n7: train(NNagent, ENVtar, conv, SB)\n8: while |C| < MAX LENGTH do\n9:\ns ←uncertainty(NNagent, ENVtar, SB)\n10:\nENVcur ←Set s as the start state of ENVtar\n11:\nSet conv to 10 episodes of no entropy reduction\n12:\ntrain(NNagent, ENVcur, conv, SB)\n13:\nC ←C ∪si\n14: end while\n15: Set conv to highest cumulative reward\n16: Make si of original Mtar the si of ENVtar\n17: train(NNagent, ENVtar, conv, )\nAlgorithm 1: READ-C\nAlgorithm 2 describes the train function used in Algorithm 1. It receives the learn-\ning model, the current environment, the convergence criteria and the state buffer as\ninputs and performs regular RL training on the environment using the learning model\nuntil the convergence criteria is met (Line 4). It initializes the environment at the start\nstate (Line 5) and selects the action to take using an ϵ-greedy policy on the learn-\ning model (Line 8). Then, it takes the selected action on the environment (Line 9)\nand stores the (s,a,r,trm,s’) tuples in RB for future training (Line 10). It also stores\nevery unique state visited so far in SB for the entropy calculation (Line 12). Then,\nit performs a single step of optimization on the learning model using a minibatch of\nsamples from RB (Line 15). Algorithm 2 affords the use of different learning models\nand policies as it (neither READ-C) does not rely on any specific policy or network\narchitecture for the entropy calculation or agent training. It can also be easily adapted\nto non-episodic environments by removing trm variable (Line 6) and replacing the\nloop that checks for terminal states (Line 7) with the outer loop that checks a suitable\nconv for non-episodic training (Line 4).\n4.3 Relative-Entropy Calculation for READ-C-TD\n9\n\n\n1: Inputs: learning model: NNagent; environment: ENV ; convergence criteria conv, state\nbuffer: SB.\n2: Consts: set of terminal states of ENV : Sg; initial state of ENV : si.\n3: Vars: replay buffer RB.\n4: while conv is not satisfied do\n5:\ns ←si of ENV\n6:\ntrm ←s ∈Sg\n7:\nwhile trm is False do\n8:\nSelect a using ϵ-greedy policy on NNagent(s)\n9:\nTake action a in ENV and observe r, s′\n10:\nStore (s, a, r, trm, s′) in RB\n11:\nif s /∈SB then\n12:\nStore s in SB\n13:\nend if\n14:\nSample a minibatch B from RB\n15:\nPerform an optimization step on NNagent using B\n16:\ns ←s′\n17:\ntrm ←s ∈Sg\n18:\nend while\n19: end while\nAlgorithm 2: Train\n1: Inputs: learning model: NNagent; environment: ENV ; state buffer: SB.\n2: Consts: a teacher model: T.\n3: Vars: Q-values for state s: Qs; probabilities for state s: P s.\n4: Outputs: the new start state: s.\n5: states ←Sample a subset of states from SB\n6: UNCERTAINTIES[0, ..., N] ←0 where N is the number of states\n7: for sj in states do\n8:\nQs\nagent ←get Q-values of sj from NNagent\n9:\nQs\nteach ←get Q-values of sj from T\n10:\nP s\nagent ←softmax(normalize(Qs\nagent))\n11:\nP s\nteach ←softmax(normalize(Qs\nteach))\n12:\nUNCERTAINTIES[j] ←entropy(P s\nteach, P s\nagent)\n13: end for\n14: k ←argmax of UNCERTAINTIES\n15: return sk\nAlgorithm 3: Relative-Entropy Calculation for READ-C-TD\nAlgorithm 3 describes the relative-entropy calculation for the teacher dependent\nvariant of READ-C. It necessitates the use of a teacher model that already knows the\noptimal policy of the target environment to compute the relative entropy values for\ncurriculum selection. If there happens to be an already trained model for the target\nMDP that could serve as the teacher, Algorithm 3 offers a simpler way of calculating\nthe uncertainty metric as it does not require β iterations of training Algorithm 4 does\non the learning model to generate two policies of the agent at different checkpoints.\nOn the other hand, if the teacher needs to be trained from scratch, Algorithm 4 offers\n10\n\n\na much faster performing framework. If there exist an agent residing in a remote\nserver which the user could query to measure the relative entropy but cannot copy\nits weights directly to the agent due to privacy and/or security reasons, then that\nagent could be used in READ-C-TD as a teacher model, which present one possible\nuse case of READ-C-TD despite its requirement of already having a learnt model.\nA more concrete example of this situation would be commercial LLMs where we are\nnot given open source access to the neural network itself but we can interact with\nthe model through an API. In a similar sense, when a deep RL model is used in a\ncopyrighted setting, we can interact with the model to generate action probabilities\nwithout having the need to access the inner mechanisms of the model. Then, using\nthe output of the model allows me to perform training on READ-C-TD and get the\nbenefit of teacher dependent curriculum generation.\nAlgorithm 3 contains a teacher model that is trained to convergence using Algo-\nrithm 2 without employing any curriculum learning. Algorithm 3 receives the agent\nmodel, the environment and the state buffer SB as its inputs and uses the teacher\nalong with the information it has on the agent to calculate relative entropy. Algo-\nrithm 3 calculates the probability values based on the Q-values estimates of the\nsampled states (Lines 10 & 11). Then, it calculates the relative entropy between the\nagent’s and teacher’s policies and uses it as the uncertainty value for the given state\n(Line 12). Since the teacher knows the optimal policy, the relative entropy, in this case,\nshows how much the agent’s knowledge is diverging from the optimal policy. Finally,\nREAD-C-TD chooses the highest-uncertainty state as the new start state (Line 14).\n4.4 Relative-Entropy Calculation for READ-C-SA\nAlgorithm 4 describes the uncertainty function given in Algorithm 1 at Line 9 and\nestimates the relative entropy for the self-assessed version of READ-C. READ-C-SA\nuses a regression model to estimate the relative entropy between the agent and teacher\nand gets rid of the requirement of having a teacher model already trained in the target\nenvironment. The regressor serves as a proxy function for estimating the true relative\nentropy using the information theoretic data generated on a simpler environment. It\nattempts to learn an underlying relationship between the agent’s knowledge of the\ntask and the agent’s uncertainty of the action selection. Since READ-C-SA uses the\ntrue policy of the simpler environment in its training, the true policy in this setting\ndoes not necessarily represent how the agent should act in the target environment\nbut due to the simpler environment being a subtask of the target environment having\nsimilar characteristics, it is expected that the knowledge acquired on the simpler task\nwould be generalizable to the target task in calculation of the uncertainty. READ-\nC-SA offers a compromise in that it allows us to predict the relative entropy values\nmore efficiently without relying on the teacher policy but it also introduces additional\nerror coming from the training of the regression model, which we try to minimize by\nselecting a proper regression model for the problem at hand.\nAlgorithm 4 samples a subset of the states in SB for the uncertainty calcula-\ntion (Line 5). This step is crucial in ensuring that the algorithm could run in large\nstate spaces especially if the state space is continuous. Then, the algorithm per-\nforms training for β iterations to generate a second learning model (Line 9). The two\n11\n\n\n1: Inputs: learning model: NNagent; environment: ENV ; state buffer: SB.\n2: Consts: # of training iterations: β.\n3: Vars: Q-values for state s: Qs; probabilities for state s: P s; convergence criteria: conv.\n4: Outputs: the new start state: s.\n5: states ←Sample a subset of states from SB\n6: UNCS[0, ..., N] ←0 where N is the number of states\n7: Set conv to β training iterations\n8: NNpast ←NNagent\n9: train(NNagent, ENV , conv, SB)\n10: for sj in states do\n11:\nQs\npast ←get Q-values of sj from NNpast\n12:\nQs\nagent ←get Q-values of sj from NNagent\n13:\nP s\npast ←softmax(normalize(Qs\npast))\n14:\nP s\nagent ←softmax(normalize(Qs\nagent))\n15:\nrelEntropy ←entropy(P s\nagent, P s\npast)\n16:\nentropyCur ←entropy(P s\nagent)\n17:\nentropyPast ←entropy(P s\npast)\n18:\nUNCS[j] ←reg(relEntropy, entropyCur, entropyPast, Qpast, Qagent)\n19: end for\n20: k ←argmax of UNCS\n21: return sk\nAlgorithm 4: Relative-Entropy Calculation for READ-C-SA\nlearning models of the agent, NNpast and NNagent, at two different checkpoints in\ntime helps us measure how much the agent is diverging from its own policy as time\npasses. Although the relative entropy calculated between NNpast and NNagent does\nnot reflect the true relative entropy, it represents the speed of learning and change in\nthe Q-value estimates of the model. For every sampled state, READ-C-SA calculates\nthe Q-value estimates of both models (Lines 11 & 12) and converts them to probabil-\nities by normalizing and softmaxing (Lines 13 & 14). The algorithm normalizes the\nQ-values using the L2-norm to make the results independent of the scale of the Q-\nvalues and uses softmax to obtain probability estimates. It calculates relative entropy\nand entropies on the probability values using Eq.3 and 4 (Lines 15, 16 & 17). By doing\nthese operations, READ-C-SA generates information theoretic data about the agent\nthat we use for estimating the relative entropy for the given state. We pass the data\nto a regressor model that was trained on the relative-entropy values of a smaller and\neasier-to-train environment. Since the input features to the regressor are independent\nfrom the state and action dimensions of the environments, we can deploy the trained\nregression model from a simpler environment to the target environment without any\nmodification. The regressor model serves as a proxy function for the relative entropy\nbetween the true policy and the agent policy of the target environment as long as the\neasier-to-train environment shares the same characteristics as the original task. The\nregressor predicts the uncertainty of the states based on the relative entropy, entropy\nand q-value estimates of the agent policy and records it in an array (Line 18). Finally,\nthe algorithm chooses the maximum uncertainty state as the new start state (Line 20)\nand returns it (Line 21).\n12\n\n\nOur regressor is a gradient-boosting machine (GBM). It trains on the data (relative\nentropy, entropy, Q-values and number of visits per state) generated by a learning\nmodel of a simpler environment. In addition to relative entropy, we also use the entropy\nfor each policy in the training of the regression model. The entropy is defined as\nH(π(·|s)) = −\nX\na∈A\nP(s, a)log(P(s, a)),\n(4)\nwhere H(π(·|s)) is the entropy of the action policy π for the state s using the proba-\nbilities P coming from a given READ-C learner’s policy. The training of the regressor\nfollows the same pattern as Algorithm 4 to generate the data (Lines 13, 14, 15, 16 & 17)\nbut instead of using the data to predict the uncertainty values, the regressor learns\na mapping from the data to the relative entropy between the agent and the optimal\npolicy. As long as the trained environment shows a high level of similarity to the tar-\nget MDP, the regressor generates accurate uncertainty estimates for every sampled\nstate. Since we employ a simpler MDP for the regressor optimization, the training of\nGBM takes much less time than the curriculum-learning process allowing READ-C-\nSA to efficiently calculate uncertainty values without having a need to measure the\ntrue policy on the target MDP.\nThe regression model takes the information theoretic data (relative entropy,\nentropy, Q-values and number of visits per state) generated on the simplified version\nof the target task as input and uses the true relative entropy as the ground truth value\nfor its training. The true relative entropy is calculated between the agent policy and\nthe policy of a teacher model trained on the simpler environment. Since it is easier\nto train on the simplified environment than the target task, generating the teacher\nmodel in the case of READ-C-SA takes much less time than READ-C-TD, allowing\nthe curriculum generation process to be more efficient. The regression model uses the\nmean squared error between its output and the true relative entropy as the loss func-\ntion. Once the regression model trains on the data generated for every visited state\nof the simpler environment, it gets deployed in the target environment to predict the\nuncertainty values, which is shown in Algorithm 4. We train different types of regres-\nsors, namely Linear Regression, Random Forest Regression, Support Vector Machine\nand GBM, and choose the best performing regressor (GBM) to be deployed in the\ntarget environment.\n4.5 Clustering of States\nOptionally, one could also perform an Agglomerative Hierarchical Clustering [40] over\nsampled states, merging clusters using the Ward criterion [41] until a distance cutoff\nis reached, and calculate the uncertainty values over the clusters of states rather than\nsingle states. In this case, the fundamental flow of the algorithm remains unchanged\nas it still calculates a single uncertainty value for every state but the uncertainty\nvalues of the states in the same cluster gets aggregated at the end of the algorithm\nto find the uncertainty of each cluster. Then, the algorithm chooses the highest-\nuncertainty cluster and selects a random state from that cluster as the new start state.\nThe clustering of the states enables us to reduce the deviation in our measurements\n13\n\n\nsince single states might show high variance in their uncertainty from one run of the\nalgorithm to the other. It is more advisable to perform clustering if the nature of the\nstate space allows such a partitioning but in domains where the state space cannot\ngenerate meaningful clusters, the algorithm needs to run in its original form without\nclustering.\n4.6 Heuristic Variants\nREAD-C as described above chooses the maximum relative entropy, which does not\naccount for the underlying structure of the problem. For example, a region of high\nuncertainty may be located in an area of the state space extremely far from the start\nand goal states of the target task, or that may be rarely encountered when following\nan optimal policy. Relative entropy alone would prioritize training in such regions even\nif not advantageous. Therefore, we present two additional relative-entropy heuristics:\n1) Proximity: proximity to positive reward terminal states, which filters out 80%\nof the regions having the worst proximity (average Euclidean distance in our case) to\nterminal states that provide a positive reward, and computes relative entry only for\nthe remaining 20%;\n2) Max Distance: proximity to low-entropy regions, which classifies regions as high-\nentropy (one standard deviation above the mean relative entropy) and low-entropy\n(one standard deviation below the mean relative entropy), and sorts the regions of\nhigh-entropy based on the (Euclidean, in our case) distance to the closest low-entropy\nregion (selecting the region with the highest distance). This technique promotes\nselected regions not resulting in lengthy training episodes to improve efficient training.\nFigure 1 depicts a 10 by 10 environment containing a single agent (straw man)\nat the top right corner and a positive reinforcement item (white circle) at the bot-\ntom left corner. The rest of the environment is clustered into regions with varying\nrelative entropies. Green denotes low-entropy regions, red (containing dotted shapes\ninside) denotes high-entropy regions, and yellow and orange denote moderate-entropy\nregions. Because there are no actions leading into walls, it is faster to learn the regions\nclose to the edges of the environment; hence, those regions are mostly low-to-moderate\nentropy. Similarly, the regions near the white circle have low entropy because of the\nshort-horizon goal-state reward. The regions near the center of the environment have\nhigh entropy. Black lines indicate the closest low-entropy region for the high entropy\nregions. In this situation, relative entropy by itself would prioritize one of the three\nred regions, proximity would prioritize the red region with light blue edges (containing\na dotted triangle), and maximum distance to the low-entropy regions would prioritize\nthe red region with dark blue edges (containing the dotted diamond). Therefore, the\nproximity variant of READ-C will aim to keep the agent close to the positive reinforc-\ning terminal state whereas the maximum distance to low entropy variant will aim to\nkeep it away from the already learned regions. The relative entropy by itself could still\nchoose a suitable region to train on in this example but due to the randomness of the\ntraining process caused by the ϵ-greediness, there will be no guarantee that the region\nof highest uncertainty the relative entropy generates would be the most useful region\nfor the agent to reduce its uncertainty. However, by using the distance metrics, the\n14\n\n\nFig. 1 Visualization of the highest uncertainty region for READ-C variants in an environment with\na single goal.\nagent would at least make sure that the region being selected is not far away from the\ngoal states and/or states with useful information regarding the optimal policy path.\n4.7 Proof of Convergence for READ-C\nLet La be the loss function for the actor and Lc be the loss function for the critic.\nLa(θt, ωt, zt) = Eτ∼π(θt,zt|s0)\n\u0014\nlog(π(at|st, θt))(Rt + γV (st+1|ωt) −V (st|ωt))\n\u0015\n(5)\nwhere π(at|st, θt) ∼N(µt, σ2\nt ), st is the environment state, at is the action taken\nat time t, Rt is the reward received from the environment, γ is the discount factor,\nωt is the weights of the critic network, θt is the weights of the actor network, V is\nthe value function estimated by the critic, π is the policy of the actor network, zt is\nthe controlled Markov process modeling non-additive noise, τ is the trajectory of the\nagent following policy π and (µ, σ) pair is the mean and standard deviation of the\nGaussian distribution modeling the actor network policy π.\nLc(θt, ωt, zt) = Eτ∼π(θt,zt|s0)\n\u00141\n2(Rt + γV (st+1|ωt) −V (st|ωt))2\n\u0015\n(6)\nThe gradient update rule for the loss in advantage actor-critic model is\n15\n\n\nθt+1 = θt −α(t)(∇La(θt, ωt, zt) + mt)\n(7)\nwhere α(t) is the learning rate for the actor at time t and mt is the martingale sequence\nnoise.\nωt+1 = ωt −β(t)(∇Lc(θt, ωt, zt) + mt)\n(8)\nwhere β(t) is the learning rate of the critic network and mt is the martingale sequence\nnoise.\nTheorem 1 Under the assumptions of Section 4.7.1, Eq. 7 & 8 converges to\n(θt, ωt) →(θ∗, ω∗) as t →∞\n(9)\n4.7.1 Assumptions\nIn this section, we describe the assumptions needed to prove the convergence of READ-\nC to an optima. We use the problem formulation from [42] for crafting the assumptions\nof our proof. We treat the actor-critic learning process as a two time-scale optimization\nprocess and make use of the actor-critic convergence proof in proving that the selection\nof curricula does not change the convergence of the reinforcement learning algorithm.\nAssumption 1\nzt is a Markov process taking values in a compact metric space w.r.t. a continuous\ntransition function.\nAssumption 2 Lc and La are J-Lipschitz and J-Smooth functions such that\n||∇La(θt, ωt, zt)|| ≤J\n∀θ & ω ∈IR\n(10)\n||∇2La(θt, ωt, zt)|| ≤J\n∀θ & ω ∈IR\n(11)\nwhere J is the Lipschitz constant.\nAssumption 3\nmt is a martingale difference sequence with bounded second moment such that\nE[m2\nt|Ft] ≤K + θ2\nt\nE[m2\nt|Ft] ≤K + ω2\nt\n(12)\nwhere Ft is history of martingale variables up until time t and K is the martingale\nconstant.\nAssumption 4\nX\nt\nα(t) = β(t) = ∞,\nX\nt\n(α(t)2 + β(t)2) < ∞\n(13)\nfor α(t), β(t), t ≥0.\nAssumption 5\nAssume that zt has an ergodic occupation measure Γ(θ, ω). Then, let lc be\n∇Lc(θt, ωt, zt) and la be ∇La(θt, ωt, zt).\n16\n\n\n¯lc =\nZ\nlc(θ, ω, z)Γ(θ, ω)\n(14)\nwhere ¯lc is the ordinary differential equations for the gradient of the loss function.\n¯lc has a global attractor set Bθ and a stable point λ(θ) such that λ(θ) is a J-\nLipschitz map.\nFurthermore,\n¯la =\nZ\nla(θ, λ(θ), z)Γ(θ, λ(θ))\n(15)\n¯la has a global attractor set A.\nAssumption 6 The weights of the neural networks have a tight upper bound such\nthat\nsup(||θt|| + ||ωt||) < ∞\n(16)\n4.7.2 Effect of Curriculum Selection on Convergence\nThe actor-critic architecture is expected to converge to an optima based on the conver-\ngence proofs provided in Wu et al. and Holzleitner et al. [43, 44] using the assumptions\nfrom two time scale optimization processes [42]. Under these conditions, we must\nshow that the selection of curriculum states does not violate the convergence of the\nactor-critic model for READ-C to also converge.\nREAD-C determines the start state for the environment based on the KL diver-\ngence between the agent’s policy and the true policy of the MDP which is given\nas\ns0 = argmaxs∈SKL(s|θt, zt)\n(17)\nIf we assume that the output of the actor-critic model is normally distributed\n(which is an assumption that holds for advantage actor-critic), then KL divergence\nwill have a closed form solution which is\nKL(s|θt, zt) = log(σlrn(s|θt, zt)\nσtrue\n) +\nσ2\ntrue\n2σ2\nlrn(s|θt, zt) + (µtrue −µlrn(s|θt, zt))2\n2σ2\nlrn(s|θt, zt)\n(18)\nwhere µ and σ refer to the mean and standard deviation of the Gaussian distribution\nthe policy samples the actions from. (µtrue, σtrue) comes from an already existing\noptimal policy and (µlrn, σlrn) comes from the current policy of the agent.\nThe expected loss for following a trajectory initiated at a starting state generated\nby the curriculum learning algorithm would be\nX\nsc∈S\nP(s0 = sc|KL(sc|θ0, z0))P(τ|s0, θ0)E[La(θt, ωt, zt, st)]\n(19)\nwhere CL refers to the set of start states s0 calculated by Eq.17 w.r.t. KL divergence\ngiven in Eq.18.\n17\n\n\nGiven the convergence of the actor-critic, we wish to show that the distribution of\nstart states above does not change the convergence of READ-C.\nFor the first base case, if sc = sg, then it is trivial to show that the algorithm\nwill converge since the agent is already at a goal state and does not need to take any\naction to reach convergence.\nFor the second base case, if sc = neighbor(sg), then the expected loss would be\nP(sg|s0, θ0)E[La(θt, ωt, zt, st)]\n(20)\nfor a sufficiently large exploration rate of ϵ, P(sg|s0, θ0) > 0 and we know that\nE[La(θt, ωt, zt, st)] converges when t →∞based on the actor-critic convergence proof\n[42, 44]. Then, the algorithm is supposed to converge to an optima when the start\nstate is a neighbor of a goal state.\nHaving established the base cases, we now assume that READ-C converges when\nsc = sn where sn is a state on trajectory τ. We need to show that READ-C converges\nwhen sc = sn−1 where sn−1 is a prior state to sn on the trajectory. The expected loss\nis\nP(s0 = sn−1|KL(sn−1|θn−1, z0))P(τ|sn−1, θn−1)E[La(θt−1, ωt−1, zt−1, st−1)]\n(21)\nwhich is equivalent to\nP(s0 = sn−1|KL(sn−1|θn−1, z0))P(sn|sn−1, θn−1)P(τ|sn, θn)E[La(θt−1, ωt−1, zt−1, st−1)]\n(22)\nE[La(θt−1, ωt−1, zt−1, st−1)] = E[La(θt, ωt, zt, st)] −⟨la(Υt−1), θt −θt−1⟩\n(23)\nbased on Taylor’s Theorem [45]. As t →∞, we know that |θt −θt−1| →0 due to\nthe convergence of the actor, which makes the inner product term 0, leaving us with\nE[La(θt, ωt, zt, st)]. Similarly,\nP(s0 = sn−1|KL(sn−1|θn−1, z0))P(sn|sn−1, θn−1) ∝P(s0 = sn|KL(sn|θn, z0)) (24)\nThen, we could rewrite Eq.22 as\nP(s0 = sn|KL(sn|θn, z0))P(τ|sn, θn)E[La(θt, ωt, zt, st)]\n(25)\nWe know that this equation converges due to the inductive assumption. Then, READ-\nC must converge for s0 = sn−1.\nWe have shown that the curriculum selection does not change the convergence\nof READ-C. Given that the curriculum selection criteria does not change the\nconvergence, READ-C must converge under the assumptions given in Section 4.7.1.\n18\n\n\n5 Evaluation Methods\nHere we describe the test environments, the experimental setup, and the state\nrepresentation used for each domain.\n5.1 Test Environments\n5.1.1 Key-Lock\nWe use a 20×20 2D grid environment as well as continuous state-action space domains\nto test the algorithms. The first domain contains keys, locks, pits, and obstacles similar\nto Narvekar et al. [25], Konidaris and Barto [46]. The agent’s task is to pick up the key\nand unlock the lock while avoiding the pits and obstacles. Each key picked up gives a\nreward of 500 and each lock unlocked gives a reward of 1,000. Falling into a pit receives\n-400. All other actions including moving into an obstacle receive -10. Moving into an\nobstacle results in no state transition. The learner can only move in cardinal directions\nand is assumed to have obtained the key or the lock if its location matches the location\nof the key or the lock. An episode terminates when the agent obtains all the keys and\nlocks, falls into a pit, or reaches 100 time steps. A state in the key-lock environment\nis represented as a vector, including the Euclidean distance from the learner in all\ncardinal directions to the nearest key and lock, four binary parameters indicating if\nthere is an obstacle in the neighboring cells, four binary parameters indicating if there\nis a key or lock in the neighboring cells, and eight binary parameters indicating if there\nis a pit in the two adjacent cells in all four directions. Lastly, two integers indicate\nthe number of keys and locks captured so far. We use the dual DQN architecture to\ntrain on Key-Lock environments since the domain has a discrete state-action space.\n5.1.2 Flags\nThe second domain contains flags. The agent’s task is to capture the flags in an order\nunknown to it prior to training. Each flag picked up gives a reward of 10 and the\nreward value increases by an additional 10 for every consecutive flag the agent cap-\ntures. All other actions including moving into an obstacle receive -10. The learner can\nonly move in cardinal directions and is assumed to have obtained the flag if its location\nmatches the location of the flag. An episode terminates when the agent obtains all the\nflags, or reaches 100 time steps. A state in the capture-the-flag environment is repre-\nsented as a vector, including the Euclidean distance from the learner in all cardinal\ndirections to all of the flags, four binary parameters indicating if there is an obstacle\nin the neighboring cells, and four binary parameters indicating if there is a flag in the\nneighboring cells. Lastly there is one integer for the number of flags captured so far.\nWe use the dual DQN architecture to train on capture-the-flag environments since\nthe domain has a discrete state-action space.\n5.1.3 Parking\nThe last domain is the parking environment from Leurent [47]. The parking lot con-\nsists of 30 parking spots, an agent and a goal item randomly positioned in one of\n19\n\n\nthe spots at the beginning of each episode. The agent always starts in the same posi-\ntion but its initial orientation changes randomly. The agent’s task is to reach the\ngoal and orient itself in the right direction. The agent takes two continuous actions:\nvelocity and angular velocity both defined in the range of [-1,1]. We use the actor-\ncritic architecture to train on Highway-Parking environment because the domain has\na continuous state-action space. Since the output of the model is continuous in this\ndomain, we assume a normal distribution for the output and use the relative entropy\nfor continuous Gaussian variables in calculating the uncertainty, which has a closed\nform solution for normal distributions given in Pardo [48]. The environment allows the\nagent to wander outside of the parking lot, which increases the size of the state space.\nThe agent receives a punishment proportional to its distance to the goal at every step\nof the training. An episode terminates when the agent attains the goal position with\nthe proper heading or when it reaches 100 time steps. A state in the parking envi-\nronment is a vector, including the agent’s position, velocity, angular velocity, and the\ngoal’s position and orientation. The input to the neural network is the concatenation\nof the state and goal information since the environment uses a goal-aware observation\nspace. We use\n5.1.4 Environments for Regressor Training\nFor the training of the regressor, we use simplified versions of the target tasks. In the\ncase of key-lock and capture-the-flag domains, the regressor learns a 10 × 10 envi-\nronment having the same goal characteristics (position and direction) as the original\nenvironment. In the parking environment, we reduce the number of parking spots\nto 8 and halve the initial distance between the agent and the parking spaces. Since\nREAD-C-SA requires the training of the regressor for a particular domain once, we\nonly need to construct a single source environment for every domain, which ends up\nbeing less time-consuming than manual curricula generation.\nFor comparison, we also train different regression models on different source tasks\nto see how the regression training affects the curriculum performance. We use three\nsettings: source task simpler than the target, source task similar to the target and\nsource task same as the target. Source task simpler than the target denotes the 10×10\nenvironment we describe in section 5.1, source task similar to target uses a 20 × 20\nenvironment with slightly different goal positions than the target and source task\nsame as target uses the target environment in training the regressor for the Key-Lock\nand Capture the Flag domain. Similarly, source task simpler than target uses half the\nnumber of parking spots with half the initial distance to the agent, source task similar\nto target uses similar number of parking spots to the target task and source task same\nas target uses the target environment in training the regressor for the Parking domain.\n5.2 Comparison Algorithm\nWe compare READ-C-TD and READ-C-SA to the method from Narvekar et al. [25],\nwhich uses the maximum change in the policy as determined by the number of states\nin which the action selected by the policy before learning the source task differs from\nthe action selected by the policy after learning the source task. The criteria selects the\n20\n\n\ntask with the highest policy change to the curriculum. We use a set of 15 manually\ngenerated for this comparison algorithm.\nFor additional comparison, we also use a randomly generated curriculum and learn-\ning directly on the target task. We generate the random curriculum by sampling start\nstates from the neighborhood of the terminal states.\n5.3 Experimental Setup and Hyperparameters\nWe compare performance based on the total reward obtained while learning the tar-\nget and curriculum tasks. We also compare the convergence times and provide 95%\nconfidence intervals. All algorithms share the same target task.\nThere are two types of hyperparameters: 1) neural network (including for the con-\nvergence criteria) and 2) curriculum selection. Table 1 contains the parameters used\nby the neural network in the key-lock environment. These parameters are not tuned\nfor any specific algorithm and all algorithms we use share these parameters for all\nof the experiments and results in the same domain. ϵ is the ϵ-greediness of the algo-\nrithm, which decays during training. α and γ are the learning rate and discount factor\nrespectively. Entropy Reduction is the number of consecutive episodes the convergence\ncriteria considers in determining if entropy is reducing or not.\nTable 2 shows the parameters for READ-C-TD. Clustering and Cutoff Distance\nrefer to the clustering algorithm used to generate regions and the cutoff point for\nstopping the merge of clusters. η denotes the number of training episodes on the target\ntask before the curriculum generation. Lastly, Curriculum Length is the number of\ntasks in the curriculum. For READ-C-SA, β is 15,000 and η is 40, 000.\nParam\nValue\nParam\nValue\nϵ\n1\nOptimizer\nAdamax\nϵ Decay\n0.995\nLoss\nMean Squared\nMinimum ϵ\n0.01\nBuffer Size\n40,000\nα\n0.005\nBatch Size\n16\nγ\n0.99\nEntropy Reduction\n10\nTable 1 Hyperparameters for the neural network.\nParameter\nValue\nParameter\nValue\nClustering\nAgglomerative\nCutoff Distance\n3\nη\n50,000\nCurriculum Length\n4\nTable 2 Hyperparameters for the curriculum learning.\n21\n\n\n6 Results and Analysis\nWe perform 25 runs for each algorithm and report the average. All algorithms use the\nsame randomly initialized weights and we alter the DQN weights every run.\n6.1 Results for Key-Lock Domain\nHere we present the evaluation results and analysis for the Key-Lock Domain. Without\nspecification, the default training setting for READ-C-TD is without any heuristic\ndistance measures. The default training setting for READ-C-SA is 200 clusters and\nthe simpler-than-target source task. READ-C-SA with No Clusters directly uses the\nvisited states in curriculum generation. Other than the comparison of performance\nunder different target environment complexity, the default target environment is the\n20 × 20 environment described in section 5.1.\nFigure 2(a) contains results for READ-C-SA in relation to the Max Policy Change\nand DQN baselines in the key-lock domain as a function of the number of training\nsteps. Figure 2(b) displays the same results while also counting the curriculum-\ngeneration overhead. Only Max Policy Change algorithm has curriculum generation\noverhead. Following the procedure from the original paper of Max Policy Change [25],\nwe perform 50,000 iterations of training on the target task to generate the prior pol-\nicy and then perform 5,000 iterations of training on each source task to obtain the\nposterior policy used to measure the policy change. We repeat this process for two\ncurriculum steps which, in total, results in 2×(50, 000+15×5, 000)−5, 000 = 245, 000\niterations of curriculum-generation overhead. READ-C and the other baselines, on the\nother hand, does not require us to perform any training for the curriculum generation.\nEach graph is offset by the amount of time required to do the curriculum training.\nFigure 2(c) compares READ-C-TD’s different heuristic variants whereas Figure 2(d)\nshows the performance of READ-C-SA with respect to READ-C-TD and no curricu-\nlum baseline. Figure 2(e) displays the results obtained from training the regressor on\ndata coming from three different source tasks. Figures 3 compares the performance\nof READ-C-SA for different cluster numbers on varying sizes of target environment.\nShaded regions are 95% confidence intervals.\nREAD-C-SA converges faster than the baseline algorithms even if we do\nnot account for the curriculum-generation overhead but its confidence intervals\nalso show an overlap with max policy change (Figure 2(a)). Accounting for\ncurriculum-generation overhead, the overlap between READ-C-TD and Max Pol-\nicy Change reduces notably indicating performance improvements from READ-C-SA\n(Figure 2(b)). READ-C-SA and proximity aware random curricula perform simi-\nlar to one another but READ-C-SA results in much shorter curriculum training\n(Figure 2(b)). Furthermore, READ-C-TD + proximity performs better than READ-\nC-TD, and READ-C-TD + max distance, showing that heuristic functions in\ncombination with relative entropy can produce higher performance (Figure 2(c)).\nREAD-C-TD performs slightly worse than READ-C-SA (Figure 2(d)) mainly due\nto the fact that a regressor trained on simpler environments generalizes better and\ncaptures more relevant parts of the agent’s uncertainty compared to a teacher that\ncalculates uncertainty for a larger state space. Training the regressor on a task Similar\n22\n\n\n(a) Curriculum + Target Performance for READ-C\nand Baselines\n(b) Curriculum + Target Performance + Generation\nOverhead for READ-C and Baselines\n(c) Curriculum + Target Performance for Variants\nof READ-C\n(d) Curriculum + Target Performance for READ-C-\nTD and READ-C-SA\n(e) Curriculum + Target Performance for Different\nRegressors\nFig. 2 Performance of the Curriculum-Learning Algorithms as a Function of Training Steps in Key-\nLock Domain.\nto Target or Same as the Target results in a performance close to READ-C-TD slightly\nbelow the performance of READ-C-SA with Simpler Than Target (Figure 2(e)), which\nis not unusual since READ-C-TD uses the training data on the target task to generate\nthe curriculum as well.\nAll READ-C approaches in a small environment give similar results despite\nthe difference in cluster sizes due to the target environment being simple to learn\n(Figure 3(a)). Moderate number of clusters such as 100 or 200 performs the best for\n23\n\n\n(a) Cluster Size Comparison for 10 × 10 Target Envi-\nronment\n(b) Cluster Size Comparison for 20 × 20 Target\nEnvironment\n(c) Cluster Size Comparison for 30 × 30 Target Envi-\nronment\nFig. 3 Effect of Cluster Size on the Performance of the Curriculum-Learning Algorithms in Key-\nLock Domain.\nmedium size environments (Figure 3(b)) while no clustering gives the best results fol-\nlowed closely by 1000 clusters in the large environment although the performance for\n500 clusters is very close to 1000 clusters as well (Figure 3(c)). In general, it is safe to\nsay that using a moderate number of clusters always guarantees good performance for\nREAD-C-SA on any size of environment even though it might not always produce the\nbest results. Here please note that we use the same set of distance cut-off thresholds\n([0.1, 1, 3, 5]) for Agglomerative Clustering in all of these environments but depend-\ning on the size of the environment, the number of clusters being generated differs from\none figure to the other, which is why the number of clusters is higher in the case of\nlarge environment compared to the moderate-size environment (Figures 3(b) & 3(c)).\nConvergence results are presented as box plots illustrating the distribution of the\nnumber of steps to reach a given cumulative reward across 25 runs (Figure 4). The\norange solid line is the median and green dotted line the mean. Each graph represents\nthe total convergence time for curriculum and target training. Runs that do not reach\nhighest reward at any point in the execution are not included in the box plots since\nthere is no convergence time to include for those runs. However, above each box, we\nalso include the percentage of runs that converge to the highest cumulative reward,\n24\n\n\n(a) The Step Number Where Each Algorithm Reaches A Reward of 900 for 10 Con-\nsecutive Episodes\n(b) The Step Number Where Best Performing 80% of the Runs Reach A Reward of\n900 for 10 Consecutive Episodes\nFig. 4\nBox Plots for the Convergence Times of the Algorithms in Key-Lock Domain.\nwhich tells us how likely it is for the runs of an algorithm to reach the highest cumula-\ntive reward during execution. If the convergence rate above the graph is low, it means\nthat the given algorithm has a lot of runs that fail to converge.\nFigure 4(b) results are selected from the fastest converging 80% of runs so the\ngraph contains the same number of runs for every algorithm.\nREAD-C-SA with 100 clusters has the lowest variance in the convergence times\nand highest rate of convergence (Figures 4(a) & 4(b)). READ-C-TD and its vari-\nants generally result in better mean and median convergence times than the baseline\nalgorithms whereas READ-C-SA approaches generate varying performance results\ndepending on how many clusters are used for the curriculum generation. READ-C-SA\nwith 100 clusters using the simpler source environment produces the lowest mean con-\nvergence time followed closely by READ-C-TD + proximity. READ-C-SA with 100\nclusters also shows only a slightly higher convergence rate than READ-C-TD and its\nvariants arguably making it the best converging variant of READ-C (Figure 4(a)). On\n25\n\n\nthe other hand, READ-C-SA with source tasks similar to target and same as target\nshow worse mean and median performance than READ-C-TD and proximity aware\nbaseline. The problem in these approaches stem from the regressor training on larger\nstate spaces with higher variance and error rates, which consequently harms the per-\nformance on the target environment and shows the importance of using a suitable\nsmall environment in modeling the uncertainty of the agent in a particular domain. If\nwe only look at 80% of the runs, the difference between median and mean performance\nclearly reduces for many of the best performing READ-C algorithms, indicating that\nthe outliers are influencing the mean convergence times being higher than medians\n(Figure 4(b)). READ-C-SA with 100 clusters still performs the best when looking at\n80% of the runs and the relative ordering of the algorithms mostly remains the same\nas before.\n6.2 Results for Capture-the-Flag Domain\nHere we provide the evaluation results and analysis for Capture-the-Flag domain. The\ndefault cluster size for READ-C-SA is 150. Other default settings are the same as\nresults for Key-Lock domain.\n(a) Curriculum + Target Performance for READ-C\nand Baselines\n(b) Curriculum + Target Performance for Variants\nof READ-C\n(c) Curriculum + Target Performance for Different\nRegressors\nFig. 5 Performance of the Curriculum-Learning Algorithms as a Function of Training Steps in\nCapture-the-Flag Domain.\n26\n\n\nFigure 5(a) shows the comparison of READ-C-SA, READ-C-TD and no curricu-\nlum baseline. Figure 5(b) shows the variants of READ-C-TD and Figure 5(c) displays\ndifferent regressors trained on a 10×10 environment, a 20×20 environment similar to\ntarget task and the target environment respectively, on the capture-the-flag domain.\nFigure 6(a) , Figure 6(b) & Figure 6(c) display the effect of different cluster numbers\non the performance of READ-C-SA using different target environment sizes. Finally,\nwe present the box plots for convergence times of READ-C approaches on the capture-\nthe-flag domain illustrating the distribution of the number of steps required to reach\na given cumulative reward for a given number of consecutive episodes similar to the\nkey-lock domain (Figure 7(a) & Figure 7(b)). Shaded regions show 95% confidence\nintervals.\n(a) Cluster Size Comparison for 10 × 10 Target Envi-\nronment\n(b) Cluster Size Comparison for 20 × 20 Target\nEnvironment\n(c) Cluster Size Comparison for 30 × 30 Target Envi-\nronment\nFig. 6 Effect of Cluster Size on the Performance of the Curriculum-Learning Algorithms in Capture-\nthe-Flag Domain.\nREAD-C-SA converges faster than the no curriculum baseline and READ-C-TD\n(Figure 5(a)). READ-C-TD + proximity performs better than other READ-C-TD\nvariants (Figure 5(b)). Using a simpler source environment for the regressor results in\nbetter performance compared to other types of regressors (Figure 5(c)). The results\nare mainly consistent with what we have seen on the key-lock domain and further\nsupports the evidence seen in the prior graphs. When it comes to the different cluster\nnumbers, there does not emerge a meaningful difference in the performance of the\n27\n\n\n(a) The Step Number Where Each Algorithm Reaches A Reward of 84 for 500 Consecutive Episodes\n(b) The Step Number Where Best Performing 80% of the Runs Reach A Reward of 75 for 500 Con-\nsecutive Episodes\nFig. 7\nBox Plots for the Convergence Times of the Algorithms in Capture-the-Flag Domain.\nalgorithms in the small environment due to the environment taking little time to train\ndespite the moderate cluster sizes reaching a slightly higher convergence point than\nthe other options (Figure 6(a)). For the moderate-size environment, using a moderate\nnumber of clusters such as 150 seems to result in the best performance although most\nof the cluster sizes show similar results in this domain (Figure 6(b)). For the large-\nsize environment, a moderate number of clusters between 159 and 600 again produces\ngood performance similar to the key-lock domain (Figure 6(c)). The choice of cluster\nnumbers does not appear to have a large influence on the performance for capture-\nthe-flag domain regardless of the target environment size and using any number of\n28\n\n\nclusters in this experiment results in more or less slightly better outcome than the\nbaseline approaches (Figures 6(b)& 6(c)).\nREAD-C-SA with no clusters shows the lowest variance in the convergence times\nand produces the best mean and median performance while having a very low con-\nvergence rate meaning that many of its runs are classified as outliers (Figure 7).\nREAD-C-SA with source task simpler than the target task gives the second best mean\nand median performance while also having the highest convergence rate (Figure 7(a)),\narguably making it the best performing algorithm in this domain. READ-C-SA with\nsource task similar to the target task and same as the target task result in poor-\nest convergence rates (Figure 7(a)) and along with the results from the key-lock\ndomain, prove that to obtain good performance on READ-C-SA, it is necessary to\ntrain the regressor on a simpler environment showing similarities to the target task.\nREAD-C-SA with 500 clusters shows good median and mean performance similar\nto READ-C-SA with 200 clusters but it displays a higher variance among its runs\nindicating that using a too high number of clusters could make the algorithm more\nsensitive to the random weight initialization (Figure 7(a)). If we only look at 80%\nof the runs, the variance among multiple executions clearly reduces for many of the\nbest performing READ-C algorithms (Figure 7(b)). Overall, the results for capture-\nthe-flag domain show less variance (when accounted for the scale of the y-axis) and\nless performance differences among different READ-C approaches compared to the\nkey-lock domain, partially being caused by less-complex structure of the environment\ncharacteristics (Figure 7(a) & Figure 7(b)).\n6.3 Results for Parking Domain\nHere we provide the evaluation results and analysis for the Parking domain. The clus-\nter size remains the same for all READ-C-SA configurations. Other default settings\nare the same as the results from the grid environments.\nFigure 8(a) contains the comparison of READ-C-SA, READ-C-TD and advantage\nactor-critic algorithm [49] for the parking domain specified in Leurent [47], Figure 8(b)\nshows the variants of READ-C-TD and Figure 8(c) displays different regressors used\nfor READ-C-SA. Figure 8 performs 25 runs of each algorithm similar to before. We\ndo not present the results for different numbers of clusters as it does not add any\nadditional significant findings to our results. Finally, we present the box plots showing\nthe convergence times of each algorithm for a given cumulative reward value for 25\nruns (Figure 9).\nREAD-C-TD performs better than READ-C-SA and A2C in the parking environ-\nment while READ-C-SA still shows slight performance improvements over the baseline\n(Figure 8(a)). The main reason for this difference between the parking and the grid\nworld environments is that since the parking domain contains continuous state and\naction spaces, and READ-C uses relative entropy for continuous variables, the error\nrate for the GBM regressor ends up being higher than before causing READ-C-SA\nto fall behind READ-C-TD in certain cases. READ-C-TD also performs better than\nits variants in this domain although READ-C-TD + proximity almost gives the same\nperformance results as READ-C-TD (Figure 8(b)). Most of the algorithms display\noverlapping confidence intervals but the confidence intervals for the baseline algorithm\n29\n\n\n(a) Curriculum + Target Performance for READ-C\nand Baselines\n(b) Curriculum + Target Performance for Variants\nof READ-C\n(c) Curriculum + Target Performance for Different\nRegressors\nFig. 8 Performance of the Curriculum-Learning Algorithms as a Function of Training Steps in the\nParking Domain.\nstill remains significantly below the variants of READ-C showing that READ-C is\nable to provide improvement in learning efficiency for both continuous and discrete\ndomains (Figure 8(b)). READ-C-SA with source task simpler than the target task\nshows better performance than the other two competing regressors showing consis-\ntent results with the grid world domains (Figure 8(c)). Although the gap between\nREAD-C and the baseline algorithm is narrower in these results, there still exists\na benefit of using READ-C in the continuous domain environments since most of\nREAD-C variants consistently manage to remain above the benchmark criteria aside\nfrom READ-C-SA with source task similar to the target task.\nFor the convergence times, all READ-C approaches show better mean and median\nperformance than the A2C algorithm but they also produce worse convergence rates\n(Figure 9(a)). The lower convergence rates in this case does not mean that the agent\nis not learning the task since we have seen in the line graphs that the asymptotic\nperformance of the agent was better than the baseline algorithm but rather, the lower\nconvergence rates indicate that the agent in some runs is not able to reach the reward\nvalue of -15 for 10 consecutive episodes despite improving its policy performance.\nHowever, if we look at the results obtained using the best performing 80% runs, we see\nthat all READ-C algorithms, except READ-SA with source task similar to the target\ntask, still produce better mean and median values while having the same convergence\n30\n\n\n(a) The Step Number Where Each Algorithm Reaches A Reward of -15 for 10 Consecutive Episodes\n(b) The Step Number Where Best Performing 80% of the Runs Reach A Reward of -20 for 10 Con-\nsecutive Episodes\nFig. 9\nBox Plots for the Convergence Times of the Algorithms in Parking Domain.\nrate (Figure 9(b)), meaning that the reason READ-C shows low convergence rate\nin the first case is because of the 20% poor performing runs. Overall, READ-C-SA\nmanages to outperform its competition albeit it cannot guarantee convergence for\nall cases. READ-C-TD shows better convergence rates than its variants and READ-\nC-SA, which is consistent with the results of the line graphs (Figure 9(a)). Also,\nREAD-C-SA with simpler source task than the target task obtains a better mean and\nmedian convergence rate than READ-C-SA with source task same as the target task\nand READ-C-SA with source task similar to the target task, which is consistent with\nthe results we obtained in the Key-Lock domain (Figure 9(b)).\n31\n\n\n7 Conclusion and Future Work\nWe presented READ-C, a novel approach to using relative entropy for automatic cur-\nriculum generation. We detailed two versions of the approach, one using a teacher\nto measure relative entropy and another self-calculating the relative entropy without\nthe teacher. We adapted a curriculum generation criteria based on max policy change\nfrom the literature to compare against READ-C and evaluated performance on the\nkey-lock domain, capture-the-flag domain and the parking domain using discrete and\ncontinuous action and state spaces. Our results showed that READ-C was able to out-\nperform a proximity-aware randomly generated curriculum, the comparison criteria,\nand learning the target task directly in all our experiments. Further, READ-C-SA pro-\nduced better results than the baselines and READ-C-TD in most cases even without\nhaving access to a teacher model.\nDeclarations\nFunding\nThe authors did not receive support from any organization for the submitted work.\nConflict of interest/Competing interests\nThe authors have no relevant financial or non-financial interests to disclose.\nEthics approval and consent to participate\nThere are no ethical considerations which we feel must be specifically highlighted here.\nConsent for publication\nAll authors consent to the publication of the materials presented in this paper.\nMaterials Availability\nNot applicable\nData and Code availability\nAll relevant resources will be made available on Github upon publication.\nAuthor contribution\nMethodology:\nMuhammed\nYusuf\nSatici;\nFormal\nanalysis\nand\ninvestigation:\nMuhammed Yusuf Satici; Writing - original draft preparation: Muhammed Yusuf\nSatici; Writing - review and editing: Muhammed Yusuf Satici, Jianxun Wang, David\nRoberts; Supervision: David Roberts.\n32\n\n\nReferences\n[1] Forbes-Riley, K., Litman, D.: Adapting to student uncertainty improves\ntutoring\ndialogues,\nvol.\n200,\npp.\n33–40\n(2009).\nhttps://doi.org/10.3233/\n978-1-60750-028-5-33\n[2] Pon-Barry, H., Schultz, K., Bratt, E., Clark, B., Peters, S.: Responding to student\nuncertainty in spoken tutorial dialogue systems. I. J. Artificial Intelligence in\nEducation 16, 171–194 (2006)\n[3] Forbes-Riley, K., Litman, D.: Designing and evaluating a wizarded uncertainty-\nadaptive spoken dialogue tutoring system. Comput. Speech Lang. 25(1), 105–126\n(2011) https://doi.org/10.1016/j.csl.2009.12.002\n[4] Jog, V., Loh, P.-L.: Teaching and learning in uncertainty. IEEE Transactions\non Information Theory 67(1), 598–615 (2021) https://doi.org/10.1109/TIT.2020.\n3030842\n[5] Vygotsky, L.S.: Mind and Society: The Development of Higher Mental Processes.\nHarvard University Press, Cambridge, MA (1978). http://www.learning-theories.\ncom/vygotskys-social-learning-theory.html\n[6] Stuyf, R.R.V.D.: Scaffolding as a teaching strategy. In: Adolescent Learning and\nDevelopment (2002)\n[7] Settles, B.: Active learning literature survey. (2009)\n[8] Settles, B., Craven, M.W.: An analysis of active learning strategies for sequence\nlabeling tasks. In: EMNLP (2008)\n[9] Khan, F., Mutlu, B., Zhu, J.: How do humans teach: On curriculum learning and\nteaching dimension. In: Shawe-Taylor, J., Zemel, R.S., Bartlett, P.L., Pereira, F.,\nWeinberger, K.Q. (eds.) Advances in Neural Information Processing Systems 24,\npp. 1449–1457. Curran Associates, Inc., ??? (2011). http://papers.nips.cc/paper/\n4466-how-do-humans-teach-on-curriculum-learning-and-teaching-dimension.\npdf\n[10] Peng, B., MacGlashan, J., Loftin, R., Littman, M.L., Roberts, D.L., Taylor, M.E.:\nCurriculum design for machine learners in sequential decision tasks. IEEE Trans-\nactions on Emerging Topics in Computational Intelligence 2(4), 268–277 (2018)\nhttps://doi.org/10.1109/TETCI.2018.2829980\n[11] MacAlpine, P., Stone, P.: Overlapping layered learning. Artificial Intelligence\n254, 21–43 (2018) https://doi.org/10.1016/j.artint.2017.09.001\n[12] Ren, Z., Dong, D., Li, H., Chen, C.: Self-paced prioritized curriculum learning\n33\n\n\nwith coverage penalty in deep reinforcement learning. IEEE Transactions on Neu-\nral Networks and Learning Systems PP, 1–11 (2018) https://doi.org/10.1109/\nTNNLS.2018.2790981\n[13] Seita, D., Chan, D., Rao, R., Tang, C., Zhao, M., Canny, J.: ZPD Teaching\nStrategies for Deep Reinforcement Learning from Demonstrations (2019)\n[14] Florensa, C., Held, D., Wulfmeier, M., Abbeel, P.: Reverse curriculum generation\nfor reinforcement learning. CoRR abs/1707.05300 (2017) arXiv:1707.05300\n[15] Zhang, Y., Abbeel, P., Pinto, L.: Automatic Curriculum Learning through Value\nDisagreement (2020)\n[16] Klink, P., D’Eramo, C., Peters, J., Pajarinen, J.: Self-Paced Deep Reinforcement\nLearning (2020)\n[17] Portelas, R., Colas, C., Hofmann, K., Oudeyer, P.-Y.: Teacher algorithms for\ncurriculum learning of Deep RL in continuously parameterized environments\n(2019)\n[18] Huang, P., Xu, M., Zhu, J., Shi, L., Fang, F., ZHAO, D.: Curriculum reinforce-\nment learning using optimal transport via gradual domain adaptation. In: Koyejo,\nS., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in\nNeural Information Processing Systems, vol. 35, pp. 10656–10670. Curran Asso-\nciates, Inc., ??? (2022). https://proceedings.neurips.cc/paper files/paper/2022/\nfile/4556f5398bd2c61bd7500e306b4e560a-Paper-Conference.pdf\n[19] Klink, P., Yang, H., D’Eramo, C., Peters, J., Pajarinen, J.: Curriculum reinforce-\nment learning via constrained optimal transport. In: Chaudhuri, K., Jegelka,\nS., Song, L., Szepesvari, C., Niu, G., Sabato, S. (eds.) Proceedings of the 39th\nInternational Conference on Machine Learning. Proceedings of Machine Learn-\ning Research, vol. 162, pp. 11341–11358. PMLR, ??? (2022). https://proceedings.\nmlr.press/v162/klink22a.html\n[20] Wu, B., Gupta, J.K., Kochenderfer, M.: Model primitives for hierarchical lifelong\nreinforcement learning. Auton Agent Multi-Agent Syst 34(28) (2020) https://\ndoi.org/10.1007/s10458-020-09451-0\n[21] Silva, F.L., Costa, A.H.R.: Object-oriented curriculum generation for reinforce-\nment learning. In: Andr´e, E., Koenig, S., Dastani, M., Sukthankar, G. (eds.)\nProceedings of the 17th International Conference on Autonomous Agents and\nMultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pp.\n1026–1034. International Foundation for Autonomous Agents and Multiagent\nSystems Richland, SC, USA / ACM, ??? (2018). http://dl.acm.org/citation.cfm?\nid=3237850\n[22] Svetlik, M., Leonetti, M., Sinapov, J., Shah, R., Walker, N., Stone, P.: Automatic\n34\n\n\ncurriculum graph generation for reinforcement learning agents. In: AAAI (2017)\n[23] Qiao, Z., M¨ulling, K., Dolan, J., Palanisamy, P., Mudalige, P.: Automatically\ngenerated curriculum based reinforcement learning for autonomous vehicles in\nurban environment. (2018). https://doi.org/10.1109/IVS.2018.8500603\n[24] Matiisen, T., Oliver, A., Cohen, T., Schulman, J.: Teacher-Student Curriculum\nLearning (2017)\n[25] Narvekar, S., Sinapov, J., Stone, P.: Autonomous task sequencing for customized\ncurriculum design in reinforcement learning. In: Proceedings of the 26th Interna-\ntional Joint Conference on Artificial Intelligence. IJCAI’17, pp. 2536–2542. AAAI\nPress, ??? (2017). http://dl.acm.org/citation.cfm?id=3172077.3172241\n[26] W¨ohlke, J., Schmitt, F., Hoof, H.: A performance-based start state curriculum\nframework for reinforcement learning. AAMAS ’20, pp. 1503–1511. Interna-\ntional Foundation for Autonomous Agents and Multiagent Systems, Richland,\nSC (2020)\n[27] Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor, M.E., Stone, P.: Cur-\nriculum Learning for Reinforcement Learning Domains: A Framework and Survey\n(2020)\n[28] Madhawa, K., Murata, T.: Active learning for node classification: An evaluation.\nEntropy 22(10) (2020) https://doi.org/10.3390/e22101164\n[29] Bougie, N., Ichise, R.: Goal-driven active learning. Auton Agent Multi-Agent Syst\n35(44) (2021) https://doi.org/10.1007/s10458-021-09527-5\n[30] Sutton, R.S., Precup, D., Singh, S.: Between mdps and semi-mdps: A framework\nfor temporal abstraction in reinforcement learning. Artificial Intelligence 112(1),\n181–211 (1999) https://doi.org/10.1016/S0004-3702(99)00052-1\n[31] Stolle, M., Precup, D.: Learning options in reinforcement learning, vol. 2371, pp.\n212–223 (2002). https://doi.org/10.1007/3-540-45622-8 16\n[32] Mankowitz, D.J., Mann, T.A., Mannor, S.: Adaptive Skills, Adaptive Partitions\n(ASAP) (2016)\n[33] Kearns, M., Mansour, Y., Ng, A.Y.: A sparse sampling algorithm for near-optimal\nplanning in large markov decision processes. In: Proceedings of the 16th Inter-\nnational Joint Conference on Artificial Intelligence - Volume 2. IJCAI’99, pp.\n1324–1331. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (1999)\n[34] Harb, J., Bacon, P.-L., Klissarov, M., Precup, D.: When Waiting is not an Option\n: Learning Options with a Deliberation Cost (2017)\n[35] Bacon, P.-L., Harb, J., Precup, D.: The Option-Critic Architecture (2016)\n35\n\n\n[36] Harutyunyan, A., Dabney, W., Borsa, D., Heess, N., Munos, R., Precup, D.: The\nTermination Critic (2019)\n[37] Sutton, R.S., Barto, A.G.: Reinforcement Learning - an Introduction, 2nd edn.\nAdaptive computation and machine learning. MIT Press, Cambridge, MA (1998).\nhttp://www.worldcat.org/oclc/37293240\n[38] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,\nGraves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie,\nC., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S.,\nHassabis, D.: Human-level control through deep reinforcement learning. Nature\n518(7540), 529–533 (2015)\n[39] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver,\nD., Kavukcuoglu, K.: Asynchronous Methods for Deep Reinforcement Learning\n(2016)\n[40] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,\nBlondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,\nCournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine\nlearning in Python. Journal of Machine Learning Research 12, 2825–2830 (2011)\n[41] Ward, J.H.: Hierarchical grouping to optimize an objective function. Journal of\nthe American Statistical Association 58(301), 236–244 (1963)\n[42] Karmakar, P., Bhatnagar, S.: Two Timescale Stochastic Approximation with\nControlled Markov noise and Off-policy temporal difference learning (2017)\n[43] Wu, Y., Zhang, W., Xu, P., Gu, Q.: A Finite Time Analysis of Two Time-Scale\nActor Critic Methods (2022)\n[44] Holzleitner, M., Gruber, L., Arjona-Medina, J., Brandstetter, J., Hochreiter, S.:\nConvergence Proof for Actor-Critic Methods Applied to PPO and RUDDER\n(2020)\n[45] Nesterov, Y.: Lectures on Convex Optimization, 2nd edn. Springer, Switzerland\nAG (2018)\n[46] Konidaris, G., Barto, A.: Building portable options: Skill transfer in reinforcement\nlearning. In: Proceedings of the 20th International Joint Conference on Artificial\nIntelligence, pp. 895–900 (2007)\n[47] Leurent, E.: An Environment for Autonomous Driving Decision-Making. GitHub\n(2018)\n[48] Pardo, L.: Statistical inference based on divergence measures. (2005). https://\napi.semanticscholar.org/CorpusID:116968026\n36\n\n\n[49] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver,\nD., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning.\nCoRR abs/1602.01783 (2016) 1602.01783\n37\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21166v1.pdf",
    "total_pages": 37,
    "title": "Autonomous Curriculum Design via Relative Entropy Based Task Modifications",
    "authors": [
      "Muhammed Yusuf Satici",
      "Jianxun Wang",
      "David L. Roberts"
    ],
    "abstract": "Curriculum learning is a training method in which an agent is first trained\non a curriculum of relatively simple tasks related to a target task in an\neffort to shorten the time required to train on the target task. Autonomous\ncurriculum design involves the design of such curriculum with no reliance on\nhuman knowledge and/or expertise. Finding an efficient and effective way of\nautonomously designing curricula remains an open problem. We propose a novel\napproach for automatically designing curricula by leveraging the learner's\nuncertainty to select curricula tasks. Our approach measures the uncertainty in\nthe learner's policy using relative entropy, and guides the agent to states of\nhigh uncertainty to facilitate learning. Our algorithm supports the generation\nof autonomous curricula in a self-assessed manner by leveraging the learner's\npast and current policies but it also allows the use of teacher guided design\nin an instructive setting. We provide theoretical guarantees for the\nconvergence of our algorithm using two time-scale optimization processes.\nResults show that our algorithm outperforms randomly generated curriculum, and\nlearning directly on the target task as well as the curriculum-learning\ncriteria existing in literature. We also present two additional heuristic\ndistance measures that could be combined with our relative-entropy approach for\nfurther performance improvements.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}