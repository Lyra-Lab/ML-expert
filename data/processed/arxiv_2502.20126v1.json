{
  "id": "arxiv_2502.20126v1",
  "text": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality\nSamples with Less Compute\nSotiris Anagnostidis†,§,∗\nGregor Bachmann†,§\nYeongmin Kim†,‡\nJonas Kohler†\nMarkos Georgopoulos†\nArtsiom Sanakoyeu†\nYuming Du†\nAlbert Pumarola†\nAli Thabet†\nEdgar Sch¨onfeld†\n100.0 % FLOPs\n84.3 % FLOPs\n68.6 % FLOPs\n52.9 % FLOPs\n37.2 % FLOPs\nFigure 1. We flexify DiTs and adjust the compute per diffusion step, generating high-quality samples with significantly less compute.\nAbstract\nDespite their remarkable performance, modern Diffusion\nTransformers (DiTs) are hindered by substantial resource\nrequirements during inference, stemming from the fixed and\nlarge amount of compute needed for each denoising step. In\nthis work, we revisit the conventional static paradigm that\nallocates a fixed compute budget per denoising iteration\nand propose a dynamic strategy instead. Our simple and\nsample-efficient framework enables pre-trained DiT mod-\nels to be converted into flexible ones — dubbed FlexiDiT—\nallowing them to process inputs at varying compute bud-\ngets. We demonstrate how a single flexible model can gen-\nerate images without any drop in quality, while reducing\nthe required FLOPs by more than 40% compared to their\nstatic counterparts, for both class-conditioned and text-\nconditioned image generation. Our method is general and\nagnostic to input and conditioning modalities.\nWe show\n∗Work done during an internship at Meta GenAI. †Meta GenAI. §ETH\nZ¨urich. ‡KAIST. Correspondence to: sanagnos@ethz.ch\nhow our approach can be readily extended for video gen-\neration, where FlexiDiT models generate samples with up\nto 75% less compute without compromising performance.\n1. Introduction\nDiffusion models [87] have recently become the core\nbuilding block of major improvements in image genera-\ntion [10, 13, 24, 90, 107]. These models gradually denoise a\nrandom sample xt, drawn typically from pnoise = N(0, I),\nby iteratively calling a neural network trained to reverse\na pre-defined noise corruption process pθ(xt−1|xt). This\nprocess enables the generation of samples from the de-\nsired distribution pdata.The remarkable performance of these\nmodels is closely tied to the amount of computational re-\nsources invested in them, as evidenced by established scal-\ning laws [52]. The Transformer architecture [95] has proven\nto be highly scalable across various modalities, leading to\nits adoption in diffusion models, in the form of the recent\nDiffusion Transformer [78]. In DiTs, the denoising pro-\narXiv:2502.20126v1  [cs.LG]  27 Feb 2025\n\n\nDenoised Image\nSpatial Frequency\nGenerative Denoising Process\npdata\npnoise\n0.0\n0.1\n0.2\nLPIPS\npdata\npnoise\n0.0\n0.1\n0.2\n0.3\nL2\npdata\npnoise\n0.5\n0.6\n0.7\n0.9\n1.0\nSSIM\npdata\npnoise\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nDreamSim\nHigh Pass\nLow Pass\n0.8\nLPIPS\nSSIM\nL2\nDreamSim\nFigure 2. Diffusion can be viewed as spectral autoregression [25]. Left: Diffusion and its effect on the spatial frequency of images. Right:\nTo investigate the role of different frequency components in image generation, we apply a low or high pass filter to a single diffusion step\nupdate (while keeping all other updates unchanged). With all other sources of randomness fixed, we compare the generated samples with\nand without filtering using LPIPS [110], L2 distance of the pixels, SSIM [99] and DreamSim [32]. Notably, the influence of low and high\npass filters varies depending on whether they are applied early or late in the denoising process.\ncess pθ(xt−1|xt) is parametrized using Transformer blocks\ninstead of traditional convolutional layers. As popularized\nin Vision Transformers [26], (latent) images of dimension\nh × w are divided into patches of size p × p, which serve\nas input tokens that are transformed via a series of attention\nand feed-forward layers. The use of DiTs offers two key\nadvantages: (i) a unified architecture and input processing\nframework that facilitates multimodal applications and gen-\neralization to other domains, such as audio and video; and\n(ii) exceptional scalability due to their signal propagation\ncharacteristics and efficient training on modern hardware.\nThe computational complexity of a DiT with hidden di-\nmension d and depth L is O(LNd2 +LN 2d), where N cor-\nresponds to the total number of tokens1. Given T steps of\nthe diffusion process, function calls to the same monolithic\nDiT model are repeated for all T steps, and the total amount\nof compute is thus uniformly allocated. However, image\ngeneration via diffusion exhibits distinct, non-uniform, and\ntemporally — with respect to the noise process — varying\ncharacteristics. Consistent with intuition, prior work has ob-\nserved that high-level image features tend to emerge at early\ngenerative diffusion steps (i.e., large t’s). In contrast, later\nsteps refine and progressively generate high-frequency and\ndetailed visual features [14]. We illustrate further differ-\nences during the denoising generation in Fig. 2. Concisely,\ndifferent denoising steps have profoundly different influence\non high and low-level features of the resulting images.\nIn theory, different denoising methods and models can\nbe employed for each step t, i.e. one can use separate pa-\nrameters θt to learn pθt(xt−1|xt). While this notion has\nbeen previously explored in the literature, prior works pro-\npose computationally intensive solutions, such as training\n1For high-resolution image generation — even when diffusion is per-\nformed in a latent space — the second term O(LN2d) that corresponds to\nthe attention operations, can quickly become the main bottleneck.\nseparate expert denoisers for different t’s [5] or using model\ncascades [91]. These approaches face two significant limita-\ntions: (i) they require multiple models to be managed during\ninference, increasing memory demands. This can quickly\nbecome a major issue, especially as current trends favor the\nuse of larger and larger models. Moreover, (ii) using sepa-\nrate models restricts the opportunity for knowledge sharing\nacross steps. Although one can treat denoising steps inde-\npendently, the denoising process itself retains certain shared\ncharacteristics across steps. This inherent smoothness im-\nplies that separate models would need to learn these shared\nproperties individually. In this work, we address these chal-\nlenges and instead propose to perform different denoising\nsteps with varying levels of compute using a single model.\nIn summary, our contributions are the following:\n• We present a simple framework that flexifies DiT models,\nallowing them to convert samples into different sequences\nof tokens by adjusting the patch size. Processing samples\nas different sequences enables us to control the compute\nbudget being used for each denoising step.\n• By leveraging specific image characteristics at different\ndenoising steps, we demonstrate that strategically allo-\ncating less compute to certain steps can yield signifi-\ncant computational savings (over 40%) without compro-\nmising the quality of generated samples for both class-\nconditioned and text-conditioned image generation. We\nalso show how denoising based on a larger patch size can\nserve as a more effective guidance signal.\n• We illustrate the versatility of our framework by extend-\ning it to other modalities.\nFor video generation, we\nachieve substantial computational savings (up to 75%)\nwith no considerable drop in performance or conceptual\ndifferences in the generated samples.\n\n\n2. Background\nPatch size \nFLOPs\n(large patch size)\n(small patch size)\nFLOPs\nFigure 3. Tokenizing im-\nages into patches.\nFor simplicity, we limit the\ndiscussion here to images, and\ndetail later changes due to dif-\nferent modalities. DiTs use a\nTransformer encoder to process\nimage patches as tokens. Here-\nafter, we refer as tokenization\nto the process of converting a\n(latent) image into a series of\ntokens and as de-tokenization\nto the opposite process,\nof\ntransforming a series of tokens\nback into an image. Given an\nimage of size h × w and a chosen patch size2 p, an input\nimage is cropped into non-overlapping patches of dimen-\nsions Rp×p×cin, where cin denotes the number of channels\nof the input image.\nThe total number of tokens is then\nequal to N = (h/p) × (w/p). This (flattened) sequence\nof patches is then projected using a linear layer Membed with\nweights Rp∗p∗cin×d and potential biases Rd. This tokeniza-\ntion process is equivalent to performing a 2D convolution,\nwhere the kernel size and stride are both equal to p × p.\nThe embedded N tokens are then processed using L Trans-\nformer encoder layers.\nThe output tokens of dimension\nRN×d are projected back to the image space with a linear\nde-embedding layer Mde-embed with weights Rd×cout∗p∗p and\npotential biases Rcout∗p∗p. Here cout denotes the number of\noutput channels, typically cout = 2cin if the prediction in-\ncludes the variance, else cout = cin. DiTs adhere to the scal-\ning properties of Transformers, as demonstrated in various\nother applications [4, 37, 41, 52, 109].\nDiTs are an increasingly popular alternative to convolu-\ntional networks for denoising corrupted images during gen-\neration, i.e. modeling pθ(xt−1|xt). Diffusion defines two\nMarkov chains, the forward and the backward process. Dur-\ning the forward process, Gaussian noise3 is added to sam-\nples of the real distribution x0 ∼q(x0) [40]:\nq(x1:T |x0) =\nT\nY\nt=1\nq(xt|xt−1),\nwhere\nq(xt|xt−1) = N(xt−1;\np\n1 −βtxt−1, βtI).\n(1)\nIn the backward process, samples are drawn from a Gaus-\nsian noise distribution p(xT ) = pnoise = N(0, I) and then\ngradually denoised, using Tweedie’s Formula [28]:\n2In reality, different patch sizes can be considered along the height and\nthe width dimensions. We will however refrain from doing that.\n3Although we focus on Gaussian noise here, other corruptions apart\nfrom Gaussian noise have also been analysed [6, 20, 75].\npθ(xT :0) = p(xT )\n1\nY\nt=T\npθ(xt−1|xt),\nwhere\npθ(xt−1|xt) = N(xt; µθ(xt, t), Σθ(xt, t)).\n(2)\nTypically, a single model is used to model the prediction\npθ(xt−1|xt) for every t.\n3. Flexible Diffusion Transformers\nWe flexify DiTs, via small architectural changes, that al-\nlow them to process images as different length sequences,\nby adjusting the patch size p used in tokenization. Flexible\ntokenization of images has been utilized before for single-\nstep inference applications [9] and to accelerate training [3].\nWe instead propose to use different patch sizes at different\nsteps in the denoising process of the same image. This is\nbased on the following intuition:\nEarly steps focus on low-frequency details, which can be\nperformed with bigger patch sizes at the same quality.\nChanging the patch size p directly affects the total number\nof tokens (recall that N = (h/p) × (w/p)) and thus the\noverall compute required for a function evaluation. Hence,\nby leveraging bigger patch sizes only for early steps, we can\nreduce the overall generation time while maintaining both\nlow and high-frequency details in the produced images.\nWe obtain a flexible model — coined FlexiDiT — by\nmodifying and fine-tuning a pre-trained DiT, which enables\nit to process and understand images with new patch sizes.\nIn our experiments we focus on efficiency, so newly added\npatch sizes are always larger than the one of the pre-trained\nmodel, leading to fewer tokens and thus a smaller computa-\ntional footprint. The single FlexiDiT model can be instan-\ntiated in different modes depending on the selected patch\nsize. We will refer to instances of the model that use the\noriginal and smaller patch size ppowerful as powerful, com-\npared to using a weak model with a larger patch size pweak.\nTo simplify the discussion, we largely ignore how additional\nconditioning may be applied for now, and instead refer to\nspecific implementation details in the experiments section.\nDiTs — as any Transformer — can process sequences\nof any arbitrary length N. Fundamentally, we just need to\nmodify (i) how tokenization and de-tokenization are per-\nformed to ensure that the input and output representation\nspace stay unchanged and (ii) ensure that the model can in-\nterpret these different length sequences. In the following,\nwe discuss and outline two different ways that FlexiDiTs\ncan be derived, based on whether the forward pass for the\npre-trained model (we refer to this as the target model) is\npreserved exactly or not. In both cases, we emphasize that\nany additional fine-tuning and new parameters are minimal.\n\n\npdata\npnoise\nDenoising steps\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nL1 between errors in predictions\nof the weak and powerful model\nUse weak model when\n denoising these steps\nUse powerful model when\n   denoising these steps\nFigure 4. Left: We flexify DiTs by allowing them to process images with more patch sizes, by changing the lightweight embedding and\nde-embedding layers. We showcase this for a class-conditioned ImageNet model. Right: We plot the difference in predictions between a\nweak and a powerful model. For the first denoising steps, differences are small, and thus using the weak model there allows accelerated\ngeneration without performance degradation.\nIn all our experiments, training is stable, and no special\ntricks are necessary, while the total compute for fine-tuning\nis less than 5% of the original pre-training compute.\n3.1. Shared Parameters for all Sequences\nWhen the original training data is available, it becomes\npossible to fine-tune the pre-trained model while preserving\nits performance, along with its existing abilities, biases, and\npotential safety features. We demonstrate that this approach\nenables processing images with varying patch sizes by in-\ntroducing only minimal additional trainable parameters, as\nshown in Fig. 4 (left). Below, we detail the specific compo-\nnents of the architecture that require adaptation.\nTokenization:\nWe introduce a new embedding layer\nM flex\nembed, for some underlying patch size p′ with new weights\nwflex\nembed ∈Rp′∗p′∗cin×d and biases bflex\nembed ∈Rd.\nThen,\nwhen instantiating a FlexiDiT with a patch size pcurrent,\nwe project these weights with a fixed projection matrix\nQembed ∈Rpcurrent∗pcurrent×p′∗p′ to the desired shape, and per-\nform the 2D convolution with kernel size and stride equal\nto pcurrent × pcurrent. We use as Qembed the pseudo-inverse\nof the bi-linear interpolation projection, as this leads to\nbetter norm preservation of the output [9]. We initialize\nthe weights based on the pre-trained parameters and the\npre-trained patch size as wflex\nembed = Qembed†wembed4 and\nbflex\nembed = bembed, preserving the functional form of the model\nfor the pre-trained patch size. When adding positional en-\ncodings, we identify for each patch its pixel coordinates in\nthe original image [15, 103]. We additionally introduce a\npatch size embedding for each of the patch sizes used by\nthe model, which we add to every token in the sequence,\nand patch size dependent layer-normalization layers. These\nlayers help with signal propagation [36, 77, 104] by preserv-\ning the norms of the activations and let the model recover\n4Here † denotes the pseudo-inverse. All projection matrices Q multiply\neach channel separately.\nits expressivity [49, 100].\nDe-tokenization:\nWe similarly adapt the de-embedding\nlayer Mde-embed.\nFor an underlying patch size p′, we\ndefine a new layer M flex\nde-embed with weights wflex\nde-embed\n∈\nRd×cout∗p′∗p′ and biases bflex\nde-embed ∈Rcout∗p′∗p′. Depend-\ning on the current patch size pcurrent in the neural net-\nwork evaluation, we project with a fixed projection ma-\ntrix Qde-embed ∈Rp′∗p′×pcurrent∗pcurrent to wflex\nde-embedQde-embed and\nbflex\nde-embedQde-embed. Again, we use as Qde-embed the pseudo-\ninverse of the bi-linear interpolation, now with flipped di-\nmensions. We initialize the new parameters as wflex\nde-embed =\nwde-embedQde-embed† and bflex\nde-embed = bde-embedQde-embed†. In\ntotal, less than 0.005 % of auxiliary parameters are intro-\nduced to attain a FlexiDiT for the models tested in this case.\n3.2. Different LoRAs for each Sequence\nIn practice, however, pre-training often requires exten-\nsive computational resources and may span multiple stages\nusing various datasets, which may not always be accessi-\nble, even for models with open weights.\nIn such cases,\nfine-tuning can have unintended effects, potentially dimin-\nishing model capabilities [46] or compromising safety guar-\nantees [80]. For such situations, where it is essential to pre-\nserve the original forward pass of a pre-trained model, we\ndemonstrate a method that enables fine-tuning across differ-\nent patch sizes with minimal additional compute and a small\nsupplementary dataset. Fig. 5 illustrates our approach.\nWe perform similar changes to the model, but instead\nof training the DiT block parameters, we freeze the origi-\nnal weights and add trainable LoRAs [42]. These LoRAs\nare specialized for each new patch size and activated only\nwhen our FlexiDiT is instantiated with that. There are no\nLoRAs for the patch size of the pre-trained model. For the\ntokenization and de-tokenization layers, we simply add new\nlayers Membed, Mde-embed for each new patch size. As before,\n\n\nImage\nEncoder\nA pyramind built\nby an advanced\nlost civilization\nText\nEncoder\nMulti-Head \nSelf-Attention\nMulti-Head \nCross-Attention\nPowerfull Patch\nEmbedder\nSelf-Attention\nLoRAs\nTraining\nPointwise\nFeedforward\nFeedforward\nLoRAs\nWeak Patch\nEmbedder\nPowerfull Patch\nDe-embedder\nWeak Patch \nDe-embedder\nMulti-Head \nSelf-Attention\nMulti-Head \nCross-Attention\nPowerfull Patch\nEmbedder\nSelf-Attention\nLoRAs\nInference\nwith LoRAs\nPointwise\nFeedforward\nFeedforward\nLoRAs\nWeak Patch\nEmbedder\nPowerfull Patch\nDe-embedder\nWeak Patch \nDe-embedder\nMulti-Head \nSelf-Attention\nMulti-Head \nCross-Attention\nPowerfull Patch\nEmbedder\nInference\nwithout LoRAs\nPointwise\nFeedforward\nWeak Patch\nEmbedder\nPowerfull Patch\nDe-embedder\nMerged-LoRAs\nMulti-Head \nSelf-Attention\nMulti-Head \nCross-Attention\nMerged LoRAs\nPointwise\nFeedforward\nWeak Patch De-\nembedder\nFrozen weights\nTrainable weights\nShared parameters\nFigure 5. We preserve the functional form of the target model for the pre-trained patch size and add new trainable parameters (LoRAs)\nfor each additional patch size we want to fine-tune the model to operate with. We showcase this for a text-to-image/video model that uses\ncross-attention for text conditioning. We find that freezing cross-attention layers without any additional LoRAs works the best. During\ninference, we can either keep the LoRAs unmerged (Inference with LoRAs) leading to a slight FLOPs increase that depends on the LoRAs’\ndimensions, or create different copies of the model for each patch size, by merging the LoRAs (Inference without LoRAs). The latter leads\nto additional memory requirements. FLOPs and parameter numbers on the right correspond to our flexible T2I Emu model.\nwe use a patch size embedding, but only for the new patch\nsizes. Note that functional preservation is imposed for the\npre-trained patch size, i.e. inference using only the pow-\nerful model generates exactly the same samples. We fine-\ntune by using the predictions of the original model (pow-\nerful model) as labels to distill knowledge [38] for the new\npatch sizes, i.e. we train to minimize:\nEt,xt∥ϵθ(xt−1|xt; ppowerful) −ϵθ(xt−1|xt; pweak)∥2.\nWe find that this leads to improved performance, faster con-\nvergence [18] and better alignment between predictions of\ndifferent patch sizes, which can be important given poten-\ntial discrepancies in the data used during pre-training and\nour fine-tuning.\nHere we use ϵθ to denote the model’s\nprediction parametrizing the distribution pθ.\nNote that\nϵθ(xt−1|xt; ppowerful) has no trainable parameters.\nDuring inference, we have two options: (i) keep the\nnew LoRA parameters unmerged, which incurs a minor\nadditional computational cost, or (ii) merge these weights\ninto a copy of the original model parameters, with some\nadditional memory cost.\nIf we keep the LoRA parame-\nters of dimension dlora unmerged, the computational com-\nplexity of the corresponding linear layer with input and\noutput (dinput, doutput) will change from Ndinputdoutput to\nN(dinputdoutput + dinputdlora + dloradoutput).\nIf we merge\nLoRAs, there is no computational overhead, but the new\nmerged parameters need to be kept in memory. Depending\non the model requirements and the available resources, one\ncan choose between the two options. We note that compute\noverhead by keeping LoRAs unmerged is minimal, see also\nFig. 5 (right). For all models tested, additional parameters\nin this case are less than 5% of the original model parame-\nters. Further details and ablations can be found in App. C.\n3.3. Inference Scheduler\nAt every denoising step, we need to decide how to instan-\ntiate our FlexiDiT, i.e. which patch size to use. Following\nour intuition, we find that for early steps of the denoising\nprocess, weak and powerful models produce similar pre-\ndictions, and thus using the weak model preserves quality\nwhile reducing computational complexity, as also seen in\nFig. 4 (right). We therefore propose an inference scheduler\nthat, starting from random noise pnoise, first denoises using\na weak model for the first Tweak steps and switches to the\npowerful model for the last Tpowerful = T −Tweak steps.\nBy adjusting the steps performed by the weak model Tweak,\nwe can adjust the amount of compute that we are saving.\nIn practice, unless otherwise mentioned, we fine-tune mod-\nels to process images with one additional patch size. We\nchoose this patch size, corresponding to the weak model,\nas 2× larger than the one corresponding to the powerful\nmodel. Then, the sequence length corresponding to tokeniz-\ning with the new patch size is 4× smaller, and thus compute\nrequired for the powerful model is > 4× compared to the\nweak model.\n3.4. Generation Guidance\nFor conditional generation,\nclassifier-free guidance\n(CFG) is typically employed [29, 39] to enhance sample\nquality. This entails performing two neural function eval-\nuations (NFEs) (or one NFE with twice the batch size)\nto compute predictions with and without the conditioning\nc, i.e.\nϵθ(xt−1|xt, c) and ϵθ(xt−1|xt, ∅).\nSampling can\n\n\nFigure 6. Left: As the weak model is used more extensively during generation, compute benefits increase, but at the cost of some\nperformance degradation. Middle: The optimal CFG scale varies depending on the extent to which the weak model is used. Each line\ncorresponds to an inference scheduler that applies the weak model for a different proportion of denoising steps. Right: Benefits from our\ninference scheduler are orthogonal to performing a smaller overall number of diffusion steps. We plot FID for different overall number of\nsteps T and different number of weak steps Tweak, using in every case the DDPM scheduler.\nthen take place as ϵθ(xt−1|xt, ∅) + scfg(ϵθ(xt−1|xt, c) −\nϵθ(xt−1|xt, ∅)), where scfg is the guidance scale. Recent\nwork [53] has shown that using a smaller or less well-\ntrained version of the model rather than an unconditional\nmodel can lead to better guidance signal [1, 83]. We adapt\nthese findings in our setting, leading to better generation\nquality without the need to train or deploy additional mod-\nels. For each denoising step, given a patch size used for the\nconditional pcond and a patch size used for guidance puncond,\nwe compute:\n\n\n\n\n\n\n\n\n\nϵθ(xt−1|xt, ∅; puncond) + scfg(ϵθ(xt−1|xt, c; pcond)−\nϵθ(xt−1|xt, ∅; puncond)),\nif pcond = puncond\nϵθ(xt−1|xt, c; puncond) + scfg(ϵθ(xt−1|xt, c; pcond)−\nϵθ(xt−1|xt, c; puncond)),\nif pcond < puncond\nIn this setup, we use the powerful model for the conditional\nprediction and leverage the weak model’s output as guid-\nance. Unlike traditional approaches, our method applies\nguidance based on the conditional prediction from the weak\nmodel. Our guidance scheme requires performing inference\nusing both the weak (for the unconditional) and the power-\nful (for the conditional) model, for some denoising steps.\nWe show in Fig. 12 (appendix) how this can be efficiently\nimplemented, making use of packing [23]. Depending on\nthe guidance signal used, optimum generation quality can\nvary with respect to the guidance scale scfg. This will be-\ncome more apparent in the following experiments.\n4. Experiments\nFor clarity, we present efficiency gains with respect to\nFLOPs and point to Section 4.4 for a detailed analysis of\nthe relationship between FLOPs and latency.\n4.1. Class-Conditioned Image Generation\nWe fine-tune models (DiT-XL/2) on ImageNet, using the\nsame setup as in [78]. Since training data is publicly avail-\nable, we fine-tune pre-trained models using the same pa-\nrameters for all sequences, without the use of LoRAs, as\ndescribed in Sec. 3.1. During training, we randomly noise\nimages according to Eq. (1), and learn to denoise using one\nof the available patch sizes. We primarily report FID and\npoint to App. B for more experiments and different metrics.\nIn practice, we use a pre-trained model with a patch size\nof 2 (powerful), that we fine-tune to also process images\nwith a patch size of 4 (weak). Since we are fine-tuning the\npowerful model, we can also “teach” it how to correct spe-\ncific mistakes made by the weak model, accumulated in the\nbackward process during the first Tweak steps. We provide\nmore details in App. B.1 on how to reduce this exposure\nbias [57, 76]. Unless otherwise mentioned, reported metrics\nare computed by generating images at resolution 256×256,\nusing 250 steps of the DDPM scheduler [40, 78].\nCompute gains.\nWe generate images with our Flex-\niDiT and the proposed inference scheduler, varying the\namount of compute by adjusting the number of initial de-\nnoising steps Tweak performed with the weak model. For\neach level of compute, we report the FID of the generated\nimages in Fig. 6 (left and middle). In general, perform-\ning a few steps with the weak model (60-100% of base-\nline compute) leads to no drop in performance. Saving even\nmore compute is possible, albeit at the cost of a minor drop\nin the quality of the generated images. When using only\nthe powerful mode of our FlexiDiT, we get the same per-\nformance — FID-50k = 2.25 — as the pre-trained DiT-\nXL/2 model — FID-50k = 2.27. Thus, fine-tuning for more\npatch sizes does not reduce the capacity of the model with\nrespect to the pre-trained one. We also show that other in-\nference schedulers, such as starting with the powerful model\nand switching to the weak model, lead to worse results (ap-\npendix Fig. 19), validating our intuition.\nRelation between T and Tweak.\nNaturally, the question\narises whether doing fewer overall diffusion steps T leads to\n\n\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14) \n10\n15\n20\n25\n30\n35\nMSCOCO FID - 10k \nFLOPs % compared to baseline\n92.9%\n85.8%\n78.8%\n71.7%\n64.6%\n57.5%\n   Baseline\nBaseline 4.5 CFG\n   Baseline\nBaseline 4.5 CFG\nModel (compute %)\nFID ↓\nCLIP ↑\nVQAScore ↑\nT2I Transf. (100 %)\n14.75\n25.60\n63.29\nT2I Transf. ( 86 %)\n14.72\n25.62\n63.30\nT2I Transf. ( 72 %)\n14.77\n25.63\n63.40\nT2I Transf. ( 58 %)\n14.71\n25.58\n63.26\nEmu (100 %)\n26.00\n26.05\n70.17\nEmu ( 84 %)\n25.96\n26.06\n70.19\nEmu ( 69 %)\n26.00\n26.08\n70.37\nEmu ( 53 %)\n26.10\n26.07\n70.09\nSteps (Powerful/Weak)\nVotes (in %)\nOurs\nBaseline\nWin\nTie\nLose\n50 (40,10)\n42 (42,0)\n33.5\n42.5\n24.0\n50 (30,20)\n34 (34,0)\n35.5\n41.5\n23.0\n50 (20,30)\n26 (26,0)\n35.5\n44.0\n20.5\n50 (10,40)\n18 (18,0)\n43.0\n41.5\n15.5\nWe asked 8 annotators to express preferences between pairs of images\nfor a given prompt. We generate images for 200 prompts and collect\n3 different votes per each pair of images, collectively 2400 votes.\nFigure 7. Left: We plot FID vs CLIP score for images generated with different CFG scales using the T2I Transf. model (we refer to\nthe appendix for results regarding our Emu model). The red line represents images generated with varying CFG scales using only the\n(powerful) target model. By employing our dynamic scheduler, we can match image quality in terms of both FID and text alignment\nwhile significantly reducing compute requirements. Middle: Our flexible models can match the baseline (for a fixed pre-defined guidance\nscale scfg) across benchmarks, with significantly less compute. Right: Human study results show votes indicating a win, tie, or loss\nfor our method compared to a baseline, which corresponds to running the pre-trained model (only the powerful model) for fewer steps.\nComparisons are between Emu inference modes that require approximately equal FLOPs and time.\nthe same efficiency improvements compared to performing\nmore steps with the weak model. After applying the DDPM\nscheduler for a different number of overall diffusion steps\nT, we plot in Fig. 6 (right) the efficiency gains across them.\nResults indicate that gains from performing weak steps are\northogonal to performing fewer overall diffusion steps. In\nother words, more steps are required to achieve better im-\nage fidelity, but performing some of the initial steps with our\nweak model is sufficient to achieve the targeted fidelity. In\nApp. B.3, we provide further insights on how the predic-\ntions of the weak model closely align with the ones of the\npowerful model. This similarity supports parameter shar-\ning, which not only reduces resource requirements during\ninference but also enables faster convergence during fine-\ntuning.\n4.2. Text-conditioned Image Generation\nThe universality of Transformers and the holistic view of\ndifferent input sources as sequences of tokens implies that\nthe generalization of the architecture to more modalities and\na variety of different inputs is straightforward. This is also\nthe case for DiTs, which have been already extended to ac-\ncommodate text conditioning [7, 15], video generation [72]\nand speech synthesis [67]. We showcase how our frame-\nwork can be applied out of the box to state-of-the-art text-to-\nimage (T2I) model architectures. T2I DiTs have the same\narchitecture as class-conditioned DiTs, with the exception\nthat conditioning is imposed via cross-attention.\nWe fine-tune T2I models, by introducing new parameters\nin the form of LoRAs. Specifically, we use a DiT following\nPIXART [15] — we refer to this as T2I Transf. — generat-\ning 256 × 256 images and a 1.7B DiT based on EMU [19]\n— we refer to this as Emu — generating 1024 × 1024 im-\nages. Implementation details are provided in App. B.5, in\nshort, we fine-tune both low and high-resolution target mod-\nels with a pre-trained patch size of 2, to also support a patch\nsize of 4. For T2I Transf., we follow the inference scheduler\nprotocol in [15], and use the DDPM solver for 100 steps.\nWe point to the App. B for results with different solvers and\nnumber of steps (namely 20 steps of DPM, 25 steps of SA-\nsolver, and a varying number of steps of the DDPM solver)\nshowing that performance benefits are orthogonal to these\nchoices. As before, we vary the percentage of denoising\nsteps with the weak model for either the conditional or the\nguidance predictions.\nCLIP-vs-FID.\nWe report FID and CLIP score alignment\nfor captions from the MS COCO dataset with varying CFG\nscale values in Fig. 7 (left). As before, we notice that lower\ncompute versions of our model require a higher CFG scale\nto generate images that match similar FID vs CLIP score\nvalues of the baseline (full compute) model. We show that\nfor a fixed CFG scale of the baseline model — we chose\nscfg = 4.5 for the T2I Transf. model as proposed in [15] and\nscfg = 6.0 for our Emu model — we can match the attained\nperformance for less than 60% of the original compute. De-\ntailed scores are given in Fig. 7 (middle).\nAdditional benchmarks.\nWe perform additional eval-\nuations on the final generated images.\nWe follow [59]\nand present text alignment for visual question-answering\n(VQA). We generate images based on the captions of Draw-\nBench, TIFA160, Pick-a-Pic, Winoground and report aver-\nage VQA scores (we point to App. B for detailed results).\nWe compare the baseline model (same CFG scales as be-\nfore), against dynamic inference with our flexible models.\nConsistent with our previous findings, results indicate that\nperforming a few denoising steps with our weak model gen-\nerally generates high-quality images. We can again save up\nto 40% of compute without a performance drop. For high-\nresolution images generated from our Emu model, we also\nembark on a human study, as shown in Fig. 7 (right). There\nwe validate that human annotators prefer images from our\ndynamic scheduler instead of using similar compute, but\n\n\nAn astronaut running through an alley in Rio de Janeiro.\nA white and orange tabby cat is seen happily darting through a dense garden, as if chasing something. Its eyes are wide and happy as it jogs forward, scanning the branches, flowers, and leaves as it walks. The path is na\nrrow as it makes its way between all the plants. the scene is captured from a ground-level angle, following the cat closely, giving a low and intimate perspective. The image is cinematic with warm tones and a grainy text\nure. The scattered daylight between the leaves and plants above creates a warm contrast, accentuating the cat s orange fur. The shot is clear and sharp, with a shallow depth of field.\n10 %\n25 %\n50 %\n75 %\n100 %\nFLOPS % to baseline\n68.5\n69.0\n69.5\n70.0\n70.5\nVBench score (%) \n70.4\n70.4\n70.5\n70.4\n70.4\n70.4\n70.4\n70.4\n70.3\n70.2\n68.7\nBaseline\n   70.4\nWeak Model\nTemporal (2,2,2)\nSpatial (1,4,4)\nFigure 8. Left: Samples from our flexible Video DiT model using 25.2% of compute compared to the pre-trained baseline. Right: We\nperform a varying number of steps of the denoising process with a weak model, using either our spatial or our temporal weak model. Both\nweak models lead to significant compute savings with little to no degradation in performance.\nthis time uniformly allocated.\n4.3. Text-Conditioned Video Generation\nFinally, we also explore other modalities, such as video\ngeneration. Text-to-video (T2V) generation typically fol-\nlows the same setup as T2I [72, 79].\nGiven a video of\ndimensions Rf×h×w×cin, where f is the temporal compo-\nnent corresponding to the number of frames, and a chosen\npatch size (pf, ph, pw) for each of the dimensions, the in-\nput (latent) video is cropped into non-overlapping spatial-\ntemporal patches of dimensions Rpf×ph×pw×cin. Patches are\nthen embedded using now a 3D convolutional layer into to-\nkens, subsequently transformed by the DiT blocks where\nevery token attends to every other token in the sequence,\nand finally projected back to patches with a linear layer.\nWe perform the same changes as for our T2I models in\nSec. 3.2, adding LoRAs for new patch sizes. We initialize\ntokenization and de-tokenization layers as before, and when\nthe temporal patch size pf is increased, we duplicate weights\nalong the temporal dimension. We fine-tune a 4.9B video\nmodel as the one in [79] — referred to as Video DiT —\nand train with distillation as before. We generate videos of\nresolution (f × h × w) = (256, 384, 704) using 250 steps\nof the DDPM scheduler as in [79]. Diffusion is performed\nin a latent space that down-samples each dimension f, h, w\nby 8. More dimensions of the latent space now offer more\npossibilities in terms of determining the characteristics of\nour weak model. For our experiments, we fine-tune a pre-\ntrained model with patch size (pf, ph, pw) = (1, 2, 2) to also\nsupport a patch size (2, 2, 2), we denote this weak model as\n‘temporal’, and (1, 4, 4), denoted as ‘spatial’. We then use\neither one of these modes as a weak model during inference.\nBoth modes could also be used iteratively for the generation\nof a single sample, which we leave for future work.\nBy adjusting the number of steps performed with our\nweak model, we can again control the overall compute re-\nquired per generated sample. We use VBench [45] to evalu-\nate the quality of the generated videos and report results in\nFig. 8. In this case, we can save up to 75% of compute with-\nout a significant drop in performance. Recent training-free\nmethods [51, 62, 113], while appealing due to their lack of\ntraining requirements, achieve significantly lower compute\nsavings before performance begins to degrade noticeably.\nThis demonstrates that allowing the model to learn opti-\nmized compute allocation is more effective than relying on\npredefined rules for inference efficiency. Additional com-\nparisons to previous work are provided in the appendix.\n4.4. FLOPs vs Latency\nHigh-resolution image/video generation is predomi-\nnantly compute-bound. To verify, we propagate sequences\nof different lengths through a fixed size DiT and measure\nperformance — FLOPs and latency — on a NVIDIA H100\nSXM5 with a batch size of 2, simulating inference with\nCFG5. In Fig. 9 we show that the weak Emu and Video\nDiT models are also compute-bound, for the setup (gen-\nerated image/video resolution) that we presented in the pa-\nper. Indeed, for T2V, FLOPs utilization is higher for our\nweak models, due to inefficiencies of the self-attention op-\neration for large sequence lengths when using the powerful\nmodel. This effect can be expected to be even more pre-\ndominant for multi-GPU inference. Consequently, latency\nbenefits are even higher than FLOPs benefits presented so\nfar.\n5. Conclusion\nWe have demonstrated how regular DiTs can be con-\nverted into flexible ones, that can process samples with dif-\nferent patch sizes after minimal fine-tuning. Adjusting the\ncompute for some denoising steps in the diffusion process\nreadily allows accelerating inference without compromis-\ning. Notably, the efficiency benefits of our approach are in-\ndependent of the chosen solver or the number of denoising\nsteps. We have displayed how our approach is generic and\ncan be straightforwardly applied to class-conditioned image\n5In all cases, we compile using torch.compile with fullgraph=True and\nmode = ’reduce-overhead’.\n\n\n102\n103\n104\n105\nOperational Intensity (FLOPs / byte)\n100T\n1000T\nPerformance (FLOPs / second)\nT2I(1, 4, 4)\n(406.1T)\nT2I(1, 2, 2)\n(417.2T)\nT2V(1, 4, 4)\n(398.2T)\nT2V(2, 2, 2)\n(373.7T)\nT2V(1, 2, 2)\n(345.1T)\nMemory Bandwidth\nPeak FLOPs Limit\nFigure 9. GPU utilization for one denoising step, when propa-\ngating sequences with different overall number of tokens, corre-\nsponding to different patch sizes (pf, ph, pw). Our T2I model has\nno temporal dimension, but we overload notation and set pf = 1.\nFor simplicity, we use a DiT of similar configuration (width and\ndepth) for both the T2I and T2V reported in this plot numbers, but\nresults generalize across model shapes.\ngeneration, low and high-resolution text-conditioned image\ngeneration, and text-conditioned video generation. Looking\nahead, we anticipate further applications of our flexible DiT\nframework across various modalities, such as audio and 3D\nmodeling. As computational resources become increasingly\nin demand, developing efficient and adaptable models like\nours will be crucial for enabling generative capabilities that\nare of high-quality, but also more scalable.\nReferences\n[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok\nJang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Ky-\nong Hwan Jin, and Seungryong Kim. Self-rectifying diffu-\nsion sampling with perturbed-attention guidance, 2024. 6\n[2] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo\nNoci, Aurelien Lucchi, and Thomas Hofmann. Dynamic\ncontext pruning for efficient and interpretable autoregres-\nsive transformers.\nAdvances in Neural Information Pro-\ncessing Systems, 36:65202–65223, 2023. 14\n[3] Sotiris Anagnostidis, Gregor Bachmann, Imanol Schlag,\nand Thomas Hofmann. Navigating scaling laws: Compute\noptimality in adaptive model training. In Forty-first Inter-\nnational Conference on Machine Learning, 2024. 3, 26\n[4] Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hof-\nmann. Scaling mlps: A tale of inductive bias. Advances in\nNeural Information Processing Systems, 36, 2024. 3\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Ait-\ntala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image\ndiffusion models with an ensemble of expert denoisers.\narXiv preprint arXiv:2211.01324, 2022. 2, 14\n[6] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid\nKazemi, Furong Huang, Micah Goldblum, Jonas Geiping,\nand Tom Goldstein. Cold diffusion: Inverting arbitrary im-\nage transforms without noise. Advances in Neural Informa-\ntion Processing Systems, 36, 2024. 3\n[7] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with\nbetter captions.\nComputer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8, 2023. 7\n[8] Lucas Beyer, Xiaohua Zhai, Am´elie Royer, Larisa Mar-\nkeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge\ndistillation: A good teacher is patient and consistent. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 10925–10934, 2022. 14\n[9] Lucas Beyer,\nPavel Izmailov,\nAlexander Kolesnikov,\nMathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias\nMinderer, Michael Tschannen, Ibrahim Alabdulmohsin,\nand Filip Pavetic. Flexivit: One model for all patch sizes.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14496–14506, 2023.\n3, 4, 14, 26\n[10] BlackForestlabs\nAI.\nFlux.\nhttps : / /\nblackforestlabs.ai/#get-flux, 2024. [Online;\naccessed 17-Oct-2024]. 1\n[11] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023. 15\n[12] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao\nZhang, Christoph Feichtenhofer, and Judy Hoffman. To-\nken merging:\nYour vit but faster.\narXiv preprint\narXiv:2210.09461, 2022. 14\n[13] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh.\nVideo generation models as world simulators,\n2024. 1\n[14] Angela Castillo, Jonas Kohler, Juan C P´erez, Juan Pablo\nP´erez, Albert Pumarola, Bernard Ghanem, Pablo Arbel´aez,\nand Ali Thabet. Adaptive guidance: Training-free accel-\neration of conditional diffusion models.\narXiv preprint\narXiv:2312.12487, 2023. 2, 21\n[15] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,\nEnze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffu-\nsion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023. 4, 7, 22, 26\n[16] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei\nYao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan\nLu, and Zhenguo Li. Pixart-\\sigma: Weak-to-strong train-\ning of diffusion transformer for 4k text-to-image genera-\ntion. arXiv preprint arXiv:2403.04692, 2024. 14\n[17] Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong\nTan, and Xinchao Wang.\nAsyncdiff: Parallelizing diffu-\nsion models by asynchronous denoising.\narXiv preprint\narXiv:2406.06911, 2024. 17\n[18] Jang Hyun Cho and Bharath Hariharan. On the efficacy of\nknowledge distillation. In Proceedings of the IEEE/CVF\n\n\ninternational conference on computer vision, pages 4794–\n4802, 2019. 5\n[19] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in\na haystack. arXiv preprint arXiv:2309.15807, 2023. 7\n[20] Giannis Daras,\nMauricio Delbracio,\nHossein Talebi,\nAlexandros G Dimakis, and Peyman Milanfar.\nSoft dif-\nfusion: Score matching for general corruptions.\narXiv\npreprint arXiv:2209.05442, 2022. 3\n[21] Giannis Daras, Yuval Dagan, Alex Dimakis, and Constanti-\nnos Daskalakis. Consistent diffusion models: Mitigating\nsampling drift by learning to be consistent. Advances in\nNeural Information Processing Systems, 36, 2024. 15\n[22] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In International Conference on Machine Learn-\ning, pages 7480–7512. PMLR, 2023. 27\n[23] Mostafa\nDehghani,\nBasil\nMustafa,\nJosip\nDjolonga,\nJonathan Heek, Matthias Minderer, Mathilde Caron, An-\ndreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M\nAlabdulmohsin, et al. Patch n’pack: Navit, a vision trans-\nformer for any aspect ratio and resolution.\nAdvances in\nNeural Information Processing Systems, 36, 2024. 6\n[24] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 1, 22\n[25] Sander Dieleman.\nDiffusion is spectral autoregression,\n2024. 2\n[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 14\n[27] St´ephane d’Ascoli, Hugo Touvron, Matthew L Leavitt,\nAri S Morcos, Giulio Biroli, and Levent Sagun. Convit: Im-\nproving vision transformers with soft convolutional induc-\ntive biases. In International conference on machine learn-\ning, pages 2286–2296. PMLR, 2021. 17\n[28] Bradley Efron.\nTweedie’s formula and selection bias.\nJournal of the American Statistical Association, 106(496):\n1602–1614, 2011. 3\n[29] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified\nflow transformers for high-resolution image synthesis. In\nForty-first International Conference on Machine Learning,\n2024. 5\n[30] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichten-\nhofer. Multiscale vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vi-\nsion, pages 6824–6835, 2021. 14\n[31] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural\npruning for diffusion models, 2023. 14\n[32] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy\nChai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream-\nsim: Learning new dimensions of human visual similar-\nity using synthetic data. arXiv preprint arXiv:2306.09344,\n2023. 2\n[33] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch,\nBernhard Sch¨olkopf, and Alexander Smola. A kernel two-\nsample test. The Journal of Machine Learning Research,\n13(1):723–773, 2012. 15\n[34] Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew\nTao, Jose M Alvarez, Jan Kautz, and Pavlo Molchanov.\nFastervit: Fast vision transformers with hierarchical atten-\ntion. arXiv preprint arXiv:2306.06189, 2023. 14\n[35] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and\nArash Vahdat. Diffit: Diffusion vision transformers for im-\nage generation. In European Conference on Computer Vi-\nsion, pages 37–55. Springer, 2025. 15\n[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectifiers: Surpassing human-level per-\nformance on imagenet classification. In Proceedings of the\nIEEE international conference on computer vision, pages\n1026–1034, 2015. 4\n[37] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun, Tom B\nBrown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws\nfor autoregressive generative modeling.\narXiv preprint\narXiv:2010.14701, 2020. 3\n[38] Geoffrey Hinton. Distilling the knowledge in a neural net-\nwork. arXiv preprint arXiv:1503.02531, 2015. 5\n[39] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural informa-\ntion processing systems, 33:6840–6851, 2020. 3, 6, 15\n[41] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. Training compute-optimal large language mod-\nels. arXiv preprint arXiv:2203.15556, 2022. 3, 25\n[42] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models, 2021.\n4\n[43] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari\nOstendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accu-\nrate and interpretable text-to-image faithfulness evaluation\nwith question answering. arXiv preprint arXiv:2303.11897,\n2023. 24\n[44] Huaibo Huang, Xiaoqiang Zhou, Jie Cao, Ran He, and Tie-\nniu Tan. Vision transformer with super token sampling. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 22690–22699, 2023. 14\n[45] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang\nSi, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang\nJin, Nattapol Chanpaisit, et al. Vbench: Comprehensive\n\n\nbenchmark suite for video generative models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 21807–21818, 2024. 8, 27\n[46] Adam Ibrahim, Benjamin Th´erien, Kshitij Gupta, Mats L.\nRichter,\nQuentin Anthony,\nTimoth´ee Lesort,\nEugene\nBelilovsky, and Irina Rish. Simple and scalable strategies\nto continually pre-train large language models, 2024. 4\n[47] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas\nHofmann, Sotiris Anagnostidis, and Sidak Pal Singh.\nTransformer fusion with optimal transport. arXiv preprint\narXiv:2310.05719, 2023. 14\n[48] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas\nHofmann, Sotiris Anagnostidis, and Sidak Pal Singh.\nTransformer fusion with optimal transport. In The Twelfth\nInternational Conference on Learning Representations,\n2024. 14\n[49] Sergey Ioffe. Batch normalization: Accelerating deep net-\nwork training by reducing internal covariate shift. arXiv\npreprint arXiv:1502.03167, 2015. 4\n[50] Bowen Jing, Gabriele Corso, Renato Berlinghieri, and\nTommi Jaakkola. Subspace diffusion generative models. In\nEuropean Conference on Computer Vision, pages 274–289.\nSpringer, 2022. 14\n[51] Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu,\nMenglin Jia, Michael S Ryoo, and Tian Xie.\nAdaptive\ncaching for faster video generation with diffusion trans-\nformers. arXiv preprint arXiv:2411.02397, 2024. 8, 15\n[52] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 1, 3\n[53] Tero Karras, Miika Aittala, Tuomas Kynk¨a¨anniemi, Jaakko\nLehtinen, Timo Aila, and Samuli Laine.\nGuiding a dif-\nfusion model with a bad version of itself. arXiv preprint\narXiv:2406.02507, 2024. 6\n[54] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta\nTakida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji,\nand Stefano Ermon.\nPagoda: Progressive growing of a\none-step generator from a low-resolution diffusion teacher.\narXiv preprint arXiv:2405.14822, 2024. 14\n[55] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland\nMatiana, Joe Penna, and Omer Levy. Pick-a-pic: An open\ndataset of user preferences for text-to-image generation.\nAdvances in Neural Information Processing Systems, 36:\n36652–36663, 2023. 24\n[56] Jonas Kohler, Albert Pumarola, Edgar Sch¨onfeld, Artsiom\nSanakoyeu, Roshan Sumbaly, Peter Vajda, and Ali Tha-\nbet. Imagine flash: Accelerating emu diffusion models with\nbackward distillation.\narXiv preprint arXiv:2405.05224,\n2024. 15\n[57] Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, and\nMarie-Francine Moens. Alleviating exposure bias in dif-\nfusion models through sampling with shifted time steps.\narXiv preprint arXiv:2305.15583, 2023. 6, 15\n[58] Yangming Li and Mihaela van der Schaar. On error prop-\nagation of diffusion models. In The Twelfth International\nConference on Learning Representations, 2023. 15\n[59] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide\nXia, Graham Neubig, Pengchuan Zhang, and Deva Ra-\nmanan. Evaluating text-to-visual generation with image-\nto-text generation. arXiv preprint arXiv:2404.01291, 2024.\n7\n[60] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide\nXia, Graham Neubig, Pengchuan Zhang, and Deva Ra-\nmanan. Evaluating text-to-visual generation with image-\nto-text generation. In European Conference on Computer\nVision, pages 366–384. Springer, 2025. 23\n[61] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maxim-\nilian Nickel, and Matt Le. Flow matching for generative\nmodeling. arXiv preprint arXiv:2210.02747, 2022. 15\n[62] Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Fac-\ncio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan-\nManuel Perez-Rua, and J¨urgen Schmidhuber. Faster diffu-\nsion via temporal attention decomposition. arXiv e-prints,\npages arXiv–2404, 2024. 8, 15\n[63] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui\nShen, and Liang-Chieh Chen. Alleviating distortion in im-\nage generation via multi-resolution diffusion models. arXiv\npreprint arXiv:2406.09416, 2024. 15\n[64] Xiangcheng Liu, Tianyi Wu, and Guodong Guo.\nAdap-\ntive sparse vit: Towards learnable adaptive token prun-\ning by fully exploiting self-attention.\narXiv preprint\narXiv:2209.13802, 2022. 14\n[65] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao,\nRuoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun,\nJianfeng Gao, et al. Sora: A review on background, technol-\nogy, limitations, and opportunities of large vision models.\narXiv preprint arXiv:2402.17177, 2024. 15\n[66] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012–10022, 2021. 14\n[67] Zhijun Liu, Shuai Wang, Sho Inoue, Qibing Bai, and\nHaizhou Li. Autoregressive diffusion transformer for text-\nto-speech synthesis.\narXiv preprint arXiv:2406.05551,\n2024. 7\n[68] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for\ndiffusion probabilistic model sampling in around 10 steps.\nAdvances in Neural Information Processing Systems, 35:\n5775–5787, 2022. 22\n[69] Chenyang Lu, Daan de Geus, and Gijs Dubbelman.\nContent-aware token sharing for efficient semantic segmen-\ntation with vision transformers.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 23631–23640, 2023. 14\n[70] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao\nWang.\nLearning-to-cache: Accelerating diffusion trans-\nformer via layer caching. arXiv preprint arXiv:2406.01733,\n2024. 20\n[71] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache:\nAccelerating diffusion models for free. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15762–15772, 2024. 14, 17\n\n\n[72] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Zi-\nwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte:\nLatent diffusion transformer for video generation.\narXiv\npreprint arXiv:2401.03048, 2024. 7, 8\n[73] Zhiyuan Ma, Yuzhu Zhang, Guoli Jia, Liangliang Zhao,\nYichao Ma, Mingjie Ma, Gaofeng Liu, Kaiyan Zhang, Jian-\njun Li, and Bowen Zhou. Efficient diffusion models: A\ncomprehensive survey from principles to practices. arXiv\npreprint arXiv:2410.11795, 2024. 15\n[74] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 14297–14306, 2023. 15\n[75] Eliya Nachmani, Robin San Roman, and Lior Wolf.\nDenoising diffusion gamma models.\narXiv preprint\narXiv:2110.05948, 2021. 3\n[76] Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and\nItir Onal Ertugrul. Elucidating the exposure bias in diffu-\nsion models. arXiv preprint arXiv:2308.15321, 2023. 6,\n15\n[77] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Anto-\nnio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal\npropagation in transformers: Theoretical perspectives and\nthe role of rank collapse. Advances in Neural Information\nProcessing Systems, 35:27198–27211, 2022. 4\n[78] William Peebles and Saining Xie. Scalable diffusion mod-\nels with transformers, 2023. 1, 6, 17, 26\n[79] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjan-\ndra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen\nShi, Chih-Yao Ma, Ching-Yao Chuang, et al.\nMovie\ngen: A cast of media foundation models. arXiv preprint\narXiv:2410.13720, 2024. 8, 15\n[80] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi\nJia, Prateek Mittal, and Peter Henderson.\nFine-tuning\naligned language models compromises safety, even when\nusers do not intend to!, 2023. 4\n[81] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-\nformers see like convolutional neural networks?\nAd-\nvances in neural information processing systems, 34:\n12116–12128, 2021. 17\n[82] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu,\nJie Zhou, and Cho-Jui Hsieh.\nDynamicvit: Efficient vi-\nsion transformers with dynamic token sparsification. Ad-\nvances in neural information processing systems, 34:\n13937–13949, 2021. 14\n[83] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and\nRomann M. Weber. No training, no problem: Rethinking\nclassifier-free guidance for diffusion models, 2024. 6\n[84] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-\nmans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in neural in-\nformation processing systems, 35:36479–36494, 2022. 24\n[85] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas\nBlattmann, Patrick Esser, and Robin Rombach. Fast high-\nresolution image synthesis with latent adversarial diffusion\ndistillation. arXiv preprint arXiv:2403.12015, 2024. 16\n[86] Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao\nGong, and Yinqiang Zheng. Resmaster: Mastering high-\nresolution image generation via structural and fine-grained\nguidance. arXiv preprint arXiv:2406.16476, 2024. 14\n[87] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International confer-\nence on machine learning, pages 2256–2265. PMLR, 2015.\n1\n[88] Yang Song and Prafulla Dhariwal.\nImproved tech-\nniques for training consistency models.\narXiv preprint\narXiv:2310.14189, 2023. 16\n[89] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever.\nConsistency\nmodels.\narXiv\npreprint\narXiv:2303.01469, 2023. 15, 16, 18\n[90] Stability AI. Stablediffusion3. https://stability.\nai/news/stable-diffusion-3, 2024. [Online; ac-\ncessed 17-Oct-2024]. 1\n[91] Stability AI. Introducing stable cascade, 2024. 2\n[92] Vadim Sushko, Edgar Sch¨onfeld, Dan Zhang, Juergen Gall,\nBernt Schiele, and Anna Khoreva. You only need adversar-\nial supervision for semantic image synthesis. arXiv preprint\narXiv:2012.04781, 2020. 17\n[93] Gerald Tesauro et al. Temporal difference learning and td-\ngammon. Communications of the ACM, 38(3):58–68, 1995.\n15\n[94] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nWinoground:\nProbing vision and language models for\nvisio-linguistic compositionality.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5238–5248, 2022. 24\n[95] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances\nin neural information processing systems, 30, 2017. 1\n[96] Dimitri Von R¨utte, Sotiris Anagnostidis, Gregor Bachmann,\nand Thomas Hofmann. A language model’s guide through\nlatent space.\nIn International Conference on Machine\nLearning, pages 49655–49687. PMLR, 2024. 17\n[97] Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin,\nand Xiaodan Liang. Qihoo-t2x: An efficiency-focused dif-\nfusion transformer via proxy tokens for text-to-any-task.\narXiv preprint arXiv:2409.04005, 2024. 14\n[98] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multi-\nscale structural similarity for image quality assessment. In\nThe Thrity-Seventh Asilomar Conference on Signals, Sys-\ntems & Computers, 2003, pages 1398–1402. Ieee, 2003. 17\n[99] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. IEEE transactions on image pro-\ncessing, 13(4):600–612, 2004. 2\n\n\n[100] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang\nDai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang,\nSam Tsai, Jonas Kohler, et al. Cache me if you can: Ac-\ncelerating diffusion models through block caching. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6211–6220, 2024. 4, 14, 17\n[101] Xinjian Wu, Fanhu Zeng, Xiudong Wang, Yunhe Wang, and\nXinghao Chen. Ppt: Token pruning and pooling for efficient\nvision transformers.\narXiv preprint arXiv:2310.01812,\n2023. 14\n[102] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Pi-\notr Doll´ar, and Ross Girshick.\nEarly convolutions help\ntransformers see better.\nAdvances in neural information\nprocessing systems, 34:30392–30400, 2021. 14, 17\n[103] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou,\nZhaoqiang Liu, Jiawei Li, and Zhenguo Li. Difffit: Un-\nlocking transferability of large diffusion models via sim-\nple parameter-efficient fine-tuning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 4230–4239, 2023. 4\n[104] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin\nZheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tieyan Liu. On layer normalization in the trans-\nformer architecture. In International Conference on Ma-\nchine Learning, pages 10524–10533. PMLR, 2020. 4\n[105] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian,\nZiming Liu, and Tommi Jaakkola. Restart sampling for im-\nproving generative processes. Advances in Neural Informa-\ntion Processing Systems, 36:76806–76838, 2023. 27\n[106] Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang,\nJiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Sa-solver:\nStochastic adams solver for fast sampling of diffusion mod-\nels. Advances in Neural Information Processing Systems,\n36, 2024. 22\n[107] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding,\nShiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong,\nXiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-\nvideo diffusion models with an expert transformer. arXiv\npreprint arXiv:2408.06072, 2024. 1\n[108] Zhihang Yuan, Pu Lu, Hanling Zhang, Xuefei Ning, Lin-\nfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai,\nand Yu Wang. Ditfastattn: Attention compression for diffu-\nsion transformer models. arXiv preprint arXiv:2406.08552,\n2024. 14, 17\n[109] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and\nLucas Beyer. Scaling vision transformers. In Proceedings\nof the IEEE/CVF conference on computer vision and pat-\ntern recognition, pages 12104–12113, 2022. 3\n[110] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586–595, 2018. 2, 17\n[111] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 14\n[112] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yib-\ning Song, Gao Huang, Fan Wang, and Yang You. Dynamic\ndiffusion transformer.\narXiv preprint arXiv:2410.03456,\n2024. 14\n[113] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You.\nReal-time video generation with pyramid attention broad-\ncast. arXiv preprint arXiv:2408.12588, 2024. 8, 14, 15,\n17\n[114] Mingjian Zhu, Yehui Tang, and Kai Han.\nVision trans-\nformer pruning. arXiv preprint arXiv:2104.08500, 2021.\n14\n\n\nA. Detailed Related Work\nEfficiency in Vision Transformers [26] falls primarily into two broad categories: reducing the computation per token or\nthe number of tokens overall.\nReducing the amount of computation per token.\nReducing the amount of computation per token can be achieved by\nreducing the model size when training via distillation [8] or pruning the network after training [47, 114]. These methods\nagain though typically work with a static inference protocol. As in [112], we compare in Fig. 10 our class-conditioned Ima-\ngeNet FlexiDiT model, against popular based pruning techniques, based on Diff pruning [31], Taylor, magnitude or random\npruning [48]. As one can see, our dynamic scheduler outperforms these baselines. We note that our method can also be\n25 %\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nFID-50k \n3.24\n2.64\n2.31\n2.23\n2.25\n2.25\n3.5: pruned w/ Diff\n3.5: pruned w/ Taylor\n3.8: pruned w/ Magnitude\n4.5: pruned w/ Random\nFigure 10. We compare our dynamic scheduler with more baselines.\napplied in conjunction with pruning techniques to achieve even higher efficiency gains, offering potential orthogonal ben-\nefits. In concurrent work, [112] propose to adjust, in a dynamic way, the model during inference in different steps. In our\nwork, we do not train separate models for each target FLOPs ratio like they do, meaning that we can train a single model\nand decide how many FLOPs we want to invest during inference. This makes our approach more versatile. Additionally,\nour training is very stable, and no specific training tricks are required to converge successfully. That is how we were able\nto extend experiments to high-resolution image and video generation, achieving significantly better speed-ups than the ones\nthey reported. We also outperform their results when the compute budget is very small. Nonetheless, this work could inspire\nadaptive per-sample schedulers, that could open new future directions.\nGiven the large computational requirements of the attention operation, many methods nowadays focus on that to reduce\nthe overhead imposed. These methods commonly use some form of hierarchical attention [34, 66], skip (usually the first)\nattention layers altogether [102], or reduce the number of the attended keys [16, 30, 108], by commonly aggregating keys in\na spatial neighborhood or applying some form of windowed attention.\nReducing number of tokens.\nOur method primarily falls within the second category of reducing the overall number of\ntokens. Previous work here, typically relied on filtering [64, 82, 101], merging [12, 44, 69] or dropping [2]. Although merging\nworks well for applications that eventually lead to some pooling operations (like classification tasks or for the task of creating\nan image-specific embedding), it works significantly less well for applications that require dense (token-level) predictions,\nwhere some un-merging operation has to be defined. In other concurrent work, [97] reduces the number of representative\ntokens to calculate the attention over. Our approach resembles most [9], where vision Transformers are trained to handle\ninputs with varying patch resolution. By applying less compute for some steps, we can reduce computational complexity\nsignificantly, without a drop in performance.\nImage generation.\nIn the context of image generation, diffusion has been largely established as the method for attaining\nstate-of-the-art results. There have been previous works that try to take advantage of potential correlations between successive\ndenoising step predictions [86], by either caching intermediate results in the activations [71, 100], or in the attention [113].\nCaching has the advantage of a training-free method. Nonetheless, potential benefits are lower. Similar to our work, [5] use\ndifferent experts for different denoising steps. Instead of using different experts that require separate training and separate\ndeployment, we show how a single model can be easily formed into a flexible one that can be instantiated in different modes,\nwith each of its modes corresponding essentially to a different expert. Similar in-spirit approaches have been proposed that\nrely on the smaller compute requirements for lower resolution image generation [54, 111]. [50] also adapt the computation\nper step, by projecting into smaller subspaces. We instead, keep the dimension of the latent space and the characteristics\n\n\nof it the same across diffusion steps. Orthogonal gains to our approach are also possible through methods such as guidance\ndistillation [56, 74] and consistency models [89]. Our approach is also largely agnostic to the diffusion process and can\nbe applied out of the box for flow matching methods [61]. We point the interested reader to [73] for a survey for further\nefficiency in diffusion models. Finally, compared to other established techniques [35, 63] we do not fundamentally change the\narchitecture, which allows us to apply our framework effortlessly for numerous pre-trained models across different modalities.\nVideo generation.\nOur approach can be easily extended for video generation, and in principle for the generation of any\nmodality where some inductive bias (spatial, temporal, etc) is employed in the diffusion (latent) space. In video generation,\ntypically, latent video tokens are processed in parallel [11, 65, 79]. Training-free methods [51, 62, 113] have been proposed\nin this case to accelerate video generation. Benefits with training-free methods are nonetheless minimal before performance\ndegradation kicks in (see Table 1 in [51], where one can typically save less than 30%).\nAn interesting direction for future work involves adapting the inference scheduler, i.e. what patch size we are using for\neach denoising step, based on the requirements of each sample. It is natural to assume that when generating more static\nvideos, increasing the temporal patch size, and thus decreasing the amount of compute along the temporal dimension, will\nresult in smaller drops in performance. The same holds for the spatial patch sizes when generating images or videos that\nrequire less high-frequency details.\nB. Additional Experiments and Details\nWe provide additional experiments, complementary to the main text.\nB.1. Exposure Bias and Accumulation of Error\npnoise\npdata\nDistribution Distance\nWithout regularizer\nWith regularizer\nT\n0\nSteps with powerful model\nFigure 11. Left: We use maximum mean discrepancy to estimate the distribution mismatch between pθ(xt|xT :t+1) and q(xt|x0). Right:\nThe proposed bootstrapped loss. During training, we perform a few denoising steps with a weak model followed by a few denoising steps\nwith a powerful model (reminiscent of the scheduler during inference) and apply a distribution matching loss on the resulting samples.\nInference with diffusion suffers from exposure bias, due to the discrepancy of the input distribution during training and\ninference [21, 57, 58, 76]. Briefly, models are trained to denoise images sampled from the q(xt|x0) distribution [40]. Infer-\nence on the other hand is characterized by repeated model evaluations pθ(xt|xt+1) and any distribution mismatch between\npθ(xt|xT :t+1) and q(xt|x0) accumulates, as also shown in Fig. 11 (left). The error at each iteration depends on the model,\nwith a perfect model resulting in 0 error and thus no error accumulated. In our case, the accumulation of error is exacerbated\nby the characteristics of our model, where weak models could lead to higher, but also specific in nature, kinds of errors.\nTraining with the standard denoising objective, where real samples are randomly noised for some t, does not make the more\npowerful model aware of the nature of the mistakes made by the weak model, rendering it unable to potentially correct them.\nWe propose to mitigate this issue by introducing a bootstrapped distribution matching loss [93], as illustrated in Fig. 11\n(right). The loss is applied in a patch size-dependent manner, according to the desired inference protocol (from weak to\npowerful model calls during inference).\nGiven natural images x0, ˜x0 ∼q(x0), we sample two time points t1 > t2 and corrupt the images with noise xtarget\nt1\n∼\nq(xt1|x0), ˜xpred\nt2\n∼q(˜xt2| ˜x0). We then apply a chain of denoising steps ϵθ(˜xpred\nt−1|˜xpred\nt\n; p) for t ∈(t1, t2] and a patch size p.\nUltimately, we wish for the distributions of xtarget\nt1\nand ˜xpred\nt1\nto match, for which we employ the maximum mean discrepancy\n(MMD) [33]. To let the powerful model learn and potentially correct mistakes of the weak model and to simulate how\nour inference patch size scheduler works, we perform the first of these denoising steps with the weak model, followed by\ndenoising steps with the powerful model. Given a set of patch sizes {pi}k\ni=1 where p1 < p2 < · · · < pk and a number of\n\n\ndenoising steps to perform with each s1, s2, . . . , sk, where Pk\nj=1 sj = t2 −t1, we denoise with a given patch size pi for all\nt’s in (t1 + Pk\nj=i+1 sj, t1 + Pk\nj=i sj]. An illustration of this process can also be seen in Fig. 11 (right). When sampling\na time step t1, we bias our sampling similar to [85]. The proposed distribution matching loss, inspired by the notion of\nconsistency [88, 89], provides a principled way to correct the errors accumulated during inference. We note that we are\noptimizing a simple distribution matching loss, instead of over-optimizing according to desired downstream metrics (namely\nFID), thus not violating Goodhart’s law6. Different distribution matching losses (including discriminator-based losses) can\nalso be used. Note that we only correct this exposure bias for the class-conditioned image generation experiments, as this is\nthe only case where we fine-tune the powerful pre-trained model.\nB.2. Inference with Packing\nApproach 1\nImage 1 C\nImage 1\nUC\nPadding\nImage 2 C\nImage 2\nUC\nPadding\nImage 3 C\nImage 3\nUC\nPadding\nImage 4 C\nImage 4\nUC\nPadding\nApproach 2\nImage 1 C\nImage 1\nUC\nImage 2 C\nImage 2\nUC\nImage 3 C\nImage 3\nUC\nImage 4 C\nImage 4\nUC\nApproach 3\nImage 1 C\nImage 1\nUC)\nImage 2 C\nImage 2\nUC)\nImage 3 C\nImage 3\nUC)\nImage 4 C\nImage 4\nUC)\nApproach 4\nImage 1 C\nImage 1\nUC\nImage 2 C\nImage 2\nUC\nImage 3 C\nImage 3\nUC\nImage 4 C\nImage 4\nUC\n 80\n100\n120\n140\n160\n 80\n100\n120\n140\n160\n180\n200\nLatency (relative)\nGenerate images: 1\n 80\n100\n120\n140\n160\n 80\n100\n120\n140\n160\n180\n200\nLatency (relative)\nGenerate images: 4\n 80\n100\n120\n140\n160\n 80\n100\n120\n140\n160\n180\n200\nLatency (relative)\nGenerate images: 16\n 80\n100\n120\n140\n160\nFLOPs (relative)\n 80\n100\n120\n140\n160\n180\n200\nLatency (relative)\nGenerate images: 64\nApproach 1\nApproach 2\nApproach 3\nApproach 4\nFigure 12. Different approaches can be employed to perform forward passes with CFG when the conditional (C) and unconditional (UC)\npredictions use different patch sizes. Here, each row corresponds to a sequence of tokens propagated through the DiT, and each bracket\ncorresponds to a batch of sequences for a single NFE. Generally ‘Approach 2’ leads to the smallest amount of FLOPs, but for batch size\n1, inference can be memory bound for low-resolution image generation. ‘Approach 4’ mostly leads to the smallest latency, as long as the\nnumber of generated images is larger than 4, i.e. the ratio of the sequence lengths between the powerful and the weak model. On the right,\nwe plot FLOPs and Latency from the four different approaches of performing inference, for a different number of generated images. Batch\nsize plays a role here (class-conditioned image generation experiments) as generated images are of lower resolution, namely 256 × 256,\nand thus sequence lengths through the Transformer are smaller. Normalized FLOPs are determined based on ‘Approach 2’ and normalized\nlatency based on ‘Approach 3’. We use torch.compile with fullgraph=True and mode = ’reduce-overhead’.\nWe provide more details in Fig. 12, on how to perform inference with CFG when the conditional and unconditional\npredictions employ a different patch size. We show this for our class-conditioned model, but results easily generalize for\nall our FlexiDiT models. Performing CFG entails NFEs with double the batch size (or 2 distinct NFEs), for the conditional\nand unconditional input, respectively. Performing the conditional and unconditional calls with different patch sizes leads to\npropagating sequences of different lengths through the DiT. Depending on how these sequences are ‘packed’ together, and\nfor lack of a hardware-specific implementation of masked attention, more FLOPs can be traded for better latency. Our weak\nmodel additionally leads to memory benefits, which can be traded for a bigger batch size when serving the model. Notice\nthat current state-of-the-art image generation models in practice require much longer sequences compared to the 256 × 256\nimages generated here (see also Section 4.4) and so generation is compute-bound even when generating with batch size equal\nto 1.\n6Goodhart’s law states that: ‘When a measure becomes a target, it ceases to be a good measure’.\n\n\nB.3. What does the Model Learn?\nTransformers are composed of a series of channel mixing components — feed-forward layers with shared weight applied\nto all tokens in the sequence — and token mixing components — attention applied to tokens in the sequence. By coercing the\nmodel to learn the denoising objective when applied to images processed with different patch sizes, we are enforcing inductive\nbias in its weights and helping it better understand global structures in the image [27, 81, 102]. We test this hypothesis and\nevaluate what the model is learning in the following ways in Fig. 13. (left) We visualize using t-SNE, centered kernel\nalignment (CKA) between feature maps across layers when performing NFEs with different patch sizes. Activations across\nlayers exhibit similar transformations [96], except the early layers, where features are lower level, i.e. more patch specific.\n(right) We visualize the Jensen–Shannon divergence (JSD) between attention maps (interpolated to the same image space)\nwhen performing NFEs with different patch sizes. We compare using our FlexiDiT model with different patch sizes (Flexible)\nversus using two static models trained with different patch sizes (namely DiT-XL/2 and a trained from scratch DiT-XL/4). Our\nflexible model showcases lower JSD, demonstrating better knowledge transfer between the different patch sizes. We believe\nthat this “transfer” of knowledge is crucial to (1) confirm that parameter sharing across patch sizes is valid and (2) ensure that\nfine-tuning can be fast and sample efficient.\nFigure 13. Interpretability of the model activations and attention scores, when propagating samples tokenized with different patch sizes.\nB.4. Class-Conditioned Image Generation\nAdditional metrics.\nFor class-conditioned experiments on the main text we focused on the DiT-XL/2 [78] and FID as a\nmetric. Here, we report more metrics apart from FID, namely Inception Score (IS), sFID, and Precision/Recall. Results are\npresented in Fig. 14 for our flexible DiT-XL/2 model. We remind that for class-conditioned models, we fine-tune models\nusing our distribution matching loss. As a result, the powerful model that we get after fine-tuning is different to the pre-\ntrained checkpoints we start from. To verify that our weak model does not lead to less diverse samples, we embark on a small\nexperimental study to guarantee the diversity of generated images. We follow [92] and generate images from the same label\nmap. We then calculate pairwise similarity/distance between these images and average across all similarities/distances and\nall label maps. We use MS-SSIM [98], LPIPS [110] and plot results in Fig. 15. Results indicate very similar values in terms\nof the diversity of the generated images. We also provide some sample images demonstrating diversity from the baseline\nmodel in Fig. 16 and our tuned model in Fig. 17. Note that the diversity of the generated images is in general high and not\naffected much by using the weak model for more of the initial denoising steps.\nCaching distance.\nIn the main text, we have shown how weak and powerful models generate more similar predictions\nduring the early steps of the denoising process. Previous papers to accelerate diffusion have largely relied on caching [17,\n71, 100, 108, 113] previous activations, by taking advantage of the similarity in activations between successive steps. For\ncompleteness, we also plot the caching distance between activations of the same layer between successive generation steps in\nFig. 18. In this paper, we do not employ caching but focus on an orthogonal approach. We advocate that all steps are important\nfor high-quality image generation, as demonstrated by our experiments on reducing the overall number of generation steps.\n\n\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n2\n3\n4\n5\n6\nFID-50k \n3.24\n2.64\n2.31\n2.23\n2.25\n2.25\n1.25\n2.0\nCFG scale\n25 %\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00\n6.25\nsFID-50k \n1.25\n2.0\nCFG scale\n25 %\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n150\n200\n250\n300\n350\nIS-50k \n1.25\n2.0\nCFG scale\n25 %\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\nPrecision-50k \n1.25\n2.0\nCFG scale\n25 %\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\nRecall-50k \n1.25\n2.0\nCFG scale\nFigure 14. More metrics for our FlexiDiT based on the DiT-XL/2 model for class-conditioned generation on ImageNet. We plot (a) FID (b)\nsFID, (c) inception score, (d) precision, and (e) recall when generating 50, 000 samples with 250 steps of the DDPM schedule for various\nvalues of the CFG scales. Red lines correspond to the values that lead to the optimum FID scores for each compute level.\n25 %\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nLPIPS \n25 %\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nMS-SSIM \nFigure 15. Average distance/similarity of images generated from the same label map. Both metrics take values between 0 and 1.\nInstead of completely skipping steps, we simply invest less compute for them, and let the model decide how to allocate this\ncompute.\nAdditional schedulers.\nBased on the results on activation distance between successive denoising steps (Fig. 18), one\ncould argue that first denoising steps are also important and thus a better inference scheduler would deploy the powerful\nmodel for these as well. In practice, we found no benefit from deploying a scheduler that works like that. Notice though\nhow activation distance is high for the first denoising steps only for some of the layers. We additionally experimented with\ndynamic schedulers that choose the patch size of each denoising step based on the activation distance of different layers\nbetween successive denoising steps. We did not find additional potential benefits.\nIn this paper, we are training a single model that can denoise images with any patch size for any denoising step. Given\na fixed desired inference scheduler — i.e. if we know exactly which t’s to run with the powerful and the weak model —,\none can train a model specifically based on that, leading to undoubtedly better quality images for the same compute. Similar\ntechniques are regularly applied in consistency models [89]. Finally, we compare our scheduler — performing the first Tweak\ndenoising steps with a weak model — versus the opposite scheduler, i.e. performing the last Tweak denoising steps with a\nweak model. Results in Fig. 19 indicate that, as expected, using the weak model in the last diffusion steps is suboptimal,\nleading to a loss in fine-grained details. We also provide qualitative examples of how these different schedulers affect image\nquality in Fig. 20.\n\n\nFigure 16. Sample images generated with the baseline DiT-XL/2 for the ImageNet category ‘Brambling’.\nFigure 17. Sample images generated with our flexible DiT-XL model when performing inference using only the powerful model, for the\nImageNet category ‘Brambling’.\nMore results on CFG.\nIn the main text, we presented results on performing inference with different CFG scales and\ndifferent invocations to our weak model for the unconditional and conditional part. The 4 generated curves in Fig. 6 (middle)\ncorrespond to performing our scheduler as 250/250, 130/130, 70/70, and 30/0 where x/y means using the powerful model\nfor the last x denoising steps for the conditional and y denoising steps for the unconditional part. When performing CFG, we\n\n\nBlock 0\n2\n4\n6\n8\n10\nBlock 1\n0\n5\n10\n15\n20\n25\n30\nBlock 2\n0\n10\n20\n30\n40\n50\n60\nBlock 3\n5\n10\n15\n20\n25\nBlock 4\n5\n10\n15\n20\nBlock 5\n5\n10\n15\n20\nBlock 6\n10\n20\n30\n40\nBlock 7\n5\n10\n15\n20\n25\n30\nBlock 8\n5\n10\n15\n20\n25\n30\n35\nBlock 9\n10\n20\n30\n40\nBlock 10\n5\n10\n15\n20\n25\n30\n35\nBlock 11\n10\n20\n30\n40\nBlock 12\n10\n20\n30\n40\nBlock 13\n10\n20\n30\n40\n50\nBlock 14\n0\n10\n20\n30\n40\n50\nBlock 15\n10\n20\n30\n40\n50\nBlock 16\n0\n20\n40\n60\nBlock 17\n10\n20\n30\n40\n50\n60\nBlock 18\n20\n40\n60\nBlock 19\n20\n40\n60\n80\nBlock 20\n20\n40\n60\n80\npdata\npnoise\nBlock 21\n20\n40\n60\n80\npdata\npnoise\nBlock 22\n20\n40\n60\n80\n100\npdata\npnoise\nBlock 23\n20\n40\n60\n80\n100\n120\npdata\npnoise\nBlock 24\n0\n50\n100\n150\n200\npdata\npnoise\nBlock 25\n0\n250\n500\n750\n1000\n1250\npdata\npnoise\nBlock 26\n0\n250\n500\n750\n1000\n1250\npdata\npnoise\nBlock 27\n0\n1000\n2000\n3000\n4000\n5000\nAttetion layer\nMLP Layer\nFigure 18. We plot average distance (L2-norm) between activation of different layers during successive steps of the denoising process of\nthe DiT-XL/2 model. Different layers exhibit different characteristics. Similar observations have been made in [70].\n50 %\n75 %\n100 %\nFLOPs % compared to baseline\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFID-50k \n3.49\n3.99\n4.29\n4.36\n4.32\n4.32\nWeak model for the first denoising steps\nWeak model for the last denoising steps\nWeak model for the first denoising steps\nWeak model for the last denoising steps\n2.25\n2.25\n2.24\n2.25\n2.45\n2.64\n3.24\nFigure 19. We compare our scheduler versus a different scheduler that uses the weak model for the last denoising steps when generating\nclass-conditioned images. Points correspond to the minimum — concerning CFG scale — FID values.\nuse the update rule as presented in the main text\n(\nϵθ(xt−1|xt, ∅; puncond) + scfg1(ϵθ(xt−1|xt, c; pcond) −ϵθ(xt−1|xt, ∅; puncond)),\nif pcond = puncond\nϵθ(xt−1|xt, c; puncond) + scfg2(ϵθ(xt−1|xt, c; pcond) −ϵθ(xt−1|xt, c; puncond)),\nif pcond < puncond\n.\nThis guidance scheme seeks to reduce errors made by the powerful model, enhancing potential differences in predictions\nof the corresponding weak model, when the two models disagree, indicating the general direction towards higher-quality\nsamples. In practice, different values of scfg1 and scfg2 lead to the best results. We find that the rule (1−scfg1)/(1−scfg2) = 2.5,\n\n\n84.9% FLOPs: Weak model\nfor the first denoising steps\n84.9% FLOPs: Weak model\nfor the  last denoising steps\n69.7% FLOPs: Weak model\nfor the first denoising steps\n69.7% FLOPs: Weak model\nfor the last denoising steps\n54.6% FLOPs: Weak model\nfor the first denoising steps\n54.6% FLOPs: Weak model\nfor the last denoising steps\nFigure 20. We compare our scheduler versus a different scheduler that uses the weak model for the last denoising steps when generating\nclass-conditioned images. Using the weak model for the last denoising steps leads to images with lower image fidelity.\ndenoising steps\nPatch size for conditional\nPatch size for unconditional\n4\n2\nworks consistently across experiments. Although we fix the value of the CFG scale during inference, different combinations\nare likely to lead to higher quality images as demonstrated by previous work [14], which we leave for future exploration.\nWe point out that our scheduler is very stable in terms of performance attained for similar compute. For instance, per-\nforming inference with a 70/70 scheduler or a 90/50 scheduler, which both require the same overall compute, produces FID\nresults of 2.64 and 2.65 respectively. Finally, we present detailed experiments on the effect of the CFG scale for different\nlevels of compute and more metrics in Fig. 21.\n\n\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nCFG scale\n2\n4\n6\n8\n10\nFID-50k \n2.25 2.25\n2.64\n3.24\nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nCFG scale\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\nsFID-50k \nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nCFG scale\n100\n150\n200\n250\n300\n350\n400\nIS-50k \nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nCFG scale\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nPrecision-50k \nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nCFG scale\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nRecall-50k \nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\nFLOPs % compared\n       to baseline\n100 %\n 64 %\n 46 %\n 29 %\nFigure 21. Effect of CFG scale on the generated images from our class-conditioned FlexiDiT model. We plot (a) FID, (b) sFID, (c)\ninception score, (d) precision, and (e) recall when generation 50, 000 samples with 250 steps of the DDPM scheduler.\nB.5. Text-to-Image Experiments\nGenerally, T2I generation is performed for a fixed target CFG scale. For our experiments we choose scfg = 4.5 for the T2I\nTransf. model, as this is the value used in [15] and scfg = 6.0 for the Emu model, as for these values we observed the best\nquality images. In general, we can match with our dynamic inference other target values of the CFG scale. One simply needs\nto adjust the used CFG scale for the dynamic inference accordingly.\nWe follow the evaluation protocol of PIXART-α [15] and perform inference using the same solvers as they do, namely\niDDPM [24] for 100 steps, DPM solver [68] for 20 steps, and SA solver [106] for 25 steps. In the main text — Fig. 7 (left)\n— we presented results for the iDDPM solver. We present results for all the schedulers with the settings used in PIXART-α\nin Fig. 22, 23 and 24. For all the schedulers, there are settings where we reach the Pareto front of FID vs CLIP score of the\nbaseline model with a lot less required compute.\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n15\n20\n25\n30\n35\nMSCOCO FID - 10k\n   Baseline\n92.9 % FLOPs\n85.8 % FLOPs\n78.8 % FLOPs\n71.7 % FLOPs\n64.6 % FLOPs\n57.5 % FLOPs\nFigure 22.\nFID vs CLIP score us-\ning iDDPM for 100 steps for the T2I\nTransf. model.\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n15\n20\n25\n30\n35\nMSCOCO FID - 10k\n   Baseline\n93.3 % FLOPs\n86.5 % FLOPs\n79.8 % FLOPs\n73.0 % FLOPs\n66.3 % FLOPs\nFigure 23.\nFID vs CLIP score using\nthe DPM-solver for 20 steps for the T2I\nTransf. model.\n21\n22\n23\n24\n25\nCLIP score (ViT/L-14)\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\nMSCOCO FID - 10k\n   Baseline\n91.5 % FLOPs\n83.0 % FLOPs\n74.5 % FLOPs\n66.0 % FLOPs\n57.5 % FLOPs\n49.1 % FLOPs\n40.6 % FLOPs\nFigure 24.\nFID vs CLIP score using\nthe SA-solver for 25 steps for the T2I\nTransf. model.\nTo better characterize the effect of reducing compute, i.e. heavier use of the weak model, we also present more detailed\nresults for the DDPM scheduler in Fig. 25. Less compute-heavy inference schedulers, often produce images with smaller\npossible maximum CLIP scores (for large CFG guidance scales scfg). In practice, as large CFG scale values lead to larger\nvalues of FID, these are less preferred. In every case, our weak models can max the FID vs CLIP score tradeoff of the base\nmodel for the default configuration used, i.e. scfg = 4.5.\n\n\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n15\n20\n25\n30\n35\nMSCOCO FID - 10k\n   Baseline\n92.9 % FLOPs\n85.8 % FLOPs\n78.8 % FLOPs\n71.7 % FLOPs\n64.6 % FLOPs\n57.5 % FLOPs\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\nFigure 25. FID vs CLIP score using DDPM for 100 steps for the T2I Transf. model, for different levels of compute. On the right, we\npresent results separately for each compute level.\nWe also provide results on using a smaller overall number of steps with the DDPM solver in Fig. 27.\nTo\ngenerate\nthe\nbaseline\ncurves,\nwe\nsample\n10, 000\nsamples\nusing\na\nCFG\nscale\nscfg\nfrom\nthe\nset\n{1.0, 1.125, 1.25, 1.375, 1.5, 1.625, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5}.\nThe\nsame values are used when sampling with our flexible models. Finally, we also provide FID vs CLIP for the Emu model in\nFig. 26. In this case, we take CFG scales scfg from the set {1.0, 1.5, 2.0, 2.5, 3.0, 4.5, 6.0, 7.5, 8.0, 9.0}. We use captions\nfrom the training set of MS COCO to generate images. Neither of the models was trained on images from this dataset.\n24.25\n24.50\n24.75\n25.00\n25.25\n25.50\n25.75\n26.00\nCLIP ViT-L/14\n20\n22\n24\n26\nMSCOCO FID - 10k\n   Baseline\n84.3 % FLOPs\n68.6 % FLOPs\n52.9 % FLOPs\n37.2 % FLOPs\n21.5 % FLOPs\nBaseline 6.0 CFG\nFigure 26. FID vs CLIP score using DDIM for 50 steps for the Emu model.\nVQA results.\nVQA scores are calculated by querying a visual-question-answering model to produce an alignment score by\ncomputing the probability of a ”Yes” answer to a simple ”Does this figure show {text}?”. question. To calculate this score,\nwe use the clip-flant5-xxl model from huggingface7 as suggested in [60]. We provide more detailed results on the VQA\nbenchmark in Tables 1 and 2. More specifically, we provide per dataset VQA scores, along with the CFG scale scfg used to\ngenerate the images for each case . As we can see, using the weak model requires a bigger CFG scale to reach the same level\nof optimality (calculated from the FID vs CLIP score tradeoff). We also note that using the weak model often leads to images\nwith better text alignment. We hypothesize that fewer tokens (as a result of larger patch sizes) help with spatial consistency\nat the beginning of the denoising process. To calculate VQA scores, we take the first 200 prompts from each dataset and\n7https://huggingface.co/zhiqiulin/clip-flant5-xxl\n\n\n21\n22\n23\n24\n25\n26\nCLIP score (ViT/L-14)\n15\n20\n25\n30\n35\nMSCOCO FID - 10k (100 denoising steps)\n   Baseline\nBaseline 4.5 CFG\n92.9 % FLOPs\n85.8 % FLOPs\n78.8 % FLOPs\n71.7 % FLOPs\n64.6 % FLOPs\n57.5 % FLOPs\n14\n15\n16\n30 steps\n14\n15\n16\n50 steps\n25.3\n25.4\n25.5\n25.6\n25.7\n25.8\n25.9\nCLIP score (ViT/L-14)\n14\n15\n16\n100 steps\nFigure 27. We plot FID vs CLIP score when generating images with different CFG scales. (left) Overall Pareto front when generating\nimages with 100 denoising steps. (right) Pareto front generating images with a different number of steps zoomed in the typical tradeoff\ngeneration values.\nuse the train split from the DrawBench [84], train split from the Pick-a-Pic [55], test split from the Winoground [94] and\ntifa v1.0 text inputs8 from the TIFA160 [43] dataset.\nCFG scale\nscfg\nDrawBench\nPick-a-Pic\nWinoground\nTIFA160\nAverage\nT2I Transf. (100 %)\n4.5\n58.93\n50.56\n62.01\n81.65\n63.29\nT2I Transf. (92.9 %)\n4.5\n58.37\n51.53\n62.09\n82.16\n63.54\nT2I Transf. (85.8 %)\n4.5\n57.58\n51.67\n62.41\n81.53\n63.30\nT2I Transf. (78.8 %)\n4.7\n58.62\n51.97\n62.04\n80.60\n63.31\nT2I Transf. (71.7 %)\n4.7\n58.44\n51.56\n63.03\n80.57\n63.40\nT2I Transf. (64.6 %)\n4.9\n60.16\n52.34\n62.98\n80.06\n63.89\nT2I Transf. (57.7 %)\n5.0\n59.04\n50.80\n63.27\n79.90\n63.26\nT2I Transf. (50.5 %)\n5.0\n56.77\n51.72\n61.87\n79.38\n62.44\nT2I Transf. (43.4 %)\n5.0\n56.87\n51.92\n61.30\n78.33\n62.11\nTable 1. Detailed VQA evaluations for the benchmarks tested with the T2I Transf. model. As in the class-conditioned experiments, using\nmore of the weak model during denoising requires a higher CFG scale scfg to reach optimum performance.\nCFG-scale scfg\nDrawBench\nPick-a-Pic\nWinoground\nTIFA160\nAverage\nEmu (100 %)\n6.0\n69.44\n58.70\n65.75\n86.77\n70.17\nEmu (84.3 %)\n6.0\n68.00\n58.93\n67.33\n86.51\n70.19\nEmu (68.6 %)\n6.25\n69.53\n60.62\n66.00\n85.33\n70.37\nEmu (52.9 %)\n6.5\n69.79\n58.14\n66.23\n86.20\n70.09\nTable 2. Detailed VQA evaluations for the benchmarks tested with the Emu model. As in the class-conditioned experiments, using more\nof the weak model during denoising requires a higher CFG scale scfg to reach optimum performance.\nAlignment between powerful and weak model.\nIt is common practice nowadays to train images (and especially videos)\nin different stages, where a large (potentially lower quality) dataset is used for the first stage, followed by a shorter fine-\ntuning stage, characterized by higher quality and aesthetically more pleasing images. Although we are directly distilling the\nweak model from the predictions of the powerful model, the data used throughout training are still important. In practice,\n8https://github.com/Yushi-Hu/tifa/blob/main/tifa_v1.0/tifa_v1.0_text_inputs.json\n\n\nour fine-tuning is sample efficient, and we find that even a few thousand images (< 5000) are enough to succeed. We thus\nsuggest fine-tuning on the last (potentially smaller) but higher-quality dataset. When generating images based on shorter\nprompts with Emu, we use a prompt re-writer, prompting a small LLM to expand on the information provided. We consider\nthis prompt re-writer as part of the model.\nC. Implementation Details\nWe provide additional details on the experiments in the main text.\nC.1. Figure Details\nPrompts used for Fig. 1.\nWe provide in Table 3 the exact prompts used to generate the images.\nPrompts for Fig. 1\nThe image shows a frog wearing a golden crown with intricate designs, sitting on a wooden log in a serene\nenvironment reminiscent of a Japanese anime setting. The frog’s crown is adorned with small gems and its eyes\nare large and expressive. The log is covered in moss and surrounded by lush greenery, with a few cherry blossoms\nvisible in the background. The frog’s skin is a vibrant shade of green with blue stripes, and it has a regal demeanor,\nas if it is a monarch of the forest. The overall atmosphere is peaceful and whimsical.\nThe image shows a serene waterfall cascading down a rocky slope in a lush tropical forest, reminiscent of Claude\nMonet’s impressionist style. Sunlight filters through the dense foliage above, casting dappled shadows on the\nmisty veil surrounding the falls. The water plunges into a crystal-clear pool, surrounded by large rocks and vibrant\ngreenery. The atmosphere is tranquil, with a warm color palette and soft brushstrokes evoking a sense of serenity.\nThe forest floor is covered in a thick layer of leaves, and the sound of the waterfall echoes through the air.\nTable 3. Details on the prompts used to generate the images in the paper.\nDetails on Fig. 2.\nIn Fig. 2 (b), we fix randomness of the denoising process in terms of the initial sampled image p(xT ), and\nfrom the denoising process in Eq. (2). Then we generate images with 250 steps using the DiT-XL/2 official public checkpoint.\nDuring denoising, we modify only 1 of the 250 denoising steps, bypassing the model predictions from that step through a\nhigh/low pass filter. We then compare the resulting generated images in terms of LPIPS, L2 distance, SSIM, and DreamSim.\nIn general, modifying one of the first denoising steps, leads to larger final image differences, due to the accumulation of\ndifferences. We still note a distinctive pattern: a high-pass filter, i.e. removing low-pass components, leads to larger image\ndifferences during the first denoising steps. The opposite holds for the last denoising steps. We can thus argue, that low-pass\ncomponents, i.e. ‘coarser’ image details are more important compared to high-frequency details, for the first denoising steps.\nTo calculate spatial frequencies, we keep the corresponding values of the FFT of an image.\nDetails on Fig. 9.\nIn Fig. 9, we plot GPU utilization when propagating different sequence lengths. All experiments are\nconducted in bfloat16 using PyTorch 2.5 and CUDA 12.4 and with our Emu DiT that has 24 layers and a hidden dimension\nof 2048. We also plot peak FLOPs and memory bandwidth for the GPU tested, an NVIDIA H100 SXM5 in this case. When\nwe report FLOPs, we count additions and multiplications separately, as it is commonly done [41]. We count FLOPs as\nthe theoretical required operations for a forward pass, and not the actual GPU operations, which might be a higher number\ndue to the potential recalculation of partial results. As bytes for the x-axis, we only consider the model parameters (2\nbytes per model parameter). Note that for our choice of weak models, the GPU is fully utilized (it reaches the maximum\ncompute intensity that can be achieved for this application). In reality, compute intensity drops when larger sequence lengths\nare used, mainly due to the larger memory footprint of intermediate activations. Thus, latency benefits are indeed even\nlarger than FLOPs benefits reported in the paper. Compiling and fusing operations is crucial to ensure that the GPU is not\nbottlenecked by waiting instructions from the CPU. When we are performing inference with the weak model without first\nmerging the LoRAs, inference time is proportional to the additional FLOPs required. For the attention operation, we use the\nmemory efficient attention operation from the xformers library. Other efficient implementations of attention do not lead to\nsignificant differences. Our T2I model operates in a 128 × 128 latent space, which means that using a patch size of (1, 2, 2)\nresults in 4096 tokens, compared to 1024 tokens for the (1, 4, 4) patch size. Our T2V model operates in a 32 × 88 × 48 latent\nspace, which means that using a patch size of (1, 2, 2), (2, 2, 2), (1, 4, 4) leads to 33792, 16896 and 8448 tokens respectively.\nWe obtain similar results using different-sized models, like our Video DiT model.\n\n\nWhen we report FLOPs in the paper, we report numbers for the denoising process, as it is commonly done. We thus ignore\nlatency induced by decoders that map samples from a latent space, or potential prompt-rewrite modules. The added latency\nby the decoder, is in our settings negligible.\nC.2. Flexifying Diffusion Transformers\nAlthough for the class-conditioned experiments, we use a single embedding and de-embedding\nlayer that we always project to the required patch size, we note that this projection can be done\nonce to pre-calculate embedding and de-embeddding parameters during inference. These projected\nembeddings can then be used out of the box, for the tradeoff of some minuscule additional memory.\nThe choice of p′ as the underlying patch size is not too important for our experiments. In practice, we\nuse a value of p′ = 4. As mentioned in the main text, we add positional embeddings according to the\ncoordinates of each patch in the original image. A schematic of this can be found on the right.\nApart from the architecture modifications listed in the main text, we experimented with adding\npatch size specific temperatures in the self-attention computation:\nsoftmax\n \nQK⊤\nτp\n√\nd\n!\nV,\nwhere Q, K, V are the queries, keys and values respectively and τp is a patch size specific temperature initialized as 1. We do\nnot include this in the end, as it occasionally leads to instabilities during fine-tuning, even under different parametrizations.\nClass-conditioned implementation details.\nWe largely use the same hyperparameters as [78] to fine-tune. When fine-\ntuning to match distributions, we train to minimize the MMD loss, as introduced in Section B.1. During bootstrapping, we\ndenoise images with a DDPM scheduler, operating on T = 250 steps, the same as the target inference scheduler. As we\nfound that MMD distance is higher for diffusion steps closer to x0 ∼q(x0) — see also Fig. 11 —, we bias the sampling to\nreflect that during training as well.\nParameter\nValue\ntraining data\nImageNet\nlearning rate\n10−4\nweight decay\n0.0\nEMA update frequency\nevery step\nEMA update rate\n0.9999\nTable 4. Class-conditioned implementation details.\nIn our experiments, we focused on fine-tuning pre-trained models, as we were interested in efficiency. We note that\ntraining with different patch sizes has been used in the past to also accelerate pre-training [3, 9]. We believe that flexible\npatch sizes can also be used in this application to accelerate pre-training.\nT2I Transf. implementation details.\nFor the T2I Transf. model, we follow exactly the recipe of [15], and fine-tune a\n256 × 256 pre-trained variant9. For fine-tuning, we use the same image dataset, namely the SAM dataset10 with captions\ngenerated from a vision-language model. The model has overall the same parameters as the DiT-XL/2 model, with the\naddition of cross-attention blocks. When adding new embedding and de-embedding layers, we initialize them as we did for\nthe class-conditioned experiments. Embedding layers are initialized to Qembed†wembed and de-embedding layers are initialized\nto wde-embedQde-embed†. Here wembed, wde-embed are the pre-trained model parameters and Qembed, Qde-embed are the same —\npatch size dependent — fixed projection matrices that better preserve the norm of the output activations at initialization. As\naforementioned, we add a patch size embedding that is added to all tokens in the sequences after the tokenization step. This\nembedding is equal to 0 for the pre-trained patch size, to ensure functional preservation. We fine-tune the T2I Transf. model\non a small subset of the SAM dataset used to originally train the target model. We add LoRAs on the self-attention and\nfeed-forward layers, with a LoRA dimension of 32. We use a higher learning rate, due to the different learning objectives —\ndistilling a powerful model’s predictions into the ones of a weal model.\n9Our starting pre-trained model exactly matches the public checkpoint https://huggingface.co/PixArt-alpha/PixArt-XL-2-SAM-\n256x256.\n10https://segment-anything.com/.\n\n\nParameter\nValue\ntraining data\nSAM with captions from a VLM model\noptimizer\nAdamW\nlearning rate\n8 × 10−4\nweight decay\n10−2\ngradient clipping\n0.02\nbatch size\n512\nEMA update frequency\nevery step\nEMA update rate\n0.9999\nLoRA rank\n32\nTable 5. Image text-conditioned implementation details.\nEmu implementation details.\nOur Emu model is fundamentally identical to the T2I Transf. model. Small variations are\ndue to different ways to calculate text embeddings, which lead to a different number and size of the cross-attention tokens,\nand slight architectural modifications — primarily the use of QK-normalization [22] and the use of learnable positional\nembeddings. We train using a high-quality aesthetic dataset. To calculate metrics based on the 1024×1024 images generated\nwith this model, we follow the evaluation protocol of [105] and resize images to 512 × 512. For both our Emu and our Video\nDiT model, we use a LoRA rank of 64.\nT2V implementation details.\nAs aforementioned, our Video DiT model has a pre-trained patch size of (pf, ph, pw) =\n(1, 2, 2). Compared to our T2I experiments, we only change the 2D convolutional layers used for tokenization with a 3D\nconvolution layer. When increasing the temporal patch size pf, we duplicate parameters along that dimension. When inter-\npolating positional embeddings, we also do that along the temporal dimension. No additional changes are made. For eval-\nuation, we use the prompts from https://github.com/facebookresearch/MovieGenBench/blob/main/\nbenchmark/MovieGenVideoBench.txt to generate videos of length equal to 256 frames. We evaluate according\nto VBench [45] and report the average over Subject Consistency, Background Consistency, Temporal Flickering, Motion\nSmoothness, Dynamic Degree, Aesthetic Quality, Imaging Quality, Temporal Style and Overall Consistency.\nC.3. Human Evaluation Details\nWe prompt humans, asking them to: “Compare the two side-by-side images. Focus on visual appeal and flawlessness,\nconsidering factors like aesthetic appeal, clarity, composition, color harmony, lighting, and overall quality, then select ’left’\nif the left image is better, ’right’ if the right image is better, or ’tie’ if both are equally appealing or you have no preference.”.\nIn total, we collected votes for the 4 different settings presented in the paper and aggregated them across 200 prompts. For\neach setting and each prompt, we ask 3 people for votes. In cases where there are 3 votes for each of ’left’, ’right’, and ’tie’,\nwe ask a fourth labeler to break the tie.\nD. Generated Samples\nWe provide more examples of generated samples.\nD.1. Text-Conditioned Image Generation\nWe showcase more examples with varying amounts of compute in Fig. 28, 29 and 30. We further show more examples of\nhow performance and diversity in image generation are preserved in Fig. 31 and 32. Finally, we show examples of the effect\nof CFG scale scfg and reducing the overall number of FLOPs used to generate an image using our method, in Fig. 33. For all\nthe images seen in the paper with our Emu model, we use 50 steps of the DDIM scheduler.\nD.2. Text-Conditioned Video Generation\nWe showcase more examples of video generation with our flexible model in Fig. 34 and 35.\nD.3. Class-Conditioned ImageNet Generation\nWe provide a more comprehensive comparison for generated images from the same original sample from pnoise, using\nvarying amounts of compute from our flexible model in Fig. 36. Note that for class-conditioned generation we do not use\n\n\n100.0 % FLOPs\n84.3 % FLOPs\n68.6 % FLOPs\n52.9 % FLOPs\n37.2 % FLOPs\nThe image portrays a finely detailed portrait of a cat office worker, donning a sleek silk office suit and glasses, exuding an air of professionalism. The suit is rendered in intricate detail, with every fiber and fold meticulously captured. The cinematic lighting casts a dramati\nc glow, accentuating the cat's features and the textures of the suit. The dark shot is balanced by neutral colors, with hints of muted hues, evoking a sense of sophistication. The image boasts an epic, realistic quality, with intricate details that rival those of a high-end photo\ngraphy studio, reminiscent of the work of renowned photographers like Oliver Wetter and Annie Leibovitz.\nThe image is a stunning underwater macro shot of a glowing jellyfish, perfectly centered and symmetrical. The jellyfish's translucent body glows with a soft, ethereal light, surrounded by vibrant corals and bulbs that add pops of color to the neutral-toned background. The image i\ns hyperdetailed and cinematic, with a complex and extremely detailed background that showcases the beauty of the ocean's depths. The bokeh effect adds a sense of depth and dimensionality to the image, drawing the viewer's eye to the jellyfish's glowing form.\nThe image depicts a stunning flower in a serene and lush botanical garden, captured with a DSLR camera. The flower is a vibrant shade of pink, with delicate petals and a prominent center. It is situated in the foreground, surrounded by an assortment of foliage and stems of varyin\ng textures and colors. The garden's tranquil atmosphere is accentuated by the soft, diffused light and the subtle mist rising from the ground. The DSLR camera's high-quality sensor has captured the intricate details of the flower and its surroundings, showcasing the beauty of nat\nure.\nThe image presents a breathtakingly realistic portrait of an orange cat with vibrant, bright eyes and majestic angel wings. The cat's fur is intricately detailed, with subtle texture and shading that gives the impression of softness. The wings are radiant and ethereal, with delic\nate feathers and a subtle glow. The photography is cinematic, with a shallow depth of field created by the 50mm lens, resulting in a beautiful bokeh effect in the background. The lighting is soft and warm, illuminating the cat's features with a gentle, heavenly glow.\nThe image shows a determined 35-year-old space colonist, wearing a high-tech space suit with a gleaming metallic finish, standing proudly on the Martian surface. The suit is adorned with various tools and gadgets, and the colonist holds a Martian soil sample in their gloved hand.\n The historic portrait, captured by a space exploration photographer in 2055, is framed through the visor of the space helmet, providing an intimate and immersive view of the Martian landscape. The vast, crimson terrain stretches out behind the colonist, punctuated by rocky forma\ntions and the distant horizon.\nFigure 28. More samples generated by our Emu model for varying amounts of compute.\nLoRAs, and images generated from the original baseline model may not be exactly the same. They do however have the\nsame characteristics (FID score) as seen in Sec 4.1. We also show samples of our flexible DiT-XL/2 model when using only\n64% of the compute of the original model in Fig. 37, 38, 39, 40, 41, 42 and 43. All images are generated using 250 DDPM\nsteps and a CFG-scale equal to 4.0.\n\n\n100.0 % FLOPs\n84.3 % FLOPs\n68.6 % FLOPs\n52.9 % FLOPs\n37.2 % FLOPs\nThe image portrays a delectable arrangement of sushi and sashimi on a sleek, minimalist plate at a 3 Stars MICHELIN restaurant. The dish is artfully presented with honey and soy sauce, radiating an alluring aroma and appearance. Soft, dramatic lighting illuminates the culinary ma\nsterpiece, accentuating its indulgent appeal. The centered composition and clean layout evoke a sense of luxury and sophistication, highlighting the expertise of the food stylist. In the evening setting, the high-end dish exudes irresistible beauty, making it a true gastronomic d\nelight.\nThe image transports viewers to a mystical realm, reminiscent of Middle Earth's hidden hobbit towns. Amidst lush hills and gardens, cascading houses with steampunk flair seem to defy gravity. The atmosphere is both ancient and mysterious, with a soft, limpid color palette that ev\nokes a sense of wonder. Every detail, from the intricate architecture to the lush foliage, is rendered in hyper-realistic clarity, as if plucked from the imagination of Howard Behrens or djamilaknopf. The cinematic quality is further enhanced by the subtle glow of luminescent ref\nlections and the subtle sheen of crystalline structures.\nThe image shows a medium shot of a snow leopard in a snowy rocky mountain forest on a cloudy day. The forest is entirely covered in snow, and the snow leopard is in full-body view, showcasing its sleek white fur and black spots. The snow leopard's eyes are highly detailed and int\nensely focused on its prey, with a predatory gaze. The image is captured in crisp focus and natural lighting, highlighting the snow leopard's majestic appearance. The surrounding environment is also highly detailed, with visible trees, rocks, and snow-covered mountains in the bac\nkground.\nThe image depicts a luxurious sakura-themed ring in a closeup product view, showcasing exquisite hyper details in 4K resolution. The ring features a combination of gemstones and diamonds, crafted with precision and elegance. The soft illumination highlights the intricate design, \ncasting a dreamy glow. Rendered using Unreal Engine, the digital art exudes high-end fashion and luxury, evoking a sense of opulence. The overall effect is one of ultra-quality, making it a showstopper on platforms like ArtStation and CGSociety. The ring's beauty and craftsmanshi\np are truly a work of art.\nThe image depicts a serene and adorable kitten, no more than a few weeks old, nestled in a cozy ball amidst a peaceful nocturnal setting. The kitten's fur is a soft gray, with distinctive white patches on its nose and paws, and its eyes are closed in tranquil slumber. The moon ca\nsts a soft, ethereal glow on the kitten's fur, illuminating the fine details of its whiskers and the gentle rise and fall of its chest as it breathes. The surrounding environment is dark, with only the faintest hint of stars and a crescent moon visible in the sky.\nThe image is a highly detailed portrait of an anthro frog mage, a human-like frog holding a wand, with intricate details and radiant light. The frog stands in a lush environment with dense foliage and twisted vines, its green skin glistening in the soft light. Its eyes are bright\n and wise, and its long fingers grasp the wand with elegance. The scene is reminiscent of fantasy art by renowned artists such as Greg Rutkowski, Loish, and Lois van Baarle, with global illumination casting a warm glow on the entire setting. The atmosphere is mystical and enchant\ning, inviting the viewer to step into the magical world.\nFigure 29. More samples generated by our Emu model for varying amounts of compute.\n\n\n100.0 % FLOPs\n84.3 % FLOPs\n68.6 % FLOPs\n52.9 % FLOPs\n37.2 % FLOPs\nThe image depicts a futuristic cityscape inspired by the mandelbulb, a three-dimensional representation of the Mandelbrot set. The city's skyline is dominated by towering structures with intricate, swirling patterns reminiscent of the mandelbulb's fractal design. The buildings se\nem to stretch on forever, with smaller, ornate details visible on their surfaces. The city's streets are bustling with activity, as strange, otherworldly vehicles move through the city, leaving trails of light behind them. In the distance, a massive, glowing mandelbulb serves as \na beacon, illuminating the city's vibrant, dreamlike atmosphere.\nThe image depicts an ancient village at dusk, with traditional Chinese houses made of wood and stone standing along the misty streets. The walls and streets are covered in moss, adding a touch of mystique to the scene. Jacaranda trees with vibrant purple flowers tower above the r\noofs, while a majestic, mystical mountain looms in the background. Three dogs lie sleeping on the cold, foggy street, seemingly undisturbed by the eerie silence. The atmosphere is ultra-realistic and cinematic, with every detail meticulously rendered to transport the viewer to a \nbygone era.\nThe image shows a flock of sheep gathered together, taking a selfie with a smartphone held by one of the sheep. They are standing on a lush grassland, surrounded by the majestic Himalayan mountains in the background. The landscape is depicted in extraordinary detail, with snow-ca\npped peaks, rolling hills, and valleys. The sheep are all looking towards the camera, with some of them smiling and others looking curiously at the phone. The grassland is teeming with life, with wildflowers and other vegetation sprouting from the ground. The atmosphere is serene\n and peaceful, with the sun shining down on the scene.\nThe image depicts the White Castle of the King, a majestic and serene structure surrounded by lush greenery and vibrant flowers, including a few roses. The sky above is a deep shade of pink, reminiscent of Miyazaki's Nausicaa, with dramatic clouds that evoke a sense of epic fanta\nsy, similar to Breath of the Wild. In the garden, a powerful warrior, akin to a berserker from anime, stands tall, exuding a sense of strength and protection. The scene is set in an ultra-wide shot, with an atmospheric and cinematic quality, boasting hyper-realistic details and p\nost-processing that rivals the Unreal Engine, all in stunning 8k resolution and rendered with Octane.\nThe image depicts a chimpanzee wearing a sleek astronaut spacesuit, posing heroically in a grandiose setting reminiscent of classical oil paintings. The hyperrealistic and highly detailed 8k image is rendered in warm, cinematic lighting that accentuates the subject's textures and\n contours. The chimpanzee's fur and the spacesuit's metallic sheen are meticulously captured, with each strand of hair and every rivet on the suit's surface visible in extraordinary detail. The background is a muted, gradient blue, evoking the vastness of space. The overall atmos\nphere is one of awe-inspiring wonder and discovery.\nFigure 30. More samples generated by our Emu model for varying amounts of compute.\n\n\n100 % FLOPs (Baseline)\n52.9 % FLOPs\n100 % FLOPs (Baseline)\n52.9 % FLOPs\nFigure 31. Samples for the prompt: ”A playful kitten just waking up.”. We showcase the image generated by the baseline and our flexible\nscheduler using only 53% of FLOPs.\n\n\n100 % FLOPs (Baseline)\n52.9 % FLOPs\n100 % FLOPs (Baseline)\n52.9 % FLOPs\nFigure 32. Samples for the prompt: ”A baby hippo swimming in the river.”. We showcase the image generated by the baseline and our\nflexible scheduler using only 53% of FLOPs.\n\n\nDecreasing FLOPs\nIncreasing CFG scale\nFigure 33. Effect of CFG and total compute for the prompt: ‘The image shows a gigantic juicy burger placed on a white plate on a wooden\ntable. The burger is composed of a large beef patty, crispy bacon, melted cheese, lettuce, tomato, onion, pickles, and a slice of red tomato,\nall sandwiched between a soft bun. The burger is so large that it occupies most of the plate, with some toppings falling out of the sides.\nThe bun is slightly toasted, and the cheese is melted to perfection, giving off a savory aroma. The burger is garnished with a side of crispy\nfries and a refreshing glass of cola.‘.\n\n\nDrone view of waves crashing against the rugged cliffs along Big Sur s garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small\n island with a lighthouse sits in the distance, and green shrubbery covers the cliff s edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff s edges jutting out over the sea. This is a v\niew that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.\nA gorgeously rendered papercraft world of a coral reef, rife with colorful fish and sea creatures.\nA litter of golden retriever puppies playing in the snow. Their heads pop out of the snow, covered in.\nSeveral giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon lig\nht with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field.\nThis close-up shot of a Victoria crowned pigeon showcases its striking blue plumage and red chest. Its crest is made of delicate, lacy feathers, while its eye is a striking red color. The bird s head is tilted slightly t\no the side, giving the impression of it looking regal and majestic. The background is blurred, drawing attention to the bird s striking appearance.\nA tropical fish swimming in ocean reefs\nAerial shot of the ocean. a maelstrom forms in the water swirling around until it reveals the fiery depths below.\nFigure 34. More samples generated by our Video DiT model, using 25.2 % compute compared to the pre-trained baseline.\n\n\nDragon-toucan walking through the Serengeti.\nA curious cat peering out from a cozy hiding spot.\nbears figure out how to launch a rocket\nA building collapsing into a puddle of lava.\nA spaceship being pulled into a blackhole.\nAn orange cat jumps onto a kitchen counter after seeing butter there.\nA gibbon swinging through the canopy.\nFigure 35. More samples generated by our Video DiT model, using 25.2 % compute compared to the pre-trained baseline.\n\n\nDiT-XL/2 (256x256)\nOurs: 100% of Compute\nOurs: 85% of Compute\nOurs: 70% of Compute\nOurs: 55% of Compute\nOurs: 39% of Compute\nFigure 36. Sample for the ImageNet dataset comparing the baseline model and varying inference schedulers of our model, using different\nlevels of compute.\n\n\nFigure 37. Samples for the ImageNet class ‘tree frog, tree-frog’ from our FlexiDiT model that uses only 46% of the compute compared to\nthe baseline model.\nFigure 38. Samples for the ImageNet class ‘prairie chicken, prairie grouse, prairie fowl’ from our FlexiDiT model that uses only 46% of\nthe compute compared to the baseline model.\n\n\nFigure 39. Samples for the ImageNet class ‘hummingbird’ from our FlexiDiT model that uses only 46% of the compute compared to the\nbaseline model.\nFigure 40. Samples for the ImageNet class ‘cairn, cairn terrier’ from our FlexiDiT model that uses only 46% of the compute compared to\nthe baseline model.\n\n\nFigure 41. Samples for the ImageNet class ‘French bulldog’ from our FlexiDiT model that uses only 46% of the compute compared to the\nbaseline model.\nFigure 42. Samples for the ImageNet class ‘Granny Smith’ from our FlexiDiT model that uses only 46% of the compute compared to the\nbaseline model.\n\n\nFigure 43. Samples for the ImageNet class ‘cock’ from our FlexiDiT model that uses only 46% of the compute compared to the baseline\nmodel.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20126v1.pdf",
    "total_pages": 40,
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "authors": [
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Yeongmin Kim",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Artsiom Sanakoyeu",
      "Yuming Du",
      "Albert Pumarola",
      "Ali Thabet",
      "Edgar Schönfeld"
    ],
    "abstract": "Despite their remarkable performance, modern Diffusion Transformers are\nhindered by substantial resource requirements during inference, stemming from\nthe fixed and large amount of compute needed for each denoising step. In this\nwork, we revisit the conventional static paradigm that allocates a fixed\ncompute budget per denoising iteration and propose a dynamic strategy instead.\nOur simple and sample-efficient framework enables pre-trained DiT models to be\nconverted into \\emph{flexible} ones -- dubbed FlexiDiT -- allowing them to\nprocess inputs at varying compute budgets. We demonstrate how a single\n\\emph{flexible} model can generate images without any drop in quality, while\nreducing the required FLOPs by more than $40$\\% compared to their static\ncounterparts, for both class-conditioned and text-conditioned image generation.\nOur method is general and agnostic to input and conditioning modalities. We\nshow how our approach can be readily extended for video generation, where\nFlexiDiT models generate samples with up to $75$\\% less compute without\ncompromising performance.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}