{
  "id": "arxiv_2502.20167v1",
  "text": "Similarity-Distance-Magnitude Universal Verification\nAllen Schmaltz\nReexpress AI, Inc., USA\nallen@re.express\nAbstract\nWe solve the neural network robustness problem by adding SIMILARITY\n(i.e., correctly predicted depth-matches into training)-awareness and DIS-\nTANCE-to-training-distribution-awareness to the existing output MAGNITUDE\n(i.e., decision-boundary)-awareness of the softmax function.\nThe resulting\nSDM activation function provides strong signals of the relative epistemic (re-\nducible) predictive uncertainty. We use this novel behavior to further address\nthe complementary HCI problem of mapping the output to human-interpretable\nsummary statistics over relevant partitions of a held-out calibration set. Estimates\nof prediction-conditional uncertainty are obtained via a parsimonious learned\ntransform over the class-conditional empirical CDFs of the output of a final-layer\nSDM activation function. For decision-making and as an intrinsic model check,\nestimates of class-conditional accuracy are obtained by further partitioning the\nhigh-probability regions of this calibrated output into class-conditional, region-\nspecific CDFs. The uncertainty estimates from SDM calibration are remarkably\nrobust to test-time distribution shifts and out-of-distribution inputs; incorporate\nawareness of the effective sample size; provide estimates of uncertainty from the\nlearning and data splitting processes; and are well-suited for selective classifi-\ncation and conditional branching for additional test-time compute based on the\npredictive uncertainty, as for selective LLM generation, routing, and composition\nover multiple models and retrieval. Finally, we construct SDM networks, LLMs\nwith uncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results.1\n1\nIntroduction\nLarge language models (LLMs) pose a challenge for interpretable and reliable deployment given\nthe non-identifiability of their parameters [22, inter alia]2, which can number in the billions or\nmore. Instead of directly interpreting parameters, instance-based, metric-learner approximations\nand hard-attention mechanisms can be constructed with task-specific inductive biases for effective\nsemi-supervised learning (i.e., feature detection) and introspection against the training set [37], which\ncan be useful for auditing predictions as a form of interpretability by example, or exemplar, over\nthe representation space of the model. However, for real-world deployments, robust approaches for\npredictive uncertainty—and relatedly, for verifying the modeling process—are also needed, both for\nhuman decision-making and for constructing sequentially dependent LLM pipelines.\nKnown theoretical results limit the statistical quantities that can be derived over LLMs. Statistical\nassurances in the distribution-free setting are limited to approximately conditional quantities [44,\n27, 13, inter alia]. Further, even typical approximately conditional quantities can be difficult to\n1https://github.com/ReexpressAI/sdm\n2Informally, this means that two or more distinct sets of values for the parameters can result in identical\noutput distributions. As a consequence, interpreting the parameters of such models is typically much more\ncomplicated than with a simple linear regression model, for example.\narXiv:2502.20167v1  [cs.LG]  27 Feb 2025\n\n\nobtain in practice, since the minimal assumption of exchangeability with a known held-out data\nset is itself often violated with co-variate and label shifts, which can be difficult to foresee with\nexisting methods. Epistemologically, the prevalence of hallucinations and highly-confident wrong\nanswers with widely deployed LLMs suggests a technical impasse in effectively modeling the\npredictive uncertainty, despite significant work from Bayesian, Frequentist, and empirically motivated\nperspectives [14, 2, 16, 26, 32, inter alia]. A foundational piece is evidently missing from the picture.\nGiven these intrinsic challenges, we approach the problem of uncertainty quantification over LLMs\nfrom a new angle and ask: Can we leverage the metric learning and dense matching capabilities of\nneural networks over high-dimensional inputs to at least aim to maximize, with minimal distribu-\ntional assumptions, the separation of aleatoric (irreducible) uncertainty and epistemic (reducible)\nuncertainty, decomposing the sources of the latter in a manner that is interpretable and actionable?\nWe answer this question in the affirmative with a conceptually parsimonious, LLM-driven partitioning\nof the data to decompose sources of epistemic uncertainty: Correctly predicted depth-matches into\nthe training set (SIMILARITY), the DISTANCE to the training set, and the distance to the decision-\nboundary (MAGNITUDE). We use these signals to construct a new activation function, the SDM\nactivation, which replaces a foundational building block of contemporary AI, the softmax operation.\nA series of distributional transforms over an SDM activation then enable us to directly target a quantity\nof interest, index-conditional calibration, well-suited for selective classification [5, 15, inter alia],\nwhich reflects the typical need for uncertainty quantification with LLMs as part of multi-stage decision\npipelines. Finally, with this new foundational behavior, we construct a new LLM architecture, the\nSDM network, with an intrinsic—and externally human interpretable—capability to verify its own\ninstruction-following.\nIn summary, in this work:\n• We introduce the SDM activation function, which encodes strong signals of epistemic\nuncertainty, to replace the softmax operation.\n• We provide a robust estimator of index-conditional uncertainty (Def. 4.3) via a final-layer\nSDM activation over existing models.\n• We propose the SDM network, a new LLM architecture and fine-tuning approach for which\nuncertainty-awareness and interpretability-by-exemplar are instrinsic properties.\n• We empirically compare the uncertainty-awareness of our estimators to existing classes of\napproaches, which we demonstrate do not reliably achieve our desired uncertainty quantity\nin the presence of—even modest—distribution shifts. As a natural, held-out blind evaluation,\nwe also demonstrate efficiently uncovering undetected annotation errors in the carefully\ncurated MMLU-PRO benchmark dataset.\n2\nMotivation\nGiven the ability of LLM’s to recursively cross-encode data, user instructions, and outputs, if we\nhad a reliable means of assessing the uncertainty over an LLM’s predictions that was also human\ninterpretable (i.e., a quantifiable and verifiable assurance in their instruction-following abilities), such\nan LLM could serve as a universal verifier over existing models, which would in effect calibrate the\npredictive uncertainty of other models: Have, e.g., an exogenous regression or multi-label model?\nSimply cross-encode the data, exogenous model, and output as input to the LLM verifier and let\nthe neural network generate the accuracy as to whether the exogenous model is correct or not. This\nprocess could be repeated, as needed, using such an LLM as a basis for building complex, compound\nAI systems, recursively cross-encoding the input and output, using the uncertainty over discrete\npredictions as the branching condition for additional test-time compute, tool calling, and human\nfeedback — and ultimately, reliable AI-assisted decision-making. In this work, we introduce the\nmechanisms for constructing such a verifier, which we formalize below.\n2\n\n\n3\nPreliminaries\n3.1\nSetting\nBoth LLM next-token prediction and standard classification tasks (e.g., predicting the sentiment of a\nmovie review) are formulated similarly as predictions over discrete classes. We are given a training\ndataset, Dtr = {(xn, yn)}N\nn=1 of inputs, x ∈X, paired with their corresponding ground-truth\ndiscrete labels, y ∈Y = {1, . . . , C}, and a labeled calibration dataset, Dca, drawn from the same\ndistribution as Dtr. We are then given a new test instance, x, from an unlabeled test set, Dte, and seek\nto estimate the label with a prediction, ˆy, via the un-normalized log probabilities (“logits”, informally)\nof a final linear layer: z = W T h+b, where h = network(x; θ) is the final hidden state of a network\nparameterized by θ. The network can be recurrent [21], convolutional [8], or self-attention-based [10],\namong others. The discrete prediction is taken as ˆy = arg max z; however, for learning θ, W , and\nb, and for human decision-making, we also seek an estimate of the predictive uncertainty, p(y | x),\nwhich is typically obtained by normalizing z via the softmax operation described next. We will\nmake a distinction between models, M (defined by θ, W , and b, and when applicable, the exemplar\nadaptor, described below), which produce the prediction, ˆy, and estimators, E, which provide an\nestimate of p(y | x), because different estimators can be used over the same model.\n3.2\nSoftmax and the Cross-Entropy loss\nThe softmax has as its origins the work of L. Boltzmann in the 19th century [see 41, inter alia]. It\nremains a central function in the natural and engineering sciences. It is ubiquitous in deep learning,\nplaying an integral role as a router in self-attention mechanisms [45] and mixture-of-experts models\n[42]; forming the basis of the cross-entropy loss used for next-token training of LLMs; and serving\nas the final interface between a model and the end-user, converting the un-normalized model logits to\nhuman interpretable probability distributions, at least in principle:\nsoftmax(z)i =\neτ·zi\nPC\nc=1 eτ·zc , 1 ≤i ≤C, τ ≥0\n(1)\nThe above function induces a parameterization of the event probabilities of a categorical distribution:\nCategorical(C = |Y|, softmax(z))\n(2)\nThe inverse-temperature parameter, τ, controls the sharpness of the distribution. As τ →0, the\noutput of softmax(z) converges to a uniform distribution where each class has probability 1\nC ; as\nτ →∞, the output converges to a distribution in which all of the mass is assigned to a single class.\nIn deep learning, τ is treated as a learnable, global hyper-parameter; instance-wise variation in the\ndistance to the decision-boundary is thus determined by the relative MAGNITUDE of zˆy. This model\nis learned by minimizing the cross-entropy loss between z and the index of the true labels over Dtr.\nThe natural logarithm of the loss is the counterpart to the base e of the softmax:\nL(θ, W , b; Dtr) = −1\nN\nN\nX\nn\nloge\n \neτ·zyn\nPC\nc=1 eτ·zc\n!\n(3)\n4\nMethods\nIn this work, we revisit Eq. 1, 2, and 3 given new observations on the statistical behavior of high-\ndimensional objects, empirically derived from large parameter neural networks. We will seek to\ndecouple the sources of epistemic uncertainty via a new activation function that is conceptually:\nSDM(z)i =\nSIMILARITYDISTANCE · MAGNITUDEi\nPC\nc=1 SIMILARITYDISTANCE · MAGNITUDEc\n(4)\nwith a corresponding negative log likelihood loss that takes into account the change of base (§ 4.1).\nWe will additionally introduce a transformation that rescales this value for an instance with exogenous\ninformation across Dca, effectively calibrating [4, 9] the model to produce reliable, interpretable\nprobabilities (§ 4.2). Finally, we integrate this behavior into the LLM architecture and training,\nyielding an LLM with an intrinsic ability to verify its own instruction following (§ 4.3).\n3\n\n\n4.1\nFrom Model Approximations via Exemplar Adaptors to SDM Activation Functions\nExemplar adaptors, 1-D CNN adaptors (with a final linear layer) over the frozen hidden states of\na network, induce distilled, compressed representations of an underlying network’s representation\nspace conditional on its predictions. This behavior can be used to faithfully approximate a model’s\npredictions as a mapping against a training, or support, set. This can be achieved, for example,\nwith instance-based, metric-learning estimators, such as weighted KNNs, where the weights are\nlearned as a transform of the exemplar adaptor’s distilled representations.3 Critically, when the\napproximations diverge from the predictions of the underlying model, the inputs tend to be from the\nsubsets of the distribution over which the underlying model is itself unreliable [37]. In other words,\nthe approximations encode strong signals of the epistemic uncertainty, a point under-appreciated\nin the existing literature and which we bring to its logical conclusion in this work. Rather than\nconstructing explicit KNN approximations, which require a separate training step and additional\nparameters, we instead quantize the closeness of a point to the training set with a discrete estimate.\nFurther, we transform the distance to the closest match as a quantile estimate over the distribution\nof distances. These quantities, combined with the output MAGNITUDE, capture the key sources of\nepistemic uncertainty for an input instance (cf. § 4.2).\n4.1.1\nExemplar Adaptor\nWe take as the CNN of our exemplar adaptor g : (h, t(z)) ∈RD 7→h′ ∈RM, a 1-D CNN that takes\nas input h (if available) of the underlying network and optionally, the concatenation of the output of\nt(z), a transform of the underlying network’s output.4 The CNN has M filters, the filter applications\nof which produce h′, the distilled representation of the underlying network. A final linear layer,\nz′ = W ′T h′ + b′, z′ ∈RC, then replaces the underlying network’s linear layer, with the discrete\nprediction taken as ˆy = arg max z′. This exemplar adaptor will then enable us to derive the key\nsignals of epistemic uncertainty, SIMILARITY, DISTANCE, and MAGNITUDE described next.\n4.1.2\nSIMILARITY\nWe define the SIMILARITY (q) of an instance to the training set as the count of consecutive nearest\nmatches in Dtr that are correctly predicted and match ˆy of the new, unseen instance. Concretely, we\nfirst sort Dtr (for which we have both model predictions and ground-truth labels) based on the L2\ndistance (2-norm) from h′,\nh\n(xtr\n(1), ˆytr\n(1), ytr\n(1)), . . . , (xtr\n(N), ˆytr\n(N), ytr\n(N))\ni\n, such that ||h′ −h′tr\n(1)||2 ≤\n. . . ≤||h′ −h′tr\n(N)||2, and then calculate q ∈{0, . . . , | Dtr |} as:\nq =\n| Dtr |\nX\ni=1\n1ˆy=ˆytr\n(i) · 1ˆytr\n(i)=ytr\n(i) · 1i−1=Pi−1\nj=1 1ˆy=ˆytr\n(j)·1ˆytr\n(j)=ytr\n(j)\n(5)\nwhere the rightmost indicator function, 1 ∈{0, 1}, ensures consecutive (depth-wise) matches. By\ndefinition, q cannot exceed the count of the most prevalent class label in Dtr, and since we assume\na reasonable relative number of points for each class, q ≪| Dtr | is typical. For the special case of\ncalculating q for x ∈Dtr, which only occurs during learning, we exclude the self-match.\n4.1.3\nDISTANCE\nThe L2 distance to the nearest match in Dtr follows from above: dnearest = ||h′ −h′tr\n(1)||2. However,\nit is difficult to work with dnearest directly since its scale can vary widely depending on the input to\ng and the size of M. Instead, we define DISTANCE, d ∈[0, 1], in terms of the class-wise empirical\nCDFs of dnearest over Dca, as the most conservative quantile relative to the distance to the nearest\n3Such instance-based, metric-learner approximations of neural networks differ from traditional KNN rules\n[7, 11, inter alia] in two critical respects: The neural network serves as a semi-supervised learner of the distances\nbetween the dense representations that identify the instances, and there is a model prediction (in addition to the\nground-truth label) for each instance in the support set. The former enables effective partitioning despite the\ncurse of high dimensions; the latter provides an additional indicator of reliability for each instance.\n4For black-box LLM API’s in particular, we will not have direct access to h and will instead construct a\nproxy of h via a transform t of the available output, which may (and typically will with current models) itself be\nthe result of a softmax operation.\n4\n\n\nmatches observed in the labeled, held-out set:\nd = min [1 −eCDFy1\nca(dnearest), . . . , 1 −eCDFyC\nca (dnearest)]\n(6)\nThe empirical CDFs are determined by the labeled points in Dca for which q > 0, where, as indicated\nby the superscripts, the stratification of points is by the true labels, y. For example, eCDFy1\nca(dnearest)\nis the empirical CDF of dnearest values in Dca for which y = 1, a notation convention we will\nuse throughout. (Points with q = 0 are effectively out-of-distribution points and treated as such in\ndownstream decision-making, so they are excluded to avoid biasing the estimates.) At test-time, we\ndo not see y; instead, the minimum is calculated over the quantiles of each of the class-conditional\neCDFs, regardless of ˆy. As with q, for the special case of calculating d for x ∈Dtr, we replace\neCDFyc\nca with the analogous eCDFyc\ntr , the class-wise empirical CDFs of dnearest over Dtr excluding\nself-matches.\n4.1.4\nMAGNITUDE\nWe take as the MAGNITUDE, or distance to the decision boundary, z′\nˆy, as in the standard softmax\ncase but via z′ from the linear layer of the exemplar adaptor.\n4.1.5\nSDM Activation: Formulation\nWe use the above quantities to define the SDM activation function:\nSDM(z′)i =\n(2 + q)d ·z′\ni\nPC\nc=1 (2 + q)d ·z′c , 1 ≤i ≤C\n(7)\nThe output distribution becomes sharper with higher values of q, d, and z′. Also note that when\ndnearest exceeds the largest distance observed in the labeled data, d = 0 and the output distribution\nis uniform, reflecting a maximally high (i.e., out-of-distribution) epistemic uncertainty estimate.\nThe standard softmax with τ = 1 is recovered by setting q = e −2, d = 1. As with the softmax\noperation, arg max SDM(z′) = arg max z′.\n4.1.6\nSDM Activation: Loss and Training\nA loss analogous to Eq. 3 then follows with the applicable change of base. We use this loss to train\nthe weights of the exemplar adaptor, which includes the parameters of the linear layer (W ′ and b′),\nas well as the convolution weights and biases, which we collectively represent with G. The weights\nof the underlying network remain fixed. (We return to training θ, W , and b of an underlying LLM\nin § 4.3.)\nL(G, W ′, b′; Dtr) = −1\nN\nN\nX\nn\nlog(2+q)\n \n(2 + q)d ·z′\nyn\nPC\nc=1 (2 + q)d ·z′c\n!\n(8)\nPseudo-code for training the SDM activation layer and SDM estimator (described in § 4.2, next) appears\nin Alg. 1. The first epoch is initialized with a standard softmax (i.e., setting q = e −2, d = 1).\nTraining then proceeds by re-estimating q and d for each x ∈Dtr after each epoch. We take as\nthe stopping criteria for one learning round as the epoch with the highest average balanced (across\nclasses) q values over Dca. We choose the final model M∗∈M over J iterations of random shuffles\nand splits of Dtr and Dca and parameter initializations as that with the globally highest average\nbalanced (across classes) q values over Dca. For learning, we assume Dtr and Dca are balanced\nacross all class labels, c ∈Y.\n4.2\nFrom SDM Activation Functions to SDM Calibration\nGiven a fixed underlying network, the SDM activation function in Eq. 7 encodes strong signals of the\nepistemic uncertainty of a single instance for a single model M∗∈M, but a priori, it is not sufficient\nalone for calibration without additional exogenous information, since it does not explicitly take into\naccount the epistemic uncertainty from the splitting of Dtr and Dca; the stochasticity of parameter\ninitialization; and the stochasticity of the learning process, more generally. Relatedly, to enable the\ninterpretability of the calibration process (e.g., to perform model checks), we need a stable mapping\nof test points to the relevant partitions of Dca.\n5\n\n\nAlgorithm 1 SDM Activation Layer and SDM Estimator Training\nInput: Dtr, Dca, α′, network, max epochs, rescaler max epochs, rescaler stopping condition\n1: Assumption: Dtr, Dca are balanced across all class labels, c ∈Y\n2: procedure SDM-ITERATIVE-TRAIN(Dtr, Dca, α′, network, max epochs)\n3:\nM∗←∅\n▷Globally best model\n4:\nDtr∗←∅, Dca∗←∅\n▷Data splits of best model\n5:\nE ←∅\n▷SDM estimator (i.e., p(ˆy)lower)\n6:\nmetric∗←0\n▷Determines final best model\n7:\nstats ←{ }\n▷Summary statistics to calculate ˜q γ\nmin, mˆy\n⌊˜q⌋(§ 4.2.4)\n8:\nfor j ∈1, . . . , J do\n▷The learning process is repeated J times\n9:\nMj∗= ∅\n▷Best model for a single learning round\n10:\nmetricj ←0\n11:\nDtr, Dca ←Random shuffle and even split of Dtr and Dca\n12:\nMj ←Random initialization of Gj, W ′\nj, b′\nj\n13:\nq ←e −2, d ←1\n▷Standard softmax for first epoch\n14:\nfor e ∈1, . . . , max epochs do\n15:\nMinimize L(G, W ′, b′; Dtr)\n▷Eq. 8\n16:\nUpdate q, d for each x ∈Dtr\n17:\nmetric ←mean balanced (across c ∈Y) q over Dca\n18:\nif metric ≥metricj then\n19:\nmetricj ←metric\n20:\nMj∗←Mj\n21:\nif metricj ≥metric∗then\n22:\nmetric∗←metricj\n23:\nM∗←Mj∗\n24:\nDtr∗, Dca∗←Dtr, Dca\n▷Data splits for calculating q, d at test-time & model checks\n25:\nMj∗←update with W\n′′ from TRAIN-RESCALER(·)\n▷Alg. 2\n26:\nstats ←update with FIND-MIN-RESCALED-Q(·)\n▷Alg. 3\n27:\nE ←Constructed from globally best model M∗(and associated values, e.g., ˜qmin∗) and stats\n28:\nreturn M∗, Dtr∗, Dca∗, E\nOutput: M∗, Dtr∗, Dca∗, E\nIn service of achieving these additional properties, we first need to specify a definition of calibration,\nof which there are conflicting quantities, definitions, and evaluation metrics [43, 25, 17]. Fortunately,\nin real-world settings with LLMs, we are primarily concerned with reliably detecting high-probability\nregions, which significantly simplifies the evaluations and removes much of the ambiguity in the\ndefinitions. To motivate our definition, we first consider two under-specified definitions of calibration,\nin which the true long-run frequencies of the ground-truth labels match the probability estimates from\nthe estimator, E, stratified by the predicted class, ˆy, and the true class, y, respectively, given some\nun-specified binning of the real-valued probabilities:\nDefinition 4.1. An estimator, E, of p(y | x) is prediction-conditional calibrated, if ∀α′ ∈[0, 1]:\np(y = ˆy | ˆy, E(x) = α′) = α′.\nDefinition 4.2. An estimator, E, of p(y | x) is class-conditional calibrated, if ∀α′ ∈[0, 1]: p(y =\nˆy | y, E(x) = α′) = α′.\nAssuming no distribution shifts, and setting aside conditioning on additional attributes and the method\nof binning, the source of the under-specification, def. 4.2 is a generally more informative quantity, but\ncannot be meaningfully estimated across all points since the true label, y, is not available at test-time.\nThus, calibration becomes a tension between the quantities desired and the regions—and the size\n(sharpness) of those regions—that can be partitioned. Most works are premised on a variation of\nDef. 4.1; an alternative compromise is taken by frequentist conformal estimators by changing the\nquantity to coverage over a discrete prediction set. We will instead seek the following quantity, which\naligns with the quantity needed for selective classification for conditional branching of LLM compute\nand final human decision-making dependent on the presence of high-probability predictions:\nDefinition 4.3. An estimator, E, of p(y | x) is index-conditional calibrated at α′ ∈( 1\nC , 1] if: p(y =\nˆy | ˆy, E(x) ≥α′) ≥α′ ∧p(y = ˆy | y, E(x) ≥α′) ≥α′.\nTo evaluate this quantity, we only consider the points for which the estimator assigns a high-probability\nof at least α′, which is typically near 1, such as 1 −α = α′ = 0.95 in our experiments. We refer to\n6\n\n\nthis set of points as the admitted, or non-rejected, set. Then, given ground-truth values for Dte, we\nassess whether the conditional accuracies of the admitted set are at least α′ when stratifying by the\npredicted labels, ˆy, and the true labels, y. Unlike evaluating Def. 4.1, there is thus no ambiguity with\nregard to the choice of binning the probabilities.\nThe estimator that rejects all points is index-conditional calibrated. Given two estimators that are\nindex-conditional calibrated, we prefer that which rejects fewer points, ceteris paribus. In other words,\nwe seek estimators that meet our reliability condition and are informative (i.e., maximize the number\nof points that are properly admitted), but when the estimator is uncertain, we prefer rejection over\nunexpectedly falling under the desired α′ probability threshold.\nThe key compromise is that we will not be able to reliably calculate a probability for all points;\nhowever, for LLM tasks, there is typically not an actionable notion of partial acceptability for final\ndecision-making, so it is a reasonable compromise. Either the complex LLM output is verified as\ncorrect, or some separate, remedial action must be taken, such as dividing the task into simpler tasks,\nreformatting and re-cross-encoding, or seeking outside information retrieval, among others, where\nagain for each of these sub-tasks, we seek index-conditional calibrated estimators at the level of the\navailable labels, where the stopping condition is eventually deferment to human adjudication.\nDespite the aforementioned compromise, and although evaluation is unambiguous, it may still seem\nmysterious that the second condition of Def. 4.3 can be meaningfully estimated. To do so, we will\nneed to perform a series of transforms over the already strong uncertainty signals from the SDM\nactivation function and re-visit the behavior of partitioning empirical CDFs, to which we turn next.\n4.2.1\nRescaling SDM Activation Output to Account for Effective Sample Sizes\nA disadvantage of using SDM(z′) directly as an estimator is that it only has an indirect, relative\nnotion of the effective sample size of Dca. Intuitively, the confidence in a prediction should be\ncommensurate with the number of comparable points in Dtr and Dca, which the SDM activation\ncaptures via SIMILARITY, DISTANCE, and MAGNITUDE. For example, an out-of-distribution point\nwill tend to have d = 0 and low values of q, reflecting a small effective sample size in the observed\ndata. However, to further improve the robustness of the estimate, we can explicitly incorporate an\nadditional, direct notion of the effective sample size via distributional statistics over Dca.\nFirst, we calculate class-conditional empirical CDFs over Dca of the output of SDM(z′). For a given\npoint, this will create a vector, v ∈RC, of the quantiles:\nv = [eCDFy1\nca(SDM(z′)1), . . . , eCDFyC\nca (SDM(z′)C)]\n(9)\nNext, we rescale q to take into account these distributional statistics, as the resulting value will be the\nbasis for our stable mapping between new, unseen test points and Dca:\n˜q = loge ((2 + q)vˆy)\n(10)\nWe seek a normalized distribution both to present to users and to enable the subsequent transform\ndescribed in § 4.2.3. Toward this end, we rescale with a linear layer, without a bias, the training\nof which we detail in § 4.2.2: v′ = W\n′′T v, v′ ∈RC. This is normalized using 2 + ˜q as the base,\no ∈RC:\noi =\n(2 + ˜q) v′\ni\nPC\nc=1 (2 + ˜q) v′c , 1 ≤i ≤C\n(11)\nUnlike the output of an SDM activation, arg max o, is not necessarily (but typically will be) equivalent\nto ˆy = arg max z′. When they are not equivalent, our convention is to set ˜q = 0 for the point, which\nwill in effect treat the point as out-of-distribution in downstream analyses.\nEffective Sample Sizes via the DKW Inequality\nEq. 11 is premised on the assumption that the\nempirical CDFs in Eq. 9 reflect the true, underlying conditional distributions, which are unspecified.5\nThat would seem to be a relatively strong assumption as the final estimate, particularly for small sam-\nple sizes, even if empirically effective over existing datasets, and is the entry point for incorporating\nan explicit notion of the effective sample size in our estimates.\n5That is also true for the eCDFs in Eq 6, but we make the reasonable assumption that the higher-level\ntransforms starting with Eq. 9 effectively account for the uncertainty in the distance eCDFs.\n7\n\n\nWe make the following conservative assumption, parameterizing the prior belief that data points with\na looser connection to Dtr reflect smaller effective sample sizes, while also explicitly accounting for\nthe count of observed points in Dca:\nAssumption 4.4. We assume the effective sample size is increasing in ˜q, class-wise over Dca.\nFor each x ∈Dte, using ˜q, we calculate the vector of effective sample sizes across classes, ˆn, relative\nto Dca as:\nˆn = [| Dca |y1 · eCDFy1\nca(˜q), . . . , | Dca |yC · eCDFyC\nca (˜q)]\n(12)\nwhere | Dca |yc is the count of calibration set points with true label y = c.\nWith these sample size estimates, we can then construct a band around the empirical CDFs using the\nsharp constant [29] of the distribution-free DKW inequality [12], calculating the error for each class\nc ∈{1, . . . , C} from the corresponding index in ˆn if ˆnc > 0:\nϵc =\ns\n1\n2 · ˆnc\nloge\n\u0012\n2\n1 −α′\n\u0013\n(13)\nIf ˆnc = 0 our convention is to set ϵc = 1. We can then construct the lower and upper counterparts to\nthe quantile vector of Eq. 9:\nvlower = [ min (max (eCDFy1\nca(SDM(z′)1) −1ˆy=1 · ϵ1 + 1ˆy̸=1 · ϵ1, 0) , 1) , . . . ,\nmin (max (eCDFyC\nca (SDM(z′)C) −1ˆy=C · ϵC + 1ˆy̸=C · ϵC, 0) , 1)]\n(14)\nvupper = [ min (max (eCDFy1\nca(SDM(z′)1) + 1ˆy=1 · ϵ1 −1ˆy̸=1 · ϵ1, 0) , 1) , . . . ,\nmin (max (eCDFyC\nca (SDM(z′)C) + 1ˆy=C · ϵC −1ˆy̸=C · ϵC, 0) , 1)]\n(15)\nfrom which ˜qlower and ˜qupper follow:\n˜qlower = loge ((2 + q)vlowerˆy )\n(16)\n˜qupper = loge ((2 + q)vupperˆy )\n(17)\nAnalogous to Eq. 11, we then construct our estimators after rescaling v′\nlower = W\n′′T vlower, v′\nlower ∈\nRC and v′\nupper = W\n′′T vupper, v′\nupper ∈RC:\np(ˆy)lower =\n(2 + ˜qlower)\nv′\nlowerˆy\nPC\nc=1 (2 + ˜qlower) v′\nlowerc\n(18)\np(ˆy)centroid = oˆy\n▷from Eq. 11\n(19)\np(ˆy)upper =\n(2 + ˜qupper)\nv′\nupperˆy\nPC\nc=1 (2 + ˜qupper) v′upperc\n(20)\nAs with Eq. 11, the convention is to set ˜qlower = 0 and/or ˜qupper = 0 for the rare cases for which the\ntransforms in Eq. 18 and/or Eq. 20, respectively, result in the arg max value of the normalized output\nvector not being equivalent to ˆy = arg max z′. (In such cases, e.g., Eq. 18 is not re-calculated with\n˜qlower = 0, but rather such values are treated separately in downstream analyses as out-of-distribution\npoints.)\nBase Estimators\np(ˆy)lower ∈R1 will be used as the basis of our primary test-time estimator\nof prediction-conditional uncertainty (see § 4.2.5 for the complete, index-conditional estimator).\np(ˆy)centroid ∈R1 (via Eq. 11) is a consequence of intermediate results needed in service of construct-\ning p(ˆy)lower (e.g., for training the re-scaler and setting a threshold on ˜q, described below), whereas\np(ˆy)upper ∈R1 is primarily only of research interest, included here to analyze the behavior of the\napproach.6\n6In practice, rather than using p(ˆy)upper, if a less stringent admission criteria is desired, the operative action\nis to reduce α′ and re-estimate p(ˆy)lower.\n8\n\n\n4.2.2\nTraining the Rescaling Transform\nWe train the C2 parameters of W\n′′ of the re-scaling linear layer over Dca (not Dtr) by minimizing\nthe following loss (Alg. 2), which is the counterpart to Eq. 11, while all other parameters remain\nfixed:\nL(W\n′′; Dca) = −\n1\n| Dca |\n| Dca |\nX\nn\nlog(2+˜q)\n \n(2 + ˜q)v′\nyn\nPC\nc=1 (2 + ˜q)v′c\n!\n(21)\nOur convention is to train with a batch size of 1 and conclude the learning process if L(W\n′′; Dca)\nincreases for a pre-specified (as a hyper-parameter) number of consecutive epochs.\nAlgorithm 2 Training the Weights of the Rescaling Transform\nInput: cached v for Dca, rescaler max epochs, rescaler stopping condition\n1: procedure TRAIN-RESCALER(cached v for Dca, rescaler max epochs, rescaler stopping condition)\n2:\nW\n′′\n∗←∅\n▷Final weights\n3:\nW\n′′ ←random initialization\n4:\nmetric ←∞\n5:\ncounter ←0\n6:\nfor e ∈1, . . . , rescaler max epochs do\n7:\nMinimize loss ←L(W\n′′; Dca)\n▷Eq. 21\n8:\nif loss < metric then\n9:\nmetric ←loss\n10:\nW\n′′\n∗←W\n′′\n11:\nif loss > metric then\n12:\ncounter ←counter + 1\n13:\nif counter > rescaler stopping condition then\n14:\nbreak\n15:\nelse\n16:\ncounter ←0\n17:\nreturn W\n′′\n∗\nOutput: W\n′′\n∗\n4.2.3\nRegion-specific eCDFs\nThe estimators p(ˆy)lower, p(ˆy)centroid, and p(ˆy)upper incorporate explicit notions of the effective\nsample sizes. Smaller effective sample sizes will be associated with lower probability estimates\n(and vice-versa). They also have strong relative notions of the highest probability regions of the\noutput distribution by virtue of the original SIMILARITY, DISTANCE, and MAGNITUDE signals,\nand the aggregated distributional statistics over these signals. However, what they lack is a human\ninterpretable, principled cutoff, or threshold, by which we can have some assurance that the new\npoints we see are reasonably comparable to the data we observed in deriving our estimators. This is a\nmore subtle and foundational problem than it may initially seem; we must account for distribution\nshifts if we seek to realistically achieve our desired notion of index-conditional calibration (Def. 4.3).\nIt will require an additional set of transforms to resolve, even with the already strong signals of\nprediction-conditional uncertainty from our estimators, to which we turn next.\nIt follows from Eq. 1 that the output of softmax(z) can be viewed as softmax(z) = △C−1, which\nis the (C −1)-dimension simplex, where the dimension reduction is a consequence of the output\nsumming to 1. The same is true of the normalized value o. If we instead consider the over-\nparameterized version in which each event probability of the categorical distribution (e.g., Eq. 2)\nis explicitly specified as an element of a vector of length C, the following indicator result directly\nfollows:\nRemark 4.5. Given the C class-conditional CDFs over categorical distributions where the 1 −α′\n(α′ ∈( 1\nC , 1]) quantile threshold ψc (ψc ∈[0, 1]) of each class c ∈{1, . . . , C} is >\n1\nC (i.e.,\nψc = inverseCDFyc(1 −α′) > 1\nC ∀c ∈{1, . . . , C}), a set of i.i.d. points sampled from the same\ndistribution as the CDFs, each of whose event probability vector e = [e1, . . . eC] has one (1) element\nat least the corresponding class threshold (i.e., |[e1, . . . eC] ≥[ψ1, . . . ψC]| = 1, with the comparison\ntaken element-wise), will have class-conditional accuracies ≥α′, in expectation.\n9\n\n\nProof Sketch. Partition the class-conditional CDFs of the categorical distributions, for which\nψc = inverseCDFyc(1 −α′) > 1\nC ∀c ∈{1, . . . , C}, at [ψ1, . . . ψC]. The resulting high-probability\npartitions—those ≥ψc—are C Bernoulli distributions each with success probability pc ≥α′. Take\nas [n1, . . . nC] the class-wise count of i.i.d. points whose event probability vector, e, satisfies\n|[e1, . . . eC] ≥[ψ1, . . . ψC]| = 1. Then by the definition of the expected value of a Binomial dis-\ntributed random variable, it follows from these trials that [ n1·pc\nn1 , . . . , nC·pC\nnC\n] = [≥α′, . . . , ≥α′],\nwhich is the desired class-conditional accuracy for this restricted set of points. Now, instead assume\nthat one or more of the Bernoulli distributions has a success probability pc < α′. This implies that the\nclass-conditional CDFs were constructed from a distribution whose event probabilities are not those of\nthe (C−1)-dimension simplex since we require ψc = inverseCDFyc(1−α′) > 1\nC ∀c ∈{1, . . . , C}\nwith the CDFs constructed class-wise relative to the true labels, which is a contradiction of the defini-\ntion of a categorical distribution since the sum of all event probabilities, each of which is a real value\nin [0, 1], must equal 1.\nNote that when ψc <\n1\nC no such assurance across all classes necessarily results, since the result-\ning thresholding of the probability vectors may induce a complex dependence across the class-\nconditional CDFs.7 In such cases, the thresholding of a new point may result in multiple classes\nabove the threshold, and the subsequent stratification of this set of points to those for which\n|[e1, . . . eC] ≥[ψ1, . . . ψC]| = 1 will not necessarily have class-conditional accuracies ≥α′, in\nexpectation.\nRemark 4.5 thus differs from set-valued estimators such as conformal estimators [46], which as\npreviously mentioned (see § 4.2, introduction) are premised on a different calibration compromise.\nFor example, with conformal estimators, there is a statistical assurance for coverage of the true class\nin a discrete prediction set (itself a distinct quantity from that considered here) across all points\nregardless of the distribution of the conformity score (e.g., instead of a categorical distribution, a\nconformity score can be an unnormalized scoring function), but no assurance conditional on the\nsubset of high-probability points. We explore the implications of these tradeoffs in our empirical\nexperiments.\nRemark 4.5 can be viewed as a useful indicator function, but it is not particularly informative as an\nestimator alone. We will use it in service of dividing the output distribution into high probability\nregions via ˜q, described next.\nCorralling the high-probability region via exclusion of the observed high-epistemic-uncertainty\npoints.\nIntuitively, higher values of ˜q correspond to points with a closer connection to the observed\ndata and thus lower epistemic uncertainty, as this single value takes into account the SIMILARITY,\nDISTANCE, and MAGNITUDE signals, and distributional statistics over those signals. The result in\nRemark 4.5 provides a principled basis for setting a threshold on ˜q over Dca that we can then apply at\ntest-time, without access to the true label, to constrain our estimates to the high-probability region of\nthe distribution.\nThe value of ˜q is real-valued, but only ≤| Dca | values are observed, so a simple iterative search\nalgorithm is sufficient to find the value of ˜q that satisfies Remark 4.5 such that all thresholds, ψc,\nover the estimates of o (Eq. 11), are at least α′. By definition, α′ >\n1\nC , so this more stringent\nrequirement satisfies the condition in Remark 4.5, while also requiring ˜q to be restricted to the\nprediction-conditional estimates of p(ˆy)centroid ≥α′. The full algorithm appears in Alg. 3, iteratively\nconstructing class-wise eCDFs over Dca restricted to progressively larger values of ˜q. (These eCDFs\nover the o values of Dca are only needed for Alg. 3 and are not needed at test-time, unlike those of\nEq. 6, Eq. 9, and Eq. 12.) Note that we only consider values of ⌊˜q⌋> 0, as points with ⌊˜q⌋= 0 are\nconsidered out-of-distribution.8 The search algorithm may fail to find a suitable final value, ˜qmin,\n7We leave to future work whether a similar result holds for a subset of the class-conditional accuracies if\nonly some, rather than all, class-wise thresholds are at least 1\nC . In our primary verification setting over LLMs,\nthe typical setting is C = 2, or some similarly small C ∈Z+, where to ensure deployment reliability, the LLM\nwould not be deployed until at least the verification task yields class-conditional accuracies, for all classes, at or\nabove α′ on the available labeled sets, so we do not consider this case here.\n8The reason for the floor operation becomes evident in the next section. ⌊˜q⌋will serve as our hard-partitioned\nmapping between the observed data and new test points to enable estimates of uncertainty over iterations of the\nentire process described thus far.\n10\n\n\nat which point the operative conclusion is that reliable estimates of index-conditional calibration\n(Def. 4.3) are not possible without reducing α′, or acquiring additional data and/or a stronger model.9\nWhen a value of ˜qmin can be found, the convention is to restrict our estimates of index-conditional\ncalibration to the new, unseen test points that satisfy ˜qlower ≥˜qmin after considering the final\nadditional sources of uncertainty from the data splitting and learning processes, which we consider\nnext.\nAlgorithm 3 Search Algorithm to Find ˜qmin to Detect High-Probability Regions\nInput: cached (˜q, o) for Dca, α′ ∈( 1\nC , 1]\n1: procedure FIND-MIN-RESCALED-Q(cached (˜q, o) for Dca, α′ ∈( 1\nC , 1])\n2:\n˜qmin ←∅\n▷A suitable ˜qmin may not exist.\n3:\n[ψ1, . . . ψC] ←[∅, . . . , ∅]\n▷Needed at test-time, if applicable\n4:\n˜qs ←sorted [˜q ∈Dca s.t. ⌊˜q⌋> 0]\n▷Restricted to ⌊˜q⌋> 0 to exclude OOD\n5:\nfor ˜q′ ∈˜qs do\n6:\nConstruct eCDFy1\nca, . . . , eCDFyC\nca for all ˜q ≥˜q′ in Dca\n▷eCDFs for o (Eq. 11), stratified by y\n7:\nCalculate ψc = inverseCDFyc\nca(1 −α′) ∀c ∈{1, . . . , C} ▷Quantile functions are inverses of L. 6\n8:\nif all( [ψ1, . . . ψC] ≥α′ ) then\n▷Element-wise comparison\n9:\n˜qmin ←˜q′\n▷Satisfies Remark 4.5 at the prediction-conditional estimate (see text) of ≥α′\n10:\nbreak\n11:\nreturn ˜qmin, [ψ1, . . . ψC]\nOutput: ˜qmin, [ψ1, . . . ψC]\n4.2.4\nAccounting for Uncertainty in the Data Splitting and Learning Processes\nAs a final step, we take into account uncertainty over the data splitting and learning processes. This\nwill incur non-trivial additional computational costs, but these are one-time development costs for\nan estimator. At test-time, our estimates will be constant offsets on ˜qmin and p(ˆy)lower, the latter\nconditional on ⌊˜q⌋∈Z0+, which will serve as a stable mapping between Dca and new, unseen test\npoints. In summary, in this section, we seek:\n˜q γ\nmin\n▷A robust estimate of ˜qmin\n(22)\nmˆy\n⌊˜q⌋\n▷A class-wise, robust correction for p(ˆy)lower, conditional on ⌊˜q⌋\n(23)\nConceptually, the estimation process is straightforward. We repeat the training and estimation\nprocesses described above J times and derive our constant offsets via summary statistics over those\nestimates. The one complication that arises is that we will have to depart from the distribution-\nassumption-light approaches above, since J will typically not be large due to the computational\nexpense. (The full process across J to construct a single estimator needs to remain reasonably\ncomputationally lightweight relative to an LLM training epoch, as it itself will be embedded into the\nlearning loop of an LLM, described below.) Instead, we will estimate each of these processes as a\nCauchy distribution, given its relatively wide tails and relatively robust scale parameter.\nA Cauchy distribution is defined by a location parameter, ν, and a scale parameter, γ:\nCauchy(ν, γ)\n(24)\nThe inverse CDF (i.e., quantile function) of a Cauchy distribution for a particular quantile, α ∈[0, 1],\ncan be calculated analytically as:\ninverseCDFCauchy(ν,γ)(α) = ν + γ tan\n\u0012\nπ\n\u0012\nα −1\n2\n\u0013\u0013\n(25)\nWe take as our estimate of γ the median absolute deviation around the median of our sample (MAD).\n9Alg. 3 could be readily modified to find an adaptive value of α′, iteratively reducing α′ if a suitable ˜qmin\nvalue is not found. However, in practice, determining α′ for LLM settings is a decision made exogenous to the\nmodel development process. We seek to develop our models (and data) to meet a given α′ value, rather than the\nother way around, so we do not consider that variation here.\n11\n\n\nRobust detection of high-probability regions.\nTo calculate γ for ˜q γ\nmin ∈R1, we take the MAD\nof the J estimates of ˜qmin. The location parameter is taken as ˜qmin∗, the estimate of ˜qmin over the\nmodel with the final chosen weights (see Alg. 1). We can then analytically calculate our desired value\nvia Eq. 25 at α′ ∈( 1\nC , 1]:\n˜q γ\nmin = inverseCDFCauchy(˜qmin∗,γ)(α′)\n(26)\nNote that since α′ corresponds to the right-tail of the distribution, ˜q γ\nmin ≥˜qmin∗, i.e., a more restrictive\nthreshold on the high-probability region. In scenarios (not considered in the experiments here) where\nthe computational budget necessitates J = 1, the convention would be to take ˜q γ\nmin := ˜qmin∗, with a\ntacit assumption that these additional sources of uncertainty have not been explicitly accounted for.\nRobust output adjustment.\nTo calculate γ for mˆy\n⌊˜q⌋conditional on ˆy and ⌊˜q⌋(i.e., γ | ˆy, ⌊˜q⌋), we\ntake the MAD of the J medians (as written) of p(ˆy)centroid over Dca, conditional on ˆy and ⌊˜q⌋.10\nSimilar to above, we can then calculate:\nmˆy\n⌊˜q⌋= inverseCDFCauchy(0,(γ | ˆy,⌊˜q⌋))(α′)\n(27)\nIn this case, ν is 0, as mˆy\n⌊˜q⌋will be subtracted from p(ˆy)lower as an offset, an assumption that each\ndistribution is centered on the given point. To simplify the presentation (and since the upper offset is\nnot needed in practice), we only consider this as a lower offset on our base estimators.\nAs ⌊˜q⌋increases, the number of points in the sample will tend to decrease, but so will the MAD, so\nthe estimates remain reasonable in practice. As we will see in our experiments, high values of ⌊˜q⌋\n(that are otherwise attested in Dca) are not uncommonly associated with MAD values that are within\n0 of numerical precision.\nAs with ˜q γ\nmin, although it is generally recommend to take these additional sources of uncertainty into\nconsideration, when J = 1, the convention would be to take mˆy\n⌊˜q⌋:= 0.\n4.2.5\nIndex-Conditional Calibration\nWith the above models and estimators, we can now robustly calculate the index-conditional uncertainty\nof a new, unseen test point x ∈Dte.\nWe first take as the prediction ˆy = arg max z′. Then, with Dtr to calculate q and dnearest; the\ncached class-wise empirical CDFs over Dca of Eq. 6, Eq. 9, and Eq. 12; ˜q γ\nmin and the thresholds\n([ψ1, . . . ψC]); and mˆy\n⌊˜q⌋, the index-conditional uncertainty estimate of p(y | x) at α′ (Def. 4.3) is:\nˆp(y | x) =\n(\nmax(0, p(ˆy)lower −mˆy\n⌊˜q⌋)\nif [˜qlower ≥˜q γ\nmin] ∧\nh\u0010\np(ˆy)lower −mˆy\n⌊˜q⌋\n\u0011\n≥ψˆy\ni\n⊥\notherwise\n(28)\nwhere ⊥indicates a rejected (non-admitted) point.\nAs noted in the previous sections, in the rare cases when the transforms after the SDM activation\nresult in the arg max index not matching ˆy, we set ˜qlower = 0, which effectively treats the point as\nout-of-distribution. In such cases, ˆp(y | x) = ⊥, since ˜q γ\nmin > 0 as a consequence of Line 4 in Alg. 3.\nTo simplify the presentation of comparisons, when not otherwise ambiguous, our convention in\nsubsequent sections will be to slightly overload the notation and refer to summary statistics and\ncomparisons of ˆp(y | x) (Eq. 28), excluding the points assigned ⊥, as estimates from the “estimator\np(ˆy)lower”. We do the same for the “estimator p(ˆy)centroid” and the “estimator p(ˆy)upper”, but where\nthe latter two quantities are calculated from the corresponding centroid and upper intermediate\nquantities, respectively.\nComplexity.\nThe added computational overhead over an underlying network with a softmax\nactivation is dominated by calculating q (and by extension, dnearest). The transforms after the SDM\nactivation function add negligible additional overhead. For perspective, this is on the order of the\nadditional computation needed for commonly used dense retrieval augmentations of LLMs, so it is\nreadily achievable at interactive speeds in practice.\n10Implementation note: Unlike the eCDFs, which are constructed by stratifying on the true label, y, in Dca,\nthis quantity is calculated by stratifying on the predicted label, ˆy, in Dca.\n12\n\n\nSharpness.\nAs noted in § 4.2, we seek estimators that are both informative (i.e., not unnecessarily\nrejecting correct predictions) and robust (i.e., we prefer rejection over falling under the expected\nα′ accuracy). The above transforms seek to achieve this by taking the uncertainty signals from an\nSDM activation and further separating the high and low probability regions of the distribution, as well\nas providing a hard cut via ˜q γ\nmin to altogether exclude predictions over high epistemic uncertainty\nregions. We explore these behaviors empirically in our experiments.\nNext, we incorporate our estimators directly into LLM next-token training.\n4.3\nFrom SDM Calibration to SDM Networks\nThe above approach is already a very powerful and easily implemented mechanism for building\ncomplex LLM pipelines. We can treat an underlying network as fixed, add an SDM activation layer,\nand then use the SDM estimator for conditional branching for test-time compute, retrieval, tool-calling,\nand related.\nHowever, earlier in the model development pipeline (e.g., as done by LLM model providers), we\nneed a mechanism for fine-tuning a network after the initial unsupervised training stage.11 In this\nsection, we show how to incorporate the SDM mechanism directly into the LLM next-token training\nprocess. We will refer to this process and the resulting model as an SDM network.\nConceptually, an SDM activation and estimator over an averaged history of frozen hidden states\nand the token-level hidden state will be trained for binary classification at the unit of analysis of\nthe available labels (e.g., the document-level). This estimator then provides the SIMILARITY and\nDISTANCE values for an SDM activation for next-token loss of the LLM during training. Because\nan SDM activation does not alter the arg max prediction, greedy token-level generation can proceed\nwithout the computational cost of the SDM activation at every token at test-time, with the global SDM\nestimator providing verification over the final generation. This process shares the same goal of\nexisting fine-tuning approaches to increase overall accuracy, add information to a model, etc.,\nas well as the new goal of increasing the proportion of verifiable high-probability generations\nfrom a model. During training, we seek to penalize the model for verification mistakes, and reward\nthe model for increasing the cardinality of the set of admitted points.\nWe first introduce our data encoding scheme in § 4.3.1 for verification. Next, orthogonal to the SDM\nmechanism itself, we introduce a parsimonious regularization method (§ 4.3.2) to enable fine-tuning\non a small amount of data while discouraging catastrophic forgetting. Finally, we introduce the\nprocess for training the SDM network (§ 4.3.3).\n4.3.1\nUniversal Verification Encoding\nIn the abstract, our data is similar to that in the previous sections: Input documents accompanied\nwith discrete labels. However, while we previously treated each document, x, as an atomic whole,\nwe will now also be concerned with the individual tokens of the document, for which we use the\nnotation Dtr = {(xn = [x1, . . . , xT ], yn, [ytask\nn\n])}N\nn=1 for our labeled training set, and similarly for\nour labeled calibration set, Dca. Each token, xt ∈{1, . . . , |V| · 2}, is represented as an index into a\nvocabulary, where V is the vocabulary of the LLM trained during the initial unsupervised training\nstage. The reason for the factor of 2 is described in the next section. Implicit in our representation is\nthat each instance will have a marker at some xt indicating a “completion” (i.e., a sequence after an\ninstruction prompt or prefix, more generally). Our document-level labels, y ∈Y = {0, 1}, are as in\nprevious sections, but specifically restricted to binary classification, where the convention is to treat\ny = 0 as representing the unverified class and y = 1 as the verified class (i.e., an acceptable\ngeneration, conditional on the instruction or context).\nFor some documents, we have classification labels, ytask ∈Z2+, for the underlying tasks encoded\nin the data. For example, for a sentiment classification task of negative and positive reviews, y = 0\nfor verification when the classification decision is wrong, whereas y = 1 for verification when the\nclassification decision is correct. Among those for y = 1, ytask = 0 could indicate a negative review\nand ytask = 1 could indicate a positive review. These task-specific labels are predicted via the\ngenerated text of the LLM, and if available, we can use them during training (e.g., as part of our\n11In principle, the methods in this section can also be used for bootstrapping a randomly initialized LLM\nagainst an existing (possibly larger) model, which we leave to future work.\n13\n\n\nstopping criteria to choose the best weights, by parsing the generated text and comparing to ytask).\nUnlike typical classification settings, these labels may—and typically will—cover multiple disparate\ntasks; hence, the designation of universal verification. When the distinction is potentially ambiguous,\nwe will add a superscript to y for the binary verification labels: yverification.\nUnlike typical preference fine-tuning encodings, we do not require prefixes (or prompts) of x to be\npaired with different completions and opposing document-level labels. However, as in the above\nsections, we will assume that Dtr and Dca are balanced across y (i.e., an approximately equal number\nof documents with y = 0 and y = 1).\nThe SDM activation layer for verification will be trained with Dtr, seeing all documents with y = 0\nand y = 1 labels. However, the LLM’s SDM activation for next-token training will only directly see\ndocuments with y = 1, with the signal of the unverified class coming indirectly via matching\ninto Dtr to calculate SIMILARITY and DISTANCE. (There is an additional nuance with train-time\ngeneration vs. train-time force-decoding that will be clarified below.) As such, the additional SDM\nmechanisms enable a unification of preference fine-tuning, instruction fine-tuning, and supervised\nfine-tuning encodings since in all of the above, we always have at least the y = 1 documents, and it is\ntypically straightforward to collect, or otherwise synthetically generate, unpaired examples to serve\nas y = 0 (i.e., generations we seek to avoid showing users).\n4.3.2\nNegative+Positive Vocabulary Normalization and Regularization\nBefore we can make progress on incorporating the SDM mechanisms, we need to address the matter\nof fine-tuning pre-trained LLMs without inducing catastrophic forgetting. This is critical, since each\nround of LLM training and fine-tuning is computationally expensive. We seek to make incremental\nchanges to the model without having to run subsequent learning processes over all previously seen\ndata. To address this, we first briefly recall the training of auto-regressive neural language models\nprior to the era of large-scale pre-training.\n[L]LM Training Redux.\nPrior to the era of large-scale pre-training of LMs that emerged at the\nend of the 2010’s, auto-regressive language models for transduction tasks (e.g., grammatical error\ncorrection) were successfully trained from random initialization using specialized input control\ntokens and output diff sequences (and associated output control tokens) that separated non-preferred\n(pre-transduction) and preferred (post-transduction) generated sequences [40]. Importantly, the bias\non the diff control sequences could be modulated to control precision and recall over the absence\nand presence of the transduction operation [38]. In-effect, the sequence transduction model could\nbe effectively used as a classifier without additional classification layers, while also having the\nexpressivity to generate token sequences, unlike standard discrete classifiers.\nInput and output control tokens are now prominent features of LLM vocabularies to structure prompts,\ninstructions, and reasoning sequences. However, while the bias of individual tokens can be modified\nwith an additive offset, current LLMs lack a mechanism to explicitly bifurcate the output distribution\ninto non-preferred and preferred regions in the manner of the earlier models. This capability can be\n(re)-added to LLMs without direct training on diff transduction sequences, as follows.\nNegative+Positive Vocabulary Normalization.\nConsider a pre-trained LLM model, Mref. Our\nreference model generates acceptable sequences over part of the data distribution, but it also produces\nnon-preferred (NEGATIVE) generations; hence, our desire for further training. However, we only\nwant to alter the behavior of Mref over the space that produces NEGATIVE generations, otherwise we\nmay unexpectedly cause the previously acceptable space of generations to also become NEGATIVE.\nIn effect, we have two regions—a bifurcation—of the output distribution: The space of existing\nacceptable generations and the space of NEGATIVE generations. We seek to replace the NEGATIVE\nregion with a new POSITIVE region of acceptable generations without (or at least minimally) impacting\nthe existing acceptable region.\nFrom Mref create two clones, Mneg and Mpos. Each model has a final linear layer that maps\nto the output vocabulary, V, via a weight matrix12: zref = W T\nrefhref, zneg = W T\nneghneg, and\nzpos = W T\nposhpos, respectively. During fine-tuning for the next-token loss, we then calculate the\nSDM activation (in-place of a standard softmax) as the concatenation of the un-normalized output\n12We ignore the bias terms here to simplify the presentation. In practice, it is not uncommon for b = 0.\n14\n\n\nof Mneg and Mpos, SDM(zneg, zpos), keeping the weights of Mneg, Wneg and θneg, fixed and\nupdating the weights of Mpos, Wpos and θpos. For the y = 1 documents that participate in fine-\ntuning, we simply take the original token indexes and add an offset, xt + |V|, for the output tokens\nwhen calculating the loss over the joint, concatenated distribution. (Input tokens retain their original\nindexes.) At test-time, the arg max output index mod |V| maps back to the original token symbol in\nthe vocabulary. In this way, an additional set of token symbols is never explicitly instantiated.\nIn the most direct sense, this then requires a copy of the full weights to be present at test-time.\nHowever, in practice, Mpos need not be a copy of all the weights; Mpos can be represented by\nadaptor layers, or similar mechanisms (e.g., only updating a subset of the model’s weights).\nRegularization.\nTo further prevent drift from the original reference distribution, we also add an\nL2 regularization term in the log(2+q) space of the normalized joint, concatenated distribution when\ncalculating the next-token loss.\nr = ||i ⊙log(2+q) (SDM(zref, zref)) −i ⊙log(2+q) (SDM(zneg, zpos)) ||2\n(29)\nwhere the Hadamard (element-wise) product (⊙) is with a mask vector i ∈R|V|·2 that lessens\nthe regularization on the peak of the distribution by not considering the arg max indexes of the\nreference, negative, and positive distributions, as well as that of the ground-truth next-token label\n(here, represented as t), in the L2 constraint:13\ni = 1 ∈R|V|·2\n(30)\niarg max (zref) = 0\niarg max (zref)+|V| = 0\niarg max (zneg) = 0\niarg max (zpos)+|V| = 0\nit = 0\nWe seek for our regularization term to be scaled relative to the loss, so we perform a simple re-scaling:\nr′ =\nq\nmax(r, 1)min(max(s,0),1),\n(31)\ns = loge L(Wpos, θpos; Dtr)\nloge r\nAfter rescaling, r′ is an additive term in the next-token training loss, described below. Next, we\ndescribe how SDM is calculated, and the structure of the network, more generally.\n4.3.3\nSDM network\nThe network makes use of two separate SDM activations. The first (VERIFICATIONLAYER) is over\nthe binary verification task, trained at the document level. This is built as described in § 4.1, but\nspecifically with an exemplar adaptor g :\n\u0000mean(hneg), mean(hpos), h−1\nneg, h−1\npos\n\u0001\n∈R4D 7→h′ ∈\nRM, trained over the concatenation of the mean of the final hidden states across tokens of both Mneg\nand Mpos, as well as the hidden state (i.e., h−1\nneg ∈RD and h−1\npos ∈RD) that predicts the end of\nsequence delimiter14, for which we use the superscript -1, all of which remain fixed when training the\nadaptor.15 This has an associated SDM estimator, p(ˆy)lower, over the binary task (§ 4.2).\nThe second SDM activation is for normalizing the linear layer over the output vocabulary for next-\ntoken training, as described in § 4.3.2. In this case, the output MAGNITUDE is determined by the\n13This also accounts for the setting, not considered in our experiments, where Mneg is not identical to Mref,\nsuch as via multiple iterations of fine-tuning where Mneg is trained away and is replaced with Mpos for a\nsubsequent fine-tuning round.\n14This is the symbol that indicates the end of the sequence at the unit of analysis of the verification labels\n(e.g., the sentence or document level).\n15In our small-scale experiments, we only train the final hidden layer of the LLM (i.e., θpos stays fixed, and\nwe only update Wpos), so we exclude the weights of Mneg as input to the exemplar adaptor, since they are\nidentical to those of Mpos.\n15\n\n\nconcatenation of (zneg, zpos), but the values of q and d are from the VERIFICATIONLAYER. In\nother words, for this second SDM activation, there is no exemplar adaptor inserted between the final\nhidden state of the LLM and the linear-layer over the vocabulary. This enables easily adapting this\nmechanism to existing architectures and pre-trained weights, if desired.\nSDM Network Next-token Loss.\nHolding the weights of the VERIFICATIONLAYER fixed, the next\ntoken loss to update the weights of Mpos, Wpos and θpos, is then:\nL(Wpos, θpos; Dtr, β, Mref) = −1\nN\nN\nX\nn\nlog(2+q)\n \n(2 + q)d ·zneg,postn\nP|V|·2\nv=1 (2 + q)d ·zneg,posv\n!\n+ β r′\n(32)\nwhere tn is the index of the correct next token, and β ∈[0, ∞) linearly increases every mini-batch in\nan epoch from βmin (e.g., 0, in our experiments) to βmax (e.g., 0.1, in our experiments).\nTrain-time Generation vs. Train-time Force-decoding.\nThe loss in Eq. 32 requires q and d, which\nare predicated on labels at the document-level, for each token prior to the model seeing the end of the\ndocument. In practice, for an (x, y = 1) ∈Dtr, prior to calculating the loss, we decode a completion\nfor x starting at the completion marker xt (e.g., starting at the instruction prompt, or given prefix, as\nnoted in § 4.3.1) with q = e −2, d = 1. Then we derive q and d from the VERIFICATIONLAYER over\nthis generated output. We otherwise discard the generated completion and calculate the loss using\nthese updated values of q and d over the correct next token. (In the present work, q and d are the same\nfor each token in a single document.) Note that the stored support set of the VERIFICATIONLAYER\n(which determines q) is constructed by force-decoding over (x, y = {0, 1}) ∈Dtr. Thus, the loss\nhas the desired semantics of rewarding the model to resemble the y = 1 data at the token-level (as in\nstandard next-token fine-tuning), while penalizing generations that are challenging to verify.\nSDM Network Training: VERIFICATIONLAYER + Next-token Loop.\nThe next-token loss and\nthe VERIFICATIONLAYER interact via q and d and the stopping criteria. However, the weight updates\nof each occur separately.\nWe seek the weights that maximize the admitted points over Dca via p(ˆy)lower for ˆy = 1, and (if\navailable), further restricting this set to those with correct ytask predictions (parsed from the generated\ntext) for the underlying tasks encoded in the data.\nThe combined training loop is conceptually straightforward (Alg. 4). First, we construct the SDM\nestimator for binary verification (VERIFICATIONLAYER) via Alg. 1 by force-decoding over Dtr and\nDca. (The convention is to shuffle Dtr and Dca in the first training of the VERIFICATIONLAYER,\nitself a process over J iterations, and then use that final split for all subsequent processes.) Next, we\ntrain one epoch of Mpos. The next-token loss (Eq. 32) uses q and d from the VERIFICATIONLAYER\nover completions generated via greedy decoding (with q = e −2, d = 1) using SDM(zneg, zpos)\nstarting at the completion marker.16 Once the epoch concludes, we retrain the VERIFICATIONLAYER\nand update q and d for Dtr. We then generate completions using SDM(zneg, zpos) over Dca and\ncalculate the number of points for which p(ˆy)lower provides an index-conditional estimate for ˆy = 1,\nfurther restricted (if applicable) to the underlying task labels, ytask, and the predictions parsed for\nthose tasks from the generated output. Next, we continue to the next epoch of updating Mpos. This\nprocess is continued until the max number of epochs has been reached.\nSDM Network Test-time Generation.\nAt test-time, we generate from SDM(zneg, zpos) up to the\noutput control token, or end-of-sequence token, at the unit-of-analysis of the verification labels, via\ngreedy (i.e., arg max) decoding with q = e −2, d = 1 (i.e., equivalent to softmax).17 We then con-\ntinue generation, or take other branching actions, based on p(ˆy)lower from the VERIFICATIONLAYER,\nwhich by extension, also provides interpretability-by-exemplar into Dtr via matching (from q) and\nagainst similarly calibrated points in Dca via ⌊˜q⌋. Each classification via the VERIFICATIONLAYER\nrequires on the order of the computation needed for commonly used dense retrieval augmentations of\nLLMs, so such test-time generation and verification is achievable even using edge devices.\n16In practice, we cache q and d before each epoch, but in principle, they can be calculated dynamically during\nan epoch as the weights change. Caching simplifies the implementation at the expense of potentially biasing the\nestimates as an epoch proceeds, which is the motivation for increasing β through the course of an epoch.\n17As previously noted, at test-time, (arg max(SDM(zneg, zpos)) mod |V|) maps back to the original token\nsymbol in the vocabulary when decoding over the joint distribution.\n16\n\n\nAlgorithm 4 SDM Network Training\nInput: Dtr, Dca, α′, max epochs, Mref, Mneg, Mpos\n1: procedure SDM-NETWORK-TRAIN(Dtr, Dca, α′, max epochs, Mref, Mneg, Mpos)\n2:\nVERIFICATIONLAYER, Dtr∗, Dca∗, E ←SDM-ITERATIVE-TRAIN(·)\n▷Alg. 4\n3:\nM∗←Initialized with Mneg, Mpos\n▷Final trained model\n4:\nmetric∗←0\n▷Determines final model\n5:\nVERIFICATIONLAYER∗←VERIFICATIONLAYER\n▷Final SDM activation layer for verification\n6:\nE∗←E\n▷Final SDM estimator (i.e., p(ˆy)lower) for verification\n7:\nβstep ←\nβmax−βmin\ntotal mini batches\n▷Used to calculate β as a function of epoch progress\n8:\nCalculate q, d for each (x, y = 1) ∈Dtr∗using VERIFICATIONLAYER over generated output from\nSDM(zneg, zpos) with q = e −2, d = 1\n9:\nfor e ∈1, . . . , max epochs do\n10:\nMinimize L(Wpos, θpos; Dtr, β, Mref)\n▷Eq. 32\n11:\nVERIFICATIONLAYER, _, _, E ←SDM-ITERATIVE-TRAIN(·)\n▷Without shuffling Dtr∗, Dca∗\n12:\nUpdate q, d for each (x, y = 1) ∈Dtr∗\n▷As in Line 8\n13:\nmetric ←cardinality of the admitted set from p(ˆy)lower for ˆy = 1 over Dca∗\n▷Restricted to\nytask = ˆytask, if available\n14:\nif metric > metric∗then\n15:\nmetric∗←metric\n16:\nM∗←Update with Wpos, θpos\n17:\nVERIFICATIONLAYER∗←VERIFICATIONLAYER\n18:\nE∗←E\n19:\nreturn M∗, Dtr∗, Dca∗, VERIFICATIONLAYER∗, E∗\nOutput: M∗, Dtr∗, Dca∗, VERIFICATIONLAYER∗, E∗\n5\nExperiments\nWe comprehensively evaluate the uncertainty-awareness of our estimators across a representative\nset of the existing classes of estimators over LLMs. First, we compare SDM calibration to existing\napproaches in a standard classification setting, using open-source models at a scale that can be\nreadily replicated with consumer-level compute (§ 5.1). Next, we show how an SDM estimator can\nbe applied to a fully black-box LLM API, only with access to the top output logits and without a\nproxy model running in parallel, using the standard MMLU benchmark (§ 5.2). In this context, we\nalso consider a data quality experiment in which we seek to detect errors in the carefully curated\nMMLU-PRO dataset. This serves as a natural, held-out blind evaluation. Finally, we examine the\nuniversal verification behavior of a SDM network by training over a composition of the classification\ntasks examined in the first set of targeted experiments (§ 5.3).\n5.1\nExperiments: Classification\nBefore introducing the additional complications of LLM generation, we first isolate the core calibra-\ntion behavior against existing classes of approaches in standard multi-class classification settings.\n5.1.1\nTask: SENTIMENT\nTask.\nOur first task (SENTIMENT) is predicting the sentiment of movie reviews using the commonly\nused benchmark data of [28]. This is a binary classification task with y ∈{0 = negative, 1 =\npositive}. Dtr and Dca are constructed from a total of 18k instances. The held-out set for evaluation,\n| Dte | = 1583, is from the same distribution as Dtr and Dca. This is a well-studied task for which\nthe surface-level signals correlated with the target labels are expected to be effectively modeled by\nlarge parameter LLMs; as such, relatively high task accuracies are expected.\nModels.\nOur base network, the parameters of which stay fixed and are used for all estimators,\nis the open-source, publicly available Faster I model from the on-device data analysis program\nREEXPRESS ONE18 from Reexpress AI. This 1.2 billion-parameter model is a late fusion of the\nencoder and decoder of Flan-T5 large [6] and mT0-base [30]. We discard the existing adaptor layers\nthat are part of the on-device program and only use the parameter fusion of the encoder and decoder,\n18https://github.com/ReexpressAI/Reexpress_one\n17\n\n\nadding the adaptors and estimators introduced in this work. We take the mean of the hidden states\nacross input tokens, resulting in a hidden state of h ∈R3774 as input to the exemplar adaptors\nand SDM activation layers with M = 1000. We use the label FASTERI+ADAPTOR for a standard\nexemplar adaptor over h ∈R3774 trained with a cross-entropy loss, and the label FASTERI+SDM for\nthe SDM activation layer over h ∈R3774.\nEstimators.\nHolding network constant, we examine representative classes of estimators used with\nneural networks, seeking index-conditional calibration at α′ = 0.95. At the most basic, but also,\nperhaps the most commonly used in practice, representing the absence of a post-hoc calibration\nmethod, we simply threshold the output, softmax(z) ≥α′, where the temperature τ = 1. As\nan established empirical approach for calibrating neural networks, we provide a comparison to\ntemperature scaling [16], a single parameter version of post-hoc Platt-scaling [34], with the label\nTEMPSCALING. In this case, the estimator is the thresholding of the output softmax(z; τ) ≥α′\nafter learning a value for τ over Dca. We also provide a comparison to two representative conformal\npredictors, the APS method of Romano et al. [35] and the adaptiveness-optimized RAPS algorithm\nof Angelopoulos et al. [2]. The admission criteria for the APS and RAPS estimators is prediction\nsets of size 1, using an α = 0.05.\nWe then compare to the primary SDM estimator p(ˆy)lower, as well as the reference comparisons\np(ˆy)centroid, and p(ˆy)upper, as defined in § 4.2.5. We train the SDM activation layer and estimator\n(Alg. 1) with J = 10, here and for the remaining experiments. Additional training hyper-parameters\nand details shared across all experiments are provided in Appendix A.3.\nAs a common point of reference, here and for all other experiments as well, we will use the label\nNO-REJECT to refer to the model predictions without any selective filtering (i.e., the raw output\naccuracies, either from a softmax or an SDM activation).\n5.1.2\nTask: SENTIMENTOOD\nTo evaluate the behavior of the estimators over out-of-distribution data, we consider an additional\ntask (SENTIMENTOOD) that uses the same models and estimators as SENTIMENT, but an out-\nof-distribution evaluation set, | Dte | = 4750. We use the SemEval-2017 Task 4a test set [36],\nwhich consists of short-form social media posts that differ in the distribution of topics, language\nstyles, and lengths relative to the movie reviews. We balance the test set, dropping the third class\n(neutral), setting the semantics of the true labels to be the same as that of the movie reviews:\ny ∈{0 = negative, 1 = positive}.\n5.1.3\nTask: FACTCHECK\nTask.\nAs a more challenging binary classification task for LLMs, we consider the fact check data\nof [3]. The training and calibration sets, a combined total of 6k instances, consist of single sentence\nstatements that have been semi-automatically generated via templates and a knowledge base. The\ntask is to determine whether the statement is true or false, y ∈{0 = false, 1 = true}. The held-out\neval set, | Dte | = 245, the focus of our analysis, has been constructed by having an LLM generate a\nstatement continued from a true statement not otherwise in the dataset. These evaluation statements\nare checked manually and assigned labels by human annotators. In addition to being a relatively\nchallenging task that evaluates—at least in principle—the latent knowledge stored within an LLM’s\nparameters, the test set is representative of the types of distribution shifts over high-dimensional\ninputs that can be problematic for real applications, and challenging to characterize without model\nassistance and ground-truth labels. It was observed in [3] that the accuracy of existing LLM classifiers\nis dramatically lower on this generated, held-out test set compared to the calibration set. However,\nthese test sentences would seem to also be simple true-false statements, reflecting that it is not always\nimmediately obvious for a human user to detect distribution shifts over high-dimensional inputs. As\nsuch, we seek for our models and estimators to reflect such shifts via the predictive uncertainty, as we\nwill not, in general, have true labels at test-time.\nModels and Estimators.\nReflecting the more challenging task, our base network is the larger 3.2\nbillion parameter Fast I model from REEXPRESS ONE, which is a late fusion of the encoder and de-\ncoder of Flan-T5 xl and mT0-base. We additionally compose the Fast I model with Mixtral 8x7B\nInstruct v0.1 [23]. This is achieved by constructing a simple re-ask verification prompt, and then a\n18\n\n\ntransform of the final layer of the Mixtral model and the output logits is concatenated to the mean of\nthe hidden states across the input tokens of Fast I.19 We use the label FASTI+MIXTRAL+ADAPTOR\nfor a standard exemplar adaptor over the resulting h ∈R5854 trained with a cross-entropy loss, and\nthe label FASTI+MIXTRAL+SDM for the SDM activation layer over h ∈R5854. The estimators are\notherwise the same as those used for the SENTIMENT task.\n5.2\nExperiments: Black-box LLM APIs\nNext, we examine the behavior of the estimators when we only have access to a black-box API\nfor an LLM that provides the generated text and the top-1 output log probabilites. In this context\nwith a state-of-the-art model, we examine an additional class of estimators, those that make use of\nuncertainty estimates explicitly encoded in the surface-level output vocabulary symbols. As a fully\nheld-out test—and real-world use example—we also consider a data quality experiment in which we\nseek to uncover annotation errors in an existing carefully curated benchmark dataset.\n5.2.1\nTask: Question Answering\nTask.\nOur evaluation is over the 4-choice question answering benchmark dataset MMLU [20] and\na 4-choice subset of the more challenging MMLU-PRO dataset [47]20, for which we use the label\nMMLU-PRO-4QA. Dtr and Dca are constructed from 102k instances from the auxiliary_train,\ndev, and val splits of MMLU and the MMLU-Pro validation set, the 4-choice subset of which only\nconsists of 29 instances. For MMLU, | Dte | = 14042. For MMLU-PRO-4QA, | Dte | = 5413.\nModels and Estimators.\nWe use gpt-4o-2024-08-06 (GPT-4O) [31] via the Microsoft Azure\nservice21 as the black-box LLM. Given the zero-shot question, the LLM is tasked with providing a\nstructured response against the JSON Schema in Listing 1, and the top-1 log probability for each\noutput token. The JSON is parsed for the answer letter, the surface-level symbol of which is the\nprediction for the NO-REJECT estimator of GPT-4O. We consider the output probability for the answer\nletter, restricted to those estimates ≥α′, as ANSWERSTRINGPROB. The output JSON is also parsed\nfor the model’s real-valued verbalized uncertainty estimate, which when restricted to estimates ≥α′,\nis the estimator VERBALIZEDPROB.22\nAs a final field, the output JSON also contains a short explanation for the response. We take the mean\nof the output probabilities corresponding to each value of the output JSON and concatenate those\nthree values with a soft feature vector of length 4, where the activated index is that of the surface-level\nanswer choice, for which we use VERBALIZEDPROB as the value, and all other indexes are 0. This\nlength 7 vector than serves as h ∈R7 as input to an SDM activation layer with M = 1000. For the\nresulting GPT-4O+SDM model, we consider the NO-REJECT, p(ˆy)lower, p(ˆy)centroid, and p(ˆy)upper\nestimators. Additional details appear in Appendix A.1.\n5.2.2\nTask: Data Quality Analysis\nThe MMLU-Pro dataset (MMLU-PRO-4QA) is a follow-up to the original MMLU benchmark\ndesigned to have more challenging questions and more reliable answer annotations. In the previously\ndescribed experiment, we examine whether calibration can be maintained over this implied distribution\nshift. Separately, we consider here whether our method can uncover additional annotation errors,\ndespite the relatively large amount of resources already spent to refine the dataset by the dataset\nconstructors. MMLU-Pro reportedly underwent multiple rounds of review with experts and annotators,\nincluding LLM assistance for targeted error detection. We focus on the Computer Science category\ngiven that the questions should have unambiguous, objectively verifiable answers. This data quality\ntest is a natural, fully held-out assessment of our approach compared to existing approaches used in\n19This process is the same as that described in the publicly available REEXPRESS ONE tutorial, but as with the\nSENTIMENT task, we discard the adaptors of the on-device application: https://github.com/ReexpressAI/\nExample_Data/tree/main/tutorials/tutorial7_factcheck.\n20Both datasets are available via https://huggingface.co/datasets\n21https://azure.microsoft.com/en-us/\n22An additional class of estimators for black-box LLMs are those that require multiple test-time forward\npasses through the model, which are related to the Bayesian approaches of [14, inter alia]. We do not consider\nthis class of approaches given the computational costs required of the estimators.\n19\n\n\npractice, with direct, real-world applications. To do so we will examine the annotations among the set\nof admitted points sorted by p(ˆy)lower for which y ̸= ˆy, where the desired behavior is for these points\nto reflect the aleatory uncertainty (exogenous to the model and estimator) of label annotation errors.\n5.3\nExperiments: Verified Generation\nNext, given the context of the above experiments, we examine the behavior of the SDM network.\nTask.\nWe construct the verification task from the SENTIMENT, SENTIMENTOOD, and\nFACTCHECK data described above (§ 5.1), taking the y labels of those earlier tasks as the ytask\nlabels. The yverification (or simply y) labels (and associated instances) are constructed by synthet-\nically inverting the text of the associated completions, as illustrated in Table 8. By design, under\nthe assumption that it is a more challenging learning setting, we do not pair the completions. For\nexample, given a single movie review, it will appear once as part of a user prompt and either the label\nyverification = 0 or yverification = 1, but not both.23\nFor analysis, we then have a standard binary classification task over the force-decoded output,\n(x, yverification ∈{0, 1}) ∈Dte. We use the following labels for the corresponding datasets:\nSENTIMENTVERIFICATION, with | Dte | = 1583; SENTIMENTOODVERIFICATION, with | Dte | =\n4750; and FACTCHECKVERIFICATION, with | Dte | = 245. These test sets are useful for analyzing\nthe behavior of the VERIFICATIONLAYER, but they do not reflect a real test-time scenario.\nFor final evaluation, we take the original test sets from SENTIMENT, SENTIMENTOOD, and\nFACTCHECK (§ 5.1) and evaluate the output of the generated JSON for the underlying task labels,\nytask, as in a standard evaluation of LLM output.\nThe corresponding system and user prompts appear in Listing 2. These design decisions enable\nexamining the instruction-following setting across multiple underlying tasks while enabling reliable\nevaluation of verification, since there is no ambiguity (up to annotation errors in the original tasks) in\nytask or yverification, and we can readily parse the JSON output for the task predictions.24\nModels and Estimators.\nFor Mref we use Phi-3.5-mini-instruct model (PHI3.5) [1], a 3.8\nbillion-parameter decoder-only Transformer model, via MLX [18], version 0.21.1. To keep the\nexperiments manageable at a level of compute that can be readily replicated on consumer hardware,\nwhile still being instructive for future larger-scale experiments, we only update the final linear-layer\nof Mpos, Wpos, in the next-token loss (Eq. 32); however, we update the full weight matrix of Wpos\nand not a lower-rank adaptor over these weights. This is instructive in this context, since our data is\nrelatively small, but with |V| = 32064 and the PHI3.5 hidden dimension of 3072, the 100 million\nparameters of Wpos would be assumed to quickly overfit, leading to degenerate output. Because\nwe only update Wpos, while θpos stays fixed, we only need to train the VERIFICATIONLAYER once\nbefore the next-token training loop begins (i.e., Line 11 in Alg. 4 is not needed), and we exclude the\nweights of Mneg as input to the SDM activation layer, since they are identical to those of Mpos. As\nsuch, the input to the SDM activation of the VERIFICATIONLAYER is\n\u0000mean(hpos), h−1\npos\n\u0001\n∈R2·3072,\nthe concatenation of the average of the final hidden states (across tokens) with the final hidden state\nthat predicts the end of sequence delimiter (here, the final closing bracket in the JSON output).\nIn this setting, our primary comparison is against the full model before fine-tuning, for which we\nuse the label PHI3.5+SDM. In this case, only the VERIFICATIONLAYER layer is trained, here for\nJ = 10 iterations of 50 epochs, but the evaluation is still over completions generated via greedy\ndecoding (i.e., arg max) over SDM(zneg, zpos) with q = e −2, d = 1. The fine-tuned model\n(PHI3.5+SDMNETWORK) uses this same VERIFICATIONLAYER, but it is also trained for 5 epochs\nwith βmin = 0 and βmax = 0.1 using the next-token loss of Alg. 4. We choose the model weights\n(as in Line 13 of Alg. 4) as those that maximize the count of admitted points over Dca via p(ˆy)lower\nfor ˆy = 1, further restricted to ytask = ˆytask, which is determined by parsing the generated JSON\n23As noted in § 4.3, y = 0 instances (here, the constructed negatives) do not participate in next-token\nfine-tuning, but they are used for training the VERIFICATIONLAYER and the support set to determine q and d.\n24Ill-formatted JSON output is treated as a wrong prediction.\n20\n\n\nTable 1: Comparison of relevant estimators for the standard document classification setting, α′ =\n0.95 . N/A indicates all predictions were rejected, which is preferred over falling under the expected\naccuracy. n = |Admitted|, the count of non-rejected documents.\nClass-conditional\nPrediction-conditional\nMarginal\ny = 0\ny = 1\nˆy = 0\nˆy = 1\ny ∈{0, 1}\nDataset\nModel\nEstimator\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nSENTIMENT\nFASTERI+ADAPTOR\nNO-REJECT\n0.982\n0.50\n0.953\n0.50\n0.955\n0.51\n0.982\n0.49\n0.968\n1.\nSENTIMENT\nFASTERI+ADAPTOR\nSOFTMAX\n0.995\n0.46\n0.983\n0.41\n0.985\n0.46\n0.994\n0.41\n0.989\n0.87\nSENTIMENT\nFASTERI+ADAPTOR\nTEMPSCALING\n0.994\n0.45\n0.986\n0.39\n0.987\n0.45\n0.994\n0.39\n0.990\n0.84\nSENTIMENT\nFASTERI+ADAPTOR\nAPS\n0.993\n0.47\n0.973\n0.45\n0.975\n0.48\n0.993\n0.44\n0.983\n0.92\nSENTIMENT\nFASTERI+ADAPTOR\nRAPS\n0.989\n0.47\n0.972\n0.44\n0.974\n0.48\n0.988\n0.44\n0.981\n0.92\nSENTIMENT\nFASTERI+SDM\nNO-REJECT\n0.971\n0.50\n0.966\n0.50\n0.966\n0.50\n0.971\n0.50\n0.968\n1.\nSENTIMENT\nFASTERI+SDM\np(ˆy)lower\n0.996\n0.32\n0.996\n0.32\n0.996\n0.32\n0.996\n0.32\n0.996\n0.65\nSENTIMENT\nFASTERI+SDM\np(ˆy)centroid\n0.996\n0.36\n0.993\n0.35\n0.993\n0.36\n0.996\n0.35\n0.995\n0.71\nSENTIMENT\nFASTERI+SDM\np(ˆy)upper\n0.997\n0.38\n0.993\n0.37\n0.993\n0.38\n0.997\n0.37\n0.995\n0.75\nSENTIMENTOOD\nFASTERI+ADAPTOR\nNO-REJECT\n0.992\n0.5\n0.394\n0.5\n0.621\n0.80\n0.979\n0.20\n0.693\n1.\nSENTIMENTOOD\nFASTERI+ADAPTOR\nSOFTMAX\n1.\n0.37\n0.251\n0.08\n0.854\n0.44\n1.\n0.02\n0.861\n0.46\nSENTIMENTOOD\nFASTERI+ADAPTOR\nTEMPSCALING\n1.\n0.34\n0.223\n0.07\n0.869\n0.39\n1.\n0.01\n0.874\n0.41\nSENTIMENTOOD\nFASTERI+ADAPTOR\nAPS\n1.000\n0.43\n0.346\n0.19\n0.770\n0.55\n0.997\n0.07\n0.795\n0.62\nSENTIMENTOOD\nFASTERI+ADAPTOR\nRAPS\n0.999\n0.43\n0.336\n0.20\n0.761\n0.56\n0.991\n0.07\n0.786\n0.63\nSENTIMENTOOD\nFASTERI+SDM\nNO-REJECT\n0.570\n0.5\n0.966\n0.5\n0.944\n0.30\n0.692\n0.70\n0.768\n1.\nSENTIMENTOOD\nFASTERI+SDM\np(ˆy)lower\nN/A\n0.\nN/A\n0.\nN/A\n0.\nN/A\n0.\nN/A\n0.\nSENTIMENTOOD\nFASTERI+SDM\np(ˆy)centroid\nN/A\n0.\nN/A\n0.\nN/A\n0.\nN/A\n0.\nN/A\n0.\nSENTIMENTOOD\nFASTERI+SDM\np(ˆy)upper\n0.\n0.06\n1.\n0.28\nN/A\n0.\n0.819\n0.35\n0.819\n0.35\nFACTCHECK\nFASTI+MIXTRAL+ADAPTOR\nNO-REJECT\n0.365\n0.51\n0.908\n0.49\n0.807\n0.23\n0.574\n0.77\n0.629\n1.\nFACTCHECK\nFASTI+MIXTRAL+ADAPTOR\nSOFTMAX\n0.211\n0.08\n0.975\n0.33\n0.667\n0.02\n0.839\n0.38\n0.828\n0.40\nFACTCHECK\nFASTI+MIXTRAL+ADAPTOR\nTEMPSCALING\n0.286\n0.06\n0.987\n0.31\n0.8\n0.02\n0.884\n0.35\n0.879\n0.37\nFACTCHECK\nFASTI+MIXTRAL+ADAPTOR\nAPS\n0.283\n0.19\n0.979\n0.38\n0.867\n0.06\n0.736\n0.51\n0.75\n0.57\nFACTCHECK\nFASTI+MIXTRAL+ADAPTOR\nRAPS\n0.341\n0.18\n0.967\n0.37\n0.833\n0.07\n0.75\n0.47\n0.761\n0.55\nFACTCHECK\nFASTI+MIXTRAL+SDM\nNO-REJECT\n0.397\n0.51\n0.899\n0.49\n0.806\n0.25\n0.585\n0.75\n0.641\n1.\nFACTCHECK\nFASTI+MIXTRAL+SDM\np(ˆy)lower\nN/A\n0.\n1.\n0.13\nN/A\n0.\n1.\n0.13\n1.\n0.13\nFACTCHECK\nFASTI+MIXTRAL+SDM\np(ˆy)centroid\nN/A\n0.\n1.\n0.17\nN/A\n0.\n1.\n0.17\n1.\n0.17\nFACTCHECK\nFASTI+MIXTRAL+SDM\np(ˆy)upper\n1.\n0.02\n0.980\n0.21\n0.8\n0.02\n1.\n0.20\n0.982\n0.22\noutput. For both models, PHI3.5+SDM and PHI3.5+SDMNETWORK, we consider the NO-REJECT\nand p(ˆy)lower estimators.25\n6\nResults\nAcross tasks and models, the SDM calibration process yields an estimator that achieves the desired\nnotion of index-conditional calibration (Def. 4.3), in contrast to the existing classes of estimators over\nLLMs, which become unreliable in the presence of even modest distribution shifts. The p(ˆy)lower\nestimator remains calibrated as the test-time distribution shifts due to the ˜q γ\nmin lower constraint\non ˜qlower, which screens points that are unlike those seen during the calibration process. With\nexisting methods, defining an out-of-distribution point has been task- and problem-specific, and\ngenerally challenging over high-dimensional inputs. In contrast, the SDM calibration process provides\na principled approach for determining such cut-offs in a data- and model-driven manner, with minimal\nhyper-parameters, resulting in a clear separation of points over which the estimator is reliable (namely,\nthe admitted points) and those over which the estimates themselves are unreliable (i.e., the rejected\npoints). The SDM network takes this behavior to its logical conclusion by incorporating it into the\nLLM architecture and fine-tuning process to serve as a universal verifier, suggesting a principled\nbasis for building large, complex LLM systems and pipelines that are both interpretable and reliable.\n6.1\nResults: Classification\nTable 1 displays the results for the binary classification tasks. The results for SENTIMENT vs. those\nof the other datasets are indicative of the under-appreciated point in the existing calibration literature\nof the importance of comparisons over—at least modest—distribution-shifts. On in-distribution\nbenchmark data with high accuracy models, the differences can be difficult to discern; after all, the\nclass-wise accuracy of the model is itself ≥α′. However, even in these otherwise straightforward\nbinary classification settings, the existing classes of estimators all but fall apart in the presence\n25In this context, p(ˆy)centroid and p(ˆy)upper are less meaningful as a comparison since p(ˆy)lower is used as\npart of the aforementioned stopping criteria for PHI3.5+SDMNETWORK, and are thus excluded.\n21\n\n\nTable 2: MAD and mˆy\n⌊˜q⌋by ⌊˜q⌋on Dca for the standard classification tasks, trained with J = 10\niterations, each of 50 epochs. As ⌊˜q⌋increases, the epistemic uncertainty decreases, and the variation\namong comparable points also decreases.\nSENTIMENT\nFACTCHECK\ny = 0\ny = 1\ny = 0\ny = 1\n⌊˜q⌋\nMAD\nm0\n⌊˜q⌋\nMAD\nm1\n⌊˜q⌋\nMAD\nm0\n⌊˜q⌋\nMAD\nm1\n⌊˜q⌋\n0\n0.007\n0.044\n0.007\n0.044\n0.024\n0.148\n0.018\n0.116\n1\n< 0.001\n0.006\n< 0.001\n0.003\n0.009\n0.056\n0.004\n0.024\n2\n< 0.001\n< 0.001\n< 0.001\n< 0.001\n0.003\n0.021\n0.001\n0.004\n3\n< 0.001\n< 0.001\n< 0.001\n< 0.001\n< 0.001\n0.004\n< 0.001\n0.002\n4\n0.\n0.\n0.\n0.\n< 0.001\n0.001\n< 0.001\n< 0.001\n5\n0.\n0.\n0.\n0.\n< 0.001\n< 0.001\n< 0.001\n< 0.001\n6\n0.\n0.\n0.\n0.\n< 0.001\n< 0.001\n0.\n0.\n7\n0.\n0.\n0.\n0.\n-\n-\n-\n-\nTable 3: ˜q γ\nmin on Dca for the standard multi-class classification experiments. The more challenging\nFACTCHECK task has a commensurately higher ˜q γ\nmin.\nSENTIMENT\nFACTCHECK\nMAD\n˜q γ\nmin\nMAD\n˜q γ\nmin\n7.9e-05\n1.004\n0.100\n2.447\nof distribution shifts, which are common in practice with high-dimensional data, such as text. In\nthis light, the existing classes of estimators are not demonstrably more effective than simply using\nan un-calibrated threshold on the output (SOFTMAX). In contrast, the p(ˆy)lower estimator achieves\nindex-conditional calibration in all cases, correctly rejecting documents over which the estimates are\nunreliable, and admitting points for which the class- and prediction-conditional accuracies are ≥α′.\nCentral to the unique behavior of the SDM estimator is that the epistemic uncertainty decreases as\n˜q increases. Furthermore, ⌊˜q⌋can be used as a mapping between Dca and a new, unseen test point,\nbecause the variation among comparable points also decreases as ˜q increases. Table 2 shows this\nfor the standard multi-class classification tasks with summary statistics over the J = 10 iterations.\nThe corresponding ˜q γ\nmin used by the p(ˆy)lower estimator (Eq. 28) appears in Table 3. Comparing\nthese ˜q γ\nmin values with Table 2 makes it clear that the ˜q γ\nmin values are effectively change points w.r.t.\nthe epistemic uncertainty: Points below have high variation and points above have increasingly low\nvariation to the point that mˆy\n⌊˜q⌋reaches 0, within numerical error.\nThis behavior is remarkable for an estimator over high-dimensional inputs, because it demonstrates\nthere are regions of the distribution that are low variation and high-probability that can be reliably\ndetected. Existing estimators marginalize over the distinctions in these regions, which can cause\nunexpected behavior at test-time, as demonstrated in our empirical results.\n6.2\nResults: Black-box LLM APIs\nTable 4 contains the results of the estimators over GPT-4O, the baseline accuracy of which\nis in-line with existing reported results for the zero-shot setting, and GPT-4O+SDM.\nNeither\nANSWERSTRINGPROB nor VERBALIZEDPROB are reliable estimators across these datasets, even\nthough the multiple-choice QA task is a common setting for LLM development and evaluation.\nConceptually, both can be viewed as encoding the output MAGNITUDE, without explicitly controlling\nfor the SIMILARITY and DISTANCE, as with a SOFTMAX estimator in a standard classification setting.\nTheir over-confidence on MMLU-PRO-4QA reflect this.\nThe results of p(ˆy)lower on MMLU-PRO-4QA are indicative of the real-world use of the SDM\nestimator. GPT-4O has a dramatically lower overall accuracy on the MMLU-PRO-4QA questions,\nwhich would come as a surprise to an end-user who was expecting behavior similar to that over\n22\n\n\nTable 4: Comparison of relevant estimators combined with GPT-4O, α′ = 0.95 . The SDM estimator,\np(ˆy)lower, remains well-calibrated even over the much more challenging MMLU-PRO-4QA dataset.\nImportantly, p(ˆy)lower is not vacuously conservative; the yield of admitted points is higher on MMLU\neven when the verbalized uncertainty of GPT-4O is well-calibrated (see underline).\nDataset\nModel\nEstimator\nAcc.\n|Admitted|\n|Dte|\nMMLU\nGPT-4O\nNO-REJECT\n0.832\n1.\nMMLU\nGPT-4O\nANSWERSTRINGPROB\n0.921\n0.74\nMMLU\nGPT-4O\nVERBALIZEDPROB\n0.953\n0.35\nMMLU\nGPT-4O+SDM\nNO-REJECT\n0.835\n1.\nMMLU\nGPT-4O+SDM\np(ˆy)lower\n0.957\n0.38\nMMLU\nGPT-4O+SDM\np(ˆy)centroid\n0.956\n0.39\nMMLU\nGPT-4O+SDM\np(ˆy)upper\n0.954\n0.41\nMMLU-PRO-4QA\nGPT-4O\nNO-REJECT\n0.648\n1.\nMMLU-PRO-4QA\nGPT-4O\nANSWERSTRINGPROB\n0.870\n0.51\nMMLU-PRO-4QA\nGPT-4O\nVERBALIZEDPROB\n0.857\n0.16\nMMLU-PRO-4QA\nGPT-4O+SDM\nNO-REJECT\n0.683\n1.\nMMLU-PRO-4QA\nGPT-4O+SDM\np(ˆy)lower\n0.958\n0.22\nMMLU-PRO-4QA\nGPT-4O+SDM\np(ˆy)centroid\n0.957\n0.23\nMMLU-PRO-4QA\nGPT-4O+SDM\np(ˆy)upper\n0.942\n0.24\nTable 5: Verified generation results, α′ = 0.95 . Task datasets are identical to those in Table 1.\nPredictions are parsed from the JSON generated by the model, with parsing errors counted as wrong\npredictions. N/A indicates all predictions were rejected, which is preferred over falling under the\nexpected accuracy. Verification via an SDM estimator is reliable regardless of fine-tuning the model,\nbut fine-tuning with SDM (PHI3.5+SDMNETWORK) can increase the task accuracy (see bold) and\nthe yield of admitted points (see underline).\nDataset\nModel\nEstimator\nACC.\n|Admitted|\n| Dte |\nSENTIMENT\nPHI3.5+SDM\nNO-REJECT\n0.751\n1.\nSENTIMENT\nPHI3.5+SDM\np(ˆy)lower\n0.997\n0.39\nSENTIMENT\nPHI3.5+SDMNETWORK\nNO-REJECT\n0.876\n1.\nSENTIMENT\nPHI3.5+SDMNETWORK\np(ˆy)lower\n0.996\n0.42\nSENTIMENTOOD\nPHI3.5+SDM\nNO-REJECT\n0.815\n1.\nSENTIMENTOOD\nPHI3.5+SDM\np(ˆy)lower\n1.\n<0.01\nSENTIMENTOOD\nPHI3.5+SDMNETWORK\nNO-REJECT\n0.896\n1.\nSENTIMENTOOD\nPHI3.5+SDMNETWORK\np(ˆy)lower\n1.\n<0.01\nFACTCHECK\nPHI3.5+SDM\nNO-REJECT\n0.706\n1.\nFACTCHECK\nPHI3.5+SDM\np(ˆy)lower\n0.973\n0.15\nFACTCHECK\nPHI3.5+SDMNETWORK\nNO-REJECT\n0.743\n1.\nFACTCHECK\nPHI3.5+SDMNETWORK\np(ˆy)lower\n0.973\n0.15\nMMLU. In contrast, the p(ˆy)lower estimator remains calibrated. For the rejected documents, the\nuser would then know to take additional action. Alternatively, if part of an automated pipeline,\nadditional test-time compute-based branching decisions (such as re-asking the model, or seeking\noutside information via retrieval) could be taken in the background before presenting a final result.\nData Quality Analysis.\nFor MMLU-PRO-4QA, we examine the 5 questions in the Computer\nScience category that were in the p(ˆy)lower index-conditional admitted set, but for which the predicted\nanswers do not match the ground-truth annotations, y ̸= ˆy. The top 4 questions sorted by p(ˆy)lower,\nall of which have p(ˆy)lower ≥0.99, all clearly have annotation errors where the model predictions\nare correct and the ground-truth annotations are incorrect. We include the question id’s in Table 6.\nThis provides an exogenous evaluation of the method: The SDM estimator has successfully separated\nthe aleatoric and epistemic uncertainty among the high-probability predictions.\n23\n\n\n6.3\nResults: Verified Generation\nThe results for the SDM network indicate effective verification of instruction following (Table 5).\nOur small-scale experiment confirms that the VERIFICATIONLAYER reliably yields a calibrated\nestimator regardless of fine-tuning, but the fine-tuning process improves overall task accuracy. That\nis, the results confirm that Alg. 4, which chose epoch 3 of 5 as the final model, is a viable fine-\ntuning loss and process. Importantly in this context, the cardinality of the set of admitted points is\nnon-decreasing relative to before fine-tuning, despite updating 100 million parameters on a small\ntraining set. Leveraging the behavior of the SDM estimator, the SDM network is, in this way, the first\nstatistically principled and robust approach to construct an LLM with an intrinsic ability to verify its\nown instruction-following and generated output.\n7\nConclusion\nWith the discovery that multi-layer networks can encode hierarchical language structures without\nexplicit human annotations [39], there has been renewed interest in deep learning as a focus of\nresearch for language modeling over the last decade, and a principled basis for scaling data and model\ncompute. However, brittleness to distribution shifts and lack of reliable uncertainty quantification have\nprecluded—or otherwise greatly diminished the potential of—the use of neural network language\nmodels in most real-world settings. In this work, we have resolved these remaining foundational\nlimitations by introducing SDM activation functions, SDM calibration, and SDM networks.\n24\n\n\nA\nAppendix\nWe provide additional experimental details and results for the black-box LLM API experiments in\n§ A.1 and the verified generation experiments in § A.2. Additional training details are included in\n§ A.3.\nCode to replicate our results is available at the URL provided in the main text. For the reader, we\nprovide a few key highlights here. We include an implementation of the SDM activation function in\n§ A.4. We provide our conventions for calculating empirical CDFs in § A.5, and we provide code\nscaffolding for an example implementation of an SDM network training loop in § A.6.\nA.1\nBlack-box LLM APIs\nThe results of the data quality analysis are included in Table 6. Following best practices, to avoid\ncontaminating the test set since research articles are commonly used for LLM training, we only\ninclude the question id’s and not the question and answer text, which can readily be retrieved from\nthe Huggingface datasets database.\nWe include the prompts used for the experiments in the code repo. The prompt is a variation on the\ntheme of that used in OpenAI’s Simple Evals repo26, with the addition of using structured outputs\nagainst the JSON Schema in Listing 1. The particular prompt and structuring of the JSON (and\nparsing of the JSON, described below) are not defining aspects of the approach and are not necessarily\nthe optimal templates. We use a direct, zero-shot approach to examine the more challenging setting—\narguably closer to real-world usage—than providing examples or systematically hill-climbing on\nprompts.\nThe embedding for input to the SDM activation layer is constructed by parsing the JSON schema\nmapped back to the top-1 probabilities of the output tokens. For each key, we average the log-\nprobabilities in probability space of the tokens of the corresponding value. For example, for the\nkey \"short_explanation_for_answer_confidence\", we parse the output to isolate the tokens\ncorresponding to the value, and take the average of the exponentiated log probabilities of the tokens.\nGiven the 3 keys in the JSON schema, this results in 3 floating-point values. (The verbalized\nuncertainty object \"confidence_in_answer_letter\" has a value of type number, but the output\nitself corresponds to a sequence of discrete tokens (e.g., “0”, “.”, “9”), so this parsing process is the\nsame as that for the values of type string.) Finally, we construct a soft one-hot vector of length\n4 where the non-zero index (if any) of the predicted letter is set to the floating-point value of the\nverbalized uncertainty (i.e., the value of the object with key \"confidence_in_answer_letter\").\nThe input embedding is then the concatenation of these 7 values. Full refusals from the LLM’s API,\nwhich are rare but can occur on some of the social science and humanities questions, are assigned\nvectors of 0’s as embeddings for Dtr and Dca instances, and treated as wrong predictions in the test\nevaluations.\nThe estimator ANSWERSTRINGPROB corresponds to the index of this embedding derived from the\nvalue of the object with key \"answer_letter\". Often this is the probability of the single token\n(i.e., “A”, “B”, “C”, “D”), but occasionally will be the average over additional tokens (e.g., “$”). The\nestimator VERBALIZEDPROB corresponds to the floating-point value of the verbalized uncertainty.\nIn our experiments, we aim for a controlled comparison with ANSWERSTRINGPROB and\nVERBALIZEDPROB; as such, the SDM activation layer is only given access to the 7 values above.\nIn particular, we do not provide access to additional signal derived from composition with another\nmodel. In applications where the uncertainty is over multiple tasks (i.e., not just question answering\nof this particular format), to avoid a marginalization over tasks, we recommend either encoding\nthe distinction across tasks in the JSON schema, or simply concatenating the LLM output with the\nhidden states of another large model. The latter is typically readily achievable by running another\nopen-source model alongside the black-box LLM’s API.\nWe train the SDM activation layer as a 4-class classification task, which is an effective but potentially\nsample-inefficient encoding, at least when assuming the absence of artifacts correlated with answer\nletters. An alternative would be to re-encode the task as binary classification, either as a leave-one-out\nclassification or as binary verification (as in § 5.3). Since the choice of encoding, as with the structure\n26https://github.com/openai/simple-evals/blob/main/common.py\n25\n\n\nTable 6: MMLU-PRO-4QA, Computer Science category. Predictions that met the index-conditional\nthreshold but were marked incorrect according to the ground-truth labels. Examination of the data\nreveals the model is correct and the ground-truth annotations are incorrect . The digit significance\nof p(ˆy) is not necessarily significant (and when shown to users, would typically be rounded, with a\ntop ceiling to avoid 1.0), but provided for reference. ˆnˆy is the effective sample size for the predicted\nclass. The final question is arguably ambiguous.\nQuestion ID\ny\nˆy\np(ˆy)lower\np(ˆy)centroid\np(ˆy)upper\nˆnˆy\n10750\nA\nD\n0.9999999029119694\n0.999999946869715\n0.999999963752475\n11563\n10682\nD\nC\n0.9999995410050875\n0.9999997504521737\n0.9999998413548795\n11774\n10458\nD\nA\n0.9997548501324156\n0.9998610348657851\n0.9999170919091862\n9129\n10533\nB\nC\n0.9897059289074643\n0.9936086673736274\n0.9957749405311342\n6891\n10479\nD\nB\n0.967751071803557\n0.9791966686070331\n0.9862756083406558\n7684\nof the prompt and JSON Schema, is orthogonal to the evaluation of the uncertainty estimates—other\nthan with respect to effective sample sizes—we keep these aspects straightforward in this set of\nexperiments to avoid complicating the presentation.\nGiven the results in the main text, a logical next step would be to use this behavior to build a re-ask\npipeline. That is, predictions with low probability can be automatically routed to re-prompt the LLM\nconditional on the previous response, a potentially effective means of building test-time compute\nsystems over otherwise black-box models. Such pipelines are not feasible without robust estimates of\npredictive uncertainty, but become conceptually straightforward—and straightforward to implement—\ngiven the behavior of SDM estimators. We leave such additional applied examples for future work to\nsystematically analyze.27\nListing 1: JSON Schema for GPT-4O Structured Outputs.\n{\n\"properties\": {\n\"answer_letter\": {\n\"title\": \"Answer Letter\",\n\"type\": \"string\"\n},\n\"confidence_in_answer_letter\": {\n\"title\": \"Confidence In Answer Letter\",\n\"type\": \"number\"\n},\n\"short_explanation_for_answer_confidence\": {\n\"title\": \"Short Explanation For Answer Confidence\",\n\"type\": \"string\"\n}\n},\n\"required\": [\n\"answer_letter\",\n\"confidence_in_answer_letter\",\n\"short_explanation_for_answer_confidence\"\n],\n\"title\": \"MultipleChoiceQuestionResponse\",\n\"type\": \"object\"\n}\nA.2\nVerified Generation\nFor reference, Table 7 provides the effectiveness over the force-decoded datasets. The support set of\nthe VERIFICATIONLAYER is constructed from the force-decoded training and calibration data, so this\ntable reflects the held-out classification ability over the verification data, which includes constructed\nnegatives for yverification = 0, as described in the main text and illustrated in Table 8. Listing 2\nincludes the system message and prompts used for the experiments.\n27An example of this behavior for generative AI-assisted search using an earlier, simplified version of the\napproach in the main text is available at https://www.youtube.com/watch?v=awA_suZ13XE.\n26\n\n\nTable 7: Verification results on the force-decoded test sets for reference, α′ = 0.95 . See Table 5\nfor generation results for the underlying tasks, which reflect real test-time usage. N/A indicates\nall predictions were rejected, which is preferred over falling under the expected accuracy. n =\n|Admitted|, the count of non-rejected documents. Additional resolution added to\nn\n| Dte | columns for\nSENTIMENTOODVERIFICATION for reference, but the number of admitted points is effectively 0.\nClass-conditional\nPrediction-conditional\nMarginal\ny = 0\ny = 1\nˆy = 0\nˆy = 1\ny ∈{0, 1}\nDataset\nModel\nEstimator\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nACC.\nn\n| Dte |\nSENTIMENTVERIFICATION\nPHI3.5+SDM\nNO-REJECT\n0.959\n0.51\n0.891\n0.49\n0.901\n0.54\n0.954\n0.46\n0.925\n1.\nSENTIMENTVERIFICATION\nPHI3.5+SDM\np(ˆy)lower\n0.996\n0.17\n0.997\n0.21\n0.996\n0.17\n0.997\n0.21\n0.997\n0.38\nSENTIMENTVERIFICATION\nPHI3.5+SDM\np(ˆy)centroid\n0.996\n0.18\n0.997\n0.22\n0.996\n0.18\n0.997\n0.22\n0.997\n0.40\nSENTIMENTVERIFICATION\nPHI3.5+SDM\np(ˆy)upper\n0.997\n0.19\n0.997\n0.23\n0.997\n0.19\n0.997\n0.23\n0.997\n0.42\nSENTIMENTOODVERIFICATION\nPHI3.5+SDM\nNO-REJECT\n0.978\n0.51\n0.639\n0.49\n0.738\n0.68\n0.966\n0.32\n0.812\n1.\nSENTIMENTOODVERIFICATION\nPHI3.5+SDM\np(ˆy)lower\n1.\n0.002\n1.\n0.0002\n1.\n0.002\n1.\n0.0002\n1.\n0.002\nSENTIMENTOODVERIFICATION\nPHI3.5+SDM\np(ˆy)centroid\n1.\n0.003\n1.\n0.0002\n1.\n0.003\n1.\n0.0002\n1.\n0.003\nSENTIMENTOODVERIFICATION\nPHI3.5+SDM\np(ˆy)upper\n1.\n0.003\n1.\n0.0004\n1.\n0.003\n1.\n0.0004\n1.\n0.004\nFACTCHECKVERIFICATION\nPHI3.5+SDM\nNO-REJECT\n0.656\n0.50\n0.732\n0.50\n0.708\n0.46\n0.682\n0.54\n0.694\n1.\nFACTCHECKVERIFICATION\nPHI3.5+SDM\np(ˆy)lower\nN/A\n0.\n1.\n0.07\nN/A\n0.\n1.\n0.07\n1.\n0.07\nFACTCHECKVERIFICATION\nPHI3.5+SDM\np(ˆy)centroid\nN/A\n0.\n1.\n0.08\nN/A\n0.\n1.\n0.08\n1.\n0.08\nFACTCHECKVERIFICATION\nPHI3.5+SDM\np(ˆy)upper\nN/A\n0.\n1.\n0.08\nN/A\n0.\n1.\n0.08\n1.\n0.08\nTable 8: JSON structure for the verified generation experiments, with Mref = PHI3.5. yverification =\n1 corresponds to the standard classification tasks, where, e.g., ytask = 0 corresponds to a negative\nreview for the sentiment task, and ytask = 1 corresponds to a factually correct statement for the\nfactcheck task. yverification = 0 flips the parity, and is used for constructing negatives for training,\nand the contrastive basis for rejection at test-time. Recall that the LLM takes as input a system\nprompt, user prompt, and the document (see Listing 2). At test-time, we seek to generate the correct\nJSON output (i.e., that corresponding to the correct ytask label), for instances with ˆyverification = 1\npredicted by the VERIFICATIONLAYER layer.\nDatasets\nLabels\nJSON output\nSENTIMENT, SENTIMENTVERIFICATION\nSENTIMENTOOD, SENTIMENTOODVERIFICATION\nytask = 0, yverification = 1\n{\"sentiment\":\n\"negative\"}\nytask = 1, yverification = 1\n{\"sentiment\":\n\"positive\"}\nytask = 0, yverification = 0\n{\"sentiment\":\n\"positive\"}\nytask = 1, yverification = 0\n{\"sentiment\":\n\"negative\"}\nFACTCHECK, FACTCHECKVERIFICATION\nytask = 0, yverification = 1\n{\"correctness\":\nfalse}\nytask = 1, yverification = 1\n{\"correctness\":\ntrue}\nytask = 0, yverification = 0\n{\"correctness\":\ntrue}\nytask = 1, yverification = 0\n{\"correctness\":\nfalse}\nListing 2: System and user messages for the sentiment and factcheck datasets of the verified\ngeneration experiments, with Mref = PHI3.5. The document text replaces TEXT for each instance.\n<|system|>\nYou are a helpful AI assistant.<|end|>\n<|user|>\nClassify the sentiment of the following movie review. Respond using the following JSON: {\"sentiment\": str}. REVIEW: TEXT<|\nend|>\n<|assistant|>\n<|system|>\nYou are a helpful AI assistant.<|end|>\n<|user|>\nCheck the following document for hallucinations and/or factual inaccuracies. Respond using the following JSON: {\"correctness\n\": bool}. DOCUMENT: TEXT<|end|>\n<|assistant|>\nA.3\nAdditional Training Details\nCompute.\nThe black-box LLM experiments require API calls, as detailed in the main text, but all\nother results can be reproduced locally on a single 2023 Mac Studio with an M2 Ultra chip with 128\n27\n\n\nGB of unified memory. These experiments are designed to fully assess the methods while still being\nreplicable with consumer hardware.\nHyper-parameters.\nIn the code repo, we include scripts for replicating our results. For all cases,\nwe train the rescaling transform (Alg. 2) for up to 1000 epochs, with early stopping if the loss exceeds\nthe min observed loss for 10 consecutive epochs. In all experiments, M = 1000 and we use a\nmini-batch size of 50. We mean center the input to g, the 1-D CNN of the SDM activation layer,\nvia the mean and standard deviation over Dtr. We train GPT-4O+SDM for J = 10 iterations of 5\nepochs, and the SDM models of SENTIMENT and FACTCHECK, as well as the VERIFICATIONLAYER\nof the SDM network, for J = 10 iterations of 50 epochs. The standard exemplar adaptors of the\nSENTIMENT and FACTCHECK classification experiments are trained with cross-entropy losses for 50\nepochs. We use the Adam optimizer [24] with a learning rate of 1 × 10−4 for training the rescaling\ntransform (Alg. 2) and 1 × 10−5 for all other cases.\nA.4\nExample Implementation of the SDM Activation Function\nWe include an implementation of the SDM activation function using PyTorch [33], version 2.3.0, in\nListing 3.\nListing 3: Implementation of the SDM activation function in PyTorch, version 2.3.0.\ndef sdm_activation_function(batch_input, q, distance_quantile_per_class=None, log=False):\n\"\"\"\nsdm activation function\nParameters\n----------\nbatch_input\ntorch.tensor\nshape == [batch size, number of classes]\nq\ntorch.tensor\nshape == [batch size, 1], with each value in [0, max q]\ndistance_quantile_per_class\ntorch.tensor, or None\nIf not None, shape == [batch size, number of classes], with each quantile in [0,1]. As a final layer\nactivation function, with batch_input $\\in \\reals$, it is assumed that the quantiles are the same\nacross classes, for a given instance. This ensures the argmax does not change relative to\ntorch.argmax(batch_input, dim=1).\nlog\nlog with change of base, for training\nNotes:\nFor context, with e.g. batch size = 1, the standard softmax is obtained by using q=torch.tensor([[torch.e-2]])\nand (distance_quantile_per_class=None or distance_quantile_per_class=torch.ones(1, number of classes) ).\nReturns\n-------\n[batch size, number of classes]\n\"\"\"\nassert len(batch_input.shape) == 2\nassert batch_input.shape[0] == q.shape[0]\nassert q.shape[1] == 1\nif distance_quantile_per_class is not None:\nassert batch_input.shape == distance_quantile_per_class.shape\nq_rescale_offset = 2\nq_factor = q_rescale_offset + q\nbatch_input = batch_input - torch.amax(batch_input, dim=1, keepdim=True) # for numerical stability\nif distance_quantile_per_class is not None:\nrescaled_distribution = q_factor ** (batch_input * distance_quantile_per_class)\nelse:\nrescaled_distribution = q_factor ** batch_input\nif log: # log_base{q}\nkEPS = torch.finfo(torch.float32).eps # adjust as applicable for platform\nrescaled_distribution = torch.log(rescaled_distribution + kEPS) - torch.log(\ntorch.sum(rescaled_distribution, dim=1) + kEPS).unsqueeze(1)\nreturn rescaled_distribution / torch.log(q_factor)\nelse:\nreturn rescaled_distribution / torch.sum(rescaled_distribution, dim=1).unsqueeze(1)\n28\n\n\nA.5\nEmpirical CDF Function\nListing 4: An implementation of the empirical CDF conventions used in this work, using NumPy,\nversion 1.26.4. See the text for a further discussion.\ndef getCDFIndex(trueClass_To_CDF, val, prediction, reverse=False, val_in_0to1=False):\n# trueClass_To_CDF is a dictionary with a key for each class, the values of which are sorted ascending lists of\nnumbers, since np.searchsorted assumes an ascending sort of its initial argument.\nif prediction not in trueClass_To_CDF or len(trueClass_To_CDF[prediction]) == 0:\nreturn 0.0\nif val_in_0to1 and len(trueClass_To_CDF[prediction]) > 0 and val >= trueClass_To_CDF[prediction][-1]: # saturation\nguard\nassert not reverse\nreturn 1.0\nindex = np.searchsorted(trueClass_To_CDF[prediction], val, side=\"left\") # will be 0 for len() == 0\nif reverse: # use for distances\nreturn 1 - index / len(trueClass_To_CDF[prediction])\nelse:\nreturn index / len(trueClass_To_CDF[prediction])\nThe conventions for implementing the empirical CDF functions follow in the expected ways, but we\nbriefly highlight the key considerations below, as they can impact the behavior of the estimators. An\nimplementation in NumPy [19], version 1.26.4, appears in Listing 4.\n1. The distance quantiles should be exclusionary at the boundaries. When dnearest = 0, the\n1 −eCDF·\nca(dnearest) quantile should be 1, and when dnearest is greater than the maximum\nobserved distance (across Dca for x ∈Dte and x ∈Dca, and across Dtr for x ∈Dtr, the\nlatter case only occurring during training), the 1 −eCDF·\nca(dnearest) quantile should be 0.\n2. For the quantiles over an SDM activation, as needed for calibration, saturated values at the\nhigh-end should be assigned a quantile of 1. In the example code, this is achieved by setting\nthe argument val_in_0to1=True.\n29\n\n\nA.6\nExample Implementation of the Negative+Positive Vocabulary Normalization and L2\nRegularization Term\nThe positive+negative vocabulary normalization and regularization loss (Eq. 32) are conceptually\nparsimonious and straightforward to implement. Code scaffolding for an example implementation of\nan SDM network training loop appears in Listing 5. For computational expediency, here (as in the\nexperiments in the main text), the q values and distance quantiles are calculated after each epoch,\nalthough in principle, they can be calculated with updated network values as an epoch progresses.\nListing 5: Code scaffolding in PyTorch, version 2.3.0, for a basic training loop of an SDM network\nwith the Negative+Positive Vocabulary Normalization and L2 regularization term, where the q values\nand distance quantiles are updated after each epoch.\npdist = nn.PairwiseDistance(p=2)\ncriterion = nn.NLLLoss()\nfor e in range(total_epochs):\ntotal_mini_batches = len(range(0, train_size, batch_size))\nbeta = min_beta\nbeta_step = (max_beta-min_beta) / total_mini_batches\nfor i in range(0, train_size, batch_size):\noptimizer.zero_grad()\nmodel.train()\nbatch_genai_y = # the next-token labels with applicable index+|V| offsets\n# the sdm activations for the negative+positive joint distribution and the concatenation of the reference\n# distribution with itself use the same q and distance quantiles for the corresponding instances:\nbatch_f_genai = # log_base{q} sdm activation(negative+positive linear layers output), where + is pseudo-code for\nconcatenation\nbatch_f_original = # log_base{q} sdm activation(reference distribution+reference distribution linear layers output)\nwith torch.no_grad():\ntop_events_k = 1\ntop_k_sort_by_largest = True\n# \"negative\" refers to indexes in the first half of the concatenated distributions, [0, |V|); \"positive\" to the\nsecond half [|V|, |V|*2):\nneg_original_max_half_distribution_i = torch.topk(batch_f_original[:, 0:model.gen_ai_vocab],\ntop_events_k, dim=1, largest=top_k_sort_by_largest)[1]\npos_original_max_half_distribution_i = torch.topk(batch_f_original[:, -model.gen_ai_vocab:],\ntop_events_k, dim=1, largest=top_k_sort_by_largest)[1] + model.\ngen_ai_vocab # note the offset\nnegative_max_half_distribution_i = torch.topk(batch_f_genai[:, 0:model.gen_ai_vocab],\ntop_events_k, dim=1, largest=top_k_sort_by_largest)[1]\npositive_max_half_distribution_i = torch.topk(batch_f_genai[:, -model.gen_ai_vocab:],\ntop_events_k, dim=1, largest=top_k_sort_by_largest)[1] + model.\ngen_ai_vocab # note the offset\ndistribution_mass_mask = (\ntorch.ones_like(batch_f_genai).scatter_(1, neg_original_max_half_distribution_i, 0.0) *\ntorch.ones_like(batch_f_genai).scatter_(1, pos_original_max_half_distribution_i, 0.0) *\ntorch.ones_like(batch_f_genai).scatter_(1, negative_max_half_distribution_i, 0.0) *\ntorch.ones_like(batch_f_genai).scatter_(1, positive_max_half_distribution_i, 0.0) *\ntorch.ones_like(batch_f_genai).scatter_(1, batch_genai_y.unsqueeze(1), 0.0)\n).to(batch_f_genai.device)\nregularization_term = pdist(\ndistribution_mass_mask * batch_f_original,\ndistribution_mass_mask * batch_f_genai).mean()\nllm_loss = criterion(batch_f_genai, batch_genai_y)\nwith torch.no_grad(): # rescaling factor for the regularization term\nregularization_scale_term = (torch.log(llm_loss + model.kEPS) /\n(torch.log(regularization_term + model.kEPS) + model.kEPS)\n).item()\nloss = llm_loss + beta * torch.sqrt(\ntorch.clamp(regularization_term, min=1.0) ** min(max(regularization_scale_term, 0.0), 1.0))\nloss.backward()\noptimizer.step()\nbeta += beta_step\n# Before the next epoch, for each training instance, update q and distance quantiles using the sdm activation layer\ntrained for verification.\n30\n\n\nReferences\n[1] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari,\nJ. Bao, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, M. Cai, Q. Cai, V. Chaudhary,\nD. Chen, D. Chen, W. Chen, Y.-C. Chen, Y.-L. Chen, H. Cheng, P. Chopra, X. Dai, M. Dixon,\nR. Eldan, V. Fragoso, J. Gao, M. Gao, M. Gao, A. Garg, A. D. Giorno, A. Goswami, S. Gu-\nnasekar, E. Haider, J. Hao, R. J. Hewett, W. Hu, J. Huynh, D. Iter, S. A. Jacobs, M. Javaheripi,\nX. Jin, N. Karampatziakis, P. Kauffmann, M. Khademi, D. Kim, Y. J. Kim, L. Kurilenko, J. R.\nLee, Y. T. Lee, Y. Li, Y. Li, C. Liang, L. Liden, X. Lin, Z. Lin, C. Liu, L. Liu, M. Liu, W. Liu,\nX. Liu, C. Luo, P. Madan, A. Mahmoudzadeh, D. Majercak, M. Mazzola, C. C. T. Mendes,\nA. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker, T. Portet, R. Pryzant,\nH. Qin, M. Radmilac, L. Ren, G. de Rosa, C. Rosset, S. Roy, O. Ruwase, O. Saarikivi, A. Saied,\nA. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, Y. Shen, S. Shukla, X. Song, M. Tanaka,\nA. Tupini, P. Vaddamanu, C. Wang, G. Wang, L. Wang, S. Wang, X. Wang, Y. Wang, R. Ward,\nW. Wen, P. Witte, H. Wu, X. Wu, M. Wyatt, B. Xiao, C. Xu, J. Xu, W. Xu, J. Xue, S. Yadav,\nF. Yang, J. Yang, Y. Yang, Z. Yang, D. Yu, L. Yuan, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang,\nY. Zhang, Y. Zhang, Y. Zhang, and X. Zhou. Phi-3 technical report: A highly capable language\nmodel locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219.\n[2] A. N. Angelopoulos, S. Bates, M. Jordan, and J. Malik. Uncertainty Sets for Image Classifiers\nusing Conformal Prediction. In International Conference on Learning Representations, 2021.\nURL https://openreview.net/forum?id=eNdiU_DbM9.\n[3] A. Azaria and T. Mitchell. The internal state of an LLM knows when it’s lying. pages\n967–976, Singapore, Dec. 2023. doi: 10.18653/v1/2023.findings-emnlp.68. URL 2023.\nfindings-emnlp.68.\n[4] G. W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather\nReview, 78(1):1 – 3, 1950.\ndoi: 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;\n2.\nURL https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_\n1950_078_0001_vofeit_2_0_co_2.xml.\n[5] C. K. Chow. An optimum character recognition system using decision functions. IRE Transac-\ntions on Electronic Computers, EC-6(4):247–254, 1957. doi: 10.1109/TEC.1957.5222035.\n[6] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani,\nS. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros,\nM. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai,\nH. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei.\nScaling instruction-finetuned language models, 2022.\n[7] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information\nTheory, 13(1):21–27, 1967. doi: 10.1109/TIT.1967.1053964.\n[8] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional\nnetworks. In Proceedings of the 34th International Conference on Machine Learning - Volume\n70, ICML’17, page 933–941. JMLR.org, 2017.\n[9] A. P. Dawid.\nThe well-calibrated bayesian.\nJournal of the American Statistical Asso-\nciation, 77(379):605–610, 1982.\ndoi: 10.1080/01621459.1982.10477856.\nURL https:\n//www.tandfonline.com/doi/abs/10.1080/01621459.1982.10477856.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,\nProceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-\npers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423/.\n[11] L. Devroye, L. Györfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. In\nStochastic Modelling and Applied Probability, 1996.\n31\n\n\n[12] A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic Minimax Character of the Sample\nDistribution Function and of the Classical Multinomial Estimator. The Annals of Mathematical\nStatistics, 27(3):642 – 669, 1956. doi: 10.1214/aoms/1177728174. URL https://doi.org/\n10.1214/aoms/1177728174.\n[13] R. Foygel Barber, E. J. Candès, A. Ramdas, and R. J. Tibshirani. The limits of distribution-\nfree conditional predictive inference.\nInformation and Inference: A Journal of the IMA,\n10(2):455–482, 08 2020.\nISSN 2049-8772.\ndoi: 10.1093/imaiai/iaaa017.\nURL https:\n//doi.org/10.1093/imaiai/iaaa017.\n[14] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncer-\ntainty in deep learning. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The\n33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine\nLearning Research, pages 1050–1059, New York, New York, USA, 20–22 Jun 2016. PMLR.\nURL https://proceedings.mlr.press/v48/gal16.html.\n[15] Y. Geifman and R. El-Yaniv. Selective classification for deep neural networks. In I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, edi-\ntors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,\nInc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\n4a8423d5e91fda00bb7e46540e2b0cf1-Paper.pdf.\n[16] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On Calibration of Modern Neural Networks. In\nProceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,\npages 1321–1330. JMLR.org, 2017.\n[17] C. Gupta and A. Ramdas.\nTop-label calibration and multiclass-to-binary reductions.\nIn\nInternational Conference on Learning Representations, 2022. URL https://openreview.\nnet/forum?id=WqoBaaPHS-.\n[18] A. Hannun, J. Digani, A. Katharopoulos, and R. Collobert. MLX: Efficient and flexible machine\nlearning on apple silicon, 2023. URL https://github.com/ml-explore.\n[19] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau,\nE. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk,\nM. Brett, A. Haldane, J. F. del Río, M. Wiebe, P. Peterson, P. Gérard-Marchant, K. Sheppard,\nT. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with\nNumPy. Nature, 585(7825):357–362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL\nhttps://doi.org/10.1038/s41586-020-2649-2.\n[20] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. Proceedings of the International Conference on\nLearning Representations (ICLR), 2021.\n[21] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780,\nNov. 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.\n1162/neco.1997.9.8.1735.\n[22] J. T. G. Hwang and A. A. Ding. Prediction intervals for artificial neural networks. Journal\nof the American Statistical Association, 92(438):748–757, 1997. ISSN 01621459. URL\nhttp://www.jstor.org/stable/2965723.\n[23] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot,\nD. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud,\nL. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao,\nT. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of experts, 2024. URL\nhttps://arxiv.org/abs/2401.04088.\n[24] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017. URL https:\n//arxiv.org/abs/1412.6980.\n[25] M. Kull, M. Perello-Nieto, M. Kängsepp, T. S. Filho, H. Song, and P. Flach. Beyond Temperature\nScaling: Obtaining Well-Calibrated Multiclass Probabilities with Dirichlet Calibration. Curran\nAssociates Inc., Red Hook, NY, USA, 2019.\n32\n\n\n[26] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty\nestimation using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Sys-\ntems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/\npaper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf.\n[27] J. Lei and L. Wasserman. Distribution-free prediction bands for non-parametric regression.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71–96, 2014.\ndoi: https://doi.org/10.1111/rssb.12021. URL https://rss.onlinelibrary.wiley.com/\ndoi/abs/10.1111/rssb.12021.\n[28] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors\nfor sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon,\nUSA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/\nanthology/P11-1015.\n[29] P. Massart. The Tight Constant in the Dvoretzky-Kiefer-Wolfowitz Inequality. The Annals of\nProbability, 18(3):1269 – 1283, 1990. doi: 10.1214/aop/1176990746. URL https://doi.\norg/10.1214/aop/1176990746.\n[30] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. Le Scao, M. S. Bari, S. Shen,\nZ. X. Yong, H. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Almubarak, S. Albanie, Z. Alyafeai,\nA. Webson, E. Raff, and C. Raffel. Crosslingual generalization through multitask finetuning.\npages 15991–16111, Toronto, Canada, July 2023. doi: 10.18653/v1/2023.acl-long.891. URL\n2023.acl-long.891.\n[31] OpenAI, :, A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow,\nA. Welihinda, A. Hayes, A. Radford, A. M ˛adry, A. Baker-Whitcomb, A. Beutel, A. Borzunov,\nA. Carney, A. Chow, A. Kirillov, A. Nichol, A. Paino, A. Renzin, A. T. Passos, A. Kirillov,\nA. Christakis, A. Conneau, A. Kamali, A. Jabri, A. Moyer, A. Tam, A. Crookes, A. Tootoochian,\nA. Tootoonchian, A. Kumar, A. Vallone, A. Karpathy, A. Braunstein, A. Cann, A. Codispoti,\nA. Galu, A. Kondrich, A. Tulloch, A. Mishchenko, A. Baek, A. Jiang, A. Pelisse, A. Woodford,\nA. Gosalia, A. Dhar, A. Pantuliano, A. Nayak, A. Oliver, B. Zoph, B. Ghorbani, B. Leimberger,\nB. Rossen, B. Sokolowsky, B. Wang, B. Zweig, B. Hoover, B. Samic, B. McGrew, B. Spero,\nB. Giertler, B. Cheng, B. Lightcap, B. Walkin, B. Quinn, B. Guarraci, B. Hsu, B. Kellogg,\nB. Eastman, C. Lugaresi, C. Wainwright, C. Bassin, C. Hudson, C. Chu, C. Nelson, C. Li,\nC. J. Shern, C. Conger, C. Barette, C. Voss, C. Ding, C. Lu, C. Zhang, C. Beaumont, C. Hal-\nlacy, C. Koch, C. Gibson, C. Kim, C. Choi, C. McLeavey, C. Hesse, C. Fischer, C. Winter,\nC. Czarnecki, C. Jarvis, C. Wei, C. Koumouzelis, D. Sherburn, D. Kappler, D. Levin, D. Levy,\nD. Carr, D. Farhi, D. Mely, D. Robinson, D. Sasaki, D. Jin, D. Valladares, D. Tsipras, D. Li, D. P.\nNguyen, D. Findlay, E. Oiwoh, E. Wong, E. Asdar, E. Proehl, E. Yang, E. Antonow, E. Kramer,\nE. Peterson, E. Sigler, E. Wallace, E. Brevdo, E. Mays, F. Khorasani, F. P. Such, F. Raso,\nF. Zhang, F. von Lohmann, F. Sulit, G. Goh, G. Oden, G. Salmon, G. Starace, G. Brockman,\nH. Salman, H. Bao, H. Hu, H. Wong, H. Wang, H. Schmidt, H. Whitney, H. Jun, H. Kirchner,\nH. P. de Oliveira Pinto, H. Ren, H. Chang, H. W. Chung, I. Kivlichan, I. O’Connell, I. O’Connell,\nI. Osband, I. Silber, I. Sohl, I. Okuyucu, I. Lan, I. Kostrikov, I. Sutskever, I. Kanitscheider,\nI. Gulrajani, J. Coxon, J. Menick, J. Pachocki, J. Aung, J. Betker, J. Crooks, J. Lennon, J. Kiros,\nJ. Leike, J. Park, J. Kwon, J. Phang, J. Teplitz, J. Wei, J. Wolfe, J. Chen, J. Harris, J. Varavva,\nJ. G. Lee, J. Shieh, J. Lin, J. Yu, J. Weng, J. Tang, J. Yu, J. Jang, J. Q. Candela, J. Beutler,\nJ. Landers, J. Parish, J. Heidecke, J. Schulman, J. Lachman, J. McKay, J. Uesato, J. Ward, J. W.\nKim, J. Huizinga, J. Sitkin, J. Kraaijeveld, J. Gross, J. Kaplan, J. Snyder, J. Achiam, J. Jiao,\nJ. Lee, J. Zhuang, J. Harriman, K. Fricke, K. Hayashi, K. Singhal, K. Shi, K. Karthik, K. Wood,\nK. Rimbach, K. Hsu, K. Nguyen, K. Gu-Lemberg, K. Button, K. Liu, K. Howe, K. Muthukumar,\nK. Luther, L. Ahmad, L. Kai, L. Itow, L. Workman, L. Pathak, L. Chen, L. Jing, L. Guy,\nL. Fedus, L. Zhou, L. Mamitsuka, L. Weng, L. McCallum, L. Held, L. Ouyang, L. Feuvrier,\nL. Zhang, L. Kondraciuk, L. Kaiser, L. Hewitt, L. Metz, L. Doshi, M. Aflak, M. Simens,\nM. Boyd, M. Thompson, M. Dukhan, M. Chen, M. Gray, M. Hudnall, M. Zhang, M. Aljubeh,\nM. Litwin, M. Zeng, M. Johnson, M. Shetty, M. Gupta, M. Shah, M. Yatbaz, M. J. Yang,\nM. Zhong, M. Glaese, M. Chen, M. Janner, M. Lampe, M. Petrov, M. Wu, M. Wang, M. Fradin,\nM. Pokrass, M. Castro, M. O. T. de Castro, M. Pavlov, M. Brundage, M. Wang, M. Khan,\n33\n\n\nM. Murati, M. Bavarian, M. Lin, M. Yesildal, N. Soto, N. Gimelshein, N. Cone, N. Staudacher,\nN. Summers, N. LaFontaine, N. Chowdhury, N. Ryder, N. Stathas, N. Turley, N. Tezak, N. Felix,\nN. Kudige, N. Keskar, N. Deutsch, N. Bundick, N. Puckett, O. Nachum, O. Okelola, O. Boiko,\nO. Murk, O. Jaffe, O. Watkins, O. Godement, O. Campbell-Moore, P. Chao, P. McMillan,\nP. Belov, P. Su, P. Bak, P. Bakkum, P. Deng, P. Dolan, P. Hoeschele, P. Welinder, P. Tillet,\nP. Pronin, P. Tillet, P. Dhariwal, Q. Yuan, R. Dias, R. Lim, R. Arora, R. Troll, R. Lin, R. G.\nLopes, R. Puri, R. Miyara, R. Leike, R. Gaubert, R. Zamani, R. Wang, R. Donnelly, R. Honsby,\nR. Smith, R. Sahai, R. Ramchandani, R. Huet, R. Carmichael, R. Zellers, R. Chen, R. Chen,\nR. Nigmatullin, R. Cheu, S. Jain, S. Altman, S. Schoenholz, S. Toizer, S. Miserendino, S. Agar-\nwal, S. Culver, S. Ethersmith, S. Gray, S. Grove, S. Metzger, S. Hermani, S. Jain, S. Zhao,\nS. Wu, S. Jomoto, S. Wu, Shuaiqi, Xia, S. Phene, S. Papay, S. Narayanan, S. Coffey, S. Lee,\nS. Hall, S. Balaji, T. Broda, T. Stramer, T. Xu, T. Gogineni, T. Christianson, T. Sanders,\nT. Patwardhan, T. Cunninghman, T. Degry, T. Dimson, T. Raoux, T. Shadwell, T. Zheng,\nT. Underwood, T. Markov, T. Sherbakov, T. Rubin, T. Stasi, T. Kaftan, T. Heywood, T. Peter-\nson, T. Walters, T. Eloundou, V. Qi, V. Moeller, V. Monaco, V. Kuo, V. Fomenko, W. Chang,\nW. Zheng, W. Zhou, W. Manassra, W. Sheu, W. Zaremba, Y. Patil, Y. Qian, Y. Kim, Y. Cheng,\nY. Zhang, Y. He, Y. Zhang, Y. Jin, Y. Dai, and Y. Malkov. Gpt-4o system card, 2024. URL\nhttps://arxiv.org/abs/2410.21276.\n[32] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan,\nand J. Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under\ndataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran\nAssociates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/\n2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf.\n[33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style,\nhigh-performance deep learning library, 2019. URL https://arxiv.org/abs/1912.01703.\n[34] J. C. Platt. Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized\nLikelihood Methods. In Advances in Large Margin Classifiers, pages 61–74. MIT Press, 1999.\n[35] Y. Romano, M. Sesia, and E. J. Candès. Classification with valid and adaptive coverage. In\nProceedings of the 34th International Conference on Neural Information Processing Systems,\nNIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n[36] S. Rosenthal, N. Farra, and P. Nakov. SemEval-2017 task 4: Sentiment analysis in Twitter. In\nS. Bethard, M. Carpuat, M. Apidianaki, S. M. Mohammad, D. Cer, and D. Jurgens, editors,\nProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages\n502–518, Vancouver, Canada, Aug. 2017. Association for Computational Linguistics. doi:\n10.18653/v1/S17-2088. URL https://aclanthology.org/S17-2088/.\n[37] A. Schmaltz. Detecting local insights from global labels: Supervised and zero-shot sequence\nlabeling via a convolutional decomposition. Computational Linguistics, 47(4):729–773, Dec.\n2021. doi: 10.1162/coli_a_00416. URL https://aclanthology.org/2021.cl-4.25.\n[38] A. Schmaltz, Y. Kim, A. M. Rush, and S. Shieber.\nSentence-level grammatical error\nidentification as sequence-to-sequence correction. In J. Tetreault, J. Burstein, C. Leacock,\nand H. Yannakoudakis, editors, Proceedings of the 11th Workshop on Innovative Use of\nNLP for Building Educational Applications, pages 242–251, San Diego, CA, June 2016.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/W16-0528.\nURL https:\n//aclanthology.org/W16-0528/.\n[39] A. Schmaltz, A. M. Rush, and S. Shieber. Word ordering without syntax. In J. Su, K. Duh, and\nX. Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 2319–2324, Austin, Texas, Nov. 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/D16-1255. URL https://aclanthology.org/D16-1255/.\n[40] A. Schmaltz, Y. Kim, A. Rush, and S. Shieber. Adapting sequence models for sentence\ncorrection. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on\n34\n\n\nEmpirical Methods in Natural Language Processing, pages 2807–2813, Copenhagen, Denmark,\nSept. 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1298. URL\nhttps://aclanthology.org/D17-1298/.\n[41] K. Sharp and F. M. Matschinsky. Translation of ludwig boltzmann’s paper \"on the relationship\nbetween the second fundamental theorem of the mechanical theory of heat and probability\ncalculations regarding the conditions for thermal equilibrium\" sitzungberichte der kaiserlichen\nakademie der wissenschaften. mathematisch-naturwissen c. Entropy, 17:1971–2009, 2015.\nURL https://api.semanticscholar.org/CorpusID:17745806.\n[42] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously\nlarge neural networks: The sparsely-gated mixture-of-experts layer. In International Con-\nference on Learning Representations, 2017. URL https://openreview.net/forum?id=\nB1ckMDqlg.\n[43] J. Vaicenavicius, D. Widmann, C. R. Andersson, F. Lindsten, J. Roll, and T. B. Schön. Evaluating\nmodel calibration in classification. In International Conference on Artificial Intelligence and\nStatistics, 2019. URL https://api.semanticscholar.org/CorpusID:67749814.\n[44] L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134–1142, nov 1984. ISSN\n0001-0782. doi: 10.1145/1968.1972. URL https://doi.org/10.1145/1968.1972.\n[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA,\n2017. Curran Associates Inc. ISBN 9781510860964.\n[46] V. Vovk, A. Gammerman, and G. Shafer. Algorithmic Learning in a Random World. Springer-\nVerlag, Berlin, Heidelberg, 2005. ISBN 0387001522.\n[47] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang,\nT. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. MMLU-pro: A more\nrobust and challenging multi-task language understanding benchmark. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024.\nURL https://openreview.net/forum?id=y10DM6R2r3.\n35\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20167v1.pdf",
    "total_pages": 35,
    "title": "Similarity-Distance-Magnitude Universal Verification",
    "authors": [
      "Allen Schmaltz"
    ],
    "abstract": "We solve the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}