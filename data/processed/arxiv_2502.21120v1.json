{
  "id": "arxiv_2502.21120v1",
  "text": "SEE: See Everything Every Time - Adaptive Brightness Adjustment for Broad\nLight Range Images via Events\nYunfan Lu1, Xiaogang Xu2, Hao Lu1, Yanlin Qian3, Pengteng Li1, Huizai Yao1,\nBin Yang4, Junyi Li1, Qianyi Cai1, Weiyu Guo1, Hui Xiong 1 *\n1 AI Thrust, HKUST(GZ); 2 CUHK; 3 DJI / Hasselblad; 4 Aalborg University\nylu066@connect.hkust-gz.edu.cn, xiaogangxu00@gmail.com\nbyang@cs.aau.dk, honza.qian@dji.com, xionghui@ust.hk\nAbstract\nEvent cameras, with a high dynamic range exceeding 120dB,\nsignificantly outperform traditional embedded cameras, ro-\nbustly recording detailed changing information under vari-\nous lighting conditions, including both low- and high-light\nsituations.\nHowever, recent research on utilizing event\ndata has primarily focused on low-light image enhancement,\nneglecting image enhancement and brightness adjustment\nacross a broader range of lighting conditions, such as nor-\nmal or high illumination. Based on this, we propose a novel\nresearch question: how to employ events to enhance and\nadaptively adjust the brightness of images captured un-\nder broad lighting conditions? To investigate this question,\nwe first collected a new dataset, SEE-600K, consisting of\n610,126 images and corresponding events across 202 sce-\nnarios, each featuring an average of four lighting condi-\ntions with over a 1000-fold variation in illumination. Sub-\nsequently, we propose a framework that effectively utilizes\nevents to smoothly adjust image brightness through the use\nof prompts. Our framework captures color through sensor\npatterns, uses cross-attention to model events as a bright-\nness dictionary, and adjusts the image‚Äôs dynamic range to\nform a broad light-range representation (BLR), which is then\ndecoded at the pixel level based on the brightness prompt.\nExperimental results demonstrate that our method not only\nperforms well on the low-light enhancement dataset but also\nshows robust performance on broader light-range image en-\nhancement using the SEE-600K dataset. Additionally, our\napproach enables pixel-level brightness adjustment, provid-\ning flexibility for post-processing and inspiring more imaging\napplications. The dataset and source code are publicly avail-\nable at: https://github.com/yunfanLu/SEE.\n*corresponding author\n1. Introduction\nEvery day, from daylight to nighttime, the illuminance varies\nfrom about 100,000 lux (bright sunlight) to approximately\n0.1 lux (starlight) [3]. Maintaining stable imaging under\ndiverse natural lighting conditions is a significant challenge\nthat energizes many embedded applications [4]. To achieve\nthis, a series of influential works have emerged, including\nautomatic exposure [5], exposures correction [6], low-light\nenhancement [7] and high dynamic range (HDR) imaging [8].\nHowever, traditional cameras are limited by their imaging\nprinciple of synchronously capturing intensity values across\nthe entire sensor, with a dynamic range of only 60 to 80\ndB [9, 10]. Consequently, these traditional methods find it\ndifficult to capture imaging information under a wide range\nof lighting conditions at the input [11, 12]. If the exposure is\ninaccurate - over and under exposures - traditional embedded\ncameras lose the potential to restore images under complex\nlighting conditions due to limited bits-width and noise. Un-\nlike traditional cameras, event cameras [12] asynchronously\nrecord pixel-level changes in illumination, outputting the\ndirection of intensity change (positive or negative) at each\npixel with extremely high dynamic range (120 dB), which\nfar exceeds the capability of traditional cameras in capturing\nvarious lighting intensity. Research leveraging the events\nfor image brightness enhancement can be divided into three\ncategories. (1) event-based image reconstruction, which\naims to reconstruct images only from events. However, these\nmethods [10, 13, 14] rely solely on events, facing uncertain-\nties during reconstruction, and the events usually contain\nnoise, which leads to color distortion and limited capabilities\nof generalization. (2) event-guided HDR imaging [15‚Äì17]\n, which targets to employ events to extend the dynamic range\nof images or videos to match human vision.\nCui et al.\n[15] introduced the first real-world dataset containing paired\ncolor events, low dynamic range images, and HDR images,\nwith only includes 1,000 HDR image pairs.\nMessikom-\nmer et al. [17] used nine images with different exposures\narXiv:2502.21120v1  [cs.CV]  28 Feb 2025\n\n\nBrightness Prompt ùêµ\n0\n1\nInputs (Image ùêº! & Event ùê∏)\nVarious of Exposure\nSEE Net\n(Ours)\nOutputs ùêº\"\nùêº!\nùê∏\nùêµ\nBrightness is \ncontrolled by ùêµ\nùëì#(ùêº\") ‚Üíùêµ\nùêº\"\nPrevious\nMethods\nùêº!\nùê∏\nInputs (Image ùêº! & Event ùê∏)\nOnly Low-Light Exposure\nOutput ùêº\"\nNo control over brightness\nùêº\"\nEvLowLight\nEvLight, et.al.\n(a)\n(b)\n(c)\n(d)\nFigure 1. (a) and (b): Brightness distributions of the SDE dataset (0‚àº0.45, low to normal light) and our SEE-600K dataset (0‚àº1, a broader\nlight range). (c): Previous methods [1, 2] directly map low-light images to normal-light images. (d): Our SEENet accepts inputs across a\nbroader brightness range and adjusts output brightness through prompts. fb refers to the function that calculates the brightness of an image.\nto synthesize an HDR image as the ground truth and uti-\nlized multi-exposure frames and events as inputs to generate\nan HDR image. While HDR imaging aims to expand dy-\nnamic range, collecting HDR datasets is difficult, and these\nmethods have not been evaluated for tasks like low-light en-\nhancement or high-light restoration [1, 2, 18‚Äì20]. (3) event-\nguided low-light enhancement [1, 2, 21, 22], which is de-\nsigned to adjust low-light images to normal-light conditions\nthrough brightness adjustment and noise reduction. Liang\net al. [2] represents the latest research and proposed the first\nevent-based low-light image enhancement dataset, SDE (see\nFig. 1 (a)). Prior to this, Liang et al. [1], Liu et al. [21], Jiang\net al. [22] explored using motion information from events\nand employed varying neural networks to improve the map-\nping from low-light images to normal-light ones, as shown\nin Fig. 1 (c). However, these strategies only focus on the\nimprovement of mapping ability for low-light inputs, limiting\ntheir capacity to adjust brightness across a broader range of\nlighting conditions, e.g., normal or high-light images. The\nfundamental reason lies in the fact that these methods rely\non datasets (such as SDE) that control brightness using a\nneutral density filter, i.e., they create paired low-light and\nnormal-light images with only one fixed lighting ratio be-\ntween the two. This approach forces the model to learn a\nrigid multiplicative enhancement strategy that only works\nwithin the narrow bounds of low-light to normal-light transi-\ntions. Consequently, the methods trained on such datasets\nare unable to generalize well to broad lighting conditions,\nparticularly those involving high-light scenarios or complex\nlighting variations. This limitation arises because the training\ndata does not reflect the full diversity of lighting conditions\nencountered in real-world applications, where illumination\ncan change dynamically and across a much wider range.\nTherefore, these methods lack the flexibility needed to adjust\nbrightness across a broader range of lighting variations.\nFurthermore, due to the uncertainty in the standard\nfor normal-light image collection‚Äîas the normal-light\nimages are relative to low-light images (as shown in\nFig. 1 (a))‚Äîthese methods introduce ambiguity during the\ntraining process because they can only map low-light im-\nages to normal-light ones based on a single set of low- and\nnormal-light data pairs captured per scene. Overall, cur-\nrent research focuses on low-light enhancement, neglecting\nimage enhancement and processing under a wider range of\nlighting conditions. Therefore, how to use events to enhance\nand adjust the brightness of images across a broader range\nof lighting conditions becomes a more worthwhile research\nquestion.\nTo address this novel research question, we first formu-\nlate the imaging model for brightness adjustment (Sec. 3)\nand define the new learning task. We aim to perceive light-\ning information from events, utilizing brightness prompts\nto convert this lighting information into images with a spe-\ncific brightness. In doing so, other image quality aspects\n(like sensor patterns, noise, and color bias) are taken into\nconsideration.\nTo realize our proposed task, we first collected a new\ndataset by emulating each scene in different lighting con-\nditions, covering a broader luminance range (Sec. 4), as\nshown in Fig. 1 (b) and (d). By capturing multiple lighting\nconditions per scene, we enable mappings across diverse\nillumination scenarios, providing rich data for model train-\n\n\ning. To tackle the challenges of spatio-temporal alignment\nof video and event streams under various lighting condi-\ntions, we design a temporal alignment strategy relying on\nprogrammable robotic arms and inertial measurement unit\n(IMU) sensors. As a result, we obtain a temporal registra-\ntion error up to one millisecond and a spatial error at the\nsub-pixel level (‚àº0.3 pixel). Finally, we build a large-scale\nand well-aligned dataset containing 202 scenes, each with 4\ndifferent lighting conditions, summing up to 610,126 images\nand the corresponding event data. We term this dataset as\nSEE-600K, which supports learning the mappings among\nmultiple lighting conditions.\nBuilding on the SEE-600K dataset, we propose a compact\nand efficient framework, SEE-Net, for the proposed new task\n(Sec. 5). An event-aware cross-attention is used to enhance\nimage brightness, and the brightness-related prompt is intro-\nduced for controlling the overall brightness. This approach\neffectively captures and adjusts lighting across a broader\nrange of illumination conditions, providing flexibility and\nprecise control during inference. Despite the performance\nadvantage, SEE-Net still remains effective, compact, and\nlightweight with only 1.9 M parameters.\nOur method has been evaluated on two real-world\ndatasets, SDE [2] and SEE-600K. Quantitative results\ndemonstrate that our framework fits well to a broader range\nof lighting conditions (Sec. 6). Furthermore, our framework\nallows for smooth brightness adjustment, providing precise\nexposure control. Therefore, this flexibility significantly\nimproves post-processing capabilities and enables potential\napplications in advanced imaging and processing tasks.\n2. Related Works\nFrame-based: These brightness enhancement methods aim\nto improve image quality under challenging illumination\nconditions. Retinexformer [23] and other Retinex-based\nframeworks [24‚Äì26] decompose reflectance and illumina-\ntion with complex training pipelines. Other approaches, e.g.,\nstructure-aware models [27, 28], utilize edge detection or\nsemantic-aware guidance to achieve sharper and more realis-\ntic results. Exposure correction strategies [29‚Äì31] target both\noverexposed and underexposed areas, leveraging multi-scale\nnetworks or perceptual image enhancement frameworks to\nsynthesize correctly exposed images. However, the reliance\non RGB frames with limited bit depth, limits the adaptability\nto dynamic lighting conditions, making it difficult to handle\na broader range of lighting scenarios.\nEvent-based: These methods focus on reconstructing im-\nages or videos exclusively from event data. For instance,\nDuwek et al. [32] introduced a two-phase neural network\ncombining convolutional neural networks and spiking neural\nnetworks, while Pan et al. [33] proposed the event-based\ndouble integral model to generate videos. Stoffregen et al.\n[13] enhanced event-based video reconstruction by intro-\nducing the new dataset. Additionally, Wang et al. [14], Liu\nand Dragotti [34] developed a model-based deep network to\nimprove reconstructed video quality. However, these event-\nbased approaches face challenges due to event data noise,\noften leading to color distortion and limited generalization.\nEvent-guided: These works are centered on enhancing im-\nages captured in low-light conditions. E.g., Zhang et al. [35]\nand Liu et al. [36] recovered lost details in low-light environ-\nments by reconstructing grayscale images. Similarly, Liang\net al. [1] and Liu et al. [21] improved low-light video en-\nhancement by leveraging motion information from events to\nenhance multi-frame videos and integrate spatiotemporal co-\nherence. Furthermore, Jin et al. [37] and Jiang et al. [22] uti-\nlized events to recover structural details and reconstruct clear\nimages under near-dark situations. Most notably, Liang et al.\n[2] introduced the first large-scale event-guided low-light en-\nhancement dataset, which is significant for the development\nof this field. While these methods use events for bright-\nness changes and structural recovery in low-light conditions,\nthey are limited to enhancing low-light images with single\nmapping and cannot handle brightness adjustments across a\nbroader range of lighting conditions, including normal- and\nhigh-light.\n3. Preliminaries and New Task Definition\nIn this section, we formalize the physical model underlying\nour approach to enhance and adjust image brightness across\na broader range of lighting conditions using events. Imaging\nis fundamentally the process of capturing the light intensity\nof a scene, represented as a radiance field L(t) varying over a\npreset slot t. The illumination intensities of light in daily life\nspan a vast range, from 0.1 lux (starlight) to 1e6 lux (direct\nsunlight). The goal of brightness adjustment is to recover or\nestimate L(t) and tone-map it into an image that is visually\nsuitable for human perception.\nTraditional cameras record light signals through expo-\nsure [38]. This voltage is influenced by the Gaussian noise\nN = N (¬µ,œÉ2) (¬µ is the mean and œÉ2 is the variance), and\nthe photon shot noise P = P(k), where k ‚àùL(t) is the num-\nber of photons, proportional to light intensity. In low-light\nconditions, Gaussian noise becomes dominant, while in high-\nlight conditions, photon shot noise becomes more significant.\nThese noise types influence the final value in the RAW im-\nage, simply represented as Iraw ‚âàQ(L(t)+P +N), where\nQ is the quantization function that converts the continuous\nvoltage into discrete digital signals, typically ranging from 8\nto 12 bits. The shape of the image Iraw is H √óW √ó1, where\nH and W are the image resolution. The RAW image is then\nfurther processed through image signal processing (ISP) fisp,\nwhich includes multiple steps e.g., denoising, linear and\nnon-linear transformations, resulting in a RGB image as\nIrgb = fisp(Iraw), with the shape of H √óW √ó3. An accurate\nimage exposure procedure recovers Irgb corresponding to\n\n\n(c) EVS Outputs with Different Filter\nf1\nf2\nf3\nf4\n(a) Data Collection Setup\n(b) IMU Data Registration\ntime\n‚ë†\n‚ë°\n‚ë¢\n‚ë£\n‚ë†Event Vison Sensor + IMU\n‚ë°Neutral Density Filters\n‚ë¢Non-Linear Trajectory\n‚ë£Universal Robots UR5e\n1/24\n2/24\n3/24\nFigure 2. (a) data collection setup: Universal Robots UR5e arm replicates precise trajectories with an error margin of 0.03mm. (b) IMU\ndata registration: b (1) shows unregistered IMU data, while b (2) displays registered data after timestamp alignment. (c) EVS outputs\nwith different filters: f1 to f4 demonstrate the different ND filters, depicting various lighting levels.\nùëÜ1\nùëá1\nùëÜ2\nùëá2\nùëÜ0 (ùë°ùëü1)\nùëá0 (ùë°ùëü2)\n‚ë†\nùëè2\nùëô2\nùëè1\nùëô1\nùëè0\nùëô0\n‚ë°\n‚ë¢\n‚ë£\n‚ë†‚ë° Average Pooling\n‚ë¢‚ë£ ‚ë§ Search in Level-0,1,2\nS, T, b, l: Source, Target, Bias, Length\n‚ë§\nLevel-2\nLevel-0\nLevel-1\nùíü= 0.10 pixel\nùë°r!\nùë°r\"\n(a) Registration Process\n(b) Two trajectories\n(c) Pixel Distance Change\nFigure 3. (a) registration process: Illustration of the multi-level registration process, showing how trajectories, S and T, at various levels\nare iteratively aligned. (b) two trajectories: The example of two aligned images captured along two trajectories. (c) pixel distance change:\nTemporal distance of pixel between two registered videos, showing a mean alignment error of 0.2957 pixels over time.\nL(t), up to a high degree meeting the following three char-\nacteristics: (1) accurate exposure: The mean value of Irgb\nfalls within the range [0.4,0.7] [39]. (2) noise-free: The\ninfluence of N and P is suppressed to a visual-acceptable\nlevel. (3) color neutrality: The gray levels calculated from\nthe RGB channels should be consistent [40]. However, tra-\nditional cameras sometimes fail to capture sufficient details\nin extreme-lighting scenes. Under such low-light condi-\ntions, images may lack visible details and be contaminated\nby noise, while in high-light conditions, images may suffer\nfrom oversaturation, losing texture and edge information.\nEvent cameras asynchronously detect illumination\nchanges at each pixel, making them ideal for capturing\nscenes with extreme or rapidly changing lighting condi-\ntions [12]. The event stream‚Äôs outputs are formatted as 4\ncomponents: (x,y) (pixel coordinates), t (timestamp), and\np ‚àà{+1,‚àí1} (polarity, indicating light intensity increase or\ndecrease). Events are triggered when the change in illumina-\ntion exceeds a threshold C (‚àÜL = log(L(t))‚àílog(L(t ‚àí‚àÜt))\nwhere |‚àÜL| > C). We jointly leverage the complementary in-\nformation from an image Irgb and its corresponding events E\nto recover a high-quality well-illuminated image ÀÜ\nIrgb that ac-\ncurately represents the scene radiance L(t), while also allow-\ning for adjustable brightness. To achieve this, we introduce\na brightness prompt B that controls the overall brightness\nof the output image. This allows us to map the L(t) into an\n\n\nimage that is optimally exposed for human observation. Our\ntask setting can thus be formulated as Eq. 1, where fsee is\nour proposed model.\nfsee(Irgb,E,B) ‚ÜíÀÜ\nIrgb.\n(1)\nThis formulation has two advantages: (1) robust training:\nBy inserting the brightness prompt B during training, we\ncan decouple the model from biases in the training data with\nspecific brightness levels, enabling the model to generalize\nbetter over illuminates domain. (2) flexible inference: Dur-\ning inference, the prompt B can be set to a default value\n(e.g., B = 0.5) to produce images with general brightness,\nor be adjusted to achieve different brightness levels, provid-\ning flexibility for applications requiring specific exposure\nadjustments or artistic effects. Please refer to the appendix\nfor more details of this section.\nDifferences with HDR Image Reconstruction: In this\nsection, we define the brightness adjustment task based on\nevents. It is important to note that HDR image reconstruc-\ntion also involves processing both low-light and high-light\nimages to capture more details in both dark and bright re-\ngions. However, HDR image reconstruction aims to extend\nthe dynamic range, which is a more ambitious and challeng-\ning task. In contrast, our goal is to adjust the dynamic range,\nwhich is easier to implement and more flexible. We highlight\nthree key differences between these two tasks: (1) Different\nObjectives: HDR image reconstruction aims to expand the\ndynamic range of an image by combining multiple exposures,\ncapturing details in both dark and bright regions. Mathemat-\nically, this involves recovering a radiance map R(x) from\nevents and images captured at different exposure levels, rep-\nresented as R(x) = f ‚àí1(ILDR(x),E), where ILDR(x) is the\nobserved LDR image. In contrast, our brightness adjustment\ntask focuses on modifying the exposure of a single image\nto recover lost details, without necessarily expanding the\ndynamic range, as shown in Eq. 1. (2) Distinct Challenges:\nHDR image reconstruction seeks to transfer the HDR charac-\nteristics of events to LDR images, requiring an HDR image\nas supervision. In contrast, our brightness adjustment task\nuses event data to adjust the exposure of a single image,\nwithout needing multiple images at different exposures for\nsupervision. (3) Different Dataset Construction: HDR re-\nconstruction datasets require capturing multiple images at\ndifferent exposure levels [17], which can be computation-\nally expensive and difficult to handle in dynamic scenes. In\ncontrast, our approach uses event cameras to capture pairs\nof images and events under varying lighting conditions, sim-\nplifying data construction and making it more adaptable to\ndynamic scenes.\n4. SEE-600K Dataset\nBased on the new problem defined in the previous section,\nwe require a large-scale dataset to support training and eval-\nuation, which includes events and images under various\nlighting conditions, with complex motion trajectories and\nspatio-temporal alignment. To address this, we propose a\nnew dataset, SEE-600K. In this section, we introduce the\nSEE-600K dataset, focusing on two aspects: the dataset\ncollection method and its diversity.\n4.1. Collection Method\nTo achieve this collection, we propose the following solu-\ntion, which consists of three key components. (1) multiple\nlighting conditions: Our approach is based on the princi-\nple that lighting transitions continuously from low to high\nintensity. Unlike previous datasets [2, 41], which captured\nonly a single pair of low-light and normal-light conditions,\nwe focus on multiple samples, as shown in Fig.\n4. To\ncover a broader lighting range, we record an average of four\nvideos per scene, using neutral density (ND) filters at three\nlevels (1/8,1/64,1/1000) and one without a filter. We also\nadjust the aperture and exposure settings to capture each\nscene under diverse lighting conditions. (2) complex mo-\ntion trajectories: We employ the Universal Robots UR5e\nrobotic arm, which can provide high stability and repeat\nthe same non-linear trajectory with an error margin of 0.03\nmm [2, 42], allowing us to capture multiple videos with\nspatial consistency, as exhibited in Fig. 3 (a). (3) spatio-\ntemporal alignment: While the robotic arm guaranteed spa-\ntial alignment, asynchronous control over the camera‚Äôs start\nand stop times inevitably introduced timing deviations. To\nresolve this, we propose an IMU-based temporal alignment\nalgorithm, as shown in Fig. 3 (b). IMU streams synchronized\nto events and videos with microsecond timestamps in the\nDVS346 camera. Additionally, the IMU stream depends\nonly on motion trajectory and enjoys a temporal resolution\nof 1000 Hz. Based on this, our algorithm achieves precise\ntemporal alignment, ensuring synchronization across the en-\ntire dataset, as displayed in Fig. 3 (c). We introduce the\nIMU-based alignment method, and the alignment evaluation\nresults as follows. Temporal IMU Registration Algorithm:\nWe propose an IMU data registration algorithm that aligns\nthe source sequence S and target sequence T by finding the\noptimal bias b and matching length l to minimize the L1\ndistance between them. Given the high resolution of IMU\ndata at 1000Hz, an exhaustive search for the optimal bias is\ncomputationally infeasible. To address this, we introduce\na multi-level iterative strategy. First, we denoise the IMU\ndata using a Kalman filter [43]. Then, the average pool-\ning is utilized to reduce the sequences to two additional\nlevels, Level-1 (S1, T1) and Level-2 (S2, T2), as shown in\nFig. 3 (a)- 1‚Éù2‚Éù. This reduces computational complexity\nwhile preserving essential alignment features. The window\nsize is chosen based on our video durations, which range\nfrom 10 to 120 seconds. We perform a coarse search for the\noptimal bias b and matching length l at the lowest resolution\n\n\n(a)\n(b)\n(c)\n(I)\n(II)\n(III)\nFigure 4. Visualization of different lighting conditions in SEE-600K. We present three scenes captured in SEE-600K, demonstrating its\nbroad illumination coverage. Each row corresponds to a different lighting condition within the same scene: (a) High-Light, (b) Normal-Light,\nand (c) Low-Light. The columns (I), (II), and (III) represent different environments. Each sub-image pair consists of a frame (left) and its\ncorresponding events (right), highlighting the ability of event cameras to capture fine-grained temporal changes across varying lighting\nconditions.\n(a) Some Scenes from SEE-600K\n(b) Word Cloud\nFigure 5. Visualization of scene diversity in SEE-600K. The left side (a) presents 12 representative scenes from SEE-600K, showcasing the\nvariety of environments captured in our dataset, including urban areas, buildings, natural scenes, industrial settings, and structured indoor\nspaces. With a total of 202 distinct scenes, SEE-600K provides a broad range of lighting conditions and structural diversity for event-based\nimaging research. The right side (b) displays a word cloud generated from our dataset, illustrating the richness of scene categories and\nobjects present in SEE-600K, further emphasizing its comprehensiveness.\n(Level-2). The results from this level serve as center points\nfor finer searches at higher resolutions. Specifically, the\n\n\nTable 1. Comparison of different datasets in event-based imaging. Our SEE-600K dataset captures a broader range of illumination conditions,\nencompassing low-light, normal-light, and high-light settings. Additionally, it uniquely provides multiple groups of recordings per scene,\nenabling more robust learning of brightness transitions across various lighting conditions. This comprehensive dataset offers significantly\nlarger scale and diversity, making it a valuable resource for event-based imaging research.\nDataset\nPublic\nAvailable\nDynamic\nMotion\nSize\n(Frames)\nDevice\nFrames\nLow-Light\nNormal-Light\nHigh-Light\nMultiple Groups\nDVS-Dark [35]\n‚úó\n‚úì\n17,765\nDAVIS240C\n‚úì\n‚úó\n‚úó\n‚úó\nLIE [22]\n‚úì\n‚úó\n2,231\nDAVIS346\n‚úì\n‚úì\n‚úó\n‚úó\nEvLowLight [1]\n‚úó\n‚úì\n‚Äî\nDAVIS346\n‚úì\n‚úì\n‚úó\n‚úó\nRLED [36]\n‚úó\n‚úì\n64,200\nProphesee EVK4\n‚úó\n‚úì\n‚úó\n‚úó\nSDE [2]\n‚úì\n‚úì\n31,477\nDAVIS346\n‚úì\n‚úì\n‚úó\n‚úó\nSEE-600K (Ours)\n‚úì\n‚úì\n610,126\nDAVIS346\n‚úì\n‚úì\n‚úì\n‚úì\nbias and length identified at each level guide local searches\nat the next level up, as displayed in Fig. 3 (a)- 3‚Éù4‚Éù5‚Éù. At\nLevel-1 and the original data level (Level-0), we only need to\nsearch locally around these center points. This hierarchical\napproach efficiently achieves high matching accuracy with\nsignificantly reduced computational effort.\nSpatial-Temporal Alignment Evaluation: To evaluate the\naccuracy of our IMU registration algorithm, we capture the\nsame scene twice under identical lighting conditions, as illus-\ntrated in Fig. 3 (b). We assess the alignment metric between\nthe two image sequences by calculating the pixel-level dis-\ntance at the corresponding timestamp. Alignment Metric:\nFor each image pair, we extract keypoints using SIFT [44]\nand then employ the FLANN matcher [45] to find matching\nkeypoints between the two images. Based on these matched\nkeypoints, we compute the affine transformation matrix us-\ning RANSAC [46]. This transformation is subsequently\napplied to each pixel, allowing us to calculate the displace-\nment distance for every pixel. Finally, the average pixel\ndistance is employed as the metric for alignment. Alignment\nResults: In the alignment evaluation, we select scenes with\nwell-defined textures, as illustrated in Fig. 3 (b). After calcu-\nlating the pixel distances, we observe that the average pixel\nerror between the paired images is 0.2967 pixels. Through-\nout the entire time sequence, the pixel-level distance remains\nbelow 0.8 pixels, with the majority of errors being under 0.5\npixels, as exhibited in Fig. 3 (c). These results demonstrate\nthat the registration accuracy of our dataset reaches sub-pixel\nprecision.\n4.2. Diversity Analysis\nTo analyze the diversity of our dataset, we evaluate the SEE-\n600K dataset in terms of both illumination and scene variety.\nFirstly, as shown in Fig. 4, for each scene, we capture mul-\ntiple lighting conditions, including high-light, normal-light,\nand low-light, ensuring a broad range of illumination cover-\nage. This approach enhances the dataset‚Äôs ability to support\ntasks requiring robust learning across diverse lighting varia-\ntions. Secondly, the SEE-600K dataset includes 202 distinct\nscenes, as shown in Fig. 5, with a total of over 600K data\nsamples, making it approximately 20 times larger than the\nlatest SDE dataset [2]. This substantial increase in scale pro-\nvides a richer foundation for training models with improved\ngeneralization capabilities. Additionally, we compare SEE-\n600K with several other event-based datasets, as shown in\nTab. 1. Unlike previous datasets such as DVS-Dark [35]\nor EvLowLight [1], SEE-600K includes a broader range of\nillumination conditions. Furthermore, SEE-600K uniquely\noffers multiple groups of recordings per scene, enabling\nmore robust learning of brightness transitions across various\nlighting conditions. This makes SEE-600K a significantly\nmore comprehensive resource in terms of scale and diversity\ncompared to other datasets like LIE [22], RLED [36], and\nSDE [2], which either lack diversity in lighting conditions or\ninclude fewer frames. Finally, we assess the diversity of the\ndataset using a word cloud generated by ChatGPT-4o [47],\nas shown in Fig. 5 (b). This word cloud illustrates the variety\nof scene categories and objects present in the dataset, fur-\nther emphasizing its comprehensive coverage. With scenes\nranging from urban environments to industrial settings and\nnatural landscapes, SEE-600K captures a diverse array of\nobjects and contexts, making it a valuable resource for event-\nbased imaging research.\n5. Methods\nOverview: As shown in Fig. 6, our framework, SEE-Net,\nconsists of four implementation parts: (a) Inputs and Prepro-\ncessing, (b) Encoding, (c) Decoding, and (d) Loss Function.\nThe input is an image Ii and its corresponding events E. The\noutput is a brightness-adjustable image Io, where the bright-\nness is controlled by the prompt B ‚àà(0,1). During training,\nthe brightness prompt B is calculated according to the target\nimage. On the other hand, during testing, B can be freely\nset, with a default value of 0.5, which follows the exposure\ncontrol constraint [39, 48]. Overall, the SEE-Net fsee can be\n\n\nImage ùêº!,\nOne of Various of Lighting\nEvents ùê∏\nSensor\nPattern\nEvent \nCamera \nPosition &\nBayer \nPattern\nEmbedding\nùëÉ\nc\nc\nùëì\"(&)\nùëì!(&)\nEvent Head\nImage Head\nùëì#!(&,&)\nTransformer\nCross Attention\nBlock\nùê∏\nùêº!\nBrightness Prompt B\nInference Stage: \nB is freely set, e.g., 0.5\n0\n1\nBrightness\nEmbedding\nSparse Encoder\n ùëì$\"(&,&)\nLoop ùêø√ó \nOutputs ùêº%\nBrightness is controlled by ùêµ\nùêπ&\nùêπ'\nùêπ!\nùëì((&,&,&)\nùêπ)\nùêπ&\nùêπ)*'\nùêπ'\nSparse Encoder Loop ùëì((&,&,&)\nùëì#\"\nùëì##\nfirst input ùêπ' and loop\nùêπ&\nùêπ)\nùêπ'\n-\n+\nùêπ)*'\nCat\nCat\nAdd\nùêº%\n+\n+\n+\nMLP √ó ùëÅ, ùëì+(%,%)\nAdd\nAdd\nùêµ\nNormal Lighting, ùêº,\nBrightness ùêµ‚àà(0.4, 0,7)\nTraining Stage:\nReference \nBrightness as \nPrompt\nLoss ‚Ñí(&,&)\n(a) Inputs and Preprocessing, Sec. 5.1\n(b) Encoding, Sec. 5.2\n(c) Decoding, Sec. 5.3\n(d) Loss Function, Sec. 5.4\n+\nMulti\nSub\nAdd\nùêπ-\nBroader Light\nRange\nRepresentation\nBLR\nùëÉ\nùëÉ\nFigure 6. Overview of our proposed framework, called SEE-Net, which is composed of four stages: (a) Inputs and Preprocessing, (b)\nEncoding, (c) Decoding, and (d) Loss Function. This framework takes as input an image captured under a wide range of lighting conditions,\nalong with its corresponding events. The output is a brightness-controllable image, where the brightness is guided by the brightness prompt\nB, enabling flexible pixel-level adjustment during inference.\ndescribed by the Eq. 2 to match our learning task in Sec. 3.\nIo = fsee(Ii,E,B).\n(2)\nBelow, we elaborate the insights and implementation details\nof each part.\nInputs and Processing: This part aims to transform initial\ninputs into features that retain original information for the\nencoding stage. The inputs consist of the image Ii and the\nevents E, where Ii has a dimension of H √óW √ó3 (with H and\nW representing the height and width, and 3 representing the\ncolor channel number). The event stream E is represented as\na voxel grid [49] with a dimension of H √óW √óM, where M\nrepresents the number of time slices of events. The events\ninclude color information [50], which was overlooked in pre-\nvious works, e.g., [1, 2]. Specifically, this DVS346 sensor\nrecords events with Bayer Pattern [51]. To effectively embed\nboth the color and positional information during framework\ntraining, we design the position and Bayer Pattern embed-\ndings, as shown in Fig. 6 (a). The position and Bayer Pattern\nare denoted as a vector (x,y,bp), where x,y is the pixel po-\nsition, and bp denotes the Bayer Pattern index, which takes\na value from 0 to 3. We embed this vector into a higher-\ndimensional feature, termed as P, and concatenate it with the\ninputs. Two layers 1√ó1 convolutions, denote fe and fi, are\nthen applied to obtain the initial event features Fe and image\nfeatures Fi. This process is described by the Eq. 3, where\nfcat denotes the concatenation function.\nFe = fe(fcat(E,P)),\nFi = fi(fcat(Ii,P)).\n(3)\nEncoding: In this stage, we aim to obtain the BLR by\nemploying the event feature Fe to enhance the image feature\nFi, facilitating noise reduction and the acquisition of broader\nlight range information. Since Fe contains rich information\nabout the lighting changes across different intensity levels,\nwe use it as the source for representing the broader light\nrange. However, event data only records changes in illu-\nmination, which differ fundamentally from the static RGB\nframe modality. This makes directly utilizing event data for\nbroader light representation challenging. To address this,\nwe employ a cross-attention [58] for feature fusion, produc-\ning the initial fused broad-spectrum feature F1, expressed as\nF1 = fc0(Fe,Fi), where fc0 is a cross-attention block. Then,\ninspired by previous works [57], we utilize sparse learning\nto generate residuals for F1 from the event features Fe. These\nresiduals are progressively generated from the loop that exe-\ncutes L times. Multiple iterations are used because they allow\nthe model to iteratively refine the residuals, capturing finer\ndetails and enhancing the feature representations by progres-\nsively integrating information from the events. A single loop\nof this process can be expressed as, Fj+1 = fl(Fe,F1,Fj),\nwhere fl is a loop function that contains two cross-attention\nblocks as shown in Fig. 6 (b), where Fj and Fj+1 are the\n\n\n(f) DCE 15.26 /0.5761\n(g) Liu et. al. 18.24/0.8685\n(h) eSL Net 17.77/0.8621\n(i) EvLowLight 16.75/0.8391\n(j) EvLight 17.94/0.8957\n(a) Events\n(b) Inputs Image\n(c) Gamma Brightening\n(d) Normal-light Image\n(e) SEE Net 23.46/0.9108\n(f) DCE 10.71 /0.294\n(g) Liu et. al. 17.76/0.7816\n(h) eSL Net 20.42/0.8037\n(i) EvLowLight 20.67/0.8165\n(j) EvLight 19.68/0.7619\n(a) Events\n(b) Inputs Image\n(c) Gamma Brightening\n(d) Normal-light Image\n(e) SEE Net 26.88/0.8458\n(f) DCE 15.72 / 0.4127\n(g) Liu et. al. 22.27/0.6271\n(h) eSL Net 28.16/0.8077\n(i) EvLowLight 25.00/0.6742\n(j) EvLight 27.72/0.8031\n(a) Events\n(b) Inputs Image\n(c) Gamma Brightening\n(d) Normal-light Image\n(e) SEE Net 30.72/0.8946\n(I)\nIndoor\n(II)\nOutdoor\n(III)\nIndoor\nFigure 7. Visualization results of three scenes from the SDE dataset under varying lighting conditions.\n\n\nTable 2. Comparison of different methods on the SDE dataset. The best performances is highlighted in bold. ‚Ä† refers to the original model\nfor the HDR task, which is fine-tuned and trained on SDE\nMethod\nFLOPs\nParams\nEvents\nindoor\noutdoor\naverage\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR\nSSIM\nDCE [52]\n0.66\n0.01\n%\n13.91\n0.2659\n13.38\n0.1842\n13.64\n0.2250\nSNR [53]\n26.35\n4.01\n%\n20.05\n0.6302\n22.18\n0.6611\n21.12\n0.6457\nUFormer [54]\n12.00\n5.29\n%\n21.09\n0.7524\n22.32\n0.7469\n21.71\n0.7497\nLLFlow [55]\n409.50\n39.91\n%\n20.92\n0.6610\n21.68\n0.6467\n21.30\n0.6539\nRetinexformer [23]\n15.57\n1.61\n%\n21.30\n0.6920\n22.92\n0.6834\n22.11\n0.6877\nE2VID+ [13]\n27.99\n10.71\n!\n15.19\n0.5891\n15.01\n0.5765\n15.10\n0.5828\nELIE [22]\n440.32\n33.36\n!\n19.98\n0.6168\n20.69\n0.6533\n20.34\n0.6350\nHDRev[16] ‚Ä†\n118.65\n13.42\n!\n21.13\n0.6239\n21.82\n0.6824\n21.47\n0.6531\n[56]\n170.32\n7.38\n!\n21.29\n0.6786\n22.08\n0.7052\n21.68\n0.6919\neSL-Net [57]\n560.94\n0.56\n!\n21.25\n0.7277\n22.42\n0.7187\n21.84\n0.7232\n[21]\n44.71\n47.06\n!\n21.79\n0.7051\n23.35\n0.6895\n22.57\n0.6973\nEvLowlight [1]\n524.95\n15.49\n!\n20.57\n0.6217\n20.04\n0.6485\n20.31\n0.6351\nEvLight [2]\n180.90\n22.73\n!\n22.44\n0.7697\n23.21\n0.7505\n22.83\n0.7601\nSEENet (Ours)\n405.72\n1.90\n!\n22.54\n0.7756\n24.60\n0.7692\n23.57\n0.7724\ninput and output of one loop. After L iterations, the final\nfeature FL represents the BLR, as described by Eq. 4.\nFL = fse(Fe,F1)\n= fl(Fe,F1, fl(Fe,F1,...fl(Fe,F1,F1)))\n= fl\nÔ£´\nÔ£¨\nÔ£≠Fe,F1, fl (Fe,F1,..., fl (Fe,F1,F1))\n|\n{z\n}\nrecursive part\nÔ£∂\nÔ£∑\nÔ£∏\n(4)\nDecoding: The objective of this part is to decode the BLR\ninto a brightness-adjustable image Io. In designing this de-\ncoder, we focus on two key insights: (1) The decoding pro-\ncess should be pixel-wise and efficient, allowing for greater\nflexibility during model deployment; (2) The embedding\nof the brightness information should be thorough and fully\nintegrated. With these insights, we design the decoder with\nonly a 5-layer MLP as shown in Fig 6 (c). Our decoder\nbegins by encoding the brightness prompt B ‚àà(0,1) into an\nembedding vector. To effectively encode the high-frequency\nbrightness prompt into features that are easier for the network\nto learn [59], we introduce a learnable embedding, denoted\nas BBB = fpe(B) = fmlp(fcat(fmlp(B),B)), which consists of\ntwo MLP layers. Through this embedding, the brightness\nprompt B is transformed into a vector BBB, matching the dimen-\nsions of the BLR channels. We then integrate this embedding\nBBB into the decoder. To ensure the brightness prompt is fully\nincorporated and prevent information loss through multiple\nMLP layers, we employ a multi-step embedding approach,\nas displayed in Eq. 5, which guarantees that the brightness\nis progressively embedded throughout the decoding process.\nDuring the training phase, the prompt B is derived from the\nreference image by applying fb to calculate the global aver-\nage brightness. In contrast, during the testing phase, B can\nbe set freely, with a typical example being a value of 0.5.\nIo = fd(FL,BBB)\n= fmlp(BBB+ fmlp(BBB+...fmlp(BBB+FL)))\n= fmlp\nÔ£´\nÔ£¨\nÔ£¨\nÔ£¨\nÔ£¨\nÔ£¨\nÔ£¨\nÔ£¨\nÔ£≠\nBBB+ fmlp\nÔ£´\nÔ£¨\nÔ£≠BBB+¬∑¬∑¬∑+ fmlp (BBB+FL)\n|\n{z\n}\nfirst layer\nÔ£∂\nÔ£∑\nÔ£∏\n|\n{z\n}\nrecursive part\nÔ£∂\nÔ£∑\nÔ£∑\nÔ£∑\nÔ£∑\nÔ£∑\nÔ£∑\nÔ£∑\nÔ£∏\n.\n(5)\nLoss Function: The purpose of our loss function is to su-\npervise the prediction Io using the ground truth It, with the\ncorresponding brightness B = fb(It). The loss function con-\nsists of two main components: image reconstruction loss\nLi and gradient loss Lg. The image reconstruction loss is\nCharbonnier loss [60], which effectively handles both small\nand large errors. Additionally, we employ gradient loss to\nimprove the structural consistency of the output image. This\nis achieved by enforcing L1 constraints on the gradients of\nboth the output and ground truth images. Therefore, the\noverall loss function is formulated as a weighted sum of the\nimage loss and gradient loss, as exhibited in Eq. 6. Here, ‚àá\ndenotes the gradient operator, and Œª1 and Œª2 are the weights\nthat balance the contributions of two loss terms.\nL (Io,It) = Œª1Li +Œª2Lg\n= Œª1\nq\n(Io ‚àíIt)2 +Œµ2 +Œª2‚à•‚àáIo ‚àí‚àáIt‚à•.\n(6)\n6. Experiments\nImplementation Details: Our experiments use the Adam\noptimizer with an initial learning rate of 2e ‚àí4 for all the\n\n\nTable 3. Evaluation on the SEE-600K dataset, with methods trained on both the SDE and SEE-600k.\nTraining\nDataset\nMethods\nlow light\nhigh light\nnormal light\nPSNR\nSSIM\nL1\nPSNR\nSSIM\nL1\nPSNR\nSSIM\nL1\nSDE\nDCE [52]\n9.10\n0.0968\n0.3572\n6.26\n0.3419\n0.4649\n10.79\n0.3992\n0.2524\neSL Net [57]\n11.92\n0.3275\n0.2703\n6.66\n0.1672\n0.4001\n7.65\n0.2685\n0.3481\n[21]\n12.41\n0.4001\n0.2487\n5.53\n0.1950\n0.4534\n6.58\n0.2805\n0.4129\nEvLowLight [1]\n12.68\n0.4341\n0.2338\n4.11\n0.3071\n0.6062\n7.01\n0.3950\n0.4520\nEvLight [2]\n13.07\n0.4651\n0.2337\n5.12\n0.1005\n0.4842\n6.29\n0.2805\n0.4336\nSEENet\n14.84\n0.5693\n0.1779\n3.84\n0.2119\n0.6123\n5.36\n0.2980\n0.5056\nSEE\neSL Net [57]\n11.95\n0.3845\n0.2421\n12.84\n0.4660\n0.2076\n13.45\n0.5682\n0.1957\nEvLowLight [1]\n12.83\n0.4511\n0.2151\n12.79\n0.4696\n0.2084\n13.04\n0.5531\n0.2144\n[21]\n13.48\n0.5068\n0.1946\n12.30\n0.4766\n0.2221\n13.70\n0.5474\n0.2151\nEvLight [2]\n13.70\n0.5150\n0.1960\n13.45\n0.4918\n0.1990\n13.63\n0.5924\n0.2004\nSEENet\n18.77\n0.6303\n0.0971\n19.21\n0.6675\n0.0806\n20.92\n0.8002\n0.0606\n(a) Events\n(b) Inputs Image (Low-light)\n(c) Normal-light Image\n(d) SEE Net (Ours) 27.17/0.7761\n(e) Liu et. al. 15.98/0.6205\n(f) eSL Net 14.32/0.5304\n(g) EvLight 26.65/0.7145\n(h) SEE Net (Ours) Prompt 0.5\n(i) Events\n(j) Inputs Image (Low-light)\n(k) Normal-light Image\n(l) SEE Net (Ours) 26.85/0.8937\n(m) Liu et. al. 18.90/0.6032\n(n) eSL Net 17.30/0.5386\n(o) EvLight 16.63/0.4684\n(p) SEE Net (Ours) Prompt 0.5\nFigure 8. Visual examples of low-light enhancement and high-light recovery on the SEE-600K dataset.\nexperiments. We train our model for 40 epochs on the SDE\ndataset [2]. On the SEE-600K dataset, we train for only 20\nepochs, as SEE-600K is extremely large. All of our training\nis conducted on an HPC cluster, with a batch size of 2. To\nenhance data diversity, we apply random cropping to the\nimages and perform random flips and rotations.\nEvaluation Metrics: We maintain consistency with previ-\nous methods [1, 2] by using PSNR and SSIM [61]. However,\nsince our proposed new problem is highly challenging and\nmost current approaches perform poorly on our SEE-600K\ndataset, we additionally introduce the L1 distance as a refer-\nence.\nDataset:\nWe conduct experiments on two real-world\ndatasets: (1) SDE [2] comprises 91 scenes, with 76 for\ntraining and 15 for testing. Each scene includes a pair of\nlow-light and normal-light images along with their corre-\n\n\n(a) Events\n(b) Inputs Image\n(c) Normal-light Image\n(d) SEE Net\n25.33/0.7187/0.0401\n(e) AAAI\n19.88/0.6314/0.660\n(f) eSL\n21.99/0.6419/0.0567\n(g) EvLight\n24.49/0.5891/0.4008\n(h) SEE Net\n(Prompt = 0.5)\n(a) Events\n(b) Inputs Image\n(c) Normal-light Image\n(d) SEE Net\n30.43/0.8631/0.0207\n(e) AAAI\n24.68/0.7001/0.3553\n(f) eSL\n17.85/0.7042/0.0995\n(g) EvLight\n13.25/0.5436/0.2047\n(h) SEE Net\n(Prompt = 0.5)\n(a) Events\n(b) Inputs Image\n(c) Normal-light Image\n(d) SEE Net\n28.49/0.9284/0.0272\n(e) AAAI\n12.98/0.6892/0.1865\n(f) eSL\n16.61/0.7494/0.1307\n(g) EvLight\n24.65/0.9128/0.0477\n(h) SEE Net\n(Prompt = 0.5)\n(I)\nOutdoor\nHigh Light\nScene\n(II)\nOutdoor\nNormal Light\nScene\n(III)\nIndoor\nLow Light\nScene\nFigure 9. Visual examples of low-light enhancement and high-light recovery on the SEE-600K dataset.\n\n\n(a) Input Frame\n(b) Output Frames with Brightness Prompt ùêµ from 0.3 to 0.7\n(b) Output Frames with Brightness Prompt ùêµ from 0.3 to 0.7\n(d) Events\n(e) Input Frame\n(f) Reference \nFrame (GT)\n(g) Output Frames with Brightness Prompt ùêµ from 0.3 to 0.7\n(g) Output Frames with Brightness Prompt ùêµ from 0.3 to 0.7\nFigure 10. Visualization of brightness adjustment using varying brightness prompts B from 0.3 to 0.7, showing smooth brightness transitions\nin SEE-600K dataset.For more visualizations, see the Appendix.\nsponding events. (2) SEE-600K consists of 202 scenes, with\neach scene containing an average of four sets of videos under\ndifferent lighting conditions, ranging from low light to bright\nlight. During each training session, we randomly select one\nset of normal-light images as the reference and use the re-\nmaining sets as inputs. For example, for one scene with\none low-light, two normal-light, and one high-light set, we\ngenerate six pairs of training data.\n6.1. Comparisons Experiments\nComparative Methods: We categorize the competitive ap-\nproaches into four groups. Firstly, DCE [52] is a classical ap-\nproach that can adjust the image brightness curve to achieve\nnormal lighting. Secondly, there are strategies that only\nuse images as input, including SNR [53], UFormer [54],\nLLFlow [55], and RetinexFormer [23]. Thirdly, we con-\nsider methods that rely solely on events, e.g., E2VID+ [13].\nTertiary, we examine event-guided low-light enhancement\nframeworks. This group includes single-frame input meth-\nods, e.g., eSL-Net [57], [21], [56]and EvLight [2], as well as\nmulti-frame input strategies like EvLowLight [1]. Further-\nmore, we also compared the HDR reconstruction method\nHDRev [16]. We retrain all methods, following the open-\nsource code when available; for approaches without open-\nsource code, we replicate them based on their respective\npapers.\nComparative on SDE Dataset: The results from our com-\nparative experiments, shown in Tab. 2 and Fig. 7, reveal\nseveral key insights as following:\n(1) performance limitations of single-modal methods:\nMethods utilizing only a single modality exhibit limited\nperformance, as shown in Tab. 2. This trend highlights the\n\n\nneed to integrate both modalities to achieve improved results,\nas demonstrated by methods like DCE [52], SRN [53], and\nUFormer [54] in Tab.2. The employment of a single modality\ndoes not supplement the dynamic range information, leading\nto relatively poorer performance, as shown in Fig.7.\n(2) effectiveness of event-guided methods: In contrast,\nevent-guided image methods demonstrate significantly better\nperformance. Methods such as SEENet (Ours), EvLight, and\nothers, as seen in Tab. 2, demonstrate the best results. These\napproaches leverage the complementary strengths of both\nevents and traditional images, leading to better outcomes in\nlow-light conditions, as shown in Fig. 7 (g-j).\n(3) impact of indoor and outdoor conditions: Notably,\nperformance in low-light indoor scenarios is inferior to that\nin outdoor settings, as shown in Fig. 7 (I) and (III). This\ndiscrepancy may be attributed to the issues of flickering light\nsources commonly found indoors [62]. Our SEE-Net consis-\ntently achieves the best results across both scenarios, with\na model size of just 1.9M‚Äî1/10 parameter count of other\nSOTA methods‚Äîdemonstrating its efficiency and compact-\nness in low-light image enhancement.\n(4) denoising is a core challenge:\nFig. 7 present vi-\nsualizations from the low-light outdoor scenes of the SDE\ndataset. These low-light environments often come with sig-\nnificant noise, which poses a substantial challenge for current\nlow-light enhancement methods. Our method demonstrates\nstable performance in addressing these noisy scenes, ef-\nfectively enhancing the image quality while mitigating the\nnoise, thereby highlighting the robustness of our approach\nin handling complex low-light conditions.\nComparison on SEE-600K Dataset: The results presented\nin Tab. 3, Fig. 8, and Fig. 9 showcase the performance of\nvarious methods on the SEE-600K dataset under different\nlighting conditions.\n(1) Models trained on SDE: Models trained on the SDE\ndataset perform reasonably well when tested on the SEE-\n600K dataset, particularly under low-light conditions. No-\ntably, the DCE [52] method achieves the best performance\nin high-light scenarios, demonstrating its strong general-\nization capability, particularly for self-supervised learning\napproaches. However, in high-light and normal-light condi-\ntions, almost all methods fail, which highlights a significant\nlimitation. This indicates that models trained on the SDE\ndataset learn a fixed multiplicative enhancement for illumi-\nnation, and are not adaptable to a wider range of lighting\nconditions. This emphasizes the importance of redefining\nlow-light enhancement as an illumination adjustment prob-\nlem, as we have proposed.\n(2) Models trained on SEE-600K: In contrast, models\ntrained on the SEE-600K dataset show improved perfor-\nmance across both low-light and high-light conditions. Our\nproposed SEE-Net method outperforms other approaches, as\nshown in Tab. 3 and Fig. 8. This success can be attributed\nTable 4. Impact of Bayer Pattern Embedding. Removing the Bayer-\npattern embedding results in a drop in PSNR and SSIM, showing\nits contribution to accuracy but not as the most critical factor.\nBayer Pattern\nPSNR\nSSIM\nfpe\n23.57\n0.7724\nwo\n22.94\n0.7686\nTable 5. Impact of Encoding Strategy. Replacing cross-attention\nwith convolution-based approaches degrades performance, high-\nlighting the importance of cross-attention.\nEncoding\nPSNR\nSSIM\nfca\n23.57\n0.7724\nadd +conv\n22.40\n0.7224\ncat +conv\n22.84\n0.7298\nTable 6. Impact of Loop Iterations. Reducing the number of loops\nfrom 20 to 10 lowers performance, showing that sufficient iterations\nare necessary for refinement.\nLoop Iterations\nPSNR\nSSIM\n20\n23.57\n0.7724\n10\n22.18\n0.6812\nto our innovative use of prompt adjustments, which effec-\ntively resolve the ambiguities typically encountered in image\nenhancement processes. Further quantitative analysis con-\nfirms that SEE-Net consistently achieves superior PSNR,\nSSIM, and L1 loss scores across lighting conditions, further\nvalidating the effectiveness of our approach.\n(3) Advantages of prompt adjustments: Unlike previous\nmethods that rely on one-way mapping, our approach with\nprompt adjustments demonstrates significant advantages, as\nshown in Fig. 8 (h, p) and Fig. 9 (h) of (I, II, III). These\nadjustments allow us to generate images that surpass ground\ntruth quality in both low-light and high-light conditions,\nparticularly when the prompt is set to 0.5. In this setting,\nthe output achieves optimal brightness and sharper textures.\nFig. 8 and Fig. 9 further demonstrate the robustness and\nconsistency of SEE-Net. Notably, SEE-Net is capable of pro-\nducing more stable, high-quality images, with some outputs\neven surpassing the ground truth normal-light images.\nHowever, there are still challenges, as illustrated in Fig. 9.\nIn areas with complex textures or high-resolution require-\nments, all methods, including SEE-Net, struggle to achieve\noptimal results. Despite this, SEE-Net outperforms existing\nmethods, particularly in terms of maintaining image quality\nand stability. These insights highlight the strengths of SEE-\nNet while also pointing out areas for potential improvement\nin future research.\n\n\nTable 7. Impact of Prompt Embedding. Replacing the learned\nembedding with a sine function achieves similar performance but\ndoes not surpass the learned embedding.\nPrompt Embedding\nPSNR\nSSIM\nfpe\n23.57\n0.7724\nsin\n23.08\n0.7692\nTable 8. Impact of Prompt Merge. Disabling the prompt merge\nfunction slightly reduces performance, indicating its importance in\noptimizing results.\nPrompt Merge\nPSNR\nSSIM\nadd\n23.57\n0.7724\nadd (disabled)\n22.26\n0.7713\nTable 9. Impact of Multi-Prompt Adjustment. Changing the prompt\nmerge function from addition to multiplication yields the highest\nSSIM.\nPrompt Merge\nPSNR\nSSIM\nadd\n23.57\n0.7724\nmultiplication\n22.94\n0.7893\n6.2. Ablation and Analytical Experiments\nIn this ablation study, we analyze the impact of various\ncomponents.\n(1) bayer pattern embedding: Removing the Bayer-\npattern embedding leads to a noticeable drop in both PSNR\nand SSIM, as shown in Tab. 4. The performance decline\nhighlights that the Bayer pattern embedding contributes pos-\nitively to the accuracy of the model, but it is not the most\ncritical factor in performance. As seen in the table, the\nPSNR drops from 23.57 to 22.94 and the SSIM decreases\nfrom 0.7724 to 0.7686, indicating its moderate but important\ncontribution.\n(2) encoding strategy: Replacing the cross-attention mod-\nule fc with a convolution-based approach results in signifi-\ncant performance degradation, as seen in Tab. 5. The PSNR\nand SSIM decrease across both the ‚Äòadd‚Äò and ‚Äòconcat‚Äò con-\nvolution strategies compared to the use of cross-attention.\nSpecifically, replacing cross-attention with the ‚Äòadd + conv‚Äò\napproach drops PSNR from 23.57 to 22.40 and SSIM from\n0.7724 to 0.7224, while using ‚Äòcat + conv‚Äò performs slightly\nbetter but still results in lower performance (22.84 PSNR and\n0.7298 SSIM). These results emphasize the critical role of\ncross-attention in capturing complex features and enhancing\nimage quality.\n(3) loop iterations: Reducing the number of loop itera-\ntions from 20 to 10 results in a noticeable performance drop,\nas reflected in Tab. 6. The PSNR decreases from 23.57 to\n22.18, and the SSIM drops from 0.7724 to 0.6812, indicating\nthat sufficient iterations are necessary for effective refine-\nment. These findings underline the importance of multiple\niterations in optimizing image quality.\n(4) prompt embedding: Switching the prompt embedding\nfrom fpe to a sine function results in similar performance,\nbut it does not surpass the learned embedding. As shown in\nTab. 7, while both embeddings yield similar PSNR and SSIM,\nthe learned embedding outperforms the sine function method\nin terms of image quality, achieving a PSNR of 23.57 and\nSSIM of 0.7724 compared to 23.08 PSNR and 0.7692 SSIM\nfor the sine function. This shows that while sine functions\nprovide comparable results, the learned embedding offers a\nslight performance advantage. (5) prompt merge: Disabling\nthe prompt merge results in a slight performance decrease,\nas shown in Tab8. The PSNR drops from 23.57 to 22.26, and\nthe SSIM decreases from 0.7724 to 0.7713, demonstrating\nthat prompt merging is important for optimizing results. This\nsuggests that prompt merging plays a role in effectively\nintegrating the image and event features.\n(6) multi-prompt adjustment: Fig. 10 demonstrates the\neffect of multi-prompt adjustments. When using gamma\ncorrection to brighten the low-light input, significant noise\nis introduced, as shown in Fig. 10 (a). However, by adjust-\ning the prompt, we can effectively control the brightness\nwhile reducing noise, as seen in Fig. 10 (b). These results\nhighlight the flexibility and robustness of our method in\npost-processing, where multiple prompt adjustments provide\nfine-grained control over image enhancement.\n(7) Generalization of Relative Motion: Although our\ndataset is designed primarily to capture static scenes, it inher-\nently includes relative motion between objects. For instance,\nin a three-dimensional space, the nonlinear movement of\nthe robotic arm causes objects at varying distances from the\ncamera to move at different speeds, thus introducing motion\nin the image, as demonstrated in Fig. 11. In real-world\nscenarios, such relative motion is common, especially in\ndynamic environments. This relative motion enables our\nmethod to model local movements within the scene. This\ncharacteristic is crucial because it ensures that our method\ncan generalize effectively to scenarios where objects in the\nscene move at varying speeds relative to the camera.\n7. Conclusion\nIn this paper, we proposed a new research problem: how to\nuse events to adjust the brightness of images across a wide\nrange of lighting conditions, from low light to high light. To\naddress this challenge, we made the following contributions.\n‚Ä¢ (1), we developed a physical model and formally defined\nthe problem of brightness adjustment using events, provid-\ning a solid theoretical foundation.\n‚Ä¢ (2), we introduced a new spatiotemporal registration algo-\nrithm based on a robotic arm and collected a large-scale\ndataset, SEE-600K, to overcome alignment issues and\n\n\n(a) Events\n(b) Inputs Image (Low-light)\n(c) Normal-light Image\n(d) SEE Net (Ours) 25.69/0.7354\n(e) Liu et. al. 16.67/0.6226\n(f) eSL Net 14.36/0.5243\n(g) EvLight 24.59/0.6779\n(h) SEE Net (Ours) Prompt 0.5\nFigure 11. Performance when there is relative motion between the foreground and the background.\nsupport our research.\n‚Ä¢ (3), we presented SEE-Net, a novel and compact frame-\nwork capable of accepting input images with a wide range\nof illumination and producing output images with ad-\njustable brightness.\n‚Ä¢ (4), we conducted extensive experiments to demonstrate\nthe effectiveness of our method.\nLimitation and Future Works: Since this is the first attempt\nto address this novel problem, brightness adjustment with\nevents, our method has not yet reached a level suitable for\ndirect commercial application, and further improvements are\nneeded in future research.\nSupplementary Materials Overview\nIn the supplementary materials, we provide a more detailed\nintroduction to the imaging process of the images in Ap-\npendix A, and also analyze how events assist in adjusting\nthe imaging process. Since this section involves many funda-\nmental concepts and models related to the imaging process,\nwe expand on them in the Appendix.\nAdditionally, the detailed explanation and derivation of\nthe IMU algorithm are also presented in the supplementary\nmaterials, specifically in Appendix B, where Kalman filter-\ning is used to process the IMU data and subsequently design\nthe registration algorithm. We hope that this registration\nmethod can also be considered for use with other datasets.\nIn the supplementary video, we showcase additional ex-\namples from the dataset, as well as the results of different\nmethods applied to the videos. Additionally, we also present\na visualization of the registration process, making it easier\nfor readers to understand our IMU registration algorithm.\nReferences\n[1] J. Liang, Y. Yang, B. Li, P. Duan, Y. Xu, and B. Shi, ‚ÄúCoherent\nevent guided low-light video enhancement,‚Äù in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\n2023, pp. 10 615‚Äì10 625. 2, 3, 7, 8, 10, 11, 13\n[2] G. Liang, K. Chen, H. Li, Y. Lu, and L. Wang, ‚ÄúTowards\nrobust event-guided low-light image enhancement: A large-\nscale real-world event-image dataset and novel approach,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2024, pp. 23‚Äì33. 2, 3, 5, 7, 8,\n10, 11, 13\n[3] R. J. Koshel, Illumination Engineering: design with nonimag-\ning optics.\nJohn Wiley & Sons, 2012. 1\n[4] S. Lin, G. Zheng, Z. Wang, R. Han, W. Xing, Z. Zhang,\nY. Peng, and J. Pan, ‚ÄúEmbodied neuromorphic synergy for\nlighting-robust machine vision to see in extreme bright,‚Äù Na-\nture Communications, vol. 15, no. 1, p. 10781, 2024. 1\n[5] J. Bernacki, ‚ÄúAutomatic exposure algorithms for digital pho-\ntography,‚Äù Multimedia Tools and Applications, vol. 79, no. 19,\npp. 12 751‚Äì12 776, 2020. 1\n[6] L. Yuan and J. Sun, ‚ÄúAutomatic exposure correction of con-\nsumer photographs,‚Äù in Computer Vision‚ÄìECCV 2012: 12th\n\n\nEuropean Conference on Computer Vision, Florence, Italy,\nOctober 7-13, 2012, Proceedings, Part IV 12.\nSpringer,\n2012, pp. 771‚Äì785. 1\n[7] C. Li, C. Guo, L. Han, J. Jiang, M.-M. Cheng, J. Gu, and C. C.\nLoy, ‚ÄúLow-light image and video enhancement using deep\nlearning: A survey,‚Äù IEEE transactions on pattern analysis\nand machine intelligence, vol. 44, no. 12, pp. 9396‚Äì9416,\n2021. 1\n[8] J. J. McCann and A. Rizzi, The art and science of HDR\nimaging.\nJohn Wiley & Sons, 2011. 1\n[9] S. W. Hasinoff, D. Sharlet, R. Geiss, A. Adams, J. T. Barron,\nF. Kainz, J. Chen, and M. Levoy, ‚ÄúBurst photography for high\ndynamic range and low-light imaging on mobile cameras,‚Äù\nACM Transactions on Graphics (ToG), vol. 35, no. 6, pp.\n1‚Äì12, 2016. 1\n[10] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, ‚ÄúHigh\nspeed and high dynamic range video with an event camera,‚Äù\nIEEE transactions on pattern analysis and machine intelli-\ngence, vol. 43, no. 6, pp. 1964‚Äì1980, 2019. 1\n[11] D. Gehrig and D. Scaramuzza, ‚ÄúLow-latency automotive vi-\nsion with event cameras,‚Äù Nature, vol. 629, no. 8014, pp.\n1034‚Äì1040, 2024. 1\n[12] G. Gallego, T. Delbr¬®uck, G. Orchard, C. Bartolozzi, B. Taba,\nA. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Dani-\nilidis et al., ‚ÄúEvent-based vision: A survey,‚Äù IEEE transac-\ntions on pattern analysis and machine intelligence, vol. 44,\nno. 1, pp. 154‚Äì180, 2020. 1, 4\n[13] T. Stoffregen, C. Scheerlinck, D. Scaramuzza, T. Drummond,\nN. Barnes, L. Kleeman, and R. Mahony, ‚ÄúReducing the sim-\nto-real gap for event cameras,‚Äù in Computer Vision‚ÄìECCV\n2020: 16th European Conference, Glasgow, UK, August 23‚Äì\n28, 2020, Proceedings, Part XXVII 16.\nSpringer, 2020, pp.\n534‚Äì549. 1, 3, 10, 13\n[14] Z. Wang, Y. Lu, and L. Wang, ‚ÄúRevisit event generation\nmodel: Self-supervised learning of event-to-video recon-\nstruction with implicit neural representations,‚Äù arXiv preprint\narXiv:2407.18500, 2024. 1, 3\n[15] M. Cui, Z. Wang, D. Wang, B. Zhao, and X. Li, ‚ÄúColor event\nenhanced single-exposure hdr imaging,‚Äù in Proceedings of\nthe AAAI Conference on Artificial Intelligence, no. 2, 2024,\npp. 1399‚Äì1407. 1\n[16] Y. Yang, J. Han, J. Liang, I. Sato, and B. Shi, ‚ÄúLearning event\nguided high dynamic range video reconstruction,‚Äù in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 13 924‚Äì13 934. 10, 13\n[17] N. Messikommer, S. Georgoulis, D. Gehrig, S. Tulyakov,\nJ. Erbach, A. Bochicchio, Y. Li, and D. Scaramuzza, ‚ÄúMulti-\nbracket high dynamic range imaging with event cameras,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 547‚Äì557. 1, 5\n[18] O. T. Tursun, A. O. Aky¬®uz, A. Erdem, and E. Erdem, ‚ÄúThe\nstate of the art in hdr deghosting: A survey and evaluation,‚Äù\nin Computer Graphics Forum, no. 2.\nWiley Online Library,\n2015, pp. 683‚Äì707. 2\n[19] K. Chen, G. Liang, H. Li, Y. Lu, and L. Wang, ‚ÄúEvlight++:\nLow-light video enhancement with an event camera: A large-\nscale real-world dataset, novel method, and more,‚Äù arXiv\npreprint arXiv:2408.16254, 2024.\n[20] S. Jayasuriya, O. Iqbal, V. Kodukula, V. Torres, R. Likamwa,\nand A. Spanias, ‚ÄúSoftware-defined imaging: A survey,‚Äù Pro-\nceedings of the IEEE, vol. 111, no. 5, pp. 445‚Äì464, 2023.\n2\n[21] L. Liu, J. An, J. Liu, S. Yuan, X. Chen, W. Zhou, H. Li,\nY. F. Wang, and Q. Tian, ‚ÄúLow-light video enhancement\nwith synthetic event guidance,‚Äù in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 37, no. 2, 2023, pp.\n1692‚Äì1700. 2, 3, 10, 11, 13\n[22] Y. Jiang, Y. Wang, S. Li, Y. Zhang, M. Zhao, and Y. Gao,\n‚ÄúEvent-based low-illumination image enhancement,‚Äù IEEE\nTransactions on Multimedia, 2023. 2, 3, 7, 10\n[23] Y. Cai, H. Bian, J. Lin, H. Wang, R. Timofte, and Y. Zhang,\n‚ÄúRetinexformer: One-stage retinex-based transformer for low-\nlight image enhancement,‚Äù in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2023, pp.\n12 504‚Äì12 513. 3, 10, 13\n[24] Y. Zhang, X. Guo, J. Ma, W. Liu, and J. Zhang, ‚ÄúBeyond\nbrightening low-light images,‚Äù International Journal of Com-\nputer Vision, vol. 129, pp. 1013‚Äì1037, 2021. 3\n[25] W. Wu, J. Weng, P. Zhang, X. Wang, W. Yang, and J. Jiang,\n‚ÄúUretinex-net: Retinex-based deep unfolding network for low-\nlight image enhancement,‚Äù in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2022, pp. 5901‚Äì5910.\n[26] H. Fu, W. Zheng, X. Meng, X. Wang, C. Wang, and H. Ma,\n‚ÄúYou do not need additional priors or regularizers in retinex-\nbased low-light image enhancement,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 18 125‚Äì18 134. 3\n[27] X. Xu, R. Wang, and J. Lu, ‚ÄúLow-light image enhancement\nvia structure modeling and guidance,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 9893‚Äì9903. 3\n[28] Y. Wang, Z. Liu, J. Liu, S. Xu, and S. Liu, ‚ÄúLow-light image\nenhancement with illumination-aware gamma correction and\ncomplete image modelling network,‚Äù in Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n2023, pp. 13 128‚Äì13 137. 3\n[29] M. Afifi, K. G. Derpanis, B. Ommer, and M. S. Brown,\n‚ÄúLearning multi-scale photo exposure correction,‚Äù in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 9157‚Äì9167. 3\n[30] K. Panetta, S. K. KM, S. P. Rao, and S. S. Agaian, ‚ÄúDeep\nperceptual image enhancement network for exposure restora-\ntion,‚Äù IEEE Transactions on Cybernetics, vol. 53, no. 7, pp.\n4718‚Äì4731, 2022.\n[31] L. Ma, D. Jin, R. Liu, X. Fan, and Z. Luo, ‚ÄúJoint over and\nunder exposures correction by aggregated retinex propagation\nfor image enhancement,‚Äù IEEE Signal Processing Letters,\nvol. 27, pp. 1210‚Äì1214, 2020. 3\n[32] H. C. Duwek, A. Shalumov, and E. E. Tsur, ‚ÄúImage recon-\nstruction from neuromorphic event cameras using laplacian-\nprediction and poisson integration with spiking and artificial\nneural networks,‚Äù in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2021, pp.\n1333‚Äì1341. 3\n\n\n[33] L. Pan, C. Scheerlinck, X. Yu, R. Hartley, M. Liu, and Y. Dai,\n‚ÄúBringing a blurry frame alive at high frame-rate with an event\ncamera,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 6820‚Äì\n6829. 3\n[34] S. Liu and P. L. Dragotti, ‚ÄúSensing diversity and sparsity\nmodels for event generation and video reconstruction from\nevents,‚Äù IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 45, no. 10, pp. 12 444‚Äì12 458, 2023. 3\n[35] S. Zhang, Y. Zhang, Z. Jiang, D. Zou, J. Ren, and B. Zhou,\n‚ÄúLearning to see in the dark with events,‚Äù in Computer Vision‚Äì\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23‚Äì28, 2020, Proceedings, Part XVIII 16.\nSpringer,\n2020, pp. 666‚Äì682. 3, 7\n[36] H. Liu, S. Peng, L. Zhu, Y. Chang, H. Zhou, and L. Yan,\n‚ÄúSeeing motion at nighttime with an event camera,‚Äù in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2024, pp. 25 648‚Äì25 658. 3, 7\n[37] H. Jin, Q. Wang, H. Su, and Z. Xiao, ‚ÄúEvent-guided low\nlight image enhancement via a dual branch gan,‚Äù Journal of\nVisual Communication and Image Representation, vol. 95, p.\n103887, 2023. 3\n[38] S. K. Mendis, S. E. Kemeny, R. C. Gee, B. Pain, C. O. Staller,\nQ. Kim, and E. R. Fossum, ‚ÄúCmos active pixel image sensors\nfor highly integrated imaging systems,‚Äù IEEE Journal of Solid-\nState Circuits, vol. 32, no. 2, pp. 187‚Äì197, 1997. 3\n[39] T. Mertens, J. Kautz, and F. Van Reeth, ‚ÄúExposure fusion:\nA simple and practical alternative to high dynamic range\nphotography,‚Äù in Computer graphics forum, no. 1.\nWiley\nOnline Library, 2009, pp. 161‚Äì171. 4, 7, 20\n[40] G. Buchsbaum, ‚ÄúA spatial processor model for object colour\nperception,‚Äù Journal of the Franklin institute, vol. 310, no. 1,\npp. 1‚Äì26, 1980. 4, 20\n[41] R. Wang, X. Xu, C.-W. Fu, J. Lu, B. Yu, and J. Jia, ‚ÄúSeeing\ndynamic scene in the dark: A high-quality video dataset with\nmechatronic alignment,‚Äù in Proceedings of the IEEE/CVF\nInternational conference on computer vision, 2021, pp. 9700‚Äì\n9709. 5\n[42] A. Brey, J. J. Quintana, M. Diaz, and M. A. Ferrer,\n‚ÄúSmartphone-based control system for universal robot ur5e:\nA tool for robotics education,‚Äù in 2024 XVI Congreso de Tec-\nnolog¬¥ƒ±a, Aprendizaje y EnseÀúnanza de la Electr¬¥onica (TAEE).\nIEEE, 2024, pp. 1‚Äì5. 5\n[43] F. M. Mirzaei and S. I. Roumeliotis, ‚ÄúA kalman filter-based\nalgorithm for imu-camera calibration: Observability analysis\nand performance evaluation,‚Äù IEEE transactions on robotics,\nvol. 24, no. 5, pp. 1143‚Äì1156, 2008. 5, 21\n[44] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant\nkeypoints,‚Äù International journal of computer vision, vol. 60,\npp. 91‚Äì110, 2004. 7\n[45] M. Muja and D. G. Lowe, ‚ÄúFast approximate nearest neigh-\nbors with automatic algorithm configuration.‚Äù VISAPP (1),\nvol. 2, no. 331-340, p. 2, 2009. 7\n[46] M. A. Fischler and R. C. Bolles, ‚ÄúRandom sample consen-\nsus: a paradigm for model fitting with applications to image\nanalysis and automated cartography,‚Äù Communications of the\nACM, vol. 24, no. 6, pp. 381‚Äì395, 1981. 7\n[47] L. A. Hern¬¥andez-Flores, J. B. L¬¥opez-Mart¬¥ƒ±nez, J. J. Rosales-\nde-la Rosa, D. Aillaud-De-Uriarte, S. Contreras-GarduÀúno, and\nR. Cort¬¥es-Gonz¬¥alez, ‚ÄúAssessment of challenging oncologic\ncases: A comparative analysis between chatgpt, gemini, and a\nmultidisciplinary tumor board,‚Äù Journal of Surgical Oncology,\n2025. 7\n[48] T. Mertens, J. Kautz, and F. Van Reeth, ‚ÄúExposure fusion,‚Äù in\n15th Pacific Conference on Computer Graphics and Applica-\ntions (PG‚Äô07).\nIEEE, 2007, pp. 382‚Äì390. 7\n[49] S. Tulyakov, A. Bochicchio, D. Gehrig, S. Georgoulis, Y. Li,\nand D. Scaramuzza, ‚ÄúTime lens++: Event-based frame in-\nterpolation with parametric non-linear flow and multi-scale\nfusion,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 17 755‚Äì\n17 764. 8\n[50] C. Scheerlinck, H. Rebecq, T. Stoffregen, N. Barnes, R. Ma-\nhony, and D. Scaramuzza, ‚ÄúCed: Color event camera dataset,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, 2019, pp. 0‚Äì0. 8\n[51] R. Lukac, K. N. Plataniotis, and D. Hatzinakos, ‚ÄúColor image\nzooming on the bayer pattern,‚Äù IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 15, no. 11, pp. 1475‚Äì\n1492, 2005. 8\n[52] C. Guo, C. Li, J. Guo, C. C. Loy, J. Hou, S. Kwong, and\nR. Cong, ‚ÄúZero-reference deep curve estimation for low-light\nimage enhancement,‚Äù in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2020,\npp. 1780‚Äì1789. 10, 11, 13, 14\n[53] X. Xu, R. Wang, C.-W. Fu, and J. Jia, ‚ÄúSnr-aware low-light\nimage enhancement,‚Äù in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2022,\npp. 17 714‚Äì17 724. 10, 13, 14\n[54] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, and H. Li,\n‚ÄúUformer: A general u-shaped transformer for image restora-\ntion,‚Äù in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2022, pp. 17 683‚Äì\n17 693. 10, 13, 14\n[55] Y. Wu, C. Pan, G. Wang, Y. Yang, J. Wei, C. Li, and H. T.\nShen, ‚ÄúLearning semantic-aware knowledge guidance for low-\nlight image enhancement,‚Äù in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2023, pp. 1662‚Äì1671. 10, 13\n[56] Q. Wang, H. Jin, H. Su, and Z. Xiao, ‚ÄúEvent-guided attention\nnetwork for low light image enhancement,‚Äù in 2023 Interna-\ntional Joint Conference on Neural Networks (IJCNN).\nIEEE,\n2023, pp. 1‚Äì8. 10, 13\n[57] B. Wang, J. He, L. Yu, G.-S. Xia, and W. Yang, ‚ÄúEvent\nenhanced high-quality image recovery,‚Äù in Computer Vision‚Äì\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23‚Äì28, 2020, Proceedings, Part XIII 16.\nSpringer, 2020,\npp. 155‚Äì171. 8, 10, 11, 13\n[58] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Tim-\nofte, ‚ÄúSwinir: Image restoration using swin transformer,‚Äù in\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021, pp. 1833‚Äì1844. 8\n[59] A. Vaswani, ‚ÄúAttention is all you need,‚Äù Advances in Neural\nInformation Processing Systems, 2017. 10\n\n\n[60] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang, ‚ÄúFast\nand accurate image super-resolution with deep laplacian pyra-\nmid networks,‚Äù IEEE transactions on pattern analysis and\nmachine intelligence, vol. 41, no. 11, pp. 2599‚Äì2613, 2018.\n10\n[61] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli,\n‚ÄúImage quality assessment: from error visibility to structural\nsimilarity,‚Äù IEEE transactions on image processing, vol. 13,\nno. 4, pp. 600‚Äì612, 2004. 11\n[62] L. Xu, G. Hua, H. Zhang, L. Yu, and N. Qiao, ‚Äú‚Äù seeing‚Äù\nelectric network frequency from events,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 18 022‚Äì18 031. 14\n[63] A. Buades, B. Coll, and J.-M. Morel, ‚ÄúA review of image\ndenoising algorithms, with a new one,‚Äù Multiscale modeling\n& simulation, vol. 4, no. 2, pp. 490‚Äì530, 2005. 20\n[64] X. Li, B. Gunturk, and L. Zhang, ‚ÄúImage demosaicing: A\nsystematic survey,‚Äù in Visual Communications and Image\nProcessing 2008, vol. 6822.\nSPIE, 2008, pp. 489‚Äì503. 20\n[65] F. Gasparini and R. Schettini, ‚ÄúColor correction for digital\nphotographs,‚Äù in 12th International Conference on Image\nAnalysis and Processing, 2003. Proceedings.\nIEEE, 2003,\npp. 646‚Äì651. 20\n[66] P. Debevec and S. Gibson, ‚ÄúA tone mapping algorithm for\nhigh contrast images,‚Äù in 13th eurographics workshop on\nrendering: Pisa, Italy. Citeseer, vol. 2, 2002. 20\n\n\nAppendix\nA. Details for Research Problem Definition\nImaging is the process of capturing light from a scene, which\ncan be represented as a radiance field L(t) that varies over\ntime t. The intensity of ambient light in real-world environ-\nments spans a wide range, from approximately 0.1lux in\nlow-light conditions to over 1e6lux under bright sunlight.\nThe goal of our learning task is to accurately recover L(t)\nand transform it into a visual representation that is suitable\nfor human perception.\nSensor Signal Acquisition and Noise Modeling:\nCameras equipped with active pixel sensors record light\nsignals through an exposure process. During the exposure\ntime te, the sensor integrates incoming photons to produce\na voltage V. The number of photons k detected is a random\nvariable following a Poisson distribution due to the quantum\nnature of light:\nk ‚àºP(Œª),\nŒª = Œ∑\nZ\nte\nL(t)dt,\n(7)\nwhere:\n‚Ä¢ Œª is the expected number of photons,\n‚Ä¢ Œ∑ is the quantum efficiency of the sensor,\n‚Ä¢ L is the light intensity,\n‚Ä¢ te is the exposure time.\nThe voltage V generated by the sensor is proportional to\nthe number of detected photons and is given by:\nV = Gk +Nd,\n(8)\nwhere:\n‚Ä¢ G is the sensor gain, usually a circuit amplifier,\n‚Ä¢ Nd ‚àºN (¬µd,œÉ2\nd ) represents the dark current noise, typi-\ncally modeled as Gaussian noise with mean ¬µd and vari-\nance œÉ2\nd .\nThe RAW image intensity Iraw is obtained by quantize the\nvoltage V:\nIraw = Q(V) = Q(Gk +Nd),\n(9)\nwhere Q is the quantization function converting continuous\nvoltage signals into discrete digital values, typically ranging\nfrom 8 bits to 14 bits.\nImage Signal Processing (ISP)\nThe RAW image Iraw undergoes an image signal pro-\ncessing pipeline fisp that includes steps such as denois-\ning [63], demosaicing [64], color correction [65], and tone\nmapping [66] to produce the final RGB image:\nIrgb = fisp(Iraw).\n(10)\nCharacteristics of Accurate Exposure\nAn accurate exposure process aims to produce Irgb with\nthe following characteristics:\n1. Accurate Exposure: The mean pixel intensity of Irgb falls\nwithin a desirable range for human observation, typically\nnormalized between 0.4 and 0.7 [39]:\n0.4 ‚â§1\nN\nN\n‚àë\ni=1\nI(i)\nrgb ‚â§0.7,\n(11)\nwhere N is the total number of pixels.\n2. Noise-Free: The influences of dark current noise Nd and\nphoton shot noise Ns are minimized or eliminated:\nVar(Irgb) ‚âàVar(GŒ∑\nZ\nte\nL(t)dt),\n(12)\nimplying that the variance due to noise is negligible.\n3. Color Neutrality: The image has no color cast; the\ngrayscale values computed from each RGB channel are\napproximately equal [40]:\nfgray(Ir) ‚âàfgray(Ig) ‚âàfgray(Ib),\n(13)\nwhere Ir, Ig, and Ib are the red, green, and blue channels\nof Irgb, and fgray is a function mapping RGB values to\ngrayscale.\nLimitations of Traditional Cameras\nTraditional cameras have a limited dynamic range of ap-\nproximately 80dB, which often results in loss of detail in\nscenes with high contrast. Under extreme lighting conditions,\nimages may exhibit overexposed highlights or underexposed\nshadows, leading to insufficient edge and texture informa-\ntion.\nAdvantages of Event Cameras\nEvent cameras overcome these limitations by offering:\n‚Ä¢ High Dynamic Range: Greater than 120dB, allowing\nthem to handle extreme lighting variations.\n‚Ä¢ High Temporal Resolution: Less than 1ms, enabling\nthem to capture fast-changing scenes.\nEvent cameras operate asynchronously by detecting\nchanges in illumination at each pixel. The output is a stream\nof events, each represented as:\n(x,y,t, p),\n(14)\nwhere:\n‚Ä¢ (x,y) are the pixel coordinates,\n‚Ä¢ t is the timestamp,\n‚Ä¢ p ‚àà{+1,‚àí1} indicates the polarity (increase or decrease\nin light intensity).\nEvent Generation Mechanism\nAn event is generated at a pixel (x,y) when the change\nin the logarithm of the light intensity exceeds a predefined\nthreshold C:\n‚àÜL(x,y,t) = log(L(x,y,t))‚àílog(L(x,y,tk)) = pC,\n(15)\nwhere:\n\n\n‚Ä¢ L(x,y,t) is the light intensity at time t,\n‚Ä¢ tk is the timestamp of the last event at pixel (x,y),\n‚Ä¢ p is the polarity,\n‚Ä¢ C is the contrast sensitivity threshold.\nThis condition can also be expressed in terms of relative\nintensity change:\nL(x,y,t)\nL(x,y,tk) = epC.\n(16)\nProposed Model for Illumination Recovery\nGiven the high dynamic range and temporal resolution\nof event cameras, we aim to utilize an images Irgb and corre-\nsponding events E to recover the scene‚Äôs illumination L(t)\nand present it in a human-friendly format. However, due\nto the extensive theoretical range of L(t), we introduce a\nbrightness control prompt B to adjust the output image‚Äôs\nmean brightness.\nOur model is defined as:\nÀÜ\nIrgb = fsee(Irgb,E,B),\n(17)\nwhere:\n‚Ä¢ fsee is a function designed to enhance the input image Irgb\nusing the events E and adjust the brightness according to\nB,\n‚Ä¢ ÀÜIrgb is the output image with improved exposure,\n‚Ä¢ B is a user-defined parameter representing the desired\nmean brightness of ÀÜIrgb:\nB = 1\nN\nN\n‚àë\ni=1\nÀÜ\nI(i)\nrgb\n(18)\nBenefits of the Proposed Approach\n1. Robust Training: By presetting the parameter B during\nthe training phase, the model can mitigate biases present\nin the training dataset, leading to more generalized per-\nformance.\n2. Flexibility in Usage: During inference, setting B = 0.5\n(assuming pixel values are normalized between 0 and\n1) aligns with common exposure levels, but users can\nadjust B for creative control over the image‚Äôs brightness\nand exposure, enabling image adjustments and editing\ncapabilities.\nB. Temporal IMU Registration Algorithm\nIn this section, we provide a more detailed description of\nour IMU data registration algorithm, which aligns a source\nsequence S and a target sequence T by finding the optimal\ntemporal bias b and matching length l that minimize the dis-\ntance between them. Due to the high sampling rate of IMU\ndata (1000 Hz), an exhaustive search over all possible biases\nis computationally prohibitive. Therefore, we introduce a\nFigure 12. The IMU sensor is calibrated by leaving the sensor alone\nfor about one hour to obtain the deviations of the IMU in various\ndirections.\nTable 10. Calibration results showing biases, variances, and stan-\ndard deviations for each axis of the accelerometer and gyroscope.\nSensor\nAxis\nBias\nVariance\nStandard Deviation\nAccelerometer\nX\n‚àí0.009256\n5.836√ó10‚àí6\n0.002416\nAccelerometer\nY\n0.993344\n6.196√ó10‚àí6\n0.002489\nAccelerometer\nZ\n‚àí0.048622\n1.348√ó10‚àí5\n0.003672\nGyroscope\nX\n1.081781\n0.010550\n0.102711\nGyroscope\nY\n‚àí1.791223\n0.011102\n0.105365\nGyroscope\nZ\n‚àí0.697237\n0.011360\n0.106582\nmulti-level iterative strategy that efficiently approximates\nthe optimal alignment.\nIMU Data Calibration and Stability\nFig. 12 illustrates the calibration results of our IMU sen-\nsor over a one-hour period during which the sensor remained\nstationary. From this figure, we observe that the IMU‚Äôs mea-\nsurement errors are stable over long durations and do not\nincrease over time. The deviations in the accelerometer‚Äôs\nthree axes and the gyroscope‚Äôs three axes are consistent, indi-\ncating reliable sensor performance. Through calibration, we\ncorrected these biases during preprocessing to enhance mea-\nsurement accuracy. Specifically, for the camera used in our\ndataset collection, the calibrated IMU errors are quantified\nshown in Tab. 10. These low variance values indicate that\nthe IMU‚Äôs measurement noise is within an acceptable and\nsmall range, affirming that our calibration process effectively\ncorrects sensor deviations. Consequently, we can achieve\naccurate results in our data registration by leveraging the\nstability of the IMU sensor. The specific implementation\nsteps of our calibration process are detailed below.\nIMU Data Preprocessing with Kalman Filter\nWe first denoise the raw IMU data using a Kalman filter\n[43]. For each IMU sequence (source and target), we model\nthe system as:\nxk = Fxk‚àí1 +wk‚àí1,\n(19)\nzk = Hxk +vk,\n(20)\nwhere xk ‚ààR6 is the state vector at time k, consisting of\n\n\naccelerometer and gyroscope measurements:\nxk =\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\naccx\naccy\naccz\ngyrx\ngyry\ngyrz\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª\nk\n,\nF ‚ààR6√ó6 is the state transition matrix (identity matrix\nin our case), wk‚àí1 is the process noise with covariance Q,\nzk ‚ààR6 is the measurement vector, H is the observation\nmatrix (also identity), and vk is the measurement noise with\ncovariance R.\nThe Kalman filter recursively estimates the state xk by:\nPrediction Step:\nÀÜxk|k‚àí1 = FÀÜxk‚àí1|k‚àí1,\n(21)\nPk|k‚àí1 = FPk‚àí1|k‚àí1F‚ä§+Q,\n(22)\nUpdate Step:\nKk = Pk|k‚àí1H‚ä§(HPk|k‚àí1H‚ä§+R)‚àí1,\n(23)\nÀÜxk|k = ÀÜxk|k‚àí1 +Kk(zk ‚àíHÀÜxk|k‚àí1), (24)\nPk|k = (I‚àíKkH)Pk|k‚àí1,\n(25)\nwhere ÀÜxk|k is the estimated state at time k, Pk|k is the\nestimated covariance, and Kk is the Kalman gain.\nThe initial state ÀÜx0|0 is set to the first measurement, and\nthe initial covariance P0|0 is set to the identity matrix.\nMulti-Level Downsampling\nTo reduce computational complexity, we create two ad-\nditional levels of downsampled sequences using average\npooling:\n‚Ä¢ Level-1: Downsampled by a factor of s1.\n‚Ä¢ Level-2: Downsampled by a factor of s1 √ós2.\nThe downsampling is performed by averaging over non-\noverlapping windows of size si, for i = 1,2. For example,\nfor Level-1, the downsampled sequence S1 is obtained as:\nS1[n] = 1\ns1\nns1\n‚àë\nk=(n‚àí1)s1+1\nS[k],\nn = 1,2,...,\n\u0016LS\ns1\n\u0017\n,\n(26)\nwhere LS is the length of the original sequence S.\nHierarchical Bias Search\nAt each level, we perform a search for the optimal tempo-\nral bias b and matching length l that minimize the distance\nbetween the source and target sequences.\nDistance Metric\nWe define the distance between two sequences S and T\nover a matching window of length l as the mean Euclidean\ndistance between their accelerometer and gyroscope data:\ndacc(S,T;b,l) = 1\nl\nl\n‚àë\nk=1\n‚à•aS[k +b]‚àíaT[k]‚à•2 ,\n(27)\ndgyr(S,T;b,l) = 1\nl\nl\n‚àë\nk=1\n‚à•gS[k +b]‚àígT[k]‚à•2 ,\n(28)\nwhere aS[k] and gS[k] are the accelerometer and gyro-\nscope measurements of sequence S at time k, respectively.\nCoarse Search at Level-2\nAt the lowest resolution (Level-2), we perform a coarse\nsearch over a large range of biases b:\nb ‚àà[bmin,bmax],\n(29)\nwhere bmin and bmax are chosen based on the expected\nmaximum temporal misalignment.\nFor each candidate bias b, we compute the distances dacc\nand dgyr and record the bias that minimizes these distances:\nb(2)\nacc = argmin\nb dacc(S2,T2;b,lb),\n(30)\nb(2)\ngyr = argmin\nb dgyr(S2,T2;b,lb),\n(31)\nwhere lb is the matching length at bias b, determined by\nthe overlapping length of the sequences after applying the\nbias.\nRefined Search at Level-1 and Level-0\nUsing the biases obtained at Level-2 as center points,\nwe perform refined searches at higher resolutions (Level-\n1 and Level-0). The search ranges at each higher level are\nnarrowed down around the biases found at the previous level:\nb(i)\nmin = b(i+1) ‚àíŒ¥ (i),\n(32)\nb(i)\nmax = b(i+1) +Œ¥ (i),\ni = 1,0,\n(33)\nwhere Œ¥ (i) is a small range that depends on the downsam-\npling factor.\nAt each level, we update the biases:\nb(i)\nacc = arg\nmin\nb‚àà[b(i)\nmin,b(i)\nmax]\ndacc(Si,Ti;b,lb),\n(34)\nb(i)\ngyr = arg\nmin\nb‚àà[b(i)\nmin,b(i)\nmax]\ndgyr(Si,Ti;b,lb),\n(35)\nfor i = 1,0.\nOptimal Bias and Alignment\nAfter performing the refined searches, we obtain the opti-\nmal biases b(0)\nacc and b(0)\ngyr at the original data level (Level-0).\n\n\nWe choose the final bias b‚àóand matching length l‚àóbased on\nthe minimum distances:\nb‚àó= median(b(0)\nacc,b(0)\ngyr),\n(36)\nl‚àó= min(LS ‚àíb‚àó,LT),\n(37)\nwhere LS and LT are the lengths of the source and target\nsequences, respectively.\nThe source and target sequences are then aligned by shift-\ning the source sequence by b‚àóand taking the first l‚àósamples:\nSaligned[k] = S[k +b‚àó],\nk = 1,2,...,l‚àó;\n(38)\nTaligned[k] = T[k],\nk = 1,2,...,l‚àó.\n(39)\nAlgorithm Summary\nThe overall algorithm can be summarized as follows:\n1. Apply Kalman filter to denoise the source and target IMU\nsequences.\n2. Downsample the sequences to create Level-1 and Level-2\nversions.\n3. At Level-2, perform a coarse search over a wide range of\nbiases to find initial estimates b(2)\nacc and b(2)\ngyr.\n4. At Level-1, perform a refined search around b(2) to obtain\nb(1).\n5. At Level-0, perform a final refined search around b(1) to\nobtain the optimal biases b(0)\nacc and b(0)\ngyr.\n6. Compute the final bias b‚àóand matching length l‚àó.\n7. Align the source and target sequences using b‚àóand l‚àó.\nImplementation Details\nIn our implementation, we set the downsampling factors\nto s1 = 10 and s2 = 10, resulting in Level-1 and Level-2\nsequences downsampled by factors of 10 and 100, respec-\ntively.\nThe search ranges at each level are defined as:\nLevel-2: b ‚àà[‚àíbmax,bmax],\nbmax = 100,\n(40)\nLevel-1: b ‚àà[b(2) ‚àí10s1,b(2) +10s1],\n(41)\nLevel-0: b ‚àà[b(1) ‚àí10s0,b(1) +10s0],\n(42)\nwhere s0 = 1 is the downsampling factor at Level-0 (orig-\ninal data).\nComputational Efficiency\nBy employing the multi-level hierarchical search, we\nsignificantly reduce the computational complexity compared\nto an exhaustive search at the original sampling rate. At\nLevel-2, the coarse search over a wide range of biases is\nfeasible due to the reduced sequence length. The refined\nsearches at higher resolutions are limited to small ranges\naround the biases found at lower levels, ensuring that the\ntotal computational cost remains manageable. Visualization\nof the Alignment Results\nFig. 13, Fig. 14 and Fig. 15 showcase the IMU registra-\ntion results for two trajectories. The high degree of overlap\nbetween the two IMU streams after alignment demonstrates\nthe effectiveness of our proposed method.\n\n\n(a) IMU Data w/o Registration\n(b) IMU Data w Registration\nFigure 13. Original IMU data and registered IMU data.\n\n\n(a) IMU Data w/o Registration\n(b) IMU Data w Registration\nFigure 14. Original IMU data and registered IMU data.\n\n\n(a) IMU Data w/o Registration\n(b) IMU Data w Registration\nFigure 15. Original IMU data and registered IMU data.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21120v1.pdf",
    "total_pages": 26,
    "title": "SEE: See Everything Every Time -- Adaptive Brightness Adjustment for Broad Light Range Images via Events",
    "authors": [
      "Yunfan Lu",
      "Xiaogang Xu",
      "Hao Lu",
      "Yanlin Qian",
      "Pengteng Li",
      "Huizai Yao",
      "Bin Yang",
      "Junyi Li",
      "Qianyi Cai",
      "Weiyu Guo",
      "Hui Xiong"
    ],
    "abstract": "Event cameras, with a high dynamic range exceeding $120dB$, significantly\noutperform traditional embedded cameras, robustly recording detailed changing\ninformation under various lighting conditions, including both low- and\nhigh-light situations. However, recent research on utilizing event data has\nprimarily focused on low-light image enhancement, neglecting image enhancement\nand brightness adjustment across a broader range of lighting conditions, such\nas normal or high illumination. Based on this, we propose a novel research\nquestion: how to employ events to enhance and adaptively adjust the brightness\nof images captured under broad lighting conditions? To investigate this\nquestion, we first collected a new dataset, SEE-600K, consisting of 610,126\nimages and corresponding events across 202 scenarios, each featuring an average\nof four lighting conditions with over a 1000-fold variation in illumination.\nSubsequently, we propose a framework that effectively utilizes events to\nsmoothly adjust image brightness through the use of prompts. Our framework\ncaptures color through sensor patterns, uses cross-attention to model events as\na brightness dictionary, and adjusts the image's dynamic range to form a broad\nlight-range representation (BLR), which is then decoded at the pixel level\nbased on the brightness prompt. Experimental results demonstrate that our\nmethod not only performs well on the low-light enhancement dataset but also\nshows robust performance on broader light-range image enhancement using the\nSEE-600K dataset. Additionally, our approach enables pixel-level brightness\nadjustment, providing flexibility for post-processing and inspiring more\nimaging applications. The dataset and source code are publicly available\nat:https://github.com/yunfanLu/SEE.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}