{
  "id": "arxiv_2502.21315v1",
  "text": "Identifying Emerging Concepts in Large Corpora\nSibo Ma\nJulian Nyarko\nStanford University\nsiboma@stanford.edu\njnyarko@stanford.edu\nAbstract\nWe introduce a new method to identify emerg-\ning concepts in large text corpora. By analyz-\ning changes in the heatmaps of the underlying\nembedding space, we are able to detect these\nconcepts with high accuracy shortly after they\noriginate, in turn outperforming common alter-\nnatives. We further demonstrate the utility of\nour approach by analyzing speeches in the U.S.\nSenate from 1941 to 2015. Our results suggest\nthat the minority party is more active in intro-\nducing new concepts into the Senate discourse.\nWe also identify specific concepts that closely\ncorrelate with the Senators’ racial, ethnic, and\ngender identities. An implementation of our\nmethod is publicly available.\n1\nIntroduction\nThe identification of new ideas and concepts within\nlarge corpora is of core interest both in compu-\ntational linguistics, the social sciences, and the\nhumanities. For instance, Hofstra et al. (2020)\ndiscover new innovations in a corpus of scien-\ntific articles. They find that minorities often in-\ntroduce novel contributions to the scientific dis-\ncourse, but those innovations are disproportionately\ndiscounted. Charlesworth et al. (2022) examine\nhow new stereotypes towards ethnic and racial mi-\nnorities evolved during the 19th and 20th centuries.\nAnd Hanley et al. (2024) identify misinformation\nat its conception and track its spread and influence\non public discourse.\nRecent advances in natural language processing\noffer more robust alternatives. Transformer-based\nmodels, such as BERT (Devlin et al., 2019) and\nT5 (Raffel et al., 2020), leverage contextual embed-\ndings to encode semantic meaning beyond individ-\nual words, making them well-suited for tracking\nemerging concepts that lack stable lexical forms.\nHowever, most existing methodologies for text\nanalysis are not specifically designed for identi-\nfying emergent concepts. Instead, they often apply\ngeneral-purpose techniques that do not account for\nthe distinct temporal patterns associated with con-\nceptual emergence.\nAs a consequence, existing approaches often\nlack sensitivity, allowing for the accurate identifica-\ntion of emerging concepts only after these concepts\nhave become well-represented in the underlying\ncorpora. To the extent that methods exist that are\nspecifically targeted at emerging concept identifi-\ncation, they tend to be supervised (Charlesworth\net al., 2022; Kulkarni et al., 2015), thus requiring\nthe investigator to know ex ante what new con-\ncepts to look for. They also tend to operate at\nthe word-level, thus making it impossible to iden-\ntify concepts that are not easily representable us-\ning a distinct unigram or bigram. But consistent\nwith the semantic perspective, we understand con-\ncepts as abstract objects—propositions that can be\nexpressed in a multitude of ways (Margolis and\nLaurence, 2007). While some concepts can be\ncaptured by individual words or phrases (e.g., “cli-\nmate change”), we assume that many concepts are\nmore complex and cannot be reduced to a single\nlexical unit (e.g., “negative sentiment towards the\nAffordable Care Act as a form of governmental in-\nvasion” (Fisher and Larsen, 2019)). This in turn re-\nquires a methodological approach that goes beyond\nthe word-level and is able to capture the broader\nsemantic structures that define emerging concepts.\nIn this paper, we introduce a novel methodology\nto identify emerging concepts in large text corpora\nthat we also make publicly available.1 Intuitively,\nour methodology leverages heatmaps of the embed-\nding space to identify those regions that are subject\nto sudden, long-lasting increases in density. Be-\ncause we benchmark the density increase against\ncommonly observed, non-systematic changes, our\nmethod performs particularly well at identifying\n1An implementation can be found at https://github.\ncom/Crabtain959/new_concept_detection.\narXiv:2502.21315v1  [cs.CL]  28 Feb 2025\n\n\nemerging concepts shortly after their inception,\nwell before they become broadly represented in\nthe underlying corpus. Our method operates at the\nsentence level (or any other, larger textual unit),\nthus allowing for identification of complex con-\ncepts that cannot easily be captured with a distinct\nword or phrase.\nIn several evaluations, we demonstrate the per-\nformance of our proposed methodology, and com-\npare it to other approaches that have been employed\nto identify emerging concepts in text. In a last step,\nwe illustrate the utility of our method for the social\nsciences and digital humanities by exploring the\nintroduction of new concepts in the U.S. Senate\ndebates from 1941 (77th Congress) to 2015 (114th\nCongress). At a macro level, we find a consistent\npattern showing that the minority party introduces\nnew concepts and ideas with greater frequency\nthan the majority. This finding lends support to\nclaims by other scholars about strategically differ-\nent behavior of the minority party in legislative bod-\nies (Jenkins et al., 2023; BALLARD and CURRY,\n2021), including their way to converse (Pozen et al.,\n2019).\n2\nRelated Work\nPerhaps closest in spirit to our motivation is a re-\ncent study by Vicinanza et al. (2022). They identify\nnovel ideas using word-level perplexity, such that\nsentences with unexpected word combinations are\ndeemed to reflect novelty. They provide convinc-\ning evidence that their approach identifies novel\nideas at a macro level. In their approach, syntac-\ntic idiosyncracies are receiving the same weight as\nunusual semantic occurrences. And since it oper-\nates at the word level, there are likely limits to the\nlinguistic complexity with which new ideas can be\nidentified.\nIn contrast, our approach centers around textual\nembeddings, a literature with a long tradition. A\nnumber of early contributions interested in the iden-\ntification of new concepts characterized the prob-\nlem in terms of diachronic meaning shift detection.\nThat is, a new concept would be identified by the\nemergence of a new word sense for a given word.\nNotably, Hamilton et al. (2016) demonstrated that\nword meaning evolution follows predictable trajec-\ntories, influenced by frequency and semantic drift,\nhighlighting the importance of tracking concept\nemergence over time. Naturally, the associated\nmethodologies are centered around the use of word\nembedding models. Several studies have explored\ndifferent approaches to detecting semantic change,\nincluding cosine distance between word embed-\ndings (Del Tredici et al., 2019; Kulkarni et al.,\n2015; Shoemark et al., 2019), Bayesian models\nfor temporal word representations (Frermann and\nLapata, 2016), and topic modeling techniques such\nas Latent Dirichlet Allocation (LDA) (Lau et al.,\n2012; Blei et al., 2001). Other refinements incorpo-\nrate contextual embeddings (Martinc et al., 2020)\nor focus on modeling semantic drift in historical\ncorpora (Periti et al., 2024; Boholm et al., 2024).\nSurveys such as Periti and Montanelli (2024) pro-\nvide an overview of these methodologies and their\nrelative strengths and limitations.\nDespite their effectiveness in tracking known\nshifts, these methods face two key limitations. First,\nsince they analyze shifts at the word-level, they are\nnot well-suited to detect emerging concepts that\ncannot easily be associated with a single word or\nphrase (Stewart and Eisenstein, 2018; Hofmann\net al., 2020). Second, these methods are supervised\nin the sense that the investigator needs to prede-\nfine the list of words for which a new meaning\nmay emerge. In that way, they are better suited\nto identify when a known semantic shift has oc-\ncurred. However, they are not aimed at identifying\npreviously unknown, emergent concepts.\nMore recent approaches have relied on the detec-\ntion of concepts by applying clustering algorithms\nto text embeddings (Sia et al., 2020; Giulianelli\net al., 2020; Angelov, 2020; Grootendorst, 2022).\nThese unsupervised methods do not require pre-\ndefined knowledge about the changing concepts,\nmaking them better-suited to detect unknown con-\ncepts in large corpora. However, clustering meth-\nods are insensitive to the temporal nature of the\ndata generating process, thus preventing them from\ntaking into account the dynamic features that ac-\ncompany an emergent concept. In addition, the\nmost commonly employed clustering methods like\nHDBSCAN (Campello et al., 2013) and KMeans\nvariants (Sia et al., 2020; Ikotun et al., 2023) suffer\nfrom parameter selection and difficulty in control-\nling sensitivity and specificity (Hanley et al., 2024).\nFor instance, the widely used HDBSCAN algo-\nrithm (Campello et al., 2013) (see, e.g. Grooten-\ndorst, 2022) is very sensitive to its core parameters,\nincluding the minimum number of points required\nto form a cluster, the minimum number of samples\nin a neighborhood for a point to be considered a\ncore point, and the distance threshold for merging\n\n\nclusters. Only most recently have researchers be-\ngun to adapt these approaches by examining the\ntemporal or usage changes within clusters (Hanley\net al., 2024), but the current clustering methods\nlack the sensitivity to capture more nuanced topics.\nWe will further evaluate this in section 4.1.\nOur method was inspired by these clustering ap-\nproaches and aims to improve upon them by using\nblob detection with Laplacian of Gaussian Filter-\ning (Kong et al., 2013) on differences of heatmaps.\nThis technique mitigates the complexity of parame-\nter selection and enhances control over robustness\nand sensitivity. Instead of clustering all data points\nacross time periods together and analyzing data\ncomposition within each cluster, we track the de-\nvelopment of concepts by detecting and examining\nregions of high, novel density in the embedding\nspace. By focusing on differences between time pe-\nriods, this approach removes noise and allows for\nmore accurate and sensitive detection of emerging\nconcepts.\n3\nMethod\nThis section outlines our method to detect emerging\nconcepts. In doing so, we follow Vicinanza et al.\n(2022) and define new linguistic concepts as those\nthat emerge as separate and distinct from existing\ndiscourse and remain with some permanence.\nOur process involves several key steps: (1) em-\nbedding sentences to capture their semantic fea-\ntures, (2) reducing the dimensionality of these em-\nbeddings and generating heatmaps to summarize\nthe embedding space and its distribution, (3) detect-\ning significant changes among the heatmaps with\nblob detection on the differences among heatmaps,\nand (4) tracking the progression of these changes\nand interpreting them as newly emerging concepts.\nEach step is designed to ensure that the analysis is\nboth comprehensive and efficient, and gives infor-\nmative results for downstream analysis. Figure 1\nillustrates our pipeline, which is explained in more\ndetail in the following subsections.\n3.1\nSentence Embedding and Dimensionality\nReduction\nWe use the MPNet model (Song et al., 2020) to\nembed all the sentences, capturing their semantic\nfeatures2.\n2When this study started in 2023, MPNet was the most\nsuitable and well-performing model for our task. It ranked\namong the top models for MTEB Clustering (Muennighoff\net al., 2023), alongside ST5-XXL. Given our objective of\nTo utilize the resulting embeddings for the cre-\nation of a heatmap, we first reduce their dimensions\nvia Uniform Manifold Approximation and Projec-\ntion (UMAP) (McInnes et al., 2020) with default\nparameters3. We opt for UMAP due to its ability\nto maintain the global topological structure of the\ndata while reducing its dimensionality. It operates\nby constructing a high-dimensional graph represen-\ntation of the data, which is then approximated in a\nlower-dimensional space. UMAP optimizes a cost\nfunction which is based on a cross-entropy between\nthe distances in the high-dimensional space and\ntheir representation in the low-dimensional space.\nThis approach allows UMAP to balance attention\nbetween local and global structures, ensuring that\nsimilar points remain close to each other in the re-\nduced space while also appropriately modeling the\nbroader dataset topology.\nTo further ensure embeddings are mapped con-\nsistently, we fit the UMAP on all the sentence em-\nbeddings, ensuring that the topological structure is\nmaintained and thus guarantees that embeddings\nwill be mapped consistently. This consistency is\ncrucial for subsequent analysis and comparison\nbecause we will create heatmaps and compare ab-\nsolute data positions across different periods.\nWe choose the target dimension of n = 2, which\nminimizes memory and computation requirements\nwhile still providing results that are informative\nenough. We discuss the tradeoff between computa-\ntional efficiency and the richness of the representa-\ntion in more detail in the next section.\n3.2\nHeatmap Generation\nFor each time period, we take the reduced-\ndimension embeddings of each year and create\nn-dimensional histograms as heatmaps.\nThese\nheatmaps represent the density and distribution\nof sentence embeddings over time in a compact\nmanner, which serves as an initial stage of general-\nization.\nThe creation of the heatmaps follows the follow-\nensuring broad accessibility and usability of our pipeline for\nsocial scientists, we selected MPNet over ST5-XXL, as it\nprovides comparable performance while being substantially\nmore computationally efficient.\n3During early experimentation, we found that final re-\nsults are not very sensitive to the parameters. Especially\nafter subtracting heatmaps, different parameters gave differ-\nence heatmaps with similarly clear patterns for blob detection.\nSince the cost-performance ratio was not high, we decided\nto illustrate our pipeline with the default parameters. A ro-\nbustness test with different parameters can be found in subsec-\ntion A.4, yielding similar results\n\n\nFigure 1: Overview of our approach for detecting new topics. Texts are embedded and processed into lower-\ndimensional heatmaps. The heatmap on the left visualizes an example distribution of embeddings after dimension-\nality reduction. Next, heatmap subtraction removes existing patterns, leaving new regions of high density. The\nheatmap on the right shows the embeddings after subtraction, with blobs representing new concepts. These blobs\nare then detected and linked to form cohesive new concepts, which are labeled in the final output (e.g., Burma\nCampaign, WWII Pacific Battles, Japanese Exclusion Act Protests)\ning process:\n1. Define the Range: Set the range of each di-\nmension from the minimum to the maximum\nvalue of all embeddings in that dimension.\n2. Create Bins: Split each dimension into m\nparts, resulting in mn bins. We choose m =\n400 to balance granularity and generalization,\nresulting in 4002 = 160000 bins in our set-\ntings.\n3. Fill Bins: For each embedding, determine the\ncorresponding bin based on its coordinates\nand count the number of embeddings in each\nbin to determine the density.\nIf a new concept appears in period p, it should\nnot be significantly present before p, but should be\nprominent in and after p for a certain number of\nperiods. Our parameters are designed to accom-\nmodate concepts with varying characteristics (e.g.,\nduration of prominence), and their specific choices\nare detailed in subsection 3.5. To capture this tem-\nporal change, we first select a set of R reference\nheatmaps Mp−r (1 ≤r ≤R) from periods pre-\nceding p. The reference heatmaps are summarized\nby taking, for each bin, the maximum value across\nMp−r, denoted as RMp. By imposing constraints\non RMp, we can guarantee that a new concept has\nnot been discussed frequently in any of the previous\nperiods.\nRMp(i, j) = max\n1≤r≤R Mp−r(i, j),\n(1)\nTo mitigate the influence of different density\nscales across periods, we normalize each heatmap\nMp+w with respect to the reference heatmap RMp.\nWe then subtract RMp from the heatmaps Mp+w\n(0 ≤w < W) corresponding to the subsequent\nW periods. The normalization and subtraction are\nshown in Equation 2. This subtraction highlights\nvariations and shifts in sentence embeddings, ef-\nfectively illustrating changes in semantic content\nover time within the window of W periods. To\nfocus on emerging patterns, we set negative values\nto zero, as they reflect disappearance. The result-\ning heatmaps, termed difference heatmaps DMp,w,\nemphasize regions with significant changes relative\nto the summarized reference periods RMp.\nDMp,w = max\n \n0,\nP\ne∈RMp e\nP\ne∈Mp+w eMp+w −RMp\n!\n(2)\n3.3\nBlob Detection\nNext, we identify emerging concepts using blob\ndetection. We use the Laplacian of Gaussian (LoG)\nmethod (Kong et al., 2013) to detect blobs4 on\nthe difference heatmaps DMp,w, 1 ≤w ≤W,\nfor each reference period p. Blobs, which are re-\ngions in the difference heatmaps where significant\nchanges in density occur, indicate new concepts\nemerging after reference period p.\nTechnical Details on LoG when DMp,w is 2-\ndimensional:\n1. Gaussian Kernel:\nThe Gaussian kernel is used to smooth the\ndifference heatmaps DM(x, y). It is defined\n4We use the python package scikit-image to detect\nblobs (Van der Walt et al., 2014).\n\n\nas the following:\nG(x, y; σ) =\n1\n2πσ2 e−x2+y2\n2σ2\n(3)\nwhere, x and y are spatial coordinates, and σ\nrepresents the standard deviation of the Gaus-\nsian kernel, which controls the smoothing\nstrength. Larger values of σ produce stronger\nsmoothing, affecting a broader area around\neach bin.\n2. Scale-Space Representation:\nThe difference heatmaps DM(x, y) are con-\nvolved with the Gaussian kernel G(x, y; σ) to\nproduce a scale-space representation:\nL(x, y; σ) = G(x, y; σ) ∗DM(x, y)\n(4)\nThe result, L(x, y; σ), is referred to as the\nscale-space representation of the difference\nheatmap DM(x, y). This operation blends\nthe bin values with the degree of blending de-\nfined by the Gaussian kernel. It reduces vari-\nations and noise in the difference heatmaps,\nmaking the resulting heatmaps more robust to\nnoise.\n3. Laplacian Operator:\nThe Laplacian operator is applied to the scale-\nspace representation to identify regions where\nthe intensity changes rapidly. The Laplacian\noperator is a second-order differential operator\nin two dimensions, calculated as the sum of\nthe second derivatives of L with respect to x\nand y (noted as Lxx and Lyy, respectively).\nThis operator measures the rate at which the\nfirst derivatives change, providing a way to\ncapture regions of rapid intensity change in\nthe image, which often correspond to edges\nor, as is relevant here, blob-like structures.\n∇2L = Lxx + Lyy = ∂2L\n∂x2 + ∂2L\n∂y2\n(5)\n4. Blob Detection: Local maxima above a calcu-\nlated minimum peak intensity in the Laplacian\nresponse are identified as blobs. The mini-\nmum peak intensity is determined by multi-\nplying the maximum intensity by a relative\nthreshold, referred to as ρ∗. The threshold\nensures that only prominent features are de-\ntected as blobs, allowing for robust detection\nof important structures.\n3.4\nBlob Linking\nTo track the development of detected blobs over\ntime, we link blobs across different years, form-\ning a temporal graph. For each blob bp,w,i in a\nperiod w with a reference period p, we identify\nblobs bp,w−q,j, with 1 ≤q ≤Q, in the earlier pe-\nriod that are within a certain threshold of distance,\nconnect them, and add bp,w,i to the graphs that end\nwith bp,w−q,j. If no close blobs in the earlier period\nare found, we initialize a graph with bp,w,i being\nthe start. This process creates a list of networks\nof blobs for each reference period, grouping sen-\ntences with similar semantic features and showing\nthe progression and transformation of significant\nsemantic regions over time. The pseudo-code for\nblob linking is presented in Algorithm 1.\n3.5\nParameter Choice\nOur parameters are set to identify suddenly emerg-\ning, long-living concepts under the assumption that\nthose are the most significant. However, those with\ninterest in identifying other temporal patterns may\nopt for a different set of parameters. For instance,\nresearchers who are interested in including fad-\ndish concepts into the analyses may choose to set\nthe window size to a small value (e.g. W = 1\nor W = 2). Separately, an investigator might be\ninterested in identifying concepts that are being re-\ndiscovered. This can be achieved by using a large\nwindow size W and a large blob distance Q (e.g.\nsetting W = 30 and Q = 20 to detect concepts\nthat appeared within 30 periods and but across up\nto 20 periods). More generally, blob linkage and\nparameter choice allow our approach to be flexi-\nbly adjusted to identify various temporal patterns.\nExamples of faddish concepts and rediscovered\nconcepts are included in subsection 5.1.\nBeyond these task-dependent parameters, practi-\ncal applications often involve limited prior knowl-\nedge about the dataset or the specific concepts of\ninterest. A common strategy, therefore, is to de-\ntect concepts over-inclusively and to then filter out\nfalse positives. For example, as discussed in sub-\nsection 4.2 and section 5, we choose a small value\nρ∗= 0.05 to permissively detect the blobs. The\nblob linking step helps mitigate noise by consider-\ning only those blobs that appear in similar regions\nacross multiple periods. Further post-processing\ncan also refine results by filtering linked blobs\nbased on additional criteria, such as only keeping\nthe linkage of blobs with a relatively large thresh-\n\n\nold of number of sentences when one is interested\nin concepts that are relatively popular.\n4\nEvaluation\nWe next turn to evaluating our algorithm, contrast-\ning its performance to the popular alternatives in-\ntroduced by Grootendorst (2022) and Hanley et al.\n(2024). A holistic evaluation of new concept detec-\ntion is inherently difficult and prohibitively costly,\nfor at least three reasons: First, there is little una-\nnimity or consistency in defining the outer con-\ntours of a concept. Two experts might disagree,\nfor instance, on whether the Pacific Battle during\nWorld War 2 is a separate concept from the at-\ntack on Pearl Harbor, or whether one is a smaller\nconcept inside the other. Second–and relatedly–\nvalidating the existence of an emerging concept\nmight require extensive domain expertise. Prior\npapers have often validated their approaches us-\ning historical corpora (Vicinanza et al., 2022; Giu-\nlianelli et al., 2020), but it can be difficult for an\nuntrained human evaluator to identify complex and\nnuanced concepts without sharp contours, such as\nthose promoting economic mobility.5 In effect, this\nmeans that the generation of human labels can be\nprohibitively costly, especially in contexts where\ndomain expertise is required. Third, creating a com-\nprehensive list of all concepts in any given domain\nis infeasible due to the sheer number of concep-\ntual developments. This makes it impossible for\nresearchers to assess Recall with any significant\nreliability.\nAlthough it is not possible for us to remedy\nthese shortcomings directly, we are taking a multi-\npronged approach to alleviate concerns as much as\npossible. In doing so, we take the following steps:\nOur first evaluation uses synthetic data. In the\nsynthetic dataset, we are able to randomize the\nbaseline corpus, introducing new concepts artifi-\ncially in a controlled way. Randomization ensures\nthat the baseline corpus–in expectation–does not\ninclude any coherent concepts, thus allowing us to\nevaluate both Precision and Recall of our approach.\nThat said, a synthetic corpus has the shortcoming\nthat it does not represent documents that were cre-\nated through a realistic data generating process,\nthreatening external validity of the results.\n5For instance, our later analysis reveals emergent concepts\nin Senate debates surrounding the development of the work-\nforce through trainings and education within economically\ndisadvantaged communities.\nFigure 2: Comparison of F1 scores for three clustering\nalgorithms: Our Approach (red), HDBSCAN (blue),\nand DPMeans (green) as the size of new topics in-\ncreases.\nWe thus complement the evaluation on a syn-\nthetic corpus with an assessment using real data,\nspecifically the Corpus of Historical American En-\nglish (COHA) (Davies, 2022).\nAs pointed out\nabove, COHA does not allow us to holistically eval-\nuate Recall because no dataset exists that contains\nthe complete set of historical concepts. In addition,\nwe cannot evaluate Precision holistically because\nthere are simply too many concepts to have each\nverified by domain experts. We thus limit our eval-\nuation on the real data to the identification of a\nlimited set of concepts that do not require extensive\ndomain knowledge for evaluation: Important world\nevents.\n4.1\nSynthetic Dataset\nWe start by evaluating our pipeline on a syn-\nthetic dataset.\nTo that end, we collected a\nnumber of keywords contained in the WikiPSE\ndataset (Yaghoobzadeh et al., 2019) and fulfill three\nconditions: (1) The word had already acquired a\ndistinct meaning in 1910, (2) since 1910, the word\nacquired an additional, new meaning, and (3) the\nword is well-represented in COHA (N ≥500). For\ninstance, the word mouse did historically describe a\nsmall rodent, but in 1964, acquired a new meaning\nas a hand-held, pointing computer device. Table 1\ncontains the full list of keywords.\nNext, we create a baseline corpus comprised of\n207,413 texts sourced from newspapers and maga-\nzines published between 1900 and 1911, available\nin COHA. To ensure that this baseline corpus does\nnot contain any temporally correlated, emerging\nconcepts, we then randomize the sentences in our\nbaseline corpus across years. In a next step, we\nuse GPT-4 to generate n sentences containing each\n\n\nselected keyword with their new meanings listed in\nTable 16. The generated sentences are then divided\ninto ten equal sets and introduced into the dataset\nfrom 1901 to 1911. Our process ensures that the\nnew concepts in this synthetic dataset exclusively\ncontain, and are limited to, the new meanings of\nthe 8 keywords we manually introduce7. While\nthe synthetic dataset is constructed around specific\nkeywords, our analysis is conducted at the sentence\nlevel, with embeddings capturing broader semantic\ncontent beyond individual words. To assess the per-\nformance along different sizes of the new topic, we\nvary n from 10 to 200, treating 1900 as the refer-\nence year and applying our pipeline with a window\nsize of W = 10 and Q = 1. The parameter Q en-\nsures that only topics present across all 10 periods\nare detected, aligning with how we construct the\nsynthetic topics.\nFigure 2 depicts our performance as F1 scores\nover the different sizes of new concepts,8 and com-\npares this performance to the clustering proposals\nby Grootendorst (2022) and Hanley et al. (2024).\nAs mentioned above, one disadvantage of these al-\nternatives is their sensitivity to the individual model\nparameters. In order to avoid biasing results in our\nfavor, we used Bayesian Optimization (Snoek et al.,\n2012) to optimize the model parameters for each\napproach at each concept size separately.\nAs can be seen, our pipeline consistently out-\nperforms the two alternatives. The differences are\nespecially pronounced for very small concept sizes.\nThis is consistent with our hypothesis that blob\ndetection is effective in capturing temporal changes,\nas new concepts shortly after inception tend to be\nsmaller and thus more easily detected with our\napproach\nIn Figure 5, we further examine the robustness\nof our model to the choice of different ρ∗, which\ndenotes the threshold for the minimum relative in-\ntensity of the peak brightness during blob detection.\nAs can be seen, with ρ∗∈(0.2, 0.4), our approach\n6The prompt for generating the sentences can be found\nin subsection A.7.\n7We use 1 NVIDIA A10G GPU for sentence embedding\nand parallelize the subsequent pipeline steps across 5 AMD\nEPYC 7R32 CPUs. The entire pipeline completes in 40 min-\nutes, with sentence embedding accounting for the majority of\nthe computational time.\n8For Recall, a true positive is defined as there being at\nleast one identified new concept with the new meaning of\nthe keyword. For Precision, each new concept with the new\nmeaning of the keyword is a true positive. For example, if the\nkeyword cool is split into 2 subconcepts, then they count as 1\ntrue positive for Recall and 2 true positives for Precision.\nyields good Precision and Recall across different\nsizes of new topics.\nA qualitative inspection of the results reveals that\nour approach often further splits keywords into co-\nherent subconcepts. For instance, the keyword cool\nis split into related sub-concepts that describe attire\nand those that describe behavior, like cool dance\nmoves. These first results suggest the pipeline out-\nperforms common alternatives in detecting changes\nin the semantic landscape, and is able to distinguish\nbetween closely related sub-concepts.\n4.2\nCOHA Dataset\nTo validate the adaptability and effectiveness of our\npipeline in more complex, real-world scenarios, we\nextended our analysis to the entire COHA dataset\nspanning from 1900 to 2000 with 2,870,795 sen-\ntences.\nFor each year p from 1900 to 2000, we analyze\nthe subsequent W = 10 of years to detect new\nconcepts that emerged at least twice from p + 1 to\np + W 9.\nGiven the broad temporal span and the diverse\nnature of content over a century, our pipeline is ex-\npected to capture a wide array of subtle and gradual\nsemantic shifts in this evaluation, which renders\nit impossible to evaluate Precision or Recall holis-\ntically. Due to the absence of a definitive list of\nemergent concepts over the 20th century, we uti-\nlized a chronology of significant political events\nto assess performance on. The list of historical\nevents used in our analysis and the criteria for their\nselection are detailed in Appendix A.6.\nOur pipeline successfully detects all referenced\nevents, though some, such as those related to World\nWar I and World War II, are fragmented into multi-\nple distinct sub-concepts, such as the Pearl Harbor\nattack and the Burma Campaign.\n5\nApplication: Emerging Concepts in the\nU.S. Congress\nTo illustrate the utility of our proposed pipeline in\na real-world scenario, we employ it to analyze U.S.\nSenate speeches derived from the U.S. Congres-\nsional Record (Matthew Gentzkow, 2018) from\n1941 (77th Congress) to 2015 (114th Congress),\nwhich includes a total of 2,254,427 speeches10.\n9We set our blob detection parameter to ρ∗= 0.05 for a\nmore permissive detection as mentioned in subsection 3.5.\n10We include only speeches from senators and filter out\nprocedural boilerplates, such as expressions of gratitude and\nrequests for unanimous consent, as they do not contribute\n\n\nFigure 3: Changes in topic size (number of sentences\ncontained in a topic) over time for Judicial Activism and\nMarriage Laws. Discussions first emerged in the 1950s\nand 1960s, with a first major spike in 1989, followed by\na series of peaks from 1995 to 2005.\nFigure 4: Proportion of new partisan concepts intro-\nduced by each party. The red line shows, among all\nRepublican speeches, the proportion of speeches dis-\ncussing newly introduced, partisan concepts (i.e. con-\ncepts for which there is an overrepresentation of Re-\npublican speeches). The blue line shows the same for\nDemocratic speeches. The shaded areas indicate periods\nof party majority: red for Republican majority and blue\nfor Democratic majority.\nOur method uncovers emergent concepts and pat-\nterns that are not readily detectable using conven-\ntional approaches that predominantly focus on syn-\ntactic feature extraction. We analyze these concepts\nboth at the macro level by party ideology and at\nthe micro level by Senator identity. As such, our\nfindings contribute to a broader literature on view-\npoint diversity at the intersection of NLP and poli-\ntics (Fridkin and Kenney, 2014; Paul et al., 2010;\nNémeth, 2023).\nFigure 6 provides summary statistics on ide-\nological, gender, and racial/ethnic identity rep-\nresentation, both in terms of personnel and in\nterms of speech, during our period of observation.\nThese show that, unsurprisingly, the proportion of\nspeeches closely tracks representation in Congress.\nInterestingly, the results show that women senators\ninitially only rarely spoke in the Senate. Indeed,\nmeaningful content to the concepts of interest\nfrom 1970 to 1990, the proportion of such speeches\nwas close to 0, although the share of women sena-\ntors increased steadily over that time period. Even\ntoday, women speak disproportionately less in the\nSenate than men. We observe a similar trend for\nracial minority senators, which we define as sena-\ntors with Asian, Black, Hispanic, Native American,\nor Pacific Islander identity.\n5.1\nIllustration of Topic Evolution and\nConcept Types\nAn illustration of concept evolution using our\npipeline is shown in Figure 3. In the topic we\ndetected, discussions on Judicial Activism and\nMarriage Laws first appeared in the 1950s–1960s.\nThe first major spike in 1989 suggests increased\nlegal debates on relationship recognition.\nThe\n1995–2005 surge reflects growing attention to mar-\nriage definitions and judicial influence.\nTo detect faddish and rediscovered concepts, we\nconfigure parameters as described in subsection 3.5,\nsetting W = 1 for short-lived fads and W = 30,\nQ = 20 for rediscovered topics. We identified a fad\nin 1943 on agricultural labor deferments, debating\nfarmworker exemptions from military service amid\nWWII labor shortages. Meanwhile, a rediscovered\ntopic on employment discrimination emerged in\n1961, resurfacing in 1987 and 2013. The sentences\nhad several focal points, including racial equality,\nresponses to legal rulings, and workplace protec-\ntions for LGBTQ+ individuals.\n5.2\nMinority Party’s Innovative Discourse\nAt a macro-level, we analyze how the introduction\nof new, partisan concepts in the Senate correlates\nwith party ideology over time.\nTo ensure a focus on substantive concepts, we\nexclude speeches with an average sentence length\nof less than 300 characters.11 We then compute,\nfor each year,\nQp,y =\nP\nt :\nSp,t\nSt > Np\nN\nSp,t\nSp\n(6)\nwhere Sp,t is the number of speeches by party p\non concept t, Sp is the total number of speeches by\nparty p, Np is the number of senators from party\n11We set our parameters to ρ∗= 0.05, consistent with sub-\nsection 4.2. Our parameters W = 10 and Q = 3 are more per-\nmissive than in our synthetic analysis (W = 10 and Q = 1)\nunder the assumption that the underlying data is not as clean\nas the synthetic data, and so new concepts might disappear\nintermittently for short periods.\n\n\np, St is the total number of speeches on concept t,\nand N is the total number of senators.\nIntuitively, our measure Qp,y captures the pro-\nportion of new partisan concepts introduced by\neach party, ignoring new concepts that do not show\na partisan leaning.\nFigure 4 illustrates our findings. Although the\ninitial years do not show a conclusive pattern12,\nwith the beginning of the Civil Rights Era in the\n50s and 60s, we observe a trend showing that each\nparties’ Senators become more active in the intro-\nduction of partisan concepts when they are in the\nminority13. This is in stark contrast to our descrip-\ntive findings in Figure 6, which have shown that\nthe general volume of speeches tracks party repre-\nsentation. In the next subsection, we further break\nthese concepts down by party affiliation, among\nothers.\nPozen et al. (2019) found that Constitutional dis-\ncourse in Congress is often shaped by the minority.\nIn particular, they suggest that the minority party\nstrategically employs the Constitution to strengthen\nits arguments against the majority. Our findings,\nalthough necessarily limited given their context,\nlend at least suggestive evidence to the hypothesis\nthat such patterns might characterize the discourse\nin the Senate in a more fundamental way. In par-\nticular, despite speaking less, the minority party\nappears to use its allotted time strategically to shift\nthe discourse towards new ideas and discourse.\n5.3\nNew Concepts and Identity\nWe complement the preceding macro-level analysis\nwith a micro-level analysis of new concept introduc-\ntion and ideological, gender, and racial/ethnic iden-\ntity in the Senate. Specifically, for each Congress,\nwe treat all preceding Congresses as reference pe-\nriods and analyze the emergence of new concepts\nin the subsequent 5 Congresses, covering a span of\n10 years.14\n12This may, at least in part, be a consequence of the fact\nthat novelty is assessed against prior speeches, and the stock\nof prior speeches is thin in early years.\n13Although Senate control shifted multiple times during the\n107th Congress (2001–2003), Democrats held the majority for\nthe longest continuous period (June 6, 2001–November 12,\n2002). The initial Republican majority (January 20–June 6,\n2001) resulted from Vice President Cheney’s tie-breaking vote,\nwhile the post-election Republican majority (November 12,\n2002–January 3, 2003) was not formally reorganized during\nthe Senate recess. Given this, we classify the 107th Congress\nas Democratic majority.\n14Due to the large number of concepts, we relied on GPT-4o\nto generate summaries, which we then checked selectively to\nconfirm accuracy. The prompt we used is presented in A.7\nWe find that the new concepts with a dispropor-\ntionate representation of women senators center\naround concepts such as climate change and envi-\nronmental policy, health care accessibility, and en-\nergy markets. The top 20 detected concepts (based\non how strongly they overrepresent women) are\nincluded in Section A.8.\nAt the same time, racial minority Senators in-\ntroduce new impulses around the preservation of\nfundamental benefits like access to healthcare and\neducation for indigenous and marginalized commu-\nnities, civil rights protections, community safety,\nand immigration reform. The top 20 detected con-\ncepts (based how strongly they overrepresent mi-\nnorities) are included in Section A.9.\nRepublican senators introduced new concepts\naround the military and national security, the rising\nfederal debt, and cold war relations with the So-\nviet Union and Spain, among others. Democratic\nsenators instead set new impulses regarding envi-\nronmental policy, small business protection, and\nhuman rights. The top 20 detected concepts (based\non how strongly they overrepresent Republicans\nand Democrats) are included in Section A.10 and\nSection A.11.\n6\nConclusion\nWe have introduced a new, unsupervised method-\nology to identify emerging concepts in large text\ncorpora. Our approach is able to identify new con-\ncepts shortly after their inception, before they be-\ncome deeply entrenched in the discourse. In doing\nso, we hope our efforts contribute to recent devel-\nopments that leverage computational linguistics to\nsupport new discoveries, especially within the so-\ncial sciences and digital humanities (Grimmer et al.,\n2022).\n7\nLimitations\nAlthough our method demonstrates high perfor-\nmance in detecting emerging concepts, there are\nnecessarily limitations to our approach.\nFor one, although our method relies on an in-\ntuitive parameter ρ∗,\nFigure 5 shows that per-\nformance can be sensitive to this parameter. To\nmitigate concerns arising from this sensitivity, we\nadopt a permissive selection of ρ∗, setting it to a\nlow threshold to maximize the capture of poten-\ntial emerging concepts. While this reduces the risk\nof missing meaningful patterns, it may also intro-\nduce false positives, requiring additional filtering\n\n\nor refinement. In addition, given the absence of a\ncomprehensive list of concepts in any real-world\ncorpus, our evaluations are limited to assessing\neither synthetic data (with potentially limited ex-\nternal validity) or a limited notions of recall in real\ndata (our world events). Finally, the concepts our\nmethod identifies may require manual review to\ndetect and filter false positives, or to merge con-\nceptual distinctions that are too nuanced for the\nrelevant inquiry. Although tools such as LLMs can\nbe employed to facilitate this task, it may still be\nassociated with significant costs.\nReferences\nDimo Angelov. 2020. Top2vec: Distributed representa-\ntions of topics. Preprint, arXiv:2008.09470.\nANDREW O. BALLARD and JAMES M. CURRY.\n2021. Minority party capacity in congress. American\nPolitical Science Review, 115(4):1388–1405.\nDavid Blei, Andrew Ng, and Michael Jordan. 2001.\nLatent dirichlet allocation. volume 3, pages 601–\n608.\nMax Boholm, Björn Rönnerstrand, Ellen Breitholtz,\nRobin Cooper, Elina Lindgren, Gregor Rettenegger,\nand Asad Sayeed. 2024. Can political dogwhistles\nbe predicted by distributional methods for analysis\nof lexical semantic change? In Proceedings of the\n5th Workshop on Computational Approaches to His-\ntorical Language Change, pages 144–157.\nRicardo J. G. B. Campello, Davoud Moulavi, and Jo-\nerg Sander. 2013. Density-based clustering based\non hierarchical density estimates. In Advances in\nKnowledge Discovery and Data Mining, pages 160–\n172, Berlin, Heidelberg. Springer Berlin Heidelberg.\nTessa E. S. Charlesworth,\nAylin Caliskan,\nand\nMahzarin R. Banaji. 2022. Historical representations\nof social groups across 200 years of word embed-\ndings from google books. Proceedings of the Na-\ntional Academy of Sciences, 119(28):e2121798119.\nMark Davies. 2022. Corpus of Historical American\nEnglish (COHA).\nMarco Del Tredici, Raquel Fernández, and Gemma\nBoleda. 2019. Short-term meaning shift: A distri-\nbutional exploration. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2069–2075, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJeffrey L Fisher and Allison Orr Larsen. 2019. Vir-\ntual briefing at the supreme court. Cornell L. Rev.,\n105:85.\nLea Frermann and Mirella Lapata. 2016. A Bayesian\nmodel of diachronic meaning change. Transactions\nof the Association for Computational Linguistics,\n4:31–45.\nKim L Fridkin and Patrick J Kenney. 2014. How the\ngender of us senators influences people’s understand-\ning and engagement in politics. The Journal of Poli-\ntics, 76(4):1017–1031.\nMario Giulianelli, Marco Del Tredici, and Raquel Fer-\nnández. 2020. Analysing lexical semantic change\nwith contextualised word representations. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3960–\n3973, Online. Association for Computational Lin-\nguistics.\nJ. Grimmer, M.E. Roberts, and B.M. Stewart. 2022. Text\nas Data: A New Framework for Machine Learning\nand the Social Sciences. Princeton University Press.\nMaarten Grootendorst. 2022.\nBertopic:\nNeural\ntopic modeling with a class-based tf-idf procedure.\nPreprint, arXiv:2203.05794.\nWilliam L. Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016. Diachronic word embeddings reveal statisti-\ncal laws of semantic change. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1489–1501, Berlin, Germany. Association for Com-\nputational Linguistics.\nHans W. A. Hanley, Deepak Kumar, and Zakir Du-\nrumeric. 2024. Specious sites: Tracking the spread\nand sway of spurious news stories at scale. Preprint,\narXiv:2308.02068.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2020. Predicting the growth of morpholog-\nical families from social and linguistic factors. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7273–\n7283, Online. Association for Computational Lin-\nguistics.\nBas Hofstra, Vivek V. Kulkarni, Sebastian Munoz-Najar\nGalvez, Bryan He, Dan Jurafsky, and Daniel A. Mc-\nFarland. 2020. The diversity–innovation paradox in\nscience. Proceedings of the National Academy of\nSciences, 117(17):9284–9291.\nAbiodun M. Ikotun, Absalom E. Ezugwu, Laith Abuali-\ngah, Belal Abuhaija, and Jia Heming. 2023. K-means\n\n\nclustering algorithms: A comprehensive review, vari-\nants analysis, and advances in the era of big data.\nInformation Sciences, 622:178–210.\nJeffery A. Jenkins, Nathan W. Monroe, and Tessa\nProvins. 2023. Toward a theory of minority-party in-\nfluence in the u.s. congress: whip counts, amendment\nvotes, and minority leverage in the house. Journal of\nPublic Policy, 43(4):722–740.\nHui Kong, Hatice Cinar Akakin, and Sanjay E. Sarma.\n2013. A generalized laplacian of gaussian filter for\nblob detection and its applications. IEEE Transac-\ntions on Cybernetics, 43(6):1719–1733.\nVivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and\nSteven Skiena. 2015. Statistically significant detec-\ntion of linguistic change. In Proceedings of the 24th\nInternational Conference on World Wide Web, WWW\n’15, page 625–635, Republic and Canton of Geneva,\nCHE. International World Wide Web Conferences\nSteering Committee.\nJey Han Lau, Paul Cook, Diana McCarthy, David New-\nman, and Timothy Baldwin. 2012. Word sense in-\nduction for novel sense detection. In Proceedings\nof the 13th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages\n591–601, Avignon, France. Association for Compu-\ntational Linguistics.\nEric Margolis and Stephen Laurence. 2007. The ontol-\nogy of concepts-abstract objects or mental represen-\ntations? Noûs, 41(4):561–593.\nMatej Martinc, Petra Kralj Novak, and Senja Pollak.\n2020. Leveraging contextual embeddings for detect-\ning diachronic semantic shift. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 4811–4819, Marseille, France. European\nLanguage Resources Association.\nMatt Taddy Matthew Gentzkow, Jesse M. Shapiro. 2018.\nCongressional record for the 43rd-114th congresses:\nParsed speeches and phrase counts.\nLeland McInnes, John Healy, and James Melville.\n2020.\nUmap: Uniform manifold approximation\nand projection for dimension reduction. Preprint,\narXiv:1802.03426.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and\nNils Reimers. 2023. MTEB: Massive text embedding\nbenchmark. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 2014–2037, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nRenáta Németh. 2023. A scoping review on the use of\nnatural language processing in research on political\npolarization: trends and research prospects. Journal\nof computational social science, 6(1):289–313.\nMichael Paul, ChengXiang Zhai, and Roxana Girju.\n2010. Summarizing contrastive viewpoints in opin-\nionated text. In Proceedings of the 2010 conference\non empirical methods in natural language processing,\npages 66–76.\nFrancesco Periti and Stefano Montanelli. 2024. Lexical\nsemantic change through large language models: a\nsurvey. ACM Comput. Surv., 56(11).\nFrancesco Periti, Sergio Picascia, Stefano Montanelli,\nAlfio Ferrara, and Nina Tahmasebi. 2024. Studying\nword meaning evolution through incremental seman-\ntic shift detection. Language Resources and Evalua-\ntion, pages 1–37.\nDavid Pozen, Eric L. Talley, and Julian Nyarko. 2019. A\ncomputational analysis of constitutional polarization.\n(3351339).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nPhilippa Shoemark, Farhana Ferdousi Liza, Dong\nNguyen, Scott Hale, and Barbara McGillivray. 2019.\nRoom to Glo: A systematic comparison of semantic\nchange detection approaches with word embeddings.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 66–76,\nHong Kong, China. Association for Computational\nLinguistics.\nSuzanna Sia, Ayush Dalmia, and Sabrina J. Mielke.\n2020. Tired of topic models? clusters of pretrained\nword embeddings make for fast and good topics too!\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 1728–1736, Online. Association for Computa-\ntional Linguistics.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\n2012. Practical bayesian optimization of machine\nlearning algorithms. Advances in neural information\nprocessing systems, 25.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan\nLiu. 2020. Mpnet: masked and permuted pre-training\nfor language understanding. In Proceedings of the\n34th International Conference on Neural Information\nProcessing Systems, NIPS ’20, Red Hook, NY, USA.\nCurran Associates Inc.\nIan Stewart and Jacob Eisenstein. 2018. Making “fetch”\nhappen: The influence of social and linguistic context\non nonstandard word growth and decline. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4360–4370,\nBrussels, Belgium. Association for Computational\nLinguistics.\nStefan Van der Walt, Johannes L Schönberger, Juan\nNunez-Iglesias,\nFrançois Boulogne,\nJoshua D\nWarner, Neil Yager, Emmanuelle Gouillart, and Tony\nYu. 2014. scikit-image: image processing in python.\nPeerJ, 2:e453.\n\n\nPaul Vicinanza, Amir Goldberg, and Sameer B Srivas-\ntava. 2022. A deep-learning model of prescient ideas\ndemonstrates that they emerge from the periphery.\nPNAS Nexus, 2(1):pgac275.\nYadollah Yaghoobzadeh, Katharina Kann, T. J. Hazen,\nEneko Agirre, and Hinrich Schütze. 2019. Probing\nfor semantic classes: Diagnosing the meaning con-\ntent of word embeddings. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5740–5753, Florence, Italy. Asso-\nciation for Computational Linguistics.\nA\nAppendix\nA.1\nEffect of Varying Blob Detection\nParameter\nFigure 5 illustrates how different values of ρ∗, the\nthreshold controlling the minimum peak intensity\nfor a blob to be identified, affect the pipeline’s\nperformance. The plot examines Precision and\nRecall across different sizes of new concepts (n),\nwith the blue and green lines representing Precision\nand Recall, respectively. This analysis highlights\nthe trade-off in parameter selection, where lower\nρ∗values capture more emerging concepts but may\nintroduce noise, while higher values risk missing\nsmaller but meaningful patterns.\nA.2\nSynthetic Evaluation Keywords\nTable 1 presents a selection of keywords used in\nour synthetic evaluation.\nA.3\nSpeech Representation Statistics\nFigure 6 provides summary statistics on ideolog-\nical, gender, and racial/ethnic identity represen-\ntation, both in personnel and speech, during our\nperiod of observation.\nA.4\nRobustness Test\nWe conduct a robustness test to evaluate the im-\npact of key parameters, including the embed-\nding model and important UMAP hyperparam-\neters.\nDetails can be found in Table 2.\nWe\nselect stella_en_400M_v5 as it is the current\nbest-performing model (under 1B parameters) on\nMTEB Clustering.\nFor UMAP dimensionality reduction, we found\nthat setting n_components beyond 3 was infeasible\ndue to excessive memory requirements (exceeding\n800GB for n_components = 4). Meanwhile, vari-\nations in n_neighbors and min_dist had minimal\nimpact on performance.\nA.5\nPseudo Code\nAlgorithm 1 Pseudo Code for Blob Linking\nInput: List of blobs Bw for each subsequent pe-\nriod following the given reference period\nOutput: List of graphs Gbend of linked blobs for\nthe given reference period, with bend being the\nending blob of the graph\n1: for each blob b1,i ∈B1 do\n2:\nInitialize Gb1,i\n3: end for\n4: for w from 2 to W do\n5:\nfor each blob bw,i ∈Bw do\n6:\nFind blobs bw−1,j\n∈\nBw−1\nwith\ndist(bw−1,j, bw,i) < d\n7:\nfor each blob bw−1,j found do\n8:\nAdd edge (bw,i, bw−1,j) to Gbw−1,j\n9:\nUpdate the end of Gbw−1,j to bp,i, thus\nGbw,i\n10:\nend for\n11:\nend for\n12: end for\nA.6\nHistorical Event Selection\nOur initial event list was derived from relevant\nWikipedia\nentries\n(https://en.wikipedia.\norg/wiki/Outline_of_the_history_of_the_\nUnited_States). We refined this list by excluding\nentries that were too broad (e.g., Patriotism), pri-\nmarily biographical (e.g., lists of U.S. presidents),\nor date-specific (e.g., war start/end dates).\nThe final selection includes: World War I, Ger-\nmania, Roaring Twenties, Great Depression, The\nNew Deal, World War II, Cold War, Korean War,\nAssassination of President McKinley, Suez Cri-\nsis, Cuban Revolution, Civil Rights Movement,\nBrown v. Board of Education and \"massive resis-\ntance,\" Vietnam War, Watergate, 1973 Oil Crisis,\nReaganomics, and the Moon Landing.\nA.7\nLLM Prompts for Sentence Generation\nand Summarization\nThe prompt we use to generate sentences contain-\ning keywords for the synthetic dataset:\n“You are an assistant who helps generate sen-\ntences containing a specific keyword. I will give\nyou a keyword, and you will generate a list of\nsentences, each of which should contain the word\n{KEYWORD}, used according to its specified mean-\ning: {KEYWORD’s NEW MEANING}. The sen-\ntences should be written in the style of sentences in\nCorpus of Historical American English, such as {5\nCOHA SENTENCES}”\n\n\nFigure 5: The effect of varying ρ∗, the threshold controlling the minimum peak intensity for a blob to be identified,\non the pipeline’s performance for different sizes of new concepts (n). The blue and green lines represent Precision\nand Recall, respectively.\nKeyword\nOld Meaning\nNew Meaning\nMouse\nSmall rodent\nComputer device\nGay\nHappy or joyous\nHomosexual\nCool\nModerately cold\nStylish or impressive\nCloud\nMass of condensed water vapor\nOnline data storage or computing services\nSurf\nRiding on the waves on a surfboard\nBrowse the internet\nBug\nInsect\nSoftware error\nVirus\nInfectious biological agent\nMalicious software (malware)\nHack\nCutting with rough blows\nUnauthorized access to systems or networks\nTable 1: Old and new meanings of selected keywords\nParameter\nValue\nPerformance\nEmbedding Model\nstella_en_400M_v5\n0.94\nUMAP n_components\n3\n0.91\nUMAP n_neighbors\n30\n0.94\nUMAP min_dist\n0.4\n0.94\nBaseline\n0.94\nTable 2: Robustness test results for the embedding model and UMAP parameters, with a concept size of n = 100.\n\n\nFigure 6: Fraction of speeches in the Congressional Record and fraction of senators by party affiliation, gender, and\nminority status.\n\n\nThe prompt we use to summarize the topics:\n“You are an assistant who is good at summa-\nrizing a list of texts. Read the list of texts and\nsummarize them in at most 2 sentences, try to be as\nspecific and detailed as possible. Remember all the\ntexts in the list have to be closely related to your\nsummarization. For example, if a list of texts are\nabout \"equal pay for women\", your summarization\nneeds to clearly mention that it is about equal pay\nfor women, not just equal pay”\nA.8\nConcepts that were overrepresented by\nwomen senators\n• Electricity Market Manipulation and Regu-\nlatory Reform: Broad discussions on prevent-\ning market manipulation in energy sectors and\nthe need for stronger regulatory frameworks\nto protect consumers and ensure fair competi-\ntion.\n• Sexual Assault in the Military: Addressing\nsystemic cultural issues and structural changes\nneeded to improve how the military handles\nsexual assault cases and victim support.\n• Energy Deregulation and Consumer Im-\npact: Examination of the broader effects of\nenergy market deregulation, with a focus on\nprice stability, consumer protections, and the\nconsequences of reduced oversight.\n• Firefighter Funding and Safety Standards:\nGeneral advocacy for increased support and\nfunding for fire departments across the U.S.,\nfocusing on preparedness, training, and com-\nmunity safety.\n• Climate Change and Environmental Policy:\nLegislative efforts to address climate change,\nfocusing on balancing economic growth with\nenvironmental sustainability and national se-\ncurity.\n• Student Loan Debt and Higher Education\nAccessibility: The growing crisis of student\nloan debt in the U.S., its economic impact,\nand strategies to make higher education more\naffordable and accessible.\n• Prescription Drug Costs and Healthcare\nAccessibility: Broader issues around prescrip-\ntion drug pricing, the economic burden on\nconsumers, and the need for improved afford-\nability and transparency in healthcare.\n• Homeland Security Funding Allocation:\nBroader discussions on effective homeland\nsecurity funding, emphasizing the need for\nrisk-based distribution and the protection of\ncritical infrastructure.\n• Forest Management and Wildfire Preven-\ntion: Legislative focus on sustainable forest\nmanagement practices to reduce the frequency\nand severity of wildfires, protect communities,\nand maintain healthy ecosystems.\n• Post-Disaster Recovery and Federal Re-\nsponse: Evaluations of federal disaster re-\nsponse strategies, accountability in recovery\nefforts, and long-term support for rebuilding\ncommunities affected by natural disasters.\n• Port Security and National Safety Con-\ncerns: Ensuring comprehensive port security\nmeasures in response to vulnerabilities in mar-\nitime transportation, focusing on inspection\nprotocols and technology improvements.\n• Judicial Diversity and Federal Court Effec-\ntiveness: The importance of maintaining a\ndiverse judiciary and filling court vacancies to\nensure effective and timely judicial processes.\n• Healthcare Access and Patient Rights: Gen-\neral discussions around ensuring that health-\ncare decisions prioritize patient needs over\nprofit, with emphasis on patient protections\nand healthcare equity.\n• Workforce Development and Economic Mo-\nbility: Legislative focus on workforce train-\ning, education, and support for economically\ndisadvantaged communities to enhance social\nmobility and reduce inequality.\n• Affordable Housing and Urban Develop-\nment: Broader topics around housing afford-\nability, urban planning, and support for vulner-\nable populations in maintaining stable hous-\ning.\n• Consumer Protection Against Deceptive\nPractices: Efforts to combat deceptive mar-\nketing and protect consumers from fraudulent\nschemes, focusing on transparency and ac-\ncountability.\n• Federal Budget Priorities and Economic\nStability: Ongoing debates around sustain-\nable federal budgeting, spending priorities,\n\n\nand the long-term economic impact of bud-\ngetary decisions.\n• Energy Security and Resource Indepen-\ndence: Discussions around ensuring energy\nsecurity, reducing reliance on foreign oil, and\ninvesting in renewable resources to build a\nresilient energy infrastructure.\n• Economic Support for Low-Income Com-\nmunities: Strategies to address poverty and\neconomic inequality through targeted social\nprograms, job creation, and educational op-\nportunities.\n• National Security and Intelligence Over-\nsight: Broader discussions on improving in-\ntelligence gathering, interagency cooperation,\nand maintaining civil liberties while ensuring\nnational security.\nA.9\nConcepts that were overrepresented by\nminority senators\n• Native American Sovereignty and Self-\nGovernance: Ongoing legislative discussions\nand policy proposals aimed at increasing tribal\nautonomy, particularly in criminal justice and\nhealthcare, while reducing federal oversight\nto promote self-determination and cultural\npreservation.\n• Environmental Justice and Water Rights\nfor Indigenous Communities: Focus on re-\nsolving water rights disputes and ensuring en-\nvironmental protections for Native American\nlands, highlighting the intersection of environ-\nmental conservation and tribal rights.\n• Recognition of Veterans’ Contributions\nand Welfare: Broad discussions around the\nimprovement of veterans’ healthcare and sup-\nport systems, reflecting a national effort to\nrecognize veterans’ sacrifices and provide eq-\nuitable services for all who served.\n• Federal Oversight and Reform in Native\nAmerican Affairs: Debates about restructur-\ning the Bureau of Indian Affairs, emphasizing\nthe need to shift control from federal agen-\ncies to tribes, promoting autonomy and local\ngovernance.\n• Civil Rights and Criminal Justice for Mi-\nnority Communities: Examination of the le-\ngal system’s fairness, particularly in relation\nto the federal death penalty’s application in\nminority and Native American communities,\nfocusing on equal justice and civil rights.\n• Economic Development and Political Sta-\ntus of Puerto Rico: A nuanced exploration of\nPuerto Rico’s political autonomy, economic\ninitiatives, and U.S. influence, reflecting on\nbroader themes of decolonization and self-\ngovernance.\n• Healthcare\nEquity\nfor\nMarginalized\nGroups: Comprehensive policy discussions\naimed at addressing disparities in healthcare\naccess and outcomes for Native Americans\nand other minority communities, advocating\nfor culturally competent care.\n• Comprehensive Immigration Reform: Ef-\nforts to balance security, economic needs, and\nthe humane treatment of undocumented immi-\ngrants, emphasizing the challenges of creating\na fair immigration system without exacerbat-\ning labor exploitation.\n• Advocacy for Educational Opportunities\nin Minority Communities: Legislation and\npolicy debates focused on improving educa-\ntional resources, preserving cultural identity,\nand supporting minority students, including\nefforts to empower local control.\n• Environmental and Energy Policy Leader-\nship: Minority senators’ involvement in craft-\ning and promoting sustainable environmental\npolicies, such as balancing resource manage-\nment with economic development and com-\nmunity health.\n• National Infrastructure and Equitable Re-\nsource Distribution: Broad discussions on\nthe need for a fair and effective allocation of\nfederal funds for infrastructure, with a focus\non supporting both urban and rural develop-\nment equitably.\n• Marine\nResource\nManagement\nand\nOceanography: Promotion of oceanographic\nresearch and resource management, empha-\nsizing the strategic and economic importance\nof U.S. leadership in marine science and\nenvironmental stewardship.\n• Combating Hate Crimes and Promoting\nCommunity Safety:\nLegislative actions\n\n\naimed at addressing and preventing hate-\nmotivated violence, emphasizing the need for\nrobust data collection and community-based\ninterventions.\n• Flood Control and Sustainable Water Man-\nagement: Proposals for long-term flood con-\ntrol strategies and comprehensive water man-\nagement plans to prevent natural disasters and\nsupport sustainable development across af-\nfected regions.\n• Healthcare\nAccess\nand\nReproductive\nRights:\nBroader debates on reproductive\nhealth policies, focusing on the rights of\nlow-income women and the implications\nof federal healthcare funding decisions on\nmarginalized groups.\n• Tourism as an Economic Driver: Recogni-\ntion of tourism’s role in economic develop-\nment, particularly in states with high reliance\non tourism, and efforts to promote the U.S. as\na global leader in travel and hospitality.\n• Empowering Small Business Development:\nInitiatives focused on reducing barriers and\npromoting economic opportunities for small\nbusinesses, particularly in underserved and mi-\nnority communities, without targeting specific\nlegislation.\n• Military and Defense Readjustments in Lo-\ncal Economies: Discussions around the eco-\nnomic and social impact of military base clo-\nsures on communities, advocating for policies\nto support local economies during defense\ndownsizing.\n• Public Health Preparedness and National\nSecurity: Thematic focus on enhancing pub-\nlic health infrastructure and preparedness to\naddress bioterrorism and pandemics, empha-\nsizing coordinated national strategies and\ninter-agency collaboration.\n• Advocacy for Comprehensive Civil Rights\nProtections: Broader legislative themes cen-\ntered on expanding civil rights protections,\naddressing discrimination in multiple areas\nsuch as employment, housing, and healthcare\nfor underrepresented groups.\nA.10\nConcepts that were overrepresented by\nRepublican senators\n• Intelligence and Military Relations: Dis-\ncussions focusing on U.S. foreign policy and\ndefense strategies in regions such as the Mid-\ndle East, Central America, and Taiwan, par-\nticularly concerning arms control and military\nalliances.\n• U.S.-Soviet Relations During the Cold War:\nStrategic assessments and briefings on Soviet\nmilitary capabilities and U.S. efforts to coun-\nteract Soviet influence globally.\n• U.S. Relations with China and Taiwan: Con-\ngressional debates on U.S. foreign policy to-\nwards China and Taiwan, focusing on diplo-\nmatic recognition and military support.\n• Military Retirement and Procurement:\nLegislative discussions on military benefits,\nincluding retirement policies and the procure-\nment process for defense equipment.\n• Environmental Regulations and Resource\nManagement: Policy debates on the Clean\nWater Act, environmental conservation, and\nmanagement of natural resources, including\nenergy innovations.\n• Energy Policy and Radioactive Waste Man-\nagement: Hearings focused on energy supply,\ninnovation, and managing radioactive waste,\nwith emphasis on nuclear energy safety.\n• Native American Sovereignty and Self-\nGovernance: Ongoing legislative discussions\naimed at increasing tribal autonomy, particu-\nlarly in criminal justice and healthcare, pro-\nmoting self-determination.\n• Economic Impacts of Rising Federal Debt:\nReview of the growing U.S. federal debt and\nits long-term economic consequences, partic-\nularly after the debt exceeded $5 trillion by\n2000.\n• Hate\nCrimes\nand\nViolence\nAgainst\nMarginalized Groups: Reports of rising hate\ncrimes based on race, sexual orientation, and\ngender identity, sparking debates on the need\nfor stronger hate crime legislation.\n\n\n• Oversight of National Security Policies:\nEvaluations of U.S. national security and de-\nfense policies, with particular focus on in-\nternational conflicts such as the invasion of\nGrenada.\n• Military Appointments and Honors: Dis-\ncussions and acknowledgments of military\nappointments, promotions, and the recogni-\ntion of veterans’ contributions across multiple\nbranches of the U.S. military.\n• Trade Relations with the European Com-\nmunity: Congressional hearings on U.S. trade\npolicies with European nations, focusing on\neconomic competition, tariffs, and diplomatic\nrelations.\n• Marshall Plan and Post-War Trade Policies:\nDebates on the impact of the Marshall Plan\nand post-WWII U.S. foreign policy, particu-\nlarly concerning trade relations with Eastern\nEurope.\n• Judicial Activism and Marriage Laws: Leg-\nislative responses to judicial rulings on mar-\nriage, particularly surrounding the definition\nof marriage and the role of federal versus state\nauthority.\n• U.S. Involvement in the Korean War: De-\nbates on U.S. military intervention in Korea,\nfocusing on constitutional authority, military\nstrategy, and the broader implications for U.S.\nforeign policy.\n• Foot-and-Mouth Disease (FMD) in Live-\nstock: Concerns over the threat of FMD out-\nbreaks in neighboring countries, leading to\ndiscussions on U.S. agricultural biosecurity\nand disease prevention measures.\n• NATO and U.S. Military Commitments:\nAnalysis of U.S. military obligations under\nNATO, with debates on the potential risks of\nentanglement in European conflicts during the\nCold War.\n• Tribal Sovereignty and Federal Law: Leg-\nislative debates on tribal sovereignty, focusing\non the application of federal death penalty\nlaws on Native American reservations.\n• U.S.-Spain Relations During the Cold War:\nCongressional discussions on U.S. military\nalliances and the strategic importance of Spain\nin the broader NATO defense framework.\n• Debt Ceiling and Fiscal Responsibility: On-\ngoing debates over raising the U.S. debt ceil-\ning, with emphasis on fiscal responsibility,\ngovernment spending, and the risk of financial\ncrises.\nA.11\nConcepts that were overrepresented by\nDemocratic senators\n• Immigration and Naturalization Laws:\nLegislative efforts to provide exemptions,\nwaivers, and status adjustments, particularly\nfor family members of U.S. citizens, reflecting\na trend towards facilitating family reunifica-\ntion and humanitarian considerations.\n• Indian Affairs and Tribal Legislation: Pub-\nlic hearings and legislative meetings focused\non Indian affairs, including land claims,\nhealthcare, housing, and tribal recognition,\nindicating ongoing efforts to address Native\nAmerican concerns.\n• Transportation and Science Oversight: Sen-\nate Committee hearings on topics like trans-\nportation safety, telecommunications, and en-\nvironmental impacts, with a focus on federal\nregulation of transportation industries during\nthe late 1970s and 1980s.\n• Environmental and Energy Policy Hear-\nings: Discussions on environmental legisla-\ntion, such as the Clean Air Act, nuclear waste\nmanagement, and global climate change, re-\nflecting legislative efforts to address pressing\nenvironmental challenges.\n• Agricultural Policy and Food Security:\nHearings focused on agricultural policy, in-\ncluding preparations for farm bills, water qual-\nity, and global warming’s impact on agricul-\nture, emphasizing the Senate’s role in shaping\nfood security and agricultural sustainability.\n• Women’s Issues and Economic Policy: Sen-\nate hearings on a wide array of topics, includ-\ning workplace discrimination, mortgage lend-\ning, healthcare, and education, reflecting leg-\nislative efforts to address social and economic\nchallenges affecting women.\n• Native American Health and Environmen-\ntal Legislation: Hearings on Native American\n\n\nhealthcare and environmental policies, includ-\ning the reauthorization of the Indian Health\nCare Improvement Act, highlighting federal\nresponsibilities toward Native communities.\n• Small Business Protections and Regulatory\nChallenges: Senate Select Committee hear-\nings on issues affecting small businesses, such\nas regulatory barriers, financial assistance pro-\ngrams, and the economic impact of federal\npolicies.\n• Civil Rights and Voting Rights Legislation:\nLegislative debates on civil rights and vot-\ning laws, addressing systemic disenfranchise-\nment and discriminatory practices, with a fo-\ncus on increasing protections for marginalized\ngroups.\n• Genocide Convention and Human Rights\nLegislation: Advocacy for the ratification\nof the United Nations Genocide Convention,\nemphasizing the U.S. commitment to human\nrights and moral leadership in preventing\natrocities.\n• Drug Pricing and Pharmaceutical Regula-\ntions: Legislative efforts to address drug pric-\ning, focusing on the cost disparity between\ngeneric and branded drugs, and the push for\nincreased transparency and competition in the\npharmaceutical industry.\n• Fishing Industry Legislation and Foreign\nCompetition: Legislative discussions on pro-\ntecting the U.S. fishing industry from foreign\ncompetition, including subsidies for Ameri-\ncan fishermen and conservation practices to\nsustain marine resources.\n• Environmental Conservation and National\nParks Legislation: Hearings on the establish-\nment of national seashores and parks, such as\nthe Oregon Dunes and Indiana Dunes, with a\nfocus on environmental preservation and pub-\nlic access.\n• Healthcare for the Elderly: Legislative ef-\nforts to provide better health insurance and\nfinancial assistance for elderly citizens, with\nproposals such as the Anderson amendment to\nintegrate medical care into the social security\nsystem.\n• Foreign Aid and Developmental Assistance:\nDebates on reforming U.S. foreign assistance\nprograms, with emphasis on efficiency, ac-\ncountability, and aligning aid with U.S. inter-\nests while promoting development in recipient\ncountries.\n• Urban Development and Housing Policy:\nHearings on urban revitalization, housing fi-\nnance reform, and addressing the impacts of\nfinancial crises on housing markets, with a fo-\ncus on providing affordable housing solutions.\n• Nuclear Weapons and Arms Control: Dis-\ncussions on U.S. nuclear policies, arms con-\ntrol agreements, and efforts to suspend nu-\nclear weapons testing, emphasizing the need\nfor international cooperation and inspection\nsystems to ensure global security.\n• Forest Management and Conservation: De-\nbates on forest conservation policies, tim-\nber management, and the need for sustain-\nable forestry practices to protect national\nforests and promote economic growth in\nforest-dependent communities.\n• Aircraft Noise Pollution and Environmen-\ntal Impact: Legislative proposals to address\nthe negative impact of aircraft noise on com-\nmunities, advocating for noise control mea-\nsures, quieter engine technology, and public\nhealth protections.\n• School Lunch Programs and Child Nu-\ntrition: Legislative efforts to address child\nhunger through the National School Lunch\nProgram, including extending food assistance\nduring summer months and maintaining food\nsecurity for low-income children.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21315v1.pdf",
    "total_pages": 19,
    "title": "Identifying Emerging Concepts in Large Corpora",
    "authors": [
      "Sibo Ma",
      "Julian Nyarko"
    ],
    "abstract": "We introduce a new method to identify emerging concepts in large text\ncorpora. By analyzing changes in the heatmaps of the underlying embedding\nspace, we are able to detect these concepts with high accuracy shortly after\nthey originate, in turn outperforming common alternatives. We further\ndemonstrate the utility of our approach by analyzing speeches in the U.S.\nSenate from 1941 to 2015. Our results suggest that the minority party is more\nactive in introducing new concepts into the Senate discourse. We also identify\nspecific concepts that closely correlate with the Senators' racial, ethnic, and\ngender identities. An implementation of our method is publicly available.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}