{
  "id": "arxiv_2502.20953v1",
  "text": "Optimality and Suboptimality of MPPI Control\nin Stochastic and Deterministic Settings\nHannes Homburger1, Florian Messerer2, Moritz Diehl2, and Johannes Reuter1\nAbstract— Model predictive path integral (MPPI) control has\nrecently received a lot of attention, especially in the robotics\nand reinforcement learning communities. This letter aims to\nmake the MPPI control framework more accessible to the\noptimal control community. We present three classes of optimal\ncontrol problems and their solutions by MPPI. Further, we\ninvestigate the suboptimality of MPPI to general deterministic\nnonlinear discrete-time systems. Here, suboptimality is defined\nas the deviation between the control provided by MPPI and the\noptimal solution to the deterministic optimal control problem.\nOur findings are that in a smooth and unconstrained setting,\nthe growth of suboptimality in the control input trajectory\nis second-order with the scaling of uncertainty. The results\nindicate that the suboptimality of the MPPI solution can be\nmodulated by appropriately tuning the hyperparameters. We\nillustrate our findings using numerical examples.\nI. INTRODUCTION AND OVERVIEW\nModel predictive control (MPC) is an optimization-based\nstate-feedback control technique that computes the control\ninputs concerning the state trajectory predicted over a finite\nhorizon as the solution of an optimal control problem (OCP).\nTo solve an OCP, often numerical methods are used that\nsequentially generate local first- or second-order models and\nthen solve them efficiently [1]. These approaches lead to\nvery fast convergence rates and can be applied even to\nhigh-dimensional problems. However, they can converge to\nlocal minima and have issues in the presence of strong\nnonlinearities. In contrast, zero-order solvers based on sam-\npling have fewer restrictions on the problem and can be\nexecuted in parallel but often lead to higher computational\neffort by ignoring the first- or second-order information. Re-\ncently, a sample-based method called Model Predictive Path\nIntegral (MPPI) control has been investigated and applied\nin numerous publications, e.g. see the survey [2]. Initially,\nthis method inspired by the path integral framework was\nused to solve a special class of stochastic closed-loop OCPs\n[3], [4]. Later it was extended based on the information-\ntheoretic framework to general nonlinear systems with input\nnoise – however, based on an open-loop problem formulation\n[5]. The certainty-equivalence principle [6] states that the\nsolution for linear systems with independent noise and costs\nis independent of the uncertainty. However, in the litera-\nture, the uncertainty-aware MPPI method is often used for\nThis research was supported by DFG via projects 504452366 (SPP 2364)\nand 525018088, and by BMWK via 03EI4057A and 03EN3054B.\n1 Institute of System Dynamics, HTWG Konstanz - University of Ap-\nplied Sciences, 78462 Konstanz, Germany {hhomburg, jreuter}\n@htwg-konstanz.de\n2 Department of Microsystems Engineering (IMTEK) and Depart-\nment of Mathematics, University of Freiburg, 79110 Freiburg, Germany\n{first.last} @imtek.uni-freiburg.de\ndeterministic OCPs, and the suboptimality of the solution\nis tacitly ignored, see e.g. [7]–[10]. In this context, noise\nis intentionally introduced for exploration, representing a\nfundamental distinction from classical stochastic optimal\ncontrol, where noise arises from the environment. Recently,\nan MPPI method for deterministic OCPs was presented [11].\nA. Contribution and Outline\nWe review the MPPI control framework from a stochastic op-\ntimal control perspective and show that the problem classes\nconsidered in the MPPI literature are stochastically equiva-\nlent to input-affine stochastic systems with noise on the input\nchannel. Subsequently, we investigate the suboptimality of\nstandard MPPI to deterministic OCPs and characterize the\nproblems. We introduce a scaling factor β ∈R+ that scales\nthe standard deviation of the MPPI sampling distribution.\nWe prove that for smooth and unconstrained problems, the\nsuboptimality of the control trajectory is O(β 2) and the sub-\noptimality of the value function is O(β 4). Structure of this\nletter: in Section II, we define different classes of stochastic\nOCPs and discuss their solution via MPPI in Section III. The\nsuboptimality is investigated in Section IV and discussed in\nSection V. In Section VI, we present numerical examples,\nfollowed by a summarizing Section VII.\nB. Notation and Preliminaries\nThe set of positive real numbers is R+ = {x ∈R | x > 0}\nand the set of non-negative real numbers is R+\n0 = {x ∈R |\nx ≥0}. The concatenation of two column vectors x ∈Rnx\nand y ∈Rny is denoted by (x,y) :=\n\u0002\nx⊤,y⊤\u0003⊤∈Rnx+ny. The\npartial derivative with respect to a variable x is represented\nas\n∂\n∂x, indicating differentiation with respect to the explicit\nargument of the function. Arguments of a function may be\nspecified using subscripts or parentheses. For instance, the\nfunction fβ(x) has two arguments, β and x. Gradients of\nfunctions are denoted by ∇x f(x) = ∂\n∂x f(x)⊤, representing the\nvector of partial derivatives with respect to x and ∇2\nx f(x)\ndenotes the Hessian. The multivariate normal distribution of\na random variable v ∈Rn is expressed as v ∼N (µ,Σ), where\nµ ∈Rn is the mean, and Σ ∈Rn×n is the covariance matrix.\nThe expectation of a random variable W ∈Rn sampled from\nQβ defined by its probability density function qβ(W), is\nEW∼Qβ [W] =\nZ\nRn Wqβ(W)dW.\nFor a function f : Rn →Rm, we write f(x) = O(g(x))\nif and only if there exists a constant C ∈R+ and a\nneighborhood A of 0 such that ∀x ∈A : ∥f(x)∥≤Cg(x).\narXiv:2502.20953v1  [eess.SY]  28 Feb 2025\n\n\nII. STOCHASTIC OPTIMAL CONTROL PROBLEMS\nWe consider the stochastic nonlinear discrete-time system\nx+ = f(x,u,w),\nwhere the state is x ∈Rnx, the control is u ∈Rnu, the random\ndisturbance is w ∈Rnw, and without loss of generality w ∼\nN (0,I) with unit matrix I. Given an initial state x0 ∈Rnx, an\ninput trajectory U = (u0,u1,...,uN−1), and a noise trajectory\nW = (w0,w1,...,wN−1), the corresponding state trajectory is\ndetermined by\nx0(U,W,x0) = x0,\n(1)\nxk+1(U,W,x0) = f(xk(U,W,x0),uk,wk),\n(2)\nusing forward simulation for k = 0,1,...,N −1. The perfor-\nmance of a trajectory can be evaluated by the overall cost\nJx0(U,W) :=\nN−1\n∑\nk=0\nL(xk(·),uk)+E(xN(·)),\n(3)\nwhere the states’ dependencies given in (1) and (2) are\nomitted for readability, the stage cost function is L : Rnx ×\nRnu →R, and E : Rnx →R is the terminal cost function.\nA. Closed-Loop Stochastic Optimal Control\nThe ideal problem is to minimize the expected overall cost\nfor the closed-loop system. This means that each control\nuk is selected with respect to the state xk(·). The key point\nis that in xk(·) all previous realizations of disturbances wi\nand control actions ui for i = 0,..,k −1 are included. This\nproblem can be formalized in the recursive form\nVcls(x0) = min\nu0 Ew0∼N (0,Σ)...min\nuN−1 EwN−1∼N (0,Σ) Jx0(U,W),\n(4)\nthat we call the closed-loop stochastic optimal control prob-\nlem (CLS-OCP). Note that this problem typically corre-\nsponds to optimization over policies because it reacts to\ndisturbances in an optimal way.\nB. Open-Loop Stochastic Optimal Control\nBecause the CLS-OCP is typically hard to solve, it is\nsometimes reasonable to optimize the expected overall cost\nfor the open-loop system instead. This means that the control\ninputs uk are chosen independent of the previous realization\nof the disturbances, but in an optimal way concerning the\nprior known distribution of the disturbances. This problem\ncan be formalized as\nVols(x0) = min\nU\nEW∼N (0,Σ)\n\u0002\nJx0(U,W)\n\u0003\n,\n(5)\nthat we call the open-loop stochastic optimal control problem\n(OLS-OCP), where Σ = diag(Σ,Σ,...,Σ) ∈RNnw×Nnw is the\nmutual covariance matrix for a whole trajectory W with un-\ncorrelated wk. Usually, the solution to this problem requires\nless effort, because the optimization concerns only one input\ntrajectory. However, when applied in an MPC fashion, the\ncontroller’s capability to react to realized disturbances is not\nmodeled, leading to a suboptimal solution for the closed-loop\nsystem.\nC. Deterministic Optimal Control\nIn this letter, we investigate the suboptimality of MPPI for\nsolving the deterministic OCP represented by the nonlinear\nprogram\nVdet(x0) = min\nU\nJx0(U,0),\n(6)\nwhere no uncertainty is considered. We call this problem\nthe deterministic optimal control problem (DET-OCP). If the\ncomputation power is limited or if there is no model of\nthe model’s uncertainty, practitioners often use this nominal\nsetting. Note that in a smooth and unconstrained setting the\nDET-OCP solution and the CLS-OCP solution are similar\nfor systems with small uncertainties [12].\nIII. MPPI SOLUTION METHODS\nIn this section, we review the MPPI methods for the OCPs\nstated in the previous section and we show that these methods\nare inherently restricted to the overall cost structure\nJx0(U,W) = Sx0(U +W)+ 1\n2∥U∥2\n2,\n(7)\nwhere the path costs with c : Rnx →R+\n0 and V := U +W are\nSx0(V) := E(xN(V,0,x0))+\nN−1\n∑\nk=0\nc(xk(V,0,x0)).\n(8)\nThus, the stage cost in (3) is restricted to the sum of a generic\nterm dependent only on the state and a quadratic control cost.\nA. MPPI Control for the CLS-OCP\nWe consider the class of CLS-OCPs, where the discrete-time\nsystem dynamics is restricted to input-affine systems\nx+ = ˜f(x)+B(x)u+G(x)w,\n(9)\nwith B : Rnx →Rnx×nu, G : Rnx →Rnx×nw, w ∼N (0,I), and\nthe stage cost\nL(x,u) = c(x)+ 1\n2u⊤R(x)u\n(10)\nwith R : Rnx →Rnu×nu, R ≻0. The main theoretical results\nare derived under the following strong assumption. We show\nthat this assumption can interpreted as effectively requiring\nthat the noise enters additively on the control.\nAssumption 1. There exists a λ ∈R+ such that\nλB(x)R(x)−1B(x)⊤= G(x)G(x)⊤.\n(11)\nProposition 1. We assume the dynamics (9), w i.i.d., the\nstage cost (10), and Assumption 1 holds. Then an equivalent\nOCP exists, which is specified by the dynamics\nx+ = ˜f(x)+B(x)(u+w),\n(12)\nand the corresponding overall cost (7).\nProof. Substituting B(x) = B(x)R(x)\n1\n2 into (11), we obtain\nλB(x)B(x)⊤= G(x)G(x)⊤and note that G(x)N (0,I) =\nN\n\u00000,G(x)G(x)⊤\u0001\n= N\n\u00000,λ ¯B(x) ¯B(x)⊤\u0001\n= ¯B(x)N (0,λI).\nWith u = R(x)−1\n2 u, the dynamics (12) with w ∼N (0,λI)\nand the overall cost (7) specify an equivalent problem.\nFor this class of CLS-OCPs in a continuous-time setting,\na closed-form solution can be derived from an exponential\n\n\ntransformation of the corresponding stochastic Hamilton-\nJacobi-Bellman (HJB) equation, resulting in a linear second-\norder partial differential equation (PDE) [3]. To obtain a\nlinear second-order PDE, the continuous-time version of\nAssumption 1 must hold [13, Sec. II.A]. This PDE can be\nevaluated as an expectation via the Feynman-Kac lemma [14,\nThm. 8.2.1]. While the theoretical foundations are based on\na continuous-time setting, to reach a discrete-time solution,\nthe Euler–Maruyama method can be applied as implicitly\nproposed in [5, Sec. IV]. The obtained dynamics (12) are\npresented as a special case in [5, Sec. IV], where the solution\nfor the CLS-OCPs is given in compact form as\nVcls(x0) = −λ logEW∼N (0,Σ)\n\u0014\nexp\n\u0012\n−1\nλ Sx0(W)\n\u0013\u0015\nof weighted uncontrolled trajectories, and the optimal control\nfor k = 0 is given by\nu⋆\n0,cls(x0) =\nEW∼N (0,Σ)\n\u0002\nexp\n\u0000−1\nλ Sx0(W)\n\u0001\nw0\n\u0003\nEW∼N (0,Σ)\n\u0002\nexp\n\u0000−1\nλ Sx0(W)\n\u0001\u0003 .\n(13)\nNote the similarity between (13) and the softargmin function,\nwhich is well known in the field of machine learning.\nB. MPPI Control for the OLS-OCP\nNow we consider a more general class of nonlinear discrete-\ntime systems with additive noise on the input, given by\nx+ = f(x,u+w,0),\nw ∼N (0,Σ),\n(14)\nquadratic costs on the inputs (10), and assumption λR−1 = Σ.\nAccording to [5, Sec. 3.A], the optimal control sequence U⋆\nols\nof the OLS-OCP (5) can be approximated by\nU⋆\nols ≈˜U⋆\nMPPI := argmin\nU\nEW∼Q⋆\n\u0014\nlog\n\u0012 q⋆(W)\nqU(W)\n\u0013\u0015\n,\n(15)\nsometimes\ncalled\ninformation-theoretic\noptimum.\nHere,\nqU(W) is the probability density function (PDF) of the\nnormal distribution N (U,Σ) and the optimal distribution Q⋆\nis defined by its PDF\nq⋆(W) := 1\nη exp\n\u0012\n−1\nλ Sx0(W)\n\u0013\npΣ(W),\n(16)\nwhere η ∈R+ is a normalization factor and pΣ(W) is the\nPDF of the normal distribution N (0,Σ). Note that the\nobjective in (15) is the Kullback–Leibler divergence [15,\nCh. 3] of the optimal distribution Q⋆and the controlled\ndistribution QU. In contrast to the presentation in [5, Eq.\n(3) and (16)], as postulated in (15), MPPI only provides\nan approximation to the OLS-OCP solution, although the\noptimal distribution [5, Sec. 3] establishes a lower bound on\nthe value function of the OLS-OCP [16]. This follows from\nthe fact that in general, the optimal distribution cannot be\nreached by QU. Consequently, the relationship between U⋆\nols\nand ˜U⋆\nMPPI should be interpreted as a heuristic, and it is not\nclear how suboptimal the relation is. The minimizer of (15)\nis given by\n˜u⋆\nk,MPPI(x0) =\nEW∼N (0,Σ)\n\u0002\nexp\n\u0000−1\nλ Sx0(W)\n\u0001\nwk\n\u0003\nEW∼N (0,Σ)\n\u0002\nexp\n\u0000−1\nλ Sx0(W)\n\u0001\u0003\n(17)\nfor k = 0,1,...,N −1 [5, Sec. 3.C]. In contrast to the pre-\nsentation in [5, Sec. 4], the equivalence of (13) and (17) for\nk = 0 implies an interpretation of (15) as an approximation of\nthe CLS-OCP. Here, suboptimality presumably depends on a\nmetric of how strongly the nonlinear dynamics (14) deviate\nfrom the input-affine case (9).\nC. MPPI Control for the D-OCP\nHowever, there is a special case in which MPPI via the\ninformation-theoretic framework converges to an exact solu-\ntion: consider the general nonlinear and deterministic system\nx+ = f(x,u,0)\nand stage costs given in the form of (10) containing quadratic\ncosts on the control inputs and arbitrary state-dependent\ncosts. The optimal deterministic input trajectory is given by\nU⋆\ndet = argmin\nU\nJx0(U,0)\n(18)\nand the kth element of this solution is denoted by u⋆\nk,det(x0).\nBy introducing the scaling parameter β ∈R+ and replacing\nλ and Σ with β 2λ resp. β 2Σ in (17), we obtain\n˜u⋆\nk,MPPI (x0,β) =\nEW∼N (0,β 2Σ)\nh\nexp\n\u0010\n−\n1\nβ 2λ Sx0(W)\n\u0011\nwk\ni\nEW∼N (0,β 2Σ)\nh\nexp\n\u0010\n−\n1\nβ 2λ Sx0(W)\n\u0011i ,\n= EW∼Q⋆(β) [wk],\n(19)\nand in the limit of shrinking uncertainty this expression with\nu⋆\nk,det(x0) = lim\nβ→0 ˜u⋆\nk,MPPI(x0,β)\n(20)\nprovides\nan\nexact\nsolution\nto\nthe\nDET-OCP\nfor\nk = 0,...,N −1.\nNote\nthat\nthe\nnoise\nis\nintentionally\nintroduced for exploration and is not part of the environment.\nThe statement (20) is proven in Thm. 1 in Section IV.\nAlgorithm 1 Deterministic MPPI solving (20)\nRequire: Initial system state x0, initial control sequence ˆU, number\nof iterations I, number of trajectory samples M, shrinking\nfactor ν, initial covariance Σ0, initial temperature λ0\nEnsure: Optimal input trajectory U⋆\ndet\n1: for j ∈{0,1,...,I −1} do\n2:\nβ ←ν j; λ ←β 2λ0; Σ ←β 2Σ0\n▷reducing uncertainty\n3:\nfor m ∈{0,1,...,M −1} do\n▷in parallel\n4:\nW m ∼N\n\u00000,Σ\n\u0001\n5:\nSm ←Sx0\n\u0000 ˆU +W m\u0001\n+λW m⊤Σ−1 ˆU; ▷2nd term is cor-\nrection because ˆU ̸= 0; see Section III-D\n6:\nψ ←minm∈{0,1,...,M−1} Sm\n▷offset only for numerics\n7:\nη ←∑M−1\nm=0 exp(−1\nλ (Sm −ψ))\n8:\nfor m ∈{0,1,...,M −1} do\n▷in parallel\n9:\nωm ←1\nη exp(−1\nλ (Sm −ψ))\n▷compute weights\n10:\nfor k ∈{0,1,...,K −1} do\n11:\nˆuk ←ˆuk +∑M\nm=1 ωmwm\nk\n▷solution to (19)\n12: return U⋆\ndet ←ˆU\n▷solution to (20)\n\n\nD. MPPI Control Algorithms\nThe expectation operator regarding the distribution of the\nuncontrolled system is part of the MPPI solution to the CLS-\nOCP (13), the OLS-OCP (17), and the DET-OCP (20). MPPI\nalgorithms typically approximate this expectation operator\nusing Monte Carlo estimation [2]. However, the sample effi-\nciency using the uncontrolled distribution is strongly limited.\nTo increase sample efficiency with importance sampling the\nsample distribution is changed to a proposal distribution. In\nthe same course, a correction term must be added in the\nexpectation to preserve its value. To change the mean of the\nnormal distribution from 0 to ˆU and sample from N ( ˆU,Σ)\ninstead of N (0,Σ), a correction term presented in [5, Sec.\n3.C] is added to Line 5 of Algo. 1, which is adapted from\n[11]. Further, for I = 1, Algo. 1 reduces to standard MPPI\npresented in [5] computing (13) resp. (17). It is important to\nnote that importance sampling with this correction term does\nnot change the value of the expectation [5, Sec. 3.C]. The\nsuboptimality caused by the Monte Carlo estimation with a\nfinite number of samples is investigated in [17]–[19] and is\nnot part of this letter.\nIV. INVESTIGATION OF SUBOPTIMALITY\nIn this section, we present and prove Thm. 1, the principal\nresult of this paper. It addresses the convergence of the\ndeterministic MPPI method and establishes the order of\nsuboptimality. These findings are employed in Corollary 1.1\nto give an order of suboptimality on the corresponding\nvalue function. Before stating Thm. 1, we briefly introduce\nLemma 1 that will be crucial later.\nLemma 1 (Erd´elyi’s formulation of Laplace’s classical\nmethod, adapted from [20, Thm. 1.1]). For the integral\nI(λ) =\nZ b\na exp(−λ f(x))g(x)dx,\nwhere (a,b) ⊆R is a real interval, which may be finite or\ninfinite, we assume that\n(i) f(x) > f(a) for all x ∈(a,b), for every δ > 0 the\ninfimum of f(x)−f(a) in [a+δ,b) is positive;\n(ii) f and g are scalar real analytic functions; and\n(iii) the integral I(λ) converges absolutely for all sufficiently\nlarge λ.\nThen\nI(λ) ≡exp(−λ f(a))\n∞\n∑\nn=0\n˜cnλ −n\n2 ,\nwhere the coefficients ˜cn are real values and the symbol ≡\ndenotes that the quotient of the left-hand side by the right-\nhand side approaches 1 as λ →+∞.\nTheorem 1. Assume Jx0 : RNnu ×RNnu →R+\n0 is continuously\ndifferentiable, has a unique global minimum Jx0(U⋆\ndet ,0), and\nsecond-order sufficient condition (SOSC) at U⋆\ndet. Then\nlim\nβ→0\n˜U⋆\nMPPI(β) = U⋆\ndet\n(21)\nand for small β > 0 the bias of the solution shrinks quadrat-\nically with the uncertainty,\n∥˜U⋆\nMPPI(β)−U⋆\ndet∥= O\n\u0000β 2\u0001\n.\n(22)\nProof. We start with the scalar case Nnu = 1 and then extend\nthis to the multivariate case. Replacing λ and Σ by their\nscaled versions, we can rewrite (16) as\nq⋆\nβ(W) :=\n1\nη(β) exp\n\u0012\n−1\nβ 2λ Sx0(W)\n\u0013\npβ 2Σ(W),\n(23)\n=\nexp\n\u0010\n−\n1\nβ 2λ\nh\nSx0(W)+ λ\n2 ∑N−1\nk=0 w⊤\nk Σ−1wk\ni\u0011\nη(β)Z(β)\n,\n=\n1\n˜η(β) exp\n\u0012\n−1\nβ 2λ Jx0(W,0)\n\u0013\n,\nwhere in the second line, the exponential kernel of the\nmultivariate normal distribution is included in the exponent,\nand its normalization factor is denoted by Z(β). In the third\nline, the term in square brackets is identified as the overall\ncosts (5), and the normalization factors are combined to\n˜η(β). Based on this representation of the PDF of the optimal\ndistribution, the MPPI solution (19) is given by\n˜U⋆\nMPPI(β) = EW∼Q⋆(β)[W]\n=\nZ\nRWq⋆\nβ(W)dW\n=\nR\nRW exp\n\u0010\n−\n1\nβ 2λ Jx0(W,0)\n\u0011\ndW\nR\nR exp\n\u0010\n−\n1\nβ 2λ Jx0(W,0)\n\u0011\ndW\n,\n(24)\nwhere the second line is the explicit representation of the\nexpected value and in the third line q⋆\nβ(W) is substituted.\nFor small β > 0, both integrals are dominated by the\nneighborhood of W = U⋆\ndet, motivating to apply Laplace’s\nmethod. Substituting β 2 = ζ in (24) and applying Laplace’s\nmethod from Lemma 1 separately to the numerator and the\ndenominator result in\n˜U⋆\nMPPI(ζ) =\nU⋆\ndetα1ζ\n1\n2 +α2ζ\n3\n2 +O\n\u0010\nζ\n5\n2\n\u0011\nα1ζ\n1\n2 +α3ζ\n3\n2 +O\n\u0010\nζ\n5\n2\n\u0011\n,\nwhere the structure follows from SOSC at U⋆\ndet and α1,α2,α3\nare scalar constants. The re-substitution of ζ = β 2 and a\nsubsequent Taylor expansion at β = 0 yields\n∥˜U⋆\nMPPI(β)−U⋆\ndet∥= |α2 −U⋆\ndetα3|\n|α1|\nβ 2 +O\n\u0000β 4\u0001\n= O\n\u0000β 2\u0001\n.\nIn the multivariate case of (24) the MPPI solution for all\nscalar elements of the solution trajectory i = 0,.., ˆN −1 with\nˆN := Nnu is given by\n˜U⋆\nMPPI,i(β) = Ewi∼Q⋆(β)[wi]\n=\nR\nR ···\nR\nR\nR\nR wi Ξ(W,β) dw0dw1 ···dw ˆN−1\nR\nR ···\nR\nR\nR\nR Ξ(W,β)dw0dw1 ···dw ˆN−1\n=\nR\nR wi ˆΞ(wi,β)dwi\nR\nR 1 ˆΞ(wi,β)dwi\n,\n(25)\nwhere Ξ(W,β) := exp\n\u0010\n−\n1\nβ 2λ Jx0(W,0)\n\u0011\nand ˆΞ(wi,β) is the\nmarginal PDF for i = 0,.., ˆN −1. The marginal PDF (25) is\n\n\nsimilar to (24) that we already treated via Laplace’s Method.\nThe same argumentation results in\n∥˜U⋆\nMPPI,i(β)−U⋆\ndet,i∥= γiβ 2 +O\n\u0000β 4\u0001\n= O\n\u0000β 2\u0001\nthat holds for all scalar elements of input trajectory with\nindex i = 0,1,..., ˆN −1 with γi ∈R+\n0 and\n∥˜U⋆\nMPPI(β)−U⋆\ndet∥= O\n\u0000β 2\u0001\n(26)\nfollows.\nCorollary 1.1. The corresponding optimal value function is\nreached in the limit and shrinks with fourth-order,\nJx0( ˜U⋆\nMPPI(β),0)−Vdet(x0) = O\n\u0000β 4\u0001\n.\n(27)\nProof. Due to the continuously differentiable setting, the\ncontinuous differentiability of the optimal value function is\npreserved [12]. Therefore, we can use the Taylor series\nJx0(U,0)−Vdet(x0) = 1\n2∆U⊤∇2\nUJx0(U⋆\ndet,0)∆U +O\n\u0000∥U∥3\u0001\n,\n(28)\nwhere ∆U := U −U⋆\ndet denotes the difference to the deter-\nministic solution trajectory. Since, with (22), the error of the\ncomputed control trajectory shrinks with O\n\u0000β 2\u0001\n, and with\n(28) there exists a local quadratic model of the overall cost\nfunction, the uncertainty manifests itself with fourth-order in\nthe value function.\nV. ALTERNATIVES AND COMPUTATIONAL EFFORT\nIn comparison to other\nMonte\nCarlo\noptimization\napproaches to solve the DET-OCP like reward-weighted\nregression [21], cross-entropy methods [22], covariance\nmatrix\nadaptation\nevolution\nstrategy\n(CMA-ES)\n[23],\nMPPI-Generic [10], or CoVo-MPC [24], the deterministic\nMPPI method considered in this paper is characterized by\nthe predefined shrinking rate of the sampling covariance\nfavorable for real-time applications [11]. However, although\nit is not part of this study, it should be noted that Monte Carlo\napproaches often require excessive computational effort.\nMethods of Newton-type optimization can be tailored to\nsolve DET-OCPs efficiently [1, Sec. 8]. To apply Newton-\ntype methods on the OLS-OCP or the CLS-OCP, an approx-\nimation of the problem is typically required. This takes the\nform of either sampling the uncertainty vector, cf. e.g., the\nscenario approach [25] and the sample average approxima-\ntion [19], or based on linearization, cf. [26]. For the CLS-\nOCP, the latter approach requires e.g. a policy parametriza-\ntion such as linear state [27] or disturbance feedback [28].\nSince the problem (18) solved by deterministic MPPI follows\na single-shooting formulation [1, Sec. 8.5.1], which is com-\nmonly addressed using Newton-type optimization methods,\nwe compare both approaches in terms of computational\neffort. The computational cost of evaluating a function F is\ndenoted as cost(F). Due to the parallel nature of sampling,\nparticularly when executed on a GPU [10], the computational\ncost of Algo. 1 is given by cost(Algo. 1) ≈I·cost(Sx0), as the\nevaluation of Sx0 is computationally expensive. In contrast,\nthe cost of computing the gradient of Sx0 using the reverse\n-1\n-0.5\n0\n0.5\n1\nW\n0\n20\n40\n60\n80\nPDF q$\n-(W)\n- = 1\n- = 0.5\n- = 0.2\n- = 0.1\n-0.1\n0\n0.1\n0\n20\n40\n60\n-0.6\n-0.55\n-0.5\n-0.45\n-0.4\n0\n2\n4\nFig. 1: PDF of optimal distribution Q⋆\nβ (W) for different β. The two\nadditional interior plots show zoomed sections of the same PDF.\nmode of algorithmic differentiation (AD) is cost(∇USx0) ≈3·\ncost(Sx0) [1, Sec. 8.4.5]. Even Newton-type methods require\nmultiple iterations, leading to an effort of ≈3K ·cost(Sx0) to\ncompute only the gradients, where K denotes the number of\niterations. From Thm. (1) in combination with the exponen-\ntial shrinking of β (Algo. 1; Line 2) a q-linear convergence\nfollows, while advanced Hessian approximations can lead to\nfaster convergence rates of Newton-type methods restricted\nto the neighborhood of the optimum [1, Thm. 8.7].\nVI. NUMERICAL EXPERIMENTS\nIn this section, we provide two numerical experiments,\nwhile more complex DET-OCPs and additional technical\ndetails are presented in our previous work [11], although\nwithout a theoretical investigation of the method’s properties.\nA) Consider the scalar polynomial value function J(U) =\n1\n24c4U4 + 1\n6c3U3 + 1\n2c2U2 with U ∈R and c2,c3,c4 ∈R\nselected to determine a global minimum at J(0) = 0, a local\nminimum at J(−1\n2) = 1\n4, and the local maximum J(−1\n4) = 2.\nThe PDF of the optimal distribution of this function is plotted\nin Fig. 1 for different values of β > 0. The suboptimality of\nthe MPPI solution is plotted as a function of β in Fig. 2. For\nsmall β, the influence of the local minimum at W = −0.5\nvanishes in Fig. 1 and the corresponding progressions of the\nerror of the MPPI solution show the theoretically predicted\nbehavior.\nB) Consider the input-affine dynamics faf = x −1\n2 sin(3x)+\nu+w and the nonlinear dynamics fnl = x+arctan(u+w) with\nx,u,w ∈R, where w ∼N\n\u00000,β 2Σ\n\u0001\n, the stage cost function\nL(x,u) = 1\n2Ru2, the asymmetric terminal cost function E(x) =\n(x−1)6+x, and a horizon with N = 2 steps. For both settings,\nthe overall cost function has the structure\nJx0(U,W) = L(x0(·),u0)+L(x1(·),u1)+E(x2(·))\n(29)\n10!3\n10!2\n10!1\n100\n-\n10!10\n100\nSuboptimality\njj ~U ?\nMPPI(-) ! U ?\ndetjj = O\n!\n-2\"\njjJx0( ~U ?\nMPPI(-); 0) ! Vdet(x0)jj = O\n!\n-4\"\nFig. 2: Suboptimality of the MPPI solution to the DET-OCP of the\ncontrols (gold) and the value function (blue).\n\n\nshrinking -\n0\n0.5\n1\n1.5\n2\nu0\n0\n0.2\n0.4\n0.6\nu1\nshrinking -\n0\n0.5\n1\n1.5\n2\nu0\n0.5\n1\n1.5\nu1\nContour of log Jx0(U; 0)\nCLS-OCP solution\nOLS-OCP solution\nDET-OCP solution\nMPPI approx. of OLS-OCP\nDeterministic MPPI iterates\nFig. 3: Numerical solutions to the three different problems DET-\nOCP, OLS-OCP, and CLS-OCP (black), all with overall cost\nfunction (29) based on input-affine dynamics faf (top) and nonlinear\ndynamics fnl (bottom) alongside the iterates of deterministic MPPI.\nand can be substituted in the CLS-OCP formulation (4),\nthe OLS-OCP formulation (5), and the nominal DET-OCP\nformulation (6). Note that the CLS-OCP solution is visu-\nalized as a set because u1 is a policy in this setting. The\nsolutions are plotted with the I = 10 iterates of deterministic\nMPPI in Fig. 3. We choose x0 = −1, ˆu0 = ˆu1 = 0, ν =\n√\n2\n2 ,\nand a sufficient number of samples. The solutions differ\nsignificantly due to the asymmetry of the problem. As\nconsidered in Section III-A, only in the case of input-affine\ndynamics, standard MPPI provides a solution to the CLS-\nOCP. While, in both cases, standard MPPI provides weak\napproximations to the OLS-OCPs, the iterates of determin-\nistic MPPI converge q-linearly to the D-OCP solution.\nVII. SUMMARY\nThis letter provides a brief review of different stochastic\nOCP formulations and their solution methods from the MPPI\ncontrol framework and simplifies their notation. Further, we\nshow that in a smooth and unconstrained setting the subop-\ntimality of the MPPI solution to deterministic OCPs shrinks\nsmoothly as the level of remaining exploration uncertainty\ndecreases. This proves the convergence of the previously\npresented deterministic MPPI algorithm and shows that the\nsuboptimality of the MPPI solution can be modulated by\nappropriately tuning hyperparameters.\nACKNOWLEDGMENT\nThe authors would like to thank Lothar Kiltz for asking\nabout the convergence of deterministic MPPI and Katrin\nBaumg¨artner for valuable discussions on Laplace’s method.\nREFERENCES\n[1] J. B. Rawlings, D. Q. Mayne, and M. Diehl, Model predictive control:\nTheory, computation, and design, 2nd ed. Nob Hill Publishing, 2017.\n[2] M. Kazim, J. Hong, M.-G. Kim, and K.-K. K. Kim, “Recent advances\nin path integral control for trajectory optimization: An overview in\ntheoretical and algorithmic perspectives,” Annual Reviews in Control,\nvol. 57, 2024.\n[3] H. J. Kappen, “Path integrals and symmetry breaking for optimal\ncontrol theory,” J. of Statistical Mechanics: Theory and Experiment,\nvol. 2005, no. 11, 2005.\n[4] E. A. Theodorou, Buchli J., and S. Schaal, “A generalized path integral\ncontrol approach to reinforcement learning,” J. of Mach. learning\nresearch, vol. 2010, no. 11, 2010.\n[5] G. Williams, P. Drews, B. Goldfain, J. M. Rehg, and E. A. Theodorou,\n“Information-theoretic model predictive control: Theory and applica-\ntions to autonomous driving,” IEEE Trans. on Rob., vol. 34, no. 6, pp.\n1603–1622, 2018.\n[6] B. D. O. Anderson and J. B. Moore, Optimal control: Linear quadratic\nmethods, dover ed., augmented republ. of the ed. 1990 ed., ser. Dover\nbooks on engineering.\nMineola, NY: Dover Publications, 2007.\n[7] Y. Zhang, C. Pezzato, E. Trevisan, C. Salmi, C. H. Corbato, and\nJ. Alonso-Mora, “Multi-modal MPPI and active inference for reactive\ntask and motion planning,” IEEE Rob. and Autom. Letters, vol. 9,\nno. 9, pp. 7461–7468, 2024.\n[8] H. Homburger, S. Wirtensohn, and J. Reuter, “MPPI control of a\nself-balancing vehicle employing subordinated control loops,” in 2023\nEuropean Control Conf. (ECC).\nIEEE, 2023, pp. 1–6.\n[9] L. L. Yan and S. Devasia, “Output-sampled model predictive path\nintegral control (o-MPPI) for increased efficiency,” in 2024 IEEE Int.\nConf. on Rob. and Autom. (ICRA).\nIEEE, 2024, pp. 14 279–14 285.\n[10] B. Vlahov, J. Gibson, M. Gandhi, and E. A. Theodorou, “MPPI-\nGeneric: A CUDA library for stochastic optimization.”\narXiv, 2024,\nDOI:10.48550/arXiv.2409.07563.\n[11] P. Halder, H. Homburger, L. Kiltz, J. Reuter, and M. Althoff, “Tra-\njectory planning with signal temporal logic costs using deterministic\npath integral optimization,” in accepted to the IEEE Int. Conf. on Rob.\nand Autom., 2025.\n[12] F. Messerer, K. Baumg¨artner, S. Lucia, and M. Diehl, “Fourth-order\nsuboptimality of nominal model predictive control in the presence of\nuncertainty,” IEEE Control Systems Letters, vol. 8, pp. 508–513, 2024.\n[13] G. Williams, A. Aldrich, and E. A. Theodorou, “Model predictive path\nintegral control: From theory to parallel computation,” J. of Guidance,\nControl, and Dynamics, vol. 40, no. 2, pp. 344–357, 2017.\n[14] B. Øksendal, Stochastic Differential Equations.\nBerlin, Heidelberg:\nSpringer Berlin Heidelberg, 2003.\n[15] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning, ser.\nAdaptive computation and Mach. learning. Cambridge, Massachusetts\nand London, England: The MIT Press, 2016.\n[16] E. Theodorou, “Nonlinear stochastic control and information theoretic\ndualities: Connections, interdependencies and thermodynamic inter-\npretations,” Entropy, vol. 17, no. 5, pp. 3352–3375, 2015.\n[17] H.-J. Yoon, C. Tao, H. Kim, N. Hovakimyan, and P. Voulgaris, “Sam-\npling complexity of path integral methods for trajectory optimization,”\nin 2022 American Control Conf. (ACC). IEEE, 2022, pp. 3482–3487.\n[18] A. Patil, G. A. Hanasusanto, and T. Tanaka, “Discrete-time stochastic\nLQR via path integral control and its sample complexity analysis,”\nIEEE Control Systems Letters, vol. 8, pp. 1595–1600, 2024.\n[19] A. Shapiro, D. Dentcheva, and A. Ruszczynski, Lectures on Stochastic\nProgramming: Modelling and Theory.\nSIAM, 2009.\n[20] G. Nemes, “An explicit formula for the coefficients in Laplace’s\nmethod,” Constructive Approximation, vol. 38, no. 3, pp. 471–487,\n2013.\n[21] J. Peters and S. Schaal, “Reinforcement learning by reward-weighted\nregression for operational space control,” in Proc. of the Int. Conf. on\nMach. learning, 2007, pp. 745–750.\n[22] Z. I. Botev, D. P. Kroese, R. Y. Rubinstein, and P. L’Ecuyer, “The\ncross-entropy method for optimization,” in Handbook of Statistics -\nMach. Learning: Theory and Applications, ser. Handbook of Statistics.\nElsevier, 2013, vol. 31, pp. 35–59.\n[23] N. Hansen, “The CMA evolution strategy: A comparing review,”\nin Towards a New Evolutionary Computation.\nBerlin, Heidelberg:\nSpringer, 2006, vol. 192, pp. 75–102.\n[24] Z. Yi, C. Pan, G. He, G. Qu, and G. Shi, “CoVO-MPC: Theoretical\nanalysis of sampling-based mpc and optimal covariance design.”\n[25] G. C. Calafiore and M. C. Campi, “The scenario approach to robust\ncontrol design,” IEEE Trans. Automat. Control, 2006.\n[26] Z. Nagy and R. Braatz, “Robust nonlinear model predictive control of\nbatch processes,” AIChE J., vol. 49, no. 7, pp. 1776–1786, 2003.\n[27] ——, “Open-loop and closed-loop robust optimal control of batch\nprocesses using distributional and worst-case analysis,” J. of Process\nControl, vol. 14, pp. 411–422, 2004.\n[28] P. J. Goulart, E. C. Kerrigan, and J. M. Maciejowski, “Optimization\nover state feedback policies for robust control with constraints,”\nAutomatica, vol. 42, pp. 523–533, 2006.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20953v1.pdf",
    "total_pages": 6,
    "title": "Optimality and Suboptimality of MPPI Control in Stochastic and Deterministic Settings",
    "authors": [
      "Hannes Homburger",
      "Florian Messerer",
      "Moritz Diehl",
      "Johannes Reuter"
    ],
    "abstract": "Model predictive path integral (MPPI) control has recently received a lot of\nattention, especially in the robotics and reinforcement learning communities.\nThis letter aims to make the MPPI control framework more accessible to the\noptimal control community. We present three classes of optimal control problems\nand their solutions by MPPI. Further, we investigate the suboptimality of MPPI\nto general deterministic nonlinear discrete-time systems. Here, suboptimality\nis defined as the deviation between the control provided by MPPI and the\noptimal solution to the deterministic optimal control problem. Our findings are\nthat in a smooth and unconstrained setting, the growth of suboptimality in the\ncontrol input trajectory is second-order with the scaling of uncertainty. The\nresults indicate that the suboptimality of the MPPI solution can be modulated\nby appropriately tuning the hyperparameters. We illustrate our findings using\nnumerical examples.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}