{
  "id": "arxiv_2502.20479v1",
  "text": "Astronomy & Astrophysics manuscript no. aanda\n©ESO 2025\nMarch 3, 2025\nLeveraging Pre-Trained Visual Transformers for Multi-Band\nPhotometric Light Curve Classification\nD. Moreno-Cartagena1, 2⋆, P. Protopapas3, G. Cabrera-Vives1, 2, 4, 5, 6, M. Cádiz-Leyton1, 7, I. Becker3, and C.\nDonoso-Oliva1, 6\n1 Department of Computer Science, Universidad de Concepción, Edmundo Larenas 219, Concepción, Chile\n2 Center for Data and Artificial Intelligence, Universidad de Concepción, Edmundo Larenas 310, Concepción, Chile\n3 John A. Paulson School of Engineering and Applied Science, Harvard University, Cambridge, MA, 02138\n4 Heidelberg Institute for Theoretical Studies, Heidelberg, Baden-Württemberg, Germany\n5 Millennium Institute of Astrophysics (MAS), Nuncio Monseñor Sotero Sanz 100, Of. 104, Providencia, Santiago, Chile\n6 Millennium Nucleus on Young Exoplanets and their Moons (YEMS), Chile\n7 Edinburgh Futures Institute, University of Edinburgh, 1 Lauriston Pl, Edinburgh EH3 9EF, UK\nReceived September 15, 1996; accepted March 16, 1997\nABSTRACT\nContext. The advent of large-scale sky surveys, such as the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST),\nwill generate vast volumes of photometric data, necessitating automatic classification of light curves to identify variable stars and\ntransient events. However, challenges such as irregular sampling, multi-band observations, and diverse flux distributions across bands\ndemand advanced models for accurate classification.\nAims. This study investigates the potential of a pre-trained visual transformer (VT) model, specifically the Swin Transformer V2\n(SwinV2), to classify photometric light curves without the need for feature extraction or multi-band preprocessing. The goal is to\nassess whether this image-based approach can accurately differentiate astronomical phenomena and if it can serve as a viable option\nfor working with multi-band photometric light curves.\nMethods. We transformed each multi-band light curve into an image. These images serve as input to the SwinV2 model, which\nis pre-trained on ImageNet-21K. The datasets employed include the public Catalog of Variable Stars from the Massive Compact\nHalo Object (MACHO) survey, using both one and two bands, and the first round of the recent Extended LSST Astronomical Time-\nSeries Classification Challenge (ELAsTiCC), which includes six bands. The model’s performance was evaluated on 6 classes for the\nMACHO dataset and 20 distinct classes of variable stars and transient events for the ELAsTiCC dataset.\nResults. The fine-tuned SwinV2 achieved better performance than models specifically designed for light curves, such as Astromer\nand the Astronomical Transformer for time series And Tabular data (ATAT). When trained on the Full MACHO dataset, it attained\na macro F1-score of 80.2% and outperformed Astromer in single-band experiments. Incorporating a second band further improved\nperformance, increasing the F1-score to 84.1%. In the ELAsTiCC dataset, SwinV2 achieved a macro F1-score of 65.5%, slightly\nsurpassing ATAT by 1.3%.\nConclusions. SwinV2, a pre-trained VT model, effectively classifies photometric light curves, outperforming traditional models and\noffering a promising approach for large-scale surveys. This highlights the potential of using visual representations of light curves,\nwith future prospects including the integration of tabular data, textual information, and multi-modal learning to enhance analysis and\nclassification in time-domain astronomy.\nKey words. Methods: statistical – Methods: data analysis – Techniques: photometric – Stars: variables: general\n1. Introduction\nThe Vera C. Rubin Observatory’s Legacy Survey of Space and\nTime (LSST; Ivezi´c et al. 2019) is set to revolutionize our under-\nstanding of the universe by capturing an unprecedented volume\nof data, with an average of 10 million alerts and ∼20 TB of\nraw data every night (Hambleton et al. 2023). This vast dataset,\ncovering a range of celestial phenomena, including transient\nevents and stellar variability, will provide astronomers with a\nunique opportunity to study the physical properties and evolution\nof a diverse array of astronomical objects. However, the scale\nof these observations necessitates the development of efficient\nand accurate classification tools, as spectroscopic follow-up will\nbe limited. Automatic classification methods, particularly those\nbased on photometric data, will be essential to distinguish be-\n⋆e-mail: dmoreno2016@inf.udec.cl\ntween different types of variable stars and transient phenomena,\nensuring that the scientific potential of the LSST data is fully\nrealized (Fraga et al. 2024).\nMachine learning and deep learning techniques have played\na central role in advancing the classification of light curves\n(Pichara et al. 2012; Kim et al. 2011; Villar et al. 2019; Becker\net al. 2020; Cabrera-Vives et al. 2024). Traditional machine\nlearning approaches have been highly effective in classifying\nvariable stars and transient events by relying on hand-engineered\nfeatures extracted from light curves. For example, Richards et al.\n(2011) and Kim et al. (2014) employed statistical and time-\ndomain features to enhance classification accuracy. Karpenka\net al. (2013) combined parametric functional fitting of super-\nnova light curves with a machine learning algorithm, yielding\naccurate and robust classifications. Lochner et al. (2016) devel-\noped a multi-stage pipeline that evaluates feature extraction tech-\nArticle number, page 1 of 12\narXiv:2502.20479v1  [astro-ph.IM]  27 Feb 2025\n\n\nA&A proofs: manuscript no. aanda\nniques and machine learning models. Boone (2019) improved\nclassification by training a boosted decision tree on features ex-\ntracted from the augmented light curves, achieving state-of-the-\nart performance in the LSST Astronomical Time-Series Classi-\nfication Challenge (PLAsTiCC; Kessler et al. 2019). However,\nthis feature engineering process is resource-intensive and re-\nquires domain knowledge to be applied (Donalek et al. 2013;\nGraham et al. 2014; Nun et al. 2017; Sánchez-Sáez et al. 2021).\nDeep learning addresses this limitation by automatically learn-\ning relevant representations directly from light curves, remov-\ning the need for manual feature extraction. This advantage has\nled to the widespread adoption of recurrent neural networks\n(RNNs; Rumelhart et al. 1986), long short-term memory net-\nworks (LSTMs; Hochreiter 1997), and gated recurrent units\n(GRUs; Cho et al. 2014) (Charnock & Moss 2017; Naul et al.\n2018; Muthukrishna et al. 2019; Carrasco-Davis et al. 2019;\nMöller & de Boissière 2020; Becker et al. 2020; Jamal & Bloom\n2020; Gómez et al. 2020; Donoso-Oliva et al. 2021), as well\nas transformer-based models, which have shown great promise\nin handling astronomical time series data (Morvan et al. 2022;\nPan et al. 2022; Pimentel et al. 2022; Donoso-Oliva et al. 2023;\nMoreno-Cartagena et al. 2023; Cádiz-Leyton et al. 2024a,b; Al-\nlam Jr & McEwen 2024; Cabrera-Vives et al. 2024), making\nthem increasingly popular for astronomical classification tasks.\nNevertheless, even with deep learning models in the time\ndomain, careful preprocessing is required when dealing with\nmulti-band observations and irregular time sampling, as each\nband is often observed at different, non-uniform time intervals.\nVarious techniques have been developed to address these chal-\nlenges, such as uniform sampling imputation between previous\nand subsequent observations (Charnock & Moss 2017), linear\ninterpolation on a regular time grid across all bands (Muthukr-\nishna et al. 2019), Gaussian Process (GP) interpolation (Boone\n2019; Villar et al. 2020; Jamal & Bloom 2020), masking mech-\nanisms to indicate the availability of each band during obser-\nvations (Möller & de Boissière 2020; Pimentel et al. 2022;\nCabrera-Vives et al. 2024), and ensembles of RNNs that inte-\ngrate information from different bands into a single representa-\ntion (Becker, I. et al. 2025). While these methods have been eval-\nuated on various datasets, their generalization across different\nsurveys and observational conditions remains an open question\n(Moreno-Cartagena et al. 2023). Additionally, many of these ap-\nproaches require training from scratch and hyperparameter tun-\ning to search for the optimal architecture for a given task.\nFoundational models, which are trained only once on mas-\nsive datasets in a self-supervised manner (pre-training) to learn\ngeneral domain representations and later adapted for specific\ntasks (fine-tuning), have demonstrated exceptional generaliza-\ntion across diverse downstream applications in natural language\nprocessing and computer vision, such as text generation (Brown\net al. 2020), machine translation (Vaswani et al. 2017), image\nclassification (Dosovitskiy et al. 2021), and object detection\n(Carion et al. 2020), among others. Notable examples include\nGPT (Radford 2018) and BERT (Devlin et al. 2019) for text,\nas well as Vision Transformer (ViT; Dosovitskiy et al. 2021)\nfor images, each specializing in a single modality. In the con-\ntext of astronomical time series data, Astromer (Donoso-Oliva\net al. 2023) represents an early effort to pre-train a transformer on\nlarge-scale real-world light curve data, positioning it as a poten-\ntial foundational model for time-domain astronomy. However,\nsuch pre-training requires significant computational resources,\noptimized architectural design, and carefully curated datasets,\noften taking several days to converge.\nGiven these constraints, we propose leveraging the knowl-\nedge embedded in pre-trained visual transformer (VT) mod-\nels for the analysis and classification of multi-band photomet-\nric light curves. These models have demonstrated the ability to\ncapture fine details in images across various tasks (Han et al.\n2023; Khan et al. 2022). By using the informative representa-\ntions learned from extensive pre-training on large-scale datasets,\nconducted on powerful GPU clusters, we can establish a start-\ning point for fine-tuning the model on light curve images, by-\npassing the need to use significant computational resources for\npre-training and searching for an optimal architecture. Moreover,\nthese models can directly process raw multi-band light curves as\nimages, offering a straightforward yet highly effective approach\nto handling the data while avoiding complex feature engineering\nor multi-band preprocessing.\nA few studies considered light curves in the time domain for\nvisual models. Mahabal et al. (2017) transformed light curves\nfrom the Catalina Real-time Transient Survey (CRTS; Drake\net al. 2012) into 2D images by creating magnitude differences\nand time differences between pairs of points as input for a convo-\nlutional neural network (CNN; LeCun et al. 1998). Pasquet-Itam\n& Pasquet (2018) and Pasquet et al. (2019) used CNNs for the\nclassification of astronomical events, representing light curves\nas images, where the vertical axis corresponds to the number of\nphotometric bands, and the horizontal axis, represents the num-\nber of days in the first study and the number of observations in\nthe second. Qu et al. (2021) and Qu & Sako (2022) transformed\nmulti-band light curves into 2D heatmaps using GPs to repre-\nsent flux and its uncertainties over time and wavelength. These\nheatmaps were stacked into 32×180×2 tensors and fed into a\nCNN for supernova classification. Szklenár et al. (2020, 2022)\nexplored the classification of phase-folded light curves using\nthe Optical Gravitational Lensing Experiment (OGLE)-III and\n-IV databases (Udalski et al. 2008, 2015), representing them as\n128×128 monochromatic pixel images with brightness measure-\nments plotted as white dots on a black background. Similarly,\nMonsalves et al. (2024) transformed light curves into 32×32\npixel 2D histograms, where phase and magnitude bins were used\nas inputs to a CNN to classify different types of variable stars.\nHowever, all these studies depended on a prior level of prepro-\ncessing and did not use pre-trained VT models.\nIn this work, we present the use of a pre-trained VT model\ncalled Swin Transformer V2 (SwinV2; Liu et al. 2022), which\nwe evaluate on two datasets: the public Catalog of Variable Stars\n(Alcock et al. 2003), derived from the Massive Compact Halo\nObject (MACHO; Alcock et al. 2000) survey, and the first round\nof the recent Extended LSST Astronomical Time-Series Classifi-\ncation Challenge (ELAsTiCC1; Narayan & Team 2023), a multi-\nband dataset of photometric light curves for variable stars and\nsupernovae designed to simulate the observations of the future\nLSST. Our work builds upon the methodology proposed by Li\net al. (2024), which focuses on benchmark datasets for time se-\nries. We applied this method to light curves, performed hyperpa-\nrameter tuning, and evaluated its performance using both single-\nand multi-band configurations for the MACHO dataset and six\nbands for the ELAsTiCC dataset. Additionally, we assess its per-\nformance in scenarios with a limited number of astronomical ob-\njects per class and imbalanced datasets. To our knowledge, this is\nthe first study to apply a pre-trained VT to raw multi-band light\ncurves in the time domain. The key contributions of this work\nare as follows:\n1 The DESC ELAsTiCC Challenge.\nArticle number, page 2 of 12\n\n\nD. Moreno-Cartagena et al.: Leveraging Pre-Trained Visual Transformers for Multi-Band Light Curve Classification\nTwo-bands\nSix-bands\n(a) Grid approach\nSingle-band\nTwo-bands\nSix-bands\n(b) Overlay approach\nFig. 1. Comparison of visualization strategies. Panel (a) illustrates the\nGrid approach, while panel (b) depicts the Overlay approach. Each\nimage was generated using the best hyperparameters identified for its\nrespective method. The single-band and two-band example is a Long-\nPeriod Variable (LPV) (ID: 24.3466.13) from the MACHO dataset,\nwhereas the six-band example is a Pair-Instability Supernova (PISN)\n(ID: 41820707) from the ELAsTiCC dataset.\n– Paving the way for the use of pre-trained VT models to ana-\nlyze and classify light curves.\n– Achieving competitive results compared to state-of-the-art\nmethods using only light curves.\n– Providing a robust solution capable of leveraging multiple\nbands simultaneously.\n– Presenting a simple and effective approach for the classifica-\ntion of photometric light curves.\n2. Methods\n2.1. Multi-band light curves to images\nTransforming time series into images is crucial for applying VT\nmodels, which have proven effective in analyzing complex vi-\nsual data (Dosovitskiy et al. 2021; Liu et al. 2021, 2023). In\nthis study, we evaluate two approaches for converting multi-band\nlight curves into images: Grid and Overlay. Figure 1 (a) shows\nthe input for the Grid approach, while (b) illustrates the input for\nthe Overlay approach.\nIn the Grid approach, light curves from different bands are\narranged in a grid, with each band displayed in a separate panel,\nfollowing a similar methodology to Li et al. (2024). We use a\nsquare layout that adjusts to the number of bands present in each\nlight curve, leaving empty panels when there are no observations\nfor a particular band. This arrangement facilitates a clear com-\nparison between bands by keeping them visually distinct. In the\nOverlay approach, all bands of a light curve are plotted together\nwithin the same space. This approach allows us to examine how\ninteractions among observations from different bands influence\nthe visual representation and model performance.\nTo ensure that all light curve images are on the same scale\nand maintain consistent x- and y-limits across different images,\nwe apply Min-Max normalization to each entire light curve (con-\nsidering all bands). This approach preserves the relative differ-\nences among bands within each light curve while scaling time\nand flux values across all light curves to the range [0, 1]. Ad-\nditionally, the flux errors are scaled using the same Min-Max\nfactor applied to the flux, maintaining the relative uncertainty\ninformation. This method ensures that all images are visually\ncomparable, with uniform axis limits and scale throughout the\ndataset.\nSince pre-trained VT models are trained on RGB images, the\ninput must conform to this expected format, requiring a three-\nchannel representation. In the Grid approach, the specific color\nassigned to each band is inconsequential, as each band occupies\na distinct spatial position. However, in the Overlay approach,\nwhere all bands are combined into the same visual space, color\nselection becomes crucial to avoid introducing visual biases that\ncould affect the model’s performance. To mitigate this issue and\nensure that all bands contribute equally to the final representa-\ntion, we assign colors that approximate distinct RGB compo-\nnents, maintaining a perceptually balanced distribution across\nthe spectrum. For datasets with six bands, we allocate two bands\nto the red spectrum with the RGB values (255, 0, 127) and (255,\n127, 0), two bands to the green spectrum with (0, 255, 127) and\n(127, 255, 0), and the remaining two to the blue spectrum with\n(0, 127, 255) and (127, 0, 255). This selection ensures an even\ndistribution of color intensity, preventing any single band from\ndominating the representation. For datasets containing only two\nbands, each is mapped to a primary RGB component to preserve\nvisual balance and maintain interpretability.\nAdditionally, in both approaches, we consider visual proper-\nties as hyperparameters to be adjusted during the conversion of\nlight curves to images. These include marker size (markersize),\nline width (linewidth), and the inclusion or exclusion of flux un-\ncertainties in the images (yerr). These plot properties influence\nnot only the visualization but also the model’s ability to extract\nrelevant features effectively. The visual hyperparameters were\noptimized to maximize classification performance, as detailed in\nthe implementation subsection.\n2.2. Visual transformer models\nVT models represent a significant evolution in deep learning\narchitectures for image processing. Unlike conventional ap-\nproaches based on convolutions, such as CNNs, VTs employ\nself-attention mechanisms to capture long-range relationships\nbetween different regions of an image (Dosovitskiy et al. 2021).\nThis methodology enables VTs to capture intricate structures\nwithin images, achieving robust performance in various tasks,\nsuch as classification (Liu et al. 2021), object detection (Carion\net al. 2020), semantic and instance segmentation (Wang et al.\n2021; Cheng et al. 2021), object tracking (Chen et al. 2021b),\nimage generation (Jiang et al. 2021), and image enhancement\n(Chen et al. 2021a), among others. Furthermore, VT models that\nare pre-trained can use the prior knowledge gained from exten-\nsive datasets, making them adaptable to particular tasks via a\nfine-tuning process.\nSwinV2 is a hierarchical VT architecture designed for ef-\nficient and scalable image processing (Liu et al. 2022). Unlike\nViT (Dosovitskiy et al. 2021), which partitions an image into\nnon-overlapping segments called patches, applying global self-\nattention to all of them simultaneously, the first Swin Trans-\nformer paper (Liu et al. 2021) introduces two key innovations\nthat allow it to manage both local and global self-attention:\nshifted windows and hierarchical feature representation. These\nfeatures enable the model to handle high-resolution images with\ngreater computational efficiency. Furthermore, SwinV2 incorpo-\nrates scaled cosine attention and logarithmic continuous position\nArticle number, page 3 of 12\n\n\nA&A proofs: manuscript no. aanda\nbias, improving both training stability and performance in tasks\nsuch as image classification, object detection, semantic segmen-\ntation, and video action classification. We selected this model as\nthe foundation of our approach because its hierarchical structure\nis particularly effective at capturing detailed information at the\npixel level, which is essential for interpreting points and lines\nwithin the light curve image. Figure 2 illustrates the model’s ar-\nchitecture and hierarchical processing. In the following items,\nwe break down its components and functionality in detail.\n1. Patch Splitting and Tokenization: Similar to ViT, Swin\nfirst partitions the input image into non-overlapping patches.\nHowever, unlike ViT, Swin progressively reduces the patch\nsize as the image propagates through the Swin Trans-\nformer block (explained in item 6). Given an input image\nof 256×256 pixels with 3 color channels (e.g., RGB) and\na patch size of 4×4, the patch partitioning process yields\n64×64 patches. This is equivalent to applying a convolu-\ntional kernel of size 4×4 with a stride of 4×4 over the im-\nage. Each patch contains 4×4×3=48 values, which represent\nthe RGB pixel intensities within that region. These values\nare then flattened and projected into a higher-dimensional\nspace via a linear transformation, producing a token map of\ndimensions 64×64×C, where C is the embedding dimension.\nIn this context, a token refers to the vector representation of\na patch after the linear transformation. This process is func-\ntionally analogous to applying a two-dimensional convolu-\ntion layer (Conv2d). The resulting token map is subsequently\nprocessed through the Swin Transformer block for hierar-\nchical processing.\nPatch Merging\nC\n Blocks\n  \n \nImages\nLinear Embedding\nSwin Transformer\nBlock\n C\n C\n C\nPatch Partition\nPatch Merging\nPatch Merging\n Blocks\n Blocks\n Blocks\nEmbedding\nSwin Transformer\nBlock\nSwin Transformer\nBlock\nSwin Transformer\nBlock\nFig. 2. SwinV2 architecture. The rounded rectangle in light blue high-\nlights the components where the model changes the dimensions of the\ninformation, while the regular rectangles indicate where they remain\nfixed.\n2. Window Multi-head Self-Attention (W-MSA): The first\nstep of the Swin Transformer block partitions the tokens\ninto non-overlapping windows of size M×M. Unlike tradi-\ntional Multi-head Self-Attention (MSA), which computes at-\ntention globally and has quadratic complexity, W-MSA re-\nstricts attention to local windows. This reduces the computa-\ntional complexity from O(N2) in global attention to O(M2N)\nin W-MSA, where N denotes the total number of tokens in\nthe image. Since M is fixed, W-MSA maintains linear com-\nplexity with respect to N, significantly reducing computa-\ntional cost while ensuring that each window effectively cap-\ntures local dependencies, making the approach scalable for\nhigh-resolution images.\n3. Shifted Window Multi-head Self-Attention (SW-MSA):\nTo address the limited cross-window interactions in W-MSA,\nSW-MSA introduces a shifted window mechanism. This\nshifts the windows from the W-MSA step, allowing tokens\nat the edges of one window to be included in the cen-\nters of overlapping windows. This process enables informa-\ntion exchange between neighboring windows. By alternat-\ning W-MSA and SW-MSA within each Swin Transformer\nblock, the model captures global context while preserving\nthe computational advantages of local attention. For a more\ndetailed explanation of the W-MSA and SW-MSA mecha-\nnisms, please refer to the first Swin Transformer paper (Liu\net al. 2021).\n4. Post Normalization and Scaled Cosine Attention: Most\nVT models, including the original Swin Transformer, apply\nlayer normalization before the attention mechanism. How-\never, as model depth increases, deeper layers develop larger\nactivation magnitudes due to residual connections, leading\nto training instability. To address this, SwinV2 adopts resid-\nual post-normalization, where normalization is applied after\nthe attention mechanism and the output of the Swin Trans-\nformer block, preventing the accumulation of large activa-\ntions. Additionally, traditional dot-product attention in large\nmodels can cause certain heads to focus excessively on a few\npixel pairs, particularly in the residual post-normalization\nsetting. To mitigate this, SwinV2 introduces scaled cosine\nattention, which replaces dot-product similarity with cosine\nsimilarity, normalizing attention scores and improving train-\ning stability.\n5. Log-spaced Continuous Position Bias: SwinV2 also re-\nplaces the relative position bias with a log-spaced continuous\nposition bias. This design prioritizes local token interactions\nwhile gradually reducing the influence of distant tokens, en-\nabling the model to generalize better across different image\nresolutions.\nRRab\nRRc\nCep 0\nCep 1\nEC\nLPV\nClass\n0\n2000\n4000\n6000\nNumber of objects\nDistribution of objects by class in MACHO labeled\nTraining\nValidation\nFig. 3. Distribution of periodic variable star classes in the training and\nvalidation sets for the MACHO dataset.\nArticle number, page 4 of 12\n\n\nD. Moreno-Cartagena et al.: Leveraging Pre-Trained Visual Transformers for Multi-Band Light Curve Classification\n6. Hierarchical Representation with Patch Merging: Fol-\nlowing the operations described in items 2 to 5, which con-\nstitute the Swin Transformer block, the model applies patch\nmerging to create a hierarchical representation. This opera-\ntion reduces the spatial resolution of the token map while in-\ncreasing its embedding dimension. For instance, a token map\nof size 64×64×96 is downsampled to 32×32×192, where\nthe resolution is halved, and the embedding dimension is\ndoubled. By incorporating this output into subsequent Swin\nTransformer blocks and further applying patch merging,\nthe hierarchical architecture enables the model to effectively\ncapture multi-scale features.\n7. Downstream Task: After the hierarchical stages, the fi-\nnal token representations undergo adaptive average pooling,\nwhich computes the mean across all tokens of size H\n32 × W\n32,\npreserving their aggregated embedding information (8C).\nThis process allows each image to be represented as a vec-\ntor of dimension 8C, which is then passed through a fully\nconnected linear layer for classification or other downstream\ntasks.\n3. Experimental Setup\n3.1. Data description\n3.1.1. MACHO labeled\nThe public Catalog of Variable Stars2 (Alcock et al. 2003) con-\ntains labeled light curves from the MACHO survey, which ob-\nserved variable stars in the Large Magellanic Cloud (LMC) and\nthe Galactic Bulge over multiple years, capturing their photo-\nmetric variability in the B and R bands. This dataset has a me-\ndian cadence of 0.75 days with a standard deviation of 5.03 days,\nconsidering both bands. The labeled portion we use corresponds\nto the same one used in Donoso-Oliva et al. (2023), which was\ncategorized into six distinct classes, comprising 20 894 variable\nstars. We will refer to this as the Full dataset.\nFollowing the approach in Donoso-Oliva et al. (2023), we\nuse scenarios with few samples per class (spc) to train the model\n2 MACHO Labeled Data of the Variable Stars.\nand assess its ability to learn from limited labeled data while\nkeeping the test set unchanged. From the Full dataset, we ex-\ntracted 20 spc and 500 spc as our new training datasets. As in the\nreferenced study, the IDs for each fold were selected randomly\nfrom the Full dataset. For example, the IDs for 20 and 500 spc\nin the first fold differ from those in the second fold. Our train-\ning, validation, and test sets in each fold use the same IDs that\nwere used in the referenced study. The test set contains 100 light\ncurves per class and remains fixed across all scenarios (20 spc,\n500 spc, and Full dataset). The training dataset is divided into\nthree stratified folds, maintaining an 80/20 split for training and\nvalidation. This results in 96 and 24 light curves for the 20 spc\nscenario, 2 400 and 600 for the 500 spc scenario, and 16 232 and\n4 062 for the Full dataset. Figure 3 illustrates the class distribu-\ntion in the training and validation sets for the Full dataset.\nIt is worth mentioning that the referenced paper used only the\nR band. However, in our work, we tested both cases: one using\nonly the R band to provide a direct comparison with Astromer\n(Donoso-Oliva et al. 2023), a transformer model specifically de-\nsigned for single-band light curve data, and another incorporat-\ning both bands to demonstrate the effectiveness of the multi-band\napproach.\n3.1.2. ELAsTiCC Challenge\nThe second dataset used in this study comes from the ELAs-\nTiCC3 challenge (Narayan & Team 2023), designed to simu-\nlate the observations of the future LSST at the Vera C. Rubin\nObservatory. It contains 1 845 146 simulated light curves, each\nrepresented across six bands: u, g, r, i, z, and Y, covering the\nvisible and near-infrared spectrum. The data includes multiple\nobservations of each object spread over several simulated years.\nELAsTiCC spans a diverse array of transient and variable stellar\nevents, categorized into 32 distinct classes, with a median ca-\ndence of 0.93 days and a standard deviation of 16.18 days, con-\nsidering all six bands. Specifically, we used the first ELAsTiCC\ncampaign, which was used in Cabrera-Vives et al. (2024), and\nadopted the same class distribution as their study, encompassing\n20 different categories. Additionally, similar to the approach out-\n3 ELAsTiCC Challenge Data Portal.\nCART\nIax\n91bg\nIa\nIb/c\nII\nSN-like/Other\nSLSN\nPISN\nTDE\nILOT\nKN\nM-dwarf Flare\nuLens\nDwarf Novae\nAGN\nDelta Scuti\nRR Lyrae\nCepheid\nEB\nClass\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\nNumber of objects\nDistribution of objects by class in ELAsTiCC dataset\nTraining\nValidation\nILOT\nKN\nM-dwarf Flare\nuLens\nDwarf Novae\n0\n5000\n10000\n15000\n20000\nFig. 4. Distribution of transients, stochastic variables, and periodic variable star classes in the training and validation sets for the first round of the\nELAsTiCC dataset.\nArticle number, page 5 of 12\n\n\nA&A proofs: manuscript no. aanda\nlined in the referenced study, we applied the PHOTFLAG4 key\nto filter out saturated data. For each light curve, we also consid-\nered both alerts and forced photometry points, beginning 30 days\nprior to the first alert up to the final detection. Non-detections\nfollowing the last detection were omitted from the dataset.\nOur training, validation, and test sets were the same as those\nused in Cabrera-Vives et al. (2024). The test set contains 1 000\nlight curves from each class, ensuring a balanced representation.\nThe remaining data is divided into five stratified folds, main-\ntaining an 80/20 ratio for training and validation. This results in\n1 460 117 and 365 029 light curves in the training and validation\nsets, respectively. Figure 4 shows the distribution of the classes\nin the training and validation sets.\n3.2. Implementation details\nThe conversion of light curves to images was performed using\nthe Matplotlib Python package, and the images were directly\ntransformed into tensors with RGB channels. We generated im-\nages with dimensions of 256×256, as this is the size for which\nthe VT model was pre-trained. In both the Grid and Overlay ap-\nproaches, the images were adjusted to minimize extra space, and\nin the Grid approach, borders were added to separate the bands\n(see Fig. 1 for an illustration of these representations).\nIn our experiments, we used the SwinV2 model pre-trained\non the ImageNet-21K dataset, available through the Hugging\nFace Python library5. Image processing was performed using\nthe AutoImageProcessor class associated with the pre-trained\nmodel, which included resizing to 256×256 pixels, rescaling\npixel values to the range [0, 1], and normalizing them using the\nmean and standard deviation from ImageNet-21K. This process\nensured the proper adaptation of the input data to the format re-\nquired by the SwinV2. The model operates on input images of\nsize 256×256×3 pixels, divides them into patches of 4×4 pixels,\nuses an embedding dimension of C = 96, and performs self-\nattention within windows of 16×16 patches. The training was\nconducted on an NVIDIA A100 GPU.\nWe experimented with varying key hyperparameters during\nthe conversion of light curves into images to identify the best\nvisual representation that enhances classification performance,\nas measured by the F1-score, which balances how accurately\nthe model identifies light curves of a given class (precision)\nand its ability to recognize all light curves that truly belong\nto that class (recall). Table 1 summarizes the hyperparameters\nused. Since the MACHO labeled dataset contains a limited num-\nber of light curves and to assess the model’s ability to learn\nfrom datasets with small to medium sample sizes per class, we\nperformed hyperparameter tuning using grid search and k-fold\ncross-validation for each scenario (20 spc, 500 spc, and Full\ndataset). This means that for each fold, a total of 160 combina-\ntions were tested for the 20 spc, 500 spc, and Full dataset. In\ncontrast, due to the considerable size of the ELAsTiCC dataset,\nwe opted to execute hyperparameter tuning with just one vali-\ndation fold. This decision stemmed from the high computational\nexpense associated with evaluating multiple folds and the an-\nticipated low variance across folds in large-scale datasets. As\nbefore, a total of 160 combinations were tested, but only in\none fold. The lower and upper bounds for the hyperparameters\nmarker size and line width were determined based on the degree\nof visual overlap between the observations and lines in the im-\nages. Figure 1 (a) and (b) present images generated using the\n4 Description of the data variables from the first round of ELAsTiCC.\n5 Pre-trained Hugging Face model used.\nTable 1. Summary of the hyperparameters used for model optimization.\nHyperparameter\nValues\nMarker size\n{1, 2, 3, 4, 5}\nLine width\n{0.5, 1.0, 1.5, 2.0}\nFlux errors\n{True, False}\nInput format\n{Grid, Overlay}\nLearning rate\n{5 · 10−5, 5 · 10−6}\nbest hyperparameters identified for the Grid and Overlay ap-\nproaches, respectively.\nTo mitigate class imbalance, we employed a weighted sam-\npling strategy6 that adjusted the selection probability of each\nclass according to its relative frequency, promoting a more bal-\nanced representation in training and improving model general-\nization. Additionally, we optimized the model using Adam and\napplied early stopping with a patience of 10 epochs for the MA-\nCHO datasets and 5 epochs for ELAsTiCC, based on the valida-\ntion F1-score.\n4. Results\nWe evaluated the classification performance of the SwinV2\nmodel on two previously mentioned datasets: MACHO and\nELAsTiCC. These datasets exhibit distinct characteristics, par-\nticularly in terms of the number of observations per light curve,\nthe number of bands, and their cadences and taxonomies, which\ninfluenced both hyperparameter choices and classification per-\nformance. We initiate our analysis with the MACHO dataset,\nexploring both single-band and two-band experiments, before\nexpanding our assessment to the ELAsTiCC dataset, which in-\ncludes six bands. We trained a separate model for each fold and\nreport the macro F1-score as the mean and standard deviation\ncomputed across the models evaluated on the test set, referred to\nin the text as simply the F1-score.\n4.1. Model performance in single- and multi-band light\ncurves\nTo evaluate the classification performance of our approach using\na single band, we compared SwinV2 with Astromer. Our initial\nexperiments focused on the R band of the MACHO dataset to\nensure direct comparability with the results reported in the ref-\nerenced study. We conducted hyperparameter tuning to optimize\nthe model’s performance and to assess its ability to learn from\n6 PyTorch’s WeightedRandomSampler.\nTable 2. F1-score (%) on the MACHO test set for different models\ntrained on varying sample sizes.\nModel\n20 spc\n500 spc\nFull dataset\nAstromer - F\n56.0 ± 2.5\n71.0 ± 1.8\n-\nAstromer - T\n44.0 ± 5.0\n73.0 ± 3.0\n-\nSwinV2 - T\n60.1 ± 4.6\n77.4 ± 1.3\n79.8 ± 2.2\nNotes. F denotes a frozen transformer, where only the classifier is up-\ndated. T represents a trainable transformer, where both the transformer\nand classifier are updated during training.\nArticle number, page 6 of 12\n\n\nD. Moreno-Cartagena et al.: Leveraging Pre-Trained Visual Transformers for Multi-Band Light Curve Classification\ndatasets with small to medium sample sizes per class. The best\ncombination of hyperparameters, using one band, for the 20 and\n500 spc used to train the model, as well as the Full dataset, can\nbe found at the top of Table A.1 in Appendix A. These combina-\ntions were then used to generate the SwinV2 results presented in\nTable 2.\nTable 2 presents the single-band classification results on\nthe MACHO dataset in the R band for SwinV2 and Astromer.\nThe table reports the F1-scores along with their standard devia-\ntions for SwinV2 (in trainable configurations) and Astromer (in\nboth frozen and trainable configurations) across different dataset\nsizes. In this context, a trainable model refers to one in which\nboth the transformer and classifier layers are updated during\ntraining, whereas a frozen model retains the transformer’s pre-\ntrained weights and only updates the classifier. Since results for\nthe Full dataset were not available for Astromer, we compared\nit only in the limited-data scenarios (20 and 500 spc). However,\nthe Full dataset results provide a basis for evaluating our multi-\nband method. As shown in Table 2, SwinV2 consistently out-\nperformed Astromer across all configurations and dataset sizes.\nFor 20 spc, SwinV2 achieved an F1-score of 60.1%, which is\n4.1% points higher than Astromer’s best result of 56.0%. For\n500 spc, SwinV2 attained 77.4%, surpassing Astromer by 4.4%\npoints. These results demonstrate the robustness and adaptabil-\nity of the VT model in achieving high classification performance\non light curves, particularly in low-data scenarios. When tested\non the unbalanced dataset (i.e., the Full dataset, as illustrated\nin Fig. 3), SwinV2 achieved an F1-score of 79.8%, surpassing\nits performance on the balanced dataset with 500 spc (77.4%).\nThis result highlights the VT model’s ability to maintain strong\nperformance even in the presence of data imbalance.\nTo demonstrate the impact of the multi-band approach, we\nextended our analysis to incorporate both the R and B bands\nfrom the MACHO dataset, employing two distinct plotting\nstrategies: Grid and Overlay. As before, we performed hyper-\nparameter tuning on each scenario. The best combination of hy-\nperparameters using two bands can be seen in the middle of Ta-\nble A.1 in Appendix A. The results in Fig. 5 show that incorpo-\nrating multi-band inputs improves F1-scores compared to single-\nband experiments. Using two bands led to performance improve-\nments of 6.8%, 4.3%, and 4.5% for the 20, 500, and Full dataset\nsettings, respectively, relative to the best multiband approach\nin each case. These improvements highlight the model’s abil-\nity to effectively leverage the additional information provided by\nmultiple bands, reinforcing the value of multi-band configura-\ntions for capturing the complex behavior of light curves through\n20 samples per class\n500 samples per class\nFull dataset\nDataset Size\n60%\n70%\n80%\n90%\nTest F1-score (%)\n60.1%\n77.4%\n79.8%\n66.9%\n80.6%\n82.3%\n64.3%\n81.7%\n84.3%\nSwinV2’s performance on different MACHO datasets\nOne-band\nTwo-band (Overlay)\nTwo-band (Grid)\nFig. 5. SwinV2 classification performance on the MACHO test set using\nsingle- and multi-band data, trained on varying sample sizes. The figure\npresents the best results from the hyperparameter search for both the\nOverlay and Grid approaches.\nRRab\nRRc\nCep 0\nCep 1\nEC\nLPV\nTrue Label\n0.4\n±0.26\n0.42\n±0.14\n0.0\n0.04\n0.12\n±0.09\n0.01\n0.25\n±0.18\n0.59\n±0.1\n0.01\n0.01\n0.14\n±0.07\n0.0\n0.02\n0.02\n0.68\n±0.07\n0.26\n±0.08\n0.02\n0.0\n0.02\n0.04\n0.28\n±0.08\n0.55\n±0.03\n0.1\n±0.05\n0.01\n0.05\n±0.03\n0.18\n±0.03\n0.02\n0.12\n±0.05\n0.56\n±0.07\n0.07\n±0.01\n0.0\n0.01\n0.0\n0.06\n±0.05\n0.05\n±0.03\n0.88\n±0.06\n0.7\n±0.14\n0.26\n±0.13\n0.0\n0.02\n0.01\n0.0\n0.44\n±0.15\n0.53\n±0.16\n0.0\n0.01\n0.02\n0.0\n0.02\n0.01\n0.58\n±0.2\n0.38\n±0.21\n0.01\n0.0\n0.05\n±0.01\n0.03\n0.16\n±0.09\n0.72\n±0.07\n0.03\n0.0\n0.12\n±0.05\n0.08\n±0.04\n0.02\n0.08\n±0.02\n0.59\n±0.06\n0.11\n±0.05\n0.0\n0.0\n0.01\n0.01\n0.06\n±0.05\n0.92\n±0.05\nRRab\nRRc\nCep 0\nCep 1\nEC\nLPV\nTrue Label\n0.76\n±0.07\n0.19\n±0.04\n0.0\n0.01\n0.05\n0.0\n0.16\n±0.03\n0.79\n±0.01\n0.0\n0.0\n0.06\n±0.02\n0.0\n0.02\n0.0\n0.72\n±0.05\n0.25\n±0.05\n0.01\n0.0\n0.02\n0.02\n0.17\n±0.07\n0.74\n±0.08\n0.04\n0.0\n0.06\n±0.04\n0.09\n±0.03\n0.03\n0.06\n±0.02\n0.7\n±0.04\n0.06\n±0.03\n0.0\n0.0\n0.0\n0.0\n0.05\n0.95\n±0.04\n0.88\n±0.03\n0.09\n±0.03\n0.0\n0.0\n0.03\n0.0\n0.24\n±0.06\n0.72\n±0.06\n0.0\n0.01\n0.03\n0.0\n0.01\n0.0\n0.82\n±0.01\n0.16\n±0.02\n0.01\n0.0\n0.04\n0.01\n0.16\n±0.04\n0.77\n±0.03\n0.02\n0.0\n0.07\n±0.03\n0.07\n±0.03\n0.02\n0.05\n0.73\n±0.04\n0.06\n±0.04\n0.0\n0.0\n0.0\n0.0\n0.01\n0.99\n±0.01\nRRab\nRRc\nCep 0\nCep 1\nEC\nLPV\nPredicted Label\nRRab\nRRc\nCep 0\nCep 1\nEC\nLPV\nTrue Label\n0.9\n±0.02\n0.07\n±0.02\n0.0\n0.0\n0.03\n0.0\n0.25\n±0.03\n0.66\n±0.06\n0.0\n0.0\n0.08\n±0.02\n0.0\n0.01\n0.0\n0.82\n±0.03\n0.14\n±0.03\n0.03\n0.0\n0.04\n0.01\n0.2\n±0.03\n0.65\n±0.02\n0.09\n±0.02\n0.0\n0.07\n±0.02\n0.04\n0.02\n0.02\n0.81\n±0.02\n0.05\n±0.03\n0.0\n0.0\n0.0\n0.0\n0.04\n0.96\n±0.02\nRRab\nRRc\nCep 0\nCep 1\nEC\nLPV\nPredicted Label\n0.89\n±0.02\n0.08\n±0.02\n0.0\n0.0\n0.03\n0.0\n0.2\n±0.02\n0.78\n±0.02\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.88\n±0.04\n0.1\n±0.04\n0.01\n0.0\n0.03\n0.0\n0.17\n±0.0\n0.72\n±0.02\n0.07\n±0.01\n0.0\n0.04\n0.05\n±0.01\n0.02\n0.02\n0.81\n±0.05\n0.05\n±0.03\n0.0\n0.0\n0.0\n0.0\n0.02\n0.98\n±0.01\nSingle-band\nMulti-band\n(a) 20 samples per class\n(b) 500 samples per class\n(c) Full dataset\nFig. 6. Confusion matrices for the best classification results obtained us-\ning SwinV2 with single-band and two-band inputs on different subsets\nof the MACHO dataset. Standard deviations are displayed for values\ngreater than or equal to 0.05.\nimage-based representations. For 20 spc, Overlay outperformed\nGrid (F1-score: 66.9% vs. 64.3%). With 500 spc, Grid achieved\nan F1-score of 81.7%, marginally surpassing Overlay (80.6%).\nWhen using the Full dataset, Grid attained a higher F1-score of\n84.3%, slightly outperforming Overlay (82.3%). These results\nindicate that Grid achieved better performance in two configura-\ntions. However, the differences were minimal.\nThe confusion matrices for the best-performing SwinV2\nmodels across different dataset sizes, for both single- and multi-\nband settings, are shown on the left and right of Fig. 6, respec-\ntively. These matrices illustrate the recall per class, providing\ninsight into how well the model correctly identifies each cate-\ngory. For the subset with 20 spc, using two bands instead of one\nimproved recall in 4 out of 6 classes (RRab, Cep_1, EC, and\nLPV) but decreased it for RRc and Cep_0. For the subset with\n500 spc, recall increased in 5 out of 6 classes (RRab, Cep_0,\nCep_1, EC, and LPV) but decreased by 7% for RRc. For the\nFull dataset, a similar pattern was observed. Recall improved\nin 4 out of 6 classes (RRc, Cep_0, Cep_1, and LPV), remained\nunchanged for EC, and decreased by 1% for RRab. The consis-\nArticle number, page 7 of 12\n\n\nA&A proofs: manuscript no. aanda\n20 samples per class\n500 samples per class\nFull dataset\nDataset Size\n30%\n40%\n50%\n60%\n70%\n80%\nTest F1-score (%)\nF1-score distribution in MACHO hyperparameter tuning\nOne-band\nTwo-band (Overlay)\nTwo-band (Grid)\nFig. 7. Distribution of F1-scores obtained during hyperparameter tun-\ning for the MACHO dataset, comparing single-band and two-band\n(Overlay and Grid) approaches across different dataset sizes (20 spc,\n500 spc, and the Full dataset).\ntent improvement observed in Cep_1, EC, and LPV suggests that\nthese classes benefit from the additional spectral information. In\ncontrast, RRab, RRc, and Cep_0 exhibit mixed results, indicat-\ning that while multi-band inputs are beneficial in most cases,\nthey do not always provide a clear advantage, especially for cer-\ntain classes that remain challenging to distinguish due to their\nsimilar flux variations in both bands. This is particularly evident\nin RRc and RRab, as well as Cep_0 and Cep_1.\nFigure 7 shows the distribution of F1-scores obtained during\nthe hyperparameter tuning process for the MACHO dataset, con-\nsidering both single-band and two-band approaches for each sce-\nnario (20 spc, 500 spc, and the Full dataset). In general, the me-\ndian F1-score values across all combinations remain consistent\nwith those of the best combinations presented in Fig. 5, where\nthe Overlay approach performed better for 20 spc, while the\nGrid approach was superior for 500 spc and the Full dataset.\nIn particular, the F1-score for 500 spc and the Full dataset ex-\nhibited little variation, regardless of the hyperparameter com-\nbinations used. The exception is the 20 spc scenario, where a\npoor choice of hyperparameters can lead to low performance due\nto the limited amount of data, which restricts the model’s abil-\nity to extract sufficient discriminative information from the light\ncurves.\n4.2. Model performance in ELAsTiCC Challenge\nTo assess the model’s performance on a large-scale dataset, we\nused ELAsTiCC, which includes six bands. In line with prior\ndatasets, we fine-tuned hyperparameters to enhance the model’s\neffectiveness. The best hyperparameter combination for Grid\nand Overlay can be found at the bottom of Table A.1 in Ap-\npendix A.\nTable 3 presents the F1-scores along with their standard devi-\nations for SwinV2’s best results, compared to the Astronomical\nTransformer for time series And Tabular data (ATAT; Cabrera-\nVives et al. 2024), a specialized time domain model designed for\nmulti-band light curves. Since ATAT provides results that com-\nbine light curves with metadata and additional features, we fo-\ncused on its light curve-based results to ensure a fair comparison\nand demonstrate the effectiveness of our approach on equal foot-\ning. We will refer to this as ATAT (LC). The results show that\nSwinV2 slightly outperformed ATAT (LC) in both configura-\ntions, with the Overlay approach achieving the highest F1-score\nof 65.5%, followed by the Grid approach with 64.6%, while\nATAT (LC) obtained 64.2%. Although the 1.3% improvement of\nSwinV2 (Overlay) over ATAT (LC) is moderate, it underscores\nTable 3. F1-score (%) on the ELAsTiCC test set for ATAT using only\nlight curves and for SwinV2 with the Overlay and Grid approaches.\nModel\nF1-score\nATAT (LC)\n64.2 ± 0.69\nSwinV2 (Overlay)\n65.5 ± 0.28\nSwinV2 (Grid)\n64.6 ± 0.22\nthe model’s ability to effectively distinguish between 20 transient\nand variable star classes in a more complex classification task.\nFigure 8 presents the confusion matrices for the ATAT (LC)\nmodel and the SwinV2 (Overlay) model, which achieved the\nbest performance on the ELAsTiCC dataset. Since the confusion\nmatrix for ATAT (LC) was not directly available, we generated it\nusing the publicly available prediction files from the referenced\nstudy7. In general terms, a similar pattern can be observed in\nFig. 8 (a) and (b), where both models tend to misclassify the\nsame classes and correctly identify light curves in similar pro-\nportions. Both primarily struggle to differentiate between var-\nious supernova types and achieve high recall in the stochastic\ngroup (ATAT: ≥82%, SwinV2: ≥84%, both except for M-dwarf\nFlare) and the periodic group (ATAT: ≥93%, SwinV2: ≥97%).\nSwinV2 (Overlay) slightly outperformed ATAT (LC) in 13 out\nof 20 classes, obtained the same results in 2 classes, and showed\na slight decrease in 5 classes. Specifically, SwinV2 (Overlay)\ndemonstrated higher recall in 8 out of 12 supernova subclasses,\nnamely: CART, Iax, 91bg, Ia, II, SN-like/Other, SLSN, and\nPISN, with improvements of more than 4% in the Iax, 91bg,\nII, and PISN types. Additionally, SwinV2 (Overlay) performed\nequally well or better in 3 out of 4 stochastic subclasses and in\nall 4 periodic subclasses, where its recall improved by more than\n4% for the Delta Scuti and RR Lyrae classes.\nFigure 9 illustrates the distribution of F1-scores obtained\nduring the hyperparameter tuning process for the ELAsTiCC\ndataset, comparing the Overlay and Grid approaches. As be-\nfore, the median F1-score values across all tested hyperparame-\nter combinations remain consistent with those of the best config-\nurations reported in Table 3, where SwinV2 (Overlay) slightly\noutperforms SwinV2 (Grid). The F1-score variations across dif-\nferent hyperparameter settings fall within a narrow range of 61%\nto 67%, indicating that classification performance remains rela-\ntively stable regardless of the chosen hyperparameters.\nIn comparison to MACHO, which consists primarily of peri-\nodic stars, ELAsTiCC presents a more complex taxonomy, mak-\ning the classification task more challenging. In both datasets,\nSwinV2 demonstrated strong performance in distinguishing pe-\nriodic stars, and in ELAsTiCC, it also proved effective at iden-\ntifying stochastic stars. As mentioned earlier, the main dif-\nficulty arises in the classification of supernovae, where both\nSwinV2 and ATAT struggled to differentiate between subclasses.\nIn Cabrera-Vives et al. (2024), ATAT was also evaluated in a\nconfiguration that incorporated both light curves and metadata\n(tabular information provided in the alert stream), referred to\nas ATAT (LC + MD). The inclusion of metadata significantly\nimproved its performance, increasing the F1-score from 64.2%\n(LC) to 82.6% (LC + MD), which emphasizes the importance of\nmetadata or additional features in improving supernova classifi-\ncation. Exploring the integration of tabular data in SwinV2 is left\nfor future work, as it could yield similar improvements to those\nobserved in ATAT.\n7 Files with the results of the ATAT paper.\nArticle number, page 8 of 12\n\n\nD. Moreno-Cartagena et al.: Leveraging Pre-Trained Visual Transformers for Multi-Band Light Curve Classification\nCART\nIax\n91bg\nIa\nIb/c\nII\nSN-like/Other\nSLSN\nPISN\nTDE\nILOT\nKN\nM-dwarf Flare\nuLens\nDwarf Novae\nAGN\nDelta Scuti\nRR Lyrae\nCepheid\nEB\nPredicted Label\nCART\nIax\n91bg\nIa\nIb/c\nII\nSN-like/Other\nSLSN\nPISN\nTDE\nILOT\nKN\nM-dwarf Flare\nuLens\nDwarf Novae\nAGN\nDelta Scuti\nRR Lyrae\nCepheid\nEB\nTrue Label\n0.16\n±0.04\n0.11\n±0.03\n0.03\n0.06\n±0.01\n0.1\n±0.03\n0.27\n±0.04\n0.1\n±0.02\n0.08\n±0.01\n0.02\n0.03\n0.02\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.05\n0.39\n±0.04\n0.02\n0.18\n±0.02\n0.07\n±0.02\n0.11\n±0.02\n0.1\n±0.03\n0.03\n0.01\n0.02\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.03\n0.63\n±0.04\n0.09\n±0.01\n0.1\n±0.03\n0.01\n0.08\n±0.02\n0.01\n0.01\n0.0\n0.0\n0.02\n0.0\n0.01\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.12\n±0.02\n0.06\n±0.01\n0.57\n±0.01\n0.06\n±0.02\n0.04\n0.08\n±0.02\n0.02\n0.01\n0.01\n0.01\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.04\n0.11\n±0.02\n0.1\n±0.01\n0.14\n±0.02\n0.29\n±0.04\n0.07\n±0.02\n0.14\n±0.02\n0.05\n0.02\n0.01\n0.01\n0.01\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.05\n±0.02\n0.08\n±0.02\n0.02\n0.05\n±0.01\n0.04\n0.5\n±0.04\n0.07\n±0.01\n0.06\n±0.01\n0.01\n0.07\n±0.02\n0.02\n0.01\n0.0\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.04\n0.11\n±0.02\n0.11\n±0.01\n0.14\n±0.01\n0.15\n±0.03\n0.12\n±0.02\n0.25\n±0.02\n0.02\n0.01\n0.01\n0.01\n0.02\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.03\n0.01\n0.03\n0.03\n0.04\n0.02\n0.67\n±0.01\n0.08\n±0.01\n0.01\n0.03\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.01\n0.02\n0.01\n0.01\n0.01\n0.07\n±0.01\n0.81\n±0.02\n0.0\n0.05\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.02\n0.0\n0.02\n0.0\n0.06\n±0.01\n0.01\n0.01\n0.0\n0.78\n±0.02\n0.02\n0.0\n0.0\n0.0\n0.0\n0.05\n0.0\n0.0\n0.0\n0.0\n0.03\n0.03\n0.01\n0.02\n0.02\n0.05\n0.02\n0.14\n±0.02\n0.15\n±0.03\n0.07\n±0.01\n0.43\n±0.08\n0.0\n0.0\n0.0\n0.0\n0.04\n0.0\n0.0\n0.0\n0.0\n0.02\n0.02\n0.1\n±0.02\n0.04\n0.04\n0.08\n±0.02\n0.13\n±0.03\n0.0\n0.0\n0.01\n0.0\n0.48\n±0.05\n0.02\n0.01\n0.04\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.02\n0.02\n0.03\n0.06\n±0.01\n0.04\n0.0\n0.0\n0.01\n0.0\n0.18\n±0.03\n0.57\n±0.02\n0.02\n0.03\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.03\n0.02\n0.02\n0.0\n0.01\n0.02\n0.02\n0.0\n0.0\n0.01\n0.0\n0.82\n±0.02\n0.03\n0.01\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.02\n0.01\n0.01\n0.01\n0.01\n0.0\n0.0\n0.01\n0.0\n0.03\n0.0\n0.04\n0.85\n±0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.02\n0.0\n0.01\n0.0\n0.08\n±0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.86\n±0.01\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.93\n±0.01\n0.03\n0.02\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.03\n0.94\n±0.01\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.97\n±0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.97\n±0.01\n(a) ATAT (LC)\nCART\nIax\n91bg\nIa\nIb/c\nII\nSN-like/Other\nSLSN\nPISN\nTDE\nILOT\nKN\nM-dwarf Flare\nuLens\nDwarf Novae\nAGN\nDelta Scuti\nRR Lyrae\nCepheid\nEB\nPredicted Label\n0.17\n±0.02\n0.14\n±0.02\n0.04\n0.08\n±0.01\n0.09\n±0.0\n0.22\n±0.01\n0.08\n±0.02\n0.08\n±0.01\n0.02\n0.04\n0.01\n0.0\n0.0\n0.0\n0.02\n0.01\n0.0\n0.0\n0.0\n0.0\n0.03\n0.44\n±0.03\n0.03\n0.18\n±0.02\n0.07\n±0.01\n0.1\n±0.02\n0.07\n±0.02\n0.02\n0.01\n0.02\n0.01\n0.0\n0.0\n0.0\n0.01\n0.01\n0.0\n0.0\n0.0\n0.0\n0.01\n0.03\n0.67\n±0.04\n0.09\n±0.02\n0.07\n±0.01\n0.01\n0.05\n0.01\n0.01\n0.0\n0.0\n0.01\n0.0\n0.01\n0.02\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.12\n±0.02\n0.06\n±0.01\n0.6\n±0.03\n0.04\n0.03\n0.04\n0.02\n0.01\n0.02\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.12\n±0.01\n0.12\n±0.02\n0.15\n±0.01\n0.28\n±0.01\n0.07\n±0.01\n0.11\n±0.02\n0.05\n0.03\n0.01\n0.0\n0.01\n0.0\n0.01\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.04\n0.08\n±0.01\n0.02\n0.06\n±0.01\n0.03\n0.54\n±0.02\n0.05\n±0.02\n0.04\n0.01\n0.08\n±0.01\n0.01\n0.0\n0.0\n0.0\n0.01\n0.02\n0.0\n0.0\n0.0\n0.0\n0.03\n0.13\n±0.01\n0.12\n±0.02\n0.13\n±0.02\n0.13\n±0.01\n0.1\n±0.02\n0.26\n±0.02\n0.02\n0.01\n0.01\n0.01\n0.01\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.03\n0.01\n0.04\n0.03\n0.03\n0.02\n0.68\n±0.02\n0.09\n±0.01\n0.01\n0.02\n0.0\n0.0\n0.01\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.01\n0.02\n0.01\n0.0\n0.01\n0.05\n0.87\n±0.01\n0.0\n0.02\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.03\n0.01\n0.02\n0.0\n0.06\n±0.01\n0.01\n0.01\n0.0\n0.77\n±0.02\n0.01\n0.0\n0.0\n0.0\n0.01\n0.06\n±0.01\n0.0\n0.0\n0.0\n0.0\n0.01\n0.04\n0.01\n0.03\n0.01\n0.05\n0.01\n0.13\n±0.02\n0.18\n±0.02\n0.06\n±0.01\n0.41\n±0.02\n0.0\n0.0\n0.0\n0.01\n0.04\n0.0\n0.0\n0.0\n0.0\n0.01\n0.03\n0.12\n±0.02\n0.05\n0.03\n0.08\n±0.01\n0.14\n±0.03\n0.0\n0.0\n0.01\n0.0\n0.43\n±0.03\n0.01\n0.02\n0.04\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.03\n0.02\n0.03\n0.08\n±0.01\n0.06\n±0.02\n0.0\n0.0\n0.01\n0.0\n0.15\n±0.02\n0.56\n±0.01\n0.01\n0.03\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.04\n0.02\n0.02\n0.0\n0.01\n0.01\n0.01\n0.0\n0.0\n0.01\n0.0\n0.84\n±0.0\n0.03\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.03\n0.01\n0.01\n0.01\n0.01\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.03\n0.88\n±0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.09\n±0.01\n0.01\n0.0\n0.0\n0.0\n0.0\n0.86\n±0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.98\n±0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.98\n±0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.99\n±0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.01\n0.0\n0.0\n0.0\n0.0\n0.97\n±0.0\n(b) SwinV2 (Overlay)\nFig. 8. Confusion matrices for the ATAT model trained exclusively on light curves (LC) and the best-performing SwinV2 model, both using the\nELAsTiCC dataset. The black lines delineate the three main groups of objects used in the ATAT paper: the Transient, Stochastic, and Periodic\ngroups, from left to right, respectively. Standard deviations are displayed for values greater than or equal to 0.05.\nSwinV2 (Overlay)\nSwinV2 (Grid)\nMulti-band approach\n62%\n63%\n64%\n65%\n66%\nTest F1-score (%)\nF1-score distribution in ELAsTiCC hyperparameter tuning\nFig. 9. Distribution of F1-scores obtained during hyperparameter tuning\nfor the ELAsTiCC dataset, comparing the Overlay and Grid multi-band\napproaches.\n5. Conclusions\nWe have introduced a simple and effective method for classify-\ning photometric light curves using a pre-trained VT model with-\nout the need for complex feature engineering or multi-band pre-\nprocessing to handle multi-band observations. We demonstrated\nthe capability and effectiveness of this method using one, two,\nand six bands of light curves, as well as in scenarios involving\nimbalanced datasets, limited data, and the classification of both\ntransient and variable stars.\nOur results show that leveraging the knowledge acquired\nfrom pre-training SwinV2 on a large dataset and fine-tuning it\nfor a specific task can lead to better performance than models\nspecifically designed for light curve data. When using a single\nband in the MACHO dataset, we demonstrated that, compared\nto Astromer, our method improves the F1-score by 4.1% in a\ndataset with 20 spc used to train the model and by 4.4% in a\ndataset with 500 spc. Additionally, we showed that it is possi-\nble to maintain a high performance (79.8%) of F1-score on the\nFull unbalanced dataset. When incorporating a second band in\nthe same dataset, we observed further improvements compared\nto SwinV2 with one band: an additional 6.8% in the F1-score\nfor 20 spc, 4.3% for 500 spc, and 4.5% in the Full dataset. Fi-\nnally, when evaluating our method on the ELAsTiCC dataset,\nwe demonstrated that it achieves an F1-score of 65.5%, which\nis 1.2% higher than the score obtained in ATAT using only light\ncurves.\nThis study highlights the discriminative power of pre-trained\nVT models, demonstrating their ability to adapt to datasets that\ndiffer entirely from those on which they were originally trained.\nBy analyzing images of light curves, the model successfully dis-\ntinguished between astronomical objects across datasets with\ndistinct observational characteristics, such as the MACHO sur-\nvey and the ELAsTiCC challenge. A key factor in this success\nis the model’s ability to capture the temporal behavior of light\ncurves, which is primarily achieved through the positional em-\nbedding. This embedding provides a structured representation of\nthe light curve’s spatial arrangement within the image, allowing\nthe model to implicitly encode ordering information. As a re-\nsult, it delivers competitive performance against models specif-\nically designed to process sequential light curves and account\nfor the distances between observations. This capability not only\nunderscores the model’s versatility but also suggests a promis-\ning direction for developing a more generalizable approach. In\nparticular, it could serve as a foundation for a model capable of\nadapting to any type of survey with minimal fine-tuning using\nonly a few light curves. However, further investigation is neces-\nsary to fully validate this potential.\nArticle number, page 9 of 12\n\n\nA&A proofs: manuscript no. aanda\nA direct extension of our approach would be to incorpo-\nrate metadata alongside light curves to provide additional con-\ntext and enhance classification performance, which should align\nwith the improvements observed in ATAT when using both light\ncurves and metadata (LC + MD). Additionally, pre-training the\nmodel on light curves could be explored to assess its ability to\nextract general information from astronomical time series us-\ning a self-supervised task. Future work could explore integrat-\ning descriptive information, such as textual captions for each\nlight curve. If such data were available, we could extend our\nmethod to vision-language transformers (VLTs), enabling tasks\nsuch as image retrieval (IR) with CLIP (Radford et al. 2021),\nvisual question answering (VQA) with BLIP-2 (Li et al. 2023),\nand even light curve image deblurring using Stable Diffusion\n(Rombach et al. 2022). Given the potential computational cost\nof these models, knowledge distillation could be employed to\nimprove inference efficiency while maintaining accuracy. These\nadvancements would further refine the classification of photo-\nmetric light curves and open new avenues for multi-modal astro-\nnomical analysis in the time domain.\nCode and data availability\nThe code used in this study, along with instructions on how to\naccess the data, is available at GitHub repository. Detailed infor-\nmation on how to obtain and use the dataset is provided in the\nREADME file.\nAcknowledgements. The computations in this paper were run on the FASRC\nCannon cluster supported by the FAS Division of Science Research Comput-\ning Group at Harvard University. The authors acknowledge support from the\nNational Agency for Research and Development (ANID) grants: Millennium\nScience Initiative ICN12_009 and AIM23-0001 (GCV), NCN2021_080 (GCV,\nCDO), and FONDECYT Regular 1231877 (DMC, GCV, MCL, CDO).\nReferences\nAlcock, C., Allsman, R., Alves, D., et al. 2003, VizieR Online Data Catalog:\nVariable Stars in the Large Magellanic Clouds (MACHO, 2001), VizieR On-\nline Data Catalog: II/247. Originally published in: MACHO Project (2001)\nAlcock, C., Allsman, R. A., Alves, D. R., et al. 2000, ApJ, 542, 281\nAllam Jr, T. & McEwen, J. D. 2024, RAS Techniques and Instruments, 3, 209\nBecker, I., Pichara, K., Catelan, M., et al. 2020, Monthly Notices of the Royal\nAstronomical Society, 493, 2981\nBecker, I., Protopapas, P., Catelan, M., & Pichara, K. 2025, A&A, 694, A183\nBoone, K. 2019, The Astronomical Journal, 158, 257\nBrown, T., Mann, B., Ryder, N., et al. 2020, Advances in neural information\nprocessing systems, 33, 1877\nCabrera-Vives, G., Moreno-Cartagena, D., Astorga, N., et al. 2024, Astronomy\n& Astrophysics, 689, A289\nCádiz-Leyton, M., Cabrera-Vives, G., Protopapas, P., Moreno-Cartagena, D., &\nDonoso-Oliva, C. 2024a, in LatinX in AI workshop, 41 st International Con-\nference on Machine Learning (ICML), PMLR 235, Vienna, Austria\nCádiz-Leyton, M., Cabrera-Vives, G., Protopapas, P., et al. 2024b, Astronomy &\nAstrophysics [arXiv:2412.10528], submitted\nCarion, N., Massa, F., Synnaeve, G., et al. 2020, in European conference on\ncomputer vision, Springer, 213–229\nCarrasco-Davis, R., Cabrera-Vives, G., Förster, F., et al. 2019, Publications of\nthe Astronomical Society of the Pacific, 131, 108006\nCharnock, T. & Moss, A. 2017, The Astrophysical Journal Letters, 837, L28\nChen, H., Wang, Y., Guo, T., et al. 2021a, in Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 12299–12310\nChen, X., Yan, B., Zhu, J., et al. 2021b, in Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 8126–8135\nCheng, B., Schwing, A., & Kirillov, A. 2021, Advances in neural information\nprocessing systems, 34, 17864\nCho, K., van Merriënboer, B., Gulcehre, C., et al. 2014, in Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), ed. A. Moschitti, B. Pang, & W. Daelemans (Doha, Qatar: As-\nsociation for Computational Linguistics), 1724–1734\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. 2019, in Proceedings of\nthe 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), ed. J. Burstein, C. Doran, & T. Solorio (Minneapolis, Min-\nnesota: Association for Computational Linguistics), 4171–4186\nDonalek, C., Djorgovski, S. G., Mahabal, A. A., et al. 2013, in 2013 IEEE Inter-\nnational Conference on Big Data, IEEE, 35–41\nDonoso-Oliva, C., Becker, I., Protopapas, P., et al. 2023, Astronomy & Astro-\nphysics, 670, A54\nDonoso-Oliva, C., Cabrera-Vives, G., Protopapas, P., Carrasco-Davis, R., & Es-\ntévez, P. A. 2021, Monthly Notices of the Royal Astronomical Society, 505,\n6069\nDosovitskiy, A., Beyer, L., Kolesnikov, A., et al. 2021, in International Confer-\nence on Learning Representations\nDrake, A. J., Djorgovski, S. G., Mahabal, A., et al. 2012, in Proceedings of the\nInternational Astronomical Union, Symposium S285: New Horizons in Time-\nDomain Astronomy, Vol. 7 (Cambridge University Press), 306–308\nFraga, B. M. O., Bom, C. R., Santos, A., et al. 2024, Astronomy & Astrophysics,\n689, A208\nGómez, C., Neira, M., Hernández Hoyos, M., Arbeláez, P., & Forero-Romero,\nJ. E. 2020, Monthly Notices of the Royal Astronomical Society, 499, 3130\nGraham, M. J., Djorgovski, S., Drake, A. J., et al. 2014, Monthly Notices of the\nRoyal Astronomical Society, 439, 703\nHambleton, K. M. et al. 2023, Publ. Astron. Soc. Pac., 135, 105002\nHan, K., Wang, Y., Chen, H., et al. 2023, IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 45, 87\nHochreiter, S. 1997, Neural Computation MIT-Press\nIvezi´c, Ž., Kahn, S. M., Tyson, J. A., et al. 2019, The Astrophysical Journal, 873,\n111\nJamal, S. & Bloom, J. S. 2020, The Astrophysical Journal Supplement Series,\n250, 30\nJiang, Y., Chang, S., & Wang, Z. 2021, Advances in Neural Information Process-\ning Systems, 34, 14745\nKarpenka, N. V., Feroz, F., & Hobson, M. 2013, Monthly Notices of the Royal\nAstronomical Society, 429, 1278\nKessler, R., Narayan, G., Avelino, A., et al. 2019, Publications of the Astronom-\nical Society of the Pacific, 131, 094501\nKhan, S., Naseer, M., Hayat, M., et al. 2022, ACM Comput. Surv., 54\nKim, D.-W., Protopapas, P., Bailer-Jones, C. A., et al. 2014, Astronomy & As-\ntrophysics, 566, A43\nKim, D.-W., Protopapas, P., Byun, Y.-I., et al. 2011, The Astrophysical Journal,\n735, 68\nLeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. 1998, Proceedings of the IEEE,\n86, 2278\nLi, J., Li, D., Savarese, S., & Hoi, S. 2023, in International conference on ma-\nchine learning, PMLR, 19730–19742\nLi, Z., Li, S., & Yan, X. 2024, in Advances in Neural Information Processing\nSystems, Vol. 36\nLiu, Y., Zhang, Y., Wang, Y., et al. 2023, IEEE Transactions on Neural Networks\nand Learning Systems\nLiu, Z., Hu, H., Lin, Y., et al. 2022, in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 12009–12019\nLiu, Z., Lin, Y., Cao, Y., et al. 2021, in Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, 10012–10022\nLochner, M., McEwen, J. D., Peiris, H. V., Lahav, O., & Winter, M. K. 2016, The\nAstrophysical Journal Supplement Series, 225, 31\nMahabal, A., Sheth, K., Gieseke, F., et al. 2017, in 2017 IEEE symposium series\non computational intelligence (SSCI), IEEE, 1–8\nMöller, A. & de Boissière, T. 2020, Monthly Notices of the Royal Astronomical\nSociety, 491, 4277\nMonsalves, N., Jaque Arancibia, M., Bayo, A., et al. 2024, Astronomy & Astro-\nphysics, 691, A106\nMoreno-Cartagena, D., Cabrera-Vives, G., Protopapas, P., et al. 2023, in Ma-\nchine Learning for Astrophysics Workshop, 40th International Conference on\nMachine Learning (ICML), PMLR 202, Honolulu, Hawaii, USA\nMorvan, M., Nikolaou, N., Yip, K. H., & Waldmann, I. 2022, in Machine Learn-\ning for Astrophysics Workshop, 39th International Conference on Machine\nLearning (ICML), PMLR 162, Baltimore, Maryland, USA\nMuthukrishna, D., Narayan, G., Mandel, K. S., Biswas, R., & Hložek, R. 2019,\nPublications of the Astronomical Society of the Pacific, 131, 118002\nNarayan,\nG.\n&\nTeam,\nT.\nE.\n2023,\nBulletin\nof\nthe\nAAS,\n55,\nhttps://baas.aas.org/pub/2023n2i117p01\nNaul, B., Bloom, J. S., Pérez, F., & Van Der Walt, S. 2018, Nature Astronomy,\n2, 151\nNun, I., Protopapas, P., Sim, B., et al. 2017, FATS: Feature Analysis for Time\nSeries, Astrophysics Source Code Library, record ascl:1711.017\nPan, J., Ting, Y.-S., & Yu, J. 2022, in Machine Learning for Astrophysics Work-\nshop, 39th International Conference on Machine Learning (ICML), PMLR\n162, Baltimore, Maryland, USA\nArticle number, page 10 of 12\n\n\nD. Moreno-Cartagena et al.: Leveraging Pre-Trained Visual Transformers for Multi-Band Light Curve Classification\nPasquet, J., Pasquet, J., Chaumont, M., & Fouchez, D. 2019, Astronomy & As-\ntrophysics, 627, A21\nPasquet-Itam, J. & Pasquet, J. 2018, Astronomy & Astrophysics, 611, A97\nPichara, K., Protopapas, P., Kim, D.-W., Marquette, J.-B., & Tisserand, P. 2012,\nMonthly Notices of the Royal Astronomical Society, 427, 1284\nPimentel, Ó., Estévez, P. A., & Förster, F. 2022, The Astronomical Journal, 165,\n18\nQu, H. & Sako, M. 2022, The Astronomical Journal, 163, 57\nQu, H., Sako, M., Möller, A., & Doux, C. 2021, The Astronomical Journal, 162,\n67\nRadford, A. 2018\nRadford, A., Kim, J. W., Hallacy, C., et al. 2021, in International conference on\nmachine learning, PMLR, 8748–8763\nRichards, J. W., Starr, D. L., Butler, N. R., et al. 2011, The Astrophysical Journal,\n733, 10\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. 2022, in Pro-\nceedings of the IEEE/CVF conference on computer vision and pattern recog-\nnition, 10684–10695\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. 1986, nature, 323, 533\nSánchez-Sáez, P., Reyes, I., Valenzuela, C., et al. 2021, The Astronomical Jour-\nnal, 161, 141\nSzklenár, T., Bódi, A., Tarczay-Nehéz, D., et al. 2020, The Astrophysical Journal\nLetters, 897, L12\nSzklenár, T., Bódi, A., Tarczay-Nehéz, D., et al. 2022, The Astrophysical Journal,\n938, 37\nUdalski, A., Szyma´nski, M. K., Soszy´nski, I., & Poleski, R. 2008, Acta Astro-\nnomica, 58, 69\nUdalski, A., Szyma´nski, M. K., & Szyma´nski, G. 2015, Acta Astronomica, 65, 1\nVaswani, A., Shazeer, N., Parmar, N., et al. 2017, Advances in neural information\nprocessing systems, 30\nVillar, V., Berger, E., Miller, G., et al. 2019, The Astrophysical Journal, 884, 83\nVillar, V. A., Hosseinzadeh, G., Berger, E., et al. 2020, The Astrophysical Jour-\nnal, 905, 94\nWang, H., Zhu, Y., Adam, H., Yuille, A., & Chen, L.-C. 2021, in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, 5463–\n5474\nArticle number, page 11 of 12\n\n\nA&A proofs: manuscript no. aanda\nTable A.1. Summary of the best hyperparameters found for the SwinV2 model across single- and multi-band cases and varying dataset sizes.\nMACHO (single-band)\nMarker size\nLine width\nFlux errors\nInput format\nLearning rate\n20 spc\n5.0\n1.5\nTrue\nOverlay\n5 · 10−5\n500 spc\n1.0\n1.0\nTrue\nOverlay\n5 · 10−6\nFull dataset\n1.0\n2.0\nTrue\nOverlay\n5 · 10−6\nMACHO (two-band)\nMarker size\nLine width\nFlux errors\nInput format\nLearning rate\n20 spc\n1.0\n2.0\nTrue\nGrid\n5 · 10−5\n20 spc\n2.0\n0.5\nTrue\nOverlay\n5 · 10−5\n500 spc\n2.0\n1.0\nFalse\nGrid\n5 · 10−5\n500 spc\n3.0\n1.0\nTrue\nOverlay\n5 · 10−5\nFull dataset\n3.0\n1.5\nTrue\nGrid\n5 · 10−6\nFull dataset\n1.0\n2.0\nTrue\nOverlay\n5 · 10−6\nELAsTiCC (six-band)\nMarker size\nLine width\nFlux errors\nInput format\nLearning rate\nFull dataset\n5.0\n2.0\nTrue\nGrid\n5 · 10−6\nFull dataset\n2.0\n0.5\nTrue\nOverlay\n5 · 10−6\nAppendix A: Hyperparameters\nTable A.1 presents the best hyperparameters obtained for marker\nsize, line width, flux errors, input format, and learning rate for\nthe MACHO dataset (single-band and two-band) in the scenar-\nios of 20 spc, 500 spc, and the Full dataset, as well as for the\nELASTiCC dataset (six-band). Hyperparameter tuning was con-\nducted using a grid search over the values listed in Table 1.\nThe results reveal a few key patterns: the optimal marker\nsize and line width depend on both the number of bands and\nthe approach used to represent multi-band observations. When\nusing the Grid approach, a larger marker size is necessary be-\ncause each light curve band is represented on a lower-resolution\ngrid, requiring larger observation markers for better visibility.\nThis effect is particularly evident in ELAsTiCC, where the grid\nis divided into six sections. The same pattern is observed in the\nFull dataset of MACHO (two-band) but not in the 20 spc and\n500 spc scenarios, where the limited number of observations in-\ncreases the likelihood of weak data representation. Conversely,\nthe Overlay approach generally exhibits less variation in the\nmarker size and line width hyperparameters across the different\nFull datasets, including MACHO single-band, MACHO two-\nband, and ELAsTiCC, remaining within a marker size range of\n1.0 to 2.0 and a line width range of 0.5 to 2.0. This consistency\noccurs because the resolution at which bands are represented re-\nmains fixed at 256 × 256. In the same context, flux errors ap-\npear to be important in almost all cases, except in the 500 spc\nscenario for MACHO (two-band). Additionally, higher learning\nrates performed better for smaller datasets, while lower learning\nrates were more effective for larger datasets. This trend likely\noccurs because higher learning rates help escape local optima\nwhen data is scarce, whereas lower learning rates enable more\nstable updates when training on larger datasets.\nArticle number, page 12 of 12\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20479v1.pdf",
    "total_pages": 12,
    "title": "Leveraging Pre-Trained Visual Transformers for Multi-Band Photometric Light Curve Classification",
    "authors": [
      "Daniel Moreno-Cartagena",
      "Pavlos Protopapas",
      "Guillermo Cabrera-Vives",
      "Martina Cádiz-Leyton",
      "Ignacio Becker",
      "Cristóbal Donoso-Oliva"
    ],
    "abstract": "This study investigates the potential of a pre-trained visual transformer\n(VT) model, specifically the Swin Transformer V2 (SwinV2), to classify\nphotometric light curves without the need for feature extraction or multi-band\npreprocessing. The goal is to assess whether this image-based approach can\naccurately differentiate astronomical phenomena and serve as a viable option\nfor working with multi-band photometric light curves. We transformed each\nmulti-band light curve into an image. These images serve as input to the SwinV2\nmodel, which is pre-trained on ImageNet-21K. The datasets employed include the\npublic Catalog of Variable Stars from the Massive Compact Halo Object (MACHO)\nsurvey, using both one and two bands, and the first round of the recent\nExtended LSST Astronomical Time-Series Classification Challenge (ELAsTiCC),\nwhich includes six bands. The performance of the model was evaluated on six\nclasses for the MACHO dataset and 20 distinct classes of variable stars and\ntransient events for the ELAsTiCC dataset. The fine-tuned SwinV2 achieved\nbetter performance than models specifically designed for light curves, such as\nAstromer and the Astronomical Transformer for Time Series and Tabular Data\n(ATAT). When trained on the full MACHO dataset, it attained a macro F1-score of\n80.2 and outperformed Astromer in single-band experiments. Incorporating a\nsecond band further improved performance, increasing the F1-score to 84.1. In\nthe ELAsTiCC dataset, SwinV2 achieved a macro F1-score of 65.5, slightly\nsurpassing ATAT by 1.3.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}