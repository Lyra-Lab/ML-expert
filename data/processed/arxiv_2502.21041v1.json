{
  "id": "arxiv_2502.21041v1",
  "text": "Fast Adversarial Training against Sparse Attacks\nRequires Loss Smoothing\nXuyang Zhong, Yixiao Huang, Chen Liu∗\nCity University of Hong Kong, Hong Kong, China\n{xuyang.zhong, yixiao.huang}@my.cityu.edu.hk, chen.liu@cityu.edu.hk\nAbstract\nThis paper studies fast adversarial training against sparse adversarial perturbations bounded\nby l0 norm. We demonstrate the challenges of employing 1-step attacks on l0 bounded per-\nturbations for fast adversarial training, including degraded performance and the occurrence\nof catastrophic overfitting (CO). We highlight that CO in l0 adversarial training is caused by\nsub-optimal perturbation locations of 1-step attack. Theoretical and empirical analyses reveal\nthat the loss landscape of l0 adversarial training is more craggy compared to its l∞, l2 and l1\ncounterparts. Moreover, we corroborate that the craggy loss landscape can aggravate CO. To\naddress these issues, we propose Fast-LS-l0 that incorporates soft labels and the trade-off loss\nfunction to smooth the adversarial loss landscape. Extensive experiments demonstrate our\nmethod can overcome the challenge of catastrophic overfitting, achieve state-of-the-art per-\nformance, and narrow down the performance gap between 1-step and multi-step adversarial\ntraining against sparse attacks.\n1\nIntroduction\nDeep neural networks have been shown vulnerable to adversarial perturbations [1]. To achieve\nrobust models, comprehensive evaluations [2–4] have demonstrated that adversarial training [5]\nand its variants [6–12] are the most effective methods. However, adversarial training is generally\ncomputationally expensive because generating adversarial perturbations in each training step needs\nmultiple forward and backward passes of the model. Such efficiency issues hinder the scalability\nof adversarial training to large models and large datasets.\nImproving the efficiency of adversarial training is tricky. Some works [13–16] employ faster but\nweaker 1-step attacks to generate adversarial perturbations for training. However, such methods\nmay suffer from catastrophic overfitting (CO) [17]: the model overfits these weak attacks instead\nof achieving true robustness against adaptive and stronger attacks.\nOn the other hand, most existing works [5, 18, 19] focus on studying adversarial perturbations\nbounded by l∞, l2 or l1 norms. In these scenarios, the set of allowable perturbations is convex,\nwhich facilitates optimizing adversarial perturbations and thus adversarial training. However, there\nare many scenarios in real-world applications where sparse perturbations, bounded by the l0 norm,\nneed to be considered [20–23]. Since the l0 norm is not a proper norm, the set of all allowable\nperturbations in this case is not convex. Consequently, from an optimization perspective, obtaining\nrobust models against sparse perturbations becomes more challenging. Compared with the l∞, l2\nand l1 counterparts, more steps are needed to generate strong l0 bounded perturbations, making\nthe corresponding adversarial training even more computationally expensive.\nAmong algorithms aiming at obtaining robust models against sparse perturbations, sAT and\nsTRADES [23] stand out as the most effective ones. These methods employ adversarial training\nagainst Sparse-PGD (sPGD) [23].\nHowever, they still require 20 steps to generate adversarial\nperturbations in each training step to achieve decent performance. As demonstrated in Table 1,\nnaively decreasing the number of steps to 1 leads to a significant performance decline for both sAT\nand sTRADES.\nIn this work, we investigate the challenges associated with fast adversarial training against\nsparse perturbations, including training instability caused by catastrophic overfitting (CO) and\n∗Correspondence author.\n1\narXiv:2502.21041v1  [cs.LG]  28 Feb 2025\n\n\nTable 1: Robust accuracy of sAT and sTRADES [23] with different steps (t). The evaluation is based on\nSparse-AutoAttack (sAA) [23], where the sparsity level is ϵ = 20. The models are PreactResNet-18 [24]\ntrained on CIFAR-10 [25].\nsAT (t = 1)\nsAT (t = 20)\nsTRADES (t = 1)\nsTRADES (t = 20)\nRobust Accuracy\n0.0\n36.2\n31.0\n61.7\nperformance decline in both robust and clean accuracy. Specifically, we highlight that CO in l0\nadversarial training is caused by sub-optimal perturbation locations of 1-step attack. Our obser-\nvation indicates that adjusting the perturbation magnitudes alone cannot help mitigate CO in\nthis context, so some existing CO mitigation methods [26–29] used in other cases do not work\nin the l0 scenario.\nAlthough the multi-ϵ strategy can mitigate sub-optimal perturbation loca-\ntions, it suffers from unstable training and degraded clean accuracy. In light of these findings,\nwe present empirical and theoretical evidence to illustrate that the loss landscape of adversarial\ntraining against l0 bounded perturbations is markedly more craggy compared to its l∞, l2, and\nl1 counterparts. Furthermore, we corroborate that the craggy loss landscape aggravates CO in l0\nadversarial training.\nDrawing from these insights, we propose to utilize soft labels and a trade-off loss function to\nenhance the smoothness of the adversarial loss objective function, thereby improving the perfor-\nmance of fast adversarial training against sparse perturbations. In addition to the performance,\nwe showcase that these techniques can eliminate CO, thus improving training stability. Finally,\nour extensive experiments demonstrate that smoothing the loss landscape can effectively narrow\nthe performance gap between 1-step adversarial training and its multi-step counterparts.\nTo the best of our knowledge, this work is the first to investigate fast adversarial training in\nthe context of l0 bounded perturbations. We summarize the contributions of this paper as follows:\n1. We highlight that catastrophic overfitting (CO) in fast l0 adversarial training is caused by\nsub-optimal perturbation locations of 1-step attack. Popular techniques in fast l∞, l2 and l1\nadversarial training are ineffective in the l0 case. Although the multi-ϵ strategy can mitigate sub-\noptimal perturbation locations, it suffers from unstable training and degraded clean accuracy.\n2. We theoretically and empirically demonstrate that the adversarial loss landscape is more craggy\nin the l0 cases than in other cases, which further aggravates CO in l0 adversarial training. In\nthis regard, we propose Fast-LS-l0 which incorporates labels and the trade-off loss function to\nprovably smooth the adversarial loss landscape.\n3. Comprehensive experiments demonstrate that smoothing the adversarial loss landscape greatly\nnarrows the performance gap between 1-step l0 adversarial training and its multi-step coun-\nterpart.\nOur method establishes a new state-of-the-art performance for efficient adversarial\ntraining against sparse perturbations.\nNotation and Terminology Consider a classification model F(x, θ) = {fi(x, θ)}K−1\ni=0 , where\nx ∈Rd is the input, θ represents the parameters of the model, and K is the number of classes,\nfi(x, θ) is the logit of the i-th class. Correspondingly, we use {hi}K−1\ni=0\nto represent the output\nprobability of each class, which is the result of softmax function applied to {fi}K−1\ni=0 . Therefore,\nthe loss objective function L based on the cross-entropy is calculated as follows:\nL(x, θ)\ndef\n= −\nK−1\nX\ni=0\nyi log hi(x, θ)\ndef\n= −\nK−1\nX\ni=0\nyi log\nexp(fi(x, θ))\nPK−1\nj=0 exp(fj(x, θ))\n(1)\nwhere y = [y1, y2, ..., yC] is the label of x in a simplex, i.e., P\ni yi = 1. In the context of adversarial\nperturbation, we use S(p)\nϵ\n(x)\ndef\n= {δ|∥δ∥p ≤ϵ, 0 ≤x + δ ≤1} to represent the adversarial budget,\ni.e., the set of all allowable input perturbations for the input x. The adversarial loss function is\nL(p)\nϵ (x, θ)\ndef\n= maxδ∈S(p)\nϵ\n(x) L(x + δ, θ). Despite no guarantee to obtain the optimal perturbation in\npractice, to simplify the notation, we denote the term L(p)\nϵ\nalso as the adversarial loss induced by\nthe actual attack algorithms and omit the superscript (p) when there is no ambiguity.\n2\n\n\n2\nRelated Works\nAdversarial Attacks: The existence of adversarial examples is first identified in Szegedy et al.\n[1], which focuses on l2 norm-bounded adversarial perturbations.\nFast gradient sign method\n(FGSM) [30] introduces an efficient approach by generating perturbations bounded by its l∞norm\nin a single step. Furthermore, projected gradient descent (PGD) [5] extends and improves FGSM\n[31] by iterative updating and random initialization. In addition to these white-box attacks where\nthe attackers have full access to the models, there are also several black-box attacks [32, 33] where\nthe attackers’ access is restricted. AutoAttack (AA) [3] is an ensemble of both white-box and\nblack-box attacks to ensure a more reliable evaluation of model’s robustness.\nAdversarial Training: Adversarial training [5–12] has emerged as a popular and reliable\nframework to obtain robust models [2, 3]. Under this framework, we first generate adversarial\nexamples and update model parameters based on these examples in each mini-batch update. Dif-\nferent adversarial training variants, such as TRADES [34] and MART [35], may have different\nloss objective functions for generating adversarial examples and updating model parameters. Fur-\nthermore, compared with training on clean inputs, adversarial training is shown to suffer more\nfrom overfitting [36, 37]. In this regard, self-adaptive training (SAT) [38], which utilizes historical\npredictions as the soft label, has demonstrated its efficacy in improving the generalization.\nSparse Perturbations: Adversarial budget defined by l1 norm is the tightest convex hull of\nthe one defined by l0 norm. In this context, SLIDE [18] extends PGD and employs k-coordinate\nascent to generate l1 bounded perturbations. Similarly, AutoAttack-l1 (AA-l1) [39] extends AA\nto the l1 case. However, AA-l1 is found to generate non-sparse perturbations that SLIDE fails to\ndiscover [19], indicating that l1 bounded perturbations are not necessarily sparse. Therefore, we\nuse l0 norm to strictly enforce sparsity. It is challenging to optimize over an adversarial budget\ndefined by l0 norm, because of non-convex adversarial budgets. While naively applying PGD in\nthis case turns out sub-optimal, there are several black-box attacks, including CornerSearch [21]\nand Sparse-RS [22], and white-box attacks, including Sparse Adversarial and Interpretable Attack\nFramework (SAIF) [40] and Sparse-PGD (sPGD) [23], which address the optimization challenge of\nfinding l0 bounded perturbations. Ultimately, Sparse-AutoAttack (sAA) [23], combining the most\npotent white-box and black-box attacks, emerges as the most powerful sparse attack.\nFast Adversarial Training: While effective, adversarial training is time-consuming due to\nthe use of multi-step attacks. To reduce the computational overhead, some studies [13, 14] em-\nploy faster one-step attacks in adversarial training. However, the training based on these weaker\nattacks may suffer from catastrophic overfitting (CO) [17], where the model overfits to these weak\nattacks instead of achieving true robustness against a variety of attacks. CO is shown to arise\nfrom distorted decision boundary caused by sub-optimal perturbation magnitudes [26]. There are\nseveral methods proposed to mitigate CO, including aligning the gradients of clean and adversarial\nsamples [27], adding stronger noise to clean sample [41] , adaptive step size [29], regularizing ab-\nnormal adversarial samples [42], adding layer-wise weight perturbations [43], and penalizing logits\ndiscrepancy [44]. Furthermore, compared to its l2 and l∞counterparts, CO is caused by overfitting\nto sparse perturbations during l1 adversarial training [19]. To address this issue, Fast-EG-l1 [19]\nis introduced to generate l1 bounded perturbations by Euclidean geometry instead of coordinate\nascent. In this work, we investigate fast adversarial training against l0 bounded perturbations.\n3\nChallenges in Fast l0 Adversarial Training\nTo obtain robust models against sparse perturbations, preliminary efforts use 20-step sPGD in\nadversarial training, which introduces significant computational overhead. To accelerate training,\nwe explore using 1-step sPGD in adversarial training. However, as reported in Table 1, the models\nobtained in this way exhibit weak robustness against stronger and comprehensive sparse attacks\nsuch as sAA. In this section, we study the underlying factors that make fast l0 adversarial training\nchallenging by both numerical experiments and theoretical analyses.\n3.1\nCatastrophic Overfitting in Fast l0 Adversarial Training\nWe plot the learning curves of adversarial training using 1-step sPGD in Figure 1. Specifically, we\nadopt the multi-ϵ strategy [19, 23] and allow for different adversarial budget sizes, i.e., ϵ, during\ntraining and testing.\nThe results in Figure 1 indicate that CO happens in all configurations.\n3\n\n\nMoreover, our observations of CO in l0 cases are different from other cases in several aspects. First,\nrandom initialization of adversarial perturbation, proven effective in l∞, l2 and l1 cases, does not\nyield similar results in the l0 case. In addition, Figure 1 showcases that the training accuracy on\nthe inputs perturbed by 1-step sPGD is even higher than their clean counterparts. What’s more,\nwhen CO happens in l∞, l2 and l1 cases, the model sharply achieves perfect robustness against 1-\nstep attacks but zero robustness against multi-step attacks, both in few mini-batch updates. Such\nphenomenon is not observed in l0 cases. By contrast, we observe dramatic performance fluctuations\non clean examples throughout the training process, even in the fine-tuning phase. Such training\ninstability indicates a non-smooth landscape of the loss function in the parameter space: a subtle\nchange in parameters θ leads to abrupt fluctuation in the loss.\n0\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nAccuracy\nClean Acc.\nRobust Acc.\n(a) ϵtrain = 20\n0\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nAccuracy\nClean Acc.\nRobust Acc.\n(b) ϵtrain = 40\n0\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nAccuracy\nClean Acc.\nRobust Acc.\n(c) ϵtrain = 120\nFigure 1: The learning curves of adversarial training against 1-step sPGD [23] with random noise initial-\nization. The models are PreactResNet-18 [24] trained on CIFAR-10 [25]. The dashed and the solid lines\nrepresent the accuracy of the training and the test set, respectively. The test robust accuracy is based on\nsAA with ϵ = 20. The values of ϵ used in training are shown as ϵtrain in captions, the training robust\naccuracy is based on the 1-step sPGD with ϵtrain.\nTable 2: Robust accuracy of the models obtained by 1-step sAT with different ϵtrain against the interpo-\nlation between perturbations generated by 1-step sPGD (ϵ = 20) and their corresponding clean examples,\nwhere α denotes the interpolation factor, i.e., xinterp = x + α · δ. The results of sAA are also reported.\nα\n0.0\n0.1\n0.2\n0.3\n0.4\n0.6\n0.8\n1.0\nsAA\nϵtrain = 20\n77.5\n69.8\n69.1\n73.7\n80.4\n88.0\n90.2\n90.4\n0.0\nϵtrain = 40\n70.2\n63.1\n64.3\n70.9\n79.8\n87.4\n89.6\n89.6\n0.0\nϵtrain = 120\n32.5\n26.5\n24.5\n29.4\n41.5\n65.2\n72.8\n67.6\n0.0\nIn l∞and l2 cases, CO occurs due to distorted decision boundary caused by sub-optimal\nperturbation magnitudes [26]. To ascertain if this applies to l0 adversarial training, we evaluate\nthe robustness accuracy of models trained by 1-step sAT with varying ϵtrain against interpolations\nbetween the clean inputs and the perturbed ones by 1-step sPGD. Table 2 shows that we cannot find\nsuccessful adversarial examples through such simple interpolations. In addition, the substantial l0\ndistance between 1-step sPGD and sAA perturbations (see in Appendix E.1) suggests that CO in l0\nadversarial training is primarily due to sub-optimal perturbation locations rather than magnitudes.\nConsequently, existing CO mitigation methods like GradAlign [27], ATTA [28], and adaptive step\nsize [29] turn out ineffective or insufficient for l0 scenarios. We defer the detailed evaluation to\nAppendix E.4.\nDespite that, we find that multi-ϵ strategy [23] mitigate the sub-optimality of perturbation\nlocation resulting from 1-step attacks to some extent. The detailed discussion is deferred to Ap-\npendix E.2. However, as illustrated in Figure 1, a larger ϵtrain, in turn, leads to unstable training\nand degraded clean accuracy. To address this challenge, we investigate the loss landscape in the\nsubsequent sections.\n3.2\nTheoretical Analyses on the Smoothness of Adversarial Loss Func-\ntions\nWe first provide theoretical analyses on the smoothness of adversarial loss function. Similar to\n[45], we assume the first-order smoothness of the model’s outputs {fi}K−1\ni=0 .\n4\n\n\nAssumption 3.1. (First-order Lipschitz condition) ∀i ∈{0, 1, ..., K −1}, the function fi\nsatisfies the following first-order Lipschitz conditions, with constants Lθ, Lx:\n∀x, θ1, θ2,\n∥fi(x, θ1) −fi(x, θ2)∥≤Lθ∥θ1 −θ2∥,\n(2)\n∀θ, x1, x2,\n∥fi(x1, θ) −fi(x2, θ)∥≤Lx∥x1 −x2∥.\n(3)\nWe then study the first-order smoothness of the adversarial loss objective function Lϵ(x, θ).\nLemma 3.2. (Lipschitz continuity of adversarial loss) If Assumption 3.1 holds, we have:\n∀x, θ1, θ2,\n∥Lϵ(x, θ1) −Lϵ(x, θ2)∥≤Aθ∥θ1 −θ2∥,\n(4)\nThe Lipschitz constant Aθ = 2 P\ni∈S+ yiLθ where S+ = {i | yi > 0, hi(x+δ1, θ2) > hi(x+δ1, θ1)},\nδ1 ∈arg maxδ∈SϵL(x + δ, θ) and δ2 ∈arg maxδ∈SϵL(x + δ, θ).\nThe proof is deferred to Appendix B.1, in which we can see the upper bound in Lemma 3.2\nis tight and can be achieved in the worst cases. Lemma 3.2 indicates that the adversarial loss\nLϵ(x, θ) is Lipschitz continuous, which is consistent with [45].\nTo study the second-order smoothness of Lϵ(x, θ), we start with the following assumption.\nAssumption 3.3. (Second-order Lipschitz condition) ∀i ∈{0, 1, ..., K −1}, the function fi\nsatisfies the following second-order Lipschitz conditions, with constants Lθθ, Lθx:\n∀x, θ1, θ2,\n∥∇θfi(x, θ1) −∇θfi(x, θ2)∥≤Lθθ∥θ1 −θ2∥,\n(5)\n∀θ, x1, x2,\n∥∇θfi(x1, θ) −∇θfi(x2, θ)∥≤Lθx∥x1 −x2∥.\n(6)\nLemma 3.4. (Lipschitz smoothness of adversarial loss) If Assumption 3.1 and 3.3 hold,\nwe have:\n∀x, θ1, θ2,\n∥∇θLϵ(x, θ1) −∇θLϵ(x, θ2)∥≤Aθθ∥θ1 −θ2∥+ Bθδ.\n(7)\nThe Lipschitz constant Aθθ = Lθθ and Bθδ = Lθx∥δ1 −δ2∥+ 4Lθ where δ1 ∈arg maxδ∈SϵL(x +\nδ, θ1) and δ2 ∈arg maxδ∈SϵL(x + δ, θ2).\nThe proof is deferred to Appendix B.2. Lemma 3.4 indicates the adversarial loss objective\nfunction Lϵ(x, θ) w.r.t. the model parameter θ is no longer smooth. That is to say, gradients in\narbitrarily small neighborhoods in the θ-space can change discontinuously. Furthermore, the degree\nof discontinuity is indicated by the value of Bθδ. Given the expression of Bθδ, we can conclude\nthat a larger ∥δ1 −δ2∥can intensify the gradient discontinuity. Additionally, as elucidated by\nTheorem 2 in [45], the gradients are non-vanishing in adversarial training. A large Bθδ introduces\nlarge gradient magnitudes asymptotically, making optimization challenging.\nHowever, in practice, we may use non-smooth activations, like ReLU, which do not strictly\nsatisfy Assumption 3.3. For example, the gradient of ReLU changes abruptly in the neighbor-\nhood around 0. In this regard, we provide a more detailed analysis of this case in Appendix C,\nwhich suggests that our analyses can be straightforwardly extended to networks with non-smooth\nactivations.\nWithout the loss of generality, the Lipschitz properties in Assumption 3.1 and 3.3 can be based\non any proper lp norm, i.e., p ∈[1, +∞], which, however, does not include l0 norm. Correspond-\ningly, ∥δ1 −δ2∥in the expression of Bθδ is based on the same norm as in the assumptions. On\nthe popular benchmark CIFAR-10, the commonly used values of ϵ in the l0, l1, l2 and l∞cases\nare 3601, 24, 0.5 and 8/255, respectively [5, 19, 23, 39]. In Appendix D, we discuss the numerical\nupper bound of ∥δ1 −δ2∥when the Lipschitz assumptions are based on different proper norms.\nThe results demonstrate that the upper bound of ∥δ1 −δ2∥in the l0 case is always significantly\nlarger than other cases, indicating a more craggy adversarial loss function in l0 adversarial train-\ning. Moreover, to corroborate the Lipschitz smoothness assumption in Inequality (6), we compare\nthe distances between the gradients induced by one-step and multi-step attacks with different\nadversarial budgets in Appendix E.3.\n1In Zhong et al. [23], the l0 adversarial budget for training on CIFAR-10 is 120 in the pixel space of RGB images,\nso the l0 norm in the feature space is 360.\n5\n\n\n0\n2\n4\n6\n8\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nValue\n1e5\ntrain = 1\ntrain = 20\ntrain = 40\ntrain = 120\n(a) Eigenvalues of ∇2\nθL(0)\nϵ\n0\n2\n4\n6\n8\nIndex\n101\n102\n103\n104\nValue\nl0\nl1\nl2\nl\n(b) Eigenvalues of ∇2\nθL(p)\nϵ\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n20\n40\n60\n(c) L(0)\nϵ\n, ϵtrain = 1\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n1\n2\n3\n4\n1.4\n1.6\n1.8\n2.0\n2.2\n(d) L(1)\nϵ\n, ϵtrain = 24\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0.0\n0.5\n1.0\n1.5\n2.0\n0.65\n0.70\n0.75\n0.80\n(e) L(2)\nϵ\n, ϵtrain = 0.5\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0.5\n1.0\n1.5\n2.0\n2.5\n0.98\n1.00\n1.02\n1.04\n1.06\n1.08\n(f) L(∞)\nϵ\n, ϵtrain = 8/255\nFigure 2:\nSmoothness of adversarial loss objective functions under different settings.\nAll losses are\ncalculated on the training set of CIFAR-10 [25] by PreactResNet-18 [24]. The l0, l1, l2 and l∞models\nare obtained by 1-step sAT [23], Fast-EG-l1 [19], 1-step PGD [36] and GradAlign [33], respectively. (a)\nTop 10 eigenvalues of ∇2\nθL(0)\nϵ (x, θ) with different values of ϵtrain in the l0 case. (b) Top 10 eigenvalues\nof ∇2\nθL(p)\nϵ (x, θ) under different choices of p, including l0 (ϵtrain = 1), l1 (ϵtrain = 24), l2 (ϵtrain =\n0.5) and l∞(ϵtrain = 8/255).\nThe y-axis is shown in the log scale.\n(c) - (f) The loss landscape of\nLϵ(x, θ + α1v1 + α2v2) where v1 and v2 are the eigenvectors associated with the top 2 eigenvalues of\n∇2\nθLϵ(x, θ), respectively. The y-scales for different sub-figures are different. (c) l0 case, ϵtrain = 1. (d) l1\ncase, ϵtrain = 24. (e) l2 case, ϵtrain = 0.5. (f) l∞case, ϵtrain = 8/255.\n3.3\nNumerical Analyzes on the Smoothness of Adversarial Loss Func-\ntions\nTo validate the conclusions in theoretical analyses, we conduct numerical experiments to study the\nproperties of loss landscape of l0 adversarial training and compare it with the l∞, l2 and l1 cases.\nWe first study the curvature in the neighborhood of model parameters, which reflects the\nsecond-order smoothness of the loss function and is dominated by top eigenvalues of Hessian\nmatrix ∇2\nθLϵ(x, θ). Numerically, we employ the power method [45–47] to iteratively estimate the\neigenvalues and the corresponding eigenvectors of Hessian matrices. We plot the top-10 eigenvalues\nof the Hessian matrices ∇2\nθLϵ(x, θ) under different ϵ in l0 cases in Figure 2 (a). In addition, we\ncompare the Hessian spectrum in the l0 case with l∞, l2 and l1 cases in Figure 2 (b). Our results\nin Figure 2 (a) demonstrate that eigenvalues of Hessian matrices in l0 cases increase as ϵ grows,\nindicating a higher degree of non-smoothness for a larger ϵ. Moreover, Figure 2 (b) indicates that\nthe adversarial loss landscape in the l0 case is more craggy than its l∞, l2 and l1 counterparts,\neven when we set ϵ = 1, i.e., perturbing only a single pixel. These observations corroborate that\nl0 adversarial training exhibits worse second-order smoothness than other cases.\nTo study the first-order smoothness, we visualize the loss landscape of different settings in\nFigures 2 (c)-(f), which demonstrate that the loss in the l0 case abruptly increases even with\nsubtle changes in the model parameters.\nThis further suggests the non-smooth nature of the\nl0 adversarial loss landscape. More loss landscape visualizations of l0 adversarial training with\ndifferent ϵ are provided in Appendix E.8. The observations are consistent with that in Figure 2.\nAccordingly, we confirm that the loss landscape of l0 adversarial loss function is more craggy than\nother cases from both theoretical and empirical perspectives. In addition, among the cases studied\nin Figure 3, the l0 cases are the only ones suffering from CO, while the l∞, l2 and l1 cases do not.\nThis indicates that the craggy loss landscape aggravates CO.\nOn the other side, we show in Figure 3 that successful attempts to obtain robust models\nagainst l0 bounded perturbation also include elements that help improve the smoothness of the loss\nlandscape. 20-step sAT in Zhong et al. [23] uses an early stopping (ES) strategy to avoid CO and to\n6\n\n\n0\n20\n40\n60\n80\n100\nEpoch t\n0\n1\n2\n3\n4\n5\n6\n7\n||\nt\n||2\n1e6\n20-step sAT\n20-step sAT w/o ES\n(a) Gradient Norm\n0\n20\n40\n60\n80\n100\nEpoch t\n0\n5\n10\n15\n20\n25\nTest robust accuracy (%)\n20-step sAT\n20-step sAT w/o ES\n(b) Test Robust Accuracy\nFigure 3:\nRelationship between craggy loss landscape and CO. (a) Gradient norm ∥∇θtLϵ∥2, which\nindicates the first-order smoothness of Lϵ. (b) Test robust accuracy against sAA (ϵ = 20). The results\nare obtained from PreactResNet-18 trained on CIFAR-10, where ϵtrain = 40. Note that since the training\nof 20-step sAT w/o ES diverges under ϵtrain = 120, the results are presented under ϵtrain = 40 instead.\nachieve competitive performance. Specifically, ES interrupts the attack iteration once the current\nperturbed input is misclassified. ES is shown to circumvent the potential for excessive gradient\nmagnitude while maintaining the efficacy of the generated perturbations. Figure 3 compares the\ncases with and without ES in terms of gradient norm and robust accuracy on the test set by\nsAA. We can observe from Figure 3 that 20-step sAT without ES still suffer from CO and the\ncorresponding gradient magnitude during training indicates a craggy loss landscape. This finding\nfurther highlights a strong correlation between CO and the craggy nature of the loss landscape in\nl0 adversarial training.\nIn summary, our results suggest that the l0 adversarial training exhibits a more craggy loss\nlandscape than other cases, which shows a strong correlation with CO. Additionally, despite the\nnon-trivial performance of 20-step sAT with ES, its performance still exhibits considerable fluc-\ntuation and can be further improved, underscoring the need for a smoother loss function. In the\nnext section, we will propose our method to address the CO issue in fast l0 adversarial training.\n4\nSoft Label and Trade-off Loss Smooth Adversarial Loss\nNotice that Aθ in Lemma 3.2 can be regarded as a function of the label y. Thus, we first study\nhow different y affects the properties of the adversarial loss objective function Lϵ(x, θ).\nLet\nyh ∈{0, 1}K and ys ∈(0, 1)K denote the hard and soft label, respectively. That is to say, yh is a\none-hot vector, while ys is a dense vector in a simplex. Then, we have the following theorem:\nTheorem 4.1. (Soft label improves Lipschitz continuity) Based on Lemma 3.2, given a hard\nlabel vector yh ∈{0, 1}K and a soft label vector ys ∈(0, 1)K, we have Aθ(ys) ≤Aθ(yh).\nThe proof is deferred to Appendix B.3. Theorem 4.1 indicates that soft labels lead to a reduced\nfirst-order Lipschitz constant, thereby enhancing the Lipschitz continuity of the adversarial loss\nfunction. However, as indicated by Lemma 3.4, the second-order Lipschitz constant remains unaf-\nfected by variations in y. Considering the poor performance on clean inputs when CO happens,\nwe introduce a trade-off loss objective function Lϵ,α which interpolates between the loss on the\nclean inputs and that on the adversarial inputs.\nLϵ,α(x, θ) = (1 −α)L(x, θ) + α max\nδ∈Sϵ(x) L(x + δ, θ)\n(8)\nwhere α ∈[0, 1] is the interpolation factor. Then, we have the following theorem:\nTheorem 4.2. (Trade-off loss function improves Lipschitz smoothness) If Assumption\n3.1 and 3.3 hold, we have:\n∥∇θLϵ,α(x, θ1) −∇θLϵ,α(x, θ2)∥≤Aθθ∥θ1 −θ2∥+ B′\nθδ\n(9)\nThe Lipschitz constant Aθθ = Lθθ and B′\nθδ = αLθx∥δ1−δ2∥+2(1+α)Lθ where δ1 ∈arg maxδ∈Sϵ(x)L(x+\nδ, θ1) and δ2 ∈arg maxδ∈Sϵ(x)L(x + δ, θ2).\n7\n\n\nThe proof is deferred to Appendix B.4. According to Theorem 4.2, the trade-off loss function\nLϵ,α enhances the second-order smoothness of adversarial loss objective function. The interpolation\nfactor α controls the balance between the loss on the clean inputs and the loss on the adversarial\ninputs. On one hand, a smaller value of α results in a smoother loss objective function, but it\nassigns less weight to the loss of the adversarial inputs and potentially hurts the robustness of the\nobtained model. On the other hand, a bigger value of α assigns more weight to the adversarial loss\nto focus on robustness, but it makes the corresponding adversarial loss objective function more\nchallenging for optimization. Furthermore, compared with l1, l2 and l∞cases, the trade-off loss\nfunction is particularly useful and necessary in the l0 case. This is supported by the analyses\nin Section 3.2 and Appendix D, which demonstrate that ∥δ1 −δ2∥is much larger in l0 bounded\nperturbations than other cases. Therefore, we expect the trade-off loss function Lϵ,α can help\nmitigate CO by improving smoothness.\nSimilar to Lemma 3.4, Theorem 4.2 can be straightforwardly extended to the networks with\nnon-smooth activations, where Assumption 3.3 is not strictly satisfied. We provide a more detailed\nanalysis in Appendix C to demonstrate the generality of our conclusions.\nIn summary, soft labels and the trade-off loss function can improve the first-order and second-\norder smoothness, respectively. Therefore, we can stabilize and improve the performance of fast\nadversarial training against l0 bounded perturbations by combining both techniques together.\nAmong various approaches available, we mainly exploit trade-off loss function, self-adaptive\ntraining (SAT) [38] and TRADES [34]. Specifically, SAT utilizes the moving average of previous\npredictions as the soft label to calculate the loss. TRADES combines the soft label and the trade-\noff loss function. It utilizes the trade-off loss function to balance the clean and robust accuracy\nand employs the prediction on the clean inputs as the soft label when calculating the loss for\nadversarial inputs. In Appendix A, we provide the pseudo-codes of both SAT and TRADES and\nthe formulation of their combination as a reference.\n5\nExperiments\nIn this section, we perform extensive experiments to investigate various approaches that can sta-\nbilize and improve the performance of fast adversarial training against l0 bounded perturbations.\nFurthermore, we compare the performance of 1-step adversarial training with the multi-step coun-\nterpart on different datasets. Our results demonstrate that approaches combining soft labels and\ntrade-off loss function significantly enhance the stability and efficacy of 1-step adversarial training,\neven surpassing some baselines of multi-step adversarial training. Finally, we validate the efficacy\nof our method on different networks in Appendix E.7, visualize the loss landscape when using soft\nlabel and trade-off loss function in Appendix E.9 to demonstrate its improved smoothness, and\nconduct ablation studies for analysis in Appendix E.10.\n5.1\nApproaches to Improving 1-Step l0 Adversarial Training\nTable 3: Comparison of different approaches and their combinations in robust accuracy (%) by sAA. The\ntarget sparsity level ϵ = 20. We compare PreAct ResNet-18 [24] models trained on CIFAR-10 [25] with\n100 epochs. The italic numbers indicate catastrophic overfitting (CO) happens.\nMethod\nsAT\nTradeoff\nsTRADES (T)\nsTRADES (F)\n1-step\n0.0\n2.6\n31.0\n55.4\n+ N-FGSM\n0.3\n17.5\n46.9\n55.9\n+ SAT\n29.3\n30.3\n61.4\n59.4\n+ SAT & N-FGSM\n43.8\n39.2\n63.0\n62.6\nWe begin our analysis by evaluating the effectiveness of different approaches and their combina-\ntions, focusing on those that incorporate either soft labels or trade-off loss functions. Additionally,\nwe explore the data augmentation technique N-FGSM [41], known for its ability to improve the\nperformance of fast adversarial training without imposing significant computational overhead. Our\nfindings, summarized in Table 3, are all based on 1-step adversarial training. The robust accuracy\nis measured using the sparse-AutoAttack (sAA) method, with ϵ set to 20.\nIn Table 3, we investigate the following approaches and their combinations: (1) sAT: adversar-\nial training against 1-step sPGD [23]. (2) Tradeoff: 1-step adversarial training with the trade-off\n8\n\n\nloss function defined in Eq. (8). (3) sTRADES: the 1-step sTRADES [23]. As discussed in\nAppendix A, it incorporates both soft label and trade-off loss function. We include two variants\nof sTRADES for comparison: sTRADES (T) is the training mode where we only use the loss\nobjective function of TRADES for training but still use the cross-entropy loss to generate adver-\nsarial examples; sTRADES (F) is the full mode where we use the KL divergence loss function for\ngenerating adversarial perturbations. Compared with 1-step sAT, sTRADES (T) introduces 25%\noverhead while sTRADES (F) introduces 50% overhead. (4) SAT: self-adaptive training [38]. As\ndiscussed in Appendix A, it introduces soft labels based on the moving average of the historical\npredictions and uses adaptive weights for training instances of different prediction confidence. (5)\nN-FGSM: data augmentation technique by adding random noise to the training data. It is proven\neffective in 1-step adversarial training [41] and may mitigate the sub-optimality of perturbation lo-\ncation by randomly perturbing more pixels. The implementation details are deferred to Appendix\nF.\nThe results in Table 3 indicate that using trade-off loss function alone still suffers from CO. In\ncontrast, using soft label, either by SAT or sTRADES, can eliminate CO and achieve notable robust\naccuracy. This suggests that the soft label has a more prominent role in mitigating overfitting\nthan the trade-off loss function in 1-step l0 adversarial training. Furthermore, sTRADES (F) alone\noutperforms sTRADES (T) along by a substantial margin of 24.4%, which can be attributed to the\ngeneration of higher-quality adversarial examples for training by sTRADES (F). Finally, both SAT\nand N-FGSM can enhance the performance of all approaches, demonstrating their effectiveness.\nIt is important to note that all the results presented in Table 3 are obtained using sAA, which is\nknown for generating the strongest attacks in terms of sparse perturbations. Our findings demon-\nstrate that incorporating soft labels and trade-off loss function yields substantial performance\nimprovements in 1-step l0 adversarial training. Among various combinations of methods explored,\nthe model trained with sTRADES (T) in combination with SAT and N-FGSM achieves the highest\nrobust accuracy against sAA, reaching an impressive 63.0%. This establishes a new state-of-the-\nart performance in the context of fast robust learning methods against l0 bounded perturbations.\nFor convenience, we name this combination (i.e., 1-step sTRADES + SAT + N-FGSM) Fast-Loss\nSmoothing-l0 (Fast-LS-l0) in the subsequent sections. Its pseudo-code is given in Algorithm 3 of\nAppendix A. Additionally, the comparison with more baselines that either mitigate CO or smooth\nthe loss function is undertaken in Appendix E.4. The results demonstrate that our method is the\nmost effective approach for fast l0 adversarial training.\n5.2\nComparison with Multi-Step Adversarial Training\nIn this section, we compare 1-step adversarial training with its multi-step counterpart. For multi-\nstep adversarial training, we follow the settings in [23] and use 20-step sPGD based on cross-entropy\nto generate adversarial perturbations in sAT and sTRADES. Similar to Table 3, we incorporate\nSAT and N-FGSM into multi-step adversarial training as well. For 1-step adversarial training, we\nfocus on the configurations with the best performance in Table 3, i.e., Fast-LS-l0.\nWe conduct extensive experiments on various datasets. The results on CIFAR-10 and ImageNet-\n100 [48] are demonstrated in Table 4. More results on CIFAR-100 [25] and GTSRB [49] are in Table\n7 and 8 of Appendix E.5, respectively. Following the settings in [23], and given the prohibitively\nhigh complexity involved, we exclude multi-step sTRADES from the evaluation on ImageNet-\n100. In addition to the performance under sAA, we report the robust accuracy of these models\nunder various black-box and white box attacks, including CornerSearch (CS) [21], Sparse-RS (RS)\n[22], SAIF [40] and two versions of sPGD [23].\nNote that, we do not include SparseFool [20]\nand PGD0 [21] for evaluation, because they only have trivial attack success rates on our models.\nMoreover, we report the clean accuracy and the total running time for reference. Finally, to more\ncomprehensively validate the effectiveness of our results, we report the standard deviation of the\nperformance in Table 9 of Appendix E.6.\nThe results in Table 4, 7 and 8 suggest that both soft labels and trade-off loss function, in-\ntroduced by SAT and TRADES, can improve the performance of both 1-step and multi-step\nadversarial training. In addition, N-FGSM, originally designed for one-step adversarial training,\nalso contributes to performance improvements in the multi-step scenario. Furthermore, these tech-\nniques can greatly narrow down the performance gaps between 1-step and multi-step adversarial\ntraining, making fast adversarial training more feasible and competitive in the context of sparse\nperturbations. With the assistance of SAT and N-FGSM, our Fast-LS-l0 can achieve a performance\n9\n\n\nTable 4: Robust accuracy (%) against sparse attacks. (a) The models are PreAct ResNet-18 trained on\nCIFAR-10, where the sparsity level ϵ = 20. CornerSearch (CS) is evaluated on 1000 samples due to its\nhigh computational complexity. (b) The models are ResNet-34 trained on ImageNet-100, where the\nsparsity level ϵ = 200. CS is not evaluated here due to its high computational complexity. Note that S and\nN denote SAT and N-FGSM, respectively. The results of vanilla 20-step sAT and sTRADES are obtained\nfrom [23]. All experiments are implemented on one NVIDIA RTX 6000 Ada GPU.\n(a) CIFAR-10, ϵ = 20\nModel\nTime\nCost\nClean\nBlack-Box\nWhite-Box\nsAA\nCS\nRS\nSAIF\nsPGDproj\nsPGDunproj\nMulti-step\nsAT\n5h 16m\n84.5\n52.1\n36.2\n76.6\n75.9\n75.3\n36.2\n+S\n5h 24m\n80.4\n58.4\n55.7\n75.0\n75.1\n74.0\n55.5\nsTRADES\n5h 30m\n89.8\n69.9\n61.8\n84.9\n84.6\n81.7\n61.7\n+S&N\n5h 22m\n82.2\n66.3\n66.1\n77.1\n74.1\n72.2\n65.5\nOne-step\nFast-LS-l0 (T)\n50m\n82.5\n69.3\n65.4\n75.7\n67.2\n67.7\n63.0\nFast-LS-l0 (F)\n59m\n82.6\n69.6\n64.1\n75.2\n64.6\n68.4\n62.6\n(b) ImageNet, ϵ = 200\nModel\nTime\nCost\nClean\nBlack-Box\nWhite-Box\nsAA\nCS\nRS\nSAIF\nsPGDproj\nsPGDunproj\nMulti-step\nsAT\n324h 57m\n86.2\n-\n61.4\n69.0\n78.0\n77.8\n61.2\n+S&N\n336h 20m\n83.0\n-\n75.0\n76.4\n78.8\n79.2\n74.8\nsTRADES\n358h 55m\n84.8\n-\n76.0\n77.4\n80.6\n81.4\n75.8\n+S&N\n359h 55m\n82.4\n-\n78.2\n79.2\n78.2\n79.8\n77.8\nOne-step\nFast-LS-l0 (T)\n43h 48m\n82.4\n-\n76.8\n75.4\n74.6\n74.6\n72.4\nFast-LS-l0 (F)\n55h 39m\n80.0\n-\n77.4\n76.0\n76.6\n74.4\n72.8\nthat is merely 2.5% lower than that of the 20-step sTRADES while requiring less than 1/6 of the\ntotal running time.\n6\nConclusion\nIn this paper, we highlight the catastrophic overfitting (CO) in the fast l0 adversarial training is\ninduced by sub-optimal perturbation locations of 1-step attacks, which is distinct from the l∞, l2\nand l1 cases. Theoretical and empirical analyses reveal that the loss landscape of l0 adversarial\ntraining is more craggy than other cases, and the craggy loss landscape strongly correlates with\nCO. To address these issues, we propose Fast-LS-l0 that incorporates soft label and trade-off\nloss function to smooth the adversarial loss function.\nExtensive experiments demonstrate the\neffectiveness of our method in mitigating CO and narrowing down the performance gap between\n1-step and multi-step l0 adversarial training. The models trained with our method exhibit state-\nof-the-art robustness against sparse attacks in the context of fast adversarial training.\n7\nFuture Work\nOur previous work [23] and this paper investigate the generation of l0 bounded adversarial per-\nturbations and the corresponding defending algorithm, respectively. Our future work will focus\non extending the algorithm we have proposed to generate structured sparse perturbations.\nIn addition to the sparsity constraint, the locations of perturbations are constrained to be within\nspecific regions, such as patches, columns, and any customized patterns, for structured sparse\n10\n\n\nperturbations.\nMoreover, I will explore other scenarios that raise concerns in the community of trustworthy\ndeep learning, e.g., Machine Unlearning [50] and Adversarial Machine Learning for Social\nGood [51].\nMachine unlearning aims to remove the effect of a small “forget set” of training\ndata on a pretrained machine learning model. Whereas, adversarial machine learning for social\ngood leverages adversarial attacks to enhance the transparency, privacy, fairness, and reliability of\nmachine learning systems.\nReferences\n[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus.\nIntriguing properties of neural networks.\narXiv preprint\narXiv:1312.6199, 2013.\n[2] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense\nof security: Circumventing defenses to adversarial examples. In International Conference on\nMachine Learning, 2018. URL https://api.semanticscholar.org/CorpusID:3310672.\n[3] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an\nensemble of diverse parameter-free attacks. In International conference on machine learning,\npages 2206–2216. PMLR, 2020.\n[4] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas\nFlammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized\nadversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020.\n[5] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian\nVladu.\nTowards deep learning models resistant to adversarial attacks.\nIn International\nConference on Learning Representations, 2018. URL https://openreview.net/forum?id=\nrJzIBfZAb.\n[6] Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast\nadaptive boundary attack. In International Conference on Machine Learning, pages 2196–\n2205. PMLR, 2020.\n[7] Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chi-\nang, and Prateek Mittal. Robust learning meets generative models: Can proxy distributions\nimprove adversarial robustness? In International Conference on Learning Representations.\n[8] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and\nTimothy Mann. Fixing data augmentation to improve adversarial robustness. arXiv preprint\narXiv:2103.01946, 2021.\n[9] Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian,\nand Timothy A Mann.\nImproving robustness using generated data.\nAdvances in Neural\nInformation Processing Systems, 34:4218–4233, 2021.\n[10] Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Helper-based adversarial training: Reduc-\ning excessive margin to achieve a better accuracy vs. robustness trade-off. In ICML 2021\nWorkshop on Adversarial Machine Learning, 2021. URL https://openreview.net/forum?\nid=BuD2LmNaU3a.\n[11] Jiequan Cui, Zhuotao Tian, Zhisheng Zhong, Xiaojuan Qi, Bei Yu, and Hanwang Zhang.\nDecoupled kullback-leibler divergence loss. arXiv preprint arXiv:2305.13948, 2023.\n[12] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan.\nBetter\ndiffusion models further improve adversarial training. In International Conference on Machine\nLearning, pages 36246–36263. PMLR, 2023.\n[13] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph\nStuder, Larry S Davis, Gavin Taylor, and Tom Goldstein.\nAdversarial training for free!\nAdvances in neural information processing systems, 32, 2019.\n11\n\n\n[14] Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only prop-\nagate once: Accelerating adversarial training via maximal principle. Advances in neural in-\nformation processing systems, 32, 2019.\n[15] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial\ntraining. In International Conference on Learning Representations.\n[16] Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, and Venkatesh Babu R.\nTo-\nwards efficient and effective adversarial training.\nIn M. Ranzato,\nA. Beygelzimer,\nY.\nDauphin,\nP.S.\nLiang,\nand\nJ.\nWortman\nVaughan,\neditors,\nAdvances\nin\nNeural\nInformation Processing Systems,\nvolume\n34,\npages\n11821–11833.\nCurran\nAssociates,\nInc.,\n2021.\nURL https://proceedings.neurips.cc/paper_files/paper/2021/file/\n62889e73828c756c961c5a6d6c01a463-Paper.pdf.\n[17] Peilin Kang and Seyed-Mohsen Moosavi-Dezfooli. Understanding catastrophic overfitting in\nadversarial training. arXiv preprint arXiv:2105.02942, 2021.\n[18] Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturba-\ntions. Advances in neural information processing systems, 32, 2019.\n[19] Yulun Jiang, Chen Liu, Zhichao Huang, Mathieu Salzmann, and Sabine S¨usstrunk. Towards\nstable and efficient adversarial training against l1 bounded adversarial attacks. In International\nConference on Machine Learning. PMLR, 2023.\n[20] Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Sparsefool: a few\npixels make a big difference. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 9087–9096, 2019.\n[21] Francesco Croce and Matthias Hein. Sparse and imperceivable adversarial attacks. In Pro-\nceedings of the IEEE/CVF international conference on computer vision, pages 4724–4732,\n2019.\n[22] Francesco Croce, Maksym Andriushchenko, Naman D Singh, Nicolas Flammarion, and\nMatthias Hein. Sparse-rs: a versatile framework for query-efficient sparse black-box adver-\nsarial attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36,\npages 6437–6445, 2022.\n[23] Xuyang Zhong, Yixiao Huang, and Chen Liu. Towards efficient training and evaluation of\nrobust models against l0 bounded adversarial perturbations. ArXiv, abs/2405.05075, 2024.\nURL https://arxiv.org/abs/2405.05075.\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n[26] Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overfitting in single-\nstep adversarial training. In AAAI Conference on Artificial Intelligence, 2020. URL https:\n//api.semanticscholar.org/CorpusID:222133879.\n[27] Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adver-\nsarial training. Advances in Neural Information Processing Systems, 33:16048–16059, 2020.\n[28] Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, and Atul Prakash. Efficient ad-\nversarial training with transferable adversarial examples. 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 1178–1187, 2019. URL https:\n//api.semanticscholar.org/CorpusID:209501025.\n[29] Zhichao Huang, Yanbo Fan, Chen Liu, Weizhong Zhang, Yong Zhang, Mathieu Salzmann,\nSabine S¨usstrunk, and Jue Wang. Fast adversarial training with adaptive step size. IEEE\nTransactions on Image Processing, 2023.\n12\n\n\n[30] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-\nsarial examples. arXiv preprint arXiv:1412.6572, 2014.\n[31] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale.\nIn International Conference on Learning Representations, 2017. URL https://openreview.\nnet/forum?id=BJm4T4Kgx.\n[32] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo\nLi. Boosting adversarial attacks with momentum. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2018.\n[33] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square\nattack: a query-efficient black-box adversarial attack via random search. In European confer-\nence on computer vision, pages 484–501. Springer, 2020.\n[34] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael\nJordan. Theoretically principled trade-off between robustness and accuracy. In International\nconference on machine learning, pages 7472–7482. PMLR, 2019.\n[35] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving\nadversarial robustness requires revisiting misclassified examples. In International Conference\non Learning Representations, 2020. URL https://openreview.net/forum?id=rklOg6EFwS.\n[36] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In\nInternational conference on machine learning, pages 8093–8104. PMLR, 2020.\n[37] Chen Liu, Zhichao Huang, Mathieu Salzmann, Tong Zhang, and Sabine S¨usstrunk. On the\nimpact of hard adversarial instances on overfitting in adversarial training, 2021.\n[38] Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical\nrisk minimization. Advances in neural information processing systems, 33:19365–19376, 2020.\n[39] Francesco Croce and Matthias Hein. Mind the box: l 1-apgd for sparse adversarial attacks on\nimage classifiers. In International Conference on Machine Learning, pages 2201–2211. PMLR,\n2021.\n[40] Tooba Imtiaz, Morgan Kohler, Jared Miller, Zifeng Wang, Mario Sznaier, Octavia Camps,\nand Jennifer Dy. Saif: Sparse adversarial and interpretable attack framework. arXiv preprint\narXiv:2212.07495, 2022.\n[41] Pau de Jorge Aranda, Adel Bibi, Riccardo Volpi, Amartya Sanyal, Philip Torr, Gr´egory Rogez,\nand Puneet Dokania. Make some noise: Reliable and efficient single-step adversarial training.\nAdvances in Neural Information Processing Systems, 35:12881–12893, 2022.\n[42] Runqi Lin, Chaojian Yu, and Tongliang Liu. Eliminating catastrophic overfitting via abnormal\nadversarial examples regularization. Advances in Neural Information Processing Systems, 36,\n2024.\n[43] Runqi Lin, Chaojian Yu, Bo Han, Hang Su, and Tongliang Liu.\nLayer-aware analysis of\ncatastrophic overfitting: Revealing the pseudo-robust shortcut dependency.\nIn Forty-first\nInternational Conference on Machine Learning, 2024.\n[44] Lin Li and Michael Spratling. Understanding and combating robust overfitting via input loss\nlandscape analysis and regularization. Pattern Recognition, 136:109229, 2023.\n[45] Chen Liu, Mathieu Salzmann, Tao Lin, Ryota Tomioka, and Sabine S¨usstrunk. On the loss\nlandscape of adversarial training: Identifying challenges and how to overcome them. Advances\nin Neural Information Processing Systems, 33:21476–21487, 2020.\n[46] Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based\nanalysis of large batch training and robustness to adversaries. Advances in Neural Information\nProcessing Systems, 31, 2018.\n[47] Xuyang Zhong and Chen Liu. Towards mitigating architecture overfitting in dataset distilla-\ntion. arXiv preprint arXiv:2309.04195, 2023.\n13\n\n\n[48] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale\nHierarchical Image Database. In CVPR09, 2009.\n[49] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer:\nBenchmarking machine learning algorithms for traffic sign recognition. Neural networks, 32:\n323–332, 2012.\n[50] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin\nTravers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE\nSymposium on Security and Privacy (SP), pages 141–159. IEEE, 2021.\n[51] Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai\nHoang, Dusit Niyato, and Ala Al-Fuqaha.\nAdversarial machine learning for social good:\nReframing the adversary as an ally. IEEE Transactions on Artificial Intelligence, 2024.\n[52] Yihua Zhang, Guanhua Zhang, Prashant Khanduri, Mingyi Hong, Shiyu Chang, and Sijia Liu.\nRevisiting and advancing fast adversarial training through the lens of bi-level optimization.\nIn International Conference on Machine Learning, pages 26693–26712. PMLR, 2022.\n[53] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2818–2826, 2016.\n[54] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust\ngeneralization. Advances in neural information processing systems, 33:2958–2969, 2020.\n[55] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\nXie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 11976–11986, 2022.\n[56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\n[57] Edoardo Debenedetti, Vikash Sehwag, and Prateek Mittal. A light recipe to train robust\nvision transformers. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning\n(SaTML), pages 225–253. IEEE, 2023.\n[58] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 630–645. Springer, 2016.\n[59] Charles Dugas, Yoshua Bengio, Fran¸cois B´elisle, Claude Nadeau, and Ren´e Garcia. Incor-\nporating second-order functional knowledge for better option pricing.\nAdvances in neural\ninformation processing systems, 13, 2000.\n14\n\n\nA\nAlgorithm Details\nAlgorithm 1 Self-Adaptive Training (SAT) [38]\n1: Input: Data: {(xi, yi)}n; Initial target {ti}n = {yi}n; Batch size: m; Classifier: f; Enabling\nepoch: Es; Momentum factor: α\n2: repeat\n3:\nFetch mini-batch data {(xi, ti)}m at current epoch e\n4:\nfor i = 1, ..., m do\n5:\npi = softmax(f(xi))\n6:\nif e > Es then\n7:\nti = α × ti + (1 −α) × pi\n8:\nend if\n9:\nwi = maxj ti,j\n10:\nend for\n11:\nCalculate the loss LSAT = −\n1\nP\ni wi\nP\ni wi\nP\nj ti,j log pi,j\n12:\nUpdate the parameters of f on LSAT\n13: until end of training\nAlgorithm 2 TRADES [34]\n1: Input: Data: (x, y); Classifier: f; Balancing factor: β; TRADES mode: mode; Sparse level:\nϵ\n2: if mode = F then\n3:\nGenerate adversarial sample ex = max(ex−x)∈Sϵ(x) KL(f(x), f(ex))\n4: else if mode = T then\n5:\nGenerate adversarial sample ex = max(ex−x)∈Sϵ(x) CE(f(ex), y)\n6: end if\n7: Calculate the loss LT RADES = CE(f(x), y) + β · KL(f(x), f(ex))\n8: Update the parameters of f on LT RADES\nThe pseudo-codes of SAT [38] and TRADES [34] are provided in Algorithm 1 and 2, respectively.\nFor SAT, the moving average of the previous predictions {ti}n can be regarded as the soft labels.\nFor TRADES, f(x) can be seen as the soft label of f(ex), and the combination of cross-entropy\nand KL divergence is also a trade-off loss function. Note that when combining SAT and TRADES,\nthe loss LS+T for a mini-batch data {(xi, yi)}m can be written as:\nLS+T = −\n1\nP\ni wi\nX\ni\nwi · CE(f(xi), ti) + β\nm\nX\ni\nKL(f(xi), f(exi))\n(10)\nIn addition, we provide the pseudo-code of the proposed Fast-LS-l0, which incorporates SAT,\nTRADES and N-FGSM, in Algorithm 3.\nB\nProofs\nB.1\nProof of Lemma 3.2\nProof. Based on the definition of δ1 and δ2, we have Lϵ(x, θ1) = L(x + δ1, θ1) and Lϵ(x, θ2) =\nL(x + δ2, θ2). In this regard, we have:\n∥Lϵ(x, θ1) −Lϵ(x, θ2)∥= ∥L(x + δ1, θ1) −L(x + δ2, θ2)∥\n(11)\n15\n\n\nAlgorithm 3 Fast-LS-l0\n1: Input: Data: {(xi, yi)}n; Initial target {ti}n = {yi}n; Batch size: m; Classifier: f; Enabling\nepoch: Es; Momentum factor: α; Balancing factor: β; TRADES mode: mode; Sparse level: ϵ\n2: repeat\n3:\nFetch mini-batch data {(xi, ti)}m at current epoch e\n4:\nfor i = 1, ..., m do\n5:\nηi ∼S2ϵ(xi)\n6:\nxi = xi + ηi\n// Augment sample with additive noise\n7:\nif mode = F then\n8:\nexi = max(exi−xi)∈Sϵ(xi) KL(f(xi), f(exi))\n9:\nelse if mode = T then\n10:\nexi = max(exi−xi)∈Sϵ(xi) CE(f(exi), ti)\n11:\nend if\n12:\npi = softmax(f(xi))\n13:\nif e > Es then\n14:\nti = α × ti + (1 −α) × pi\n15:\nend if\n16:\nwi = maxj ti,j\n17:\nend for\n18:\nCalculate LS+T in Eq. (10)\n19:\nUpdate the parameters of f on LS+T\n20: until end of training\nWhen L(x + δ1, θ1) ≥L(x + δ2, θ2) we have\n∥L(x + δ1, θ1) −L(x + δ2, θ2)∥\n=∥L(x + δ1, θ1) −L(x + δ1, θ2) + L(x + δ1, θ2) −L(x + δ2, θ2)∥\n≤∥L(x + δ1, θ1) −L(x + δ1, θ2)∥\n(12)\nThe inequality above is derived from the optimality of δ2, which indicates L(x+δ1, θ2)−L(x+\nδ2, θ2) ≤0 and the assumption L(x + δ1, θ1) ≥L(x + δ2, θ2).\nSimilarly, when L(x + δ1, θ1) ≤L(x + δ2, θ2) we have\n∥L(x + δ1, θ1) −L(x + δ2, θ2)∥\n=∥L(x + δ1, θ1) −L(x + δ2, θ1) + L(x + δ2, θ1) −L(x + δ2, θ2)∥\n≤∥L(x + δ2, θ1) −L(x + δ2, θ2)∥\n(13)\nWithout the loss of generality, we further bound ∥Lϵ(x, θ1) −Lϵ(x, θ2)∥based on (12). The\nderivation can be straightforwardly extended to (13) by replacing δ1 with δ2.\nBased on the formulation of L in (1), ∥Lϵ(x, θ1) −Lϵ(x, θ2)∥can be further derived as follows:\n∥Lϵ(x, θ1) −Lϵ(x, θ2)∥≤\n\f\f\f\f\f\f\nX\ni∈S+\nyi log hi(x + δ1, θ2)\nhi(x + δ1, θ1)\n\f\f\f\f\f\f\n=\nX\ni∈S+\nyi\n\f\f\f\f\flog\n1 + P\nj̸=i exp(fj(x + δ1, θ2) −fi(x + δ1, θ2))\n1 + P\nj̸=i exp(fj(x + δ1, θ1) −fi(x + δ1, θ1))\n\f\f\f\f\f\n(14)\nwhere S+ = {i | yi > 0, hi(x + δ1, θ2) > hi(x + δ1, θ1)}. Then, according to the mediant\ninequality, we have\n16\n\n\n\f\f\f\f\flog\n1 + P\nj̸=i exp(fj(x + δ1, θ2) −fi(x + δ1, θ2))\n1 + P\nj̸=i exp(fj(x + δ1, θ1) −fi(x + δ1, θ1))\n\f\f\f\f\f\n≤\n\f\f\f\f\flog\nP\nj̸=i exp(fj(x + δ1, θ2) −fi(x + δ1, θ2))\nP\nj̸=i exp(fj(x + δ1, θ1) −fi(x + δ1, θ1))\n\f\f\f\f\f\n≤max\nk\n\f\f\f\flog exp(fk(x + δ1, θ2) −fi(x + δ1, θ2))\nexp(fk(x + δ1, θ1) −fi(x + δ1, θ1))\n\f\f\f\f\n≤max\nk\n|fk(x + δ1, θ2) −fk(x + δ1, θ1)| + |fi(x + δ1, θ2) −fi(x + δ1, θ1)|\n≤2Lθ∥θ1 −θ2∥\n(15)\nNote that the bound on the right of (15) is tight. The upper bound can be achieved asymptot-\nically if the condition in (16) and the Lipschitz bound in Assumption 3.1 are satisfied.\n\f\f\f|fk(x + δ1, θ2) −fi(x + δ1, θ2)| −|fk(x + δ1, θ1) −fi(x + δ1, θ1)|\n\f\f\f\n≫max\nj̸=k\n\f\f\f|fj(x + δ1, θ2) −fi(x + δ1, θ2)| −|fj(x + δ1, θ1) −fi(x + δ1, θ1)|\n\f\f\f\n(16)\nCombining (11)-(15), we have\n∥Lϵ(x, θ1) −Lϵ(x, θ2)∥≤Aθ∥θ1 −θ2∥,\n(17)\nwhere Aθ = 2 P\ni∈S+ yiLθ.\nB.2\nProof of Lemma 3.4\nProof. Given (1), ∇θL is computed as\n∇θL(x, θ) = −\nK−1\nX\ni=0\nyi\n\"\n∇θfi(x, θ) −\nP\nj exp(fj(x, θ))∇θfj(x, θ)\nP\nj exp(fj(x, θ))\n#\n=\nP\nj exp(fj(x, θ))∇θfj(x, θ)\nP\nj exp(fj(x, θ))\n−\nK−1\nX\ni=0\nyi∇θfi(x, θ)\ndef\n=\nK−1\nX\nj=0\nhj(x, θ)∇θfj(x, θ) −\nK−1\nX\ni=0\nyi∇θfi(x, θ)\n(18)\nThe second equality is based on the fact that {yi}K−1\ni=0 is in a simplex. To simplify the notation,\nthe last equation is based on the definition that {hj}K−1\nj=0 is the result of softmax function applied to\n{fj}K−1\nj=0 , i.e., hj(x, θ) =\nexp(fj(x,θ))\nP\nk exp(fk(x,θ)). Therefore, we have PK−1\nj=0 hj(x, θ) = 1 and ∀j, hj(x, θ) >\n0.\nAccording to the triangle inequality, we have:\n∥∇θ1L(x + δ1, θ1) −∇θ2L(x + δ2, θ2)∥\n≤∥∇θ1L(x + δ1, θ1) −∇θ1L(x + δ2, θ1)∥+ ∥∇θ1L(x + δ2, θ1) −∇θ2L(x + δ2, θ2)∥\n(19)\nPlug (18) to the first term on the right hand side of (19), we obtain:\n∥∇θ1L(x + δ1, θ1) −∇θ1L(x + δ2, θ1)∥≤\nK−1\nX\ni=0\nyi ∥∇θ1fi(x + δ1, θ1) −∇θ1fi(x + δ2, θ1)∥\n+\n\r\r\r\r\r\r\nK−1\nX\nj=0\nhj(x + δ1, θ1)∇θfj(x + δ1, θ1) −\nK−1\nX\nj=0\nhj(x + δ2, θ1)∇θfj(x + δ2, θ1)\n\r\r\r\r\r\r\n(20)\n17\n\n\nThe first term can be bounded based on Assumption 3.1. The second term can be bounded as\nfollows:\n\r\r\r\r\r\r\nK−1\nX\nj=0\nhj(x + δ1, θ1)∇θfj(x + δ1, θ1) −\nK−1\nX\nj=0\nhj(x + δ2, θ1)∇θfj(x + δ2, θ1)\n\r\r\r\r\r\r\n≤\n\r\r\r\r\r\r\nK−1\nX\nj=0\nhj(x + δ1, θ1)∇θfj(x + δ1, θ1)\n\r\r\r\r\r\r\n+\n\r\r\r\r\r\r\nK−1\nX\nj=0\nhj(x + δ2, θ1)∇θfj(x + δ2, θ1)\n\r\r\r\r\r\r\n≤\nK−1\nX\nj=0\nhj(x + δ1, θ1)\n\r\r\r\rmax\nk\n∇θfk(x + δ1, θ1)\n\r\r\r\r +\nK−1\nX\nj=0\nhj(x + δ2, θ1)\n\r\r\r\rmax\nk\n∇θfk(x + δ2, θ1)\n\r\r\r\r\n≤2Lθ\n(21)\nNote that the bound on the right of (21) is tight. The first inequality is based on the triangle\ninequality. The second inequality and the third inequality can be achieved asymptotically when\nthe equality of first-order Lipschitz continuity in Assumption 3.1 is achieved and the following\ncondition is satisfied.\n∃k1 ∈arg maxiL(i)\nθ , hk1(x + δ1, θ1) →1, max\nj̸=k1 hj(x + δ1, θ1) →0\n∃k2 ∈arg maxiL(i)\nθ , hk2(x + δ2, θ1) →1, max\nj̸=k2 hj(x + δ2, θ1) →0\n(22)\nNote that k1 and k2 are not always the same, since there may exist more than one biggest first-order\nLipschitz constant.\nCombining (20) and (21) together, we obtain:\n∥∇θ1L(x + δ1, θ1) −∇θ1L(x + δ2, θ1)∥≤2Lθ + Lθx∥δ2 −δ1∥\n(23)\nSimilarly, we have:\n∥∇θ1L(x + δ2, θ1) −∇θ2L(x + δ2, θ2)∥≤2Lθ + Lθθ∥θ2 −θ1∥\n(24)\nCombing the two inequalities above, we have:\n∥∇θL(x + δ1, θ1) −∇θL(x + δ2, θ2)∥≤Aθθ∥θ1 −θ2∥+ Bθθ\n(25)\nwhere\nAθθ = Lθθ;\nBθθ = 4Lθ + Lθx∥δ1 −δ2∥\n(26)\nB.3\nProof of Theorem 4.1\nProof. For hard label yh ∈{0, 1}K, let that the j-th elements of yh be 1 and the rest be 0. By\nthe definition of Aθ in Lemma 3.2, we have\nAθ(yh) = 2Lθ.\n(27)\nIt is known that PK−1\ni=0 hi(x, θ) = 1, which means ∃j, hj(x + δ1, θ2) ≤hj(x + δ1, θ1). Then, for\nsoft label ys ∈(0, 1)K, we have |S+| < K where S+ = {i | yi > 0, hi(x + δ1, θ2) > hi(x + δ1, θ1)}.\nThus, it holds\nAθ(ys) = 2\nX\ni∈S+\ny(i)\ns Lθ ≤Aθ(yh).\n(28)\nThe equality can be achieved asymptotically if P\ni/∈S+ y(i)\ns\n→0.\n18\n\n\nB.4\nProof of Theorem 4.2\nProof. By the definition of Lϵ,α in (8), we have\n∥∇θ1Lϵ,α(x, θ1) −∇θ2Lϵ,α(x, θ2)∥\n≤(1 −α)∥∇θ1L(x, θ1) −∇θ1L(x, θ2)∥+ α∥∇θ1Lϵ(x, θ1) −∇θ1Lϵ(x, θ2)∥\n(29)\nAccording to (24) in the proof of Lemma 3.4, the first term of the right hand side of (29) can be\nderived as\n∥∇θ1L(x, θ1) −∇θ2L(x, θ2)∥≤Lθθ∥θ1 −θ2∥+ 2Lθ.\n(30)\nAccording to Lemma 3.4, the second term of the right hand side of (29) satisifies\n∥∇θ1Lϵ(x, θ1) −∇θ2Lϵ(x, θ2)∥≤Lθθ∥θ1 −θ2∥+ Lθx∥δ1 −δ2∥+ 4Lθ.\n(31)\nCombining (29), (30) and (31), we have\n∥∇θ1Lϵ,α(x, θ1) −∇θ2Lϵ,α(x, θ2)∥≤Aθθ∥θ1 −θ2∥+ B′\nθδ,\n(32)\nwhere Aθθ = Lθθ and B′\nθδ = αLθx∥δ1 −δ2∥+ 2(1 + α)Lθ.\nC\nTheoretical Analysis of ReLU Networks\nSimilar to [45], we first make the following assumptions for the functions {fi}K−1\ni=0\nrepresented by\na ReLU network.\nAssumption C.1. ∀i ∈{0, 1, ..., K −1}, the function fi satisfies the following conditions:\n∀x, θ1, θ2,\n∥fi(x, θ1) −fi(x, θ2)∥≤Lθ∥θ1 −θ2∥,\n(33)\n∀θ, x1, x2,\n∥fi(x1, θ) −fi(x2, θ)∥≤Lx∥x1 −x2∥,\n(34)\n∀x, θ1, θ2,\n∥∇θfi(x, θ1) −∇θfi(x, θ2)∥≤Lθθ∥θ1 −θ2∥+ Cθθ,\n(35)\n∀θ, x1, x2,\n∥∇θfi(x1, θ) −∇θfi(x2, θ)∥≤Lθx∥x1 −x2∥+ Cθx.\n(36)\nCompared to Assumption 3.1 and 3.3, we modify the the second-order smoothness assumptions\nby adding two constants Cθθ and Cθx, respectively. They denote the upper bound of the gradient\ndifference in the neighborhood at non-smooth point.\nThus, they quantify how drastically the\n(sub)gradients can change in a sufficiently small region in the parameter space.\nBased on Assumption C.1, we have the following corollary:\nCorollary C.2. If Assumption C.1 is satisfied, it holds\n∥Lϵ(x, θ1) −Lϵ(x, θ2)∥≤Aθ∥θ1 −θ2∥,\n(37)\n∥∇θLϵ(x, θ1) −∇θLϵ(x, θ2)∥≤Aθθ∥θ1 −θ2∥+ Bθδ + Cθθ + Cθx.\n(38)\nThe Lipschitz constant Aθ = 2 P\ni∈S+ yiLθ, Aθθ = Lθθ and Bθδ = Lθx∥δ1 −δ2∥+ 4Lθ where\nδ1 ∈arg maxδ∈SϵL(x + δ, θ1) and δ2 ∈arg maxδ∈SϵL(x + δ, θ2).\nThe proof is similar to that of Lemma 3.2 and 3.4. Corollary C.2 indicates a more craggy loss\nlandscape in the adversarial training of networks with non-smooth activations.\nAdditionally, the Theorem 4.2 can be easily extended to accommodate Assumption C.1.\nCorollary C.3. If Assumption C.1 holds, then we have\n∥∇θLϵ,α(x, θ1) −∇θLϵ,α(x, θ2)∥≤Aθθ∥θ1 −θ2∥+ B′\nθδ + Cθθ + Cθx.\n(39)\nThe Lipschitz constant Aθθ = Lθθ and B′\nθδ = αLθx∥δ1−δ2∥+2(1+α)Lθ where δ1 ∈arg maxδ∈SϵL(x+\nδ, θ1) and δ2 ∈arg maxδ∈SϵL(x + δ, θ2).\n19\n\n\nD\nDiscussion of the Upper Bound of ∥δ1 −δ2∥\nWe define the lp adversarial budget for the perturbation δ ∈Rd as S(p)\nϵ\n= {δ | ∥δ∥p ≤ϵ, 0 ≤\nx + δ ≤1}. Therefore, we have ∥δ1 −δ2∥p ≤2ϵ, and ∀i, 0 ≤|δ(i)\n1\n−δ(i)\n2 | ≤1 where δ(i)\n1\nand δ(i)\n2\nare the i-th element of δ1 and δ2, respectively. For convenience, we denote δ1 −δ2 as ∆δ and\nδ(i)\n1\n−δ(i)\n2\nas ∆δi in the following.\nAssume that ϵ ≪d for l0, l1 and l2 bounded perturbations, and ϵ ≪1 for the l∞bounded\nperturbation. Then, ∀q ≥1, we have\nl0 budget:\nX\ni\n|∆δi|q ≤2ϵ,\nl1 budget:\nX\ni\n|∆δi|q ≤D1 + (2ϵ −D1)q,\nl2 budget:\nX\ni\n|∆δi|q ≤D2 + (4ϵ2 −D2)\nq\n2 ,\nl∞budget:\nX\ni\n|∆δi|q ≤d × (2ϵ)q,\n(40)\nwhere D1 = ⌊2ϵ⌋and D2 = ⌊4ϵ2⌋. The derived upper bounds are tight because\n(1) l0 budget: The equality achieves when the location of non-zero elements in δ1 and δ2 has\nno overlap, and the magnitude of their non-zero elements reaches ±1.\n(2) l1 budget: Since 0 ≤|∆δi| ≤1, the equality achieves when there exists at most one ∆δk\nsuch that |∆δk| < 1 and ∀j ̸= k, |∆δj| = 1. The maximum number of ∆δj is ⌊2ϵ⌋. Then, according\nto ∥∆δ∥1 ≤2ϵ, we have |∆δk| = 2ϵ −1 × ⌊2ϵ⌋.\n(3) l2 budget: The derivation is similar to that of the l1 case.\n(4) l∞budget: The equality achieves when δ1 = −δ2.\nOn popular benchmark CIFAR-10, d = 32 × 32 × 3 = 3072, and the commonly used values of ϵ\nin the l0, l1, l2 and l∞cases are 360, 24, 0.5 and 8/255, respectively [5, 19, 23, 39]. Substitute these\ninto (40), we can easily get that ∀q ≥1, the upper bound of P\ni |∆δi|q is significantly larger in the\nl0 case than the other cases. For instance, (2ϵ −D1)q, (4ϵ2 −D2)\nq\n2 and (2ϵ)q reach their respective\nmaximum values when q = 1, since all of them are smaller than 1. Then, the upper bounds of\nP\ni |∆δi|1 in the l0, l1, l2 and l∞cases are 720, 24, 1 and 49152/255 ≈192.8, respectively.\nFurthermore, the lq norm of ∆δ is defined as follows:\n∥∆δ∥q =\n X\ni\n|∆δi|q\n! 1\nq\n.\n(41)\nSince the upper bound of P\ni |∆δi|q in the l0 case is larger than 1 for all q ≥1, we can also derive\nthat ∀q ≥1, the upper bound of ∥∆δ∥q is always significantly larger in the l0 case than the other\ncases.\nE\nMore Experimental Details\nE.1\nLocation Difference between Adversarial Examples Generated by\n1-step sPGD and sAA\nAs illustrated in Figure 4(a), the adversarial perturbations generated by one-step sPGD during\ntraining are almost completely different from those generated by sAA in location rather than\nmagnitude. Combining with the results in Table 2, we can demonstrate that CO in l0 adversarial\ntraining is primarily due to sub-optimal perturbation locations rather than magnitudes.\n20\n\n\n1.75\n1.80\n1.85\n1.90\n1.95\n2.00\n||\ntrain\nsAA||0 /\ntrain\n0.0\n0.1\n0.2\n0.3\n0.4\nProbability\ntrain = 20\ntrain = 80\ntrain = 120\n(a) Location difference\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n||\ntrain\ntest||0 /\ntest\n0.0\n0.1\n0.2\n0.3\n0.4\nProbability\ntrain = 20\ntrain = 80\ntrain = 120\n(b) Location overlapping\nFigure 4:\nVisualization of location difference and location overlapping.\n(a) The distribution of the\nnormalized l0 distance between training adversarial examples generated by 1-step sPGD and sAA. The\nmodels trained on 20-step sAT with different training ϵ are evaluated. (b) The distribution of the location\noverlapping rate between the perturbations generated by attacks used in training (20-step sPGD) and test\n(sAA), where ϵtest = 20. The models trained on 20-step sAT with different training ϵ are evaluated.\nE.2\nMulti-ϵ Strategy Mitigating Sub-optimality of Perturbation Loca-\ntions\nAs illustrated in Figure 4(b), the perturbations generated by 1-step attack with larger ϵtrain overlap\nmore with those generated by sAA with a smaller and fixed ϵtest in terms of location. Further-\nmore,the multi-ϵ strategy has been shown to be particularly effective in l0 adversarial training [23].\nThese findings suggest that the sub-optimality of perturbation locations brought by 1-step attacks\ncan be mitigated to some extent by multi-ϵ strategy.\nE.3\nDistances between Gradients Induced by 1-step and Multi-step At-\ntacks\nTable 5: Average l2 distances between gradients induced by 1-step and multi-step attacks, represented\nby ∥∇θLϵ(x + δone) −∇θLϵ(x + δmulti)∥2. The gradients are calculated of the training set of CIFAR-10\n[25].\nThe l0, l1, l2 and l∞models are obtained by 1-step sAT [23], Fast-EG-l1 [19], 1-step PGD [36]\nand GradAlign [33], respectively. The 1-step and multi-step l0 attacks are 1-step and 10000-step sPGD\n[23], respectively. The 1-step and multi-step l1 attacks are 1-step Fast-EG-l1 and 100-step APGD [39],\nrespectively.The 1-step and multi-step attacks for other norms are 1-step PGD [5] and 100-step APGD [3],\nrespectively.\nModel\nl0 (ϵ = 1)\nl1 (ϵ = 24)\nl2 (ϵ = 0.5)\nl∞(ϵ = 8/255)\nl2 distance\n15.8\n9.1 × 10−4\n3.6 × 10−4\n6.7 × 10−4\nBased on the Lipschitz smoothness assumption in Inequality (6), the gradient difference arising\nfrom approximated adversarial perturbations is bounded by Lθx∥δ1 −δ2∥where δ1 is the pertur-\nbation generated by 1-step attack and δ2 is the optimal perturbation. Based on the same reason\nthat l0 norm is not a proper norm, ∥δ1 −δ2∥is significantly larger in l0 cases than l∞, l2 and l1\ncases, which makes 1-step adversarial training more challenging in l0 cases. To corroborate this, we\ncompare the distance between gradients induced by 1-step and multi-step attacks. As presented\nin Table 5, the average distance between gradients induced by 1-step and multi-step l0 attacks is\n5 orders of magnitude greater than those in the l1, l2 and l∞cases, even when a single pixel is\nperturbed. This finding indicates that the loss landscape of l0 adversarial training is significantly\nmore craggy than other cases in the input space.\nE.4\nComparison with Other Baselines\nIn this section, we undertake a more comprehensive comparison between our proposed Fast-LS-l0\nand other baselines (ATTA [28], GradAlign (GA) [27], Fast-BAT [52], N-AAER [42], N-LAP [43],\nlabel smoothing (LS) [53], NuAT [16], AdvLC [44], MART [35] and AWP [54]), which either claim\n21\n\n\nTable 6: Comparison with other baselines in robust accuracy (%) by sAA. The target sparsity level ϵ = 20.\nWe compare PreAct ResNet-18 [24] models trained on CIFAR-10 [25] with 100 epochs. The italic numbers\nindicate catastrophic overfitting (CO) happens.\nMethod\nATTA\nATTA\n+ S&N\nGA\nGA\n+ S&N\nFast-BAT\nFLC\nPool\nN-AAER\nRobust Acc.\n0.0\n54.7\n0.0\n34.4\n14.1\n0.0\n0.1\nMethod\nN-LAP\nLS\nNuAT\nAdvLC\nMART\nOurs\n+ AWP\nOurs\nRobust Acc.\n0.0\n0.0\n51.9\n59.6\n48.0\n65.2\n63.0\nto mitigate catastrophic overfitting or claim to incorporate different smoothing techniques. Note\nthat all baselines are tuned through a hyperparameter search.\nAs demonstrated in Table 6, our method achieves the strongest robustness against sAA. First,\nnaive LS turns out ineffective under the l0 setting. The performance of Fast-BAT, NuAT, Ad-\nvLC and MART is not as good as the method we use. Second, FLC Pool, N-AAER, N-LAP,\nATTA and GradAlign suffer from CO, since they incorporate neither soft labels nor trade-off loss\nfunction. Combining ATTA and GradAlign with SAT and N-FGSM, which introduces soft labels,\ncan effectively mitigate CO, but these settings still underperform our method by a large margin.\nFinally, although our method also benefits from AWP, AWP introduces additional computational\noverhead, thereby not being adopted in our method.\nE.5\nMore Results of Section 5.2\nTable 7: Robust accuracy (%) of various models on different attacks that generate l0 bounded perturba-\ntions, where the sparsity level ϵ = 10. The models are PreAct ResNet-18 trained on CIFAR-100 [25] with\nϵ = 60. Note that the results of vanilla sAT and sTRADES are obtained from [23], CornerSearch (CS) is\nevaluated on 1000 samples due to its high computational complexity.\nModel\nTime\nCost\nClean\nBlack-Box\nWhite-Box\nsAA\nCS\nRS\nSAIF\nsPGDproj\nsPGDunproj\nMulti-step\nsAT\n4h 27m\n67.0\n44.3\n41.6\n60.9\n56.8\n58.0\n41.6\n+S&N\n4h 58m\n64.3\n53.0\n52.9\n61.2\n59.2\n59.6\n52.8\nsTRADES\n5h 10m\n70.9\n52.8\n50.3\n65.2\n64.0\n63.7\n50.2\n+S&N\n5h 40m\n63.8\n56.5\n55.6\n61.2\n60.5\n59.0\n55.3\nOne-step\nFast-LS-l0 (T)\n1h 05m\n65.3\n54.5\n54.3\n60.4\n55.6\n54.4\n52.2\nFast-LS-l0 (F)\n1h 26m\n65.0\n56.2\n54.6\n60.8\n54.9\n54.9\n52.3\nTable 8: Robust accuracy (%) of various models on different attacks that generate l0 bounded perturba-\ntions, where the sparsity level ϵ = 12. The models are PreAct ResNet-18 trained on GTSRB [49] with\nϵ = 72. All methods are evaluated on 500 samples, and CornerSearch (CS) is not evaluated here due to\nits high computational complexity.\nModel\nTime\nCost\nClean\nBlack-Box\nWhite-Box\nsAA\nCS\nRS\nSAIF\nsPGDproj\nsPGDunproj\nMulti-step\nsAT\n1h 3m\n98.4\n-\n43.2\n92.4\n96.0\n96.2\n43.2\n+S&N\n1h 2m\n98.4\n-\n77.8\n97.4\n96.8\n95.4\n77.6\nsTRADES\n1h 6m\n97.8\n-\n67.6\n94.0\n95.6\n95.0\n67.4\n+S&N\n1h 7m\n95.6\n-\n75.4\n93.6\n92.6\n91.2\n75.2\nOne-step\nFast-LS-l0 (T)\n7m\n97.8\n-\n75.2\n89.2\n74.4\n74.4\n63.2\nFast-LS-l0 (F)\n9m\n98.6\n-\n80.4\n94.2\n75.0\n79.8\n67.8\n22\n\n\nThe results on CIFAR-100 and GTSRB datasets are presented in Table 7 and 8, respectively.\nThe findings are consistent with those observed in Table 4(a), further validating the effectiveness\nof the proposed methods across different datasets. In contrast to the settings in [23], we resize the\nimages in GTSRB to 32 × 32 instead of 224 × 224 and retrain the models from scratch. The model\nare trained with ϵ = 72 and evaluated for robustness with ϵ = 12. It is important to note that\ndue to the smaller search space resulting from low-resolution images, the attack success rate of the\nblack-box Sparse-RS (RS) under this setting is significantly higher than that reported in [23].\nE.6\nStandard Deviation of Robust Accuracy against Sparse-AutoAttack\nof Table 4(a)\nTable 9: Average robust accuracy against sAA [23] obtained from three runs, where the sparsity level\nϵ = 20. The variances are shown in brackets. The configurations are the same as in Table 4(a). Note that\nwe do not include the results of vanilla sAT and sTRADES since their results are obtained from [23].\nModel\nsAT + S&N\nsTRADES + S&N\nFast-LS-l0 (T)\nFast-LS-l0 (F)\nAcc.\n61.2 (± 0.2)\n65.5 (± 0.7)\n63.0 (± 0.7)\n62.1 (± 0.6)\nTo better validate the effectiveness of our method, we report the standard deviations of robust\naccuracy against sAA in Table 9. We calculate these standard deviations by running the experi-\nments three times with different random seeds. The configurations are the same as in Table 4(a).\nIt can be observed that the fluctuation introduced by different random seeds does not outweigh\nthe performance gain from the evaluated approaches.\nE.7\nEvaluation on Different Networks\nTable 10: Robust accuracy (%) of various networks against sAA on CIFASR-10, where the sparsity level\nϵ = 20.\nThe networks are adversarially trained with different methods, including 1-step sAT, 1-step\nsTRADES and the proposed Fast-LS-l0.\nPRN-18\nConvNeXt-T\nSwin-T\n1-step sAT\n0.0\n0.8\n0.1\n1-step sTRADES\n31.0\n71.0\n43.2\nFast-LS-l0\n63.0\n78.6\n58.9\nDespite the effectiveness of our method on PreActResNet-18 (PRN-18) and ResNet-34, the\nperformance of our Fast-LS-l0 and its ablations on different networks remains unexplored.\nIn\nthis regard, we further evaluate our method on two popular architectures, i.e., ConvNeXt [55]\nand Swin Transformer [56]. Note that we adopt their tiny versions for CIFAR-10, which have a\nsimilar number of parameters as ResNet-18, and we follow the training settings of their CIFAR-10\nimplementations. The other experimental settings are the same as those described in Section 5.1.\nAs shown in Table 10, vanilla adversarial training results in CO on all networks, and our method\nproduces the best robust accuracy against sAA, demonstrating the effectiveness of our method\non different networks.\nNotably, ConvNeXt shows surprisingly strong robustness against sAA,\nsuggesting that advanced architecture design and dedicated hyperparameter tuning can provide\nadditional performance gains. However, as Transformers has struggled to perform well on small\ndatasets without pretraining [57], Swin Transformer also underperforms CNN-based networks in\nthis scenario.\nE.8\nLoss Landscape of one-step sAT with Different ϵ\nAs supplementary of Figure 2, we visualize the loss landscapes of 1-step sAT [23] with different ϵ,\nincluding 20, 40 and 120, in Figure 5. It can be observed that the l0 adversarial loss exhibits a\ndrastic increase in response to relatively minor alterations in the θ-space. Moreover, the degree of\nnon-smoothness increases in proportion to ϵ, which is consistent with the observation in Figure 2\n(a).\n23\n\n\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n10\n20\n30\n40\n50\n(a) L(0)\nϵ\n, ϵ = 20\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n20\n40\n60\n80\n(b) L(0)\nϵ\n, ϵ = 40\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n20\n40\n60\n(c) L(0)\nϵ\n, ϵ = 120\nFigure 5: Loss landscape of 1-step sAT [23] with different ϵ values on the training set of CIFAR-10 [25].\nThe architecture of the model is PreactResNet-18. (a) Landscape of L(0)\nϵ (x, θ +α1v1 +α2v2) with ϵ = 20,\nwhere v1 and v2 are the eigenvectors corresponding to the top 2 eigenvalues of the Hessian matrices,\nrespectively. (b) Landscape of L(0)\nϵ\nwith ϵ = 40. (c) Landscape of L(0)\nϵ\nwith ϵ = 120.\nE.9\nSmoother Loss Landscape Induced by Soft Label and Trade-off Loss\nFunction\n0\n2\n4\n6\n8\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nValue\n1e5\nA\nT(T)\nT(F)\nT(T)+S\nT(F)+S\n(a) Eigenvalues of ∇2\nθL(0)\nϵ\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n20\n40\n60\n(b) 1-step sAT\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n10\n20\n30\n40\n(c) 1-step sTRADES (T)\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n10\n20\n30\n40\n50\n(d) 1-step sTRADES (F)\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n5\n10\n15\n(e) 1-step sTRADES (T) + SAT\n1\n0.04\n0.02\n0.00\n0.02\n0.04\n2\n0.04\n0.02\n0.00\n0.02\n0.04\n loss\n0\n20\n40\n60\n5\n10\n15\n(f) 1-step sTRADES (F) + SAT\nFigure 6: Smoothness visualization of different methods with ϵ = 120 on the training set of CIFAR-10\n[25]. The architecture of the model is PreactResNet-18. (a) Top-10 eigenvalues of ∇2\nθL(0)\nϵ (x, θ) of different\nmethods. A and T denote 1-step sAT and 1-step sTRADES, respectively. T and F in the brackets are\ntwo respective versions of sTRADES indicated in Sec. 5.1. (b) Loss landscape of 1-step sAT. (c) Loss\nlandscape of 1-step sTRADES (T). (d) Loss landscape of 1-step sTRADES (F). (e) Loss landscape of\n1-step sTRADES (T) + SAT. (f) Loss landscape of 1-step sTRADES (F) + SAT.\nThe effectiveness of soft label and trade-off loss function in improving the performance of l0\nadversarial training is demonstrated in Section 5.1 and 5.2. Additionally, we visualize the curves\nof top-10 eigenvalues of Hessian matrices of the different methods discussed in Section 5.1 and\ntheir respective loss landscapes in Figure 6. Note that since N-FGSM results in a larger upper\nbound of ∥δ1 −δ2∥, it is not considered here to make a fair comparison.\nFigure 6 (a) shows\nthat sTRADES induces considerably smaller eigenvalues of Hessian matrices compared to sAT,\nwhile the difference between sTRADES (T) and sTRADES (F) is negligible. SAT, on the other\nhand, has only a marginal effect on the eigenvalues. However, as illustrated in Figure 6 (b)-(f),\nSAT plays a crucial role in smoothing the loss landscape, which relates to the change rate of loss,\n24\n\n\ni.e., the first-order smoothness. These observations align with the theoretical derivation presented\nin Section 4, indicating that soft label improves the first-order smoothness, while trade-off loss\nfunction contributes to the second-order smoothness.\nE.10\nAblation Studies\nIn this section, we conduct more ablation studies on the results in Section 5.1. Specifically, we\nfocus on the best configuration in Table 3: Fast-LS-l0 (T) (i.e., 1-step sTRADES (T) + SAT &\nN-FGSM). Unless specified, we adopt the same training settings as in Table 3.\nTable 11 presents a performance comparison of the model when SAT is enable in different\ntraining phases. We can see that the performance achieves the best when enabling SAT at the\n50-th epoch. This observation demonstrates that the best performance in 1-step sTRADES is\nachieved when SAT is enabled at the intermediate epoch where the learning rate is relatively low.\nIn Table 12, we compare the performance when using different labels, either the hard label\nfrom ground truth or the soft label by SAT, to generate adversarial perturbations for training.\nThe results indicate that using soft labels to generate adversarial perturbations results in slightly\nbetter performance compared to using hard ones.\nIn Table 13, we compare the performance when using different momentum factor in SAT. We\ncan see that the default setting in [38], i.e., 0.9, provides the best performance.\nIn Table 14, we compare the performance when using different balance factor β in TRADES.\nIt can be observed that β = 3 and 6 induce similar results, indicating the default setting in [34],\ni.e., 6, is the optimal.\nTable 11:\nAblation study on the epoch of en-\nabling SAT. The evaluated attack is sAA, where\nthe sparsity level ϵ = 20.\nSAT epoch\n30\n50\n70\nRobust Accuracy\n60.2\n63.0\n62.8\nTable 12: Ablation study on the labels used to gen-\nerate adversarial samples.\nThe evaluated attack is\nsAA, where the sparsity level ϵ = 20.\nLabel\nHard\nSoft\nRobust Accuracy\n62.6\n63.0\nTable 13: Ablation study on the momentum factor\nof SAT. The evaluated attack is sAA, where the\nsparsity level ϵ = 20.\nSAT momentum\n0.5\n0.7\n0.9\nRobust Accuracy\n55.4\n60.4\n63.0\nTable 14: Ablation study on the balance factor β\nin TRADES loss function. The evaluated attack is\nsAA, where the sparsity level ϵ = 20.\nTRADES β\n1\n3\n6\nRobust Accuracy\n58.7\n63.0\n63.0\nF\nImplementation Details\nGenerally, the epoch of enabling SAT is 1/2 of the total epochs. For N-FGSM, the random noise for\naugmentation is the random sparse perturbation with sparsity level ranging from 0 to 2ϵ, where\nϵ is the sparsity level of adversarial perturbations. The interpolation factor α in trade-off loss\nfunction is set to 0.75. The balance factor β in TRADES loss function is set to 6. The optimizer\nis SGD with a momentum factor of 0.9 and a weight decay factor of 5 × 10−4. The learning rate\nis initialized to 0.05 and is divided by a factor of 10 at the 1/4 and 3/4 of the total epochs. The\nspecific settings for different datasets are listed as follows:\n• CIFAR-10, CIFAR-100 [25] and GTSRB [49]: The adopted network is PreAct ResNet-18\n[58] with softplus activation [59]. The training batch size is 128. We train the model for 100\nepochs.\n• ImageNet-100 [48]: The adopted network is ResNet-34 [24]. The training batch size is 48.\nWe train the model for 50 epochs.\nUnless specified, the hyperparameters of attacks and other configurations are the same as in\n[23].\n25\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21041v1.pdf",
    "total_pages": 25,
    "title": "Fast Adversarial Training against Sparse Attacks Requires Loss Smoothing",
    "authors": [
      "Xuyang Zhong",
      "Yixiao Huang",
      "Chen Liu"
    ],
    "abstract": "This paper studies fast adversarial training against sparse adversarial\nperturbations bounded by $l_0$ norm. We demonstrate the challenges of employing\n$1$-step attacks on $l_0$ bounded perturbations for fast adversarial training,\nincluding degraded performance and the occurrence of catastrophic overfitting\n(CO). We highlight that CO in $l_0$ adversarial training is caused by\nsub-optimal perturbation locations of $1$-step attack. Theoretical and\nempirical analyses reveal that the loss landscape of $l_0$ adversarial training\nis more craggy compared to its $l_\\infty$, $l_2$ and $l_1$ counterparts.\nMoreover, we corroborate that the craggy loss landscape can aggravate CO. To\naddress these issues, we propose Fast-LS-$l_0$ that incorporates soft labels\nand the trade-off loss function to smooth the adversarial loss landscape.\nExtensive experiments demonstrate our method can overcome the challenge of\ncatastrophic overfitting, achieve state-of-the-art performance, and narrow down\nthe performance gap between $1$-step and multi-step adversarial training\nagainst sparse attacks.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}