{
  "id": "arxiv_2502.21154v1",
  "text": "HYPERGRAPH MULTI-MODAL LEARNING FOR EEG-BASED\nEMOTION RECOGNITION IN CONVERSATION\nZijian Kang1,⋆, Yueyang Li1,⋆, Shengyu Gong1, Weiming Zeng1,†,\nHongjie Yan2, Lingbin Bian3, Wai Ting Siok3, and Nizhuan Wang3,† ∗1\n1Lab of Digital Image and Intelligent Computation, Shanghai Maritime University, Shanghai 201306, China\n2Affiliated Lianyungang Hospital of Xuzhou Medical University, Lianyungang 222002, China\n3Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University, Hong Kong SAR, China\nABSTRACT\nEmotional Recognition in Conversation (ERC) is an important method for diagnosing health condi-\ntions such as autism or depression, as well as understanding emotions in individuals who struggle to\nexpress their feelings. Current ERC methods primarily rely on complete semantic textual informa-\ntion, including audio and visual data, but face challenges in integrating physiological signals such\nas electroencephalogram (EEG). This paper proposes a novel Hypergraph Multi-Modal Learning\nFramework (Hyper-MML), designed to effectively identify emotions in conversation by integrating\nEEG with audio and video information to capture complex emotional dynamics. Experimental results\ndemonstrate that Hyper-MML significantly outperforms traditional methods in emotion recognition.\nThis is achieved through a Multi-modal Hypergraph Fusion Module (MHFM), which actively models\nhigher-order relationships between multi-modal signals, as validated on the EAV dataset. Our pro-\nposed Hyper-MML serves as an effective communication tool for healthcare professionals, enabling\nbetter engagement with patients who have difficulty expressing their emotions.\nKeywords Emotion Recognition in Conversation · Hypergraph Learning · Multi-modal Feature Fusion · EEG · Audio ·\nVideo.\n1\nIntroduction\nEmotion Recognition in Conversation (ERC) holds significant potential for diagnosing health and mental conditions\nsuch as autism and depression. Recent research suggests that individuals with these conditions frequently exhibit\nunique communication challenges, including speech impairments, emotional disturbances, literal interpretation of\nquestions, and difficulty sustaining coherent dialogue [1]. Current research in ERC primarily focuses on textual analysis,\nsupplemented by visual cues like facial expressions and acoustic cues such as intonation and loudness [2]. These\nmethods typically rely on complete, uninterrupted texts or dialogue transcripts, integrating multi-modal data to provide\ncontext for individual utterances. Context modeling in ERC generally incorporates three key elements: 1) the content of\npreceding exchanges, 2) the timing of conversational turns, and 3) speaker-specific details such as identity and evolving\nemotional states [4, 5]. However, disruptions in the textual flow – such as fragmented sentences or missing dialogue\nsegments – can break the semantic structure and distort logical relationships between utterances. This incoherence\nreduces the accuracy of emotion recognition, limiting its practical applications in diagnosing and treating conditions\nlike autism and depression. To address these limitations, psychotherapists need alternative indicators that remain robust\neven when conversational data is imperfect.\nPhysiological signals – particularly electroencephalogram (EEG) data – provide a direct window into neural activity\nand emotional states, surpassing text-based methods in objectivity and immediacy [3, 18]. While textual analysis\n∗⋆: Zijian Kang and Yueyang Li are co-first authors. †: Weiming Zeng and Nizhuan Wang are corresponding authors. This work\nwas supported by the National Natural Science Foundation of China (No.31870979), the Hong Kong Polytechnic University Faculty\nReserve Fund (Project ID: P0053738), and the Hong Kong Polytechnic University Start-up Fund (Project ID: P0053210).\narXiv:2502.21154v1  [cs.HC]  28 Feb 2025\n\n\nPRIME AI paper\nEEG input\nspeaker\nVideo input\nAudio input\nUnimodality Encoders\nEmotion Classifier\nMulti-modal Hypergraph Fusion Module\n- I have an exciting \nplan for this weekend!\n- How about we bring \nour friends and  have a \nboard game night?\n-We can play all our \nfavorite games.\nConversation\nv1\ne v2\ne\nv1\na v2\na\nv1\nv v2\nv\nv1\ne\nv2\ne\nv1\na\nv2\na\nv1\nv\nv2\nv\ne1\nm\ne2\nm\ne1\nc\nv1\na\nv1\ne\nv2\ne\nv1\nv\nv1\ne\nv1\nv\nv1\na\nv1\ne\nv1\na\nv1\nv\n…\nHypergraph construction\nMulti-modal Hypergraph Fusion\n-vi\ne \n-vi\na \n-vi\nv\nHappy\nSad\nClam\nAngry\nNeutral\n5s clip\nGaussian \nnoise injection\nPrediction\nConcatenation\nFigure 1: Overall framework of the proposed Hyper-MML.\ndepends on extended linguistic context, EEG signals operate on shorter timescales, making them ideal for detecting\ntransient emotional shifts (e.g., sudden frustration or momentary joy) in real time. By integrating EEG signals with\ntext-based modalities, clinicians can address key limitations of language-driven approaches, such as distortions caused\nby fragmented or incomplete dialogue. EEG’s language-independent nature avoids language-related biases, enabling\nclearer and more objective emotion measurement. Furthermore, combining EEG with multi-modal data (e.g., audio and\nvideo) outperforms single-source EEG analysis, enhancing diagnostic accuracy [14]. This integrative framework allows\npsychologists to correlate physiological responses (e.g., brainwave patterns) with behavioural cues (e.g. voice tone,\nfacial expressions), constructing a comprehensive emotional profile. These insights support the development of tailored\ntreatment strategies that better address individual patient needs.\nIn multi-modal dialogue recognition tasks, a standard approach involves using graph neural networks (GNNs) to\nmodel interactions by capturing contextual and multi-modal data (e.g., text, audio, visual). However, GNNs face a\ncritical limitation: they can only model complex interactions by chaining together simple pairwise relationships (e.g.,\nbetween two nodes at a time). This sequential approximation of high-order relationships – such as group dynamics\nor multi-modal dependencies – often leads to suboptimal accuracy. Hypergraph theory overcomes this limitation\nby natively supporting high-order connections (e.g., linking three or more nodes simultaneously), enabling direct\nmodeling of intricate multi-modal interactions. For instance, a hyperedge could connect a speaker’s utterance, their\nfacial expression, and a listener’s reaction in a signle interaction step. This capability makes hypergraphs a more precise\nand efficient framework for multi-modal dialogue analysis [7].\nIn this study, we propose a novel Hypergraph Multi-Modal Learning framework (Hyper-MML) centered on EEG\nsignals, which has been validated as a state-of-the-art (SOTA) method on the EAV dataset [8]. Our framework advances\nERC through two key innovations:\n1) Hypergraph Multi-Modal Learning Framework (Hyper-MML): We introduce an end-to-end architecture that\nintegrates EEG signals with audio and visual data to model complex emotional dynamics (e.g., shifts between\nfrustration, surprise, or relief) in conversations. Unlike traditional text-centric approaches, Hyper-MML directly\nleverages physiological (EEG) and behavioral (audio-visual) cues, bypassing the limitations of language-based ambiguity\nor incomplete dialogue transcripts.\n2) Multi-modal Hypergraph Fusion Module (MHFM): A specialized module that enhances cross-modal interaction\n(EEG-audio-video) within hypergraph structures. This module employs adaptive weighted aggregation to dynamically\nprioritize the most informative modalities (e.g., emphasizing EEG during subtle emotional shifts or audio during\ntone-based cues). This strategy optimizes information propagation across modalities, significantly improving emotion\nrecognition accuracy.\n2\n\n\nPRIME AI paper\n2\nMethod\n2.1\nProblem Definition\nThe EEG-based ERC aims to infer the emotional state of each incomplete semantic segment of utterances. For each\ncomplete utterance, we segment it into N segments {s1, s2, ..., sN}, where each segment involves three sources of\nsegment-aligned data corresponding to three modalities: EEG (se\ni), audio (sa\ni ), and video (sv\ni ), represented as follows:\nsi = {se\ni, sa\ni , sv\ni }\n(1)\nThe objective of EEG-based ERC task is to predict the emotional category of each fragment si from a predefined set\nof C emotional categories, i.e., Happy, Sad, Calm, Angry and Neutral. Figure 1 illustrates the proposed Hyper-MML\nframework based on EEG-audio-video triplets. In general, the Hyper-MML consists of three key modules: Unimodality\nEncoders, Multi-modal Hypergraph Fusion Module and Emotion Classifier.\n2.2\nUnimodality Encoders\nEffectively capturing contextual information between utterance segments is challenging due to incomplete semantic\ndata. To address this, we propose extracting short-context-window embeddings within each of three modalities (EEG,\naudio, video) at the segment level. For EEG signals, which reflect instantaneous neural activity, our focus lies in\nextracting temporal features and dynamic patterns to capture rapid emotional shifts.\n1) Acoustic and Visual Embedding: For audio and video modalities, we used established fully connected networks\n[9, 10] as encoders. The short-context-aware feature encoding for each segment can be formulated as follows:\nva\ni = W1f a\ni + ba\ni , vv\ni = W2f v\ni + bv\ni\n(2)\nwhere f a\ni , f v\ni are the context-independent raw feature of segment i from the audio and video modalities, respectively.\nThe raw audio features f a\ni are extracted using the openSMILE toolkit with the IS10 configuration [9] from the\naudios, while the raw facial expressions features f v\ni are extracted using a pre-trained MA-NET [10] from the videos.\nThe unimodality encoder for each audio and video outputs the short-context-aware raw feature embedding va\ni , vv\ni\naccordingly.\n2) EEG Embedding: For EEG signals, to capture effective subject-specific information, we use a specialized EEG\nencoder NESTA that jointly learns subject-specific channel transformations and adaptively captures spectral patterns\nwhile preserving key temporal-spectral information within the EEG signals [11].\nve\ni = NESTA(sa\ni )\n(3)\n2.3\nMulti-modal Hypergraph Fusion Module (MHFM)\nCurrent approaches to ERC often simplify cross-modal interactions by modeling them as pairwise relationships\n(e.g., audio-text or video-text). In our study, our MHFM uses hypergraphs to directly capture complex higher-order\nrelationships (e.g., simultaneous EEG-audio-video dependencies), which better reflect the group dynamics of multi-\nmodal emotional cues. Furthermore, since each modality contributes uniquely to detecting instantaneous emotional\nshifts, we integrate learnable modality-specific weights. These weights are dynamically adjusted during training to\nprioritize the most informative modalities.\n1) Hypergraph Construction: Generally, a conversation with N utterance segments can be reformulated as a hypergraph\nH = (VH, EH), in which each node v ∈VH corresponds to a unimodal segment, and every hyperedge e ∈EH encodes\nmultimodal or contextual dependencies. Let I ∈R|VH|×|EH| represent the incident matrix, in which a nonzero entry\nIve = 1 indicates that the hyperedge e is incident with the node v; otherwise Ive = 0. Each segment is represented by\nthree nodes, i.e., ve\ni , va\ni , vv\ni , in the hypergraph, corresponding to EEG, audio and video modalities, respectively.\nTo capture relationships that extend beyond pairwise interactions in multi-modal emotion recognition based on utterance\nsegments, the complex multi-modal relationships of each utterance segment are constructed as hyperedges. Each node\nvx\ni (x ∈{e, a, v}) is connected to other modalities of the same utterance segment {vy\ni |y ̸= x, y ∈{e, a, v}}, forming\na multi-modal hyperedge em. Additionally, we connect EEG signals {ve\ni |i ∈[1, N]} from different segments of the\nsame utterance to create short-context hyperedges ec. The hyperedges EH are therefore divided into two subsets: the\nmulti-modal hyperedge set Em and the contextual hyperedge set Ec. This approach enables the constructed hypergraph\nto capture higher-order mutual information and contextual information between multi-modal data, thereby transcending\nthe limitations of pairwise interactions.\n3\n\n\nPRIME AI paper\nInspired by the hypergraph study of edge-dependent vertex weights [12], we set different node weights for multi-\nmodal hyperedges and context-dependent nodes, aiming to distinguish the contributions of modality nodes to different\nrelational patterns. Therefore, the edge-dependent vertex weights can be represented by a weighted incidence matrix:\nˆHij =\n\n\n\nγm(ej),\nif vi ∈ej and ej ∈Em;\nγc(ej),\nif vi ∈ej and ej ∈Ec;\n0,\notherwise.\n(4)\nin which γm(ej) is the multi-modal edge-dependent vertex weights, while γc(ej) is the context edge-dependent vertex\nweights. Analogously, the hyperedge weight matrix can be defined as follow:\nWe = diag(wm(e1), ..., wm(e|Em|), wc(e1), ..., wc(e|Ec|))\n(5)\nwhere wm(ei) and wc(ei) is the multi-modal hyperedge weight and the context hyperedge weight, respectively.\n2) Hypergraph Feature Fusion: Inspired by M 3Net [6], the core of MHFM involves a hypergraph convolution\noperation that propagates multivariate embeddings across the hypergraph. In this operation, we dynamically adjust the\nimportance of each modality through learnable weights, enabling the model to prioritize the most relevant features for\nemotion recognition tasks. This flexibility allows MHFM to adaptively capture the complex interactions and contextual\nrelationships inherent in multi-modal data.\nMathematically, the module updates the embeddings based on the aggregated features of neighboring nodes, enhancing\nthe representation of each modality while maintaining the integrity of the hypergraph structure. The formula for MHFM\nis as follows:\nV (l+1) = σ\n\u0010\nD−1\nH IWeB−1\nH ˆHT V (l)\u0011\n(6)\nin which V (l) = {vx\ni,(l)|i ∈[1, N], x ∈{e, a, v}} ∈RVH×EH is the input at layer l. σ is the non-linear activation\nfunction. DH ∈R|VH|×|VH| is the node degree matrix used to normalize node features. BH ∈R|EH|×|EH| and\nhyperedge degree matrix that reflects the connectivity of hyperedges. After performing L iterations, we get the outputs\nof the last iteration vx\ni,(L) as the multivariate representations:\n¯ve\ni = ve\ni,(L),\n¯va\ni = va\ni,(L),\n¯vv\ni = vv\ni,(L)\n(7)\nFinally, we concatenate the embeddings of three modalities to obtain the emotional embedding of the utterance segment\nas follow:\nei = ¯ve\ni ⊕¯va\ni ⊕¯vv\ni\n(8)\n3) Emotion Classification: The emotion classifier takes as input the concatenated multivariate representations to\nperform emotion prediction. Referring to prior works [5], we finally feed ei into a multilayer perceptron (MLP) with\nfully connected layers to predict the emotion label ˆyi for the segment:\nli = ReLU(Wlei + bl),\n(9)\nPi = softmax(Wsmaxli + bsmax),\n(10)\nˆyi = arg max\nτ (Pi[τ])\n(11)\nin which li is the output of the hidden layer after applying the ReLU activation function, Wl and bl are the weight\nmatrix and bias vector for the input layer, respectively. Pi denotes the probability distribution over the emotion classes,\nwith Wsmax and bsmax representing the weight matrix and bias vector for the output layer. Finally, ˆyi is the predicted\nemotion label for the utterance segment. We use categorical cross-entropy along with L2-regularization as the loss\nfunction during training, following the work [15]:\nL = −\n1\nPN\ns=1 c(s)\nN\nX\ni=1\nc(i)\nX\nj=1\nlog Pi,j[yi,j] + λ∥θ∥2\n(12)\nwhere N represents the number of dialogues, c(i) denotes the number of utterance segments within dialogue i.\nAdditionally, Pi,j refers to the probability distribution of class labels, while yi,j indicates the ground-truth label for\nsegment j in the dialogue i. The parameter λ is used as the weight for L2-regularization, and θ represents the parameters\nthat can be trained in the model.\n4\n\n\nPRIME AI paper\nTable 1: Accuracy and F1-score compared with baseline. * indicates significant improvement over AMERL (p < 0.05).\nMethod\nMethod\nAMERL\nOurs\nAMERL\nOurs\nsubject\nACC(%)\nF1(%)\nACC(%)\nF1(%)\nsubject\nACC(%)\nF1(%)\nACC(%)\nF1(%)\n1\n66.60\n-\n67.50\n67.15\n22\n76.37\n-\n81.67\n81.71\n2\n76.27\n-\n85.00\n84.89\n23\n64.63\n-\n78.33\n77.77\n3\n75.43\n-\n85.83\n86.06\n24\n85.13\n-\n85.83\n85.66\n4\n81.83\n-\n83.33\n82.17\n25\n67.73\n-\n74.17\n74.68\n5\n59.47\n-\n70.83\n69.62\n26\n68.57\n-\n70.83\n70.17\n6\n69.73\n-\n80.00\n79.58\n27\n83.87\n-\n85.83\n85.27\n7\n80.43\n-\n81.67\n81.44\n28\n81.17\n-\n83.33\n83.99\n8\n68.97\n-\n74.17\n74.80\n29\n62.53\n-\n69.17\n68.15\n9\n76.43\n-\n83.33\n83.18\n30\n56.10\n-\n80.83\n81.30\n10\n69.37\n-\n70.00\n66.16\n31\n64.30\n-\n84.17\n84.60\n11\n57.03\n-\n75.00\n74.46\n32\n63.83\n-\n65.00\n63.96\n12\n55.17\n-\n76.67\n75.81\n33\n80.10\n-\n83.33\n82.99\n13\n75.27\n-\n84.17\n84.12\n34\n68.20\n-\n70.83\n69.79\n14\n57.97\n-\n70.00\n68.71\n35\n62.97\n-\n67.50\n67.55\n15\n65.53\n-\n85.83\n85.76\n36\n74.23\n-\n74.17\n73.97\n16\n57.83\n-\n70.83\n70.17\n37\n61.83\n-\n62.50\n61.96\n17\n89.63\n-\n99.17\n99.17\n38\n76.27\n-\n85.83\n85.98\n18\n74.97\n-\n76.67\n76.44\n39\n71.50\n-\n76.67\n75.24\n19\n68.10\n-\n71.67\n71.94\n40\n66.90\n-\n70.83\n70.89\n20\n86.50\n-\n92.50\n92.46\n41\n72.33\n-\n76.67\n76.27\n21\n78.10\n-\n87.50\n87.04\n42\n77.10\n-\n78.33\n77.98\nAverage accuracy and F1-score of 42 subjects\n70.86\n-\n76.65*\n76.80\n3\nExperiments and Results\n3.1\nDataset and Experimental Setting\nThe recently released multi-modal dialogue emotion dataset, EAV [8], includes EEG data from 30 channels, audio\nrecordings, and facial expression videos from 42 subjects. This dataset represents the first publicly available collection\nthat integrates EEG, audio, and video in a conversational context. Each subject engaged in 200 interactions within\nprompt-based dialogue scenarios, eliciting five distinct emotions: Neutral, Anger, Happy, Sad and Calm. Each\ninteraction consisted of 20 seconds of listening followed by 20 seconds of speaking. For our evaluation, we focused\nexclusively on the speaking data of the subjects and followed the authors’ preprocessing methods, segmenting the\n20-second speech data stream into 5-second intervals. In addition, the proposed model is implemented using PyTorch\nand torch-geometric packages. The networks are trained with 1 NVIDIA GeForce RTX 3090. We use accuracy and\nF1-score as the metrics to measure the performance. Paired t-test is performed to test the significance of performance\nimprovement with a default significance level of 0.05. Models are trained using Adam [17] with a batch size of 16.\n3.2\nComparison with Baseline\nTo evaluate Hyper-MML, we compared our framework with the attention-based multi-modal emotion recognition\nframework (AMERL), which utilizes attention mechanisms to dynamically adjust the contributions of different\nmodalities and is currently the only framework that applies multi-modal physiological signals to emotion recognition in\nconversations [13]. Specifically, AMERL uses an attention mechanism to dynamically adjust the attention weights of\nvarious modalities, prioritizing the key features of each modality and adapting to different input sizes. We evaluated\nthe emotion classification accuracy and F1-score of 42 participants, and it is evident that our proposed Hyper-MML\nsignificantly outperformed the previous method on the EAV dataset, achieving new state-of-the-art (SOTA) records.\nThe corresponding results are presented in Table 1.\n3.3\nAblation Studies\nTo better demonstrate the rationale and effectiveness of the proposed model, we conducted an ablation study on the\nkey components of Hyper-MML, with the results presented in Table 2. First, we validated the effectiveness of EEG\nmodality compared to the text modality under conditions of incomplete semantic integrity in the utterance segments.\n5\n\n\nPRIME AI paper\nTable 2: Results of ablation experiments. * indicates significant improvement. E:EEG, T:Text, A:Audio, V:Video\nModality\nGraph Type\nAccuracy(%)\nF1-score(%)\n1\nT+A+V\nHypergraph\n68.83\n67.73\n2\nE+A+V\nGraph\n71.93\n72.93\nHyper-MML\nE+A+V\nHypergraph\n76.65*\n76.80*\nWe use speech recognition on the acoustic signals to obtain the transcribed text of the utterance segments. Subsequently,\nwe extracted the features of the raw text using a pre-trained RoBERTa large language model [16]. Our experiments\nshowed that the performance of EEG-based multi-modal recognition model outperformed that of the text-based multi-\nmodal recognition model in the emotion recognition task involving incomplete utterance segments. Additionally, we\nverified the effectiveness of our hypergraph fusion module by replacing it with a standard graph convolution module,\nwhich facilitates complex interactions between multiple modalities through several pairwise relationships. Under this\nconfiguration, we observed a decrease in average accuracy by 4.72% on the EAV dataset, and the F1-score similarly\ndropped by 3.97%. This demonstrates the effectiveness of hypergraph modeling to capture higher-order relationships\nbetween modalities and contextual elements.\n4\nConclusion\nIn this study, we introduced the Hypergraph Multi-Modal Learning framework (Hyper-MML) for EEG-based emotion\nrecognition in conversations, addressing the limitations of traditional methods that primarily rely on textual information.\nBy integrating EEG signals with audio and video data, our framework effectively captures the intricate emotional\ndynamics inherent in conversational interactions. The MHFM significantly enhances the model’s ability to process\nand integrate various modalities, leading to a more nuanced understanding of emotional states. Our experiments on\nthe EAV dataset demonstrate that the proposed framework not only improves classification accuracy but also sets new\nbenchmarks in emotion recognition. Future research should explore the generalizability of the Hyper-MML framework\nacross diverse datasets and real-world applications, such as mental health monitoring and human-computer interaction.\nReferences\n[1] Amaia Hervás. Autism and Depression: clinical presentation, evaluation and treatment. Medicina (Argentina),\n83(Suppl 2):37–42, 2023.\n[2] Soujanya Poria, Navonil Majumder, Rada Mihalcea, and Eduard Hovy. Emotion recognition in conversation:\nResearch challenges, datasets, and recent advances. IEEE access, 7:100943–100953, 2019.\n[3] Xiang Li, Yazhou Zhang, Prayag Tiwari, Dawei Song, Bin Hu, Meihong Yang, Zhigang Zhao, Neeraj Kumar, and\nPekka Marttinen. EEG based emotion recognition: A tutorial and review. ACM Computing Surveys, 55(4):1–57,\n2022.\n[4] Vishal Chudasama, Purbayan Kar, Ashish Gudmalwar, Nirmesh Shah, Pankaj Wasnik, and Naoyuki Onoe. M2fnet:\nMulti-modal fusion network for emotion recognition in conversation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 4652–4661, 2022.\n[5] Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin. MMGCN: Multimodal fusion via deep graph convolution\nnetwork for emotion recognition in conversation. arXiv preprint arXiv:2107.06779, 2021.\n[6] Feiyu Chen, Jie Shao, Shuyuan Zhu, and Heng Tao Shen. Multivariate, multi-frequency and multimodal:\nRethinking graph neural networks for emotion recognition in conversation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10761–10770, 2023.\n[7] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar.\nHypergcn: A new method for training graph convolutional networks on hypergraphs. Advances in Neural\nInformation Processing Systems, 32, 2019.\n[8] Min-Ho Lee, Adai Shomanov, Balgyn Begim, Zhuldyz Kabidenova, Aruna Nyssanbay, Adnan Yazici, and Seong-\nWhan Lee. EAV: EEG-Audio-Video dataset for emotion recognition in conversational contexts. Scientific data,\n11(1):1026, 2024.\n[9] Florian Eyben, Martin Wöllmer, and Björn Schuller. Opensmile: the munich versatile and fast open-source audio\nfeature extractor. In Proceedings of the 18th ACM international conference on Multimedia, pages 1459–1462,\n2010.\n6\n\n\nPRIME AI paper\n[10] Zengqun Zhao, Qingshan Liu, and Shanmin Wang. Learning deep global multi-scale and local attention features\nfor facial expression recognition in the wild. IEEE Transactions on Image Processing, 30:6544–6556, 2021.\n[11] Yueyang Li, Zijian Kang, Shengyu Gong, Wenhao Dong, Weiming Zeng, Hongjie Yan, Wai Ting Siok, and\nNizhuan Wang. Neural-mcrl: Neural multimodal contrastive representation learning for eeg-based visual decoding.\narXiv preprint arXiv:2412.17337, 2024.\n[12] Uthsav Chitra and Benjamin Raphael. Random walks on hypergraphs with edge-dependent vertex weights. In\nInternational Conference on Machine Learning, pages 1172–1181. PMLR, 2019.\n[13] Kang Yin, Hye-Bin Shin, Dan Li, and Seong-Whan Lee. Eeg-based multimodal representation learning for\nemotion recognition. arXiv preprint arXiv:2411.00822, 2024.\n[14] Yimin Zhao and Jin Gu. Feature fusion based on mutual-cross-attention mechanism for eeg emotion recognition.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 276–285.\nSpringer, 2024.\n[15] Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander Gelbukh, and Erik Cambria.\nDialoguernn: An attentive rnn for emotion detection in conversations. In Proceedings of the AAAI conference on\nartificial intelligence, volume 33, pages 6818–6825, 2019.\n[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n[18] Yueyang Li, Weiming Zeng, Wenhao Dong, Di Han, Lei Chen, Hongyu Chen, Hongjie Yan, Wai Ting Siok, and\nNizhuan Wang. A tale of single-channel electroencephalogram: Devices, datasets, signal processing, applications,\nand future directions. arXiv preprint arXiv:2407.14850, 2024.\n7\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21154v1.pdf",
    "total_pages": 7,
    "title": "Hypergraph Multi-Modal Learning for EEG-based Emotion Recognition in Conversation",
    "authors": [
      "Zijian Kang",
      "Yueyang Li",
      "Shengyu Gong",
      "Weiming Zeng",
      "Hongjie Yan",
      "Lingbin Bian",
      "Wai Ting Siok",
      "Nizhuan Wang"
    ],
    "abstract": "Emotional Recognition in Conversation (ERC) is an important method for\ndiagnosing health conditions such as autism or depression, as well as\nunderstanding emotions in individuals who struggle to express their feelings.\nCurrent ERC methods primarily rely on complete semantic textual information,\nincluding audio and visual data, but face challenges in integrating\nphysiological signals such as electroencephalogram (EEG). This paper proposes a\nnovel Hypergraph Multi-Modal Learning Framework (Hyper-MML), designed to\neffectively identify emotions in conversation by integrating EEG with audio and\nvideo information to capture complex emotional dynamics. Experimental results\ndemonstrate that Hyper-MML significantly outperforms traditional methods in\nemotion recognition. This is achieved through a Multi-modal Hypergraph Fusion\nModule (MHFM), which actively models higher-order relationships between\nmulti-modal signals, as validated on the EAV dataset. Our proposed Hyper-MML\nserves as an effective communication tool for healthcare professionals,\nenabling better engagement with patients who have difficulty expressing their\nemotions.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}