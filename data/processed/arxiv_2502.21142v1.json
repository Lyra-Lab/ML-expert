{
  "id": "arxiv_2502.21142v1",
  "text": "Multimodal Dreaming: A Global Workspace Approach to\nWorld Model-Based Reinforcement Learning\nL´eopold Mayti´e (leopold.maytie@univ-tlse3.fr)\nCerCo, CNRS UMR5549\nArtificial and Natural Intelligence Toulouse Institute\nUniversit´e de Toulouse\nRoland Bertin Johannet (roland.bertin-johannet@cnrs.fr)\nCerCo, CNRS UMR5549\nUniversit´e de Toulouse\nRufin VanRullen (rufin.vanrullen@cnrs.fr)\nCerCo, CNRS UMR5549\nArtificial and Natural Intelligence Toulouse Institute\nUniversit´e de Toulouse\nAbstract\nHumans leverage rich internal models of the world to\nreason about the future, imagine counterfactuals, and\nadapt flexibly to new situations. In Reinforcement Learn-\ning (RL), world models aim to capture how the envi-\nronment evolves in response to the agent’s actions, fa-\ncilitating planning and generalization. However, typical\nworld models directly operate on the environment vari-\nables (e.g. pixels, physical attributes), which can make\ntheir training slow and cumbersome; instead, it may be\nadvantageous to rely on high-level latent dimensions that\ncapture relevant multimodal variables. Global Workspace\n(GW) Theory offers a cognitive framework for multimodal\nintegration and information broadcasting in the brain, and\nrecent studies have begun to introduce efficient deep\nlearning implementations of GW. Here, we evaluate the\ncapabilities of an RL system combining GW with a world\nmodel. We compare our GW-Dreamer with various ver-\nsions of the standard PPO and the original Dreamer algo-\nrithms. We show that performing the dreaming process\n(i.e., mental simulation) inside the GW latent space allows\nfor training with fewer environment steps.\nAs an addi-\ntional emergent property, the resulting model (but not its\ncomparison baselines) displays strong robustness to the\nabsence of one of its observation modalities (images or\nsimulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improv-\ning decision-making in RL agents.\nKeywords:\nWorld Models; Global Workspace Theory; Re-\ninforcement Learning; Multimodal Representation Learning;\nMental Simulation\nIntroduction\nHumans possess the ability to anticipate the consequences\nof their actions before executing them in the real world. This\ncapacity suggests that humans construct an internal World\nModel (see e.g. Friston (2010); Clark (2013), among others).\nIn artificial intelligence (AI), this concept has been partic-\nularly applied in World Model-based reinforcement learning\n(RL), a subset of model-based RL. In model-based RL, tran-\nsition dynamics of the environment are traditionally specified\nas Markov decision processes (MDPs), either manually de-\nfined (Sutton, 1991; Atkeson & Santamaria, 1997) or empir-\nically estimated from interaction data (Dearden et al., n.d.;\nSzita & Szepesv´ari, n.d.). While model-based RL is generally\nmore sample-efficient than model-free RL, constructing accu-\nrate transition models remains challenging. Learning a World\nModel directly from data facilitates decision-making in environ-\nments where transition dynamics are either unknown or too\ncomplex to specify explicitly. This idea was popularized by\nHa & Schmidhuber (2018) and later extended by the Dreamer\nframework (Hafner et al., 2024), allowing agents to learn by\nperforming mental simulations of episodes rather than relying\nsolely on direct interaction with the environment.\nMore generally, AI research on world models has gained\nfurther momentum with the advent of large-scale World Foun-\ndation Models, such as Genie (Bruce et al., 2024) trained in an\nunsupervised manner, or World Foundation Models platforms\nlike Cosmos from Nvidia (NVIDIA et al., 2025)\nBeyond their ability to anticipate the consequences of their\nactions, humans perceive the world through multiple sensory\nmodalities, leading to a rich and robust representation of their\nenvironment.\nThe Global Workspace Theory (GWT), intro-\nduced by Baars (1988) and later expanded by Dehaene et al.\n(1998), provides a framework to explain such integrative cog-\nnitive processes. According to this theory, specialized mod-\nules compete to encode their information into a shared space\ncalled the Global Workspace. Through a broadcasting mech-\nanism, this information becomes accessible to various brain\nregions, shaping our conscious experience.\nTheoretical proposals have linked World Models and Global\nWorkspace Theory (GWT). VanRullen & Kanai (2021) align\nclosely with GWT, suggesting a World Model module that in-\nteracts with a shared representational space. Similarly, the\nIntegrated World Model Theory (IWMT) Safron (2022) seeks\narXiv:2502.21142v1  [cs.AI]  28 Feb 2025\n\n\nto unify these concepts by incorporating predictive and gen-\nerative mechanisms for structuring internal representations.\nWhile both emphasize information integration, GWT focuses\non selective broadcasting for decision-making and awareness,\nwhereas IWMT prioritizes constructing an internal model for\nprediction and planning. Inspired by these frameworks, our\nwork explores multimodal integration and predictive mecha-\nnisms without committing to (or rejecting) their assumptions\nabout consciousness.\nIn this paper we introduce a system bridging the ideas\nof World Model and Global Workspace (VanRullen & Kanai,\n2021; Safron, 2022). We took inspiration from the architecture\nproposed in Dreamer algorithms (Ha & Schmidhuber, 2018;\nHafner et al., 2024) and extended the Global Workspace im-\nplementation proposed by Devillers et al. (2024) to implement\nour Global Workspace Dreamer (GW-Dreamer). The key orig-\ninality of GW-Dreamer is that it learns to represent the World-\nModel transitions using multimodal GW representations. We\ncompared our model in two different Reinforcement Learning\nenvironments against standard PPO and the original Dreamer\nalgorithm as well as a variant of Dreamer that shared the\nsame visual input module as GW-Dreamer. Thanks to its ef-\nficient GW multimodal latent representation, our model learns\nwith fewer environment interactions; in addition, it proves more\nrobust to missing modalities (as already shown in the context\nof model-free RL by Mayti´e et al. (2024)).\nModel\nIn this study, we consider RL environments with multimodal\nobservations. By consequence, the state of an environment\nat time t leads to multiple observations ot ∈O, which can be\neither an RGB image ov\nt , or an attribute vector oattr\nt\ndescribing\nphysical attributes of the simulation. From these observations,\nthe agent predicts an action at ∈A to interact with the envi-\nronment, leading to a reward rt+1.\nTo interact with these multimodal environments we propose\na model composed of three main components: a representa-\ntion model, called Global Workspace, a World Model and an\nActor-Critic RL policy. The training process consists of two\nmain steps. First, the Global Workspace is trained to repre-\nsent the multimodal environment using a dataset of environ-\nment observations collected randomly or via an expert agent\n(Figures 1,2). Then, the World Model and the Actor-Critic are\ntrained through interaction with the environment (Figure 3). In\nthe following subparts, we provide a detailed description of the\narchitecture and training procedure for each component.\nGlobal Workspace\nThe Global Workspace serves as a representation model,\nencoding multiple modalities into a unified latent representa-\ntion.\nThis latent representation is then used by the World\nModel to encapsulate the agent’s perception of the environ-\nment at time t. Our proposed Global Workspace is inspired by\nthe approach introduced by Devillers et al. (2024). However,\nwe modify both the architecture and training procedure to pro-\nduce a single unified representation, rather than maintaining\nseparate representations for each modality. This architecture\nand its training losses are illustrated in Figure 1.\nAs proposed by VanRullen & Kanai (2021) and imple-\nmented by Devillers et al. (2024), we do not train our set of\nencoders and decoders directly from raw modalities. Instead,\nwe employ two pretrained and frozen modules (in this case,\nVAEs) to transform raw representations (denoted ov for im-\nages and oattr for attributes) into unimodal latent represen-\ntations (uv and uattr).\nThese unimodal representations are\nthen encoded into pre-fusion latent variables zv = ev(uv) and\nzattr = eattr(uattr). However, in contrast to the model proposed\nby Devillers, we do not always directly decode from these pre-\nfusion representations. We combine them using element-wise\nweighted sum followed by a Tanh activation function to form a\nunified representation denoted z. From this unified represen-\ntation, we can recover the unimodal representations through\na set of decoders dv and dattr.\nThe GW model is trained using two loss functions: the con-\ntrastive loss and the broadcast loss.\nThe contrastive loss\nLcont follows a similar formulation to the one proposed in CLIP\n(Radford et al., 2021); it is designed to align the representa-\ntions zv and zattr before fusion, supporting the development of\namodal representations in the Global Workspace. The broad-\ncast loss is inspired by the broadcast principle at the heart of\nGWT; it is computed by comparing the predicted ( ˆuv, ˆuattr)\nand ground-truth unimodal representations (uv, uattr) using\nthe mean squared error (MSE). Specifically, it consists of a\nweighted sum of multiple sub-losses, including the cycle loss\n(Lcy), demi-cycle loss (Ldcy), and translation loss (Ltr), as\nintroduced by Devillers et al. (2024).\nAdditionally, a fusion\nloss (Lfusion) is incorporated. The primary objective of these\nlosses is to ensure that the unimodal latent vectors can be ac-\ncurately reconstructed after the weighted-sum fusion of pre-\nGlobal Workspace representations (zv and zattr), regardless\nof the exact fusion weights employed. The weighting factors\nαattr and αv are adjusted for each specific sub-loss (and with\nthe constraints αv ≥0,αattr ≥0,αv + αattr = 1), as defined\nbelow. (For an in-depth discussion of the usefulness of each\nloss term Lcont, Lcy, Ldcy and Ltr, see Devillers et al. (2024)).\n∀(x,y) ∈{attr,v},\nx ̸= y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLdcy = ∥dx(tanh(ex(ux)))−ux∥2\n2,\n(αx=1,αy=0)\nLcy = ∥dx(tanh(ey(dy(tanh(ex(ux))))))−ux∥2\n2,\n(αx=1,αy=0), then (αx=0,αy=1)\nLtr = ∥dy(tanh(ex(ux)))−uy∥2\n2,\n(αx=1,αy=0)\nLfusion = ∥dx(tanh(αx.ex(ux)+αy.ey(uy)))−ux∥2\n2,\n(αx>0,αy>0,αx+αy=1)\n(1)\nLbroad = βdcy.Ldcy +βcy.Lcy +βtr.Ltr +βfusion.Lfusion\n(2)\n\n\nov\noattr\nGW\nz\ndv\ndattr\nev\neattr\nuattr\nûv\nuv\nzv\nzattr\nûattr\nαv\nαattr\nVAE\nVAE\nLcont\n[0 1 0]\n8\n8\n...\n6\nFigure 1: Overview of the Global Workspace model for multimodal representation. Raw environment inputs (image pixels, simu-\nlation attributes) are encoded in their latent unimodal representation (uv or uattr) thanks to pretrained (and frozen) VAEs. These\nunimodal latent representations are then processed by encoders ev and eattr (respectively) to produce pre-GW representations\n(zv and zattr). The final Global Workspace representation z ∈Z is obtained by fusing these pre-GW representations through an\nelement-wise weighted sum (with weights αv ≥0 and αattr ≥0, αv + αattr = 1) followed by a Tanh activation. The unimodal\nlatent vectors can be retrieved from z with a set of decoders dv and dattr. The GW component networks ev, eattr, dv and dattr\nare trained by combining a contrastive loss Lcont and a broadcast Lbroad. The former encourages the pre-GW representations\nto align across modalities; the latter also promotes this objective (see Devillers et al. (2024)), and ensures that decoded or\n“broadcasted” GW representations resemble the original unimodal latent representations, regardless of each modality’s initial\ncontribution to the GW representation (as captured by the fusion weights αv and αattr). Once trained, the GW model is frozen\nfor learning the World Model and RL policy (Figure 3), and the fusion weights are fixed (αv = αattr = 0.5).\nThe full training objective of the Global Workspace is a\nweighted sum of the contrastive loss and the broadcast loss,\nas shown below. The weight of the broadcast loss is fixed at\none (its overall contribution can be adjusted by modifying the\nindividual weights that constitute it: βdcy,βcy,βtr,β fusion).\nLGW = βcont.Lcont +Lbroad\n(3)\nFigure 2 illustrates the functional properties of the trained\nGW when processing multimodal inputs. Here, for illustration\npurposes, two images (left) are chosen to differ from the at-\ntributes vector (right) along the color and size dimensions. By\nmodulating the fusion coefficients αv and αattr, the GW can\nperform distinct functional operations. For instance, it can per-\nform attribute-to-image translation by setting the fusion coef-\nficient to αv = 0 and αattr = 1. This configuration ensures\nthat only the attributes representation propagates through the\nGW, while visual information is disregarded. The following de-\ncoding step (using dv and the visual VAE), reconstructs an\nimage that is inferred purely from the attribute representation.\nThe results, shown above the ”tr” label in Figure 2, confirm\nthat the reconstructed image remains unaffected by the sup-\npressed visual input. A second operational mode, referred to\nas a demi-cycle, is illustrated in the “dcy” section of Figure\n2. Here, the GW selectively propagates only the visual repre-\nsentation by setting αv = 1 and αattr = 0. The reconstructed\nimages exclusively reflect the visual inputs. Beyond unimodal\nprocessing, the GW also supports multimodal fusion, enabling\nthe integration of heterogeneous information streams. In the\n”fusion” section of Figure 2, both modalities are encoded into\nthe GW with equal weighting (αv = 0.5,αattr = 0.5), allow-\ning information from both sources to be jointly encoded in the\nshared latent space. Decoding this fused representation re-\nsults in an image that integrates features from both the orig-\ninal visual input and attributes: the reconstructed color and\nsize are halfway between those of the attributes and image in-\nputs. Thus, by appropriately tuning the fusion coefficients fol-\nlowing encoding (ev,eattr) and by selecting the appropriate de-\ncoding pathway (dv or dattr), one can dynamically reconfigure\nthe functional role of the GW. Specifically, it can transition be-\ntween unimodal reconstruction, cross-modal translation, and\nmultimodal fusion, providing a flexible framework for integrat-\ning and transforming heterogeneous information sources.\nWorld Model\nThe World Model (WM) is a fundamental component that\nenables to train the RL agent through a mental simulation or\n“dreaming” process. At each time step t, this recurrent net-\nwork (with internal hidden state ht) receives as input the GW\nrepresentation (zt) and an action at, from which it predicts the\n\n\nshape\nposition\nangle\nsize\n(R,G,B)\nvalues\nattributes\n[0, 1, 0]\n[10, 15]\n2.10\n7\n(243,57,82)\nαv \n1\nαattr\n0\ndcy\nαv \n0\nαattr\n1\ntr\nz\nev\neattr\ndv\nVAE\nVAE\nfusion\nαv \nαattr\n0.5\n0.5\nFigure 2: Illustration of the behaviour of the GW. We start from a fixed attribute vector describing a small red egg-shape (top\nright), and two images (top left) that are chosen to differ in terms of color (right-most image) or both color and size (left-most\nimage). These inputs are encoded into the GW using different fusion weights αv and αattr, indicated in green below each\nconfiguration, and subsequently decoded into an image. The resulting images at the bottom illustrate three distinct functional\nmodes of operation. In the translation mode (tr, bottom right), both modalities are encoded, but only attribute information is\ntransmitted through the GW, while visual input is disregarded. The reconstructed images, obtained by decoding the GW latent\nvector z as an image using dv and the visual VAE, demonstrate the successful translation of attribute information into the visual\ndomain. In the demi-cycle mode (dcy, bottom left), both modalities are encoded, but only the visual information is propagated\nthrough the GW. The absence of distortions due to attributes information in the reconstructed images confirms that attribute\ninformation was effectively suppressed. In the fusion mode (bottom middle), both modalities are encoded with equal weights,\nallowing information from both sources to be integrated inside the GW. The decoded images reflect a hybrid representation of\nvision and attributes features, resulting in an intermediate color and size.\nGW representation at the next time step (zt+1), the reward\nassociated with the action (rt+1), and a Boolean termination\nsignal (dt+1) indicating whether the task is complete.\nThe\nGW representation is computed from environmental observa-\ntions using the pre-trained (and frozen) GW model described\nabove. The model employs a Gated Recurrent Unit (GRU)\n(Cho et al., 2014) for recurrence, while the prediction heads\nconsist of a set of Multi-Layer Perceptrons (MLPs).\nThe World Model is trained on data collected from the en-\nvironment using the current policy of the Actor (see below),\nwhile keeping the GW model frozen. Its objective is to predict\nthe GW representation, reward, and termination flag at the\nnext time step (t+1). The corresponding loss function (LWM)\nis computed as a weighted sum, combining MSE for the pre-\ndicted GW representation and reward, with Binary Cross-\nEntropy (BCE) for the termination flag. This is illustrated in\nFigure 3 (1).\nActor-Critic\nThe Actor policy is learned concurrently with the World\nModel in alternating steps, as detailed below. Initially, n data\npairs (ot, at, ot+1, rt+1, dt+1) are collected through interaction\nwith the environment and stored in a replay buffer. Once the\ndata is gathered, m learning steps are performed. The learn-\ning alternates between training the World Model as described\npreviously and the Actor-Critic network. The Actor-Critic takes\nas input the internal representation from the GRU, ht, and pre-\ndicts both the action at and the state value vt. This approach\nclosely follows the methodology proposed in Dreamer (Hafner\net al., 2024). It learns from ”mental simulations” or ”dreaming”\ninstead of interacting directly with the environment.\nDuring\ntraining, the observations are provided only at the first step.\nFor the following ones, the World Model simulates the environ-\nment for a predetermined number of steps, using the actions\npredicted by the Actor, without access to true observations,\nas shown in Figure 3 (2). During AC training, the gradient\ndoes not propagate through the World Model, ensuring that\nthe learned policy does not directly influence the internal dy-\nnamics of the World Model (and vice-versa, WM training gra-\ndients do not propagate to the AC network). The simulated ac-\ntions and values, along with the predicted rewards and done\nsignals, are used in the loss function of the Actor-Critic net-\nwork (following the standard Actor-Critic algorithm (Konda &\nTsitsiklis, 1999) modified in Dreamer (Hafner et al., 2024)).\nThis entire training procedure is described explicitly in Algo-\nrithm 1.\n\n\n(1) World Model training\n(2) Actor Critic training\nzt\nẑt+1\nAC\nat+1\nvt+1\nWM\nht\nht+1\nWM\nAC\nat\nvt\nẑt+2\nov\nt\noattr\nt\nGW\nVAE\nVAE\nzt\nat\nẑt+1\nat+1\nzt+1\nrt+1\ndt+1\nWM\nht\nht+1\nWM\nov\nt\nrt+1\n^\ndt+1\n^\noattr\nt\nov\nt+1\noattr\nt+1\nLWM\nGW\nVAE\nVAE\nGW\nVAE\nVAE\n(\n(\n(\n(\nFigure 3: (1) World Model training: At each time step, the environment provides observations (ov\nt , oattr\nt\n), a reward rt, and a\ntermination signal dt. A pretrained and frozen Global Workspace (GW) model, incorporating a Variational Autoencoder (VAE)\nfor each modality, encodes observations into a GW representation zt. The WM is trained on sequences of data collected from\nthe environment using the current AC policy. Given zt and the action at predicted by the policy, the WM (implemented as a\nGRU: Gated Recurrent Unit) updates its internal state from ht to ht+1. Using this updated state, the WM predicts the next\nGW representation zt+1, the expected reward rt+1, and the termination signal dt+1 with three separate prediction heads. The\nloss function LWM is computed as a weighted sum of the Mean Squared Error (MSE) for zt+1 and rt+1, and the Binary Cross-\nEntropy (BCE) loss for predicting dt+1. (2) Actor-Critic training: The AC model is trained using ”mental simulation”. The GW\nrepresentation zt derived from observations is provided only at the first time step. For subsequent steps, the WM generates\nnovel states by processing the previously predicted GW representation and the action selected by the AC. The AC loss functions\nare computed exclusively from the predicted elements within the simulated trajectory, including the generated termination signal\nˆd, reward ˆr, and actions taken based on the latent state h.\nSimple Shapes Environment\nThe different models were tested in an environment called\n‘Simple Shapes’. This environment was introduced in Dev-\nillers et al. (2024) as a fixed dataset, and in\nMayti´e et al.\n(2024) as an RL environment.\nThe Simple Shapes environment is multimodal, the agent\ncan receive two types of observations: 32×32 pixel RGB im-\nages of a 2D shape on a black background, or a set of eight at-\ntributes directly describing the environment’s state (Figure 4).\nThere are three different types of shapes, an egg-like shape,\nan isosceles triangle, and a diamond. The shapes possess\ndifferent properties: a size s ∈[smin,smax], a position (x,y) ∈\n[ smax\n2 ,32 −smax\n2 [2, a rotation θ ∈[0,2π[ and an HSL color\n(ch,cs,cl) ∈[0,1]2 × [lmin,1].\nThe agent does not observe\nthese properties directly, but instead receives transformed at-\ntributes as observations: the rotation angle θ is decomposed\ninto (cθ,sθ) = (cos(θ),sin(θ)); HSL colors are translated to\nthe RGB domain, finally, the shape variable is expressed as a\none-hot vector of size three, and all variables are normalized\nbetween -1 and 1.\nAt the beginning of each episode, attributes are randomly\nsampled within their respective domains; the starting point is\nthus a random shape of a random orientation, located some-\nwhere in the image. The agent’s goal is to move the shape to\nthe center of the image and align it to point to the top. For this\npurpose, six different actions are available to the agent: mov-\ning the shape by one pixel in cardinal directions (left, right, up,\nor down) and rotating the shape by an angle of π\n32 clockwise\nor anti-clockwise. The reward is initialized at zero. At each\ntimestep, the reward is equal to minus the current distance\n(in pixels) between the shape’s position and the image center\nminus the smallest angle (in radians) between the shape’s ori-\nentation and the null angle times a rotation reward coefficient\nequal to 10 by default. The episode ends when the shape\nreaches the goal state, with no additional reward.\nResults\nWe evaluated the performance of the GW-Dreamer model\nagainst different PPO and Dreamer variants. GW and VAE\ncomponents were identical across all models and were pre-\ntrained using data randomly sampled from the environment.\nSpecifically, we trained: (1) a standard PPO model using only\nvisual inputs (ov), (2) a ”VAE-PPO” that uses concatenated\nrepresentations from both attribute and image VAEs, and (3)\na ”GW-PPO” that employs a single input representation com-\ning from the GW. This model is expected to have certain ad-\nvantages relative to the previous two baselines, owing to its\nstrong multimodal abilities (as shown by Mayti´e et al. (2024)),\n\n\nAlgorithm 1 GW-Dreamer Training Procedure\n1: Require Pretrained Global Workspace (GW)\n2: Initialize World Model (WM), Actor-Critic (AC), Replay\nBuffer\n3: while not max number environment steps do\n4:\nCollect n transitions (ot,at,ot+1,rt+1,dt+1) from the\nenvironment and store in Replay Buffer\n5:\nfor m training steps do\n6:\nTrain World Model:\n7:\nTransform Replay Buffer observations (ov\nt ,oattr\nt\n)\ninto GW latent vectors (zt)\n8:\nPredict next GW representation, reward and\ndone signal: WM(zt,at) = ( ˆdt+1, ˆrt+1, ˆzt+1)\n9:\nCompute\nLWM\nby\ncomparing\nagainst\n(dt+1,rt+1,zt+1)\n10:\nUpdate WM by backpropagating LWM\n11:\nTrain Actor-Critic:\n12:\nEncode the first observation through GW to get\nlatent representation zt\n13:\nGenerate imagined trajectories using WM and\nthe actions predicted by AC\n14:\nUpdate AC using simulated rewards and transi-\ntions\n15:\nend for\n16: end while\nActions :\nPlace shape at the center pointing to the top\nGoal :\nshape\nposition\nangle\nsize\n(R,G,B)\nvalues\nattributes\n[1, 0, 0]\n[4, 11]\n1.25\n9\n(220,12,20)\nFigure 4: Illustration of the Simple Shapes environment and\nthe task used in this study.\nThe figure presents examples\nof raw observations, including four example images (left) and\none example set of attributes (right). The agent’s goal is to\nplace the shape at the center and pointing upward. The agent\ncan move the shape one pixel at a time in four directions (up,\ndown, left, right) or rotate it clockwise or counterclockwise.\nbut it does not include a World Model. Additionally, we trained\n(4) the standard Dreamer algorithm using both attribute and\nimage modalities and (5) a ”VAE-Dreamer” model that re-\nceives VAE-based representations of attributes and images\nas inputs. The motivation for incorporating latent representa-\ntions (VAEs) was to enable the models to operate in a fully\nlatent space, reducing the high compute associated with re-\nconstructing images through decoders. At the same time, this\nmade the underlying architecture closer to GW-Dreamer, ex-\ncept for the absence of a Global Workspace.\nAll models were trained in the Simple Shapes environment,\nand the results are illustrated in Figure 5. Returns (cumulative\nsum of rewards) were normalized such that a return of zero\ncorresponds to the performance of a random agent.\nAddi-\ntionally, a “return criterion” was defined as 75% of the highest\nsmoothed reward obtained by any of the models in the figure.\nWe verified visually that this criterion corresponds to a level of\nreturn at which the task begins to be solved efficiently.\nFigure 5 presents several key findings. First, when com-\nparing different PPO variants, we observe that both mul-\ntimodal PPO models (VAE-PPO and GW-PPO) reach the\nreturn criterion significantly earlier than the standard PPO\nmodel. While the standard PPO model meets the criterion\njust before 1,000,000 steps, VAE-PPO reaches it at 400,000\nsteps, and GW-PPO at only 200,000 steps. This suggests that\nincorporating multimodal latent representations can greatly\naccelerate policy learning, with GW representations yielding\neven faster convergence than VAE representations.\nSecond, when comparing Dreamer-based models to PPO-\nbased ones, Dreamer models generally reach the return cri-\nterion earlier, corroborating previous findings that world mod-\nels improve sample efficiency Hafner et al. (2024). In addi-\ntion, GW-PPO exhibits sample efficiency comparable to both\nstandard Dreamer and VAE-Dreamer, with all three models\nreaching the threshold at approximately 200,000 environment\nsteps. This suggests that a strong multimodal representation\n(GW) can bring as much to the model’s efficiency as a World\nModel could. Can the two advantages (GW and World Model)\nbe combined to yield an even more efficient architecture?\nThis is what Figure 5 seems to suggest: the GW-Dreamer\nmodel outperforms all other models, reaching the criterion\nin just 20,000 steps, i.e.\nabout 10X faster than standard\nDreamer and VAE-Dreamer.\nSince both GW-Dreamer and\nVAE-Dreamer operate in a multimodal latent space, this result\ndirectly highlights the effectiveness of the GW representation\nin improving sample efficiency within a world-model frame-\nwork. These findings suggest that the GW representation en-\nables more efficient learning when combined with a dreaming-\nbased approach.\nOne advantage of training a policy from two input modali-\nties (images and attributes) is that the resulting agent could\nprove particularly robust in conditions where one of the two\nmodalities becomes noisy or unreliable. We thus conducted\nan additional experiment to evaluate the zero-shot robustness\nof GW-Dreamer compared to other multimodal variants when\none sensory modality is removed.\nThis scenario simulates\nreal-world conditions where a robot may experience sensor\nfailure or a human may lose one of their senses. Once the\nmodels were trained, their parameters were frozen, and we\nsystematically removed either the attribute or image inputs.\nFor models using a GW representation, the fusion mecha-\n\n\n104\n105\n106\nEnvironment Steps\n1500\n1000\n500\n0\n500\n1000\n1500\n2000\n2500\nSmoothed Return\nPPO\nVAE-PPO\nGW-PPO\nDreamer\nVAE-Dreamer\nGW-Dreamer\nreturn criterion\nFigure 5: Performance (cumulative sum of rewards or “return”) as a function of the number of environment steps (log scale)\nduring training. A fixed baseline, corresponding to the performance of a fully random policy, was subtracted from the episode\nreturns. Thus, a random policy’s performance is equal to zero. The returns are smoothed using a sliding window of length 10,\nwith the shaded region indicating the standard error of the mean over this window. The return criterion is defined as 75% of the\nmaximum smoothed return. It corresponds (as verified visually) to a performance at which the task starts to be solved properly.\nnism was adjusted accordingly: if attributes were removed, full\nweight was assigned to the visual modality (αv = 1,αattr = 0),\nand conversely, if vision was removed the opposite adjustment\nwas made (αv = 0,αattr = 1).\nThe results, shown in Figure 6, reveal a striking contrast in\nthe robustness of the models. Models using other represen-\ntations than a GW (VAE-PPO, VAE-Dreamer and Dreamer)\nshowed a complete performance collapse when a modality\nwas removed, indicating a strong dependence on both sen-\nsory inputs. In contrast, models using a GW representation\n(GW-PPO and GW-Dreamer) were the only models to main-\ntain performance above the return criterion and remaining\nclose to their original performances. This demonstrates that\nGW-Dreamer is capable of solving the task even when one\nmodality is missing, highlighting the importance of a multi-\nmodal representation like GW that effectively integrates dif-\nferent sensory inputs.\n(GW-PPO also benefited from this\nGW representation, but was less sample-efficient than GW-\nDreamer).\nThese findings underscore the advantages of combining\nthe Global Workspace framework with a World Model.\nBy\nleveraging this integration, RL algorithms can be trained effi-\nciently while maintaining robustness to modality perturbations,\nmaking them more suitable for real-world applications where\nsensory failures may occur.\nDiscussion and Conclusion\nThis paper represents a first step in bridging Global\nWorkspace Theory and World Models in AI. It builds upon\nthe architecture proposed by VanRullen & Kanai (2021) and\nimplemented by Devillers et al. (2024), adapting it to be com-\npatible with World-Model-based reinforcement learning algo-\nrithms such as Dreamer (Ha & Schmidhuber, 2018; Hafner et\nal., 2024).\nHowever, some limitations remain. In terms of absolute per-\nformance (measured by return), GW-Dreamer slightly under-\nperforms compared to the original Dreamer algorithm. This\ndifference is somewhat offset, of course, by the improved\nsample-efficiency and robustness of GW-Dreamer. Addition-\nally, this study relied on a pre-trained GW model trained on\nrandomly sampled data, which may not always be represen-\ntative of an agent’s environment. In more complex environ-\nments, expert-collected data would likely be necessary to en-\nsure sufficient coverage of states that are difficult to reach by\nchance.\nAn alternative approach could involve training the\nGW representation jointly with the rest of the model. How-\n\n\nNo vision\nReturn Criterion\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nReturn\nGW-Dreamer\nDreamer\nVAE-Dreamer\nVAE PPO\nGW PPO\nNo attributes\nFigure 6: Performance (return) of the different models when one sensory modality is removed. The left part shows results when\nthe attributes modality is discarded, while the right corresponds to the removal of the vision modality. The return criterion is the\nsame as in Figure 5, indicating the performance level at which the task is reliably solved. Dashed colored lines represent the\nmaximum performance of each model from Figure 5, with line and bar colors corresponding to the same models. Performance\nvalues are averaged over 10 trials, with error bars representing the standard error of the mean.\never, the GW itself requires pre-trained latent representations\nas inputs (VanRullen & Kanai, 2021), modeled here as a VAE;\npre-training this latent input space would still potentially lead\nto the same challenge of appropriate environment sampling,\neven if the GW itself was trained jointly with the World Model\nand Actor-Critic components. One way of solving this problem\ncould be to either use a pretrained foundation model capable\nof encoding arbitrary images in a latent space (Oquab et al.,\n2023) or to create our own “foundation” encoder trained on\nlarge-scale open datasets that have already been collected,\ne.g. in robotics (Walke et al., 2023; Collaboration et al., 2024)\nAnother limitation of the present study is that the advan-\ntages of GW-Dreamer were demonstrated here in a single,\nrelatively simple test environment. An important direction for\nfuture work will be to evaluate the model in more complex sce-\nnarios, such as robotic environments.\nDespite these limitations, this study demonstrates the ad-\nvantages of integrating GWT and WM. This combination sig-\nnificantly improves training efficiency in RL, allowing GW-\nDreamer to learn with about 10 times fewer environment\nsteps. This accelerated training is not due to operating entirely\nin a latent space, as GW-Dreamer also surpasses (in terms of\nsample efficiency) a Dreamer variant that relies on latent VAE\nrepresentations.\nFurthermore, GW-Dreamer displays zero-\nshot robustness to modality loss. Unlike the original Dreamer,\nGW-Dreamer maintains its performance even when visual in-\nputs or attribute information are removed (similarly to findings\nobtained for model-free RL algorithms in Mayti´e et al. (2024),\nand corroborated here with our GW-PPO variant).\nThe findings have potential implications in cognitive neuro-\nscience as a practical test of GWT. First, compared with exist-\ning approaches (Radford et al., 2021; Hafner et al., 2024), the\nGW tends to produce a superior multimodal representation,\nowing to its semi-supervised training procedure inspired by\nthe “broadcast” principle (Baars, 1988). This fact was already\nsuggested by a number of recent studies (Devillers et al.,\n2021, 2024; Mayti´e et al., 2024), and is confirmed here in the\ncontext of model-based RL. Second, we show that GW mul-\ntimodal representations can be leveraged by a World Model\nto produce mental simulations that help the system converge\nto an optimal decision strategy. This resembles “dreaming” in\nhumans and animals, and more generally, captures the ability\nof such biological systems to imagine the potential outcome\nof a planned sequence of actions before making a decision.\n\n\nWhile this evidently does not suffice to entirely validate GWT,\nit confirms its potential relevance as a theory of higher-level\ncognition.\nUltimately, this research tackles key challenges in RL, such\nas the large amount of environment interactions required for\npolicy training and the need for strong multimodal representa-\ntions, particularly in robotics. It opens the way for future work\nto further integrate GWT and World Models.\nAcknowledgments\nThis work was supported by an ANITI Chair (ANR grant\nANR-19-PI3A-004), an ANR grant COCOBOT (ANR21-FAI2-\n0005) and by “D´efi Cl´e Robotique centr´ee sur l’humain”\nfunded by R´egion Occitanie, France. This research is also\npart of a project that has received funding from the European\nResearch Council (ERC) under the European Union’s Horizon\n2020 research and innovation programme (Grant agreement\nNo.101096017).\nReferences\nAtkeson, C., & Santamaria, J.\n(1997).\nA comparison of\ndirect and model-based reinforcement learning.\nIn Pro-\nceedings of International Conference on Robotics and Au-\ntomation (Vol. 4).\nRetrieved from https://ieeexplore\n.ieee.org/document/606886\ndoi:\n10.1109/ROBOT\n.1997.606886\nBaars, B. J. (1988). A Cognitive Theory of Consciousness.\nNew York: Cambridge University Press.\nBruce, J., Dennis, M., Edwards, A., Parker-Holder, J., Shi, Y.,\nHughes, E., . . . Rockt¨aschel, T. (2024). Genie: Generative\nInteractive Environments. arXiv. Retrieved from http://\narxiv.org/abs/2402.15391 (arXiv:2402.15391 [cs])\nCho, K., van Merri¨enboer, B., Gulcehre, C., Bahdanau, D.,\nBougares, F., Schwenk, H., & Bengio, Y. (2014). Learning\nPhrase Representations using RNN Encoder–Decoder for\nStatistical Machine Translation.\nIn A. Moschitti, B. Pang,\n& W. Daelemans (Eds.), Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP) (pp. 1724–1734).\nDoha, Qatar:\nAs-\nsociation for Computational Linguistics.\nRetrieved from\nhttps://aclanthology.org/D14-1179/\ndoi: 10.3115/\nv1/D14-1179\nClark, A. (2013). Whatever next? Predictive brains, situated\nagents, and the future of cognitive science.\nThe Behav-\nioral and Brain Sciences, 36(3), 181–204. doi: 10.1017/\nS0140525X12000477\nCollaboration, O. X.-E., O’Neill, A., Rehman, A., Maddukuri,\nA., Gupta, A., Padalkar, A., . . . Lin, Z. (2024). Open x-\nembodiment: Robotic learning datasets and rt-x models :\nOpen x-embodiment collaboration0. In 2024 ieee interna-\ntional conference on robotics and automation (icra).\nRe-\ntrieved from http://arxiv.org/abs/2310.08864\ndoi:\n10.1109/ICRA57147.2024.10611477\nDearden, R., Friedman, N., & Russell, S. (n.d.). Bayesian\nQ-Learning.\nDehaene, S., Kerszberg, M., & Changeux, J.-P. (1998). A\nneuronal model of a global workspace in effortful cognitive\ntasks. Proceedings of the National Academy of Sciences,\n95(24), 14529–14534. Retrieved from https://www.pnas\n.org/doi/full/10.1073/pnas.95.24.14529\ndoi: 10\n.1073/pnas.95.24.14529\nDevillers, B., Choksi, B., Bielawski, R., & VanRullen, R.\n(2021). Does language help generalization in vision mod-\nels?\nIn Proceedings of the 25th Conference on Compu-\ntational Natural Language Learning.\nOnline: Association\nfor Computational Linguistics.\nRetrieved from https://\naclanthology.org/2021.conll-1.13/\ndoi: 10.18653/\nv1/2021.conll-1.13\nDevillers, B., Mayti´e, L., & VanRullen, R.\n(2024).\nSemi-\nSupervised Multimodal Representation Learning Through\na Global Workspace.\nIEEE Transactions on Neural\nNetworks\nand\nLearning\nSystems,\n1–15.\nRetrieved\nfrom\nhttps://ieeexplore.ieee.org/abstract/\ndocument/10580966\n(Conference Name: IEEE Trans-\nactions on Neural Networks and Learning Systems)\ndoi:\n10.1109/TNNLS.2024.3416701\nFriston, K.\n(2010).\nThe free-energy principle:\na unified\nbrain theory?\nNature Reviews Neuroscience, 11(2),\n127–138.\nRetrieved from https://www.nature.com/\narticles/nrn2787 doi: 10.1038/nrn2787\nHa,\nD.,\n&\nSchmidhuber,\nJ.\n(2018).\nRecurrent\nWorld\nModels\nFacilitate\nPolicy\nEvolution.\nIn\nAd-\nvances\nin\nNeural\nInformation\nProcessing\nSystems\n(Vol. 31).\nCurran Associates,\nInc.\nRetrieved from\nhttps://papers.nips.cc/paper files/paper/2018/\nhash/2de5d16682c3c35007e4e92982f1a2ba-Abstract\n.html\nHafner, D., Pasukonis, J., Ba, J., & Lillicrap, T. (2024). Master-\ning diverse domains through world models. Retrieved from\nhttps://arxiv.org/abs/2301.04104\nKonda, V., & Tsitsiklis, J.\n(1999).\nActor-Critic Algo-\nrithms.\nIn Advances in Neural Information Process-\ning Systems (Vol. 12).\nMIT Press.\nRetrieved from\nhttps://papers.nips.cc/paper files/paper/1999/\nhash/6449f44a102fde848669bdd9eb6b76fa-Abstract\n.html\nMayti´e, L., Devillers, B., Arnold, A., & VanRullen, R. (2024).\nZero-shot cross-modal transfer of Reinforcement Learn-\ning policies through a Global Workspace. Reinforcement\nLearning Journal, 3. Retrieved from http://arxiv.org/\nabs/2403.04588\n(ISSN: 2996-8577 arXiv:2403.04588\n[cs]) doi: https://doi.org/10.48550/arXiv.2403.04588\nNVIDIA, Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E.,\n. . . Zolkowski, A. (2025). Cosmos World Foundation Model\nPlatform for Physical AI. arXiv. Retrieved from http://\narxiv.org/abs/2501.03575\n(arXiv:2501.03575 [cs])\ndoi: 10.48550/arXiv.2501.03575\nOquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec,\nM., Khalidov, V., . . . Bojanowski, P. (2023). DINOv2: Learn-\n\n\ning Robust Visual Features without Supervision.\nTrans-\nactions on Machine Learning Research.\nRetrieved from\nhttps://openreview.net/forum?id=a68SUt6zFt\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., . . . Sutskever, I.\n(2021).\nLearning Trans-\nferable Visual Models From Natural Language Supervi-\nsion.\narXiv.\nRetrieved from http://arxiv.org/abs/\n2103.00020\n(arXiv:2103.00020 [cs])\ndoi:\n10.48550/\narXiv.2103.00020\nSafron, A.\n(2022).\nIntegrated world modeling theory\nexpanded:\nImplications\nfor\nthe\nfuture\nof\nconscious-\nness.\nFrontiers in Computational Neuroscience,\n16.\nRetrieved\nfrom\nhttps://www.frontiersin.org/\njournals/computational-neuroscience/articles/\n10.3389/fncom.2022.642397/full\ndoi:\n10.3389/\nfncom.2022.642397\nSutton, R. S.\n(1991).\nDyna, an integrated architecture\nfor learning, planning, and reacting.\nSIGART Bull., 2(4),\n160–163. Retrieved from https://dl.acm.org/doi/10\n.1145/122344.122377 doi: 10.1145/122344.122377\nSzita, I., & Szepesv´ari, C. (n.d.). Model-based reinforcement\nlearning with nearly tight exploration complexity bounds.\nVanRullen,\nR.,\n&\nKanai,\nR.\n(2021).\nDeep\nlearn-\ning\nand the\nGlobal Workspace Theory.\nTrends in\nNeurosciences,\n44(9),\n692–704.\nRetrieved\nfrom\nhttps://www.cell.com/trends/neurosciences/\nabstract/S0166-2236(21)00077-1\ndoi:\n10.1016/\nj.tins.2021.04.005\nWalke, H. R., Black, K., Zhao, T. Z., Vuong, Q., Zheng, C.,\nHansen-Estruch, P., . . . Levine, S.\n(2023).\nBridgeData\nV2: A Dataset for Robot Learning at Scale.. Retrieved from\nhttps://openreview.net/forum?id=f55MlAT1Lu\n\n\nModel Parameters\nGW Parameters\nThis section details the architecture used in the Global\nWorkspace.\nIt begins with the VAEs used to pass from\nraw observations (ov,oattr) to latent unimodal representation\n(zv,zattr). The visual VAE, detailed in Table 1, is composed\nof Convolutional Layers with Batch Norm and ReLU. Its latent\ndimension is of size 10 and it counts 5.8M parameters in to-\ntal. The attributes VAE, detailed in Table 2, is much smaller,\nwith 11,000 parameters. It is composed of Multiple Linear and\nReLU layers with a latent dimension of 10. The last layer of\nthe decoder is divided in two parts, one of size 3 to predict the\nclass of the shape (one-hot encoding), another one of size 8\nwith a Tanh activation to predict the other attributes. These\nVAEs are used for the Global Workspace model, but also for\nVAE-PPO and VAE-Dreamer.\nVAE encoder (2.8M params)\nVAE decoder (3M params)\nx ∈R3×32×32\nz ∈R10\nConv128 −BN−ReLU\nFC8×8×1024\nConv256 −BN−ReLU\nConvT512 −BN−ReLU\nConv512 −BN−ReLU\nConvT256 −BN−ReLU\nConv1024 −BN−ReLU\nConvT128 −BN−ReLU\nFlatten−FC2×10\nConv1 −Sigmoid\nTable 1: Architecture and number of parameters of the visual\nVAE used to encode ov to zv.\nVAE encoder (6,700 params)\nVAE decoder (4,700 params)\nx ∈R11\nz ∈R10\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC10 −ReLU\nFC3 × FC8 −Tanh\nFC2×10\nTable 2: Architecture and number of parameters of the at-\ntributes VAE used to encode oattr to zattr.\nTable 3 gives details about encoders (ev, eattr) and de-\ncoders (dv, dattr) architecture of the Global Workspace. Be-\ncause they are identical for both modalities only one table pro-\nvides the architecture’s details. The encoders and decoders\nare simply a sequence of Linear layers with ReLU activation\nfunction, and the latent dimension of the GW is of size 10.\nThis GW model takes inputs from the VAEs described before\nand is used in the GW-Dreamer and GW-PPO models.\nWM Parameters\nThis part gives details about the architectures and parame-\nters used for the World Model inside GW-Dreamer. It is com-\nposed of two main elements. First, a dynamic part, which is\na one-layer GRU counting 16,000 parameters. It takes as in-\nput a vector of size 17 (GW representation and actions) with\na hidden representation ht of size 64. Second, it has 3 dif-\nferent heads to retrieve different information from the GRU\nGW encoder (13,800 params)\nGW decoder (13,800 params)\nx ∈R10\nz ∈R10\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC64 −ReLU\nFC10\nFC10\nTable 3: Architecture and number of parameters of the en-\ncoders and decoders of the Global Workspace model for one\nmodality.\nlatent space. The architecture of the different heads is de-\ntailed in Table 4.\nThe head used to predict the GW latent\nvector is a simple linear layer with a Tanh activation function.\nThe two others (predicting the scalar reward and a Boolean\n“done” termination flag) have an identical architecture. They\nare composed of a sequence of Linear layers and ReLU acti-\nvation function. These heads count 429,000 parameters, and\nin total the World Model is composed of 445,000 parameters\nGW head (650 params)\nreward / done head (214,000 params)\nx ∈R64\nz ∈R64\nFC10 −Tanh\nFC256 −ReLU\nFC256 −ReLU\nFC256 −ReLU\nFC1\nTable 4: Architecture and number of parameters of the differ-\nent heads composing the World Model.\nAC Parameters\nFinally, this part describes the number of parameters used\nfor the Actor-Critic inside the GW-Dreamer model. The archi-\ntecture is similar to the one proposed in Dreamer (Hafner et\nal., 2024). As shown in Table 5 they are composed of a se-\nquence of Linear layers with ReLU activation function. The Ac-\ntor outputs the parameters of a distribution (mean, variance)\nabout possible actions. The Critic also predicts a distribution\nabout the value of a state. As in Dreamer, the space is dis-\ncretized in different bins to apply a categorical cross-entropy\nloss function between the two hot-encoded targets and the\npredicted softmax distribution (for more details, see Dreamer\nimplementation (Hafner et al., 2024)).\nActor (500K params)\nCritic (800K params)\nx ∈R64\nz ∈R64\nFC512 −ReLU\nFC512 −ReLU\nFC512 −ReLU\nFC512 −ReLU\nFC2×7\nFC2×255\nTable 5: Architecture and number of parameters of the Actor-\nCritic.\n\n\nGW losses Parameters\nThis section gives more details about the hyperparameters\nused during the training of the GW model. The losses used\nto train the model are described in Equations 2 and 3. These\nlosses are scaled by different weights that we fixed as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nβdcy = 1\nβcy = 1\nβtr = 1\nβ fusion = 1\nβcont = 0.1\n(4)\nThese values were taken from the implementation done by\nDevillers et al. (2024), where the weight of the contrastive loss\n(measured by a cross-entropy function) was always smaller\ncompared to the other ones (measured using MSE distance).\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21142v1.pdf",
    "total_pages": 12,
    "title": "Multimodal Dreaming: A Global Workspace Approach to World Model-Based Reinforcement Learning",
    "authors": [
      "Léopold Maytié",
      "Roland Bertin Johannet",
      "Rufin VanRullen"
    ],
    "abstract": "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}