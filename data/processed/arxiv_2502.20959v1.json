{
  "id": "arxiv_2502.20959v1",
  "text": "1\nCicada: A Pipeline-Efficient Approach to Serverless\nInference with Decoupled Management\nZhaorui Wu, Yuhui Deng, Jia Hu, Lin Cui, Zhen Zhang, Lingfang Zeng, Geyong Min\nAbstract—Serverless computing has emerged as a pivotal\nparadigm for deploying Deep Learning (DL) models, offering\nautomatic scaling and cost efficiency. However, the inherent cold\nstart problem in serverless ML inference systems, particularly\nthe time-consuming model loading process, remains a significant\nbottleneck. Utilizing pipelined model loading improves efficiency\nbut still suffer from pipeline stalls due to sequential layer\nconstruction and monolithic weight loading. In this paper, we\npropose Cicada, a novel pipeline optimization framework that\ncoordinates computational, storage, and scheduling resources\nthrough three key mechanisms: (1) MiniLoader: which reduces\nlayer construction overhead by opportunistically optimizing pa-\nrameter initialization; (2) WeightDecoupler: decoupling weight\nfile processing from layer construction, enabling asynchronous\nweight retrieval and out-of-order weight application; (3) Priority-\nAware Scheduler: dynamically allocating resources to ensure high-\npriority inference tasks are executed promptly. Our experimental\nresults demonstrate that Cicada achieves significant performance\nimprovements over the state-of-the-art PISeL framework. Specif-\nically, Cicada reduces end-to-end inference latency by an average\nof 61.59%, with the MiniLoader component contributing the\nmajority of this optimization (53.41%), and the WeightDecou-\npler achieves up to 26.17% improvement. Additionally, Cicada\nachieves up to 2.52x speedup in the inference pipeline utlization\ncompared to PISeL.\nIndex Terms—Cloud computing, serverless computing, batch-\ning request, resource management\nI. INTRODUCTION\nServerless computing represents a significant advancement\nin cloud resource management, enabling automatic scaling and\nenhancing cost efficiency for modern applications [1], [2], [3].\nThe growing demand for efficient Deep Learning (DL) deploy-\nment across industries has accelerated the adoption of server-\nless architectures for model serving, particularly in scenarios\nrequiring rapid scaling [4], [5]. Major cloud providers have\nintegrated this approach into their services, including Amazon\nSageMaker [6], Azure Machine Learning [7], and Google\nCloud Vertex AI [8]. Recent studies indicate that server-\nless implementations can lower operational costs compared\nto traditional cloud services while maintaining performance,\nZ. Wu, Y. Deng, L. Cui, Z. Zhang are with the Department of Com-\nputer Science, Jinan University, Guangzhou, Guangdong Province 510632,\nChina. E-mail: diom wu@163.com, tyhdeng@jnu.edu.cn,tcuilin@jnu.edu.cn,\nzzhang@jnu.edu.cn.\nZ. Wu is also with the Department of Computer Science, University of\nExeter, Exeter, EX4 4QF, U.K. E-mail: zw467@exeter.ac.uk.\nL. Zeng is with the Zhejiang Lab, Hangzhou 311121, China. E-mail:\nzenglf@zhejianglab.org.\nJ. Hu and G. Min are with the Department of Computer Science,\nUniversity of Exeter, Exeter, EX4 4QF, U.K. E-mail: g.min@exeter.ac.uk,\nj.hu@exeter.ac.uk.\nBuild Layer\nLoad Weight\nTimeline\nL1 L2 L3\nW1,W2,W3\nLayers\nWeight\nInput\nOutput\nInference Process\nLoaded in Sequence \nLoaded in Whole \nL3\nW3\nL2\nW2\nL1\nW1\nL1\nW1 \nW2 \nW3\nL2\nL3\nStorage\nBuild Layers\nLoad Weight \nLayers are \n Idle\nInference\nRequest\nI/O\nL1W1\nL2\nL3\nW2\nW3\nResponse\nFig. 1: Traditional model loading\nparticularly for applications with fluctuating workloads [9],\n[10].\nIn serverless DL inference systems, user requests trigger on-\ndemand execution of model-encapsulated functions resident\nin isolated execution environments, such as containers [11].\nThese operations face a critical performance contrast: con-\ntainer provisioning (cold start) versus reuse of active instances\n(warm start). Empirical studies reveal cold starts incur 5-\n10x longer latency due to container initialization and model\nloading processes [12], [13]. The cold start challenge in server-\nless inference systems comprises two phases: (1) function\ninstance startup and (2) model loading. Existing optimization\napproaches have improved instance startup efficiency through\ncontainer management techniques [14], [15], [16], yet model\nloading optimization remains constrained by preloading [17],\n[4] and layer-sharing methods [18], [19].\nPreloading-based approaches fundamentally depend on ei-\nther precise request prediction mechanisms or the availability\nof existing warmed containers. Similarly, sharing-based meth-\nods also rely on the existence of existing warmed contain-\ners. However, request prediction-based approaches generally\nfail to achieve objective accuracy in general scenarios [20],\nwhile potentially introducing additional resource overhead,\nsuch as prediction strategies based on artificial intelligence\nmodels [21]. Furthermore, maintaining shareable model layer\nstructures within existing containers requires persistent mem-\nory allocation of layer data in completed containers [18], [22].\nThis architecture also introduces potential cross-node trans-\nmission overhead when accessing layers not locally available,\ncompounded by the dynamic lifecycle management inherent\nin serverless computing environments.\nWhile existing optimization strategies demonstrate partial\neffectiveness, their reliance on conventional model construc-\ntion paradigms fundamentally limits loading efficiency. As\nillustrated in Fig. 1, the traditional loading process exhibits\ntwo key characteristics: (i) sequential layer construction, re-\nquiring full architectural instantiation before weight loading,\narXiv:2502.20959v1  [cs.DC]  28 Feb 2025\n\n\nand (ii) monolithic weight loading, encompassing storage I/O\noperations and parameter application. The temporal execution\npattern (Fig.1, left) reveals three sequential bottlenecks: layer\nconstruction →weight loading →request inferencing. In\nthis conventional approach, each model layer is constructed\nsequentially, followed by a unified weight loading phase. Only\nafter the entire model is fully loaded can it begin serving user\nrequests. The weight loading process consists of two stages:\n(i) retrieving and loading weight files from disk (I/O phase in\nFig. 1), and (ii) applying the deserialized weight parameters\nto the instantiated layer architecture. Therefore, layers are idle\nduring the weight loading phase, delaying the efficiency of the\ninference process.\nThe latest research, PISeL [23], introduces pipelined loading\nmechanisms to enhance the efficiency of serverless inference.\nThis approach divides model loading into three layer-wise\nexecution units within a pipeline: (i) layer construction, (ii)\nweight loading, and (iii) request inference. To further improve\npipeline efficiency, model loading and inference are executed\nin parallel. The feasibility of this pipeline arises from the\ninherently modular architecture of DL models, where neural\nnetworks naturally decompose into sequentially dependent\ncomputational layers.\nDespite the benefits of pipelined execution, there remain\ninefficiencies in resource utilization. Our motivating results\n(Sec. II) reveal two key inefficiencies in the model inference\npipeline. First, constructing model layers is inherently time-\nconsuming and computationally demanding. Moreover, this\nprocess introduces unnecessary overhead in parameter regis-\ntration and initialization, thereby prolonging the time required\nto instantiate layer structures. In addition, weight loading only\nbegins after the corresponding layer is fully built, leading to\npipeline stalls that further delay inference. Second, the weight\nloading process itself is highly I/O-intensive, causing execu-\ntion stalls as the weight loading pipeline unit remains idle\nwhile waiting for data transfers to complete. This imbalance\nbetween computation and data movement exacerbates overall\ninefficiencies, further limiting pipeline performance. Together,\nthese challenges highlight the fundamental bottlenecks in\nexisting inference pipelines and underscore the need for a more\nefficient execution strategy.\nTo address these limitations, we propose Cicada—a novel\npipeline optimization framework that coordinates computa-\ntional, storage, and scheduling resources. For layer construc-\ntion, Cicada implements MiniLoader, which employs an op-\nportunistic strategy to optimize layer initialization and thereby\nreduce the time required for structural preparation. Aiming to\nimprove the pipeline utilization, Cicada partitions the weight\nloading unit into two distinct stages: (1) retrieving weight files\nfrom disk and (2) applying deserialized weight parameters to\nlayer structures. Building on this separation, WeightDecoupler\nis introduced, which presents an asynchronous retrieval strat-\negy to process weight files retrieval before execution, thereby\nenabling data transfers to overlap with layer construction. The\nWeightDecoupler facilitates out-of-order weight application\nduring the second stage, allowing layers to integrate weights\nas soon as their structures are ready. Consequently, pipeline\nstalls are minimized, thereby improving overall efficiency.\nInference Requests\nInference Handler\n \ndef main(args):\n    model = load_model() \n    inp_tensor = get_input(args) \n    with torch.no_grad(): \n        output = model(inp_tensor)\n    return {\"output\": output}\nHTTP Proxy\nL\nW\nE\nInput\nOutput\nContainer\nFunction Handler\n1\n2\n3\nFig. 2: Overview of serverless inference\nTo manage task execution effectively, Cicada employs a\nPriority-Aware Scheduler that monitors resource usage across\nthe pipeline and reallocates resources to alleviate contention\nbottlenecks, thus prioritizing critical inference requests and\nensuring stable system performance.\nWe summarize our contributions as follows.\n• We identify key performance bottlenecks in pipelined\nmodel inference, highlighting inefficiencies in layer con-\nstruction delays and I/O-induced stalls that create an\nimbalance between computation and data movement.\n• We design Cicada with three novel optimizations: (1)\nopportunistic initialization optimization during layer con-\nstruction to reduce structural overhead, (2) asynchronous\nweight retrieval with out-of-order weight application to\neliminate loading stalls, and (3) a priority-aware sched-\nuler that allocates resources between model preparation\nand inference execution to ensure high-priority inference\ntasks are executed promptly.\n• We implement a prototype system of Cicada and conduct\nextensive experiments to validate its effectiveness. The\nexperimental results demonstrate that Cicada achieves an\naverage 61.59% reduction in end-to-end inference latency\ncompared to the state-of-the-art PISeL framework, while\ngains up to 2.52x improvement in pipeline utilization.\nII. BACKGROUND AND MOTIVATION\nIn this section, we analyze the serverless inference life cycle,\nidentify the performance bottlenecks in the pipelined model\ninference (Sec. II-A), and show the motivation for designing\nCicada (Sec. II-B and Sec. II-C).\nA. Serverless Inference Life Cycle\nThe serverless inference lifecycle comprises three sequential\nphases: (i) execution environment provisioning if needed (i.e.,\nfunction cold start), (ii) model loading, and (iii) inference\nexecution. Following standard serverless paradigms, the pro-\nvisioning phase establishes an isolated runtime environment\nwith necessary dependencies and handler functions.\nDue to the ephemeral nature of serverless function execu-\ntion, model loading and inference execution must be repeated\n2\n\n\nResNet50\nResNet101\nResNet152\nVGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n200\n400\n600\n800\n1000\nMemory (MB)\n1162\nFig. 3: Overview of model memory overhead\nStorage\nBuild Layer 1\nLoad Weight\nInference\nRequest\nResponse\nI/O\nBuild Layer 2\nLoad Weight\nExecute Layer 1\nExecute Layer 2\nBuild Layer 3\nLoad Weight\nExecute Layer 3\nParallel\nL1\nL2\nL3\nW1\nW2\nW3\nE1\nL3\nE3\nI/O\nI/O\nParallel\nTimeline\nInput\nBuild Layer\nLayer Idle\nLoad Weight\nOutput\nL1\nW1\nL1\nW1\nL2\nW2\nL2\nW2\nL3\nW3\nL3\nW3\nFig. 4: Pipeline-based model loading\nResNet50\nResNet101\nResNet152\nVGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n200\n400\n600\n800\n1000\nTime (ms)\n2874\n(a) Layer construction overhead\nResNet50\nResNet101\nResNet152\nVGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n200\n400\n600\n800\n1000\nTime (ms)\n(b) Space allocation overhead\nResNet50\nResNet101\nResNet152\nVGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n200\n400\n600\n800\n1000\nTime (ms)\nWeights Applying\nWeights File Processing\n(c) Weight retrieval and applying overhead\nFig. 5: Execution overhead in layer construction and weight loading phases across different models\neven when requests are routed to warmed instances (i.e., func-\ntion warm starts). This requirement stems from the container-\nlevel process isolation mechanism [24], [14]. Fig. 2 illustrates\nthe execution process of serverless inference. A container-\nembedded HTTP proxy receives user requests and triggers the\nfunction handler to initiate inference through the deep learning\nruntime (e.g., PyTorch, TensorFlow). The runtime executes the\ncomputational graph by deserializing persisted model artifacts,\nfollowing three stages: 1⃝initializing the model structure, 2⃝\nloading weight files and applying parameters to the instanti-\nated model, and 3⃝performing inference, where the model\nprocesses input data through forward propagation to generate\npredictions.\nExisting research efforts [18], [19] have explored main-\ntaining loaded models in memory to avoid deserialization\noverhead or sharing models across containers. However, these\napproaches neglect the critical memory pressure imposed by\npersistent model retention. To quantify this overhead, we\nconduct empirical measurements across three prominent model\narchitectures: ResNet [25], VGG [26], and Vision Transform-\ners (ViT) [27]. As depicted in Fig. 3, our results demonstrate\nsubstantial memory requirements ranging from 97.49 MB for\nResNet-50 to 1.16 GB for ViT-L-16. This substantial mem-\nory footprint renders persistent model retention impractical\nfor production-grade serverless inference systems, particularly\nunder concurrent request handling scenarios.\nTo alleviate the memory overhead, the latest research\nPISeL [23] introduces pipelined model loading mechanisms to\nenhance the efficiency of serverless inference. Fig. 4 illustrates\nthe model inference process of PISeL. In this approach,\nmodel inference is divided into three layer-wise execution\nunits within a pipeline, including layer construction (green\nprism), weight loading (yellow rectangle), and request infer-\nence, where the yellow rectangle is on top of the green prism.\nWith this pipeline design, inference can be executed asyn-\nchronously alongside the preceding stages, thereby reducing\nlatency and improving serverless inference throughput. For\ninstance, as illustrated on the right side of Fig. 4, the weight\nloading stage (Wi) follows the layer construction stage (Li)\nand precedes the inference stage (Ei). Meanwhile, while the\ninference unit is processing Ei, the model loading unit simul-\ntaneously executes Li+1 and Wi+1. However, this pipeline-\nbased model loading approach fails to address the essential\nproblem of pipeline stalls. The reason behind this is that the\nlayer construction process is highly time-consuming, which\ncauses the weight loading and inference execution to wait\n(pipeline stall) for the completion of the layer construction.\nTo further analyze the performance overhead of the pipeline,\nwe conduct a detailed study of the execution overhead.\nB. Construction Overhead of Model Layers\nBy instrumenting PyTorch, a widely adopted deep learning\nruntime, we identify two critical phases in layer construction.\nThe first phase involves the instantiation of layers (e.g., Conv,\nBatchNorm) to establish the model’s computational graph,\nwhile the second phase entails space allocation through param-\neter registration and initialization, thereby ensuring designated\nmemory slots for subsequent weight loading.\nAlthough the initial phase defines the computational de-\npendencies of the model, it is the space allocation stage that\npre-allocates memory slots, and this stage incurs superfluous\noverhead in inference scenarios. For example, in a 3x3 convo-\nlutional layer, parameter registration results in the allocation\nof thousands of placeholders for weight parameters. Moreover,\n3\n\n\nduring the parameter initialization phase, these parameters are\nassigned values using normalization techniques such as the\nKaiming normal distribution [28]. However, since pre-trained\nweights are available during inference, this initialization be-\ncomes redundant.\nFigure 5 illustrates the execution characteristics of three\nprominent model architectures—ResNet, VGG, and ViT. The\ntotal layer construction time is determined by the combined\nduration of both phases—instantiating the computational graph\nand performing space allocation—as illustrated in Figures 5(a)\nand 5(b). Fig.5(a) breaks down the construction overhead\nacross different layers (marked by different colors), showing\nthat different layers contribute different overheads to the con-\nstruction process, ranging from 100ms to 900ms. Beside, our\nanalysis reveals that the parameter initialization phase alone\naccounts for more than 50% of the total construction time\n(Fig.5(b)), despite being unnecessary for inference with pre-\ntrained weights. This excessive overhead propagates through\nthe pipeline, delaying weight loading and inference execution.\nUltimately, these structural inefficiencies undermine the ben-\nefits of pipelined execution, as both the weight loading and\ninference units remain blocked, waiting for layer construction\nto complete.\nIn summary, although pipeline-based model loading accel-\nerates execution, inefficiencies in layer construction inherently\nlimit its overall performance. Therefore, optimizing layer\ninstantiation with a lightweight construction process is crucial\nto fully unlocking the performance potential of pipelined\nexecution.\nC. Opportunity of Out-of-Order Execution\nInference execution requires pre-trained weights to be\nloaded into memory, as deep learning models rely on precom-\nputed parameters rather than learning them during inference. In\nserverless inference, weights are preferred to be stored in local\nstorage (e.g., alongside container images), to avoid additional\nremote I/O overhead. Therefore, the weight loading process\ncomprises two distinct phases: (i) weight file processing and\n(ii) weight application to the instantiated model. In the first\nphase, weight files are accessed from storage, involving oper-\nations such as disk retrieval and data deserialization to recon-\nstruct the stored tensors. Once the weight data is available in\nmemory, the second phase assigns the deserialized parameters\nto their corresponding model layers, ensuring proper data\nalignment and readiness for execution.\nFig.5(c) quantifies the execution overhead of these two\nphases across different model architectures. The solid-colored\nbars represent I/O-bound weight file processing, while the\nhatched bars indicate weight application. On average, weight\nfile processing accounts for 79.3% of the total execution time,\nmaking it the dominant factor in weight loading. Moreover,\nthis disproportionate 4:1 ratio between data movement and\ncomputation results in severe pipeline underutilization.\nIn addition to the overhead incurred by layer construction\n(see Fig. 5(a)), the I/O-intensive nature of weight file process-\ning can cause stalls in subsequent pipeline stages, including\nweight application and inference execution. The primary cause\nof this bottleneck is that weight application is contingent upon\nthe complete execution of weight file processing, while infer-\nence execution demands that the entire layer be fully loaded.\nConsequently, it is imperative to devise an efficient strategy\nthat decouples these two operations, thereby mitigating the\nresultant delays and enhancing overall system performance.\nTherefore, we advocate for a decoupling approach wherein\nweight file processing is separated from weight application,\nthereby enabling the former to overlap with layer construction.\nDecoupling weight file processing from weight application\nreduces the risk of pipeline stalls by enabling parallel execu-\ntion with layer construction. This decoupling directly breaks\nthe sequential dependency between weight file processing and\nweight application, allowing weight application tasks to be\nscheduled independently. Therefore, weight file processing can\nbe overlapped with layer construction since fetching weight\nfiles is independent of layer construction.\nDecoupling weight file processing from weight application\nalso creates an opportunity for out-of-order execution of\nweight application, without waiting for the current weight file\nprocessing to complete, alleviates stalls caused by sequential\ndependencies and thereby enhances overall pipeline efficiency.\nHowever, it is important to note that this decoupling introduces\na new challenge related to the scheduling of weight application\nfrom a performance stability perspective—a topic that will\nbe further explored in Section III-E. To ensure performance\nstability, we propose a priority-aware scheduler that assigns\nprecedence to inference execution units based on workload\ndemand.\nMiniLoader\nWeightDecoupler\nModel\nLayer Queue\nWeight Queue\nW2\nW1\nA2\nA1\nE2\nE1\n          Inference\n3\n      Weight\n2\nInference Queue\nScheduler\nW3\nE3E4E5\nContainer\nPrioAwareScheduler\n         Layer\n1\nA 4-Byte element\nA 1-bit element\nFig. 6: Cicada Overview\nIII. DESIGN\nA. Design Overview\nIn this section, we introduce Cicada, a novel pipeline\noptimization framework that effectively coordinates compu-\ntational, storage, and scheduling resources. As with other\nserverless inference systems, users submit inference re-\nquests—including the desired model, weight, and input\ndata—to the serverless platform, which subsequently invokes\nthe Cicada framework to process the request. Embedded within\nthe serverless runtime (e.g., a container), Cicada comprises\nthree primary components (shown in Fig. 6):\n4\n\n\n1) MiniLoader: This module applies a lightweight strategy\nto dynamically adjust parameters initialization to mini-\nmize unnecessary overhead.\n2) WeightDecoupler: To accelerate weight processing, Ci-\ncada decouples weight file processing from weight ap-\nplication, allowing its overlap with layer construction.\nThis separation permits out-of-order weight application\n(A1, A2) by executing weight file operations (W1, W2)\nasynchronously alongside layer construction.\n3) Priority-Aware Scheduler: In the inference execution\nphase, Cicada employs a Priority-Aware Scheduling\nmechanism, dynamically prioritizing inference execution\nunits (E1, E2) based on workload demand.\nUpon receiving an inference request, Cicada initiates layer\nconstruction, weight loading, and inference execution in a\nlayer-by-layer manner. During this process, the MiniLoader\nidentifies space allocation operations and opportunistically\nsubstitutes the corresponding parameters to reduce construc-\ntion overhead and temporarily alleviate memory stress. Fur-\nthermore, the WeightDecoupler employs a decoupling strat-\negy for weight file processing by overlapping it with layer\nconstruction. This decoupling allows weight file processing to\nproceed independently of layer construction, thereby enabling\nweight application to be executed out of order. Finally, by\nutilizing the Priority-Aware Scheduler, Cicada guarantees\nthat inference execution is assigned the highest priority. This\nensures that critical tasks are executed promptly, thereby\noptimizing overall pipeline efficiency.\nB. MiniLoader\nAs detailed in Section II-B, the parameter registration and\ninitialization phase during layer construction incurs super-\nfluous overhead in inference scenarios. Conventionally, the\nregistration process allocates full-precision (typically float32,\nor 4 bytes) placeholders according to each layer’s dimensions\n(e.g., in channels, out channels, kernel size, etc.) This\napproach, while ensuring numerical accuracy and preserving\nthe integrity of the layer’s computation graph, results in mem-\nory overhead during model construction. For instance, a layer\nmay instantiate a matrix with n elements solely as a structural\ncontainer to define its shape and memory layout; however, the\nactual numerical values in this matrix are temporary, destined\nto be overwritten by pre-trained weights during inference.\nGiven that these initial parameter values are ultimately\nreplaced by pre-trained weights, it is feasible to utilize a low-\nprecision format without compromising the structural require-\nments. To that end, Cicada introduces MiniLoader, an oppor-\ntunistic adjustment mechanism that reduces both initialization\nlatency and memory footprint. Specifically, MiniLoader takes\ncontrol of the parameter initialization process by employing\nlow-precision techniques that compress the conventional 4-\nbyte placeholders into 1-bit representations (see the lower left\npanel of Fig. 6). This compression directly alleviates memory\noverhead during layer construction.\nIn inference scenarios, the availability of pre-trained weights\nobviates the need for subsequent parameter initialization.\nWhile the numerical initialization phase—often executed via\nLayer Construction\nWeight File Processing\nWeight Applying\nInference Execution\n(b) Async File Retrieval\n(c) Out-of-Order (Idea)\n(d) Out-of-Order (Bad)\n(a) Sequential Pipeline\nTime\nL1\nE1\nL3\nE2\nL2\nE3\nL4\nL5\nE4\nE5\nW1 A1\nW2\nW3\nA2\nA3\nW4\nA4 W5 A5\nE1\nE2\nE3\nL1\nL3\nL2\nL4\nL5\nE4\nE5\nW1\nW2\nA1\nA3\nW3\nW4\nW5\nA4 A5\nA2\nE1\nE2\nE3\nL1\nL3\nL2\nL4\nL5\nE4\nE5\nW1\nW2\nA1\nA3\nA2\nW3\nW4\nW5\nA4\nA5\nE1\nE2\nE3\nL1\nL3\nL2\nL4\nL5\nE4\nE5\nW1\nW2\nA1\nA3\nA2\nW3\nW4\nW5\nA4\nA5\nFig. 7: Pipeline Scheduling Strategies in a Five-layer Model\nsophisticated schemes such as Kaiming initialization—remains\nessential for establishing the layer’s computation graph, its\noutcomes are ultimately superseded by the loaded weights.\nRecognizing that the subsequent parameter assignment is\nredundant during inference, MiniLoader bypasses these super-\nfluous steps entirely, thereby halving the overall initialization\ntime, as illustrated in Fig. 5(a) and Fig. 5(b).\nC. Weight Decoupler\nWe now turn our attention to optimizing the weight loading\nprocess with the Weight Decoupler of Cicada. In this process,\nthe deep learning runtime (e.g., PyTorch, TensorFlow) is\nresponsible for applying pre-trained weights to an instantiated\nmodel. Typically, pre-trained weights are stored as weight files\n(.pth for PyTorch and .pb for TensorFlow), which must\nbe retrieved from storage and loaded into memory before\nbeing applied to the model. This weight loading process\ncan be divided into two sequencial stages: (1) weight file\nprocessing which is I/O-bound and (2) weight application\nwhich is computation-bound. Fig. 7(a) illustrates a sequential\npipeline-based inference process of a 5-layer model in existing\ndesigns, where the weight loading process (Wi and Ai) is\nsequentially dependent on the layer construction process (Li),\nleading to inefficiencies caused by blocked execution.\nFortunately, layer construction and weight file processing\noperate independently. Exploiting this natural separation, the\nWeight Decoupler enables Asynchronous File Retrieval by\noverlapping weight file processing with layer construction,\nwith the aim of improving pipeline efficiency. As shown in\nFig. 7(b), weight file operations (W1 and W3) are processed\nconcurrently during the construction of layer 1 (L1). Once W3\ncompletes, the weight application (A1) is performed, allowing\ninference of layer 1 (E1) to proceed. The feasibility of this\ndecoupling is supported by the operation overhead analysis\nin Fig. 5, which shows that weight loading (including file\nprocessing and weight application) is lighter compared to layer\nconstruction.\nIn a nutshell, in an active pipeline, Cicada’s decoupling\nstrategy overlaps weight file processing with layer construc-\ntion, enabling increased concurrency in weight file retrieval\nwithin a shared execution window. This improved alignment\nminimizes I/O bottlenecks and maximizes computational re-\nsource utilization, ultimately yielding a more efficient and\nscalable inference pipeline.\n5\n\n\nD. Out-of-Order Weight Application\nAsynchronous file retrieval ensures that weight files are\nfetched concurrently from storage while layers are being\ninstantiated, reducing idle waiting time and enhancing overall\npipeline throughput. In practice, while weight file processing\nhandles I/O tasks—where the CPU submits I/O requests to the\nkernel, which then transfers data from storage to memory—the\nweight loading unit remains idle and available to perform\nother operations, such as weight application. Moreover, this\nmechanism enables out-of-order execution of weight applica-\ntion operations, which further improves pipeline utilization and\nincreases the efficiency of the weight loading unit.\nIn practice, while weight file processing handles I/O tasks,\nthe weight loading unit remains idle. This is because, after\nthe CPU submits an I/O request to the kernel, the kernel is\nresponsible for transferring data from the storage to memory\nand subsequently returning it to the CPU. During this interval,\nthe weight loading unit is free to perform other operations,\nsuch as weight application. Moreover, asynchronous file re-\ntrieval overlaps multiple weight file processing tasks with a\nsingle layer construction operation (see W1, W3 and L1 in\nFig. 7(b)). As a result, once layer construction is complete,\nthe weight loading execution unit is no longer bound by the\nprogress of the ongoing weight file processing, thus permitting\nout-of-order execution. For example, as illustrated in Fig. 7(c),\nwhen L1 is completed, weight application for layer 1 (A1)\ncan commence, even if weight file processing task W3 is still\nunderway.\nE. Priority-Aware Scheduler\nThe primary goal of Cicada is to improve pipeline utilization\nwhile maintaining low inference latency. To this end, we\nadopt strategies such as asynchronous file retrieval and out-of-\norder weight application. By decoupling weight file processing\nfrom layer construction, these approaches allow inference\nexecution to advance to the next pipeline unit more rapidly.\nHowever, the out-of-order mechanism may sometimes suffer\nfrom instability, as weight file processing operations might not\nreturn in the same order in which they were requested due to\nunpredictable I/O completion.\nDue to the inherent complexity of I/O schedulers [29], [30],\n[31] and storage media (e.g., SSDs, HDDs), asynchronous I/O\nrequests cannot guarantee a specific return order. For example,\nas the red rectaggle shown in Fig. 7(d), even if the request for\nW2 is issued before those W5 and W4 (with the order illus-\ntrated in Fig. 7(c)), the response for W2 may still be delayed\nrelative to the others. This uncertainty in asynchronous I/O\nmay compromise pipeline stability, particularly by affecting\nthe timing of weight application.\nTo address this issue, we employ a priority-aware scheduler\nto dynamically prioritize inference execution units based on\nworkload demand. This mechanism ensures that high-priority\ninference requests are executed promptly while optimizing\npipeline efficiency.\nThis strategy schedules tasks by monitoring the execution\nstatus of individual pipeline units. Specifically, it adjusts the\nweight file loading processes that overlap with layer con-\nstruction based on the current progress of the layers. For\nexample, when L2 begins, the scheduler tries to ensure that\nW2 completes before L2 finishes so that subsequent operations\nA2 and E2 can be executed immediately. To achieve this, the\nscheduler records the execution times of each layer (L) and\nweight file (W) operation and employs I/O process blocking\nas needed.\nFor instance, suppose L2 starts at time t0 with a construction\nduration of DL2, and W2, W4, and W5 are issued sequentially\nat time (t0 + a) with I/O durations of DW2, DW4, and\nDW5, respectively. Here, a is relatively small, representing\nthe additional latency introduced by pipeline unit scheduling\noverhead. To guarantee that A2 can commence at t0+DL2 (the\nend of L2), the scheduler checks whether W2 has completed\nby (t0 + a) + DW2 (expected completion time). If W2 is still\nprocessing, the scheduler sends blocking signals to W4 and\nW5 to suspend their I/O activities, thereby prioritizing W2.\nThe pseudocode of this strategy is shown in Algorithm 1.\nIn worst-case scenarios, the algorithm demonstrates a lin-\near time complexity O(n), where n denotes the number of\nconcurrent weight operations. This worst-case scenario arises\nwhen all operations in the set W require suspending (as\nindicated in Lines 3–5). In practice, however, the number\nof concurrently executed weight loading tasks is inherently\nlimited; for example, PyTorch’s implementations of ResNet,\nVGG, and ViT comprise 10, 3, and 4 layers, respectively.\nConsequently, the incurred overhead is effectively constant in\nscale. Furthermore, the algorithm’s space complexity is O(1)\nas it only necessitates the storage of constant-level variables\nsuch as the current time and expected completion time.\nBy dynamically adjusting the priority of weight operations,\nthe algorithm guarantees that high-priority tasks are executed\npromptly, optimizing resource utilization and ensuring perfor-\nmance stability during the model inference process.\nAlgorithm 1 Adjust Weight Operation Priority for Li\nInput:\nt0: Start time of layer Li\nDWi: I/O duration for Wi\nW: Set of running weight operations (including Wi)\nt: Current time\nOutput: Priority status for Wi\n1: expected completion ←(t0 + a) + DWi\n2: if t ≥expected completion and Wi is not completed\nthen\n3:\nfor each weight operation W ∈W with W ̸= Wi do\n4:\nblock W\n▷Suspend lower-priority I/O\n5:\nend for\n6:\nset priority of Wi to HIGH\n7: else\n8:\nset priority of Wi to NORMAL\n9: end if\n10: return priority of Wi\n6\n\n\nIV. EVALUATION ENVIRONMENT AND METHODOLOGY\nA. Setup and Baselines\nThis study focuses on the performance and efficiency of\nCicada running on a single-node server, rather than a efficiency\nof clustered server. This this regard, we develope and evlaute\nCicada on our own Ubuntu 22.04 server environment, which is\nequipped with 2 Intel(R) Xeon(R) Gold 6248R CPUs (96 cores\nin total) and 128GB DDR4 memory. Since this study mainly\nfocuses on optimizing the inference performance in serverless\nenvironment, all invocations are dispathced after containers are\nlaunched. Therefore, the cold start latency of containers is not\nconsidered in this study.\nWe compare Cicada with the traditional synchronous\npipeline and state-of-the-art inference pipeline, PISeL [23], as\nwell as with different component activation strategies within\nCicada—specifically, the Mini, Preload and full Cicada (Mini\n+ Preload) strategies. The Mini strategy incorporates the\nMiniLoader component (sec. III-B) into the PISeL framework,\nwhile the Preload strategy integrates the WeightDecoupler\ncomponent (sec. III-C) on top of the PISeL framework.\nNotably, the Priority-Aware Scheduler (sec. III-E) is employed\nin both the Preload and Cicada strategies to mitigate potential\nperformance instability arising from the out-of-order weight\napplication in the WeightDecoupler component, whereas the\nMini strategy does not require this component.\n04:39\n04:47\n04:55\n05:03\n05:11\n05:19\nTimeline\n0\n50\n100\nInvocations per Minute\nFig. 8: Invocation Pattern\nB. Workloads\nWe evaluate Cicada’s performance across a diverse set\nof workloads, including inference requests from the ResNet\nfamily (ResNet50, ResNet101, ResNet152), the VGG family\n(VGG11, VGG13, VGG16, VGG19), and the ViT family (ViT-\nB-16, ViT-B-32, ViT-L-16). The primary goal of this study is\nto optimize the processes of model construction and weight\nloading, excluding the inference execution phase. To this end,\nfor each model, we generate a random 4-dimensional tensor\nthat represents an input data. This tensor has dimensions\n1x3x224x224—where 1 is the batch size, 3 corresponds to\nthe red, green, and blue color channels, and 224x224 is the\nimage resolution, which is the standard size used in ImageNet\ndatasets.\nSince Cicada is designed to optimize the process of\nserverless inference requests, we drive these workloads using\nthe Azure function trace [32]. The Azure trace comprises\n1,980,951 function execution times and invocation timestamps\ncollected over a 14-day period. For our experiments, we\nutilize data from day 14, randomly assigning functions to the\nevaluated models over a one-hour duration, resulting in a total\nof 2426 invocations. Fig. 8 illustrates the invocation pattern\naggregated by the number of invocations per minute, which we\nselected as a representative example due to its strong indication\nof the burstiness characteristic of serverless functions.\nC. Evaluation Metrics\nCicada introduces a suite of components designed to op-\ntimize the efficiency of serverless inference pipelines. To\nevaluate the effectiveness of Cicada, we aim to address the\nfollowing key questions:\nQ1: How do Cicada perform in terms of inference latency?\nIn this study, we define inference latency as the total time\nthe serverless function takes from the inference request arrival\nto the response completion. Note that this metric does not\ninclude the time for container initialization.\nQ2: What memory advantages does the MiniLoader offer?\nAs discussed in Sec. III-B, the MiniLoader component\nreduces memory overhead during the interval between layer\nconstruction and weight loading by optimizing the parameter\nregistration and initialization phases. In this context, memory\noverhead is recorded at the end of the layer construction phase\nand the start of the weight application phase.\nSince weight application assigns pre-trained weights to\nmodel\nparameters,\nthe\nmemory\nreduction\nachieved\nby\nMiniLoader persists until the weight application is complete.\nQ3: What is the time overhead within each pipeline units?\nAn inference pipeline consists of a sequence of pipeline\nunits, including layer construction, weight loading, and infer-\nence execution. In this context, we focus on the time overhead\nincurred by each pipeline unit, which comprises both working\ntime and waiting time.\nThe working time of a unit is determined by sampling its\nstart and end times, while the waiting time is calculated as\nthe difference between the start time of the current unit and\nthe end time of the previous unit. For example, the waiting\ntime for a layer in the weight loading unit can be derived by\nsubtracting the end time of layer construction from the start\ntime of weight loading.\nQ4: How efficient is Cicada’s pipeline?\nThe execution processes of the various pipeline units over-\nlap. For instance, the inference phase of layer i can run\nconcurrently with the construction phase of layer i + 1. Thus,\npipeline utilization and visualization of pipeline timeline\n(Gantt chart) are introduced to evaluate the efficiency and par-\nallelism of resource scheduling within the inference pipeline.\nPipeline utilization is calculated as the ratio of the total\nbusy time to the overall inference latency. The busy time\nis determined by aggregating the time intervals associated\nwith individual pipeline stages after merging any overlapping\nperiods to ensure that concurrent operations are not counted\nmultiple times.\n7\n\n\nResNet50\nResNet101\nResNet152VGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n500\n1000\n1500\n2000\n2500\n3000\nInference Time (ms)\nPISeL\nPreload\nMini\nCicada\nFig. 9: End-to-end Inference Latency\nResNet50\nResNet101\nResNet152VGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n100\n200\n300\n400\n500\n600\nMemory Usage (MB)\nPISeL\nMini\n0\n100\n200\n300\n400\n500\n600\n700\nMemory Usage Time (ms)\n1162\nPISeL\nMini\nFig. 10: Memory overhead\nV. EXPERIMENT RESULTS\nThis section presents the experimental results of Cicada’s\nperformance across different strategies by answering the ques-\ntions in Sec. IV-C.\nA. Evaluating Overall Inference Latency\nThis section details the measurement of overall inference\nlatency, defined as the total time from inference request arrival\nto response completion (excluding container initialization).\nWe compare different strategies to quantify the improvements\nachieved by Cicada in reducing end-to-end latency, as shown\nin Fig. 9.\nAccording to the results, the Preload, Mini, and Cicada\nstrategies yield various degrees of performance enhancement\nrelative to PISeL. On average, these strategies reduce latency\nby up to 6.15%, 53.41%, and 61.59% compared to PISeL,\nrespectively. This indicates that the MiniLoader component\nin Cicada plays a significant role, suggesting that the models\nexperience substantial unnecessary parameter registration and\ninitialization (see Sec. II-B). Furthermore, the average opti-\nmization provided by MiniLoader—evidenced by the perfor-\nmance gap between Cicada and Preload—is 58.57%. Specif-\nically, the VGG family benefits most from the MiniLoader,\nwith improvements ranging from 73.71% (VGG19) to 74.69%\n(VGG11); The ResNet family also exhibits gains relative to\nWeightDecoupler, with improvements ranging from 66.82%\n(ResNet152) to 69.78% (ResNet50). On the other hand, the ad-\nditional average improvement of Cicada over Mini, attributed\nto the WeightDecoupler component, is 16.21%, with the ViT\nfamily benefiting the most from this optimization, achieving\nimprovements between 15.11% (ViT-B-16) and 26.17% (ViT-\nL-16).\nOverall, among the different Cicada implementations, the\noptimization brought by MiniLoader is more pronounced,\nwhile the WeightDecoupler module further refines the weight\nloading phase through asynchronous weight retrieval and out-\nof-order weight application. This outcome is largely due to\nthe fact that the overhead associated with layer construction\nduring model loading is considerably higher—up to twice as\nhigh—than that incurred by file retrieval during weight loading\n(as observed in Fig. 5(b) and Fig. 5(c) for the VGG family).\nNonetheless, compared with the state-of-the-art pipeline solu-\ntion PISeL, Cicada and its various implementations achieve\nnotable improvements in inference latency, thereby demon-\nstrating Cicada’s effectiveness in reducing both computational\nand storage overhead.\nB. Evaluating Memory Overhead of MiniLoader\nHere, we assess the memory overhead associated with the\nMiniLoader component. In Cicada, the MiniLoader reduces\nmemory overhead during the interval between layer construc-\ntion and weight loading. In Preload, where MiniLoader is\nnot utilized, the memory overhead remains the same as in\nPISeL. Similarly, Cicada shares the same memory overhead\nas Mini, as both employ MiniLoader. Therefore, we evaluate\nthe memory overhead of Mini and PISeL in this section.\nFig. 10 presents the memory overhead and memory usage\ntime for different models across these two strategies. Memory\noverhead indicates the memory consumption during the inter-\nval between layer construction and before weight application,\nwhile memory usage time represents the cumulative duration\nacross all layers of the model during this interval.\nFrom the results, we observe that the Mini strategy extends\nmemory usage time compared to PISeL. On average, it in-\ncreases by 27.22%, with a maximum increase of 80.93% in\nResNet152. This increase stems from MiniLoader accelerating\nlayer construction, thereby imposing greater pressure on the\nweight loading phase (will be detailed in Sec. V-C).\nRegarding memory overhead, the Mini strategy exhibits\na substantial reduction compared to PISeL. Specifically, its\nmemory consumption ranges from 3.04MB to 36.27MB across\ndifferent models, amounting to only 1/32 of PISeL’s us-\nage (97.49MB to 1162MB). This reduction is attributed to\nMiniLoader replacing PISeL’s default float32 (8-byte, 32-bit)\nprecision with 1-bit precision data. This approach is feasible\nbecause these values function solely as placeholders between\nlayer construction and before weight application. Crucially,\nMiniLoader restores the parameters to their default precision\nbefore weight application to ensure accurate inference.\nIn summary, the MiniLoader component significantly re-\nduces memory overhead during request serving, thereby en-\nabling serverless platforms to accommodate a higher number\nof concurrent requests.\n8\n\n\nPISeL\nPreload\nMini\nCicada\n0\n20\n40\n60\nTime (ms)\nResNet101\nPISeL\nPreload\nMini\nCicada\n0\n25\n50\n75\nResNet152\nPISeL\nPreload\nMini\nCicada\n0\n10\n20\n30\nResNet50\nPISeL\nPreload\nMini\nCicada\n0\n200\n400\n600\nTime (ms)\nVGG11\nPISeL\nPreload\nMini\nCicada\n0\n200\n400\n600\nVGG16\nPISeL\nPreload\nMini\nCicada\n0\n200\n400\n600\nVGG19\nPISeL\nPreload\nMini\nCicada\n0\n50\n100\nTime (ms)\nViT-B-16\nPISeL\nPreload\nMini\nCicada\n0\n50\n100\nViT-B-32\nPISeL\nPreload\nMini\nCicada\n0\n200\n400\nViT-L-16\nLayer Work\nWeight Work\nWeight Wait\nCompute Work\nCompute Wait\nPreload Time\nFig. 11: Metrics breakdown for different models across strategies\nC. Breakdown of Time Overhead of Pipeline Units\nIn this section, we provide a detailed breakdown of the\ntime overhead incurred by individual pipeline units—Layer,\nWeight, and Compute—during execution. This overhead is\ndivided into two components: working time and waiting time.\nWorking time denotes the duration during which a unit actively\nperforms its designated tasks, whereas waiting time represents\nthe period during which the unit remains idle while waiting\nfor the completion of computations from the preceding unit.\nFig. 11 provides a comprehensive overview of the time\noverhead for each pipeline (layer) unit across different models\nand strategies. Specifically, it presents the average working\ntime for each layer within its corresponding pipeline unit.\nIt is noteworthy that the waiting time for the Layer unit\n(Layer Wait), which represents the interval from when the\nfunction receives the request to the start of layer construction\n(associated with pipeline resource initialization), is negligible.\nAs a result, this phase is not shown in the figure. From Fig. 11,\nwe can observe that the working time of the Layer unit is\nsignificantly higher than that of the Weight unit, which is\nconsistent with the findings in Sec. II-B.\nPISeL and the Preload strategy achieve similar layer con-\nstruction overhead (Layer Work); however, the Preload strat-\negy yields an average of 78.44% improvement in Weight Work\ncompared to PISeL across all models, specifically, ranging\nfrom 72.53% to 85.43%. This improvement is attributed to the\nWeightDecoupler, which splits the weight loading phase into\nweight file processing and weight applying, thereby shifting\nthe file retrieval overhead into the Preload Time (represented\nby the brown bars in the figure). Note that the file retrieval in\nWeightDecoupler overlaps with the layer construction process,\nso this preload time does not result in any additional overall\ntime overhead. The Mini strategy, relative to PISeL, demon-\nstrates a clear improvement in the Layer Work phase, achiev-\ning an average optimization of 63.12% compared to PISeL,\nwith individual improvements ranging from 37.99% to 75.65%\nacross different models. Above improvements are arrtibuted to\nthe MiniLoader, which reduces the layer construction overhead\nby skipping the parameter initialization phases.\nDelving deeper into the pipeline, we observe that Cicada\nand its various implementations exhibit prolonged waiting\nphases compared to PISeL. Notably, the MiniLoader com-\nponent increases both the Weight Wait and Compute Wait\ndurations, whereas the WeightDecoupler primarily extends the\nCompute Wait phase. Specifically, the Mini strategy experi-\nences increases: in the Weight Wait phase, it shows an average\ndegradation of 27.22% and reaches up to 80.93% (observed on\nResNet152); for Compute Wait, it exhibits an average degrada-\ntion of 29.96% and peaks at 87.58% (observed on ResNet101).\nThe underlying reason is that MiniLoader accelerates the layer\nconstruction process, causing the Weight Loading component\nto receive more requests per unit time, which in turn increases\n9\n\n\nResNet50\nResNet101\nResNet152VGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n25\n50\n75\n100\nPipeline Utilization (%)\nPISeL\nPreload\nMini\nCicada\nFig. 12: Pipeline utilization\nResNet50\nResNet101\nResNet152\nVGG11\nVGG16\nVGG19\nViT-B-16\nViT-L-16\nViT-B-32\n0\n600\n1200\n1800\n2400\nTime (ms)\nStrategy\nCicada\nPISeL\nTime Type\nActive Time\nTotal Pipeline Time\nFig. 13: Pipeline times comparison\nthe waiting time for weight loading tasks. Furthermore, as the\ndownstream stages cannot process these additional requests\nquickly enough, the Compute Wait phase is also prolonged,\nalbeit to a lesser extent than the Weight Wait phase. In contrast,\nthe Preload strategy demonstrates a maximum increase in\nCompute Wait of 25.89% (observed on VGG11), with an\naverage increase of 15.63%. This behavior stems from the\nWeightDecoupler, which overlaps weight file processing with\nlayer construction, thereby expediting the weight applying\nphase. However, this acceleration results in an accumulation\nof tasks in the Compute Unit, leading to increased Compute\nWait time.\nEven though MiniLoader and WeightDecoupler introduce\nthe aforementioned waiting overhead, their overall contribu-\ntion to reducing inference latency is more substantial. There-\nfore, the additional waiting overhead is considered acceptable\nin the context of the overall performance improvements.\nD. Evaluating Pipeline’s Efficiency\nThis section examines the overall efficiency of the inference\npipeline by evaluating pipeline utilization and visualizing the\npipeline timeline.\n(1) Pipeline Utilization:\nPipeline utilization represents the ratio between the union\nof the active times of the pipeline units (after merging over-\nlapping intervals) and the total pipeline execution time. This\nmetric provides an effective evaluation of the efficiency of\nresource scheduling and the degree of parallelism within the\ninference pipeline.\nFig. 12 presents the pipeline utilization for different models\nacross PISeL, Mini, Preload and Cicada. From the results\nanalysis, the pipeline utilizations of PISeL and Preload are\nsimilar, while those of the Mini and Cicada strategies are\nalso comparable. The primary distinction between these two\ngroups is that the latter incorporates the MiniLoader compo-\nnent. Specifically, the strategies with MiniLoader (Mini and\nCicada) achieve pipeline utilizations of 99.84% for ResNet,\n99.61% for VGG, and 99.95% for ViT, compared to 36.27%,\n28.29%, and 70.34% for PISeL and Preload, respectively.\nThis corresponds to improvements of 175.24%, 252.13%, and\n42.09% for ResNet, VGG, and ViT, respectively, yielding an\naverage improvement of 156.49% across the model families.\nIn the following analysis, we dissect the pipeline’s active\nand total pipeline time by categorizing the strategies into two\ngroups: strategies with MiniLoader (Cicada) and strategies\nwithout MiniLoader (PISeL). This grouping is motivated by\nthe observation that within each group, the pipeline utilization\nvalues are quite similar, allowing us to more effectively\ninvestigate the decisive contributors to pipeline utilization.\nFig. 13 illustrates the active time and total pipeline time for\nthe Cicada and PISeL strategies. The results indicate that the\ndisparity in pipeline utilization primarily stems from PISeL\nincurring a significantly longer total pipeline time compared to\nCicada. In PISeL, the total pipeline time substantially exceeds\nthe active time, suggesting that a considerable portion of the\npipeline duration is idle.\nIt is important to note that the idle time of the pipeline\ndoes not directly equate to the sum of the waiting times for\neach phase. In general, the idle time is less than the total\nwaiting time, Because the pipeline may still be executing other\ntasks while one unit is waiting—for instance, during a weight\nloading task waiting for aviable resource—the layer loading\nor inference execution units remain active. For instance, as\ndiscussed in Sec. V-C, Cicada and its various implemen-\ntations exhibit prolonged waiting phases relative to PISeL,\nlonger waiting times do not necessarily imply inefficiency.\nMoreover,Cicada’s active time is nearly equivalent to its total\npipeline time, which is the key factor enabling this strategy to\nachieve a pipeline utilization close to 100%.\n(2) Pipeline Timeline:\nFig.14 illustrates the pipeline timeline (Gantt Chart) for\nthree representative models—ResNet152, VGG19, and ViT-L-\n16—across different strategies. In each subfigure, the x-axis\nrepresents the processing time of the inference pipeline, and\nthe y-axis, arranged from top to bottom, represents the pipeline\nstages: Layer Construction (Layer), Weight File Retrieval (Re-\ntrieve), Weight Applying (Weight), and Inference Execution\n(Compute). Note that the Retrieve stage is only available\nin the Preload and Cicada strategies, which benefit from\nthe WeightDecoupler component. Additionally, in the Preload\nand PISeL strategies, hatched bars represent the processing\ntime of parameter registration and initialization (see Sec.II-B).\nMoreover, different colors are used to distinguish between\nlayers, as defined by the PyTorch model specification.\n10\n\n\nCompute\nWeight\nRetrieve\nLayer\nCicada\nCompute\nWeight\nRetrieve\nLayer\nPreload\nCompute\nWeight\nRetrieve\nLayer\nMini\n0\n100\n200\n300\n400\n500\n600\n700\n800\nCompute\nWeight\nRetrieve\nLayer\nPISeL\nTime (ms)\nWeight Init\nrelu\nmaxpool\navgpool\nfc\nconv1\nbn1\nlayer1\nlayer2\nlayer3\nlayer4\n(a) ResNet152\nCompute\nWeight\nRetrieve\nLayer\nCicada\nCompute\nWeight\nRetrieve\nLayer\nPreload\nCompute\nWeight\nRetrieve\nLayer\nMini\n0\n500\n1000\n1500\n2000\nCompute\nWeight\nRetrieve\nLayer\nPISeL\nTime (ms)\nWeight Init\nfeatures\navgpool\nclassifier\n(b) VGG19\nCompute\nWeight\nRetrieve\nLayer\nCicada\nCompute\nWeight\nRetrieve\nLayer\nPreload\nCompute\nWeight\nRetrieve\nLayer\nMini\n0\n500\n1000\n1500\n2000\nCompute\nWeight\nRetrieve\nLayer\nPISeL\nTime (ms)\nWeight Init\nconv_proj\nclass_token\nencoder\nheads\n(c) ViT-L-16\nFig. 14: Pipeline timeline for different models across strategies\nFrom Fig. 14, it is evident that Cicada delivers a significant\nenhancement in the inference pipeline compared to other\nstrategies.\nComparing the Preload and Cicada strategies, where the\nMiniLoader component is the only difference, we can see that\nthe Layer stage is significantly reduced, therefore advancing\nthe Retrieve, Weight and Compute start time. Similarly, Mini\nreduces the Layer procedure compared to PISeL, showing the\nfeasibility of the MiniLoader. This advancement indicates that\nthe MiniLoader component effectively eliminates the param-\neter registration and initialization phases, which are the most\ntime-consuming operations. Comparing the Mini and Cicada\nstrategies, where the WeightDecoupler component is the only\ndifference, we can see that with the ability of overlapping the\nRetrieve stage with the Layer stage, in turns,the Weight stage\ncan be executed immediately after finishing the Layer stage,\nwhich further advances the Compute stage. When it comes to\nPISeL and Preload, where the WeightDecoupler component is\nthe only difference, we can see that the Retrieve stage reduces\nthe overall inference latency in VGG19 and ViT-L-16. The\nreason the Preload fails to achieve optimization in ResNet152\nis that the Layer stage is almost contributing to the overall\nlatency, whereas the Weight stage is relatively short. Therefore,\nthe benefit of the WeightDecoupler is overwhelmed by the\nLayer stage.\nWhen comparing the Preload and Cicada strategies, where\nthe only difference is the presence of the MiniLoader compo-\nnent, the Layer stage is notably reduced, thereby advancing\nthe start times of the Retrieve, Weight, and Compute stages.\nSimilarly, the Mini strategy reduces the duration of the Layer\nprocedure relative to PISeL, demonstrating the feasibility of\nthe MiniLoader component. This advancement indicates that\nthe MiniLoader effectively eliminates the parameter registra-\ntion and initialization phases, which are among the most time-\nconsuming operations.\nFurthermore, comparing the Mini and Cicada strategies,\nwhere the only difference is the WeightDecoupler component,\nreveals that overlapping the Retrieve stage with the Layer\nstage allows the Weight stage to commence immediately after\nthe Layer stage, which further advances the Compute stage.\nNotably, with the WeightDecoupler in place, Cicada prorcess\nthe weight applying in Weight stage immediately after the\nLayer stage is finished, whereas the Mini strategy still needs\nto wait for the weight file processing to finish, which is the\nbottleneck of the weight loading phase.\nLastly, when examining PISeL and Preload, where the\nWeightDecoupler component distinguishes the two, the addi-\ntion of the Retrieve stage results in a reduction of overall infer-\nence latency in VGG19 and ViT-L-16. However, the Preload\nstrategy fails to achieve optimization in ResNet152 because the\nLayer stage predominantly contributes to the overall latency,\nwhile the Weight stage is relatively short, thereby diminishing\nthe potential benefits of the WeightDecoupler.\nVI. RELATED WORK\nExtensive research has focused on optimizing cold start\nlatency in serverless computing, mainly through runtime-level\noptimizations [15], [14], [16], [33], [34] and application-level\nstrategies [35], [36], [24], [37], [38].\nIn serverless DL inference, the cold start problem is further\nexacerbated by the overhead of loading large machine learn-\ning models into memory [21], [4], [17], [39]. To accelerate\nmodel inference, prior works have investigated resource reuse\nstrategies. Tetris [18] and Optimus [19] are two notable ex-\namples: Tetris enhances memory efficiency by sharing tensors\nacross model-serving instances, while Optimus reduces model\nloading overhead by restructuring layers across inter-function\ninvocations. However, both approaches introduce inherent\ntrade-offs: Tetris necessitates a centralized tensor pool within\nthe serverless platform, doubling memory overhead (due to\ntensor pool allocation alongside container memory), whereas\nOptimus relies on idle containers to retain model structures,\nincreasing memory consumption. In contrast, Cicada optimizes\nlayer construction by addressing inefficiencies in parameter\nregistration and initialization. Through precision adjustment\nand selective initialization bypassing, it reduces both memory\n11\n\n\nfootprint and construction time, offering a more lightweight\nand efficient alternative.\nBeyond traditional model loading optimizations, researchers\nhave explored pipeline-based designs to enhance inference\nefficiency. SplitWise [40] partitions compute-intensive prompt\nprocessing and memory-intensive token generation across\ndifferent devices, employing pipelining techniques for KV-\ncache transmission. ServerlessLLM [41] introduces a server-\nless LLM inference framework that leverages checkpointing\nto expedite model loading and live migration strategies to\ndynamically allocate checkpoints across devices, maximizing\nserverless scalability. While these approaches primarily focus\non optimizing resource allocation and scheduling during in-\nference, Cicada complements them by specifically improving\nmodel loading efficiency, contributing to a more comprehen-\nsive optimization strategy.\nSeveral studies have also investigated pipelined model load-\ning for improved efficiency. DistMind [42] and Demand Lay-\nering [43] partition the traditionally sequential model loading\nprocess into three stages, leveraging SSD hardware and zero-\ncopy techniques for parameter retrieval. Similarly, PISeL [23]\nintroduces a three-stage pipeline tailored for serverless envi-\nronments. However, existing pipeline designs overlook two\ncritical aspects: (1) the unnecessary overhead incurred during\nlayer construction in inference scenarios and (2) the feasibility\nof decoupling I/O operations from computational tasks in\nthe weight-loading phase. Cicada addresses these limitations\nthrough its MiniLoader and WeightDecoupler components,\nrespectively, pushing the frontier of pipeline-based model\nloading optimization.\nVII. CONCLUSIONS\nThis paper presents Cicada, a pipeline optimization frame-\nwork for serverless machine learning inference. By system-\natically analyzing the bottlenecks in existing pipeline-based\nmodel loading, Cicada introduces innovative optimizations\nacross computational, storage, and scheduling dimensions.\nFirst, the MiniLoader component significantly reduces layer\nconstruction overhead through a lightweight parameter ini-\ntialization strategy. Second, the WeightDecoupler decouples\nweight file processing from layer construction, enabling asyn-\nchronous weight prefetching and out-of-order weight appli-\ncation. Finally, the priority-aware scheduler dynamically al-\nlocates resources to ensure timely execution of high-priority\ninference tasks. Experimental results demonstrate that Cicada\nachieves substantial improvements in inference latency and\npipeline utilization, effectively addressing the performance\nbottlenecks in traditional pipeline-based model loading.\nREFERENCES\n[1] H. Shafiei, A. Khonsari, and P. Mousavi, “Serverless computing:\nA\nsurvey\nof\nopportunities,\nchallenges,\nand\napplications,”\nACM\nComput. Surv., vol. 54, no. 11s, Nov. 2022. [Online]. Available:\nhttps://doi.org/10.1145/3510611\n[2] Z. Li, L. Guo, J. Cheng, Q. Chen, B. He, and M. Guo, “The serverless\ncomputing survey: A technical primer for design architecture,” ACM\nComput. Surv., vol. 54, no. 10s, Sep. 2022. [Online]. Available:\nhttps://doi.org/10.1145/3508360\n[3] Y. Li, Y. Lin, Y. Wang, K. Ye, and C. Xu, “Serverless computing: State-\nof-the-art, challenges and opportunities,” IEEE Transactions on Services\nComputing, vol. 16, no. 2, pp. 1522–1539, 2023.\n[4] Q. Pei, Y. Yuan, H. Hu, Q. Chen, and F. Liu, “Asyfunc: A\nhigh-performance and resource-efficient serverless inference system via\nasymmetric functions,” in Proceedings of the 2023 ACM Symposium on\nCloud Computing, ser. SoCC ’23.\nNew York, NY, USA: Association\nfor Computing Machinery, 2023, pp. 324–340. [Online]. Available:\nhttps://doi.org/10.1145/3620678.3624664\n[5] J. Gu, Y. Zhu, P. Wang, M. Chadha, and M. Gerndt, “Fast-gshare:\nEnabling efficient spatio-temporal gpu sharing in serverless computing\nfor deep learning inference,” in Proceedings of the 52nd International\nConference on Parallel Processing, ser. ICPP ’23.\nNew York, NY,\nUSA: Association for Computing Machinery, 2023, pp. 635–644.\n[Online]. Available: https://doi.org/10.1145/3605573.3605638\n[6] “Amazon sagemaker,” Accessed: Feb, 7, 2025. [Online]. Available:\nhttps://aws.amazon.com/sagemaker/\n[7] “Azure machine learning,” Accessed: Feb, 7, 2025. [Online]. Available:\nhttps://azure.microsoft.com/en-us/products/machine-learning\n[8] “Google cloud vertex ai,” Accessed: Feb, 7, 2025. [Online]. Available:\nhttps://cloud.google.com/vertex-ai\n[9] Y. Wu, T. T. A. Dinh, G. Hu, M. Zhang, Y. M. Chee, and B. C.\nOoi, “Serverless data science - are we there yet? a case study of\nmodel serving,” in Proceedings of the 2022 International Conference\non Management of Data, ser. SIGMOD ’22.\nNew York, NY, USA:\nAssociation for Computing Machinery, 2022, pp. 1866–1875. [Online].\nAvailable: https://doi.org/10.1145/3514221.3517905\n[10] Y. Yang, L. Zhao, Y. Li, H. Zhang, J. Li, M. Zhao, X. Chen, and K. Li,\n“Infless: a native serverless system for low-latency, high-throughput\ninference,” in Proceedings of the 27th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems, ser. ASPLOS ’22.\nNew York, NY, USA: Association\nfor Computing Machinery, 2022, pp. 768–781. [Online]. Available:\nhttps://doi.org/10.1145/3503222.3507709\n[11] “Docker container,” Accessed: Feb, 21, 2025. [Online]. Available:\nhttps://www.docker.com/\n[12] X. Cai, Q. Sang, C. Hu, Y. Gong, K. Suo, X. Zhou, and D. Cheng, “In-\ncendio: Priority-based scheduling for alleviating cold start in serverless\ncomputing,” IEEE Transactions on Computers, pp. 1–14, 2024.\n[13] L. Pan, L. Wang, S. Chen, and F. Liu, “Retention-aware container\ncaching for serverless edge computing,” in Proceedings of IEEE Confer-\nence on Computer Communications (INFOCOM), 2022, pp. 1069–1078.\n[14] H. Yu, R. Basu Roy, C. Fontenot, D. Tiwari, J. Li, H. Zhang, H. Wang,\nand S.-J. Park, “Rainbowcake: Mitigating cold-starts in serverless with\nlayer-wise container caching and sharing,” in Proceedings of the 29th\nACM International Conference on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS), 2024, pp. 335–350.\n[15] Z. Li, L. Guo, Q. Chen et al., “Help Rather Than Recycle: Alleviating\nCold Startup in Serverless Computing Through Inter-Function Container\nSharing,” in Proceedings of the 33rd USENIX Annual Technical Con-\nference (ATC), 2022, pp. 69–84.\n[16] D. Du, T. Yu, Y. Xia et al., “Catalyzer: Sub-millisecond startup for\nserverless computing with initialization-less booting,” in Proceedings\nof the 25th International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), 2020, pp.\n467–481.\n[17] Y. Sui, H. Yu, Y. Hu, J. Li, and H. Wang, “Pre-warming is\nnot enough: Accelerating serverless inference with opportunistic pre-\nloading,” in Proceedings of the 2024 ACM Symposium on Cloud\nComputing, ser. SoCC ’24.\nNew York, NY, USA: Association\nfor Computing Machinery, 2024, pp. 178–195. [Online]. Available:\nhttps://doi.org/10.1145/3698038.3698509\n[18] J. Li, L. Zhao, Y. Yang, K. Zhan, and K. Li, “Tetris: Memory-\nefficient\nserverless\ninference\nthrough\ntensor\nsharing,”\nin\n2022\nUSENIX Annual Technical Conference (USENIX ATC 22).\nCarlsbad,\nCA: USENIX Association, Jul. 2022. [Online]. Available: https:\n//www.usenix.org/conference/atc22/presentation/li-jie\n[19] Z. Hong, J. Lin, S. Guo, S. Luo, W. Chen, R. Wattenhofer, and\nY. Yu, “Optimus: Warming serverless ml inference via inter-function\nmodel transformation,” in Proceedings of the Nineteenth European\nConference on Computer Systems, ser. EuroSys ’24.\nNew York, NY,\nUSA: Association for Computing Machinery, 2024, pp. 1039–1053.\n[Online]. Available: https://doi.org/10.1145/3627703.3629567\n[20] M. Shahrad, R. Fonseca, ´I. Goiri et al., “Serverless in the wild:\nCharacterizing and optimizing the serverless workload at a large cloud\nprovider,” in Proceedings of the 31st USENIX Annual Technical Con-\nference (ATC), 2020, pp. 205–218.\n12\n\n\n[21] M. Golec, G. K. Walia, M. Kumar, F. Cuadrado, S. S. Gill, and\nS. Uhlig, “Cold start latency in serverless computing: A systematic\nreview, taxonomy, and future directions,” ACM Comput. Surv., vol. 57,\nno. 3, Nov. 2024. [Online]. Available: https://doi.org/10.1145/3700875\n[22] D. Saxena, T. Ji, A. Singhvi, J. Khalid, and A. Akella, “Memory\ndeduplication for serverless computing with medes,” in Proceedings\nof the Seventeenth European Conference on Computer Systems, ser.\nEuroSys ’22.\nNew York, NY, USA: Association for Computing\nMachinery, 2022, pp. 714–729. [Online]. Available: https://doi.org/10.\n1145/3492321.3524272\n[23] M. Rahimi Jafari, J. Su, Y. Zhang, O. Wang, and W. Zhang, “PISeL:\nPipelining DNN Inference for Serverless computing,” in Proceedings of\nthe 33rd ACM International Conference on Information and Knowledge\nManagement, ser. CIKM ’24.\nNew York, NY, USA: Association\nfor Computing Machinery, 2024, pp. 1951–1960. [Online]. Available:\nhttps://doi.org/10.1145/3627673.3679824\n[24] J. Stojkovic, T. Xu, H. Franke, and J. Torrellas, “MXFaaS: Resource\nsharing in serverless environments for parallelism and efficiency,” in\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture (ISCA), 2023.\n[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2016.\n[26] K. Simonyan and A. Zisserman, “Very deep convolutional networks\nfor large-scale image recognition,” 2015. [Online]. Available: https:\n//arxiv.org/abs/1409.1556\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\n[28] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification,” in\nProceedings of the IEEE International Conference on Computer Vision\n(ICCV), December 2015.\n[29] G. Aupy, A. Gainaru, and V. L. F`evre, “I/o scheduling strategy for\nperiodic applications,” ACM Trans. Parallel Comput., vol. 6, no. 2, Jul.\n2019. [Online]. Available: https://doi.org/10.1145/3338510\n[30] I. Sa˜nudo, R. Cavicchioli, N. Capodieci, P. Valente, and M. Bertogna, “A\nsurvey on shared disk i/o management in virtualized environments under\nreal time constraints,” SIGBED Rev., vol. 15, no. 1, pp. 57–63, Mar.\n2018. [Online]. Available: https://doi.org/10.1145/3199610.3199618\n[31] F. Chen, B. Hou, and R. Lee, “Internal parallelism of flash memory-\nbased solid-state drives,” ACM Trans. Storage, vol. 12, no. 3, May\n2016. [Online]. Available: https://doi.org/10.1145/2818376\n[32] Y. Zhang, I. n. Goiri, G. I. Chaudhry, and oyhers, “Faster and cheaper\nserverless computing on harvested resources,” in Proceedings of the 28th\nACM Symposium on Operating Systems Principles, 2021, pp. 724–739.\n[33] Y. Lan, X. Peng, and Y. Wang, “Snapipeline: Accelerating snapshot\nstartup for faas containers,” in Proceedings of the 2024 ACM\nSymposium on Cloud Computing, ser. SoCC ’24.\nNew York, NY,\nUSA: Association for Computing Machinery, 2024, pp. 144–159.\n[Online]. Available: https://doi.org/10.1145/3698038.3698513\n[34] W. Shin, W.-H. Kim, and C. Min, “Fireworks: a fast, efficient, and safe\nserverless framework using vm-level post-jit snapshot,” in Proceedings\nof the 17th European Conference on Computer Systems (EuroSys), 2022,\npp. 663–677.\n[35] Z. Wu, Y. Deng, Y. Zhou, J. Li, and S. Pang, “Faasbatch: Enhancing the\nefficiency of serverless computing by batching and expanding functions,”\nin 2023 IEEE 43rd International Conference on Distributed Computing\nSystems (ICDCS), 2023, pp. 372–382.\n[36] Z. Wu, Y. Deng, Y. Zhou, L. Cui, and X. Qin, “Hashcache: Accelerating\nserverless computing by skipping duplicated function execution,” IEEE\nTransactions on Parallel and Distributed Systems, vol. 34, no. 12, pp.\n3192–3206, 2023.\n[37] V. M. Bhasi, J. R. Gunasekaran, P. Thinakaran et al., “Kraken: Adap-\ntive container provisioning for deploying dynamic dags in serverless\nplatforms,” in Proceedings of the 12th ACM Symposium on Cloud\nComputing (SoCC), 2021, pp. 153–167.\n[38] A. Fuerst and P. Sharma, “FaaSCache: Keeping serverless computing\nalive with greedy-dual caching,” in Proceedings of the 26th ACM\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), 2021, pp. 386–400.\n[39] L. Wang, Y. Jiang, and N. Mi, “Advancing serverless computing\nfor\nscalable\nai\nmodel\ninference:\nChallenges\nand\nopportunities,”\nin Proceedings of the 10th International Workshop on Serverless\nComputing, ser. WoSC10 ’24.\nNew York, NY, USA: Association\nfor\nComputing\nMachinery,\n2024,\npp.\n1–6.\n[Online].\nAvailable:\nhttps://doi.org/10.1145/3702634.3702950\n[40] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and\nR. Bianchini, “Splitwise: Efficient generative llm inference using phase\nsplitting,” in 2024 ACM/IEEE 51st Annual International Symposium on\nComputer Architecture (ISCA), 2024, pp. 118–132.\n[41] Y. Fu, L. Xue, Y. Huang, A.-O. Brabete, D. Ustiugov, Y. Patel,\nand L. Mai, “Serverlessllm: Low-latency serverless inference for\nlarge language models,” in 18th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 24).\nSanta Clara, CA:\nUSENIX Association, Jul. 2024, pp. 135–153. [Online]. Available:\nhttps://www.usenix.org/conference/osdi24/presentation/fu\n[42] X. Jin, Z. Bai, Z. Zhang, Y. Zhu, Y. Zhong, and X. Liu, “Distmind: Effi-\ncient resource disaggregation for deep learning workloads,” IEEE/ACM\nTransactions on Networking, vol. 32, no. 3, pp. 2422–2437, 2024.\n[43] M. Ji, S. Yi, C. Koo, S. Ahn, D. Seo, N. Dutt, and J.-C. Kim, “Demand\nlayering for real-time dnn inference with minimized memory usage,” in\n2022 IEEE Real-Time Systems Symposium (RTSS), 2022, pp. 291–304.\n13\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20959v1.pdf",
    "total_pages": 13,
    "title": "Cicada: A Pipeline-Efficient Approach to Serverless Inference with Decoupled Management",
    "authors": [
      "Z. Wu",
      "Y. Deng",
      "J. Hu",
      "L. Cui",
      "Z. Zhang",
      "L. Zeng",
      "G. Min"
    ],
    "abstract": "Serverless computing has emerged as a pivotal paradigm for deploying Deep\nLearning (DL) models, offering automatic scaling and cost efficiency. However,\nthe inherent cold start problem in serverless ML inference systems,\nparticularly the time-consuming model loading process, remains a significant\nbottleneck. Utilizing pipelined model loading improves efficiency but still\nsuffer from pipeline stalls due to sequential layer construction and monolithic\nweight loading. In this paper, we propose \\textit{Cicada}, a novel pipeline\noptimization framework that coordinates computational, storage, and scheduling\nresources through three key mechanisms: (1) \\textit{MiniLoader}: which reduces\nlayer construction overhead by opportunistically optimizing parameter\ninitialization; (2) \\textit{WeightDecoupler}: decoupling weight file processing\nfrom layer construction, enabling asynchronous weight retrieval and\nout-of-order weight application; (3) \\textit{Priority-Aware Scheduler}:\ndynamically allocating resources to ensure high-priority inference tasks are\nexecuted promptly. Our experimental results demonstrate that Cicada achieves\nsignificant performance improvements over the state-of-the-art PISeL framework.\nSpecifically, Cicada reduces end-to-end inference latency by an average of\n61.59\\%, with the MiniLoader component contributing the majority of this\noptimization (53.41\\%), and the WeightDecoupler achieves up to 26.17\\%\nimprovement. Additionally, Cicada achieves up to 2.52x speedup in the inference\npipeline utlization compared to PISeL.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}