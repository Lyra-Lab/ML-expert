{
  "id": "arxiv_2502.20647v1",
  "text": "CONSISTENCY EVALUATION OF NEWS ARTICLE SUMMARIES\nGENERATED BY LARGE (AND SMALL) LANGUAGE MODELS\nColleen Gilhuly, Haleh Shahzad\nRoyal Bank of Canada\nChief Data Office\nToronto\n{colleen.gilhuly, haleh.shahzad}@rbc.com\nABSTRACT\nText summarizing is a critical Natural Language Processing (NLP) task with applications ranging from\ninformation retrieval to content generation. Large Language Models (LLMs) have shown remarkable\npromise in generating fluent abstractive summaries but they can produce hallucinated details not\ngrounded in the source text. Regardless of the method of generating a summary, high quality\nautomated evaluations remain an open area of investigation. This paper embarks on an exploration of\ntext summarization with a diverse set of techniques, including TextRank, BART, Mistral-7B-Instruct,\nand OpenAI GPT-3.5-Turbo. The generated summaries are evaluated using traditional metrics such\nas the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score and Bidirectional Encoder\nRepresentations from Transformers (BERT) Score, as well as LLM-powered evaluation methods\nthat directly assess a generated summary’s consistency with the source text. We introduce a meta\nevaluation score which directly assesses the performance of the LLM evaluation system (prompt +\nmodel). We find that that all summarization models produce consistent summaries when tested on the\nXL-Sum dataset, exceeding the consistency of the reference summaries.\nKeywords Text Summarization · Large Language Models · Natural Language Processing · Evaluation · Hallucination\n1\nIntroduction\nThe rapid growth of domain-specific knowledge has posed unprecedented challenges in efficiently utilizing the\ninformation. With the increasing volume and diversity of such information, generating specific and precise summaries\nis critical. An informative summary is useful as a primer and can be used in various downstream applications or for\nindividuals who may not have time to read the entire original document. Ultimately, the goal of a summary is to enable\nbetter understanding with less time and effort.\nThe gold standard of summarization is human-written summaries, but there are many limitations in scalability, cost-\neffectiveness, and consistency. Writing a summary is a high cognitive load task and becomes much more difficult as\nthe length of the source text increases. Some technical topics may require some subject matter expertise in addition to\nlanguage proficiency, increasing the cost of the summarization task. Furthermore, human summaries are very subjective\nand variable which makes this approach to some extent unreliable. Machine-generated summaries can be created\non-demand and can be more easily scaled due to lower cost and time requirements. This has been an area of interest in\nNLP research for decades.\nSummaries can be divided into two types: extractive summaries which contain exact subsets of the source text, and\nabstractive summaries which contain new phrases and words that are not found in the source text. Historically, most\nsummary models have been extractive in nature. Extractive summarization is considered as a sequence classification\nproblem wherein each sentence is visited sequentially in the source document order and a binary decision is made\n(taking into account previous decisions made) in terms of whether or not it should be included in the summary.\nThere are many examples of models trained for extractive summarization. For instance, [1] presents SummaRuNNer,\nwhich uses a two-layer bi-directional GRU-RNN sequence model for extractive summarization of documents. In [2],\narXiv:2502.20647v1  [cs.CL]  28 Feb 2025\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\ntextual units are selected to include in the summary based on a rich set of sparse features whose weights are learned on\na large corpus. [3] presents a conditional recurrent neural network, which acts as a decoder to generate the summary of\nan input sentence.\nAbstractive summarization is a sequence-to-sequence problem where the output sequence (summary) generation is\nconditioned on the input sequence (source document). There are many instances of Transformer models of various\narchitectures being trained for abstractive summarization. In [4], a BERT model is used for both abstractive and\nextractive summaries. For the extractive summary, the model acts as a classifier that predicts the sentences that should\nbe included in the summary. To generate the abstractive summary, the problem is formulated as sequence-to-sequence\nproblem with a standard encoder-decoder architecture. In [5], a dataset containing one million news article-summary\npairs (XL-Sum) is collected and used for fine-tuning the mT5 model [6]. [7] introduces BART, which learns to\nreconstruct the original text after it has been corrupted by a noise function.\nIn the past few years, Large Language Models (LLMs) have risen to prominence as highly fluent text generators which\nare capable of a variety of tasks without specific task training. Instead, instructions and examples can be provided at\ninference time to adapt the model to the task at hand [8,9]. LLMs are able to generate coherent text with a great deal\nof flexibility on the style, format, and length of output, making them very well suited to a variety of summarization\napplications. One of the primary risks of using LLMs is their ability to generate hallucinations, which can take many\nforms and have varying levels of severity [10]. For closed text summarization tasks, it is not desirable for the model to\ninclude general world knowledge seen during training even if it is correct and relevant to the source text. And yet, this\nis one of the most benign forms of hallucination. Incorrect facts or biased inferences can also appear in LLM-generated\ntexts.\nWhether adopting an abstractive or extractive approach, it is often difficult to generate a high quality summary and even\nmore difficult to automatically evaluate the quality of a summary in a relevant way. Criteria used in human evaluation\nare intangible or open-ended, and there are many ways to write a good summary. The perceived quality of a summary\nalso depends on the context and the intended audience. Less subjective criteria like factual consistency are still difficult\nto evaluate automatically. Consistency remains an important metric for model-generated summaries and other texts\nbecause of LLMs’ propensity for hallucinations.\nIn this paper, we present an exploration of summary generation for news articles using a broad variety of both extractive\nand abstractive methods. We evaluate and compare the performance of these methods using a variety of techniques\nincluding standard metrics as well as LLM-powered approaches that directly evaluate consistency and hallucination.\nWe also introduce reference-based meta evaluations which assess the overall performance of the evaluation systems\n(prompts and evaluator LLM). Beyond establishing correlation with human judgment, it is important to know the\naccuracy of LLM-powered evaluations.\n2\nData\nWe use the XL-Sum [5] dataset as the basis for our exploration of summarization and subsequent evaluation. XL-Sum\nhas collected one million pairs of news articles and summaries from the BBC website. The summaries are identified\namong the full article webpage contents by their bolded formatting. They are written by the same author as the main\narticle and are abstractive rather than extractive. The XL-Sum dataset covers 44 languages; however English is our only\nlanguage of interest for this work. There are 301k English article-summary pairs available in the dataset, with 11.5k in\nthe test split.\nWe exclude articles that are over 400 words long in order to make fair comparisons with smaller Transformer-based\nsummarization models with a limited context window. This reduces the test split to 6484 article-summary pairs.\nIn XL-Sum, 37.37% of manually reviewed English summaries were found to contain information that cannot be inferred\nfrom the article [5]. This is a natural consequence of human authors applying their real-world background knowledge in\nwriting the summary, but the authors note that this can also happen when the summary introduces information that is\nthen abbreviated in the article (e.g. acronyms or referring to someone by their surname only) or for certain types of\narticles (e.g. blogs and opinion pieces). For a model-generated summary this would be labeled as hallucination, and we\ntherefore wish to avoid these examples where possible.\nWe note that spot-checked XL-Sum articles shorter than 100 words very frequently had some information in the\nsummary that was not present in the article. For these short articles, the extracted summary is closer in style to a\nfirst-line summary, as in the XSum dataset [11]. These short articles tend to cover routine police blotter reporting or\nphotos.\n2\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nWe impose a minimum article length of 100 words in order to avoid this high concentration of summaries with lower\nfactual overlap. 114 article-summary pairs are removed in this way, bringing our final sample size to 6370. This\nsimple cut does not remove all instances of this problem but does offer some improvement. We will return to evaluate\nthe factual consistency of the XL-Sum reference summaries in Section 5. No filtering or cleaning was applied to the\nXL-Sum test dataset other than the requirements on article length described above.\n3\nSummarization methods\nIn this section, we introduce the models that we use to generate summaries in this work. Our objective is to sample a\nbroad variety of models. The models include both extractive and abstractive approaches, and Transformer-based models\nof a variety of sizes. When selecting models to include, we gave some preference to model flavours that have a high\nnumber of all-time downloads on HuggingFace, therefore capturing popular models across several years.\nWith the rapid improvements to existing LLMs and release of new models, the models we have tested will be behind the\ncutting edge of performance by the time this paper is published. This does not diminish the value of the comparison of\nsmaller models such as T5 and BART with multi-billion parameter LLMs such as Llama and GPT-3.5-Turbo. The latest\nmodels may not be available for on-premises deployments and smaller models can shine as cost-effective solutions in\nmany use cases.\n3.1\nTextRank\nTextRank [12] is a graph-based ranking model inspired by PageRank [13] that can be used in variety of NLP applications\nincluding extractive summaries. A graph is constructed where each sentence in a document is represented by a vertex\nand some measure of similarity represents the edges. Sentences that are highly similar to many other sentences receive\na higher score and are considered more important. TextRank is better described as an algorithm than as a model, but for\nbrevity we will collectively refer to TextRank and the models below as “the models” throughout this paper.\nIn our implementation of TextRank, we use SpaCy to split the article texts into sentences with their associated embedding\nvectors. Cosine similarity of the sentence embedding vectors is adopted as the similarity metric. We make use of the\npagerank algorithm implementation in the python package NetworkX to produce scores. The top two sentences are\ntaken as the TextRank summary, since summaries in the XL-Sum dataset are typically 1-2 sentences long.\n3.2\nT5-small\nThe Text-to-Text Transfer Transformer (T5) model architecture offers a unified text-to-text framework where the input\nand output are always text strings, in contrast to BERT-style models [14]. This allows a single model to be trained on a\nvariety of NLP tasks using the same loss function and hyperparameters, building transfer learning into the pre-training\nprocess.\nWe select a T5-small model which has been fine tuned for summarization (https://huggingface.co/Falconsai/\ntext_summarization). The datasets used for fine-tuning are undisclosed. This model has a soft input token limit\nof 512 tokens, the lowest of all models covered in this work and therefore setting the maximum article length of our\nsample.\n3.3\nBART-Large-CNN\nThe Bidirectional and Auto-Regressive Transformers (BART) model [7] is a sequence to sequence model named for its\nbidirectional (BERT-like) encoder and autoregressive (GPT-like) decoder. BART is pre-trained by corrupting text by\nmasking tokens and shuffling sentences, and then learning a model to reconstruct the original text. The pre-training data\nincludes 160 GB of news, books, stories, and internet text. We select a BART model fine-tuned on the CNN Daily Mail\ndataset (https://huggingface.co/datasets/abisee/cnn_dailymail).\n3.4\nMistral-7B-Instruct\nMistral-7B [15] is an LLM designed to balance performance and efficiency. It makes use of grouped-query attention to\nimprove inference speed and sliding window attention to reduce the computational cost of long sequences. Mistral-7B\nis trained on openly available but undisclosed text sources. The base model is then fine-tuned on instruction datasets\nfrom HuggingFace and the resulting model is named Mistral-7B-Instruct.\n3\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\n3.5\nLlama3-8B-Instruct\nMeta’s Llama3 set of models [16] aim to provide a high quality set of foundation models by focusing on increased\nquantity and quality of training data and simplifying architecture to enable training at scale. Their cleaned and curated\npre-training data includes about 15 trillion tokens in total, with a cutoff date of March 2023. The fine-tuning data\nincludes publicly available instruction datasets, as well as over 10M human-annotated examples. We select the smallest\nof the Llama3 set, Llama-8B-Instruct.\n3.6\nFalcon-40B-Instruct\nThe Falcon series of models [17] offer open training data and detailed pre-training information in addition to open\nmodel weights in order to foster further research in the field. The training dataset, RefinedWeb [18], consists of filtered\nand de-duplicated web text supplemented with curated datasets. The 40B size is pre-trained on 1 trillion tokens.\n3.7\nGPT-3.5-Turbo\nOpenAI’s GPT-3.5-Turbo model is a further refined version of InstructGPT [8, 9] and is closely related to the first\nreleased version of the ChatGPT service which ignited the ongoing and widespread excitement for LLMs. Details\naround the model architecture and training data/procedure are undisclosed. We access this model via Microsoft Azure.\n3.8\nSummary prompt\nFor simplicity, we use the same basic summarization prompt with minimal instructions for all of the LLMs (Mistral-7B,\nLlama3-8B, Falcon-40B, and GPT-3.5-Turbo): “Write a 1-2 sentence summary of the article above.” This may not lead\nto the best performance for each individual model but it does provide a shared basis for comparison with the pre-trained\nsummarization models which do not benefit from any specialized instructions.\nThe actual input to each model varies according to their chat tokenization or recommended tags to indicate instructions\n(i.e. »CONTEXT«, »QUESTION«, and »ANSWER« for Falcon-40B; [INST] and [\\INST] for Mistral-7B).\nAll LLM summaries and evaluations are run with temperature set to zero.\n4\nPerformance Evaluation methods\nIn this section we introduce the methods used to evaluate the model-generated summaries. The evaluation methods\ninclude standard similarity-based metrics and LLM-as-evaluator metrics.\nThe standard metrics have been fading in popularity as the field of machine text summarization has advanced and as it\nhas become clearer that these metrics have low correlation with human judgment [19–23]. LLM metrics offer a faster\nand cheaper alternative to human evaluations that can be designed to target specific criteria.\nSome LLM evaluations involve prompting the evaluator model to directly output a numerical rating for subjective but\nmeaningful properties (such as coherence and fluency) that contribute to the perceived quality of a summary [24–26].\nThese evaluations can also be structured as choosing the best of two candidates for a given property [26,27]. While\nthese approaches are promising, LLMs have been demonstrated to be inconsistent evaluators, exhibiting differing\ndegrees of alignment with human evaluations depending on the candidate or property being assessed [27]. LLMs are\nalso subject to various biases when acting as evaluators [28]. The current state of LLM evaluations can distinguish\nbetween good and bad summaries but not which of the good summaries is the best [27,29].\nEvaluations of factual consistency are complimentary to the above metrics which are focused on the perceived quality\nof a summary. In this work we implement two structured consistency evaluations which are quantitative without asking\nthe model to directly generate a number.\n4.1\nStandard Metrics\nStandard approaches to evaluating generated text are primarily based on counting n-gram overlap. These methods\nassume access to one or more reference texts and score a generated text based on the precision and recall of all reference\nn-grams.\nROUGE-N [30] measures the proportion of N-grams that occur in both the generated summary and the reference\nsummary. We use N = 1, ROUGE-1, which operates on single words. ROUGE-L, another commonly used variant, is\n4\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nFigure 1: Overview of the LLM-powered QA evaluation.\nrelated to the length of the longest common sub-sequence between the generated and reference summaries. The words\nin the sub-sequence do not need to be consecutive but must appear in the same order. For both ROUGE metrics, we\nreport the F1 score.\nBecause the ROUGE metrics are dependent on exact word choice in the generated summary, a generated summary\nconveying the same information with different wording would not score highly. This word-specific weakness is\nalleviated with BERTScore [31], which compares the semantic similarity of words by leveraging an embedding model\nas opposed to relying on exact matches.\nBoth metrics are unable to detect factual inconsistencies or model hallucinations, which are among the greatest concerns\nwith LLM-generated summaries. These metrics also rely on one or more reference texts for comparison, and it can be\nexpensive and challenging to obtain high-quality references summaries.\n4.2\nQuestion-Answer Evaluation\nOur first LLM-powered consistency evaluation is the question-answer (QA) evaluation. Earlier works relied on task-\nspecific models for question generation and question answering [22, 32–35]. Challenges of this approach included\nfiltering out low quality questions and measuring similarity of generated answers. The advent of LLMs has made fluent\nquestion generation and answering much easier to implement [36], though the risk of hallucinations during these tasks\nremains.\nWhether using task-specific models or an LLM evaluator as in this work, QA evaluations present a promising alternative\nto standard evaluation metrics. The method is based on the intuition that if we ask questions about a summary and its\nsource, we will receive similar answers if the information in the summary is factually consistent with the source. This\nmethod can be implemented in production settings to pinpoint, and even correct, factual discrepancies in real time with\nminimal human oversight. Questions can be derived from either the summary or the source text, eliminating the need\nfor a human-annotated reference summary. By generating questions from the source text, the informativeness of the\n5\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nSource Answer\nSummary Answer\nClassification\nYes\nYes\nFactually consistent (strong)\nNo\nNo\nFactually consistent (strong)\nUnknown\nUnknown\nFactually consistent (weak)\nYes\nNo\nFactual inconsistency\nNo\nYes\nFactual inconsistency\nUnknown\nYes/No\nHallucination\nYes/No\nUnknown\nNon-informative\nTable 1: Possible scenarios of answers from source and summary\nsummary can be assessed [35]. Conversely, by generating questions from the summary, consistency and hallucinated\ncan be assessed.\nAn overview of our QA evaluation is shown in Figure 1. Our evaluation focuses on binary (yes/no) questions generated\nfrom the summaries. Binary questions simplify the process of comparing answers from summary and source, rather\nthan requiring another LLM call to determine whether the answers are consistent with each other (given the summary\nas context). We use GPT-3.5-Turbo to generate questions and answers as our evaluator LLM.\nThe prompt template that we use for question generation is shown in Appendix A. This prompt was developed over\nmany iterations. The first challenge was a tendency to generate only “Yes” questions. We therefore prompt the model to\ngenerate an answer key alongside the questions to allow later questions to be conditioned on the answers of previously\ngenerated ones.\nThe question generation prompt uses generic placeholder questions to demonstrate the desired output format because\nwe found that any specific example questions were frequently copied exactly in the output, even when the subject was\nnot relevant to the summary at hand. Adherence to the output format is critical for clean extraction of both questions\nand answers for the next phase of the evaluation.\nAs the summaries in our sample were generated to match XL-Sum lengths, they are short and sometimes do not have\nmuch information to create question. We include some flexibility in the prompt on the number of questions and the\nability to specify an “Unknown” answer to avoid generating a fourth question with a verbose explanation that it cannot\nbe answered instead of an answer key.\nRegular expressions are used to extract the questions and answers from the model response. The prompt to generate\nquestion responses based on an input text (either the summary or source article) is shown in Appendix A.\nThe answers from each response are extracted and compared against one another. The possible scenarios of the answer\ncomparisons are shown in Table 1. We define the consistency score as the fraction of of questions where the summary\nanswer equals the source answer, including when both answers are “Unknown.” The hallucination score is defined as the\nfraction of questions where the source answer equals “Unknown” and the summary answer does not equal “Unknown”.\nQuestions where the summary answer does not equal the source answer are not considered in the hallucination score.\nLarger values of consistency scores and lower values of hallucination scores are indicative of a highly factually accurate\nsummary.\nWe supplement these metrics with a meta evaluation of the QA evaluation system, defined as the fraction of questions\nwhere the summary answer equals the answer key that was generated alongside the question. The meta evaluation score\nis set to zero if some questions are unanswered or if the answer key format is incorrect. If the question generation and\nquestion answering performance of the prompt & LLM evaluator is perfect, these answers will always be the same and\nthe meta evaluation score would be equal to 1.\n4.3\nFact-checking Method\nOur second LLM-powered consistency evaluation is a fact-checking evaluation. As with the QA evaluation, this\napproach was first explored and implemented with task-specific models for extracting structured fact tuples [20] or\ntextual entailment [23,33]. There has been a great deal of effort to expand upon this approach using LLMs [37–40].\nThese metrics achieve improved correlation with human evaluations, although the LLM evaluator often has positive\nbias towards its own generations.\nFact-checking evaluations aim to break the information contained in a summary into small, self-contained facts that can\nthen be individually compared against the source text. This method more directly addresses consistency than the QA\n6\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nFigure 2: Overview of the LLM-powered fact-checking evaluation.\nevaluation approach and shares the advantages of granularity, enabling automated monitoring and correcting, and not\ndepending on a reference summary.\nAn overview of our fact-checking evaluation is shown in Figure 2. Our method uses GPT-3.5-Turbo to extract a\nnumbered list of facts from the summary and then judge whether or not they are consistent with either the summary or\nsource article. The prompts used for these steps are shown in Appendix A. Notably, GPT-3.5-Turbo performed very\nwell on the fact extraction task and very little prompt engineering was necessary for this stage. The prompt used for fact\nchecking required more experimentation to produce answers in a consistent format without occasionally generating\nverbose explanations for each response.\nThe true/false judgments are extracted from the model responses using regular expressions. The consistency score is\ndefined as the fraction of summary facts determined to be true based on the source article. The meta evaluation score is\ndefined as the fraction of summary facts determined to be true based on the summary text. If the fact extraction and\nevaluation performance of the prompt & LLM evaluator is perfect, the facts will all be re-evaluated as true and the meta\nevaluation score would be equal to 1.\n4.4\nApplying evaluations to XL-Sum reference summaries\nThis work required a summarization dataset with reference summaries in order to compare standard summary evaluation\nmetrics to LLM-powered evaluations, but the LLM-powered evaluations themselves do not require a reference summary.\nAs an additional exercise, we apply our LLM-powered evaluations to the XL-Sum reference summaries as if they were\nmodel-generated summaries. It is known that a significant fraction of the reference summaries contain information\nnot found in the corresponding articles across all languages covered in the dataset [5]. By applying our consistency\nevaluations to the reference summaries, we can independently assess the frequency of unsupported information within\nour subset of interest.\n7\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nModel name\nNumber of\nAverage summary\nfailed summaries\nlength (words)\nTextRank\n3\n51.5\nT5-small\n–\n45.2\nBART-large\n–\n51.0\nMistral-7B\n–\n52.5\nLlama3-8B\n–\n61.5\nFalcon-40B\n–\n66.0\nGPT-3.5-Turbo\n388\n39.0\nXL-Sum Reference\n–\n20.6\nTable 2: Properties of generated summaries by model\nFigure 3: Average ROUGE scores for each model. The blue bars show ROUGE-1 and the orange bars show ROUGE-L.\nThe small black error bars depict the approximate 95% confidence interval of the averages. The left panel depicts the\ntraditional ROUGE evaluation against reference summaries while the right panel depicts a modified ROUGE evaluation\nagainst the source article. The XL-Sum reference summaries are included in the modified evaluation on the right,\ndepicted in grey.\nFor completeness, we also tabulate modified ROUGE and BERTScore evaluations where the source article is treated as\nthe reference text so that we can apply standard evaluations to the XL-Sum reference summaries. This modification\nsignificantly changes the meaning of the evaluations, particularly for the token-based ROUGE scores.\n5\nResults\n5.1\nSummary generation\nAn overview of the summarization results is shown in Table 2 and example summaries are shown in Table 3. All models\ngenerate much longer summaries than the XL-Sum summaries on average, by a factor of 2 or more in word count.\nLlama3-8B and Mistral-7B tend to stick to the 2 sentence limit specified in the prompt but have many clauses in their\nsentences. Falcon-40B summaries often had more than the 2 sentence limit. Most models were able to successfully\ngenerate summaries for all articles in our sample. TextRank failed in 3 cases where the algorithm failed to converge.\nThe Azure OpenAI content filter rejected 388 of the articles in our sample, which therefore could not be summarized by\nGPT-3.5-Turbo. These tended to be articles related to violent crimes.\n8\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nOriginal text\nThe proposal features a new forest visitor centre, a medieval-style castle and areas\nfor events and entertainment. Nottinghamshire County Council said the privately-\nfunded venture, which could be open by spring 2015, could bring major investment\ninto the area. At the same time Nottingham City Council said it would be looking\nto develop its own overhaul of the city’s castle site, centred on the outlaw. The\nSherwood project would mean the existing visitor facilities and car park near\nEdwinstowe closing down, with the new attraction built on nearby open land ...\nXL-Sum Reference\nSherwood Forest could get a £13m Robin Hood-themed attraction under new plans.\nTextRank\nChair of the Castle Working Group, Ted Cantle says: “Our ambition is to seek\nto transform Nottingham Castle and the surrounding Castle Quarter area into\na world-class, internationally recognised tourist attraction.” The chairman of\nNottinghamshire County Council’s culture committee, John Cottee, said: “The\ncounty council is committed to raising the profile of Robin and Sherwood locally,\nnationally and internationally, whilst at the same time protecting ancient Sherwood\nForest for future generations.”\nT5-small\nNottinghamshire County Council said the privately-funded venture could bring\nmajor investment into the area . The Sherwood project would mean the existing\nvisitor facilities and car park near Edwinstowe closing down . Robin Hood is a\nglobal brand that “deserved to be celebrated”\nBART-large\nThe privately-funded venture could be open by spring 2015. Nottinghamshire\nCounty Council hopes it will create up to 100 jobs. The existing visitor facilities\nand car park near Edwinstowe will close. Nottingham City Council is also looking\nto develop its own overhaul of the city’s castle site.\nMistral-7B\nNottinghamshire County Council has proposed a new forest visitor center, a\nmedieval-style castle, and areas for events and entertainment in Sherwood Forest.\nThe project, called “The Sherwood Experience,” is expected to bring major invest-\nment into the area and create up to 100 jobs.\nLlama3-8B\nNottinghamshire County Council has proposed a new £17.50 per person attraction,\n“The Sherwood Project”, featuring a forest visitor centre, medieval-style castle, and\nentertainment areas, which could bring major investment to the area and create\nup to 100 jobs. Meanwhile, Nottingham City Council is also planning to develop\nits own overhaul of the city’s castle site, centred on the Robin Hood legend, with\nplans to seek funding options to meet the multi-million pound cost.\nFalcon-40B\nThe article reports on a proposal to build a new forest visitor centre and medieval-\nstyle castle in Nottinghamshire, England. The privately-funded venture is expected\nto bring major investment into the area and create up to 100 jobs. The project is\npart of a larger effort to raise the profile of Robin Hood and Sherwood Forest, and\nto improve the tourist experience at Nottingham Castle.\nGPT-3.5-Turbo\nNottinghamshire County Council and Nottingham City Council are both planning\nto develop new visitor attractions centered around the legend of Robin Hood,\nwith the county council proposing a new forest visitor centre and the city council\nplanning an overhaul of the city’s castle site.\nTable 3: Example summaries generated by each model for article id uk-england-nottinghamshire-20124450, titled\n“Robin Hood £13m tourist ‘castle’ plan unveiled.”\n9\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nFigure 4: Average BERTScore scores for each model. The small black error bars depict the approximate 95% confidence\ninterval of the averages. The left panel depicts the traditional ROUGE evaluation against reference summaries while the\nright panel depicts a modified ROUGE evaluation against the source article. The XL-Sum reference summaries are\nincluded in the modified evaluation on the right, depicted in grey.\n5.2\nEvaluating summaries with standard metrics\nThe average ROUGE scores for each summarization model are shown in Figure 3 and Table 4. According to both\nROUGE-1 and ROUGE-L, the GPT-3.5-Turbo summaries have the greatest N-gram similarity (on average) to the\nXL-Sum reference summaries. This may indicate that the GPT-3.5-Turbo summaries include similar details that\nhuman authors would select, or that the language style is most similar to human authors. The other LLMs (Mistral-7B,\nLlama3-8B, and Falcon-40B) have similar scores, while the smaller fine-tuned models plus TextRank are a significant\nstep lower.\nThe modified ROUGE scores using the the source article text for a different perspective, and the reference summaries\nthemselves can be evaluated in this way. The reference summaries have the lowest average ROUGE-1 and ROUGE-L\nby far. This is expected; human written summaries can use synonyms, apply real world knowledge, and can reflect the\nauthor’s opinion. Furthermore, the reference summaries are notably shorter, which penalizes token-based similarity\nscores.\nThe modified ROUGE scores are inversely related to the degree of abstraction and condensing in the summary. GPT-\n3.5-turbo has the lowest score among the models considered, showing that it is most able to deviate more from simply\nrepeating words in the article. This does not make it a “better” summarization model per se but certainly a more creative\none for this particular task. Alternatively, this may simply reflect that the GPT-3.5-Turbo summaries are the shortest of\nthe model-generated summaries on average. This is a metric where neither very low (zero overlap) nor very high (exact\nduplication) scores are desirable outcomes for summarization.\nThe average BERTScore scores for each summarization model are shown in Figure 4 and Table 4. All models score\nbetween 0.800 and 0.850, indicating that all models generate summaries with highly semantically similar content to the\nreference summaries.\nWhen considering a modified BERTScore comparing summaries to the source articles, the XL-Sum reference summaries\nagain receive the lowest score. However, the gap in scores is much smaller due to no longer relying on exact token\nmatches. All model scores are slightly higher and the minimal spread among the model scores remains.\nIt is interesting to see that TextRank achieves a lower or equivalent score to all other models for both types of BERTScore.\nThe strategy of copying the two most important sentences apparently leaves out some amount semantically important\ninformation that the other summaries are able to capture. GPT-3.5-Turbo scores high for the regular BERTScore metric\nbut low for the modified BERTScore, likely due to the shorter average summary length compared to other models.\n10\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nFigure 5: Average consistency, hallucination, and meta evaluation scores for the QA evaluation when applied to model\ngenerated summaries (blue bars) and the XL-Sum reference summaries (grey bar). The small black error bars depict the\napproximate 95% confidence interval of the averages.\nFigure 6: Average consistency and meta evaluation scores for the fact-based evaluation when applied to model\ngenerated summaries (blue bars) and the XL-Sum reference summaries (grey bar). The small black error bars depict the\napproximate 95% confidence interval of the averages.\n11\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\n5.3\nLLM-powered summary evaluations\nThe results of the QA evaluation are shown in Figure 5 and Table 4. There are minor differences in consistency scores\nacross the models, all achieving roughly 80% consistency. All model summaries have low average rates of hallucination\nas well. The average meta evaluation scores are significantly lower than 1.0, clearly indicating that the evaluation\nsystem is not perfectly applying the intended metrics. Due to the Azure OpenAI content filter rejecting certain harmful\nor violent subjects, some of our summary evaluations failed either at the question generation or question answering step.\nThese are excluded from our results and do not negatively impact the meta evaluation scores.\nBART-large achieves the highest consistency score and the lowest hallucination score. It does not also have the highest\nmeta evaluation score, so it is not the unambiguous highest consistency summarization model according the the QA\nevaluation.\nThe purely extractive method TextRank is not rated as perfectly consistent nor as having zero hallucination because\nthe evaluation system performance is not perfect. Even still, TextRank does not achieve the highest consistency or the\nlowest hallucination scores. It is closer to the middle or bottom of the pack in performance.\nMistral-7B, Llama3-8B, and Falcon-40B have higher rates of hallucination than the fine-tuned summary models or\nGPT-3.5-Turbo. LLMs have greater language generation capabilities, which also comes with a greater ability to make\nthings up. Perhaps GPT-3.5-Turbo summaries are distinguished here as the LLM with the least amount of hallucinations\nin this experiment, but this could also reflect an evaluator bias.\nThe small variations in average meta evaluation scores reflect how differences in style of the summaries can affect the\naccuracy of subsequent text generations. The variations in average consistency and hallucination scores among the\nmodels are much smaller than the performance gap, 1 −Smeta. One could argue that it is not possible to definitively\nrank one model’s summaries as more consistent than the other if the difference in their scores is small compared to the\nevaluation performance gap.\nThe XL-Sum reference summaries are rated as the lowest consistency and highest hallucination rate according to the QA\nevaluation, with a meta evaluation score in line with that of the model-generated summaries. The difference between\nthe XL-Sum scores and the model scores is large enough that the meta evaluation score does not cast doubt the outcome,\nbut the exact deficit in consistency is not clear. Given the significantly lower consistency of the reference summaries, it\ndoes not provide value to evaluate the models with ROUGE or BERTScore against these references.\nThe results of the fact-checking evaluation are shown in Figure 6 and Table 4. All consistency and meta evaluation\nscores are higher than for the QA evaluation, indicating that the QA evaluation is likely underestimating consistency\nscores due to imperfect evaluation system performance. The fact-checking evaluation is more likely to provide an\naccurate relative ranking of model summary consistency due to the improved evaluation performance. As with the QA\nevaluation, some fact-checking evaluations failed due to the Azure OpenAI content filter and are therefore excluded\nfrom our results.\nThe full range of consistency scores is similar to that of the QA evaluation, but the grouping is different. Falcon-40B\nhas the lowest consistency score (as it does for the QA evaluation), GPT-3.5-Turbo has the highest consistency score,\nand all of the other models are roughly halfway between these extremes. The XL-Sum reference summaries again\nachieve a significantly lower consistency score than the model summaries.\nBoth styles of LLM evaluation have higher meta evaluation scores for the LLM summaries than those generated by\npre-trained models. This could reflect a bias towards the writing style of LLMs.\n12\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nMetric\nTextRank\nT5-small\nBART-large\nMistral-7B\nLlama3-8B\nFalcon-40B\nGPT-3.5-Turbo\nXL-Sum Reference\nROUGE-1\n0.133\n0.214\n0.216\n0.255\n0.250\n0.242\n0.267\n–\nROUGE-L\n0.133\n0.140\n0.139\n0.176\n0.171\n0.172\n0.188\n–\nROUGE-1 (article)\n0.347\n0.299\n0.351\n0.322\n0.331\n0.365\n0.242\n0.107\nROUGE-L (article)\n0.291\n0.290\n0.321\n0.240\n0.211\n0.267\n0.171\n0.071\nBERTScore\n0.800\n0.805\n0.813\n0.840\n0.849\n0.850\n0.822\n–\nBERTScore (article)\n0.885\n0.904\n0.914\n0.913\n0.912\n0.910\n0.885\n0.846\nQA Consistency\n0.798\n0.816\n0.831\n0.807\n0.804\n0.788\n0.806\n0.620\nQA Hallucination\n0.096\n0.077\n0.071\n0.099\n0.110\n0.103\n0.078\n0.181\nFact Consistency\n0.879\n0.876\n0.876\n0.872\n0.874\n0.854\n0.899\n0.686\nQA Meta Evaluation\n0.835\n0.833\n0.834\n0.859\n0.872\n0.861\n0.858\n0.845\nFact Meta Evaluation\n0.888\n0.865\n0.867\n0.906\n0.921\n0.910\n0.931\n0.939\nTable 4: Results of all evaluations on all model-genenerated summaries and the XL-Sum reference summaries. The highest score for each metric (or lowest score for\nQA Hallucination) is shown in bolded blue font.\n13\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\n6\nDiscussion\nWhen considering all metrics together in Table 4, BART-large has the best or second-best scores in many cases,\nsuggesting that it produces the most consistent summaries. GPT-3.5-Turbo also performs well, notably achieving\nthe highest score on the LLM-powered fact-checking evaluation, but its scores may benefit from bias. Falcon-40B\nsummaries are highly rated by standard metrics but score the lowest for both QA and fact-checking consistency, and\nhave the second-highest QA hallucination score. The two smaller LLMs, Llama3-8B and Mistral-7B, are likely better\noptions in terms of cost and performance when self-hosting an LLM for summary generation.\nUltimately, all of the models produce fairly high consistency summaries. A more accurate evaluation would be required\nto determine which model is the most consistent (and by how much).\n6.1\nQA versus Fact-checking\nWe find that our implementation of a QA-based consistency metric is less reliable than our fact-checking consistency\nmetric, as measured by the meta evaluation scores. Both of our implementations were relatively simple and could\ncertainly be further optimized.\nThere does appear to be broader interest in fact-based [37–40] rather than QA evaluations [36]. In terms of alignment\nwith human evaluations, DCE [40] achieves higher correlation on consistency than SummEQuAL [36] on the SummEval\nbenchmark when both approaches use GPT-3.5-Turbo as the evaluator.\nIt is easier to prompt an LLM to extract all or nearly all of the information content of a summary than to generate\nquestions that cover all information. The former is easier as less transformation is required on the input text. There\nare template-based approaches to ensure full and consistent coverage of QA evaluations, such as adopting a fixed set\nof questions for each type of named entity [32] or having an expert define a list of task-related questions [36]. These\napproaches can generate questions that are unanswerable on both the summary and source text, but such uninformative\nquestions could be excluded when aggregating QA responses into a score.\n6.2\nLLM capabilities as evaluators\nPrevious investigations into the capabilities of LLMs as evaluators of machine-generated text have highlighted their\nbiases [28], inconsistent correlation with human judgment [27], and inability to identify the best of two good candidates\n[29]. With our meta evaluations, we now demonstrate that LLM evaluation systems can deviate considerably from\nperfect accuracy in a situation where the correct outcome is unambiguous. This shows that structured evaluations\nface the same challenge in differentiating candidates of similar quality, even when scores are not directly generated or\nhead-to-head comparisons are not made. We expect higher meta evaluation scores when using a more sophisticated\nmodel as the evaluator, such as GPT-4.\nA meta evaluation score that directly assesses an LLM evaluation system’s accuracy provides important context to any\nresults obtained using the system, but can also be used during development phases. For example, meta evaluation scores\ncould be used to guide prompt engineering towards prompts that better implement the desired metric. These scores\ncould also be used to help select an evaluator LLM. For consistency evaluations, our implementation could be extended\ninto a two-sided approach that also evaluates the source text against itself.\nWe strongly recommend the adoption of meta evaluations into a wider array of LLM evaluations where feasible. The\nbase requirement is a reference or situation where an ideal evaluation system would always yield a perfect score. For\nfactual consistency, this situation is comparing a text (indirectly) against itself. By definition, a text must be factually\nconsistent with itself unless it contains explicit internal contradictions. Unfortunately, this concept does not translate\nwell to intangible qualities related to language style. When it comes to style, there are many acceptable outcomes and it\ndoesn’t necessarily make sense to declare a given high-quality reference to be perfect.\n6.3\nEvaluations applied to human benchmark summaries\nAll of the consistency metrics tested in this work would agree that the XL-Sum reference summaries are the least\nconsistent. The human authors of the XL-Sum articles and summaries had a specific objective: to catch the interest of\nhuman readers and encourage them to read the rest of the article. This is not necessarily the same as writing a consistent\nand comprehensive summary, and this task misalignment is important to keep in mind.\nThe creativity and world knowledge of the human authors is in conflict with strict factual consistency. Human authors\ncan also include subjective interpretations based on their values in their summaries. These attributes are desirable in\nhuman-written summaries but would be classified as hallucinations in model-generated summaries. Creativity aside,\n14\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nthere are many genuine instances in the XL-Sum dataset where the summaries contain information that is not present in\nthe article text. To human readers, this may be desirable especially for shorter articles so that the reading experience is\nnot too repetitive.\nThere are implications for fine-tuning a summarization model on human-written summaries: do these samples provide\nimpossible examples that effectively encourage hallucination? This is less of a concern going forward with the broad\ntransition to foundation models versus task-specific models, but this aspect of data quality will continue to be important\nduring pre-training and instruct fine-tuning.\nThe assumption underlying reference-based evaluations and training/fine-tuning on article-summary datasets is that the\nhuman written summary is the gold standard of quality and accuracy. However, all such datasets have some trade-offs\nbetween size, manual effort, and quality; prioritizing all three is rarely done due to the high cost involved. XL-Sum and\nother summarization datasets scraped from the internet are primarily optimizing for large size and low manual effort.\nOthers have raised concerns on the quality of news summarization datasets sourced by web scraping [33,41]. We apply\nour LLM-powered consistency evaluations to high quality human benchmark summaries of news articles shared by [41].\nThis also provides a test of our evaluation performance on longer articles; the summaries themselves are similar to our\nmodel-generated summaries\nThe average consistency scores on these human benchmark summaries are 0.761 (QA) and 0.856 (Fact-checking), both\nof which are approximately 0.23 points higher than the corresponding XL-Sum scores. The fact-checking consistency\nscore on this human benchmark is now comparable with the model scores. The average QA hallucination score is\n0.130, an improvement of 0.05 compared to XL-Sum. The average meta evaluation scores (0.852 and 0.915) are in\nline with those seen for the models and XL-Sum reference summaries, indicating that there was no substantial change\nin evaluation system performance on the new dataset. Overall, the high quality human benchmark summaries are\nsignificantly more factually accurate than the XL-Sum reference summaries, and the summarization models tested in\nthis work achieve comparable factuality to this benchmark.\n6.4\nAreas for future work\nThe evaluation prompts and systems used in this work are relatively simple, and there are many possible avenues for\nimprovement. For example, the evaluator LLM may produce more accurate responses if it only answers one question at\na time. This would increase the time and cost of question-answering inference, which could be a problem for longer\nsource texts. Allowing the LLM evaluator to generate “thoughts” about a given fact or question could also improve\naccuracy if it is prompted to reflect before generating its final answer [42].\nIn use cases limited to a specific domain (e.g. finance, medicine), adding domain-specific context could improve both\nsummary quality and evaluation system performance. The wide variety of topics news articles in XL-Sum dataset was\nnot a good fit for this approach. Few-shot prompting may improve performance for specific tasks as well. We mainly\nexperimented with few-shot prompting for question generation, and found that it led to repeating the example question\neven when it was not relevant to the summary. It is possible that this approach would be more applicable to a different\ntask or a specific domain.\nIn general, the investigation of strategies for LLM evaluations is ongoing. The ability to prompt in natural language\nleads to many creative approaches, such as having LLMs take on the roles of different stakeholders during head-to-head\nevaluation of two candidates [43]. LLMs can be used to automatically explore design of evaluation prompts that lead to\nthe best alignment with human evaluations [44]. Techniques such as this could also incorporate meta evaluations to test\nsystem accuracy in addition to human alignment.\n7\nConclusion\nIn this paper, we evaluated the factual consistency of a wide variety of summarization models. We found that even\nrelatively simple models such as T5-small and BART-large are able to produce highly consistent summaries. Our LLM\nevaluations of consistency were found to have less than ideal performance via meta evaluations of summary consistency\nagainst itself. A more accurate evaluation system would be needed to distinguish which of the summarization models\nproduces the most consistent summaries.\nIt is important to develop more reliable evaluations and continue to assess the evaluation performance not just against\nhuman preferences but also against scenarios where the correct answer is known. We recommend implementing meta\nevaluation scores for LLM-powered evaluations in order to guide the development process and to provide context on\nthe final results.\n15\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nLow consistency scores on the XL-Sum reference summaries indicated that this dataset is an inadequate point of\ncomparison to modern summarization models. Likewise, traditional reference-based metrics such as ROUGE and\nBERTScore do not provide value when the model-generated summaries are reasonably capable of fluent writing,\nespecially when compared to lower quality references.\nA\nLLM Evaluation Prompts\nA.1\nQuestion Generation\nNews article summary:\n{text}\nPlease write 4 yes-or-no questions based on the key facts presented in the summary\nabove. Keep the questions simple, with either a \"Yes\" or \"No\" answer. If the summary\nis very short and you can’t come up with 4 questions, 3 is also acceptable. At least\none question should have \"No\" as the correct answer. After each question, include\nthe correct answer in square brackets for the answer key, e.g.:\n1. <your question here> [No]\n2. <your next question here> [Yes]\nIdeally, every question should be answerable using only the news article summary.\nIf the question cannot be answered using the summary, write \"Unknown\" as the\ncorrect answer.\nQuestions:\nA.2\nQuestion Answering\nNews article:\n{text}\nBased on the news article above, please answer the following numbered questions.\nAnswer each question with either \"Yes\", \"No\", or \"Unknown\" if the answer can’t be\ndetermined based on the information in the article.\nQuestions:\n{questions}\nAnswers:\nA.3\nFact extraction\n{text}\nPlease list all of the facts presented in the summary above as a numbered list.\nA.4\nFact checking\nNews article:\n‘‘‘\n{text}\n‘‘‘\nAre the statements below factually consistent with the article? Please respond with\nTRUE or FALSE accordingly for each statement, e.g.:\n1. TRUE\n2. TRUE\n16\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\n3. FALSE\n----------\nStatements:\n{facts}\nAnswers (TRUE/FALSE):\nReferences\n[1] Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence\nmodel for extractive summarization of documents. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 31, 2017.\n[2] Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. Learning-based single-document summarization with\ncompression and anaphoricity constraints. arXiv preprint arXiv:1603.08887, 2016.\n[3] Sumit Chopra, Michael Auli, and Alexander M Rush. Abstractive sentence summarization with attentive recurrent\nneural networks. In Proceedings of the 2016 conference of the North American chapter of the association for\ncomputational linguistics: human language technologies, pages 93–98, 2016.\n[4] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. arXiv preprint arXiv:1908.08345,\n2019.\n[5] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Samin, Yuan-Fang Li, Yong-Bin Kang, M Sohel\nRahman, and Rifat Shahriyar. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages.\narXiv preprint arXiv:2106.13822, 2021.\n[6] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934,\n2020.\n[7] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\nStoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark\nChen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates, Inc., 2020.\n[9] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie\nSimens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models\nto follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran\nAssociates, Inc., 2022.\n[10] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles,\ntaxonomy, challenges, and open questions. ACM Transactions on Information Systems, November 2024.\n[11] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware\nconvolutional neural networks for extreme summarization. In Ellen Riloff, David Chiang, Julia Hockenmaier,\nand Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pages 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics.\n[12] Rada Mihalcea and Paul Tarau. Textrank: Bringing order into text. In Proceedings of the 2004 conference on\nempirical methods in natural language processing, pages 404–411, 2004.\n17\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\n[13] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank Citation Ranking: Bringing\nOrder to the Web. Technical Report 1999-66, Stanford InfoLab, November 1999. Previous number = SIDL-WP-\n1999-0120.\n[14] Varun Deokar and Kanishk Shah. Automated text summarization of news articles. International Research Journal\nof Engineering and Technology (IRJET) publication, 8(09):2395–0072, 2021.\n[15] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne\nLachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\nMistral 7b, 2023.\n[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra,\nArchie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen\nGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu,\nCorinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny\nLivshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank\nZhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang,\nGuillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,\nIsabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay\nMahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng\nChi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca,\nJoshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li,\nKenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren\nRantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan,\nLubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat\nSingh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike\nLewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay\nBogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li,\nPetar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu,\nQing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert\nStojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly,\nRoss Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell,\nSeohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye\nWan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane\nCollot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou,\nThomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor\nGupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng\nXie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen\nZhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya\nSingh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva\nGoldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda\nKallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho,\nAndrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury,\nAshley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin\nLeonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden\nHancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly\nBurton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai,\nChris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David\nAdkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang,\nDuc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily\nWood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat\nOzgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella\nSchwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi,\nZhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb,\nHarrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov,\nIrina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff\n18\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\nMarcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi\nYang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang,\nKai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich,\nKaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang,\nLailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron\nMoshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas\nMankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan\nKeneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan,\nMike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish\nBansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo,\nNicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia\nHart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip\nBontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang,\nRachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li,\nRebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu,\nSamyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji\nYamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy\nZha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith\nChintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,\nSungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser,\nTamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook\nShaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla,\nVítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,\nWenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang,\nXide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi\nAdi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick,\nZhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024.\n[17] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane\nDebbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models, 2023.\n[18] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon LLM: Outperforming\ncurated corpora with web data only. In Thirty-seventh Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2023.\n[19] Natalie Schluter. The limits of automatic summarisation according to ROUGE. In Mirella Lapata, Phil Blunsom,\nand Alexander Koller, editors, Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 2, Short Papers, pages 41–45, Valencia, Spain, April 2017. Association for\nComputational Linguistics.\n[20] Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. Assessing the factual accuracy of generated text.\nIn Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\nKDD ’19, page 166–175, New York, NY, USA, 2019. Association for Computing Machinery.\n[21] Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Neural text\nsummarization: A critical evaluation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceed-\nings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 540–551, Hong Kong, China, November\n2019. Association for Computational Linguistics.\n[22] Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency\nof summaries. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020, Online, July 2020.\nAssociation for Computational Linguistics.\n[23] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. SummaC: Re-visiting NLI-based models\nfor inconsistency detection in summarization. Transactions of the Association for Computational Linguistics,\n10:163–177, 2022.\n[24] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation\nusing gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2511–2522, Singapore,\nDecember 2023. Association for Computational Linguistics.\n19\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\n[25] Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham\nNeubig, and Chunting Zhou. Multi-dimensional evaluation of text summarization with in-context learning. In\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational\nLinguistics: ACL 2023, pages 8487–8495, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[26] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. Human-like summarization\nevaluation with chatgpt, 2023.\n[27] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large language models are not yet\nhuman-level evaluators for abstractive summarization. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\nFindings of the Association for Computational Linguistics: EMNLP 2023, pages 4215–4233, Singapore, December\n2023. Association for Computational Linguistics.\n[28] Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. Benchmarking\ncognitive biases in large language models as evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,\neditors, Findings of the Association for Computational Linguistics: ACL 2024, pages 517–545, Bangkok, Thailand,\nAugust 2024. Association for Computational Linguistics.\n[29] Tempest A. van Schaik and Brittany Pugh. A field guide to automatic evaluation of llm-generated summaries.\nIn Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR ’24, page 2832–2836, New York, NY, USA, 2024. Association for Computing Machinery.\n[30] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out,\npages 74–81, 2004.\n[31] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675, 2019.\n[32] Ping Chen, Fei Wu, and Tong Wang. A semantic qa-based approach for text summarization evaluation. Proceedings\nof the AAAI Conference on Artificial Intelligence, 32, 04 2017.\n[33] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive\nsummarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, Online, July 2020.\nAssociation for Computational Linguistics.\n[34] Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. QAFactEval: Improved QA-based factual\nconsistency evaluation for summarization. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir\nMeza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 2587–2601, Seattle, United States, July 2022.\nAssociation for Computational Linguistics.\n[35] Potsawee Manakul, Adian Liusie, and Mark Gales. MQAG: Multiple-choice question answering and generation\nfor assessing information consistency in summarization. In Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry\nWijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi, editors, Proceedings of the 13th International Joint Conference\non Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 39–53, Nusa Dua, Bali, November 2023. Association\nfor Computational Linguistics.\n[36] Junyuan Liu, Zhengyan Shi, and Aldo Lipani. SummEQuAL: Summarization evaluation via question answer-\ning using large language models. In Bhavana Dalvi Mishra, Greg Durrett, Peter Jansen, Ben Lipkin, Danilo\nNeves Ribeiro, Lionel Wong, Xi Ye, and Wenting Zhao, editors, Proceedings of the 2nd Workshop on Natural\nLanguage Reasoning and Structured Explanations (@ACL 2024), pages 46–55, Bangkok, Thailand, August 2024.\nAssociation for Computational Linguistics.\n[37] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. Chatgpt as a factual inconsistency evaluator for abstractive\ntext summarization. 03 2023.\n[38] Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. Evaluating the\nfactual consistency of large language models through news summarization. In Anna Rogers, Jordan Boyd-Graber,\nand Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages\n5220–5255, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[39] Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, and Fei Liu. Identifying factual inconsistencies in\nsummaries: Grounding llm inference via task taxonomy, 2024.\n[40] Wendi Cui, Jiaxin Zhang, Zhuohang Li, Lopez Damien, Kamalika Das, Bradley Malin, and Sricharan Kumar.\nDcr-consistency: Divide-conquer-reasoning for consistency evaluation and improvement of large language models,\n2024.\n20\n\n\nConsistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models\n[41] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto.\nBenchmarking large language models for news summarization. Transactions of the Association for Computational\nLinguistics, 12:39–57, 2024.\n[42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny\nZhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n[43] Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. Large language models are diverse\nrole-players for summarization evaluation, 2023.\n[44] Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun,\nand Qi Zhang. Calibrating llm-based evaluator, 2023.\n21\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20647v1.pdf",
    "total_pages": 21,
    "title": "Consistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models",
    "authors": [
      "Colleen Gilhuly",
      "Haleh Shahzad"
    ],
    "abstract": "Text summarizing is a critical Natural Language Processing (NLP) task with\napplications ranging from information retrieval to content generation. Large\nLanguage Models (LLMs) have shown remarkable promise in generating fluent\nabstractive summaries but they can produce hallucinated details not grounded in\nthe source text. Regardless of the method of generating a summary, high quality\nautomated evaluations remain an open area of investigation. This paper embarks\non an exploration of text summarization with a diverse set of techniques,\nincluding TextRank, BART, Mistral-7B-Instruct, and OpenAI GPT-3.5-Turbo. The\ngenerated summaries are evaluated using traditional metrics such as the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) Score and\nBidirectional Encoder Representations from Transformers (BERT) Score, as well\nas LLM-powered evaluation methods that directly assess a generated summary's\nconsistency with the source text. We introduce a meta evaluation score which\ndirectly assesses the performance of the LLM evaluation system (prompt +\nmodel). We find that that all summarization models produce consistent summaries\nwhen tested on the XL-Sum dataset, exceeding the consistency of the reference\nsummaries.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}