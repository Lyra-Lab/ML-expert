{
  "id": "arxiv_2502.21266v1",
  "text": "Supporting the development of Machine Learning for fun-\ndamental science in a federated Cloud with the AI_INFN\nplatform\nLucio Anderlini1, Matteo Barbetti2,∗, Giulio Bianchini3,4, Diego Ciangottini3, Stefano Dal\nPra2, Diego Michelotto2, Carmelo Pellegrino2, Rosa Petrini1,∗∗, Alessandro Pascolini2, and\nDaniele Spiga3\n1Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Firenze, Italy\n2Istituto Nazionale di Fisica Nucleare (INFN), CNAF, Italy\n3Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Perugia, Italy\n4Department of Physics, University of Perugia, Italy\nAbstract. Machine Learning (ML) is driving a revolution in the way scientists\ndesign, develop, and deploy data-intensive software. However, the adoption of\nML presents new challenges for the computing infrastructure, particularly in\nterms of provisioning and orchestrating access to hardware accelerators for de-\nvelopment, testing, and production. The INFN-funded project AI_INFN (“Ar-\ntificial Intelligence at INFN”) aims at fostering the adoption of ML techniques\nwithin INFN use cases by providing support on multiple aspects, including the\nprovision of AI-tailored computing resources. It leverages cloud-native solu-\ntions in the context of INFN Cloud, to share hardware accelerators as effec-\ntively as possible, ensuring the diversity of the Institute’s research activities is\nnot compromised. In this contribution, we provide an update on the commis-\nsioning of a Kubernetes platform designed to ease the development of GPU-\npowered data analysis workflows and their scalability on heterogeneous, dis-\ntributed computing resources, possibly federated as Virtual Kubelets with the\ninterLink provider.\n1 Introduction\nOver the past decade, Artificial Intelligence (AI) has seen rapid and widespread adoption,\nestablishing itself as a standard tool for processing large, complex datasets, and extracting\ninsights from multi-modal, multi-domain data [1]. The proliferation of text-to-image apps\nand the advent of AI-powered chat-bots have helped propel AI into the mainstream, consol-\nidating its explosion in both usage and development. Today, AI is reshaping the computing\nlandscape, driving technological evolution, influencing hardware market trends, and domi-\nnating software development worldwide.\nCloud computing has also played a key role in accelerating the adoption of AI techniques\nby making computing power and data storage more accessible to developers by outsourc-\n∗e-mail: matteo.barbetti@cnaf.infn.it\n∗∗e-mail: rosa.petrini@fi.infn.it\narXiv:2502.21266v1  [cs.DC]  28 Feb 2025\n\n\ning resource management to service providers like AWS1 or GCP2. This approach enables\nready-to-use software to be delivered directly to the end users, a model known as Software-\nas-a-Service (SaaS). A typical example is JupyterHub3, which provides multiple users with\naccess to notebooks for data visualization and interactive computation using resources provi-\nsioned by the Cloud. The growing community of Data Scientists makes large use of this or\nsimilar technologies, contributing to expanding the catalog of libraries and tools available for\ndeveloping and deploying AI applications.\nThe scientific community is closely following the evolution of Machine Learning (ML),\naiming at adapting advanced ML-based algorithms for fundamental research. This is par-\nticularly true in the High Energy Physics (HEP) field, where researchers are exploring AI-\ndriven computing solutions to accelerate the workflow, from data acquisition and simulation\nto physics analysis [2]. The paradigm of interactive computing is also gaining interest within\nthe HEP community as a promising solution to meet the growing resource demand expected\nfrom next-generation experiments. These efforts are leading to the design and implementa-\ntion of Analysis Facilities (AFs), a collection of infrastructures and services that integrates\ndata, software, and computational resources to execute one or more elements of an analysis\nworkflow [3].\nThe Italian National Institute for Nuclear Physics (INFN) is the coordinating institution\nfor nuclear, particle, astroparticle, medical and theoretical physics in Italy. It promotes,\ncoordinates, and conducts scientific research, along with the technological developments\nneeded for the activities in these fields. In response to the paradigm shift driven by AI and\nCloud technologies, INFN is reorganizing its computing infrastructure to support emerging\ntrends, strengthening its resources [4], and expanding the range of services offered through\nINFN Cloud4 [5]. Within this context, the AI_INFN (“Artificial Intelligence at INFN”) ini-\ntiative was launched, aiming at sharing hardware resources, learning best practices, and de-\nveloping AI-powered applications relevant to INFN research. In this document we present\nthe AI_INFN platform and discuss its relevance to the outlined scopes.\n2 The AI_INFN initiative\nThe AI_INFN initiative, aims to connect the scientific communities developing infrastruc-\ntures, algorithms, and applications. It provides tools, computing infrastructures, and social\nconnections to foster collaboration and facilitate the adoption of AI-powered computing tech-\nnologies within INFN research fields.\nAI_INFN is organized in four work packages: procuring and operating computing infras-\ntructure with hardware accelerators, organizing training events for Machine Learning adop-\ntion, building a community of ML experts and developers across INFN units, and develop-\ning the competence to profit from hardware accelerators beyond Graphics Processing Units\n(GPU), such as FPGAs and Quantum Processors, for AI model training and inference.\nThe AI_INFN platform is a cloud-native toolset developed in collaboration with the\nDataCloud initiative operating INFN Cloud to support the activities of the four work pack-\nages of AI_INFN. In this document we present the AI_INFN platform and discuss its rele-\nvance to the outlined scopes.\nThe AI_INFN platform is a SaaS provided by INFN Cloud. The underlying specialized\nand dedicated hardware was designed for High Performance Computing (HPC) tasks and\nis hosted and managed at INFN CNAF in Bologna, few steps away from the pre-exascale\n1Amazon Web Services, https://aws.amazon.com.\n2Google Cloud Platform, https://cloud.google.com.\n3Multi-user platform for collaborative coding and data analysis using notebooks, https://jupyter.org/hub.\n4Cloud resources for INFN research, https://www.cloud.infn.it.\n\n\nCINECA supercomputer Leonardo5. The infrastructure comprises four servers, clustered in\nan OpenStack6 tenancy, acquired and installed between 2020 and 2024 as the demand for\nCloud-accessible HPC computing resources increased:\n• Server 1 (2020), with 64 CPU cores, 750 GB of memory, 12 TB of NVMe disk, eight\nNVIDIA Tesla T4 GPUs and five NVIDIA RTX 5000 GPUs;\n• Server 2 (2021), with 128 CPU cores, 1024 GB of memory, 12 TB of NVMe disk, two\nNVIDIA Ampere A100 GPU, one NVIDIA Ampere A30 GPU, two AMD-Xilinx U50\nboards and an AMD-Xilinx U250 board;\n• Server 3 (2023), with 128 CPU cores, 1024 GB of memory, 24 TB of NVMe disk, three\nNVIDIA Ampere A100 GPUs and five AMD-Xilinx U250 boards;\n• Server 4 (2024), with 128 CPU cores, 1024 GB of memory, 12 TB of NVMe disk, one\nNVIDIA RTX 5000 GPUs and two AMD-Xilinx Versal V70 boards.\nBefore AI_INFN started its activity in January 2024, the farm was maintained by another\nINFN initiative named ML_INFN that, created as a proof-of-concept for designing a plat-\nform for sharing accelerated resources, was developed with a provisioning model relying on\nVirtual Machines (VMs) assigned to groups of users developing a data analysis or Machine\nLearning study [6]. During the late period of ML_INFN, however, an increase in the user\nbase highlighted some limitations to the efficiency of this provisioning model. These limi-\ntations were related to administrative and user-support burden, very long idling times, and\ndangerous eviction of the stateful user’s deployments. In 2023, the security risks grew to an\nunacceptable level and called for the introduction of an alternative model enabling users to\ntune the resources provisioned to their cloud-based computing environment, without becom-\ning administrators of a multi-user web service.\nAt the time of writing, 72 researchers working on 16 research activities have requested\nand gained access to the platform. On average, 10 to 15 researchers connect at least once to\nthe platform in a working day. Two dedicated clones of the AI_INFN platform were tem-\nporarily deployed at CNAF and at ReCaS Bari to provision the GPU-accelerated resources to\nthe 30 participants of the first AI_INFN hackathon, an advanced training event organized in\nPadua in November 2024.\n3 SaaS provisioning model, the AI_INFN platform\nThe AI_INFN platform was deployed on a Kubernetes7 cluster spanning on at least three\nVMs within the dedicated OpenStack tenancy providing part of the storage resource, the\nmonitoring infrastructure and the Kubernetes control plane. A minimal amount of compute\nresources is also provisioned to make it possible for users to access their data on the platform\nanytime [7]. Additional compute resource provided by VMs can be attached to the cluster\nand detached to be used as standalone machines running an Ansible8 playbook, or reassigned\nto another cluster in the same tenancy. AI_INFN users are identified through INFN Cloud\nIndigo IAM [8] instance. Once authenticated, users can configure and spawn their JupyterLab\ninstance using JupyterHub.\nThe main platform file system is distributed through the containers via NFS. One of the\nplatform nodes runs an NFS server in a Kubernetes pod and exports data to the containers\nspawned by JupyterHub. At spawn time, JupyterHub is configured to create the user’s home\n5CINECA Leonardo, https://leonardo-supercomputer.cineca.eu\n6OpenStack, https://www.openstack.org.\n7Kubernetes, https://kubernetes.io.\n8Ansible, https://www.ansible.com.\n\n\ndirectories and project-dedicated shared volumes. A special directory of the platform file\nsystem, that users can use directly or clone and extend in their directories, is reserved for\ndistributing managed software environments, configure using virtual environments. The plat-\nform file system is subject to regular encrypted backup. Backup data is stored in a remote\nCeph [9] volume provisioned by INFN Cloud using the BorgBackup9 package to ensure data\ndeduplication.\nLarge datasets must be stored in a centralized object storage service based on Rados\nGateway10 and centrally managed by DataCloud. To ease accessing the datasets with the\nPython frameworks commonly adopted in Machine Learning projects, a patched version of\nrclone11 was developed to enable mounting the user’s bucket in the JupyterLab instance\nusing the same authentication token used to access JupyterHub. The mount operation is\nautomated at spawn time.\nTo address the bandwidth limitations of a virtual file system with a remote backend, which\ncan hinder iterative training and data analysis requiring to process the whole dataset multiple\ntimes, the AI_INFN platform provides also an ephemeral file system. This system is mapped\ndirectly to a logical volume on the hypervisor’s NVMe storage. The indications for the users\nis to copy the required data to this fast volume at the beginning of each session. These\nephemeral volumes are also useful as a cache for intermediate results or to extend RAM\nthrough memory mapping.\nAt the opposite extreme of the I/O performance spectrum there are distributed virtual\nfile systems that can be mounted on multiple computing resources, enabling the sharing of\nnotebooks and user-defined computing environments across multiple computing sites and\ncompute backends. JuiceFS12 is a cloud-based, high-performance, POSIX-compliant dis-\ntributed file system specifically designed for multi-cloud and serverless computing. It de-\ncouples data and metadata delegating these tasks to highly optimized third party projects,\ncombining a metadata engine implemented with either key-value databases (such as Redis13)\nor relational database management systems (such as PostgreSQL14) with storage systems\naccessed through S3, WebDAV or other high-throughput protocols.\nFinally users can install or upgrade packages in their containers. Installing new software\nwill introduce ephemeral modifications in OverlayFS layer on top of the container file system.\nA more effective and popular alternative to installing packages in the container is to rely\non the binaries distributed through the CERN VM file system (cvmfs). CVMFS, used to\ndistribute software through the nodes of the WLCG, is made available to the platform users\nthrough a Kubernetes installation that shares the caches among different users and sessions.\nIn the AI_INFN platform, the NVIDIA GPU operator15, is used to install and maintain the\nGPU drivers. The NVIDIA GPU Operator on Kubernetes streamlines the management and\ndeployment of NVIDIA GPU resources by automating the installation and configuration of\nrequired components within a Kubernetes cluster. In general, the GPU Operator is designed\nto simplify the management of clusters with a large number of GPU accelerators, enabling\nefficient scaling and resource optimization for GPU-accelerated workloads such as AI, ML,\nand data processing. This approach enables centralized management of GPUs, ensuring a\nconsistent and scalable configuration across all nodes in the cluster, while simplifying main-\ntenance and updates.\n9BorgBackup, https://borgbackup.readthedocs.io/en/stable/#.\n10Rados Gateway, https://docs.ceph.com/en/reef/radosgw.\n11Diego Ciangottini, rclone (2022). GitHub repository: https://github.com/DODAS-TS/rclone.\n12JuiceFS, https://juicefs.com/en.\n13Redis, https://redis.io.\n14PostgreSQL, https://www.postgresql.org.\n15NVIDIA GPU Operator, https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html.\n\n\nOne of the most common support requests with the VM-based provisioning model in-\nvolved setting up a GPU-accelerated Python software stack. While the TOSCA template [10]\nand Ansible playbook handled the installation of the NVIDIA driver and runtime, choosing\nand installing the Python libraries required for the application was left to the users.\nTypically, managing a data science project’s dependencies is the responsibility of developers\nand analysts, and many analysis projects require multiple computing environments. When\nintroducing the AI_INFN platform, particular attention was given to ensuring user sessions\nwere highly customizable and adaptable by providing mechanisms for users to create and\nmanage their own computing environments. The most radical customization option is to\nbuild and pick a custom OCI image. Both communities and individual users can modify\nthe default OCI image by adding system libraries or software packages or by altering the\nJupyterLab service itself.\nWhile users often prefer conda16 for custom software environments, Apptainer17 images are\ngaining popularity. Unlike conda, which consists of thousands of small files, Apptainer uses\nSquashFS18, a compressed read-only file system, to package the entire environment into a\nsingle file. This makes Apptainer images easier to share and distribute through object stores.\nThe AI_INFN platform provides documentation to help users export conda environments as\nApptainer images and use them as Jupyter kernels. It also offers pre-built conda environments\nand Apptainer images with software versions optimized for GPU-accelerated Machine Learn-\ning frameworks. Users can clone these environments and add project-specific dependencies,\ntypically related to data loading and visualization, and independent of the GPU software\nstack.\nA notable exception is represented by the software environment to develop Quantum Machine\nLearning (QML), featuring Python modules that simulate the effect of quantum operators on\nGPU and therefore requiring the same attention as other GPU-accelerated ML libraries to\nmatch the versions of the underlying software.\nIn addition, Apptainer images specialized for the data processing of the LHC experiments\ncan be obtained via CVMFS.\nA dedicated monitoring and accounting system has been set up for the platform in order\nto effectively control the use of all the platform’s resources and in particular of the GPUs.\nSeveral metric exporters have been configured to collect the information of interest and then\nexpose it to a Prometheus19 instance running in the platform. Some of these exporters are\nalready available as Free and Open Source Software, such as Kube Eagle20, which manages\ninformation about the use of the cluster’s CPUs and memory resources by the various com-\nponents of Kubernetes, or NVIDIA GPU DCGM exporter21. Other exporters were developed\non purpose, for example to monitor the usage of storage resources. All the metrics collected\nby Prometheus are then made visible and accessible through a Grafana22 dashboard. Grafana\nis run in a VM independent of the platform cluster and is used to monitor other VMs in\nthe AI_INFN OpenStack tenancy. It also hosts a PostgreSQL database for the accounting\nmetrics, updated at regular intervals by averaging the metrics obtained from the monitoring\nPrometheus service.\n16Conda, https://conda.io.\n17Apptainer, https://apptainer.org/.\n18Squashfs, https://www.kernel.org/doc/Documentation/filesystems/squashfs.txt.\n19Prometheus, https://prometheus.io\n20Martin Schneppenheim, Kube Eagle (2020). GitHub repository: https://github.com/cloudworkz/kube-eagle.\n21NVIDIA\nDCGM\nExporter,\nhttps://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/latest/\ndcgm-exporter.html.\n22Grafana Labs: Grafana Documentation, https://grafana.com/docs/.\n\n\n4 Offloading: scale the applications beyond cluster boundaries\nWhile the AI_INFN platform is primarily conceived as the to-go solution for researchers\nmoving their first steps with hardware-accelerated and machine learning software develop-\nment, it was designed to enable packaging and scaling the developed applications with com-\nputing resources made available in remote computing centers with a mechanism known as\noffloading.\nThe architecture of the offloading capabilities of the AI_INFN platform consists of few\nself-consistent components that can be used by the application to scale beyond the single\nnotebook instance. Indeed, different components may introduce different limits or limitations\non the scaling capabilities and should be selected wisely on a per-application basis.\nUsers are allowed to scale beyond their notebook instance by creating Kubernetes jobs,\nenqueued and assigned to either local or remote resources by the Kueue controller23. Kueue\nis designed to use local resources in an opportunistic way, configuring the running batch jobs\nto be immediately evicted in case new notebook instances are spawned pushing the cluster\nin a condition of resource contention. User do not create jobs directly accessing Kubernetes\nAPIs, but passing through a dedicated microservice, named vkd, that validates user’s request\nbased on memberships criteria and manage Kubernetes secrets that are not intended to be\nexposed to users, but still are needed for their jobs to be executed in the platform.\nAn interesting feature supported by vkd is the ability of cloning the notebook instance,\nreplacing the start-up commands spawning the notebook with user-defined commands. In\npractice, these Bunshin Jobs provide a very simple interface to scale the application within the\ncluster boundaries as the applications developed within the notebook instance are guaranteed\nto run identically in the cloned instances.\nUsers can flag their jobs at submission time as compatible with offloading. The compati-\nbility of a job with offloading should be evaluated considering technical aspects (for example,\nan offloaded job cannot rely on the local storage resources such as NFS), practical consider-\nation (for example, the longer delay between submission and execution in large data centers\nmay make offloading ineffective for very short jobs), and policy restrictions (for example,\nsecrets to access confidential data cannot be shared with a remote data center). Kueue may\nthen assign jobs marked as compatible with offloading to virtual nodes.\nVirtual nodes are Kubernetes nodes that are not backed by a Linux kernel but mimic a\nKubernetes kubelet in the interactions with the Kubernetes API server. The software compo-\nnent providing this interface is named Virtual Kubelet24 and is designed to ease the integration\nwith various resource providers. The AI_INFN platform relies on the InterLink [11] provider.\nA further abstraction layer defining a simplified set of REST APIs that can be implemented by\nthe so-called InterLink plugins providing the actual access to the compute resources. At the\ntime of writing, the AI_INFN platform is interfaced with plugins accessing HTCondor [12],\nSlurm [13] and Podman25 resources. Following a recent integration test, a Kubernetes plugin\nwill be brought to production soon. A schematic representation of the architecture is provided\nin Figure 1.\nTo ease the deployment of application in the remote data centers, the AI_INFN platform\nrelies on dedicated and distributed file system based on JuiceFS using Redis as metadata\nengine and an S3 endpoint for data storage. The secrets to mount the shared file system\nare shipped to the remote data center that, if allowed by site-specific policies, can make it\navailable to the applications as a FUSE file system. Relying on the distributed file system\n23Kueue, https://kueue.sigs.k8s.io/.\n24Virtual Kubelet, https://virtual-kubelet.io/.\n25Podman, https://podman.io/.\n\n\nFigure 1. Enabling offloading using interLink\nas Virtual Kubelet provider.\nFigure 2.\nScalability test involving:\nthe\nINFN-Tier1 at CNAF in Bologna provisioned\nvia HTCondor (labeled infncnaf); the CINECA\nLeonardo super computer in Bologna provisioned\nvia Slurm (leonardo); a Virtual Machine in the\nCloud provisioned via Podman (podman); a Ter-\nabit HPC-Bubble in Padova provisioned via Slurm\n(terabitpadova). The label recas in the legend\nrefers to a WLCG Tier-2 site in Bari, integrated, but\nnot taking part to the test.\ndrastically hinder the scalability of the developed application, but provides a precious inter-\nmediate level between cluster-local development and multi-site distributed production.\nFigure 2 reports a recent scalability test involving resources provisioned by four different\nsites, without distributing the file system and for CPU-only payloads of the LHCb Flash\nSimulation [14].\n5 Conclusion\nMachine Learning and Artificial Intelligence have been reshaping the landscape of data pro-\ncessing and data analysis applications, making it easier for data analysts and data scientist\nto accelerate a variety of computing-intensive tasks on GPUs. The AI_INFN initiative is\ndeveloping and serving a highly customizable development platform, integrated in the ser-\nvice portfolio of INFN Cloud and provisioning different GPU models, possibly installed in\nremote computing centers through offloading techniques. Although primarily a research and\ndevelopment project, the AI_INFN platform is gaining recognition from several small ex-\nperiments within the AI_INFN research lines as a potential provider of GPU-accelerated\ncomputing resources, and is poised to play a trailblazing role in the future landscape of the\nINFN computing infrastructure.\nAcknowledgements\nThe work presented in this paper is performed in the framework of Spoke 0 and Spoke 2 of\nthe ICSC project – Centro Nazionale di Ricerca in High Performance Computing, Big Data\nand Quantum Computing, funded by the NextGenerationEU European initiative through the\nItalian Ministry of University and Research, PNRR Mission 4, Component 2: Investment 1.4,\nProject code CN00000013 - CUP I53C21000340006.\nReferences\n[1] R. Bommasani et al., On the Opportunities and Risks of Foundation Models, https:\n//crfm.stanford.edu/report.html (2021), 2108.07258\n\n\n[2] K. Albertsson et al., Machine Learning in HEP Community White Paper, J. Phys. Conf.\nSer. 1085, 022008 (2018), 1807.02876. 10.1088/1742-6596/1085/2/022008\n[3] D. Ciangottini et al., Analysis Facilities White Paper, FERMILAB-PUB-24-0763-\nCSAID (2024), 2404.02100\n[4] C. Grandi et al., ICSC: The Italian National Research Centre on HPC, Big\nData and Quantum computing, EPJ Web Conf. 295, 10003 (2024). 10.1051/epj-\nconf/202429510003\n[5] D. Salomoni et al., INFN and the evolution of distributed scientific computing in Italy,\nEPJ Web Conf. 295, 10004 (2024). 10.1051/epjconf/202429510004\n[6] L. Anderlini et al., ML_INFN project: Status report and future perspectives, EPJ Web\nConf. 295, 08013 (2024). 10.1051/epjconf/202429508013\n[7] L. Anderlini et al., Developing Artificial Intelligence in the Cloud: the AI_INFN plat-\nform, in 2nd International Workshop on Machine Learning and Quantum Computing\nApplications in Medicine and Physics (2024)\n[8] A. Ceccanti et al., The INDIGO-Datacloud Authentication and Authorization Infras-\ntructure, J. Phys. Conf. Ser. 898, 102016 (2017). 10.1088/1742-6596/898/10/102016\n[9] S.A. Weil et al., Ceph: a scalable, high-performance distributed file system, in Proceed-\nings of the 7th Symposium on Operating Systems Design and Implementation (USENIX\nAssociation, USA, 2006), OSDI ’06, p. 307–320, ISBN 1931971471\n[10] M. Antonacci, D. Salomoni (INFN Cloud Team), Leveraging TOSCA orchestration to\nenable fully automated cloud-based research environments on federated heterogeneous\ne-infrastructures, PoS ISGC&HEPiX2023, 020 (2023). 10.22323/1.434.0020\n[11] D. Ciangottini et al., Unlocking the compute continuum: scaling out from cloud to HPC\nand HTC resources, in 27th International Conference on Computing in High Energy &\nNuclear Physics (2024)\n[12] B. Bockelman et al., Principles, technologies, and time: The translational journey of the\nHTCondor-CE, Journal of Computational Science 52, 101213 (2021), case Studies in\nTranslational Computer Science. https://doi.org/10.1016/j.jocs.2020.101213\n[13] A.B. Yoo et al., SLURM: Simple Linux Utility for Resource Management, in Job\nScheduling Strategies for Parallel Processing, edited by D. Feitelson, L. Rudolph,\nU. Schwiegelshohn (Springer Berlin Heidelberg, Berlin, Heidelberg, 2003), pp. 44–60\n[14] M. Barbetti, The flash-simulation paradigm and its implementation based on Deep Gen-\nerative Models for the LHCb experiment at CERN, CERN-THESIS-2024-108 (2024)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21266v1.pdf",
    "total_pages": 8,
    "title": "Supporting the development of Machine Learning for fundamental science in a federated Cloud with the AI_INFN platform",
    "authors": [
      "Lucio Anderlini",
      "Matteo Barbetti",
      "Giulio Bianchini",
      "Diego Ciangottini",
      "Stefano Dal Pra",
      "Diego Michelotto",
      "Carmelo Pellegrino",
      "Rosa Petrini",
      "Alessandro Pascolini",
      "Daniele Spiga"
    ],
    "abstract": "Machine Learning (ML) is driving a revolution in the way scientists design,\ndevelop, and deploy data-intensive software. However, the adoption of ML\npresents new challenges for the computing infrastructure, particularly in terms\nof provisioning and orchestrating access to hardware accelerators for\ndevelopment, testing, and production. The INFN-funded project AI_INFN\n(\"Artificial Intelligence at INFN\") aims at fostering the adoption of ML\ntechniques within INFN use cases by providing support on multiple aspects,\nincluding the provision of AI-tailored computing resources. It leverages\ncloud-native solutions in the context of INFN Cloud, to share hardware\naccelerators as effectively as possible, ensuring the diversity of the\nInstitute's research activities is not compromised. In this contribution, we\nprovide an update on the commissioning of a Kubernetes platform designed to\nease the development of GPU-powered data analysis workflows and their\nscalability on heterogeneous, distributed computing resources, possibly\nfederated as Virtual Kubelets with the interLink provider.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}