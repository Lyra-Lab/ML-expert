{
  "id": "arxiv_2502.21257v1",
  "text": "RoboBrain: A Unified Brain Model for Robotic Manipulation\nfrom Abstract to Concrete\nYuheng Ji2,3,6,∗, Huajie Tan1,2,∗, Jiayu Shi1,2,∗, Xiaoshuai Hao2,∗,†, Yuan Zhang1,2, Hengyuan Zhang1,2\nPengwei Wang2,†, Mengdi Zhao2, Yao Mu5, Pengju An1,2, Xinda Xue1,2, Qinghang Su2,4, Huaihai Lyu2,3,6\nXiaolong Zheng3,6, Jiaming Liu1,2, Zhongyuan Wang2, Shanghang Zhang1,2,B\n1 State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University\n2 Beijing Academy of Artificial Intelligence 3 Institute of Automation, Chinese Academy of Sciences\n4 Institute of Information Engineering, Chinese Academy of Sciences 5 The University of Hong Kong\n6 School of Artificial Intelligence, University of Chinese Academy of Sciences\nShareRobot\nPlace a cup onto a plate\n1. Reach for the cup\n2. Grasp the cup\n3. Lift the cup\n4. Move the cup towards the plate\n5. Lower the cup onto the plate\n6. Release the cup\n01 Planning\n03 Trajectory\n02 Affordance\nGrasp the cup, \nand provide the \nAffordance\nGive the affordance box \ncoordinates according to \nthe instruction. \nGive the Trajectory \nwaypoints according to \nthe instruction. \nLong-term task planning.\nOpenEQATask\nRoboVQATask\n1_Q To reach the goal of \n<picking up an apple>, \nwhich task should be \nprioritized next?\n1_A \nlift \nthe \napple\n2_Q Is now a \nsuitable time to \ncarry out <lift \nthe apple>?\n2_A \nyes, \nit \nis\nFigure 1. Overview of RoboBrain. RoboBrain consists of three key robotic capabilities: planning capability, affordance perception, and\ntrajectory prediction. RoboBrain outperforms previous MLLMs in robotics tasks. The bottom part shows the composition of RoboBrain’s\ntraining data and provides a specific example of visual question answering from our proposed ShareRobot. Best viewed on screen.\nAbstract\nRecent advancements in Multimodal Large Language\nModels (MLLMs) have shown remarkable capabilities\n∗Equal contribution.\n† Project leaders.\nB Corresponding author.\nacross various multimodal contexts. However, their appli-\ncation in robotic scenarios, particularly for long-horizon\nmanipulation tasks, reveals significant limitations. These\nlimitations arise from the current MLLMs lacking three\nessential robotic brain capabilities:\nPlanning Capabil-\nity, which involves decomposing complex manipulation in-\nstructions into manageable sub-tasks; Affordance Per-\n1\narXiv:2502.21257v1  [cs.RO]  28 Feb 2025\n\n\nception, the ability to recognize and interpret the affor-\ndances of interactive objects; and Trajectory Prediction,\nthe foresight to anticipate the complete manipulation tra-\njectory necessary for successful execution.\nTo enhance\nthe robotic brain’s core capabilities from abstract to con-\ncrete, we introduce ShareRobot, a high-quality heteroge-\nneous dataset that labels multi-dimensional information\nsuch as task planning, object affordance, and end-effector\ntrajectory. ShareRobot’s diversity and accuracy have been\nmeticulously refined by three human annotators.\nBuild-\ning on this dataset, we developed RoboBrain, an MLLM-\nbased model that combines robotic and general multi-modal\ndata, utilizes a multi-stage training strategy, and incorpo-\nrates long videos and high-resolution images to improve its\nrobotic manipulation capabilities. Extensive experiments\ndemonstrate that RoboBrain achieves state-of-the-art per-\nformance across various robotic tasks, highlighting its po-\ntential to advance robotic brain capabilities. Project web-\nsite: RoboBrain.\n1. Introduction\nRecent advancements in Multimodal Large Language Mod-\nels (MLLMs) have significantly advanced the pursuit of Ar-\ntificial General Intelligence (AGI). By leveraging extensive\nmultimodal datasets sourced from the internet and employ-\ning self-supervised learning techniques, MLLMs demon-\nstrate exceptional capabilities in visual perception and un-\nderstanding human language instructions, excelling in tasks\nsuch as visual question answering [3, 14, 15], image cap-\ntioning [35, 37], and sentiment analysis [17]. Despite sig-\nnificant progress in MLLMs, the exploration of their appli-\ncation in robotics remains in its early stages, highlighting a\ncrucial area for further research and innovation.\nRecent studies have examined the application of MLLMs\nin robotics, focusing on planning and subgoal decompo-\nsition [6, 25], action sequencing [8, 9], and replanning\nand feedback [41, 46].\nHowever, their effectiveness in\nrobotic scenarios—particularly for long-horizon manipula-\ntion tasks—reveals significant limitations.\nThese limita-\ntions stem from the current MLLMs’ lack of three critical\nrobotic capabilities: planning, affordance perception, and\ntrajectory prediction, as illustrated in Fig. 1. For instance,\nconsider a robotic arm tasked with lifting a teapot and pour-\ning water into a cup. The MLLM should be capable of de-\ncomposing this task into sub-tasks, such as “approach the\nteapot and lift it”, “move the teapot until the spout is po-\nsitioned over the cup”, and “tilt the teapot to pour”. For\neach sub-task, such as “approach and grasp the teapot”,\nthe MLLM must utilize affordance perception to accurately\nidentify the graspable regions of the teapot. Additionally,\ntrajectory prediction is essential for determining the com-\nplete path from the starting point to the graspable part of\nthe teapot. This challenge for existing MLLMs primarily\narises from the scarcity of large-scale, fine-grained datasets\nspecifically designed for robotic operation tasks.\nTo empower the RoboBrain’s core capabilities from ab-\nstract to concrete, we first introduce ShareRobot, a large-\nscale, fine-grained dataset specifically designed for robotic\noperation tasks. Specifically, we label multi-dimensional\ninformation such as task planning, object affordance, and\nend-effector trajectory.\nBuilding upon ShareRobot, we\ndeveloped RoboBrain, an MLLM model based on the\nLLaVA [40] architecture, aimed at enhancing the percep-\ntion and planning capabilities of robots in complex tasks.\nIn the process of training RoboBrain, we meticulously de-\nsigned the ratio of robotic data to general multi-modal data,\nimplemented a multi-stage training strategy, and incorpo-\nrated long videos and high-resolution images.\nThis ap-\nproach endowed RoboBrain with powerful visual informa-\ntion perception capabilities in robotic scenarios, support-\ning historical frame memory and high-definition image in-\nput, thereby further enhancing the ability in robotic manip-\nulation planning. Extensive experimental results demon-\nstrate that RoboBrain outperforms existing models across\nmultiple robotic benchmarks, including RoboVQA [60]\nand OpenEQA [49], achieving state-of-the-art performance.\nAdditionally, it shows competitive results in trajectory and\naffordance prediction accuracy. These findings validate the\neffectiveness of the proposed dataset and framework in en-\nhancing robotic brain capabilities. In summary, the main\ncontributions of this paper are as follows:\n• We propose RoboBrain, a unified multimodal large lan-\nguage model designed for robotic manipulation, which\nfacilitates more efficient task execution by transforming\nabstract concepts into concrete actions.\n• We meticulously designed the ratio of robotic data to gen-\neral multi-modal data, implemented a multi-stage train-\ning strategy, and incorporated long videos and high-\nresolution images. This approach provided RoboBrain\nwith historical frame memory and high-resolution image\ninput, thereby further enhancing its capabilities in robotic\nmanipulation planning.\n• We introduce ShareRobot, a high-quality heterogeneous\ndataset that labels multi-dimensional information, includ-\ning task planning, object affordance, and end-effector tra-\njectory, effectively enhancing various robotic capabilities.\n• Comprehensive experimental results demonstrate that\nRoboBrain achieves state-of-the-art performance across\nvarious embodied benchmarks, highlighting its potential\nfor real-world applications in robotics.\n2. Related Work\nMLLM for Robotic Manipulation Planning Existing\nstudies mostly utilize MLLMs primarily focus on under-\nstanding natural language and visual observation tasks [6–\n2\n\n\nData Selection\nPlanning\nPlanning \nremaining steps\nPlanning with\ncontext task\nSuccess \n(negative)\nSuccess \n(positive)\nPast description\nGenerative \naffordance\nFuture \nprediction\nAffordance\n(positive)\nAffordance \n(negative)\nQ: The objective is <task summary>. What \nshould be the next step to move forward?\nA: <task n>\nQ: You are towards <task summary>. After \ncompleting steps 1-<task 1>, ..., n-1-<task \nn-1>, what is the next immediate task? \nA: <task n>\nQ: Given the progress : 1-<task 1>, ..., n-1-\n<task n-1>, what is the next set of five \nsteps to move closer to <long-horizon>?\nA: 1-<task n> ... 5-<task n+4>\nQ: After <task n-1>, what’s the most \nprobable next event?\nA: <task n>\nQ: Was the execution of \n<task n> successful or not?\nA: yes\nQ: Has <task n> been fully carried out?\nA: no\nQ: Is <task n> something that can be \naccomplished right now?\nA: yes\nQ: Are you doing <random \ntask> at this very moment?\nA: no\nQ: What can you do at this moment?\nA: <task n>\nQ: What just occurred?\nA: <task n>\n•\nHigh-resolution image\n•\nAccurate description\n•\nSuccess status\n•\nLong video length\n…\nData Labeling - Planning \nTask summary: Placing a cucumber into a cup.\nTask n-1: Reach for the cucumber\nTask n: Grasp the cucumber\nTask n+1: Move the cucumber towards the cup\n…\nAdd water to \nthe coffee pot.\nPour the \ncoffee from \nthe coffee \npot.\nData Labeling - Affordance \nData Labeling - Trajectory\nMove towards \nthe spoon.\n…\nFigure 2. The generation procession of our ShareRobot dataset. Our dataset labels multi-dimensional information, including task\nplanning, object affordance, and end-effector trajectories. The task planning is first annotated by atomic tasks and then augmented by\nconstructing question-answer pairs. The affordance and trajectory are labeled on the images according to the specific instructions.\n8, 30], with fewer addressing the decomposition of high-\nlevel task instructions into actionable steps. PaLM-E [19]\ngenerates multimodal inputs by mapping real-world ob-\nservations into the language embedding space. RT-H [6]\nand Robomamba [42] generate reasoning results along with\nrobot actions obtained from an additional policy head.\nHowever, while these models generate planning texts and\nactions, they still lack adequate mechanisms for executing\ncomplex atomic tasks, highlighting the need for enhanced\naffordance perception and trajectory prediction.\nDatasets for Manipulation Planning Early datasets for\nManipulation [11, 22, 31, 44, 62] mainly comprise anno-\ntated images and videos that highlight fundamental hand-\nobject interactions, including grasping and pushing. Recent\nadvancements [18, 60] in robotic manipulation emphasize\nmulti-modal and cross-embodiment datasets for enhanced\ngeneralization.\nDatasets such as RH20T [20], Bridge-\nDataV2 [68], and DROID [28] enhance scene diversity,\nbroadening the range of manipulation scenarios. Notably,\nRT-X [54] compiles data from 60 datasets across 22 em-\nbodiments into the Open X-Embodiment (OXE) repository.\nIn this work, we extract high-quality data from OXE, de-\ncompose high-level descriptions into low-level planning in-\nstructions, and adapt these into a question-answer format to\nenhance model training.\n3. ShareRobot Dataset\nTo enhance the RoboBrain’s capability of planning, affor-\ndance perception, and trajectory prediction, we develop\na dataset called ShareRobot–a large-scale, fine-grained\ndataset specifically designed for robotic operation tasks.\nThe generation procession of our dataset is shown as Fig. 2\nThe details are described in the following sections.\n3.1. Overview\nShareRobot is a comprehensive dataset, facilitates more ef-\nficient task execution by transforming abstract concepts into\nconcrete actions.\nThe main features of the ShareRobot\ndataset include:\n• Fine-grained\nUnlike\nthe\nOpen\nX-Embodiment\ndataset[53], which only offers generalized high-level task\ndescriptions, each data point in ShareRobot includes de-\ntailed low-level planning instructions linked to individual\nframes. This specificity enhances the model’s ability to\nexecute tasks accurately at the right moment.\n• Multi-dimensional To enhance RoboBrain’s capabilities\nfrom abstract to concrete, we label task planning, ob-\nject affordances, and end-effector trajectories, allowing\nfor greater flexibility and precision in task processing.\n• High quality We establish rigorous criteria for selecting\n3\n\n\n(a) Source Data Distribution\n(b) Cross-embodiment Distribution\n(c) Statics of types of atomic tasks\nFigure 3. The diversity of our ShareRobot dataset. Our dataset involves (a) 23 original datasets, (b) 12 embodiments and (c) 107 types\nof atomic tasks. The distribution of the top 20 most frequent atomic actions within our ShareRobot dataset is presented in (c).\ndata from the Open-X-Embodiment dataset[53], focusing\non high resolution, accurate descriptions, successful task\nexecution, visible affordance, and clear motion trajecto-\nries. Based on these criteria, we validate 51,403 instances\nto ensure high quality, forming the foundation for Robo-\nBrain’s core capabilities.\n• Large scale With 1,028,060 question-answer pairs,\nShareRobot is the largest open-source dataset for task\nplanning, affordance prediction, and trajectory prediction,\nenabling deeper understanding of complex relationships\nfrom abstract to concrete.\n• Rich diversity In contrast to the RoboVQA[60] dataset’s\nlimited scenes, ShareRobot features 102 scenes across 12\nembodiments and 107 types of atomic tasks, as shown\nin Fig. 3. This diversity allows MLLMs to learn from\nvaried real-world contexts, enhancing robustness in com-\nplex, multi-step planning.\n• Easy scalability Our data generation pipeline is de-\nsigned for high scalability, facilitating expansion as new\nrobotic embodiments, task types, and environments de-\nvelop. This adaptability ensures the ShareRobot dataset\ncan support increasingly complex manipulation tasks.\n3.2. Data Selection\nBased on the Open X-embodiment dataset [53], we care-\nfully selected 51,403 instances, mainly focusing on image\nquality, description accuracy and success status. Our data\ncollection process adheres to the following principles:\n• High-resolution image We eliminate videos lacking im-\nages or those with low resolution. Any video with a reso-\nlution below 128 pixels is removed.\n• Accurate description Videos without descriptions or\nwith vague descriptions are filtered out to avoid affecting\nthe planning capability of the model.\n• Success status We discard videos conducting failed tasks,\nas unsuccessful demonstrations affect the model’s learn-\ning.\n• Long video length Videos with fewer than 30 frames are\nexcluded, as they contain only atomic tasks.\n• Object not covered We remove any videos where the tar-\nget object or end-effector is covered by other objects, as\nour model has to accurately identify the positions of end-\neffectors and the object’s affordance.\n• Clear Trajectories We exclude the demonstrations with\nunclear or incomplete trajectories, as trajectory recogni-\ntion is one of our RoboBrain’s capabilities.\n3.3. Data Labeling\nPlanning Labeling We extract 30 frames from each robotic\noperation demonstration. We use these frames along with\ntheir high-level descriptions to decompose them into low-\nlevel planning instructions using Gemini [63]. Three anno-\ntators then review and refine these instructions to ensure the\nprecision of labeling. The low-level planning data is for-\nmatted to align with the RoboVQA [60] structure for model\ntraining, employing question templates for the 10 question\ntypes in RoboVQA. This process transforms 51,403 low-\nlevel planning entries into 1,028,060 question-answer pairs,\nwith annotators monitoring data generation to maintain the\ndataset’s integrity.\nAffordance Labeling We filter 8,511 images from the\ndataset and annotate each with affordance areas. For each\n30-frame demonstration, we label the affordance in the\nfirst frame, corresponding to the contact regions between\nthe end-effectors and the objects. We identify the contact\nframe, where the end-effectors first contact the object, and\nlabel the ground truth bounding box in the first frame as\n{l(x), l(y), r(x), r(y)}, where {l(x), l(y)} are the top left co-\nordinates and {r(x), r(y)} are the bottom right corner coor-\ndinates.\nTrajectory Labeling We annotate 8,511 images with\nbounding boxes for the gripper, maintaining consistency\nwith the affordance bounding box format.\nEach end-\neffector is labeled with three parts: the entire gripper, the\nleft finger, and the right finger. This data serves to calcu-\nlate trajectory positions and train a gripper detector. The\ntrajectory position is determined by averaging the bound-\ning boxes of the left and right fingers, allowing for efficient\nlabeling of additional data.\n4\n\n\nQwen2.5\nTokenizer\nProjector\nSiglip\n1. Reach for the cup\n2. Grasp the cup\n3. Lift the cup\n4. ..\n5. ..\n6. Release the cup\n01 Task Planning\n+\nA-LoRA\n+\nT-LoRA\n02 Affordance\nHigh-level Task \nDescription\n<𝒍𝒍(𝒙𝒙), 𝒍𝒍(𝒚𝒚) , 𝒓𝒓(𝒙𝒙) , 𝒓𝒓(𝒚𝒚) >\n03 Trajectory\ngrasp the \ncup, and \nprovide \naffordance\nProvide \nthe \nwaypoints \nof \ntrajectory\nEGO View\nInstruction\nSub-Task\nDescription\nPlace a cup \nonto a plate\nSub-Task\nDescription\nVideo\nImage\nMultiple Images\nFigure 4. The pipeline of our RoboBrain. The images, multiple images, and videos are sent into our model to pre-train a foundation\nrobotic brain. Besides, we fine-tune the RoboBrain via A-LoRA and T-LoRA to develop Affordance and Trajectory skills. In practical\napplications, the model first generates detailed plans, and then splits it into sub-task descriptions to execute specific robotic tasks.\n3.4. Data Statistics\nWe\nselect\n23\noriginal\ndatasets\nfrom\nthe\nOpen\nX-\nembodiment dataset[53].\nThe distribution of the source\ndata is shown in the Fig. 3. The data involves 102 vari-\nous scenes (e.g. bedroom, laboratory, kitchen, office), and\ncovers 12 different robot bodies. According to statistics,\nthere are 132 types of atomic actions in this dataset, tasks\nwith higher word frequency are shown in Fig. 3 (c). The\n5 most frequent atomic tasks are “pick”, “move”, “reach”,\n“lift”, and “place”, which are frequent task types in real\nrobotic operation scenarios. This suggests that the distribu-\ntion of our dataset is reasonable. Finally, we get 1,028,060\nquestion-answer pairs for planning. For the planning QA\npairs dataset, we split 1 million QA pairs as the training\nset and 2,050 QA pairs as the test set. For the affordance\ndataset, we split 8000 images as the training set and 511\nimages as the test set. For the trajectory dataset, we allocate\n8000 images for training and 511 images for testing.\n4. RoboBrain Model\nIn this section, we provide an overview of RoboBrain. Our\ngoal is to enable the Multi-modal Large Language Model\n(MLLM) to understand abstract instructions and explic-\nitly output object affordance regions and potential oper-\national trajectories, facilitating a transition from abstract\nto concrete.\nWe employ a multi-stage training strategy:\nPhase 1 focuses on general OneVision (OV) training to de-\nvelop a foundational MLLM with strong understanding and\ninstruction-following abilities. Phase 2, the robotic training\nphase, aims to empower the core capabilities of RoboBrain\nfrom abstract to concrete.\n4.1. Model Architecture\nRoboBrain consists of three modules:\nthe foundational\nmodel for planning, the A-LoRA model for affordance per-\nception, and the T-LoRA model for trajectory prediction.\nIn practical applications, the model first generates detailed\nplans, and then splits it into sub-task descriptions to exe-\ncute affordance perception and trajectory prediction. The\npipeline of our RoboBrain is shown to Fig. 4.\nFoundational Model for Planning We utilize LLaVA\nas the foundational Model for RoboBrain, which consists\nof three main modules: the Vision Encoder (ViT) g(·), the\nProjectior h(·), and the Large Language Model (LLM) f(·).\nSpecifically, we employ SigLIP [74], a 2-layer MLP [39],\nand Qwen2.5-7B-Instruct [64]. Given an image or video\nXv as visual input, ViT encodes it into visual features Zv =\ng(Xv), which are then mapped to the semantic space of the\nLLM through Projectior, resulting in a sequence of visual\ntokens Hv = h(Zv). Finally, the LLM generates a textual\nresponse in an autoregressive manner based on the human\nlanguage instruction Xt and Hv.\n5\n\n\nStage-1\nStage-1.5\nStage-2\nStage-3\nStage-4\nSingle-Image\nOneVision\nA-LoRA\nT-LoRA\nVision\nResolution\n384\nMax 384×{2×2}\nMax 384×{6×6}\nMax 384×{6×6}\nMax 384×{6×6}\nMax 384×{6×6}\nMax 384×{6×6}\n#Tokens\n729\nMax 729×5\nMax 729×37\nMax 729×37\nMax 729×37\nMax 729×37\nMax 729×37\nData\nDataset\nLCS\nImage\nImage\nImage & Video\nRobotic Data\nAfford. Data\nTraj. Data\n#Samples\n558K\n4M\n3.2M\n1.6M\n3M\n10K\n400K\nModel\nTrainable\nProjector\nFull Model\nFull Model\nFull Model\nFull Model\nA-LoRA\nT-LoRA\n#Tunable Parameters\n17.0M\n8.0B\n8.0B\n8.0B\n8.0B\n28.0M\n28.0M\nTraining\nBatch Size\n8\n2\n1\n1\n1\n4\n4\nLR: ψViT\n-\n2 ×10−6\n2 ×10−6\n2 ×10−6\n2 ×10−6\n2 ×10−6\n2 ×10−6\nLR: {θProj., ϕLLM, ϕLoRA}\n1×10−3\n1 ×10−5\n1 ×10−5\n1 ×10−5\n1 ×10−5\n1 ×10−5\n1 ×10−5\nEpoch\n1\n1\n1\n1\n1\n1\n1\nTable 1. Detailed configuration for each training stage of the RoboBrain.\nA-LoRA Module for Affordance Perception The term\naffordance in our work refers to the area where the human\nhand makes contact with objects. During interactions, hu-\nmans instinctively engage with various objects within spe-\ncific regions. We utilize bounding boxes to represent affor-\ndances. Formally, consider an image I consisting of multi-\nple objects with their affordances: Oi = {A0\ni , A1\ni , ..., AN\ni },\nwhere the ith object owns N affordances. The format of af-\nfordance is defined as {l(x), l(y), r(x), r(y)}, and {l(x), l(y)}\nrepresents the top left corner coordinates of affordance,\nwhile {r(x), r(y)} is the bottom right corner coordinates.\nT-LoRA Module for Trajectory Prediction The term\ntrajectory in our work refers to the concept of 2D visual\ntraces, as presented in [21]. We define trajectory waypoints\nas a series of 2D coordinates representing the movement of\nthe end-effector or hand throughout the process. Formally,\nat time step t, the trajectory waypoints can be represented\nas Pt:N = {(xi, yi) | i = t, t + 1, . . . , N}, where (xi, yi)\ndenotes the i-th coordinate in the visual trace, and N repre-\nsents the total number of time steps in the episode.\n4.2. Training\nPhase 1: General OV Training In Phase 1, we drew on the\nstate-of-the-art training data and strategies from LLaVA-\nOneVision [34] to construct a foundational model with gen-\neral multi-modal understanding and visual instruction fol-\nlowing capabilities. This lays the groundwork for enhanc-\ning the model’s robotic manipulation planning abilities in\nPhase 2. Detailed information is provided in Tab. 1.\nIn Stage 1, we utilize the image-text data from the LCS-\n558K dataset [10, 59] to train Projector, facilitating the\nalignment of visual features Zv with the LLM semantic fea-\ntures Hv. In Stage 1.5, we train the entire model using 4M\nhigh-quality image-text data to enhance the model’s multi-\nmodal general knowledge understanding capabilities.\nIn\nStage 2, we further train the entire model with 3.2M single-\nimage data and 1.6M image and video data from LLaVA-\nOneVision-Data [34], aiming to enhance the instruction-\nfollowing abilities of RoboBrain and improve understand-\ning of high-resolution image and video.\nPhase 2: Robotic Training In Phase 2, we build upon\nthe robust multi-modal foundational model developed in\nPhase 1 to create a more powerful model for robotic manip-\nulation planning. Specifically, we aim for RoboBrain to un-\nderstand complex, abstract instructions, support the percep-\ntion of historical frame information and high-resolution im-\nages, and output object affordance regions while predicting\npotential manipulation trajectories. This will facilitate the\ntransition from abstract to concrete in manipulation plan-\nning tasks. Detailed information is provided in Tab. 1.\nIn stage 3, we collected a dataset of 1.3M robotic data to\nimprove the model’s robotic manipulation planning capa-\nbilities. Specifically, this data is sourced from RoboVQA-\n800K [60], ScanView-318K including MMScan-224K [24,\n47], 3RScan-43K[24, 67], ScanQA-25K [4, 24], SQA3d-\n26K [24, 48], and the subset of ShareRobot-200K intro-\nduced in this paper. These datasets contain a substantial\namount of scene-scanning image data, long video data, and\nhigh-resolution data to support the model’s ability to per-\nceive diverse environments. Additionally, the fine-grained,\nhigh-quality planning data within the ShareRobot dataset\nfurther enhances the robotic manipulation planning capabil-\n6\n\n\n(a)  OpenEQA Benchmark\n(b)  ShareRobot Benchmark\n(c)  RoboVQA Benchmark\nFigure 5. The performance of our model RoboBrain on the OpenEQA, ShareRobot, and RoboVQA benchmarks. RoboBrain surpassed all\nbaseline models, achieving state-of-the-art results.\nities of RoboBrain. To mitigate the issue of catastrophic for-\ngetting [75], we selected a subset of high-quality image-text\ndata from Phase 1 about 1.7M to mix with the robotic data\ncollected in Stage 3 for training, tuning the entire model ac-\ncordingly. In Stage 4, we further enhanced the model’s abil-\nity to perceive object affordances based on instructions and\npredict manipulation trajectories, utilizing affordance and\ntrajectory data annotated in the ShareRobot dataset. This\nwas achieved by introducing LoRA [23] modules for train-\ning to realize fine-grained planning capabilities. For the\nspecific construction of training data and the training meth-\nods employed, please refer to the supplementary materials.\n5. Experiment\n5.1. Implementation Details\nDuring the entire training phase,\nwe employed the\nZero3 [58] distributed training strategy, conducting all ex-\nperiments on a cluster of servers, each equipped with\n8×A800 GPUs. The training components for each stage,\nincluding image resolution settings, batch size, epochs, and\nlearning rates, are provided in Tab. 1.\n5.2. Evaluation Metrics\nPlanning\nTask\nWe\nselected\nRoboVQA\n[60],\nOpenEQA [49], and the test set of ShareRobot ex-\ntracted from the proposed ShareRobot dataset as robotic\nbenchmarks for multi-dimensional assessment.\nFor\nRoboVQA, we adopt the BLEU1 to BLEU4 metrics [56]\nused in RoboMamba [42] for evaluation.\nAdditionally,\nfor OpenEQA and ShareRobot, we use GPT-4o [55] as\nthe evaluation tool, scoring based on the alignment or\nsimilarity between model predictions and ground truth,\nwhich serves as the final performance score for the model.\nTrajectory Prediction We evaluate the similarity be-\ntween ground truth and predicted trajectories, both rep-\nresented as sequences of 2D waypoints normalized to\n[0, 1000), following Qwen2-VL [70]. The evaluation uses\nthree metrics: Discrete Fr´echet Distance (DFD) [21], Haus-\ndorff Distance (HD), and Root Mean Square Error (RMSE).\nDFD captures overall shape and temporal alignment, HD\nidentifies maximum deviation, and RMSE measures aver-\nage pointwise error. Together, these metrics provide a com-\nprehensive assessment of trajectory accuracy and similarity.\nAffordance Prediction Here, we utilize the average pre-\ncision (AP) to evaluate the affordance performance of our\nmodel. AP metric summarizes the precision-recall affor-\ndance curve, which plots the relationship between preci-\nsion and recall at various threshold settings. It is calculated\nacross multiple IoU (Intersection over Union) thresholds to\nobtain a more comprehensive evaluation.\n5.3. Evaluation on Robot Brain Task\nEvaluation on Planning Task We selected 6 powerful\nMLLMs as our baselines for comparison, including both\nopen-source and closed-source models with different archi-\ntectures. Specifically, these models include GPT-4V [2],\nClaude3 [1], llava1.5 [40], LLaVA-OneVision-7b [34],\nQwen2-VL-7b [69] and RoboMamba [42]. Our specific ex-\nperimental results are shown in Fig. 5. Our RoboBrain out-\nperformed all baseline models across four robotic bench-\nmarks. RoboBrain significantly outperformed all baseline\nmodels on OpenEQA and ShareRobot, which can be at-\ntributed to its robust capabilities in understanding robotic\ntasks and perceiving long videos. Additionally, this pat-\ntern was observed in other benchmarks as well, with Robo-\nBrain consistently demonstrating superior performance on\nRoboVQA, achieving a BLEU-4 score that exceeded that\nof the second-place model by 18.75. This result highlights\nits capability to decompose complex long-range task plan-\nning. Please refer to the supplementary materials for more\nablation studies due to space limitation.\nEvaluation on Trajectory Prediction We compare sev-\n7\n\n\nFigure 6. This visualization illustrates that RoboBrain can interpret human instructions and visual images to generate action plans and\nassessments based on real-time image feedback. Furthermore, it predicts trajectories for each step and identifies corresponding affordances.\nTable 2.\nTrajectory Prediction Results Comparison.\nDis-\ncrete Fr´echet Distance (DFD), Hausdorff Distance (HD), and Root\nMean Square Error (RMSE).\nMethod\nDFD ↓\nHD ↓\nRMSE ↓\nRoboBrain (Base)\n0.191\n0.171\n0.133\n+ Start Points\n0.176\n0.157\n0.117\n+ Max Points\n0.185\n0.163\n0.125\n+ Spec Token\n0.109 (42.9%↓) 0.010 (94.2%↓) 0.091 (31.6%↓)\neral variants of our model, and the results are in Tab. 2:\n(1) Baseline, fine-tuned on trajectory-related VQA data;\n(2) Start Points, which adds the 2D start coordinates of\nthe end-effector; (3) Max Points, limiting waypoints to 10\nvia uniform sampling; and (4) Spec Token & End Points,\nwhich adds end-effector positions and special tokens to em-\nphasize waypoints and start/goal points. Each variant builds\non the previous one, with the final model integrating all\ncomponents. Our most effective model integrates all de-\nsign choices. As shown in the last row of Tab. 2, DFD, HD,\nand RMSE decreased by 42.9%, 94.2%, and 31.6%, respec-\ntively, compared to the baseline. We found that adding start\npoints corrected the translational offset between the gener-\nated trajectory and the end-effector.\nEvaluation on Affordance Prediction Our results are\nsummarized in Tab. 3.\nWe compare the Qwen2-VL-7B\nand LLaVA-NeXT-7B models. Qwen2-VL [69] has a supe-\nrior visual grounding ability and LLaVA-NeXT [36] owns\na high-resolution and strong vision tower. We test them all\nTable 3. The comparison of affordance prediction. We utilize\nAP as the metric, and test them on affordance test set.\nModel\nAP ↑\nLLaVA-NeXT-7B [40]\n9.8 %\nQwen2-VL-7B [5]\n12.5 %\nRoboBrain (Ours)\n27.1 % (14.6↑)\non the AGD20K affordance test set. Our RoboBrain outper-\nforms significantly the other models. It surpasses Qwen2-\nVL [69] by 14.6 AP, and LLaVA-NeXT by 17.3 AP. It vali-\ndates our RoboBrain can understand the physical properties\nof objects and provide the affordance accurately.\n5.4. Visualization\nIn this section, we present visual examples of RoboBrain,\nas shown in Fig 6. Given human language instructions and\nvisual images, RoboBrain can engage in multi-turn inter-\nactions with humans, understanding and predicting future\nsteps. Additionally, it outputs more concrete trajectories\nand affordances.\n6. Conclusion\nIn this paper, we introduce ShareRobot, a high-quality\ndataset that labels multi-dimensional information, including\ntask planning, object affordance, and end-effector trajec-\ntory. We also present RoboBrain, an MLLM-based model\nthat integrates robotic and general multi-modal data, em-\nploys a multi-stage training strategy, and leverages long\n8\n\n\nvideos and high-resolution images to enhance robotic ma-\nnipulation. Extensive experiments demonstrate that Robo-\nBrain achieves state-of-the-art performance across various\nrobotic tasks, underscoring its potential to significantly ad-\nvance robotic capabilities.\nAcknowledgments This work was supported by the\nNational Natural Science Foundation of China (62476011,\n72225011 and 72434005).\nReferences\n[1] The claude 3 model family: Opus, sonnet, haiku. 7\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 7, 13, 14, 15\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In ICCV, pages 2425–2433,\n2015. 2\n[4] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki\nKawanabe. Scanqa: 3d question answering for spatial scene\nunderstanding. In CVPR, pages 19129–19139, 2022. 6, 13\n[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al.\nQwen technical report.\narXiv preprint\narXiv:2309.16609, 2023. 8, 12, 16\n[6] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet,\nQuon Vuong, Jonathan Tompson, Yevgen Chebotar, De-\nbidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies\nusing language. arXiv preprint arXiv:2403.01823, 2024. 2,\n3\n[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-\nishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\nRt-1: Robotics transformer for real-world control at scale.\narXiv preprint arXiv:2212.06817, 2022.\n[8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:\nVision-language-action models transfer web knowledge to\nrobotic control. arXiv preprint arXiv:2307.15818, 2023. 2,\n3\n[9] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex\nIrpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. In CoRL, pages\n287–318, 2023. 2\n[10] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts.\nIn CVPR,\npages 3558–3568, 2021. 6, 12\n[11] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov,\nAnkur Handa, Jonathan Tremblay, Yashraj S. Narang,\nKarl Van Wyk, Umar Iqbal, Stan Birchfield, Jan Kautz, and\nDieter Fox. Dexycb: A benchmark for capturing hand grasp-\ning of objects. In CVPR, pages 9044–9053, 2021. 3\n[12] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Juny-\ning Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jian-\nquan Li, Xiang Wan, and Benyou Wang. Allava: Harness-\ning gpt4v-synthesized data for a lite vision-language model.\narXiv preprint arXiv:2402.11684, 2024. 12\n[13] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, et al. Are we on the right way for evaluating large\nvision-language models? arXiv preprint arXiv:2403.20330,\n2024. 14\n[14] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-\nscaled multilingual language-image model. In ICLR. 2, 12\n[15] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. In CVPR,\npages 24185–24198, 2024. 2, 13, 14\n[16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 16\n[17] Ringki Das and Thoudam Doren Singh. Multimodal senti-\nment analysis: a survey of methods, trends, and challenges.\nACM Computing Surveys, 55(13s):1–38, 2023. 2\n[18] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair,\nBernadette Bucher, Karl Schmeckpeper, Siddharth Singh,\nSergey Levine, and Chelsea Finn.\nRobonet: Large-scale\nmulti-robot learning. In CoRL, pages 885–897, 2019. 3\n[19] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha\nChowdhery,\nBrian\nIchter,\nAyzaan\nWahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-\ne: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023. 3\n[20] Haoshu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu,\nChenxi Wang, Junbo Wang, Haoyi Zhu, and Cewu Lu.\nRH20T: A comprehensive robotic dataset for learning di-\nverse skills in one-shot. In ICRA, pages 653–660, 2024. 3\n[21] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan\nFu, Keerthana Gopalakrishnan, Zhuo Xu, et al. Rt-trajectory:\nRobotic task generalization via hindsight trajectory sketches.\narXiv preprint arXiv:2311.01977, 2023. 6, 7\n[22] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vin-\ncent Lepetit. Honnotate: A method for 3d annotation of hand\nand object poses. In CVPR, pages 3193–3203, 2020. 3\n[23] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank\nadaptation of large language models. In ICLR. 7, 12\n[24] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun\nLinghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu,\nBaoxiong Jia, and Siyuan Huang. An embodied generalist\nagent in 3d world. In ICLR 2024 Workshop: How Far Are\nWe From AGI. 6\n[25] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor\n9\n\n\nMordatch, Yevgen Chebotar, et al. Inner monologue: Em-\nbodied reasoning through planning with language models.\nIn CoRL, pages 1769–1782. PMLR, 2023. 2\n[26] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, et al.\nMistral 7b.\narXiv preprint\narXiv:2310.06825, 2023. 16\n[27] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In ECCV, pages 235–251. Springer,\n2016. 12, 14\n[28] Alexander Khazatsky,\nKarl Pertsch,\nSuraj Nair,\nAsh-\nwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti,\nSoroush Nasiriany, Mohan Kumar Srirama, Lawrence Yun-\nliang Chen, Kirsty Ellis, et al.\nDroid:\nA large-scale\nin-the-wild robot manipulation dataset.\narXiv preprint\narXiv:2403.12945, 2024. 3\n[29] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon\nNam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sang-\ndoo Yun, Dongyoon Han, and Seunghyun Park.\nOcr-free\ndocument understanding transformer. In ECCV, 2022. 12\n[30] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao,\nAshwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan\nFoster, Grace Lam, Pannag Sanketi, et al.\nOpenvla: An\nopen-source vision-language-action model. arXiv preprint\narXiv:2406.09246, 2024. 3, 16\n[31] Taein Kwon, Bugra Tekin, Jan St¨uhmer, Federica Bogo, and\nMarc Pollefeys. H2O: two hands manipulating objects for\nfirst person interaction recognition. In ICCV, pages 10118–\n10128, 2021. 3\n[32] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 14\n[33] Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan\nZhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan\nLi. Llava-next: What else influences visual instruction tun-\ning beyond data, 2024. 12, 16\n[34] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng\nLi, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and\nChunyuan Li.\nLlava-onevision: Easy visual task transfer.\narXiv preprint arXiv:2408.03326, 2024. 6, 7, 12, 13, 16\n[35] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming\nYan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao,\net al. mplug: Effective and efficient vision-language learning\nby cross-modal skip-connections. In EMNLP, pages 7241–\n7259, 2022. 2\n[36] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li,\nWei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave:\nTackling multi-image, video, and 3d in large multimodal\nmodels. arXiv preprint arXiv:2407.07895, 2024. 8, 12, 13,\n14, 15\n[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In ICML,\npages 19730–19742. PMLR, 2023. 2\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 16\n[39] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In CVPR,\npages 26296–26306, 2024. 5\n[40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. NeurIPS, 36, 2024. 2, 7, 8, 12\n[41] Jiaming Liu, Chenxuan Li, Guanqun Wang, Lily Lee,\nKaichen Zhou, Sixiang Chen, Chuyan Xiong, Jiaxin Ge,\nRenrui Zhang, and Shanghang Zhang. Self-corrected mul-\ntimodal large language model for end-to-end robot manipu-\nlation. arXiv preprint arXiv:2405.17418, 2024. 2\n[42] Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Lily Lee,\nKaichen Zhou, Pengju An, Senqiao Yang, Renrui Zhang,\nYandong Guo, and Shanghang Zhang. Robomamba: Mul-\ntimodal state space model for efficient robot reasoning and\nmanipulation. arXiv preprint arXiv:2406.04339, 2024. 3, 7,\n14, 15\n[43] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen\nYu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen\nJin, and Xiang Bai. Ocrbench: on the hidden mystery of\nocr in large multimodal models. Science China Information\nSciences, 67(12):220102, 2024. 14\n[44] Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang\nZhang, Yebin Liu, and Li Yi. TACO: benchmarking general-\nizable bimanual tool-action-object understanding. In CVPR,\npages 21740–21751, 2024. 3\n[45] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? In ECCV, pages 216–233. Springer, 2025.\n14\n[46] Zeyi Liu, Arpit Bahety, and Shuran Song. Reflect: Summa-\nrizing robot experiences for failure explanation and correc-\ntion. In CoRL, pages 3468–3484. PMLR, 2023. 2\n[47] Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan\nMao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming\nZhu, Dahua Lin, and Jiangmiao Pang. Mmscan: A multi-\nmodal 3d scene dataset with hierarchical grounded language\nannotations. arXiv preprint arXiv:2406.09401, 2024. 6, 13\n[48] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao\nLiang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated\nquestion answering in 3d scenes. In ICLR, 2023. 6, 13\n[49] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav\nPutta, Sriram Yenamandra, Mikael Henaff, et al. Openeqa:\nEmbodied question answering in the era of foundation mod-\nels. In CVPR, pages 16488–16498, 2024. 2, 7, 14, 15, 16\n[50] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In CVPR, pages\n3195–3204, 2019. 12\n[51] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question an-\nswering about charts with visual and logical reasoning. arXiv\npreprint arXiv:2203.10244, 2022. 14\n10\n\n\n[52] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\nDocvqa: A dataset for vqa on document images. In Proceed-\nings of the IEEE/CVF winter conference on applications of\ncomputer vision, pages 2200–2209, 2021. 14\n[53] Abby O’Neill, Abdul Rehman, Abhinav Gupta, Abhiram\nMaddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham\nLee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al.\nOpen x-embodiment:\nRobotic learning datasets and rt-x\nmodels. arXiv preprint arXiv:2310.08864, 2023. 3, 4, 5\n[54] Abby O’Neill, Abdul Rehman, Abhiram Maddukuri, Ab-\nhishek Gupta, Abhishek Padalkar, Abraham Lee, et al. Open\nx-embodiment: Robotic learning datasets and RT-X models\n: Open x-embodiment collaboration. In ICRA, pages 6892–\n6903, 2024. 3\n[55] OpenAI. Hello gpt-4o, 2024. 7, 13, 14, 15\n[56] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, pages 311–318, 2002. 7\n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748–8763. PMLR, 2021. 12\n[58] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. Deepspeed: System optimizations enable train-\ning deep learning models with over 100 billion parameters.\nIn SIGKDD, pages 3505–3506, 2020. 7\n[59] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. NeuIPS, 35:25278–\n25294, 2022. 6, 12\n[60] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, De-\nbidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan,\nGabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi,\net al.\nRobovqa: Multimodal long-horizon reasoning for\nrobotics. In ICRA, pages 645–652, 2024. 2, 3, 4, 6, 7, 13,\n14, 15, 16\n[61] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In CVPR,\npages 8317–8326, 2019. 14\n[62] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-\nitrios Tzionas.\nGRAB: A dataset of whole-body human\ngrasping of objects. In ECCV, pages 581–600, 2020. 3\n[63] Gemini Team, Rohan Anil, Sebastian Borgeaud, et al. Gem-\nini: A family of highly capable multimodal models, 2024.\n4\n[64] Qwen Team. Qwen2.5: A party of foundation models, 2024.\n5, 12\n[65] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun\nWoo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-\n1: A fully open, vision-centric exploration of multimodal\nllms. arXiv preprint arXiv:2406.16860, 2024. 12\n[66] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 16\n[67] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico\nTombari, and Matthias Nießner.\nRio: 3d object instance\nre-localization in changing indoor environments. In ICCV,\npages 7658–7667, 2019. 6, 13\n[68] Homer Rich Walke, Kevin Black, Tony Z. Zhao, Quan\nVuong, Chongyi Zheng, Philippe Hansen-Estruch, An-\ndre Wang He, Vivek Myers, Moo Jin Kim, Max Du, Abra-\nham Lee, Kuan Fang, Chelsea Finn, and Sergey Levine.\nBridgedata V2: A dataset for robot learning at scale.\nIn\nCoRL, pages 1723–1736, 2023. 3\n[69] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 7, 8, 13, 14, 15, 16\n[70] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 7\n[71] x.ai. Grok 1.5v vision preview. https://x.ai/blog/\ngrok-1.5v, 2024. Accessed: 2024-11-21. 14\n[72] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nGuohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang,\net al. Ureader: Universal ocr-free visually-situated language\nunderstanding with multimodal large language model. arXiv\npreprint arXiv:2310.05126, 2023. 12\n[73] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi. In CVPR, pages 9556–9567, 2024. 14\n[74] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn ICCV, pages 11975–11986, 2023. 5, 12\n[75] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu,\nYong Jae Lee, and Yi Ma. Investigating the catastrophic for-\ngetting in multimodal large language models. In NeurIPS\n2023 Workshop on Instruction Tuning and Instruction Fol-\nlowing. 7\n[76] Kaichen\nZhang,\nBo\nLi,\nPeiyuan\nZhang,\nFanyi\nPu,\nJoshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan\nZhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Re-\nality check on the evaluation of large multimodal models,\n2024. arXiv preprint arXiv:2407.12772. 13\n[77] Yuan Zhang,\nFei Xiao,\nTao Huang,\nChun-Kai Fan,\nHongyuan Dong, Jiawen Li, Jiacong Wang, Kuan Cheng,\nShanghang Zhang, and Haoyuan Guo. Unveiling the tapestry\nof consistency in large vision-language models.\narXiv\npreprint arXiv:2405.14156, 2024. 13\n11\n\n\nAppendix\nThis supplementary material provides more details of the\nproposed method and experiment results that are omitted\nfrom the manuscript due to the page limit. Sec. A presents\nadditional details of the models and training strategies.\nSec. B presents details of training dataset. Sec. C com-\nplements more experiment results and analysis.\nSec. D\nshows more visualization results to prove the effectiveness\nof RoboBrain. Sec. E introduces more details about the con-\nstruction of ShareRobot dataset.\nA. Details of Models and Training\nModel Setting. RoboBrain is built upon the LLaVA [40]\nframework and consists of three main components: the vi-\nsual encoder, projector, and large language model (LLM).\nFor the visual encoder, we utilized the SigLIP [74]\nmodel, specifically the siglip-so400m-patch14-384, which\nis pre-trained on WebLi [14] at a resolution of 384x384.\nThe SigLIP model improves upon traditional CLIP [57] ar-\nchitectures by employing a sigmoid loss function that op-\nerates solely on image-text pairs, eliminating the need for\nglobal normalization of pairwise similarities. This enhance-\nment allows for more efficient scaling of batch sizes while\nmaintaining performance, even at smaller scales. SigLIP\nhas 27 hidden layers and processes input images using\npatches of size 14x14, resulting in 729 visual tokens per\nimage. The projector consists of a two-layer MLP [36] that\nprojects the visual tokens obtained from the visual encoder\nto the dimensions of the text embeddings. For the LLM,\nwe adopted the Qwen2.5-7B-Instruct [64] model, which is\na state-of-the-art open-source LLM that is part of the latest\nQwen series [5]. It features 28 hidden layers and supports\nlong-context inputs of up to 128K tokens, providing multi-\nlingual capabilities across more than 29 languages.\nIn Stage 4, we introduced LoRA [23] to train RoboBrain,\nenabling it to acquire affordance perception and trajectory\nprediction capabilities. LoRA is a technique that allows for\nparameter-efficient fine-tuning of large models by adding\nlow-rank parameter matrices to existing layers. We incorpo-\nrated LoRA modules with a rank of 64 into the feed-forward\nnetwork layers of both the Projector and the LLM, freezing\nall parameters except those of the LoRA modules during\ntraining.\nTraining Setting. In the main text of the paper, we em-\nployed a staged training strategy, with complete settings\npresented in Tab. 4. We primarily referenced the training\nstrategy of LLaVA-Onevision [34], a state-of-the-art mul-\ntimodal large language model, and built upon this founda-\ntion to expand the robotic training phase. During the entire\ntraining phase, we conducted all experiments on a cluster of\nservers, each equipped with 8×A800 GPUs.\nFigure 7. The distribution of the entire training dataset.\nB. Details of Training Dataset\nIn the main body of the paper, we emphasize the impor-\ntance of the training data and the proportion of robotic data.\nIn this section, we will provide a detailed overview of the\ntraining data and its sources. The distribution of the entire\ntraining dataset is illustrated in Fig. 7.\n• LCS-558K is a subset of the LAION/CC/SBU dataset\n[10, 59], specifically designed as the LLaVA Visual In-\nstruct Pretrain [40] Dataset. This dataset has been care-\nfully filtered to achieve a more balanced distribution of\nconcept coverage, ensuring diverse and representative vi-\nsual content. The primary purpose of LCS-558K is to fa-\ncilitate the alignment between the visual encoder and the\nLLM, enabling the LLM to comprehend visual informa-\ntion.\n• Image-4M comprises 8 data sources, including 3 from\nthe LLaVA-Recap series [33]: BLIP558K, COCO118K,\nand CC3M, as well as UReader [72], Instruct Azure\nDC [33], Evol-Instruct [12], and SynthDog [29] We\nutilized the download links provided by the LLaVA-\nOneVision team for the data acquisition.\n• SI-3.2M [34] consists of 3.2 million samples, carefully\ncurated to support multimodal learning. It includes sub-\nsets from existing datasets such as Cambrian [65], Caul-\ndron [33], and UReader [72], which were subjected to\ncleaning and re-annotation to ensure data quality. Addi-\ntionally, it incorporates single-image data from sources\nlike AI2D [27] and OKVQA [50], alongside a newly\ncompiled single-image collection designed to achieve a\nDue to the unavailability of certain datasets, the actual data used\namounts to 3.1M.\n12\n\n\nTable 4. Detailed configuration for each training stage of the RoboBrain.\nStage-1\nStage-1.5\nStage-2\nStage-3\nStage-4\nSingle-Image\nOneVision\nA-LoRA\nT-LoRA\nVision\nResolution\n384\nMax 384×{2×2}\nMax 384×{6×6}\nMax 384×{6×6}\nMax 384×{6×6}\nMax 384×{6×6}\nMax 384×{6×6}\n#Tokens\n729\nMax 729×5\nMax 729×37\nMax 729×37\nMax 729×37\nMax 729×37\nMax 729×37\nModel\nTrainable\nProjector\nFull Model\nFull Model\nFull Model\nFull Model\nA-LoRA\nT-LoRA\n#Tunable Parameters\n17.0M\n8.0B\n8.0B\n8.0B\n8.0B\n28.0M\n28.0M\nTraining\nPer-device Batch Size\n8\n2\n1\n1\n1\n4\n4\nGradient Accumulation\n1\n2\n2\n2\n2\n2\n2\nLR: ψViT\n-\n2 ×10−6\n2 ×10−6\n2 ×10−6\n2 ×10−6\n2 ×10−6\n2 ×10−6\nLR: {θProj., ϕLLM, ϕLoRA}\n1×10−3\n1 ×10−5\n1 ×10−5\n1 ×10−5\n1 ×10−5\n1 ×10−5\n1 ×10−5\nEpoch\n1\n1\n1\n1\n1\n1\n1\nOptimizer\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\nDeepspeed\nZero3\nZero3\nZero3\nZero3\nZero3\nZero2\nZero2\nWeight Decay\n0\n0\n0\n0\n0\n0\n0\nWarmup Ratio\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\nLR Schedule\ncosine\ncosine\ncosine\ncosine\ncosine\ncosine\ncosine\nProjector Type\nmlp2x gelu\nmlp2x gelu\nmlp2x gelu\nmlp2x gelu\nmlp2x gelu\nmlp2x gelu\nmlp2x gelu\nVision Select Layer\n-2\n-2\n-2\n-2\n-2\n-2\n-2\nPatch Merge Type\nspatial unpad\nspatial unpad\nspatial unpad\nspatial unpad\nspatial unpad\nspatial unpad\nspatial unpad\nFrames Upbound\n-\n-\n-\n32\n32\n32\n32\nMax Seq Length\n8192\n32768\n32768\n32768\n32768\n4096\n4096\nGPU Nums\n16*8\n16*8\n20*8\n20*8\n22*8\n4*8\n4*8\nbalanced and diverse dataset.\n• OV-1.6M [34] comprises 1.6 million samples, which\nincludes approximately 800K high-quality samples re-\nsampled from earlier SI-3.2M datasets with a data replay\nstrategy, ensuring improved data reliability and relevance.\nAdditionally, the dataset incorporates M4-Instruct data to\nenrich instructional learning tasks. A significant compo-\nnent of OV-1.6M is its video data, which has been re-\nleased alongside LLaVA-video data. The video subset\nused in the dataset is specifically aligned with the pre-\nvious annotation format, providing a diverse multimodal\nresource for advancing vision-language learning.\n• RoboVQA-800K [60] consists of realistic data gathered\nfrom various user requests, utilizing different embodi-\nments including robots, humans, and humans equipped\nwith grasping tools.\nThe dataset features 5,246 long-\nhorizon episodes and 92,948 medium-horizon episodes\nof robotic tasks, with each episode accompanied by cor-\nresponding image and text prompt inputs. The primary\npurpose of RoboVQA-800K is to enhance RoboBrain’s\nreasoning capabilities in robotic-related scenarios.\n• ScanView-318K totals 318K samples,\nwhich inte-\ngrates data from several high-quality sources, includ-\ning MMScan-224K [47], 3RScan-43K [67], ScanQA-\n25K [4], and SQA3D-26K [48], each contributing unique\nstrengths.\nMMScan-224K provides multimodal scene\ndata with detailed annotations, such as object segmen-\ntation and textual descriptions. 3RScan-43K offers 3D\nreconstructions and semantic annotation. ScanQA-25K\nDue to the vague descriptions and missing key information regarding\ndataset filtering in the original paper, we ended up using 2.4M data.\nincludes question-answer pairs based on 3D scanned en-\nvironments. SQA3D-26K focuses on spatial question an-\nswering. Together, these datasets provide diverse scene-\nscanning image data, long video sequences, and high-\nresolution samples, equipping models with fine-grained\nenvironmental perception and reasoning abilities.\nC. Complementary Experiments\nIn this section, we present the complete experiments and\nresults that are omitted from the manuscript due to page\nlimitations. This includes an exploration of the impact of\nincorporating ShareRobot on training, the effects of vary-\ning proportions of robotic data in the training dataset, and\nmore comprehensive results comparing RoboBrain with the\nbaselines on both general and robotic benchmarks.\nAdditionally, we explore the impact of different archi-\ntectures and pre-trained VLMs, as well as different LLM\nbackbones on our experimental results. We also conduct\nablation studies at various stages to meticulously analyze\nthe contributions of each stage to overall performance.\nC.1. More Results on General Benchmarks\nTo evaluate performance on general tasks in real-world sce-\nnarios, as is commonly done with MLLMs [2, 15, 36, 55,\n69], we conducted experiments using a diverse set of im-\nage benchmarks summarized in Table 5. We leveraged the\ncomprehensive evaluation toolkit, LMMs-Eval[76, 77], to\nevaluate RoboBrain’s performance on general benchmarks.\nThese benchmarks are categorized into three classes:\n• Chart, Diagram, and Document Understanding. As\nkey visual formats for structured OCR data, benchmarks\n13\n\n\nTable 5. Performance comparison on multiple general benchmarks.\nDataset\nSplit\nRoboBrain (Ours)\nGPT-4V [2]\nLLaVA-OV-7B [36]\nInternVL2-8B [15]\nQwen2-VL-7B [69]\nGPT-4o [55]\nA12D[27]\ntest\n82.03\n78.2\n81.4\n83.8\n-\n94.2\nChartQA[51]\ntest\n80.48\n78.5\n80\n83.3\n83\n85.7\nDocVQA[52]\ntest\n88\n88.4\n87.5\n91.6\n94.5\n92.8\nTextVQA[61]\nval\n75.85\n-\n71.07\n77.4\n84.3\n-\nMMMU[73]\nval\n49\n56.8\n48.8\n51.8\n54.1\n69.1\nMMStar[13]\ntest\n61.23\n57.1\n61.7\n61.5\n60.7\n63.9\nOCRBench[43]\n-\n677\n656\n697\n794\n845\n805\nRealWorldQA[71]\ntest\n68.89\n61.4\n66.3\n64.4\n70.1\n58.6\nSeedBench[32]\nimage\n71.03\n49.9\n75.4\n76.2\n-\n76.2\nMMbench[45]\nen-dev\n81.52\n81.3\n83.2\n-\n-\n83.4\nMMbench[45]\nen-test\n80.44\n75\n80.8\n81.7\n83\n-\nMME[? ]\ntest\n2084\n1926\n1998\n2210\n2327\n-\nsuch as AI2D [27], ChartQA [51], DocVQA [52], and\nOCRBench [43] were utilized. Open-source models like\nInternVL2-8B [15] and LLAVA-OV-7B [36] have demon-\nstrated comparable performance to closed-source mod-\nels such as GPT-4V [2]. For RoboBrain, despite being\noptimized primarily for multidimensional robotic tasks,\nit surpasses LLAVA-OV-7B [36] and GPT-4V [2] on\nthese benchmarks, achieving a significant improvement\nin structured OCR tasks, with the only exceptions be-\ning DocVQA [52], where it performs slightly lower than\nGPT-4V [2], and OCRBench [43], where it falls slightly\nbehind LLAVA-OV-7B [36].\n• Visual Perception and Multi-domain Reasoning. This\ncategory focuses on complex visual perception and mul-\ntidisciplinary reasoning tasks.\nBenchmarks for vi-\nsual perception include MMStar [13], MMBench [45],\nand MME [?\n], while reasoning benchmarks include\nMMMU [73] and SeedBench [32]. RoboBrain demon-\nstrates comparable performance to GPT-4V [2] and\nLLAVA-OV-7B [36] across multiple benchmarks.\n• Real-world Understanding and Interaction. Evaluat-\ning MLLMs [2, 15, 36, 55, 69] as general-purpose assis-\ntants in real-world settings is crucial, as these scenarios\nextend beyond controlled environments. For this, the Re-\nalworldQA [71] benchmark was utilized. Results indicate\nthat RoboBrain not only outperforms open-source models\nlike LLAVA-OV-7B [36] and InternVL2-8B [15], but also\nexceeds closed-source models such as GPT-4V [2] and\nGPT-4o [55], showcasing its extensive knowledge base\nand strong generalization capabilities.\nC.2. More Results on Robotic Benchmarks.\nTo evaluate RoboBrain’s performance on robotic capabil-\nities in real-world scenarios, we selected RoboVQA [60],\nOpenEQA [49], and the test set of ShareRobot, extracted\nfrom the proposed ShareRobot dataset, as robotic bench-\nmarks for multi-dimensional assessment, as shown in Ta-\nble 6. The chosen baselines include MLLMs such as GPT-\n4V [2], LLaVA-OV-7B [36], and Qwen2-VL-7B [69], as\nwell as robotic models like RoboMamba [42]. Detailed de-\nscriptions of the three selected robotic benchmarks and the\nanalysis of each results are provided below:\n• RoboVQA [60] provides a robotics VQA benchmark and\na long-horizon planning benchmark with an interven-\ntion mechanism on real robots. Specifically, this bench-\nmark includes 18,248 video-text pairs designed from 100\nlong-horizon episodes for various robotic VQA tasks,\nincluding planning, planning with context, planning re-\nmaining steps, future prediction, generative affordance,\npast description, success (positive/negative), and discrim-\ninative affordance (positive/negative). Similar to Robo-\nMamba [42], we utilized BLEU-1∼BLEU-4 to evaluate\nthe average performance across all tasks. According to\nthe evaluation results, our proposed model, RoboBrain,\noutperforms all baselines, achieving approximately 30%\nhigher performance than the second-best model.\n• OpenEQA [49] provides a robotics VQA benchmark\nwith over 1,600 high-quality human-generated questions\ndrawn from more than 180 real-world scenes, targeting\nthe task of Embodied Question Answering (EQA) for en-\nvironment understanding. For fairness, we evaluated all\nmodels using the prompt templates and the LLM-Score\nmetric provided by OpenEQA [49]. Based on the eval-\nuation results, our proposed model, RoboBrain, outper-\nforms GPT-4V [2] overall and achieves comparable per-\nformance to other baselines. In the future, we plan to fur-\nther enhance RoboBrain’s spatial intelligence to improve\nits generalization across scenes.\n• ShareRobot (Eval) provides a cross-scene and cross-\nembodiment robotics benchmark consisting of 2,050\nVQA pairs, drawn from 102 diverse scenes (e.g., bed-\nroom, laboratory, kitchen, office) and covering 12 differ-\n14\n\n\nTable 6. Performance comparison on RoboVQA, OpenEQA and ShareRobot Benchmarks.\nDataset\nSplit / Metric\nRoboBrain (Ours)\nGPT-4V [2]\nLLaVA-OV-7B [36]\nRoboMamba [42]\nQwen2-VL-7B [69]\nRoboVQA[60]\nBLEU1\n72.05\n32.23\n38.12\n54.9\n33.22\nBLEU2\n65.35\n26.51\n33.56\n44.2\n26.11\nBLEU3\n59.39\n24.65\n31.76\n39.5\n20.98\nBLEU4\n55.05\n23.94\n30.97\n36.3\n17.37\nOpenEQA[49]\nOBJECT-STATE-RECOGNITION\n70.4\n63.2\n72.02\n-\n72.06\nOBJECT-RECOGNITION\n49.54\n43.4\n51.73\n-\n61.91\nFUNCTIONAL-REASONING\n57.14\n57.4\n55.53\n-\n54.23\nSPATIAL-UNDERSTANDING\n46.46\n33.6\n48.98\n-\n50.39\nATTRIBUTE-RECOGNITION\n66.7\n57.2\n75.52\n-\n73.88\nWORLD-KNOWLEDGE\n53.12\n50.7\n56.46\n-\n57.3\nOBJECT-LOCALIZATION\n47.45\n42\n45.25\n-\n47.29\nShareRobot (Eval)\nDISCRIMINATIVE\n99.02\n-\n57.9\n-\n76.47\nFUTURE-PREDICTION\n72.92\n-\n13.1\n-\n8.04\nGENERATIVE\n32.43\n-\n5.44\n-\n4.63\nPAST-DESCRIPTION\n37.07\n-\n4.4\n-\n13.65\nPLANNING-REMAINING\n71.29\n-\n24.5\n-\n7.56\nPLANNING-TASK\n52.43\n-\n25\n-\n36.34\nPLANNING-WITH\n91.95\n-\n44.25\n-\n45.12\nSUCCESS\n61.7\n-\n58.5\n-\n54.63\nent robot bodies. Similar to RoboVQA [60], we cate-\ngorized various robotic VQA tasks into planning, plan-\nning with context, planning remaining steps, future pre-\ndiction, generative affordance, past description, success\n(positive/negative), and discriminative affordance (posi-\ntive/negative). Unlike RoboVQA benchmark [60], we uti-\nlized GPT-4o [55] to score the evaluation results instead\nof BLEU metrics for each task, aiming for more accu-\nrate performance assessment. Based on the results, our\nproposed model, RoboBrain, outperforms all baselines,\ndemonstrating its exceptional planning capabilities across\ndiverse scenes and embodiments.\nC.3. Effectiveness of ShareRobot\nIn this subsection, we investigate the effectiveness of the\nproposed ShareRobot dataset for training RoboBrain. We\nmaintain the ratio of robotic data to general data used in\nthe main body of the paper, approximately 4:6. Based on\nthe original data source proportions, we randomly sampled\n200K samples, which include:\n• Exp A consists of 40% robotic data, with 20% sourced\nfrom ShareRobot and 20% from other robotic sources,\nalong with 60% general data.\n• Exp B consists of 40% robotic data, excluding ShareR-\nobot, with the same other robotic data resampled as in\nExperiment A, resulting in a total of 40%. It also includes\n60% general data, which is identical to that of Exp A.\nWe conducted a complete epoch for all the experiments\nmentioned above.\nThe results are presented in Tab 7.\nAs shown in the table, the inclusion of ShareRobot data\nenhances the model’s performance compared to scenarios\nwithout ShareRobot.\nC.4. Effectiveness of Robot Data Proportion\nIn this subsection, we investigate the effectiveness of the\nratio of robotic data (including ShareRobot) to general data\nused in training RoboBrain. We maintain a constant total\ntraining dataset size of 200K while varying the sampling\nproportions of robotic and general data. The configurations\nare as follows:\n• Exp C utilizes a ratio of 3:7, comprising 30% robotic data\nand 70% general data.\n• Exp D utilizes a ratio of 4:6, comprising 40% robotic data\nand 60% general data, same to Exp A.\n• Exp E utilizes a ratio of 5:5, with 50% robotic data and\n50% general data.\n• Exp F utilizes a ratio of 6:4, featuring 60% robotic data\nand 40% general data.\n• Exp G utilizes a ratio of 7:3, containing 70% robotic data\nand 30% general data.\nWe conducted a complete epoch for all the experiments\nmentioned above. The results are presented in Tab 7. As\nshown in the table, a 4:6 ratio of robotic data represents a\ngood choice for training data, effectively balancing perfor-\nmance on both the robotic and the general benchmark.\n15\n\n\nTable 7. Experimental results for effectiveness of ShareRobot and different robot data proportion.\nExp. Name\nGeneral Data (%)\nRobotic Data (%)\nGeneral Benchmarks\nRobotic Benchmarks\nAverage\nOneVision\nShareRobot\nOthers\nType-1\nType-2\nType-3\nRoboVQA[60]\nOpenEQA[49]\nShareRobot\nEXP A\n60%\n20%\n20%\n62.44\n71.98\n70.33\n48.29\n58.74\n63.11\n62.48\nEXP B\n60%\n0%\n40%\n62.36\n71.38\n66.01\n49.20\n57.96\n27.03\n55.66\nEXP C\n70%\n15%\n15%\n62.73\n72.19\n68.10\n45.96\n56.59\n61.73\n61.22\nEXP D\n60%\n20%\n20%\n62.44\n71.98\n70.33\n48.29\n58.74\n63.11\n62.48\nEXP E\n50%\n25%\n25%\n62.28\n71.25\n66.54\n49.34\n58.76\n63.35\n61.92\nEXP F\n40%\n30%\n30%\n62.39\n71.61\n68.37\n49.22\n56.24\n64.57\n62.07\nEXP G\n30%\n35%\n35%\n62.69\n71.92\n69.54\n47.74\n55.72\n65.22\n62.14\nTable 8. Additional Experimental Results. “SFT Data (G:R)”\nindicates the ratio of training data for fine-tuning VLMs, where\n“G” represents general VQA data and “R” denotes robot data (with\nhalf being ShareRobot). The total dataset size is 1.47M.\nModel\nSFT Data(G:R)\nRoboVQA\nShareRobot\nMME\nMMMU\nLLaVA-OV-7b\n6:0\n36.29\n27.04\n2001\n49.65\n6:4\n43.63\n54.66\n1945\n48.83\n(a) Qwen2VL-7B\n6:0\n24.05\n28.17\n2313\n52.10\n6:4\n58.94\n58.86\n2295\n52.33\nOpenVLA-7B\n6:0\n4.11\n21.44\n1681\n35.07\n6:4\n54.79\n60.56\n1722\n37.25\nLLaVA1.5-Qwen\n6:0\n24.17\n26.73\n1720\n44.28\n6:4\n49.01\n43.41\n1732\n48.33\n(b)\nLLaVA1.5-LLaMA\n6:0\n21.40\n25.06\n1529\n46.40\n6:4\n49.67\n54.87\n1722\n43.41\nLLaVA1.5-Vicuna\n6:0\n26.19\n22.18\n1668\n30.09\n6:4\n50.40\n51.42\n1650\n31.51\nLLaVA1.5-Mistral\n6:0\n14.30\n21.88\n1602\n23.91\n6:4\n36.29\n57.47\n1548\n24.32\nC.5. Different Architecture and Pre-trained VLMs\nTo validate the effectiveness of different architecture and\npre-trained VLMs and training data in the stage 3 training\nsetup, we selected LLaVA-OneVision-7B [34], OpenVLA-\n7B [30], and Qwen2VL-7B [69], each representing a dis-\ntinct architecture among VLMs, and conducted SFT using\nthe same proportion of training data described in the main\ntext. As shown in Tab. 8 (a), the results demonstrated that\nincorporating ShareRobot can significant performance im-\nprovements. For unaligned VLMs like LLaVA 1.5 [38] and\nOpenVLA, we first aligned the MLP using BLIP-558k [33];\nother models were directly finetuned.\nC.6. Different LLM Backbones\nTo demonstrate the effectiveness of different LLM back-\nbones when finetuned on the ShareRobot dataset, we con-\nducted experiments using four distinct LLMs [5, 16, 26, 66].\nThese models were finetuned using the ShareRobot data,\nand the experimental results are summarized in Tab. 8 (b).\nThe findings indicate that different LLMs benefit from the\nShareRobot data.\nTable 9. Additional Evaluation Results.\nStage\nRoboVQA\nShareRobot\nMME\nMMMU\nAffordance↑\nTrajectory↓\nS1.5\n2.60\n9.81\n1406\n46.00\n0.00\n1.00\nS2-si\n28.90\n13.31\n2110\n50.76\n3.11\n1.00\nS2-ov\n31.81\n34.84\n2083\n49.95\n8.50\n1.00\nS3\n62.96\n65.05\n2084\n49.00\n7.14\n1.00\nS4-A\n62.96\n65.05\n2084\n49.00\n27.1\n-\nS4-T\n62.96\n65.05\n2084\n49.00\n-\n0.09\nC.7. Ablation Studies of Different Stages\nWe present the evaluation results for each stage in Tab. 9.\nThe results demonstrate that staged training from stage 1\nto stage 3 consistently and effectively improves the model’s\nplanning performance, while stage 4 enhances the model’s\naffordance and trajectory capabilities.\nD. More Qualitative Results\nIn this section, we provide additional visual results for plan-\nning, affordance perception, and trajectory prediction. This\nincludes the presentation of both positive and negative sam-\nples, as well as further analysis.\nD.1. Visualization on Planning\nHere, we present additional embodied planning for robotic\ntasks generated by RoboBrain, as shown in Fig. 8. In this\nfigure, we demonstrate the planning results of RoboBrain\nfor four distinct robotic manipulation tasks: ”Water plants”,\n”Put the pot in the drawer”, ”Cluster blocks of the same\ncolor into different corners”, and ”Clean the desk”, where\nthe first three are categorized as good cases, and the last one\nas a bad case. Additionally, the model provides a rationale\nand detailed explanation for each step of the planning pro-\ncess across all four cases.\nFrom the first three planning cases, it is evident that\nRoboBrain effectively utilizes environmental information\nand the states of interactive objects—captured from first-\nor third-person perspective images—to generate task plans\nfor various types of robotic manipulation tasks. Notably, in\nthe ”Cluster blocks of the same color into different corners”\ntask, RoboBrain not only analyzes the number of blocks of\neach color on the table in Steps 1 and 2 but also provides\n16\n\n\ndetailed sub-steps in Step 3, i.e., ”Move the objects to form\nclusters”.\nSpecifically, it plans the movement of blocks\nof four different colors to their designated locations: ”top\nleft corner”, ”top right corner”, ”bottom left corner”, and\n”bottom right corner”. The exceptional task generalization\ncapability of RoboBrain in planning further validates the ef-\nfectiveness of our training dataset—including the proposed\nShareRobot dataset—and the Multi-Phase training strategy.\nWe also present a bad case for RoboBrain, namely the\n”Clean the desk” task. In this case, the first-person perspec-\ntive image depicts a work desk spilled with coffee, where\nthe main objects of focus include a ”tissue box”, a ”tipped-\nover coffee cup”, and the ”spilled coffee liquid”. The errors\nin the planning results inferred by RoboBrain are summa-\nrized as follows: (1) Object recognition error. The only\navailable object for wiping the desk in the image is a ”tis-\nsue”, rather than a ”disinfectant wipe”. (2) Omission of\ncritical steps. Before wiping the desk, it is necessary to\nextract a tissue from the tissue box. However, this step is\nmissing in RoboBrain’s planning. (3) Action decision de-\nviation. In Step 2, i.e., ”Wipe down the desk with a disinfec-\ntant wipe”, the detailed description states, ”Start from one\nend of the desk and move to the other”. This implies that\nRoboBrain fails to prioritize wiping the ”spilled coffee liq-\nuid” specifically, focusing instead on cleaning ”the entire\ndesk”. The primary cause might be the similarity in color\nbetween the desk and the spilled coffee, making it difficult\nfor the model to distinguish.\nIn our extensive testing, although a small number of un-\nreasonable bad cases like the one described above were ob-\nserved, RoboBrain demonstrated robust planning capabil-\nities in the vast majority of cases. This provides a solid\nfoundation for executing long-horizon manipulation tasks.\nD.2. Visualization on Affordance\nHere, we present the visualizations of RoboBrain’s percep-\ntion of affordance areas, as shown in Fig.9. The text below\neach subfigure indicates the task instructions, while the red\nbounding boxes represent the affordance areas predicted by\nthe RoboBrain model. The visualizations in the first three\nrows demonstrate that our RoboBrain model can effectively\nprovide reasonable affordance areas based on human in-\nstructions and visual information. For example, given the\ninstruction “drink with the bottle”, RoboBrain can deter-\nmine that the bottle cap is in a closed state, thus provid-\ning affordance information for the cap area. This highlights\nRoboBrain’s strong understanding of abstract instructions.\nWe also present several failure cases, as illustrated in the\nfourth row of Fig.9. These include misidentified objects,\ninterference from other objects in the scene, and instances\nwhere no objects were recognized. These issues may stem\nfrom the model’s limited ability to perceive and localize in\nnoisy environments.\nD.3. Visualization on Trajectory\nHere, we present additional visualizations generated by\nRoboBrain using start points, as shown in Fig.10. In this fig-\nure, the red-to-purple gradient curves represent the ground\ntruth, while the green-to-blue gradient curves indicate the\npredicted trajectories. For clarity, waypoints are omitted.\nThe first three rows demonstrate that, regardless of the com-\nplexity of the end-effector trajectory, RoboBrain accurately\npredicts 2D trajectories based on visual observations and\ntask instructions. These predictions closely align with the\nstructure of the ground truth and remain executable.\nAdditionally, RoboBrain’s predictions often capture the\nessential features of the trajectories, leading to smoother\nand potentially more efficient paths compared to the ground\ntruth. This improvement may stem from the inherent vari-\nability in the robot’s actual trajectories, which can include\nredundant waypoints under similar manipulation scenarios.\nBy learning from a large, embodied dataset and utilizing the\nreasoning capabilities of large language models, RoboBrain\nis able to infer effective and optimized execution paths.\nThe visualizations in the third row further suggest that\nRoboBrain avoids overfitting; it generalizes well across dif-\nferent scenarios, producing trajectories that are both exe-\ncutable and reasonable.\nWe also present several failure cases, as shown in the\nfourth row of Fig. 10. These include the robot’s end-effector\nfailing to accurately locate the cup, neglecting the articu-\nlated nature of the fridge door while opening it, and not\naccounting for the deformable properties of clothing during\nfolding. These examples highlight the need for improved\nspatial perception, as well as the incorporation of object-\nspecific physical constraints and world knowledge to gener-\nate more feasible and realistic trajectories.\nE. Details of ShareRobot Dataset\nIn the previous section, we introduced the process of col-\nlecting and annotating our ShareRobot dataset. Here, we\nwill provide detailed prompts for data labeling and display\nexamples.\nE.1. Prompts\nThe prompts used for data planning labeling are shown in\nFig.11.\nE.2. High-level Descriptions Examples\nIn our ShareRobot dataset, there are 10,290 long-horizon\nhigh-level descriptions. We provide the 50 most frequently\noccurring ones below.\n• Closing a drawer\n• Opening a drawer\n• Opening a cabinet door\n• Dragging a strainer across a table\n17\n\n\n• Picking up a bowl\n• Inserting a three-pronged object into its matching slot\n• Inserting a double-square object into its matching slot\n• Opening a door\n• Closing a cabinet door\n• Inserting a star-shaped object into its corresponding slot\n• Opening a laptop\n• Inserting an oval object into its corresponding slot\n• Picking up a ketchup bottle from a table\n• Moving a banana from a plate to a table\n• Closing a door\n• Switching a light switch\n• Inserting an arch-shaped object into its corresponding slot\n• Inserting a square-circle object into its matching slot\n• Dragging a strainer backwards\n• Dragging a mug from left to right\n• Dragging a mug forward\n• Picking up a red object from a table\n• Placing a ketchup bottle onto a plate\n• Placing a bowl inside an oven\n• Inserting a hexagonal object into its corresponding slot\n• Closing a microwave door\n• Moving a banana from a table to a plate\n• Turning on a toaster\n• Opening a microwave\n• Closing an oven door\n• Making tea\n• Dragging a strainer forward\n• Placing a bowl into an oven\n• Picking up a banana and placing it in a mug\n• Inserting an arch-shaped object into its matching slot\n• Closing a tea container\n• Inserting a green object into a designated slot\n• Picking up a banana and placing it in a strainer\n• Moving a cloth to the left side of a table\n• Dragging a mug backwards\n• Placing a bottle into a pot\n• Dragging a strainer forwards\n• Inserting a rectangular prism into its matching slot\n• Opening a refrigerator\n• Opening a tea container\n• Opening a double door\n• Inserting a cylinder into a matching hole\n• Picking up a piece of toast\n• Dragging a mug from left to right on a table\n• Closing a refrigerator door\nE.3. Low-level Instructions Examples\nOur ShareRobot dataset contains 28,181 low-level instruc-\ntions. The top 50 frequency occurrences are displayed be-\nlow.\n• Grasp the ketchup bottle\n• Reach for the ketchup bottle\n• Grasp the banana\n• Lift the ketchup bottle\n• Lift the banana\n• Reach for the strainer\n• Reach for the banana\n• Reach for the mug\n• Grasp the mug\n• Lift the pot\n• Lift the bowl\n• Pull the drawer open\n• Reach for the bowl\n• Reach for the pot\n• Grasp the strainer\n• Reach for the drawer handle\n• Grasp the handle\n• Lift the spoon\n• Grasp the bowl\n• Reach for the spoon\n• Place the ketchup bottle on the table\n• Release the banana\n• Reach the drawer\n• Place the banana on the table\n• Lift the mug\n• Reach the cabinet door\n• Grasp the pot\n• Grasp the strainer\n• Grasp the drawer handle\n• Release the mug\n• Grasp the pot\n• Grasp the spoon\n• Place the mug down\n• Move the banana towards the table\n• Grasp the bowl\n• Pull the drawer closed\n• Move towards the bowl\n• Reach for the cloth\n• Release the pot\n• Grasp the bottle\n• Lift the cloth\n• Lift the red object\n• Grasp the banana\n• Lift the butter\n• Reach for the banana on the table\n• Place the ketchup bottle on the plate\n• Grasp the drawer handle\n• Move the ketchup bottle towards the plate\n• Move towards the red object\n• Place the banana on the plate\n18\n\n\nFigure 8. Additional embodied planning of RoboBrain. (a)∼(c) show some good cases of RoboBrain’s embodied planning, while (d)\nshows its bad case. More detailed analysis can be found in Sec.D.1.\n19\n\n\nhold the book\nhold the cup\ndrink_with the bottle\nhold the hammer\nopen the microwave\nstick the fork\nhold the tennis_racket\ndrink_with the cup\ncatch the soccer_ball\npick the knife\nwash the wine glass\nwash the pot\nFigure 9. Additional visualizations of diverse affordance areas. The text below each subfigure indicates the task instructions, while the\nred bounding boxes represent the affordance areas predicted by the RoboBrain model. The visualizations in the first three rows demonstrate\nthat our RoboBrain model effectively identifies reasonable affordance areas based on human instructions and visual information. The fourth\nrow presents several failure cases, which may stem from the model’s lack of ability to perceive and localize in noisy environments. This\nlimitation could be attributed to the absence of such scenarios in the training data used during Stage 4. The complete prompt provided\nto RoboBrain is: ”You are a Franka robot using joint control. The task is $TASK. Please predict all possible affordance areas of the end\neffector.” Here, $TASK represents specific task instructions, such as ”drink with the cup.”\n20\n\n\nplace green cube on table\nplace green rice chip bag into top \ndrawer\nopen bottom drawer\nmake a cup of coffee with keurig \nmachine\nPick up a white plate, and then \nplace it on the red plate\nplace green rice chip bag into \ntop drawer\npick up the blue cup and put it \ninto the brown cup\nmake a piece of toast with \nthe oven\npick sponge from middle drawer \nand place on counter\nPick up the object on the table \nand place it in the cup\n opening the fridge\nfolding a cloth\nFigure 10. Additional visualizations of diverse 2D trajectories. The red-to-purple gradient curves represent the ground truth, while the\ngreen-to-blue gradient curves indicate the predicted trajectories. The visualizations in the first two rows demonstrate that our RoboBrain\nmodel effectively generates end-effector manipulation curves based on the robot’s observations and task instructions. The third row shows\nthat RoboBrain is not merely fitting trajectories but also exhibits the ability to generate more reasonable and feasible curves. The fourth row\npresents some failure cases, which stem from a lack of spatial awareness and world knowledge. These limitations result in an inability to\naccurately localize the objects involved in interactions, account for physical constraints, and adapt to the variability of deformable objects.\n21\n\n\nFigure 11. Additonal visualizations of prompts for Gemini. The prompts encapsulate the task description for robotic arm action\nrecognition, the components of the target, and the desired response format. Additionally, an example is included to assist Gemini in\nunderstanding the specific task.\n22\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21257v1.pdf",
    "total_pages": 22,
    "title": "RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete",
    "authors": [
      "Yuheng Ji",
      "Huajie Tan",
      "Jiayu Shi",
      "Xiaoshuai Hao",
      "Yuan Zhang",
      "Hengyuan Zhang",
      "Pengwei Wang",
      "Mengdi Zhao",
      "Yao Mu",
      "Pengju An",
      "Xinda Xue",
      "Qinghang Su",
      "Huaihai Lyu",
      "Xiaolong Zheng",
      "Jiaming Liu",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have shown\nremarkable capabilities across various multimodal contexts. However, their\napplication in robotic scenarios, particularly for long-horizon manipulation\ntasks, reveals significant limitations. These limitations arise from the\ncurrent MLLMs lacking three essential robotic brain capabilities: Planning\nCapability, which involves decomposing complex manipulation instructions into\nmanageable sub-tasks; Affordance Perception, the ability to recognize and\ninterpret the affordances of interactive objects; and Trajectory Prediction,\nthe foresight to anticipate the complete manipulation trajectory necessary for\nsuccessful execution. To enhance the robotic brain's core capabilities from\nabstract to concrete, we introduce ShareRobot, a high-quality heterogeneous\ndataset that labels multi-dimensional information such as task planning, object\naffordance, and end-effector trajectory. ShareRobot's diversity and accuracy\nhave been meticulously refined by three human annotators. Building on this\ndataset, we developed RoboBrain, an MLLM-based model that combines robotic and\ngeneral multi-modal data, utilizes a multi-stage training strategy, and\nincorporates long videos and high-resolution images to improve its robotic\nmanipulation capabilities. Extensive experiments demonstrate that RoboBrain\nachieves state-of-the-art performance across various robotic tasks,\nhighlighting its potential to advance robotic brain capabilities.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}