{
  "id": "arxiv_2502.21267v1",
  "text": "ReaLJam: Real-Time Human-AI Music Jamming with\nReinforcement Learning-Tuned Transformers\nAlexander Scarlatos∗\najscarlatos@cs.umass.edu\nUniversity of Massachusetts Amherst\nUnited States\nYusong Wu∗\nyusong.wu@umontreal.ca\nUniversity of Montreal, Mila\nUnited States\nIan Simon\niansimon@google.com\nGoogle DeepMind\nUnited States\nAdam Roberts\nadarob@google.com\nGoogle DeepMind\nUnited States\nTim Cooijmans\ncooijmans.tim@gmail.com\nUniversity of Montreal, Mila\nUnited States\nNatasha Jaques\nnatashajaques@google.com\nGoogle DeepMind\nUniversity of Washington\nUnited States\nCassie Tarakajian\nctarakajian@google.com\nGoogle DeepMind\nUnited States\nCheng-Zhi Anna Huang\nannahuang@google.com\nGoogle DeepMind\nUnited States\nFigure 1: The live jamming task. Based on the session history, the agent anticipates the user’s melody and plans what chords to\nplay. Similarly, the user plans what melody to play, but anticipates chords by viewing the agent’s plan displayed onscreen. The\nuser and agent play their respective parts, synchronized by the client, to create a live jam.\nAbstract\nRecent advances in generative artificial intelligence (AI) have cre-\nated models capable of high-quality musical content generation.\nHowever, little consideration is given to how to use these models\nfor real-time or cooperative jamming musical applications because\nof crucial required features: low latency, the ability to communicate\nplanned actions, and the ability to adapt to user input in real-time.\nTo support these needs, we introduce ReaLJam, an interface and\nprotocol for live musical jamming sessions between a human and a\nTransformer-based AI agent trained with reinforcement learning.\n∗Work done during internships at Google DeepMind.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI EA ’25, Yokohama, Japan\n© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1395-8/2025/04\nhttps://doi.org/10.1145/3706599.3720227\nWe enable real-time interactions using the concept of anticipation,\nwhere the agent continually predicts how the performance will\nunfold and visually conveys its plan to the user. We conduct a\nuser study where experienced musicians jam in real-time with the\nagent through ReaLJam. Our results demonstrate that ReaLJam en-\nables enjoyable and musically interesting sessions, and we uncover\nimportant takeaways for future work.\nCCS Concepts\n• Applied computing →Sound and music computing; • Human-\ncentered computing →Human computer interaction (HCI).\nKeywords\nAnticipation, Human-AI collaboration, Jamming, Music generation,\nSynchronization\nACM Reference Format:\nAlexander Scarlatos, Yusong Wu, Ian Simon, Adam Roberts, Tim Cooi-\njmans, Natasha Jaques, Cassie Tarakajian, and Cheng-Zhi Anna Huang.\n2025. ReaLJam: Real-Time Human-AI Music Jamming with Reinforcement\narXiv:2502.21267v1  [cs.HC]  28 Feb 2025\n\n\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nScarlatos et al.\nLearning-Tuned Transformers. In Extended Abstracts of the CHI Conference\non Human Factors in Computing Systems (CHI EA ’25), April 26-May 1, 2025,\nYokohama, Japan. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3706599.3720227\n1\nIntroduction\nWhile there have been many recent advances in AI-generated mu-\nsic, how to leverage these advances for live generation, particularly\ncooperatively with humans, remains understudied. Cooperative\nhuman-AI music creation in a live setting has many potential appli-\ncations: it can assist in the spontaneous creation of ideas, enhance\nimprovised performances, and provide practice partners or musi-\ncal tutors to people who may not have access to them otherwise.\nMany of these applications can manifest as a live “jam”, where a\nhuman and AI agent play respective musical parts live, building off\neach others’ musical contributions to form a cohesive improvised\nperformance.\nHowever, utilizing AI for live jamming comes with many chal-\nlenges: First, in order for the agent to play in a way that is cohesive\nwith the human, the agent must anticipate what the human will\nplay in the short-term future, planning its own performance based\non this prediction. This act of anticipation happens naturally for hu-\nmans [11], but has also been leveraged in non-live generative music\nmodels [28] to improve the quality of accompaniment. Second, the\nhuman must similarly anticipate the agent’s future actions. When\nreading music or playing music games like Guitar Hero [8], humans\nlook ahead to prepare for what to play next. During live perfor-\nmances, humans will signal future plans to each other through\nintricate gestures via their head, body, and hands [15]. Therefore,\nan effective jamming agent should have a similar way of conveying\nits plan to the human to help them anticipate upcoming actions.\nFinally, the interface must synchronize the user and agent notes\nalong the time dimension so they are played at their intended times\nwith no discernible delay. Prior works either accept a delay in sys-\ntem responses (in non-jamming settings) [7] or use small models,\nsuch as LSTMs, with unnoticeable delay [2]. However, more pow-\nerful Transformer-based music models [10] have slower inference\nand run on specialized compute hardware, often requiring them to\nrun remotely on a server and adding further delay to responses.\nWhile there are many prior works on real-time human-AI col-\nlaboration in music [2–4, 7, 17–19, 23, 26, 27], only one studies a\nreal-time jamming system [2], but does not fully address the above\nchallenges (see a detailed related work in Appendix B). We note\nthat anticipation and synchronization are relevant in other real-\ntime tasks, such as collaborative drawing [33] or speech-to-speech\nconversation [29], necessitating solutions for these increasingly\nimportant challenges.\n1.1\nContributions\nSystem In this work, we introduce ReaLJam (Real-Time Reinforce-\nment Learning Jamming), a holistic system that enables live human-\nAI jamming via a web interface and techniques for interacting with\nAI music agents in real-time. We specifically study chord accompa-\nniment, where the user plays a melody and a Transformer-based\nagent plays along with accompanying chords. To the best of our\nknowledge, our system is the first to achieve live jamming with large\nTransformer models. We conduct a user study where experienced\nmusicians play multiple performances with ReaLJam, validating the\nsystem’s effectiveness and enjoyment for users while uncovering\nmeaningful findings regarding music modeling and the importance\nof giving users fine-grained control over interface settings. See\nvideo and audio examples of users playing live with ReaLJam on\nour demo page: https://storage.googleapis.com/genjam/index.html.\nAnticipation and Synchronization Our technical contributions\naddress the challenges of anticipation, i.e., both the user and agent\nbeing able to reasonably predict how the other will act, and syn-\nchronization, i.e., ensuring that user and agent notes are played\nat the intended times without delay. We propose multiple technical\nand design solutions to address these challenges, such as using the\nagent to construct a near-term musical plan and displaying upcom-\ning chords to the user via a waterfall display. Figure 1 illustrates\nthe collaborative task between the user and agent, including the\nroles of anticipation and synchronization.\n2\nReaLJam\nIn this section, we detail the various components of ReaLJam and\nhow they come together to enable live human-AI jamming. We\naddress the underlying challenges of anticipation to improve\nuser and agent predictions, and synchronization of the user and\nagent parts to enable a fluid jamming experience. We detail our\nsolutions to these challenges through the design of the user interface\n(Section 2.1), the use of an agent trained for online chord generation\nwith reinforcement learning (Section 2.2), and the communication\nprotocol for client-server synchronization (Section 2.3). We show a\nscreenshot of a user playing with ReaLJam in Figure 2.\n2.1\nUser Interface\nIn the ReaLJam interface, a user is presented with a piano at the\nbottom of the screen, a control bar at the top, and a grid region in\nthe middle where upcoming chords will appear. Users play notes\nwith a MIDI device or computer keyboard and see them light up\non the piano. This design is similar to the popular piano learning\napp Synthesia [16].\nAgent Interaction After the user clicks the button to start a live\nsession, their first note initiates the silence period, where an agent\nwill first wait and “listen” to user’s notes. This period allows the\nagent to collect sufficient information to make informed predictions\nby the time it starts generating. After this period, the agent will\nbegin to produce chords. Inspired by waterfall displays in music\ngames like Synthesia [16] and Guitar Hero [8], the user will see the\nupcoming chords falling down the grid region. When the chords\nreach the piano, they are played and the associated keys light up in\nblue. The number of beats into the future which the user can see\nupcoming chords is called the lookahead. Chords in the immediate\nfuture up to a commit time (before the end of the lookahead) are\nsaid to be committed; that is, the agent can no longer change those\npredictions. Beyond the commit time, the agent is free to update its\npredictions. Uncommitted chords are rendered semi-transparently,\nand may change until they are committed. The commitment ensures\nthat the user is given enough time to react to the incoming chords,\nand the user can adjust the commit time for a tradeoff between\n\n\nReaLJam: Real-Time Human-AI Music Jamming with RL-Tuned Transformers\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nFigure 2: The ReaLJam interface. Users see a control bar at the top and a piano at the bottom. Keys for sustained notes\nare highlighted for the human (orange) and agent (blue). Anticipated chords fall from the control bar to the piano, with\nuncommitted chords rendered semi-transparently. See a video of a user playing live with ReaLJam on our demo page: https:\n//storage.googleapis.com/genjam/index.html\nknowing chords ahead of time and having the chords be more up\nto date with the melody.\nUser Control Following prior work showing the importance of\nuser control in music generation interfaces [17], we provide fine-\ngrained settings for various aspects of our system. Specifically, users\ncan configure the number of beats for the silence period, lookahead,\nand commit time, whether to show incoming chords, the agent\nmodel and sampling temperature, a metronome with tempo and\ntime signature, and digital instruments for the melody and chord\nparts.\n2.2\nMusic Generation Agent\nReaLJam’s chord generation agent is based on ReaLchords [32],\nrecent work which was the first to enable real-time online chord ac-\ncompaniment generation with a Transformer model by fine-tuning\nwith reinforcement learning (RL). The agent conditions on an input\nmelody and previously generated chords to predict what chord\nshould be played next. Inputs and outputs are discretized into time\nframes of 1/16th notes, with tokens representing melody notes or\nchords. ReaLchords includes online and offline models, where the\nonline model is used for real-time generation, seeing only what has\nbeen played so far, while the offline model sees the past and future\nof the melody. Both models are initially pre-trained with a dataset\nfrom Hooktheory [1] containing ∼30,000 annotated pop song snip-\npets, after which the online model undergoes further fine-tuning\nthrough RL. This RL training involves generating chords for fixed\nmelodies and constructing a reward based on the offline model\nand other reward models. We experiment with two RL variants of\nReaLchords: RL-S, which evaluates rewards at sequence endpoints,\nand RL-M, which evaluates rewards throughout the sequence to\nensure local coherence, with the pre-trained online model named\nOnline.\n2.3\nCommunication Protocol\nIn order for users to perceive that the agent is immediately reacting\nto their input, we introduce a protocol for real-time synchroniza-\ntion between the web client running the interface (Section 2.3.1)\nand a server-hosted agent (Section 2.3.2). We illustrate an example\nsynchronization timeline in Figure 3.\n2.3.1\nClient. ReaLJam’s interface runs in a web client, where con-\ntinuous requests are made to a stateless server to get up-to-date\nchord predictions for the near-term future. The client performs the\nfollowing in a persistent loop:\nRequest Each request, the client sends the entire jam session\nhistory (melody and chords) plus chords currently in the lookahead,\nthe target frame to start generating in, and all relevant settings such\nas the number of frames for lookahead, commit, etc. The chords in\nthe request are simply tokens copied from the previous response.\nResponse After receiving the server’s response, the client sched-\nules the returned chords in the lookahead. All chords previously\nscheduled to play after the current frame are canceled and replaced\nby the chords in the response. The client uses the chords’ associated\nframes in the response to schedule them at the correct times based\non the beats per minute and the session start time, achieving time\nsynchronization. In cases where the response takes longer than one\nframe, scheduled chords play while waiting, ensuring no client-side\ndelay or interruption as long as the number of lookahead frames is\nhigher than the number of frames spent waiting. In practice, using\na single device for inference, a majority of responses return within\n100 milliseconds, fast enough for single-frame round trips at 150\nbeats per minute. After the response is processed, the client makes\na new request for the next frame, continuing until the user ends\nthe session.\n2.3.2\nServer. The server’s job is to run the agent to generate chords\nfor the lookahead given the current session history. For each request\nit receives, the server performs the following steps in order:\nTokenize Melody The server converts the melody in the request\nto a monophony so that at most one note is registered in each frame.\nWithin each frame, we simply take the first note played and ignore\nany others.\nWarm-Start Generation To improve chord quality early in the\nsession, we warm-start online generation with offline-generated\n\n\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nScarlatos et al.\nFigure 3: Synchronization in ReaLJam is robust to server latency. We show a request/response loop with a 2 beat round-trip\nlatency for demonstration, with a 4 beat lookahead and 2 beat commit time. The client plays cached future chords (green and\nblue) while waiting for responses. Each request, the client sends the server the played melody (black), played chords (black),\nand committed chords (green). The agent predicts chords several frames into the future by anticipating user input (gray) with\ncommitted chords unchanged. Upon receiving the response, the client schedules new chords (blue) after the commit period and\nsends a new request.\nchords in the silence period. Once per session, 4 frames before the\nsilence period ends, we use the offline model to generate chords to\naccompany the melody so far. We do not use the online model for\nthis step since it cannot see the full melody in the silence period,\nwhereas the offline model can. The offline-generated chords are\nreturned to the client to be echoed back for future generations, but\nare not played by the client. This design is motivated by previous\nwork that conditions online models with ground-truth data [13, 32].\nCommit Period For the frames in the commit period, we fix the\npreviously predicted chords (sent in the request) but anticipate\nnotes in the user melody that will play over these chords. We do\nthis by having ReaLchords generate chord and melody tokens up to\nthe last committed frame of the lookahead (identified in the request),\nconditioned on the last 512 tokens of the jam session history. We\nthen replace these generated chords with the previously committed\nchords.\nAdaptive Period Finally, we generate chords for the adaptive\nperiod, i.e., the frames between the end of the commit period and\nthe end of the lookahead. Conditioned on the jam session history,\nthe committed chords, and the melody anticipated in the previous\nstep, we use ReaLchords to generate chord and melody tokens up to\nthe last frame of the lookahead. By doing so, the agent concurrently\nanticipates user actions while constructing a plan for its own actions.\nAll chords in this new lookahead are then converted from symbolic\nform to pitches and returned to the client.\n3\nUser Study\nTo investigate ReaLJam’s viability for live human-AI jamming and\nthe quality of its user experience, we conducted a user study where\n6 participants, each an experienced musician, play multiple live per-\nformances with ReaLJam. While this is a small participant pool, we\nconduct extensive interviews with each, collecting a high amount\nof quality data for in-depth analysis. We find that our study is\nsufficient to validate the effectiveness of our system, identify its\nstrengths and weaknesses, and discover important directions for\nfuture work. We now detail our study design.\nParticipants For recruitment, we posted calls for volunteer par-\nticipation in music interest groups within our institution, where\nthe only prerequisites for participants were to have some piano\nexperience and be comfortable improvising. Candidates applied\nvia email and were selected on a first-come-first-serve basis. We\nconducted a trial version of the study with 2 participants, after\nwhich we refined the study and conducted a final version with 6\nparticipants.\nStudy Overview For each participant, we carry out a 1 hour session\nover a video call, during which participants perform with ReaLJam\nand answer questions. After we explain the interface, participants\npractice with ReaLJam to get a feel for the interaction and set\nthe tempo to their preference. When participants are ready, they\nplay a 45-second baseline performance with ReaLJam. Afterwards,\nthey play 8 45-second performances, where in each one a single\ninterface setting differs from the baseline. Finally, participants play\none last 45-second performance using any settings they like. After\nthe video call, participants answer a survey to provide aggregated\nquantitative and qualitative data.\nSetting Experiments We examine the impact of various settings\nfor the User Interface: whether incoming chords are shown (I.C.)\nand whether the metronome is on (Met.), settings for the Agent: the\nunderlying Model used (between RL-S, RL-M, and Online) and the\nsampling temperature (Temp.) (between 1 and 0), and settings for\nSynchronization: the number of beats to commit (Com.) (between\n4, 2, and 0) and the initial beats of silence (I.S.) (between 8 and 0).\nThe lookahead is always 4 beats. We always use a commit time\nof 0 for the Online model since it is highly sporadic otherwise,\nlikely due to a lack of ability to plan ahead induced by RL training.\nThe settings for the baseline performance are as follows: incoming\nchords are shown, the metronome is on, the agent is RL-S, the\ntemperature is 1, the commit is 4 beats, and the initial silence is 8\nbeats. For each subsequent performance, we modify exactly one\nsetting from the baseline and fix the rest. We randomize the order of\nexperiments for each participant to avoid bias, with the constraints\nthat commit must be tested before models and the two commit tests\nwill occur consecutively as will the two model tests.\nIntra-Study Metrics For each performance following the baseline,\nwe ask participants 4 questions regarding how their experience\nchanged relative to the baseline: “Which performance was more\n\n\nReaLJam: Real-Time Human-AI Music Jamming with RL-Tuned Transformers\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nmusically interesting?” (Int.), “In which performance did the model\nadapt to your melody better?” (Adpt.), “In which performance did\nyou feel more in control?” (Ctrl.), and “Which performance did\nyou enjoy playing more?” (Enj.). For each question, participants\nidentify which performance was better or can respond with “No\nDifference.” We chose to ask for comparisons rather than absolute\nratings since the former have higher validity and are more accurate\n(e.g. [6, 14, 30]).\nPost-Study Metrics After their sessions, we sent participants a\nsurvey with questions on their musical background and impres-\nsions of their experience with ReaLJam. We asked the users the\nfollowing quantitative questions, all using a 1-5 Likert scale: For\nuser background, we asked “How experienced are you at the pi-\nano?” (Exp. Piano) and “How experienced are you at improvising\nmusic with other people?” (Exp. Improv.). Having users reflect\non when the system was working at its best, we asked them “How\nmusically interesting/exciting (in terms of harmonization, rhythm,\netc.) was the session?” (Int.), “How well did the chord model adapt\nto your melody?” (Adpt.), “How much control/agency did you feel\nduring the session?” (Ctrl.), and “How much did you enjoy the\njamming experience?” (Enj.). Regarding their experience overall,\nwe asked users “How often would you use this software outside\nof the experimental setting?” (Util.). We also asked users the fol-\nlowing open-ended questions regarding their experience overall:\n“What were some things that you enjoyed most about the jamming\nexperience?”, “What were some things that you disliked about the\nexperience?”, “In what ways was jamming with the AI similar to\nor different than jamming with a real human?”, “Do you have any\nsuggestions for improving the jamming experience?”, and “Do you\nhave any other thoughts or comments?”.\n4\nResults and Discussion\nWe now detail our main findings by analyzing data from the intra-\nstudy questions and the post-study survey, behaviors from partici-\npants during the sessions, and musical artifacts collected from the\nperformances. We find that 1) ReaLJam successfully enables users\nto jam with an AI in an enjoyable and meaningful way, 2) RL agents\ngreatly improve musical quality but lack some desirable behaviors,\nand 3) user experience is highly impacted by interface settings with\nregard to personal preference. We summarize the quantitative re-\nsults in Table 1 with a more detailed breakdown Appendix C. Our\nfindings are further supported by audio examples in our demo page:\nhttps://storage.googleapis.com/genjam/index.html.\n4.1\nReaLJam successfully enables human-AI\njamming\nA real-time system that works Users were very positive about\ntheir experiences live jamming with an AI for the first time. One\nuser said “It was nice to be able to jam without scheduling with\nanother person” (P4). Others noted the “low latency” (P1), “ease of\nsetup/integration” (P1), and “low overhead to just jamming away”\n(P6). This sentiment shows that our client-server synchronization\nsuccessfully supports a seamless and low-latency jamming experi-\nence.\nExceptional musical moments Another positive factor for users\nwas having moments when the agent surprised them with especially\ncohesive or interesting chords. When asked what they liked about\nthe experience, users noted “surprise! Moments of unexpected joy”\n(P1), “It kept me on my toes [with respect to] which chord comes\nnext” (P3), and “There were several times where the model did\npredict the chord I had in my mind - those moments were really\nthe strong positive moments in the whole experience” (P5). These\nmoments contributed to the Interesting and Enjoyable scores of\n3.5 and 4.3, respectively. These results are especially encouraging\nconsidering that users reported high levels of experience in piano\nand improvisation, indicating that the user experience is approved\nby a knowledgeable audience.\nFuture use Users also expressed that they would like to use a tool\nsimilar to ReaLJam in their workflows. One user said, “This was\nfun, would be great to have it as a MIDI plugin or DAW tool” (P2),\nand another, “I could see this being more useful as a compositional\ntool in ‘here’s a melody, and I have chords, but I’m not going to tell\nyou and want to hear your chords you think fit the entire melody’.\nThat would be pretty useful in my workflow” (P6). Users also gave\nan average Utility score of 3.2, indicating ReaLJam could be useful\nto musicians in real-world settings.\n4.2\nReinforcement learning is necessary\nRL agents significantly outperform pre-trained models When\ncomparing the ReaLchords-S (RL) and Online (pre-trained) models,\nusers preferred ReaLchords-S in all 4 metrics by a large margin.\nThis is mainly because the Online model frequently entered failure\nmodes; it would generate mostly coherent chords for some time, but\nthen begin generating sporadic, unpredictable chords and extended\nperiods of silence. This can be explained by the Online model not\nhandling out-of-distribution data well and not being able to recover\nfrom random variations in user input or its own generations, similar\nto findings in prior work [32]. We also observe that ReaLchords-M\nis generally preferred by users, outperforming ReaLchords-S on all\n4 categories by a small margin and being chosen by half of users\nin their last session. This indicates that not only is RL important,\nbut the choice of reward function can have a meaningful impact on\nusers.\nBest agents still have shortcomings While the RL agents gen-\nerally enabled an enjoyable musical experience, users expressed\ntwo main issues with the chord predictions: they did not always\nmatch what users were expecting, and the agent had little sense of\nmusical structure. Users noted that chords sometimes felt “random”\n(P1, P2), and “It was more fun playing with a model that seemed\nto ‘get’ what I was trying to do, and less fun when I felt like it was\njust guessing” (P5). Additionally, users noted “little (if any) regard\nto song structure” (P3), the agent struggling to “pick up on the ‘4\nbar pattern”’ (P4), and there “didn’t seem to be any inherent desire\nto resolve on bar 8” (P6). These issues could likely be addressed\nwith new reward functions, such as structure-based [12, 24] or\ndiversity-based [22] rewards.\nTechniques to improve chord quality By observing musical ar-\ntifacts created during the user study, we find that the silence period\nand longer commit periods can improve chord quality. Without\n\n\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nScarlatos et al.\nOverall Scores\nExp. Piano\nExp. Improv.\nInt.\nAdpt.\nCtrl.\nEnj.\nUtil.\n3.3\n3.8\n3.5\n2.7\n2.7\n4.3\n3.2\nAverage System Setting Comparison Scores\nI.C. On\nMet. On\nRL-M\nOnline\nTemp. 1\nCom. 2\nCom. 0\nI.S. 8\nAvg. Preference\n-0.21\n0.17\n0.29\n-0.58\n-0.08\n0.08\n0.21\n0.25\nChosen in Last\n5/6\n3/6\n3/6\n0/6\n5/6\n1/6\n3/6\n5/6\nTable 1: Top: Users’ prior experience and average overall scores for the system, each in the range [1, 5]. Users have high levels of\nprior experience and enjoyed using ReaLJam. Bottom: Users’ intra-study experiment scores for each setting, averaged over\nusers and all 4 preference questions. A user choosing the indicated setting gives a score of 1, choosing “No Difference” gives a\nscore of 0, and choosing the alternative gives a score of -1. The Online model is greatly dispreferred to the RL models, whereas\nother settings are mostly balanced across users and measures. We also show the portion of times each setting was chosen by\nusers for their last performance, where most users chose to see incoming chords and include the silence period, two of our key\ntechnical contributions.\nthe silence period, the initial first chord is almost always dissonant\nwith the melody, whereas allowing the agent to listen for 8 beats\nalmost always makes the first chord harmonic with the melody.\nAdditionally, when the number of commit beats is 0, there are often\nrapid unnatural chord changes due to the agent changing its plan.\nUsing 4 beats of commit greatly improves the plan’s stability.\n4.3\nSettings greatly impact user experience\nPreferred settings are highly user specific Perhaps surprisingly,\nwe found that users cared greatly about the particular interface set-\ntings yet also disagreed with each other about which settings were\nbest. These effects were strongest with showing incoming chords,\nthe commit beats, and the metronome. For incoming chords, P1 in-\ndicated they enjoyed seeing incoming chords because it reminded\nthem of getting visual feedback from humans and P6 struggled\nto synchronize their melody with the chords when they weren’t\nshown, yet P5 generally didn’t even look at the screen when the\nchords were shown. This discrepancy indicates that while assisting\nwith anticipation can be helpful for some users, others may be\nable to sufficiently predict agent actions on their own. For commit,\nsome users felt setting the value to 0 allowed for more accurate\npredictions by being able to generate chords “just in time” (P2),\nwhile another felt that setting it to 0 made it “step on your toes” too\nmuch by changing chords unpredictably (P1). For the metronome,\nsome users were unable to time their melodies well without it (P6),\nwhile another felt it made them feel too “boxed in” (P5). Overall,\nthese results make it clear that it is critical to give users a high\nlevel of control over interface settings, due to their diverse personal\npreferences and musical styles, consistent with findings in prior\nwork [17].\nUser preference transcends objective measures We find that\nthe settings users preferred did not always match the objective rat-\nings given, indicating that user preference is complex and further\nmotivating high levels of user control in musical interfaces. For in-\nstance, users generally agreed that not seeing the incoming chords\nmade the session more musically interesting, likely because of the\nsurprisal factor of not knowing what would come next. However,\n5/6 users still chose to see incoming chords in the last performance,\nindicating a tradeoff between musical interest and ease of anticipat-\ning the agent’s chords. Similarly, users thought that a temperature\nof 0 made the agent more adaptive due to less random predictions,\nbut 5/6 ultimately set temperature to 1 to increase chord diversity.\n5\nConclusion\nIn this paper, we introduce ReaLJam, a system for real-time mu-\nsical jamming between users and AI agents. We propose utilizing\nanticipation to improve predictions for both the user and agent,\nwhich involves making a future plan with the agent and convey-\ning the plan to the user via a waterfall display. We also propose a\ncommunication protocol to synchronize the user and agent musical\nparts in real-time by further utilizing the agent’s future plan. We\nconduct a user study where musicians jam with ReaLJam and reflect\non their experiences, and find that ReaLJam successfully enables\nlive jamming with a high level of enjoyment, musical interest and\nutility.\nThere are many avenues for future work. First, our findings from\nthe user study can be used to inform future generative online mu-\nsic model designs. In particular, rewards should be developed to\nencourage high-level musical structure and adapt to a diversity\nof melodic styles. Second, more high-level control over the agent\nshould be given to users, such as specifying the musical genre or\ndesired sparsity of model outputs. Third, additional need finding\nuser studies should be carried out to determine how ReaLJam can\nbest support both expert and novice musicians, informing future\ndesign decisions. Finally, our solutions for anticipation and syn-\nchronization can be integrated into other applications for real-time\nhuman-AI collaboration, both for music and other creative domains.\nAcknowledgments\nWe would like to thank Jaewook Lee and Noah Constant for their\nfeedback on the paper, the anonymous reviewers for their sugges-\ntions, and the rest of the Google Magenta team for their advice and\nsupport throughout the project.\nReferences\n[1] Chris Anderson, Ryan Miyakawa, and Dave Carlton. 2013. Hooktheory. https:\n//www.hooktheory.com/\n[2] Christodoulos Benetatos, Joseph VanderStel, and Zhiyao Duan. 2020. BachDuet:\nA Deep Learning System for Human-Machine Counterpoint Improvisation. In\nNew Interfaces for Musical Expression. https://api.semanticscholar.org/CorpusID:\n221668784\n\n\nReaLJam: Real-Time Human-AI Music Jamming with RL-Tuned Transformers\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\n[3] John Biles et al. 1994. GenJam: A genetic algorithm for generating jazz solos. In\nICMC, Vol. 94. Ann Arbor, MI, 131–137.\n[4] John A Biles. 1998. Interactive GenJam: Integrating real-time performance with\na genetic algorithm. In ICMC.\n[5] Chris Donahue, Antoine Caillon, Adam Roberts, Ethan Manilow, Philippe Esling,\nAndrea Agostinelli, Mauro Verzetti, Ian Simon, Olivier Pietquin, Neil Zeghidour,\net al. 2023. SingSong: Generating musical accompaniments from singing. arXiv\npreprint arXiv:2301.12662 (2023).\n[6] Richard D Goffin and James M Olson. 2011. Is it all relative? Comparative\njudgments and the possible improvement of self-ratings and ratings of others.\nPerspectives on Psychological Science 6, 1 (2011), 48–60.\n[7] Google. 2024. MusicFX. https://aitestkitchen.withgoogle.com/tools/music-fx\n[8] Harmonix. 2005. Guitar Hero.\n[9] Cheng-Zhi Anna Huang, Curtis Hawthorne, Adam Roberts, Monica Dinculescu,\nJames Wexler, Leon Hong, and Jacob Howcroft. 2019. The bach doodle: Ap-\nproachable music composition with machine learning at scale. arXiv preprint\narXiv:1907.06637 (2019).\n[10] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis\nHawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Din-\nculescu, and Douglas Eck. 2019. Music Transformer. In International Conference\non Learning Representations. https://openreview.net/forum?id=rJe4ShAcF7\n[11] David Huron. 2008. Sweet anticipation: Music and the psychology of expectation.\nMIT press.\n[12] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez Lobato,\nRichard E. Turner, and Doug Eck. 2017. Tuning Recurrent Neural Networks With\nReinforcement Learning. In International Conference on Learning Representations\nworkshop. https://openreview.net/pdf?id=Syyv2e-Kx\n[13] Nan Jiang, Sheng Jin, Zhiyao Duan, and Changshui Zhang. 2020. RL-Duet: Online\nMusic Accompaniment Generation Using Deep Reinforcement Learning. In AAAI\nConference on Artificial Intelligence. https://api.semanticscholar.org/CorpusID:\n211069124\n[14] Ian Jones and Chris Wheadon. 2015. Peer assessment using comparative and\nabsolute judgement. Studies in Educational Evaluation 47 (2015), 93–101.\n[15] Elaine King and Jane Ginsborg. 2016. Gestures and glances: Interactions in\nensemble rehearsal. In New perspectives on music and gesture. Routledge, 177–\n202.\n[16] Synthesia LLC. 2006. Synthesia.\n[17] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J. Cai.\n2020. Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative\nModels. In Proceedings of the 2020 CHI Conference on Human Factors in Computing\nSystems (Honolulu, HI, USA) (CHI ’20). Association for Computing Machinery,\nNew York, NY, USA, 1–13. doi:10.1145/3313831.3376739\n[18] Ryan Louie, Jesse Engel, and Cheng-Zhi Anna Huang. 2022. Expressive Commu-\nnication: Evaluating Developments in Generative Models and Steering Interfaces\nfor Music Creation. In 27th International Conference on Intelligent User Interfaces\n(Helsinki, Finland) (IUI ’22). Association for Computing Machinery, New York,\nNY, USA, 405–417. doi:10.1145/3490099.3511159\n[19] Jon McCormack, Patrick Hutchings, Toby Gifford, Matthew Yee-King,\nMaria Teresa Llano, and Mark D’inverno. 2020. Design Considerations for Real-\nTime Collaboration with Creative Artificial Intelligence. Organised Sound 25, 1\n(2020), 41–52. doi:10.1017/S1355771819000451\n[20] Jérôme Nika and Marc Chemillier. 2012. Improtek: integrating harmonic controls\ninto improvisation in the filiation of OMax. In International Computer Music\nConference (ICMC). 180–187.\n[21] Jérôme Nika, Marc Chemillier, and Gérard Assayag. 2017. Improtek: introducing\nscenarios into human-computer music improvisation. Computers in Entertainment\n(CIE) 14, 2 (2017).\n[22] Takayuki Osa, Voot Tangkaratt, and Masashi Sugiyama. 2022.\nDiscovering\ndiverse solutions in deep reinforcement learning by maximizing state–action-\nbased mutual information. Neural Networks 152 (2022), 90–104.\n[23] François Pachet, Pierre Roy, Julian Moreira, and Mark d’Inverno. 2013. Reflexive\nloopers for solo musical improvisation. In Proceedings of the SIGCHI Conference\non Human Factors in Computing Systems. 2205–2208.\n[24] Cansu Sancaktar, Justus Piater, and Georg Martius. 2023. Regularity as Intrin-\nsic Reward for Free Play. In Intrinsically-Motivated and Open-Ended Learning\nWorkshop @NeurIPS2023. https://openreview.net/forum?id=Pqx2EED04F\n[25] Ian Simon, Dan Morris, and Sumit Basu. 2008. MySong: automatic accompaniment\ngeneration for vocal melodies. In Proceedings of the SIGCHI conference on human\nfactors in computing systems.\n[26] Kıvanç Tatar and Philippe Pasquier. 2019. Musical agents: A typology and state\nof the art towards Musical Metacreation. Journal of New Music Research 48, 1\n(2019), 56–105.\n[27] Notto JW Thelle and Philippe Pasquier. 2021. Spire Muse: A Virtual Musical\nPartner for Creative Brainstorming. In NIME 2021. PubPub.\n[28] John Thickstun, David Leo Wright Hall, Chris Donahue, and Percy Liang. 2024.\nAnticipatory Music Transformer. Transactions on Machine Learning Research\n(2024). https://openreview.net/forum?id=EBNJ33Fcrl\n[29] Bandhav Veluri, Benjamin N Peloquin, Bokai Yu, Hongyu Gong, and Shyamnath\nGollakota. 2024. Beyond Turn-Based Interfaces: Synchronous LLMs as Full-\nDuplex Dialogue Agents. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and\nYun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida,\nUSA, 21390–21402. doi:10.18653/v1/2024.emnlp-main.1192\n[30] Michael A Vidulich and Pamela S Tsang. 1987. Absolute magnitude estimation and\nrelative judgement approaches to subjective workload assessment. In Proceedings\nof the Human Factors Society Annual Meeting, Vol. 31. SAGE Publications Sage\nCA: Los Angeles, CA, 1057–1061.\n[31] Zihao Wang, Kejun Zhang, Yuxing Wang, Chen Zhang, Qihao Liang, Pengfei Yu,\nYongsheng Feng, Wenbo Liu, Yikai Wang, Yuntao Bao, et al. 2022. Songdriver:\nReal-time music accompaniment generation without logical latency nor exposure\nbias. In Proceedings of the 30th ACM International Conference on Multimedia. 1057–\n1067.\n[32] Yusong Wu, Tim Cooijmans, Kyle Kastner, Adam Roberts, Ian Simon, Alexan-\nder Scarlatos, Chris Donahue, Cassie Tarakajian, Shayegan Omidshafiei, Aaron\nCourville, Pablo Samuel Castro, Natasha Jaques, and Cheng-Zhi Anna Huang.\n2024. Adaptive Accompaniment with ReaLchords. In Forty-first International\nConference on Machine Learning.\n[33] Chao Zhang, Cheng Yao, Jiayi Wu, Weijia Lin, Lijuan Liu, Ge Yan, and Fangtian\nYing. 2022. StoryDrawer: A Child–AI Collaborative Drawing System to Support\nChildren’s Creative Visual Storytelling. In Proceedings of the 2022 CHI Conference\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI ’22).\nAssociation for Computing Machinery, New York, NY, USA, Article 311, 15 pages.\ndoi:10.1145/3491102.3501914\nA\nEthics Statement\nThere are several potential positive societal outcomes related to\nour work, while there are ethical risks to consider as well. Primar-\nily, providing people with a high quality automated jam partner\ncould increase access to artistic creation and education, since many\npeople do not have access to human jam partners due to finances,\ninability to travel, or other reasons. On the other hand, it is possi-\nble that automated jam partners could replace paid musicians to\nsome extent, which is a concern across AI domains. Also, in our\nwork, we use models trained on a dataset of primarily Western pop\nmusic, which may make our system less accessible to people from\nother cultures. However, we note that our interface is designed\nto work with any frame-based model that can anticipate future\nactions, allowing more culturally diverse models to be applied in\nfuture work.\nB\nRelated Work\nThere is a long history to designing intelligent music systems that\nadapt to musicians in real-time [19, 26], enabling a wide range of\nhuman-AI relationships in music. GenJam [3, 4] trades fours and\neights with a musician. Reflexive Loopers [23] extends loop pedals\nby adapting its choice of samples to features of a musician’s per-\nformance. Spiremuse [27] serves as a brainstorming partner which\nretrieves or recombines musical responses that shadow, mirror or\ncouple with the musician. MusicFX DJ [7] allows users to add and\nbalance text prompts as music is being generated. Cococo [17]\nfills in musical parts based on user settings on high-level aspects\nlike emotion, and [18] provides users with options for subsequent\ngenerations based on previous user selections. However, all these\nreal-time interactions are facilitated through turn-taking, built-in\ndelay, or loose coupling, therefore being unsuitable for the jamming\nsetting where users and agents create music concurrently with no\ndiscernible delay.\nIn contrast, we focus on the tightly coupled setting of real-time\naccompaniment, which requires the machine to be highly synchro-\nnized with the musician. Prior work in accompaniment generation\n\n\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nScarlatos et al.\noften assumes that the system is offline [5, 9, 25] or that a pre-\ndefined plan is given, such as a Jazz standard [20, 21]. For the\nreal-time setting, we need to additionally account for delay and\nalso planning. This requires not just generative agents that support\nonline generation and have anticipatory capabilities but also user\ninterfaces that can communicate to a user how an agent is planning\nand re-planning its trajectory.\nRL-Duet [13], SongDriver [31], and ReaLchords [32] are genera-\ntive models and agents that target online generation. However, they\nhave only been tested in simulation and not yet with users, do not\nrun in real-time, and do not have a user interface. To the best of our\nknowledge, only BachDuet [2] built a complete interactive system\nfor real-time counterpoint accompaniment. Compared to ReaLJam,\nthe implementation of BachDuet is simpler and more genre-specific:\nit is trained on two-part Bach chorales, uses a locally-run LSTM\nmodel, and trains with a supervised objective. In comparison, ReaL-\nJam uses the ReaLchords model, a Transformer trained on a large\ncorpus of pop music using RL fine-tuning, and interacts with the\nmodel through a client-server architecture.\n\n\nReaLJam: Real-Time Human-AI Music Jamming with RL-Tuned Transformers\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nC\nDetailed Intra-Study Results\nI.C. On\nMet. On\nRL-M\nOnline\nInt.\nAdpt.\nCtrl.\nEnj.\nInt.\nAdpt.\nCtrl.\nEnj.\nInt.\nAdpt.\nCtrl.\nEnj.\nInt.\nAdpt.\nCtrl.\nEnj.\nP1\n-1\n0\n0\n1\n-1\n-1\n1\n-1\n0\n-1\n-1\n-1\n1\n0\n-1\n0\nP2\n-1\n0\n1\n1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\nP3\n-1\n0\n0\n-1\n1\n1\n1\n1\n1\n0\n0\n1\n-1\n-1\n-1\n-1\nP4\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n0\n1\n0\n0\n-1\n-1\n-1\n-1\nP5\n-1\n-1\n-1\n-1\n0\n0\n0\n-1\n0\n0\n0\n-1\n-1\n-1\n-1\n-1\nP6\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n1\n0\n1\nAvg.\n-0.67\n-0.17\n0.00\n0.00\n0.00\n0.17\n0.50\n0.00\n0.50\n0.33\n0.17\n0.17\n-0.50\n-0.50\n-0.83\n-0.50\nTemp. 1\nCom. 2\nCom. 0\nI.S. 8\nInt.\nAdpt.\nCtrl.\nEnj.\nInt.\nAdpt.\nCtrl.\nEnj.\nInt.\nAdpt.\nCtrl.\nEnj.\nInt.\nAdpt.\nCtrl.\nEnj.\nP1\n1\n0\n1\n1\n0\n1\n1\n1\n-1\n0\n1\n-1\n1\n1\n1\n1\nP2\n1\n-1\n0\n-1\n1\n1\n0\n1\n1\n1\n-1\n1\n-1\n-1\n-1\n-1\nP3\n0\n-1\n-1\n-1\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1\n0\nP4\n-1\n-1\n-1\n-1\n0\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\nP5\n1\n0\n1\n1\n-1\n1\n0\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\nP6\n-1\n-1\n1\n1\n0\n0\n-1\n-1\n1\n1\n-1\n-1\n-1\n-1\n-1\n0\nAvg.\n0.17\n-0.67\n0.17\n0.00\n0.17\n0.33\n-0.17\n0.00\n0.33\n0.50\n0.00\n0.00\n0.17\n0.17\n0.33\n0.33\nTable 2: Users’ intra-study experiment scores, where a user choosing the indicated setting gets a value of 1, choosing “No\nDifference” gets a value of 0, and choosing the alternative gets a value of -1.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21267v1.pdf",
    "total_pages": 9,
    "title": "ReaLJam: Real-Time Human-AI Music Jamming with Reinforcement Learning-Tuned Transformers",
    "authors": [
      "Alexander Scarlatos",
      "Yusong Wu",
      "Ian Simon",
      "Adam Roberts",
      "Tim Cooijmans",
      "Natasha Jaques",
      "Cassie Tarakajian",
      "Cheng-Zhi Anna Huang"
    ],
    "abstract": "Recent advances in generative artificial intelligence (AI) have created\nmodels capable of high-quality musical content generation. However, little\nconsideration is given to how to use these models for real-time or cooperative\njamming musical applications because of crucial required features: low latency,\nthe ability to communicate planned actions, and the ability to adapt to user\ninput in real-time. To support these needs, we introduce ReaLJam, an interface\nand protocol for live musical jamming sessions between a human and a\nTransformer-based AI agent trained with reinforcement learning. We enable\nreal-time interactions using the concept of anticipation, where the agent\ncontinually predicts how the performance will unfold and visually conveys its\nplan to the user. We conduct a user study where experienced musicians jam in\nreal-time with the agent through ReaLJam. Our results demonstrate that ReaLJam\nenables enjoyable and musically interesting sessions, and we uncover important\ntakeaways for future work.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}