{
  "id": "arxiv_2502.20156v1",
  "text": "Adaptive H&E-IHC information fusion staining\nframework based on feature extractor\nYifan Jia*2,4, Xingda Yu*4, Zhengyang Ji2,4, Songning Lai1,2, and Yutao Yue1\n,2 ,3(B)\n1 HKUST(GZ)\nyutaoyue@hkust-gz.edu.cn\n2 Deep Interdisciplinary Intelligence Lab\n3 Institute of Deep Perception Technology, JITRI\n4 Shandong University\nAbstract. Immunohistochemistry (IHC) staining plays a significant role\nin the evaluation of diseases such as breast cancer. The H&E-to-IHC\ntransformation based on generative models provides a simple and cost-\neffective method for obtaining IHC images. Although previous models\ncan perform digital coloring well, they still suffer from (i) coloring only\nthrough the pixel features that are not prominent in HE, which is easy\nto cause information loss in the coloring process; (ii) The lack of pixel-\nperfect H&E-IHC groundtruth pairs poses a challenge to the classical\nL1 loss.To address the above challenges, we propose an adaptive infor-\nmation enhanced coloring framework based on feature extractors. We\nfirst propose the VMFE module to effectively extract the color infor-\nmation features using multi-scale feature extraction and wavelet trans-\nform convolution, while combining the shared decoder for feature fu-\nsion. The high-performance dual feature extractor of H&E-IHC is trained\nby contrastive learning, which can effectively perform feature alignment\nof HE-IHC in high latitude space. At the same time, the trained fea-\nture encoder is used to enhance the features and adaptively adjust the\nloss in the HE section staining process to solve the problems related\nto unclear and asymmetric information. We have tested on different\ndatasets and achieved excellent performance.Our code is available at\nhttps://github.com/babyinsunshine/CEFF\nKeywords: H&E-to-IHC virtual staining · Generative adversarial net ·\nContrastive learning · Feature fusion\n1\nIntroduction\nImmunohistochemistry (IHC) staining is a widely used technique in pathology\nfor visualizing common abnormal cells in tumors, which is crucial for developing\nprecise treatment plans. However, traditional detection methods are both time-\nconsuming and labor-intensive, with standard tissue pathology imaging involving\n1 * Equal Contribution\n2 Under Review\narXiv:2502.20156v1  [cs.CV]  27 Feb 2025\n\n\n2\nAnonymized Author et al.\nin vivo tissue sampling, tissue fixation, tissue processing, section staining, micro-\nscopic observation, image capture, and image analysis [1]. These factors hinder\nthe widespread applicability of IHC staining in tissue pathology. With advance-\nments in computer vision technology, researchers have applied computer vision\ntechniques to the slide staining process (virtual staining), significantly improving\ndetection efficiency and saving valuable time for patient treatment [2–5].\nExisting virtual staining methods are mainly based on adversarial genera-\ntion techniques. Liu et al. proposed PyramidPix2Pix [6], which applies Gaussian\nconvolutions to image pairs and processes them at multiple scales, reducing\nthe requirement for pixel-level precise alignment. Li et al. introduced a novel\nloss function designed to mitigate the negative impact of these inconsistencies\non model performance [6]. This loss function enables the model to better han-\ndle noise or low-quality data, thereby improving the robustness of the staining\ntransformation. Li et al. also designed a multi-layer weak pathological consis-\ntency constraint, combined with an adaptive weight strategy and discriminator\ncontrastive regularization loss, which significantly enhances the pathological con-\nsistency and realism of generated tissue slices [7].\nAlthough the aforementioned studies have made significant advancements in\nthe field of virtual staining, there are still several aspects that have not been\nfully addressed. i) Existing works mainly focus on j pixel information based\nstain generation task, overlooking the correspondence between potential staining\ngrade labels of HE and IHC slides, which is often a key factor that doctors\nconsider during diagnosis. ii) The feature extraction methods used in current\ngenerator networks are limited and tend to overlook critical details, leading to\npoor detail in the generated IHC images.The information features in HE slides\nare not immediately apparent, which places a significant demand on the feature\nextraction capabilities of the generator network. iii)The lack of pixel-perfect\nH&E-IHC groundtruth pairs poses a challenge to the classical L1 loss.\nTo address the aforementioned issues, we make the following contributions: 1)\nWe propose the VMFE module, which employs multi-scale feature extraction and\nutilizes wavelet transform convolutions [8,13,14] for efficient extraction of stain-\ning information features, while incorporating a shared decoder for feature fusion.\n2) Inspired by contrastive learning [15,16], we pre-train feature encoders for HE\n(Hematoxylin and Eosin) and IHC (Immunohistochemistry) images, aiming to\nunsupervisedly align staining labels for HE and IHC images in the latent space.\n3) We leverage the trained feature encoders to enhance features and adaptively\nadjust the loss during the staining process for HE slides [17], addressing issues\nrelated to unclear and asymmetric information. Finally, we conduct extensive\ntesting across multiple datasets to validate the effectiveness of our method.\n2\nMethod\nFigure 1 provides an overview of our proposed framework for adaptive IHC\nvirtual staining. As shown in Figure 1(a), the architecture is centered on the\nMulti-Scale Modulated Feature Fusion Generator, which utilizes the Virtual\n\n\nTitle Suppressed Due to Excessive Length\n3\nFig. 1. Overview of the proposed framework.\nMulti-scale Feature Extractor (VMFE) to process H&E images. It achieves this\nby processing the downsampled features through the VMFE module and fusing\nthem with later-layer feature maps to fully leverage information. Additionally,\nwe use the Cross-Attention module (CoA) to fuse the feature maps obtained\nfrom the encoded H&E images with those from the generator, providing more\nguidance for IHC image generation. Figure 1(b) highlights the pre-training pro-\ncess of the HE Encoder and IHC Encoder, where contrastive learning (using\nthe InfoNCE loss function) trains the encoders to capture the semantic relation-\nships between H&E and IHC images. Figure 1(c) illustrates the adaptive L1 loss\nmechanism, which dynamically adjusts the loss weights based on the cosine sim-\nilarity between the patch embedding vectors of the generated IHC image and the\nground truth image, obtaining an adaptive weighted L1 (AWL) loss to address\nthe non-strict symmetry issue between H&E and IHC images, thereby improving\nstaining accuracy.\n2.1\nMulti-Scale Feature Extraction and Fusion\nConsidering the issues of error propagation during sampling from low reso-\nlution to high resolution in a U-Net-like generator, as well as the insufficient\ninformation processing in large-scale skip connections, we propose a generator\nbased on Virtual Multi-scale Feature Extraction (VMFE). The basic structure\n\n\n4\nAnonymized Author et al.\nof this generator is resnet-6blocks, with VMFE replacing its downsampling com-\nponent. VMFE primarily consists of wavelet convolution downsampling and a\nMulti-scale Sequential Feature Processing Module (MSFPM). For the input im-\nage X, wavelet convolution-based downsampling layers produce multi-scale fea-\nture maps X1, X2, and X3 with scales of 1, 1/2, and 1/4, respectively. These\nmulti-scale feature maps have a larger receptive field.\nThen, mimicking the coarse-to-fine approach of traditional U-Net, we sequen-\ntially input the multi-scale feature maps X3, X2, and X1 into the MSFPM. The\nMSFPM utilizes a convolution-based Gated Recurrent Unit (GRU) to modu-\nlate the content between the previous activation ht−1 and the current input ht.\nThis is because there exists an abstract temporal relationship among the fea-\nture maps obtained through downsampling. By using this module, we aim to\nenable the network to comprehensively consider the sequential relationship of\neach feature map. The hidden state update of the module can be simplified as:\nht = MSFPM(Xt, ht−1),\nwhere Xt (t = 3, 2, 1) denotes the input feature map, and ht represents the\noutput hidden state. Since X3, X2, and X1 have different scales, we downsample\neach feature map to a 1/4 scale, denoted as ˜Xt = Downsample(Xt, 1/4). Then,\nthe module’s outputs h3, h2, and h1 are respectively fused with the second, third,\nand fourth feature maps in the generator block via addition, i.e.,\nF ′\nk = Fk + h4−k,\nk = 2, 3, 4,\n(5)\nwhere k denotes the index of the feature maps in the generator block (correspond-\ning to k = 2, 3, 4 for the second, third, and fourth feature maps, respectively),\nFk represents the original feature map, and F ′\nk denotes the fused feature map,\nthereby enhancing the model’s performance.\n2.2\nContrastive Learning Strategy of Dual Encoders\nIn medical image processing, there must be an inherent connection between\nthe images before and after staining, that is, they contain a large amount of\nthe same semantic information. Based on this, we propose to use the method of\ncontrastive learning to train two encoders, which respectively encode the images\nbefore and after staining. Pathological images contain a vast amount of complex\ninformation, and it is difficult to comprehensively capture all features using a\nsingle encoder. Therefore, we use two independent encoders for separate train-\ning to ensure that various features in the images can be fully mined, and to\nimprove the comprehensiveness and accuracy of feature extraction. To measure\nthe similarity and dissimilarity between encoded features, guiding the encoder to\nlearn more discriminative and representative image features,We use the InfoNCE\nloss [18] function:\nLNCE = −1\nN\nN\nX\ni=1\nlog\nexp(s(zi, z+\ni )/τ)\nPM\nj=1 exp(s(zi, zj)/τ)\n,\n(1)\n\n\nTitle Suppressed Due to Excessive Length\n5\nwhere zi and z+\ni represent the feature vector of the i-th sample and its positive\ncounterpart, s(zi, zj) is the cosine similarity score, and τ is the temperature\nparameter. N is the batch si and M is the number of negative samples. To\nprevent overfitting during training, we introduce an L2 regularization term:\nL2 = λ\nX\nw∈θ\n|w|2\n2,\n(2)\nwhere λ is the regularization strength and θ represents the set of model param-\neters.\nFinally, our total loss formulations are as follows:\nL = LNCE + λ\nX\nw∈θ\n|w|2\n2.\n(3)\n2.3\nCross-Attention Feature Fusion between Encoder and\nGenerator\nTo leverage the information captured by the trained H&E encoder—owing to\nthe use of contrastive loss, which encodes mutual information between H&E and\nIHC images—we propose a cross-attention fusion module. This module integrates\na feature map from a specific layer of the encoder with the first feature map of\nthe generator block to guide the staining process.\nGiven the generator feature map Fgen ∈RB×C×H×W and the encoder feature\nmap Fenc ∈RB×C×H×W , we generate queries Q, keys K, and values V via 1×1\nconvolutions, followed by reshaping into RB×N×d, where N = H × W and d is\nthe feature dimension. The fusion process is defined as follows:\nThe output feature map is computed as:\nFout = Fgen + α · BN\n\u0012\nWout ∗\n\u0012\nsoftmax\n\u0012QK⊤\n√\nd\n\u0013\nV\n\u0013\u0013\n,\n(4)\nwhere Wout denotes the 1×1 convolution weight, ∗represents the convolution\noperation, BN is batch normalization, and α is a hyperparameter controlling the\nfusion strength. Through this approach, the generator effectively incorporates\nthe encoder’s information, improving the accuracy of generating IHC images\nfrom H&E images.\n2.4\nAdaptive L1 Loss\nDue to the non-strict symmetry between H&E images and IHC images, we\nadapt the L1 loss weight by leveraging the encoding information from the IHC\nencoder. The generated image and the ground truth are divided into multiple\npatches, and the cosine similarity of the corresponding patches’ embedding vec-\ntors, after passing through the IHC encoder, is computed. The adaptive L1 loss\nis defined as:\n\n\n6\nAnonymized Author et al.\nTable 1. Comparative Performance Evaluation on Histopathology Datasets\nHER2Bci\nERMist\nMethod\nMetrics\nMethod\nMetrics\nPSNR↑SSIM↑FID↓\nPSNR↑SSIM↑FID↓\nCycleGAN\n14.201\n0.424 63.7\nCycleGAN\n11.900\n0.181 88.7\nCUT\n17.322\n0.438 65.0\nCUT\n12.030\n0.183 47.1\nPyramidP2P 21.160\n0.477 80.1\nPyramidP2P 12.100\n0.191 80.8\nASP\n17.869\n0.492 54.3\nASP\n13.890\n0.206 41.2\nESI\n19.132\n0.499 50.1\nESI\n13.900\n0.209 34.9\nOurs\n21.380\n0.504 94.1\nOurs\n15.562\n0.243 30.9\nPRMist\nKi67Mist\nMethod\nPSNR↑SSIM↑FID↓\nMethod\nPSNR↑SSIM↑FID↓\nCycleGAN\n12.990\n0.187 78.6\nCycleGAN\n12.917\n0.201 100.8\nCUT\n13.560\n0.192 53.2\nCUT\n13.697\n0.212 53.1\nPyramidP2P 14.430\n0.224 79.2\nPyramidP2P 13.987\n0.248 89.8\nASP\n14.330\n0.216 44.5\nASP\n14.824\n0.241 50.9\nESI\n15.936\n0.248 34.2\nESI\n16.093\n0.262 31.1\nOurs\n15.990\n0.290 93.7\nOurs\n16.210\n0.316 107.6\nThe red value indicates the best performance case. Blue indicates the second-best\nperformance case.\nL1 =\nn−1\nX\ni=0\n(α + β · Simi) /n\n(5)\nwhere Simi is the cosine similarity between the embedding vectors of the\ncorresponding patch pair, and lower similarity often indicates poor symmetry,\nthus reducing the L1 loss weight.\n3\nExperiments\n3.1\nExperimental Setup\nDatasets In this study, we selected two key datasets: the Breast Cancer Im-\nmunohistochemistry (BCI) [6] Challenge dataset and the MIST dataset [9]. The\nBCI dataset comprehensively covers different levels of HER2 expression, provid-\ning a rich data foundation for in - depth research on the characteristics related to\nHER2 expression. The MIST dataset, on the other hand, contains immunohis-\ntochemical staining data for HER2, PR, ER, and Ki67, presenting information\non breast cancer - related indicators from multiple dimensions. Our division of\nthe test set and training set is consistent with that in the original paper.\n\n\nTitle Suppressed Due to Excessive Length\n7\nFig. 2. Encoder performance analysis on BCI dataset.\nExperimental Details Our model was trained on an NVIDIA RTX 3090 GPU.\nFor both the encoder and the model, we employed the Adam optimizer. The\nencoder was trained for 300 epochs with a batch size of 64, while the model was\ntrained for 100 epochs with a batch size of 1. We randomly cropped the images\nto a size of 512×512 for training.The fusion strength was set to 0.2, while the\nparameters α and β of the adaptive L1 loss were both set to 50.\nEvaluation Methods To comprehensively evaluate the model, we adopted\nmultiple metrics. PSNR measures the distortion between generated and real\nimages, with a higher value indicating better quality. SSIM assesses structural\nsimilarity, closer to 1 meaning more similar structures and better aligning with\nhuman vision. FID quantifies the difference between the distributions of gener-\nated and real images, with a lower value denoting better quality and diversity.\n3.2\nComparative Experiments\nThe performance of the dual encoder. Dual Encoders aim to capture the\nconsistency of paired H&E and IHC images using contrastive learning. In this\nsection, we show the effectiveness of the dual encoder. We test the performance\nof the dual encoder by constructing paired pairs of positive samples and unpaired\nTable 2. Ablation Study on ERMist Dataset\nConfiguration\nPSNR ↑SSIM ↑FID ↓\nWithout Multi-Scale Feature Extraction\n15.357\n0.235\n66.66\nWithout Cross-Attention Feature Fusion 15.423\n0.237\n67.73\nWithout Adaptive L1 Loss\n15.437\n0.240\n89.77\nFull Model (All Methods)\n15.562\n0.243\n30.9\n\n\n8\nAnonymized Author et al.\nFig. 3. Visualize different methods on different dataset images.\npairs of negative samples. As shown in Figure 2, after coding and calculating the\nsimilarity between H&E and IHC, we can find that there is a clear boundary\nbetween the similarity of positive sample pairs and negative sample pairs. When\nthe similarity boundary is 0.44, the recognition accuracy of positive and negative\nsample pairs reaches up to 92.37%. When the similarity is less than 0.44, HE\nand IHC are mostly unpaired data, and when the similarity is greater than 0.44,\nit is mostly paired data.\nComparison with State-of-the-arts. Table 1 summarizes the quantitative\ncomparison results on the BCI dataset. We compared our proposed method\nwith the following five methods: CycleGAN [10], Cut [11], Pyramid Pix2Pix [6],\nASP [12], and ESI [7].Our proposed method achieved competitive performance\nacross various datasets, attributable to the integration of contrastive learning,\nmulti-scale feature fusion, and adaptive L1 loss. On the MIST dataset, which\nincludes multiple IHC markers (HER2, PR, ER, Ki67), our method maintained\nits superiority, particularly in the PSNR and SSIM metric.Our method per-\nforms slightly worse on the FID metric, but also achieves sota results on some\ndatasets.Fig. 3 illustrates representative IHC images generated from H&E in-\nputs. Compared to the baselines, our method produced sharper edges, richer\ntextures, and more accurate protein expression patterns, especially in regions\nwith complex tissue morphology.\nAblation Experiments To evaluate the contribution of each component in\nour proposed framework, we conducted an ablation study on the BCI Challenge\ndataset, as shown in Table 2. We used the full model, incorporating all com-\nponents, as the performance reference. Replacing the VMFE module with the\noriginal network led to a decline in the ability to preserve pathological details,\n\n\nTitle Suppressed Due to Excessive Length\n9\nresulting in reduced overall performance. Removing the cross-attention feature\nfusion module decreased the utilization efficiency of information between the en-\ncoder and generator, affecting staining accuracy. Excluding the adaptive L1 loss\nexacerbated the issue of image asymmetry, further degrading performance. These\nresults underscore the importance of each component, demonstrating that their\ncombined effect is crucial for achieving superior H&E-to-IHC virtual staining\nperformance.\n4\nConclusion\nWe propose an adaptive IHC virtual staining method framework using contrastive-\nencoding feature fusion. By aligning H&E and IHC features via dual-branch con-\ntrastive learning, enhancing structural consistency with cross-attention fusion,\nand mitigating asymmetry with a dynamic L1 loss, our method outperforms\nexisting approaches. Experiments and ablation studies validate its effectiveness\nin improving staining quality and detail preservation. This framework offers a\npromising tool for rapid, cost-effective pathological diagnosis with potential clin-\nical impact.\nAcknowledgments. This work was supported by Guangzhou-HKUST(GZ)\nJoint Funding Program(Grant No.2023A03J0008), Education Bureau of Guangzhou\nMunicipality.\nReferences\n1. F. Anglade, D. A. Milner Jr, and J. E. Brock, Can pathology diagnostic services for\ncancer be stratified and serve global health?, Cancer 126, 2431–2438 (2020).\n2. Y. Rivenson et al., Virtual histological staining of unlabelled tissue-autofluorescence\nimages via deep learning, Nature biomedical engineering 3, 466–477 (2019).\n3. D. Li, H. Hui, Y. Zhang, W. Tong, F. Tian, X. Yang, J. Liu, Y. Chen, and J. Tian,\nDeep learning for virtual histological staining of bright-field microscopic images of\nunlabeled carotid artery tissue, Molecular imaging and biology 22, 1301–1309 (2020).\n4. S. Liu, C. Zhu, F. Xu, X. Jia, Z. Shi, and M. Jin, Bci: Breast cancer\nimmunohisto- chemical image generation through pyramid pix2pix, in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n1815–1824, 2022.\n5. F. Li, Z. Hu, W. Chen, and A. Kak, Adaptive supervised patchnce loss for learn-\ning h&e-to-ihc stain translation with inconsistent groundtruth image pairs, arXiv\npreprint arXiv:2303.06193 (2023).\n6. Liu, S., Zhu, C., Xu, F., Jia, X., Shi, Z., Jin, M.: Bci: Breast cancer immunohisto-\nchemical image generation through pyramid pix2pix. 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 1814–1823\n(2022).\n7. Li, Y., Guan, X., Wang, Y., Zhang, Y. (2024). Exploiting Supervision Information\nin Weakly Paired Images for IHC Virtual Staining. In: Linguraru, M.G., et al.\nMedical Image Computing and Computer Assisted Intervention – MICCAI 2024.\n\n\n10\nAnonymized Author et al.\nMICCAI 2024. Lecture Notes in Computer Science, vol 15004. Springer, Cham.\nhttps://doi.org/10.1007/978-3-031-72083-3_11.\n8. L. Li, F. Mao, W. Qian and L. P. Clarke, \"Wavelet transform for directional feature\nextraction in medical imaging,\" Proceedings of International Conference on Image\nProcessing, Santa Barbara, CA, USA, 1997, pp. 500-503 vol.3, https://doi.org/\n10.1109/ICIP.1997.632167.\n9. Li, F., Hu, Z., Chen, W., Kak, A. (2023). Adaptive Supervised PatchNCE Loss\nfor Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image\nPairs. In: Greenspan, H., et al. Medical Image Computing and Computer Assisted\nIntervention – MICCAI 2023. MICCAI 2023. Lecture Notes in Computer Science,\nvol 14225. Springer, Cham. https://doi.org/10.1007/978-3-031-43987-2_61\n10. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. 2017 IEEE International Conference on\nComputer Vision (ICCV) pp. 2242–2251 (2017)\n11. Park, T., Efros, A.A., Zhang, R., Zhu, JY. (2020). Contrastive Learning for Un-\npaired Image-to-Image Translation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm,\nJM. (eds) Computer Vision – ECCV 2020. ECCV 2020. Lecture Notes in Com-\nputer Science(), vol 12354. Springer, Cham. https://doi.org/10.1007/978-3-030-\n58545-7_19\n12. Li, F., Hu, Z., Chen, W., Kak, A.C.: Adaptive supervised patchnce loss for learning\nh&e-to-ihc stain translation with inconsistent groundtruth image pairs. In: Interna-\ntional Conference on Medical Image Computing and Computer-Assisted Interven-\ntion (2023)\n13. Yuan S, Luo L, Hui Z, et al. UnSAMFlow: Unsupervised Optical Flow Guided by\nSegment Anything Model[C]//Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition. 2024: 19027-19037.\n14. Zhou S, He R, Tan W, et al. Samflow: Eliminating any fragmentation in optical\nflow with segment anything model[C]//Proceedings of the AAAI Conference on\nArtificial Intelligence. 2024, 38(7): 7695-7703.\n15. Xiao T, Wang X, Efros A A, et al. What should not be contrastive in contrastive\nlearning[J]. arXiv preprint arXiv:2008.05659, 2020.\n16. Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models\nfrom natural language supervision[C]//International conference on machine learn-\ning. PmLR, 2021: 8748-8763.\n17. Zamir, Syed Waqas, et al. \"Learning enriched features for real image restoration\nand enhancement.\" Computer Vision–ECCV 2020: 16th European Conference, Glas-\ngow, UK, August 23–28, 2020, Proceedings, Part XXV 16. Springer International\nPublishing, 2020.\n18. Parulekar, Advait, et al. \"Infonce loss provably learns cluster-preserving represen-\ntations.\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20156v1.pdf",
    "total_pages": 10,
    "title": "Adaptive H&E-IHC information fusion staining framework based on feature extra",
    "authors": [
      "Yifan Jia",
      "Xingda Yu",
      "Zhengyang Ji",
      "Songning Lai",
      "Yutao Yue"
    ],
    "abstract": "Immunohistochemistry (IHC) staining plays a significant role in the\nevaluation of diseases such as breast cancer. The H&E-to-IHC transformation\nbased on generative models provides a simple and cost-effective method for\nobtaining IHC images. Although previous models can perform digital coloring\nwell, they still suffer from (i) coloring only through the pixel features that\nare not prominent in HE, which is easy to cause information loss in the\ncoloring process; (ii) The lack of pixel-perfect H&E-IHC groundtruth pairs\nposes a challenge to the classical L1 loss.To address the above challenges, we\npropose an adaptive information enhanced coloring framework based on feature\nextractors. We first propose the VMFE module to effectively extract the color\ninformation features using multi-scale feature extraction and wavelet transform\nconvolution, while combining the shared decoder for feature fusion. The\nhigh-performance dual feature extractor of H&E-IHC is trained by contrastive\nlearning, which can effectively perform feature alignment of HE-IHC in high\nlatitude space. At the same time, the trained feature encoder is used to\nenhance the features and adaptively adjust the loss in the HE section staining\nprocess to solve the problems related to unclear and asymmetric information. We\nhave tested on different datasets and achieved excellent performance.Our code\nis available at https://github.com/babyinsunshine/CEFF",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}