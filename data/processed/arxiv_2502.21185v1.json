{
  "id": "arxiv_2502.21185v1",
  "text": "A Survey of Link Prediction in Temporal Networks\nJiafeng Xiong1*, Ahmad Zareie2 and Rizos Sakellariou1\n1*Department of Computer Science, University of Manchester, Oxford\nRd, Manchester M13 9PL, UK.\n2School of Computer Science, University of Sheffield, 211 Portobello,\nSheffield S1 4DP, UK.\n*Corresponding author(s). E-mail(s): jiafeng.xiong@manchester.ac.uk;\nContributing authors: A.Zareie@sheffield.ac.uk; rizos@manchester.ac.uk;\nAbstract\nTemporal networks have gained significant prominence in the past decade for\nmodelling dynamic interactions within complex systems. A key challenge in this\ndomain is Temporal Link Prediction (TLP), which aims to forecast future con-\nnections by analysing historical network structures across various applications\nincluding social network analysis. While existing surveys have addressed specific\naspects of TLP, they typically lack a comprehensive framework that distin-\nguishes between representation and inference methods. This survey bridges this\ngap by introducing a novel taxonomy that explicitly examines representation and\ninference from existing methods, providing a novel classification of approaches\nfor TLP. We analyse how different representation techniques capture temporal\nand structural dynamics, examining their compatibility with various inference\nmethods for both transductive and inductive prediction tasks. Our taxonomy\nnot only clarifies the methodological landscape but also reveals promising unex-\nplored combinations of existing techniques. This taxonomy provides a systematic\nfoundation for emerging challenges in TLP, including model explainability and\nscalable architectures for complex temporal networks.\nKeywords: Temporal networks, Dynamic networks, Link prediction, Graph learning,\nMachine learning\n1\narXiv:2502.21185v1  [cs.AI]  28 Feb 2025\n\n\n1 Introduction\nA temporal network is a network whose structure and properties evolve over time. In\nthe past decade, temporal networks have been used to model interactions between the\ncomponents of complex systems with dynamic structures and characteristics. A tem-\nporal network consists of two key elements: a set of nodes representing components of\na system and a set of links indicating interactions between pairs of nodes. In tempo-\nral networks, the links between nodes may change over time; new nodes may also join\nthe network. Identifying the likelihood of connecting between nodes to predict future\nlinks in a network (link prediction) is a fundamental problem that can be applied to\nvarious domains such as social networks [1], telecommunication networks [2], traffic\nforecasting [3, 4] or knowledge graphs [5, 6].\nLink prediction was first discussed in the context of static networks [7] where the\naim is to predict future links based on a network’s current state. In this paper this\nis referred to as traditional link prediction. However, the temporal link prediction\n(TLP) problem uses a network’s historical states to predict the likelihood of future\nlinks. Incorporating historical dynamics into the TLP problem introduces additional\ncomplexity [8, 9].\nRecently, numerous methods have been proposed to address the TLP problem,\neach method focusing on different network characteristics. Solving TLP requires two\nkey ingredients: first, a representation unit, which models how a temporal network can\nbe captured to reflect its traits and historical dynamics; and second, an inference unit,\nwhich determines the likelihood of future links based on the learned representation.\nSince all TLP models necessarily need both units, this review is confined to approaches\nconcerning either representation or inference. In the paper, TLP models are classified\naccording to whether their primary contribution is to representation or inference and\nthe defining attributes in each unit form the basis for the taxonomy adopted here. The\nsurvey begins by examining various representation methods and discusses their merits\nand challenges. It then classifies and evaluates the inference approaches that have been\nexplored in the literature. Guided by this methodological perspective, the discussion\nis subsequently divided into two sections (Section 4 and Section 5) dedicated to each\nunit respectively.\nThe field presents significant research opportunities through two primary avenues:\n(i) unexplored combinations of existing representation and inference units and (ii) the\ndevelopment of novel unit designs that could introduce new computational paradigms\nfor either representation or inference. These opportunities could lead to more effective\nTLP approaches.\nVarious surveys have been conducted to review TLP studies. Some surveys con-\ncentrate on specific network structures, such as homogeneous networks [10, 11], or\ndirected networks [12]. Other surveys focus on the application of TLP across various\ndomains, encompassing temporal knowledge graphs [13] and social networks [1, 14, 15].\nOur survey sets itself apart in several aspects:\n• The survey introduces a novel taxonomy that distinguishes between the represen-\ntation and inference units. Unlike existing surveys [10–12] which classify models\nunder a single hierarchical framework, our approach separates these components to\n2\n\n\nprovide deeper insights into the models’ applicability and performance variations\nwhilst also exploring diverse network characteristics pertinent to the TLP problem.\n• The survey provides a comprehensive review of the network representation unit.\nWhile previous surveys [1, 14] provided some discussions of these models, we offer a\nsystematic classification and analyse their ability to capture temporal and structural\ndynamics, thereby addressing a significant gap in the literature.\n• The survey identifies current challenges and outlines potential directions for future\nresearch in network representations, network characteristics, and TLP methodolo-\ngies, offering valuable guidance for subsequent investigations in this field.\nPapers included in this survey were identified using Web of Science, Scopus, and\nGoogle Scholar and keywords on temporal networks, network representation and link\nprediction. Only articles written in English in the period 2000–2024 that propose\nrelevant algorithms or methods were considered.\nThe rest of this paper is organised as follows. Section 2 presents the background on\ntemporal graph representation and the problem statements. Following this, Section 3\nintroduces the taxonomy framework. Section 4 and Section 5 provide an overview\nof TLP methods, classifying them based on their representation and inference units.\nSection 6 and Section 7 explore variations of the TLP problem and outline potential\ndirections for future research. Finally, Section 8 presents the survey’s conclusion.\n2 Background\n2.1 Temporal Networks\nA temporal network represents interactions between components in a dynamic system\nover time. A dynamic system is composed of elements that interact with each other,\nand these interactions may evolve and change over time.\nA temporal network can be modelled using a temporal graph G = (V, E, T , X) [8],\nwhere V is a set of nodes, and E is a set of edges denoting the interaction between\npairs of nodes. The notation T and X indicate a time domain and a set of nodes’\nattributes, respectively. A temporal network can be defined as G = (V, E, T ) if the\nnodes’ attributes are ignored.\n2.2 Problem Statement\nTemporal Link Prediction (TLP): Given a temporal network G = (V, E, T , X) and a\ncurrent timestamp τ ∈T , TLP aims to predict future edges formed between nodes in\nthe set V after timestamp τ, based on the historical graph preceding τ.\nUnlike traditional link prediction, which focuses primarily on predicting links using\nnode attributes and current topology, TLP requires models capable of capturing com-\nplex temporal and spatial network dynamics. This presents the challenging task of\nforecasting future link formation based on both historical and current network states.\n3\n\n\n2.3 Representation Methods\nTemporal graphs can be described using two distinct approaches concerning the time\ndimension:\n1. Discrete-time Dynamic Graphs (DTDG): A DTDG simplifies a temporal network\ninto a sequence of timestamped snapshots, converting the continuous temporal\nspace into discrete timestamps. This allows dynamic visualization to be transformed\ninto a static, analysable format, though some information may be lost between\nsnapshots during discretisation.\n2. Continuous-time Dynamic Graphs (CTDG): This method abstains from discretiz-\ning the time dimension. Instead, it characterises the presence of all elements inde-\npendently within a continuous temporal space, preserving the inherent temporal\ncontinuity of temporal networks.\n2.3.1 Discrete-time Dynamic Graphs\nFig. 1: Discrete-time Dynamic Graphs (DTDG)\nThe discrete-time dynamic graphs define a temporal network G = (V, E, T , X) as\na sequence of snapshots G = (G1, G2, . . . , G|T |) over a discrete set of timestamps\nT = {1, 2, . . . , |T |} where |·| is the size of the set. Accordingly, the sequence of edges E,\nnodes V and corresponding attributes X can be expressed as E = (E1, E2, . . . , E|T |),\nV = (V1, V2, . . . , V|T |) and X = {X1, X2, . . . , X|T |}. The interval between consecutive\nsnapshots is presumed to be regular.\nEach snapshot at timestamp t can be represented as a tuple Gt = (Vt, Et, Xt),\nwhere Vt = {vt\n1, vt\n2, . . . , vt\nNt} denotes the set of nodes; Nt is the number of nodes at\ntimestamp t. Et = {((vt\ni, vt\nj), wij) | vt\ni, vt\nj ∈Vt, wij ∈R+} denotes the set of edges\nand ((vt\ni, vt\nj), wij) represents the edge (also called link or event) between node vt\ni and\nnode vt\nj and may be associated with a weight wij. An unweighted graph is a special\ncase of a weighted graph with all edges weighting 1. Similarly, the attribute set Xt is\n{ϕ(vt\n1), ϕ(vt\n2), . . . , ϕ(vt\nNt)} and ϕ(·) is the mapping function. Note that the attribute\n4\n\n\nformation is usually a vector xt\ni ∈Rd, and thus Xt becomes an attribute matrix\nXt ∈RNt×d.\nMoreover, the topology of snapshots can be expressed by an adjacency matrix\nset A = {A1, A2, . . . , At, . . . }. Here, At ∈RNt×Nt corresponds to the snapshot Gt.\nFor undirected unweighted graphs, At = {at\nij}Nt×Nt\ni=1,j=1 is a symmetric binary matrix.\nConversely, for undirected weighted graphs, At is the same as the weight matrix whose\nelements are the weight of the edges.\nFigure 1 provides an example of the discrete-time dynamic graphs for a temporal\ngraph. Each timestamp t features a snapshot Gt that depicts the system’s behaviour,\nakin to user interactions in online social networks at a specific time. To minimise\ninformation loss, the interval between two adjacent snapshots should ideally equate to\nthe minimum event duration. However, this can lead to significant data sparsity and\nredundancy in certain scenarios. For instance, if a changing email network is displayed\nas a series of snapshots, many snapshots might be empty or show very few connec-\ntions [16]. Consequently, this method often entails a trade-off between information\nloss and representational efficiency, which needs to be tailored to the specific research\ndemands. The fundamental premise of this approach is to depict temporal networks\nas a sequence of static networks.\n2.3.2 Continuous-time Dynamic Graphs\n(a) Contact sequences\n(b) Interval graph\nFig. 2: Continuous-time Dynamic Graphs (CTDG)\nUnlike discrete-time dynamic graphs, continuous-time dynamic graphs do not\nrequire equal time intervals between timestamps. For a period T ⊆T , a continuous-\ntime dynamic graph G = (V, E, T , X) can be expressed as GT = (VT , ET , T, XT ),\nwhere T = {t1, t2, . . . , t|T | | t1 < t2 < · · · < t|T |} contains all timestamps throughout\nG, VT = {v1, v2, . . . , vNT } represents the nodes and XT = {ϕ(v, t) | v ∈VT , t ∈T}\ndenotes node attributes, with ϕ(·) mapping nodes to their attributes.\n5\n\n\nContinuous-time dynamic graphs employ two principal methodologies to capture\ntemporal network dynamics:\n1. Contact sequences: These record interactions as discrete events at specific\ntimestamps, treating each interaction’s duration as negligible. For instance, in an\nemail network, edges represent instantaneous communication between nodes. This\napproach is also termed a graph stream [16–20]. Formally, the edge set is defined as\nET = {((vi, vj), t, wij) | vi, vj ∈VT , wij ∈R+, t ∈T} ⊆\n\u0000VT\n2\n\u0001\n× T × R+, indicating\nan edge (vi, vj) with weight wij exists at timestamp t. In Figure 2a, timestamps\nt2 and t4 mark when the edge (v3, v4) was observed, with unequal intervals [t1, t2]\nand [t2, t3].\n2. Interval graphs: These account for the duration of interactions [17], with time\nintervals explicitly representing how long connections persist. The edge set is defined\nas ET = {((vi, vj), [ta, tb], w) | vi, vj ∈VT , w ∈R+, ta, tb ∈T, ta < tb} ⊆\n\u0000VT\n2\n\u0001\n×\n\u0000T\n2\n\u0001\n× R+. For example, the edge ((v3, v4), [t3, t4]) in Figure 2b indicates that this\nconnection persists from t3 to t4.\nAlthough continuous-time dynamic graphs present greater challenges than discrete-\ntime dynamic graphs when applying traditional static graph methods, they avoid\nthe critical trade-off between information preservation and computational efficiency\ninherent in discrete-time approaches.\n2.3.3 Notation\nThis survey uses M to represent any temporal variable in temporal networks. The\nnotation M e\ns serves as a shorthand for variable M in a temporal network from times-\ntamp s to timestamp e. Despite differences between discrete-time and continuous-time\ndynamic graphs regarding the temporal variable M, this survey unifies them through\ntimestamp indices.\n1. Discrete-time Dynamic graphs: The sequences of snapshots of the variable M from\ntimestamp s to timestamp e are denoted as M e\ns = (Ms, Ms+1, ..., Me).\n2. Continuous-time Dynamic Graphs: The continuous-time representation of the\nvariable M from timestamp s to timestamp e is denoted as M e\ns .\nIn addition, the rest of this survey uses τ to denote the index of the current timestamp,\n∆as the forthcoming timestamps and ∆′ as the preceding timestamps.\n2.4 Inference Tasks\n2.4.1 Transductive Tasks\nA transductive task (or transductive inference) involves learning and predicting among\nobserved data [21]. In transductive TLP, the model predicts edges between observed\nnodes. Transductive TLP models fTT use the historical graph Gτ\nτ−∆′ from (τ −∆′)\nto τ and node attributes Xτ+∆\nτ−∆′ (if available) from (τ −∆′) to (τ + ∆) as input to\npredict the future graph from τ to (τ + ∆). The inference process of the transductive\n6\n\n\ntask can be represented as:\nˆGτ+∆\nτ\n= fTT(Gτ\nτ−∆′, Xτ+∆\nτ−∆′)\n(1)\nwhere (∆≥1), (τ ≥∆′ ≥0) and ˆGτ+∆\nτ\nis the predicted graph. As the node set V has\nbeen observed during training, their information is encompassed within Gτ\nτ−∆′.\n2.4.2 Inductive Tasks\nAn inductive task (or inductive inference) pertains to the reasoning from observed\ntraining data to generalise to the unseen data [22]. Inductive TLP methods fIT use the\nhistorical graph Gτ\nτ−∆′ from timestamp (τ −∆′) to τ, the node attributes Xτ+∆\nτ−∆′ (if\navailable) from timestamp (τ −∆′) to (τ +∆), and future nodes V τ+∆\nτ\nfrom timestamp\nτ to (τ +∆) as input to predict the future graph ˆGτ+∆\nτ\n, induced by nodes V τ+∆\nτ\n. This\ncan be articulated as:\nˆGτ+∆\nτ\n= fIT(Gτ\nτ−∆′, Xτ+∆\nτ−∆′, V τ+∆\nτ\n)\n(2)\nwhere (∆≥1) and (τ ≥∆′ > 0). TLP can be categorised into one-step tasks (∆= 1)\nand multi-step tasks (∆> 1) based on discrete-time dynamic graphs. Most existing\nmethods primarily address one-step tasks.\nNote that all the methods presented in this survey that can deal with induction\ntasks apply to transductive tasks but the reverse is not necessarily true.\n3 Taxonomy\n3.1 The Framework of the Taxonomy\nInference Unit\nOther\nComponents\nPrediction\nData\nRepresentation\nUnit\nFeatures/\nEmbeddings\n(a) Framework\nSpatial Unit\nInference Unit\nSpatial Unit\nRepresentation Unit\nCombinatorial Framwork\nDiscrete-time Graph\nRepresentation (DTGR)\nTransduction Task\nContinuous-time Graph\nRepresentation (CTGR\nInduction Task\n(b) Overview of the Taxonomy\nFig. 3: The Framework of the Taxonomy\nThis survey proposes a taxonomy framework for TLP. It contends that all TLP\nmethods comprise two sub-tasks: representing temporal networks and making predic-\ntions, which are processed by two distinct components: the Representation Unit (RU)\n7\n\n\nRepresentation Unit\nDiscrete-time Graph\nRepresentation\nContinuous-time Graph\nRepresentation\nFeature Extraction\nEigenvalues\nDecomposition\nTensor\nFactorisation\nPath-based\nNode-based\nGNN-based\nLatent Variables\nLatent Variables\nFramework\nRepresentation\nMethods\nTemporal Walk\nTemporal Point\nProcess with GNN\nRandom Walk\nDynamic Graph\nSummarisation\nAutoencoder\nRBM-based\nSingular Value\nDecomposition\nMatrix Factorisation\nNon-negative Matrix\nFactorisation\nSnapshots\nAdjacency Matrix\nGNN-based\nTraditional GNN\nNeighbour\nSequence\nFig. 4: Taxonomy of Representation Unit\nand the Inference Unit (IU). The representation unit captures the temporal network\nstructure and dynamics, whilst the inference unit leverages these representations to\nforecast future links. As depicted in Figure 3a, nodes, edges, and attributes in tem-\nporal networks must undergo processing via a representation unit to feed them into\nthe model and represent dynamic patterns. Subsequently, the inference units process\nfeatures or embeddings derived from the representation unit, training a model to infer\nfuture links. In [23], transductive and inductive tasks relate to inference units but not\nto graph representation.\nThis survey categorises TLP methods based on their underlying elements, despite\nmany approaches being hybrid, combining different representation and inference units\nor integrating general machine learning strategies. Figure 3a shows each method must\nincorporate at least one representation unit and one inference unit. Since temporal\nnetworks can be modelled as either discrete-time or continuous-time dynamic graphs,\nand TLP tasks can be either transductive or inductive, methodologies are classified\ninto four distinct categories based on these combinations, as illustrated in Figure 3b.\n3.2 Representation Unit\nTemporal networks can be represented as discrete-time or continuous-time dynamic\ngraphs, each of which needs different graph representation capabilities to capture\n8\n\n\nTable 1: Summary of Methods in Representation Unit. Key: GNN: Graph Neural\nNetwork; DGS: Dynamic Graph Summarisation; MF: Matrix Factorisation; RBM:\nRestricted Boltzmann Machine.\nMethods\nRepresentation\nInference\nRepresentation Unit\nNode-based\nDiscrete\nTransductive\nFeature Extraction\nCOMMLP-FULL [24]\nDiscrete\nTransductive\nFeature Extraction\nCAES [25]\nDiscrete\nTransductive\nFeature Extraction\nPath-based\nDiscrete\nTransductive\nFeature Extraction\nDGS [26]\nDiscrete\nTransductive\nDGS\nED [27]\nDiscrete\nTransductive\nMF\nTSVD/CP [28]\nDiscrete\nTransductive\nMF\nLIST [29]\nDiscrete\nTransductive\nMF\nTMF [30]\nDiscrete\nTransductive\nMF\nTBNS [31]\nDiscrete\nTransductive\nMF\nDeepEye [32]\nDiscrete\nTransductive\nMF\nCRJMF [33]\nDiscrete\nTransductive\nMF\nGrNMF [34]\nDiscrete\nTransductive\nMF\nSNMF-FC [35]\nDiscrete\nTransductive\nMF\nAM-NMF [36]\nDiscrete\nTransductive\nMF\nDeepWalk [37]\nDiscrete\nTransductive\nRandom Walk\nNode2Vec [38]\nDiscrete\nTransductive\nRandom Walk\nDynNode2Vec [39]\nDiscrete\nTransductive\nRandom Walk\nSGNE [40]\nDiscrete\nInductive\nRandom Walk\nSDNE [41]\nDiscrete\nTransductive\nAutoencoder\nDyn-VGAE [42]\nDiscrete\nTransductive\nAutoencoder\nDynGEM [43]\nDiscrete\nInductive\nAutoencoder\nctRBM [44]\nDiscrete\nTransductive\nRBM-based\nCTDNE [45]\nContinuous\nTransductive\nTemporal Walk\nGraph WaveNet [46]\nDiscrete\nTransductive\nGNN-based\nSDG [47]\nDiscrete\nTransductive\nGNN-based\nTGN [48]\nContinuous\nTransductive\nGNN-based\nTGGDN [49]\nContinuous\nTransductive\nGNN-based\nCoDyG [50]\nContinuous\nTransductive\nGNN-based\nHTNE [51]\nContinuous\nTransductive\nGNN-based\nMTNE [52]\nContinuous\nTransductive\nGNN-based\nM2DNE [53]\nContinuous\nTransductive\nGNN-based\nDyGCN [54]\nDiscrete\nInductive\nGNN-based\nGCN-MA [55]\nDiscrete\nInductive\nGNN-based\nTGAT [56]\nContinuous\nInductive\nGNN-based\nTREND [57]\nContinuous\nInductive\nGNN-based\nGraphMixer [58]\nContinuous\nInductive\nNeighbour Sequence\nDyGFormer [59]\nContinuous\nInductive\nNeighbour Sequence\nFreeDyG [60]\nContinuous\nInductive\nNeighbour Sequence\n9\n\n\nspatial and temporal information. Representation unit methodologies are used to\nrepresent real-world network data and extract features or latent information.\nThe representation unit fundamentally encapsulates the spatial dynamics within\nthe temporal networks. As illustrated in Figure 4, three principal methodologies are\nidentified for the processing of graph dynamics in TLP:\n1. Snapshots: This approach necessitates directly feeding network snapshots into the\npredictive model. Because each snapshot can be considered as a static network\nand structurally depicted via an adjacency matrix A, the Snapshots technique is\ncommonly employed within discrete-time dynamic graphs.\n2. Features Extraction: This approach entails extracting explicit features from the\nnode, edge, or entire graph levels such as similarity or centrality degree. These\nfeatures are then used as input for the model. Features extraction is often employed\nto gather complex information beyond Snapshots to enhance the performance of\nmodels.\n3. Latent Variable: It represents latent variables that echo underlying factors and\nthey are typically used to reduce the dimensionality and suppress noise within\ntemporal networks. These approaches are commonly used when graph dynamics\nare highly complex and not easily summarised by explicit feats. Discrete-time and\ncontinuous-time dynamic graphs employ distinct approaches. However, all methods\nfor continuous-time dynamic graphs fall under latent variables while discrete-time\ndynamic graphs methods encompass all three types mentioned above.\nTable 1 catalogues the representation unit methods referenced in this survey,\nwhich are examined thoroughly in Section 4. To address TLP, researchers have pro-\nposed various representation units that fall into two categories: task-dependent units\n(specifically designed for TLP) and task-independent units (applicable across diverse\ndownstream tasks in temporal networks). The process of generating latent variables\nthrough these representation units is broadly termed Dynamic Network Embedding\n(DNE) [10, 11, 61]. DNE maps nodes to high-dimensional vector representations whilst\npreserving the dynamic patterns of topology and attributes. Formally, DNE learns a\nfunction fDNE : V 7→Rd, d < N that projects the node set V into a lower-dimensional\nspace.\nIn TLP applications, task-dependent DNE methods are optimised specifically for\nlink prediction, yielding superior performance but limited transferability. In contrast,\ntask-independent DNE methods create versatile embeddings suitable for multiple\ndownstream tasks, including node classification and community detection, though they\nmay deliver comparatively lower performance on TLP tasks.\n3.3 Inference Unit\nModelling temporal dynamics is related to transductive and inductive tasks as shown\nin Figure 5, which demands different abilities to integrate the upstream representation\nand learn temporal patterns for TLP. Inference units are the methods employed to\nprocess graph representations derived from representation units and learn dynamic\npatterns of temporal networks for TLP.\n10\n\n\nInference Unit\nTransduction Task\nInduction Task\nTime Series\nAttention-based\nFramework\nInference\nMethods\nClassification\nDirect Inference\nRNN-based\nFig. 5: Taxonomy of Inference Unit\nTable 2: Summary of Methods in Inference Unit. Key: GNN: Graph Neural Network;\nDGS: Dynamic Graph Summarisation; MF: Matrix Factorisation; RBM: Restricted\nBoltzmann Machine; RNN: Recurrent Neural Network.\nMethods\nRepresentation\nInference\nRepresentation Unit\nInference Unit\nTVRC [26]\nDiscrete\nTransductive\nDGS\nClassification\nGTRBM [62]\nDiscrete\nTransductive\nRBM-based\nClassification\nTSalton [63]\nDiscrete\nTransductive\nNode-based\nTime Series\nSR [64]\nDiscrete\nTransductive\nMF\nTime Series\nTRMF [65]\nDiscrete\nTransductive\nMF\nTime Series\nARIMA–Kalman [66]\nContinuous\nTransductive\nSnapshots\nTime Series\nDDNE [67]\nDiscrete\nTransductive\nSnapshots\nRNN-based\nDynGraph2Vec [68]\nDiscrete\nTransductive\nAutoencoder\nRNN-based\nEvolveGCN [69]\nDiscrete\nTransductive\nGNN-based\nRNN-based\nGCN-GAN [70]\nDiscrete\nTransductive\nGNN-based\nRNN-based\nNetworkGAN [71]\nDiscrete\nTransductive\nGNN-based\nRNN-based\nDGNN [20]\nContinuous\nInductive\nGNN-based\nRNN-based\nCAW-N [72]\nContinuous\nInductive\nTemporal Random Walk\nRNN-based\nGAT [73]\nDiscrete\nInductive\nGNN-based\nAttention-based\nDySAT [74]\nDiscrete\nInductive\nGNN-based\nAttention-based\nASTGCN [75]\nDiscrete\nInductive\nGNN-based\nAttention-based\nSTGSN [76]\nDiscrete\nInductive\nGNN-based\nAttention-based\nNeiDyHNE [77]\nDiscrete\nInductive\nGNN-based\nAttention-based\nMAGNA [78]\nContinuous\nInductive\nGNN-based\nAttention-based\nDyRep [79]\nContinuous\nInductive\nGNN-based\nAttention-based\nDirect inference is a prevalent approach in representation unit applications,\nwhereby embeddings or features from dynamic network embedding methods directly\nyield predictions without requiring additional model training. Techniques such as\nEuclidean Distance or adjacent matrix transformations are commonly employed. The\nmethod’s straightforward nature makes it particularly popular for one-step tasks.\n11\n\n\nMoreover, it is important to highlight that several inference methods, including\nRNN-based and attention-based models, initially designed for inductive tasks, can\neffectively handle transductive tasks. This shows the adaptability of these models and\nsuggests the potential for their broader application in different TLP scenarios. Table 2\nsummarises all the methods of inference units which are elaborated in Section 5\n4 Review of Representation Units for TLP\n4.1 Discrete-time Dynamic Graphs Methods\n4.1.1 Snapshots\nThe Snapshots approach is a fundamental method employed in the domain of discrete-\ntime dynamic graphs for TLP. This technique involves directly inputting network\ntemporal adjacency matrices into the inference unit. The snapshots are treated as a\nsequence of static networks and are structurally represented by adjacency matrices.\nThis method is effective for tasks that resemble time-series inference. The direct use\nof snapshots simplifies the representation and processing of dynamic graphs, making\nit an efficient choice for many application scenarios.\n4.1.2 Feature Extraction\nNode-based Similarity Approaches\nEntities tend to form new connections with those highly similar to themselves. Node\nneighbourhood is a crucial factor in similarity calculation. Building on this natu-\nral observation, researchers have developed numerous neighbour-based methods that\nutilise neighbourhood topological information from discrete snapshots for TLP. In this\nsurvey, Γ(vτ\ni ) represents the set of neighbours of the node vτ\ni , and |Γ(vτ\ni )| denotes the\ndegree of the node vτ\ni (the number of its neighbours).\nCommon Neighbours (CN) [80] It measures the number of shared neighbours of\ntwo nodes vτ\ni and vτ\nj . The higher the number of common neighbours, the higher the\nlikelihood of a link between the nodes:\nCN(vτ\ni , vτ\nj ) ≡|Γ(vτ\ni ) ∩Γ(vτ\nj )|\n(3)\nPreferential Attachment (PA) [81] This index suggests that the likelihood of a\nnew connection between two nodes is proportional to the product of their degrees. It\nis often used in growing scale-free networks:\nPA(vτ\ni , vτ\nj ) ≡|Γ(vτ\ni )| · |Γ(vτ\nj )|\n(4)\nJaccard Coefficient (JC) [82] The JC measures the common neighbours by the\ntotal number of unique neighbours of both nodes:\nJC(vτ\ni , vτ\nj ) ≡|Γ(vτ\ni ) ∩Γ(vτ\nj )|\n|Γ(vτ\ni ) ∪Γ(vτ\nj )|\n(5)\n12\n\n\nSorensen Index (SI) [83] The SI compares the number of common neighbours to\nthe sum of the degrees of both nodes. It is more robust than the Jaccard Coefficient\nagainst outliers:\nSI(vτ\ni , vτ\nj ) ≡2|Γ(vτ\ni ) ∩Γ(vτ\nj )|\n|Γ(vτ\ni )| + |Γ(vτ\nj )|\n(6)\nCosine Similarity (CS) [84] The CS measures the similarity between two nodes by\ncalculating the cosine of the angle between their neighbour vectors:\nCS(vτ\ni , vτ\nj ) ≡\n|Γ(vτ\ni ) ∩Γ(vτ\nj )|\nq\n|Γ(vτ\ni )| · |Γ(vτ\nj )|\n(7)\nAdamic/Adar (AA) [85] AA is based on shared features; it assigns more weight to\ncommon neighbours with a smaller degree value:\nAA(vτ\ni , vτ\nj ) ≡\nX\nvτp∈Γ(vτ\ni )∩Γ(vτ\nj )\n1\nlog |Γ(vτp)|\n(8)\nNode-based similarity approaches typically serve as the foundation for many\ncommunity-based TLP models, such as COMMLP-FULL [24] and CAES [25]. These\nmethods offer significant advantages in terms of interpretability.\nPath-based Similarity Approaches\nPath-based methods provide another perspective for measuring the similarity between\nnodes in a network. These methods concentrate on the length and quantity of paths\nbetween nodes. Unlike node-based approaches, path-based methods view networks as a\nseries of connected paths, enabling the identification of similarities beyond immediate\nneighbourhoods. Some path-based methods also incorporate random Walks to account\nfor the uncertainty and temporal evolution of networks.\nIn the rest of the survey, (M)a,b or (M)a,b,c are denoted as elements in the\nmatrix M or tensor M in the a-th row, b-th column, and c-th depth, respectively. ma\nrepresents the a-th vector of the matrix M.\nShortest Path (SP) [86] The SP converts the shortest path length between nodes\ninto a similarity measure by taking its negative value. This transformation ensures\nthat shorter paths correspond to higher similarity scores, while longer paths indicate\nlower similarity.\nSP(vτ\ni , vτ\nj ) ≡−|d(vτ\ni , vτ\nj )|\n(9)\nwhere d(·) is the shortest path between the node pair (vτ\ni , vτ\nj ) computed by the Dijkstra\nalgorithm [87].\nLocal Path (LP) [88] LP makes use of information on local paths between node\nvτ\ni and node vτ\nj with a 2, 3 and 4-length rather than the nearest paths. LP suggests\nthat 2-length paths should have greater significance than 3-length paths and 3-length\npaths in relation to 4-length paths. To account for this, an adjustment factor α is\napplied.\nLP(vτ\ni , vτ\nj ) ≡A2\nτ + αA3\nτ + α2A4\nτ,\n(10)\n13\n\n\nwhere A2\nτ, A3\nτ and A4\nτ denote adjacency matrices about the nodes having 2, 3 and\n4-length distances at timestamp τ, respectively. The parameter α is close to 0.\nKatz Index (KI) [89] The KI measures similarity based on the number of paths\nof different lengths between two nodes. Shorter paths have larger similarities.\nKI(vτ\ni , vτ\nj ) ≡\n∞\nX\nℓ=1\nβℓ|pathsℓ(vτ\ni , vτ\nj )| =\n ∞\nX\nℓ=1\nβℓAℓ\nτ\n!\ni,j\n= (I −βAτ)−1\ni,j −Ii,j\nwhere pathsℓ(vτ\ni , vτ\nj ) is the set of total ℓ-length paths between nodes vτ\ni and vτ\nj . β is\na damping factor, which constrains the path weights and I is the identity matrix.\nCosine Similarity Time (CST) [90] The CST is a variation of Cosine Similarity:\nCST(vτ\ni , vτ\nj ) ≡\n(L†\nτ)i,j\nq\n(L†\nτ)i,i(L†\nτ)j,j\n(11)\nSimRank (SR) [91] The SR supposes that two nodes are considered similar if\nthey are connected to similar nodes. This method calculates similarity based on the\nprobability that two random walkers, starting from nodes vτ\ni and vτ\nj , will meet at the\nsame node in the future. The walkers move to a random neighbour at each step and\nthe similarity is computed recursively.\nSR(vτ\ni , vτ\nj ) ≡\n( 1\nvτ\ni = vτ\nj\nα ·\nP\nvτp ∈T (vτ\ni )\nP\nvτq ∈T (vτ\nj ) SR(vτ\np,vτ\nq )\n|Γ(vτ\ni )|·|Γ(vτ\nj )|\nvτ\ni ̸= vτ\nj\n(12)\nThese feature extraction approaches perform TLP through direct classification of\nthe extracted features. Additionally, these approaches primarily focus on deriving one-\nstep tasks based solely on the current snapshot Gτ. These methods, while effective in\ncertain scenarios, do not fully exploit the dynamics of temporal networks.\n4.1.3 Latent Variables\nDynamic Graph Summarisation (DGS)\nDGS is primarily a task-independent representation unit used to integrate historical\ngraph snapshots into one comprehensive weighted snapshot. In the context of TLP,\nDGS plays a critical role in synthesizing historical network dynamics. DGS collapses\nsuccessive historical snapshots Gτ\nτ−∆′ into a comprehensive weighted snapshot ¯Gτ\nτ−∆′\nusing some kernel functions to aggregate while ensuring that essential properties of\nthe dynamic topology are preserved. The DGS process is given by:\n¯Gτ\nτ−∆′ ≡ατ−∆′Gτ−∆′ + ατ−∆′+1Gτ−∆′+1 + · · · + ατGτ =\nτ\nX\nt=τ−∆′\nK(Gt; τ, θ)\n(13)\n14\n\n\nThe work in [26, 92] introduced DGS as a representation unit and compared it to\ntraditional classification models, referred to as inference unit. They built the model\nbased on the successive adjacency matrix A.\nThe primary distinction among various DGS methods lies in their kernel func-\ntions, such as the exponential and linear functions. Hill et al. [93] demonstrated DGS\nmethods’ applicability to TLP through direct inference.\nMatrix Factorisation (MF)\nMF, also known as matrix decomposition, decomposes historical adjacency matrices\nAτ\nτ−∆′ or their transformations to extract latent features of temporal networks. These\nfeatures can then be combined with other inference units or make direct inferences of\nfuture snapshots.\nSeveral matrix factorisation techniques have been applied to TLP with considerable\nsuccess. These include Eigenvalue Decomposition (ED), Singular Value Decomposition\n(SVD), Tensor Factorisation (TF), and Non-negative Matrix Factorisation (NMF), all\nof which are frequently used as task-dependent dynamic network embedding methods\nfor TLP.\nEigenvalue Decomposition (ED) ED [94] utilises spectral graph theory to\nmine latent variables for tracking and predicting temporal networks. Spectral analysis\ninvolves the ED of the Laplacian matrix for each snapshot. In graph theory, each eigen-\nvalue can be seen as a specific frequency revealing particular aspects of the graph’s\nstructure, such as its connectivity, robustness, and community structure. For a given\ntimestamp t, the ED can be written as:\nAt = UtΛtU⊤\nt\n(14)\nwhere At is the adjacency matrix of a network, Ut is an orthogonal matrix, and Λt =\ndiag(λt\n1, λt\n2, . . . , λt\nN) is the eigenvalue diagonal matrix at the timestamp t. Here, N\ndenotes the number of nodes in the network, which also corresponds to the dimension\nof the adjacency matrix At. Kunegis et al. [27] studied spectral evolution based on\nED and took advantage of two successive timestamps to solve TLP.\nSingular Value Decomposition (SVD) SVD is a generalised Eigenvalue\nDecomposition (ED) that extends to non-square matrices. SVD is commonly inte-\ngrated with spectral analysis or dynamic graph summarisation to generate embeddings\nfor TLP. For a collapsing adjacency matrix, ¯Aτ\nτ−∆′ of dynamic graph summarisation,\nits SVD form is:\n¯Aτ\nτ−∆′ = UΣV⊤\n(15)\nwhere U and V are orthogonal matrices, and Σ is a diagonal matrix containing the\nsingular values. Typically, U is the target embedding of the historical graph, while V\nand Σ are supplementary matrices.\nTruncated SVD (TSVD) TSVD is a variant of SVD that provides a low-rank\napproximation of the input matrix. Applying a K-rank approximation, the TSVD of\n¯Aτ\nτ−∆′ can be given by:\n¯Aτ\nτ−∆′ ≈UKΣKV⊤\nK\n(16)\n15\n\n\nwhere UK and VK are matrices formed from the first K columns of U and V, respec-\ntively, and ΣK is the principal K × K submatrix of Σ. The work in [28, 95] leveraged\nthe Katz Index [89] based on the collapsing adjacency matrix to generate a similarity\nscore matrix for TLP.\nLIST [29] and TMF [30] both employ SVD to analyse network dynamics. LIST\nintegrates spatial and temporal data for both one-step and multi-step tasks, using\ngradient descent to optimise its inference unit. This methodology is advantageous for\ntemporal networks with diverse structural properties but may introduce significant\ncomputational overhead during the optimisation process. TMF is less computationally\ndemanding, making it more efficient but potentially less effective in networks where\nspatial relationships are key and only suitable for multi-step tasks.\nTensor Factorisation (TF) TF is derived from SVD. The CanDecomp/Parafact\n(CP) decomposition is one of the most important TF methods for TLP [96]. The work\nin [28, 95] examined homogeneous and bipartite graphs, such as user-item networks.\nThese graphs have nodes and edges of the same type, divided into two disjoint sets\nof size m and n (where m + n = N), such that their sum equals the total number of\nnodes, N. They defined the relationship matrix R of size m × n, where each element\n(R)i,j represents the relationship between vi and vj:\n(R)i,j =\n(\n1\nif vi links to vj\n0\notherwise\n(17)\nFor temporal networks, this relationship matrix becomes Rt at timestamp t, and\nextends to a tensor Rm×n×|T | across all snapshots.\nTBNS [31] utilises tensor factorisation as the representation unit, capturing\ntime series trends in network data. TBNS processes network snapshots into a two-\ndimensional matrix using exponential smoothing, enabling effective node similarity\ncalculations for link prediction. However, TBNS generates sparse matrices, which may\nresult in storage inefficiencies.\nNon-negative Matrix Factorisation (NMF) Gao et al. [33] introduced NMF\nfor TLP. Huang et al. [97] showed that NMF decomposes a non-negative matrix such as\nan adjacency matrix At into two non-negative matrices Ut ∈RNt×d\n+\nand Vt ∈RNt×d\n+\n.\nHere d is the dimension of the latent space (d < Nt). Their product approximates the\noriginal matrix:\nAt ≈UtV⊤\nt\n(18)\nwhere Ut is the target embedding matrix. This method is also known as classic bi-\nfactor NMF [97]. It needs to optimise the following Frobenius norm ∥· ∥F of each\nsnapshot:\narg min\nUt≥0,Vt≥0\nJBNMF ≡∥At −UtV⊤\nt ∥2\nF\n(19)\nCRJMF [33] and AM-NMF [36] comprehensively integrate temporal snapshots with\nnode attributes. CRJMF utilises a tri-factorisation method, which is thorough but\ncomputationally intensive. AM-NMF combines matrix factorisation from consecutive\nsnapshots, balancing historical and recent data through a decay factor for TLP.\n16\n\n\nSNMF-FC [35] and DeepEye [32] prioritise computational efficiency by simplifying\nthe modelling process. SNMF-FC focuses on temporal data through a decayed sum-\nmation of feature matrices, potentially overlooking spatial details. DeepEye stabilises\nnetwork dynamics across snapshots with consensus matrices, efficient in networks with\ngradual changes but possibly missing abrupt shifts.\nGrNMF [34] extends SNMF-FC by incorporating graph regularisation to bet-\nter maintain spatial relationships among nodes. This approach enhances the model’s\nability to capture both spatial and temporal information albeit with increased\ncomputational demands.\nIn summary, CRJMF and GrNMF are suitable for detailed spatial-temporal anal-\nysis but require significant computational resources. SNMF-FC and DeepEye provide\nefficient alternatives for less dynamic networks, while AM-NMF’s adaptable frame-\nwork makes it versatile for a variety of network analysis scenarios, balancing detail\nwith computational efficiency.\nRandom Walk\nIn a static network, a classic random walk is a sequence of nodes where each vertex\nis randomly chosen from the neighbours of the current node. The random walk starts\nat an initial vertex and continues by selecting a neighbouring vertex with a certain\ntransition probability and length L to form a walk W = (vt(1)\na\n, vt(2)\nb\n, . . . , vt(L)\nc\n) such\nthat the r-th node vt(r)\ni\n∈Vt and edge (vt(r)\ni\n, vt(r+1)\nj\n) ∈Et.\nThere are static and dynamic approaches for the random walk in discrete-time\ndynamic graphs for TLP. Static random walk methods model each snapshot inde-\npendently to generate embeddings, which are then used by the representation units\nfor TLP. Well-known algorithms that apply this approach include DeepWalk [37] and\nNode2Vec [38, 98]. In contrast, dynamic random walk methods directly model the\ndynamics of the network such as DynNode2Vec [39] and SGNE [40].\nStatic Methods DeepWalk [37] and Node2Vec [38] are influential graph analy-\nsis methods that utilise random walks to learn node embeddings, drawing inspiration\nfrom Skip-gram model [99]. DeepWalk generates node sequences through random walks\nand employs the Skip-gram model to capture higher-order proximity between nodes,\neffectively preserving local connectivity patterns. In contrast, Node2Vec extends Deep-\nWalk’s methodology by introducing biased random walks that balance breadth-first\nand depth-first search [100, 101], enabling the model to capture both local and global\ntopological information. This makes Node2Vec particularly adept at exploring complex\nnetwork structures.\nDe Winter et al. [98] first applied Node2Vec to temporal networks by using\nNode2Vec on each snapshot independently to generate an embedding sequence for\neach node. Then, they utilised traditional classifiers as inference units to make TLP,\nincluding Logistic Regression [102], Random Forests [103], and Gradient Boosted\nRegression Trees [104]. However, these static methods have issues with time-consuming\ncomputations and consistency.\nIn summary, Node2Vec offers a more flexible approach than DeepWalk by allowing\nfor adjustable exploration of a node’s neighbourhood. However, both methods face\n17\n\n\nscalability issues and computational challenges when applied to temporal networks,\nnecessitating considerations of efficiency and dynamic consistency in their application.\nDynamic Methods Du et al. [40] proposed SGNE, extending the Skip-gram\nalgorithm based on DynNode2Vec [39]. While both methods adapt Skip-gram to cap-\nture temporal dynamics in discrete-time representation, SGNE introduces two key\ninnovations: a decomposable loss function for learning representations of new nodes,\nand a selection mechanism that identifies the most influential nodes affected by net-\nwork evolution. The focus on influential nodes potentially enables SGNE to achieve\nhigher accuracy in TLP, particularly in networks where specific nodes drive structural\nchanges.\nAutoencoder\nAutoencoder usually consists of encoders and decoders. The encoder transforms inputs\ninto the latent representation space, whereas the decoder maps these embeddings back\nto obtain reconstructed inputs. Embeddings can be learned by minimising the recon-\nstruction loss function. Nowadays, autoencoders are widely implemented by various\nkinds of neural networks.\nDynGEM [43], SDNE [41], and Dyn-VGAE [42] are three distinct approaches\nto dynamic node embeddings, each utilising variations of autoencoders to adapt to\nnetwork evolution. DynGEM leverages a deep autoencoder, incrementally updating\nembeddings from one snapshot to the next and dynamically adjusting its architecture\nto accommodate growing network sizes. This flexibility in handling network expansion\nis advantageous but may lead to increased complexity in the neural network con-\nfiguration. SDNE [41], a semi-supervised model, reconstructs the adjacency matrix\nto maintain first-order and second-order proximities, which helps preserve local and\nglobal topological features. This dual focus allows SDNE to maintain high fidelity in\nrepresenting node relationships but at the cost of potentially higher computational\ndemands due to the semi-supervised nature of the loss function. Dyn-VGAE [42]\nadvances this by integrating Variational Graph Autoencoders (VGAE) [105] with\nKullback-Leibler divergence [106], optimising embeddings for current snapshots while\nensuring temporal consistency. This approach provides a robust framework for tem-\nporal coherence but requires careful tuning of hyper-parameters to balance immediate\nembedding accuracy with longitudinal consistency. Overall, each model presents a\nunique strategy to handle dynamic networks, with varying degrees of complexity and\nfocus on either architectural flexibility, topological fidelity, or temporal stability.\nRestricted Boltzmann Machine-based Approaches\nThe Restricted Boltzmann Machine (RBM) is a deep learning structure originating\nfrom the Markov random field. It has two layers: a visible layer v ∈RN×1 and a hidden\nlayer h ∈Rd×1. The elements in v and h are stochastic binary variables representing\nobservable data and latent representations [107]. RBM defines a joint distribution over\nv and h through the energy function:\nPr(v, h) =\nexp{v⊤Wh + a⊤v + b⊤h}\nP\nv,h exp{v⊤Wh + a⊤v + b⊤h}\n(20)\n18\n\n\nwhere W ∈RN×d is the weight matrix; the bias vectors a ∈RN×1 and b ∈Rd×1\ncorrespond to v and h respectively. The loss function JRBM aims to minimise the\nnegative log-likelihood:\nmin\nW JRBM(W; v, h) ≡−ln\n X\nh\nPr(v, h)\n!\n(21)\nctRBM [44] can capture complex non-linear variations using an exponentially large\nstate space. ctRBM consists of two separate layers of visible units, the historical layer\n˜v and the predictive layer v, both fully connected to hidden units h. These layers\nreceive inputs from the historical topology Gτ\nτ−∆′+1 and the prediction result ˆGτ+1\n(or the training ground truth), respectively. ctRBM makes a prediction based on\nthe current time window (τ −∆′, τ] and local neighbour. For each node vi, define\n˜v = ((Aτ−∆′+1)i,:, . . . , (Aτ)i,:)⊤∈RN∆′×1 and v = (Aτ+1)⊤\ni,: ∈RN×1. ctRBM uses\ndirect inference to predict the future adjacency matrix ˜Aτ+1:\n( ˆAτ+1)⊤\ni,: = fTT(Gτ\nτ−∆′) ≡Pr(v | h; δ) = σ(Wh + a)\n(22)\nGraph Neural Networks-based Discrete-time Approaches\nGraph neural networks (GNNs) are a type of neural networks that operate on graph-\nstructured data. As one of the most popular models in recent years, GNNs possess\npowerful abilities in graph representation learning [108]. Most GNNs are based on\nthe message-passing mechanism. This powerful mechanism consists of memorizing and\naggregating the messages of nodes in the network. Therefore, GNNs can capture the\ndynamics in the temporal networks and update the embeddings of nodes and edges\naccordingly. Apart from their representation ability, the flexible architectures of GNNs\nhelp them integrate with various other dynamic network embedding methods. Thus,\nGNNs can handle both discrete-time and continuous-time dynamic graphs. Same as\nrandom walks, GNNs could also be divided into static and dynamic methods for\ndiscrete-time dynamic graphs.\nStatic Methods Several GNNs have been proposed and achieved impressive\nresults on static networks such as Graph Convolutional Network (GCN) [109],\nLINE [110] and GraphSAGE [111]. They all generate node embeddings. GCN uses\nconvolutions to aggregate information from node neighbours. LINE preserves first-\norder and second-order proximities between nodes to learn embeddings. GraphSAGE\nis based on GCN and uses a sampling technique to handle large graphs.\nSpatial-temporal GNNs (STGNNs) are a type of GNN that extends traditional\nstatic GNNs to handle temporal networks with spatial information [112, 113]. STGNNs\nconstitute a distinct class of GNNs. These networks model the dynamics by account-\ning for dependencies between connected nodes. STGNNs are widely applied in traffic\nforecasting [114–116], and epidemic prediction [117].\nWu et al. [46] proposed Graph WaveNet by extending the Convolutional Neural\nNetwork (CNN) [118, 119] to an STGNN. The Graph WaveNet consists of a spatial\nconvolution layer and a temporal convolution layer. The spatial convolution layer com-\nbines a diffusion convolution layer [120] with a self-adaptive adjacency matrix, while\n19\n\n\nthe temporal convolution layer adopts a gated version of a dilated causal convolution\nnetwork [121].\nDynamic Methods Dynamic GNNs are directly designed to handle discrete-time\ndynamic graphs. There are also several other dynamic GNNs based on the Graph\nConvolutional Network (GCN). For example, Stochastic Gradient Descent (SGD) [122]\ncombines GCN with PageRank similarity [47, 123, 124].\nDyGCN [54] is a typical task-independent dynamic network embedding method\nbased on GCN to address the challenges of temporal networks. The key of DyGCN\nis to generalise the embedding propagation scheme of GCN to a dynamic setting in\nan efficient manner to update the node embedding matrix Zτ. DyGCN assumes the\nfeature matrix X is fixed and the node embeddings are updated according to the\nchange of aggregated message.\nGCN-MA [55] uses a GCN as the representation unit. GCN-MA includes the\nnovel Node Representation Node Aggregation Effect (NRNAE) algorithm which is a\ncombination of GCN, RNN, multi-head attention mechanisms, enhancing node rep-\nresentation through node degree, clustering coefficient, and neighbour relationships.\nThus, GCN-MA has an improved ability to capture global and local temporal patterns.\n4.1.4 Summary\nDiscrete-time dynamic graph approaches transform temporal networks into sequences\nof static snapshots, enabling the extraction of meaningful representations for TLP.\nThese representations can either be integrated with specialised inference units or\ndirectly applied to prediction tasks. The methods in this category include dynamic\ngraph summarisation, matrix factorisation, and random walk techniques, each cap-\nturing distinct aspects of graph structure and temporal evolution. Neural approaches\nsuch as autoencoders provide efficient data compression, whilst Restricted Boltzmann\nMachines and Graph Neural Networks (GNNs) offer powerful modelling of structural\nand temporal patterns.\n4.2 Continuous-time Dynamic Graphs Methods\n4.2.1 Latent Variables\nGraph Neural Networks-based Continuous-time Approaches\nTraditional GNN TGAT [56] extends the dynamic network embedding ability of\nGAT [73] via Function Time Encoding. GAT is used for static settings and does\nnot consider the temporal dynamics between neighbours. To process continuous-time\ndynamic graphs, the time features used in TGAT are derived from concepts based\non Bochner’s Theorem to map time to Rd. Then, Φ(t) is concatenated with node\nembedding for GAT to solve TLP. However, TGAT cannot maintain the historical\nstate of the nodes.\nTGN [48] can memorise the history information to generate node embeddings for\ncontinuous-time dynamic graphs. The model comprises several modules, including\nMemory, Message Function, Message Aggregator, Memory Updater, and Embedding.\nThe Memory module stores a vector for each node representing the node’s history in\n20\n\n\na compressed format at a certain timestamp. Other modules implement the message-\npassing mechanism to generate temporal node embedding. Therefore, TGN performs\nbetter than the TGAT.\nTGGDN [49] employs a group affinity matrix to model both local and long-distance\ninteractions within networks, incorporating a transformer architecture for temporal\ndata processing and enhanced interpretability. CoDyG [50] introduces a co-attention\nmechanism alongside a temporal difference encoding strategy to effectively capture\nevolving correlations between node pairs over time.\nThese approaches represent distinctive methodologies for modelling temporal\ngraph dynamics, with TGGDN focusing on group-level interactions and broader\nnetwork patterns, whilst CoDyG emphasises the fine-grained temporal evolution of\npairwise node relationships.\nTemporal Point Process with GNN The temporal point process is a probabilis-\ntic generative model for continuous-time event sequences, which involves a stochastic\nprocess whose realization consists of a list of discrete events localised in time [125].\nAssuming an event happens within a tiny period [t, t+dt), the temporal point process\nrepresents the conditional probability of this event given historical events as λ(t)dt.\nThe Hawkes process [126] is a widely-used temporal point process method for TLP.\nThe Hawkes process is described by the equation [127]:\nλ(t) = µ(t) +\nZ t\n−∞\nκ(t −s)dE(s)\n(23)\nwhere the conditional intensity function λ(t) represents the instantaneous rate of event\noccurrence at timestamp t; µ(t) represents the base intensity, which indicates the rate\nat which spontaneous events occur at timestamp t; κ(t −s) is the kernel, modelling\nthe time decay of past events’ influence on the current intensity; E(t) signifies the\nnumber of events up to timestamp t. This process incorporates the self-exciting and\nthe past influence mechanisms of events, allowing it to effectively capture the complex\ndependencies and dynamics in continuous-time event sequences [128, 129].\nZuo et al. [51] introduced HTNE, which captures network evolution by modelling\nhow past interactions influence future connections through an attention mechanism\nthat weighs historical interactions based on temporal proximity.\nBuilding upon this foundation, M2DNE [53] incorporates both micro-dynamics\n(individual interactions) and macro-dynamics (subgraph changes) through a dual-\nattention mechanism that enhances TLP by balancing direct interactions with\noverarching network evolution patterns.\nMTNE [52] addresses limitations in previous approaches by modelling network\ndynamics through triad motif evolution and the Hawkes process. This method captures\nmesoscopic structural patterns neglected by HTNE and M2DNE, which primarily focus\non direct neighbour interactions and broad network changes, respectively.\nTREND [57] combines the Hawkes process with Temporal Graph Networks to\nmodel both event dynamics and node dynamics simultaneously. It characterises each\nedge formation as an event with properties determined by the participating nodes at\nspecific timestamps.\n21\n\n\nThese approaches contribute different methodological perspectives to continuous-\ntime dynamic graph embedding: HTNE focuses on temporal relevance through\nattention mechanisms, M2DNE addresses micro and macro network dynamics, MTNE\ncaptures mesoscopic structural patterns via motif evolution, and TREND combines\nevent and node dynamics. Each method offers distinct techniques for modelling tempo-\nral network relationships, providing various analytical frameworks for understanding\ncomplex network behaviours.\nTemporal Walk\nA temporal walk extends the concept of random walks [37, 38] to continuous-\ntime dynamic graphs. Formally, a temporal walk is defined as a sequence of nodes\nWT = (v(1)\na , v(2)\nb , . . . , v(L)\nc\n) with length L during time domain T, where the r-th node\nv(r) ∈VT , the r-th edge ((v(r), v(r+1)), t(r)) ∈ET , and each timestamp t(r) ∈T. Cru-\ncially, timestamps must follow a temporal ordering (either ascending or descending)\nto properly capture network dynamics.\nUnlike standard random walks, temporal walks feature irregular time intervals\nbetween steps whilst respecting the temporal ordering of connections, making them\nparticularly suitable for continuous-time dynamic graphs.\nCTDNE [45] leverages temporal walks for dynamic network embedding. The\nmethod first generates these walks using edge selection based on uniform, exponential,\nor linear distributions, then applies the Skip-Gram model [130] to learn the resulting\nembeddings. As a task-independent dynamic network embedding approach, CTDNE\nproduces representations applicable across various temporal network tasks.\nNeighbour Sequence\nA Neighbour Sequence is an ordered collection of a node’s interactions with its one-hop\nneighbours over time. Each interaction in the sequence typically includes information\nsuch as the neighbour’s identity and the time of interaction. By restricting attention\nto one-hop neighbours, models can focus on the most relevant local context whilst\nsignificantly reducing computational and storage requirements compared to methods\nthat capture larger neighbourhoods or more intricate structures.\nRecent works demonstrate the growing adoption of this strategy. For instance,\nGraphMixer [58] utilises a multi-layer perceptrons-based link encoder and a mean-\npooling node encoder on the one-hop neighbour sequence, achieving outstanding link\nprediction performance with simpler architectures. DyGFormer [59] employs a neigh-\nbour co-occurrence encoding scheme on neighbour sequences alongside a Transformer\nbased on attention mechanism [131], thereby efficiently capturing long-term dependen-\ncies. Building upon neighbour sequence, FreeDyG [60] integrates frequency encoding\nto further exploit the node interactions, reinforcing the effectiveness of focusing on a\nnode’s immediate neighbourhood whilst unveiling temporal frequency patterns.\n4.2.2 Summary\nGNN-based methods offer computational efficiency and effective retention of his-\ntorical information, whilst temporal walk approaches excel at capturing relative\n22\n\n\npositional relationships between nodes. Consequently, both methodologies can deliver\noutstanding performance.\n5 Review of Inference Units for TLP\n5.1 Transductive Inference Methods\n5.1.1 Direct Inference\nDirect inference is a widely used technique to solve TLP. This approach is characterised\nby its simplicity and directness, allowing the embeddings or features generated through\ndynamic network embedding methods to be utilised immediately without the need for\nfurther training in other inference units. Commonly implemented techniques in this\ncontext include matrix factorisation, dynamic graph summarisation, GTRBM [62],\nand ctRBM [44]. These techniques are valued for their straightforward application in\none-step tasks, making them one of the most popular choices in the field.\n5.1.2 Classification\nTLP can be framed as a binary classification problem, where the aim is to predict the\npresence or absence of a link between two nodes in the future. Almost all the methods\nthat fall into this category rely on supervised learning. These classifiers usually utilise\nfeatures or embeddings obtained from representation units for TLP. Various classifiers\nhave been employed for link prediction including Logistic Regression [102], Support\nVector Machine (SVM) [132], and Decision Trees (DT) [133].\nTVRC [26] combines dynamic graph summarisation as the representation unit with\nWeighted Relational Bayes Classifiers [134] as the inference unit. TVRC takes the\ninputs from dynamic graph summarisation into Weighted Relational Bayes Classifiers\n(RBC) [134] or Weighted Relational Probability Trees (RPT) [135] for TLP. RBC\nextends naive Bayes classifiers and RPT extends standard probability estimation trees\nto a relational setting. Both RBC and RPT incorporate weights from the collapsed\nsnapshots from dynamic graph summarisation to better model the temporal networks.\nGTRBM [62] uses RBM as the representation unit and Gradient Boosting Decision\nTree (GBDT) [136] as the inference unit to form the model. GTRBM used the same\nRBM as ctRBM but the embeddings are input into the GBDT which has better infer-\nence ability than Bayes Classifier. As a result, GTRBM provides better performance\nthan ctRBM which relies on direct inference for TLP.\n5.1.3 Time Series\nTemporal networks can be conceptualised as time-series data, allowing TLP problems\nto be approached through established time-series methodologies. These approaches\nvary based on their applicability to discrete-time or continuous-time dynamic graphs.\nFor discrete-time dynamic graphs\nClassical time series techniques including Auto-Regressive (AR) [137], Auto-\nRegressive Moving Average (ARMA), Auto-Regressive Integrated Moving Average\n23\n\n\n(ARIMA) [138, 139], and Vector Auto-Regressive (VAR) [140], form the backbone of\nmany TLP models when combined with various representation units.\nFeature extraction methods incorporate similarity measures such as Common\nNeighbours, Adamic-Adar and Jaccard Coefficient [80, 82, 85] with time series analy-\nsis. For example, TSalton [63] integrates ARIMA as its inference unit to predict node\ncentrality, thereby enhancing TLP adaptability dynamically.\nMatrix factorisation approaches such as SimRank (SR) [64] and TRMF [65] apply\ntime series principles to dynamic matrix representations. SR combines spectral analysis\nwith ARMA to predict future eigenvectors for graph reconstruction, employing K-rank\napproximation as Equation (16) to address computational challenges in large net-\nworks. Similarly, TRMF pairs Non-negative Matrix Factorisation with AR to predict\ntemporal embeddings, effectively adapting matrix factorisation to dynamic contexts.\nThese methods offer different strengths: feature extraction approaches like TSalton\ndirectly incorporate temporal dynamics into node similarity metrics, whilst matrix\nfactorisation methods capture the structural information. Computational require-\nments vary significantly, with spectral methods requiring dimensional simplifications\nand NMF methods offering more straightforward implementation but potentially less\nstructural detail.\nFor continuous-time dynamic graphs\nContinuous-time methods expand the traditional Discrete-time approaches such as\nAR and ARMA to Continuous-time AR (CAR) [141] and Continuous-time ARMA\n(CARMA) [142]. Moreover, [66] combined ARIMA and Kalman filtering as the\nARIMA-Kalman model for continuous-time dynamic graphs. The ARIMA model is\nemployed to initialise the Kalman measurement and state equations. The model\ncombines the linear pattern-capturing strengths of ARIMA and the adaptive noise\nreduction ability of Kalman filtering. Therefore, ARIMA–Kalman improves the\nflexibility and forecasting accuracy of TLP.\n5.1.4 Summary\nTransductive inference methods employ graph-based representations for specific tasks\nsuch as classification and time series analysis. While these approaches effectively utilise\nstructural and temporal information for predictive modelling, they struggle to gener-\nalise beyond observed samples, highlighting limitations in their predictive capabilities.\nMoreover, most of these inference units are only compatible with representation units\nin discrete-time dynamic graphs.\n5.2 Inductive Inference Methods\n5.2.1 Recurrent Neural Networks-based Approaches\nRecurrent Neural Networks (RNNs) refer to a class of neural networks where connec-\ntions between nodes can form cycles, enabling output from some nodes to influence\nsubsequent input to the same nodes. This allows RNNs to model sequential data and\nprocess the dynamics of temporal networks. RNNs have already succeeded in many\nother tasks such as speech recognition or language translation.\n24\n\n\nHowever, traditional RNNs suffer from gradient descent and gradient explosion\nproblems. Consequently, RNNs may be unable to capture long-range data dynam-\nics dependencies. To overcome these issues, Long Short-Term Memory (LSTM) [143]\nand Gated Recurrent Unit (GRU) [144] networks were developed based on RNNs to\nhandle long-term learning issues. RNNs could also be divided into discrete-time and\ncontinuous-time methods.\nFor discrete-time dynamic graphs\nDDNE [67] uses adjacency matrices to create time-specific embeddings. DDNE\nemploys two Gated Recurrent Units (GRUs) to analyse these embeddings, capturing\nthe network’s evolution over time. This method enables the generation of advanced\nembeddings and dynamic inferences using Multi-Layer Perceptions (MLP). The dual\nGRU – one processing data from past to present, the other in reverse – allows DDNE\nto understand temporal dependencies within networks comprehensively. This facil-\nitates accurate TLPs, showcasing DDNE’s capability to harness network dynamics\neffectively.\nDynGraph2Vec [68] uses an autoencoder as the representation unit and RNN as\nthe inference unit. The method processes adjacency matrices from previous time steps,\nemploying a Multi-Layer Perceptron (MLP) as the encoder to transform these matrices\ninto embeddings. This is followed by an LSTM-based decoder focused on TLP. Dyn-\nGraph2Vec introduces variants such as DynGraph2VecAE, DynGraph2VecAERNN,\nand DynGraph2VecRNN to cater to different requirements and optimisation strategies.\nThe encoding process leverages MLP to convert the series of adjacency matri-\nces over time into a sequence of node embeddings, effectively capturing the graph’s\nevolving structure. The decoder, on the other hand, uses an LSTM layer fed by the\ngenerated embeddings to predict future adjacency matrices as TLP. This approach\nallows DynGraph2Vec to understand and anticipate the dynamic changes in graph\nstructures, facilitating accurate embeddings for temporal networks.\nEvolveGCN [69] utilises a Graph Convolutional Network (GCN) module as the rep-\nresentation unit to learn node embeddings in temporal networks. EvolveGCN employs\na GRU to update the weights of the GCN for inference at each timestamp, allowing the\nmodel to capture the latent dynamic patterns of the temporal networks. For TLP, it\nemploys a Multi-Layer Perceptron (MLP) that uses the final GCN-generated embed-\ndings to predict future connections, which efficiently captures and predicts dynamics\nin temporal networks.\nGCN-GAN [70] leverages GCN as the representation unit and LSTM as the infer-\nence unit under the architecture of Generative Adversarial Network (GAN) [145–147].\nThe GAN contains a generator and a discriminator. The generator contains the rep-\nresentation unit and the inference unit to make TLP, while the discriminator is\nan auxiliary structure to refine prediction results given by the generator. Similarly,\nNetworkGAN [71] improved the GCN-GAN model with matrix factorisation-based\nrepresentation units.\n25\n\n\nFor continuous-time dynamic graphs\nDyGNN [20] tracks the effects of new edges in temporal networks, blending GNN\nand LSTM to generate and update embeddings. This model captures both direct\ninteractions and the influence on adjacent nodes through Continuous-time LSTM. A\nsubsequent LSTM layer, equipped with attention and decay mechanisms, spreads the\nupdates across the network, ensuring dynamic and precise embeddings. DyGNN excels\nin reflecting network changes and improving TLP.\nCAW-N is an anonymised variant of the temporal walk, capturing motif evolution\nin temporal networks. It operates on the premise that nodes with interacting motif\nstructures over time have a higher likelihood of forming links. CAW-N employs tempo-\nral walks as representation units to track events between node motifs chronologically,\nthen utilises RNNs for prediction.\n5.2.2 Attention-based Approaches\nAttention-based models refer to a set of methods that can mimic cognitive attention,\nallowing the models to focus on the important parts of the data while diminishing\nless relevant parts [148]. These models, such as the global or local attention [149],\nself/multi-head attention [131], Simple Neural Attentive Learner (SNAIL) [150] and\nCardinality Preserved Attention (CPA) [78], have gained popularity in many fields\nincluding Computer Vision and Natural Language Processing and have achieved state-\nof-the-art results in various tasks. One well-known model in graph learning is the\nGraph Attention Network (GAT) [73]. GAT uses GNNs as the representation unit to\nprocess the graph structure and attention mechanism to improve the updating process\nof node embeddings.\nFor discrete-time dynamic graphs\nDySAT [74] employs a multi-layer GAT [131] as its representation unit alongside a\nself-attention module for inference. The inductive nature of attention models enables\nDySAT to generalise to previously unseen nodes, contributing to the increasing\npopularity of attention-based approaches in temporal graph analysis.\nBuilding on this foundation, ASTGCN [75] and STGSN [76] further develop\nspatial-temporal modelling techniques. ASTGCN implements distinct attention mod-\nules to capture various temporal patterns through spatial-temporal mechanisms and\nconvolution techniques. Similarly, STGSN utilises temporal attention mechanisms to\nmodel network evolution from both spatial and temporal perspectives, enhancing the\ninterpretability of dynamic interactions.\nNeiDyHNE [77] extends these concepts to heterogeneous networks by integrating\nneighbourhood interactions with node attributes. It combines a hierarchical structure\nattention module for analysing node features with a convolutional temporal attention\nmodule that captures evolutionary patterns. This integrated approach enables effective\nmanagement of complex dynamics in heterogeneous networks, improving predictive\nperformance for future connections.\n26\n\n\nFor continuous-time dynamic graphs\nWhilst many approaches employ continuous-time GNNs with attention mechanisms for\ntransductive tasks, alternative methods can directly embed continuous-time dynamic\ngraphs using attention models, such as those based on the deep learning Hawkes\nprocess described earlier by Equation (23).\nMAGNA [78] extends the attention mechanism from GAT [131] by computing\npreliminary node embeddings using 1-hop attention matrices, then incorporates multi-\nhop neighbours through summed powers of these matrices. The model aggregates\nnode features weighted by attention values and processes them through a feed-forward\nneural network to generate embeddings for TLP.\nDyRep [79] takes a different approach based on temporal point processes, modelling\ntemporal networks through two evolving processes: the association process (capturing\nglobal network growth) and the communication process (representing local informa-\ntion propagation). DyRep employs the Hawkes process as its representation unit and\napplies attention mechanisms to compute edge embeddings for TLP.\n5.2.3 Summary\nThe inference units for inductive tasks in TLP rely primarily on deep learning architec-\ntures, particularly RNN-based and attention-based models capable of generalising to\nunseen data. These architectures offer considerable flexibility, enabling them to either\ndirectly perform TLP after learning dynamic patterns or generate embeddings for\ndownstream components. Their compatibility with both discrete-time and continuous-\ntime dynamic graph representations demonstrates their effectiveness in capturing\ndiverse temporal and structural patterns.\n6 Variations of the TLP Problem\nThis section explores extensions of TLP, from undirected homogeneous temporal net-\nworks to more complex variants, including directed, heterogeneous and hypergraph\ntemporal networks, along with their applications. All network definitions are founded\non discrete-time dynamic graph principles.\nLink Prediction in Directed Temporal Networks\nA directed temporal network features links that evolve over time with specific direc-\ntionality between nodes [151, 152]. In a temporal network G = (V, E, T , X) where\nE ⊆V × V, the edge (vt\ni, vt\nj) indicates a connection from node vt\ni to node vt\nj,\ndistinct from edges in weighted homogeneous attribute temporal networks. These net-\nworks model phenomena including rumour or disease propagation via social networks\n[153, 154], ad hoc message passing [155] and public transport dynamics [156]. Temporal\nKnowledge Graphs represent a particularly significant application.\nTemporal Knowledge Graph\nA Temporal Knowledge Graph is a special type of knowledge graph that incorporates\ntime-varying information into the graph, representing the dynamics of entities and\n27\n\n\nrelationships in the knowledge graph [157]. In Temporal Knowledge Graphs, methods\nfor TLP need to consider both the knowledge and evolution of networks [6, 158].\nBy incorporating time-aware representation learning models, Temporal Knowledge\nGraphs can reason missing temporal facts and relationships [157, 159]. Various models\nhave been proposed for TLP in Temporal Knowledge Graphs [160, 161] or combined\nwith some Natural Language Processing techniques such as contrastive learning [162]\nor temporal logic [162].\nLink Prediction in Heterogeneous Temporal Networks\nA heterogeneous temporal network is a network whose nodes and edges are of dif-\nferent types and change over time [163]. Given a heterogeneous temporal network\nG = (V, E, T ), V = Sn\ni=1 Vi represents n types of nodes, and E = Sm\nj=1 Ej denotes m\ntypes of edges. Examples of heterogeneous temporal networks include complex social\nnetworks [164] and recommendation systems [165].\nCollective Link Prediction refers to a set of methods used for predicting the\nprobability of new relationships forming between nodes in heterogeneous temporal\nnetworks [166, 167], and it is often based on attention models [168–170].\nLink Prediction in Temporal Hypergraph Networks\nA temporal hypergraph network is a network whose edges can join any number of nodes\nand changes over time [171]. Temporal hypergraph networks can represent and analyse\ncomplex real-world systems such as economic transactions or transportation [172].\nRecently, they have drawn attention and were studied on the TLP problem achieving\ngood performance based on GNNs and attention-based models [173, 174].\n7 Future Research Directions\nTLP represents a vital area of research within complex network science and graph\nrepresentation learning. This section explores emerging challenges and unresolved\nquestions in the field.\nBuilding new models for TLP based on different representation units and\ninference units\nIn light of the novel framework introduced by this survey, which is characterised by its\ncomposite nature, there is a compelling argument for the development of new models\nfor TLP that harnesses the potential of innovative combinations. This framework, by\nits ability to amalgamate diverse techniques and methodologies, opens up the possi-\nbilities for tackling future TLP challenges in novel and efficacious ways. The essence\nof this approach lies in its flexibility and adaptability, encouraging the exploration of\nuncharted territories within the domain of TLP. By integrating different representa-\ntion units and inference units, this composite framework sets the stage for a holistic\nand nuanced understanding of network dynamics. As such, not only does it broaden\nthe horizon for TLP research but also serves as a beacon for future investigations,\nguiding them towards developing solutions that are both innovative and tailored to\nthe complicated temporal network evolution.\n28\n\n\nEnhancing Explainability in TLP Models\nA pivotal future direction emerges in the enhancement of model explainability. This\ninitiative is critical for understanding what is viewed as the black box of TLP models,\nelevating their trustworthiness and deepening the comprehension of the mechanisms\nand dynamics in temporal networks. This direction does not merely aim to refine the\npredictive ability of TLP models but seeks to illuminate the underlying dynamics that\ncharacterise temporal networks. Therefore, this push for enhanced interpretability and\nexplainability represents not just a trend towards innovation and integration within\nTLP frameworks but also emerges as a new direction aimed at developing models\nthat balance explainability with capability. This direction may be one of the crucial\nfoundations for future practical applications.\nStudying TLP on continuous-time dynamic graphs in complex networks.\nTLP has been a classical task on weighted homogeneous attribute temporal networks\nfor a long time. However, real-world temporal networks are often much more com-\nplex. They can be represented as directed, multi-layer, heterogeneous, or hypergraph\nnetworks. These complex networks pose challenges in TLP as their structure and evo-\nlution patterns are more intricate than the weighted homogeneous attribute temporal\nnetworks. Hence, this field remains an open challenge and requires novel techniques\nfor such highly complex networks.\nFurther Exploration of the Graph Dynamics Mechanism.\nRecent studies have examined mechanisms including Micro- and Macro-dynamics [53]\nand Motif structures [52, 175–177]. Nevertheless, a deeper understanding of complex\nnetwork dynamics and how these mechanisms influence temporal network proper-\nties, such as varying speeds, remains elusive. Additionally, developing innovative\napproaches that leverage these mechanisms to more effectively capture evolution\npatterns in temporal networks is crucial.\nGeneral TLP Models in Large-scale Complex Networks.\nIn recent years, the volume of data has been growing exponentially and the devel-\nopment of large models has been progressing rapidly [178, 179]. For instance, social\nnetworks are becoming increasingly complex, encompassing various data types. Con-\nsequently, constructing a general and large-scale graph prediction model based on\ncomplex networks is becoming more and more promising. Such a model would have\na strong ability in multi-task scenarios with domain knowledge, including recommen-\ndation systems, knowledge reasoning, epidemic spreading, etc. Future research in this\narea should focus on designing scalable and robust TLP models capable of processing\nlarge-scale and complex datasets and combining few-shot learning, transfer learning,\nand other advanced techniques to improve performance.\n8 Conclusion\nThis survey has introduced a novel taxonomy for Temporal Link Prediction (TLP) that\ndistinguishes between representation units and inference units, providing a structured\n29\n\n\nframework for analysing existing approaches. Through this lens, we have system-\natically reviewed and classified the literature, revealing the diverse methodological\ncombinations employed across the field. The survey has also examined advanced TLP\napplications in directed, heterogeneous and hypergraph temporal networks, whilst\nidentifying promising research directions including model explainability, complex\nnetwork dynamics, and scalable solutions for large-scale networks.\nThe taxonomy framework presented here offers significant value to researchers by\nclarifying the fundamental components of TLP methods and highlighting unexplored\ncombinations that could yield performance improvements. By emphasising the sepa-\nration between representation and inference mechanisms, this work facilitates more\ntargeted methodological innovations. Furthermore, our examination of the evolution\nfrom traditional feature-based approaches to advanced neural architectures provides a\ncomprehensive roadmap for the field’s development. We anticipate that this structured\nperspective will stimulate new research directions and accelerate progress towards\nmore effective, interpretable and versatile TLP models capable of addressing the\ncomplex challenges inherent in temporal network analysis.\nDeclaration\nThe authors have no competing interests to declare that are relevant to the content\nof this article.\nReferences\n[1] Wang, P., Xu, B., Wu, Y., Zhou, X.: Link Prediction in Social Networks: the\nState-of-the-Art. CoRR abs/1411.5118 (2014)\n[2] Borgnat, P., Dewaele, G., Fukuda, K., Abry, P., Cho, K.: Seven Years and One\nDay: Sketching the Evolution of Internet Traffic. In: IEEE INFOCOM 2009,\npp. 711–719. IEEE, Rio de Janeiro, Brazil (2009). https://doi.org/10.1109/\nINFCOM.2009.5061979\n[3] Vinchoff, C., Chung, N., Gordon, T., Lyford, L., Aibin, M.: Traffic Predic-\ntion in Optical Networks Using Graph Convolutional Generative Adversarial\nNetworks. In: 2020 22nd International Conference on Transparent Optical Net-\nworks (ICTON), pp. 1–4. IEEE, Bari, Italy (2020). https://doi.org/10.1109/\nICTON51198.2020.9203477\n[4] Liu, C., Xiao, Z., Wang, D., Cheng, M., Chen, H., Cai, J.: Foreseeing private car\ntransfer between urban regions with multiple graph-based generative adversarial\nnetworks. World Wide Web 25(6), 2515–2534 (2022) https://doi.org/10.1007/\ns11280-021-00995-z\n[5] Nickel, M., Murphy, K., Tresp, V., Gabrilovich, E.: A review of relational\nmachine learning for knowledge graphs. Proceedings of the IEEE 104(1), 11–33\n(2015)\n30\n\n\n[6] Wang, M., Qiu, L., Wang, X.: A survey on knowledge graph embeddings for link\nprediction. Symmetry 13(3), 485 (2021)\n[7] L¨u, L., Zhou, T.: Link prediction in complex networks: A survey. Physica A:\nStatistical Mechanics and its Applications 390(6), 1150–1170 (2011) https://\ndoi.org/10.1016/j.physa.2010.11.027\n[8] Holme, P., Saram¨aki, J. (eds.): Temporal Network Theory. Computational Social\nSciences. Springer, Cham (2023). https://doi.org/10.1007/978-3-031-30399-9\n[9] Kumar, A., Singh, S.S., Singh, K., Biswas, B.: Link prediction techniques, appli-\ncations, and performance: A survey. Physica A: Statistical Mechanics and its\nApplications 553, 124289 (2020)\n[10] Qin, M., Yeung, D.-Y.: Temporal Link Prediction: A Unified Framework,\nTaxonomy, and Review. arXiv preprint arXiv:2210.08765 (2022)\n[11] Xue, G., Zhong, M., Li, J., Chen, J., Zhai, C., Kong, R.: Dynamic network\nembedding survey. Neurocomputing 472, 212–223 (2022) https://doi.org/10.\n1016/j.neucom.2021.03.138\n[12] Ghorbanzadeh, H., Sheikhahmadi, A., Jalili, M., Sulaimany, S.: A hybrid method\nof link prediction in directed graphs. Expert Systems with Applications 165,\n113896 (2021) https://doi.org/10.1016/j.eswa.2020.113896\n[13] Tian, L., Zhou, X., Wu, Y.-P., Zhou, W.-T., Zhang, J.-H., Zhang, T.-S.: Knowl-\nedge graph and knowledge reasoning: A systematic review. Journal of Electronic\nScience and Technology 20(2), 100159 (2022) https://doi.org/10.1016/j.jnlest.\n2022.100159\n[14] Haghani, S., Keyvanpour, M.R.: A systemic analysis of link prediction in social\nnetwork. Artificial Intelligence Review 52, 1961–1995 (2019)\n[15] Daud, N.N., Ab Hamid, S.H., Saadoon, M., Sahran, F., Anuar, N.B.: Applica-\ntions of link prediction in social networks: A review. Journal of Network and\nComputer Applications 166, 102716 (2020) https://doi.org/10.1016/j.jnca.2020.\n102716\n[16] Holme, P.: Modern temporal network theory: a colloquium. The European Phys-\nical Journal B 88(9), 234 (2015) https://doi.org/10.1140/epjb/e2015-60657-4\n[17] Divakaran, A., Mohan, A.: Temporal link prediction: A survey. New Generation\nComputing 38, 213–258 (2020)\n[18] McGregor, A.: Graph stream algorithms: a survey. Sigmod Record 43(1), 9–20\n(2014) https://doi.org/10.1145/2627692.2627694 . Place: New York, NY, USA\nPublisher: Association for Computing Machinery tex.issue date: March 2014\n31\n\n\n[19] Yu, W., Cheng, W., Aggarwal, C.C., Zhang, K., Chen, H., Wang, W.: NetWalk:\nA Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Net-\nworks. In: Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining. KDD ’18, pp. 2672–2681. Association for\nComputing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/\n3219819.3220024\n[20] Ma, Y., Guo, Z., Ren, Z., Tang, J., Yin, D.: Streaming graph neural networks.\nIn: Proceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, pp. 719–728 (2020)\n[21] Vapnik, V.N.: An overview of statistical learning theory. IEEE transactions on\nneural networks 10(5), 988–999 (1999)\n[22] Mitchell, T.: The Need for Biases in Learning Generalizations. Technical Report\nCBM-TR-117, Rutgers University (1980)\n[23] Srinivasan, B., Ribeiro, B.: On the equivalence between positional node embed-\ndings and structural graph representations. arXiv preprint arXiv:1910.00452\n(2019)\n[24] Kumar, M., Mishra, S., Singh, S.S., Biswas, B.: Community-enhanced Link\nPrediction in Dynamic Networks. ACM Trans. Web 18(2), 24–12432 (2024)\nhttps://doi.org/10.1145/3580513\n[25] Choudhury, N.: Community-Aware Evolution Similarity for Link Prediction in\nDynamic Social Networks. Mathematics 12(2), 285 (2024) https://doi.org/10.\n3390/math12020285\n[26] Sharan, U., Neville, J.: Temporal-Relational Classifiers for Prediction in Evolv-\ning Domains. In: 2008 Eighth IEEE International Conference on Data Mining,\npp. 540–549. IEEE, Pisa, Italy (2008). https://doi.org/10.1109/ICDM.2008.125\n[27] Kunegis, J., Fay, D., Bauckhage, C.: Network growth and the spectral evolution\nmodel. In: Proceedings of the 19th ACM International Conference on Infor-\nmation and Knowledge Management. CIKM ’10, pp. 739–748. Association for\nComputing Machinery, New York, NY, USA (2010). https://doi.org/10.1145/\n1871437.1871533\n[28] Acar, E., Dunlavy, D.M., Kolda, T.G.: Link Prediction on Evolving Data Using\nMatrix and Tensor Factorizations. In: 2009 IEEE International Conference on\nData Mining Workshops, pp. 262–269. IEEE, Miami, FL, USA (2009). https:\n//doi.org/10.1109/ICDMW.2009.54\n[29] Yu, W., Cheng, W., Aggarwal, C.C., Chen, H., Wang, W.: Link prediction with\nspatial and temporal consistency in dynamic networks. In: IJCAI, pp. 3343–3349\n(2017)\n32\n\n\n[30] Yu, W., Aggarwal, C.C., Wang, W.: Temporally Factorized Network Model-\ning for Evolutionary Network Analysis. In: Proceedings of the Tenth ACM\nInternational Conference on Web Search And Data Mining. WSDM ’17, pp.\n455–464. Association for Computing Machinery, New York, NY, USA (2017).\nhttps://doi.org/10.1145/3018661.3018669\n[31] Yang, X., Tian, Z., Cui, H., Zhang, Z.: Link prediction on evolving network\nusing tensor-based node similarity. In: 2012 IEEE 2nd International Conference\non Cloud Computing and Intelligence Systems, vol. 01, pp. 154–158 (2012).\nhttps://doi.org/10.1109/CCIS.2012.6664387\n[32] Ahmed, N.M., Chen, L., Wang, Y., Li, B., Li, Y., Liu, W.: DeepEye: link predic-\ntion in dynamic networks based on non-negative matrix factorization. Big Data\nMining and Analytics 1(1), 19–33 (2018)\n[33] Gao, S., Denoyer, L., Gallinari, P.: Temporal link prediction by integrating con-\ntent and structure information. In: Proceedings of the 20th ACM International\nConference on Information and Knowledge Management, pp. 1169–1174. ACM,\nGlasgow Scotland, UK (2011). https://doi.org/10.1145/2063576.2063744\n[34] Ma, X., Sun, P., Wang, Y.: Graph regularized nonnegative matrix factoriza-\ntion for temporal link prediction in dynamic networks. Physica A: Statistical\nMechanics and its Applications 496, 121–136 (2018) https://doi.org/10.1016/j.\nphysa.2017.12.092\n[35] Ma, X., Sun, P., Qin, G.: Nonnegative matrix factorization algorithms for\nlink prediction in temporal networks using graph communicability. Pattern\nRecognition 71, 361–374 (2017) https://doi.org/10.1016/j.patcog.2017.06.025\n[36] Lei, K., Qin, M., Bai, B., Zhang, G.: Adaptive Multiple Non-negative Matrix\nFactorization for Temporal Link Prediction in Dynamic Networks. In: Pro-\nceedings of the 2018 Workshop on Network Meets AI & ML. NetAI’18, pp.\n28–34. Association for Computing Machinery, New York, NY, USA (2018).\nhttps://doi.org/10.1145/3229543.3229546\n[37] Perozzi, B., Al-Rfou, R., Skiena, S.: DeepWalk: online learning of social represen-\ntations. In: Proceedings of the 20th ACM SIGKDD International Conference on\nKnowledge Discovery and Data mining. KDD ’14, pp. 701–710. Association for\nComputing Machinery, New York, NY, USA (2014). https://doi.org/10.1145/\n2623330.2623732\n[38] Grover, A., Leskovec, J.: node2vec: Scalable feature learning for networks. In:\nProceedings of the 22nd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data mining, pp. 855–864 (2016)\n[39] Mahdavi, S., Khoshraftar, S., An, A.: dynnode2vec: Scalable Dynamic Network\nEmbedding. In: 2018 IEEE International Conference on Big Data (Big Data),\n33\n\n\npp. 3762–3765 (2018). https://doi.org/10.1109/BigData.2018.8621910\n[40] Du, L., Wang, Y., Song, G., Lu, Z., Wang, J.: Dynamic Network Embedding:\nAn Extended Approach for Skip-gram based Network Embedding. In: Pro-\nceedings of the Twenty-Seventh International Joint Conference on Artificial\nIntelligence, pp. 2086–2092. International Joint Conferences on Artificial Intelli-\ngence Organization, Stockholm, Sweden (2018). https://doi.org/10.24963/ijcai.\n2018/288\n[41] Wang, D., Cui, P., Zhu, W.: Structural Deep Network Embedding. In: Pro-\nceedings of the 22nd ACM SIGKDD International Conference on Knowledge\nDiscovery And Data Mining. KDD ’16, pp. 1225–1234. Association for Comput-\ning Machinery, New York, NY, USA (2016). https://doi.org/10.1145/2939672.\n2939753\n[42] Mahdavi, S., Khoshraftar, S., An, A.: Dynamic Joint Variational Graph Autoen-\ncoders. In: Cellier, P., Driessens, K. (eds.) Machine Learning and Knowledge Dis-\ncovery in Databases. Communications in Computer and Information Science, pp.\n385–401. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-43823-4\n32\n[43] Goyal, P., Kamra, N., He, X., Liu, Y.: DynGEM: Deep Embedding Method for\nDynamic Graphs. arXiv. Version Number: 1 (2018). https://doi.org/10.48550/\nARXIV.1805.11273\n[44] Li, X., Du, N., Li, H., Li, K., Gao, J., Zhang, A.: A Deep Learning Approach\nto Link Prediction in Dynamic Networks. In: Proceedings of the 2014 SIAM\nInternational Conference on Data Mining (SDM), pp. 289–297. https://doi.org/\n10.1137/1.9781611973440.33\n[45] Nguyen, G.H., Lee, J.B., Rossi, R.A., Ahmed, N.K., Koh, E., Kim, S.:\nContinuous-Time Dynamic Network Embeddings. In: Companion Proceedings\nof the The Web Conference 2018. WWW ’18, pp. 969–976. International World\nWide Web Conferences Steering Committee, Republic and Canton of Geneva,\nCHE (2018). https://doi.org/10.1145/3184558.3191526\n[46] Wu, Z., Pan, S., Long, G., Jiang, J., Zhang, C.: Graph wavenet for deep spatial-\ntemporal graph modeling. arXiv preprint arXiv:1906.00121 (2019)\n[47] Fu, D., He, J.: SDG: a simplified and dynamic graph neural network. In: Pro-\nceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pp. 2273–2277 (2021)\n[48] Rossi, E., Chamberlain, B., Frasca, F., Eynard, D., Monti, F., Bronstein, M.:\nTemporal Graph Networks for Deep Learning on Dynamic Graphs. arXiv (2020).\nhttp://arxiv.org/abs/2006.10637\n34\n\n\n[49] Huang, D., Lei, F.: Temporal group-aware graph diffusion networks for dynamic\nlink prediction. Information Processing & Management 60(3), 103292 (2023)\nhttps://doi.org/10.1016/j.ipm.2023.103292\n[50] Chen, J., Pan, Z., Chen, H.: Correlation-enhanced Dynamic Graph Learning for\nTemporal Link Prediction. In: 2024 IEEE International Conference on Evolving\nand Adaptive Intelligent Systems (EAIS), pp. 1–7 (2024). https://doi.org/10.\n1109/EAIS58494.2024.10570036\n[51] Zuo, Y., Liu, G., Lin, H., Guo, J., Hu, X., Wu, J.: Embedding temporal network\nvia neighborhood formation. In: Proceedings of the 24th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery & Data Mining, pp. 2857–2866\n(2018)\n[52] Huang, H., Fang, Z., Wang, X., Miao, Y., Jin, H.: Motif-Preserving Temporal\nNetwork Embedding. In: IJCAI, pp. 1237–1243 (2020)\n[53] Lu, Y., Wang, X., Shi, C., Yu, P.S., Ye, Y.: Temporal network embedding\nwith micro-and macro-dynamics. In: Proceedings of the 28th ACM International\nConference on Information and Knowledge Management, pp. 469–478 (2019)\n[54] Cui, Z., Li, Z., Wu, S., Zhang, X., Liu, Q., Wang, L., Ai, M.: DyGCN: Effi-\ncient Dynamic Graph Embedding With Graph Convolutional Network. IEEE\nTransactions on Neural Networks and Learning Systems (2022)\n[55] Mei, P., Zhao, Y.h.: Dynamic network link prediction with node representation\nlearning from graph convolutional networks. Scientific Reports 14(1), 538 (2024)\nhttps://doi.org/10.1038/s41598-023-50977-6\n[56] Xu, D., Ruan, C., Korpeoglu, E., Kumar, S., Achan, K.: Inductive representation\nlearning on temporal graphs. arXiv: 2002.07962 [cs.LG] (2020). https://arxiv.\norg/abs/2002.07962\n[57] Wen, Z., Fang, Y.: TREND: TempoRal Event and Node Dynamics for Graph\nRepresentation Learning. In: Proceedings of the ACM Web Conference 2022.\nWWW ’22, pp. 1159–1169. Association for Computing Machinery, New York,\nNY, USA (2022). https://doi.org/10.1145/3485447.3512164\n[58] Cong, W., Zhang, S., Kang, J., Yuan, B., Wu, H., Zhou, X., Tong, H., Mah-\ndavi, M.: Do We Really Need Complicated Model Architectures For Temporal\nNetworks? arXiv (2023). https://doi.org/10.48550/arXiv.2302.11636\n[59] Yu, L., Sun, L., Du, B., Lv, W.: Towards better dynamic graph learning: new\narchitecture and unified library. In: Proceedings of the 37th International Con-\nference on Neural Information Processing Systems. NIPS ’23. Curran Associates\nInc., Red Hook, NY, USA (2023). event-place: New Orleans, LA, USA\n35\n\n\n[60] Tian, Y., Qi, Y., Guo, F.: FreeDyG: Frequency Enhanced Continuous-Time\nDynamic Graph Model for Link Prediction. In: The Twelfth International Con-\nference on Learning Representations (2024). https://openreview.net/forum?id=\n82Mc5ilInM\n[61] Barros, C.D.T., Mendon¸ca, M.R.F., Vieira, A.B., Ziviani, A.: A survey on\nembedding dynamic graphs. ACM Computing Surveys (CSUR) 55(1), 1–37\n(2021)\n[62] Li, T., Wang, B., Jiang, Y., Zhang, Y., Yan, Y.: Restricted Boltzmann Machine-\nBased Approaches for Link Prediction in Dynamic Networks. IEEE Access 6,\n29940–29951 (2018) https://doi.org/10.1109/ACCESS.2018.2840054\n[63] Zhang, T., Zhang, K., Lv, L., Li, X.: Temporal link prediction using node\ncentrality and time series. Int J Fut Comput Commun 9(3), 62–65 (2020)\n[64] Fang, C., Lu, J., Ralescu, A.: Graph spectra regression with low-rank approxi-\nmation for dynamic graph link prediction. In: NIPS2010 Workshop on Low-rank\nMethods for Large-scale Machine Learning, Vancouver, Canada (2010)\n[65] Yu, H.-F., Rao, N., Dhillon, I.S.: Temporal regularized matrix factorization\nfor high-dimensional time series prediction. Advances in neural information\nprocessing systems 29 (2016)\n[66] Xu, D.-w., Wang, Y.-d., Jia, L.-m., Qin, Y., Dong, H.-h.: Real-time road traffic\nstate prediction based on ARIMA and Kalman filter. Frontiers of Information\nTechnology & Electronic Engineering 18(2), 287–302 (2017) https://doi.org/10.\n1631/FITEE.1500381\n[67] Li, T., Zhang, J., Yu, P.S., Zhang, Y., Yan, Y.: Deep Dynamic Network Embed-\nding for Link Prediction. IEEE Access 6, 29219–29230 (2018) https://doi.org/\n10.1109/ACCESS.2018.2839770\n[68] Goyal, P., Chhetri, S.R., Canedo, A.: dyngraph2vec: Capturing network dynam-\nics using dynamic graph representation learning. Knowledge-Based Systems 187,\n104816 (2020) https://doi.org/10.1016/j.knosys.2019.06.024\n[69] Pareja, A., Domeniconi, G., Chen, J., Ma, T., Suzumura, T., Kanezashi, H.,\nKaler, T., Schardl, T., Leiserson, C.: Evolvegcn: Evolving graph convolutional\nnetworks for dynamic graphs. In: Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 34, pp. 5363–5370 (2020)\n[70] Lei, K., Qin, M., Bai, B., Zhang, G., Yang, M.: GCN-GAN: A Non-linear\nTemporal Link Prediction Model for Weighted Dynamic Networks. In: IEEE\nINFOCOM 2019 - IEEE Conference on Computer Communications, pp. 388–396\n(2019). https://doi.org/10.1109/INFOCOM.2019.8737631\n36\n\n\n[71] Yang, M., Liu, J., Chen, L., Zhao, Z., Chen, X., Shen, Y.: An Advanced Deep\nGenerative Framework for Temporal Link Prediction in Dynamic Networks.\nIEEE Transactions on Cybernetics 50(12), 4946–4957 (2020) https://doi.org/\n10.1109/TCYB.2019.2920268\n[72] Wang, Y., Chang, Y.-Y., Liu, Y., Leskovec, J., Li, P.: Inductive Represen-\ntation Learning in Temporal Networks via Causal Anonymous Walks. arXiv.\narXiv:2101.05974 [cs] (2022). https://doi.org/10.48550/arXiv.2101.05974\n[73] Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., Bengio, Y.:\nGraph Attention Networks. arXiv (2018). https://doi.org/10.48550/arXiv.1710.\n10903\n[74] Sankar, A., Wu, Y., Gou, L., Zhang, W., Yang, H.: DySAT: Deep Neural\nRepresentation Learning on Dynamic Graphs via Self-Attention Networks. In:\nProceedings of the 13th International Conference on Web Search and Data Min-\ning. WSDM’20, pp. 519–527. Association for Computing Machinery, New York,\nNY, USA (2020). https://doi.org/10.1145/3336191.3371845\n[75] Guo, S., Lin, Y., Feng, N., Song, C., Wan, H.: Attention based spatial-temporal\ngraph convolutional networks for traffic flow forecasting. In: Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 33, pp. 922–929 (2019)\n[76] Min, S., Gao, Z., Peng, J., Wang, L., Qin, K., Fang, B.: STGSN — A\nSpatial–Temporal Graph Neural Network framework for time-evolving social\nnetworks. Knowledge-Based Systems 214, 106746 (2021) https://doi.org/10.\n1016/j.knosys.2021.106746\n[77] Wei, X., Wang, W., Zhang, C., Ding, W., Wang, B., Qian, Y., Han, Z., Su,\nC.: Neighbor-enhanced Representation Learning for Link Prediction in Dynamic\nHeterogeneous Attributed Networks. ACM Trans. Knowl. Discov. Data (2024)\nhttps://doi.org/10.1145/3676559\n[78] Wang, G., Ying, R., Huang, J., Leskovec, J.: Multi-hop attention graph neural\nnetwork. arXiv preprint arXiv:2009.14332 (2020)\n[79] Trivedi, R., Farajtabar, M., Biswal, P., Zha, H.: Dyrep: Learning representations\nover dynamic graphs. In: International Conference on Learning Representations\n(2019)\n[80] Newman, M.E.J.: Clustering and preferential attachment in growing networks.\nPhys. Rev. E 64, 025102 (2001) https://doi.org/10.1103/PhysRevE.64.025102\n[81] Barab´asi, A.L., Jeong, H., N´eda, Z., Ravasz, E., Schubert, A., Vicsek, T.: Evo-\nlution of the social network of scientific collaborations. Physica A: Statistical\nMechanics and its Applications 311(3), 590–614 (2002) https://doi.org/10.1016/\nS0378-4371(02)00736-7\n37\n\n\n[82] Goodall, D.W.: Sample similarity and species correlation. In: Whittaker, R.H.\n(ed.) Ordination of Plant Communities, pp. 99–149. Springer, Dordrecht (1978).\nhttps://doi.org/10.1007/978-94-009-7989-5 5\n[83] Sørensen, T., Sørensen, T., Biering-Sørensen, T., Sørensen, T., Sorensen, J.T.: A\nmethod of establishing group of equal amplitude in plant sociobiology based on\nsimilarity of species content and its application to analyses of the vegetation on\nDanish commons. (1948). https://api.semanticscholar.org/CorpusID:135206594\n[84] Salton, G., Wong, A., Yang, C.S.: A vector space model for automatic index-\ning. Communications of The Acm 18(11), 613–620 (1975) https://doi.org/10.\n1145/361219.361220 . Place: New York, NY, USA Publisher: Association for\nComputing Machinery tex.issue date: Nov. 1975\n[85] Adamic, L.A., Adar, E.: Friends and neighbors on the Web. Social Networks\n25(3), 211–230 (2003) https://doi.org/10.1016/S0378-8733(03)00009-1\n[86] Liben-Nowell, D., Kleinberg, J.: The link prediction problem for social net-\nworks. In: Proceedings of the 12th International Conference on Information and\nKnowledge Management. CIKM’03, pp. 556–559. Association for Computing\nMachinery, New York, NY, USA (2003). https://doi.org/10.1145/956863.956972\n[87] Dijkstra, E.W.: A note on two problems in connexion with graphs. Numerische\nMathematik 1(1), 269–271 (1959) https://doi.org/10.1007/BF01386390\n[88] L¨u, L., Jin, C.-H., Zhou, T.: Similarity index based on local paths for link\nprediction of complex networks. Physical Review E: Statistical Physics, Plas-\nmas, Fluids, and Related Interdisciplinary Topics 80(4), 046122 (2009) https:\n//doi.org/10.1103/PhysRevE.80.046122 . Publisher: American Physical Society\n[89] Katz, L.: A new status index derived from sociometric analysis. Psychometrika\n18(1), 39–43 (1953) https://doi.org/10.1007/BF02289026\n[90] Fouss, F., Pirotte, A., Renders, J.-m., Saerens, M.: Random-Walk Computation\nof Similarities between Nodes of a Graph with Application to Collaborative Rec-\nommendation. IEEE Transactions on Knowledge and Data Engineering 19(3),\n355–369 (2007) https://doi.org/10.1109/TKDE.2007.46\n[91] Jeh, G., Widom, J.: SimRank: a measure of structural-context similarity. In: Pro-\nceedings of the Eighth ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining. Kdd ’02, pp. 538–543. Association for Computing\nMachinery, New York, NY, USA (2002). https://doi.org/10.1145/775047.775126\n[92] Yang, J., You, J., Wan, X.: Graph embedding via graph summarization. IEEE\naccess : practical innovations, open solutions 9, 45163–45174 (2021) https://doi.\norg/10.1109/ACCESS.2021.3067901\n38\n\n\n[93] Hill, S., Agarwal, D.K., Bell, R., Volinsky, C.: Building an Effective Representa-\ntion for Dynamic Networks. Journal of Computational and Graphical Statistics\n15(3), 584–608 (2006) https://doi.org/10.1198/106186006X139162\n[94] Wu, T., Chang, C.-S., Liao, W.: Tracking network evolution and their applica-\ntions in structural network analysis. IEEE Transactions on Network Science and\nEngineering 6(3), 562–575 (2018)\n[95] Dunlavy, D.M., Kolda, T.G., Acar, E.: Temporal link prediction using matrix\nand tensor factorizations. ACM Transactions on Knowledge Discovery from Data\n(TKDD) 5(2), 1–27 (2011)\n[96] Faber, N.K.M., Bro, R., Hopke, P.K.: Recent developments in CANDECOM-\nP/PARAFAC algorithms: a critical review. Chemometrics and Intelligent Labo-\nratory Systems 65(1), 119–137 (2003) https://doi.org/10.1016/S0169-7439(02)\n00089-8\n[97] Huang, Z., Zhou, A., Zhang, G.: Non-negative Matrix Factorization: A Short\nSurvey on Methods and Applications. In: Li, Z., Li, X., Liu, Y., Cai, Z. (eds.)\nComputational Intelligence and Intelligent Systems. Communications in Com-\nputer and Information Science, pp. 331–340. Springer, Berlin, Heidelberg (2012).\nhttps://doi.org/10.1007/978-3-642-34289-9 37\n[98] De Winter, S., Decuypere, T., Mitrovi´c, S., Baesens, B., De Weerdt, J.: Combin-\ning Temporal Aspects of Dynamic Networks with Node2Vec for a more Efficient\nDynamic Link Prediction. In: 2018 IEEE/ACM International Conference on\nAdvances in Social Networks Analysis And Mining (ASONAM), pp. 1234–1241\n(2018). https://doi.org/10.1109/ASONAM.2018.8508272\n[99] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781 (2013)\n[100] Moore, E.F.: The shortest path through a maze. In: Proc. Int. Symp. Switching\nTheory, 1959, pp. 285–292 (1959)\n[101] Tarjan, R.: Depth-first search and linear graph algorithms. SIAM journal on\ncomputing 1(2), 146–160 (1972)\n[102] Cox, D.R.: The regression analysis of binary sequences. Journal of the Royal\nStatistical Society: Series B (Methodological) 20(2), 215–232 (1958)\n[103] Breiman, L.: Random forests. Machine learning 45, 5–32 (2001)\n[104] Shi, Y., Li, J., Li, Z.: Gradient boosting with piece-wise linear regression trees.\narXiv preprint arXiv:1802.05640 (2018)\n[105] Kipf, T.N., Welling, M.: Variational graph auto-encoders. arXiv preprint\n39\n\n\narXiv:1611.07308 (2016)\n[106] Phoenix, S.J.D.: Elements of Information Theory. Journal of Modern Optics\n39(7), 1600–1601 (1992) https://doi.org/10.1080/09500349214551641\n[107] Hinton, G.E., Osindero, S., Teh, Y.-W.: A fast learning algorithm for deep belief\nnets. Neural Computation 18(7), 1527–1554 (2006) https://doi.org/10.1162/\nneco.2006.18.7.1527\n[108] Khoshraftar, S., An, A.: A survey on graph representation learning methods.\narXiv preprint arXiv:2204.01855 (2022)\n[109] Kipf, T.N., Welling, M.: Semi-Supervised Classification with Graph Convolu-\ntional Networks. arXiv (2017). https://doi.org/10.48550/arXiv.1609.02907\n[110] Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., Mei, Q.: Line: Large-scale infor-\nmation network embedding. In: Proceedings of the 24th International Conference\non World Wide Web, pp. 1067–1077 (2015)\n[111] Hamilton, W.L., Ying, R., Leskovec, J.: Inductive representation learning on\nlarge graphs. In: Proceedings of the 31st International Conference on Neural\nInformation Processing Systems. NIPS’17, pp. 1025–1035. Curran Associates\nInc., Red Hook, NY, USA (2017). event-place: Long Beach, California, USA\n[112] Bui, K.-H.N., Cho, J., Yi, H.: Spatial-temporal graph neural network for traffic\nforecasting: An overview and open research issues. Applied Intelligence 52(3),\n2763–2774 (2022) https://doi.org/10.1007/s10489-021-02587-w\n[113] Sahili, Z.A., Awad, M.: Spatio-temporal graph neural networks: a survey. arXiv:\n2301.10569 [cs.LG] (2023). https://arxiv.org/abs/2301.10569\n[114] Zhang, Q., Chang, J., Meng, G., Xiang, S., Pan, C.: Spatio-Temporal Graph\nStructure Learning for Traffic Forecasting. Proceedings of the AAAI Conference\non Artificial Intelligence 34(01), 1177–1185 (2020) https://doi.org/10.1609/aaai.\nv34i01.5470\n[115] Zhang, X., Huang, C., Xu, Y., Xia, L., Dai, P., Bo, L., Zhang, J., Zheng,\nY.: Traffic flow forecasting with spatial-temporal graph diffusion network. In:\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 35, pp.\n15008–15015 (2021)\n[116] Ali, M.A., Venkatesan, S., Liang, V., Kruppa, H.: TEST-GCN: Topologically\nEnhanced Spatial-Temporal Graph Convolutional Networks for Traffic Fore-\ncasting. In: 2021 IEEE International Conference on Data Mining (ICDM),\npp. 982–987. IEEE, Auckland, New Zealand (2021). https://doi.org/10.1109/\nICDM51629.2021.00110\n40\n\n\n[117] Wang, L., Adiga, A., Chen, J., Sadilek, A., Venkatramanan, S., Marathe, M.:\nCausalGNN: Causal-Based Graph Neural Networks for Spatio-Temporal Epi-\ndemic Forecasting. Proceedings of the AAAI Conference on Artificial Intelligence\n36(11), 12191–12199 (2022) https://doi.org/10.1609/aaai.v36i11.21479\n[118] LeCun, Y., Bengio, Y.: Convolutional networks for images, speech, and time\nseries. The handbook of brain theory and neural networks 3361(10), 1995 (1995)\n[119] O’Shea, K., Nash, R.: An introduction to convolutional neural networks. arXiv\npreprint arXiv:1511.08458 (2015)\n[120] Li, Y., Yu, R., Shahabi, C., Liu, Y.: Diffusion convolutional recurrent neural\nnetwork: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926 (2017)\n[121] Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions.\narXiv preprint arXiv:1511.07122 (2015)\n[122] Ruder, S.: An overview of gradient descent optimization algorithms. arXiv\npreprint arXiv:1609.04747 (2016)\n[123] Gasteiger, J., Bojchevski, A., G¨unnemann, S.: Predict then propagate: Graph\nneural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997\n(2018)\n[124] Bojchevski, A., Gasteiger, J., Perozzi, B., Kapoor, A., Blais, M., R´ozemberczki,\nB., Lukasik, M., G¨unnemann, S.: Scaling graph neural networks with approx-\nimate pagerank. In: Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pp. 2464–2473 (2020)\n[125] Shchur, O., T¨urkmen, A.C., Januschowski, T., G¨unnemann, S.: Neural Temporal\nPoint Processes: A Review. arXiv (2021). http://arxiv.org/abs/2104.03528\n[126] Hawkes, A.G.: Spectra of some self-exciting and mutually exciting point\nprocesses. Biometrika 58(1), 83–90 (1971)\n[127] Yuan, B., Li, H., Bertozzi, A.L., Brantingham, P.J., Porter, M.A.: Multivariate\nspatiotemporal Hawkes processes and network reconstruction. SIAM Journal on\nMathematics of Data Science 1(2), 356–382 (2019)\n[128] Saha, A., Ganguly, N.: Modeling Inter-process Dynamics in Competitive Tem-\nporal Point Processes. Journal of the Indian Institute of Science 101(3), 455–484\n(2021)\n[129] Morariu-Patrichi, M., Pakkanen, M.S.: State-dependent Hawkes processes and\ntheir application to limit order book modelling. Quantitative Finance 22(3),\n563–583 (2022). Publisher: Taylor & Francis\n41\n\n\n[130] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-\nsentations of words and phrases and their compositionality. Advances in neural\ninformation processing systems 26 (2013)\n[131] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural\ninformation processing systems 30, 5998–6008 (2017)\n[132] Bliss, C.A., Frank, M.R., Danforth, C.M., Dodds, P.S.: An evolutionary algo-\nrithm approach to link prediction in dynamic social networks. Journal of\nComputational Science 5(5), 750–764 (2014) https://doi.org/10.1016/j.jocs.\n2014.01.003\n[133] Wang, D., Pedreschi, D., Song, C., Giannotti, F., Barabasi, A.-L.: Human\nmobility, social ties, and link prediction. In: Proceedings of the 17th ACM\nSIGKDD International Conference on Knowledge Discovery and Data mining,\npp. 1100–1108 (2011)\n[134] Neville, J., Jensen, D., Gallagher, B.: Simple estimators for relational Bayesian\nclassifiers. In: Third IEEE International Conference on Data Mining, pp. 609–612\n(2003). https://doi.org/10.1109/ICDM.2003.1250989\n[135] Neville, J., Jensen, D., Friedland, L., Hay, M.: Learning relational probability\ntrees. In: Proceedings of the Ninth ACM SIGKDD International Conference on\nKnowledge Discovery and Data mining, pp. 625–630 (2003)\n[136] Friedman, J.H.: Greedy function approximation: a gradient boosting machine.\nAnnals of statistics, 1189–1232 (2001)\n[137] Kalman, R.E.: A New Approach to Linear Filtering and Prediction Problems.\nJournal of Basic Engineering 82(1), 35–45 (1960) https://doi.org/10.1115/1.\n3662552\n[138] Cholette, P.A.: Prior information and ARIMA forecasting. Journal of Forecast-\ning 1(4), 375–383 (1982)\n[139] Brockwell, P.J., Davis, R.A.: Introduction to Time Series and Forecasting.\nSpringer Texts in Statistics. Springer, Cham (2016). https://doi.org/10.1007/\n978-3-319-29854-2\n[140] L¨utkepohl, H.: New Introduction to Multiple Time Series Analysis. Springer,\nBerlin, Heidelberg (2005). https://doi.org/10.1007/978-3-540-27752-1\n[141] Harvey, A.C., Stock, J.H.: Continuous time autoregressive models with common\nstochastic trends. Journal of Economic Dynamics and Control 12(2-3), 365–384\n(1988) https://doi.org/10.1016/0165-1889(88)90046-2\n42\n\n\n[142] Brockwell, P.J.: On the Use of Continuous-time ARMA Models in Time\nSeries Analysis. In: Robinson, P.M., Rosenblatt, M. (eds.) Athens Conference\non Applied Probability and Time Series Analysis. Lecture Notes in Statis-\ntics, pp. 88–101. Springer, New York, NY (1996). https://doi.org/10.1007/\n978-1-4612-2412-9 7\n[143] Gers, F.A., Schmidhuber, J., Cummins, F.: Learning to forget: Continual\nprediction with LSTM. Neural computation 12(10), 2451–2471 (2000)\n[144] Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated\nrecurrent neural networks on sequence modeling. arXiv: 1412.3555 [cs.NE]\n(2014). https://arxiv.org/abs/1412.3555\n[145] Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint\narXiv:1411.1784 (2014)\n[146] Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial net-\nworks. In: Proceedings of the 34th International Conference on Machine Learning\n- Volume 70. ICML’17, pp. 214–223. JMLR.org, Sydney, NSW, Australia (2017)\n[147] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y.: Generative adversarial networks. Communications\nof the ACM 63(11), 139–144 (2020)\n[148] Soydaner, D.: Attention mechanism in neural networks: where it comes and\nwhere it goes. Neural Computing and Applications 34(16), 13371–13385 (2022)\nhttps://doi.org/10.1007/s00521-022-07366-3\n[149] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel,\nR., Bengio, Y.: Show, Attend and Tell: Neural Image Caption Generation\nwith Visual Attention. In: Bach, F., Blei, D. (eds.) Proceedings of the 32nd\nInternational Conference on Machine Learning. Proceedings of Machine Learn-\ning Research, vol. 37, pp. 2048–2057. PMLR, Lille, France (2015). https://\nproceedings.mlr.press/v37/xuc15.html\n[150] Mishra, N., Rohaninejad, M., Chen, X., Abbeel, P.: A simple neural atten-\ntive meta-learner. arXiv: 1707.03141 [cs.AI] (2018). https://arxiv.org/abs/1707.\n03141\n[151] Badie-Modiri, A., Rizi, A.K., Karsai, M., Kivel¨a, M.: Directed percolation in\ntemporal networks. Physical Review Research 4(2), 022047 (2022) https://doi.\norg/10.1103/PhysRevResearch.4.L022047\n[152] Lv, L., Bardou, D., Hu, P., Liu, Y., Yu, G.: Graph regularized nonnegative matrix\nfactorization for link prediction in directed temporal networks using PageRank\ncentrality. Chaos, Solitons & Fractals 159, 112107 (2022) https://doi.org/10.\n1016/j.chaos.2022.112107\n43\n\n\n[153] Daley, D.J., Kendall, D.G.: Epidemics and rumours. Nature 204, 1118–1118\n(1964)\n[154] Dietz, K.: Epidemics and rumours: A survey. Journal of the Royal Statistical\nSociety: Series A (General) 130(4), 505–528 (1967)\n[155] Aguilar Igartua, M., Cuomo, F., Gu´erin-Lassous, I.: Special issue on “Modeling\nand Performance Evaluation of Wireless Ad-Hoc Networks”. Ad hoc networks\n24(B), 1–2 (2015)\n[156] Nassir, N., Hickman, M., Malekzadeh, A., Irannezhad, E.: A utility-based\ntravel impedance measure for public transit network accessibility. Transporta-\ntion Research Part A: Policy and Practice 88, 26–39 (2016) https://doi.org/10.\n1016/j.tra.2016.03.007\n[157] Cai, B., Xiang, Y., Gao, L., Zhang, H., Li, Y., Li, J.: Temporal knowledge graph\ncompletion: a survey. arXiv preprint arXiv:2201.08236 (2022)\n[158] Zhang, J., Liang, S., Sheng, Y., Shao, J.: Temporal knowledge graph representa-\ntion learning with local and global evolutions. Knowledge-Based Systems 251,\n109234 (2022) https://doi.org/10.1016/j.knosys.2022.109234\n[159] Zuo, Y., Zhou, Y., Liu, Z., Wu, J., Zhan, M.: Learning Temporal and Spatial\nEmbedding for Temporal Knowledge Graph Reasoning. In: Khanna, S., Cao, J.,\nBai, Q., Xu, G. (eds.) PRICAI 2022: Trends in Artificial Intelligence. Lecture\nNotes in Computer Science, pp. 127–138. Springer, Cham (2022). https://doi.\norg/10.1007/978-3-031-20865-2 10\n[160] Schlichtkrull, M., Kipf, T.N., Bloem, P., Van Den Berg, R., Titov, I., Welling, M.:\nModeling Relational Data with Graph Convolutional Networks. In: Gangemi,\nA., Navigli, R., Vidal, M.-E., Hitzler, P., Troncy, R., Hollink, L., Tordai, A.,\nAlam, M. (eds.) The Semantic Web vol. 10843, pp. 593–607. Springer, Cham\n(2018). https://doi.org/10.1007/978-3-319-93417-4 38\n[161] Jung, J., Jung, J., Kang, U.: Learning to walk across time for interpretable tem-\nporal knowledge graph completion. In: Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining, pp. 786–795 (2021)\n[162] Liu, Y., Ma, Y., Hildebrandt, M., Joblin, M., Tresp, V.: Tlogic: Temporal logical\nrules for explainable link forecasting on temporal knowledge graphs. In: Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 4120–4127\n(2022)\n[163] Jaya Lakshmi, T., Durga Bhavani, S.: Link Prediction in Temporal Heteroge-\nneous Networks. In: Wang, G.A., Chau, M., Chen, H. (eds.) Intelligence and\nSecurity Informatics. Lecture Notes in Computer Science, pp. 83–98. Springer,\nCham (2017). https://doi.org/10.1007/978-3-319-57463-9 6\n44\n\n\n[164] Dong, Y., Tang, J., Wu, S., Tian, J., Chawla, N.V., Rao, J., Cao, H.: Link\nPrediction and Recommendation across Heterogeneous Social Networks. In: 2012\nIEEE 12th International Conference on Data Mining, pp. 181–190 (2012). https:\n//doi.org/10.1109/ICDM.2012.140\n[165] Xie, Y., Wang, Z., Yang, C., Li, Y., Ding, B., Deng, H., Han, J.: Komen:\nDomain knowledge guided interaction recommendation for emerging scenarios.\nIn: Proceedings of the ACM Web Conference 2022, pp. 1301–1310 (2022)\n[166] Yang, Y., Chawla, N., Sun, Y., Hani, J.: Predicting Links in Multi-relational\nand Heterogeneous Networks. In: 2012 IEEE 12th International Conference on\nData Mining, pp. 755–764 (2012). https://doi.org/10.1109/ICDM.2012.144\n[167] Negi, S., Chaudhury, S.: Link Prediction in Heterogeneous Social Networks.\nIn: Proceedings of the 25th ACM International on Conference on Information\nand Knowledge Management. CIKM ’16, pp. 609–617. Association for Comput-\ning Machinery, New York, NY, USA (2016). https://doi.org/10.1145/2983323.\n2983722\n[168] Wang, X., Ji, H., Shi, C., Wang, B., Ye, Y., Cui, P., Yu, P.S.: Heterogeneous\ngraph attention network. In: The World Wide Web Conference. Www ’19, pp.\n2022–2032. Association for Computing Machinery, New York, NY, USA (2019).\nhttps://doi.org/10.1145/3308558.3313562 . Place: San Francisco, CA, USA\n[169] Xue, H., Yang, L., Jiang, W., Wei, Y., Hu, Y., Lin, Y.: Modeling Dynamic\nHeterogeneous Network for Link Prediction Using Hierarchical Attention with\nTemporal RNN. In: Hutter, F., Kersting, K., Lijffijt, J., Valera, I. (eds.) Machine\nLearning and Knowledge Discovery in Databases vol. 12457, pp. 282–298.\nSpringer, Cham (2021). https://doi.org/10.1007/978-3-030-67658-2 17\n[170] Yang, L., Xiao, Z., Jiang, W., Wei, Y., Hu, Y., Wang, H.: Dynamic Het-\nerogeneous Graph Embedding Using Hierarchical Attentions. In: Jose, J.M.,\nYilmaz, E., Magalh˜aes, J., Castells, P., Ferro, N., Silva, M.J., Martins, F. (eds.)\nAdvances in Information Retrieval vol. 12036, pp. 425–432. Springer, Cham\n(2020). https://doi.org/10.1007/978-3-030-45442-5 53\n[171] Lee, G., Shin, K.: THyMe+: Temporal hypergraph motifs and fast algorithms for\nexact counting. 2021 IEEE International Conference on Data Mining (ICDM),\n310–319 (2021)\n[172] Fischer, M.T., Arya, D., Streeb, D., Seebacher, D., Keim, D.A., Worring, M.:\nVisual Analytics for Temporal Hypergraph Model Exploration. IEEE Trans-\nactions on Visualization and Computer Graphics 27(2), 550–560 (2021) https:\n//doi.org/10.1109/TVCG.2020.3030408\n[173] Sun, X., Yin, H., Liu, B., Chen, H., Meng, Q., Han, W., Cao, J.: Multi-level\nhyperedge distillation for social linking prediction on sparsely observed networks.\n45\n\n\nIn: Proceedings of the Web Conference 2021, pp. 2934–2945 (2021)\n[174] Luo, X., Peng, J., Liang, J.: Directed hypergraph attention network for traffic\nforecasting. IET Intelligent Transport Systems 16(1), 85–98 (2022) https://doi.\norg/10.1049/itr2.12130\n[175] Paranjape, A., Benson, A.R., Leskovec, J.: Motifs in Temporal Networks. In:\nProceedings of the Tenth ACM International Conference on Web Search And\nData Mining, pp. 601–610. ACM, Cambridge United Kingdom (2017). https:\n//doi.org/10.1145/3018661.3018731\n[176] Yao, Q., Chen, B., Evans, T.S., Christensen, K.: Higher-order temporal network\neffects through triplet evolution. Scientific Reports 11(1), 15419 (2021) https:\n//doi.org/10.1038/s41598-021-94389-w\n[177] Qiu, Z., Wu, J., Hu, W., Du, B., Yuan, G., Yu, P.S.: Temporal Link Pre-\ndiction With Motifs for Social Networks. IEEE Transactions on Knowledge\nand Data Engineering 35(3), 3145–3158 (2023) https://doi.org/10.1109/TKDE.\n2021.3108513\n[178] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win-\nter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,\nBerner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language\nmodels are few-shot learners. In: Proceedings of the 34th International Confer-\nence on Neural Information Processing Systems. Nips ’20. Curran Associates\nInc., Red Hook, NY, USA (2020). Place: Vancouver, BC, Canada tex.articleno:\n159\n[179] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R.,\nGray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language\nmodels. arXiv: 2001.08361 [cs.LG] (2020). https://arxiv.org/abs/2001.08361\n46\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21185v1.pdf",
    "total_pages": 46,
    "title": "A Survey of Link Prediction in Temporal Networks",
    "authors": [
      "Jiafeng Xiong",
      "Ahmad Zareie",
      "Rizos Sakellariou"
    ],
    "abstract": "Temporal networks have gained significant prominence in the past decade for\nmodelling dynamic interactions within complex systems. A key challenge in this\ndomain is Temporal Link Prediction (TLP), which aims to forecast future\nconnections by analysing historical network structures across various\napplications including social network analysis. While existing surveys have\naddressed specific aspects of TLP, they typically lack a comprehensive\nframework that distinguishes between representation and inference methods. This\nsurvey bridges this gap by introducing a novel taxonomy that explicitly\nexamines representation and inference from existing methods, providing a novel\nclassification of approaches for TLP. We analyse how different representation\ntechniques capture temporal and structural dynamics, examining their\ncompatibility with various inference methods for both transductive and\ninductive prediction tasks. Our taxonomy not only clarifies the methodological\nlandscape but also reveals promising unexplored combinations of existing\ntechniques. This taxonomy provides a systematic foundation for emerging\nchallenges in TLP, including model explainability and scalable architectures\nfor complex temporal networks.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}