{
  "id": "arxiv_2502.21057v1",
  "text": "Robust Deterministic Policy Gradient for Disturbance Attenuation and\nIts Application to Quadrotor Control*\nTaeho Lee1 and Donghwan Lee1\nAbstract— Practical control systems pose significant chal-\nlenges in identifying optimal control policies due to uncer-\ntainties in the system model and external disturbances. While\nH∞control techniques are commonly used to design robust\ncontrollers that mitigate the effects of disturbances, these\nmethods often require complex and computationally intensive\ncalculations. To address this issue, this paper proposes a\nreinforcement learning algorithm called Robust Deterministic\nPolicy Gradient (RDPG), which formulates the H∞control\nproblem as a two-player zero-sum dynamic game. In this\nformulation, one player (the user) aims to minimize the cost,\nwhile the other player (the adversary) seeks to maximize it. We\nthen employ deterministic policy gradient (DPG) and its deep\nreinforcement learning counterpart to train a robust control\npolicy with effective disturbance attenuation. In particular, for\npractical implementation, we introduce an algorithm called\nrobust deep deterministic policy gradient (RDDPG), which\nemploys a deep neural network architecture and integrates\ntechniques from the twin-delayed deep deterministic policy\ngradient (TD3) to enhance stability and learning efficiency.\nTo evaluate the proposed algorithm, we implement it on\nan unmanned aerial vehicle (UAV) tasked with following a\npredefined path in a disturbance-prone environment. The\nexperimental results demonstrate that the proposed method\noutperforms other control approaches in terms of robustness\nagainst disturbances, enabling precise real-time tracking of\nmoving targets even under severe disturbance conditions.\nI. INTRODUCTION\nIn recent years, significant advancements have been made\nin deep reinforcement learning (DRL), which integrates deep\nlearning and reinforcement learning (RL). The successes\nof DRL have been demonstrated across various domains,\nincluding games [12], [13] and control [5], [14], [15].\nAmong the various challenging domains, this paper focuses\non dynamic system control problems with continuous state-\naction spaces.\nPractical dynamic control systems pose significant chal-\nlenges in identifying optimal control policies due to unstruc-\ntured uncertainties in the system model and external distur-\nbances. Current state-of-the-art DRL methods for continuous\ncontrol tasks, such as deep deterministic policy gradient\n(DDPG) [5], twin delayed DDPG (TD3) [4], soft actor-\ncritic (SAC) [2], and proximal policy optimization (PPO) [3],\ngenerally fail to address these issues effectively. For instance,\na DRL agent trained in a specific real-world or simulated\n*This work was supported by the Institute of Information Commu-\nnications Technology Planning Evaluation (IITP) funded by the Korea\ngovernment under Grant 2022-0-00469.\n1Taeho\nLee\nand\nDonghwan\nLee\nare\nwith\nthe\nSchool\nof\nElectrical\nEngineering,\nKorea\nAdvanced\nInstitute\nof\nScience\nand\nTechnology\n(KAIST),\nDaejeon\n34141,\nSouth\nKorea.\nemail:\neho0228,donghwan@kaist.ac.kr\nenvironment may struggle to maintain optimal performance\nwhen deployed in different environments where the patterns\nof external disturbances changed. Therefore, developing a\nDRL agent policy that is robust to unseen disturbances is\ncrucial for ensuring reliability in real-world applications.\nThe main goal of this paper is to introduce the robust\ndeterministic policy gradient (RDPG) algorithm, which en-\nhances the robustness of the DPG algorithm by integrating\nthe concept of the H∞control problem with a two-player\nzero-sum dynamic game framework.\nThe H∞control method is a robust control technique\nthat designs an optimal controller to minimize the impact\nof disturbances on its performance. It is well-known that,\nunder certain assumptions, the H∞control problem can\nbe transformed into a two-player zero-sum dynamic game\n(or a minmax optimization problem), where the first player,\nor the controller, tries to minimize a specific cost, while\nthe second player, or the adversary, attempts to maximize\nit [21]. The minmax optimization problem can be solved\nby the Hamilton–Jacobi–Isaacs (HJI) equation. If the system\nis linear with quadratic costs, the solution of HJI equa-\ntion can be derived through a Riccati equation [22], [23].\nHowever, if the system has nonlinearity or the cost is non-\nquadratic, the HJI equation is extremely difficult to solve\ndirectly. Therefore, some methods have tried to obtain the\napproximate solution of HJI equations including power series\nexpansion-based method [6], locally linearized mode [7],\nonline learning algorithm with neural nets [10], and relaxed\npolicy iteration algorithm with bounded policy evaluation\nerror [11]. However, these methods are still associated with\na high computational cost.\nIn this paper, we present the so-called robust determinis-\ntic policy gradient (RDPG) algorithm, which improves the\nrobustness of DPG algorithm from the two-player zero-sum\ndynamic game perspectives. In particular, we transform the\nH∞problem into a two-player zero-sum dynamic game be-\ntween the user (controller) and the adversary that represents\nthe disturbance. Then, we propose a version of deterministic\npolicy gradient method to train both the user and adversary,\nwhere the user and adversary each learn their own policy to\nminimize and maximize the objective function, respectively.\nConsequently, the robust controller can determine the optimal\ncontrol input, which is robust against disturbances. The\nproposed RDPG is then extended to a DDPG variant, called\nRDDPG (robust DDPG), which combines the idea of RDPG\nwith TD3.\nAdditionally, we aim to develop a robust control strategy\nfor quadrotors, a commonly used unmanned aerial vehicle\narXiv:2502.21057v1  [cs.RO]  28 Feb 2025\n\n\n(UAV) across various applications. Ensuring robust quadrotor\ncontrol is essential, particularly for tasks such as tracking\nmoving targets or following predefined waypoints in dynamic\nenvironments. We demonstrate that the proposed RDDPG\nmethod effectively determines optimal control inputs in the\npresence of external disturbances. Furthermore, it outper-\nforms other DRL-based control approaches by achieving\nlower cost under severe disturbance conditions.\nIn conclusion, the main contributions can be summarized\nas follows:\n1) We propose the robust deterministic policy gradient\n(RDPG) algorithm, which effectively improves the\nrobustness against external disturbances by solving the\nH∞problem through the dynamic game perspectives\nin combination with the actor-critic method.\n2) Moreover, we develop a new DDPG algorithm, called\nRDDPG, which combines the concept of RDPG with\nTD3. Through numerical simulations of the quadro-\ntor control problems, the effectiveness of RDDPG is\ndemonstrated.\n3) Through the developed method, we propose a robust\ntracking control design method for quadrotor applica-\ntions. Comprehensive experimental results are given to\ndemonstrate that the proposed approach significantly\noutperforms other state-of-the-art DRL approaches\nsuch as DDPG, PPG, SAC, and TD3.\nII. RELATED WORKS\nThe H∞control methods are effective in designing robust\ncontrollers against disturbances and have demonstrated sat-\nisfactory performance in nonlinear dynamic systems [6]–[9].\nHowever, these methods require precise model linearization\nand incur a high computational costs due to the need to solve\nHJI equations. To address this issue, alternative approaches\nleverage deep reinforcement learning (DRL) to determine the\noptimal control policy, which does not require solving the\nnonlinear HJI equation.\nUntil now, the two-player zero-sum game and H∞control\nframeworks have been applied to deep reinforcement learn-\ning (DRL) to enhance robustness under various scenarios.\nFor instance, [18] developed a robust DRL approach to\nbridge the gap between different environments for quad-\ncopter control tasks. In [24], the authors proposed a novel\napproach for robust locomotion control of quadruped robots\nagainst external disturbances. A robust adversarial reinforce-\nment learning strategy was proposed in [19] to address\nmodeling errors and discrepancies between training and\ntesting conditions. Although their underlying philosophy is\nsimilar to ours, their approaches are significantly different.\nThe method in [24] solves two separate constrained opti-\nmization problems based on the ideas in PPO. The DRL\nin [18] primarily considers model uncertainties rather than\nexternal disturbances and applies the so-called action robust\nRL in [20], which has environmental structures different\nfrom our setting. The authors in [19] propose more general\nalternating optimization procedures and do not consider\ndeterministic policy gradient.\nIII. BACKGROUND\nA. H∞Control\nLet us consider the discrete time nonlinear system\nxk+1 = f(xk, uk, wk, vk)\nyk = g(xk, uk, wk)\n(1)\nUsing the state-feedback controller u = π(x), the system\ncan be reduced to the autonomous closed-loop system\nxk+1 = f(xk, π(xk), wk, vk)\nyk = g(xk, π(xk), wk)\nAssume that the initial state x0 is determined by x0 ∼\nρ(·), where ρ is the initial state distribution. Defining the\nstochastic processes w0:∞:= (w0, w1, . . .) and y0:∞:=\n(y0, y1, . . .), the system can be seen as a stochastic mapping\nfrom w0:∞to y0:∞as follows:\ny0:∞∼Tπ(·|w0:∞).\nwhere Tπ is the conditional probability of y0:∞given w0:∞.\nMoreover, defining the L2 norm for the general stochastic\nprocess z0:∞:= (z0, z1, . . .) by\n||z0:∞||L2 :=\nv\nu\nu\nt\n∞\nX\nk=0\nE[zT\nk zk],\nthe H∞norm of the autonomous system is defined as\n||Tπ||∞:=\nsup\nw0:∞̸=0\n∥y0:∞∥L2\n∥w0:∞∥L2\n(2)\nThe goal of H∞control is to design a control policy π\nthat minimizes the H∞norm of the system Tπ. However,\nminimizing ||Tπ||∞directly is often difficult in practical\nimplementations. Instead, we typically aim to find an ac-\nceptable η > 0 and a controller satisfying ||Tπ||∞≤η,\nwhich is called suboptimal H∞control problem. Then, the\nH∞control problem can be approximated to the problem of\nfinding a controller π that satisfies the constraint\n||Tπ||2\n∞=\nsup\nw0:∞̸=0\n||y0:∞||2\nL2\n||w0:∞||2\nL2\n≤η2\n(3)\nDefining\nJπ :=\nsup\nw0:∞̸=0\nE\n\" ∞\nX\nk=0\n(yT\nk yk −η2wT\nk wk)\n\f\f\f\f\f π\n#\nthe problem can be equivalently written by finding a con-\ntroller π satisfying Jπ ≤0. The problem can be solved by\nπ∗:= arg min\nπ Jπ.\n\n\nB. Two-player zero-sum dynamic game perspective\nAccording to [21], the suboptimal H∞control problem\ncan be equivalently viewed as solving a zero-sum dy-\nnamic game under certain assumptions. In particular, we\nconsider two decision making agents called the user and\nthe adversary, respectively. They sequentially take control\nactions uk (user) and wk (adversary) to minimize and max-\nimize cumulative discounted costs, respectively, of the form\nE\n\u0002P∞\nk=0 γkc(xk, uk, wk)\n\u0003\n, where γ ∈(0, 1] is the discount\nfactor, uk is the control input of the first agent, and wk is the\ncontrol input of the second agent. The user’s primary goal\nentails minimizing the costs, while the adversary strives to\nhinder the user’s progress by maximizing the costs. There\nexist two categories of the two-player dynamic games, the\nalternating two-player dynamic game and simultaneous two-\nplayer dynamic game. In the alternating two-player dynamic\ngame, two agents engaged in decision making take turns in\nselecting actions to maximize and minimize the cost, respec-\ntively. On the other hand, in the simultaneous two-player\ndynamic game, the two agents take actions simultaneously\nto maximize and minimize the cost. In this paper, we mainly\nfocus on the simultaneous two-player dynamic game.\nWe consider the state-feedback deterministic control pol-\nicy for each agent, defined as π : Rn →Rm and µ : Rn →\nRp\nu = π(x) ∈Rm,\nw = µ(x) ∈Rp,\nx ∈Rn\nThe corresponding cost function is defined as\nJπ,µ := E\n\" ∞\nX\nk=0\nγkc(xk, uk, wk)\n\f\f\f\f\f π, µ\n#\n(4)\nThe goal of the dynamic game is to find a saddle-point pair\nof equilibrium policies (π, µ) (if exists) for which\nJπ∗,µ ≤Jπ∗,µ∗≤Jπ,µ∗,\n∀π ∈Π,\n∀µ ∈M\nwhere Π denotes the set of all admissible state-feedback\npolicies for the user and M denotes the set of all admissible\nstate-feedback policies for the adversary. The above relation\nequivalently means\nµ∗= max\nµ∈MJπ∗,µ,\nπ∗= min\nπ∈ΠJπ,µ∗\nBy the min-max theorem, this also implies that\nmax\nµ∈Mmin\nπ∈ΠJπ,µ = min\nπ∈Πmax\nµ∈MJπ,µ\nThe optimal cost is now defined as\nJ∗:= Jπ∗,µ∗\nTo show a connection between the dynamic game framework\nand the H∞control problem, let us consider the specific per-\nstep cost function\nc(x, u, w) = g(x, u, w)T g(x, u, w) −η2wT w\nwhere v ∼p(·) and γ = 1. The corresponding cost function\nis then written as\nJπ,µ = E\n\" ∞\nX\nk=0\n(yT\nk yk −η2wT\nk wk)\n\f\f\f\f\f π, µ\n#\nThe user’s optimal policy is\nπ∗:= arg min\nπ∈Πmax\nµ∈MJπ,µ\nwhich is structurally very similar to the policy with the H∞\nperformance in the previous subsection. It is known that\nunder some special conditions such as the linearity, the H∞\nsuboptimal control policy and the saddle-point policy are\nequivalent when J∗≤0.\nC. Bellman equations\nLet us define the value function\nV π,µ(x) := E\n\" ∞\nX\nk=t\nγkc(xk, uk, wk)\n\f\f\f\f\f π, µ, xt = x\n#\nso\nthat\nthe\ncorresponding\ncost\nfunction\nis\nJπ,µ\n=\nE[V π,µ(x)|x ∼ρ(·)]. We can also prove that the value\nfunction satisfies the Bellman equation\nV π,µ(x) = c(x, π(x), µ(x)) + γE[V π,µ(x′)|π, µ]\nwhere x′ implies the next state given the current state x and\nthe action taken by (π, µ). Let us define the optimal value\nfunction as V ∗:= V π∗,µ∗. Then, the corresponding optimal\nBellman equation can be obtained by replacing (π, µ) by\n(π∗, µ∗). Similarly, let us define the so-called Q-function by\nQπ,µ(x, u, w)\n:=E\n\" ∞\nX\nk=t\nγkc(xk, uk, wk)\n\f\f\f\f\f xt = z, ut = u, wt = w, π, µ\n#\nwhich satisfies the Q-Bellman equation\nQπ,µ(x, u, w) = c(x, u, w) + γE[Qπ,µ(x′, u′, w′)|π, µ]\nwhere x′ implies the next state given the current state x\nand action pair (u, w), u′ means the next action of the user\ngiven x′ and under π, and w′ implies the next action of the\nadversary given x′ and under µ. Defining the optimal Q-\nfunction Q∗:= Qπ∗,µ∗, one can easily prove the optimal\nQ-Bellman equation\nQ∗(x, u, w) = c(x, u, w) + γE[Q∗(x′, u′, w′)|π∗, µ∗]\nIV. PROPOSED METHOD\nIn this section, we will present the proposed method.\nA. Robust deterministic policy gradient\nTo apply DPG to the two-player zero-sum dynamic games,\nwe consider the parameterized deterministic control policies\nu = πθ(x) ∈Rm,\nw = µϕ(x) ∈Rp,\nx ∈Rn\nThe saddle-point problem is then converted to\nθ∗:= arg min\nθ max\nϕ Jπθ,µϕ\nwhich can be solved using the primal-dual iteration\nθk+1 =θk −αθ ∇θJπθ,µϕk |θ=θk,\nϕk+1 =ϕk + αϕ ∇ϕJπθk+1,µϕ|ϕ=ϕk\n\n\nwhere αθ > 0 and αϕ > 0 are step-sizes. According to\n[1], the deterministic gradients can be obtained using the\nfollowing theorem.\nTheorem 1: The deterministic policy gradients for the user\nand adversary policies are given by\n∇θJπθ,µϕ = E\nh\n∇θQπ,µϕ(s, πθ, µϕ)|π=πθ\n\f\f\f s ∼ρπθ,µϕ\ni\nand\n∇ϕJπθ,µϕ = E\nh\n∇ϕQπθ,µ(s, πθ, µϕ)|µ=µϕ\n\f\f\f s ∼ρπθ,µϕ\ni\nrespectively.\nThe proof is a simple extension of the DPG theorem in [1],\nso it is omitted here. A reinforcement learning counterpart to\nDPG can be obtained by using the samples of the gradients.\nNext, we will consider the following per-step cost function:\nc(x, u, w) = ˜c(x, u, w) −η2wT w\nwhere ˜c : Rn×Rm×Rp×Rn →R is the cost function for the\nuser. Note that the cost function ˜c can be set arbitrarily and\nis more general than the quadratic cost in the H∞control\nproblem. This is because the H∞control problem can be\nrecovered by setting\n˜c(x, u, w) = g(x, u, w, v)T g(x, u, w, v)\nIn this way, we can consider the two-player zero-sum\ndynamic game as an extension of the usual reinforcement\nlearning tasks by considering robustness against the adver-\nsarial disturbance.\nLet us assume that we found some approximate solutions\nθ∗\nϵ , ϕ∗\nϵ such that Jπθ∗ε ,µϕ∗ε ≤0. Then, this implies that\nE\n\u0014 ∞\nP\nk=0\nγk˜c(xk, uk, wk)\n\f\f\f\f πθ∗ε , µϕ∗ε\n\u0015\nE\n\u0014 ∞\nP\nk=0\nγkwT\nk wk\n\f\f\f\f πθ∗ε , µϕ∗ε\n\u0015\n≤η2\nB. Implementation based on TD3 (RDDPG)\nTo implement RDPG for practical high-dimensional tasks,\nwe will apply the techniques from twin-delayed deep deter-\nministic policy gradient (TD3) in [4], which is called robust\ndeep deterministic policy gradient (RDDPG). In particular,\nwe introduce the following networks:\n1) The online actor network πθ for the user and the online\nactor network µϕ for the adversary\n2) The corresponding target networks πθ′, µϕ′ for the two\nactors\n3) Two online critic networks Qψ1(x, u, w), Qψ2(x, u, w)\n4) The\ncorresponding\ntarget\nnetworks\nQψ′\n1(x, u, w),\nQψ′\n2(x, u, w)\nNow, following [4], the actor and critic networks are trained\nthrough the following procedures.\n1) Critic update: The critic is trained by the gradient\ndescent step to the loss\nLcritic(ψi; B) := 1\n|B|\nX\n(x,u,w,c,x′)∈B\n(y −Qψi(x, u, w))2,\ni ∈{1, 2}\nwhere B is the mini-batch, |B| is the size of the mini-batch,\nc is the cost incurred at the same time as the state x, and x′\nmeans the next state. Moreover, the target y is defined as\ny = c + γ max\ni∈{1,2} Qψ′\ni(x′, ˜u, ˜w)\nwhen x′ is not the terminal state, and\ny = c\nwhen x′ is the terminal state (in the episodic environments),\nwhere\n˜u = πθ′(x′) + clip(ϵ1, −c, c) ϵ1 ∼N(0, σ)\n˜w = µϕ′(x′) + clip(ϵ2, −c, c) , ϵ2 ∼N(0, σ)\nwhere ϵ1, ϵ2 are random noise vectors added in order to\nsmooth out the target values, N(0, σ) implies the Gaussian\ndistribution with zero mean and variance σ, and the clip\nfunction is added in order to guarantee the boundedness of\nthe control inputs. The critic’s online parameters ϕ1, ϕ2 are\nupdated by the gradient descent step to minimize the loss\nψi ←ψi −αcriticLcritic(ψi; B),\ni ∈{1, 2}\nwhere αcritic is the step-size.\n2) Actor update: The actor networks for the user and\nadversary are updated using the sampled deterministic policy\ngradient\nθ ←θ −αactor∇θLactor(θ, ϕ; B)\nϕ ←ϕ + βactor∇ϕLactor(θ, ϕ; B)\nwhere αactor and βactor are the step-sizes, and\nLactor(θ, ϕ; B) =\n1\n|B|\nX\n(x,u,w,r,x′)∈B\nQψ1(x, πθ(x), µϕ(x))\nPlease note the opposite signs of the gradients for the user\nand adversary updates, which are due to their opposite roles.\n3) Target parameters update: The target parameters are\nupdated through the averaging\nϕ′ ←τϕ + (1 −τ)ϕ′,\nθ′ ←τθ + (1 −τ)θ′\nfor the actor target parameters and\nψi\n′ ←τψi + (1 −τ)ψi\n′,\ni ∈{1, 2}\nfor the critic target parameters.\n4) Exploration: For exploration, we can use\nuk =πθ(xk) + e1\nwk =µϕ(xk) + e2\nwhere e1, e2 ∼N(0, σ) are exploration noises [4].\n\n\n5) Optimizing η: As an objective function estimate, we\nuse the target y used above. In particular, we simply assume\nJπθ′,µϕ′ ∼= ˆJ :=\n1\n|B|\nX\n(x,u,w,r,x′)∈B\ny\nwhere the target y is defined as before. Then, we can apply\nJ ←(1 −αJ)J + αJ ˆJ\nη ←η + αηJ\nwhere J is an estimate of the cost function Jπθ,µϕ. The basic\nidea of the update rule for η is as follows: if J > 0, then\nincrease η > 0 so that J ≤0; if J < 0, then decrease η > 0\nso that J ≤0. In this way, we can find an optimal η such that\nJ ≤0. The overall algorithm is summarized in Algorithm 1.\nAlgorithm 1 RDDPG\n1: Initialize the online critic networks Qψ1, Qψ2\n2: Initialize the actor networks πθ, µϕ for the user and\nadversary, respectively.\n3: Initialize the target parameters ψ′\n1 ←ψ1, ψ′\n2 ←ψ2, θ′ ←\nθ, ϕ′ ←ϕ\n4: Initialize the replay buffer D\n5: for Episode i=1,2,...Niter do\n6:\nObserve s0\n7:\nfor Time step k=0,1,2,...τ-1 do\n8:\nSelect actions uk = πθ(xk) + e1 and wk =\nµϕ(xk) + e2, where e1, e2 ∼N(0, σ) are exploration\nnoises.\n9:\nCompute the cost ck := c(xk, uk, wk)\n10:\nObserve the next state xk+1\n11:\nStore the transition tuple (xk, uk, wk, ck, xk+1)\nin the replay buffer D\n12:\nUniformly sample a mini-batch B from the re-\nplay buffer D\n13:\nUpdate critic network:\nψi ←ψi −αcriticLcritic(ψi; B),\ni ∈{1, 2}\n14:\nUpdate actor networks by the deterministic policy\ngradient:\nθ ←θ −αactor∇θLactor(θ, ϕ; B)\nϕ ←ϕ + βactor∇ϕLactor(θ, ϕ; B)\n15:\nUpdate J and η:\nJ ←(1 −αJ)J + αJ ˆJ\nη ←η + αηJ\n16:\nSoft update target networks:\n17:\nθ′\ni ←τθi + (1 −τ)θ′\ni , ϕ′\ni ←τθi + (1 −τ)ϕ′\ni\n18:\nend for\n19: end for\nV. QUADROTOR APPLICATION\nA. Quadrotor dynamics model\nThe quadrotor dynamics can be described as follows:\nmak = m\n\n\n0\n0\n−g\n\n+ RTk + FD + wk\n(5)\n∆ωk = I−1(−ωk−1 × Iωk−1 + τB)\n(6)\nvk = vk−1 + Tsak\n(7)\npk = pk−1 + Tsvk\n(8)\nωk = ωk−1 + Ts∆ωk\n(9)\nρk = ρk−1 + Tsωk\n(10)\nwhere m is the mass of the quadrotor, g is the gravity accel-\neration, Ts is the sampling time, wk = [wk,x, wk,y, wk,z]T\nmeans an external disturbance applied to the quadro-\ntor, ak\n= [ak,x, ak,y, ak,z]T denotes linear acceleration,\nvk = [vk,x, vk,y, vk,z]T represents linear velocity, pk =\n[pk,x, pk,y, pk,z]T describes the position of the quadrotor\nin the inertial frame, ∆ωk = [∆ωk,ϕ, ∆ωk,θ, ∆ωk,ψ]T is\nangular acceleration, ωk = [ωk,ϕ, ωk,θ, ωk,ψ]T is the angular\nvelocity, and ρk = [ϕk, θk, ψk]T represents the roll, pitch,\nyaw of the quadrotor in the body frame. Moreover, R is the\nrotation matrix from the body frame to the inertial frame.\nMoreover, TB is the total thrust of the quadrotor along the\nz axis in the body frame and defined as follows:\nTk =\n4\nX\ni=1\nTk,i = kF\n\n\n0\n0\nP RPM2\nk,i\n\n\nwhere Tk,i is the thrust generated by ith motor, kF is the\nthrust coefficient, and RPMk,i is the speed of ith motor at\ntime step k. FD is a drag force, which is generated by motor\nspinning, defined as\nFD = −KD\n 4\nX\ni=1\nRPMk,i\n!\nvk\nwhere KD = diag(KD,x, KD,y, KD,z) is a matrix of drag\ncoefficients. Among the four motors, two opposing motors\n(1th and 3rd) rotate clockwise, while the others (2nd and\n4th) rotate counterclockwise. This causes torque τk in the\nroll, pitch, yaw directions\nτk =\n\n\nτk,ϕ\nτk,θ\nτk,ψ\n\n\n=\n\n\nl′kF (RPM2\nk,2 −RPM2\nk,4)\nl′kF (−RPM2\nk,1 + RPM2\nk,3)\nkM(RPM2\nk,1 −RPM2\nk,2 + RPM2\nk,3 −RPM2\nk,4)\n\n\nwhere l′ = cos( π\n4 )l is the scaled arm length, because the\nmotors of the quadrotor are positioned at a 45-degree angle\nrelative to the x and y axes when the yaw angle is 0, kF\nis the thrust coefficient, and kM is the torque coefficient.\nThe inertia matrix, denoted by the symbol I, represents the\ninertial properties of the quadrotor’s body.\n\n\nB. State space\nThe state xk of the quadrotor is defined as\nxk = [epos\nk , evel\nk , ρk, ωk]T ∈R12\nwhere\nepos\nk\n= ptarget\nk\n−pk\n= [ptarget\nk,x −pk,x, ptarget\nk,y −pk,y, ptarget\nk,z −pk,z]T ∈R3\nevel\nk = vtarget\nk\n−vk\n= [vtarget\nk,x −vk,x, vtarget\nk,y\n−vk,y, vtarget\nk,z\n−vk,z]T ∈R3\nρk = [ϕk, θk, ψk]T ∈R3\nωk = [ωk,ϕ, ωk,θ, ωk,ψ]T ∈R3\nwhere epos\nk\nis the position error between the target ptarget\nk\nand\nthe quadrotor position pk, evel\nk\nis the velocity error between\nthe target vtarget\nk\nand the quadrotor vk, ρk is the Euler angle\nvector, and ω represents the angular velocity vector of the\nquadrotor in its body frame. As in the method used in DQN\n[13], we used the last four time steps of the state as input\nto the neural network to help the network better capture\ndynamics in the environment.\nC. Action space\nThe action of the quadrotor, uk, is the speed of four motors\nRPMk,i, i ∈{1, 2, 3, 4}, which is continuous value between\n0 and 21713.714, as follows:\nuk := [RPMk,1, RPMk,2, RPMk,3, RPMk,4]T ∈R4.\nThe action of the disturbance adversary, wk, represents the\nexternal forces to the quadrotor along the x, y, and z axes\nof the body frame as follows:\nwk := [wk,x, wk,y, wk,z]T ∈R3\nD. Cost function\nThe cost function c(xk, uk, wk) is comprised of the cost\nfunction for the user ˜c(xk, uk) and the disturbance quadratic\ncost η2wT\nk wk. The cost function for the user ˜c(xk, uk) is the\nsum of the following terms: ˜c(xk, uk) = ck,p +ck,v +ck,ρ +\ncl + ctr\n1) Position error cost ck,p : Position error between the\ntarget and quadrotor\nck,p = α × ||epos\nk ||2\n2\n2) Velocity error cost ck,v : Velocity error between the\ntarget and quadrotor\nck,v = β × ||evel\nk ||2\n2\n3) Angle cost ck,ρ : Magnitude of roll, pitch, and yaw of\nthe quadrotor\nck,ρ = ϵ × ||ρk||2\n2\n4) Living cost cl : Constant cost at every step\ncl = λ\n5) Penalty cost ctr : Penalty when the quadrotor fails to\ntrack the target\nctr =\n(\nζ\nif ||epos\nk ||2 ≥5 or ϕk, θk ≥π\n2\n0\nelse\nwhere α, β, ϵ, ζ > 0, and λ < 0 are scaling factors. Since α\nand β are positive, the agent learns a policy that minimizes\nposition and velocity errors. If the quadrotor’s angles become\ntoo large, it becomes more susceptible to disturbances and\nmay flip easily. Therefore, minimizing the angles cost cρ\nis crucial for stability. Additionally, the living cost cl is a\nnegative reward that encourages the quadrotor to maintain\nstability. The penalty cost ctr is a large positive value applied\nwhen the quadrotor fails to track the target—specifically,\nwhen the distance between the target and the quadrotor\nexceeds 5 meters, or when the quadrotor flips with roll\nor pitch angles exceeding π\n2 . The agent learns a policy to\navoid receiving this penalty, which can accelerate the initial\nlearning process.\nVI. EXPERIMENTS AND RESULTS\nA. Experiments setups\nFor experiments, we used gym-pybullet-drones environ-\nment [17], which is an open-source quadrotor simulator,\nbuilt using Python and the Bullet Physics engine. This\nenvironment provides a modular and precise physics imple-\nmentation, supporting both low-level and high-level controls.\nCrazyflie 2.0 quadrotor is modeled in the simulator, which\nis used for our experiments. In experiment, we compare\nRDDPG with DDPG [5], TD3 [4], SAC [2], and PPO [3].\nThese methods utilize a single agent aiming to minimize the\ncost function ˜c. To enhance their robustness against distur-\nbances, random external disturbances were applied during the\nlearning process. From our own experiences, learning under\nsevere disturbances with the four existing methods failed\nto yield policies that demonstrated reasonable performance.\nTherefore, we applied only minor disturbances, limited to\n±5 [m/s] when applying the four methods.\nFor evaluation, we generate three trajectories as shown\nin Fig. 1. During evaluation, we introduce random wind\ndisturbances in the x and y directions of the inertia frame. We\nconsider four scenarios where the maximum wind velocity is\nlimited to ±5, ±10, ±15, and ±20 [m/s], which are referred\nto as the ±5, ±10, ±15, and ±20 wind speed scenarios in the\ntables. In each scenario, the initial wind speed is randomly\nset within a given range and updated every 10 time steps\n(1)\n(2)\n(3)\nFig. 1.\nTrajectories for evaluation.\n\n\nTABLE I\nTHE MEAN AND STANDARD DEVIATION OF THE COST\nBold indicates the best performance algorithm\nAlgorithm\nTrajectory\n(1)\n(2)\n(3)\n±5 wind speed scenario\nRDDPG\n−9.20 ± 0.03\n−9.11 ± 0.02\n−8.94 ± 0.03\nDDPG\n−7.63 ± 0.09\n−7.67 ± 0.10\n−7.49 ± 0.12\nTD3\n−4.25 ± 2.47\n−7.50 ± 1.65\n−6.00 ± 2.53\nSAC\n−7.42 ± 0.98\n−8.68 ± 0.26\n−8.34 ± 0.30\nPPO\n19.26 ± 12.01\n7.72 ± 8.37\n11.03 ± 9.54\n±10 wind speed scenario\nRDDPG\n−9.00 ± 0.12\n−8.96 ± 0.16\n−8.73 ± 0.21\nDDPG\n−7.41 ± 0.24\n−7.41 ± 0.28\n−7.20 ± 0.43\nTD3\n0.45 ± 5.95\n−2.20 ± 5.00\n−0.94 ± 5.00\nSAC\n−7.16 ± 1.05\n−7.94 ± 0.71\n−7.54 ± 0.87\nPPO\n22.89 ± 15.31\n22.22 ± 15.42\n22.71 ± 15.64\n±15 wind speed scenario\nRDDPG\n−8.49 ± 0.52\n−8.36 ± 0.71\n−8.17 ± 0.62\nDDPG\n−5.95 ± 2.03\n−5.98 ± 1.90\n−5.76 ± 1.78\nTD3\n3.02 ± 6.49\n1.90 ± 6.28\n2.64 ± 5.45\nSAC\n−2.86 ± 4.55\n−3.70 ± 4.03\n−3.32 ± 4.13\nPPO\n23.63 ± 17.10\n23.45 ± 16.95\n23.27 ± 15.77\n±20 wind speed scenario\nRDDPG\n−5.04 ± 3.90\n−4.77 ± 4.07\n−4.46 ± 4.03\nDDPG\n0.18 ± 5.56\n−0.04 ± 5.76\n−0.51 ± 5.28\nTD3\n3.76 ± 5.32\n3.27 ± 4.88\n3.66 ± 4.77\nSAC\n0.18 ± 4.64\n0.49 ± 4.96\n0.61 ± 4.98\nPPO\n20.81 ± 15.58\n21.15 ± 15.12\n21.32 ± 16.77\nwith normally distributed noise. Each algorithm was tested\nover 500 episodes and evaluated using three metrics: the\nmean and standard deviation of the total cost per episode, the\naverage H∞norm, empirically evaluated as ˜c(xk,uk,wk)\nwT\nk wk\n, and\nthe mean and standard deviation of the position error between\nthe quadrotor and the target. All metrics were averaged over\n500 episodes.\nB. Results\nFig. 2 shows the sum of costs during the training steps,\ndemonstrating that RDDPG effectively minimizes the cost as\ntraining progresses\nFig. 2. The cost curve of RDDPG during training. This shows that RDDPG\nsuccess to minimize the cost as training progresses.\nTABLE II\nEMPIRICALLY EVALUATED H∞NORM.\nBold indicates the best performance algorithm\nAlgorithm\nTrajectory\n(1)\n(2)\n(3)\n±5 wind speed scenario\nRDDPG\n247.14\n284.04\n299.59\nDDPG\n7874.49\n570.36\n999.50\nTD3\n1421.04\n573.83\n820.78\nSAC\n708.79\n749.96\n593.65\nPPO\n8360.81\n4086.99\n7548.96\n±10 wind speed scenario\nRDDPG\n74.17\n112.76\n84.36\nDDPG\n185.82\n282.43\n202.59\nTD3\n422.65\n331.02\n351.93\nSAC\n209.82\n144.25\n123.55\nPPO\n2314.39\n4058.31\n2308.93\n±15 wind speed scenario\nRDDPG\n59.29\n44.00\n41.09\nDDPG\n75.45\n87.31\n82.17\nTD3\n216.96\n147.81\n216.73\nSAC\n103.45\n75.54\n89.90\nPPO\n6286.04\n1281.48\n2862.71\n±20 wind speed scenario\nRDDPG\n140.33\n30.06\n53.50\nDDPG\n76.90\n83.34\n79.91\nTD3\n284.40\n205.07\n176.43\nSAC\n88.42\n73.83\n78.32\nPPO\n1278.23\n885.98\n1576.79\nTable I presents the mean and standard deviation of the\ntotal cost sum per episode. Across all scenarios, RDDPG\nachieved the lowest total cost sum, which demonstrates\nsuperior performance compared to other algorithms. Notably,\nTD3, the baseline algorithm for RDDPG, exhibited signifi-\ncant performance degradation as wind conditions intensified.\nIn contrast, RDDPG maintained a lower cost sum and greater\nstability, indicating that RDPG enhances the robustness of\nthe DPG algorithm.\nTable II describes the mean of H∞norm at time step k in\nall wind speed scenarios. As the wind speed decreases, the\nmagnitude of disturbances also diminishes. Since the H∞\nnorm is inversely proportional to the disturbance magnitude,\nscenarios with weaker wind conditions tend to exhibit higher\nH∞norm values. RDDPG exhibits a smaller H∞norm than\nthe other algorithms; this implies that RDDPG effectively\nminimizes the cost in disturbance scenarios\nTable III presents the mean and standard deviation of the\npositional errors between the target and the quadrotor at time\nstep k. DDPG performs best in low wind speed scenarios,\nRDDPG achieves lower errors as wind speed increases.\nAlthough DDPG exhibits smaller distance errors, its higher\ntotal cost in Table I suggests that the quadrotor experiences\nlarge angular deviations and significant oscillations. These\noscillations make the DDPG controller more vulnerable to\nstrong winds, ultimately causing instability in high-wind\nscenarios.\n\n\nTABLE III\nTHE MEAN AND STANDARD DEVIATION OF THE POSITION ERROR\n||epos\nk\n||2 BETWEEN THE TARGET AND THE QUADROTOR\nBold indicates the best performance algorithm\nAlgorithm\nTrajectory\n(1)\n(2)\n(3)\n±5 wind speed scenario\nRDDPG\n0.17 ± 0.02\n0.16 ± 0.02\n0.17 ± 0.03\nDDPG\n0.13 ± 0.05\n0.12 ± 0.05\n0.12 ± 0.07\nTD3\n0.35 ± 0.28\n0.21 ± 0.11\n0.26 ± 0.17\nSAC\n0.27 ± 0.16\n0.20 ± 0.07\n0.22 ± 0.09\nPPO\n0.91 ± 0.65\n0.86 ± 0.43\n0.76 ± 0.51\n±10 wind speed scenario\nRDDPG\n0.18 ± 0.04\n0.18 ± 0.03\n0.19 ± 0.05\nDDPG\n0.16 ± 0.06\n0.15 ± 0.07\n0.15 ± 0.08\nTD3\n0.48 ± 0.39\n0.38 ± 0.32\n0.42 ± 0.33\nSAC\n0.29 ± 0.16\n0.25 ± 0.11\n0.27 ± 0.13\nPPO\n1.16 ± 0.82\n1.13 ± 0.78\n1.14 ± 0.77\n±15 wind speed scenario\nRDDPG\n0.22 ± 0.07\n0.21 ± 0.07\n0.22 ± 0.07\nDDPG\n0.23 ± 0.15\n0.23 ± 0.15\n0.23 ± 0.15\nTD3\n0.56 ± 0.42\n0.52 ± 0.37\n0.55 ± 0.40\nSAC\n0.46 ± 0.28\n0.43 ± 0.26\n0.43 ± 0.27\nPPO\n1.50 ± 1.16\n1.50 ± 1.12\n1.50 ± 1.14\n±20 wind speed scenario\nRDDPG\n0.22 ± 0.07\n0.21 ± 0.07\n0.22 ± 0.07\nDDPG\n0.37 ± 0.36\n0.36 ± 0.36\n0.35 ± 0.33\nTD3\n0.60 ± 0.47\n0.57 ± 0.44\n0.60 ± 0.46\nSAC\n0.51 ± 0.36\n0.51 ± 0.37\n0.51 ± 0.38\nPPO\n1.60 ± 1.28\n1.57 ± 1.26\n1.53 ± 1.20\nVII. CONCLUSION\nThis paper proposes RDPG, which combines the con-\ncept of the H∞control problem and a two-player zero-\nsum dynamic game framework, to overcome the robustness\nproblem of DRL algorithms. In RDPG, the user aims to\nminimize the cost function while the adversary tries to\nmaximize it. Additionally, we introduce the RDDPG algo-\nrithm, which integrates RDPG with TD3 to improve both\nrobustness and learning efficiency. In order to evaluate the\nrobustness of RDDPG, we implement it on the tracking tasks\nof the quadrotor. Experimental results show that RDDPG can\nachieve the optimal control policy and maintain stability in\nvarious environments with disturbances. This demonstrates\nthat RDDPG outperforms other DRL methods in terms of\nrobustness.\nREFERENCES\n[1] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Ried-\nmiller, M. (2014, January). Deterministic policy gradient algorithms.\nIn International conference on machine learning (pp. 387-395). PMLR.\n[2] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018, July). Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning\nwith a stochastic actor. In International conference on machine learning\n(pp. 1861-1870). Pmlr.\n[3] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov,\nO. (2017). Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\n[4] Fujimoto, S., Hoof, H., and Meger, D. (2018, July). Addressing\nfunction approximation error in actor-critic methods. In International\nconference on machine learning (pp. 1587-1596). PMLR.\n[5] Lillicrap, T. P. (2015). Continuous control with deep reinforcement\nlearning. arXiv preprint arXiv:1509.02971.\n[6] Huang, J., and Lin, C. F. (1995). Numerical approach to computing\nnonlinear H-infinity control laws. Journal of Guidance, Control, and\nDynamics, 18(5), 989-994.\n[7] Rigatos, G., Siano, P., Wira, P., and Profumo, F. (2015). Nonlinear\nH-infinity feedback control for asynchronous motors of electric trains.\nIntelligent Industrial Systems, 1, 85-98.\n[8] Al-Tamimi, A., Lewis, F. L., and Abu-Khalaf, M. (2007). Model-\nfree Q-learning designs for linear discrete-time zero-sum games with\napplication to H-infinity control. Automatica, 43(3), 473-481.\n[9] Le Ballois, S., and Duc, G. (1996). H-infinity control of an Earth\nobservation satellite. Journal of guidance, control, and dynamics,\n19(3), 628-635.\n[10] Modares, Hamidreza, Frank L. Lewis, and Mohammad-Bagher\nNaghibi Sistani. Online solution of nonquadratic two-player zero-sum\ngames arising in the H-infinity control of constrained input systems.\nInternational Journal of Adaptive Control and Signal Processing 28.3-5\n(2014): 232-254.\n[11] Li, Jie, et al. ”Relaxed Policy Iteration Algorithm for Nonlinear\nZero-Sum Games With Application to H-Infinity Control.” IEEE\nTransactions on Automatic Control 69.1 (2023): 426-433.\n[12] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang,\nA., Guez, A., ... and Hassabis, D. (2017). Mastering the game of go\nwithout human knowledge. nature, 550(7676), 354-359.\n[13] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., ... and Hassabis, D. (2015). Human-level control\nthrough deep reinforcement learning. nature, 518(7540), 529-533.\n[14] Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., ...\nand Levine, S. (2018, October). Scalable deep reinforcement learning\nfor vision-based robotic manipulation. In Conference on robot learning\n(pp. 651-673). PMLR.\n[15] Wang, S., Jia, D., and Weng, X. (2018). Deep reinforcement learning\nfor autonomous driving. arXiv preprint arXiv:1811.11329.\n[16] Yuste, P. C., Mart´ınez, J. A. I., and de Miguel, M. A. S. (2024).\nSimulation-based evaluation of model-free reinforcement learning\nalgorithms for quadcopter attitude control and trajectory tracking.\nNeurocomputing, 608, 128362.\n[17] Panerati, J., Zheng, H., Zhou, S., Xu, J., Prorok, A., and Schoellig,\nA. P. (2021, September). Learning to fly—a gym environment with\npybullet physics for reinforcement learning of multi-agent quadcopter\ncontrol. In 2021 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS) (pp. 7512-7519). IEEE.\n[18] Deshpande, A. M., Minai, A. A., and Kumar, M. (2021). Robust deep\nreinforcement learning for quadcopter control. IFAC-PapersOnLine,\n54(20), 90-95.\n[19] Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. (2017, July).\nRobust adversarial reinforcement learning. In International conference\non machine learning (pp. 2817-2826). PMLR.\n[20] Tessler, C., Efroni, Y., and Mannor, S. (2019, May). Action robust\nreinforcement learning and applications in continuous control. In In-\nternational Conference on Machine Learning (pp. 6215-6224). PMLR.\n[21] Bas¸ar, Tamer, and Pierre Bernhard. H-infinity optimal control and\nrelated minimax design problems: a dynamic game approach. Springer\nScience and Business Media, 2008.\n[22] Basar, Tamer. ”A dynamic games approach to controller design:\nDisturbance rejection in discrete time.” Proceedings of the 28th IEEE\nConference on Decision and Control,. IEEE, 1989.\n[23] Stoorvogel, A. A., and Weeren, A. J. (1994). The discrete-time riccati\nequation related to the H∞control problem. IEEE Transactions on\nAutomatic Control, 39(3), 686-691.\n[24] Long, J., Yu, W., Li, Q., Wang, Z., Lin, D., and Pang, J. (2024). Learn-\ning H-Infinity Locomotion Control. arXiv preprint arXiv:2404.14405.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21057v1.pdf",
    "total_pages": 8,
    "title": "Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control",
    "authors": [
      "Taeho Lee",
      "Donghwan Lee"
    ],
    "abstract": "Practical control systems pose significant challenges in identifying optimal\ncontrol policies due to uncertainties in the system model and external\ndisturbances. While $H_\\infty$ control techniques are commonly used to design\nrobust controllers that mitigate the effects of disturbances, these methods\noften require complex and computationally intensive calculations. To address\nthis issue, this paper proposes a reinforcement learning algorithm called\nRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\\infty$\ncontrol problem as a two-player zero-sum dynamic game. In this formulation, one\nplayer (the user) aims to minimize the cost, while the other player (the\nadversary) seeks to maximize it. We then employ deterministic policy gradient\n(DPG) and its deep reinforcement learning counterpart to train a robust control\npolicy with effective disturbance attenuation. In particular, for practical\nimplementation, we introduce an algorithm called robust deep deterministic\npolicy gradient (RDDPG), which employs a deep neural network architecture and\nintegrates techniques from the twin-delayed deep deterministic policy gradient\n(TD3) to enhance stability and learning efficiency. To evaluate the proposed\nalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with\nfollowing a predefined path in a disturbance-prone environment. The\nexperimental results demonstrate that the proposed method outperforms other\ncontrol approaches in terms of robustness against disturbances, enabling\nprecise real-time tracking of moving targets even under severe disturbance\nconditions.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}