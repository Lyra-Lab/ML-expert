{
  "id": "arxiv_2502.20111v1",
  "text": "MITracker: Multi-View Integration for Visual Object Tracking\nMengjie Xu1‚àó\nYitao Zhu1‚àó\nHaotian Jiang1\nJiaming Li1\nZhenrong Shen2\nSheng Wang1,2\nHaolin Huang1\nXinyu Wang1\nQing Yang1,3\nHan Zhang1,3\nQian Wang1,3‚Ä†\n1School of Biomedical Engineering & State Key Laboratory of\nAdvanced Medical Materials and Devices, ShanghaiTech University\n2School of Biomedical Engineering, Shanghai Jiao Tong University\n3Shanghai Clinical Research and Trial Center\n{xumj2023, zhuyt, jianght2023, lijm2024}@shanghaitech.edu.cn\n{zhenrongshen, wsheng}@sjtu.edu.cn\n{huanghl2023, wangxy42023, yangqing, zhanghan2, qianwang}@shanghaitech.edu.cn\nAbstract\nMulti-view object tracking (MVOT) offers promising solu-\ntions to challenges such as occlusion and target loss, which\nare common in traditional single-view tracking.\nHow-\never, progress has been limited by the lack of comprehen-\nsive multi-view datasets and effective cross-view integra-\ntion methods. To overcome these limitations, we compiled\na Multi-View object Tracking (MVTrack) dataset of 234K\nhigh-quality annotated frames featuring 27 distinct objects\nacross various scenes. In conjunction with this dataset, we\nintroduce a novel MVOT method, Multi-View Integration\nTracker (MITracker), to efficiently integrate multi-view ob-\nject features and provide stable tracking outcomes.\nMI-\nTracker can track any object in video frames of arbitrary\nlength from arbitrary viewpoints. The key advancements of\nour method over traditional single-view approaches come\nfrom two aspects:\n(1) MITracker transforms 2D image\nfeatures into a 3D feature volume and compresses it into\na bird‚Äôs eye view (BEV) plane, facilitating inter-view in-\nformation fusion; (2) we propose an attention mechanism\nthat leverages geometric information from fused 3D fea-\nture volume to refine the tracking results at each view. MI-\nTracker outperforms existing methods on the MVTrack and\nGMTD datasets, achieving state-of-the-art performance.\nThe code and the new dataset will be available at mii-\nlaboratory.github.io/MITracker.\n1. Introduction\nVisual object tracking, a core computer vision task, involves\nestimating class-agnostic target positions across video se-\n* These authors contributed equally. ‚Ä† Corresponding author.\n1\nùêæ\n‚Ä¶\n3D Feature Volume Space\nTarget Visible\nProjection\nRefinement\nTrajectory\nFigure 1. Overview of MITracker‚Äôs multi-view integration mech-\nanism. Given K camera views, our method projects features from\nviews with visible targets into a 3D feature volume space, which is\nthen used to refine tracking in views where the target is occluded.\nquences.\nThis technique is crucial for applications such\nas augmented reality and autonomous driving, where it\nis essential to continuously monitor and predict the tra-\njectories of various objects within dynamic environments.\nDespite notable advances in single-view tracking through\nSiamese networks [10, 25] and transformers [3, 9, 46],\nsignificant challenges persist ‚Äì particularly occlusions, ap-\npearance changes, and target loss. While approaches like\nRTracker [22] attempt to address these challenges by deter-\nmining target loss and detection mechanisms, the inherent\nlimitations of single viewpoint information remain a funda-\nmental constraint.\nMulti-camera systems offer a promising solution by\nleveraging complementary viewpoints to maintain contin-\nuous tracking, particularly for handling occlusions through\ncamera overlap [48]. However, the development of effective\narXiv:2502.20111v1  [cs.CV]  27 Feb 2025\n\n\nmulti-view object tracking (MVOT) faces several critical\nchallenges. First, existing multi-view datasets are largely\nrestricted to specific object categories like humans or birds\n[15, 40], limiting their applicability for generic object track-\ning. Second, current MVOT approaches [17, 19, 42] primar-\nily focus on tracking specific categories of objects using de-\ntection and re-identification methods, which are not suitable\nfor class-agnostic object tracking. Even when attempting\nto track generic objects across multiple views, researchers\nhave to rely on single-view datasets for training due to the\nabsence of comprehensive multi-view data [38]. This limi-\ntation severely restricts models‚Äô ability to understand com-\nplex spatial relationships and appearance variations across\ndifferent viewpoints.\nTo address these challenges, we first construct a Multi-\nView object Tracking (MVTrack) dataset.\nMVTrack\ndataset contains 234K frames captured from 3-4 cameras,\nwith precise bounding box (BBox) annotations covering 27\ndistinct objects across 9 challenging tracking attributes such\nas occlusion and deformation. Unlike existing datasets such\nas GMTD [38] which only provides testing data, MVTrack\ndataset offers both training and evaluation sets, enabling de-\nvelopment and validation of MVOT models.\nTo effectively utilize MVTrack dataset, we propose\na novel MVOT method named Multi-View Integration\nTracker (MITracker) for tracking any object in video\nframes of arbitrary length from arbitrary viewpoints. As\nillustrated in Figure 1, MITracker can integrate multi-view\nfeatures into a unified 3D feature volume and further refine\ntracking in occluded views, thus producing robust tracking\noutcomes. The framework of MITracker consists of two im-\nportant modules: View-Specific Feature Extraction and\nMulti-View Integration. The first module employs a Vi-\nsion Transformer (ViT) [12] to extract view-specific fea-\ntures of the target object from the current search frame in\na streaming manner, where the target object is indicated by\na reference frame. The second module constructs a 3D fea-\nture volume by fusing 2D features from multiple views and\nleveraging bird‚Äôs eye View (BEV) guidance, which signif-\nicantly enhances the model‚Äôs spatial understanding. This\n3D feature volume is then deployed in spatial-enhanced at-\ntention to improve tracking accuracy. MITracker allows for\nthe maintenance of stable tracking results and demonstrates\nstrong recovery capabilities in challenging cases such as oc-\nclusions and out of view objects.\nIn summary, our main contributions are as follows:\n‚Ä¢ We introduce MVTrack, a large-scale multi-view track-\ning dataset containing 234K frames from 3-4 calibrated\ncameras. It has precise BBox annotations of 27 object\ncategories across 9 challenging tracking attributes, which\nprovides the first comprehensive benchmark for train-\ning class-agnostic MVOT methods and enriches the ap-\nproaches for evaluating these methods.\n‚Ä¢ We propose MITracker, a novel multi-view tracking\nmethod that constructs BEV-guided 3D feature volumes\nto enhance spatial understanding and utilize a spatial-\nenhanced attention mechanism to enable robust recovery\nfrom target loss in specific views.\n‚Ä¢ Our extensive experiments demonstrate that MITracker\nachieves state-of-the-art (SOTA) performance on both\nMVTrack and GMTD datasets, improving recovery rate\nfrom 56.7% to 79.2% to reduce target loss in challenging\nscenarios.\n2. Related Work\n2.1. Visual Object Tracking\nVisual object tracking has garnered significant research in-\nterest, leading to many breakthroughs. Numerous single-\nview datasets [13, 20, 21, 23, 24, 27, 29, 36, 39] span a wide\nrange of categories, aimed at enhancing models‚Äô ability to\ntrack arbitrary objects. With the expansion of these datasets,\nsingle-view tracking methods have also advanced rapidly.\nEarly approaches based on Siamese networks [10, 25] use\nCNNs to extract features from reference and search regions,\nestablishing a linear relationship between them. More re-\ncent works have incorporated transformers for enhanced\nfeature extraction [9, 43], while others introduce attention\nmodules to enable nonlinear relationships [5]. However,\nthese methods lack temporal continuity as they process each\nframe independently. Algorithms like dynamic template up-\ndating [6] and spatio-temporal trajectory tracking [37, 46]\nhave shown promising results in addressing this issue. De-\nspite these advancements, recovering from target loss re-\nmains a significant challenge.\nTo re-track the target after a tracking failure, RTracker\n[22] leverages a tree-structured memory system to detect\ntarget loss and a dedicated detector for self-recovery. How-\never, this approach is constrained by its complex design\nand the detector‚Äôs reliance on specific categories. Single-\nview tracking suffers from inherent limitations due to its\nrestricted field of view, which is an inevitable challenge. In\ncontrast, GMT [38] incorporates multi-view tracking within\na single-view training framework.\nThis limits its capac-\nity to effectively model the intricate relationships between\nmulti-view appearances and background contexts in the real\nworld.\n2.2. Multi-View Object Tracking\nMVOT provides more comprehensive information about the\ntarget, effectively addressing issues such as occlusion. To\nleverage multi-view information, various fusion strategies\nhave been developed for target association across view-\npoints. Some approaches establish multi-view relationships\nby projecting detection results onto a BEV plane [42]. How-\never, this method is prone to detection errors, especially\n\n\nBenchmark\nAim\nCamera\nClass\nTotal\nVideos\nMean\nAbsent\nAtt.\nOverlap\nMove\nCalib.\nFrames\nFrames\nlabel\nOTB 2015 [39]\nEva.\n1\n16\n59K\n100\n590\n‚úó\n11\n-\n-\n-\nNfS [23]\nEva.\n1\n17\n383K\n100\n3,830\n‚úó\n9\n-\n-\n-\nVOT 2017 [24]\nEva.\n1\n24\n21K\n60\n356\n‚úó\n24\n-\n-\n-\nTrackingNet [27]\nTra./Eva.\n1\n27\n14.43M\n30,643\n471\n‚úó\n15\n-\n-\n-\nLaSOT [13]\nTra./Eva.\n1\n70\n3.52M\n1,400\n2,053\n‚úì\n14\n-\n-\n-\nGOT-10k [21]\nTra./Eva.\n1\n563\n1.45M\n9,935\n149\n‚úì\n6\n-\n-\n-\nTNL2K [36]\nTra./Eva.\n1\n-\n1.24M\n2,000\n622\n‚úì\n17\n-\n-\n-\nVideoCube [20]\nTra./Eva.\n1\n89\n7.46M\n500\n4,008\n‚úì\n12\n-\n-\n-\nVastTrack [29]\nTra./Eva.\n1\n2,115\n4.20M\n50,610\n83\n‚úì\n10\n-\n-\n-\nCAMPUS‚Ä† [42]\nEva.\n4\n1\n83K\n16\n5,188\n-\n-\n‚úì\n‚úó\n‚úó\nWildtrack‚Ä† [4]\nEva.\n7\n1\n2.80K\n7\n401\n-\n-\n‚úì\n‚úó\n‚úì\nMMPTRACK‚Ä† [15]\nTra./Eva.\n4-6\n1\n2.98M\n-\n-\n-\n-\n‚úì\n‚úó\n‚úì\nDIVOTrack‚Ä† [16]\nTra./Eva.\n3\n1\n81K\n75\n1,080\n-\n-\n‚úì\n‚úì\n‚úó\nGMTD [38]\nEva.\n2-3\n8\n18K\n23\n764\n‚úó\n6\n‚úì\n‚úì\n‚úó\nMVTrack (Ours)\nTra./Eva.\n3-4\n27\n234K\n260\n901\n‚úì\n9\n‚úì\n‚úó\n‚úì\nTable 1. Comparison of current datasets for object tracking. The upper part of the table focuses on single-view datasets, while the lower\npart is dedicated to multi-view datasets. Datasets marked with ‚Ä† are designed for multi-object tracking, the others are for visual object\ntracking. ‚ÄòTra.‚Äô and ‚ÄòEva.‚Äô indicate training and evaluation, respectively. ‚Äò-‚Äô denotes not available, ‚ÄòAtt.‚Äô stands for attributes, ‚ÄòOverlap‚Äô\nrefers to the overlapping of multi-view images, ‚ÄòMove‚Äô indicates the movement of the camera position, and ‚ÄòCalib.‚Äô represents calibration.\nwith occlusion. To tackle this issue, methods [18, 19] im-\nprove it by incorporating multi-view information at the de-\ntection stage (known as early fusion) through feature pro-\njection onto the ground plane, enabling the model to cap-\nture richer interaction information across views. Building\non these approaches, methods such as [17, 33, 34] map fea-\ntures into 3D space at multiple heights, which reduces dis-\ntortions caused by ground plane projection.\nWhile these methods enhance multi-view tracking, ex-\nisting multi-view datasets are relatively scarce and often\nlimited to specific target categories, such as pedestrians\n[4, 15, 16]. This results in a reliance on detection outcomes,\nlimiting the ability to track arbitrary objects. GMTD [38]\nexpands the target to multiple categories, but its scale re-\nmains small, primarily designed for evaluation purposes.\nThere is a pressing need for a multi-view tracking dataset\ncapable of handling arbitrary objects.\n3. MVTrack Dataset\nMVTrack dataset is designed to fill the gaps in the field of\nMVOT and has received approval for data collection from\nan Institutional Review Board. As shown in Table 1, com-\npared to single-view datasets, we maintain competitive class\ndiversity while adding multi-view capabilities. Compared\nto MVOT datasets, we provide significantly richer object\ncategories (27 vs 1-8 classes) and more videos (260) with\npractical camera setups (3-4 views). MVTrack dataset is the\nonly dataset that combines multi-view tracking, rich object\ncategories, absent label annotations, and calibration infor-\nmation.\nData Collection.\nWe employ a multi-camera system\n(a) umbrella1-1: Deformation, Aspect Ratio Change and Scale Variation.\n(b) phone3-3: Low Resolution, Fully Occlusion and Partial Occlusion.\n(c) tenis5-1: Out of View, Motion Blur and Background Clutter.\nFigure 2. Example sequences, annotations, and their correspond-\ning tracking attributes in the MVTrack dataset.\nfor data collection, consisting of 3 or 4 time-synchronized\nAzure Kinect cameras. All video sequences are recorded at\na resolution of 1920√ó1080 with 30 FPS. These cameras are\npositioned to ensure multiple overlapping views, and their\nintrinsic parameters are provided by the manufacturer. The\nextrinsic parameters are obtained through calibration and\nfinely adjusted using MeshLab [7, 8]. With this calibration\ninformation, we set the central point of the scene as the ori-\ngin of the world coordinate system, aligning all viewpoints\nto this unified coordinate system.\nData Annotation.\nMVTrack dataset provides frame-\nlevel annotations, including 2D object BBoxes and ground\ncoordinate annotations in a unified coordinate system (i.e.,\n\n\nBEV annotations). Following an annotation strategy simi-\nlar to LaSOT [13], where for each visible frame, an axis-\naligned BBox tightly encloses the target, and an ‚Äòinvisi-\nble‚Äô label is assigned for the invisible target. The BBox\nannotations are generated semi-automatically, with trackers\n[6, 41, 46] used for initial labeling. The machine-generated\nannotations are then manually adjusted and double-checked\nfor accuracy. Subsequently, using camera calibration pa-\nrameters, the 2D object BBoxes from multiple viewpoints\nare projected into the unified coordinate system to compute\nthe BEV coordinates.\nChallenging Attributes. In our dataset, we particularly\nfocus on 9 common tracking challenges to better assess\ntracker performance: Background Clutter, Motion Blur,\nPartial Occlusion, Full Occlusion, Out of View, Deforma-\ntion, Low Resolution, Aspect Ratio Change, and Scale Vari-\nation.\nMore specifically, Figure 2 illustrates three challenging\nsamples in the MVTrack dataset. Figure 2a shows signif-\nicant deformation and scale changes of an umbrella being\nopened. Figure 2b demonstrates the tracking of small, low-\nresolution objects like a mobile phone under full and partial\nocclusions. Figure 2c highlights the impact of fast motion\ncausing blur when tracking a tennis ball. These attributes\ncan significantly aid in training the model to achieve more\nrobust results.\nStatistical Analysis. MVTrack dataset consists of five\nindoor scenes, captured with a total of ten sets of calibration\nparameters. It covers 27 everyday objects, ranging from\nsmall objects like pens to larger objects such as umbrellas.\nThe dataset includes 68 sets of multi-view data, comprising\n260 videos and a total of 234,430 frames.\nWe divide the dataset into training, validation, and test-\ning sets. The training set consists of 196 videos and 180K\nframes, while the validation set contains 30 videos and\n28K frames. The testing set comprises 34 videos and 26K\nframes. We include an unseen scene in the validation and\ntesting sets that are distinct from the scenes in the training\nset. Furthermore, the testing set includes both object cate-\ngories that appear in the training set and new object cate-\ngories not present during training. This enables evaluation\nof the model‚Äôs performance across various targets and set-\ntings.\nMore details about MVTrack dataset are provided in the\nAppendix.\n4. MITracker\nWe propose MITracker, a novel multi-view tracking frame-\nwork that robustly tracks class-agnostic objects across mul-\ntiple camera views. As illustrated in Figure 3, MITracker\nconsists of two main components: (1) a view-specific fea-\nture extraction module (Sec. 4.1) that encodes frame fea-\ntures and generates single-view tracking results in a stream-\ning fashion, and (2) a multi-view integration module (Sec.\n4.2) that fuses multi-view features with BEV guidance and\nrefines view-specific feature with a spatial-enhanced atten-\ntion mechanism.\n4.1. View-Specific Feature Extraction\nAs shown in Figure 3a, this module processes the video\nstream from a specific viewpoint k, and extracts target-\naware features in the search frame at a timepoint t based\non the reference frame that indicates the target object.\nView-Specific Encoder. We employ ViT as the back-\nbone of our view-specific encoder.\nThe visual inputs of\nthe view-specific encoder consist of a search frame S ‚àà\nR3√óHs√óWs and a reference frame R ‚ààR3√óHr√óWr. As the\ntransformer block processes a series of tokens, we segment\nthe frames into non-overlapping patches with p √ó p resolu-\ntion. The search and reference frames are individually em-\nbedded into a token sequence, represented by IS ‚ààRNs√óD\nand IR ‚ààRNr√óD, where D is the hidden dimension, Ns =\nHsWs\np2\nis the number of search tokens, and Nr = HrWr\np2\nis\nthe number of reference tokens.\nTo ensure temporal continuity between frames, akin to\nthe method utilized in ODTrack [46], two specialized tem-\nporal tokens are also included in the inputs of the view-\nspecific encoder to facilitate the propagation of temporal\ninformation. Specifically, at any given time t, a learnable to-\nken Tt is randomly initialized, which is designed to capture\ntemporal information of the current frame. Concurrently,\nwe incorporate a token Tt‚àí1 that carries temporal informa-\ntion from the preceding frame, which leverages historical\nfeatures to enhance tracking accuracy and continuity. The\ninput token sequence of our view-specific encoder can be\nformulated as the composition of the visual and temporal\ntokens f = [Tt, Tt‚àí1, IR, IS], while the output token se-\nquence is denoted as f ‚Ä≤ = [T ‚Ä≤\nt, T ‚Ä≤\nt‚àí1, I‚Ä≤\nR, I‚Ä≤\nS].\nAfter obtaining f ‚Ä≤, T ‚Ä≤\nt is used to compute attention\nweights in conjunction with I‚Ä≤\nS to utilize temporal informa-\ntion for adjustments, which can be described as follows:\nIU = I‚Ä≤\nS ¬∑ (I‚Ä≤\nS √ó (T ‚Ä≤\nt)‚ä§),\n(1)\nwhere IU represents the extracted feature that encapsulates\nattention focused on the target object in the search frame.\nSingle-View Tracking Result. We employ a BBox head\nbased on the CenterNet architecture [47] to output tracking\nresults from the extracted feature IU. This head comprises\nthree distinct sub-networks, each designed to compute the\nclassification score map, BBox dimensions, and offset sizes,\nrespectively. The highest-scoring position on the classifi-\ncation score map is identified as the target location. This\nconfiguration establishes a robust framework capable of ef-\nfectively handling single-view visual object tracking tasks.\nTo facilitate further multi-view integration, we also ap-\nply convolutional layers to map IU to a 2D feature map\n\n\nùê∏\nViT\nùêºùëÖ\nView-Specific Feature Extraction\nMulti-View Integration\nPervious \nTemporal Token\nùëáùë°‚àí1\nReference Frame\nCurrent \nSearch Frame\n(c) Spatial-Enhanced Attention\nBBox\nHead\nCNN\nAttn.\nUnrefined Result\nBEV Head\n(a)  View-Specific Encoder\nùêπ2ùê∑\n1\nùêπ2ùê∑\n2\nùêπ2ùê∑\nùêæ\n(b) 3D Feature Proj. and Agg.\nPatch & Pos embed\n‚Ä¶\nTransformer Blocks\n‚Ä¶\nùêºùëÜ\nùëáùë°\nùêºùëÜ\n‚Ä≤\nùêºùëÖ\n‚Ä≤\nùëáùë°‚àí1\n‚Ä≤\nùëáùë°\n‚Ä≤\nùêºùëà\nùêπ3ùê∑\nùêπ3ùê∑\n‚Ä≤\nRefined Results\nBEV Map\nùëá3ùê∑\nùêºùëà\n1\nùêºùëà\n2\nùêºùëà\nùêæ\nBBox Head\nAggregation\nProjection\nView k, Time t\nùêπ2ùê∑\nùëò\nCNN\nView k\nView 1\nView 2\nView K\nFigure 3. The framework of MITracker. (a) The view-specific feature extraction module employs a ViT that utilizes temporal tokens to\nprocess each view independently, outputting unrefined results that can be further improved by multi-view information. The multi-view\nintegration module contains (b) 3D feature volume construction that aggregates features into 3D space with BEV guidance and (c) spatial-\nenhanced attention that refines tracking results by 3D spatial information.\nwith original image size, denoted as F2D ‚ààR32√óHs√óWs.\nThis establishes a pixel-wise correspondence between the\nextracted feature and the search image, which is crucial for\nreconstructing the 3D feature space in the following section.\n4.2. Multi-View Integration\nTo effectively integrate 2D feature maps F 1\n2D, F 2\n2D, ..., F K\n2D\nfrom K viewpoints, we project them into a 3D feature\nspace and then aggregate them under the supervision of\nBEV guidance.\nFinally, we embed the aggregated fea-\nture to a 3D-aware token to refine all view-specific features\nI1\nU, I2\nU, ..., IK\nU via spatial-enhanced attention, thus produc-\ning stable tracking results across different viewpoints.\n3D Feature Projection. As illustrated in Figure 3b, we\nconstruct a 3D feature volume of size X √ó Y √ó Z, where\n(X, Y ) represents the horizontal plane and Z axis denotes\nthe vertical direction following [17, 44]. For a viewpoint k,\nwe project the (u, v) coordinates in F k\n2D to (x, y, z) coordi-\nnates in the 3D feature volume by the formula below:\nÔ£´\nÔ£≠\nu\nv\n1\nÔ£∂\nÔ£∏= CK[CR|Ct]\nÔ£´\nÔ£¨\nÔ£¨\nÔ£≠\nx\ny\nz\n1\nÔ£∂\nÔ£∑\nÔ£∑\nÔ£∏,\n(2)\nwhere CK represents the camera‚Äôs intrinsic matrix, CR de-\nnotes the rotation matrix describing the camera‚Äôs orienta-\ntion, and Ct is the translation vector specifying the camera‚Äôs\nposition in space. Upon establishing the mapping matrix,\nwe implement bilinear sampling to populate the 3D feature\nvolume. In scenarios that involve multiple viewpoints, we\ncompute the average of the mapped values from each view\nto ensure consistency. Consequently, we derive a 3D feature\nvolume represented as F3D ‚ààR32√óX√óY √óZ.\n3D Feature Aggregation. To better integrate multi-view\nspatial information, we apply 1D convolutional layers to\naggregate features along the Z-axis of F3D, resulting in\nF ‚Ä≤\n3D ‚ààR32√óX√óY , thereby consolidating spatial informa-\ntion within the (X, Y ) plane. Subsequently, a classifica-\ntion head (i.e., BEV head) is employed to generate a BEV\nscore map from F ‚Ä≤\n3D. This BEV map delineates the ob-\nject positions on the horizontal plane, thereby imposing su-\npervision constraints on information fusion across multiple\nviewpoints. This integrative approach allows for precise lo-\ncalization and mapping within multi-view scenarios.\nSpatial-Enhanced Attention.\nBEV guidance for the\naggregated 3D feature F ‚Ä≤\n3D only implicitly constrains the\noriginal single-view output, but it is insufficient to address\nthe potential target loss issue due to the lack of direct\nsupervision on tracking results.\nTo remedy this, we in-\ntroduce spatial-enhanced attention to explicitly incorporate\nF ‚Ä≤\n3D into the tracking process as shown in Figure 3c.\nWe first use convolutional layers to embed F ‚Ä≤\n3D into a\n3D-aware token T3D ‚ààR1√óD, which inherits multi-view\nspatial information.\nFor all the K viewpoints, we then\nindividually concatenate T3D with their unrefined features\nI1\nU, I2\nU, ..., IK\nU produced by the view-specific encoder. For a\nviewpoint k, a series of transformer blocks take in its com-\nposite token sequence (T3D, Ik\nU) and refine them using at-\ntention mechanisms that leverage fused 3D spatial informa-\ntion. A final BBox head outputs the refined tracking results,\nwhere potential errors such as target loss are corrected.\n\n\nDataset\nMVTrack\nGMTD\nMethod\nSingle-View\nMulti-View\nSingle-View\nAUC(%)\nPNorm(%)\nP(%)\nAUC(%)\nPNorm(%)\nP(%)\nAUC(%)\nPNorm(%)\nP(%)\nDiMP [1]\n43.14\n59.52\n53.13\n35.77\n49.04\n51.65\n52.71\n68.24\n66.04\nPrDiMP [10]\n48.61\n66.09\n58.93\n38.49\n54.68\n57.95\n57.76\n76.21\n70.49\nTrDiMP [35]\n50.54\n67.67\n60.44\n39.71\n55.31\n58.52\n59.51\n78.94\n73.48\nMixFormer [9]\n57.59\n75.44\n67.72\n43.29\n58.07\n62.70\n62.03\n82.60\n78.48\nOSTrack [43]\n60.04\n77.72\n70.06\n49.10\n65.19\n67.34\n58.44\n77.37\n73.23\nGRM [14]\n52.53\n69.91\n62.31\n41.47\n57.33\n58.76\n55.67\n74.02\n70.27\nSeqTrack [6]\n58.37\n76.63\n69.03\n43.88\n59.11\n63.60\n62.97\n83.20\n79.32\nARTrack [37]\n53.23\n70.25\n62.49\n42.52\n58.00\n60.50\n59.56\n78.12\n74.23\nHIPTrack [2]\n60.45\n78.92\n70.53\n48.43\n63.69\n66.26\n62.20\n80.62\n76.94\nEVPTrack [32]\n61.37\n79.76\n71.97\n46.36\n61.84\n67.20\n63.89\n83.76\n79.93\nAQATrack [41]\n61.93\n80.00\n72.69\n45.24\n59.76\n65.33\n63.57\n83.04\n79.44\nODTrack [46]\n63.36\n82.25\n74.46\n48.05\n63.55\n67.70\n61.43\n82.37\n78.35\nSAM2‚àó[30]\n46.49\n63.12\n56.82\n39.08\n53.49\n57.34\n59.88\n74.66\n73.25\nSAM2Long‚àó[11]\n55.30\n72.84\n67.40\n45.40\n59.12\n65.30\n62.80\n78.60\n77.40\nMITracker\n68.57\n88.77\n80.93\n71.13\n91.87\n83.95\n65.96\n87.05\n82.07\nTable 2. Comparison with SOTA methods on the MVTrack and GMTD datasets. MITracker is for multi-view tracking, while others are\nsingle-view methods. Methods with ‚àóuse pre-trained weights without fine-tuning. Best results are bolded, second-best are underlined.\n5. Experiments\n5.1. Dataset\nIn addition to MVTrack dataset, we use two external\ndatasets for training and evaluation, which are detailed as\nfollows.\n‚Ä¢ GOT10K. GOT-10K [21] is a large and diverse dataset\nwith a wide range of object categories. Its training set\ncontains 9,335 videos across 480 moving object cate-\ngories.\n‚Ä¢ GMTD. GMTD [38] is a multi-view tracking test set with\n10 scenes, captured by 2-3 uncalibrated cameras in indoor\nand outdoor settings. It includes 6 target types and various\ntracking challenges.\n5.2. Implementation Details\nLoss Function. For the BBox head, we employ a weighted\nfocal loss [26] Lcls for classification, along with the gener-\nalized intersection over union loss [31] Lgiou and L1 loss for\nBBox regression. Additionally, a focal loss Lbev is utilized\nfor BEV map supervision. The overall loss function of the\nmodel is formulated as follows:\nLtrack = Lcls + ŒªgiouLgiou + ŒªL1L1 + ŒªbevLbev,\n(3)\nwhere Œªgiou = 5, ŒªL1 = 2, and Œªbev = 0.1 are the coeffi-\ncients that balance the contributions from each loss .\nTraining Setup. We initialize our view-specific encoder\nwith pre-trained DINOv2 [28] parameters using the ViT-\nbase model [12]. For the visual inputs, we set the refer-\nence frame with 182 √ó 182 pixels, and the search frame\nwith 364 √ó 364 pixels. We utilize the camera parameters\nfrom the dataset for projection, with the 3D feature volume\nhaving dimensions X = 200, Y = 200, and Z = 3.\nOur training process consists of two stages. In the first\nstage, we only train the view-specific feature extraction\nmodule. Specifically, we train the view-specific encoder\nand BBox head using single-view inputs from GOT-10K\nand MVTrack datasets until convergence. For each view-\npoint, we include one reference frame and two random\nsearch frames from 200 frame interval in each iteration,\nthus promoting temporal information propagation between\nframes. In the second stage, we fine-tune the view-specific\nencoder and train the entire framework using multi-view\ndata from the MVTrack dataset. For each training sample,\nwe randomly select 2 to 4 viewpoints with one reference and\ntwo search frames in each iteration. All the training proce-\ndures are conducted on 2 NVIDIA A100 80GB GPUs.\nFor detailed implementation and training specifics,\nplease refer to the Appendix.\n5.3. Evaluation Metrics\nWe evaluate our method using three standard performance\nmeasures from the single-view tracking benchmark [13, 27,\n39]: Area Under Curve (AUC), Precision (P), and Normal-\nized Precision (PNorm):\n‚Ä¢ AUC: The Intersection over Union (IoU) measures the\noverlap between predicted and ground truth BBoxes in\neach frame. The AUC metric is calculated by varying the\nIoU threshold to evaluate the area error in the tracking\nregion.\n\n\n0\n0.2\n0.4\n0.6\n0.8\n1\nOverlap threshold\n0\n20\n40\n60\n80\n100\nSuccess rate (%)\n[71.1] MITracker\n[63.4] ODTrack\n[61.9] AQATrack\n[61.4] EVPTrack\n[60.5] HIPTrack\n[60.0] OSTrack\n[58.4] SeqTrack\n[57.6] MixFormer\n[55.3] SAM2Long\n[53.2] ARTrack\n[52.5] GRM\n[50.5] TrDiMP\n[48.6] PrDiMP\n[46.5] SAM2\n[43.1] DiMP\n(a) Success plot.\n0\n5\n10\n15\n20\n25\n30\nFrame numbers needed for recovery\n0\n20\n40\n60\n80\n100\nRecovery rate (%)\n[79.2] MITracker\n[56.7] SAM2Long\n[56.7] ODTrack\n[54.2] EVPTrack\n[54.2] SeqTrack\n[50.8] OSTrack\n[48.3] HIPTrack\n[44.2] MixFormer\n[43.3] AQATrack\n[35.8] SAM2\n[35.0] TrDiMP\n[34.2] GRM\n[27.5] ARTrack\n[26.7] PrDiMP\n[25.8] DiMP\n(b) Recovery ability plot.\n1.5\n2\n2.5\n3\nRestart count\n400\n450\n500\n550\n600\nMax length\nMITracker\nODTrack\nAQATrack\nMixFormer\nEVPTrack\nSeqTrack\nOSTrack\nARTrack\nHIPTrack\nGRM\n(c) Robust tracking plot.\nFigure 4. General experiments on the MVTrack dataset evaluate tracking robustness. MITracker provides multi-view results, while other\nmethods yield single-view results. In (a), methods are ranked by AUC and noted in the legend. For (b), the numbers in the legend represent\nthe method‚Äôs recovery rate within 10 frames after the target disappears.\n‚Ä¢ P: Precision is defined as the distance between the pre-\ndicted and ground truth BBox centers. This metric is used\nto assess the positional error in tracking.\n‚Ä¢ PNorm: To mitigate biases due to variations in BBox size,\nwe normalize the center point by the width and height of\nthe ground truth BBox. This adjustment provides a more\naccurate metric.\n5.4. Comparison with Existing Methods\nSOTA Performance on Benchmark. We evaluate tracking\nperformance with single-view visual object tracking meth-\nods, training all models (except SAM2 and SAM2Long) on\nthe GOT10K and MVTrack datasets. The models are tested\non both the MVTrack and GMTD datasets under single-\nview and multi-view settings.\nHowever, single-view methods cannot handle multi-view\ninputs or generate multi-view predictions. To address this,\nwe employ a post-fusion strategy to obtain multi-view re-\nsults.\nSpecifically, single-view predictions are first pro-\njected into the 3D world coordinate system. The region\nwith maximum overlap is identified as the target position,\nwhich is then reprojected onto the 2D image plane of each\nviewpoint to generate the fused multi-view tracking results.\nAs shown in Table 2, MITracker achieves superior per-\nformance in both multi- and single-view tracking across dif-\nferent datasets. In multi-view scenarios with 3-4 cameras,\nMITracker outperforms other methods that rely on post-\nprocessing for multi-view fusion, surpassing the second-\nbest method OSTrack by approximately 26% in PNorm.\nIn single-view settings, MITracker surpasses SOTA meth-\nods on the MVTrack dataset, achieving an AUC of 68.57%,\nwhich outperforms ODTrack by approximately 5%.\nNotably, MITracker exhibits strong generalization ca-\npabilities by achieving exceptional performance on the\nGMTD, despite it not being included in the training data.\nThis demonstrates the robustness of our multi-view ap-\nproach even in single-view scenarios. We attribute these\nimprovements to our multi-view training strategy, which en-\nables the model to better understand spatial relationships\ncrucial for precise tracking.\nIt is also noteworthy that\npost-processing degrades the performance of all single-view\nmethods. This indicates a substantial distribution gap in\nview-independent feature detection across models, making\neffective fusion through geometric projections challenging.\nStable Continuous Tracking Capability.\nTo further\nevaluate tracking robustness, we conducted three compara-\ntive experiments on the MVTrack dataset. Only MITracker\nutilizes multi-view inputs, other methods use single-view\ninputs and generate BBoxes independently.\nFirst, we analyzed tracking success rates across various\nIoU thresholds, as shown in Figure 4a. MITracker con-\nsistently outperforms competing methods regardless of the\nthreshold value.\nSecond, we evaluated the recovery capability after the\ntarget was invisible by measuring the proportion of suc-\ncessful tracking resumption within given frame intervals\n[22]. As illustrated in Figure 4b, with a 10-frame interval,\nMITracker achieves a high success rate of 79.2% in these\nrecovery tests. In comparison, SAM2Long only achieves\na 56.7% recovery rate under the same setting, highlight-\ning our method‚Äôs exceptional ability to quickly reestablish\ntracking after the target dissapears.\nIn practical applications, users can manually intervene\nto restart the model‚Äôs tracking by providing an accurate ini-\ntial position. In this experimental setup, we measured the\nmaximum continuous tracking length of video frames and\nthe average number of restarts (triggered when target loss\nexceeds 10 frames, using ground truth for repositioning)\n[45]. As shown in Figure 4c, MITracker achieves nearly\n100 frames longer tracking duration than ODTrack while\n\n\nrequiring fewer restart counts.\n5.5. Ablation Study\nResults in Table 3 demonstrate that BEV Loss, which pro-\nvides implicit multi-view information feedback, signifi-\ncantly enhances model performance. This improvement is\nattributed to its ability to augment spatial awareness during\nsingle-view feature extraction. The Spatial attention, which\nutilizes fused information to adjust outputs from single-\nview perspectives, also contributes to notable performance\nimprovements in the model.\nBEV Loss\nSpatial Attention\nAUC(%)\nPNorm(%)\nP(%)\n63.99\n82.82\n75.00\n‚úì\n69.64\n89.85\n82.01\n‚úì\n‚úì\n71.13\n91.87\n83.95\nTable 3. Ablation study for multi-view evaluation on MVTrack\ndataset.\n5.6. Visualization Comparison\nOur qualitative evaluation focuses on the influence of occlu-\nsion and fast motion. In the upper part of Figure 5, we se-\nlect two viewpoints from the MVTrack dataset and evaluate\nthem on MITracker and ODTrack, which has the second-\nbest performance on this dataset. The gray areas in the\ngraph represent periods when the object is out of view or\nfully occluded by other objects. We can easily observe that\nMITracker is able to re-track the object shortly after it reap-\npears, whereas ODTrack tends to continue in a lost state.\nFor instances #405 and #515 in V2, even when the ob-\nject reappears in the frame, ODTrack still mistakenly locks\nonto the wrong object. The bottom of Figure 5 presents\ntests conducted on the GMTD, where we also selected the\nsecond-best method, EVPTrack, for comparison with MI-\nTracker. When a pedestrian reappears after being obscured\nby a pillar, EVPTrack mistakenly locks onto the wrong tar-\nget, whereas MITracker is able to maintain stable and con-\ntinuous tracking.\nWe also visualize the predicted BEV trajectories from\nMITracker in Figure 6. Referencing the ground-truth trajec-\ntories, MITracker effectively integrates multi-view features\nand provides accurate 3D spatial information.\n6. Discussion\nIn previous sections, we provide a detailed introduction to\nthe MVTrack dataset and demonstrate the outstanding per-\nformance of MITracker. However, there are some areas that\ncould be improved in future work.\nLimitations. Although MVTrack dataset includes a di-\nverse set of scenes, it currently consists of indoor environ-\nments only, potentially limiting the generalization of meth-\nEVPTrack\nTarget Invisible\nODTrack\nMITracker\nGT\n#405\nV1 #405\n#421\n#472\n#414\n#700\nV2 #414\nV2 #515\nV1 #414\nV1 #515\nV2 #405\n0\n1\nIoU\n0\n1\nIoU\n0\n1\nIoU\n#414\n#515\n#405 #414\n#515\n#421\n#472\n#700\nFigure 5.\nQualitative comparison results.\nComparison of our\ntracker with two SOTA methods on MVTrack dataset (top) and\nGMTD (bottom).\nEach frame is cropped for better visualiza-\ntion. IoU curves of each method‚Äôs prediction and ground truth\nare shown above, where IoU reflects tracking quality. MITracker\ndemonstrates superior re-tracking performance upon target reap-\npearance.\nEnd\nEnd\nStart\nStart\nMITracker\nGT\nFigure 6.\nVisualization of BEV trajectories on the MVTrack\ndataset. Left: scene captured by four cameras. Right: scene cap-\ntured by three cameras.\nods trained on it to outdoor settings.\nAdditionally, MI-\nTracker relies on camera calibration for multi-view fusion,\nwhich may restrict its applicability in scenarios where cali-\nbration is challenging or infeasible.\nFuture work. We plan to extend MVTrack dataset by\nincluding outdoor scenes and a wider range of tracking\nobjects to enable the development of more generalizable\n\n\nmulti-view tracking algorithms. Furthermore, we aim to\nenhance MITracker by reducing its dependency on precise\ncamera calibration, making it more adaptable to scenarios\nwhere accurate calibration is challenging.\n7. Conclusion\nIn this study, we address key challenges such as occlu-\nsion and target loss in MVOT by making two significant\ncontributions: (1) MVTrack, a comprehensive dataset with\n234K high-quality annotations across diverse scenes and\nobject categories, and (2) MITracker, a novel visual track-\ning method that effectively integrates multi-view object fea-\ntures. MITracker achieves SOTA results on MVTrack and\nGMTD datasets, demonstrating its ability to provide stable\nand reliable tracking across different viewpoints and video\ndurations. Our contributions lay the foundation for future\nadvancements in MVOT, enabling the development of more\nrobust and accurate tracking systems for real-world scenar-\nios.\n8. Acknowledgments\nThis work was partially supported by STI 2030-Major\nProjects (2022ZD0209000) and HPC Platform of Shang-\nhaiTech University.\nReferences\n[1] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte.\nLearning discriminative model prediction for\ntracking.\nIn Proceedings of the IEEE/CVF international\nconference on computer vision, pages 6182‚Äì6191, 2019. 6\n[2] Wenrui Cai, Qingjie Liu, and Yunhong Wang.\nHiptrack:\nVisual tracking with historical prompts. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2024. 6\n[3] Yidong Cai, Jie Liu, Jie Tang, and Gangshan Wu. Robust\nobject modeling for visual tracking. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9589‚Äì9600, 2023. 1\n[4] Tatjana Chavdarova, Pierre Baqu¬¥e, St¬¥ephane Bouquet, An-\ndrii Maksai, Cijo Jose, Timur Bagautdinov, Louis Lettry,\nPascal Fua, Luc Van Gool, and Franc¬∏ois Fleuret.\nWild-\ntrack: A multi-camera hd dataset for dense unscripted pedes-\ntrian detection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5030‚Äì5039,\n2018. 3\n[5] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang,\nand Huchuan Lu. Transformer tracking. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8126‚Äì8135, 2021. 2\n[6] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han\nHu. Seqtrack: Sequence to sequence learning for visual ob-\nject tracking. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 14572‚Äì\n14581, 2023. 2, 4, 6\n[7] Paolo Cignoni, Alessandro Muntoni, Guido Ranzuglia, and\nMarco Callieri. MeshLab. 3\n[8] Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Mat-\nteo Dellepiane, Fabio Ganovelli, and Guido Ranzuglia.\nMeshLab:\nan Open-Source Mesh Processing Tool.\nIn\nEurographics Italian Chapter Conference. The Eurographics\nAssociation, 2008. 3\n[9] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu.\nMixformer: End-to-end tracking with iterative mixed at-\ntention.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 13608‚Äì\n13618, 2022. 1, 2, 6\n[10] Martin Danelljan, Luc Van Gool, and Radu Timofte. Prob-\nabilistic regression for visual tracking.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 7183‚Äì7192, 2020. 1, 2, 6\n[11] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi\nWang.\nSam2long: Enhancing sam 2 for long video seg-\nmentation with a training-free memory tree. arXiv preprint\narXiv:2410.16268, 2024. 6\n[12] Alexey Dosovitskiy.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 6\n[13] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\nYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\nLasot: A high-quality benchmark for large-scale single ob-\nject tracking. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5374‚Äì5383,\n2019. 2, 3, 4, 6\n[14] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized\nrelation modeling for transformer tracking. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18686‚Äì18695, 2023. 6\n[15] Xiaotian Han, Quanzeng You, Chunyu Wang, Zhizheng\nZhang, Peng Chu, Houdong Hu, Jiang Wang, and Zicheng\nLiu. Mmptrack: Large-scale densely annotated multi-camera\nmultiple people tracking benchmark. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 4860‚Äì4869, 2023. 2, 3\n[16] Shengyu Hao, Peiyuan Liu, Yibing Zhan, Kaixun Jin,\nZuozhu Liu, Mingli Song, Jenq-Neng Hwang, and Gaoang\nWang.\nDivotrack: A novel dataset and baseline method\nfor cross-view multi-object tracking in diverse open scenes.\nInternational Journal of Computer Vision, 132(4):1075‚Äì\n1090, 2024. 3\n[17] Adam W Harley, Zhaoyuan Fang, Jie Li, Rares Ambrus, and\nKaterina Fragkiadaki. Simple-bev: What really matters for\nmulti-sensor bev perception?\nIn 2023 IEEE International\nConference on Robotics and Automation (ICRA), pages\n2759‚Äì2765. IEEE, 2023. 2, 3, 5\n[18] Yunzhong Hou and Liang Zheng. Multiview detection with\nshadow transformer (and view-coherent data augmentation).\nIn Proceedings of the 29th ACM International Conference on\nMultimedia, pages 1673‚Äì1682, 2021. 3\n[19] Yunzhong Hou, Liang Zheng, and Stephen Gould. Multi-\nview detection with feature perspective transformation. In\n\n\nComputer Vision‚ÄìECCV 2020: 16th European Conference,\nGlasgow, UK, August 23‚Äì28, 2020, Proceedings, Part VII\n16, pages 1‚Äì18. Springer, 2020. 2, 3\n[20] Shiyu Hu, Xin Zhao, Lianghua Huang, and Kaiqi Huang.\nGlobal instance tracking:\nLocating target more like hu-\nmans. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 45(1):576‚Äì592, 2022. 2, 3\n[21] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A\nlarge high-diversity benchmark for generic object tracking in\nthe wild. IEEE transactions on pattern analysis and machine\nintelligence, 43(5):1562‚Äì1577, 2019. 2, 3, 6\n[22] Yuqing Huang, Xin Li, Zikun Zhou, Yaowei Wang, Zhenyu\nHe, and Ming-Hsuan Yang. Rtracker: Recoverable track-\ning via pn tree structured memory.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19038‚Äì19047, 2024. 1, 2, 7\n[23] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva\nRamanan, and Simon Lucey.\nNeed for speed: A bench-\nmark for higher frame rate object tracking. In Proceedings of\nthe IEEE international conference on computer vision, pages\n1125‚Äì1134, 2017. 2, 3\n[24] Matej Kristan, Jiri Matas, AleÀás Leonardis, Tom¬¥aÀás Voj¬¥ƒ±Àár, Ro-\nman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih\nPorikli, and Luka ÀáCehovin. A novel performance evaluation\nmethodology for single-target trackers.\nIEEE transactions\non pattern analysis and machine intelligence, 38(11):2137‚Äì\n2155, 2016. 2, 3\n[25] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.\nHigh performance visual tracking with siamese region pro-\nposal network. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8971‚Äì8980,\n2018. 1, 2\n[26] T Lin. Focal loss for dense object detection. arXiv preprint\narXiv:1708.02002, 2017. 6\n[27] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-\nsubaihi, and Bernard Ghanem. Trackingnet: A large-scale\ndataset and benchmark for object tracking in the wild. In\nProceedings of the European conference on computer vision\n(ECCV), pages 300‚Äì317, 2018. 2, 3, 6\n[28] Maxime Oquab, Timoth¬¥ee Darcet, Th¬¥eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 6\n[29] Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua\nDong, Zhipeng Zhang, Heng Fan, and Libo Zhang. Vast-\ntrack: Vast category visual object tracking. arXiv preprint\narXiv:2403.03493, 2024. 2, 3\n[30] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR¬®adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Doll¬¥ar, and Christoph Feicht-\nenhofer. Sam 2: Segment anything in images and videos.\narXiv preprint arXiv:2408.00714, 2024. 6\n[31] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding\nbox regression. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 658‚Äì666,\n2019. 6\n[32] Liangtao Shi, Bineng Zhong, Qihua Liang, Ning Li, Sheng-\nping Zhang, and Xianxian Li. Explicit visual prompts for\nvisual object tracking. In AAAI, 2024. 6\n[33] Liangchen Song, Jialian Wu, Ming Yang, Qian Zhang, Yuan\nLi, and Junsong Yuan. Stacked homography transformations\nfor multi-view pedestrian detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6049‚Äì6057, 2021. 3\n[34] Torben Teepe, Philipp Wolters, Johannes Gilg, Fabian Her-\nzog, and Gerhard Rigoll. Earlybird: Early-fusion for multi-\nview tracking in the bird‚Äôs eye view. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 102‚Äì111, 2024. 3\n[35] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li.\nTransformer meets tracker: Exploiting temporal context for\nrobust visual tracking.\nIn Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 1571‚Äì1580, 2021. 6\n[36] Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei\nWang, Yonghong Tian, and Feng Wu. Towards more flexible\nand accurate object tracking with natural language: Algo-\nrithms and benchmark.\nIn Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 13763‚Äì13773, 2021. 2, 3\n[37] Xing Wei, Yifan Bai, Yongchao Zheng, Dahu Shi, and Yi-\nhong Gong. Autoregressive visual tracking. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9697‚Äì9706, 2023. 2, 6\n[38] Minye Wu, Haibin Ling, Ning Bi, Shenghua Gao, Qiang\nHu, Hao Sheng, and Jingyi Yu. Visual tracking with mul-\ntiview trajectory prediction.\nIEEE Transactions on Image\nProcessing, 29:8355‚Äì8367, 2020. 2, 3, 6\n[39] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-\ning benchmark. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 37:1‚Äì1, 2015. 2, 3, 6\n[40] Shiting Xiao, Yufu Wang, Ammon Perkes, Bernd Pfrommer,\nMarc Schmidt, Kostas Daniilidis, and Marc Badger. Multi-\nview tracking, re-id, and social network analysis of a flock\nof visually similar birds in an outdoor aviary. International\nJournal of Computer Vision, 131(6):1532‚Äì1549, 2023. 2\n[41] Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang,\nLiangtao Shi, Shuxiang Song, and Rongrong Ji.\nAutore-\ngressive queries for adaptive tracking with spatio-temporal\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 19300‚Äì\n19309, 2024. 4, 6\n[42] Yuanlu Xu, Xiaobai Liu, Yang Liu, and Song-Chun Zhu.\nMulti-view people tracking via hierarchical trajectory com-\nposition.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4256‚Äì4265,\n2016. 2, 3\n[43] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and\nXilin Chen. Joint feature learning and relation modeling for\ntracking: A one-stream framework. In European Conference\non Computer Vision, pages 341‚Äì357. Springer, 2022. 2, 6\n\n\n[44] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenyu Liu,\nand Wenjun Zeng.\nVoxeltrack: Multi-person 3d human\npose estimation and tracking in the wild. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 45(2):2613‚Äì\n2626, 2022. 5\n[45] Xin Zhao, Shiyu Hu, Yipei Wang, Jing Zhang, Yimin Hu,\nRongshuai Liu, Haibin Ling, Yin Li, Renshu Li, Kun Liu,\net al. Biodrone: A bionic drone-based single object track-\ning benchmark for robust vision.\nInternational Journal of\nComputer Vision, 132(5):1659‚Äì1684, 2024. 7\n[46] Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo,\nShengping Zhang, and Xianxian Li. Odtrack: Online dense\ntemporal token learning for visual tracking. In Proceedings\nof the AAAI Conference on Artificial Intelligence, pages\n7588‚Äì7596, 2024. 1, 2, 4, 6\n[47] Xingyi Zhou, Dequan Wang, and Philipp Kr¬®ahenb¬®uhl. Ob-\njects as points. arXiv preprint arXiv:1904.07850, 2019. 4\n[48] Yitao Zhu, Sheng Wang, Mengjie Xu, Zixu Zhuang, Zhixin\nWang, Kaidong Wang, Han Zhang, and Qian Wang. Muc:\nMixture of uncalibrated cameras for robust 3d human body\nreconstruction. arXiv preprint arXiv:2403.05055, 2024. 1\n\n\nMITracker: Multi-View Integration for Visual Object Tracking\nSupplementary Material\nSection 9 provides additional information on the MV-\nTrack dataset, while Section 10 includes further implemen-\ntation details and experimental results of MITracker.\n9. Dataset Details\nData Annotation. In the BEV annotations, the MVTrack\ndataset covers an 8m √ó 8m area. Ground truth labels are\nprojected onto a 400 √ó 400 grid, where each cell is 2cm √ó\n2cm in size.\nAttributes Definition. MVTrack dataset contains nine\nattributes to assess tracking robustness, as shown in Table\n4. We provide frame-level binary labels for five attributes:\nBackground Clutter (BC), Motion Blur (MB), Partial Oc-\nclusion (POC), Full Occlusion (FOC), and Out of View\n(OV). These are manually annotated for each frame. De-\nformation (DEF) is labeled according to whether the tracked\ntarget deforms. Low Resolution (LR), Aspect Ratio Change\n(ARC), and Scale Variation (SV) are automatically com-\nputed from changes in the BBox size.\nAtt.\nDefinition\nBC\nThe background has similar appearance as the\ntarget\nMB\nThe target region is blurred due to target motion\nPOC\nThe target is partially occluded in the frame\nFOC\nThe target is fully occluded in the frame\nOV\nThe target completely leaves the video frame\nDEF\nThe target is deformable during tracking\nLR\nThe target BBox is smaller than 1000 pixels\nARC\nThe ratio of BBox aspect ratio is outside the\nrange [0.5, 2]\nSV\nThe ratio of BBox is outside the range [0.5, 2]\nTable 4. Description of 9 attributes in MVTrack dataset.\nStatistical Details. The MVTrack dataset contains 260\nvideos averaging around 900 frames each, as shown in Fig-\nure 7a. As illustrated in Figure 7b, a key challenge is oc-\nclusion, which often results from subject-object interactions\nthat cause partial or complete occlusion.\nConsequently,\ntracking models need to manage occlusion to perform ro-\nbustly and adeptly on this dataset.\n10. Experiment Details\n10.1. Training and Resource Analysis\nTraining Details. We process the visual inputs by cropping\nthe reference frame to 2 times the target‚Äôs BBox size and\n0\n5\n10\n15\n20\n25\n500\n700\n900\n1100\n1300\nProbability [%]\nFrames\n(a) Frames distribution.\n65\n179\n260\n219\n50\n109\n233\n253 260\n0\n50\n100\n150\n200\n250\n300\nBC\nMB POC FOC OV DEF LR ARC SV\nNumber of Videos\nAttribute\n(b) Attributes distribution.\nFigure 7. Distribution of sequences in each attribute and length in\nour MVTrack dataset.\nresizing it to 182 √ó 182 pixels. The search frame is cropped\nat 4.5 times the target box area and resized to 364 √ó 364\npixels to expand the search region. During projection, we\ntransform the camera intrinsic matrix CK accordingly and\nadd noise to the translation vector Ct to prevent overfitting\nin multi-view fusion.\nTraining consists of two stages. In the first stage, we\noptimize the view-specific encoder using AdamW with a\nlearning rate of 1 √ó 10‚àí5 and the rest of the model at\n1 √ó 10‚àí4. We train for 50 epochs, sampling 10,000 image\npairs per epoch with a batch size of 32. In the second stage,\nwe fine-tune the encoder at 1 √ó 10‚àí6 while keeping other\ncomponents at 1√ó10‚àí4. We use the MVTrack dataset, sam-\npling 2,500 multi-view image pairs per epoch for 40 epochs\nwith a batch size of 4. AdamW is used throughout.\nComputational Resource. We evaluate MITracker and\nthe single-view model ODTrack under the same input (4\nviews) on an NVIDIA A100, as summarized in Table 5.\nAlthough multi-view fusion introduces additional compu-\ntational overhead, it remains within an acceptable range.\nMethod\nParameters (M)\nGRAM (MB)\nFPS\nODTrack\n92.12\n365.82\n18.78\nMITracker\n101.65\n407.78\n14.08\nTable 5. Comparison of computational complexity and resource.\n\n\n0\n10\n20\n30\n40\n50\nLocation error threshold\n0\n20\n40\n60\n80\n100\nPrecision (%)\n[83.9] MITracker\n[74.5] ODTrack\n[72.7] AQATrack\n[72.0] EVPTrack\n[70.5] HIPTrack\n[70.1] OSTrack\n[69.0] SeqTrack\n[67.7] MixFormer\n[67.4] SAM2Long\n[62.5] ARTrack\n[62.3] GRM\n[60.4] TrDiMP\n[58.9] PrDiMP\n[56.8] SAM2\n[53.1] DiMP\n(a) Precision plot on MVTrack dataset.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nNormalized location error threshold\n0\n20\n40\n60\n80\n100\nNormalized precision (%)\n[91.9] MITracker\n[82.2] ODTrack\n[80.0] AQATrack\n[79.8] EVPTrack\n[78.9] HIPTrack\n[77.7] OSTrack\n[76.6] SeqTrack\n[75.4] MixFormer\n[72.8] SAM2Long\n[70.2] ARTrack\n[69.9] GRM\n[67.7] TrDiMP\n[66.1] PrDiMP\n[63.1] SAM2\n[59.5] DiMP\n(b) Normalized precision plot on MVTrack dataset.\n0\n10\n20\n30\n40\n50\nLocation error threshold\n0\n20\n40\n60\n80\n100\nPrecision (%)\n[82.1] MITracker\n[79.9] EVPTrack\n[79.4] AQATrack\n[79.3] SeqTrack\n[78.5] MixFormer\n[78.4] ODTrack\n[77.4] SAM2Long\n[76.9] HIPTrack\n[74.2] ARTrack\n[73.5] TrDiMP\n[73.2] SAM2\n[73.2] OSTrack\n[70.5] PrDiMP\n[70.3] GRM\n[66.0] DiMP\n(c) Precision plot on GMTD.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nNormalized location error threshold\n0\n20\n40\n60\n80\n100\nNormalized precision (%)\n[87.0] MITracker\n[83.8] EVPTrack\n[83.2] SeqTrack\n[83.0] AQATrack\n[82.6] MixFormer\n[82.4] ODTrack\n[80.6] HIPTrack\n[78.9] TrDiMP\n[78.6] SAM2Long\n[78.1] ARTrack\n[77.4] OSTrack\n[76.2] PrDiMP\n[74.7] SAM2\n[74.0] GRM\n[68.2] DiMP\n(d) Normalized precision plot on GMTD.\n0\n0.2\n0.4\n0.6\n0.8\n1\nOverlap threshold\n0\n20\n40\n60\n80\n100\nSuccess rate (%)\n[66.0] MITracker\n[63.9] EVPTrack\n[63.6] AQATrack\n[63.0] SeqTrack\n[62.8] SAM2Long\n[62.2] HIPTrack\n[62.0] MixFormer\n[61.4] ODTrack\n[59.9] SAM2\n[59.6] ARTrack\n[59.5] TrDiMP\n[58.4] OSTrack\n[57.8] PrDiMP\n[55.7] GRM\n[52.7] DiMP\n(e) Success plot on GMTD.\n1.5\n2\n2.5\n3\nRestart count\n450\n500\n550\n600\nMax length\nMITracker\nODTrack\nMixFormer\nHIPTrack\nAQATrack\nSeqTrack\nEVPTrack\nOSTrack\nARTrack\nGRM\n(f) Robust tracking plot on GMTD.\nFigure 8. Comparative results across MVTrack and GMTD datasets, with rankings noted in the legends. Parts (a) and (c) sort methods by\nP with a 20-pixel threshold, parts (b) and (d) by Pnorm with a 0.2 threshold, and part (e) by AUC.\n\n\nTarget Invisible\n1 View Input\nGT\n#326\n0\n1\nIoU\n#326\n#455\n#621\n#455\n#621\n3 Views Input\nFigure 9. Qualitative comparison results on the impact of different numbers of input views. For a specific view, we compare the effects of\nusing only that view versus including two additional overlapping views.\n10.2. Comparison on Benchmark Details\nIn Figure 8, we provide further quantitative evaluations of\nthe AUC, P, and Pnorm across various threshold settings for\nboth the MVTrack and GMTD datasets. In most settings,\nMITracker consistently outperforms other methods.\nDuring zero-shot testing on the GMTD, SAM2 and\nSAM2Long perform better under lenient threshold condi-\ntions but lacks the ability to localize objects precisely. Fur-\nthermore, as shown in Figure 8f, MITracker sustains longer\ntracking durations with fewer reinitializations on this un-\nseen dataset.\n10.3. More Ablation Study\nImpact of Input Views. To assess the importance of the\nnumber of views for tracking, we select a fixed camera\nfrom each scenario in the testing set. We then examine how\nmodel performance changes as we increase the number of\nadditional cameras. The results in Table 6 highlight the ben-\nefits of adding more cameras.\nFigure 9 illustrates the challenges faced by the single-\nview model: after a prolonged target disappearance, it mis-\ntracks a white bottle. In contrast, the multi-view model ini-\ntially mistakes a white trash can for the target but quickly\nrecovers and maintains stable tracking with the aid of addi-\ntional views.\nInput views\nAUC(%)\nPNorm(%)\nP(%)\n1\n62.27\n84.71\n73.92\n2\n63.97\n87.07\n76.30\n3\n67.97\n91.50\n80.73\n3/4\n68.65\n92.37\n81.55\nTable 6. Ablation study for the impact of different numbers of\ninput views on MVTrack dataset.\nImpact of Multi-View Training. Our experiment shows\nthat multi-view training improves single-view performance\nby exposing the model to richer spatial information, which\nenhances its ability to handle occlusion and reappearance.\nTable 7 compares results with MITracker SV trained under\nsingle-view settings, highlighting the advantages of multi-\nview training even for single-view scenarios.\nMethod\nAUC (%)\nPNorm (%)\nP (%)\nMITracker SV\n63.42\n82.97\n79.67\nMITracker\n65.96\n87.05\n82.07\nTable 7. Zero-shot performance of single-view results on GMTD.\nImpact of Temporal Token. The temporal token incor-\nporates tracking information from previous frames, Table 8\nhighlights the improvements achieved through the temporal\ntoken.\nTemporal Token\nAUC (%)\nPNorm (%)\nP (%)\n69.30\n89.62\n81.60\n‚úì\n71.13\n91.87\n83.95\nTable 8. Ablation study for temporal token.\n10.4. More Visualization Results\nWe provide additional visual comparison results as illus-\ntrated in Figure 10 and Figure 11 from the MVTrack\ndataset, and Figure 12 from the GMTD. MITracker exhibits\nenhanced re-tracking capabilities both in multi-view and\nsingle-view scenarios. Furthermore, multi-view informa-\ntion assists in correcting instances of mistracking. To facil-\nitate better visualization, each frame is cropped to a fixed\narea. The IoU curves above further illustrate the tracking\naccuracy by comparing each method‚Äôs predictions to the\nground truth.\n\n\n#79\n0\n1\nIoU\n#177\n#290\n#79\n0\n1\nIoU\n#177\n#290\nV1#79\nV1#177\nV1#290\nV4#79\nV4#177\nV4#290\n(a) Two views: pingpong5-1 and pingpong5-4. ODTrack tends to lose track after extended periods of target disappearance, whereas MI-\nTracker demonstrates robust recovery capabilities.\n#415\n0\n1\nIoU\n#521\n#598\nV1#415\nV1#521\nV1#598\n#415\n#521\n#598\nV2#415\nV2#521\nV2#598\n0\n1\nIoU\n(b) Two views: umbrella2-1 and umbrella2-2. Under the interference of a similar object, ODTrack fails to re-track the correct target. In\ncontrast, with the aid of multi-view assistance, MITracker can correct tracking errors from frame V1#415 to #521.\nTarget Invisible\nODTrack\nGT\nMITracker\nFigure 10. Qualitative comparison results on the MVTrack dataset using ODTrack.\n\n\n#493\n0\n1\nIoU\n#562\n#636\nV1#493\nV1#562\nV1#636\n#493\n0\n1\nIoU\n#562\n#636\nV2#493\nV2#562\nV2#636\n#493\n0\n1\nIoU\n#562\n#636\nV4#493\nV4#562\nV4#636\nTarget Invisible\nODTrack\nGT\nMITracker\n(a) Three views: bottle3-1, bottle3-2 and bottle3-4. In V2 #493, MITracker momentarily mistracks a similar object as the target but success-\nfully re-tracks the target by #562. In contrast, ODTrack struggles to recover once it mistracks.\n#242\n#295\n#559\n0\n1\nIoU\nV4#242\nV4#295\nV4#559\nTarget Invisible\nGT\nMITracker\nSAM2Long\n(b) Sequence: book4-4. SAM2Long completely loses the target following disappearances at frames #242 and #295. Upon re-tracking, it\nfails to adapt to target deformation, resulting in diminished IoU by frame #559.\nFigure 11. Qualitative comparison results on the MVTrack dataset using ODTrack and SAM2Long.\n\n\n#1000\n#1260\n#1650\n0\n1\nIoU\n#1000\n#1260\n#1650\n(a) Sequence: cola-2. MITracker demonstrates faster re-tracking capabilities than EVPTrack upon target reappearance.\n#350\n#500\n#550\n0\n1\nIoU\n#350\n#500\n#550\n(b) Sequence: manInOffice-2. EVPTrack fails to correct after mistracking. In contrast, MITracker exhibits superior recovery capabilities, as\ndemonstrated between frames #500 and #550.\nTarget Invisible\nEVPTrack\nGT\nMITracker\nFigure 12. Qualitative comparison results on the GMTD using EVPTrack.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20111v1.pdf",
    "total_pages": 17,
    "title": "MITracker: Multi-View Integration for Visual Object Tracking",
    "authors": [
      "Mengjie Xu",
      "Yitao Zhu",
      "Haotian Jiang",
      "Jiaming Li",
      "Zhenrong Shen",
      "Sheng Wang",
      "Haolin Huang",
      "Xinyu Wang",
      "Qing Yang",
      "Han Zhang",
      "Qian Wang"
    ],
    "abstract": "Multi-view object tracking (MVOT) offers promising solutions to challenges\nsuch as occlusion and target loss, which are common in traditional single-view\ntracking. However, progress has been limited by the lack of comprehensive\nmulti-view datasets and effective cross-view integration methods. To overcome\nthese limitations, we compiled a Multi-View object Tracking (MVTrack) dataset\nof 234K high-quality annotated frames featuring 27 distinct objects across\nvarious scenes. In conjunction with this dataset, we introduce a novel MVOT\nmethod, Multi-View Integration Tracker (MITracker), to efficiently integrate\nmulti-view object features and provide stable tracking outcomes. MITracker can\ntrack any object in video frames of arbitrary length from arbitrary viewpoints.\nThe key advancements of our method over traditional single-view approaches come\nfrom two aspects: (1) MITracker transforms 2D image features into a 3D feature\nvolume and compresses it into a bird's eye view (BEV) plane, facilitating\ninter-view information fusion; (2) we propose an attention mechanism that\nleverages geometric information from fused 3D feature volume to refine the\ntracking results at each view. MITracker outperforms existing methods on the\nMVTrack and GMTD datasets, achieving state-of-the-art performance. The code and\nthe new dataset will be available at\nhttps://mii-laboratory.github.io/MITracker/.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}