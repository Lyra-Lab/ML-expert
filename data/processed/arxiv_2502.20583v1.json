{
  "id": "arxiv_2502.20583v1",
  "text": "LITEASR: Efficient Automatic Speech Recognition with\nLow-Rank Approximation\nKeisuke Kamahori1,2\nJungo Kasai2\nNoriyuki Kojima2\nBaris Kasikci1\n1University of Washington 2Kotoba Technologies Inc.\n{kamahori,baris}@cs.washington.edu, {jkasai,nkojima}@kotoba.tech\nAbstract\nModern automatic speech recognition (ASR)\nmodels, such as OpenAI’s Whisper, rely on\ndeep encoder-decoder architectures, and their\nencoders are a critical bottleneck for efficient\ndeployment due to high computational inten-\nsity. We introduce LITEASR, a low-rank com-\npression scheme for ASR encoders that signifi-\ncantly reduces inference costs while maintain-\ning transcription accuracy. Our approach lever-\nages the strong low-rank properties observed in\nintermediate activations: by applying principal\ncomponent analysis (PCA) with a small calibra-\ntion dataset, we approximate linear transforma-\ntions with a chain of low-rank matrix multipli-\ncations, and further optimize self-attention to\nwork in the reduced dimension. Evaluation re-\nsults show that our method can compress Whis-\nper large-v3’s encoder size by over 50%, match-\ning Whisper medium’s size with better tran-\nscription accuracy, thereby establishing a new\nPareto-optimal frontier of efficiency and perfor-\nmance. The code of LITEASR is available at\nhttps://github.com/efeslab/LiteASR.\n1\nIntroduction\nAutomatic speech recognition (ASR) systems have\nmade significant strides in recent years, achieving\nnear-human transcription performance (Radford\net al., 2023; Puvvada et al., 2024). Modern ASR\nmodels, such as OpenAI’s Whisper family, typi-\ncally adopt an encoder-decoder architecture (Rad-\nford et al., 2023). For instance, Whisper large-\nv3 comprises 32 Transformer blocks in both its\nencoder and decoder, totaling approximately 1.6\nbillion parameters, and has set new standards in\nmultilingual transcription accuracy.\nDespite these advances, deploying ASR systems\nin real-world applications poses substantial effi-\nciency challenges. First, many applications, such\nas live transcription, voice assistants, and real-\ntime translation, impose strict latency requirements\n(Macháˇcek et al., 2023; Bevilacqua et al., 2024;\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nEncoder Size (\n)\n1e8\n10\n11\n12\n13\n14\n15\nWER (\n)\nLarge-v3\nTurbo\nMedium\nCompress with LiteASR\nModel\nLarge-v3\nTurbo\nMedium\nFigure 1: The relationship between encoder size and\naccuracy, as measured by word error rate (WER), for\nmodels in the Whisper family. The stars denote variants\ncompressed via our method, which achieves the Pareto-\noptimal balance between accuracy and efficiency.\nNguyen et al., 2020; Wang et al., 2022; Jeffries\net al., 2024). Latency refers to the delay between\nthe input of audio and the output of the transcribed\ntext. In real-time applications, even a few seconds\nof delay can significantly degrade user experience.\nSecond, while the overall model size may be\nmoderate compared to the latest large language\nmodels (LLMs), ASR encoders are computation-\nally intensive due to the long input sequences they\nmust process. For instance, the encoder Transform-\ners in the Whisper series consistently process input\nsequences of length 1500. For real-time applica-\ntions, this encoder must be processed frequently,\nmaking it a significant computational bottleneck.\nThese challenges are acute in both on-device\nand data center settings. In on-device scenarios\n(e.g., laptops or smartphones), limited hardware\ncapabilities make it difficult to meet latency con-\nstraints. Even in data center environments, which\nserve multiple concurrent users, the high computa-\ntional intensity of ASR encoders becomes a critical\nbottleneck. Although batching can improve serv-\ning throughput for memory-bound workloads, such\nas ASR decoders, it provides limited benefits for\ncompute-bound encoders (as discussed in §2).\nMoreover, recent works have shown that the\n1\narXiv:2502.20583v1  [cs.LG]  27 Feb 2025\n\n\ndecoder component of ASR models can be ag-\ngressively compressed. For example, OpenAI’s\nWhisper large-v3-turbo successfully reduced the\nnumber of decoder layers from 32 down to 4 lay-\ners via distillation. Other variants, such as Distill-\nWhisper and Kotoba-Whisper, have taken this even\nfurther, compressing the decoder to as few as 2\nlayers (Gandhi et al., 2023; Kotoba Technologies,\n2024). However, the encoder part remains largely\nunexplored, making its optimization increasingly\ncrucial for efficient ASR systems.\nIn this work, we propose LITEASR, a novel\ncompression scheme that targets ASR encoders by\nexploiting the low-rank structure of hidden acti-\nvations during inference. A key insight driving\nour approach is that intermediate activations, both\nin self-attention and multi-layer perception (MLP)\nlayers, consistently exhibit low-rank properties\nacross a wide variety of inputs. This phenomenon\nstems from ASR encoders’ use of Mel spectro-\ngrams, the 2D time-frequency audio representa-\ntions. Real-world audio (e.g., human speech) ex-\nhibits strong correlations between frequency com-\nponents (Huang et al., 2012; Zergat and Amrouche,\n2013; Tian et al., 2024; Kacha et al., 2020), result-\ning in low-rank characteristics of the intermediate\nfeatures.\nOur method first analyzes the low-rank proper-\nties of activations using a small amount of calibra-\ntion data. We then perform a principal component\nanalysis (PCA) (Wold et al., 1987) to extract the\ndominant components and approximate linear trans-\nformations with rank-k projections. This factoriza-\ntion allows each weight matrix to be expressed as\nthe product of two lower-rank matrices, thereby\nreducing the total number of floating point opera-\ntions (FLOPs) required for inference. We employ\nan adaptive mechanism based on the threshold to\ndetermine the optimal degree of low-rank approxi-\nmation for each layer.\nTo further capitalize on the optimization, we also\nmodify the self-attention algorithm to operate in the\nreduced dimension. We implement a specialized\nGPU kernel based on FlashAttention (Dao et al.,\n2022) to accelerate the computation of attention\nscores and outputs.\nOur evaluation shows that LITEASR achieves\na Pareto-optimal balance between speed and ac-\ncuracy (see Figure 1). When applied to Whisper\nlarge-v3, LITEASR reduces the encoder size by ap-\nproximately 40%, yielding an execution speedup of\naround 1.4x with negligible accuracy loss. In alter-\nnative configurations, we further reduce the model\nsize to less than half, resulting in a model compa-\nrable in size to Whisper medium, while delivering\nimproved accuracy. We also demonstrate the appli-\ncability of the method across different languages\nand models (§4).\nIn summary, this paper makes the following con-\ntributions:\n1. We introduce LITEASR, a compression\nmethod for ASR encoders using a low-rank ap-\nproximation of activation values. This method\napproximates linear layers with a chain of low-\nrank matrix multiplications and optimizes self-\nattention to operate in a reduced dimension.\n2. We present a comprehensive evaluation\ndemonstrating that our method achieves a\nPareto-optimal balance between accuracy and\nefficiency.\nThe rest of this paper is organized as follows: §2\ngives background on ASR efficiency, §3 presents\nour low-rank approximation framework, §4 details\nthe experimental setup, results, and analysis, §5\nreviews related work, and §6 concludes the paper.\n2\nBackground\n2.1\nAutomatic Speech Recognition (ASR)\nASR models convert spoken language into text by\ntransforming raw audio into a compact representa-\ntion, such as a Mel spectrogram, and processing it\nwith neural networks. Modern systems often use\nencoder-decoder architectures, typically employ-\ning Transformers (Radford et al., 2023; Puvvada\net al., 2024; Rekesh et al., 2023; Gulati et al.,\n2020). For instance, OpenAI’s Whisper mainly\nuses Transformer blocks, each of which consists\nof self-attention and MLP layers with a large num-\nber of linear transformations (query/key/value/out\nprojections for self-attention and two larger lin-\near transformations for MLP). A notable recent\ntrend in ASR models is the reduction in decoder\nsize without compromising performance, as exem-\nplified by models such as Whisper large-v3-turbo\n(Radford et al., 2023) and Distill-Whisper (Gandhi\net al., 2023), which reduced the number of decoder\nlayers from 32 to 4 and 2, respectively.\n2.2\nCompute Requirements of ASR Models\nSince the encoder often processes long sequences\n(e.g., fixed at 1500 for Whisper), it often emerges\n2\n\n\n0\n50\n100\nM1 Pro\nRTX A6000\nRTX 4090\nLarge-v3, bs=1\n0\n50\n100\nTurbo, bs=1\n0\n50\n100\nLarge-v3, bs=8\n0\n50\n100\nTurbo, bs=8\nEncoder\nDecoder\nFigure 2: Latency breakdown of encoder and decoder\nrelative to end-to-end latency for Whisper large-v3 and\nWhisper large-v3-turbo models under varying batch\nsizes (1 and 8).\nas the primary runtime bottleneck. Figure 2 shows\nthe latency breakdown between the encoder and\ndecoder across three hardware setups (NVIDIA\nRTX 4090, NVIDIA RTX A6000, and Apple M1\nPro), two models (Whisper large-v3 and Whisper\nlarge-v3-turbo), and two batch sizes (1 and 8)1.\nAlthough the encoder only accounts for about\n15% of the overall latency for single-batch Whisper\nlarge-v3 on GPUs, it represents a more significant\nbottleneck in other scenarios. For the newer Whis-\nper large-v3-turbo model, the latency contribution\nof the encoder increases significantly due to the re-\nduced size of the decoder. Similarly, for on-device\ninference (e.g., M1 Pro), the encoder’s relative la-\ntency is higher due to the limited computational\npower of such devices compared to GPUs.\nIn data center deployment scenarios where mul-\ntiple requests are batched, the encoder’s latency\nimpact is further exacerbated. For example, with a\nbatch size of 8 and using Whisper large-v3-turbo,\nthe encoder can consume over 90% of the total la-\ntency. This disproportionate latency is primarily\ndue to the encoder’s high computational intensity\n(Williams et al., 2009); batching is therefore inef-\nfective at increasing throughput for encoders. In\ncontrast, the decoder generates tokens one at a time\nin an autoregressive manner and is memory-bound,\nbottlenecked by memory bandwidth rather than\ncomputational power, making batching an effective\nstrategy to enhance throughput (Chen, 2023). Con-\nsequently, although batching can substantially im-\nprove serving throughput for the decoder, it offers\nlimited benefits for the compute-bound encoder\nand the encoder becomes a notable bottleneck for\n1We use vLLM (Kwon et al., 2023) (ver. 0.7.0) and MLX\n(Hannun et al., 2023) (ver. 0.21.1) to transcribe a sample audio\nclip from the ESB dataset (Gandhi et al., 2022).\nX\nY\n≈\n×\n×\nPCA\nY\nW\nX\nW\n×\n×\nY\n×\n=\nLow-Rank\nWeight\nOriginal\nWeight\nFigure 3: A simplified illustration of our proposal. We\nuse low-rank decomposition of activation values (Y) to\ncompress the weight (W).\nlarge batch sizes.\nThese findings collectively highlight the encoder\nas a critical bottleneck for efficient ASR deploy-\nment in both on-device and data center environ-\nments. This issue becomes more pronounced with\nrecent trends toward smaller decoders. Therefore,\nthere is a strong demand for methods to reduce the\ncomputational requirements of the encoder.\n3\nMethodology\nOur method, LITEASR, compresses the ASR en-\ncoder by extracting the low-rank features from ac-\ntivations at different layers of the model. To do so,\nwe first use calibration data to analyze activations\nand then convert the dense matrix multiplication\nwithin the model to the product of low-rank matri-\nces (Figure 3 shows a simplified overview of the\nmethod). We further modify the self-attention al-\ngorithm to work efficiently on reduced dimensions.\nIn this section, we explain the methodologies in\ndetail.\n3.1\nAnalyzing Activations in Transformers\nConsider a linear layer defined by\nY = XW + b,\n(1)\nwhere the weight matrix W ∈RDin×Dout and the\nbias vector b ∈RDout are learnable model param-\neters. Here, the input activations X ∈RL×Din\nproduce the output activations Y ∈RL×Dout dur-\ning the forward pass. In this notation, Din and Dout\ndenote the input and output dimensions of the layer,\nrespectively, and L is the sequence length2.\n2For Whisper encoders, this is always 1500.\n3\n\n\nTo study the distribution of activations, we col-\nlect calibration data consisting of Ncalib inputs. For\neach linear layer, we record the corresponding out-\nput Y . The resulting dataset can be viewed as\nL × Ncalib samples, where each sample is a Dout-\ndimensional vector. For simplicity, we refer to this\ncollection of samples as Y .\nOur goal is to approximate the observed activa-\ntions by projecting them onto their principal com-\nponents. First, let YM ∈RDout denote the mean\nvector of the dataset Y . Following the standard\nPCA procedure, we perform a singular value de-\ncomposition (SVD) on the mean-centered data:\nU, S, V = SVD(Y −YM).\n(2)\nHere, V ∈RDout×Dout is the matrix of right singular\nvectors. By selecting the first k columns of V ,\ndenoted by Vk ∈RDout×k, we capture the top-k\nprincipal components of the data. The original\nactivations can then be approximated as:\nY −YM ≈(Y −YM) Vk V ⊤\nk .\n(3)\nThis approximation retains the most significant fea-\ntures of Y while reducing its dimensionality.\n3.2\nCompressing Model Layers\nUsing the PCA approximation from Equation 3, we\ncan rewrite the original linear layer Y = XW + b\nas a combination of low-rank matrix multiplica-\ntions. Substituting Y = XW + b gives\nY −YM ≈(XW + b −YM) Vk V ⊤\nk\nY ≈(XW + b −YM) Vk V ⊤\nk + YM.\n(4)\nThis expression can be reorganized as\nY ≈X(WVk)V ⊤\nk +\n\u0010\nYM+(b−YM) Vk V ⊤\nk\n\u0011\n. (5)\nIn this factorization, the original layer is decom-\nposed into:\n• Two low-rank linear transformations, with\nweight matrices WVk ∈RDin×k and V ⊤\nk\n∈\nRk×Dout, and\n• A constant bias term given by YM + (b −\nYM) Vk V ⊤\nk .\nSince both weight matrices and bias can be pre-\ncomputed using calibration data, this decomposi-\ntion significantly reduces FLOPs when k is much\nsmaller than the original dimension.\n3.2.1\nHow to Choose k\nChoosing the appropriate value for k involves\na trade-off between accuracy and efficiency. A\nsmaller k leads to a more aggressive approxima-\ntion, which increases efficiency but may incur a\nlarger accuracy loss.\nAccuracy Constraint.\nTo preserve accuracy, the\ntop-k principal components must capture a suf-\nficient portion of total variance. Let S ∈RDout\ndenote the singular values from the SVD of the\nmean-centered activations (assumed to be sorted in\ndecreasing order). We enforce\nk\nX\ni=1\nS2\ni > θ\nDout\nX\ni=1\nS2\ni ,\n(6)\nwhere θ is a threshold that controls the trade-off\nbetween accuracy and efficiency (i.e., the extent of\ndata compression).\nEfficiency Constraint.\nThe original linear layer\nrequires O(LDinDout) FLOPs for its matrix mul-\ntiplication. In contrast, the decomposed form in\nEquation 4 requires O(LDink + LkDout) FLOPs.\nTo ensure that our approximation results in a reduc-\ntion of computation, we require\nLDink + LkDout < LDinDout,\n(7)\nwhich simplifies to\nk(Din + Dout) < DinDout.\n(8)\nFor example, in Whisper large-v3, the dimen-\nsions for self-attention layers are (Din, Dout) =\n(1280, 1280),\nand for MLP layers they are\n(1280, 5120) or (5120, 1280). This implies that\nthe efficiency constraint requires k < 640 for self-\nattention and k < 1024 for MLP layers.\nPractical Considerations.\nTo maximize the\nGPU efficiency, we further restrict k to be a multi-\nple of 16. Therefore, we choose k as the smallest\nmultiple of 16 that satisfies both of Equation 6\nand Equation 7. We empirically find that θ values\nbetween 0.99 and 0.999 achieve a good balance\nbetween accuracy and efficiency. A detailed sensi-\ntivity study on the choice of θ is provided in §4.\n3.2.2\nOptimizing Self-Attention\nMoreover, there is a potential to optimize the self-\nattention layers further. Specifically, if the rank k is\nsmaller than the per-head dimension, we can com-\npute the attention score and the value projection in\nalternative ways to reduce the FLOPs requirement\nwhile preserving the mathematical operations.\n4\n\n\nStandard Self-Attention.\nFor multi-head atten-\ntion, let Dhead denote the dimension per head and\nh the number of heads (i.e., the total model dimen-\nsion is Dhead × h). In the i-th head, given an input\nactivation matrix X ∈RL×Din, the self-attention\nmechanism first computes three linear projections:\nQi = XW i\nQ,\nKi = XW i\nK,\nVi = XW i\nV ,\n(9)\nwhere W i\nQ, W i\nK, W i\nV ∈RDin×Dhead are the corre-\nsponding weight matrices. The standard attention\noutput is then given by\nAttention(Qi, Ki, Vi) = softmax\n\u0012 QiK⊤\ni\n√Dhead\n\u0013\nVi,\n(10)\nwith the softmax applied row-wise.\nAttention Score Computation. Using our low-\nrank approximation, we can factorize each projec-\ntion as follows:\nQi = (XWQ1)W i\nQ2 + bi\nQ,\nKi = (XWK1)W i\nK2 + bi\nK,\nVi = (XWV1)W i\nV2 + bi\nV ,\n(11)\nwhere WQ1 ∈RDin×kQ, W i\nQ2 ∈RkQ×Dhead, and\nbi\nQ ∈RDhead are parameters relevant for i-th head\nafter low-rank approximation (with analogous defi-\nnitions for K and V ). Here, kQ, kK, and kV are the\nrespective rank sizes. For brevity, let A = XWQ1\nand B = XWK1. Expanding the product QiK⊤\ni ,\nwe obtain:\nQiK⊤\ni =\n\u0010\nAW i\nQ2 + bi\nQ\n\u0011\u0010\nBW i\nK2 + bi\nK\n\u0011⊤\n=\n\u0010\nAW i\nQ2 + bi\nQ\n\u0011\u0010\nW i ⊤\nK2 B⊤+ bi ⊤\nK\n\u0011\n= AW i\nQ2W i ⊤\nK2 B⊤\n+ AW i\nQ2bi ⊤\nK + bi\nQW i ⊤\nK2 B⊤+ bi\nQbi ⊤\nK .\n(12)\nIn this expansion, the term AW i\nQ2W i ⊤\nK2 B⊤domi-\nnates the computational cost, while the other three\nterms are bias contributions.\nThe standard approach (Equation 10) computes\nQi and K⊤\ni separately and then multiplies them,\nwhich requires approximately O(L2Dhead) FLOPs.\nIn contrast, Equation 12 allows us to first compute\nthe smaller matrix product W i\nQ2W i ⊤\nK2 and then mul-\ntiply by A and B, reducing the computational cost\nto O(L kQ kK + L2 min(kQ, kK)). This is benefi-\ncial when min(kQ, kK) < Dhead3. Thus, we adopt\nEquation 12 if the rank is sufficiently small.\n3We take the minimum of kQ and kK because we can\nchoose the multiplication order to minimize computation.\nValue projection. After computing the attention\nscore matrix\nSi = softmax\n\u0012 QiK⊤\ni\n√Dhead\n\u0013\n∈RL×L,\n(13)\nthe final output is obtained by multiplying Si with\nVi:\nSiVi = Si\n\u0010\n(XWV1)W i\nV2 + bi\nV\n\u0011\n= Si(XWV1)W i\nV2 + Sibi\nV .\n(14)\nConventionally,\none\nwould\nfirst\ncompute\n(XWV1)W i\nV2 and then multiply by Si, which\nwould cost O(L2Dhead + L kV Dhead) FLOPs.\nHowever, by computing Si(XWV1) first, the cost\nbecomes O(L2kV + L kV Dhead) FLOPs, making\nthis approach more efficient when kV < Dhead.\nMoreover, since each row of Si sums to 1, the\nbias term simplifies4:\nSibi\nV = bi\nV .\n(15)\nThus, the value projection can be rewritten as:\nSiVi =\n\u0010\nSi(XWV1)\n\u0011\nW i\nV2 + bi\nV ,\n(16)\nwhich is more efficient if kV < Dhead.\nImplementation. To efficiently execute the oper-\nations in Equations 12 and 16, we implement a\nspecialized kernel using Triton (Tillet et al., 2019).\nThis kernel extends the original FlashAttention im-\nplementation (Dao et al., 2022) to handle our opti-\nmized computation strategy.\n4\nExperiments\nIn this section, we describe our experimental setup\nand results, focusing on both the accuracy and effi-\nciency of LITEASR.\n4.1\nSetup\nOur primary accuracy evaluation focuses on com-\npressing Whisper large-v3 and Whisper large-v3-\nturbo, both of which have encoders of the same\nsize. We use test data from the End-to-end Speech\nBenchmark (ESB) (Gandhi et al., 2022), a com-\nprehensive collection of English ASR benchmark-\ning datasets, to assess the word error rate (WER)\nof both the compressed and original models. We\nrandomly choose 1000 audio clips from each\nof the eight subsets of ESB: Voxpopuli, AMI,\nEarnings-22, GigaSpeech, LibriSpeech (test.clean\n4Here, bi\nV is broadcasted across each row of Si.\n5\n\n\nModel\nConfig.\nWER (↓)\nSize (↓)\nVP\nAMI\nE22\nGS\nLS-C\nLS-O\nSG\nTED\nAvg.\nLarge-v3\nOriginal\n8.8\n25.9\n19.5\n11.1\n2.4\n5.5\n3.3\n4.4\n10.1\n635M (100.0%)\nLITEASR (a)\n8.7\n25.7\n18.9\n11.1\n2.5\n5.0\n3.4\n5.1\n10.1\n429M (67.6%)\nLITEASR (b)\n8.4\n28.7\n15.8\n12.0\n2.7\n6.1\n3.1\n4.8\n10.2\n377M (59.4%)\nLITEASR (c)\n8.7\n33.4\n17.2\n12.3\n2.8\n7.4\n3.5\n5.4\n11.3\n308M (48.5%)\nTurbo\nOriginal\n9.5\n26.8\n17.4\n11.4\n2.6\n5.5\n3.8\n4.3\n10.1\n635M (100.0%)\nLITEASR (a)\n9.0\n27.7\n17.0\n11.4\n2.8\n6.2\n3.1\n4.5\n10.2\n421M (66.2%)\nLITEASR (b)\n8.9\n43.2\n16.7\n11.7\n3.1\n7.8\n4.0\n5.0\n12.6\n374M (58.8%)\nLITEASR (c)\n10.8\n69.7\n35.1\n16.0\n4.2\n13.7\n5.0\n6.4\n20.1\n313M (49.3%)\nMedium\nOriginal\n8.7\n31.3\n25.9\n25.9\n3.9\n8.8\n5.9\n8.2\n14.8\n306M (48.1%)\nTable 1: Accuracy measured by WER percentages on ESB benchmarks and encoder sizes across different configu-\nrations. Abbreviations: VP (Voxpopuli), AMI (AMI), E22 (Earnings-22), GS (GigaSpeech), LS-C (LibriSpeech\ntest.clean), LS-O (LibriSpeech test.other), SG (SPGISpeech), TED (TED-LIUM). For encoder size, we show relative\nsize against the original Whisper large-v3 inside parenthesis.\nand test.other), SPGISpeech, and TED-LIUM. For\nthe calibration data, we randomly select 100 clips\n(non-overlapping with the test data), and the cal-\nibration process is completed within 10 minutes\nusing a single RTX 4090 GPU. We employ greedy\nsampling with a temperature set to 0.\nWe present three configurations of θ for different\ndeployment requirements: (a) Quality-Focused:\nθ = 0.999 for all layers. (b) Balanced: θ = 0.99\nfor self-attention layers and θ = 0.999 for MLP\nlayers. (c) Efficiency-Focused: θ = 0.99 for self-\nattention layers and θ = 0.995 for MLP layers.\nLater, we conduct a sensitivity study for different\nvalues of θ, languages, and models.\nRegarding efficiency, we evaluated the encoder\nlatency on NVIDIA RTX 4090, NVIDIA RTX\nA6000, and Apple M1 Pro. For GPUs, we mod-\nify OpenAI’s Whisper implementation5 to use\nCUDA Graph with PyTorch (Ansel et al., 2024)\n(ver. 2.5.1), and we use Triton (Tillet et al., 2019)\n(ver. 3.2.0) for a customized self-attention GPU\nkernel. On the Apple device, we use MLX (Hannun\net al., 2023) (ver. 0.21.1). The presented latency\ndata are averaged over 10 runs. Note that the en-\ncoder always takes fixed-length audio as input, so\nthe computation requirement is exactly the same\nfor different data.\n4.2\nAccuracy Evaluation\nTable 1 compares the WER and encoder size.\nLITEASR is evaluated on Whisper large-v3 and\nWhisper large-v3-turbo models, with Whisper\nmedium as a reference. The quality-focused config-\n5https://github.com/openai/whisper\nRTX 4090\nRTX A6000\nM1 Pro\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nSpeedup\nOriginal\nLiteASR (a)\nLiteASR (b)\nLiteASR (c)\nFigure 4: Execution speed of the encoder in Whisper\nlarge-v3, compared as a ratio to the original model.\nuration (a) cuts model size by over 30% with less\nthan 0.1 percentage points degradation of WER\nfor both Whisper large-v3 and Whisper large-v3-\nturbo. For more efficiency-focused scenarios, con-\nfiguration (b) reduces encoder size by over 40%\nwith comparable WER for Whisper large-v3, and\nabout 2.5 points degradation for Whisper large-\nv3-turbo. Configuration (c) compresses Whisper\nlarge-v3 model to less than half, matching Whis-\nper medium’s size, with better WER by about 3.5\npoints. Overall, LITEASR significantly reduces\nthe model size while largely maintaining accuracy.\n4.3\nEfficiency Evaluation\nFigure 4 presents the efficiency evaluation results,\nmeasuring the speedup of end-to-end latency of\nthe encoder execution compared to the original\nmodel. LITEASR consistently achieves speed im-\nprovements across all three hardware setups, with\naverage speedups of 1.29x for (a), 1.38x for (b),\nand 1.54x for (c). The best performance is ob-\nserved with the RTX 4090 using (c), reaching a\n6\n\n\nRTX 4090\nRTX A6000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nSpeedup\nTorch SDPA\nrank=32\nrank=16\nFigure 5: Our Triton kernel’s performance against Py-\nTorch implementation.\nQ_proj\nK_proj\nV_proj\nO_proj\nFC1\nFC2\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\nLayer 13\nLayer 14\nLayer 15\nLayer 16\nLayer 17\nLayer 18\nLayer 19\nLayer 20\nLayer 21\nLayer 22\nLayer 23\nLayer 24\nLayer 25\nLayer 26\nLayer 27\nLayer 28\nLayer 29\nLayer 30\nLayer 31\nLayer 32\n0.03 0.03 0.04 0.03 0.06 0.21\n0.05 0.05 0.10 0.09 0.04 0.17\n0.04 0.04 0.12 0.12 0.03 0.09\n0.04 0.04 0.15 0.12 0.04 0.07\n0.05 0.04 0.16 0.11 0.04 0.12\n0.04 0.04 0.15 0.11 0.04 0.12\n0.06 0.05 0.24 0.16 0.04 0.17\n0.07 0.06 0.20 0.16 0.05 0.34\n0.07 0.06 0.23 0.17 0.06 0.40\n0.07 0.06 0.23 0.17 0.06 0.40\n0.05 0.04 0.23 0.19 0.06 0.41\n0.06 0.05 0.25 0.17 0.06 0.41\n0.05 0.05 0.28 0.17 0.07 0.46\n0.05 0.04 0.23 0.19 0.08 0.54\n0.06 0.05 0.35 0.20 0.09 0.51\n0.04 0.04 0.34 0.20 0.07 0.60\n0.05 0.04 0.21 0.17 0.08 0.62\n0.07 0.06 0.29 0.23 0.09 0.61\n0.06 0.05 0.24 0.20 0.10 0.66\n0.06 0.06 0.33 0.21 0.10 0.59\n0.07 0.06 0.34 0.28 0.08 0.01\n0.09 0.07 0.35 0.26 0.13 0.75\n0.11 0.09 0.42 0.35 0.12 0.76\n0.10 0.07 0.36 0.33 0.14 0.81\n0.14 0.10 0.44 0.35 0.15 0.84\n0.14 0.10 0.46 0.33 0.15 0.86\n0.15 0.11 0.49 0.39 0.17 0.85\n0.15 0.10 0.47 0.31 0.18 0.80\n0.16 0.11 0.46 0.35 0.18 0.74\n0.21 0.14 0.44 0.39 0.19 0.76\n0.21 0.15 0.44 0.36 0.17 0.78\n0.21 0.14 0.34 0.36 0.16 0.57\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nCompression Ratio\nFigure 6: Compression ratio for each linear layer of\nWhisper large-v3. Smaller values mean more aggressive\ncompression\npeak speedup of 1.57x.\nMoreover, Figure 5 compares our Triton kernel’s\nperformance with PyTorch’s scaled dot product\nattention (SDPA) implementation in Whisper large-\nv3’s encoder self-attention layers. The RTX 4090\nshows roughly 17% and 30% improvements over\nbaseline, while the RTX A6000 exhibits gains of\napproximately 14% and 22% for matrix ranks 32\nand 16, respectively (i.e., kQ, kK, kV in §3, assum-\ning all share the same value).\n4.4\nAnalysis\n4.4.1\nCompression Ratio per Layer\nFigure 6 illustrates the compression ratio (i.e., de-\nfined as the quotient of k divided by the original\ndimension size) for each linear layer within the\n0.95\n0.99\n0.995\n0.999\n10\n12\n14\n16\n18\n20\nWER\n for attention = 0.95\n for attention = 0.99\n for attention = 0.995\n for attention = 0.999\n0.95\n0.99\n0.995\n0.999\n for MLP\n2\n3\n4\nEncoder size\n1e8\nFigure 7: Sensitivity of WER and encoder size on the\nvalue of θ.\nWhisper large-v3 encoder. The data is presented\nfor configuration (c). In general, the initial lay-\ners allow for more substantial compression, with\nsome exceptions, such as the FC2 stage in layer 21.\nThis tendency is most pronounced in FC2 layers,\nwhere the earlier layers exhibit a compression ra-\ntio of less than 0.2, whereas the subsequent layers\nreach values larger than 0.8. Among the layers, the\nQ/K projection and FC1 layers display a smaller\ncompression ratio compared to other layers.\n4.4.2\nSensitivity to θ\nFigure 7 analyzes the sensitivity of the average\nWER and encoder size to θ by independently vary-\ning θ from 0.95 to 0.999 for self-attention and MLP\nlayers in Whisper large-v3. Our results show a sig-\nnificant increase in WER when θ is below 0.99\nfor both layers. In contrast, WER improves as θ\nincreases, with θ = 0.999 achieving the best per-\nformance. The encoder size exhibits the opposite\ntrend, positively correlated with θ in a steady and\nroughly linear fashion. In the extreme scenario\nwhere θ = 0.95 is applied to both layers, the en-\ncoder size can be reduced by around 80%, although\nthis comes with a significant increase in WER.\n4.4.3\nSensitivity to Languages\nTo further investigate how LITEASR generalizes\nto out-of-distribution data and its sensitivity to lan-\nguages, we extend our evaluation to non-English\nbenchmarks. We use MLS (Pratap et al., 2020)\nfor French and German, and the JSUT basic5000\n(Sonobe et al., 2017) for Japanese6. Here, we use\n6For Japanese, we use the character error ratio (CER) in-\nstead of the WER since Japanese does not have explicit word\n7\n\n\nConfig\nWER (↓)\nCER (↓)\nSize (↓)\nFR\nDE\nJA\nOriginal\n7.2\n13.2\n10.8\n635M\nLITEASR (a)\n7.4\n8.7\n10.7\n429M\nLITEASR (b)\n6.8\n7.7\n11.2\n377M\nLITEASR (c)\n9.1\n10.1\n12.4\n308M\nTable 2: Sensitivity study on other languages. Abbrevi-\nations: FR (French), DE (German), JA (Japanese).\nConfig\nWER (↓)\nSize (↓)\nOriginal\n9.1\n609M (100.0%)\nLITEASR (a)\n9.1\n593M (97.3%)\nLITEASR (b)\n9.1\n579M (95.0%)\nLITEASR (c)\n9.1\n545M (89.4%)\nTable 3: Accuracy and encoder size with Canary 1B\nmodel.\nthe same English calibration data as in previous\nexperiments to compress Whisper large-v3, and\nevaluate its accuracy on non-English audio. The re-\nsults presented in Table 2 demonstrate LITEASR’s\nrobustness: for (a), there is almost no degradation\nin accuracy, and even for (c), the degradation is less\nthan 2 percentage points in WER/CER. In some\ncases, such as with German, we even observe an\nimprovement in accuracy.\n4.4.4\nSensitivity to Models\nWe also evaluate on Canary 1B (Puvvada et al.,\n2024), NVIDIA’s state-of-the-art ASR model, to\ndetermine LITEASR’s applicability to a broader\nrange of models.\nThe encoder of Canary em-\nploys the FastConformer architecture (Rekesh et al.,\n2023), and our optimizations are confined to linear\nlayers within the feed-forward and self-attention\nmodules, leaving the convolution modules unal-\ntered. Table 3 presents the encoder size and the\naverage WER for ESB datasets. The data indicates\nthat there is minimal degradation in the WER, al-\nthough the reduction in size is moderate compared\nto the Whisper models, achieving approximately a\n10% reduction for configuration (c).\n5\nRelated Work\n5.1\nEfficient ASR Inference\nSeveral prior works have aimed to enhance ASR\nmodel efficiency. FasterWhisper uses optimized\nboundaries.\ninference kernels (SYSTRAN, 2023), while Whis-\nperX further improves it for long-form audio (Bain\net al., 2023).\nWhisper.cpp is a C/C++ imple-\nmentation for portability on both the CPU and\nGPU (Gerganov, 2023). Whisper_streaming sup-\nports live transcription for streaming purposes\n(Macháˇcek et al., 2023). NVIDIA’s NeMo is a mod-\nular toolkit for deploying speech models (Harper\net al.). However, they do not effectively reduce\nASR encoder computational demands. Some works\nprovide model weight quantization, but they are\nlimited to weights (weight-only quantization) and\ndo not accelerate the compute-bound encoder in-\nference. Our approach can be integrated with these\nframeworks.\nVarious studies, including Whisper large-v3-\nturbo, Distill-Whisper, and Kotoba-Whisper use\ndistillation techniques to shrink decoder size (Rad-\nford et al., 2023; Gandhi et al., 2023; Kotoba Tech-\nnologies, 2024). Other approaches combine dis-\ntillation with quantization or lightweight modular\nASR fine-tuning for underrepresented languages\n(Shao et al., 2023; Ferraz et al., 2023). Our work\ncomplements these efforts by further reducing the\nencoder’s computational requirements.\n5.2\nModel Compression with Low-Rank\nApproximation\nThe low-rank approximation has been used to\ncompress machine learning models, such as for\nparameter-efficient fine-tuning (Hu et al., 2021)\nor the LLM’s KV cache compression (Liu et al.,\n2024; Chang et al., 2024). Yu and Wu (2023) has\nsuggested that activations in Transformer models\nexhibit low-rank and compressed models, mainly\ntargeting vision models. However, their method is\nlimited to linear layers, leaving self-attention layers\nunoptimized, and its applicability to speech models\nhas not been studied.\n6\nConclusion\nIn this work, we introduced a compression method\nfor ASR encoders that leverages the inherent low-\nrank structure of activations in linear layers. By\napplying the PCA algorithm, this method approxi-\nmates linear layers with a chain of low-rank matrix\nmultiplications and optimizes self-attention to op-\nerate in a reduced dimension. Our comprehensive\nevaluation demonstrates that our method achieves\na Pareto-optimal balance between accuracy and ef-\nficiency, paving the way for more efficient ASR\n8\n\n\ndeployments for both on-device and data center\nenvironments.\n7\nLimitations\nOur method focuses on compressing linear layers\nand self-attention mechanism, yielding substantial\nimprovements for Whisper models. However, other\narchitectures, such as the Conformer, include ad-\nditional components such as convolution layers,\nwhich may provide further compression opportu-\nnities (see §4).\nAdditionally, our evaluation is\ncurrently limited to standard benchmarks in En-\nglish and a few other major languages; evaluat-\ning performance on low-resource languages and\ndomain-specific applications remains an important\ndirection for future research. Finally, while our im-\nprovements do not introduce new risks per se, the\nenhanced efficiency could accelerate the broader\nadoption of ASR systems, which may amplify con-\ncerns related to privacy, surveillance, or inherent\nbiases in large-scale deployments.\n8\nEthics Statement\nAll data and models used in this paper are pub-\nlicly accessible and are distributed under Creative\nCommons, Apache-2.0, MIT, or other open-source\nlicenses that permit research use.\nReferences\nJason Ansel, Edward Yang, Horace He, Natalia\nGimelshein, Animesh Jain, Michael Voznesensky,\nBin Bao, Peter Bell, David Berard, Evgeni Burovski,\net al. 2024.\nPytorch 2: Faster machine learning\nthrough dynamic python bytecode transformation and\ngraph compilation. In Proceedings of the 29th ACM\nInternational Conference on Architectural Support\nfor Programming Languages and Operating Systems,\nVolume 2, pages 929–947.\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zis-\nserman. 2023.\nWhisperx: Time-accurate speech\ntranscription of long-form audio.\narXiv preprint\narXiv:2303.00747.\nAntonio Bevilacqua, Paolo Saviano, Alessandro Ami-\nrante, and Simon Pietro Romano. 2024. Whispy:\nAdapting stt whisper models to real-time environ-\nments. arXiv preprint arXiv:2405.03484.\nChi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-\nYan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi\nHuang, Luis Ceze, Mohamed S Abdelfattah, and\nKai-Chiang Wu. 2024.\nPalu: Compressing kv-\ncache with low-rank projection.\narXiv preprint\narXiv:2407.21118.\nLequn Chen. 2023. Dissecting batching effects in gpt\ninference.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher Ré. 2022.\nFlashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nAdvances in Neural Information Processing Systems,\n35:16344–16359.\nThomas Palmeira Ferraz, Marcely Zanon Boito, Car-\noline Brun, and Vassilina Nikoulina. 2023. Distil-\nwhisper: Efficient distillation of multi-task speech\nmodels via language-specific experts. arXiv preprint\narXiv:2311.01070.\nSanchit Gandhi, Patrick Von Platen, and Alexander M\nRush. 2022. Esb: A benchmark for multi-domain\nend-to-end speech recognition.\narXiv preprint\narXiv:2210.13352.\nSanchit Gandhi, Patrick von Platen, and Alexander M\nRush. 2023. Distil-whisper: Robust knowledge distil-\nlation via large-scale pseudo labelling. arXiv preprint\narXiv:2311.00430.\nGeorgi Gerganov. 2023. whisper.cpp: Port of openai’s\nwhisper model in c/c++.\nhttps://github.com/\nggerganov/whisper.cpp.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al.\n2020. Conformer: Convolution-augmented trans-\nformer for speech recognition.\narXiv preprint\narXiv:2005.08100.\nAwni Hannun, Jagrit Digani, Angelos Katharopoulos,\nand Ronan Collobert. 2023.\nMLX: Efficient and\nflexible machine learning on apple silicon.\nEric Harper, Somshubra Majumdar, Oleksii Kuchaiev,\nLi Jason, Yang Zhang, Evelina Bakhturina, Vahid\nNoroozi, Sandeep Subramanian, Koluguri Nithin,\nHuang Jocelyn, Fei Jia, Jagadeesh Balam, Xuesong\nYang, Micha Livne, Yi Dong, Sean Naren, and Boris\nGinsburg. NeMo: a toolkit for Conversational AI\nand Large Language Models.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nPo-Sen Huang, Scott Deeann Chen, Paris Smaragdis,\nand Mark Hasegawa-Johnson. 2012. Singing-voice\nseparation from monaural recordings using robust\nprincipal component analysis. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 57–60. IEEE.\nNat Jeffries, Evan King, Manjunath Kudlur, Guy Nichol-\nson, James Wang, and Pete Warden. 2024. Moon-\nshine: Speech recognition for live transcription and\nvoice commands. arXiv preprint arXiv:2410.15608.\n9\n\n\nAbdellah Kacha, Francis Grenez, Juan Rafael Orozco-\nArroyave, and Jean Schoentgen. 2020.\nPrincipal\ncomponent analysis of the spectrogram of the speech\nsignal: Interpretation and application to dysarthric\nspeech. Computer Speech & Language, 59:114–122.\nKotoba\nTechnologies.\n2024.\nKotoba-whisper\n(v2.0). https://huggingface.co/kotoba-tech/\nkotoba-whisper-v2.0.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th\nSymposium on Operating Systems Principles, pages\n611–626.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. 2024.\nDeepseek-v3 technical report.\narXiv preprint\narXiv:2412.19437.\nDominik Macháˇcek, Raj Dabre, and Ondˇrej Bojar. 2023.\nTurning whisper into real-time transcription system.\narXiv preprint arXiv:2307.14743.\nThai Son Nguyen, Jan Niehues, Eunah Cho, Thanh-Le\nHa, Kevin Kilgour, Markus Muller, Matthias Sperber,\nSebastian Stueker, and Alex Waibel. 2020. Low la-\ntency asr for simultaneous speech translation. arXiv\npreprint arXiv:2003.09891.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel\nSynnaeve, and Ronan Collobert. 2020. Mls: A large-\nscale multilingual dataset for speech research. arXiv\npreprint arXiv:2012.03411.\nKrishna C Puvvada, Piotr ˙Zelasko, He Huang, Olek-\nsii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan,\nSomshubra Majumdar, Elena Rastorgueva, Zhehuai\nChen, Vitaly Lavrukhin, et al. 2024. Less is more:\nAccurate speech recognition & translation without\nweb-scale data. arXiv preprint arXiv:2406.19674.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak su-\npervision. In International conference on machine\nlearning, pages 28492–28518. PMLR.\nDima Rekesh, Nithin Rao Koluguri, Samuel Kriman,\nSomshubra Majumdar, Vahid Noroozi, He Huang,\nOleksii Hrinchuk, Krishna Puvvada, Ankur Kumar,\nJagadeesh Balam, et al. 2023. Fast conformer with\nlinearly scalable attention for efficient speech recog-\nnition. In 2023 IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU), pages 1–8.\nIEEE.\nHang Shao, Wei Wang, Bei Liu, Xun Gong, Haoyu\nWang, and Yanmin Qian. 2023. Whisper-kdq: A\nlightweight whisper via guided knowledge distilla-\ntion and quantization for efficient asr. arXiv preprint\narXiv:2305.10788.\nRyosuke Sonobe, Shinnosuke Takamichi, and Hiroshi\nSaruwatari. 2017.\nJsut corpus: free large-scale\njapanese speech corpus for end-to-end speech synthe-\nsis. arXiv preprint arXiv:1711.00354.\nSYSTRAN. 2023.\nFaster whisper transcription\nwith ctranslate2. https://github.com/SYSTRAN/\nfaster-whisper.\nYusheng Tian, Junbin Liu, and Tan Lee. 2024. User-\ndriven voice generation and editing through latent\nspace navigation. arXiv e-prints, pages arXiv–2408.\nPhilippe Tillet, Hsiang-Tsung Kung, and David Cox.\n2019. Triton: an intermediate language and com-\npiler for tiled neural network computations. In Pro-\nceedings of the 3rd ACM SIGPLAN International\nWorkshop on Machine Learning and Programming\nLanguages, pages 10–19.\nPeidong Wang, Eric Sun, Jian Xue, Yu Wu, Long\nZhou, Yashesh Gaur, Shujie Liu, and Jinyu Li. 2022.\nLamassu: Streaming language-agnostic multilingual\nspeech recognition and translation using neural trans-\nducers. arXiv preprint arXiv:2211.02809.\nSamuel Williams, Andrew Waterman, and David Pat-\nterson. 2009. Roofline: an insightful visual perfor-\nmance model for multicore architectures. Communi-\ncations of the ACM, 52(4):65–76.\nSvante Wold, Kim Esbensen, and Paul Geladi. 1987.\nPrincipal component analysis. Chemometrics and\nintelligent laboratory systems, 2(1-3):37–52.\nHao Yu and Jianxin Wu. 2023. Compressing transform-\ners: features are low-rank, but weights are not! In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 37, pages 11007–11015.\nKawthar Yasmine Zergat and Abderrahmane Amrouche.\n2013. Robust support vector machines for speaker\nverification task. arXiv preprint arXiv:1306.2906.\n10\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20583v1.pdf",
    "total_pages": 10,
    "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
    "authors": [
      "Keisuke Kamahori",
      "Jungo Kasai",
      "Noriyuki Kojima",
      "Baris Kasikci"
    ],
    "abstract": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper,\nrely on deep encoder-decoder architectures, and their encoders are a critical\nbottleneck for efficient deployment due to high computational intensity. We\nintroduce LiteASR, a low-rank compression scheme for ASR encoders that\nsignificantly reduces inference costs while maintaining transcription accuracy.\nOur approach leverages the strong low-rank properties observed in intermediate\nactivations: by applying principal component analysis (PCA) with a small\ncalibration dataset, we approximate linear transformations with a chain of\nlow-rank matrix multiplications, and further optimize self-attention to work in\nthe reduced dimension. Evaluation results show that our method can compress\nWhisper large-v3's encoder size by over 50%, matching Whisper medium's size\nwith better transcription accuracy, thereby establishing a new Pareto-optimal\nfrontier of efficiency and performance. The code of LiteASR is available at\nhttps://github.com/efeslab/LiteASR.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}