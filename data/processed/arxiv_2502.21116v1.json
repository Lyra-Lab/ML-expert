{
  "id": "arxiv_2502.21116v1",
  "text": "The two filter formula reconsidered:\nSmoothing in partially observed Gauss–Markov models\nwithout information parametrization\nFilip Tronarp\nCentre for Mathematical Sciences, Lund University\nMarch 3, 2025\nAbstract\nIn this article, the two filter formula is re-examined in the setting of partially\nobserved Gauss–Markov models. It is traditionally formulated as a filter running\nbackward in time, where the Gaussian density is parametrized in “information form”.\nHowever, the quantity in the backward recursion is strictly speaking not a distribution,\nbut a likelihood. Taking this observation seriously, a recursion over log-quadratic likeli-\nhoods is formulated instead, which obviates the need for “information” parametrization.\nIn particular, it greatly simplifies the square-root formulation of the algorithm. Fur-\nthermore, formulae are given for producing the forward Markov representation of the\na posteriori distribution over paths from the proposed likelihood representation.\n1\nIntroduction\nConsider the following partially observed Markov process\nx0 ∼π0(· )\n(1a)\nxt | xt−1 ∼πt|t−1(· | xt−1)\n(1b)\nyt | xt ∼ht|t(· | xt),\n(1c)\nwhere xt is the state at time t, taking values in Rn, and yt is the measurement or\nobservation at time t, taking values in Rm, with m ≤n. Let ti ≤¯ti for i = 1, 2, 3, denote\ntheir corresponding ranges by ti : ¯ti, and denote the distribution of xt2:¯t2 conditioned on\nxt1:¯t1 and yt3:¯t3 by\nπt3:¯t3\nt2:¯t2|t1:¯t1(xt2:¯t2 | xt1:¯t1, yt3:¯t3),\n(2)\nwhere the ranges t1 : ¯t1 and t2 : ¯t2 are non-overlapping. Similarly, denote the distribution\nof yt2:¯t2 conditioned on xt1:¯t1 and yt3:¯t3 by\nht3:¯t3\nt2:¯t2|t1:¯t1(yt2:¯t2 | xt1:¯t1, yt3:¯t3)\n(3)\nwhere the ranges t1 : ¯t1 and t2 : ¯t2 are non-overlapping. Lastly, the distribution of yt2:¯t2\nconditioned on yt3:¯t3 is denoted by\nLt2:¯t2|t3:¯t3(yt2:¯t2 | yt3:¯t3),\n(4)\n1\narXiv:2502.21116v1  [stat.ME]  28 Feb 2025\n\n\nwhere the ranges t1 : ¯t1 and t2 : ¯t2 are non-overlapping. Given a sequence of measurements\ny1:T the marginal likelihood, L1:T , and path posterior, π1:T\n0:T , are given by marginalization\nand Bayes’ rule:\nL1:T (y1:T ) =\nZ\nπ0(x0)\nT\nY\nt=1\nht|t(yt | xt)πt|t−1(xt | xt−1) dx0:T\n(5a)\nπ1:T\n0:T (x0:T | y1:T ) = L−1\n1:T (y1:T )π0(x0)\nT\nY\nt=1\nht|t(yt | xt)πt|t−1(xt | xt−1).\n(5b)\nAs the measurement sequence shall be considered fixed it will henceforth be omitted in\nthe expressions (2), (3), (4), and (5). The problem is thus to efficiently compute L1:T\nand interesting quantities from π1:T\n0:T , such as the smoothing distributions, π1:T\nt\n, or the a\nposteriori transition distributions π1:T\nt|s . There are essentially two methods to do this, the\nforward-backward method (Askar and Derin, 1981) and backward-forward method (Cappé\net al., 2005), a catalogue of different forward-backward and backward-forward methods is\nprovided by Ait-El-Fquih and Desbouvries (2008). The forward-backward method has\nthe advantage that it recursively computes the filtering distributions π1:t\nt\nforward in time;\nit is thus suitable for on-line inference (Anderson and Moore, 2012, Kailath et al., 2000,\nSärkkä and Svensson, 2023). It can also compute the backward transition distributions,\nπ1:t\nt|t+1, from which the smoothing distributions can be computed by the Rauch–Tung–\nStriebel algorithm (Rauch et al., 1965). On the other hand, the backward-forward method\ncomputes the likelihoods of the future ht:T|t−1 backward in time. It can also compute the\nforward transition distributions, πt:T\nt|t−1, from which the smoothing distributions can be\ncomputed once π1:T\n0\nhas been obtained (see theorem 1 below). Both methods can also be\ncombined to compute the smoothing distributions via the two-filter formula Bresler (1986)\nπ1:T\nt\n(x) ∝ht+1:T|t(x)π1:t\nt (x) ∝ht:T|t(x)π1:t−1\nt\n(x)\n(6)\nThe two filter formula has been used to construct algorithms for approximate inference\nin Markovian switching systems (Helmick et al., 1995), jump Markov linear systems\n(Balenzuela et al., 2022), navigation problems (Liu et al., 2010), and in sequential Monte\nCarlo (Lindsten et al., 2013, 2015, Särkkä et al., 2012). The abstract formulae for computing\nthe likelihoods ht:T|t, the a posteriori transition distributions πt:T\nt|t−1, and the marginal\nlikelihood, L1:T are given by theorem 1 (Cappé et al., 2005, chapter 3).\nTheorem 1 (The backward-forward method). Consider the partially observed Markov\nprocess in (1), then ht:T|t satisfy the following recursion:\nht:T|t−1(xt−1) =\nZ\nht:T|t(xt)πt|t−1(xt | xt−1) dxt\n(7a)\nht−1:T|t−1(xt−1) = ht−1|t−1(xt−1)ht:T|t−1(xt−1),\n(7b)\nand the marginal likelihood, L1:T is given by\nL1:T =\nZ\nh1:T|0(x0)π0(x0) dx0.\n(8)\nAdditionally, the posterior over paths is given by\nπ1:T\n0\n(x0) = h1:T|0(x0)π0(x0)\nL1:T\n(9a)\nπt:T\nt|t−1(xt | xt−1) = ht:T|t(xt)πt|t−1(xt | xt−1)\nht:T|t−1(xt−1)\n(9b)\nπ1:T\n0:T (x0:T ) = π1:T\n0\n(x0)ΠT\nt=1πt:T\nt|t−1(xt | xt−1).\n(9c)\n2\n\n\nIn this article, the backward-forward algorithm is re-examined when (1) is Gauss–\nMarkov. More specifically, π0, πt|t−1, and ht|t are then given by\nπ0(x) = N(x; µ0, Σ0)\n(10a)\nπt|t−1(x | z) = N(x; µt|t−1(z), Qt,t−1)\n(10b)\nht|t(y | x) = N(y; Ctx, Rt),\n(10c)\nwhere the conditional mean function, µt|t−1 is given by\nµt|t−1(z) = Φt,t−1z + ut,t−1.\n(11)\nThe general idea has been around since Fraser and Potter (1969), Mayne (1966) and\ninvovles treating ht:T|t as a distributon and computing it recursively backward in time\nstarting from hT|T by what is essentially a Kalman filter (Kalman, 1961). The method\nis related to the Lagrange multiplier method for the maximum a posteriori trajectory\n(Watanabe, 1985). However, ht:T|t is not a distribution, and may not be normalizable on\nRn (Cappé et al., 2005). This problem is circumvented by propagating an information\nvector, ξt:T\nt\n, and information matrix, Λt:T\nt\ninstead of a mean vector and covariance matrix\n(Fraser and Potter, 1969). The information matrix is related to the covariance matrix by\ninversion. More specifically, ht:T|t is parametrized according to (Balenzuela et al., 2022)\nlog ht:T|s(x) = log N\n\u0010\nx; (Λt:T\nt|s )−1ξt:T\nt|s , (Λt:T\nt|s )−1\u0011\n+ const\n= −1\n2⟨x, Λt:T\nt|s x⟩+ ⟨ξt:T\nt|s , x⟩+ const\n(12)\nwhere the inverse is strictly formal since Λt:T\nt|s may not be invertible. As remarked by\nBalenzuela et al. (2018, 2022), the constant of proportionality, say ˜ct|s, is often omitted\nfrom the computation. This is troublesome in parameter estimation problems as it is\nrequired for computing the marginal likelihood, L1:T (c.f. theorem 1). The backward\nrecursion for the parametrization (12) is essentially a so-called information filter is run\nbackwards in time (Anderson and Moore, 2012, sec 6.3). Some formulas in the literature\nrequire Φt,t−1 to be invertible (Kailath et al., 2000, section 10.4.2). Other formulas require\nthat Qt,t−1 be invertible (Kitagawa, 2023, section 3.2). These assumptions are restrictive\nand not even required (Balenzuela et al., 2018, 2022, Fraser and Potter, 1969, Särkkä and\nGarcía-Fernández, 2020).\n1.1\nContribution\nBy regarding the backward method as a recursion over likelihoods rather than as a recursion\nover distributions, an algorithm is obtained which obviates the need for “information”\nparametrization. More specifically, ht:T|t is parametrized according to\nht:T|t(x) = ct:T|t exp\n\u0014\n−1\n2 |¯yt:T|t −¯Ct:T|tx|\n2 \u0015\n.\nThis has the consequence that ht:T|t−1 and πt:T\nt|t−1 can be computed by a method that is\nalgebraically equivalent to the classical Kalman update in mean/covariance parametrization.\nIn particular, the author is not aware of anyone giving a method for computing πt:T\nt|t−1 from\nthe backwards recursion. The price to be paid for this convenience is that the formula for\ncomputing ht−1:T|t−1 from ht:T|t−1 and ht|t is slightly more complicated. More specifically,\n3\n\n\na QR factorization is required to ensure that the dimensionality ¯mt|t of ¯yt:T|t does not grow\nbeyond n. Additionally, a method only operating on covariance square-roots is developed,\nassuming that the square-roots R1/2\nt\nand Q1/2\nt,t−1 are given. 1 2\nThe remainder of this article is organizes as follows. In section 2 the forward-backward\nmethod in the proposed likelihood parametrization is developed, in section 3 this is\ngeneralized to only operate on Cholesky factors of covariances, and in section 4 it is shown\nthat the two filter formula becomes algebraically equivalent to the classical Kalman update\nin the proposed parametrization. Section 5 discusses the interpretation of the likelihood as\na density and maximum likelihood estimation of the state based on future measurements.\nIn section 6, the proposed algorithm is demonstrated in a state estimation problem with\nflat initial distribution, and a concluding discussion is given in section 7.\n2\nThe new backward-forward recursion\nIn this section, a method for implementing theorem 1 is derived. The novelity lies in that\nht:T|s for s = t −1, t is identified with a linear Gaussian likelihood with identity covariance.\nConsequently, the likelihood recursion (7) reduces to a recursion over a measurement\nvector ¯yt:T|s and a measurement matrix ¯Ct:T|s. The following assumption is not essential\nto carry out this program, but simplifies the exposition greatly.\nAssumption 1. The measurement covariance matrices {Rt}T\nt=1 are positive definite.\n2.1\nThe backward recursion\nLet y be a vector and C and R matrices of appropriate dimensions. Denote by Q(x; y, C, R)\nthe following quadratic function:\nQ(x; y, C, R) = (y −Cx)∗R−1(y −Cx).\n(13)\nWhen R is the identity matrix, R = I, it is omitted from the notation like so:\nQ(x; y, C, I) = Q(x; y, C).\n(14)\nNote that when R is positive definite then (13) is overparametrized in the sense that R\nhas a unique Cholesky factorization R = R1/2R∗/2, therefore 3\nQ(x; y, C, R) = (y −Cx)∗R−1(y −Cx) = |R−1/2y −R−1/2Cx|\n2\n= Q(x; R−1/2y, R−1/2C).\n(15)\nThis section is dedicated to proving the following theorem and in the process deriving a\nrecursion for the parameters in (16b) and (16c). The method of proof is by induction.\nTheorem 2. Let assumption 1 be in effect, then πt:T\nt|t−1 and ht:T|s for s = t −1, t and\nt ≤T have the following representation:\nµt:T\nt|t−1(z) = Φt:T\nt,t−1z + ut:T\nt,t−1\n(16a)\nπt:T\nt|t−1(x | z) = N(x; µt:T\nt|t−1(z), Qt:T\nt,t−1)\n(16b)\nht:T|s(x) = ct:T|s exp\n\u0014\n−1\n2Q(x; ¯yt:T|s, ¯Ct:T|s)\n\u0015\n,\n(16c)\n1However, R1/2 is already required for the first method.\n2When the estimation problem originates from a continuous-discrete problem, then Q1/2\nt,t−1 may be\nobtained directly by a recent algorithm (Stillfjord and Tronarp, 2023).\n3Note that Q(x; y, C) is still overparametrized for general C. By the QR factorization, C = QCRC so\nQ(x; y, C) = Q(x; Q∗\nCy, RC) since QC is unitary. This is to some extent taken care of by proposition 3.\n4\n\n\nwhere ¯yt:T|s ∈R ¯mt:T |s and ¯Ct:T|s ∈R ¯mt:T |s×n and m ≤¯mt:T|s ≤n.\nThe formula (16c) clearly hold when T = t = s, since by assumption 1 and (15)\nht|t(x) = N(yt; Ctx, Rt) =|2πRt|−1/2 exp\n\u0014\n−1\n2Q(x; yt, Ct, Rt)\n\u0015\n=|2πRt|−1/2 exp\n\u0014\n−1\n2Q(x; R−1/2\nt\nyt, R−1/2\nt\nCt)\n\u0015\n= ct|t exp\n\u0014\n−1\n2Q(x; ¯yt|t, ¯Ct|t)\n\u0015\n,\nwhere the parameters are given by\nct|t =|2πRt|−1/2 ,\n(17a)\n¯yt|t = R−1/2\nt\nyt,\n(17b)\n¯Ct|t = R−1/2\nt\nCt.\n(17c)\nIn particular that means that the terminal condition to the the recurrence in theorem 1,\nhT:T|T = hT|T , satisfies (16c). The following proposition shows that (16b) and (16c) hold\nfor s = t −1 provided that (16c) hold for s = t.\nProposition 1. Assume that the likelihood, ht:T|t, satisfies (16c), then the likelihood,\nht:T|t−1, and the transition distribution, πt:T\nt|t−1, satisfies (16c) and (16b), respectively, with\nparameters:\nˆRt:T|t−1 = I + ¯Ct:T|tQt,t−1 ¯C∗\nt:T|t,\n(18a)\n¯yt:T|t−1 = ˆR−1/2\nt:T|t−1\n\u0010\n¯yt:T|t −¯Ct:T|tut,t−1\n\u0011\n(18b)\n¯Ct:T|t−1 = ˆR−1/2\nt:T|t−1 ¯Ct:T|tΦt,t−1\n(18c)\nlog ct:T|t−1 = log ct:T|t−1 −1\n2 log | ˆRt:T|t−1|\n(18d)\n¯Kt:T\nt|t−1 = Qt,t−1 ¯C∗\nt:T|t ˆR−1\nt:T|t−1\n(18e)\nΦt:T\nt,t−1 =\n\u0010\nI −¯Kt:T\nt|t−1 ¯Ct:T|t\n\u0011\nΦt,t−1\n(18f)\nut:T\nt,t−1 = ut,t−1 + ¯Kt:T\nt|t−1\n\u0010\n¯yt:T|t −¯Ct:T|tut,t−1\n\u0011\n(18g)\nQt:T\nt,t−1 = Qt,t−1 −¯Kt:T\nt|t−1 ˆRt:T|t−1( ¯Kt:T\nt|t−1)∗,\n(18h)\nwhere ¯yt:T|t−1 ∈R ¯mt:T |t−1, ¯Ct:T|s ∈R ¯mt:T |t−1×n, and ˆRt:T|s ∈R ¯mt:T |t−1×n with\n¯mt:T|t−1 = ¯mt:T|t.\n(19)\nProof. The likelihood, ht:T|t, is by assumption given by\nht:T|t(x) = ct:T|t exp\n\u0014\n−1\n2Q(x; ¯yt:T|t, ¯Ct:T|t)\n\u0015\n= ct:T|t(2π) ¯mt|tN(¯yt:T|t; ¯Ct:T|tx, I).\nThe computation of ht:T|t−1 and πt:T\nt|t−1 is then by by theorem 1, up to a multiplicative\nconstant, equivalent to inference and marginalization in the following model\nxt | xt−1 ∼N(µt|t−1(xt−1), Qt,t−1)\n¯yt:T|t | xt, xt−1 ∼N(¯yt:T|t; ¯Ct:T|txt, I),\n5\n\n\nconditionally on xt−1. The solution is given by (Särkkä and Svensson, 2023, c.f. lemma\nA.2 and A.3)\nxt | xt−1, ¯yt:T|t ∼N(µt:T\nt|t−1(xt−1), Qt:T\nt,t−1)\n¯yt:T|t | xt−1 ∼N\n\u0010 ¯Ct:T|t\n\u0010\nΦt,t−1xt−1 + ut,t−1\n\u0011\n, ˆRt:T|t−1\n\u0011\n,\nwith the conditional mean function given by\nµt:T\nt|t−1(x) = µt|t−1(x) + ¯Kt:T\nt|t−1\n\u0010\n¯yt:T|t −¯Ct:T|tµt,t−1(x)\n\u0011\n= Φt:T\nt,t−1x + ut:T\nt,t−1.\nThe last equality is obtained by using (11) and simply matching terms. Furthermore,\nre-introducing the constant of proportionality gives ht:T|t−1 as\nht:T|t−1(x) = ct:T|t(2π) ¯mt|tN\n\u0010\n¯yt:T|t; ¯Ct:T|t\n\u0010\nΦt,t−1x + ut,t−1\n\u0011\n, ˆRt:T|t−1\n\u0011\n= ct:T|t(2π) ¯mt|tN(¯yt:T|t−1; ¯Ct:T|t−1x, I)\n= ct:T|t | ˆRt:T|t−1|\n−1/2 exp\n\u0014\n−1\n2Q(x; ¯yt:T|t−1, ¯Ct:T|t−1)\n\u0015\n,\nwhich concludes the proof.\nWith proposition 1 established, it remains to show that if ht:T|t−1 satisfies (16c), then\nso does ht−1:T|t−1. Under this presupposition, then by theorem 1 and (17), ht−1:T|t−1 is\ngiven by\nht−1:T|t−1(x) = ht:T|t−1(x)ht−1|t−1(x)\n= ct:T|t−1 exp\n\u0014\n−1\n2Q(x; ¯yt:T|t−1, ¯Ct:T|t−1)\n\u0015\n× ct−1|t−1 exp\n\u0014\n−1\n2Q(x; ¯yt−1|t−1, ¯Ct−1|t−1)\n\u0015\n(20)\nNow let ˆmt−1 = ¯mt:T|t−1 + m and define ˆyt:T|t−1 ∈R ˆmt−1 and ˆCt:T|t−1 ∈R ˆmt−1×n by\nˆyt−1:T|t−1 =\n \n¯yt:T|t−1\n¯yt−1|t−1\n!\n(21a)\nˆCt−1:T|t−1 =\n ¯Ct:T|t−1\n¯Ct−1|t−1\n!\n,\n(21b)\nthen the sum of quadratic functions in (20) evalutes to\nQ(x; ¯yt:T|t−1, ¯Ct:T|t−1) + Q(x; ¯yt−1|t−1, ¯Ct−1|t−1) = Q(x; ˆyt−1:T|t−1, ˆCt−1:T|t−1).\nIf ˆmt−1 ≤n this immediately gives an expression for ht−1:T|t−1 which satisfies the statement\nof theorem 1. The following proposition summarizes this observation.\nProposition 2. Assume ht:T|t−1 satisfies (16c) and that ˆmt−1 = ¯mt:T|t−1 + m ≤n, then\nht−1:T|t−1 satisfies (16c) with parameters\n¯yt−1:T|t−1 = ˆyt−1:T|t−1\n(22)\n¯Ct−1:T|t−1 = ˆCt−1:T|t−1\n(23)\nct−1:T|t−1 = ct:T|t−1ct|t,\n(24)\nmt−1:T|t−1 = ¯mt:T|t−1 + m.\n(25)\n6\n\n\nThe overdetermined case, when ¯mt:T|t−1 + m > n, requires slightly more effort.\nProposition 3. Assume ht:T|t−1 satisfies (16c) and ˆmt−1 = ¯mt:T|t−1 + m > n. Let VtUt\nbe a QR factorization of ˆCt−1:T|t−1 and partition the factors according to\nˆCt−1:T|t−1 = VtUt =\n\u0010\nVt,1\nVt,2\n\u0011  ¯Ct−1:T|t−1\n0( ˆmt−1−n)×n\n!\n,\n(26)\nwhere Vt,1 ∈R ˆmt−1×n, Vt,2 ∈R ˆmt−1×( ˆmt−1−n), and ¯Ct−1:T|t−1 ∈Rn×n. Furthermore, define\n¯yt−1:T|t−1 and et−1 by\n¯yt−1:T|t−1 = V ∗\nt,1ˆyt−1:T|t−1\n(27)\net−1 = V ∗\nt,2ˆyt−1:T|t−1,\n(28)\nthen ht−1:T|t−1 satisfies (16c) with remaining parameters given by\nct−1:T|t−1 = ct:T|t−1ct|t exp\n\u0014\n−|et−1|2\n2\n\u0015\n(29)\n¯mt−1:T|t−1 = n.\n(30)\nProof. By assumption and theorem 1, ht−1:T|t−1 is given by\nht−1:T|t−1(x) = ht:T|t−1(x)ht−1|t−1(x)\n= ct:T|t−1ct|t exp\n\u0014\n−1\n2Q(x; ˆyt−1:T|t−1, ˆCt−1:T|t−1)\n\u0015\n= ct:T|t−1ct|t exp\n\u0014\n−1\n2Q(x; ˆyt−1:T|t−1, VtUt)\n\u0015\n= ct:T|t−1ct|t exp\n\u0014\n−1\n2Q(x; V ∗\nt ˆyt−1:T|t−1, Ut)\n\u0015\n= ct:T|t−1ct|t exp\n\u0014\n−|et−1|2\n2\n\u0015\nexp\n\u0014\n−1\n2Q(x; ¯yt−1:T|t−1, ¯Ct−1:T|t−1)\n\u0015\n,\nwhich is the desired result.\nWith propositions 1, 2, and (3) established, theorem 2 hold by induction. It remains\nto give formulae for the forward recursion. This is done in the following.\n2.2\nThe forward recursion\nThe previous section established expressions for πt:T\nt|t−1 and ht:T|s for s = t −1, t, the\nparameters of which are obtained by alternating between proposition 1 and proposition 2\nor 3. It remains to compute the a posteriori time marginals, {π1:T\nt\n}T\nt=0, and the marginal\nlikelihood, L1:T . They are all clearly Gaussian and will be denoted by\nπ1:T\nt\n(x) = N(x; µ1:T\nt\n, Σ1:T\nt\n),\nt = 0, 1, . . . , T\n(31)\nAccording to theorem 1, L1:T and π1:T\n0\nare simply obtained by Bayes’ rule. By the same\nargument as in proposition 1, mutatis mutandis, they are given by\nS0 = ¯C1:T|0Σ0 ¯C∗\n1:T|0 + I\n(32a)\nK1:T\n0\n= Σ0 ¯C∗\n1:T|0S−1\n0\n(32b)\nµ1:T\n0\n= µ0 + K1:T\n0\n\u0010\n¯y1:T|0 −¯C1:T|0µ0\n\u0011\n(32c)\nΣ1:T\n0\n= Σ0 −K1:T\n0\nS0(K1:T\n0\n)∗\n(32d)\nL1:T = c1:T|0(2π) ¯mt|0/2N(¯y1:T|0; ¯C1:T|0µ0, S0)\n(32e)\n7\n\n\nFurthermore, the remaining time marginals are then obtained by\nµ1:T\nt\n= Φt:T\nt,t−1µ1:T\nt−1 + ut:T\nt,t−1\n(33a)\nΣ1:T\nt\n= Φt:T\nt,t−1Σ1:T\nt−1(Φt:T\nt,t−1)∗+ Qt:T\nt,t−1.\n(33b)\n3\nThe backward-forward recursion in Cholesky form\nIn order to ensure excellent numerical performance of the algorithm it is preferable to only\nwork with Cholesky factors of the covariance matrices (Kailath et al., 2000, section 12).\nComputing ht−1:T|t−1 from ht−1:T|t−1 and ht−1|t−1 does not involve any covariances as at\nall. Hence the only problem in the backward recursion is to compute ht:T|t−1 from ht:T|t\nand πt|t−1 without ever forming a covariance matrix. This can be done by so-called array\nalgorithms (Anderson and Moore, 2012, Kailath et al., 2000), the key idea is that for a\ngiven matrix A an upper triangular Cholesky factor of A∗A may be obtained by the QR\ndecomposition A = QU. For an arbitrary matrix A, write A ∼= U when U is the upper\ntriangular factor in the QR factorization of A.\nProposition 4. Consider the same setting as propositon 1, then\n\n\nI\n0\nQ∗/2\nt,t−1 ¯C∗\nt:T|t\nQ∗/2\nt,t−1\n\n∼=\n\nˆR∗/2\nt:T|t−1\n( ˆKt:T\nt|t−1)∗\n0\n(Qt:T\nt,t−1)∗/2\n\n,\n(34)\nwhere ˆKt:T\nt|t−1 is related to ¯Kt:T\nt|t−1 by\n¯Kt:T\nt|t−1 = ˆKt:T\nt|t−1 ˆR−1/2\nt:T|t−1,\n(35)\nand the parameters of ht:T|t−1 are given by\n¯yt:T|t−1 = ˆR−1/2\nt:T|t−1\n\u0010\n¯yt:T|t −¯Ct:T|tut,t−1\n\u0011\n(36)\n¯Ct:T|t−1 = ˆR−1/2\nt:T|t−1 ¯Ct:T|tΦt,t−1\n(37)\nlog ct:T|t−1 = log ct:T|t−1 −log | ˆR1/2\nt:T|t−1| .\n(38)\nFurthermore, the remaining parameters of πt:T\nt|t−1 are given by\nΦt:T\nt,t−1 = Φt,t−1 −ˆKt:T\nt|t−1 ¯Ct:T|t−1\n(39)\nut:T\nt,t−1 = ut,t−1 + ˆKt:T\nt|t−1¯yt:T|t−1.\n(40)\nRemark 1. Pay attention to the time index of ¯y and ¯C in the expressions for Φt:T\nt,t−1 and\nut:T\nt,t−1 in proposition 4. It differs from the corresponding expression in proposition 1.\nProof. The identities (34) and (35) are known (Anderson and Moore, 2012, section 6.5).\nThe proof concludes by inserting these into the relations given by proposition 1 and\nidentifying terms.\nThe parameters of the initial distribution are obtained by the same argument as\nproposition 4. More specifically, substitue Qt,t−1 and ¯Ct:T|t on the left-hand side of (34)\nwith Σ0 and ¯C1:T|0 to obtain S1/2\n0\n, ˆK1:T\n0\n, and (Σ1:T\n0\n)∗/2 as the right-hand side of (34). The\n8\n\n\nremaining parameter, µ1:T\n0\n, is then obtained by using the relation between ˆK1:T\n0\nand K1:T\n0\ncorresponding to (35) and substitution into (32)\nµ1:T\n0\n= µ0 + ˆK1:T\n0\nS−1/2\n0\n\u0010\n¯y1:T|0 −¯C1:T|0µ0\n\u0011\n.\n(41)\nFurthermore, the mean of the smoothing marginals is still obtained by (33), while the\nCholesky factor of the covariances clearly satisifes the following recursion:\n\u0010\nΦt:T\nt,t−1(Σ1:T\nt−1)1/2\n(Qt:T\nt,t−1)1/2\u0011∗∼= (Σ1:T\nt\n)∗/2.\n(42)\nThat is, (Σ1:T\nt\n)∗/2 is the upper triangular factor i the QR factorization of the left-hand\nside of the preceeding expression.\n4\nThe two-filter formula\nAssume the filtering densities π1:t\nt\nhave been obtained, by for example running a standard\nKalman filter (Särkkä and Svensson, 2023) or a square-root variant (Anderson and Moore,\n2012, section 6.5), then they are given by\nπ1:t\nt (x) = N(x; µ1:t\nt , Σ1:t\nt ).\n(43)\nFurthermore, by theorem 2, the two-filter formula (6) reduces to\nπ1:T\nt\n(x) ∝ht+1:T|t(x)π1:t\nt (x) ∝N(¯yt+1:T|t; ¯Ct+1:T|tx, I)N(x; µ1:t\nt , Σ1:t\nt ).\n(44)\nThis is equivalent to obtaining the posterior in the following model,\nxt ∼N(µ1:t\nt , Σ1:t\nt ),\n(45)\n¯yt+1:T|t | xt ∼N(¯yt+1:T|t; ¯Ct+1:T|txt, I),\n(46)\nhence the mean, µ1:T\nt\nand covariance, Σ1:T\nt\n, of π1:t\nt\ncan be obtained by a classical Kalman\nupdate or its’ square-root variant. (Anderson and Moore, 2012).\n5\nRelationship to information parametrization, maximum\nlikelihood estimation, and the likelihood as a density\nThe purpose of this section is to relate the present parametrization of ht:T|s with the\ninformation parametrization and to clarify its interpretation as a density. From the\npreceeding discussion, the likelihood ht:T|s for s = t −1, t is given by\nht:T|s(x) = c1:T|s exp\n\u0014\n−1\n2Q(x; ¯yt:T|s, ¯Ct:T|s)\n\u0015\n.\n(47)\nThe quadratic form above is given by\nQ(x; ¯yt:T|s, ¯Ct:T|s) = ⟨x, ¯C∗\nt:T|s ¯Ct:T|sx⟩−2⟨¯C∗\nt:T|s¯yt:T|s, x⟩+ const.\n(48)\nThe correspondence to the information parametrization is then (c.f. (12))\nξt:T\ns\n= ¯C∗\nt:T|s¯yt:T|s,\n(49a)\nΛt:T\ns\n= ¯C∗\nt:T|s ¯Ct:T|s.\n(49b)\n9\n\n\nHence, ¯Ct:T|s may be viewed as a ¯mt|s×n square-root of Λt:T\ns . The relationship between ξt:T\ns\nand ¯yt:T|s is a bit more opaque and perhaps best understood in terms of the mean/covariance\nparametrization. Let µt:T\ns\nbe the minimum norm solution to the following equation\n0 = ∇log ht:T|s(x) = ¯C∗\nt:T|s(¯yt:T|s −¯Ct:T|sx),\n(50)\nthen µt:T\ns\nis the maximum likelihood estimate of xs based on data yt:T of smallest norm. 4\nNow define Σt:T\ns\nby\nΣt:T\ns\n= (Λt:T\ns )+ = ( ¯C∗\nt:T|s ¯Ct:T|s)+ = ¯C+\nt:T|s( ¯C∗\nt:T|s)+\n(51)\nwhere + denotes a generalized matrix inverse and the last equality is taken known from\ne.g. Greville (1966). An explicit expression of µt:T\ns\nis then given by\nµt:T\ns\n= Σt:T\ns\n¯C∗\nt:T|s¯yt:T|s = ¯C+\nt:T|s( ¯C∗\nt:T|s)+ ¯C∗\nt:T|s¯yt:T|s = (Σt:T\ns )1/2( ¯C∗\nt:T|s)+ ¯C∗\nt:T|s¯yt:T|s.\n(52)\nThe observation vector ¯yt:T|s has identity covariance conditioned on xs (by construction),\nhence the conditional covariance of µt:T\ns\nis given by\nΣt:T\ns\n¯C∗\nt:T|s ¯Ct:T|sΣt:T\ns\n= (Λt:T\ns )+Λt:T\ns (Λt:T\ns )+ = (Λt:T\ns )+ = Σt:T\ns ,\n(53)\nwhich justifies the notation Σt:T\ns\nfor (Λt:T\ns )+. In view of (52), the observation vector ¯yt:T|s\nmay then be interpreted as a the mean vector µt:T\ns\n“whitened” by Σt:T\ns .\nIt is now tempting to interpret ht:T|s as an unnormalized Gaussian density with mean\nµt:T\ns\nand covariance Σt:T\ns . This can indeed be done, but the density will not not be\nsupported on the whole of Rn but rather on an affine subset, Ωt:T|s, which is given by\n(Rao, 1973, Chapter 8)\nΩt:T|s =\nn\nx: x = µt:T\ns\n+ v,\nv ∈range Σt:T\ns\no\n.\n(54)\nThe integral of ht:T|s over Ωt:T|s is then given by\nZ\nΩt:T |s\nht:T|s(x) dx = c1:T|s\nZ\nΩt|s\nexp\n\u0014\n−1\n2(x −µt:T\ns )∗(Σt:T\ns )+(x −µt:T\ns )\n\u0015\ndx\n= c1:T|s |2πΣt:T\ns |\n1/2\n+ ,\n(55)\nwhere|· |+ denotes the product of all non-zero eigenvalues, the so-called pseudo-determinant.\nThe notation dx is here overloaded to also mean the Lebesgue measure on Ωt:T|s. Thus\nthe inference problem can be solved even when π0 is flat (improper) on Ωt|s. In particular,\nthe following result is obtained by the preceeding discussion.\nProposition 5. Let the a priori initial distribution be given by\nπ0(x) = χΩ1:T |0(x),\n(56)\nwhere χA is the indicator function on the set A. Then the marginal likelihood, L1:T , and\nthe a posteriori initial density, π1:T\n0\n, are given by\nL1:T = c1:T|0 |2πΣt:T\n0 |\n1/2\n+\n(57a)\nπ1:T\n0\n(x) =\n\n\n\nN(x; µt:T\n0 , Σt:T\n0 ),\nx ∈Ω1:T|0\n0,\nx ∈Ωc\n1:T|0\n.\n(57b)\n4Since the rank of ¯Ct:T |s may be smaller than n, the maximum likelihood estimate is in general not\nunique.\n10\n\n\nNote that π1:T\n0\ncan still be obtained by similar means as in proposition 5 when π0 is\nflat on all of Rn. The difference being that π1:T\n0\nwill remain flat on Ωc\n1:T|0 rather than\nbeing unsupported. However, the marginal likelihood, L1:T , will be infinite. Therefore, in\nsuch a siutation it is perhaps more appropriate to treat x0 as a parameter in which case\nthe marginal likelihood may identified with h1:T|0.\n6\nAn example of unknown origin\nConsider an object moving in the plane and denote its position at time τ by p(τ). Let the\ndynamics of the object be governed by the following continuous-time model\nd¨p(τ) =\n \nσ1\n0\n0\nσ2\n!\ndw(τ),\nτ ≥0,\n(58)\nwhere ¨p is the acceleration and w is a two dimensional standard Wiener process. Starting\nat time τ = 127, the position of the object is measured at every time unit up until τ = 256\naccording to\ny(τk) = p(τk) +\n √λ1\n0\n0\n√λ2\n!\nv(τk),\n(59)\nwhere v(τk) is a sequence of two dimensional independent standard Gaussian vectors,\nτk = k. Collecting the acceleration, ¨p(τ), the velocity, ˙p(τ), and the position, p(τ), into a\nstate vector x(τ) gives a continuous-discrete stochastic state-space model representation\nof (58) and (59). Furthermore, by discretizing the continuous-time process and setting\nxk = x(τk) and taking x0 as completely unknown gives the following discrete-time model:\nx0 ∼χR6(· )\n(60a)\nxk | xk−1 ∼N(Φxk−1, Q),\nk = 1, 2, . . . , 256,\n(60b)\nyk | xk ∼N(Cxk, R),\nk = 127, 128, . . . , 256.\n(60c)\nThe problem is to determine the position, p(τk), of the object for k = 0, 1, . . . , 126, that is\nat times before the measurement process started.\nTwo options based on the backward-forward method are considered:\n1. The complete forward-backward algorithm using proposition 5 to compute π127:256\n0\n2. Computing the maximum likelihood estiamte of xk by maximizing h127:256|k.\nThe results are shown in figures 1 and 2, respectively. As expected, the estimation\nquality of running the complete forward-backward algorithm is better than just doing\nmaximum likelihood estimation.\n7\nDiscussion\nThis article presents a new backward-forward method was developed for inference in\npartially observed Gauss–Markov models. The benefit is that all computations are carried\nout with respect to the covariance parametrization or its’ square-root variant. Furthermore,\nsimple expressions for the forward a posteriori transition distributions, πt:T\nt|t−1, were obtained.\nThis in turn gives a forward Markov representation of the path posterior, π1:T\n0:T , which\nmay be simpler to work with in certain applications such as parameter estimation using\nvariational methods.\n11\n\n\nt\n0\n100\n200\np1(t)\n−6000\n−4000\n−2000\n0\nt\n0\n100\n200\np2(t)\n0\n2500\n5000\nFigure 1:\nThe position of the object (black), the position observations (red), and a ±2σ\ncredible interval of the position (blue). The estimate was obtained by the forward-backward\nalgorithm.\nt\n0\n100\n200\np1(t)\n−6000\n−4000\n−2000\n0\nt\n0\n100\n200\np2(t)\n0\n2500\n5000\nFigure 2:\nThe position of the object (black), the position observations (red), and a\n±2σ confidence interval of the position (blue). The estimate was obtained by maximum\nlikelihood.\nAcknowledgements\nFT was partially supported by the Wallenberg AI, Autonomous Systems and Software\nProgram (WASP) funded by the Knut and Alice Wallenberg Foundation.\nReferences\nAit-El-Fquih, B. and Desbouvries, F. (2008).\nOn Bayesian fixed-interval smoothing\nalgorithms. IEEE Transactions on Automatic Control, 53(10):2437–2442.\nAnderson, B. D. O. and Moore, J. B. (2012). Optimal filtering. Courier Corporation.\nAskar, M. and Derin, H. (1981). A recursive algorithm for the Bayes solution of the\nsmoothing problem. IEEE Transactions on Automatic Control, 26(2):558–561.\nBalenzuela, M. P., Dahlin, J., Bartlett, N., Wills, A. G., Renton, C., and Ninness, B.\n(2018). Accurate Gaussian mixture model smoothing using a two-filter approach. In\n2018 IEEE Conference on Decision and Control (CDC), pages 694–699. IEEE.\nBalenzuela, M. P., Wills, A. G., Renton, C., and Ninness, B. (2022). A new smoothing\nalgorithm for jump Markov linear systems. Automatica, 140:110218.\n12\n\n\nBresler, Y. (1986). Two-filter formulae for discrete-time non-linear Bayesian smoothing.\nInternational Journal of Control, 43(2):629–641.\nCappé, O., Moulines, E., and Rydén, T. (2005). Inference in Hidden Markov Models.\nSpringer.\nFraser, D. and Potter, J. (1969). The optimum linear smoother as a combination of two\noptimum linear filters. IEEE Transactions on Automatic Control, 14(4):387–390.\nGreville, T. N. E. (1966). Note on the generalized inverse of a matrix product. SIAM\nReview, 8(4):518–521.\nHelmick, R. E., Blair, W. D., and Hoffman, S. A. (1995). Fixed-interval smoothing for\nMarkovian switching systems. IEEE Transactions on Information Theory, 41(6):1845–\n1855.\nKailath, T., Sayed, A. H., and Hassibi, B. (2000). Linear estimation. Prentice Hall.\nKalman, R. E. (1961). New results in linear filtering and prediction theory. Transactions\nof the ASME, Journal of Basic Engineering, 82:35–45.\nKitagawa, G. (2023). Revisiting the two-filter formula for smoothing for state-space models.\narXiv preprint arXiv:2307.03428.\nLindsten, F., Bunch, P., Godsill, S. J., and Schön, T. B. (2013). Rao-Blackwellized particle\nsmoothers for mixed linear/nonlinear state-space models. In 2013 IEEE International\nConference on Acoustics, Speech and Signal Processing, pages 6288–6292. IEEE.\nLindsten, F., Bunch, P., Särkkä, S., Schön, T. B., and Godsill, S. J. (2015).\nRao-\nBlackwellized particle smoothers for conditionally linear Gaussian models. IEEE Journal\nof Selected Topics in Signal Processing, 10(2):353–365.\nLiu, H., Nassar, S., and El-Sheimy, N. (2010). Two-filter smoothing for accurate INS/GPS\nland-vehicle navigation in urban centers. IEEE Transactions on Vehicular Technology,\n59(9):4256–4267.\nMayne, D. Q. (1966). A solution of the smoothing problem for linear dynamic systems.\nAutomatica, 4(2):73–92.\nRao, C. R. (1973). Linear Statistical Inference and Its Applications, volume 2. Wiley New\nYork.\nRauch, H. E., Tung, F., and Striebel, C. T. (1965). Maximum likelihood estimates of\nlinear dynamic systems. AIAA journal, 3(8):1445–1450.\nSärkkä, S., Bunch, P., and Godsill, S. J. (2012). A backward-simulation based Rao-\nBlackwellized particle smoother for conditionally linear Gaussian models. IFAC Pro-\nceedings Volumes, 45(16):506–511.\nSärkkä, S. and García-Fernández, Á. F. (2020). Temporal parallelization of Bayesian\nsmoothers. IEEE Transactions on Automatic Control, 66(1):299–306.\nSärkkä, S. and Svensson, L. (2023). Bayesian Filtering and Smoothing. Cambridge\nUniversity Press.\n13\n\n\nStillfjord, T. and Tronarp, F. (2023). Computing the matrix exponential and the Cholesky\nfactor of a related finite horizon Gramian. arXiv preprint arXiv:2310.13462.\nWatanabe, K. (1985). On the relationship between the Lagrange multiplier method and\nthe two-filter smoother. International Journal of Control, 42(2):391–410.\n14\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21116v1.pdf",
    "total_pages": 14,
    "title": "The two filter formula reconsidered: Smoothing in partially observed Gauss--Markov models without information parametrization",
    "authors": [
      "Filip Tronarp"
    ],
    "abstract": "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}