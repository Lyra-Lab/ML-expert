{
  "id": "arxiv_2502.21196v1",
  "text": "Preprint, Under Review\nAMPLE: EVENT-DRIVEN ACCELERATOR\nFOR MIXED-\nPRECISION INFERENCE OF GRAPH NEURAL NETWORKS\nPedro Gimenes ∗, Yiren Zhao, George Constantinides\nDepartment of Electrical & Electronic Engineering, Imperial College London\n{pedro.gimenes19, a.zhao, g.constantinides}@ic.ac.uk\nABSTRACT\nGraph Neural Networks (GNNs) have recently gained attention due to their performance\non non-Euclidean data. The use of custom hardware architectures proves particularly bene-\nficial for GNNs due to their irregular memory access patterns, resulting from the sparse\nstructure of graphs.\nHowever, existing FPGA accelerators are limited by their double\nbuffering mechanism, which doesn’t account for the irregular node distribution in typi-\ncal graph datasets. To address this, we introduce AMPLE (Accelerated Message Passing\nLogic Engine), an FPGA accelerator leveraging a new event-driven programming flow. We\ndevelop a mixed-arithmetic architecture, enabling GNN inference to be quantized at a node-\nlevel granularity. Finally, prefetcher for data and instructions is implemented to optimize\noff-chip memory access and maximize node parallelism. Evaluation on citation and social\nmedia graph datasets ranging from 2K to 700K nodes showed a mean speedup of 243× and\n7.2× against CPU and GPU counterparts, respectively.\n1\nINTRODUCTION\nGraphs serve as powerful representations for capturing relationships between entities, which are represented\nas nodes, connected together by edges. This structure enables modeling a wide range of complex systems,\nincluding social networks (Alamsyah et al., 2021), biological interactions (Wu et al., 2021), and recom-\nmendation systems (Wang et al., 2021). Graph Neural Networks (GNNs) have emerged as a transformative\napproach for processing graph data, designed to learn from complex relational information by exploiting the\ninterconnections within the graph (Kipf & Welling, 2016; Veliˇckovi´c et al., 2017).\nInference on GNN models can be divided into two main computational phases, (1) Aggregation and (2)\nTransformation (Gilmer et al., 2017). In the Aggregation phase, a permutation-invariant function such as\nsummation or mean is applied over the feature embeddings of a node’s neighbors. The results of this phase\nare then utilized in the Transformation phase, which consists of a fully-connected layer used to generate\nthe updated feature embedding for each node. While the Transformation phase presents a highly regular\ncomputational pattern, which can be effectively accelerated on a parallelized device such as a GPU, the\nAggregation phase involves many irregular memory accesses due to the random and sparse nature of typical\ngraph data. Additionally, aggregation latency is a function of a node’s degree, which follows a highly non-\nuniform distribution. As such, an efficiently-designed GNN accelerator needs to alleviate the computational\nirregularity of the Aggregation phase while leveraging the regularity of the Transformation phase (Yan et al.,\n2020).\nAlthough CPU memory systems are a mature and highly optimized technology, the sparse structure of graph\ndata renders traditional cache systems less effective, since node aggregation incurs a high number of accesses\nto non-contiguous memory ranges. Inference on GPUs offers higher performance due to the deep level of\nparallelism, however, these devices are limited by high-latency memory management mechanisms. Addition-\nally, there is no support for inter-phase pipelining, meaning aggregation results must be stored into off-chip\nmemory before being re-fetched for the transformation phase. Finally, modern devices have limited support\nfor computation with low-precision numerical formats.\nThese considerations have motivated the design of several GNN accelerators. HyGCN leverages a set of\nSingle Instruction Multiple Data (SIMD) cores for aggregation, and a systolic array for node transformation\n(Yan et al., 2020). Meanwhile, GenGNN was proposed as a model-agnostic framework for GNN acceleration,\n∗Corresponding author.\n1\narXiv:2502.21196v1  [cs.AR]  28 Feb 2025\n\n\nPreprint, Under Review\nTable 1: Summary of graph processing features across hardware platforms. Although CPU parallelization is\npossible, multi-threaded CPUs have limited core count compared to parallelized accelerators. Event-driven\nprogramming is possible in GPUs, however computation does not follow a node-centric flow. Although pre-\nfetching is possible in GenGNN, all incoming messages for nodes in flight are required to be stored on-chip.\nFinally, no existing accelerators support arbitrary multi-precision computation.\nHardware Platform\nParallelization\nEvent-Driven Programming\nNode Pre-Fetching\nMulti-Precision\nCPU (Intel Xeon)\n✗\n✓\n✗\n✗\nGPU (RTX A6000)\n✓\n✓\n✗\n✗\nHyGCN (Yan et al., 2020)\n✓\n✗\n✗\n✗\nGenGNN (Abi-Karam et al., 2022)\n✓\n✗\n✓\n✗\nAMPLE\n✓\n✓\n✓\n✓\naddressing the gap between the development pace of GNN models and custom accelerators (Abi-Karam et al.,\n2022) through High-Level Synthesis tools. Table 1 summarizes the characteristics of available GNN hardware\nplatforms. Despite the benefits of previously proposed GNN accelerators, (i) the double-buffering mechanism\ndeployed in HyGCN is not well suited for graph computation due to the non-uniform distribution of node\ndegrees. Under this paradigm, low degree nodes must wait for higher degree nodes before computation can\nproceed, causing a high number of pipeline gaps. This highlights the need for an event-driven programming\nflow, where nodes are independently allocated resources and scheduled onto the accelerator. Additionally,\n(ii) neither accelerator offers hardware support for model quantization. As observed by Tailor et al. (Tailor\net al., 2020), the accuracy cost of quantization in GNNs is predominantly due to the aggregation phase and\ndirectly correlated to a node’s degree. As such, casting low-degree nodes to lower-precision formats while\npreserving high-degree nodes in high precision leads to reduced memory cost and resource usage at a low\ncost to model accuracy. Finally, (iii) existing accelerators require on-chip buffering of node embeddings for\nthe entire input graph. As such, these have limited applicability for inference on large graphs (> 100k nodes)\nwhere embeddings cannot feasibly be stored on-chip, highlighting the need for a node-centric pre-fetching\nsystem to hide memory access latency while the accelerator is busy.\nWe address these shortcomings by introducing a novel GNN accelerator, AMPLE, contributing the following:\n• We showcase an event-driven programming model for GNN acceleration, by enabling the host to\nprogram nodes asynchronously through memory-mapped registers.\n• We propose an architecture featuring a heterogeneous pool of multi-precision Aggregation Cores\nconnected through a Network-on-Chip, which are dynamically allocated to nodes according degree\nand precision.\n• We evaluate AMPLE on large-scale graph datasets ranging from 2K to 700K nodes, achieving an\naverage speedup of 243× and 7.2× compared to CPU and GPU baselines, respectively.\nThe body of this paper is structured as follows. Section 2 covers background on GNNs and neural network\nquantization. Section 3 explains the architecture of the AMPLE accelerator, including how each high-level\nfeature is achieved at the circuit level. Finally, Section 4 explains the testing methodology and experimental\nresults against CPU/GPU baselines.\n2\nBACKGROUND\n2.1\nGRAPH REPRESENTATION\nA graph G = (V, E) is a set of nodes/vertices V and edges E. The set of feature representations at layer l\nis denoted by matrix X(l) ∈RN×D, where N = |V| is the number of nodes and D is the feature size. An\nelement ei,j = (vi, vj) present in E indicates that there is a connection between nodes vi and vj, meaning\nnode vj is contained in the set of i’s neighbors, Ni, and vi is contained in Nj. In an undirected graph, the edge\nelement ei,j corresponds to ej,i. The connections in a graph can be represented using an N × N adjacency\nmatrix, where each element Ai,j represents an edge between nodes i and j.\n2\n\n\nPreprint, Under Review\n2.2\nGRAPH NEURAL NETWORKS (GNNS)\nWithin a GNN, graph data is transformed over several layers to perform classification and/or regression\ntasks on the full graph or individual nodes/edges. GNNs can be represented through the Message Passing\nMechanism (Gilmer et al., 2017), which generalizes the node update law as follows.\nxl+1\ni\n= γ(xl\ni, Aj∈N(i)(ϕ(xl\ni, xj, el\ni,j)))\n(1)\nIt can be seen that in the general case, each node aggregates incoming messages represented as an arbi-\ntrary function ϕ, which is equivalent to aggregating neighboring embeddings when ϕ = xl\nj. Messages are\naggregated through an arbitrary permutation-invariant aggregation function Aj∈N(i) over the neighborhood\nof node i, and and an arbitrary transformation function γ(xl\ni, ml\ni), where ml\ni is the result of aggregation\n(i.e. mi = Aj∈N(i)ϕ(xl\ni, xl\nj, el\ni,j)).\n2.2.1\nGRAPH CONVOLUTIONAL NETWORKS (GCN)\nGCNs emerged as a solution analogous to Convolutional Neural Networks in the computer vision domain\n(Kipf & Welling, 2016). The element-wise node update law for a single GCN layer is shown in Equation 2.\nxl+1\ni\n= W\n\n\nX\nj∈Ni∪{i}\nej,i\nq\nˆdj ˆdi\nxl\nj\n\n\n(2)\nThe normalization factors are given by ˆdi = 1 + P\nj∈N(i) ej,i. It can be seen that A is taken as the summation\nA = P\nj∈Ni ϕ(xj, ei,j), with γ(xi, mi) = Wmi.\n2.2.2\nGRAPH ISOMORPHISM NETWORKS (GIN)\nGIN was proposed as a model that can provably generate distinct feature updates for two graphs that can\nbe shown to be non-isomorphic through the Weisfeiler-Lehman test (Leman, 2018), thus maximizing its\nrepresentational capacity (Xu et al., 2018). The update law is given by the following, where ϵ is a small scalar\nfor numerical stability.\nxl+1\ni\n= MLP\n\n(1 + ϵ) · xl\ni +\nX\nj∈N(i)\nxl\nj\n\n\n(3)\nThe same aggregation A is used as in GCN. In contrast to GCN, GIN does not make use of normalization\nfactors in aggregation (i.e. ϕ = xj), and a residual connection is added after aggregation, which is equivalent\nto a self-connection in the graph’s adjacency matrix.\n2.2.3\nGRAPHSAGE\nGraphSAGE was proposed as an inductive framework to generate feature embeddings with high representa-\ntional capacity for unseen nodes and/or sub-graphs (Hamilton et al., 2017).\nxl+1\ni\n= W1xi + W2 ·\n\u0012\nmean\nj∈N(i)σ(W3xl\nj + b)\n\u0013\n(4)\nIt can be seen that the message passing function ϕ is taken as a fully-connected layer with activation σ over the\nneighbouring embeddings xj, A is taken as the mean, and the transformation γ(xi, mi) = W1xi + W2mi\nwhere W1, W2 are linear projection matrices. The projection parameterized by W1 can be seen as a scaled\nresidual connection.\n2.3\nNEURAL NETWORK QUANTIZATION\nQuantization has been widely explored as a method for reducing model complexity and computational latency\nin neural networks. Quantization-Aware Training (QAT) enables minimizing accuracy loss at low-precision\nrepresentations by quantizing activations in the forward pass, making use of the Straight-Through Estimator\n3\n\n\nPreprint, Under Review\nAggregation Engine\n(AGE)\nTransformation Engine\n(FTE)\nNode Instruction\nDecoder (NID)\n...\nAggregation Buffer\nNodeslot 0\nNodeslot 63\nWeight Bank\n...\nFeature Bank\nPrefetcher\n...\nInstruction\nPrefetcher\nAXI-L\nHost CPU\n32\nHigh-Bandwidth\nMemory (HBM)\nDRAM C0\nFigure 1: AMPLE Top Level Diagram. Packets propagate through dimension-order routing in the Aggrega-\ntion Engine’s Network-on-Chip (shown in green), and are driven diagonally into the Transformation Engine’s\nsystolic array (shown in red). Dashed lines represent control flow interfaces, while solid lines represent data\nflow between units. Node embeddings are fetched through HBM, while instructions are stored in DRAM.\n(STE) in the backwards pass to estimate the non-differentiable quantization gradients. In general, activations\nare quantized following Equation 5, where qmin, qmax form the chosen range of representable values, s is the\nscaling factor to place x into the required range, z is the zero-point (floating point equivalent of the value 0\nin the quantized space) and the brackets represent the rounding operation.\nxq = min(qmax, max(qmin,\njx\ns + z\nm\n))\n(5)\nThe min and max functions are in place to show that any values beyond the specified range assume the\nfixed-point value at the limit. Following this, activation can be de-quantized by ˆx = (xq −z)s, where ˆx is an\napproximation of the original floating-point value.\n2.3.1\nQUANTIZATION-AWARE TRAINING FOR GRAPH NEURAL NETWORKS\nDegree-Quant, proposed by Tailor et al., was one of the first approaches applying Quantization-Aware Train-\ning to Graph Neural Networks (Tailor et al., 2020). Firstly, Tailor et al. suggest that the aggregation phase\nof GNN inference is the predominant source of quantization error. This effect is observed more heavily in\nnodes with higher in-degrees, which can be intuitively understood since the absolute magnitude of aggrega-\ntion grows with the number of neighbors. The growth in expected aggregation for high-degree nodes affects\nthe qmax and qmin values, reducing the quantization resolution due to these outliers in the distribution of\naggregation results.\nThe authors of Degree-Quant address the issue of quantization error by stochastically applying a protection\nmask at each layer following the Bernoulli distribution (Tailor et al., 2020). Protected nodes operate in\nfloating-point, while non-protected nodes operate in fixed-point. A node’s probability of protection is a\nfunction of its degree, interpolated within a parametrizable range [pmin, pmax], where the graph nodes with\nminimum/maximum neighbor counts are assigned the limit probabilities.\n3\nARCHITECTURE\nAs shown in Figure 1, AMPLE is composed of the following functional units, with their respective functions.\n• Node Instruction Decoder (NID): communication with the host device and driving other functional\nunits to schedule work onto the accelerator.\n• Prefetcher: fetching and storing layer weights and neighbouring feature embeddings into local\nmemories (the Weight Bank and Feature Bank, respectively).\n4\n\n\nPreprint, Under Review\nTable 2: Example state of the Node Scoreboard at runtime, with nodeslots are found in various states. The\nadjacency list and updated feature pointers indicate the memory address from which to fetch the list of\nneighbouring node IDs and write updated feature embeddings, respectively. The precision field dictates\nwhich arithmetic cores are allocated at runtime.\nSlot\nNode ID\nPrecision\nState\nNeighbors\nAdjacency List Pointer\nUpdated Feature Pointer\n0\n267\nfloat\nTransformation\n32\n0x3BC90188\n0x4FE8B774\n1\n268\nfloat\nAggregation\n8\n0xCAF5C03F\n0xE672109F\n...\n.. .\n. ..\n. . .\n.. .\n.. .\n.. .\n63\n330\nint4\nPrefetch\n1\n0x78E26A27\n0xA4D89ED9\n• Aggregation Engine (AGE): performing permutation-invariant aggregation functions over all\nneighbours of a node through a Network-on-Chip architecture.\n• Aggregation Buffer (ABF): storage element containing aggregated feature embeddings generated\nby the AGE.\n• Feature Transformation Engine (FTE): computing the updated feature embeddings for each node\nby performing a matrix multiplication between weights in the Weight Bank and aggregation results\nin the Aggregation Buffer.\n3.1\nEVENT-DRIVEN PROGRAMMING THROUGH THE NODE INSTRUCTION DECODER\nCommunication between AMPLE and the host is handled by the Node Instruction Decoder (NID), which is a\nmemory-mapped register bank comprised of a configurable number of nodeslots. As shown in Table 2, each\nnodeslot contains the information required to perform a node’s aggregation and transformation steps, and a\nstate machine is maintained indicating each node’s state. Algorithm 1 shows how work can be offloaded\nby the host, which runs concurrently with the accelerator. First, the NID is programmed with a number of\nglobal and layer-wise parameters, including node/feature counts and aggregation functions. Subsequently, the\nhost programs the nodeslots and updates values in the mask available nodeslots ∈{0, 1}n where n is the\nnumber of nodeslots. While a node is programmed, the accelerator performs aggregation and transformation\nover previously-programmed nodes. The available nodeslots mask is then deasserted independently by\nthe accelerator when the computation is finished. Thus, the accelerator supports a node-wise, event-driven\ncomputation paradigm. Note that ′1 and ′0 indicate a mask full of ones and zeros, respectively.\nAlgorithm 1 Host programming pseudocode\nRequire: global parameters P, layers L, nodes V\nnid register bank.global parameters ←P\navailable nodeslots ←’1\nfor layer in L do\nnid register bank.layer config ←layer\nlayer.prefetch layer weights()\nwhile V ̸= ∅do\nif available nodeslots != ’0 then\nchosen nodeslot ←choose(available nodeslots)\nchosen nodeslot.programming ←V.pop head()\navailable nodeslots [chosen nodeslot] ←0\nend if\nend while\nend for\nAfter a nodeslot is programmed, the NID then drives the Prefetcher, AGE and FTE to perform the compu-\ntation, and updates the node’s internal state machine after each functional step. No further intervention is\nrequired from the host, and an interrupt is sent after step 7 to indicate the nodeslot can be reused. It should\nbe noted that the order in which nodes are programmed within the nodeslots does not imply any priority or\ntime correlation. Typical graph datasets often display high variance in execution time per node, depending\non neighbour count and numerical precision. Whenever a nodeslot finishes its computation, it can be im-\nmediately reprogrammed by the host with the next node. This event-driven control flow requires the host to\n5\n\n\nPreprint, Under Review\nAGC0\nAGCm*n\n...\nBMm\nFloat Mesh\nmask\nAGM0\nmask\nAGMn\n...\nFeature Bank\nFloat AGMs\nNode Scoreboard (NSB)\n...\nBM0\nAGC0\nAGCm*n\n...\nBMm\nInt8 Mesh\n...\nBM0\nAGC0\nAGCm*n\n...\nBMm\nInt4 Mesh\n...\nBM0\nmask\nAGM0\nmask\nAGMn\n...\nInt8 AGMs\nmask\nAGM0\nmask\nAGMn\n...\nInt4 AGMs\nFigure 2: Microarchitecture of AGE configured with three supported precisions. NID requests drive the\nAggregation Managers (AGMs), which receive fetched embeddings from the Feature Bank (See Figure 1).\nThese are then transferred to the Aggregation Cores (AGCs) through the network. Aggregation results are\nthen buffered by the Buffering Managers (BMs).\nrun concurrently with the accelerator to monitor its state and drive further work when resources are avail-\nable. Within the NID, nodes running concurrently are serviced with round-robin arbitration to grant access\nto shared resources within the Aggregation and Transformation Engines.\n3.2\nMIXED-PRECISION ARITHMETIC\nComputing units within the AGE and FTE are locally homogeneous, meaning each processing element sup-\nports a single numerical precision. Within the Aggregation Engine, these are arranged in a Network-on-Chip\n(NoC) architecture comprising a heterogeneous grid of processing elements, where the ratio of PEs allocated\nto each precision can be configured at compile time according to the application requirements. Each PE is\ncoupled to a router responsible for transferring packets over the network. Each packet is comprised of a\nhead flit carrying routing payloads, an arbitrary number of body flits carrying data, and a tail flit. Since there\nis no requirement for communication between PEs of different precisions, these are placed within isolated\nsub-networks as shown in Figure 2, which acts to reduce packet congestion.\nAs discussed in Section 1, static pipelining through the double buffering mechanism leads to pipeline gaps\nwhen computing over graphs with high variance in node degree, since low-degree nodes must wait for high-\ndegree nodes to release resources. This is alleviated in the AGE by dynamically allocating processing el-\nements within each aggregation sub-network according to a node’s feature count and precision. As such,\nnodeslots are allocated resources independently of any other ongoing workload, and these resources can be\nimmediately reused upon completion, thus forming an event-driven programming model.\n3.3\nLARGE GRAPH PROCESSING\nInference over large graphs is enabled by the Feature Bank in the Prefetcher, which contains a storage el-\nement named “Fetch Tag” for each nodeslot in the NID. A group containing a parametrizable number of\nFetch Tags is coupled to each HBM bank on the Alveo U280 card, meaning up to 32 Fetch Tags can access\nmemory resources concurrently, alleviating the inherent memory boundedness associated with sparse graph\ndata. Within each Fetch Tag group, access to the HBM bank is granted using round robin arbitration.\nThe Feature Bank supports the large graph use case via its partial response mechanism. For nodes with\ndegree higher than the Fetch Tag capacity, the Fetch Tag fills the Message Queue and directly unblocks the\nAGE to begin the aggregation process. Once aggregation begins, the Fetch Tag is re-enabled and continues\nto fetch the remaining neighbours, hiding the memory access latency. This mechanism leads to lower storage\nrequirement per nodeslot, allowing a higher number of Fetch Tags in the Feature Bank, i.e. deeper node\nparallelism.\n4\nEXPERIMENTAL RESULTS\nThree foundational GNN models were deployed for evaluating the accelerator, with varied architectures, as\nshown in Table 3. See Section 2 for each model’s update laws. Furthermore, 6 graph datasets were chosen,\nthe first three being small citation networks, and the last three being larger social media graphs. Table 4\n6\n\n\nPreprint, Under Review\nRound-Robin Arbiter\nREQ\nFetch Tag 0\nAXI Read Master\nAggregation\nManager 0\nFeature Bank\nREQ\nMessages\nAddresses\n...\n...\nREQ\nFetch Tag 63\nREQ\nMessages\nAddresses\nAggregation\nManager 63\nFigure 3: Fetch Tags in the Feature Bank make concurrent memory access requests in a two-stage process;\nFirst, the list of neighbouring node IDs is stored in the Address Queue, and these are then used as pointers\nfor the neighbouring feature embeddings, which are stored in the Message Queue.\nTable 3: GNN models used for benchmarking the accelerator. A residual connection denotes the addition of\nthe node’s original embedding after the aggregation or transformation steps.\nModel\nAggregation\nResidual\nNormalization\nGCN\nsum\n✗\naggregation\nGIN\nsum\naggregation\n✗\nGraphSAGE\nmean\ntransformation\ntransformation\nshows the node count and mean node degree for each evaluated dataset - the latter acts as an indicator of\ngraph sparsity, with an inverse relationship between sparsity and mean degree.\n4.1\nMIXED-PRECISION ARITHMETIC\nThe DegreeQuant algorithm was used to assign the precision for each node in the graph datasets, by\nstochastically protecting nodes according to their degree (see Section 2). As shown in Table 4, the ratio\nof protected nodes is below 3% for all datasets, suggesting a similar ratio of resources on the accelerator\nshould be allocated to float. Configuration parameters were then chosen as follows; given two node groups,\nfor float and int8 nodes, a resource budget Rmax,r\np\n(where p ∈[float, int8] is the numerical format and\nr ∈[LUT, FF, BRAM, DSP] is the resource type) is allocated to each group using the ratio obtained from\nDegreeQuant. A single-arithmetic variant was synthesized for float and int8, and the resource utilization per\nnodeslot Cr\np was estimated for each precision and resource type. Finally, the number of nodeslots Np for\neach precision is determined as shown in equation 6, where the brackets denote rounding up to the nearest\ninteger.\nNp =\n\u0018\nmin\nr\nRmax,r\np\nCrt\np\n\u0019\n(6)\nIt was expected that at lower ratios of protected nodes, resources can be distributed across a higher number\nof nodeslots, due to the lower resource usage of fixed-point cores. In fact, it was found that allocating a\nsingle nodeslot to floating-point is normally enough to meet the precision requirement for task accuracy\nwhile maximising hardware node parallelism.\n4.2\nPERFORMANCE ANALYSIS\nEach model was first benchmarked on the Intel Xeon CPU and RTX A6000 GPU across all datasets, with\nrandomly initialized node features and layer weights. In each case, the mean latency was obtained over 100\ntrials to account for runtime jitter due to non-deterministic processes. The GPU cache was emptied prior\n7\n\n\nPreprint, Under Review\nTable 4: Datasets used for benchmarking. DQ ratio shows the ratio of nodes mapped to float precision by the\nDegreeQuant algorithm, with the rest running in int8.\nName\nNodes\nMean Degree\nFeatures\nDQ Ratio\nCR\nCora\n2,708\n3.9\n1,433\n2.1 %\nCS\nCiteSeer\n3,327\n2.7\n3,703\n2.7 %\nPB\nPubMed\n19,717\n4.5\n500\n2.9 %\nFL\nFlickr\n89,250\n10.0\n500\n0.2 %\nRD\nReddit\n232,965\n99.6\n602\n2.7 %\nYL\nYelp\n716,847\n19.5\n300\n0.4 %\nCR\nCS\nPB\nFL\nRD\nYL\n100\n101\n102\n103\nSpeed-up\n33×\n24×\n75×\n32×\n5×\n6×\n994×\n831×\n224×\n65×\n38×\n13×\nGPU\nAMPLE\nCR\nCS\nPB\nFL\nRD\nYL\n54×\n55×\n50×\n44×\n9×\n13×\n829×\n695×\n127×\n34×\n20×\n9×\nCR\nCS\nPB\nFL\nRD\nYL\nGCN (Mean: 12.9×)\nGIN (Mean: 5.7×)\n31×\n24×\n32×\n45×\n10×\n16×\n235×\n195×\n38×\n10×\n8×\n2×\nGraphSAGE (Mean: 3.0×)\nFigure 4: Inference speedup compared to Intel Xeon CPU baseline obtained on the RTX A6000 GPU and\nAMPLE simulation. The GPU shows an average speedup of 29.8×, 37.8× and 26.7× across all datasets\nfor GCN, GIN and GraphSAGE, respectively. Equivalent speedups on AMPLE were 361.3×, 285.8× and\n81.7×.\nto each prediction step such that latency readings include off-chip memory access for features and weights.\nGPU warm-up time was not included, meaning inference times are taken after driver initialization is complete.\nFinally, inference latency on AGILE was obtained from Modelsim 19.2 simulation results at a frequency of\n200MHz, obtained for the Alveo U280 card using the Vivado 23.1 toolflow. As shown in Figure 4, AMPLE\nled to an improvement in mean inference time compared to the CPU/GPU baselines across all models. Table\n5 shows the obtained values for latency and node throughput for GCN.\n5\nCONCLUSION\nThis work presented AMPLE, an FPGA accelerator for GNN inference over large graphs. An event-driven\nprogramming flow was introduced, coupled with a dynamic resource allocation mechanism through on-chip\nnetwork communication, overcoming the performance bottleneck associated with node batching in graphs\nwith non-uniform distribution of node degrees. Using a node-centric data prefetcher, we alleviate the require-\nment for on-chip storage of weights and activations, enabling GNN acceleration over social media graph\ndatasets. These factors led to an average speedup of 243× and 7.2× compared to CPU and GPU baselines.\nFinally, we provide the first platform to accelerate graphs quantized at node granularity, demonstrating an\noptimal resource mapping to maximise node parallelism at a low cost to model accuracy.\nTable 5: Inference time for evaluated datasets using a single-layer GCN model. Mean latency is reported over\n100 iterations.\nCPU (Intel Xeon)\nGPU (RTX A6000)\nAMPLE @200MHz\nMean\nThroughput\nMean\nThroughput\nMean\nThroughput\nLatency\nLatency\nLatency [ms]\n[nodes/ms]\nLatency [ms]\n[nodes/ms]\nLatency [ms]\n[nodes/ms]\nGain (CPU)\nGain (GPU)\nCora\n244.4\n11.1\n7.2\n376.3\n0.246\n11,022.0\n994.8×\n29.3×\nCiteSeer\n244.3\n13.6\n10.1\n330.0\n0.294\n11,318.6\n831.2×\n34.3×\nPubMed\n362.4\n54.4\n4.8\n4,099.5\n1.617\n12,193.2\n224.1×\n3.0×\nFlickr\n475.4\n187.8\n14.5\n6,146.2\n7.227\n12,350.0\n65.8×\n2.0×\nReddit\n953.3\n244.4\n171.0\n1,362.0\n24.6\n9,463.6\n38.7×\n6.9×\nYelp\n760.8\n942.2\n110.9\n6461.6\n57.5\n12,471.7\n13.2×\n1.9×\nAverage\n506.8\n242.2\n53.1\n3,129.3\n15.2\n11,469.9\n361.1×\n12.9×\n8\n\n\nPreprint, Under Review\nREFERENCES\nStefan Abi-Karam, Yuqi He, Rishov Sarkar, Lakshmi Sathidevi, Zihang Qiao, and Cong Hao. GenGNN: A\nGeneric FPGA Framework for Graph Neural Network Acceleration. 1 2022. doi: 10.48550/arxiv.2201.\n08475. URL https://arxiv.org/abs/2201.08475v1.\nAndry Alamsyah, Budi Rahardjo, and Kuspriyanto. Social Network Analysis Taxonomy Based on Graph\nRepresentation. 2 2021. URL https://arxiv.org/abs/2102.08888v1.\nJustin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message\npassing for quantum chemistry, 2017.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\nIn Neural Information Processing Systems, 2017. URL https://api.semanticscholar.org/\nCorpusID:4755450.\nThomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks.\n5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 9\n2016. doi: 10.48550/arxiv.1609.02907. URL https://arxiv.org/abs/1609.02907v4.\nAdrien Leman. The reduction of a graph to canonical form and the algebra which appears therein. 2018.\nURL https://api.semanticscholar.org/CorpusID:49579538.\nShyam A. Tailor, Javier Fernandez-Marques, and Nicholas D. Lane. Degree-Quant: Quantization-Aware\nTraining for Graph Neural Networks. 8 2020. doi: 10.48550/arxiv.2008.05000. URL https://arxiv.\norg/abs/2008.05000v3.\nPetar Veliˇckovi´c, Arantxa Casanova, Pietro Li`o, Guillem Cucurull, Adriana Romero, and Yoshua Bengio.\nGraph Attention Networks. 6th International Conference on Learning Representations, ICLR 2018 - Con-\nference Track Proceedings, 10 2017. doi: 10.48550/arxiv.1710.10903. URL https://arxiv.org/\nabs/1710.10903v3.\nShoujin Wang, Liang Hu, Yan Wang, Xiangnan He, Quan Z. Sheng, Mehmet A. Orgun, Longbing Cao,\nFrancesco Ricci, and Philip S. Yu. Graph Learning based Recommender Systems: A Review. IJCAI\nInternational Joint Conference on Artificial Intelligence, pp. 4644–4652, 5 2021. ISSN 10450823. doi:\n10.24963/ijcai.2021/630. URL https://arxiv.org/abs/2105.06339v1.\nHsiang-Yun Wu, Martin N¨ollenburg, and Ivan Viola. Graph Models for Biological Pathway Visualization:\nState of the Art and Future Challenges. 10 2021. URL https://arxiv.org/abs/2110.04808v1.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph neural net-\nworks? ArXiv, abs/1810.00826, 2018. URL https://api.semanticscholar.org/CorpusID:\n52895589.\nMingyu Yan, Lei Deng, Xing Hu, Ling Liang, Yujing Feng, Xiaochun Ye, Zhimin Zhang, Dongrui Fan, and\nYuan Xie. HyGCN: A GCN Accelerator with Hybrid Architecture. Proceedings - 2020 IEEE International\nSymposium on High Performance Computer Architecture, HPCA 2020, pp. 15–29, 1 2020. doi: 10.48550/\narxiv.2001.02514. URL https://arxiv.org/abs/2001.02514v1.\n9\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21196v1.pdf",
    "total_pages": 9,
    "title": "AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks",
    "authors": [
      "Pedro Gimenes",
      "Yiren Zhao",
      "George Constantinides"
    ],
    "abstract": "Graph Neural Networks (GNNs) have recently gained attention due to their\nperformance on non-Euclidean data. The use of custom hardware architectures\nproves particularly beneficial for GNNs due to their irregular memory access\npatterns, resulting from the sparse structure of graphs. However, existing FPGA\naccelerators are limited by their double buffering mechanism, which doesn't\naccount for the irregular node distribution in typical graph datasets. To\naddress this, we introduce \\textbf{AMPLE} (Accelerated Message Passing Logic\nEngine), an FPGA accelerator leveraging a new event-driven programming flow. We\ndevelop a mixed-arithmetic architecture, enabling GNN inference to be quantized\nat a node-level granularity. Finally, prefetcher for data and instructions is\nimplemented to optimize off-chip memory access and maximize node parallelism.\nEvaluation on citation and social media graph datasets ranging from $2$K to\n$700$K nodes showed a mean speedup of $243\\times$ and $7.2\\times$ against CPU\nand GPU counterparts, respectively.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}