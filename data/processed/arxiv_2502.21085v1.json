{
  "id": "arxiv_2502.21085v1",
  "text": "BST: Badminton Stroke-type Transformer for Skeleton-based Action\nRecognition in Racket Sports\nJing-Yuan Chang\nNational Tsing Hua University\nva6lue@gapp.nthu.edu.tw\nAbstract\nBadminton, known for having the fastest ball speeds among\nall sports, presents significant challenges to the field of com-\nputer vision, including player identification, court line de-\ntection, shuttlecock trajectory tracking, and player stroke-\ntype classification. In this paper, we introduce a novel video\nsegmentation strategy to extract frames of each player’s\nracket swing in a badminton broadcast match. These seg-\nmented frames are then processed by two existing models:\none for Human Pose Estimation to obtain player skeletal\njoints, and the other for shuttlecock trajectory detection to\nextract shuttlecock trajectories.\nLeveraging these joints,\ntrajectories, and player positions as inputs, we propose\nBadminton Stroke-type Transformer (BST) to classify player\nstroke-types in singles. To the best of our knowledge, exper-\nimental results demonstrate that our method outperforms\nthe previous state-of-the-art on the largest publicly avail-\nable badminton video dataset, ShuttleSet, which shows that\neffectively leveraging ball trajectory is likely to be a trend\nfor racket sports action recognition.\n1. Introduction\nIn recent years, the rapid development of deep learning has\ncatalyzed significant progress in sports analysis [11, 13, 14,\n17, 23, 25, 26], aiming to provide athletes with objective\nstatistical data to refine their techniques and devise effective\nstrategies for continuous improvement. Badminton, one of\nthe world’s most popular sports and known for having the\nfastest ball speeds among all sports, presents challenges for\ncomputer vision, including player identification, court line\ndetection, shuttlecock trajectory tracking, and player stroke-\ntype classification.\nFor the task of player stroke-type classification, models\nfrom the field of Human Action Recognition (HAR) can be\nleveraged. The domain of HAR, which focuses on iden-\ntifying the actions performed by individuals in videos, has\nevolved from directly analyzing RGB bounding boxes in the\nframes [28] to first extracting skeletal joints from these in-\ndividuals and then analyzing them. This extraction process\nis called Human Pose Estimation (HPE) [3, 4, 18, 29, 38],\nand the remaining process is known as Skeleton-based Ac-\ntion Recognition (SAR) [9, 10, 12, 20–22, 24, 33, 36, 37].\nWith this approach, we can effectively filter out extraneous\nbackground details and enable a concentrated focus on the\nnuances of human motion. Nevertheless, even with these\nadvanced SAR models [9, 22, 33, 37] (see Sec. 2.2 for more\ndetails), achieving high accuracy in a broadcast badminton\ndataset faces several challenges. These models are primar-\nily designed to recognize human actions in everyday sce-\nnarios, where movements often exhibit significant variation,\nfor instance, the contrast between the static action of drink-\ning water and the dynamic motion of running. However,\nplayers’ racket-swinging actions can be categorized under\na general ”hitting” action in broader classifications, regard-\nless of the specific stroke-type. Furthermore, these actions\nare both rapid and brief, making it even more difficult to dis-\ntinguish between different strokes. Additionally, as noted\nin [21], although some models support multi-person in-\nput, they do not fully take into account the relationship be-\ntween interacting players. These limitations underscore the\nchallenge of distinguishing subtle differences in movements\nwhile utilizing limited information and determining which\nof the two players executed a particular stroke.\nFortunately, in badminton singles, the shuttlecock trajec-\ntory can serve as an interactive medium between the two\nplayers. To the best of our knowledge, TemPose [16] is\nthe first model to incorporate shuttlecock trajectory infor-\nmation as an auxiliary input. However, we argue that the\nshuttlecock trajectory plays an even more crucial role in\nrecognizing badminton stroke actions. Consider a hypo-\nthetical scenario where the shuttlecock is completely re-\nmoved from a badminton match video, leaving only the two\nplayers performing air swings. From a human perspective,\nwould it still be possible to accurately determine the type of\nstroke performed based solely on the players’ movements?\nThis highlights the indispensable role of shuttlecock trajec-\ntory information in a robust badminton stroke recognition\n1\narXiv:2502.21085v1  [cs.CV]  28 Feb 2025\n\n\nmodel. Therefore, we treat the shuttlecock trajectory as a\nprimary input in our model. The main contributions of this\npaper are summarized as follows:\n• We introduce a novel badminton video segmentation\nstrategy to extract frames that have high relevance to\nthe target badminton stroke swung by the player.\n• We propose Badminton Stroke-type Transformer\n(BST) models that outperform the previous state-of-\nthe-art on the largest publicly available badminton\nvideo dataset, ShuttleSet [32].\n• We show that effectively leveraging shuttlecock trajec-\ntory information can significantly improve the perfor-\nmance of badminton stroke-type classification.\n2. Related Work\n2.1. Badminton Analysis System\nAnurag Ghosh et al. proposed an end-to-end frame-\nwork [13] for automatic attribute tagging and analysis in\nbadminton videos. Building on this, Wei-Ting Chen et al.\nintroduced a more advanced end-to-end analysis system [5],\nwhich incorporates a visually appealing user interface for\nenhanced usability. In the domain of stroke classification,\nWei-Ta Chu and Samuel Situmeang developed a method [7]\nspecifically targeting the bottom player in singles matches.\nSimilarly, Yongkang Zhao proposed another stroke clas-\nsification approach [35] that also focuses on the bottom\nplayer, utilizing deep learning techniques to improve accu-\nracy. Magnus Ibh et al. introduced TemPose [16] that can\nclassify top and bottom player strokes in singles matches.\nFor serve detection, See Shin Yue et al. presented a special-\nized model [34] tailored for badminton. Further advancing\nthis area, the TrackNet series [1, 6, 15, 30] primarily fo-\ncuses on shuttlecock tracking and provides 2D trajectory\npredictions for downstream analysis systems. Paul Liu and\nJui-Hsien Wang proposed MonoTrack [23], an end-to-end\nsystem that not only tracks the shuttlecock but also extends\nits functionality by simulating 3D trajectories from 2D tra-\njectory data.\n2.2. Skeleton-based Action Recognition (SAR)\nIn this field, the development of Graph Convolutional Net-\nwork (GCN) [19]-based models has been as rapid and com-\npetitive as that of Transformer [31]-based models.\nSijie Yan et al. proposed ST-GCN [33], the first model\nto integrate GCN and Temporal Convolutional Network\n(TCN) [2] for SAR. At the time, it was considered a ground-\nbreaking development.\nYuxuan Zhou et al. introduced BlockGCN [37], which\nemploys BlockGC (a custom-designed GCN) along with\na multi-scale TCN. Before applying these components,\nthey incorporated Dynamic Topological Encoding. Within\nBlockGC, they utilized a Static Topological Encoding based\non the hop distance between joints and a learnable adja-\ncency matrix to provide the model with greater flexibility in\nlearning. Remarkably, BlockGCN achieved state-of-the-art\nperformance without relying on any attention mechanisms\nand was published at CVPR 2024.\nSkateFormer [9], proposed by Jeonghyeok Do and\nMunchurl Kim, builds upon the Transformer architec-\nture with several modifications.\nThey integrated GCN,\nTCN, and their custom-designed Skate-MSA into the self-\nattention module. The core idea of Skate-MSA is to trans-\nform the input data into four different types, which were\nconstructed based on their observations of human motion\npatterns. Each type is then processed separately. Their work\nwas later published at ECCV 2024.\nHongda Liu et al. proposed ProtoGCN [22], which is\nalso based on a custom GCN and a multi-scale TCN.\nAdditionally, they incorporated Class-Specific Contrastive\nLoss to enhance model training. Their approach achieved\nstrong performance on several well-known public datasets\nin SAR.\n3. Method\nIn this section, we present the key components of our\nmethod. In Sec. 3.1, we present a video segmentation strat-\negy which segments a match video into clips. Each clip con-\ntains two or three strokes. In Sec. 3.2, we introduce a data\npreprocessing work which produces player poses, positions\nand shuttlecock trajectories from stroke clips. In Sec. 3.3,\nwe present a Transformer-based architecture which takes\nposes as inputs and produces classification results.\n3.1. Video Segmentation Strategy\nIn this subsection, we explain how we segment a match\nvideo into stroke clips. There are many rallies in a match\nvideo. A rally is a sequence of strokes that start from a\nserve and ends when the shuttlecock falls on the court and a\npoint is awarded to a player. A stroke refers to a badminton\nswing that hits the shuttlecock. We first denote every rally i\nthat contains n(i) strokes j to be a set like\nRi = {S(i)\nj }n(i)\nj=1 = {S(i)\n1 , S(i)\n2 , . . . , S(i)\nn(i)},\nfor i = 1, 2, . . . , r, where r is the total number of rallies in\nthe match video, and j = 1, 2, . . . , n(i). We now segment\nevery rally into stroke clips. Suppose a match video contain\nN frames denoted by F1, F2, . . . , FN. Some of the frames\nin the match video are hit frames, which are the frames in\nwhich the shuttlecock is in contact with, or closest to, the\nracket during a stroke. Thus, every stroke S(i)\nj\nin rally Ri\nhas a one-to-one correspondence with the hit frame number\nh(i)\nj . A fixed-width clipping strategy similar to the approach\nused in [35] define the j-th stroke clip in rally Ri to be\nC(i)\nj\n= {Fh(i)\nj\n−t, Fh(i)\nj\n−t+1, . . . , Fh(i)\nj\n+t},\n(1)\n2\n\n\nfor j = 1, 2, . . . , n(i) and 1 ≤i ≤r. In Eq. (1), t is ideally\nhalf of the width of every hit frame number. However, it\nis a fixed parameter, and clearly, all stroke clips defined in\nEq. (1) realistically contain at least one hit frame and possi-\nbly many hit frames.\nIntuitively, consecutive strokes are highly correlated, and\ncorrelation between far-away strokes drops very rapidly.\nThus, to increase the stroke classification accuracy, we\nwould like to provide additional strokes to the classifier.\nOn the other hand, we would not like to provide too many\nstrokes to the classifier, as far-away strokes are much less\ncorrelated with, and can be even irrelevant to, the target\nstroke being classified. Our goal is for each stroke clip to\ninclude information only from the previous and next strokes\nswung by the opponent. Achieving this goal is difficult with\na fixed-width clipping strategy using a fixed t. For this rea-\nson, we propose a variable-width clipping strategy as fol-\nlows. First, define\nˆh(i)\nj−1 =\n(\nh(i)\nj−1,\nif j > 1\nh(i)\nj\n−t,\nif j = 1,\nˆh(i)\nj+1 =\n(\nh(i)\nj+1 + ϵ,\nif j < n(i)\nh(i)\nj\n+ t,\nif j = n(i),\n(2)\nand a new stroke clip ˆC(i)\nj\nin rally Ri to be\nˆC(i)\nj\n= {Fˆh(i)\nj−1, Fˆh(i)\nj−1+1, . . . , Fˆh(i)\nj+1}.\n(3)\nIf the target stroke being classified is the first or last stroke\nin a rally, we set the lower or upper bound of the stroke clip\nto be the same value as used in Eq. (1). In Eq. (2), ϵ is a\nsmall parameter less than t. Looking at this strategy from\nthe perspective of shuttlecock trajectory, it captures clean\nand complete trajectories in three stages: (1) the shuttlecock\nflying towards the target player’s court, (2) the shuttlecock\ntrajectory after being hit by the target player, and (3) the\ndirection of the shuttlecock trajectory after the opponent’s\nresponse. These stages respectively represent (1) the target\nplayer’s observation and reaction phase, (2) the actual tra-\njectory of the target stroke, and (3) the opponent’s response\npattern. This allows the model to not only judge based on\n(2) but also understand the target player’s common strate-\ngies after going through (1) and infer the target stroke-type\nback from (3). The reason that the stroke information in (3)\nis less than that in (1), which means that ϵ cannot be set too\nlarge, is to preserve the characteristic of keeping the target\nhit frame close to the middle of the stroke clip under the\nideal condition in Eq. (1).\nWe remark that the dataset (Sec. 4.1) we used to train our\nmodel contains hit frame and rally information. To classify\na match video without hit frame and rally information, we\ncan use a model called HitNet proposed in MonoTrack [23]\nto detect hit frames. To detect rallies, we note that time in-\ntervals between successive rallies are in general much larger\nthan the time differences between two typical successive hit\nframes in one rally. In practice, we also set some limits on\nˆh(i)\nj−1 and ˆh(i)\nj+1 to prevent the model from accessing irrele-\nvant information caused by remaining unclean data.\n3.2. Model Inputs Extracted from Clips\nNow, we have a set of stroke clips extracted from the match\nvideos. The model inputs we need are player poses, shuttle-\ncock trajectories, and player positions.\nFor the player poses and positions, we extract human\nposes from the clips using MMPose [8] toolbox at first. The\nmodels we utilized in MMPose are RTMPose [18] for 2D\npose estimation and MotionBERT [38] for further 3D pose\nestimation. (We choose 2D poses for the input of the model.\nSee Sec. 5 for more details.) However, there are more than\ntwo people, including the two players, spectators, and ref-\nerees, in the clips. To extract the exact two player poses,\nwe need the court lines to determine the player positions to\nfilter out the irrelevant poses. Fortunately, the court lines\ninformation is available in our dataset (Sec. 4.1). Even if\nthe court lines are not available, we can use the algorithm\nproposed in MonoTrack [23] to extract the court lines.\nFor the shuttlecock trajectories, we utilize the 2D shut-\ntlecock positions generated by TrackNetV31 [1, 6] for each\nclip. Despite the existence of the 3D shuttlecock trajectory\nphysical model MonoTrack [23], the first work to estimate\n3D shuttlecock trajectories over 2D, precise 3D estimation\nremains challenging.\n3.3. Badminton Stroke-type Transformer (BST)\nThe architecture of BST is derived from a Transformer-\nbased model TemPose [16], which, to our knowledge, is\nthe state-of-the-art badminton stroke classification model.\nA key distinction is that BST shifts its focus from player\nposes to shuttlecock trajectory information. Below, we de-\nscribe several versions of BST, as follows: BST-0 serves\nas a prototype for the BST series. BST-0 takes player poses\nand shuttlecock trajectory information as its inputs, whereas\nBST-1, BST-2, and BST-3 incorporate additional player po-\nsition information.\n3.3.1. BST-0\nInitially, the player poses2 and shuttlecock positions are\nprocessed through a TCN module. After the outputs from\nthis TCN are passed through a Transformer Encoder, indi-\nvidual class tokens with spatiotemporal representations are\nobtained, as shown in Fig. 1.\n1There are two versions of TrackNetV3 developed by different authors.\nWe choosed the one using attention for faster trajectory generating.\n2In practice: Top player pose is Pose 1; Bottom player pose is Pose 2.\n3\n\n\nFigure 1. Architecture of BST-0\nThe Transformer modules in this paper are more simi-\nlar to that used in TemPose [16] than the original Trans-\nformer [31]. At the beginning, a learnable class token is\nprepended to the input sequence for summarization, and a\nlearnable positional embedding is added to the input as well.\nWe can formulate our Transformer Encoder as follows:\n˜X(l) = X(l−1) + MHSA(split(LN(X(l−1))))\n(4)\nX(l) = ˜X(l) + FFN(LN( ˜X(l)))\n(5)\nwhere X(l) ∈RS×Dmodel is the output to the l-th layer, S\nis the sequence length, Dmodel is the dimension used in the\nmodel, LN denotes the layer normalization, MHSA is the\nmulti-head self-attention mechanism, and FFN is the feed-\nforward network. Before passing through the MHSA, the\ninput X(l) which simplifies here to X is splitted into:\nQ = [ Q1 · · · QH ] = XWQ ∈RS×(DA×H)\n(6)\nK = [ K1 · · · KH ] = XWK ∈RS×(DA×H)\n(7)\nV = [ V1 · · · VH ] = XWV ∈RS×(DA×H)\n(8)\nwhere WQ, WK, WV ∈RDmodel×(DA×H) are the weight\nmatrices, DA is the hyperparameter representing the dimen-\nsion used in the attention modules, and H is the number of\nthe heads. Further in the MHSA, we formulate as follows:\nMHSA(Q, K, V ) = [ · · · SA(Qh, Kh, Vh) · · · ] WO\n(9)\nSA(Qh, Kh, Vh) = softmax\n\u0012Qh(Kh)T\n√DA\n\u0013\nVh\n(10)\nwhere WO ∈R(DA×H)×Dmodel represents the linear trans-\nformation from the dimenstion DA to the original model\ndimension Dmodel.\nNext, a key concept of our approach is ensuring that\nthe model focuses on extracting critical information from\nthe shuttlecock trajectory.\nTo achieve this, we design a\nCross Transformer layer with a Multi-Head Cross Atten-\ntion (MHCA), which differs from MHSA in one key aspect:\nin the attention mechanism, K and V are derived from the\nshuttlecock trajectory latent, while Q is generated from the\nplayer pose latent or the player position latent, respectively,\nto perform cross-attention. As a result, the subsequent out-\nputs follow two distinct pathways. One pathway is dedi-\ncated to the trajectory information related to the first player\nposes, while the other pathway focuses on the trajectory in-\nformation related to the second player poses. Both path-\nways are fed into a second Transformer Encoder.\nFinally, the model generates logits through an MLP\n(multi-layer perceptron) head, which are converted into\nprobability values using the softmax function. The model\nis trained using cross-entropy loss.\n3.3.2. Pose Postion Fusion (PPF)\nIn subsequent models, player position information was in-\ncorporated as an additional input to the model. X is prepro-\ncessed by a Pose Position Fusion (PPF) module to gener-\nate a fused representation of pose and position, as shown in\nFig. 2. The output of the MLP in the PPF module multiplies\nback with the original poses and becomes the coefficients on\nthe player poses.\nFigure 2. Pose Postion Fusion (PPF)\n3.3.3. BST-1\nIn BST-1 shown in Fig. 3, a Clean Gate module is designed\nto refine the information derived from the shuttlecock tra-\njectory by leveraging the interaction strength between the\n4\n\n\ntwo players and the shuttlecock. This helps the model filter\nout noise in the trajectory caused by the opponent’s strokes.\nThe Clean Gate module shown in Fig. 4 takes three inputs:\n(1) the class token representing shuttlecock information, (2)\nthe fusion class token of the shuttlecock with the player one,\nand (3) the fusion class token of the shuttlecock with the\nplayer two. The input (2) and (3) are processed through\nmin pooling and a MLP to generate values, which are then\nused to adjust the shuttlecock information class token, input\n(1), accordingly.\nFigure 3. Architecture of BST-1\nFigure 4. Clean Gate\n3.3.4. BST-2\nIn BST-2, shown in Fig. 5, we compare the cosine similar-\nity between input (1) and (2) with that between input (1)\nand (3). The difference between these two values gives ˜α.\nSince cosine similarity ranges from -1 to 1, ˜α ∈[−2, 2].\nWe normalize it using α = (˜α + 2)/4 so that α ∈[0, 1]. α\ndenotes the coefficient on the target player information, so\n1 −α is the coefficient on his/her opponent’s information.\nThis ensures that the player with a stronger correlation to\nthe overall shuttlecock trajectory exerts a greater influence\non the input of the MLP head.\nFigure 5. Architecture of BST-2\n3.3.5. BST-3\nBST-3, as shown in Fig. 6, combines the benefits of BST-\n1, which focuses on purifying shuttlecock information, and\nBST-2, which emphasizes the information derived from the\ntarget player.\n4. Experiments\nIn this section, we present the dataset used in our exper-\niments and compare our results with those obtained by\nseveral existing methods, including ST-GCN [33], Block-\n5\n\n\nFigure 6. Architecture of BST-3\nGCN [37], SkateFormer [9], ProtoGCN [22], and Tem-\nPose [16].\n4.1. Dataset\nShuttleSet3 [32], the largest publicly available badminton\nvideo dataset, contains 104 sets, 3,685 rallies, and 36,492\nstrokes across 44 matches played between 2018 and 2021,\nfeaturing 27 top-ranking male and female singles players.\nAfter excluding incorrectly labeled and problematic data, a\ntotal of 40 matches, 33429 strokes remained for analysis.\nWe set 30 matches for training, 5 matches for validation,\nand 5 matches for testing.\nThe original ShuttleSet dataset consists of 19 distinct\nstroke categories, including a ”None” type. One category,\ndriven flight, containing fewer than 50 strokes across the en-\ntire dataset was merged into the ”None” category by us. Ad-\nditionally, since the dataset does not differentiate between\nstrokes played by the top or bottom player within each cat-\n3ShuttleSet has labels and video links. These videos are on BWF TV\n(https://www.youtube.com/@bwftv).\negory (except for ”None”), the number of stroke categories\n(except for ”None”) was doubled. This adjustment resulted\nin a total of 35 categories, including ”None”.\n4.2. Results\nTab. 1 demonstrates that even the most advanced SAR mod-\nels struggle to achieve satisfactory accuracy. Without incor-\nporating additional inputs, such as shuttlecock trajectories\nand player positions, significant performance improvements\nare difficult to attain. However, integrating such informa-\ntion is not straightforward for these models. This highlights\nthe need for specialized models like TemPose [16], which\nare specifically designed to handle these additional inputs.\nInitially, by focusing solely on player position fu-\nsion, TemPose-PF achieves results that surpass those of\nTemPose-V. The reason for this improvement is that player\npositions imply some information about the shuttlecock tra-\njectory, that is, the players are always chasing the shuttle-\ncock during a rally. On the other hand, integrating only\nshuttlecock positions, TemPose-SF leads to a substantial\nimprovement of over two percentage points. Furthermore,\ncombining both types of fusion further enhances perfor-\nmance. Notably, our architecture BST, achieves a modest\nimprovement over TemPose when using a fixed-width clip-\nping strategy.\nFinally, the incorporation of our segmentation strategy\nresults in even greater performance gains across all BST\nmodels, significantly exceeding the performance of Tem-\nPose, as shown in Tab. 2.\n5. Discussion\n5.1. Classifying Difficulty on ShuttleSet\nAlthough our model achieved the state-of-the-art perfor-\nmance on ShuttleSet [32], the accuracy of it was still not\nhigh enough. We suspect that this is primarily due to the\nexcessive granularity of certain categories. The normalized\nconfusion matrices of BST-0 are shown in Fig. 7. The 0th to\n16th strokes are executed by the top player, the 17th to 33rd\nstrokes are hit by the bottom player, and the 34th stroke be-\nlongs to the ”None” category. Observing on both confusion\nmatrices, the model struggles to differentiate between simi-\nlar stroke-types, such as the 2nd and 3rd strokes (top smash\nand top wrist smash) and the 19th and 20th strokes (bot-\ntom smash and bottom wrist smash). A closer inspection\nreveals that the model often misclassifies the 1st stroke (top\nreturn net) as the 13th stroke (top defensive return drive)\nthat shown in the left matrix. Additionally viewing the diag-\nonal elements in the right matrix, the 13th stroke (top defen-\nsive return drive) has the worst recall. These results suggest\nthat the model has difficulty distinguishing between subtle\nvariations in the strokes with similar hitting characteristics.\nWe observed the same phenomenon when employing other\n6\n\n\nModel\nPublication\nModality\nSP\nPP\nAcc\nMacro-F1\nAcc-2\nNumParams\nST-GCN [33]\nAAAI 2018\nJ\n×\n×\n0.728\n0.656\n0.897\n3.08M\nBlockGCN [37]\nCVPR 2024\nJ\n×\n×\n0.715\n0.627\n0.886\n1.50M\nSkateFormer [9]\nECCV 2024\nJ\n×\n×\n0.712\n0.627\n0.859\n2.38M\nProtoGCN [22]\n(arXiv 2024)\nJ\n×\n×\n0.723\n0.637\n0.897\n4.11M\nTemPose-V [16]\nCVPRW 2023\nJ\n×\n×\n0.720\n0.642\n0.892\n1.62M\nTemPose-V [16]\nCVPRW 2023\nJ+B\n×\n×\n0.730\n0.645\n0.895\n1.62M\nTemPose-PF\nJ+B\n×\n✓\n0.738\n0.677\n0.905\n1.68M\nTemPose-SF\nJ+B\n✓\n×\n0.7602\n0.6922\n0.9196\n1.65M\nBST-0\nJ+B\n✓\n×\n0.7626\n0.6908\n0.9206\n1.81M\nTemPose-TF [16]\nCVPRW 2023\nJ+B\n✓\n✓\n0.7683\n0.7027\n0.9242\n1.71M\nBST-1\nJ+B\n✓\n✓\n0.7681\n0.7004\n0.9252\n1.85M\nBST-2\nJ+B\n✓\n✓\n0.7689\n0.7023\n0.9260\n1.79M\nBST-3\nJ+B\n✓\n✓\n0.7695\n0.7043\n0.9267\n1.85M\nTable 1. Model comparison on the testing set without our segmentation strategy. J and B in the modality denote the players’ joints and\nbones. SP and PP represent additional inputs of shuttlecock and player positions. TemPose-PF and TemPose-SF are intermediate products\nderived by us from the original model, positioned between TemPose-V and TemPose-TF. The suffix ”F” denotes the fusion of the additional\ninputs. Acc-2 denotes top-2 accuracy.\nModel\nModality\nSP\nPP\nAcc\nMacro-F1\nAcc-2\nTemPose-SF\nJ+B\n✓\n×\n0.7534\n0.6833\n0.9194\nBST-0\nJ+B\n✓\n×\n0.7655\n0.6976\n0.9306\nTemPose-TF [16]\nJ+B\n✓\n✓\n0.7580\n0.6902\n0.9214\nBST-1\nJ+B\n✓\n✓\n0.7704\n0.7026\n0.9355\nBST-2\nJ+B\n✓\n✓\n0.7687\n0.7000\n0.9312\nBST-3\nJ+B\n✓\n✓\n0.7710\n0.7042\n0.9340\nTable 2. Model comparison on the testing set after our segmentation strategy.\nmodels as well.\n5.2. Player 2D Joints vs. 3D Joints\nWhile we explored using 3D player joints as input to our\nmodel, the performance was inferior to that achieved with\n2D joints. We attribute this to the fact that the HPE model is\ntrained on a general HPE dataset, encompassing a broader\nrange of poses than those specific to badminton players.\nAdditionally, the 3D joints are estimated from the 2D joints,\nwhich may further contribute to a bias towards general hu-\nman poses, rather than the specialized poses of badminton\nplayers. We can see an example of this in Fig. 8. This issue\nwould mislead the model to predict the wrong action, since\nthe 3D pose is not accurate enough to represent the player’s\npose in the badminton game, as shown in Tab. 3.\n5.3. Limitations and Future Work\nOur method has an obvious limitation: it relies on accurate\nhit frame detection and shuttlecock trajectory tracking mod-\nels. Fortunately, these issues are expected to be resolved in\nthe future, similar to how the transition from HAR to SAR\nwas facilitated by advanced HPE models, which enabled\nSAR to perform more effectively. Moreover, the existing\n2D shuttlecock trajectory tracking models are quite mature,\nand we believe that there will be more 3D tracking mod-\nels in the future. Although this paper primarily focuses on\nstroke classification in singles badminton, the potential ap-\nplicability of our method to other racket sports, such as ten-\nnis and table tennis, or even doubles, is worth exploring.\n6. Conclusion\nIn this paper, we first introduce a novel segmentation strat-\negy that segments a match video into clips containing two\nto three badminton strokes. We propose BST for badminton\nstroke classification. Through experiments, we show that\nour method outperforms existing state-of-the-art methods,\nhighlighting the importance of the shuttlecock informa-\ntion.\n7\n\n\nFigure 7. Normalized confusion matrices of BST-0 on our dataset. The sum of the elements in each column in the left matrix is equal to 1,\nand the sum of the elements in each row in the right matrix is equal to 1.\n(a) 2D pose\n(b) Rear view of the 3D pose\n(c) Front view of the 3D pose\n(d) Top view of the 3D pose\nFigure 8. Example of a 2D pose vs. 3D pose in a badminton video frame. The ninth joint represents the human nose. The red arrows\nindicate the directions the bottom player was facing. We can see the bottom player was facing her opponent in fact, but the 3D pose exhibits\na clear error in the facing direction. We suspect that this 3D model will often assume that if both forearms are pointing roughly forward,\nit will lead the model to assume that the face is also pointing in that direction. (The visualizations in (b), (c), and (d) are generated by the\nMayavi [27] python package.)\nModel\nAccuracy\nMacro-F1\nAccuracy-2\nTemPose-SF\n0.7602 / 0.7458\n0.6922 / 0.6696\n0.9196 / 0.9066\nBST-0\n0.7626 / 0.7498\n0.6908 / 0.6768\n0.9206 / 0.9080\nTemPose-TF [16]\n0.7683 / 0.7563\n0.7027 / 0.6852\n0.9242 / 0.9164\nBST-3\n0.7695 / 0.7560\n0.7043 / 0.6806\n0.9267 / 0.9155\nTable 3. Model with 2D vs. 3D poses comparison on the testing set without our segmentation strategy. The value on the left side of the\nslash is the result of the model with 2D poses, and the right side is the result of the model with 3D poses.\n8\n\n\nReferences\n[1] Alenzenx.\nTracknetv3:\nBeyond tracknetv2 ,and “first”\ntracknet using attention.\nhttps://github.com/\nalenzenx/TrackNetV3, 2023. 2, 3\n[2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An em-\npirical evaluation of generic convolutional and recurrent net-\nworks for sequence modeling. CoRR, abs/1803.01271, 2018.\n2\n[3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affinity\nfields. In CVPR, pages 1302–1310, 2017. 1\n[4] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and\nYaser Sheikh.\nOpenpose: Realtime multi-person 2d pose\nestimation using part affinity fields.\nIEEE TPAMI, 43(1):\n172–186, 2021. 1\n[5] Wei-Ting Chen, Hsiang-Yun Wu, Yun-An Shih, Chih-Chuan\nWang, and Yu-Shuen Wang.\nExploration of player be-\nhaviours from broadcast badminton videos.\nComputer\nGraphics Forum, 42(6):e14786, 2023. 2\n[6] Yu-Jou Chen and Yu-Shuen Wang. Tracknetv3: Enhancing\nshuttlecock tracking with augmentations and trajectory recti-\nfication. In ACM International Conference on Multimedia in\nAsia, New York, NY, USA, 2024. Association for Computing\nMachinery. 2, 3\n[7] Wei-Ta Chu and Samuel Situmeang. Badminton video anal-\nysis based on spatiotemporal and stroke features. In ACM on\nInternational Conference on Multimedia Retrieval (ICMR),\npages 448–451, New York, NY, USA, 2017. Association for\nComputing Machinery. 2\n[8] MMPose Contributors. Openmmlab pose estimation tool-\nbox and benchmark. https://github.com/open-\nmmlab/mmpose, 2020. 3\n[9] Jeonghyeok Do and Munchurl Kim. Skateformer: skeletal-\ntemporal transformer for human action recognition.\nIn\nECCV, pages 401–420. Springer, 2025. 1, 2, 6, 7\n[10] Haodong Duan,\nJiaqi Wang,\nKai Chen,\nand Dahua\nLin.\nDg-stgcn:\nDynamic spatial-temporal modeling for\nskeleton-based action recognition.\narXiv e-prints, art.\narXiv:2210.05895, 2022. 1\n[11] Javier Fern´andez and Luke Bornn. Soccermap: A deep learn-\ning architecture for visually-interpretable analysis in soc-\ncer.\nIn Machine Learning and Knowledge Discovery in\nDatabases. Applied Data Science and Demo Track, pages\n491–506, Cham, 2021. Springer International Publishing. 1\n[12] Zhimin Gao, Peitao Wang, Pei Lv, Xiaoheng Jiang, Qidong\nLiu, Pichao Wang, Mingliang Xu, and Wanqing Li. Focal\nand global spatial-temporal transformer for skeleton-based\naction recognition. In ACCV, pages 155–171, Berlin, Hei-\ndelberg, 2022. Springer-Verlag. 1\n[13] Anurag Ghosh, Suriya Singh, and C. V. Jawahar.\nTo-\nwards structured analysis of broadcast badminton videos. In\nIEEE Winter Conference on Applications of Computer Vision\n(WACV), pages 296–304, 2018. 1, 2\n[14] Mei-Ling Huang and Yun-Zhi Li. Use of machine learning\nand deep learning to predict the outcomes of major league\nbaseball matches. Applied Sciences, 11(10), 2021. 1\n[15] Yu-Chuan Huang, I-No Liao, Ching-Hsuan Chen, Ts`ı-U´ı ˙Ik,\nand Wen-Chih Peng. Tracknet: A deep learning network for\ntracking high-speed and tiny objects in sports applications.\nIn IEEE International Conference on Advanced Video and\nSignal Based Surveillance (AVSS), pages 1–8, 2019. 2\n[16] Magnus Ibh, Stella Grasshof, Dan Witzner, and Pascal\nMadeleine.\nTempose: a new skeleton-based transformer\nmodel designed for fine-grained motion recognition in bad-\nminton. In CVPRW, pages 5199–5208, 2023. 1, 2, 3, 4, 6, 7,\n8\n[17] Kan Jiang, Jiayu Li, Zhaoyu Liu, and Chen Dong. Court\ndetection using masked perspective fields network. In IEEE\n28th Pacific Rim International Symposium on Dependable\nComputing (PRDC), pages 342–345, 2023. 1\n[18] Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han,\nChengqi Lyu, Yining Li, and Kai Chen. Rtmpose: Real-time\nmulti-person pose estimation based on mmpose, 2023. 1, 3\n[19] Thomas N. Kipf and Max Welling. Semi-supervised classi-\nfication with graph convolutional networks. In International\nConference on Learning Representations, 2017. 2\n[20] Jungho Lee, Minhyeok Lee, Dogyoon Lee, and Sangyoun\nLee.\nHierarchically decomposed graph convolutional net-\nworks for skeleton-based action recognition. In ICCV, pages\n10444–10453, 2023. 1\n[21] Meng Li, Yaqi Wu, Qiumei Sun, and Weifeng Yang. Two-\nstream proximity graph transformer for skeletal person-\nperson interaction recognition with statistical information.\nIEEE Access, 12:193091–193100, 2024. 1\n[22] Hongda Liu, Yunfan Liu, Min Ren, Hao Wang, Yunlong\nWang, and Zhenan Sun. Revealing key details to see differ-\nences: A novel prototypical perspective for skeleton-based\naction recognition. arXiv preprint arXiv:2411.18941, 2024.\n1, 2, 6, 7\n[23] Paul Liu and Jui-Hsien Wang. Monotrack: Shuttle trajec-\ntory reconstruction from monocular badminton video.\nIn\nCVPRW, pages 3512–3521, 2022. 1, 2, 3\n[24] Woomin Myung, Nan Su, Jing-Hao Xue, and Guijin\nWang.\nDegcn: Deformable graph convolutional networks\nfor skeleton-based action recognition. IEEE TIP, 33:2477–\n2490, 2024. 1\n[25] Masato Nakai, Yoshihiko Tsunoda, Hisashi Hayashi, and\nHideki Murakoshi. Prediction of basketball free throw shoot-\ning by openpose. In New Frontiers in Artificial Intelligence,\npages 435–446, Cham, 2019. Springer International Publish-\ning. 1\n[26] Nadav Oved, Amir Feder, and Roi Reichart. Predicting in-\ngame actions from interviews of nba players. Computational\nLinguistics, 46(3):667–712, 2020. 1\n[27] Prabhu Ramachandran and Ga¨el Varoquaux.\nMayavi: a\npackage for 3d visualization of scientific data.\nCoRR,\nabs/1010.4891, 2010. 8\n[28] Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, and Gang\nWang. Deep multimodal feature analysis for action recog-\nnition in rgb+d videos.\nIEEE TPAMI, 40(05):1045–1058,\n2018. 1\n[29] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose esti-\nmation. In CVPR, 2019. 1\n9\n\n\n[30] Nien-En Sun, Yu-Ching Lin, Shao-Ping Chuang, Tzu-Han\nHsu, Dung-Ru Yu, Ho-Yi Chung, and Ts`ı-U´ı ˙Ik. Tracknetv2:\nEfficient shuttlecock tracking network. In International Con-\nference on Pervasive Artificial Intelligence (ICPAI), pages\n86–91, 2020. 2\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS. Curran\nAssociates, Inc., 2017. 2, 4\n[32] Wei-Yao Wang, Yung-Chang Huang, Tsi-Ui Ik, and Wen-\nChih Peng.\nShuttleset: A human-annotated stroke-level\nsingles dataset for badminton tactical analysis.\nCoRR,\nabs/2306.04948, 2023. 2, 6\n[33] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\nral graph convolutional networks for skeleton-based action\nrecognition. AAAI, 32(1), 2018. 1, 2, 5, 7\n[34] See Shin Yue, Raveendran Paramesran, and Ganesh Krish-\nnasamy. Ready-to-serve detection in badminton videos. In\nInternational Conference on Electronics, Information, and\nCommunication (ICEIC), pages 1–5, 2024. 2\n[35] Yongkang Zhao.\nAutomatic shuttlecock motion recogni-\ntion using deep learning. IEEE Access, 11:111281–111291,\n2023. 2\n[36] Yuxuan Zhou, Zhi-Qi Cheng, Chao Li, Yifeng Geng, Xu-\nansong Xie, and Margret Keuper.\nHypergraph trans-\nformer for skeleton-based action recognition. arXiv preprint\narXiv:2211.09590, 2022. 1\n[37] Yuxuan Zhou, Xudong Yan, Zhi-Qi Cheng, Yan Yan, Qi\nDai, and Xian-Sheng Hua. Blockgcn: Redefining topology\nawareness for skeleton-based action recognition. In CVPR,\n2024. 1, 2, 6, 7\n[38] Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne\nWu, and Yizhou Wang. Motionbert: A unified perspective on\nlearning human motion representations. In ICCV, 2023. 1, 3\n10\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21085v1.pdf",
    "total_pages": 10,
    "title": "BST: Badminton Stroke-type Transformer for Skeleton-based Action Recognition in Racket Sports",
    "authors": [
      "Jing-Yuan Chang"
    ],
    "abstract": "Badminton, known for having the fastest ball speeds among all sports,\npresents significant challenges to the field of computer vision, including\nplayer identification, court line detection, shuttlecock trajectory tracking,\nand player stroke-type classification. In this paper, we introduce a novel\nvideo segmentation strategy to extract frames of each player's racket swing in\na badminton broadcast match. These segmented frames are then processed by two\nexisting models: one for Human Pose Estimation to obtain player skeletal\njoints, and the other for shuttlecock trajectory detection to extract\nshuttlecock trajectories. Leveraging these joints, trajectories, and player\npositions as inputs, we propose Badminton Stroke-type Transformer (BST) to\nclassify player stroke-types in singles. To the best of our knowledge,\nexperimental results demonstrate that our method outperforms the previous\nstate-of-the-art on the largest publicly available badminton video dataset,\nShuttleSet, which shows that effectively leveraging ball trajectory is likely\nto be a trend for racket sports action recognition.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}