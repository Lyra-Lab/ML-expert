{
  "id": "arxiv_2502.21019v1",
  "text": "Nano Drone-based Indoor Crime Scene Analysis*\nMartin Cooney,1 Sivadinesh Ponrajan,1 and Fernando Alonso-Fernandez1\nAbstract— Technologies such as robotics, Artificial Intelli-\ngence (AI), and Computer Vision (CV) can be applied to\ncrime scene analysis (CSA) to help protect lives, facilitate\njustice, and deter crime, but an overview of the tasks that\ncan be automated has been lacking. Here we follow a speculate\nprototyping approach: First, the STAIR tool is used to rapidly\nreview the literature and identify tasks that seem to have not\nreceived much attention, like accessing crime sites through\na window, mapping/gathering evidence, and analyzing blood\nsmears. Secondly, we present a prototype of a small drone\nthat implements these three tasks with 75%, 85%, and 80%\nperformance, to perform a minimal analysis of an indoor crime\nscene. Lessons learned are reported, toward guiding next work\nin the area.\nI. INTRODUCTION\nThe current paper reports on gaps and lessons learned\ndesigning a prototype of a small drone for indoor crime scene\nanalysis.\nAutomation of crime scene analysis (CSA) is an important\nproblem: For example, violence alone was estimated to cost\n14.76 USD trillion globally in 2017 [1]. When crimes occur\nbehind closed doors, investigation can be required to reveal\nthe truth of what transpired. Currently, however, investigators\ncan suffer physical danger from criminals or traps at the\nscene, illnesses from contagious or toxic materials, and\npsychological harm [2]. Victims as well often do not receive\njustice, given numerous challenges such as human error, bias,\ncontamination, understaffing and underfunding; degradation\nof evidence over time; and old-fashioned methods that can\nbe inefficient and inaccurate such as measuring bloodstains\nby hand, paper and pencil drawings, carrying and changing\nlight box filters, or working in the dark with flashlights and\npotentially missing evidence [2].1 As in Fig. 1, we imagine\nthat small drones like in Table I, that can navigate in tight\nindoor spaces, could help to reduce risks to investigators\nand facilitate justice for victims by accessing crime scenes\nquickly, reducing risks of contamination, and using sensory\nand visualizing modalities not available to humans to better\ncapture and share information. Thereby, the idea is not that\nhumans should be replaced, but that a drone can help humans\nby providing rapid initial processing.\nHere, the term \"drone\" is defined loosely to comprise\nunmanned aerial vehicles (UAV), unmanned aircraft systems\n(UAS), unmanned aircraft vehicle systems (UAVS), remotely\n*We gratefully acknowledge support from the Swedish Innovation\nAgency (Vinnova) for the project \"AI-Powered Crime Scene Analysis\" and\nadvice from the Swedish Police\n1M. Cooney, S. Ponrajan, and F. Alonso-Fernandez are with the School\nof Information Technology, Halmstad University 301 18 Halmstad, Sweden\nmartin.daniel.cooney@gmail.com\n1universalclass.com/articles/law/setting-crime-scene-perimeters.htm\nFig. 1.\nBasic concept: a drone could help to quickly and safely analyze\ncrime scenes, (a) accessing the scene to gain situation awareness, (b)\ngathering evidence, (c) and conducting initial analysis for an investigation.\nTABLE I\nSOME NANO DRONES THAT COULD BE USED FOR INDOOR CRIME SCENE\nANALYSIS (CSA).\nHarvard RoboBee [3]\n0.08 g\nAeroVironment Nano Hummingbird\n˜19 g\nBlade Nano QX\n22 g\nCrazyflie\n27 g\nZano\n55 g\nTeledyne FLIR Black Hornet 3/4\n32 g/70 g\nTrashcan drone [4]\n72 g\nDJI Ryze Tello\n80 g\npiloted aerial vehicles (RPAV), and remotely piloted air-\ncraft systems (RPAS), including quadcopters, quadrocopters,\nquadrotors, and micro aerial vehicles (MAVs). Furthermore,\nwe follow the convention that \"nano drones\" are defined as\nless than 250 g.2\nA challenge is that it’s unclear which tasks nano drones\ncould do, since CSA is highly complex. To gain insight,\nwe follow a speculative prototyping approach [5], guided\nby the STAIR tool from forensic science, which states that\nan analyst must understand the Situation, carry out Tasks,\nAnalyze evidence, and Investigate, to obtain Results [6]. 3\nThus, our contribution here is two-fold:\n• Theoretical. We speculate on how drones could be\nuseful for crime scene analysis, including a list of some\npotential desired capabilities.\n• Practical.\nWe\nprototype\ndrones\nthat\ncan\nsemi-\nautonomously carry out some actions within a mock-up\ncrime scene.\nThe remainder of the paper is structured as follows: In\nSection II, we position the current paper in regard to previous\nwork, identifying gaps. In Section III, we explore three\ngaps via prototyping, with evaluation results discussed in\n2euro-sd.com/2024/04/articles/37409/nano-uav-and-micro-uav-\ndevelopments\n3https://online.campbellsville.edu/infographics/7-steps-of-a-crime-scene-\ninvestigation/\narXiv:2502.21019v1  [cs.RO]  28 Feb 2025\n\n\nSection IV. Our aim is to provide insights that could inspire\ndiscussion and interest, toward bringing justice in a faster,\nmore accurate way.\nII. RELATED WORK\nTo survey the area, top results were analyzed from ACM,\nIEEE Xplore, and Google Scholar with the search phrase\n\"(drone OR UAV OR robot) AND (crime)\". Bibliographies\nof relevant papers were scoured, and some known papers\nalso added. To try to avoid missing important information,\nwe also followed up with some extra queries to Scite.AI and\nChatGPT. Results outside of the scope of the paper were\nremoved, regarding crimes by drones, security of drones,\nand forensic analysis of drones to catch human criminals.\nThis led to 32 papers being reviewed (newest: 2024, oldest:\n1991), which are summarized below according to the STAIR\nframework.\nA. Situation Awareness\nThe Situation step in STAIR involves accessing a crime\nscene to infer if the threat has passed and what has happened\n(where and when, and to whom).\nPrevious work has begun to describe how robots can be\npiloted to go quickly to crime scenes (e.g. from rooftops\nin Sweden, like a mini-helicopter),4 detect doors and win-\ndows,5 and enter buildings by opening doors,6, breaking\nwalls or being thrown inside by a human.7 Drones can even\nacrobatically pass through narrow gaps [7]. Some drones\ncapable of threat detection have also been designed to\ndetect dangerous traps, pathogens, and criminals, such as\nImprovised Explosive Devices (IEDs) with radar, disease-\ncarrying animal carcasses via thermal camera [8], and es-\ncaping or hiding people [9].8 In regard to detecting offenses,\ncameras have also been used to recognize fighting [10]. More\ngenerally, robots have used observation to automatically infer\nrules underlying human behavior and interact, in games like\nchasing, follow-the-leader, and tag [11].\nThus, much remains to be done in this area. Our brain-\nstorming suggested that gaps could include:\n• Access in challenging cases. Drones could traverse\npartially open windows or doors, or pick locks.\n• Threat detection. Indoor hiding criminals and victims\nwho are paused or moving to avoid detection (e.g. in a\ncloset, under furniture, or behind a wall in a neighboring\nroom) could be detected via radar or thermal cameras.\n• Offense detection. Foundation models and Large lan-\nguage models (LLMs) could be shown multimodal\ndata (e.g., surveillance footage of crimes) and queried;\ni.e., to determine roles (even for groups coming and\ngoing), infer past/future actions, estimate levels of force,\n4youtube.com/watch?v=wV8rNjvqM9A\n5github.com/sayedmohamedscu/YOLOv8-Door-detection-for-visually-\nimpaired-people\n6youtube.com/watch?v=wV8rNjvqM9A\n7wired.com/2016/07/11-police-robots-patrolling-around-world\n8straitstimes.com/singapore/courts-crime/keeping-watch-from-the-skies-\npolice-unveil-two-new-drones-for-crowd-management-search-and-rescue\npinpoint focal points of a crime (location and cause,\nlike where a person was stabbed or a fire or explosion\nstarted), detect anomalies (such as broken lamp), and\ngenerate crime scene sketches, etc.\nB. Tasks\nThe Tasks step in STAIR involves dealing with active\nthreats to ensure safety; freezing and controlling a perimeter;\ndetecting, gathering, and preserving evidence; as well as\ncleaning.\nRobots have been designed to deal with threats like\nIEDs (improvised explosive devices) [12], disinfect complex\nsurfaces with ultraviolet irradiation [13], and safely extract\nsamples from corpses [14]. To avoid psychological harm to\nhumans or privacy infringements, video can be altered via\nblurring, masking, or mosaics [15]. Our previous work also\nlists various robots carrying weapons such as explosives, stun\nguns, pepper spray, water, firearms, or nets, some of which\nhave been used to detect, negotiate with, or neutralize threats,\nincluding via computer vision [16]. A makeshift perimeter\nto protect evidence could also be created by dropping bricks\nat appropriate locations, a strategy which has been used\npreviously by drones to construct a building [17].\nAs shown in Table II, previous exploratory work in se-\ncuring evidence can be roughly split into work manually\npiloting a drone or using a lidar to take 3D scans of a mock-\nup crime scene to assess potential benefits or demerits of\ntechnology for CSA; variables of interest have mainly been\nbloodstains, guns, knives, and bodies. Furthermore, some\ndatasets have been created that could be used to train a\nvisual system to detect evidence for crime scene analysis,\nas exemplified in Table III. Various photogrammetry tools\nalso exist that could be used to create 3D models, such as\n3DF Zephyr,9 Scaniverse,10 Luma AI,11 OpenDroneMap,12\nand DroneDeploy.13\nThus, much remains to be done in this area. Gaps include:\n• Handling threats. A drone could estimate backdrops\nand infer where and when force could be used; as well,\ndrones could clear rooms and enter in a fast, effective,\nunpredictable, and safe way, like a SWAT team.\n• Geofencing. In relation to detecting where a crime took\nplace, a drone could create a perimeter, establish a\n\"path of contamination\" or common approach path, and\nmanage the scene–preventing unauthorized entry and\nimproper actions.\n• Handling evidence. When mapping, useful informa-\ntion, like the distances between evidence (e.g., blood\nand body and weapon/objects) should also be calcu-\nlated. Another important step after detecting evidence\nis gathering it (e.g., blood with a swab). For blood,\npresumptive testing could be conducted with lumi-\nnol or tetramethylbenzidine (Hemastix) mounted to a\n9https://www.3dflow.net/3df-zephyr-photogrammetry-software/\n10https://scaniverse.com/\n11https://lumalabs.ai/dream-machine\n12https://www.opendronemap.org/\n13https://www.dronedeploy.com/\n\n\nTABLE II\nCRIME SCENE EVIDENCE.\nUrbanova et al. [18]\ndrone/CV (3D scanning outdoors via a piloted drone and\nphotogrammetry (Agisoft PhotoScan))\ndummy, bones, and artificial blood, in grass\nGeorgiou et al. [19]\ndrone/CV (piloting a DJI Spark drone outdoors for accu-\nrate, fast, reliable detection)\ncolored square foam pads\nCooney et al. drone/CV [20]\ndrone/CV (piloting a Ryze Tello drone indoors to detect\nvia YOLO/heat traces)\nhidden cameras\nBucknell\nand\nBassin-\ndale [21]\ndrone (piloted a Parrot AR.Drone 2.0 at various heights\nto check the effects of downwash on sensitive evidence)\ntextile fibres on various substrates\nRymansaib et al. [22]\ndrone/sonar\nmannequin \"body\" (future target possibly also weapons,\nnarcotics, or IEDs)\nAraujo et al. [23]\nsimulation of a drone detecting evidence based on Air-\nSim/Unreal Engine/YOLO\nbody, bloodstain, gun, and knife (augmented MS-COCO\ndata set)\nButt et al. [24].\nCV (YOLO)\nbullet holes\nNandhini\nand\nThinakaran [25]\nCV (classification 7-layer CNN) in infrared images\nfirearms, knives, money, blood, animals, cars, and cell-\nphones (some unclarity regarding datasets)\nBuck et al. [26]\n3D scanning\nbullet trajectories, and damage to bodies and objects\nEsposito et al. [27]\n3D scanning\nlocations and distances between corpses, bloodstains, and\nweapons, bullet trajectories, paths where the victim might\nhave moved before dying, balcony height\nGalanakis et al. [28]\n3D scanning\n\"dead body\" and various objects such as a phone, screws,\ntape, and a box (dataset released)\nLiscio et al. [29]\n3D scanning (using an apparatus)\n\"cast-off\" bloodstains–from blood flying off a weapon\nsuch as a bat, hammer, knife or pipe–estimating a \"Path\nVolume Envelope\" for the path the weapon took\nTABLE III\nSOME DATASETS.\nWeapons\nWeapon\ndetection\ndatasets14\n9261,\n10039,\n23,672,\n3000, 2078, 3255 images\nWeapons\nInternet Movie Firearms\nDatabase15\n31,147 articles\nShoe prints\nCrime\nScene\nFootwear\nImpressions\nDatabase [30]\n170 samples\nShoe prints\nMUES-SR10KS2S\ndataset [31]\n10,096 samples\nShoe prints\nFID-300 dataset [32]\n1,475 samples (not pub-\nlic)\nVarious\nRoboflow Universe16\n350\nmillion+\nimages,\n500,000+ datasets\ndrone. Other targets that could be important to de-\ntect include footprints/shoeprints, clothing (e.g., gloves,\nshoes), other weapons (bullets, bullet casings, clubs),\nrestraints (e.g., cable ties, handcuffs), extra DNA ev-\nidence (skin, hairs, body fluids other than blood–e.g.,\nlike saliva on cigarettes, drinks, food, or cutlery), and\nobjects which might have broken due to an altercation\n(e.g., glass/pottery fragments, paints, particulates, and\nfragment patterns (“fractography”)). Furthermore any\nobject close to evidence such as a corpse or blood might\nalso be interesting. Basic inference could be conducted\non corpses (identity, pose, age, gender, and estimated\ntime of death (e.g., based on body temperature detected\nwith thermal camera)). As well, a thermal camera could\nbe used to estimate when a person last touched evidence\nlike weapons or drugs if the drone arrives fast enough.\n• Visualizing. RViz in Robot Operating System (ROS),\nUnreal Engine (UE), or Unity, could be used to visual-\nize, \"replay\" crimes, update models when new evidence\nis discovered, and prepare initial reports. The data could\nbe accessed also via Extended Reality (XR) or 3D\nprinted, and different evidence could be marked with\ndifferent colors.\n• Cleaning. A robot such as Roomba could be used\nin simple cases.17 More complex cases might require\nremoving all carpet, scrubbing floors, and more inten-\nsive cleaning. Such a robot could be designed for easy\nwashing and sterilization.\nC. Analysis and Investigation\nIn the Analysis step, theories can be drawn from the\nraw data about motives, opportunities, and means. Theories\nfrom multiple sources (e.g., witness testimonies and physical\nevidence) are then compared in the Investigation step to\nnarrow down what likely happened.\nVarious work is starting to be done with blood pattern\nanalysis (BPA). For example, a review from Weber and\nLednev describes how time since deposition (TSD) can be\nestimated using CV via magenta values, brightness, or blood\npool/crack ratio/colorimetric analysis, despite challenges due\nto effects of substrate, environment (temperature, humidity),\nand contamination [33]. Bergman et al. also describe using\nCV (CNN-based classification) to discriminate between pas-\nsive drip vs. active spatter bloodstains [34]. As well, various\nmathematical models exist; for example, in a crime scene\nwith a broken teapot, a model could be used to infer how\nthe teapot broke [35].\nRegarding investigation, one robot was designed to pub-\nlish a report summarizing anomalies it detected along\nwith interviews from nearby people [36]. More generally,\nlogic programming languages like Prolog, and inference\napproaches such as Maximum Likelihood Estimation (MLE)\n17www.irobot.com/en_US/roomba.html\n\n\nor its Bayesian counterpart Maximum a posteriori inference\n(MAP) can also be used to narrow down possibilities.\nHowever, it seems little work has focused so far on the\nanalysis and investigating steps for CSA robots, possibly\nsince they build on the simpler preceding steps that have\nmostly not yet been implemented in robots. A few examples\nof specific gaps are as follows:\n• Transfer stains. In some cases, blood patterns could\ncarry information about how people moved and so-\ncalled \"second locations\". Moreover, composite scenes\ncomprising various kinds of patterns could be analyzed.\n• Affordances. The possibility for nearby objects to be\nused as makeshift weapons could be assessed when\nhunting for a missing weapon in a violent crime.\n• Motive Inference. For example, a missing or open safe\ncould indicate a robbery; conversely, a murder victim\nwith a wallet could indicate an alternative motive. Also,\na distinctive modus operandi could suggest the work of\na serial criminal.\nThus, while much inspiration can be taken from the litera-\nture, we did not encounter an overview of how drones could\nbe used for crime scene analysis; however, it appeared that\nmany opportunities exist for development, some of which we\nexplore in the next section.\nIII. METHOD\nPrototyping can reveal challenges and opportunities that\nmight not be apparent if only theory is considered. From\nthe identified gaps, we first chose three initial capabilities\nthat seemed feasible and useful to explore: accessing a crime\nscene through a partially opened window, mapping evidence,\nand analyzing motion from a bloody path.\nThen, we constructed proof-of-concepts. For hardware, a\nDJI Ryze Tello drone was augmented to be able to see\nboth below and in front, by popping out its side camera\nto point downwards, and adding a 3D printed stand on top\nof its chassis to carry an ESP32-Cam, powered by a lithium\npolymer battery. The Ryze Tello drone is inexpensive, easily\nprogrammed, and has a large payload (> 60 g); ESP32-\nCam is also inexpensive and easily programmed to wirelessly\nstream SVGA@30fps from its 2 Megapixel OV2640 camera\nto a python client. We also set up some software on an\nexternal laptop, including OpenCV18 and YOLO 11.19 Fig. 2\nand Fig. 3 visually depict some of the challenges and\nsuccesses we encountered using this setup, which are detailed\nbelow.\nA. Capability 1: Situation. Access via a partially open\nwindow\nRapid access is a crucial reason for using drones, and\nhumans cannot be expected to be on scene to let them in, so\ndrones could seek to gain entry in various ways, including\nvia windows. In some cases, however, a window might be\npartially open, such that acrobatic entry could be difficult,\n18opencv.org\n19docs.ultralytics.com/models/yolo11/\nFig. 2.\nExamples of initial challenges: (a) getting caught, (b) markers not\nbeing detected, and (c) false detections in poor illumination\nFig. 3.\nExamples of successful trials: (a) pushing open a window, (b)\nmapping distances, and (c) inferring direction\nand lock-picking unnecessary, but how to handle such cases\nwas unclear. Although hardware is continually advancing,\nclearly it could be difficult currently for a nano drone to force\nopen a tough window. For example, Yeong et al. estimate\nthat opening a door requires 35 N [37]–whereas, based on\nNewton’s second law of motion (F=ma), a drone like a Ryze\nTello with approximate mass 100 g and acceleration 5m/s2\nwould have only 0.5 N of thrust force. While there are larger\ndrones like DJI Matrice and first person view (FPV) racing\ndrones with high acceleration, safety was also a concern.\nThus, our goal was to explore how to safely get started\nwith testing ideas. We chose to prepare an initial mock-up\nwindow using a cardboard box with plastic wrap in place\nof glass. Of the four basic kinds of window–rotating hor-\nizontally (casement) or vertically (awning), and translating\nhorizontally (sliding) or vertically (double hung)–a common\n\"casement\" form was selected. Various opening mechanisms\nfor the drone were also considered, from wedges to actuated\ntongs. Eventually, a simplified approach was selected by\nadding a plastic spike to the underside of the drone with\nputty.\nInitial trials with manually piloting the drone to open the\nwindow with the spike indicated substantial challenges with\nlow force, downwash (inaccurate control), and propellers\ngetting caught, with a performance of roughly only 20%.\nTherefore, we redesigned the test window to be lighter (91\ng), without deep walls, and with paper in place of plastic\nwrap, and conducted 20 trials. The result was 75% successful\nperformance. A main factor in the five failed attempts was\nthe imperfect control due to the inexpensive drone used: for\nexample, the drone sometimes came into contact with the\nwindow near the hinges where torque would be insufficient\nor was knocked outside of the area of the window.\n\n\nB. Capability 2: Task. Mapping evidence\nAfter gaining access, a drone should map the crime\nscene, calculating distances between samples and gathering\nevidence, which was unclear how to realize. Our approach\nconsisted in setting the drone to fly twice, once to create a\n3D map, and the second time to find evidence. To simplify\nour initial exploration, a small 2m x 2m mock-up murder\ncrime scene was created using some photos of guns, bullet\ncasings, and blood, and four ArUco markers to mark walls.\nThe gun was placed on a table, and the other photos on the\nground.\nFirst, a 3D map was initialized. The Ryze Tello drone\ntook off and rotated to detect markers via its side camera\n(ESP32-Cam), whose video feed was streamed through WiFi\nto an external laptop. The first marker detected was set to\nbe located at the origin relative to the x and z axes and\nat a height along the y axis inferred based on the drone’s\nheight given by its bottom infrared range sensor. As the drone\nrotated, newly detected markers were sequentially localized\nwith respect to the first marker. Once mapping was complete,\nthe drone landed.\nSecond, the drone took off again, looking for evidence\nwithin the 3D map, by moving between markers. The drone’s\nvideo feed from its downward facing camera was again\nrelayed to the external laptop, which used YOLO 11 with\na GPU to detect three classes of object (blood, guns, and\nbullet casings). Visual servoing was conducted to center the\ndrone over the detected evidence. Then, information about\nevidence was added to the 3D map, based on the drone’s\nestimated location and altitude. Furthermore, the drone was\nset to land on the center of the detected evidence to mimic\ngathering a sample (e.g., harvesting blood via a swab on\nits underside). Finally, when the drone’s search ended and\nits map was completed, the distances between all detected\nobjects was calculated and output.\nInitial trials indicated problems with ArUco markers being\ntoo small to be detected when the drone was far or too\nbig when the drone was close. Some trials were conducted\nwith multiple markers. Eventually, a 4x4 ChArUco Board\nwas adopted, which combines a chess board pattern and\nmultiple ArUco markers to enhance accuracy and handle\npartial occlusions.\nTo evaluate the system, 20 trials were conducted. Accuracy\nof detecting objects was 85.0%. Reasons for some inac-\ncuracy included illumination and lack of contrast between\nthe bloodstain and the floor, as well as limited amounts\nof training images. The average discrepancy between actual\ndistances and estimated distances was also found to be 2.395\ncm (SD 0.496 cm). This error might have been due to\nchallenges with the stability of the drone when capturing\nimages, illumination, and the limited resolution of the drone’s\ncameras (ESP32-Cam), as a lower resolution was used to\navoid lag. Additionally, we noted that YOLO worked at 24\nfps, and trials took approximately three minutes.\nC. Capability 3: Analysis. Blood smear inference\nOnce evidence has been documented, it should be ana-\nlyzed. For example, BPA on transfer stains such as smears\nand swipes could indicate where a criminal went or if a body\nwas moved, given that it could be hidden to delay or confuse\ninvestigators.\nFor initial exploration, a simplified dataset was created\nusing red carmine dye (cochineal- E120) on white paper.\nBody parts, continuity, and direction were varied. Continuity\ninvolved if the body part was constantly in contact with the\npaper (\"legato\", like a corpse being dragged) or set down at\nregular intervals (e.g., \"staccato\", like a shoe). Specifically,\nwe walked over paper with dyed shoes and crawled on our\nhands to mimic a criminal or victim fleeing; and, dragged\na hand, and touched down or dragged an amorphous blob\nusing a soaked tissue, to mimic a corpse being moved. In\ntotal, twenty samples were created: five samples of each were\nobtained, which were viewed four times each by a rotating\ndrone to point up, down, left, and right.\nA python program was written to calculate the perceived\ndirection of motion using a simplified approach involving\ncolor picking and image moments, given that the centroid\nshould be closer to the start position where there is more\nblood than the end position where little blood remains. First,\nred color was detected via hue (two ranges), then contours.\nThen a line was fitted to the centroids of contours. Finally, the\nposition of the centroid of centroids along the line was used\nto estimate the direction the bloody object was moved with\ndot product and an adjustment for the quadrant. A codebook\nvector/class label was selected as the closest to the resulting\nangle, and the error between prediction and ground truth\ncalculated.\nInitial trials showed various challenges, such as many\nsmall false contours using standard hue values for red, lines\nnot being fit for long \"legato\" smears with a single contour,\nas well as some errors estimating direction with multiple\ncontours, given that area was not taken into account. There-\nfore, the range for orange-red hue was removed, contour area\nwas checked, code was written to handle base cases (e.g.,\nif only one contour, which can happen if blood picking is\ngood and an object is being dragged, moments are used to\nfind a line based on the contour’s shape), and the midpoint\nof the detected blood pixels was used in place of the center\nof centroids to estimate direction.\nAs a result, accuracy was 80%, with an average error\nof 50.3 degrees. Hand staccato, shoe staccato, hand legato\nwere completely detected. However, the amorphous \"tissue\"\nstains seemed to be predicted at the level of random chance\n(50%). We noted that we could also not tell from observation\nourselves which side was the start or end point. As well, the\nreason the average error was large was because of the four\nerror cases, mostly again the \"tissue\" samples: three were\n180 degrees wrong (in other words, the \"line of motion\" is\nestimated correctly in 19 of 20 cases, so that the error would\nbe much less if the start and end are not required). Thus, the\nresults indicated that some cases can be easier to infer than\n\n\nothers.\nIV. DISCUSSION\nWe presented an overview of indoor crime scene analysis\n(CSA) by a nano drone. First, we identified theoretical gaps,\nbased on a \"big picture\" (rapid scoping) review. Second,\npractical challenges were explored by iteratively designing\nthree prototype capabilities with performances of 75%, 85%,\nand 80%. While nano drones do not seem capable yet of,\ne.g., opening windows, skillfully gathering evidence, and\nanalyzing complex blood patterns in the real world, we\nbelieve that our results suggest the feasibility of initial\nideation and testing within a laboratory setting. A video\nsummarizes this work. 20\nA. Limitations and Future Work\nOur approach is exploratory, involving rapid, low-fidelity\nprototyping; i.e., using red paint in place of blood and\nphotos of guns instead of real firearms. Furthermore, wireless\ntransmission of potentially sensitive data from drones to\nremote laptops for processing could present an security\nthreat, allowing criminals to intercept and possibly alter\ncommunications between drone and humans. As well, the\nscope of this paper did not allow us to explore all of the\nidentified gaps.\nFuture work should move farther from the laboratory into\nthe real world. Stronger drones will autonomously fly in\nmore accurate settings, onboard sensors like lidars will be\nused rather than markers for mapping, and real blood stain\npatterns will be analyzed. Processing should be preferably\ndone by computers on the drone. Additionally, we have\nstarted to explore a broader range of capabilities we identified\nas potential gaps, conducting some development on each of\nour ideas. These additional scenarios are visualized in Fig. 4,\nand in the accompanying video.\nIn conclusion, while much work remains to be done to\nmake this vision of nano drones helping to solve crimes a\nreality, we believe that the day is not too far off in which\nsuch technologies will be help preserve justice and impact\nsociety in a positive way.\nREFERENCES\n[1] M. Iqbal, H. Bardwell, and D. Hammond, “Estimating the global\neconomic cost of violence: Methodology improvement and estimate\nupdates,” Defence and Peace Economics, vol. 32, no. 4, pp. 403–426,\n2021.\n[2] S. Police, personal communication (project workshop 2024-10-30),\n2024.\n[3] E. F. Helbling, S. B. Fuller, and R. J. Wood, “Altitude estimation and\ncontrol of an insect-scale robot with an onboard proximity sensor,”\nRobotics Research: Volume 1, pp. 57–69, 2018.\n[4] S. Li, “Visual navigation and optimal control for autonomous drone\nracing,” Ph.D. dissertation, Delft University of Technology, 2020.\n[5] M.\nTironi,\n“Speculative\nprototyping,\nfrictions\nand\ncounter-\nparticipation:\nA\ncivic\nintervention\nwith\nhomeless\nindividuals,”\nDesign Studies, vol. 59, pp. 117–138, 2018.\n[6] R. Gehl and D. Plecas, “Strategic investigative response,” Introduction\nto Criminal Investigation: Processes, Practices and Thinking, 2017.\n20https://youtu.be/An8TIGr_NRA\nFig. 4.\n(a) Revealing hidden blood stains, (b) documenting evidence\nwithout disturbance via a blimp, (c) using a flashlight to operate in the\ndark, and (d) detecting hidden people with thermal or radar.\n[7] D. Falanga, E. Mueggler, M. Faessler, and D. Scaramuzza, “Aggres-\nsive quadrotor flight through narrow gaps with onboard sensing and\ncomputing using active vision,” in 2017 IEEE international conference\non robotics and automation (ICRA).\nIEEE, 2017, pp. 5774–5781.\n[8] J. Rietz, S. T. van Beeck Calkoen, N. Ferry, J. Schlüter, H. Wehner, K.-\nH. Schindlatz, T. Lackner, C. Von Hoermann, F. J. Conraths, J. Müller\net al., “Drone-based thermal imaging in the detection of wildlife\ncarcasses and disease management,” Transboundary and Emerging\nDiseases, vol. 2023, no. 1, p. 5517000, 2023.\n[9] Y.-J. Zheng, Y.-C. Du, H.-F. Ling, W.-G. Sheng, and S.-Y. Chen,\n“Evolutionary collaborative human-uav search for escaped criminals,”\nIEEE Transactions on Evolutionary Computation, vol. 24, no. 2, pp.\n217–231, 2019.\n[10] M. Perez, A. C. Kot, and A. Rocha, “Detection of real-world fights\nin surveillance videos,” in ICASSP 2019-2019 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp. 2662–2666.\n[11] C. Crick and B. Scassellati, “Controlling a robot with intention derived\nfrom motion,” Topics in Cognitive Science, vol. 2, no. 1, pp. 114–126,\n2010.\n[12] J. Gwozdz, N. Morin, and R. P. Mowris, “Enabling semi-autonomous\nmanipulation on irobot’s packbot,” Worcester Polytechnic Institute,\n2014.\n[13] A. G. Sanchez, N. Bernhart, and W. D. Smart, “Improving uv\ndisinfection of objects by a robot using human feedback,” in 2024\n33rd IEEE International Conference on Robot and Human Interactive\nCommunication (ROMAN).\nIEEE, 2024, pp. 1697–1704.\n[14] M. Neidhardt, S. Gerlach, R. Mieling, M.-H. Laves, T. Weiß,\nM. Gromniak, A. Fitzek, D. Möbius, I. Kniep, A. Ron et al., “Robotic\ntissue sampling for safe post-mortem biopsy in infectious corpses,”\nIEEE transactions on medical robotics and bionics, vol. 4, no. 1, pp.\n94–105, 2022.\n[15] Z. Jiang, B. Tong, X. Du, A. Alhammadi, and J. Zhou, “Beyond visual\nappearances: Privacy-sensitive objects identification via hybrid graph\nreasoning,” arXiv preprint arXiv:2406.12736, 2024.\n[16] M. Cooney, M. Shiomi, E. K. Duarte, and A. Vinel, “A broad view\non robot self-defense: Rapid scoping review and cultural comparison,”\nRobotics, vol. 12, no. 2, p. 43, 2023.\n[17] F. Augugliaro, S. Lupashin, M. Hamer, C. Male, M. Hehn, M. W.\nMueller, J. S. Willmann, F. Gramazio, M. Kohler, and R. D’Andrea,\n\n\n“The flight assembled architecture installation: Cooperative construc-\ntion with flying machines,” IEEE Control Systems Magazine, vol. 34,\nno. 4, pp. 46–64, 2014.\n[18] P. Urbanová, M. Jurda, T. Vojtíšek, and J. Krajsa, “Using drone-\nmounted cameras for on-site body documentation: 3d mapping and\nactive survey,” Forensic science international, vol. 281, pp. 52–62,\n2017.\n[19] A. Georgiou, P. Masters, S. Johnson, and L. Feetham, “Uav-assisted\nreal-time evidence detection in outdoor crime scene investigations,”\nJournal of forensic sciences, vol. 67, no. 3, pp. 1221–1232, 2022.\n[20] M. Cooney, L. M. W. Klasén, and F. Alonso-Fernandez, “Designing\nrobots to help women,” in 14th Scandinavian Conference on Artificial\nIntelligence (SCAI 2024): AI for a better society, June 10-11, 2024,\nJönköping, Sweden.\nIEEE, 2024, pp. 168–177.\n[21] A. Bucknell and T. Bassindale, “An investigation into the effect of\nsurveillance drones on textile evidence at crime scenes,” Science &\njustice, vol. 57, no. 5, pp. 373–375, 2017.\n[22] Z. Rymansaib, B. Thomas, A. A. Treloar, B. Metcalfe, P. Wilson,\nand A. Hunter, “A prototype autonomous robot for underwater crime\nscene investigation and emergency response,” Journal of field robotics,\nvol. 40, no. 5, pp. 983–1002, 2023.\n[23] P. Araujo, J. Fontinele, and L. Oliveira, “Multi-perspective object\ndetection for remote criminal analysis using drones,” IEEE Geoscience\nand Remote Sensing Letters, vol. 17, no. 7, pp. 1283–1286, 2019.\n[24] M. Butt, N. Glas, J. Monsuur, R. Stoop, and A. de Keijzer, “Appli-\ncation of yolov8 and detectron2 for bullet hole detection and score\ncalculation from shooting cards,” AI, vol. 5, no. 1, pp. 72–90, 2023.\n[25] T. Nandhini and K. Thinakaran, “Detection of crime scene objects\nusing deep learning techniques,” in 2023 International Conference on\nIntelligent Data Communication Technologies and Internet of Things\n(IDCIoT).\nIEEE, 2023, pp. 357–361.\n[26] U. Buck, S. Naether, B. Räss, C. Jackowski, and M. J. Thali, “Accident\nor homicide–virtual crime scene reconstruction using 3d methods,”\nForensic science international, vol. 225, no. 1-3, pp. 75–84, 2013.\n[27] M. Esposito, F. Sessa, G. Cocimano, P. Zuccarello, S. Roccuzzo, and\nM. Salerno, “Advances in technologies in crime scene investigation. di-\nagnostics 2023, 13, 3169. h ps,” doi. org/10.3390/diagnostics13203169\nAcademic Editor: Hiroshi Ikegaya Received, vol. 15, 2023.\n[28] G.\nGalanakis,\nX.\nZabulis,\nT.\nEvdaimon,\nS.-E.\nFikenscher,\nS. Allertseder, T. Tsikrika, and S. Vrochidis, “A study of 3d\ndigitisation\nmodalities\nfor\ncrime\nscene\ninvestigation,”\nForensic\nsciences, vol. 1, no. 2, pp. 56–85, 2021.\n[29] E. Liscio, P. Bozek, H. Guryn, and Q. Le, “Observations and 3d\nanalysis of controlled cast-off stains,” Journal of Forensic Sciences,\nvol. 65, no. 4, pp. 1128–1140, 2020.\n[30] E.-T. Lin, T. DeBat, and J. A. Speir, “A simulated crime scene footwear\nimpression database for teaching and research purposes,” Journal of\nForensic Sciences, vol. 67, no. 2, pp. 726–734, 2022.\n[31] Y. Wu, X. Wang, and T. Zhang, “Crime scene shoeprint retrieval using\nhybrid features and neighboring images,” Information, vol. 10, no. 2,\np. 45, 2019.\n[32] D. R. Bailey Kong, James Supancic and C. Fowlkes, “Fine-grained\nforensic matching,” in Proceedings of the British Machine Vision\nConference (BMVC), G. B. Tae-Kyun Kim, Stefanos Zafeiriou\nand K. Mikolajczyk, Eds.\nBMVA Press, September 2017, pp.\n188.1–188.12. [Online]. Available: https://bmva-archive.org.uk/bmvc/\n2017/papers/paper188/index.html\n[33] A. R. Weber and I. K. Lednev, “Crime clock–analytical studies\nfor approximating time since deposition of bloodstains,” Forensic\nChemistry, vol. 19, p. 100248, 2020.\n[34] T. Bergman, M. Klöden, J. Dreßler, and D. Labudde, “Automatic clas-\nsification of bloodstains with deep learning methods,” KI-Künstliche\nIntelligenz, vol. 36, no. 2, pp. 135–141, 2022.\n[35] A. Norton, G. Turk, B. Bacon, J. Gerth, and P. Sweeney, “Animation\nof fracture by physical modeling,” The visual computer, vol. 7, pp.\n210–219, 1991.\n[36] R. Matsumoto, H. Nakayama, T. Harada, and Y. Kuniyoshi, “Journalist\nrobot: Robot system making news articles from real world,” in 2007\nIEEE/RSJ International Conference on Intelligent Robots and Systems.\nIEEE, 2007, pp. 1234–1241.\n[37] C. F. Yeong, A. Melendez-Calderon, R. Gassert, and E. Burdet,\n“Reachman: a personal robot to train reaching and manipulation,” in\n2009 IEEE/RSJ International Conference on Intelligent Robots and\nSystems.\nIEEE, 2009, pp. 4080–4085.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21019v1.pdf",
    "total_pages": 7,
    "title": "Nano Drone-based Indoor Crime Scene Analysis",
    "authors": [
      "Martin Cooney",
      "Sivadinesh Ponrajan",
      "Fernando Alonso-Fernandez"
    ],
    "abstract": "Technologies such as robotics, Artificial Intelligence (AI), and Computer\nVision (CV) can be applied to crime scene analysis (CSA) to help protect lives,\nfacilitate justice, and deter crime, but an overview of the tasks that can be\nautomated has been lacking. Here we follow a speculate prototyping approach:\nFirst, the STAIR tool is used to rapidly review the literature and identify\ntasks that seem to have not received much attention, like accessing crime sites\nthrough a window, mapping/gathering evidence, and analyzing blood smears.\nSecondly, we present a prototype of a small drone that implements these three\ntasks with 75%, 85%, and 80% performance, to perform a minimal analysis of an\nindoor crime scene. Lessons learned are reported, toward guiding next work in\nthe area.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}