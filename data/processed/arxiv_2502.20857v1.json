{
  "id": "arxiv_2502.20857v1",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nJiTTER: Jigsaw Temporal Transformer for Event\nReconstruction\nfor Self-Supervised Sound Event Detection\nHyeonuk Nam, member, IEEE, Yong-Hwa Park, member, IEEE,\nAbstract—Sound event detection (SED) has significantly ben-\nefited from self-supervised learning (SSL) approaches, partic-\nularly masked audio transformer for SED (MAT-SED), which\nleverages masked block prediction to reconstruct missing audio\nsegments. However, while effective in capturing global dependen-\ncies, masked block prediction disrupts transient sound events and\nlacks explicit enforcement of temporal order, making it less suit-\nable for fine-grained event boundary detection. To address these\nlimitations, we propose JiTTER (Jigsaw Temporal Transformer for\nEvent Reconstruction), an SSL framework designed to enhance\ntemporal modeling in transformer-based SED. JiTTER intro-\nduces a hierarchical temporal shuffle reconstruction strategy,\nwhere audio sequences are randomly shuffled at both the block-\nlevel and frame-level, forcing the model to reconstruct the correct\ntemporal order. This pretraining objective encourages the model\nto learn both global event structures and fine-grained transient\ndetails, improving its ability to detect events with sharp onset-\noffset characteristics. Additionally, we incorporate noise injection\nduring block shuffle, providing a subtle perturbation mechanism\nthat further regularizes feature learning and enhances model\nrobustness. Experimental results on the DESED dataset demon-\nstrate that JiTTER outperforms MAT-SED, achieving a 5.89%\nimprovement in PSDS, highlighting the effectiveness of explicit\ntemporal reasoning in SSL-based SED. Our findings suggest\nthat structured temporal reconstruction tasks, rather than simple\nmasked prediction, offer a more effective pretraining paradigm\nfor sound event representation learning.\nIndex Terms—Sound event detection, self-supervised learning,\ntemporal modeling, transformer, hierarchical shuffle\nI. INTRODUCTION\nSound event detection (SED) is a fundamental task in\nmachine listening and plays a crucial role in auditory intel-\nligence, enabling applications in AI-driven perception, smart\nenvironments, and bioacoustic monitoring [1]–[6]. In addition\nto SED, various research efforts have focused on speech and\nspeaker recognition [7]–[16], sound event recognition [17]–\n[21], sound event localization and detection (SELD) [22]–\n[24], automated audio captioning (AAC) [25]–[27], few-shot\nbioacoustic detection [28], [29], human auditory perception\n[30], [31], highlighting the broad impact of auditory perception\nin real-world applications. Furthermore, recent advancements\nin sound synthesis [32]–[34] have explored the generative\nmodeling of sound events from textual descriptions, providing\nnew perspectives in sound representation learning.\nThis paper was produced by the IEEE Publication Technology Group. They\nare in Piscataway, NJ.\nManuscript received April 19, 2021; revised August 16, 2021.\nFig. 1.\nIllustration of the hierarchical temporal shuffle strategy in JiTTER,\ndesigned to improve temporal modeling for self-supervised SED. (a) Block-\nLevel Shuffle: The input audio sequence is divided into non-overlapping\nblocks, and a portion of these blocks (in darker grey) is randomly shuffled\nalong the time axis. This disrupts global event dependencies while preserving\nintra-block structures, forcing the model to reconstruct event sequences at\na higher level. (b) Frame-Level Shuffle: A subset of blocks is randomly\nselected, and within each selected block (in blue and orange), a fraction\nof frames (in darker blue and orange) is randomly shuffled. This introduces\nfine-grained perturbations while maintaining the overall event order, helping\nthe model learn transient sound characteristics. Together, these two levels of\nshuffle perturbations encourage the model to reconstruct the correct temporal\norder, improving both global event structure comprehension and fine-grained\nboundary detection in SED.\nSED aims to classify sound events while precisely lo-\ncalizing their temporal boundaries, making it a fundamental\nresearch area in auditory intelligence. Recent advances in\ndeep learning have significantly improved SED performance\n[35]–[42], driven by both architectural innovations and in-\nsights from auditory cognition [43]–[47]. Early approaches\nprimarily relied on convolutional neural networks (CNNs) to\nmodel spectral and temporal patterns in audio signals [48]–\n[50]. While CNN-based models demonstrated strong feature\nextraction capabilities, they struggled to capture long-range\ndependencies, which are essential for distinguishing between\nsequential and overlapping sound events. To overcome this\nlimitation, transformer-based architectures pretrained on large-\nscale datasets such as AudioSet have been explored, enabling\nmore effective temporal modeling [38]–[40], [51], [52]. These\ntransformer-based models leverage self-attention mechanisms\nto capture global contextual information, leading to significant\nimprovements in SED accuracy and robustness.\nAmong recent transformer-based approaches, the masked\n0000–0000/00$00.00 © 2025 IEEE\narXiv:2502.20857v1  [eess.AS]  28 Feb 2025\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\naudio transformer for SED (MAT-SED) introduced a self-\nsupervised learning (SSL) strategy on full transformer model\nknown as masked block prediction [41]. This method in-\nvolves masking temporal blocks of audio input and training\nthe model to reconstruct the missing features, improving its\nability to learn meaningful audio representations. MAT-SED\nemploys the patchout fast spectrogram transformer (PaSST)\nas an encoder network and a transformer with relative po-\nsitional encoding (RPE) as a context network, forming a\nfully transformer-based SED model [53]. The prototype-based\nmasked audio model (PMAM) further extended this approach\nby introducing Gaussian mixture model (GMM)-based proto-\ntypical representations as semantically rich frame-level pseudo\nlabels [42]. While these methods enhance the model’s ability\nto capture long-range dependencies and improve generaliza-\ntion, they exhibit two fundamental limitations: transient event\nloss and lacking explicit temporal order constraints.\nTo overcome these challenges, we propose JiTTER (Jig-\nsaw Temporal Transformer for Event Reconstruction), a self-\nsupervised learning framework that introduces hierarchical\ntemporal shuffle reconstruction to improve temporal modeling\nin SED. Unlike masked block prediction, which removes infor-\nmation, JiTTER shuffles audio segments at different temporal\nscales, forcing the model to learn to reconstruct their correct\norder. As shown in Figure 1, JiTTER applies two levels of\nperturbation:\n• Block-Level Shuffle: Randomly shuffles non-overlapping\nblocks of audio, disrupting global event structures while\npreserving local coherence.\n• Frame-Level Shuffle: Randomly selects blocks and shuf-\nfles a fraction of frames within each selected block,\nperturbing fine-grained temporal information.\nBy solving this hierarchical jigsaw-like reconstruction task,\nJiTTER explicitly enforces global event continuity and tran-\nsient event representation, enhancing the model’s ability to\nrecognize event boundaries while capturing long-range de-\npendencies. Additionally, we introduce noise injection during\nblock shuffle to provide a controlled level of information\ncorruption. Unlike masked block prediction, which completely\nremoves event cues, noise injection partially obscures informa-\ntion while retaining weak structural signals, promoting more\nrobust feature learning.\nOur experimental results on the Domestic Environment\nSound Event Detection (DESED) dataset demonstrate that\nJiTTER achieves a 5.89% improvement in PSDS over MAT-\nSED, highlighting the importance of explicitly modeling tem-\nporal dependencies in self-supervised pretraining for SED.\nMoreover, ablation studies confirm the optimal shuffle config-\nurations, suggesting that block-level shuffle primarily captures\nlong-range temporal dependencies, while frame-level shuffle\nenhances short-range temporal structures. Multitask learning,\nwhich combines both shuffle strategies, yields the highest\nperformance gains, further indicating that these perturbations\noperate at different temporal scales. This highlights the ben-\nefit of integrating multiple levels of temporal reordering to\nstrengthen event representations.\nThe key contributions of this paper are:\n1) We introduce JiTTER, a self-supervised pretraining\nframework that enhances SED performance by recon-\nstructing temporally shuffled sequences.\n2) We propose hierarchical temporal shuffle reconstruc-\ntion, combining block-level and frame-level shuffling\nto improve global event continuity and transient event\nrecognition.\n3) We incorporate noise injection during block shuffle to\nenhance feature learning while preserving weak struc-\ntural cues.\n4) Extensive ablation studies validate the effectiveness of\ndifferent shuffle configurations and multitask learning in\nimproving SED performance.\n5) Our experiments on the DESED dataset demonstrate that\nJiTTER outperforms conventional masked prediction\nmethods, achieving a 5.89% PSDS improvement over\nMAT-SED.\nThe official implementation code is available on GitHub1.\nII. RELATED WORKS\nA. Self-Supervised Learning for Sound Recognition\nSSL has been widely explored for audio tagging and\ngeneral-purpose audio representation learning. SSL enables\nmodels to learn meaningful features without requiring explicit\nhuman annotations, making it particularly valuable in domains\nwith limited labeled data. Several SSL approaches have been\nproposed to enhance audio tagging, focusing on capturing\nhigh-level acoustic representations.\nBYOL-A [54] applies contrastive learning to augmented\naudio segments, ensuring consistency across different transfor-\nmations. SSAST, an extension of the audio spectrogram trans-\nformer (AST), combines discriminative and generative masked\nspectrogram patch modeling to improve generalization across\ndiverse audio tasks [20], [55]. AudioMAE employs a masked\nautoencoder framework to reconstruct spectrogram patches\nusing a transformer encoder-decoder [56], while BEATs itera-\ntively optimizes an acoustic tokenizer alongside an SSL model\nto refine bidirectional audio representations [21].\nWhile these SSL methods have shown success in general-\npurpose audio learning and audio tagging, they are not specifi-\ncally designed for SED, which requires precise event boundary\nlocalization. Unlike audio tagging, where the objective is to\nclassify the presence of sound categories, SED demands ac-\ncurate onset-offset detection and a structured understanding of\nsequential dependencies. Many existing SSL models prioritize\ngeneral acoustic feature extraction but lack mechanisms to\ncapture fine-grained temporal structures necessary for event-\nlevel modeling.\nTo address these challenges, SSL techniques specifically\ndesigned for SED have been proposed. MAT-SED introduced\nmasked block prediction, where randomly selected temporal\nblocks in an audio sequence are masked, and the model is\ntrained to reconstruct the missing features [41]. This approach\nenhances the model’s ability to capture global contextual de-\npendencies and improves its robustness to missing information.\n1https://github.com/frednam93/JiTTER-SED\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nThe prototype-based masked audio model (PMAM) further\nextended this technique by incorporating Gaussian mixture\nmodel (GMM)-based pseudo labels, providing additional se-\nmantic guidance during reconstruction [42]. These methods\nhave demonstrated state-of-the-art performance in modeling\nlong-range dependencies for self-supervised SED.\nB. Shuffle-Based Learning in Representation Learning\nSegment reordering tasks have been widely explored in\nself-supervised representation learning, particularly in vision\nand video processing. Jigsaw pretext tasks involve permuting\nimage patches and training models to recover the correct order,\nleading to stronger spatial feature learning [57], [58]. In the\nvideo domain, Shuffle and Learn demonstrated that training\nmodels to predict the correct sequence of shuffled frames\nenhances motion feature learning [59].\nIn audio processing, shuffle-based learning has been largely\nunexplored, with existing self-supervised approaches like\nTERA and BYOL-A focusing on temporal coherence rather\nthan explicit sequence reconstruction. TERA promotes tempo-\nral consistency by reconstructing spectrograms altered along\ntime, frequency, and magnitude axes, ensuring robustness to\ndistortions but without enforcing strict sequential dependen-\ncies [60]. BYOL-A enhances temporal coherence through\nrandom resized cropping (RRC), which approximates pitch\nshifting and time stretching by randomly resizing and crop-\nping spectrogram segments. This forces the model to learn\nrepresentations that remain stable across variations in time and\nfrequency [54]. However, neither method explicitly enforces\nevent ordering, which is critical for SED. In contrast, JiTTER\nextends shuffle-based learning to SED by introducing hier-\narchical temporal shuffle reconstruction, where models must\nrecover the correct sequence after structured perturbations\nat both block and frame levels. Unlike contrastive learning,\nJiTTER does not rely on negative pairs but directly optimizes\nfor sequence restoration, making it more suitable for fine-\ngrained temporal reasoning in SED.\nC. Temporal Modeling for Sound Event Detection\nTemporal modeling is critical in SED, where accurately\ncapturing event onsets and offsets is essential. Transformer-\nbased architectures have demonstrated strong capabilities in\nmodeling long-range dependencies. Patchout fast spectrogram\ntransformer (PaSST) [53] improves computational efficiency\nwhile preserving contextual modeling, making it a strong\nbackbone for SED. More recently, studies have explored\nrelative positional encoding (RPE) [61], which helps capture\nfine-grained temporal relationships.\nA key challenge in SED is distinguishing overlapping\nevents from sequentially occurring ones. Previous works\nhave attempted to improve event boundary detection through\nclassification-based segmentation [62] and data augmentation\ntechniques [7], [37], but these approaches lack a structured\nlearning objective for temporal order recovery. JiTTER ad-\ndresses this gap by enforcing explicit temporal reasoning\nthrough hierarchical shuffle-based reconstruction, leading to\nimproved event boundary detection and reduced reliance on\npost-processing techniques.\nIII. METHODOLOGY\nJiTTER (Jigsaw Temporal Transformer for Event Recon-\nstruction) is a self-supervised learning framework designed\nto improve temporal representation learning for SED. Unlike\nmasked prediction, JiTTER introduces a hierarchical temporal\nshuffle reconstruction strategy, challenging the model to re-\ncover the correct sequence of shuffled segments. This enhances\nthe model’s ability to capture both local transient structures\nand long-term event dependencies, leading to improved sound\nevent boundary detection.\nA. Limitations of Masked Block Prediction\nMasked block prediction, as employed in MAT-SED, re-\nmoves entire audio segments, leading to the loss of transient\nsound events such as footsteps, door slams, and alarms, which\nare brief and temporally localized [41]. Since these masked\nsegments are absent during training, the model is forced to\nreconstruct them solely from surrounding context. This often\nresults in inaccurate reconstructions, as the model may over-\nrely on background noise or unrelated acoustic cues instead of\nlearning to capture fine-grained transient event structures. The\nabsence of direct supervision on these short-duration events\nreduces the model’s ability to accurately detect event onsets\nand offsets, which is crucial for SED.\nAdditionally, masked block prediction does not explicitly\nenforce temporal order learning, making it less effective in\ndistinguishing between overlapping and sequential events.\nTransformers inherently model attention-based relationships,\nbut without explicit temporal constraints, they may struggle to\nrecover the correct event sequence when multiple events occur\nin succession. This limitation weakens the model’s ability\nto differentiate between events that share similar spectral\ncharacteristics but differ in their temporal positioning. As a\nresult, event boundaries may become blurred, reducing the\nprecision of SED predictions.\nTo address these issues, JiTTER introduces hierarchical\ntemporal shuffle reconstruction, which perturbs audio se-\nquences at both the block and frame levels while preserving\nall content. Unlike masked block prediction, which removes\ninformation, JiTTER retains all temporal data but disrupts its\norder, forcing the model to reconstruct the correct sequence.\nThis structured perturbation explicitly enforces temporal co-\nherence, improving two critical aspects of SED: event bound-\nary precision and transient event recognition.\nBy preserving all acoustic information while shuffling its\norder, JiTTER eliminates the risk of interpolation artifacts and\nensures that the model learns event-level temporal dependen-\ncies rather than relying on surrounding context for reconstruc-\ntion. This approach enhances both global event continuity and\nfine-grained transient modeling, leading to more accurate SED\nperformance compared to masked block prediction.\nB. Hierarchical Temporal Shuffle Reconstruction\nJiTTER consists of two levels of temporal perturbation:\nblock-level shuffle and frame-level shuffle, which are applied\nin parallel within a single forward pass. These perturbations\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\noperate at different temporal scales: block-level shuffle dis-\nrupts global event structures, while frame-level shuffle intro-\nduces local variations. The objective is to learn robust repre-\nsentations by solving a temporal jigsaw puzzle that requires\nthe model to infer both long-range and fine-grained event\ndependencies.\n1) Block-Level Shuffle: Block-level shuffle (Figure 1a) par-\ntitions an audio sequence into non-overlapping blocks and\nrandomly shuffles a portion (pb), keeping the first and last\nblocks fixed as anchors.\nGiven an input audio sequence X of length T, it is divided\ninto B blocks:\nX = {B1, B2, ..., BB}\n(1)\nwhere each block Bi consists of multiple consecutive frames.\nOne example of a randomly shuffled sequence is:\n˜Xb = {B1, B2, BB−3, B4, ..., B3, BB}\n(2)\nwhere\n˜Xb represents a sequence perturbed by block-level\nshuffling. The model is pretrained to recover the correct order\nX from\n˜Xb, which encourages the transformer to capture\nglobal contextual dependencies.\nInspired by masked prediction, we introduce Gaussian noise\ninjection into shuffled blocks, partially obscuring information\nwithout full masking [41], [42]. The goal is to obscure infor-\nmation slightly rather than removing it completely, allowing\nthe model to retain weak structural cues while still reinforcing\nits ability to restore the original sequence.\nGiven a shuffled block Bi, noise injection is performed as:\nBnoise\ni\n= Bi + λNi,\nNi ∼N(0, I)\n(3)\nwhere Ni is Gaussian noise sampled from a normal distribu-\ntion with zero mean and unit variance. Here, Ni has the same\ndimensions as Bi, ensuring that noise is applied consistently\nacross the entire block. The scaling factor λ controls the\nintensity of perturbation, and we set λ = 0.1 to introduce\nsubtle distortion while preserving core temporal structures.\nThis additional perturbation encourages the model to be more\nrobust to small variations in real-world recordings while still\nmaintaining the integrity of the shuffled reconstruction task.\n2) Frame-Level Shuffle: As shown in Figure 1 (b), frame-\nlevel shuffling randomly selects a subset of blocks and applies\nintra-block frame shuffling. The hyperparameters pfb and pff\ndenote the proportion of blocks undergoing frame shuffling\nand the proportion of frames shuffled within each selected\nblock, respectively.\nFor a chosen block Bi = {fi1, fi2, ..., fiF }, a portion of its\nframes is randomly permuted as follows:\n˜Bi = {fi3, fi2, fiF , fi4, ..., fi1}\n(4)\nwhere ˜Bi represents an example of a shuffled block. The\nperturbed sequence ˜Xf is then reconstructed by combining\nall modified blocks:\n˜Xf = { ˜B1, B2, ..., ˜Bm, Bm+1, ..., BB}\n(5)\nwhere m represents the number of shuffled blocks and ˜Xf rep-\nresents an example of a frame-level shuffled sequence. Unlike\nblock shuffle, this preserves global order while introducing\nlocal perturbations, enhancing fine-grained temporal modeling.\nC. Training Objective\nJiTTER is trained using a reconstruction loss that explicitly\nenforces temporal structure learning by requiring the model\nto restore the correct event sequence from perturbed versions\n˜Xb and ˜Xf. Unlike masked block prediction, which removes\ninformation entirely, JiTTER preserves all temporal data but\ndisrupts their order, making reconstruction a more structured\nlearning objective.\nThe reconstruction loss is formulated as follows:\nLrec( ˜X, X) =\nT\nX\nt=1\n||Fθ( ˜X)(t) −X(t)||2\n(6)\nwhere X(t) represents the t-th frame of the original sequence,\nand Fθ is the transformer network parameterized by θ, which\naims to reconstruct X from its shuffled counterpart ˜X. The\nmodel is optimized to recover both global event continuity\nand local transient details, leading to stronger representations\nfor SED.\nJiTTER’s overall training objective is defined as:\nLJiTTER = Lrec( ˜Xb, X) + Lrec( ˜Xf, X)\n(7)\nwhere ˜Xb and ˜Xf correspond to block-shuffled and frame-\nshuffled sequences, respectively.\nThis structured loss function ensures that:\n• Block-level shuffle encourages learning long-range tem-\nporal dependencies by forcing the model to recover the\ncorrect high-level sequence order.\n• Frame-level shuffle improves fine-grained event local-\nization by requiring the model to reconstruct disrupted\ntransient event patterns.\nMultitask learning, which combines both perturbations, al-\nlows the model to integrate global and local event structures\neffectively, leading to more robust temporal reasoning in SED.\nIV. EXPERIMENTAL SETUPS\nThis section describes the dataset, input feature extraction\nprocess, model architecture, training procedure, and evaluation\nmetrics used in our experiments.\nA. Dataset\nWe train, validate, and evaluate our models using the Do-\nmestic Environment Sound Event Detection (DESED) dataset\n[2], a widely used benchmark for domestic sound event\ndetection. The dataset consists of 10-second-long audio clips\nrecorded at a sampling rate of 16 kHz. It includes both\nsynthetic and real recordings that simulate common household\nacoustic environments, covering ten sound event classes such\nas alarms, speech, running water, and vacuum cleaners.\nThe DESED dataset is divided into four subsets:\n• Strongly labeled synthetic data: Includes precise onset\nand offset annotations for each event.\n• Weakly labeled real data: Indicates only the presence of\nevents in each clip without specifying time boundaries.\n• Unlabeled real data: Used in a self-supervised and semi-\nsupervised learning setup to improve generalization.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\n• Strongly labeled real validation data: Reserved for model\nevaluation.\nTo ensure a fair comparison, we follow the standard Detection\nand Classification of Acoustic Scenes and Events (DCASE)\n2021 Task 4 data partitioning scheme [2]. The strongly labeled\nsynthetic data is used for supervised learning, while weakly\nlabeled and unlabeled data are leveraged in semi-supervised\ntraining. We also include the real strongly labeled training data\nfrom the DCASE 2022 Task 4 Challenge.\nB. Input Feature Extraction\nRaw audio waveforms are first normalized to a maximum\nabsolute value of one to standardize input levels. We then\nextract log-mel spectrograms as input features using a short-\ntime Fourier transform (STFT) with FFT size of 1024, Hop\nlength of 320 (corresponding to 20 ms at 16 kHz), Hanning\nwindowing function and 128 mel frequency bins. The resulting\nlog-mel spectrograms serve as input to JiTTER’s transformer-\nbased architecture.\nC. Data Augmentation\nTo enhance the model’s robustness against environmental\nvariability, we apply a diverse set of augmentation techniques\nfor fine-tuning stages:\n• Frame shift [2]: Shifts the audio features by a small\nnumber of frames to introduce temporal variations.\n• Mixup [63]: Linearly interpolates pairs of spectrograms\nand labels, encouraging smoother decision boundaries.\n• Time masking [7]: Randomly masks sections of the time\naxis, simulating missing event cues.\n• FilterAugment [43]: Randomly reweights frequency re-\ngions to simulate different acoustic environments.\n• Frequency Distortion [41]: Perturbs frequency compo-\nnents to improve spectral robustness.\nWe apply Mixup to both strongly and weakly labeled datasets,\nwhile time masking is applied jointly to the input spectrogram\nand its corresponding labels to maintain consistency.\nD. Model Architecture\nJiTTER extends the MAT-SED framework by replacing\nmasked block prediction with hierarchical temporal shuffle\nreconstruction. The model consists of:\n• A patchout fast spectrogram transformer (PaSST) en-\ncoder, which extracts rich spectral-temporal representa-\ntions [53].\n• A Transformer-based context network with relative po-\nsitional encoding (RPE), designed to capture long-range\ntemporal dependencies [61].\n• A fully connected (FC) classification head, which predicts\nframe-wise event occurrences.\nUnlike conventional masked prediction, JiTTER applies block-\nlevel and frame-level shuffle perturbations before input se-\nquences enter the transformer model. The network is then\npretrained to reconstruct the original sequence, enforcing\ntemporal structure learning.\nE. Training Procedure\nJiTTER follows a three-stage training paradigm to progres-\nsively refine temporal representations for SED:\n1) Pretraining: The context network, consisting of the\ntransformer with relative positional encoding (RPE), is\ntrained using the hierarchical temporal shuffle strategy\nto reconstruct shuffled sequences. During this phase, the\ncontext network is updated, while the encoder network\n(PaSST) remains frozen. This stage runs for 6000 steps.\n2) Feature adaptation: The pretrained transformer re-\nmains fixed, while the SED and AT heads are trained\nseparately using the SED objective. This step allows\nthe classification layers to adapt to structured temporal\nrepresentations. Training runs for an additional 6000\nsteps.\n3) Fine-tuning: The entire model, including the trans-\nformer, PaSST encoder, SED, and AT heads, is jointly\noptimized in an end-to-end manner with the SED objec-\ntive to refine overall event detection performance. This\nfinal stage runs for another 6000 steps.\nAll training is conducted on NVIDIA RTX 3090 GPUs using\nthe AdamW optimizer with a weight decay of 1 × 10−4.\nF. Loss Function\nJiTTER is trained using a multi-stage loss function that\ncombines self-supervised reconstruction loss for pretraining\nand semi-supervised classification loss for SED fine-tuning.\n1) Pretraining Loss: During the pretraining stage, JiTTER\nis optimized with a reconstruction loss that encourages the\nmodel to infer the original sequence from temporally shuffled\nversions. The objective is formulated as:\nLrec( ˜X, X) =\nT\nX\nt=1\n||Fθ( ˜X)(t) −X(t)||2\n(8)\nwhere X(t) is the t-th frame of the original sequence, and\nFθ( ˜X)(t) is the model’s reconstructed prediction. JiTTER\noptimizes both block-shuffled and frame-shuffled sequences:\nLJiTTER = Lrec( ˜Xb, X) + Lrec( ˜Xf, X)\n(9)\nwhere\n˜Xb and\n˜Xf denote the block-level and frame-level\nshuffled sequences, respectively.\n2) SED Fine-Tuning Loss: After pretraining, JiTTER is\noptimized using a supervised classification loss for polyphonic\nSED. Following standard SED frameworks [41], [42], the loss\nfunction consists of:\n• Strong classification loss (Ls): Applied to strongly la-\nbeled data using binary cross-entropy (BCE).\n• Weak classification loss (Lw): Applied to weakly labeled\ndata using BCE.\n• Consistency loss (Lc): Ensures consistency between the\nstudent and teacher models using mean square error\n(MSE).\nThe overall fine-tuning loss is defined as:\nLSED = B(SP , ls) + wW B(WP , lw) + wCLcons\n(10)\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nwhere SP , WP are the strong and weak predictions from the\nstudent model, ls, lw are the corresponding ground-truth labels,\nand wW and wC are weighting factors for weak classification\nand consistency losses.\nThe consistency loss Lcons is given by:\nLcons = M(SP , sg(ST\nP )) + M(WP , sg(W T\nP ))\n(11)\nwhere sg(·) denotes a stop-gradient operation, and ST\nP , W T\nP\nare the strong and weak predictions from the teacher model.\nThis multi-stage optimization strategy ensures that JiTTER\nfirst learns robust temporal representations via self-supervised\npretraining, which are later refined with supervised event\nclassification.\nG. Post-Processing\nTo refine event predictions, we apply a post-processing\npipeline consisting of two steps. First, weak prediction mask-\ning is applied, where strong predictions are retained only if\ntheir confidence surpasses the corresponding weak predictions\n[37]. This ensures that detected events are supported by\nglobal event presence information. Next, median filtering is\nused to smooth predictions and reduce spurious detections.\nSpecifically, we apply median filters with a window size of\n5 for transient sound events and a window size of 20 for\nstationary sound events. This strategy helps to suppress short-\nduration false positives for transient events while preventing\nabrupt fluctuations in stationary event predictions.\nH. Evaluation Metrics\nTo evaluate SED performance, the polyphonic sound detec-\ntion score (PSDS) was used [5]. PSDS is a metric specifically\ndesigned for evaluating polyphonic SED systems, addressing\nthe limitations of conventional collar-based event F-scores and\nevent error rates by incorporating intersection-based event de-\ntection. In addition, unlike traditional metrics, PSDS considers\nthe full polyphonic receiver operating characteristic (ROC)\ncurve, summarizing system performance across multiple oper-\nating points. This allows for a more robust and comprehensive\nassessment of an SED system’s capabilities, making it less\nsensitive to subjective annotation errors and more suitable for\nreal-world applications. Additionally, PSDS provides better\ninsights into classification stability across different sound\nclasses and dataset biases.\nFor the DCASE Challenge 2021, 2022, and 2023 Task 4\nbenchmarks [2], two types of PSDS were employed: PSDS1\nand PSDS2. Among them, PSDS2 is more aligned with the\naudio tagging task rather than SED, as it emphasizes the\nclassification of event presence rather than precise temporal\nlocalization [37], [64]. Therefore, we report only PSDS1 in\nthis work, as it directly measures an SED system’s ability to\ndetect sound events with accurate onset and offset timings.\nPSDS values reported in the tables represent the best scores\nobtained from six independent training runs, ensuring that the\nevaluation reflects a reliable and well-optimized performance\nestimate of the proposed model.\nTABLE I\nABLATION STUDY ON JITTER USING BLOCK-LEVEL SHUFFLE,\nFRAME-LEVEL SHUFFLE, AND MULTITASK LEARNING. PSDS VALUES ARE\nAVERAGED OVER SIX INDEPENDENT TRAINING RUNS.\nMethod\npb\npfb\npff\nPSDS (↑)\nMAT-SED (Baseline)\n-\n-\n-\n0.543\nBlock-Level Shuffle\nJiTTER (Block Shuffle)\n0.25\n-\n-\n0.566\nJiTTER (Block Shuffle)\n0.5\n-\n-\n0.569\nJiTTER (Block Shuffle)\n0.75\n-\n-\n0.570\nFrame-Level Shuffle\nJiTTER (Frame Shuffle)\n-\n0.25\n0.25\n0.560\nJiTTER (Frame Shuffle)\n-\n0.25\n0.5\n0.563\nJiTTER (Frame Shuffle)\n-\n0.25\n0.75\n0.563\nJiTTER (Frame Shuffle)\n-\n0.5\n0.25\n0.564\nJiTTER (Frame Shuffle)\n-\n0.5\n0.5\n0.562\nJiTTER (Frame Shuffle)\n-\n0.5\n0.75\n0.563\nJiTTER (Frame Shuffle)\n-\n0.75\n0.25\n0.563\nJiTTER (Frame Shuffle)\n-\n0.75\n0.5\n0.562\nJiTTER (Frame Shuffle)\n-\n0.75\n0.75\n0.562\nMultitask Learning (Block + Frame-Level Shuffle)\nJiTTER (Multitask) - Best\n0.75\n0.5\n0.25\n0.574\nJiTTER (Multitask)\n0.5\n0.5\n0.25\n0.570\nJiTTER (Multitask)\n0.75\n0.25\n0.25\n0.567\nJiTTER (Multitask)\n0.75\n0.75\n0.25\n0.571\nJiTTER (Multitask)\n0.75\n0.5\n0.5\n0.573\nV. RESULTS AND DISCUSSION\nTo evaluate the effectiveness of the proposed JiTTER frame-\nwork, we compare it with the baseline MAT-SED model and\nconduct an ablation study on block-level shuffle, frame-level\nshuffle, and multitask learning. Additionally, we analyze the\nimpact of block flipping and noise injection, then compare the\nbest JiTTER with MAT-SED under controlled experimental\nconditions.\nA. Block-Level Shuffle\nBlock-level shuffle aims to disrupt global temporal de-\npendencies by perturbing the order of event segments while\npreserving all acoustic information. Unlike masked block\nprediction, which removes entire segments and requires the\nmodel to hallucinate missing content, block shuffle enforces\nexplicit temporal reasoning by requiring the model to recover\nthe correct event sequence rather than merely interpolating\ngaps. To evaluate its impact, we vary the block shuffle rate pb\nwhile keeping frame-level shuffle disabled (pfb = 0, pff = 0).\nEach audio sequence consists of 100 time frames, which are\npartitioned into 20 non-overlapping blocks of size 5. The\nresults are presented in Table I.\nBlock shuffle consistently improves PSDS over the baseline,\nwith the highest improvement of 4.97% at pb = 0.75. This\nsuggests that reconstructing shuffled event blocks forces the\nmodel to capture long-range temporal dependencies, which\nplay a crucial role in transformer pretraining. The performance\ndifference across different shuffle rates is relatively small,\nindicating that block shuffling provides stable improvements.\nHowever, excessive shuffling (pb = 1.0) leads to performance\ndegradation, likely due to the introduction of excessive tem-\nporal disorder, making event reconstruction overly difficult.\nOne limitation of masked block prediction in MAT-SED is\nthat it removes entire blocks, forcing the model to hallucinate\nmissing content based on surrounding context [41]. This can\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nlead to unintended interpolation artifacts, where the model\nlearns to reconstruct missing regions by exploiting statistical\ncorrelations rather than truly modeling event sequences. In\ncontrast, JiTTER’s block shuffle retains all acoustic infor-\nmation while disrupting event ordering, requiring the model\nto reason explicitly about temporal coherence. This explicit\nenforcement of sequence reconstruction provides a more ef-\nfective pretraining signal for SED.\nB. Frame-Level Shuffle\nFrame-level shuffle introduces fine-grained temporal pertur-\nbations, allowing the model to learn local transient depen-\ndencies. Unlike block-level shuffle, which primarily alters the\nevent sequence at a coarse level, frame shuffle operates within\nindividual event segments, perturbing intra-event structures\nwhile preserving the overall event order. This enables the\nmodel to refine event localization and better capture transient\nevent characteristics. To analyze its effect, we vary the frame\nblock selection rate (pfb) and the fraction of shuffled frames\nper block (pff), while keeping block-level shuffle disabled\n(pb = 0). Each audio sequence consists of 100 time frames,\nwhich are divided into 5 non-overlapping blocks of size 20.\nThe results are summarized in Table I.\nFrame shuffle improves performance over the baseline,\nthough its impact is less significant compared to block\nshuffling. The best configuration (pfb = 0.5, pff = 0.25)\nleads to a 3.8% improvement. This suggests that while local\ntemporal order is important, it has a smaller effect than\nglobal event structure. The results indicate that SED models\nbenefit more from capturing event boundary structures at a\ncoarser granularity (block level) rather than relying solely on\nfine-grained perturbations (frame level). Nevertheless, frame\nshuffle enhances event localization by introducing intra-block\nvariability, which aids in transient event detection.\nUnlike block shuffle, which restructures event sequences,\nframe shuffle focuses on intra-event variability by perturbing\nlocal structures without affecting the sequence at a higher\nlevel. This distinction suggests that block shuffle plays a more\ndominant role in shaping global event representations, whereas\nframe shuffle refines detailed event characteristics.\nC. Multitask Learning of Block and Frame-Level Shuffle\nWe evaluate multitask learning, where block and frame-\nlevel shuffle are applied in separate training iterations. The\nresults are presented in Table I. We experimented with the\ncombination of the best block shuffle setting and the best frame\nsetting, along with additional variations.\nMultitask learning yields the highest PSDS of 0.574, im-\nproving by 5.71% over the baseline. This suggests that\ncombining global (block-level) and local (frame-level) per-\nturbations enables more effective pretraining, as the model\nlearns both event-level continuity and finer transient vari-\nations. Furthermore, the best-performing model arises not\nfrom simply stacking perturbations, but from a combination\nof optimal block0lvel and frame-level configurations. These\nfindings highlight the importance of designing task-specific\nTABLE II\nEFFECT OF BLOCK FLIP AUGMENTATION IN JITTER MULTITASK\nLEARNING.\nMethod\nflip rate\nPSDS (↑)\nJiTTER (Multitask)\n-\n0.574\nJiTTER (Multitask + Flip)\n0.25\n0.572\nJiTTER (Multitask + Flip)\n0.5\n0.570\nJiTTER (Multitask + Flip)\n0.75\n0.571\npretraining objectives rather than arbitrarily applying multiple\naugmentations.\nRather than relying solely on contextual interpolation, as\nseen in MAT-SED’s masked block prediction, JiTTER explic-\nitly enforces temporal order reconstruction by reconstructing\nshuffled sequences. This prevents the model from over-relying\non surrounding context and instead optimizes it to capture both\nfine-grained temporal order and global event structures. The\nresults demonstrate that integrating perturbations at multiple\ntemporal scales improves the model’s ability to generalize\nacross various sound event patterns. This highlights the advan-\ntage of explicitly modeling hierarchical temporal dependencies\nin SED pretraining.\nD. Block Flip in Block-level Shuffle\nTo further assess the impact of temporal transformations,\nwe apply block flipping to the best multitask configuration. In\nthis augmentation, shuffled blocks are reversed along the time\naxis with a probability defined by the hyperparameter flip rate.\nThe results are presented in Table II. Contrary to expectations,\nblock flipping does not improve performance and leads to a\nslight drop in PSDS. This suggests that excessive disruption\nof transient structures reduces the model’s ability to capture\nglobal temporal dependencies. One possible explanation is that\nblock flipping distorts natural event progression by reversing\nlocalized patterns within audio sequences.\nFrom an auditory perception perspective, humans rarely\nencounter temporally inverted sound patterns in real-world\nscenarios. As a result, models trained with block flipping may\nlearn non-representative patterns that do not generalize well to\nnatural sound event structures. Additionally, phase incoherence\nintroduced by time-reversed blocks may disrupt the model’s\nability to capture spectral-temporal relationships, which are\ncrucial for precise event boundary detection.\nMoreover, the pretext task of temporal reconstruction be-\ncomes significantly more challenging when flipped blocks\nare introduced. Unlike shuffled blocks, where the original\norder can be inferred through contextual reasoning, flipped\nblocks fundamentally alter the spectral envelope and transient\nstructures of events, making it difficult for the model to\nlearn meaningful representations. Similar findings have been\nobserved in prior works on time-reversed signal processing,\nwhere artificially reversing signals degraded classification per-\nformance in audio and speech tasks.\nE. Noise Injection in Block-level Shuffle\nTo explore a softer form of perturbation compared to block\nflipping, we investigate the impact of injecting Gaussian noise\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nTABLE III\nEFFECT OF NOISE INJECTION ON JITTER MULTITASK LEARNING.\nMethod\nNoise Scale λ\nPSDS (↑)\nJiTTER (Multitask)\n-\n0.574\nJiTTER (Multitask + Noise)\n0.05\n0.570\nJiTTER (Multitask + Noise)\n0.1\n0.575\nJiTTER (Multitask + Noise)\n0.2\n0.567\nJiTTER (Multitask + Noise)\n0.4\n0.569\ninto shuffled blocks. Unlike masked prediction tasks, which\nremove entire regions of input audio, noise injection slightly\nobscures information while preserving weak structural cues.\nThis allows the model to enhance feature robustness without\nfully disrupting temporal reconstruction.\nWe apply Gaussian noise sampled from N(0, I) to shuffled\nblocks, scaling its magnitude by λ. As shown in Table III,\nintroducing mild noise (λ = 0.1) improves PSDS to 0.575,\nsurpassing the baseline multitask configuration (0.574). How-\never, increasing λ beyond this threshold leads to performance\ndegradation, with PSDS dropping to 0.567 at λ = 0.2 and\n0.569 at λ = 0.4. These results suggest that a small degree of\ninformation corruption can regularize training and encourage\nbetter generalization. When applied in moderation, noise injec-\ntion acts as a form of stochastic feature smoothing, preventing\nthe model from overfitting to minute acoustic details that may\nnot generalize well.\nThis finding aligns with prior work in robust speech recog-\nnition and audio classification, where slight perturbations in\nthe feature space have been shown to enhance generalization\nby encouraging invariance to minor spectral variations. The\ntheoretical basis for this effect lies in stochastic regularization,\nwhere controlled noise injection forces the model to rely\non robust acoustic features rather than overfitting to specific\nwaveform characteristics.\nHowever, excessive noise injection disrupts key acoustic\npatterns, making it harder for the model to reconstruct mean-\ningful temporal structures. As the noise magnitude increases,\nthe signal-to-noise ratio (SNR) decreases, and event bound-\naries become harder to distinguish, leading to a decline in\ndetection accuracy. This phenomenon aligns with perspectives\nin information theory, where too much noise obscures discrim-\ninative features necessary for accurate classification, ultimately\ndegrading performance.\nThe results from both experiments indicate that block flip-\nping and noise injection have fundamentally different effects\non JiTTER’s pretraining dynamics. Block flipping disrupts the\nfundamental auditory structure by inverting event sequences,\nmaking it difficult for the model to reconstruct coherent time-\naligned representations. This forces the model to develop se-\nquence reordering capabilities but can also introduce unnatural\nphase distortions that degrade generalization. Noise injection,\nin contrast, preserves event order while slightly perturbing\nfeature representations. This acts as a form of robustness\nenhancement rather than a restructuring task, promoting gen-\neralization without forcing the model to learn unrealistic event\nsequences.\nThese findings highlight an important consideration for\ndesigning self-supervised pretraining objectives: perturbations\nTABLE IV\nCOMPARISON OF MAXIMUM PSDS SCORES ACROSS TRAINING RUNS.\nMethod\nMax PSDS (↑)\nMAT-SED [41]\n0.587\nMAT-SED (Reproduced)\n0.552\nJiTTER (Best)\n0.584\nshould balance informative learning signals with the risk of\nintroducing non-naturalistic distortions. Our study suggests\nthat structured shuffling, such as JiTTER’s hierarchical shuffle\nstrategy, is a more effective SSL task than aggressive augmen-\ntations like full inversion or high-magnitude noise injection.\nWhile noise injection provides a useful regularization mech-\nanism, its impact remains secondary to structured temporal\nperturbations like block and frame-level shuffle. Future re-\nsearch could explore adaptive noise scheduling strategies or\ncontext-aware noise injection to enhance its benefits while\nminimizing potential degradation.\nF. Comparison with MAT-SED with Maximum PSDS\nTo ensure a fair comparison, we report the maximum\nPSDS scores measured across multiple training runs in Ta-\nble IV. While JiTTER achieves a maximum PSDS of 0.584,\nslightly below the originally reported MAT-SED result (0.587),\nit outperforms our reproduced MAT-SED baseline (0.552),\ndemonstrating the robustness of our approach. The discrepancy\nbetween our reproduced MAT-SED score and the originally\nreported result can be attributed to three main factors:\n1) Differences in training infrastructure, such as GPU mod-\nels, software versions, and CUDA configurations.\n2) Heuristically optimized hyperparameters may not gen-\neralize consistently across different environments.\n3) Random variations in the generation of the synthetic\nstrongly labeled DESED dataset used for training.\nThese factors underscore the challenges of reproducing\nresults in deep learning and emphasize the need for fair\ncomparisons under controlled settings.\nVI. CONCLUSION\nIn this work, we introduced JiTTER (Jigsaw Tempo-\nral Transformer for Event Reconstruction), a self-supervised\nlearning framework designed to enhance temporal represen-\ntation learning for SED. Unlike masked block prediction,\nwhich removes entire segments and forces interpolation, JiT-\nTER preserves all information while enforcing explicit event\nreordering through hierarchical temporal shuffle reconstruc-\ntion, enabling the model to capture both global event struc-\ntures and fine-grained transient details. Experiments on the\nDESED dataset show that JiTTER achieves a 5.89% PSDS\nimprovement over MAT-SED, with ablation studies confirming\nthat block-level shuffle strengthens long-range dependencies,\nframe-level shuffle enhances transient event detection, and\nmultitask learning combining both perturbations yields the\nhighest gains. Additional investigations reveal that block flip-\nping disrupts essential event ordering, degrading performance,\nwhile moderate noise injection acts as a useful regulariza-\ntion mechanism. These findings highlight the importance of\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nstructured pretraining objectives that maintain event integrity\nwhile enforcing temporal reasoning. Beyond SED, JiTTER’s\napproach is applicable to broader auditory tasks such as audio\ncaptioning, speaker recognition, and bioacoustic monitoring.\nOur results demonstrate that temporal order reconstruction is\na crucial pretraining signal for event-aware SSL, providing a\nstronger alternative to conventional masking strategies.\nREFERENCES\n[1] T. Virtanen, M. D. Plumbley, and D. Ellis, Computational Analysis\nof Sound Scenes and Events, 1st ed.\nSpringer Publishing Company,\nIncorporated, 2017, pp. 3–11, 71–77.\n[2] N. Turpault, R. Serizel, A. P. Shah, and J. Salamon, “Sound event\ndetection in domestic environments with weakly labeled data and\nsoundscape synthesis,” in DCASE Workshop, 2019.\n[3] E. C¸ akır, G. Parascandolo, T. Heittola, H. Huttunen, and T. Virtanen,\n“Convolutional recurrent neural networks for polyphonic sound event\ndetection,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 25, no. 6, pp. 1291–1303, 2017.\n[4] A. Mesaros, T. Heittola, and T. Virtanen, “Metrics for polyphonic sound\nevent detection,” Applied Sciences, vol. 6, no. 6, 2016.\n[5] C¸ . Bilen, G. Ferroni, F. Tuveri, J. Azcarreta, and S. Krstulovi´c, “A\nframework for the robust evaluation of sound event detection,” in\nICASSP, 2020, pp. 61–65.\n[6] H. Nam, S.-H. Kim, B.-Y. Ko, D. Min, and Y.-H. Park, “Study on\nfrequency dependent convolution methods for sound event detection,”\nin Proc. INTER-NOISE, 2024.\n[7] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V. Le, “SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition,” in Proc. Interspeech, 2019.\n[8] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution-\naugmented Transformer for Speech Recognition,” in Proc. Interspeech,\n2020.\n[9] H. Nam and Y.-H. Park, “Coherence-based phonemic analysis on the ef-\nfect of reverberation to practical automatic speech recognition,” Applied\nAcoustics, vol. 227, p. 110233, 2025.\n[10] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A\nframework for self-supervised learning of speech representations,” in\nAdvances in Neural Information Processing Systems, 2020.\n[11] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and\nA. Mohamed, “Hubert: Self-supervised speech representation learning\nby masked prediction of hidden units,” IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 2021.\n[12] K. Okabe, T. Koshinaka, and K. Shinoda, “Attentive statistics pooling\nfor deep speaker embedding,” in Proc. Interspeech, 2018.\n[13] W. Cai, J. Chen, and M. Li, “Exploring the encoding layer and loss\nfunction in end-to-end speaker and language recognition system,” in\nProc. Interspeech, 2018.\n[14] S.-H. Kim, H. Nam, and Y.-H. Park, “Analysis-based optimization of\ntemporal dynamic convolutional neural network for text-independent\nspeaker verification,” IEEE Access, vol. 11, 2023.\n[15] J. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating fre-\nquency translational invariance in tdnns and frequency positional in-\nformation in 2d resnets to enhance speaker verification,” in Proc.\nInterspeech, 2021.\n[16] J. Li, Y. Tian, and T. Lee, “Convolution-based channel-frequency atten-\ntion for text-independent speaker verification,” in ICASSP, 2023.\n[17] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley,\n“Panns: Large-scale pretrained audio neural networks for audio pattern\nrecognition,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2020.\n[18] G.-T. Lee, H. Nam, S.-H. Kim, S.-M. Choi, Y. Kim, and Y.-H. Park,\n“Deep learning based cough detection camera using enhanced features,”\nExpert Systems with Applications, vol. 206, 2022.\n[19] S.-H. Kim, H. Nam, S.-M. Choi, and Y.-H. Park, “Real-time sound\nrecognition system for human care robot considering custom sound\nevents,” IEEE Access, vol. 12, 2024.\n[20] Y. Gong, Y.-A. Chung, and J. Glass, “Ast: Audio spectrogram trans-\nformer,” in Proc. Interspeech, 2021.\n[21] S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, W. Che,\nX. Yu, and F. Wei, “Beats: Audio pre-training with acoustic tokenizers,”\nin ICML, 2023.\n[22] A. Politis, A. Mesaros, S. Adavanne, T. Heittola, and T. Virtanen,\n“Overview and evaluation of sound event localization and detection in\ndcase 2019,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 29, 2020.\n[23] A. Politis, K. Shimada, P. Sudarsanam, S. Adavanne, D. Krause,\nY. Koyama, N. Takahashi, S. Takahashi, Y. Mitsufuji, and T. Virtanen,\n“STARSS22: A dataset of spatial recordings of real scenes with spa-\ntiotemporal annotations of sound events,” in DCASE Workshop, 2022.\n[24] B.-Y. Ko, H. Nam, S.-H. Kim, D. Min, S.-D. Choi, and Y.-H. Park,\n“Data augmentation and squeeze-and-excitation network on multiple\ndimension for sound event localization and detection in real scenes,”\nDCASE Challenge, Tech. Rep., 2022.\n[25] K. Drossos, S. Adavanne, and T. Virtanen, “Automated audio captioning\nwith recurrent neural networks,” in IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics, 2017.\n[26] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: an audio captioning\ndataset,” in ICASSP, 2020.\n[27] I. Choi, H. Nam, D. Min, S.-D. Choi, and Y.-H. Park, “Chatgpt caption\nparaphrasing and fense-based caption filtering for automated audio\ncaptioning,” DCASE Challenge, Tech. Rep., 2024.\n[28] J. Liang, I. Nolasco, B. Ghani, H. Phan, E. Benetos, and D. Stowell,\n“Mind the Domain Gap: a Systematic Analysis on Bioacoustic Sound\nEvent Detection,” arXiv preprint arXiv:2403.18638, 2024.\n[29] D. Min, H. Nam, and Y.-H. Park, “Few-shot bioacoustic event detection\nutilizing spectro-temporal receptive field,” in Proc. INTER-NOISE, 2024.\n[30] B.-Y. Ko, G.-T. Lee, H. Nam, and Y.-H. Park, “Prtfnet: Hrtf individual-\nization for accurate spectral cues using a compact prtf,” IEEE Access,\nvol. 11, 2023.\n[31] B.-Y. Ko, Y.-H. Park, G.-T. Lee, and H. Nam, “Deep learning based\nprediction of human auditory brainstem response for sound localization\nin median plane,” in International Congress on Acoustics (ICA), 2022.\n[32] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang,\nand M. D. Plumbley, “AudioLDM: Text-to-audio generation with latent\ndiffusion models,” in ICML, 2023.\n[33] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D´efossez, J. Copet,\nD. Parikh, Y. Taigman, and Y. Adi, “Audiogen: Textually guided audio\ngeneration,” in International Conference on Learning Representations\n(ICLR), 2023.\n[34] J. Lee, H. Nam, and Y.-H. Park, “Vifs: An end-to-end variational\ninference for foley sound synthesis,” DCASE Challenge, Tech. Rep.,\n2023.\n[35] K. Miyazaki, T. Komatsu, T. Hayashi, S. Watanabe, T. Toda, and\nK. Takeda, “Convolution-augmented transformer for semi-supervised\nsound event detection,” DCASE Challenge, Tech. Rep., 2020.\n[36] X. Zheng, H. Chen, and Y. Song, “Zheng ustc team’s submission for\ndcase2021 task4 – semi-supervised sound event detection,” DCASE\nChallenge, Tech. Rep., 2021.\n[37] H. Nam, B.-Y. Ko, G.-T. Lee, S.-H. Kim, W.-H. Jung, S.-M. Choi, and\nY.-H. Park, “Heavily augmented sound event detection utilizing weak\npredictions,” DCASE Challenge, Tech. Rep., 2021.\n[38] J. W. Kim, S. W. Son, Y. Song, H. K. Kim, I. H. Song, and J. E. Lim,\n“Semi-supervised learning-based sound event detection using frequency\ndynamic convolution with large kernel attention for DCASE challenge\n2023 task 4,” DCASE Challenge, Tech. Rep., 2023.\n[39] F. Schmid, P. Primus, T. Morocutti, J. Greif, and G. Widmer, “Improving\naudio spectrogram transformers for sound event detection through multi-\nstage training,” DCASE2024 Challenge, Tech. Rep., 2024.\n[40] N. Shao, X. Li, and X. Li, “Fine-tune the pretrained atst model for sound\nevent detection,” in ICASSP, 2024.\n[41] P. Cai, Y. Song, K. Li, H. Song, and I. McLoughlin, “Mat-sed: A\nmasked audio transformer with masked-reconstruction based pre-training\nfor sound event detection,” in Proc. Interspeech, 2024.\n[42] P. Cai, Y. Song, N. Jiang, Q. Gu, and I. McLoughlin, “Prototype\nbased masked audio model for self-supervised learning of sound event\ndetection,” arXiv preprint arXiv:2409.17656, 2024.\n[43] H. Nam, S.-H. Kim, and Y.-H. Park, “Filteraugment: An acoustic\nenvironmental data augmentation method,” in ICASSP, 2022.\n[44] H. Nam, S.-H. Kim, B.-Y. Ko, and Y.-H. Park, “Frequency Dynamic\nConvolution: Frequency-Adaptive Pattern Recognition for Sound Event\nDetection,” in Proc. Interspeech, 2022.\n[45] H. Nam, S.-H. Kim, D. Min, B.-Y. Ko, and Y.-H. Park, “Towards\nunderstanding of frequency dependence on sound event detection,” arXiv\npreprint arXiv:2502.07208, 2025.\n[46] D. Min, H. Nam, and Y.-H. Park, “Application of spectro-temporal re-\nceptive field on soft labeled sound event detection,” DCASE Challenge,\nTech. Rep., 2023.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\n[47] ——, “Auditory neural response inspired sound event detection based\non spectro-temporal receptive field,” in DCASE Workshop, 2023.\n[48] H. Nam, S.-H. Kim, D. Min, and Y.-H. Park, “Frequency & channel\nattention for computationally efficient sound event detection,” in DCASE\nWorkshop, 2023.\n[49] H. Nam, S.-H. Kim, D. Min, J. Lee, and Y.-H. Park, “Diversifying\nand expanding frequency-adaptive convolution kernels for sound event\ndetection,” in Proc. Interspeech, 2024.\n[50] H. Nam and Y.-H. Park, “Pushing the limit of sound event detec-\ntion with multi-dilated frequency dynamic convolution,” arXiv preprint\narXiv:2406.13312, 2024.\n[51] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and\nhuman-labeled dataset for audio events,” in ICASSP, 2017.\n[52] H. Nam, D. Min, I. Choi, S.-D. Choi, and Y.-H. Park, “Self training\nand ensembling frequency dependent networks with coarse prediction\npooling and sound event bounding boxes,” in DCASE Workshop, 2024.\n[53] K. Koutini, J. Schl¨uter, H. Eghbal-zadeh, and G. Widmer, “Efficient\ntraining of audio transformers with patchout,” in Proc. Interspeech, 2022.\n[54] D. Niizumi, D. Takeuchi, Y. Ohishi, N. Harada, and K. Kashino, “Byol\nfor audio: Self-supervised learning for general-purpose audio represen-\ntation,” in 2021 International Joint Conference on Neural Networks\n(IJCNN), 2021.\n[55] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, “Ssast: Self-supervised\naudio spectrogram transformer,” Proceedings of the AAAI Conference\non Artificial Intelligence, 2022.\n[56] P.-Y. Huang, H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze,\nand C. Feichtenhofer, “Masked autoencoders that listen,” in Advances\nin Neural Information Processing Systems, 2022.\n[57] P. Chen, S. Liu, and J. Jia, “Jigsaw clustering for unsupervised visual\nrepresentation learning,” in IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2021.\n[58] Y. Chen, X. Shen, Y. Liu, Q. Tao, and J. A. Suykens, “Jigsaw-vit:\nLearning jigsaw puzzles in vision transformer,” Pattern Recognition\nLetters, 2023.\n[59] I. Misra, C. L. Zitnick, and M. Hebert, “Shuffle and learn: Unsupervised\nlearning using temporal order verification,” in ECCV, 2016.\n[60] A. T. Liu, S.-W. Li, and H.-y. Lee, “Tera: Self-supervised learning of\ntransformer encoder representation for speech,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2021.\n[61] Z. Dai*, Z. Yang*, Y. Yang, W. W. Cohen, J. Carbonell, Q. V. Le, and\nR. Salakhutdinov, “Transformer-XL: Language modeling with longer-\nterm dependency,” in International Conference on Learning Represen-\ntations (ICLR), 2019.\n[62] S. Venkatesh, D. Moffat, and E. R. Miranda, “You only hear once: a\nyolo-like algorithm for audio segmentation and sound event detection,”\nApplied Sciences, 2022.\n[63] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\nempirical risk minimization,” in International Conference on Learning\nRepresentations (ICLR), 2018.\n[64] J. Ebbers, F. G. Germain, G. Wichern, and J. L. Roux, “Sound event\nbounding boxes,” in Proc. Interspeech, 2024.\nHyeonuk Nam received B.S. and M.S. degrees in\nmechanical engineering from Korea Advanced Insti-\ntute of Science and Technology, Daejeon, Korea, in\n2018 and 2020 respectively. He is currently pursuing\nthe Ph.D. degree in mechanical engineering at the\nsame institute. His research interest includes various\nauditory intelligence themes including sound event\ndetection, sound event localization and detection,\nautomatic audio captioning, sound scene synthesis\nand human auditory perception.\nYong-Hwa Park received BS, MS, and PhD in\nMechanical Engineering from KAIST in 1991,\n1993, and 1999, respectively. In 2000, he joined to\nAerospace Department at the University of Colorado\nat Boulder as a research associate. From 2003-2016,\nhe worked for Samsung Electronics in the Visual\nDisplay Division and Samsung Advanced Institute\nof Technology (SAIT) as a Research Master in the\nfield of micro-optical systems with applications to\nimaging and display systems. From 2016, he joined\nKAIST as professor of NOVIC+ (Noise & Vibration\nControl Plus) at the Department of Mechanical Engineering devoting to\nresearch on vibration, acoustics, vision sensors, and condition monitoring with\nAI. His research fields include structural vibration; condition monitoring from\nsound and vibration using AI; health monitoring sensors; and 3D sensors,\nand lidar for vehicles and robots. He is the conference chair of MOEMS\nand miniaturized systems in SPIE Photonics West since 2013. He is a vice-\npresident of KSME, KSNVE, KSPE, and member of IEEE and SPIE\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20857v1.pdf",
    "total_pages": 10,
    "title": "JiTTER: Jigsaw Temporal Transformer for Event Reconstruction for Self-Supervised Sound Event Detection",
    "authors": [
      "Hyeonuk Nam",
      "Yong-Hwa Park"
    ],
    "abstract": "Sound event detection (SED) has significantly benefited from self-supervised\nlearning (SSL) approaches, particularly masked audio transformer for SED\n(MAT-SED), which leverages masked block prediction to reconstruct missing audio\nsegments. However, while effective in capturing global dependencies, masked\nblock prediction disrupts transient sound events and lacks explicit enforcement\nof temporal order, making it less suitable for fine-grained event boundary\ndetection. To address these limitations, we propose JiTTER (Jigsaw Temporal\nTransformer for Event Reconstruction), an SSL framework designed to enhance\ntemporal modeling in transformer-based SED. JiTTER introduces a hierarchical\ntemporal shuffle reconstruction strategy, where audio sequences are randomly\nshuffled at both the block-level and frame-level, forcing the model to\nreconstruct the correct temporal order. This pretraining objective encourages\nthe model to learn both global event structures and fine-grained transient\ndetails, improving its ability to detect events with sharp onset-offset\ncharacteristics. Additionally, we incorporate noise injection during block\nshuffle, providing a subtle perturbation mechanism that further regularizes\nfeature learning and enhances model robustness. Experimental results on the\nDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%\nimprovement in PSDS, highlighting the effectiveness of explicit temporal\nreasoning in SSL-based SED. Our findings suggest that structured temporal\nreconstruction tasks, rather than simple masked prediction, offer a more\neffective pretraining paradigm for sound event representation learning.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}