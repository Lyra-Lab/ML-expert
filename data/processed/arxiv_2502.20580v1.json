{
  "id": "arxiv_2502.20580v1",
  "text": "Training Large Neural Networks With\nLow-Dimensional Error Feedback\nMaher Hanut and Jonathan Kadmon\nEdmond and Lily Center for Brain Sciences,\nThe Hebrew University,\nJerusalem.\n{maher.hanut, jonathan.kadmon}@mail.huji.ac.il\nAbstract\nTraining deep neural networks typically relies on backpropagating high-dimensional\nerror signals—a computationally intensive process with little evidence supporting its\nimplementation in the brain. However, since most tasks involve low-dimensional out-\nputs, we propose that low-dimensional error signals may suffice for effective learning.\nTo test this hypothesis, we introduce a novel local learning rule based on Feedback\nAlignment that leverages indirect, low-dimensional error feedback to train large net-\nworks. Our method decouples the backward pass from the forward pass, enabling\nprecise control over error signal dimensionality while maintaining high-dimensional\nrepresentations. We begin with a detailed theoretical derivation for linear networks,\nwhich forms the foundation of our learning framework, and extend our approach\nto nonlinear, convolutional, and transformer architectures. Remarkably, we demon-\nstrate that even minimal error dimensionality—on the order of the task dimension-\nality—can achieve performance matching that of traditional backpropagation. Fur-\nthermore, our rule enables efficient training of convolutional networks, which have\npreviously been resistant to Feedback Alignment methods, with minimal error. This\nbreakthrough not only paves the way toward more biologically accurate models of\nlearning but also challenges the conventional reliance on high-dimensional gradient\nsignals in neural network training. Our findings suggest that low-dimensional error\nsignals can be as effective as high-dimensional ones, prompting a reevaluation of\ngradient-based learning in high-dimensional systems. Ultimately, our work offers a\nfresh perspective on neural network optimization and contributes to understanding\nlearning mechanisms in both artificial and biological systems.\n1\narXiv:2502.20580v1  [cs.LG]  27 Feb 2025\n\n\n1.\nIntroduction\nThe brain, despite its vast complexity, operates within the constraints of a low-dimensional\nworld. Every movement—lifting a hand, turning a head—follows finite joint constraints.\nNavigation unfolds in modest spatial dimensions, and object recognition is drawn from\na limited set of familiar categories. Even when possibilities seem vast, they are dwarfed\nby the sheer number of neurons and synapses in any brain region. Artificial neural net-\nworks reflect this paradox: the number of neurons in each layer often far exceeds the\ntask-relevant variables. This contrast between task and internal representation dimen-\nsionality challenges us to rethink how we train such large networks and explore new\nlearning paradigms.\nModern deep learning methods rely on high-dimensional gradients that propagate back-\nward through layers, ensuring a precise credit assignment to each parameter [1, 2]. How-\never, this process is computationally intensive and biologically implausible, given the lack\nof direct evidence for backpropagation in the brain [3, 4]. If the loss gradient does not\npropagate back through the network, it must be obtained through alternative routes [5].\nIn this case, there is no reason to assume that each neuron receives detailed informa-\ntion about the local gradient. Moreover, the brain likely operates with low-dimensional\nerror signals, reflecting the inherently low-dimensional nature of tasks. This raises an\nessential question: Can large neural networks be trained efficiently with constrained,\nlow-dimensional error signals? If error dimensionality is tied to task complexity rather\nthan network size, it could revolutionize how we train large networks and improve our\nunderstanding of biological learning.\nDespite the importance of this question, the effect of low-dimensional error feedback on\nlearning and neural representations was never properly explored. The strong coupling of\nfeedforward and backward passes in backpropagation, where both processes use the same\nsynapses, has limited investigations into alternative training methods. While approaches\nsuch as Feedback Alignment have been proposed [5], the necessity of full gradients for\neffective learning has not been questioned.\nIn this study, we propose that high-dimensional error propagation is not strictly neces-\nsary for training deep, overparameterized networks. We introduce a novel learning rule\nbased on Feedback Alignment that decouples forward and backward passes, enabling\nprecise control over the dimensionality of error signals. Our method exploits the low-\ndimensional nature of real-world tasks to train large networks efficiently without com-\npromising their representational capacity, achieving performance on par with backprop-\nagation.\nOur theoretical analysis in linear settings highlights the potential pitfalls of\nnaive low-dimensional error propagation and shows that a simple local online learning\nrule can recover backpropagation-level performance. Extending this approach to nonlin-\near, convolutional, and transformer-based networks, we demonstrate that learning with\n2\n\n\nlow-dimensional error signals is both feasible and universal.\nFinally, we explore how\nerror dimensionality influences neural representations by examining its effects on recep-\ntive fields in an early visual system model, underscoring the broader implications of our\nfindings.\n2.\nBackground and related work\nWe consider a multilayered perceptron with L layers, each layer l computes its output\nas hl = f(Wlhl−1), where Wl is the weight matrix and f is an element-wise activation\nfunction. The input to the network is h0 = x, and the final network output is ˆy =\nfL(WLhL−1), which approximates the target y. The task dimensionality, denoted d, is at\nmost the number of components in y and ˆy. Training the network involves minimizing a\nloss function L(y, ˆy) by adjusting the weights {Wl}. The error signal at the output layer,\nδL = ∂L\n∂ˆy, is a d-dimensional vector, typically much smaller than the number of neurons in\nthe hidden layers.\nBackpropagation (BP) [1] is the standard approach for training neural networks. It\npropagates the error backward through the network using δl = W T\nl+1δl+1⊙f ′(Wlhl−1), and\nupdates the weights using ∆Wl = −ηδlhT\nl−1, where η is the learning rate. However, this\nmethod requires an exact transposition of the forward weights, W T\nl+1, which is biologically\nimplausible [6, 7]. Moreover, BP tightly couples the error propagation with the forward\npass, limiting the ability to explore how different properties of the error signal affect\nlearning dynamics.\nFeedback Alignment (FA) [5] was proposed to address the biological limitations of BP\nby replacing W T\nl+1 with a fixed random matrix Bl. The error is computed as:\nδl = Blδl+1 ⊙f ′(Wlhl−1),\n(1)\ndecoupling the forward and backward weights and providing a more biologically plausi-\nble mechanism. However, FA struggles to scale effectively in deep architectures, such as\nconvolutional neural networks (CNNs), where it often fails to achieve competitive perfor-\nmance [8].\nAn extension of FA involves adapting Bl by updating it alongside the forward weights Wl\nto improve their alignment [9, 10]:\n∆Bl = −ηhl−1δT\nl −λBl,\n∆Wl = −ηδlhT\nl−1 −λWl,\n(2)\nwhere λ is a regularization parameter. Although this adaptive approach improves perfor-\nmance by better aligning forward and backward weights, it still requires high-dimensional\nerror signals and struggles to match BP performance in complex architectures like CNNs.\nFurthermore, in Section 3, we show that this approach fails when the matrix B is low-rank\n3\n\n\nand the dimensionality of the error is constrained.\nOther studies have explored the use of fixed sparse feedback matrices to reduce the di-\nmensionality of error propagation [11]. However, these approaches result in significantly\nlower performance and do not provide a systematic framework for studying how error\nconstraints affect learning and representation formation.\nBeyond FA-based methods, several studies have shown that weight updates using back-\npropagation can result in a low-dimensional weight update [12–14] and favor low-rank\nsolutions [15]. These findings support our hypothesis that low-dimensional feedback is\nsufficient to train deep networks. However, no previous work has considered training with\na constrained error pathway, and the effects of error dimensionality and training have not\nbeen systematically studied.\nIn this work, our aim is to systematically investigate how constraining the dimensionality\nof the error signal affects the training and performance of neural networks. To this end,\nwe introduce a novel learning scheme, Restricted Adaptive Feedback (RAF), that allows\nflexible control over the dimensionality of the errors (Figure 1).\nOur main contributions are:\n1. We present a novel learning rule, Restricted Adaptive Feedback (RAF), which matches\nBP performance while requiring minimal error signals. We provide a detailed deriva-\ntion of the learning dynamics in a simple linear case, establishing a foundational\nunderstanding of how RAF operates.\n2. We demonstrate that nonlinear networks can efficiently learn nontrivial datasets\nusing low-dimensional error signals, highlighting the versatility of RAF in practical\nscenarios.\n3. We show that more complex yet highly useful architectures, such as convolutional\nnetworks and transformers, can also be effectively trained with low-dimensional\nfeedback.\n4. We reveal that error dimensionality shapes the receptive fields in a model of the\nventral visual system, offering new insights into the relationship between learning\nmechanisms and biological neural representations.\nIn the final section, we discuss the broader implications of our results for both neuroscience\nand machine learning.\n4\n\n\nLayer 1\nLayer 3\nLayer 2\nD\nVZ7rLMZFZEiOgZj4z2qR8eOM=\">AB6nicdVDLSsNAFL2pr\n1pfVZduBovgKiRa03QjBTcuK9oHtKFMpN26OTBzEQoZ/gx\noUibv0id/6Nk7aCih64cDjnXu69x084k8qyPozCyura+kZxs\n7S1vbO7V94/aMs4FYS2SMxj0fWxpJxFtKWY4rSbCIpDn9OP\n7nK/c49FZLF0Z2aJtQL8ShiASNYaem2MzgflCuWeFWnXMX5\naRed2sL4tSryDatOSqwRHNQfu8PY5KGNFKEYyl7tpUoL8NCM\ncLprNRPJU0wmeAR7Wka4ZBKL5ufOkMnWhmiIBa6IoXm6veJD\nIdSTkNfd4ZYjeVvLxf/8nqpClwvY1GSKhqRxaIg5UjFKP8bD\nZmgRPGpJpgIpm9FZIwFJkqnU9IhfH2K/iftM9N2TOemWmlcL\nuMowhEcwynYUIMGXEMTWkBgBA/wBM8GNx6NF+N10VowljOH8\nAPG2yeNrI4B</latexit>W3\nW2\nW1\nW T\n3\nW T\n2\nBackpropagation\n(BP)\nLayer 1\nLayer 3\nLayer 2\nM\n=\">AB6nicdVDLSsNAFL2pr1pfVZduBovgKiRa03QjBTcuK9oHtKFMpN26OTBzEQoZ/gxoUibv0id/6Nk7aCih64cDjnX\nu69x084k8qyPozCyura+kZxs7S1vbO7V94/aMs4FYS2SMxj0fWxpJxFtKWY4rSbCIpDn9OP7nK/c49FZLF0Z2aJtQL8ShiA\nSNYaem2MzgflCuWeFWnXMX5aRed2sL4tSryDatOSqwRHNQfu8PY5KGNFKEYyl7tpUoL8NCMcLprNRPJU0wmeAR7Wka4ZBKL\n5ufOkMnWhmiIBa6IoXm6veJDIdSTkNfd4ZYjeVvLxf/8nqpClwvY1GSKhqRxaIg5UjFKP8bDZmgRPGpJpgIpm9FZIwFJkqnU\n9IhfH2K/iftM9N2TOemWmlcLuMowhEcwynYUIMGXEMTWkBgBA/wBM8GNx6NF+N10VowljOH8APG2yeNrI4B</latexit>W3\nW2\nW1\nU\n=\">AB6nicdVDLSgNBEOyNrxhfUY9eBoPgKezGvLxI0IvHiOYByRJmJ7PJkNnZWZWCEs+wYsHRbz6Rd78GyfJCipa0FBUd\ndPd5UWcKW3bH1ZmZXVtfSO7mdva3tndy+8ftFUYS0JbJOSh7HpYUc4EbWmOe1GkuLA47TjTa7mfueSsVCcaenEXUDPBLMZ\nwRrI91eDkqDfMEu2gsgu1iuV2q1siFOtX5WPUdOahUgRXOQf+8PQxIHVGjCsVI9x460m2CpGeF0luvHikaYTPCI9gwVOKDKT\nRanztCJUYbID6UpodFC/T6R4ECpaeCZzgDrsfrtzcW/vF6s/bqbMBHFmgqyXOTHOkQzf9GQyYp0XxqCaSmVsRGWOJiTbp5\nEwIX5+i/0m7ZGIpVm/KhcZFGkcWjuAYTsGBGjTgGprQAgIjeIAneLa49Wi9WK/L1oyVzhzCD1hvnx5+jbY=</latexit>B2\nB1\nFeedback Alignment\n(FA)\nLayer 1\nLayer 3\nLayer 2\nK\n9oHtKFMpN26OTBzEQoZ/gxoUibv0id/6Nk7aCih64cDjnXu69x084k8qyPozCyura+kZxs7S1vbO7V94/aMs4FYS2SMxj0fWxpJxFtKWY4rSbCIpDn9OP7nK/c49FZLF0Z2aJtQL8ShiASNYaem2MzgflCuWeFWnXMX5aRed2sL4tSryDatOSqwRHNQf\nu8PY5KGNFKEYyl7tpUoL8NCMcLprNRPJU0wmeAR7Wka4ZBKL5ufOkMnWhmiIBa6IoXm6veJDIdSTkNfd4ZYjeVvLxf/8nqpClwvY1GSKhqRxaIg5UjFKP8bDZmgRPGpJpgIpm9FZIwFJkqnU9IhfH2K/iftM9N2TOemWmlcLuMowhEcwynYUIMGXEMTWkBgB\nA/wBM8GNx6NF+N10VowljOH8APG2yeNrI4B</latexit>W3\nW1\nW2\nr\noQ1ls920SzebsLsRSulP8OJBEa/+Im/+GzdtBRV9MPB4b4aZeUHCmdKO82HlVlbX1jfym4Wt7Z3dveL+QVvFqS0RWIey06AFeVM0JZmtNOIimOAk7vgvFl5t/dU6lYLG71JKF+hIeChYxgbaSbi365Xyw5tuvV61UXOXbFrZVrlYyceY5XQa7tzFGCJZr94ntvEJ\nM0okITjpXquk6i/SmWmhFOZ4VeqmiCyRgPadQgSOq/On81Bk6McoAhbE0JTSaq98npjhSahIFpjPCeqR+e5n4l9dNdVj3p0wkqaCLBaFKUc6RtnfaMAkJZpPDMFEMnMrIiMsMdEmnYIJ4etT9D9pl01QtndLTXOl3Hk4QiO4RcqEDrqAJLSAwhAd4gmeLW4/W\ni/W6aM1Zy5lD+AHr7RMuPo3A</latexit>B2\nB1\nRestricted Adaptive \nFeedback (RAF)\nLayer 1\nLayer 3\nLayer 2\nW3\nW2\nW1\nr\noQ1ls920SzebsLsRSulP8OJBEa/+Im/+GzdtBRV9MPB4b4aZeUHCmdKO82HlVlbX1jfym4Wt7Z3dveL+QVvFqS0RWIey06AFeVM0JZmtNOIimOAk7vgvFl5t/dU6lYLG71JKF+hIeChYxgbaSbi365Xyw5tuvV61UXOXbFrZVrlYyceY5XQa7tzFGCJZr94ntvEJ\nM0okITjpXquk6i/SmWmhFOZ4VeqmiCyRgPadQgSOq/On81Bk6McoAhbE0JTSaq98npjhSahIFpjPCeqR+e5n4l9dNdVj3p0wkqaCLBaFKUc6RtnfaMAkJZpPDMFEMnMrIiMsMdEmnYIJ4etT9D9pl01QtndLTXOl3Hk4QiO4RcqEDrqAJLSAwhAd4gmeLW4/W\ni/W6aM1Zy5lD+AHr7RMuPo3A</latexit>B2\nB1\ndirect Restricted Adaptive \nFeedback (dRAF)\nFigure 1: Illustration of different approaches for propagating error to hidden layers. From\nleft to right: Backpropagation (BP) propagates error using the exact transpose of the\nforward weights. Feedback Alignment (FA) replaces the transposed weights with fixed\nrandom feedback matrices, which gradually align with the forward weights during training.\nRestricted Adaptive Feedback (RAF) constrains error propagation through low-rank\nfeedback matrices, limiting error dimensionality and preventing an exact mirroring of\nBP. Direct Restricted Adaptive Feedback (dRAF) extends RAF by enabling error\nsignals to bypass intermediate layers, propagating directly from the output layer or other\nnon-adjacent layers.\n3.\nLow-dimensional error feedback in linear networks\nWe begin our analysis by studying learning dynamics in multilayered linear networks.\nAlthough linear models may seem overly simplistic, they can exhibit rich learning dynam-\nics due to the nonlinearity introduced by the loss function [16]. Additionally, imposing\ndimensional constraints on linear networks yields insightful results that extend beyond\nthe linear case.\nA linear problem\nWe consider a simple linear transformation problem with a low-\ndimensional structure, y = Ax. Here, x ∈Rn represents the n-dimensional input, and\ny ∈Rm represents the target. The matrix A is a rank-d matrix defined as A = Pd\nj=1 ujvT\nj ,\nwhere uj ∈Rn and vj ∈Rm are random Gaussian vectors, and we assume d ≪n. Our\ndata set consists of p training samples {xµ, yµ}p\nµ=1, with each input vector xµ being i.i.d.\naccording to the standard normal distribution xµ ∼N(0, 1). The labels are given by yµ =\nAxµ + ξµ, where ξµ is additive Gaussian noise with zero mean and unit variance.\nThe goal is to learn the low-dimensional structure of A from the p samples using a linear\nneural network. For simplicity, we assume that p is sufficiently large to allow the network\nto fully recover the structure of A.\nA linear network model.\nTo study the effects of restricted error pathways, we consider\na simple linear network with three layers: an input layer x ∈Rn, a hidden layer h ∈Rk,\n5\n\n\nand an output layer y ∈Rm. The input and hidden layers are connected by the weight\nmatrix W1 ∈Rk×n, and the hidden and output layers are connected by the weight matrix\nW2 ∈Rm×k. The output of the network can be expressed as y = W2W1x (Figure 2).\nThe network is trained to minimize the quadratic empirical loss function:\nL = 1\np\nX\nµ\n∥y(xµ) −W2W1xµ∥2.\n(3)\nWe apply Feedback Alignment (FA) to update W1, which does not have direct access\nto the loss gradient. Instead of backpropagating the error through W T\n2 , we use a fixed\nlow-rank feedback matrix B. This provides an alternative pathway for propagating the\nerror signal to W1.\nFor a given data point {xµ, yµ}, the weight updates, derived from the FA framework, are\ngiven by:\n∆W µ\n1 = ηBT(yµ −W2W1xµ)xµT, and\n∆W µ\n2 = η(yµ −W2W1xµ)xµTW T\n1 .\n(4)\nHere, η represents the learning rate, and the update for W1 is computed using the indirect\nerror feedback provided by B, while W2 receives the full error signal directly from the\noutput.\nConstraining error dimensionality with low-rank feedback\nTo control the dimen-\nsionality of the error feedback, we impose a low-rank constraint on the feedback matrix\nB. Rather than allowing full-dimensional feedback, we decompose B as B = QP, where\nQ ∈Rk×r and P ∈Rr×m. When r < min(k, m), B is low rank, which means that it can\nproject the error signal onto at most r independent directions.\nThis low-rank structure introduces an \"r-bottleneck,\" which limits the flow of error infor-\nmation. In linear settings, the problem is solvable using a single weight matrix, rendering\nthe training of W1 unnecessary. However, backpropagation dynamics still adjust these\nweights [16]. By controlling the value of r, we can systematically study how reducing\nthe dimensionality of the error signal impacts learning. Initially, we follow the original\nFeedback Alignment framework, keeping Q and P as random matrices. However, as we\nwill demonstrate, allowing Q and P to learn is crucial to high performance.\n3.1\nLearning dynamics\nOur analysis extends the framework established by Saxe et. al. [16] to incorporate indirect\nfeedback with constrained dimensionality. We begin by characterizing the task across\nthe p data points through the input-output covariance matrix, Σio = 1\np\nPp\nµ=1 yµ(xµ)T,\nwhich captures the correlation between input vectors x and output vectors y. Performing\nSingular Value Decomposition (SVD), we obtain Σio = USV T, where U ∈Rm×m and\n6\n\n\nV ∈Rn×n contain the left and right singular vectors, respectively, and S ∈Rm×n is a\nrectangular diagonal matrix of singular values. For sufficiently large p, the first d singular\nvalues in S correspond to the prominent directions in the data (i.e., the singular values\nof A), while the remaining singular values are O(1/√p) and dominated by noise.\nTo track how training aligns the network weights with these prominent directions, we\nrotate the weight matrices W1, W2, and B using the singular vectors of Σio. This trans-\nformation simplifies the analysis by aligning the network’s weight dynamics with the key\ndata directions:\nW1 = ¯W1V T,\nW2 = U ¯W2,\nB = ¯BU T,\nwhere ¯W1, ¯W2, and ¯B represent the transformed weight matrices. This rotation aligns\nthe weight dynamics with the dominant singular vectors, allowing us to focus on how the\nnetwork captures the important features of the data.\nSince the inputs are uncorrelated, we can apply these transformations to the iterative\nweight-update equations derived from the FA learning rule.\nAssuming a small learn-\ning rate η ≪1 with full-batch updates, we express the weight updates in continuous\ntime:\nτ d ¯W1\ndt\n= ¯BT(S −¯W2 ¯W1),\nτ d ¯W2\ndt\n= (S −¯W2 ¯W1) ¯W T\n1 ,\n(5)\nwhere η = dt/τ. This continuous form captures the dynamics of the learning process,\nallowing us to study it from a dynamical systems perspective. By analyzing these equa-\ntions, we can identify fixed points and evaluate their stability, providing insight into how\nthe network converges and learns under constrained feedback. Figure 2, shows how the\nsingular vectors of W2W1 align to the corresponding singular vectors of Σio .\nStationary solutions for the training.\nTraining halts when the right-hand side of the\nweight update equations (5) vanishes, indicating that the dynamics have reached a stable\nfixed point. At this fixed point, the update equation for ¯W1 leads to the condition:\n¯B(S −¯W2 ¯W1) = 0 =⇒SjjB:,j =\nm\nX\ni=1\nB:,i( ¯W2 ¯W1)ij\n∀j,\n(6)\nwhere B:,j ∈Rk is the j-th column of ¯B, and Sjj is the j-th singular value of Σio. This\nequation indicates that, at the stationary point, the weight products ¯W2 ¯W1 must align\nwith the singular modes of the data.\nHowever, since B is of rank r, the feedback matrix ¯B can only span at most r independent\ndirections. If r = m, the system has enough feedback dimensionality to align perfectly\nwith the singular values in S, recovering the full structure of Σio as demonstrated in Saxe\net. al. [16]. In this case, the training successfully converges to a unique solution where\n¯W2 ¯W1 = S (Figure 2b).\nCrucially, when r < m, the feedback matrix ¯B lacks the sufficient rank to fully capture the\n7\n\n\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(a)\n6\n4=\"QjKvg5F3BJjgH9u1mQt6j5I29ik=\">AB6\nnicdVDJSgNBEK1xjXGLevTSGARPw0wMd6CXj\nxGNAskQ+jp9CRNunuG7h4hDPkELx4U8eoXefN\nv7CyC64OCx3tVNULE8608bx3Z2l5ZXVtPbeR3\n9za3tkt7O03dZwqQhsk5rFqh1hTziRtGY4bS\neKYhFy2gpHl1O/dUeVZrG8NeOEBgIPJIsYwcZK\nN61eqVcoem65el4pe+g38V1vhiIsUO8V3r9m\nKSCSkM41rje4kJMqwMI5xO8t1U0wSTER7Qjq\nUSC6qDbHbqB1bpY+iWNmSBs3UrxMZFlqPRWg7\nBTZD/dObin95ndRE1SBjMkNlWS+KEo5MjGa/\no36TFi+NgSTBSztyIyxAoTY9PJ2xA+P0X/k2b\nJ9Stu5fq0WLtYxJGDQziCE/DhDGpwBXVoAIEB\n3MjPDnceXCenZd565KzmDmAb3BePwBt/43u<\n/latexit>W2\nW1\nB = QP\nx →Rn\nh →Rk\nb\nase64=\"/Z6CuYRaiH6lgnpUVEMRs\n+fzy7U=\">ACBHicbVC7TsMwFHX\nKq5RXgLGLRYXEVCUIFcYKFsaC6ENq\nQuU4TmvVdiLbQYqiDiz8CgsDCLHy\nEWz8DU7bAVquZPnonHt1z1BwqjSj\nvNtlVZW19Y3ypuVre2d3T17/6Cj4\nlRi0sYxi2UvQIowKkhbU81IL5E8\nYCRbjC+KvTuA5GKxuJOZwnxORoKGl\nGMtKEGdtULYhaqjJsPZh4VHkd6FA\nTw9p4P7JpTd6YFl4E7BzUwr9bA/vL\nCGKecCI0ZUqrvOon2cyQ1xYxMKl6\nqSILwGA1J30CBOF+Pj1iAo8NE8I\noluYJDafs74kcVX4NJ2FRbWoFeR/\nWj/V0YWfU5Gkmg8WxSlDOoYFonA\nkEqCNcsMQFhS4xXiEZIa5NbxYTg\nLp68DqndbdRb9yc1ZqX8zjKoAqOw\nAlwTlogmvQAm2AwSN4Bq/gzXqyX\nqx362PWrLmM4fgT1mfP73amCo=</\nlatexit>y →Rm\nbackward rank\nbackward rank\nFeedback alignment (FA)\nRestricted adaptive feedback (RAF)\nr > d\nr > d\nr < d\nr < d\nFigure 2: Learning dynamics and component alignment in linear networks. (a) Schematic\nof the network architecture with input dimension n = 128, hidden layer size k = 64, and\noutput dimension m = 64. The feedback matrix B is factorized as B = QP and con-\nstrained to rank r. (b, c) Comparison of theoretical predictions (dashed) and numerical\nsimulations (solid) for low-rank Feedback Alignment (FA), training only Q, with r = 64\nand r = 8, respectively. The y-axis shows the alignment of singular vectors between W2W1\nand Σio. (d, e) Same as (b, c) but for Restricted Adaptive Feedback (RAF), where both\nQ and P are trained. (f) When P is not trained, the singular modes align on average\n(bold), but the top r components fail to recover fully. (g) Training P to align with the\nerror, as in RAF, ensures full recovery of the top r singular components, improving con-\nvergence and alignment.\n8\n\n\nm independent directions in S. As a result, eq. (6) becomes under-determined, leading to\npotentially infinite solutions. This means that the trained weights ¯W2 ¯W1 may not align\nwith the true data structure encoded in Σio (Figure 2c)\nThe implications of the uderdetrimed fixed point solutions are surprising. Naively, one\nwould expect that if the bottleneck is not too narrow, the projections of the error\nwould maintain the necessary structure to guide the learning.\nSpecifically, the John-\nson–Lindenstrauss lemma [17] suggests that as long as r > d log m, the pairwise correla-\ntion structure of the error signal would be maintained. Nevertheless, our analysis shows\nthat the solutions weight are not guaranteed to converge to the correct solution. Thus,\nin the case of low-rank feedback, Feedback Alignment (FA) is likely to fail. The solution\nto this predicament, as we show next, is aligning the feedback weights with the data,\nensuring that the network learns the correct representations.\n3.2\nTraining the feedback weights\nFrom the previous analysis, it is apparent that the feedback matrix B must be trained for\nthe learning dynamics to converge to the correct solution of Eq. (6). When B is low-rank\nand fixed, the network lacks sufficient capacity to transmit the full error information,\nwhich can impede learning.\nOne approach is to adopt a learning rule inspired by Kolen-Pollack, as in eq. (2). When\nB = QP, the updates are given by\n∆W µ\n2 = ηδµxµTW T\n1 −λW2\nand\n∆Qµ = ηW1xµδµTP T −λQ,\n(7)\nwhere δµ = ∂L(xµ,yµ)\n∂y\n= yµ −W2W1xµ is the error gradient for the µ-th data point, η is\nthe learning rate, and λ is the regularization parameter. Importantly, in this framework\nonly the column space of B, or in our case Q are updated.\nHowever, adopting this learning framework is insufficient when P remains fixed and ran-\ndomly initialized (Figure 2c). Since P can project onto at most r unique directions of the\nerror, it may not align with the relevant error subspace. In this case, the network may\nconverge to an incorrect solution, as indicated by the non-uniqueness of solutions to (6)\nwhen m > r.\nIdeally, we want P to be an orthogonal matrix whose r columns span the top r principal\ndirections of the output-output correlation matrix Σyy. This alignment ensures that the\nmost significant components of the error are propagated back through the network.\nIn cases where the output correlations are unknown, we can update P using a modified\nOja learning rule [18]:\n∆P µ = ηPyµyµT(I −P TP) −λP,\n(8)\nwhere I is the identity matrix. This rule adjusts P incrementally so that its columns\n9\n\n\nconverge to the top r principal components of the outputs {yµ}.\nBy training both Q and P, we allow the feedback matrix B = QP to adaptively align\nwith the relevant error directions, enabling the network to learn the correct mappings\neven under constrained feedback dimensionality.\nRepeating the linear analysis that led to (5), we extend the derivation to our case with\nadaptive feedback weights. We define the transformed feedback matrix as P = ¯PU T.\nThis transformation aligns the feedback matrix P with the principal components of the\ndata, simplifying the analysis.\nTaking the continuous-time limit (with η →0 and ηp = τ), we obtain a set of differential\nequations that describe the learning dynamics of the forward and backward weights:\nτ d ¯W1\ndt\n= ¯BT(S −¯W2 ¯W1),\nτ d ¯W2\ndt\n= (S −¯W2 ¯W1) ¯W T\n1 −λ ¯W2,\n(9)\nand\nτ dQ\ndt = ¯W1(S −¯W2 ¯W1)T ¯P T −λQ,\nτ d ¯P\ndt = ¯PSST(I −¯P T ¯P) −λ ¯P.\n(10)\nHere, ¯B = Q ¯P and λ is the regularization parameter. Note that Q is not affected by\nrotation, as it does not come in contact with either the input or the output. The full\nderivation can be found in the Appendix.\nNotably, the updates to the feedback weights are local and follow learning rules that were\nwell-studied in theoretical neuroscience [18–21]. Local plasticity makes our framework an\nattractive alternative for backpropagation in models of brain circuits.\nWe refer to this learning framework as Restricted Adaptive Feedback (RAF). Figure 2\ncompares the learning dynamics of RAF with those of BP and FA where only the Q\nmatrix is learned.\nThe results demonstrate that RAF effectively aligns the feedback\nweights, enabling the network to converge to the correct solution despite the constrained\nerror dimensionality. Furthermore, RAF will learn the top r components of Σ, even if\nr < d (Figure 2e).\n3.3\nRestricted adaptive feedback in deep architectures\nOur weight-update equations can be naturally extended to deeper networks. In the single\nhidden layer model above, the backward weights P were updated using the true labels\nyµ. However, in deeper models, hidden layers do not have ground-truth representations.\nInstead, each layer relies on the local error signal, which propagates through the network\naccording to δµ\nl = QlPlδµ\nl+1. This error signal δµ\nl provides the necessary information for\n10\n\n\nlearning at the layer l.\nTo update the feedback weights Pl in the absence of ground-truth representations, we use\nlocal error signals δµ\nl+1. As in the single-layer model, we use Oja’s rule to adjust Pl to\nspan the principal components of the error at the next layer, δµ\nl+1, ensuring efficient error\npropagation.\nThe complete update rules for layer l are given by\n∆W µ\nl = ηQlPlδµ\nl+1hµT\nl\n−λWl\nand\n∆Qµ\nl = ηhµ\nl Plδµ\nl+1 −λQl,\n∆P µ\nl = ηPlδµ\nl+1δµT\nl+1(I −P T\nl Pl) −λP,\n(11)\nwhere η is the learning rate, λ is the regularization parameter, hµ\nl is the activation of\nlayer l, and δµ\nl+1 is the error signal from the next layer.\nBy updating Pl using the error signals, we ensure that the feedback weights of each layer\nare adapted to capture the most relevant directions in the error space, facilitating effective\nlearning throughout the network. Notably, while we use the same learning rate η and\nweight decay λ for all components {Wl}, {Ql}, and {Pl}, it can potentially differ.\nIn the interest of brevity, we omit simulations of deep linear networks, as the extension\nfrom the single-layer case is straightforward. Instead, we proceed directly to deep nonlin-\near networks, where the impact of constrained error feedback presents more complex and\ninteresting dynamics.\n4.\nNonlinear networks and complex architectures\nAdapting our Restricted Adaptive Feedback (RAF) framework to nonlinear networks is\nstraightforward because the core principles of local learning and constrained error feedback\nremain applicable.\nThe local update rules in eq. (11) remain the same; the primary\ndifference lies in the introduction of nonlinear activation functions during the propagation\nof signals and errors.\nSpecifically, the forward and backward passes are modified as\nfollows:\nhl = f(Wlhl−1),\nδl = QlPlδl+1 ⊙f ′(hl),\n(12)\nwhere f is the nonlinear activation function applied element-wise, and f ′ is its deriva-\ntive.\nTo test whether our learning rule extends effectively from linear to nonlinear models, we\ntrained deep networks on the CIFAR-10 dataset. We used a simple nonlinear model with\nfour fully connected layers of 512 ReLU neurons each. While not state-of-the-art, this\nmodel provides a suitable testbed for evaluating our theory’s applicability to nonlinear\narchitectures and complex data.\nTo isolate the impact of feedback dimensionality, we applied the RAF rule in eq. (11),\n11\n\n\nconstraining the rank of feedback matrices in one specific layer at a time while leaving\nthe others unrestricted. We then measured the network’s test accuracy and compared it\nto backpropagation (BP) as a baseline. Figure 3a shows the accuracy as a function of the\nfeedback rank rl, with each curve representing a different constrained layer.\nConsistent with our findings for the linear model, constraining the feedback rank to r =\nd = 10, in any layer, match BP performance (Figure 3a), where d is the number of classes.\nInterestingly, the shallower layers performed well even under tighter rank constraints\n(rl < d), suggesting that the deeper layers compensate for the limited feedback in the\nearlier layers by effectively adjusting their weights.\nThis compensatory effect is possible because no information is lost during the feedforward\npass, unlike in a bottlenecked network. To demonstrate that the network still utilizes\nhigh-dimensional representations, we compared the performance of RAF-trained networks\nwith constrained feedback to narrower networks without rank restrictions (Figure 3b).\nThe results confirm that RAF-trained networks leverage their width to maintain high\nperformance despite feedback constraints.\nMoreover, as shown in Figure 3b, nonlinear networks trained with low-rank feedback in\nall layers using RAF can still match BP performance, demonstrating RAF’s robustness\neven with dimensional constraints across the entire network.\nTask dimensionality determines the minimal rank\nOur linear analysis suggests\nthat the error signal dimensionality needed for effective learning is tied to the loss gradient\ndimensionality, which depends on the number of classes in the data. To test this, we\ntrained networks on subsets of CIFAR-100 with 50, 75, and 100 classes, constraining the\nranks of all feedback matrices to r (Figure 3c).\nThe results show that network performance matches BP when the feedback rank equals\nthe number of classes (r = d). It indicates that the minimal rank required for effective\nlearning aligns with the task’s complexity, defined by the number of output classes.\nLow-dimensional error signal from the top layer\nOur theory shows how to propagate error signals from deeper layers to shallower ones using\nrestricted adaptive feedback. However, error projections can also bypass intermediate lay-\ners entirely, leading to different variants of Direct Restricted Adaptive Feedback (dRAF),\nanalogous to Direct Feedback Alignment [22]. For example, we can make direct connec-\ntions from the output or the penultimate layer to earlier layers, training these connections\nusing our algorithm (Figure 3d). As with RAF, this direct projection method matches\nthe performance of BP when the rank of the feedback matrices satisfies r ≥d. This model\nis particularly important because it is more flexible and has greater potential to explain\nlearning in the brain, where error signals may arrive from different pathways.\n12\n\n\n(b)\naccuracy\n(a)\nbackward rank\nBP\na\nOTNPSWx4zXQ=\">AB63icdVBNS8NAEJ34WetX1aOXxSJ4Cklb0noQCl48VrAf0Iay2W7apbtJ2N0IJfQvePGgiFf/kDf/jZu2go+GHi8N8PMvCDhTGnH+bDW1jc2t7YLO8X\ndvf2Dw9LRcUfFqS0TWIey16AFeUsom3NKe9RFIsAk67wfQ697v3V\nCoWR3d6lBf4HEQkawziV5Va0MS2XHdr1Go+Yix69Uq9mpNLz/GqyLWdBcqwQmtYeh+MYpIKGmnCsVJ910m0n2GpGeF0XhykiaYTPGY9g2NsKDKzxa3ztG5UYojKWpSKOF+n0iw0KpmQhMp8B6on57ufiX10912PAz\nFiWphFZLgpTjnSM8sfRiElK\nNJ8Zgolk5lZEJlhiok08RPC16fof9KpmKBs7ZWbnqrOApwCmdwAS7UoQk30I2EJjAzBsyWsR+vFel2rlmrmRP4AevtE7RijgM=</latexit>r = 32\no\nWizs9S6gf4bFgISNY5K8cr1hqezYrtdo1Fzk2FW3XqlXc3LpOV4VubazQBlWaA1L74NRTNKICk04VqrvOon2Myw1I5zOi4NU0QSTKR7TvqECR1T52eLWOTo3ygiFsTQlNFqo3ycyHCk1iwLTGWE9Ub+9XPzL6c6bPgZ\nE0mqSDLRWHKkY5R/jgaMU\nmJ5jNDMJHM3IrIBEtMtImnaEL4+hT9TzoVE5Tt3dbKTW8VRwFO4QwuwIU6NOEGWtAGAhN4gCd4tiLr0XqxXpeta9Zq5gR+wHr7BLdojgU=</latexit>r = 16\n<latexit sha1_base64=\"d7CdEgwPFxAVeDE\na5MaH+QR4shg=\">AB63icdVDLSgMxFM34rPVdekmWARXQ6Yt07oQCm5cVrAPaIeSTNtaJIZkoxQhv6CGxeKuPWH3Pk3ZtoKnrgwuGce7n3njDhTBuEPpy19Y3Nre3CTnF\n3b/gsHR03NFxqghtk5jHqhdiTmTtG2Y4bSXKIpFyGk3nF7nfveKs1\nieWdmCQ0EHksWMYJNLqkrDw1LZeR6fqNR8yByq169Uq/m5NJHfhV6LlqgDFZoDUvg1FMUkGlIRxr3fdQYoIMK8MIp/PiINU0wWSKx7RvqcSC6iBb3DqH51YZwShWtqSBC/X7RIaF1jMR2k6BzUT/9nLxL6+fmqgRZEwm\nqaGSLBdFKYcmhvnjcMQUJY\nbPLMFEMXsrJBOsMDE2nqIN4etT+D/pVGxQrn9bKzf9VRwFcArOwAXwQB0wQ1ogTYgYAIewBN4doTz6Lw4r8vWNWc1cwJ+wHn7BK5Qjf8=</latexit>r = 10\nFA\n512 neurons\n64\nneurons\nN\neWQ6OdJj9dvLxb+8bqHQZhRkaSaCLxYNEwZ1DHMH4cDKgnWbGoIwpKaWyEeI4mwNvGUTAhfn8L/SevMBGX7t16lfrWMowiOwDE4BS6ogTq4AQ3QBiMwQN4As8Wtx6tF+t10VqwljOH4Aest0/Jdo4b</latexit>r = 64\n(c)\nbackward rank\nBP\nBP\nBP\nbackward rank\naccuracy\nbackward rank\nBP\nr = n/2 r = n/4 r = n/8\nbackward rank\n(d) direct feedback (dRAF)\n(e) convolutional networks\n(f) transformer architecture\nFigure 3: Restricted Adaptive Feedback (RAF) efficiently trains nonlinear networks on\nCIFAR-10.\n(a) Constraining feedback in any layer to r = d = 10 matches full BP\nperformance. Different curves (B1, B2, and B3) correspond to restricting feedback at\ndifferent layers. (b) Reducing feedback dimensionality has minimal impact on perfor-\nmance, whereas reducing network width significantly degrades accuracy, indicating that\nhigh-dimensional representations are still utilized. (c) Minimal error dimensionality is\nsufficient for learning, as shown by subsampling classes from CIFAR-100. All feedback\nmatrices are constrained to the same rank r. (d) Direct RAF: all layers receive a low-\ndimensional error either directly from the output layer (solid) or from the penultimate\nlayer (dashed), both converging to BP-level performance. (e) Performance of a 4-block\nVGG-like convolutional network trained with RAF, constraining the layers with 512 chan-\nnels. Inset: Training the same network with all layers constrained to a fraction of their\nsize; the smallest layer has 64 channels. (f) A transformer-based architecture trained to\nclassify images, demonstrating that RAF generalizes to more advanced network architec-\ntures. See Supplemental Information for details on all implementations.\n13\n\n\nConvolutional neural networks\nOur previous results demonstrate that fully connected networks can learn effectively from\nminimal error signals, even on complex datasets, matching backpropagation (BP) perfor-\nmance. Here, we extend this investigation to convolutional architectures.\nTraining convolutional networks with Feedback Alignment (FA) is notoriously difficult [8,\n23]. Recent work has made progress by learning feedback weights within the FA frame-\nwork [24], but our approach differs by restricting the error signal’s dimensionality using\nRestricted Adaptive Feedback (RAF). We aim to determine whether convolutional net-\nworks can also benefit from low-dimensional error feedback.\nWe trained a VGG-like convolutional network [25] with four blocks and batch normal-\nization on the CIFAR-10 dataset (see Appendix for details). Using RAF, we decoupled\nthe error propagation from the feedforward pass in all layers. Initially, we constrained\nthe feedback error only in the blocks containing 512 (Figure 3e). Consistent with our\nfindings in fully connected networks, convolutional networks learn well with a feedback\nmatrix with a rank similar to the number of classes, d = 10.\nTo test whether convolutional networks can train when constraining all feedback paths,\nwe further restricted each block to have feedback matrices with ranks equal to 1/2, 1/4,\nor 1/8 of the block width (Figure 3e inset). Our results indicate that reducing the error\ndimensionality has a minimal impact on performance, except in the most extreme case.\nSpecifically, constraining the feedback rank to 1/8 of the block width resulted in a notice-\nable drop in performance. This finding aligns with our previous results, as the layer with\n64 channels received feedback with a rank of r = 8. Overall, our results demonstrate that\nconvolutional networks can be efficiently trained using a minimal error signal.\nTransformers\nTransformers have become a dominant architecture in natural language processing and,\nmore recently, have achieved state-of-the-art performance in image classification tasks [26,\n27]. Unlike convolutional and standard feedforward architectures, transformers rely on\na self-attention mechanism that computes interactions between tokens through the mul-\ntiplication of key, query, and value matrices [28]. These operations introduce complex,\nnonlinear dependencies that make it unclear whether low-dimensional error feedback, as\nproposed in our framework, can effectively support learning in transformer models.\nTo investigate this question, we applied our approach to visual transformer architec-\ntures trained for image recognition on the CIFAR-10 dataset. Each weight matrix in the\ntransformer architecture, including key, query, and value matrices, was trained using low-\ndimensional error feedback from the subsequent layer. Feedback matrices were trained\nusing the RAF framework (see the appendix for details). The network’s performance as\na function of the feedback rank, which was kept identical for all matrices, is shown in\n14\n\n\nFigure 3f.\nOur results indicate that, consistent with our findings in both nonlinear and convolu-\ntional networks, an error signal with dimensionality as low as the task dimensionality (i.e.\nthe number of output classes) is sufficient to train transformer architectures effectively\n(Figure 3f). Despite the intricate nature of self-attention computations, training with\nlow-dimensional feedback yielded performance comparable to conventional backpropaga-\ntion on CIFAR-10 with rank r = 10. Notably, training transformers with low-dimensional\nfeedback using RAF not only matched but, in some cases, consistently outperformed\nbackpropagation by a small margin. This improvement was observed even when hyperpa-\nrameters were independently optimized for both BP and RAF. The improved generaliza-\ntion could be understood as a regularization effect achieved by constraining the feedback.\nHowever, since we observe it only in the transformer architecture, it is not a general\nproperty of a low-dimensional error signal, and we leave further investigation to a future\nstudy.\n5.\nError dimensionality shapes neural receptive fields\nWe have shown that neural networks can be efficiently trained using minimal error signals\ncomparable to task dimensionality. Here, we investigate how error dimensionality affects\nneural representations, providing insights into receptive fields observed in the brain.\nLindsey et al. found that in convolutional models of the visual system, narrow feedforward\nbottlenecks between the retina and the brain led to center-surround receptive fields in the\nretinal layer, similar to mammals [29]. Wider bottlenecks resulted in orientation-selective\nreceptive fields, as seen in salamanders. We hypothesize that these effects are due to\nconstraints on the error signal reaching the retina, rather than the feedforward bottlenecks\nthemselves.\nTo test this hypothesis, we trained a model similar to that of Lindsey et. al\n[29] but\nwith full-width layers throughout. Instead of constraining the feedforward pathway, we\nconstrained only the error signal using a low-rank feedback matrix trained with RAF\n(Figure 4c), thereby isolating the impact of error dimensionality on neural representa-\ntions. We extracted the receptive fields of neurons in the retinal layer using visualization\ntechniques [30] (Figure 4d).\nConsistent with our hypothesis, constraining the feedback rank led to the emergence of\ncenter-surround receptive fields in the retinal layer. To further validate our hypothesis,\nwe trained a model that included the feedforward bottleneck, similar to [29] but used\ndRAF to train the retinal layer without restricting the backward pathway. In line with\nour expectations, the retinal receptive fields exhibited orientation selectivity (see the\nappendix).\n15\n\n\nr = 2\nr = 4\nr = 32\ndog\ncat\ndeer\nfrog\nretina\nVVS\n \ndog\ncat\ndeer\nfrog\nretina\nVVS\n \ndRAF\ndog\ncat\ndeer\nfrog\nretina\nVVS\n(a)\nNo bottleneck\nBottleneck layer\nBottleneck in error feedback\n(d) Bottleneck only in \nfeedforward (full error)\nOriented\nreceptive ﬁelds\ncenter-surround\n receptive ﬁelds\nOriented\nreceptive ﬁelds\ncenter-surround\n receptive ﬁelds\n(b) training with backpropagation\n(c) Training with RAF\nFigure 4: Receptive fields (RFs) are shaped by feedback dimensionality rather than feedfor-\nward constraints. (a) Model of the early ventral visual stream. Top: Retinal-to-cortical\nconnectivity is modeled as a bottleneck in the network; adapted from [29]. Middle: A\nbottleneck is applied only to the feedback connections, restricting error dimensionality.\nBottom: Using direct RAF (dRAF), retinal convolutional layers receive error signals from\nthe penultimate layer, enabling full error feedback even with a feedforward bottleneck.\n(b) Receptive fields (RFs) in a model trained with BP, corresponding to the setup in\n(a) top. Left: No bottleneck. Right: Bottleneck at the retinal output, which induces\ncenter-surround RFs. (c) Training the retinal output with RAF. Restricting the rank of\nthe feedback matrix produces center-surround RFs. (d) RFs in a network with a bottle-\nneck at the retinal output but full-dimensional error feedback via dRAF. The presence of\nhigh-dimensional error signals results in oriented RFs.\n16\n\n\nThis experiment demonstrates that error dimensionality influences neuronal tuning and\nneural representations. Specifically, lower-dimensional error signals promote higher sym-\nmetries in emergent receptive fields. Our findings underscore the importance of consider-\ning the dimensionality and pathways of error signals when studying neural computations\nin the brain.\n6.\nDiscussion\nOur work demonstrates that neural networks can be trained effectively using minimal error\nsignals constrained to the task’s intrinsic dimensionality rather than the higher dimen-\nsionality of the network’s representations. By adopting a factorized version of Feedback\nAlignment with low-rank matrices and training both left and right spaces of the feedback\nmatrix, we showed that deep networks—linear, nonlinear, and convolutional—can match\nthe performance of full backpropagation even under stringent error-dimensionality con-\nstraints. This finding highlights that the essential information required for learning is tied\nto the complexity of the task, measured by the number of output classes, and does\nnot scale with the number of parameters or size of the model. Additionally, we\nrevealed that constraining error dimensionality influences neural representations, provid-\ning a potential explanation for biological phenomena, such as center-surround receptive\nfields in the retina.\nThis work aims to explore potential mechanisms for implementing gradient descent in the\nbrain. Recognizing that high-dimensional feedback is not necessary for effective learning\nis a significant step toward developing more flexible and biologically realistic models of\nlearning. This insight suggests that the brain might utilize low-dimensional error signals\nto drive learning processes, aligning with anatomical and physiological constraints.\nBridging biological constraints and gradient-based learning.\nOur work aims to\nreconcile gradient-based learning with the anatomical constraints of cortical circuits. In\nthe cortex, feedback connections are generally sparser, less structured, and more selective\nthan their feedforward counterparts [31], suggesting that learning may rely on a restricted\ntop-down error signal.\nSeveral alternative frameworks have been proposed to explain\nlearning in hierarchical biological circuits. Predictive coding models suggest that the cor-\ntex continuously generates predictions about sensory input and computes discrepancies,\nor prediction errors, between expected and actual signals [32, 33] . Equilibrium prop-\nagation [34] offers another biologically inspired alternative, demonstrating how energy-\nbased models can approximate backpropagation through local weight updates.\nWhile\nthese approaches provide valuable insights into neural computation, they deviate from\nthe conventional framework of gradient-based optimization. In contrast, our approach\nremains firmly within the gradient descent paradigm, allowing direct loss minimization.\nThis key property enables its seamless application across diverse architectures, from fully\n17\n\n\nconnected nonlinear networks to convolutional models and transformer-based attention\nmechanisms.\nBetween reward-based learning and full gradient descent.\nBy showing that deep\nnetworks can be trained with low-dimensional error signals, our framework bridges the gap\nbetween gradient-based learning and reward-based learning. Studies have suggested that\ndopaminergic neurons in the ventral tegmental area (VTA) encode reward signals that\ncapture multiple task attributes rather than a single scalar value [35–37]. Our findings\nprovide a theoretical foundation for moving beyond scalar reward-based learning toward\na framework where low-dimensional reward signals carry richer information about the un-\nderlying loss (or gain). This perspective positions our model as an intermediary between\nreinforcement learning’s scalar feedback and the full backpropagation of error gradients.\nMore broadly, our results suggest that learning could emerge through the strategic com-\npression of error signals into task-relevant representations, offering a biologically plausible\nalternative to high-dimensional gradient propagation.\nAligning to the error space.\nOn its surface, our novel learning rule is similar to\nprevious Feedback Alignment (FA) schemes. However, it offers a conceptual novelty. In\ntraditional FA, learning the feedback weights aims to align them with the feedforward\nweights [5, 10], mirroring full backpropagation. In contrast, by factorizing the feedback\nmatrix B = QP, we also align the row space of the feedback matrix with the\nsource of the error, thereby improving the quality of the error signal itself.\nComputational complexity.\nDespite the additional requirement of training the feed-\nback matrices, the computational cost of using low-rank feedback does not increase signifi-\ncantly. The updates for the low-rank matrices are linear in the number of neurons or filters\nper layer, while the forward pass and the primary weight updates remain quadratic [38].\nThis efficiency makes our approach practical for large-scale networks and suggests that\nsimilar mechanisms could be feasible in biological neural networks.\nGradient descent in high dimensions.\nOur findings invite a rethinking of gradient\ndescent dynamics in large, overparameterized networks. Typically, the weight dynam-\nics during training are high-dimensional [39].\nHowever, when the error signal is low-\ndimensional, the weight updates in each layer are confined to a much lower-dimensional\nsubspace. This constraint could have implications for understanding the generalization\ncapabilities of neural networks, as it suggests that effective learning does not require\nexploring the full parameter space. Indeed, previous studies found that the Hessian of\nthe loss function shows low-rank structure [40]. Exploring this connection could open\nnew avenues for understanding the dynamics of gradient descent in high-dimensional loss\nspaces [41].\n18\n\n\nExploring the potential of low-dimensional feedback in more complex archi-\ntectures.\nThis approach not only improves learning efficiency but may also act as a\nform of regularization, as can be seen from the improved performance of low-rank feed-\nback matrices on visual transformers—the possibility of using restricted error signals as\na form of regularization warrants further investigation.\nIn summary, our findings not only challenge the conventional reliance on high-dimensional\nerror propagation but also pave the way for the development of biologically inspired, com-\nputationally efficient learning algorithms that hold promise for advancing both artificial\nintelligence and our understanding of brain function.\" This would leave the reader with\na clear impression of both the scientific and translational impact of your work\nAcknowledgments\nThis work was supported by the Gatsby Charitable Foundation and the Azrieli Founda-\ntion.\nReferences\n[1] Rumelhart DE, Hinton GE, Williams RJ (1986) Learning representations by back-\npropagating errors. nature 323(6088):533–536.\n[2] Chinta LV, Tweed DB (2012) Adaptive optimal control without weight transport.\nNeural computation 24(6):1487–1518.\n[3] Stork (1989) Is backpropagation biologically plausible? in International 1989 Joint\nConference on Neural Networks. (IEEE), pp. 241–246.\n[4] Lillicrap TP, Santoro A, Marris L, Akerman CJ, Hinton G (2020) Backpropagation\nand the brain. Nature Reviews Neuroscience 21(6):335–346.\n[5] Lillicrap TP, Cownden D, Tweed DB, Akerman CJ (2016) Random synaptic feedback\nweights support error backpropagation for deep learning. Nature communications\n7(1):13276.\n[6] Grossberg S (1987) Competitive learning: From interactive activation to adaptive\nresonance. Cognitive science 11(1):23–63.\n[7] Crick F (1989) The recent excitement about neural networks. Nature 337(6203):129–\n132.\n[8] Bartunov S, et al. (2018) Assessing the scalability of biologically-motivated deep\nlearning algorithms and architectures. Advances in neural information processing\nsystems 31.\n19\n\n\n[9] Kolen JF, Pollack JB (1994) Backpropagation without weight transport in Proceed-\nings of 1994 IEEE International Conference on Neural Networks (ICNN’94). (IEEE),\nVol. 3, pp. 1375–1380.\n[10] Akrout M, Wilson C, Humphreys P, Lillicrap T, Tweed DB (2019) Deep learning\nwithout weight transport. Advances in neural information processing systems 32.\n[11] Crafton B, Parihar A, Gebhardt E, Raychowdhury A (2019) Direct feedback align-\nment with sparse connections for local learning. Frontiers in neuroscience 13:450947.\n[12] Liao Q, Leibo J, Poggio T (2016) How important is weight symmetry in backpropa-\ngation? in Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30.\n[13] Gunasekar S, Lee JD, Soudry D, Srebro N (2018) Implicit bias of gradient descent\non linear convolutional networks. Advances in neural information processing systems\n31.\n[14] Caro JO, et al. (2024) Translational symmetry in convolutions with localized kernels\ncauses an implicit bias toward high frequency adversarial examples. Frontiers in\nComputational Neuroscience 18.\n[15] Patel N, Shwartz-Ziv R (2024) Learning to compress: Local rank and information\ncompression in deep neural networks. arXiv preprint arXiv:2410.07687.\n[16] Saxe AM, McClelland JL, Ganguli S (2013) Exact solutions to the nonlinear dynamics\nof learning in deep linear neural networks. arXiv preprint arXiv:1312.6120.\n[17] Johnson WB (1984) Extensions of lipshitz mapping into hilbert space in Conference\nmodern analysis and probability, 1984. pp. 189–206.\n[18] Oja E (1982) Simplified neuron model as a principal component analyzer. Journal\nof mathematical biology 15:267–273.\n[19] Clopath C, Büsing L, Vasilaki E, Gerstner W (2010) Connectivity reflects coding: a\nmodel of voltage-based stdp with homeostasis. Nature neuroscience 13(3):344–352.\n[20] Turrigiano GG (2008) The self-tuning neuron:\nsynaptic scaling of excitatory\nsynapses. Cell 135(3):422–435.\n[21] Pehlevan C, Hu T, Chklovskii DB (2015) A hebbian/anti-hebbian neural network for\nlinear subspace learning: A derivation from multidimensional scaling of streaming\ndata. Neural computation 27(7):1461–1495.\n[22] Nøkland A (2016) Direct feedback alignment provides learning in deep neural net-\nworks. Advances in neural information processing systems 29.\n20\n\n\n[23] Launay J, Poli I, Krzakala F (2019) Principled training of neural networks with direct\nfeedback alignment. arXiv preprint arXiv:1906.04554.\n[24] Bacho F, Chu D (2024) Low-variance forward gradients using direct feedback align-\nment and momentum. Neural Networks 169:572–583.\n[25] Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556.\n[26] Dosovitskiy A, et al. (2020) An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929.\n[27] Khan S, et al. (2022) Transformers in vision: A survey. ACM computing surveys\n(CSUR) 54(10s):1–41.\n[28] Vaswani A, et al. (2017) Attention is all you need. Advances in neural information\nprocessing systems 30.\n[29] Lindsey J, Ocko SA, Ganguli S, Deny S (2019) A unified theory of early visual\nrepresentations from retina to cortex through anatomically constrained deep cnns.\narXiv preprint arXiv:1901.00945.\n[30] Erhan D, Bengio Y, Courville A, Vincent P (2009) Visualizing higher-layer features\nof a deep network. University of Montreal 1341(3):1.\n[31] Markov NT, et al. (2014) Anatomy of hierarchy: feedforward and feedback pathways\nin macaque visual cortex. Journal of comparative neurology 522(1):225–259.\n[32] Rao RP, Ballard DH (1999) Predictive coding in the visual cortex: a functional inter-\npretation of some extra-classical receptive-field effects. Nature neuroscience 2(1):79–\n87.\n[33] Whittington JC, Bogacz R (2017) An approximation of the error backpropagation al-\ngorithm in a predictive coding network with local hebbian synaptic plasticity. Neural\ncomputation 29(5):1229–1262.\n[34] Scellier B, Bengio Y (2017) Equilibrium propagation: Bridging the gap between\nenergy-based models and backpropagation. Frontiers in computational neuroscience\n11:24.\n[35] Cohen JY, Haesler S, Vong L, Lowell BB, Uchida N (2012) Neuron-type-specific sig-\nnals for reward and punishment in the ventral tegmental area. nature 482(7383):85–\n88.\n[36] Parker NF, et al. (2016) Reward and choice encoding in terminals of midbrain\ndopamine neurons depends on striatal target. Nature neuroscience 19(6):845–854.\n21\n\n\n[37] Engelhard B, et al. (2019) Specialized coding of sensory, motor and cognitive variables\nin vta dopamine neurons. Nature 570(7762):509–513.\n[38] Livni R, Shalev-Shwartz S, Shamir O (2014) On the computational efficiency of\ntraining neural networks. Advances in neural information processing systems 27.\n[39] Jacot A, Gabriel F, Hongler C (2018) Neural tangent kernel: Convergence and gen-\neralization in neural networks. Advances in neural information processing systems\n31.\n[40] Sagun L, Bottou L, LeCun Y (2017) Empirical analysis of the hessian of over-\nparameterized neural networks in Proceedings of the International Conference on\nLearning Representations (ICLR).\n[41] Zhang C, Bengio S, Hardt M, Recht B, Vinyals O (2017) Understanding deep learning\nrequires rethinking generalization in Proceedings of the International Conference on\nLearning Representations (ICLR).\n22\n\n\nAppendix\nA.\nFull linear theory\nin this section, we provide the analysis of the linear networks for both Feedback Alignment\nand RAF\nDetailed analysis of Feedback Alignment learning dynamics\nFrom eq. (4) in the main text, the weight updates for a single sample µ are given by:\n∆W µ\n1 = ηB (yµ −W2W1xµ) xµ⊤,\n∆W µ\n2 = η (yµ −W2W1xµ) xµ⊤W ⊤\n1 ,\n(A1)\nwhere:\n• η is the learning rate,\n• xµ ∈Rn is the input vector for sample µ,\n• yµ ∈Rm is the corresponding target output,\n• W1 ∈Rk×n and W2 ∈Rm×k are the weight matrices,\n• B ∈Rk×m is a predefined matrix (e.g., a feedback or scaling matrix).\nWe introduce the empirical covariance matrices:\nΣio = 1\np\np\nX\nµ=1\nyµxµ⊤,\n(A2)\nΣoo = 1\np\np\nX\nµ=1\nyµyµ⊤,\n(A3)\nΣii = 1\np\np\nX\nµ=1\nxµxµ⊤= I,\n(A4)\nwhere Σii = I assumes that the input vectors are whitened (i.e., have unit covariance).\nSumming over all p training examples, we obtain the average weight updates:\n∆W1 = η\np\np\nX\nµ=1\n∆W µ\n1\n= η\np\np\nX\nµ=1\nB (yµ −W2W1xµ) xµ⊤.\n(A5)\n23\n\n\nUsing these definitions, the update for W1 simplifies to:\n∆W1 = ηB (Σio −W2W1Σii)\n(A6)\n= ηB (Σio −W2W1) .\n(A7)\nUnder the limit as η →0 with η = dt\nτ (where τ is a time constant), we transition from\ndiscrete updates to continuous-time dynamics:\nτ dW1\ndt\n= B (Σio −W2W1) ,\n(A8)\nwhich matches the weight dynamics presented in eq. (5) of the main text.\nSimilarly, the update for W2 becomes:\n∆W2 = η\np\np\nX\nµ=1\n∆W µ\n2\n(A9)\n= η\np\np\nX\nµ=1\n(yµ −W2W1xµ) xµ⊤W ⊤\n1\n(A10)\n= η (Σio −W2W1) W ⊤\n1 ,\n(A11)\nwhich simplifies under the same limit to:\nτ dW2\ndt\n= (Σio −W2W1) W ⊤\n1 .\n(A12)\nEq. (A8) and eq. (A12) describe the continuous-time dynamics of the weights W1 and\nW2 under the given learning rule.\nWe consider the singular value decomposition (SVD) of the covariance matrix Σio:\nΣio = USV ⊤,\n(A13)\nwhere:\n• U ∈Rm×d and V ∈Rn×d are matrices with orthonormal columns,\n• S ∈Rd×d is a diagonal matrix containing the singular values,\n• d is the rank of Σio.\nWe perform a rotation of the weight matrices and B as follows:\nW1 = ¯W1V ⊤,\nW2 = U ¯W2,\nB = ¯BU ⊤.\n(A14)\n24\n\n\nSubstituting these into the previous weight dynamics, we have for W1:\nτ dW1\ndt\n= B (Σio −W2W1)\n(A15)\n= ¯BU ⊤\u0000USV ⊤−U ¯W2 ¯W1V ⊤\u0001\n.\n(A16)\nSince U ⊤U = I (due to orthonormal columns of U), we can simplify:\nτ dW1\ndt\n= ¯B\n\u0000U ⊤U\n\u0001 \u0000S −¯W2 ¯W1\n\u0001\nV ⊤\n(A17)\n= ¯B\n\u0000S −¯W2 ¯W1\n\u0001\nV ⊤.\n(A18)\nRecognizing that W1 = ¯W1V ⊤, we can write dW1\ndt = d ¯\nW1\ndt V ⊤. Thus, multiplying both sides\non the right by V (since V ⊤V = I):\nτ d ¯W1\ndt\n= ¯B\n\u0000S −¯W2 ¯W1\n\u0001\n.\n(A19)\nSimilarly, for W2:\nτ dW2\ndt\n= (Σio −W2W1) W ⊤\n1\n(A20)\n=\n\u0000USV ⊤−U ¯W2 ¯W1V ⊤\u0001 \u0000 ¯W1V ⊤\u0001⊤\n(A21)\n(A22)\nUsing the fact that V ⊤V = I we simplify:\nτ dW2\ndt\n=\n\u0000US −U ¯W2 ¯W1\n\u0001 ¯W ⊤\n1\n(A23)\n(A24)\nSince W2 = U ¯W2, we have dW2\ndt = U d ¯\nW2\ndt . Multiplying both sides on the left by U ⊤:\nτ d ¯W2\ndt\n=\n\u0000S −¯W2 ¯W1\n\u0001 ¯W ⊤\n1 .\n(A25)\nEqs. (A19) and (A25) describe the dynamics of the rotated weights ¯W1 and ¯W2:\nτ d ¯W1\ndt\n= ¯B\n\u0000S −¯W2 ¯W1\n\u0001\n,\nτ d ¯W2\ndt\n=\n\u0000S −¯W2 ¯W1\n\u0001 ¯W ⊤\n1 .\n(A26)\n25\n\n\nFull derivation of Restricted Adaptive Feedback (RAF) learning\ndynamics\nWe consider an alternative algorithm where the matrix B is replaced by B = QP, with\nQ ∈Rk×r and P ∈Rr×m.\nThe updates for Q and P for a single sample µ are given by:\n∆Qµ = ηW1xµP (yµ −W2W1xµ) ,\n∆P µ = ηPyµyµ⊤\u0000I −P ⊤P\n\u0001\n.\n(A27)\nSumming over all p training examples, we obtain the average updates:\n∆Q = η\np\np\nX\nµ=1\n∆Qµ = ηW1\n \n1\np\np\nX\nµ=1\nxµ (yµ −W2W1xµ)⊤P ⊤\n!\n,\n∆P = η\np\np\nX\nµ=1\n∆P µ = ηP\n \n1\np\np\nX\nµ=1\nyµyµ⊤\n!\n\u0000I −P ⊤P\n\u0001\n.\n(A28)\nWe simplify the update for Q:\n∆Q = ηW1\n  \n1\np\np\nX\nµ=1\nxµ (yµ −W2W1xµ)⊤\n!\nP ⊤\n!\n(A29)\n= ηW1\n\u0000\u0000Σ⊤\nio −W ⊤\n1 W ⊤\n2 Σii\n\u0001\nP ⊤\u0001\n(A30)\n= ηW1\n\u0010\n(Σio −W2W1)⊤P ⊤\u0011\n(A31)\nSimilarly, the update for P simplifies to:\n∆P = ηPΣoo\n\u0000I −P ⊤P\n\u0001\n.\n(A32)\nThus, the updates for Q and P become:\n∆Q = ηW1 (Σio −W2W1)⊤P ⊤,\n∆P = ηPΣoo\n\u0000I −P ⊤P\n\u0001\n.\n(A33)\nUnder the continuous-time assumption, where η →0 with η = dt\nτ , the updates for Q and\nP become differential equations:\nτ dQ\ndt = W1 (Σio −W2W1)⊤P ⊤,\n(A34)\nand\nτ dP\ndt = PΣoo\n\u0000I −P ⊤P\n\u0001\n.\n(A35)\n26\n\n\nFinally, substituting B = QP into the update for W1 from eq. A8, we find that the\ndynamics for W1 become:\nτ dW1\ndt\n= QP (Σio −W2W1)\n(A36)\nWe perform rotations similar to before:\nW1 = ¯W1V ⊤,\nW2 = U ¯W2,\nP = ¯PU ⊤.\n(A37)\nSubstituting for Q we get:\nτ dQ\ndt = ¯W1V ⊤\n1\n\u0000USV ⊤−U ¯W2 ¯W1V ⊤\u0001⊤(PU ⊤)⊤\n(A38)\n= ¯W1V ⊤\n1 V\n\u0000S −¯W2 ¯W1\n\u0001T U ⊤U ¯P ⊤\n(A39)\n= ¯W1\n\u0000S −¯W2 ¯W1\n\u0001⊤¯P ⊤\n(A40)\nSubstituting these rotations into the previous derivations, we update the dynamics.\nFor W1, the update equation is:\nτ dW1\ndt\n= B (Σio −W2W1) .\n(A41)\nSince B = QP = Q ¯PU ⊤, W1 = ¯W1V ⊤, W2 = U ¯W2, and Σio = USV ⊤, we have:\nτ d( ¯W1V ⊤)\ndt\n= Q ¯PU ⊤\u0000USV ⊤−U ¯W2 ¯W1V ⊤\u0001\n(A42)\n= Q ¯P\n\u0000SV ⊤−¯W2 ¯W1V ⊤\u0001\n.\n(A43)\nSince V ⊤is constant, we can write:\nτ d ¯W1\ndt V ⊤= Q ¯P\n\u0000SV ⊤−¯W2 ¯W1V ⊤\u0001\n.\n(A44)\nMultiplying both sides on the right by V (using V ⊤V = I):\nτ d ¯W1\ndt\n= Q ¯P\n\u0000S −¯W2 ¯W1\n\u0001\n.\n(A45)\n27\n\n\nSubstituting P = ¯PU ⊤and Σoo = US2U ⊤in (A35), we get:\nτ d( ¯PU ⊤)\ndt\n= ¯PU ⊤\u0000US2U ⊤\u0001 \u0000I −U ¯P ⊤¯PU ⊤\u0001\n(A46)\n= ¯PS2 \u0000I −¯P ⊤¯P\n\u0001\nU ⊤.\n(A47)\nMultiplying both sides on the right by U (since U ⊤U = I):\nτ d ¯P\ndt = ¯PS2 \u0000I −¯P ⊤¯P\n\u0001\n.\n(A48)\nIn summary, under the rotations, the updated dynamics are:\nτ d ¯W1\ndt\n= Q ¯P\n\u0000S −¯W2 ¯W1\n\u0001\n,\nτ d ¯W2\ndt\n=\n\u0000S −¯W2 ¯W1\n\u0001 ¯W ⊤\n1 ,\nτ d ¯P\ndt = ¯PS2 \u0000I −¯P ⊤¯P\n\u0001\n.\nτ d ¯Q\ndt = ¯W1\n\u0000S −¯W2 ¯W1\n\u0001⊤¯P ⊤\n(A49)\nB.\nLearning rule implementation\nOur implementation and optimization were conducted entirely using PyTorch. For the\nRAF layers, we initialized all weights using Kaiming uniform initialization. We modified\nthe backward pass by adjusting the gradients with respect to the input, ensuring they\nalign with our proposed update rule. In the output layer, we learn the projection matrix\nP to capture the principal directions of the target labels y. In the hidden layers, P is\nlearned to project onto the principal directions of the error signal from the subsequent\nlayer, represented as δl+1.\nSpecifically, for a layer l with input dimension n, output\ndimension m, and rank constraint r, we proceed as follows:\n28\n\n\nAlgorithm 1: Modified Backward Pass for RAF Layer\nInput: Error signal δl+1 ∈Rb×m, activations hl ∈Rb×n, matrices Ql ∈Rn×r,\nPl ∈Rr×m, weight matrix Wl ∈Rm×n\nOutput: Gradient w.r.t. input grad_input, updates ∆Ql, ∆Pl, ∆Wl\nCompute covariance matrix:\nif use_targets then\nC ←(y −¯y)⊤(y −¯y);\nelse\nC ←(δl+1 −\n¯\nδl+1)⊤(δl+1 −\n¯\nδl+1);\nγ ←max (diag (C));\nCompute ’gradient’ w.r.t. input:\ngrad_input ←\n\u0000QlPlδ⊤\nl+1\n\u0001⊤∈Rb×n;\nCompute updates for Ql and Pl:\n∆Ql ←h⊤\nl\n\u0000Plδ⊤\nl+1\n\u0001⊤;\n∆Pl ←Pl\n\u0012C\nγ\n\u0013 \u0000I −P ⊤\nl Pl\n\u0001\n;\nCompute update for Wl:\n∆Wl ←δ⊤\nl+1hl;\nThe derivation of the dRAF (directed restricted adaptive feedback) follows the same\nprocedure, with the key difference being that the error signal δ does not necessarily\noriginate from the next layer; instead, it can come from any subsequent layer.\nC.\nNumerical experiments\nLayer-Wise Constraints\nIn Figure [3.a], we present the results of training a network with four hidden layers, each\ncontaining 512 neurons, and an output layer with 10 neurons on the CIFAR-10 dataset.\nThe network was trained using RAF, as described earlier, without rank constraints, except\nfor one layer at a time. For comparison, we also trained the network using standard\nbackpropagation as a baseline.\nAll networks were trained with a batch size of 32, a\nlearning rate of 6 × 10−4, and weight decay of 4 × 10−4.\nThe Adam optimizer with\nAMSGrad was used, with training conducted for 160 epochs and an exponential learning\nrate decay factor of 0.975. Each experiment was repeated 10 times.\nConstraining All Layers\nFor the results shown in Figure [3.b], we trained the same network with four hidden\nlayers, each containing 512 neurons. This time, we applied rank constraints to all layers\nsimultaneously, with rank values r = 64, 32, 16, 10.\n29\n\n\nTo further demonstrate that the network still utilizes high-rank representations, we also\ntrained a variant of the network with 64 neurons in each hidden layer, without applying\nany rank constraints. All training was conducted with a batch size of 32, a learning rate\nof 6×10−4, and weight decay of 4×10−4. The Adam optimizer with AMSGrad was used,\nwith training carried out for 160 epochs and an exponential learning rate decay factor of\n0.975. Each experiment was repeated 10 times.\nCIFAR-100 Sub-sampling\nFor the results shown in Figure [3.c], we trained the same model on the CIFAR-100\ndataset, sampling different numbers of classes d, with d = 50, 75, 100.\nFor each sub-\nsample, we trained the model while applying rank constraints to all layers, using various\nrank values. This was done to demonstrate that the dimensionality of the error signal\ndepends on the task dimensionality d. All optimizations were performed with a batch size\nof 32, a learning rate of 6×10−4, and weight decay of 4×10−4. The Adam optimizer with\nAMSGrad was used, with training conducted for 160 epochs and an exponential learning\nrate decay factor of 0.975. Each training run was repeated 5 times.\ndRAF\nFor the results shown in Figure [3.d], we trained the same model using dRAF. In one\nexperiment, we propagated the error signal from the last layer to all preceding layers. In\na separate experiment, we propagated the error signal directly from the penultimate layer\nto all earlier layers, applying rank constraints to these layers while keeping the last layer\nat full rank. All optimizations were performed with a batch size of 32, a learning rate of\n6 × 10−4, and weight decay of 4 × 10−4. The Adam optimizer with AMSGrad was used,\ntraining for 160 epochs with an exponential learning rate decay factor of 0.975. Each\ntraining run was repeated 5 times\nConvolutional Neural Networks\nTo extend our RAF algorithm to convolutional layers, we apply the rank constraint to\nthe number of channels in the error signal. This effectively constrains the dimensionality\nof the error signal across the spatial dimensions. Similar to the implementation for fully\nconnected layers in Algorithm 1, we modify the backward pass in the same way for\nconvolutional layers.\nThe key difference in the convolutional context is how the projection matrix P operates\non the error signals. In convolutional layers, P functions as a 1 × 1 convolutional filter,\nprojecting the error signal at each spatial location from m channels down to r chan-\nnels. This reduces the error signal’s dimensionality to r per pixel, adhering to the rank\nconstraint.\n30\n\n\nAs in the fully connected case, the matrix P is learned using Oja’s rule. However, the\ncovariance matrix C is computed over both the batch and spatial dimensions—that is,\nacross all pixels in all images within the batch. This approach captures the covariance\nstructure of pixel representations more effectively, enabling P to project the error signals\nappropriately in the convolutional setting.\nVGG-like Architecture\nWe trained a VGG-like convolutional neural network comprising four convolutional blocks,\nas detailed in Table C1. For the experiments presented in Figure 4(a), a rank constraint\nwas applied exclusively to the final convolutional block, which consists of 512 channels.\nIn contrast, for the results shown in Figure 4(b), rank constraints were imposed on all\nlayers, reducing the rank to 1/2, 1/4, and 1/8 of the original channel dimensions.\nEach network was trained using the Adam optimizer with a learning rate of 5 × 10−4, a\nweight decay of 5 × 10−5, and an exponential learning rate decay factor of 0.98. Train-\ning was conducted for 250 epochs, with each experiment repeated five times to ensure\nrobustness.\nVision Transformer (ViT)\nWe implemented a Vision Transformer (ViT) model that consists of seven self-attention\nblocks, each with an embedding dimension of 384 and four attention heads. The input\nimages were partitioned into nonoverlapping patches, which were linearly projected into\na 384-dimensional feature space. These patch embeddings were augmented with learned\npositional encodings and processed through a stack of seven transformer encoder lay-\ners. Each encoder block employed multi-head self-attention with four heads, followed by\npre-norm layer normalization and dropout for regularization. The final representation\nwas obtained from a learned class token, which was subsequently processed by a fully\nconnected layer for classification.\nTraining was carried out using a batch size of 32, a learning rate of 3×10−4, and a weight\nloss of 1 × 10−4. We used the Adam optimizer with AMSGrad, training for 150 epochs\nwith an exponential learning rate decay factor of 0.98. Each experiment was repeated five\ntimes to ensure robustness.\nNeural Receptive Fields\nTo analyze neural receptive fields, we trained the same convolutional network architecture\nas used in Lindsey et. al. [29]. The model consists of two convolutional layers with\nReLU activations to simulate retinal processing, followed by three additional convolutional\nlayers with ReLU activations to model the ventral visual stream (VVS). Fully connected\nlayers were used for classification, with the complete architecture detailed in Table C2. A\n31\n\n\nbottleneck was introduced between the retinal and VVS layers to constrain the information\nflow.\nTraining was performed using the Restricted Adaptive Feedback (RAF) method, where\nrank constraints were applied to feedback sent to the retina with ranks r = 2, 4, 32. The\nnetwork was trained with hyperparameters similar to those in Linsdey et. al. [29], using\nthe RMSProp optimizer with a learning rate of 1 × 10−4, a weight decay of 1 × 10−5,\nand an exponential learning rate decay factor of 0.985. Each model was trained for 120\nepochs, and all experiments were repeated five times to ensure robustness.\nAdditionally, we conducted experiments with dynamic Restricted Adaptive Feedback\n(dRAF), where feedback to the retina originated from higher visual layers while feed-\nforward connections within the retina were constrained to four channels.\n32\n\n\nTable C1: CNN Architecture\nLayer(s)\nOutput Size\nDetails\nInput\n32 × 32 × 3\nConvolutional Block 1\nConv2D + ReLU\n32 × 32 × 64\n3 × 3 conv, 64 filters, padding=1\nBatchNorm2D\n32 × 32 × 64\nConv2D + ReLU\n32 × 32 × 64\n3 × 3 conv, 64 filters, padding=1\nBatchNorm2D\n32 × 32 × 64\nMaxPool2D\n16 × 16 × 64\n2 × 2 max pool, stride=2\nConvolutional Block 2\nConv2D + ReLU\n16 × 16 × 128\n3 × 3 conv, 128 filters, padding=1\nBatchNorm2D\n16 × 16 × 128\nConv2D + ReLU\n16 × 16 × 128\n3 × 3 conv, 128 filters, padding=1\nBatchNorm2D\n16 × 16 × 128\nMaxPool2D\n8 × 8 × 128\n2 × 2 max pool, stride=2\nConvolutional Block 3\nConv2D + ReLU\n8 × 8 × 256\n3 × 3 conv, 256 filters, padding=1\nBatchNorm2D\n8 × 8 × 256\nConv2D + ReLU\n8 × 8 × 256\n3 × 3 conv, 256 filters, padding=1\nBatchNorm2D\n8 × 8 × 256\nMaxPool2D\n4 × 4 × 256\n2 × 2 max pool, stride=2\nConvolutional Block 4\nConv2D + ReLU\n4 × 4 × 512\n3 × 3 conv, 512 filters, padding=1\nBatchNorm2D\n4 × 4 × 512\nConv2D + ReLU\n4 × 4 × 512\n3 × 3 conv, 512 filters, padding=1\nBatchNorm2D\n4 × 4 × 512\nAdaptiveAvgPool2D\n1 × 1 × 512\nOutput size (1, 1)\nFlatten\n512\nFlatten to vector\nClassifier\nFully Connected + ReLU\n256\nLinear layer, 512 →256\nDropout\n256\nDropout probability p = 0.4\nFully Connected\nC\nLinear layer, 256 →C\n33\n\n\nTable C2: Retina model Architecture\nLayer(s)\nOutput Size\nDetails\nInput\n32 × 32 × 1\nGrayscale input\nRetina\nConv2D + ReLU\n32 × 32 × 32\n9 × 9 conv, 32 filters, padding=4\nConv2D + ReLU\n32 × 32 × 32\n9 × 9 conv, 32 filters, padding=4\nVVS\nConv2D + ReLU\n32 × 32 × 32\n9 × 9 conv, 32 filters, padding=4\nConv2D + ReLU\n32 × 32 × 32\n9 × 9 conv, 32 filters, padding=4\nConv2D + ReLU\n32 × 32 × 32\n9 × 9 conv, 32 filters, padding=4\nFlatten\n32, 768\nFlatten to vector\nClassifier\nFully Connected + ReLU\n1, 024\nLinear layer, 32, 768 →1, 024\nDropout\n1, 024\nDropout probability p = 0.5\nFully Connected\nC\nLinear layer, 1, 024 →C\n34\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20580v1.pdf",
    "total_pages": 34,
    "title": "Training Large Neural Networks With Low-Dimensional Error Feedback",
    "authors": [
      "Maher Hanut",
      "Jonathan Kadmon"
    ],
    "abstract": "Training deep neural networks typically relies on backpropagating high\ndimensional error signals a computationally intensive process with little\nevidence supporting its implementation in the brain. However, since most tasks\ninvolve low-dimensional outputs, we propose that low-dimensional error signals\nmay suffice for effective learning. To test this hypothesis, we introduce a\nnovel local learning rule based on Feedback Alignment that leverages indirect,\nlow-dimensional error feedback to train large networks. Our method decouples\nthe backward pass from the forward pass, enabling precise control over error\nsignal dimensionality while maintaining high-dimensional representations. We\nbegin with a detailed theoretical derivation for linear networks, which forms\nthe foundation of our learning framework, and extend our approach to nonlinear,\nconvolutional, and transformer architectures. Remarkably, we demonstrate that\neven minimal error dimensionality on the order of the task dimensionality can\nachieve performance matching that of traditional backpropagation. Furthermore,\nour rule enables efficient training of convolutional networks, which have\npreviously been resistant to Feedback Alignment methods, with minimal error.\nThis breakthrough not only paves the way toward more biologically accurate\nmodels of learning but also challenges the conventional reliance on\nhigh-dimensional gradient signals in neural network training. Our findings\nsuggest that low-dimensional error signals can be as effective as\nhigh-dimensional ones, prompting a reevaluation of gradient-based learning in\nhigh-dimensional systems. Ultimately, our work offers a fresh perspective on\nneural network optimization and contributes to understanding learning\nmechanisms in both artificial and biological systems.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}