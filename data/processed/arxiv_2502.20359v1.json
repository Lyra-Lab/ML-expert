{
  "id": "arxiv_2502.20359v1",
  "text": " \nDavid C. Wyld et al. (Eds): AIMLA, CCNET, NLTM – 2025 \npp. 11-22, 2025. CS & IT - CSCP 2025                                                          DOI: 10.5121/csit.2025.150302 \n \nEVALUATING THE LONG-TERM \nVIABILITY OF EYE-TRACKING FOR \nCONTINUOUS AUTHENTICATION IN \nVIRTUAL REALITY \n \nSai Ganesh Grandhi  and Saeed Samet \n \nSchool of Computer Science, University of Windsor, Windsor, Canada \n \nABSTRACT \n \nTraditional authentication methods, such as passwords and biometrics, verify a user’s \nidentity only at the start of a session, leaving systems vulnerable to session hijacking. \nContinuous authentication, however, ensures ongoing verification by monitoring user \nbehavior. This study investigates the long-term feasibility of eye-tracking as a behavioral \nbiometric for continuous authentication in virtual reality (VR) environments, using data \nfrom the GazebaseVR dataset. Our approach evaluates three architectures—Transformer \nEncoder, DenseNet, and XGBoost—on short- and long-term data to determine their \nefficacy in user identification tasks. Initial results indicate that both Transformer Encoder \nand DenseNet models achieve high accuracy rates of up to 97% in short-term settings, \neffectively capturing unique gaze patterns. However, when tested on data collected 26 \nmonths later, model accuracy declines significantly, with rates as low as 1.78% for some \ntasks. To address this, we propose periodic model updates incorporating recent data, \nrestoring accuracy to over 95%. These findings highlight the adaptability required for \ngaze-based continuous authentication systems and underscore the need for model \nretraining to manage evolving user behavior. Our study provides insights into the efficacy \nand limitations of eye-tracking as a biometric for VR authentication, paving the way for \nadaptive, secure VR user experiences. \n \nKEYWORDS \n \nContinuous authentication, Virtual reality, Eye-tracking, Biometrics, Transformers \n \n1. INTRODUCTION \n \nThe most prevalent authentication method today is static authentication, which includes \npasswords, biometrics, PINs, and more. While techniques like these are challenging to by pass \ndue to their complex identity structure, static authentication has a noteworthy drawback. Once a \nmalicious actor successfully navigates through static authentication, they gain unrestricted access \nto the system, which continues to recognize them as a legitimate user[1]. \n \nIn contrast, continuous authentication offers a dynamic alternative that uniquely combines \nongoing user identification with a seamless user experience. Traditional authentication methods \ntypically verify the identity only at the beginning of a session, assuming 1 that the user remains \nunchanged throughout the session. However, continuous authentication monitors user behavior in \nreal time, making it considerably more challenging for unauthorized individuals to maintain \naccess. Any deviations from established behavioral patterns can trigger immediate responses, \n\n\n12                                       Computer Science & Information Technology (CS & IT) \n \nsuch as reauthentication or session termination. This proactive approach significantly mitigates \nthe risk of session hijacking, in which an attacker could take control of an already authenticated \nsession[2]. \n \nThe evolution of continuous authentication has progressed from desktop environments to mobile \ndevices and now extends to virtual reality (VR) headsets. These headsets utilize behavioral \nbiometrics such as eye tracking, hand movements, and head gestures. Recent studies, including \nthe work by Lohr et al. (2022)[3] and their 2024 follow-up[4], have demonstrated the \neffectiveness of eye tracking as a behavioral biometric, achieving low Equal Error Rates (EERs) \nin user identification[3]. \n \nAlthough these advances are promising, behavioral biometric patterns can change over time. It \nhappens due to users changing behavioral patterns; as noted in various studies, behavioral \npatterns are known to evolve[5]. When behavioral patterns evolve and change, authenticating \nimpostors versus legitimate users becomes difficult, and as a result, EER scores increase. Other \nbehavioral biometrics, such as gaze and touch patterns used in mobile devices, also undergo \nchanges that require continuous updates during the registration phase[5]. \n \nIn this study, we investigate the long-term usability of eye tracking for continuous authentication \nusing the GazebaseVR dataset, which spans 26 months and encompasses three distinct rounds of \ndata collection. We developed user models based on the transformer encoder architecture \nutilizing eye-tracking information from Round 1 (the first month) and tested the model’s ability to \npredict user data from Round 3 (captured after 26 months). Initial results showed low accuracy \nscores of approximately 10 percent. However, when incorporating data from all three rounds into \nthe user model, accuracy scores surged beyond 95 percent. This finding suggests that for eye \ntracking to serve as a reliable behavioral biometric, the user model must be updated with \nbehavioral data over time. \n \nOur research aims to analyze eye tracking as a viable behavioral biometric for VR/AR headsets \nby examining its long-term usability. In this study we also explore various authentication \narchitectures—including DenseNet, Transformer Encoder, and XGBoost—to ascertain which \nalgorithms yield optimal results for user identification. Our research findings indicate that while \nall three architectures achieve commendable accuracy rates, XGBoost demonstrates lower \nperformance with accuracies ranging from 85% to 90%. In contrast, both Transformer Encoder \nand DenseNet achieve accuracies between 90% and 97%. \n \n2. LITERATURE REVIEW \n \n2.1. Datasets for Eye Tracking in Virtual Reality \n \nThe development of reliable and scalable continuous authentication systems based on eye-\ntracking requires comprehensive datasets. Several datasets have been introduced in recent years, \neach offering unique attributes relevant to eye movement biometrics. \n \n2.1.1. GazeBaseVR \n \nGazeBaseVR, developed by Lohr et al. (2023), is a large-scale longitudinal dataset capturing eye \nmovements from 407 college-aged participants using a VR-enabled eye tracker at 250 Hz[6]. \nCollected over 26 months, the dataset includes 5,020 recordings across five tasks (vergence, \nsmooth pursuit, video viewing, reading, and random saccades), allowing for diverse eye \nmovement analysis. Unique to GazeBaseVR, the dataset provides 3D positional data (X, Y, Z) for \n\n\n \nComputer Science & Information Technology (CS & IT)                                           13 \n \nboth eyes and offers rich demographic diversity, contributing significantly to the field of eye \nmovement biometrics. Compared to its predecessor, GazeBase, this dataset adds novel task types, \nlike vergence, and supports binocular tracking, making it ideal for advanced VR-specific EMB \nstudies. Such extensive data supports the development of robust, generalizable machine learning \nmodels for eye movement analysis and authentication applications. \n \n2.1.2. GazeBase \n \nThe GazeBase dataset, presented by Griffith et al. (2021)[7], is a comprehensive longitudinal \ndataset featuring 12,334 monocular eye-movement recordings from 322 collegeaged \nparticipants[7]. Collected across nine rounds over 37 months, the data includes seven eye-\ntracking tasks, such as fixation, saccades, reading, and free viewing. All recordings were captured \nusing an EyeLink 1000 eye tracker at 1,000 Hz, with calibration performed for each task to \nensure accuracy. Due to its scale and repeated measures, GazeBase is well-suited for studies in \neye movement biometrics and machine learning applications focused on eye signal analysis. \nAdditionally, classification labels and pupil area data are available for a subset, providing \nvaluable resources for supervised learning in gaze analysis[7]. \n \n2.2. Eye Tracking in Continuous Authentication \n \nEye movement biometrics (EMB) have gained significant attention as potential mechanisms for \ncontinuous authentication in VR, where eye-tracking sensors can facilitate real-time identity \nverification. Recent studies have shown that gaze-driven biometrics can yield low equal error \nrates (EERs), essential for effective authentication. \n \n2.2.1. EKYT and DenseNet Implementation \n \nThe Eye Know You Too (EKYT)[3], based on DenseNet architecture, is optimized for eye \nmovement-based biometrics. The EKYT network employs eight convolutional layers with \ndensely connected layers to enhance feature extraction, followed by a global average pooling \nlayer and a fully connected layer, which generates a 128-dimensional embedding for each user. \nThis architecture has demonstrated robust performance for gaze-based continuous authentication, \nand its DenseNet foundation supports the efficient reuse of features across layers, addressing \nchallenges related to feature extraction in eye-tracking data[3]. \n \n2.2.2. Gaze Base VR and DenseNet Implementation \n \nThe GazeBaseVR dataset collects 5020 binocular eye movement recordings from 407 college-\naged participants over three rounds, enabling EMB research in VR environments[6]. Raju et al.[4] \nhave implemented the EKYT architecture in the GazeBaseVR data set. This study contrasts the \nbiometric performance of VR-collected data with a high-end 1,000 Hz eye tracker, showing that \nwhile VR data are noisier, it remains viable for authentication, achieving an equal error rate \n(EER) of 1.67% in short-term scenarios. These findings underline the potential of VR-based \nEMB, suggesting that VR eye-tracking data, despite challenges, may offer a convenient, accurate \nbiometric solution. \n \n2.3. Challenges in Eye Tracking for Continuous Authentication \n \nWhile gaze-based biometrics hold promise, challenges remain, particularly in terms of \ncalibration, signal quality, and user behavior variation over time. For instance, the visual axis, \nrequiring user-specific calibration, can be challenging for continuous authentication. Raju et al. \n(2024) noted that spatial accuracy directly affects authentication performance, with higher error \n\n\n14                                       Computer Science & Information Technology (CS & IT) \n \nrates observed when calibration is not consistently maintained. Further, the dynamic nature of \nuser behavior suggests a need for adaptive models that update with user data to maintain high \nauthentication accuracy over time. \n \n3. METHODOLOGY \n \n3.1. Dataset \n \nThe GazebaseVR dataset stands as the most comprehensive publicly accessible dataset focused \non eye-tracking data acquired from virtual reality (VR) and augmented reality (AR) headsets. \nThis dataset encompasses eye-tracking data collected from both eyes of participants while \nimmersed in VR, a setup essential for analyzing human gaze behaviors in virtual environments. \nThe study began with 465 individuals, but 58 were later excluded due to various considerations. \nThe data collection spanned three recording rounds over a 26-month period, with each round \nincorporating two separate recording sessions approximately 30 minutes apart. The eye-tracking \ndata (ET) were recorded using SensoMotoric Instruments’ (SMI’s) VR device, which samples \ndata from both eyes at a nominal rate of 250 Hz. Such a high sampling frequency enables precise \ncapture of eye movements, making the dataset ideal for analyzing fine-grained eye movement \npatterns. \n \n3.1.1. Dataset Tasks \n \nTo capture a comprehensive set of eye movement patterns, researchers instructed participants to \nperform five distinct tasks. These tasks were specifically designed to induce various eye \nmovements such as vergence, smooth pursuit, saccades, and fixations, providing a rich basis for \neye movement analysis. \n \nTable1:Overview of Eye Movement Tasks [6] \n \nTask \nFeatures \nDescription \nVergence task(VRG) \nConvergence and divergence A black dot appears on a large \nSquare plane and alternates between \ndifferent depths. \nSmooth \n(PUR) \npursuit task Saccades, fixations \nA \nsmall \nblack \nsphere \nmoves \nSmoothly between the left and right edges \nof the viewing region. \nVideo \n(VID) \nviewing task Multiple features \nA video is displayed on a large, \nRectangular plane. \nReading task(TEX) \nMultiple features \nAnexcerptofapproximately820 \ncharacters \nfrom \nNational \nGeo- \ngraphic is displayed. \nRandoms accade task \n(RAN) \nSaccades, fixations \nA small black sphere jump storan- \nDom screen positions. \n \n3.1.2. Dataset features \n \nThe ET API provided by SMI produces 3-dimensional unit vectors representing the gaze \ndirection of each eye and timestamps with nanosecond precision. There are 250 timestamp \nrecords for each second (250Hz), which provides a rich analysis of eye movements [6]. The \nfollowing features are collected for each user. We only utilize features n, clx, cly, clz, crx, cry, crz \n\n\n \nComputer Science & Information Technology (CS & IT)                                           15 \n \nand an additional feature created by us called user. The first 7 features provide patterns of eye \nmovements such as fixations, saccades, blink and more of users and these are used to train the \nuser model which is described in further sections. The last column user is used for multi-\nclassification, it basically represents which users information and we classify a user. \n \n3.2. Pre-Processing of Eye-Tracking Data \n \nThe pre-processing of raw eye-tracking data is essential to ensure consistency and quality for \nsubsequent analysis and model training. This involves selecting relevant features, normalizing \ndata temporally and spatially, and structuring it into segments suitable for model input. \n \nFigure1: Overview of the Methodology \n \n3.2.1. Feature Selection \n \nKey features are selected from the raw data to focus on essential gaze patterns. These include the \ntimestamp (’n’) and six spatial coordinates: ’clx’, ’cly’, ’clz’ for the left eye, and ’crx’, ’cry’, \n’crz’ for the right eye. For temporal normalization, timestamps are converted from milliseconds \nto seconds, ensuring consistency across devices and enabling standardized time-based pattern \nanalysis[4]. \n \n3.2.2. Normalization \n \nThe normalization process for eye-tracking data in this study involves both temporal and spatial \naspects, ensuring consistency and optimal input for model training. Temporal normalization \nconverts timestamps from milliseconds to seconds: \n \n \nSpatial normalization adjusts each coordinate to a range of [-1, 1] using Min-Max normalization: \n \n \n \nwhere x is the original value and xmin and xmax are the minimum and maximum values across \nthe dataset. The range is centered around zero, which can be beneficial for algorithms like \nTransformers, which those using activation functions. [8]. \n \n3.2.3. Windowing and Data Structuring \n \nTo prepare the continuous eye-tracking data for model input, we employ a segmentation strategy \nadapted from the DenseNet architecture [3]. The data stream is divided into fixed-size windows, \neach containing 1250 samples. This window size corresponds to a 5-second interval when \n\n\n16                                       Computer Science & Information Technology (CS & IT) \n \nsampled at 250 Hz, striking a balance between capturing meaningful temporal patterns and \nmaintaining a manageable input size for machine learning algorithms. The data within each \nwindow is then restructured into a 2D array format. In this arrangement, rows represent the \nspatial coordinates of eye movements, while columns correspond to discrete time points. This \norganization results in a 3D array structure (windows × coordinates × time points), which is \nparticularly well-suited for sequential data processing in transformer models [9]. \n \n3.3. Transformer Encoder Architecture \n \nThe proposed architecture, known as the Transformer Model, is specifically designed for \nprocessing eye movement biometrics (EMB). This model performs a mapping f : R(C×T) → RN , \nwhere C represents the number of input channels, T denotes the length of the input sequence and \nN corresponds to the number of output classes. The architecture is inspired by the original \nTransformer model [9] and has been adapted to efficiently handle the time-series data inherent in \neye tracking applications. \n \nThe Transformer Encoder architecture has not previously been applied to continuous \nauthentication using eye tracking as a behavioral biometric. This study seeks to benchmark its \nperformance against state-of-the-art models like EKYT and DenseNet [3]. Our findings reveal \nthat the Transformer Encoder achieved strong results and, in some instances, surpassed the \nDenseNet architecture. \n \nThe Transformer Model employs a dimensionality of dmodel = 64, utilizes 4 attention heads, and \ncomprises 2 transformer encoder layers. To mitigate over fitting, dropout is systematically \napplied throughout the architecture. This design ensures that the model remains compact and \nefficient, making it suitable for deployment in resource-constrained environments such as virtual \nreality (VR) and augmented reality (AR) devices. \n \nThe choice of dmodel = 64 was made to maintain a compact model size while preserving \nsufficient representational capacity. This dimensionality allows for efficient processing of the \neye-tracking time series data, which consists of 7 features (timestamp and 3D coordinates for \neach eye) sampled at 250 Hz. The model uses 4 attention heads and 2 encoder layers. This \nconfiguration was chosen to capture complex temporal dependencies in eye movement patterns \nwhile keeping the model lightweight. \n \n3.3.1. Training Parameters \n \nThe model is trained for 50 epochs. This number was chosen to provide sufficient iterations for \nthe model to learn patterns in the eye-tracking data while balancing computational resources. An \ninitial learning rate of 0.001 is used with the Adam optimizer. This relatively low learning rate \nwas selected to ensure stable training, particularly important for transformer models which can be \nsensitive to learning rate. A batch size of 32 is employed, striking a balance between \ncomputational efficiency and providing sufficient stochastic gradient estimates. These parameter \nchoices reflect a balance between model complexity and computational efficiency, tailored to the \nspecific requirements of continuous authentication in VR environments using eye-tracking data. \n \n3.4. Model Training for Eye-Tracking Data \n \nThe model training process involves distinct steps for each of the three models: XGBoost, \nTransformer Encoder, and DenseNet. Each model is tailored to leverage eye-tracking data for \nuser identification, utilizing varying architectures and methodologies to capture unique gaze \npatterns. The primary motivation for training XGBoost is its lower computational cost compared \n\n\n \nComputer Science & Information Technology (CS & IT)                                           17 \n \nto neural network-based approaches. This study aims to evaluate whether computationally \nefficient machine learning algorithms can deliver comparable performance to neural network \nmodels. \n \n3.4.1. Training XG Boost Model \n \nThe XGBoost model, widely recognized for its gradient boosting capabilities, is used here with a \nflattened 2D input derived from the original 3D eye-tracking data. The data is first reshaped to a \n2D format, allowing XGBoost to process it as a feature matrix where each row represents a user \nsample. XGBoost is highly suitable for tabular data due to its ability to optimize complex feature \ninteractions through gradient boosting [10]. For 7 model evaluation, the dataset is split in an \n80:20 ratio, with the training set comprising 80% of the data and the test set comprising 20 \n \nThe training objective is set to ”multi:softmax” for multi-class classification, with a learning rate \n(η) of 0.3 and maximum depth (max depth) of 6. A DMatrix is created for each dataset split, \nproviding a structured way for XGBoost to handle labeled data. After training, predictions are \nmade on the test set \n \nThis approach capitalizes on XGBoost’s strength in handling non-linear data interactions, \ndelivering high accuracy on gaze-based classification tasks. \n \n3.4.2. Training Transformer Encoder Model \n \nThe Transformer Encoder model is designed to leverage the sequential nature of eyetracking data, \nusing attention mechanisms to capture temporal dependencies and spatial relationships in the gaze \npatterns. In this setup, the pre-processed eye-tracking data is first converted to PyTorch tensors, \nwith user labels encoded for classification. The data is split into training and testing sets, with \nbatches handled by PyTorch’s ‘DataLoader‘ to optimize memory and computation. \n \nThe model architecture consists of an embedding layer that maps the input to a dmodel \ndimensional space, followed by a positional encoding layer to account for sequence order. The \nencoder structure includes multi-head self-attention and feed forward layers, following the \nformula: \n \n \n \nwhere Q, K, and V represent query, key, and value matrices, respectively, and dk is the \ndimensionality of the keys. The model is trained for 50 epochs with Cross Entropy Loss as the \ncriterion, and parameter updates are managed by the Adam optimizer with a learning rate of \n0.001 [9]. After each epoch, average loss is recorded to track model convergence. This model \neffectively learns sequential dependencies in eye movements, a crucial feature for reliable user \nidentification. \n \n3.4.3.  Training DenseNet Model \n \nThe DenseNet model, adapted from a convolutional neural network (CNN) structure, is tailored \nfor eye-tracking data through dense connections that encourage feature reuse and efficient \ngradient flow[11]. The model architecture begins with an initial convolutional layer, followed by \nmultiple dense layers, where each layer receives input from all preceding layers within the dense \nblock. This connectivity enhances learning efficiency and mitigates the vanishing gradient \n\n\n18                                       Computer Science & Information Technology (CS & IT) \n \nproblem, a common issue in deep CNNs. \n \nThe DenseNet model processes each input sample in a 1D convolutional format, with dense \nblocks of increasing dilation rates, allowing it to capture spatial dependencies across varying \nscales. The loss function used is Cross Entropy Loss, and the Adam optimizer updates parameters \nto minimize classification error. Similar to the Transformer model, DenseNet is trained for 50 \nepochs, with loss values logged for each epoch. \n \nDenseNet’s structure is advantageous for gaze data, as its dense connections capture both fine-\ngrained spatial details and broader contextual information, contributing to high performance in \ngaze-based user identification tasks. \n \n4. RESULTS AND DISCUSSION \n \n4.1. Short-Term Model Results \n \nThe initial phase of the experiment focused on short-term model performance, where XGBoost, \nTransformer Encoder, and DenseNet models were trained on Round 1 eyetracking data of 407 \nusers and evaluated on the same round, using an 80:20 train-test split. Table 2 shows the \naccuracies achieved for different eye movement tasks, demonstrating that both transformer and \nDenseNet models performed exceptionally well in classifying users based on their gaze patterns. \n \nIn this short-term scenario, the models achieved accuracies between 80.25% and 97.77% across \nvarious tasks, with few exceptions going below 80%, indicating that Transformer Encoder and \nDenseNet effectively capture unique gaze characteristics over a short timeframe. However, \nXGBoost is limited in performance, with 79.31% accuracy combined with all tasks. Although the \naccuracy of XGBoost does not meet with neural network architecture, it still provides exemplary \naccuracy. On the other hand, high accuracies with neural networks suggest that gaze patterns \ncontain distinct features that can differentiate users with a high degree of reliability when the data \ncollection and testing occur within a relatively close period. For tasks like Vergence (VRG) and \nSmooth Pursuit (PUR), which involve precise eye movements, the accuracy was exceptionally \nhigh, reflecting the stability of these gaze patterns over a short term. \n \nThis finding highlights the feasibility of using gaze-based biometrics for short-term \nauthentication in VR settings, where users’ gaze patterns remain stable and predictable. The high \nshort-term accuracy also underscores the potential of Transformer-based architectures to handle \nsequential eye-tracking data effectively. The window size for all tasks is 5 seconds, and data from \nall 407 users is used in training and testing. \n \nTable2:Model Accuracy Comparison-Short-term Training \n \nTask \nDenseNet \nTransformer \nXG Boost \nTrain Round \nTest Round \nAll \n97.09% \n97.20% \n79.31% \nRound1 \nRound1 \nPUR \n96.61% \n96.80% \n84.16% \nRound1 \nRound1 \nRAN \n95.52% \n95.58% \n80.25% \nRound1 \nRound1 \nTEX \n90.22% \n91.00% \n57.48% \nRound1 \nRound1 \nVID \n87.22% \n90.50% \n57.66% \nRound1 \nRound1 \nVRG \n96.47% \n97.77% \n87.39% \nRound1 \nRound1 \n \n \n\n\n \nComputer Science & Information Technology (CS & IT)                                           19 \n \n4.2. Long-Term Model Results \n \nTo evaluate the long-term stability of gaze-based biometrics, we assessed the model performance \nby training on the data of Round 1 and testing the data of Round 3 collected 26 months later. As \nshown in Table 3, this resulted in a significant drop in accuracy, with scores ranging from 1.78% \nto 10.28% depending on the task and model. This dramatic decrease in performance suggests that \ngaze patterns are not static and may evolve over 9 time, potentially influenced by factors such as \nchanges in user behavior, eye health, or VR interaction habits. This marked decrease suggests that \ngaze patterns undergo considerable changes over time[?], presenting significant challenges for \nmaintaining reliable user differentiation in long-term scenarios. \n \nDenseNet demonstrated the highest overall accuracy at 7.79% when combining all tasks, while \nthe Transformer Encoder showed the poorest performance at 3.01%. XGBoost exhibited mixed \nresults, outperforming other models in some tasks like TEX (12.11%) and VRG (11.46%), but \nunderperforming in others such as PUR (4.89%) and VID (1.78%). \n \nThe task-specific variations in accuracy suggest that certain gaze behaviors may be more stable \nover time, while others are highly variable. For instance, tasks related to text reading (TEX) and \nvergence (VRG) showed relatively higher accuracies, indicating potentially more consistent gaze \npatterns for these activities. In contrast, the video-watching task (VID) yielded the lowest \naccuracies across all models, highlighting the complexity of long-term gaze-based user \nidentification, particularly for dynamic visual stimuli[12]. \n \nThese findings underscore the need for more robust models and feature extraction techniques that \ncan adapt to temporal changes in gaze patterns. Future research should focus on developing \nmethods that can maintain higher accuracy levels over extended periods, possibly by \nincorporating adaptive learning mechanisms or by identifying more stable, long-term gaze \ncharacteristics[12]. \n \nThe low long-term accuracy indicates that models trained on older data fail to generalize well \nwhen tested on data collected after a long interval. For example, the Random Saccade (RAN) and \nVideo Viewing (VID) tasks, which depend heavily on dynamic gaze shifts, experienced \nsubstantial performance degradation. The results highlight a critical limitation in the long-term \nuse of gaze patterns for continuous authentication. Behavioral biometrics, like gaze data, are \ninherently dynamic[13], and this variability over time implies that models trained on gaze data \nmust be periodically updated. This need for frequent retraining or model adjustment aligns with \nfindings in related studies on continuous authentication, where user behavior tends to evolve, \nleading to potential identification challenges. \n \n \nTable3:Model Accuracy Comparison-Long-term Testing \n \nTask \nDenseNet \nTransformer \nXG Boost \nTrain Round \nTest Round \nAll \n7.79% \n3.01% \n4.85% \nRound1 \nRound3 \nPUR \n10.28% \n9.94% \n4.89% \nRound1 \nRound3 \nRAN \n5.98% \n7.67% \n6.15% \nRound1 \nRound3 \nTEX \n8.40% \n3.71% \n12.11% \nRound1 \nRound3 \nVID \n4.00% \n2.45% \n1.78% \nRound1 \nRound3 \nVRG \n8.06% \n7.57% \n11.46% \nRound1 \nRound3 \n \n \n \n\n\n20                                       Computer Science & Information Technology (CS & IT) \n \n4.3. Revised Long-term Model Results with Updated Data \n \nTo address the observed decline in long-term accuracy, a revised model was trained on a \ncombined dataset of Round 1 and Round 3 data. In this setup, the model was tested 10 on \npreviously unused Round 3 data to assess whether incorporating recent data would enhance \nperformance. As seen in Table 4, this approach resulted in significant improvement, with \naccuracies reaching up to 98.71%, closely matching the short-term results. \n \nThese improved results suggest that by continuously updating the training dataset with recent \ndata, the model can better adapt to evolving gaze patterns. This approach, which mirrors periodic \nretraining, can help maintain high authentication accuracy even as user behavior changes over \ntime. For continuous authentication systems to remain reliable, incorporating recent data into \ntraining may be essential, particularly for biometrics subject to temporal variability, like gaze. \n \nThis finding underlines the importance of adaptive modeling in the context of continuous \nauthentication. Behavioral biometrics, unlike static identifiers, require flexible models that can \naccommodate gradual changes in user behavior. Consequently, for eyetracking authentication \nsystems to be feasible in the long term, regular updates with recent behavioral data are likely \nrequired. Future research could explore optimal retraining intervals and data selection strategies \nto achieve a balance between computational cost and authentication accuracy. \n \nTable4:Model Accuracy Comparison-Long-term Training with Updated Data \n \nTask \nDenseNet \nTransformer \nXG Boost \nTrain Round \nTest Round \nAll \n98.71% \n96.52% \n93.25% \nRound1+3 \nRound3 \nPUR \n98.50% \n97.46% \n88.50% \nRound1+3 \nRound3 \nRAN \n97.14% \n98.32% \n83.66% \nRound1+3 \nRound3 \nTEX \n98.75% \n98.22% \n55.13% \nRound1+3 \nRound3 \nVID \n86.22% \n91.78% \n55.78% \nRound1+3 \nRound3 \nVRG \n97.17% \n94.48% \n87.55% \nRound1+3 \nRound3 \n \n4.4. Ethical Implications of Eye-Tracking for Continuous Authentication \n \nWhile eye-tracking offers promising advancements in continuous authentication for virtual reality \nenvironments, it also raises several ethical concerns that must be carefully addressed: \n \n4.4.1. Privacy Concerns \n \nEye movement patterns can potentially reveal sensitive information about a user’s mental or \nphysical state. For instance, certain gaze patterns might indicate cognitive load, emotional states, \nor even medical conditions such as attention deficit disorders or early signs of neurological \ndiseases[14]. It is crucial to ensure that this data is used solely for authentication purposes and not \nfor unauthorized analysis or profiling. \n4.4.2. Data Security \n \nThe secure storage and transmission of eye-tracking data is paramount. Given the sensitive nature \nof biometric information, robust encryption and data protection measures must be implemented to \nprevent unauthorized access or data breaches[15]. Developers of VR systems must adhere to strict \ndata protection standards and regularly audit their security protocols. \n \n \n \n\n\n \nComputer Science & Information Technology (CS & IT)                                           21 \n \n4.4.3. User Consent and Control \n \nClear and comprehensive user agreements are essential when implementing eye-tracking \nauthentication. Users must be fully informed about what data is collected, how it is used, and who \nhas access to it. Additionally, users should have the option to opt out of continuous authentication \nor choose alternative methods, ensuring their autonomy in deciding how their biometric data is \nused[15]. \n \nBy addressing these ethical considerations, we can work towards developing eye tracking \nauthentication systems that not only enhance security but also respect user privacy, promote \ninclusivity, and maintain high ethical standards in the rapidly evolving field of virtual reality \ntechnology. \n \n5. CONCLUSION \n \nThis study investigated the use of eye-tracking data as a behavioral biometric for continuous user \nauthentication in virtual reality (VR) environments, with a focus on both short-term and long-\nterm usability. Using the GazebaseVR dataset, we evaluated the performance of Transformer \nEncoder and DenseNet models for user identification, achieving promising results in short-term \nexperiments with accuracy levels reaching over 97%. These results indicate that gaze patterns in \nthe short term can serve as a reliable biometric, with both the Transformer and DenseNet \narchitectures proving effective in classifying users based on unique eye movement characteristics. \n \nHowever, when testing model performance over an extended period, significant accuracy \ndegradation was observed, with accuracy dropping to as low as 1.78% for certain tasks after 26 \nmonths. This decline highlights a key limitation of behavioral biometrics such as eye tracking: \ngaze patterns are subject to temporal changes, likely influenced by behavioral shifts, health \nfactors, or user adaptation to VR environments. The findings underscore that, while effective in \nthe short term, static models fail to generalize well over time, making continuous model updates \nessential for sustaining high accuracy in real-world applications. \n \nTo address this, we explored an adaptive model training approach by incorporating recent data \ninto the training set. This method restored accuracy to near short-term levels, with performance \nimprovements exceeding 98%. Such results suggest that periodic model retraining with recent \ndata is crucial to maintaining the viability of gaze-based continuous authentication systems. \nAdaptive modeling, where data from subsequent sessions are used to update the user model, can \npotentially offset the temporal variability in gaze patterns, providing a practical solution for long-\nterm user identification in VR. \n \nIn conclusion, this study demonstrates the feasibility of using eye-tracking as a behavioral \nbiometric for continuous authentication in VR settings, while emphasizing the need for adaptive \nmodel retraining to account for behavioral drift over time. Future work could focus on \ndetermining optimal retraining intervals and exploring additional features such as head or hand \nmovements to enhance model robustness. As VR applications grow in importance, developing \nreliable, adaptive biometric systems for continuous authentication will be essential for enhancing \nuser security in immersive environments. \n \n \n \n \n \n\n\n22                                       Computer Science & Information Technology (CS & IT) \n \nACKNOWLEDGEMENTS \n \nWe extend our thanks to the School of Computer Science at the University of Windsor for \nproviding the resources necessary for conducting this research. Special appreciation goes to Dr. \nOleg V. Komogortsev and his team for developing and sharing the GazeBaseVR dataset, which \nplayed a crucial role in our analysis. Additionally, we are grateful to our colleagues for their \nconstructive feedback and insightful discussions that enriched our work. \n \nREFERENCES \n \n[1] \nKeystrike. Continuous authentication vs two-factor authentication, 2022. \n[2] \nBeyond Identity. Continuous authentication: A dynamic approach to user verification, 2022. \n[3] \nDillon Lohr and Oleg V. Komogortsev. Eye know you too: Toward viable end-to-end eye movement \nbiometrics for user authentication. IEEE Transactions on Information Forensics and Security, \n17:3151–3164, 2022. \n[4] \nMehedi Hasan Raju, Dillon J Lohr, and Oleg V Komogortsev. Evaluating eye movement biometrics \nin virtual reality: A comparative analysis of vr headset and high-end eye-tracker collected dataset, \n2024. \n[5] \nMingming Hu, DingWang, Chen Li, Yang Xu, and Bibo Tu. Behavioral biometrics based continuous \nauthentication using a lightweight latent representation masked one-class autoencoder. IEEE \nTransactions on Dependable and Secure Computing, pages 1–16, 2024. \n[6] \nDavid Lohr, Shahbaz Aziz, Linda Friedman, et al. Gazebasevr, a large-scale, longitudinal, binocular \neye-tracking dataset collected in virtual reality. Scientific Data, 10:177, 2023. \n[7] \nHannah Griffith, David Lohr, Evgeniy Abdulin, et al. Gazebase, a large-scale, multistimulus, \nlongitudinal eye movement dataset. Scientific Data, 8:184, 2021. \n[8] \nSimon Eberz, Kasper B. Rasmussen, Vincent Lenders, and Ivan Martinovic. Looks like eve: Exposing \ninsider threats using eye movement biometrics. ACM Trans. Priv. Secur., 19(1), June 2016. \n[9] \nAshish Vaswani, John Shardlow, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \nŁukasz Kaiser, Simon Kattner, and Niki Parmar. Attention isall you need. Advances in Neural \nInformation Processing Systems, 2017. \n[10] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the \n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, \npage 785–794. ACM, August 2016. \n[11] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected \nconvolutional networks, 2018. \n[12] Efe Bozkir, Benedikt B¨uhler, Hanna Deininger, Peter Goldberg, Peter Gerjets, Ulrich Trautwein, and \nEnkelejda Kasneci. Gaze-based detection of mind wandering during audio-guided panorama viewing, \n2024. \n[13] Benedikt B¨uhler, Efe Bozkir, Hanna Deininger, Peter Goldberg, Peter Gerjets, Ulrich Trautwein, and \nEnkelejda Kasneci. Gaze-based detection of mind wandering during audio-guided panorama viewing. \nScientific Reports, 14(1):2724, 2024. \n[14] Maria K Eckstein, Bel´en Guerra-Carrillo, Alison T Miller Singley, and Silvia A Bunge. Beyond eye \ngaze: What else can eyetracking reveal about cognition and cognitive development? Developmental \ncognitive neuroscience, 25:69–91, 2017. \n[15] Daniel J Liebling and S¨oren Preibusch. Privacy considerations for a pervasive eye tracking world. \nProceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous \nComputing: Adjunct Publication, pages 1169–1177, 2014. \n \n \n \n \n© 2025 By AIRCC Publishing Corporation. This article is published under the Creative Commons \nAttribution (CC BY) license. \n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20359v1.pdf",
    "total_pages": 12,
    "title": "Evaluating the long-term viability of eye-tracking for continuous authentication in virtual reality",
    "authors": [
      "Sai Ganesh Grandhi",
      "Saeed Samet"
    ],
    "abstract": "Traditional authentication methods, such as passwords and biometrics, verify\na user's identity only at the start of a session, leaving systems vulnerable to\nsession hijacking. Continuous authentication, however, ensures ongoing\nverification by monitoring user behavior. This study investigates the long-term\nfeasibility of eye-tracking as a behavioral biometric for continuous\nauthentication in virtual reality (VR) environments, using data from the\nGazebaseVR dataset. Our approach evaluates three architectures, Transformer\nEncoder, DenseNet, and XGBoost, on short and long-term data to determine their\nefficacy in user identification tasks. Initial results indicate that both\nTransformer Encoder and DenseNet models achieve high accuracy rates of up to\n97% in short-term settings, effectively capturing unique gaze patterns.\nHowever, when tested on data collected 26 months later, model accuracy declined\nsignificantly, with rates as low as 1.78% for some tasks. To address this, we\npropose periodic model updates incorporating recent data, restoring accuracy to\nover 95%. These findings highlight the adaptability required for gaze-based\ncontinuous authentication systems and underscore the need for model retraining\nto manage evolving user behavior. Our study provides insights into the efficacy\nand limitations of eye-tracking as a biometric for VR authentication, paving\nthe way for adaptive, secure VR user experiences.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}