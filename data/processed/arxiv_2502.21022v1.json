{
  "id": "arxiv_2502.21022v1",
  "text": "When Unsupervised Domain Adaptation meets One-class Anomaly Detection:\nAddressing the Two-fold Unsupervised Curse by Leveraging Anomaly Scarcity\nNesryne Mejri1, Enjie Ghorbel2,1, Anis Kacem1, Pavel Chernakov1,\nNiki Foteinopoulou1, Djamila Aouada1\n1Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg\n2Cristal Laboratory, National School of Computer Sciences, University of Manouba\n{nesryne.mejri, anis.kacem, pavel.chernakov, niki.foteinopoulou, djamila.aouada}@uni.lu\nenjie.ghorbel@isamm.uma.tn\nAbstract\nThis paper introduces the first fully unsupervised domain\nadaptation (UDA) framework for unsupervised anomaly de-\ntection (UAD). The performance of UAD techniques de-\ngrades significantly in the presence of a domain shift, dif-\nficult to avoid in a real-world setting. While UDA has con-\ntributed to solving this issue in binary and multi-class clas-\nsification, such a strategy is ill-posed in UAD. This might\nbe explained by the unsupervised nature of the two tasks,\nnamely, domain adaptation and anomaly detection. Herein,\nwe first formulate this problem that we call the two-fold\nunsupervised curse. Then, we propose a pioneering solu-\ntion to this curse, considered intractable so far, by assuming\nthat anomalies are rare. Specifically, we leverage cluster-\ning techniques to identify a dominant cluster in the target\nfeature space. Posed as the normal cluster, the latter is\naligned with the source normal features. Concretely, given\na one-class source set and an unlabeled target set composed\nmostly of normal data and some anomalies, we fit the source\nfeatures within a hypersphere while jointly aligning them\nwith the features of the dominant cluster from the target\nset. The paper provides extensive experiments and analysis\non common adaptation benchmarks for anomaly detection,\ndemonstrating the relevance of both the newly introduced\nparadigm and the proposed approach. The code will\nbe made publicly available.\n1. Introduction\nAnomaly Detection (AD) can be seen as the identification\nof outliers deviating from a usual pattern. The growing in-\nterest for AD in both academia and industry is mainly due to\nits relevance in numerous practical scenarios, such as early\ndisease detection in medical imaging [1, 14] and industrial\ninspection [3, 7, 21, 22, 34]. By definition, anomalies rarely\n(b)\n(a)\nSource only\n(No adaptation)\nDirect Adaptation\n(Alignment results)\nBefore Adaptation\n(Source-target alignment)\nLabeled normal source data\n One-class learned decision boundary\nUnlabeled normal/anomaly data\n/\nDomain alignment\nFigure 1. Illustration of the two-fold unsupervised curse: (a)\nThe decision boundary learned from the source set without any\nadaptation does not allow generalization to the target domain. (b)\nDirect alignment of the unlabeled target with the one-class source\nfeatures leads to the confusion of normal and abnormal samples.\noccur. Annotating anomalous data is, therefore, often diffi-\ncult and costly [4, 13, 34], hindering the collection of large-\nscale datasets. As a result, state-of-the-art methods mostly\ntackle AD as an unsupervised problem [11, 29], where the\nobjective is to learn from the normal class.\nDespite achieving promising results, recent approaches\nin AD [7, 13, 28, 33, 35] typically assume that training and\ninference data are drawn from the same distribution. This\nassumption does not always hold in unconstrained scenar-\nios, where a domain shift [26] between training and testing\ndata can naturally arise due to varying setups, such as differ-\nent lighting conditions and variations in object pose [4]. As\na result, a model trained on a dataset sampled from a given\ndomain, usually called source dataset, will show degraded\nperformance when tested on a dataset from a different do-\nmain, generally termed target dataset. For instance, an AD\nmodel for medical imaging trained on images acquired us-\ning a given Medical Resonance Imaging (MRI) device can\nfail to generalize to samples captured with a different MRI\nsystem.\n1\narXiv:2502.21022v1  [cs.LG]  28 Feb 2025\n\n\nTo reduce such a domain gap while avoiding costly\nannotation\nefforts,\nUnsupervised\nDomain\nAdaptation\n(UDA) [15, 37] has proven to be an effective solution in bi-\nnary and multi-class classification tasks [15, 31]. UDA aims\nat learning domain-invariant features by relying on labeled\nsource and unlabeled target data at the same time. However,\nthe task of unsupervised domain adaptation for unsuper-\nvised anomaly detection (UAD) is ill-posed as the goal is to:\nalign the source and the target feature distributions using\nonly normal source data and unlabeled target data formed\nby both normal and anomalous samples (see Figure 2 (c)).\nHence, a direct extension of standard UDA techniques de-\nveloped for binary/multi-class classification [15, 37] would\nnot be applicable, as these methods usually aim at minimiz-\ning the distance between the estimated distributions from\nthe entire source and target training sets. Indeed, this would\nlead to the erroneous alignment of both normal and anoma-\nlous target samples with normal source samples, as illus-\ntrated in Figure 1 (b). Given the learned decision bound-\nary, this would lead to the confusion of normal and abnor-\nmal samples from the target set. As it requires addressing\ntwo unsupervised tasks simultaneously, we refer to this de-\nscribed problem as the two-fold unsupervised curse.\nTo the best of our knowledge, no prior work has tried\nto address this two-fold unsupervised challenge, i.e., UDA\nfor one-class image anomaly detection described in Fig-\nure 2 (c).\nIndeed, related works have mainly simplified\nthe problem by either (1) assuming the availability of la-\nbeled abnormal and normal source data, resulting in UDA\nfor a binary classification setting [16] (see Figure 2 (a)),\nor (2) maintaining the source one-class setup while ac-\ncessing only few normal target data referred to as few-\nshot supervised adaptation for unsupervised anomaly detec-\ntion [6, 16, 19, 20, 38] (see Figure 2 (b)). Nevertheless, an-\nnotating a few samples might still be constraining, particu-\nlarly in the field of anomaly detection, where expert knowl-\nedge is often needed, such as for tumor annotation in med-\nical images [1, 14] or for industrial inspection [7, 21, 35].\nMoreover, few-shot approaches are known to be prone to\noverfitting issues since few shots cannot fully represent the\nnormal target distribution [32]. This calls for a fully un-\nsupervised domain adaptation approach that leverages the\ndiversity of the available large unlabeled target datasets.\nIn this paper, we propose solving the two-fold unsu-\npervised curse by leveraging the fact that the occurrence\nof anomalies tends to be rare.\nWe herein propose the\nfirst unsupervised domain adaptation framework for un-\nsupervised image anomaly detection. Our solution starts\nby identifying a dominant cluster assumed to be formed\nby normal target data and then aligning it with normal\nsource samples. Specifically, our method utilizes a train-\nable ResNet-based [12] feature extractor to process both\nthe source and target features. A frozen CLIP visual en-\ncoder [27] is also used to generate corresponding target\nfeatures, which are then clustered using K-means to iden-\ntify the samples of the dominant cluster. These samples\nare mapped into the ResNet-based [12] feature space and\naligned with the source features. For the domain adaptation\ntask, a contrastive strategy [23, 27] ensures the similarity\nbetween the dominant target cluster and normal source sam-\nples, while for the anomaly detection task, a Deep Support\nVector Data Description (DSVDD) [28] objective enforces\nfeature compactness on the normal source data. Our frame-\nwork is modular, allowing for flexible component changes,\nand supports various adaptation strategies, including sta-\ntistical and adversarial alignment. Experiments performed\non standard UDA benchmarks [25, 30, 36] for semantic\nanomaly detection [33] demonstrate its effectiveness. Our\napproach achieves state-of-the-art (SoA) performance, even\ncompared to few-shot methods.\nContributions. The main contributions of this work can be\nsummarized as follows:\n• The two-fold unsupervised curse of UDA for one-class\nanomaly detection is formalized, and the induced chal-\nlenges are outlined.\n• A solution to the two-fold unsupervised problem is pro-\nposed by leveraging an intrinsic property of anomalies,\ni.e., their scarcity.\n• A UDA method for one-class anomaly detection is in-\ntroduced, leveraging a Vision Language Model, namely\nCLIP [27], for dominant cluster identification and align-\nment using a contrastive strategy.\n• Extensive experiments and analysis are conducted on sev-\neral benchmarks [17, 25, 30, 36], demonstrating the rele-\nvance of the proposed framework under both fully unsu-\npervised and few-shot settings.\nPaper Organization. Section 2 reviews UAD works under\ndomain shift. Section 3 defines the two-fold unsupervised\ncurse, while Section 4 and Section 5 detail the proposed\nsolution for solving it. Section 6 and Section 7 cover the\nexperiments and limitations of this method. Section 8 con-\ncludes and outlines future work.\n2. Related Works: Anomaly detection under\ndomain shift\nUnsupervised\nimage\nanomaly\ndetection\nis\na\nwell-\nestablished research area [7, 11, 13, 28, 29, 33, 35]\nwhere the aim is to learn a function ζ using a single class\ncorresponding to normal data from the normal-only dataset\nDn = {(Xi, yi); yi = 0}N\ni=1, to classify whether an input\nimage X is normal (y = 0) or not (y = 1). This is achieved\nby optimizing the following objective,\nmin\nζ\nE(Xi,yi)∼Dn [L (ζ(Xi), yi = 0)] ,\n(1)\n2\n\n\n(c)\n(a)\nLabeled target\nLabeled source\n(b)\nSource: normal +\nanomalous\nTarget: normal \n One-class source decision boundary\nDomain Alignment\nUnlabeled target\nUnlabeled source\nAnomaly data\nNormal data\nSource: normal  \nTarget: normal \n(few-shot)\nSource: normal  \nTarget: normal +\nanomalous\nFigure 2.\nComparison of our setting with previous works:\n(a) supervised source anomaly detection with supervised domain\nadaptation [16], (b) unsupervised one-class source anomaly detec-\ntion with few-shot domain adaptation [19, 20, 38], (c) our con-\nsidered setting: unsupervised one-class source anomaly detection\nwith unsupervised domain adaptation.\nwhere L is a loss enforcing feature compactness as in\nDSVDD [28] or a reconstruction loss typically used in\nautoencoders-based methods [7, 35]. Although achieving\nimpressive performance on standard benchmarks, the ma-\njority of AD methods [7, 11, 13, 28, 29, 33, 35] overlook\nthe domain gap problem where training and testing data\ndenoted as Ds and Dt, respectively, follow different dis-\ntributions due to uncontrolled variations in the acquisition\nsetting [4, 5]. This domain shift induces, therefore, a signif-\nicant drop in performance. To solve this issue, a handful of\ndomain generalization methods for UAD has been proposed\nrecently [4, 5]. In [5], multiple source domains are consid-\nered for learning domain-invariant features, thereby assum-\ning the availability of diverse large-scale datasets which is\nnot always guaranteed. To avoid relying on multiple do-\nmains during training, a self-supervised strategy is adopted\nin [4].\nNevertheless, the success of this approach heav-\nily depends on the similarity between the augmented data\nand target samples.\nAs a result, it necessitates tailoring\naugmentation techniques to unseen target datasets, if at all\npossible.\nGiven its effectiveness, domain adaptation has\nalso been explored to address the domain shift problem\nin AD [6, 16, 18–20]. Those techniques usually adopt a\nfew-shot adaptation paradigm by having access to a limited\nnumber of annotated target samples. While these methods\noffer innovative solutions for aligning source and target nor-\nmal data, they still rely on costly annotations [13] and are\nexposed to overfitting risks [32]. This emphasizes the need\nfor a fully unsupervised domain adaptation for UAD. How-\never, solving this challenge remains challenging given the\ndoubly unsupervised nature of the problem resulting from\nboth UAD and UDA, which is further described in the next\nsection.\n3. The Two-fold Unsupervised Curse\nLet us denote as Ds = {(Xs\ni, ys\ni )}Ns\ni=1 a labeled dataset\nfrom a given domain called source formed by Ns sam-\nples, where a sample Xs\ni ∈Rh×w×c and its associated la-\nbel ys\ni ∈{0, 1}, ∀i = {1, ..., Ns}. Let Dt be a second\nunlabeled dataset from a different domain, i.e., target, de-\nnoted as Dt = {Xt\ni}Nt\ni=1 and formed by Nt samples where\nXt\ni ∈Rh×w×c, ∀i = {1, ..., Nt}. In the following, we as-\nsume that Dt shares the same label space as Ds and that\nthere exists a domain gap between Ds and Dt. The goal of\nUnsupervised Domain Adaptation (UDA) for anomaly de-\ntection (whether formulated as a binary or one-class classi-\nfication problem), is to learn a model ζ : Rh×w×c →{0, 1}\nusing both Ds and Dt that generalizes to the target domain.\nIn other words, it aims at learning a domain invariant fea-\nture extractor f : Rh×w×c 7→X such that ζ = g ◦f with\ng : X 7→{0, 1} being the classifier and X the feature space\ngiven by f. This objective is achieved by minimizing the\nfollowing adaptation upper bound [2],\nϵt ≤ϵs + d(f(Ds), f(Dt)) + λ ,\n(2)\nwhere ϵt and ϵs are the expected classification errors on the\ntarget and source domains, respectively; d(f(Ds), f(Dt))\nestimates the discrepancy between the feature distributions\nfrom the two domains, and λ accounts for the error of an\nideal detector.\nWhile strategies for minimizing this upper bound are fea-\nsible in the context of binary or even multi-class classifi-\ncation [15, 31, 37], the non-availability of anomalous data\nduring training makes it difficult in the context of one-class\nclassification, where d(f(Ds), f(Dt)) cannot be estimated.\nIn fact, we can only use a subset Ds,n ⊂Ds formed by nor-\nmal data for training. For that reason, existing works on do-\nmain adaptation for one-class anomaly detection [6, 19, 38]\nrevisit the formulation given in Eq (2) by slightly simplify-\ning the problem. They pose it as a few-shot domain adapta-\ntion setting (instead of a fully unsupervised scenario). This\nmeans that they assume having access to a small subset\nDt,n ⊂Dt composed of normal samples only. As a result,\nthey reformulate Eq (2) as follows,\nϵt,n ≤ϵs,n + d(f(Ds,n), f(Dt,n)) + λ .\n(3)\nwhere ϵs,n and ϵt,n represent the source and target expected\nclassification errors related to the normal class, respectively,\nsince ϵs is not measurable in this context.\nNevertheless, in a fully unsupervised setting, we have ac-\ncess to Dt = Dt,a ∪Dt,n where Dt,a represents the subset\nof Dt formed by anomalies, without any prior information\nregarding the labels. Hence, directly aligning the feature\ndistributions estimated from the source and target data by\napproximating d(f(Ds,n), f(Dt)) would lead to obtaining\n3\n\n\na classification boundary that is completely obsolete for tar-\nget data, as shown in Figure 1 (b). We call this problem\nthe two-fold unsupervised curse as it is a consequence of a\nlack of supervision: (1) in the task of anomaly detection, as\nit is formulated as a one-class problem where only normal\nsource data are used; and (2) in the task of domain adap-\ntation which is fully unsupervised where only an unlabeled\ntarget set is available. Given that the problem is ill-posed, it\nremains a significant challenge that has not been addressed\nin the existing UAD literature.\n4. Rare Anomalies to the Rescue\nTo tackle the two-fold unsupervised curse described in Sec-\ntion 3, we introduce a key assumption and the main hypoth-\nesis it entails for enabling unsupervised domain adaptation\nfor one-class anomaly detection.\nAssumption (anomaly scarcity). For an unlabeled target\ndataset Dt = Dt,n ∪Dt,a, we assume that the number\nof anomalous samples is significantly smaller than the\nnumber of normal samples, i.e., |Da| << |Dn|, with |.|\nrefers to the cardinality.\nHypothesis (dominant cluster existence). Considering a\ntarget unlabeled anomaly detection dataset Dt = Dt,n ∪\nDt,a under the anomaly scarcity assumption, where Dt,n\nand Dt,a are respectively the normal and abnormal sub-\nsets, we hypothesize that there exists a feature extractor\nψ : Rh×w×c →X that generates from Dt a compact dom-\ninant cluster C ∈X predominated by normal samples.\nThe anomaly scarcity assumption often holds as it reflects\nmost real-world scenarios where anomalies are rare com-\npared to normal instances. In summary, our main objective\nis, therefore, to find or learn a feature extractor that verifies\nthe dominant cluster existence hypothesis. This hypothesis\nis a core component of the proposed method discussed in\nSection 5, as it allows the introduction of a novel paradigm\nto approach UDA for one-class UAD consisting of (1) find-\ning a feature exactor ψ that can generate a compact domi-\nnant cluster of features C corresponding to normal samples\nwithin an unlabeled target dataset Dt, (2) identifying the\nsubset of samples ˜Dt,n corresponding to this cluster in the\nfeature space of ψ, and (3) aligning the identified subset ˜Dt,n\nwith the source normal samples Ds,n in the feature space of\nthe source feature extractor f. Formally, we revisit Eq (3)\nas follows,\nϵt,n ≤ϵs,n + d(f(Ds,n), f( ˜Dt,n)) + λ ,\n(4)\nwhere ˜Dt,n = {Xt\ni | ψ(Xt\ni) ∈C}. Note that ψ can be ob-\ntained by focusing on learning compact cross-domain fea-\ntures from which C can be identified through feature group-\ning and selection techniques such as clustering or filtering.\nAs such, the proposed paradigm for UDA in one-class UAD\nlays the foundation for future research, where various tech-\nnical choices can be explored at each stage.\n5. Proposed Solution to the Two-fold Unsuper-\nvised Curse\nBuilding on the assumption and hypothesis formulated\nin Section 4, we present the technical choices, implemented\nas one possible solution for addressing UDA for semantic\none-class-based image UAD, as illustrated in Figure 3.\nOur approach has two branches. The upper branch de-\npicts a trainable backbone f that learns from both source\nand target domain data. The source features are optimized\nusing a Deep Support Vector Data Description (DSVDD)\nobjective [28].\nThe lower branch focuses on visual fea-\nture extraction from the unlabeled target domain through\na frozen CLIP visual encoder [27], defined as the ψ feature\nextractor. Clustering is applied to these visual features to\nestimate the dominant cluster C. Samples identified within\nC in the ψ visual encoder’s representation space are then se-\nlected within the space of the feature extractor f and then\naligned with the normal source features.\nSpecifically, given source and target image datasets Ds,n\nand Dt, we apply DSVDD on the source data, enforcing\nfeature compactness by minimizing the radius of a hyper-\nsphere to encapsulate the normal source representations.\nThis is done by solving the following optimization problem,\nmin\nθf LAD = min\nθf\n1\nNs\nNs\nX\ni=1\n∥f(Xs\ni) −µs,n∥2\n2, ∀Xs\ni ∈Ds,n,\n(5)\nwhere µs,n is the mean of the source features. For clus-\ntering, we use a K-means algorithm. Note that ψ can be\nf itself or any frozen visual encoder such as CLIP [27] or\nDINO-v2 [24].\nThe dominant cluster is identified as,\nC = arg max\nCk |Ck| for k ∈{1, ..., K},\n(6)\nwhere |Ck| is the size of the k-th cluster Ck, and K is a\nhyper-parameter defining the number of expected compo-\nnents in the space of ψ(Dt). When clustering is applied to\nf(Dt), the selected features for alignment are ˜Dt,n = C.\nWhen clustering is applied to ψ(Dt), the selected samples\nare:\n˜Dt,n = {f(Xt\ni) | ψ(Xt\ni) ∈C} ∀Xt\ni ∈Dt\n(7)\nAlignment between source and target features is achieved\nusing a contrastive strategy, where the loss of a single posi-\n4\n\n\nUDA loss\n-generated unlabeled target features \n-generated unlabeled target features\nVisual \nEncoder\nAD loss\nFeature\nExtractor\nFeature clustering\nSample selection\nTarget features\nSource features\nTarget features\nDSVDD results\nAlignment results\nOne-class source set\nUnlabeled target set\nTraining data\nNormal\nNormal\nAnomaly\nNormal\nNormal\nNormal\nFeature extractor’s feature space\nVisual encoder’s feature space\nLabeled source features\nSelected target features as part of \nNormal / Anomalous sample\n/\n<\n<\n<\n<\n<\n<\n<\n<\n<\n<\n<\nFigure 3. Overview of the proposed method: The top branch uses a trainable feature extractor with a DSVDD objective for one-class\nsource data. The bottom branch clusters the features using a frozen CLIP visual encoder to identify the dominant feature cluster and align\nit with normal source representations.• and ⋆denote normal and anomalous samples respectively.\ntive source-target pair ℓi,j:\nℓi,j = −log\nexp\n\u0012\nsim(f(Xs\ni ),f(Xt\nj))\nτ\n\u0013\nPNt\np=1 1[Xtp /∈˜\nDt,n] exp\n\u0012\nsim(f(Xs\ni ),f(Xtp))\nτ\n\u0013,\n(8)\nwhere sim(·, ·) denotes the cosine similarity, and τ is a tem-\nperature hyper-parameter. The UDA loss is computed as:\nLUDA =\n1\nNs × | ˜Dt,n|\nNs\nX\ni=1\n| ˜\nDt,n|\nX\nj=1\nℓi,j,\n(9)\nFinally, the overall loss is given by:\nL = λ1 LAD + λ2 LUDA,\n(10)\nwhere λ1 and λ2 are hyper-parameters for LAD and LUDA.\nAt inference, note that the visual encoder ψ is discarded\nand that only the feature extractor f is used for estimat-\ning whether the input data falls inside or outside the hyper-\nsphere estimated by the DSVDD model.\n6. Experimental Results\nAfter presenting the experimental setting in Section 6.1,\nwe compare experimentally our method against prior works\nin Section 6.2. Finally, we conduct additional experiments\nto assess the contributions of individual components within\nthe proposed method in Section 6.3.\n6.1. Experimental Setting\nThis section describes the datasets and the baselines used in\nour experiments. The evaluation metrics and the implemen-\ntation details are provided in supplementary materials. In\nall the experiments, the best and second performances are\nhighlighted in bold and underlined, respectively.\nDatasets.\nWe evaluate our approach on four stan-\ndard UDA benchmark datasets, Office-Home [30], Of-\nfice31 [30],VisDA [25], and PACS [17]. For the AD task,\nwe adopt a standard one-vs-all protocol, where a single\nclass is considered normal and the remaining are anoma-\nlies. We adopt the experimental protocol of previous DA\nworks [6, 19, 38] to allow for a fair comparison –that is,\nwe show results on ten classes from the ClipArt and Prod-\nuct domains for Office-Home, ten classes from Webcam and\nAmazon for Office31, and twelve classes from CAD to Real\nfrom VisDA. On PACS [17], like [4], we consider the Photo\ndomain as source and the remaining domains as targets.\nBaselines.\nAs no other works on UDA for semantic image\nUAD were previously introduced, we compare our method\nwith several few-shot SoA approaches. More specifically,\nwe consider BiOST [6] which is a one-shot approach and\nTSA [18], ILDR [16], IRAD [38], and MsRA [19] that are\nfew-shot methods. Furthermore, we introduce a few-shot\nvariant (Ours-Few-shot) of our approach, which augments\nthe target domain with normal and pseudo-anomalous sam-\nples similar to [9]. This augmentation yields semantically\npositive and negative pairs [9] useful for the contrastive\nalignment strategy described in Section 5. Additional de-\ntails are provided in the supplementary materials.\n6.2. Comparison against State-of-the-art.\nOur method outperforms previous SoA on the three con-\nsidered benchmarks used in our evaluation, as seen in Ta-\nble 1 and Table 2. More specifically, our fully unsuper-\nvised UDA-UAD importantly improves upon previous few-\n5\n\n\nNormal\nclass\nSource only\nDSVDD\nFew-shot\nUnsupervised\nBiOST\nTSA\nILDR IRAD MsRA\nOurs\nOurs\nClip Art →Product (C →P)\nBike\n97.48\n43.00\n69.10 89.90\n90.30\n94.30\n98.34\n85.71\nCalculator\n83.47\n69.00\n72.20 84.90\n82.20\n98.70\n97.76\n97.70\nDrill\n81.57\n66.40\n66.20 75.30\n73.00\n84.50\n74.19\n96.64\nHammer\n83.32\n50.10\n77.40 74.70\n84.50\n80.10\n89.55\n82.63\nKettle\n87.74\n63.00\n63.10 77.50\n75.80\n85.50\n94.08\n89.16\nKnives\n78.09\n48.80\n51.90 55.20\n63.90\n64.40\n79.25\n76.63\nPan\n74.00\n57.70\n63.70 72.20\n76.00\n80.50\n93.08\n91.07\nPaperclip\n53.04\n27.40\n74.70 78.70\n67.40\n79.70\n71.18\n67.98\nScissors\n86.45\n56.40\n64.70 79.50\n68.90\n85.50\n87.71\n88.43\nSoda\n51.21\n50.20\n57.40 70.30\n53.30\n72.40\n61.16\n92.37\nAvg.\n77.64\n53.20\n66.04 75.82\n73.53\n82.56\n84.63\n86.83\n±std\n±14.04\n±11.65 ±7.36 ±8.81 ±10.24\n±9.33\n±11.93\n±8.66\nProduct →Clip Art (P →C)\nBike\n82.55\n52.70\n65.80 83.10\n85.70\n86.60\n82.06\n92.99\nCalculator\n62.82\n65.20\n63.40 87.20\n79.20\n91.90\n91.59\n89.88\nDrill\n71.81\n47.00\n57.10 63.90\n71.20\n73.50\n70.58\n77.54\nHammer\n68.02\n43.70\n68.60 60.20\n77.00\n73.00\n84.33\n65.42\nKettle\n71.85\n47.70\n61.50 68.80\n70.00\n73.40\n75.38\n78.19\nKnives\n57.22\n63.10\n57.50 65.30\n70.30\n73.10\n77.74\n71.99\nPan\n71.44\n49.30\n63.50 69.30\n72.80\n80.00\n83.72\n82.46\nPaperclip\n26.19\n45.10\n49.90 69.70\n61.80\n69.00\n67.05\n55.93\nScissors\n63.42\n38.60\n70.10 66.20\n70.00\n72.30\n86.35\n77.63\nSoda\n66.82\n56.90\n55.80 60.20\n63.29\n59.40\n69.08\n62.63\nAvg.\n64.21\n50.93\n61.32 69.39\n72.13\n75.22\n78.79\n75.47\n±std\n±14.22\n±8.11\n±5.94 ±8.55\n±6.76\n±8.62\n±7.74\n±11.13\n(a) OfficeHome [36]\nNormal\nclass\nSource only\nDSVDD\nFew-shot\nUnsupervised\nBiOST\nTSA\nILDR IRAD MsRA Ours\nOurs\nWebcam →Amazon (W →A)\nBackpack\n86.48\n59.90\n76.30 91.90 90.20\n95.20\n95.40\n97.62\nBookcase\n35.77\n56.60\n59.60 78.40 82.20\n84.50\n76.25\n91.16\nBottle\n70.00\n60.80\n66.80 74.50 72.10\n74.00\n72.48\n77.32\nDesk Chair\n56.92\n57.60\n63.40 85.30 80.90\n87.20\n85.50\n92.06\nDesk Lamp\n82.26\n50.50\n60.90 72.60 67.50\n70.00\n82.38\n81.50\nHeadphones\n88.91\n57.60\n75.90 88.90 81.60\n92.20\n92.53\n95.06\nKeyboard\n79.83\n58.20\n69.90 88.30 93.20\n95.40\n95.40\n93.36\nLaptop\n51.79\n59.10\n63.00 86.20 98.10\n99.00\n95.63\n79.97\nMouse\n83.95\n65.80\n53.40 84.90 79.60\n89.90\n96.65\n92.97\nPen\n48.54\n68.50\n69.10 75.50 71.40\n73.90\n72.72\n71.20\nAvg.\n68.45\n59.46\n65.83 82.65 81.68\n86.13\n86.49\n87.22\n±std\n±17.84\n±4.70\n±6.86 ±6.46 ±9.37\n±9.72\n±9.44\n±8.48\nAmazon →Webcam (A →W)\nBackpack\n79.42\n47.90\n59.00 81.60 91.20\n97.50\n99.28\n97.59\nBookcase\n60.68\n49.90\n72.30 88.90 89.40\n93.10\n85.23\n94.29\nBottle\n40.94\n66.00\n69.80 86.90 95.30\n96.20\n93.65\n94.95\nDesk Chair\n71.66\n67.00\n66.20 76.10 90.30\n90.10\n93.67\n99.08\nDesk Lamp\n94.63\n55.50\n68.60 73.10 81.30\n83.90\n94.57\n97.61\nHeadphones\n70.99\n68.30\n72.40 93.70 91.60\n96.00\n96.54\n96.04\nKeyboard\n77.90\n66.00\n76.90 91.10 95.70\n98.10\n90.62\n76.59\nLaptop\n91.61\n62.10\n72.20 85.70 97.10\n98.20\n94.32\n97.67\nMouse\n72.17\n69.10\n69.40 82.20 85.40\n86.50\n96.35\n81.41\nPen\n44.26\n79.10\n86.10 97.60 98.90\n99.60\n97.09\n99.99\nAvg.\n70.43\n63.09\n71.29 85.69 91.62\n93.92\n94.13\n93.52\n±std\n±16.81\n±9.03\n±6.66 ±7.26 ±5.15\n±5.11\n±3.72\n±7.52\n(b) Office31 [30]\nTable 1. Ten-run average and standard deviation of AUC (%) on the Office datasets against previous SoA.\nNormal\nclass\nSource only\n(DSVDD)\nFew-shot\nUnsup.\nBiOST\nMsRA\nOurs\nOurs\nCAD →Real\nAeroplane\n67.71\n36.80\n81.56\n81.55\n84.86\nBicycle\n65.12\n59.20\n68.45\n74.58\n81.45\nBus\n66.01\n47.90\n68.12\n72.26\n82.17\nCar\n78.65\n53.80\n69.44\n82.78\n62.76\nHorse\n67.24\n58.00\n68.77\n80.17\n83.52\nKnife\n62.43\n54.10\n70.39\n71.52\n68.82\nMotorcycle\n69.45\n58.10\n65.64\n80.16\n91.15\nPerson\n42.11\n58.70\n59.18\n51.24\n69.68\nPlant\n57.77\n42.10\n65.81\n71.46\n70.58\nSkateboard\n60.70\n41.60\n61.30\n63.17\n83.71\nTrain\n54.75\n52.40\n69.73\n60.62\n69.98\nTruck\n62.08\n43.10\n59.05\n73.67\n57.84\nAvg.\n62.84\n50.48\n67.28\n71.93\n75.54\n±std\n±8.55\n±7.55\n±5.79\n±9.08\n± 9.80\nTable 2. Ten-run average and standard deviation of AUC (%) on\nthe on the VisDA dataset [25] against previous SoA.\nshot SoA on C →P and W →A of the Office-Home [36]\nand Office31 [30] datasets. In addition, we observe an im-\nprovement of over 10% in the VisDA dataset [25] with the\nfully unsupervised methodology over previous few-shot ap-\nproaches despite being challenged by the two-fold unsuper-\nvised curse. These results highlight the relevance of the pro-\nposed method, even in the presence of a large domain gap,\nFeature\nExtractor\nw/\nAdaptation\nw/\nClustering\nw/\nCLIP\nAUC (%)\nResnet18\n✗\n✗\n✗\n56.80±10.90\n✓\n✗\n✗\n56.08±7.22\n✓\n✓\n✗\n63.43±8.39\n✓\n✓\n✓\n72.47±11.04\nResnet50\n✗\n✗\n✗\n62.84±8.55\n✓\n✗\n✗\n64.33±6.42\n✓\n✓\n✗\n68.47±8.30\n✓\n✓\n✓\n75.54±9.80\nTable 3. Ablation on the components of the proposed method.\nas in the case of CAD images and real-world photos.\nIn the P →C and A →W adaptation of the Office\ndatasets, our few-shot variant also registers SoA perfor-\nmance, closely followed by our model trained under the\nfully unsupervised setting. These results highlight the flex-\nibility of our framework, which can leverage minimal la-\nbeled target data when available but remains highly effec-\ntive in a fully unsupervised setting.\nFurthermore, we compare the performance of our model\nto two pretrained CLIP visual encoders [27], namely CLIP-\nR50 and CLIP-ViT-B32 in Table 4. While the larger CLIP-\nViT-B32 architecture achieves an average AUC of 72.08%,\nour few-shot variant (71.93%) and unsupervised method\n(75.54%) still outperform it on the VisDA dataset [25]. In\n6\n\n\nNormal\nclass\nPretrained Visual encoders\nSource only\n(finetuned)\nFew-shot Adaptation\nUnsup. Adapt.\nOracle\n(Supervised)\nCLIP-R50\nCLIP-ViT-B32\nBiOST\nMsRA\nOurs\nOurs\nCAD →Real\nAeroplane\n33.97\n74.97\n67.71\n36.80\n81.56\n81.55\n84.86\n90.91\nBicycle\n53.52\n90.28\n65.12\n59.20\n68.45\n74.58\n81.45\n81.73\nBus\n46.40\n42.27\n66.01\n47.90\n68.12\n72.26\n82.17\n72.16\nCar\n67.84\n64.16\n78.65\n53.80\n69.44\n82.78\n62.76\n68.42\nHorse\n44.80\n75.48\n67.24\n58.00\n68.77\n80.17\n83.52\n88.70\nKnife\n67.76\n95.28\n62.43\n54.10\n70.39\n71.52\n68.82\n78.90\nMotorcycle\n61.22\n82.25\n69.45\n58.10\n65.64\n80.16\n91.15\n83.46\nPerson\n56.50\n56.26\n42.11\n58.70\n59.18\n51.24\n69.68\n85.19\nPlant\n62.06\n89.65\n57.77\n42.10\n65.81\n71.46\n70.58\n82.63\nSkateboard\n67.97\n91.52\n60.70\n41.60\n61.30\n63.17\n83.71\n83.73\nTrain\n53.61\n57.74\n54.75\n52.40\n69.73\n60.62\n69.98\n85.11\nTruck\n44.82\n45.08\n62.08\n43.10\n59.05\n73.67\n57.84\n78.91\nAvg.±std\n55.04±10.47\n72.08±17.83\n62.84±8.55\n50.48±7.55\n67.28±5.79\n71.93±9.08\n75.54±9.80\n81.65±6.11\nTable 4. Performance in AUC (%) of our Resnet50-based one-class anomaly detector on the target domain of the VisDA dataset [25] using\nvarious adaptation paradigms (from zero-shot to supervised).\nMethod\nPh. →Art\nPh. →Cartoon\nPh. →Sketch\nSource only\n64.06\n64.08\n57.35\nGNL [4]\n65.62\n67.96\n62.39\nOurs\n67.20\n75.35\n74.04\nTable 5. AUC (%) performance comparison with domain gener-\nalization for anomaly detection trained on the PACS [17] where\nPhoto (Ph.) is used as the source domain.\nDataset\nSource Only\nGRL [8]\nMMD [10]\nContrastive [23]\nVisDA\n62.84±08.55\n71.84±10.89\n73.12±10.36\n75.54±9.80\nA→W\n72.57±18.69\n90.43±10.68\n86.86±12.97\n94.82±07.52\nW→A\n67.70±18.32\n83.49±11.04\n83.92±09.90\n87.72±12.63\nC→P\n77.31±15.13\n83.40±12.35\n82.10±12.16\n90.54±14.06\nP→C\n63.88±15.60\n66.78±15.02\n67.00±14.41\n70.92±11.35\nTable 6. Performance in terms of AUC (%) using different adap-\ntation losses on the three UDA benchmarks.\nDataset\nSource only\nw/ Adaptation\nKmeans\nGMM\nMeanshift\nVisDA\n62.84±08.55\n75.54±10.23\n72.24±08.81\n74.14±07.44\nA→W\n72.57±18.69\n94.82±07.52\n96.45±05.43\n87.32±12.53\nW→A\n67.70±18.32\n87.72±12.63\n87.00±13.60\n86.68±08.53\nC→P\n77.31±15.13\n90.54±14.06\n90.85±11.18\n85.95±10.94\nP→C\n63.88±15.60\n70.92±11.36\n76.15±13.32\n73.50±13.49\nTable 7. Ablation on different clustering techniques. GMM and\nK-means use 10 components for VisDA and 2 for other datasets.\ncontrast, the smaller CLIP-R50 model gives significantly\nlower performance, with an average AUC of 55.04%. These\nresults demonstrate that despite their strong performance,\npretrained visual encoders are not specifically tailored for\nthe domain adaptation task; thus, they remain vulnerable\nto domain shift.\nTherefore, training domain adaptation-\nspecific models is still necessary to effectively bridge the\ngap between two given domains.\n6.3. Additional Experiments\nUnless stated otherwise, all the following experiments are\nperformed on VisDA [25]. We also report additional results\nand implementation details in the supplementary materials.\nFew-shot versus unsupervised paradigms.\nThe results\npresented in Table 4 compare pretrained visual encoders and\nsource-only detectors with different adaptation paradigms,\ni.e., few-shot, unsupervised, and supervised (oracle). The\nsource-only finetuned model improves slightly over the pre-\ntrained CLIP-R50 visual encoder [27] but still has lower\nperformance than the adaptation approaches, achieving an\naverage AUC of 62.84%. Among the few-shot methods, our\nfew-shot variant outperforms BiOST [6] and MsRA [19],\nachieving the highest AUC of 71.93%, which is comparable\nto the performance of a pretrained CLIP-ViT-B32 visual en-\ncoder. However, our unsupervised adaptation method sur-\npasses all these models, with an average AUC of 75.54%,\ndemonstrating its ability to effectively mitigate domain gaps\nwithout relying on labeled target data.\nThis can be ex-\nplained by the fact that after clustering, our model has ac-\ncess to more representative normal target data than few-shot\nmodels, hence better generalizing to the target normal class.\nOn the other hand, the oracle-supervised model, with ac-\ncess to full target labels, achieves the highest performance\n(81.65%). The relatively small gap between our unsuper-\nvised method and oracle confirms the relevance of the pro-\nposed unsupervised framework.\nAblation on the framework components.\nTable 3 pro-\nvides the results obtained when each component, namely\nthe adaptation loss, the dominant cluster identification\nthrough clustering, the use of an auxiliary visual encoder\n7\n\n\n2\n4\n6\n8\n10\nK-Means Components\n64\n66\n68\n70\n72\n74\n76\nAUC (%)\nFigure 4. Effect of varying the number of components in K-Means.\nψ(Dt) or the trainable features f(Dt). The results show\nthat without adaptation, a model trained only on source data\ngeneralizes poorly to the target domain with only 56.80%\nand 62.84% for ResNet18 and ResNet50, respectively. Di-\nrect adaptation of the source and the unlabeled target with-\nout clustering leads to inconsistent results, indicating low\ngeneralization capabilities to the target domain. Introducing\nclustering results in a significant performance boost. This\ncan be seen when clustering is applied to the original rep-\nresentations of the feature extractor, as the performance of\nResNet18 improves by +6.63% and ResNet50 by +5.63%,\nhighlighting the importance of identifying the dominant\ncluster prior to alignment. Note that our method using clus-\ntering and contrastive alignment strategy still outperforms\nthe best few-shot baseline MsRA [19]. Finally, the best re-\nsults are achieved when all components are combined. This\nconfiguration boosts the AUC to 72.47% for ResNet18 and\n75.54% for ResNet50. The substantial performance gains\ncan be attributed to the CLIP’s rich visual features, which,\ntogether with clustering and alignment, help achieve a more\nrobust anomaly detector capable of effectively handling do-\nmain shift.\nClustering methods.\nWe compare different clustering\ntechniques on all three UDA benchmarks in Table 7. The\nfirst observation we make is that any type of clustering con-\ntributes to improving the performance. K-means and GMM\nhave comparable results, without one clearly and consis-\ntently outperforming the other across datasets and adapta-\ntion directions. Meanshift clustering offers a performance\nincrease compared to source-only models. However, its per-\nformance remains lower than that of the other clustering\nmethods. In our experiments, we choose K-means clus-\ntering as it offers the best performance improvement with\nfewer parameters and simpler optimization than GMM. We\nfurther investigate the optimal number of K-Means compo-\nnents, as shown in Figure 4. The figure indicates that using\n8 to 10 components yields the highest performance, with\nan AUC of approximately 75-76%. Decreasing the number\nof components would gradually degrade the performance.\nThis suggests that a lower number of clusters may not cap-\nture the characteristics of the majority class, leading to in-\naccurate clustering and thus negatively impacting the gen-\neralization of the anomaly detection model across domains.\nAlignment strategies.\nBy comparing several alignment\nstrategies in Table 6, we observe that any alignment strat-\negy, in general, improves the performance consistently for\nall adaptation benchmarks. Contrastive alignment consis-\ntently outperforms other adaptation losses, including statis-\ntical (MMD) [10] and adversarial (GRL) [8] strategies.\nComparison against domain generalization methods.\nTable 5 compares the results of the GNL [4] with proposed\nUDA method on the PACS dataset [17], with Photo as the\nsource and Art, Cartoon, and Sketch as the target domains.\nIt can be seen that our UDA approach consistently outper-\nforms the GNL [4] method, particularly on Cartoon and\nSketch domains. This suggests the suitability of UDA for\nsemantic UAD as as it exposes the model to the target do-\nmain even if unlabeled.\n7. Limitations and Future Work\nOur method, presented in Section 5, is one possible solu-\ntion for addressing the problem of UDA for UAD. How-\never, it is worth noting that it was tested in the context\nof semantic anomaly detection [33], adopting a one-vs-all\nprotocol, to facilitate the comparison with the closest base-\nlines, namely [19, 38]. These methods typically require the\nuse of global features in contrast to standard anomaly de-\ntection, where fine-grained representations are usually tar-\ngeted. For that reason, our method focuses mostly on global\nrepresentations, while local features would be conceptually\nmore suitable for fine-grained anomaly detection. In future\nworks, we aim to extend our study to fine-grained anomaly\ndetection by exploiting more relevant local representations.\n8. Conclusion\nThis work is the first to address unsupervised domain\nadaptation\n(UDA)\nfor\none-class-based\nunsupervised\nanomaly detection (UAD), subject to what we refer to as\nthe two-fold unsupervised curse. To address this ill-posed\nproblem, an inherent property of anomalies, namely, their\nscarcity, is leveraged. This characteristic allows utilizing\nclustering, –as one possible solution– for identifying a\ndominant cluster within the unlabeled target set. Assuming\nthis cluster to be predominantly composed of normal\ndata, a contrastive alignment strategy is then used to\nalign its features with the normal source representations.\nExtensive experiments on standard UDA benchmarks\ndemonstrate that the proposed method effectively miti-\ngates the domain gap and enhances anomaly detection\nperformance across different domains, outperforming other\nsupervised adaptation approaches without requiring target\nannotations. Finding the optimal feature extractor remains\nan open research question. In future work, we intend to\nfurther explore compact representations across domains\nto improve the proposed domain adaptation framework.\n8\n\n\nReferences\n[1] Jinan Bao, Hanshi Sun, Hanqiu Deng, Yinsheng He, Zhaox-\niang Zhang, and Xingyu Li. Bmad: Benchmarks for medical\nanomaly detection. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR)\nWorkshops, pages 4042–4053, 2024. 1, 2\n[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando\nPereira. Analysis of representations for domain adaptation.\nIn Advances in Neural Information Processing Systems. MIT\nPress, 2006. 3\n[3] Marius Beul, David Droeschel, Matthias Nieuwenhuisen,\nJan Quenzel, Sebastian Houben, and Sven Behnke.\nFast\nautonomous flight in warehouses for inventory applications.\nIEEE Robotics and Automation Letters, 3(4):3121–3128,\n2018. 1\n[4] Tri Cao, Jiawen Zhu, and Guansong Pang.\nAnomaly de-\ntection under distribution shift.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6511–6523, 2023. 1, 3, 5, 7, 8\n[5] Jo˜ao Carvalho, Mengtao Zhang, Robin Geyer, Carlos\nCotrini, and Joachim M Buhmann. Invariant anomaly detec-\ntion under distribution shifts: a causal perspective. Advances\nin Neural Information Processing Systems, 36, 2024. 3\n[6] Tomer Cohen and Lior Wolf. Bidirectional one-shot unsu-\npervised domain mapping. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 1784–\n1792, 2019. 2, 3, 5, 7\n[7] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse\ndistillation from one-class embedding.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 9737–9746, 2022. 1, 2, 3\n[8] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International conference\non machine learning, pages 1180–1189. PMLR, 2015. 7, 8\n[9] Songwei Ge, Shlok Mishra, Chun-Liang Li, Haohan Wang,\nand David Jacobs. Robust contrastive learning using nega-\ntive samples with diminished semantics. Advances in Neural\nInformation Processing Systems, 34:27356–27368, 2021. 5\n[10] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard\nSch¨olkopf, and Alex Smola. A kernel method for the two-\nsample-problem. Advances in neural information processing\nsystems, 19, 2006. 7, 8\n[11] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang,\nand Yue Zhao. Adbench: Anomaly detection benchmark.\nAdvances in Neural Information Processing Systems, 35:\n32142–32159, 2022. 1, 2, 3\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2\n[13] Romain Hermary, Vincent Gaudilliere, Abd El Rahman\nShabayek, and Djamila Aouada. Removing geometric bias\nin one-class anomaly detection with adaptive feature pertur-\nbation. In IEEE Winter Conference on Applications of Com-\nputer Vision, WACV 2025, Tucson, AZ, USA, February 28-\nMarch 4, 2025. IEEE, 2025. 1, 2, 3\n[14] Chaoqin Huang, Aofan Jiang, Jinghao Feng, Ya Zhang, Xin-\nchao Wang, and Yanfeng Wang. Adapting visual-language\nmodels for generalizable anomaly detection in medical im-\nages. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 11375–11385,\n2024. 1, 2\n[15] Tarun Kalluri, Sreyas Ravichandran, and Manmohan Chan-\ndraker. Uda-bench: Revisiting common assumptions in un-\nsupervised domain adaptation using a standardized frame-\nwork. ECCV, 2024. 2, 3\n[16] Atsutoshi Kumagai, Tomoharu Iwata, and Yasuhiro Fuji-\nwara. Transfer anomaly detection by inferring latent domain\nrepresentations. In Advances in Neural Information Process-\ning Systems. Curran Associates, Inc., 2019. 2, 3, 5\n[17] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M\nHospedales. Deeper, broader and artier domain generaliza-\ntion. In Proceedings of the IEEE international conference on\ncomputer vision, pages 5542–5550, 2017. 2, 5, 7, 8\n[18] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu,\nYulin Wang, and Wei Li. Transferable semantic augmenta-\ntion for domain adaptation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 11516–11525, 2021. 3, 5\n[19] Shuang Li, Shugang Li, Mixue Xie, Kaixiong Gong, Jianxin\nZhao, Chi Harold Liu, and Guoren Wang. End-to-end trans-\nferable anomaly detection via multi-spectral cross-domain\nrepresentation alignment. IEEE Transactions on Knowledge\nand Data Engineering, 35(12):12194–12207, 2023. 2, 3, 5,\n7, 8\n[20] Yachun Li, Ying Lian, Jingjing Wang, Yuhui Chen, Chun-\nmao Wang, and Shiliang Pu.\nFew-shot one-class domain\nadaptation based on frequency for iris presentation attack de-\ntection. In ICASSP 2022-2022 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\npages 2480–2484. IEEE, 2022. 2, 3\n[21] Jiaqi Liu, Guoyang Xie, Jinbao Wang, Shangnian Li,\nChengjie Wang, Feng Zheng, and Yaochu Jin. Deep indus-\ntrial image anomaly detection: A survey. Machine Intelli-\ngence Research, 21(1):104–135, 2024. 1, 2\n[22] Nesryne Mejri, Laura Lopez-Fuentes, Kankana Roy, Pavel\nChernakov, Enjie Ghorbel, and Djamila Aouada. Unsuper-\nvised anomaly detection in time-series: An extensive evalua-\ntion and analysis of state-of-the-art methods. Expert Systems\nwith Applications, page 124922, 2024. 1\n[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 2, 7\n[24] Maxime Oquab, Timoth´ee Darcet, Theo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\nsell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. Dinov2: Learning robust visual features without\nsupervision, 2023. 4\n[25] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman,\nDequan Wang, and Kate Saenko. Visda: The visual domain\n9\n\n\nadaptation challenge.\narXiv preprint arXiv:1710.06924,\n2017. 2, 5, 6, 7\n[26] Joaquin Qui˜nonero-Candela, Masashi Sugiyama, Anton\nSchwaighofer, and Neil D. Lawrence. Dataset shift in ma-\nchine learning. MIT Press, London, England, 2022. 1\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 2, 4, 6, 7\n[28] Lukas Ruff, Robert A. Vandermeulen, Nico G¨ornitz, Lucas\nDeecke, Shoaib A. Siddiqui, Alexander Binder, Emmanuel\nM¨uller, and Marius Kloft. Deep one-class classification. In\nProceedings of the 35th International Conference on Ma-\nchine Learning, pages 4393–4402, 2018. 1, 2, 3, 4\n[29] Lukas Ruff, Jacob R. Kauffmann, Robert A. Vander-\nmeulen, Gr´egoire Montavon, Wojciech Samek, Marius\nKloft, Thomas G. Dietterich, and Klaus-Robert M¨uller. A\nunifying review of deep and shallow anomaly detection. Pro-\nceedings of the IEEE, 109(5):756–795, 2021. 1, 2, 3\n[30] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell.\nAdapting visual category models to new domains. In Com-\nputer Vision–ECCV 2010: 11th European Conference on\nComputer Vision, Heraklion, Crete, Greece, September 5-\n11, 2010, Proceedings, Part IV 11, pages 213–226. Springer,\n2010. 2, 5, 6\n[31] Inder Pal Singh, Enjie Ghorbel, Anis Kacem, Arunkumar\nRathinam, and Djamila Aouada. Discriminator-free unsuper-\nvised domain adaptation for multi-label image classification.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 3936–3945, 2024. 2,\n3\n[32] Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mondal,\nand Jyoti Prakash Sahoo. A comprehensive survey of few-\nshot learning: Evolution, applications, challenges, and op-\nportunities. ACM Comput. Surv., 55(13s), 2023. 2, 3\n[33] Luc PJ Str¨ater, Mohammadreza Salehi, Efstratios Gavves,\nCees GM Snoek, and Yuki M Asano. Generalad: Anomaly\ndetection across domains by attending to distorted features.\narXiv preprint arXiv:2407.12427, 2024. 1, 2, 3, 8\n[34] Han Sun, Kevin Ammann, Stylianos Giannoulakis, and Olga\nFink. Continuous test-time domain adaptation for efficient\nfault detection under evolving operating conditions. arXiv\npreprint arXiv:2406.06607, 2024. 1\n[35] Tran Dinh Tien, Anh Tuan Nguyen, Nguyen Hoang Tran,\nTa Duc Huy, Soan T.M. Duong, Chanh D. Tr. Nguyen, and\nSteven Q. H. Truong.\nRevisiting reverse distillation for\nanomaly detection. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 24511–24520, 2023. 1, 2, 3\n[36] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,\nand Sethuraman Panchanathan. Deep hashing network for\nunsupervised domain adaptation.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 5018–5027, 2017. 2, 6\n[37] Garrett Wilson and Diane J Cook. A survey of unsupervised\ndeep domain adaptation. ACM Transactions on Intelligent\nSystems and Technology (TIST), 11(5):1–46, 2020. 2, 3\n[38] Ziyi Yang, Iman Soltani, and Eric Darve. Anomaly detection\nwith domain adaptation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2958–2967, 2023. 2, 3, 5, 8\n10\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21022v1.pdf",
    "total_pages": 10,
    "title": "When Unsupervised Domain Adaptation meets One-class Anomaly Detection: Addressing the Two-fold Unsupervised Curse by Leveraging Anomaly Scarcity",
    "authors": [
      "Nesryne Mejri",
      "Enjie Ghorbel",
      "Anis Kacem",
      "Pavel Chernakov",
      "Niki Foteinopoulou",
      "Djamila Aouada"
    ],
    "abstract": "This paper introduces the first fully unsupervised domain adaptation (UDA)\nframework for unsupervised anomaly detection (UAD). The performance of UAD\ntechniques degrades significantly in the presence of a domain shift, difficult\nto avoid in a real-world setting. While UDA has contributed to solving this\nissue in binary and multi-class classification, such a strategy is ill-posed in\nUAD. This might be explained by the unsupervised nature of the two tasks,\nnamely, domain adaptation and anomaly detection. Herein, we first formulate\nthis problem that we call the two-fold unsupervised curse. Then, we propose a\npioneering solution to this curse, considered intractable so far, by assuming\nthat anomalies are rare. Specifically, we leverage clustering techniques to\nidentify a dominant cluster in the target feature space. Posed as the normal\ncluster, the latter is aligned with the source normal features. Concretely,\ngiven a one-class source set and an unlabeled target set composed mostly of\nnormal data and some anomalies, we fit the source features within a hypersphere\nwhile jointly aligning them with the features of the dominant cluster from the\ntarget set. The paper provides extensive experiments and analysis on common\nadaptation benchmarks for anomaly detection, demonstrating the relevance of\nboth the newly introduced paradigm and the proposed approach. The code will be\nmade publicly available.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}