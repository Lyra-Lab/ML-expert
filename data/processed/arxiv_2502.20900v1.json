{
  "id": "arxiv_2502.20900v1",
  "text": "2025-3-3\nDexGraspVLA: A Vision-Language-Action Framework\nTowards General Dexterous Grasping\nYifan Zhong1,2*, Xuchuan Huang1,2*, Ruochong Li2,3, Ceyao Zhang1,2, Yitao Liang1,2, Yaodong Yang1,2â€ ,\nYuanpei Chen1,2â€ \n1Institute for AI, Peking University, 2PKU-PsiBot Joint Lab, 3Hong Kong University of Science and Technology (Guangzhou)\nDexGraspVLA\n90+% success rate under 1200+ unseen \nobject, lighting, and background combinations\nFigure 1 | We propose DexGraspVLA, a hierarchical vision-language-action framework that reaches a 90+%\ndexterous grasping success rate under thousands of unseen object, lighting, and background combinations in a\nâ€œzero-shotâ€ real-world environment.\nAbstract\nDexterous grasping remains a fundamental yet chal-\nlenging problem in robotics. A general-purpose robot\nmust be capable of grasping diverse objects in ar-\nbitrary scenarios. However, existing research typi-\ncally relies on specific assumptions, such as single-\nobject settings or limited environments, leading to con-\nstrained generalization. Our solution is DexGraspVLA,\na hierarchical framework that utilizes a pre-trained\nVision-Language model as the high-level task planner\nand learns a diffusion-based policy as the low-level\nAction controller. The key insight lies in iteratively\ntransforming diverse language and visual inputs into\ndomain-invariant representations, where imitation\nlearning can be effectively applied due to the alle-\nviation of domain shift. Thus, it enables robust gener-\nalization across a wide range of real-world scenarios.\nNotably, our method achieves a 90+% success rate\nunder thousands of unseen object, lighting, and back-\nground combinations in a â€œzero-shotâ€ environment.\nEmpirical analysis further confirms the consistency of\ninternal model behavior across environmental varia-\ntions, thereby validating our design and explaining\nits generalization performance. We hope our work\ncan be a step forward in achieving general dexter-\nous grasping. Our demo and code can be found at\nhttps://dexgraspvla.github.io/.\n1. Introduction\nDexterous multi-fingered hands, as versatile robotic\nend-effectors, have demonstrated remarkable capabil-\nities across various manipulation tasks [1, 2, 3, 4, 5,\n6, 7, 8, 9]. Among these capabilities, grasping serves\nas the most fundamental prerequisite, yet remains\none of the most challenging problems. Existing dex-\nterous grasping approaches are primarily evaluated\non isolated objects or under simplified settings. Nev-\nertheless, real-world applications demand more gen-\nâˆ—Equal contribution. â€ Corresponding author(s): Yuanpei Chen{yuanpei.chen312@gmail.com},Yaodong Yang{yaodong.yang@pku.edu.cn}.\nÂ© 2025 PsiBot. All rights reserved\narXiv:2502.20900v1  [cs.RO]  28 Feb 2025\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\neral grasping capabilities that can function reliably\nin diverse scenarios such as industrial manufacturing\nand household environments. However, developing\ngeneral dexterous grasping capabilities presents multi-\nfaceted challenges. At the object level, the policy must\ngeneralize across diverse physical properties includ-\ning geometries, masses, textures, and orientations.\nBeyond object characteristics, the system must also\ndemonstrate robustness to various environmental fac-\ntors, such as lighting conditions, background complex-\nities, and potential disturbances. Compounding these\nchallenges, multi-object scenarios introduce additional\ncomplexity that demands sophisticated reasoning ca-\npabilities. For instance, in cluttered or stacked envi-\nronments, planning the optimal sequence to grasp all\nobjects becomes a crucial cognitive task that extends\nbeyond simple grasp execution.\nTraditional approaches in dexterous grasping follow\na two-stage pipeline: first predicting target grasp pose\nfrom single-frame perception, then executing open-\nloop motion planning to reach the pose [10, 11, 12].\nHowever, such methods are heavily constrained by\nprecise camera calibration and mechanical accuracy\nrequirements. End-to-end approaches like imitation\nlearning and reinforcement learning, enable closed-\nloop grasping by continuously adjusting actions based\non real-time perceptual feedback, offering more robust\nand adaptive solutions. Recent years have witnessed\nremarkable progress in applying reinforcement learn-\ning to robotic systems [13, 14, 15, 16]. Leveraging\nlarge-scale parallel simulation, reinforcement learn-\ning enables robots to undergo extensive training in\nsimulation and then deploy the learned policies to\nthe real-world. Despite such progress, the complexity\nof real-world physical parameters presents significant\nchallenges in simulation modeling, leading to an in-\nevitable sim-to-real gap. Meanwhile, researchers have\nexplored imitation learning approaches to learn ma-\nnipulation skills [17, 18, 19]. These methods collect\nhuman demonstration data through teleoperation and\ndirectly learn the mapping from raw perceptual input\nto robot control commands using supervised learn-\ning. Nevertheless, such approaches often struggle with\ngeneralization beyond the demonstration data. While\ngeneral grasping requires handling diverse objects and\nenvironments, collecting demonstrations for all situ-\nations is impractical. Thus, the key challenge lies in\nhow to efficiently utilize the demonstration data to\nachieve broader generalization.\nThe rapid emergence of vision and language foun-\ndation models [20, 21, 22, 23, 24] presents promising\nopportunities for robotic manipulation. Leveraging\nvast amounts of internet-scale data in pre-training,\nthese models demonstrate remarkable scene under-\nstanding and generalization capabilities for visual and\nlinguistic inputs. While it appears intuitive to directly\ntask these models with generating robotic control com-\nmands [25, 26], this straightforward strategy faces\nfundamental limitations. The absence of physical in-\nteraction data in their training process results in mod-\nels with limited spatial intelligence. An alternative\napproach integrates vision-language models (VLMs)\ninto robotic control policies, training them in an end-\nto-end manner [27, 28]. However, this paradigm typ-\nically demands an enormous volume of manually col-\nlected demonstrations [29, 30] in an attempt to en-\ncompass the full range of real-world diversity and\ncomplexity. Even so, these models exhibit markedly\nreduced performance on unseen scenarios and still re-\nquire further data collection and fine-tuning to handle\nnew conditions. In addition, the substantial dispar-\nity between robotics datasets and the massive pre-\ntraining corpora leads to catastrophic forgetting, com-\npromising the modelâ€™s valuable long-range reasoning\ncapabilities. Effectively utilizing foundation modelsâ€™\nworld knowledge to enhance generalization in robotic\npolicies remains challenging.\nIn this paper, we present DexGraspVLA, the first\nhierarchical Vision-Language-Action (VLA) framework\nfor general dexterous grasping that integrates the com-\nplementary strengths of foundation models and im-\nitation learning. At the high level, it utilizes a pre-\ntrained VLM as a task planner, which interprets and\nreasons about language instructions, plans the over-\nall grasping task, and provides supervisory signals.\nGuided by these signals and multimodal inputs, a low-\nlevel diffusion-based modularized controller produces\nclosed-loop action sequences. The essence of Dex-\nGraspVLA lies in leveraging foundation models to iter-\natively transform diverse vision and language inputs\ninto domain-invariant representations, where it then\nefficiently and effectively applies diffusion-based imi-\ntation learning to capture the action distribution in our\ndexterous grasping dataset. As a result, novel scenar-\nios outside the training set no longer induce failures,\nbecause the foundation models translate them into\nrepresentations resembling those encountered during\ntraining â€” thus remaining within the learned pol-\nicyâ€™s domain. This approach fuses the extensive world\nknowledge of foundation models with the strong ac-\ntion modeling capacity of imitation learning, thereby\nenabling robust generalization performance in real-\nworld applications.\nNotably, DexGraspVLA achieves an unprecedented\n90.8% success rate for grasping in cluttered scenes\nspanning 1,287 unseen object, lighting, and back-\nground combinations, all tested in a â€œzero-shotâ€ en-\nvironment. Systematic evaluations on a single-object\ngrasping benchmark show that DexGraspVLA reaches\na 98.6% aggregated success rate, outperforming exist-\n2\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\ning baselines whose controller learns directly from raw\nvisual inputs by at least 48%. Furthermore, our empir-\nical analysis reveals that the internal representations\nand the attention maps within DexGraspVLA remain\nconsistent across varying environments, thereby sub-\nstantiating its framework design and explaining its\nperformance. These results confirm that DexGraspVLA\ncan learn effectively from a modest amount of single-\ndomain human demonstrations while generalizing reli-\nably to a broad range of real-world situations, marking\na promising step toward general dexterous grasping.\n2. Related work\n2.1. Dexterous Grasping\nDexterous grasping typically falls into two categories:\ntwo-stage approaches and end-to-end methods. Two-\nstage approaches first generate a grasp pose and then\ncontrol the dexterous hand targeting this pose. The\nmain challenge is generating high-quality grasp poses\nbased on visual observation. Current methods em-\nploy sample-based [31, 32], optimization-based [11,\n12, 33, 34, 35, 36], or regression-based [37, 38] ap-\nproaches to generate target grasp poses, followed by\nmotion planning for robot execution. For instance,\nSpringGrasp [10] models uncertainties in partial ob-\nservations using optimization-based methods to im-\nprove grasp pose generation quality. UGG [39] pro-\nposes a diffusion-based approach to unify the genera-\ntion of grasping poses and object geometries. While\nthese methods benefit from decoupled perception and\ncontrol and simulation data generation, they typically\nsuffer from the lack of closed-loop feedback and sen-\nsitivity to disturbances and calibration errors.\nEnd-to-end methods directly model grasping trajec-\ntories using imitation learning or reinforcement learn-\ning. Recent works explored training dexterous manip-\nulation using reinforcement learning in simulation en-\nvironments and transfer to the real-world [40, 1, 41, 2,\n42, 43, 13, 14, 15, 16, 44, 3, 4, 5, 6, 7, 45, 46, 47, 48].\nDexVIP [49] and GRAFF [50] generate affordance\nhints using computer vision methods and use rein-\nforcement learning to train policies based on these\nfeatures. DextrAH-G [51] and DextrAH-RGB [52]\ndemonstrate some generalization capabilities in the\nreal-world through large-scale parallel training in\nsimulation. However, this reliance on simulation in-\nevitably introduces sim-to-real gaps, while direct train-\ning in the real-world suffers from poor sample ef-\nficiency. Recently, imitation learning using human\ndemonstration has shown remarkable results in com-\nplex tasks [50, 53, 54, 17, 18, 19, 55, 56, 57, 58, 59,\n60, 61, 62, 63, 64, 65, 66, 67]. These methods re-\nquire human teleoperation to collect demonstration\ndata and directly learn the distribution in the dataset.\nWhile being easier to train, this approach limits their\ngeneralization capabilities. SparseDFF [68] and Neu-\nral Attention Field [69] explore how to enhance gen-\neralization through 3D distilled feature fields.\n2.2. Foundation Models for Robotics\nRecent years have witnessed remarkable progress in\nfoundation models pretrained on large-scale datasets.\nVision foundation models [23, 24, 20, 70, 71] exhibit\nrobust out-of-distribution generalization, while VLMs\nincluding GPT-4o [22] and Qwen2.5-VL [72] demon-\nstrate sophisticated multimodal reasoning abilities.\nLeveraging these foundation models effectively has\nbecome a promising direction in robotics research.\nOne prominent approach, exemplified by RT-X [30],\nOpenVLA [27], Pi0 [28], and more [73, 74, 75, 76,\n77],involves directly fine-tuning VLMs on robotic data.\nHowever, this strategy demands a large volume of\ndemonstrations spanning diverse real-world condi-\ntions to achieve generalization. Even the largest cur-\nrently available robotic datasets [29, 30, 28] fall short\nof covering the full breadth of scenarios; models\ntrained on them still struggle to match their perfor-\nmance on seen domains and typically require addi-\ntional data collection and fine-tuning for new environ-\nments. Moreover, these models often sacrifice some\nof their advanced reasoning abilities due to the com-\nplexity of robotic manipulation tasks and the scarcity\nof specialized data. Another line of research, exempli-\nfied by VoxPoser [25] and Rekep [26], leverages VLMs\nto generate task-specific outputs, such as affordance\nmaps or constraint points, that can then be integrated\nwith conventional motion planning. While this hier-\narchical strategy generally preserves VLMsâ€™ inherent\nreasoning capacities, it relies on sufficiently robust\nlow-level controllers to execute high-level commands,\nmaking the design of effective interfaces critical. Our\nwork harnesses pre-trained foundation models to gen-\nerate domain-invariant representations, which facili-\ntates the learning of a dexterous grasping policy. By\noffloading much of the real-world complexity to the\nfoundation models, we can significantly reduce the\namount of demonstration data required and, at the\nsame time, realize strong zero-shot generalization.\n3. Problem Formulation\nOur goal is to develop a vision-based control policy\nfor language-guided dexterous grasping, formulated\nas a sequential decision-making problem. Initially, a\nlanguage instruction ğ‘™is given, e.g. â€œgrasp the toyâ€, to\ndirectly specify the target object. At each timestep ğ‘¡,\nthe policy ğœ‹receives a first-view image Iw\nğ‘¡âˆˆâ„ğ»Ã—ğ‘ŠÃ—3\nfrom the wrist camera (ğ»and ğ‘Šdenote the height and\n3\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nFigure 2 | Overview of our framework. DexGraspVLA adopts a hierarchical architecture composed of an\noff-the-shelf VLM-based high-level planner and a diffusion-based low-level controller. Given a cluttered\nscene, the planner reasons about the user prompt, e.g., â€œclear the tableâ€, decomposing it into multiple grasping\ninstructions when necessary. For each instruction ğ‘™, e.g., â€œgrasp the cookieâ€, the planner identifies the target\nobject ğ´from the head image Ih\nğ‘¡0 and marks its bounding box (ğ‘¥ğ´\n1 , ğ‘¦ğ´\n1 , ğ‘¥ğ´\n2 , ğ‘¦ğ´\n2 ) at initial time ğ‘¡0. The controller\nconsists of four parts: (1) two segmentation models including SAM, which obtains the objectâ€™s mask mğ‘¡0 at ğ‘¡0,\nand Cutie, a video segmentation model that continuously tracks the mask mğ‘¡during each grasping process; (2)\nthree vision encoders including two frozen DINOv2 that extract features from the third-view image Ih\nğ‘¡and the\nfirst-view image Iw\nğ‘¡, and a trainable ViT that deals with the mask mğ‘¡; (3) three MLP projectors that map the\nvisual features and robot proprioceptive state into the same feature space, forming a feature sequence; and (4)\na DiT that predicts an action chunk from ağ‘¡to ağ‘¡+ğ»âˆ’1. During the controllerâ€™s grasping process, the planner\nmonitors the execution, checks whether grasping succeeds, and assists re-grasping when failing. This process\ncontinues until the user prompt is fully completed.\nwidth of the image), a third-view image Ih\nğ‘¡âˆˆâ„ğ»Ã—ğ‘ŠÃ—3\nfrom the head camera, and the robot proprioception\nsğ‘¡âˆˆâ„13 consisting of seven arm joint angles sarm\nğ‘¡\nâˆˆâ„7\nand six hand joint angles shand\nğ‘¡\nâˆˆâ„6. Conditioned\non these observations, the robot produces an action\nağ‘¡= (aarm\nğ‘¡\n, ahand\nğ‘¡\n) âˆˆâ„13, where aarm\nğ‘¡\nâˆˆâ„7 and ahand\nğ‘¡\nâˆˆ\nâ„6 denote the target joint angles for arm and hand\nrespectively, by sampling from the action distribution\nğœ‹(Â·|{Iw\nğ‘—}ğ‘¡\nğ‘—=0, {Ih\nğ‘—}ğ‘¡\nğ‘—=0, {sğ‘—}ğ‘¡\nğ‘—=0, ğ‘™). This process continues\nuntil a termination condition is reached. The robot\nreceives a binary reward ğ‘Ÿâˆˆ{0, 1} indicating whether\nit has completed the instruction ğ‘™successfully. The goal\nof the policy ğœ‹is to maximize the expected reward\nğ”¼ğ‘™,{(Iw\nğ‘—,Ih\nğ‘—,sğ‘—,ağ‘—)}ğ‘‡\nğ‘—=0 [ğ‘Ÿ].\nMore generally, we consider the case where the\nuser prompt ğ‘may be a long-horizon task involving\nmultiple grasping processes, such as â€œclear the tableâ€.\nThis requires the policy ğœ‹to reason about the prompt,\ndecompose it into individual grasping instructions {ğ‘™ğ‘–},\nand complete them sequentially.\n4\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\n4. Methods\nThis section introduces DexGraspVLA, the first hierar-\nchical VLA framework for dexterous grasping. We will\nfirst elaborate DexGraspVLA framework (Section 4.1)\nand then detail our data collection procedure (Sec-\ntion 4.2), which together enable the training of a dex-\nterous grasping policy.\n4.1. DexGraspVLA Framework\nAs illustrated in Figure 2, DexGraspVLA adopts a hier-\narchical and modularized architecture composed of a\nplanner and a controller. Below we explain how each\npart is designed.\nPlanner. We recognize that to achieve general dex-\nterous grasping, the model needs to be able to handle\nmultimodal inputs, perform visual grounding, and\nconduct reasoning about user prompts. Building upon\nrecent advances in VLMs, we adopt an off-the-shelf\npre-trained Qwen-VL-Chat [78] as a high-level planner\nto outline and monitor the dexterous grasping work-\nflow. Given the user prompt ğ‘, the planner reasons\nabout the execution plan conditioning on the head\ncamera observation. Specifically, if ğ‘is a long-horizon\ntask description involving multiple grasping steps, e.g.,\nâ€œclear the tableâ€, the planner considers object positions\nand orientations on the table, and proposes a suitable\ngrasping instruction ğ‘™1 as a first step, e.g., â€œgrasp the\ncookieâ€. Otherwise, if ğ‘directly targets one object for\ngrasping, the planner regards it as the instruction ğ‘™.\nFor each instruction ğ‘™, the planner guides the low-\nlevel controller by marking the target object bounding\nbox (ğ‘¥1, ğ‘¦1, ğ‘¥2, ğ‘¦2) in the head camera image Ih\nğ‘¡0 at the\ninitial timestep ğ‘¡0. While the phrasing and content of\nlanguage instruction can be diverse and flexible for dif-\nferent users and cases, i.e., showing domain-variance,\nthe bounding box is a consistent format for object po-\nsitioning regardless of the changes in language and\nvisual inputs, i.e., achieving domain-invariance. Thus,\nthis transformation alleviates the learning challenge\nfor the controller.\nOn receiving the bounding box, the controller be-\ngins execution. During this process, the planner mon-\nitors the progress by querying the current head image\nat a frequency of 1Hz. If it finds that the robot suc-\ncessfully grasps the object, the planner executes a\nscripted placing motion to place the object into a bag\nand then resets the robotic arm and hand to the initial\nstate. Afterward, the planner proposes new grasping\ninstruction ğ‘™2 by reasoning about the prompt and the\nremaining objects in its view, until the prompt ğ‘is\nfully completed. On the other hand, if the controller\nfails to grasp the target object, the planner resets the\nrobot and re-initializes the grasping loop with a new\ninstruction based on the current object states.\nController.\nBased on the target bounding box\n(ğ‘¥1, ğ‘¦1, ğ‘¥2, ğ‘¦2), the controller aims to grasp the in-\ntended object in cluttered environments. We feed\nthis bounding box as input to SAM [23] to obtain an\ninitial binary mask m0 âˆˆ{0, 1}ğ»Ã—ğ‘ŠÃ—1 of the target\nobject and then use Cutie [79] to continuously track\nthe mask over time, producing mğ‘¡at each timestep ğ‘¡.\nThis ensures accurate identification in cluttered scenes\nthroughout the process. The problem is to learn the\npolicy ğœ‹that effectively models the action distribution\nğœ‹(Â·|Iw\nğ‘¡, Ih\nğ‘¡, sğ‘¡, mğ‘¡).\nTo achieve general-purpose dexterous grasping ca-\npabilities, the system must generalize effectively across\ndiverse real-world scenarios. However, the high vari-\nability in raw visual inputs Iw\nğ‘¡, Ih\nğ‘¡poses a fundamental\nchallenge to learning task-critical representations. Tra-\nditional imitation learning approaches often fail catas-\ntrophically even under minor variations in objects or\nenvironmental conditions. To address this issue, our\nsolution is again to convert potentially domain-varying\ninputs into domain-invariant representations suitable\nfor imitation learning. We recognize that while pixel-\nlevel perception can vary widely, the fine-grained se-\nmantic features extracted by large foundation models\ntend to be more robust and consistent. This will be\nempirically substantiated in Section 5.5. Thus, we\nutilize a feature extractor ğœ™, such as DINOv2 [20]\nthat has been pre-trained on internet-scale data, to\nobtain features from raw images. At each timestep ğ‘¡,\nwe obtain head camera image features\nzh\nğ‘¡= ğœ™h(Ih\nğ‘¡) âˆˆâ„ğ¿hÃ—ğ·h,\nand wrist camera image features\nzw\nğ‘¡= ğœ™w(Iw\nğ‘¡) âˆˆâ„ğ¿wÃ—ğ·w,\nwhere ğ¿h, ğ·h, ğ¿w, ğ·w denote length and hidden dimen-\nsion of the feature sequences for head and wrist re-\nspectively. As we show in Section 5.5, these extracted\nfeatures remain comparatively invariant to distracting\nvisual factors.\nUp to now, raw language and vision inputs, includ-\ning instruction ğ‘™and images Iw\nğ‘¡, Ih\nğ‘¡, have been iter-\natively transformed into domain-invariant represen-\ntations, including mask mğ‘¡and features zh\nğ‘¡, zw\nğ‘¡, by\nleveraging foundation models. This lays the stage for\nimitation learning. We now learn the policy ğœ‹that\npredicts an action chunk of horizon ğ»conditioning on\nthese representations.\nTo fuse the object mask with head camera features,\nwe project mğ‘¡into the head image feature space using\na randomly initialized ViT, producing zm\nğ‘¡\nâˆˆâ„ğ¿hÃ—ğ·h.\n5\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nWe then concatenate zm\nğ‘¡and zh\nğ‘¡patch-wise to form\nÂ¯zh\nğ‘¡âˆˆâ„ğ¿hÃ—2ğ·h.\nSubsequently, we map Â¯zh\nğ‘¡, the wrist-camera features\nzw\nğ‘¡, and the robot state sğ‘¡into a common embedding\nspace with separate MLPs, yielding Ëœzh\nğ‘¡, Ëœzw\nğ‘¡, and Ëœzs\nğ‘¡.\nThese embeddings are then concatenated to form the\nfull observation feature sequence\nËœzobs\nğ‘¡\nâˆˆâ„(1+ğ¿h+ğ¿w)Ã—ğ·.\nFor action prediction, we employ a diffusion trans-\nformer (DiT) [80] to generate multi-step actions, fol-\nlowing the diffusion policy paradigm [81, 76, 28].\nConcretely, at each timestep ğ‘¡, we bundle the next ğ»\nactions into a chunk Ağ‘¡= ağ‘¡:ğ‘¡+ğ»= [ağ‘¡, ağ‘¡+1, . . . , ağ‘¡+ğ»âˆ’1].\nDuring training, a random diffusion step ğ‘¡ğ‘‘= ğ‘˜is sam-\npled, and Gaussian noise ğis added to Ağ‘¡, yielding the\nnoised action tokens xğ‘˜. Formally,\nxğ‘˜= ğ›¼ğ‘˜Ağ‘¡+ ğœğ‘˜ğ,\nwhere ğ›¼ğ‘˜and ğœğ‘˜are the standard DDPM coefficients.\nWe then feed xğ‘˜into the DiT alongside the observation\nfeature sequence Ëœzobs\nğ‘¡\n. Each DiT layer performs bidi-\nrectional self-attention over the action tokens, cross-\nattention to Ëœzobs\nğ‘¡\n, and MLP transformations, ultimately\npredicting the original noise ğ. By minimizing the\ndiscrepancy between predicted and true noise, the\nmodel learns to reconstruct the ground-truth action\nchunk Ağ‘¡. At inference time, iterative denoising steps\nrecover the intended multi-step action sequence from\nthe learned distribution, enabling robust imitation of\ncomplex, long-horizon behaviors. We also employ the\nreceding horizon control strategy that only executes\nthe first ğ»ğ‘actions before generating a new action\nchunk prediction, enhancing real-time responsiveness.\nOverall, DexGraspVLA performs imitation learn-\ning on domain-invariant representations derived from\ndomain-varying inputs via foundation models. This\napproach not only leverages the world knowledge and\ngeneralization capabilities of foundation models, but\nalso effectively captures the mapping from these ab-\nstracted representations to the final action output.\nNext, we discuss our data collection that fuels Dex-\nGraspVLA training.\n4.2. Data Collection\nTo train our dexterous grasping policy, we manu-\nally collect a dataset comprising 2,094 successful\nepisodes of grasping in cluttered scenes. This dataset\ninvolves 36 household objects spanning a broad range\nof sizes, weights, geometries, textures, materials, and\ncategories. Each episode ğœ= {(Ih\nğ‘¡, Iw\nğ‘¡, sğ‘¡, mğ‘¡, ağ‘¡)}ğ‘‡\nğ‘¡=0\nHead Camera\n(Realsense D435)\nWrist Camera\n(Realsense D405C)\n7-DoF Robotic Arm\n(Realman RM75-6F)\n6-DoF Robotic Hand\n(PsiBot G0-R)\nOperational \nWorkspace\nFigure 3 | The hardware platform used for dexterous\ngrasping.\nrecords the raw camera images Ih\nğ‘¡, Iw\nğ‘¡, the robot pro-\nprioception sğ‘¡, the object mask mğ‘¡, and the action\nağ‘¡at each timestep ğ‘¡. The mask mğ‘¡is labeled in the\nsame way as in the controller. For each object, we\nplace it at nine locations arranged in a 3 Ã— 3 grid and\ncollect multiple grasping demonstrations at each po-\nsition. The other objects in the cluttered scenes are\nrandomized between episodes. These demonstrations\nare performed at typical human motion speeds, tak-\ning about 3.5 s each. They undergo rigorous manual\ninspection to ensure quality and reliability. The Dex-\nGraspVLA controller is trained on this dataset with\nimitation learning.\n5. Experiments\nIn this section, we comprehensively evaluate the per-\nformance of DexGraspVLA. All experiments are con-\nducted on a different robot and environment from\nthe demonstration setup. This \"zero-shot\" setting is\nfundamentally more challenging than most prior im-\nitation learning research, which relies on few-shot\nlearning for high performance. Our experiments seek\nto address the following questions:\n1. How effectively does DexGraspVLA generalize to\nthousands of diverse, previously unseen object,\nlighting, and background combinations in clut-\ntered scenes? (Section 5.2)\n2. How does DexGraspVLAâ€™s generalization advan-\ntage compare against baselines that do not utilize\nfrozen feature extractors and learn directly from\nraw visual inputs? (Section 5.3)\n3. How accurate are bounding-box predictions from\nthe high-level planner of DexGraspVLA across\ndifferent scenarios? (Section 5.4)\n4. Does DexGraspVLA exhibit consistent internal\n6\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\n(a)\n(b)\nFigure 4 | (a) A representative part of unseen ob-\njects used in our experiments. In total, we test Dex-\nGraspVLA on 360 unseen objects that span a wide\nrange of categories. (b) A t-SNE projection illustrat-\ning the diversity of all unseen objects. Each point\ndenotes one object, where color indicates shape (e.g.,\nbottle, cylinder, sphere), marker type denotes sur-\nface roughness (smooth, medium, rough), and point\nsize reflects mass. The plot is derived from a high-\ndimensional attribute space (including length, width,\nheight, mass, roughness, and shape), highlighting the\nbroad coverage of sizes, weights, geometries, and tex-\ntures represented by these objects.\nmodel behaviors under varying environments?\n(Section 5.5)\n5.1. Experiment Setups\nHardware Platform. As illustrated in Figure 3, the\nrobot we use for dexterous grasping is a 7-DoF Real-\nman RM75-6F arm paired with a 6-DoF PsiBot G0-R\nhand. A Realsense D405C camera, mounted on the\narmâ€™s wrist, provides a first-person viewpoint, while\na Realsense D435 camera on the robotâ€™s head offers\na third-person perspective. Objects to be grasped are\nplaced on a table in front of the robot. The control\nfrequency of the robot is 20 Hz.\nBaselines. To the best of our knowledge, there are no\nexisting work that can directly serve as baselines for\ncomparison. Most of dexterous grasping methods can-\nUnseen\nObjects\n(360)\nUnseen\nBackgrounds\n(6 Ã— 103)\nUnseen\nLightings\n(3 Ã— 103)\nAggregated\n(1287)\nOurs@1\n91.1%\n90.5%\n90.9%\n90.8%\nOurs@2\n95.3%\n94.2%\n95.1%\n94.7%\nOurs@3\n96.7%\n96.7%\n97.4%\n96.9%\nTable 1 | The performance of DexGraspVLA under dif-\nferent unseen conditions. The first row suggests that\nDexGraspVLA consistently achieves a 90+% success\nrate across thousands of unseen object, background,\nand lighting combinations, highlighting robustness\nto environmental changes and strong generalization.\nThe second and third row shows its potential to reach\neven higher success rates given more chances.\nnot process language inputs for cluttered scene, while\nexisting VLA frameworks that accept language inputs\nare incompatible with dexterous hands. Therefore,\nwe compare the following methods: (1) DexGraspVLA\n(Ours): A full implementation of DexGraspVLA. (2)\nDexGraspVLA (DINOv2-train): Same design as Ours\nexcept that the two DINOv2 models are trainable in-\nstead of frozen. (3) DexGraspVLA (ViT-small): Same\ndesign as Ours except that the two DINOv2 models are\nreplaced with two small trainable pretrained ViTs (the\nR26-S-32 ResNet-ViT hybrid from Steiner et al. [82]).\nEmpirically, DexGraspVLA (ViT-small) represents an\nenhanced version of diffusion policy [81]. Implemen-\ntation details of these methods are provided in Ap-\npendix A. In preliminary experiments, we find that\nfailures may arise from randomness in policy inference\nand can be overcome with additional attempts. Thus,\nin Section 5.2, we compare DexGraspVLA (Ours@ğ‘˜),\nwith ğ‘˜ranging from 1 to 3. These are the same as\nOurs except that they are allowed ğ‘˜attempts for each\ntest respectively. Ours@1 is equivalent to Ours. Note\nthat re-grasps performed by the policy after an initial\nfailure within a single attempt are allowed and do\nnot count as separate attempts. In Section 5.4, we\nreport the bounding box prediction performance of\nDexGraspVLA (planner), which is a full implementa-\ntion of the high-level planner in DexGraspVLA.\n5.2. Large-Scale Generalization Evaluation\nTasks. We curate 360 previously unseen objects, 6\nunseen backgrounds, and 3 unseen lighting conditions.\nThe objects are carefully selected to ensure they cover\na broad range of sizes, weights, geometries, textures,\nmaterials, and categories, while also being graspable\nby our dexterous hand. This diversity is visualized in\nFigure 4. The backgrounds and lighting conditions are\nalso chosen to be highly different. Based on this setup,\n7\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nwe design three types of grasping tasks in cluttered\nscenes, each cluttered scene involving approximately\nsix objects: (1) Unseen Objects: Grasp an unseen object\nfrom a random scene on a white table under white\nlight. Each of the 360 unseen objects is targeted once,\nsumming up to 360 tests. (2) Unseen Backgrounds: We\nfirst randomly select 103 unseen objects as the object\nsubset S. For each background, we randomly arrange\n103 cluttered scenes with objects in S under white\nlight. Each of the 103 objects is targeted once, leading\nto 618 tests in total. (3) Unseen Lightings: For each\nunseen lighting, we construct 103 cluttered scenes\nwith objects in S on a white table. We target each of\nthe 103 objects once, amounting to 309 tests. Details\ncan be found in Appendix B.\nMetric. We consider a grasping attempt successful if\nit holds the object 10 cm above the table for 20 s. We\nreport success rate as the evaluation metric, which is\ndefined as the number of successful tests divided by\nthe total number of tests. We also report the aggre-\ngated performance as a weighted sum of individual\nsuccess rates based on their proportions.\nResults. We present the quantitative results in Ta-\nble 1. From the first row (â€œOurs@1â€), DexGraspVLA\nachieves a 91.1% single-attempt success rate on 360\nunseen objects, 90.5% on 6 unseen backgrounds, and\n90.9% under 3 unseen lighting conditions, resulting in\na 90.8% aggregated success rate. These results demon-\nstrate that DexGraspVLA accurately controls the dex-\nterous hand to grasp the specified object from clutter,\nwhile remaining robust to environmental changes. No-\ntably, although the evaluation environment is novel\nand the tasks are previously unseen, DexGraspVLA\nconsistently achieves high success rates without any\ndomain-specific fine-tuning, underscoring strong gen-\neralization. This suggests that our framework substan-\ntially alleviates the longstanding challenge in imita-\ntion learning â€” namely, overfitting to a single domain\nand relying on finetuning for satisfactory performance\nâ€” and is potentially meaningful for a wide range of\napplications. We further analyze the source of this\ngeneralization in 5.5.\nQualitatively, DexGraspVLA learns to dexterously\nadjust the arm and hand to accommodate varying ob-\nject geometries, sizes, and positions. Although phys-\nical interference or suboptimal actions occasionally\ncause missed grasps, the closed-loop nature of our\npolicy facilitates re-grasps based on updated observa-\ntions, enhancing robustness. The method also toler-\nates human-induced perturbations, as the robot can\ntrack and follow repositioned objects until a successful\ngrasp is achieved.\nFrom the second and third rows (â€œOurs@2â€ and\nâ€œOurs@3â€), we observe that while randomness and\n60.0%\n30.0%\n98.5%\n35.0%\n43.5%\n98.8%\n50.5%\n34.8%\n98.6%\nSeen Objects\nUnseen Objects\nAggregated\n0\n20\n40\n60\n80\n100\nViT-small\nDINOv2-train\nOurs\nSuccess rate (%)\nFigure 5 | DexGraspVLA significantly outperforms al-\nternative designs in both seen and unseen object grasp-\ning experiments in the â€œzero-shotâ€ environment. This\nconfirms the effectiveness of applying imitation learn-\ning on domain-invariant features.\nincidental failures can occur on a single attempt, mul-\ntiple attempts typically lead to success, elevating the\noverall performance to 96.9% at three tries. This in-\ndicates that our approach has the capacity to reach\neven higher success rates. Finally, our model takes\naround 6 s on average to grasp an object, which is\nclose to that of humans and ensures practical usability\nin real-world scenarios.\nOverall, our large-scale evaluations confirm that\nDexGraspVLA can robustly handle a wide spectrum\nof unseen scenarios, representing a meaningful step\ntoward general dexterous grasping and promising\nbroader real-world deployment.\n5.3. Comparison to Baselines without Frozen\nVision Encoders\nTasks. To systematically compare DexGraspVLA with\nbaselines that do not employ frozen vision encoders\nand learn directly from raw visual inputs, we conduct\nsingle-object grasping experiments using 13 seen ob-\njects from the training dataset and 8 unseen objects.\nWe select five locations on the table that both span the\noperational workspace and remain within the robotâ€™s\nreach and the head cameraâ€™s field of view. Each object\nis placed at these five points, and at each point, we\nlet the policy grasp it twice. Note that the two grasps\nof the same object at the same point are counted as\ntwo separate tests rather than repeated attempts of\nthe same test. This approach quantitatively accounts\nfor the randomness in the experiments. In total, this\ninvolves 210 tests. The environmental conditions in\nthese experiments are white tabletop and white light.\nMetric. We report the success rates of each method\nin the same way as Section 5.2.\nResults. Figure 5 demonstrates that DexGraspVLA\n(Ours) consistently reaches a more than 98% success\n8\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nNo\nDistraction\nBackground\nDistraction\nLighting\nDistraction\nAggregated\nPlanner\n96.7%\n100.0%\n100.0%\n99.3%\nTable 2 | DexGraspVLA planner robustly achieves near-\nperfect accuracy for bounding-box prediction in clut-\ntered scenes across different environment conditions.\nrate on both seen and unseen object grasping exper-\niments, outperforming both DexGraspVLA (DINOv2-\ntrain) and DexGraspVLA (ViT-small) by a significant\nmargin. The fact that the aggregated performance\nof our method in the â€œzero-shotâ€ test environment is\nnear-perfect shows that DexGraspVLA (ours) is not\naffected by domain shift in visual inputs. We also no-\ntice that the performance on unseen objects is even\nslightly better than on seen objects, which again con-\nfirms that our model learns to accomplish the grasping\ntask instead of overfitting to seen data in the training\nset. By contrast, the alternative designs fail to work\nproperly in a novel environment, since they directly\nmap raw inputs to actions and the perceptual changes\neasily render them out of distribution.\n5.4. Bounding-box Prediction Accuracy of\nPlanner\nTasks. The bounding-box prediction accuracy of the\nplanner is crucial to the success of grasping, as it de-\ntermines the target for the controller. To evaluate this\naccuracy, we design three types of tasks featuring dif-\nferent environmental distractions: (1) No Distraction\n(1 scenario): The cluttered scene is arranged on a\nwhite table under white light; (2) Background Distrac-\ntion (2 scenarios): The cluttered scene is placed on\neither a calibration board or a brightly colored table-\ncloth, both under white light; (3) Lighting Distraction\n(2 scenarios): The scene is set up in a dark room il-\nluminated by either a desk lamp or a disco light. For\neach scenario, we randomly arrange five cluttered\nscenes, each containing six randomly selected objects,\nand then record head-camera images. For each object,\nwe provide a textual prompt describing its appear-\nance and location, and check whether the plannerâ€™s\nbounding-box prediction accurately marks the target.\nIn total, No Distraction accounts for 30 tests, while\nBackground Distraction and Lighting Distraction both\nhave 60 tests, amounting to 150 tests overall.\nMetric. We define a bounding box as accurate if it\ntightly encloses the target object. Accuracy is then\nmeasured as the proportion of accurate bounding\nboxes over all tested objects.\nResults. The accuracy is reported in Table 2. For 150\nprompts, the planner only mislabels one bounding box\nWhite\nMosaic\nTablecloth\nTablecloth\n& Disco Light\nRaw \nImage\nDINOv2 \nFeature\nBinary\nMask\nAttention \nMap\nAttention \nVisualized\nFigure 6 | DexGraspVLA is robust to environmental\nvariations. The first row presents the cropped raw\nhead images of the same cluttered objects in four\ndifferent environments: a white table, a calibration\nboard, a tablecloth, and a tablecloth under disco light.\nThe second row shows that the DINOv2 features of\nimages are consistent across variations. The third row\nis the masks of the target objects accurately tracked\nby Cutie. The fourth row reflects that the averaged\nattention maps of DiT to head image features are also\nconsistent regardless of perceptual differences. The\nfifth row confirms DexGraspVLA is attending to the\ncorrect object. See Appendix B for full images.\nwhile succeeding in the other 149 tests, resulting in\nan aggregated accuracy exceeding 99%. This proves\nthat our planner reliably performs visual grounding\nof user prompts and is capable of marking the correct\nbounding box for the controller across varying degrees\nof background and lighting complexity.\n5.5. Internal Model Behavior Analysis\nTo further validate our design, we empirically prove\nthat the internal model behavior is consistent across\nvisual variations and show the results in Figure 6. Due\nto space constraints, we only show the relevant por-\ntion of each image containing the tabletop workspace.\nThe complete, uncropped images are provided in Ap-\npendix B. Specifically, we design four vastly different\nenvironmental conditions: a white table, a calibration\nboard, a colorful tablecloth, and a colorful tablecloth\nwith disco light. In each environment, we construct\nthe same cluttered scene containing nine objects and\nlet DexGraspVLA â€œgrasp the blue yogurt in the middleâ€.\nWhile the head images in the first row of Figure 6 ap-\npear to be markedly diverse, the DINOv2 features in\nthe second row look rather consistent. These features\nare visualized by mapping principal components to\nRGB channels as done in Oquab et al. [20]. Across\nenvironments, the object properties are robustly main-\n9\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\ntained and matched, which fundamentally allows Dex-\nGraspVLA trained on a single data domain to general-\nize. The third row shows that Cutie accurately tracks\nthe object, providing the correct guidance to the con-\ntroller. Based on the domain-invariant mask and the\nDINOv2 features, the DiT action head now predicts the\nsubsequent actions. In the fourth row, we average and\nnormalize all cross-attentions to the head image from\nDiT. We find that all attention maps exhibit the same\nbehavior of focusing on the target object instead of be-\ning distracted by environments. The fifth row overlays\nthe attention map on the raw image to confirm the\nreasonable attention pattern. All visualization details\nare provided in Appendix B. Therefore, we substanti-\nate that DexGraspVLA indeed transforms perceptually\ndiverse raw inputs into invariant representations, on\nwhich it effectively applies imitation learning to model\nthe data distribution, explaining its superior general-\nization performance. Expectedly, it successfully grasps\nthe yogurt in all four environments.\n6. Limitations\nAlthough DexGraspVLA achieves high success rates\nacross a range of unseen scenarios, certain limita-\ntions remain. First, due to the time limit, our train-\ning dataset does not encompass very small objects or\nextremely cluttered environments; performance on\nthese more challenging cases could improve with ded-\nicated data collection. Additionally, we have not yet\nexplored functional grasping for subsequent object\nusage, which is a promising direction for future work.\n7. Conclusion\nThis paper presents DexGraspVLA, the first hierarchi-\ncal vision-language-action framework that advances\ntoward general dexterous grasping. It utilizes a pre-\ntrained VLM as the high-level planner to plan the\ngrasping process and a diffusion-based policy as the\nlow-level controller to perform closed-loop action\nprediction for grasping. Within this paradigm, Dex-\nGraspVLA capitalizes on the world knowledge of foun-\ndation models to understand diverse raw inputs and\ntransform them into domain-invariant representations.\nImitation learning is then applied to model the map-\nping from representation to action distribution, which\nis highly effective due to the alleviation of domain\nshift. Our large-scale evaluations show that it attains\na success rate exceeding 90% across thousands of\nunseen cluttered scenes in a â€œzero-shotâ€ test environ-\nment, demonstrating robust generalization. An em-\npirical analysis of its internal model behavior further\nvalidates the underlying framework design. Overall,\nDexGraspVLA demonstrates the promise of leverag-\ning foundation models to enhance generalization in\ndexterous grasping. We plan to further refine its per-\nformance and broaden its applications in future work.\nAcknowledgments\nWe would like to express our sincere gratitude to Yijie\nXu for his substantial assistance with the experiments,\nto Yu Li, Yue Chen, and Qianxu Wang for their insight-\nful discussions, to Jie Chu for exploring the capabilities\nof foundation models, and to Chengdong Ma and Zijie\nYu for their generous help. Additionally, we deeply\nappreciate the significant support from our colleagues\nat Psibot in hardware, engineering, and procurement.\nReferences\n[1] Tao Chen, Megha Tippur, Siyang Wu, Vikash\nKumar, Edward Adelson, and Pulkit Agrawal. Vi-\nsual dexterity: In-hand dexterous manipulation\nfrom depth. In Icml workshop on new frontiers in\nlearning, control, and dynamical systems, 2023.\n[2] Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike\nLambeta, Yi Ma, Roberto Calandra, and Jitendra\nMalik. General in-hand object rotation with vi-\nsion and touch. In Conference on Robot Learning,\npages 2549â€“2564. PMLR, 2023.\n[3] Binghao Huang, Yuanpei Chen, Tianyu Wang,\nYuzhe Qin, Yaodong Yang, Nikolay Atanasov,\nand Xiaolong Wang. Dynamic handover: Throw\nand catch with bimanual hands. In 7th Annual\nConference on Robot Learning, 2023.\n[4] Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter\nAbbeel,\nand\nJitendra\nMalik.\nTwisting\nlids off with two hands.\narXiv preprint\narXiv:2403.02338, 2024.\n[5] Yuanpei Chen, Tianhao Wu, Shengjie Wang,\nXidong Feng, Jiechuan Jiang, Zongqing Lu,\nStephen McAleer, Hao Dong, Song-Chun Zhu,\nand Yaodong Yang. Towards human-level biman-\nual dexterous manipulation with reinforcement\nlearning. Advances in Neural Information Pro-\ncessing Systems, 35:5150â€“5163, 2022.\n[6] Kelvin Xu, Zheyuan Hu, Ria Doshi, Aaron Rovin-\nsky, Vikash Kumar, Abhishek Gupta, and Sergey\nLevine. Dexterous manipulation from images:\nAutonomous real-world rl via substep guidance.\nIn 2023 IEEE International Conference on Robotics\nand Automation (ICRA), pages 5938â€“5945. IEEE,\n2023.\n10\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\n[7] Yuanpei Chen, Chen Wang, Li Fei-Fei, and\nC Karen Liu. Sequential dexterity: Chaining\ndexterous policies for long-horizon manipula-\ntion. In Conference on Robot Learning, pages\n3809â€“3829, 2023.\n[8] Abhishek Gupta, Justin Yu, Tony Z Zhao, Vikash\nKumar, Aaron Rovinsky, Kelvin Xu, Thomas De-\nvlin, and Sergey Levine. Reset-free reinforce-\nment learning via multi-task learning: Learning\ndexterous manipulation behaviors without hu-\nman intervention. In 2021 IEEE International\nConference on Robotics and Automation (ICRA),\npages 6664â€“6671. IEEE, 2021.\n[9] Kevin Zakka, Laura Smith, Nimrod Gileadi,\nTaylor Howell, Xue Bin Peng, Sumeet Singh,\nYuval Tassa, Pete Florence, Andy Zeng, and\nPieter Abbeel. Robopianist: A benchmark for\nhigh-dimensional robot control. arXiv preprint\narXiv:2304.04150, 2023.\n[10] Sirui Chen, Jeannette Bohg, and C Karen Liu.\nSpringgrasp: An optimization pipeline for ro-\nbust and compliant dexterous pre-grasp synthe-\nsis. arXiv preprint arXiv:2404.13532, 2024.\n[11] Dylan Turpin, Tao Zhong, Shutong Zhang, Guan-\nglei Zhu, Eric Heiden, Miles Macklin, Stavros\nTsogkas, Sven Dickinson, and Animesh Garg.\nFast-graspâ€™d: Dexterous multi-finger grasp gen-\neration through differentiable simulation. In\n2023 IEEE International Conference on Robotics\nand Automation (ICRA), pages 8082â€“8089. IEEE,\n2023.\n[12] Dylan Turpin, Liquan Wang, Eric Heiden, Yun-\nChun Chen, Miles Macklin, Stavros Tsogkas,\nSven Dickinson, and Animesh Garg. Graspâ€™d:\nDifferentiable contact-rich grasp synthesis for\nmulti-fingered hands. In European Conference\non Computer Vision, pages 201â€“221. Springer,\n2022.\n[13] Ilge Akkaya, Marcin Andrychowicz, Maciek\nChociej, Mateusz Litwin, Bob McGrew, Arthur\nPetron, Alex Paino, Matthias Plappert, Glenn\nPowell, Raphael Ribas, et al.\nSolving ru-\nbikâ€™s cube with a robot hand. arXiv preprint\narXiv:1910.07113, 2019.\n[14] Max Yang, Chenghua Lu, Alex Church, Yijiong\nLin, Chris Ford, Haoran Li, Efi Psomopoulou,\nDavid AW Barton, and Nathan F Lepora. Any-\nrotate: Gravity-invariant in-hand object rota-\ntion with sim-to-real touch.\narXiv preprint\narXiv:2405.07391, 2024.\n[15] Johannes Pitz, Lennart RÃ¶stel, Leon Sievers, and\nBerthold BÃ¤uml. Dextrous tactile in-hand manip-\nulation using a modular reinforcement learning\narchitecture. In 2023 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages\n1852â€“1858. IEEE, 2023.\n[16] Ankur Handa, Arthur Allshire, Viktor Makoviy-\nchuk, Aleksei Petrenko, Ritvik Singh, Jingzhou\nLiu, Denys Makoviichuk, Karl Van Wyk, Alexan-\nder Zhurkevich, Balakumar Sundaralingam,\net al. Dextreme: Transfer of agile in-hand ma-\nnipulation from simulation to reality. In 2023\nIEEE International Conference on Robotics and\nAutomation (ICRA), pages 5977â€“5984, 2023.\n[17] Chen Wang, Haochen Shi, Weizhuo Wang, Ruo-\nhan Zhang, Li Fei-Fei, and C Karen Liu. Dexcap:\nScalable and portable mocap data collection sys-\ntem for dexterous manipulation. arXiv preprint\narXiv:2403.07788, 2024.\n[18] Zoey Qiuyu Chen, Karl Van Wyk, Yu-Wei Chao,\nWei Yang, Arsalan Mousavian, Abhishek Gupta,\nand Dieter Fox.\nDextransfer:\nReal world\nmulti-fingered dexterous grasping with mini-\nmal human demonstrations.\narXiv preprint\narXiv:2209.14284, 2022.\n[19] Aravind Rajeswaran, Vikash Kumar, Abhishek\nGupta, Giulia Vezzani, John Schulman, Emanuel\nTodorov, and Sergey Levine. Learning complex\ndexterous manipulation with deep reinforce-\nment learning and demonstrations. In Robotics:\nScience and Systems, 2017.\n[20] Maxime\nOquab,\nTimothÃ©e\nDarcet,\nThÃ©o\nMoutakanni, Huy Vo, Marc Szafraniec, Vasil\nKhalidov, Pierre Fernandez, Daniel Haziza,\nFrancisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without\nsupervision. Transactions on Machine Learning\nResearch, 2023.\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy,\nAditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In In-\nternational conference on machine learning, pages\n8748â€“8763. PMLR, 2021.\n[22] Aaron Hurst, Adam Lerer, Adam P Goucher,\nAdam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec\nRadford, et al.\nGpt-4o system card.\narXiv\npreprint arXiv:2410.21276, 2024.\n11\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\n[23] Alexander Kirillov, Eric Mintun, Nikhila Ravi,\nHanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C\nBerg, Wan-Yen Lo, et al. Segment anything. In\nProceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 4015â€“4026,\n2023.\n[24] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu,\nRonghang Hu, Chaitanya Ryali, Tengyu Ma,\nHaitham Khedr, Roman RÃ¤dle, Chloe Rolland,\nLaura Gustafson, et al. Sam 2: Segment any-\nthing in images and videos.\narXiv preprint\narXiv:2408.00714, 2024.\n[25] Wenlong Huang, Chen Wang, Ruohan Zhang,\nYunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipu-\nlation with language models. In Conference on\nRobot Learning, pages 540â€“562, 2023.\n[26] Wenlong Huang, Chen Wang, Yunzhu Li, Ruo-\nhan Zhang, and Li Fei-Fei.\nRekep: Spatio-\ntemporal reasoning of relational keypoint con-\nstraints for robotic manipulation. In 8th Annual\nConference on Robot Learning, 2024.\n[27] Moo Jin Kim, Karl Pertsch, Siddharth Karam-\ncheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair,\nRafael Rafailov, Ethan Foster, Grace Lam, Pan-\nnag Sanketi, et al. Openvla: An open-source\nvision-language-action model.\narXiv preprint\narXiv:2406.09246, 2024.\n[28] Kevin Black, Noah Brown, Danny Driess, Ad-\nnan Esmail, Michael Equi, Chelsea Finn, Nic-\ncolo Fusai, Lachy Groom, Karol Hausman, Brian\nIchter, et al. ğœ‹0: A Vision-Language-Action Flow\nModel for General Robot Control. arXiv preprint\narXiv:2410.24164, 2024.\n[29] Alexander Khazatsky, Karl Pertsch, Suraj Nair,\nAshwin Balakrishna, Sudeep Dasari, Siddharth\nKaramcheti, Soroush Nasiriany, Mohan Kumar\nSrirama, Lawrence Yunliang Chen, Kirsty Ellis,\net al.\nDroid: A large-scale in-the-wild robot\nmanipulation dataset. In Robotics: Science and\nSystems, 2024.\n[30] Abby Oâ€™Neill, Abdul Rehman, Abhinav Gupta,\nAbhiram Maddukuri,\nAbhishek Gupta,\nAb-\nhishek Padalkar, Abraham Lee, Acorn Pooley,\nAgrim Gupta, Ajay Mandlekar, et al. Open x-\nembodiment: Robotic learning datasets and rt-x\nmodels. arXiv preprint arXiv:2310.08864, 2023.\n[31] Jens Lundell, Francesco Verdoja, and Ville Kyrki.\nDdgc: Generative deep dexterous grasping in\nclutter. IEEE Robotics and Automation Letters, 6\n(4):6899â€“6906, 2021.\n[32] Andrew T Miller and Peter K Allen. Graspit! a\nversatile simulator for robotic grasping. IEEE\nRobotics & Automation Magazine, 11(4):110â€“\n122, 2004.\n[33] Tengyu Liu, Zeyu Liu, Ziyuan Jiao, Yixin Zhu,\nand Song-Chun Zhu. Synthesizing diverse and\nphysically stable grasps with arbitrary hand\nstructures using differentiable force closure esti-\nmator. IEEE Robotics and Automation Letters, 7\n(1):470â€“477, 2021.\n[34] Puhao Li, Tengyu Liu, Yuyang Li, Yiran Geng,\nYixin Zhu, Yaodong Yang, and Siyuan Huang.\nGendexgrasp: Generalizable dexterous grasping.\nIn 2023 IEEE International Conference on Robotics\nand Automation (ICRA), pages 8068â€“8074. IEEE,\n2023.\n[35] Ruicheng Wang, Jialiang Zhang, Jiayi Chen,\nYinzhen Xu, Puhao Li, Tengyu Liu, and He Wang.\nDexgraspnet: A large-scale robotic dexterous\ngrasp dataset for general objects based on simu-\nlation. In 2023 IEEE International Conference on\nRobotics and Automation (ICRA), pages 11359â€“\n11366. IEEE, 2023.\n[36] Jialiang Zhang, Haoran Liu, Danshi Li, XinQiang\nYu, Haoran Geng, Yufei Ding, Jiayi Chen, and\nHe Wang. Dexgraspnet 2.0: Learning generative\ndexterous grasping in large-scale synthetic clut-\ntered scenes. In 8th Annual Conference on Robot\nLearning, 2024.\n[37] Yiming Li, Wei Wei, Daheng Li, Peng Wang,\nWanyi Li, and Jun Zhong. Hgc-net: Deep an-\nthropomorphic hand grasping in clutter. In 2022\nInternational Conference on Robotics and Automa-\ntion (ICRA), pages 714â€“720. IEEE, 2022.\n[38] Min Liu, Zherong Pan, Kai Xu, Kanishka Gan-\nguly, and Dinesh Manocha. Deep differentiable\ngrasp planner for high-dof grippers. In Robotics:\nScience and Systems, 2020.\n[39] Jiaxin Lu, Hao Kang, Haoxiang Li, Bo Liu, Yiding\nYang, Qixing Huang, and Gang Hua. Ugg: Uni-\nfied generative grasping. In European Conference\non Computer Vision, pages 414â€“433. Springer,\n2025.\n[40] Tao Chen, Jie Xu, and Pulkit Agrawal. A sys-\ntem for general in-hand object re-orientation.\nConference on Robot Learning, 2021.\n[41] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin,\nQifeng Chen, and Xiaolong Wang. Rotating with-\nout seeing: Towards in-hand dexterity through\ntouch. In Robotics: Science and Systems, 2023.\n12\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\n[42] Haozhi Qi, Ashish Kumar, Roberto Calandra,\nYi Ma, and Jitendra Malik. In-hand object rota-\ntion via rapid motor adaptation. In Conference on\nRobot Learning, pages 1722â€“1732. PMLR, 2023.\n[43] Sudeep Dasari, Abhinav Gupta, and Vikash Ku-\nmar. Learning dexterous manipulation from ex-\nemplar object trajectories and pre-grasps. In\n2023 IEEE International Conference on Robotics\nand Automation (ICRA), pages 3889â€“3896. IEEE,\n2023.\n[44] Gagan Khandate, Siqi Shang, Eric T Chang, Tris-\ntan Luca Saidi, Yang Liu, Seth Matthew Dennis,\nJohnson Adams, and Matei Ciocarlie. Sampling-\nbased exploration for reinforcement learning of\ndexterous manipulation. In Robotics: Science and\nSystems, 2023.\n[45] Yuanpei Chen, Chen Wang, Yaodong Yang, and\nC Karen Liu. Object-centric dexterous manipu-\nlation from human motion data. In 8th Annual\nConference on Robot Learning, 2024.\n[46] Weikang Wan, Haoran Geng, Yun Liu, Zikang\nShan, Yaodong Yang, Li Yi, and He Wang.\nUnidexgrasp++: Improving dexterous grasping\npolicy learning via geometry-aware curriculum\nand iterative generalist-specialist learning. In\nProceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 3891â€“3902,\n2023.\n[47] Hui Zhang, Sammy Christen, Zicong Fan, Ot-\nmar Hilliges, and Jie Song. Graspxl: Generating\ngrasping motions for diverse objects at scale. In\nEuropean Conference on Computer Vision, pages\n386â€“403. Springer, 2024.\n[48] Zhecheng Yuan, Tianming Wei, Shuiqi Cheng,\nGu Zhang, Yuanpei Chen, and Huazhe Xu. Learn-\ning to manipulate anywhere: A visual gener-\nalizable framework for reinforcement learning.\nCoRR, abs/2407.15815, 2024. doi: 10.48550/\nARXIV.2407.15815. URL https://doi.org/\n10.48550/arXiv.2407.15815.\n[49] Priyanka\nMandikal\nand\nKristen\nGrauman.\nDexvip: Learning dexterous grasping with hu-\nman hand pose priors from video. In Conference\non Robot Learning, pages 651â€“661. PMLR, 2022.\n[50] Priyanka Mandikal and Kristen Grauman. Learn-\ning dexterous grasping with object-centric visual\naffordances. In 2021 IEEE international confer-\nence on robotics and automation (ICRA), pages\n6169â€“6176. IEEE, 2021.\n[51] Tyler Ga Wei Lum, Martin Matak, Viktor\nMakoviychuk, Ankur Handa, Arthur Allshire,\nTucker Hermans, Nathan D Ratliff, and Karl\nVan Wyk. Dextrah-g: Pixels-to-action dexterous\narm-hand grasping with geometric fabrics. In\n8th Annual Conference on Robot Learning, 2024.\n[52] Ritvik Singh, Arthur Allshire, Ankur Handa,\nNathan Ratliff, and Karl Van Wyk. Dextrah-rgb:\nVisuomotor policies to grasp anything with dex-\nterous hands. arXiv preprint arXiv:2412.01791,\n2024.\n[53] Zoey Qiuyu Chen, Karl Van Wyk, Yu-Wei Chao,\nWei Yang, Arsalan Mousavian, Abhishek Gupta,\nand Dieter Fox. Learning robust real-world dex-\nterous grasping policies via implicit shape aug-\nmentation.\nIn Conference on Robot Learning,\npages 1222â€“1232, 2022.\n[54] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak.\nVideodex:\nLearning dexterity from internet\nvideos. In Conference on Robot Learning, pages\n654â€“665. PMLR, 2023.\n[55] Ilija Radosavovic, Xiaolong Wang, Lerrel Pinto,\nand Jitendra Malik. State-only imitation learn-\ning for dexterous manipulation.\nIn 2021\nIEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 7865â€“7871.\nIEEE, 2021.\n[56] Sridhar Pandian Arunachalam, Irmak GÃ¼zey,\nSoumith Chintala, and Lerrel Pinto. Holo-dex:\nTeaching dexterity with immersive mixed reality.\nIn 2023 IEEE International Conference on Robotics\nand Automation (ICRA), pages 5962â€“5969. IEEE,\n2023.\n[57] Irmak Guzey, Ben Evans, Soumith Chintala,\nand Lerrel Pinto. Dexterity from touch: Self-\nsupervised pre-training of tactile representations\nwith robotic play. In 7th Annual Conference on\nRobot Learning, 2023.\n[58] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky\nLiang, Yu-Wei Chao, Qian Wan, Stan Birchfield,\nNathan Ratliff, and Dieter Fox. Dexpilot: Vision-\nbased teleoperation of dexterous robotic hand-\narm system. In 2020 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages\n9164â€“9170. IEEE, 2020.\n[59] Aravind Sivakumar, Kenneth Shaw, and Deepak\nPathak. Robotic telekinesis: Learning a robotic\nhand imitator by watching humans on youtube.\nIn Robotics: Science and Systems, 2022.\n13\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\n[60] Yuzhe Qin, Wei Yang, Binghao Huang, Karl\nVan Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao,\nand Dietor Fox. Anyteleop: A general vision-\nbased dexterous robot arm-hand teleoperation\nsystem. arXiv preprint arXiv:2307.04577, 2023.\n[61] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Han-\nwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong\nWang. Dexmv: Imitation learning for dexterous\nmanipulation from human videos. In European\nConference on Computer Vision, pages 570â€“587.\nSpringer, 2022.\n[62] Zichen Jeff Cui, Yibin Wang, Nur Muham-\nmad Mahi Shafiullah, and Lerrel Pinto. From\nplay to policy: Conditional behavior generation\nfrom uncurated robot data. In The International\nConference on Learning Representations, 2023.\n[63] Siddhant Haldar, Jyothish Pari, Anant Rai, and\nLerrel Pinto. Teach a robot to fish: Versatile\nimitation from one minute of demonstrations.\narXiv preprint arXiv:2303.01497, 2023.\n[64] Yuzhe Qin, Hao Su, and Xiaolong Wang. From\none hand to multiple hands: Imitation learning\nfor dexterous manipulation from single-camera\nteleoperation.\nIEEE Robotics and Automation\nLetters, 7(4):10873â€“10881, 2022.\n[65] Sridhar Pandian Arunachalam, Sneha Silwal,\nBen Evans, and Lerrel Pinto.\nDexterous imi-\ntation made easy: A learning-based framework\nfor efficient dexterous manipulation. In 2023\nieee international conference on robotics and au-\ntomation (icra), pages 5954â€“5961, 2023.\n[66] Irmak Guzey, Yinlong Dai, Ben Evans, Soumith\nChintala, and Lerrel Pinto. See to touch: Learn-\ning tactile dexterity through visual incentives. In\n2024 IEEE International Conference on Robotics\nand Automation (ICRA), pages 13825â€“13832.\nIEEE, 2024.\n[67] Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent\nYi, Sergey Levine, and Jitendra Malik. Learning\nvisuotactile skills with two multifingered hands.\narXiv preprint arXiv:2404.16823, 2024.\n[68] Qianxu Wang, Haotong Zhang, Congyue Deng,\nYang You, Hao Dong, Yixin Zhu, and Leonidas\nGuibas. Sparsedff: Sparse-view feature distil-\nlation for one-shot dexterous manipulation. In\nThe Twelfth International Conference on Learning\nRepresentations, 2023.\n[69] Qianxu Wang, Congyue Deng, Tyler Ga Wei Lum,\nYuanpei Chen, Yaodong Yang, Jeannette Bohg,\nYixin Zhu, and Leonidas Guibas. Neural atten-\ntion field: Emerging point relevance in 3d scenes\nfor one-shot dexterous grasping. In 8th Annual\nConference on Robot Learning, 2024.\n[70] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang\nXu, Jiashi Feng, and Hengshuang Zhao. Depth\nanything: Unleashing the power of large-scale\nunlabeled data. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 10371â€“10381, 2024.\n[71] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen\nZhao, Xiaogang Xu, Jiashi Feng, and Heng-\nshuang Zhao. Depth anything v2. Advances\nin Neural Information Processing Systems, 37:\n21875â€“21911, 2025.\n[72] Qwen Team.\nQwen2.5-vl, January 2025.\nURL https://qwenlm.github.io/blog/\nqwen2.5-vl/.\n[73] Anthony Brohan, Noah Brown, Justice Carbajal,\nYevgen Chebotar, Xi Chen, Krzysztof Choroman-\nski, Tianli Ding, Danny Driess, Avinava Dubey,\nChelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic con-\ntrol. arXiv preprint arXiv:2307.15818, 2023.\n[74] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre\nSermanet, Quon Vuong, Jonathan Tompson, Yev-\ngen Chebotar, Debidatta Dwibedi, and Dorsa\nSadigh. Rt-h: Action hierarchies using language.\narXiv preprint arXiv:2403.01823, 2024.\n[75] Jiayuan Gu, Sean Kirmani, Paul Wohlhart,\nYao Lu, Montserrat Gonzalez Arenas, Kan-\nishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana\nGopalakrishnan, Zhuo Xu, et al. Rt-trajectory:\nRobotic task generalization via hindsight trajec-\ntory sketches. arXiv preprint arXiv:2311.01977,\n2023.\n[76] Songming Liu, Lingxuan Wu, Bangguo Li,\nHengkai Tan, Huayu Chen, Zhengyi Wang,\nKe Xu, Hang Su, and Jun Zhu. Rdt-1b: a diffu-\nsion foundation model for bimanual manipula-\ntion. arXiv preprint arXiv:2410.07864, 2024.\n[77] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao\nKong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao\nWu, Jiafeng Xu, Yichu Yang, et al. Gr-2: A gen-\nerative video-language-action model with web-\nscale knowledge for robot manipulation. arXiv\npreprint arXiv:2410.06158, 2024.\n[78] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie\nWang, Sinan Tan, Peng Wang, Junyang Lin,\nChang Zhou, and Jingren Zhou. Qwen-vl: A\nversatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv\npreprint arXiv:2308.12966, 1(2):3, 2023.\n14\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\n[79] Ho Kei Cheng, Seoung Wug Oh, Brian Price,\nJoon-Young\nLee,\nand\nAlexander\nSchwing.\nPutting the object back into video object segmen-\ntation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 3151â€“3161, 2024.\n[80] William Peebles and Saining Xie. Scalable diffu-\nsion models with transformers. In Proceedings of\nthe IEEE/CVF International Conference on Com-\nputer Vision, pages 4195â€“4205, 2023.\n[81] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric\nCousineau, Yilun Du, Benjamin Burchfiel, Russ\nTedrake, and Shuran Song. Diffusion policy:\nVisuomotor policy learning via action diffusion.\nThe International Journal of Robotics Research,\n2023.\n[82] Andreas Peter Steiner, Alexander Kolesnikov, Xi-\naohua Zhai, Ross Wightman, Jakob Uszkoreit,\nand Lucas Beyer. How to train your vit? data,\naugmentation, and regularization in vision trans-\nformers. Transactions on Machine Learning Re-\nsearch, 2022.\n[83] Yiheng\nLi,\nHeyang\nJiang,\nAkio\nKodaira,\nMasayoshi Tomizuka, Kurt Keutzer, and Chen-\nfeng Xu. Immiscible diffusion: Accelerating dif-\nfusion training with noise assignment. arXiv\npreprint arXiv:2406.12303, 2024.\n[84] Jiaming Song, Chenlin Meng, and Stefano Er-\nmon. Denoising diffusion implicit models. In\nInternational Conference on Learning Representa-\ntions, 2020.\nA. Implementation Details\nIn this section, we present the details of DexGraspVLA\nimplementation (Appendix A.1), baseline implemen-\ntation (Appendix A.2), and dataset collection (Ap-\npendix A.3).\nA.1. Details of DexGraspVLA Implementation\nPlanner. The high-level planner operates as described\nin Section 4.1. To obtain the bounding box for the\nlow-level controller using Qwen-VL-Chat, we begin\nby cropping a fixed region of the head-camera image\nthat corresponds to the tabletop workspace. We then\nfeed this cropped area into the VLM, which identifies\nthe target objectâ€™s bounding box within it. Finally, the\ncoordinates are mapped back to the original image to\ndetermine the bounding box used by the controller.\nSeveral labeling results are shown in Figure 10.\nBy leveraging an off-the-shelf VLM as the planner,\nour framework gains remarkable flexibility, enabling\neasy replacement of the original VLM with more ad-\nvanced models for enhanced performance. Our obser-\nvations indicate that Qwen2.5-VL-72B-Instruct [72]\noutperforms Qwen-VL-Chat (used in our experiments,\nSection 5) in reasoning and instruction following,\nleading to improved long-horizon task completion.\nFurthermore, with the 72B model, the planner can\nachieve near-perfect bounding box prediction accu-\nracy without image cropping. Therefore, we provide\nthe prompts for the DexGraspVLA planner based on\nQwen2.5-VL-72B-Instruct below.\nThese prompts mainly instruct the VLM to function\nas DexGraspVLA planner, including understanding the\nuser prompt, proposing an object as the current grasp-\ning instruction, marking the target object bounding\nbox, checking if the grasp has succeeded, assessing\nwhether the current instruction is completed, and eval-\nuating whether the entire user prompt is fully fulfilled.\nSpecifically, when a user prompt ğ‘is provided, the\nplanner classifies its type based on whether ğ‘explicitly\nspecifies target objects with the following prompt.\nAnalyze the following user prompt: <user\nprompt>\nUser prompt types:\n- Type I (return True): User prompts with any\nspecific descriptions\nExamples:\n* Color-based: \"green objects\"\n* Position-based: \"objects from the right\"\n* Property-based: \"all cups\"\n* Combination: \"the red cup on the left\"\n- Type II (return False): Abstract prompts\nwithout any object descriptions\nExamples:\n\"clear the table\",\n\"clean up\",\n\"remove everything\"\nPlease determine:\n- Is this a Type I prompt? (True/False)\n- Provide your reasoning\nReturn format:\nTrue/False: your reasoning\nExamples:\n- \"grab the green cup\" -> True: Contains spe-\ncific object (cup) and property (green)\n- \"clear the table\" -> False: No specific object\ncharacteristics mentioned\nIf ğ‘is classified as â€œType Iâ€, meaning it explicitly\n15\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nspecifies the objects to grasp, the planner generates\nan ordered list of grasping instructions, using the fol-\nlowing prompt and head-camera image.\nFor user prompt: <user prompt>\nProcess:\n1.\nAnalyze the user prompt and image to-\ngether:\n- Match user prompt descriptions with visible\nobjects in the image\n- If a description (e.g., \"green objects\") matches\nmultiple objects, include all matching objects\n- Verify each mentioned object actually exists\nin the image\n2.\nBased on the robot armâ€™s position\n(right edge of the screen) and table layout\n3. Determine the most efficient grasping se-\nquence\n4. Generate a reordered list of objects to grasp\nRequirements:\n- Only include objects mentioned in the original\nuser prompt\n- Keep position information for each object\n- Return as a list, ordered by grasping sequence\nExpected output format:\n[\"object with position 1\", \"object with position\n2\", ...]\nAlternatively, if ğ‘is classified as â€œType IIâ€, indicating\nthat the user intends to grasp all objects on the table,\nthe planner selects the optimal target object as the\ncurrent instruction. This selection is based on the\nremaining objects on the table, using the following\nprompt and head-camera image.\nAnalyze the current desktop layout and select\nthe most suitable object to grasp, considering\nthe following factors:\nGrasping Strategy:\n1. The robotic arm is positioned on the far right\n(outside the frame)\n2. Grasping Priority Order:\n- Prioritize objects on the right to avoid knock-\ning over other objects during later operations\n- Then consider objects in the middle\n- Finally, consider objects on the left\n3. Accessibility Analysis:\n- Relative positions between objects\n- Potential obstacles\n- Whether the grasping path might interfere\nwith other objects\nPlease provide your response in the fol-\nlowing JSON format:\n{\n\"analysis\": {\n\"priority_consideration\": \"\nexplanation of why this\nobject has priority\",\n\"accessibility\": \"analysis\nof objectâ€™s accessibility\n\",\n\"risk_assessment\": \"\npotential risks in\ngrasping this object\"\n},\n\"target\": \"a comprehensive\ndescription of the target\nobject (e.g., â€™the blue\ncube on the far right of\nthe desktop, next to the\nred cylinderâ€™)\"\n}\nEnsure the output is in valid JSON format.\nNote: The â€™targetâ€™ field should ONLY contain\nthe objectâ€™s color, shape, and position in a nat-\nural, flowing sentence. Do not include any\nanalysis or reasoning in this field.\nFor each grasping instruction ğ‘™, the planner marks\nthe bounding box of the target object using the fol-\nlowing prompt and head-camera image.\nAnalyze the image and identify the best match-\ning object with the description: <target ob-\nject>.\nInstructions for object analysis:\n1. Select ONE object that best matches the\ndescription\n2. For the selected object, provide:\n- A concise label, object name (3-4 words max)\n- A detailed description (position, color, shape,\ncontext)\n- Accurate bbox coordinates\nRequired JSON format with an example:\n{\n\"bbox_2d\": [x1, y1, x2, y2],\n\"label\": \"green cup\",\n# Keep\nthis very brief (3-4 words)\n\"description\": \"a cylindrical\ngreen ceramic cup located\n16\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\non the right side of the\nwooden table, next to the\nlaptop\"\n# Detailed\ndescription\n}\nCritical requirements:\n- Return EXACTLY ONE object\n- \"label\": Must be brief (3-4 words) for quick\nreference\n- \"description\": Must be detailed and include\nspatial context\n- Use single JSON object format, not an array\n- Ensure bbox coordinates are within image\nboundaries\nDuring the controllerâ€™s execution, the planner veri-\nfies whether the object has been successfully grasped,\nusing the following prompt and head-camera image.\nAnalyze the image and determine if the robotic\narm has successfully grasped an object:\n1. Observe the spatial relationship between\nthe robotic hand and the object\n2. Output format: explain your reasoning, then\nconclude with a boolean value (True=grasped,\nFalse=not grasped)\nWhen a grasping attempt ends, the robot resets\nto its initial state, and the planner checks whether\nthe current instruction has been completed, using the\nfollowing prompt and head-camera image.\nPlease check whether <target object> exists\non the desktop. If it does not exist, output\nTrue; otherwise, output False.\nFor a â€œType Iâ€ user prompt ğ‘, the planner consid-\ners it fulfilled if all specified objects have been suc-\ncessfully grasped. For a â€œType IIâ€ user prompt ğ‘, the\nplanner checks whether the prompt has been fully\ncompleted after each grasping attempt, using the fol-\nlowing prompt and head-camera image.\nPlease analyze the table in the image:\nRequirements:\n- Only detect physical objects with noticeable\nheight/thickness (3D objects)\n- Exclude from consideration:\n* Flat items (papers, tablecloths, mats)\n* Light projections\n* Shadows\n* Surface patterns or textures\nReturn format:\n- True: if the table is empty of 3D objects\n- False: if there are any 3D objects, followed\nby their names\nExample responses:\nTrue (for empty table)\nFalse: cup, bottle, plate (for table with objects)\nController. All raw images are produced by head and\nwrist cameras at a resolution of 640 Ã— 480 Ã— 3. Cor-\nrespondingly, the resolution of mask is 640 Ã— 480 Ã— 1.\nThrough preliminary model selection, we decide to\nuse DINOv2 ViT-B/14 as the feature extractor ğœ™h for\nhead camera images and DINOv2 ViT-L/14 as the\nfeature extractor ğœ™w for wrist camera images. Be-\nfore feeding images into DINOv2, we resize them to\n518 Ã— 518 Ã— 3. During training, we apply domain ran-\ndomization via color jittering. Finally, the images are\nnormalized and fed into DINOv2 models. This leads\nto features zh\nğ‘¡âˆˆâ„1369Ã—768 and zw\nğ‘¡\nâˆˆâ„1369Ã—1024. By\nprocessing the mask mğ‘¡with a randomly initialized\nViT, we extract its features zm\nğ‘¡âˆˆâ„1369Ã—768. Patch-wise\nconcatenation of zh\nğ‘¡and zm\nğ‘¡leads to Â¯zh\nğ‘¡âˆˆâ„1369Ã—1536.\nWe then project Â¯zh\nğ‘¡, zw\nğ‘¡, sğ‘¡to the same feature space\nof dimension 1024 with separate MLPs, yielding Ëœzh\nğ‘¡âˆˆ\nâ„1369Ã—1024, Ëœzw\nğ‘¡\nâˆˆâ„1369Ã—1024, Ëœzs\nğ‘¡âˆˆâ„1Ã—1024, and con-\ncatenate them to form the full observation feature\nsequence Ëœzobs\nğ‘¡\n= (Ëœzh\nğ‘¡, Ëœzw\nğ‘¡, Ëœzs\nğ‘¡) âˆˆâ„2739Ã—1024.\nFor action modeling, we define an action chunk\nhorizon of ğ»= 64. When we add noise to the action\nduring training, we employ Immiscible Diffusion [83]\nto improve data-noise mapping. The noised action\nchunk Ë†Ağ‘¡belongs to â„64Ã—13.\nThe DiT implementation is based on the original\nDiT paper [80], diffusion policy [81], and RDT [76].\nIt first embeds the diffusion timestep to the same\nhidden space as Ëœzobs\nğ‘¡\n, yielding Ëœzd\nğ‘¡\nâˆˆâ„1Ã—1024, and\nconcatenates it with Ëœzobs\nğ‘¡\nto form the condition se-\nquence Ëœzğ‘¡= (Ëœzobs\nğ‘¡\n, Ëœzd\nğ‘¡) âˆˆâ„2740Ã—1024. We project the\nnoised action chunk to the same hidden space, deriv-\ning ËœzA\nğ‘¡âˆˆâ„64Ã—1024, and feed it into DiT. Each DiT layer\nperforms bi-directional attention within action tokens,\ncross-attention to the condition sequence, and MLP\nprojections. Finally, the output is projected back to the\naction space to be the modelâ€™s prediction of noise. Dur-\ning training, we compute MSE loss between the noise\nprediction and ground truth, and back-propagate the\ngradient to update all trainable parameters. During\ninference, we start from Gaussian noise and iteratively\ndenoise it using DDIM sampling [84]. At each step,\nthe DiT model predicts the noise given the condition\n17\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nHyper-parameter\nValue\nepoch\n84\nlearning rate\n0.0001\nlearning rate scheduler\ncosine\nlearning rate warmup steps\n2000\nweight decay\n0.0001\nAdamW betas\n[0.95, 0.999]\nseed\n42\nbatch size per GPU\n48\naction horizon\n64\nnumber of DiT layers\n12\nnumber of DiT head\n8\nattention dropout\n0.1\nnoise scheduler\nDDIMScheduler\nnum_train_timesteps\n50\nbeta_start\n0.0001\nbeta_end\n0.02\nbeta_schedule\nsquaredcos_cap_v2\nclip_sample\nTrue\nset_alpha_to_one\nTrue\nsteps_offset\n0\nprediction_type\nepsilon\nnum_inference_steps\n16\nTable 3 | Hyper-parameters of DexGraspVLA.\nsequence, and we update the action chunk using the\nDDIM scheduler until we obtain the final action. The\ncontroller only executes the first six actions in the pre-\ndicted action chunk before making a new prediction.\nIn total, the controller possesses 163M trainable pa-\nrameters. To accelerate training, we utilize bfloat16\nmixed-precision training, reducing memory usage and\nimproving computational efficiency. Additionally, we\nemploy FusedAdamW as the optimizer to further\nspeed up training through optimized memory access\nand fused kernel execution. With these techniques,\nwe train the controller for 84 epochs over our dataset\non an 8-A800 GPU server, which takes less than one\nday to complete. All hyper-parameters in our imple-\nmentation are presented in Appendix A.1.\nA.2. Details of Baseline Implementation\nThe baseline DexGraspVLA (DINOv2-train) is the same\nas DexGraspVLA (Ours) described in Appendix A.1 ex-\ncept that the two DINOv2 models are trainable instead\nof frozen. The baseline DexGraspVLA (ViT-small) is\nthe same as DexGraspVLA (Ours) except that the two\nDINOv2 models are replaced with two small trainable\npretrained ViTs (the R26-S-32 ResNet-ViT hybrid from\nSteiner et al. [82]). Correspondingly, we resize the\nimages to 224 Ã— 224 Ã— 3 to feed them into ViT-small.\nEach image is split into 49 patches, and the feature\ndimension is 384.\n(a) Our data collection site.\n(b) The test environment where all experiments are\nconducted.\nFigure 7 | A comparison of the data collection and test\nenvironments, which are located in different rooms.\nNotably, the scenes captured by the robotâ€™s cameras\nvary significantly, especially for the wrist camera.\nA.3. Details of Data Collection\nWe collect demonstrations through kinesthetic teach-\ning. At the beginning, the robot is set to teaching\nmode, allowing manual guidance to grasp target ob-\njects. The operator then physically guides the robot to\nthe target position and performs the grasping motion.\nThe entire process follows a fixed duration (75 joint\nangle values recorded at 20Hz). Subsequently, we re-\nset the environment and execute PD control using the\nrecorded joint angles as target. At the same frequency,\nthese target joint angles serve as actions, while images\nand current joint angles are collected as states. Fol-\nlowing the same approach as the low-level controller,\nwe post-process the collected data to generate masks,\ncompleting one demonstration sequence.\nB. Experiment Details\nB.1. The â€œZero-Shotâ€ Evaluation Environment\nFigure 7 contrasts our data collection site and the test\nsite, which are located in separate rooms. We gather\nall 2,094 human demonstrations at the data collection\nsite (Figure 7a), whereas the experiments in Section 5\n18\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nare conducted at the test site (Figure 7b). Because\nthese sites differ in layout and background, both the\nhead camera and the wrist camera encounter scenes\nnot present in the training data during evaluation â€”\nparticularly the wrist camera, which observes a no-\ntably altered environment, capturing a variety of front\nand peripheral views during operation. Despite these\nenvironmental discrepancies, we do not collect any\ndata from the test site to fine-tune the models. Instead,\nthe models are deployed and evaluated directly, re-\nsulting in a genuinely â€œzero-shotâ€ testing environment.\nEven under these conditions, DexGraspVLA achieves\nan over 90% success rate in grasping tasks in cluttered\nscenes across thousands of unseen object, lighting,\nand background combinations, clearly demonstrating\nits strong generalization capability.\nB.2. Additional Details of Objects, Lightings,\nand Backgrounds\nWe collect a total of 360 unseen objects, from which\n103 items are randomly selected to be the object subset\nS. In the main paper, the Unseen Objects experiment\nis conducted on all 360 objects, whereas the Unseen\nLightings and Unseen Backgrounds experiments use\nonly the objects in S. The three unseen lighting condi-\ntions comprise disco light, lamp light, and dark light.\nMeanwhile, the six unseen backgrounds include a\nblack mouse pad, a pink towel, a colorful tablecloth,\na black-and-white mouse pad, a wooden board, and a\ncalibration board. These conditions are illustrated in\nFigure 8.\nB.3. Details of Visualization\nIn this part, we explain how we visualize the internal\nmodel behavior shown in Figure 6. Due to space con-\nstraints, Figure 6 only presents the relevant portion\nof images containing the tabletop workspace. The full\nversion is shown in Figure 9. The first row is raw im-\nages from the head camera resized to 518 Ã— 518 Ã— 3.\nThe second row illustrates the DINOv2 ViT-B/14 fea-\ntures following the practice introduced in DINOv2\npaper [20]. To make the resulting feature map rec-\nognizable for visualization purpose, we enlarge both\nthe height and weight of images by a factor of six\nbefore feeding them into DINOv2. After obtaining\nthe feature sequences for all four images, we combine\nthese features, perform a PCA between all patches,\nand set a threshold to remove background regions.\nWe then apply PCA again, this time to the remain-\ning foreground features, map the top three principal\ncomponents to the RGB channels, and normalize the\nresult. This yields the visualization shown in the sec-\nond row. The third row showcases the binary masks\nmğ‘¡âˆˆâ„518Ã—518Ã—1 tracked by Cutie. The fourth row\ndisplays the averaged DiT attention maps over the\nhead image features. This is computed by summing\nattention weights to each head image patch across\nall diffusion steps, DiT layers, DiT heads, and action\ntokens, and normalize the sum to one. The shape of\nthe averaged attention map is 37 Ã— 37 Ã— 1. Finally, we\nupsample the attention map to 518Ã—518Ã—1, multiply\nit by 2 to increase brightness, and use it to scale the\nvalue channel of head images in HSV space, resulting\nin the visualization shown in the fifth row.\nC. Additional Results\nThis section provides additional results for the exper-\niments in the main paper. In Table 4, we report the\ndetailed success rates for our large-scale generaliza-\ntion evaluation under each environment condition,\ncorresponding to Table 1 in Section 5.2. From the first\nrow (â€œOurs@1â€), it is evident that DexGraspVLA main-\ntains consistently high success rates across various un-\nseen object, lighting, and background combinations.\nMany observed failures stem from randomness in pol-\nicy inference; allowing additional attempts often re-\ncovers these failed cases. Accordingly, the second and\nthird rows (â€œOurs@2â€ and â€œOurs@3â€) show further\nimprovements in performance, highlighting the po-\ntential for DexGraspVLA to reach even higher success\nrates. In Figure 10, we present examples of bounding-\nbox predictions produced by the DexGraspVLA plan-\nner. Despite substantial variation in environmental\nconditions, the planner consistently grounds grasping\ninstructions in cluttered scenes and provides the cor-\nrect bounding boxes. Notably, we can label objects by\nnames such as â€œCoca Colaâ€ or â€œmilk,â€ reflecting the sys-\ntemâ€™s extensive common sense and world knowledge.\nBy drawing on the broad knowledge embedded in\neach of its foundation models, DexGraspVLA achieves\nrobust generalization across diverse scenarios.\n19\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nUnseen\nObjects\nUnseen\nLightings\nUnseen\nBackgrounds\nUnseen\nBackgrounds\nWhite table, \nwhite light\nWhite table, \ndisco light\nWhite table,\nlamp light \nWhite table,\ndark light \nBlack mouse pad,\nwhite light \nPink towel, \nwhite light\nColorful tablecloth, \nwhite light\nBlack-white \nmouse pad, \nwhite light\nWooden board, \nwhite light\nCalibration board, \nwhite light\nFigure 8 | Environment conditions used in our large-scale generalization evaluation (Section 5.2).\nTasks\nUnseen\nObjects\n(360)\nUnseen Lightings\n(3 Ã— 103)\nUnseen Backgrounds\n(6 Ã— 103)\nAggregated\n(1287)\nLighting\nConditions\nWhite\nLight\nDisco\nLight\nLamp\nLight\nDark\nLight\nWhite\nLight\nWhite\nLight\nWhite\nLight\nWhite\nLight\nWhite\nLight\nWhite\nLight\nBackground\nConditions\nWhite\nTable\nWhite\nTable\nWhite\nTable\nWhite\nTable\nBlack\nMouse Pad\nPink\nTowel\nColorful\nTablecloth\nBlack-White\nMouse Pad\nWooden\nBoard\nCalibration\nBoard\nOurs@1\n91.1%\n92.2%\n89.3%\n91.2%\n94.2%\n84.5%\n90.3%\n92.2%\n93.2%\n88.3%\n90.8%\nOurs@2\n95.3%\n97.0%\n95.1%\n93.2%\n97.1%\n90.3%\n91.3%\n95.1%\n98.1%\n93.2%\n94.7%\nOurs@3\n96.7%\n98.1%\n98.1%\n96.1%\n98.1%\n91.3%\n94.2%\n98.1%\n100.0%\n98.1%\n96.9%\nTable 4 | The detailed performance of DexGraspVLA under different unseen conditions, which indicates that\nour approach consistently achieves high success rates across various objects, lightings, and backgrounds. The\nsecond and third rows highlight its potential to reach even higher success rates given more chances.\n20\n\n\nDexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping\nWhite\nMosaic\nTablecloth\nTablecloth\n& Disco Light\nRaw \nImage\nDINOv2 \nFeature\nBinary\nMask\nAttention \nMap\nAttention \nVisualized\nFigure 9 | The complete, uncropped version of Figure 6.\nFigure 10 | Bounding-box predictions made by DexGraspVLA planner based on Qwen-VL-Chat. Across diverse\nlighting and background conditions, it accurately grounds the language instruction to the target object in\ncluttered scenes and marks the correct bounding box.\n21\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20900v1.pdf",
    "total_pages": 21,
    "title": "DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping",
    "authors": [
      "Yifan Zhong",
      "Xuchuan Huang",
      "Ruochong Li",
      "Ceyao Zhang",
      "Yitao Liang",
      "Yaodong Yang",
      "Yuanpei Chen"
    ],
    "abstract": "Dexterous grasping remains a fundamental yet challenging problem in robotics.\nA general-purpose robot must be capable of grasping diverse objects in\narbitrary scenarios. However, existing research typically relies on specific\nassumptions, such as single-object settings or limited environments, leading to\nconstrained generalization. Our solution is DexGraspVLA, a hierarchical\nframework that utilizes a pre-trained Vision-Language model as the high-level\ntask planner and learns a diffusion-based policy as the low-level Action\ncontroller. The key insight lies in iteratively transforming diverse language\nand visual inputs into domain-invariant representations, where imitation\nlearning can be effectively applied due to the alleviation of domain shift.\nThus, it enables robust generalization across a wide range of real-world\nscenarios. Notably, our method achieves a 90+% success rate under thousands of\nunseen object, lighting, and background combinations in a ``zero-shot''\nenvironment. Empirical analysis further confirms the consistency of internal\nmodel behavior across environmental variations, thereby validating our design\nand explaining its generalization performance. We hope our work can be a step\nforward in achieving general dexterous grasping. Our demo and code can be found\nat https://dexgraspvla.github.io/.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}