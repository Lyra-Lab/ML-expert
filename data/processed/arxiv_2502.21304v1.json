{
  "id": "arxiv_2502.21304v1",
  "text": "Clustering Context in Off-Policy Evaluation\nDaniel Guzm´an-Olivares\nPhilipp Schmidt\nJacek Golebiowski\nArtur Bekasov\nBulil Technologies, UAM*\ndaniel.guzman@buliltec.com\nAmazon\nphschmid@amazon.com\ndistil labs*\ngolebiowski.j@gmail.com\nAmazon\nabksv@amazon.com\nAbstract\nOff-policy evaluation can leverage logged data to\nestimate the effectiveness of new policies in e-\ncommerce, search engines, media streaming ser-\nvices, or automatic diagnostic tools in healthcare.\nHowever, the performance of baseline off-policy\nestimators like IPS deteriorates when the logging\npolicy significantly differs from the evaluation\npolicy. Recent work proposes sharing informa-\ntion across similar actions to mitigate this prob-\nlem. In this work, we propose an alternative esti-\nmator that shares information across similar con-\ntexts using clustering. We study the theoretical\nproperties of the proposed estimator, character-\nizing its bias and variance under different con-\nditions.\nWe also compare the performance of\nthe proposed estimator and existing approaches\nin various synthetic problems, as well as a real-\nworld recommendation dataset. Our experimen-\ntal results confirm that clustering contexts im-\nproves estimation accuracy, especially in defi-\ncient information settings.1\n1\nINTRODUCTION\nThe contextual bandit process models many real-world\nproblems across industry and research, including health-\ncare, finance, and recommendation systems (Bouneffouf,\nRish, and Aggarwal, 2020). In this setting, an agent ob-\nserves a context, chooses an action according to a pol-\nicy, and observes a reward. Off-policy evaluation (OPE)\n*Work done while at Amazon.\n1The code for reproducing our experimental implementation\nis available at https://github.com/amazon-science/ope-cluster-\ncontext\nProceedings of the 28th International Conference on Artificial In-\ntelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand.\nPMLR: Volume 258. Copyright 2025 by the author(s).\nmethods aim to estimate the effectiveness of a policy with-\nout empirically testing it, which can be particularly useful\nwhen A/B tests are costly, or if there is an inherent risk as-\nsociated with poor policy performance, as is often the case\nin healthcare (Bastani and Bayati, 2019). Existing OPE\nmethods can be broadly divided into parametric methods\nbased on the direct method (DM), non-parametric methods\nbased on inverse propensity score weighting (IPS, Horvitz\nand Thompson, 1952), and a combination of the two, such\nas the doubly robust method (DR, Dud´ık, Langford, and Li,\n2011). When every action with non-zero probability under\nthe evaluation policy also has a non-zero probability un-\nder the logging policy, IPS is unbiased. This condition is\nrarely satisfied in real-world problems, however, so IPS is\ntypically biased in practice, especially for actions that vio-\nlate the condition, or have close-to-zero probabilities in the\nlogging policy (Sachdeva, Su, and Joachims, 2020; Dud´ık,\nLangford, and Li, 2011; Saito and Joachims, 2022).\nRecently proposed Marginalized Inverse Propensity Score\nestimator (MIPS, Saito and Joachims, 2022) improves upon\nIPS in large action spaces by pooling information across\naction embeddings. At the same time, MIPS suffers from\nthe same problem as IPS for contexts in which a signifi-\ncant proportion of actions have low probability under the\nlogging policy. In this case, MIPS lacks information about\nthe actions to accurately estimate the importance weights,\nresulting in additional bias. In our work, we hypothesize\nthat closeness at the context level should translate into sim-\nilar behaviour for actions and rewards (for example, two\nmovies of the same franchise in a recommendation sys-\ntem).\nBased on this hypothesis, we propose an estima-\ntor that clusters the context space, and pools information\nacross all the contexts within a cluster. Informally, the pro-\nposed method solves the problem of deficient action infor-\nmation for a particular context by leveraging the informa-\ntion from all other contexts within the same cluster.\nWe define and analyze the theoretical bandit setup with\ncontext clusters in Section 3, which leads to the formal\nderivation of the CHIPS estimator, for which we analyze\nbias and variance. In section 4, we compare the estima-\ntor’s performance to the baselines on several synthetic and\nreal-world datasets, verifying the theoretical findings, and\narXiv:2502.21304v1  [cs.LG]  28 Feb 2025\n\n\nClustering Context in Off-Policy Evaluation\ndemonstrating its effectiveness. Finally section 5 explores\nfuture lines of work and CHIPS’ limitations.\n2\nBACKGROUND ON OFF-POLICY\nEVALUATION AND RELATED WORK\nThe off-policy evaluation problem (OPE) is usually framed\ninside the general contextual bandit setup. Given an agent,\ndetermined by the policy π ∶X × A →[0,1], the ban-\ndit’s data generation process is defined as iterative logging\nof the agent’s behavior when presented with different con-\ntexts. In each iteration, a context x ∈X ⊆Rdx is drawn\ni.i.d. from an unknown probability distribution p(x) over\nthe context space, an action a ∼π(a∣x) is selected from a\nfinite action space A, and a bounded reward r ∈[0,Rmax]\nis observed as a sample from an unknown conditional dis-\ntribution p(r∣a,x). The off-policy evaluation problem has\nbeen extensively studied from both a theoretical (McNel-\nlis et al., 2017; Saito et al., 2021; Dumitrascu, Feng, and\nEngelhardt, 2018; Irpan et al., 2019; Wang, Agarwal, and\nDud´ık, 2017) and a practical point of view given its appli-\ncations in fields such as recommendation systems (Li et al.,\n2011; Bendada, Salha, and Bontempelli, 2020; Saito et al.,\n2020) or healthcare (Varatharajah and Berry, 2022).\nWe measure the performance of a policy π through its\nvalue, that we define as:\nV (π) ∶= Ep(x)π(a∣x)p(r∣a,x)[r] = Ep(x)π(a∣x)[q(a,x)] (1)\nHere q(a,x) = Ep(r∣a,x) [r] denotes the conditional ex-\npected reward given an action a and a context x.\nIn practice, we are interested in finding a policy maximiz-\ning the expected reward observed in the bandit process. A\nvital part of this process is the off-policy evaluation prob-\nlem, in which we estimate the value of a policy π given\na dataset D ∶= {(xi,ai,ri)}N\ni=1 collected under a logging\npolicy π0 (i.e. D ∼∏N\ni=1 p(x)π0(a∣x)p(r∣a,x)). We use\nthe mean squared error (MSE) to quantify how well the es-\ntimate ˆV (π) approximates the real policy value V (π):\nMSE( ˆV ) = ED [(V (π) −ˆV (π;D)2)]\n= Bias( ˆV (π;D))\n2 + VD [ ˆV (π;D)]\nA wide variety of approaches have been proposed in the lit-\nerature to estimate V (π). From them, three can be distin-\nguished for being commonly used as starting points for de-\nveloping new estimators. The first one is the Direct Method\n(DM), which tries to estimate q(a,x) directly from Equa-\ntion (1):\nˆVDM(π;D, ˆq) = 1\nN\nN\n∑\ni=1\n∑\na∈A\nˆq(a,xi)\nThe bias of DM depends on the accuracy of the ˆq(a,x) ≈\nq(a,x) approximation, but the variance is usually lower\nthan in other approaches. Supervised learning in the DM’s\napproach can be particularly useful when generalization of\nan agent’s behaviour is needed due to limited information\nin the logging data (Sachdeva, Su, and Joachims, 2020).\nHowever, when the reward function has a high variance, or\nthe representation capacity is limited for the context-action\npairs in the evaluation policy domain, ˆq(a,x) could fail\nto accurately approximate q(a,x) (Farajtabar, Chow, and\nGhavamzadeh, 2018; Beygelzimer and Langford, 2009;\nKallus and Uehara, 2019). This problem, known as re-\nward misspecification, can be quite difficult to detect in\nreal-world examples (Farajtabar, Chow, and Ghavamzadeh,\n2018; Voloshin et al., 2021), and is the reason why DM is\ngenerally regarded as a highly biased estimator.\nThe second base approach is Inverse Propensity Scoring\n(IPS, Horvitz and Thompson, 1952), which approximates\nthe policy value by reweighting the rewards to correct the\nshift in action probabilities between the logging and evalu-\nation policies:\nˆVIPS(π;D) = 1\nN\nN\n∑\ni=1\nπ(ai∣xi)\nπ0(ai∣xi)ri = 1\nN\nN\n∑\ni=1\nw(ai,xi)ri\nAs per this definition, the context-action pairs selected by\nπ in which π0(a∣x) = 0 could be problematic, which moti-\nvates the following assumption:\nAssumption 2.1. (Common Support) Given an evaluation\npolicy π and a logging policy π0, the latest has common\nsupport for π if\nπ0(a∣x) > 0\n∀a ∈A,x ∈X ∶π(a∣x) > 0\nThe IPS estimator is unbiased under Assumption 2.1. How-\never, even when assumption 2.1 holds, IPS can present ex-\ncessive variance due to the weights w(ai,xi) taking larger\nvalues (Dud´ık, Langford, and Li, 2011; Saito and Joachims,\n2022).\nThis case is especially notable when π0 and π\nare significantly different or when trying to achieve uni-\nversal support (π0(a∣x) > 0 ∀a ∈A,x ∈X) in large ac-\ntion spaces (Saito and Joachims, 2022; Peng et al., 2023;\nSaito et al., 2021). Controlling the scaling of the propen-\nsity scores has motivated many approaches based on IPS,\nusing techniques such as weight clipping (Su et al., 2020;\nSu, Srinath, and Krishnamurthy, 2020; Swaminathan and\nJoachims, 2015a) and self normalization (Swaminathan\nand Joachims, 2015b; Kuzborskij et al., 2020). The Dou-\nbly Robust (DR) estimator combines DM and IPS, aiming\nto obtain a low-bias, low-variance estimate:\nVDR(π;D, ˆq) ∶= VDM(π; ˆq)\n+ 1\nN\nN\n∑\ni=1\nw(ai,xi)(ri −ˆq(ai,xi))\nThe DR estimator has been the cornerstone of multiple ap-\nproaches that modify the base estimator to address prob-\nlems such as low overlap between π and π0 (Wang, Agar-\nwal, and Dud´ık, 2017; Metelli, Russo, and Restelli, 2021;\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\nZhan et al., 2021; Guo et al., 2024), reward misspecifica-\ntion (Farajtabar, Chow, and Ghavamzadeh, 2018), and lim-\nited samples in logging data (Su et al., 2020; Felicioni et al.,\n2022). Unfortunately, the DR estimator can still inherit\nthe large variance problem from IPS, for example, when\ndealing with large action spaces (Saito, Ren, and Joachims,\n2023; Saito and Joachims, 2022; Shimizu and Forastiere,\n2023; Sachdeva et al., 2023; Taufiq et al., 2023). The prob-\nlem of dealing with large action spaces was recently stud-\nied, resulting in the Marginalized Inverse Propensity Scor-\ning (MIPS) (Saito and Joachims, 2022) estimator, in which\nthe authors pool information between similar actions given\nsome embedding representation e ∈E ⊂Rd\ne of them to ad-\ndress deficient actions in the logging policy. For this pur-\npose, they introduce an IPS-based estimator marginalizing\nthe probability over the action space:\nˆVMIPS(π;D) ∶= 1\nn\nn\n∑\ni=1\np(ei ∣xi,π)\np(ei ∣xi,π0)ri\n= 1\nn\nn\n∑\ni=1\nw (xi,ei)ri.\n(2)\nWhere p(e∣x,π) ∶= ∑a∈A p(e∣x,a)π(a∣x).\nThe idea of\nestimating deficient items’ behaviour by closely observed\nones inspired new approaches, like partitioning the ac-\ntion space in clusters (Peng et al., 2023; Saito, Ren,\nand Joachims, 2023), or an adaptive method for ranking\npolicies by optimizing user classification into given be-\nhavioural models and estimating independently for each\ngroup (Kiyohara et al., 2023). The MR estimator (Tau-\nfiq et al., 2023) diverged from the action space transfor-\nmations and proposed marginalization over the rewards\ndensity through a regression estimate of the importance\nweights:\nˆVMR(π;D) ∶= 1\nn\nn\n∑\ni=1\nw (ri)ri\n(3)\nWhere w(r) is defined as:\nw(r) ∶= fϕ∗(r) ∶=argminEϕ [(w(a,x) −fϕ(r))2]\nfϕ ∈{fϕ ∶R →R ∣ϕ ∈Φ}\n(4)\nMotivated by these approaches, as well as the fact that es-\ntimating from similar actions or make a regression over re-\nwards could prove challenging if a significant proportion of\nthese actions are missing for a given context, we propose\nthe Context-Huddling Inverse Propensity Score (CHIPS)\nestimator that we introduce in the next section.\n3\nTHE CHIPS ESTIMATOR\nThe CHIPS estimator is based on the idea of partitioning\nthe context space into clusters to extrapolate the behaviour\nof an agent when presented with a previously unseen or un-\nderrepresented context x. The assumption needed for this\napproximation to the OPE problem is that, given a policy,\nall contexts belonging to a cluster c should have a similar\nprobability of observing an action a and will observe sim-\nilar rewards when that action is chosen. Formally, we will\nconsider a finite partition of the context space as the cluster\nspace C ∶= {Ci}K\ni=1 with Ci ⊂X and ci ∩Cj = ∅. We assume\nthat we are given a c ∈C for each context x ∈X, where we\nassume that c is drawn i.i.d from an unknown distribution\np(c∣x). Thus, given a policy π, we can compute its value\nby refining Equation (1):\nV (π) ∶= Ep(x)p(c∣x)π(a∣x)p(r∣a,c,x)[r]\n= Ep(x)p(c∣x)π(a∣x)[q(a,c,x)].\n(5)\nWhere we denote q(a,c,x)\n∶=\nEp(r∣a,c,x) [r] and it\nis important to note that Ep(c∣x)π(a∣x) [q(a,c,x)]\n=\nEπ(a∣x) [q(a,x)], and therefore the refinement is consistent\nwith Equation (1). Similar to the common support con-\ndition in IPS, we formulate the following property as the\nequivalent for the CHIPS estimator of Assumption 2.1.\nAssumption 3.1. (Common Cluster Support)\nGiven an\nevaluation policy π and a logging policy π0, the latest has\ncommon cluster support for π if\np(a∣c,π0) > 0\n∀a ∈A,c ∈C ∶p(a∣c,π) > 0\nWhere we denote\np(a∣c,π) = ∫X π(a∣x)p(x∣c)dx\nAssumption 3.1 is weaker than Assumption 2.1 since for a\ngiven triplet (x,c,a) ∈X × C × A, the fact that π0(a∣x) =\n0,π(a∣x) > 0 does not ensure the same holds for every con-\ntext within c. The idea of a homogeneous behaviour for ev-\nery context inside a given cluster would make the CHIPS\nestimator circumvent the bias increase when Assumption\n2.1 is not met for the IPS estimator (if Assumption 3.1\nholds). Regarding the reward, this concept is formalized\nin the following assumption.\nAssumption 3.2. (Reward Homogeneity) We say that we\nobserve reward homogeneity if the context x does not affect\non the reward r given some action a and some context c\n(i.e., rx ∣c,a).\nThe reward homogeneity assumption eliminates the depen-\ndency of the context on the reward when provided with the\ncluster and the action. Note that complying with Assump-\ntion 3.2 implies q(a,c,x) = q(a,c,y) = q(a,c), where\nx,y ∈X, which together with Assumption 3.1 gives an al-\nternative expression for the policy value in the following\nproposition:\nProposition 3.3. Given a policy π, if Assumptions 3.1 and\n3.2 hold, then we have that\nV (π) ∶= Ep(c)p(a∣c,π) [q(a,c)]\n(6)\nPlease refer to Appendix A.1 for a complete proof.\n\n\nClustering Context in Off-Policy Evaluation\nConsidering the similarity of Equation (6) with the origi-\nnal policy value definition (Equation (1)), Proposition 3.3\nnaturally motivates the analytical expression of the CHIPS\nestimator:\nˆVCHIPS(π;D) ∶= 1\nN\nN\n∑\ni=1\np(ai∣ci,π)\np(ai∣ci,π0)ri = 1\nN\nN\n∑\ni=1\nw(ai,ci)ri\n3.1\nTheoretical Analysis\nFirst, we characterize the bias of the CHIPS estimator de-\npending on the compliance with Assumptions 3.1 and 3.2.\nProposition 3.4. Under the Common Cluster Assumption\n(3.1) and the Cluster Homogeneity Assumption (3.2), the\nCHIPS estimator is unbiased for any given policy π:\nED [ ˆVCHIPS(π;D)] = V (π)\nPlease refer to Appendix A.2 for a complete proof.\nWe note here that Proposition 3.4 implies that even when\nthe Common Support Assumption (2.1) fails to ensure the\nunbiasedness of the IPS estimator, the CHIPS estimator\ncan still use the more permissive Common Cluster Sup-\nport (3.1), and the Reward Homogeneity (3.2) Assumption\nto ensure an unbiased estimate. Although Assumption 3.2\nguarantees homogeneity at the reward level, a completely\nhomogeneous behaviour would also eliminate the context\ndependency at the action level, implying a deterministic\npolicy given cluster, i.e. p(a∣c,π) = π(a∣x) ∀x ∈c. Both\nhomogeneity conditions present a desirable scenario for the\nCHIPS estimator; however, they rarely occur when work-\ning in real-world data environments, which motivate the\nfollowing assumption as a relaxation of the action-context\nindependence:\nAssumption 3.5. (δ-Homogeneity) Given a policy π, we\nsay that the policy presents δ-homogeneity if for any given\naction a ∈A, and any given cluster c ∈C, there exist\nδ−\nπ,c,a ≤1 and δ+\nπ,c,a ≥1 such that:\nδ−\nπ,c,a ≤π(a∣x)\np(a∣c,π) ≤δ+\nπ,c,a\n∀x ∈X\nIt is worth noting that if p(a∣c,π) ≠0 ∀(x,c,a) ∈D\nthen it is always possible to find δ−\nπ,c,a, δ+\nπ,c,a satisfying\nδ-Homogeneity. The following proposition gives an upper\nbound for the bias of the CHIPS estimator when Assump-\ntion 3.2 cannot be ensured:\nProposition 3.6. Given the logging data {(xi,ai,ri)}N\ni=1\nobserved under some logging policy π0, and an evaluation\npolicy π if the latest has common cluster support over the\nearliest, then we have that\n∣Bias( ˆVCHIPS(π))∣≤∣Ep(c)p(x∣c)p(a∣c,π)[q(a,c,x) ⋅∆c,a]∣\nWhere by Assumption 3.5 we have bounds (δ−\nπ,c,a, δ+\nπ,c,a)\nfor π, (δ−\nπ0,c,a, δ+\nπ0,c,a) for π0, and we denote ∆c,a =\nmax{δ+\nπ,c,a,δ+\nπ0,c,a} −min{δ−\nπ,c,a,δ−\nπ0,c,a}. Please refer to\nAppendix A.3 for a complete proof.\nProposition 3.6 formalizes the intuition on how the bias\nof the estimator under Assumption 3.1 depends on the ex-\ntent to which the contexts inside a cluster behave homoge-\nneously under a given policy. Formally, the gap δ+\nπ −δ−\nπ\ndetermines how close the CHIPS is to being unbiased, be-\ning the case δ−\nπ,c,a = δ+\nπ,c,a = 1 the perfect scenario. In\nthis case, we have that π(a∣x) = p(a∣c,π), which means\nthat the weights in IPS w(a,x) = w(a,c), and we could\nin theory substitute any context for any other within the\nsame cluster for calculations, mitigating the problems that\narise when Assumption 2.1 does not hold. Additionally, we\ncan also provide an expression for the difference in mean\nsquared error with respect to IPS in the same conditions as\nProposition 3.6:\nProposition 3.7. Under the same conditions as in Propo-\nsition 3.6, the difference in mean squared error between\nCHIPS and MIPS can be expressed as\nMSE( ˆVIPS(π)) −MSE( ˆVCHIPS)\n= VD [ ˆVIPS(π)] −VD [VCHIPS(π;D)]\n−Bias( ˆVCHIPS(π))\n2\nPlease refer to Appendix A.4 for a complete proof.\nIt is also worth studying the bias of the CHIPS estimator\nwhen the Common Cluster Support assumption does not\nhold, while the Assumption 3.2 holds. For this purpose, we\nacknowledge that the bias of the IPS estimator when As-\nsumption 2.1 is not met can be given in terms of the actions\nviolating such assumption (Sachdeva, Su, and Joachims,\n2020):\n∣Bias( ˆVIPS(π;D))∣= Ep(x)\n⎡⎢⎢⎢⎢⎣\n∑\nU(x,π0)\nπ(a∣x)q(a,c,x)\n⎤⎥⎥⎥⎥⎦\nWhere U(x,π0) ∶= {a ∈A ∣π0(a,x) = 0} are known as\nthe deficient actions. Following a similar approach we in-\ntroduce the following proposition:\nProposition 3.8. Given the logging policy π0 and some\nevaluation policy π, the absolute bias of the CHIPS esti-\nmator when Assumption 3.2 holds can be expressed as\n∣Bias( ˆVCHIPS(π;D))∣= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\nU(c,π0)\np(a∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\nWhere U(c,π0) ∶= {a ∈A ∣p(a∣π0,c) = 0}. Please refer to\nAppendix A.5 for a complete proof.\nCorollary 3.9. Under the conditions of Proposition 3.8, we\nhave that\n∣Bias( ˆVIPS(π;D))∣−∣Bias( ˆVCHIPS(π;D))∣\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\nU(x,π0)∖U(c,π0)\np(a ∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\n.\nPlease refer to Appendix A.5 for a complete proof.\nNote that in this case, the CHIPS’ reduction in absolute bias\ndepends directly on the number of actions that violate As-\nsumption 2.1, but still comply with Assumption 3.2. Thus,\nthe greater the number of deficient actions by Common\nSupport condition covered by the Common Cluster Sup-\nport, the more significant the bias reduction with respect to\nIPS. In this conditions, its also interesting to study the dif-\nference in bias with respect to the other two transformation-\nbased methods (MR and MIPS), a result given by the next\nproposition:\nProposition 3.10. Let fϕ∗be defined as in Equation (4)\nwith fϕ∗= w(a,x) + ϵ for some ϵ ∈R and e ∈E give action\nembeddings. Under the conditions of the Proposition 3.8,\nwe have that:\n∣Bias( ˆVMR;D)∣−∣Bias( ˆVCHIPS;D)∣\n= −Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈(U(c,π0)/U(c,π0))\nq(a,c)p(a ∣π,c)\n⎤⎥⎥⎥⎥⎦\n+ ϵEp(c) [ ∑\na∈A\nq(a,c)p(a ∣π0,c)]\n∣Bias( ˆVMIPS;D)∣−∣Bias( ˆVCHIPS;D)∣\n= Ep(x)\n⎡⎢⎢⎢⎢⎣\n∑\ne∈U(e,π0)\np(e ∣x,π)q(x,e)\n⎤⎥⎥⎥⎥⎦\n−Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π0)\np(a ∣c,π)q(a,c)\n⎤⎥⎥⎥⎥⎦\n.\nPlease refer to Appendix A.6 for a complete proof.\nWhen studying homogeneity at an action level, we have\nfocused on the probability of observing an action for a par-\nticular context x within a cluster c (i.e., π(a∣x)). Con-\nversely, we can also study the predictability of a context\ngiven an action and a cluster under a policy π, which we\ndenote as p(x∣a,c) = π(x∣a,c). Ideally, we would have\nthat the conditional probability distribution of the context\ngiven the action and the cluster is uniform (i.e., π(xi∣a,c) =\nπ(xj∣a,c) ∀xi,xj ∈c). Predictability is used in the fol-\nlowing proposition, that characterizes the relation between\nthe reduction in variance of the CHIPS estimator with re-\nspect to IPS:\nProposition 3.11. Given a logging policy π0, under the\nCommon Support Assumption (2.1) and the Reward Homo-\ngeneity Assumption (3.2) we have that\nN (VD [ ˆVIPS(π;D)] −VD [ ˆVCHIPS(π;D)])\n= Ep(c)p(a∣c,π0) [Vπ0(x∣a,c) [w2(a,x)]Ep(r∣a,c) [r2]].\nNote that this quantity is always positive, implying that\nCHIPS always reduces the variance of IPS. Please refer\nto Appendix A.7 for a complete proof.\nProposition 3.11 indicates that when Assumptions 2.1 and\n3.2 hold, the variance reduction of CHIPS compared to\nIPS corresponds to the total decrease in mean squared er-\nror when approximating the actual policy value V (π), as\nboth estimators are unbiased under these conditions. This\nmean squared error gap is influenced by two factors: First,\nEp(r∣a,c) [r2], reflecting the noise in rewards for actions\nwithin the same cluster (related to Assumption 3.2). Sec-\nond, the variance of IPS weights conditioned on the pre-\ndictability p(x∣a,c), which increases when w(a,x) varies\nwidely (e.g., when logging and evaluation policies differ)\nor when π(x∣a,c) is uninformative (contexts behave uni-\nformly given the cluster and action). Thus, the variance re-\nduction in CHIPS is particularly pronounced when IPS ex-\nhibits high variance and contexts within a cluster are simi-\nlar. Furthermore, if MIPS and CHIPS are in the same space\n(considering contexts c ∈C as described and action embed-\ndings e ∈E), Proposition 3.11 can be extended to show that\nCHIPS has less variance than MIPS:\nProposition 3.12. In context-action-embedding joint space\n(X →C →A →E →[0,Rmax]), if Assumptions 3.1 and\n3.2 hold, as well as their MIPS counterparts (Common Em-\nbedding Support and No Direct Effect), then we have that\nVD( ˆVIPS(π)) ≥VD( ˆVMIPS(π)) ≥VD( ˆVCHIPS(π)) ≥0\nPlease refer to Appendix A.8 for a complete proof.\n3.2\nEmpirical Calculations\nThe alternative analytical expression for the policy value\ngiven in Equation 6 eliminates the dependency on the orig-\ninal definition of policy value and motivates the CHIPS es-\ntimator under assumptions 3.1 and 3.2. However, in prac-\ntice, assessing if such conditions hold is complicated, par-\nticularly if we have limited logging data. To mitigate this\nproblem and justify using CHIPS in real-world settings, we\nneed to make an approximation to context-homogeneous\nbehavior on both action and reward levels within a cluster.\nIn practice, we have a clustering method ξ ∶X →C, and we\nuse the transformation:\nτ∶(X,A,[0,Rmax]) →(X,C,A,[0,Rmax])\n(x,a,r) ↦(x,ξ(x),a,r).\nGiven a policy π and a cluster c, we use the definition to\nestimate p(a∣c,π):\np(a∣c,π) = ∫X π(a∣x)p(x∣c)dx\n\n\nClustering Context in Off-Policy Evaluation\n= ∫x∈c π(a∣x)p(x∣c)dx\n≈\n1\n∣Dc∣∑\nx∈Dc\nπ(a∣x),\n(7)\nHere, we denote Dc = {(x, ˜c,a,r) ∈τ(D) ∶˜c = c}. In\nEquation 7, we used that p(x∣c) = 0 if c ≠ξ(x). Since\nthis equation is essentially Ep(x∣c) [π(a∣x)], we approxi-\nmate this value by averaging π(a∣x) over all contexts inside\nthe given cluster.\nThe second approximation needed involves the reward\nbeing independent of the context given the action and\nthe cluster, i.e., q(a,c,x) = q(a,c).\nFollowing a sim-\nilar approach than in the previous case, for a particular\n(given) action a and cluster c, we observe that q(a,c) =\nEp(x∣c) [π(r∣a,c,x)], which motivates the idea of an av-\nerage reward per cluster. In our synthetic experiments, the\nreward is binary, therefore we will assume that the observa-\ntions inside a cluster are observations in a Bernoulli process\n(i.e., Rc ∼Ber(θ)) and estimate this average reward using\ntwo different approaches:\n• Maximum Likelihood (ML) In which we just average\nthe rewards observed within a cluster c for each ac-\ntion a as ˆrmean(a,c) =\n1\n∣Rc∣∑Rc rk with Rc ∶= {rk ∶\n(xk,ck,ak,rk) ∈Dc}.\n• Maximum A Posteriori (MAP). In this setting, estimat-\ning the average reward is equivalent to estimating the\nmost probable θ using a beta prior, where we obtain:\nˆrbayes(α, ˆβ;c) = (α −1) + ∑Rc rk\nα + ˆβ + ∣Rc∣−2\nWhere we denote α, ˆβ as the parameters of the prior Beta\ndistribution. In our experiments, we use non-informative\npriors (α = ˆβ) (Tuyl, Gerlach, and Mengersen, 2008;\nKerman, 2011) and we explore the choosing of this pa-\nrameter for arbitrary problems in Appendix D.4. Please\nrefer to Appendix B for the complete derivations of the\nMAP and ML estimations.\n4\nEXPERIMENTS\n4.1\nSynthetic dataset\nWe compare CHIPS with other baseline estimators (IPS,\nDM, DR, SNIPS (Swaminathan and Joachims, 2015b),\nDRoS (Su et al., 2020), SNDR (Thomas and Brunskill,\n2016), MR (Taufiq et al., 2023)) in estimating the evalu-\nation policy value in a cluster-based synthetic dataset in\nwhich we can control the difficulty of the OPE problem.\nA description of all hyperparameters used for generation\n(e.g., anum, cexp ...) can be found in Appendix C. We\nstart by generating cluster centers C ∶= {ck}m\nk=1 inside a\ndx-dimensional ball B(0,cexp) ∶= {x ∈Rdx ∶∣∣x∣∣2 < cexp}\nusing a variation of the Box-Muller transformation (Box\nand Muller, 1958):\nck = cexp ⋅uk−dx ⋅zk\n∣∣zk∣∣\n,\nwhere U ∶= {uk}m\nk=1 ∼U[0,1] and Z ∶= {zk}m\nk=1 ∼\nN(0,1dx). We sample S ∶= {sk}m\nk=1 ∼U[0,1], and use\nthe softmax transformation ϕ(S) to define p(ci) = ϕ(S)i.\nThen, we sample cluster centers according to this distribu-\ntion w = {wi}xnum\ni=1 ∼ϕ(S), and, for each center ci, we uni-\nformly sample points belonging to the n-ball centered on\nci, using the same variation of the Box-Muller transform\nthat we used previously:\nXi = (x1\ni ,...,xhi\ni ) ∼U[B(ci,crad)]\nNote here that hi = ∑xnum\ni=1 1{ci=wi}. We define the context\nspace as the union of these generated points X = ⋃m\ni=1 Xi =\n{xi}xnum\ni=1 . We sample V = {vi}xnum\ni=1 ∼N(0,1) and define\np(xi) = ϕ(V)i using the ϕ softmax transformation again.\nWe then use these probabilities to sample the logging (Xlog)\nand evaluation (Xeval) data, with ∣Xeval∣= elen and ∣Xlog∣=\nblen. To generate the policies, we sample yi = {yj\ni }anum\nj=1 ∼\nN(0,1) for every cluster ci (where anum is the number of\nactions) and z = {zk}xnum\ni=1 ∼N(0,1) to define the policies\nfor every context in cluster ci as:\nπ(aj∣ci,xk) =\neyj\ni +σzk\n∑anum\nm=1 eym\ni +σzk\nπ0(aj∣ci,xk) =\neβ(yj\ni +σzk)\n∑anum\nm=1 eβ(ym\ni +σzk) ,\n−1 ≤β ≤1\nGiven a context xk, both policies are determined by a term\nthat depends on the cluster and the action (uj\ni), and a term\nthat depends on the context itself (xk). Here 0 ≤σ ≤1\ncontrols how independent a policy is from the context and\nβ how close the logging and evaluation policies are. For\nobtaining the actions, we sample Alog ∼π0 and Aeval ∼π.\nFor generating the rewards, we create a misspecified reward\nsetting by defining:\nr(ai,ci,xi) = 1{ui < π(ai∣ci,xi) ⋅∣∣xi∣∣1\ncexpdx\n},\nwhere ui ∼U[0,1].\nThe reward depends on two fac-\ntors; the first one is the Manhattan norm of the context;\nthe further from 0, the more likely it is to observe a pos-\nitive reward. The second factor is the evaluation policy\nπ(ai∣ci,xi), which makes this a misspecified reward set-\nting when the logging and evaluation policies are differ-\nent enough. In this case, the (ai,ci,xi) triplets having the\nhighest probability of observation under the evaluation pol-\nicy are more likely to observe positive rewards, resulting\nin a significant difference with respect to the observed re-\nwards under the logging policy for such triplets. We sample\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n0\n200\n400\n600\n800\n1000\nNº Clusters\n10\n3\n10\n2\nMSE\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n10\n4\n10\n3\n10\n2\nMSE\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of Deficient Actions\n10\n1\n100\nMSE\nDM\nIPS\nDR\nCHIPS_bayes\nCHIPS_mean\nMR\nSNDR\nDRoS\nSNIPS\nFigure 1: From left to right, the mean square error in the synthetic dataset experiments varying the number of clusters,\nthe distributional shift between logging and evaluation policy (β), and the number of deficient actions in the logging data\n(normalized w.r.t. IPS).\nrewards using this method for the logging (Rlog) and eval-\nuation (Reval) data to obtain Dlog ∶= (X,C,Alog,Rlog) and\nDeval ∶= (X,C,Aeval,Reval). Finally, we select a subset for\nN samples from both sets. A representation of the gener-\nated structure can be found in Figure 20.\n4.1.1\nSynthetic results\nIn this section we analyze CHIPS performance while vary-\ning parameters of the synthetic dataset.\nIn our experi-\nments, the generation process for each parameter value is\nrepeated 100 times with different random seeds. The fi-\nnal reported results are the average over all experiments,\nwith the standard deviation corresponding to the lighter\nbands represented in all the figures. The basic configura-\ntion for the parameters used throughout the experiments\ncan be found in Appendix C, along with the specifications\nof the hardware used. We use Random Forest (Breiman,\n2001) to obtain ˆq(x,a) in DM-based methods and mini-\nbatch KMeans (Sculley, 2010) implementation in SciKit-\nLearn (Pedregosa et al., 2011) as the clustering method\nfor CHIPS (alternative clustering methods and their per-\nformance are also discussed in Appendix D). We also use\nβ = −1, maximizing the distributional shift between log-\nging and evaluation policies.\nNumber of clusters.\nFor this experiment, we vary the\nnumber of clusters the CHIPS estimator uses, with values\nranging from 1 to 1000. Since β = −1, the implementation\nof CHIPS using ML reward estimation is unsuccessful (see\nAppendix D.3 for a further discussion). On the other hand,\nfor the MAP case, we observe a v-shaped error graph (see\nFigure 1 (left)), suggesting that CHIPS performance is sen-\nsitive to effectiveness of clustering. In particular, we have\na highly biased estimation when assuming insufficient or\nexcessive clusters (see Figure 3). The reason for this bias\nin the first case might be an oversimplification of the struc-\nture of the cluster space. Conversely, we progressively gain\nbias when we select too many clusters according to Propo-\nsition 3.8 as CHIPS converges to IPS. In this case, CHIPS\nis also vulnerable to reward misspecification, which causes\nan increase in variance. In practice, this parameter can be\nselected by considering the possible CHIPS estimates as a\nparametric family depending on the number of clusters and\nuse the PAS-IF technique (Udagawa et al., 2023) to choose\nthe optimal number of clusters.\nBeta.\nThis experiment examines the impact of the dis-\ntributional policy shift between π and π0. Lower values\nin our range (i.e., π0 ←→π) result in significant policy\nshifts that introduce bias in IPS estimates for large context-\naction spaces (Saito and Joachims, 2022; Sachdeva, Su,\nand Joachims, 2020). The CHIPS estimator mitigates this\nby treating all context-action samples within a cluster as if\nthey share the same context. However, when β is low, these\nvirtual extra samples may not suffice for accurate estima-\ntion, as the most relevant (x,a) pairs (π(a∣x) near 1) are\nunderrepresented (see Appendix D.3). In such cases, ML\nestimation in CHIPS is ineffective, while MAP estimation\nprovides some resistance by pushing reward estimates to-\nwards the posterior expectation, making it sensitive to prior\nchoice. However, this resistance can be counterproductive\nwhen the distributional shift is small (β close to 1), as both\nML estimates and IPS converge faster to more accurate es-\ntimations (see Figure 1 (center)).\nDeficient actions. In this setting we explicitly set the prob-\nability (π0) of observing a variable number of actions in\nthe action space to 0 and evaluate CHIPS’ response in a\nspace with 200 actions and β = −1. This setting is quite\nchallenging as not only we have deficient actions but also a\nsignificant distributional shift between policies. The major-\nity of baselines perform at a similar level than IPS with the\nexception of DRoS (Su et al., 2020), that performs slightly\nbetter but is still outperformed by CHIPS.\nAdditional experiments and discussions of results varying\nother parameters, different clustering methods, and a time\n\n\nClustering Context in Off-Policy Evaluation\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n101\n102\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(\nMSE(V)CLIPS\nMSE(V)IPS < x)\n0.99\n0.81\n0.40\n0.41\n0.41\n0.40\n0.78\n0.80\nMSE 50,000 samples\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n101\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(\nMSE(V)CLIPS\nMSE(V)IPS < x)\n1.00\n0.90\n0.32\n0.38\n0.38\n0.35\n0.79\n0.90\nMSE 100,000 samples\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(\nMSE(V)CLIPS\nMSE(V)IPS < x)\n1.00\n0.99\n0.25\n0.11\n0.11\n0.20\n0.92\n0.99\nMSE 500,000 samples\nIPS\nCHIPS_bayes\nCHIPS_mean\nDM\nDR\nMRDR\nMIPS (w/o SLOPE)\nMIPS (w/ SLOPE)\nMR\nIPS\nCHIPS_bayes\nCHIPS_mean\nDM\nDR\nMRDR\nMIPS (w/o SLOPE)\nMIPS (w/ SLOPE)\nMR\nIPS\nCHIPS_bayes\nCHIPS_mean\nDM\nDR\nMRDR\nMIPS (w/o SLOPE)\nMIPS (w/ SLOPE)\nMR\nFigure 2: ECDF of the relative mean squared error with respect to IPS for the real dataset using 50000 (left), 100000\n(center), and 500000 (right) logging samples.\ncomplexity analysis can be found in Appendix G.\n4.2\nReal dataset\nFollowing the literature, for assessing the capabilities of the\nCHIPS estimator in a real-world environment, we compare\nthe performance in the Open Bandit Dataset (OBD) (Saito\net al., 2020) of IPS, DM, DR, MRDR (Farajtabar, Chow,\nand Ghavamzadeh, 2018) and MIPS (Saito and Joachims,\n2022), with and without SLOPE (Su, Srinath, and Krish-\nnamurthy, 2020). The OBD dataset was gathered using\ntwo different policies during an A/B test: uniform random,\nwhich we consider as logging (i.e., π0), and Thompson\nsampling (Thompson, 1933, 1935), which we consider as\nevaluation (i.e., π). The dataset is based on a recommenda-\ntion system for fashion e-commerce. We observe user data\nas contexts x, items to recommend a ∈A (with ∣A∣= 240)\nand rewards r ∈{0,1} representing user interactions.\nFollowing the experimental protocol of Saito and Joachims\n(2022) (see Appendix F), we experiment with the real\ndataset varying the number of logging samples available for\nthe estimation using 50000, 100000, and 500000 samples\nto compute the Empirical Cumulative Distribution Func-\ntion (ECDF) of the normalized mean squared error with re-\nspect to IPS. We increase the number of clusters for CHIPS\nas more logging samples are available to try to maximize\nperformance, following the intuition from our earlier ex-\nperiments on the synthetic dataset (see Figure 12 (right)).\nWe use 8 clusters for 100000 samples as a reference from\nour results for 240 actions in the synthetic dataset (see Fig-\nure 12 (left)). Regarding the clustering method, we use\nagain mini-batch KMeans.\nWe observe that the CHIPS estimator using the ML approx-\nimation is slightly better (+3%) than MIPS when few sam-\nples are available (see Figure 2, (left)). This performance\ngap widens (+11%) as the CHIPS estimator has more sam-\nples available (see Figure 2, (center)) and starts narrowing\n(+7%) as the number of samples is enough for MIPS to\nalso start making more accurate estimations (see Figure 2,\n(right)).\nUsing the MAP reward estimation for CHIPS provides a\nconsiderable advantage in all experiments since the real\ndataset present severe reward misspecification, as discussed\nin Appendix D.3. Similarly to the synthetic dataset, the\npartition structure of the cluster space and the α parame-\nter in MAP are sensitive parameters. In particular, for the\nnumber of clusters, we observe that using an insufficient\nor excessive number of clusters can negatively impact per-\nformance (see Figure 14 (left)) as we discussed in section\nSection 4.1.1. Regarding the value of α for the Beta prior,\nfollowing the results from the synthetic experiment study-\ning the effect of this parameter conjointly with the distri-\nbutional shift between logging and evaluation policies (see\ndiscussion in Appendix D.4 and Figure 13), we used α = 20\nas the logging policy is uniform (the equivalent of β = 0 in\nthe synthetic dataset). Figure 14 (right) shows how choos-\ning a lower or higher value for α deteriorates the perfor-\nmance of the CHIPS estimator, reaffirming the results ob-\nserved in the synthetic dataset (see Figure 13).\n5\nCONCLUSIONS, LIMITATIONS AND\nFUTURE WORK\nIn this work we have explored an alternative approach to\nthe OPE problem by clustering contexts instead of pooling\ninformation over actions to mitigate the problems arising\nin IPS when the Common Support condition does not hold.\nThe proposed setup for the OPE problem using contexts led\nto the CHIPS estimator, which uses a similar approach to\nIPS applied over clusters instead of contexts. We have stud-\nied this estimator extensively from a theoretical and prac-\ntical perspective, evaluating its performance for different\nconfigurations in a controlled synthetic dataset and a real-\nworld example. The results obtained in the experiments for\nboth cases demonstrate that the CHIPS estimator provides\na significant improvement in estimation accuracy, outper-\nforming existing estimators if the context space has a clus-\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\nter structure. The accuracy of CHIPS is also influenced by\nthe accuracy of the clustering method and the homogeneity\nbehaviour of contexts inside the same cluster. Additionally,\nchoosing a balanced number of clusters to avoid over- and\nunder-simplification of the cluster structure is an impor-\ntant part of the estimation process and opens the possibility\nof exploring if it is possible to estimate the optimal value\nfor hyperparameters beyond empirical estimation or even\nif combining CHIPS with pure action-embedding methods\nlike MIPS can improve general performance.\nReferences\nAnkerst, M.; Breunig, M. M.; Kriegel, H.-P.; and Sander, J.\n1999. OPTICS: Ordering Points to Identify the Cluster-\ning Structure. SIGMOD Rec., 28(2): 49–60.\nAttias, H. 1999.\nA Variational Baysian Framework for\nGraphical Models. In Solla, S.; Leen, T.; and M¨uller, K.,\neds., Advances in Neural Information Processing Sys-\ntems, volume 12. MIT Press.\nBastani, H.; and Bayati, M. 2019. Online Decision Mak-\ning with High-Dimensional Covariates. Operations Re-\nsearch, 68.\nBendada, W.;\nSalha, G.; and Bontempelli, T. 2020.\nCarousel Personalization in Music Streaming Apps with\nContextual Bandits.\nIn Proceedings of the 14th ACM\nConference on Recommender Systems, RecSys ’20,\n420–425. New York, NY, USA: Association for Com-\nputing Machinery. ISBN 9781450375832.\nBeygelzimer, A.; and Langford, J. 2009. The Offset Tree\nfor Learning with Partial Labels. In Proceedings of the\n15th ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, KDD ’09, 129–138.\nNew York, NY, USA: Association for Computing Ma-\nchinery. ISBN 9781605584959.\nBishop, C. M. 2006.\nPattern Recognition and Machine\nLearning (Information Science and Statistics). Berlin,\nHeidelberg: Springer-Verlag. ISBN 0387310738.\nBlei, D.; and Jordan, M. 2006. Variational inference for\nDirichlet process mixtures. Bayesian Analysis, 1.\nBouneffouf, D.; Rish, I.; and Aggarwal, C. 2020. Survey on\nApplications of Multi-Armed and Contextual Bandits.\nIn 2020 IEEE Congress on Evolutionary Computation\n(CEC), 1–8.\nBox, G. E. P.; and Muller, M. E. 1958.\nA Note on the\nGeneration of Random Normal Deviates. The Annals of\nMathematical Statistics, 29(2): 610 – 611.\nBreiman, L. 2001. Random Forests. Machine Learning,\n45: 5–32.\nComaniciu, D.; and Meer, P. 2002. Mean Shift: A Robust\nApproach Toward Feature Space Analysis. IEEE Trans.\nPattern Anal. Mach. Intell., 24(5): 603–619.\nDeng, L. 2012. The mnist database of handwritten digit\nimages for machine learning research. IEEE Signal Pro-\ncessing Magazine, 29(6): 141–142.\nDua, D.; and Graff, C. 2017.\nUCI Machine Learning\nRepository.\nDud´ık, M.; Langford, J.; and Li, L. 2011. Doubly Robust\nPolicy Evaluation and Learning. In International Con-\nference on Machine Learning.\nDumitrascu, B.; Feng, K.; and Engelhardt, B. E. 2018. PG-\nTS: Improved Thompson Sampling for Logistic Contex-\ntual Bandits. In Neural Information Processing Systems.\nEster, M.; Kriegel, H.-P.; Sander, J.; and Xu, X. 1996.\nA Density-Based Algorithm for Discovering Clusters in\nLarge Spatial Databases with Noise.\nIn Proceedings\nof the Second International Conference on Knowledge\nDiscovery and Data Mining, KDD’96, 226–231. AAAI\nPress.\nEveritt, B. 1996. An introduction to finite mixture distri-\nbutions. Statistical Methods in Medical Research, 5(2):\n107–127. PMID: 8817794.\nFarajtabar, M.; Chow, Y.; and Ghavamzadeh, M. 2018.\nMore Robust Doubly Robust Off-policy Evaluation. In\nDy, J.; and Krause, A., eds., Proceedings of the 35th\nInternational Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Research,\n1447–1456. PMLR.\nFelicioni, N.; Dacrema, M. F.; Restelli, M.; and Cremonesi,\nP. 2022. Off-Policy Evaluation with Deficient Support\nUsing Side Information. In Neural Information Process-\ning Systems.\nFrey, B. J.; and Dueck, D. 2007. Clustering by Passing\nMessages Between Data Points.\nScience, 315(5814):\n972–976.\nGuo, Y.; Liu, H.; Yue, Y.; and Liu, A. 2024. Distribution-\nally Robust Policy Evaluation under General Covariate\nShift in Contextual Bandits. ArXiv, abs/2401.11353.\nHorvitz, D. G.; and Thompson, D. J. 1952. A General-\nization of Sampling Without Replacement from a Finite\nUniverse. Journal of the American Statistical Associa-\ntion, 47: 663–685.\nIrpan, A.; Rao, K.; Bousmalis, K.; Harris, C.; Ibarz, J.; and\nLevine, S. 2019. Off-Policy Evaluation via Off-Policy\nClassification. In Wallach, H.; Larochelle, H.; Beygelz-\nimer, A.; d'Alch´e-Buc, F.; Fox, E.; and Garnett, R.,\neds., Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc.\nKallus, N.; and Uehara, M. 2019. Intrinsically Efficient,\nStable, and Bounded off-Policy Evaluation for Rein-\nforcement Learning. Red Hook, NY, USA: Curran As-\nsociates Inc.\n\n\nClustering Context in Off-Policy Evaluation\nKerman, J. 2011. Neutral noninformative and informative\nconjugate beta and gamma prior distributions. Electronic\nJournal of Statistics, 5.\nKiyohara, H.; Uehara, M.; Narita, Y.; Shimizu, N.; Ya-\nmamoto, Y.; and Saito, Y. 2023.\nOff-Policy Evalua-\ntion of Ranking Policies under Diverse User Behavior.\nIn Proceedings of the 29th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, KDD ’23,\n1154–1163. New York, NY, USA: Association for Com-\nputing Machinery. ISBN 9798400701030.\nKrizhevsky, A.; Nair, V.; and Hinton, G. 2009. CIFAR-100\n(Canadian Institute for Advanced Research).\nKuzborskij, I.; Vernade, C.; Gyorgy, A.; and Szepesvari,\nC. 2020. Confident Off-Policy Evaluation and Selection\nthrough Self-Normalized Importance Weighting. ArXiv,\nabs/2006.10460.\nLi, L.; Chu, W.; Bellevue, M.; Langford, J.; and Wang,\nX. 2011. An Unbiased Offline Evaluation of Contex-\ntual Bandit Algorithms with Generalized Linear Models.\nJournal of Machine Learning Research, 1.\nMcNellis, R.; Elmachtoub, A. N.; Oh, S.; and Petrik,\nM. 2017. A Practical Method for Solving Contextual\nBandit Problems Using Decision Trees. In Elidan, G.;\nKersting, K.; and Ihler, A. T., eds., Proceedings of the\nThirty-Third Conference on Uncertainty in Artificial In-\ntelligence, UAI 2017, Sydney, Australia, August 11-15,\n2017. AUAI Press.\nMetelli, A. M.; Russo, A.; and Restelli, M. 2021. Sub-\ngaussian and Differentiable Importance Sampling for\nOff-Policy Evaluation and Learning.\nIn Ranzato, M.;\nBeygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan,\nJ. W., eds., Advances in Neural Information Processing\nSystems, volume 34, 8119–8132. Curran Associates, Inc.\nPedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.;\nThirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.;\nWeiss, R.; Dubourg, V.; Vanderplas, J.; Passos, A.; Cour-\nnapeau, D.; Brucher, M.; Perrot, M.; and Duchesnay, E.\n2011. Scikit-learn: Machine Learning in Python. Jour-\nnal of Machine Learning Research, 12: 2825–2830.\nPeng, J.; Zou, H.; Liu, J.; Li, S.; Jiang, Y.; Pei, J.; and\nCui, P. 2023. Offline Policy Evaluation in Large Action\nSpaces via Outcome-Oriented Action Grouping. In Pro-\nceedings of the ACM Web Conference 2023, WWW ’23,\n1220–1230. New York, NY, USA: Association for Com-\nputing Machinery. ISBN 9781450394161.\nSachdeva, N.; Su, Y.-H.; and Joachims, T. 2020. Off-policy\nBandits with Deficient Support. Proceedings of the 26th\nACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining.\nSachdeva, N.; Wang, L.; Liang, D.; Kallus, N.; and\nMcAuley, J. 2023. Off-Policy Evaluation for Large Ac-\ntion Spaces via Policy Convolution. arXiv:2310.15433.\nSaito, Y.; Aihara, S.; Matsutani, M.; and Narita, Y. 2020.\nOpen Bandit Dataset and Pipeline: Towards Realistic\nand Reproducible Off-Policy Evaluation.\nIn NeurIPS\nDatasets and Benchmarks.\nSaito, Y.; and Joachims, T. 2022. Off-Policy Evaluation for\nLarge Action Spaces via Embeddings. In International\nConference on Machine Learning.\nSaito, Y.; Ren, Q.; and Joachims, T. 2023.\nOff-Policy\nEvaluation for Large Action Spaces via Conjunct Effect\nModeling. In Krause, A.; Brunskill, E.; Cho, K.; Engel-\nhardt, B.; Sabato, S.; and Scarlett, J., eds., Proceedings\nof the 40th International Conference on Machine Learn-\ning, volume 202 of Proceedings of Machine Learning\nResearch, 29734–29759. PMLR.\nSaito, Y.; Udagawa, T.; Kiyohara, H.; Mogi, K.; Narita,\nY.; and Tateno, K. 2021.\nEvaluating the Robustness\nof Off-Policy Evaluation.\nIn Proceedings of the 15th\nACM Conference on Recommender Systems, RecSys\n’21, 114–123. New York, NY, USA: Association for\nComputing Machinery. ISBN 9781450384582.\nSculley, D. 2010.\nWeb-Scale k-Means Clustering.\nIn\nProceedings of the 19th International Conference on\nWorld Wide Web, WWW ’10, 1177–1178. New York,\nNY, USA: Association for Computing Machinery. ISBN\n9781605587998.\nShi, J.; and Malik, J. 2000. Normalized cuts and image\nsegmentation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 22(8): 888–905.\nShimizu, T.; and Forastiere, L. 2023.\nDoubly Robust\nEstimator for Off-Policy Evaluation with Large Action\nSpaces. arXiv:2308.03443.\nSu, Y.; Dimakopoulou, M.; Krishnamurthy, A.; and Dudik,\nM. 2020.\nDoubly robust off-policy evaluation with\nshrinkage. In III, H. D.; and Singh, A., eds., Proceedings\nof the 37th International Conference on Machine Learn-\ning, volume 119 of Proceedings of Machine Learning\nResearch, 9167–9176. PMLR.\nSu, Y.; Srinath, P.; and Krishnamurthy, A. 2020. Adaptive\nEstimator Selection for Off-Policy Evaluation.\nIn III,\nH. D.; and Singh, A., eds., Proceedings of the 37th Inter-\nnational Conference on Machine Learning, volume 119\nof Proceedings of Machine Learning Research, 9196–\n9205. PMLR.\nSwaminathan, A.; and Joachims, T. 2015a.\nCounterfac-\ntual Risk Minimization: Learning from Logged Bandit\nFeedback. In Bach, F.; and Blei, D., eds., Proceedings\nof the 32nd International Conference on Machine Learn-\ning, volume 37 of Proceedings of Machine Learning Re-\nsearch, 814–823. Lille, France: PMLR.\nSwaminathan, A.; and Joachims, T. 2015b.\nThe Self-\nNormalized Estimator for Counterfactual Learning. In\nAdvances in Neural Information Processing Systems, 28.\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\nTaufiq, M. F.; Doucet, A.; Cornish, R.; and Ton, J.-F.\n2023. Marginal Density Ratio for Off-Policy Evaluation\nin Contextual Bandits. In Thirty-seventh Conference on\nNeural Information Processing Systems.\nThomas, P. S.; and Brunskill, E. 2016. Data-efficient off-\npolicy policy evaluation for reinforcement learning. In\nProceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning - Volume\n48, ICML’16, 2139–2148. JMLR.org.\nThompson, W. R. 1933. On the Likelihood that One Un-\nknown Probability Exceeds Another in View of the Evi-\ndence of Two Samples. Biometrika, 25: 285–294.\nThompson, W. R. 1935. On the Theory of Apportionment.\nAmerican Journal of Mathematics, 57(2): 450–456.\nTuyl, F.; Gerlach, R.; and Mengersen, K. 2008. A Compari-\nson of Bayes–Laplace, Jeffreys, and Other Priors. Amer-\nican Statistician - AMER STATIST, 62: 40–44.\nUdagawa, T.; Kiyohara, H.; Narita, Y.; Saito, Y.; and\nTateno, K. 2023. Policy-Adaptive Estimator Selection\nfor Off-Policy Evaluation. Proceedings of the AAAI Con-\nference on Artificial Intelligence, 37: 10025–10033.\nVaratharajah, Y.; and Berry, B. 2022. A Contextual-Bandit-\nBased Approach for Informed Decision-Making in Clin-\nical Trials. Life, 12(8).\nVoloshin, C.; Le, H.; Jiang, N.; and Yue, Y. 2021. Em-\npirical Study of Off-Policy Policy Evaluation for Rein-\nforcement Learning. In Vanschoren, J.; and Yeung, S.,\neds., Proceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks, volume 1.\nCurran.\nWang, Y.-X.; Agarwal, A.; and Dud´ık, M. 2017.\nOpti-\nmal and Adaptive Off-Policy Evaluation in Contextual\nBandits. In Proceedings of the 34th International Con-\nference on Machine Learning - Volume 70, ICML’17,\n3589–3597. JMLR.org.\nWard, J. H. 1963. Hierarchical Grouping to Optimize an\nObjective Function. Journal of the American Statistical\nAssociation, 58(301): 236–244.\nZhan, R.; Hadad, V.; Hirshberg, D. A.; and Athey, S. 2021.\nOff-Policy Evaluation via Adaptive Weighting with Data\nfrom Contextual Bandits. Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery & Data\nMining.\nZhang, T.; Ramakrishnan, R.; and Livny, M. 1996. BIRCH:\nAn Efficient Data Clustering Method for Very Large\nDatabases. In Proceedings of the 1996 ACM SIGMOD\nInternational Conference on Management of Data, SIG-\nMOD ’96, 103–114. New York, NY, USA: Association\nfor Computing Machinery. ISBN 0897917944.\nChecklist\n1. For all models and algorithms presented, check if you\ninclude:\n(a) A clear description of the mathematical setting,\nassumptions, algorithm, and/or model.\n[Yes]. Justification: The mathematical setting of\nboth synthetic and real experiments is detailed in\nSection 4, the theoretical assumptions, its analy-\nsis when they don’t hold, and the main algorithm\nare detailed in Section 3.\n(b) An analysis of the properties and complexity\n(time, space, sample size) of any algorithm.\n[Yes]. Justification: A complete analysis on the\ncomplexity of the method can be found in Ap-\npendix G, while the analysis of properties can be\nfound in Section 3.\n(c) (Optional) Anonymized source code, with spec-\nification of all dependencies, including external\nlibraries. [Yes/No/Not Applicable]\n[Yes]. Justification: The code, an explanation\non how to execute it and a Poetry environment to\ntake care of the dependencies are included in the\nsupplemental materials.\n2. For any theoretical claim, check if you include:\n(a) Statements of the full set of assumptions of all\ntheoretical results.\n[Yes]. Justification: The full set of assumptions,\nrelaxation of them and all the theoretical analysis\nof the method can be found in Section 3.\n(b) Complete proofs of all theoretical results.\n[Yes]. Justification: For every proposition in the\ntheoretical analysis (Section 3), the proof is ref-\nerenced and can be found in Appendix A.\n(c) Clear explanations of any assumptions.\n[Yes]. Justification: For every proposition in the\ntheoretical analysis (Section 3), the assumptions\nused for the results are detailed in the introduc-\ntion of the proposition.\n3. For all figures and tables that present empirical results,\ncheck if you include:\n(a) The code, data, and instructions needed to repro-\nduce the main experimental results (either in the\nsupplemental material or as a URL).\n[Yes]. Justification: The code contains the nec-\nessary scripts to reproduce every experiment in\nthe paper, as well as instructions on how to exe-\ncute it.\n(b) All the training details (e.g., data splits, hyperpa-\nrameters, how they were chosen).\n\n\nClustering Context in Off-Policy Evaluation\n[Yes]. Justification: The training details and pa-\nrameter variations can be found in Section 4. Ex-\ntra experiments varying more parameters, 2 pa-\nrameters at the same time or different clustering\noptions can be found in Appendix D. Addition-\nally, a table with the base value of every parame-\nter can be found in Appendix C.\n(c) A clear definition of the specific measure or\nstatistics and error bars (e.g., with respect to the\nrandom seed after running experiments multiple\ntimes).\n[Yes]. Justification: All deviation bars obtained\nafter 100 runs with different seeds for every pa-\nrameter are included in the result figures as ex-\nplained in Section 4.\n(d) A description of the computing infrastructure\nused.\n(e.g., type of GPUs, internal cluster, or\ncloud provider).\n[Yes]. Justification: The table with the specifi-\ncations of the computing infrastructure used for\nthe experiments can be found in Appendix C.\n4. If you are using existing assets (e.g., code, data, mod-\nels) or curating/releasing new assets, check if you in-\nclude:\n(a) Citations of the creator If your work uses existing\nassets.\n[Yes]. Justification: The evaluation pipeline that\nwe use for the real experiments, the systems that\nwe compare our method with, and the code im-\nplementations of known algorithms for different\nlibraries that we use are referenced in the paper.\n(b) The license information of the assets, if applica-\nble.\n[Yes]. Justification: The license included in the\ncode does not conflict with the license of the used\nresources.\n(c) New assets either in the supplemental material or\nas a URL, if applicable.\n[Yes].\nJustification: We provide all the code\nwith our pipeline and implementations for com-\nmon ope algorithms in the supplemental mate-\nrial.\n(d) Information\nabout\nconsent\nfrom\ndata\nproviders/curators.\n[Yes]. Justification: The code includes a licence\nto use.\n(e) Discussion of sensible content if applicable, e.g.,\npersonally identifiable information or offensive\ncontent.\n[Not Applicable]. Justification: There is not\npersonally identifiable information or offensive\ncontent of any kind in our data or experiments.\n5. If you used crowdsourcing or conducted research with\nhuman subjects, check if you include:\n(a) The full text of instructions given to participants\nand screenshots.\n[Not Applicable]. Justification: Our research\ndid not use crowdsourcing or was conducted with\nhuman subjects.\n(b) Descriptions of potential participant risks, with\nlinks to Institutional Review Board (IRB) ap-\nprovals if applicable.\n[Not Applicable]. Justification: Our research\ndid not use crowdsourcing or was conducted with\nhuman subjects.\n(c) The estimated hourly wage paid to participants\nand the total amount spent on participant com-\npensation.\n[Not Applicable]. Justification: Our research\ndid not use crowdsourcing or was conducted with\nhuman subjects.\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\nA\nTHEORETICAL RESULTS PROOFS\nA.1\nProposition 3.3\nGiven a policy π, if both Assumption 3.1 and 3.2 hold, from the refinement of the policy value definition in a cluster-based\nbandits process (introduced in Section 3), we have that:\nV (π) ∶= Ep(x)p(c∣x)π(a∣x)p(r∣a,c,x) [r]\n= Ep(c)p(x∣c)π(a∣x) [q(a,c,x)]\n(8)\n= Ep(c) [∫X p(x∣c) ∑\na∈A\np(a∣c)q(a,c)dx]\n(9)\n= Ep(c) [∫X ∑\na∈A\np(x∣c)π(a∣x)q(a,c)dx]\n= Ep(c) [ ∑\na∈A\nq(a,c)∫X p(x∣c)π(a∣x)dx]\n= Ep(c) [ ∑\na∈A\np(a∣c,π)q(a,c)]\n(10)\n= Ep(c)p(a∣c,π) [q(a,c)]\nWhere in Equation 8 we used the Bayes Theorem, in Equation 9 the fact that under Assumption 3.2 q(a,c,x) = q(a,c),\nand the definition of p(a∣c,π) in Equation 10.\nA.2\nProposition 3.4\nGiven a policy π and under Assumptions 3.1 and 3.2 we have that:\nED [ ˆVCHIPS(π;D)] = ED [w(a,c)r]\n(11)\n= Ep(x)p(c∣x)π0(a∣x)p(r∣a,c,x) [w(a,c)r]\n= Ep(c)p(x∣c)π0(a∣x) [w(a,c)q(a,c)]\n(12)\n= Ep(c) [∫X p(x∣c) ∑\na∈A\nπ0(a∣x)w(a,c)q(a,c)dx]\n= Ep(c) [∫X ∑\na∈A\np(x∣c)π0(a∣x)w(a,c)q(a,c)dx]\n= Ep(c) [ ∑\na∈A\nw(a,c)q(a,c)(∫X p(x∣c)π0(a∣x)dx)]\n= Ep(c) [ ∑\na∈A\np(a∣c,π)\n\u0018\u0018\u0018\u0018\u0018\np(a∣c,π0) q(a,c)\u0018\u0018\u0018\u0018\u0018\np(a∣c,π0)]\n(13)\n= Ep(c)p(a∣c,π) [q(a,c)]\n= Ep(x)p(c∣x)π(a∣x)p(r∣a,c,x) [r]\n(14)\n= V (π)\nIn Equation 11, we have used the linearity of expectation, in Equation 12 the definition of q(a,c,x) and Assumption\n3.2. Equation 13 is just using the definition of p(a∣c,π) while Equation 14 is a combination of Proposition 3.3 and the\nequivalence q(a,c) = q(a,c,x) under the given assumptions.\nA.3\nProposition 3.6\nGiven the logging data D = {(xi,ai,ri)}, a logging policy π0, and an evaluation policy π having common cluster support\nover it, we have that:\nBias( ˆVCHIPS(V ;D)) = ED [w(c,a)r] −V (π)\n\n\nClustering Context in Off-Policy Evaluation\n= Ep(x)p(c∣x)π0(a∣x)p(r∣a,c,x) [w(a,c)r] −V (π)\n= Ep(x)p(c∣x)π0(a∣x) [w(a,c)q(a,c,x)] −V (π)\n= Ep(x)p(c∣x)π0(a∣x) [w(a,c)q(a,c,x)] −Ep(x)p(c∣x)π(a∣x) [q(a,c,x)]\n= Ep(c)p(x∣c)π0(a∣x) [w(a,c)q(a,c,x)] −Ep(c)p(x∣c)π(a∣x) [q(a,c,x)]\n= Ep(c) [∫X p(x∣c) ∑\na∈A\nπ0(a∣x)w(c,a)q(a,c,x)dx]\n−Ep(c) [∫X p(x∣c) ∑\na∈A\nπ(a∣x)q(a,c,x)dx]\n(15)\nUnder Assumption 3.5 we have that δ−\nπ,c,a ≤π(a∣c,x)\np(a∣c,π) ≤δ+\nπ,c,a,\nδ−\nπ0,c,a ≤π0(a∣c,x)\np(a∣c,π0) ≤δ+\nπ0,c,a\n∀x ∈X given an action\na ∈A and a context c ∈C. We denote then δ+\nc,a = max{δ+\nπ,c,a,δ+\nπ0,c,a},δ−\nc,a = min{δ−\nπ,c,a,δ−\nπ0,c,a},∆a,c = δ+\nc,a −δ−\nc,a, and\nwe can give an upper bound as follows:\nEp(c) [∫X p(x∣c) ∑\na∈A\nπ0(a∣x)w(c,a)q(a,c,x)dx]\n(16)\n−Ep(c) [∫X p(x∣c) ∑\na∈A\nπ(a∣x)q(a,c,x)dx]\n≤Ep(c) [∫X p(x∣c) ∑\na∈A\nδ+\nc,ap(a∣c,π0)w(c,a)q(a,c,x)dx]\n(17)\n−Ep(c) [∫X p(x∣c) ∑\na∈A\nδ−\nc,ap(a∣c,π)q(a,c,x)dx]\n= Ep(c) [ ∑\na∈A\np(a∣c,π)∫X p(x∣c)δ+\nc,a\np(a∣c,π)\n\u0018\u0018\u0018\u0018\u0018\np(a∣c,π0) \u0018\u0018\u0018\u0018\u0018\np(a∣c,π0)q(a,c,x)dx]\n−Ep(c) [ ∑\na∈A\np(a∣c,π)∫X p(x∣c)δ−\nc,aq(a,c,x)dx]\n= Ep(c)p(a∣c,π) [δ+\nc,a ∫X p(x∣c)q(a,c,x)dx] −Ep(c)p(a∣c,π) [δ−\nc,a ∫X p(x∣c)q(a,c,x)dx]\n= Ep(c)p(a∣c,π) [Ep(x∣c) [q(a,c,x)](δ+\nc,a −δ−\nc,a)]\n= Ep(c)p(a∣c,π) [Ep(x∣c) [q(a,c,x)]∆a,c]\nNote that in Equation 17 we can follow an analogous path to establish a lower bound:\nEp(c) [∫X p(x∣c) ∑\na∈A\nπ0(a∣x)w(c,a)q(a,c,x)dx] −Ep(c) [∫X p(x∣c) ∑\na∈A\nπ(a∣x)q(a,c,x)dx]\n≥Ep(c) [∫X p(x∣c) ∑\na∈A\nδ−\nc,ap(a∣c,π0)w(c,a)q(a,c,x)dx]\n−Ep(c) [∫X p(x∣c) ∑\na∈A\nδ+\nc,ap(a∣c,π)q(a,c,x)dx]\n= −Ep(c)p(a∣c,π) [Ep(x∣c) [q(a,c,x)]∆a,c]\nFrom which we have:\n∣Bias( ˆVCHIPS(π;D))∣≤∣Ep(c)p(x∣c)p(a∣c,π) [q(a,c,x) ⋅∆a,c]∣\nA.4\nProposition 3.7\nSince the observations are independent we have that\nN (MSE( ˆVIPS (π)) −MSE( ˆVCHIPS(π))\n= Vx,a,r[ω(x,a)r] −Vc,a,r[ω(a,c)r] −N Bias( ˆVCHIPS(π))\n2\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\nWe now analyze the difference in variance:\nVp(c)p(x∣c)π0(a∣x)p(r∣a,c,x)[ω(x,a)r] −Vp(c)p(x∣c)π0(a∣x)p(r∣a,c,x)[ω(a,c)r]\n= Ep(c∣p(x∣c)π0(a∣x)p(r∣a,c,x) [ω(x,a)r2] −V (π)2\n−(Ep(c)p(x∣c)πb(a∣x)p(r∣a,c,x) [ω(a,c)2 ⋅r2] −(V (π) + Bias( ˆVCHIPS(π)))\n2\n= Ep(c)p(x∣c)π0(a∣x) [(ω(x,a)2 −ω(a,c)2)Ep(r∣a,c,x) [r2]]\n+ 2V (π)Bias( ˆVCHIPS(π)) + Bias( ˆVCHIPS(π))\n2\nThis implies that\nN (MSE( ˆVIPS (π)) −MSE( ˆVCHIPS(π))\n= Ep(c)p(x∣c)π0(a∣x) [(ω(x,a)2 −ω(a,c)2)Ep(r∣a,c,x) [r2]]\n+ 2V (π)Bias( ˆVCHIPS(π)) + (1 −N)Bias( ˆVCHIPS(π))\n2\nA.5\nProposition 3.8\nGiven the logging policy π0 and some evaluation policy π, the absolute bias of the CHIPS estimator when Assumption 3.2,\nwe have that:\nBias( ˆVCHIPS(V ;D)) = ED [w(c,a)r] −V (π)\n= Ep(x)p(c∣x)π0(a∣x)p(r∣a,c,x) [w(a,c)r] −V (π)\n= Ep(c)p(x∣c)π0(a∣x) [w(a,c)q(a,c)] −Ep(c)p(x∣c)π(a∣x) [w(a,c)q(a,c)]\n= Ep(c)p(x∣c)π0(a∣x) [w(a,c)q(a,c)] −Ep(c)p(x∣c)π(a∣x) [w(a,c)q(a,c)]\n= Ep(c) [∫X p(x∣c) ∑\na∈A\nπ0(a∣x)w(c,a)q(a,c)dx]\n−Ep(c) [∫X p(x∣c) ∑\na∈A\nπ(a∣x)q(a,c)dx]\n= Ep(c) [ ∑\na∈A\nw(c,a)q(a,c)∫X p(x∣c)π0(a∣x)dx]\n−Ep(c) [ ∑\na∈A\nq(a,c)∫X p(x∣c)π(a∣x)dx]\n= Ep(c) [ ∑\na∈A\nw(c,a)q(a,c)p(a∣c,π0)] −Ep(c) [ ∑\na∈A\nq(a,c)p(a∣c,π)]\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π0)c\nw(c,a)q(a,c)p(a∣c,π0)\n⎤⎥⎥⎥⎥⎦\n−Ep(c) [ ∑\na∈A\nq(a,c)p(a∣c,π)]\n(18)\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π0)c\np(a∣c,π0)\n\u0018\u0018\u0018\u0018\u0018\np(a∣c,π0) \u0018\u0018\u0018\u0018\u0018\np(a∣c,π0)q(a,c)\n⎤⎥⎥⎥⎥⎦\n−Ep(c) [ ∑\na∈A\nq(a,c)p(a∣c,π)]\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n−\n∑\na∈U(c,π0)\np(a∣c,π)q(a,c)\n⎤⎥⎥⎥⎥⎦\nWhere in Equation 18 we note that p(a∣c,π0) = 0 if a ∈U(c,π0). Following an analogous procedure we can give an\nexpression for the bias of IPS in a cluster bandits setup:\nBias( ˆVIPS(V ;D)) = ED [w(a,x)r] −V (π)\n= Ep(x)p(c∣x)π0(a∣x)p(r∣a,c,x) [w(a,x)r] −V (π)\n= Ep(c)p(x∣c)π0(a∣x) [w(a,x)q(a,c)] −Ep(c)p(x∣c)π(a∣x) [q(a,c)]\n\n\nClustering Context in Off-Policy Evaluation\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∫X p(x∣c)\n∑\na∈U(c,x,π0)c\nπ0(a∣x)w(a,x)q(a,c)dx\n⎤⎥⎥⎥⎥⎦\n−Ep(c) [∫X p(x∣c) ∑\na∈A\nπ(a∣x)q(a,c)dx]\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,x,π0)c\nq(a,c)∫X p(x∣c) π(a∣x)\n\u0018\u0018\u0018\u0018\nπ0(a∣x) \u0018\u0018\u0018\u0018\nπ0(a∣x)dx\n⎤⎥⎥⎥⎥⎦\n−Ep(c) [ ∑\na∈A\nq(a,c)∫X p(x∣c)π(a∣x)dx]\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,x,π0)c\nq(a,c)p(a∣c,π)\n⎤⎥⎥⎥⎥⎦\n−Ep(c) [ ∑\na∈A\nq(a,c)p(a∣c,π)]\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n−\n∑\na∈U(c,x,π0)\np(a∣c,π)q(a,c)\n⎤⎥⎥⎥⎥⎦\nSince q(a,c) ≥0 in the binary reward setting, it follows that ∣Bias( ˆVCHIPS(π;D))∣= Ep(c) [∑U(c,π0) p(a∣π,c)q(a,c)] and\n∣Bias( ˆVIPS(π;D))∣= Ep(c) [∑U(c,x,π0) p(a∣π,c)q(a,c)] and consequently we have that:\n∣Bias( ˆVIPS(π;D))∣−∣Bias( ˆVCHIPS(π;D))∣= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\nU(c,x,π0)\np(a∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\n−Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\nU(c,π0)\np(a∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\nU(c,x,π0)∖U(c,π0)\np(a∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\nA.6\nProposition 3.10\nAssuming that we have a set of embeddings e ∈E ⊂Rde associated with the actions a ∈A and an approximation fϕ∗(r) to\nthe importance weights w(a,x):\nfϕ∗(r) ∶=argminEϕ [(w(a,x) −fϕ(r))2]\nfϕ ∈{fϕ ∶R →R ∣ϕ ∈Φ}\n(19)\nThen if we assume that fϕ∗(r) = w(a,x) + ϵ for some ϵ ∈R we have that\n∣Bias( ˆVMR;D)∣−∣Bias( ˆVCHIPS ;D)∣\n= −Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π0)\np(a ∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\n+ Bias( ˆVIPS ;D) + ED [fϕ∗(r)r] −ED(w(a,x)]\n= −Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π)\np(a ∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\n−V (π) + ED[fϕ∗(r)r]\n= −Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π0)\np(a ∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\n−V (π) + ED[w(a,x)r] + ϵED[r]\n= −Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π)\np(a ∣π,c)q(a,c)\n⎤⎥⎥⎥⎥⎦\n+ Ep(c)\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∑\na∈U(x,c,π0)\nq(a,c)∫x∈x p(x ∣c)π(a ∣x)dx\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\np(a∣π,c)\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n−Ep(c)\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∑\na∈A\nq(a,c)∫x∈x p(x ∣c)π(a ∣x)dx\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\np(a∣π,c)\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\n+ εED[r]\n= Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(x,c,π0)∖U(c,π0)\nq(a,c)p(a ∣π,c)\n⎤⎥⎥⎥⎥⎦\n−Ep(c) [ ∑\na∈A\nq(a,c)p(a ∣π,c)] + εED[r]\n= −Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈(U(x,c,π0)∖U(c,π0))c\nq(a,c)p(a ∣π,c)\n⎤⎥⎥⎥⎥⎦\n+ ϵEp(c) [ ∑\na∈A\nq(a,c)p(a ∣π0,c)]\nfor the MIPS case, we note that MIPS’ bias can also be expressed similarly to CHIPS’:\nBias( ˆVMIPS ;D) =\n= ED[w(x,e)r] −V (π)\n= Ep(x)π0(a∣x)p(e∣x,a)p(r∣x,a,e)[w(x,e)r] −V (π)\n= Ep(x) [ ∑\na∈A\nπ0(a ∣x) ∑\ne∈E\np(e ∣x,a)w(x,e)q(x,e)]\n−Ep(x) [ ∑\na∈A\nπ(a ∣x) ∑\ne∈E\np(e ∣x,a)w(x,e)q(x,e)]\n= Ep(x) [∑\ne∈E\nq(x,e)( ∑\na∈A\nπ0(a ∣x),p(e ∣x,a))]\n−Ep(x) [∑\ne∈E\nq(x,e)( ∑\na∈A\nπ(a ∣x)p(e ∣x,a))]\n= Ep(x)\n⎡⎢⎢⎢⎢⎣\n∑\ne∈U(e,π0)c p(e ∣x,π0)q(x,e) p(e ∣x,π)\np(e ∣x,π0)\n⎤⎥⎥⎥⎥⎦\n−Ep(x) [∑\ne∈E\nq(x,e)p(e ∣x,π)]\n= −Ep(x)\n⎡⎢⎢⎢⎢⎣\n∑\ne∈U(e,π0)\np(e ∣x,π)q(x,e)\n⎤⎥⎥⎥⎥⎦\nTherefore the difference in bias is:\n∣Bias( ˆVMIPS;D)∣−∣Bias( ˆVCHIPS ;D)∣\n= Ep(x)\n⎡⎢⎢⎢⎢⎣\n∑\ne∈U(e,π0)\np(e ∣x,π)q(x,e)\n⎤⎥⎥⎥⎥⎦\n−Ep(c)\n⎡⎢⎢⎢⎢⎣\n∑\na∈U(c,π0)\np(a ∣c,π)q(a,c)\n⎤⎥⎥⎥⎥⎦\nA.7\nProposition 3.11\nLemma A.1. Given a policy π, under Assumption 3.1 we have the transformation:\nw(a,c) = Eπ0(x∣a,c) [w(a,x)]\nProof:\nGiven a logging policy π0 and an evaluation policy π, in the cluster setting of the bandits problem we have that:\nw(a,c) = p(a∣π,c)\np(a∣π0,c)\n\n\nClustering Context in Off-Policy Evaluation\n= ∫X π(a∣x)p(x∣c)\np(a∣π0,c)\n(20)\n= \u0018\u0018\u0018\u0018\u0018\np(a∣c,π0) ∫X\nπ(a∣x)\nπ0(a∣x)π0(x∣a,c)\n\u0018\u0018\u0018\u0018\u0018\np(a∣π0,c)\n(21)\n= Eπ0(x∣a,c) [w(a,x)]\nWhere we have used the definition p(a∣π,c) = ∫X π(a∣x)p(x∣c) in Equation 20, and that π0(x∣a,c) = p(x∣c)π0(a∣x)\np(a∣c,π0)\nin\nEquation 21.\nGiven a logging policy π0 and an evaluation policy π, under Assumption 3.1 and Assumption 3.2 we have that\nN (VD [ ˆVIPS(π;D)] −VD [ ˆVCHIPS(π;D)])\n= N (VD [ 1\nN\nN\n∑\ni=1\nπ(ai∣xi)\nπ0(ai∣xi)ri] −VD [ 1\nN\nN\n∑\ni=1\np(ai∣ci,π)\np(ai∣ci,π0)ri])\n= VD [ π(a∣x)\nπ0(a∣x)r] −VD [ p(a∣c,π)\np(a∣c,π0)r]\n(22)\n=\n⎛\n⎜⎜⎜\n⎝\nED [w(a,x)2r2] −\u0018\u0018\u0018\u0018\u0018\u0018\u0018\n:0\nED [w(a,x)r]2\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nV (π)\n⎞\n⎟⎟⎟\n⎠\n−\n⎛\n⎜⎜⎜\n⎝\nED [w(a,c)2r2] −\u0018\u0018\u0018\u0018\u0018\u0018\u0018\n:0\nED [w(a,c)r]2\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nV (π)\n⎞\n⎟⎟⎟\n⎠\n(23)\n= Ep(x)p(c∣x)π0(a∣x) [w(a,x)2Ep(r∣a,c,x) [r2]] −Ep(x)p(c∣x)π0(a∣x) [w(a,c)2Ep(r∣a,c,x) [r2]]\n= Ep(x)p(c∣x)π0(a∣x) [(w(a,x)2 −w(a,c)2)Ep(r∣a,c,x) [r2]]\n= Ep(c)p(x∣c)π0(a∣x) [(w(a,x)2 −w(a,c)2)Ep(r∣a,c,x) [r2]]\n= Ep(c) [∫X p(x∣c) ∑\na∈A\nπ0(a∣x)(w(a,x)2 −w(a,c)2)Ep(r∣a,c,x) [r2]dx]\n= Ep(c) [ ∑\na∈A∫X p(x∣c)π0(a∣x)(w(a,x)2 −w(a,c)2)Ep(r∣a,c,x) [r2]dx]\n= Ep(c) [ ∑\na∈A∫X\nπ0(x∣a,c)p(a∣c,π0)\n\u0018\u0018\u0018\u0018\nπ0(a∣x)\n\u0018\u0018\u0018\u0018\nπ0(a∣x) (w(a,x)2 −w(a,c)2)Ep(r∣a,c,x) [r2]dx]\n(24)\n= Ep(c) [ ∑\na∈A\np(a∣c,π0)∫X π0(x∣a,c)(w(a,x)2 −w(a,c)2)Ep(r∣a,c,x [r2]dx]\n= Ep(c)p(a∣c,π0)\n⎡⎢⎢⎢⎢⎣\n⎛\n⎝∫X π0(x∣a,c)w(a,x)2dx −w(a,c)2\n\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\n:1\n∫X π0(x∣a,c)dx\n⎞\n⎠Ep(r∣a,c,x [r2]\n⎤⎥⎥⎥⎥⎦\n= Ep(c)p(a∣c,π0) [(Eπ0(x∣a,c) [w(a,x)2] −Eπ0(x∣a,c) [w(a,x)]2)Ep(r∣a,c,x [r2]]\n(25)\n= Ep(c)p(a∣c,π0) [Vπ0(x∣a,c) [w(a,x)]Ep(r∣a,c,x) [r2]] ≥0\nNote in Equation 22 we used that the samples in D are i.i.d, in particular the linearity of variance under this condition.\nThe cancellation of terms in Equation 23 results from IPS and CHIPS being unbiased under Assumptions 3.1 and 3.2. In\nEquation 24 we used that π0(x∣a,c) = p(x∣c)π0(a∣x)\np(a∣c,π0)\n, while Equation 25 uses Lemma A.1.\nA.8\nProposition 3.12\nThe first thing we need to note is that CHIPS and MIPS are in different spaces regarding the contextual bandits generating\nprocess. MIPS assumes the existence of an action embedding space e ∈E ⊆Rde and CHIPS assumes the existence of a\npartition of the context space C ∶= {Ci}K\ni=1 with Ci ⊂X and ci ∩cj = ∅. For joining this spaces, we assume that given\na policy π, at every iteration of the data generation process, apart from the classical context (x ∈X), action (a ∈A) and\nreward (r ∈[0,rmax] ⊂R), we observe a cluster c ∼p(c ∣x) and an action embedding e ∼p(e ∣a,c,x). Given a policy π\nthe policy value V (π) equation can be then refined to:\nV (π) ∶= Ep(x)p(c∣x)π(a∣x)p(e∣a,c,x)p(r∣e,a,c,x)[r]\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n= Ep(x)p(c∣x)π(a∣x)p(e∣a,c,x)q(e,a,c,x)\nHere q(e,a,c,x) ∶= Ep(r∣e,a,c,x)[r]. Note that as in MIPS and CHIPS case, the refinement does not contradict the classical\npolicy value definition.\nWe also need to refine p(a ∣c,π) (from CHIPS) and p(e ∣a,π) (from MIPS) in the joint space:\np(a ∣c,π) = ∑\ne∈ε∫χ p(e ∣a,c,x)p(x ∣c)π(a ∣x)\np(e ∣x,π) ∶= ∑\nc∈C\n∑\na∈A\np(e ∣a,c,x)p(c ∣x)π(a ∣x)\nIt is important to note that after joining the context space, to make a fair comparison between MIPS and CHIPS, there\nare some dependencies that we want to eliminate to prevent information from passing between variables that were not\noriginally in the definition of MIPS and CHIPS. In particular, for CHIPS, we eliminate the dependency of the cluster with\nrespect to the embedding given the context and the action (i.e., ce ∣(x,a)), and for MIPS, the dependency of the action\nwith respect to the cluster given the embedding and the context (i.e., ac ∣(x,e)). From Proposition 3.11 we know that\nVD [ ˆVIPS(π;D)] ≥VD [ ˆVCHIPS(π;D)] and from MIPS Theorem 3.6 we know that VD [ ˆVIPS(π;D)] ≥VD [ ˆVMIPS(π;D)].\nTherefore, we need to make a comparison between VD [ ˆVMIPS(π;D)] and VD [ ˆVCHIPS(π;D)].\nTo follow the structure of Proposition 3.11, we are going to assume that Assumptions 3.1 and 3.2 hold as well as their\ncounterparts from MIPS. The following identities hold under these conditions:\np(x ∣c)π(a ∣x) = p(e ∣x,π)p(c ∣x,a)π0(a ∣x)p(x)\np(e ∣x,π0)p(c)\np(e ∣x,a,c) = p(x ∣e,a,c)p(e ∣a,c)p(a ∣c,π0)\np(c ∣x,a)π0(a ∣x)p(x)\nNow, under these conditions, we need a relation between the weights of MIPS and CHIPS:\nω(a,c)2 = p(a ∣c,π)\np(a ∣c,π0)\n= ∫X p(x ∣c)∑e∈E π(a ∣x)p(e ∣c,a,x)\np(a ∣c,π0)\n= ∫x ∑e∈E w(e,x)p(x ∣e,a,c)p(e∣a,c)p(a ∣c,π0)\np(a ∣c,π0)\n= ∑\ne∈ε\np(e∣a,c)∫x p(x ∣e,a,c)ω(e,x)\n= Ep(e∣a,c)p(x∣e,a,c)[ω(e,x)]\nTherefore the scaled difference in variance can be expressed as:\nN (VD [ ˆVMIPS(π;D)] −VD [ ˆVCHIPS(π;D)])\n= N (VD [ 1\nN\nN\n∑\ni=1\nπ (ei ∣xi)\nπ0 (ei ∣xi)ri] −VD [ 1\nN\nN\n∑\ni=1\np(ai ∣ci,π)\np(ai ∣ci,π0)ri])\n= VD [ π(e ∣x)\nπ0(e ∣x)r] −VD [ p(a ∣c,π)\np(a ∣c,π0)r]\n= (ED [ω(e,x)2r2] −ED[ω(e,x)r]2\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nV (π)\n) −(ED [ω(a,c)2r2] −ED[ω(a,c)r]2\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nV (π)\n)\n= Ep(c) [∫X p(x ∣c) ∑\na∈A\nπ0(a ∣x) ∑\ne∈E\np(e ∣a,c,x)(ω(e,x)2 −ω(a,c)2)Ep(r∣a,c) [r2]dx]\n\n\nClustering Context in Off-Policy Evaluation\n= Ep(c) [ ∑\na∈A\np(a ∣c,π0) ∑\ne∈E\np(e ∣a,c)∫X p(x ∣e,a,c)(ω(e,x)2 −ω2(a,c))Ep(r∣a,c) [r2]dx]\n= Ep(c)p(a∣c,π0) [Ep(r∣a,c) [r2](Ep(e∣a,c)p(x∣e,a,c)2 [ω(e,x)2])]\n−Ep(c)p(a∣c,π0) [Ep(r∣a,c) [r2](ω(a,c)2 ∑\ne∈E\np(e ∣a,c)∫X p(x ∣e,a,c)dx)]\n= Ep(c)p(a∣c,π0) [Ep(r∣a,c) [r2](Ep(e∣a,c)p(x∣e,a,c) [ω(e,x)2] −Ep(e∣a,c)p(x∣e,a,c) [ω(e,x)]2 )]\n= Ep(c)p(a∣c,π0) [Ep(r∣a,c) [r2]Vp(e∣a,c)p(x∣e,a,c)[ω(e,x)]] ≥0\nThis implies that under Assumptions 3.1 and 3.2 (and their counterparts in MIPS), the variance of CHIPS is lower than the\nvariance of MIPS, proving the proposition.\nB\nREWARD ESTIMATES DERIVATION\nB.1\nMAP\nFrom the setting in Subsection 3.2 we denote Rc ∶= {ri}M\ni=1 as the rewards observed in cluster c from the logging data2.\nWe consider Rc as independent trials of a Bernoulli random variable with parameter θ (i.e., Rc\ni.i.d.\n∼\nBer(θ)). Therefore,\nwe have that the likelihood can be expressed as:\np(Rc∣θ) =\nM\n∏\ni=1\np(ri∣θ)\n=\nM\n∏\ni=1\nθri(1 −θ)1−ri\n= θ∑M\ni=1 ri(1 −θ)M−∑M\ni=1\nUsing a Beta distribution as a prior we have that:\np(θ) = Beta(θ∣α, ˆβ) =\n1\nB(α, ˆβ)\nθα−1(1 −θ)\nˆβ−1\nWhere B(α, ˆβ) = Γ(α)Γ( ˆβ)\nΓ(α+ ˆβ) and Γ(⋅) is the Gamma function. The posterior probability can then be expressed as:\np(θ∣Rc) ∝p(Rc∣θ)p(θ)\n∝θ∑M\ni=1 ri(1 −θ)M−∑M\ni=1\n1\nB(α, ˆβ)\nθα−1(1 −θ)\nˆβ−1\n∝θα−1+∑M\ni=1 ri(1 −θ)\nˆβ−1+M−∑M\ni=1 ri\n∝Beta(θ ∣α +\nM\n∑\ni=1\nri, ˆβ + M −\nM\n∑\ni=1\nri)\nThe MAP estimator of θ is the mode of the resulting Beta distribution, i.e.\nˆθMAP = (α −1) + ∑M\ni=1 ri\nα + ˆβ + M −2\nB.2\nML\nUsing the same setting as in the previous section (Rc\ni.i.d.\n∼\nBer(θ)) we have that the maximum likelihood estimation can\nbe expressed as\nˆθML = arg max\nθ∈Θ\n{\nM\n∏\ni=1\nθri(1 −θ)1−ri}\n2Here we refer to the already transformed version using clusters. See definition of τ in Subsection 3.2\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n= arg max\nθ∈Θ\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩\nlog(θ) ⋅\nM\n∑\ni=1\nri + log((1 −θ)) ⋅\nM\n∑\ni=1\n(1 −ri)\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nl(θ)\n⎫⎪⎪⎪⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎪⎪⎪⎭\nWe now search for local maxima by setting the differential to 0:\n∂l(θ)\n∂θ\n= 0 Ô⇒∑M\ni=1 ri\nθ\n+ ∑M\ni=1(1 −ri)\n(1 −θ)\n= 0\nÔ⇒\nM\n∑\ni=1\nri −θ\nM\n∑\ni=1\nri = θ\nM\n∑\ni=1\n(1 −ri)\nÔ⇒ˆθML = 1\nM\nM\n∑\ni=1\nri\nC\nEXPERIMENTAL PARAMETERS AND HARDWARE\nParameter\nValue\nDescription\ncexp\n10\nRadius of the n-dimensional ball for context space generation.\ncrad\n1\nCluster generation radius.\ndx\n2\nDimension of context vectors.\nxnum\n1.000\nNo. of different context vectors in the experiment.\nanum\n10\nNo. of actions in the experiment.\ncnum\n10\nNo. of clusters in the experiment.\nnsamples\n50.000\nNo. of logged samples to use in the experiment.\nempc num\n100\nNo. of clusters to use empirically by the clustering method.\nelen\n1.000.000\nNo. of samples extracted from the dataset for the evaluation policy\nblen\n1.000.000\nNo. of samples extracted from the dataset for the evaluation policy\nσ\n0.2\nContext-specific behaviour deviation from cluster behaviour.\nβ\n-1\nDeviation between evaluation and logging policies.\nα\n20\nParameter from beta distribution in Bayesian inference\nˆβ\n20\nParameter from beta distribution in Bayesian inference\nTable 1: Parameters used in the basic configuration for experiments for generation and estimation.\nCPU\nAMD Ryzen Threadripper PRO 3975WX\nRAM\n256 GB\nCores\n64\nGPU\n2x Nvidia A100 160GB\nTable 2: Specifications of the machine in which the experiments were executed.\nD\nADDITIONAL EXPERIMENTS\nD.1\nSynthetic Experiments\nNumber of actions. From the fixed basic configuration that uses 100 clusters for CHIPS’ estimates, we observe a progres-\nsive deterioration in the estimator capabilities when increasing the number of actions (see Figure 4). We theorize that this\nbehaviour might be a consequence of the violation of Assumption 3.1 when trying to group contexts using an excessive\nnumber of clusters in a large action space, resulting in deficient actions inside the clusters. This problem can be mitigated\nby decreasing the number of clusters used in the clustering method for the CHIPS estimation (see Figure 12 (left)).\nNumber of samples. We observe an approximation to the performance of IPS as we increase the number of samples\nin the logged data that we identify as an effect of reducing the number of observed deficient action-context pairs in IPS,\n\n\nClustering Context in Off-Policy Evaluation\nconverging to an unbiased estimator under Assumption 2.1 (see Figure 1 (right)). In this case, the clustering effects under\nCHIPS become less noticeable according to Corollary 3.9 since U(c,x,π0) ∖U(c,π0) →∅. It is worth mentioning that\nincreasing the number of clusters when enough samples are available, as well as reducing it in the opposite case, can\nimprove the performance of the CHIPS estimates, as shown in Figure 12 (right).\nCluster radius. Increasing the cluster radius in the generation process affects the separability of the cluster space and\ncomplicates the partitioning in clusters complying with Assumption 3.2. In this case, we could find significant differences\nin context behaviour for both actions and rewards within a cluster, resulting in increased bias from the empirical approx-\nimations. Therefore, we observe a convergence to IPS’ performance as cluster radius increases since the context space\nbecomes less separable (see Figure 6).\nSigma. Increasing context-specific noise in the generation process produces a similar effect as in the cluster radius case. In\nparticular, the larger the noise, the more common it is to observe inconsistent behaviour in actions and rewards for contexts\nwithin a cluster, complicating the approximation of a homogeneous cluster-wise behaviour and resulting again in a bias\nincrease (see Figure 8).\nAlpha (prior). In this experiment, we vary the alpha parameter of the Beta prior maintaining all other settings fixed.\nLike in the number of clusters case, we observe a similar v-shaped graph indicating that, as expected from the previous\nβ analysis (see Section 4.1.1), the CHIPS (MAP) estimator is sensitive to the prior. In particular, lower values push the\nexpected reward of each cluster to the ML’s estimate, while higher values push it to the prior’s expected value, decreasing\nperformance in both cases (see Figure 1 center). For different values of distributional shift (β), the optimal value will\ndepend on the resistance MAP offers to converge to the ML estimate, favouring lower values as β becomes larger (see\nFigure 13).\nClustering Method. In this experiment, we evaluate the performance of the CHIPS (MAP) estimator using different\nclustering methods while varying the clustering radius in the synthetic generation process. In Figure 10, we observe that\nusing Mean Shift (Comaniciu and Meer, 2002) or Bayesian Gaussian Mixture (Bishop, 2006; Attias, 1999; Blei and Jordan,\n2006) fails to separate the context space resulting in the same performance as IPS. DBSCAN (Ester et al., 1996) mitigates\nIPS’ increase in mean squared error when the context space is easier to separate (i.e., lower radii values) but converges to\nIPS when the context is complicated to separate (i.e., higher radii values). Affinity Propagation (Frey and Dueck, 2007)\nfollows a similar behaviour to DBSCAN but still offers some improvement with respect to IPS when the space is difficult to\nseparate. OPTICS (Ankerst et al., 1999) makes a general improvement to the Affinity Propagation performance, especially\nnoticeable when the context space is separable. MiniBatch K-Means (Sculley, 2010), Gaussian Mixture (Everitt, 1996),\nBirch (Zhang, Ramakrishnan, and Livny, 1996), Spectral Clustering (Shi and Malik, 2000), and Agglomerative Clustering\n(Ward, 1963) have similar performance, outperforming Affinity Propagation for the separable case. We also note a general\nupward tendency in mean squared error for every clustering method as the space becomes more complicated to separate.\n100\n101\n102\n103\nNº Clusters\n1.7e-04\n8.0e-04\n3.6e-03\n1.7e-02\n7.6e-02\nMSE\n100\n101\n102\n103\nNº Clusters\n2.3e-04\n6.0e-04\n1.6e-03\n4.1e-03\n1.1e-02\nSquared Bias\n100\n101\n102\n103\nNº Clusters\n7.8e-10\n7.3e-08\n6.8e-06\n6.3e-04\n5.8e-02\nVariance\nDM\nIPS\nDR\nCHIPS_bayes\nCHIPS_mean\nMR\nSNDR\nDRoS\nSNIPS\nFigure 3: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying the\nnumber of clusters.\n3In this case we used a slightly different version of the configuration settings to make a more challenging environment in which we\nuse 10.000 samples and consequently reduce the number of empirical cluster estimation to 30 to easily assess the role that similarity of\nlogging and evaluation policies play in CHIPS capabilities.\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n101\n102\nNº Actions\n9.7e-03\n1.1e-01\n1.2e+00\n1.3e+01\n1.5e+02\nNormalized MSE\n101\n102\nNº Actions\n4.1e-02\n3.1e-01\n2.3e+00\n1.7e+01\n1.3e+02\nNormalized Squared Bias\n101\n102\nNº Actions\n2.4e-04\n4.0e-02\n6.7e+00\n1.1e+03\n1.9e+05\nNormalized Variance\nDM\nIPS\nDR\nCHIPS_bayes\nCHIPS_mean\nMR\nSNDR\nDRoS\nSNIPS\nFigure 4: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying the\nnumber of actions.\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n6.4e-05\n4.0e-04\n2.5e-03\n1.5e-02\n9.5e-02\nMSE\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n1.1e-04\n3.0e-04\n8.3e-04\n2.3e-03\n6.2e-03\nSquared Bias\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n3.7e-09\n2.4e-07\n1.6e-05\n1.0e-03\n6.8e-02\nVariance\nDM\nIPS\nDR\nCHIPS_bayes\nCHIPS_mean\nMR\nSNDR\nDRoS\nSNIPS\nFigure 5: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying β\nvalues.3\nD.1.1\nBi-parametric variations\nThe experiments varying single parameters described in the previous section indicate that increasing the number of actions\nin a fixed configuration progressively deteriorates CHIPS’ performance. This behaviour is expected since the larger the\naction space, the more likely it is to incur in a situation in which Assumption 3.1 does not hold with a fixed number of\nclusters. In this situation, we found that reducing the number of clusters can mitigate the performance decay by pooling\ninformation from broader contexts clusters while increasing it could be beneficial in reduced action spaces (see Figure 12\n(a)). Similarly, the number of samples from the logging policy also conditions how significant the performance gap\nbetween CHIPS and IPS is. In particular, the higher the number of samples, the more beneficial it is to use a higher number\nof clusters to try to obtain a more detailed partition structure of the context space, while a reduced number of clusters has\nan edge on few-sample cases (see Figure 12 (b)).\nWe also study the effect of varying the α parameter in CHIPS’ (MAP) Beta prior, using different values of the distributional\nshift between policies (β). In Figure 13, we observe that mid values of α (30-50) offer better performance when there is a\nconsiderable distributional shift between logging and evaluation policy (i.e., β ≈−1) since the expected reward per cluster\nis pushed towards the prior’s expectation, creating some resistance from converging to the average observed rewards\n(i.e., mitigating the reward misspecification existing under this conditions). As the distributional gap closes, lower values\nof α are more favourable since the samples observed per cluster are better representatives of the real expected reward.\nHowever, higher values for α (80-100) result in excessive resistance that deteriorates CHIPS’ performance. It is also worth\nmentioning that as the distributional gap closes, CHIPS (MAP) loses its advantage with respect to IPS since the logging\nand evaluation policies are closer, and the ML estimates would offer better results, as previously shown in Figure 1.\n\n\nClustering Context in Off-Policy Evaluation\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nCluster Ratio\n2.4e-04\n1.2e-03\n6.1e-03\n3.1e-02\n1.5e-01\nMSE\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nCluster Ratio\n3.4e-04\n1.1e-03\n3.4e-03\n1.1e-02\n3.4e-02\nSquared Bias\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nCluster Ratio\n3.9e-08\n1.5e-06\n5.6e-05\n2.1e-03\n7.9e-02\nVariance\nDM\nIPS\nDR\nCHIPS_bayes\nMR\nSNDR\nDRoS\nSNIPS\nFigure 6: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying the\nradius of the clusters generated.\n5.0 103 1.0 104 2.0 104 3.0 104 5.0 104 7.0 104 1.0 105 1.5 105\nNº Samples\n2.0e-02\n1.2e-01\n7.2e-01\n4.3e+00\n2.6e+01\nNormalized MSE\n5.0 103 1.0 104 2.0 104 3.0 104 5.0 104 7.0 104 1.0 105 1.5 105\nNº Samples\n4.2e-02\n1.6e-01\n6.1e-01\n2.3e+00\n8.8e+00\nNormalized Squared Bias\n5.0 103 1.0 104 2.0 104 3.0 104 5.0 104 7.0 104 1.0 105 1.5 105\nNº Samples\n2.6e-05\n1.0e-02\n3.8e+00\n1.4e+03\n5.5e+05\nNormalized Variance\nDM\nIPS\nDR\nCHIPS_bayes\nCHIPS_mean\nMR\nSNDR\nDRoS\nSNIPS\nFigure 7: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying the\nnumber of samples provided from the logging policy.\nD.2\nReal Experiments\nD.3\nMAP vs ML\nIn this section, we analyze the reason behind the jump in performance using the CHIPS estimator with the MAP estimate\nfor the expected reward per cluster. For this purpose, we have conducted two experiments, one in the synthetic dataset and\nthe other in the real dataset. For the synthetic experiment, given a distributional shift value β, we select the most relevant\ncontext-action pair (x∗,a∗) under the evaluation policy π (i.e., (x∗,a∗) = arg max(x,a)∈X×A π(a∣x)). Then we analyze the\nmean squared error of the expected reward (given x∗) estimations made by CHIPS (MAP) (i.e., w(a∗,c∗)ˆrbayes(a∗,c∗))\nand CHIPS (ML) (i.e., w(a∗,c∗)ˆrmean(a∗,c∗)) w.r.t IPS (where c∗is the cluster associated with x∗). We also compute\nthe number of observations in c∗in which action a∗was selected. This process is repeated 100 times with different\npolicies generated under different random seeds, and the results for the number of samples per cluster and squared errors\nare averaged. We repeat this for ten different values of β ranging from -1 to 1 and represent the moving averages for\nrelative squared errors and samples in Figure 16. We observe that the number of samples per cluster increases with β as\nboth policies become closer. This increase in the number of samples makes the ML estimates progressively more accurate\nsince the extra samples push the estimated expected value to the real expected value. For lower values of β, when the gap\nbetween policies is more significant, although some samples are available in the cluster, the values for the rewards observed\non them are non-informative of the real expected value (hence the difficulty of ML to make an accurate estimation and the\ndifference between MAP and ML for misspecified reward settings as depicted in Figure 1). For the real dataset, we follow\na similar procedure, but instead of the most relevant context-action pair, we select the top 15 and compare the conditional\nexpected reward estimates MSE with respect to IPS’ (see Figure 15). Since the logging policy for this dataset is uniform,\nthe distributional shift between the logging and evaluation policies is not as significant as the one presented in the base\nconfiguration of the synthetic dataset (β = −1). In practice, this means that the CHIPS estimation of the expected reward\nper cluster using ML is more accurate than in the synthetic dataset but still far from the performance jump of the CHIPS\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.8e-04\n8.4e-04\n4.0e-03\n1.9e-02\n9.2e-02\nMSE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2.7e-04\n7.2e-04\n1.9e-03\n5.2e-03\n1.4e-02\nSquared Bias\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n3.5e-08\n1.3e-06\n4.5e-05\n1.6e-03\n5.7e-02\nVariance\nDM\nIPS\nDR\nCHIPS_bayes\nMR\nSNDR\nDRoS\nSNIPS\nFigure 8: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying the\ncontext-specific noise σ.\n20\n40\n60\n80\n100\n (Beta prior)\n1.5e-02\n1.0e-01\n6.6e-01\n4.3e+00\n2.8e+01\nNormalized MSE\n20\n40\n60\n80\n100\n (Beta prior)\n2.6e-02\n7.1e-02\n1.9e-01\n5.2e-01\n1.4e+00\nNormalized Squared Bias\n20\n40\n60\n80\n100\n (Beta prior)\n5.8e-04\n3.6e-02\n2.2e+00\n1.4e+02\n8.5e+03\nNormalized Variance\nDM\nIPS\nDR\nCHIPS_bayes\nMR\nSNDR\nDRoS\nSNIPS\nFigure 9: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying the α\nparameter.\nestimate using MAP, as we would expect from the results in Figure 2.\nD.4\nChoosing alpha in Arbitrary Problems\nIn Figure 14 (b), we observe that the hyperparameters of the MAP estimation process can heavily impact the performance\nof the method. As previously discussed in Appendix D.1.1, MAP hyperparameters control the resistance with which the\nexpected reward per cluster is pulled towards the prior’s expectation. This resistance is particularly noticeable in smaller\nsize clusters, in which estimating a reward based on observations alone is much more challenging. Since in these clusters\nthe partitioning method cannot ensure high homogeneity at reward level, in our experimentation we decided to use a non-\ninformative prior (i.e., α = ˆβ), to mitigate possible violations of Assumption 3.2 and reward misspecification. Intuitively,\nan optimal value for α under these conditions needs to balance the prior’s resistance to prevent reward misspecification\nwithout incurring into creating a quasi-uniform reward estimation (excessively large values of α). In Figure 17 we explore\nthe optimal value of α for a given average number of datapoints per cluster-action. As expected, for small size clusters,\nlower values of alpha are favoured since the pull towards the prior’s expectation is soft, while on bigger clusters, the value\nof α (and consequently the resistance) needs to grow to effectively control reward misspecification (otherwise the expected\nreward value would be pulled towards the value of the observed samples).\nTo choose the value of α in an arbitrary problem, we propose the following selection process:\n1. Determine the number of clusters to use depending on the number of clusters (reference in Figure 12 (a).\n2. Partition the context space X in clusters c1,c2,...,cn.\n3. Generate synthetic data ˆ\nXev using Xtrain and πe.\n\n\nClustering Context in Off-Policy Evaluation\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nCluster radius\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Mse CHIPS (MAP) Using Different Clustering Methods\nclustering_method\nkmeans\nmean_shift\nspectral_clustering\nagglomerative_clustering\ndbscan\noptics\nbirch\nbayes_gmm\naffinity_propagation\ngmm\nFigure 10: Normalized MSE of CHIPS (MAP) using different clustering methods with respect to IPS.\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of Deficient Actions\n2.7e-02\n7.5e-02\n2.1e-01\n5.7e-01\n1.6e+00\nNormalized MSE\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of Deficient Actions\n4.0e-02\n9.3e-02\n2.2e-01\n5.1e-01\n1.2e+00\nNormalized Squared Bias\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of Deficient Actions\n1.8e-01\n2.7e+00\n4.1e+01\n6.3e+02\n9.5e+03\nNormalized Variance\nDM\nIPS\nDR\nCHIPS_bayes\nMR\nSNDR\nDRoS\nSNIPS\nFigure 11: From left to right, MSE, Bias, and Variance of the CHIPS estimator compared to baselines while varying the\nnumber of deficient actions.\n4. Estimate number of average data points per cluster-action from ˆ\nXev.\n5. Choose α from the reference in Figure 17.\nFor testing this selection process, following the experimental protocol of Taufiq et al. (2023), we transform five UCI\ndatasets (Dua and Graff, 2017), MNIST (Deng, 2012) , and CIFAR-100 (Krizhevsky, Nair, and Hinton, 2009) from multi-\nclass classification problems into contextual bandits data (Dud´ık, Langford, and Li, 2011). The results (averaged 50 times)\nin Figure 18 show a consistent improvement with respect to existing methods, empirically proving the effectiveness of the\nα selection process.\nAdditionally, we perform an alternative experiment using the real dataset, in which instead of fixing α and vary the number\nof clusters according to the reference in Figure 12 (b) with 50000, 100000 and 500000 samples (see Figure 2, we follow\nthe α selection process, fix the number of clusters and increase the value of α according to Figure 17. In Figure 19 we\nobserve equivalent results as in our previous experiment confirming the equivalence of using a reference for the number of\nsamples and varying the number of clusters with a fixed value for α, or varying α with a fixed number of clusters obtained\nby using a reference for the number of actions.\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\nE\nCLUSTER STRUCTURE IN DATASETS\nThe generated synthetic dataset ensures that the expected reward inside a cluster is similar and that the best possible action\nis usually the same for all the context within the cluster (see Figure 20), mimicking real-world settings like e-commerce in\nwhich we can expect similar behaviour for close contexts.\nF\nEXPERIMENTAL PROTOCOL\nFor evaluation in the real dataset, we follow (Saito and Joachims, 2022) protocol to evaluate estimators’ accuracy given\ntwo sources of data. Given a logging policy π, a dataset collected under it D, a logging policy π0, and the dataset collected\nunder it D0, we follow the following procedure:\n1. Extract n independent bootstrap samples with replacement from the logging dataset D∗\n0 ∶= {(xi,ai,ri)}n\ni=1.\n2. Estimate the policy value of π using the sample D∗\n0. We denote this estimate as ˆV (π;D∗\n0).\n3. Compute the relative mean squared error with respect to IPS:\nZ( ˆV ,D∗\n0) =\n(V (π) −ˆV (π;D∗\n0))\n2\n(V (π) −ˆVIPS(π;D∗\n0))\n2\nWhere V (π) ∶=\n1\n∣D∣∑(⋅,⋅,ri)∈D ri.\n4. Repeat steps 1,2, and 3 T = 100 times and compute the Empirical Cumulative Distribution Function (ECDF) as:\nˆFZ(x) ∶= 1\nT\nT\n∑\nt=1\n1{Zt( ˆV ,D∗\n0) ≤x}\nG\nCOMPLEXITY\nAlgorithmically, since the CHIPS estimator can be regarded as performing the same procedure as IPS with different weights\nand rewards,the time complexity given n logging samples, and clustering method ξ can be expressed as:\ncomplexity(CHIPS(n;ξ)) = complexity(IPS(n)) + complexity(ξ(n))\nFor example, since the time complexity of IPS is O(n), using DBSCAN (O(nlog n)) as a clustering method, we would\nget a time complexity for CHIPS of O(nlog n). In our experiments, we used batch-Kmeans (Sculley, 2010) as clustering\nmethod, that has a time complexity of O(mkdxt) where m is the batch size, k is the number of clusters, dx is the\ndimension of the features and t is the number of iterations. In the implementation used, we fixed m = 1024 and t = 100,\ntherefore, in this case, the time complexity of the CHIPS method is O(kdx) + O(n). The time complexity of the MIPS\nestimator can be estimated similarly as O(nde) + O(n) = O(nde), where the O(nde) term comes from the logistic\nregression used to estimate π0(a∣x,e) (being e an action embedding) and de is the action embedding dimension. The\nmethods using a supervised classifier (DM, DR, and MRDR) get their dominant term in time complexity from the training\nprocess of the classifier, in our case O(ndslog n) with s being the number of trees. In practice, this means that DM, , and\nMRDR will have significantly higher execution times (see Figure 21 (a)), and CHIPS will generally be faster than MIPS\nsince k << n to leverage the cluster structure, as we can appreciate in Figure 21 (b). The space complexity of CHIPS\ncan be estimated following a similar approach, for example when using the KMeans algorithm the space complexity is\nO(n(k + d)) + O(n) = O(n(k + d)).\n\n\nClustering Context in Off-Policy Evaluation\n10\n50\n100\n150\n200\n350\n550\nNº Actions\n10\n2\n10\n1\n100\n101\ny\nNormalized Mse (log scale)\nCHIPS_bayes Nº Clusters:1\nCHIPS_bayes Nº Clusters:5\nCHIPS_bayes Nº Clusters:10\nCHIPS_bayes Nº Clusters:15\nCHIPS_bayes Nº Clusters:20\nCHIPS_bayes Nº Clusters:50\nCHIPS_bayes Nº Clusters:100\nCHIPS_bayes Nº Clusters:150\nCHIPS_bayes Nº Clusters:250\nCHIPS_bayes Nº Clusters:350\nCHIPS_bayes Nº Clusters:550\nCHIPS_bayes Nº Clusters:750\nCHIPS_bayes Nº Clusters:1000\n(a) Normalized performance of CHIPS (MAP) with respect to IPS using different number of clusters and actions.\n1000\n5000\n10000\n20000\n30000\n50000\n70000\n100000\n150000\nNº Samples\n10\n1\n100\n101\ny\nNormalized Mse (log scale)\nCHIPS_bayes Nº Clusters:1\nCHIPS_bayes Nº Clusters:5\nCHIPS_bayes Nº Clusters:10\nCHIPS_bayes Nº Clusters:15\nCHIPS_bayes Nº Clusters:20\nCHIPS_bayes Nº Clusters:50\nCHIPS_bayes Nº Clusters:100\nCHIPS_bayes Nº Clusters:150\nCHIPS_bayes Nº Clusters:250\nCHIPS_bayes Nº Clusters:350\n(b) Normalized performance of CHIPS (MAP) with respect to IPS using different number of clusters and logging\nsamples.\nFigure 12: Bi parametric experiments results using different number of clusters for analyzing CHIPS capabilities when\nincreasing actions (a) and logging samples (b).\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n10\n2\n10\n1\n100\n101\n102\n103\n104\n105\nNormalized Mse CHIPS (MAP) varying policy closeness and Beta prior's alpha\n20\n40\n60\n80\n100\n (prior)\nFigure 13: Normalized performance of CHIPS (MAP) with respect to IPS using different values for the α parameter in the\nBeta prior and distributional shift between logging and evaluation policies (β).\n\n\nClustering Context in Off-Policy Evaluation\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n101\n102\nRelative Squared Error w.r.t. IPS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability\nCHIPS (MAP) with Different Cluster Sizes\nIPS\nNº Clusters_1\nNº Clusters_2\nNº Clusters_4\nNº Clusters_8\nNº Clusters_16\nNº Clusters_32\nNº Clusters_64\n(a) ECDFs of CHIPS (MAP) using different number of clusters in the real dataset.\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n101\n102\nrelative squared error w.r.t. IPS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability\nReal Dataset with different values for  in Beta prior\nIPS\n= 1\n= 5\n= 10\n= 20\n= 30\n= 50\n= 100\n(b) ECDFs of CHIPS (MAP) using different values of α for the Beta prior in the\nreal dataset.\nFigure 14: Additional experiments varying the number of clusters and the α parameter in the Beta prior for CHIPS (MAP)\nin the real dataset (using 100000 samples).\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nContext\n10\n1\n100\nNormalized Squared Error\nNormalized Context Squared Error\nIPS\nMIPS (w/ SLOPE)\nCHIPS_mean\nCHIPS_bayes\nFigure 15: Normalized MSE with respect to IPS of the expected rewards for the 15 most common context-action pairs in\nthe real logging dataset.\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nNormalized Mean Squared Error\nCHIPS_bayes\nCHIPS_mean\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n30\n35\n40\n45\n50\n55\n60\n65\n70\nFrequency of context-action pair in sample\nCHIPS_bayes\nCHIPS_mean\nFigure 16: Normalized MSE of CHIPS with respect to IPS (left) and samples in the associated cluster (right) for the most\ncommon context-action pair in the evaluation policy while varying the distributional shift (β) in the synthetic dataset.\n\n\nClustering Context in Off-Policy Evaluation\n100\n101\n102\nAvg. points per cluster/action\n10\n1\n100\n101\nNormalized Mse CHIPS (MAP) varying avg. points per cluster/action\n20\n40\n60\n80\n100\n (prior)\nFigure 17: Normalized MSE of CHIPS (MAP) with respect to IPS using different values of α and number of expected data\npoints per cluster-action.\ndigits\nletter\nmnist\noptdigits\npendigits\nsatimage\ncifar100\n10\n5\n10\n4\n10\n3\n10\n2\nMean Squared Error for Multilabel datasets\nIPS\nDR\nDM\nCHIPS_bayes\nMR\nSwitchDR\nDRos\nFigure 18: MSE of CHIPS (MAP) using α selection policy with respect to IPS, DR, DM, MR (Taufiq et al., 2023), DRos(Su\net al., 2020) and SwitchDR (Wang, Agarwal, and Dud´ık, 2017).\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n101\n102\nrelative squared error w.r.t. IPS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability\n0.99\n0.4\n0.41\n0.41\n0.4\n0.78\n=10\n10\n6\n10\n4\n10\n2\n100\nrelative squared error w.r.t. IPS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability\n1.0\n0.32\n0.38\n0.38\n0.35\n0.79\n=20\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\nrelative squared error w.r.t. IPS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability\n1.0\n0.25\n0.11\n0.11\n0.2\n0.92\n=100\nIPS\nCHIPS_bayes\nDM\nDR\nMRDR\nMIPS (w/o SLOPE)\nMIPS (w/ SLOPE)\nFigure 19: ECDF of the relative mean squared error with respect to IPS for the real dataset using 50000 (left), 100000\n(center), and 500000 (right) logging samples and the α selection process.\n\n\nClustering Context in Off-Policy Evaluation\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\nx1\n10\n5\n0\n5\n10\nx2\n[r] in Synthetic Dataset (2D, rad=1)\n0.05\n0.10\n0.15\n0.20\n0.25\n(a) Expected reward per context for a sepcific action in the synthetic dataset.\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\nx1\n10\n5\n0\n5\n10\nx2\nBest action in Synthetic Dataset (2D, rad=1)\nr\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(b) Action maximizing the expected reward per context in the synthetic dataset.\nFigure 20: Representation of the synthetic dataset using 2-dimensional contexts.\n\n\nDaniel Guzm´an-Olivares, Philipp Schmidt, Jacek Golebiowski, Artur Bekasov\n10000\n64444\n118888\n173333\n227777\n282222\n336666\n391111\n445555\n500000\nNº Samples\n0\n100\n200\n300\n400\n500\nTime (s)\nReal Dataset Execution Time\nIPS\nCHIPS_bayes\nDM\nDR\nMRDR\nMIPS (w/ SLOPE)\n(a) Execution times for the real dataset including DM, DR, and MRDR.\n10000\n64444\n118888\n173333\n227777\n282222\n336666\n391111\n445555\n500000\nNº Samples\n0\n2\n4\n6\n8\nTime (s)\nReal Dataset Execution Time\nIPS\nCHIPS_bayes\nMIPS (w/ SLOPE)\n(b) Execution times for the real dataset for IPS, CHIPS, and MIPS.\nFigure 21: Average execution times increasing the sample size (100 executions per sample size).\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21304v1.pdf",
    "total_pages": 35,
    "title": "Clustering Context in Off-Policy Evaluation",
    "authors": [
      "Daniel Guzman-Olivares",
      "Philipp Schmidt",
      "Jacek Golebiowski",
      "Artur Bekasov"
    ],
    "abstract": "Off-policy evaluation can leverage logged data to estimate the effectiveness\nof new policies in e-commerce, search engines, media streaming services, or\nautomatic diagnostic tools in healthcare. However, the performance of baseline\noff-policy estimators like IPS deteriorates when the logging policy\nsignificantly differs from the evaluation policy. Recent work proposes sharing\ninformation across similar actions to mitigate this problem. In this work, we\npropose an alternative estimator that shares information across similar\ncontexts using clustering. We study the theoretical properties of the proposed\nestimator, characterizing its bias and variance under different conditions. We\nalso compare the performance of the proposed estimator and existing approaches\nin various synthetic problems, as well as a real-world recommendation dataset.\nOur experimental results confirm that clustering contexts improves estimation\naccuracy, especially in deficient information settings.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}