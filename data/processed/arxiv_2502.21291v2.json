{
  "id": "arxiv_2502.21291v2",
  "text": "MIGE: A Unified Framework for Multimodal Instruction-Based\nImage Generation and Editing\nXueyun Tian♠♡, Wei Li♠, Bingbing Xu♠, Yige Yuan♠♡, Yuanzhuo Wang♠, Huawei Shen♠♡\n♠CAS Key Laboratory of AI Safety,\nInstitute of Computing Technology, Chinese Academy of Sciences, Beijing, China\n♡University of Chinese Academy of Sciences, Beijing, China\n{tianxueyun23z, xubingbing, yuanyige20z, wangyuanzhuo, shenhuawei}@ict.ac.cn\nweili.ucas.ict@gmail.com\nFollow the instruction to edit the image:\nFollow the text instruction to generate an image with the given objects, preserving the subject's identity:\nSubject-driven editing:\n1. make <imagehere> look like a cartoon\n2. have there be a stream running through the field in <imagehere>\n3. put a red bow on the elephant's head in <imagehere>\n4. substitute tomatoes for oranges in <imagehere>\n5. remove the word \"STOP\" from the red sign in <imagehere>\nObject Replacement\nStyle Transfer\nObject Addition\nDetail Removal\nScene Modification\nSingle Subject\nMultiple Subjects\nInstruction-based \nImage Editing\nSubject-driven \nImage generation\nInstruction-based \nSubject-driven \nImage Editing\nInstruction-based Subject Addition\nadd <imagehere> to the lower left of \n<imagehere>. The squirrel is small and \npositioned on a tree stump, surrounded \nby green foliage.\nadd <imaghere> to the bottom left of \n<imagehere>. The pen is relatively \nsmall compared to the image size and \nis positioned near a large smartphone.\nreplace the pillow on the left in \n<imagehere> with <imagehere>\nreplace the candle in <imagehere> with \n<imagehere>\nreplace the transparent jug in \n<imagehere> with <imagehere> in \nthe middle right\nadd <ima g e h e r e > to  th e  l e ft o f \n<imagehere>. The red rose is relatively \nsmall and positioned near the left edge, \nnestled within dark hair.\nInstruction-based Subject Replacement\na n  u n r i p e \n< i m a g e h e r e > \nnear a mountain \nstream\n<imagehere> \nw e a r i n g \n<imagehere> \ni s  e a t i n g \n<imagehere>\n<imagehere> \nw e a r i n g \n<imagehere> \ni s  p l a y i n g \n<imagehere>\n<imagehere> in \nfront of a colorful \nhouse\n< i m a g e h e r e > \nwearing a santa \nhat\nFigure 1: Demonstrating the comprehensive capabilities of MIGE. As a unified framework, MIGE excels in\nsubject-driven generation and instruction-based editing while performing well in the new compositional task of\ninstruction-based subject-driven editing.\nAbstract\nDespite significant progress in diffusion-based\nimage generation, subject-driven generation\nand instruction-based editing remain challeng-\ning. Existing methods typically treat them sep-\narately, struggling with limited high-quality\ndata and poor generalization. However, both\ntasks require capturing complex visual varia-\ntions while maintaining consistency between in-\nputs and outputs. Therefore, we propose MIGE,\na unified framework that standardizes task rep-\nresentations using multimodal instructions. It\ntreats subject-driven generation as creation on\na blank canvas and instruction-based editing\nas modification of an existing image, establish-\ning a shared input-output formulation. MIGE\nintroduces a novel multimodal encoder that\nmaps free-form multimodal instructions into\na unified vision-language space, integrating vi-\nsual and semantic features through a feature fu-\nsion mechanism. This unification enables joint\ntraining of both tasks, providing two key ad-\nvantages: (1) Cross-Task Enhancement: By\nleveraging shared visual and semantic represen-\ntations, joint training improves instruction ad-\nherence and visual consistency in both subject-\ndriven generation and instruction-based edit-\ning. (2) Generalization: Learning in a unified\nformat facilitates cross-task knowledge trans-\nfer, enabling MIGE to generalize to novel com-\npositional tasks, including instruction-based\nsubject-driven editing. Experiments show that\nMIGE excels in both subject-driven generation\nand instruction-based editing while setting a\nstate-of-the-art in the new task of instruction-\nbased subject-driven editing. Code and model\nhave been publicly available at this link.\n1\nIntroduction\nRecent advances in diffusion models (Rombach\net al., 2022; Peebles and Xie, 2023; Labs, 2023;\nEsser et al., 2024) have greatly advanced cus-\ntomized image generation. Central to this field are\nsubject-driven image generation, which focuses on\npreserving a given subject, and instruction-based\narXiv:2502.21291v2  [cs.CV]  3 Mar 2025\n\n\nimage editing, which emphasizes flexible modifica-\ntions based on textual instructions.\nExisting methods address these tasks separately,\neach tailored to a specific input-output format,\nlimiting generalization and hindering a unified\ninstruction-following model. Subject-driven gen-\neration aims to preserve subject identity while ad-\nhering to textual instructions. Fine-tuning methods,\nlike DreamBooth (Ruiz et al., 2023), require sepa-\nrate training for each subject during testing, which\nlimits generalization. In contrast, zero-shot meth-\nods like KOSMOS-G (Pan et al., 2023) rely on\nmultimodal alignment but struggle with instruc-\ntion following, mainly due to the scarcity of high-\nquality multimodal training data, especially for\nmulti-subject inputs. Instruction-based editing, ex-\nemplified by InstructPix2Pix (Brooks et al., 2023)\nand EmuEdit (Sheynin et al., 2024), modifies exist-\ning images based on instructions. These methods\nrequire large, high-quality editing datasets to han-\ndle diverse instructions but struggle with maintain-\ning local-to-global consistency.\nBoth tasks face significant challenges due to\nlimited data, which hinders their ability to adapt\nto diverse subjects or editing requirements. Fur-\nthermore, they are typically trained independently,\nlacking cross-task consistency, which prevents a\nunified understanding of the tasks. Despite these\nchallenges, the two tasks share a fundamental prin-\nciple: preserving visual consistency between inputs\nand outputs while capturing visual variations based\non instructions. Additionally, they also follow a\nsimilar input-output paradigm, with complemen-\ntary data focuses. Subject-driven image generation\ntrains models to produce subject-accurate images,\nwhile instruction-based editing focuses on manipu-\nlating images without altering irrelevant identities.\nThe above limitations as well as the complementary\nnature underscore the need for a unified framework\nthat combines the strengths of both tasks to achieve\nimproved performance.\nInspired by the above analysis, we propose a uni-\nfied framework, MIGE, which leverages Multimodal\nInstructions to unify subject-driven Generation and\ninstruction-based Editing. This vision-language\nrepresentation enables substituting both entities and\nentire images in text prompts with their visual coun-\nterparts, allowing flexible task and instruction com-\nbinations. Structurally, we model subject-driven\ngeneration as creating an image on a blank can-\nvas and instruction-based editing as modifying an\nexisting image. This coherent input-output map-\nping simplifies the process while enabling both\ntasks to reinforce each other through joint training.\nMoreover, this integrated approach fosters emer-\ngent compositional capabilities beyond what either\ntask can achieve alone.\nBuilding on our unified framework, we address\ntwo critical challenges. First, for multimodal in-\nstruction encoding, existing methods (Li et al.,\n2023a; Pan et al., 2023; Li et al., 2024b) primar-\nily extract semantic features from images by CLIP\nvision encoder (Radford et al., 2021), which is in-\nsufficient for preserving fine-grained subject de-\ntails. To overcome this limitation, we introduce\na multimodal encoder equipped with a feature fu-\nsion mechanism. This innovation integrates VAE\n(Kingma, 2013) visual features into semantic to-\nkens, effectively capturing both detailed visual in-\nformation and semantics. Secondly, we enhance\ncompositionality through joint training, facilitating\ninstruction-based subject-driven editing. To fur-\nther optimize performance in this complex task, we\ndevelop a novel data construction pipeline based\non a Multimodal Large Language Model (MLLM).\nThis pipeline autonomously generates diverse mul-\ntimodal instructions and corresponding output im-\nages. Moreover, to address the absence of an eval-\nuation benchmark for this new composition task,\nwe introduce MIGEBench. This specialized bench-\nmark evaluates compositionality in terms of subject\npreservation and instruction adherence, providing\na comprehensive assessment of the new task.\nOur experiments demonstrate that joint training\nunder the MIGE framework enables mutual rein-\nforcement between subject-driven generation and\ninstruction-based editing by leveraging comple-\nmentary data, leading to significant performance\ngains over training on individual tasks. Specifi-\ncally, we observe improved instruction adherence\non DreamBench (Ruiz et al., 2023) and enhanced\ndetail preservation on EmuEdit and MagicBrush\n(Zhang et al., 2024) test sets. Furthermore, MIGE\nachieves state-of-the-art results on MIGEBench,\nshowcasing its fine-grained controllability in the\nemerging compositional tasks. Extensive case stud-\nies (as shown in Figure 5 and Figure 6) further\nillustrate the ability of MIGE to generate precise and\nconsistent outputs, highlighting its effectiveness in\ndiverse scenarios.\nIn summary, our contributions include:\n• Unified Framework: We propose MIGE, a uni-\nfied framework that integrates subject-driven\n\n\ngeneration and instruction-based editing, facil-\nitating joint training and mutual enhancement\nof both tasks.\n• Compositional Capability: We observed that\njoint training unlocks the compositional ca-\npability of instruction-based subject-driven\nediting. We propose a novel data construc-\ntion pipeline and introduce the MIGEBench\nbenchmark for comprehensive evaluation.\n• Strong Performance: The extensive experi-\nmental results show that MIGE achieves com-\npetitive results in both subject-driven gener-\nation and instruction-based editing while es-\ntablishing a new state-of-the-art in instruction-\nbased subject-driven image editing.\n2\nRelated Work\nIn this section, we focus on discussing the most\nrelevant studies in this section. A more comprehen-\nsive discussion, including additional related work,\ncan be found in the Appendix D.\nUniversal generative model\nUnifying tasks in\nimage generation is challenging, and several ap-\nproaches have been proposed. Pixwizard (Lin et al.,\n2024) leverages task vectors to integrate image\ngeneration and editing but struggles with vision-\nlanguage instructions due to architectural limita-\ntions. Instruct-Imagen (Hu et al., 2024) unifies\ntasks through multimodal instructions but remains\nineffective for editing even after fine-tuning. ACE\n(Han et al., 2024) encodes various inputs as condi-\ntion units within a transformer but is limited to edit-\ning. OmniGen (Xiao et al., 2024b) concatenates\nconditions before the noise and uses autoregres-\nsive transformers, but fails to distinguish between\nbackground and reference images, resulting in poor\nbackground preservation. UniReal (Chen et al.,\n2024) makes finer distinctions but requires complex\nprompts for task differentiation. In contrast, MIGE\nensures input-output consistency with conditional\ninput and multimodal instructions, simplifying task\ndistinction and offering better scalability without\nthe need for complex hierarchical prompts.\nSubject-Driven Image Editing\nSubject-driven\nimage editing allows image modifications based on\na user-specified subject, typically through addition\nor replacement. Methods like DreamEdit (Li et al.,\n2023b) and DreamMix (Yang et al., 2024) require\nmultiple input images for fine-tuning, associating\nMIGE\nMultimodal Instruction\nConditional Input\nadd Homer Simpson \nlooking at the donuts in \n<imagehere>\n<imagehere> \nin the jungle\n add <imagehere> to the upper left of \n<imagehere>. The small blue and yellow \nbird is perched on a metal structure, \noccupying a moderate portion of the upper \nleft quadrant of the image.\nMIGE\nMIGE\nInstruction-based image editing\nSubject-driven image generation\nInstruction-based subject-driven image generation\nFigure 2: Demonstration of MIGE as a unified frame-\nwork for processing multimodal instructions and condi-\ntional inputs across diverse tasks and scenarios.\nnew concepts with special tokens, followed by iter-\native inpainting. PBE (Yang et al., 2023) conditions\na diffusion model on compressed reference image\nrepresentations, while TIGIC (Li et al., 2025) in-\ntegrates reference images by positioning them in\npredefined backgrounds. MADD (He et al., 2024)\noffers more diverse control by providing masks,\nboxes, or target coordinates. Overall, these meth-\nods rely on multiple reference images or precise\ncues like bounding boxes and masks, and cannot\nfollow raw textual instructions for accurate and\ncontrollable generation, making them inflexible.\n3\nUnified Framework\n3.1\nTask Unification\nCurrent methods treat subject-driven generation\nand instruction-based image editing separately, hin-\ndering performance due to data limitations and poor\ngeneralization. Since both aim to preserve visual\nconsistency while incorporating instructed changes,\nunifying them allows mutual improvement. Joint\ntraining on diverse data enhances subject preserva-\ntion and instruction following beyond the capabili-\nties of individual task-specific models.\nTherefore, we propose MIGE to unify the two\ntasks for joint training. By using multimodal in-\nstructions as a unified task representation, it sup-\nports flexible combinations and provides multi-\nmodal guidance. Additionally, we use conditional\ninput to structurally unify the tasks, enhancing vi-\nsual consistency. Their combination not only pro-\nvides rich visual and instructional information but\nalso naturally represents different tasks. We demon-\nstrate the input-output format of MIGE in Figure 2.\nUnified Multimodal Instruction\nTo enable joint\ntraining across multiple tasks, a unified task rep-\n\n\nscale\nPointwise\nFeedforward\nscale, shift\nscale\nMulti-Head\nSelf-Attention\nscale, shift\nMultimodal\nCross-Attention\nMLP\nTime Embedding\nx Ƹ\nVAE\n...\nconditional input\nlatent noise\n❄️\nFeature Fusing Mechanism\nMultimodal Encoder\nLLM\nProjector\nCLIP\n(semantic)\nQ-Former\nVAE\n(visual)\n❄️\n❄️\n\" R e p l a c e  f l o w e r \narrangements and \nmiscellaneous items \non the table in\"\n\"with\"\n...\n...\nSemantic Feature\nVisual Feature\n...\nLinear\nLinear\nLinear\nGELU\nNorm\nNorm\nMultiheadAttention\nMLP \nNorm\nQ\nK\nV\nFigure 3: Overall framework of MIGE. MIGE consists of two components: a multimodal encoder for processing\nmultimodal instructions and a transformer-based diffusion model for modeling input-output relationships. The\nencoder incorporates a feature fusion mechanism to integrate visual and semantic features from reference image.\nresentation is essential. We introduce multimodal\ninstructions composed of interleaved images and\ntext, which provide both visual references and tex-\ntual guidance for various controllable generation\ntasks. As shown in Figure 2, “<imagehere>” serves\nas a placeholder, sequentially replaced by input\nimages, which can be the reference subject or the\nentire scene, complementing the semantics to form\ninterleaved expressions. This unified approach ef-\nfectively accommodates subject-driven generation\nand instruction-based editing, while remaining ex-\ntensible to the more complex combination task.\nUnified Conditional Input\nWe adopt a condi-\ntional input design to unify tasks structurally, en-\nsuring clear task distinction while enabling shared\ncapabilities. By concatenating different conditions,\nwe differentiate initial generation states and capture\ntask-specific nuances, enhancing execution accu-\nracy. As shown in Figure 2, for instruction-based\nediting, we concatenate the VAE-encoded source\nimage with the noise tensor, guiding generation\nbased on the given image. For subject-driven gen-\neration, we use an all-zero tensor akin to a blank\ncanvas, prompting the model to generate freely\nwhile preserving specified visual features. This de-\nsign effectively captures the differences between\nthe two tasks while ensuring input-output consis-\ntency in editing. Moreover, structuring tasks within\na unified framework allows the model to leverage\nshared capabilities and easily extend to new tasks.\n3.2\nArchitecture\nThe architecture of MIGE, as shown in Figure 3,\nconsists of two main components: a multimodal\nencoder for processing multimodal instructions and\na transformer-based diffusion model (Peebles and\nXie, 2023) for modeling input-output relationships.\nThe diffusion model takes concatenated latent noise\nand conditional input along the channel dimension\nas input, performing controllable generation under\nthe control of multimodal conditions. To further\nenhance the integration of visual and semantic in-\nformation from the reference image, we introduce\na novel feature fusion mechanism in the encoder.\nMultimodal Encoder\nTo map the multimodal in-\nstructions into a unified vision-language semantic\nspace, we design a multimodal encoder compris-\ning a Large Language Model (LLM) and an image\nfeature encoding component, which includes a pre-\ntrained VAE encoder (Kingma, 2013) for visual fea-\nture extraction, a pretrained ViT from EVA-CLIP\n(Fang et al., 2023) for semantic feature extraction,\na Q-Former (Li et al., 2023a), and a linear projec-\ntion layer. Each image is represented by 32 tokens,\nwhich are then encoded by the LLM alongside text\ntokens to form a unified multimodal condition.\n\n\n×\nRule-based Segmentation Filter\naesthetic score = 5.32\narea ratio: 0.48\n0.226\n0.13\n0.006\n0.036\nBackground Inpaint\nbackground\nMultimodal Instruction Construct\n\"Add <imagehere> to \nthe center-bottom of \n<imagehere>. The car \ni s  p a r k e d  o n  t h e \nstreet, adjacent to a \nsidewalk with people \nand a dog nearby.\"\nEntity Identify\nEntity Filter\n\"car\"\nmain\nentity\ntarget image\nEntity Segment\n(SAM)\nStep1: Entity Segmentation and Filtering\nStep2: Background Inpainting\nStep3: Multimodal Instruct Construction\n×\n×\n×\n(a) Subject Addition Dataset Construction Pipeline\nEntity Segment\n(Grounded SAM)\n\"replace the bird in <imagehere> with <imagehere>\"\naesthetic score = 5.66\nEntity Filter\nRule-based \nMultimodal Instruction Construct\n\"parrot\"\n\"parrot\"\n(b) Subject Replacement Dataset Construction Pipeline\nFigure 4: Data construction pipelines for instruction-based subject-driven image editing.\nUnlike prior works (Li et al., 2024a; Pan et al.,\n2023; Li et al., 2024b) that mainly extract seman-\ntic features of the reference images, which lack\nthe fine-grained details necessary for preserving\nsubject-specific features. To address this, we pro-\npose a feature fusion mechanism that combines the\nstrengths of different visual encoders. ViT serves as\na semantic feature extractor, while a VAE encoder\nacts as a visual feature extractor, leveraging its abil-\nity to compress and reconstruct images. As shown\nin Figure 3, we use the ViT semantic features com-\npressed by the Q-Former, denoted as fs, as guid-\nance to adaptively incorporate the visual features fv\nextracted by the VAE. The fusion mechanism is ex-\npressed as: fimg = fs + MLP(Attn(fs, fv)) where\nAttn(fs, fv) = Q(fs)·K(fv)\n√\nd\nV (fv). Here d denotes\nthe output dimension of the features, Q is a linear\nlayer, and both K and V are two-layer networks,\nwhere K(·) = V (·) = Linear(GELU(Linear(·))).\nThrough this fusion mechanism, we can effec-\ntively capture both visual and semantic information\nof reference images simultaneously without intro-\nducing additional image tokens.\n3.3\nJoint Training\nMultimodal instructions and conditional input\nunify task representation and input-output formats,\nenabling joint training. We fine-tune MIGE on data\nfrom all tasks to enhance cross-task synergy. Ex-\ncept for the two image encoders, all parameters are\njointly trained to align the conditional space of the\ndiffusion model with the multimodal encoder, as\nshown in Figure 3. This approach improves task\ncoordination and consistency across modalities.\nData Construction\nJoint training enables multi-\ntask learning, balancing subject preservation and in-\nstruction, followed by modeling task relationships.\nWe create a multi-task dataset for joint multimodal\ninstruction tuning, covering subject-driven image\ngeneration, instruction-based image editing, and\ninstruction-based subject-driven image generation.\nFor subject-driven image generation, we follow\nthe data construction methods of KOSMOS-G (Pan\net al., 2023) and UNIMO-G (Li et al., 2024b), us-\ning an LLM to extract entities from captions, which\nare then fed into Grounded SAM (Ren et al., 2024)\nfor segmentation. Subjects200k dataset from Om-\nniControl (Tan et al., 2024) is also incorporated for\nbetter object preservation. For instruction-based\nediting, we filter existing datasets and use a rule-\nbased strategy to construct multimodal instructions.\nInstruction-based subject-driven image genera-\ntion is an emerging task that involves two subtasks:\ninstruction-based subject addition and subject re-\nplacement. This allows users to add or replace a\nspecified subject in an image using multimodal in-\nstructions, as shown in Figure 1. However, there is\nno sufficient dataset available for this task.\nFor instruction-based subject addition, we pro-\npose a pipeline inspired by SAM-FB (He et al.,\n2024), as shown in Figure 4a. Starting with the\nSA-1B (Kirillov et al., 2023) dataset, we construct\nthe source image and a multimodal instruction for\ninput-output pairs. We use SAM (Kirillov et al.,\n2023) to segment annotated entities, filter and re-\ntain the main subject using MLLM, inpaint the\nremaining parts to create the background, and then\ncombine the subject name with the target image\nto generate the multimodal instruction via GPT-4o\n(Hurst et al., 2024). Due to resource limitations,\n\n\nRef.+Prompt\nOmniGen\nOurs\nKOSMOS-G\nPrompt\nACE\nPixWizard\nOmniGen\nImage\nOurs\nchange the dog to \na poodle in \n<imagehere>\nadd a Daffy Duck \nimage to the \nsuitcase in \n<imagehere>\na <imagehere> in the snow\na <imagehere> wearing a red hat\na <imagehere> is playing with <imagehere>\na <imagehere> wearing <imagehere> is eating <imagehere>\nRef.+Prompt\nOmniGen\nOurs\nKOSMOS-G\nMagicBrush\nFigure 5: Qualitative comparison for subject-driven image generation (top rows) and instruction-based image\nediting (bottom rows). We compare the universal model and task-specific models on the two tasks, respectively.\nThe prompts listed in the figure are used for MIGE and vary according to the usage of each model.\nwe only process a part of the SA-1B dataset and\nobtain about 200k samples, but the pipeline can\nbe scaled to generate more. For instruction-based\nsubject replacement, we filter from existing editing\ndata, using Grounded SAM to obtain subject seg-\nmentation and construct the multimodal instruction\nto form input-output pairs, as shown in Figure 4b.\nWe also introduce virtual try-on data constructed\nusing IDM-VTON (Choi et al., 2024), resulting in\napproximately 110k data samples. More details\nabout training data construction are in Appendix B.\n4\nExperiments\n4.1\nImplementation Details\nMIGE includes a conditional diffusion model and a\nmultimodal encoder. Our design allows flexible se-\nlection of various diffusion models, and we initial-\nize with PIXART-α (Chen et al., 2023) pretrained\nat a 512×512 resolution. Parameters introduced for\nhandling conditional inputs are initialized to zero,\nwhile the original weights of the model remain un-\nchanged. The multimodal encoder consists of a\npretrained Flan-T5-XXL (Chung et al., 2024) as\nLLM for initialization and an image encoding com-\nponent. This includes query tokens, Q-Former, and\na projector, all initialized with a BLIP-2 (Li et al.,\n2023a) checkpoint (pretrain_flant5xxl). The frozen\nVAE encoder, used as the visual feature extractor,\nis the same as the one in the diffusion model. Addi-\ntionally, a zero-initialized MLP layer is introduced\nin the feature fusing mechanism for progressive\nvisual feature integration.\nMIGE is trained on our multi-task dataset using\nthe AdamW optimizer (Loshchilov, 2017) with a\nweight decay of 0.03 and a learning rate of 1e-5 for\n18 epochs on 48 H20 GPUs, totaling six days of\ntraining with a batch size of 960 (20 per GPU). A\n1:1 sampling strategy is applied for subject addition\nand replacement tasks, and during training, there\nis a 5% chance of dropping either the conditional\ninput or multimodal condition, with an additional\n5% chance of dropping both, enabling classifier-\nfree guidance during inference.\n4.2\nEvaluation Results\nAs a unified model, MIGE excels in various image\ngeneration and editing tasks, outperforming exist-\ning task-specific models. In this section, we high-\nlight its strong performance in subject-driven im-\nage generation and instruction-based editing, show-\ncasing its emerging capability in instruction-based\nsubject-driven image generation on our new bench-\nmark. For detailed evaluation results, refer to Ap-\npendix C. Additional results on these tasks can be\nfound in Figure 12.\n\n\nMethods\nDINO ↑\nCLIP-I ↑\nCLIP-T ↑\nTask-specific Models\nKOSMOS-G (Pan et al., 2023)\n0.694\n0.847\n0.287\nUNIMO-G (Li et al., 2024b)\n0.668\n0.841\n0.329\nUniversal Models\nOmnigen (our test) (Xiao et al., 2024b)\n0.711\n0.800\n0.312\nUnireal (Chen et al., 2024)\n0.702\n0.806\n0.326\nMIGE (ours)\n0.744\n0.830\n0.293\n/only_subject data\n0.726\n0.823\n0.289\n/wo_VAE feature\n0.741\n0.828\n0.287\nTable 1: Quantitative results for subject-driven im-\nage generation on DreamBench. MIGE outperforms\nuniversal models in subject preservation and remains\ncompetitive with task-specific models.\n4.2.1\nSubject-driven Image Generation\nGenerating images that satisfy both image and text\nconstraints from a multimodal prompt is a chal-\nlenging task. We compare MIGE with two task-\nspecific methods that also use MLLM to encode\nmultimodal conditions, as well as two universal\nmodels, as shown in Table 1. DINO and CLIP-I\nwere used to assess subject fidelity, while CLIP-\nT evaluated adherence to multimodal instructions.\nOur results on DreamBench demonstrate that MIGE\nachieves competitive text fidelity while better pre-\nserving subject features, particularly outperforming\nthe DINO metric. The qualitative comparison in\nFigure 5 further demonstrates that MIGE not only\nperforms better in single-subject generation tasks\nbut also preserves the distinct characteristics of\neach subject in multi-subject generation, whereas\nother models either fail to retain all subjects or\nlose their individual features. This advantage arises\nfrom the ability of MIGE to flexibly combine mul-\ntiple reference entities in multimodal instructions\nand integrate additional visual features through its\nfeature fusion mechanism.\n4.2.2\nInstruction-based Image Editing\nInstruction-based image editing enables users to\nmodify a source image based on free-form mul-\ntimodal instructions, including adding, removing,\nmodifying object properties, or altering the over-\nall style. Table 2 presents a quantitative analysis\nof the Emu Edit and MagicBrush test sets. DINO\nand CLIP-I evaluate similarity to the source image,\nwhile CLIP-T measures alignment with the target\ndescription. CLIPdir quantifies the alignment be-\ntween CLIP text and image embedding changes,\nwhereas L1 and L2 capture pixel-level differences.\nAs shown in Table 2, MIGE achieves the high-\nest CLIP-T score and surpasses all task-specific\nmodels in CLIPdir, demonstrating its superior abil-\nity to effectively follow multimodal instructions.\nAs a universal model, MIGE outperforms all other\nuniversal models across all metrics on the Mag-\nicBrush test set, achieving the lowest L1 and L2\nscores and the highest CLIP-I, DINO, and CLIP-T\nscores, highlighting its strong instruction fidelity\nand fine-detail preservation. This capability is fur-\nther illustrated in Figure 5, where MIGE is the only\nmodel that accurately follows the instruction to add\na Daffy Duck image to the red suitcase without\naltering other unrelated areas.\n4.2.3\nInstruction-based Subject-driven\nImage Editing\nBenchmark\nConstruction\nInstruction-based\nsubject-driven image editing is a novel task.\nExisting methods rely on masks or positional\ncoordinates for editing (Li et al., 2023b; Yang\net al., 2024) but lack support for instruction-based\nediting. Current benchmarks for subject addition\nand replacement separately evaluate foreground\nand background similarities without providing a\ncomplete edited image as ground truth, making\nthem unsuitable for this task.\nTo address these issues,\nwe construct a\nbenchmark of 1,000 test samples:\n500 for\ninstruction-based subject addition and 500 for\nsubject replacement.\nData is sourced from\nSEED-Data-Edit (Ge et al., 2024), with subjects\nextracted using Grounded SAM (Ren et al., 2024).\nCaptions for target images are generated by\nGPT-4o and refined through manual review. For\ncompatibility with prior methods, our benchmark\nalso includes masks. Details of the construction\nprocess are provided in Appendix A.\nEvaluation Results\nOur evaluation focuses on\nediting ability and subject preservation. Editing\nability is assessed using DINO, CLIP-I, and CLIP-\nT: DINO and CLIP-I measure similarity to the\nground truth image, while CLIP-T evaluates align-\nment with the target caption. Subject preservation\nis evaluated by extracting the edited subject via\nGrounded SAM and comparing it to the input sub-\nject image using DINO and CLIP-I. This separates\nthe evaluation of image-level editing from subject-\nlevel feature preservation. Methods that do not\nsupport instruction-based editing are marked with\na cross in the table, and masks used during testing.\nQuantitative comparisons with other methods are\n\n\nEmu Edit Test set\nMagicBrush Test set\nMethods\nDINO ↑CLIP-I ↑CLIP-T ↑CLIPdir ↑\nL1 ↓\nL2 ↓\nDINO ↑CLIP-I ↑CLIP-T ↑CLIPdir ↑\nL1 ↓\nL2 ↓\nTask-specific Models\nInstructPix2Pix (Brooks et al., 2023)\n0.759\n0.831\n0.288\n0.086\n0.122\n0.036\n0.763\n0.843\n0.289\n0.105\n0.097\n0.028\nMagicBrush (Zhang et al., 2024)\n0.800\n0.857\n0.295\n0.102\n0.085\n0.027\n0.847\n0.888\n0.304\n0.127\n0.062\n0.018\nUltraEdit (Zhao et al., 2024)\n0.862\n0.875\n0.301\n0.100\n0.050\n0.008\n0.879\n0.897\n0.305\n0.119\n0.042\n0.006\nUniversal Models\nPixWizard (Lin et al., 2024)\n0.824\n0.862\n0.288\n0.066\n0.105\n0.038\n0.865\n0.897\n0.289\n0.054\n0.073\n0.023\nACE (Han et al., 2024)\n0.847\n0.877\n0.299\n0.104\n0.079\n0.028\n0.856\n0.899\n0.304\n0.127\n0.065\n0.023\nOmniGen (Xiao et al., 2024b)\n0.766\n0.819\n0.299\n0.123\n0.160\n0.065\n0.821\n0.863\n0.289\n0.087\n0.116\n0.045\nMIGE (ours)\n0.832\n0.865\n0.306\n0.114\n0.088\n0.027\n0.889\n0.905\n0.306\n0.119\n0.055\n0.013\n/only_edit data\n0.785\n0.841\n0.302\n0.104\n0.117\n0.046\n0.796\n0.862\n0.300\n0.111\n0.094\n0.036\n/wo_VAE feature\n0.799\n0.846\n0.300\n0.098\n0.103\n0.035\n0.811\n0.868\n0.299\n0.105\n0.098\n0.036\n/wo_multimodal instruction\n0.592\n0.723\n0.280\n0.113\n0.177\n0.074\n0.548\n0.714\n0.277\n0.135\n0.170\n0.066\nTable 2: Quantitative results for instruction-based image editing on Emu Edit test set and MagicBrush test set.\nMIGE achieves the best overall performance on MagicBrush test set among universal models, with strong instruction\nfidelity and detail preservation.\nMethods\nEditing\nSubject Preserving\nInstruction DINO ↑CLIP-I ↑CLIP-T ↑DINO ↑CLIP-I ↑\nsource-target\n0.668\n0.842\n0.271\nPBE (Yang et al., 2023)\n%\n0.810\n0.885\n0.304\n0.521\n0.792\nTIGIC (Li et al., 2025)\n%\n0.789\n0.874\n0.313\n0.453\n0.744\nMADD (He et al., 2024)\n%\n0.736\n0.852\n0.284\n0.446\n0.742\nOmniGen (Xiao et al., 2024b)\n\"\n0.802\n0.868\n0.296\n0.580\n0.792\nMIGE(ours)\n\"\n0.863\n0.909\n0.307\n0.652\n0.834\n/subject data + edit data\n0.789\n0.873\n0.299\n0.503\n0.764\n/only_compositional data\n0.780\n0.860\n0.303\n0.585\n0.808\n(a) Results on instruction-based subject replacement.\nMethods\nEditing\nSubject Preserving\nInstruction DINO ↑CLIP-I ↑CLIP-T ↑DINO ↑CLIP-I ↑\nsource-target\n0.783\n0.903\n0.295\nPBE (Yang et al., 2023)\n%\n0.843\n0.908\n0.321\n0.495\n0.794\nTIGIC (Li et al., 2025)\n%\n0.840\n0.901\n0.325\n0.455\n0.753\nMADD (He et al., 2024)\n%\n0.885\n0.930\n0.316\n0.519\n0.785\nOmniGen (Xiao et al., 2024b)\n\"\n0.791\n0.870\n0.312\n0.605\n0.814\nMIGE(ours)\n\"\n0.909\n0.940\n0.322\n0.638\n0.838\n/subject data + edit data\n0.807\n0.895\n0.304\n0.309\n0.683\n/only_compositional data\n0.879\n0.928\n0.324\n0.577\n0.820\n(b) Results on instruction-based subject addition.\nTable 3: Quantitative results on instruction-based subject-driven editing. Methods marked with a cross in\nthe instruction column use masks, while the others generate images using multimodal instructions. Overall, MIGE\nsignificantly outperforms others in both tasks, showcasing superior editing and subject preservation abilities.\nshown in Tables 3a and 3b. We computed DINO\nand CLIP-I metrics between the source and target\nimages, as well as the CLIP-T metric between the\nsource image and target caption, shown in the first\nrow of the table (denoted as “source-target”) as a\nbaseline. In terms of editing ability, MIGE achieves\nthe highest overall improvement across all metrics,\ndemonstrating its effectiveness in performing edits\nguided by multimodal instructions. As shown in\nthe qualitative comparison in Figure 6, MIGE un-\nderstands the meaning of “replace” in instructions,\nrather than simply pasting the entity onto the im-\nage. For subject preservation, the results show that\nMIGE achieves the best performance in both tasks,\nas demonstrated in Figure 6.\n4.3\nFurther Analysis\n4.3.1\nEffectiveness of Joint Training\nTo assess the effectiveness of joint training, we\ntrain models separately on individual datasets (de-\nnoted as “only_subject data,” “only_edit data,” and\n“only_compositional data”) and compare their per-\nformance with the jointly trained model. The re-\nsults in Table 1 and Table 2 show that joint training\nleads to consistent improvements across all metrics,\ndemonstrating that subject-driven generation and\ninstruction-based editing reinforce each other. As\nshown in Table 3, joint training also enhances the\nperformance of the compositional new task, further\nhighlighting its overall benefits. These findings em-\nphasize both the effectiveness and necessity of joint\ntraining. In conclusion, joint training of subject-\ndriven generation and instruction-based editing\nwithin our unified framework not only boosts\ncompositional capability but also improves the\nperformance of each individual task.\n4.3.2\nEffectiveness of Feature Fusing\nMIGE employs a feature fusion mechanism in the\nmultimodal encoder to integrate semantic features\nfrom ViT and visual features from VAE. As shown\nin Table 1 and Table 2, compared to the model\nwithout VAE features (denoted as “wo_VAE fea-\nture”), incorporating VAE features significantly im-\nproves detail preservation in reference images, ben-\nefiting both subject-driven image generation and\ninstruction-based image editing. This is particu-\nlarly evident in the improved CLIP-I and DINO\nscores and the significant reduction in L1 and L2\nmetrics, demonstrating that the inclusion of addi-\n\n\nOurs\nPBE\nOmniGen\nTIGIC\nMADD\nMultimodal Instruction\nAdd<imagehere>to the lower left \nof<imagehere>. The tent is medium-sized \nand is positioned near a stream with \ngrass and logs around it.\nAdd<imagehere>to the bottom center \nof<imagehere>. The man occupies a medium \nportion of the image, positioned \ncentrally, with a camera and easel nearby.\nAdd<imagehere>to the left of<imagehere>. \nThe bird occupies a medium size portion \nof the image.\nreplace the bird \nin<imagehere>with<imagehere>\nreplace the glass \nin<imagehere>with<imagehere>at the \nbottom\nreplace the juice glass \nin<imagehere>with<imagehere>in the \nmiddle\nFigure 6: Qualitative results on the benchmark for the subject addition and subject replacement. The upper\nsection compares subject addition results, while the lower section compares subject replacement. During testing,\nthe <imagehere> placeholder in the multimodal instruction is replaced according to the image sequence. MIGE\ndemonstrates flexibility in editing and excels in subject preservation ability and input-output consistency.\ntional visual features helps maintain consistency\nbetween the input and output.\n4.3.3\nEffectiveness of Instruction-based\nSubject-driven Image Editing Dataset\nJoint training in subject-driven image generation\nand instruction-based image editing enables gener-\nalization to instruction-based subject-driven image\nediting (denoted as “subject data + edit data”). To\nenhance the capability of MIGE in this new task,\nparticularly in understanding spatial terms and size\ndescriptions, we constructed a task-specific dataset\nfor joint training. As shown in Table 3a and Table\n3b, the task-specific data significantly improved\nthe model’s overall abilities. This demonstrates\nthe effectiveness of our constructed dataset, and\nthe proposed data generation pipeline serves as a\nvaluable reference for future dataset construction.\n4.3.4\nEffectiveness of Multimodal Instruction\nExisting instruction-based editing works (Brooks\net al., 2023; Sheynin et al., 2024; Zhang et al.,\n2024) typically use text instructions as the con-\nditional input, while we extend this to multimodal\ninstructions. To measure the benefit of multimodal\ninstructions, we trained the model with text-only\nediting instructions for comparison. As shown in\nTable 2, using multimodal instructions consistently\nimproves performance over text-only instructions\n(denoted as “wo_multimodal instruction”). This\nenhances input-output consistency and instruction-\nfollowing ability in multi-task training. The sig-\nnificant improvement in the L1 and L2 metrics\nindicates finer control over images and more ac-\ncurate edits. While text-only instructions provide\nthe necessary changes, the high CLIPdir score and\nlower values in other metrics show that multimodal\ninstructions add visual context, enabling more pre-\ncise and faithful modifications.\n5\nConclusion\nWe present MIGE, a unified framework that com-\nbines subject-driven generation and instruction-\nbased editing. By leveraging multimodal instruc-\ntions and conditional input, MIGE enables joint\ntraining that reinforces both tasks. This approach\nimproves task synergy and addresses data scarcity.\nJoint training also unlocks new capabilities like\ninstruction-based subject-driven image editing. We\nintroduce pipelines for this new task to construct\ntraining data and MIGEBench for evaluation. Our\nexperiments show that joint training leads to signif-\nicant improvements in subject fidelity and instruc-\ntion adherence, demonstrating the effectiveness of\nunifying these tasks. This integration enhances\ncontrollability and offers promising directions for\nfuture multimodal image generation and editing.\n\n\n6\nLimitations\nOur model faces common challenges in context\naccuracy, complex composition, and visual faithful-\nness, especially in multi-entity tasks. Ethical con-\ncerns, like deepfakes, also arise. Nevertheless, our\nframework shows promise in enabling unified con-\ntrolled image generation. Instruction-based subject-\ndriven generation remains challenging, with diffi-\nculties in handling spatial relationships and adjust-\ning subject sizes to fit the background. Addition-\nally, the technology carries the risk of generating\nmisleading images, but our work is intended solely\nfor research, emphasizing its exploratory nature\nrather than real-world deployment.\nReferences\nAbubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,\nAbdulrahman Alfozan, and James Zou. 2019. Gradio:\nHassle-free sharing and testing of ml models in the\nwild. arXiv preprint arXiv:1906.02569.\nTim Brooks, Aleksander Holynski, and Alexei A Efros.\n2023. Instructpix2pix: Learning to follow image edit-\ning instructions. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 18392–18402.\nAlper Canberk, Maksym Bondarenko, Ege Ozguroglu,\nRuoshi Liu, and Carl Vondrick. 2025. Erasedraw:\nLearning to insert objects by erasing them from im-\nages. In European Conference on Computer Vision,\npages 144–160. Springer.\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei\nYao, Enze Xie, Yue Wu, Zhongdao Wang, James\nKwok, Ping Luo, Huchuan Lu, et al. 2023. Pixart-\nα: Fast training of diffusion transformer for pho-\ntorealistic text-to-image synthesis. arXiv preprint\narXiv:2310.00426.\nXi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye\nKim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan\nZhao, Yilin Wang, et al. 2024. Unireal: Universal\nimage generation and editing via learning real-world\ndynamics. arXiv preprint arXiv:2412.07774.\nYisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyung-\nwon Choi, and Jinwoo Shin. 2024. Improving diffu-\nsion models for authentic virtual try-on in the wild.\narXiv preprint arXiv:2403.05139.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2024. Scaling instruction-finetuned language models.\nJournal of Machine Learning Research, 25(70):1–53.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas Müller, Harry Saini, Yam Levi, Do-\nminik Lorenz, Axel Sauer, Frederic Boesel, et al.\n2024. Scaling rectified flow transformers for high-\nresolution image synthesis. In Forty-first Interna-\ntional Conference on Machine Learning.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell\nWu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. 2023. Eva: Exploring the limits of\nmasked visual representation learning at scale. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 19358–\n19369.\nYuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying\nShan. 2024. Seed-data-edit technical report: A hy-\nbrid dataset for instructional image editing. arXiv\npreprint arXiv:2405.04007.\nZhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang,\nChaojie Mao, Chenwei Xie, Yu Liu, and Jingren\nZhou. 2024. Ace: All-round creator and editor fol-\nlowing instructions via diffusion transformer. arXiv\npreprint arXiv:2410.00086.\nJixuan He, Wanhua Li, Ye Liu, Junsik Kim, Donglai\nWei, and Hanspeter Pfister. 2024. Affordance-aware\nobject insertion via mask-aware dual diffusion. arXiv\npreprint arXiv:2412.14462.\nHexiang Hu, Kelvin CK Chan, Yu-Chuan Su, Wenhu\nChen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue\nBen, Boqing Gong, William Cohen, et al. 2024.\nInstruct-imagen: Image generation with multi-modal\ninstruction. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 4754–4763.\nMude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi,\nHeng Wang, Peng Wang, Yuyin Zhou, and Cihang\nXie. 2024.\nHq-edit: A high-quality dataset for\ninstruction-based image editing.\narXiv preprint\narXiv:2404.09990.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford,\net al. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276.\nDiederik P Kingma. 2013. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen Lo,\net al. 2023. Segment anything. In Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pages 4015–4026.\nBlack Forest Labs. 2023. Flux. https://github.com/\nblack-forest-labs/flux.\nDongxu Li, Junnan Li, and Steven Hoi. 2024a. Blip-\ndiffusion: Pre-trained subject representation for con-\ntrollable text-to-image generation and editing. Ad-\nvances in Neural Information Processing Systems,\n36.\n\n\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023a. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In International conference on ma-\nchine learning, pages 19730–19742. PMLR.\nPengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu,\nYuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang,\nand Feng Zheng. 2025.\nTuning-free image cus-\ntomization with image and text guidance. In Eu-\nropean Conference on Computer Vision, pages 233–\n250. Springer.\nTianle Li, Max Ku, Cong Wei, and Wenhu Chen. 2023b.\nDreamedit: Subject-driven image editing.\narXiv\npreprint arXiv:2306.12624.\nWei Li, Xue Xu, Jiachen Liu, and Xinyan Xiao. 2024b.\nUNIMO-G: Unified image generation through multi-\nmodal conditional diffusion. In Proceedings of the\n62nd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers),\npages 6173–6188, Bangkok, Thailand. Association\nfor Computational Linguistics.\nWeifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shi-\ntian Zhao, Siyuan Huang, Junlin Xie, Yu Qiao, Peng\nGao, and Hongsheng Li. 2024. Pixwizard: Versatile\nimage-to-image visual assistant with open-language\ninstructions. arXiv preprint arXiv:2409.15278.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei\nYang, Hang Su, et al. 2025. Grounding dino: Mar-\nrying dino with grounded pre-training for open-set\nobject detection. In European Conference on Com-\nputer Vision, pages 38–55. Springer.\nI Loshchilov. 2017. Decoupled weight decay regulariza-\ntion. arXiv preprint arXiv:1711.05101.\nXichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. 2023. Kosmos-g: Gen-\nerating images in context with multimodal large lan-\nguage models. arXiv preprint arXiv:2310.02992.\nWilliam Peebles and Saining Xie. 2023. Scalable diffu-\nsion models with transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pages 4195–4205.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nTianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kun-\nchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang\nChen, Feng Yan, et al. 2024. Grounded sam: As-\nsembling open-world models for diverse visual tasks.\narXiv preprint arXiv:2401.14159.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages\n10684–10695.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael\nPritch, Michael Rubinstein, and Kfir Aberman. 2023.\nDreambooth: Fine tuning text-to-image diffusion\nmodels for subject-driven generation. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 22500–22510.\nShelly Sheynin, Adam Polyak, Uriel Singer, Yuval\nKirstain, Amit Zohar, Oron Ashual, Devi Parikh,\nand Yaniv Taigman. 2024. Emu edit: Precise image\nediting via recognition and generation tasks. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8871–8879.\nJing Shi, Ning Xu, Trung Bui, Franck Dernoncourt,\nZheng Wen, and Chenliang Xu. 2020. A benchmark\nand baseline for language-driven image editing. In\nProceedings of the Asian Conference on Computer\nVision.\nRoman\nSuvorov,\nElizaveta\nLogacheva,\nAnton\nMashikhin, Anastasia Remizova, Arsenii Ashukha,\nAleksei Silvestrov, Naejin Kong, Harshith Goka,\nKiwoong\nPark,\nand\nVictor\nLempitsky.\n2022.\nResolution-robust large mask inpainting with fourier\nconvolutions.\nIn Proceedings of the IEEE/CVF\nwinter conference on applications of computer\nvision, pages 2149–2159.\nZhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu\nXue, and Xinchao Wang. 2024. Ominicontrol: Mini-\nmal and universal control for diffusion transformer.\narXiv preprint arXiv:2411.15098, 3.\nMosaicML NLP Team et al. 2023. Introducing mpt-\n7b: A new standard for open-source, commercially\nusable llms.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-\nhao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei\nDu, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang\nZhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-\nvl: Enhancing vision-language model’s perception\nof the world at any resolution.\narXiv preprint\narXiv:2409.12191.\nGuangxuan Xiao, Tianwei Yin, William T Freeman,\nFrédo Durand, and Song Han. 2024a. Fastcomposer:\nTuning-free multi-subject image generation with lo-\ncalized attention. International Journal of Computer\nVision, pages 1–20.\nShitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan,\nXingrun Xing, Ruiran Yan, Shuting Wang, Tiejun\nHuang, and Zheng Liu. 2024b. Omnigen: Unified\nimage generation. arXiv preprint arXiv:2409.11340.\n\n\nLe Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan,\nSenthil Purushwalkam, Honglu Zhou, Viraj Prabhu,\nYutong Dai, Michael S Ryoo, et al. 2024. xgen-mm\n(blip-3): A family of open large multimodal models.\narXiv preprint arXiv:2408.08872.\nBinxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xue-\njin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen.\n2023. Paint by example: Exemplar-based image edit-\ning with diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 18381–18391.\nYicheng Yang, Pengxiang Li, Lu Zhang, Liqian Ma,\nPing Hu, Siyu Du, Yunzhi Zhuge, Xu Jia, and\nHuchuan Lu. 2024. Dreammix: Decoupling object\nattributes for enhanced editability in customized im-\nage inpainting. arXiv preprint arXiv:2411.17223.\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang.\n2023. Ip-adapter: Text compatible image prompt\nadapter for text-to-image diffusion models. arXiv\npreprint arXiv:2308.06721.\nAhmet Burak Yildirim, Vedat Baday, Erkut Erdem,\nAykut Erdem, and Aysegul Dundar. 2023.\nInst-\ninpaint: Instructing to remove objects with diffusion\nmodels. arXiv preprint arXiv:2304.03246.\nKai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and\nYu Su. 2024. Magicbrush: A manually annotated\ndataset for instruction-guided image editing.\nAd-\nvances in Neural Information Processing Systems,\n36.\nHaozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si,\nRujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing\nLi, and Baobao Chang. 2024. Ultraedit: Instruction-\nbased fine-grained image editing at scale.\narXiv\npreprint arXiv:2407.05282.\n\n\nA\nBenchmark Construction\nInstruction-based subject-driven image editing is a\nnovel task lacking evaluation benchmarks. Previ-\nous works (Li et al., 2023b; Yang et al., 2024) did\nnot support instructions and lacked ground truth tar-\nget images. To address this, we designed a bench-\nmark construction pipeline with multiple manual\ninspections to ensure image and caption quality.\nAs shown in Figure 7, we first filter valid edit-\ning pairs using a Gradio (Abid et al., 2019) in-\nterface, as shown in Figure 8. Then, Qwen2.5-\n14B-Instruct extracts entity names and constructs\nmultimodal instructions. After manually review-\ning object names, we use Grounding DINO (Liu\net al., 2025) for bounding boxes and SAM for seg-\nmentation. Cropped entities are saved with black\nand white backgrounds for diverse test scenarios.\nPrompts are detailed in Appendix E.\nFor evaluation, Qwen2-VL-7B generates cap-\ntions that compare the generated images with their\nground truth counterparts. These captions are then\nmanually reviewed to identify and remove any un-\nrecognized entities. This review process ensures\nthat the generated results are accurate and reliable.\nThe final benchmark consists of 500 samples for\nboth subject addition and replacement tasks. By\ncarefully avoiding artifacts from inpainting and in-\ncorporating multiple rounds of manual review, we\nguarantee the high quality of the data. Figures 9\nand 10 display some example pairs from the bench-\nmark to illustrate the evaluation process.\nEntity Extract & Multimodal Instruction Construct\n(Qwen2.5-14B-Instruct)\nEntity Segment\n(Grounded SAM)\n\"replace the woman in \n<imagehere> with <imagehere>\"\nTarget Image Caption Generate\n(Qwen2-VL-7B-Instruct)\n\"replace a woman with a man\"\n\"The image shows a red tram with \nthe number 223 on it. A man is \nstanding on the platform, \nlooking up at the tram. There \nare several people walking on \nthe sidewalk in the background.\"\n\"a man\"\nFigure 7: The pipeline of benchmark construction.\nB\nDataset Construction\nTo enable joint training of MIGE, we designed a\nseries of pipelines for data construction and pro-\ncessing. Our dataset includes three tasks: subject-\ndriven image generation, instruction-based image\nediting, and instruction-based subject-driven image\nediting. The proportions of the training data are\nshown in Figure 11. In this section, we will detail\nthe pipelines for processing each data component\nand other related details.\n使⽤ Gradio 构建\nreplace the human to flowerpot with stand\nSave and Next\nSave All\nEntity Image\nImage\nCaption\nObject\nMulti Prompt\nTarget Caption\nFigure 8: The annotation interface for benchmarks is\nbuilt with Gradio.\nEntity\nSource Image\nTarget Image\nFigure 9: Subject addition examples in our benchmark.\nEntity\nSource Image\nTarget Image\nFigure 10: Subject replacement examples in our bench-\nmark.\nSubject-driven image \ngeneration\n40%\nInstruction-based \nimage editing\n49%\nInstruction-based \nsubject-driven image \nediting\n11%\nMultimodal \nInstruction \nDataset\nFigure 11: Composition of training data.\n\n\nSubject-driven Image Generation Data Con-\nstruction\nWe construct multimodal instruction-\nimage pairs by replacing entities in image captions\nwith corresponding images. The dataset is sourced\nfrom BLIP3-GROUNDING-50M (Xue et al.,\n2024) and internal data. For the former, we discard\nlow-quality pairs with CLIP-T scores below 0.255.\nEntities are filtered based on annotated bounding\nboxes, retaining those with sizes between 0.05 and\n0.8. After segmenting entities using SAM (Kirillov\net al., 2023), we further compute the CLIP-T score\nbetween “a photo of [entity]” and the segmented\nimage to mitigate incomplete pairs. For internal\ndata, we follow the KOSMOS-G and UNIMO-G\nprocessing pipelines,\nusing MPT-7B-Instruct\n(Team et al., 2023) to extract entities. The original\nimage and entity text are input into Grounded SAM\nto obtain entity images, which are filtered by a\nCLIP-T score above 0.255 and a size ratio between\n0.1 and 0.85. To enhance subject preservation, we\nincorporate 112,846 samples from subject200k, as\nin OminiControl (Tan et al., 2024).\nInstruction-based Image Editing Data Con-\nstruction\nPrevious works on instruction-based\nediting relied solely on text, covering object-level\nadditions, deletions, modifications, and global\nchanges in background or style. We integrated\ndata from InstructPix2Pix (Brooks et al., 2023),\nUltraEdit (Zhao et al., 2024), MagicBrush (Zhang\net al., 2024), SEED-Data-Edit (Ge et al., 2024),\nGIER (Shi et al., 2020), and HQ-Edit (Hui et al.,\n2024), filtering out instances with target image\naesthetic scores above 5.5. To enhance multimodal\ninstruction alignment, we reformulated text-based\ninstructions based on task type.\nSpecifically,\nfor instructions containing phrases like “in the\nimage” or “to the image,” we replaced them with\n“in<imagehere>” or “to<imagehere>.” For others,\nwe appended “of<imagehere>” for global edits or\n“in<imagehere>” for localized modifications.\nInstruction-based Subject-driven Image Edit-\ning Data Construction\nTo enhance instruction-\nbased subject-driven image editing, we propose\ndedicated data construction pipelines. For subject\naddition, we design a pipeline inspired by SAM-\nFB (He et al., 2024) to construct background im-\nages, foreground entities, and multimodal instruc-\ntions, as shown in Figure 4a. We start with filtering\nSA-1B (Kirillov et al., 2023) images with an aes-\nthetic score above 5 and select entities where the\nforeground-to-image ratio is between 0.1 and 0.5.\nQwen2-VL-7B (Wang et al., 2024) is used to verify\nentity completeness. The selected entities serve as\nforegrounds, while the backgrounds are inpainted\nusing LAMA (Suvorov et al., 2022). We then ex-\ntract foreground-related text using Qwen2-VL-7B\nand generate multimodal instructions with GPT-4o\n(Hurst et al., 2024). Due to resource constraints,\nwe process the first 500,000 SA-1B samples, yield-\ning 193,247 multimodal instruction-foreground-\nbackground pairs.\nFor subject replacement, as shown in Figure 4b,\nwe aim at constructing foreground entities and mul-\ntimodal instructions. We filter SEED-Data-Edit’s\npart1-unsplash based on predefined rules, retaining\nonly replace-task data where the match score in\nthe annotations exceeds 0.3 and the target image’s\naesthetic score is above 5.5. Qwen2.5-14B-Instruct\n(Team, 2024) identifies whether the main entity is\nbeing replaced. The target entity is replaced with\n<imagehere> to construct a multimodal prompt.\nUsing the annotated edited subject information, we\napply Grounded SAM for segmentation, obtain-\ning 79,693 multimodal instruction-replace entity\nimage-source image-target image pairs. In addi-\ntion, we construct 34,947 virtual try-on samples\nusing IDM-VTON (Choi et al., 2024).\nC\nEvaluation Details\nWe evaluated the subject-driven generation capabil-\nity of MIGE on DreamBench, which contains 750\nprompts covering 30 subjects. For each prompt,\nfour images were generated using seeds 0, 1, 2, and\n3, resulting in a total of 3,000 images. Following\nKOSMOS-G, we selected one image per prompt.\nThen we extracted the subject using Grounded\nSAM and used it as input, aligning with the training\nprocess. Subject fidelity was assessed using DINO\nand CLIP-I, while CLIP-T measured adherence to\nmultimodal instructions.\nFor instruction-based editing evaluation, the\nEmu Edit benchmark contains known issues, in-\ncluding incorrect image-caption pairs and duplicate\nsource-target captions. Prior works have handled\nthese inconsistencies differently, leading to incom-\nparable results. Therefore, we reimplemented all\ncurrently available open-source methods on both\ntest sets using a fixed random seed (seed=0) for\nconsistency. Consequently, CLIP-I, L1, and DINO\nwere computed on 3,589 Emu Edit test samples,\n\n\nwhile CLIPdir and CLIP-T were evaluated on 2,899\nsamples after removing problematic data.\nD\nRelated Work\nSubject-Driven\nImage\nGeneration\nRecent\nadvances in generative models have significantly\nimproved image customization, enabling finer\nand more flexible control over generated content.\nSubject-driven image generation, which preserves\nthe features of given images while creating\nnew content, has garnered increasing attention.\nDreamBooth (Ruiz et al., 2023) fine-tunes the\nentire U-Net using multiple images of the same\nsubject, making it resource-intensive. IP-Adapter\n(Ye et al., 2023) introduces a test-time tuning-free\napproach by decoupling image and text, training\nonly additional attention layers, but struggles to\nbalance prompt adherence and image fidelity. Fast-\nComposer (Xiao et al., 2024a) and Blip-Diffusion\n(Li et al., 2024a) leverage embeddings to condition\nimages on text but face challenges in subject\npreservation and efficiency with generalized\nvision-language inputs. KOSMOS-G (Pan et al.,\n2023) and UNIMO-G (Li et al., 2024b) use\nMultimodal Large Language Models (MLLMs) as\nmultimodal encoders for greater input flexibility\nbut require large datasets to train task-specific\nencoders and alignment networks.\nHowever,\nthese methods lack strong instruction-following\ncapabilities, limiting their scalability to new tasks\nand applications.\nInstruction-based Image Editing\nInstruction-\nbased image editing modifies images at the object\nlevel (e.g., adding, removing, or altering objects)\nor the image level (e.g., changing style or environ-\nment) based on user instructions. This approach\noffers greater flexibility than caption-based meth-\nods but demands stronger instruction-following ca-\npabilities. Inst-inpaint (Yildirim et al., 2023) fo-\ncuses on object removal, while EraseDraw (Can-\nberk et al., 2025) fine-tunes a model for object\naddition using data derived from removal tasks. In-\nstructPix2Pix (Brooks et al., 2023), trained on a\nlarge synthetic dataset, supports four types of ed-\nits, and MagicBrush (Zhang et al., 2024) refines\nthis capability by fine-tuning on real, manually an-\nnotated data. EmuEdit (Sheynin et al., 2024) en-\nhances diffusion-based editing with a task vector to\ndifferentiate tasks. However, these methods often\nstruggle with maintaining input-output consistency.\nE\nPrompts\nLLMs and MLLMs are integral to our training\ndata and benchmark construction pipeline, and the\nprompts used are presented in this section.\n\n\nin a chef outfit\nin the snow\nis wet\nwith a city in the background\nstyle\nbackground\naddition\nreplacement\nremoval\nInstruction-based subject addition\nInstruction-based subject replacement\non the beach\nFigure 12: Qualitative results of subject-driven image generation (top) , instruction-based image editing (middle),\nand instruction-based subject-driven image editing (bottom).\n\n\nPrompt for Qwen2-VL-7B to determine the completeness of subject in the image\nPrompt: Determine if the subject in the image is complete. If it is complete and not an abstract object\nsuch as background, grass, sky, tree, stone, or part of another item, please return True. Otherwise, return\nFalse.\nTable 4: Prompt design for subject completeness evaluation in an image. The model should identify whether the\nsubject is complete, not abstract elements.\nPrompt for GPT-4o to generate multimodal instruction\nPrompt: The object is {object_name}. It is located in a bounding box with coordinates ({x}, {y},\n{w}, {h}) on an image of size {width}x{height}. Describe its size, relative position, and relation to\nsurrounding objects. Avoid describing the overall scene or unrelated elements.\nYour response should start with “Add <imagehere> to the [position] of <imagehere>”. (The first\n<imagehere> indicates the object and the second indicates the image.) Keep the <imagehere> symbol in\nthe first sentence in your reply.\nAnswer briefly in two sentences:\nTable 5: Prompt design for GPT-4o to generate multimodal instruction for subject addition training data.\nPrompt for Qwen2.5-14B-Instruct to determine whether an edit instruction pertains to the main subject of\nan image\nPrompt: You are an assistant that determines whether an edit instruction pertains to the main subject of\nan image.\nThe main subject refers to humans or animals only. If the edit instruction is related to the main subject,\nrespond with ’yes’.\nIf it pertains to background, large areas of vegetation, or environmental information, respond with ’no’.\nHere are some examples:\n1. replace the grass with sand →no\n2. replace the trees with palm trees →no\n3. replace the dirt road with a cobblestone path →no\n4. replace the bird with a parrot →yes\nNow, analyze the following instruction and respond accordingly: {instruction}\nTable 6: Prompt design for determining if an edit instruction pertains to the main subject of the image. The model\nevaluates whether the instruction relates to humans, animals, or other environmental features.\n\n\nPrompt for extracting the main subject and modifying the edit instruction\nPrompt 1: Replace Examples\nYou are an assistant that determines the main subject of an edit instruction and outputs the extracted main\nsubject along with a modified prompt.\nGiven an edit instruction like “replace X with Y”:\n- Extract “Y” (the replacement subject).\n- Output the extracted subject.\n- Modify the instruction by replacing “Y” with <imagehere> in the original prompt and output the new\ninstruction.\nExamples:\n1. replace one woman with a man →Output: a man, replace one woman with <imagehere>\n2. replace the castle with another castle →Output: castle, replace the castle with <imagehere>\n3. replace the desk with a white one in the bottom middle →Output: desk, replace the desk with\n<imagehere> in the bottom middle\nPrompt 2: Add Examples\nYou are an assistant that extracts the added subject in the edit instruction.\nGiven an edit instruction like “add X to the image”:\n- Extract “X” (the added subject).\n- Output the extracted subject.\n- Modify the instruction by replacing “X” with <imagehere> in the original prompt and output the new\ninstruction.\nHere are some examples:\n1. add human over the stone in the bottom left →Output: human, add <imagehere> over the stone in the\nbottom left\n2. add a car on the road at the bottom →Output: a car, add <imagehere> on the road at the bottom\n3. Add an owl on the left shoulder →Output: an owl, Add <imagehere> on the left shoulder\nTable 7: Prompt designs for extracting and modifying edit instructions based on either replacement or addition of\nsubjects.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21291v2.pdf",
    "total_pages": 18,
    "title": "MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing",
    "authors": [
      "Xueyun Tian",
      "Wei Li",
      "Bingbing Xu",
      "Yige Yuan",
      "Yuanzhuo Wang",
      "Huawei Shen"
    ],
    "abstract": "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https://github.com/Eureka-Maggie/MIGE.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}