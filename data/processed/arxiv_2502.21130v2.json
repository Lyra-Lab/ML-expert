{
  "id": "arxiv_2502.21130v2",
  "text": "Fast and Accurate Gigapixel Pathological Image Classification\nwith Hierarchical Distillation Multi-Instance Learning\nJiuyang Dong1,\nJunjun Jiang1*,\nKui Jiang1,\nJiahan Li1 ,\nYongbing Zhang2*\n1Harbin Institute of Technology, 2Harbin Institute of Technology, Shenzhen\n{jiuyang.dong, jiahan.li}@stu.hit.edu.cn,\n{jiangjunjun, jiangkui, ybzhang08}@hit.edu.cn\nAbstract\nAlthough multi-instance learning (MIL) has succeeded in\npathological image classification, it faces the challenge of\nhigh inference costs due to processing numerous patches\nfrom gigapixel whole slide images (WSIs).\nTo address\nthis, we propose HDMIL, a hierarchical distillation multi-\ninstance learning framework that achieves fast and accu-\nrate classification by eliminating irrelevant patches. HD-\nMIL consists of two key components: the dynamic multi-\ninstance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-\nresolution WSIs, while LIPN operates on the corresponding\nlow-resolution counterparts. During training, DMIN are\ntrained for WSI classification while generating attention-\nscore-based masks that indicate irrelevant patches. These\nmasks then guide the training of LIPN to predict the rele-\nvance of each low-resolution patch. During testing, LIPN\nfirst determines the useful regions within low-resolution\nWSIs, which indirectly enables us to eliminate irrelevant re-\ngions in high-resolution WSIs, thereby reducing inference\ntime without causing performance degradation. In addition,\nwe further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology,\nwhich enhances the performance of HDMIL through learn-\nable activation layers. Extensive experiments on three pub-\nlic datasets demonstrate that HDMIL outperforms previ-\nous state-of-the-art methods, e.g., achieving improvements\nof 3.13% in AUC while reducing inference time by 28.6%\non the Camelyon16 dataset.\nThe project is available at\nhttps://github.com/JiuyangDong/HDMIL.\n1. Introduction\nRecently, multi-instance learning (MIL) has emerged as the\nleading approach for analyzing pathological whole slide\n*Corresponding author: Junjun Jiang, Yongbing Zhang\nrelevant\ninstances\n(b)\n0\n1\n(a)\nFigure 1. What makes inference slow? (a) Time-consuming\ndata pre-processing: After comparing the time required for data\npre-processing (WSI cropping, feature extraction) and MIL net-\nwork classification, it is clear that data pre-processing is the main\nspeed bottleneck. (b) Redundant irrelevant patches: For ex-\nample, in a randomly selected WSI, numerous instances have ex-\ntremely low attention scores [19], indicating their minimal contri-\nbution, if any, to the bag-level classification.\nimages (WSIs), demonstrating significant success in tasks\nsuch as tumor detection, subtyping [10, 27, 34, 41, 47, 61],\ntissue micro-environment quantification [13, 20, 35, 38, 44,\n45], and survival prediction [6, 46, 57, 59].\nTo handle gigapixel WSIs, the MIL framework treats\neach WSI as a bag, cropping it into thousands of patches,\neach treated as an instance. Before being fed into the MIL\nnetworks for classification, all patches need to undergo fea-\nture extraction. Considering that each WSI contains thou-\nsands of patches, the process of WSI cropping and feature\nextraction can be very time-consuming. As shown in Fig. 1,\ndata pre-processing is the primary speed bottleneck, requir-\ning hundreds of times more time than the MIL classifiers.\nMoreover, WSIs often contain redundant patches with min-\nimal contribution to the bag-level classification. For exam-\nple, by adding up the attention scores of only a small frac-\narXiv:2502.21130v2  [cs.CV]  3 Mar 2025\n\n\ntion (about 10%) of the patches in the selected WSI, we can\nobtain 99% of the total attention scores. Therefore, the re-\nmaining patches can be safely considered as irrelevant and\nremoved without affecting the performance.\nBased on the above analysis, a straightforward idea to\nreduce the inference time is discarding irrelevant instances\nbased on attention scores. Unfortunately, existing MIL al-\ngorithms need to extract the features of all cropped patches\nbefore calculating their attention scores, which brings up\nthe “chicken and egg” problem. To accelerate WSI clas-\nsification, Yu et al. proposed SMT [60]. Instead of crop-\nping each WSI into patches, SMT employs cascading vi-\nsion transformer (ViT) blocks to gradually search for “sus-\npicious” areas and ultimately uses only a small area of\nthe entire WSI for classification. As pointed out by Yu et\nal., the classification performance of SMT heavily relies\non accurately identifying potential tumor areas. However,\nthe pathological information provided by the low-resolution\nthumbnails, used as the initial input of SMT, is insufficient,\nwhich can easily lead to inappropriate regions of interest\nbeing focused. Consequently, the accumulation of errors\nresults in inferior classification performance of SMT when\ncompared to other non-accelerated MIL methods.\nIn this paper, we propose a hierarchical distillation multi-\ninstance learning (HDMIL) framework aiming to quickly\nidentify irrelevant patches and thus achieve fast and ac-\ncurate classification.\nDuring training, instance-level fea-\ntures extracted from all cropped patches in the high-\nresolution WSIs are leveraged to train a dynamic multi-\ninstance network (DMIN) with a self-distillation strategy.\nThis self-distillation strategy constrains the teacher and stu-\ndent branches in DMIN, which use all and partial in-\nstances for classification respectively, to obtain consistent\nresults, thus making the student branch selected instances\nnon-irrelevant. Afterwards, we can obtain a binary mask\nfor each instance depending on whether the instance is con-\nsidered relevant to the slide classification. The masks are\nthen utilized to guide the training of a lightweight instance\npre-screening network (LIPN), which learns to identify the\nbinary relevance of each patches in the corresponding low-\nresolution WSIs. During testing, after LIPN indicates irrel-\nevant low-resolution patches, we can determine which high-\nresolution patches can be skipped, thereby saving infer-\nence time. Furthermore, a Chebyshev-polynomials-based\nKolmogorov-Arnold (CKA) classifier is designed for more\naccurate classification, where learnable activation layers\nhave powerful capabilities.\nOverall, this paper makes three key contributions:\n• This paper offers a crucial insight: eliminating irrelevant\ninstances not only speeds up the inference process but\nalso improves the classification performance. This find-\ning challenges the conventional trade-off between speed\nand performance and provides valuable inspiration for fu-\nture research in multi-instance classification.\n• We are the first to propose and apply the Chebyshev-\npolynomials-based\nKolmogorov-Arnold\nclassifier\nto\ncomputational pathology, which can greatly improve the\nclassification performance.\n• Extensive experiments on three public datasets demon-\nstrate the effectiveness of our method. For example, on\nthe Camelyon16 dataset, HDMIL achieves an AUC of\n90.88% and an accuracy of 88.61%, outperforming pre-\nvious best methods by 3.13% and 3.18%, respectively.\nMoreover, the inference time was reduced by 28.6%.\n2. Related Work\nMIL for WSI Classification.\nMIL for WSI classifica-\ntion can be divided into two categories: instance-based and\nembedding-based. Instance-based methods [9, 21, 24, 36,\n39, 42, 63] first classify each instance and then aggregate\nthe predictions using Max-Pooling, Mean-Pooling, or other\npre-defined pooling operations to generate the final bag-\nlevel prediction. Embedding-based methods [10, 19, 27,\n34, 47, 57, 61] use networks to assess the significance of\neach instance and weight all instances accordingly, produc-\ning the bag-level representation for classification. For the\nembedding-based methods, it is observed that different in-\nstances within each WSI have varying contributions to the\nbag-level representation. Building on this observation, we\ndesign the HDMIL framework to achieve fast and accurate\nclassification by selectively removing irrelevant instances.\nDynamic Neural Networks. Dynamic neural networks [4,\n14–18, 28, 30, 51, 56] can adjust their architecture dynam-\nically according to the input data, thereby controlling the\ncomputational redundancy adaptively. In the era of Visual\nTransformers, many studies [31, 37, 43, 48, 53, 62] have\nattempted to improve inference efficiency by reducing to-\nken redundancy. In addition to bridging the gap in the field\nof computational pathology by utilizing dynamic networks\nto reduce instances and speed up inference, our HDMIL\nalso addresses the aforementioned “chicken or egg” prob-\nlem. This problem cannot be resolved using existing dy-\nnamic networks that solely rely on end-to-end training.\nKolmogorov-Arnold Networks. Most previous studies [8,\n23, 25, 26, 32, 49] before KAN [33] used the original 2-\nlayer structure to explore the possibility of constructing\nneural networks based on the Kolmogorov-Arnold repre-\nsentation theorem. KAN extended this theorem to networks\nof arbitrary width and depth, exploring its potential as a fun-\ndamental model of “AI+Science”. Subsequent research has\nprimarily focused on improving the integration of KAN into\nvarious tasks [11, 12, 22, 29, 52, 54] or modify its architec-\nture [1, 3, 5, 50, 55, 58]. In this paper, we propose to re-\nplace the spline function in KAN with first-kind Chebyshev\npolynomials to develop a more powerful MIL classifier for\nreal-world pathological image classification.\n\n\n3. Method\nAs illustrated in Fig. 2, our proposed HDMIL framework\nmainly consists of two stages: training and inference. As\nshown in Fig. 2(a), in the training stage, we first employ\na self-distillation training strategy to train the DMIN on\nhigh-resolution WSIs for bag-level classification and in-\ndicating irrelevant regions.\nWith the guidance from the\ntrained DMIN, we perform cross-distillation training to\nget LIPN using low-resolution WSIs, which achieves dis-\ncrimination of the binary importance (important or not) of\neach region with extremely low computational cost. In the\ninference stage, as shown in Fig. 2(b), LIPN relies on low-\nresolution WSIs to quickly identify regions that are irrele-\nvant to classification and discard the corresponding patches\nwithin high-resolution WSIs. Subsequently, the remaining\npatches are fed into the feature extractor and DMIN to gen-\nerate the classification results.\nBefore training, we first pre-process the input data fol-\nlowing the standard procedure for pathological WSIs [34].\nThe dataset {Xi}S\ni=1 comprises S WSI pyramids with slide\nlabels, where each Xi contains a pair of high-resolution\n(20×) and low-resolution (1.25×) WSIs, respectively re-\nferred to as Xi,HR and Xi,LR. It should be noted that WSI\npyramids typically contain WSIs at various magnification\nlevels ranging from 1.25× to 40×, but in this paper only\nthe two representative magnifications are utilized. After re-\nmoving the background regions, we get Ni pairs of 16×16\npatches from Xi,LR and 256×256 patches from Xi,HR.\n3.1. Self-Distillation Training of DMIN\nAs shown in Fig. 2(c), DMIN is designed to classify high-\nresolution WSIs and identify instances irrelevant to the\nbag-level classification. Specifically, DMIN comprises five\nmodules, namely, the projection module, attention module,\nteacher branch, student branch, and CKA classifiers.\nProjection and Attention Module. During training, all\npatches extracted from the high-resolution WSI Xi,HR are\nfed into a pre-trained feature extractor to generate a set of\ninstance-level features Ii,HR. Subsequently, Ii,HR is fed\ninto the projection module for dimensionality reduction,\nproducing a new feature set Fi,HR ∈RNi×Q, where Q de-\nnotes the dimensionality of the reduced features. Then, the\ndimension-reduced Fi,HR is fed into the attention module\nto compute the un-normalized attention scores:\nAi,HR = [ϕ(Fi,HRV ) ⊙σ(Fi,HRU)]W,\n(1)\nwhere ϕ(·) and σ(·) denote the tanh and sigmoid function.\nThe weight matrices U, V , and W are the learnable param-\neters. The attention module here uses the same dual branch\nattention network as CLAM [34]. In the binary classifica-\ntion tasks discussed in this paper, the attention matrices cor-\nresponding to the first and second categories are denoted as\nAi,HR,1 ∈RNi×1 and Ai,HR,2 ∈RNi×1, respectively.\nTeacher Branch. The dimension-reduced Fi,HR is then\nlinearly weighted by the attention matrix for each category\nto produce the bag-level representation, which are used for\nfinal classification:\nEtea\ni,HR,c = φ(Ai,HR,c)⊤⊗Fi,HR,c, c ∈{1, 2}.\n(2)\nHere φ(·) represents the softmax function and Etea\ni,HR,c ∈\nR1×Q denotes the bag-level representation corresponding\nto the c-th category in the teacher branch.\nStudent Branch and Self-Distillation. The student branch\nis designed to compute bag-level representations using only\na subset of instances with larger attention scores, and we\nimpose a constraint to ensure that the bag-level representa-\ntions in the student branch remain as consistent as possible\nwith the representations obtained in the teacher branch us-\ning all the instances. In this way, the attention module is en-\ncouraged to focus more on instances that are important for\nbag-level classification and filters out irrelevant instances.\nHowever, directly using instances with high atten-\ntion scores is a discrete operation, resulting in a non-\ndifferentiable problem during optimization.\nTo address\nthis issue, we employ the Gumbel trick [43] to selectively\nchoose instances with higher attention scores for end-to-end\ntraining. First, we incorporate the Gumbel Noise [18] to\n“sigmoid” the un-normalized attention matrices:\nˆAi,HR,c = σ(Ai,HR,c + G1,c −G2,c\nτ\n), c ∈{1, 2}.\n(3)\nHere σ represents the sigmoid function, G1,c ∈RNi×1 and\nG2,c ∈RNi×1 are two noises matrices randomly sampled\nfrom the Gumbel distribution, and τ is the temperature coef-\nficient. Next, we binarize the “sigmoided” attention scores\nin a differentiable way:\nM j\ni,HR,c = B( ˆAj\ni,HR,c, γ) −D( ˆAj\ni,HR,c) + ˆAj\ni,HR,c, (4)\nwhere M j\ni,HR,c ∈{0, 1} represents the mask value of\nthe j-th instance and γ denotes the threshold as a hyper-\nparameter. B(a, b) here represents the discrete binarization\nfunction, which equals 1 when a is greater than b, and 0 oth-\nerwise. D(·) represents the gradient truncation operation.\nFurthermore, we propose an attention masking mech-\nanism to eliminate the impact of instances with zero mask\nvalues on the bag-level representations:\nEstu\ni,HR,c =\nNi\nX\nj=1\nexp(Aj\ni,HR,c)M j\ni,HR,c\nPNi\ns=1 exp(As\ni,HR,c)M s\ni,HR,c\nF j\ni,HR,c, (5)\nwhere Estu\ni,HR,c ∈R1×Q represents the bag-level represen-\ntation of the c-th class in the student branch.\nCKA Classifier. In order to enhance the capacity of the\nMIL classifier, we propose to use the Kolmogorov-Arnold\n\n\nStudent Branch\nGumbel-\nSigmoid\nDifferentiable\nBinarization\n1\n0\n1\n0\n1\n0\n1\n0\nSoftmax\nSoftmax\nAttention\nModule\nTeacher Branch\nParameter Sharing\nParameter Sharing\nProjection\nModule\n(c)\nFeature\nExtractor\nAttention\nMasking\nDifferentiable\nBinarization\ncrop\n0\n0\n1\n1\n1\n0\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n0\n1\n0\n1\n1\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n1\n0\n1\n1\n1\n0\n0\nLIPN\nDMIN\nFeature\nExtractor\n×16×16×3\n×256×256×3\ncrop\nCorrespond \nSpatially\nFeature\nExtractor\nDMIN\nDifferentiable\nBinarization\nLIPN\ncrop\nSpatial \nCorrespondence\nSelectively \nCropping\n×256×256×3\n(b)\n(a)\nSelf-distillation \nTraining (Step1)\nCross-distillation \ntraining (Step2) \n×16×16×3\nFigure 2. Overview of our HDMIL framework. (a) During training, we start by utilize the high-resolution WSI Xi,HR for self-distillation\nof DMIN, enabling it to classify Xi,HR and generate per-instance mask Mi,HR which indicates the relevance of each region to the bag-\nlevel classification. Afterwards we froze DMIN and employ the masks Mi,HR to distill LIPN, which learns the contribution of each region\nusing the low-resolution Xi,LR. (b) During inference, the LIPN can identify which patches within Xi,HR need to be used for classification\nby evaluating Xi,LR. (c) The self-distillation training of DMIN on the high-resolution Xi,HR.\nnetwork to learn nonlinear activation functions instead of\nusing fixed activation functions in the classifier. Specifi-\ncally, we employ the iterative form of K-order Chebyshev\npolynomials to represents the basis functions TK(x):\nTK(x) = 2xTK−1(x) −TK−2(x), K ≥2.\n(6)\nHere x ∈R1×Q represents a bag-level representation,\nwhere the baseline condition are T0(x) = ⃗1 and T1(x) = x.\nBy multiplying the basis functions T(x) by the learnable\ncoefficients Ω∈RQ×O×(K+1), we can get the prediction\nof the classifier Φ(x):\nΦ(x)[o] =\nK\nX\nk=0\nQ\nX\nq=1\nTk(x)[q] ∗Ω[q, o, k],\n(7)\nwhere O represents the dimension of the prediction result.\nSince we use a dual-branch attention module, it’s natural\nto calculate the classification results for the two branches\nindividually, so O is equal to 1, and the predictions of the\nteacher branch and the student branch are:\n(eY tea\ni,HR = [Φ1(ϕ(Etea\ni,HR,1)) ⊕Φ2(ϕ(Etea\ni,HR,2))].\neY stu\ni,HR = [Φ1(ϕ(Estu\ni,HR,1)) ⊕Φ2(ϕ(Estu\ni,HR,2))].\n(8)\nHere ⊕represents the concatenation operation and the tanh\nfunction ϕ maps the input values of the CKA classifiers to\n[−1, 1], ensuring that the inputs meet the requirements of\nthe Chebyshev polynomial.\nHybrid Loss Function. The training objectives of DMIN\nare threefolds: 1) The teacher branch can correctly clas-\nsify Xi,HR; 2) The classification results of the student\nbranch (using partial instances) and teacher branch (us-\ning all instances) should be consistent; 3) The proportion\nof instances selected should be controllable. Specifically,\nwe first use the cross-entropy loss Ltea\ncls to ensure that the\nteacher branch performs accurate classification:\nLtea\ncls = CE(eY tea\ni,HR, Yi),\n(9)\nwhere CE(·) represents the cross entropy loss function and\nYi is the slide-level label of Xi. Next, we constrain the bag-\nlevel representation Estu\ni,HR and classification logit eY stu\ni,HR in\nthe student branch by knowledge distillation:\n(\nLstu\ndis,1 = L2(Estu\ni,HR, Etea\ni,HR),\nLstu\ndis,2 = LKL(eY stu\ni,HR, eY tea\ni,HR).\n(10)\nHere, L2(·) and LKL(·) denote the 2-norm and KL diver-\ngence loss function, respectively. Finally, we constrain the\nproportion of learned relevant instances eri,HR to be close to\na preset retention ratio r:\nLstu\nrate = L2(eri,HR, r).\n(11)\nHere, the j-th instance is considered relevant if either\nM j\ni,HR,1 or M j\ni,HR,2 are not zero. Additionally, we utilize\nthe clustering loss Ltea\nclu proposed in CLAM [34] to optimize\nthe feature space of DMIN. In conclusion, the hybrid loss\nfunction of DMIN is:\nLDMIN = α1Ltea\ncls +α2Ltea\nclu+α3Lstu\ndis,1+α4Lstu\ndis,1+α5Lstu\nrate.\n(12)\n\n\nFor the coefficients of different loss terms, we did not per-\nform hyper-parameter search, but empirically set α1 and α2\nto 0.7 and 0.3 according to CLAM [34], and set α3, α4, and\nα5 to 0.5, 0.5, and 2.0 according to DynamicViT [43].\n3.2. Cross-Distillation Training of LIPN\nAlthough DMIN can successfully identify irrelevant re-\ngions within WSIs, it does not improve the inference speed.\nThis is because DMIN needs to use all patches’ features\ngenerated by the feature extractor to determine which in-\nstances should be discarded.\nHowever, this patch-wise\nfeature extraction is actually the bottleneck for WSI in-\nference speed. To solve this problem, we propose using\nDMIN to distill LIPN, a lightweight instance pre-screening\nnetwork specifically tailored for low-resolution WSIs, as\nshown in Fig. 2(a).\nAfter training, LIPN can quickly\nidentify the irrelevant regions within low-resolution WSIs,\nthereby indirectly indicating the irrelevant patches within\nhigh-resolution WSIs.\nSpecifically, the Ni 16×16 patches obtained from Xi,LR\nare directly fed into LIPN, generating dual-branch predic-\ntion matrices Pi,LR,c, c ∈{1, 2} for the two categories.\nSince these low-resolution patches contain relatively little\ninformation, we do not require LIPN to learn the specific\ncontribution score of each patch to the bag-level classifica-\ntion like DMIN does. On the contrary, it is easier for LIPN\nto learn whether each patch contributes to the bag-level clas-\nsification or not. Therefore, Pi,LR,c is first binarized:\nM j\ni,LR,c = B(P j\ni,LR,c, γ) −D(P j\ni,LR,c) + P j\ni,LR,c. (13)\nNext, Mi,LR,c is forced to be consistent with Mi,HR,c, so\nthat Mi,LR,c can also indicate whether an patch is relevant.\nWhat’s more, the ratio of learned relevant patches eri,LR is\nalso constrained to be close to r. Overall, the hybrid loss\nfunction of LIPN is:\nLLIP N = β1\n2\nX\nc=1\nL1(Mi,LR,,c, Mi,HR,c)\n2\n+β2L2(eri,LR, r).\n(14)\nHere, L1(·) denotes the 1-norm loss function. In our imple-\nmentation, we employed the widely-used ResNet-50 pre-\ntrained on ImageNet as the feature extractor, and used\na lightweight variant of MobileNetV4 [40] for the pre-\nscreening network LIPN. The detailed architecture of LIPN\nis illustrated in the supplementary material.\n3.3. Efficient Inference\nAs shown in Fig. 2(b), our proposed efficient inference pro-\ncess consists of three steps: 1) Cropping all patches from\nXi,LR, with the total number of patches being Ni. 2) Feed-\ning these patches into LIPN to identify regions relevant to\nclassification, generating Mi,LR; 3) Selectively cropping\nrelevant eri,LRNi patches from Xi,HR based on Mi,LR, and\nthen feeding them into the feature extractor and DMIN. Af-\nterwards, we calculate the bag-level representations and the\nfinal classification results using the student branch across\ncategories separately.\n4. Experimental Results\n4.1. Settings\nWe evaluated our proposed algorithm on three public\ndatasets: 1) for breast cancer lymph node metastasis de-\ntection using the Camelyon16 [2] dataset; 2) for lung can-\ncer subtyping using the TCGA-NSCLC dataset; and 3) for\nbreast cancer subtyping using the TCGA-BRCA dataset.\nAll WSIs were pre-processed using tools developed by\nCLAM [34]. All experiments adhered to the principle of\n10-fold Monte Carlo cross-validation. For Camelyon16,\nthe official training set was divided into training and valida-\ntion sets at a 9:1 ratio based on the number of cases in each\nfold, while the official test set was used for testing across all\nfolds. The TCGA-NSCLC and TCGA-BRCA datasets were\nsplit into the training, validation, and test sets in an 8:1:1 ra-\ntio, again based on the number of cases in each fold. The\nimplementation details are in the supplementary material.\n4.2. Comparative Results on Test Sets\nClassification Performance. Table 1 compares the clas-\nsification performance of our proposed HDMIL against\nexisting MIL methods on the Camelyon16 [2], TCGA-\nNSCLC, and TCGA-BRCA test sets.\nHDMIL† means\nusing only DMIN for inference without pre-screening in-\nstances through LIPN. From the table we can find: 1)\nBoth HDMIL† and HDMIL consistently outperform exist-\ning methods across these datasets. 2) When the dataset is\nlarge enough, the speedup brought by HDMIL does not\nmeans a decrease in classification performance. For exam-\nple, the test performance gap between HDMIL† and HD-\nMIL is small on TCGA-NSCLC and TCGA-BRCA, both\nof which contain about 1000 WSIs. Meanwhile, the AUC\nscore of HDMIL decreases slightly on Camelyon16, but is\nstill much better than existing MIL methods. We believe\nthat the performance degradation of HDMIL compared to\nHDMIL† on the Camelyon16 dataset can be primarily at-\ntributed to the small dataset size (less than 400 WSIs), rather\nthan inherent shortcomings of HDMIL itself.\nA further\nanalysis is presented in Sec. 4.5.\nInference Time. From Tab. 1, it is evident that the pro-\ncessing time of HDMIL† is nearly identical to that of ex-\nisting methods since they need to process the same num-\nber of high-resolution patches. However, HDMIL out-\nperforms all other methods, significantly reducing the pro-\ncessing time. Compared to HDMIL†, HDMIL achieves an\ntotal speed improvement of 28.6%, 21.8%, and 7.2% on\nthe three datasets, respectively. To analyze how HDMIL\n\n\nComparative\nMethods\nCamelyon16\nTCGA-NSCLC\nTCGA-BRCA\nAUC↑\nACC↑\nTime(s)↓\nAUC↑\nACC↑\nTime(s)↓\nAUC↑\nACC↑\nTime(s)↓\nMax-Pooling\n83.261.54\n82.410.73\n23.46\n94.662.33 86.403.73\n57.16\n88.037.76 86.053.88\n36.49\nMean-Pooling\n61.802.15\n70.541.41\n23.46\n92.823.54 84.934.78\n57.16\n88.235.67 86.742.44\n36.49\nABMIL [19]\n84.883.38\n82.792.68\n23.46\n94.922.29 88.033.65\n57.16\n87.706.15 87.683.51\n36.49\nCLAMSB [34]\n83.494.46\n79.614.40\n23.46\n95.052.72 88.743.39\n57.16\n88.256.12 87.584.92\n36.49\nCLAMMB [34]\n87.513.23\n82.563.11\n23.46\n95.592.16 88.013.38\n57.16\n90.225.18 88.273.52\n36.49\nDSMIL [27]\n75.9410.81 75.356.12\n23.46\n92.112.97 83.673.80\n57.16\n83.337.48 82.593.66\n36.49\nTransMIL [47]\n82.265.67\n81.016.85\n23.47\n94.572.03 88.213.04\n57.17\n88.335.73 87.553.78\n36.49\nDTFDAFS [61]\n87.403.17\n85.122.42\n23.46\n95.592.08 88.763.89\n57.16\n87.247.38 86.833.98\n36.49\nDTFDMAS [61]\n87.752.07\n85.432.03\n23.46\n95.022.32 89.023.78\n57.17\n87.809.65 87.484.13\n36.49\nS4MIL [10]\n86.401.99\n80.392.79\n23.47\n96.191.89 89.692.86\n57.17\n90.405.73 88.173.88\n36.49\nMambaMIL [57]\n87.066.19\n83.262.93\n23.47\n95.371.70 89.623.13\n57.16\n89.695.91 87.784.27\n36.49\nHDMIL†\n93.171.83\n88.922.51\n23.46\n96.472.20 89.752.86\n57.16\n90.434.86 88.683.17\n36.49\nHDMIL\n90.882.75\n88.612.04\n16.75\n96.352.26 89.783.11\n44.71\n90.454.42 88.272.47\n33.86\nTable 1. Comparison of HDMIL with the state-of-the-art MIL methods on Camelyon16, TCGA-NSCLC, and TCGA-BRCA. The 10-fold\ntest AUC and accuracy (ACC) scores are reported in the form of meanstd. The best and second best results are indicated in red and blue,\nrespectively. The average processing time per WSI on each test sets are also shown. HDMIL† means using only DMIN for inference.\nMethods Dataset LIPN Crop\nFea\nDMIN\nTotal\nCame16\nHDMIL†\n-\n13.45\n10.00\n0.02\n23.46\nHDMIL 0.01\n10.88\n5.84\n0.02\n16.75\n∆\n-\n−19.1%−41.6%\n-\n−28.6%\nNSCLC\nHDMIL†\n-\n47.02\n10.12\n0.02\n57.16\nHDMIL 0.01\n37.21\n7.48\n0.02\n44.71\n∆\n-\n−20.9%−26.1%\n-\n−21.8%\nBRCA\nHDMIL†\n-\n27.17\n9.30\n0.02\n36.49\nHDMIL 0.01\n25.84\n8.00\n0.02\n33.86\n∆\n-\n−4.90%−14.0%\n-\n−7.2%\nTable 2. Comparison of HDMIL and HDMIL† when splitting the\ninference time (seconds) into four stages: instance pre-screening\n(LIPN), WSI cropping (“Crop”), feature extraction (“Fea”), and\nbag classification (DMIN).\nachieves this time-saving effect, we divide the WSI infer-\nence process into four stages: instance pre-screening, WSI\ncropping, feature extraction, and MIL classification, as pre-\nsented in Tab. 2. Although LIPN causes a slight increase in\ninference time (approximately 0.01 seconds), it reduces the\nnumber of instances that require cropping and feature ex-\ntraction, thereby significantly reducing total inference time.\n4.3. Focusing and Discarding Visualization\nFigure 3 shows two tumor WSIs with patch-level annota-\ntions, attention maps generated by DMIN, the instance re-\ntention after LIPN pre-screening, DMIN-focused patches,\nand LIPN-discarded patches. As expected, the first branch\nof DMIN focuses on normal tissue regions, while the\nsecond branch emphasizes tumor regions, demonstrating\nNormal Tissues Focused by DMIN (Branch1)\nTumor Tissues Focused by DMIN (Branch2)\nAdipose Tissues Discarded by LIPN\nRetained Region\n0\n1\n0\n1\nNormal Tissues Focused by DMIN (Branch1)\nTumor Tissues Focused by DMIN (Branch2)\nAdipose Tissues Discarded by LIPN\nRetained Region\nAttention1 \nInput WSI\nAttention2 \nAttention1 \nInput WSI\nAttention2 \n0\n0\n1\n0\n1\nFigure 3. Visualization analysis of two randomly selected WSIs.\nThe pathologists marked the tumor areas in the input WSIs with\nred lines. The dual-branch attention maps in DMIN (“Attention1”\nand “Attention2”) are shown, and the instances selected by LIPN\nare marked with blue masks (“Retained Region”)\nDMIN’s ability to identify regions related to bag-level clas-\nsification. Moreover, the regions to which DMIN assigns\ngreater importance are retained by LIPN, while instances\nderived from adipose tissues, which contribute minimally\nto classification, are effectively discarded by LIPN.\n\n\nDMIN\nLIPN\nCamelyon16\nTCGA-NSCLC\nTCGA-BRCA\nAverage\nCKA\nSelfDist\nAUC\nACC\nAUC\nACC\nAUC\nACC\nAUC\nACC\n✗\n✗\n✗\n94.674.51\n91.545.38\n95.363.51\n89.444.51\n88.826.41\n86.474.21\n92.95\n89.15\n✓\n✗\n✗\n97.153.27\n93.854.86\n95.193.01\n89.673.57\n91.225.40\n89.003.62\n94.52\n90.84\n✓\n✓\n✗\n97.702.54\n95.004.81\n95.583.27\n90.293.90\n93.334.58\n89.832.71\n95.54\n91.71\n✓\n✓\n✓\n97.642.93\n95.383.97\n95.883.02\n90.503.44\n93.274.87\n88.703.92\n95.60\n91.53\nTable 3. The effect of each component in HDMIL on classification performance. The 10-fold validation AUC and ACC scores are reported\nin the form of meanstd. “SelfDist” is the abbreviation for self-distillation.\nProjection\nAttention\nClassifier\nParams\n7.082M\n3.942M\n0.828M\nAUC\n92.796.92\n85.038.36\n97.153.27\nACC\n86.549.11\n77.316.40\n93.854.86\n(1) Comparison of the position of the CKA layer.\nFC\nMLP\nKA [33]\nCKA\nParams\n0.791M\n1.842M\n0.828M\n0.828M\nAUC\n94.674.51\n94.975.05\n96.422.88\n97.153.27\nACC\n91.545.38\n92.696.13\n91.166.03\n93.854.86\n(2) Comparison of FC, MLP, KA, and CKA as classifiers.\nK=4\nK=8\nK=12\nK=16\nParams\n0.803M\n0.816M\n0.828M\n0.840M\nAUC\n94.674.29\n94.614.48\n97.153.27\n96.182.59\nACC\n89.627.91\n90.387.08\n93.854.86\n90.396.60\n(3) Comparison of using different orders in CKA classifier.\nTable 4. Analysis of the CKA classifier. The 10-fold validation\nperformance on the Camelyon16 dataset are reported.\n4.4. Ablation Study on Validation Sets\nEffect of Each Components. Table 3 presents the impact\nof each module in HDMIL on the classification results. No-\ntably, replacing the conventional linear layer-based classi-\nfier with the proposed CKA classifiers and incorporating\nself-distillation into the DMIN training, both significantly\nimprove the classification performance. In addition, using\nLIPN for instance pre-screening does not result in a obvi-\nous decrease in the classification performance on the vali-\ndation set, slightly differing from the situation on the test\nset in Tab. 1. This will also be discussed in Sec. 4.5. In\nthe following subsections, we analyze the reasons why each\ncomponent works by 10-fold cross-validation experiments.\nCKA Classifier in DMIN. As shown in Tab. 4, we analyze\nthe proposed CKA classifiers from three perspectives: 1)\nthe impact of employing the CKA layer at different posi-\ntions; 2) comparison with other classification layers; and 3)\nthe impact of different Chebyshev polynomial orders K. To\neliminate the impact of other factors, all experiments here\nonly utilize the teacher branch trained on all instances.\n• When using CKA layers as the projection or attention\nmodule, the number of trainable parameters increases dra-\nmatically, accompanied by a significant drop in perfor-\nmance. It seems that our CKA layer is also better at solv-\ning modeling problems in lower dimensional spaces, sim-\nilar to the vanilla KAN [33].\n• When compared with other classifiers such as the FC\nlayer, two-layer MLP, and KA [33] layer, CKA demon-\nstrates superior classification performance. Although FC,\nMLP, and KA can sometimes achieve similar perfor-\nmance to CKA in certain folds, there tends to be a larger\nperformance gap in other folds. Thus, CKA is a more\npowerful and robust classifier.\n• When the Chebyshev polynomial order changes from 4 to\n16, the number of parameters of the entire DMIN does not\nchange much. Nevertheless, there is a noticeable dispar-\nity in classification performance, with the best outcome\nachieved at an order of 12. Further increasing the order\ndoes not lead to better improvements in classification per-\nformance, probably due to the limited training data.\nSelf-Distillation of DMIN. We believe that self-distillation\nenhances the classification performance by enforcing the at-\ntention module to focus on crucial instances, thereby reduc-\ning the impact of irrelevant regions. This can be seen as\na form of “denoising”. To verify this viewpoint, we eval-\nuated the quality of the bags after the “denoising” effect\nof self-distillation by considering three types of instances\nto represent each bag: all instances within each WSI, in-\nstances selected by the trained DMIN, and randomly sam-\npled instances.\nNewly trained Max-Pooling models are\nused to evaluate the quality of these three types of bags\nlike linear probing [7]. As shown in Tab. 5, MIL models\ntrained with instances selected by DMIN outperform mod-\nels trained with randomly sampled instances and even out-\nperform models trained with all instances. This suggests\nthat self-distillation improves the quality of bags for clas-\nsification by instance selection.\nDistillation Methods in LIPN. Table 6 explores the ef-\nfects of different distillation methods when using DMIN\nto distill LIPN. The symbol AH →PL represents the dis-\n\n\nMetrics\nAll\nRandom\nDMIN\nAUC/ACC\n92.06/84.62\n89.40/83.46\n92.73/85.78\nTable 5. Average performance of Max-Pooling trained with differ-\nent kinds of bags on the Camelyon16 validation set. The number\nof instances in each “random” bag were kept consistent with the\nnumber of instances in each “DMIN” bag.\ntillation from attention Ai,HR to instance-wise predictions\nPi,LR, while MH →ML denotes the distillation between\nMi,HR and Mi,LR. It can be seen that distilling among\ndiscrete masks yields significantly better results, especially\non the Camelyon16 dataset. This is because low-resolution\npatches lose too much information, in which case learning\nto predict the specific contribution score of each instance\nbecomes challenging for LIPN, compared to learning the\nbinary decision of the instance (keep or discard).\nDistillation\nManner\nCamelyon16 TCGA-NSCLC TCGA-BRCA\nAUC ACC AUC\nACC\nAUC\nACC\nAH →PL 81.70 83.85 95.01\n89.29\n90.49 87.10\nMH →ML 97.64 95.38 95.88\n90.50\n93.27 88.70\nTable 6. Performance of HDMIL on the validation set when us-\ning different distillation methods. AH, PL, MH, and ML are the\nabbreviations for Ai,HR, Pi,LR, Mi,HR, and Mi,LR respectively.\nImpact of the Preset Instance Retention Ratio. We exam-\nine the impact of the preset instance ratio r on the classifi-\ncation performance, actual learned instance retention ratios,\nand inference time, as depicted in Fig. 4: 1) Generally, the\nperformance of HDMIL and HDMIL† exhibits a pattern of\ninitial improvement followed by a decline as r increases.\nThis can be attributed to the fact that when r is small,\nthe scarcity of patches leads to the loss of classification-\nrelated information. Conversely, when r becomes exces-\nsively large, the self-distillation training fails to eliminate\nthe interference caused by irrelevant instances. 2) In addi-\ntion, it can be seen that the instance retention rate actually\nlearned by HDMIL† and HDMIL is roughly equivalent to\nthe preset r. 3) The inference time gradually increases as\nr increases. When r reaches 0.9, the total inference time\nof HDMIL surpasses that of HDMIL† due to the minimal\nnumber of discarded instances and the additional process-\ning time for low-resolution WSIs.\n4.5. Further Analysis: the Impact of Dataset Size\nDespite the performance gap between HDMIL† and HD-\nMIL on the Camelyon16 test set (depicted in Tab. 1), both\nmethods show similar performance on the validation sets\nacross all three datasets (shown in Tab. 3 and Fig. 4). The\ndrop in performance of HDMIL, on the Camelyon16 test\n0.1\n0.4\n0.7\n1.0\n0.94\n0.96\n0.98\nCamelyon16\nArea Under the Curve\nHDMIL\nHDMIL\n0.3\n0.6\n0.9\n0.25\n0.50\n0.75\n1.00\nLearned Retention Rate\nHDMIL\nHDMIL\n0.3\n0.6\n0.9\n0\n10\n20\nInference Time\nHDMIL\nHDMIL\n0.1\n0.4\n0.7\n1.0\n0.92\n0.94\n0.96\nTCGA-NSCLC\n0.3\n0.6\n0.9\n0.25\n0.50\n0.75\n1.00\n0.3\n0.6\n0.9\n0\n20\n40\n60\n0.1\n0.4\n0.7\n1.0\nValidation Set\n0.91\n0.92\n0.93\nTCGA-BRCA\n0.3\n0.6\n0.9\nTest Set\n0.25\n0.50\n0.75\n1.00\n0.3\n0.6\n0.9\nTest Set\n0\n20\n40\nFigure 4. The impact of the preset instance retention rate r (hyper-\nparameter) on classification performance, actual learned instance\nretention ratio, and inference time (seconds).\nset, is likely attributed to the bias introduced by select-\ning models for evaluation based on their validation perfor-\nmance. This bias can result in suboptimal performance on\nthe test sets, especially when there is a distribution differ-\nence between the validation and test sets, which becomes\nmore pronounced when dealing with smaller datasets.\nTCGA-NSCLC\nTCGA-BRCA\nValid Set\nTest Set\nValid Set\nTest Set\n25%\n100%\n25%\n100%\n25%\n100%\n25%\n100%\n95.84 95.88 95.46 96.35 94.55 93.27 90.06 90.45\nTable 7. The 10-fold average validation and test AUC scores of\nHDMIL, when the number of cases in the validation set is reduced\nto 25% and the cases in the training and test sets are unchanged.\nTo demonstrate our point, we conducted specific ex-\nperiments on TCGA-NSCLC and TCGA-BRCA, as shown\nin Tab. 7. It can found that when the number of cases in the\nvalidation set was reduced to 25% of the original size, the\nperformance of HDMIL on the validation set did not de-\ncrease significantly, while the models selected using these\nvalidation sets performed worse on the test sets. This find-\ning demonstrates that the performance decline of HDMIL\non Camelyon16 can be attributed to the small dataset size\nrather than inherent algorithmic flaws.\n5. Conclusion\nIn this paper, HDMIL offers a novel approach for acceler-\nating WSI classification while ensuring high classification\naccuracy. By hierarchical-distillation, HDMIL efficiently\n\n\nfilters out irrelevant patches within WSIs, significantly re-\nducing inference time. Extensive experiments demonstrate\nthat HDMIL outperforms current state-of-the-art methods\nin both classification performance and inference speed. To\nfurther improve MIL efficiency, we will explore strategies\nto alleviate the inference burden on the feature extractor and\nintegrate them with HDMIL in the future.\nReferences\n[1] Alireza Afzal Aghaei.\nrkan: Rational kolmogorov-arnold\nnetworks. arXiv preprint arXiv:2406.14495, 2024. 2\n[2] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes\nVan Diest, Bram Van Ginneken, Nico Karssemeijer, Geert\nLitjens, Jeroen AWM Van Der Laak, Meyke Hermsen,\nQuirine F Manson, Maschenka Balkenhol, et al. Diagnos-\ntic assessment of deep learning algorithms for detection of\nlymph node metastases in women with breast cancer. Jama,\n318(22):2199–2210, 2017. 5\n[3] Alexander\nDylan\nBodner,\nAntonio\nSantiago\nTepsich,\nJack Natan Spolski, and Santiago Pourteau.\nConvo-\nlutional kolmogorov-arnold networks.\narXiv preprint\narXiv:2406.13155, 2024. 2\n[4] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh\nSaligrama. Adaptive neural networks for efficient inference.\nIn International Conference on Machine Learning, pages\n527–536. PMLR, 2017. 2\n[5] Zavareh\nBozorgasl\nand\nHao\nChen.\nWav-kan:\nWavelet kolmogorov-arnold networks.\narXiv preprint\narXiv:2405.12832, 2024. 2\n[6] Richard\nJ\nChen,\nMing\nY\nLu,\nMuhammad\nShaban,\nChengkuan Chen, Tiffany Y Chen, Drew FK Williamson,\nand Faisal Mahmood.\nWhole slide images are 2d point\nclouds: Context-aware survival prediction using patch-based\ngraph convolutional networks. In Medical Image Computing\nand Computer Assisted Intervention–MICCAI 2021: 24th In-\nternational Conference, Strasbourg, France, September 27–\nOctober 1, 2021, Proceedings, Part VIII 24, pages 339–349.\nSpringer, 2021. 1\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597–1607. PMLR, 2020. 7\n[8] Daniele\nFakhoury,\nEmanuele\nFakhoury,\nand\nHendrik\nSpeleers. Exsplinet: An interpretable and expressive spline-\nbased neural network. Neural Networks, 152:332–346, 2022.\n2\n[9] Ji Feng and Zhi-Hua Zhou. Deep miml network. In Proceed-\nings of the AAAI conference on artificial intelligence, 2017.\n2\n[10] Leo Fillioux, Joseph Boyd, Maria Vakalopoulou, Paul-Henry\nCourn`ede, and Stergios Christodoulidis.\nStructured state\nspace models for multiple instance learning in digital pathol-\nogy. In International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention, pages 594–604.\nSpringer, 2023. 1, 2, 6\n[11] Remi\nGenet\nand\nHugo\nInzirillo.\nTkan:\nTempo-\nral\nkolmogorov-arnold\nnetworks.\narXiv\npreprint\narXiv:2405.07344, 2024. 2\n[12] Remi Genet and Hugo Inzirillo. A temporal kolmogorov-\narnold transformer for time series forecasting. arXiv preprint\narXiv:2406.02486, 2024. 2\n[13] Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza,\nAyesha Azam, Yee Wah Tsang, Jin Tae Kwak, and Nasir\nRajpoot. Hover-net: Simultaneous segmentation and classi-\nfication of nuclei in multi-tissue histology images. Medical\nimage analysis, 58:101563, 2019. 1\n[14] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui\nWang, and Yulin Wang. Dynamic neural networks: A sur-\nvey. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 44(11):7436–7456, 2021. 2\n[15] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji\nSong, Junfeng Cao, Wenhui Huang, Chao Deng, and Gao\nHuang.\nLearning to weight samples for dynamic early-\nexiting networks. In European conference on computer vi-\nsion, pages 362–378. Springer, 2022.\n[16] Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran\nPan, Yifan Pu, Chao Deng, Junlan Feng, Shiji Song, and Gao\nHuang. Dynamic perceiver for efficient visual recognition.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5992–6002, 2023.\n[17] Yizeng Han, Zeyu Liu, Zhihang Yuan, Yifan Pu, Chaofei\nWang, Shiji Song, and Gao Huang. Latency-aware unified\ndynamic networks for efficient image recognition.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n2024.\n[18] Charles Herrmann, Richard Strong Bowen, and Ramin\nZabih.\nChannel selection using gumbel softmax.\nIn Eu-\nropean conference on computer vision, pages 241–257.\nSpringer, 2020. 2, 3\n[19] Maximilian Ilse,\nJakub Tomczak,\nand Max Welling.\nAttention-based deep multiple instance learning. In Inter-\nnational conference on machine learning, pages 2127–2136.\nPMLR, 2018. 1, 2, 6\n[20] Sajid Javed, Arif Mahmood, Muhammad Moazam Fraz,\nNavid Alemi Koohbanani, Ksenija Benes, Yee-Wah Tsang,\nKatherine Hewitt, David Epstein, David Snead, and Nasir\nRajpoot. Cellular community detection for tissue phenotyp-\ning in colorectal cancer histology images. Medical image\nanalysis, 63:101696, 2020. 1\n[21] James Keeler, David Rumelhart, and Wee Leow. Integrated\nsegmentation and recognition of hand-printed numerals. Ad-\nvances in neural information processing systems, 3, 1990. 2\n[22] William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhi-\ndong Zhang, Jiashuai Liu, and Mireia Crispin-Ortuzar.\nCoxkan:\nKolmogorov-arnold networks for interpretable,\nhigh-performance\nsurvival\nanalysis.\narXiv\npreprint\narXiv:2409.04290, 2024. 2\n[23] Mario K¨oppen. On the training of a kolmogorov network.\nIn Artificial Neural Networks—ICANN 2002: International\nConference Madrid, Spain, August 28–30, 2002 Proceedings\n12, pages 474–479. Springer, 2002. 2\n\n\n[24] Oren Z Kraus, Jimmy Lei Ba, and Brendan J Frey. Classify-\ning and segmenting microscopy images with deep multiple\ninstance learning. Bioinformatics, 32(12):i52–i59, 2016. 2\n[25] Ming-Jun Lai and Zhaiming Shen. The kolmogorov super-\nposition theorem can break the curse of dimensionality when\napproximating high dimensional functions. arXiv preprint\narXiv:2112.09963, 2021. 2\n[26] Pierre-Emmanuel Leni, Yohan D Fougerolle, and Fr´ed´eric\nTruchetet. The kolmogorov spline network for image pro-\ncessing.\nIn Image Processing: Concepts, Methodologies,\nTools, and Applications, pages 54–78. IGI Global, 2013. 2\n[27] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple\ninstance learning network for whole slide image classifica-\ntion with self-supervised contrastive learning. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 14318–14328, 2021. 1, 2, 6\n[28] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang,\nZhihui Li, and Xiaojun Chang. Dynamic slimmable network.\nIn Proceedings of the IEEE/CVF Conference on computer\nvision and pattern recognition, pages 8607–8617, 2021. 2\n[29] Chenxin Li, Xinyu Liu, Wuyang Li, Cheng Wang, Hengyu\nLiu, and Yixuan Yuan. U-kan makes strong backbone for\nmedical image segmentation and generation. arXiv preprint\narXiv:2406.02918, 2024. 2\n[30] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu\nZhang, Xingang Wang, and Jian Sun.\nLearning dynamic\nrouting for semantic segmentation.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8553–8562, 2020. 2\n[31] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song,\nJue Wang, and Pengtao Xie. Not all patches are what you\nneed: Expediting vision transformers via token reorganiza-\ntions. arXiv preprint arXiv:2202.07800, 2022. 2\n[32] Ji-Nan Lin and Rolf Unbehauen. On the realization of a kol-\nmogorov network. Neural Computation, 5(1):18–20, 1993.\n2\n[33] Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle,\nJames Halverson, Marin Soljaˇci´c, Thomas Y Hou, and\nMax Tegmark. Kan: Kolmogorov-arnold networks. arXiv\npreprint arXiv:2404.19756, 2024. 2, 7\n[34] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J\nChen, Matteo Barbieri, and Faisal Mahmood. Data-efficient\nand weakly supervised computational pathology on whole-\nslide images. Nature biomedical engineering, 5(6):555–570,\n2021. 1, 2, 3, 4, 5, 6\n[35] Faisal Mahmood, Daniel Borders, Richard J Chen, Gre-\ngory N McKay, Kevan J Salimian, Alexander Baras, and\nNicholas J Durr. Deep adversarial training for multi-organ\nnuclei segmentation in histopathology images. IEEE trans-\nactions on medical imaging, 39(11):3257–3267, 2019. 1\n[36] Oded Maron and Tom´as Lozano-P´erez.\nA framework for\nmultiple-instance learning. Advances in neural information\nprocessing systems, 10, 1997. 2\n[37] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,\nZuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim.\nAdavit:\nAdaptive vision transformers for efficient image recognition.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12309–12318, 2022.\n2\n[38] Erick Moen, Dylan Bannon, Takamasa Kudo, William Graf,\nMarkus Covert, and David Van Valen.\nDeep learning for\ncellular image analysis. Nature methods, 16(12):1233–1246,\n2019. 1\n[39] Pedro O Pinheiro and Ronan Collobert. From image-level\nto pixel-level labeling with convolutional networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 1713–1721, 2015. 2\n[40] Danfeng Qin, Chas Leichner, Manolis Delakis, Marco\nFornoni, Shixin Luo, Fan Yang, Weijun Wang, Colby Ban-\nbury, Chengxi Ye, Berkin Akin, et al.\nMobilenetv4-\nuniversal models for the mobile ecosystem. arXiv preprint\narXiv:2404.10518, 2024. 5\n[41] Linhao Qu, Manning Wang, Zhijian Song, et al.\nBi-\ndirectional weakly supervised knowledge distillation for\nwhole slide image classification. Advances in Neural Infor-\nmation Processing Systems, 35:15368–15381, 2022. 1\n[42] Jan Ramon. Multi instance neural networks. In ML-2000\nWorkshop Attribute-Value and Relational Learning, 2000. 2\n[43] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie\nZhou, and Cho-Jui Hsieh.\nDynamicvit: Efficient vision\ntransformers with dynamic token sparsification. Advances\nin neural information processing systems, 34:13937–13949,\n2021. 2, 3, 5\n[44] Joel Saltz, Rajarsi Gupta, Le Hou, Tahsin Kurc, Pankaj\nSingh, Vu Nguyen, Dimitris Samaras, Kenneth R Shroyer,\nTianhao Zhao, Rebecca Batiste, et al. Spatial organization\nand molecular correlation of tumor-infiltrating lymphocytes\nusing deep learning on pathology images. Cell reports, 23\n(1):181–193, 2018. 1\n[45] Denis Schapiro, Hartland W Jackson, Swetha Raghuraman,\nJana R Fischer, Vito RT Zanotelli, Daniel Schulz, Charlotte\nGiesen, Ra´ul Catena, Zsuzsanna Varga, and Bernd Boden-\nmiller. histocat: analysis of cell phenotypes and interactions\nin multiplex image cytometry data. Nature methods, 14(9):\n873–876, 2017. 1\n[46] Wei Shao, Tongxin Wang, Zhi Huang, Zhi Han, Jie Zhang,\nand Kun Huang. Weakly supervised deep ordinal cox model\nfor survival prediction from whole-slide pathological im-\nages. IEEE Transactions on Medical Imaging, 40(12):3739–\n3747, 2021. 1\n[47] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian\nZhang, Xiangyang Ji, et al. Transmil: Transformer based\ncorrelated multiple instance learning for whole slide image\nclassification.\nAdvances in neural information processing\nsystems, 34:2136–2147, 2021. 1, 2, 6\n[48] Lin Song, Songyang Zhang, Songtao Liu, Zeming Li, Xum-\ning He, Hongbin Sun, Jian Sun, and Nanning Zheng. Dy-\nnamic grained encoder for vision transformers.\nAdvances\nin Neural Information Processing Systems, 34:5770–5783,\n2021. 2\n[49] David A Sprecher and Sorin Draghici. Space-filling curves\nand kolmogorov superposition-based neural networks. Neu-\nral Networks, 15(1):57–67, 2002. 2\n\n\n[50] Hoang-Thang Ta. Bsrbf-kan: A combination of b-splines\nand radial basic functions in kolmogorov-arnold networks.\narXiv preprint arXiv:2406.11173, 2024. 2\n[51] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung\nKung.\nBranchynet: Fast inference via early exiting from\ndeep neural networks.\nIn 2016 23rd international con-\nference on pattern recognition (ICPR), pages 2464–2469.\nIEEE, 2016. 2\n[52] Cristian J Vaca-Rubio, Luis Blanco, Roberto Pereira, and\nM`arius Caus. Kolmogorov-arnold networks (kans) for time\nseries analysis. arXiv preprint arXiv:2405.08790, 2024. 2\n[53] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao\nHuang. Not all images are worth 16x16 words: Dynamic\ntransformers for efficient image recognition.\nAdvances in\nneural information processing systems, 34:11960–11973,\n2021. 2\n[54] Anfeng Xu, Biqiao Zhang, Shuyu Kong, Yiteng Huang,\nZhaojun Yang, Sangeeta Srivastava, and Ming Sun. Effec-\ntive integration of kan for keyword spotting. arXiv preprint\narXiv:2409.08605, 2024. 2\n[55] Jinfeng Xu, Zheyu Chen, Jinze Li, Shuo Yang, Wei Wang,\nXiping Hu, and Edith C-H Ngai. Fourierkan-gcf: Fourier\nkolmogorov-arnold network–an effective and efficient fea-\nture transformation for graph collaborative filtering. arXiv\npreprint arXiv:2406.01034, 2024. 2\n[56] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and\nGao Huang. Resolution adaptive networks for efficient in-\nference.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2369–2378,\n2020. 2\n[57] Shu Yang, Yihui Wang, and Hao Chen.\nMambamil: En-\nhancing long sequence modeling with sequence reordering in\ncomputational pathology. arXiv preprint arXiv:2403.06800,\n2024. 1, 2, 6\n[58] Xingyi Yang and Xinchao Wang. Kolmogorov-arnold trans-\nformer. arXiv preprint arXiv:2409.10594, 2024. 2\n[59] Jiawen Yao, Xinliang Zhu, Jitendra Jonnagaddala, Nicholas\nHawkins, and Junzhou Huang. Whole slide images based\ncancer survival prediction using attention guided deep multi-\nple instance learning networks. Medical Image Analysis, 65:\n101789, 2020. 1\n[60] Xiaotian Yu, Haoming Luo, Jiacong Hu, Xiuming Zhang,\nYuexuan Wang, Wenjie Liang, Yijun Bei, Mingli Song, and\nZunlei Feng. Hundredfold accelerating for pathological im-\nages diagnosis and prognosis through self-reform critical re-\ngion focusing. 2\n[61] Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao,\nXiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-\nmil: Double-tier feature distillation multiple instance learn-\ning for histopathology whole slide image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18802–18812, 2022. 1,\n2, 6\n[62] Wangbo Zhao, Jiasheng Tang, Yizeng Han, Yibing Song, Kai\nWang, Gao Huang, Fan Wang, and Yang You. Dynamic tun-\ning towards parameter and inference efficiency for vit adap-\ntation. arXiv preprint arXiv:2403.11808, 2024. 2\n[63] Wentao Zhu, Qi Lou, Yeeleng Scott Vang, and Xiaohui\nXie. Deep multi-instance networks with sparse label assign-\nment for whole mammogram classification. In Medical Im-\nage Computing and Computer Assisted Intervention- MIC-\nCAI 2017: 20th International Conference, Quebec City, QC,\nCanada, September 11-13, 2017, Proceedings, Part III 20,\npages 603–611. Springer, 2017. 2\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21130v2.pdf",
    "total_pages": 11,
    "title": "Fast and Accurate Gigapixel Pathological Image Classification with Hierarchical Distillation Multi-Instance Learning",
    "authors": [
      "Jiuyang Dong",
      "Junjun Jiang",
      "Kui Jiang",
      "Jiahan Li",
      "Yongbing Zhang"
    ],
    "abstract": "Although multi-instance learning (MIL) has succeeded in pathological image\nclassification, it faces the challenge of high inference costs due to\nprocessing numerous patches from gigapixel whole slide images (WSIs). To\naddress this, we propose HDMIL, a hierarchical distillation multi-instance\nlearning framework that achieves fast and accurate classification by\neliminating irrelevant patches. HDMIL consists of two key components: the\ndynamic multi-instance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN\noperates on the corresponding low-resolution counterparts. During training,\nDMIN are trained for WSI classification while generating attention-score-based\nmasks that indicate irrelevant patches. These masks then guide the training of\nLIPN to predict the relevance of each low-resolution patch. During testing,\nLIPN first determines the useful regions within low-resolution WSIs, which\nindirectly enables us to eliminate irrelevant regions in high-resolution WSIs,\nthereby reducing inference time without causing performance degradation. In\naddition, we further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology, which enhances the\nperformance of HDMIL through learnable activation layers. Extensive experiments\non three public datasets demonstrate that HDMIL outperforms previous\nstate-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while\nreducing inference time by 28.6% on the Camelyon16 dataset.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}