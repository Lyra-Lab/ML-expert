{
  "id": "arxiv_2502.21171v1",
  "text": "QFAL: Quantum Federated Adversarial Learning\nWalid El Maouaki1, Nouhaila Innan2,3, Alberto Marchisio2,3, Taoufik Said1,\nMohamed Bennai1, and Muhammad Shafique2,3\n1Quantum Physics and Spintronic Team, LPMC, Faculty of Sciences Ben M’sick,\nHassan II University of Casablanca, Morocco\n2eBRAIN Lab, Division of Engineering, New York University Abu Dhabi (NYUAD), Abu Dhabi, UAE\n3Center for Quantum and Topological Systems (CQTS), NYUAD Research Institute, NYUAD, Abu Dhabi, UAE\nwalid.elmaouaki-etu@etu.univh2c.ma, nouhaila.innan@nyu.edu, alberto.marchisio@nyu.edu, taoufik.said@univh2c.ma,\nmohamed.bennai@univh2c.ma, muhammad.shafique@nyu.edu\nAbstract—Quantum federated learning (QFL) merges the\nprivacy advantages of federated systems with the computational\npotential of quantum neural networks (QNNs), yet its vulnerability\nto adversarial attacks remains poorly understood. This work\npioneers the integration of adversarial training into QFL,\nproposing a robust framework, quantum federated adversarial\nlearning (QFAL), where clients collaboratively defend against\nperturbations by combining local adversarial example generation\nwith federated averaging (FedAvg). We systematically evaluate\nthe interplay between three critical factors: client count (5, 10,\n15), adversarial training coverage (0–100%), and adversarial\nattack perturbation strength (ϵ = 0.01 −0.5), using the MNIST\ndataset. Our experimental results show that while fewer clients\noften yield higher clean-data accuracy, larger federations can\nmore effectively balance accuracy and robustness when partially\nadversarially trained. Notably, even limited adversarial coverage\n(e.g., 20%–50%) can significantly improve resilience to moderate\nperturbations, though at the cost of reduced baseline performance.\nConversely, full adversarial training (100%) may regain high\nclean accuracy but is vulnerable under stronger attacks. These\nfindings underscore an inherent trade-off between robust and\nstandard objectives, which is further complicated by quantum-\nspecific factors. We conclude that a carefully chosen combination\nof client count and adversarial coverage is critical for mitigating\nadversarial vulnerabilities in QFL. Moreover, we highlight\nopportunities for future research, including adaptive adversarial\ntraining schedules, more diverse quantum encoding schemes,\nand personalized defense strategies to further enhance the\nrobustness–accuracy trade-off in real-world quantum federated\nenvironments.\nIndex Terms—Quantum Machine Learning, Quantum Feder-\nated Learning, Adversarial Attacks, Quantum Neural Networks\nI. INTRODUCTION\nFederated Learning (FL) has emerged as an effective\nparadigm for collaborative training of Machine Learning (ML)\nmodels across distributed clients without requiring raw data\nto be centralized. This approach addresses critical concerns\nrelated to privacy, data sovereignty, and communication costs\n[1]. Simultaneously, quantum computing has demonstrated\nthe potential to accelerate certain computational tasks and\nenable new forms of ML architectures through quantum neural\nnetworks (QNNs) [2]–[6]. By combining FL’s collaborative\ntraining mechanism with quantum computing’s ability to encode\nand process information in qubits, quantum federated learning\n(QFL) has the potential to harness benefits from both domains,\npointing to a promising avenue for next-generation distributed\nintelligence [7], [8]. However, ML models, both classical and\nquantum, are susceptible to adversarial attacks [9]–[15]. Minor,\ncarefully crafted perturbations to input data can significantly\ndegrade a model’s performance at inference time. In the context\nof QFL, the distributed nature of training and the unique\nstructure of QNNs create new challenges in both detecting\nand countering adversarial attacks. Existing adversarial defense\nstrategies in FL often focus on classical deep networks, leaving\nopen questions about how best to incorporate adversarial\nrobustness techniques in a QFL setting [16]. At the same\ntime, partial adoption of adversarial training by only a subset\nof federated clients introduces unexplored dimensions for\nmitigating attack vectors, especially when dealing with the\nlimited qubit resources and data encoding restrictions inherent\nto many quantum devices.\nIn this work, we propose a novel QFL framework that\nintegrates local adversarial training within each client’s QNN.\nBy systematically varying the number of clients, the fraction of\nclients that perform adversarial training, and the perturbation\nstrengths of the adversarial examples, we aim to evaluate the\nframework’s robustness and scalability. Our results illustrate\nthe trade-offs between clean-data accuracy and defense against\nadversarial examples in quantum federated scenarios, provid-\ning guidance for practitioners and researchers interested in\ndesigning secure, privacy-conscious QML solutions.\nOur work offers several key contributions to quantum QFL\nand adversarial robustness:\n• We propose QFAL, a novel quantum federated adversarial\nlearning framework that seamlessly integrates adversarial\ntraining into quantum federated learning, specifically\ndesigned to counteract the unique vulnerabilities inherent\nto quantum federated learning.\n• We develop a tailored local adversarial training protocol\nthat leverages PGD-based adversarial example generation\nwithin each quantum client, enabling a dynamic balance\nbetween training on clean and perturbed data. This method\nenhances resilience against malicious inputs during infer-\nence, and introduces client-level heterogeneity in defense\nstrategies.\n• We conduct a comprehensive empirical evaluation of\nQFAL by systematically varying client counts, adversarial\narXiv:2502.21171v1  [cs.LG]  28 Feb 2025\n\n\ntraining coverage, and perturbation strengths. Our exper-\niments reveal that even partial adversarial training (e.g.,\n20% coverage in a 5-client system) can boost robustness\nby over 12% compared to baseline models, while larger\nfederations (10–15 clients) yield a more favorable trade-off\nbetween clean-data accuracy and adversarial resilience.\nPaper Organization: Section II discusses the background\nand related work in QML, QFL, and security. Section III\npresents the details of our proposed QFAL framework. Sec-\ntion IV reports the experimental results. Section V summarizes\nthe key findings of this study. Section VI concludes the paper\nand discusses future directions.\nII. BACKGROUND AND RELATED WORK\nA. Quantum Machine Learning\nQML is an interdisciplinary field combining quantum com-\nputing with classical ML paradigms to enhance computational\nefficiency and tackle complex learning tasks across various\ndomains [17], [18]. At the core of QML are QNNs, which\nleverage parameterized quantum circuits (PQCs) as trainable\nmodels for various learning problems.\nQNNs extend classical neural networks by utilizing quan-\ntum circuits as function approximators. A QNN consists of\nmultiple quantum layers, each composed of quantum gates\nparameterized by trainable variables θ. The evolution of the\nquantum state follows:\n|ψ(θ)⟩= UL(θL) . . . U2(θ2)U1(θ1)|0⟩⊗n,\n(1)\nwhere each unitary transformation Ui(θi) corresponds to a\nquantum layer applied sequentially to an initial quantum state.\nAfter processing the input state, the final quantum state |ψ(θ)⟩\nis measured to extract predictions.\nEncoding classical data into quantum states is a fundamental\nstep in QNNs. Given a classical input vector x ∈Rd, the\ngoal is to map it to a quantum state |ψ(x)⟩in Hilbert space.\nSeveral encoding techniques exist, including basis encoding,\nangle encoding, and amplitude encoding.\nQNNs are trained by minimizing a loss function, typically\nin a supervised learning setting. The loss function is computed\nafter measurements are performed on the quantum state. A\ncommonly used loss function is the Mean Squared Error (MSE):\nL(θ) = 1\nN\nN\nX\ni=1\n(yi −f(θ; xi))2,\n(2)\nwhere yi represents the true label, and f(θ; xi) is the predicted\noutput obtained from quantum measurements.\nSince quantum measurements collapse the wavefunction,\nmultiple circuit evaluations (shots) are required to estimate the\nexpectation value accurately. The function f(θ; x) is defined\nas the expectation value of an observable ˆO, often chosen as\na Pauli operator (e.g., ˆZ):\nf(θ; x) = ⟨ψ(θ)| ˆO|ψ(θ)⟩.\n(3)\nHowever, due to shot noise, this estimation introduces statistical\nvariance that can impact training stability, particularly in\ngradient-based optimization.\nTo optimize the parameters θ, classical gradient-based\ntechniques are employed. The parameter-shift rule, widely used\nin quantum circuits, efficiently extracts gradients for certain\nparameterized gates:\n∂L(θ)\n∂θj\n= L(θ + π\n2 ej) −L(θ −π\n2 ej)\n2\n,\n(4)\nwhere ej is a unit vector in the direction of the j-th parameter.\nThis technique is valid when the parameterized quantum gates\nare of the form e−iθG, where G is a Hermitian generator (such\nas Pauli operators). Once the gradients are extracted using the\nparameter-shift rule, optimization algorithms such as Adam\nadaptively update the parameters to minimize the loss function.\nB. QFL\nQFL extends classical FL to the quantum domain, enabling\nmultiple quantum clients to collaboratively train a global\nquantum model while preserving data privacy and adhering\nto quantum constraints such as the no-cloning theorem. In\na standard QFL setup, K quantum clients, each holding a\nlocal quantum dataset, train individual QML models. Instead\nof sharing raw quantum data, the global model is updated\nthrough an aggregation process that integrates updates from all\nclients. Each training round consists of the following steps (see\nAlgorithm 1), each client trains its local QML model using its\navailable quantum data, computes classical representations of\nquantum gradients or parameter updates (e.g., via the parameter-\nshift rule), and transmits them to a central server. The central\nserver aggregates the updates using a weighted averaging\nmechanism:\nθ(t+1) =\nK\nX\nk=1\nwkθ(t)\nk ,\n(5)\nwhere θ(t)\nk\ndenotes the parameters of the k-th QNNs at iteration\nt, wk =\n|Dk|\nPK\ni=1 |Di| is the weight assigned to each client k,\nproportional to its dataset size, typically proportional to dataset\nsize or computational resources, and θ(t+1) is the updated\nglobal model. The weights satisfy the normalization condition:\nK\nX\nk=1\nwk = 1.\n(6)\nThe updated global model is redistributed to all clients, and\nthis process repeats until convergence based on a predefined\nstopping criterion. This ensures privacy preservation while\nfacilitating collaborative quantum model training across decen-\ntralized clients.\nC. Adversarial Attacks and Defenses in Classical ML\nML models, particularly deep neural networks, are highly\nvulnerable to adversarial examples—small, imperceptible per-\nturbations to input data that lead to incorrect model predictions\n[19]. As simple representation of an adversarial attack scenario\nis shown in Fig. 1. This phenomenon raises significant concerns\nin security-sensitive applications such as finance, healthcare,\nand autonomous systems. Addressing these vulnerabilities re-\nquires a thorough understanding of adversarial attack strategies\nand corresponding defense mechanisms.\n2\n\n\nAlgorithm 1 QFL\nRequire: Number of clients K, number of rounds T, local\nepochs E, learning rate η\n1: Initialize global model θ(0)\n2: for each round t = 0, 1, . . . , T −1 do\n3:\nfor each client k ∈{1, . . . , K} in parallel do\n4:\nDownload global model θ(t)\n5:\nfor each local epoch e = 1, . . . , E do\n6:\nCompute\nquantum\ngradients\nusing\nthe\nparameter-shift rule\n7:\nUpdate local model: θ(t)\nk\n←θ(t)\nk\n−η∇θLk\n8:\nend for\n9:\nSend local update θ(t)\nk\nto the server\n10:\nend for\n11:\nServer aggregates updates: θ(t+1) = PK\nk=1 wkθ(t)\nk\n12: end for\n13: Return final global model θ(T )\nQML, QFL, or\nClassical ML\nModel\nInput\nClass\nOutput\nAttack\nAdv.\nNoise\nFig. 1. A simplified representation of an adversarial attack scenario, where\nthe perturbations added to the image alter the classification process. A similar\napproach for the attack can be applied to various models, including classical\nML, QML, and QFL.\n1) Projected Gradient Descent (PGD) Attack: One of the\nmost effective and widely used adversarial attack methods\nis the Projected Gradient Descent (PGD) attack [20]. PGD\niteratively crafts adversarial perturbations by maximizing the\nclassification loss of a given model under a bounded constraint.\nGiven a classifier parameterized by θ, the PGD attack itera-\ntively perturbs an input sample to maximize the classification\nloss while ensuring the perturbation remains within a predefined\nbound. This process involves adjusting the input in the direction\nof the loss gradient, guided by a predefined step size. The\nadversarial modifications are constrained within a bounded\nregion around the original input to ensure they remain within\nan acceptable perturbation range. The iterative nature of PGD\nmakes it one of the strongest first-order attacks, as it effectively\nrefines the perturbation to maximize its impact on the model’s\nprediction.\nFormally, given a classifier fθ parameterized by θ, an input\nsample x, and its corresponding label y, PGD generates an\nadversarial example x′ using:\n2) Adversarial Training: Adversarial training is a widely\nused defense mechanism that enhances model robustness by\ntraining on adversarially perturbed samples [20]. The core idea\nis to modify the training process such that the model learns\nto correctly classify adversarial examples. By incorporating\nadversarial examples into the training process, adversarial\ntraining increases model resilience against such attacks.\nHowever, adversarial training introduces computational over-\nhead due to the iterative nature of attack generation during\ntraining and may lead to slight performance degradation on\nclean samples [21]. Despite these trade-offs, adversarial training\nremains a cornerstone defense method for securing ML models\nagainst adversarial threats.\nIn the context of QFL, integrating adversarial training\nwith quantum models presents additional challenges due to\nlimited quantum resources and noise-induced perturbations. Our\nwork builds upon these principles to develop robust federated\nquantum models resistant to adversarial manipulations.\nD. Related work\nEarly QFL research demonstrated the feasibility of feder-\nated training on hybrid quantum-classical models, achieving\ncomparable accuracy with faster training [22]. QFL has since\nbeen extended to variational quantum algorithms, optimizing\nvariational quantum circuits while preserving data privacy\n[23]. The federated quantum neural network framework further\nadvanced QFL by integrating quantum machine learning with\nclassical federated learning, showing high accuracy across\ngenomics and healthcare datasets [24]. Additionally, QFL\nhas been explored in spiking neural networks, where the FL-\nQDSNNs framework leverages a dynamic threshold mechanism\nto enhance learning efficiency while ensuring scalability and\nprivacy in distributed quantum environments [25].\nPrivacy-preserving QFL protocols leveraging blind quantum\ncomputation [26] have enhanced security in distributed quantum\nsystems. Other developments include quantum fuzzy federated\nlearning for improved robustness in non-IID settings [27]\nand federated quantum natural gradient descent for reducing\ncommunication overhead and accelerating convergence [28].\nQuantum differential privacy has been explored to mitigate\nmodel inversion attacks and data leakage [29], while QFL\nintegration with fully homomorphic encryption has improved\nsecurity and robustness in multimodal federated environments\n[30], [31].\nDespite advancements in QFL, adversarial robustness re-\nmains underexplored, with limited research addressing vul-\nnerabilities to malicious interventions. In security-sensitive\napplications such as autonomous vehicles and cybersecurity,\nFL models are susceptible to adversarial attacks, including\ndata poisoning and Byzantine attacks. For instance, quantum-\nbehaved particle swarm optimization has been employed to\nenhance model resilience against poisoning attacks in federated\nautonomous vehicle networks [32], while quantum-inspired\nfederated averaging techniques have been introduced for cyber-\nattack detection using spatio-temporal attention networks [33].\nIn the QFL context, Byzantine resilience has been studied,\ndemonstrating that classical Byzantine-tolerant algorithms can\nbe adapted to quantum federated settings [34]. However, the\nimpact of adversarial evasion attacks—exploiting quantum-\nspecific properties such as superposition, entanglement, and\nmeasurement uncertainty—remains largely unexplored. While\nclassical adversarial methods like FGSM [35] and PGD [20]\n3\n\n\nare well studied, their applicability to quantum models within\na federated framework is unclear. Motivated by this critical\nshortfall, our work aims to rigorously analyze the impact of\nevasion attacks in a QFL context and propose novel defense\nmechanisms that enhance the adversarial robustness of QFL\nmodels.\nIII. QFAL METHODOLOGY\nOur QFAL framework integrates a QNN within an FL\nsetup, employing adversarial training with a quantum-enhanced\ndefense algorithm to improve robustness. The evaluation\nassesses accuracy, resilience, and privacy. A schematic is shown\nin Fig. 2, and the algorithm is detailed in Algorithm 2.\nA. QNN Architecture\nEach participating client maintains a local QNN trained on\nan independently drawn, identically distributed (IID) subset of\nthe MNIST dataset (restricted to digits 0, 1, and 2, resized to\n8×8 pixels for quantum state preparation). The quantum model\nis based on PQCs, which apply trainable quantum gates to\nprocess classical inputs. The QNN employs amplitude encoding,\nwhere classical input features are embedded into the amplitudes\nof a quantum state, allowing for an efficient representation of\nhigh-dimensional data. Given a normalized input vector x =\n(x1, x2, . . . , xd), amplitude encoding maps it into a quantum\nstate as:\n|ψ(x)⟩=\nd\nX\ni=1\nxi|i⟩,\n(7)\nwhere each coefficient xi is directly encoded in the amplitude\nof the computational basis state |i⟩. This approach enables an\nexponential compression of classical data into quantum states\nbut requires quantum operations that maintain normalization\nconstraints. The trainable quantum layers perform feature\ntransformations through entangling gates and rotation operators,\nwith final measurements in the Pauli-Z basis extracting class\npredictions.\nB. QFL Training Protocol\nFederated training in QFAL follows a standard FL procedure\nusing the FedAvg aggregation scheme. At each communication\nround t, a subset of K participating clients receives the current\nglobal model θ(t) and trains their local QNNs using their private\ndatasets. Each client k updates its model parameters θ(t)\nk\nby\nminimizing a local loss function Lk, typically computed over\nits dataset Dk:\nθ(t+1)\nk\n= θ(t)\nk\n−η∇θLk(θ(t)\nk ),\n(8)\nwhere η represents the learning rate. After local training, each\nclient transmits its updated model parameters to the central\nserver. The server performs aggregation using the FedAvg\napproach, computing the new global model as a weighted sum\nof the client updates as per equation 5. The updated global\nparameters θ(t+1) are then distributed back to all participating\nclients, and this process iterates for multiple rounds until\nconvergence. The iterative optimization aims to minimize a\nglobal objective function defined as:\nmin\nθ\nK\nX\nk=1\n|Dk|\n|D| Lk(θ),\n(9)\nwhere |D| = PK\nk=1 |Dk| is the total dataset size across\nall clients. This decentralized approach enables collaborative\ntraining of the global QNN while preserving client data privacy.\nThe process continues until a predefined stopping criterion is\nmet, such as a convergence threshold on the loss function or a\nfixed number of communication rounds.\nC. Adversarial Training for Robust QFL\nA core innovation in QFAL is the incorporation of local\nadversarial training, which significantly improves robustness\nagainst adversarial perturbations. During local training, selected\nclients generate adversarial examples using the PGD attack,\nformulated as:\nxt+1 = ΠBϵ(x) (xt + α · sign (∇xL(fθ(xt), y))) ,\n(10)\nwhere L is the model loss function, α is the step size,\nBϵ(x) is the adversarial perturbation constraint, and Π ensures\nthe perturbation remains within the permissible range. To\nenhance robustness, participating clients mix clean samples\nand adversarially perturbed samples within each mini-batch\nduring training. This adversarial training follows a min-max\noptimization paradigm:\nmin\nθ\nE(x,y)∼D max\n∥δ∥≤ϵ L(fθ(x + δ), y),\n(11)\nwhere δ represents adversarial perturbations, ϵ controls pertur-\nbation strength. Clients performing adversarial training update\ntheir QNN parameters accordingly before sending them to the\nserver.\nD. Testing and Evaluation\nTo systematically assess the effectiveness and scalability of\nQFAL, we evaluate model performance across three key factors:\nthe number of participating clients, the extent of adversarial\ncoverage, and the perturbation strength of adversarial attacks.\nFirst, we analyze the impact of varying the number of clients by\nconsidering scenarios with 5, 10, and 15 clients, allowing us to\nmeasure how increasing client participation affects model accu-\nracy and robustness. Second, we assess the effect of adversarial\ntraining coverage by varying the fraction of clients performing\nadversarial training, considering cases where 0%, 20%, 50%,\nand 100% of clients incorporate adversarial examples into\ntheir local training. This helps determine the trade-off between\nadversarial robustness and federated training stability. Finally,\nwe evaluate the model’s resilience to adversarial perturbations\nby progressively increasing the PGD attack strength parameter\nϵ, providing insight into how the global model performs under\ndifferent levels of adversarial pressure.\nFor optimization, we employ the Adam optimizer, which\ncombines momentum-based updates with adaptive learning\n4\n\n\nSelect number of \nclients for FL (5, \n10, or 15)\nEach client uses \nclean data \n(baseline: 0% \nadversarial)\nClients perform \nlocal training\nSelect the \nportion of \nclients to do \nadversarial \ntraining (20%, \n50%, or 100%)\nClients with \nadversarial \ntraining, attack a \nsubset of their \ndata and add it \nto their training \ndata\nAdversarial and \nclean clients \nperform local \ntraining \nImpact on Robustness via Varying Adversarial Coverage\nImpact on Robustness by Varying the Number of Clients\nAggregate local updates\nRefine the \nglobal QNN\nGlobal model \narchitecture and \nparameters \nAssess global model robustness on PGD‐attacked MNIST at increasing perturbation strengths \n(0, 0.01, …, 0.5).\nFig. 2. Overview of our proposed QFL framework for adversarially robust quantum training. Each client maintains a local QNN, trained on its unique subset\nof MNIST data and (optionally) augmented with adversarial examples generated via PGD. After a set number of local epochs, clients upload their updated\nparameters to a central server, where they are aggregated using FedAvg.\n0\n1\n2\n3\n4\n5\n|\nStronglyEntanglingLayers\nFig. 3. QNN circuit utilizing amplitude embedding for state preparation, where\nclassical input features are encoded into a normalized quantum state, followed\nby strongly entangling layers that apply trainable parametrized rotations and\ncontrolled entanglement to enhance feature representation. Measurement in\nthe Pauli-Z basis on selected qubits provides expectation values corresponding\nto class predictions, enabling quantum-assisted classification.\nrates to improve convergence stability. The first and second\nmoment estimates of the gradients are updated as:\nmt = β1mt−1+(1−β1)∇θL(θ),\nvt = β2vt−1+(1−β2)∇θL(θ)2,\n(12)\nwhere mt and vt represent the first and second moment\nestimates of the gradients, respectively, and β1 and β2 are\ndecay rates that control momentum and variance adaptation.\nTo correct for bias in these estimates, they are normalized as:\nˆmt =\nmt\n1 −βt\n1\n,\nˆvt =\nvt\n1 −βt\n2\n.\n(13)\nThe model parameters are then updated using:\nθt+1 = θt −\nη\n√ˆvt + ϵ ˆmt,\n(14)\nwhere η is the learning rate, and ϵ is a small constant to prevent\nnumerical instability.\nIV. RESULTS\nA. Experimental Setup\nIn this study, we focus on a subset of the MNIST dataset\ncontaining only the digits 0, 1, and 2. To reduce the input\ndimension for our QNN, each 28 × 28 image is downsampled\nto an 8 × 8 resolution. This enables amplitude encoding into\na 6-qubit circuit for the QNN. Once processed, the dataset is\nsplit into multiple parts and distributed among clients in an\nIID manner, ensuring each client receives a balanced portion\nof the data. Each client independently maintains and updates a\nQNN composed of multiple layers of parameterized quantum\ngates, mixed with entanglement operations. In our approach, we\n5\n\n\nAlgorithm 2 QFAL\nRequire: D: MNIST (classes 0,1,2) resized to 8 × 8, nC ∈\n{5, 10, 15}, Coverage ∈{0%, 20%, 50%, 100%}, FE-\nDAVG(·)\n1: Split D IID into {D1, . . . , DnC}\n2: for each cov ∈{0%, 20%, 50%, 100%} do\n3:\nif cov = 0% then\n4:\nGlobal0 ←initModel(); Rounds = 50\n▷Initialize\na fresh global model\n5:\nelse\n6:\nGlobal0 ←loadBaseline(); Rounds = 20\n▷\nUse the model from previous coverage = 0% as a starting\npoint\n7:\nend if\n8:\nfor r = 1 to Rounds do\n9:\nfor i = 1 to nC do\n10:\nLocali ←Globalr−1\n11:\nif i in coverage portion then\n12:\nWi ←localTrainAdv(Locali, Di)\n13:\nelse\n14:\nWi ←localTrainClean(Locali, Di)\n15:\nend if\n16:\nend for\n17:\nGlobalr ←FEDAVG({Wi}nC\ni=1)\n18:\nend for\n19:\nEVALUATE(Globalr, CleanTest, AdvTest)\n▷Test on\nclean and adversarial data at various ϵ\n20:\nSAVEMODEL(Globalr, cov)\n21: end for\nemploy two layers of strongly entangled PQCs following the\ndata encoding step. The final layer, known as the measurement\nlayer, produces class probabilities, which are used to compute a\ncross-entropy loss during training. We use the Adam optimizer\nwith a learning rate of 0.01, and each client runs local updates\nfor four epochs per federated training round on mini-batches\nof size 64. To coordinate learning across clients, we employ a\nstandard federated averaging approach. After completing their\nlocal training, clients send their updated parameters to a central\nserver. The server aggregates these parameters by averaging\nthem to form a global model, which is then redistributed to\nall clients.\nIn terms of robustness, clients designated for adversarial\ntraining generate adversarial examples using a PGD-based\nmethod, parameterized by ϵ = 0.1, ten iterations, and a step\nsize of α = 0.01. During each local update, the mini-batch is\nsplit in half: one portion is trained on clean images, while the\nother portion is trained on adversarially perturbed images. This\nadversarial training framework is then scaled across different\nproportions of clients (0%, 20%, 50%, 100%), reflecting the\nfraction of the population actively mitigating attacks. We\ninvestigate the influence of adversarial training under varying\ncoverage (0%, 20%, 50%, and 100% of clients) and different\nclient counts (5, 10, and 15), to determine how the federated\nsystem scales. In the baseline case (0% adversarial coverage),\nall clients train exclusively on clean data for 50 rounds. For\nthe subsequent scenarios (20%, 50%, and 100% coverage),\nwe initialize local models from the global model obtained\nin the baseline and continue training for an additional 20\nrounds, allowing us to quantify the trade-off between clean data\naccuracy and adversarial robustness, while also revealing the\neffect of partial versus complete adversarial training coverage\nin a QFL environment.\nWe further examine how the system performs against varying\nadversarial strengths at inference. Specifically, we evaluate the\nfinal global model by generating test-time adversarial examples\nwith ϵ values ranging from 0.01 to 0.5. In each case, ten\niterations of PGD are used, and the step size is set to α = ϵ/10.\nThrough these configurations, we obtain a comprehensive view\nof the model’s resilience as the threat level escalates. The\nresults are reported in Tables I, II, III, and IV, where each cell\nshows the accuracy (%) of its respective experiment.\nB. Convergence and Performance Under Adversarial Coverage\nFigures 4, 5 and 6 illustrate the convergence of global test\nloss and test accuracy over 50 QFL rounds for 5, 10, and 15\nclients in clean training, and over 20 rounds for adversarial\ntraining. Compare performance under both clean conditions (0%\nadversarial data) and varying levels of adversarial data coverage\n(20%, 50%, and 100%). In the clean case, test loss decreases\n(e.g., from ≈0.96 to ≈0.80 for 5 clients) and accuracy rises\nsteadily (plateauing near 81% for 5 clients), confirming stable\nconvergence in non-adversarial conditions. As adversarial data\ncoverage increases, final accuracy drops and test loss remains\nhigher, but the system continues to converge across all client\nconfigurations, demonstrating the underlying robustness of\nQFL to adversarial presence, albeit with diminished ultimate\nperformance relative to the purely clean baseline.\nC. Impact of Client Count on Robustness\nTable I presents a comparison of global model accuracy\nunder adversarial attacks with varying perturbation strengths\n(ϵ) for systems comprising 5,10 , and 15 clients, all trained\nwithout adversarial defense (0% coverage). The results indicate\nthat clean accuracy (ϵ = 0) is highest for the system with\n5 clients (81.73%), followed by 15 clients (76.26%) and 10\nclients (72.35%). As the perturbation strength increases, all\nconfigurations experience a sharp decline in accuracy. Notably,\nat ϵ = 0.1, the system with 10 clients demonstrates marginally\nhigher robustness (37.88%) compared to those with 5 and\n15 clients, which achieve 32.13% and 32.19%, respectively.\nHowever, for ϵ ≥0.2, accuracy collapses to near-zero across\nall configurations, indicating a high vulnerability to strong\nadversarial perturbations.\nD. Effect of Adversarial Training Coverage\nTable II presents the performance of a system with 5 clients\nunder different levels of adversarial defense coverage. The\nbaseline configuration (0% coverage) achieves the highest clean\naccuracy (81.73%) but exhibits poor robustness, with accuracy\ndropping to 32.13% at ϵ = 0.1 . Introducing 20% coverage\n6\n\n\nenhances robustness at ϵ = 0.1 to 44.61%, representing a\n12.48% improvement over the baseline, while maintaining\nmoderate clean accuracy (76.90%) . Increasing coverage to\n50% further strengthens robustness at ϵ = 0.2 (9.53%);\nhowever, this comes at the cost of a reduction in clean\naccuracy (69.91%). A fully adversarially trained model (100%\ncoverage) strikes a balance, achieving a clean accuracy of\n78.33% while maintaining moderate robustness, with accuracy\nreaching 39.80% at ϵ = 0.1.\nTable III presents the performance of a system with 10\nclients under different levels of adversarial defense coverage.\nThe configuration with 50% coverage achieves the highest clean\naccuracy (84.52%) and exhibits the best robustness at ϵ = 0.05,\nreaching an accuracy of 64.57%. Increasing the coverage to\n100% results in the highest robustness at ϵ = 0.1, achieving\nan accuracy of 42.39%, which represents an improvement of\n4.51% over the baseline. Meanwhile, the 20% coverage setting\ndemonstrates the highest resilience at ϵ = 0.2, with an accuracy\nof 9.37%, but at the cost of reduced clean accuracy (68.16%).\nTable IV presents the performance of a system with 15 clients\nunder varying levels of adversarial defense coverage. The\nconfiguration with 20% coverage provides optimal robustness\nat ϵ = 0.1 and ϵ = 0.2, achieving accuracies of 40.90%\nand 8.29%, respectively. In contrast, the 100% coverage\nsetting maximizes clean accuracy (81.41%) and exhibits the\nhighest robustness at ϵ = 0.05 (61.84%), but its performance\ndeteriorates at higher perturbation strengths. The baseline\nconfiguration, lacking adversarial defense, experiences the\nsteepest accuracy decline, dropping to 32.19% at ϵ = 0.1.\nOur analysis reveals inherent trade-offs between adversarial\nrobustness and clean accuracy in QFL systems. Adversarial\ntraining enhances resilience to attacks (e.g., 50% coverage\nin 10 clients improves ϵ = 0.05 accuracy by +7.4%) but\noften reduces clean accuracy (−8.17% versus baseline). Higher\nadversarial coverage (50 −100%) amplifies robustness to\nmoderate perturbations (ϵ ≤0.1) at the expense of clean\nperformance, while systems with 10 −15 clients strike the\nmost effective balance between these metrics. Notably, all\nconfigurations collapse catastrophically (≤0.5% accuracy) for\nϵ ≥0.3, highlighting the diminishing returns of adversarial\ntraining against stronger attacks. Accuracy declines nonlinearly\nwith increasing ϵ: for instance, in a 5-client system with\n20% coverage, accuracy significantly drops from 74.10%\n(ϵ = 0.01) to 3.46% (ϵ = 0.2). Partial adversarial coverage\n(20-50%) proves optimal for practical deployments, particularly\nin intermediate-sized federations (10 clients), where robustness\ngains outweigh clean accuracy losses. These findings under-\nscore the need to tailor adversarial training coverage to both\nclient count and anticipated threat severity.\nFig. 7 presents the predictions for MNIST digits 0, 1, and\n2 in a federated learning setup with 5 clients, comparing\nclean images to those perturbed by a PGD attack (ϵ = 0.1)\nacross models trained with 0%, 20%, and 50% adversarial\ntraining coverage. In row 1, the baseline (0% coverage)\nmodel correctly classifies all digits with moderate confidence\n(e.g., 42.6% for digit 0, 59.3% for digit 1 , and 36.8% for\nTABLE I\nGLOBAL MODEL ACCURACY (%) WHEN VARYING THE NUMBER OF QFL\nCLIENTS (5, 10, 15).\nNum of Clients\nTest Accuracy (%)\nϵ = 0\n0.01\n0.05\n0.1\n0.2\n0.3\n0.5\n5 clients\n81.73\n77.09\n58.63\n32.13\n0.13\n0\n0\n10 clients\n72.35\n68.86\n57.17\n37.88\n2.93\n0\n0\n15 clients\n76.26\n71.40\n54.85\n32.19\n0.35\n0\n0\nTABLE II\nGLOBAL MODEL ACCURACY (%) FOR 5 CLIENTS. WE EVALUATE ON THE\nCLEAN TEST SET (ϵ = 0) AND ON ADVERSARIALLY PERTURBED TEST SETS\nWITH DIFFERENT ϵ.\nAdv Training\nTest Accuracy (%)\nϵ = 0\n0.01\n0.05\n0.1\n0.2\n0.3\n0.5\n0% (Baseline)\n81.73\n77.09\n58.63\n32.13\n0.13\n0\n0\n20%\n76.90\n74.10\n62.54\n44.61\n3.46\n0\n0\n50%\n69.91\n67.75\n58.47\n41.40\n9.53\n0.13\n0\n100%\n78.33\n75.47\n59.93\n39.08\n4.42\n0\n0\nTABLE III\nGLOBAL MODEL ACCURACY (%) FOR 10 CLIENTS. WE EVALUATE ON THE\nCLEAN TEST SET (ϵ = 0) AND ON ADVERSARIALLY PERTURBED TEST SETS\nWITH DIFFERENT ϵ.\nAdv Training\nTest Accuracy (%)\nϵ = 0\n0.01\n0.05\n0.1\n0.2\n0.3\n0.5\n0% (Baseline)\n72.35\n68.86\n57.17\n37.88\n2.93\n0\n0\n20%\n68.16\n65.40\n54.94\n37.40\n9.37\n0.06\n0\n50%\n84.52\n81.34\n64.57\n38.99\n2.38\n0\n0\n100%\n73.09\n69.65\n58.28\n42.39\n6.55\n0\n0\nTABLE IV\nGLOBAL MODEL ACCURACY (%) FOR 15 CLIENTS. WE EVALUATE ON THE\nCLEAN TEST SET (ϵ = 0) AND ON ADVERSARIALLY PERTURBED TEST SETS\nWITH DIFFERENT ϵ.\nAdv Training\nTest Accuracy (%)\nϵ = 0\n0.01\n0.05\n0.1\n0.2\n0.3\n0.5\n0% (Baseline)\n76.26\n71.40\n54.85\n32.19\n0.35\n0\n0\n20%\n65.59\n63.11\n55.20\n40.90\n8.29\n0\n0\n50%\n65.43\n64.16\n57.07\n39.18\n4.93\n0\n0\n100%\n81.41\n77.57\n61.84\n35.84\n0.44\n0\n0\ndigit 2). In row 2, under attack, the 0% model misclassifies\ndigit 0 as 1 (35.8% confidence) and digit 2 as 0 (34.8%),\nthough digit 1 remains correct (51.2%). With 20% adversarial\ntraining (row 3), digit 0 is still misclassified (38.7%), but\ndigits 1 and 2 are predicted correctly (53.1% and 36.1%\nrespectively). Finally, row 4 shows that the 50% coverage\nmodel, while still misclassifying digit 0 (35.9%), correctly\nidentifies digits 1 and 2 with improved confidence (60.9% and\n33.9%). Green borders denote correct predictions, while red\nborders indicate misclassifications, highlighting that increased\nadversarial training coverage enhances robustness against PGD\nattacks in this federated framework.\n7\n\n\na)\nc)\nd)\nb)\nFig. 4. Convergence of global test loss and progression of test accuracy for\n5 clients. (a) Convergence on clean data across 50 rounds, (b) Convergence\nwith 20% adversarial-data coverage across 20 rounds, (c) Convergence with\n50% adversarial-data coverage across 20 rounds, and (d) Convergence with\n100% adversarial-data coverage across 20 rounds.\na)\nc)\nd)\nb)\nFig. 5. Convergence of global test loss and progression of test accuracy for\n10 clients. (a) Convergence on clean data across 50 rounds, (b) Convergence\nwith 20% adversarial-data coverage across 20 rounds, (c) Convergence with\n50% adversarial-data coverage across 20 rounds, and (d) Convergence with\n100% adversarial-data coverage across 20 rounds.\nV. DISCUSSION\nThe integration of adversarial training into QFL introduces\nnuanced trade-offs between model accuracy, robustness, and\nscalability. Our experiments reveal several insights into the\ndynamics of QFL under adversarial conditions. A central\ntension emerges between maintaining high clean accuracy and\nachieving adversarial robustness. While adversarial training\nenhances resilience to perturbations (e.g., 20% coverage in\n5-client systems improves accuracy at ϵ = 0.1 by 12.48%), it\noften degrades performance on clean data (Table 1). This aligns\nwith classical FL observations, where robustness improvements\ntypically come at the cost of generalization [35].\nThe number of clients significantly influences system perfor-\nmance. Smaller federations (5 clients) achieve superior baseline\naccuracy (81.73%, Table 1), likely due to reduced parame-\na)\nc)\nd)\nb)\nFig. 6. Convergence of global test loss and progression of test accuracy for\n15 clients. (a) Convergence on clean data across 50 rounds, (b) Convergence\nwith 20% adversarial-data coverage across 20 rounds, (c) Convergence with\n50% adversarial-data coverage across 20 rounds, and (d) Convergence with\n100% adversarial-data coverage across 20 rounds.\nter divergence during FedAvg aggregation. However, larger\nfederations (10–15 clients) demonstrate greater potential for\nbalancing accuracy and robustness through adversarial training.\nFor example, 10-client systems with 50% coverage retain\nmoderate robustness (ϵ = 0.1: 38.99%) without sacrificing\nclean accuracy (84.52%), while 15-client systems with 20%\ncoverage achieve the highest robustness at ϵ = 0.2 (8.29%,\nTable 3). This implies that distributed adversarial training\nacross more clients—even at lower coverage—can enhance\ngeneralization by diversifying adversarial examples during\naggregation.\nDespite its benefits, adversarial training in QFL struggles\nagainst high-intensity attacks (ϵ ≥0.3), where accuracy\ncollapses to near-zero levels across all configurations. This\ncontrasts with classical FL, where adversarial defenses often\nmaintain non-trivial robustness even under extreme perturba-\ntions [20]. We hypothesize that amplitude encoding—while\nefficient for quantum state preparation—may amplify vulnera-\nbility to input perturbations, as small adversarial changes in\npixel values propagate nonlinearly through the quantum circuit.\nFuture work could explore alternative encoding schemes (e.g.,\nfeature map embeddings) or employ complex quantum circuits\ndesigns to counteract adversarial perturbations [36].\nInterestingly, partial adversarial coverage (20–50%) fre-\nquently outperforms full coverage (100%) in balancing accuracy\nand robustness in certain ϵ ranges. For example, 20% coverage\nin 5-client systems achieves 44.61% accuracy at ϵ = 0.1 (Table\n1), whereas 100% coverage yields only 39.08%. This suggests\nthat heterogeneous training—where some clients focus on clean\ndata and others on adversarial examples—preserves diverse\nfeature representations, preventing overfitting to adversarial\npatterns. This finding echoes recent classical FL studies\nadvocating for task-specialized clients [37], but extends the\nconcept to quantum domains.\nThese findings suggest that practitioners may tune both the\n8\n\n\nTrue: 0\nPred: 0 (42.6%)\nTrue: 1\nPred: 1 (59.3%)\nTrue: 2\nPred: 2 (36.8%)\nTrue: 0\nPred: 1 (35.8%)\nTrue: 1\nPred: 1 (51.2%)\nTrue: 2\nPred: 0 (34.8%)\nTrue: 0\nPred: 1 (38.7%)\nTrue: 1\nPred: 1 (53.1%)\nTrue: 2\nPred: 2 (36.1%)\nTrue: 0\nPred: 1 (35.9%)\nTrue: 1\nPred: 1 (60.9%)\nTrue: 2\nPred: 2 (33.9%)\nFig. 7. MNIST digit predictions for digits 0, 1, and 2 under varying levels of\nadversarial robustness. The first row displays clean images with no adversarial\ntraining (0%). The second row presents images attacked using PGD (ϵ = 0.1)\nfrom the 0% adversarial training model. The third row shows PGD-attacked\nimages from the 20% adversarial training model, while the fourth row contains\nPGD-attacked images from the 50% adversarial training model. Green borders\nindicate correct predictions, whereas red borders highlight misclassifications,\nwith confidence scores provided for each prediction.\nnumber of clients and the fraction of adversarially trained\nclients to balance clean accuracy with robustness, depending\non the anticipated threat model. Smaller FL networks can\nexcel in benign conditions but may require a higher fraction\nof adversarial training when moderate-to-high perturbations\nare likely. Conversely, larger networks might benefit from\ncarefully chosen subsets of adversarially trained clients, which\ncan improve robustness without sacrificing too much baseline\nperformance. In general, the observed dynamics indicate\nthat there is a necessity for more refined strategies that\ntailor adversarial training levels to the distribution of data\nacross clients. Future work could explore adaptive adversarial\ntraining schedules (e.g., high-risk clients train with stronger\nperturbations), hybrid defensive techniques (e.g quantum noise\ninjection [38] or dynamic adversarial client selection strategies)\nto reduce the impact of balancing robustness and accuracy.\nAdditionally, expanding the scope to stronger or more diverse\nattacks could reveal new insights into quantum federated\ndefenses for real-world scenarios.\nVI. CONCLUSION\nIn this work, we introduced QFAL, a framework that inte-\ngrates adversarial training into quantum federated learning to\naddress the unique vulnerabilities of quantum neural networks\nin distributed environments. Our investigation, conducted on\na tailored MNIST subset, demonstrated that even limited\nadversarial participation among clients can markedly enhance\nresilience against moderate attacks, while also uncovering a\nnontrivial trade-off between clean-data accuracy and robustness.\nNotably, our experiments indicate that the optimal balance\ndepends on both the size of the client federation and the\nfraction of clients engaging in adversarial training.\nBeyond validating the feasibility of adversarial defenses in\nQFL, our results highlight key challenges such as the collapse\nof accuracy under strong perturbations and the sensitivity\nintroduced by amplitude encoding. These insights suggest\nthat further refinement of quantum data encoding methods,\nas well as adaptive and heterogeneous training strategies, may\nbe necessary to fully harness the potential of quantum federated\nsystems in adversarial settings.\nLooking forward, expanding our framework to incorporate\ndynamic adversarial scheduling, exploring alternative quantum\narchitectures, and testing under a broader array of attack\nmodels will be crucial for advancing secure distributed quantum\nmachine learning. We anticipate that the QFAL framework and\nour experimental results will inspire further exploration into\nresilient quantum federated systems, paving the way for the\ncreation of dependable quantum-enhanced solutions in practical\napplications.\nACKNOWLEDGMENT\nThis work was supported in parts by the NYUAD Center\nfor Quantum and Topological Systems (CQTS), funded by\nTamkeen under the NYUAD Research Institute grant CG008,\nand the NYUAD Center for Cyber Security (CCS), funded by\nTamkeen under the NYUAD Research Institute Award G1104.\nREFERENCES\n[1] M. Asad, S. Shaukat, D. Hu, Z. Wang, E. Javanmardi, J. Nakazato, and\nM. Tsukada, “Limitations and future aspects of communication costs in\nfederated learning: A survey,” Sensors, vol. 23, no. 17, p. 7358, 2023.\n[2] K. Beer, D. Bondarenko, T. Farrelly, T. J. Osborne, R. Salzmann,\nD. Scheiermann, and R. Wolf, “Training deep quantum neural networks,”\nNature communications, vol. 11, no. 1, p. 808, 2020.\n[3] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner,\n“The power of quantum neural networks,” Nature Computational Science,\nvol. 1, no. 6, pp. 403–409, 2021.\n[4] N. Innan, A. Marchisio, M. Bennai, and M. Shafique, “Lep-qnn: Loan\neligibility prediction using quantum neural networks,” arXiv preprint\narXiv:2412.03158, 2024.\n[5] N. Innan, B. K. Behera, S. Al-Kuwari, and A. Farouk, “Qnn-vrcs: A\nquantum neural network for vehicle road cooperation systems,” IEEE\nTransactions on Intelligent Transportation Systems, 2025.\n[6] N. Innan and M. Bennai, “A variational quantum perceptron with grover’s\nalgorithm for efficient classification,” Physica Scripta, vol. 99, no. 5, p.\n055120, 2024.\n9\n\n\n[7] N. Innan, M. A.-Z. Khan, A. Marchisio, M. Shafique, and M. Bennai,\n“Fedqnn: Federated learning using quantum neural networks,” arXiv\npreprint arXiv:2403.10861, 2024.\n[8] M. Chehimi and W. Saad, “Quantum federated learning with quantum\ndata,” in ICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP).\nIEEE, 2022, pp. 8617–8621.\n[9] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S.\nKohane, “Adversarial attacks on medical machine learning,” Science, vol.\n363, no. 6433, pp. 1287–1289, 2019.\n[10] A. Marchisio, M. A. Hanif, and M. Shafique, “Adversarial ml for dnns,\ncapsnets, and snns at the edge,” in Embedded Machine Learning for\nCyber-Physical, IoT, and Edge Computing: Use Cases and Emerging\nChallenges.\nSpringer, 2023, pp. 463–496.\n[11] N. Chattopadhyay, A. Guesmi, M. A. Hanif, B. Ouni, and M. Shafique,\n“Defending against adversarial patches using dimensionality reduction,”\nin Proceedings of the 61st ACM/IEEE Design Automation Conference,\n2024, pp. 1–6.\n[12] A. Guesmi, R. Ding, M. A. Hanif, I. Alouani, and M. Shafique, “Dap: A\ndynamic adversarial patch for evading person detectors,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2024, pp. 24 595–24 604.\n[13] W. El Maouaki, A. Marchisio, T. Said, M. Bennai, and M. Shafique,\n“Advqunn: A methodology for analyzing the adversarial robustness of\nquanvolutional neural networks,” in 2024 IEEE International Conference\non Quantum Software (QSW).\nIEEE, 2024, pp. 175–181.\n[14] W. El Maouaki, A. Marchisio, T. Said, M. Shafique, and M. Bennai,\n“Robqunns: A methodology for robust quanvolutional neural networks\nagainst adversarial attacks,” in 2024 IEEE International Conference on\nImage Processing Challenges and Workshops (ICIPCW).\nIEEE, 2024,\npp. 4090–4095.\n[15] S. Lu, L.-M. Duan, and D.-L. Deng, “Quantum adversarial machine\nlearning,” Physical Review Research, vol. 2, no. 3, p. 033212, 2020.\n[16] J. Zhang, B. Li, C. Chen, L. Lyu, S. Wu, S. Ding, and C. Wu, “Delving\ninto the adversarial robustness of federated learning,” in Proceedings of\nthe AAAI conference on artificial intelligence, vol. 37, no. 9, 2023, pp.\n11 245–11 253.\n[17] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd,\n“Quantum machine learning,” Nature, vol. 549, no. 7671, pp. 195–202,\n2017.\n[18] M. Schuld, I. Sinayskiy, and F. Petruccione, “An introduction to quantum\nmachine learning,” Contemporary Physics, vol. 56, no. 2, pp. 172–185,\n2015.\n[19] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,\nR. Fergus et al., “Intriguing properties of neural networks,” arXiv preprint\narXiv:1312.6199, 2013.\n[20] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards\ndeep learning models resistant to adversarial attacks,” in International\nConference on Learning Representations, 2018.\n[21] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry, “Robust-\nness may be at odds with accuracy,” arXiv preprint arXiv:1805.12152,\n2018.\n[22] S. Y.-C. Chen and S. Yoo, “Federated quantum machine learning,”\nEntropy, vol. 23, no. 4, p. 460, 2021.\n[23] R. Huang, X. Tan, and Q. Xu, “Quantum federated learning with\ndecentralized data,” IEEE Journal of Selected Topics in Quantum\nElectronics, vol. 28, no. 4: Mach. Learn. in Photon. Commun. and\nMeas. Syst., pp. 1–10, 2022.\n[24] N. Innan, M. A.-Z. Khan, A. Marchisio, M. Shafique, and M. Bennai,\n“Fedqnn: Federated learning using quantum neural networks,” in 2024\nInternational Joint Conference on Neural Networks (IJCNN), 2024, pp.\n1–9.\n[25] N. Innan, A. Marchisio, and M. Shafique, “Fl-qdsnns: Federated\nlearning with quantum dynamic spiking neural networks,” arXiv preprint\narXiv:2412.02293, 2024.\n[26] W. Li, S. Lu, and D.-L. Deng, “Quantum federated learning through blind\nquantum computing,” Science China Physics, Mechanics & Astronomy,\nvol. 64, no. 10, p. 100312, 2021.\n[27] Z. Qu, L. Zhang, and P. Tiwari, “Quantum fuzzy federated learning\nfor privacy protection in intelligent information processing,” IEEE\nTransactions on Fuzzy Systems, 2024.\n[28] J. Qi and M.-H. Hsieh, “Federated quantum natural gradient descent for\nquantum federated learning,” in Federated Learning.\nElsevier, 2024,\npp. 329–341.\n[29] R. Rofougaran, S. Yoo, H.-H. Tseng, and S. Y.-C. Chen, “Federated\nquantum machine learning with differential privacy,” in ICASSP 2024-\n2024 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2024, pp. 9811–9815.\n[30] S. Dutta, N. Innan, S. B. Yahia, M. Shafique, and D. E. B. Neira,\n“Mqfl-fhe: Multimodal quantum federated learning framework with fully\nhomomorphic encryption,” arXiv preprint arXiv:2412.01858, 2024.\n[31] S. Dutta, P. P. Karanth, P. M. Xavier, I. L. de Freitas, N. Innan,\nS. B. Yahia, M. Shafique, and D. E. B. Neira, “Federated learning\nwith quantum computing and fully homomorphic encryption: A novel\ncomputing paradigm shift in privacy-preserving ml,” arXiv preprint\narXiv:2409.11430, 2024.\n[32] J. Qi, X.-L. Zhang, and J. Tejedor, “Optimizing quantum federated\nlearning based on federated quantum natural gradient descent,” in ICASSP\n2023-2023 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2023, pp. 1–5.\n[33] G. Subramanian and M. Chinnadurai, “Hybrid quantum enhanced\nfederated learning for cyber attack detection,” Scientific Reports, vol. 14,\nno. 1, p. 32038, 2024.\n[34] Q. Xia, Z. Tao, and Q. Li, “Defending against byzantine attacks in\nquantum federated learning,” in 2021 17th International Conference on\nMobility, Sensing and Networking (MSN).\nIEEE, 2021, pp. 145–152.\n[35] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” arXiv preprint arXiv:1412.6572, 2014.\n[36] W. E. Maouaki, A. Marchisio, T. Said, M. Shafique, and M. Bennai,\n“Designing robust quantum neural networks: Exploring expressibility,\nentanglement, and control rotation gate selection for enhanced quantum\nmodels,” arXiv preprint arXiv:2411.11870, 2024.\n[37] F. Yu, W. Zhang, Z. Qin, Z. Xu, D. Wang, C. Liu, Z. Tian, and X. Chen,\n“Heterogeneous federated learning,” arXiv preprint arXiv:2008.06767,\n2020.\n[38] Y. Du, M.-H. Hsieh, T. Liu, D. Tao, and N. Liu, “Quantum noise protects\nquantum classifiers against adversaries,” Physical Review Research, vol. 3,\nno. 2, p. 023153, 2021.\n10\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21171v1.pdf",
    "total_pages": 10,
    "title": "QFAL: Quantum Federated Adversarial Learning",
    "authors": [
      "Walid El Maouaki",
      "Nouhaila Innan",
      "Alberto Marchisio",
      "Taoufik Said",
      "Mohamed Bennai",
      "Muhammad Shafique"
    ],
    "abstract": "Quantum federated learning (QFL) merges the privacy advantages of federated\nsystems with the computational potential of quantum neural networks (QNNs), yet\nits vulnerability to adversarial attacks remains poorly understood. This work\npioneers the integration of adversarial training into QFL, proposing a robust\nframework, quantum federated adversarial learning (QFAL), where clients\ncollaboratively defend against perturbations by combining local adversarial\nexample generation with federated averaging (FedAvg). We systematically\nevaluate the interplay between three critical factors: client count (5, 10,\n15), adversarial training coverage (0-100%), and adversarial attack\nperturbation strength (epsilon = 0.01-0.5), using the MNIST dataset. Our\nexperimental results show that while fewer clients often yield higher\nclean-data accuracy, larger federations can more effectively balance accuracy\nand robustness when partially adversarially trained. Notably, even limited\nadversarial coverage (e.g., 20%-50%) can significantly improve resilience to\nmoderate perturbations, though at the cost of reduced baseline performance.\nConversely, full adversarial training (100%) may regain high clean accuracy but\nis vulnerable under stronger attacks. These findings underscore an inherent\ntrade-off between robust and standard objectives, which is further complicated\nby quantum-specific factors. We conclude that a carefully chosen combination of\nclient count and adversarial coverage is critical for mitigating adversarial\nvulnerabilities in QFL. Moreover, we highlight opportunities for future\nresearch, including adaptive adversarial training schedules, more diverse\nquantum encoding schemes, and personalized defense strategies to further\nenhance the robustness-accuracy trade-off in real-world quantum federated\nenvironments.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}