{
  "id": "arxiv_2502.20661v1",
  "text": "Published as a conference paper at ICLR 2025\nDIMENSION AGNOSTIC NEURAL PROCESSES\nHyungi Lee, Chaeyun Jang, Dong bok Lee, Juho Lee\nKAIST\n{lhk2708, jcy9911, markhi, juholee}@kaist.ac.kr\nABSTRACT\nMeta-learning aims to train models that can generalize to new tasks with limited\nlabeled data by extracting shared features across diverse task datasets. Additionally,\nit accounts for prediction uncertainty during both training and evaluation, a concept\nknown as uncertainty-aware meta-learning. Neural Process (NP) is a well-known\nuncertainty-aware meta-learning method that constructs implicit stochastic pro-\ncesses using parametric neural networks, enabling rapid adaptation to new tasks.\nHowever, existing NP methods face challenges in accommodating diverse input\ndimensions and learned features, limiting their broad applicability across regression\ntasks. To address these limitations and advance the utility of NP models as general\nregressors, we introduce Dimension Agnostic Neural Process (DANP). DANP in-\ncorporates Dimension Aggregator Block (DAB) to transform input features into a\nfixed-dimensional space, enhancing the model’s ability to handle diverse datasets.\nFurthermore, leveraging the Transformer architecture and latent encoding layers,\nDANP learns a wider range of features that are generalizable across various tasks.\nThrough comprehensive experimentation on various synthetic and practical regres-\nsion tasks, we empirically show that DANP outperforms previous NP variations,\nshowcasing its effectiveness in overcoming the limitations of traditional NP models\nand its potential for broader applicability in diverse regression scenarios.\n1\nINTRODUCTION\nIn real-world datasets, there are many tasks that come with various configurations (such as input\nfeature dimensions, quantity of training data points, correlation between training and validation data,\netc.). However, each task has a limited number of data points available, making it difficult to train\na model capable of robust generalization solely based on the provided training data. To tackle this\nissue, meta-learning aims to train a model capable of generalizing to new tasks with few labeled\ndata by learning generally shared features from diverse training task datasets. In cases of limited\nlabeled data for new target tasks, ensuring model trustworthiness involves accurately quantifying\nprediction uncertainty, which is as critical as achieving precise predictions. A meta-learning strategy\nthat considers prediction uncertainty during training and evaluation is known as uncertainty-aware\nmeta-learning (Nguyen & Grover, 2022; Almecija et al., 2022).\nOne of the well-known uncertainty-aware meta-learning methods is Neural Process (NP) (Garnelo\net al., 2018a;b). NP employs meta-learning to understand the data-generation process governing\nthe relationship between input-output pairs in meta-training and meta-validation data. Unlike the\ntraditional approach to learning stochastic processes, where model selection from a known class, e.g.\nGaussian Processes (GPs), precedes computing predictive distributions based on training data, NP\nconstructs an implicit stochastic process using parametric neural networks trained on meta-training\ndata. It then optimizes parameters to maximize the predictive likelihood for both the meta-train and\nmeta-validation data. Consequently, when NP effectively learns the data-generation process solely\nfrom data, it can quickly identify suitable stochastic processes for new tasks. Thus, NP can be viewed\nas a data-driven uncertainty-aware meta-learning method for defining stochastic processes.\nHowever, previous works (Gordon et al., 2020; Foong et al., 2020; Lee et al., 2020; Nguyen & Grover,\n2022; Lee et al., 2023) in NP literature lack two crucial attributes essential for broad applicability\nacross different regression tasks: 1) the ability to directly accommodate diverse input and output\ndimensions, and 2) the adaptability of learned features for fine-tuning on new tasks that exhibit\nvarying input and output dimensions. Due to the absence of these two properties, it is necessary\n1\narXiv:2502.20661v1  [cs.LG]  28 Feb 2025\n\n\nPublished as a conference paper at ICLR 2025\nto train each NP model separately for different dimensional tasks. These limitations hinder the\nutility of NP models as general regressors across diverse datasets compared to traditional stochastic\nprocesses (Lee et al., 2021). Traditional stochastic processes naturally accommodate varying input\ndimensions, particularly in regression tasks involving high-dimensional input features with limited\ntraining data, such as hyperparameter optimization tasks.\nTo tackle these limitations and advance the utilization of NP models as general regressors for a\nwide range of regression tasks, we introduce a novel extension of NP called Dimension Agnostic\nNeural Process (DANP). In DANP, we propose a module called Dimension Aggregator Block (DAB),\nwhich transforms input features of varying dimensions into a fixed-dimensional representation space.\nThis allows subsequent NP modules to effectively handle diverse datasets and generate predictive\ndensity for the meta-validation data. We also add the Transformer architecture (Vaswani et al.,\n2017) alongside latent encoding layers based on the architecture of Transformer Neural Processes\n(TNP) (Nguyen & Grover, 2022) to enhance the model’s ability to learn a wider range of features\nand effectively capture functional uncertainty, which can be applied across different tasks. Through\nexperimentation on a variety of synthetic and real-world regression tasks with various situations,\nwe show that DANP achieves notably superior predictive performance compared to previous NP\nvariations.\n2\nBACKGROUND\n2.1\nPROBLEM SETTINGS\nLet X be an input space defined as S\ni∈N Xi with each Xi ⊆Ri for all i ∈N. Similarly, let Y =\nS\ni∈N Yi represent the output space, where each Yi ⊆Ri for all i ∈N. Let T = {τj}j∈N be a task set\ndrawn in i.i.d. fashion from a task distribution ptask(τ). Given two dimension mapping functions u, v :\nN →N, each task τj comprises a dataset Dj = {dj,k}nj\nk=1, where dj,k = (xj,k, yj,k) ∈Xu(j)×Yv(j)\nrepresents an input-output data pair, along with an index set cj ⊊[nj] where [m] := {1, . . . , m}\nfor all m ∈N. We assume elements in Dj are i.i.d. conditioned on some function fj. Here, the\nset of indices cj defines the context set Dj,c := {dj,k}k∈cj. Similarly, the target set is defined as\nDj,t := {dj,k}k∈tj where tj := [nj] \\ cj. We aim to meta-learn a collection of random functions\nfj : Xu(j) →Yv(j), where each function within this set effectively captures and explains the\nconnection between input x and output y pairs. For any given meta-training task τj, we can regard its\ncontext set Dj,c as the meta-training set and its target set Dj,t as the meta-validation set.\n2.2\nNEURAL PROCESSES\nFor the previous NP variants, their objective was to meta-learn a set of random functions fj : Xdin →\nYdout, for some fixed din, dout ∈N, which is equal to the situation where dimension mapping functions\nu, v are constant functions, i.e., u(j) = din and v(j) = dout for all j ∈N. In this context, to select\na suitable random function fj for the task τj, NPs meta-learns how to map the context set Dj,c to\na random function fj that effectively represents both the context set Dj,c and the target set Dj,t.\nThis entails maximizing the likelihood for both meta-training and meta-validation datasets within an\nuncertainty-aware meta-training framework. The process involves learning a predictive density that\nmaximizes the likelihood using the following equation:\np(Yj|Xj, Dj,c) =\nZ \u0014 Y\nk∈[nj]\np(yj,k|fj, xj,k)\n\u0015\np(fj|Dj,c)dfj,\n(1)\nwhere Xj = {xj,k}nj\nk=1 and Yj = {yj,k}nj\nk=1. In line with our discussion in Section 2.1, we make\nthe assumption that given the random function fj, the outputs collection Yj are i.i.d. Employing the\nGaussian likelihood and parameterizing fj with latent variable rj ∈Rdj, Eq. 1 reduces to,\np(Yj|Xj, Dj,c) =\nZ \u0014 Y\nk∈[nj]\nN\n\u0010\nyj,k|µrj(xj,k), diag(σ2\nrj(xj,k))\n\u0011 \u0015\np(rj|Dj,c)drj,\n(2)\nwhere µrj : Xdin →Ydout and σ2\nrj : Xdin →Rdout\n+ .\nThen different NP variants aim to ef-\nfectively design the model structures of the encoder, denoted as fenc, and the decoder, de-\n2\n\n\nPublished as a conference paper at ICLR 2025\nFigure 1: Model comparison between TNP and DANP. While TNP (Nguyen & Grover, 2022) solely\nemploys a deterministic pathway with Masked Transformer layers, DANP incorporates both DAB and\nan extra latent pathway alongside Transformer layers and a Self-Attention layer.\nnoted as fdec. These components are responsible for describing the distributions p(rj|Dj,c) and\nN\n\u0010\nyj,k|µrj(xj,k), diag(σ2\nrj(xj,k))\n\u0011\n, respectively.\nNP variations can be roughly categorized into two classes based on their approach to modeling\np(rj|Dj,c): 1) Conditional Neural Processes (CNPs) (Garnelo et al., 2018a; Gordon et al., 2020;\nNguyen & Grover, 2022) and 2) (latent) NPs (Garnelo et al., 2018b; Foong et al., 2020; Lee et al.,\n2023). CNPs establish a deterministic function, called deterministic path, from Dj,c to rj and represent\np(rj|Dj,c) as a discrete point measure, expressed as:\np(rj|Dj,c) = δ¯rj(r),\n¯rj = fenc(Dj,c; ϕ),\n(3)\nwhere ϕ is the parameter of fenc. In contrast, NPs address functional uncertainty or model uncertainty\nin modeling p(rj|Dj,c). Typically, they employ a variational posterior q(rj|Dj,s), called latent path,\nto approximate p(rj|Dj,s) for any subset Dj,s ⊆Dj, defined as:\nq(rj|Dj,s) = N(rj|mDj,s, diag(s2\nDj,s)),\n(mDj,s, s2\nDj,s) = fenc(Dj,s; ϕ).\n(4)\nThen both of the classes decode the mean and variance of the input xj,k as follows:\n(µrj(xj,k), σ2\nrj(xj,k)) = fdec(xj,k, rj; ψ),\n(5)\nwhere fdec is another feedforward neural net ψ.\nTraining CNPs involves maximizing the average predictive log-likelihood across meta-training tasks\nτj, i.e. Eτj[log p(Yj|Xj, Dj,c)]. On the other hand, NPs are typically trained by maximizing the\nEvidence Lower BOund (ELBO), which is expressed as:\nEτj[log p(Yj|Xj, Dj,c)] ≥Eτj\n\nX\nk∈[nj]\nEq(rj|Dj) [log Nj,k] −KL[q(rj|Dj)|q(rj|Dj,c)]\n\n,\n(6)\nwhere Nj,k is a shorthand for N\n\u0010\nyj,k|µrj(xj,k), diag(σ2\nrj(xj,k))\n\u0011\n.\nThere have been several attempts to enhance the flexibility of the encoder fenc to improve the\npredictive performance (Garnelo et al., 2018a; Kim et al., 2018; Gordon et al., 2020; Nguyen &\nGrover, 2022). In this study, we adopt the state-of-the-art TNP model as our base structure, which\nleverages masked self-attention layers as encoding layers and also belongs to the category of CNPs\nvariants.\n3\nDIMENSION AGNOSTIC NEURAL PROCESS\nAs we mentioned in Section 1 and Section 2.2, the limitation of the previous NP variants is that they\nprimarily handle scenarios where the input space and output space are confined to Xdin and Ydout\nfor the fixed din, dout ∈N. To address this constraint, we introduce a novel NP variant called DANP.\nInitially, we elucidate how the predictive density evolves when the dimension mapping functions u, v\nare not constant in Section 3.1. Subsequently, we expound on how we can convert input features\nof varying dimensions into a fixed-dimensional representation space utilizing the DAB module in\nSection 3.2. Finally, we detail the strategies employed to augment the model’s capacity for learning\ndiverse features in Section 3.3.\n3\n\n\nPublished as a conference paper at ICLR 2025\n3.1\nA MODEL OF PREDICTIVE DENSITY WITH VARYING DIMENSIONS\nFigure 2: The overview of DAB\nmodule.\nA DAB can encode\nand decode inputs and outputs\nof varying dimensions.\nGiven our assumption that all the tasks may have varying input\nand output dimensions, we write Dj = {dj,k}nj\nk=1 where xj,k :=\n[x1\nj,k . . . xu(j)\nj,k ] ∈Ru(j) and yj,k := [y1\nj,k . . . y(v(j)\nj,k\n] ∈Rv(j).\nGiven the context Dj,c, the equation for the predictive density\np(Yj|Xj, Dj,c) remains the same with Eq. 2. However, due to\nthe varying dimensions, the computation of both the likelihood\nNj,k and the context representation posterior p(rj|Dj,c) poses\na challenge. In a fixed dimension setting, only the size of the\ncontext varies across different tasks, and this could be processed\nby choosing fenc as a permutation-invariant set functions (Zaheer\net al., 2017). However, in our scenario, for two different tasks τ1\nand τ2, a single encoder should compute,\nfenc(D1,c; ϕ) = fenc({((x1\n1,k, . . . , xu(1)\n1,k ), (y1\n1,k, . . . , yv(1)\n1,k ))}k∈c1; ϕ),\n(7)\nfenc(D2,c; ϕ) = fenc({((x1\n2,k, . . . , xu(2)\n2,k ), (y1\n2,k, . . . , yv(2)\n2,k ))}k∈c2; ϕ).\n(8)\nThe existing permutation-invariant encoder can process when\n|c1| ̸= |c2|, it cannot handle when (u(1), v(1)) ̸= (u(2), v(2)),\nbecause this disparity happens at the lowest level of the encoder,\ntypically implemented with a normal feed-forward neural network.\nThe standard architecture in the previous NP models is to employ\na Multi-Layer Perceptron (MLP) taking the concatenated inputs,\nfor instance,\nfenc(Dj,c; ϕ) =\n1\n|cj|\nX\nk∈cj\nMLP(concat(xj,k, yj,k)),\n(9)\nand the MLP encoder can only process fixed-dimensional inputs. A similar challenge also applies to\nthe decoder fdec computing the predictive mean µrj(xj,k) and variance σ2\nrj(xj,k). To address this\nchallenge, we need a new neural network that is capable of handling sets of varying dimensions.\n3.2\nDIMENSION AGGREGATOR BLOCK\nTo enable our model to process sets with elements of varying dimensions, we introduce a module\ncalled Dimension Aggregator Block (DAB). The module can encode inputs with varying feature\ndimensions into a fixed-dimensional representation and varying dimensional representations, which\nwe will describe individually below. The overall architecture is depicted in Fig. 2.\nEncoding x into a fixed dimensional representation.\nConsider an input (x, y) where x =\n[x1 . . . xdx] ∈Rdx and y = [y1 . . . ydy] ∈Rdy. To maintain sufficient information for each\ndimension of the input data, even after it has been mapped to a fixed-dimensional representation, we\ninitially expand each of the dx + dy dimensions of the input data to dr dimensions using learnable\nlinear projection w ∈Rdr as follows:\n[˜x, ˜y] = w[concat(x, y)]⊤∈Rdr×(dx+dy).\n(10)\nWhen encoding a point in the target set Dj,t without the label y, we simply encode the zero-padded\nvalue concat(x, 0). Next, following Vaswani et al. (2017), we incorporate cosine and sine positional\nencoding to distinguish and retain positional information for each input dimension as follows:\nPEX(2i,j) = sin(j/P(i)), PEX(2i+1,j) = cos(j/P(i)),\n(11)\nPEY(2i,l) = cos(l/P(i)), PEY(2i+1,l) = sin(l/P(i)),\n(12)\nwhere P(i) = 100002i/dr and PEX, PEY represent the positional encoding for x and y respectively.\nHere, j ∈[dx] and l ∈[dy] respectively denote the position indices of ˜x and ˜y, while i ∈⌊dr\n2 ⌋\n4\n\n\nPublished as a conference paper at ICLR 2025\nrepresents a dimension in the representation space. Since the dimensions of both x and y vary and\nthe representation must be divided between corresponding positions in x and y, e.g., x1 and y1, we\nuse distinct positional embeddings, PEX for x and PEY for y. After adding positional encoding to\n(˜x, ˜y), we further compute,\n(˜x, ˜y) = SelfAttn(concat(˜x, ˜y) + (PEX, PEY)),\n(13)\nwhere SelfAttn indicates a self-attention layer (Vaswani et al., 2017). Here, we can regard (˜x, ˜y)\nbefore the self-attention layer as akin to context-free embeddings (Rong, 2014), because they remain\nunchanged when position and value are fixed. Conversely, (˜x, ˜y) after the Self-Attention layer can\nbe likened to contextual embeddings (Devlin et al., 2018), which is known as advanced embedding\ncompared to context-free embedding, as the final representation may vary depending on alterations\nin value and position across other dimensions due to the interaction through the self-attention layer.\nThen, we employ average pooling for the ˜x to integrate all the information across varying dimensions\nin x into a fixed-dimensional representation, i.e., ˆx = AvgPool(˜x) ∈Rdr, with AvgPool representing\nthe average pooling operation across the feature dimension.\nHandling variable number of outputs for y.\nThe DAB should produce representations that can be\nused in the decoder later to produce outputs with varying dimensions. To achieve this, unlike for the\nx, we keep the sequence ˜y without average pooling. Instead, for each ℓ= 1, . . . , dy, we concatenate\nˆx and ˜yl and put into the decoder to get the predictive mean and variances. The dimension of the\nencoding for y from the DAB would be the same as original dimension of y. Note that this is not like\nthe sequential decoding in autoregressive models, and is possible because we know the dimension of\ny before we actually have to decode the representation.\n3.3\nLEARNING MORE GENERAL FEATURE UTILIZING LATENT PATH\nLet ˜Dj := (ˆxj,k, (˜yl\nj,k)v(j)\nl=1 )nj\nk=1 be the representations obtained by DAB for the dataset Dj. The\nnext step involves computing the predictive density using the encoder and decoder structure. Here,\nwe employ TNP (Nguyen & Grover, 2022), a variant of CNPs, as our base model structure. In TNP,\nMasked Transformer layers are utilized as the encoder for the deterministic path, while a simple MLP\nstructure serves as the decoder. To improve the model’s capacity to learn generally shared features\nacross various tasks and effectively capture functional uncertainty, we introduce a new latent path\ncomprising Transformer layers and a Self-Attention layer alongside the single deterministic path\nencoder. In specific, we pass the entire ˜Dj into Masked Transformer layers to make deterministic\nparameter rdet\nj\nas follows:\nzl\nj,k = concat(ˆxj,k, ˜yl\nj,k) ∈R2dr,\n(14)\nrdet\nj\n= MTFL(concat({{zl\nj,k}v(j)\nl=1 }nj\nk=1), Mj) ∈Rv(j)nj×2dr,\n(15)\nwhere MTFL denotes Masked Transformer layers with mask Mj, and concat({{zl\nj,k}v(j)\nl=1 }nj\nk=1)\nindicate concatenation operation which concatenate zl\nj,k for all l ∈[v(j)] and k ∈[nj]. In this\ncontext, for all l1, l2 ∈[v(j)], the mask Mj ∈Rv(j)nj×v(j)nj assigns a value of 1 to the index\n(l1k1, l2k2) if both k1 and k2 are elements of cj, or if k1 is in tj and k2 is in cj; otherwise, it assigns\na value of 0.\nFor the latent path, we only pass context set ˜Dj,c through Transformer layers, followed by one\nself-attention and MLP operation to determine the latent parameter rlat\nj as follows:\n¯rlat\nj = AvgPool(SelfAttn(TL(concat({{zl\nj,k}v(j)\nl=1 }k∈cj))),\n(16)\n(mDj,c, s2\nDj,c) = MLP(¯rlat\nj ),\n(17)\nrlat\nj ∼q(rlat\nj | ˜Dj,c) = N(rlat\nj |mDj,c, diag(s2\nDj,c)),\n(18)\nwhere TL denotes Transformer layers. Finally, we concatenate the deterministic parameter rdet\nj\nand\nlatent parameter rlat\nj to make the final parameter rj before forwarding them to the decoder module.\nThen, by utilizing a variational posterior with the latent path, our training objective transforms into\nEτj[log p(Yj|Xj, Dj,c)] ≥Eτj\n\nX\nk∈[nj]\nEq(rlat\nj |Dj) [log Nj,k] −KL[qj∥qj,c]\n\n,\n(19)\n5\n\n\nPublished as a conference paper at ICLR 2025\nwhere qj and qj,c denotes q(rlat\nj |Dj) and q(rlat\nj |Dj,c), respectively. Refer to Fig. 1 to observe the\ncontrast in the architecture between TNP and DANP.\n4\nRELATED WORKS\nNeural Processes\nThe first NPs model, called CNP (Garnelo et al., 2018a), utilized straightforward\nMLP layers for both its encoder and decoder. Similarly, NP (Garnelo et al., 2018b) adopted MLP\nlayers but introduced a global latent variable to capture model uncertainty, marking an early attempt\nto address uncertainty in NP frameworks. Conditional Attentive Neural Process (CANP) and Attentive\nNeural Process (ANP) (Kim et al., 2018) are notable for incorporating attention mechanisms within\nthe encoder, enhancing the summarization of context information relevant to target points. Building\non these ideas, TNP (Nguyen & Grover, 2022) employs masked transformer layers in its encoder,\ndelivering state-of-the-art performance among NPs across multiple tasks. Louizos et al. (2019)\nintroduced a variant that used local latent variables instead of a global latent variable to improve\nthe model’s ability to capture uncertainty. Following this, Bootstrapping Attentive Neural Process\n(BANP) (Lee et al., 2020) proposed the residual bootstrap method (Efron, 1992), making NPs more\nrobust to model misspecification. Lastly, Martingale Posterior Attentive Neural Process (MPANP) (Lee\net al., 2023) addressed model uncertainty with the martingale posterior (Fong et al., 2021), offering a\nmodern alternative to traditional Bayesian inference methods. Refer to Appendix B to see a more\ndetailed review of previous Neural Processes works.\nNeural Diffusion Process\nSimilar to DANP, there are prior works (Liu et al., 2020; Kossen et al.,\n2021; Dutordoir et al., 2023) that utilize bi-dimensional attention blocks to facilitate more informative\ndata feature updates or to ensure the permutation invariance property both at the data-instance level\nand the dimension level. Specifically, Neural Diffusion Process (NDP) (Dutordoir et al., 2023)\nemploys bi-dimensional attention blocks to guarantee permutation invariance both at the data and\ndimension levels, naturally leading to dimension-agnostic properties. However, NDP has a structural\nlimitation in that it is only partially dimension-agnostic for x when y = 1, and is not dimension-\nagnostic for other combinations. This makes it difficult to use as a general regressor. Additionally,\nthe use of diffusion-based sampling to approximate the predictive distribution leads to significantly\nhigh computational costs during inference and results in limited likelihood performance. Refer to\nAppendix D.1 to see the empirical comparison between DANP and NDP.\n5\nEXPERIMENTS\nIn this section, we carry out a series of experiments to empirically showcase the efficacy of DANP\nacross different situations, especially in various regression tasks and Bayesian Optimization task. To\nestablish a robust experimental foundation, we employ five distinct variations of NP, encompassing\nstate-of-the-art model: CANP, ANP, BANP, MPANP, and TNP. For a fair comparison, we maintain\nan identical latent sample size in the latent path across all models, except for deterministic models\nsuch as CANP and TNP. We marked the best performance value with boldfaced underline, and the\nsecond-best value with underline in each column in all tables. All the performance metrics are\naveraged over three different seeds and we report 1-sigma error bars for all experiments. Refer to\nAppendix C for experimental details containing data description and model structures.\n5.1\nGP REGRESSION\nTo empirically verify the effectiveness of DANP, we initially conducted GP regression experiments\nunder various conditions: From-scratch, Zero-shot, and Fine-tuning. In the From-scratch scenario,\nwe compared DANP against other baselines using fixed input dimensional GP data for both training\nand testing. In the Zero-shot scenario, we demonstrated the ability of DANP to generalize to different\ndimensional input GP data without direct training on that data. Lastly, in the Fine-tuning scenario, we\nconducted experiments where we fine-tuned on unseen dimensional GP data, using limited training\ndata points, utilizing pre-trained DANP alongside other baseline models.\nFrom-scratch\nTo validate the capability of DANP to effectively learn generally shared features and\ncapture functional uncertainty across tasks, we first compare DANP against other baseline models in the\n6\n\n\nPublished as a conference paper at ICLR 2025\n2\n1\n0\n1\n0.5\n0.0\n0.5\n1.0\n1.5\nZero-shot\nground truth\ncontext points\nFull Img\nContext\nMean\nStd\nFigure 3: Posterior samples of DANP in (Left) the Zero-shot scenario with a 1-dimensional GP\ndataset and (Right) the Image completion task using the EMNIST and CelebA datasets. (Left) Black\nstars represent the context points and the dashed line indicates the ground truth for the target points.\nEach color represents different posterior samples generated from the latent path. (Right) Displays the\nfull image, context points, predictive mean, and standard deviation of DANP for both the EMNIST\nand CelebA datasets. Outputs for both images are produced by a single model.\nTable 1: Results of the context and target log-likelihood for the GP regression task in the From-scratch\nscenario. nD A in the first row denotes the n-dimensional GP dataset using the A kernel.\nModel\n1D RBF\n1D Matern\n2D RBF\n2D Matern\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\nCANP\n1.377 ±0.000\n0.839 ±0.002\n1.377 ±0.000\n0.663 ±0.007\n1.377 ±0.001\n0.165 ±0.015\n1.373 ±0.001\n-0.066 ±0.007\nANP\n1.377 ±0.000\n0.855 ±0.004\n1.377 ±0.000\n0.681 ±0.003\n1.378 ±0.000\n0.170 ±0.014\n1.346 ±0.005\n-0.107 ±0.006\nBANP\n1.377 ±0.000\n0.864 ±0.001\n1.377 ±0.000\n0.689 ±0.004\n1.378 ±0.000\n0.228 ±0.004\n1.378 ±0.000\n-0.033 ±0.013\nMPANP\n1.376 ±0.000\n0.856 ±0.006\n1.376 ±0.000\n0.679 ±0.005\n1.378 ±0.001\n0.242 ±0.001\n1.376 ±0.002\n-0.029 ±0.007\nTNP\n1.381 ±0.000\n0.904 ±0.003\n1.381 ±0.000\n0.710 ±0.001\n1.383 ±0.000\n0.362 ±0.001\n1.383 ±0.000\n0.060 ±0.002\nDANP (ours)\n1.381 ±0.000\n0.921 ±0.003\n1.382 ±0.000\n0.723 ±0.003\n1.383 ±0.000\n0.373 ±0.001\n1.383 ±0.000\n0.068 ±0.001\nFrom-scratch scenario in diverse fixed dimensional GP regression tasks. In this experiment, the meta-\ntraining datasets are produced using GP under four distinct configurations: either one-dimensional\nor two-dimensional input, utilizing either the RBF or Matern kernels. The results presented in\nTable 1 demonstrate that DANP consistently surpasses other baseline models across various settings,\nparticularly excelling on the target dataset in terms of log-likelihood. These results prove that DANP\neffectively grasps common features and captures functional uncertainty, outperforming other baseline\nmodels even with various settings in fixed-dimensional GP regression tasks.\nZero-shot\nIn the Zero-shot scenario, we train a single NP model using various dimensional GP\ndatasets and then evaluate the performance of the model on a range of GP datasets with different\ndimensions. Specifically, we consider two different cases: one where the training datasets include 2\nand 4-dimensional GP datasets, and another where the training datasets include 2, 3, and 4-dimensional\nGP datasets. After training, we assess our pre-trained model on GP datasets ranging from 1 to 5 and\n7 dimensions. We validate results for our model as DANP, in distinction from other baselines, is\ncapable of simultaneously training on and inferring from datasets with diverse dimensions. Table 2a\ndemonstrates that DANP successfully learns the shared features across different dimensional GP\ndatasets and generalizes effectively to test datasets with the same dimensions as the training datasets.\nRemarkably, DANP also generalizes well to test datasets with previously unseen dimensions. For\ninstance, the zero-shot log-likelihood for the 1-dimensional GP dataset, when DANP is trained on 2, 3,\nand 4-dimensional datasets, is nearly comparable to the log-likelihood of CANP with From-scratch\ntraining in Table 1. These findings suggest that DANP efficiently captures and learns general features\nacross various tasks, allowing it to explain tasks with unseen dimensions without additional training.\nFor further results using 2 and 3-dimensional GP datasets or different kernels during training, see\nAppendix D. The trends are consistent, showing that DANP generalizes well across various tasks.\nRefer to Fig. 3 to see the zero-shot posterior samples for the 1-dimensional GP regression task. And\nalso refer to Appendix D.2.3 to see the results on the additional zero-shot scenarios, especially\nextrapolation scenarios.\nFine-tuning\nIn the fine-tuning scenario, we fine-tuned pre-trained NP models on a limited set of\n160 1-dimensional GP regression tasks. For the baselines, we used pre-trained models that were\ntrained on 2-dimensional tasks as described in the From-scratch experiments. For DANP, we used\nmodels pre-trained on 2, 3, and 4-dimensional tasks as mentioned in the Zero-shot experiments. In\nTable 2b, ‘Full fine-tuning’ refers to the process where all pre-trained neural network parameters\nare adjusted during fine-tuning, while ‘Freeze fine-tuning’ means that the shared parameters in the\n7\n\n\nPublished as a conference paper at ICLR 2025\nTable 2: Log-likelihood results for the GP regression task in (a) the Zero-shot and (b) the Fine-\ntuning scenarios using RBF kernel. For (a), nD in the first column denotes the outcomes for the\nn-dimensional GP dataset. The colored cell\nindicates the data dimension used to pre-train DANP.\n(a) Zero-shot scenario\nDimension\nDANP trained on 2D & 4D\nDANP trained on 2D & 3D & 4D\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.336 ±0.047\n0.806 ±0.048\n1.366 ±0.004\n0.826 ±0.018\n2D RBF\n1.383 ±0.000\n0.340 ±0.007\n1.383 ±0.000\n0.335 ±0.014\n3D RBF\n1.377 ±0.007\n-0.360 ±0.063\n1.383 ±0.000\n-0.261 ±0.025\n4D RBF\n1.379 ±0.007\n-0.589 ±0.056\n1.383 ±0.000\n-0.568 ±0.042\n5D RBF\n1.357 ±0.012\n-0.689 ±0.004\n1.359 ±0.032\n-0.676 ±0.004\n7D RBF\n1.348 ±0.016\n-0.726 ±0.026\n1.355 ±0.022\n-0.723 ±0.022\n(b) Fine-tuning scenario\nMethod\nFull fine-tuning\nFreeze fine-tuning\ncontext\ntarget\ncontext\ntarget\nCANP\n-0.305 ±0.043\n-0.495 ±0.048\n-0.061 ±0.236\n-0.386 ±0.132\nANP\n-0.273 ±0.121\n-0.365 ±0.093\n-0.311 ±0.034\n-0.369 ±0.037\nBANP\n-0.292 ±0.044\n-0.379 ±0.022\n-0.131 ±0.199\n-0.193 ±0.281\nMPANP\n-0.254 ±0.339\n-0.414 ±0.235\n-0.481 ±0.032\n-0.563 ±0.026\nTNP\n-0.042 ±0.016\n-0.448 ±0.228\n0.357 ±0.372\n-0.087 ±0.295\nDANP(ours)\n1.376 ±0.000\n0.893 ±0.004\n1.376 ±0.001\n0.890 ±0.005\nTable 3: Log-likelihood results for context and target values were obtained for (a) image completion\ntasks using the EMNIST and CelebA datasets, and (b) fine-tuning on video completion tasks. For (a),\nDANP was trained concurrently on both the EMNIST and CelebA datasets. For (b), † indicates the\nzero-shot performance of DANP.\n(a) Image completion\nModel\nEMNIST\nCelebA\ncontext\ntarget\ncontext\ntarget\nCANP\n1.378 ±0.001\n0.837 ±0.003\n4.129 ±0.004\n1.495 ±0.004\nANP\n1.372 ±0.005\n0.863 ±0.011\n4.131 ±0.003\n1.993 ±0.016\nBANP\n1.373 ±0.004\n0.901 ±0.004\n4.127 ±0.005\n2.292 ±0.021\nMAPNP\n1.365 ±0.008\n0.787 ±0.057\n4.127 ±0.004\n1.505 ±0.011\nTNP\n1.378 ±0.001\n0.945 ±0.004\n4.140 ±0.005\n1.632 ±0.005\nDANP (ours)\n1.382 ±0.001\n0.969 ±0.002\n4.149 ±0.000\n2.027 ±0.006\n(b) Fine-tuning on CelebA video data\nModel\ncontext\ntarget\nCANP\n-1.013 ±0.116\n-1.053 ±0.076\nANP\n-0.498 ±0.143\n-0.517 ±0.128\nBANP\n-0.037 ±0.334\n-0.099 ±0.273\nMAPNP\n-1.341 ±0.132\n-1.336 ±0.136\nTNP\n-1.574 ±0.471\n-2.747 ±0.501\nDANP† (ours)\n4.086 ±0.036\n0.503 ±0.063\nDANP (ours)\n4.094 ±0.041\n0.560 ±0.086\nencoder layers remain unchanged during the fine-tuning process. The results in Table 2b clearly show\nthat all the NP models, except for DANP, fail to achieve high generalization performance. Furthermore,\nthe performance of DANP shows a clear improvement over the zero-shot log-likelihood result in\nTable 2a following the fine-tuning with the limited data. This indicates that the features from the\npre-trained baselines are not effectively applied to unseen dimensional downstream datasets with a\nlimited amount of data. In contrast, DANP is able to generalize well on these unseen dimensional\ndownstream datasets with only a small amount of downstream data. Refer to Appendix D.2.4 to see\nthe results on additional fine-tuning scenarios.\n5.2\nIMAGE COMPLETION AND VIDEO COMPLETION\nImage Completion\nTo validate our model’s capability to meta-train implicit stochastic processes\nfor varying output dimensions, we perform image completion tasks on two different datasets: EM-\nNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015). In these tasks, we randomly select some\npixels as context points and use NP models to predict the selected target pixels. Here, we use the\n2-dimensional position value as input and the channel value as output. Previous NP models were\nunable to formulate the predictive distribution for varying output dimensions, failing to learn image\ncompletion tasks with different numbers of channels. However, our DANP model can handle varying\noutput dimensions, allowing it to simultaneously learn various image completion tasks with different\nnumbers of channels. The experimental results for EMNIST and CelebA reported in Table 3 were\nmeasured using models trained separately for each dataset for the baselines, whereas ours were ob-\ntained using a single model trained simultaneously for both datasets. Table 3 demonstrates that DANP\nsuccessfully learns both image completion tasks, validating its ability to formulate the predictive\ndensity for outputs with varying dimensions. Refer to Fig. 3 and Appendix D for the visualizations of\npredicted mean and standard deviation of completed images.\nFine-tuning on Video Completion\nTo further validate the capability of utilizing pre-trained features\nin DANP for tasks with unseen dimensions, we created a simple video dataset based on the CelebA\ndataset. Specifically, we used the original CelebA data as the first frame at time t = 0 and gradually\ndecreased the brightness by subtracting 5 from each channel for each time t ∈[9]. This process\nresulted in an input dimension of 3, combining the time axis with position values. We fine-tuned pre-\ntrained NP models on only 5 video data, simulating a scenario with limited data for the downstream\ntask. For the baselines, we used models pre-trained on the CelebA dataset. For DANP, we used\n8\n\n\nPublished as a conference paper at ICLR 2025\n0\n50\n100\n150\n200\n# BO steps\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nNormalized Regret\n6-dim BO (Inter.)\n0\n50\n100\n150\n200\n# BO steps\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n9-dim BO (Extra.)\n0\n50\n100\n150\n200\n# BO steps\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\n10-dim BO (Extra.)\n0\n50\n100\n150\n200\n# BO steps\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n16-dim BO (Extra.)\nANP\nCANP\nBANP\nMPANP\nTNP\nDANP\nFigure 4: Results for Bayesian Optimization (BO) on 6-, 9-, 10-, and 16-dimensional hyperparameter\ntuning tasks in the HPO-B benchmark. Note that DANP is pre-trained on 2-, 3-, 8-dimensional tasks.\nTable 4: Ablation results for 1D, 2D GP regression and image completion tasks. In this table, we\nreport the log-likelihood results for only the target dataset, excluding the context dataset.\nModel\n1D RBF\n1D Matern\n2D RBF\n2D Matern\nEMNIST\nCelebA\nTNP\n0.904 ± 0.003\n0.710 ± 0.001\n0.362 ± 0.001\n0.060 ± 0.002\n0.945 ± 0.004\n1.632 ± 0.005\n+ DAB\n0.907 ± 0.001\n0.713 ± 0.001\n0.365 ± 0.001\n0.061 ± 0.000\n0.949 ± 0.004\n1.645 ± 0.014\n+ Latent\n0.923 ± 0.003\n0.722 ± 0.001\n0.371 ± 0.001\n0.064 ± 0.001\n0.967 ± 0.010\n1.973 ± 0.023\nDANP\n0.921 ± 0.003\n0.723 ± 0.003\n0.373 ± 0.001\n0.068 ± 0.001\n0.969 ± 0.002\n2.027 ± 0.006\n- Pos\n0.922 ± 0.002\n0.724 ± 0.001\n-0.395 ± 0.022\n-0.446 ± 0.006\n0.376 ± 0.012\n0.631 ± 0.030\n+ PMA\n0.921 ± 0.001\n0.721 ± 0.001\n0.372 ± 0.004\n0.067 ± 0.002\n0.975 ± 0.007\n2.025 ± 0.007\nmodels pre-trained on both the EMNIST and CelebA datasets. Table 3b demonstrates that, while\nother baseline methods fail to generalize to the increased input dimensional dataset, our method\nsuccessfully learns and generalizes well with a scarce amount of training data. Refer to Appendix D\nfor the example of video data and the predicted mean and standard deviation.\n5.3\nBAYESIAN OPTIMIZATION FOR HYPERPARAMETER TUNING\nTo illustrate the real-world applicability of DANP, we conducted BO (Brochu et al., 2010) experiments\non 6, 9, 10, and 16-dimensional hyperparameter tuning tasks in HPO-B (Pineda-Arango et al., 2021)\nbenchmark. HPO-B is a large-scale black-box hyperparameter optimization benchmark, which is\nassembled and preprocessed from the OpenML repository with 8 different hyperparameter dimensions\n(2, 3, 6, 8, 9, 10, 16, and 18-dim) and evaluated sparsely on 196 datasets with a total of 6.4 million\nhyperparameter evaluations. We target 6, 9, 10, and 16-dimensional hyperparameter tuning tasks.\nTo do so, we pre-trained baselines on the 2-dimensional tasks in the meta-train split of the HPO-B\nbenchmark and then fine-tune the baselines on a limited set of 4 meta-batches sampled from each\ntarget task. In contrast, for DANP, we follow the zero-shot setting, where we use a single model\npre-trained on 2, 3, and 8-dimensional tasks in the meta-train split without fine-tuning on target tasks.\nPlease see more detailed setups in Appendix C.7. We use Expected Improvement (Jones et al., 1998)\nas an acquisition function for all experiments and measured performance using normalized regret,\nymax−y\nymax−ymin , where ymax and ymin denotes the global best and worst value, respectively. We run 200\niterations for all the BO experiments and report the average and standard deviation of normalized\nregrets over 10 different random seeds. The results in Fig. 4 demonstrate that DANP outperforms other\nbaselines in terms of regret with the same iteration numbers. This demonstrates that DANP is capable\nof serving as a surrogate model for different BO-based hyperparameter tuning tasks using only a single\nmodel without additional training on new BO tasks, and it also effectively learns generalized shared\nfeatures across a wide range of tasks. Surprisingly, the gap between DANP and baselines is even\nlarger for the dimension extrapolation settings (9, 10, 16-dim), which empirically validates that the\nDANP is capable of handling unseen, varying-dimensional data, including cases where extrapolation\nis required. Refer to Appendix D.10 to see the results on synthetic BO tasks. The trends are similar.\n5.4\nABLATION STUDY\nTo analyze the roles of each module in DANP through ablation experiments, we conducted a series\nof ablation experiments on various modules. The ablation experiments were categorized into three\nmain parts: 1) the roles of the DAB module and Latent path, 2) the role of positional encoding in\nthe DAB module, and 3) experiments replacing mean pooling with attention-based averaging in the\n9\n\n\nPublished as a conference paper at ICLR 2025\nDAB module. The experiments were conducted on 1D GP regression, 2D GP regression, and image\ncompletion tasks. In Table 4, we analyze the log-likelihood results only for the target, excluding the\ncontext. For the full results, including the context, please refer to Appendix D.3. It can be observed\nthat the context exhibits similar trends to the target.\nThe Role of the DAB Module and Latent Path\nAs mentioned in Section 3.2 and Section 3.3, the\nDAB module is used as a module for handling varying dimensional input, and the latent path is added\nto capture more accurate model uncertainty, thereby improving model performance and increasing\nthe capacity to learn generally shared features among tasks. Table 4 show that the performance trends\nalign well with the direction we described and intended in the Section 3. In Table 4, the DAB and\nLatent rows show the performance when DAB and Latent paths are added to TNP, respectively. We\ncan observe that in all data and experiments, adding only the DAB to TNP results in a performance\nclose to TNP, while adding the latent path results in a performance closer to DANP. This demonstrates\nthat adding only the DAB module allows for the handling of varying dimensional data, but there\nare limitations in improving model performance. However, adding the latent path improves model\nperformance but still has the issue of not being able to handle varying dimensional data.\nThe Role of Positional Encoding in the DAB Module\nWhen treating the diverse dimensional\ntasks, permuting the orders of the features should not affect the result, but note that the permutation\nshould apply simultaneously for all inputs. For instance, for a model having three features, say we\npermute the features to (3,1,2) for the first input and (1,3,2) for the second input. Then there is no\nway for the model to distinguish different feature values. Removing positional embeddings from the\nDAB is effectively doing this; since we treat all the features as elements in a set, it allows different\npermutations applied for different inputs, so the model does not distinguish different features.\nWe’ve tested the necessity of positional encoding through additional experiments, which confirmed\nits importance. In Table 4, “Pos” indicates the case when we extract the positional encoding from the\nDAB module. For the 1D GP regression tasks, because there is only one dimension for the input x,\nthe existence of positional encoding does not affect the final performance. However, as seen in 2D\nregression tasks and image completion tasks, the existence of positional encoding is crucial for the\nfinal performance. Refer to Appendices D.5 to D.7 to see additional ablation results using Rotary\nPosition Embedding (RoPE; Touvron et al., 2023).\nAttention-based averaging\nTo verify if the mean pooling in the DAB module can be enhanced by\nusing attention-based averaging, we employed the Pooling by Multihead Attention (PMA; Lee et al.,\n2019) module. This module uses a learnable token that is updated through cross-attention layers\nby pooling the input tokens using attention. In Table 4, the PMA row shows the results when mean\npooling in the DAB is replaced with the PMA module. The results consistently indicate that mean\npooling and attention-based averaging yield similar performance across nearly all tasks. Refer to\nAppendix D to see extensive additional empirical analysis and additional experiments.\n6\nCONCLUSION\nIn this paper, we present a novel NP variant that addresses the limitations of previous NP variants\nby incorporating a DAB block and a Transformer-based latent path. Our approach offers two key\nadvantages: 1) the ability to directly handle diverse input and output dimensions, and 2) the capacity\nfor learned features to be fine-tuned on new tasks with varying input and output dimensions. We\nempirically validate DANP across various tasks and scenarios, consistently demonstrating superior\nperformance compared to the baselines. Conducting various experiments only with a single model\ncan be a starting point for the utilization of NP models as general regressors for a wide range of\nregression tasks.\nLimitation and Future work\nIn this study, DANP concentrated on the regression task, but it can\nnaturally be extended to other tasks, such as classification. A promising direction for future work\nwould be to pre-train the encoder, which includes the DAB module to handle diverse dimensional data,\nusing various datasets from different tasks and then fine-tuning with a small amount of downstream\ndata for various tasks using appropriate decoders.\n10\n\n\nPublished as a conference paper at ICLR 2025\nReproducibility Statement.\nWe provide the architecture of our proposed model along with the\narchitectures of other baseline models in Appendix C.1. Additionally, the hyperparameters used in the\nexperiments, metrics, and detailed information about the data utilized in each experiment are described\nin Appendix C. Furthermore, additional experimental results, including ablation experiments and\nadditional visualizations, are presented in Appendix D.\nEthics Statement.\nOur research introduces new NP variants that calculate predictive density for\ncontext and target points in tasks with varying inputs. It is unlikely that our work will have any\npositive or negative societal impacts. Also, we utilize openly accessible standard evaluation metrics\nand datasets. Furthermore, we refrain from publishing any novel datasets or models that may pose a\npotential risk of misuse.\nAcknowledgements\nThis work was partly supported by Institute of Information & communi-\ncations Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT)\n(No.RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST)), Institute of In-\nformation & communications Technology Planning & Evaluation(IITP) grant funded by the Korea\ngovernment(MSIT) (No.RS-2024-00509279, Global AI Frontier Lab), and Institute of Information\n& communications Technology Planning & Evaluation(IITP) grant funded by the Korea govern-\nment(MSIT) (No.RS-2022-II220713, Meta-learning Applicable to Real-world Problems).\nREFERENCES\nCesar Almecija, Apoorva Sharma, and Navid Azizan. Uncertainty-aware meta-learning for multi-\nmodal task distributions. arXiv preprint arXiv:2210.01881, 2022.\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky,\nBin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will\nConstable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael\nGschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos,\nMario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian\nPuhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil\nTillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren Zhou,\nRichard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala. PyTorch 2: Faster\nMachine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.\nIn 29th ACM International Conference on Architectural Support for Programming Languages and\nOperating Systems, Volume 2 (ASPLOS ’24). ACM, April 2024. doi: 10.1145/3620665.3640366.\nURL https://pytorch.org/assets/pytorch2-2.pdf.\nMatthew Ashman, Cristiana Diaconu, Junhyuck Kim, Lakee Sivaraya, Stratis Markou, James Re-\nqueima, Wessel P Bruinsma, and Richard E Turner. Translation equivariant transformer neural\nprocesses. arXiv preprint arXiv:2406.12409, 2024.\nThomas Back. Evolutionary algorithms in theory and practice: evolution strategies, evolutionary\nprogramming, genetic algorithms. Oxford university press, 1996.\nMaximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-\ndrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo\nBayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. URL\nhttp://arxiv.org/abs/1910.06403.\nEric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive\ncost functions, with application to active user modeling and hierarchical reinforcement learning.\narXiv preprint arXiv:1012.2599, 2010.\nWessel P Bruinsma, Stratis Markou, James Requiema, Andrew YK Foong, Tom R Andersson, Anna\nVaughan, Anthony Buonomo, J Scott Hosking, and Richard E Turner. Autoregressive conditional\nneural processes. arXiv preprint arXiv:2303.14468, 2023.\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist\nto handwritten letters. In 2017 international joint conference on neural networks (IJCNN), pp.\n2921–2926. IEEE, 2017.\n11\n\n\nPublished as a conference paper at ICLR 2025\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nVincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion processes.\nIn International Conference on Machine Learning, pp. 8990–9012. PMLR, 2023.\nBradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pp.\n569–593. Springer, 1992.\nLeo Feng, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed Osama Ahmed. Latent bottle-\nnecked attentive neural processes. arXiv preprint arXiv:2211.08458, 2022a.\nLeo Feng, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed Osama Ahmed. Efficient queries\ntransformer neural processes. In Sixth Workshop on Meta-Learning at the Conference on Neural\nInformation Processing Systems, 2022b.\nLeo Feng, Frederick Tung, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed Osama\nAhmed. Memory efficient neural processes via constant memory attention block. arXiv preprint\narXiv:2305.14567, 2023.\nEdwin Fong, Chris Holmes, and Stephen G Walker. Martingale posterior distributions. arXiv preprint\narXiv:2103.15671, 2021.\nA. Y. K. Foong, W. P. Bruinsma, J. Gordon, Y. Dubois, J. Requeima, and R. E. Turner. Meta-learning\nstationary stochastic process prediction with convolutional neural processes. In Advances in Neural\nInformation Processing Systems 33 (NeurIPS 2020), 2020.\nJacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson.\nGpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances\nin Neural Information Processing Systems, 2018.\nM. Garnelo, D. Rosenbaum, C. J. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh,\nD. J. Rezende, and S. M. A. Eslami. Conditional neural processes. In Proceedings of The 35th\nInternational Conference on Machine Learning (ICML 2018), 2018a.\nM. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. M. A. Eslami, and Y. W.\nTeh. Neural processes. ICML Workshop on Theoretical Foundations and Applications of Deep\nGenerative Models, 2018b.\nJ. Gordon, W. P. Bruinsma, A. Y. K. Foong, J. Requeima, Y. Dubois, and R. E. Turner. Convolutional\nconditional neural processes. In International Conference on Learning Representations (ICLR),\n2020.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a\nfreely accessible critical care database. Scientific data, 3(1):1–9, 2016.\nDonald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive\nblack-box functions. Journal of Global optimization, 13:455–492, 1998.\nH. Kim, A. Mnih, J. Schwarz, M. Garnelo, S. M. A. Eslami, D. Rosenbaum, and V. Oriol. Attentive\nneural processes. In International Conference on Learning Representations (ICLR), 2018.\nJungtaek Kim and Seungjin Choi. BayesO: A Bayesian optimization framework in Python. Journal\nof Open Source Software, 8(90):5320, 2023.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations (ICLR), 2015.\nJannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. Self-\nattention between datapoints: Going beyond individual input-output pairs in deep learning. Ad-\nvances in Neural Information Processing Systems, 34:28742–28756, 2021.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.\n12\n\n\nPublished as a conference paper at ICLR 2025\nTuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosenbaum, Jonathan Schwarz, and Yee Whye\nTeh. Empirical evaluation of neural process objectives. In NeurIPS workshop on Bayesian Deep\nLearning, pp. 71, 2018.\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne\nHubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.\nNeural computation, 1(4):541–551, 1989.\nHyungi Lee, Eunggu Yun, Hongseok Yang, and Juho Lee. Scale mixtures of neural network gaussian\nprocesses. In International Conference on Learning Representations (ICLR), 2021.\nHyungi Lee, Eunggu Yun, Giung Nam, Edwin Fong, and Juho Lee. Martingale posterior neural\nprocesses. In International Conference on Learning Representations (ICLR), 2023.\nJ. Lee, Y. Lee, J. Kim, E. Yang, S. J. Hwang, and Y. W. Teh. Bootstrapping neural processes. In\nAdvances in Neural Information Processing Systems 33 (NeurIPS 2020), 2020.\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set trans-\nformer: A framework for attention-based permutation-invariant neural networks. In Proceedings\nof The 36th International Conference on Machine Learning (ICML 2019), 2019.\nSulin Liu, Xingyuan Sun, Peter J Ramadge, and Ryan P Adams. Task-agnostic amortized inference\nof gaussian process hyperparameters. Advances in Neural Information Processing Systems, 33:\n21440–21452, 2020.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), December 2015.\nC. Louizos, X. Shi, K. Schutte, and M. Welling. The functional neural process. In Advances in\nNeural Information Processing Systems 32 (NeurIPS 2019), 2019.\nStratis Markou, James Requeima, Wessel P Bruinsma, Anna Vaughan, and Richard E Turner. Practical\nconditional neural processes via tractable dependent predictions. arXiv preprint arXiv:2203.08775,\n2022.\nTung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via\nsequence modeling. In Proceedings of The 38th International Conference on Machine Learning\n(ICML 2022), 2022.\nSebastian Pineda-Arango, Hadi S. Jomaa, Martin Wistuba, and Josif Grabocka. HPO-B: A large-scale\nreproducible benchmark for black-box HPO based on openml. Neural Information Processing\nSystems (NeurIPS) Track on Datasets and Benchmarks, 2021.\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\nLeonard Andreeviˇc Rastrigin. Systems of extremal control. Nauka, 1974.\nXin Rong. word2vec parameter learning explained. arXiv preprint arXiv:1411.2738, 2014.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems 30 (NIPS 2017), 2017.\nQi Wang and Herke Van Hoof. Learning expressive meta-representations with mixture of expert\nneural processes. Advances in neural information processing systems, 35:26242–26255, 2022.\n13\n\n\nPublished as a conference paper at ICLR 2025\nQi Wang, Marco Federici, and Herke van Hoof. Bridge the inference gaps of neural processes via\nexpectation maximization. In The Eleventh International Conference on Learning Representations,\n2023.\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and\nAlexander J Smola. Deep sets. In Advances in Neural Information Processing Systems 30 (NIPS\n2017), 2017.\n14\n\n\nPublished as a conference paper at ICLR 2025\nA\nADDITIONAL DISCUSSION FOR THE FUTURE WORK\nAs demonstrated in our experiments with the MIMIC-III dataset in Appendix D.12, the ultimate goal\nin the Neural Processes field should be developing a general foundation regressor model capable of\nhandling a wide range of data structures and realistic scenarios, such as diverse time series data and\ncases with missing features. We view the DANP research as the initial step toward achieving this\nambitious objective.\nA key focus of this future work direction will be to extend the model’s ability to appropriately process\ninputs with varying dimensions, numbers of context and target points, and diverse data structures\n(for example there can be lots of different tasks with the same dimensional inputs such as EMNIST\nimage completion and 2d GP regression task). Developing a model that can flexibly adapt to such\nvariability without specific data processing based on inductive bias while providing accurate and\nreliable inferences across these scenarios remains a critical challenge and an exciting direction for\nfurther exploration.\nB\nADDITIONAL RELATED WORKS\nThe first Neural Process (NP) model, the Conditional Neural Process (CNP) (Garnelo et al., 2018a),\nutilized straightforward MLP layers for both the encoder and decoder. Neural Process (NP) (Garnelo\net al., 2018b) extended this by incorporating a global latent variable to capture model uncertainty.\nEnhancements followed with Attentive Neural Processes (ANP) (Kim et al., 2018) and Conditional\nAttentive Neural Processes (CANP), which introduced attention mechanisms in the encoder for better\ncontext summarization. Transformer Neural Processes (TNP) (Nguyen & Grover, 2022) replaced\nMLPs with masked transformer layers, achieving state-of-the-art performance.\nFurther advancements include Functional Neural Processes (Louizos et al., 2019), which employed\nlocal latent variables to improve uncertainty capture, and Bootstrapping Attentive Neural Processes\n(BANP) (Lee et al., 2020), which utilized a residual bootstrap approach to address model misspec-\nification. Martingale Posterior Attentive Neural Processes (MPANP) (Lee et al., 2023) addressed\nuncertainty with the martingale posterior, offering a Bayesian alternative.\nRecent developments have expanded NPs’ scalability and expressiveness. For example, Translation\nEquivariant Transformer Neural Processes (TE-TNP) (Ashman et al., 2024) enhance spatio-temporal\nmodeling with translation equivariance, which leverages symmetries in posterior predictive maps\ncommon in stationary data. Latent Bottlenecked Attentive Neural Processes (LBANP) (Feng et al.,\n2022a) and Mixture of Expert Neural Processes (MoE-NPs) (Wang & Van Hoof, 2022) improve latent\nvariable modeling through bottlenecks and dynamic mixtures, improving computational efficiency\nand generalization across various meta-learning tasks. Autoregressive Conditional Neural Processes\n(AR CNPs) (Bruinsma et al., 2023) address temporal dependencies by autoregressively defining a joint\npredictive distribution, while Self-normalized Importance weighted Neural Process (SI-NP) (Wang\net al., 2023) refine inference through iterative optimization.\nOther contributions include Constant Memory Attentive Neural Processes (CMANPs) (Feng et al.,\n2023), which reduce memory usage with constant memory attention blocks, and Gaussian Neural\nProcesses (GNPs) (Markou et al., 2022), focusing on tractable dependent predictions by modeling the\ncovariance matrix of a Gaussian predictive process. Efficient Queries Transformer Neural Processes\n(EQTNPs) (Feng et al., 2022b) improve TNPs by applying self-attention only to the context points,\nretrieving information for target points through cross-attention. Together, these advancements address\nkey limitations in uncertainty modeling, inference, and computational efficiency, forming the basis\nfor further progress in NP research.\nC\nEXPERIMENTAL DETAILS\nTo ensure reproducibility, we have included our experiment code in the supplementary material.\nOur code builds upon the official implementation1 of TNP (Nguyen & Grover, 2022). We utilize\nPyTorch (Ansel et al., 2024) for all experiments, and BayesO (Kim & Choi, 2023), BoTorch (Balandat\n1https://github.com/tung-nd/TNP-pytorch.git\n15\n\n\nPublished as a conference paper at ICLR 2025\net al., 2020), and GPyTorch (Gardner et al., 2018) packages for Bayesian Optimization experiments.\nAll experiments were conducted on either a single NVIDIA GeForce RTX 3090 GPU or an NVIDIA\nRTX A6000 GPU. For optimization, we used the Adam optimizer (Kingma & Ba, 2015) with cosine\nlearning rate scheduler. Unless otherwise specified, we selected the base learning rate, weight decay,\nand batch size from the following grids: {5×10−5, 7×10−5, 9×10−5, 1×10−4, 3×10−4, 5×10−4}\nfor learning rate, {0, 1 × 10−5} for weight decay, and {16, 32} for batch size, based on validation\ntask log-likelihood.\nC.1\nDETAILS OF MODEL STRUCTURES\nTable 5: Model structure details of CANP\nCATEGORY\nDETAILS\nMODEL SPECIFICATIONS\nDETERMINISTIC PATH HIDDEN DIMENSION\n128\nMLP DEPTH FOR VALUE IN CROSS ATTENTION LAYER\n4\nMLP DEPTH FOR KEY AND QUERY IN CROSS ATTENTION LAYER\n2\nMLP DEPTH FOR SELF-ATTENTION INPUT LAYER\n4\nMLP DEPTH FOR SELF-ATTENTION OUTPUT LAYER\n2\nDECODER DEPTH\n3\nNUMBER OF PARAMETERS FOR 1D GP REGRESSION\n331906\nTable 6: Model structure details of ANP\nCATEGORY\nDETAILS\nMODEL SPECIFICATIONS\nDETERMINISTIC PATH HIDDEN DIMENSION\n128\nLATENT PATH HIDDEN DIMENSION\n128\nMLP DEPTH FOR VALUE IN CROSS ATTENTION LAYER\n4\nMLP DEPTH FOR KEY AND QUERY IN CROSS ATTENTION LAYER\n2\nMLP DEPTH FOR SELF-ATTENTION INPUT LAYER\n4\nMLP DEPTH FOR SELF-ATTENTION OUTPUT LAYER\n2\nDECODER DEPTH\n3\nNUMBER OF PARAMETERS FOR 1D GP REGRESSION\n348418\nIn this section, we summarize the structural details of CANP, ANP, BANP, MPANP, TNP, and DANP.\nIt is important to note that while we report the number of parameters for the baseline models in\na 1-dimensional GP regression scenario, their parameter counts increase as the input and output\ndimensions increase. In contrast, the number of parameters in our model remains constant regardless\nof the input and output dimension combinations. This proves that our model is structurally efficient\ncompared to other baseline models. Also, following Lee et al. (2023), we model MPANP without the\nSelf-Attention layer in the deterministic path.\n16\n\n\nPublished as a conference paper at ICLR 2025\nTable 7: Model structure details of BANP\nCATEGORY\nDETAILS\nMODEL SPECIFICATIONS\nDETERMINISTIC PATH HIDDEN DIMENSION\n128\nMLP DEPTH FOR VALUE IN CROSS ATTENTION LAYER\n4\nMLP DEPTH FOR KEY AND QUERY IN CROSS ATTENTION LAYER\n2\nMLP DEPTH FOR SELF-ATTENTION INPUT LAYER\n4\nMLP DEPTH FOR SELF-ATTENTION OUTPUT LAYER\n2\nDECODER DEPTH\n3\nNUMBER OF PARAMETERS FOR 1D GP REGRESSION\n364674\nTable 8: Model structure details of MPANP\nCATEGORY\nDETAILS\nMODEL SPECIFICATIONS\nDETERMINISTIC PATH HIDDEN DIMENSION\n128\nHIDDEN DIMENSION FOR EXCHANGEABLE GENERATIVE MODEL\n128\nDEPTH FOR EXCHANGEABLE GENERATIVE MODEL\n1\nMLP DEPTH FOR VALUE IN CROSS ATTENTION LAYER\n4\nMLP DEPTH FOR KEY AND QUERY IN CROSS ATTENTION LAYER\n2\nMLP DEPTH FOR SELF-ATTENTION INPUT LAYER\n4\nMLP DEPTH FOR SELF-ATTENTION OUTPUT LAYER\n2\nDECODER DEPTH\n3\nNUMBER OF PARAMETERS FOR 1D GP REGRESSION\n892418\nC.2\nEVALUATION METRIC FOR THE TASKS\nFollowing Le et al. (2018), we used the normalized predictive log-likelihood:\n1\n|o|\nX\nk∈o\nlog p(yj,k|xj,k, Dj,c)\n(20)\nfor the CNP variants CANP and TNP, where o ∈{cj, tj} denotes context or target points. For the other\nmodels, we approximated the normalized predictive log-likelihood as follows:\n1\n|o|\nX\nk∈o\nlog p(yj,k|xj,k, Dj,c) ≈1\n|o|\nX\nk∈o\nlog 1\nK\nK\nX\nk=1\np(yj,k|xj,k, θ(k)\nj\n),\n(21)\nwhere θ(k)\nj\nare independent samples drawn from q(θj|Dj,c) for k ∈[K]. Again, o ∈{cj, tj} indicates\ncontext or target points.\n17\n\n\nPublished as a conference paper at ICLR 2025\nTable 9: Model structure details of TNP\nCATEGORY\nDETAILS\nMODEL SPECIFICATIONS\nHIDDEN DIMENSION FOR EMBEDDING LAYERS\n64\nNUMBER OF LAYERS FOR EMBEDDING LAYERS\n4\nHIDDEN DIMENSION FOR MASKED TRANSFORMER LAYERS\n128\nNUMBER OF LAYERS FOR MASKED TRANSFORMER LAYERS\n6\nNUMBER OF HEADS FOR MASKED TRANSFORMER LAYERS\n4\nDECODER DEPTH\n2\nNUMBER OF PARAMETERS FOR 1D GP REGRESSION\n222082\nTable 10: Model structure details of DANP\nCATEGORY\nDETAILS\nMODEL SPECIFICATIONS\nHIDDEN DIMENSION FOR LINEAR PROJECTION IN DAB\n32\nHIDDEN DIMENSION FOR SELF-ATTENTION IN DAB\n32\nHIDDEN DIMENSION FOR TRANSFORMER LAYERS IN LATENT PATH\n64\nNUMBER OF LAYERS FOR TRANSFORMER LAYERS IN LATENT PATH\n2\nHIDDEN DIMENSION FOR SELF-ATTENTION IN LATENT PATH\n64\nHIDDEN DIMENSION FOR MLP LAYERS IN LATENT PATH\n128\nNUMBER OF LAYERS FOR MLP LAYERS IN LATENT PATH\n2\nHIDDEN DIMENSION FOR MASKED TRANSFORMER LAYERS\n128\nNUMBER OF LAYERS FOR MASKED TRANSFORMER LAYERS\n6\nNUMBER OF HEADS FOR MASKED TRANSFORMER LAYERS\n4\nDECODER DEPTH\n2\nNUMBER OF PARAMETERS FOR 1D GP REGRESSION\n334562\nC.3\nDATASET DETAILS OF N-DIMENSIONAL GP REGRESSION TASK\nIn an n-dimensional Gaussian Process (GP) regression task, we start by sampling the context and\ntarget inputs. Specifically, we first determine the number of context points |c| by drawing from a\nuniform distribution Unif(n2 × 5, n2 × 50 −n2 × 5). The interval for this uniform distribution\nis scaled by n2 to ensure that as the input dimension increases, the number of context points also\nincreases, which is necessary for constructing an accurate predictive density for the target points.\nNext, we sample the number of target points |t| from a uniform distribution Unif(n2×5, n2×50−|c|)\nto keep the total number of points within a manageable range. After determining the number of\n18\n\n\nPublished as a conference paper at ICLR 2025\ncontext and target points, we sample the input x for each context and target point from the uniform\ndistribution Unif(−2, 2) independently for each dimension i in [n].\nWe then generate the outputs y using the corresponding kernel functions. We employ the RBF\nkernel k(x, x′) = s2 exp\n\u0010\n−||x−x′||2\n2ℓ2\n\u0011\nand the Matern 5/2 kernel k(x, x′) = s2 \u0010\n1 +\n√\n5d\nℓ\n+ 5d2\n3ℓ2\n\u0011\n,\nwhere d = ||x −x′||. For these kernels, the parameters are sampled as follows: s ∼Unif(0.1, 1.0),\nℓ∼Unif(0.1, 0.6), and p ∼Unif(0.1, 0.5).\nC.4\nEMNIST DATASET\nWe employed the EMNIST 2 Balanced dataset (Cohen et al., 2017), which consists of 112,800\ntraining samples and 18,800 test samples. This dataset encompasses 47 distinct classes, from which\nwe selected 11 classes for our use. Consequently, our training and test datasets comprise 26,400 and\n4,400 samples, respectively. Each image is represented by a 28 × 28 grid with a single channel. We\nmapped the pixel coordinates to a range from -0.5 to 0.5 and normalized the pixel values to lie within\n[-0.5, 0.5]. We sample the number of context points |c| ∼Unif(5, 45) and the number of target points\n|t| ∼Unif(5, 50 −|c|).\nC.5\nCELEBA DATASET\nWe utilized the CelebA 3 dataset (Liu et al., 2015), which includes 162,770 training samples, 19,867\nvalidation samples, and 19,962 test samples. The images were center-cropped to 32x32 pixels,\nresulting in a 32 × 32 grid with 3 RGB channels. As with the EMNIST dataset, we scaled the\npixel coordinates to a range of -0.5 to 0.5 and normalized each pixel value within [-0.5, 0.5]. We\nsampled the number of context points |c| from Unif(5, 45) and the number of target points |t| from\nUnif(5, 50 −|c|).\nC.6\nCELEBA VIDEO DATASET\nFor the CelebA Video dataset, we generated a simple video dataset using the CelebA image dataset.\nSpecifically, after normalizing the pixel coordinates and values according to the pre-processing steps\nfor the CelebA dataset, we set the original CelebA data as the initial frame at time t = 0. We then\ngradually decreased the brightness by subtracting 5/255 from each channel for each time step t ∈[9],\nconcatenating each generated image to the previous ones. This resulted in a simple video with a\n32 × 32 grid, 3 RGB channels, and 10 frames. Consequently, the input dimension was 3, combining\nthe time axis with pixel coordinates. As with the CelebA dataset, we sampled the number of context\npoints |c| from Unif(5, 45) and the number of target points |t| from Unif(5, 50 −|c|).\nC.7\nBAYESIAN OPTIMIZATION\nExcept for the BO experiments on HPO-B benchmark, we adjust the objective function to have the\ndomain of [−2.0, 2.0]. We evaluated our method using various benchmark datasets and real-world\nscenarios. Below, we provide details of these experiments.\nHyperparameter Tuning on HPO-B benchmark\nWe utility the HPO-B benchmark (Pineda-\nArango et al., 2021), which consists of 176 search spaces (algorithms) evaluated sparsely on 196\ndatasets with a total of 6.4 million hyperparameter evaluations. In this benchmark, the continuous\nhyperparameters normalized in [0, 1], and the categorical hyperparameters are one-hot encoded.\nWe use all the search space except for the 18-dimensional space, i.e., 2-, 3-, 6-, 8-, 9-, 10-, and\n16-dimensional search spaces are used for the experiments. To construct meta-batch from each task,\nwe sample the number of context points |c| from Unif(5, 50) and the number of target points |t| from\nUnif(5, 50 −|c|); therefore, we exclude tasks which lesser than 100 (= 50 + 50) hyperparameter\nevaluations. For baselines, we first pre-train them on all the tasks collected from all the 2-dimensional\nsearch spaces of meta-train split. We then fine-tune them on 4 meta-batches randomly sampled from\neach target task (6-, 9-, 10-, 16-dim). To prevent an overfitting on the limited data, we early-stop the\n2https://www.nist.gov/itl/products-and-services/emnist-dataset\n3https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n19\n\n\nPublished as a conference paper at ICLR 2025\ntraining with respect to the likelihood on meta-validation split of target task. For DANP, we pre-train\nit on all the tasks collected from 2-, 3-, and 8-dimensional search spaces of meta-train split. We then\nevaluate it without fine-tuning on target tasks, which corresponds to zero-shot setting. For BO, we\nrun 200 iterations for each hyperparameter tuning task in meta-test split; therefore, we exclude tasks\nwhich have lesser than 210 hyperparameter evaluations. Furthermore, the order of hyperparameter\ndimensions is randomly shuffled to make the task diverse.\n1 dimensional BO with GP generated objective functions\nTo evaluate basic BO performance when\nusing each model as a surrogate for the black-box objective function, we first create an oracle sample\nusing a GP with an RBF kernel and evaluate how well each model approximates these samples. We\nconducted 1D BO for 100 iterations across 100 tasks using the expected improvement acquisition\nfunction.\n2 and 3 dimensional BO benchmarks\nWe utilize three benchmark objective functions:\nAckley (Back, 1996) function\nf(x) = −a exp\n\u0012\n−b\nv\nu\nu\nt1\nd\nd\nX\ni=1\nx2\ni\n\u0013\n−exp\n\u00121\nd\nd\nX\ni=1\ncos(cxi)\n\u0013\n+ a + exp(1)\n(22)\nwhere xi ∈[−32.768, 32.768], for all i = 1, · · · , d and global minimum is x∗≈(0, · · · , 0).\nCosine function\nf(x) =\nd\nX\ni=1\ncos(xi)\n\u00120.1\n2π |xi| −1\n\u0013\n(23)\nwhere xi ∈[−2π, 2π], for all i = 1, · · · , d and global minimum is x∗≈(0, · · · , 0).\nRastrigin (Rastrigin, 1974) function\nf(x) = 10d +\nd\nX\ni=1\n[x2\ni −10 cos(2πxi)]\n(24)\nwhere xi ∈[−5.12, 5.12], for all i = 1, · · · , d and global minimum is x∗≈(0, · · · , 0).\nTo evaluate the models in multi-dimensional scenarios, we conduct experiments with cases\nof d = 1 and d = 2 for all the aforementioned benchmark functions. We perform evaluations over\n100 iterations for each of the 100 tasks, utilizing the expected improvement acquisition function.\nCNN BO\nFor evaluation in a real-world BO scenario, we utilized the CIFAR-10 dataset (Krizhevsky\net al., 2009) and a Convolutional Neural Network (CNN) (LeCun et al., 1989). The CIFAR-10 dataset\nconsists of 50,000 training samples and 10,000 test samples across 10 classes. In this setting, we\ngenerated 1,000 samples by creating combinations of weight decay, learning rate, and batch size,\nand trained the model for 20 epochs using the Adam optimizer (Kingma & Ba, 2015). The range for\neach hyperparameter is [1e −05, 1e −01] for learning rate, [1e −04, 1e −01] for weight decay, and\n[128, 256] for batch size, with 10 values selected uniformly within each range. These 1,000 samples\nwere pre-generated, and we evaluated each BO task with 1 initial sample and conducted 50 iterations\nfor each of the 10 tasks using the expected improvement acquisition function.\nD\nADDITIONAL EXPERIMENTS\nD.1\nCOMPARISON BETWEEN NEURAL DIFFUSION PROCESS AND DIMENSION AGNOSTIC\nNEURAL PROCESSES\nAs we discussed in Section 4, NDP has two major issues: 1) it has a structural limitation, being only\npartially dimension-agnostic for x when y = 1, and not dimension-agnostic for other combinations,\nand 2) its reliance on diffusion-based sampling to approximate the predictive distribution results in\n20\n\n\nPublished as a conference paper at ICLR 2025\nTable 11: Additional results of the context and target log-likelihood for the GP regression task in the\nFrom-scratch scenario on NDP and DANP.\nModel\n1D RBF\n1D Matern\n2D RBF\n2D Matern\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\nNDP\n5.914 ±0.097\n-0.376 ±0.077\n5.924 ±0.046\n-0.503 ±0.016\n5.945 ±0.044\n-0.570 ±0.006\n6.079 ±0.114\n-0.704 ±0.021\nDANP (ours)\n1.381 ±0.000\n0.921 ±0.003\n1.382 ±0.000\n0.723 ±0.003\n1.383 ±0.000\n0.373 ±0.001\n1.383 ±0.000\n0.068 ±0.001\nTable 12: Additional results of zero-shot scenario. The colored cell\nindicates the data dimension\nused to pre-train DANP and NDP.\nDimension\nNDP trained on 2D & 3D & 4D\nDANP trained on 2D & 3D & 4D\ncontext\ntarget\ncontext\ntarget\n1D RBF\n5.5664 ±0.001\n-0.5665 ±0.097\n1.366 ±0.004\n0.826 ±0.018\n2D RBF\n5.9409 ±0.002\n-1.5654 ±0.092\n1.383 ±0.000\n0.335 ±0.014\n3D RBF\n5.5935 ±0.001\n-4.5919 ±0.098\n1.383 ±0.000\n-0.261 ±0.025\n4D RBF\n5.9792 ±0.005\n-7.8666 ±0.095\n1.383 ±0.000\n-0.568 ±0.042\n5D RBF\n5.3512 ±0.008\n-8.4127 ±0.103\n1.359 ±0.032\n-0.676 ±0.004\n7D RBF\n5.4938 ±0.009\n-14.6106 ±0.101\n1.355 ±0.022\n-0.723 ±0.022\nsignificantly high computational costs during inference and limited likelihood performance. Table 11\nand Table 12 clearly show that while NDP outperforms DANP in terms of context likelihood, it\nsignificantly underperforms in target likelihood. This discrepancy arises because NDP relies on a\ndiffusion-based sampling method to generate possible outputs and then calculates the empirical\nposterior distribution from the gathered samples. This approach leads the model to predict the context\npoints with high accuracy and low variance, thus achieving high context likelihood. However, for\ntarget points, the model struggles to accurately predict the distribution, resulting in a lower target\nlikelihood. Moreover, in most of our tasks, it is more important to achieve high likelihood predictions\nfor unseen target points rather than focusing on the observed context points. Therefore, having a\nhigher target point likelihood is more crucial than having a high context likelihood. Specifically,\nas shown in Table 12, NDP struggles to simultaneously learn across diverse dimensional inputs,\ndemonstrating that it cannot function effectively as a general regressor for unseen dimensions.\nD.2\nGP REGRESSION TASK\nD.2.1\nOTHER METRICS\nTable 13: Additional evaluation of CRPS metric and confidence interval coverage on the 1D GP\nregression task with RBF kernel. For the CRPS metric, a smaller value indicates better performance.\nModel\ncontext CI\ntarget CI\ncontext CRPS (↓)\ntarget CRPS (↓)\nANP\n0.999 ± 0.000\n0.889 ± 0.014\n0.024 ± 0.000\n0.075 ± 0.004\nBANP\n0.999 ± 0.000\n0.904 ± 0.001\n0.024 ± 0.000\n0.075 ± 0.000\nCANP\n0.999 ± 0.000\n0.899 ± 0.001\n0.024 ± 0.000\n0.076 ± 0.000\nMPANP\n0.999 ± 0.000\n0.898 ± 0.001\n0.024 ± 0.000\n0.075 ± 0.000\nTNP\n0.999 ± 0.000\n0.909 ± 0.001\n0.024 ± 0.001\n0.071 ± 0.000\nDANP\n0.999 ± 0.000\n0.912 ± 0.002\n0.024 ± 0.000\n0.068 ± 0.000\nCRPS and Empirical Confidence interval coverage\nHere, we further evaluate DANP and other\nbaselines using additional metrics like continuous ranked probability score (CRPS) and empirical\nconfidence interval coverage. We have measured these metrics for the 1D and 2D GP regression tasks.\nFor the fair comparison, we used the checkpoints from the From-scratch experiment in Section 5.1\nfor all models. The results are presented in Table 13.\n21\n\n\nPublished as a conference paper at ICLR 2025\nFirst, regarding the confidence interval coverage, it is observed that the context confidence interval\ntends to be too wide for all models. This issue arises because Neural Process models set a minimum\nstandard deviation of 0.1 during inference to account for training stability and data noise. Additionally,\nthe value of 0.1 is simply a conventional choice when designing Neural Process models. Therefore,\nsetting this value lower during model construction can help ensure an appropriate confidence interval.\nOn the other hand, the target confidence interval tends to be relatively narrow, with the DANP model\nshowing the best results. Additionally, when looking at CRPS scores, it is clear that for context\npoints, the models generally perform at a similar level, but for target points, DANP shows better\nscores compared to the other models.\nTable 14: Results on additional metrics containing MAE, RMSE, R2, RMSCE, MACE, and MA on\n1d GP regression task. Except for R2, lower values for all these metrics indicate better alignment\nwith the target and improved calibration performance.\nModel\nMAE (↓)\nRMSE (↓)\nR2 (↑)\nRMSCE (↓)\nMACE (↓)\nMA (↑)\nANP\n0.126 ± 0.001\n0.176 ± 0.003\n0.788 ± 0.012\n0.273 ± 0.001\n0.238 ± 0.003\n0.240 ± 0.003\nBANP\n0.125 ± 0.001\n0.175 ± 0.003\n0.811 ± 0.001\n0.273 ± 0.007\n0.237 ± 0.001\n0.239 ± 0.002\nCANP\n0.127 ± 0.001\n0.178 ± 0.002\n0.801 ± 0.005\n0.267 ± 0.008\n0.239 ± 0.002\n0.237 ± 0.005\nMPANP\n0.124 ± 0.001\n0.173 ± 0.003\n0.807 ± 0.005\n0.274 ± 0.014\n0.242 ± 0.007\n0.244 ± 0.008\nTNP\n0.122 ± 0.002\n0.173 ± 0.001\n0.808 ± 0.002\n0.287 ± 0.003\n0.251 ± 0.005\n0.253 ± 0.006\nDANP\n0.120 ± 0.001\n0.165 ± 0.002\n0.816 ± 0.002\n0.259 ± 0.000\n0.230 ± 0.003\n0.228 ± 0.002\nMetrics related to calibration\nWe additionally measure some other metrics related to calibration.\nWe measured and reported the following 6 additional metrics: 1) Mean Absolute Error (MAE),\n2) Root Mean Square Error (RMSE), 3) Coefficient of Determination (R2), 4) Root Mean Square\nCalibration Error (RMSCE), 5) Mean Absolute Calibration Error (MACE), 6) Miscalibration Area\n(MA). Except for R2, lower values for all these metrics indicate better alignment with the target\nand improved calibration performance. We conducted the evaluation using models trained on a 1d\nGP regression task, comparing our method with the baselines. The results, summarized in Table 14,\ndemonstrate that DANP achieves the best performance across a range of metrics. This observation\nreaffirms that DANP not only outperforms in terms of NLL but also achieves improved performance\nin calibration-related metrics compared to the baselines. These additional evaluations highlight the\nrobustness of our method across diverse aspects of model performance.\nD.2.2\nFINE-GRAINED EVALUATION ON THE 1D GP REGRESSION TASKS\nTable 15: Additional fine-grained evaluation on the 1D GP regression task. Here, we evaluate for the\nless context and more context scenarios.\nModel\nLess context\nMore context\ncontext\ntarget\ncontext\ntarget\nANP\n1.380 ± 0.000\n0.323 ± 0.006\n1.375 ± 0.000\n1.165 ± 0.002\nBANP\n1.380 ± 0.000\n0.334 ± 0.002\n1.375 ± 0.000\n1.172 ± 0.001\nCANP\n1.380 ± 0.001\n0.291 ± 0.005\n1.374 ± 0.000\n1.156 ± 0.001\nMPANP\n1.380 ± 0.000\n0.317 ± 0.007\n1.374 ± 0.001\n1.165 ± 0.003\nTNP\n1.382 ± 0.000\n0.363 ± 0.005\n1.379 ± 0.001\n1.209 ± 0.004\nDANP\n1.383 ± 0.000\n0.396 ± 0.003\n1.380 ± 0.000\n1.214 ± 0.002\nHere, we evaluate how uncertainty behavior changes under various conditions for each method. To\nexplore these changes, we considered three settings in GP regression tasks: 1) scenarios with a small\nnumber of context points versus a large number, 2) situations with high noise scale, and 3) cases\nwhere the training kernel differs from the evaluation kernel. The experimental results can be found in\nTable 15 and Table 16.\nFirst, in the 1D GP regression experiment reported in Section 5.1, the number of context points\nranged randomly from a minimum of 5 to a maximum of 45 for evaluation. For the first setting, ’small\n22\n\n\nPublished as a conference paper at ICLR 2025\nTable 16: Additional fine-grained evaluation on the 1D GP regression task. Here, we evaluate the\nincreased noise scale and kernel change scenarios.\nModel\nNoise\nKernel change\ncontext\ntarget\ncontext\ntarget\nANP\n1.377 ± 0.000\n0.855 ± 0.004\n1.373 ± 0.000\n0.667 ± 0.003\nBANP\n1.377 ± 0.000\n0.864 ± 0.001\n1.373 ± 0.000\n0.688 ± 0.003\nCANP\n1.377 ± 0.000\n0.837 ± 0.003\n1.372 ± 0.000\n0.641 ± 0.003\nMPANP\n1.376 ± 0.001\n0.856 ± 0.005\n1.372 ± 0.001\n0.663 ± 0.005\nTNP\n1.381 ± 0.000\n0.906 ± 0.005\n1.380 ± 0.000\n0.697 ± 0.003\nDANP\n1.381 ± 0.001\n0.922 ± 0.002\n1.381 ± 0.000\n0.717 ± 0.008\nTable 17: Additional experimental results showing the context and target log-likelihoods for the GP\nregression task in Zero-shot scenarios. The first column labeled nD represents the outcomes for the\nn-dimensional GP dataset with an RBF kernel. Cells highlighted in\nindicate the data dimension\nused to pre-train DANP. The initial context and target log-likelihood results are derived from the\nDANP model trained on 2 and 3-dimensional GP datasets with an RBF kernel. The subsequent results\nare obtained from the DANP model trained on 2, 3, and 4-dimensional GP datasets with both RBF and\nMatern kernels.\nDimension\n2D & 3D with RBF\n2D & 3D & 4D with RBF and Matern\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.354 ±0.023\n0.826 ±0.048\n1.360 ±0.022\n0.830 ±0.030\n2D RBF\n1.383 ±0.000\n0.341 ±0.008\n1.383 ±0.001\n0.318 ±0.021\n3D RBF\n1.383 ±0.001\n-0.251 ±0.008\n1.383 ±0.001\n-0.296 ±0.050\n4D RBF\n1.368 ±0.003\n-0.661±0.012\n1.383 ±0.001\n-0.601 ±0.063\n5D RBF\n1.293 ±0.050\n-0.711 ±0.012\n1.378 ±0.003\n-0.679 ±0.005\ncontext’ refers to using 5 to 15 context points, while ’large context’ involves 30 to 45 context points\nfor evaluation. In the second setting, the variance of the Gaussian noise used was increased to 2.5\ntimes the original value, and the model was evaluated using this adjusted evaluation set. Lastly, for\nthe third setting, we evaluated the model, trained on an RBF kernel, with an evaluation set generated\nusing a Matern 5/2 kernel.\nAs shown in Table 15, DANP clearly outperforms other baselines in both small and large context\nscenarios. Notably, DANP demonstrates superior performance compared to the other baselines in\nthe small context scenario, indicating its ability to accurately predict the predictive distribution with\na limited number of observations. Moreover, as illustrated in Table 16, DANP excels in both the\nincreased noise scale and differing kernel scenarios. These results confirm that DANP can effectively\nadapt to unseen tasks by learning generally shared features across different tasks.\nD.2.3\nADDITIONAL RESULTS FOR THE ZERO-SHOT SCENARIO\nIn the Zero-shot scenario for the GP regression task, we conducted two additional experiments\nutilizing DANP pre-trained with: 1) 2 and 3-dimensional GP datasets using the RBF kernel, and 2)\n2, 3, and 4-dimensional GP datasets using both RBF and Matern kernels. The log-likelihood results\nin Table 17 for the first experiment indicate that DANP can effectively predict the density of unseen\ndimensional GP datasets, though performance slightly declines for higher-dimensional datasets that\nare farther from the trained dimensions, as compared to the results in Table 2a. This demonstrates that\nwhile DANP can perform zero-shot inference on unseen dimensional datasets, training on a diverse\nrange of dimensions enhances predictive performance.\nIn the second experiment, the log-likelihood results show that DANP can be trained on diverse tasks\nwith different kernels. Notably, DANP was able to simultaneously train on 2, 3, and 4-dimensional GP\n23\n\n\nPublished as a conference paper at ICLR 2025\nTable 18: Additional experimental results showing the context and target log-likelihoods for the\nGP regression in Fine-tuning scenarios. Here, we utilize 1-dimensional GP regression task with the\nMatern kernel as a downstream task.\nMethod\nFull fine-tuning\nFreeze fine-tuning\ncontext\ntarget\ncontext\ntarget\nCANP\n-0.352 ±0.053\n-0.512 ±0.034\n-0.233 ±0.108\n-0.408 ±0.084\nANP\n-0.280 ±0.094\n-0.348 ±0.080\n-0.343 ±0.044\n-0.422 ±0.023\nBANP\n-0.320 ±0.073\n-0.401 ±0.037\n-0.025 ±0.145\n-0.207 ±0.154\nMPANP\n-0.128 ±0.246\n-0.259 ±0.125\n-0.260 ±0.092\n-0.490 ±0.142\nTNP\n-0.086 ±0.024\n-0.476 ±0.139\n0.336 ±0.128\n-0.243 ±0.162\nDANP(ours)\n1.372 ±0.001\n0.689 ±0.004\n1.372 ±0.001\n0.684 ±0.004\nTable 19: Ablation results for 1D, 2D GP regression and image completion tasks.\nModel\n1D RBF\n1D Matern\n2D RBF\n2D Matern\nEMNIST\nCelebA\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\ncontext\ntarget\nTNP\n1.381 ± 0.000\n0.904 ± 0.003\n1.381 ± 0.000\n0.710 ± 0.001\n1.383 ± 0.000\n0.362 ± 0.001\n1.383 ± 0.000\n0.060 ± 0.002\n1.378 ± 0.001\n0.945 ± 0.004\n4.140 ± 0.005\n1.632 ± 0.005\n+ DAB\n1.381 ± 0.000\n0.907 ± 0.001\n1.382 ± 0.000\n0.713 ± 0.001\n1.383 ± 0.000\n0.365 ± 0.001\n1.383 ± 0.000\n0.061 ± 0.000\n1.378 ± 0.001\n0.949 ± 0.004\n4.146 ± 0.001\n1.645 ± 0.014\n+ Latent\n1.381 ± 0.000\n0.923 ± 0.003\n1.382 ± 0.000\n0.722 ± 0.001\n1.383 ± 0.000\n0.371 ± 0.001\n1.383 ± 0.000\n0.064 ± 0.001\n1.379 ± 0.001\n0.967 ± 0.010\n4.140 ± 0.002\n1.973 ± 0.023\nDANP\n1.381 ± 0.000\n0.921 ± 0.003\n1.382 ± 0.000\n0.723 ± 0.003\n1.383 ± 0.000\n0.373 ± 0.001\n1.383 ± 0.000\n0.068 ± 0.001\n1.382 ± 0.001\n0.969 ± 0.002\n4.149 ± 0.000\n2.027 ± 0.006\n- Pos\n1.381 ± 0.000\n0.922 ± 0.002\n1.382 ± 0.000\n0.724 ± 0.001\n1.381 ± 0.000\n-0.395 ± 0.022\n1.381 ± 0.001\n-0.446 ± 0.006\n1.279 ± 0.009\n0.376 ± 0.012\n3.117 ± 0.005\n0.631 ± 0.030\n+ PMA\n1.381 ± 0.000\n0.921 ± 0.001\n1.382 ± 0.000\n0.721 ± 0.001\n1.383 ± 0.000\n0.372 ± 0.004\n1.383 ± 0.000\n0.067 ± 0.002\n1.381 ± 0.000\n0.975 ± 0.007\n4.150 ± 0.001\n2.025 ± 0.007\ndatasets with both RBF and Matern kernels without increasing model size. By increasing the model\nsize to accommodate more features and additional structural layers, DANP can generalize to a wider\nvariety of tasks with different generating processes.\nD.2.4\nADDITIONAL RESULTS FOR THE FINE-TUNING SCENARIO WITH DIFFERENT KERNEL\nIn the Fine-tuning scenario for the GP regression task, we fine-tuned on 160 1-dimensional GP\nregression tasks using the Matern kernel. For baselines, we utilized pre-trained models that had been\ntrained on 2-dimensional GP regression tasks with the RBF kernel, as detailed in Section 5.1. For\nDANP, we used models pre-trained on 2, 3, and 4-dimensional GP regression tasks with the RBF\nkernel, also as described in Section 5.1. The results in Table 18 clearly demonstrate that while the\nbaselines fail to generalize, DANP can generalize to the 1-dimensional GP regression task with the\nMatern kernel almost as effectively as the From-scratch results in Table 1 with only a few datasets.\nThis indicates that DANP not only generalize well to unseen dimensional GP tasks with a known\nkernel but also to unseen dimensional GP tasks with an unknown kernel, compared to other baselines.\nD.3\nFULL EXPERIMENTAL RESULTS FOR THE SECTION 5.4\nIn this subsection, we report the full log-likelihood results from the ablation study on both the context\nand target datasets. In Table 19, it can be easily observed that the context exhibits similar trends to\nthe target as we discussed in Section 5.4.\nD.4\nABLATION RESULTS ON DIFFERENT OBJECTIVES\nAs highlighted in Foong et al. (2020), the ELBO loss we used for DANP does not provide the exact\nELBO for the −log pθ(yt|xt, Dc), because we use q(θ|Dc) instead of p(θ|Dc). More precisely, the\nMaximum Likelihood Loss is a biased estimator of −log pθ(yt|xt, Dc), and the ELBO we used\nis a lower bound of the same quantity. Therefore, both losses still share the same issue, and the\neffectiveness of each loss depends on the model.\nTypically, the maximum likelihood loss tends to exhibit larger variance compared to variational\ninference, so, given our model’s need to handle multiple varying dimensional tasks simultaneously,\nwe opted for variational inference to ensure stability. However, it is worth experimenting with other\nloss functions. Therefore, we conducted additional experiments and included the results from training\nwith the maximum likelihood loss as well.\n24\n\n\nPublished as a conference paper at ICLR 2025\nTable 20: Comparison of zero-shot performance between DANP trained with the variational loss and\nthe maximum likelihood loss. Here, each method trained with 2 and 4D GP datasets with RBF kernel\nwhile performing inference on the 1, 2, 3, 4, and 5D GP datasets with RBF kernel\nMethod\nVariational Inference\nMarginal Likelihood\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.336 ±0.047\n0.806 ±0.048\n1.340 ±0.025\n0.790 ±0.008\n2D RBF\n1.383 ±0.000\n0.340 ±0.007\n1.383 ±0.000\n0.330 ±0.012\n3D RBF\n1.377 ±0.007\n-0.360 ±0.063\n1.381 ±0.001\n-0.420 ±0.112\n4D RBF\n1.379 ±0.007\n-0.589 ±0.056\n1.383 ±0.000\n-0.614 ±0.045\n5D RBF\n1.357 ±0.012\n-0.689 ±0.004\n1.356 ±0.040\n-0.701 ±0.023\nTable 21: Comparison of zero-shot performance between DANP trained with the variational loss and\nthe maximum likelihood loss. Here, each method trained with 2, 3, and 4D GP datasets with RBF\nkernel while performing inference on the 1, 2, 3, 4, and 5D GP datasets with RBF kernel\nMethod\nVariational Inference\nMarginal Likelihood\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.366 ±0.004\n0.826 ±0.018\n1.360 ±0.006\n0.805 ±0.021\n2D RBF\n1.383 ±0.000\n0.355 ±0.014\n1.382 ±0.000\n0.285 ±0.012\n3D RBF\n1.383 ±0.000\n-0.261 ±0.025\n1.383 ±0.001\n-0.320 ±0.044\n4D RBF\n1.383 ±0.000\n-0.568 ±0.042\n1.381 ±0.002\n-0.658 ±0.039\n5D RBF\n1.359 ±0.032\n-0.676 ±0.004\n1.364 ±0.021\n-0.742 ±0.006\nWe conducted ablation experiments on the ML loss and VI loss using DANP trained on 2 and 4d GP\ndata, as well as DANP trained on 2d, 3d, and 4d GP data. These experiments were performed in a\nzero-shot scenario by inferring on 1, 2, 3, 4, and 5d GP regression data. The results, presented in\nTable 20 and Table 21, show that while ML loss occasionally yields better log-likelihoods for context\npoints, the VI loss consistently provides superior performance for the target points, which are of\ngreater interest during inference. This trend is particularly evident in experiments trained on 2, 3, and\n4d GP data. These findings demonstrate that using the VI loss for training DANP is generally more\nbeneficial for improving generalization compared to the ML loss.\nD.5\nABLATION EXPERIMENTS ON POSITIONAL EMBEDDING IN DAB MODULE\nMany previous works have shown that sinusoidal positional encoding tends to perform poorly (Press\net al., 2021) in terms of generalization when extrapolating to longer sequence lengths for Large\nLanguage Models. In response to this, approaches like Rotary Position Embedding (RoPE; Touvron\net al., 2023; Su et al., 2024) have been proposed and used to address these limitations. While sinusoidal\npositional encoding successfully handled interpolation and extrapolation in our experimental settings,\nRoPE could potentially improve this performance. Therefore, we conducted additional experiments\nusing a modified RoPE-based encoding tailored for the DAB module.\nIn our implementation, we retained the basic formulation of RoPE while ensuring different positional\nencodings for the each x and y dimensions, similar to the approach we used with DAB. Specifically,\nwe distinguished the embeddings added to queries and keys from x and y by alternating the cosine\nand sine multiplications for each. For example, if for x we calculate q1x · cos(pos) + q2x · sin(pos),\nthen for y, we compute q1y · sin(pos) + q2y · cos(pos).\nUsing this modified positional encoding, we conduct additional experiments on the zero-shot and the\nfine-tune scenario in Gaussian Process regression tasks using the same settings in the main paper to\nevaluate the impact of RoPE on the performance of our model.\nWe conducted ablation experiments on sinusoidal PE and RoPE in a zero-shot scenario by inferring\non 1D, 2D, 3D, 4D, and 5D GP regression data using DANP models trained on 2D and 4D GP\n25\n\n\nPublished as a conference paper at ICLR 2025\nTable 22: Comparison of zero-shot performance between DANP trained with the sinusoidal positional\nembedding and RoPE. Here, each method trained with 2, and 4D GP dataset with RBF kernel while\nperforming inference on the 1, 2, 3, 4, and 5D GP dataset with RBF kernel\nPositional Embedding\nsinusoidal PE\nRoPE\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.336 ±0.047\n0.806 ±0.048\n1.352 ±0.012\n0.777 ±0.035\n2D RBF\n1.383 ±0.000\n0.340 ±0.007\n1.383 ±0.000\n0.348 ±0.003\n3D RBF\n1.377 ±0.007\n-0.360 ±0.063\n1.381 ±0.001\n-0.360 ±0.013\n4D RBF\n1.379 ±0.007\n-0.589 ±0.056\n1.383 ±0.000\n-0.577 ±0.008\n5D RBF\n1.357 ±0.012\n-0.689 ±0.004\n1.351 ±0.024\n-0.704 ±0.019\nTable 23: Comparison of zero-shot performance between DANP trained with the sinusoidal positional\nembedding and RoPE. Here, each method trained with 2, 3, and 4D GP dataset with RBF kernel\nwhile performing inference on the 1, 2, 3, 4, and 5D GP dataset with RBF kernel\nPositional Embedding\nsinusoidal PE\nRoPE\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.366 ±0.004\n0.826 ±0.018\n1.367 ±0.002\n0.787 ±0.021\n2D RBF\n1.383 ±0.000\n0.355 ±0.014\n1.382 ±0.000\n0.334 ±0.007\n3D RBF\n1.383 ±0.000\n-0.261 ±0.025\n1.383 ±0.001\n-0.256 ±0.006\n4D RBF\n1.383 ±0.000\n-0.568 ±0.042\n1.383 ±0.002\n-0.576 ±0.036\n5D RBF\n1.359 ±0.032\n-0.676 ±0.004\n1.367 ±0.014\n-0.679 ±0.007\nregression data, as well as on 2D, 3D, and 4D GP regression data. The results, presented in Table 22\nand Table 23, indicate that while sinusoidal PE consistently outperforms RoPE in the 1D case, their\nperformance is largely similar across other dimensions. This suggests that for these scenarios, both\nsinusoidal PE and RoPE exhibit comparable interpolation and extrapolation capabilities.\nWe also conducted experiments using the trained models to perform few-shot learning on 1D GP\nregression, following the setup in the main paper. As shown in Table 24, while there were some\nperformance differences in the zero-shot setting for the 1D GP regression task, these differences\nlargely disappeared after few-shot fine-tuning. This indicates that the choice of positional embed-\nding—whether sinusoidal PE or RoPE—has minimal impact on performance once the model is\nfine-tuned.\nD.6\nABLATION ON GP REGRESSION SETUP AND ZERO-SHOT EVALUATION\nBecause most of the models have trouble with extrapolation rather than interpolation, it is important\nto analyze our method’s extrapolation capabilities as compared to its performance in interpolation\nsettings. To address this, we conducted additional experiments by training on the {1, 2}, and {3, 4}\ndimensional cases, then evaluating the results on {1, 2, 3, 4, 5} dimensional test data.\nHere, we train DANP utilizing both sinusoidal PE and RoPE to further analyze their generalization\nability. Table 25 and Table 26 present the performance of DANP when trained on data from {1, 2}\ndimensions and {3, 4} dimensions, respectively.\nFrom Table 25, we observe that when trained on the limited range of {1, 2} dimensions, both\npositional embedding methods fail to learn sufficient general features, leading to lower generalization\nperformance compared to training on {2, 4} or {2, 3, 4} dimensions. This result emphasizes the\nimportance of training on higher-dimensional data to capture general features that enable better\ngeneralization to unseen dimensions. A similar pattern is evident in Table 26.\nHowever, a distinct trend emerges in Table 25 compared to Table 22 and Table 23. While both\nsinusoidal PE and RoPE performed similarly when sufficient general features could be learned from\nmore diverse training dimensions, RoPE demonstrates noticeably weaker generalization ability than\n26\n\n\nPublished as a conference paper at ICLR 2025\nTable 24: Comparison of fine-tune performance between DANP trained with the sinusoidal PE and\nthe RoPE. Here, each method trained with 2, and 4D GP datasets or 2, 3, and 4D GP datasets with\nRBF kernel while performing few-shot training on the 1D GP dataset with RBF kernel. Here, we\nreport the performance for both the full fine-tuning and freeze finetuning\nPositional Embedding\nFull finetuning\nFreeze finetuning\ncontext\ntarget\ncontext\ntarget\n2,4D sinusoidal PE\n1.375 ±0.001\n0.890 ±0.004\n1.375 ±0.001\n0.889 ±0.002\n2,3,4D sinusoidal PE\n1.375 ±0.000\n0.893 ±0.004\n1.376 ±0.001\n0.890 ±0.005\n2,4D RoPE\n1.375 ±0.001\n0.886 ±0.020\n1.374 ±0.001\n0.884 ±0.015\n2,3,4D RoPE\n1.376 ±0.000\n0.882 ±0.006\n1.376 ±0.001\n0.882 ±0.007\nTable 25: Comparison of zero-shot performance between DANP trained with the sinusoidal PE and\nthe RoPE. Here, each method trained with 1, and 2D GP datasets with RBF kernel while performing\ninference on the 1, 2, 3, 4, and 5D GP datasets with RBF kernel.\nPositional Embedding\nsinusoidal PE\nRoPE\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.381 ±0.000\n0.916 ±0.003\n1.381 ±0.012\n0.916 ±0.002\n2D RBF\n1.383 ±0.000\n0.346 ±0.001\n1.383 ±0.000\n0.350 ±0.006\n3D RBF\n1.307 ±0.004\n-0.633 ±0.030\n1.056 ±0.204\n-0.919 ±0.172\n4D RBF\n1.138 ±0.012\n-0.817 ±0.005\n0.101 ±0.676\n-1.685 ±0.416\n5D RBF\n0.885 ±0.022\n-0.961 ±0.069\n-1.223 ±0.758\n-2.899 ±0.360\nsinusoidal PE when the training data is limited to the narrow dimensional range of {1, 2}. This result\nhighlights the dependency of RoPE on richer training data which contains richer general features to\nachieve high generalization ability.\nD.7\nADDITIONAL EXTRAPOLATION RESULTS FOR THE FINE-TUNING SCENARIO\nWe conducted additional fine-tuning experiments on 5 d GP regression data to analyze the extrapo-\nlation ability of our method. In this experiment, we aim to compare not only the performance of a\nsingle DANP model against the baselines but also evaluate and compare multiple variants of DANP\ntrained on different dimensional GP data. Specifically, we include DANP models trained on {1, 2},\n{3, 4}, {2, 4}, and {2, 3, 4} dimensional GP data, as well as the corresponding DANP models where\nsinusoidal PE is replaced with RoPE.\nThe results in Table 27 clearly demonstrate that DANP outperforms the baselines in extrapolation\nfew-shot scenarios, showcasing its robustness in handling these challenging tasks. Additionally, we\nobserve that the DANP trained with 1,2d RoPE shows a notable improvement in generalization perfor-\nmance when provided with a few-shot setting. However, despite this improvement, its performance\non the target data remains inferior compared to other DANP training settings, such as those utilizing\nhigher-dimensional data ({3, 4}, {2, 4}, or {2, 3, 4}) or sinusoidal PE.\nD.8\nTRAINING BOTH GP REGRESSION AND IMAGE COMPLETION\nTo further demonstrate the ability of DANP to learn various tasks simultaneously, we conducted an\nexperiment involving both GP regression tasks and image completion tasks. Specifically, we trained\nour model on 2 and 3-dimensional GP regression tasks with the RBF kernel, as well as on EMNIST\nand CelebA image completion tasks. We then evaluated our model using an additional 1-dimensional\nGP regression task. As shown in Table 28, although the performance slightly decreased compared\nto training each task separately, DANP successfully learned all training tasks and generalized well\nto the unseen task. This demonstrates that DANP is capable of simultaneously training on diverse\n27\n\n\nPublished as a conference paper at ICLR 2025\nTable 26: Comparison of zero-shot performance between DANP trained with the sinusoidal PE and\nthe RoPE. Here, each method trained with 3, and 4D GP datasets with RBF kernel while performing\ninference on the 1, 2, 3, 4, and 5D GP datasets with RBF kernel.\nPositional Embedding\nsinusoidal PE\nRoPE\ncontext\ntarget\ncontext\ntarget\n1D RBF\n1.130 ±0.042\n0.501 ±0.016\n1.239 ±0.021\n0.472 ±0.019\n2D RBF\n1.301 ±0.008\n0.178 ±0.010\n1.369 ±0.001\n0.248 ±0.012\n3D RBF\n1.383 ±0.000\n-0.278 ±0.005\n1.383 ±0.001\n-0.265 ±0.002\n4D RBF\n1.383 ±0.000\n-0.582 ±0.014\n1.383 ±0.000\n-0.556 ±0.006\n5D RBF\n1.359 ±0.012\n-0.701 ±0.015\n1.242 ±0.024\n-0.726 ±0.044\nTable 27: Comparison of fine-tuning performance between DANP with various settings and the\nbaselines. Here, we use the few-shot 5d GP regression task with RBF kernel for the evaluation. We\nalso compare the performances for both full finetuning and freeze finetuning for all models.\nMethod\nFull fine-tuning\nFreeze fine-tuning\ncontext\ntarget\ncontext\ntarget\nANP\n-0.851 ±0.017\n-0.852 ±0.016\n-0.837 ±0.024\n-0.837 ±0.025\nBANP\n-0.817 ±0.012\n-0.813 ±0.011\n-0.830 ±0.013\n-0.828 ±0.016\nCANP\n-0.854 ±0.026\n-0.856 ±0.022\n-0.847 ±0.057\n-0.851 ±0.050\nMPANP\n-0.862 ±0.081\n-0.863 ±0.083\n-0.910 ±0.016\n-0.911 ±0.015\nTNP\n-0.825 ±0.081\n-0.831 ±0.083\n-0.830 ±0.021\n-0.831 ±0.023\n2,4D sinusoidal PE\n1.382 ±0.005\n-0.674 ±0.003\n1.382 ±0.001\n-0.674 ±0.003\n2,3,4D sinusoidal PE\n1.382 ±0.001\n-0.672 ±0.004\n1.382 ±0.001\n-0.671 ±0.006\n1,2D sinusoidal PE\n1.301 ±0.020\n-0.772 ±0.034\n1.300 ±0.021\n-0.774 ±0.030\n3,4D sinusoidal PE\n1.377 ±0.006\n-0.683 ±0.004\n1.377 ±0.006\n-0.684 ±0.004\n2,4D RoPE\n1.381 ±0.001\n-0.672 ±0.001\n1.382 ±0.001\n-0.672 ±0.001\n2,3,4D RoPE\n1.382 ±0.000\n-0.672 ±0.003\n1.382 ±0.001\n-0.672 ±0.004\n1,2D RoPE\n1.126 ±0.010\n-0.901 ±0.006\n1.124 ±0.009\n-0.903 ±0.005\n3,4D RoPE\n1.371 ±0.009\n-0.693 ±0.023\n1.374 ±0.006\n-0.691 ±0.021\ntasks and generalizing across different tasks. The model’s performance could be further improved by\nincreasing its capacity, either by expanding the feature space or adding more layers.\nD.9\nADDITIONAL EXTRAPOLATION EXPERIMENTS FOR THE IMAGE COMPLETION TASK\nWe conducted an additional experiment on the CelebA landmark (Liu et al., 2015) task to further\ndemonstrate the capabilities of our method. In the standard CelebA landmark task, the goal is to\npredict the locations of five facial landmarks: left eye, right eye, left mouth corner, right mouth corner,\nand nose, based on a single image. However, since Neural Processes predict a distribution over the\ntarget points using a given context, we adapted the CelebA landmark task to better fit this approach.\nWe modified the task by combining the image’s RGB values with the corresponding coordinates for\neach landmark, creating a 5-dimensional input. The output was restructured as a 5-dimensional label\nrepresenting which of the five facial regions the prediction corresponds to. This setup allowed us to\ntrain and evaluate the model in a way that aligns with the predictive distribution framework of Neural\nProcesses.\nFor the experiment, we used pre-trained models for the baselines, specifically the CelebA image\ncompletion models, while we trained DANP on both the EMNIST dataset and CelebA image\ncompletion tasks. This approach allowed us to assess the performance of DANP under a slightly\nmodified but challenging setup, testing its ability to generalize across different types of tasks. Table 29\nvalidates that DANP still performs well on the different types of tasks compared to other baselines.\n28\n\n\nPublished as a conference paper at ICLR 2025\nTable 28: Additional results for training both GP regression tasks and image completion tasks. We\ntrained DANP with 2 and 3-dimensional GP dataset and EMNIST and CelebA image completion tasks.\nDataset\ncontext\ntarget\n1D RBF\n1.299 ±0.023\n0.710 ±0.032\n2D RBF\n1.381 ±0.000\n0.294 ±0.005\n3D RBF\n1.381 ±0.000\n-0.313 ±0.020\nEMNIST\n1.382 ±0.000\n0.888 ±0.004\nCelebA\n4.148 ±0.000\n1.895 ±0.024\nTable 29: Experimental results on the modified CelebA landmark task. Here, we fine-tuned baselines\nwith 100-shot CelebA landmark dataset.\nMethod\nFull fine-tuning\nFreeze fine-tuning\ncontext\ntarget\ncontext\ntarget\nANP\n0.572 ±0.024\n0.557 ±0.027\n0.568 ±0.022\n0.554 ±0.027\nBANP\n0.636 ±0.031\n0.574 ±0.020\n0.628 ±0.027\n0.568 ±0.023\nCANP\n0.525 ±0.030\n0.506 ±0.028\n0.523 ±0.031\n0.504 ±0.028\nMPANP\n0.536 ±0.036\n0.485 ±0.023\n0.535 ±0.034\n0.487 ±0.024\nTNP\n0.658 ±0.020\n0.557 ±0.035\n0.653 ±0.021\n0.554 ±0.033\nDANP(ours)\n1.354 ±0.001\n0.674 ±0.007\n1.340 ±0.002\n0.672 ±0.005\nFor the zero-shot scenario, DANP achieves 1.171 ± 0.020 for the context dataset and 0.252 ± 0.003\nfor the target dataset. These results demonstrate that although the target likelihood of zero-shot DANP\nis lower compared to that of fine-tuned baselines—primarily due to variations in both input and\noutput dimensions from the training data—DANP quickly surpasses other baselines after fine-tuning.\nThis highlights DANP’s robust ability to generalize effectively in challenging zero-shot scenarios\nwhile rapidly improving with minimal fine-tuning.\nD.10\nBAYESIAN OPTIMIZATION\nTo illustrate the wide-ranging applicability of DANP, we conducted BO (Brochu et al., 2010) experi-\nments across various scenarios: 1) a 1-dimensional BO experiment using objective functions derived\nfrom GPs with an RBF kernel, 2) 2 and 3-dimensional BO benchmarks, and 3) hyperparameter tuning\nfor a 3-layer CNN (LeCun et al., 1989) on the CIFAR-10 (Krizhevsky et al., 2009) classification task.\nFollowing Nguyen & Grover (2022), we utilized Ackley, Cosine, and Rastrigin benchmark functions\nas the objective functions for the 2 and 3-dimensional BO experiments. For the hyperparameter\ntuning of the 3-layer CNN, we initially trained 1000 CNN models with varying hyperparameters,\nincluding learning rate, batch size, and weight decay, and then identified the optimal hyperparameter\ncombination using NP models. We measured performance using best simple regret, which measures\nthe difference between the current best value and the global best value. And, we run 50 iterations\nfor all the BO experiments. For detailed information about the objective functions in the 2 and\n3-dimensional BO and CNN training, see Appendix C. As baselines, we employed pre-trained models\nfor each n-dimensional GP regression task corresponding to the n-dimensional BO tasks. In contrast,\nfor DANP, we used a single model pre-trained with 2, 3, and 4-dimensional GP regression tasks in\nthe Zero-shot scenario. The results in Fig. 5 demonstrate that DANP outperforms other baselines in\nterms of regret with same iteration numbers. This demonstrates that DANP is capable of serving as a\nsurrogate model for different BO tasks using only a single model without additional training using BO\ndatasets. In Fig. 5, we only report BO results with 2-dimensional Cosine and 3-dimensional Ackley\nobjective function among various 2 and 3-dimensional BO benchmarks.\nFull results for the synthetic Bayesian Optimization\nHere, we present the comprehensive experi-\nmental results for 2 and 3-dimensional BO benchmark objective functions, including Ackley, Cosine,\n29\n\n\nPublished as a conference paper at ICLR 2025\n0\n10\n20\n30\n40\n50\nIterations\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nRegret\n1D BO\n0\n10\n20\n30\n40\n50\nIterations\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n2D Cosine\n0\n10\n20\n30\n40\n50\nIterations\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3D Ackley\n0\n10\n20\n30\n40\n50\nIterations\n1.0\n1.5\n2.0\n2.5\n3.0\nCNN BO\nANP\nBANP\nCANP\nMPANP\nTNP\nDANP\nFigure 5: Results for BO with various BO tasks. These four figures, from left to right, show the regret\nresults for 1-dimensional GP with RBF kernel, 2-dimensional cosine, 3-dimensional Ackley, and the\nCNN BO experiments.\n0\n10\n20\n30\n40\n50\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nRegret\n1D BO\n0\n10\n20\n30\n40\n50\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n2D Cosine\n0\n10\n20\n30\n40\n50\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n2D Rastrigin\n0\n10\n20\n30\n40\n50\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n2D Ackley\n0\n10\n20\n30\n40\n50\nIterations\n2\n4\n6\n8\n10\nCumulative Regret\n0\n10\n20\n30\n40\n50\nIterations\n0\n2\n4\n6\n8\n10\n0\n10\n20\n30\n40\n50\nIterations\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\nIterations\n5\n10\n15\n20\n25\n30\n35\nANP\nBANP\nCANP\nMPANP\nTNP\nDANP\nFigure 6: Full Results for BO with 1-dimensional GP generated BO tasks and 2-dimensional bench-\nmark BO tasks. Here, we present cumulative regret results in addition to regret results.\nand Rastrigin. Additionally, we report cumulative regret results alongside regret results for all BO\nexperiments. Similar to the BO experiments outlined in Appendix D.10, we employed pre-trained\nmodels for each n-dimensional GP regression task corresponding to the n-dimensional BO tasks as\nbaselines. In contrast, for DANP, we utilized a single model pre-trained with 2, 3, and 4-dimensional\nGP regression tasks in the Zero-shot scenario. The results depicted in Fig. 6 and Fig. 7 demonstrate\nthat DANP is proficient in serving as a surrogate model for various BO tasks using only a single model,\nwithout requiring additional training on BO datasets.\nD.11\nIMAGE COMPLETION AND VIDEO COMPLETION\nAdditional visualization examples for the Image completion and Video completion\nIn this\nsection, we provide additional visualization examples for the image completion task using the\nEMNIST and CelebA datasets, as well as for the video completion task with the CelebA video\ndatasets. First, in Fig. 8, we display true video examples generated using the process described in\nAppendix C. It is visually apparent that the images gradually become darker over time. Next, we\nvisualize 10 example images from the EMNIST and CelebA datasets. In Fig. 9 and Fig. 10, the full\nimages are shown in the first column and the context points in the second column. Following that,\nwe sequentially visualize the predicted mean and variance of CANP, ANP, BANP, MPANP, TNP, and\nDANP. And finally, in Fig. 11, we report predictive mean and variance of video trained DANP.\n30\n\n\nPublished as a conference paper at ICLR 2025\n0\n10\n20\n30\n40\n50\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRegret\n3D Cosine\n0\n10\n20\n30\n40\n50\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n3D Rastrigin\n0\n10\n20\n30\n40\n50\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3D Ackley\n0\n10\n20\n30\n40\n50\n1.0\n1.5\n2.0\n2.5\n3.0\nCNN BO\n0\n10\n20\n30\n40\n50\nIterations\n2\n4\n6\n8\n10\nCumulative Regret\n0\n10\n20\n30\n40\n50\nIterations\n0\n50\n100\n150\n200\n0\n10\n20\n30\n40\n50\nIterations\n0\n25\n50\n75\n100\n125\n150\n175\n0\n10\n20\n30\n40\n50\nIterations\n0\n10\n20\n30\n40\n50\n60\n70\n80\nANP\nBANP\nCANP\nMPANP\nTNP\nDANP\nFigure 7: Full Results for BO with 3-dimensional benchmark BO tasks and CNN BO tasks. Here, we\npresent cumulative regret results in addition to regret results.\nTable 30: Empirical results on the time series blood pressure estimation task.\nMethod\nFull fine-tuning\nFreeze fine-tuning\ncontext\ntarget\ncontext\ntarget\nCANP\n0.964 ±0.030\n0.875 ±0.024\n0.962 ±0.031\n0.870 ±0.022\nANP\n1.037 ±0.021\n0.950 ±0.017\n1.035 ±0.021\n0.947 ±0.019\nBANP\n1.104 ±0.018\n0.968 ±0.011\n1.100 ±0.017\n0.966 ±0.012\nMPANP\n1.012 ±0.016\n0.938 ±0.018\n1.010 ±0.014\n0.930 ±0.010\nTNP\n1.165 ±0.020\n0.987 ±0.013\n1.160 ±0.022\n0.986 ±0.011\nDANP(ours)\n1.235 ±0.001\n1.184 ±0.006\n1.230 ±0.002\n1.180 ±0.005\nD.12\nTIME-SERIES EXPERIMENT\nTo further validate the practicality, we conducted additional experiments on time series data using the\nblood pressure estimation task from the MIMIC-III dataset (Johnson et al., 2016). Specifically, we\nassumed real-world scenarios where certain features from patient data might be missing, or entirely\ndifferent sets of features could be collected. Under this assumption, we trained the model using only\na subset of features from the MIMIC-III dataset and evaluated its performance when additional or\ndifferent sets of features became available.\nSpecifically, we considered five features: T, Heart Rate, Respiratory Rate, SpO2, and Temperature.\nFor pre-training, we utilized T and Heart Rate features, while for the fine-tuning scenario, we assumed\nonly Respiratory Rate, SpO2, and Temperature features were available (this scenario can happen if\nwe assume that we trained our model with data from hospital A and want to evaluate on the data\nin hospital B). We pre-trained the models with 32,000 training samples and fine-tuned them with\nonly 320 samples. And here, we considered observations from time 0, ..., t as context points and\nt+1, ..., T as target points. As shown in Table 30, our DANP achieved strong performance in the time\nseries blood pressure estimation task, demonstrating robustness and adaptability in this real-world\nscenario. These results are consistent with the findings presented in the main paper, further validating\nDANP’s effectiveness in handling diverse and practical challenges.\n31\n\n\nPublished as a conference paper at ICLR 2025\nt=0\nt=1\nt=2\nt=3\nt=4\nt=5\nt=6\nt=7\nt=8\nt=9\nFigure 8: Examples of video data constructed following Appendix C.\nTable 31: Wall clock time evaluation for the TNP and DANP in various settings. Here, we utilize\nRTX 3090 GPU for the evaluation.\nModel\n1D regression\n2D regression\nEMNIST\nCelebA\nTNP\n1 min 30 sec\n1 min 50 sec\n1 min\n1 min 20 sec\nDANP\n1 min 50 sec\n2 min 40 sec\n1 min 20 sec\n1 min 40 sec\nD.13\nDISCUSSION ON THE RESOURCE REQUIREMENTS\nHere, we will analyze the time complexity compared to the TNP both theoretically and practically.\nFirst theoretically, let us denote B, N, dx, dy, dr, Ld, and Ll denote the batch size, the number\nof data points (union of context and target), the dimension of input x, the dimension of output y,\nthe representation dimension, the number of layers in the deterministic path, and the number of\nlayers in the latent path, respectively. The additional computational cost for the DAB module is\nO(BN(dx + dy)2)dr), and for the latent path, it is O(LlBN 2dr). Since the computational cost\nfor TNP is O(LdBN 2dr), the overall computational cost of DANP can be expressed as O((Ll +\n32\n\n\nPublished as a conference paper at ICLR 2025\nFull Img\nContext\nCANP \nCANP \nANP \nANP \nBANP \nBANP \nMPANP \nMPANP \nTNP \nTNP \nDANP \nDANP \nFigure 9: Predicted mean and variance of EMNIST dataset with baselines and DANP.\nFull Img\nContext\nCANP \nCANP \nANP \nANP \nBANP \nBANP \nMPANP \nMPANP \nTNP \nTNP \nDANP \nDANP \nFigure 10: Predicted mean and variance of CelebA dataset with baselines and DANP.\nLd)BN 2dr) + O(BN(dx + dy)2dr). Generally, since N >> (dx + dy)2 holds, the dominant term\nin the computational cost can be approximated as O((Ll + Ld)BN 2dr).\nFor the practical time cost, we measure the time cost to train 5000 steps for the GP regression tasks\nand image completion tasks for TNP and DANP. The results are shown in Table 31.\n33\n\n\nPublished as a conference paper at ICLR 2025\nFull img\ncontext\nmean\nstd\nFigure 11: Predicted mean and variance of video data with DANP when training with video dataset.\n34\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20661v1.pdf",
    "total_pages": 34,
    "title": "Dimension Agnostic Neural Processes",
    "authors": [
      "Hyungi Lee",
      "Chaeyun Jang",
      "Dongbok Lee",
      "Juho Lee"
    ],
    "abstract": "Meta-learning aims to train models that can generalize to new tasks with\nlimited labeled data by extracting shared features across diverse task\ndatasets. Additionally, it accounts for prediction uncertainty during both\ntraining and evaluation, a concept known as uncertainty-aware meta-learning.\nNeural Process(NP) is a well-known uncertainty-aware meta-learning method that\nconstructs implicit stochastic processes using parametric neural networks,\nenabling rapid adaptation to new tasks. However, existing NP methods face\nchallenges in accommodating diverse input dimensions and learned features,\nlimiting their broad applicability across regression tasks. To address these\nlimitations and advance the utility of NP models as general regressors, we\nintroduce Dimension Agnostic Neural Processes(DANP). DANP incorporates\nDimension Aggregator Block(DAB) to transform input features into a\nfixed-dimensional space, enhancing the model's ability to handle diverse\ndatasets. Furthermore, leveraging the Transformer architecture and latent\nencoding layers, DANP learns a wider range of features that are generalizable\nacross various tasks. Through comprehensive experimentation on various\nsynthetic and practical regression tasks, we empirically show that DANP\noutperforms previous NP variations, showcasing its effectiveness in overcoming\nthe limitations of traditional NP models and its potential for broader\napplicability in diverse regression scenarios.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}