{
  "id": "arxiv_2502.21147v1",
  "text": "SAME ACCURACY, TWICE AS FAST: CONTINUOUS TRAINING\nSURPASSES RETRAINING FROM SCRATCH\nEli Verwimp1\nKU Leuven\neli.verwimp@kuleuven.be\nGuy Hacohen1\nKU Leuven\nguy.hacohen@kuleuven.be\nTinne Tuytelaars\nKU Leuven\ntinne.tuytelaars@kuleuven.be\nABSTRACT\nContinual learning aims to enable models to adapt to new datasets without losing performance on\npreviously learned data, often assuming that prior data is no longer available. However, in many\npractical scenarios, both old and new data are accessible. In such cases, good performance on both\ndatasets is typically achieved by abandoning the model trained on the previous data and re-training\na new model from scratch on both datasets. This training from scratch is computationally expen-\nsive. In contrast, methods that leverage the previously trained model and old data are worthy of\ninvestigation, as they could significantly reduce computational costs. Our evaluation framework\nquantifies the computational savings of such methods while maintaining or exceeding the perfor-\nmance of training from scratch. We identify key optimization aspects – initialization, regularization,\ndata selection, and hyper-parameters – that can each contribute to reducing computational costs.\nFor each aspect, we propose effective first-step methods that already yield substantial computational\nsavings. By combining these methods, we achieve up to 2.7x reductions in computation time across\nvarious computer vision tasks, highlighting the potential for further advancements in this area.\n1\nIntroduction\nIn machine learning applications, the available data tends to expand and evolve over time. This often requires updating\na model that was trained on a large dataset (‘old data’) to be further trained and adapted to also perform well on a new\ndataset (‘new data’). Such new datasets often include data from different data generating distributions, which may\nentail additional classes, new domains or corner cases. In practical scenarios, a common approach is to retrain the\nmodel from scratch using both the old and new dataset. Yet when model and dataset sizes increase, retraining from\nscratch becomes computationally expensive. Reducing these costs requires more efficient methods to train models\ncontinuously, starting from a model trained on a part of the full dataset.\nA large part of continual learning research approached this problem in a resource-constrained setting, aiming to reach\nthe highest possible performance on both old and new data under some constraints (Verwimp et al., 2024). A common\nconstraint is to limit the amount of memory usage, which restricts how much old data can be stored. Such constraints\ncan lead to suboptimal performance, in part due to catastrophic forgetting (French, 1999). In e.g. industry applications,\nit is often undesirable to sacrifice performance and retraining from a randomly initialized model (‘from scratch’) is\npreferred over using older models (Huyen, 2022). This is a wasteful approach; there is a model available that performs\nwell on the old data, so why not use it? In practice, it has been shown to be difficult to continue to train previously\ntrained models, even without storage constraints on past examples (Ash & Adams, 2020).\nIn this paper, we aim to reduce the cost of training a model on new data, when it has already been trained on some old\ndata. The cost of training this old model is treated as a sunk cost (Kahneman & Tversky, 1972), its training happened\n1Both authors contributed equally.\narXiv:2502.21147v1  [cs.LG]  28 Feb 2025\n\n\n1.1\nOur contribution\n2\n0.0\n1.0\n2.0\n3.0\n4.0\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nIterations\n100%\n99%\n95%\nOurs\nNaive solution\nScratch\nx2.7 faster\nx2.3 faster\nx2 faster\nFigure 1: Test accuracy on CIFAR100 (70+30) with a model pre-trained on 70 classes. The ‘scratch’ method starts\nfrom random initialization, while the ‘naive’ approach uses the pre-trained model without modification. ‘Ours’ mod-\nifies the optimization process (see Section 3) and matches the scratch performance with 2.7× lower computational\ncost.\nin the past and the price for this training has already been paid. In contrast, future costs for training on new data can\nbe reduced or mitigated, and in this paper, we show that using old models can be an effective way of doing so. In\nmany computational problems, the total cost consists of both memory and computational aspects. However, for the\nsize of modern networks, the computational costs tend to outweigh the memory costs. For example, the hard disks\nto store ImageNet21k are about 50 times cheaper than training a large vision transformer on the same data once (see\nAppendix A.1 for details of this estimate).\nInstead of the wasteful retraining from scratch approach, we propose to focus on continuous solutions, which leverage\nthe existing model when new data becomes available. The simplest of these solutions is to continue the training\nof the available model on a combined dataset of old and new data. Even though this solution uses a pre-trained\ninitialization, it converges at a similar speed as retraining from scratch (Figure 1). Besides slower convergence, often\nworse performance is obtained when starting from a previous solution (Ash & Adams, 2020), making the challenge of\nfinding computational gains even more difficult. There are good reasons to study these problems in memory-restricted\nsettings, without access to all old data. Yet as a first attempt to reduce the computational cost in continuous settings,\nwe allow to store all old data in this paper. We believe by first tackling this problem, future work can solve the same\nproblem with tighter restrictions on the memory.\nIn Section 3, we start from the canonical SGD update rule and show that all of its aspects – the initialization, the\nobjective function, the sampling process, and the hyper-parameters – can significantly reduce computational costs. We\noutline and evaluate several strategies – inspired by the literature – that act on these aspects. Each of them presents\na promising avenue for future research to accelerate the convergence of continuous models. In Section 4, we show\nthat while these methods individually reduce computational costs, their effects are to some extent complementary;\ncombining them yields even greater reductions. In many of the tested scenarios, our approach leads to more than a\ntwo-fold decrease in computational complexity, compared to retraining from scratch. Such savings can make a sub-\nstantial difference, especially when retraining happens repeatedly. Finally, we demonstrate that the proposed methods\nimprove training efficiency across a variety of image classification datasets, multi-task settings, and domain incremen-\ntal scenarios, highlighting the robustness and potential of our method to reduce the computational burden of re-training\nmachine learning models.\n1.1\nOur contribution\n• We propose a novel way of evaluating continuous training, allowing models full access to previous data and\nmeasuring their computational cost rather than only their accuracy.\n• We discuss the different areas in the optimization process that can be explored to accelerate convergence.\nEach area is broad enough to be the focus of a different method.\n• For each area, we introduce a first-step method based on the literature, which already significantly enhances\nthe learning speed of models that are trained continuously.\n• We demonstrate that improvements in these areas are complementary and applicable in many scenarios,\nmeaning advancements in one area do not preclude further gains in others.\n\n\n1.2\nRelated Work\n3\n1.2\nRelated Work\nWarm start.\nStarting from non-random initialization is most commonly used in transfer learning, where a pre-\ntrained model (typically on ImageNet) serves as a starting point to kick-start training downstream tasks (Zhuang et al.,\n2020). Contrary to our work, these works are often not concerned with performance on the pre-training (i.e. old) data.\nWhile beneficial, pre-training may hurt downstream performance (Zoph et al., 2020). This may be explained by a loss\nof plasticity in trained networks (Dohare et al., 2024; Abbas et al., 2023). Ash & Adams (2020) showed that when\ncontinuously training on the same data source, lower performance is reached than when starting from scratch. Gupta\net al. (2023); Parmar et al. (2024) study how to continually pre-train NLP models which is strongly related to our\nsetting. They examine learning rate scheduling but do not consider loss of plasticity, regularization, and data selection\nas done in this paper.\nFaster optimization.\nAt its core, machine learning involves solving a challenging and computationally expensive\noptimization problem, and many approaches have been proposed to streamline this process (Sun et al., 2019). First-\norder methods, such as stochastic gradient descent (SGD)(Robbins & Monro, 1951), are the most widely used, where\nthe full gradient is typically approximated using small batches. However, SGD can suffer from slow convergence,\nespecially in high-variance settings(Johnson & Zhang, 2013), which can be mitigated by techniques like Nesterov\nmomentum (Sutskever et al., 2013). Adaptive approaches such as AdaGrad (Duchi et al., 2011) and Adam (Kingma,\n2014) help by adjusting the learning rate dynamically, though explicit learning rate scheduling often enhances their\nperformance (Loshchilov & Hutter, 2017; Smith & Topin, 2019). Second-order methods, despite their promise, are\nhampered by the computational expense of estimating the Hessian (Martens, 2016). Goyal et al. (2017) also highlights\nthe intricate relationship between batch size and learning rate in neural network optimization. Additionally, regular-\nization techniques like batch normalization (Ioffe & Szegedy, 2015) and weight normalization (Salimans & Kingma,\n2016) can further improve convergence. These considerations become especially important when optimizing from a\npre-trained model, as is the case in this work, rather than from random initialization (Narkhede et al., 2022).\nContinual learning.\nContinual learning concerns cases where data is not available all at once (see e.g. (De Lange\net al., 2021; Wang et al., 2024) for surveys). Most studies have focused on settings with strong constraints on the\namount of old data that can be stored (Verwimp et al., 2024). Replay methods (Chaudhry et al., 2019; Buzzega et al.,\n2020) aim to use this stored data as effectively as possible. However, even with replay mechanisms, learning new\ndata often leads to (catastrophic) forgetting of previously learned examples. Other approaches modify the model\narchitectures (e.g. Yan et al., 2021) and regularization losses (e.g. Li & Hoiem, 2017) to reduce forgetting. In contrast,\nsome works have looked at settings where computational cost is restricted rather than memory costs. In these cases,\nstandard replay outperforms other methods (Prabhu et al., 2023). Later works (Smith et al., 2024; Harun et al., 2023)\nimproved replay techniques in these settings. Our work, however, imposes no memory restrictions and instead focuses\non accelerating learning compared to models trained from scratch.\n2\nProblem Description\nWe consider the following setup: an existing dataset, denoted as Dold = (Xold, Yold), where Xold represents the input\nexamples and Yold the corresponding labels. A model fold : Xold →Yold, has already been trained on this dataset. We\nare then provided with some new data, Dnew = (Xnew, Ynew). The objective is to get a model f : (Xold ∪Xnew) →\n(Yold ∪Ynew) that performs well on both the new and the old datasets, D = Dold ∪Dnew, with a computational cost\nas low as possible.\nTo ensure a fair comparison between models, we use the same architecture when comparing the computation costs of\ndifferent training methods. This, and keeping a fixed batch size, leads to a fixed number of FLOPs per iteration. Their\nequivalence allows us to report training iterations, which is easier to measure. Let f i represent the model f after i\ntraining iterations. We define the speed s of a model f in achieving a target accuracy, a, as the first iteration in which\nf reached or surpassed that accuracy. Formally:\ns(f, a) = min\ni\n\n\ni ∈N :\n1\n|D|\n|D|\nX\nj\n1[f i(xj) = yj] ≥a\n\n\n\n(1)\nWe use the relative speed-up Lr when comparing the performance of different models against a baseline model. The\nbaseline model fscratch is trained from scratch on the combined dataset D and achieves an accuracy of ascratch. Then\nwe can define Lr as:\n\n\n2.1\nImplementation details\n4\nLr(f, fscratch) = s(fscratch, ascratch)\ns(f, r/100 · ascratch)\n(2)\nor the relative number of iterations that a model f requires to reach the same (L100) or a fraction of (e.g. L99) the final\naccuracy of a model trained from scratch. For instance, L99 = 2 would indicate that model f attains an accuracy of\n0.99 · ascratch with a 2 times lower computational cost than was required to train the full model from scratch to attain\nascratch, or equivalently, half the number of iterations. To reduce notation complexity, we will simply use Lr in the\nremainder of the paper when the models can be inferred from context. Note that this measure only works when each\niteration has the same computational cost, but the idea can easily be extended to when this is not the case.\n2.1\nImplementation details\nDatasets.\nWe conducted experiments on a variety of image classification datasets, including CIFAR-10 (Krizhevsky\net al., 2009), CIFAR-100, subsets of ImageNet (ImageNet-100 and ImageNet-200) (Deng et al., 2009), and Adaptiope\n(Ringwald & Stiefelhagen, 2021). For continuous training, each dataset was divided into disjoint subsets, and training\nproceeded in a cumulative manner. For example, in CIFAR-100 (80+10+10), classes are split into three groups: 80\nclasses, followed by 10, and 10. The model was trained sequentially, first on the 80 classes, then on the combined 80 +\n10, and finally on all 100 classes. Class splits were randomized, where the specific seeds are available in the attached\ncode.\nTraining and baselines.\nUnless otherwise specified, we used ResNet-18 with a cosine annealing scheduler\n(Loshchilov & Hutter, 2017), the Adam optimizer, and a learning rate of 0.001. All models are trained with stan-\ndard cropping and horizontal flipping augmentations. Additional model and hyperparameter details can be found in\nthe attached code and Appendix A.2. The scratch baseline indicates a model that is trained from a random initial-\nization on both old and new data together. The naive baseline represents a continuous model that simply continues\ntraining from the old model, without any modification.\nExperimental details.\nEvery experiment shown in this paper is repeated five times. The results shown are the\naverages of these experiments, accompanied by their standard error in plots. The results presented are always obtained\nby using the combined test sets of the old and the new datasets.\n3\nMethod\nIn deep learning, optimization is typically performed using variants of stochastic gradient descent, where model pa-\nrameters θi are updated by subtracting an estimate of the gradient of the objective function, based on a small batch of\nsamples. The standard minibatch SGD update rule is:\nθi+1 = θi −η\nN\nX\nj∈Bi\n∇L(θi, (xj, yj))\n(3)\nwhere η is the learning rate, L is the objective function, and Bi a batch of N examples sampled from the full dataset\nD. All components of this update – model initialization (θ0), batch composition, learning rate adjustments, and\nmodifications to the objective function – play a critical role in the optimization trajectory and convergence speed.\nFor other optimizers like e.g. Adam (Kingma, 2014), the simple average here would be replaced by a momentum-\nbased average, but the idea remains the same. In the following sections, we explore how each of these elements can\nsignificantly accelerate continuous model training. CIFAR-100 (70+30) serves as a case study for comparing various\nstrategies.\n3.1\nInitialization\nWhen training models from scratch, careful random initialization offers several advantageous properties (Narkhede\net al., 2022). However, during continuous training, many of these advantages are lost as training does not start from a\nrandom set of weights. Several works have shown that this potentially leads to reduced plasticity, i.e. not being able\nto learn new information as fast and accurately as a model trained from scratch (Ash & Adams, 2020; Dohare et al.,\n2024). This issue is similarly observed in continuous training, as shown in Figure 2. The naive benchmark is both\nworse and converges slower than retraining from scratch.\n\n\n3.2\nObjective function and regularization\n5\n0.0\n1.0\n2.0\n3.0\n4.0\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nIterations\n100%\n99%\n95%\nShrink & Perturb\nNaive\nScratch\nFigure 2: Initialization.\nNaive continuous training is\nslower and less accurate than retraining from scratch. Re-\nintroducing plasticity with shrink-and-perturb improves\nboth speed and accuracy, surpassing scratch training.\n0.0\n1.0\n2.0\n3.0\n4.0\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nIterations\n100%\n99%\n95%\nL2-init\nL2\nScratch + L2-init\nFigure 3: Objective function. Regularizing the objective\nfunction with L2-losses is beneficial in both from scratch\nand continuous learning, yet the latter outperforms the for-\nmer when using L2-init regularization.\nTo restore plasticity during warm-start training without distribution shifts, Ash & Adams (2020) introduced the ‘shrink\nand perturb’ method. Rather than using the previously trained model’s weights directly, the weights are shrunk by a\nfactor α and combined with a small portion of randomly initialized parameters θrandom. The resulting initialization\nθinit is computed as:\nθinit = αθold + βθrandom\n(4)\nIn our experiments, we used α = 0.4 and β = 0.001 without tuning, as proposed by the original authors. However,\na broad range of α and β values yielded qualitatively similar results (see Appendix A.3). In Figure 2, we compare\nResNet-18 models trained continuously on CIFAR-100 (70+30) with and without the shrink-and-perturb method.\nThe results show that re-introducing plasticity can not only accelerate convergence but also potentially improve final\naccuracy compared to both scratch training and continuous training without it.\n3.2\nObjective function and regularization\nAn alternative approach to tackle the reduced plasticity problem is to prevent it from arising in the first place by\nchanging the objective function L. Inspired by the continual backpropagation idea of Dohare et al. (2024), Kumar\net al. (2023) propose a simplified version that uses an L2 regularizer towards the initial random weights θ0, rather\nthan towards the origin as is typically done in L2 regularization. In our setting, this involves modifying the objective\nfunction L to include this L2-init regularizer:\nLreg(θi, (xj, yj)) = L(θi, (xj, yj)) + λ∥θi −θ0∥2\n(5)\nIn our experiments, we used λ = 0.01 without tuning, as proposed by the original authors. However, a broad range of λ\nvalues yielded qualitatively similar results (see Appendix A.4). In Figure 3, we train ResNet-18 models on CIFAR-100\n(70+30) and compare the results of models trained from scratch with those trained continuously, both with the L2-init\nregularizer. The results indicate that adding this regularization term accelerates the convergence of the continuously\ntrained models, more so than it improves models trained from scratch. Since regularization can also benefit models\ntrained from scratch, we use this baseline here, as well as the more standard L2 regularization (i.e. θ0 = 0) which is\nnot as effective as L2-init regularization.\n3.3\nBatch composition\nEstimating the full gradient of a dataset is expensive in the deep learning case, as many iterations are required to\nreach a good solution. To overcome this, minibatches containing a small part of the entire dataset are used to estimate\nthe gradient. Typically, examples are sampled from the full dataset with uniform probability. In continual learning\nmethods, it is a common practice to balance examples from old and new data in a batch (Rolnick et al., 2019), which\neither increases or decreases the sampling probability of old data depending on whether there is either more or less old\nthan new data.\n\n\n3.4\nLearning rate scheduling\n6\n0.0\n1.0\n2.0\n3.0\n4.0\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nEasy / hard sampling\nOld / new sampling \nScratch\nNaive\nIterations\n100%\n99%\n95%\nFigure 4: Batch composition.\n‘Old / new’ sampling\nbalances old and new examples in each batch, unlike\nthe naive baseline, which uses proportional sampling.\n‘Easy / hard’ sampling reduces the inclusion of the easiest\nand hardest old examples, significantly improving perfor-\nmance. (Naive and ’old/new’ results nearly overlap.)\n0.0\n1.0\n2.0\n3.0\n4.0\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nIterations\n100%\n99%\n95%\n1/4 iterations\n2/4 iterations\n3/4 iterations\n4/4 iterations\nScratch\nFigure 5: Hyperparameters. Shortening the learning rate\nscheduler allows for faster convergence but at the cost of\nlower final accuracy. On its own, changing the sched-\nuler does not reach the required accuracy, yet when com-\nbined with the other aspects, it becomes important (see\nSection 4.1)\nIn Figure 4, we show that having an equal number of new and old examples (‘old / new sampling’) does not improve\nthe results compared to the naive approach, which balances old and new examples in a batch according to their ratio in\nthe full dataset (i.e. 70% old examples and 30% new ones in this example). Katharopoulos & Fleuret (2018) show that\nthe optimal sampling distribution is proportional to the gradient norms of the individual examples. In Appendix A.7,\nwe show that at the very start of training the gradient norms of the old examples are indeed smaller, but after about\n100 iterations, there is no noticeable difference on the class level, which explains the results.\nWhile the ratio of old and new examples does not have an immediate effect, not all data in the replay memory is\nequally useful. Often in continual learning, access to the old dataset Dold is limited to a small fraction. In our case, the\nreplay memory has infinite capacity. We build on the work of Hacohen & Tuytelaars (2024), who propose a sampling\nstrategy that reduces the importance of very easy and very difficult examples in the memory. More formally, they\ndefine learning speed ls of a sample xj as the relative epoch in which a sample is classified correctly:\nls(xj, yj) = 1\nE\nE\nX\ni=1\n1[f i(xj) = yj]\n(6)\nwith E the total number of epochs and f i the model at epoch i.\nThe learning speed is used to order the old examples from easy to hard, given that the necessary information is recorded\nduring training of the old model (For more details, see Appendix A.5). Using this order, we reduce the sampling\nprobability of the 10% easiest and highest examples to one-tenth of the other examples. In the easy examples, there\nis no information left, and the hardest ones are never learned, and thus as useful. The results in Figure 4 show that\nthis approach (‘easy / hard sampling’) is helpful and speeds up training in the continuous case. In Appendix A.5 we\nshow that this is robust to the exact hyperparameters and that, while sacrificing some convergence speed, the easiest\nand hardest examples can be removed entirely.\n3.4\nLearning rate scheduling\nThe learning rate controls the step size in stochastic gradient descent, directly influencing convergence speed. In deep\nlearning, the learning rate typically starts high to allow faster progress toward optimal solutions and decreases gradu-\nally to avoid overshooting (Zeiler, 2012). Common scheduling strategies include ‘multistep’ scheduling (Zagoruyko\n& Komodakis, 2016), which reduces the learning rate at set intervals, and cosine annealing (Loshchilov & Hutter,\n2017), which decays it more smoothly over time.\nWhen training a model continuously, since the model is already trained on parts of the data, a more aggressive learning\nrate scheduling can accelerate convergence. While the learning rate still needs to decrease over time, this can happen\nmore rapidly over fewer iterations, as the model has already learned key information. As shown in Figure 15 in\nthe Appendix, the loss curve for the continuous model reaches a plateau much faster compared to scratch training,\nindicating that the model approaches a local optimum more quickly. This supports the need for a faster reduction in\nthe learning rate (Zeiler, 2012).\n\n\n4\nRESULTS\n7\nTable 1: Ablation results of the four different aspects studied in Section 3 under ‘continuous’ on CIFAR100 (70+30).\nAll speed-ups are relative to the model trained from scratch without any modification, ‘/’ indicates the required ac-\ncuracy was not reached. Each of the aspects individually establishes a speed-up, and combining them improves the\nresults further, albeit not fully cumulative. On the right hand side of the table, ‘scratch’ shows the result of a model\ntrained from scratch using the same techniques. Some of them improve the training speed, but to a lesser extent than\nin the continuous case.\nContinuous\nScratch\nInitialization\nRegularization\nData\nScheduler\nMax Acc\nL99\nL100\nMax Acc\nL99\nL100\n65.26\n/\n/\n65.92\n×1.33\n×1.00\n✓\n66.43\n×1.49\n×1.20\n65.06\n/\n/\n✓\n68.60\n×1.94\n×1.66\n67.90\n×1.69\n×1.51\n✓\n66.65\n×1.60\n×1.31\n66.61\n×1.54\n×1.31\n×0.25\n64.15\n/\n/\n63.72\n/\n/\n✓\n✓\n68.91\n×2.19\n×1.82\n68.01\n×1.75\n×1.54\n✓\n✓\n66.76\n×2.92\n×2.53\n66.61\n×1.54\n×1.31\n✓\n✓\n68.86\n×1.96\n×1.69\n68.30\n×1.67\n×1.49\n✓\n✓\n✓\n69.47\n×2.19\n×1.84\n68.31\n×1.69\n×1.52\n✓\n✓\n✓\n×0.25\n68.26\n×5.73\n×5.32\n63.74\n/\n/\nWe train ResNet-18 models on CIFAR-100 (70+30 split), using different lengths of cosine learning rate schedulers.\nThe most aggressive scheduler used only 25% of the total iterations, while others used 50% and 75%. The learning\ncurves, depicting the mean test errors from these experiments, are shown in Figure 5. The results indicate that more\naggressive schedulers lead to faster convergence of the continuous model to a performance level comparable to the\nscratch solution. However, overly aggressive scheduling hurts the final performance, as the networks fail to achieve\n100% of the scratch model’s final accuracy. On its own, reducing the learning length is not helpful, yet when combined\nwith the other aspects, it becomes important (See Section 4.1). Similar qualitative results were observed with multistep\nscheduling, see Appendix A.6. For the remainder of this paper, any adjustments to the learning rate scheduler for\nspecific methods are explicitly mentioned, and a comparison to the unmodified variant is provided.\n4\nResults\nIn the previous section, we explored several key aspects of SGD optimization, and each can serve as the foundation\nfor methods that reduce the computational cost of continuous training. By leveraging insights from various works in\nthe literature, we proposed a method for each aspect, demonstrating the effectiveness of each method individually.\nIn this section, we begin by showing that these methods are complementary, with their combination further accelerating\nconvergence beyond what is achieved by applying them separately. We show that although the same techniques can\nlead to better convergence when training from scratch, the benefit is larger in the continuous setting. We then extend\nthis combined approach to various datasets, scenarios, and data splits, demonstrating its applicability across image\nclassification tasks.\n4.1\nAblations\nIn the previous section, we introduced several methods aimed at speeding up the convergence of continuous models,\neach targeting a different aspect of the optimization process. We now examine whether these improvements are\ncomplementary or if the benefits of one method negate the effectiveness of others. Table 1 presents results from\ntraining five ResNet-18 models and compares all combinations. While not fully cumulative, each combination offers\nbenefits, either in convergence speed or in final accuracy. Additionally, we applied the same techniques to a model\ntrained from scratch, to rule out the possibility that the techniques are only improving overall learning capabilities.\nThe results show that while some optimization aspects have an effect on the scratch models as well (most notably\nregularization), their influence is always stronger in the continuous case. This is especially clear when shortening the\nlearning rate scheduler, which allows the continuous model to learn faster, yet greatly reduces the final accuracy of\nthe from-scratch model. These results indicate that the proposed techniques are effective in leveraging the benefits of\nhaving a model trained on the old data available. Future research targeting different aspects of the continuous learning\n\n\n4.2\nMultiple tasks\n8\nprocess could yield further gains in training speed, as multiple optimization strategies can be effectively integrated to\nspeed up the learning.\n4.2\nMultiple tasks\nWe used CIFAR100 (70+30) as a case study in Section 3, adding 30 new classes. In many continual learning scenarios,\nnew data is not only added once, but repeatedly. In Figure 6 we show that applying the same aspects on consecutive\ntasks continues to work as in the one-task case. For each task, we also train a model from scratch on all classes available\nup to this point. In Figure 6, the yellow dots indicate when the continuous model reaches the same performance (L100)\nas the from-scratch model that is trained on that task and all previous data. For every task L100 is larger than one\nand gets progressively larger. This indicates that our methods is repeatedly applicable, and benefits from larger total\naccumulated learning time.\nThese results highlight an important insight: in total, the continuous models have trained for more iterations than the\nmodels that are trained from scratch, which also explains why their final accuracy may surpass that of training from\nscratch. However, when accumulating past costs, the total cost of the continuous model is significantly lower than the\nsum of costs for models trained from scratch for every task.\nT0\nT1\nT2\nT3\nT4\nT5\nT6\nT7\nT8\nT9\nTasks\n0.2\n0.4\n0.6\n0.8\nTest Accuracy\nFigure 6: Test accuracy on the full CIFAR-100 when\ntraining a continuous model with our method on CI-\nFAR100 (50 + P10\ni=1 5). Yellow dots mark the L100 iter-\nation, where the continuous model outperforms the from-\nscratch model. The shaded area indicates maximum pos-\nsible performance based on data availability (e.g., 55%\nduring the first task 50 + 5).\n0.0\n1.0\n2.0\n3.0\n4.0\n0.65\n0.70\n0.75\n0.80\nTest Accuracy\nIterations\n100%\n99%\n95%\nOurs\nNaive\nScratch\nFigure 7: Domain adaptation results using the Adaptiope\ndataset. The ‘old’ data contains product images, the ‘new’\ndata has real life images. The test accuracy is on both\ndomains. Although naively continuing to train is nearly as\nfast as our method in this setting, our final test accuracy is\nconsiderably higher.\n4.3\nDomain adaptation\nAll previous experiments had new classes in the ‘new’ data, often referred to as class-incremental learning in continual\nlearning (De Lange et al., 2021). Here, we use the Adaptiope dataset (Ringwald & Stiefelhagen, 2021) to show that\nour method also works when new data contains no new classes, but the same classes from a different domain. In\nparticular, the ‘old’ data consists of 123 categories of product images from a shopping website and the ‘new’ data are\nreal-life images of the same products captured by users. The objective is to achieve strong performance across both\ndomains, ideally with faster convergence than re-training the model from scratch on both datasets.\nFigure 7 shows how our method outperforms both training from scratch and the naive continuous approach when\nincluding a new domain. The relative speed-up compared to the naive baselines is smaller than in the class-incremental\ncase, but the final accuracy is considerably improved.\n4.4\nOther datasets and scenarios\nIn Section 3, we demonstrated the effectiveness of each method using the CIFAR-100 (70+30) dataset for consis-\ntency across experiments. Table 2a extends these results to a range of datasets, including CIFAR-10, ImageNet-100,\nImageNet-200, and the product images of Adaptiope. For each dataset, we compare the performance of models trained\nfrom scratch, a naive continuous model, and a continuous model using our full method. We report the relative speed-up\nto reach 99% and 100% of the scratch-trained model’s accuracy, along with the final accuracy. Across all the different\n\n\n5\nCONCLUSION\n9\nTable 2: Results of our method across various datasets and splits. We report final test accuracy and relative speedup to\nreach similar performance compared to the scratch solution. The multiplier after the algorithm name indicates learning\nrate scheduling aggressiveness (e.g., Ours (×0.5) means the minimum learning rate is achieved at half the iterations\nof Ours (×1)). ‘/’ indicates that the accuracy level was not reached. Table (a) presents results on different datasets,\nwhile Table (b) provides an in-depth analysis of CIFAR-100 with various class splits compared to the better scratch +\nL2-init model, hence the slightly lower results compared to Table 1\n(a) More image classification benchmarks\nDataset\nAlgorithm\nMax Acc\nL99\nL100\nCIFAR10\n(8+2)\nScratch\n83.32\n×1.55\n×1\nContinuous\n81.37\n/\n/\nOurs (×1)\n83.95\n×1.83\n×1.29\nOurs (×0.5)\n84.33\n×3.61\n×2.92\nAdaptiope-PI\n(100+23)\nScratch\n74.59\n×1.14\n×1\nContinuous\n74.93\n×1.17\n1.02\nOurs (×1)\n77.51\n×1.55\n×1.42\nOurs (×0.5)\n76.75\n×2.36\n×2.24\nImageNet-100\n(80+20)\nScratch\n62.34\n×1.18\n×1\nContinuous\n62.21\n×1.23\n/\nOurs (×1)\n67.41\n×2.16\n×1.97\nOurs (×0.5)\n66.26\n×3.39\n×3.09\nImageNet-200\n(180+20)\nScratch\n55.16\n×1.18\n×1\nContinuous\n53.5\n/\n/\nOurs (×1)\n59.62\n×1.69\n×1.64\nOurs (×0.5)\n58.54\n×2.84\n×2.75\n(b) Different ratios of old and new classes.\nDataset\nAlgorithm\nMax Acc\nL99\nL100\nCIFAR100\nScratch + L2-init\n67.90\n×1.29\n×1.00\nScratch (×0.5)\n67.14\n/\n/\n90+10\nOurs (×1.0)\n69.42\n×1.56\n×1.41\nOurs (×0.5)\n69.07\n×2.95\n×2.65\nOurs (×0.25)\n68.42\n×5.05\n×4.47\n70+30\nOurs (×1.0)\n69.47\n×1.61\n×1.45\nOurs (×0.5)\n68.77\n×2.84\n×2.56\nOurs (×0.25)\n68.26\n×4.74\n×4.34\n50+50\nOurs (×1.0)\n69.09\n×1.50\n×1.35\nOurs (×0.5)\n68.42\n×2.70\n×2.37\nOurs (×0.25)\n67.57\n×4.34\n/\n30+70\nOurs (×1.0)\n68.81\n×1.49\n×1.31\nOurs (×0.5)\n68.10\n×2.56\n×2.30\nOurs (×0.25)\n66.93\n/\n/\ndatasets we tested, our method consistently enhances the speed of convergence both for the L99 and L100 cases, often\nalso surpassing the final accuracy of the scratch model.\nTable 2b presents results for different class split ratios, including CIFAR-100 splits of (90+10), (70+30), (50+50),\nand (30+70). Our methods consistently accelerate training, with speed-up being more significant when the second\ndata split is smaller. This aligns with intuition: when the initial ‘old’ data is larger, the old model already has more\nknowledge, requiring fewer updates when exposed to new data, compared to training from scratch, which treats both\nsplits equally.\n5\nConclusion\nIn this work, we have shown that the computational cost that is paid to train models on old data should not be treated\nas something that is lost, but rather as a starting point for further training whenever new data becomes available. We\nused all old data that was available. Future work can either focus on removing that constraint step-by-step, or further\nimproving the computational gains we started. Throughout this paper, we studied the four main components of the\ntraditional SGD update rule – the initialization, objective function, data selection and hyperparameters – and showed\nthat each of them individually can contribute to faster convergence on old and new data combined when starting from\nan old model. These aspects are thoroughly tested to be robust and effective across a wide range of scenarios. Our\nproposals should be seen as starting points to work further towards solutions that are even more effective in re-using\nold models, and saving computational costs across the board in machine learning development and applications.\nReferences\nZaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C Machado. Loss of plasticity in continual\ndeep reinforcement learning. In Conference on Lifelong Learning Agents, pp. 620–636. PMLR, 2023.\nJordan Ash and Ryan P Adams. On warm-starting neural network training. Advances in neural information processing\nsystems, 33:3884–3894, 2020.\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general\ncontinual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920–15930,\n2020.\n\n\nREFERENCES\n10\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, P Dokania, P Torr, and M Ran-\nzato. Continual learning with tiny episodic memories. In Workshop on Multi-Task and Lifelong Reinforcement\nLearning, 2019.\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and Tinne\nTuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern\nanalysis and machine intelligence, 44(7):3366–3385, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\nShibhansh Dohare, J Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, A Rupam Mahmood, and Richard S\nSutton. Loss of plasticity in deep continual learning. Nature, 632(8026):768–774, 2024.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic opti-\nmization. Journal of machine learning research, 12(7), 2011.\nRobert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128–135,\n1999.\nGoogle. Pricing — Cloud Storage — Google Cloud — cloud.google.com. https://cloud.google.com/storage/\npricing, 2024a. [Accessed 01-10-2024].\nGoogle.\nGPU pricing — Compute Engine: Virtual Machines (VMs) —no Google Cloud — cloud.google.com.\nhttps://cloud.google.com/compute/gpus-pricing, 2024b. [Accessed 01-10-2024].\nPriya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He.\nAccurate, large minibatch sgd: Training imagenet in 1 hour.\narXiv preprint\narXiv:1706.02677, 2017.\nKshitij Gupta, Benjamin Th´erien, Adam Ibrahim, Mats Leon Richter, Quentin Gregory Anthony, Eugene Belilovsky,\nIrina Rish, and Timoth´ee Lesort. Continual pre-training of large language models: How to re-warm your model?\nIn Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https://openreview.net/\nforum?id=pg7PUJe0Tl.\nGuy Hacohen and Tinne Tuytelaars. Forgetting order of continual learning: Examples that are learned first are forgot-\nten last. arXiv preprint arXiv:2406.09935, 2024.\nMd Yousuf Harun, Jhair Gallardo, Junyu Chen, and Christopher Kanan. Grasp: a rehearsal policy for efficient online\ncontinual learning. arXiv preprint arXiv:2308.13646, 2023.\nChip\nHuyen.\nReal-time\nmachine\nlearning:\nchallenges\nand\nsolutions,\nJan\n2022.\nURL\nhttps:\n//huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html#\ntowards-continual-learning. Online; accessed 29-August-2024.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/abs/1502.03167.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. Advances\nin neural information processing systems, 26, 2013.\nDaniel Kahneman and Amos Tversky. Subjective probability: A judgment of representativeness. Cognitive psychol-\nogy, 3(3):430–454, 1972.\nAngelos Katharopoulos and Franc¸ois Fleuret. Not all samples are created equal: Deep learning with importance\nsampling. In International conference on machine learning, pp. 2525–2534. PMLR, 2018.\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical Report,\n2009.\nSaurabh Kumar, Henrik Marklund, and Benjamin Van Roy. Maintaining plasticity in continual learning via regenera-\ntive regularization. In Proceedings of The 3th Conference on Lifelong Learning Agents, 2023.\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine\nintelligence, 40(12):2935–2947, 2017.\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference\non Learning Representations, 2017. URL https://openreview.net/forum?id=Skq89Scxx.\nJames Martens. Second-order optimization for neural networks. University of Toronto (Canada), 2016.\n\n\nREFERENCES\n11\nJohn C McCallum. Price and performance changes of computer technology with time. https://ourworldindata.\norg/grapher/historical-cost-of-computer-memory-and-storage, 2023.\nMeenal V Narkhede, Prashant P Bartakke, and Mukul S Sutaone. A review on weight initialization strategies for\nneural networks. Artificial intelligence review, 55(1):291–322, 2022.\nNvidia. NVIDIA A100 GPUs Power the Modern Data Center — nvidia.com. https://www.nvidia.com/en-us/\ndata-center/a100/, 2024. [Accessed 01-10-2024].\nJupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Reuse, don’t retrain:\nA recipe for continued pretraining of language models. arXiv preprint arXiv:2407.07263, 2024.\nAmeya Prabhu, Hasan Abed Al Kader Hammoud, Puneet K Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem,\nand Adel Bibi. Computationally budgeted continual learning: What does matter? In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 3698–3707, 2023.\nTobias Ringwald and Rainer Stiefelhagen. Adaptiope: A modern benchmark for unsupervised domain adaptation. In\nProceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 101–110, 2021.\nMaxime\nRio.\nTech\nInsights:\nA\nbehind-the-scenes\nlook\nat\nrolling\nout\nnew\nGPU\nre-\nsources\nfor\nNZ\nresearchers\n—\nnesi.org.nz.\nhttps://www.nesi.org.nz/case-studies/\ntech-insights-behind-scenes-look-rolling-out-new-gpu-resources-nz-researchers,\n2023.\n[Accessed 01-10-2024].\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pp.\n400–407, 1951.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for con-\ntinual learning. Advances in neural information processing systems, 32, 2019.\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep\nneural networks. Advances in neural information processing systems, 29, 2016.\nJames Seale Smith, Lazar Valkov, Shaunak Halbe, Vyshnavi Gutta, Rogerio Feris, Zsolt Kira, and Leonid Karlinsky.\nAdaptive memory replay for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 3605–3615, 2024.\nLeslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning\nrates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pp.\n369–386. SPIE, 2019.\nShiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao. A survey of optimization methods from a machine learning\nperspective. IEEE transactions on cybernetics, 50(8):3668–3681, 2019.\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum\nin deep learning. In International conference on machine learning, pp. 1139–1147. PMLR, 2013.\nA Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\nEli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L Hayes,\nEyke H¨ullermeier, Christopher Kanan, Dhireesha Kudithipudi, et al. Continual learning: Applications and the road\nforward. Transactions on Machine Learning Research, 2024.\nLiyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: theory, method\nand application. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.\nShipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental\nlearning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3014–3023,\n2021.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\nMatthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A\ncomprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43–76, 2020.\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking\npre-training and self-training. Advances in neural information processing systems, 33:3833–3845, 2020.\n\n\nA\nAPPENDIX\n12\nA\nAppendix\nA.1\nCost estimation\nTo train a machine learning model there is both a memory (storage) and compute cost. Here we will work out an ex-\nample for training a Vision Transformer (ViT) (Vaswani, 2017) on ImageNet 21k. This dataset is about 1.13 terabytes.\nCloud storage for common providers averages around 0.023$ per GB each month, which would be around 30.13$ per\nmonth (Google, 2024a). However, local storage is much cheaper than cloud storage. Today hard disk storage is about\n11$ per TB (McCallum, 2023), which can last for 10 years or more.\nViT models were trained on 8 Nividia P100 gpus for 3.5 days (Vaswani, 2017), which cost $1.46 per hour on Google\nCloud, which would be $981 in total for the entire training (Google, 2024b). Buying the GPUs would be much more\nexpensive, with prices for a single A100 GPU above $10.000 (Nvidia, 2024). Modern A100 GPUs are about 3.5 times\nfaster on real-life work loads (Rio, 2023), but more expensive to rent. At 4$4.05 per hour, the total price would still be\n$777.5.\nA.2\nHyperparameters\nUnless specified otherwise, all experiments are trained with the Adam optimizer (Kingma, 2014), with default settings\nof β1 = 0.9, β2 = 0.999 and no weight decay. The starting learning rate is equal to 0.001 and the batch size\nis consistently 128 in all experiments. Unless specified differently, a cosine scheduler is used where the minimal\nlearning rate 1e −6 is reached after 39.100 iterations, which is equal to 50 epochs using the complete CIFAR100\ndataset. All experiments use cropping and random horizontal flip augmentations.\nA.3\nInitialization\nFigure 8 shows different values of α in the shrink and perturb update rule. In general, shrinking more gives better\nresults, although it slows down learning results at the start of learning. Shrinking enough is necessary and not doing\nso may lead to suboptimal accuracies because of loss of plasticity. In Figure 9 different values of β are tested, which\nadd various levels of noise to the initialization. There is no visible effect of the size of this parameter, which is roughly\nin line with (Ash & Adams, 2020).\n0.0\n1.0\n2.0\n3.0\n4.0\nIterations\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\n𝛼=0.1\n𝛼=0.3\n𝛼=0.5\n𝛼=0.7\n𝛼=0.9\nNaive\nScratch\nFigure 8: Grid test of the α (shrink) hyperparameter in the\nshrink and perturb update rule.\n0.0\n1.0\n2.0\n3.0\n4.0\nIterations\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nβ=0.001\nβ=0.01\nβ=0.1\nNaive\nScratch\nFigure 9: Grid test of the β (perturb) hyperparameter in\nthe shrink and perturb update rule\nA.4\nRegularization\nFigure 10 shows the influence of the λ parameter in the L2-init regularization, which controls the strength of the\nregularization. A too low value will not have any effect, while setting this value too high, will lead to slower than\nnecessary learning speeds.\n\n\nA.5\nBatch composition\n13\n0.0\n1.0\n2.0\n3.0\n4.0\nIterations\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nλ=0.001\nλ=0.01\nλ=0.1\nNaive\nScratch\nFigure 10: Ablation of the λ parameter in the L2-init regularization.\nA.5\nBatch composition\nFigure 11 shows how fast examples are learned during the first task of CIFAR100 (70+30). The x-axis shows the\nepochs, while each row indicates a single sample. The examples on the bottom are green almost from the start of\ntraining, these are the ones that are very easy: the model almost has no difficulty learning them, which also means\nthey do not carry a lot of information. The ones on the top are almost completely red: they are never learned. These\nexamples are equally not very useful: they are so hard the model can not learn them anyway (which may be a result of\nbad labeling, bad images etc.).\nIn Figure 12, we ablate two of the parameters of the ‘easy / hard’ sampling process. c indicates what proportion of the\neasy and hard examples is influenced. e.g. c = 0.2 means that 20% of the examples is affected, or the 10% easiest\nand the 10% hardest. r indicates the probability that the easy and hard examples are sampled in a batch, relative to\nthe examples in the middle. e.g. with r = 0.1, an easy or hard example is 10 times less likely to be in a minibatch.\nThe result with r = 0 shows that we can even get rid of these examples, reducing the memory requirements for this\nmethod. When r = 0, this becomes the method proposed by (Hacohen & Tuytelaars, 2024), on which this idea is\nbased.\n0\n10\n20\n30\n40\nCorrect\nWrong\nCuto\u0001s\nEpochs\nExamples   (easy        hard)\nFigure 11: How fast examples are learned during the\ninitial learning phase of training on the old data of CI-\nFAR100 70+30 (i.e. the first 70 classes). Some examples\nare almost learned instantly (bottom), others never (top).\nThe cutoffs indicate both the 10% easiest and hardest ex-\namples.\n0.0\n1.0\n2.0\n3.0\n4.0\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nc = 0.2,  r = 0.0\nc = 0.2,  r = 0.1\nc = 0\nScratch:  c = 0.2,  r = 0.1\nScratch:  c = 0\nIterations\n100%\n99%\n95%\nFigure 12: Test accuracy of CIFAR100 on the 70 + 30\nbenchmark. Removing the easiest and hardest examples\n(r = 0) barely has an influence. Sampling them with a\nlow probability (r = 0.1) allows to learn a little faster on\nthe continuous model, without such an effect on the from\nscratch model.\nA.6\nSchedulers\nFigure 13 shows an experiment on CIFAR100 (70+30), with a multistep learning rate scheduler (which reduces the\nlearning rate by a fixed factor at fixed steps) rather than a cosine learning rate scheduler. This scheduler reaches the\nsame conclusion, although in this case the iterations where the scheduler kicks in has a much larger influence than in\nthe cosine scheduler case. In the continuous case, the learning rate is kept too high for too long, preventing the model\nto learn the last details, while the from scratch model is still learning.\n\n\nA.7\nGradient Norms\n14\n0.0\n0.5\n1.0\n1.5\n2.0\nIterations\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\nOurs\nNaive\nScratch\nFigure 13: When using multi-step schedulers the qualitative result stays the same, although cosine schedulers are\nbetter suited to this task of continuous learning.\nA.7\nGradient Norms\nFigure 14 shows the average gradient norm of examples of old and new data in the CIFAR100 (70+30) baseline during\ntraining of the naive baseline. While at the very start the gradient of new data is considerably higher, this difference\nhas completely disappeared after more than 100 iterations, which is nearly immediately considering 40.000 iterations\nin total. This explains mostly why it is not useful to oversample new data, which would indicate that new data is more\nimportant to learn than remembering the old data, which is not the goal.\n1\n10\n20\n40\n80\n160\n320\n640\nIterations\n0\n50\n100\nGradient norm\nOld data\nNew data\nFigure 14: Gradient norms of ‘old’ and ‘new’ data during training of the naive baseline on CIFAR100 (70+30).\nA.8\nLoss plateau\nFigure 15 shows how the loss of the naive solution plateaus earlier than when training from scratch, thus allowing to\nschedule learning rates earlier. However, it is not because such a loss plateau is reached that the final result will be\nbetter, it merely indicates that the results won’t get any better when training longer than from the point the plateau is\nreached. In fact, as can be seen in Figure 15, the loss starts to increase again as overfitting takes place. In this sense, it\nmight even be necessary to schedule early enough to obtain the best possible results.\n\n\nA.8\nLoss plateau\n15\n0.0\n1.0\n2.0\nLoss plateau reached\n0\n1\n2\n3\n4\nTest Loss\nNaive solution\nScratch\nIterations\nFigure 15: The continuous naive baseline loss plateaus earlier than that of the from scratch baseline.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21147v1.pdf",
    "total_pages": 15,
    "title": "Same accuracy, twice as fast: continuous training surpasses retraining from scratch",
    "authors": [
      "Eli Verwimp",
      "Guy Hacohen",
      "Tinne Tuytelaars"
    ],
    "abstract": "Continual learning aims to enable models to adapt to new datasets without\nlosing performance on previously learned data, often assuming that prior data\nis no longer available. However, in many practical scenarios, both old and new\ndata are accessible. In such cases, good performance on both datasets is\ntypically achieved by abandoning the model trained on the previous data and\nre-training a new model from scratch on both datasets. This training from\nscratch is computationally expensive. In contrast, methods that leverage the\npreviously trained model and old data are worthy of investigation, as they\ncould significantly reduce computational costs. Our evaluation framework\nquantifies the computational savings of such methods while maintaining or\nexceeding the performance of training from scratch. We identify key\noptimization aspects -- initialization, regularization, data selection, and\nhyper-parameters -- that can each contribute to reducing computational costs.\nFor each aspect, we propose effective first-step methods that already yield\nsubstantial computational savings. By combining these methods, we achieve up to\n2.7x reductions in computation time across various computer vision tasks,\nhighlighting the potential for further advancements in this area.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}