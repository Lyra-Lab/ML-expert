{
  "id": "arxiv_2502.21077v1",
  "text": "Enhancing deep neural networks through complex-valued\nrepresentations and Kuramoto synchronization dynamics\nSabine Muzellec\nsabine_muzellec@brown.edu\nCerCo - CNRS, University of Toulouse, France\nCarney Institute for Brain Science, Brown University, USA\nAndrea Alamia\nandrea.alamia@cnrs.fr\nCerCo - CNRS, University of Toulouse, France\nThomas Serre\nthomas_serre@brown.edu\nCarney Institute for Brain Science, Brown University, USA\nRufin VanRullen\nrufin.vanrullen@cnrs.fr\nCerCo - CNRS, University of Toulouse, France\nAbstract\nNeural synchrony is hypothesized to play a crucial role in how the brain organizes visual\nscenes into structured representations, enabling the robust encoding of multiple objects\nwithin a scene. However, current deep learning models often struggle with object binding,\nlimiting their ability to represent multiple objects effectively. Inspired by neuroscience, we\ninvestigate whether synchrony-based mechanisms can enhance object encoding in artificial\nmodels trained for visual categorization. Specifically, we combine complex-valued represen-\ntations with Kuramoto dynamics to promote phase alignment, facilitating the grouping of\nfeatures belonging to the same object. We evaluate two architectures employing synchrony: a\nfeedforward model and a recurrent model with feedback connections to refine phase synchro-\nnization using top-down information. Both models outperform their real-valued counterparts\nand complex-valued models without Kuramoto synchronization on tasks involving multi-\nobject images, such as overlapping handwritten digits, noisy inputs, and out-of-distribution\ntransformations. Our findings highlight the potential of synchrony-driven mechanisms to\nenhance deep learning models, improving their performance, robustness, and generalization\nin complex visual categorization tasks.\n1\nIntroduction\nLearning structured representations in artificial neural networks (ANNs) has been a topic of extensive re-\nsearch (Zhang et al., 2013; Chiou, 2022; Dittadi, 2023; Le Khac, 2024), yet it remains an open challenge (Schott\net al., 2021; Dittadi, 2023). Notably, some researchers argue that the inability of ANNs to effectively bind and\nmaintain structured representations may underlie their limited generalization capabilities and susceptibility\nto distributional shifts (Greff et al., 2020).\nIn neuroscience, the Binding Problem (Treisman, 1996; Roskies, 1999; Singer, 2007) refers to the brain’s\ncapacity to integrate various attributes of a stimulus—such as color, shape, motion, and location—into a\nunified perception. This process involves understanding how distinct features of an object are combined\nacross different processing stages, enabling the brain to construct meaningful and cohesive representations of\nindividual objects within a scene. Neural synchrony has been proposed as a key mechanism underlying this\nintegrative process (Singer, 2007; Uhlhaas et al., 2009).\nKuramoto dynamics and related oscillator models have been widely employed in computational neuroscience\nto explore synchronization phenomena in neural systems (Breakspear et al., 2010; Chauhan et al., 2022).\n1\narXiv:2502.21077v1  [cs.CV]  28 Feb 2025\n\n\nThese models provide insights into complex neural processes, such as phase synchronization and coordinated\nneural activity. Beyond neuroscience, the Kuramoto model has also found applications in artificial intelligence,\noffering a framework for understanding synchronization in complex systems (Ódor & Kelling, 2019; Rodrigues\net al., 2016). Recently, its utility has extended to computer vision tasks (Ricci et al., 2021; Miyato et al.,\n2024), demonstrating its potential to enhance representation learning in ANNs.\nBuilding on these insights, we propose leveraging the Kuramoto model to investigate the role of neural\nsynchrony in convolutional neural networks (CNNs) for multi-object classification. We hypothesize that\nincorporating neural synchrony, inspired by the brain’s solution to the Binding Problem, can be implemented\nusing Kuramoto dynamics within ANNs, thereby enhancing their generalization abilities.\nTo test this hypothesis, we design a hierarchical model, KomplexNet, that integrates layers of complex-valued\nunits with a bottom-up information flow. In KomplexNet, Kuramoto dynamics are applied at the initial layer\nto induce a synchronized state, which is then propagated through subsequent layers via carefully designed\ncomplex-valued operations. This approach enables the model to exploit the phase dimension of its neurons\nto bind visual features and organize visual scenes into distinct object representations, while the amplitude\ndimension retains standard CNN functionality.\nWe further extend KomplexNet by incorporating feedback connections to refine synchronization through\ntop-down information. This extension demonstrates the critical role of top-down processes in enhancing phase\nsynchrony and structuring object representations. Overall, our findings highlight the potential of neural\nsynchrony mechanisms, modeled using Kuramoto dynamics, to improve the robustness, generalization, and\nrepresentational capacity of deep learning architectures.\nOverall, our contributions are as follows:\n• We introduce KomplexNet, a complex-valued neural network that leverages Kuramoto dynamics for\nmulti-object classification.\n• KomplexNet has better classification accuracy than comparable baselines.\n• KomplexNet also exhibits better robustness to images perturbed with Gaussian noise and generaliza-\ntion to out-of-distribution classification problems.\n• Extending KomplexNet with feedback connections leads to better phase synchrony, exhibiting higher\nrobustness and generalization abilities than KomplexNet without feedback.\n2\nRelated work\nComplex-valued models.\nComplex-valued neural networks are popular and extensively used in signal\nprocessing to model complex-valued data, such as spectrograms (see (Bassey et al., 2021) for a review).\nThe term Complex-Valued Neural Network (CVNN) is commonly used to refer to fully complex networks:\nnot only is the activation function complex, but so are the parameters. Trabelsi et al. (2017) proposes a\nlist of operations adapted to a parametrization in the complex domain, including convolutions, activation\nfunctions, and normalizations. Moenning & Manandhar (2018) systematically compares complex-valued\nnetworks and their real-valued counterparts for object classification. Their findings highlight the importance\nof the choice of activation functions and architectures reflecting the interaction of the real and imaginary\nparts. However, when applied to real-valued data, the field lacks appropriate conversion mechanisms to the\ncomplex domain. One proposal by Yadav & Jerripothula (2023) includes a novel way to convert a real input\nimage into the complex domain and a loss acting on both the magnitude and the phase. They implement\ntheir transformation on several convolutional architectures and outperform their real-valued counterpart on\nvisual categorization datasets. None of these papers use phase synchrony as a mechanism for perceptual\norganization in multi-object scenes.\nSynchrony in artificial models.\nSome work has explored binding by synchrony or leveraging synchrony\nin artificial models without complex-valued activity. Ricci et al. (2021) proposes a framework for learning\nin oscillatory systems, harnessing synchrony for generalization. This approach is, however, limited by its\n2\n\n\nlearning procedure: the model is designed to learn to segment one half of an image and generalize on the\nother. Zheng et al. (2022) extends a spiking neural network with attention mechanisms to solve the binding\nproblem, though the impact of synchrony on performance or robustness is not evaluated. While these models\nsuccessfully group entities by synchronizing the spikes of neurons, synchrony was not designed to assist in\nsolving visual tasks. In particular, it is unclear from the work if and how the resulting representations help\nimprove the neural network’s overall performance, robustness, or generalization ability.\nComplex-valued representations and binding by synchrony.\nA growing body of literature uses\ncomplex-valued representations to explicitly model neural synchrony. Early models were designed to implement\na form of binding by synchrony via complex-valued units to perform phase-based image segmentation (Zemel\net al., 1995; Weber & Wermter, 2005) or object-based attention (Behrmann et al., 1998). These models were\nshallow architectures, and they were trained on small datasets. Different mechanisms, including feedback\nmechanisms (Rao et al., 2008; Rao & Cecchi, 2010; 2011), were later explored to influence synchrony in deeper\narchitectures. However, all models were trained on datasets that remained limited to toy objects. Specifically,\ndespite the simplicity of the objects, the images contained individual objects only, limiting the potential benefit\nof synchrony. Finally, Reichert & Serre (2013) scaled to Boltzmann machines and multi-object datasets. The\nauthors proposed a general framework that included operations for binding by synchrony. Binding was shown\nto emerge through the phase of neurons. However, the approach did not include backpropagation training or\nend-to-end deep learning. Specifically, a real-valued Boltzmann machine was first trained, and the phases\nwere introduced during test. Synchrony was, therefore, a completely emergent property and did not take any\npart in helping the model learn the task. Building on this work, Löwe et al. (2022) adapted the approach for\ntraining a complex auto-encoder to reconstruct multi-object images. This model was fully complex, even\nduring training. The phase synchrony helped the model reconstruct an input image and outperform its\nreal-valued counterpart. Finally, Stanić et al. (2023) scaled up the model to more objects and color images by\nadding a contrastive objective on the phases, followed by Gopalakrishnan et al. (2024) who improved the\nphase synchrony using recurrence and complex-weights. Our work distinguishes itself from previous work,\nnotably in how we exploit synchrony mechanisms. In all the aforementioned approaches, synchrony is sought\nas an emergent property of the task and the neural operations implemented in the network. In contrast, we\npropose to introduce it using a Kuramoto system as an explicit synchronizer. We then study the benefit of\nsynchrony for object categorization performance, as well as robustness and generalization.\n3\nBinding by synchrony and Gestalt criteria\nFigure 1: The binding by synchrony hypothesis. Brain activity exhibits an oscillatory pattern affecting\na population of neurons; local neuronal interactions (e.g., excitation, inhibition) result in different groups of\nneurons being activated at different phase values. According to the binding by synchrony theory, neurons\nfiring together (at the same phase) encode for the same object. Here, we use the phase of a complex number\nto represent this mechanism (right panel).\nThe binding by synchrony theory, as supported by a body of research both experimental (Singer, 2007; von der\nMalsburg, 1981) and computational Milner (1974); Grossberg (1976) (but see Roelfsema (2023); Shadlen &\n3\n\n\nMovshon (1999) for alternative hypothesis), provides a comprehensive theory to understand how the brain\nintegrates and perceives diverse sensory inputs. This theory asserts that synchronous neural activity, induced\nby neural oscillations, plays a foundational role in cognitive processes. According to this theory, when distinct\nfeatures of a sensory stimulus are processed by specialized regions of the brain, the neurons responsible for\nrepresenting these features synchronize their firing patterns at specific frequencies. This synchronization of\nneural oscillations enables precise coordination and the temporal binding of neuronal responses from different\nregions, thus uniting them into a coherent percept (Fries et al., 1997; 2002).\nThe concept of binding is also linked with the core principles of Gestalt psychology (Wertheimer, 1938), a\nfield dedicated to understanding perceptual organization and how meaningful structures emerge from sensory\ndata (Gray & Singer, 1989). Gestalt principles, such as proximity, similarity, and closure, highlight the\nbrain’s innate tendency to organize sensory information into coherent and structured wholes rather than\nprocess isolated parts (Todorovic, 2008). Synchrony can therefore be viewed as a mechanism that induces the\ngrouping and integration of sensory elements based on these Gestalt principles (Gray et al., 1989).\nIn summary, the binding by synchrony theory suggests that synchronized neural activity is crucial for binding\ndistributed sensory responses. In this paper, we use complex-valued representations to mimic this synchrony\nmechanism and the Kuramoto dynamic to support Gestalt principles of proximity and similarity. In our model,\nwe assume an oscillation at a single frequency (comparatively to the brain which can exhibit oscillations at\ndifferent frequency bands) for simplicity purposes. The phase of the complex-valued neuron will, therefore,\nrepresent the phase with respect to an ongoing oscillation, and a group of neurons sharing the same phase\nvalue will be akin to a synchronized population.\n4\nKomplexNet: Kuramoto synchronized complex-valued network\nIn the following, we describe the details of KomplexNet’s implementation. We use a Kuramoto system at the\nfirst layer L0 to synchronize the phases and apply operations in the complex domain for subsequent layers,\nand we finish with the addition of feedback connections affecting the phase synchrony.\n4.1\nKuramoto dynamic\nKuramoto model.\nInstead of considering synchrony as an emergent phenomenon, we choose to induce it\nusing a Kuramoto system (Kuramoto, 1975) to organize the phases of the first layer. This process serves to\nsynchronize the phases before incorporating them into the activity of the network. As we are dealing with\ncomplex-valued activity, we also need an amplitude value for each neuron at the first layer. This information\nis obtained by applying a real-valued convolution to the input image. To reach a synchronized state, we adapt\nthe original equation of the Kuramoto model (Equation 3 in Kuramoto (1975)) and propose the dynamic\ndescribed in Equation 1.\n˙θcij = η × [\nC×H×W\nX\nk=0\n(rk,cij −ϵ) · sin(θk(t) −θcij(t)) · tanh(ak)]\n(1)\nThe core idea of the Kuramoto model is to synchronize a population of oscillators by mutual influence. In\nour case, the oscillators are the phases θ ∈RC×H×W (with H and W denoting the size of the input image),\nwhere a phase θcij will be influenced through the sine of the difference between itself and the rest of the\npopulation. This influence will be modulated by a learnable coupling kernel r ∈RC×C×h×w (with h ≤H and\nw ≤W) together with a global desynchronization interaction ϵ ∈R1, as well as the amplitude, a, associated\nto each influencing phase. In other words, each phase synchronizes with its neighbors (defined by the spatial\nrange of the kernel) and desynchronizes with phases further apart. At the population level, the system favors\nthe emergence of several clusters of phases. Lastly, η ∈R1 acts as a gain parameter, modulating the phase\nupdate at each timestep.\nLearning the coupling kernel.\nThe coupling kernel is expected to capture the interactions and mutual\ninfluence between phases. A positive/negative value in this kernel means that the corresponding two neurons\n4\n\n\nFigure 2: Overview of KomplexNet. We show the global architecture on the left and illustrate the phase\ndynamic on the right. The phases start with a random initialization and evolve with time according to\nKuramoto’s equation. The first complex representation results from the amplitudes and the phases of the\nfirst layer (L0 in blue). The plain lines on the right represent the local connections inside the layer. The\ndashed lines represent some possible top-down connections to control synchronization. At every timestep, the\nphase of individual neurons gets updated by one Kuramoto iteration and propagated to the next network\nlayers via complex-valued operations.\nwill tend to synchronize/desynchronize their phases. The kernel should learn the inherent structure of\nthe objects in the dataset to adjust the interactions between nearby phases. For example, in the case of\nhandwritten digits, the objects are mostly vertical, hence the kernel should favor positive interactions between\nphases along the vertical axis. Here, we initialize the kernel with a 2D Gaussian to encourage interactions\nwith closer neighbors.\nTo learn this kernel, we use the cluster synchrony loss defined in Ricci et al. (2021):\nCSLoss(θ) = 1\n2( 1\nG\nG\nX\nl=1\nVl(θ) + 1\n2G\n\f\f\f\nG\nX\nl=1\nei⟨θ⟩l\n\f\f\f\n2\n)\n(2)\nwhere V (θ) is the circular variance, ⟨θ⟩the average of a phase group, and G the number of groups. The\nfirst part of this loss measures the intra-cluster synchrony while the second part represents the inter-cluster\ndesynchrony. Minimizing the loss resolves to minimize the variance inside groups (each phase cluster should\nhave the same value) and minimize the proximity between the centroids of the clusters on the unitary circle\n(the clusters should cancel each other out).\nIn Figure 3, we show some visualization of the phases obtained after 15 steps of our Kuramoto model on\nthe Multi-MNIST dataset (Sabour et al., 2017) (see also Fig. 9 to visualize the learned kernel). The plot\nrepresents the 8 convolution channels, masked by the intensity of the amplitude, with the color indicating\nthe phase value. The phases from the same digits are synchronized, and the two clusters of phases are\n5\n\n\ndesynchronized with an almost opposite position on the unitary circle. Interestingly, the model does not learn\nto systematically affect one specific phase value to a class of digits (as observable in Figure 3 with two ’9’\nrepresented by distinct colors/phase values in the first two images). Indeed, in the binding by synchrony\ntheory, there is no requirement that a given object should always and systematically be assigned the same\nphase value, as this would seriously limit the flexibility and adaptability of the coding system.\nFigure 3: Phase synchronization. Given the input image shown at the top, we present visualizations\nof the phases from the first layer (across each of the 8 convolution channels) at the last timestep of the\nKuramoto dynamic. The color represents the complex phase and we use the complex amplitude to mask out\nthe background/non-active pixels.\n4.2\nComplex representations in an artificial neural network\nOverall architecture.\nOur model, KomplexNet combines the Kuramoto dynamic from Section 4.1 and\nadditional complex operations described below, as represented in Figure 2 and Algorithm 1. To summarize, we\nstart by extracting features from the input images by performing a real non-strided convolution (8 channels).\nThe initial complex activity comprises these features along with random phases to perform one step of\nour Kuramoto dynamic. The resulting activation, zL0 ∈C8×32×32, is propagated in a bottom-up manner\nthrough one strided complex-convolution (zL1 ∈C8×16×16) and two linear layers (respectively zL2 ∈C50 and\nzL4 ∈C10). This is repeated for several timesteps to allow the phases to reach a stable synchronized state\nthrough the Kuramoto dynamic.\n6\n\n\nEach step of the Kuramoto model outputs a new state of the phases, combined with the amplitude extracted\nvia the first real convolution to instantiate the complex activity.\nComplex operations.\nWe first redefine the standard operations of a convolutional network to be compatible\nwith complex activations z = mz.eiθz ∈C. We apply some biologically plausible transformations that allow\nthe model to perform well, as used in (Reichert & Serre, 2013; Löwe et al., 2022; Stanić et al., 2023). We\ndefine the linear operations (convolutions and dense layers) as such:\nz1 = fw(z) = fw(Re(z)) + fw(Im(z)).i ∈C\n(3)\nThe newly obtained activity results from applying real weights to both the real and imaginary parts of the\ninput activity, modifying the amplitude and the phase jointly. Then, as first proposed by (Reichert & Serre,\n2013), we apply the classic term, representing a gating mechanism that selectively weakens out-of-phase inputs,\npreventing inhibition (caused by a negative weight) from leading to the same result as desynchronization\n(phases in the opposite direction):\nχ = fw(|z|)\nmz2 = 1\n2(mz1 + χ)\n(4)\nLastly, we apply a ReLU non-linearity only on the amplitude. Considering that a complex amplitude is by\ndefinition positive, we start by normalizing it before applying the desired function, as done by (Löwe et al.,\n2022; Stanić et al., 2023). This last step ends a block of operations representing one layer in our model.\nz3 = ReLU(InstanceNorm(mz2)).eiθz1 ∈C\n(5)\nWe can additionally observe, in Figure 4 the effect of the complex operations by visualizing the phase\ndistribution at each layer of the model at the last timestep. The first row represents the polar distribution of\nthe color-coded phases at each layer. We can see that the Kuramoto model allowed the phases to reach a\nstate with two opposite clusters – one for each digit – and this distribution is conserved in all the subsequent\nlayers. We show in the second row the same activity but detailing the spatial information provided by the\nconvolutions of the first and second layers.\n4.3\nImplementing feedback\nWe finally propose an extension of KomplexNet by implementing feedback connections to influence the phase\nsynchronization (dashed lines on the right in Figure 2). At each timestep, the higher-level representations\nfrom the latter layers (carrying information about advanced features, object parts, and object classes) can\nhelp the synchronization process of the first layer. We combine the lateral synchronization Kuramoto dynamic\nof L0 with feedback synchronization from higher layers. The resulting phase update per timestep at the first\nlayer is described analytically in Equation 6. The first line defines Kcij as the lateral synchrony in L0 (same\nas Equation 1) and the second combines Kcij with the sum of the Kuramoto dynamics across layers.\nKcij = η × [\nC×H×W\nX\nk=0\n(rk,cij −ϵ) · sin(θk(t) −θcij(t)) · tanh(ak)]\n˙θcij = Kcij +\nNl\nX\nl=1\n[η × [\nCl×Dl\nX\nk=0\n(rlk,cij) · sin(θlk(t) −θcij(t)) · tanh(alk)]]\n(6)\nWe instantiate the activity of each layer at the first timestep through a feedforward propagation and introduce\nthe feedback starting at the second step. Algorithm 2 details the corresponding update in the dynamic of\nthe whole model. Similarly to the lateral coupling kernel of layer L0, the feedback coupling kernel coming\nfrom L1 is defined in RC×C×h1×w1. However, the coupling matrices from the dense layers are defined in\n7\n\n\nFigure 4: Phases per layer. Given an input image (top left corner), we show how the complex operations\npropagate the phase grouping instantiated at the first layer to the last decision layer (all activations are\nmeasured at the last step of the Kuramoto dynamic). As the first two layers (L0 and L1) are convolutional,\nwe additionally visualize the eight convolutional channels in each layer (bottom).\nRC×Dl with Dl representing the number of neurons in each layer. More specifically, the convolutional layer\nL1 still provides spatially structured information to the phases of the first layer L0. Conversely, the phases of\nthe dense layers affect all the phases of L0 without spatial structure but provide generic information about\nthe identity of the objects. The feedback couplings in the Kuramoto equation do not comprise the global\nϵ desynchronization term (unwarranted since the latter layers are not spatially structured). However, we\ninitialize all the feedback kernels around 0 (before training) to facilitate reaching negative coupling values,\nthus desynchronizing certain phases of L0 with phases from higher layers when they do not encode for the\nsame object.\n4.4\nExperimental setup\nDatasets.\nWe perform our experiments on two different datasets. The first one is the Multi-MNIST dataset\n(Sabour et al., 2017), consisting of images containing two hand-written digits taken from the MNIST dataset.\nMore specifically, we generate empty (black) images of size 32 × 32, downsample the MNIST images by a\nfixed factor (depending on the number of digits we want to fit in the image), and place the first digit at a\nrandom location around the upper-left corner. We then randomly pick a distinct second digit and assign\nit a position in the image under two constraints: not surpassing a predefined maximum amount of overlap,\nwhile still fully appearing in the image (none of the digits are cut by the image border). We generate a\nnon-overlapping version of the dataset (maximum overlap = 0%) and an overlapping version where the digits\ncan overlap up to 25% of their active pixels. We use the same procedure to generate images containing more\nthan two digits. Similarly, we generate a version of this dataset with greyscaled CIFAR10 (Krizhevsky et al.,\n2009) images in the background. This makes the digits less easy to separate for the Kuramoto model and\nrepresents a more ecological setting to test our models.\nFor both of these datasets, the models are evaluated on their ability to recognize and classify the two digits,\nout of 10 different possible classes. When generating the images, we also generate an associated mask tagging\nthe different objects: first digit and second digit (the same logic potentially extends to a higher number of\ndigits). When the digits overlap, the overlapping region is considered as an additional object. The background\n8\n\n\nis not considered as an object. These masks are used to compute the cluster synchrony loss (Equation 2) and\ndo not provide information about the identity (label) of the digits.\nBaselines.\nWe compare both versions of our model (KomplexNet and KomplexNet with feedback) with\ndifferent baselines to highlight our contributions: a real model (with an architecture and a number of\nparameters equivalent to KomplexNets) and a complex model without the Kuramoto synchrony (random\nphases at the first layer). We additionally show the performance of a complex model with an ideal phase\nseparation as an upper baseline: for this model, using the ground-truth masks, we assign the phases by\nrandomly sampling N equidistant groups on the unitary circle and affecting each value to one object in the\nimage (with N the number of objects, not including the background). When digits overlap, we affect an\nintermediate phase value (circular mean of the two-digit values) to the overlapping pixels.\nModel and training.\nWe train each family of models end-to-end using Adam (Kingma & Ba, 2014), a\nfixed learning rate of 1e-3, and a batch size of 128 or 32 depending on the dataset. KomplexNets are trained\nby accumulating the binary cross-entropy loss at each timestep and then combining it with the synchrony\nloss from the last timestep. The balance between the two quantities is modulated by a hyperparameter, as\nillustrated in Equation 7. All experiments are implemented in Pytorch 1.13 (Paszke et al., 2017) and run on a\nsingle NVIDIA V100. Each curve in the following plot represents the average and standard deviation over 50\nruns with different random initializations. Complementarily, we present in the appendix the test accuracy of\nthe best models on the validation set. To obtain the best hyper-parameter values, we run a hyper-parameter\nsearch and use the best combination of values out of 100 simulations. The concerned parameters are: the\ndesynchronization term ϵ, the gain parameters ηl for each layer l, the coupling kernel sizes kl0 and kl1, and\nthe balance of the losses τ.\nL(ˆy, y, θ) =\nT\nX\nt=0\nBCELoss(ˆyt, y) + τ.CSLoss(θT )\n(7)\nEvaluation.\nAs we are optimizing two different losses (cluster synchrony and classification), we systemati-\ncally quantify the performance of our models compared to the baselines along those two separate axes. In all\nthe following sections, we show the first loss under the synchrony label and use the performance label for\nthe classification objective. The models are all trained with two or three non-overlapping digits. In the next\nsections, we present the results on in-distribution images (two-digit, non-overlapping images, all different\nfrom the training set) and then evaluate the robustness and generalization abilities of the models (without\nre-training or fine-tuning). We define here robustness as the model’s ability to perform the same task given\nan altered image (compared to the training distribution), while generalization represents the capability of the\nmodel to perform a slightly different task from the one it was trained on (here, the categorization of more or\nless digits than during training).\n5\nResults\n5.1\nIn-distribution performance\nSynchrony.\nWe start by evaluating the ability of the Kuramoto model to correctly separate the objects,\ncompared to the other complex baselines (we cannot explicitly evaluate object separation for the real-valued\nbaseline, as this requires phase information). In Figure 5 panel A, we quantify the quality of the solutions\nprovided by the four models using Equation 2, with the random case as a lower baseline and the ideal\ncase as an upper baseline; we see that the solutions obtained with Kuramoto (and especially with feedback\nconnections) converge over time towards the ideal value. In panel C, we show a qualitative example of\nphase synchrony for an image taken from the test set (illustrated in panel A). As expected, when the digits\nare spatially separated on a clean image, the phases obtained with the Kuramoto model almost perfectly\nsynchronize inside the digits and desynchronize between digits: the two clusters have opposite values on\nthe unitary circle (polar plots in the bottom row) and all the pixels within one digit have similar phase\nvalues (phase maps across the 8 convolution channels, middle row). In this image, the area where the\n9\n\n\ndigits are close and almost touching is assigned a phase value in-between the two clusters; this seems a\nreasonable solution because the coupling kernel around this region will tend to synchronize the phases across\nthe two digits. A smaller coupling kernel would avoid this problem, but it would then take more timesteps to\nreach a synchronized state. Visually, the obtained phase maps with KomplexNets seem to lie between the\nrandom-phase model (left sub-panel) and the ideal-phase model (right sub-panel) where the clusters show no\nvariance even in ambiguous regions of the image. KomplexNet with feedback shows slightly less dispersed\nclusters than the feedforward KomplexNet model (middle two sub-panels).\nFigure 5: In distribution results. Panel A shows the evolution of the cluster synchrony loss through time\n(computed on the whole test set; lower values indicate better phase separation across digits). Panel B contains\nthe classification performance of KomplexNets compared to the same baselines, as well as the real-valued\nmodel. Panel C represents the phases of KomplexNets (red, and KomplexNet with feedback in purple), at the\nlast timestep, compared to the two complex baselines (random phases and ideal phases) on in-distribution\nimages. We show an example of an image (in panel A), the phases of the 8 output channels of the first layer\n(center sub-panels), and the polar distribution of all the phases - 8 channels combined (bottom sub-panels).\nPerformance.\nWe then evaluate the effect of synchrony on the performance of the model. At each Kuramoto\nstep, we evaluate the KomplexNets and obtain an accuracy curve over time. Conversely, the baselines are\ndeprived of temporal dynamics and therefore yield only one accuracy value. The results are reported in\nFigure 5, panel B. We can observe that the KomplexNets’ performance starts below the real-model and\nrandom-phase baselines (because the phases are not synchronized yet) and out-perform them after 5 timesteps\n(for KomplexNet with feedback) or 7 timesteps (for KomplexNet without feedback). Surprisingly, after about\n10 timesteps, KomplexNet reaches a performance on par with the ideal phase model, while KomplexNet with\nfeedback even outperforms it. This observation sheds light on the phase-synchronization strategy we define\nas “ideal” here. Because the phase initialization takes place after a convolution, the resulting activity is\nspread over a slightly larger extent than the \"ideal\" mask (based on active image pixels). Because of this\ndiscrepancy, some activated neurons around the digits are not affected by the phase initialization process,\nintroducing noise in the phase information that is later sent to the rest of the network. Conversely, the\nKuramoto dynamic acts on all active neurons, potentially leading to more faithful phase information in the\nin-distribution case.\n10\n\n\nWe provide in Appendix Figure 10 additional experiments testing the models on more timesteps than during\ntraining; the results indicate that the synchronized Kuramoto state is stable over time, and the associated\nperformance improvements persist. We also test in Appendix Figure 12 the effect of the feedback coming\nfrom each layer (L1, L2, L3) separately; every single layer provides an amelioration, but the model with\nfeedback from all layers combined shows the greatest performance.\n5.2\nRobustness\nWe then evaluate the robustness of our trained models on out-of-distribution images. The task remains two-\ndigit classification, and we evaluate both objectives (synchrony and performance) on images with overlapping\ndigits or with additive Gaussian noise.\nSynchrony.\nSimilarly to the previous section, we observe the cluster synchrony loss of KomplexNet with\nfeedback reaching a lower (i.e. better) score compared to KomplexNet (see Figure 6, first row), for both\nthe “overlap” and the “noise” conditions. In both cases but not surprisingly, the gap with the ideal phases\nremains higher than before, since the ground-truth phase masks (used for the “ideal-phase” model) are not\naffected by noise or by digit overlap (when digits overlap, pixels from the overlapping region are considered as\na separate group in the ground-truth mask, but this group is not included in the synchrony loss computation).\nWe provide a visualization of the phases for one image (the example shown in Figure 6) in the Appendix\nFigure 14.\nFigure 6: Robustness performance. We report the average\nperformance of KomplexNet (red) and KomplexNet with feedback\n(purple) over time along with the standard deviation for 50 repe-\ntitions. We compare it with its real-valued counterpart (blue), a\ncomplex model with random phase initialization (green), and the\nideal phase cluster synchrony (orange). The models are tested on\noverlapping digits (left column) and noisy images (right column).\nPerformance.\nKomplexNet and Kom-\nplexNet with feedback show more robust-\nness than the baselines, outperforming the\nreal model in accuracy by 10 to 15% (Fig-\nure 6, second row). The real model and\nthe random-phases complex model are\nvery affected by the perturbations, while\nthe ideal-phase complex model shows less\naltered performance. The accuracy val-\nues of KomplexNets lie between the upper\nbaseline and the two other models and sur-\npass them after fewer timesteps than in\nthe previous case (Figure 5), showing how\nsynchronized phases help resolve ambigu-\nous cases. More specifically, KomplexNet\nwith feedback remains more robust than\nKomplexNet, motivating the use of feed-\nback connections to improve phase syn-\nchronization.\n5.3\nGeneralization\nTo evaluate our models’ generalization\nabilities, we report in this section their\nsynchrony and performance when trained\non either two or three digits in the images\nand then tested on the same or a different\nnumber of digits, from two to nine digits.\nSynchrony\nFigure 7 illustrates the gen-\neralization abilities of the KomplexNets\nat synchronizing the phases in this out-of-\ndistribution setting. Interestingly, despite\n11\n\n\nnot having seen 3 digits during training,\nthe coupling kernel of the model trained\non two digits can create a third cluster, equidistant to the others on the unitary circle, and correctly affect a\nsingle phase value per digit. Likewise, the model trained on three digits adapts well to the two-digit case:\nthe model doesn’t create a third phase cluster but only shows higher variance in the two opposite clusters\ncompared to the model trained on two digits.\nThe evolution of the cluster-synchrony losses illustrates well the general case: losses for both KomplexNets\nstart around the random-phase case and end close to the ideal-phase case. Interestingly, for a given test\nsetting, no matter the training setting (same or different digit number), the models reach approximately\nthe same synchrony values at test time. This observation highlights an additional form of robustness of\nour models and suggests an object representation ability not present in non-complex and non-synchronized\n(random-phase) complex models.\nFigure 7: Generalization to more or less digits. We show here the generalization ability of KomplexNets\ntrained on two or three digits and tested on two or three digits, evaluated on the synchrony objective. On each\npanel, we present visualizations of the phases of KomplexNet with feedback on one representative example,\nat the last timestep, a polar and spatial representation to observe the distribution of the phases and their\nlink with the objects, and the evolution of the cluster synchrony loss through time (over the entire test set),\nin comparison with the value of the two baselines.\nPerformance\nIn the same way, we report the classification accuracy of the models and their baselines\nwhen trained on two or three digits and tested on two or three digits (Figure 8, panel A). Consistent with\nthe previous results, both KomplexNets outperform the real and random-phases model baselines, both when\ntesting in-distribution and out-of-distribution. More interestingly, both versions of KomplexNet reach almost\nthe same performance on each given test set (within ±3%), no matter the number of digits seen during\ntraining. Conversely, the baselines (real and random-phase models) suffer more from this change, leading to a\nvery consequent gap in performance (up to 10%) on the off-diagonal (out-of-distribution testing) plots.\nGiven this success in generalizing the classification task to one more or one less digit compared to the training\nset, we next evaluate the maximum number of objects that can be handled by KomplexNets. Consequently, we\nreport the performance of all the models on two to nine digits in the image. In Appendix Figure 13, we report\nthe absolute performance of the models at the last timesteps for each test set (one test set corresponding\nto a fixed number of digits in the images). As could be expected, performance decreases rapidly as the\nnumber of simultaneous objects to classify grows from two or three digits (the number that the models were\n12\n\n\nFigure 8: Performance on generalization. We report the average performance of KomplexNet (red) and\nKomplexNet with feedback (purple) over time along with the standard deviation for 50 repetitions. We\ncompare it with its real-valued counterpart (blue), a complex model with random phase initialization (green)\nas well as a complex model with an ideal phase initialization (orange). The models are trained to classify two\nor three digits and tested on two or three digits (panel A), and up to nine digits (panels B and C). Panel A\nshows the classification performance per timestep. Panel B and panel C respectively show the difference in\nperformance between KomplexNet and KomplexNet with feedback and the baseline at the last timesteps\nwhen tested on different numbers of digits.\ntrained on) up to 9 digits. However, performance drops more quickly for the baselines (real and random-phase\nmodels) than for KomplexNets. As the exact advantage of KomplexNets is hard to quantify from this plot,\nwe also report in Figure 8 the difference in performance between all the models and KomplexNet (panel\nB), and all the models and KomplexNet with feedback (panel C), when trained on either two (first column)\nor three digits (second column). On these plots, zero difference means that the tested model reaches the\nsame performance as KomplexNet (for panel B, or KomplexNet with feedback for panel C), while a negative\ndifference means that the tested model performed worse than KomplexNet (and conversely). In panel B, we\nobserve a negative difference between KomplexNet and the baselines (except KomplexNet with feedback),\npersisting up to 8 digits with a peak around 3 to 5 digits. Similarly, KomplexNet with feedback (panel C)\nsystematically outperforms all the other models from two to 8 digits. The nine-digit test set is very hard for\nall the models because it is the furthest one from the in-distribution case and, as observable in Figure 13,\naccuracy is very low.\nOverall, these results reveal that phase synchronization makes the models more robust and general by\nrendering them less sensitive to out-of-distribution shifts.\n5.4\nAdditional experiments\nExperiments on non-uniform background.\nTo demonstrate that our method is not restricted to images\nwith empty backgrounds, we train our models and the baselines on a version of the multi-MNIST dataset\nwith randomly drawn CIFAR images in the background. The results are presented in Figure 15. Because the\nmeaningful information contained in the image is more difficult to extract, both synchrony and performance\nmeasures are affected for all the models. However, KomplexNet and especially KomplexNet with feedback\nstill clearly outperform the baselines. This additional experiment highlights the robustness of the Kuramoto\nmodel: the difficulty is in ignoring noisy information in the background, therefore relying less on proximity\nand more on similarity principles. For this reason, the cluster synchrony loss of KomplexNet is very altered\n13\n\n\nand far from the ideal case, but the feedback connections of KomplexNet with feedback help to bridge the\ngap.\nLeveraging the temporal dynamic from temporal inputs.\nFinally, we evaluate the advantage of the\nKuramoto dynamic on dynamic inputs. More specifically, we investigate whether the temporal dimension of\nthe Kuramoto dynamic can act as a memory mechanism when the input is transformed from static images to\nvideos (with digits moving across the frames). We show in Figure 16 the accuracy per timestep relative to the\nframe with the maximum amount of overlap (denoted timestep 0). Compared to the real baseline, as well as\nKomplexNets tested on each frame separately, we observe a significant increase in accuracy when the models\nare tested on the moving object test set. These results suggest that the phase information coming from the\nprevious frames (where the digits overlapped less) is maintained, leading to better performance. Overall, it\nconfirms the hypothesis that Kuramoto can act as a system with memory, able to use information from the\nprevious timesteps to create a more qualitative phase separation than a system deprived of such context.\n6\nConclusion\n6.1\nSummary\nHere, we propose a model combining complex-valued activations with a Kuramoto phase-synchronization\ndynamic modeling of the binding-by-synchrony theory in neuroscience.\nA complex-valued neuron can\nsimultaneously indicate the presence of a feature by its activation amplitude (just as in standard real-valued\nneural networks), and tag the group or object to which this feature belongs by its activation phase. The\nKuramoto model serves as a synchronization process, where the coupling kernels implement Gestalt principles\nof proximity and similarity and act as an inductive bias. We show that our model outperforms its real\ncounterpart as well as a complex-valued model with random phases on multi-object recognition. More\ninterestingly, the phases play an important role in introducing some notions of object representation in the\nmodels, making them more robust to ambiguous cases such as overlapping digits, noisy images, more digits\nin the images, etc.\nWe additionally propose a way to introduce feedback connections in the model, acting on the phases by using\ninformation from higher layers to enhance synchrony. As the phase information becomes more reliable (better\ncluster synchrony score) with the feedback extension, the performance, robustness, and generalization of the\nmodel increase, highlighting the added value of good phase synchronization for multi-digit classification.\n6.2\nLimitations and discussion\nModel Depth and Scalability\nThe models studied in this work are relatively shallow. We intentionally\ndesigned a limited architecture to restrict feature representation abilities and highlight the benefits of\nsynchrony in such a setting. While this serves as a proof of concept, scaling up our approach requires\nidentifying tasks and datasets where state-of-the-art models struggle with feature binding in multi-object\nscenarios. Given the computational demands of training large models on extensive datasets, we opted for a\nsmaller-scale demonstration.\nHyper-Parameter Dependence\nThe Kuramoto dynamics introduced in our model rely on several\nhyper-parameters, including ϵ, ηl for each layer l, the coupling kernel sizes kl0 and kl1, and the loss balance\nparameter τ. Finding optimal values for these parameters requires hyper-parameter tuning to ensure phase\nsynchronization. However, our experiments indicate that these values remain consistent across different tasks\n(see Table 1), suggesting that hyper-parameter tuning is only necessary when changing datasets.\nExplicit vs. Emergent Synchronization\nA potential limitation of our approach is the explicit addition\nof Kuramoto dynamics, in contrast to prior work where synchrony emerges through training (Löwe et al.,\n2022; Stanić et al., 2023). As noted by Stanić et al. (2023), the Complex Autoencoder (CAE) (Löwe et al.,\n2022) can achieve phase synchronization, but only on simple datasets with a limited number of objects. While\nStanić et al. (2023) proposed a method to scale this behavior, they required an additional objective to achieve\nsynchronization on more complex datasets. The emergence of synchrony at scale for complex visual scenes\n14\n\n\nremains an open and non-trivial challenge. Our approach circumvents this difficulty by directly incorporating\nsynchrony via Kuramoto dynamics, providing a controlled framework to assess its benefits. This serves as a\nproof of concept, demonstrating the advantages of complex-valued representations and synchronized activity\nin visual tasks. Furthermore, our approach aligns with experimental findings in neuroscience (Fries et al.,\n1997), emphasizing the role of synchrony in early visual processing.\nNeuroscientific Relevance\nOur study is primarily motivated by the binding by synchrony theory, a\nconcept widely discussed in neuroscience. While this theory remains influential, it has faced experimental\nchallenges (Roelfsema, 2023; Shadlen & Movshon, 1999). However, recent evidence supports the functional\nimportance of synchronized activity (Fries, 2023). Our work does not aim to provide new insights into\nbiological vision but instead proposes synchrony as a viable solution for feature binding in artificial models.\nBy leveraging neuroscientific principles, we offer an alternative computational mechanism that may inspire\nfuture advancements in deep learning architectures.\n7\nAcknowledgment\nOur work is supported by ONR (N00014-24-1-2026), NSF (IIS-2402875) to T.S. and ERC (ERC Advanced\nGLOW No. 101096017) to R.V., as well as “OSCI-DEEP” [Joint Collaborative Research in Computational\nNeuroScience (CRCNS) Agence Nationale Recherche-National Science Fondation (ANR-NSF) Grant to R.V.\n(ANR-19-NEUC-0004) and T.S. (IIS-1912280)], and the ANR-3IA Artificial and Natural Intelligence Toulouse\nInstitute (ANR-19-PI3A-0004) to R.V. and T.S. Additional support was provided by the Carney Institute for\nBrain Science and the Center for Computation and Visualization (CCV). We acknowledge the Cloud TPU\nhardware resources that Google made available via the TensorFlow Research Cloud (TFRC) program as well\nas computing hardware supported by NIH Office of the Director grant S10OD025181.\n15\n\n\nReferences\nJoshua Bassey, Lijun Qian, and Xianfang Li. A survey of complex-valued neural networks. arXiv preprint\narXiv:2101.12249, 2021.\nMarlene Behrmann, Richard S Zemel, and Michael C Mozer. Object-based attention and occlusion: evidence\nfrom normal participants and a computational model. Journal of Experimental Psychology: Human\nPerception and Performance, 24(4):1011, 1998.\nMichael Breakspear, Stewart Heitmann, and Andreas Daffertshofer. Generative models of cortical oscillations:\nneurobiological implications of the kuramoto model. Frontiers in human neuroscience, 4:190, 2010.\nKanishk Chauhan, Ali Khaledi-Nasab, Alexander B Neiman, and Peter A Tass. Dynamics of phase oscillator\nnetworks with synaptic weight and structural plasticity. Scientific Reports, 12(1):15003, 2022.\nMeng-Jiun Chiou. Learning Structured Representations of Visual Scenes. PhD thesis, National University of\nSingapore (Singapore), 2022.\nAndrea Dittadi. On the generalization of learned structured representations. arXiv preprint arXiv:2304.13001,\n2023.\nPascal Fries. Rhythmic attentional scanning. Neuron, 111(7):954–970, 2023.\nPascal Fries, Pieter R Roelfsema, Andreas K Engel, Peter König, and Wolf Singer. Synchronization of\noscillatory responses in visual cortex correlates with perception in interocular rivalry. Proceedings of the\nNational Academy of Sciences, 94(23):12699–12704, 1997.\nPascal Fries, Jan-Hinrich Schröder, Pieter R Roelfsema, Wolf Singer, and Andreas K Engel. Oscillatory neu-\nronal synchronization in primary visual cortex as a correlate of stimulus selection. Journal of Neuroscience,\n22(9):3739–3754, 2002.\nAnand Gopalakrishnan, Aleksandar Stanić, Jürgen Schmidhuber, and Michael Curtis Mozer. Recurrent\ncomplex-weighted autoencoders for unsupervised object discovery. arXiv preprint arXiv:2405.17283, 2024.\nCharles M Gray and Wolf Singer. Stimulus-specific neuronal oscillations in orientation columns of cat visual\ncortex. Proceedings of the National Academy of Sciences, 86(5):1698–1702, 1989.\nCharles M Gray, Peter König, Andreas K Engel, and Wolf Singer. Oscillatory responses in cat visual cortex\nexhibit inter-columnar synchronization which reflects global stimulus properties. Nature, 338(6213):334–337,\n1989.\nKlaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. On the binding problem in artificial neural\nnetworks. arXiv preprint arXiv:2012.05208, 2020.\nStephen Grossberg. Adaptive pattern classification and universal recoding: Ii. feedback, expectation, olfaction,\nillusions. Biological cybernetics, 23(4):187–202, 1976.\nDiederik P Kingma and Jimmy Ba.\nAdam:\nA method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nYoshiki Kuramoto. Self-entrainment of a population of coupled non-linear oscillators. In International\nSymposium on Mathematical Problems in Theoretical Physics: January 23–29, 1975, Kyoto University,\nKyoto/Japan, pp. 420–422. Springer, 1975.\nPhuc H Le Khac. Toward efficient learning of structured representations in computer vision. PhD thesis,\nDublin City University, 2024.\nSindy Löwe, Phillip Lippe, Maja Rudolph, and Max Welling. Complex-valued autoencoders for object\ndiscovery. arXiv preprint arXiv:2204.02075, 2022.\n16\n\n\nPeter M Milner. A model for visual shape recognition. Psychological review, 81(6):521, 1974.\nTakeru Miyato, Sindy Löwe, Andreas Geiger, and Max Welling. Artificial kuramoto oscillatory neurons.\narXiv preprint arXiv:2410.13821, 2024.\nNils Moenning and Suresh Manandhar. Complex-and real-valued neural network architectures. 2018.\nGéza Ódor and Jeffrey Kelling. Critical synchronization dynamics of the kuramoto model on connectome and\nsmall world graphs. Scientific reports, 9(1):19621, 2019.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\nA Ravishankar Rao and Guillermo A Cecchi. An objective function utilizing complex sparsity for efficient\nsegmentation in multi-layer oscillatory networks. International Journal of Intelligent Computing and\nCybernetics, 3(2):173–206, 2010.\nA Ravishankar Rao and Guillermo A Cecchi. The effects of feedback and lateral connections on perceptual\nprocessing: A study using oscillatory networks. In The 2011 international joint conference on neural\nnetworks, pp. 1177–1184. IEEE, 2011.\nA Ravishankar Rao, Guillermo A Cecchi, Charles C Peck, and James R Kozloski. Unsupervised segmentation\nwith dynamical units. IEEE Transactions on Neural Networks, 19(1):168–182, 2008.\nDavid P Reichert and Thomas Serre. Neuronal synchrony in complex-valued deep networks. arXiv preprint\narXiv:1312.6115, 2013.\nMatthew Ricci, Minju Jung, Yuwei Zhang, Mathieu Chalvidal, Aneri Soni, and Thomas Serre. Kuranet:\nsystems of coupled oscillators that learn to synchronize. arXiv preprint arXiv:2105.02838, 2021.\nFrancisco A Rodrigues, Thomas K DM Peron, Peng Ji, and Jürgen Kurths. The kuramoto model in complex\nnetworks. Physics Reports, 610:1–98, 2016.\nPieter R Roelfsema. Solving the binding problem: Assemblies form when neurons enhance their firing\nrate—they don’t need to oscillate or synchronize. Neuron, 111(7):1003–1019, 2023.\nAdina L Roskies. The binding problem. Neuron, 24(1):7–9, 1999.\nSara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. Advances in neural\ninformation processing systems, 30, 2017.\nLukas Schott, Julius Von Kügelgen, Frederik Träuble, Peter Gehler, Chris Russell, Matthias Bethge, Bernhard\nSchölkopf, Francesco Locatello, and Wieland Brendel. Visual representation learning does not generalize\nstrongly within the same domain. arXiv preprint arXiv:2107.08221, 2021.\nMichael N Shadlen and J Anthony Movshon. Synchrony unbound: a critical evaluation of the temporal\nbinding hypothesis. Neuron, 24(1):67–77, 1999.\nWolf Singer. Binding by synchrony. Scholarpedia, 2(12):1657, 2007.\nAleksandar Stanić, Anand Gopalakrishnan, Kazuki Irie, and Jürgen Schmidhuber. Contrastive training of\ncomplex-valued autoencoders for object discovery. arXiv preprint arXiv:2305.15001, 2023.\nD. Todorovic. Gestalt principles. Scholarpedia, 3(12):5345, 2008.\nChiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, João Felipe Santos,\nSoroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep complex networks (2017).\narXiv preprint arXiv:1705.09792, 2017.\nAnne Treisman. The binding problem. Current opinion in neurobiology, 6(2):171–178, 1996.\n17\n\n\nPeter Uhlhaas, Gordon Pipa, Bruss Lima, Lucia Melloni, Sergio Neuenschwander, Danko Nikolić, and Wolf\nSinger. Neural synchrony in cortical networks: history, concept and current status. Frontiers in integrative\nneuroscience, 3:543, 2009.\nChristoph von der Malsburg. The correlation theory of brain function (internal report 81-2). Goettingen:\nDepartment of Neurobiology, Max Planck Intitute for Biophysical Chemistry, 1981.\nCornelius Weber and Stefan Wermter. Image segmentation by complex-valued units. In Artificial Neural\nNetworks: Biological Inspirations–ICANN 2005: 15th International Conference, Warsaw, Poland, September\n11-15, 2005. Proceedings, Part I 15, pp. 519–524. Springer, 2005.\nMax Wertheimer. Laws of organization in perceptual forms. 1938.\nSaurabh Yadav and Koteswar Rao Jerripothula. Fccns: Fully complex-valued convolutional networks using\ncomplex-valued color model and loss function. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 10689–10698, 2023.\nRichard S Zemel, Christopher KI Williams, and Michael C Mozer. Lending direction to neural networks.\nNeural Networks, 8(4):503–512, 1995.\nYangmuzi Zhang, Zhuolin Jiang, and Larry S Davis. Learning structured low-rank representations for image\nclassification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n676–683, 2013.\nHao Zheng, Hui Lin, Rong Zhao, and Luping Shi. Dance of snn and ann: Solving binding problem by\ncombining spike timing and reconstructive attention. Advances in Neural Information Processing Systems,\n35:31430–31443, 2022.\nA\nAppendix\nA.1\nAlgorithms\nWe detail here the different steps of both versions of KomplexNet using the pseudo-code algorithm. Algorithm\n1 describes the operations of KomplexNet and Algorithm 2 specifies how feedback connections integrate in\nthe previous dynamic.\nAlgorithm 1 KomplexNet\nRequire: Input image X, number of timesteps T, set of layers Li with i ∈0, ..., Nl −1, Kuramoto function\nK, Kuramoto parameters : coupling kernel R, desynchrony term ϵ, learning rate λ\nfor t ←0 to T −1 do\nat ←L0(X)\nθ0,t ←θ0,t−1 + K(at, θ0,t−1, R, ϵ, λ)\nz0t ←at.ei.θ0,t\nfor i ←1 to Nl −1 do\nzlt ←Li(zl−1t)\nreturn Predictions |zNl−1,T −1|\nA.2\nCoupling kernel\n18\n\n\nAlgorithm 2 KomplexNet with feedback\nRequire: Input image X, Initial phases with random values θinit, number of timesteps T, set of layers Li,\nKuramoto function K, Kuramoto parameters : coupling kernels Ri, desynchrony term ϵi, learning rate λi\nwith i ∈0, ..., Nl −1\na0 ←L0(X)\nθ0,0 ←θinit + K(a0, θinit, R0, ϵ0, λ0)\nz0,0 ←a0.ei.θ0;0\nfor i ←1 to Nl −1 do\nzl,0 ←Li(zl−1,0)\nfor t ←1 to T −1 do\nat ←L0(X)\nθ0,t ←θ0,t−1 + K(at, θ0,t−1, R0, ϵ0, λ0) + PNl−1\nl=1 [K(|zlt−1|, θl,t−1, Rl, ϵl, λl)]\nz0,t ←at.ei.θ0,t\nfor i ←1 to Nl −1 do\nzl,t ←Li(zl−1,t)\nreturn Predictions |zNl−1,T −1|\nA.3\nComplementary results\nWe show in this section the complementary results and various tests, to give more insights into the functioning\nof the proposed method.\nTest on more timesteps.\nWithout retraining KomplexNets, we test the models on more timesteps than\nduring training. The resulting (Figure 10) performance shows the robustness of the Kuramoto dynamic to\nmaintain a good phase representation over time.\nBest validation model.\nThe results presented in the main paper are obtained by averaging the perfor-\nmance (synchrony and accuracy) on 50 different initializations and completing with the standard deviation.\nComplementary to this choice, we select the best model on the validation set and plot in Figure 11 the\nsynchrony on the first row and the performance on the second row for each type of model and three different\ntest sets (in-distribution, overlap, and noise). The results are consistent with the ones in the main text\n(except for the performance of KomplexNet with feedback on noisy images).\nInfluence of the feedback layer.\nKomplexNet with feedback receives feedback information from all the\nlayers. To highlight the contribution of each layer specifically, we train separated models with feedback coming\nfrom only one of the layers and we report the performance in Figure 12. We can first observe that KomplexNet\nwith feedback (coming from all the layers) outperforms all the other versions on robustness tests, emphasizing\nthe need for various types of information to solve ambiguous cases. Conversely, KomplexNet performs worse\nthan all the models with feedback (both in-distribution and robustness). We additionally provide for each\ncase the value of the hyper-parameters found to maximize the performance in Table 1. In conformance with\nour expectations, ϵ has to be higher when the models are provided with feedback connections to compensate\nfor more synchronized activity coming from other layers. The size of the local kernel k remains the same\nacross the versions, corresponding more or less to the size of the digit in the image. Likewise, kl1 is smaller\nthan k to account for the downsampling of L1. Finally, the influence of local synchrony, modulated by λ is\nhigher than the influence of the feedback phases (λli ≤λ for i ∈1, 2, 3).\nRaw performance on the generalization test sets.\nThe main text reports the difference in the\nperformance of KomplexNet and KomplexNet with feedback when tested on two to nine digits to evaluate\ntheir generalization abilities. We show in Figure 13 the raw performance of each model and the baselines\ntrained on two or three digits. All the models show more difficulty in classifying correctly as the number of\ndigits in the image increases. However, KomplexNets show consistently a higher performance (see Figure 8).\n19\n\n\nFigure 9: Visualization of the learned coupling kernel after training. We show the coupling kernels\n(rk,cij in Equation 1) learned by KomplexNet, associated with the feature weights learned by the convolution.\nEach coupling kernel rk,cij represents how phases (θk) in the source channel influence phases (θcij) in the\ntarget channel (resulting in a non-symmetric interaction).\nA.4\nVisualizations\nSimilarly to the in-distribution tests, we provide in Figure 14 visualizations of the phases of all complex\nmodels. The random case remains unchanged. However, we can observe that KomplexNets find a solution for\nthe overlapping digits by almost creating three equidistant clusters, with the overlapping pixels belonging\nto a cluster in between the two others on the unitary circle. Despite no supervision, KomplexNets found a\nsolution close to the one we adopted for the “‘ideal” case, consisting of affecting the overlapping pixels to a\nthird cluster, but keeping the two other clusters opposite to each other. Additionally, when we add Gaussian\nnoise to the images, the models struggle more to separate the digits, but KomplexNet with feedback remains\nable to affect opposite values to the digits (the clusters just show more variance).\nPhases obtained on robustness tests.\n20\n\n\nFigure 10: Results on multi-MNIST when tested on more timesteps.\nWe report the average\nperformance of KomplexNet (red) and KomplexNet with feedback (purple) over time along with the standard\ndeviation for 50 repetitions. We compare it with its real-valued counterpart (blue), a complex model with\nrandom phase initialization (green), and the ideal phase cluster synchrony (orange). The models are tested\non the in-distribution dataset (left plot), overlapping digits (middle plot), and noisy images (right plot). The\nvertical bar represents the number of timesteps in the training condition.\nϵ\nk\nkl1\nλ\nλl1\nλl2\nλl3\nKomplexNet\n0.2\n13\n-\n0.006\n-\n-\n-\nKomplexNet with feedback from l2\n0.4\n13\n5\n0.006\n0.003\n-\n-\nKomplexNet with feedback from l3\n0.5\n13\n-\n0.005\n-\n0.004\n-\nKomplexNet with feedback from l4\n0.5\n13\n-\n0.005\n-\n-\n0.004\nKomplexNet with feedback from l2,3,4\n0.5\n13\n5\n0.009\n0.005\n0.004\n0.004\nTable 1: We perform a hyper-parameter search to find the optimal parameters for the Kuramoto dynamic\nand report the values for the different versions of the models. ϵ represents the desynchrony term in the local\ndynamic, k and kl1 respectively the size of the local coupling kernel and the kernel coming from L1, and λ\nand λli for i ∈1, 2, 3 modulate the influence of the phase modification by the local dynamic and the one\ncoming from the feedback layers Li for i ∈1, 2, 3.\nA.5\nAdditional experiments\nWe finally show the results corresponding to the additional experiments detailed in the main text with\nvariants in the datasets to show different uses of KomplexNet.\nCIFAR10 images in the background.\nWe create an additional dataset with the same digits but with\nthe background filled with RGB content, namely images from the CIFAR10 dataset. We retrain the models\non the new images (the task remains unchanged: 2-digit classification) and report the cluster synchrony loss\nand accuracy on the test set in Figure 15. Compared to the previous datasets (with a uniform background),\nthe models achieve a lower performance. Indeed, the task is now harder due to less salient information to\nextract. This version is particularly harder for the Kuramoto model: the propagation of phase synchrony is\nenhanced by the activated background, even though the information is irrelevant. For this reason, the cluster\nsynchrony is much higher than before and very far from the ideal scenario. As a result, the accuracy is also\nfurther from the ideal case. However, KomplexNets are still better than the baselines, confirming that the\nKuramoto dynamic remains helpful on RGB images. More specifically, the feedback connections show a clear\nadvantage on the cluster synchrony loss compared to KomplexNet without feedback, resulting in a slight\nincrease in test accuracy.\n21\n\n\nFigure 11: Results on multi-MNIST. We report the test synchrony (first row) and performance (second\nrow) of KomplexNet (red) and KomplexNet with feedback (purple) over time along of the best model on\nthe validation set. We compare it with its real-valued counterpart (blue), a complex model with random\nphase initialization (green), and the ideal phase cluster synchrony (orange). The models are tested on the\nin-distribution dataset (left plot), overlapping digits (middle plot), and noisy images (right plot).\nMoving digits.\nThe second variant in the dataset is to convert static images into videos with both digits\nmoving across frames. With this version, we aim to evaluate whether we can use the Kuramoto dynamic\nalong with the phase information as a memory mechanism to help resolve ambiguous cases (overlap between\ndigits in this case). To test such a hypothesis, we generate videos of 30 frames, starting from the two digits\noverlapping (with a maximum of 25% of the active pixels) in the center of the image. We then generate\na random trajectory for the first digit and use the opposite trajectory for the second. We let the objects\nmove along these trajectories for 15 frames (making them bounce on the border of the image to prevent\nthem from disappearing and controlling for the amount of overlap at each frame) and use these frames as the\nfirst half of the video. We take the opposite trajectories and apply the same procedure to generate the 15\nother half of the video. In other words, the resulting videos start with both digits at a random location, then\nslowly move closer to each other, reaching a maximum amount of overlap in the middle before moving away\nfrom each other. We are interested in the performance of the model around this frame of maximum overlap\n(10 frames before and after). We use the first frames to let the Kuramoto model converge to a clean phase\nseparation. We evaluate KomplexNet with and without feedback on the videos, as well as the same models\ntested on each frame separately. For the models tested on the videos, the task has the additional difficulty of\nadapting the phase information to moving information: each frame corresponds to a single Kuramoto step,\npotentially leading to noisy artifacts in the phase information because the models were trained on static\nimages. However, the models tested on each frame separately, despite having the time to converge on clean\nphase separation, do not have access to frames with less overlap between digits. We show in Figure 16 (left\npanel) the raw performance 10 frames before and after reaching the maximum overlap (denoted frame 0)\nfor KomplexNet with (purple) and without feedback (red) given the two types of input (static in dashed\nlines and videos in plain lines). We report the performance of the real model (tested on static images only\n22\n\n\nFigure 12: Performance of Komplexnet with various types of feedback. We report the average\nperformance of KomplexNet (red) and KomplexNet with feedback (purple) over time along with the standard\ndeviation for 50 repetitions. We additionally show the performance of KomplexNet with feedback from a\nsingle layer (L2 in green, L3 in light blue, and l4 in yellow). We compare the models with the usual baselines\n(real model in dark blue, random phase model in dark green, and ideal phase model in orange). Panel A\nrepresents the average and standard deviations over 50 different initializations and Panel B reports the test\naccuracy of the best initialization on the validation set.\nbecause of the absence of temporal dynamic in the model) to confirm the benefit of KomplexNet on this\nversion as well. We present the videos from Frame 0 to Frame 30 as well as from Frame 30 to Frame 0 to\ncorrect for potential bias in the generation of the dataset. On the left Panel, we observe that the models\nwhich were presented the moving digits are outperformed by the models tested on single images at the first\ntimesteps. This is explained by the fact that the digits do not overlap (or barely do) 10 frames before Frame\n0. Therefore, given several timesteps to converge on the frame, the \"static\" models reach a cleaner phase\nseparation leading to better accuracy. However, as we get closer to the maximum overlap, the \"static\" models\nshow a drop in performance, compared to the \"dynamic\" models suggesting the use of the phase separation\ncoming from the frames where the digits did not overlap. Finally, as the digits get away from each other,\nthe \"static\" models recover their performance, outperforming after a few frames the dynamic model. This\nresult is shown differently on the center panel. We show here the difference in the accuracy of each model\nfrom Frame 0 to Frame 30 with the accuracy from Frame 30 to Frame 0. Every \"static\" model has a null\ndifference because the dataset was symmetrized. However, the \"dynamic\" models show a positive difference\nbefore Frame 0 and a negative difference after, suggesting that they use the clean phase from the previous\nsteps to keep a good phase separation when the digits overlap. However, they take a few steps to recover\nfrom the phase corruption induced by a lot of overlap between digits. Finally, we show on the right panel\nthe difference in performance between static and dynamic models. We can observe a maximum difference at\n23\n\n\nFigure 13: Results on multi-MNIST with more digits. We report the average performance of KomplexNet\n(red) and KomplexNet with feedback (purple) over time along with the standard deviation for 50 repetitions.\nWe compare the models with the usual baselines (real model in dark blue, random phase model in dark green,\nand ideal phase model in orange). The left plot shows the test performance of the models trained on 2 digits,\nand the right plot is for the models trained on 3 digits.\nFrame 0 in favor of dynamic models, outperformed by their static counterparts when the amount of overlap\nis reduced. These results confirm the possible use of the Kuramoto dynamic as a mechanism of memory,\nhelping resolve extremely overlapping cases and being compatible with dynamical inputs.\n24\n\n\nFigure 14: Robustness on synchrony visualizations. As in Figure 5, we show the phases of KomplexNet\nand KomplexNet with feedback, at the last timestep, compared to the two complex baselines (random phases\non the left, and ideal phases on the right) on representative examples of overlapping digits (panel A) and\nadditive Gaussian noise (panel B).\nFigure 15: Results on multi-MNIST with CIFAR in the background. We report the cluster synchrony\n(first column) and performance (second column) of KomplexNet (red) and KomplexNet with feedback (purple)\nover time for 50 repetitions (mean and standard deviation in the first row, and best model on the validation\nset in the second row). We compare it with its real-valued counterpart (blue), a complex model with random\nphase initialization (green), and the ideal phase cluster synchrony (orange).\n25\n\n\nFigure 16: Results on moving digits. We evaluate KomplexNet with (purple) and without feedback (red),\nas well as a real model (blue) given moving digits and evaluate KomplexNets on static frames (dashed line)\nversus dynamic videos (plain line). The left panel shows the test accuracy of each model around the frame\nwith the maximum overlap between digits (Frame 0). The middle panel represents the difference in accuracy\nwhen tested from Frame 0 to Frame 30 or reversed. The right panel shows the accuracy of the dynamic\nmodels versus their static counterparts.\n26\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21077v1.pdf",
    "total_pages": 26,
    "title": "Enhancing deep neural networks through complex-valued representations and Kuramoto synchronization dynamics",
    "authors": [
      "Sabine Muzellec",
      "Andrea Alamia",
      "Thomas Serre",
      "Rufin VanRullen"
    ],
    "abstract": "Neural synchrony is hypothesized to play a crucial role in how the brain\norganizes visual scenes into structured representations, enabling the robust\nencoding of multiple objects within a scene. However, current deep learning\nmodels often struggle with object binding, limiting their ability to represent\nmultiple objects effectively. Inspired by neuroscience, we investigate whether\nsynchrony-based mechanisms can enhance object encoding in artificial models\ntrained for visual categorization. Specifically, we combine complex-valued\nrepresentations with Kuramoto dynamics to promote phase alignment, facilitating\nthe grouping of features belonging to the same object. We evaluate two\narchitectures employing synchrony: a feedforward model and a recurrent model\nwith feedback connections to refine phase synchronization using top-down\ninformation. Both models outperform their real-valued counterparts and\ncomplex-valued models without Kuramoto synchronization on tasks involving\nmulti-object images, such as overlapping handwritten digits, noisy inputs, and\nout-of-distribution transformations. Our findings highlight the potential of\nsynchrony-driven mechanisms to enhance deep learning models, improving their\nperformance, robustness, and generalization in complex visual categorization\ntasks.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}