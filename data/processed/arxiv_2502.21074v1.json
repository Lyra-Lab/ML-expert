{
  "id": "arxiv_2502.21074v1",
  "text": "CODI: Compressing Chain-of-Thought into Continuous Space via\nSelf-Distillation\nZhenyi Shen1, Hanqi Yan1, Linhai Zhang1, Zhanghao Hu1, Yali Du1,2, Yulan He1,2\n1King’s College London\n2The Alan Turing Institute\n{zhenyi.shen, yanqi.yan, linhai.zhang, zhanghao.hu}@kcl.ac.uk\n{yali.du, yulan.he}@kcl.ac.uk\nAbstract\nChain-of-Thought (CoT) enhances Large Lan-\nguage Models (LLMs) by enabling step-by-\nstep reasoning in natural language. However,\nthe language space may be suboptimal for rea-\nsoning. While implicit CoT methods attempt\nto enable reasoning without explicit CoT to-\nkens, they have consistently lagged behind ex-\nplicit CoT method in task performance. We\npropose CODI (Continuous Chain-of-Thought\nvia Self-Distillation), a novel framework that\ndistills CoT into a continuous space, where a\nshared model acts as both teacher and student,\njointly learning explicit and implicit CoT while\naligning their hidden activation on the token\ngenerating the final answer. CODI is the first\nimplicit CoT method to match explicit CoT’s\nperformance on GSM8k while achieving 3.1×\ncompression, surpassing the previous state-of-\nthe-art by 28.2% in accuracy. Furthermore,\nCODI demonstrates scalability, robustness, and\ngeneralizability to more complex CoT datasets.\nAdditionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its\nreasoning process transparent. Our findings es-\ntablish implicit CoT as not only a more efficient\nbut a powerful alternative to explicit CoT.\n1\nIntroduction\nLarge Language Models (LLMs) have exhibited\nremarkable reasoning capabilities (OpenAI, 2024;\nAnthropic, 2024; Google, 2024), with Chain-of-\nThought (CoT) (Wei et al., 2022) emerging as a\nkey technique for enabling step-by-step reasoning\nvia natural language rationales.\nHowever, neuroscientific studies (Amalric and\nDehaene, 2016, 2019) show that human mathemat-\nical reasoning does not primarily involve language\nprocessing areas in brains, suggesting that natural\nlanguage may not be the most effective medium\nfor reasoning. Furthermore, (Lin et al., 2025) high-\nlights that LLMs often depend heavily on specific\ntokens during reasoning, which can lead to er-\nrors despite the tokens being commonsensically\nFigure 1: Illustration of the training process of differ-\nent reasoning approaches. CoT-SFT is standard CoT\nfinetuning. Coconut learns implicit CoT by curriculum\nlearning. CODI learns implicit CoT by self-distillation.\nvalid. Replacing these tokens with alternatives has\nbeen shown to improve performance, underscoring\nLLMs’ sensitivity to linguistic features in natural\nlanguage rationales. These insights motivate a shift\nfrom natural language CoT representations (Ex-\nplicit CoT and Discrete Tokens) to dense, continu-\nous representations (Implicit CoT and Continuous\nThoughts) that may better align with the compact\nand abstract nature of reasoning.\nImplicit CoT requires two major design consid-\nerations: the forward function and the training ob-\njective. Various forward functions have been ex-\nplored, including removing all reasoning tokens\n(Deng et al., 2023, 2024), adding fixed learning\ntokens (Pfau et al., 2024; Goyal et al., 2024), per-\nforming autoregression by connecting the last hid-\nden activation to the next input embedding (Hao\net al., 2024). The autoregression approach appears\nto perform the best, likely because it increases the\neffective computational depth. However, the key\ndesign challenge for implicit CoT lies in the train-\ning objective. While explicit CoT learns reasoning\nthrough language modeling over annotated CoT\ntokens, implicit CoT cannot rely on this standard\nlanguage modeling approach, as it must avoid gen-\nerating explicit CoT tokens by definition.\nTo address this challenge, Coconut (Hao et al.,\n1\narXiv:2502.21074v1  [cs.CL]  28 Feb 2025\n\n\n2024), the state-of-the-art method, adopts a cur-\nriculum learning strategy initially introduced by\n(Deng et al., 2024) as illustrated in Figure 1. It\ngradually replaces the initial CoT tokens with con-\ntinuous thoughts while maintaining the language\nmodeling objective on the remaining CoT tokens.\nIn this way, the language modeling loss encourages\nthe continuous thoughts to behave like the removed\nCoT tokens, and at the final stage of learning, all\nCoT tokens are replaced with continuous thoughts,\nachieving full implicit CoT. However, while Co-\nconut outperforms the No-CoT baseline (which\nentirely omits CoTs), it still lags behind CoT-SFT\nby 20% on GSM8k, exposing a key limitation in\nthe implicit CoT paradigm. We believe these limi-\ntations stem from the curriculum learning strategy\nitself—the multi-stage process delays the acquisi-\ntion of a complete discrete CoT. If the model fails\nto generate the correct continuous thought at any\nstage due to forgetting or incomplete learning, er-\nrors propagate through subsequent stages, limiting\noverall performance.\nWe propose CODI (Continuous Chain-of-\nThought via Self-Distillation), a novel framework\nthat distills explicit CoT into implicit CoT by align-\ning the hidden activation of the token responsi-\nble for generating the final answer as illustrated\nin Figure 1. CODI reframes implicit CoT learn-\ning as a self-distillation task (Wang et al., 2023;\nGou et al., 2021), where the same model serves as\nboth teacher and student. Unlike conventional self-\ndistillation having the two roles of equal capability,\nCODI enhances the teacher’s knowledge by provid-\ning them distinct input contexts: the teacher learns\nfrom the groundtruth CoT and final answer using\na language modeling objective, while the student\ngenerates continuous thoughts before predicting\nthe final answer–our target task. Distillation oc-\ncurs at the token preceding the final answer, which\nOrgad et al. (2025) identify as encoding crucial rea-\nsoning information. Since we can formally show\nthat CoT influences the hidden activation of this\ntoken only by a shift (Section 3.3), CODI enforces\nalignment between the teacher and student by min-\nimizing their hidden activation differences using\nan L1 distance loss, effectively injecting explicit\nCoT supervision into implicit CoT generation. This\nsingle-step distillation in feature space mitigates\nthe forgetting issues inherent in curriculum learn-\ning, enabling more effective implicit CoT training.\nThe main contributions are threefold:\n• We propose CODI, a novel self-distillation frame-\nwork that enables LLMs to reason in a compact\ncontinuous space, providing an alternative to ac-\ncelerate reasoning with high performance.\n• We demonstrate the effectiveness of distilling\nknowledge from explicit CoT (teacher) to im-\nplicit CoT (student) by aligning the hidden activa-\ntion of a single token, simplifying the distillation\nprocess and improving efficiency.\n• Extensive experiments show that CODI is robust,\nscalable, and generalizable to more complex CoT\ndatasets. Additionally, CODI maintains inter-\npretability, making its reasoning process trans-\nparent.\n2\nRelated Work\nImplicit Chain-of-Thought Reasoning.\nIm-\nplicit CoT methods aim to enhance reasoning\nwithout verbalizing intermediate steps as in CoT,\nthereby accelerating inference speed. Theoretical\nwork (Strobl et al., 2024; Merrill and Sabharwal,\n2024) establishes that additional computational\ntokens enhance transformers’ reasoning capacity.\nEmpirical studies (Pfau et al., 2024; Goyal et al.,\n2024) validate these insights by training LLMs with\nextra dummy tokens before answering though in\na limited scale and effect. Recent efforts (Deng\net al., 2023, 2024) distills CoT reasoning by fine-\ntuning. They improve over the No-CoT baseline,\nbut fall behind CoT finetuning possibly due to dis-\ncarding all intermediate tokens. Addressing this,\nCoconut (Hao et al., 2024) reintroduces interme-\ndiate reasoning tokens via autoregressive hidden\nstate propagation, combining curriculum learning\nfrom (Deng et al., 2024). While this achieves some\nimprovement over (Deng et al., 2024), Coconut\nstill lags behind explicit CoT, which we attribute to\nforgetting in curriculum learning. CODI replaces\ncurriculum learning with a novel self-distillation\nframework, enabling a single-step learning process\nthat avoids forgetting issues. Our work is also in-\nspired by in-context compression (Ge et al., 2024;\nLi et al., 2024b), though our work is compressing\nthe generation instead of the existing contexts.\nKnowledge Distillation.\nKnowledge distillation\n(KD) (Gou et al., 2021; Xu et al., 2024) has\nemerged as a key strategy for transferring CoT rea-\nsoning capabilities from teacher to student mod-\nels. Traditional approaches (Hsieh et al., 2023; Ho\net al., 2023) train smaller student models to mimic\nstep-by-step outputs from larger teacher LLMs, mo-\ntivated by findings that CoT reasoning emerges pre-\ndominantly in large models (Wei et al., 2022). Self-\n2\n\n\nFigure 2: CODI enables the model to generate continuous CoTs by jointly training a student and teacher task\nwithin a shared LLM, distilling knowledge from the teacher to the student. The Student task (left) generates the\nanswer by autoregressively decoding continuous thoughts, while the Teacher task (right) generates the answer using\nthe groundtruth CoT via teacher forcing. Both tasks learn the generated texts via cross-entropy loss (Lstudent and\nLteacher), and share the same LLM. Knowledge distillation is achieved by applying LKD (L1 loss) between student\nand teacher hidden activation across all layers (hstudent and hteacher).\ndistillation (Yang et al., 2024; Dong et al., 2024)\nleverage self-distillation to preserve the model’s\noriginal behavior, akin to the KL divergence loss\nused in RLHF (Ouyang et al., 2022). Our work\nis based on self-distillation framework, but further\nstrengthens the teacher by providing it with richer\ninput contexts, enabling the student to learn from\nit like knowledge distillation. Since the teacher\nand student tasks differ, CODI can also be viewed\nas a form of multitask learning (Crawshaw, 2020).\nMoreover, CODI distinguishes itself by allowing\nreason in the latent space other than natural lan-\nguage, which is rarely explored in prior knowledge\ndistillation works. This innovation enables more\nflexible and efficient reasoning.\n3\nCODI: Continuous Chain-of-Thought\nvia Self Distillation\nUnlike traditional CoT reasoning, CODI bypasses\nautoregression in the vocabulary space, and directly\nconnects the last hidden representation to the sub-\nsequent input. The key challenge in training such a\nmodel with continuous thoughts lies in designing\nan appropriate training objective. Conventional rea-\nsoning learning in explicit CoT fine-tuning relies\non a language modeling objective over annotated\nCoT tokens, which inevitably leads to discrete CoT\ntoken generation—contradicting the definition of\nimplicit CoT.\n3.1\nOverview\nCODI addresses this by introducing a self-\ndistillation framework (Figure 2) with two train-\ning tasks: a teacher task and a student task. The\nteacher task learns explicit CoT generation, while\nthe student task learns implicit continuous CoT\ngeneration. Knowledge distillation is achieved by\naligning the hidden activation of a key token from\nthe teacher to the student via LKD. The overall\ntraining objective is a weighted sum of three losses,\nwhich will be detailed later:\nL = αLteacher + βLstudent + γLKD,\n(1)\nwhere α, β, and γ are hyperparameters controlling\nthe balance among the objectives..\nA Python implementation of this framework is\nprovided in Figure A3.\n3.2\nStudent Task\nThe student task (Figure 2, left), the target task,\ngenerates continuous thoughts by autoregressively\npropagating the last hidden states and learns to\ngenerate the answer token using a cross-entropy\nloss:\nLstudent = −1\nN\nN\nX\ni=1\nlog P(yi | y1:i−1, Q, Z), (2)\nwhere P is the probability distribution of the LLM,\ny refers the answer label, Q refers the question\ntokens, and Z refers the continuous thoughts.\nOn its own, the model benefits only marginally\nfrom the additional computation (Goyal et al.,\n2024) compared with the No-CoT scenario be-\ncause there are no supervision for the continuous\nthoughts.\nAdditionally, CODI applies modifications exclu-\nsively to the student task. Two special tokens, bot\nand eot, mark the start and end of continuous rea-\nsoning, inspired by (Hao et al., 2024). A two-layer\n3\n\n\nMLP followed by layer normalization transforms\nthe hidden representations of continuous thought\ntokens before feeding them into the next step.\n3.3\nTeacher Task\nUnlike the student task, the teacher task (Figure\n2, right) performs explicit CoT generation using a\nlanguage modeling objective:\nLteacher = −1\nN\nN\nX\ni=1\nlog P(yi | y1:i−1, Q),\n(3)\nwhere y refers both the CoT and the answer labels,\nand Q refers the question tokens.\nThe teacher task serves two key functions: (1)\nReference Learning: By learning explicit CoTs,\nthe teacher task equips the model with structured\nreasoning patterns, offering a foundational refer-\nence for the student task. (2) Latent Supervi-\nsion: As the teacher has access to ground-truth\nCoT tokens, its hidden activation at the answer-\ngenerating token encapsulate essential reasoning\ninformation by attending to all preceding CoT to-\nkens. In contrast, the student initially operates\nwithout such structured guidance. To address this\ndisparity, CODI aligns the hidden activation of this\ntoken between the teacher and student across all\nlayers using an L1 loss:\nLKD = 1\nM\nM\nX\nl=1\n|sg[hl\nteacher] −hl\nstudent|,\n(4)\nwhere M indicates the number of layers in the\nLLM, sg denotes stop gradient, and hl is the hidden\nactivation of the LLM’s l-th layer.\nThe Distilled Token.\nRather than aligning with\nall tokens in the generated sentence, we select a dis-\ntillation token for alignment. Inspired by the recent\nobservations (Orgad et al., 2025) that the hidden\nactivation of the token intermediately preceding the\nanswer, i.e., the colon (“:”) in the answer prompt\n“The answer is:” (as shown in Figure 2), encodes\nfar more information than output logits. We select\nthis token’s hidden activation, h, for distillation.\nThis selection can be further verified by the shift\nmechanism in in-context learning. Recent work (Li\net al., 2024a; Liu et al., 2023) demonstrates that in-\ncontext examples influence the final query token by\nshifting its hidden activation values. Extending this\nidea, we show that CoT tokens similarly induce a\nshift in hidden activation values of this target token,\nas formalized in Equation 5:\nhl\nCoT ≈hl\nno-CoT + f\n\u0010\nWV R(WKR)T q\n\u0011\n,\n(5)\nwhere q is the query of this target token, hl\nCoT is the\nhidden activation at layer l with CoT (equivalent\nto hl\nteacher), hl\nno-CoT is the corresponding activation\nwithout CoT, and the remaining term quantifies the\nshift introduced by the CoT rationale R.\nThis suggests that the target token’s hidden acti-\nvation encode the influence of preceding reasoning\nsteps, and hl\nstudent can learn this shift by minimiz-\ning a simple distance metric, such as L1 loss, with\nhl\nteacher. A formal proof of this “CoT shift” phe-\nnomenon is provided in Appendix B.\n3.4\nTraining and Inference\nTraining.\nThe continuous thoughts are generated\ndynamically during training, as they are not known\nbeforehand. To achieve this, we decode them step\nby step, with a cache storing previous keys and val-\nues to maintain efficiency. When applying distance\nloss between two hidden activation, we observed\na significant norm variations across layers (Deng\net al., 2023; Cheng and Durme, 2024). To address\nthis, we normalize each layer’s activation by divid-\ning them by the standard deviation of the teacher’s\nhidden activation within the current batch.\nFor the distillation task, we employed the same\nmodel for the teacher task and the student task\nfor two reasons: (1) Warm-up: When the teacher\ntrains alongside the student, it creates a warm-up ef-\nfect for LKD. Both components start from the same\ninitialization point, diverge during training, and\ngradually converge as the student adapts. In con-\ntrast, a static pre-trained teacher initially presents\nan overly challenging objective, as its hidden states\nreflect fully developed reasoning patterns that the\nuntrained student cannot immediately match. (2)\nShared model representations: Using the same\nmodel mitigates alignment issues in hidden acti-\nvation that arise when using separate models, en-\nabling smoother and more effective information\ntransfer between the teacher and student. The cor-\nresponding ablation studies, which validate these\nfindings, are detailed in Table 3.\nFor training data, we exclude the final CoT\nstep—the step responsible for generating the final\nanswer—because including this step could allow\nthe teacher’s hidden activation to take a shortcut.\nSpecifically, the model might directly copy the re-\nsult from the last CoT step to the token responsible\nfor generating the exact answer token, bypassing\nthe reasoning process. This behavior would under-\nmine the quality of the target hidden activation, as\nthey would no longer fully encode the reasoning\npatterns. The ablation results demonstrating the\n4\n\n\nimpact of this exclusion are presented in Table 3.\nInference.\nThe inference process in CODI mir-\nrors the student task during training (Figure 2, left).\nThe model autoregressively decodes n continuous\nthoughts following the question and the bot token.\nOnce the reasoning process is complete, the eot\ntoken is manually inserted to terminate continu-\nous reasoning and switch the model to language\ngeneration mode, decoding the final answer.\n4\nExperiments\nWe demonstrate the effectiveness of CODI’s rea-\nsoning in a continuous space through experiments\non mathematical reasoning tasks.\n4.1\nExperimental Setup\nTraining Data.\nWe utilize two datasets to train\nour models–GSM8k-Aug and GSM8k-Aug-NL. (1)\nWe use the GSM8k-Aug dataset from (Deng et al.,\n2023), which has proven effective for training im-\nplicit CoT methods (Deng et al., 2024; Hao et al.,\n2024). This dataset extends the original GSM8k\ntraining set (Cobbe et al., 2021) to 385k samples by\nprompting GPT-4. To facilitate implicit CoT train-\ning, all natural language interleaving within the\nCoT is removed, leaving only structured mathemat-\nical expressions such as “<< 10 ÷ 5 = 2 >><<\n2 × 2 = 4 >><< 6 × 4 = 24 >>”. (2) We\nalso use GSM8k-Aug-NL, a version that preserves\nnatural language explanations, to assess both the\ngeneralizability and effectiveness of our approach\nto compress more verbose CoTs. Examples and\nstatistics are in Appendix C.\nEvaluation Benchmarks for OOD.\nIn addition\nto the test split of GSM8k, we assess model ro-\nbustness on three out-of-domain (OOD) bench-\nmarks: (1) SVAMP (Patel et al., 2021), a dataset of\nelementary-school arithmetic word problems with\nsimple variations designed for robustness test; (2)\nGSM-HARD (Gao et al., 2022), a modified version\nof the GSM8k test split where numbers are replaced\nwith values of larger magnitude to increase diffi-\nculty; and (3) MultiArith (Roy and Roth, 2015), a\nsubset of MAWPS (Koncel-Kedziorski et al., 2016)\ncontaining multi-step mathematical word problems.\nExamples and statistics are in Appendix C.\nBaselines.\nWe consider the following baselines:\n(1) CoT-SFT: Finetunes the model on CoT data,\nenabling it to generate intermediate steps followed\nby the final answer. As CoT-SFT relies on sam-\npling, we set the temperature to 0.1 and report the\naverage result over 10 runs. (2) No-CoT-SFT: Fine-\ntunes the model using only direct answers, with-\nout generating intermediate steps. (3) iCoT (Deng\net al., 2024): Implements a curriculum learning\nstrategy called \"Stepwise Internalization\", which\ninjects CoT’s reasoning patterns into the model’s\ninternal states. This allows the model to generate di-\nrect answers with higher accuracy during inference.\n(4) Coconut (Hao et al., 2024): Build upon iCoT\nby autoregressively generating intermediate contin-\nuous CoT representations, similar to the approach\nin our work. (5) CODI: our method trained with\nsix continuous thought tokens, matching the setup\nin Coconut. Baselines (2)–(5) are deterministic\nmodels, and their results are reported from a single\nrun. Two base models are considered GPT-2 (Rad-\nford et al., 2019) and LLaMA3.2-1b (Meta, 2024).\nMore implementation details are in Appendix A.\n4.2\nMain Results\nMathematical Reasoning.\nTable 1 shows the\nevaluation results on GSM8k. CODI achieves a\nsignificant performance improvement over other\nimplicit CoT methods. In the settings of GPT-2,\nCODI surpasses iCoT by 45.7% and Coconut by\n28.2%. Notably, CODI is the first continuous CoT\nmethod to perform on par with CoT-SFT, achieving\n99.1% of CoT-SFT’s performance. Unlike iCoT\nand Cococnut failing to scale up to larger models\n(Hao et al., 2024), CODI successfully scales to\nLLaMA1b, achieving 90.3% of CoT-SFT’s perfor-\nmance. These results highlight CODI’s superiority\nin terms of accuracy for in-domain mathematical\nreasoning tasks.\nEfficiency.\nCODI utilizes a fixed set of six con-\ntinuous thoughts, enclosed by two special tokens,\nresulting in a total of eight \"tokens\" for reason-\ning. As shown in Table 2, CODI achieves substan-\ntial efficiency gains, with a speedup of approxi-\nmately 2.7× (3.1× CoT compression) for compact\nCoTs trained on GSM8k-Aug and 5.9× (7.8× CoT\ncompression) for verbose CoTs trained on GSM8k-\nAug-NL, demonstrating CODI’s effectiveness in\nreducing reasoning overhead.\nRobustness.\nTo assess robustness, we evaluate\nCODI on out-of-distribution datasets.\nNotably,\nCODI consistently outperforms CoT-SFT across all\nthree benchmarks for GPT-2. We attribute this to\nCODI’s reduced tendency to overfit, as evidenced\nby its significantly lower training accuracy com-\npared to CoT-SFT (Table A3). This difference\narises because CODI lacks exact imitation targets\n5\n\n\nMethod\nIn-Domain\nOut-of-Distribution\nGSM8k\nGSM8k-NL\nSVAMP\nGSM-Hard\nMultiArith\nGPT-2 Small\nCoT-SFT\n44.1%\n34.8%\n41.8%\n9.8%\n90.7%\nNo-CoT-SFT\n19.1%\n19.1%\n16.4%\n4.3%\n41.1%\niCoT\n30.1%*\n3.2%\n29.4%\n5.7%\n55.5%\nCoconut\n34.1%*\n−\n−\n−\n−\nCODI (Ours)\n43.7%\n35.3%\n42.9%\n9.9%\n92.8%\nLLaMA3.2-1b\nCoT-SFT\n61.6%\n54.1%\n66.7%\n15.6%\n99.3%\nNo-CoT-SFT\n30.9%\n30.9%\n44.1%\n7.1%\n70.9%\niCoT\n19.0%\n15.2%\n40.9%\n4.4%\n39.0%\nCODI (Ours)\n55.6%\n49.7%\n61.1%\n12.8%\n96.1%\nTable 1: Results on four datasets: GSM8k, SVAMP, GSM-Hard, and MultiArith. GSM8k-NL indicates that the\ntraining data is GSM8k-Aug-NL, the verbose dataset, instead of GSM8k-Aug. Results marked with * are taken\nfrom the corresponding papers (Deng et al., 2024; Hao et al., 2024). Coconut’s results are incomplete due to the\nunavailability of open-source code.\nfor continuous thoughts during training, making it\nless prone to memorizing patterns and more adapt-\nable to novel scenarios.\nCompress CoTs with Natural Language.\nPre-\nvious works (Deng et al., 2024; Hao et al., 2024)\nprimarily trained on GSM8k-Aug, which consists\nonly of mathematical expressions. To evaluate\nCODI’s generalizability, we extend our analysis\nto a more complex CoT dataset, GSM8k-Aug-NL.\nTable 1 shows that both GPT-2 and LLaMA1b\nperform worse on it compared to GSM8k-Aug.\nThis decrease in performance stems from the ad-\nditional natural language tokens, which add noise\nand make imitation learning more difficult. Sur-\nprisingly, CODI surpasses CoT-SFT when using\nGPT-2 and achieves a higher relative score improve-\nment on LLaMA1b compared to models trained on\nGSM8k-Aug. Moreover, iCoT almost fails in this\ntask because the longer sequence makes curriculum\nlearning challenging. Furthermore, with the aver-\nage CoT length increasing to 62.1 (Table 2), CODI\nachieves a compression ratio of 7.8, suggesting that\nthe optimal compression ratio is dataset-dependent.\nThese results demonstrates CODI’s ability to han-\ndle more complex CoT training data, showcasing\nits applicability to diverse reasoning datasets.\nCompression Ratio.\nThe number of continuous\nthoughts used during training is a crucial hyper-\nparameter, influencing both the computation allo-\ncation and the compression ratio. As shown in\nFigure 3, CODI consistently outperforms Coconut\nacross all compression ratios. Interestingly, both\nmethods exhibit a similar trend: accuracy peaks\nwhen using six continuous thoughts. We attribute\nMethod\nGSM8k-Aug\nGSM8k-Aug-NL\nTime (#Tokens)\nTime (#Tokens)\nGPT-2\nCoT-SFT\n0.17s (25.1)\n0.36s (62.1)\nNo-CoT-SFT\n0.035s (0)\n0.035s (0)\nCODI\n0.062s (8)\n0.062s (8)\nLLaMA-1b\nCoT-SFT\n0.73s (25.4)\n1.62s (68.8)\nNo-CoT-SFT\n0.16s (0)\n0.16s (0)\nCODI\n0.27s (8)\n0.27s (8)\nTable 2: Efficiency comparison of different reasoning\nmethods in terms of inference time per math problem\non GSM8k. Measured with batch size = 1 on an Nvidia\nA100 GPU. CoT Token counts are shown in parentheses.\nthis to the dataset’s structure, specifically the av-\nerage number of CoT steps. When fewer than six\ncontinuous thoughts are used, the model lacks suf-\nficient expressiveness to capture reasoning steps\neffectively. Conversely, beyond six, the additional\ncomplexity may not provide further benefits, as\nmost problems do not require additional reasoning\nsteps. Instead, the increased sequence length in-\ntroduces optimization challenges, outweighing any\npotential gains.\nFigure 3: Accuracy on GSM8k against the number of\ncontinuous thought tokens used during training.\n6\n\n\nMethods (GPT-2)\nAccuracy(%)\nNo-CoT\n19.1%\nCODI\n43.7%\n- ind. static teacher\n27.1%\nw/ multitask student\n42.2%\n- ind. trained teacher\n−\nw/ multitask student\n42.7%\n- w/o L1 loss\n24.5%\n- w/ CoT last step\n31.7%\n- w/o Projection\n42.5%\nTable 3: Ablation studies. ind. static teacher refers\nto training an independent teacher model beforehand.\nw/ multitask student extends it by allowing the student\nmodel to also learn CoT generation. ind. trained teacher\nrefers to training an independent teacher model along\nwith the student model.\n4.3\nAblation Studies\nIndependent Teacher.\nTo evaluate the need of\nself-distillation, we tested settings where the stu-\ndent does not share the model with the teacher\n(Table 3). Without learning explicit CoT genera-\ntion (ind. static teacher), the model performs\nbadly and fails to generate meaningful continu-\nous CoTs after decoding. Adding an explicit CoT\ngeneration objective (w/ multitask student)\nsignificantly restores performance, indicating the\nimportance of reference learning. Additionally,\ntraining the teacher alongside the student (ind.\ntrained teacher) leads to better results than us-\ning a pre-trained, static teacher (ind.\nstatic\nteacher), supporting the argument of the warm-\nup effect. Finally, using a unified model (CODI)\noutperforms maintaining separate teacher-student\nmodels (ind. trained teacher), reinforcing the\nidea that shared model representations help miti-\ngate alignment issues in hidden states.\nDistillation Loss.\nTable 3 shows that removing\nthe L1 loss (Equation 4) linking the teacher and\nstudent processes (w/o L1 Loss) leads to a signifi-\ncant performance drop, indicating the importance\nof supervision from the distillation token. While\nthe model still performs well in CoT generation, it\nfails to integrate this skill into continuous CoT rea-\nsoning, treating them as independent tasks rather\nthan a unified reasoning process.\nOthers.\nKeeping the final step of the CoT chain\nappears to negatively impact performance, support-\ning our claim that it provides shortcuts. Further-\nmore, the projection layer of continuous thought\ntokens (shown as the MLP layer before each of\nthe continous thought token z1 · · · zk in Figure 2)\nenhances CODI’s effectiveness, likely by helping\nto discriminate discrete and continuous CoT repre-\nsentations.\n5\nFurther Analysis\n5.1\nInterpretability Analysis\nInterpreting CODI’s continuous thoughts is inher-\nently challenging because these representations\nlack explicit imitation targets. However, CODI\nexhibits an ability to produce observable intermedi-\nate results (Figure 4) within its continuous thoughts\nby projecting its last hidden state into vocabulary\nspace via the model’s word embeddings – treating it\nin the same way as a standard text token. Addition-\nally, the corresponding operands contributing to\nthese intermediate results can often be found in the\nattended tokens of the latent representation. For\nexample, the second thought token, z2, attends to\nboth \"1\" and \"7\" to produce the decoded token \"7\".\nWhile the operator itself (e.g., ×) is not explicitly\nvisible in the attention mechanism—since opera-\ntors are in the context—it is reasonable to infer\nthat the transformer layers implicitly perform this\noperation. Another interesting observation is that\neach intermediate result is separated by a seemingly\nmeaningless continuous token. We hypothesize\nthat these tokens act as placeholders or transitional\nstates during the computation of intermediate re-\nsults. This aligns with the idea that the transformer\nmay require multiple passes to complete the calcu-\nlation for each intermediate step. More case studies\nare in the Appendix E.\nTotal Steps\n1\n2\n3\nAccuracy\n97.1%\n83.9%\n75.0%\nTable 4: CODI’s top-5 intermediate results matching\nreference CoT across problems requiring different num-\nbers of step.\nBeyond the case study, we aim to establish that\nCODI’s interpretability is a general pattern by an\naccuracy metric. We extract all correctly predicted\nanswers, decode the corresponding intermediate\nresults, and compare them against the reference\nintermediate solutions. Table 4 reveals that when\nthere is only one intermediate result, CODI cor-\nrectly matches the reference 97.1% of the time. For\nCoT sequences with lengths up to 3, CODI con-\nsistently achieves over 75% accuracy in decoding\nvalid intermediate results. These findings high-\nlight CODI’s reliability in generating meaningful\nintermediate reasoning steps, demonstrating its po-\ntential to effectively handle reasoning tasks with\ninterpretable intermediate outputs.\n7\n\n\nFigure 4: A case study illustrating CODI’s interpretability by analyzing its attended tokens and decoded tokens\nof each of the six latent thought tokens, z1 · · · z6. Attended tokens: these represent the top-10 tokens that the\ncontinuous thought attends to when generating the next thought/token. Some attended tokens appear in the form\nof ‘zi = x’, indicating attention to the i-th continuous thought. Here x represents the top-1 token that the latent\nthought maps to in vocabulary space. Decoded tokens: these are the top-5 words that the continuous thoughts are\nprojected back to in vocabulary space by multiplying them with the vocabulary embeddings.\n5.2\nCODI’s Pattern Learning\nGPT-2\nCODI\nCoconut\nRes\nOp-Res\nAccuracy\n43.7%\n34.1%\n34.0%\n35.7%\nTable 5: Comparison of GPT-2 finetuned on two datasets\nderived from CODI’s decoded thoughts. Res: using\nintermediate results as CoT. Op-Res: using intermediate\noperators and results as CoT.\nGiven that CODI’s continuous thoughts can of-\nten be decoded into intermediate results, it raises\na question: is CODI effectively equivalent to a\nGPT-2 fine-tuned on a dataset containing CODI’s\ndecoded patterns? We created a dataset contain-\ning only intermediate results (e.g., “CoT: 20, 7,\n27. Result: 9” translated from the case study\nin Figure 4). Additionally, since some cases of\nCODI show decoded operators like ‘×’ and ‘−’ in-\nterleaved with intermediate results, we also create a\nsynthetic CoT dataset that includes both operators\nand results (e.g., “CoT: ×, 20, ×, 7, +, 27.\nResult: 9”). As shown in Table 5, while models\ntrained on the two synthetic datasets outperform\nthe No-CoT baseline, they give significantly infe-\nrior results compared to CoT and CODI, though\nperform on par with Coconut. These result sug-\ngest that CODI learns richer information from the\nteacher task through distillation than pure imitation\non language-level intermediate results alone, high-\nlighting the advantages of our training framework.\n6\nConclusion\nWe introduced CODI, a novel paradigm for reason-\ning in the continuous space. Our extensive experi-\nments demonstrate CODI’s effectiveness as the first\ncontinuous CoT method to match the performance\nof explicit CoT, while achieving a high compres-\nsion ratio. Furthermore, CODI shows its scalability,\nrobustness, generalizability to more complex CoT\ndata, while retaining interpretability. Future re-\nsearch should explore CODI’s application to more\ndiverse and challenging tasks. A promising direc-\ntion is the integration of multimodality, leverag-\ning continuous embeddings for seamless modality\nmerging. We hope this work inspires further ex-\nploration into reasoning in representations more\ncompact than language, paving the way for more\nefficient and versatile reasoning paradigms.\n8\n\n\n7\nLimitations\nIn the paper, our approach focuses on knowledge\ntransfer by probing the token (”:”) responsible for\ngenerating the first answer token. However, this\nchoice may be suboptimal, as some answers begin\nwith “-”, and removing such cases improves perfor-\nmance, suggesting that critical reasoning informa-\ntion might also reside in the token generating the\nsecond answer token. Additionally, probing the to-\nken that concludes the CoT reasoning—potentially\nsummarizing the entire process—could offer alter-\nnative supervision signals. Furthermore, the cur-\nrent answer prompt, “The answer is:”, is an arbi-\ntrary design choice that may influence the effec-\ntiveness of knowledge transfer. Investigating these\naspects further could enable CODI to extend its\ndistillation framework to broader reasoning tasks.\nAnother limitation of the current continuous\ntraining approach is the absence of intermediate\ngradients until the end of the sequence. With six\ncontinuous thought tokens, the first token’s gradi-\nent is backpropagated from six or more steps away\n(specifically, from the token generating the final\nanswer), which may introduce optimization chal-\nlenges. This issue could become more pronounced\nwhen scaling to more complex problems requiring\nlonger continuous reasoning chains.\nFurthermore, CODI’s current configuration is\nfully deterministic, whereas one advantage of CoT\nis its inherent stochasticity, which allows models to\nimprove performance through self-consistency, tree\nsearch, and Monte Carlo Tree Search (MCTS). In-\ntroducing controlled randomness into CODI could\nrestore this property. Potential solutions include in-\ntegrating dropout layers, leveraging Variational Au-\ntoencoders (VAEs), or applying diffusion models\natop the current projection layer to enable diverse\nand robust reasoning paths.\nFinally, our studies primarily focus on mathe-\nmatical problems, as mathematical CoT training\ndata are more abundant and mathematical problems\ninherently require complex reasoning. However,\nthis focus may limit the generalizability of CODI\nto broader reasoning scenarios. Future work should\nexplore its applicability to more diverse reasoning\ntasks beyond mathematical domains.\nAcknowledgments\nThis work was supported by the UK Engineer-\ning and Physical Sciences Research Council (EP-\nSRC) through a Turing AI Fellowship (grant no.\nEP/V020579/1, EP/V020579/2). ZS is supported\nby a PhD studentship provided by the Chinese\nScholarship Council. The authors acknowledge\nthe use of King’s Computational Research, Engi-\nneering and Technology Environment (CREATE)\nat King’s College London.\nReferences\nM. Amalric and S. Dehaene. 2016. Origins of the brain\nnetworks for advanced mathematics in expert mathe-\nmaticians. Proceedings of the National Academy of\nSciences, 113(18):4909–4917.\nM. Amalric and S. Dehaene. 2019. A distinct cortical\nnetwork for mathematical knowledge in the human\nbrain. NeuroImage, 189:19–31. Epub 2019 Jan 3.\nAnthropic. 2024. Claude 3.5 sonnet.\nJeffrey Cheng and Benjamin Van Durme. 2024. Com-\npressed chain of thought: Efficient reasoning through\ndense representations. Preprint, arXiv:2412.13171.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. ArXiv, abs/2110.14168.\nMichael Crawshaw. 2020.\nMulti-task learning\nwith deep neural networks:\nA survey.\nArXiv,\nabs/2009.09796.\nYuntian Deng, Yejin Choi, and Stuart Shieber. 2024.\nFrom explicit cot to implicit cot: Learning to inter-\nnalize cot step by step. ArXiv, abs/2405.14838.\nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul\nSmolensky, Vishrav Chaudhary, and Stuart Shieber.\n2023. Implicit chain of thought reasoning via knowl-\nedge distillation. ArXiv, abs/2311.01460.\nYijiang River Dong, Hongzhou Lin, Mikhail Belkin,\nRamon Huerta, and Ivan Vuli´c. 2024.\nUndial:\nSelf-distillation with adjusted logits for robust un-\nlearning in large language models.\nPreprint,\narXiv:2402.10052.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen,\nand Furu Wei. 2024. In-context autoencoder for con-\ntext compression in a large language model. In The\nTwelfth International Conference on Learning Repre-\nsentations.\nGoogle. 2024. Our next-generation model: Gemini 1.5.\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and\nDacheng Tao. 2021.\nKnowledge distillation: A\nsurvey. International Journal of Computer Vision,\n129(6):1789–1819.\n9\n\n\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Kr-\nishna Menon, Sanjiv Kumar, and Vaishnavh Nagara-\njan. 2024. Think before you speak: Training lan-\nguage models with pause tokens. In The Twelfth\nInternational Conference on Learning Representa-\ntions.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li,\nZhiting Hu, Jason Weston, and Yuandong Tian. 2024.\nTraining large language models to reason in a contin-\nuous latent space. Preprint, arXiv:2412.06769.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2023.\nLarge language models are reasoning teachers. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 14852–14882, Toronto, Canada.\nAssociation for Computational Linguistics.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay\nKrishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-\ntilling step-by-step! outperforming larger language\nmodels with less training data and smaller model\nsizes. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 8003–8017,\nToronto, Canada. Association for Computational Lin-\nguistics.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\nA math word problem repository. In Proceedings of\nthe 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1152–1157, San\nDiego, California. Association for Computational\nLinguistics.\nDongfang Li, zhenyu liu, Xinshuo Hu, Zetian Sun, Bao-\ntian Hu, and Min Zhang. 2024a. In-context learning\nstate vector with inner and momentum optimization.\nIn The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems.\nZongqian Li, Yixuan Su, and Nigel Collier. 2024b.\n500xcompressor:\nGeneralized prompt compres-\nsion\nfor\nlarge\nlanguage\nmodels.\nPreprint,\narXiv:2408.03094.\nZicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing\nWang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu\nYang, and Zhaopeng Tu. 2025. Critical tokens matter:\nToken-level contrastive estimation enhances llm’s rea-\nsoning capability. Preprint, arXiv:2411.19943.\nSheng Liu, Haotian Ye, Lei Xing, and James Y. Zou.\n2023. In-context vectors: Making in context learning\nmore effective and controllable through latent space\nsteering. ArXiv, abs/2311.06668.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nWilliam Merrill and Ashish Sabharwal. 2024. The ex-\npressive power of transformers with chain of thought.\nIn The Twelfth International Conference on Learning\nRepresentations.\nMeta. 2024. The llama 3 herd of models. Preprint,\narXiv:2407.21783.\nOpenAI. 2024. Hello gpt-4o.\nHadas Orgad, Michael Toker, Zorik Gekhman, Roi Re-\nichart, Idan Szpektor, Hadas Kotek, and Yonatan Be-\nlinkov. 2025. LLMs know more than they show: On\nthe intrinsic representation of LLM hallucinations. In\nThe Thirteenth International Conference on Learning\nRepresentations.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094, Online.\nAssociation for Computational Linguistics.\nJacob Pfau, William Merrill, and Samuel R. Bowman.\n2024. Let’s think dot by dot: Hidden computation\nin transformer language models. In First Conference\non Language Modeling.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1743–1752, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nLena Strobl, William Merrill, Gail Weiss, David Chi-\nang, and Dana Angluin. 2024.\nWhat formal lan-\nguages can transformers express? a survey. Transac-\ntions of the Association for Computational Linguis-\ntics, 12:543–561.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\n10\n\n\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508, Toronto, Canada. Association\nfor Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824–24837. Curran Associates,\nInc.\nXiaohan Xu, Ming Li, Chongyang Tao, Tao Shen,\nReynold Cheng, Jinyang Li, Can Xu, Dacheng Tao,\nand Tianyi Zhou. 2024. A survey on knowledge\ndistillation of large language models.\nPreprint,\narXiv:2402.13116.\nZhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang,\nWei Chen, Minfeng Zhu, and Qian Liu. 2024. Self-\ndistillation bridges distribution gap in language\nmodel fine-tuning. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1028–\n1043, Bangkok, Thailand. Association for Computa-\ntional Linguistics.\nA\nImplementation Details\nFor all experiments, we use the AdamW optimizer\n(Loshchilov and Hutter, 2019) with a cosine sched-\nuler (without cycles) and a linear warm-up over the\nfirst 3% of steps. The effective batch size is 128.\nBoth α and β are set to 1 (Equation 1). We ap-\nply LoRA (Hu et al., 2022) finetuning with a rank\nof 128 and an alpha value of 32, using bfloat16\nprecision.\nFor GPT-2, we set the learning rate to 3e-3 and\nγ to 1. Training runs for 40 epochs, taking approx-\nimately 36 hours on a single A100 (80GB).\nFor LLaMA-3.2-1b, we use a learning rate of 8e-\n4 and set γ to 20, as we observe that its distillation\nloss has a much smaller magnitude. The model is\ntrained for 10 epochs, requiring approximately 48\nhours on a single A100 (80GB).\nFor iCoT training of GPT-2, we use a learning\nrate of 5e-5 and train for 100 epochs, removing 4\ntokens per epoch for GSM8k-Aug-NL. For iCoT\ntraining of LLaMA-1b, we use a learning rate of\n1e-5 and train for 50 epochs, removing 8 tokens per\nepoch for GSM8k-Aug and 16 tokens per epoch\nfor GSM8k-Aug-NL.\nB\nProof: CoTs Contribute a Shift in\nHidden Activation\nIn this section, we provide a proof to demonstrate\nwhy Chain-of-Thought (CoT) contributes a shift\nin hidden activation. This proof is largely inspired\nby the work of (Li et al., 2024a), which analyzed\nIn-Context Learning.\nIn a typical CoT training dataset, the input usu-\nally consists of four components: the question Q,\nthe rationale R, the prompt for the answer P (e.g.,\n\"The answer is:\"), and the final answer A.\nWe analyze the attention activation of the last\nprompt token, q—in this case, \":\"—at the l-th trans-\nformer layer. The output activation al from the\nattention heads of this token is given by:\nal = WV [Q; R; P]softmax(WK[Q; R; P]T q\n√\nd\n)\n(6)\nwhere WK and WV are the model’s key and\nvalue parameters, [Q; R; P] represents the concate-\nnation of the three inputs, and\n√\nd is a scaling fac-\ntor.\nFor simplicity of analysis, inspired by (Li et al.,\n2024a), we omit the softmax operation and the scal-\ning factor, as these do not affect the core conclusion.\nWith this simplification, the following derivation\nholds:\nal ≈WV [Q; R; P]WK[Q; R; P]T q\n=\n\u0010\nWV Q(WV Q)T + WV R(WV R)T\n+ WV P(WV P)T \u0011\nq\n=\n\u0010\nWV [Q; P](WV [Q; P])T\n+ WV R(WV R)T \u0011\nq\n=\n\u0010\nWno-CoT + WV R(WKR)T \u0011\nq\n= al\nno-CoT + WV R(WKR)T q\nHere,\nWno-CoT\nis\ndefined\nas\nWV [Q; P](WK[Q; P])T ,\naccounting\nfor\nthe\ncontribution of Q and P without the CoT rationale.\nCorrespondingly, al\nno-CoT represents the attention\nactivation excluding CoT.\nThe additional term WV R(WKR)T q represents\nthe contribution of the CoT rationale R to the hid-\nden activation. We can get the hidden activation\nby transforming the attention activation by a non-\nlinear function f:\nhl ≈hl\nno-CoT + f\n\u0010\nWV R(WKR)T q\n\u0011\n(7)\nThus, we conclude that the rationale R in the\nCoT primarily contributes a shift in hidden acti-\nvation values, emphasizing its role as an additive\n11\n\n\nfactor in the latent representation. This shift can be\neffectively captured and learned using a distance\nmetric.\nC\nDatasets\nWe provide examples and statistics of training\ndatasets and evaluation benchmarks.\nC.1\nExamples\nGSM8k-Aug*\nQuestion = \"Jen shared a pack of\nchocolates among her friends.\nShe\ngave 20% to Lucy, 30% to Sarah and\nthe remaining were shared equally\namong\nfour\nothers.\nIf\nthe\npack\ncontained 100 chocolates, how many\nchocolates were each of the four\nothers getting?\"\nCoT = \"The total percentage given to\nLucy and Sarah is 20% + 30% = 50%.\nSo, the remaining percentage that\nwas shared among the others is 100%\n- 50% = 50%.\nThe total number of\nchocolates shared among the others\nis 100 * 50 / 100 = 50 chocolates.\nSo, each of the four others received\n50 / 4 = 12.5 chocolates.\"\nAnswer = \"12.5\"\nGSM8k-Aug\nQuestion = \"Out of 600 employees\nin\na\ncompany,\n30%\ngot\npromoted\nwhile 10% received bonus. How many\nemployees\ndid\nnot\nget\neither\na\npromotion or a bonus?\"\nCoT\n=\n\"«600*30/100=180»\n«600*10/100=60»\n«180+60=240»\n«600-240=360»\"\nAnswer = \"360\"\nSVAMP\nQuestion = \"There are 87 oranges and\n290 bananas in Philip’s collection.\nIf the bananas are organized into\n2 groups and oranges are organized\ninto 93 groups.\nHow big is each\ngroup of bananas?\" Answer = \"145\"\nMultiArith\nQuestion = \"There are 64 students\ntrying out for the school’s trivia\nteams.\nIf 36 of them didn’t get\npicked for the team and the rest\nwere put into 4 groups, how many\nstudents would be in each group?\"\nAnswer = \"7\"\nGSM-Hard\nQuestion = \"Janet’s ducks lay 16\neggs\nper\nday.\nShe\neats\nthree\nfor\nbreakfast\nevery\nmorning\nand\nbakes muffins for her friends every\nday with 4933828.\nShe sells the\nremainder at the farmers’ market\ndaily for $2 per fresh duck egg.\nHow much in dollars does she make\nevery day at the farmers’ market?\"\nAnswer = \"-9867630.0\"\nC.2\nStatistics\nThe statistics of training data are shown in Table\nA1, and the statistics of evaluation benchmarks are\nshown in Table A2.\nTraining Dataset\nNum. Data\nAvg. CoT Tokens\nGSM8k-Aug\n385,620\n20.3\nGSM8k-Aug*\n384,625\n49.0\nTable A1: Training data statistics.\nEvaluation Benchmark\nNum. Data\nGSM8k\n1319\nSVAMP\n1000\nGSM-Hard\n1319\nMultiArith\n500\nTable A2: Evaluation Benchmark statistics.\nD\nPerformance on Training Data\nThe training accuracy of CODI and CoT-SFT is\nshown in Table A3.\nE\nInterpretability Case Studies\nMore case studies on the interpretability of CODI\nare provided in Figure A1 and Figure A2\n12\n\n\nGPT-2\nLLaMA1b\nCoT-SFT\n99.1%\n98.9%\nCODI (Ours)\n81.5%\n95.0%\nTable A3: Training Accuracies on GSM8k-Aug.\nF\nCODI Code\nThe example Python code of CODI is illustrated in\nFigure A3.\n13\n\n\nFigure A1: CODI’s interpretability on problems involving two steps.\nFigure A2: CODI’s interpretability on problems involving one step.\n14\n\n\nclass ContinuousCoTviaKnowledgeDistillation:\ndef __init__(self,):\nself.num_latent = 6\nself.alpha, self.beta, self.gamma = 1, 1, 1\nself.llm = get_gpt2_model()\nself.prj = nn.Sequential(\nnn.Linear(hidden_dim, hidden_dim),\nnn.GELU(),\nnn.Linear(hidden_dim, hidden_dim),\n)\ndef forward(x, y, x_cot_y):\n# teacher learning\ny_teacher = self.llm(x_cot_y)\nteacher_ce_loss = cross_entropy(y_teacher, x_cot_y) # loss1\n# student learning\nlatent = self.llm(torch.cat([x, bot_token], dim=1))[:, -1]\nlatent = self.prj(latent)\npast_key_values = latent.past_key_values\n# continuous CoT reasoning\nfor i in range(self.num_latent):\nlatent = self.llm(latent, past_key_values)\nlatent = self.prj(latent)\npast_key_values = latent.past_key_values\ny_student = self.llm(torch.cat([eot_token, y], dim=1), past_key_values)\nstudent_ce_loss = cross_entropy(y_student, y) # loss2\n# knowledge distillation\nknowledge_distillation_loss = smooth_l1_loss(\ny_teacher.hidden_states[:, teacher_exact_answer_token_position-1],\ny_student.hidden_states[:, student_exact_answer_token_position-1]\n) # loss3\n# normalisation\nknowledge_distillation_loss /= y_teacher.hidden_states[:,\nteacher_exact_answer_token_position-1].std()\nreturn self.alpha*teacher_ce_loss + self.beta*student_ce_loss + self.gamma*\nknowledge_distillation_loss\nFigure A3: Example Python code illustrating the ContinuousCoTviaKnowledgeDistillation class.\n15\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21074v1.pdf",
    "total_pages": 15,
    "title": "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation",
    "authors": [
      "Zhenyi Shen",
      "Hanqi Yan",
      "Linhai Zhang",
      "Zhanghao Hu",
      "Yali Du",
      "Yulan He"
    ],
    "abstract": "Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling\nstep-by-step reasoning in natural language. However, the language space may be\nsuboptimal for reasoning. While implicit CoT methods attempt to enable\nreasoning without explicit CoT tokens, they have consistently lagged behind\nexplicit CoT method in task performance. We propose CODI (Continuous\nChain-of-Thought via Self-Distillation), a novel framework that distills CoT\ninto a continuous space, where a shared model acts as both teacher and student,\njointly learning explicit and implicit CoT while aligning their hidden\nactivation on the token generating the final answer. CODI is the first implicit\nCoT method to match explicit CoT's performance on GSM8k while achieving 3.1x\ncompression, surpassing the previous state-of-the-art by 28.2% in accuracy.\nFurthermore, CODI demonstrates scalability, robustness, and generalizability to\nmore complex CoT datasets. Additionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its reasoning process transparent. Our\nfindings establish implicit CoT as not only a more efficient but a powerful\nalternative to explicit CoT.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}