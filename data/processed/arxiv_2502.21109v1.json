{
  "id": "arxiv_2502.21109v1",
  "text": "“No negatives needed”: weakly-supervised regression for interpretable\ntumor detection in whole-slide histopathology images\nMarina D’Amato1, Jeroen van der Laak1, Francesco Ciompi1\n1Computational Pathology Group, Radboud University Medical Center, Nijmegen, The Netherlands\nCorresponding author: marina.damato@radboudumc.nl\nAbstract\nAccurate tumor detection in digital pathology whole-slide images (WSIs) is crucial for cancer diagnosis and treatment\nplanning. Multiple Instance Learning (MIL) has emerged as a widely used approach for weakly-supervised tumor detection\nwith large-scale data without the need for manual annotations. However, traditional MIL methods often depend on\nclassification tasks that require tumor-free cases as negative examples, which are challenging to obtain in real-world clinical\nworkflows, especially for surgical resection specimens. We address this limitation by reformulating tumor detection as a\nregression task, estimating tumor percentages from WSIs, a clinically available target across multiple cancer types. In this\npaper, we provide an analysis of the proposed weakly-supervised regression framework by applying it to multiple organs,\nspecimen types and clinical scenarios. We characterize the robustness of our framework to tumor percentage as a noisy\nregression target, and introduce a novel concept of “amplification technique” to improve tumor detection sensitivity when\nlearning from small tumor regions. Finally, we provide interpretable insights into the model’s predictions by analyzing\nvisual attention and logit maps.\n1\nIntroduction\nEarly and accurate cancer diagnosis is crucial for success-\nful treatment and improved patient outcomes. Tradition-\nally, histopathological analysis, where pathologists exam-\nine tissue sections under a microscope, serves as the gold\nstandard for diagnosing tumors. However, this process is\nlabor-intensive, time-consuming and subject to inter-reader\nvariability ([30], [21], [24]).\nThe digitization of whole-slide images (WSIs) has enabled\nthe development of automated analysis methods using ar-\ntificial intelligence (AI) and deep learning, which can en-\nhance diagnostic efficiency and accuracy [29]. Automating\ntumor detection can alleviate the workload on pathologists\npotentially improving the diagnostic process and reducing\nturnaround time. Traditional deep learning models for tu-\nmor detection rely heavily on fully-supervised approaches,\nnecessitating detailed pixel- or patch-level annotations ([32],\n[19], [11]). This dependency on extensive annotations poses\nchallenges in scalability and generalizability, primarily due\nto the scarcity of annotated datasets and the limited avail-\nability of expert pathologists.\nWeakly-supervised learning (WSL), which requires only\ncoarse annotations or slide-level labels, has emerged as a\nviable alternative to fully-supervised models, substantially\nreducing the annotation burden. While various WSL ap-\nproaches exist, including those based on image compression\n[28, 2] and streaming techniques [25, 12], Multiple Instance\nLearning (MIL) ([7], [15]) has emerged as the predominant\nframework in digital pathology. MIL treats each WSI as a\n”bag” containing image patches (”instances”), where only\nthe bag-level label is available (e.g., tumor presence or ab-\nsence). MIL has been successfully applied to binary classifi-\ncation tasks across various cancer types, including prostate\ncancer and basal cell carcinoma ([6]), breast cancer ([20]),\ncolorectal cancer ([9]), and skin cancer ([16]), achieving per-\nformance comparable to fully-supervised models without\nthe need for manual annotations.\nDespite its advantages, most investigations have re-\nmained confined to classification problems with categorical\noutcomes, such as tumor presence or absence.\nA signifi-\ncant limitation in tumor detection formulated as a weakly-\nsupervised binary classification task is the need for large\ndatasets containing both positive (tumor present) and neg-\native (tumor absent) cases. However, obtaining WSIs com-\npletely tumor-free in real-world clinical settings can be chal-\nlenging as most resected specimens typically contain at least\nsome degree of tumor.\nWhile biopsies and lymph nodes\ninclude both tumor-positive and tumor-negative cases, fo-\ncusing on these tissues alone introduces biases and limits\nmodel generalizability. Unlike resections, these samples do\nnot capture the full morphological diversity of tumors or\ntheir surrounding microenvironments. Since surgical speci-\nmens offer a broader and more representative range of tu-\nmor variations, they are a more suitable focus for large-scale\napplications. However, the scarcity of completely negative\nresections hinders the scalability of classification-based tu-\nmor detection models.\nTo address this challenge, researchers have explored alter-\nnative targets for weakly-supervised learning, such as tumor\npercentages. In clinical practice, pathologists routinely de-\nlineate the tumor bed on WSIs as a preparatory step for\nmolecular diagnostics, as shown in the first step of Figure1.\nThis involves marking the approximate tumor area, often\nusing a pen marker, and visually estimating the percent-\nage of tumor cells within the annotated region to ensure\nadequate tumor content for molecular testing.\nBuilding on this workflow, [22] proposed a weakly-\nsupervised learning approach that leverages the tumor cell\npercentage provided by pathologists as a proxy to train\na weakly-supervised segmentation algorithm derived from\nMIL. Their method uses the percentage of tumor cells within\nthe tumor bed to assign proxy instance-level labels to im-\nage patches to approximate segmentation maps that match\nthe global percentage estimate.\nHowever, the tumor cell\n1\narXiv:2502.21109v1  [eess.IV]  28 Feb 2025\n\n\nInput WSI\nTissue Segmentation\nMarker Segmentation\nTumor Area\nPercentage Tumor Area: 26.57%\nFigure 1: Visual example of a procedure to extract an approximate tumor percentage area from a slide used in molecular\ndiagnostics procedures. From a coarse annotation of the tumor area, often provided with a pen marker, we can derive\nthe tumor percentage via simple image analysis steps: 1) segmentation of foreground tissue versus background, 2)\nidentification of the pen marker and area filling, 3) intersection of tissue and marker area, 4) calculation of the tumor\npercentage.\npercentage does not always accurately represent the true\nspatial extent of the tumor in the slide. Moreover, their\nwork primarily focused on frozen sections and showed lim-\nited success when applied to data from routine diagnostics\nbased on formalin-fixed, paraffin-embedded (FFPE) tissue\nfrom two test cohorts.\nWe propose to shift the focus from tumor cell percent-\nages to tumor area percentages, under the hypothesis that\ntumor area percentage provides a more informative learn-\ning signal for tumor detection, as it better reflects the actual\ndistribution of the tumor within the tissue. By leveraging\nthe pathologist-annotated tumor areas as a starting point,\nwe employ simple image analysis steps (illustrated in Fig-\nure 1, more information in the Supplementary Material) to\ncalculate the percentage of tumor area relative to the en-\ntire tissue section. We then use this percentage as a direct\ntarget to train a regression MIL framework, an approach\nthat has been previously used for stromal tumor-infiltrating\nlymphocytes ([27]), inferring gene expression profiles ([31],\n[17]), survival prediction ([8]), and molecular biomarker pre-\ndiction ([14]), but never for tumor detection.\n1.1\nOur contributions\nOur key contributions are as follows. First, we introduce a\nnovel application of Multiple Instance Learning (MIL) for\ntumor detection with weakly-supervised regression, circum-\nventing the need for extensive annotations and tumor-free\ncases. We explore the applicability of this approach on var-\nious tissue types and compare various weakly-supervised\nmethods in a regression setting, examining the impact of\ndifferent models and pooling strategies.\nSecond, we acknowledge that visual estimation of the tu-\nmor area made by pathologists is subject to variability and\nnoise. To address this, we conduct a robustness analysis to\nevaluate the performance when building our model under\ndifferent levels of synthetic noise in the tumor percentage\ntargets. This analysis is crucial for ensuring the reliability\nof our approach in real-world clinical settings, where such\nvariability is unavoidable.\nAdditionally, we introduce and analyze the effectiveness\nof a non-linear transformation of the target, which we refer\nto as the “amplification technique” to enhance the training\nprocess of our regression models, particularly when dealing\nwith cases involving small lesions.\nLastly, we assess the interpretability of our models by\ncomparing raw prediction maps based on instance-level pre-\ndictions with attention maps derived from attention scores.\nThese heatmaps provide valuable insights into the model’s\ndecision-making process, aligning with the clinical need for\nFigure 2: Examples of WSIs paired with manual annota-\ntions (when available) or tumor segmentation masks, illus-\ntrating the computed tumor percentages. The first column\npresents the input WSIs, the second column displays either\nmanual annotations (for CAM16) or the raw output of the\nsegmentation algorithm, and the third column showcases\nthe binarized maps highlighting the tumor regions.\nexplainable AI in medical image analysis.\nWe conduct a\nquantitative evaluation of the interpretability performance\nby comparing the heatmaps with ground truth annotations.\n2\nMaterials\nWe evaluated weakly-supervised regression models on five\ndatasets consisting of WSIs stained with hematoxylin and\neosin (H&E), covering a diverse range of specimen types, in-\ncluding surgical resections, lymph nodes, and biopsies from\ndifferent organs. Figure 2 provides a visual overview of the\ninput data and computed tumor percentages.\n2\n\n\nBreast TNBC\nThe TNBC dataset [3] includes 595 cases\nof triple negative breast cancer (TNBC) surgical resections\nfrom an equal amount of patients. This dataset was con-\nstructed from a multicenter, retrospective cohort study and\nencompasses patients diagnosed with TNBC between 2006\nand 2014 from several hospitals in the Eastern Netherlands.\nThe slides were scanned using a Pannoramic 1000 DX scan-\nner (3DHISTECH) at a pixel resolution of 0.24 µm. Tumor\npercentages in this dataset were computed using the pub-\nlicly available HookNet algorithm [26], as manual annota-\ntions were not available. HookNet demonstrated strong seg-\nmentation performance on invasive tumors, achieving Dice\nscores of 0.9 and 0.91 for IDC and ILC, respectively, mak-\ning it well-suited for estimating tumor percentages in this\ndataset. All the slides in this dataset contain cancer with\npercentages ranging from 2% to 66%. The mean and me-\ndian percentage of tumor across the slides are 25% and 24%\nrespectively, indicating a relatively even distribution of tu-\nmor burden without significant skewness.\nCAMELYON16\nThe\nCAMELYON16\ndataset\n([13])\n(CAM16) consists of 399 WSIs of lymph node sections (one\nslide per patient) from two medical centers in the Nether-\nlands: Radboud University Medical Center (RUMC) and\nUniversity Medical Center Utrecht (UMCU). RUMC images\nwere scanned using a digital slide scanner (Pannoramic 250\nFlash II; 3DHISTECH) with a 20× objective lens, result-\ning in a specimen-level pixel size of 0.243 µm × 0.243 µm.\nUMCU images were scanned with a NanoZoomer-XR Dig-\nital slide scanner C12000-01 (Hamamatsu Photonics) using\na 40× objective lens, producing a pixel size of 0.226 µm ×\n0.226 µm. This dataset includes pixel-level annotations for\nboth macro-metastases and micro-metastases which we used\nto compute tumor percentages. Of the 399 slides, 160 con-\ntain metastases while the remaining depict normal tissue.\nThe percentages of tumor in tumor slides range from 0.003%\nto 71%, with 91 slides having a percentage less than 1%.\nThe mean percentage of tumor across the tumorous slides\nis approximately 5% while the median is only 0.4%, indicat-\ning a skewed distribution towards lower tumor percentages,\nhighlighting the challenge of detecting small metastases in\nthis dataset.\nEXAMODE Colon\nThe ExaMode colon dataset [10]\nconsists of 8556 H&E-stained colorectal biopsy WSIs cut\nfrom 6556 paraffin blocks of 3501 patients collected from\nthe Radboud University Medical Center between 2000 and\n2009. As the diagnoses were reported at the block level,\nslides from the same block were combined into a single new\nslide (“packed” slide [1]) by minimizing the area of back-\nground between sections. Pathology reports associated with\nthese slides provide labels for five classes: normal, hyper-\nplastic polyps, low-grade dysplasia (lgd), high-grade dyspla-\nsia (hgd), and cancer. For this study, we grouped normal\nand hyperplastic polyps into a single “normal” class, result-\ning in two classes: normal (4546 cases) and abnormal (lgd,\nhgd, and cancer; 2010 cases). Tumor percentages in abnor-\nmal cases were derived from segmentation maps created us-\ning a colon tissue segmentation algorithm [5] which demon-\nstrated high performance, achieving a Dice score of 0.89 for\ntumor segmentation. The percentages in this dataset range\nfrom 0.033% to 62%, with a mean of 14% and median of\n10%. The small size and sparse distribution of tumor areas\nin these slides pose significant challenges for accurate tumor\ndetection.\nAQUILA Colon\nThe AQUILA colon dataset includes\n571 WSIs of colon tissue from surgical resections, with one\nslide per patient. These slides were scanned at two institu-\ntions: Radboud University Medical Center (RUMC) and\nLaboratorium Pathologie Oost-Nederland (LabPON). As\nfor the EXAMODE cohort, tumor percentages were com-\nputed using the same colon tissue segmentation algorithm\n([5]), grouping the segmentation labels of high-grade dys-\nplasia (HGD), low-grade dysplasia (LGD), and tumor into\na single “abnormal” class, while all other tissue types were\nclassified as “normal.” All the slides in this dataset contain\ncancer with percentages ranging from 0.078% to 62%, with\na mean percentage of 13.33% and a median of 12.27%.\nCOBRA\nThe COBRA dataset ([16]) consists of 5147\nslides from 4066 patients obtained from Radboud Univer-\nsity Medical Center between 2016 and 2020.\nIt includes\npatients diagnosed with Basal Cell Carcinoma (BCC), epi-\ndermal dysplasia (actinic keratosis or Bowen’s disease), or\nbenign conditions.\nAmong the slides, 2661 contain BCC\ntumors, while 2476 are classified as non-BCC. All slides\nwere scanned using a 3DHistech Pannoramic 1000 scanner\nat 20×magnification (pixel resolution 0.24 µm). Tumor per-\ncentages were computed using an in-house skin tumor seg-\nmentation algorithm (more information in the Supplemen-\ntary Material) and range from 0.01% to 89%, with a mean\nof 16% and a median of 10%. This distribution suggests a\nmoderate range of tumor burden within the BCC-positive\nslides.\n3\nMethods\nWe investigated adaptations of the classical MIL classifi-\ncation approach [6] for regression tasks, using the tumor\npercentage within a WSI as the continuous target. In this\nsection, we first retrace the main concepts of MIL, we then\nintroduce the proposed approaches for MIL regression, the\nanalysis that we performed on the impact of noisy labels as\nwell as the motivation and the effect of the “amplification\ntechnique”.\n3.1\nMultiple Instance Learning\nIn Multiple Instance Learning, a WSI is treated as a bag\ncontaining multiple image patches, each referred to as an\ninstance xi, where i = 0, 1, ..., N and N represents the\ntotal number of patches in the slide. The task is to pre-\ndict a bag-level target y based on the instances, where each\npatch xi does not have a direct label but contributes to\nthe overall prediction. Instead of assigning individual la-\nbels to each patch, the model learns a bag-level prediction\nbased on the aggregated information from all patches within\nthe WSI. MIL methods are typically divided into two cate-\ngories: instance-based and embedding-based approaches, de-\npending on how the individual patches are processed and\naggregated. The instance-based method starts by extract-\ning features from each image patch in the WSI. These fea-\ntures are then passed through an instance-level prediction\nlayer, which generates a prediction for each patch indepen-\ndently. After obtaining the instance-level predictions, these\nare combined into a single prediction for the entire slide us-\ning a pooling mechanism, which can be as simple as mean\n3\n\n\npatch-level\nfeature extractor\ninstance-level features\ninstance-level\nprediction layer\n0.63\n0.76\n0.67\n0.72\n0.59\npooling\n0.67\ninstance-level predictions\n...\n...\nattention module\n0.1\n0.02\n... 0.3\n0.08 0.12\n...\nattention scores\npatches\ninput WSI\n🔥\n🔥\nFigure 3: Overview of the instance-based multiple instance learning (MIL) framework. The instance-based approach pro-\ncesses individual patches from the input image independently, making predictions at the instance level before aggregating\nthem.\npooling or more sophisticated, such as attention-weighted\npooling.\nIn this study, we focus on instance-based approaches to\nMIL (see Figure 3) because it aligns with the framework\nused in [22], where proxy-labels are assigned to individual\ninstances, and instance-level predictions are made.\nAd-\nditionally, we prioritize interpretability in our approach,\nwhich is facilitated by instance-based MIL since it allows\nfor direct visualization of individual patch predictions, en-\nabling a clearer understanding of how the model makes its\ndecisions.\n3.2\nMultiple instance learning to regress\ntumor percentage\nWe evaluate four different MIL approaches, each adapted\nto the task of tumor percentage regression and to the\ninstance-based formulation of MIL. These methods vary in\ntheir strategies for aggregating information from individual\npatches to predict the tumor percentage for the entire slide.\nMeanPool\nThe first method, Meanpool MIL (MeanPool),\nis a classical approach that uses the average of all patch\npredictions within a slide as the final tumor percentage,\nproviding a simple and intuitive way to combine information\nfrom all patches.\nAttention-based MIL\nThe second method, Attention-\nbased MIL (ABMIL), introduced by [20], leverages the at-\ntention mechanism as a pooling operator. It assign weights\n(attention scores) to each patch, indicating its relative im-\nportance for the final prediction. Higher scores signify that\nthe model considers the corresponding patch to be more\ninformative about the tumor content. ABMIL uses these\nattention scores to create a weighted average of the patch\npredictions, focusing more on informative regions within the\nWSI.\nCLAM\nBuilding upon ABMIL’s interpretability,\nthe\nthird\nmethod,\nClustering\nConstrained\nAttention\nMIL\n(CLAM), incorporates an instance clustering loss function\n([23]). This loss function encourages the model to identify\na subset of patches with high attention scores (likely con-\ntaining tumor) and a separate subset with low attention\nscores (likely non-tumor). These patches are then used for\nadditional training with pseudo-labels, potentially improv-\ning the model’s ability to differentiate between tumor and\nnon-tumor regions.\nIn this study, we adapt the original\nembedding-based CLAM formulation to an instance-based\napproach.\nWeSEG\nLastly, we examined Weakly Supervised Segmen-\ntation (WeSEG) ([22]). While previous methods employ a\ntwo-step approach, where a frozen encoder maps all patches\nto low-dimensional embeddings, WeSEG trains the encoder\nend-to-end to generate patch-level tumor probability maps\nusing weak labels. It employs a recursive training mech-\nanism to generate proxy labels and iteratively refines the\nmodel’s predictions. At every step, it creates proxy labels by\nassigning label 1 to the p% patches with the highest proba-\nbility and label 0 to the remaining ones, where p is the target\ntumor percentage. While WeSEG was not explicitly opti-\nmized to predict tumor percentages, we can derive percent-\nage estimates from its patch-level predictions. Specifically,\nwe convert patch-level probabilities to binary predictions\nusing a threshold optimized per experiment, then calculate\nthe tumor percentage as the ratio of patches classified as\ntumor to the total number of tissue patches.\nThis post-\nprocessing step allows us to compare the predicted tumor\npercentage directly with the target annotation.\n3.3\nEffect of noisy targets\nPathologists’ annotations of the tumor bed provide a valu-\nable basis for estimating tumor area percentages. However,\nthese annotations are inherently noisy due to their reliance\non coarse manual delineations and visual estimation.\nIn\nthis work, we benefit from more precise tumor area esti-\nmates based on detailed manual annotations or AI-assisted\ndelineation techniques ([26], [5]). To account for the inher-\nent variability in clinical practice, we introduce synthetic\nnoise into the tumor percentage labels during training. This\nreflects the imprecision that naturally arises from visual es-\ntimation, allowing us to assess the robustness of the frame-\nwork to noisy inputs, while testing and validation are per-\nformed using the true, unaltered labels.\nWe simulated three levels of noise, ranging from mild to\nsubstantial deviations, to model potential underestimation\nor overestimation by pathologists in real-world clinical sce-\nnarios. For each training example, we added noise to the\ntrue percentages by sampling from uniform distributions:\n4\n\n\n[-0.1, 0.1] for mild noise (±10%), [-0.3, 0.3] for moderate\nnoise (±30%), and [-0.5, 0.5] for substantial noise (±50%).\nFigure 4: The first panel illustrates the fifth root trans-\nformation, with annotated points demonstrating the effect\nof amplification on selected values, particularly enhancing\nlower percentages. The other five panels display the distri-\nbution of tumor percentages across various cohorts, shown\nonly for tumorous slides (excluding negative cases), before\nand after the amplification technique.\n3.4\nTarget amplification\nIn the context of our regression task for tumor detec-\ntion, cases with small lesions or low tumor percentages\ncan be particularly difficult for the model to distinguish\nfrom tumor-free cases, potentially leading to reduced per-\nformance.\nTo address this challenge, we employ a target\namplification technique designed to enhance the model’s\nsensitivity to small tumor percentages.\nThis technique involves transforming the target labels us-\ning a root transformation ˆy =\nn√y that expands lower-end\nvalues (small tumor percentages), making subtle differences\nbetween normal tissue and areas with small tumor lesions\nmore discernible. For this work, we selected the fifth root\ntransformation (n = 5), based on empirical results demon-\nstrating that it strikes a good balance between amplifying\nsmall tumor values without overly distorting higher tumor\npercentages.\nThis transformation is particularly valuable for datasets\nsuch as CAM16, COBRA, and ExaMode consisting of biop-\nsies and lymph nodes, which usually include either small\ntumor percentages or entirely tumor-free regions. Figure 4\nillustrates the distributions of tumor percentages before and\nafter amplification, demonstrating how the transformation\nexpands the range of small values, enhancing the separa-\ntion between tumor-free and low-tumor cases. On the other\nhand, datasets such as AQUILA and TNBC consist of sur-\ngical resections with no tumor-free cases, where tumor areas\nare relatively large. In these datasets, amplification is un-\nnecessary and was not applied in our analysis, as the clear\ndistinction between tumor regions does not require further\nenhancement.\n4\nExperiments\nData preprocessing\nWe first segmented the tissue from\nthe background using a pre-trained tissue segmentation net-\nwork ([4]). From the segmented tissue regions, we extracted\nnon-overlapping 256x256 patches at a spatial resolution of\n0.5 µm using the hs2p library ([18]).\nModel Training\nWith the exception of WeSEG, all in-\nvestigated methods follow a two-step process involving a\nfrozen convolutional neural network (CNN) encoder and a\nfinal classification layer (see Figure 3).\nIn the first step,\nwe used a modified ResNet50 pretrained on ImageNet as\na patch-level feature extractor, following the approach de-\nscribed by [23], where the model was adapted by applying\nadaptive mean-spatial pooling after the 3rd residual block.\nThe encoder’s weights were frozen, meaning they were not\nupdated during training. The second step involved training\nthe final layer for tumor percentage estimation. The models\nwere trained by minimizing the mean squared error (MSE)\nloss between the target and predicted tumor percentages\nwith the Adam optimizer. Training included L2 weight de-\ncay set at 10−5 and a learning rate of 2 × 10−4 was used.\nTraining proceeded for at least 50 epochs and up to a maxi-\nmum of 200 epochs, with early stopping implemented if the\nvalidation loss plateaued.\nUnlike previous methods that rely on fixed feature extrac-\ntion, WeSEG operates directly on the image patches using a\nmodel that learns to extract features during training. Here,\nthe pre-trained ResNet50 architecture was used for end-to-\nend training, where the feature representations were contin-\nuously refined with each iteration. At each training step, 30\ntiles were randomly sampled from each whole-slide image,\nas done in [22]. Due to computational limitations, we re-\nstricted the sampling to a single whole-slide image per step.\nBased on the original work, data augmentation techniques\nwere applied which included random vertical and horizontal\nflips, and color jittering with defined parameters for bright-\nness, contrast, saturation, and hue (as implemented in [22]).\nThe model adopted the Adam optimizer with binary cross-\nentropy loss, with weight decay of 10−5 and a learning rate\nof 0.0005.\nTraining spanned a maximum of 100 epochs,\nincorporating early stopping with a 50-epoch patience.\nAll experiments were conducted using a 5-fold cross val-\nidation strategy for each dataset to assess model perfor-\nmance generalizability. For each fold, we first selected the\ntest set, which consisted of 20% of the total slides. This en-\nsured that, across all 5 folds, the test sets collectively rep-\nresented the entire dataset, with no overlap between test\nsets. The remaining 80% of the slides were then split into\ntraining (85%) and validation sets (15%). Stratified sam-\npling was employed during the partitioning process to en-\nsure that the distribution of tumor percentages remained\nconsistent across the training, validation, and test sets. For\ncases containing multiple slides, where a ”case” refers to all\nthe slides from a single patient, all slides belonging to the\nsame case were assigned to the same fold. Additionally, for\nCAM16 and COBRA, we performed separate training and\nvalidation on the official sets to obtain results comparable\nto those reported in literature.\nEvaluation metrics\nWe evaluated the regression task us-\ning the Spearman’s and Pearson’s correlation coefficients\nto quantify how closely our models’ predictions align with\nthe reference standard. Additionally, for datasets contain-\ning negative cases, we used receiver operating characteristic\n(ROC) curve analysis with the area under the curve (AUC)\nto assess tumor detection performance. In this analysis, bi-\n5\n\n\nTNBC\nAQUILA\nCAM16\nExaMode\nCOBRA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson Correlation\n0.97\n0.96\n0.85\n0.95\n0.96\n0.97\n0.96\n0.81\n0.95\n0.97\n0.97\n0.96\n0.88\n0.95\n0.95\n0.87\n0.81\n0.35\n0.81\n0.89\n(a)\nTNBC\nAQUILA\nCAM16\nExaMode\nCOBRA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpearman Correlation\n0.97\n0.96\n0.49\n0.73\n0.86\n0.97\n0.96\n0.46\n0.73\n0.84\n0.97\n0.96\n0.51\n0.74\n0.81\n0.88\n0.80\n0.18\n0.57\n0.82\nMethod\nABMIL\nCLAM\nMeanPool\nWeSEG\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCAM16 Tumor Detection\nABMIL (AUC = 0.708 ± 0.059)\nCLAM (AUC = 0.683 ± 0.058)\nMEANPOOL (AUC = 0.712 ± 0.056)\nWESEG (AUC = 0.580 ± 0.058)\nRandom Chance (AUC = 0.5)\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nExaMode Tumor Detection\nABMIL (AUC = 0.927 ± 0.016)\nCLAM (AUC = 0.927 ± 0.011)\nMEANPOOL (AUC = 0.928 ± 0.006)\nWESEG (AUC = 0.813 ± 0.014)\nRandom Chance (AUC = 0.5)\n(d)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCOBRA Tumor Detection\nABMIL (AUC = 0.909 ± 0.009)\nCLAM (AUC = 0.887 ± 0.023)\nMEANPOOL (AUC = 0.870 ± 0.015)\nWESEG (AUC = 0.895 ± 0.012)\nRandom Chance (AUC = 0.5)\n(e)\nFigure 5: Comparison of bar plots and ROC curves across methods and datasets. (a) Pearson correlation coefficients\nfor different methods across the five datasets, indicating the strength of linear relationships between predicted and true\ntumor percentages. (b) Spearman correlation coefficients across the five datasets, showcasing the rank-based correlation\nbetween predictions and ground truth. (c-e) Receiver Operating Characteristic (ROC) curves for tumor detection with\nAUC scores for CAM16 (c), ExaMode (d), and COBRA (e), illustrating the trade-off between true positive and false\npositive rates. The shaded areas represent the confidence intervals over the 5-fold cross-validation, and the mean AUC\nvalues with standard deviations are reported for each dataset. These results are based on models trained using true\ntumor percentages.\nnary labels (tumor vs. no tumor) were derived from tumor\npercentages to classify slides as positive or negative. Addi-\ntionally, following [22], we used AUC to evaluate the inter-\npretability of attention scores and instance-level predictions\nby comparing them against pathologists’ ground-truth an-\nnotations or tumor segmentation masks. This comparison\nhelps determine if attention heatmaps or instance-level pre-\ndictions offer advantages for interpretability in regression\ntasks.\n5\nResults\n5.1\nAssessing tumor percentage prediction\nFigure 5(a-b) present the Pearson and Spearman correlation\nresults for various methods across multiple datasets. Over-\nall, most methods demonstrate very strong correlations,\nhighlighting their effectiveness in aligning predicted tumor\npercentages with the ground truth. However, WeSEG shows\nnotably poorer performance compared to the other meth-\nods. This difference arises because WeSEG was not explic-\nitly optimized to predict tumor percentages; rather, tumor\npercentages were derived from patch-level predictions using\na binarization step, as described in section 3.2. This addi-\ntional processing step introduces potential inaccuracies, as\nerrors in the segmentation or imprecisions when converting\nto a percentage can impact the final result.\nIn the AQUILA and TNBC datasets, all methods (except\nWeSEG) achieve high Pearson (r > 0.95) and Spearman\n(ρ > 0.95) correlations, demonstrating robust performance\nin both value prediction and rank ordering. Surprisingly,\nMeanPool performs comparably to more complex methods\nsuch as ABMIL and CLAM, suggesting that methods with\nsimpler pooling mechanisms can be effective in contexts\nwhere tumor areas are relatively large, such as in surgical\nresections.\nFor the ExaMode dataset, while Pearson correlations re-\nmain strong (r > 0.95), the Spearman correlations are lower\n(ρ ≈0.73).\nThis discrepancy highlights potential issues\nsuch as the presence of outliers which may cause the Pear-\nson correlation to appear overly optimistic. Specifically, a\ndetailed analysis reveals a significant number of tumor-free\ncases with predicted tumor percentages greater than zero\n(see the scatter plots in the Supplementary Material). For\nCOBRA, all methods show strong performance, with Pear-\nson correlations ranging from 0.95 to 0.97 for the attention-\nbased methods.\nHowever, the Spearman correlations are\nslightly lower (ρ ≈0.85), suggesting that the models may\n6\n\n\n47\n(b)\n(a)\n(c)\nFigure 6: Variation of Pearson, Spearman, and AUC metrics across different noise levels. Subplots (a), (b), and (c)\ndepict line plots showing how the Pearson correlation, Spearman correlation, and AUC, respectively, change as noise\nlevels increase. These metrics highlight the robustness of different methods to increasing levels of synthetic noise.\nnot consistently maintain the correct rank order of tumor\npercentages despite their strong linear relationships.\nThe CAM16 dataset poses unique challenges due to its\nheavily skewed tumor percentage distribution, with numer-\nous samples containing extremely low tumor burdens (per-\ncentages < 1%). This makes it difficult for models to dis-\ncern small differences in tumor percentage values, which is\nreflected in lower Spearman correlations (ρ < 0.51) despite\nstrong Pearson correlations (r > 0.81). WeSEG particularly\nunderperforms on this dataset due to its reliance on post-\nprocessed patch-level predictions, which are less effective in\ndetecting extremely small tumor regions.\n5.2\nAssessing tumor detection capabilities\nIn this subsection, we assess the tumor detection capabil-\nities on CAM16, ExaMode, and COBRA datasets, which\ncontain both tumor and normal cases. The ROC curves and\ncorresponding AUC scores for each method are presented in\nFigure 5(c-d-e).\nOn the CAM16 dataset, ABMIL achieves the strongest\nperformance with an AUC of 0.716, followed by MeanPool\n(0.707) and CLAM (0.698). WeSEG showed notably lower\nperformance with an AUC of 0.588. These results, however,\nare substantially lower compared to the state-of-the-art per-\nformance reported for binary classification approaches on\nthis dataset. This performance gap highlights the increased\ncomplexity of tumor percentage regression when compared\nto traditional binary classification tasks, where learning\nto distinguish between tumor and normal cases is more\nstraightforward.\nMost methods show substantially higher performance on\nthe ExaMode dataset.\nABMIL, CLAM, and MeanPool\nachieve nearly identical performance with AUC scores rang-\ning from 0.926 to 0.928. WeSEG, while showing improved\nabsolute performance compared to CAM16, still underper-\nforms with an AUC of 0.807. This suggests challenges in\nextracting a reliable tumor detection signal from weakly su-\npervised segmentation maps, limiting the performance and\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC Score\nABMIL\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCLAM\nDataset\nCAM16\nEXAMODE\nCOBRA\nOriginal\nAmplified\nAmplified+10%\nAmplified+30%\nAmplified+50%\nNoise Level\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC Score\nMeanPool\nOriginal\nAmplified\nAmplified+10%\nAmplified+30%\nAmplified+50%\nNoise Level\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWeSeg\nFigure 7: Impact of amplification on model performance.\nLine plot showing AUC variation with amplification across\nnoise levels.\nusability of the method.\nOn the COBRA dataset, all methods achieve strong per-\nformance. ABMIL leads with an AUC of 0.908. Interest-\ningly, WeSEG showed competitive performance here with\nan AUC of 0.889, followed by CLAM (0.880) and MeanPool\n(0.870). While these results are promising, they remain be-\nlow the reported benchmarks for binary tumor detection\ntasks on this dataset using classification labels, consistent\nwith our observations reported on CAM16.\n5.3\nAssessing the robustness to noise\nFigure 6(a-b) present the Pearson and Spearman correlation\nresults across different noise levels, showcasing the models’\nability to predict tumor percentages under increasing noise.\n7\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCAM16 Tumor Detection\nnormal (AUC = 0.683 ± 0.058)\namplified (AUC = 0.893 ± 0.049)\namp+10% noise (AUC = 0.895 ± 0.023)\namp+30% noise (AUC = 0.881 ± 0.015)\namp+50% noise (AUC = 0.877 ± 0.020)\nRandom Chance (AUC = 0.5)\n(a)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nExaMode Tumor Detection\nnormal (AUC = 0.927 ± 0.011)\namplified (AUC = 0.964 ± 0.008)\namp+10% noise (AUC = 0.962 ± 0.008)\namp+30% noise (AUC = 0.961 ± 0.011)\namp+50% noise (AUC = 0.959 ± 0.012)\nRandom Chance (AUC = 0.5)\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCOBRA Tumor Detection\nnormal (AUC = 0.887 ± 0.023)\namplified (AUC = 0.989 ± 0.003)\namp+10% noise (AUC = 0.993 ± 0.002)\namp+30% noise (AUC = 0.992 ± 0.003)\namp+50% noise (AUC = 0.992 ± 0.002)\nRandom Chance (AUC = 0.5)\nnormal (AUC = 0.887 ± 0.023)\namplified (AUC = 0.989 ± 0.003)\namp+10% noise (AUC = 0.993 ± 0.002)\namp+30% noise (AUC = 0.992 ± 0.003)\namp+50% noise (AUC = 0.992 ± 0.002)\nRandom Chance (AUC = 0.5)\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCAM16 Tumor Detection Official Test Set\nCLAM - AUC=0.815\nRandom Chance (AUC = 0.5)\nCLAM - AUC=0.815\nRandom Chance (AUC = 0.5)\n(d)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nCOBRA Tumor Detection Official Test Set\nCLAM - AUC=0.992\nRandom Chance (AUC = 0.5)\nCLAM - AUC=0.992\nRandom Chance (AUC = 0.5)\n(e)\nFigure 8: (a-c) ROC curves for the CAM16, ExaMode, and COBRA datasets with confidence interval over the 5-fold cross\nvalidation, incorporating both amplification and noise effects. (d-e) ROC curves for the CAM16 and COBRA datasets\nevaluated on the official test sets.\nAs expected, the models trained with the true tumor per-\ncentages exhibit the best overall performance. Despite the\nexpected decrease in performance when noise is added, the\nresults show that the impact of noise is relatively moder-\nate. Notably, performance degradation becomes more pro-\nnounced when 50% noise is introduced, but this represents\nan extreme and unrealistic scenario in real-world applica-\ntions.\nFor the TNBC dataset, all models demonstrated strong\nrobustness to noise, maintaining high correlation scores even\nunder substantial noise.\nABMIL and MeanPool showed\nparticularly robust performance, with Pearson and Spear-\nman correlations decreasing by only 14% at 50% noise and\nmaintaining correlations above 0.83.\nCLAM, despite its\nstrong baseline, showed greater sensitivity, with a 20% de-\ncrease under the same conditions. WeSEG, while starting\nfrom a lower baseline correlation, showed robustness with a\n12% decrease. This indicates that its segmentation quality\nremained effective at identifying tumor areas despite the\nadded noise, indirectly supporting tumor percentage esti-\nmation.\nFor the AQUILA dataset, models were more affected by\nnoise overall. At 30% noise, correlations dropped by 17%\nfor MeanPool (Pearson: 0.964 to 0.804, Sperman: 0.962\nto 0.516) and up to 27% for WeSEG (Pearson: 0.815 to\n0.597, Sperman:\n0.8 to 0.607).\nAt 50% noise, ABMIL\nand MeanPool maintained correlations above 0.5 but ex-\nperienced 41% and 46% declines, respectively.\nWeSEG,\ndespite its lower baseline correlations, performed compara-\nbly to ABMIL and MeanPool under 50% noise, maintaining\ncorrelations around 0.57. Surprisingly, CLAM showed the\ngreatest sensitivity, with correlations declining by approxi-\nmately 80%.\nThe CAM16 dataset presented the most challenging sce-\nnario, with overall lower baseline correlation scores across\nall models. ABMIL and CLAM emerged as the most robust\nmodels, experiencing a 28% reduction in Pearson correla-\ntion at 50% noise (ABMIL: 0.847 to 0.612, CLAM: 0.811\nto 0.596). MeanPool, on the other hand, showed a larger\ndecrease of 37% (0.876 to 0.552), while WeSEG exhibited\nthe weakest performance with correlations dropping by over\n59%, indicating that the quality of the segmentation is more\naffected by the challenges posed by small lesions. Spearman\ncorrelations showed some unexpected trends, with CLAM’s\nperformances remaining stable, while ABMIL and WeSEG\nshowed initial drops at 30%, followed by an increase of per-\nformance at 50% noise.\nFor the ExaMode dataset, models demonstrated strong\nrobustness, with CLAM and MeanPool experiencing a drop\nof 10% at 50% noise (CLAM: 0.95 to 0.849, MeanPool:\n0.953 to 0.854), and ABMIL a drop of 13% (0.95 to 0.827).\nWeSEG showed higher sensitivity to noise, with a small\n6.6% decline at 30% noise but a significant 61% drop at\n50% noise.\nInterestingly, Spearman correlations for AB-\nMIL and CLAM improved with noise, whereas MeanPool\nshowed slight declines.\nLastly, all models performed robustly on the COBRA\ndataset, with MeanPool showing only a 6% drop (0.954\n8\n\n\nto 0.892) at 50% noise. CLAM, WeSEG and ABMIL fol-\nlowed with 15%, 20% and 21% decline, respectively. Spear-\nman correlations remained stable or slightly improved for\nCLAM, ABMIL, and MeanPool, while WeSEG showed mi-\nnor decreases.\nTo further assess robustness, we analyzed AUC scores\nacross noise levels for the CAM16, EXAMODE, and CO-\nBRA datasets (Figure 6c). ABMIL and CLAM consistently\nachieved the highest AUC values, underscoring the robust-\nness of attention-based methods. Interestingly, they showed\nimproved results under noise conditions across the three\ndatasets.\nFor ExaMode, ABMIL’s AUC increased from\n0.927 to 0.947, and CLAM’s from 0.927 to 0.943.\nSimi-\nlarly, in the COBRA dataset, CLAM’s AUC improved from\n0.887 to 0.974, and ABMIL’s improved from 0.909 to 0.963\nat 50% noise. For the CAM16 dataset, ABMIL showed a\nmodest improvement from 0.708 to 0.732, while CLAM’s\nAUC increased from 0.683 to 0.737. In contrast, WeSEG\nshowed a slight drop of performance on the three datasets,\nespecially on ExaMode (0.813 to 0.608) and COBRA (0.895\nto 0.782).\n5.4\nAssessing the role of the amplification\ntechnique\nFigure 7 shows AUC results across datasets and methods\nunder different noise levels, while Figure 8(a-c) highlights\nROC curves for the best-performing model, CLAM. As il-\nlustrated by the AUC results, the amplification technique\nsignificantly improved the performance of attention-based\nmethods such as ABMIL and CLAM. For CAM16, ABMIL’s\nAUC increased from 0.708 to 0.851, and CLAM’s from 0.683\nto 0.893, demonstrating enhanced detection of small lesions\nand better differentiation between tumor and tumor-free\ncases.\nIn contrast, MeanPool and WeSEG showed only\nminor improvements, with AUC increasing from 0.712 to\n0.743 for MeanPool, and from 0.58 to 0.59 for WeSEG.\nThese findings suggest that these models do not benefit\nfrom the amplification technique as much as ABMIL and\nCLAM. We also present the results with the injection of\nnoise, which align with the previous findings where we ob-\nserved that AUC generally remains stable or even improves\nwith added noise. On CAM16, ABMIL achieved its highest\nAUC (0.899) at 30% noise, while CLAM peaked at 0.895\nwith 10% noise.\nFor EXAMODE, the amplification technique similarly\nshowed robust improvements, especially with ABMIL and\nCLAM where AUC increased to 0.962 and 0.964, respec-\ntively.\nMeanPool showed only minor improvements from\n0.928 to 0.955.\nInterestingly, WeSEG experienced a sig-\nnificant drop in AUC to 0.534, indicating that amplifi-\ncation might not be beneficial for all models.\nFor CO-\nBRA, the AUC demonstrated a similar same trend, with\nattention-based methods consistently outperforming, main-\ntaining AUC always above 0.9, and WeSEG showing a de-\ncrease of performance after the amplification.\nTo further evaluate the benefits of using the amplification\ntechnique and provide results comparable to the literature,\nwe conducted an additional experiment on the official test\nsets for CAM16 and COBRA, in contrast to the previous\nresults computed using the combined test sets from 5-fold\ncross-validation. We trained two separate models with am-\nplification using the official splits from the publicly available\ndatasets, and the results are presented in Figure 8(d-e). For\nCOBRA, the amplification method achieved an AUC of 0.99\non the test set, aligning closely with the results reported in\n[16], where CLAM was applied to binary classification tasks.\nSimilarly, despite CAM16 showing the lowest performance\nin the original setting, the amplified model still achieved\nresults comparable to those reported in [12] for the corre-\nsponding classification task with CLAM, reaching an AUC\nof 0.81.\n5.5\nVisual interpretability\nTo evaluate interpretability, we first extract patch-level la-\nbels by assigning binary labels (0 or 1) to image patches\nbased on the percentage of tumor area within each patch.\nA threshold of 50% tumor content is used, where patches\nwith more than 50% tumor area are labeled as tumor. These\npatch-level labels serve as the reference standard, based on\nthe tumor annotations or segmentation masks, to quantify\nthe tumor presence within each patch. We then compare\nthese patch-level labels with the instance logits and atten-\ntion scores, and compute the AUC for each method.\nThe results in Table 1 reveal a significant performance\ngap. Interestingly, instance logits consistently yield much\nhigher AUC scores across various noise levels and datasets\ncompared to attention scores. Attention scores show consid-\nerable variability, with values ranging from low to moderate.\nTo further investigate this phenomenon, we analyzed the\nattention heatmaps and identified an interesting behavior.\nIn some cases, both the attention heatmaps and instance\nlogits heatmaps closely align with the segmentation map,\neffectively focusing on tumor regions, as shown in Figure\n9a. However, in other instances, as shown in Figure 9b, the\ninstance logits correctly focus on the tumor areas, while the\nattention heatmap mistakenly assigns higher scores to nor-\nmal tissue and lower scores to the tumor, producing an ”in-\nverted attention heatmap”. These variations lead to lower\noverall performance for attention scores and highlight their\nlimitations in regression tasks. While attention mechanisms\nhave proven highly effective in classification tasks for similar\ntumor detection scenarios, our findings suggest significant\nlimitations when applied to regression problems.\nAdditionally, we evaluate the impact of amplification\nand noise on interpretability, as shown in Figure 10. Am-\nplified models produce more consistent and better-aligned\nheatmaps, with attention focused more accurately on tu-\nmor regions. Additionally, the issue of ”inverted attention\nheatmaps” observed in some cases, appears to be mitigated\nwith amplification. Importantly, the qualitative results re-\nmain consistent across varying noise levels, with heatmaps\nshowing no degradation even at higher noise levels.\n6\nDiscussion and conclusion\nIn this study, we introduced a novel weakly-supervised re-\ngression framework for tumor detection in whole-slide im-\nages, leveraging tumor percentage as a clinically relevant\ntarget. While precise annotations or AI-generated segmen-\ntation masks were used in this study to compute ground\ntruth tumor percentages, these percentages can be easily\nestimated from the clinically available coarse annotations\nof the tumor burden routinely documented in molecular\npathology workflows. By shifting the focus from traditional\nbinary classification to regression, this framework addresses\n9\n\n\nModel\nTask\nTNBC\nAQUILA\nCAM16\nExaMode\nCOBRA\nAttention\nLogits\nAttention\nLogits\nAttention\nLogits\nAttention\nLogits\nAttention\nLogits\nABMIL\nNo noise\n0.539\n0.939\n0.568\n0.98\n0.511\n0.915\n0.648\n0.863\n0.738\n0.786\n10% noise\n0.383\n0.927\n0.457\n0.967\n0.719\n0.951\n0.591\n0.853\n0.737\n0.781\n30% noise\n0.325\n0.898\n0.219\n0.887\n0.865\n0.959\n0.625\n0.822\n0.727\n0.693\n50% noise\n0.542\n0.879\n0.353\n0.793\n0.763\n0.886\n0.715\n0.8\n0.732\n0.664\nAmplified\n-\n-\n-\n-\n0.841\n0.945\n0.738\n0.786\n0.849\n0.899\nCLAM\nNo noise\n0.554\n0.94\n0.736\n0.982\n0.571\n0.959\n0.774\n0.869\n0.741\n0.754\n10% noise\n0.449\n0.926\n0.619\n0.967\n0.594\n0.953\n0.705\n0.856\n0.733\n0.728\n30% noise\n0.256\n0.899\n0.307\n0.887\n0.669\n0.931\n0.707\n0.814\n0.731\n0.657\n50% noise\n0.272\n0.878\n0.27\n0.636\n0.653\n0.876\n0.756\n0.769\n0.728\n0.578\nAmplified\n-\n-\n-\n-\n0.935\n0.897\n0.741\n0.754\n0.848\n0.895\nMeanPool\nNo noise\n-\n0.936\n-\n0.982\n-\n0.924\n-\n0.863\n-\n0.793\n10% noise\n-\n0.925\n-\n0.974\n-\n0.928\n-\n0.851\n-\n0.787\n30% noise\n-\n0.897\n-\n0.945\n-\n0.933\n-\n0.832\n-\n0.775\n50% noise\n-\n0.873\n-\n0.789\n-\n0.93\n-\n0.828\n-\n0.769\nAmplified\n-\n-\n-\n-\n-\n0.924\n-\n0.793\n-\n0.854\nWeSEG\nNo noise\n-\n0.83\n-\n0.877\n-\n0.68\n-\n0.785\n-\n0.918\n10% noise\n-\n0.885\n-\n0.958\n-\n0.8\n-\n0.67\n-\n0.89\n30% noise\n-\n0.842\n-\n0.904\n-\n0.44\n-\n0.68\n-\n0.889\n50% noise\n-\n0.846\n-\n0.835\n-\n0.66\n-\n0.585\n-\n0.871\nAmplified\n-\n-\n-\n-\n-\n0.74\n-\n0.505\n-\n0.566\nTable 1: This table summarizes the AUC results for attention scores and instance logits compared to the true tumor\nmasks. Results are presented for different methods and datasets for training experiments without noise, with noise, and\nwith amplification.\n(a)\n(b)\nFigure 9: Visualization and comparison of logits heatmaps and attention heatmaps for tumor detection. The first row\nshows examples from the TNBC dataset, while the second row shows examples from the AQUILA dataset. The set of\nimages in column (a) shows good examples where the attention heatmap and logits heatmap align with tumor regions,\nwhile column (b) shows examples with reversed attention heatmaps, where the attention mechanism highlights non-tumor\nregions instead.\nthe challenges posed by datasets with scarce or absent nega-\ntive cases, offering a flexible alternative for tumor detection.\nOur experiments demonstrated that weakly-supervised\nmodels can effectively learn to estimate tumor percentages\nacross diverse datasets. More importantly, we showed that\nthese predicted percentages can serve as a proxy for tumor\ndetection, the primary aim of this framework. However, we\nrecognize that clinical annotations of tumor burden inher-\nently contain noise, introducing variability into the reference\nstandard. To address this, we simulated real-world noise\nin clinical practice by introducing synthetic noise into our\ndata. Our results showed that the regression framework re-\nmained robust even under significant noise levels, especially\nfor attention-based methods (ABMIL and CLAM), demon-\nstrating its potential usability despite noisy annotations.\nHowever, it is worth noting that our noise simulations, con-\nducted using uniform distributions with three noise levels\n(±10%, ±30%, ±50%), may not fully reflect the exact dis-\ntribution of variability found in clinically available annota-\ntions. Future works could aim at better modeling the noise\nencountered in clinical practice and integrating real-world\nnoisy annotations to better understand the model’s robust-\nness.\nAddressing tumor detection in datasets with small lesions\nproved to be challenging due to the difficulty of the regres-\nsion framework in distinguishing between small percentages\nand tumor-free cases, resulting in suboptimal performance\ncompared to classification benchmarks. To mitigate this, we\nintroduced a target amplification technique, which proved\nparticularly valuable for datasets containing biopsies or sub-\ntle lesions like CAM16, ExaMode, and COBRA. While tar-\nget amplification improves the model’s sensitivity to small\ntumor percentages, it introduces a trade-off by compressing\nhigher-end values, which may reduce the model’s precision\nin predicting tumor percentages for cases with extensive le-\nsions. This limitation could affect the accuracy of regres-\nsion predictions for large tumors. However, it is important\nto emphasize that the ultimate goal of this framework is\nrobust tumor detection, not precise tumor percentage re-\ngression.\nThe regression task is primarily used as a tool\n10\n\n\nFigure 10: Visualization and comparison of attention heatmaps for tumor detection, illustrating the effects of the ampli-\nfication technique and noise on the qualitative results. The figure demonstrates how amplification and increasing noise\nlevels affect the attention mechanism’s ability to highlight tumor regions.\nto enable training without relying on negative cases, but\nthe end goal remains the accurate identification of tumor\nregions.\nAn important focus of this study was on model inter-\npretability.\nAttention mechanisms, while widely used in\nMIL models for tumor segmentation in classification tasks,\nhave been less explored in regression tasks. Interestingly, in\nour regression framework, attention scores exhibited a coun-\nterintuitive behavior, often assigning lower scores to tumor\nregions and higher scores to normal regions, resulting in ”in-\nverted attention heatmaps”. Despite this, instance-based\nMIL frameworks allowed us to compare attention scores\nwith patch-level predictions, which proved to be more ef-\nfective in localizing tumor regions. Nevertheless, the inter-\npretability of attention scores for regression models remains\nan open challenge. While the amplification technique im-\nproved the performance of attention heatmaps, potentially\nmitigating the reversed behavior observed in unamplified\nsettings, attention mechanisms may require further refine-\nment and adaptation for use in regression tasks, where the\ntask is to predict a continuous value rather than making\nbinary classifications.\nOur approach has limitations. For this work, we relied on\nImageNet pre-trained encoders.\nAlthough ImageNet pre-\ntraining is a standard practice and has shown general util-\nity in histopathology task, using domain-specific pretrain-\ning could provide a better could provide a better repre-\nsentation of histopathology data.\nFuture work could ex-\nplore leveraging encoders pretrained on large-scale H&E-\nstained histopathology datasets, which may better capture\ndomain-specific features and improve generalizability. An-\nother area for improvement lies in the interpretability of\nattention-based heatmaps. Future research should focus on\ndesigning more robust interpretability techniques tailored\nfor regression, which could provide clearer insights into the\nmodel’s decision-making process.\nAdditionally, the fifth-\nroot transformation used for target amplification was em-\npirically effective but may not be the optimal choice for\nall datasets. Further investigations could explore alterna-\ntive amplification strategies, or even dynamic amplification\nmethods that adapt based on the specific characteristics of\nthe dataset. Finally, while we simulated noise to evaluate\nrobustness, a reader study with multiple pathologists an-\nnotating the tumor burden on the same slides would help\nquantify the actual variability in clinical annotations and\nprovide valuable insights into the practical applicability of\nthis framework.\nAcknowledgments\nWe would like to thank Daan Geijs for the development\nof the skin segmentation model, John-Melle Bokhorst for\nthe development of the colon segmentation model and Mart\nRijthoven for the development of the breast segmentation\nmodel. This project has received funding from the Innova-\ntive Medicines Initiative 2 Joint Undertaking under grant\nagreement No 945358. This Joint Undertaking receives sup-\nport from the European Union’s Horizon 2020 research and\ninnovation program and EFPIA (www.imi.europe.eu).\n7\nDeclaration of Generative AI and\nAI-assisted technologies in the\nwriting process\nDuring the preparation of this work the author(s) used\nChatGPT in order to improve readability and language.\nAfter using this tool/service, the author(s) reviewed and\nedited the content as needed and take(s) full responsibility\nfor the content of the publication.\nReferences\n[1] Aswolinskiy,\nW.,\n2023.\npathology-whole-slide-\npacker.\nhttps://github.com/DIAGNijmegen/\npathology-whole-slide-packer.\n[2] Aswolinskiy, W., Tellez, D., Raya, G., van der Woude,\nL., Looijen-Salamon, M., van der Laak, J., Grunberg,\nK., Ciompi, F., 2021. Neural image compression for\nnon-small cell lung cancer subtype classification in HE\nstained whole-slide images, in: Medical Imaging 2021:\nDigital Pathology, SPIE. pp. 1 – 7. doi:10.1117/12.\n2581943.\n[3] Balkenhol, M.C.A., Vreuls, W., Wauters, C.A.P., Mol,\nS.J.J., van der Laak, J.A.W.M., Bult, P., 2020. His-\ntological subtypes in triple negative breast cancer are\n11\n\n\nassociated with specific information on survival. An-\nnals of Diagnostic Pathology 46, 151490. doi:10.1016/\nj.anndiagpath.2020.151490.\n[4] B´andi, P., Balkenhol, M., van Ginneken, B., van der\nLaak, J., Litjens, G., 2019.\nResolution-agnostic tis-\nsue segmentation in whole-slide histopathology images\nwith convolutional neural networks.\nPeerJ 7, e8242.\ndoi:10.7717/peerj.8242.\n[5] Bokhorst, J., Nagtegaal, I., Fraggetta, F., Vatrano, S.,\nMesker, W., Vieth, M., van der Laak, J.A., Ciompi, F.,\n2023. Deep learning for multi-class semantic segmen-\ntation enables colorectal cancer detection and classifi-\ncation in digital pathology images 13, 8398. doi:10.\n1038/s41598-023-35491-z.\n[6] Campanella, G., Hanna, M.G., Geneslaw, L., Mi-\nraflor, A., Werneck Krauss Silva, V., Busam, K.J.,\nBrogi, E., Reuter, V.E., Klimstra, D.S., Fuchs, T.J.,\n2019.\nClinical-grade computational pathology using\nweakly supervised deep learning on whole slide im-\nages. Nature Medicine 25, 1301–1309. doi:10.1038/\ns41591-019-0508-1.\n[7] Carbonneau,\nM.A.,\nCheplygina,\nV.,\nGranger,\nE.,\nGagnon, G., 2018. Multiple Instance Learning: A Sur-\nvey of Problem Characteristics and Applications. Pat-\ntern Recognition 77, 329–353. doi:10.1016/j.patcog.\n2017.10.009.\n[8] Chen, R.J., Chen, C., Li, Y., Chen, T.Y., Trister, A.D.,\nKrishnan, R.G., Mahmood, F., 2022.\nScaling vision\ntransformers to gigapixel images via hierarchical self-\nsupervised learning, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 16144–16155.\n[9] Chikontwe,\nP.,\nKim,\nM.,\nNam,\nS.J.,\nGo,\nH.,\nPark,\nS.H.,\n2020.\nMultiple\ninstance\nlearning\nwith center embeddings for histopathology classifica-\ntion, in:\nInternational Conference on Medical Im-\nage Computing and Computer-Assisted Intervention,\nSpringer. pp. 519–528.\ndoi:https://doi.org/10.\n1007/978-3-030-59722-1_50.\n[10] Contreras, N.S.L., Grisi, C., Aswolinskiy, W., Vatrano,\nS., Fraggetta, F., Nagtegaal, I., D’Amato, M., Ciompi,\nF., 2024.\nBenchmarking hierarchical image pyramid\ntransformer for the classification of colon biopsies and\npolyps histopathology images, pp. 1–4. doi:10.1109/\nisbi56570.2024.10635841.\n[11] Cruz-Roa, A., Basavanhally, A., Gonz´alez, F., Gilmore,\nH., Feldman, M., Ganesan, S., Shih, N., Tomaszewski,\nJ., Madabhushi, A., 2014. Automatic detection of inva-\nsive ductal carcinoma in whole slide images with con-\nvolutional neural networks, in: Medical Imaging 2014:\nDigital Pathology, SPIE. p. 904103. doi:10.1117/12.\n2043872.\n[12] Dooper, S., Pinckaers, H., Aswolinskiy, W., Hebeda,\nK., Jarkman, S., van der Laak, J., Litjens, G., 2023.\nGigapixel end-to-end training using streaming and at-\ntention. Medical Image Analysis 88, 102881. doi:10.\n1016/j.media.2023.102881.\n[13] Ehteshami Bejnordi, B., Veta, M., Johannes van Di-\nest, P., van Ginneken, B., Karssemeijer, N., Litjens,\nG., van der Laak, J.A.W.M., the CAMELYON16 Con-\nsortium, Hermsen, M., Manson, Q.F., Balkenhol, M.,\nGeessink, O., Stathonikos, N., van Dijk, M.C., Bult, P.,\nBeca, F., Beck, A.H., Wang, D., Khosla, A., Gargeya,\nR., Irshad, H., Zhong, A., Dou, Q., Li, Q., Chen,\nH., Lin, H.J., Heng, P.A., Haß, C., Bruni, E., Wong,\nQ., Halici, U., ¨Oner, M.¨U., Cetin-Atalay, R., Berseth,\nM., Khvatkov, V., Vylegzhanin, A., Kraus, O., Sha-\nban, M., Rajpoot, N., Awan, R., Sirinukunwattana,\nK., Qaiser, T., Tsang, Y.W., Tellez, D., Annuscheit,\nJ., Hufnagl, P., Valkonen, M., Kartasalo, K., Lato-\nnen, L., Ruusuvuori, P., Liimatainen, K., Albarqouni,\nS., Mungal, B., George, A., Demirci, S., Navab, N.,\nWatanabe, S., Seno, S., Takenaka, Y., Matsuda, H.,\nAhmady Phoulady, H., Kovalev, V., Kalinovsky, A.,\nLiauchuk, V., Bueno, G., Fernandez-Carrobles, M.M.,\nSerrano, I., Deniz, O., Racoceanu, D., Venˆancio, R.,\n2017.\nDiagnostic Assessment of Deep Learning Al-\ngorithms for Detection of Lymph Node Metastases in\nWomen With Breast Cancer. JAMA 318, 2199–2210.\ndoi:10.1001/jama.2017.14585.\n[14] El Nahhas, O.S.M., Loeffler, C.M.L., Carrero, Z.I., van\nTreeck, M., Kolbinger, F.R., Hewitt, K.J., Muti, H.S.,\nGraziani, M., Zeng, Q., Calderaro, J., Ortiz-Br¨uchle,\nN., Yuan, T., Hoffmeister, M., Brenner, H., Brobeil,\nA., Reis-Filho, J.S., Kather, J.N., 2024. Regression-\nbased Deep-Learning predicts molecular biomarkers\nfrom pathology slides.\nNature Communications 15,\n1253. doi:10.1038/s41467-024-45589-1.\n[15] Fatima, S., Ali, S., Kim, H.C., 2023. A Comprehensive\nReview on Multiple Instance Learning. Electronics 12,\n4323. doi:10.3390/electronics12204323.\n[16] Geijs, D.J., Dooper, S., Aswolinskiy, W., Hillen, L.M.,\nAmir, A.L., Litjens, G., 2024. Detection and subtyping\nof basal cell carcinoma in whole-slide histopathology\nusing weakly-supervised learning. Medical Image Anal-\nysis 93, 103063. doi:10.1016/j.media.2023.103063.\n[17] Graziani, M., Marini, N., Deutschmann, N., Janakara-\njan, N., M¨uller, H., Rodriguez Martinez, M., 2022.\nAttention-based Interpretable Regression of Gene Ex-\npression in Histology, Interpretability of Machine In-\ntelligence in Medical Image Computing. pp. 44–60.\ndoi:10.1007/978-3-031-17976-1_5.\n[18] Grisi,\nC.,\n2023.\nhs2p.\nhttps://github.com/\nclemsgrs/hs2p.\n[19] Halicek, M., Shahedi, M., Little, J.V., Chen, A.Y., My-\ners, L.L., Sumer, B.D., Fei, B., 2019. Head and Neck\nCancer Detection in Digitized Whole-Slide Histology\nUsing Convolutional Neural Networks. Scientific Re-\nports 9, 14043. doi:10.1038/s41598-019-50313-x.\n[20] Ilse, M., Tomczak, J.M., Welling, M., 2018. Attention-\nbased Deep Multiple Instance Learning, in: Proceed-\nings of the 35th International Conference on Machine\nLearning, pp. 2127–2136.\n[21] Krieger, N., Hiatt, R.A., Sagebiel, R.W., Clark, W.H.,\nMihm, M.C., 1994.\nInter-observer variability among\n12\n\n\npathologists’ evaluation of malignant melanoma: Ef-\nfects upon an analytic study. Journal of Clinical Epi-\ndemiology 47, 897–902. doi:10.1016/0895-4356(94)\n90193-7.\n[22] Lerousseau, M., Classe, M., Battistella, E., Estienne,\nT., Henry, T., Leroy, A., Sun, R., Vakalopoulou, M.,\nScoazec, J.Y., Deutsch, E., Paragios, N., 2021. Weakly\nsupervised pan-cancer segmentation tool, Medical Im-\nage Computing and Computer Assisted Intervention –\nMICCAI 2021. doi:10.1007/978-3-030-87237-3_24.\n[23] Lu,\nM.Y.,\nWilliamson,\nD.F.,\nChen,\nT.Y.,\nChen,\nR.J.,\nBarbieri,\nM.,\nMahmood,\nF.,\n2021.\nData-\nefficient and weakly supervised computational pathol-\nogy on whole-slide images. Nature Biomedical Engi-\nneering 5, 555–570.\ndoi:https://doi.org/10.1038/\ns41551-020-00682-w.\n[24] Marr´on-Esquivel,\nJ.M.,\nDuran-Lopez,\nL.,\nLinares-\nBarranco, A., Dominguez-Morales, J.P., 2023.\nA\ncomparative study of the inter-observer variability\non Gleason grading against Deep Learning-based ap-\nproaches for prostate cancer. Computers in Biology and\nMedicine 159, 106856.\ndoi:10.1016/j.compbiomed.\n2023.106856.\n[25] Pinckaers, H., Bulten, W., Litjens, G., 2019.\nHigh\nresolution whole prostate biopsy classification using\nstreaming stochastic gradient descent, in:\nMedical\nImaging 2019:\nDigital Pathology.\ndoi:10.1117/12.\n2512817.\n[26] van Rijthoven, M., Balkenhol, M., Silina, K., van der\nLaak, J., Ciompi, F., 2021. Hooknet: Multi-resolution\nconvolutional neural networks for semantic segmenta-\ntion in histopathology whole-slide images. Medical Im-\nage Analysis 68, 101890. doi:10.1016/j.media.2020.\n101890.\n[27] Schirris, Y., Engelaer, M., Panteli, A., Horlings, H.M.,\nGavves, E., Teuwen, J., 2022. WeakSTIL: Weak whole-\nslide image level stromal tumor infiltrating lymphocyte\nscores are all you need, SPIE Medical Imaging 2022.\ndoi:10.1117/12.2611528.\n[28] Tellez, D., Litjens, G., van der Laak, J., Ciompi,\nF., 2021.\nNeural image compression for gigapixel\nhistopathology image analysis. IEEE Transactions on\nPattern Analysis and Machine Intelligence 43, 567–578.\ndoi:10.1109/TPAMI.2019.2936841.\n[29] van der Laak, J., Litjens, G., Ciompi, F., 2021.\nDeep learning in histopathology:\nThe path to the\nclinic.\nNature Medicine 27, 775–784.\ndoi:10.1038/\ns41591-021-01343-4.\n[30] Wang, X., Chen, H., Gan, C., Lin, H., Dou, Q.,\nTsougenis, E., Huang, Q., Cai, M., Heng, P.A., 2020.\nWeakly Supervised Deep Learning for Whole Slide\nLung Cancer Image Analysis.\nIEEE transactions on\ncybernetics 50, 3950–3962. doi:10.1109/TCYB.2019.\n2935141.\n[31] Weitz, P., Wang, Y., Hartman, J., Rantalainen, M.,\n2021.\nAn investigation of attention mechanisms in\nhistopathology whole-slide-image analysis for regres-\nsion objectives, in:\n2021 IEEE/CVF International\nConference on Computer Vision Workshops (ICCVW),\nIEEE, Montreal, BC, Canada. pp. 611–619.\ndoi:10.\n1109/ICCVW54120.2021.00074.\n[32] Xia, T., Kumar, A., Feng, D., Kim, J., 2018. Patch-\nlevel Tumor Classification in Digital Histopathology\nImages with Domain Adapted Deep Learning, in: 2018\n40th Annual International Conference of the IEEE En-\ngineering in Medicine and Biology Society (EMBC),\nIEEE, Honolulu, HI. pp. 644–647. doi:10.1109/EMBC.\n2018.8512353.\n13\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21109v1.pdf",
    "total_pages": 13,
    "title": "\"No negatives needed\": weakly-supervised regression for interpretable tumor detection in whole-slide histopathology images",
    "authors": [
      "Marina D'Amato",
      "Jeroen van der Laak",
      "Francesco Ciompi"
    ],
    "abstract": "Accurate tumor detection in digital pathology whole-slide images (WSIs) is\ncrucial for cancer diagnosis and treatment planning. Multiple Instance Learning\n(MIL) has emerged as a widely used approach for weakly-supervised tumor\ndetection with large-scale data without the need for manual annotations.\nHowever, traditional MIL methods often depend on classification tasks that\nrequire tumor-free cases as negative examples, which are challenging to obtain\nin real-world clinical workflows, especially for surgical resection specimens.\nWe address this limitation by reformulating tumor detection as a regression\ntask, estimating tumor percentages from WSIs, a clinically available target\nacross multiple cancer types. In this paper, we provide an analysis of the\nproposed weakly-supervised regression framework by applying it to multiple\norgans, specimen types and clinical scenarios. We characterize the robustness\nof our framework to tumor percentage as a noisy regression target, and\nintroduce a novel concept of amplification technique to improve tumor detection\nsensitivity when learning from small tumor regions. Finally, we provide\ninterpretable insights into the model's predictions by analyzing visual\nattention and logit maps. Our code is available at\nhttps://github.com/DIAGNijmegen/tumor-percentage-mil-regression.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}