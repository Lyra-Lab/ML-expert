{
  "id": "arxiv_2502.20571v1",
  "text": "PFformer: A Position-Free Transformer\nVariant for Extreme-Adaptive Multivariate\nTime Series Forecasting\nYanhong Li1 and David C. Anastasiu1\n1Department of Computer Science and Engineering, Santa Clara University, Santa Clara, CA, USA\nMultivariate time series (MTS) forecasting is vital in fields like\nweather, energy, and finance. However, despite deep learning\nadvancements, traditional Transformer-based models often di-\nminish the effect of crucial inter-variable relationships by sin-\ngular token embedding and struggle to effectively capture com-\nplex dependencies among variables, especially in datasets with\nrare or extreme events. These events create significant imbal-\nances and lead to high skewness, complicating accurate pre-\ndiction efforts.\nThis study introduces PFformer, a position-\nfree Transformer-based model designed for single-target MTS\nforecasting, specifically for challenging datasets characterized\nby extreme variability. PFformer integrates two novel embed-\nding strategies:\nEnhanced Feature-based Embedding (EFE)\nand Auto-Encoder-based Embedding (AEE). EFE effectively\nencodes inter-variable dependencies by mapping related se-\nquence subsets to high-dimensional spaces without positional\nconstraints, enhancing the encoder’s functionality. PFformer\nshows superior forecasting accuracy without the traditional lim-\nitations of positional encoding in MTS modeling.\nWe evalu-\nated PFformer across four challenging datasets, focusing on\ntwo key forecasting scenarios: long sequence prediction for 3\ndays ahead and rolling predictions every four hours to reflect\nreal-time decision-making processes in water management. PF-\nformer demonstrated remarkable improvements, from 20% to\n60%, compared with state-of-the-art models.\nDeep Learning | Time Series Forecasting | Hydrology | Transformer.\nIntroduction\nEffective time series forecasting is critical for strategic\ndecision-making in many industries, including meteorol-\nogy (1, 2), financial markets (3, 4), and energy manage-\nment (5). Advancements in deep learning have resulted in a\nwave of complex multivariate time series (MTS) models that\nhave greatly improved forecasting accuracy by utilizing intri-\ncate data relationships, especially where sporadic but major\nextreme events (6, 7) such as flash floods and droughts exist.\nAfter its immense success in language processing and com-\nputer vision, the Transformer model (8), known for its strong\ncapabilities in depicting pairwise dependencies in sequences,\nis now making significant inroads into time series forecast-\ning (9–11). However, these models typically compress mul-\nThis research was made possible through computational resources pro-\nvided by Supermicro and NVIDIA.\nCorresponding author: David C. Anastasiu.\nE-mail: danastasiu@\nscu.edu\ntiple variables of the same timestamp into a single token, po-\ntentially obscuring vital multivariate correlations. As high-\nlighted by Liu et al. (12) and Zhang and Yan (13), data points\nwithin the same timestamp often represent distinct physical\nphenomena and are measured inconsistently. This embed-\nding into a singular token can erase crucial inter-variable re-\nlationships, and the localized receptive field of a single times-\ntamp may not capture beneficial information effectively, es-\npecially when dealing with events that are not aligned in time.\nMoreover, the application of permutation-invariant attention\nmechanisms, which fail to consider the importance of se-\nquence order, may not be suitable for analyzing temporal data\nwhere order significantly impacts series variations. Conse-\nquently, these flaws can limit the Transformer’s capacity to\nrepresent and generalize across various multivariate time se-\nries effectively. Even with closely connected multivariate in-\nputs available, like the streamflow and rain time series, the\nTransformer struggles to use these correlations to accurately\npredict time series with extreme events (14).\nIn response to these challenges, we propose PFformer, a\nposition-free Transformer variant for adaptive multivariate\ntime series forecasting.\nThe PFformer model leverages a\nposition-free embedding strategy that allows for a more flex-\nible and precise representation of both time and variable de-\npendencies. By abstracting away from fixed positional en-\ncodings, the model can better distinguish and integrate rele-\nvant features across both dimensions, enhancing its predic-\ntive capabilities. Our approach is designed to not only pre-\nserve but also accentuate the inherent dependencies within\nthe data, enabling the PFformer to deliver superior forecast-\ning performance on highly skewed datasets. The main con-\ntributions of this work are as follows:\n• By employing EFE and AEE, PFformer overcomes tra-\nditional challenges associated with positional encoding\nand enables a more flexible and effective handling of\nspatial-temporal relationships.\n• The PFformer model incorporates a novel clustering-\nbased importance enhanced sampling strategy that\nadeptly pinpoints critical features and trends within\ndatasets by relying on the learned mixture distribution\nof the data.\n• To improve the model’s robustness to severe events,\nPFformer innovatively uses AEE to emphasize short-\nLi et al.\n|\nFebruary 27, 2025\n|\n1–7\n\n\nterm predictions in the loss penalty, which makes the\nauxiliary variables more accountable for the overall ac-\ncuracy of the model.\nWe evaluated PFformer on four separate datasets and found\nthat PFformer significantly outperforms state-of-the-art base-\nlines by 20% to 60%. Additionally, we carried out several\nablation studies to understand the effects of specific design\ndecisions.\nRelated work\nTo fully leverage auxiliary variables in prediction, multivari-\nate time series studies have utilized various algorithms, from\ntraditional methods like vector autoregression and multivari-\nate exponential smoothing to advanced deep learning meth-\nods. Vector autoregressive (VAR) models (15) are statistical\nmodels that assume linear dependencies both across differ-\nent dimensions and over time. Wang et al. (16) developed a\nhybrid model that combines Empirical Mode Decomposition\n(EMD), Ensemble EMD (EEMD), and ARIMA for long-term\nstreamflow forecasting. Additionally, graph neural networks\n(GNNs) (17) explicitly capture cross-dimensional dependen-\ncies by combining temporal and graph convolutional layers.\nRecently,\ntransformer-based techniques such as Auto-\nformer (18) and Reformer (19) have been proposed for long-\nterm forecasting, offering sophisticated dependency discov-\nery and modeling capabilities.\nInformer (9) introduced a\nProbSparse self-attention mechanism and a generative-style\ndecoder to significantly increase inference speed for long-\nsequence predictions. FEDFormer (10) improved long-term\nforecasting by randomly selecting a fixed number of Fourier\ncomponents to capture the global characteristics of time se-\nries. However, recent research suggests that simpler linear\nmodels (11, 20) may outperform these complex approaches.\nOther methods, including representation learning (14) and\nhybrid techniques (21–23), have also been explored for long-\nterm forecasting. PatchTST (24) enhances time series mod-\neling by using patching techniques to extract local semantics\nand maintain channel independence. Crossformer (13) fea-\ntures a cross-scale embedding layer and Long Short Distance\nAttention (LSDA) to effectively capture cross-time and cross-\nvariable dependencies in MTS, and iTransformer (12) opti-\nmizes Transformer inputs to improve time-series modeling,\nfocusing on more accurate data interpretation and prediction.\nDespite extensive research on time series prediction, deep\nlearning models face difficulties when dealing with time se-\nries data that contain rare or extreme occurrences because of\nthe obvious imbalance in the dataset. Predicting these types\nof data is notably difficult as their distribution is heavily in-\nfluenced by extreme values, leading to high skewness. This\ncalls for the creation of specialist models intended for pre-\ncise forecasting of extreme events. An and Cho (25) pro-\nposed a novel method that focused on anomaly detection us-\ning reconstruction probability as a lens. This approach clev-\nerly takes into account the data distribution’s intrinsic vari-\nability. Similar to this, the Uber TSF model (26) automat-\nically extracts extra features from the auto-encoder LSTM\nnetwork, priming it to capture intricate time-series dynam-\nics during large-scale events. New inputs are then fed into\nthe LSTM forecaster for prediction. Ding et al. (27) focused\ntheir research on enhancing the deep learning models’ ability\nto identify and forecast exceptional events. They used a tech-\nnique that modifies predictions according to how closely the\ncurrent data resembles extreme occurrences that have been\nobserved in the past. Additionally, Additionally, an earlier\nmodel we designed, NEC+ (28) is specifically designed to\naccommodate extreme values in hydrologic flow prediction\nby employing the Gaussian mixture model (GMM) (29) and\ntraining three predictors in parallel. Another model we previ-\nously proposed, DAN (14), learns and merges rich represen-\ntations to adaptively predict streamflow.\nHowever, few of these prior works have addressed both long-\nterm predictions in prolonged sequences and used auxiliary\nvariables effectively to enhance extreme value prediction. To\nbridge this gap, we propose PFformer to address the chal-\nlenges of highly skewed single-target MTS forecasting in the\nhydrology domain. Experiments on four real-life hydrologic\nstreamflow datasets show that PFformer significantly outper-\nforms state-of-the-art methods for hydrologic and long-term\ntime series prediction.\nPreliminaries\nProblem Statement. Suppose we have a collection of m\n(m >= 1) related univariate time series, with each row in\nthe input matrix corresponding to a different time series. We\nare going to predict the next h time steps for the first time se-\nries x1, given historical data from multiple length-t observed\nseries. The problem can be described as,\n\n\nx1,1\n···\nx1,t\nx2,1\n···\nx2,t\n...\n...\n...\nxm,1\n···\nxm,t\n\n∈Rm×t →[x1,t+1,...,x1,t+h] ∈Rh,\nwhere xi,j denotes the value of time series i at time j. The\nmatrix on the left are the inputs, and x1,t+1 to x1,t+h are the\noutputs of our method. We first define this task by modeling\nthe objective time series x1 as the ordinary series and the\ngroup of related time series x2 to xm as auxiliary series.\nTable 1. Input Stream Data Statistics\nStatistic / Stream\nRoss\nSaratoga\nUpperPen\nSFC\nmin\n0.00\n0.00\n0.00\n0.00\nmax\n1440.00\n2210.00\n830.00\n7200.00\nmean\n2.91\n5.77\n6.66\n20.25\nstd. deviation\n24.43\n26.66\n21.28\n110.03\nskewness\n19.84\n19.50\n13.42\n18.05\nkurtosis\n523.16\n697.78\n262.18\n555.18\nData Descriptions. Our study uses a hydrologic dataset first\nintroduced in (14) that captures streamflow from four Cali-\nfornia streams: Ross, Saratoga, UpperPen, and SFC. Given\nCalifornia’s lack of rainfall during the summer, we follow\nthe same problem design as in (14) and focus on forecast-\ning the months from September to May, deliberately exclud-\ning the summer period.\nData for training and validation\n2\n|\narXiv\nLi et al.\n|\nPFformer: A Position-Free Transformer Variant\n\n\nAuto-Encoder-based \nEmbedding\nMulti-Head\nAttention\nAdd & Norm\nAdd & Norm\nLinear\nMulti-Head\nAttention\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nEnhanced \nFeature-based \nEmbedding\nFeed\nForward\nAdd & Norm\nLinear\nInput\nSequences\nN X\nM X\nOutput\nSequence\nLoss \nComputation\nFig. 1. The PFformer framework; L\ndenotes the element-wise addition. The PF-\nformer framework is a transformer-based variant optimized for multivariate time se-\nries forecasting. It replaces the encoder’s positional encoding layer with Enhanced\nFeature-based Embedding (EFE) to capture complex inter-variable relationships.\nIn the decoder, Auto-Encoder-based Embedding (AEE) substitutes the positional\nembedding, enabling direct, fixed-length predictions without error propagation or\nmasking.\nwas drawn from January 1988 to August 2021 and we aim\nto accurately project the streamflow for the subsequent year\n(September 2021 to May 2022), with predictions made every\nfour hours. Each prediction estimates the upcoming 3 days\nbased on the preceding 15 days of data. The performance\nmetrics we employed are Root Mean Square Error (RMSE)\nand Mean Absolute Percentage Error (MAPE). We add 1 to\nboth the true value and the predicted value to avoid instabil-\nity in the calculation when the true values are close to zero.\nSince the sensors measure the streamflow and precipitation\nevery 15 minutes, we are attempting a lengthy forecasting\nhorizon (h = 288). Table 1 shows several statistics of our\ninput time series, which provide valuable insights into the\nshape and distribution of the data, including min, max, mean,\nmedian, variance, skewness, and kurtosis. The presence of\nhigh skewness and kurtosis values suggests that our data ex-\nhibit significant asymmetry and departure from the symmet-\nric bell-shaped curve of a Normal distribution. Specifically,\nthe positive skewness values indicate that the distribution is\nskewed to the right, resulting in a longer tail on the right side.\nThis implies that there are more extreme values or outliers on\nthe higher end of the distribution.\nMethods\nPFformer Architecture. Fig. 1 illustrates the PFformer ar-\nchitecture, a transformer-based framework optimized for\nmultivariate time series forecasting. It consists of two main\ncomponents: the encoder and the decoder, both structured to\nenhance forecasting accuracy through advanced embedding\ntechniques.\nEncoder. As shown in the left part of Fig. 1, the encoder sec-\ntion of our model closely follows the standard Transformer\nEncoder PF\nEmbedding \nLayer\nxi\nxi+1\nxi+2\nxi+3\nxi+4\n…\nxi+t\nxi-1\nxi\nxi+1\nxi+2\nxi+3\n…\nxi+t-1\nDense\nI0\nxi-2\nxi-1\nxi\nxi+1\nxi+2\n…\nxi+t-2\n…\n…\n…\n…\n…\n…\n…\nxi-s\nxi-s+1\nxi-s+2\nxi-s+3\nxi-s+4\n…\nxi-s+t\nDense\nDense\nDense\nDense\nDense\nDense\nTanh\nTanh\nTanh\nTanh\nTanh\nTanh\nTanh\nI1\nI2\nI3\nI4\n…\nIt\nInput\nSequences\nEncoder\nInput\nxi-1\nxi\nxi+1\nxi+2\nxi+3\n…\nxi+t-1\nFig. 2. PFformer encoder embedding module. X and A are the stream and rain\nvalue sequences in our dataset. The output of EFE is the input to the transformer\nencoder.\nencoder design aside from the embedding layer, which uses\nEnhanced Feature-based Embedding (EFE). This strategy\nmaps input sequences to high-dimensional spaces without\npositional encoding and effectively captures complex inter-\nvariable relationships. These are then processed through lay-\ners of multi-head attention and feed-forward networks, each\nfeaturing an “Add and Norm” step for output integration and\nnormalization, crucial for stabilizing the learning process.\nDecoder.\nPFformer bypasses the use of target sequences\nas inputs to avoid error propagation common with recur-\nsive prediction methods.\nInstead, it predicts all outcomes\ndirectly, eliminating the need for both masking mechanisms\nand padding due to the fixed prediction length. The AEE\nlayer is used to create rich representations that enhance input\ndata for the attention mechanisms. Additionally, we simpli-\nfied the cross-attention layer by applying attention comput-\ning directly on the output of AEE in the transformer decoder,\nenabling effective learning from the richly represented em-\nbedding layers.\nOutput. To enhance the model’s adaptability to extreme val-\nues, the AEE module also plays a critical role in the loss cal-\nculation. Specifically, the output from AEE passes through\na linear layer, which is combined with the output from the\ndecoder’s linear layer to form the final output, as further de-\nscribed in the Multiple-Objective Loss Function section.\nEnhanced Feature-Based Embedding (EFE). Fig. 2\nshows the EFE module in PFformer. For an input sequence\nof length t, taking the first time point as an example, EFE\ncombines the predicted sequence value at this time point, cor-\nresponding values from auxiliary sequences, and s preceding\nvalues from each of these auxiliary sequences. These ele-\nments are concatenated to form a multi-spatial subsequence\nunique to that time point. This subsequence is then processed\nthrough a dense layer coupled with a nonlinear activation\nfunction. Then, the output of the EFE module serves as the\ninput vector for the encoder at this fixed time point. Across\nall time points, this forms a high-dimensional input sequence\nfor the encoder layer. The EFE mechanism focuses on cap-\nLi et al.\n|\nPFformer: A Position-Free Transformer Variant\narXiv\n|\n3\n\n\nc,h\n…\ndi+t+1 di+t+2 …\ndi+t+h\nXi+1\nAi+1\nXi+2\nAi+2\nXi+t\nAi+t\nI1\nI2\n…\nIh\nFig. 3. AEE module. The input of the AEE encoder is the aligned multivariate\nseries. X and A are the stream and rain value sequences in our dataset. The input\nof the AEE decoder is the time stamp of the forecasted series.\nturing the intricate relationships between different time series\nvariables across time. By constructing subsets of related se-\nquences from preceding time points, EFE directly encapsu-\nlates inter-variable dependencies within the embeddings for\neach time instance. Such an approach is particularly advan-\ntageous for the Transformer encoder, which can then operate\non these self-contained embeddings without the usual con-\nstraints imposed by positional encoding.\nAuto-Encoder-Based Embedding (AEE). Fig. 3 illus-\ntrates the Auto-Encoder-Based Embedding (AEE) layer,\nwhich is designed to handle aligned multivariate time series\ndata, such as streamflow and rainfall in our dataset.\nThe\nencoder module takes as input the aligned multivariate se-\nries, where X represents the streamflow sequence, and A\ncorresponds to the rainfall sequence. These sequences are\nprocessed step-by-step to generate the latent states c and h,\nwhich summarize the temporal patterns and dependencies in\nthe input data. The decoder module then uses the latent states,\nalong with the time stamps of the forecasted series, to predict\nfuture values.\nThe output of the AEE fulfills two key roles. First, it is inte-\ngrated with the outputs from the PFformer decoder, enhanc-\ning the overall prediction accuracy. Second, its capacity for\nshort-term prediction is measured as a component of the loss\nfunction. This inclusion acts as a penalty term, underscoring\nthe critical role of auxiliary variables in refining short-term\nforecast accuracy.\nClustering-Based Oversampling Policy. Since the total\nlength of each time series in our dataset is approximately 1.4\nmillion, the sampling strategy is crucial during model train-\ning. However, simply oversampling extreme values can de-\ngrade overall prediction quality for the rest of the time series,\nas previously demonstrated in research. To address this, we\npropose a Clustering-Based Oversampling Policy which aims\nto capture significant data points based on statistic distribu-\ntions. We employ a Gaussian Mixture Model (GMM) to clus-\nter the data into M clusters with mean values µ1,µ2,...,µM.\nSince the cluster with the highest mean value z represents\nthose extreme values, we marked a data point as important if\nits value exceeds η × z. For each peak point, we treat step\nsize s and scope ν as hyperparameters. The sampling starts\nν/2 points to the left of an identified peak, and samples are\ncollected every s points, effectively generating ν/s samples\naround each peak.\nTo manage the volume of oversampled data, we set a ra-\ntio, os, which caps the oversampled data at os% of the total\ntraining set volume. We employ grid search to determine the\noptimal value of os to ensure our sampling method remains\ntargeted and efficient without overwhelming the model with\nexcessive data.\nMultiple-Objective Loss Function. To force the AEE\nto learn rich representations, we use multiple loss func-\ntions (30–32) when training the PFformer model. We build\nour loss items as follows,\nL1 = RMSE(ˆyaux[: s],y[: s]),\nL2 = RMSE(ˆy,y).\nwhere ˆy is the output of PFformer, ˆyaux is the output of the\nAEE module after a linear transformation, and s represents\nthe length of the short-term interval, which is set to 16 (4\nhours) in our experiments. The L1 regularization component\nis responsible for short-term prediction accuracy, while L2\nfocuses on the overall model accuracy. Then, the overall loss\nis composed as,\nL = λ×L1 +L2,\nλ = max(−1·e\nepoch\n45\n+α,β).\nλ acts as a scaling factor for regularization, initially set high\nto steer the AEE to be more responsible for short-term pre-\ndiction accuracy. This ‘teacher mode’ diminishes over time;\nλ starts at α and decreases to β over epochs.\nEvaluation\nExperimental Settings. This study utilized the same time\nseries data as in (14). In our experiments, we consistently\nset N = 3, M = 1, and η = 1.2 across all datasets (Ross,\nSaratoga, UpperPen, and SFC). The oversampling percent-\nages os% were adjusted to 20% for Ross, Saratoga, and\nUpperPen, and to 15% for SFC, reflecting the unique dis-\ntributions of each dataset. The oversampling policies, de-\nnoted as (s,v), and the α and β values for the regular-\nization parameters were tailored to the distinct characteris-\ntics of each dataset. We performed a grid search with s ∈\n{1,2,4}, v ∈{4,8,16}, α ∈{1,1.2,1.5,1.8,2,2.2}, and β ∈\n{0.5,0.6,...,1} and obtained the best results at (s,v,α,β) of\n(1,8,1.5,0.9), (2,16,1.8,0.8), (4,16,2.0,0.8), and (2,16,2.0,0.9)\nfor the Ross, Saratoga, UpperPen, and SFC datasets, respec-\ntively. The hidden dimensions for the attention and linear lay-\ners were set to [384, 268, 288, 300], and for the AEE LSTM\nlayer to [384, 268, 320, 256]. Furthermore, the AEE fea-\ntured one LSTM layer for SFC and two for the other sensors.\nAll models were trained for a maximum of 40 epochs with\nearly stopping triggered after four consecutive epochs with-\nout improvement.We used the Adam gradient descent algo-\nrithm with a starting learning rate of 0.0005, decaying by 0.9\nafter every epoch, and set the batch size to 24 for SFC, 48 for\nSaratoga and UpperPen, and 96 for Ross. All models were\ntrained using PyTorch 1.11.0+cu102 on a Supermicro SYS-\n420GP-TNAR+ system equipped with NVIDIA HGX A100\n8-way GPUs (80 GB RAM each) running Rocky Linux 9.4\n(Blue Onyx), but only used one GPU for all training and in-\nference. We also verified training and inference of our model\non a system with one NVIDIA V100 GPU (32 GB RAM).\n4\n|\narXiv\nLi et al.\n|\nPFformer: A Position-Free Transformer Variant\n\n\nTable 2. 3-day/4-hour Long-Term (h = 288) Series Forecasting Results\nRMSE\nMAPE\nMethods\nRoss\nSaratoga\nUpperPen\nSFC\nRoss\nSaratoga\nUpperPen\nSFC\n3 d\n4 h\n3 d\n4 h\n3 d\n4 h\n3 d\n4 h\n3 d\n4 h\n3 d\n4 h\n3 d\n4 h\n3 d\n4 h\nFEDformer\n6.01\n3.95\n6.01\n4.82\n3.05\n2.55\n23.54\n17.11\n2.10\n2.05\n1.55\n1.54\n1.87\n1.75\n2.35\n2.16\nInformer\n7.84\n6.76\n5.04\n3.78\n5.88\n5.00\n39.89\n23.21\n4.05\n4.71\n1.43\n1.54\n4.10\n3.99\n8.64\n3.61\nNlinear\n6.10\n2.76\n5.23\n4.13\n1.57\n0.51\n18.47\n5.08\n1.99\n0.52\n0.83\n0.82\n0.45\n0.16\n0.92\n0.52\nDlinear\n7.16\n3.31\n4.33\n1.79\n3.53\n1.35\n21.62\n8.75\n3.10\n1.15\n1.40\n0.65\n2.35\n0.69\n2.74\n1.45\nLSTM-Atten\n7.35\n6.84\n6.49\n5.59\n6.35\n4.75\n34.17\n23.09\n3.74\n4.10\n1.80\n1.79\n4.76\n3.67\n9.90\n6.25\nNEC+\n9.44\n2.07\n1.88\n0.26\n2.22\n0.33\n17.00\n2.36\n4.80\n0.45\n0.17\n0.07\n0.95\n0.06\n1.07\n0.07\niTransformer\n4.56\n2.14\n2.37\n0.94\n1.12\n0.58\n17.04\n11.00\n0.57\n0.43\n0.27\n0.18\n0.11\n0.06\n0.47\n0.54\nDAN\n4.25\n2.61\n1.80\n0.62\n1.10\n0.43\n15.23\n3.73\n0.07\n0.46\n0.14\n0.22\n0.15\n0.07\n0.26\n0.22\nPFformer\n4.21\n1.52\n1.69\n0.22\n1.01\n0.24\n14.98\n2.86\n0.10\n0.03\n0.10\n0.04\n0.06\n0.01\n0.18\n0.06\nBaseline Methods. We compared our proposed method,\nPFformer1, against a wide array of state-of-the-art time se-\nries and hydrologic prediction methods we discussed in the\nRelated Works section. FEDFormer (10) enhances the stan-\ndard Transformer by incorporating seasonal-trend decompo-\nsition. Informer (9) introduces a prob-sparse self-attention\nmechanism tailored for long-term time series prediction.\nNEC+ (28) utilizes LSTM-based models optimized for hy-\ndrologic time series prediction in series with extreme events.\nNLinear (11) is an effective linear model with one order dif-\nference preprocessing for long-term time series, while DLin-\near (11) focuses on decomposing trends for similar appli-\ncations.\nAttention-LSTM (33) serves as a state-of-the-art\nmultivariate model in hydrology. Finally, iTransformer (12)\nachieved state-of-the-art results on challenging multivariate\ntime series prediction challenges, and DAN (14) learns and\nmerges rich representations to adaptively predict streamflow.\nMain Results. The experimental results, detailed in Table\n2, highlight the best and second-best values for each met-\nric, marked in bold and underline, respectively. Overall, PF-\nformer consistently outperforms other models. While mod-\nels tailored for hydrologic data forecasting, like NEC+ and\nDAN, perform better than more generic time series predic-\ntion models, they do not match the effectiveness of PFformer.\nSpecifically, PFformer not only surpasses DAN across all\nRMSE metrics but also achieves remarkable improvements\nof 23% to 64% in 4-hour rolling prediction RMSE. Com-\npared to NEC+ and iTransformer, PFformer shows an aver-\nage increase of 22% and 37% across all RMSE metrics re-\nspectively, showing its extreme-adaptive ability.\nTransformer-based methods such as FEDFormer and In-\nformer struggle significantly with datasets that exhibit large\nvariance and fail to adapt to extreme values effectively, not\nfully leveraging the attention mechanism’s potential. While\niTransformer effectively utilizes variate tokens to capture\nmultivariate correlations through attention, it risks losing\nintra-variable temporal relationships by focusing solely on\ninverted dimensions. This limitation is significant in forecast-\ning time series with extreme values, where both inter-variable\nrelationships and intra-variable temporal sequences are cru-\ncial for accuracy. Our EFE and AEE address this by preserv-\ning both. In contrast, fully connected networks like NLin-\n1Code\nfor\nour\nmethod\nis\nat\nhttps://github.com/\ndavidanastasiu/pfformer.\near and DLinear, which decompose data into main trends and\nresiduals, perform better than some Transformer-based meth-\nods, particularly in rolling prediction scenarios. However,\nthey still fall short of achieving high accuracy.\n3-Days Prediction. In the realm of single-shot 3-day predic-\ntions, PFformer consistently outperforms baselines in RMSE\nacross all four sensors. Notably, when compared to DAN,\nPFformer achieves a 60% improvement in MAPE for the\nUpperPen dataset.\nFurthermore, against NEC+, PFformer\ndemonstrates substantial enhancements on the Ross and Up-\nperPen datasets, with over 50% improvement in RMSE and\nover 90% in MAPE. For the Saratoga dataset, PFformer\noutperforms iTransformer by 28% and 62% in RMSE and\nMAPE, respectively.\n4-Hour Rolling Prediction. In the scenario of 4-hour rolling\npredictions, which is more common in practical applica-\ntions, the PFformer model showcases exceptional perfor-\nmance. Compared to DAN, it achieves an average RMSE\nimprovement of 43% and a MAPE improvement of 83%.\nAgainst NEC+, PFformer demonstrates a 22% improvement\nin RMSE and a 55% increase in MAPE. Compared with\niTransformer, PFformer is superior by 59% and 85% in\nRMSE and MAPE, respectively. This scenario holds signifi-\ncant practical value, reflecting the model’s strong applicabil-\nity in real-world settings.\nVisual Analysis. Fig. 4 shows some example predictions for\nPFformer and the next two best performing algorithms. As\nseen in the figure, PFformer’s predictions closely match the\nground truth, particularly in datasets with considerable oscil-\nlations. Notably, PFformer more accurately captures short-\nterm oscillations and severe values than DAN, as evidenced\nby lower RMSE values. This improved performance is pri-\nmarily due to the rich expressive capabilities of the embed-\ndings mixed with the attention mechanism.\nAblation Study\nIn this ablation study, we aim to explore several key re-\nsearch questions that address the prediction effectiveness of\nour model, including (1) the impact of our clustering-based\noversampling policy on model performance; (2) the signifi-\ncance of subsequence length s in our model’s EFE module;\n(3) the influence of loss function parameters on its predictive\nLi et al.\n|\nPFformer: A Position-Free Transformer Variant\narXiv\n|\n5\n\n\n0\n50\n100\n150\n200\n250\n300\n350\n400\ntime points (every 15 minutes)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nstreamflow\nGround Truth\nDAN : 0.65\nNEC+ : 0.89\nPFformer : 0.09\n0\n50\n100\n150\n200\n250\n300\n350\n400\ntime points (every 15 minutes)\n0\n10\n20\n30\n40\nstreamflow\nGround Truth\nDAN : 11.09\nNEC+ : 9.94\nPFformer : 4.26\nFig. 4. Comparative examples with the best baselines reveal thatPFformer excels in predicting streamflow values with greater accuracy, both in the short and long terms.\naccuracy; and (4) the overall effect of both the EFE and AEE\nmodules.\nEffect of Parameters. To address the first three research\nquestions, we first established the optimal parameter com-\nbination for the Ross dataset as our baseline.\nThen, fix-\ning all other parameters, adjusted only one parameter at\na time to analyze its influence on model performance.\nFig. 5 shows our results for oversampling percentages os ∈\n{10,20,30,40,50,60} (left), the EFE susequence length s ∈\n{20,40,60,80,100,120} (center), and the loss multiplier pa-\nrameter α ∈{0,1,1.5,2.0,2.5,3.0} (right), for both 3-day\nprediction (y1-axis) and 4-hour prediction (y2-axis) scenar-\nios.\nOversampling. For the 3-day RMSE, the model performs\nbest at a 20% rate, while short-term predictions are more sen-\nsitive to extreme values in the training set, but a 20% rate is\nthe best overall.\nEFE Subsequence Length. Regarding the EFE parameter s,\ntheoretically, a larger s contains more information. By ad-\njusting the accompanying hidden size and number of layers,\na better model might be found, but this also increases redun-\ndancy and computation load. In our experiments, s = 60 was\nbest.\nLoss Regularization. Higher values of the loss regulariza-\ntion parameter α force PFformer to focus more on the accu-\nracy of short-term predictions, but should be balanced with\nthe need for overall good long-term predictions. In our ex-\nperiment, α = 1.5 produced the best results in both 3-day\nahead and 4-hour predictions.\nTable 3. PFformer RMSE With Different Embeddings\nDataset\nType\nEFE/AEE\nPosition\nToken\nRoss\n3-day\n4.21\n4.41\n4.55\nrolling\n1.52\n2.24\n2.61\nSFC\n3-day\n14.98\n21.68\n21.38\nrolling\n2.86\n17.13\n16.72\nUpperPen\n3-day\n1.01\n3.91\n2.44\nrolling\n0.24\n3.92\n1.47\nSaratoga\n3-day\n1.69\n2.59\n3.88\nrolling\n0.29\n1.71\n3.88\nEffect of Architecture. In Table 3, the first column shows\nthe RMSE values obtained when using both the EFE and\nAEE layers. In the second column, we removed EFE and\nAEE and reverted to using the original Transformer’s combi-\nnation of position embedding and token embedding. In the\nthird column, we eliminated position embedding altogether\nand solely utilized token embedding. The results show that\nthe EFE and AEE introduced by PFformer significantly en-\nhance overall predictive performance by enabling the atten-\ntion layer to focus on short-term performance in a rolling pre-\ndiction mode. Moreover, the position embedding inherent in\ntraditional Transformers has an inconsistent effect on time se-\nries forecasting, failing to enhance performance on datasets\nlike SFC and UpperPen.\nConclusion\nThis study has successfully demonstrated the efficacy of\nposition-free trainable embedding techniques—Enhanced\nFeature-based Embedding (EFE) and Auto-Encoder-based\nEmbedding (AEE)—in improving hydrologic flow predic-\ntion.\nBy comparing these techniques against traditional\nTransformer embeddings and other state-of-the-art methods,\nwe have shown that our method significantly enhances fore-\ncasting accuracy, especially in scenarios with extreme values.\nOur findings suggest that traditional positional information\nmay be less crucial for time series than previously thought,\nindicating a promising direction for future research to further\nrefine these models for broader applications in complex time\nseries forecasting scenarios.\nBibliography\n1.\nYanhong Li and David C. Anastasiu. Multivariate segment expandable encoder-decoder\nmodel for time series forecasting. IEEE Access, 12:185012–185026, 2024. doi: 10.1109/\nACCESS.2024.3513256.\n2.\nSicheng Zhou and David C. Anastasiu. Long-term hydrologic time series prediction with\nlspm. In Proceedings of the 33rd ACM International Conference on Information and Knowl-\nedge Management, CIKM ’24, page 4308–4312, New York, NY, USA, 2024. Association for\nComputing Machinery. ISBN 9798400704369. doi: 10.1145/3627673.3679957.\n3.\nKady Sako, Berthine Nyunga Mpinda, and Paulo Canas Rodrigues. Neural networks for\nfinancial time series forecasting. Entropy, 24(5):657, 2022.\n4.\nSaloni Mohan, Sahitya Mullapudi, Sudheer Sammeta, Parag Vijayvergia, and David C.\nAnastasiu. Stock price prediction using news sentiment analysis. In 2019 IEEE Fourth In-\nternational Conference on Big Data Computing Service and Applications (BigDataService),\nBDS 2019. IEEE, April 2019.\n5.\nYu-Hsiu Lin, Huei-Sheng Tang, Ting-Yu Shen, and Chih-Hsien Hsia. A smart home energy\nmanagement system utilizing neurocomputing-based time-series load modeling and fore-\ncasting facilitated by energy decomposition for smart home automation. IEEE Access, 10:\n116747–116765, 2022.\n6.\nMi Zhang, Daizong Ding, Xudong Pan, and Min Yang. Enhancing time series predictors with\ngeneralized extreme value loss. IEEE Transactions on Knowledge and Data Engineering,\n2021.\n7.\nYifan Zhang, Jiahao Li, Ablan Carlo, Alex K Manda, Scott Hamshaw, Sergiu M. Dascalu,\nFrederick C. Harris, and Rui Wu.\nData regression framework for time series data with\nextreme events.\nIn 2021 IEEE International Conference on Big Data (Big Data), pages\n5327–5336, 2021. doi: 10.1109/BigData52589.2021.9671387.\n8.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\n9.\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\nZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting.\nIn Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106–\n11115, 2021.\n6\n|\narXiv\nLi et al.\n|\nPFformer: A Position-Free Transformer Variant\n\n\n10\n20\n30\n40\n50\n60\nOversampling rate (os)\n4.25\n4.35\n4.45\n4.55\n4.65\n3-day RMSE\n3-day RMSE\n1.55\n1.75\n1.95\n2.15\n2.35\n2.55\n4-hour RMSE\n4-hour RMSE\n20\n40\n60\n80\n100\n120\nSubsequence length (s)\n4.20\n4.25\n4.30\n4.35\n4.40\n3-day RMSE\n3-day RMSE\n1.55\n1.65\n1.75\n1.85\n1.95\n2.05\n4-hour RMSE\n4-hour RMSE\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLoss multiplier ( )\n4.20\n4.25\n4.30\n4.35\n4.40\n3-day RMSE\n3-day RMSE\n1.55\n1.65\n1.75\n1.85\n1.95\n2.05\n4-hour RMSE\n4-hour RMSE\nFig. 5. PFformer RMSE on Ross given different oversampling percentage os (left), EFE subsequence length s (center), and loss multiplier α (right) values, under both 3-day\nprediction (y1-axis) and 4-hour prediction (y2-axis) scenarios.\n10.\nTian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:\nFrequency enhanced decomposed transformer for long-term series forecasting. In Interna-\ntional Conference on Machine Learning, pages 27268–27286. PMLR, 2022.\n11.\nAiling Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\nforecasting? arXiv preprint arXiv:2205.13504, 2022.\n12.\nYong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng\nLong. itransformer: Inverted transformers are effective for time series forecasting. arXiv\npreprint arXiv:2310.06625, 2023.\n13.\nYunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension depen-\ndency for multivariate time series forecasting. In The eleventh international conference on\nlearning representations, 2023.\n14.\nYanhong Li, Jack Xu, and David Anastasiu. Learning from polar representation: An extreme-\nadaptive model for long-term time series forecasting. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 38, pages 171–179, 2024.\n15.\nHelmut Lütkepohl. Vector autoregressive models. In Handbook of research methods and\napplications in empirical macroeconomics, pages 139–164. Edward Elgar Publishing, 2013.\n16.\nZhi-Yu Wang, Jun Qiu, and Fang-Fang Li. Hybrid models combining emd/eemd and arima\nfor long-term streamflow forecasting. Water, 10(7), 2018. ISSN 2073-4441. doi: 10.3390/\nw10070853.\n17.\nZonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang.\nConnecting the dots: Multivariate time series forecasting with graph neural networks. In The\n26th ACM SIGKDD international conference on knowledge discovery & data mining, pages\n753–763, 2020.\n18.\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition\ntransformers with auto-correlation for long-term series forecasting. Advances in Neural In-\nformation Processing Systems, 34:22419–22430, 2021.\n19.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\narXiv preprint arXiv:2001.04451, 2020.\n20.\nAbhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu. Long-term fore-\ncasting with tide: Time-series dense encoder. arXiv preprint arXiv:2304.08424, 2023.\n21.\nEmadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xi-\naoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual\ncontrasting. arXiv preprint arXiv:2106.14112, 2021.\n22.\nYanhong Li, Jack Xu, and David C. Anastasiu. Seed: An effective model for highly-skewed\nstreamflow time series data forecasting. In 2023 IEEE International Conference on Big Data\n(Big Data), IEEE BigData 2023, Los Alamitos, CA, USA, Dec 2023. IEEE Computer Society.\n23.\nHao Li, Jie Shao, Kewen Liao, and Mingjian Tang. Do simpler statistical methods perform\nbetter in multivariate long sequence time-series forecasting?\nIn Proceedings of the 31st\nACM International Conference on Information & Knowledge Management, pages 4168–\n4172, 2022.\n24.\nYuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is\nworth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730,\n2022.\n25.\nJinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using\nreconstruction probability. Special lecture on IE, 2(1):1–18, 2015.\n26.\nNikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event\nforecasting with neural networks at uber. In International conference on machine learning,\nvolume 34, pages 1–5. sn, 2017.\n27.\nDaizong Ding, Mi Zhang, Xudong Pan, Min Yang, and Xiangnan He. Modeling extreme\nevents in time series prediction. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pages 1114–1122, 2019.\n28.\nYanhong Li, Jack Xu, and David C Anastasiu. An extreme-adaptive time series prediction\nmodel based on probability-enhanced lstm neural networks. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 37, pages 8684–8691, 2023.\n29.\nN. E. Day. Estimating the components of a mixture of normal distributions. Biometrika, 56\n(3):463–474, 12 1969. ISSN 0006-3444. doi: 10.1093/biomet/56.3.463.\n30.\nSachin Sudhakar Farfade, Mohammad J Saberian, and Li-Jia Li. Multi-view face detection\nusing deep convolutional neural networks. In Proceedings of the 5th ACM on International\nConference on Multimedia Retrieval, pages 643–650, 2015.\n31.\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task\nrelationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the\n24th ACM SIGKDD international conference on knowledge discovery & data mining, pages\n1930–1939, 2018.\n32.\nAlex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 7482–7491, 2018.\n33.\nYan Le, Changwei Chen, Ting Hang, and Youchuan Hu.\nA stream prediction model\nbased on attention-lstm.\nEarth Science Informatics, 14:1–11, 06 2021.\ndoi: 10.1007/\ns12145-021-00571-z.\nLi et al.\n|\nPFformer: A Position-Free Transformer Variant\narXiv\n|\n7\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20571v1.pdf",
    "total_pages": 7,
    "title": "PFformer: A Position-Free Transformer Variant for Extreme-Adaptive Multivariate Time Series Forecasting",
    "authors": [
      "Yanhong Li",
      "David C. Anastasiu"
    ],
    "abstract": "Multivariate time series (MTS) forecasting is vital in fields like weather,\nenergy, and finance. However, despite deep learning advancements, traditional\nTransformer-based models often diminish the effect of crucial inter-variable\nrelationships by singular token embedding and struggle to effectively capture\ncomplex dependencies among variables, especially in datasets with rare or\nextreme events. These events create significant imbalances and lead to high\nskewness, complicating accurate prediction efforts. This study introduces\nPFformer, a position-free Transformer-based model designed for single-target\nMTS forecasting, specifically for challenging datasets characterized by extreme\nvariability. PFformer integrates two novel embedding strategies: Enhanced\nFeature-based Embedding (EFE) and Auto-Encoder-based Embedding (AEE). EFE\neffectively encodes inter-variable dependencies by mapping related sequence\nsubsets to high-dimensional spaces without positional constraints, enhancing\nthe encoder's functionality. PFformer shows superior forecasting accuracy\nwithout the traditional limitations of positional encoding in MTS modeling. We\nevaluated PFformer across four challenging datasets, focusing on two key\nforecasting scenarios: long sequence prediction for 3 days ahead and rolling\npredictions every four hours to reflect real-time decision-making processes in\nwater management. PFformer demonstrated remarkable improvements, from 20% to\n60%, compared with state-of-the-art models.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}