{
  "id": "arxiv_2502.20629v1",
  "text": "Highlights\nTowards Privacy-Preserving Split Learning: Destabilizing Adversarial Inference and Reconstruc-\ntion Attacks in the Cloud\nGriffin Higgins,Roozbeh Razavi-Far,Xichen Zhang,Amir David,Ali Ghorbani,Tongyu Ge\n• The end-to-end pipeline in the system is implemented, which includes the primary image classification with edge-\ncloud collaboration architecture, the image reconstruction models based on white-box attack, and the sensitive attribute\ninference attack. The basic components can support comprehensive model evaluations and results comparison.\n• An effective plug-in approach is developed and designed for privacy protection in the system. This novel technique\nis the combination of AE and CAMs, which can provide explainable and attribute-specific privacy protection with all\nthe plug-in properties, without having to retrain or modify the original model. Training of the plug-in protection is\nallowed.\n• A comprehensive set of experiments are conducted to validate the performance of the proposed solutions. The\nsuperiority of the proposed novel method with respect to the primary task’s performance, image reconstruction, and\nsensitive attribute inference. Under certain experimental settings, upon applying the proposed protection technology,\nperformance loss at the cloud-side model is around 4%, whereas the performance drop at the adversarial side is more\nthan 20%.\narXiv:2502.20629v1  [cs.CR]  28 Feb 2025\n\n\nTowards Privacy-Preserving Split Learning: Destabilizing Adversarial\nInference and Reconstruction Attacks in the Cloud\nGriffin Higginsa, Roozbeh Razavi-Fara,∗,1, Xichen Zhangb, Amir Davida, Ali Ghorbania and\nTongyu Gec\naCanadian Institute for Cybersecurity, University of New Brunswick, 46 Dineen Drive, Fredericton, E3B 5A3, New Brunswick, Canada\nbSobey School of Business, Saint Mary’s University, Halifax, B3H 3C3, Nova Scotia, Canada\ncHuawei Technologies Canada, 300 Hagey Blvd, Waterloo, N2L 0A4, Ontario, Canada\nA R T I C L E I N F O\nKeywords:\nSplit Learning\nEdge-cloud Collaborative Systems\nPrivacy-Preserving Learning\nAutoencoder\nDimensionality Reduction\nPrivacy and Utility\nA B S T R A C T\nThis work aims to provide both privacy and utility within a split learning framework while considering\nboth forward attribute inference and backward reconstruction attacks. To address this, a novel\napproach has been proposed, which makes use of class activation maps and autoencoders as a plug-in\nstrategy aiming to increase the user’s privacy and destabilize an adversary. The proposed approach\nis compared with a dimensionality-reduction-based plug-in strategy, which makes use of principal\ncomponent analysis to transform the feature map onto a lower-dimensional feature space. Our work\nshows that our proposed autoencoder-based approach is preferred as it can provide protection at an\nearlier split position over the tested architectures in our setting, and, hence, better utility for resource-\nconstrained devices in edge-cloud collaborative inference () systems.\n1. Introduction\nThe last decade has witnessed unprecedented growth in\nthe number of smart and Internet of Things (IoT) devices,\nsuch as mobile phones, tablets, smartwatches, GPS-enabled\ndevices, smart sensors, AI-driven smart assists, edge de-\nvices, and other wearable devices (Dhar, Guo, Liu, Tripathi,\nKurup and Shah, 2021). In addition, Artificial Intelligence\n(AI) techniques have achieved remarkable progress in var-\nious real-world applications, such as natural language un-\nderstanding and translation (Liu, Zhang, Wang, Hou, Yuan,\nTian, Zhang, Shi, Fan and He, 2023), image classification\n(Li, Liu, Yang, Peng and Zhou, 2021b), object detection\nand segmentation (Zhao, Zheng, Xu and Wu, 2019), and\nvideo processing (Jiao, Zhang, Liu, Yang, Hou, Li and Tang,\n2021). The extensive advancement of AI technologies re-\nsulted in the fast proliferation of on-device AI services. It has\nnever been easier than today to use AI-driven models on our\nmobile devices for data labeling, image processing, business\nanalytics, and decision making (Cao, Zhang and Cao, 2021;\nMuhammad, Khan, Del Ser and De Albuquerque, 2020; Li,\nMei, Prokhorov and Tao, 2016). With on-device AI models\nlots of tasks can be completed in a real-time, low latency, and\neven offline manner. Typical tasks include barcode scanning,\nface detection, image classification, object detection and\ntracking, language translation, selfie segmentation, and text\nrecognition.\nEven though the on-device AI applications have bright\nand broad marketing prospects, they still face critical chal-\nlenges. The state-of-the-art AI models, such as Visual\nGeometry Group (VGG) (Simonyan and Zisserman, 2014),\nResidual Network (ResNet) (He, Zhang, Ren and Sun,\n∗Corresponding author\nroozbeh.razavifar@unb.ca ( Roozbeh Razavi-Far)\nORCID(s):\n2016), and transformers (Vaswani, Shazeer, Parmar, Uszko-\nreit, Jones, Gomez, Kaiser and Polosukhin, 2017) are not\nonly well known for their superb performance but their huge\nsize and sophisticated architecture as well. For instance,\nthe popular ResNet50 and VGG16 models have 23 million\nand 138 million parameters, respectively (Luo, Pearson, Xu\nand Rich, 2022). As for the most famous GPT3 model, its\nnumber of parameters is staggering, as high as 175 billion\n(Gao, Tong, Wu, Chen, Zhu and Wang, 2023). Therefore, as\none major issue, it is infeasible to deploy huge and complex\nAI models on edge devices due to their computational and\nstorage limitations.\nIn recent years, a new computational architecture, \nsystem, has received considerable attention in both academia\nand industries. Rather than deploying the whole model on\nthe edge device, in the system, a deep neural network\n(DNN) is split into two components. The shallow part of the\nDNN model is deployed on the edge devices, whereas the\nlarger portion is installed on the cloud side (Wang, Zhang,\nBao, Zhu, Cao and Yu, 2018). In systems, clients’ raw\ndata is fed into the client-side model and transformed into\nabstract feature maps. Then, these feature maps are sent\nto the cloud-side model for the final inference and predic-\ntion (He, Zhang and Lee, 2020; Wang, He, Castiglione,\nGupta, Karuppiah and Wu, 2022). Fig. 1 shows a typical\nConvolutional Neural Network (CNN) architecture in the\nsystems, where the first two convolutional layers are\nin the edge devices as the client model, and the rest of\nthe layers (including convolutional layers, flatten layers, and\nthe fully connected layers) are deployed on the cloud side\nas the cloud model. With the collaboration between edge\ndevices and cloud servers, users cannot only enjoy advanced\nhigh-performance AI services without cumbersome compu-\ntational overheads but also avoid sharing their original local\ndata with other parties.\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 1 of 15\n\n\nTowards Privacy-Preserving Split Learning\nFigure 1: The typical architecture of the system.\nNevertheless, the privacy issues are still unresolved,\nwhich hinders the further adoption of the systems into\nmore practical applications. To be specific, the feature maps\ntransferred between the edge devices and the cloud platforms\nstill contain sensitive information. By taking advantage\nof advanced attacking methods, adversaries and malicious\ncloud servers can infer private knowledge from the trans-\nferred feature maps. For instance, model inversion attacks\ncan be performed in the system to reconstruct images in\nan image classification system (He et al., 2020; He, Zhang\nand Lee, 2019; Gu, Huang, Zhang, Su, Jamjoom, Lamba,\nPendarakis and Molloy, 2018a; Yang, Zheng, Zhang, Chen,\nWong and Li, 2022a; Duan, Wang, Ren, Lyu, Zhang, Wu and\nShen, 2022). By querying the client model and generating\nan adversarial training dataset, attackers can build effective\ndeep learning (DL) models to get sensitive information as\nwell (Wang, Bao, Sun, Zhu, Cao and Philip, 2019; Osia,\nShamsabadi, Sajadmanesh, Taheri, Katevas, Rabiee, Lane\nand Haddadi, 2020; Chi, Owusu, Yin, Yu, Chan, Tague\nand Tian, 2018; Osia, Shamsabadi, Taheri, Katevas, Rabiee,\nLane and Haddadi, 2017; Erdoğan, Küpçü and Çiçek, 2022).\nThere are a number of endeavors that aim to protect\nclient’s data privacy from being disclosed in the sys-\ntems, which can be mainly divided into two categories.\nThe first category is called the plug-in-based approaches,\nwhich mainly includes the dimensionality reduction (DR)\napproach and noisy injection. The second category is called\nthe non-plug-in approaches, where adversarial training and\nmodel compression are the typical examples. Currently,\nadversarial training is the most commonly studied solution\n(Li, Rakin, Chen, He, Fan and Chakrabarti, 2022; Thapa,\nMahawaga Arachchige, Camtepe and Sun, 2022). In such\napproaches, the loss functions are usually modified based\non privacy purposes. Essentially, by adding new items in\nthe loss function, the modified network architecture can\nmaintain the primary task as accurately as possible, but\nreduce the adversarial model’s sensitive task performance to\na large extent. Here the primary or sensitive task can be clas-\nsification, regression, or any type of machine learning (ML)\ntask. Typical representations of such techniques are Wang\net al. (2018); Li, Guo, Yang, Salim and Chen (2021a); Ding,\nFang, Zhang, Choo and Jin (2020). One major drawback of\nthe non-plug-in approaches such as adversarial training is\nthat extra modifications should be performed on the neural\nnetwork models, which makes the new pipeline not\ncompatible with the existing AI algorithms. Moreover, an\nadversarial training dataset should be collected and extra\ntraining should be conducted before the new models are\nlaunched. Compared with the non-plug-in approaches, plug-\nin techniques are well known for their lightweight and easy-\nto-perform properties. Considering the add-on component,\nplug-in solutions usually add random noise into the fea-\nture maps, which does not require any data collection or\ntraining on the existing DL models. Except for other good\ncharacteristics, such as low communication overheads and\nlow computational costs, its limitations are also obvious.\nAfter adding noise, the perturbed feature maps can affect the\nadversary’s attacking model and the cloud-side model at the\nsame time. In other words, although clients’ privacy can be\npreserved, the performance of the primary task will also be\ndegraded and the utility cannot be maintained.\nWith many works focusing on non-plug-in privacy-\npreserving solutions, we found that the plug-in approaches\nhave not been adequately studied in the past. Most of the\nplug-in methods are used as the baseline techniques for\ncomparison purposes (Osia et al., 2020). In this paper, we\npropose a novel plug-in-based solution, called Autoencoder-\nbased Delta Protection (ADP), by combining the autoen-\ncoder (AE) (Zhai, Zhang, Chen and He, 2018) and Class\nActivation Maps (CAMs) (Zhou, Khosla, Lapedriza, Oliva\nand Torralba, 2016) for protecting the sensitive information\nin the image inference system. In particular, in attacking\nsimulations, CAMs are first used to find the most essential\nand important regions in the images where the adversarial\nmodel is focusing. Next, a blurring technique is used on\nthe image to filter out the most significant region for the\nsensitive attribute inference, but preserve the information for\nthe primary task. After that, AE is trained and optimized\nusing the feature maps that are coming from the blurred\nimages and will be for the privacy protection in the \nplatforms. The major contributions of this paper can be\nspecified as follows:\n• The end-to-end pipeline in the system is imple-\nmented, which includes the primary image classi-\nfication with edge-cloud collaboration architecture,\nthe image reconstruction models based on white-box\nattack, and the sensitive attribute inference attack. The\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 2 of 15\n\n\nTowards Privacy-Preserving Split Learning\nbasic components can support comprehensive model\nevaluations and results comparison.\n• An effective plug-in approach is developed and de-\nsigned for privacy protection in the system. This\nnovel technique is the combination of AE and CAMs,\nwhich can provide explainable and attribute-specific\nprivacy protection with all the plug-in properties,\nwithout having to retrain or modify the original \nmodel. Training of the plug-in protection is allowed.\n• A comprehensive set of experiments are conducted\nto validate the performance of the proposed solu-\ntions. The superiority of the proposed novel method\nwith respect to the primary task’s performance, im-\nage reconstruction, and sensitive attribute inference.\nUnder certain experimental settings, upon applying\nthe proposed protection technology, performance loss\nat the cloud-side model is around 4%, whereas the\nperformance drop at the adversarial side is more than\n20%.\nThe experimental evaluations are based on the state-of-\nthe-art object detection models. The remainder of this paper\nis organized as follows. Section II reviews related state-of-\nthe-art works. Section III describes the problem statement.\nSection IV describes our proposed plug-in-based protection\napproach, which is based on AE and CAMs. This sec-\ntion also explains the Principal Component Analysis (PCA)\nbased protection approach, which is used as a competitor to\nevaluate the performance of the proposed novel technique.\nThe set-up description, parameter settings, and experimental\nresults are discussed in Section V. Finally, the paper is\nconcluded in Section VI.\n2. Literature Review\nIn this section, we review the state-of-the-art privacy-\nprotection solutions in settings. Additionally, we ex-\namine similar solutions in non-settings where relevant\n(Zhang, Razavi-Far, Isah, David, Higgins and Zhang, 2025).\nPrincipal Component Analysis (PCA), as a represen-\ntative dimensionality reduction algorithm, is widely used\nin several works to protect client-side outputs (i.e., feature\nmaps) (Malekzadeh, Clegg and Haddadi, 2017; Osia et al.,\n2020, 2017). Osia et al. (2020), employ PCA as a feature\nextractor to retain essential information while removing\nunnecessary details from client-side input (e.g., images). A\ndense reduction matrix is applied in the final layer to main-\ntain consistent input/output configuration for DL models.\nAnother common plug-in approach is the use of noisy\ninjections, employed in various works such as He et al.\n(2020); Mireshghallah, Taram, Ramrakhyani, Tullsen and\nEsmaeilzadeh (2019); Mao, Hong, Wang, Li and Zhong\n(2020); Wang et al. (2018). For example, in Wang et al.\n(2018), Differential Privacy (DP) noise is introduced to\ntransform client-side outputs securely. Specifically, the client-\nside input is multiplied by a mask matrix for nullification,\nfollowed by the addition of differential privacy noise at a\nparticular layer of the client-side model to further enhance\nprivacy. In some cases, noisy injections are combined with\nadversarial training techniques to increase the model’s ro-\nbustness to random perturbation after training.\nIn non-settings, Zheng, Chen, Shangguan, Ming,\nYang and Yang (2023) introduce GONE, which uses DP to\nprotect the output class probabilities targeted by adversaries\nin membership inference and model-stealing attacks. How-\never, this method only protects server-side outputs. Regard-\ning the adversarial training solutions, Li et al. (2021a) train\nfour components simultaneously, an obfuscator, classifier,\nadversary reconstructor, and adversary classifier. Similarly,\nDing et al. (2020) address both the primary image classifi-\ncation task and sensitive attribute inference attacks concur-\nrently. By designing specific loss functions, their proposed\nframework ensures the primary task minimizes loss while\nmaximizing the loss for attackers.\nWang et al. (2018) propose a noisy training technique\ncombining differential privacy, generative adversarial train-\ning, and transfer learning. During the noisy training phase,\nboth the original and perturbed client-side inputs are sent to\nthe DL model for parameter optimization, making the model\nmore robust and stable in real-world applications.\nInspired by the Mix-up architecture, Liu, Wu, Gan, Zhu\nand Han (2020) introduce a privacy protection technique\nfor the systems, called Datamix. This approach applies\nmixing and de-mixing operations to both the client-side\ninputs and the server-side outputs. A key difference of this\nwork is that the entire CNN model is divided into three com-\nponents instead of two. The client runs both the first and third\nmodels as a feature extractor and predictor, respectively. This\ndesigned enables the platform to achieve high accuracy,\nefficiency, and privacy preservation.\nIn non-settings, generative adversarial networks\n(GANs) are used by Kairouz, Liao, Huang, Vyas, Welfert\nand Sankar (2022); Singh, Garza, Chopra, Vepakomma,\nSharma and Raskar (2022); Nguyen, Zhuang, Wu and\nChang (2020); Li, Duan, Yang, Chen and Yang (2020);\nJamshidi, Veisi, Mojahedian and Aref (2024) to establish an\nadversarial interaction between an encoder, adversary, and\nmodel. In this setup, the encoder learns to apply protection\nin the form of noise, optimized through a discriminator\nand generator during training to maximize the utility for\nthe model and minimize the utility for the adversary (i.e.,\nenhancing privacy). The noise applied may serve to mask,\ndistort, censor, obfuscate, decorrelate, decouple, or other-\nwise separate sensitive features from primary features in the\noriginal input, which is presumably drawn from the same\ntraining distribution. In some settings the output may not\nretain the same dimensionality as the input, which can limit\nits applicability. Additionally, a threshold may be applied\nduring the learning process to control the utility privacy\ntrade-off. The protection mechanism can be either general or\nspecific, depending on whether the adversarial task is known\nin addition to the strength of the adversary.\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 3 of 15\n\n\nTowards Privacy-Preserving Split Learning\nImportantly, the deployment of GAN-based protection\nin non-settings is typically aimed at safeguarding client-\nside inputs. However, this type of protection is less effective\nin an system, where client-side inputs, and their respec-\ntive intermediate representations, remain entirely private. As\na result, only client-side outputs require protection, making\nthe proposed protections overly eager. Moreover, the client-\nside utility is further impacted if a retraining restriction is\nimposed on the client, preventing adjustments for protection-\nrelated perturbations in the input. This is distinct from train-\ning the protection method itself, which is permissible as long\nas it is plug-in in nature. Nevertheless, it may be possible to\nadapt some of the aforementioned GAN-based approaches\nto protect client-side outputs in an system. To the best of\nour knowledge, this remains unexplored.\nApart from noisy injection and adversarial training\ncryptographic techniques (Rouhani, Hussain, Lauter and\nKoushanfar, 2018; Mishra, Lehmkuhl, Srinivasan, Zheng\nand Popa, 2020; Gu, Huang, Zhang, Su, Lamba, Pendarakis\nand Molloy, 2018b) and model compression (Alwani, Wang\nand Madhavan, 2022; Lee, Kohlbrenner, Shinde, Asanović\nand Song, 2020; Yang, Zhang, Zhao, Song, Guo and Li,\n2022b; Zhang, Zhang, Zhang and Yuan, 2022) are also used.\nAs a typical example of cryptographic solutions, Rouhani\net al. (2018) propose a reconfigurable hardware-accelerated\nframework for empowering a privacy-preserving inference\nsystem, called ReDCrypt. In the proposed system, the client\ncan dynamically analyze their data over time without the\nrequirement of queuing the samples to meet a certain batch\nsize. They require neither re-training the AI models nor\nrelying on two non-colluding servers in their system design.\nBy executing the popular Yao’s Garbled Circuit protocol,\nthe inference system is divided into two phases, namely,\nthe privacy-insensitive computation at the client-side, and\nthe privacy-sensitive computation between the client and\nthe cloud. Experimental results show that the designed\nsolution can output 57-fold higher throughput per core with\nno obvious accuracy loss compared with the state-of-the-art\nsolutions. Liu, Juuti, Lu and Asokan (2017) study a novel\nsolution that can transform an existing DNN model into an\noblivious neural network supporting privacy protection for\nedge-cloud collaborative inference systems. The proposed\nmodel outperforms other solutions in terms of response\nlatency and data size. Gu et al. (2018b) aims to protect\nthe security of input data in the edge-cloud collaborative\nsystems based on a trusted execution environment. Potential\ninformation exposure in the edge-cloud-based framework is\ninvestigated systematically and a secure enclave technique is\ndesigned against reconstruction privacy attack. In addition,\nthe adversaries’ attacking capacities are quantified with\ndifferent threat models and attacking strategies.\nIn summary, each solution has its own advantages and\ndrawbacks. Cryptographic solutions and adversarial training\ncan guarantee the performance of the primary task while\nprotecting the client privacy, but they introduce significant\ncomputational and communication overhead. On the other\nhand, non-solutions, when applied in the setting, are\neither too lenient, only protecting the server outputs (and\nthus offering no privacy benefit), or overly aggressive, pro-\ntecting the client inputs (which reduces utility without pro-\nviding additional privacy). Model compression is straight-\nforward to deploy on the client model but suffers from low\nperformance, as it is very challenging for a compact model\nto achieve high classification accuracy. In contrast to prior\napproaches, we focus plug-in solutions to protect client-side\noutputs. Without requiring any retraining of the existing\nCNN models or modifications to the network architecture,\nour proposed model achieves a reasonable balance between\nutility and privacy.\n3. Problem Statement\nSplit learning can be leveraged to improve the compu-\ntational performance (or load) by splitting an existing pre-\ntrained model into two halves. The first is placed on the client\nand the second is placed on the server. A shallower client-\nside split is preferable since the client is assumed to be a\nresource-constrained device, Here, an image is first passed\nthrough the client-side layers until the given split position.\nThen the output, a feature map, is passed from the client to\nthe server. The server then passes the feature map through\nthe rest of its server-side layers before reaching the final\ninference layer. This technique improves the utility on the\nserver side since the server does not have to pass the feature\nmap through the entire architecture. Thus, the server’s utility\nis improved. Most importantly, unless either the client or\nserver weights are retrained or modified the output of the\nsplit model at any given split is equivalent to the original\nnon-split model.\nHowever, both split and non-split scenarios elicit major\nprivacy concerns as they relate to the information passed\nfrom the client to the server. Formally the privacy problem\nreduces to: how can the server be trusted to only infer\ntasks that it primarily purports to instead of other more\nsensitive tasks? For example, consider a server model that\ncan accurately detect dogs in images. Also consider a client\nimage sent to the server containing both a cat and a dog in\nthe same image. A curious or malicious server, referred to\nhereafter as the adversary, could trivially detect if there is a\ncat in the image by simply using a different model. One that\nis trained to detect cats instead of dogs. This same scenario is\npresent in the split case but requires that the adversary have\nthe ability to query a shadow client model. The adversary\nwould then uses the shadow model to generate a new labeled\ndataset of feature maps to labels based on the output of the\nclient 𝐶. This dataset can then be used to train a server-\nside adversary model, 𝐴𝑓, to predict a sensitive task other\nthan that which it purports to. This is defined hereafter as\nforward inference attack. Additionally, an adversary, 𝐴𝑏,\ncould also attempt to reconstruct the original image. This\ncan be done by passing the received feature map through an\ninverted client model or another method, defined hereafter as\nbackward reconstruction attack. Both these adversarial sce-\nnarios, shown in Fig. 2, are realistic since the cloud already\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 4 of 15\n\n\nTowards Privacy-Preserving Split Learning\nFigure 2: Split model primary and sensitive tasks, partially adapted from Gildenblat and contributors (2021).\nacts as a semi-trusted entity with the special knowledge,\ndata, and resources needed to realize the attack that a third\nparty presumably would not. For example, decrypted client-\nside feature maps, initial training dataset, shadow client\nmodel weights, etc. Additionally, in this setting the cloud\nis recognized to have relatively unbounded computation\npower to realize the aforementioned attacks, adding to the\nseriousness of the privacy risks.\nTo address these privacy risks, it is important to develop\nprotection specifically designed to provide a high degree\nof privacy while also enabling a high degree of utility.\nBased on the aforementioned statement, the problem can be\nformulated in the following manner:\n⎧\n⎪\n⎨\n⎪⎩\n𝑆𝛼≈𝑆𝛽\n𝐴𝛼≫𝐴𝛽\n|𝑆𝛼−𝑆𝛽| ≪|𝐴𝛼−𝐴𝛽|\n(1)\nwhere 𝑆and 𝐴stand for the server and adversary ac-\ncuracy, respectively, and 𝛼and 𝛽indicate before and after\nprotection placed at a given split position.\nHowever, several challenges, limitations, and trade-offs\nmust be addressed. Firstly, the protection must not involve\nretraining either the client or server-side model weights as\nthis would violate the previously mentioned equivalency\nwith the original non-split model, presumably leading to\nunpredictable accuracy or large retraining costs. This means\nthat any solution must be plug-in. Secondly, the plug-in\nprotection must either be placed before the client (front-end\nprotection) acting on the input image or after the client (plug-\nin protection) acting on the feature map before it is sent to\nthe server. Placing the protection entirely on the server is\nnot feasible as the server cannot be trusted to protect the\nfeature map it receives. Thirdly, due to client and server-\nside retraining restrictions a strong adversary will always be\nable to theoretically retrain on the protected feature maps,\nwith any state-of-the-art deep learning architecture, and,\nthus, have a distinct advantage over the server. Fourthly, a\nreasonable assessment of the Utility versus Privacy trade-\noff (server versus adversary accuracy after protection) must\noccur on a case-by-case basis and consider (1) primary\nand sensitive task selection, (2) client, server, adversary,\nand/or protection hyperparameters and architectures, and (3)\nadversary capabilities.\n4. Plug-in-based Protection Approaches\nWhen considering solutions to the aforementioned re-\nverse reconstruction and forward inference attacks, a generic\nprotection solution can involve a multitude of statistical,\ndeep learning, and other approaches such as PCA and AE\n(Osia et al., 2020, 2017). In the case of deep learning, the\nprotection model must be iteratively trained to convert an\ninput, be it an image or a feature map, into a protected\noutput of matching dimensions. However, the dimensions\ncan vary as the intermediate representations pass through\nthe various hidden layers in the middle of the protection\narchitecture. Additionally, specific data must be carefully\nselected to achieve high accuracy in the training phase to\nensure the protection model generates the correct type of\nprotection. In the inference phase, the protection model is\nplugged in at the specific split position it was trained on a\npriori. Afterward, inputs passed to the protection are actively\nprotected as they pass through the various layers on their\nway to the server model. It is presumed that the input to the\nprotection model is drawn from the same sample context as\nthe original data used to train the protection model.\n4.1. PCA-based Protection Baseline\nIn this baseline plug-in approach, a dimensionality re-\nduction algorithm is used to decompose the feature map onto\nprincipal components that capture the maximum variability\nof the data. In the real-time deployment of the plug-in ap-\nproach, feature maps of new images that are coming from the\nclient will be reconstructed by means of the extracted princi-\npal components in order to minimize the sensitive or private\nattributes of the feature maps. Here, the reduced Singular\nValue Decomposition (SVD) algorithm is implemented to\nperform dimensionality reduction on flattened feature maps\nfrom a ResNet50 model split at various bottleneck layers\n(Bonk). The dimensions of these feature maps are 512 by\n28 by 28 (flattened to 401,408).\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 5 of 15\n\n\nTowards Privacy-Preserving Split Learning\n4.2. Proposed Autoencoder-based Delta Protection\n(ADP)\nIn this section we discuss our proposed plug-in approach\nthat uses both AE and CAMs to protect feature maps with an\nnovel delta Δ protection.\n4.2.1. Autoencoder\nGenerally speaking a convolutional AE works similarly\nto the convolutional layers in both VGG16 and ResNet50 and\nmay contain the same types of supporting layers depending\non construction, i.e., Rectified Linear Activation Function\n(ReLU) layers. One main difference specific to AEs are the\nuse of convolutional transposition or de-convolutional layers\nthat aim to invert a prior convolution operation. However, the\ninverted result may not be perfectly equivalent, depending on\nimplementation. Importantly, the output dimensions should\nbe the same as the original input to the prior convolutional\nlayer. Generally, AEs use the input and convert it into either\na lower or higher dimensional space, and learn the chosen\noutput. Oftentimes the chosen output is the original input. In\nthis way AEs learn to reduce noise in a given input image or\ncomplex data to better standardize the inputs such that later\ninference is easier in the fully connected layers. However,\nit is possible to instead map the smaller reduced space to a\nmuch different output as in our case.\n4.2.2. Class Activation Maps\nCAMs allow for the visual inspection of what a deep\nlearning model uses to infer a given class-based prediction\nof a given input image. Here, heatmaps relating to the last\nor multiple convolutional layers of a deep learning model\nare used to discern what the most important parts of an\nimage are in the final prediction by projecting hot and cold\nareas over the input image. Generally, CAMs are used to\ndebug models and explain how deep-learning models make\npredictions after their weights are fully or partially trained.\nIn our work, we adapt code from Gildenblat and contributors\n(2021) for CAM generation.\n4.2.3. The Proposed Delta Approach\nIn our novel delta Δ approach, shown at a high level in\nFigure 3, the use of CAMs is combined with AEs so that\na protection, 𝑃, mapping between original and protected\nfeature maps can be learned, and, then, applied in real time.\nHere, two delta strategies are considered, the first where the\nsensitive adversary tasks are known or can be inferred, and\nthe second where they cannot. In the former strategy, CAMs\nof both the offline adversary, 𝐴𝑜, and the server are first\nused to construct the protected image through a process we\ngenerally refer to as Δ𝑚𝑖𝑛, shown in Algorithm 1.\nThis process involves protecting the intersection of the\ncold-server and hot-adversary CAM spaces according to a\npredefined temperature threshold. Afterward, the protected\nspace is projected back over the original input image to\ncreate the final protected image. Both images are then passed\nthought the client where the original feature map is passed as\ninput to the encoder 𝑒and the protected feature map is passed\nto the decoder 𝑑such that the protection can be learned. This\nAlgorithm 1: Delta Δ𝑚𝑖𝑛Strategy\nData: 𝑛≥1, Input image 𝐼, Client-model 𝐶,\nServer-model 𝑆, Offline-adversary 𝐴𝑜, Model\nconcatenation operator ⊕, Class Activation Map\nfunction CAM, CAM negation function ¬,\nProtection intersection function Δ, Temperature\nthreshold 𝑡, Intensity threshold 𝑖, Protection method\n𝛿(either “black-out\" or “blur-out”), Original and\nprotected client-side feature map 𝐹𝑜and 𝐹𝑝\nResult: 𝐹𝑜, 𝐹𝑝\n1 𝐹𝑜←𝐶(𝐼);\n2 repeat 𝑛times\n3\n𝑀𝐴𝑜←CAM(𝐼, 𝐶⊕𝐴𝑜);\n4\n𝑀𝑆←¬ CAM(𝐼, 𝐶⊕𝑆);\n5\n𝐼←Δ𝑚𝑖𝑛(𝐼, 𝑀𝑆, 𝑀𝐴𝑜, 𝑡, 𝑖, 𝛿);\n6 end\n7 𝐹𝑝←𝐶(𝐼);\nstrategy can be reapplied in a Δ𝑚𝑖𝑛\n𝑛\nscheme in which 𝑛stands\nfor number of iterations, where the protected image of the\nprevious delta simply becomes the input to the next. Here,\neach successive delta is thought to apply more protection\nsince previously targeted hot-adversary regions in the CAM\nare protected, which generally forces the adversary CAM\nto change location in contrast to the server regions that do\nnot. This is of course by design as the hot-adversary regions\nare specifically targeted for protection while the hot-server\nregions are purposely left unprotected. However, it is entirely\npossible that the hot-adversary and hot-server regions may\nintersect. If this happens, we simply prioritize utility over\nprivacy and opt not to protect the intersecting region as doing\nso will likely cause a significant server-side accuracy drop.\nTherefore, the feasibility of providing protection with this,\nor any, delta strategy must consider the degree to which\nprimary and sensitive attribute hot CAM regions intersect. In\nthe second strategy, where the adversary tasks are unknown,\nprotection is only applied to the cold-server CAM regions\nalso predefined by a given temperature threshold, shown in\nAlgorithm 2. We define this strategy as Δ𝑚𝑎𝑥since it protects\nthe maximal amount of information not directly used by the\nserver according to the server CAM. Here, a Δ𝑚𝑎𝑥\n𝑛\nscheme\ncould theoretically be applied but is not explored in our\nwork.\nFurthermore, the specific protection method used to\ndefine and protect regions in a given protection strategy falls\ninto two categories. The first, black-out, consists of setting\nall values in the protected space to the same value, in our case\nblack. Here, a threshold intensity value is used to define the\nprotected space by creating a boolean mapping of hot or cold\nregions. This is accomplished by measuring the blue value in\neach input CAM pixel to determine if it is less than or greater\nthan the chosen threshold value. The second method, blur-\nout, uses the same threshold intensity to define the protected\nspace but also applies blur protection according to a given\nblur intensity. Either protection method may be applied to\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 6 of 15\n\n\nTowards Privacy-Preserving Split Learning\nAlgorithm 2: Delta Δ𝑚𝑎𝑥Strategy\nData: 𝑛≥1, Input image 𝐼, Client-model 𝐶,\nServer-model 𝑆, Model concatenation operator ⊕,\nClass Activation Map function CAM, CAM\nnegation function ¬, Protection intersection\nfunction Δ, Temperature threshold 𝑡, Intensity\nthreshold 𝑖, Protection method 𝛿(either “black-out\"\nor “blur-out”), Original and protected client-side\nfeature map 𝐹𝑜and 𝐹𝑝\nResult: 𝐹𝑜, 𝐹𝑝\n1 𝐹𝑜←𝐶(𝐼);\n2 repeat 𝑛times\n3\n𝑀𝑆←¬ CAM(𝐼, 𝐶⊕𝑆);\n4\n𝐼←Δ𝑚𝑎𝑥(𝐼, 𝑀𝑆, 𝑡, 𝑖, 𝛿);\n5 end\n6 𝐹𝑝←𝐶(𝐼);\neither of the two protection strategies previously mentioned\nwith a high degree of flexibility depending on protection\nrequirements.\nAfter selecting a given combination of delta method,\nstrategy, and applicable parameters a dataset mapping orig-\ninal to protected images can be created. However, since AE\nwill not be accepting input images directly unless deployed\nas a front-end protection, both the original image and pro-\ntected image need to be converted to feature maps. This\nis accomplished by passing both the original and protected\nimages through the client separately at a given split position\nin order to obtain the respective feature maps. AE can then\ntrain on the feature maps to theoretically learn the protection\nmapping, ultimately derived from the CAMs of the client,\nserver, and offline adversary (in the Δ𝑚𝑖𝑛case), respectively.\nIn AE training process, several types of architectures can\nbe constructed to either increase or decrease the Z dimen-\nsion of the feature maps. Additionally, the convolution and\nde-convolutional layers in AE may additionally shrink or\nexpand the X and Y feature map size. Since feature map\ndimensions may change depending on layer and architecture\nit may be necessary to create custom AEs that vary the\nstride, padding and dilation when required if they need to\nbe especially deep. Other parameters such as learning rate,\nloss function, optimizer, number of epochs, etc. can also be\nadjusted accordingly. All that remains after this is to deploy\nAE at the given split where it will take input feature maps and\napply the protection mapping it learned during the training\nphase in the testing phase.\n4.3. Split Position and Feature Map\nConsiderations\nWhile different deep learning architectures operate with\nvast differences in terms of the type, number, and con-\nfiguration of layers several high-level observations of the\narchitectures are worth noting. Firstly, in the case of VGG16\nand ResNet50 both architectures take a color image as input.\nHere, the image is a three-dimensional tensor representing\nthe red, green, and blue dimension combination values used\nto display color at every pixel in an image. Importantly,\nthis input image is merely a special case feature map, one\nthat has yet to pass through any given layer. As such, it\nstill has the exact same properties as later feature maps.\nNamely, a Z or depth dimension, and an X and Y or height\nand width dimension for every Z dimension. Secondly, both\narchitectures progressively perform a series of operations\nthat increase the depth of the feature map while generally\nreducing the height and width of each depth dimension. The\nmain aim of this general approach is to recursively reduce the\noverall amount of information while retaining only the most\nimportant information in the feature extraction phase. This\nis needed since in the latter classification phase the fully-\nconnected layers need a reduced feature space. Importantly,\nfeature map slices or X and Y cross sections are more\nrepresentative of the original input image at shallower layers,\nand hence split positions, since they will have greater height\nand width dimensions and increased overall information.\nThese observations are very important from several im-\nplementation and theoretical perspectives. Firstly, from an\nimplementation perspective, any dataset that could be used\nto train or fit a given adversarial or protection model com-\nprised of feature maps rather than images will grow by a\nconstant factor of the depth of the feature maps generated\nat a given split position relative to the same dataset of input\nimages. This is especially important since attempting to hold\neven a moderate dataset of feature maps in memory or on\ndisk in many cases is not feasible. Typically, during the\ntraining session, feature maps must be computed from their\nrespective input images by passing them through the client\nevery time they are used. Secondly, from a theoretical per-\nspective, it is much easier to protect feature maps at deeper\nsplit positions for a multitude of reasons. Specifically, feature\nmaps at later positions, while greater in number, individually\ncontain less information making both reconstruction and\ninference attacks increasingly difficult or in extreme cases\ninfeasible beyond acceptable thresholds. While it is tempting\nto use this observation naïvely as a direct form of protection\nit unfortunately contradicts the utility provided by the nature\nof the split learning to begin with as it places most of the\nlayers on the resource-constrained client. However, the split\nposition still plays a very important role in the efficacy\nof implemented protections and can significantly increase\nor reduce the general challenges associated with providing\nadequate protection.\n5. Experimental Results\nHere, the experimental setting is initially explained, and,\nthen, the achieved results through each privacy protection\nstrategy are presented.\n5.1. ADP Experimental Setup\nInitially, the process of data generation, training, and\ntesting are explained, and, then, network architecture and its\nparameter settings are reported.\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 7 of 15\n\n\nTowards Privacy-Preserving Split Learning\nFigure 3: The general diagram of the proposed novel delta approach, called ADP.\n5.1.1. Dataset Generation\nDataset generation in our work follows a scheme, where\n𝑛and 𝑚binary samples are evenly selected from the Celeb-\nFaces Attributes Dataset (CelebA). Samples are selected\naccording to a given primary or sensitive attribute without\nreplacement to ensure that there are no overlapping train and\ntest samples in the respective datasets. Primary and sensitive\nattribute overlap is not considered, consistent with the real-\nworld adversary case. The delta dataset follows a similar\nscheme but is generated separately and aggregates n evenly\nselected binary samples for one or more attributes. Both\nsensitive or primary attribute samples can be used.\n5.1.2. Protection Training and Testing\nWhen testing the delta approach with any combination\nof strategy, method, or other parameters, it is important\nto consider several factors that impact protection training\nand testing. Firstly, it is important to recognize that the\ndelta dataset generation process is strongly dependent on the\nserver 𝑆, adversary 𝐴, and especially the client model 𝐶.\nThis is the case since the client model is used for converting\noriginal and protected samples to feature maps while all\nmodels are used via their CAMs (that are strongly asso-\nciated with the weights of the various models) to create\nthe protection training samples directly. In the ideal case, a\nseparate set of client, server, and adversary models should\nbe used to train and test AE protection. This approach\nwould theoretically indicate if AE protection is generalizable\nenough to plug into any given client-server model or if it\nonly affords protection to the specific client-server model it\nwas trained on. Similar generalizability would also extend to\nadversaries that presumably have the same task and indicate\nif the protection only applies to the specific adversary the\nprotection was trained on. However, we consider that the\nprotection ought only to apply to the client-server model that\nthe protection was trained on since it has an undue influence\nover the CAMs used to generate the protection. Conse-\nquently, we do consider both training and testing adversaries,\nreferred to hereafter as offline and inference adversaries\n𝐴𝑜and 𝐴𝑖, respectively, to evaluate the protection in order\nto strike an acceptable level of adversarial generalizability.\nImportantly, the offline adversary model need not be the\nsame as the inference adversary model. Either model may\nutilize the existing server-side architecture or an entirely\ndifferent, and presumably deeper, architecture. We refer to\nthese two adversary cases generally as split and full, respec-\ntively. Additionally, in the full case, the arbitrary architecture\nmust be able to accept a feature map of Z dimension after a\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 8 of 15\n\n\nTowards Privacy-Preserving Split Learning\nTable 1\nParameter setting for VGG16 and ResNet50 experiments\nthrough autoencoder.\nExperimental Parameters\nVGG16\nResNet50\nPrimary Attribute\nWearing_Lipstick\nWearing_Lipstick\nPrimary Train Dataset\n2000 balanced samples\n1000 balanced samples\nPrimary Test Dataset\n400 balanced samples\n200 balanced samples\nSensitive Attribute\nWearing_Hat\nWearing_Hat\nSensitive Train Dataset\n2000 balanced samples\n1000 balanced samples\nSensitive Test Dataset\n400 balanced samples\n200 balanced samples\nDelta Dataset\n4000\n2000\nDelta Threshold Intensity\n99%\n99%\nDelta Blur Intensity\n40\n40\nΔ𝑚𝑖𝑛\n𝑛\n2\n2\nArchitecture\nVGG16\nResNet50\nSplit Positions\nConvolutional layers {4,8,12}\nBottleneck layers {4,8,12}\nReconstruction Iterations\n20,000\n—\ngiven client split and have a large enough remaining feature\nmap of X and Y dimensions to pass through the remaining\narchitecture without being reduced to zero by the remaining\nconvolutional layers. Nonetheless, both offline and inference\nadversaries need to be trained on the same client-server and\nhave similar, if not identical, sensitive tasks. The client-\nserver will always fall under the split case by definition.\n5.1.3. Experiment Architecture\nIn our work, we employ a pipeline architecture to carry\nout various types of experiments. This is the case since many\naspects of testing are strongly dependent on each other. The\nclient-server training must proceed with adversary training,\ndelta dataset creation must proceed with protection training,\nand so on. However, there are certain benefits to this archi-\ntecture as certain sections of the pipeline can be cached and\nreused in consecutive experiments while others sections can\nbe run in parallel.\nHere, we provide the setup parameters, shown in Table 1,\nas they relate to the experiments conducted on the VGG16\nand ResNet50 architectures using the ADP approach. The\nmain difference between the VGG16 and ResNet50 param-\neters is that the VGG16 parameters use bigger datasets\nto achieve higher accuracy as the VGG16 architecture is\nsmaller compared to the ResNet50 model.\n5.1.4. Autoencoder Architecture\nIn our work, we utilize several AEs that decrease the\nsize of the feature map depending on if a decreasing, de-\ncreasing_deep, or decreasing extra_deep AE is chosen. In\nthe case of the decreasing AE, only one layer is used to\nreduce or encode the Z dimension of the feature map by a\nfactor of two before it is decoded to the prior dimensions.\nWith the increasing depth of the AE, the reversibility be-\ncomes an issue depending on the input dimensions of the\nfeature map. As such not all AEs work on all layers of all\narchitectures. This is the case for Table 6 that only includes\nbottleneck layer 4. However, it is possible to create specific\nAEs for specific layers at the cost of comparability when\nevaluating results, though we do not do so here. In the case\nof the decreasing_deep and decreasing_extra_deep AE, they\nreduce the Z dimension of the feature maps by a factor of four\nand eight, respectively. In our work, we apply the decreasing\nTable 2\nParameter setting for the ResNet50 experiment through PCA.\nExperimental Parameters\nPCA\nPrimary Attribute\nWearing_Lipstick\nSensitive Attribute\nWearing_Hat\nTrain Dataset\n1000 balanced samples\nTest Dataset\n200 balanced samples\nAdversary Test Dataset\n200 balanced samples\nAdversary Architecture\nResNet50 split\nNumber of Components\n1000\nAEs to all architectures and select layers as we only apply\nthe decreasing_deep and decreasing_extra_deep AE to the\nshallowest layer, where they are able to revert the feature\nmap back to the correct output dimensions for the tested split\npositions.\n5.2. PCA Experimental Setup\nIn this experiment, the PCA-based plug-in strategy is\nevaluated with the experimental setup, as reported in Table\n2, to match the AE experimental setup.\nAll components used for reconstruction are selected\nfrom the start (i.e., from index 0).\n5.3. Evaluation Criteria\nIn this work, the effectiveness of various attacks and\nproposed protections is evaluated using several criteria. In\nthe forward inference attack, the server and the adversary\naccuracy before and after protection are used to determine\nif Equation 1 is satisfied. For the backwards reconstruction\nattack the Multi-scale Structured Similarity Index (MS-\nSSIM) (Wang, Simoncelli and Bovik, 2003) is used, which\nis an extension of the Structural Similarity Index (SSIM)\n(Wang, Bovik, Sheikh and Simoncelli, 2004), that given\ntwo images, in our case original 𝜙and reconstructed 𝜃, it\nprovides a good automatic approximation of how well a\nhuman could distinguish the difference in quality between\nthe two as follows:\nMS-SSIM(𝜙,𝜃) = [𝑙𝑀(𝜙, 𝜃)]𝜁𝑀\n𝑀\n∏\n𝑗=1\n[𝑐𝑗(𝜙, 𝜃)]𝜂𝑗[𝑠𝑗(𝜙, 𝜃)]𝛾𝑗\n(2)\n𝑙(𝜙, 𝜃) =\n2𝜇𝜙𝜇𝜃+ Γ1\n𝜇2\n𝜙+ 𝜇2\n𝜃+ Γ1\n,\n(3)\n𝑐(𝜙, 𝜃) =\n2𝜎𝜙𝜎𝜃+ Γ2\n𝜎2\n𝜙+ 𝜎2\n𝜃+ Γ2\n,\n(4)\n𝑠(𝜙, 𝜃) =\n𝜎𝜙𝜃+ Γ3\n𝜎𝜙𝜎𝜃+ Γ3\n,\n(5)\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 9 of 15\n\n\nTowards Privacy-Preserving Split Learning\nΓ1 = (Θ1Ξ)2, Γ2 = (Θ2Ξ)2, Γ3 = Γ2∕2\n(6)\nMS-SSIM computes this similarity by considering lumi-\nnance 𝑙, contrast 𝑐, and structure 𝑠at various scales 𝑀and\ncombing them using a normalized equation. Additionally,\n𝜇𝜙, 𝜇𝜃, 𝜎𝜙, and 𝜎𝜃, are the mean and variance of 𝜙and 𝜃,\nrespectively, (indicating luminance and contrast) in addition\nto 𝜎𝑜𝑟covariance (indicating structure). Γ1, Γ2, and Γ3 are\nused as small constants, where Ξ is the dynamic range of\npixel values and Θ1 and Θ2 are small scaler values less\nthan one. The exponents 𝜁, 𝜂, and 𝛾, are used to adjust\nthe relative importance of luminance, contrast, and structure\ncomponents respectively.\n5.4. ADP Results\nIn the forward inference attack, experiment values are\nlisted in Tables 4-6 show the attained accuracies for the\nserver before and after protection, 𝑆𝛼and 𝑆𝛽, as well as\noffline, 𝐴𝑜\n𝛼and 𝐴𝑜\n𝛽, and inference 𝐴𝑖\n𝛼and 𝐴𝑖\n𝛽adversaries\nbefore and after protection. Various architectures, split po-\nsitions, and protections are also taken into account. Addi-\ntionally, each unique combination of protection architec-\nture (decrease and decrease_extra_deep) and delta method\n(Blur and Black) are reported as an independent experiment.\nHowever, the corresponding Δ𝑚𝑖𝑛and Δ𝑚𝑎𝑥strategies are\ndependently tested on the same client-server architecture and\ndatasets. Some level of accuracy fluctuation across indepen-\ndent experiments is expected through random chance alone\nbut are comparable as they are drawn from the same dataset\ndistribution according to the same random process. Also, a\nspecial case exists where 𝐴𝑜\n𝛼and 𝐴𝑜\n𝛽values are reported in\nthe Δ𝑚𝑎𝑥case even though they have no influence on AE\ntraining. Here, we still list these results as they can give\ninsight into testing a split or full adversary architecture or\nsimply validate the results.\nIn the backward reconstruction attack we list MS-SSIM\nexperiment values in Table 3 before and after protection.\nAdditionally, various split positions and protections as those\nin the forward inference attack case are listed. We only\nconsider the VGG16 architecture in the backward recon-\nstruction attack case since it is the only architecture we\ncan use to attack with successful results without protection.\nLastly, a single preselected image, shown in Figure 4, is\nused to measure the reconstruction threshold before and after\nprotection in all reported experiments. This is the case as the\nbackward reconstruction attack only considers one image at\na time and is computationally intensive.\n5.5. Analyzing Results Obtained by ADP\nBased on the results of ADP we observe the following\ngeneral trends and identify specific promising cases. Our\nresults are reported in Tables 4-6, interms of accuracies\nfor the server and adversary before and after protection for\neach protection strategy as well as the split position the\nprotection was applied to. MS-SSIM values for the backward\nreconstruction attack are also reported in Table 3 for VGG16\nTable 3\nVGG16 backward reconstruction attack results for primary\ntask: Wearing_Lipstick and sensitive task: Wearing_Hat with\n400 test samples, 2000 train samples, and 2000 delta samples.\n𝐶, 𝑆, and 𝐴𝑜,𝑖are trained for 250 epochs. All models utilize\nthe corresponding split architecture. Autoencoder protection\nis trained for 120 epochs with a decreasing architecture.\nΔ\nΔ\n𝑆𝑝𝑙𝑖𝑡\nMS-SSIM\nMS-SSIM\n𝑆𝑡𝑟𝑎𝑡𝑒𝑔𝑦\n𝑀𝑒𝑡ℎ𝑜𝑑\nWithout Protection\nWith Protection\nConv04\n0.80\n0.18\nΔ𝑚𝑖𝑛\n2\nBlur-out\nConv08\n0.63\n0.23\nConv12\n0.57\n0.23\nConv04\n0.75\n0.18\nΔ𝑚𝑖𝑛\n2\nBlack-out\nConv08\n0.63\n0.20\nConv12\n0.57\n0.20\nConv04\n0.80\n0.18\nΔ𝑚𝑎𝑥\nBlur-out\nConv08\n0.63\n0.18\nConv12\n0.57\n0.18\nConv04\n0.75\n0.17\nΔ𝑚𝑎𝑥\nBlack-out\nConv08\n0.63\n0.17\nConv12\n0.57\n0.16\nTable 4\nVGG16 forward inference attack results using the same\nexperimental parameters as in 3, except using a decreas-\ning_extra_deep architecture.\nΔ\nΔ\n𝑆𝑝𝑙𝑖𝑡\n𝑆𝛼\n𝑆𝛽\n𝐴𝑜\n𝛼\n𝐴𝑜\n𝛽\n𝐴𝑖\n𝛼\n𝐴𝑖\n𝛽\n𝑆𝑡𝑟𝑎𝑡𝑒𝑔𝑦\n𝑀𝑒𝑡ℎ𝑜𝑑\nConv04\n87%\n86%\n93%\n93%\n90%\n89%\nΔ𝑚𝑖𝑛\n2\nBlur-out\nConv08\n87%\n86%\n92%\n91%\n88%\n87%\nConv12\n87%\n87%\n90%\n90%\n87%\n87%\nConv04\n89%\n89%\n93%\n88%\n94%\n83%\nΔ𝑚𝑖𝑛\n2\nBlack-out\nConv08\n89%\n80%\n91%\n84%\n91%\n88%\nConv12\n89%\n66%\n93%\n71%\n89%\n73%\nConv04\n87%\n79%\n93%\n77%\n90%\n76%\nΔ𝑚𝑎𝑥\nBlur-out\nConv08\n87%\n72%\n92%\n85%\n88%\n80%\nConv12\n87%\n70%\n90%\n84%\n87%\n80%\nConv04\n89%\n50%\n93%\n73%\n94%\n50%\nΔ𝑚𝑎𝑥\nBlack-out\nConv08\n89%\n50%\n91%\n51%\n91%\n50%\nConv12\n89%\n49%\n93%\n50%\n89%\n51%\nonly. Together these tables provide a view into how the\nserver and adversary are impacted by various ADP strategies\nas well as their effectiveness at various split positions.\nFirstly, in the case of the forward inference attack, we\nobserve that the Δ𝑚𝑎𝑥strategy, shown in rows 5-8 of Table\n5, rows 5-8 of Table 6, and rows 3-4 of Table 4, often drops\nboth the server and inference adversary accuracy to a near-\nrandom guess. The only exception to this is on row 3 of Table\n4. However, such a large drop is somewhat expected due to\nthe aggressiveness required by the Δ𝑚𝑎𝑥strategy in order to\nprovide protection in the extreme cases that the adversary\ntask is unknown or cannot be inferred. However, even though\nthis strategy is not currently viable it does lend support to\nthe hypothesis that the blur-out protection method generally\nallows for higher accuracy results on both the server and the\nadversary over the black-out method. Evidence of this can be\nobserved in rows 3-4 of Table 4, where the general accuracy\ndrop for the server and the adversary is very pronounced\nin the split VGG16 case compared to rows 5-6 of Table\n5 in the split ResNet50 case. Additionally, rows 7-8 in\nTable 5 and rows 7-8 in Table 6 exhibit similar trends in\ndifferent ResNet50 cases. This finding is important because\nit suggests that a relationship exists between combinations\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 10 of 15\n\n\nTowards Privacy-Preserving Split Learning\n(a)\n(b)\n(c)\nFigure 4: Example of backward reconstruction attack on VGG16 convolutional layer 4, with original image shown in (a),\nreconstructed image in (b), and protected image in (c). The protection is provided using the ADP strategy: Δ𝑚𝑖𝑛\n2\nand delta\nmethod: blur-out along with an offline adversary architecture: VGG16 split and inference adversary architecture: VGG16 split.\nTable 5\nResNet50 forward inference attack results for primary task:\nWearing_Lipstick and sensitive task: Wearing_Hat with 200\ntest samples, 1000 train samples, and 1000 delta samples.\n𝐶, 𝑆, and 𝐴𝑜,𝑖are trained for 100 epochs. All models utilize\nthe corresponding split architecture unless indicated otherwise\nas full in bold. Autoencoder is trained for 60 epochs with a\ndecreasing architecture. Values in blue and red correspond to\nserver and inference adversary values reported in Figure 5(a-c),\nrespectively, at epoch 60.\nΔ\nΔ\n𝑆𝑝𝑙𝑖𝑡\n𝑆𝛼\n𝑆𝛽\n𝐴𝑜\n𝛼\n𝐴𝑜\n𝛽\n𝐴𝑖\n𝛼\n𝐴𝑖\n𝛽\n𝑆𝑡𝑟𝑎𝑡𝑒𝑔𝑦\n𝑀𝑒𝑡ℎ𝑜𝑑\nBonk04\n88%\n82%\n88%\n69%\n90%\n88%\nΔ𝑚𝑖𝑛\n2\nBlur-out\nBonk08\n88%\n87%\n88%\n86%\n90%\n89%\nBonk12\n88%\n87%\n88%\n86%\n90%\n89%\nBonk04\n84%\n82%\n88%\n81%\n87%\n79%\nΔ𝑚𝑖𝑛\n2\nBlack-out\nBonk08\n84%\n81%\n88%\n84%\n88%\n73%\nBonk12\n84%\n87%\n88%\n79%\n88%\n77%\nBonk04\n77%\n78%\n62%\n62%\n75%\n72%\nΔ𝑚𝑖𝑛\n2\nBlur-out\nBonk08\n77%\n74%\n69%\n63%\n78%\n72%\nBonk12\n77%\n77%\n71%\n70%\n77%\n60%\nBonk04\n84%\n75%\n65%\n46%\n71%\n53%\nΔ𝑚𝑖𝑛\n2\nBlack-out\nBonk08\n84%\n85%\n68%\n62%\n73%\n50%\nBonk12\n85%\n77%\n63%\n57%\n72%\n65%\nBonk04\n88%\n52%\n88%\n60%\n90%\n75%\nΔ𝑚𝑎𝑥\nBlur-out\nBonk08\n88%\n49%\n88%\n62%\n90%\n68%\nBonk12\n88%\n58%\n88%\n60%\n90%\n70%\nBonk04\n84%\n48%\n88%\n56%\n87%\n51%\nΔ𝑚𝑎𝑥\nBlack-out\nBonk08\n84%\n51%\n88%\n68%\n88%\n62%\nBonk12\n84%\n50%\n88%\n50%\n87%\n56%\nBonk04\n77%\n58%\n62%\n57%\n75%\n57%\nΔ𝑚𝑎𝑥\nBlur-out\nBonk08\n77%\n59%\n69%\n63%\n78%\n65%\nBonk12\n77%\n78%\n71%\n59%\n77%\n59%\nBonk04\n84%\n49%\n65%\n50%\n71%\n50%\nΔ𝑚𝑎𝑥\nBlack-out\nBonk08\n84%\n49%\n68%\n46%\n73%\n50%\nBonk12\n85%\n49%\n63%\n48%\n72%\n50%\nof blurring strength protection and the architectures of the\nclient-server and adversary to produce varying accuracy\ndifferentials on the server and adversary. Perhaps even more\nimportant, is the implication that even non-important infor-\nmation, indicated by the server CAM cold regions, is still\nlikely needed at some level in order to provide high-level\ncontext for the server and the adversary in the final inference\nstage.\nTable 6\nResnet50 forward inference attack results using the same\nexperimental parameters as in 5, except using a decreas-\ning_extra_deep architecture.\nΔ\nΔ\n𝑆𝑝𝑙𝑖𝑡\n𝑆𝛼\n𝑆𝛽\n𝐴𝑜\n𝛼\n𝐴𝑜\n𝛽\n𝐴𝑖\n𝛼\n𝐴𝑖\n𝛽\n𝑆𝑡𝑟𝑎𝑡𝑒𝑔𝑦\n𝑀𝑒𝑡ℎ𝑜𝑑\nΔ𝑚𝑖𝑛\n2\nBlur-out\nBonk04\n81%\n64%\n88%\n61%\n90%\n52%\nΔ𝑚𝑖𝑛\n2\nBlack-out\nBonk04\n79%\n57%\n91%\n49%\n89%\n67%\nΔ𝑚𝑖𝑛\n2\nBlur-out\nBonk04\n84%\n53%\n61%\n57%\n76%\n57%\nΔ𝑚𝑖𝑛\n2\nBlack-out\nBonk04\n78%\n50%\n61%\n60%\n78%\n50%\nΔ𝑚𝑎𝑥\nBlur-out\nBonk04\n81%\n55%\n88%\n56%\n90%\n64%\nΔ𝑚𝑎𝑥\nBlack-out\nBonk04\n79%\n49%\n91%\n76%\n89%\n50%\nΔ𝑚𝑎𝑥\nBlur-out\nBonk04\n84%\n57%\n61%\n55%\n76%\n56%\nΔ𝑚𝑎𝑥\nBlack-out\nBonk04\n78%\n49%\n61%\n50%\n78%\n50%\nSecondly, in the case of the Δ𝑚𝑖𝑛strategy we generally\nobserve a reduced accuracy drop, shown in rows 1-4 of Table\n5, rows 1-4 in Table 6 and rows 1-2 of Table 4, compared to\nthe Δ𝑚𝑎𝑥strategy. This matches our expectations since the\nadversary task is known in the Δ𝑚𝑖𝑛case thus allowing for\nit to be specifically targeted. Additionally, the Δ𝑚𝑖𝑛strategy\nalso keeps as much unprotected information needed by the\nserver as possible. When comparing the server and infer-\nence adversary generally, we found that the accuracy drop\nobserved on the adversary side was also reflected similarly\non the server side. Nonetheless, a few promising exceptions\nexist. Notably in the case of a full-adversary architecture\nused to train an AE with the black-out method and a split-\nadversary architecture used to test the protection. In these\ncases, shown in Figure 5(a-c) and in row 3 of Table 5,\nwe found the inference adversary accuracy drops to a near\nrandom guess with only a small accuracy drop on the server.\nThirdly, concerning of the backward reconstruction at-\ntack, we define a successful reconstruction attack as one\nwhere the attacker achieves an MS-SSIM value exceed-\ning 0.35. Our initial investigation indicated that our best\neffective attack was only successful against the VGG16\narchitecture, not the ResNet50 architecture, using the white-\nbox attack described in He et al. (2019). This is because the\nVGG16 architecture is much shallower and simpler lending\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 11 of 15\n\n\nTowards Privacy-Preserving Split Learning\n10\n20\n30\n40\n50\n60\nNumber  f Ep chs\n40\n50\n60\n70\n80\n90\n100\nAccuracy\nADP at b ttleneck layer 4\nSβ 12% mean baseline dr p\nAβ 16% mean baseline dr p\nSα\nAα\n(a)\n10\n0\n10\n1\n10\n2\n10\n3\nNumber of componen s\n40\n50\n60\n70\n80\n90\n100\nAccuracy\nPCA a  bo  leneck layer 4\nSβ, PCA\nAβ, PCA\nSα\nAα\n(d)\n10\n20\n30\n40\n50\n60\nNumber  f Ep chs\n40\n50\n60\n70\n80\n90\n100\nAccuracy\nADP at b ttleneck layer 8\nSβ 6% mean baseline dr p\nAβ 22% mean baseline dr p\nSα\nAα\n(b)\n10\n0\n10\n1\n10\n2\n10\n3\nNumber of componen s\n40\n50\n60\n70\n80\n90\n100\nAccuracy\nPCA a  bo  leneck layer 8\nSβ, PCA\nAβ, PCA\nSα\nAα\n(e)\n10\n20\n30\n40\n50\n60\nNumber  f Ep chs\n40\n50\n60\n70\n80\n90\n100\nAccuracy\nADP at b ttleneck layer 12\nSβ 4% mean baseline dr p\nAβ 11% mean baseline dr p\nSα\nAα\n(c)\n10\n0\n10\n1\n10\n2\n10\n3\nNumber of componen s\n40\n50\n60\n70\n80\n90\n100\nAccuracy\nPCA a  bo  leneck layer 12\nSβ, PCA\nAβ, PCA\nSα\nAα\n(f)\nFigure 5: Panel a (b) (c) shows obtained accuracies by 𝑆and 𝐴𝑖at bottleneck layer 4 (8) (12) with decreasing AE, the ADP\nstrategy: Δ𝑚𝑖𝑛\n2\nand delta method: black-out, offline adversary architecture: ResNet50 full and inference adversary architecture:\nResNet50 split (measured accuracy per epoch). Panel d (e) (f) shows obtained accuracies by 𝑆and 𝐴𝑖at bottleneck layer 4 (8)\n(12) with PCA protection. Inference adversary architecture: ResNet50 split (measured accuracy per component).\nitself to backward reconstruction attacks more aptly as op-\nposed to the ResNet50 architecture that does not. As such,\nwe only consider the VGG16 architecture, shown in Table 3\nand in Figure 4. We observed that in every case we were able\nto provide protection against the backward reconstruction\nattack far below the reconstruction threshold. Additionally,\nwe observe that the Δ𝑚𝑎𝑥strategy affords slightly better\nreconstruction protection generally than the Δ𝑚𝑖𝑛strategy\nand so do the black-out and blur-out methods, respectively.\nLastly, by virtue of using a decreasing AE the size of the\nfeature map is reduced in every scenario, where it is applied.\n5.6. PCA Results\nAttained accuracies by adversary and server using PCA\nat various bottleneck layers are shown in Figure 5(d-f).\nThought 𝐴and 𝑆show the profiles accuracy determined by\nincreasing the number of principal components used in the\nreconstruction. As the data spans several orders of magni-\ntude, a logarithmic scale was used for plotting the results\nobtained by PCA to better visualize the data by spreading\nout the data samples more evenly.\n5.7. Analyzing the Results Obtained by PCA\nThe results obtained by the PCA protection indicate that\nthe deeper PCA is integrated into the architecture, the fewer\ncomponents are needed for reconstruction to achieve good\nperformance. This is characterized by a small drop in the\nserver performance and a significant drop in the adversary\nperformance. Notably, at bottleneck layer 12, using as few\nas 5 components the server can achieve a performance that\nmatches the server baseline, and the adversary’s perfor-\nmance drops to the random guess.\n5.8. Comparing ADP and PCA Outcomes\nHere, we compare the ADP strategy with the PCA pro-\ntection on the same primary and sensitive tasks, Wear-\ning_Lipstick and Wearing_Hat, respectively, for bottleneck\nlayers 4, 8, and 12 as shown in Figures 5 and 6. We also\nutilize the server and inference adversary weights used in the\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 12 of 15\n\n\nTowards Privacy-Preserving Split Learning\nADP experiments to ensure results are comparable. Figures\n5(a-c) show accuracies attained through ADP as the number\nof training epochs are increased at bottleneck splits 4, 8,\nand 12, similarly for Panels (d-f) with respect to PCA with\nthe number of components are increased instead of training\nepochs.\nBefore interpreting the results it is important to reiter-\nate important considerations related to an increase in split\nposition. Firstly, as the split is increased, the client gains\nmore convolutional layers, whereas the server and adversary\n(in the split adversary case) lose convolutional layers, and,\nhence, influence over the remaining “post-protection” or\n“post-split” decision. However, this is not entirely true for\nthe server as the split position increases, server layers simply\nshift back onto the client before the protection, presumably\nmaking it more consistent with its original baseline model,\nfree from the effects of unwanted perturbation resulting from\nthe applied protection. This is particularly evident in the\ncase of the split adversary architecture we analyze here.\nConsequently, this would suggest a higher initial accuracy\nfor the server as the split position is increased, while the\nadversary should remain consistently lower. Secondly, re-\ngardless of how many convolutional layers remain on the\nserver or adversary, the feature map and information within\nit will also change and become more compressed (reduced\nheight and width) and varied (greater depth) as the split po-\nsition is increased, which may impact performance. Lastly,\nit is important to recognize that the server is disadvantaged\ncompared to any adversary in this case since it is not allowed\nto retrain on protected feature maps in our setting due to the\nplugin nature of the protection, while the adversary model\ncan.\nAt bottleneck layer 4 for ADP, Figure 5(a), we observe\nthat both the server and adversary accuracies remain rela-\ntively stable, with a slight increase in server accuracy and a\ndrop in adversary accuracy to a level near random guessing\nas the number of training epochs increases. This observation\ngenerally aligns with Equation 1, Additionally, in the case of\nPCA, Figure 5(d), it can be observed that as the number of\ncomponents increases, both server and adversary accuracy\nincrease until they reach a certain point where they begin\nto diverge. The server accuracy increases to near baseline\naccuracy, while the adversary accuracy decreases, albeit\nnot reaching the level of a near random guess, thus not\nsatisfying Equation 1. Importantly, at this split position the\nadversary and the server have the greatest influence on the\nfinal decision compared to the client, as they retain most of\nthe convolutional layers.\nAt bottleneck layer 8 for ADP, Figure 5(b), we observe\nperfect alignment with Equation 1 such that as the number\nof training epochs increases the server accuracy gradually\nincreases to a point slightly above its previous baseline accu-\nracy, whereas the adversary accuracy remains consistently at\na near random guess with little to no deviation. Importantly,\nPCA results in Figure 5(e), show that as the number of com-\nponents increases, both the server and adversary converge\non their respective baselines ultimately providing little to no\nbonk4\nbonk8\nbonk12\nSplit position\n40\n50\n60\n70\n80\n90\n100\nAccu acy\nADP, PCA, and Baseline Compa ison\nSβ, AE\nAβ, AE\nSβ, PCA\nAβ, PCA\nSα\nAα\nFigure 6: Obtained accuracies by 𝑆and 𝐴𝑖at bottleneck\nlayers 4, 8, and 12 with ADP and PCA protections. Inference\nadversary architecture: ResNet50 split.\nprotection, even in the best case near 101.25 components. At\nthis split position, the adversary and the server have arguably\nthe same influence on the final decision as they both have the\nsame number of convolutional layers as the client.\nAt bottleneck layer 12 for ADP, Figure 5(c), we observe\na very high accuracy, near baseline, for the server that\ndecreases overtime and the adversary accuracy that increases\nclose to its baseline value, as the number of training epochs\nincreases. This trend does not satisfy Equation 1. For PCA,\nFigure 5(f), we observe that server accuracy sharply rises\nto its baseline value as more components are added along\nwith the adversary accuracy that increases more gradually.\nIn the best case, near 100.7, Equation 1, is fulfilled. At\nthis split position, the adversary and server have arguably\nlimited influence on the final decision as they have fewer\nconvolutional layers compared to the client.\nOverall, we observe that the ADP strategy is preferred\nsince it is able to drop the adversary to the near-random guess\nand maintain an acceptable level of server performance for\nthe bottleneck layers 4 and 8, whereas the PCA plug-in\nstrategy only does so for the bottleneck layer 12, shown in\nFigure 6. Additionally, ADP is preferred since it can achieve\nresults at an earlier split position where it might be feasibly\nimplemented as opposed to a later split position close to\nthe server. Therefore, we conclude that the ADP strategy is\npreferred since it can provide protection at an earlier split\nposition with a greater utility at the earliest split for the\nResNet50 architecture, whereas PCA does not.\n6. Conclusion\nThis work reviews various protection technologies in\nthe existing literature and proposes a novel plug-in-based\nprotection strategy to address both privacy and utility con-\ncerns between the server and adversary in the cloud. We\ncompare PCA with our novel method that combines AE and\nCAMs to implement what we call the ADP approach, which\ncan destabilize the adversary in most if not all cases, while\nmaintaining the server performance with improvements over\nPCA. Specifically, the plugin nature of the approach and\nreduced size of the feature map are achieved in all cases\nby the design and implementation of the ADP approach.\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 13 of 15\n\n\nTowards Privacy-Preserving Split Learning\nFurther work is needed to improve the ADP strategy in\naddition to testing a greater depth and breadth of cases to\nfully understand how to optimize the protection and achieve\nthe desired universal results.\nReferences\nAlwani, M., Wang, Y., Madhavan, V., 2022. Decore: Deep compression\nwith reinforcement learning, in: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 12349–12359.\nCao, M., Zhang, L., Cao, B., 2021. Toward on-device federated learning:\nA direct acyclic graph-based blockchain approach. IEEE Transactions\non Neural Networks and Learning Systems .\nChi, J., Owusu, E., Yin, X., Yu, T., Chan, W., Tague, P., Tian, Y., 2018.\nPrivacy partitioning: Protecting user data during the deep learning\ninference phase. arXiv preprint arXiv:1812.02863 .\nDhar, S., Guo, J., Liu, J., Tripathi, S., Kurup, U., Shah, M., 2021.\nA\nsurvey of on-device machine learning: An algorithms and learning\ntheory perspective. ACM Transactions on Internet of Things 2, 1–49.\nDing, X., Fang, H., Zhang, Z., Choo, K.K.R., Jin, H., 2020.\nPrivacy-\npreserving feature extraction via adversarial training. IEEE Transactions\non Knowledge and Data Engineering .\nDuan, S., Wang, D., Ren, J., Lyu, F., Zhang, Y., Wu, H., Shen, X.,\n2022. Distributed artificial intelligence empowered by end-edge-cloud\ncomputing: A survey. IEEE Communications Surveys & Tutorials .\nErdoğan, E., Küpçü, A., Çiçek, A.E., 2022.\nUnsplit: Data-oblivious\nmodel inversion, model stealing, and label inference attacks against\nsplit learning, in: Proceedings of the 21st Workshop on Privacy in the\nElectronic Society, pp. 115–124.\nGao, Y., Tong, W., Wu, E.Q., Chen, W., Zhu, G., Wang, F.Y., 2023.\nChat with chatgpt on interactive engines for intelligent driving. IEEE\nTransactions on Intelligent Vehicles .\nGildenblat, J., contributors, 2021. Pytorch library for cam methods. https:\n//github.com/jacobgil/pytorch-grad-cam.\nGu, Z., Huang, H., Zhang, J., Su, D., Jamjoom, H., Lamba, A., Pendarakis,\nD., Molloy, I., 2018a. Confidential inference via ternary model parti-\ntioning. arXiv preprint arXiv:1807.00969 .\nGu, Z., Huang, H., Zhang, J., Su, D., Lamba, A., Pendarakis, D., Molloy,\nI., 2018b. Securing input data of deep learning inference systems via\npartitioned enclave execution. arXiv preprint arXiv:1807.00969 , 1–14.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image\nrecognition, in: Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 770–778.\nHe, Z., Zhang, T., Lee, R.B., 2019.\nModel inversion attacks against\ncollaborative inference, in: Proceedings of the 35th Annual Computer\nSecurity Applications Conference, pp. 148–162.\nHe, Z., Zhang, T., Lee, R.B., 2020. Attacking and protecting data privacy\nin edge–cloud collaborative inference systems. IEEE Internet of Things\nJournal 8, 9706–9716.\nJamshidi, M.A., Veisi, H., Mojahedian, M.M., Aref, M.R., 2024.\nAd-\njustable privacy using autoencoder-based learning structure.\nNeuro-\ncomputing 566, 127043. URL: https://www.sciencedirect.com/science/\narticle/pii/S0925231223011669, doi:https://doi.org/10.1016/j.neucom.\n2023.127043.\nJiao, L., Zhang, R., Liu, F., Yang, S., Hou, B., Li, L., Tang, X., 2021. New\ngeneration deep learning for video object detection: A survey. IEEE\nTransactions on Neural Networks and Learning Systems 33, 3195–3215.\nKairouz, P., Liao, J., Huang, C., Vyas, M., Welfert, M., Sankar, L.,\n2022. Generating fair universal representations using adversarial mod-\nels. IEEE Transactions on Information Forensics and Security 17, 1–1.\ndoi:10.1109/TIFS.2022.3170265.\nLee, D., Kohlbrenner, D., Shinde, S., Asanović, K., Song, D., 2020.\nKeystone: An open framework for architecting trusted execution en-\nvironments, in: Proceedings of the Fifteenth European Conference on\nComputer Systems, pp. 1–16.\nLi, A., Duan, Y., Yang, H., Chen, Y., Yang, J., 2020.\nTiprdc: Task-\nindependent privacy-respecting data crowdsourcing framework for deep\nlearning with anonymized intermediate representations, in: Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, Association for Computing Machinery, New\nYork, NY, USA. p. 824–832. URL: https://doi.org/10.1145/3394486.\n3403125, doi:10.1145/3394486.3403125.\nLi, A., Guo, J., Yang, H., Salim, F.D., Chen, Y., 2021a. Deepobfuscator:\nObfuscating intermediate representations with privacy-preserving ad-\nversarial learning on smartphones, in: Proceedings of the International\nConference on Internet-of-Things Design and Implementation, pp. 28–\n39.\nLi, J., Mei, X., Prokhorov, D., Tao, D., 2016.\nDeep neural network\nfor structural prediction and lane detection in traffic scene.\nIEEE\ntransactions on neural networks and learning systems 28, 690–703.\nLi, J., Rakin, A., Chen, X., He, Z., Fan, D., Chakrabarti, C., 2022. Ressfl:\nA resistance transfer framework for defending model inversion attack in\nsplit federated learning, in: Proceedings - 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2022, IEEE Com-\nputer Society. pp. 10184–10192.\ndoi:10.1109/CVPR52688.2022.00995.\npublisher Copyright: © 2022 IEEE.; 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022 ; Conference\ndate: 19-06-2022 Through 24-06-2022.\nLi, Z., Liu, F., Yang, W., Peng, S., Zhou, J., 2021b. A survey of convo-\nlutional neural networks: analysis, applications, and prospects. IEEE\ntransactions on neural networks and learning systems .\nLiu, J., Juuti, M., Lu, Y., Asokan, N., 2017.\nOblivious neural network\npredictions via minionn transformations, in: Proceedings of the 2017\nACM SIGSAC conference on computer and communications security,\npp. 619–631.\nLiu, Y., Zhang, Y., Wang, Y., Hou, F., Yuan, J., Tian, J., Zhang, Y., Shi, Z.,\nFan, J., He, Z., 2023. A survey of visual transformers. IEEE Transactions\non Neural Networks and Learning Systems .\nLiu, Z., Wu, Z., Gan, C., Zhu, L., Han, S., 2020. Datamix: Efficient privacy-\npreserving edge-cloud inference, in: European Conference on Computer\nVision, Springer. pp. 578–595.\nLuo, C.Y., Pearson, P., Xu, G., Rich, S.M., 2022. A computer vision-based\napproach for tick identification using deep learning models. Insects 13,\n116.\nMalekzadeh, M., Clegg, R.G., Haddadi, H., 2017. Replacement autoen-\ncoder: A privacy-preserving algorithm for sensory data analysis. arXiv\npreprint arXiv:1710.06564 .\nMao, Y., Hong, W., Wang, H., Li, Q., Zhong, S., 2020. Privacy-preserving\ncomputation offloading for parallel deep neural networks training. IEEE\nTransactions on Parallel and Distributed Systems 32, 1777–1788.\nMireshghallah, F., Taram, M., Ramrakhyani, P., Tullsen, D.M., Es-\nmaeilzadeh, H., 2019. Shredder: Learning noise to protect privacy with\npartial dnn inference on the edge. CoRR, abs/1905.11814 6.\nMishra, P., Lehmkuhl, R., Srinivasan, A., Zheng, W., Popa, R.A., 2020.\nDelphi: A cryptographic inference system for neural networks, in:\nProceedings of the 2020 Workshop on Privacy-Preserving Machine\nLearning in Practice, pp. 27–30.\nMuhammad, K., Khan, S., Del Ser, J., De Albuquerque, V.H.C., 2020. Deep\nlearning for multigrade brain tumor classification in smart healthcare\nsystems: A prospective survey. IEEE Transactions on Neural Networks\nand Learning Systems 32, 507–522.\nNguyen, H., Zhuang, D., Wu, P.Y., Chang, M., 2020.\nAutogan-based\ndimension reduction for privacy preservation.\nNeurocomputing 384,\n94–103.\nURL: https://www.sciencedirect.com/science/article/pii/\nS0925231219316352, doi:https://doi.org/10.1016/j.neucom.2019.12.002.\nOsia, S.A., Shamsabadi, A.S., Sajadmanesh, S., Taheri, A., Katevas, K.,\nRabiee, H.R., Lane, N.D., Haddadi, H., 2020. A hybrid deep learning\narchitecture for privacy-preserving mobile analytics. IEEE Internet of\nThings Journal 7, 4505–4518.\nOsia, S.A., Shamsabadi, A.S., Taheri, A., Katevas, K., Rabiee, H.R., Lane,\nN.D., Haddadi, H., 2017. Privacy-preserving deep inference for rich user\ndata on the cloud. arXiv preprint arXiv:1710.01727 .\nRouhani, B.D., Hussain, S.U., Lauter, K., Koushanfar, F., 2018. Redcrypt:\nreal-time privacy-preserving deep learning inference in clouds using\nfpgas. ACM Transactions on Reconfigurable Technology and Systems\n(TRETS) 11, 1–21.\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 14 of 15\n\n\nTowards Privacy-Preserving Split Learning\nSimonyan, K., Zisserman, A., 2014. Very deep convolutional networks for\nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 .\nSingh, A., Garza, E., Chopra, A., Vepakomma, P., Sharma, V., Raskar, R.,\n2022. Decouple-and-sample: Protecting sensitive information in task\nagnostic data release, in: Avidan, S., Brostow, G., Cissé, M., Farinella,\nG.M., Hassner, T. (Eds.), Computer Vision – ECCV 2022, Springer\nNature Switzerland, Cham. pp. 499–517.\nThapa, C., Mahawaga Arachchige, P.C., Camtepe, S., Sun, L., 2022.\nSplitfed: When federated learning meets split learning. Proceedings of\nthe AAAI Conference on Artificial Intelligence 36, 8485–8493. URL:\nhttps://ojs.aaai.org/index.php/AAAI/article/view/20825, doi:10.1609/\naaai.v36i8.20825.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Advances in\nneural information processing systems 30.\nWang, J., Bao, W., Sun, L., Zhu, X., Cao, B., Philip, S.Y., 2019. Private\nmodel compression via knowledge distillation, in: Proceedings of the\nAAAI Conference on Artificial Intelligence, pp. 1190–1197.\nWang, J., He, D., Castiglione, A., Gupta, B.B., Karuppiah, M., Wu, L.,\n2022. Pcnncec: Efficient and privacy-preserving convolutional neural\nnetwork inference based on cloud-edge-client collaboration.\nIEEE\nTransactions on Network Science and Engineering .\nWang, J., Zhang, J., Bao, W., Zhu, X., Cao, B., Yu, P.S., 2018. Not just pri-\nvacy: Improving performance of private deep learning in mobile cloud,\nin: Proceedings of the 24th ACM SIGKDD international conference on\nknowledge discovery & data mining, pp. 2407–2416.\nWang, Z., Bovik, A., Sheikh, H., Simoncelli, E., 2004. Image quality assess-\nment: from error visibility to structural similarity. IEEE Transactions on\nImage Processing 13, 600–612. doi:10.1109/TIP.2003.819861.\nWang, Z., Simoncelli, E., Bovik, A., 2003. Multiscale structural similarity\nfor image quality assessment, in: Conference Record of the Asilomar\nConference on Signals, Systems and Computers, pp. 1398 – 1402 Vol.2.\ndoi:10.1109/ACSSC.2003.1292216.\nYang, J., Zheng, J., Zhang, Z., Chen, Q., Wong, D.S., Li, Y., 2022a.\nSecurity of federated learning for cloud-edge intelligence collaborative\ncomputing. International Journal of Intelligent Systems 37, 9290–9308.\nYang, S., Zhang, Z., Zhao, C., Song, X., Guo, S., Li, H., 2022b. Cnnpc: End-\nedge-cloud collaborative cnn inference with joint model partition and\ncompression. IEEE Transactions on Parallel and Distributed Systems\n33, 4039–4056.\nZhai, J., Zhang, S., Chen, J., He, Q., 2018. Autoencoder and its various\nvariants, in: 2018 IEEE international conference on systems, man, and\ncybernetics (SMC), IEEE. pp. 415–419.\nZhang, M., Zhang, H., Zhang, C., Yuan, D., 2022. Communication-efficient\nquantized deep compressed sensing for edge-cloud collaborative indus-\ntrial iot networks. IEEE Transactions on Industrial Informatics .\nZhang, X., Razavi-Far, R., Isah, H., David, A., Higgins, G., Zhang, M.,\n2025. A survey on deep learning in edge–cloud collaboration: Model\npartitioning, privacy preservation, and prospects.\nKnowledge-Based\nSystems 310, 112965. URL: https://www.sciencedirect.com/science/\narticle/pii/S0950705125000139, doi:https://doi.org/10.1016/j.knosys.\n2025.112965.\nZhao, Z.Q., Zheng, P., Xu, S.t., Wu, X., 2019. Object detection with deep\nlearning: A review. IEEE transactions on neural networks and learning\nsystems 30, 3212–3232.\nZheng, H., Chen, J., Shangguan, W., Ming, Z., Yang, X., Yang,\nZ., 2023.\nGone: A generic o(1) noise layer for protecting pri-\nvacy of deep neural networks.\nComputers & Security 135,\n103471.\nURL: https://www.sciencedirect.com/science/article/pii/\nS0167404823003814, doi:https://doi.org/10.1016/j.cose.2023.103471.\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., 2016. Learning\ndeep features for discriminative localization, in: Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 2921–2929.\nG. Higgins et al.: Preprint submitted to Elsevier\nPage 15 of 15\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20629v1.pdf",
    "total_pages": 16,
    "title": "Towards Privacy-Preserving Split Learning: Destabilizing Adversarial Inference and Reconstruction Attacks in the Cloud",
    "authors": [
      "Griffin Higgins",
      "Roozbeh Razavi-Far",
      "Xichen Zhang",
      "Amir David",
      "Ali Ghorbani",
      "Tongyu Ge"
    ],
    "abstract": "This work aims to provide both privacy and utility within a split learning\nframework while considering both forward attribute inference and backward\nreconstruction attacks. To address this, a novel approach has been proposed,\nwhich makes use of class activation maps and autoencoders as a plug-in strategy\naiming to increase the user's privacy and destabilize an adversary. The\nproposed approach is compared with a dimensionality-reduction-based plug-in\nstrategy, which makes use of principal component analysis to transform the\nfeature map onto a lower-dimensional feature space. Our work shows that our\nproposed autoencoder-based approach is preferred as it can provide protection\nat an earlier split position over the tested architectures in our setting, and,\nhence, better utility for resource-constrained devices in edge-cloud\ncollaborative inference (EC) systems.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}