{
  "id": "arxiv_2502.21029v1",
  "text": "1\nSixth-Sense: Self-Supervised Learning\nof Spatial Awareness of Humans from a Planar Lidar\nSimone Arreghini, Nicholas Carlotti, Mirko Nava, Antonio Paolillo, and Alessandro Giusti\nAbstract—Localizing humans is a key prerequisite for any\nservice robot operating in proximity to people. In these scenarios,\nrobots rely on a multitude of state-of-the-art detectors usually\ndesigned to operate with RGB-D cameras or expensive 3D\nLiDARs. However, most commercially available service robots\nare equipped with cameras with a narrow field of view, making\nthem blind when a user is approaching from other directions,\nor inexpensive 1D LiDARs whose readings are difficult to inter-\npret. To address these limitations, we propose a self-supervised\napproach to detect humans and estimate their 2D pose from\n1D LiDAR data, using detections from an RGB-D camera as a\nsupervision source. Our approach aims to provide service robots\nwith spatial awareness of nearby humans. After training on 70\nminutes of data autonomously collected in two environments, our\nmodel is capable of detecting humans omnidirectionally from 1D\nLiDAR data in a novel environment, with 71% precision and\n80% recall, while retaining an average absolute error of 13 cm\nin distance and 44◦in orientation.\nIndex Terms—Self-Supervised Learning, Human Perception,\nService Robotics, Human-Robot Interaction.\nI. INTRODUCTION\nS\nERVICE\nrobots\noperating\nhuman-populated\nenviron-\nments [1]–[3] must be capable to perceive humans, predict\ntheir behavior [4] and even intentions [5]. Indeed, humans’\ndetection and localization are crucial for Human-Robot Inter-\naction (HRI) applications where humans and robots are close,\nto ensure safe cooperation, improve navigation in crowded\nspaces, and provide insightful social cues. A typical sensing\nsetup for service robots includes a combination of a wide Field\nof View (FOV) laser sensor, often seeing 360◦around the\nrobot, and a narrow FOV RGB-D camera. Despite 3D LiDAR\nsensors offering rich information about the surroundings [6],\ntheir use in service robots is limited to a small percent-\nage of high-end platforms. Instead, the majority is equipped\nwith simpler 1D planar LiDAR sensors, typically located at\nwheel height, striking a balance between production costs and\nrichness of information. Examples span from large research\nrobots [7] to those commonly present in many households\nsuch as robot vacuum cleaners, or lawnmowers [8].\nDeep learning models capable of reliably detecting and\nlocalizing humans from RGB-D data have been extensively\ninvestigated in literature [9], [10]; however, doing the same\nwith 1D laser sensors remains an open problem due to\nthe sparse nature and complex interpretation of the sensor’s\nAll authors are with the Dalle Molle Institute for Artificial Intelligence\n(IDSIA), USI-SUPSI, Lugano, Switzerland. Corresponding author: Simone\nArreghini, simone.arreghini@supsi.ch\nThis work was supported by the European Union through the project SER-\nMAS, by the Swiss State Secretariat for Education, Research and Innovation\n(SERI) under contract number 22.00247, and by the Swiss National Science\nFoundation, grant number 213074.\nFig. 1.\nOur approach uses a human detector from the narrow FOV Azure\nKinect as a source of labels to train a 1D FCN that, given planar LiDAR\nscans, predicts the presence and relative 2D pose of humans around the robot.\nThe training approach relies only on hardware onboard the robot and can\nautonomously collect data in any environment. In this environment (Lab), a\nMotion Capture system collects ground truth used for evaluation purposes.\nreadings. Notably, the typical environment for a service robot\nmay feature structures that result in readings mimicking the\nprofile of human beings, such as the legs of tables, bars\nin railings, or pets; differentiating humans from the rest\nrequires recognizing subtle geometric and dynamic patterns.\nNonetheless, awareness of the presence and direction of nearby\nhumans significantly improves the robot’s behavior in social\ncontexts [11], even when these perceptions are uncertain and\npossibly inaccurate. Indeed, inspired by animal perception\nwhere peripheral vision and hearing direct visual focus towards\nareas of interest [12], a robot could use uncertain detections\nfrom 1D LiDAR data to trigger further sensing by the more\nreliable RGB-D camera. Unlike previous approaches that rely\non 3D LiDAR [13], [14], we handle a less informative sensing\nmodality, leading to inherently lower detection accuracy but\nwith a potential widespread adoption by many robotic plat-\nforms.\nOur approach relies on a deep learning model that can\nbe trained, or fine-tuned, directly by the robot during its\ndeployment using self-supervision: an off-the-shelf detector\nreceiving data from the onboard RGB-D camera is used to\nprovide detections of humans considered as training labels,\nas shown in Fig. 1. This is an instance of the general class\nof approaches using one sensor to supervise the training of a\nmodel which interprets data from a different sensor [15], [16];\nthe same paradigm has been applied to skeleton joint pose esti-\narXiv:2502.21029v1  [cs.RO]  28 Feb 2025\n\n\n2\nmation from 3D LiDAR, using an image-based human detector\nproviding the 2D skeleton joints pose as supervision [17].\nOur model receives as input a moving time window of\n1D LiDAR scans and predicts the presence and 2D pose\n(2D position and relative bearing) of humans around the\nrobot. Specifically, we employ a loss function to minimize the\ndistance between the model’s predicted detections from 1D\nLiDAR and those coming from the detector, enforced only\nwhere the two fields of view overlap. In this context, the self-\nsupervised learning paradigm: allows the model to adapt to the\nspecific deployment environment and sensor characteristics;\neliminates the need for large-scale pre-collected datasets; and\nis robust to cluttered environments that generate scans falsely\nmimicking human presence. Further, we adopt a 1D FCN [18]\nand leverage its translation-invariance to extend the detection\nability to the wider FOV of the LiDAR sensor, even in\ndirections never covered by the camera.\nWe describe our main contribution in Section II: a practical\nmethodology and open-source implementation for training\nand running a human detector and pose estimator from 1D\nLiDAR data using camera detections as self-supervision. As a\nsecondary contribution, datasets collected using this setup, as\nwell as the pre-trained models resulting from our approach, are\nmade publicly available. Section III details the experimental\nsetup comprising our platform, sensing setup, data collection,\nand network architecture. Section IV presents the quantitative\nanalysis of our approach’s performance against the ground\ntruth, while deployed on a TIAGo robot equipped with an\nAzure Kinect camera and two 1D LiDAR sensors. Section V\nconcludes the article with final remarks and a discussion on\nfuture research directions.\nII. METHODOLOGY\nWe train a 1D FCN model on the task of estimating 2D\nposes of humans around the robot, giving as input a time\nwindow of readings from a planar LiDAR sensor located at\nthe center of the robot base with a uniform angular resolution\nacross the entire FOV. In practice, we use a tensor of 360\nelements representing rays equally spaced around the robot\nwith a resulting angular resolution of 1◦, and having n\nchannels as the time window history length. We reproject past\nmeasurements as if they were captured from the robot’s current\nposition accounting for the robot’s motion as estimated by its\nFig. 2.\nWalking people and static structures as perceived by the LiDAR:\nlighter shades of green indicate older scans in the temporal window, whereas\nblack arrows indicate the people’s instantaneous orientation.\nFig. 3. Training data is autonomously collected by the robot in a University\nCorridor, on the top, and a Break Area, on the bottom.\nodometry. As a result, static obstacles yield overlapping points\nacross the channels, while moving obstacles leave trail-like\npoint patterns, as shown in Fig. 2. The model is tasked to\npredict the presence of humans in the environment, and their\ndistance and bearing relative to the robot. For each ray, our\nmodel predicts: the scalar ˆp ∈[0, 1] representing the likelihood\nof human presence; the relative distance ˆd ∈[dmin, dmax],\nwhere dmin and dmax are the working range of the sensor;\nand the sine and cosine of the relative bearing ˆo ∈[−π, π]\nexpressed as the difference between the person’s orientation\nand the direction parallel to the ray, with zero indicating a\nperson directly facing the robot. During inference, a discrete\nset of detections is obtained by thresholding and applying Non-\nmaxima Suppression (NMS) to the model’s predicted presence.\nWe aim to extend the detection ability of the model from\nthe narrow area covered by the camera’s FOV to the wider\nLiDAR’s FOV, including areas where the supervision is scarce\nor absent, such as behind the robot. To this end, we rely on\nthe translational invariance of convolutions, in which patterns\nare detected regardless of their position within the input.\nThe self-supervision signal is derived from the front-facing\nonboard camera providing the 3D pose of people’s joints in\nthe camera’s FOV. Among the joints, we specifically select the\none located at the pelvis as it’s closely tied to the legs’ motion,\nand project its pose onto the horizontal plane to get 2D poses.\nLabels for human presence p, distance d, and relative bearing\no are obtained from such poses for each ray of the LiDAR\nscan intersecting a person. The presence p is set to 1 when a\nperson is detected along the ray, or 0 otherwise; accordingly,\nthe relative distance and bearing labels of people are assigned\nto rays in which they are detected, or left undefined otherwise.\nOur model is trained to regress the three components using\na masked loss, considering only errors generated from the\nrays corresponding to the area covered by the camera’s FOV.\nAdditionally, distance and orientation losses are computed\nonly for rays in which the supervision labels indicate the\npresence of humans.\n\n\n3\nIII. EXPERIMENTAL SETUP\nA. Hardware\nWe use a customized version of the PAL Robotics TIAGo\nrobot composed of a differential drive base, a torso with a\nprismatic joint, a 7 degrees-of-freedom manipulator, and a\nhead that can pan in the range ±75◦and tilt from −60◦to\n45◦w.r.t. the horizontal plane. Our TIAGo is equipped with\nadditional sensors to better suit HRI applications: a Microsoft\nAzure Kinect RGB-D camera located in the head reliably\ntracks humans up to 6 m [10], has a narrow horizontal FOV\nof 65◦, and frame rate of 15 Hz; a secondary LiDAR sensor\nmounted on the back of the robot’s base, in addition to the\nbuilt-in one located on the front, as shown in Fig. 1. The\nfront-facing LiDAR is at an height of 95 mm, has a FOV of\n190◦, and scan rate of 15 Hz; the back-facing one is at an\nheight of 329 mm, FOV of 255◦, and scan rate of 10 Hz; both\nsensors’ operating range goes from 0.05 m to 10 m. To obtain a\nsingle, omnidirectional, and radially symmetric sensor, we fuse\nthe two physical LiDARs into a virtual one: the two sensors’\nreadings are time-synchronized at a rate of 10 Hz, projected\nonto the 2D plane, expressed in the frame of the robot base,\nand aggregated into 1◦-wide bins; each bin is assigned the\nvalue of the closest point among its members; when there are\nno members, a default value of 10 m is used.\nB. Dataset\nWe collected data across 9 days in three environments:\nUniversity Corridor (shown in Fig. 3, top), a public transit\narea between classrooms with study desks on the side and\npassers-by (36k samples); Break Area (Fig. 3, bottom), a\nlarge room with tables and chairs where expert individuals\ninteract with the robot (12k samples); and Lab (Fig. 1), a\nlaboratory setting with expert individuals interacting with the\nrobot (7k samples). During data collection, the robot’s base\nmotion and the head panning are randomized to increase the\ndata variability and area covered by the camera. The robot\nbase is manually controlled in the University Corridor for\nsecurity reasons whereas, in the other environments, it moves\nautonomously following random trajectories while avoiding\ncollisions. In all environments, we record body joints from the\nAzure Kinect and scans from the two LiDARs. Additionally,\nthe Lab environment provides ground truth poses for people\nand the robot at 100 Hz from an OptiTrack motion capture\nsystem featuring 18 cameras. The data is split into a training\nset composed of all samples from University Corridor and half\nof those from Break Area, totaling 42k samples; the remaining\n6k samples from Break Area are used as validation set, and\nthe 7k samples from Lab are used as the test set.\nC. Model architecture\nOur model, depicted in Fig. 4, is composed of 7 1D\nconvolutional layers, each with 32 output channels and layer\nnormalization. It features dilated circular convolutions with in-\ncreasing kernel dimension from 3 to 7, resulting in a receptive\nfield of 43◦. Specifically, dilation is used to have a low model\ncomplexity while achieving a receptive field large enough to\nLaser Sensor Readings \nRobot Front / \nAngle\n7 Convolutional Layers\nti\nti-n+1\n65° Camera FOV\nMasked MSE loss\n43° Receptive Field\n Presence\n Distance\n Cosine\n Sine\n Presence\n Distance\n Cosine\n Sine\nOutputs\nLabels\nAngle\nFig. 4.\nOur model uses a temporal window of n LiDAR scans to predict\nthe presence p of nearby people, their distance d, and relative bearing o\n(represented by sine and cosine). Dilated circular convolutions handle omni-\ndirectional scans and yield a 43◦receptive field. A masked MSE loss is only\nenforced on predictions that overlap with the camera FOV (red shaded area).\nFig. 5. Left: Precision-Recall curve for detection. Right: relative bearing error\ndistribution. Results computed against mocap ground truth in the Lab test set.\neffectively capture human motion. During training, the model\nminimizes a squared error between predictions and label values\nof each output. We train our model for 500 epochs at a constant\nlearning rate of 3e−4 using the Adam optimizer [19] and select\nthe model weights resulting in the lowest validation loss. We\napply additive Gaussian noise and mirroring to the input as\ndata augmentations during model training.\nIV. RESULTS\nWe compare our model using the last n = 30 scans collected\nat 10 Hz over three seconds, called History, with an ablated\nbaseline named No History that uses only the current sensor\nreading as input (n = 0). All models are tested using the\nground truth poses collected in the Lab environment, as shown\nin Fig. 1. On the left of Fig. 5, we report the Precision-Recall\ncurve illustrating the human detection performance of our\nmodels. This curve is generated by progressively increasing the\ndetection threshold applied to the presence output ˆp. Human’s\npredicted positions are obtained by projecting a point in the\ndirection of the corresponding ray at the estimated distance\nˆd. A prediction is considered a match (true positive) when it\nhas an Euclidean distance smaller than 50 cm w.r.t. the ground\n\n\n4\n2 m\nHumans\nMotion Capture\nAzure Kinect\nTrue Positive (TP)\nTP\nTP\nTP\nFalse Positive (FP)\n2 m\nSensor\nLiDAR\nAzure Kinect FOV\nModel Output\nRejected\nAccepted\nTP\nFP\nFN\nFN\nFig. 6. Results on the test set: third-person view of two frames (left) and corresponding top view (right) depicting the LiDAR scan; camera FOV and detected\npose arrows; Motion Capture ground truth pose arrows. The predicted presence ˆp is shown as a gray line when below the detection threshold of 90% (dashed\ncircle centered on the robot), or black otherwise. Predictions are represented by arrows colored differently for true positives (TP), and false positives (FP).\ntruth; the orientation component does not influence the match-\ning procedure. The plot shows that, for recall values above\n60%, the History model (in purple) consistently outperforms\nNo History (in light blue). On the right of Fig. 5, we compare\nthe orientation error distributions. We include a Dummy model\nthat always returns the average ground truth orientation and\ndistance in the test set. Errors are computed only for matched\npredictions (true positives), following the same approach used\nfor the Precision-Recall curve; for the Dummy model, instead,\nwe consider the whole test set, i.e., assuming an ideal detec-\ntion. Results show that temporal information is required for\nrelative bearing estimation: History yields a mean absolute\norientation error of 44◦compared to No History with 74◦\nand Dummy with 75◦; this result confirms the findings in the\nliterature on the human motion model [20], [21].\nThe qualitative performance of our approach on detection\nand 2D pose estimation are shown in Fig. 6, with the TIAGo\nrobot deployed in the Lab environment. Failed detections\nderive from the choice of threshold yielding false negatives for\nhigh predictions that fall below the threshold. Table I summa-\nrizes the models’ performance with the following metrics: for\nhuman detection, we report the precision on the true positive\npredictions corresponding to the threshold yielding a recall of\n80%, represented as P80, where the History model outperforms\nNo History by scoring P80 = 70.6% against P80 = 60.7%. On\nthe true positive predictions, we additionally measure the mean\norientation Eo and distance Ed absolute errors.\nTABLE I\nMODELS’ PERFORMANCE ON HUMAN DETECTION P80, MEAN RELATIVE\nBEARING ERROR EO, AND MEAN DISTANCE ERROR ED ON THE TEST SET.\nModel\nP80\nEo\nEd\nBarplot for P80 [%] →\n[%] ↑\n[deg] ↓\n[cm] ↓\nDummy\n−\n75\n64\n40\n60\n80\n100\nNo History\n60.7\n74\n10\nHistory\n70.6\n42\n13\nV. CONCLUSION\nWe presented a novel approach for human detection and\npose estimation in service robots using 1D LiDAR sensors. It\nleverages a state-of-the-art detector from an RGB-D camera\nas the source of self-supervision, requiring no pre-collected\ndatasets. This approach can adapt to different sensing setups,\nassuming only to have a precise albeit narrow source of\nsupervision for interpreting readings from a much wider FOV,\npossibly omnidirectional sensor. The code required to collect\ndata, train models, and run them in real time is made publicly\navailable for the benefit of the community; we also provide\npre-trained model weights and datasets. Future work will focus\non the use of pretext tasks to leverage unlabeled data at training\ntime, and extend the model predictions with more complex\nsocial cues, such as the intention to interact, which could be\nhighly beneficial in human-robot interaction scenarios.\n\n\n5\nREFERENCES\n[1] F. B. V. Benitti, “Exploring the educational potential of robotics in\nschools: A systematic review,” Comp. & Education, vol. 58, no. 3, pp.\n978–988, 2012.\n[2] C. S. Gonz´alez-Gonz´alez, V. Violant-Holz, and R. M. Gil-Iranzo, “Social\nrobots in hospitals: a systematic review,” Appl. Sci., vol. 11, no. 13, p.\n5976, 2021.\n[3] M. M. O. Youngjoon Choi, Miju Choi and S. S. Kim, “Service robots\nin hotels: understanding the service quality perceptions of human-robot\ninteraction,” J. of Hospitality Marketing & Management, vol. 29, no. 6,\npp. 613–635, 2020.\n[4] A. Zaraki, M. Giuliani, M. B. Dehkordi, D. Mazzei, A. D’ursi, and\nD. De Rossi, “An RGB-D based social behavior interpretation system\nfor a humanoid social robot,” in RSI/ISM Int. Conf. on Robot. and\nMechatronics, 2014, pp. 185–190.\n[5] S. Arreghini, G. Abbate, A. Giusti, and A. Paolillo, “Predicting the\nintention to interact with a service robot: the role of gaze cues,” in\nIEEE Int. Conf. Robot. and Autom., 2024, pp. –.\n[6] R. Martin-Martin, M. Patel, H. Rezatofighi, A. Shenoi, J. Gwak,\nE. Frankel, A. Sadeghian, and S. Savarese, “Jrdb: A dataset and\nbenchmark of egocentric robot visual perception of humans in built\nenvironments,” IEEE Trans. on Pattern Anal. and Machine Intell.,\nvol. 45, no. 6, pp. 6748–6765, 2021.\n[7] J. Pages, L. Marchionni, and F. Ferro, “TIAGo: the modular robot that\nadapts to different research needs,” in International workshop on robot\nmodularity, IROS, vol. 290, 2016.\n[8] H. Mahdi, S. A. Akgun, S. Saleh, and K. Dautenhahn, “A survey on the\ndesign and evolution of social robots—past, present and future,” Elsevier\nRobotics and Autonomous Systems, vol. 156, p. 104193, 2022.\n[9] M. Paul, S. M. Haque, and S. Chakraborty, “Human detection in\nsurveillance videos and its applications-a review,” Springer Journal on\nAdvances in Signal Processing, vol. 2013, no. 1, pp. 1–16, 2013.\n[10] M. T¨olgyessy, M. Dekan, and L. Chovanec, “Skeleton tracking accuracy\nand precision evaluation of kinect v1, kinect v2, and the azure kinect,”\nMDPI Applied Sciences, vol. 11, no. 12, p. 5756, 2021.\n[11] N. K. Dhiman, D. Deodhare, and D. Khemani, “Where am i? creating\nspatial awareness in unmanned ground robots using slam: A survey,”\nSpringer Sadhana, vol. 40, pp. 1385–1433, 2015.\n[12] M. D. Vernon, “The peripheral perception of movement,” Cambridge\nUniversity Press British Journal of Psychology, vol. 23, no. 3, p. 209,\n1933.\n[13] Z. Yan, T. Duckett, and N. Bellotto, “Online learning for 3d lidar-based\nhuman detection: experimental analysis of point cloud clustering and\nclassification methods,” Springer Autonomous Robots, vol. 44, no. 2,\npp. 147–164, 2020.\n[14] J. N. Hayton, T. Barros, C. Premebida, M. J. Coombes, and U. J. Nunes,\n“Cnn-based human detection using a 3d lidar onboard a uav,” in IEEE\nInt. Conf. Auton. Robot. Sys. and Comp., 2020, pp. 312–318.\n[15] M. Nava, J. Guzzi, R. O. Chavez-Garcia, L. M. Gambardella, and\nA. Giusti, “Learning long-range perception using self-supervision from\nshort-range sensors and odometry,” IEEE Robot. and Autom. Lett., vol. 4,\nno. 2, pp. 1279–1286, 2019.\n[16] M. Nava, A. Paolillo, J. Guzzi, L. M. Gambardella, and A. Giusti,\n“Uncertainty-aware self-supervised learning of spatial perception tasks,”\nIEEE Robot. and Autom. Lett., vol. 6, no. 4, pp. 6693–6700, 2021.\n[17] P. Cong, Y. Xu, Y. Ren, J. Zhang, L. Xu, J. Wang, J. Yu, and Y. Ma,\n“Weakly supervised 3d multi-person pose estimation for large-scale\nscenes based on monocular camera and single lidar,” in AAAI Conference\non Artificial Intelligence, vol. 37, no. 1, 2023, pp. 461–469.\n[18] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in IEEE/CVF Conf. on Comp. Vision and\nPattern Recogn., 2015, pp. 3431–3440.\n[19] D. P. Kingma and J. Ba, “Adam: a method for stochastic optimization,”\nin Int. Conf. on Learn. Represent., 2015.\n[20] W. Liu, Y. Zhang, S. Tang, J. Tang, R. Hong, and J. Li, “Accurate\nestimation of human body orientation from rgb-d sensors,” IEEE Trans.\non Cybernetics, vol. 43, no. 5, pp. 1442–1452, 2013.\n[21] Z. Luo, S. A. Golestaneh, and K. M. Kitani, “3d human motion\nestimation via motion compression and refinement,” in Asian Conference\non Computer Vision, 2020.\n\n\n6\nTABLE II\nDESCRIPTION OF SUPPLEMENTARY MATERIALS\nSupplementary material\nDescription\nZenodo Dataset Link\nLink to the Zenodo page for the publicly available dataset. The excessive size of our\ndataset prevents direct upload. Further dataset explanation is available at the linked page.\nURL: https://zenodo.org/records/14936069\nIEEE DataPort Link\nLink to the IEEE DataPort page for the publicly available dataset. URL: https://ieee-\ndataport.org/documents/sixth-sense-indoor-human-spatial-awareness-dataset\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21029v1.pdf",
    "total_pages": 6,
    "title": "Sixth-Sense: Self-Supervised Learning of Spatial Awareness of Humans from a Planar Lidar",
    "authors": [
      "Simone Arreghini",
      "Nicholas Carlotti",
      "Mirko Nava",
      "Antonio Paolillo",
      "Alessandro Giusti"
    ],
    "abstract": "Localizing humans is a key prerequisite for any service robot operating in\nproximity to people. In these scenarios, robots rely on a multitude of\nstate-of-the-art detectors usually designed to operate with RGB-D cameras or\nexpensive 3D LiDARs. However, most commercially available service robots are\nequipped with cameras with a narrow field of view, making them blind when a\nuser is approaching from other directions, or inexpensive 1D LiDARs whose\nreadings are difficult to interpret. To address these limitations, we propose a\nself-supervised approach to detect humans and estimate their 2D pose from 1D\nLiDAR data, using detections from an RGB-D camera as a supervision source. Our\napproach aims to provide service robots with spatial awareness of nearby\nhumans. After training on 70 minutes of data autonomously collected in two\nenvironments, our model is capable of detecting humans omnidirectionally from\n1D LiDAR data in a novel environment, with 71% precision and 80% recall, while\nretaining an average absolute error of 13 cm in distance and 44{\\deg} in\norientation.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}