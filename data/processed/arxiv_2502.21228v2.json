{
  "id": "arxiv_2502.21228v2",
  "text": "ECLEKTIC: a Novel Challenge Set for\nEvaluation of Cross-Lingual Knowledge Transfer\nOmer Goldman∗γβ, Uri Shaham∗γ, Dan Malkinγ,\nSivan Eigerγ, Avinatan Hassidimγ, Yossi Matiasγ, Joshua Maynezδ,\nAdi Mayrav Giladyγ, Jason Riesaδ, Shruti Rijhwaniδ, Laura Rimellδ,\nIdan Szpektorγ, Reut Tsarfatyγ, Matan Eyalγ\nβBar-Ilan University γGoogle Research, δGoogle DeepMind\n{ogoldman, urishaham, matane}@google.com\nAbstract\nTo achieve equitable performance across lan-\nguages, multilingual large language models\n(LLMs) must be able to abstract knowledge\nbeyond the language in which it was ac-\nquired. However, the current literature lacks\nreliable ways to measure LLMs’ capability\nof cross-lingual knowledge transfer. To that\nend, we present ECLEKTIC,1 a multilingual\nclosed-book QA (CBQA) dataset that Evalu-\nates Cross-Lingual Knowledge Transfer in a\nsimple, black-box manner. We detected in-\nformation with uneven coverage across lan-\nguages by controlling for presence and absence\nof Wikipedia articles in 12 languages.\nWe\ngenerated knowledge-seeking questions in a\nsource language, for which the answer appears\nin a relevant Wikipedia article and translated\nthem to all other 11 languages, for which the\nrespective Wikipedias lack equivalent articles.\nAssuming that Wikipedia reflects the promi-\nnent knowledge in the LLM’s training data, to\nsolve ECLEKTIC’s CBQA task the model is\nrequired to transfer knowledge between lan-\nguages. Experimenting with 8 LLMs, we show\nthat SOTA models struggle to effectively share\nknowledge across, languages even if they can\npredict the answer well for queries in the same\nlanguage the knowledge was acquired in.\n1\nIntroduction\nIdeally, multilingual large language models (LLMs;\nGemini Team, 2024; Llama Team, 2024; OpenAI,\n2024, inter alia) should perform similarly and con-\nsistently in all languages they were trained on and\nspecifically be as knowledgeable across these lan-\nguages. In addition to making models more human-\nlike, this would enable speakers of languages with\n*Equal contribution\n1The dataset is available at https://www.kaggle.com/\ndatasets/googleai/eclektic\nA model without knowledge transfer\nIn deutschpachigen\nfilmen, wer\nsynchronisiert\nBrad Pitt?\nIn German-speaking\nfilms, who dubs\nBrad Pitt?\nTobias Meister\nEn las películas\nen alemán, ¿quién\ndobla\na Brad Pitt?\nIn German-speaking\nfilms, who dubs\nBrad Pitt?\nTobias Meister\nIn deutschpachigen\nfilmen, wer\nsynchronisiert\nBrad Pitt?\nIn German-speaking\nfilms, who dubs\nBrad Pitt?\nTobias Meister\nEn las películas\nen alemán, ¿quién\ndobla\na Brad Pitt?\nIn German-speaking\nfilms, who dubs\nBrad Pitt?\nNo lo sé  (I don’t know)\nTraining data\nA model with knowledge transfer\nDE\nTobias Meister\nTobias Meister (9. Juni 1957 in Köln)\nist vor allem als deutsche Stimme\ndes Schauspielers Brad Pitt bekannt...\nES\nTobias Meister\n404 - page not found\nFigure 1: A model incapable of cross-lingual knowledge\ntransfer (top box) can only answer factual questions in\ntheir source language, that is, the language in which\nthe information appeared in its training. It cannot an-\nswer the same question when translated into another\ntarget language. A transfer capable model (bottom\nbox), is able to answer questions no matter the language.\nECLEKTIC allows distinguishing between the two by\ntargeting facts that unevenly distributed in the model’s\ntraining data as approximated, in the figure and in the\npaper, by Wikipedia.\narXiv:2502.21228v2  [cs.CL]  3 Mar 2025\n\n\nsmaller resource footprints on the Web to have eq-\nuitable access to the world’s knowledge, and speak-\ners of higher resource languages, to access a wider\nrange of information. Alas, LLMs are known to be\ninconsistent across languages (e.g., Ohmer et al.,\n2023), and in particular, their performance on fac-\ntual queries varies significantly depending on the\nlanguage in which the model is queried (Jiang et al.,\n2020; Kassner et al., 2021; Qi et al., 2023).\nLLMs may learn similar knowledge in differ-\nent languages when such documents are encoun-\ntered during pretraining. But, since observing all\nof the world’s knowledge in all languages, e.g.,\nvia translation, is not feasible, a complementary\nrequirement from LLMs may be to share acquired\nknowledge between languages, irrespective of the\nlanguage it was acquired in. Models with such\ncapability would either represent knowledge in\na language-agnostic way, or implicitly translate\nknowledge at inference time from the language in\nwhich it was acquired to the target language. How-\never, measuring this knowledge-sharing capability\nis not trivial. Methods suggested in the literature\nrequire either causal interventions that are far from\nperfect and limited in their transparency (Ifergan\net al., 2024; Wei et al., 2024) or careful dissection\nof the model’s inner states that may be noisy (Chen\net al., 2024; Zhao et al., 2024a).\nIn this paper, we address the problem of em-\npirically quantifying the cross-lingual knowledge-\nsharing ability of LLMs. Aiming for a simple,\nblack-box evaluation, we introduce ECLEKTIC\nto Evaluate Cross-Lingual Knowledge Transfer,\nby means of closed book QA. Consider, for exam-\nple, a question asking who usually dubs characters\nplayed by Brad Pitt in German movies. Since in\nthe German part of the internet it is well-known\nthat the answer is Tobias Meister, a modern LLM\nis expected to answer this question easily when\nasked in German. However, to answer that in other\nlanguages, in which the internet contains little ev-\nidence of this fact, LLMs may struggle without\nbeing able to internally retrieve the German knowl-\nedge (see Figure 1).\nTo target facts like the one above, we constructed\nECLEKTIC, by targeting articles in Wikipedias\nin 12 languages that have no equivalent articles\nin the other languages, such as the article ded-\nicated to Tobias Meister that exists only on the\nGerman Wikipedia. We generated fact-seeking\nquestion/answer pairs based on those articles us-\ning Gemini (Gemini Team, 2024) and translated\nthem to the other tested languages. The entire gen-\neration and translation phases were manually ver-\nified by human annotators. The result is a set of\nquestions and answers that relate to a fact that is\nwell known only in one language (henceforth: the\nsource language), but are contained in the dataset\nin all 12 languages (the target languages). Each\nquestion-answer pair is accompanied by the rele-\nvant Wikipedia context. Various LLMs were then\nasked these questions across all target languages\nin a closed-book setting and their predictions were\njudged by another model (Chiang and Lee, 2023;\nZheng et al., 2023) in an open-book setting.\nBased on the predictions for each question in the\n12 languages, we defined 2 metrics: overall success\nthat reflects the model’s ability to solve ECLEK-\nTIC as a whole by transferring knowledge across\nlanguages, and transfer ability that only measures\nthe model’s ability to transfer correct answers. We\nexperimented with 8 top-performing models, both\nopen-source and proprietary, to demonstrate that,\nacross the board, ECLEKTIC poses a significant\nchallenge. The best performing model, Gemini 2.0\nPro, achieves overall success of 41.3% and man-\nages to transfer only 65.0% of the facts it was able\nto retrieve in the the respective source language.\nBreaking down the results by source and target\nlanguage, we show that shared script is a major\nfactor in the ease of transfer, corroborating findings\nfrom previous works (Qi et al., 2023; Ifergan et al.,\n2024). Finally, we tested models of various sizes,\nall from the Qwen 2.5 model series (Qwen Team,\n2025), and found that bigger models are not able\nto transfer more knowledge in relative terms, i.e.,\nin terms of transfer ability, although they are more\nsuccessful in terms of overall success.\nAll in all, the contribution of this paper is\ntwofold. First, we introduce ECLEKTIC, a novel\nbenchmark for cross-lingual transfer evaluation,\nalong with its construction and evaluation process.\nSecond, we present a systematic evaluation of state-\nof-the-art models on ECLEKTIC, showing lack\nof knowledge transfer across languages, leaving\nsignificant headroom for further research towards\nmore capable and consistent multilingual models.\n2\nThe Challenge of Cross-lingual\nTransfer Evaluation\nWith the rapid evolution of open-source and pro-\nprietary LLMs, we need robust black-box methods\nfor evaluating and scrutinizing various model ca-\n\n\nPrediction\nDE\nTobias Meister\nTobias Meister (9. Juni 1957 in Köln)\nist vor allem als deutsche Stimme\ndes Schauspielers Brad Pitt bekannt...\nES\nTobias Meister\nQA Generation\nData Creation\nEvaluation\nWer synchronisiert\nBrad Pitt?\nWho dubs Brad Pitt?\n404 - page not found\nDecontextualization\nIn deutschpachigen\nfilmen, wer\nsynchronisiert\nBrad Pitt?\nIn German-speaking\nfilms, who...\nTranslation\nEn las películas de \nhabla alemana, \n¿quién dobla a \nBrad Pitt?\nLLM as a Judge\n41.3\n38.8\n34.4\n7.1\nJuan Perez            No lo sé            Tobias Meister\nFigure 2: A schematic overview of the creation of ECLEKTIC and its application in evaluating language models.\npabilities. In this work, we target the cross-lingual\nknowledge transfer abilities of multilingual LLMs.\nSpecifically, we wish to assess whether consistent\noutputs for the same input in different languages oc-\ncur due to genuine knowledge sharing — stemming\nfrom internal orthogonal treatment of knowledge\nand language — or due to incidental exposure and\nmemorization of the same information in multiple\nlanguages during training.\nTo the best of our knowledge, prior work for\ndirectly assessing parametric knowledge-sharing\nand cross-lingual knowledge-transfer capabilities\nmainly looked at the internal mechanisms of open-\nsource models. One line of work utilizes knowl-\nedge editing to determine whether the model’s rep-\nresentation of knowledge is causally linked across\nlanguages (Ifergan et al., 2024; Wei et al., 2024).\nHowever, editing methods are far from perfect as\nthe field is still evolving. Another approach uses\ndirect observation of neuron activations in order\nto understand the extent to which actual transfer\noccurs (Chen et al., 2024; Zhao et al., 2024a). One\ndownside of this approach is that models’ inner\nstates are not easily interpretable. Yet, more limit-\ning is the fact that all the above methods are white-\nbox inspection approaches that can only be applied\nto open-source models. These methods cannot as-\nsess modern SOTA proprietary models, for which\nonly black-box analysis is possible.\nIn this work, we present ECLEKTIC to black-\nbox evaluate cross-lingual knowledge transfer us-\ning a well-established method of closed-book QA\nto query parametric knowledge (AlKhamissi et al.,\n2022) by carefully selecting questions that would\nindicate genuine knowledge transfer.\n3\nECLEKTIC\nIn this section we introduce the ECLEKTIC bench-\nmark. We detail the construction of the dataset and\nthen describe the evaluation procedure and metrics.\n3.1\nThe ECLEKTIC Dataset Construction\nAs ECLEKTIC is a QA benchmark, we want to\ninclude only questions whose answers were ex-\nposed to the model in a single language during pre-\ntraining. Then, when we query such questions in\nother languages, an LLM could answer them from\nits parametric memory if it has a representation of\nthat knowledge that is language agnostic.\nTo generate such question/answer pairs, we se-\nlected articles in Wikipedia that exist only in one\nof the 12 target languages (see list in Table 1). Con-\ncretely, we analyzed the July 2023 Wikipedia dump\nand for each language sampled 100 articles that\ncontain at least 200 characters, had at least 100\nviews during 2023,2 and most importantly do not\nhave equivalent articles in any of the other 11 lan-\nguages. From each such article we extracted the\n10 first sentences and based on them we instructed\nGemini to generate a question and an answer.\nThe context, the question, and the answer of all\ncandidate generations were validated by human\nannotators. First, the annotators checked that the\nquestion is answerable in a closed book setting, i.e.,\n2Statistics available on:\nhttps://stats.wikimedia.\norg/#/all-wikipedia-projects.\n\n\nSource Language\n# Examples\nENGLISH\n39\nFRENCH\n22\nGERMAN\n16\nHEBREW\n29\nHINDI\n64\nINDONESIAN\n31\nITALIAN\n29\nJAPANESE\n26\nKOREAN\n33\nMANDARIN CHINESE\n35\nPORTUGUESE\n28\nSPANISH\n32\nTOTAL GENERATED\n384\nTOTAL TRANSLATED\n4224\nTOTAL EVALUATED\n4608\nTable 1: Statistical information on the examples in\nECLEKTIC, broken down by source language.\nit does not refer explicitly to the context or mention\nthe answer. Second, they validated that the question\nis related to a fact that is particularly relevant to the\nlanguage in question, e.g., it does not relate to a\nscience or other general knowledge fact. Questions\nand answers that did not meet these criteria were\ndiscarded. Third, the annotators made sure that the\nquestion contains all the information needed to be\nanswerable when translated. For example, a ques-\ntion in Hebrew relating to the TV series \"Survivor\"\nwas disambiguated by the annotators to explicitly\nmention \"the Israeli adaption of Survivor\". Named\nentities were also clarified similarly, so a question\nreferring to \"Ambev\" was modified refer to \"the\nBrazilian brewing company, Ambev\".\nFinally, each retained question, answer and con-\ntext were automatically translated to the other 11\nlanguages. The translations were verified by an-\nother set of human annotators and modified when\nneeded. At this stage some examples were also\ndiscarded if they were proved to be untranslatable.\nFor example, when a question explicitly refers to\nthe meaning of a word in the source language. To\novercome the difficulties in the verification of trans-\nlation between non-trivial language pairs, this stage\nwas done through English as a pivot language.\nThe complete annotation process is depicted in\nFigure 2. All prompts that were used in the data\ncreation are detailed in Appendix A. The statistics\nof the final benchmark are depicted in Table 1.\n3.2\nECLEKTIC Metrics\nTo empirically assess the cross-lingual knowledge-\ntransfer capabilities over ECLEKTIC, we devise\ntwo metrics that accompany the benchmark. The\nfirst is overall success, which measures the extent\nto which a model succeeds in answering correctly\nthe questions in ECLEKTIC, in both source and tar-\nget language. The second is transfer score, which\nmeasures the success in the knowledge transfer\nitself, taking into account only the questions an-\nswered correctly in their source language.\nFormally, for each question/answer pair we de-\nfine example-level success, based on the target lan-\nguage in which they are written lt and on the source\nlanguage from which they were translated ls, as\nSq,a\nM = 1\n\u0000M(qlt) = alt ∧M(qls) = als\n\u0001\nwhere M(ql) is the prediction of model M for ques-\ntion q in language l. Sq,a\nM is a 0/1 score that indi-\ncates whether questions that are expected to be cor-\nrectly answered in language ls are indeed answered\ncorrectly both in this language as well as in another\ntarget language lt, capturing positive knowledge\nsharing between the two languages. Given a set of\nexamples D, the final metric overall simply aver-\nages across all question/answer pairs.\nSoverall\nM\n=\nP\n({q,a}∈D) Sq,a\nM\n|D|\nSoverall is the main metric of ECLEKTIC.\nTo strictly measure the knowledge transfer prob-\nability, we consider only question/answer pairs for\nwhich the model provided the correct answer in the\nsource language: K = {(q, a)|M(qls) = als}. We\nthen define transfer score as the number of ques-\ntions that were answered correctly by the model in\nlt given that they were answered correctly in ls:\nStransfer\nM\n=\nP\n({q,a}∈K) Sq,a\nM\n|K|\nNote that this metric does not explicitly reflect the\nnumber of questions answered correctly in their\nsource language. As a result, it can be maximized\neven by weak models, for which |K| is small, as\nlong as they can answer the same questions in all\nlanguages.\nTo determine whether a model gives the correct\nanswer to a specific question in a specific language\nM(ql)\n?= al we use an LLM as a judge (Zheng\net al., 2023). This is done in order to avoid the\npitfalls of automatic metrics that are less correlated\nwith human judgements (Chen et al., 2019). The\njudge model, in our case Gemini 2.0 Flash, receives\nas input the question and the prediction of the tested\nmodel as well as the translated context in order to\nverify that the predicted answer is correct (Zhou\n\n\nModel\nOverall\nTransfer\nGemini 2.0 Pro\n41.6±1.5\n65.0±1.8\nGPT 4o\n38.8±1.4\n67.0±1.8\nGemini 2.0 Flash\n34.6±1.4\n62.3±1.9\nClaude 3.5 Sonnet\n34.4±1.4\n60.8±1.9\nGemma 2 9B\n8.7±0.8\n40.3±3.1\nMistral Nemo\n7.1±0.8\n38.9±3.4\nQwen 2.5 7B\n2.8±0.5\n23.5±3.7\nOlmo 2 7B\n1.6±0.3\n17.2±3.7\nTable 2: Performance for all proprietary and open mod-\nels over all examples in ECLEKTIC in both metrics.\net al., 2025). The prompt used for judgement is in\nAppendix A.\n3.3\nAssumptions in ECLEKTIC\nThere are several important assumptions we made\nin the construction of ECLEKTIC, which we make\nexplicit here.\nFirst, we assume that all the tested models are ex-\nposed to the articles we extracted from Wikipedia.\nMoreover, we assume that the model was exposed\nto the information in those articles in their respec-\ntive source languages multiple times during its\ntraining and the knowledge is therefore more acces-\nsible to it. In practice, Wikipedia itself is repeated\nin the pre-training data of most LLMs due to the\nquality of its texts (Brown et al., 2020; Chowdhery\net al., 2023), and we assume that it holds also for\nall multilingual models. Additionally, we assume\nthat the existence of a Wikipedia article reflects\na general interest of online speakers in the same\ntopic so the information is also likely to be repeated\nin the same language outside of Wikipedia. This\nis even more straightforward given that we only\ntargeted articles with significant yearly view count.\nConversely, we assume that the absence of an\narticle from a certain Wikipedia reflects the lack\nof interest of online speakers in that topic. The\ninformation on that topic is therefore assumed to\nappear sparsely on the internet, if at all, and be far\nless accessible to the model in that language.\nTogether, these assumptions allow us to treat the\n(in)existence of a fact in Wikipedia as an approx-\nimate to the (in)exposure of a fact in a specific\nlanguage to a multilingual model that was trained\non the entire internet.\n4\nExperiments\n4.1\nLLMs Struggle with Knowledge Transfer\nTo demonstrate ECLEKTIC’s value in cross-\nlingual transfer evaluation, we measured the per-\nformance of several models. We included open\nmodels, namely the latest versions of Gemma,3\nMistral,4 Qwen5, and Olmo,6, all in sizes of be-\ntween 7 and 9 billion parameters that underwent\ninstruction tuning. In addition, the black-box na-\nture of ECLEKTIC allows the evaluation of closed\nmodels as well. We therefore included also GPT\n4o,7 Claude 3.5 Sonnet,8 and Gemini 2.0 in both\nPro9 and Flash10 versions.\nAll models were evaluated in a zero-shot setting\nand were prompted with the question on its own,\ni.e., without an explicit instruction. The results of\ncan be found in Table 2. They show a clear gap in\nperformance between the two groups, with propri-\netary models clearly outperforming the open ones\nby a very wide margin, probably due to their bigger\nsize in terms of parameters (see also Section 4.3).\nGemini 2.0 Pro is the best performing model in\nsolving the task of ECLEKTIC, as defined in terms\nof overall success. It manages to answer correctly,\nin both source and target language, 41.3% of the\nexamples. This score reveals that there is still a\nclear room for improvement in knowledge retrieval\nand transfer capabilities of all models. In terms of\nthe transfer score, that is the portion of questions\nthat were answered correctly in the target language\nout of those that got the right answer in their source\nlanguages, the performance in much better, with\nall closed models achieving more than 60%, but it\nis still far from perfect.\nAll in all, we conclude that ECLEKTIC presents\na serious challenge to modern LLMs despite their\nimpressive abilities overall.\n4.2\nShared Script Eases Transfer\nIn order to provide further insight into the factors\nthat affect knowledge transfer, Figure 3 details a\nbreakdown per language of our results in terms of\n3https://huggingface.co/google/gemma-2-9b-it\n4https://huggingface.co/mistralai/\nMistral-Nemo-Instruct-2407\n5https://huggingface.co/Qwen/Qwen2.\n5-7B-Instruct\n6https://huggingface.co/allenai/\nOLMo-2-1124-7B-Instruct\n7We used version gpt-4o-2024-11-20.\n8Version claude-3-5-sonnet-20241022.\n9Currently an experimental release.\n10Version gemini-2.0-flash-001\n\n\nde\nen\nfr\nes\nit\npt\nid\nhe\nhi\nko\nja\nzh\nSource language\nde\nen\nfr\nes\nit\npt\nid\nhe\nhi\nko\nja\nzh\nTarget language\n100.0%\n6/6\n73.5%\n25/34\n84.6%\n11/13\n72.0%\n18/25\n82.3%\n14/17\n82.3%\n14/17\n72.7%\n16/22\n54.5%\n6/11\n77.8%\n35/45\n55.0%\n11/20\n27.3%\n3/11\n65.2%\n15/23\n70.6%\n66.7%\n4/6\n100.0%\n34/34\n84.6%\n11/13\n76.0%\n19/25\n88.2%\n15/17\n58.8%\n10/17\n77.3%\n17/22\n81.8%\n9/11\n80.0%\n36/45\n55.0%\n11/20\n63.6%\n7/11\n82.6%\n19/23\n76.2%\n66.7%\n4/6\n79.4%\n27/34\n100.0%\n13/13\n72.0%\n18/25\n88.2%\n15/17\n88.2%\n15/17\n59.1%\n13/22\n63.6%\n7/11\n77.8%\n35/45\n60.0%\n12/20\n72.7%\n8/11\n56.5%\n13/23\n73.7%\n100.0%\n6/6\n70.6%\n24/34\n76.9%\n10/13\n100.0%\n25/25\n82.3%\n14/17\n70.6%\n12/17\n77.3%\n17/22\n72.7%\n8/11\n86.7%\n39/45\n70.0%\n14/20\n54.5%\n6/11\n69.6%\n16/23\n77.6%\n83.3%\n5/6\n76.5%\n26/34\n61.5%\n8/13\n64.0%\n16/25\n100.0%\n17/17\n82.3%\n14/17\n72.7%\n16/22\n54.5%\n6/11\n80.0%\n36/45\n55.0%\n11/20\n72.7%\n8/11\n69.6%\n16/23\n72.7%\n100.0%\n6/6\n67.7%\n23/34\n61.5%\n8/13\n72.0%\n18/25\n88.2%\n15/17\n100.0%\n17/17\n72.7%\n16/22\n45.5%\n5/11\n77.8%\n35/45\n70.0%\n14/20\n36.4%\n4/11\n65.2%\n15/23\n71.4%\n100.0%\n6/6\n70.6%\n24/34\n69.2%\n9/13\n68.0%\n17/25\n58.8%\n10/17\n64.7%\n11/17\n100.0%\n22/22\n45.5%\n5/11\n71.1%\n32/45\n60.0%\n12/20\n45.5%\n5/11\n60.9%\n14/23\n67.8%\n66.7%\n4/6\n44.1%\n15/34\n61.5%\n8/13\n56.0%\n14/25\n58.8%\n10/17\n64.7%\n11/17\n54.5%\n12/22\n100.0%\n11/11\n57.8%\n26/45\n65.0%\n13/20\n27.3%\n3/11\n47.8%\n11/23\n58.7%\n83.3%\n5/6\n58.8%\n20/34\n84.6%\n11/13\n40.0%\n10/25\n47.1%\n8/17\n41.2%\n7/17\n59.1%\n13/22\n45.5%\n5/11\n100.0%\n45/45\n60.0%\n12/20\n27.3%\n3/11\n60.9%\n14/23\n59.0%\n66.7%\n4/6\n55.9%\n19/34\n38.5%\n5/13\n36.0%\n9/25\n47.1%\n8/17\n64.7%\n11/17\n50.0%\n11/22\n36.4%\n4/11\n51.1%\n23/45\n100.0%\n20/20\n54.5%\n6/11\n47.8%\n11/23\n54.0%\n66.7%\n4/6\n67.7%\n23/34\n46.2%\n6/13\n56.0%\n14/25\n47.1%\n8/17\n23.5%\n4/17\n59.1%\n13/22\n45.5%\n5/11\n62.2%\n28/45\n75.0%\n15/20\n100.0%\n11/11\n73.9%\n17/23\n60.2%\n66.7%\n4/6\n64.7%\n22/34\n69.2%\n9/13\n68.0%\n17/25\n70.6%\n12/17\n58.8%\n10/17\n50.0%\n11/22\n63.6%\n7/11\n75.6%\n34/45\n80.0%\n16/20\n63.6%\n7/11\n100.0%\n23/23\n69.2%\n80.6%\n69.1%\n69.9%\n65.0%\n71.6%\n66.7%\n67.0%\n59.1%\n74.8%\n67.1%\n53.8%\n66.7%\nFigure 3: Transfer score results of Gemini 2.0 Pro broken down per source and target language. blue is better, red is\nworse. Note the diagonal is perfect by the definition of the transfer metric.\ntransfer score of our best performing model, Gem-\nini 2.0 Pro. This analysis shows that the model’s\nability to transfer knowledge is highly dependent\non the source and target language, with average\nscores ranging from 23.5 (transfer from Portuguese\nto Japanese) to 100.0 (from German to Indonesian).\nIt is also possible to see that transfer is much\nhigher between languages with the same script.\nThis is evident from the high transfer score be-\ntween German, English, French, Spanish, Italian,\nPortuguese, and Indonesian. Note that the latter is\nnot genealogically related to the rest of the Latin-\nwritten languages, yet it performs on par with the\nothers when serving as a source or a target to other\nLatin-written languages. The dependence on script\ncan also be seen in the transfer scores between Chi-\nnese and Japanese, especially when compared to\nthe transfer from Japanese to other languages. This\nfinding aligns with previous works in the litera-\nture on the importance of script to transfer (Malkin\net al., 2022; Mittal et al., 2023; Ifergan et al., 2024).\nAdditionally, this analysis reveals an asymmetry\nin transfer scores depending on the role of the lan-\nGemini 2.0 Flash\nOverall\nTransfer\nClosed-book\n34.5±1.4\n62.3±1.9\nGeneral hint\n35.3±1.4\n64.4±1.9\nSource language name\n41.4±1.5\n70.0±1.8\nSource language title\n47.4±1.4\n75.8±1.6\nCross-lingual open-book\n94.3±0.7\n96.0±0.6\nTable 3:\nPerformance of Gemini 2.0 Flash when\nprompted with hints adding increasing amounts of infor-\nmation, from a general hint to use knowledge in another\nlanguage to a cross-lingual open-book QA.\nguage, as a source or a target. For example, knowl-\nedge seems to be easily transferable from Hindi,\nmostly to Latin-written languages and to Chinese,\nresulting in a macro-averaged transfer score of 78.6.\nBut the transfer to Hindi is much worse, averaging\nin only 59.6 when Hindi is the target language.\nA similar breakdown for the overall success met-\nric can be found in Appendix B.\n4.3\nBigger Isn’t Necessarily Better\nTo further explore the role of model size in the\nability to transfer knowledge, we evaluated all 7\n\n\nFigure 4: Performance of Qwen 2.5 models in various\nsizes in terms of both overall success and transfer score.\nWhile transfer saturates at around 14B parameters, over-\nall success keeps improving with the increasing model\nsize.\nmodel sizes of the Qwen 2.5 series, in sizes of\n0.5, 1.5, 3, 7, 14, 32, and 73 billion parameters.\nWe plotted the transfer and overall scores of these\nmodels in Figure 4.\nThe difference in the curves of the transfer and\nthe overall scores is clearly evident. The perfor-\nmance in terms of overall score continues to im-\nprove with almost every increase in model size, and\nit seems plausible that an even bigger version of\nQwen would have an even better performance. On\nthe other hand, in terms of transfer score, the perfor-\nmance improves rapidly with the increase in model\nsize up until 14B parameters and then somewhat\nsaturates, as more than a 5-fold increase in model\nsize only give an improvement of about 7 percent-\nage points in transfer. Taken together, this leads to\nthe conclusion that the improvement in the over-\nall scores of the bigger model comes mostly from\ntheir ability to retrieve more facts in their source\nlanguage while the proportion of facts transferred\nrises only marginally.\n4.4\nHinting LLMs Into Success Is Not Easy\nIn the evaluation setting we examined until now\n— a zero-shot setting where models are only given\nthe question itself with no instruction — we found\nthat models have a significant room for improve-\nment. In order to characterize the pitfalls that make\nmodels fail and how to avoid them, we conducted\nan ablation study by incorporating more and more\ninformation into the prompt given to the model. We\nexperimented with Gemini 2.0 Flash in the follow-\ning settings:\n• Closed-book. This is the setting used for the\nmain experiment, giving the model only the\nquestion to be answered.\n• General hint. In this setting an instruction\nis given to the model to answer the question\nwhile also instructing it to use its knowledge\nin other languages if it sees fit.\n• Source language name. This setting is very\nsimilar to the previous one but the name of\nlanguage with the relevant knowledge is given\nexplicitly as part of the prompt.\n• Source language title. Here the model is given\nnot only the name of the language but also the\ntitle of the Wikipedia article from which the\nquestion was generated. The title is given in\nits source language.\n• Cross-lingual open-book. In this last setting,\nthe prompt includes the context in its source\nlanguage and the question in the target lan-\nguage, so the model can completely disregard\nits parametric knowledge and it is only re-\nquired to bridge the language differences in\nits input. This experiment is equivalent to that\ndone by Chua et al. (2024).\nThe prompts used in all of these settings are given\nin Appendix A.\nThe results, given in Table 3, show that while\nsimply hinting the model towards cross-lingual\nknowledge is not enough and provides only in-\nsignificant improvement. Revealing the source lan-\nguage and the topic leads to a much improved per-\nformance of about 7 and 12 percentage points, re-\nspectively, indicating that in some cases the knowl-\nedge may be partially available to the model, but it\nrequires some guidance.\nHowever, the most substantial improvement, al-\nmost to the point of solving ECLEKTIC, comes\nwhen we gave the model the correct context, just\nin the source language. The model has no problem\nto reason across languages to produce the correct\nanswer in 94.3% of the examples. This means that\nwhen the knowledge is available in the prompt,\ntransferring it is less of a problem. The limited\nperformance over ECLEKTIC is then more likely\nto arise from the difficulty in retrieving knowledge\ncross-lingually rather than processing it.\n5\nECLEKTIC Popular Pages\nBeyond reliance on Wikipedia article distribution,\nwhen constructing ECLEKTIC we also made a de-\ncision to base the questions and answers on pages\nthat exist in only one language out of the 12 in\nour selection. While providing control over the\n\n\nModel\nPopular Pages\nOverall\nTransfer\nGemini 2.0 Pro\n36.7±1.2\n67.8±1.7\nGPT 4o\n34.5±1.2\n67.3±1.7\nGemini 2.0 Flash\n31.6±1.2\n65.8±1.8\nClaude 3.5 Sonnet\n23.8±1.1\n56.8±2.0\nGemma 2 9B\n6.0±0.6\n33.0±2.9\nMistral Nemo\n4.0±0.5\n31.5±3.4\nQwen 2.5 7B\n1.5±0.3\n18.9±3.6\nOlmo 2 7B\n0.8±0.2\n9.3±2.5\nTable 4: Performance for all proprietary and open mod-\nels over all examples in ECLEKTIC popular pages in\nboth metrics.\nlanguage from which the knowledge has to be\ntransferred, it also means that ECLEKTIC may\ninclude many questions on topics that are some-\nwhat marginal and less consequential to users in\nthe target languages.\nTo examine whether our results are due to lim-\nited exposure of the models to their topics, we\nexperimented with another variant of our dataset,\nnamely ECLEKTIC popular pages. In this version\nwe sampled articles that were popular in terms of\nviews, instead of prioritizing lack of equivalents.\nThus, this version gives emphasis to topics that are\nmore likely to interest average users and appear\nmore frequently outside Wikipedia.\nSpecifically, from each Wikipedia we took the\n200 articles that had the most views during April\n2023 and lacked equivalent in at least one of the\n12 Wikipedias. Then,S through the same pipeline\ndescribed in Section 3.1, we created, translated and\nverified questions and answers based on the con-\ntent of the articles. We ended up with 964 unique\nquestion/answer pairs. However, each example was\nevaluated on a subset of target languages consisting\nonly of languages whose Wikipedias do not include\nan article on the question’s topic, resulting in a total\nof 6,628 examples summed over all languages.11\nWe evaluated the same models of Section 4.1 on\nthis data as well. The results, detailed in Table 4,\nare inline with the results over the main ECLEK-\nTIC dataset in Table 2. The order of models by\nperformance didn’t change, Gemini 2.0 Pro is the\nbest model, followed by GPT 4o, and open 7-9B\nmodels far worse. This experiment indicates on the\nrobustness of our results to the article selection cri-\nterion and on the validity of our assumptions about\nthe link between Wikipedia articles distribution and\n11The data for this variant is also included in https://www.\nkaggle.com/datasets/googleai/eclektic.\nthe necessity of transfer.\n6\nRelated Work and Discussions\n6.1\nTypes of Cross-Lingual Transfer\nThe term cross-lingual transfer has been exten-\nsively used in the NLP literature. The idea of\none language benefiting from resources in another\nthrough a shared representation goes back to Mc-\nDonald et al. (2011) at least. However, this term\nwas used to refer to different specific experimental\nsettings, some of them significantly different than\nthe setting of this work.\nThe first distinction in the literature concerns\nwhat is being transferred, specifically the differ-\nence between cross-lingual skill transfer and knowl-\nedge transfer (Rajaee and Monz, 2024). Cross-\nlingual skill transfer is the ability to generalize a\ngiven skill to unseen languages regardless of the\nlanguage that was used to learn it. E.g. perfect-\ning multilingual summarization when learning to\nsummarize from English data, or excelling at mul-\ntilingual instruction following while only exposed\nto a few languages (Hu et al., 2020; Turc et al.,\n2021; Malkin et al., 2022; Huang et al., 2023; Sha-\nham et al., 2024). On the other hand, cross-lingual\nknowledge transfer is the ability to retrieve factual\nknowledge from one language’s data when queried\nwith any language (Asai et al., 2020; Limkonchoti-\nwat et al., 2022; Mittal et al., 2023; Chua et al.,\n2024; Litschko et al., 2024).\nIn addition, methods for evaluation of cross-\nlingual transfer can also be orthogonally catego-\nrized into two broad approaches: fine-tuning on\nsources, then testing on targets (e.g., Shaham et al.,\n2024; Limkonchotiwat et al., 2022) vs. zero-shot\nevaluation of the targets (e.g., Malkin et al., 2022;\nChua et al., 2024). While the earlier enables higher\ncontrol over the data and experimental setting, the\nlatter is less expensive and reflects how models\nbehave in the wild.\nLastly, when dealing with cross-lingual knowl-\nedge transfer, the knowledge can be either para-\nmetric (e.g., Rajaee and Monz, 2024) or contextual\n(e.g., Chua et al., 2024; Mondshine et al., 2025), a\ndistinction explored by Neeman et al. (2022) in a\nmonolingual setting.\nWithin this taxonomy of cross-lingual transfer\nrelated works, ECLEKTIC clearly belongs to meth-\nods evaluating parametric knowledge transfer in a\nzero-shot setting. In addition, the experiment in\nSection 4.4 gradually transition from parametric\n\n\nto contextual knowledge transfer, with the cross-\nlingual open-book experiment occupying the other\nend of that spectrum.\n6.2\nECLEKTIC and Cross-Lingual\nConsistency\nCross-lingual knowledge transfer is closely related\nto cross-lingual consistency, which has been ex-\nplored extensively in recent literature. Starting with\nmonolingual English settings, early works noticed\nthat LLMs lack a guarantee on consistency due to\nthe statistical nature of their training. That is to\nsay that models may generate contradictory state-\nments when presented with semantically equivalent\ninputs (Kassner and Schütze, 2020; Ravichander\net al., 2020) or may rephrase identical factual in-\nformation in different ways (Elazar et al., 2021).\nIn the context of modern LLMs, there has been\nincreasing attention on ensuring output consistency\nwhen faced with variations in prompts (Mizrahi\net al., 2024; Sclar et al., 2024; Zhao et al., 2024b).\nA variety of methods have been proposed to im-\nprove this consistency, including the augmentation\nand diversification of instructions during training\n(Liang et al., 2023; Zhao et al., 2024b).\nIn multilingual models, the challenge becomes\neven more complex, as it introduces the issue of\ncross-lingual inconsistency, where models fail to\nprovide consistent responses to semantically equiv-\nalent inputs in different languages. To assess cross-\nlingual consistency, earlier works translated En-\nglish datasets, either derived from knowledge bases\n(Kassner et al., 2021; Jiang et al., 2020; Ifergan\net al., 2024) or from NLP tasks (Ohmer et al., 2023),\ninto multiple languages. These studies then mea-\nsured the variance in LLM responses, typically in\nterms of answer overlap or ranking (Qi et al., 2023).\nHowever, when applying translation across-the-\nboard to entire datasets, these consistency-focused\nbenchmarks may include some well-known facts\nthat the model saw and memorize in many lan-\nguages separately and therefore could easily predict\nconsistently (see discussion in Section 2). More-\nover, starting with English-constructed datasets\nmay introduce biases into these benchmarks.\nThe QA task of ECLEKTIC, although created\nwith transfer in mind, may also serve as a better\nbenchmark for cross-lingual consistency. To begin\nwith, ECLEKTIC is not generated in English but in\nall languages of the dataset. But more importantly,\nby targeting specific knowledge that is less known\nin most languages, we present models with a far\ngreater challenge in keeping their answers consis-\ntent. It is therefore possible to view the results over\nECLEKTIC also as a tighter upper bound on the\nconsistency of multilingual models.\n7\nConclusions\nIn this paper we presented ECLEKTIC, a closed-\nbook QA dataset for evaluating the abilities of mod-\nels to transfer knowledge from their parametric\nmemory across languages. This black-box eval-\nuation was made possible by carefully phrasing\nquestions that target topics that are highly visible\nin one language and not in any of the others. Our\nbenchmark allows for simple and reliable transfer\nevaluation which is, for the first time, easily appli-\ncable also to API-fenced models. Our results show\nthat cross-lingual knowledge transfer is a difficult\ntask that is far from being solved, and that ECLEK-\nTIC can be employed to indicate on the progress\nmade towards consistent and inclusive multilingual\nlanguage models.\n8\nLimitations\nTime sensitivity\nWhen constructing ECLEKTIC\nwe relied on the distribution of topics in Wikipedia\nin different languages as reflected in July 2023.\nSince then, and in the future, it is of course possible\nfor the article distribution to change, mostly as new\ntopics become more prominent for the speakers of a\ngiven language, on Wikipedia and in general. This\nmakes ECLEKTIC somewhat time-dependent, so\nin the future it would probably require an update.\nNumber\nof\nlanguages\nAlthough\nthe\ndata\nECLEKTIC covers varied languages, its coverage\nis obviously partial. It covers only 6 out of the 10\nmost spoken languages according to Ethnologue,12\nand 8 out of the 10 most active Wikipedias in terms\nof active users.13 However, due to the translation of\nall examples to all languages, adding significantly\nmore languages may make the evaluation using\nECLEKTIC less cheap and fast.\nThe limited number of languages also make the\nconclusions of Section 4.2 less unequivocal, but\nthe fact the previous works pointed to the same\nconclusions (Malkin et al., 2022; Mittal et al., 2023;\nIfergan et al., 2024) provides some reassurance.\n12https://www.ethnologue.com/statistics/\n13https://en.wikipedia.org/wiki/List_of_\nWikipedias\n\n\nReferences\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona\nDiab, and Marjan Ghazvininejad. 2022. A review on\nlanguage models as knowledge bases. arXiv preprint\narXiv:2204.06031.\nAkari Asai, Jungo Kasai, Jonathan H Clark, Kenton Lee,\nEunsol Choi, and Hannaneh Hajishirzi. 2020. Xor\nqa: Cross-lingual open-retrieval question answering.\narXiv preprint arXiv:2010.11856.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. 2019. Evaluating question answer-\ning evaluation. In Proceedings of the 2nd Workshop\non Machine Reading for Question Answering, pages\n119–124, Hong Kong, China. Association for Com-\nputational Linguistics.\nYuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, and\nJun Zhao. 2024. Journey to the center of the knowl-\nedge neurons: Discoveries of language-independent\nknowledge neurons and degenerate knowledge neu-\nrons. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, volume 38, pages 17817–17825.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607–15631, Toronto,\nCanada. Association for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research, 24(240):1–113.\nLynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Ka-\nmath, Ravi Kumar, Pasin Manurangsi, Amer Sinha,\nChulin Xie, and Chiyuan Zhang. 2024. Crosslingual\ncapabilities and knowledge barriers in multilingual\nlarge language models. In NeurIPS 2024 Workshop\non Compositional Learning: Perspectives, Methods,\nand Paths Forward.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nGemini Team. 2024. Gemini: A family of highly capa-\nble multimodal models. Preprint, arXiv:2312.11805.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generalisa-\ntion. In International Conference on Machine Learn-\ning, pages 4411–4421. PMLR.\nHaoyang Huang, Tianyi Tang, Dongdong Zhang, Xin\nZhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not\nall languages are created equal in LLMs: Improv-\ning multilingual capability by cross-lingual-thought\nprompting. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2023, pages 12365–\n12394, Singapore. Association for Computational\nLinguistics.\nMaxim Ifergan, Leshem Choshen, Roee Aharoni, Idan\nSzpektor, and Omri Abend. 2024.\nBeneath the\nsurface of consistency:\nExploring cross-lingual\nknowledge representation sharing in llms. Preprint,\narXiv:2408.10646.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020. X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5943–5959, On-\nline. Association for Computational Linguistics.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowledge\nin multilingual pretrained language models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 3250–3258, Online.\nAssociation for Computational Linguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\nShihao Liang, Runchu Tian, Kunlun Zhu, Yujia Qin,\nHuadong Wang, Xin Cong, Zhiyuan Liu, Xiaojiang\nLiu, and Maosong Sun. 2023. Exploring format con-\nsistency for instruction tuning. Transactions on Ma-\nchine Learning Research.\nPeerat Limkonchotiwat, Wuttikorn Ponwitayarat, Can\nUdomcharoenchaikit, Ekapol Chuangsuwanich, and\nSarana Nutanong. 2022. Cl-relkt: Cross-lingual lan-\nguage knowledge transfer for multilingual retrieval\nquestion answering. In Findings of the Association\nfor Computational Linguistics: NAACL 2022, pages\n2141–2155.\nRobert Litschko, Oliver Kraus, Verena Blaschke,\nand Barbara Plank. 2024.\nCross-dialect informa-\ntion retrieval: Information access in low-resource\nand high-variance languages.\narXiv preprint\narXiv:2412.12806.\n\n\nLlama Team. 2024.\nThe llama 3 herd of models.\nPreprint, arXiv:2407.21783.\nDan Malkin,\nTomasz Limisiewicz,\nand Gabriel\nStanovsky. 2022. A balanced data approach for eval-\nuating cross-lingual transfer: Mapping the linguistic\nblood bank. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4903–4915, Seattle, United States.\nAssociation for Computational Linguistics.\nRyan McDonald, Slav Petrov, and Keith Hall. 2011.\nMulti-source transfer of delexicalized dependency\nparsers. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Processing,\npages 62–72, Edinburgh, Scotland, UK. Association\nfor Computational Linguistics.\nShubham Mittal, Keshav Kolluru, Soumen Chakrabarti,\net al. 2023. mokb6: A multilingual open knowledge\nbase completion benchmark. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n201–214.\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror,\nDafna Shahaf, and Gabriel Stanovsky. 2024. State\nof what art? a call for multi-prompt LLM evaluation.\nTransactions of the Association for Computational\nLinguistics, 12:933–949.\nItai Mondshine, Tzuf Paz-Argaman, and Reut Tsarfaty.\n2025. Beyond english: The impact of prompt trans-\nlation strategies across languages and tasks in mul-\ntilingual llms. In Findings of the Association for\nComputational Linguistics: NAACL 2025. Associa-\ntion for Computational Linguistics.\nElla Neeman, Roee Aharoni, Or Honovich, Leshem\nChoshen, Idan Szpektor, and Omri Abend. 2022.\nDisentqa: Disentangling parametric and contextual\nknowledge with counterfactual question answering.\narXiv preprint arXiv:2211.05655.\nXenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2023.\nSeparating form and meaning: Using self-consistency\nto quantify task understanding across multiple senses.\nIn Proceedings of the Third Workshop on Natu-\nral Language Generation, Evaluation, and Metrics\n(GEM), pages 258–276, Singapore. Association for\nComputational Linguistics.\nOpenAI. 2024.\nGpt-4 technical report.\nPreprint,\narXiv:2303.08774.\nJirui Qi, Raquel Fernández, and Arianna Bisazza. 2023.\nCross-lingual consistency of factual knowledge in\nmultilingual language models. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10650–10666, Singa-\npore. Association for Computational Linguistics.\nQwen Team. 2025. Qwen2.5 technical report. Preprint,\narXiv:2412.15115.\nSara Rajaee and Christof Monz. 2024.\nAnalyzing\nthe evaluation of cross-lingual knowledge transfer\nin multilingual language models.\narXiv preprint\narXiv:2402.02099.\nAbhilasha Ravichander, Eduard Hovy, Kaheer Suleman,\nAdam Trischler, and Jackie Chi Kit Cheung. 2020.\nOn the systematicity of probing contextualized word\nrepresentations: The case of hypernymy in BERT. In\nProceedings of the Ninth Joint Conference on Lex-\nical and Computational Semantics, pages 88–102,\nBarcelona, Spain (Online). Association for Computa-\ntional Linguistics.\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane\nSuhr. 2024. Quantifying language models’ sensitiv-\nity to spurious features in prompt design or: How i\nlearned to start worrying about prompt formatting.\nIn The Twelfth International Conference on Learning\nRepresentations.\nUri Shaham, Jonathan Herzig, Roee Aharoni, Idan\nSzpektor, Reut Tsarfaty, and Matan Eyal. 2024. Mul-\ntilingual instruction tuning with just a pinch of mul-\ntilinguality. In Findings of the Association for Com-\nputational Linguistics: ACL 2024, pages 2304–2317,\nBangkok, Thailand. Association for Computational\nLinguistics.\nIulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei\nChang, and Kristina Toutanova. 2021. Revisiting the\nprimacy of english in zero-shot cross-lingual transfer.\narXiv preprint arXiv:2106.16171.\nZihao Wei, Jingcheng Deng, Liang Pang, Hanxing Ding,\nHuawei Shen, and Xueqi Cheng. 2024. Mlake: Mul-\ntilingual knowledge editing benchmark for large lan-\nguage models. ArXiv, abs/2404.04990.\nYiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji\nKawaguchi, and Lidong Bing. 2024a. How do large\nlanguage models handle multilingualism? Preprint,\narXiv:2402.18815.\nYukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang\nXing, Shuaiqiang Wang, Chong Meng, Zhicong\nCheng, Zhaochun Ren, and Dawei Yin. 2024b. Im-\nproving the robustness of large language models\nvia consistency alignment. In Proceedings of the\n2024 Joint International Conference on Computa-\ntional Linguistics, Language Resources and Eval-\nuation (LREC-COLING 2024), pages 8931–8941,\nTorino, Italia. ELRA and ICCL.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36:46595–46623.\nJin Peng Zhou, Sébastien M. R. Arnold, Nan Ding,\nKilian Q. Weinberger, Nan Hua, and Fei Sha. 2025.\nGraders should cheat: privileged information en-\nables expert-level automated evaluations. Preprint,\narXiv:2502.10961.\n\n\nA\nPrompts\nA.1\nECLEKTIC Creation\nDuring the creation of ECLEKTIC, LLMs were\nused for question and answer generation and for\nthe translation of the examples from their respec-\ntive source languages to all other target languages.\nAlthough human annotators verified and fixed their\noutputs, we also provide here the prompts that were\nused.\nQuestion and Answer Generation\n**Task:** Formulate a question in {lang_name}\nthat requires a deep understanding of a given\n{lang_name} Wikipedia paragraph.\n**Requirements:**\n* **Context-Specific:** The question must be\nanswerable solely through information presented\nwithin the paragraph, excluding general knowledge\nor common sense.\n* **Self-Contained:** The question should be\ncompletely self-explanatory, providing all neces-\nsary context within its phrasing. Assume the reader\nhas no access to the paragraph when answering the\nquestion.\n* **Single Concrete Factual Detail:**\n- The question should not require multiple an-\nswers or involve listing multiple details.\n- Avoid asking about opinions, interpretations.\n- In you can’t answer the question, prefer to\ngenerate another question.\n- Focus on extracting a specific, concrete, factual\ndetail that the paragraph directly states.\n- Be specific:\n- If you are asking about an entity be clear about\nit – Use full names for example.\n- Mention expected granularity: If you are asking\nabout a date, instead of asking \"when\", ask for a\ndecade, year, month, date etc. If you are asking\nabout a location, instead of asking \"where\", ask for\na country, state, city, street, landmark etc.\n- Avoid asking questions that their answers are\nacronyms.\nEven for non-English examples keep the con-\nvention of using the English words \"Paragraph\",\n\"Response\", \"question\" and \"answer\" for specify-\ning the parts being generated.\nGenerate only the question and answer. No need\nto continue with additional examples.\n**Examples:**\nParagraph: The Great Barrier Reef is the world’s\nlargest coral reef system, composed of over 2,900\nindividual reefs and 900 islands stretching for over\n2,300 kilometers (1,400 mi) over an area of approx-\nimately 344,400 square kilometers (133,000 sq mi).\nThe reef is located in the Coral Sea, off the coast of\nQueensland, Australia. The Great Barrier Reef can\nbe seen from outer space and is the world’s biggest\nsingle structure made by living organisms.\nResponse: Question: Where is the Great Barrier\nReef located? Answer: Coral Sea, off the coast of\nQueensland, Australia\nParagraph: Die Cazoo Snookerweltmeisterschaft\n2023 wurde vom 15. April bis 1. Mai im Cru-\ncible Theatre in Sheffield ausgetragen. Mit ihr\nendete die Saison 2022/23 der World Snooker\nTour.[1] Titelverteidiger Ronnie O’Sullivan scheit-\nerte im Viertelfinale gegen Luca Brecel. Der Bel-\ngier erreichte das Finale und schlug dort den vier-\nfachen Weltmeister Mark Selby mit 18:15. Bre-\ncel ist damit der erste Kontinentaleuropäer, der\nWeltmeister wurde. In diesem Jahr wurden noch\nweitere Bestmarken in Bezug auf die 47-jährige\n„Crucible-Ära“ aufgestellt. Unter anderem über-\ntraf Ronnie O’Sullivan mit seiner 31. Endrunden-\nteilnahme die 30 Teilnahmen von Steve Davis.[2]\nO’Sullivan erzielte auch sein 200. WM-Century-\nBreak. Zweimal wurde ein Maximum Break erzielt,\nwas es 2008 bereits einmal gegeben hatte; das „per-\nfekte Break“ in einem WM-Finale gelang 2023\nerstmals Mark Selby.\nResponse: Question: Gegen wen verlor Ron-\nnie O’Sullivan im Viertelfinale der Snooker-\nWeltmeisterschaft 2023? Answer: Luca Brecel\nParagraph: {context}\nResponse:\nTranslation\nTranslate the provided text from {in_lang} to\n{out_lang} while maintaining the original mean-\ning and intent. Ensure accuracy and preserve the\nentities and concepts expressed in the source text.\nInput in {in_lang}:\n{text} Response in\n{out_lang}:\nA.2\nJudging Prompt\n**Task:** Determine if an answer to the question\nis supported by a given text.\n**Input (in {target_language}):** - Text - Ques-\ntion - Answer\n**Single Word Output (in English):** - YES:\nAnswer is derived from the text. - NO: Answer is\nnot derived from the text.\nText: {context}\nQuestion: {question}\nAnswer: {predicted_answer}\n\n\nde\nen\nfr\nes\nit\npt\nid\nhe\nhi\nko\nja\nzh\nSource language\nde\nen\nfr\nes\nit\npt\nid\nhe\nhi\nko\nja\nzh\nTarget language\n37.5%\n6/16\n64.1%\n25/39\n50.0%\n11/22\n56.2%\n18/32\n48.3%\n14/29\n50.0%\n14/28\n51.6%\n16/31\n20.7%\n6/29\n54.7%\n35/64\n33.3%\n11/33\n11.5%\n3/26\n42.9%\n15/35\n43.4%\n25.0%\n4/16\n87.2%\n34/39\n50.0%\n11/22\n59.4%\n19/32\n51.7%\n15/29\n35.7%\n10/28\n54.8%\n17/31\n31.0%\n9/29\n56.2%\n36/64\n33.3%\n11/33\n26.9%\n7/26\n54.3%\n19/35\n47.1%\n25.0%\n4/16\n69.2%\n27/39\n59.1%\n13/22\n56.2%\n18/32\n51.7%\n15/29\n53.6%\n15/28\n41.9%\n13/31\n24.1%\n7/29\n54.7%\n35/64\n36.4%\n12/33\n30.8%\n8/26\n37.1%\n13/35\n45.0%\n37.5%\n6/16\n61.5%\n24/39\n45.5%\n10/22\n78.1%\n25/32\n48.3%\n14/29\n42.9%\n12/28\n54.8%\n17/31\n27.6%\n8/29\n60.9%\n39/64\n42.4%\n14/33\n23.1%\n6/26\n45.7%\n16/35\n47.4%\n31.2%\n5/16\n66.7%\n26/39\n36.4%\n8/22\n50.0%\n16/32\n58.6%\n17/29\n50.0%\n14/28\n51.6%\n16/31\n20.7%\n6/29\n56.2%\n36/64\n33.3%\n11/33\n30.8%\n8/26\n45.7%\n16/35\n44.3%\n37.5%\n6/16\n59.0%\n23/39\n36.4%\n8/22\n56.2%\n18/32\n51.7%\n15/29\n60.7%\n17/28\n51.6%\n16/31\n17.2%\n5/29\n54.7%\n35/64\n42.4%\n14/33\n15.4%\n4/26\n42.9%\n15/35\n43.8%\n37.5%\n6/16\n61.5%\n24/39\n40.9%\n9/22\n53.1%\n17/32\n34.5%\n10/29\n39.3%\n11/28\n71.0%\n22/31\n17.2%\n5/29\n50.0%\n32/64\n36.4%\n12/33\n19.2%\n5/26\n40.0%\n14/35\n41.7%\n25.0%\n4/16\n38.5%\n15/39\n36.4%\n8/22\n43.8%\n14/32\n34.5%\n10/29\n39.3%\n11/28\n38.7%\n12/31\n37.9%\n11/29\n40.6%\n26/64\n39.4%\n13/33\n11.5%\n3/26\n31.4%\n11/35\n34.8%\n31.2%\n5/16\n51.3%\n20/39\n50.0%\n11/22\n31.2%\n10/32\n27.6%\n8/29\n25.0%\n7/28\n41.9%\n13/31\n17.2%\n5/29\n70.3%\n45/64\n36.4%\n12/33\n11.5%\n3/26\n40.0%\n14/35\n36.1%\n25.0%\n4/16\n48.7%\n19/39\n22.7%\n5/22\n28.1%\n9/32\n27.6%\n8/29\n39.3%\n11/28\n35.5%\n11/31\n13.8%\n4/29\n35.9%\n23/64\n60.6%\n20/33\n23.1%\n6/26\n31.4%\n11/35\n32.6%\n25.0%\n4/16\n59.0%\n23/39\n27.3%\n6/22\n43.8%\n14/32\n27.6%\n8/29\n14.3%\n4/28\n41.9%\n13/31\n17.2%\n5/29\n43.8%\n28/64\n45.5%\n15/33\n42.3%\n11/26\n48.6%\n17/35\n36.3%\n25.0%\n4/16\n56.4%\n22/39\n40.9%\n9/22\n53.1%\n17/32\n41.4%\n12/29\n35.7%\n10/28\n35.5%\n11/31\n24.1%\n7/29\n53.1%\n34/64\n48.5%\n16/33\n26.9%\n7/26\n65.7%\n23/35\n42.2%\n30.2%\n60.3%\n41.3%\n50.8%\n41.9%\n40.5%\n47.6%\n22.4%\n52.6%\n40.7%\n22.8%\n43.8%\nFigure 5: Overall success results of Gemini 2.0 Pro broken down per source and target language. blue is better, red\nis worse.\nOutput:\nA.3\nPrompts for Ablation in Section 4.4\nClosed-book\n{question}\nGeneral hint\nAnswer the following question based on your\nknowledge in another language.\n{question}\nSource language name\nAnswer the following question based on your\nknowledge in {in_lang}.\n{question}\nSource language title\nAnswer the following question based on your\nknowledge in {in_lang} about {original_title}.\n{question}\nCross-lingual open-book\nContext: {original_context}\n{question}\nB\nPer-Language Breakdown - Overall\nSuccess\nFigure 5 contains per-language breakdown of the\nresults in terms of overall success of the best per-\nforming model, Gemini 2.0 Pro, similar to the trans-\nfer score breakdown done in Section 4.2. It shows\nthat when taking into account the ability to answer\nquestions in their source language, some languages\nlead to worse performance when used as a source.\nFor example, the low scores on the diagonal for\nGerman, Hebrew and Japanese lead to worse scores\non the entire respective columns.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21228v2.pdf",
    "total_pages": 13,
    "title": "ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer",
    "authors": [
      "Omer Goldman",
      "Uri Shaham",
      "Dan Malkin",
      "Sivan Eiger",
      "Avinatan Hassidim",
      "Yossi Matias",
      "Joshua Maynez",
      "Adi Mayrav Gilady",
      "Jason Riesa",
      "Shruti Rijhwani",
      "Laura Rimell",
      "Idan Szpektor",
      "Reut Tsarfaty",
      "Matan Eyal"
    ],
    "abstract": "To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}