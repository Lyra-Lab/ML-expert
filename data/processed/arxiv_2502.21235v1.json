{
  "id": "arxiv_2502.21235v1",
  "text": "A new block covariance regression model\nand inferential framework for massively\nlarge neuroimaging data\nHyoshin Kim1, Sujit K. Ghosh1, and Emily C. Hector1\n1Department of Statistics, North Carolina State University\nAbstract\nSome evidence suggests that people with autism spectrum disorder exhibit pat-\nterns of brain functional dysconnectivity relative to their typically developing\npeers, but specific findings have yet to be replicated. To facilitate this replication\ngoal with data from the Autism Brain Imaging Data Exchange (ABIDE), we pro-\npose a flexible and interpretable model for participant-specific voxel-level brain\nfunctional connectivity.\nOur approach efficiently handles massive participant-\nspecific whole brain voxel-level connectivity data that exceed one trillion data\npoints. The key component of the model is to leverage the block structure induced\nby defined regions of interest to introduce parsimony in the high-dimensional\nconnectivity matrix through a block covariance structure. Associations between\nbrain functional connectivity and participant characteristics – including eye status\nduring the resting scan, sex, age, and their interactions – are estimated within a\nBayesian framework. A spike-and-slab prior facilitates hypothesis testing to iden-\ntify voxels associated with autism diagnosis. Simulation studies are conducted to\nevaluate the empirical performance of the proposed model and estimation frame-\nwork.\nIn ABIDE, the method replicates key findings from the literature and\nsuggests new associations for investigation.\n1\nIntroduction\nAlthough Autism Spectrum Disorder (ASD) is frequently described as a disorder of\ndysconnection, there remains no consensus on which specific brain regions exhibit hyper-\nor hypo-connectivity. Additionally, ASD is highly heterogeneous, meaning that partic-\nipants with the disorder vary across several dimensions in their clinical presentation,\n1\narXiv:2502.21235v1  [stat.ME]  28 Feb 2025\n\n\nincluding behavioral deficits, intellectual functioning, sex, executive functioning, and\ndevelopmental histories (Lord et al., 2018). Much of the current research seeks to fur-\nther understand the functional relationships between different brain areas and their\nconnection to clinical phenotypes in ASD.\nResting-state functional MRI (rfMRI) data from the Autism Brain Imaging Data\nExchange (ABIDE) repository (Di Martino et al., 2014) represent a unique opportunity\nto replicate existing findings and build consensus among existing results due to their\nsize and scope. The data we consider are preprocessed using the Configurable Pipeline\nfor the Analysis of Connectomes (CPAC) (Craddock et al., 2013) with a preprocessing\nstrategy that includes band-pass filtering and global signal regression, and registered to\na common template. We select the hierarchical multiresolution 17-network parcellation\n(Schaefer et al., 2018), which consists of 200 regions of interest (ROIs) and is widely\nused in functional connectivity research (Hansen et al., 2022; Wang et al., 2024), includ-\ning studies of ASD. This parcellation is well-suited for capturing large-scale functional\nnetworks while maintaining a suitable spatial resolution of ROIs. Based on this scheme,\nfor each participant i ∈{1, . . . , n}, the Mi ≈42,750 voxels are grouped into J = 200\nROIs, which are themselves organized into 17 brain networks. We consider lag-2 thinned\nrfMRI time points of length Ti in each voxel, where the thinning process yields observa-\ntions that are approximately independent for each participant. The participant-specific\nfunctional connectivity matrix is computed by calculating the sample correlation be-\ntween the Mi voxels, treating the Ti time points as replicates. In whole-brain analyses,\nthis Mi × Mi connectivity matrix, which corresponds to the between-voxel sample cor-\nrelation matrix, is used to explore patterns of co-activation and reciprocal activation\nbetween voxels. The resulting voxel-level dataset consists of Pn\ni=1 M 2\ni between-voxel\ncorrelations, exceeding one trillion entries. This extensive dataset facilitates an unparal-\nleled, high-resolution exploration of brain connectivity, but frequently only summaries\nare used due to the prohibitive computational and memory needs of analyzing such\nmassive and complex data.\nFigure 1 displays the voxel-level connectivity matrix for the first five ROIs of a\nrandomly selected participant in ABIDE, highlighting several key characteristics and\nchallenges associated with handling these data.\nFirst, Figure 1 shows that connec-\ntivity values consist of a mix of positive and negative correlations, which complicates\nthe modeling process. Flexibly capturing both negative and positive correlations while\nbuilding a data generating model and maintaining the positive-definiteness of the co-\nvariance matrix is challenging. Common positive-definite covariance structures, such\nas Compound Symmetry, AR(1), and Mat´ern, effectively handle positive correlations\nbut have limited capacity for modeling negative correlations.\nAdditionally, the ob-\nserved connectivity matrix is not particularly sparse, making many known techniques\n2\n\n\nFigure 1: ROI-level functional connectivity matrix (left) and voxel-level functional\nconnectivity matrix for five ROIs (right) for one participant. The first five ROIs are\nshown based on the Schaefer et al. (2018) hierarchical multi-resolution parcellation with\n17 networks and 200 ROIs. The matrix dimensions are 867× 867, corresponding to the\nfollowing ROI sizes: 259, 129, 216, 178, and 85 voxels. Black horizontal and vertical\nlines indicate boundaries between ROIs.\nfor sparse covariance modeling unsuitable in this case (Mohammadi and Wit, 2015;\nKhondker et al., 2013; Samanta et al., 2022).\nSecond, we observe clear patterns of connectivity in the marginal functional con-\nnectivity matrix that is lost when examining the conditional functional connectivity\nmatrix (see Supplementary Material). This agrees with existing studies, which have\nshown that marginal functional connectivity tends to improve diagnostic performance\ncompared to conditional connectivity (Ronicko et al., 2020; Haghighat et al., 2022).\nThird, and most importantly, the voxel-level connectivity matrix shows a clear block\nstructure corresponding to the ROIs. This block structure arises from the hierarchical\nnature of brain parcellations, which nests voxels within a ROI, and ROIs within a\nnetwork. Due to this structure, it is common in the literature to “coarsen” the outcomes\nby averaging the time series across voxels in each ROI at each time point, and to base\nthe subsequent analysis on the ROI-level connectivity matrix. However, this method\nresults in a substantial loss of information, as it fails to consider important details such\nas the size of each ROI and the variability among voxels in each ROI. Our replication\nefforts focus on making use of these rich high-resolution voxel-level data to jointly model\nparticipant-specific brain functional connectivity.\nBuilding on these observations, our aim is to develop a flexible model for the high-\ndimensional covariance matrix that can handle both positive and negative correlations,\nmaintain positive-definiteness, and account for the matrix’s inherent block structure.\n3\n\n\nAn important goal is to ensure that the model remains interpretable, allowing us to\nquantify how participant-specific covariates such as autism status influence the func-\ntional connectivity between voxels. To handle these requirements, we propose a block\nstructure to model the high-dimensional covariance matrix. This approach has recently\ngained attention (see, e.g., Engle and Kelly, 2012; Archakov and Hansen, 2022; Yang\net al., 2024, 2023) due to the widespread observation of organized block patterns in co-\nvariance matrices across various biomedical and financial data types. To the best of our\nknowledge, block structures have not yet been applied to extremely high-dimensional\nsettings such as ours. Our block covariance structure specifies a parametric covariance\nmodel where each block shares the same covariance parameters, while allowing different\nparameters across blocks. This significantly reduces the number of parameters to be\nestimated while preserving the block structure. Among the various parameterization\nmethods available for block structures, we adopt the canonical representation of block\nmatrices proposed by Archakov and Hansen (2022), which is particularly advantageous\nfor efficient computation of key matrix quantities, such as the determinant and inverse.\nThe above model, however, does not permit heterogeneity between participants due\nto participant-level characteristics. We further develop a block covariance model that\nincorporates the effect of participant-specific covariates xi ∈Rp. To achieve this, we\ncarefully construct a covariate-dependent formulation for the Cholesky factors of the\nblock covariance structure. This provides substantial flexibility by assigning different\nregression coefficients to each covariance block while ensuring the positive-definiteness\nof the covariance matrix. Due to their positive definiteness constraints, models that\nintegrate covariates into the covariance matrix, otherwise referred to as covariance\nregression models, often take forms that can be difficult to interpret. For example, Chiu\net al. (1996); Zhao et al. (2021) model the elements of the logarithm of the covariance\nmatrix as linear functions of covariates. Zou et al. (2017) link the covariance matrix to\na linear combination of similarity or distance matrices of covariates. Muschinski et al.\n(2022) explore distributional regression, linking parameters of the covariance matrix,\nderived through the modified Cholesky decomposition, to covariates. Models by Hoff\nand Niu (2012); Fox and Dunson (2015) result in a quadratic function of the covariates.\nThis complexity often makes interpreting regression coefficients in covariance regression\nchallenging.\nWhile our model similarly produces a quadratic relationship between covariance\nand covariates, we show that our covariance model can be intuitively interpreted to\nestimate covariate effects on the dynamic correlation. Dynamic correlation, a concept\nused in genomic analyses, explores how the correlation structure evolves in response to\nvariations in covariates. Unlike traditional methods that focus on linear or nonlinear\nfirst-order relationships, dynamic correlation emphasizes second-order patterns. For\n4\n\n\ninstance, Li (2002) introduced a statistical measure to quantify dynamic correlations\nbetween two genes modulated by a third gene. Building on this concept, subsequent\nstudies have used dynamic correlation to model microarray expression data (Yu, 2018;\nYang and Ho, 2022).\nFinally, we construct a hypothesis testing framework to quantify the strength of\nassociation between ASD status and functional connectivity. Practically, we achieve\nthis by employing a Bayesian hierarchical model that incorporates a continuous spike-\nand-slab prior (George and McCulloch, 1993) for the ASD effect, which is expected\nto be sparse, alongside a continuous prior for other parameters of interest.\nUsing\nMarkov Chain Monte Carlo (MCMC) sampling, we calculate 95% posterior credible\nintervals that quantify evidence of the influence of ASD on dynamic correlation. Given\nthe Bayesian framework and the incorporation of covariates within a block covariance\nstructure, we term our methodology the block covariance regression (BloCR) model.\nAs demonstrated in Section 3, all model parameters can be efficiently sampled using a\nGibbs sampler that depends only on low-dimensional summary statistics of the data,\nsubstantially improving memory usage and computational efficiency.\nThe remainder of this paper is organized as follows. Section 2 introduces the BlocR\nmodel and explains how it enables an intuitive interpretation of covariate effects on\ndynamic correlation. Section 3 details the prior specifications and describes the Gibbs\nsampling procedure for efficient parameter estimation. Section 4 presents simulation\nstudies evaluating the model’s performance across different sparsity levels and data\ndimensions. Section 5 investigates ASD-related dysconnections in ABIDE using our\nproposed BlocR model. Section 6 concludes.\n2\nBlock covariance regression (BlocR) model\n2.1\nCanonical representation of block covariance matrices\nIn our analysis of ABIDE, let i = 1, . . . , n represent the number of participants, and\nt = 1, . . . , Ti represent the number of (thinned) time points for each participant i. The\ntotal number of rfMRI time points across all participants is given by N = Pn\ni=1 Ti. Let\nMi denote the number of voxels for participant i in the voxel-level rfMRI data, with\nMi ≈42,750 in ABIDE. The Mi-dimensional vector of rfMRI outcomes for participant\ni at time t is denoted by yit, where the outcomes have been centered and scaled. We\nmodel the outcomes as independently following Mi-variate Gaussian distributions given\nby yit ∼NMi(0, Σi). This assumes that the voxel-level covariance matrix Σi ∈RMi×Mi\nis specific to each participant.\nA key feature of our modeling approach is the use of a block covariance structure\n5\n\n\nfor Σi. This assumes a predetermined block structure within the covariance matrix,\ndefining a parametric model where all elements within a block share the same covari-\nance parameters, while allowing these parameters to vary across blocks. In ABIDE,\nthe hierarchical organization of brain parcellations naturally induces this block struc-\nture. Let J denote the number of blocks, and let the predetermined block partition be\nrepresented by di = (di1, . . . , diJ), where dij represents the size of the j-th block for\nj = 1, . . . , J of the i-th participant, satisfying Mi = PJ\nj=1 dij. Since we use the same\nbrain parcellation for all participants, the number of blocks J remains constant across\nindividuals. The number of voxels per participant may vary, leading to differences in\nblock sizes across participants. Each covariance matrix Σi can then be expressed in\nblock form as follows:\nΣi =\n\n\n\n\n\n\nΣi11\nΣi12\n. . .\nΣi1J\nΣi21\nΣi22\n. . .\nΣi2J\n...\n...\n...\n...\nΣiJ1\nΣiJ2\n. . .\nΣiJJ\n\n\n\n\n\n\n.\n(1)\nIn equation (1), the (j, ℓ)-th block is denoted as Σijℓ∈Rdij×diℓ. Define 1dij×diℓ∈Rdij×diℓ\nthe matrix of ones, 0dij×diℓ∈Rdij×diℓthe matrix of zeros and Idij ∈Rdij×dij the identity\nmatrix. To parameterize the block structure, we adopt the canonical representation\nproposed by Archakov and Hansen (2022). Under this framework, each (j, ℓ)-th block\nof Σi is defined as\nΣijℓ= δijℓP ijℓ+ 1(j = ℓ)ηijP ⊥\nijj,\n(2)\nwhere δijℓ∈R and ηij ∈R are block-specific parameters. The matrix P ijℓ∈Rdij×diℓ\nis defined with all elements equal to 1/\np\ndijdiℓ, i.e., P ijℓ= 1/\np\ndijdiℓ1dij×diℓ.\nFor\ndiagonal blocks, the matrix P ⊥\nijj ∈Rdij×dij is the complement of the projection matrix\nP ijj, defined as P ⊥\nijj = Idij −1/dij1dij×dij. This formulation allows diagonal blocks to\nhave different diagonal and off-diagonal elements, while all elements in an off-diagonal\nblock are identical.\nLet νdij = 1/\np\ndij1dij×1 denote a scaled vector of ones, and let νdij⊥∈Rdij×(dij−1)\ndenote an orthonormal matrix orthogonal to νdij. For example, νdij⊥can be constructed\nfrom the standard Helmert matrix (Lancaster, 1965) of order dij, excluding its first row.\nUsing these definitions, the matrices in equation (2) can be expressed as P ijℓ= νdijν⊤\ndiℓ\nand P ⊥\nijj = νdij⊥ν⊤\ndij⊥. With these components, we construct the block matrices ˜νi ∈\n6\n\n\nRMi×J and ˜νi⊥∈RMi×(Mi−J) as follows:\n˜νi =\n\n\n\n\n\n\nνdi1\n0di1×1\n. . .\n0di1×1\n0di2×1\nνdi2\n. . .\n0di2×1\n...\n...\n...\n...\n0diJ×1\n0diJ×1\n. . .\nνdiJ\n\n\n\n\n\n\n,\n˜νi⊥=\n\n\n\n\n\n\nνdi1⊥\n0di1×(di2−1)\n. . .\n0di1×(diJ−1)\n0di2×(di1−1)\nνdi2⊥\n. . .\n0di2×(diJ−1)\n...\n...\n...\n...\n0diJ×(di1−1)\n0diJ×(di2−1)\n. . .\nνdiJ⊥\n\n\n\n\n\n\n.\nUsing ˜νi and ˜νi⊥, we define the orthonormal matrix Qi = (˜νi ˜νi⊥) ∈RMi×Mi. With\nstraightforward calculations, the covariance matrix Σi can then be written in its canon-\nical form,\nΣi = QiDiQ⊤\ni where Di = Block-diag(∆i, ηi1Idi1−1, . . . , ηiJIdiJ−1),\n(3)\nand ∆i = (δijℓ)J\njℓ=1 ∈RJ×J.\nHere, Qi serves to rotate Σi into its canonical form\nDi. This representation forms the basis of equation (2), connecting it to the canonical\nstructure of Σi. Equation (3) establishes that Σi is positive definite if and only if all\nηij > 0 and ∆i is positive definite. This insight is particularly valuable as it simplifies\nthe task of constructing a structured, positive-definite high-dimensional covariance ma-\ntrix Σi. The parameterization is effectively reduced to two key components: ∆i and\nηi = (ηi1, . . . , ηiJ) ∈RJ, for i = 1, . . . , n.\n2.2\nCovariate dependent modified Cholesky decomposition\nTo guarantee that Σi is positive definite, it is necessary and sufficient that ηij > 0 for\nall j and that ∆i is positive definite. The first condition is straightforward to satisfy.\nNotably, the term ηij > 0 influences the diagonal elements of the diagonal blocks Σijj,\nand can thus be interpreted as participant-specific, block-wise error terms.\nTo ensure that ∆i is positive definite, we parameterize it using the modified Cholesky\ndecomposition, a method known for its mathematical and computational simplicity.\nThis approach offers an unconstrained parameterization that guarantees positive defi-\nniteness without imposing complex constraints. The utility of the modified Cholesky de-\ncomposition has been highlighted in prior work on covariance matrix modeling (Muschin-\nski et al., 2022; Pourahmadi, 1999). Our application is distinct in that we apply the\ndecomposition specifically to ∆i, rather than to the entire covariance matrix Σi. A\nsymmetric matrix ∆i is positive definite if and only if there exists a unique unit lower\ntriangular matrix Li and a unique diagonal matrix Λi with positive diagonal entries\nsuch that ∆i = LiΛiL⊤\ni . In this formulation, Li is straightforward to model, as its lower\noff-diagonal entries can take any real value without constraint. Positive definiteness is\nensured by requiring that the diagonal entries of Λi, denoted as λi = (λi1, . . . , λij) ∈RJ,\nsatisfy λij > 0 for all j = 1, . . . , J.\n7\n\n\nOur primary objective is to examine the effect of participant-specific covariates on\nfunctional connectivity between voxels. To achieve this, we propose introducing a linear\nmodel of covariates xi = (xi1, . . . , xip) ∈Rp to each element of the Cholesky factors.\nSpecifically, we define the entries of the unit lower triangular matrix Li as follows:\nLiℓℓ= 1;\nLijℓ= 0 for ℓ> j;\nLijℓ= x⊤\ni βjℓfor ℓ< j, j = 2, . . . , J,\nwhere the lower triangular off-diagonal elements Lijℓare modeled as linear functions\nof the covariates, allowing full flexibility by assigning distinct regression coefficients\nβjℓ= (β1jℓ, . . . , βpjℓ) ∈Rp to each Lijℓ. This setup leads to the following expression for\nthe entries of ∆i = (δijℓ)J\njℓ=1, and consequently for Σijℓ:\nδijℓ=\nℓ\nX\nℓ′=1\nλi,ℓ′Lijℓ′Liℓℓ′ =\nℓ\nX\nℓ′=1\nλiℓ′x⊤\ni βjℓ′β⊤\nℓℓ′xi.\n(4)\nSubstituting equation (4) into equation (2), the covariance block Σijℓis expressed as:\nΣijℓ=\n1\np\ndijdiℓ\n \nℓ\nX\nℓ′=1\nλiℓ′x⊤\ni βjℓ′β⊤\nℓℓ′xi\n!\n1dij×diℓ+ 1(j = ℓ)ηijP ⊥\nijj.\n(5)\nThis formulation shows that the covariance block is a quadratic function of the covari-\nates, scaled by the Cholesky diagonal entries λiℓ′ and the block sizes dij and diℓ. The\nflexibility of Σijℓstems from the assignment of distinct regression coefficients βjℓto\neach off-diagonal element Lijℓ, allowing the covariance structure to adaptively capture\nthe effects of covariates for each participant and each block.\n2.3\nInterpreting covariate effects via second-order patterns\nThe representation of the covariance block Σijℓin equation (5), and consequently the\nentire covariance matrix, appears fairly complicated, a common characteristic of co-\nvariance regression models. These models must satisfy positive definiteness constraints,\noften resulting in intricate forms that are challenging to interpret. This issue has been\nnoted in various modeling strategies in the literature (Chiu et al., 1996; Zhao et al.,\n2021; Zou et al., 2017; Muschinski et al., 2022; Hoff and Niu, 2012; Fox and Dunson,\n2015) and applies to our model as well. The inherent complexity of the model can\nobscure the interpretation of regression coefficients in covariance regression models. In\nour approach, however, we demonstrate that the seemingly complex structure of the\ncovariance matrix provides a clear and interpretable explanation of the covariate effects.\nRather than directly interpreting the quadratic first-order patterns, we focus on\n8\n\n\nsecond-order patterns. This approach aligns with the interpretation of dynamic corre-\nlation (Li, 2002; Yu, 2018; Yang and Ho, 2022), a framework used in genomic analyses\nto explore how correlation structures between genetic markers evolve in response to\nvariations in covariates. In our analysis, we investigate how the functional connectiv-\nity changes in response to variations in participant-specific clinical characteristics. In\nABIDE, these clinical variables include ASD diagnostic group, age at the time of the\nscan, sex, group-sex interaction, and eye status during the resting scan. Additional de-\ntails about these clinical variables and the rationale behind their selection are provided\nin Section 5.\nTo quantify this relationship, we take the partial derivative of the covariance block\nΣijℓwith respect to the qth covariate xiq:\n∂Σijℓ\n∂xiq\n=\n(\n1\np\ndijdiℓ\nℓ\nX\nℓ′=1\nλiℓ′\n\u0010\nβqjℓ′β⊤\nℓℓ′xi + βpℓℓ′β⊤\njℓ′xi\n\u0011)\n1dj×dℓ.\n(6)\nThe partial derivative ∂Σijℓ/∂xiq in equation (6) measures the sensitivity of covariance\nbetween voxels in the (j, ℓ)-th ROI to changes in the q-th participant-specific covariate\nxiq. When the covariate is binary, a similar expression can be derived using a finite\ndifference approach. The corresponding formulation is provided in Section 5 for the\nanalysis of ABIDE, where the covariate of interest (ASD diagnostic group) is binary.\nLarge absolute values of these derivatives indicate strong evidence of dynamic cor-\nrelation, meaning that the covariance between voxels in the (j, ℓ)-th ROI are highly in-\nfluenced by changes in xiq. Conversely, values close to zero suggest weak or no dynamic\ncorrelation, indicating minimal influence of changes in xiq on the covariance between\nvoxels in the ROI. This analysis corresponds to a hypothesis test of the following form:\n• H0: Changes in xiq have no association with the values in the covariance block Σijℓ\nas described by the model.\n• H1: Changes in xiq are associated with the values in the covarince block Σijℓas\ndescribed by the model.\nTo conduct this hypothesis test, we employ a Bayesian hierarchical modeling approach.\nUsing Markov chain Monte Carlo (MCMC) samples from the posterior distributions of\nparameters of interest, we calculate 95% posterior credible intervals for equation (6).\nIf the credible interval does not include zero, this provides strong evidence of dynamic\ncorrelation, leading to the rejection of the null hypothesis. To ensure the model can\neffectively capture scenarios where ∂Σijℓ/∂xiq = 0, we employ a continuous spike-and-\nslab prior (George and McCulloch, 1993) on the regression coefficients where sparsity is\nexpected, alongside a continuous prior for other parameters. This approach enhances\n9\n\n\nthe model’s ability to discern cases of no dynamic correlation while maintaining its\nflexibility to detect strong evidence when present.\n3\nPrior specifications and Gibbs sampler\nGiven the complexity of our BlocR model and the scale of the data, an efficient compu-\ntational approach is essential. This section describes the prior distributions of the model\nparameters and demonstrates how conditional posterior distributions can be efficiently\nsampled using the Gibbs sampler. A key component of this approach is leveraging\nalgebraic simplifications to construct a Gibbs sampling scheme that depends only on\nlow-dimensional summaries of the data.\nWe first present the complete hierarchical\nmodel formulation for each participant i = 1, . . . , n:\nηij ∼Inv-Gamma(a0, b0) for j = 1, . . . , J,\nλij ∼Inv-Gamma(a1, b1) for j = 1, . . . , J,\nπjℓ∼Ber(q1) for ℓ< j, j = 2, . . . , J,\nβpjℓ| πjℓ∼(1 −πjℓ)N(0, τ 2\n0 ) + πjℓN(0, τ 2\n1 ) for ℓ< j, j = 2, . . . , J,\nβqjℓ∼N(0, τ 2\n2 ) for q = 1, . . . , p −1 and ℓ< j, j = 2, . . . , J,\nLijℓ= x⊤\ni βjℓfor ℓ< j, j = 2, . . . , J,\n∆i = LiΛiL⊤\ni ,\nDi = Block-diag(∆i, ηi1Id1−1, . . . , ηijIdJ−1),\nΣi = QDiQ⊤,\nyit | ηi, λi, β ∼NMi(0, Σi) for t = 1, . . . , Ti.\n(7)\n3.1\nSimplified likelihood formulation for Gibbs sampling\nFor each Mi-variate outcome yit ∈RMi, we define Y i = (yi1, . . . , yiTi)⊤∈RTi×Mi.\nThe sample covariance for the i-th participant is given by Si = 1/Ti\nPTi\nt=1 yity⊤\nit =\nY ⊤\ni Y i/Ti ∈RMi×Mi. We define Ai = ˜ν⊤\ni Si˜νi ∈RJ×J, where its entries, (aijℓ)J\nj,ℓ=1, are\ngiven by aijℓ= ν⊤\nijSijℓνiℓ= 1/\np\ndijdiℓ1⊤\ndijSijℓ1diℓ. Here, Sijℓ∈Rdij×diℓrepresents the\n(j, ℓ)-th block of Si, and Ai,1:j,1:j ∈Rj×j represents the upper-left submatrix of Ai,\ncontaining the first j rows and columns.\nWe introduce the unit upper triangular matrix U i = (L−1\ni )⊤∈RJ×J. Since the\ninverse of a lower triangular matrix is also lower triangular, U i follows naturally from\nthis property. The vector U i,1:j,j = (Ui1j, Ui2j, . . . , Uijj) ∈Rj consists of the first j\nentries of the j-th column of U i, with Uijj = 1 by construction.\nWe first present our main result, with a proof given below.\n10\n\n\nProposition 1 (Simplified likelihood function). The likelihood corresponding to the\nmodel specified in (7) simplifies to\nn\nY\ni=1\np(Y i | ηi, λi, β) ∝\nn\nn\nY\ni=1\nJ\nY\nj=1\nη\n−\nTi(dij−1)\n2\nij\no\nexp\nh\n−1\n2\nn\nX\ni=1\nJ\nX\nj=1\nTiη−1\nij\nn\ntr(Sijj) −1\ndij\n1⊤\ndijSijj1dij\noi\n\u0010\nn\nY\ni=1\nJ\nY\nj=1\nλ\n−Ti\n2\nij\n\u0011\nexp\n\u0010\n−1\n2\nn\nX\ni=1\nJ\nX\nj=1\nTiλ−1\nij U ⊤\ni,1:j,jAi,1:j,1:jU i,1:j,j\n\u0011\n.\n(8)\nA key advantage of this expression is that all i = 1, . . . , n and j = 1, . . . , J com-\nponents can be decomposed into products, leading to a computationally efficient form\nused throughout the derivation of conditional posterior distributions.\nProposition 1 allows for substantial computational and privacy advantages. The\nmodel structure allows the simplified likelihood in equation (8) to be computed with-\nout requiring participant-level outcomes. Instead, it relies on participant-level sum-\nmary statistics derived from sample covariance matrices: tr(Sijj), 1⊤\ndijSijj1dij, Ai,1:j,1:j\nfor j = 1, . . . , J. These summary statistics need only be calculated once before be-\nginning MCMC sampling, providing huge computational and memory gains. This ap-\nproach ensures computational feasibility for the massive ABIDE data, which consist\nover one trillion data points. Additionally, in other applications that may require ac-\ncess to sensitive data, our formulation guarantees data privacy by avoiding direct use\nof participant-level observations, relying instead on aggregated summary measures.\nFurther, Proposition 1 suggests a natural estimator for ∆i. Indeed, by construction,\nthe matrix Ai serves as a reasonable estimate of ∆i.\nThis property will be useful\nin Sections 4 and 5, where we evaluate the accuracy of our model by comparing ∆i\ncalculated from maximum a posteriori (MAP) estimates of the parameters with the\nobserved Ai.\nThe derivations supporting these key results are presented in the following proof.\nProof. The likelihood for the i-th participant is expressed as:\np(Y i | ηi, λi, β) ∝det(Σi)−Ti/2 exp\nn\n−Ti\n2 tr(SiΣ−1\ni )\no\n.\n(9)\nTo simplify this expression, we examine the participant components of equation (9).\nUsing the canonical representation of Σi from equation (3) and noting that Qi is an\n11\n\n\northonormal matrix, the determinant in equation (9) can be rewritten as:\ndet(Σi) = det(QiDiQ⊤\ni ) = det(Di) =\nJ\nY\nj=1\nλijη\ndij−1\nij\n.\n(10)\nSimilarly, the precision matrix Σ−1\ni\ncan also be derived using the canonical representa-\ntion of Σi:\nΣ−1\ni\n= QiD−1\ni Q⊤\ni = ˜νi∆−1\ni ˜ν⊤\ni + Block-diag(η−1\ni1 P ⊥\ni11, . . . , η−1\nij P ⊥\nijj).\n(11)\nSubstituting equation (11) into the trace term in equation (9), we obtain:\ntr(SiΣ−1\ni ) = tr(Si˜ν∆−1\ni ˜ν⊤) + PJ\nj=1 η−1\nij\nn\ntr(Sijj) −\n1\ndij 1⊤\ndijSijj1dij\no\n,\n(12)\nUsing the modified Cholesky decomposition of ∆i, we express ∆−1\ni\nas:\n∆−1\ni\n= (L−1\ni )⊤Λ−1\ni L−1\ni\n= U iΛ−1\ni U ⊤\ni .\n(13)\nSubstituting equation (13) into the first term in equation (12), we can simplify the term\ninvolving ∆−1\ni\nas follows:\ntr(Si˜νi∆−1\ni ˜ν⊤\ni ) = tr(˜ν⊤\ni Si˜νi U iΛ−1\ni U ⊤\ni ) = tr(Λ−1\ni U ⊤\ni AiU i),\n(14)\nTo calculate the diagonal components of the term in equation (14), we write:\n(Λ−1\ni U ⊤\ni AiU i)jj = λ−1\nij U ⊤\ni,1:j,jAi,1:j,1:jU i,1:j,j for j = 1, . . . , J,\n(15)\nBy combining Equations (10), (12), (14), and (15), we obtain the simplified likelihood\nform given in Equation (8).\n3.2\nGibbs sampling: Full conditional distributions of ηij and\nλij\nIndependent priors are assigned to the block-wise error terms η = (η1, . . . , ηn) ∈\nRnJ, with ηij ∼Inv-Gamma(a0, b0). Similarly, independent priors are assigned to the\nCholesky diagonal entries λ = (λ1, . . . , λn) ∈RnJ, with λij ∼Inv-Gamma(a1, b1).\nLeveraging the conjugacy of the Gaussian likelihood in equation (8) and the Inverse-\nGamma prior, we can derive the conditional posterior distribution for ηij and λij in\nclosed form. Using the Gibbs sampler, we update ηij from the following conditionally\n12\n\n\nindependent posteriors:\np(ηij | ·) ∝Inv-Gamma\nh\na0 + Ti(dij −1)\n2\n, b0 + Ti\n2\nn\ntr(Sjj) −1\ndij\n1⊤\ndijSijj1dij\noi\n.\nSimilarly, we sample λij from the following conditionally independent posteriors:\np(λij | ·) ∝Inv-Gamma\n\u0010\na1 + Ti\n2 , b1 + Ti\n2 U ⊤\ni,1:j,jAi,1:j,1:jU i,1:j,j\n\u0011\n.\n3.3\nGibbs sampling: Full conditional distribution of βj\nThis subsection describes the efficient Gibbs sampling procedure for the regression coef-\nficient parameters, first outlining key results followed by their derivations. To facilitate\nour discussion, we express the regression coefficients in a vectorized format:\nβq,j,1:(j−1) = (βqj1, . . . , βqj,j−1) ∈Rj−1 for q = 1, . . . , p,\nβj := β1:p,j,1:(j−1) = (β1,j,1:(j−1), . . . , βp,j,1:(j−1)) ∈Rp(j−1) for j = 2, . . . , J\nβ = (β2, . . . , βJ) ∈RpJ(J−1)/2.\n(16)\nAmong the p covariates, we apply a continuous spike-and-slab prior (George and Mc-\nCulloch, 1993) exclusively to the regression coefficient parameters associated with the\np-th covariate, βp,j,1:(j−1). For the regression coefficients corresponding to the remaining\ncovariates, βq,j,1:(j−1) for q = 1, . . . , p −1, we use Gaussian priors. This prioritization\nreflects the ABIDE analysis in Section 5, where the ASD diagnostic group membership\nvariable is the primary focus for interpretation among the regression coefficients. The\ninduced sparsity in βp,j,1:(j−1) helps capture scenarios where ∂Σijℓ/∂xip = 0. Naturally,\nthis framework can be extended to incorporate sparsity in the regression coefficients\nof other covariates as needed, without loss of generality. The priors are specified as\nfollows:\nπjℓ∼Ber(q1) for ℓ< j, j = 2, . . . , J,\n(βpjℓ| πjℓ) ∼(1 −πjℓ)N(0, τ 2\n0 ) + πjℓN(0, τ 2\n1 ),\nβq,j,1:(j−1) ∼N(0, τ 2\n2 Ij−1) for q = 1, . . . , p −1.\n(17)\nHere, N(0, τ 2\n1 ) and N(0, τ 2\n0 ) represent the spike and slab components of the prior,\nrespectively, with hyperparameters τ 2\n2 > 0 and τ 2\n1 ≫τ 2\n0 > 0. The mixing probabilities\nπjℓdetermine whether a coefficient is drawn from the slab or spike distribution. In high-\ndimensional settings, the mixing probability hyperparameter q1 is often chosen to be\nsmall. In our simulation experiments and ABIDE analysis, we set q1 = 0.5 by default.\nThe use of a continuous spike-and-slab prior allows for effective modeling of sparsity,\n13\n\n\nas these priors have become a cornerstone of Bayesian variable selection (Tadesse and\nVannucci, 2021; Biswas et al., 2022).\nProposition 2 (Full Conditional Posterior distribution of βj). The conditional poste-\nrior distribution of βj corresponding to the prior specified in (17) and the hierarchical\nmodel in (7) can be expressed as\nβj = C−1/2\nj\nn\nC−1/2\nj\nµj + Z\no\nwhere Z ∼N(0, Ip(j−1)),\n(18)\nwith µj ∈Rp(j−1) and Cj ∈Rp(j−1)×p(j−1) given by\nµj =\nn\nX\ni=1\nTiλ−1\nij ˜xiU⊤\ni,1:(j−1),1:(j−1)Ai,1:(j−1),j\nCj =\nn\nX\ni=1\nTiλ−1\nij ˜xiU⊤\ni,1:(j−1),1:(j−1)Ai,1:(j−1),1:(j−1)Ui,1:(j−1),1:(j−1)˜x⊤\ni + Πj.\nIn Proposition 2, ˜xi = (xi ⊗Ij−1) ∈Rp(j−1)×(j−1).\nThe diagonal matrix Πj ∈\nRp(j−1)×p(j−1) has the following structure: the first (p −1)(j −1) diagonal elements are\nset to τ −2\n2 . For the remaining (j −1) diagonal elements, the value of the ℓ-th term\ndepends on πjℓ. If πjℓ= 1, the diagonal term is τ −2\n1\n(spike); if πjℓ= 0, the diagonal\nterm is τ −2\n0\n(slab). This formulation provides an explicit Gibbs sampling scheme for\nβj, with the inverse term C−1/2\nj\ncomputed via eigendecomposition.\nThe derivation of the conditional posterior distribution of βj in Proposition 2 is\ngiven in the following proof.\nProof. We first recall the vectorized representation in equation (16), which facilitates\nprior specification.\nThis representation differs from an alternative format, βjℓ=\n(β1jℓ, . . . , βpjℓ) ∈Rp for ℓ< j, j = 2, . . . , J, which is primarily used for modeling\nthe covariance block Σijℓand interpreting second-order patterns.\nLet πj := πj,1:(j−1) = (πj1, . . . , πj,j−1) ∈Rj−1 denote the vector of mixing proba-\nbilities. For j = 2, . . . , J, the priors on the regression coefficients can be expressed in\nvectorized form as follows:\np(βj | πj) ∝p(β1,j,1:(j−1), . . . , βp−1,j,1:(j−1)) p(βp,j,1:(j−1) | πℓ,j) = N(0, Π−1\nj ).\n(19)\nA key step in deriving the conditional posterior distribution is obtaining a closed-form\nexpression for the unit upper triangular matrix U i = (L−1\ni )⊤. Solving the equation\nU iU −1\ni\n= U iL⊤\ni = IJ via forward substitution, for each ℓ< j and j = 2, . . . , J, we\n14\n\n\nobtain:\nUiℓj = −Lijℓ−\nj−1\nX\nℓ′=ℓ+1\nUiℓℓ′Lijℓ′ = −x⊤\ni βjℓ−\nj−1\nX\nℓ′=ℓ+1\nUiℓℓ′\n\u0010\nx⊤\ni βjℓ′\n\u0011\n.\nFrom this, the j-th subcolumn U i,1:(j−1),j ∈Rj−1 can be written as:\nU i,1:(j−1),j = −Ui,1:(j−1),1:(j−1)˜x⊤\ni βj,\n(20)\nSubstituting equation (20) into the likelihood term from equation (8), we rewrite:\nU ⊤\ni,1:j,jAi,1:j,1:jU i,1:j,j\n= aijj + 2Ai,j,1:(j−1)U i,1:(j−1),j + U ⊤\ni,1:(j−1),jAi,1:(j−1),1:(j−1)U i,1:(j−1),j\n= aijj −2Ai,j,1:(j−1)Ui,1:(j−1),1:(j−1)˜x⊤\ni βj\n+ β⊤\nj ˜xiU⊤\ni,1:(j−1),1:(j−1)Ai,1:(j−1),1:(j−1)Ui,1:(j−1),1:(j−1)˜x⊤\ni βj.\n(21)\nBy combining equations (8), (19), and (21), we derive the Gaussian conditional posterior\ndistributions for βj for j = 2, . . . , J as:\np{βj | (βℓ)j−1\nℓ=1, ·}\n∝exp\nn\n−1\n2β⊤\nj Πjβj\no\nexp\nn\n−1\n2\nn\nX\ni=1\nTiλ−1\nij\n\u0010\n−2Ai,j,1:(j−1)Ui,1:(j−1),1:(j−1)˜x⊤\ni βj\n+ β⊤\nj ˜xiU⊤\ni,1:(j−1),1:(j−1)Ai,1:(j−1),1:(j−1)Ui,1:(j−1),1:(j−1)˜x⊤\ni βj\n\u0011o\n∝N\n\u0010\nC−1\nj µj, C−1\nj\n\u0011\n.\nThis Gaussian conditional posterior distribution leads to the posterior sampling formu-\nlation for βj, as given in Equation (18).\n3.4\nGibbs sampling: Full conditional distribution of πjℓ\nFrom the prior specification of the regression coefficients in equation (17), we derive\nthe conditional posterior distribution for the mixing probabilities π = (π2, . . . , πJ) ∈\nRJ(J−1)/2, where πj := πj,1:(j−1) = (πj1, . . . , πj,j−1) ∈Rj−1 denotes a vector of mixing\nprobabilities. For ℓ< j, j = 2, . . . , J, the conditional posterior probabilities under the\ncases πjℓ= 0 and πjℓ= 1 are given by:\np(πjℓ= 0 | ·) ∝p(πjℓ= 0)p(βpjℓ| πjℓ= 0) ∝(1 −q1) N(0, τ 2\n0 ),\np(πjℓ= 1 | ·) ∝p(πjℓ= 1)p(βpjℓ| πjℓ= 1) ∝q1 N(0, τ 2\n1 ).\n15\n\n\nUsing these expressions, the conditional posterior probability for πjℓis computed as:\nq⋆\n1 =\nq1N(βpjℓ; 0, τ 2\n1 )\n(1 −q1)N(βpjℓ; 0, τ 2\n0 ) + q1N(βpjℓ; 0, τ 2\n1 ),\np(πjℓ| ·) ∼Ber(q⋆\n1).\nThus, posterior samples of πjℓare drawn using the Gibbs sampler from a Bernoulli\ndistribution with success probability q⋆\n1, which reflects the relative contributions of the\nspike and slab components.\n4\nNumerical illustrations using simulated data\nWe investigate the performance of our estimation approach through simulations de-\nsigned to reflect the characteristics of ABIDE. We consider two settings with differ-\nent sparsity levels and data dimensions. For both simulations, the sample consists of\nn = 500 participants, each observed over Ti = 200 time points. The block partition vec-\ntor di is generated using a multinomial distribution, ensuring that voxels are assigned\napproximately evenly across the ROIs.\nIn the first simulation, we examine the effect of varying sparsity levels in a high-\ndimensional setting, where the number of voxels is Mi = 5,000 and the number of ROIs\nis set to J = 50. We consider three sparsity scenarios for the p-th covariate in the\ncovariance matrix Σi: highly sparse (95% sparsity), moderately sparse (80% sparsity),\nand relatively sparse (65% sparsity). This means that the true values of π ∈RJ(J−1)/2\nare drawn from a Bernoulli distribution with a success probability chosen to yield a\ncovariance matrix Σi exhibiting the specified sparsity level for the p-th covariate. For\nexample, in the moderately sparse setting, 80% of voxel pairs within the covariance\nmatrix satisfy ∂Σi/∂xip = 0. In the second simulation, we investigate a very high-\ndimensional setting where the number of voxels is Mi = 10,000 and the number of\nROIs is set to J = 100 or J = 200, resulting in data dimensions that are smaller than\nbut comparable to those of ABIDE. We assume a moderate sparsity structure, ensuring\nthat the covariance matrix Σi exhibits 80% sparsity for the p-th covariate.\nThe remaining parameters are specified as follows. The true values of participant-\nspecific block-wise error terms ηij are independently sampled from a uniform sequence\nwith spacing of 0.05 between 0.05 and 1.5. The true values of the scaling factors of\nthe Cholesky decomposition λij are set to 1/j for all i = 1, . . . , n. The true regression\ncoefficients βqjℓare independently drawn from a standard Gaussian distribution for\nall q = 1, . . . , p −1 and ℓ< j, where j = 2, . . . , J. For the p-th covariate, we set\nβpjℓ= 21(πjℓ= 1). The participant-specific covariates xi include p = 3 variables,\n16\n\n\nincluding an intercept term xi1, a Bernoulli-distributed covariate xi2 with a success\nprobability of 0.5, and a uniformly distributed covariate xi3 ranging from −0.5 to 0.5.\nWe next describe the prior distributions.\nThe priors for ηij and λij are both\nInv-Gamma(2.01, 1.01).\nThe hyperparameter for the mixing probabilities is set to\nq1 = 0.5, resulting in a prior distribution of πjℓ∼Bern(0.5), which ensures that each\nβpjℓhas an equal prior probability of inclusion. The hyperparameters of the regression\ncoefficients are specified as τ 2\n2 = 1, τ 2\n1 = 1, and τ 2\n0 = 0.01.\nResults are evaluated based on 50 simulated data sets, with the model in each\nreplicate estimated with 6,000 MCMC iterations, including a burn-in period of 1,000\niterations. All simulations and data analyses are conducted on a SLURM server running\nUbuntu 20.04.6 LTS, with an Intel Xeon Gold 6248 CPU (2.50 GHz, 1 core per task).\nThe software environment included R 4.2.1 with Intel Math Kernel Library (MKL)\nfor optimized linear algebra operations, as well as Rcpp and RcppArmadillo for high-\nperformance computations and seamless integration of C++ code.\nWe emphasize that both simulations involve an extremely high-dimensional setting\nin terms of both data size and the number of parameters to be sampled. The total\nnumber of between-voxel correlations is given by Pn\ni=1 M 2\ni , which amounts to 1.25×1010\nfor the first simulation and 5 × 1010 for the second.\nDespite the massive scale of\nthe data, our implementation of the BlocR model significantly reduces the number\nof parameters while maintaining flexibility.\nThe parameters to be sampled include\nη ∈RnJ, λ ∈RnJ, β ∈RpJ(J−1)/2, and π ∈RJ(J−1)/2. The parameter space, however,\nremains substantial. For J = 50, the number of parameters to be sampled is 54,900;\nfor J = 100, it increases to 119,800; and for J = 200, it reaches 279,600.\nAs J\nincreases, computational complexity grows accordingly, leading to longer computation\ntimes, which are nonetheless reasonable given the large number of parameters being\nsampled.\nTable 1 presents the results from the first simulation, reporting the coverage rates of\n95% credible intervals for key quantities of interest, averaged over 50 simulations. Cover-\nage is consistently maintained across all sparsity scenarios for the block-wise error terms\nη and the regression coefficients {β1:(p−1),jℓ}J\nℓ<j,j=2. Coverage for the mixing parameters\nπ is assessed by verifying whether the true values match the median posterior proba-\nbility after rounding, given that the mixing parameter is binary. Coverage is achieved\nin the highly sparse (95%) and moderately sparse (80%) settings, while undercoverage\nis observed in the relatively sparse (65%) scenario. This outcome is expected, as prior\nstudies have shown that continuous spike-and-slab posteriors for high-dimensional lin-\near regression models tend to perform best in high-sparsity settings (Narisetty and He,\n2014; Chen and Walker, 2019). An important quantity in this study is the coverage\nrate for the partial derivative ∂Σi/∂xip, which measures the sensitivity of the covariance\n17\n\n\nstructure to changes in the p-th covariate. Coverage is well maintained in the highly\nsparse and moderately sparse settings, while slight undercoverage is observed in the\nrelatively sparse case. However, the coverage remains within an acceptable range. The\ncomputation time per iteration averages 6 seconds across all sparsity scenarios.\ncoverage rate (se)\nSparsity\nη\n{β1:(p−1),jℓ}J\nℓ<j,j=2\nπ\n∂Σi/∂xip\n95 %\n0.95 (< 0.01)\n0.93 (0.04)\n0.96 (0.01)\n0.94 (0.05)\n80 %\n0.95 (< 0.01)\n0.95 (0.03)\n0.94 (0.01)\n0.94 (0.06)\n65 %\n0.95 (< 0.01)\n0.95 (0.04)\n0.73 (0.01)\n0.84 (0.06)\nTable 1: First simulation with M = 5,000, J = 50. Coverage rates (standard errors)\nof 95% credible intervals for quantities of interest averaged across 50 simulations, for\nthree sparsity scenarios: highly sparse (95%), moderately sparse (80%), and relatively\nsparse (65%).\nTable 2 presents results from the second simulation, which considers larger data di-\nmensions (M = 10,000) with J = 100 and J = 200 under the moderately sparse (80%)\nsetting. As in the first simulation, coverage rates of 95% credible intervals for key quan-\ntities of interest are reported, averaged over 50 simulations. Coverage is consistently\nmaintained across all parameters. The computation time per iteration increases with\nmodel complexity, averaging 1 minute for J = 100 and 15 minutes for J = 200.\ncoverage rate (se)\nMi\nJ\nη\n{β1:(p−1),jℓ}J\nℓ<j,j=2\nπ\n∂Σi/∂xip\n10, 000\n100\n0.95 (< 0.01)\n0.94 (0.03)\n0.94 (0.01)\n0.95 (0.03)\n10, 000\n200\n0.95 (< 0.01)\n0.95 (0.04)\n0.93 (0.02)\n0.94 (0.05)\nTable 2: Second simulation with M = 10, 000 with J = 100 and J = 200 under the\nmoderately sparse (80%) setting.\nCoverage rates (standard errors) of 95% credible\nintervals for quantities of interest averaged across 50 simulations.\nFigure 2 visually evaluates the performance of the BlocR model in estimating ∆i,\na key component of the covariance matrix. The plot compares the scaled ∆i computed\nfrom the true parameters (left), the scaled observed summary statistic Ai (middle), and\nthe scaled estimated ∆i obtained from maximum a posteriori (MAP) parameter esti-\nmates (right) under the moderate sparsity scenario in the first simulation for a randomly\nselected participant. Due to the high-dimensional nature of the data, directly compar-\ning entire covariance matrices is challenging. Instead, focusing on the key quantity ∆i\nallows for a more interpretable assessment. The results show that, by construction,\nthe observed summary statistic Ai closely approximates the true ∆i. Moreover, the\nestimated ∆i closely resembles the true values, demonstrating the model’s ability to\naccurately recover the parameters. Note that all comparisons are based on the scaled\n18\n\n\nversions of these quantities, as estimating the scaling factors λ is relatively more chal-\nlenging. However, this does not impact the interpretability of the model.\nFigure 2: Comparison of the scaled ∆i from the true parameters (left), the scaled\nobserved summary statistic Ai (middle), and the scaled estimated ∆i obtained from\nMAP parameter estimates (right) under the moderate sparsity scenario in the first\nsimulation (Mi = 5, 000, J = 50).\n5\nIdentification of functional connectivity influenced\nby ASD status\n5.1\nABIDE preprocessing and covariate selection\nOur objective is to identify which voxel pairs’ dynamic covariance is influenced by a\nparticipant’s membership in either the Autism Spectrum Disorder (ASD) group or the\ntypically developing (TD) control group. Our parcellation is the hierarchical multi-\nresolution 17-network parcellation of Schaefer et al. (2018), which contains J = 200\nROIs.\nA majority voting approach was used to assign voxels to ROIs due to the\ndifference in resolution between the atlas (1×1×1 mm3) and the data (3×3×3 mm3).\nAfter excluding voxels located outside the brain due to imperfections of the registration,\neach participant retains on average Mi ≈42,750 voxels (with mean 42,750.45 and\nstandard deviation 1,591.21 number of voxels). The number of voxels within each ROI,\ndij, has a mean of 213.75 and a standard deviation of 95.24.\nWe included only those participants who passed the functional quality assessment\nconducted through manual inspection by three independent raters. Additionally, par-\nticipants were required to have at least one recorded voxel value for all ROIs. The\ncovariates xi ∈Rp include selected demographic and clinical characteristics of the par-\nticipants. We chose candidate covariates based on previous studies (Qi et al., 2020;\nHeinsfeld et al., 2018), focusing on phenotypic information relevant to ASD. The final\ncovariate set includes p = 6 variables: intercept, ASD status, age at the time of the\nscan, sex, group-sex interaction term, and eye status during the resting scan. These\n19\n\n\nvariables were selected based on an examination of Manhattan plots, which display the\np-values from univariate regressions of each candidate covariate against pairwise corre-\nlations among ROIs. Figure 3 displays the Manhattan plots for the selected variables,\nwith Manhatten plots for other considered variables available in the Supplementary\nMaterial.\nAs the primary objective is to identify voxels influenced by ASD or TD\ngroup membership, we assign the continuous spike-and-slab prior to the diagnostic\ngroup variable, treated as the p-th covariate. The covariates and outcomes Y i are cen-\ntered and scaled for analysis, making the covariance matrix effectively equivalent to the\ncorrelation matrix.\nFigure 3: Manhattan plot showing the p-values from univariate regression of each of\nthe selected covariates against the pairwise correlations between J = 200 ROIs. The\nhorizontal red line indicates the significance threshold using the Bonferroni correction.\nThe horizontal green line represents the False Discovery Rate threshold calculated us-\ning the Benjamini-Hochberg procedure. Any points above these lines are considered\nstatistically significant at the corresponding thresholds.\nThe total number of participants used in the analysis is n = 764, with 374 in the\nASD group and 390 in the TD group. A phenotypic summary of these participants\nbased on the selected covariates is provided in Table 3. For each participant, let Ti\ndenote the length of the rfMRI time series, which has a mean of 98.11 and a standard\ndeviation of 29.86 after applying lag-2 thinning. This thinning process helps ensure\nthat the rfMRI outcomes are approximately independent. The total number of data\npoints is Pn\ni=1 M 2\ni ≈1.4 × 1012.\n20\n\n\nsample size\nage\nsex\nIQ\neye status\nmean (sd)\nmale\nfemale\nmean (sd)\nopen\nclosed\nASD\n374\n15.2 (6.4)\n330\n44\n104.3 (17.1)\n318\n56\nTD\n390\n15 (5.8)\n307\n83\n110.6 (13)\n316\n74\nTable 3: Phenotypic summary of ASD and TD groups for variables: age at the time of\nthe scan, sex, full-scale IQ standard score, and eye status during the resting scan. Mean\nand standard deviation are provided for continuous variables, counts are reported for\ncategorical variables.\n5.2\nEvaluating covariance matrix estimation\nResults are based on 5,000 MCMC iterations, including a burn-in period of 1,000 it-\nerations, with a computation time of 52 minutes per iteration. To evaluate whether\nthe model accurately estimates the covariance matrix, Figure 4 compares the scaled\nobserved summary statistic Ai (left panel) with the scaled estimated ∆i obtained from\nMAP parameter estimates (right panel) for a randomly selected participant, as pre-\nviously done in Section 4. A visual examination indicates that the two matrices are\nindeed similar.\nFigure 4: Comparison of the scaled observed summary statistic Ai (left) and the scaled\nestimated ∆i from MAP parameter estimates (right) for a randomly selected partici-\npant. The red banded structure along the diagonals and off-diagonals reflects the ROI\nordering in the 17-network parcellation, where the first 100 ROIs correspond to the left\nhemisphere and the last 100 to the right hemisphere.\nBoth matrices exhibit patterns of positive values highlighted in red along the di-\nagonals and off-diagonals, forming a banded structure. This pattern arises due to the\nordering of ROIs in the 17-network parcellation: the first 100 ROIs correspond to the 17\nnetworks in the left hemisphere, while the last 100 ROIs correspond to the 17 networks\n21\n\n\nin the right hemisphere. A similar structure was observed in the ROI-level functional\nconnectivity matrix shown in Figure 1. It is important to clarify that while the ROI-\nlevel functional connectivity matrix represents the sample correlation matrix, the scaled\nAi is a summary statistic of the sample covariance matrix, adjusted for the number of\nvoxels within each ROI. Thus, the two matrices are related but not identical.\n5.3\nInference on functional connectivity and ASD status\nWe conduct a hypothesis test to evaluate whether changes in ASD status xip (xip = 0\nfor ASD group, xip = 1 for TD control) are associated with the covariance block Σijℓas\nspecified by the model, for all j, ℓ= 1, . . . , J. Since xip is binary, we assess its effect on\nΣijℓby computing the discrete change when switching from xip = 0 to xip = 1, rather\nthan using a partial derivative (which is more applicable for continuous covariates):\nΣijℓ(xip = 1) −Σijℓ(xip = 0)\n(22)\n=\nh\n1\np\ndjdℓ\nℓ\nX\nℓ′=1\nλiℓ′\nn\nβpjℓ′βpℓℓ′ + βpjℓ′\n\u0010 p−1\nX\nq=1\nxiqβqℓℓ′\n\u0011\n+ βpℓℓ′\n\u0010 p−1\nX\nq=1\nxiqβqjℓ′\n\u0011oi\n1dj×dℓ.\nTo assess statistical significance, we compute 95% posterior credible intervals for equa-\ntion (22). If the credible interval does not contain zero, it provides strong evidence\nagainst the null hypothesis, indicating that voxels within the (j, ℓ)-th ROI exhibit a\nsignificant association with changes in ASD status.\nFigure 5: Covariance blocks (ROIs) shown in red indicate regions where changes in\nASD status (xip) are statistically significant in over 95% of participants. Black lines\nseparate left and right hemispheres.\n22\n\n\nWe test our hypotheses across all 764 participants in the study. Our analysis iden-\ntifies voxels within the (j, ℓ)-th ROI that exhibit statistical significance in over 95% of\nparticipants. These regions are particularly important as they reliably contribute to\ndifferentiating ASD from TD participants by capturing consistent differences in func-\ntional connectivity patterns. Figure 5 presents these results, revealing patterns of con-\nnectivity both within and between hemispheres. Notably, there appears to be a greater\nconcentration of significant regions in the upper left and lower right quadrants across\nall hemispheric areas.\n20%\n29%\n48%\n16%\n31%\n23%\n35%\n29%\n32%\n17%\n21%\n29%\n21%\n26%\n22%\n30%\n29%\n34%\n27%\n19%\n25%\n24%\n16%\n48%\n26%\n43%\n15%\n30%\n23%\n19%\n30%\n24%\n20%\n21%\n17%\n16%\n22%\n15%\n20%\n22%\n22%\n28%\n15%\n27%\n20%\n31%\n30%\n30%\n18%\n20%\n24%\n16%\n23%\n29%\n23%\n22%\n22%\n16%\n17%\n35%\n34%\n19%\n18%\n22%\n18%\n17%\n20%\n17%\n27%\n22%\n16%\n30%\n21%\n15%\n29%\n19%\n30%\n28%\n20%\n17%\n17%\n30%\n20%\n16%\n32%\n25%\n24%\n24%\n18%\n19%\n24%\n18%\n18%\n20%\n17%\n15%\n24%\n27%\n20%\n21%\n16%\n18%\n24%\n23%\n52%\n20%\n17%\n15%\n20%\n52%\n37%\n21%\n16%\n20%\n16%\n19%\n20%\n21% 17%\n20%\nVisualC (4)\nVisualB (13)\nVisualA (14)\nSomMotB (10)\nSomMotA (14)\nAud (9)\nDorsAttnB (9)\nDorsAttnA (15)\nSalVenAttnB (14)\nSalVenAttnA (12)\nContC (15)\nContB (12)\nContA (13)\nLanguage (8)\nDefaultC (9)\nDefaultB (15)\nDefaultA (14)\nDefaultA (14)\nDefaultB (15)\nDefaultC (9)\nLanguage (8)\nContA (13)\nContB (12)\nContC (15)\nSalVenAttnA (12)\nSalVenAttnB (14)\nDorsAttnA (15)\nDorsAttnB (9)\nAud (9)\nSomMotA (14)\nSomMotB (10)\nVisualA (14)\nVisualB (13)\nVisualC (4)\nFigure 6: Percentage of ROIs in each network with statistically significant connections\nto the other 17 networks. Only ROIs with at least 15% significant connections are\nshown. The number of ROIs per network is indicated in parentheses.\nFigure 6 provides a network-level summary of our findings, illustrating the per-\ncentage of statistically significant ROIs in each network that connect to the other\n17 networks.\nTo enhance interpretability, we focus on ROIs where at least 15% of\nconnections are statistically significant. This visualization offers insights into broader\nfunctional connectivity differences associated with ASD.\nOur findings align with previous research, supporting reported patterns of sub-\nnetwork and internetwork connectivity atypicalities in ASD. Specifically, the results\nhighlight increased involvement of the default mode subnetworks (Padmanabhan et al.,\n2017), sensorimotor subnetworks (Coll et al., 2020), and connectivity between the con-\n23\n\n\ntrol and attention networks (Farrant and Uddin, 2016). These findings suggest that\nASD-related patterns of atypical network connectivity are present in individuals with\nfrequently co-occurring conditions. Future studies with larger, well-characterized ASD\ncohorts are needed to confirm these results.\n6\nConclusion\nAlthough developed for large-scale neuroimaging data, the BlocR model has broader\napplications.\nExploring its use in other domains, such as financial data, could be\nvaluable. While block structures have been studied in finance (Archakov and Hansen,\n2022), they have not been examined at this scale nor have they incorporated covariates\nin the model.\nFuture work should assess whether our inferential framework yields\nsimilarly robust insights in this context.\nReferences\nIlya Archakov and Peter Reinhard Hansen. A canonical representation of block matrices\nwith applications to covariance and correlation matrices. Review of Economics and\nStatistics, pages 1–39, 2022.\nNiloy Biswas, Lester Mackey, and Xiao-Li Meng. Scalable spike-and-slab. In Interna-\ntional Conference on Machine Learning, pages 2021–2040. PMLR, 2022.\nSu Chen and Stephen G. Walker. Fast Bayesian variable selection for high dimensional\nlinear models: Marginal solo spike and slab priors. Electronic Journal of Statistics,\n13(1):284 – 309, 2019.\nTom YM Chiu, Tom Leonard, and Kam-Wah Tsui. The matrix-logarithmic covariance\nmodel. Journal of the American Statistical Association, 91(433):198–210, 1996.\nSarah-Maude Coll, Nicholas EV Foster, Alexa Meilleur, Simona M Brambati, and\nKrista L Hyde. Sensorimotor skills in autism spectrum disorder: A meta-analysis.\nResearch in Autism Spectrum Disorders, 76:101570, 2020.\nCameron Craddock, Yassine Benhajali, Carlton Chu, Francois Chouinard, Alan Evans,\nAndr´as Jakab, Budhachandra Singh Khundrakpam, John David Lewis, Qingyang Li,\nMichael Milham, et al. The neuro bureau preprocessing initiative: open sharing of\npreprocessed neuroimaging data and derivatives. Frontiers in Neuroinformatics, 7\n(27):5, 2013.\n24\n\n\nAdriana Di Martino, Chao-Gan Yan, Qixiang Li, and et al. The autism brain imaging\ndata exchange: towards a large-scale evaluation of the intrinsic brain architecture in\nautism. Molecular Psychiatry, 19(6):659–667, 2014.\nRobert Engle and Bryan Kelly. Dynamic equicorrelation. Journal of Business & Eco-\nnomic Statistics, 30(2):212–228, 2012.\nKristafor Farrant and Lucina Q Uddin. Atypical development of dorsal and ventral\nattention networks in autism. Developmental Science, 19:550–563, 2016.\nEmily B Fox and David B Dunson. Bayesian nonparametric covariance regression. The\nJournal of Machine Learning Research, 16(1):2501–2542, 2015.\nEdward I George and Robert E McCulloch.\nVariable selection via gibbs sampling.\nJournal of the American Statistical Association, 88(423):881–889, 1993.\nHossein Haghighat, Mitra Mirzarezaee, Babak Nadjar Araabi, and Ali Khadem. A\nsex-dependent computer-aided diagnosis system for autism spectrum disorder using\nconnectivity of resting-state fmri. Journal of Neural Engineering, 19(5):056034, 2022.\nJustine Y Hansen, Golia Shafiei, Ross D Markello, Kelly Smart, Sylvia ML Cox, Martin\nNørgaard, Vincent Beliveau, Yanjun Wu, Jean-Dominique Gallezot, ´Etienne Aumont,\net al. Mapping neurotransmitter systems to the structural and functional organization\nof the human neocortex. Nature neuroscience, 25(11):1569–1581, 2022.\nAnibal S´olon Heinsfeld, Alexandre Rosa Franco, R Cameron Craddock, Augusto Buch-\nweitz, and Felipe Meneguzzi. Identification of autism spectrum disorder using deep\nlearning and the abide dataset. NeuroImage: Clinical, 17:16–23, 2018.\nPeter D Hoff and Xiaoyue Niu. A covariance regression model. Statistica Sinica, pages\n729–753, 2012.\nZakaria S. Khondker, Hongtu Zhu, Haitao Chu, Wei Lin, and Joseph G. Ibrahim. The\nbayesian covariance lasso. Statistical Interface, 6(2):243–259, 2013.\nHO Lancaster. The helmert matrices. The American Mathematical Monthly, 72(1):\n4–12, 1965.\nKer-Chau Li. Genome-wide coexpression dynamics: theory and application. Proceedings\nof the National Academy of Sciences, 99(26):16875–16880, 2002.\nCatherine Lord, Mayada Elsabbagh, Gillian Baird, and Jeremy Veenstra-Vanderweele.\nAutism spectrum disorder. The lancet, 392(10146):508–520, 2018.\n25\n\n\nA. Mohammadi and E. C. Wit. Bayesian Structure Learning in Sparse Gaussian Graph-\nical Models. Bayesian Analysis, 10(1):109 – 138, 2015.\nThomas Muschinski, Georg J Mayr, Thorsten Simon, Nikolaus Umlauf, and Achim\nZeileis. Cholesky-based multivariate gaussian regression. Econometrics and Statistics,\n2022.\nNaveen Naidu Narisetty and Xuming He. Bayesian variable selection with shrinking\nand diffusing priors. The Annals of Statistics, 42(2):789 – 817, 2014.\nAarthi Padmanabhan, Charles J Lynch, Marie Schaer, and Vinod Menon. The de-\nfault mode network in autism. Biological Psychiatry: Cognitive Neuroscience and\nNeuroimaging, 2(6):476–486, 2017.\nMohsen Pourahmadi. Joint mean-covariance models with applications to longitudinal\ndata: Unconstrained parameterisation. Biometrika, 86(3):677–690, 1999.\nShile Qi, Robin Morris, Jessica A Turner, Zening Fu, Rongtao Jiang, Thomas P Dera-\nmus, Dongmei Zhi, Vince D Calhoun, and Jing Sui. Common and unique multimodal\ncovarying patterns in autism spectrum disorder subtypes. Molecular autism, 11:1–15,\n2020.\nJac Fredo Agastinose Ronicko, John Thomas, Prasanth Thangavel, Vineetha Koneru,\nGeorg Langs, and Justin Dauwels. Diagnostic classification of autism using resting-\nstate fmri data improves with full correlation functional brain connectivity compared\nto partial correlation. Journal of Neuroscience Methods, 345:108884, 2020.\nS. Samanta, K. Khare, and G. Michailidis. A generalized likelihood-based bayesian\napproach for scalable joint regression and covariance selection in high dimensions.\nStatistical Computing, 32(47), 2022.\nAlexander Schaefer, Ru Kong, Evan M. Gordon, Timothy O. Laumann, Xi-Nian Zuo,\nAvram J. Holmes, Simon B. Eickhoff, and B. T. Thomas Yeo. Local-global parcella-\ntion of the human cerebral cortex from intrinsic functional connectivity mri. Cerebral\nCortex, 28(9):3095–3114, 2018. doi: 10.1093/cercor/bhx179.\nMahlet G Tadesse and Marina Vannucci. Handbook of Bayesian variable selection. CRC\nPress, 2021.\nMin Wang, Jiaying Lu, Ying Zhang, Qi Zhang, Luyao Wang, Ping Wu, Matthias Bren-\ndel, Axel Rominger, Kuangyu Shi, Qianhua Zhao, et al. Characterization of tau\npropagation pattern and cascading hypometabolism from functional connectivity in\nalzheimer’s disease. Human brain mapping, 45(7):e26689, 2024.\n26\n\n\nYifan Yang, Hwiyoung Lee, and Shuo Chen. A new representation of uniform-block\nmatrix and applications. arXiv preprint arXiv:2304.08553, 2023.\nYifan Yang, Chixiang Chen, and Shuo Chen. Covariance matrix estimation for high-\nthroughput biomedical data with interconnected communities. The American Statis-\ntician, pages 1–20, 2024.\nZhen Yang and Yen-Yi Ho. Modeling dynamic correlation in zero-inflated bivariate\ncount data with applications to single-cell rna sequencing data. Biometrics, 78(2):\n766–776, 2022.\nTianwei Yu. A new dynamic correlation algorithm reveals novel functional aspects in\nsingle cell and bulk rna-seq data. PLoS computational biology, 14(8):e1006391, 2018.\nYi Zhao, Bingkai Wang, Stewart H Mostofsky, Brian S Caffo, and Xi Luo. Covariate\nassisted principal regression for covariance matrix outcomes.\nBiostatistics, 22(3):\n629–645, 2021.\nTao Zou, Wei Lan, Hansheng Wang, and Chih-Ling Tsai. Covariance regression analysis.\nJournal of the American Statistical Association, 112(517):266–281, 2017.\n27\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21235v1.pdf",
    "total_pages": 27,
    "title": "A new block covariance regression model and inferential framework for massively large neuroimaging data",
    "authors": [
      "Hyoshin Kim",
      "Sujit K. Ghosh",
      "Emily C. Hector"
    ],
    "abstract": "Some evidence suggests that people with autism spectrum disorder exhibit\npatterns of brain functional dysconnectivity relative to their typically\ndeveloping peers, but specific findings have yet to be replicated. To\nfacilitate this replication goal with data from the Autism Brain Imaging Data\nExchange (ABIDE), we propose a flexible and interpretable model for\nparticipant-specific voxel-level brain functional connectivity. Our approach\nefficiently handles massive participant-specific whole brain voxel-level\nconnectivity data that exceed one trillion data points. The key component of\nthe model is to leverage the block structure induced by defined regions of\ninterest to introduce parsimony in the high-dimensional connectivity matrix\nthrough a block covariance structure. Associations between brain functional\nconnectivity and participant characteristics -- including eye status during the\nresting scan, sex, age, and their interactions -- are estimated within a\nBayesian framework. A spike-and-slab prior facilitates hypothesis testing to\nidentify voxels associated with autism diagnosis. Simulation studies are\nconducted to evaluate the empirical performance of the proposed model and\nestimation framework. In ABIDE, the method replicates key findings from the\nliterature and suggests new associations for investigation.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}