{
  "id": "arxiv_2502.20977v1",
  "text": "MultiResolution Low-Rank Regularization of Dynamic\nImaging Problems\nTommi Heikkilä∗\nORCID: 0000-0001-5505-8136\ntommi.heikkila@lut.fi\nMarch 3, 2025\nAbstract\nMultiResolution Low-Rank decomposition is formulated for regularization of dy-\nnamic image sequences. The decomposition applies a local low-rank decomposition on\na sequence of discrete wavelet transforms. Its effective formulation as a regularization\nfunctional is discussed and numerically tested for dynamic X-ray tomography in com-\nparison to other low-rank methods. The results suggest it is similar to traditional locally\nlow-rank decomposition but produces less severe block artifacts.\n1\nIntroduction\nIn dynamic imaging problems such as magnetic resonance imaging (MRI) and computed\ntomography (CT) of moving targets it is often better to drastically shorten the measurement\ntime since noisy and undersampled data is easier to correct than the underlying unknown\nmovement. Over the years many low-rank methods have been proposed and shown to be\neffective in resolving the static components of sequential data [8, 29, 7, 21]. If the imaging\nproblem at hand is linear, the slowly changing and static parts of the image sequence can be\nreconstructed independently and more robustly. This also lowers the complexity of recon-\nstructing the dynamic components and adds robustness since a rough outline of the object\nof interest is already provided, assuming the changes are not too drastic.\nLocal low-rank (LLR) methods consider only small patches of the images at once and try\nto find low-rank approximations for the smaller regions. Originally introduced for dynamic\nMRI [22], the method has also been used in magnetic resonance fingerprinting (MRF) [4].\nMany of these low-rank methods also include sparsity promoting terms, either at the same\ntime [21, 4] or in separate components [7, 16, 18]. See also [11] for different patch based\ntemporal regularization method.\nRecently many low-rank representations (LRRs) have also been developed in the context\nof tensors [12, 6, 13], in order to extend the familiar tools and ideas from matrices (i.e. 2-\ndimensional arrays) to higher dimensional objects.\nThis way vital underlying structure\nof the data is still preserved in dynamic (location and time), hyperspectral (location and\nfrequency) and phase-space (location and momentum) applications, just to name a few.\n∗LUT University, School of Engineering Science, Finland\n1\narXiv:2502.20977v1  [math.NA]  28 Feb 2025\n\n\nFurthermore in data processing and machine learning, extracting some lower dimensional\nsubspace or manifold from hugely large and complicated data sets is often helpful in keeping\nthe computations and storage manageable.\nIn this work the perspective is limited only\nto the traditional low-rank matrix factorizations (namely the singular value decomposition\n(SVD)), but there is no inherent reason why the proposed method could not be extended to\ntensors as in [14, 13].\nIn this work a locally low-rank decomposition is formulated by applying the patched low-\nrank decomposition on the wavelet domain rather than the usual spatial domain. This idea\nhas already been used in (hyperspectral) image denoising and deblurring [17, 28], where it\nevolved from local wavelet coefficient thresholding methods. Alternate approach of wavelet-\ndenoising the principal components has also been proposed even earlier [2].\nThanks to the multiscale nature of the wavelet decomposition, the low-rank decomposi-\ntion will also cover multiple scales. Moreover, due to the smooth and overlapping wavelet\nelements, the patches should have less distinct boundary artifacts. We show that this mul-\ntiresolution low-rank (MRLR) decomposition is unitary and briefly showcase its use as a\nregularization term with numerical examples in dynamic X-ray tomography application.\n2\nLow-rank methods\nWe consider a sequence of time-dependent inverse problems of the form\nAtft = mt + εt, for t = 1, 2, . . . , T.\n(1)\nHere each ft ∈RN is a Nr × Nc image flattened into vector with N = NrNc values in\ntotal. Extension to 3D volumes would follow in similar fashion. For simplicity, we denote\nthe value at i’th row and j’th column of the image by ft[i, j], even though formally ft is a\none dimensional vector.\nWe can stack each problem in eq. (1) by denoting f =\n\u0000ft\n\u0001T\nt=1, m+ε =\n\u0000mt +εt\n\u0001T\nt=1 and\nA = diag\n\u0000A1, . . . , AT\n\u0001\nthe block diagonal operator. Then we can write (1) as Af = m + ε\nandthe least squares term is then\n\r\rAf −m\n\r\r2\n2 =\nT\nX\nt=1\n\r\rAtft −mt\n\r\r2\n2.\nSince each time step t is currently independent, we regularize the problem by requiring\nthat the evolution of f over time must have a low-rank structure.\nWe form the Casorati matrix [8] using the associated operator M given as\nM : f 7−→F =\n\u0002\nf1\nf2\n. . .\nfT\n\u0003\n=\n\n\nf1[1, 1]\nf2[1, 1]\n. . .\nfT [1, 1]\nf1[2, 1]\nf2[2, 1]\n. . .\nfT [2, 1]\n...\n...\n...\n...\nf1[Nr, Nc]\nf2[Nr, Nc]\n. . .\nfT [Nr, Nc]\n\n,\n(2)\nwhere F ∈RN×T . Since this is simply a reordering of the terms, it can be reverted with\nM\n−1 : F 7−→f.\n(3)\nWe note also that for any f, g ∈RNT we have\n⟨f, g⟩= ⟨Mf, Mg⟩F ,\n(4)\n2\n\n\nwhere ⟨·, ·⟩F is the Frobenius inner product.\nWe could attempt penalizing the rank of the matrix F = Mf, but this corresponds to\nminimizing the ℓ0-\"norm\" of the singular values. The usual remedy from compressed sensing\nand other sparsity promoting regularization methods is to use the convex ℓ1-norm instead.\nThis leads to penalizing the nuclear norm of the matrix F:\n\r\rF\n\r\r\n∗=\nr\nX\ni=1\n\f\fσ(F)i\n\f\f =\n\r\rσ(F)\n\r\r\n1,\n(5)\nwhere r = rank(F) ≤min{N, T} and σ(F) = diag(Σ) ∈Rr denotes the singular values of\nF given by the singular value decomposition (SVD):\nF = UΣV T =\n\n\n|\n|\n|\nu1\nu2\n. . .\nur\n|\n|\n|\n\n\n\n\nσ(F)1\n...\nσ(F)r\n\n\n\n\nvT\n1\nvT\n2...\nvT\nr\n\n.\n(6)\nHence the global low-rank (GLR) method is given by minimizing the functional\nGLR\n\u0000f\n\u0001\n=\n\r\rAf −m\n\r\r2\n2 + λ ∥Mf∥∗,\n(7)\nwhere λ > 0 is the regularization parameter.\nIn order to minimize eq. (7) effectively, we need to analyze the low-rank operator properly.\nLet diag denote the linear operator which takes the diagonal values from any matrix. For\nrectangular m × n matrices, if m > n, the output is zero-padded to be in Rm. Formally we\ncan write diag(M) = (M ⊙I)1, where ⊙is the Hadamard product (elementwise product of\nmatrix elements) and 1 ∈Rr a column vector of all ones. Finally for any y ∈Rr we can\nalso define diag∗(y) = (y1T ) ⊙I, which gives a diagonal matrix with elements of y on the\ndiagonal.\nNow consider\nL : f 7−→diag\n\u0000UT M(f)V\n\u0001\n.\n(8)\nBy definition L extracts the diagonal matrix Σ from the SVD of Mf and we obtain a vector\nof r singular values with the diag operator. It is easy to verify that L : RNT →Rr is a linear\noperator, but note that it only yields the singular values of the specific vector f since the\nunitary matrices U and V depend on the SVD of f. However, we have the following result\nindependent of the particular f ∈RNT .\nProposition 1. The operator L as defined in (8) is unitary and for any y ∈Rr\nL∗y = M\n−1 \u0000Udiag∗(y)V T \u0001\n.\n(9)\nProof. The result follows easily by considering the Frobenius inner product, for which\n⟨AB, C⟩F = ⟨B, AT C⟩F for any compatible sized matrices A, B and C.\nLet x =\n\u0000xt\n\u0001T\nt=1 ∈RNT and y ∈Rr be arbitrary and consider\n⟨Lx, y⟩= ⟨diag\n\u0000UT M(x)V\n\u0001\n, y⟩= ⟨UT M(x)V, diag∗(y)⟩F\n= ⟨M(x), Udiag∗(y)V T ⟩F = ⟨x, M\n−1\u0000Udiag∗(y)V T \u0001\n⟩\n3\n\n\nTherefore eq. (9) holds. Then we can check that\nLL∗y = diag\n\u0010\nUT M\n\u0000M\n−1(Udiag∗(y)V T )\n\u0001\nV\n\u0011\n= diag\n\u0000UT Udiag∗(y)V T V\n\u0001\n=\n\u0000(y1T ) ⊙I ⊙I\n\u0001\n1 = y,\nand\nL∗Lx = M\n−1\u0000Udiag∗\u0000diag(UT M(x)V )\n\u0001\nV T \u0001\n= M\n−1\u0000UUT M(x)V V T \u0001\n= M\n−1\u0000M(x)\n\u0001\n= x.\nThis concludes the proof.\nProposition 1 gives an alternative definition to eq. (7) as\nGLR\n\u0000f\n\u0001\n=\n\r\rAf −m\n\r\r2\n2 + λ ∥Lf∥1 ,\n(10)\nwhich can be minimized with relatively standard methods.\nRemark. An easier way of defining the diag-operator would be to simply use a vector of\nones 1 ∈Rr. For diagonal matrices the operations diag(Σ) and Σ1 would give identical\nresults, but the adjoints would be different since diag∗(diag(Σ)) = Σ, but Σ11T ̸= Σ and L\nwould not be unitary but r-times redundant.\n2.1\nLocally low-rank methods\nBy construction the global low-rank method in eq. (7) can only provide a very coarse\napproximation of the image sequence f since each principal component is of same size as\nft. And since the SVD is limited by the dimensions N and T (whichever is smaller), the\nnumber of components may be very limited, even when f has a very high spatial resolution\nor is a 3D volume, for example.\nOne way to overcome these limitations is to divide each ft into K patches of size p × p\n(or more generally size p1 × · · · × pd in d dimensions) [22]. We assume the patches are non-\noverlapping. We can then consider a sequence of K patched Casorati matrices, independently\nfor each patch, as\nMp×p : f 7−→(Fk)K\nk=1 ,\n(11)\nwhere for example, the first patch is\nF1 =\n\n\nf1[1, 1]\nf2[1, 1]\n. . .\nfT [1, 1]\n...\n...\n...\n...\nf1[p, 1]\nf2[p, 1]\n. . .\nfT [p, 1]\nf1[1, 2]\nf2[1, 2]\n. . .\nfT [1, 2]\n...\n...\n...\n...\nf1[p, p]\nf2[p, p]\n. . .\nfT [p, p]\n\n\n(12)\nand so on. This process is also illustrated in fig. 1.\nHence the local low-rank (LLR) method is given by\n4\n\n\np\np\nft, t = 1, . . . , T\nMp×p\nFk, k = 1, . . . , K\np2\nT\nFigure 1: Illustration how the patched Casorati matrices are formed.\nLLR\n\u0000f\n\u0001\n=\n\r\rAf −m\n\r\r2\n2 + λ\nK\nX\nk=1\n\r\r\u0000Mp×pf\n\u0001\nk\n\r\r\n∗,\n(13)\nwhere λ > 0 is the regularization parameter. Defining a patched variant Lp of the uni-\ntary operator L would be straight forward (with either square or more general rectangular\npatches) but the construction is omitted here.\nChanging the patch sizes gives a lot of control over the regions where each low-rank\napproximation is carried out. Notice also that the patches can have different sizes as long as\neach patch is present in every time step ft since the patches are separated in to the columns\nof matrices Fk in the sequence, and Fk and Fl need not have the same size for k ̸= l. Let\np(k) denote the size of the k’th patch from now on.\nHowever, this simple separation suffers from distinct artifacts at the boundaries of the\npatches since each is handled independently of the neighboring patches. This can be partly\nresolved with overlapping patches, but this makes the associated operator M more compli-\ncated and then the minimization of eq. (13) can become more difficult. Some overlapping\npatching strategies have been proposed, in particular for dynamic MRI [1, 15].\n3\nMultiresolution low-rank method\nIn the proposed multiresolution low-rank method the (patched) Casorati matrix is not\nformed from the image sequence f = (ft)T\nt=1 but from the corresponding 2D wavelet decom-\npositions (Wft)T\nt=1. The coefficients from the wavelet decomposition retain some of the local\nspatial information from f, but the smoothness of f is no longer tied to the values of neigh-\nboring matrix elements but the underlying wavelet system. In other words, even though the\npatched Casorati matrices split the input into independent, non-overlapping blocks, once\nthe wavelet coefficients are reconstructed (via the inverse wavelet transform W−1 = W∗),\nthe end result should not show sharp artifacts at the edges of the patches since the wavelet\nelements ψj,k can overlap with nearby wavelets.\nMoreover, since the wavelet transform covers multiple spatial scales, the local low-rank\napproximation of said transform will also cover multiple scales. Since we are free to change\nthe patch size for the different decomposition levels (and directional subbands, but we omit\nthis), this gives more control than the direct low-rank approximations considered earlier.\n5\n\n\nFormally we consider the 2D discrete wavelet transform W and write\nW : f 7−→(cj)J\nj=0 = c,\n(14)\nwhere the scale j = 0 corresponds to the scaling coefficients and the scales j ⩾1 the detail\ncoefficients. The different translates ℓ= (l1, l2) ∈Z2 are given as\ncj[ℓ, d] = ⟨f, ψ(d)\nj,ℓ⟩= ⟨f, 2jψ(d)(2j · −ℓ)⟩,\n(15)\nwhere the different directional wavelets are constructed from 1D wavelet and scaling func-\ntions in the following manner:\nhorizontal: ψHL(x1, x2) = ψ(x1)ϕ(x2),\nvertical: ψLH(x1, x2) = ϕ(x1)ψ(x2),\ndiagonal: ψHL(x1, x2) = ψ(x1)ψ(x2).\nThe 2D scaling function is denoted ψLL(x1, x2) = ϕ(x1)ϕ(x2).\nThe multiresolution low-rank (MRLR) method is given by\nMRLR\n\u0000f\n\u0001\n=\n\r\rAf −m\n\r\r2\n2 + λ\nJ\nX\nj=0\nKj\nX\nk=1\n\r\r\u0000Mp(j,k)Wf\n\u0001\nk\n\r\r\n∗,\n(16)\nwhere at every scale j, the wavelet subbands are broken in to Kj local patches of size\npj(k) = pj,k\n1\n× pj,k\n2 . Again the patch sizes could vary, but for simplicity let us consider\nsquare pj ×pj patches chosen solely based on the scale j. Unlike [14], the different directional\nsubbands are kept in separate Casorati matrix patches (accounted by the index k).\nThis regularization term can be written as\nMRLR\n\u0000f\n\u0001\n=\n\r\rAf −m\n\r\r2\n2 + λ ∥LW,pf∥1 .\n(17)\nHere we define\nLW,p : (ft)T\nt=1 7−→\n\n\nLp(0)(Wf)0\nLp(1)(Wf)1\n...\nLp(J)(Wf)J\n\n,\nwhere each Lp(j)(Wf)j itself is a local low-rank decomposition in to Kj vectors of singular\nvalues.\nSince the wavelet transform corresponding to orthogonal wavelets is unitary, the operator\nLW,p is also unitary. The process is illustrated in fig. 2.\n4\nNumerical examples\nTo assess the quality and viability of the different low-rank methods the following nu-\nmerical tests are performed on dynamic X-ray tomography problems.\nWe consider an ill-posed inverse problem of reconstructing moving object of interest\nfrom a single measurement organized into a sinogram. Since the underlying movement is\n6\n\n\nW\nW\nW\nft, t = 1, . . . , T\nWft, t = 1, . . . , T\npj\npj\nj = 3\n. . .\nj = 2\nj = 1\nj = 0\nMp(j)\nj = 0, 1, . . . , J;\nk = 1, . . . , Kj\nFj,k, k = 1, . . . , Kj\nscale:\napproximation\ncoefficients\np2\nj\nT\nFigure 2: Illustration of the multiresolution low-rank scheme using J = 3 scales. Note how\nthere is a lot of flexibility in choosing the patch sizes for each scale and directional subband.\nunknown, we approximate the problem as a sequence of static limited angle problems which\nare notoriously difficult to reconstruct and often suffer from streaking artifacts.\nIn addition to the proposed MRLR-method given in (17), all tested data is also recon-\nstructed using three existing low-rank methods. First the Local Low-Rank (LLR) method\n[22] as formulated in (13). Then the Low-rank + Sparse (L+S) method [16, 7], which is a\ndifferent kind regularization scheme which decomposes the unknown dynamics into separate\nlow-rank and sparse components (using some multiresolution representation system such as\nwavelets). The objective functional for the L+S method is given as\nLplusS\n\u0000L, S\n\u0001\n= ∥A(L + S) −m∥2\n2 + λL∥ML∥∗+ λS∥WS∥1,\n(18)\nwhere L, S ∈RNT , M is the operator defined earlier in eq. (2) and W is again a 2D wavelet\ntransform applied separately to each time step St as in (16). Once again, with a suitable\nunitary operator, the nuclear norm can be replaced with the ℓ1-norm. The two regularization\nterms have separate regularization parameters λ = (λL, λS). And finally the Fast algorithm\nfor Total Variation and Nuclear Norm Regularization (FTVNNR) [26, 27]. Its objective\nfunctional can be given as\nFTVNNR\n\u0000f\n\u0001\n= ∥Af −m∥2\n2 + λ1\nT\nX\nt=1\nTV (ft) + λ2∥Mf∥∗.\n(19)\nIn short, it incorporates a spatial anisotropic total variation penalty on each time step in\naddition to the global low-rank penalty term. The implementation is based on the codes\navailable on github.com/uta-smile/FTVNNR_Dynamic_MRI_MEDIA, and only slightly modi-\nfied for CT.\nIn [4] the authors propose using both locally low-rank and (wavelet) sparsity regular-\nization terms on the unknown image sequence f for MRF. This is more closely related to\nthe earlier LLR method in (13) than the L+S method, since the locally low-rank and sparse\ncomponents are not separated.\nThe chosen patch sizes and different regularization and wavelet parameters are all listed\nin table 1. The LLR and MRLR functionals are minimized using the primal-dual fixed point\n7\n\n\n(PDFP) algorithm [3] with non-negativity constraints on the attenuation values. All algo-\nrithms are ran until the relative change between iterates drops below 5 · 10−4. Daubechies-3\nwavelets [5] are used with every wavelet-based method and data, with periodic boundary\nconditions to ease the choice of subband patch sizes. The Matlab implementation of the\nL+S algorithm is based on the example code from [10]. The repository with all the Matlab\ncodes is available on github.com/tommheik/WaveletLowRank.\nData\nMethod\nWavelet\ntransform\nRegularization\nparameters λ\nNo. of patches\nPatch sizes\nSimulated\nMRLR\ndb3 (J = 2)\n1.0\n[4×4, 2×2, 2×2]\n[64×64, 64×64, 64×64]\nLLR\n-\n0.1\n32 × 32\n8 × 8\nL+S\ndb3 (J = 3)\n(0.2, 0.08)\n-\n-\nFTVNNR\n-\n(0.001, 5)\n-\n-\nSTEMPO\nMRLR\ndb3 (J = 3)\n1.0\n[4×4, 2×2, 1×1, 1×1]\n[35×35, 35×35, 35×35, 35×35]\nLLR\n-\n0.1\n40 × 40\n7 × 7\nL+S\ndb3 (J = 3)\n(0.35, 0.02)\n-\n-\nFTVNNR\n-\n(0.01, 2)\n-\n-\nTable 1: Table of parameters used\n4.1\nData\nTests are performed both using simulated and real data:\nSimulated data consists of multiple objects of different shapes and attenuation. One\nobject is translated from left to right, the rest undergo periodic and affine deformation,\nand one object is slightly rotated counterclockwise at the same time. The reconstruction\nresolution is 256 × 256, but the data was initially simulated at twice the spatial resolution\nand white Gaussian noise of 3% variance was added to avoid inverse crime. A total of 360\nprojections with 1◦steps were computed using 180 unique time steps (i.e. 2 projections per\ntime step).\nFor the reconstructions, the data is organized into just 32 batches, each with 19 con-\nsecutive projections and overlap of 11 projections. The forward operator follows a parallel\nbeam geometry and is implemented using the ASTRA Toolbox [24, 23].\n32 reference time steps are chosen (out of the original 180) to match the average state\nof the batches and some of these are shown in fig. 3.\nt = 1\nt = 12\nt = 22\nt = 32\nGround truth\nFigure 3: Selected time steps of the synthetic ground truth object.\n8\n\n\nReal data is stempo_cont360_2d_b8.mat from the STEMPO dataset [9, 10] which\nconsists of 360 unique fan-beam projections measured at 1◦steps of a motorized dynamic\ntomography phantom. Detailed documentation is available in [9]. With the chosen binning\nlevel (8), the reconstruction resolution is set to 280 × 280.\nFor the reconstructions, the sinogram is organized into just 32 batches, each with 19\nconsecutive projections and overlap of 11 projections, just like the simulated data. The\nforward operator is implemented using the ASTRA Toolbox.\n4.2\nResults\nReconstructions of the simulated data are shown in fig. 4 and reconstructions of the\nSTEMPO data are shown in fig. 5. In fig. 4 the relative L2-error, SSIM [25] and HaarPSI\n[19] values for each individual time step are also shown.\nWith both data, all low-rank methods show similar movement artifacts and have particu-\nlar difficulty reconstructing the translated object (e.g. object at the bottom center in fig. 4).\nHowever, with the chosen wavelet decomposition and block sizes, the MRLR regularization\ndoes not produce visible block-artifacts unlike the traditional LLR method. The relative\nL2-error and HaarPSI metrics also favor MRLR whereas the SSIM values are consistently\nbetter for LLR. FTVNNR performs almost as well and the images have less noise thanks\nto the TV. However the images are also more blurry. This is especially noticeable with the\nSTEMPO reconstructions. None of the methods reconstruct the intermediate time steps\nwell and there is a distinct jump in the translations seen between time steps 12 and 22.\nThe L+S method worked surprisingly poorly with the tested data both visually and\nbased on the numerical error metrics. Before the non-negativity projection was included in\nthe implementation (without any mathematical rigor), its results were even poorer. Due to\nthe two distinct regularization parameters (see (18)), it is somewhat harder to fine tune than\nthe other methods which might also affect the results. The MRLR also requires choosing\nthe wavelet decomposition and different patch sizes but the changes have clearer effect on\nthe behavior.\nAlthough the MRLR should be slower to compute than the LLR, in practice the difference\nin computational times seems negligible.\n5\nConclusions\nIn this paper the multiresolution low-rank decomposition is formulated as a possible\nregularization strategy for reconstructing dynamic image sequences. As a combination of\ndiscrete wavelet transform and a local low-rank decomposition, it forms a unitary operator\nand should be easily applicable to many optimization strategies. The numerical examples\nindicate that heavy regularization produces less severe artifacts compared to traditional low-\nrank regularization methods. However, in practice it still has very similar regularizing effect\non the reconstructions and should be applied in place of traditional low-rank methods or\nin tandem with some different prior. In the future it would be interesting to pair it with\nstronger spatial regularization such as total variation [20, 11, 27].\nAcknowledgments\nThe author is grateful of the funding from the Vilho, Yrjö and Kalle Väisälä Founda-\ntion; the Finnish Foundation for Technology Promotion; Academy of Finland (the Centre\n9\n\n\n0.481/ 0.535/ 0.223\n0.383/ 0.566/ 0.265\n0.408/ 0.555/ 0.262\n0.461/ 0.518/ 0.241\nFTVNNR\n0.615/ 0.273/ 0.207\n0.534/ 0.313/ 0.243\n0.543/ 0.261/ 0.237\n0.575/ 0.333/ 0.218\nL+S\n0.467/ 0.630/ 0.205\n0.412/ 0.674/ 0.256\n0.432/ 0.647/ 0.235\n0.447/ 0.648/ 0.223\nLLR\n0.453/ 0.526/ 0.266\nL2 / SSIM / HaarPSI\nt = 1\n0.348/ 0.616/ 0.298\nL2 / SSIM / HaarPSI\nt = 12\n0.378/ 0.538/ 0.285\nL2 / SSIM / HaarPSI\nt = 22\n0.427/ 0.550/ 0.268\nL2 / SSIM / HaarPSI\nt = 32\nMRLR\nFigure 4: Simulated data reconstructions using different low rank methods: MultiResolution\nLow-Rank, Local Low-Rank, Low-rank + Sparse decomposition and Total Variation and\nNuclear Norm regularization. The L2-error, SSIM and HaarPSI values of each time step is\nalso shown.\nof Excellence in Inverse Modelling and Imaging, decision 312339); and Research Council\nof Finland (Flagship of Advanced Mathematics for Sensing Imaging and Modelling grant\n359183).\nReferences\n[1] Candes, E.J., Sing-Long, C.A., Trzasko, J.D.: Unbiased risk estimates for singular value\nthresholding and spectral estimators. IEEE transactions on signal processing 61(19),\n4643–4657 (2013)\n[2] Chen, G., Qian, S.E.: Denoising of hyperspectral imagery using principal component\nanalysis and wavelet shrinkage. IEEE Transactions on Geoscience and remote sensing\n49(3), 973–980 (2010)\n10\n\n\nFTVNNR\nL+S\nLLR\nt = 1\nt = 12\nt = 22\nt = 32\nMRLR\nFigure 5: STEMPO data reconstructions using different low rank methods: MultiResolution\nLow-Rank, Local Low-Rank, Low-rank + Sparse decomposition and Total Variation and\nNuclear Norm regularization.\n[3] Chen, P., Huang, J., Zhang, X.: A primal–dual fixed point algorithm for convex sep-\narable minimization with applications to image restoration. Inverse Problems 29(2),\n025011 (2013)\n[4] Lima da Cruz, G., Bustin, A., Jaubert, O., Schneider, T., Botnar, R.M., Prieto, C.:\nSparsity and locally low rank regularization for MR fingerprinting. Magnetic resonance\nin medicine 81(6), 3530–3543 (2019)\n[5] Daubechies, I.: Ten lectures on wavelets. Society for industrial and applied mathematics\n(1992)\n[6] Du, S., Liu, B., Shan, G., Shi, Y., Wang, W.: Enhanced tensor low-rank representation\nfor clustering and denoising. Knowledge-Based Systems 243, 108468 (2022)\n[7] Gao, H., Cai, J.F., Shen, Z., Zhao, H.: Robust principal component analysis-based\nfour-dimensional computed tomography. Physics in Medicine & Biology 56(11), 3181\n(2011)\n11\n\n\n[8] Haldar, J.P., Liang, Z.P.: Spatiotemporal imaging with partially separable functions:\nA matrix recovery approach. In: 2010 IEEE International Symposium on Biomedical\nImaging: From Nano to Macro. pp. 716–719. IEEE (2010)\n[9] Heikkilä, T.: Stempo—dynamic x-ray tomography phantom. In: INdAM Workshop:\nAdvanced Techniques in Optimization for Machine learning and Imaging. pp. 1–14.\nSpringer (2022)\n[10] Heikkilä,\nT.:\nSTEMPO\n-\ndynamic\nX-ray\ntomography\nphantom\n(2023).\nhttps://doi.org/10.5281/zenodo.8239013, v1.2.0 [data set]\n[11] Kazantsev, D., Thompson, W.M., Lionheart, W.R., Van Eyndhoven, G., Kaestner,\nA.P., Dobson, K.J., Withers, P.J., Lee, P.D.:\n4D-CT reconstruction with unified\nspatial-temporal patch-based regularization. Inverse problems and imaging 9(2), 447–\n467 (2015)\n[12] Kilmer, M.E., Martin, C.D.: Factorization strategies for third-order tensors. Linear\nAlgebra and its Applications 435(3), 641–658 (2011)\n[13] Liu, S., Lei, M., Cao, J., Yang, T.: Dynamic mri reconstruction via multi-directional\nlow-rank tensor regularization. Biomedical Signal Processing and Control 99, 106848\n(2025)\n[14] Liu, S., Li, W., Cao, J., Zhang, K., Hu, S.: Image restoration via wavelet-based low-rank\ntensor regularization. Optik 273, 170415 (2023)\n[15] Meyer, N.K., Kang, D., Ahmed, Z., In, M.H., Shu, Y., Huston III, J., Bernstein, M.A.,\nTrzasko, J.D.: Locally Low-Rank Denoising of Multi-Echo Functional MRI Data With\nApplication in Resting-State Analysis. Topics in Magnetic Resonance Imaging 32(5),\n37–49 (2023)\n[16] Otazo, R., Candes, E., Sodickson, D.K.: Low-rank plus sparse matrix decomposition\nfor accelerated dynamic MRI with separation of background and dynamic components.\nMagnetic resonance in medicine 73(3), 1125–1136 (2015)\n[17] Palsson, F., Ulfarsson, M.O., Sveinsson, J.R.: Hyperspectral image denoising using\na sparse low rank model and dual-tree complex wavelet transform. In: 2014 IEEE\nGeoscience and Remote Sensing Symposium. pp. 3670–3673. IEEE (2014)\n[18] Ravishankar, S., Moore, B.E., Nadakuditi, R.R., Fessler, J.A.: Low-rank and adaptive\nsparse signal (lassi) models for highly accelerated dynamic imaging. IEEE transactions\non medical imaging 36(5), 1116–1128 (2017)\n[19] Reisenhofer, R., Bosse, S., Kutyniok, G., Wiegand, T.: A haar wavelet-based perceptual\nsimilarity index for image quality assessment. Signal Processing: Image Communication\n61, 33–43 (2018)\n[20] Rudin, L.I., Osher, S., Fatemi, E.: Nonlinear total variation based noise removal algo-\nrithms. Physica D: nonlinear phenomena 60(1-4), 259–268 (1992)\n[21] Trémoulhéac, B., Dikaios, N., Atkinson, D., Arridge, S.R.:\nDynamic mr image\nreconstruction–separation from undersampled (k, t)-space via low-rank plus sparse\nprior. IEEE transactions on medical imaging 33(8), 1689–1701 (2014)\n12\n\n\n[22] Trzasko, J., Manduca, A., Borisch, E.: Local versus global low-rank promotion in\ndynamic MRI series reconstruction. In: Proc. Int. Symp. Magn. Reson. Med. vol. 19,\np. 4371 (2011)\n[23] Van Aarle, W., Palenstijn, W.J., Cant, J., Janssens, E., Bleichrodt, F., Dabravolski,\nA., De Beenhouwer, J., Joost Batenburg, K., Sijbers, J.: Fast and flexible X-ray to-\nmography using the ASTRA toolbox. Optics express 24(22), 25129–25147 (2016)\n[24] Van Aarle, W., Palenstijn, W.J., De Beenhouwer, J., Altantzis, T., Bals, S., Batenburg,\nK.J., Sijbers, J.: The ASTRA Toolbox: A platform for advanced algorithm development\nin electron tomography. Ultramicroscopy 157, 35–47 (2015)\n[25] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from\nerror visibility to structural similarity. IEEE transactions on image processing 13(4),\n600–612 (2004)\n[26] Yao, J., Xu, Z., Huang, X., Huang, J.: Accelerated dynamic MRI reconstruction with\ntotal variation and nuclear norm regularization. In: International Conference on Medical\nImage Computing and Computer-Assisted Intervention. pp. 635–642. Springer (2015)\n[27] Yao, J., Xu, Z., Huang, X., Huang, J.: An efficient algorithm for dynamic MRI using\nlow-rank and total variation regularizations. Medical image analysis 44, 14–27 (2018)\n[28] Zhao, B., Sveinsson, J.R., Ulfarsson, M.O., Chanussot, J.: Wavelet-based block low-\nrank representations for hyperspectral denoising. In: 2021 IEEE International Geo-\nscience and Remote Sensing Symposium IGARSS. pp. 2484–2487. IEEE (2021)\n[29] Zhao, B., Haldar, J.P., Brinegar, C., Liang, Z.P.: Low rank matrix recovery for real-\ntime cardiac MRI. In: 2010 ieee international symposium on biomedical imaging: From\nnano to macro. pp. 996–999. IEEE (2010)\n13\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20977v1.pdf",
    "total_pages": 13,
    "title": "MultiResolution Low-Rank Regularization of Dynamic Imaging Problems",
    "authors": [
      "Tommi Heikkilä"
    ],
    "abstract": "MultiResolution Low-Rank decomposition is formulated for regularization of\ndynamic image sequences. The decomposition applies a local low-rank\ndecomposition on a sequence of discrete wavelet transforms. Its effective\nformulation as a regularization functional is discussed and numerically tested\nfor dynamic X-ray tomography in comparison to other low-rank methods. The\nresults suggest it is similar to traditional locally low-rank decomposition but\nproduces less severe block artifacts.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}