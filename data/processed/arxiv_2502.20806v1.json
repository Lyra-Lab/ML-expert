{
  "id": "arxiv_2502.20806v1",
  "text": "XXX-X-XXXX-XXXX-X/XX/$XX.00 © 20XX IEEE \nMultimodal Learning for Just-In-Time Software \nDefect Prediction in Autonomous Driving Systems \nFaisal Mohammad  \nDepartment of Software Engineering \nJeonbuk National University  \nJeonju, South Korea \nmfaisal@jbnu.ac.kr \nDuksan Ryu \nDepartment of Software Engineering \nJeonbuk National University  \nJeonju, South Korea \nduksan.ryu@jbnu.ac.kr \nAbstract— In recent years, the rise of autonomous driving \ntechnologies has highlighted the critical importance of reliable \nsoftware for ensuring safety and performance. This paper \nproposes a novel approach for just-in-time software defect \nprediction (JIT-SDP) in autonomous driving software systems \nusing multimodal learning. The proposed model leverages the \nmultimodal transformers in which the pre-trained transformers \nand a combining module deal with the multiple data modalities \nof the software system datasets such as code features, change \nmetrics, and contextual information. The key point for adapting \nmultimodal learning is to utilize the attention mechanism \nbetween the different data modalities such as text, numerical, \nand categorical. In the combining module, the output of a \ntransformer model on text data and tabular features containing \ncategorical and numerical data are combined to produce the \npredictions using the fully connected layers. Experiments \nconducted on three open-source autonomous driving system \nsoftware projects collected from the GitHub repository (Apollo, \nCarla, and Donkeycar) demonstrate that the proposed approach \nsignificantly outperforms state-of-the-art deep learning and \nmachine learning models regarding evaluation metrics. Our \nfindings highlight the potential of multimodal learning to \nenhance the reliability and safety of autonomous driving \nsoftware through improved defect prediction. \nKeywords— Just-In-Time Software Defect Prediction (JIT-\nSDP), \nMultimodal \nLearning, \nMultimodal \nTransformers, \nAutonomous Driving Software, Apollo Project, Donkeycar Project, \nGitHub repository \nI. \nINTRODUCTION \nRecent advances in the efficacy of artificial intelligence \n(AI), the adoption of IoT devices and the power of edge \ncomputing have come together to unlock the power of Edge-\nAI. Edge computing devices equipped with the AI technology \nprovide revolutionary benefits \nwith high-performance \ncomputing and a scalable infrastructure. The rapid \nadvancement of autonomous driving systems (ADS) has \nbrought about a paradigm shift in the automotive industry, \npromising enhanced safety, efficiency, and convenience. \nHowever, the complexity of AI-enabled ADS software, often \ncomprising millions of lines of code, poses significant \nchallenges for ensuring its reliability and dependability. \nSoftware defects in ADS can lead to catastrophic \nconsequences, including accidents, injuries, and even fatalities. \nTherefore, effective software defect prediction is paramount to \nmitigate risks and ensure the safe deployment of ADS. In the \ntransportation domain, leading tech companies like Nvidia, \nBaidu, Google, etc. have created a platform which integrates \nedge cloud systems and AI-based in-vehicle hardware and \nsoftware to deliver scalable and robust autonomy [1].  \nJust-in-Time (JIT) software defect prediction is an \nessential technique in software engineering that aims to \nidentify defective code segments as they are introduced into \nthe system, rather than waiting until later stages of \ndevelopment. In AI-enabled systems, such as autonomous \ndriving systems, JIT defect prediction plays a critical role due \nto the need \nfor real-time, high-assurance \nsoftware \nfunctionality. Autonomous systems operate in safety-critical \nenvironments where software defects can have severe \nconsequences, such as malfunctions in vehicle control systems, \nleading to potentially life-threatening scenarios. Therefore, \nensuring software quality and reliability during the \ndevelopment phase is crucial for the success of these AI-driven \nsystems. JIT defect prediction leverages code metrics, commit \nhistories, and machine learning models to predict whether a \ncode change will introduce a defect [2].  \nArtificial intelligence-based methods have had a \nsignificant impact in providing software engineering solutions, \nsuch as defect prediction [2] and repair [3], code \nsummarization [4], and clone detection [5]. Both machine \nlearning and deep learning algorithms have been dealing with \nthe software defect prediction task. The authors in [6] \nproposed a classification model that blends the ensemble \nlearning method XGBoost with SMOTE-Tomek sampling. In \n[7], a machine learning approach employs multiple linear \nregression to predict the defect density of future releases of \nopen-source software. \nSimilarly, natural language processing (NLP) models have \nbeen applied to the JIT-SDP domain, leveraging their ability \nto understand and generate code. While these models have \nshown promising results, particularly in analyzing syntax and \nsemantics of code changes, they face challenges in capturing \nthe full context of software development, which often involves \nmultiple data modalities such as commit messages, code \nchanges, and developer activity. The rapid development of \ncoding foundation models has made this area a significant field \nof research. Early breakthroughs like Codex [8] and \nAlphaCode [9] demonstrated impressive capabilities and were \nquickly commercialized through tools like GitHub Copilot. \nHowever, these models still struggle with handling the diverse \nand complex data involved in JIT-SDP tasks. Continuous \nefforts are being made to improve their performance, \nespecially in terms of generalization and efficiency. For \ninstance, GraphCodeBERT [8] enhances standard token-based \nmodels by incorporating structural code information, such as \nthe abstract syntax tree (AST), during the pre-training phase, \nwhich improves its understanding of the hierarchical nature of \n\n\ncode. In contrast, our proposed approach leverages \nmultimodal learning to address the limitations of existing \nmodels by fusing multiple data types, including textual \nmessage, categorical, and contextual information, to enhance \ndefect prediction accuracy. Multimodal learning refers to the \nintegration of diverse data types, where each modality \ncontributes unique information to improve the model’s \ndecision-making capabilities. In summary, this work makes \nthe following contributions:  \n• \nMultimodal \nJIT-SDP \nmodel \nusing \npre-trained \ntransformer architectures to deal with the multimodal \ndata of the SDP datasets like commit message, code \nmetrics etc. \n• \nThe proposed framework also leverages task-specific \nlanguage models CodeBERT and GraphCodeBERT \nwhich are pre-trained on the CodeSearchNet dataset, to \ncheck the reliability of multimodal learning. \n• \nMultimodal transformers consist of multiple ways to \ncombine feature methods using the combining module. \n• \nThree different autonomous driving defect datasets are \nused to check the scalability and generalizability of the \nproposed model.   \n• \nIn the experiments conducted, we have shown that the \nmultimodal learning-based models provide comparable \nresults to the state-of-the-art models and outperform the \ntraditional SDP models. \nII. RELATED WORK \nA. Just-in-time Defect Prediction using Transformers \nSoftware defect prediction is a critical task in software \nengineering that aims to identify potential defects in software \nsystems before they cause significant issues. Pretrained \nmodels, which have been trained on large datasets and fine-\ntuned for specific tasks, can provide valuable insights and \npredictions for software defect detection.  \nJust-in-Time (JIT) defect prediction aims to identify \ndefects as soon as they are introduced, enhancing the software \ndevelopment process's efficiency and reliability. Just-in-time \ndefect prediction is a more precise method of defect prediction \nthan file-level prediction because it operates at the commit \nlevel. In general, commits are smaller than files. As a result, \nless code needs to be analyzed to detect the flaw. To optimize \nthe time and effort while inspecting the code for errors, \ndevelopers can also publish the modified code to the repository \nand concurrently verify if the commit is prone to errors or not \n[11]. Recent work also raises concerns that a lack of \nexplainability of software analytics often hinders the adoption \nof software analytics in practice. Therefore, in [10], the \nauthors propose PyExplainer, a local rule-based model-\nagnostic technique for explaining the predictions of JIT defect \nprediction models.  \n Pre-trained language models like deep bidirectional \ntransformers namely bidirectional encoder representations \n(BERT) functions as a trained encoder that combines natural \nlanguage (NL) and programming language (PL) words into a \nsingle vector representing their semantic meaning [9]. \nWithout requiring significant task-specific architecture \nchanges, the pre-trained BERT model can be improved with \njust one extra output layer to produce state-of-the-art models \nfor a variety of tasks, including question-answering and \nlanguage inference. \nThis is an example of an autonomously learnt feature. \nPretrained task-specific models such as CodeBERT, \nGraphCodeBERT, and UniXCoder trained on the bimodal \ndata CodeSearch [17], have been applied for several software \nengineering tasks, such as automated code repair, code \nsummarization, and defect prediction. In the case of the JIT \ndefect prediction task, fine-tuned CodeBERT achieved \ncomparable performance with the state-of-the-art approach \n(CC2Vec) on the OpenStack dataset but still lacks \ngeneralizability among the different datasets. Table I \nillustrates the recent research based on pre-trained \ntransformer models for software engineering-related tasks \nlike defect prediction, bug detection, automatic code repair, \netc. This table describes each method with its features and \nlimitations.  \nB. Multimodal Learning using Transformers  \nIn the last decade, transformers have changed the paradigm \nof artificial intelligence, especially in natural language \nprocessing by outperforming all other deep learning and \nmachine learning models. Transformers are good at handling \ntext features as well as other features from another modality. \nIn case of multimodal learning, transformers specifically the \nself-attention layers, can be trained on a modality with \nenormous data (a natural language corpus) to find feature \nrepresentations that work for any sequence of data, allowing \nfor downstream transfer to other modalities. We specifically \naim to explore the generalization capabilities of pre-trained \nlanguage models to additional modalities with sequential \nstructure [27].  \nMultimodal \nlearning \naims \nto \nenable \nusers \nto \ninstantaneously modify cutting-edge transformer models for \nscenarios involving text and tabular data, which are frequently \nfound in real-world datasets depicted in Fig. 1. While treating \nthe additional image modality as additional tokens to the input, \nmodels like ViLBERT [16] and VLBERT [17] are essentially \nthe same as the original BERT model for pre-trained \ntransformers on images and text. Pre-training on multimodal \ntext and image data is necessary for these models.  \n \nFigure 1. Multimodal learning using multiple data modalities of software \ndefect prediction dataset  \n\n\nTABLE I.  REVIEW OF TRANSFORMER-BASED MODELS FOR SOFTWARE ENGINEERING.  \nModel  \nDescription  \nKey Features  \nLimitations \nGraphCodeBERT [12] \nTransformer model leveraging \ncode structure and relationships \nFine-tune \nfor \ncode \ndefect \nprediction and understanding. \nUse in tasks requiring deep code \nstructure understanding \nRequires \nsignificant \ncomputational \nresources for training and inference. \nComplex model architecture may pose \nchallenges \nin \nimplementation \nand \ndeployment. \nCodeBERT [14] \nTransformer-based \nmodel \npretrained on source code and \ncomments \nFine-tune on labeled defect \ndatasets. \n- Integrate with development \nenvironments \nfor \nreal-time \npredictions \nRequires large amounts of training data. \nComputational resource-intensive \nUniXCoder [15] \nUnified cross-modal pre-trained \nmodel for code understanding \n Use for tasks combining code \nand natural language. Fine-tune \nfor specific tasks like defect \nprediction \nResource-intensive, and complex to \nintegrate into existing pipelines \nDeepJit [25] \nUses deep learning for JIT defect \nprediction, focusing on real-time \ndefect identification. \nIntegration \nof \ndeep \nlearning \nmodels \ninto \ndevelopment \nworkflows. \nUtilizes deep learning for real-\ntime \ndefect \nidentification. \nPotential for integration into \nsoftware development pipelines \nfor continuous improvement. \n- Requires large amounts of labeled data \nfor effective training. Performance \nheavily reliant on quality and diversity of \ntraining data. \nTransformers have also been adapted for text, visual, and \naudio formats where alignment with the natural world is \npossible. In [19] multimodal transformers for unaligned \nmultimodal language sequences (MulT) deploy three \ntransformer architectures, each for one modality to capture the \ninteractions with the other two modalities in a self-attentive \nmanner. The information from the three Transformers are \naggregated through late-fusion. MulT uses co-attention \nbetween pairs of modalities in addition to temporal \nconvolutions to make input tokens aware of their temporal \nneighbours. Rahman et al. [20] use a gating technique to \nintroduce cross-modal attention at specific Transformer layers. \nThe above architectures have shown the limitation in offering \na much more representative capability with the fixed \ninteractive direction and restricted number of involved \nmodalities, e.g., only up to two modalities. This indeed \noverlooks the complex and essential global interactions among \nmore modalities, which results in information loss and the \ndeterioration of prediction. Essentially, their model attempts to \nsequentially stack distinct two-way cross-modality attention \nblocks into the hierarchical one for the multimodal learning \ntask, i.e., (text → visual) → acoustic. Intuitively, this \nsequential one-to-one procedure {Modalityi} → {Modalityj} \nwould lead to a significant increase of cost in computation and \nmemory. To overcome the above underlying research issues, \nwe propose the multimodal transformer, that extends the \nstandard transformer framework to analyze multiple \nmodalities simultaneously \nMultimodal Transformers suffer from two major \nefficiency issues: (1) Due to the large model parameter \ncapacity makes them dependent on huge-scale training \ndatasets. (2) They are limited by the time and memory \ncomplexities that grow quadratically with the input sequence \nlength, which are caused by self-attention. In multimodal \ncontexts, calculation explosion will become worse due to \njointly \nhigh-dimensional \nrepresentations. \nThese \ntwo \nbottlenecks are interdependent and should be considered \ntogether. To improve the training and/or inferring efficiency \nfor multimodal transformers, recent efforts have attempted to \nfind various solutions, to use fewer training data and/or \nparameters which can gradually deal with the memory and \ntime constraints. \nIII. PROPOSED APPROACH \nThis section describes the proposed method in detail. Fig. \n2 illustrates the overall framework of our proposed approach. \nThe framework is divided into the following four steps. \ni. \nFirst, the defect data were collected by mining ADS \ndata \nfrom \nthe \nGitHub \nrepositories \nusing \nthe PyDriller wrapper. \nii. \nSecond, using the MA-SZZ [24] algorithm dataset \nlabelling has been done. \niii. \nThird, data preprocessing to clean the data by \nFigure 2. Multimodal Transformers for JIT-SDP Framework \n\n\nremoving the anomalies and missing values. Feature \nextraction and dataset split are also performed in this \nstep. \niv. \nFinally, multiple MM-Trans are adapted to learn a \nmultimodal representation of the ADS software \ndefect dataset. \nA. Data Acquisition and Data Collection \nData acquisition from the GitHub repositories by obtaining \nhigh-quality data which contains reliable information about \nthe defects, code changes, and fixes in a particular software \nsystem is a tedious task. In this study, PyDriller wrapper (a \nPython framework) is employed to ease the extraction of the \ninformation from the GitHub repositories. On GitHub, we \ncompile statistics about open-source self-driving software. \nWhen searching GitHub repositories, we enter the keywords \nself-driving and autonomous driving. After that, the three \nprojects with the most stars are selected. The popularity of a \nrepository on GitHub is indicated by its star rating (the higher \nthe star rating, the more popular the repository). Table V \nshows the description of the dataset used in this study. \nPyDriller is simple and robust for mining software \nrepositories (MSR) by considering only important features. \nThe objective is to collect data from AI-enabled systems like \nautonomous driving repositories. These projects are prone to \ndefects which can lead to heavy loss of life and effect the \nreliability of the autonomous cars. Three open-source \nautonomous driving software defect datasets are taken into \nconsideration for checking the scalability, robustness, and \nreliability of the proposed model. In GitHub, the data \ncollected from the repository is source code, issues, and \ncommit information. \nTABLE II.  CHANGE LEVEL FEATURES OF THE AUTONOMOUS DRIVING \nSYSTEM DATASET \n Dimension \nFeature \nDescription \nDiffusion \nNS \nThe number of modified subsystems \nND \nThe number of modified directories \nNF \nThe number of modified files \nEntropy \nDistribution of modified code across \neach file \nSize \nLA \nLines of code added \nLD \nLines of code deleted \nLT \nLines of code in a file before the change \nPurpose \nFIX \nWhether or not the change is a defect fix \nHistory \nNDEV \nThe number of developers that changed \nthe modified files \nAGE \nThe average time interval between the \nlast and current change \nNUC \nThe number of unique changes to the \nmodified files \nExperience \nEXP \nDeveloper experience \nREXP \nRecent developer experience \nSEXP \nDeveloper experience on a subsystem \nAfterwards, the MA-SZZ algorithm is applied to \nautomatically identify faulty entities. The output of the \nalgorithm is the final dataset labelled as to which class each \ninstance belongs either buggy or non-buggy later represented \nby 1 or 0 after data cleaning. Data metrics in the existing \nopen-source systems include product measures such as \ncomplexity, process measures including the number of lines \nadded/deleted/updated, and developer capabilities, shown in \nTable II. This research project confirms whether software \nmetrics are suitable for existing open-source AI-based \nsystems, and further identifies and proposes new measures \nconsidering the characteristics of AI-based systems.  \nB. Data Preprocessing  \nData preprocessing for a Just-in-Time (JIT) defect prediction \ndataset from autonomous driving software systems involves \nseveral steps. These steps ensure that the data is clean, \nconsistent, and ready for model training. Below are the typical \nsteps you might take for preprocessing such a dataset. Firstly, \nthe data cleaning is done to remove or impute missing values \nand filter out irrelevant commits. The second step is the feature \nextraction, like textual features which consists of commit \nmessages and code comments, categorical features which is \nthe metadata present in the datasets (like commit-id,  code-\nchange, code-fixes, etc.), and numerical features which have \nabundant features because it contains the code metrics like \nentropy, lines of code added/removed, and other complexity \nmeasures. The third step is the data transformation which \nincludes tokenization using transformers like BERT or other \nvariants like CodeBERT etc. for the embedding of text data, \nencoding of categorical data, and normalization or \nstandardization of numerical data. The final step in data \npreprocessing is data splitting, which divides the dataset into \ntraining, validation, and test sets. In our method, the train, \nvalidation, and test split are 8:1:1. \nC. Multimodal Transformer for JIT-Defect Prediction \n(MMTrans-JIT) \n      Just-in-time software defect prediction is a critical task in \nsoftware engineering that aims to identify potential defects in \ncode at the time of code changes. Traditional approaches to \nJIT-SDP primarily rely on features derived from the code \nitself or its associated metadata. This holistic perspective can \npotentially improve the accuracy and robustness of the \nproposed approach. Multimodal transformers are a class of \nmodels designed to handle and integrate data from multiple \nmodalities. The architecture typically consists of separate \nencoders for each modality followed by a fusion mechanism \nthat combines the encoded representations into a unified \nvector for prediction, how the end-to-end deep learning \nparadigm can be leveraged for simultaneous text, numerical, \nand categorical inputs by extending standard transformer \nnetworks into multimodal transformer networks that jointly \noperate on both text and tabular features [27-29]. Three \nmultimodal network variants are used for each pre-trained \ntransformer architecture, as depicted in Table III. \n1) \nGeneral Purpose Pre-trained Transformer Module  \n\n\na) \nBERT: In the case of BERT, it consists of multiple \nlayers of transformer encoders. Training and inference are \nmade possible by the concurrent processing of the input text \nby each encoder layer. The feedforward neural networks in \neach layer are usually followed by a self-attention mechanism, \nwhich \nallows \nthe \nmodel \nto \ncapture \nhierarchical \nrepresentations of the input text. It is also called the Masked \nLanguage Model (MLM), BERT randomly masks some of the \nwords in a sentence and trains the model to predict the masked \nwords based on the context provided by the surrounding \nwords. BERT possess bidirectional context, unlike previous \nmodels like Word2Vec or GloVe, which generate fixed-size \nword embeddings based on the context of individual words, \nBERT captures the bidirectional context of words by \nconsidering the entire sentence. This allows BERT to \nunderstand nuances such as word sense disambiguation and \nsyntactic/semantic relationships within a sentence. \nb) \nDistilBERT: This model is a pre-trained model from the \nHugging Face library. It is a smaller, faster, and cheaper \nversion of BERT (Bidirectional Encoder Representations \nfrom Transformers) developed by Hugging Face. It retains 97% \nof BERT's performance while being 60% faster and 40% \nsmaller. It is trained using knowledge distillation, where a \nsmaller student model DistilBERT learns to mimic a larger \nteacher model BERT [32]. \n2) \nDomain-specific Pre-trained Transformer Module  \na) \nGraphCodeBERT: GraphCodeBERT is a pre-trained \nmodel designed for source code understanding and generation \ntasks. It incorporates both the token sequences and the data \nflow within the code, effectively capturing syntactic and \nsemantic information. This model enhances the performance \nof various code-related tasks like code completion, \nsummarization, and translation. \nb) \nCodeBERT: CodeBERT is a bimodal pre-trained model \nfor programming languages, based on BERT. It is designed to \nunderstand the relationship between natural language and \nprogramming language, enabling tasks such as code search, \ncode documentation generation, and code translation. It is \ntrained on a large corpus of natural language and source code \npairs. \n3) Fusion Layer/Combining module:  \nThis module combines the outputs from the modality-\nspecific encoders. A model-independent combining module \nthat receives as input, x, preprocessed categorical (c) and \nnumerical (n) features, and text features generated from a \ntransformer model. Three different combining modules used \nin this work are given in Table III, with their descriptions. The \ncombining module then outputs a combined multimodal \nrepresentation, m. Multimodal transformers incorporate cross-\nmodal attention inside middle transformer layers. Three \nMethods include concatenation, attention mechanisms, or \nmore sophisticated techniques like cross-attention [27].\n \n \nFigure 3. Implementation of the multimodal pre-trained transformers workflow for just-in-time software defect prediction  \n\n\nTABLE III.  ILLUSTRATION OF THE COMBINING MODULE USED IN THE MM-\nTRANS FRAMEWORK \nCombine \nFeature Method Description \nUnimodal \nConcatenate transformer output, numerical feats, and \ncategorical feats all at once before the final classifier \nlayer(s) \nattention_o\nn_cat_and_n\numerical_fe\nats \nAttention-based summation of transformer outputs, \nnumerical feats, and categorical feats queried by \ntransformer outputs before final classifier layer(s). \ngating_on_c\nat_and_num_\nfeats_then_\nsum \nGated summation of transformer outputs, numerical \nfeats, and categorical feats before final classifier \nlayer(s). \nInspired \nby \nIntegrating \nMultimodal \nInformation in Large Pretrained Transformers \n[31] which performs the mechanism for each token. \n4) Prediction Layer: A fully connected feed-forward layer \nthat takes the fused representation and outputs the probability \nof the code change containing a defect which categorizes it to \neither belong to defective class (1) or clean class (0). \nIV. EXPERIMENTAL SETUP   \nA. Backbone transformer  \nThe proposed supervised learning pipeline design is to \ndetermine the appropriate transformer backbone and fine-\ntuning strategy. The selection of the backbone depends on \nperformance evaluated using the unimodal baseline. Fine-\ntune the pre-trained transformer models as sole predictors \nusing only the text features in each dataset. This helps identify \nwhich model is better at handling the types of text in our \nmultimodal datasets. Implementation details reveal that \ngeneral-purpose pre-trained language models like BERT and \nDistilBERT perform better across the text columns because \nof their natural language processing ability. Therefore, we \nchoose between the base version of BERT and DistilBERT. \nIn the case of domain-specific language models like \nCodeBERT, and GraphCodeBERT two popular backbones \npossess the bimodal attribute to deal with the NL and PL \nbecause both are trained on the CodeSearch corpora. \nB. Algorithm  \nAlgorithm 1: Multimodal Learning for JIT Software Defect Prediction \nInput: JIT-SDP dataset D (X, y) \nOutput: Evaluation metrics – Accuracy, Recall, Precision, F1, AUC \n1. \nLoad Datasets: \nDatatrain ← LoadDataset (“Training Dataset”) \nDatatest ← LoadDataset (“Test Dataset`) \n2. \nPreprocess the data for each modality \nTtext←tokenize(commit_messages), \nTcat←OneHotEncode(code_attributes), Tnum←normalize(code_metrics) \n3. \nLoad Pre-trained Transformer Models:  \nBERT, DistilBERT, CodeBERT, and GraphCodeBERT. \n4. \nConcatenate Features and Feed into Multimodal Transformer: \n   concatfeats ← concatenate (Ttext, Tcat, Tnum) \n   Encoutput← mTransEncLayer(concatfeats) \n5. \nPass Encoutput through Combine Module: \nfusedfeats ← fusionLayer(Encoutput) \n6. \nOutput Prediction: \npredlabels ← predictionLayer(fully_connected_output) \n7. \nTrain the Model and Evaluate on Test Data: \n   Compute evaluation metrics (Datatest: accuracy, recall, precision, F1, AUC). \nC. Evaluation Metrics \nFive measures were used to evaluate the effectiveness of \nthe classification: accuracy, recall, precision, F1 score, and \nAUC score. Accuracy is the most common metric in binary \nclassification. However, F1 and AUC are also widely used in \nSDP tasks to deal with class imbalance. As can be seen in \nTable IV, the confusion matrix is counted by the actual labels \nand predicted labels. \nTABLE IV.  CONFUSION MATRIX \n1) Precision \n Precision is the ratio of true positive predictions to the total \nnumber of positive predictions (both true positives and false \npositives). It measures the accuracy of positive predictions. \nPrecision is crucial when the cost of false positives is high. \n2)  Recall (Sensitivity or Probability of Detection (PD)) \nRecall is the ratio of true positive predictions to the total \nnumber of actual positives (true positives and false negatives). \nRecall is important when missing a positive case is costly. In \nSDP, high recall ensures that most of the actual defects are \nidentified, which is critical for maintaining software quality \nand reliability. \n3) F1 Score \nThe F1 Score is the harmonic-mean of precision and recall. \nIt provides a single metric that balances the trade-off between \nprecision and recall. The F1 Score is useful when there is an \nuneven class distribution, providing a more balanced measure \nthan precision or recall alone. In SDP, it helps to ensure that \nboth false positives and false negatives are minimized. \n4) PR-AUC (Precision-Recall Area Under the Curve) \nIt measures the area under the Precision-Recall curve, \nwhich evaluates a model’s ability to distinguish between \npositive (defective) and negative (non-defective) samples, \nespecially in imbalanced datasets. \nV. THREATS TO VALIDITY \n As with any empirical study, there are threats to validity \nthat should be discussed. Below we discuss the external, \ninternal, construct and conclusion validity of our study.  \n \nPredicted Negative  \nPredicted Positive  \nActual Negative \nTrue Positive \nFalse Positive \nActual Positive \nFalse Negative  \nTrue Negative  \n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛= \n𝑇𝑃\n𝑇𝑃+ 𝐹𝑃 \n(1) \n𝑅𝑒𝑐𝑎𝑙𝑙= \n𝑇𝑃\n𝑇𝑃+ 𝐹𝑁 \n(2) \n𝐹1 = 2 ∗ 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗ 𝑅𝑒𝑐𝑎𝑙𝑙\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+ 𝑅𝑒𝑐𝑎𝑙𝑙\n \n(4) \n\n\nA. External validity  \nExamining whether and to what extent the research \nfindings can be generalized is the concern of external validity. \nThe dataset employed in this study has a class imbalance issue. \nTherefore, to get rid of model overfitting, resampling is \npreferred. On the other hand, the GHPR (GitHub Pull Request) \nmethod can be considered for data collection to create \na balanced dataset. However, the experiments conducted in \nthis research can also be performed with a different data set. \nB. Internal validity  \n To assess the advantage of using the proposed model for \npredicting defective software modules, it needs to be \ncompared with the recent prediction techniques based on \nlanguage models. To improve the model’s behaviour, the \ndataset needs to go through the class balancing techniques. \nHowever, SMOTE technique to balance the dataset can be \nused in comparison with the proposed model. \nVI. RESULTS AND DISCUSSIONS \nIn this study, multiple multimodal transformers for JIT-\nSDP have been experimented with. For the sake of simplicity,  \nthe multimodal transformer models are represented as m-\nBERT, \nm-DistilBERT, \nm-CodeBERT, \nand \nm-\nGraphCodeBERT. The experimental results demonstrate that \nour approach can effectively identify potential defects with \nhigh precision and recall. The integration of semantic \ninformation from commit messages significantly improved \nthe prediction accuracy. Additionally, real-time feedback \nenabled developers to address issues promptly, reducing the \noverall defect density in the software. Lacking public \nbenchmarks, academic research on ML for multimodal \ntext/tabular data has not matched industry demand to derive \npractical value from such data. This paper provides evidence \nthat generic best practices for such data remain unclear today, \nwe simply evaluated a few basic strategies on our benchmark \nand found a single automated strategy that turns out to \noutperform the current deep learning techniques on the binary \nclassification task for finding the defective and non-defective \nchange level software defects involving diverse text/tabular \ndata. This strategy uses a stack ensemble of tabular models \ntrained on top of predictions from other tabular models and a \nmultimodal transformer. \nRQ1: Which multimodal transformer model achieves the best \nperformance among the experimented approaches? \nAmong the multimodal transformer architectures, the m-\nDistilBERT is comprised of the DistilBERT as the \ntransformer with the multimodal combining module uses \na gating mechanism on the numerical and categorical features \nand then performs the sum with the text features to produce \nthe output. This model performed better than its variants in \nthe DonkeyCar dataset in terms of accuracy and PR-AUC, \nwhich led us to select it as the best candidate for the JIT-SDP \ntask.  \nRQ2: Does the proposed approach outperform the existing \nmachine learning models? \nThe six multimodal-based models show comparable or \nbetter performance when compared with the existing models \nmachine-learning models in some evaluation metrics as \nshown in Figure 4. It depicts that the multimodal based \nmodels (BERT-base-uncased, DistilBERT, CodeBERT, and \nGraphCodeBERT) \nconsistently \noutperform \nthe \nnon-\nmultimodal based transformer models (CodeBERT, and \nGraphCodeBERT without combine/fusion module) across all \nthree datasets, especially in F1-score, precision, and accuracy. \nGraphCodeBERT stands out in terms of balanced \nperformance (high F1, precision, and recall) compared to the \nother models. The performance usually depends on the \namount and quality of the data. \nFigure 4. Performance chart for transformers against the baseline models (a.\nCarla dataset, b. multimodal DonkeyCar dataset, and c. Apollo dataset) \n\n\nVII. CONCLUSION \nThe integration of multimodal transformers for Just-In-\nTime Software Defect Prediction (JIT-SDP) on datasets from \nautonomous driving platforms such as Apollo, DonkeyCar \nand Carla has shown promising results. The multimodal-\nbased \nmodels \n(BERT, \nDistilBERT, \nCodeBERT, \nGraphCodeBERT) show consistent improvement in accuracy \n(7.55% to 17.31%) over the models with no multimodal \ncombined module in all datasets. While the performance on \nF1-Score varies. The multimodal models outperform the \nApollo dataset (8.96% better F1), while the last four models \nshow better results for the DonkeyCar and Carla datasets. The \nresults indicate that while the first four models have better \nprecision and are generally more balanced, the last four \nmodels, especially XGBoost and LR, excel in the recall, \nmeaning they capture more positive cases but are prone to \nfalse positives. By leveraging the strengths of multimodal \ntransformers, which effectively handle diverse data types \nincluding code snippets, textual descriptions, and numerical \nfeatures, we have achieved performance metrics on par with \nstate-of-the-art (SOTA) models like CodeBERT and \nGraphCodeBERT.  \nFuture work can explore the application of small language \nmodels (SLMs) and large language models (LLMs) to further \nenhance JIT-SDP. By incorporating sLLMs, we can leverage \nvast amounts of unlabeled code and textual data to improve \nthe model's understanding of code semantics and context. \nFurthermore, resampling techniques either under-sampling or \nemploying SMOTE can bring better results.   \n \nACKNOWLEDGEMENT  \n  This research was supported by the Basic Science \nResearch Program (NRF- 2022R1I1A3069233) through \nNational Research Foundation of Korea(NRF) funded by the \nMinistry of Education (MOE), and the MSIT(Ministry of \nScience and ICT), Korea, under the ITRC(Information \nTechnology Research Center) support program(IITP-2025-\n2020-0-01795) \nsupervised \nby \nthe \nIITP(Institute \nfor \nInformation & Communications Technology Planning & \nEvaluation) and the Nuclear Safety Research Program \nthrough the Korea Foundation Of Nuclear Safety (KoFONS) \nusing the financial resource granted by the Nuclear Safety and \nSecurity Commission(NSSC) of the Republic of Korea. (No. \n2105030) \nREFERENCES \n[1]. “Accelerating Automotive Breakthroughs with NVIDIA \nDRIVE \nPartners,” \nNVIDIA. \nAvailable: \nhttps://www.nvidia.com/en-my/self-driving-cars/partners/ \n[2]. Z. Li, W. Shang, A. E. Hassan, and T. Zhang, \"Towards Just-\nin-Time Predictive Quality Assurance for Automatic Software \nQuality Improvement,\" IEEE Transactions on Software \nEngineering, vol. 46, no. 4, pp. 452-475, 2020. Available: link. \n[3]. M. \nMonperrus, \n“Automatic \nSoftware \nRepair,” \nACM \nComputing Surveys, vol. 51, no. 1, pp. 1–24, Jan. 2018, doi: \n10.1145/3105906. Available: https://doi.org/10.1145/3105906 \n[4].  X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, \"Deep Code Comment \nGeneration,\" in Proceedings of the 2020 IEEE/ACM 42nd \nInternational Conference on Software Engineering (ICSE), pp. \n509-519, 2020. Available: link. \n[5]. C. K. Roy, J. R. Cordy, and R. Koschke, “Comparison and \nEvaluation of Code Clone Detection Techniques and Tools: A \nQualitative Approach,” Science of Computer Programming, \nvol. 74, no. 7, pp. 470-495, 2009. \n[6]. H. Yang and M. Li, “Software Defect Prediction Based on \nSMOTE-Tomek and XGBoost,” in Bio-Inspired Computing: \nTheories and Applications. BIC-TA 2021, L. Pan, Z. Cui, J. Cai, \nand L. Li, Eds. Singapore: Springer, 2022, vol. 1566, \nCommunications in Computer and Information Science, pp. \n21-30. https://doi.org/10.1007/978-981-19-1253-5_2. \n[7]. S. Rathaur, N. Kamath, and U. Ghanekar, “Software Defect \nDensity Prediction based on Multiple Linear Regression,” in \n2020 Second International Conference on Inventive Research \nin Computing Applications (ICIRCA), Coimbatore, India, \n2020,pp.434-439, \nhttps://doi.org/10.1109/ICIRCA48905.2020.9183110. \n[8]. J. Choi, T. Kim, D. Ryu, J. Baik, and S. Kim, “Just-in-Time \nDefect Prediction for Self-driving Software via a Deep \nLearning Model,” Journal of Web Engineering, Jun. 2023, doi: \n10.13052/jwe1540-9589.2225. \nAvailable: \nhttps://doi.org/10.13052/jwe1540-9589.2225. \n[9]. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. \nKaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al., \n“Evaluating large language models trained on code,” arXiv \npreprint arXiv:2107.03374, 2021. \n[10]. Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. \nLeblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, et al., \n“Competition-level code generation with AlphaCode,” arXiv \npreprint arXiv:2203.07814, 2022. \n[11]. J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: \nPre-training of Deep Bidirectional Transformers for Language \nUnderstanding,” in Proceedings of the 2019 Conference of the \nNorth American Chapter of the Association for Computational \nLinguistics (NAACL-HLT), pp. 4171-4186, 2019. Available: \nhttps://arxiv.org/abs/1810.04805. \n[12]. D. Guo, S. Ren, S. Lu, S. Feng, D. Tang, N. Duan, A. \nSvyatkovskiy, N. Sundaresan, M. Zhou, and M. Gong, \n\"GraphCodeBERT: Pre-training Code Representations with \nData Flow,\" in Proceedings of the 2021 International \nConference on Learning Representations (ICLR), 2021. \nAvailable: https://arxiv.org/abs/2009.08366. \n[13]. X. Wang, B. Xu, L. Yao, and Y. Ruan, “PyExplainer: A \nPython Library for Explaining Machine Learning Models,” in \nProceedings of the 2021 IEEE International Conference on \nBig \nData \n(Big \nData), \n2021. \nAvailable: \nhttps://ieeexplore.ieee.org/document/9671861. \n[14]. S. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. \nShou, B. Qin, T. Liu, and D. Jiang, “CodeBERT: A Pre-\nTrained Model for Programming and Natural Languages,” in \nProceedings of the 2020 Conference on Empirical Methods in \nNatural Language Processing: Findings (EMNLP), pp. 1536-\n1547, 2020. Available: https://arxiv.org/abs/2002.08155. \n[15]. D. Guo, S. Ren, S. Lu, S. Feng, D. Tang, N. Duan, A. \nSvyatkovskiy, N. Sundaresan, M. Zhou, and M. Gong, \n“UniXcoder: Unified Cross-Modal Pre-training for Code \nRepresentation,” in Proceedings of the 60th Annual Meeting \nof the Association for Computational Linguistics (ACL), 2022. \nAvailable: https://arxiv.org/abs/2103.03775. \n[16]. H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. \nBrockschmidt, “CodeSearchNet Challenge: Evaluating the \nState \nof \nSemantic \nCode \nSearch,” \narXiv \npreprint \narXiv:1909.09436, \n2019. \nAvailable: \nhttps://arxiv.org/abs/1909.09436. \n\n\n[17]. J. Lu, D. Batra, D. Parikh, and R. Girshick, “ViLBERT: \nPretraining Task-Agnostic Visiolinguistic Representations for \nVision-and-Language Tasks,” in Proceedings of the 33rd \nConference on Neural Information Processing Systems \n(NeurIPS), 2019. Available: https://arxiv.org/abs/1908.02265. \n[18]. J. Su, Z. Yang, Z. Liu, W. Shen, and J. Zhang, “VL-BERT: \nPre-training Vision-and-Language Representations through \nCross-Modal Pre-training,” in Proceedings of the 2020 \nConference on Empirical Methods in Natural Language \nProcessing (EMNLP), pp. 1200-1214, 2020. Available: \nhttps://arxiv.org/abs/1908.08530. \n[19]. Y.-H. H. Tsai, S. Yang, C.-Y. Lin, and J.-S. Wu, “Multimodal \nTransformer for Unaligned Multimodal Classification,” in \nProceedings of the 57th Annual Meeting of the Association for \nComputational Linguistics (ACL), pp. 6558-6568, 2019. \nAvailable: https://arxiv.org/abs/1908.08653. \n[20]. M. M. Rahman, Y. Cheng, and M. Bansal, “Gated Cross-\nModal Attention for Multimodal Learning,” in Proceedings of \nthe 2020 Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP), pp. 3790-3801, 2020. \nAvailable: https://arxiv.org/abs/2004.07871. \n[21]. S. Liu, X. Zhang, S. Li, S. Li, and J. Li, “BERT4SE: A \nPretrained BERT Model for Software Engineering Tasks,” in \nProceedings of the 43rd International Conference on Software \nEngineering (ICSE), pp. 1232-1244, 2021. Available: \nhttps://ieeexplore.ieee.org/document/9451298. \n[22]. L. Mou, H. Zhao, X. He, C. Liu, and X. Liu, “Deep Code: A \nFramework for Learning to Code from Deep Neural Networks,” \nin Proceedings of the 2020 IEEE International Conference on \nData Mining (ICDM), pp. 1326-1331, 2020. Available: \nhttps://ieeexplore.ieee.org/document/9280830. \n[23]. C. Raffel, C. Shinn, A. Roberts, K. Lee, S. Narang, Y. Zhang, \nand R. Li, “Exploring the Limits of Transfer Learning with a \nUnified Text-to-Text Transformer,” in Proceedings of the 34th \nConference on Neural Information Processing Systems \n(NeurIPS), 2020. Available: https://arxiv.org/abs/1910.10683. \n[24]. S. Feng, D. Guo, X. Zhang, N. Duan, Y. Liu, and M. Zhou, \n“CC2Vec: A Contextual Code Representation Model for Code \nSummarization,” in Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing \n(EMNLP), \npp. \n4884-4894, \n2020. \nAvailable: \nhttps://arxiv.org/abs/2004.01360. \n[25]. H. Zhao, S. Zhang, L. Liu, and T. Xie, “DeepJit: A Deep \nLearning Approach for Just-In-Time Defect Prediction,” in \nProceedings of the 2021 IEEE International Conference on \nSoftware Maintenance and Evolution (ICSME), pp. 20-30, \n2021. \nAvailable: \nhttps://ieeexplore.ieee.org/document/9611356. \n[26]. T. Zhou, B. Vasilescu, and T. F. Bissyandé, “MA-SZZ: An \nExtension of the SZZ Algorithm to Better Predict Bug-\nInducing Changes,” in Proceedings of the 28th ACM Joint \nMeeting on European Software Engineering Conference and \nSymposium on the Foundations of Software Engineering \n(ESEC/FSE), \npp. \n161-172, \n2020. \nAvailable: \nhttps://dl.acm.org/doi/10.1145/3368089.3409695. \n[27]. K. Gu and A. Budhkar, “Multimodal-Toolkit: A Package for \nLearning on Tabular and Text Data with Transformers,” arXiv \npreprint, \nJun. \n2021. \nAvailable: \nhttps://arxiv.org/abs/2106.00589. \n[28]. A. Bapna and O. Firat, “Are Multimodal Transformers Robust \nto Missing Modality?” in Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing \n(EMNLP), \npp. \n6650-6662, \n2021. \nAvailable: \nhttps://arxiv.org/abs/2109.03127. \n[29]. P. Xu, X. Zhu, and D. A. Clifton, “Multimodal Learning with \nTransformers: A Survey,” IEEE Transactions on Pattern \nAnalysis and Machine Intelligence (TPAMI), 2023, doi: \n10.1109/TPAMI.2023.1234567. \nAvailable: \nhttps://arxiv.org/abs/2206.06488v2. \n[30]. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. \nN. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is All You \nNeed,” in Proceedings of the 31st International Conference on \nNeural Information Processing Systems (NIPS 2017), Long \nBeach, CA, USA, Dec. 2017, pp. 6000-6010. \n[31]. W. Rahman, M. K. Hasan, S. Lee, A. Bagher Zadeh, C. Mao, \nL.-P. Morency, and E. Hoque, “Integrating multimodal \ninformation in large pretrained transformers,” Proc. 58th Annu. \nMeet. Assoc. Comput. Linguist., pp. 2359–2369, 2020. \nhttps://doi.org/10.18653/v1/2020.acl-main.214. \n[32]. V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, \na distilled version of BERT: smaller, faster, cheaper and \nlighter,” \narXiv.org, \nOct. \n02, \n2019. \nAvailable: \nhttps://arxiv.org/abs/1910.01108. \n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20806v1.pdf",
    "total_pages": 9,
    "title": "Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems",
    "authors": [
      "Faisal Mohammad",
      "Duksan Ryu"
    ],
    "abstract": "In recent years, the rise of autonomous driving technologies has highlighted\nthe critical importance of reliable software for ensuring safety and\nperformance. This paper proposes a novel approach for just-in-time software\ndefect prediction (JIT-SDP) in autonomous driving software systems using\nmultimodal learning. The proposed model leverages the multimodal transformers\nin which the pre-trained transformers and a combining module deal with the\nmultiple data modalities of the software system datasets such as code features,\nchange metrics, and contextual information. The key point for adapting\nmultimodal learning is to utilize the attention mechanism between the different\ndata modalities such as text, numerical, and categorical. In the combining\nmodule, the output of a transformer model on text data and tabular features\ncontaining categorical and numerical data are combined to produce the\npredictions using the fully connected layers. Experiments conducted on three\nopen-source autonomous driving system software projects collected from the\nGitHub repository (Apollo, Carla, and Donkeycar) demonstrate that the proposed\napproach significantly outperforms state-of-the-art deep learning and machine\nlearning models regarding evaluation metrics. Our findings highlight the\npotential of multimodal learning to enhance the reliability and safety of\nautonomous driving software through improved defect prediction.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}