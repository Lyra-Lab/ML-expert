{
  "id": "arxiv_2502.21038v1",
  "text": "Published as a conference paper at ICLR 2025\nREWARD LEARNING FROM\nMULTIPLE FEEDBACK TYPES\nYannick Metz1,2, Andr´as Geiszl2, Rapha¨el Baur2, Mennatallah El-Assady2\n1University of Konstanz, Germany\nyannick.metz@uni-konstanz.de\n2ETH Zurich, Switzerland\nageiszl@inf.ethz.ch, {raphael.baur, menna.elassady}@ai.ethz.ch\nABSTRACT\nLearning rewards from preference feedback has become an important tool in the\nalignment of agentic models. Preference-based feedback, often implemented as\na binary comparison between multiple completions, is an established method to\nacquire large-scale human feedback. However, human feedback in other contexts\nis often much more diverse. Such diverse feedback can better support the goals of a\nhuman annotator, and the simultaneous use of multiple sources might be mutually\ninformative for the learning process or carry type-dependent biases for the reward\nlearning process. Despite these potential benefits, learning from different feedback\ntypes has yet to be explored extensively. In this paper, we bridge this gap by\nenabling experimentation and evaluating multi-type feedback in a broad set of\nenvironments. We present a process to generate high-quality simulated feedback\nof six different types. Then, we implement reward models and downstream RL\ntraining for all six feedback types. Based on the simulated feedback, we investigate\nthe use of types of feedback across ten RL environments and compare them to pure\npreference-based baselines. We show empirically that diverse types of feedback\ncan be utilized and lead to strong reward modeling performance. This work is the\nfirst strong indicator of the potential of multi-type feedback for RLHF.\n1\nINTRODUCTION\nReinforcement learning from human feedback (RLHF) is a powerful tool to train agents when it\nis difficult to specify a reward function or when human knowledge can improve training efficiency.\nRecently, using multiple forms of human feedback for reward modeling has come into focus (Jeon\net al., 2020; Ghosal et al., 2023; Ibarz et al., 2018; Bıyık et al., 2022a; Mehta & Losey, 2022). Using\ndiverse sources of information opens up several possibilities: (1) feedback from different sources\nallows for correcting potential biases in the data; (2) the feedback type can be adapted to a partic-\nular task or user based on preferences, knowledge state, or available input modalities; (3) agents\ncan actively select an appropriate type of feedback during training to optimize learning (Jeon et al.,\n2020). However, there are only a few empirical investigations on the characteristics of different feed-\nback types (Mehta & Losey, 2022; Ghosal et al., 2023; Bıyık et al., 2022a). Even though there have\nbeen some efforts to unify different types of reward (Jeon et al., 2020), and attempts to learn from\na combination of two or three feedback types (Ibarz et al., 2018; Mehta & Losey, 2022), the use\nof a larger number of different feedback types has not been fully explored. This paper addresses\nthree key questions: (1) How can we define, model, and simulate different explicit types of human\nfeedback consistently? (2) What characteristics do reward functions learned from these feedback\ntypes exhibit? (3) Can we combine reward models from different feedback types for robust learning?\nContributions — To answer these, we contribute by (1) Implementing synthetic generation and\nreward models for six distinct human feedback types, along with a joint-reward modeling approach\nbased on a reward function ensemble (section 3), (2) Empirically investigating the effectiveness and\ncomplementarity of these feedback types (section 4), and (3) Analyzing the performance of joint\ntraining with multiple feedback types, highlighting the potential of learning from diverse human\nfeedback in future applications (section 5).\n1\narXiv:2502.21038v1  [cs.LG]  28 Feb 2025\n\n\nPublished as a conference paper at ICLR 2025\n2\nRELATED WORK\nReinforcement Learning from Human Feedback — Using human feedback as the sole or an\nadditional source of reward information has gained traction in research (Ng et al., 2000; Knox & Stone,\n2009; Griffith et al., 2013; Christiano et al., 2017), especially for the alignment of large language\nmodels (Ouyang et al., 2022). This paper focuses on learning rewards solely from human feedback,\nexcluding approaches like interactive reward shaping (Knox & Stone, 2009; Warnell et al., 2017).\nSynthetic Generation of Human Feedback — So far, the generation of synthetic feedback has been\nprimarily explored in the area of preferences (Bıyık et al., 2022b; Christiano et al., 2017). In their\nfoundational work for modern RLHF, (Christiano et al., 2017) utilize a simple oracle model based on\nunderlying ground truth reward, which uses the sum of rewards over segments to compute preferences.\nTo account for human error, they assume a fixed miss-labeling probability of 10%, implemented via\npreference label switching. (Bıyık et al., 2022b) implement a comprehensive toolbox to simulate\npreferences with adaptable levels of noise/irrationality and methods to collect data from humans. We\nextend these efforts into a more versatile multi-type feedback setting.\nCombining Multiple Reward Functions and Multi-Type Feedback — Different types of feedback,\nranging from ratings, demonstrations (Ng et al., 2000; Abbeel & Ng, 2004), comparisons (Wirth\net al., 2017) to interruptions (Hadfield-Menell et al., 2017) or even language and narrations (Fish\net al., 2018; Sumers et al., 2022b) has been proposed. A wide range of different types of feedback can\nbe interpreted via a common framework of reward-rational choice (Jeon et al., 2020). There has been\nsome work on combining multiple feedback types, most notably demonstrations and preferences Ibarz\net al. (2018); Bıyık et al. (2022a). However, the authors used separate phases of demonstrations and\npreference-based learning. Most similar in spirit is work by Mehta & Losey, who also train a single\nreward model based on the reward rational choice framework. However, they restrict themselves to\ntwo (three) feedback types, including demonstrations and preferences (as well as stops).\nSumers et al. investigate evaluative, instructive, and descriptive language feedback in a simple bandit\nenvironment. We see our work as an extension of these efforts towards more complex environments\nand more diverse and dynamic feedback.\nRecently, there has been some work on extending the space of possible feedback by presenting\nimplementations for collection from human annotators (Metz et al., 2023; Yuan et al., 2024).\nHowever, these works do not investigate the full implementation of diverse reward models and have\nnot analyzed reward learning and RL from diverse feedback.\n3\nDEFINING AND SYNTHESIZING MULTI-TYPE FEEDBACK\nWe start our investigation by defining relevant feedback types, describing our approach to simulate\nfeedback of these different types, and discussing the implementation of reward models.\nPreliminaries — We assume a RLHF scenario with an agent that acts within an environment,\nobserving states st ∈S ⊆RN and performing actions at ∈A ⊆RM. The agents select actions\naccording to a policy π, which throughout training is optimized with respect to a learned reward\nfunction ˆr : S × A →R. In RLHF, the reward function estimator ˆr is updated based on human\nfeedback. In RLHF, humans often provide feedback over trajectories ξ ∈Ξ, with Ξ being the set of\nall possible trajectories. Trajectories are sequences of states and actions, e.g., generated by the agent\nacting within the environment. Providing feedback for trajectories requires fewer human interactions,\nas human labelers do not need to label every state-action pair. Finally, in this paper, we refer to an\nexpert policy πe. πe is the policy that maximizes the true reward function, which is not observable in\nhumans. We use π(ξt:t+H) := Qt+H\nt\nπ(st) as a shorthand for the probability of a policy acting over\na segment.\n3.1\nFEEDBACK TYPES\nIn the scope of this study, we decided on six exemplary types of feedback motivated by existing\nwork (Metz et al., 2023). We briefly present how these six types of feedback can be defined\nindividually.\n2\n\n\nPublished as a conference paper at ICLR 2025\n3.1.1\nEVALUATIVE FEEDBACK\nF1: Rating Feedback — We define this type of feedback for cases in which a human gives a\nnumerical or otherwise quantifiable judgment of a target, e.g., binary feedback or a numeric score\n(Arzate Cruz & Igarashi, 2020). We define evaluative feedback as a mapping from a target T to a\nscalar value in a value set vfb ∈V ⊆R. Common choices for the value set can be a binary score\nV = −1, 1 or a rating V = 1, .., 10, which we choose for our experiments:\nFeval(ξ) = vfb ∈V = {1, 2, 3, ..., 10}\nF2: Comparative Feedback — Here, a user makes a relative judgment, i.e., a pairwise comparison\nbetween two targets or a ranking of multiple targets. This type of preference-based feedback is widely\nused because it is assumed that it is easier for humans to give comparative judgment compared to\nabsolute scores (Wirth et al., 2017). Comparative feedback is a set of targets with an associated\nordering relation. We define an instance of comparative feedback as a preference over two segments:\nFcomp(ξ1, ξ2) = vfb ∈{≻, ≺, =}\n3.1.2\nINSTRUCTIONAL FEEDBACK\nF3: Demonstrative Feedback — The human is asked to provide a reference of optimal behavior\nthat the agent should imitate (Ng et al., 2000). We generally assume that these demonstrations are\noptimal w.r.t. the ”internal” human reward function. A demonstration is not conditioned on any\nspecific trajectory but rather independent and is given for the entire trajectory space.\nFdemo(Ξ) := ¯ξ\nwith\n¯ξ ∼πe\nwith T being a final state (e.g., a terminal environment state).\nF4: Corrective Feedback — Here, the user has a trajectory showcasing imperfect behavior as a\nreference and needs to provide an improved trajectory. This can be done either implicitly, e.g., by\npushing a robot into a correct position (Mehta & Losey, 2022), or explicitly via specifying a better\naction. We define a correction as:\nFcorr(ξt:t+H) = ∆ξt:t+H,\nwith\nξt:t+H + ∆ξt:t+H ≻ξt:t+H\n3.1.3\nDESCRIPTIVE FEEDBACK\nF5: Descriptive Feedback — The definition of descriptive feedback is more open, and we could\ninterpret any declarative statement about an MDP as descriptive (Rodriguez-Sanchez et al., 2023).\nWe chose a formulation in which descriptive feedback is a qualitative judgment about features,\ncompared to (Sumers et al., 2022a), i.e., information about ”state-action pairs like these are...” As\nsuch, descriptive feedback should not inform about a single trajectory but instead, serve as a way to\nassign reward information about a set of features:\nFdesc(fs ⊆S, fa ⊆A) = vfb ∈R\nF6: Descriptive Preferences — As an extension to descriptions that mirror comparative and\ncorrective feedback, which can be interpreted as relative versions of evaluative and demonstrative\nfeedback, respectively, we propose descriptive preferences. Descriptive preferences are similar to\nfeature preferences proposed before Yuan et al. (2024).\nFdes.pref.((f 1\ns ⊆S, f 1\na ⊆A), (f 2\ns ⊆S, f 2\na ⊆A)) = vfb ∈{≻, ≺, =}\nThese six feedback types are a non-exhaustive selection of human feedback, and many more can\nbe imagined (Jeon et al., 2020; Kaufmann et al., 2024; Metz et al., 2023). However, we argue that\nour selection represents a set of fundamental, general, and explicit human feedback types. Many\nother types of human feedback are implicit (Kovaˇc et al., 2021; Kaufmann et al., 2024), i.e., need\nto be reduced to an explicit reward signal eventually, or collect meta-information that support the\ninterpretation of human feedback (Kaufmann et al., 2024).\n3\n\n\nPublished as a conference paper at ICLR 2025\nπe,1, πe,2, ...\nExpert Models\nRolloutBuffer\nΞR\nξ0\nξ1\nξm\nCheckpoints\nc0, ..., cF\n...\n∀ξ ∈ΞR : r(ξ)\nBinning with #Bins\nFeval(ξ) = #Bins −B(r(ξ))\nRating\nrcomp(ξ1, ξ2) = sgn(r(ξ1) −r(ξ2))\nComparison\nξ ∈ΞR\n∀ξ ∈ΞR : ξdemo := Ea πE[(s0, a0), ...(sH, aH)]\nrdemo(ξ) = ξdemo,t:t+H\nDemonstration\nrcorr(ξ) = (ξ, ξdemo)\nCorrection\nState-Action Clustering\nrdesc(c) = (µ(sc), µ(ac), µ(r(c))\nGenerate optimal traj. with expert policy\nEvaluative Feedback\nDescription\n(rdesc(c1), rdesc(c2), sgn(r(c1) −r(c2))\nDescriptive Preference\nFeedbackDataset\nDR = ∀ξ ∈ΞR : r<type>(ξ)\nrdesc.comp.(c1, c2) =\nc ∈C = KMeans(ΞR, m)\nFigure 1: Generation of Simulated Feedback of different types: Based on an existing expert model\nand rollout buffer, we generate six types of feedback, including ratings and comparisons, demos and\ncorrections, as well as descriptions and descriptive preferences.\n3.2\nSIMULATING MULTI-TYPE HUMAN FEEDBACK\nWe base our experimental evaluation on simulated feedback of different types to enable larger-\nscale environments and ensure reproducibility. While there exists some work to simulate individual\nfeedback types (e.g., preference feedback for locomotion environments, Atari (Brown et al., 2019b)\nor language modeling (Dubois et al., 2024)), pipelines to provide feedback for multiple feedback\ntypes has been limited so far (Yuan et al., 2024; Metz et al., 2023). Yuan et al. describe five feedback\ntypes but only collected annotated datasets for two feedback types.\nTo tackle these challenges, we implement a lightweight * software library to generate feedback of six\ntypes mentioned above. The library is fully interoperable with well-established RL frameworks such\nas Gymnasium (Towers et al., 2024), Stable Baselines (Raffin et al., 2021), and Imitation (Gleave\net al., 2022). It provides utilities for synthetic feedback generation, reward models, and agent training.\n3.2.1\nGENERATION PROCESS\nWe describe a method to generate fixed labeled feedback datasets for reward model training. Our\napproach can also be adapted for dynamic feedback generation with minimal modifications. Figure 1\ngives an overview of the synthetic generation process. We aim to create synthetic feedback for\ntrajectory data based on the six abovementioned types.\nAs the foundation for feedback datasets, we sample segments across 20 model checkpoints from four\nexpert RL models for each environment to ensure diversity, with random start indices and truncation\nat the end of episodes (Details in subsection B.6).\nEvaluative Feedback — We simulate ratings (1-10) based on discounted segment returns, creating a\nmore realistic scenario than raw rewards. Using a calibration set Ξcal := ξ0, ξ1, ..., we implement\nequal-width binning of discounted returns, assigning scores decrementally from highest (10) to\nlowest (1) bins (distribution shown in Figure 11). Online feedback generation compares incoming\nreturns against this binning, with optional dynamic updates to maintain distribution consistency. For\ncomparative feedback, we derive pairwise comparisons from reward differences, excluding pairs with\nsimilar rewards (difference < 10% of return standard deviation).\nInstructive Feedback — For both corrective and demonstrative feedback, we utilize best-performing\nexpert checkpoints. We generate expert trajectories from saved initial states using policy πe, selecting\nthe highest-performing samples from multiple expert models. For corrective feedback, we retain only\ncorrections achieving higher discounted returns than original segments (comparison in Figure 15).\nWhen state resets are unavailable, single-step corrections via expert action queries provide an\nalternative. Demonstrative feedback utilizes these high-quality corrections independently.\nDescriptive Feedback — We implement clustering using mini-batch k-means on concatenated\nstate-action pairs. The number of clusters matches other feedback types’ sample sizes, providing\ncomparable trajectory space resolution. Each cluster’s mean representative serves as the reward\nmodel target, with averaged rewards as description scores. Descriptive preferences are derived from\nreward differences between randomly sampled cluster pairs.\n*The code is available at: https://github.com/ymetz/multi-type-feedback\n4\n\n\nPublished as a conference paper at ICLR 2025\n3.3\nINTRODUCING FEEDBACK NOISE\nWe implement a consistent noise scheme across feedback types based on human feedback rational-\nity modeling (Jeon et al., 2020; Ghosal et al., 2023). To ensure consistency, rather than directly in-\ntroducing noise for each individual feedback type, we introduce noise into the underlying reward\ndistribution, which then influences generated feedback.\nFor evaluative feedback, we add noise via a truncated Gaussian distribution NT (µ, σ, lower, upper):\nffb,eval = ffb + NT (µ = 0, σ = β ∗10, 1, 10)\nPreference labels are flipped according to perturbed rewards, effectively reducing rationality by\nincreasing incorrect preference probability. More nuanced than random flips, this approach primarily\naffects segments with similar rewards.\nFor descriptive feedback, the noise scale is determined by cluster reward ranges:\nffb,descr = r(c) + NT (µ = 0, σ = β ∗|max(rc) −min(rc)|, min(rc), max(rc))\nFor demonstrative feedback, we perturb states and actions based on the observed standard deviations\nin the dataset:\n¯ξ = (si + NT (µ = 0, σ = β ∗σ(s), ai + NT (µ = 0, σ = β ∗σ(a))t:t+H\nIn Appendix B, we detail the feedback dataset generation process and its characteristics to ensure\nreproducibility. We see the documentation of the generated datasets as a key part of our approach, as\nit influences down-stream analysis. An alternative regret-based scheme is discussed in subsection B.5.\nReal human feedback may contain various biases, and we encourage human-subject studies to\ndetermine appropriate values for different feedback types.\n3.4\nREWARD FUNCTIONS FROM FEEDBACK TYPES\nFor our baseline implementation of single reward functions, we closely follow the established\nmethodology from RLHF: For feedback mapping to scalar values (Ratings and Descriptions), our\nreward model is optimized with an MSE-loss between the predicted reward ˆr and annotated scalar\nfeedback values vfb. For relative feedback (such as Comparisons, Corrections and Descriptive\nPreferences), we optimize the reward function with a cross-entropy loss between predicted and\nannotated pairwise preferences.\nLMSE(ˆr) = 1\nn\nX\nξ∈D\n(vfb(ξ) −\nX\ns∈ξ\nˆr(s))2\n(1)\nLMLE(ˆr) = −\nX\n(ξ1,ξ2,µ)∈D\nµ(1) log ˆP[ξ1 ≻ξ2] + µ(2) log ˆP[ξ2 ≻ξ1].\n(2)\nwith ˆP[ξ1 ≻ξ2] following the established Bradley-Terry model (Bradley & Terry, 1952):\nˆP[ξ1 ≻ξ2] =\nexp β P\ni ˆr(o1\ni , a1\ni )\nexp β P\ni ˆr(o1\ni , a1\ni ) + exp β P\ni ˆr(o2\ni , a2\ni )\n(3)\nTo train the reward model, we also model Demonstrative feedback as preferences between the optimal\ndemonstrations and trajectories based on random policies, i.e., random behavior. In our experiments,\nsampling against random behavior has been stronger than sampling against other sub-optimal rollouts.\nWe follow the reward (rational) choice formalism (Jeon et al., 2020) for relative feedback. We discuss\nthe formalization of our proposed feedback types under this framework in Appendix A. For our\nbaseline experiments, we assume optimal reward, i.e., a rationality coefficient of β = ∞. However,\nwe will investigate learning from sub-optimal, more irrational feedback later. In line with existing\nwork (Christiano et al., 2017), we optimize our reward model over trajectory segments ξ, with a\nmaximal length of 50 steps, except for descriptive feedback, which uses single states.\nIn our analysis, we pre-train reward models based on trajectories collected from different checkpoints\nof an RL model, similar to (Brown et al., 2019a). This approach should lead to the model learning the\nreward distribution for a representative set of state-action pairs. For details, we refer to Appendix D.\n5\n\n\nPublished as a conference paper at ICLR 2025\n(ξ, vfb), LMSE\nRatings\n(ξi, ξj, ≺), LMLE\nComparisons\n(ξrand, ¯ξi, ≺), LMLE\nDemonstrations\n(ξi, ξe\ni , ≺), LMLE\nCorrections\n(sc, ac, rc), LMSE\nDescriptions\n((sc,1, ac,1), (sc,2, ac,2)), ≺), LMLE\nDescriptive Comparison\nTable 1: A summary of the reward model implementations for different feedback types, i.e., the\nreward inputs and loss function used to optimize the reward model.\n4\nINVESTIGATING THE EFFECTIVENESS OF DIFFERENT FEEDBACK TYPES\nWe investigate the effectiveness and characteristics of reward learning across different feedback types,\nfocusing on three key research questions: (1) How do various feedback types compare to sampled\npairwise comparisons under both optimal and perturbed conditions? (2) What distinct characteristics\nemerge in reward functions and agent behavior across different feedback types? (3) Does combining\nmultiple feedback types offer advantages over single-type approaches? We explore this through an\nensemble of feedback-type reward functions in RL agent training.\n4.1\nTRAINING SETUP\nWe evaluate our approach on established benchmark environments: Mujoco locomotion environments\n(HalfCheetah-v5, Walker2d-v5, Swimmer-v5, Ant-v5, Hopper-v5, Humanoid-v5), a MetaWorld Yu\net al. (2019) environments (sweep-into-v2), and three discrete action space environment from High-\nwayEnv Leurent (2018): merge-v0, highway-fast-v0, roundabout-v0.\nFor expert model training, we use PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018),\nimplementing tuned hyperparameters from available implementations (Raffin, 2020; Cobbe et al.,\n2019; Machado et al., 2018). The reward model consists of a six-layer MLP with ReLU activations.\nOur feedback datasets comprise 10000 segments of 50 steps or 10000 descriptive clusters. We\ngenerate five separate feedback datasets to control for dataset composition variations due to random\nsampling and environment stochasticity. Reward models undergo supervised pre-training on labeled\ndatasets (see Appendix D).\nWe evaluate learned reward functions using a simplified RLHF setup with a single pre-trained reward\nmodel. Agent training follows a standard PPO configuration (details in Appendix E), with results\naggregated across five random seeds unless specified otherwise. As an additional baseline, we train\npolicies via behavioral cloning on the collected demonstration datasets (details in subsection E.5).\n4.2\nRL WITH LEARNED REWARD FUNCTIONS FROM OPTIMAL FEEDBACK\nInitial reward model training across all feedback types shows effective learning with decreasing\nvalidation loss curves (detailed in Appendix D). All reward types demonstrate steady loss reduction\neven when noise is introduced, with specific effects discussed in the following section.\nFigure 2 demonstrates that different feedback types achieve competitive performance across envi-\nronments. Some feedback types can match the expert model performance for three environments\nwith ground-truth feedback. However, performance is significantly worse in other environments like\nSwimmer-v5 or Ant-v5 than training with a ground-truth reward function. Descriptive feedback and\ndescriptive preferences generally yield the best results, though specific environments show excep-\ntions—for instance, demonstrative feedback outperforms others in Swimmer-v5.\n4.3\nSENSITIVITY TO FEEDBACK VALUE NOISE\nWhile our initial analysis assumed optimal feedback relative to ground-truth rewards, real human\nfeedback inevitably contains errors. We investigate this by introducing controlled noise into the\nfeedback data, using comparable perturbation scales across feedback types.\nFigure 3 illustrates varying sensitivity to noise across feedback types. While the validation loss of\nreward function decreases across feedback types, downstream RL performance varies by feedback\n6\n\n\nPublished as a conference paper at ICLR 2025\n(a) Episode Returns for all Mujoco-Environments\n(b) Episode Returns/Success rate for all Highway-Env and Metaworld\nFigure 2: RL from individual feedback type reward models. The area boundaries indicate minimum\nand maximum values out of the sampled runs. Results are averaged over five random seeds/ feedback\ndatasets. For full training curves see subsection E.2.\n(a) Reward Model Validation Loss: Half-Cheetah-v5\n(b) RL Episode Returns: Half-Cheetah-v5\nFigure 3: Showcasing the influence of noise on different feedback types in the HalfCheetah-v5\nenvironment.\ntype. For Half-Cheetah-v5 feedback seems more susceptible, whereas descriptive feedback is more\nrobust. Notably, moderate noise can sometimes improve downstream RL performance despite\nnegatively affecting reward learning, particularly for corrective feedback. We hypothesize this acts as\na form of label smoothing, encouraging the model to focus on more distinctly different pairs rather\nthan highly similar ones affected by noise. We provide additional results in subsection E.3.\nSummary: — Our experiments demonstrate that various feedback types are effective across different\nenvironments and training stages. Comparative feedback is matched or even surpassed by other\ntypes across environments. While descriptive feedback shows consistently strong performance,\nits real-world collection presents challenges, though approaches using state clustering have been\nproposed Zhang et al. (2022). These findings suggest potential benefits in leveraging multiple\nfeedback types to exploit their respective strengths in specific scenarios.\n4.4\nANALYZING REWARD FUNCTIONS FROM DIFFERENT FEEDBACK TYPES\nBeyond downstream RL agent performance, we analyze the learned reward functions to better\nunderstand the characteristics of different feedback types.\nCorrelation with Ground-Truth Reward Function — Figure 4 illustrates pairwise correlations\nbetween ground-truth and learned reward functions, computed on a separate validation dataset\naggregated across five feedback datasets.\nMost feedback types demonstrate a high correlation with the ground-truth reward function in Mujoco,\n7\n\n\nPublished as a conference paper at ICLR 2025\n(a) Aggregated reward function correlations\n(Mujoco, five seeds, optimal). Details see sub-\nsection D.5\n(b) Relationship between the correlation of ground-truth re-\nward function, learning reward function, and downstream re-\nward (normalized by expert rewards).\nFigure 4: Correlation of Learned Reward Functions for Different Feedback Types: (a) Correlation\ndiffers between types (b) We do not observe a strong relationship between GT correlation and rewards.\nexcept for corrections and demonstrations, which show greater divergence due to their dependence\non expert policy rather than direct derivation from ground-truth. As shown in Figure 4(b), the weak\ncorrelation between ground-truth reward similarity and final reward suggests that alternative feedback\ntypes can uncover different yet effective reward functions. Furthermore, high correlation is itself not\nindicative of downstream performance.\nEffectiveness for Different Noise Levels — Figure 5 demonstrates varying noise sensitivity across\nfeedback types in two environments. Increased noise generally reduces correlation with ground-truth\nrewards. Descriptive feedback shows more stability to noise, consistent with its robust downstream\nRL performance. In contrast, evaluative and comparative rewards exhibit larger performance drops.\nDemonstrative feedback has a lower correlation and is less affected by noise. These effects are also\nvisible across other environments subsection D.6.\n(a) Correlation with ground truth reward function over\ndifferent noise levels for HalfCheetah-v5\n(b) Correlation with ground truth reward function over\ndifferent noise levels for Walker-v5\nFigure 5: Robustness of different reward functions for different noise levels: With increasing noise\nlevels, we see a drop in performance for types, but to a different degree.\nComparing Predictions over Sequences — Analyzing reward predictions across 100-step trajec-\ntories reveals interesting patterns. Figure 6 demonstrates that a high correlation with ground-truth\nrewards does not necessarily translate to strong RL performance. For instance, models achieve near-\nexpert performance in the Humanoid environment despite low reward correlation, particularly with\ndemonstrative feedback. This suggests that effective RL training can occur with various reward func-\ntions independent of their ground-truth correlation.\nSummary: — Our analysis reveals that most feedback types achieve high ground-truth accuracy,\nexcluding demonstrative and corrective. However, even uncorrelated reward functions, like demon-\nstrative feedback, can be highly informative and enable learning in downstream RL. The varying\nperformance characteristics across feedback types, especially with introduced noise, suggest comple-\n8\n\n\nPublished as a conference paper at ICLR 2025\n(a) Sequential reward predictions for Ant-v5\n(b) Sequential reward predictions for Humanoid-v5\nFigure 6: Reward predictions from learned reward functions: (a) Ant-v5 shows high correlation\nwith ground-truth rewards yet suboptimal agent performance. (b) Humanoid-v5 maintains strong\ndownstream performance despite lower reward correlations than other Mujoco environments.\nmentary strengths that could be combined.\n5\nJOINT-MODELING TO LEARN REWARDS FROM MULTI-TYPE HUMAN\nFEEDBACK\nWhile different feedback types effectively learn reward functions, their performance varies across\nscenarios and noise conditions. We explore a proof-of-concept approach combining multiple feedback\ntypes through ensemble methods. Averaging Model — Using our pre-trained reward models, we\nimplement a basic ensemble that averages predictions from all six feedback types. Uncertainty-\nWeighted Ensemble — We extend the basic ensemble by incorporating epistemic uncertainty to\nmitigate underfitting. Using standard deviations from individual reward model ensembles Durasov\net al. (2021), we weigh each feedback type’s predictions based on model inverse uncertainty to\nleverage complementary strengths.\n5.1\nRESULTS\nFigure 7 demonstrates environment-specific success. The ensemble matches the best individual\nfeedback performance in HalfCheetah-v5, significantly outperforming the single-feedback average.\nHowever, Walker2d-v5 shows limited improvement, performing at or below average, indicating room\nfor improvement in complementary learning. These results indicate that the combination of feedback\ntypes might harness the information within the set of feedback types. However, it might fail in other\nscenarios.\nOur preliminary experiments (see subsection C.2) suggest that selective feedback type combinations\noutperform full ensembles, opening promising directions for optimizing feedback type selection and\ncombination strategies. As shown in Figure 7, this approach has limitations, including potentially\nimproper uncertainty calibration, though we expect it to provide full utility and present additional\nchallenges in an online learning setup where specific feedback types might be queried, and we\nstrongly encourage further exploration in this research area.\n6\nDISCUSSION AND OUTLOOK\nOur work yields two key insights: (1) Multiple feedback types effectively enable reward learning\nand RL, with noise-robust results across environments, challenging the presumed superiority of\ncomparative feedback. (2) While simultaneous learning from multiple reward models shows promise,\nfurther research is needed to realize its potential fully.\nOur implementation advances multi-type feedback beyond theoretical frameworks (Jeon et al., 2020).\nWe identify several key directions for future work:\n9\n\n\nPublished as a conference paper at ICLR 2025\n(a) HalfCheetah-v5\n(b) Walker2d-v5\n(c) Ant-v5\nc\n(d) Humanoid-v5\nFigure 7: Comparison between the ensemble (with and without uncertainty-based scaling), average\nreturns across all single feedback RL runs, descriptive and comparative baselines (averaged over 12\nruns). The joint modeling approach can match the best single type but also has failure cases.\nDynamic Reward Models — Transitioning from fixed pre-trained to continuously updating models\ncould enable more dynamic learning processes. This approach could incorporate complex querying\nmechanics Zhan et al. (2023); Jeon et al. (2020); Metz et al. (2023) and leverage different feedback\ntypes at various training stages, building on our weighted ensemble implementation.\nScaling to Complex Domains — While our evaluation spans multiple RL environments, extending\nthese findings to more complex domains remains crucial, particularly LLM alignment and fine-\ntuning. Additionally, expanding to more feedback types presents opportunities and challenges—while\nintegration is straightforward, efficient strategies for managing multiple feedback sources need\ninvestigation.\nHuman Feedback Integration — A natural extension is transitioning to real human feedback,\nwhere adapting synthetic reward generation to observed human feedback patterns could improve\nexperimental validity. In particular, we base our simulation on simple assumptions, such as modeling\nnoise via an additive Gaussian distribution. Fitting characteristics to measurements from human\nexperiments promises more realistic experiments in the future.\nExploring RL from AI Feedback — Strongly related to the concept of synthetic feedback discussed\nin this paper is the use of RL from AI Feedback (RLAIF) Lee et al. (2024); Bai et al. (2022).\nAs human feedback annotations may be hard to acquire at scale, RLAIF based on limited human\nguidance can vastly improve efficiency Liu et al. (2024); Lee et al. (2024). Multi-type feedback can\nenable new ways of integrating various sources of human and AI feedback. Our framework allows\nthe investigation and comparison of new sources and comparison of their effectiveness.\n7\nCONCLUSION\nThis paper demonstrates the effectiveness of multi-type human feedback for reward learning, moving\nbeyond traditional binary preferences. We defined six feedback types and described a protocol\nto generate synthetic feedback. We systematically evaluate learned reward functions and reveal\nsome of their characteristics and complementary strengths in reward learning and downstream RL\nperformance. Our analysis of joint reward models suggests that joint modeling can perform well but\nneeds further exploration. We encourage researchers to explore more diverse human-AI interaction\nparadigms and contribute to the development of more capable and reliable AI systems.\n10\n\n\nPublished as a conference paper at ICLR 2025\nREPRODUCIBILITY STATEMENT\nSource code for all experiments is available in the supplementary materials, and will be open sourced.\nWe used the Stable-Baselines3 Raffin et al. (2021) library (Version 2.3.2) to train all RL expert models\nwith the provided default architectures for MLP- and CNN-based policies. Furthermore, we used the\nRL Baselines3 Zoo Raffin (2020) framework (Version 2.3.2) to train the agents, and used the included\nhyperparamters to train both the expert models and downstream RL models with learned reward\nmodels. Hyperparameters and scores of expert models are reported in subsection B.6. Reward models\nwere implemented and trained as Pytorch Lightning-models, with the training and architecture details\nreported in Appendix D.\nAll experiments were performed on machines with single Nvidia V100 GPUs. Most tasks were CPU-\nbound and required relatively little GPU memory. We estimate that reproducing all experiments\nresults reported in the paper, takes approximately 500 hours on a single node with 4 GPUs and 64\ncores. The runtime for feedback generation of 10,000 steps is approximately around 60 minutes\n(depending on network inference time and environment complexity), fitting of reward models takes\n10-30 minutes. The training times of (expert) RL models are highly dependent on the scenario\n(timesteps and availability of parallel simulation environments). With a learned reward function,\ntraining of a Mujoco environment with SAC and 1 million environment steps takes around one hour.\nTraining with a reward model ensemble does only impose a modest runtime increase, but potentially\na large memory increase (if all reward models are kept in GPU memory).\nACKNOWLEDGEMENTS\nThe authors acknowledge support by the state of Baden-W¨urttemberg through bwHPC. Part of this\nresearch was conducted with financial support from a DAAD Research grants for doctoral students.\nREFERENCES\nPieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In\nProceedings of the Twenty-First International Conference on Machine Learning, ICML ’04, pp.\n1, New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581138385. doi:\n10.1145/1015330.1015430. URL https://doi.org/10.1145/1015330.1015430.\nChristian Arzate Cruz and Takeo Igarashi. A survey on interactive reinforcement learning: Design\nprinciples and open challenges. DIS 2020 - Proceedings of the 2020 ACM Designing Interactive\nSystems Conference, pp. 1195–1209, July 2020. doi: 10.1145/3357236.3395525. URL https://\narxiv.org/abs/2105.12949v1. arXiv: 2105.12949 Publisher: Association for Computing\nMachinery, Inc ISBN: 9781450369749.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado,\nNova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,\nSheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom\nHenighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness\nfrom ai feedback, 2022. URL https://arxiv.org/abs/2212.08073.\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method\nof paired comparisons. Biometrika, 39(3/4):324–345, 1952.\nDaniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating Beyond\nSuboptimal Demonstrations via Inverse Reinforcement Learning from Observations, July 2019a.\nURL http://arxiv.org/abs/1904.06387. arXiv:1904.06387 [cs, stat].\nDaniel S. Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning via\n11\n\n\nPublished as a conference paper at ICLR 2025\nautomatically-ranked demonstrations. In Proceedings of the 3rd Conference on Robot Learning,\n2019b.\nEugene Bykovets, Yannick Metz, Mennatallah El-Assady, Daniel A Keim, and Joachim M Buh-\nmann. How to enable uncertainty estimation in proximal policy optimization. arXiv preprint\narXiv:2210.03649, 2022.\nErdem Bıyık, Dylan P. Losey, Malayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk, and Dorsa\nSadigh. Learning reward functions from diverse sources of human feedback: Optimally integrating\ndemonstrations and preferences. The International Journal of Robotics Research, 41(1):45–67,\nJanuary 2022a. ISSN 0278-3649, 1741-3176. doi: 10.1177/02783649211041652. URL http:\n//journals.sagepub.com/doi/10.1177/02783649211041652.\nErdem Bıyık, Aditi Talati, and Dorsa Sadigh. Aprel: A library for active preference-based reward\nlearning algorithms, 2022b.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\nKarl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation\nto benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: a simulation framework for methods\nthat learn from human feedback. In Proceedings of the 37th International Conference on Neural\nInformation Processing Systems, NIPS ’23, Red Hook, NY, USA, 2024. Curran Associates Inc.\nNikita Durasov, Timur Bagautdinov, Pierre Baque, and Pascal Fua. Masksembles for uncertainty\nestimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 13539–13548, 2021.\nHsiao-Yu Fish, Tung Adam, W Harley, Liang-Kang Huang, and Katerina Fragkiadaki. Reward\nLearning from Narrated Demonstrations. 2018. arXiv: 1804.10692v1.\nGaurav R. Ghosal, Matthew Zurek, Daniel S. Brown, and Anca D. Dragan. The Effect of Modeling\nHuman Rationality Level on Learning Rewards from Multiple Feedback Types, August 2023. URL\nhttp://arxiv.org/abs/2208.10687. arXiv:2208.10687 [cs].\nAdam Gleave, Mohammad Taufeeque, Juan Rocamonde, Erik Jenner, Steven H. Wang, Sam Toyer,\nMaximilian Ernestus, Nora Belrose, Scott Emmons, and Stuart Russell. imitation: Clean imitation\nlearning implementations. arXiv:2211.11972v1 [cs.LG], 2022. URL https://arxiv.org/\nabs/2211.11972.\nShane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea Thomaz.\nPolicy Shaping: Integrating Human Feedback with Reinforcement Learning. Advances in Neural\nInformation Processing Systems, 2013.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International conference\non machine learning, pp. 1861–1870. PMLR, 2018.\nDylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. The off-switch game. In\nProceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI’17, pp.\n220–227. AAAI Press, 2017. ISBN 9780999241103.\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward\nlearning from human preferences and demonstrations in atari. Advances in neural information\nprocessing systems, 31, 2018.\nHong Jun Jeon, Smitha Milli, and Anca D. Dragan. Reward-rational (implicit) choice: A unifying\n12\n\n\nPublished as a conference paper at ICLR 2025\nformalism for reward learning, December 2020.\nURL http://arxiv.org/abs/2002.\n04833. arXiv:2002.04833 [cs].\nTimo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H¨ullermeier. A survey of reinforcement\nlearning from human feedback, 2024. URL https://arxiv.org/abs/2312.14925.\nW. Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The tamer\nframework. In The Fifth International Conference on Knowledge Capture, September 2009. URL\nhttp://www.cs.utexas.edu/users/ai-lab?KCAP09-knox.\nW Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott Niekum, Peter Stone, and Alessan-\ndro Allievi.\nModels of human preference for learning reward functions.\narXiv preprint\narXiv:2206.02231, 2022.\nGrgur Kovaˇc, R´emy Portelas, Katja Hofmann, and Pierre-Yves Oudeyer. SocialAI: Benchmarking\nSocio-Cognitive Abilities in Deep Reinforcement Learning Agents. July 2021. doi: 10.48550/\narxiv.2107.00956. URL https://arxiv.org/abs/2107.00956v3. arXiv: 2107.00956.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton\nBishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs. rlhf: Scaling\nreinforcement learning from human feedback with ai feedback, 2024. URL https://arxiv.\norg/abs/2309.00267.\nEdouard Leurent. An environment for autonomous driving decision-making. https://github.\ncom/eleurent/highway-env, 2018.\nJinyi Liu, Yifu Yuan, Jianye Hao, Fei Ni, Lingzhi Fu, Yibin Chen, and Yan Zheng. Enhancing robotic\nmanipulation with ai feedback from multimodal large language models, 2024. URL https:\n//arxiv.org/abs/2402.14245.\nJames MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts, Matthew E\nTaylor, and Michael L Littman. Interactive learning from policy-dependent human feedback. In\nInternational conference on machine learning, pp. 2285–2294. PMLR, 2017.\nMarlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and\nMichael Bowling. Revisiting the arcade learning environment: evaluation protocols and open\nproblems for general agents. J. Artif. Int. Res., 61(1):523–562, jan 2018. ISSN 1076-9757.\nShaunak A. Mehta and Dylan P. Losey. Unified Learning from Demonstrations, Corrections, and\nPreferences during Physical Human-Robot Interaction, July 2022. URL http://arxiv.org/\nabs/2207.03395. arXiv:2207.03395 [cs].\nYannick Metz, David Lindner, Rapha¨el Baur, Daniel Keim, and Mennatallah El-Assady. Rlhf-blender:\nA configurable interactive interface for learning from diverse human feedback. arXiv preprint\narXiv:2308.04332, 2023.\nAndrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1,\npp. 2, 2000.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback, March 2022.\nURL http://arxiv.org/abs/2203.02155. arXiv:2203.02155 [cs].\nAntonin Raffin. Rl baselines3 zoo. https://github.com/DLR-RM/rl-baselines3-zoo,\n2020.\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah\nDormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of\nMachine Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/\n20-1364.html.\nRafael Rodriguez-Sanchez, Benjamin Adin Spiegel, Jennifer Wang, Roma Patel, Stefanie Tellex,\n13\n\n\nPublished as a conference paper at ICLR 2025\nand George Konidaris. Rlang: a declarative language for describing partial world knowledge to\nreinforcement learning agents. In International Conference on Machine Learning, pp. 29161–\n29178. PMLR, 2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nTheodore R. Sumers, Robert D. Hawkins, Mark K. Ho, Thomas L. Griffiths, and Dylan Hadfield-\nMenell. How to talk so AI will learn: Instructions, descriptions, and autonomy, October 2022a.\nURL http://arxiv.org/abs/2206.07870. arXiv:2206.07870 [cs].\nTheodore R. Sumers, Robert D. Hawkins, Mark K. Ho, Thomas L. Griffiths, and Dylan Hadfield-\nMenell. Linguistic communication as (inverse) reward design. April 2022b. doi: 10.48550/arxiv.\n2204.05091. URL https://arxiv.org/abs/2204.05091v1. arXiv: 2204.05091 ISBN:\n2204.05091v1.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. CoRR,\nabs/1703.01365, 2017. URL http://arxiv.org/abs/1703.01365.\nMark Towers, Ariel Kwiatkowski, Jordan Terry, John U Balis, Gianluca De Cola, Tristan Deleu,\nManuel Goul˜ao, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. Gymnasium: A standard\ninterface for reinforcement learning environments. arXiv preprint arXiv:2407.17032, 2024.\nGarrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep TAMER: Interactive\nAgent Shaping in High-Dimensional State Spaces. 32nd AAAI Conference on Artificial Intelligence,\nAAAI 2018, pp. 1545–1553, September 2017. doi: 10.48550/arxiv.1709.10163. URL https:\n//arxiv.org/abs/1709.10163v2. arXiv: 1709.10163 Publisher: AAAI press ISBN:\n9781577358008.\nB. P. Welford. Note on a method for calculating corrected sums of squares and products. Techno-\nmetrics, 4(3):419–420, 1962. doi: 10.1080/00401706.1962.10490022. URL https://www.\ntandfonline.com/doi/abs/10.1080/00401706.1962.10490022.\nChristian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F¨urnkranz. A survey of preference-\nbased reinforcement learning methods. J. Mach. Learn. Res., 18(1):4945–4990, jan 2017. ISSN\n1532-4435.\nWanqi Xue, Bo An, Shuicheng Yan, and Zhongwen Xu. Reinforcement learning from diverse human\npreferences. arXiv preprint arXiv:2301.11774, 2023.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\nIn Conference on Robot Learning (CoRL), 2019. URL https://arxiv.org/abs/1910.\n10897.\nYifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, and\nYan Zheng. Uni-RLHF: Universal platform and benchmark suite for reinforcement learning with\ndiverse human feedback. In The Twelfth International Conference on Learning Representations,\nICLR, 2024. URL https://openreview.net/forum?id=WesY0H9ghM.\nWenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. How to query human feedback\nefficiently in rl? 2023.\nDavid Zhang, Micah Carroll, Andreea Bobu, and Anca Dragan. Time-Efficient Reward Learning via\nVisually Assisted Cluster Ranking, November 2022. URL http://arxiv.org/abs/2212.\n00169. arXiv:2212.00169 [cs].\n14\n\n\nPublished as a conference paper at ICLR 2025\nA\nDETAILS ON THE FORMALIZATION OF FEEDBACK TYPES\nThis study investigates six exemplary feedback types motivated by existing work (Metz et al., 2023).\nHere, we will briefly model these feedback types according to the reward rational choice formalism\npresented in previous work (Jeon et al., 2020). Table 2 and Table 3 show the choice set, grounding\nfunctions and constraint-based formulations respectively. The formalism proposes that feedback types\ncan be described by an explicit or implicit set of (feedback) choices, that C humans optimize over, and\na grounding function ψ : C →fΞ, which maps these choices c ∈C to a distribution over trajectories.\nTable 2: Feedback types, choice sets, and grounding functions\nFeedback type\nC (Choice set)\nψ : C →Ξ (Grounding func.)\nRating\n(ξ, ffb) ∈{ξ} × [a, b], [a, b] ⊂R\nψ(ξ, ffb) = ξ\nComparative\nξi ∈{ξ1, ξ2}\nid\nCorrective\n(ξ, ∆ξ) ∈{ξ} × {ξc −ξ | ξc ∈Ξ}\nψ(ξ, ∆ξ) = ξ + ∆ξ\nDemonstrative\nξd ∈Ξ\nid\nDescriptive\n(fs ∈S, fa ∈A, r ∈R)\n{(s, a) ∈Ξ|s = fs, a = fa}\nDescr. Pref.\n(fs,i, fa,i) ∈{(fs,1 ∈S, fa,1 ∈A), (fs,2 ∈S, fa,2 ∈A)})\n{(s, a) ∈Ξ|s = fs, a = fa}\nTable 3: Feedback types and corresponding constraints\nFeedback type\nConstraint\nEvaluative\nr(ξ) = ffb\nComparative\n\u001ar(ξ1) ≥r(ξ2)\nif\nξi = ξ1\nr(ξ2) ≥r(ξ1)\nif\nξi = ξ2\nCorrective\nr(ξ + ∆ξ) ≥r(ξ)\nDemonstrative\nr(ξd) ≥r(ξ)\n∀ξ ∈Ξ\nDescription\nr({(s, a) ∈Ξ|s = fs, a = fa}) = vfb\nDescriptive Preference\n\u001ar({(s, a) ∈Ξ|s = fs,1, a = fa,1}) ≥r({(s, a) ∈Ξ|s = fs,2, a = fa,2})\nif\ni = 1\nr({(s, a) ∈Ξ|s = fs,2, a = fa,2}) ≥r({(s, a) ∈Ξ|s =s,1, a = fa,1})\nif\ni = 2\nB\nANALYSIS OF SYNTHETIC GENERATED FEEDBACK\nIn this first appendix, we want to highlight the characteristics of different types of synthetic feedback.\nWe want to discuss details because the simulated feedback is integral to our investigation.\nB.1\nDETAILS OF EVALUATIVE FEEDBACK\nEvaluative feedback is based on discounted segments returns (i.e., the discounted sum of step rewards\nwithin a segment). The discount factors γ match the discount factors of the expert models (as reported\nin subsection B.6). Figure 8 and Figure 9 show the distribution of discounted segments returns for the\nfeedback datasets. The distribution of rewards is a result of the data collection strategy, i.e. collecting\nsegments from a set of checkpoints throughout training. To generate ratings, we use an equal-width\nbinning approach of returns, and assign ratings of 1 to 10 for each bin.\nFigure 8: Histogram of total discounted reward distribution (for 10000 randomly sampled segments\nwith max. length 50/ truncated at episode end). Each color represents a different generated feedback\ndataset.\n15\n\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 1 Generation of Evaluative and Comparative Feedback\nRequire: Environment E, Expert models E = {e1, . . . , en}, Novice model checkpoints Θ =\n{θ1, . . . , θm}, Steps per checkpoint T, Segment length L, Discount factor γ\n1: S ←∅\n▷Set of segments\n2: for θ ∈Θ do\n3:\nπθ ←NoviceModel(θ)\n4:\ns0 ←E.reset()\n5:\nF ←∅, C ←∅\n▷Feedback and state copy collection\n6:\nfor t = 1 to T do\n7:\nC ←E.get state()\n8:\nat ←πθ(st−1)\n9:\nst, rt ←E.step(at)\n10:\nF ←F ∪{(st−1, at, rt)}\n11:\nend for\n12:\nS ←S ∪CreateSegments(F, L)\n13: end for\n14: for σ ∈S do\n15:\nV0 ←\n1\n|E|\nP\ne∈E V e(σ0)\n16:\nVL ←\n1\n|E|\nP\ne∈E V e(σL)\n17:\nRσ ←PL−1\nt=0 γtrt\n▷Discounted reward sum\n18:\nFeval ←10 −BinIndex(Rσ, 10)\n▷Evaluative feedback\n19: end for\n20: Fcomp ←GetPreferencePairs(S, Rσ)\n▷Comparative feedback\n21: return Feval, Fcomp\nFigure 9: Cont. Histogram of total discounted reward distribution (for 10000 randomly sampled\nsegments with max. length 50/ truncated at episode end). Each color represents a different generated\nfeedback dataset.\nFigure 10 shows the extracted ratings (plotted as 0-9) in relation to the underlying rewards.\nFigure 10: Comparison between ratings and total discounted reward (for 10000 randomly sampled\nsegments with max. length 50/ truncated at episode end). Colors indicated the origin model of the\nrating (out of the expert model ensemble). Segments are sorted by discounted return.\nComparative feedback is directly extracted from the and therefore shares the characteristics with\nrating-based feedback. 1 contains the pseudo-code describing the feedback generation process for\nevaluative (rating) feedback, and comparisons.\nFigure 11 and Figure 12 plot the return difference between preferred and non-preferred segments in\nthe dataset. As the segments with higher return is preferred (in the optimal non-noise case), we only\nsee values in the upper-left triangle. We see, that the segments pairs have a wide distribution of return\n16\n\n\nPublished as a conference paper at ICLR 2025\ndifferences, i.e. the dataset contains pairs with very similar or dissimilar returns, i.e. very similar or\ndissimilar levels of performance with respect to the ground truth reward function. Furthermore, pairs\nare collected across a wide range of returns, indicating the effectiveness of our expert model-based\nsegment sampling approach.\nFigure 11: Scatter plot of discounted rewards for preference pairs: On the y-axis, we plot the rewards\nof the preferred segments, on the x-axis are the rewards of the non-preferred segments. Because the\nreturn of the preferred result in the optimal case (Noise 0.0) is always preferred, we only observe\npoints above the diagonal.\nFigure 12: Continuation: Scatter plot of discounted rewards for preference pairs: On the y-axis, we\nplot the rewards of the preferred segments, on the x-axis are the rewards of the non-preferred segments.\nBecause the return of the preferred result in the optimal case (Noise 0.0) is always preferred, we only\nobserve points above the diagonal.\nFigure 13 and Figure 14 show histograms of the segment pair return differences for different environ-\nments.\n17\n\n\nPublished as a conference paper at ICLR 2025\nFigure 13: A histogram showing the distribution of return differences between preference pairs (for\n10000 randomly sampled segments with max. length 50/ truncated at episode end).\nFigure 14: Continuation: A histogram showing the distribution of return differences between\npreference pairs (for 10000 randomly sampled segments with max. length 50/ truncated at episode\nend).\n18\n\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 2 Generation of Demonstrative and Corrective Feedback\nRequire: Environment E, Expert models E = {e1, . . . , en}, State copies C = {c1, . . . , cm}, Seg-\nments S = {s1, . . . , sm}, Segment length L, Discount factor γ\nEnsure: Demonstrative feedback Fdemo, Corrective feedback Fcorr\n1: Fdemo ←∅, Fcorr ←∅\n2: for i ∈{1, . . . , m} do\n3:\nD ←∅\n▷Current demonstrations\n4:\nR ←∅\n▷Current expert returns\n5:\nfor e ∈E do\n6:\ns ←E.set state(ci)\n7:\nd ←∅\n▷Current demonstration\n8:\nfor j ∈{1, . . . , L} do\n9:\na ←e(s)\n10:\ns′, r ←E.step(a)\n11:\nd ←d ∪{(s, a, r)}\n12:\ns ←s′\n13:\nend for\n14:\nD ←D ∪{d}\n15:\nR ←R ∪{PL−1\nt=0 γtrt : (st, at, rt) ∈d}\n16:\nend for\n17:\nk ←arg max R\n18:\nd∗←Dk, r∗←Rk\n19:\nrorig ←PL−1\nt=0 γtrt : (st, at, rt) ∈si\n20:\nif r∗> rorig then\n21:\nFdemo ←Fdemo ∪{d∗}\n22:\nFcorr ←Fcorr ∪{(si, d∗)}\n23:\nend if\n24: end for\n25: return Fdemo, Fcorr\nB.2\nDISTRIBUTION AND EXAMPLES OF DEMONSTRATIVE AND CORRECTIVE FEEDBACK\nA key novelty over existing work is the ability to also generate demonstrations/action advice and\ncorrections dynamically for given segments based on direct access to the expert policy. 2 contains\npseudo-code for the generation of demonstrative and corrective feedback. However, this approach\nraises several questions, which we should investigate: (1) How does the distribution of generated\ndemonstrations/corrections look like? (2) How do they compare to the existing episodes within the\ndataset? (3) How does the choice of an expert policy influence the generated feedback?\nComparison of Returns between Learner and Experts — First, we want to make sure that the\nexpert model policies that we use to correct and demonstrate advanced behavior, actually meaningfully\nimprove over the baseline learner performance. As we can see in Figure 15 and Figure 16, expert\npolicies can improve performance over the learner. However, depending on the situation, we see a\nwide range of achieved rewards for both policy types. This is caused by the initial start state of a\ntrajectory, which might be highly non-optimal, and the expert policy might only be able to recover\nfrom a failure state but not achieve fully optimal reward.\nExamples for Learner and Corrected Trajectories — To further illustrate the difference between\nweak learner trajectories and more optimal expert trajectories, we give some examples: Figure 17-\nFigure 18 highlight the differences in behavior by the weak and strong policy. As expected, we can\nidentify that expert policies better exploit good initial states and are often able to recover from failure\ncases. More video examples are provided in the supplementary material.\nB.3\nDETAILS OF DESCRIPTIVE FEEDBACK\nIn subsection 3.1, we have defined descriptive feedback as a mapping from abstract state-/action-\nfeature values to a feedback value. We have implemented this feedback type via a clustering approach:\n19\n\n\nPublished as a conference paper at ICLR 2025\nFigure 15: Comparison of achieved segment return between learner and expert demonstrations that\nwe use as corrective and demonstrative feedback.\nFigure 16: Comparison of achieved segment return between learner and expert demonstrations that\nwe use as corrective and demonstrative feedback.\nAll collected state-action pairs (s, a) ∈D are clustered into N clusters (with N being the number of\nfeedback instances/queries). In our experiment, we match the number of clusters with the number\nof segments, e.g., 10.000. We utilize mini-batch k-means clustering with a batch size of 1000 to\ncluster the (s, a)-pairs. We then average the observation, action, and optimality gaps for a cluster to\nget a single feedback instance (s, a, vfb) that describes part of the state space without referring to a\nspecific state.\n20\n\n\nPublished as a conference paper at ICLR 2025\nFigure 17: HalfCheetah-v5 - Comparison of key-frames between learner and expert demonstrations,\nthat we use as corrective and demonstrative feedback: Top Row are states from the learner segment,\nbottom row is the expert segment.\nFigure 18: Ant-v5 - Comparison of key-frames between learner and expert demonstrations that we\nuse as corrective and demonstrative feedback: Top Row is stated from the learner segment, bottom\nrow is the expert segment.\nIn Figure 19, we show a sub-sample of state-action pairs (5000 samples) alongside a sample of\nthe computed averaged cluster representatives (500) and the associated step reward for each pair\nmarked in red. In Figure 20, we show t-sne projections of the full set clusters alongside the associated\ndescriptive rewards\nB.4\nLIMITATIONS AND ALTERNATIVE FORMULATIONS\nLastly, the generation scheme has some limitations. The biggest limitation in our mind, is the potential\nmismatch between optimality of the evaluative/descriptive feedback, which is based on the underlying\nreward function, and the demonstrative/corrective feedback that is based on expert policies. While\nthese experts policies were optimized with respect to the equivalent ground-truth discounted reward\nfunctions, they might still be sub-optimal compared to an optimal policy. However, in none of our\nexplored environments, we have access to an optimal expert policy.\nThe feedback datasets were collected over trajectories sampled by rolling out the models from various\ncheckpoints of the expert policies. This can be seen both as a positive or negative thing: On the positive\nthing, this ensures a better comparability between feedback types (including demonstrative/corrective\nfeedback(, because this means feedback is available for roughly the same space of trajectories across\ntypes, and should therefore transport comparable underlying biases and non-optimality. On the flip-\nside, this means that the learned reward functions are only well-defined for the trajectories in the\n21\n\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 3 Generation of Descriptive Feedback\nRequire: Segments S = {s1, . . . , sn}, Number of feedback instances k\nEnsure: Descriptive feedback Fdesc, Descriptive preferences Fdesc.pref.\n1: X ←∅, R ←∅\n2: for s ∈S do\n3:\nX ←X ∪{Concatenate(xi, ai) : (xi, ai, ri) ∈s}\n4:\nR ←R ∪{ri : (xi, ai, ri) ∈s}\n5: end for\n6: C ←KMeans(X, k)\n▷Cluster assignments\n7:\n¯\nX ←{\n1\n|Ci|\nP\nj∈Ci Xj : i ∈{1, . . . , k}}\n▷Cluster representatives\n8: ¯R ←{\n1\n|Ci|\nP\nj∈Ci Rj : i ∈{1, . . . , k}}\n▷Cluster mean rewards\n9: Fdesc ←{( ¯\nXi, ¯Ri) : i ∈{1, . . . , k}}\n▷Cluster descriptions\n10: Fdesc.pref. ←GetPreferencePairs( ¯\nX, ¯R)\n11: return Fdesc, Fdesc.pref.\nFigure 19: t-SNE projection of state-action pairs, colored with the associated ground-truth reward.\nOverlayed, marked by X are computed cluster representatives. These cluster representatives, along-\nside the average reward of underlying pairs, are used as descriptive feedback.\ndatasets, i.e. also better-than-expert trajectories that deviate significantly from the observed behavior,\nmust be considered out-of-distribution. We therefore do not expect that trained RL models can easily\nsurpass expert performance. Indeed, this is mostly supported by the experimental results.\nOur method is well suited for generating fixed, offline training datasets. However, our approach\nrequires some modification to generate online oracle feedback. This includes initializing the histogram\nof ratings based on an initial dataset, but updating it based on online data. Similarly, the clustering\nused to generate descriptive feedback must also be re-computed on a dynamic buffer to adapt to online\ntrajectories. In the provided code repository, we provide an implementation of dynamic feedback\noracle which can be used for online RLHF with simulated multi-type feedback.\nAlternative Formulations: — During the design process, we discussed several alternative ways to\n22\n\n\nPublished as a conference paper at ICLR 2025\n(a) HalfCheetah-v5\n(b) Swimmer-v5\n(c) Walker2d-v5\n(d) Hopper-v5\n(e) Ant-v5\n(f) Humanoid-v5\n(g) merge-v0\n(h) highway-fast-v0\n(i) roundabout-v0\n(j) Metaworld-Sweep-Into-v2\nFigure 20: Descriptive Feedback Analysis for the benchmarked environments. Each subfigure shows\nthe T-SNE visualization of states (left) and the distribution of rewards (right) for the respective\nenvironment.\n23\n\n\nPublished as a conference paper at ICLR 2025\ngenerate synthetic feedback:\n• (Evaluative feedback) Disagreement with expert policy: A possible alternative to using the\nexpert’s value function is to base the feedback directly on the policy. Suppose the selected\naction observed in the segment deviates from the agent’s prediction. In that case, we assume\nit is against the intended series of actions that the expert agent prefers. However, this\nformulation does not have room for alternative solution strategies that might be recognized\nby a human annotator, including segments that are even better than the expert. In both\ncases, the optimality-gap-based approach will lead to low and potentially even negative gaps,\nwhich will be converted into high-score feedback.\n• (Descriptive feedback) The formulation of descriptive feedback is more undetermined\nthan the other types. We explored an alternative approach to assign scores or preferences\nto features instead of segments using feature attribution methods of the value estimator.\nThis allows us to identify features that are relevant for a high/low value. We can then\ninterpret the difference in prediction for the presence/absence of an important feature as a\nreward assignment. However, we have found the attributions generated by methods such as\nIntegrated Gradients (Sundararajan et al., 2017) overly noisy to be effective in our context.\nHowever, in our experiments, the attributions maps proved rather unreliable and only\nprovided a weak signal due to being similar across states. Furthermore, they were only\nsuited to create description of state, and not state-action pairs.\nAn additional approach based on regret, that we explored more extensively, is presented in the\nfollowing.\nB.5\nALTERNATIVE GENERATION OF EVALUATIVE AND COMPARATIVE FEEDBACK WITH\nOPTIMALITY GAPS\nWe extensively explored to use the value function of an expert model as a source of feedback. Existing\nwork (Brown et al., 2019a; Xue et al., 2023; Christiano et al., 2017) utilizing simulated feedback\n(often called an oracle), use the ground-truth reward function of the environment to simulate human\nfeedback.\nIn actor-critic RL algorithms like SAC (Haarnoja et al., 2018) or PPO (Schulman et al., 2017), we\ncan utilize the value V (s) ∈R or q-value Q(s, a) ∈R estimate directly to generate feedback values,\nand the learned policy for demonstrative feedback. Similarly, we can directly transfer this approach\nto purely value-based RL like Q-Learning.\nWe explored an alternative way to generate evaluative feedback on a notion similar to regret, i.e.,\nwe rate a segment not based on the actual collected environment reward but instead on whether\nit showcases optimal behavior from the perspective of an expert model. This notion aligns more\nclosely with reward behavior observed in humans, which have been shown to rate relative to their\nexpectations (MacGlashan et al., 2017; Knox et al., 2022; Jeon et al., 2020). Furthermore, feedback\nbased on ground-truth rewards is not entirely suited for sparse-reward tasks, as we might only observe\na few rewards during a typical segment, resulting in a weak learning signal. The observed regret is\ncomputed by comparing the expected future rewards with the observed ones:\n∆opt(ξ0:H) := Ve(s0) −(\nH−1\nX\ni=0\nγiri + γHVe(sH))\nRegret and Optimality Gap — We want to briefly clarify the relation between the formal regret as\ncommonly defined in RL research and our definition of\nLemma B.1. Equality of Regret and Optimality Gap Under the assumption that the expert policy is\noptimal, the optimality gap ∆e,opt is equivalent to the expected regret for a segment in fixed-horizon\ntasks.\nProof. We can infer this from the common definition of regret RH over a time horizon H (in our\ncase, the length of a segment). With ground-truth reward function r(st, at) and (st, at)H\nt=0 being the\n24\n\n\nPublished as a conference paper at ICLR 2025\nsegment:\nRH := E[\nH\nX\nt=0\nr(st, a∗\nt ) −r(st, at)]\n= E[\nH\nX\nt=0\nr(st, a∗\nt ) −\nH\nX\nt=0\nr(st, at)]\n= E[(\nH\nX\nt=0\nr(st, a∗\nt ) +\nT\nX\nt=H+1\nr(st, a∗\nt ) −\nT\nX\nt=H+1\nR(st, a∗\nt )) −\nH\nX\nt=0\nr(st, at)]\n= E[(\nT\nX\nt=0\nr(st, a∗\nt ) −\nT\nX\nt=H+1\nr(st, a∗\nt )) −\nH\nX\nt=0\nR(st, at)]\n= E[V ∗(s0) −V ∗(sH) −\nH\nX\nt=0\nr(st, at)]\nV ∗=Ve\n=\nE[Ve(s0) −Ve(sH) −\nH\nX\nt=0\nr(st, at)]\nHowever, in infinite horizon tasks, i.e., Mujoco is treated as an infinite horizon task in the common\nimplementations of PPO, we often observe that V e(s0) and V e(sH) are very similar or even equiva-\nlent, except for states that show catastrophic failure, because states cannot be easily differentiated\nfrom each other and therefore will have very similar value estimates. Theorem B.1 also does not hold\nfor the infinite horizon case due to infinite sums.\nIn practice, this means that the optimality gap ∆opt,e is similar to the negative sum of (discounted)\nground truth rewards, i.e., it reduces to the standard implementations of feedback.\nAnother complicating factor is environmental stochasticity. Due to these factors, we chose the term\noptimality gap instead of regret to indicate the distinction.\nComparison with Environment Ground-Truth Feedback — As described above, we use regret-\nbased optimality gap w.r.t., an expert value function, as our foundation for evaluative feedback. As a\nfirst sanity check for the validity of this generated feedback, we investigate the relationship between\nthese optimality gaps and sums of ground-truth reward (akin to existing work): Visible is a linear\ncorrelation between achieved rewards and the negative optimality gap. In particular for non-episodic\ntasks with low influence of stochasticity like Mujoco-environments, negative optimality gaps, and\ndiscounted total rewards almost co-inside, with the imperfect nature of value estimates leading to the\nintroduction of some noise. For some of the other environments, we observe both the difficulty and\npromise of our approach over ground-truth reward: Return (i.e., the sum of (discounted) rewards over\na segment) is an uninformative measure in many environments with sparse rewards (like Atari). In\ncontrast, the simulated reward can serve as a stronger signal.\nThe effect of value-function ensembles on simulated feedback — In the main paper, we use the\nground truth reward function to generate feedback. We have experimented with using multiple trained\nagents to get more reliable value estimates, including a broader coverage and the ability to detect out-\nof-distribution samples that may lead to faulty results. We observed that some models tend to slightly\nover- or under-estimate the return, leading to slightly overlapping bins. We argue that this expert\nmodel noise is compatible with the expected behavior of human labels. Such label noise has been\nreported in crowd-sourcing tasks by human annotators. We average the optimality gap predictions\nacross four models to achieve representative results.\nWe see that the value estimates for austere environments are often close together, and the averaged\nestimates have less noise, which means the estimated optimality gaps more closely mirror the observed\nground-truth returns. For more complex environments, estimates can vary more. In the future, we\nmight want to investigate further increasing the number of expert value functions for even more\nreliable optimality gap estimates.\n25\n\n\nPublished as a conference paper at ICLR 2025\nB.6\nEXPERT POLICY: RL HYPERPARAMETER SETTINGS\nTo train expert models, we chose the default hyperparameter settings of RL training given in Stable-\nBaselines3 Zoo (Raffin, 2020).\nTable 4: RL-Training hyperparameters for Mujoco expert agents trained with PPO.\nHyperparameters for Mujoco Experiments: PPO\nEnvironment\nAnt-v5\nSwimmer-v5\nWalker2d-v5\nAlgorithm\nSAC\nNetwork Arch.\n3-layer MLP\nHidden size\n256\n256\n64\nTraining Timesteps\n1e6\n1e6\n1e6\nNum. Parallel Env.\n1\n4\n1\nBatch Size\n64\n256\n32\nLearning Rate\n2.063e-5\n6e-4\n5e-05\nNum. Epochs\n20\n10\n20\nSampled Steps per Env.\n512\n1024\n512\nGAE lambda\n0.92\n0.98\n0.95\nGamma\n0.98\n0.9999\n0.99\nEnt. Coeff\n0.0004\n0.0554757\n0.0005\nVF Coeff.\n0.581\n0.38782\n0.872\nMax. Grad Norm\n0.8\n0.6\n1.0\nClip Range\n0.1\n0.3\n0.1\nObs. Normalization\nfalse\nReward Normalization\nfalse\nTable 5: RL-Training hyperparameters for Mujoco expert agents trained with SAC.\nHyperparameters for Mujoco Experiments: SAC\nEnvironment\nAnt-v5\nHopper-v5\nHumanoid-v5\nAlgorithm\nSAC\nNetwork Arch.\n3-layer MLP\nHidden size\n256\n256\n256\nTraining Timesteps\n1e6\n1e6\n2e6\nNum. Parallel Env.\n1\n1\n1\nBatch Size\n256\n256\n256\nLearning Rate\n3e-4\n3e-4\n3e-4\nGamma\n0.99\n0.99\n0.99\nEnt. Coeff\n”auto”\n”auto”\n”auto”\nObs. Normalization\nfalse\nReward Normalization\nfalse\nFinally, in Table 8, we report the final evaluation returns by the trained expert policies (no-\ndeterministic sampling, ten episodes). We ran five separate seeds for each environment. We selected\nthe four best-performing seeds as part of the model ensemble for simulated feedback.\n26\n\n\nPublished as a conference paper at ICLR 2025\nTable 6: RL-Training hyperparameters for Highway-Env expert agents trained with PPO.\nHyperparameters for Highway-Env: PPO\nEnvironment\nmerge-v0\nhighway-fast-v5\nroundabout-v5\nAlgorithm\nPPO\nNetwork Arch.\n3-layer MLP\nHidden size\n256\n256\n256\nTraining Timesteps\n1e5\n1e5\n2e5\nNum. Parallel Env.\n64\n64\n64\nBatch Size\n32\n32\n32\nLearning Rate\n5e-4\n5e-4\n5e-4\nNum. Epochs\n10\n10\n10\nGamma\n0.8\n0.8\n0.8\nEnt. Coeff\n0.0\n0.0\n0.0\nObs. Normalization\nfalse\nReward Normalization\nfalse\nTable 7: RL-Training hyperparameters for Metaworld expert agents trained with SAC.\nHyperparameters for Metaworld Experiments: SAC\nEnvironment\nAnt-v5\nAlgorithm\nSAC\nNetwork Arch.\n3-layer MLP\nHidden size\n256\nTraining Timesteps\n1e6\nNum. Parallel Env.\n1\nBatch Size\n256\nLearning Rate\n3e-4\nGamma\n0.99\nEnt. Coeff\n“auto”\nObs. Normalization\nfalse\nReward Normalization\nfalse\nTable 8: Sorted expert evaluation scores for selected environments, with averages. We only retain the\ntop four expert models to generate synthetic feedback.\nHalfCheetah-v5\nWalker2d-v5\nSwimmer-v5\nAnt-v5\nHopper-v5\nHumanoid-v5\n5549.8062\n5860.0417\n359.1049\n4306.0373\n3556.5895\n6254.5279\n5254.2713\n5071.9133\n355.3227\n4132.0158\n3268.0585\n5893.5920\n5124.0117\n4018.3773\n320.6619\n4122.3491\n3250.1369\n5853.7701\n5095.0633\n3717.2990\n317.2805\n3989.7485\n3205.6200\n5682.9198\n4907.9425\n3098.4732\n295.3936\n3601.7154\n3191.8214\n5403.7185\n5186.2190\n4353.2209\n329.5527\n4030.3732\n3294.4453\n5817.7057\n27\n\n\nPublished as a conference paper at ICLR 2025\nB.7\nVISUALIZING THE ARTIFICIAL NOISE IN GENERATED FEEDBACK\nFinally, we want to visualize the effect of introduced noise on the generated feedback. Figure 21\nand Figure 21 shows the effect of truncated noise added onto the reward distribution. Figure 24 shows\nthe impact of noise on the descriptive feedback. We showcase the adaptations for one of the datasets\nin the HalfCheetah-v5 environment.\nFigure 21: Scatterplot displaying original and perturbed rewards: With increasing noise, the underly-\ning reward distribution gets perturbed.\nFigure 22: Histogram displaying original and perturbed rewards: With increasing noise, the underly-\ning reward distribution gets perturbed.\nFigure 23: Scatterplot displaying the rewards of preference pairs: For optimal feedback, pairs\nalways have a perfect relationship between good and bad reward. For increased noise, more and more\npairs get flipped, i.e., indicate a wrong preference w.r.t. the ground-truth reward function.\nOur library includes utilities to investigate and visualize the details of the datasets, including the\nperturbed data. We argue that this kind of transparency is crucial for reproducibility.\nAs a future extension of our work, we may develop common reporting standards for the content of\nfeedback datasets, including sample and rating distributions, as well as underlying assumptions.\n28\n\n\nPublished as a conference paper at ICLR 2025\nFigure 24: Displaying the effect of artificial noise on descriptive feedback: The assigned cluster\nreward values are perturbed by the introduced noise.\nC\nEXPERIMENTS WITH SIMPLIFIED REWARDS\nIn preparation for the main experiments reported in this paper, we ran trials using a method that used\nthe reward predictions for each step separately. In this setup, we adopted reward models trained\nfrom five types (evaluative, comparative, descriptive, corrective, and demonstrative) of synthetically\ngenerated feedback to provide dense rewards to an agent trained using the SAC algorithm in the Half\nCheetah v3 environment. To train the reward models, we employed a slightly modified version of the\ntraining procedure described earlier, where we used feedback given on single steps as inputs to the\nneural network that was trained to predict a reward for that step. In the case of the runs using ensemble\nnetworks, the individual model outputs were averaged to get the final reward for the current step.\n(a) Simple reward networks\n(b) Ensemble reward networks\nFigure 25: RL Training Curves for training from different individual feedback reward models\n(displayed are the ground-truth rewards). The line labeled Expert represents the performance of the\nexpert policies used for generating feedback.\nC.1\nUSING REWARD FUNCTION ENSEMBLES\nThe summary of mean reward curves from 4 training runs is shown in figure 25. First, we compare\nthe training of the simple reward model networks (left) to those trained using ensembles (right). In\nthese figures, we can see that, in some cases, using ensembles dramatically improves performance.\nFor example, after introducing ensembles to the comparative and descriptive feedback models, their\nperformance reached that of the agent trained using environment rewards (labeled Expert), and the\nstandard deviation of the rewards achieved during the four runs also decreased. In other cases, such\nas for evaluative, demonstrative, and corrective feedback models, the performance of the trained RL\n29\n\n\nPublished as a conference paper at ICLR 2025\nagent decreased.\nC.2\nCOMBINATION OF FEEDBACK TYPES BY AVERAGING\nIn addition to using the feedback models by themselves, we can combine them by averaging their\nreward predictions. Additionally, since the magnitudes of these outputs are not on the same scale and\ncan vary, we also need to normalize them before averaging. We keep a rolling mean and standard\ndeviation for this and update them using Welford’s algorithm (Welford, 1962).\n(a) Combination of 2 feedback types\n(b) Combination of 3 feedback types\n(c) Combination of 4 feedback types\nFigure 26: RL Training Curves for training from different combined feedback reward models\n(displayed are the ground-truth rewards). The line labeled as sac halfCheetah-v3 represents the\nperformance of the expert policies used for generating feedback.\nThe results of runs using different numbers of feedback types are in figure 26. From the training\nsessions using the combination of 2 feedback types, we can see that some significantly improved the\nperformance over agents trained using their feedback components, reaching mean rewards similar to\nwhat the expert model obtained while also improving the standard deviation of rewards throughout\nthe different runs. Specifically, all combinations that did not include the corrective feedback type\nachieved better scores than both of their single feedback components.\nWe get a similar picture by looking at the figures of combinations of 3 feedback types, where if we\ndo not include the worst-performing corrective feedback, then the rewards obtained by the trained RL\nagent significantly increase. This might also be the reason for the relatively poor performance of the\nagent trained with four feedback types, including the corrective feedback.\nOverall, combining multiple modalities of feedback, the agents’ scores were higher than those\nobtained using the worst single feedback component - but in many cases, better than all - which\nindicates that the strengths of these individual modalities are combined through mixed-feedback\ntraining.\n30\n\n\nPublished as a conference paper at ICLR 2025\nD\nREWARD MODEL TRAINING DETAILS\nIn this appendix, we want to discuss details of reward model training, including ablations and\nhyperparameters settings.\nD.1\nTRAINING DETAILS\nRewards were trained on the collected feedback datasets in a standard, supervised way. For the\nMujoco environments, we used a 6-layer MLP with 256 hidden units and ReLu activation function.\nWe use the Adam optimizer with a learning rate of 1e −5 and weight decay 1e −2. Models are\ntrained for 100 epochs, with early stopping (5 patience epochs) enabled. We use a batch size of 128.\nBased on best practices from previous work and our preliminary experiments (see subsection C.1),\nwe use reward function ensembles instead of single monolithic models and average the predictions of\nthe submodels. We implemented these ensembles by using Masksembles layers (Durasov et al., 2021)\nafter each fully-connected layer of the network. Similar to techniques like Monte-Carlo Dropout,\na model with Masksembles computes multiple predictions of partially overlapping models during\ninference. We chose a mask number of 4 with a scale factor of 1.8, corresponding to four ensemble\nmembers. Using Masksembles reduced the required training compute and parameter count compared\nto a ”true” reward model ensemble while achieving comparable performance in many cases (Durasov\net al., 2021; Bykovets et al., 2022).\nD.2\nLEARNING FROM NOISY FEEDBACK\nFor our baseline experiments, we have analyzed reward modeling and RL performance for optimal\nfeedback (w.r.t. to the expert models) (Griffith et al., 2013). However, expecting real human\nfeedback to be optimal is a very strong assumption that can generally not be met. We investigated\nthe performance of different feedback types under increasing noise to the generated labels. In\norder to enable meaningful comparison between different feedback types, we designed a consistent\nperturbation approach across feedback types:\nBasic approach: — We modify the underlying reward distribution by adding Gaussian noise of\nvarying degree. The values for evaluative feedback and cluster description reward are updated\naccordingly, and preferences are flipped if the rewards change. The level of noise is controlled by\nsingle parameter β. First, the range of the reward distribution is computed, e.g, [rmin, rmax]. For\nevaluative feedback and corrections, this is the range of discounted segment returns; for descriptive\nfeedback this coincides with the range of single cluster rewards. Each single feedback value is\nthen perturbed by additive noise sampled from a truncated gaussian distribution with µ = vfb and\nσ = β ∗|rmax −rmin]|.\nTruncated Gaussian Distribution: — We use a truncated distribution, to ensure comparability with\nthe binning based approach to generate rating feedback, as well as to respect bounds of observation-\ns/actions, which is relevant for demonstrative feedback. Compared to a Gaussian distribution with\nfixed clipping values, sampling from truncated Gaussian distribution has a lower variance: Based on\na brief analysis of both approaches on a subset of our data, we expect that we need to set the standard\ndeviation sigma of the truncated normal distribution NT to around four times the standard deviation\nof an un-truncated normal distribution N to have a comparable level of perturbation (as measured by\nthe L2-norm between original and perturbed value distribution). The β-values reported in this paper\nare for the truncated normal distribution, thus for replicating the results with non-truncated gaussian\ndistribution for noise would therefore correspond roughly to a value β\n4 .\nRating Feedback — We perturb rating feedback, by adding from truncated Gaussian distribution\nwith mean at the original rating, and the standard deviation controlled by the parameter β ∈R+,\nand truncate the values at the edges of the rating scale. This simulates the effect of a human user\n”incorrectly” switching between two neighboring ratings.\nComparative, Corrective, Descriptive Preferences — For these preference-based feedback types,\nwe assign labels according to the newly perturbed segment return distribution. In effect, this means\nthat preference pairs with similar performance are more often flipped than examples with a clear\ndifference in performance, which matches our intuition. However, this approach might actually\nunderestimate human error, that can be observed in real labeling tasks due to miss-clicks or miss-\n31\n\n\nPublished as a conference paper at ICLR 2025\nunderstanding.\nDemonstrative Feedback — Because we model demonstrative feedback effectively as preference-\nbased feedback, we could also employ label flipping here. However, this does not represent the type\nof error a human labeler would make. Instead, for demonstrative feedback, noise should occur in sub-\noptimal demonstrations, i.e., sub-optimal action selection. For the sake of simplicity, being able to add\ndistortion to already collected data, we do not perform full rollouts with a distorted policy. Instead we\napply additive Gaussian noise on the observations and actions, simulating imperfect demonstrations.\nWe acknowledge that this is a simplistic assumption and will consider further investigations and\nencourage further investigations to enable more human-aligned perturbations.\nDescriptive Feedback — As descriptive feedback maps to scalar rewards, similar to rating feedback,\nwe use a uniform noise schema for rating feedback. Because there are now fixed bounds for ratings\n(i.e., 1-10), we infer the feedback range based on the collected segments within the datasets D and\nsingle-step rewards. Feedback scores are then modified by uniform sampling from a truncated normal\ndistribution with width β ∗|maxD(r) −minD(r)|, conforming to the existing range of β ∈R+.\nTo summarize, we can introduce different noise levels to the different feedback types, controlled\nby a parameter β, corresponding to a comparable degree of perturbation. For our experiments, we\ninvestigate the behavior of reward functions within a range of β from 0.1 to 3.0, A β-value of 3.0\nalready corresponds to a very severe change in distribution.\n32\n\n\nPublished as a conference paper at ICLR 2025\nD.3\nREWARD MODEL TRAINING RESULTS\nShown in Figure 27 and Figure 28, almost all reward models train effectively with optimal feedback,\nachieving low validation loss. We conclude that the reward models can fit the reward data to a high\ndegree, and in turn that our chosen architecture and training protocol are generally appropriate for the\ntask.\n(a) Rating Feedback\n(b) Comparative Feedback\n(c) Demonstrative Feedback\n(d) Corrective Feedback\n(e) Descriptive Feedback\n(f) Descriptive Preference Feedback\nFigure 27: Validation Loss Curves for Different Feedback Types (Mujoco): Steps is the number of\noptimizer steps. Averaged over five feedback datasets.\nWhen analyzing results for MetaWorld and Highway, we observe more variance compared to the\nMujoco-Envs.\n33\n\n\nPublished as a conference paper at ICLR 2025\n(a) Rating Feedback\n(b) Comparative Feedback\n(c) Demonstrative Feedback\n(d) Corrective Feedback\n(e) Descriptive Feedback\n(f) Descriptive Preference Feedback\nFigure 28: Continuation of Validation Loss Curves for Different Feedback Types (Highway-Env,\nMetaWorld): Steps is the number of optimizer steps. Averaged over five feedback datasets.\n34\n\n\nPublished as a conference paper at ICLR 2025\nD.4\nREWARD MODEL TRAINING RESULTS WITH NOISE\nAs shown above, with optimal feedback, all reward models train effectively, achieving low validation\nloss. In the following plots, we show the reward model validation loss for different levels of noise,\nwith β = {0., 1, 0.25, 0.5, 0.75}, averaged across 3 seeds. We can observe, that the introduced noise\nleads to a clear decrease in validation loss.\nFigure 29: Reward Learning Loss Curve for HalfCheetah-v5 at different levels of noise\nFigure 30: Reward Learning Loss Curve for Walker2d-v5 at different levels of noise\n35\n\n\nPublished as a conference paper at ICLR 2025\nFigure 31: Reward Learning Loss Curve for Swimmer-v5 at different levels of noise\nFigure 32: Reward Learning Loss Curve for Ant-v5 at different levels of noise\nFigure 33: Reward Learning Loss Curve for Hopper-v5 at different levels of noise\n36\n\n\nPublished as a conference paper at ICLR 2025\nFigure 34: Reward Learning Loss Curve for Humanoid-v5 at different levels of noise\n37\n\n\nPublished as a conference paper at ICLR 2025\nD.5\nCORRELATION BETWEEN GROUND-TRUTH AND LEARNED REWARD FUNCTIONS\n(a) Noise: 0.25\n(b) Noise: 0.5\n(c) Noise: 0.75\n(d) Noise: 1.5\n(e) Noise: 3.0\nFigure 35: Summary of pairwise reward function correlations between feedback types at different\nnoise levels ( Correlations are averaged across all six Mujoco environments, 3 seeds per env. and\nnoise level)\n38\n\n\nPublished as a conference paper at ICLR 2025\nD.5.1\nCORRELATION ACROSS FEEDBACK TYPES FOR INDIVIDUAL ENVIRONMENTS\n(a) HalfCheetah-v5\n(b) Walker-v5\n(c) Swimmer-v5\n(d) Ant-v5\n(e) Hopper-v5\n(f) Humanoid-v5\nFigure 36: Pairwise correlations between the ground truth and learned reward functions from different\nfeedback types. Optimal feedback without noise.\n39\n\n\nPublished as a conference paper at ICLR 2025\n(a) HalfCheetah-v5\n(b) Walker-v5\n(c) Swimmer-v5\n(d) Ant-v5\n(e) Hopper-v5\n(f) Humanoid-v5\nFigure 37: Pairwise correlations between the ground truth and learned reward functions from different\nfeedback types. Noise level 0.5.\n40\n\n\nPublished as a conference paper at ICLR 2025\nD.6\nINFLUENCE OF NOISE ON CORRELATION WITH GROUND-TRUTH REWARD FUNCTION\n(a) HalfCheetah-v5\n(b) Walker-v5\n(c) Swimmer-v5\n(d) Ant-v5\n(e) Hopper-v5\n(f) Humanoid-v5\nFigure 38: Influence of noise on correlation with ground-truth reward function\n41\n\n\nPublished as a conference paper at ICLR 2025\nD.7\nSEQUENTIAL REWARD PREDICTIONS\n(a) HalfCheetah-v5\n(b) Walker-v5\n(c) Swimmer-v5\n(d) Ant-v5\n(e) Hopper-v5\n(f) Humanoid-v5\nFigure 39: Ground-Truth reward function and reward model predictions for randomly sampled\nsequences.\n42\n\n\nPublished as a conference paper at ICLR 2025\nE\nAGENT TRAINING DETAILS\nE.1\nTRAINING CONFIGURATION\nFor the training of downstream RL-agents, we chose the same hyperparamter configuration as for the\ninitial expert models (see Table 4 and Table 5).\nWe implemented the reward function as a wrapper around the Gymnasium-environment, which\nreplaces the ground-truth environment rewards with the prediction of the reward model. A reward is\npredicted for each observed state-action pair. As mentioned above, for our analysis, we use a single\npre-trained reward model. instead of a consciously updated reward model.\nFor the environments trained with PPO, we chose to standardize the rewards by subtracting a running\nmean, and dividing by the standard deviation. This follows the existing hyperparameters of RL for\nthese environments, as available in StableBaselines3-Zoo (Raffin et al., 2021). The SAC agents are\ntrained with unnormalized rewards for all runs.\nE.2\nDETAILED REWARD CURVES FOR RL TRAINING WITH OPTIMAL REWARD MODELS\n(a) HalfCheetah-v5\n(b) Walker-v5\n(c) Swimmer-v5\n(d) Ant-v5\n(e) Hopper-v5\n(f) Humanoid-v5\nFigure 40: RL reward learning curves for optimal rewards. The shaded area indicates the maximum\nand minimum reward curves. Rewards are aggregated over 5 feedback sets.\n43\n\n\nPublished as a conference paper at ICLR 2025\n(a) merge-v0\n(b) roundabout-v5\n(c) highway-fast-v0\n(d) metaworld-sweep-into-v2\nFigure 41: RL reward learning curves for optimal rewards. The shaded area indicates the maximum\nand minimum reward curves. Rewards are aggregated over 5 feedback sets.\n44\n\n\nPublished as a conference paper at ICLR 2025\nE.3\nDETAILED REWARD CURVES FOR RL TRAINING WITH NOISY REWARD MODELS\nThe following plots show the downstream RL performance for agents trained with reward function\nof multiple noise levels. Rewards are aggregated over 3 seeds per noise level. We can identify two\ntrends: (1) Often, RL performance is surprisingly robust, even as reward model are way less accurate,\nand multiple feedback types are competitive in this regard. Descriptive feedback seems to be the\nmost robust, only being heavily influenced by noise in one environment (Swimmer-v5), but actually\nimproving its’s performance over optimal feedback, which we attribute to an implicit regularization\neffect. (2) We do see that different environments can be more or less susceptible to introduced noise in\ngeneral. Environments like Hopper-v5 and HalfCheetah-v5 seem more robust than an environment\nlike Swimmer-v5.\nFigure 42: RL Reward Curves for HalfCheetah-v5 at different levels of noise\nFigure 43: RL Reward Curves for Walker2d-v5 at different levels of noise.\n45\n\n\nPublished as a conference paper at ICLR 2025\nFigure 44: RL Reward Curves for Swimmer-v5 at different levels of noise\nFigure 45: RL Reward Curves for Ant-v5 at different levels of noise\nFigure 46: RL Reward Curves for Hopper-v5 at different levels of noise\n46\n\n\nPublished as a conference paper at ICLR 2025\nFigure 47: RL Reward Curves for Humanoid at different levels of noise\n47\n\n\nPublished as a conference paper at ICLR 2025\nE.4\nDETAILED REWARD CURVES FOR RL TRAINING WITH VARYING AMOUNTS OF FEEDBACK\nThe following plots show the downstream RL performance for agents trained with varying amounts\nof feedback. We created datasets with fewer feedback instances by sampling random segments/state\nclusters from the full dataset. Rewards are aggregated over 3 seeds per amount of feedback.\nFigure 48: RL Rewards for HalfCheetah-v5 with different amounts of feedback.\nFigure 49: RL Rewards for Walker2d-v5 with different amounts of feedback.\nWe find that all feedback types are sensitive to decreasing number of feedback instances. However we\nstill can highlight some particular observations: Comparative feedback is relatively efficient, showing\nbetter performance with few feedback instances. Other types, like evaluative and descriptive feedback\nshow a noticeable drop-off for fewer instances. However, sub-sampling might impact evaluative and\ndescriptive feedback types in an particular manner: Because the reward histograms, and the clustering\nare computed for the entire dataset, sampling from the full dataset is not representative of recomputed\nstatistics/a recomputed coarse clustering. We therefore report this experiment as an important indictor\nfor the stability of different feedback types, and encourage future exploration.\n48\n\n\nPublished as a conference paper at ICLR 2025\nFigure 50: RL Rewards for Swimmer-v5 with different amounts of feedback.\nFigure 51: RL Rewards for Ant-v5 with different amounts of feedback.\nFigure 52: RL Rewards for Hopper-v5 with different amounts of feedback.\n49\n\n\nPublished as a conference paper at ICLR 2025\nFigure 53: RL Rewards for Humanoid with different amounts of feedback.\nE.5\nBEHAVIORAL CLONING BASELINES\nFor each environment, we also train a baseline via supervised behavioral cloning (BC). As the training\ndataset, we utilize the demonstrations, generated for the feedback data. Segments are flattened, i.e.\n10,100 segments of 50 steps each lead to 500,000 state-action pairs for training of the policy. For the\npolicy network, we use a two-layer neural network with 32 units (as is the default in the imitation\nlibrary Gleave et al. (2022)). Additionally, entropy-regularization is used to avoid over-fitting of the\nsupervised policies. We have trained networks with a batch size of 32, and 20 maximum epochs on\nthe dataset. However, we often did not see significant improvement after around 5 episodes.\nF\nJOINT-REWARD MODEL DETAILS\nF.1\nTRAINING CONFIGURATION\nSimilarly to the other training runs, we chose default training parameters. However, more extensive\nhyper-parameter search might yield better performance in the future.\nFor training, the RL algorithm is unmodified from a normal training setup. We only adapted the reward\nfunction. As before, we implement learning with the multi-type ensemble via a wrapper around the\nenvironment. This time, instead of a single reward function, we load a set of reward functions.\nFor a state-action pair, each individual reward model is queried and predicts a reward. As mentioned\nabove, we standardize the individual reward functions, as they can be in dissimilar value ranges.\nWe do this by standardizing each individual reward function, by subtracting the rolling mean, and\ndividing by the rolling standard deviation. We compute the rolling standard deviation with Welford’s\nalgorithm Welford (1962).\nGiven\nthe\nensemble\nof\nsingle\nfeedback\nreward\nfunctions\nRfb\n=\n{ˆreval, ˆrcomp, ˆrdemo, ˆrcorr, ˆrdescr, ˆrdescr.pref.} we implement two variants:\n• Averaging: We just average the normalized reward into a joint reward prediction\nˆ\nr(s, a) =\n1\n|Rfb|\nX\nrtype∈Rfb\nrtype(s, a)\n• Uncertainty-Weighted Ensemble: Each single feedback reward model is an ensemble in itself\n(i.e., relying on the Masksembles architecture). We can therefore receive an approximate\nposterior probability for each feedback type, i.e. a mean and standard deviation between\nthe sub-model predictions. We experimented with using these standard deviations as\nuncertainties for each feedback type. We then weighted the reward predictions with the\ninverted standard deviation, i.e., a feedback type with lower uncertainty for a state-action\n50\n\n\nPublished as a conference paper at ICLR 2025\npair contributes to the prediction.\nˆµ =\nPn\ni=1\nµi\n√\nσ2\ni\nPn\ni=1\n1\n√\nσ2\ni\n51\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21038v1.pdf",
    "total_pages": 51,
    "title": "Reward Learning from Multiple Feedback Types",
    "authors": [
      "Yannick Metz",
      "András Geiszl",
      "Raphaël Baur",
      "Mennatallah El-Assady"
    ],
    "abstract": "Learning rewards from preference feedback has become an important tool in the\nalignment of agentic models. Preference-based feedback, often implemented as a\nbinary comparison between multiple completions, is an established method to\nacquire large-scale human feedback. However, human feedback in other contexts\nis often much more diverse. Such diverse feedback can better support the goals\nof a human annotator, and the simultaneous use of multiple sources might be\nmutually informative for the learning process or carry type-dependent biases\nfor the reward learning process. Despite these potential benefits, learning\nfrom different feedback types has yet to be explored extensively. In this\npaper, we bridge this gap by enabling experimentation and evaluating multi-type\nfeedback in a broad set of environments. We present a process to generate\nhigh-quality simulated feedback of six different types. Then, we implement\nreward models and downstream RL training for all six feedback types. Based on\nthe simulated feedback, we investigate the use of types of feedback across ten\nRL environments and compare them to pure preference-based baselines. We show\nempirically that diverse types of feedback can be utilized and lead to strong\nreward modeling performance. This work is the first strong indicator of the\npotential of multi-type feedback for RLHF.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}