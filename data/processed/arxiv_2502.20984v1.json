{
  "id": "arxiv_2502.20984v1",
  "text": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP\nModels for Multilingual Multimodal Idiomaticity Representation\nThanet Markchom1 and Tong Wu2 and Liting Huang3 and Huizhi Liang3\n1Department of Computer Science, University of Reading, Reading, UK\n2 Formerly at School of Computing, Newcastle University, Newcastle upon Tyne, UK\n3School of Computing, Newcastle University, Newcastle upon Tyne, UK\nthanet.markchom@reading.ac.uk, tongwuwhitney@gmail.com ,\nL.Huang29@newcastle.ac.uk, huizhi.liang@newcastle.ac.uk\nAbstract\nSemEval-2025 Task 1 focuses on ranking\nimages based on their alignment with a given\nnominal compound that may carry idiomatic\nmeaning in both English and Brazilian Por-\ntuguese. To address this challenge, this work\nuses generative large language models (LLMs)\nand multilingual CLIP models to enhance\nidiomatic compound representations. LLMs\ngenerate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic\ninterpretation.\nThese meanings are then\nencoded using multilingual CLIP models,\nserving as representations for image ranking.\nContrastive learning and data augmentation\ntechniques are applied to ne-tune these\nembeddings\nfor\nimproved\nperformance.\nExperimental results show that multimodal\nrepresentations extracted through this method\noutperformed those based solely on the original\nnominal compounds. The ne-tuning approach\nshows promising outcomes but is less effective\nthan using embeddings without ne-tuning.\nThe source code used in this paper is available\nat\nhttps://github.com/tongwu17/\nSemEval-2025-Task1-UoR-NCL.\n1\nIntroduction\nIn Natural Language Processing (NLP), generating\nrepresentations for idiomatic expressions presents a\nsignicant challenge due to their inherent complex-\nity and non-literal meanings (Phelps et al., 2024).\nTo address this challenge, SemEval-2025 Task 1:\nAdvancing Multimodal Idiomaticity Representa-\ntion (AdMIRe) (Pickard et al., 2025) introduced\ntwo subtasks: Subtask A and Subtask B. Sub-\ntask A involves ranking ve images based on how\nwell they represent the meaning of a potentially\nidiomatic nominal compound in a given context\nsentence, in both English and Brazilian Portuguese.\nThis work focuses on Subtask A.\nExisting NLP models, particularly those based\non transformer architectures such as GPT (Radford\nand Narasimhan, 2018) and CLIP (Contrastive Lan-\nguage–Image Pre-training) (Radford et al., 2021),\nhave made signicant strides in language represen-\ntation (Markchom et al., 2022; Phelps et al., 2024;\nXiong et al., 2024). However, they often struggle\nwith idiomatic expressions due to their reliance\non surface-level word associations and composi-\ntional semantics (He et al., 2024). This problem\nnecessitates further exploration of methods that\ncan improve the models’ capacity to understand\nand represent idioms effectively.\nTo address this issue, this paper uses generative\nLLMs and multilingual CLIP models to tackle Sub-\ntask A in both English and Brazilian Portuguese.\nSpecically, an LLM is used to produce idiomatic\nmeanings for potentially idiomatic compounds.\nThese generated meanings provide richer seman-\ntic information about the idiom and may better\ncapture the compound’s intended meaning com-\npared to its original form. A multilingual CLIP\nmodel is then used to extract embeddings of the\ncompounds (based on their generated meanings)\nand corresponding images to compute similarities\nand rank the images accordingly. Furthermore, to\nimprove the effectiveness of the CLIP embeddings,\nthe extracted embeddings are ne-tuned using a\ncontrastive learning method combined with various\ndata augmentation techniques (rotation, cropping,\nipping, brightness and contrast adjustments, and\nGaussian blur for images and back translation and\nparaphrasing for image captions). By combining\ngenerative LLMs and CLIP models, our approach\noffers a robust framework for generating more ac-\ncurate idiomatic representations for this task.\n2\nProposed Method\nFigure 1 illustrates an overview of the proposed\nmethod. It starts with the idiomatic meaning gen-\neration step, where a generative LLM produces\nidiomatic meanings for potential idiomatic com-\npounds. Next, the embedding extraction and image\n\n\nranking step is described, where compound, im-\nage, and caption embeddings are extracted using\nthe CLIP model and used to compute an image\nranking score. Then, an ensemble method is intro-\nduced to enhance the accuracy of image ranking.\nFinally, a contrastive learning method to ne-tune\nthe extracted CLIP embeddings is described.\n2.1\nIdiomatic Meaning Generation\nAn LLM-based classication method is employed\nto determine whether a given compound phrase\nis used idiomatically or literally. The model is\nqueried with a structured prompt that incorporates\nboth the compound and its contextual sentence, as\nshown in Figure 1. The LLM directly returns a clas-\nsication label (“Idiomatic” or “Literal”) for each\ncompound. To enhance classication robustness,\nthis prompting process is repeated T times, and the\nmajority answer is selected for the nal prediction.\nAfter obtaining the compound type, if it is classi-\ned as “idiomatic”, the meaning of the compound\nis generated by prompting an LLM with the prompt\nshown in Figure 1. This approach enables an auto-\nmated method for generating idiomatic meanings.\n2.2\nEmbedding Extraction and Image\nRanking\nIn this step, the embeddings of the compound, can-\ndidate images, and their corresponding captions are\nextracted using a multilingual CLIP model. For the\ncompound embedding, if its predicted type is “lit-\neral”, the text embedding of the original compound,\nobtained from the CLIP model, is used as the com-\npound embedding. If the type is “idiomatic”, the\ntext embedding of the generated idiomatic mean-\ning from the previous step is used as the compound\nembedding. This ensures that, if the compound\nis idiomatic, its embedding (representation) incor-\nporates additional information that reects its id-\niomatic meaning. The same CLIP model is used to\nextract image embeddings for the candidate images.\nFor caption embeddings, each caption is truncated\nat the end to the maximum input text length of the\nCLIP model, keeping the rst part, and its text em-\nbedding is then extracted. Once all embeddings are\nextracted, the ranking score rc,i of the nominal com-\npound (c) and the candidate image i is computed\nusing the similarity between the compound embed-\nding (ec) and each candidate image embedding (ei)\nalong with its corresponding caption embedding\n(et) as follows: rc,i = s(ec, ei) + s(ec, et) where\ns(·, ·) denotes a similarity function. This work uses\ncosine similarity to avoid magnitude invariance.\n2.3\nEnsemble Method\nWhen generating idiomatic meanings for com-\npounds, multiple LLMs can be utilized to capture\ndiverse interpretations. To further enhance image\nranking, an ensemble approach leveraging multiple\nLLMs is proposed. For each input (a compound, an\nimage, and a caption), each LLM generates its in-\nterpretation of the compound’s idiomatic meaning.\nA ranking score for the images is then computed\nbased on these meanings. The individual scores\nfrom the LLMs are averaged to produce a nal rank-\ning score for each image. This ensemble method\nintegrates insights from multiple LLMs, thereby\nimproving the overall ranking performance.\n2.4\nFine-Tuning with Contrastive Learning\nTo enhance the CLIP embeddings and improve the\nalignment between idiomatic compounds and their\ncorresponding images, ne-tuning is performed\nusing a contrastive learning model.\nData Augmentation\nData augmentation is ap-\nplied to improve the robustness of the ne-tuning\nmodel. Images are randomly cropped to 450×450\npixels (50% probability), rotated within ±45◦(50%\nprobability), and ipped horizontally (50% proba-\nbility) and vertically (50% probability). Brightness\nand contrast are adjusted randomly (20% probabil-\nity), and Gaussian blur is applied (20% probabil-\nity) to simulate noise. For augmenting image cap-\ntions, back translation and paraphrasing techniques\nare used. Back translation is performed using the\nHelsinki-NLP models—opus-mt-de-en and opus-\nmt-en-de—which translate the text from English\nto German and back to English (Tiedemann et al.,\n2023). The google-t5/t5-base (Raffel et al., 2020)\nmodel is used for paraphrasing.\nContrastive Learning Model\nTo train the con-\ntrastive learning model, the dataset is prepared by\nconstructing anchor-positive-negative triplets from\nthe extracted embeddings. The compound embed-\nding of each sample is an anchor. The ground-truth\ntop-ranked image and its associated augmented\nimage, caption, back-translated caption, and para-\nphrased caption are positive samples. Hard neg-\natives are selected from the rest of the images\nand their associated augmented images, captions,\nback-translated captions, and paraphrased captions.\nMoreover, to enhance the learning process, soft\n\n\nFigure 1: Overview of the proposed method: An LLM determines whether a compound is idiomatic or literal\nbased on its context sentence. If idiomatic, the LLM generates its idiomatic meaning. A CLIP model then extracts\nembeddings of the original compound (if literal) or the generated meaning (if idiomatic), along with image and\ncaption embeddings. Finally, cosine similarity is used to compute the ranking score.\nnegatives are randomly selected from other K sam-\nples (other compounds) within the dataset.\nThe contrastive learning model is designed to\nproject the embeddings into a shared latent space\nto maximize the similarity between anchor-positive\npairs and minimize it for anchor-negative pairs.\nThe model consists of a two-layer fully connected\nneural network with ReLU activation and dropout\nregularization. The output is projected into a la-\ntent space with a xed dimensionality of 768. The\nmodel is trained using the InfoNCE-based (Noise\nContrastive Estimation) loss function (Oord et al.,\n2018) where the loss for each sample s is\nLs = −\nPM\nm=1\nh\nlog\nf(a,pm)\nf(a,pm)+PN\nn=1 f(a,nm,n)\ni\nM\n(1)\nwhere\nf(a, pm)\n=\nexp (s(a, pm)/τ)\nand\nf(a, nm,n) = exp (s(a, nm,n)/τ) where M is the\nnumber of positive samples per anchor, N is the\nnumber of negative samples per anchor, a is the\nanchor embedding, pm is the positive sample em-\nbedding for modality m, nm,n is the n-th negative\nsample embedding for modality m, τ is the temper-\nature parameter, and s(·, ·) is the cosine similarity.\nThe total loss is given by 1\nS\nPS\ns=1 Ls, where S is\nthe total number of training samples.\n3\nExperimental Setup\nThree generative LLMs—GPT-3.5, GPT-4, and\nGPT-4o—were used for idiomatic meaning gener-\nation, and three multilingual CLIP models (Carls-\nson et al., 2022)—LABSE ViT-L/14 (LABSE),\nXLM-R Large ViT-B/32 (XLM-32), and XLM-R\nLarge ViT-L/14 (XLM-14)—for embedding gen-\neration. All methods in the experiments, including\nbaselines and variations of the proposed method,\nare categorized as follows: (1) Baselines: CLIP\nmodels applied directly to compounds to compute\nranking scores without LLM-generated meanings;\n(2) Compound and Image without Fine-Tuning\n(CI): Ranking scores computed using only com-\npound and image embeddings. Combinations of\nLLMs and CLIP models, including the ensemble\nmethod, were considered; (3) Compound, Im-\nage, and Caption without Fine-Tuning (CIC):\nRanking scores computed using compound, im-\nage, and caption embeddings. Combinations of\nLLMs and CLIP models were the same as the pre-\nvious approach; (4) Compound and Image with\nFine-Tuning (CI-F): Ranking scores computed\nwith ne-tuned compound and image embeddings\n(see Section 2.4), using the best LLM and CLIP\nmodel combination from the non-ne-tuning ap-\nproaches; (5) Compound, Image, and Caption\n\n\nwith Fine-Tuning (CIC-F): Ranking scores com-\nputed with ne-tuned compound, image, and cap-\ntion embeddings, using the best LLM and CLIP\nmodel combination as in the previous approach.\nDatasets\nTwo datasets, English and Brazilian\nPortuguese, were provided for Subtask A of\nSemEval-2025 Task 1. The English dataset con-\ntains 70 training, 15 development, 15 test, and 100\nextended test samples, while the Portuguese dataset\ncontains 32 training, 10 development, 13 test, and\n55 extended test samples. Fine-tuning was per-\nformed on the augmented training sets (see Section\n2.4). Note that the ne-tuning datasets are based\non ground-truth compound types provided in the\ntraining sets. This avoids misclassication errors\nwhen using the proposed method for compound\ntype prediction. For each language, the augmented\ndata was split into training (70%), validation (10%),\nand test (20%) sets. This resulted in 50 training,\n6 validation, and 14 test samples for English and\n23 training, 3 validation, and 6 test samples for\nBrazilian Portuguese.\nHyperparameter Settings\nThe number of rep-\netitions for prompting the LLM to determine the\ncompound type (T) was set to 5. For CI-F and\nCIC-F, the hyperparameters for contrastive learn-\ning models were varied including batch size (16,\n32), learning rate (1e-3, 1e-4, 1e-5), number of soft\nnegatives K (10, 30, 49), temperature τ (0.08, 0.09,\n0.1), and dropout rate (0.1, 0.3, 0.5). The Adam\noptimizer was used. Early stopping was applied\nbased on validation loss to prevent overtting.\nEvaluation Metrics\nFor the compound-type pre-\ndiction task, accuracy was used for evaluation. For\nthe image ranking task, top-1 accuracy, Spearman’s\nrank correlation and DCG score were used.\n4\nResults and Discussion\n4.1\nCompound Type Detection Results\nTable 1 shows the accuracy of GPT-3.5, GPT-4,\nand GPT-4o on the English and Portuguese training\nsets. From this table, GPT-4 outperformed the other\nmodels on both datasets. This highlights GPT-4’s\nsuperior performance, which may be attributed to\nits more advanced architecture and training. GPT-\n4o also performed well on the English dataset but\nperformed the worst on Portuguese. This lower\nperformance of GPT-4o compared to GPT-4 could\nbe due to the new tokenizer in GPT-4o. This to-\nkenizer compresses tokens to reduce input length\nTable 1: Accuracy of compound type detection using\ndifferent LLMs on English and Portuguese training sets\nModel\nEnglish\nPortuguese\nGPT-3.5\n0.7857\n0.5938\nGPT-4\n0.8714\n0.6563\nGPT-4o\n0.8286\n0.4688\nand improve efciency (OpenAI, 2024). Some\nword sequences that were previously tokenized as\nseparate tokens in GPT-4 could be merged into a\nsingle token in GPT-4o, affecting the model’s abil-\nity to understand a compound’s meaning.\n4.2\nImage Ranking Results\nDue to the small size of the development sets, only\nthe results of the test and extended test sets are\ndiscussed in this section for a comprehensive evalu-\nation. See Appendix B for development set results.\nTable 2 shows the performance of baselines and\nvariations of the proposed method on the complete\ntest sets combining both the test and extended test\nsamples. In this table, all the baselines performed\nworse than the proposed approach. This highlights\nthe effectiveness of the proposed approach in gen-\nerating more effective idiomaticity representations\nfor the image ranking task.\nAs for CI, the results show that the ensemble\nmethod with XLM-32 achieved the best top-1 accu-\nracy and DCG score for English. For Portuguese,\nthe method using GPT-3.5 with LABSE-14 per-\nformed the best in top-1 accuracy and DCG score.\nThis suggests that these methods were particularly\neffective at selecting the most similar images that\nmatched the compounds. In contrast, the ensemble\nmethod using LABSE-14 outperformed the others\nin terms of correlation for both languages. This\nsuggests its potential for capturing nuanced levels\nof similarity between images and compounds.\nConsidering CIC, the methods in this approach\noverall performed worse compared to CI. This sug-\ngests that the addition of caption embeddings with-\nout ne-tuning did not signicantly enhance the\nmodels’ ability to match compounds with images\neffectively. One possible reason is that the cap-\ntions are lengthy, making their embeddings from\nthe CLIP models less effective.\nBased on the results of CI and CIC, LABSE-\n14 demonstrated the highest effectiveness in rank-\ning. Consequently, the embeddings obtained using\nLABSE-14 with different LLMs were ne-tuned\nin CI-F and CIC-F. Multiple contrastive models\n\n\nTable 2: Evaluation results on the complete test sets\n(test and extended test sets combined) for both English\n(EN) and Brazilian Portuguese (PT). The highest values\nin each column are highlighted in bold.\nTest EN\nTest PT\nLLM\nCLIP model\nAcc\nCorr\nDCG\nAcc\nCorr\nDCG\nBaselines\n-\nXLM-14\n0.400\n0.050\n2.659\n0.351\n0.130\n2.584\n-\nXLM-32\n0.417\n0.053\n2.655\n0.398\n0.118\n2.649\n-\nLABSE-14\n0.409\n0.126\n2.648\n0.445\n0.161\n2.666\nCompound and Image without Fine-Tuning (CI)\nGPT-3.5\nXLM-14\n0.478\n0.165\n2.831\n0.430\n0.095\n2.732\nGPT-4\nXLM-14\n0.504\n0.126\n2.906\n0.418\n0.157\n2.749\nGPT-4o\nXLM-14\n0.478\n0.106\n2.898\n0.418\n0.137\n2.766\nEnsemble\nXLM-14\n0.513\n0.143\n2.919\n0.376\n0.166\n2.731\nGPT-3.5\nXLM-32\n0.435\n0.102\n2.757\n0.487\n0.107\n2.823\nGPT-4\nXLM-32\n0.539\n0.183\n2.897\n0.414\n0.138\n2.732\nGPT-4o\nXLM-32\n0.513\n0.171\n2.899\n0.481\n0.172\n2.829\nEnsemble\nXLM-32\n0.557\n0.122\n2.939\n0.450\n0.175\n2.798\nGPT-3.5\nLABSE-14\n0.470\n0.177\n2.816\n0.530\n0.184\n2.846\nGPT-4\nLABSE-14\n0.496\n0.163\n2.883\n0.471\n0.178\n2.778\nGPT-4o\nLABSE-14\n0.504\n0.187\n2.899\n0.481\n0.194\n2.825\nEnsemble\nLABSE-14\n0.522\n0.195\n2.913\n0.487\n0.198\n2.831\nCompound, Image, and Caption without Fine-Tuning (CIC)\nGPT-3.5\nXLM-14\n0.287\n0.043\n2.480\n0.315\n0.005\n2.503\nGPT-4\nXLM-14\n0.296\n0.052\n2.491\n0.305\n0.009\n2.495\nGPT-4o\nXLM-14\n0.296\n0.063\n2.573\n0.315\n0.023\n2.530\nEnsemble\nXLM-14\n0.287\n0.061\n2.509\n0.293\n0.071\n2.490\nGPT-3.5\nXLM-32\n0.313\n0.050\n2.549\n0.384\n0.132\n2.632\nGPT-4\nXLM-32\n0.357\n0.074\n2.594\n0.368\n0.107\n2.623\nGPT-4o\nXLM-32\n0.365\n0.107\n2.650\n0.384\n0.067\n2.651\nEnsemble\nXLM-32\n0.365\n0.032\n2.626\n0.384\n0.067\n2.640\nGPT-3.5\nLABSE-14\n0.252\n0.044\n2.465\n0.293\n0.059\n2.515\nGPT-4\nLABSE-14\n0.278\n0.064\n2.525\n0.277\n0.088\n2.477\nGPT-4o\nLABSE-14\n0.330\n0.072\n2.591\n0.277\n0.076\n2.501\nEnsemble\nLABSE-14\n0.278\n0.066\n2.525\n0.293\n0.089\n2.501\nCompound and Image with Fine-Tuning (CI-F)\nGPT-3.5\nLABSE-14\n0.391\n0.027\n2.709\n-\n-\n-\nGPT-4\nLABSE-14\n0.400\n0.079\n2.778\n-\n-\n-\nGPT-4o\nLABSE-14\n0.365\n0.056\n2.707\n-\n-\n-\nCompound, Image, and Caption with Fine-Tuning (CIC-F)\nGPT-3.5\nLABSE-14\n0.391\n0.053\n2.697\n-\n-\n-\nGPT-4\nLABSE-14\n0.417\n0.155\n2.813\n-\n-\n-\nGPT-4o\nLABSE-14\n0.374\n0.084\n2.722\n-\n-\n-\nwere trained on individual sets of embeddings from\nvarious LLMs. The selected hyperparameters for\neach model can be found in Appendix A. Overall,\nthe ne-tuned embeddings did not perform as well\nas the non-ne-tuned embeddings. Figure 2 shows\nthe training and validation losses, as well as the test\naccuracy, during the ne-tuning of embeddings ob-\ntained using LABSE-14 with GPT-3.5, GPT-4, and\nGPT-4o. These gures suggest that the models ef-\nfectively learned the ne-tuned embeddings, as test\naccuracy gradually increased over training epochs.\nHowever, the models began overtting before the\ntest accuracy could improve further. This could be\ndue to the amount of training data being insufcient\nfor the model to generalize well to unseen data.\nDue to the lack of performance improvement on\nthe English dataset during ne-tuning, experiments\non the Portuguese dataset were not conducted.\nMore detailed results on the individual test and\n(a) GPT-3.5\n(b) GPT-4\n(c) GPT-4o\nFigure 2: Training loss, validation loss, and test ac-\ncuracy, during the ne-tuning of embeddings obtained\nusing LABSE-14 with GPT-3.5, GPT-4, and GPT-4o.\nextended test sets for both languages can be found\nin Appendix B (Table 5).\n5\nConclusions\nThis work explored the use of generative LLMs and\nmultilingual CLIP models to enhance idiomatic\ncompound representations for image ranking in\nSemEval-2025 Task 1. By using LLMs to gener-\nate idiomatic meanings and leveraging multilingual\nCLIP models to extract multimodal embeddings,\nthe proposed method improved representation qual-\nity compared to using original nominal compounds.\nExperimental results demonstrated the effective-\nness of the proposed method. For English, the\nensemble method using GPT-3.5, GPT-4, and GPT-\n4o, with the XLM-R Large ViT-B/32 multilingual\nCLIP model achieved superior performance com-\npared to the other selected LLMs and CLIP mod-\nels. For Brazilian Portuguese, GPT-3.5 with the\nLABSE ViT-L/14 multilingual CLIP model outper-\nformed the others. Fine-tuning CLIP embeddings\nperformed worse than using embeddings extracted\nfrom pretrained CLIP models. This is likely due to\nlimitations in ne-tuning data and the capacity of\nthe proposed contrastive learning model. However,\nit could still be a promising approach for further\nimprovement. Future work could focus on rening\nne-tuning strategies and expanding training data\nto further enhance idiomaticity representation.\n\n\nReferences\nFredrik Carlsson, Philipp Eisen, Faton Rekathati, and\nMagnus Sahlgren. 2022. Cross-lingual and multilin-\ngual clip. In Proceedings of the Language Resources\nand Evaluation Conference. European Language Re-\nsources Association.\nWei He, Marco Idiart, Carolina Scarton, and Aline\nVillavicencio. 2024. Enhancing idiomatic representa-\ntion in multiple languages via an adaptive contrastive\ntriplet loss. arXiv preprint arXiv:2406.15175.\nThanet Markchom, Huizhi Liang, and Jiaoyan Chen.\n2022.\nUoR-NCL at SemEval-2022 task 3: Fine-\ntuning the BERT-based models for validating taxo-\nnomic relations. In Proceedings of the 16th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2022), pages 260–265, Seattle, United States. Asso-\nciation for Computational Linguistics.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nOpenAI. 2024. Hello GPT-4o.\nDylan Phelps, Thomas M. R. Pickard, Maggie Mi,\nEdward Gow-Smith, and Aline Villavicencio. 2024.\nSign of the times: Evaluating the use of large lan-\nguage models for idiomaticity detection. In Proceed-\nings of the Joint Workshop on Multiword Expressions\nand Universal Dependencies, Italia.\nThomas Pickard, Aline Villavicencio, Maggie Mi, Wei\nHe, Dylan Phelps, Carolina Scarton, and Marco Idiart.\n2025. Semeval-2025 task 1: Admire - advancing\nmultimodal idiomaticity representation. In Proceed-\nings of the 19th International Workshop on Semantic\nEvaluations (SemEval-2025), Vienna, Austria. Asso-\nciation for Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nAlec Radford and Karthik Narasimhan. 2018.\nIm-\nproving language understanding by generative pre-\ntraining.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unied text-to-text trans-\nformer. Journal of Machine Learning Research.\nJörg Tiedemann, Mikko Aulamo, Daria Bakshandaeva,\nMichele Boggia, Stig-Arne Grönroos, Tommi Niem-\ninen, Alessandro Raganato Yves Scherrer, Raul\nVazquez, and Sami Virpioja. 2023. Democratizing\nneural machine translation with OPUS-MT. Lan-\nguage Resources and Evaluation, (58):713–755.\nFeng Xiong, Thanet Markchom, Ziwei Zheng, Subin\nJung, Varun Ojha, and Huizhi Liang. 2024. NCL-\nUoR at SemEval-2024 task 8: Fine-tuning large lan-\nguage models for multigenerator, multidomain, and\nmultilingual machine-generated text detection. In\nProceedings of the 18th International Workshop on\nSemantic Evaluation, Mexico.\nA\nHyperparameters Settings\nTable 3 shows the selected hyperparameters for\ndifferent contrastive learning models in the CI-F\nand CIC-F approaches.\nTable 3: The selected hyperparameters for different\ncontrastive learning models.\nModel\nBatch\nSize\nLearning\nRate\nK\nτ\nDropout\nRate\nGPT-3.5 + LABSE-14\n16\n1e-5\n49\n0.1\n0.1\nGPT-4 + LABSE-14\n16\n1e-5\n30\n0.1\n0.3\nGPT-4o + LABSE-14\n16\n1e-4\n10\n0.08\n0.5\nB\nDetailed Evaluation Results\nTable 4 shows the results on English and Por-\ntuguese development sets.\nTable 4: Evaluation results for English (EN) and Por-\ntuguese (PT) development sets, with the highest values\nin bold and the second-highest underlined.\nLLM\nCLIP model\nDev EN\nDev PT\nAcc\nCorr\nDCG\nAcc\nCorr\nDCG\nUse only compound and image embeddings without ne-tuning\nGPT-3.5\nXLM-14\n0.600\n0.313\n3.055\n0.400\n0.320\n2.620\nGPT-4\nXLM-14\n0.533\n0.193\n2.818\n0.400\n0.220\n2.562\nGPT-4o\nXLM-14\n0.600\n0.233\n2.943\n0.400\n0.220\n2.582\nEnsemble\nXLM-14\n0.600\n0.353\n3.005\n0.400\n0.260\n2.582\nGPT-3.5\nXLM-32\n0.733\n0.427\n3.219\n0.400\n0.160\n2.487\nGPT-4\nXLM-32\n0.533\n0.273\n2.794\n0.300\n0.230\n2.375\nGPT-4o\nXLM-32\n0.600\n0.293\n2.918\n0.300\n0.050\n2.338\nEnsemble\nXLM-32\n0.667\n0.260\n3.005\n0.300\n0.110\n2.375\nGPT-3.5\nLABSE-14\n0.600\n0.293\n3.006\n0.200\n0.280\n2.376\nGPT-4\nLABSE-14\n0.533\n0.153\n2.781\n0.300\n0.280\n2.469\nGPT-4o\nLABSE-14\n0.600\n0.153\n2.993\n0.300\n0.280\n2.413\nEnsemble\nLABSE-14\n0.600\n0.253\n2.919\n0.300\n0.240\n2.450\nUse compound image and caption embeddings without ne-tuning\nGPT-3.5\nXLM-14\n0.400\n0.013\n2.682\n0.300\n-0.120\n2.452\nGPT-4\nXLM-14\n0.400\n0.040\n2.645\n0.300\n-0.030\n2.452\nGPT-4o\nXLM-14\n0.467\n0.273\n2.719\n0.300\n-0.080\n2.452\nEnsemble\nXLM-14\n0.400\n0.087\n2.682\n0.300\n-0.100\n2.452\nGPT-3.5\nXLM-32\n0.533\n0.240\n2.970\n0.300\n-0.070\n2.508\nGPT-4\nXLM-32\n0.467\n0.173\n2.719\n0.300\n-0.030\n2.508\nGPT-4o\nXLM-32\n0.533\n0.267\n2.857\n0.200\n-0.020\n2.396\nEnsemble\nXLM-32\n0.533\n0.260\n2.794\n0.300\n-0.050\n2.508\nGPT-3.5\nLABSE-14\n0.400\n-0.140\n2.671\n0.200\n-0.210\n2.378\nGPT-4\nLABSE-14\n0.467\n-0.020\n2.707\n0.200\n-0.130\n2.378\nGPT-4o\nLABSE-14\n0.467\n-0.067\n2.682\n0.200\n-0.190\n2.378\nEnsemble\nLABSE-14\n0.400\n-0.013\n2.620\n0.200\n-0.170\n2.378\nUse only compound and image embeddings with ne-tuning\nGPT-3.5\nLABSE-14\n0.600\n0.213\n3.159\n-\n-\n-\nGPT-4\nLABSE-14\n0.600\n0.107\n3.019\n-\n-\n-\nGPT-4o\nLABSE-14\n0.667\n0.187\n3.131\n-\n-\n-\nUse compound image and caption embeddings with ne-tuning\nGPT-3.5\nLABSE-14\n0.600\n0.127\n3.158\n-\n-\n-\nGPT-4\nLABSE-14\n0.533\n0.047\n2.844\n-\n-\n-\nGPT-4o\nLABSE-14\n0.600\n0.113\n3.005\n-\n-\n-\n\n\nTable 5: Evaluation results on the English (EN), Portuguese (PT), Extended English (XE) and Extended Portuguese\n(XP) test sets. The highest values in each column are in bold, and the second-highest values are underlined.\nLLM\nCLIP model\nTest EN\nTest PT\nTest XE\nTest XP\nAcc\nCorr\nDCG\nAcc\nCorr\nDCG\nAcc\nCorr\nDCG\nAcc\nCorr\nDCG\nBaselines\n-\nXLM-14\n0.333\n-0.027\n2.579\n0.385\n0.415\n2.661\n0.410\n0.062\n2.671\n0.345\n0.087\n2.573\n-\nXLM-32\n0.267\n-0.173\n2.482\n0.385\n0.223\n2.669\n0.440\n0.087\n2.681\n0.400\n0.102\n2.646\n-\nLABSE-14\n0.467\n0.120\n2.706\n0.385\n0.146\n2.578\n0.400\n0.127\n2.639\n0.455\n0.164\n2.680\nUse only compound and image embeddings without ne-tuning\nGPT-3.5\nXLM-14\n0.533\n0.220\n2.943\n0.385\n0.415\n2.637\n0.470\n0.157\n2.815\n0.436\n0.047\n2.746\nGPT-4\nXLM-14\n0.533\n0.133\n2.970\n0.538\n0.354\n2.951\n0.500\n0.125\n2.897\n0.400\n0.127\n2.719\nGPT-4o\nXLM-14\n0.467\n0.193\n2.867\n0.538\n0.285\n3.045\n0.480\n0.093\n2.903\n0.400\n0.115\n2.724\nEnsemble\nXLM-14\n0.533\n0.233\n2.921\n0.462\n0.269\n2.792\n0.510\n0.130\n2.919\n0.364\n0.151\n2.722\nGPT-3.5\nXLM-32\n0.333\n-0.013\n2.690\n0.462\n0.131\n2.749\n0.450\n0.119\n2.767\n0.491\n0.104\n2.834\nGPT-4\nXLM-32\n0.533\n0.167\n2.940\n0.385\n0.223\n2.747\n0.540\n0.186\n2.891\n0.418\n0.125\n2.729\nGPT-4o\nXLM-32\n0.467\n0.087\n2.849\n0.538\n0.092\n2.953\n0.520\n0.184\n2.907\n0.473\n0.184\n2.810\nEnsemble\nXLM-32\n0.467\n0.053\n2.821\n0.538\n0.169\n2.866\n0.570\n0.132\n2.957\n0.436\n0.176\n2.788\nGPT-3.5\nLABSE-14\n0.667\n0.360\n3.102\n0.308\n0.123\n2.486\n0.440\n0.149\n2.773\n0.564\n0.193\n2.900\nGPT-4\nLABSE-14\n0.600\n0.147\n2.993\n0.462\n0.131\n2.771\n0.480\n0.165\n2.867\n0.473\n0.185\n2.779\nGPT-4o\nLABSE-14\n0.533\n0.267\n2.963\n0.538\n0.223\n2.947\n0.500\n0.175\n2.889\n0.473\n0.189\n2.807\nEnsemble\nLABSE-14\n0.600\n0.247\n2.985\n0.462\n0.269\n2.691\n0.510\n0.187\n2.902\n0.491\n0.187\n2.852\nUse compound image and caption embeddings without ne-tuning\nGPT-3.5\nXLM-14\n0.333\n0.087\n2.566\n0.231\n0.023\n2.337\n0.280\n0.037\n2.468\n0.327\n0.002\n2.527\nGPT-4\nXLM-14\n0.400\n0.153\n2.645\n0.154\n0.054\n2.278\n0.280\n0.037\n2.468\n0.327\n0.002\n2.527\nGPT-4o\nXLM-14\n0.333\n0.153\n2.888\n0.231\n0.046\n2.378\n0.290\n0.050\n2.525\n0.327\n0.020\n2.553\nEnsemble\nXLM-14\n0.333\n0.113\n2.566\n0.308\n0.092\n2.433\n0.280\n0.053\n2.501\n0.291\n0.067\n2.498\nGPT-3.5\nXLM-32\n0.267\n0.060\n2.456\n0.154\n0.223\n2.344\n0.320\n0.049\n2.563\n0.418\n0.118\n2.675\nGPT-4\nXLM-32\n0.333\n0.047\n2.527\n0.154\n0.215\n2.344\n0.360\n0.078\n2.603\n0.400\n0.091\n2.665\nGPT-4o\nXLM-32\n0.267\n0.127\n2.456\n0.154\n0.262\n2.354\n0.380\n0.104\n2.680\n0.418\n0.038\n2.695\nEnsemble\nXLM-32\n0.267\n0.020\n2.448\n0.154\n0.223\n2.344\n0.380\n0.034\n2.653\n0.418\n0.044\n2.685\nGPT-3.5\nLABSE-14\n0.333\n0.027\n2.569\n0.308\n-0.008\n2.440\n0.240\n0.047\n2.450\n0.291\n0.069\n2.526\nGPT-4\nLABSE-14\n0.333\n0.007\n2.562\n0.308\n0.023\n2.480\n0.270\n0.073\n2.520\n0.273\n0.098\n2.477\nGPT-4o\nLABSE-14\n0.333\n0.027\n2.569\n0.308\n0.077\n2.530\n0.330\n0.079\n2.594\n0.273\n0.076\n2.497\nEnsemble\nLABSE-14\n0.333\n-0.020\n2.567\n0.308\n0.054\n2.496\n0.270\n0.079\n2.519\n0.291\n0.095\n2.502\nUse only compound and image embeddings with ne-tuning\nGPT-3.5\nLABSE-14\n0.400\n0.107\n2.814\n-\n-\n-\n0.390\n0.015\n2.694\n-\n-\n-\nGPT-4\nLABSE-14\n0.333\n0.233\n2.784\n-\n-\n-\n0.410\n0.056\n2.777\n-\n-\n-\nGPT-4o\nLABSE-14\n0.267\n-0.073\n2.676\n-\n-\n-\n0.380\n0.075\n2.711\n-\n-\n-\nUse compound image and caption embeddings with ne-tuning\nGPT-3.5\nLABSE-14\n0.333\n0.051\n2.719\n-\n-\n-\n0.400\n0.053\n2.694\n-\n-\n-\nGPT-4\nLABSE-14\n0.267\n0.133\n2.724\n-\n-\n-\n0.440\n0.158\n2.826\n-\n-\n-\nGPT-4o\nLABSE-14\n0.267\n0.040\n2.607\n-\n-\n-\n0.390\n0.091\n2.740\n-\n-\n-\nTable 5 shows detailed evaluation results for the\nbaselines and variations of proposed method on the\nEnglish (EN), Portuguese (PT), Extended English\n(XE), and Extended Portuguese (XP) test sets.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20984v1.pdf",
    "total_pages": 7,
    "title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation",
    "authors": [
      "Thanet Markchom",
      "Tong Wu",
      "Liting Huang",
      "Huizhi Liang"
    ],
    "abstract": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}