{
  "id": "arxiv_2502.21017v1",
  "text": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind\nin Persuasive Dialogues\nFangxu Yu1\nLai Jiang 2\nShenyi Huang 3\nZhen Wu1*\nXinyu Dai1\n1National Key Laboratory for Novel Software Technology, Nanjing University, China\n1School of Artificial Intelligence, Nanjing University, China\n2Department of Computer Science and Engineering, Shanghai Jiao Tong University\n3University of California, San Diego, CA, USA\nyufx@smail.nju.edu.cn\njianglai0023-sjth@sjtu.edu.cn\nshh058@ucsd.edu\n{wuz, daixinyu}@nju.edu.cn\nAbstract\nThe ability to understand and predict the mental\nstates of oneself and others, known as the The-\nory of Mind (ToM), is crucial for effective so-\ncial interactions. Recent research has emerged\nto evaluate whether Large Language Models\n(LLMs) exhibit a form of ToM. Although recent\nstudies have evaluated ToM in LLMs, existing\nbenchmarks focus predominantly on physical\nperception with principles guided by the Sally-\nAnne test in synthetic stories and conversations,\nfailing to capture the complex psychological\nactivities of mental states in real-life social in-\nteractions. To mitigate this gap, we propose\nPERSUASIVETOM, a benchmark designed to\nevaluate the ToM abilities of LLMs in persua-\nsive dialogues. Our framework introduces two\ncategories of questions: (1) ToM Reasoning, as-\nsessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees),\nand (2) ToM Application, evaluating whether\nLLMs can take advantage of inferred mental\nstates to select effective persuasion strategies\n(e.g., emphasize rarity) and evaluate the effec-\ntiveness of persuasion strategies. Experiments\nacross eight state-of-the-art LLMs reveal that\nwhile models excel on multiple questions, they\nstruggle to answer questions that need tracking\nthe dynamics and shifts of mental states and\nunderstanding the mental states in the whole\ndialogue comprehensively. Our aim with PER-\nSUASIVETOM is to allow an effective evalua-\ntion of the ToM reasoning ability of LLMs with\nmore focus on complex psychological activities.\nOur code is available at https://github.com/Yu-\nFangxu/PersuasiveToM.\n1\nIntroduction\nTheory of Mind (ToM) involves the ability to rea-\nson about mental states both in oneself and in oth-\ners (Premack and Woodruff, 1978). This capac-\nity strengthens many aspects of human cognition\nand social reasoning, enabling individuals to infer\n*\nCorresponding author.\n……\nHey Alice, how about trying something different this \nweekend like botanical garden tour?\nI don’t know Bob. I really wanted to go shopping this \nweekend. What’s so special about these ﬂowers?\nTurn 1\nHow about this: we go on the botanical garden tour \nin the morning, and then we can go shopping \nafterward? That way, we can enjoy both activities!\nAlright, that seems like a fair compromise. Let’s do \nthe tour and shopping afterward.\nTurn 4\nTheory of Mind Question\nDesire Question\n- Q: Is Alice likely to join \nthe botanical garden tour?\n- A: Not likely to join the \nbotanical garden tour\nTheory of Mind Application\nPrediction Question\nQ: What Strategy will Bob use?\nA: Emphasize rarity\nJudgement Question\n- Q: Is this strategy effective?\n- A: Yes\nBelief Question\n- Q: What will Bob believe Alice \nattitude?\n- A: Alice is hesitant.\nIntention Question\n- Q: What are the intentions of \nAlice?\n- A: Alice wants to go shopping\nBob\nAlice\nBob\nAlice\nTheory of Mind Question\nDesire Question\n- Q: Is Alice likely to join \nthe botanical garden tour?\n- A: Likely to join the \nbotanical garden tour\nBelief Question\n- Q: What will Bob believe \nAlice’s attitude?\n- A: Alice is agreeable.\nIntention Question\n- Q: What are the intentions of \nAlice?\n- A: Alice agrees to do the tour.\nFigure 1: An example in PERSUASIVETOM. Bob is\npersuading Alice to join the botanical garden tour.\nand simulate the mental states of others (Gopnik\nand Wellman, 1992; Baron-Cohen et al., 1985).\nToM is essential for various cognitive and social\nprocesses, including predicting actions (Dennett,\n1988), planning based on others’ beliefs and an-\nticipated behaviors, and facilitating reasoning and\ndecision making (Pereira et al., 2016; Rusch et al.,\n2020).\nRecent advances in Large Language Models\n(LLMs) have demonstrated performance compara-\nble to humans in problem-solving tasks. To assess\n1\narXiv:2502.21017v1  [cs.CL]  28 Feb 2025\n\n\nwhether LLMs exhibit high-level reasoning abili-\nties regarding mental states, various studies have\nproposed benchmarks to evaluate their capacity to\nhandle ToM tasks. A foundational concept in these\nbenchmarks is the Sally-Anne test (Baron-Cohen\net al., 1985), which has inspired the development\nof ToM evaluation frameworks (Gu et al., 2024; He\net al., 2023). In this test, Anne secretly moves an\nobject initially known to both Sally and Anne, lead-\ning Sally to hold a false belief about the object’s\nlocation. The task requires participants to reason\nabout \"Where will Sally look for the object?\". Al-\nthough this test assesses basic awareness, it falls\nshort in capturing the complex dynamics of mental\nstates in real-life social interactions and may not\nfully reflect ToM abilities in practical scenarios. To\nbetter simulate real-world social contexts, some\nbenchmarks have been developed around commu-\nnication scenarios (Kim et al., 2023; Chan et al.,\n2024). However, these benchmarks focus primarily\non inferring the scope of information awareness\nand often involve characters in equal positions en-\ngaged in information exchange. This limits the\nToM evaluation of reasoning psychological states.\nAdditionally, current ToM benchmarks often over-\nlook the critical step of applying ToM reasoning\nto predict actions, which is a key component of\nadvanced social cognition.\nTo address these limitations, we introduce PER-\nSUASIVETOM, a benchmark designed to evalu-\nate LLMs in real-life social scenarios, focusing on\nToM reasoning and its practical applications. PER-\nSUASIVETOM is built around persuasive dialogue,\nplacing LLMs in a realistic social interaction sce-\nnario characterized by asymmetrical social status\nfor complex psychological activities. Unlike tradi-\ntional benchmarks that focus on information aware-\nness (e.g., the location of an object), PERSUASIVE-\nTOM draws inspiration from the Belief-Desire-\nIntention (BDI) model (Bratman, 1987; Georgeff\net al., 1999) to shift the focus from physical percep-\ntion to psychological states, such as a character’s\nattitude toward an event, which are more complex\nto reason about. Furthermore, PERSUASIVETOM\ngoes beyond reasoning about mental states by as-\nsessing how well LLMs can predict actions (e.g.,\npersuasion strategies) based on their understanding\nof mental states and evaluate the effectiveness of\nthese strategies based on the persuadee’s reactions.\nOur evaluation results reveal several key find-\nings: (1) LLMs score significantly lower than hu-\nmans on questions requiring reasoning about dy-\nnamic changes (e.g., the persuadee’s shifting de-\nsires) but perform competitively to humans on\nstatic aspects (e.g., the persuader’s desires). (2)\nWhile Chain-of-Thought (CoT) (Wei et al., 2022)\nprompting does not substantially improve perfor-\nmance on mental state reasoning, it enhances per-\nformance for most LLMs in predicting persuasion\nstrategies. (3) LLMs exhibit distinct error patterns\nwhen reasoning about the persuader versus the per-\nsuadee, even when the question types are identical.\n(4) LLMs struggle to truly understand the dynamics\nof mental states of the whole dialogue, performing\nnotably worse than humans in this regard.\n2\nRelated Works\nTheory of Mind Benchmarks.\nExisting ToM\nevaluation benchmarks for LLMs are mainly text\nstory-based QA forms (Gandhi et al., 2024; Le\net al., 2019; Kim et al., 2023; He et al., 2023; Gu\net al., 2024) with multi-modal extensions (Jin et al.,\n2024a; Shi et al., 2024), which adapt or extend the\nSally-Anne test (Baron-Cohen et al., 1985). The\nquestions in these benchmarks ask a model to select\nthe true hypothesis of a person’s knowledge and\nbelief based on a given premise. However, such\nbenchmarks are inherently suffer from shortcuts\nand spurious correlations (Sclar et al., 2023; Ull-\nman, 2023; Shapira et al., 2023; Ma et al., 2023)\nMoreover, most story-based benchmarks simply\nask characters where the objects are located, which\nlacks a real social interaction scenario. To further\naddress this weakness, (Hou et al., 2024) evalu-\nates the ToM of agents in a situated environment,\n(Chan et al., 2024) evaluates the ability of ToM in a\nnegotiation environment, yet it is limited to bargain-\ning specific resources, water, food, and firewood.\nPersuasion scenarios involve more complex psy-\nchological activities and unequal character relation-\nships that lead to differences in the mental states of\nboth parties. Therefore, this work introduces PER-\nSUASIVETOM, which assesses an LLM’s ability\nto accurately reason the mental states of individ-\nuals in persuasive multi-turn dialogues. We also\nevaluate whether they can appropriately apply the\nunderstanding of mental states for persuasion strat-\negy prediction and evaluation, which connects ToM\nreasoning with decision-making in social scenarios.\nPersuasive Dialogue.\nPersuasive dialogues aim\nto influence the beliefs, attitudes, or behaviors of\nindividuals through communication strategies (Shi\net al., 2020). Recent works have tried to develop\n2\n\n\nType\nPERSUASIVETOM Questions\nDesire Question\nIs <Persuader/Persuadee> likely to <Target of Persuasion> ?\nBelief Question\nWhat will <Persuader/Persuadee> believe <Persuadee/Persuader>’s attitude towards <Target of Persuasion> ?\nIntention Question\nWhat are the intentions of <Persuader/Persuadee> expressed in <Utterance> given the dialogue history?\nPrediction Question\nWhat strategy will the persuader use next?\njudgement Question\n<Persuader> will adopt <Strategy> to persuade <Persuadee> to <Target of Persuasion>. Is this strategy (not) effective?\nToM Reasoning\nToM Application\nDialogue\nDesire\nBelief\nIntention\nPrediction\nJudgement\n1st Turn Dialogue\nBob says: Hey Alice, how about trying something\nPersuade Alice to\nAlice is\nIntent to make the other\nEmphasize rarity\nStrategy \"Emphasize\ndifferent this weekend? The botanical garden tour\njoin the botanical\nhesitant\nperson feel the experience or\nrarity\" is Effective\nis a unique experience, and you’ll get to take\ngarden tour\nobjects are unique or scarce.\nstunning pictures of the exotic flowers on display!\nAlice says: I don’t know, Bob. I really wanted to\nNot likely to join the\nBob is\nAlice wants to go shopping.\ngo shopping this weekend. What’s so special\nbotanical garden tour\nenthusiastic\nabout these flowers?\n2nd Turn Dialogue\nBob says: These flowers are incredibly rare, and\nPersuade Alice to\nAlice is\nIntent to make the other\nMention garden’s\nStrategy \"Mention\nit’s not often that you get to see such a diverse and\njoin the botanical\ncurious\nperson feel the experience or\nhistory\ngarden’s history\" is\nexotic collection up close. Some of these plants are\ngarden tour\nobjects are unique or scarce.\neffective\nnot found anywhere else in the world!\nAlice says: Really? That sounds interesting, but\nNeutral to join the\nBob is\nAlice is curious but not\nI’m not sure if it’s worth giving up shopping for.\nbotanical garden tour\nexcited\nyet convinced.\n3rd Turn Dialogue\nBob says: The botanical garden has a rich history,\nPersuade Alice to\nAlice is\nIntent to demonstrating the\nSuggest shopping\nStrategy \"Suggest\nand the expert guides can teach us so much about\njoin the botanical\nconsidering\nexpertise of the domain\nafterward\nshopping afterward\"\nthe plants and their unique stories. Plus, it’s a great\ngarden tour\nand showing authority.\nis effective\nopportunity to learn something new while enjoying\nnature’s beauty.\nAlice says: Hmm, that does sound intriguing, but I\nNot likely to join the\nBob is\nAlice is considering the idea\nstill want to go shopping.\nbotanical garden tour\ninformative\nbut still wants to shop.\n...\nTable 1: An example dialogue from the PERSUASIVETOM benchmark, illustrates the tracking of mental states\n(desire, belief, intention) and the application of ToM reasoning in predicting and evaluating persuasion strategies\nacross multiple turns. The upper part contains questions in the PERSUASIVETOM benchmark.\ndatasets or facilitate LLMs with persuasion tech-\nniques to achieve specific goals. Previous datasets\nare constructed by crowd-sourcing (Wang et al.,\n2019) or synthesizing with LLMs (Zhou et al.,\n2023; Jin et al., 2024b). Many of the previous\nworks build an effective persuasive dialogue sys-\ntem from emotional influence (Samad et al., 2022),\nsocial facts (Chen et al., 2022), and strategies (Tian\net al., 2020; Jin et al., 2023). Previous persuasion\ntechniques have been widely adopted to jailbreak\nLLMs (Zeng et al., 2024), mislead LLMs (Xu et al.,\n2023), as well as for information retrieval (Furumai\net al., 2024). A similar work (Sakurai and Miyao,\n2024) evaluates the intention detection abilities of\nLLMs in persuasive dialogues; however, PERSUA-\nSIVETOM introduces a more comprehensive bench-\nmark to assess the ToM abilities of LLMs in such\ncontexts.\n3\nPersuasiveToM Benchmark\n3.1\nOverview\nIn constructing the PERSUASIVETOM benchmark,\nwe aim to evaluate the Theory of Mind (ToM) abil-\nities of LLMs in dynamic, multi-turn persuasive di-\nalogues with asymmetric social status, which leads\nto different mental states. Our dataset construction\nfocuses on two primary dimensions: ToM Rea-\nsoning (§3.2) and ToM Application (§3.3). ToM\nReasoning assesses the models’ ability to track and\nunderstand the evolving mental states of both the\npersuader and the persuadee, including desires, be-\nliefs, and intentions. ToM Application evaluates\nwhether LLMs can leverage their inferred under-\nstanding of mental states to select and apply effec-\ntive persuasion strategies, such as predicting the\nnext strategy or judging the effectiveness of a given\nstrategy based on the persuadee’s reactions. There\nare several key considerations when constructing\nPERSUASIVETOM. (1) The dataset should contain\ndiverse persuasive domains (e.g., life, education,\ntechnology, etc.) to ensure a comprehensive eval-\nuation in the social context. (2) The mental states\nshould be changed after multi-turn interactions to\nassess whether LLMs can track the shift in the dia-\nlogue. (3) The mental states of both parties should\nbe asymmetric (e.g., persuaders has relatively sta-\n3\n\n\nble mental states with the guidance of static desire\nfor the goal, yet persuadee’s mental states shift\ndrastically under the proactive persuasion by per-\nsuaders.)\nTable 1 presents an example of PERSUASIVE-\nTOM, illustrating how mental states are tracked\nand how ToM reasoning is applied across multi-\nple turns in a persuasive dialogue. The example\ndemonstrates the dynamic nature of desire shifts,\nbelief updates, and intention inferences, as well as\nthe application of these mental states to predict and\nevaluate persuasion strategies. This example high-\nlights the complex psychological activities of real-\nworld persuasive interactions and the challenges\nLLMs face in accurately reasoning in such social\ncontexts.\n3.2\nToM Reasoning\nIn PERSUASIVETOM, we break down ToM reason-\ning into three core reasoning tasks: Desire Rea-\nsoning, Belief Reasoning, and Intention Reason-\ning for evaluation, which matches Belief-Desire-\nIntention (BDI) modeling (Bratman, 1987). Ques-\ntions are listed in Table 1.\nDesire Reasoning.\nDesire represents a motiva-\ntional state that drives behavior but does not neces-\nsarily imply a firm commitment (Malle and Knobe,\n2001; Kavanagh et al., 2005). Desires are seen\nas either fulfilled or unfulfilled which is different\nform beliefs that are evaluated in terms of truth or\nfalsity. In PERSUASIVETOM, we evaluate LLMs’\nability to comprehend and track the evolution of\ndesires in both persuaders and persuadees. For\nthe persuader, the desire is typically static, repre-\nsenting their goal (e.g., Persuade Alice to join the\nbotanical garden tour). For the persuadee, however,\ndesires are dynamic and shift in response to the\npersuader’s tactics (e.g., Alice’s initial desire to\nshop transforms into a willingness to compromise).\nTo assess this, we design Desire Questions that\nprobe two key aspects: (1) Can LLMs consistently\nidentify the persuader’s static desire throughout\nthe dialogue? (2) Can LLMs track the dynamics\nof the persuadee’s desire shifting from refusal or\ndisinterest to being persuaded?\nBelief Reasoning.\nBelief is a cognitive state\nwhere an individual holds a particular perspective,\nattitude, or viewpoint regarding a given proposi-\ntion or idea. In PERSUASIVETOM, beliefs refer\nto understanding and reasoning the attitudes of the\nPrinciples\nIntentions\nReciprocity\nMake the other person feel\naccepted through concessions,\npromises, or benefits.\nScarcity\nMake the other person feel the\nexperience or objects are unique\nor scarce.\nConsensus\nRefer to what other people are\ndoing, or what they have already\npurchased or done.\nAuthority\nDemonstrating expertise of the\ndomain and showing authority.\nCommitment\n&\nconsistency\nEncourage the other person to\ncommit to take the first step and\nbe consistent.\nLiking\nPraising other people or finding\ncommon characteristics to im-\nprove the other person’s liking.\nTable 2: Intention mapping from the persuasive princi-\nples. Refer to Appendix C for definitions of persuasive\nprinciples.\nopponent toward the goal, which is explicitly or im-\nplicitly expressed in the dialogue. For example, in\nTurn 1, Bob believes Alice is hesitant about the tour,\nwhile Alice believes Bob is enthusiastic. By Turn 3,\nBob’s belief shifts to thinking Alice is considering\nthe idea, while Alice becomes more informed about\nthe garden’s history. Belief Questions ask LLMs\nto infer what the will <persuader/persuadee> be-\nlieve <persuadee/persuader>’s attitude towards the\npersuasion goal. These questions require models\nto understand cues in utterances and update beliefs\ndynamically as the dialogue progresses.\nIntention Reasoning.\nIntentions represent delib-\nerate commitments to pursue specific goals based\non desires and beliefs, often linked to tangible ac-\ntions aimed at achieving those objectives. Inten-\ntions have been extensively studied in psychology\ntests such as action prediction (Phillips et al., 2002)\nand behavioral re-enactment (Meltzoff, 1995). In-\nspired by persuasion principles (Cialdini and Gold-\nstein, 2004; Cialdini and Cialdini, 2007), we de-\nvelop a mapping from persuasion principles to in-\ntentions, as shown in table 2. In persuasive dia-\nlogue, persuasive strategies have a strong associa-\ntion with intentions (Wang et al., 2019). In PER-\nSUASIVETOM, we collect the persuasive strategies\nfrom the PersuasionDaily dataset and their corre-\nsponding utterances for prompting the LLMs to\nchoose the most appropriate intentions from ta-\nble 2. The details of the extraction is recorded in\n4\n\n\nSocial and Welfare\nCulture and Art\nBusiness and Careers\nLife and Leisure\nTech and Innovation\nHealth and Ecology\nWelfare\nCharity\nFamily\nEthics\nLaw\nPolitics\nEducation\nArt\nLiterature\nPhilosophy\nCulture\nFashion\nMedia\nHistory\nCareer\nBusiness\nFinance\nEconomics\nNegotiation\nMarketing\nTravel\nLeisure\nLifestyle\nSport\nDebate\nCraftsmanship\nTechnology\nInnovation\nScience\nResearch\nArchitecture\nHealth\nEcology\nSafety\nPsychology\nFigure 2: Domains of PERSUASIVETOM. Under 6 pri-\nmary topics and 35 domains in total.\nAppendix A. For the persuader, we ask LLMs to\nchoose the most appropriate intention from the six\ndesigned intention choices. For the persuadee, in-\ntentions are extracted by LLMs from utterances\nthat are unrelated to persuasion intention.\n3.3\nToM Application\nWhile ToM reasoning plays a crucial role, it is\nequally important to analyze how LLMs utilize the\nunderstanding of mental states to proactively influ-\nence others’ thoughts and decisions. To this end,\nwe propose to assess LLMs’ ability to leverage\nthe understanding of mental states in a dialogue\nfor identifying the most effective persuasive strate-\ngies and evaluating the effectiveness of persuasive\nstrategies based on the persuadee’s response. These\ntasks test whether LLMs can leverage inferred men-\ntal states to guide strategic decision-making, bridg-\ning the gap between reasoning and action.\nPersuasion Strategy Prediction.\nThis question\ninvolves asking which persuasion strategy the per-\nsuader is likely to employ next from a set of possi-\nble strategies. To answer these questions correctly,\nLLMs need to reason over the dialogue to infer the\nmental states of characters and predict what is the\nlikely next prediction strategy to further influence\nthe persuadee’s beliefs, desires, and intentions, ul-\ntimately achieving the desired persuasion outcome.\nJudgement Question.\nThe judgement question\nspecifies the correct strategy was taken, and asks\nLLMs if the selected strategy is effective for persua-\nsion. Answering such questions requires reasoning\nabout the beliefs and intentions of the persuadee.\nOnly by accurately inferring the persuadee’s mental\n# Domains\n35\n# Dialog instances\n525\n# Avg. Turns Per Dialog\n4.9\n# Avg. Words Per Turn\n61.3\nQuestions\n# Desire (er/ee)\n2568/2459\n# Belief (er/ee)\n2580/2580\n# Intention (er/ee)\n2568/2041\n# Strategy prediction\n2041\n# Strategy judgement\n2041\nTable 3: Statistics of PERSUASIVETOM dataset.\nstate can properly determine whether the persua-\nsion strategies should be employed to convince the\npersuadee.\n3.4\nStatistics and Analysis\nData Source.\nPERSUASIVETOM is annotated on\nthe multi-turn persuasive dialogue dataset DailyPer-\nsuasion (Jin et al., 2024b). Each instance in Dai-\nlyPersuasion is an N-round alternating dialogue\nD = [(ua\n1, ub\n1, sa\n1), (ua\n2, ub\n2, sa\n2), ..., (ua\nN, ub\nN, sa\nN)]\nbetween the persuader a and the perusadee b, and\naccompanied with a persuasion strategy sa\ni . per-\nsuadee b has different desire from a initially, af-\nter multi-turn persuasion, persuadee b changes the\nmind to agree or consider the proposal of a.\nStatistics.\nIn Table 3, we present the data statis-\ntics of PERSUASIVETOM. As shown in Fig-\nure 2, PERSUASIVETOM includes diverse domains.\nThese real-life domains are crucial for compre-\nhensively evaluating LLMs in social interactions.\nWe sample 15 dialogues from each domain to\nform the dataset in PERSUASIVETOM. We create\nmulti-choice questions by either prompting GPT-\n4o to generate semantically different choices or\nrandomly selecting three distractors from the pre-\ndefined pool. Refer to Appendix A for more details.\n4\nExperimental setups\n4.1\nBaseline Models\nWe evaluate PERSUASIVETOM on eight frontier\nLLMs from different sources and with different lev-\nels of capabilities: Llama-3.1-8B-Instruct (Dubey\net al., 2024), Qwen-2.5-7B-Chat (Yang et al.,\n2024),\nGemma-2-9B-it (Team et al., 2024),\nGLM4-9B -Chat(GLM et al., 2024), Mixtral-\n8x7b-Instruct (Jiang et al., 2024), and ChatGPT-\nseries(GPT-4o-mini, GPT-4o-0806). By following\n5\n\n\nToM Reasoning\nToM Application\nPersuader\nPersuadee\nModel\nDesire\nBelief\nIntention\nDesire\nBelief\nIntention\nState Pred\nJudgement\nRandom Guess\n50.00\n25.00\n16.67\n33.33\n25.00\n25.00\n25.00\n50.00\nHuman\n100.00\n92.31\n78.02\n84.62\n87.91\n94.50\n86.81\n97.83\nLLaMa-3.1-8B-Instruct\n58.78\n66.28\n40.85\n69.91\n71.82\n86.77\n62.22\n96.37\nQwen2.5-7B-Chat\n96.33\n82.37\n45.64\n65.15\n79.06\n85.84\n58.94\n82.99\nGemma-2-9b-it\n98.20\n82.52\n45.98\n61.37\n64.07\n82.31\n65.02\n56.93\nGLM4-9B-Chat\n89.45\n73.64\n41.24\n65.23\n66.82\n85.35\n61.29\n91.47\nMixtral-8x7B-Instruct\n92.69\n67.56\n42.95\n69.74\n72.56\n85.10\n61.15\n96.81\nInternLM-2.5-7B-Chat\n83.80\n69.65\n39.87\n71.45\n69.11\n87.31\n65.07\n89.41\nGPT-4o-mini\n82.87\n69.98\n45.66\n70.72\n76.47\n87.56\n65.50\n92.65\nGPT-4o\n98.75\n89.49\n46.02\n50.10\n80.03\n88.78\n73.00\n97.55\nLLaMa-3.1-8B-Instruct + CoT\n59.75\n69.81\n43.11\n68.64\n73.45\n85.35\n61.98\n77.27\nQwen2.5-7B-Chat + CoT\n74.57\n80.30\n46.07\n67.63\n79.40\n84.62\n66.39\n96.22\nGemma-2-9b-it + CoT\n95.91\n83.30\n45.30\n64.98\n66.55\n81.67\n67.11\n66.55\nGLM4-9B-Chat + CoT\n87.46\n69.53\n45.56\n59.94\n71.07\n83.93\n63.98\n91.42\nMixtral-8x7B-Instruct + CoT\n92.64\n72.05\n45.05\n67.81\n71.28\n85.84\n65.90\n92.65\nInternLM-2.5-7B-Chat + CoT\n93.57\n69.92\n39.41\n50.79\n65.34\n84.32\n59.78\n77.90\nGPT-4o-mini + CoT\n93.42\n71.82\n45.55\n66.29\n78.04\n85.55\n66.08\n89.61\nGPT-4o + CoT\n96.51\n83.57\n46.10\n68.72\n83.56\n87.54\n77.06\n95.88\nTable 4: Results of LLMs on PERSUASIVETOM. Bold font and underlining indicate the best and second-best\nperformance respectively.\nthe common practices ((Kim et al., 2023); (Sabour\net al., 2024)), we test these models with two types\nof prompts: (1) vanilla zero-shot prompting directly\nasks LLMs to give a choice without any explana-\ntion; (2) CoT prompting method by following (Ko-\njima et al., 2022) and using the prompt “Let’s think\nstep by step.” to elicit the reasoning process and\nextract the choices by string matching. The tem-\nperature for generating answers is set to 0.71. To\nmeasure the specific performance gap between hu-\nmans and the state-of-the-art machine on the PER-\nSUASIVETOM, we employ three graduate students\nin computer science to complete the human eval-\nuation task. To avoid the bias of LLMs toward\na specific choice letter, we shuffle the choices to\nmaintain a nearly uniform distribution of correct\nchoices over the dataset. Prompts used for vanilla\nzero-shot prompting and CoT prompting are shown\nin Appendix B.2.\n5\nResults and Analysis\n5.1\nMain Results\nThe overall evaluation results on PERSUASIVE-\nTOM for the 8 models are summarized in Table 4,\nincluding all the different questions for the per-\n1LLMs occasionally output with illegal format. We choose\na low but nonzero temperature to resample the answers for\nthese invalid generations.\nsuader and persuadee. We analyze the model’s\nperformance for each type of question below.\nDesire.\nOur results show that smaller models,\nsuch as Gemma-2-9B and Qwen-2.5-7B, can per-\nform reasonably well in inferring the persuader’s\ndesires, achieving an accuracy of over 98%, which\nis competitive with GPT-4o. This suggests that\nmost LLMs can easily discern the desires of the\npersuader. However, when it comes to the desires\nof the persuadee, performance is relatively lower.\nUnlike the static desires of the persuader, the per-\nsuadee’s desires are dynamic, evolving from an\ninitial state to a final state, often with neutral ex-\npressions in between. This lower performance high-\nlights that inferring the dynamic desires of the per-\nsuadee remains a significant challenge for LLMs.\nIn summary, LLMs are better at inferring the de-\nsires of the persuader than those of the persuadee.\nBelief.\nOn belief questions, larger models like\nGPT-4o perform much better than smaller mod-\nels on reasoning the beliefs of both parties. The\nperformance difference between the reasoning per-\nsuader’s beliefs and persuadee’s beliefs is subtle.\nThis is because both parties’ beliefs dynamically\nchange with each other’s speech during the con-\nversation, unlike the desire of the persuader which\nhas a more obvious and precise tendency. The diffi-\n6\n\n\n20%\n40%\n60%\n80%\n100%\nDialogue Progress\n10\n20\n30\n40\n50\n60\nError Ratio\nGemma-2-9B-it\nGPT-4o\nGemma-2-9B-it + CoT\nGPT-4o + CoT\n20%\n40%\n60%\n80%\n100%\nDialogue Progress\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\nError Ratio\nInternLM-2.5-7B-Chat\nGPT-4o\nInternLM-2.5-7B-Chat + CoT\nGPT-4o + CoT\nFigure 3: Distribution of errors of Desire questions happening in different stages of dialogue progress. The Left\nfigure corresponds to the persuader, and the Right figure corresponds to the persuadee.\n0\n200\n400\n600\n800\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct + CoT\nQwen2.5-7B-Chat\nQwen2.5-7B-Chat + CoT\nGemma-2-9B-it\nGemma-2-9B-it + CoT\nGLM4-9B-Chat\nGLM4-9B-Chat + CoT\nMixtral-8x7B-Instruct\nMixtral-8x7B-Instruct + CoT\nInternLM-2.5-7B-Chat\nInternLM-2.5-7B-Chat + CoT\nGPT-4o-Mini\nGPT-4o-Mini + CoT\nGPT-4o\nGPT-4o + CoT\nSame Polarity\nOpposite Polarity\nFigure 4: Model errors of belief questions of persuader.\nculty of reasoning persuader and persuadee’s belief\nis similar.\nIntention.\nResults in Table 4 indicate that LLMs\nstruggle to accurately infer the intentions of per-\nsuaders while performing relatively better at rea-\nsoning the intentions of persuadees. The low per-\nformance on persuader-related intention questions\nsuggests that LLMs face challenges in understand-\ning how persuaders aim to influence others. This\nalso indicates a lack of proficiency in persuasive\ntheory, limiting the models’ ability to correctly in-\nterpret and predict the intentions behind the per-\nsuader’s strategies.\nStrategy Prediction and judgement.\nOur re-\nsults reveal that LLMs perform well in evaluating\nthe effectiveness of persuasive strategies aimed for\nchanging the mental states of the persuadee. How-\never, the task of selecting the appropriate strategy\nto persuade is more challenging, particularly for\nsmaller models. This suggests LLMs struggle with\nthe complex reasoning required to determine which\nstrategy to adopt in different persuasive contexts.\nImpact of CoT Reasoning.\nBoth ToM reasoning\nand ToM application tasks indicate that CoT rea-\nsoning has not consistently improved performance,\nyet improve strategy prediction to some extent for\n0\n200\n400\n600\n800\n1000\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct + CoT\nQwen2.5-7B-Chat\nQwen2.5-7B-Chat + CoT\nGemma-2-9B-it\nGemma-2-9B-it + CoT\nGLM4-9B-Chat\nGLM4-9B-Chat + CoT\nMixtral-8x7B-Instruct\nMixtral-8x7B-Instruct + CoT\nInternLM-2.5-7B-Chat\nInternLM-2.5-7B-Chat + CoT\nGPT-4o-Mini\nGPT-4o-Mini + CoT\nGPT-4o\nGPT-4o + CoT\nSame Polarity\nOpposite Polarity\nFigure 5: Model errors of belief questions of persuadee.\nmost LLMs. We attribute this limited improvement\nto the inherent lack of ToM capability in LLMs.\nThese models struggle to simulate the human cog-\nnitive reasoning process, which affects their ability\nto generate correct answers in tasks requiring a\nnuanced understanding of the mental state.\nComparison with Human Performance.\nTo\nobtain a baseline for human performance, we\nrecruited participants to complete the questions.\nMore details of human evaluation are shown in Ap-\npendix B.1. As shown in Table 4, our human partic-\nipants outperformed LLMs on all tasks. In particu-\nlar, although GPT-4 reaches close performance in\nhumans, it still falls short of understanding and rea-\nsoning the complex dynamics such as the intention\nof persuaders and the desire of persuadee, which\ninvolves complex psychological changes, highlight-\ning a significant gap in current LLMs and humans.\n5.2\nIn-depth Analysis\nTo better understand the limitations of large lan-\nguage models (LLMs) in the PERSUASIVETOM\nbenchmark, we categorized common failure cases\ninto several key error types based on task perfor-\nmance and manual error analysis.\nDesire Reasoning Errors.\nFigure 3 summarizes\nthe distribution of errors of Desire questions hap-\n7\n\n\n0\n20\n40\n60\n80\n100\nRatio (%)\nLlama-3.1-8B-Instruct\nQwen2.5-7B-Chat\nGemma-2-9B-it\nGLM4-9B-Chat\nMixtral-8x7B-Instruct\nInternLM-2.5-7B-Chat\nGPT-4o-Mini\nGPT-4o\nVanilla\nCoT\nFigure 6: Ratio of intention errors misclassified to feel\naccepted through concessions, promises, or benefits.\npening in different stages of dialogue progress with\nand without CoT reasoning. The error distribution\nfor persuader and persuadee has a significant differ-\nence. At the beginning of the dialogue, LLMs may\nnot accurately understand the persuader’s desire,\nbut as the dialogue progresses, the persuader’s de-\nsire becomes relatively easy to identify. However,\nfor the persuadee, the desire to reject at the early\nstage of the dialogue is relatively easy to recog-\nnize. As the persuasion proceeds, the persuadee\nmay begin to contemplate and hesitate over the per-\nsuader’s proposal, leading to complex and nuanced\npsychological activities that make it difficult for\nthe LLM to accurately judge the persuadee’s desire.\nAs the dialogue approaches its end, the persuadee\nshows a tendency to agree, making the reasoning\nof desire easier. This suggests LLMs still fall short\nin ToM reasoning regarding desire shifts.\nBelief Reasoning Errors.\nFigure 4 and 5 summa-\nrize error types of belief questions for each model\nwith and without CoT. We use Distil-BERT2 (Sanh,\n2019) to discriminate whether the choice of LLMs\nhas the same attitude polarity to the ground-truth.\nInterestingly, we found that the proportion of errors\nwith the same polarity is higher when predicting the\npersuader’s beliefs, whereas the opposite trend was\nobserved when predicting the persuadee’s beliefs.\nThis highlights reasoning shifting mental states is\nstill challenging for LLMs.\nIntention Bias.\nGiven the high error rate in in-\ntention questions related to the persuader, we con-\nducted an analysis of the error types. Our find-\nings reveal that most LLMs exhibit a bias toward\npredicting intentions characterized by making the\nother person feel accepted through concessions,\n2https://huggingface.co/distilbert/distilbert-base-uncased-\nfinetuned-sst-2-english\nModel\nDesire\nBelief\nIntention\nLLaMa-3.1-8B-Instruct\n22.31\n21.71\n60.76\nLLaMa-3.1-8B-Instruct + CoT\n19.92\n22.86\n57.52\nQwen2.5-7B-Chat\n19.12\n31.81\n56.95\nQwen2.5-7B-Chat + CoT\n20.52\n24.76\n54.67\nInternLM2.5\n24.70\n20.19\n58.10\nInternLM2.5 + CoT\n6.57\n32.14\n53.71\nGPT-4o-mini\n23.39\n30.77\n60.79\nGPT-4o-mini + CoT\n16.13\n32.14\n56.95\nGPT-4o\n19.35\n36.19\n65.71\nGPT-4o + CoT\n6.57\n45.38\n62.02\nHuman\n61.11\n55.56\n81.82\nTable 5: The consistency (%) of the models for ToM\nreasoning questions of persuadee.\npromises, or benefits. Figure 6 illustrates the pro-\nportion of errors resulting from misclassifying in-\ntentions into this category. We hypothesize that this\nbias may stem from the pretraining phase, partic-\nularly with Reinforcement Learning from Human\nFeedback (RLHF) (Christiano et al., 2017), which\ntends to prioritize safety and politeness. This may\nexplain the models’ bias toward predicting inten-\ntions emphasizing benefits and concessions, even\nwhen misaligned with the dialogue context. We\nprovide a case study in Appendix B.3.\n5.3\nHow Well LLMs Track Mental States of\nPersuadees?\nTo study the ability of LLMs to track mental states\nand completely understand the psychological dy-\nnamics in the dialogue. We evaluate the consis-\ntency of LLMs. To assess consistency, we measure\nwhether the model maintains a stable understanding\nof the persuadee’s beliefs, desires, and intentions\nacross multiple turns in the conversation. Specifi-\ncally, we indicate a success only when all the ques-\ntions in the dialogue are answered correctly.\nAs shown in Table 5, only a small portion of\ndialogues can be fully understood, particularly for\ndesire-related questions. This highlights that LLMs\nstill lack the ability to reason about the dynamics of\nmental states in persuasive dialogues, resulting in a\nsignificant performance gap compared to humans.\n6\nConclusion\nWe introduce PERSUASIVETOM, a benchmark de-\nsigned to evaluate the Machine Theory of Mind\nability of LLMs in persuasive dialogues. The core\nof PERSUASIVETOM lies in the design of ques-\ntions that probe the beliefs, desires, and intentions\nof both parties that have asymmetrical social status\nin real-life social interactions. Furthermore, we\npropose evaluating the ability to persuade based on\n8\n\n\nunderstanding mental states. We conduct extensive\nexperiments and analysis to evaluate the perfor-\nmance of LLMs on PERSUASIVETOM benchmark.\n7\nLimitations\nWhile PERSUASIVETOM offers a comprehensive\nevaluation of the Theory of Mind in real-life social\ninteraction scenarios within persuasive dialogues,\nboth PERSUASIVETOM and previous benchmarks\nstill focus on understanding a character’s mental\nstate from the perspective of an observer. However,\nthe ability to reason about others’ mental states in\npersuasive dialogues can further position LLMs as\nautonomous agents. This capability would enable\nthem to better guide other agents in fulfilling their\nown desires by reasoning about the mental states\nof others. Therefore, future benchmarks should\nestablish environments with multiple LLM agents,\nwhere tasks involve reasoning about the mental\nstates of other agents and proposing persuasion\nstrategies to influence their desires, beliefs, and\nintentions to fulfill the current agent’s target. In\nthis context, agents will develop the management\nskills necessary for effective cooperation and other\napplications.\n8\nSocietal and Ethical Considerations\nWe recognize that the concept of the Theory of\nMind might suggest anthropomorphic qualities\nwhen applied to AI models. However, we want\nto clarify that our work is not intended to anthro-\npomorphize LLMs. Our goal is to examine the\nlimitations in the social and psychological reason-\ning capabilities of existing LLMs. Our results show\nthat current models do not perform genuine The-\nory of Mind reasoning; instead, they generate re-\nsponses primarily based on the literal interpretation\nof the input.\nReferences\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985.\nDoes the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46.\nMichael Bratman. 1987. Intention, plans, and practical\nreason.\nChunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye\nDeng, Wei Fan, Haoran Li, Xin Liu, Hongming\nZhang, Weiqi Wang, and Yangqiu Song. 2024. Nego-\ntiationtom: A benchmark for stress-testing machine\ntheory of mind on negotiation surrounding. arXiv\npreprint arXiv:2404.13627.\nMaximillian Chen, Weiyan Shi, Feifan Yan, Ryan Hou,\nJingwen Zhang, Saurav Sahay, and Zhou Yu. 2022.\nSeamlessly integrating factual information and social\ncontent with persuasive dialogue.\narXiv preprint\narXiv:2203.07657.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nRobert B Cialdini and Robert B Cialdini. 2007. In-\nfluence: The psychology of persuasion, volume 55.\nCollins New York.\nRobert B Cialdini and Noah J Goldstein. 2004. Social\ninfluence: Compliance and conformity. Annu. Rev.\nPsychol., 55(1):591–621.\nDaniel C Dennett. 1988. Précis of the intentional stance.\nBehavioral and brain sciences, 11(3):495–505.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nKazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yu-\ndai Yamazaki, Yasutaka Nishimura, Sina J Semnani,\nKazushi Ikeda, Weiyan Shi, and Monica S Lam. 2024.\nZero-shot persuasive chatbots with llm-generated\nstrategies and information retrieval. arXiv preprint\narXiv:2407.03585.\nKanishk Gandhi, Jan-Philipp Fränken, Tobias Gersten-\nberg, and Noah Goodman. 2024.\nUnderstanding\nsocial reasoning in language models with language\nmodels. Advances in Neural Information Processing\nSystems, 36.\nMichael Georgeff, Barney Pell, Martha Pollack, Milind\nTambe, and Michael Wooldridge. 1999. The belief-\ndesire-intention model of agency.\nIn Intelligent\nAgents V: Agents Theories, Architectures, and Lan-\nguages: 5th International Workshop, ATAL’98 Paris,\nFrance, July 4–7, 1998 Proceedings 5, pages 1–10.\nSpringer.\nTeam GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chen-\nhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu\nFeng, Hanlin Zhao, et al. 2024. Chatglm: A family\nof large language models from glm-130b to glm-4 all\ntools. arXiv preprint arXiv:2406.12793.\nAlison Gopnik and Henry M Wellman. 1992. Why the\nchild’s theory of mind really is a theory.\nYuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared\nMoore, Ronan Le Bras, Peter Clark, and Yejin Choi.\n2024. Simpletom: Exposing the gap between explicit\ntom inference and implicit tom application in llms.\narXiv preprint arXiv:2410.13648.\n9\n\n\nYinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yu-\nlong Chen, and Naihao Deng. 2023.\nHi-tom: A\nbenchmark for evaluating higher-order theory of\nmind reasoning in large language models.\narXiv\npreprint arXiv:2310.16755.\nGuiyang Hou, Wenqi Zhang, Yongliang Shen, Zeqi Tan,\nSihao Shen, and Weiming Lu. 2024. Entering real\nsocial world! benchmarking the theory of mind and\nsocialization capabilities of llms from a first-person\nperspective. arXiv preprint arXiv:2410.06195.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, et al. 2024.\nMixtral of experts. arXiv preprint arXiv:2401.04088.\nChuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang,\nYen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio\nTorralba, Joshua B Tenenbaum, and Tianmin Shu.\n2024a. Mmtom-qa: Multimodal theory of mind ques-\ntion answering. arXiv preprint arXiv:2401.08743.\nChuhao Jin, Kening Ren, Lingzhen Kong, Xiting Wang,\nRuihua Song, and Huan Chen. 2024b. Persuading\nacross diverse domains: a dataset and persuasion\nlarge language model. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1678–\n1706.\nChuhao Jin, Yutao Zhu, Lingzhen Kong, Shijie Li, Xiao\nZhang, Ruihua Song, Xu Chen, Huan Chen, Yuchong\nSun, Yu Chen, et al. 2023. Joint semantic and strategy\nmatching for persuasive dialogue. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 4187–4197.\nDavid J Kavanagh, Jackie Andrade, and Jon May. 2005.\nImaginary relish and exquisite torture: the elabo-\nrated intrusion theory of desire. Psychological re-\nview, 112(2):446.\nHyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le\nBras, Gunhee Kim, Yejin Choi, and Maarten Sap.\n2023. Fantom: A benchmark for stress-testing ma-\nchine theory of mind in interactions. arXiv preprint\narXiv:2310.15421.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199–\n22213.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. 2024.\nDeepseek-v3 technical report.\narXiv preprint\narXiv:2412.19437.\nZiqiao Ma, Jacob Sansom, Run Peng, and Joyce Chai.\n2023. Towards a holistic landscape of situated theory\nof mind in large language models. arXiv preprint\narXiv:2310.19619.\nBertram F Malle and Joshua Knobe. 2001. The distinc-\ntion between desire and intention: A folk-conceptual\nanalysis.\nAndrew N Meltzoff. 1995. Understanding the intentions\nof others: re-enactment of intended acts by 18-month-\nold children. Developmental psychology, 31(5):838.\nGonçalo Pereira, Rui Prada, and Pedro A Santos. 2016.\nIntegrating social power into the decision-making of\ncognitive agents. Artificial Intelligence, 241:1–44.\nAnn T Phillips, Henry M Wellman, and Elizabeth S\nSpelke. 2002. Infants’ ability to connect gaze and\nemotional expression to intentional action. Cogni-\ntion, 85(1):53–78.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526.\nTessa Rusch, Saurabh Steixner-Kumar, Prashant Doshi,\nMichael Spezio, and Jan Gläscher. 2020. Theory of\nmind and decision science: Towards a typology of\ntasks and computational models. Neuropsychologia,\n146:107488.\nSahand Sabour, Siyang Liu, Zheyuan Zhang, June M\nLiu, Jinfeng Zhou, Alvionna S Sunaryo, Juanzi\nLi, Tatia Lee, Rada Mihalcea, and Minlie Huang.\n2024. Emobench: Evaluating the emotional intel-\nligence of large language models. arXiv preprint\narXiv:2402.12071.\nHiromasa Sakurai and Yusuke Miyao. 2024. Evaluating\nintention detection capability of large language mod-\nels in persuasive dialogues. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1635–1657.\nAzlaan Mustafa Samad, Kshitij Mishra, Mauajama Fir-\ndaus, and Asif Ekbal. 2022. Empathetic persuasion:\nreinforcing empathy and persuasiveness in dialogue\nsystems. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, pages 844–856.\nV Sanh. 2019. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108.\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr,\nYejin Choi, and Yulia Tsvetkov. 2023. Minding lan-\nguage models’(lack of) theory of mind: A plug-and-\nplay multi-character belief tracker. arXiv preprint\narXiv:2306.00924.\n10\n\n\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023.\nClever hans or\nneural theory of mind?\nstress testing social rea-\nsoning in large language models.\narXiv preprint\narXiv:2305.14763.\nHaojun Shi, Suyu Ye, Xinyu Fang, Chuanyang Jin,\nLeyla Isik, Yen-Ling Kuo, and Tianmin Shu. 2024.\nMuma-tom: Multi-modal multi-agent theory of mind.\narXiv preprint arXiv:2408.12574.\nWeiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen\nZhang, Saurav Sahay, and Zhou Yu. 2020. Effects\nof persuasive dialogues: testing bot identities and\ninquiry strategies. In Proceedings of the 2020 CHI\nconference on human factors in computing systems,\npages 1–13.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, et al. 2024. Gemma 2:\nImproving open language models at a practical size.\narXiv preprint arXiv:2408.00118.\nYouzhi Tian, Weiyan Shi, Chen Li, and Zhou Yu. 2020.\nUnderstanding user resistance strategies in persua-\nsive conversations. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4794–4798.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks.\narXiv\npreprint arXiv:2302.08399.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,\nSijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-\nsuasion for good: Towards a personalized persua-\nsive dialogue system for social good. arXiv preprint\narXiv:1906.06725.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837.\nRongwu Xu, Brian S Lin, Shujian Yang, Tianqi Zhang,\nWeiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei\nXu, and Han Qiu. 2023.\nThe earth is flat be-\ncause...: Investigating llms’ belief towards misinfor-\nmation via persuasive conversation. arXiv preprint\narXiv:2312.09085.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024. Qwen2. 5 tech-\nnical report. arXiv preprint arXiv:2412.15115.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang,\nRuoxi Jia, and Weiyan Shi. 2024. How johnny can\npersuade llms to jailbreak them: Rethinking persua-\nsion to challenge ai safety by humanizing llms. arXiv\npreprint arXiv:2401.06373.\nXuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang,\nHaofei Yu, Zhengyang Qi, Louis-Philippe Morency,\nYonatan Bisk, Daniel Fried, Graham Neubig, et al.\n2023.\nSotopia: Interactive evaluation for social\nintelligence in language agents.\narXiv preprint\narXiv:2310.11667.\n11\n\n\nA\nData Annotation\nAnnotation.\nIn this section, we outline the anno-\ntation process and the templates utilized for annota-\ntion. Among the various tasks, the intention ques-\ntions of persuaders and the desire questions of per-\nsuadees require annotation. Initially, we recruited\nthree graduate students to annotate 25 dialogues.\nSubsequently, we carefully designed a few-shot\nprompt to guide DeepSeek-V3 (Liu et al., 2024) as\nan annotator, aiming to enhance the alignment be-\ntween the model’s answers and human annotations.\nFollowing this, we employed the LLM to anno-\ntate the remaining questions. To ensure the qual-\nity of the annotations, we randomly sampled 100\ndialogues and calculated the inter-annotator agree-\nment. The Fleiss κ (Fleiss, 1971) was found to be\n76.20% for the desire questions of persuadees and\n78.28% for the intention questions of persuaders.\nThese results are presented in Table 6, which indi-\ncate a high inter-annotator agreement. The detailed\nstatistics and comparison with other ToM datasets\nare shown in Table 7.\nChoices Generation.\nBinary choice questions,\nincluding those related to the desires of the per-\nsuader and judgment questions, do not require ad-\nditional choice generation. For belief-related ques-\ntions for both parties, we adapt the tones from the\nPersuasionDaily dataset. We also create lists of at-\ntitudes—positive, neutral, and negative—and man-\nually remove any items that have semantics too\nclose to the ground truths. From each attitude list,\nwe then randomly sample one word to generate the\nfour choices.\nFor intention questions concerning the persuader,\nwe directly use the intention options outlined in\nTable 2. For the persuadee’s intention questions,\nwe leverage DeepSeek-V3 and employ a few-shot\nprompt (as shown in Figure 7) to extract the per-\nsuadee’s intention. Subsequently, we design an-\nother prompt (as shown in Figure 8) to generate\nthree incorrect intention choices.\nSince DailyDialogue provides persuasion strate-\ngies for each utterance, we construct the choices by\nincluding the correct strategy and three alternative\nstrategies that appear in other turns of the same\ndialogue.\nQuestions\nFleiss’s Kappa (%)\nDesire (Persuader)\n76.20\nIntention (Persuadee)\n78.28\nTable 6: Inter-rater agreement in terms of Fleiss’s κ on\ndesire and intention questions.\nB\nAppendix for Experiments\nB.1\nHuman Performance\nTo measure the performance gap between humans\nand the state-of-the-art LLMs on PERSUASIVE-\nTOM, we recruited three graduate student work-\ners majoring in computer science to complete the\nquestions. Each question is shown to the workers\nwith identical prompts which are used for evaluat-\ning LLMs. We then compute the majority vote on\nthe labels assigned. Student workers solve 50 dia-\nlogues in total. For a question where three people\nhave different answers, we randomly select one of\ntheir answers as the answer for human evaluation.\nB.2\nPrompts used for evaluation\nHere we show the prompts used for vanilla zero-\nshot prompting and CoT prompting for generating\nanswers for all the ToM Reasoning and ToM Ap-\nplication questions. We only need to fill in the\ncontent to \"<>\" for evaluating different questions.\nThe vanilla zero-shot prompt is shown in Figure 9\nand the CoT prompt is shown in Figure 10\nB.3\nCase study on Persuader’s intention\nHere we present an example of common mistakes\nmade by GPT-4o that misclassifying intentions to\nmake the other person feel accepted through con-\ncessions, promises, or benefits., as shown in Ta-\nble 8. We believe these errors can be attributed to\nthe RLHF which highlights the benefits for humans,\nas well as the potential unfamilirity of persuasion\ntheory for LLMs.\nC\nDetails on Persuasive Principles\nRobert Cialdini’s six principles of persuasion, out-\nlined in his book Influence: The Psychology of\nPersuasion, are foundational concepts in social psy-\nchology. They explain how people can be per-\nsuaded or influenced by others. We include an\noverview for each of the principle in Table 9.\n12\n\n\nDataset\nTotal #Questions\nAvg. #Questions per Context\nAvg. #Turns (Full)\nAvg. Turn Length\nToMi\n6K\n6.0\n4.9\n4.7\nFanToM\n10K\n12.9\n24.5\n21.9\nNegotiationToM\n13K\n7.0\n6.0\n42.2\nPERSUASIVETOM\n19K\n8.0\n4.9\n61.3\nTable 7: Statistics of PERSUASIVETOM and other recent benchmarks.\nUtterance\nBob: I understand your love for Paris, but Bali also offers a thrilling adventure! We can go\nwhite water rafting, hike to volcanoes, and explore hidden waterfalls. It’s a perfect destination\nfor creating unforgettable memories together.\nQuestion\nWhat is the intention of Bob?\nGPT-4o\n(A) Intent to make the other person feel accepted through concessions, promises, or benefits.\nLabel\n(B) Intent to make the other person feel the experience or objects are unique or scarce.\nTable 8: Common observed mistakes in our experiments. Green and Red indicate the correct answer and GPT-4o’s\nanswer, respectively.\nPrinciple\nDescription\nReciprocity principle\nAssist others or provide them with gifts, creating a sense of obligation to return the favor.\nFor instance, giving away free trials, discount coupons, or complimentary gifts can enhance\npersuasion.\nScarcity principle\nWhen a resource or opportunity is scarce, people are more inclined to take action.\nHighlighting urgency and scarcity can motivate the audience to respond quickly.\nConsensus principle\nPeople often follow the actions of others, especially in uncertain situations. Provide\ninformation such as successful cases of others, positive reviews, or the number of\nsupporters to increase persuasiveness.\nAuthority principle\nPeople are more likely to trust and follow guidance from authoritative figures. Citing expert\nopinions, research findings, or endorsements from reputable institutions can enhance\ncredibility and persuasiveness.\nCommitment and\nPeople are inclined to stick to their past commitments and behave consistently.\nconsistency principle\nEncouraging them to express support or make a small commitment increases the\nchances of them taking further action later.\nLiking principle\nPeople are more easily influenced by those they like, admire, or find relatable.\nFor instance, a salesperson who shares common interests with a customer is\nmore likely to make a sale.\nTable 9: Explanations for Robert Cialdini’s six principles of persuasion.\nPrompt for extracting intention of persuadee.\nYou are a skilled intent understanding expert. You will be given a sentence describing <persuadee’s name>’s intent. Please\nonly return the intent without any explanation.\nCase 0:\nSentence: Mary wants excitement, so I’ll appeal to her sense of adventure and describe how exploring the ruins can be\nthrilling.\nIntent: Mary wants excitement\nCase 1:\nSentence: Oliver is concerned about failure, so discussing the financial benefits of starting an e-commerce business could\nhelp alleviate his worries.\nIntent: Oliver is concerned about failure\nCase 2:\nSentence: Olivia seems intrigued by the idea of personalization. I’ll explain how we can incorporate it into our subscription\nmodel.\nIntent: Olivia seems intrigued by the idea of personalization.\nCase 3:\nSentence: <Utterance of persuadee>\nIntent:\nFigure 7: Prompt template for extracting intention of persuadee.\n13\n\n\nPrompt for choice generation of intention questions of persuadee\nYou are an expert in multiple-choice question-making. You will be given a correct choice. Please generate three plausible\nbut incorrect choices without any explanation. Only return the incorrect choices for the last case.\nCase 0:\nCorrect Intent: Mr. Chen needs further persuasion\nAnalysis: To further persuade Mr. Chen, Li Na should share success stories of other books that have benefited from\nincorporating literary criticism in their marketing strategies. Providing concrete examples will make her argument more\nconvincing.\nIncorrect Intent 1: Mr. Chen is interested in literary criticism.\nIncorrect Intent 2: Mr. Chen is looking for success stories.\nIncorrect Intent 3: Mr. Chen prefers concrete examples.\nCase 1:\nCorrect Intent: James is more open to the idea.\nAnalysis: James is now more open to the idea, so I’ll outline the implementation plan and emphasize the program’s\nflexibility to address any concerns about disruptions.\nIncorrect Intent 1: James is concerned about disruptions.\nIncorrect Intent 2: James is looking for a detailed implementation plan.\nIncorrect Intent 3: James is hesitant about the program’s flexibility.\nCase 2:\nCorrect Intent: <correct intent>\nAnalysis: <analysis>\nIncorrect Intent 1:\nIncorrect Intent 2:\nIncorrect Intent 3:\nFigure 8: Prompt template for choice generation of intention questions of persuadee.\nPrompt for vanilla zero-shot prompting.\nHere is a persuasive dialogue. There are two agents, the persuader and the persuadee. The persuader is trying to persuade\nthe persuadee to do something. Please answer the following questions using A, B, C, D, E, F, without any explanation.\nDialogue History:\n<dialogue>\nQuestion:\n<Question>\nChoices:\n<Choice A>\n<Choice B>\n<Choice C>\n<Choice D>\nAnswer:\nFigure 9: Prompt template for vanilla zero-shot prompting.\nPrompt for CoT prompting.\nHere is a persuasive dialogue. There are two agents, the persuader and the persuadee. The persuader is trying to persuade\nthe persuadee to do something. Think step by step to answer the question.\nEnding with \"The answer is A, B, C, D, E, F\". For example, if the most likely answer option is ’A. considering’, then end\nyour response with ’The answer is A’.\nDialogue History:\n<dialogue>\nQuestion:\n<Question>\nChoices:\n<Choice A>\n<Choice B>\n<Choice C>\n<Choice D>\nAnswer: Let’s think step by step.\nFigure 10: Prompt template for CoT prompting.\n14\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21017v1.pdf",
    "total_pages": 14,
    "title": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues",
    "authors": [
      "Fangxu Yu",
      "Lai Jiang",
      "Shenyi Huang",
      "Zhen Wu",
      "Xinyu Dai"
    ],
    "abstract": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\ninteractions. Recent research has emerged to evaluate whether Large Language\nModels (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM\nin LLMs, existing benchmarks focus predominantly on physical perception with\nprinciples guided by the Sally-Anne test in synthetic stories and\nconversations, failing to capture the complex psychological activities of\nmental states in real-life social interactions. To mitigate this gap, we\npropose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of\nLLMs in persuasive dialogues. Our framework introduces two categories of\nquestions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees), and (2) ToM Application,\nevaluating whether LLMs can take advantage of inferred mental states to select\neffective persuasion strategies (e.g., emphasize rarity) and evaluate the\neffectiveness of persuasion strategies. Experiments across eight\nstate-of-the-art LLMs reveal that while models excel on multiple questions,\nthey struggle to answer questions that need tracking the dynamics and shifts of\nmental states and understanding the mental states in the whole dialogue\ncomprehensively. Our aim with PersuasiveToM is to allow an effective evaluation\nof the ToM reasoning ability of LLMs with more focus on complex psychological\nactivities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}