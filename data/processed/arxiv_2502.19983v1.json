{
  "id": "arxiv_2502.19983v1",
  "text": "Efficient Time Series Forecasting via Hyper-Complex Models\nand Frequency Aggregation\nEyal Yakir, Dor Tsur, and Haim Permuter\nSchool of Electrical and Computer Engineering\nBen-Gurion University of the Negev, Be’er Sheva, Israel\n{eyalyak, dortz}@post.bgu.ac.il, haimp@bgu.ac.il\nAbstract\nTime series forecasting is a long-standing problem in statistics and machine learning.\nOne of the key challenges is processing sequences with long-range dependencies. To\nthat end, a recent line of work applied the short-time Fourier transform (STFT), which\npartitions the sequence into multiple subsequences and applies a Fourier transform to\neach separately. We propose the Frequency Information Aggregation (FIA)-Net, which\nis based on a novel complex-valued MLP architecture that aggregates adjacent window\ninformation in the frequency domain. To further increase the receptive field of the FIA-\nNet, we treat the set of windows as hyper-complex (HC) valued vectors and employ HC\nalgebra to efficiently combine information from all STFT windows altogether. Using the\nHC-MLP backbone allows for improved handling of sequences with long-term dependence.\nFurthermore, due to the nature of HC operations, the HC-MLP uses up to three times\nfewer parameters than the equivalent standard window aggregation method. We evaluate\nthe FIA-Net on various time-series benchmarks and show that the proposed methodologies\noutperform existing state of the art methods in terms of both accuracy and efficiency. Our\ncode is publicly available on https://anonymous.4open.science/r/research-1803/.\n1\nIntroduction\nTime series forecasting (TSF) is a long standing challenge, which plays a key role in various\ndomains, such as energy management [1], traffic prediction [2] and financial analysis [3]. With\nthe development of deep learning, myriad neural network (NN) architectures had been proposed,\nand have gradually improved the accuracy on the TSF problem. Two key architectures has been\nused for TSF are recurrent NNs (RNNs) [4, 5, 6] and transformers [7, 8, 9, 10], each of which aims\nto capture long-term dependencies through a different functional feature extraction procedure.\nWhile both methods were proven useful, RNNs struggled with long-term dependencies [11]\nor non-stationary data patterns, While transformer architectures may overlook important\ntemporal information due to permutation invariance [12], they require many parameters and\nmay suffer from long runtime. Additional NN-based approaches for TSF consider graph NNs\n(GNNs) [13] and decomposition models [14].\nRecent advancements have demonstrated promising results in processing and extracting\nfeatures from the frequency domain [15] . Techniques leveraging frequency-based transfor-\nmations have been in various contexts, ranging from computational efficiency improvements\n[9] to seasonal-trend decomposition [8]. To better process the frequency domain data, [16]\n1\narXiv:2502.19983v1  [cs.LG]  27 Feb 2025\n\n\ndeveloped a complex-valued MLP, which demonstrated superior capability in capturing both\ntemporal and cross-channel dependencies. To better handle nonstationarities in the data, [17]\nsubstituted the standard FFT with the Short-Time Fourier Transform (STFT) [18], which\ndivides the sequence into separate windows and transforms each window individually into the\nfrequency domain. While showing better suitability for non-stationary time series data, the\nSTFT yields a set of windows, each of which represents exclusive information on the sequence.\nHowever, in practice, adjacent windows are highly correlated, albeit processed separately by\ncurrent STFT-based models.\nTo incorporate the overlooked shared information, we propose the FIA-Net, a novel\nTSF model that is designed to handle long-term dependencies in the data by aggregating\ninformation from subsets of STFT windows. The FIA-Net has an MLP backbone that processes\nthe STFT windows in the frequency domain. We propose two novel MLP architectures. The\nfirst, is termed window-mixing MLP (WM-MLP), which mixes each STFT windows with\nits neighboring bands. The second is the HC-MLP. The HC-MLP leverages HC algebra to\nefficiently combine information from all STFT together. By using HC algebra, the STFT is\nimplements with three times less parameters than the equivalent WM-MLP.\nThe main contributions of this paper are as follows\n• We construct the FIA-Net and the WM-MLP backbone. The resulting TSF model\ncaptures inter-window dependencies in the frequency domain and benefits from a forward\npass complexity of O(L log L/p) operations, where L is the lookback window length and\np is the number of STFT windows.\n• We propose a novel HC-MLP backbone that expands the receptive field of the WM-MLP,\nwhile requiring a fraction of total parameters.\n• To reduce the model size and complexity, we filter the STFT windows, leaving only the\ntop-M frequency components. We show that accuracy is maintained even when M is\nsignificantly smaller than the total number of components.\n• We provide an array of experiments that demonstrate the performance of the model and\nits efficiency. We show that the FeeqShiftNet improves upon existing models accuracy\nby up to 20%.\n• We provide an ablation study, in which explores the effect of operating over the complex\nplane and compare the performance of the two considered MLP backbones.\n2\nRelated Work\nTime-Series Forecasting\nThe first notable works on TSF utilize classical statistical\nlinear models such as ARIMA [19, 20] which consider series decomposition. Those were\nthen generalized to a non-linear setting in [21]. To overcome the limitations posed by the\nclassical models, deep learning was incorporated, where initially, sequential deep learning\nwas performed by RNN-based models. Two key RNN models are long-short term memory\nnetworks [5] which introduce a sophisticated gating mechanism and the DeepAR model\n[4] that connected the RNN model with AR modeling. Despite their expressive power for\nsequential modeling, RNN demonstrated low efficiency and introduced high runtimes in\n2\n\n\nboth the forward and backward pass [11].\nTwo popular architectures were proposed to\nimprove upon RNNs; transformers and GNNs.\nNotable transformer-based methods are\nInformer [22], Reformer [23], and PatchTST [24], each leveraging the attention mechanism to\ncapture temporal dependencies, while proposing sophisticated methods to reduce the attention\noperation complexity. GNNs, however, allowed for better modeling of dependencies between\ntime series variables by treating them as graph nodes, making them particularly suitable\nfor capturing spatio-temporal patterns. For example, AGCRN [25] introduced an adaptive\ngraph convolution mechanism to dynamically adjust the graph structure based on inter-series\nrelationships, while MTGNN [13] combined graph convolutions with temporal convolutional\nlayers to jointly learn spatial-temporal dependencies.\nFrequency Domain Models for Time Series Forecasting\nA recent line of work\nattempts to solve the TFS problem in the frequency domain [15], with the purpose of revealing\npatterns that may be hidden in the time domain. The FEDformer [8] uses a Fourier-based\nframework to separate trend and seasonal components by leveraging the Fourier Transform\non sub-sequences, allowing it to isolate periodic patterns more effectively. ETSformer [26]\ncombines exponential smoothing and applies attention in the frequency domain to enhance\nseasonality modeling by capturing both short- and long-term dependencies. In FiLM [27],\nFourier projections are used to reduce noise and emphasize relevant features. Additionally,\nSFM [28] and StemGNN [29] utilize frequency decomposition and Graph Fourier Transforms\nto handle complex temporal dependencies in multivariate time series. FRETS [16] extends this\napproach by proposing frequency-domain MLPs to learn complex relationships between real\nand imaginary components of the FFT. FREQTSF [17] uses STFT with attention mechanisms\nto capture temporal patterns across overlapping time windows. While frequency models, and\nspecifically the recent use of STFT, have shown significant improvement in TFS performance,\neach STFT window is often processed separately, ignoring the strong correlations between\nadjacent windows.\nHyper-complex Numbers HC numbers extend the complex number system to higher\ndimensions [30]. Base-4 HC numbers, have been widely used in computer graphics to model 3D\nrotations [31]. Base-8 HC numbers have been explored in image classification and compression\n[31, 32], developing an HC network that showed favorable performance on popular datasets.\nThe merit of HC numbers to extract relevant information in time-series was explored in [33], in\nwhich an HC-net was used to analyse brain-wave data, and in [34], which explored HC-network\nfor financial data. In this work, we explore the utility of HC architectures for the efficient\nprocessing of STFT windows in the frequency domain.\n3\nProposed Model : FIA-Net\nIn this section, we describe FIA-Net, a TSF model that leverages shared information between\nSTFT windows. We begin by discussing the existing gap in current frequency domain TSF\nmethods, followed by a brief introduction to frequency domain MLPs [35]. We then outline\nthe FIA-Net components, presenting the novel complex MLP backbone, discussing a simple\nfrequency compression step that reduces the MLP input dimension, and outline the complete\nmodel.\nMotivation\nEven though most real-world time-series data is nonstationary, it may adhere to a piecewise\n3\n\n\nx\nx\nISTFT\nPrediction\nDecomposition Stage\nReconstruction Stage\nLookBack Window\nFFT\nPredection\nFFT\nFFT\nFFT\nxx\nFFT\nFigure 1: Window Mixing mechanism. An input X is transformed into a set of p STFT\nwindows which are transformed to the frequency domain and are then fed into the WM-MLP,\nwhich aggregates adjacent windows. The WM-MLP outputs are then transformed back to the\ntime domain via a real STFT, from which the prediction (red) is obtained.\nstationary structure, as observed in speech signals [36] and financial data [37]. This local\nstationarity allows us to partition the series into stationary correlated STFT subsequences that\ncan be transformed in the frequency domain. The correlation between the STFT sequences\nhas been efficiently utilized in recent works, even though, as we later show, it affects the\ndownstream model accuracy in the task of time prediction.\n \n \n \n \n \n \n \nFigure 2: FD-MLP\narchitecture.\nFrequency Domain MLPs\nAs we handle complex-valued data, we adopt the frequency do-\nmain MLP (FD-MLP) unit from [16]. The FD-MLP generalizes\nthe simple neuron to operate with complex-valued weights and\nbiases. Incorporating complex MLPs has been shown to improve\nthe model performance as it aligns better with the geometrical\nstructure induced by the complex plane. The FD-MLP unit is\nvisualized in Figure 2. In Section 4, we will discuss the expansion\nof the FD-MLP for hyper-complex numbers.\n3.1\nAdjacent Information Aggregation\nConsider a sequence X = {x1, . . . , xL} ∈RD×L where xi ∈RD, L is the sequence length,\nwhich we refer to as the lookback size, and D is the latent space dimension. Our objective is\nto predict the next T elements of the sequence ˆX = {ˆxL+1, . . . , ˆxL+T } ∈RD×T , where T is a\npredetermined prediction horizon. We are interested in processing X in the frequency domain.\nWe utilize the STFT, which partitions X into p windows and applies the FFT separately to\neach window. In addition, we exploit the real-valued inputs to perform a Real STFT, which\n4\n\n\nresults in half the frequency coefficients. The STFT for the i-th window is defined as:\nSTFT{X}(ω, τi) =\nL\nX\nt=1\nxtw(t −τi)e−jωt,\n(1)\nWhere, w(t −τi) is the window function centered at the location of the i-th window\n(i ∈{1, . . . , p}), ω represents the angular frequency, and j satisfies j2 = −1. Each window\nis defined by its center τi and has a size of NFFT\n2\n+ 1. The output of the STFT consists of p\nwindows, each producing a spectrum of length NFFT\n2\n+ 1. We propose the window mixing MLP\n(WM-MLP), which adapts the FD-MLP to properly aggregate neighboring STFT windows to\nincorporate shared information. Given a set of complex transformed windows {C1, . . . , Cp},\nthe WM-MLP operates on the ith window Cin\ni as follows:\nCout\ni\n= σ\n\u0000Cin\ni Wi→i + Cin\ni−1W (i−1)→i + Cin\ni+1W (i+1)→i + Bi\n\u0001\n(2)\nwhere σ(·) is an activation function, (W(i−1)→i, Wi→i, W(i+1)→i)p\ni=1 are the WM-MLP weight\nmatrices with Cj being a matrix of zeros for j /∈{1, . . . , p}, and (Bi)p\ni=1 are the WM-MLP bias\nvectors, and W is the elementwise complex conjugate of W. The outputs of the WM-MLP are\ntransformed back to the time domain using the element-wise inverse STFT, which is given by:\niSTFT{XF (w, τi)}(t) =\nX\nω\nXF (ω, τ)ejωtw(t −τi)\n(3)\nThe STFT, WM-MLP operation, and inverse transform are depicted by Figure 1. In highly\nnonstationary data, energy transition between adjacent windows can be sharp. To that end,\nwe introduce a minor overlap between adjacent windows of NFFT −L−NF F T\np−1\n, which implicitly\nadjusts their statistics prior to processing by the TSF model by increasing the inter-window\ncorrelations.\n3.2\nImplementation Details and Complete System\nSelective Frequency Compression To reduce the input dimensionality to the WM-MLP,\nwe compress each transformed window Ci ∈CNFFT×D along the frequency axis. Specifically,\nwe select the top M frequency components based on their real and imaginary values across\neach dimension and denote the compressed window with CM\ni . Then, (CM\n1 , . . . , CM\np ) is fed into\nthe WM-MLP layer. The top-M procedure is given by\nCM\ni\n= Top-M\nj=1,...,M\n|Ci,j|C\n(4)\nwhere Ci,j is the jth component of Ci and |z|C is the magnitude of z ∈C. Additionally, we\nstore the top component indices of (4) in a list I(i), which encodes the band from which the\ninformation came. To transform the WM-MLP output Cout\ni\nback to the time domain, we\nperform a position-aware zero padding, which adds NFFT −M zeros while placing the nonzero\ncomponents in their original indices, which correspond to the original frequency bands, i.e.,\nCpadded\ni,j\n=\n(\nCout\ni,j ,\nj ∈I(i)\n0,\nelse.\n5\n\n\nEmbedding Layer\nR - SFTF (p windows)\n \nS-M\nS-M\nS-M\nS-M\nR - ISFTF (p windows)\nResize Block\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nPad\nPad\nPad\nPad\nWindow Mixing MLP\n \nFigure 3: FIA-Net Model: The input, denoted X, is first fed into the embedding layer,\nresulting in XE, which is transformed to the frequency domain via the STFT. We then\nextract the top-M components of each STFT window and feed the compressed windows\nthrough the WM-MLP. The MLP outputs are then passed through position-aware zero\npadding, whose outputs are transformed back to the time domain and summed with XE via\nskip connection. The model output ˆX is then given by applying a linear transformation.\nIn Section 5, we demonstrate that, in addition to improving computational efficiency, this\nfrequency compression procedure enhances the performance of downstream TSF tasks. The\nselection of top-M components allows us to reduce the model’s complexity while maintaining\nthe most relevant frequency information.\nComplete Model\nThe complete FIA-Net, as shown in Figure 3, operates as follows:\nGiven an input X ∈RB×L×D, the dimension of X is expanded through a learned embedding\nlayer, resulting in XE ∈RB×L×D×E. This expanded representation is then fed into an STFT\nblock that uses the real input to perform R −STFT on XE. The transformed signal is passed\nthrough the SM block, whose output is further processed by the WM-MLP. The WM-MLP\noutputs are subsequently padded and transformed back to the temporal axis, where they are\nintegrated with XE via a skip connection and resized to the desired output sequence shape\nusing a two-layer MLP decomposition.\nModel Complexity The forward pass complexity of the WM-MLP is primarily determined\nby the STFT complexity, which is O(L log(L\np )). This represents a significant reduction in\ncomplexity compared to transformer-based methods, which employ intricate mechanisms to\nreduce their O(L2) attention complexity to O(L log L). Additionally, the application of top-M\nfrequency selection further optimizes the forward pass in the frequency domain, reducing\nboth computational demands and the corresponding MLP size. A detailed analysis of these\ncomplexities is provided in Table 11.\n4\nWindow Aggregation via Hyper-Complex Models\nEven though the WM-MLP backbone integrates valuable information that benefits the FIA-\nNet’s accuracy, information is not only shared between two adjacent STFT windows. In fact,\nthe stronger the dependencies on the long-term past, the more information is shared between\ntwo distant windows on the frequency axis. Ideally, we would like to aggregate information\n6\n\n\nbetween all p STFT windows. Unfortunately, a straightforward extension of the WM-MLP\nrequires O(p2) weight matrices, which may impair the training procedure and increase model\ncomplexity. To address that, we interpret the set of windows as an HC vector and propose an\nHC-based MLP that efficiently processes the set of STFT windows. We begin with a short\nintroduction on HC-algebras, followed by the construction of the proposed MLP backbone for\nthe FIA-Net.\n4.1\nHyper-complex Numbers\nHC numbers generalize the complex field by introducing additional dimensions while maintain-\ning algebraic properties. HC number systems are defined by a parameter q that determines\nthe number of components in the number system. Complex numbers can thus be viewed as\nan HC number with q = 2, and an HC number of base q can be represented with p = q/2\ncomplex numbers. In what follows, we focus on HC numbers with p = 4, termed Octonions O,\nwhose elements are denoted o = (α1, α2, α3, α4) ∈O, with αi ∈C for i = 1, . . . , 4. Additional\ndiscussion on p ̸= 4 is given in Appendix C.\nThe addition of two Octonions, o1 = (α1, . . . , α4) and o2 = (β1, . . . , β4), is given by their\ncomponentwise sum, while their multiplication follows the Cayley-Dickson construction [38].\nThe product o3 = o1 · o2 = (γ1, γ2, γ3, γ4) is given by:\nγ1 = α1β1 −α2β2 −α3β3 −α4β4\nγ2 = α2β1 + α1β2 + α3β4 −α4β3\nγ3 = α3β1 + α4β2 + α1β3 −α2β4\nγ4 = α4β1 + α2β3 + α1β4 −α3β2\n(5)\nHyper-complex numbers exhibit additional properties such as closed-form expressions for\nnorm calculations and norm preservation for specific bases. For completeness, we provide\nadditional information on HC-numbers in Appendix C, where the proposed MLP is presented\nunder specific bases.\n4.2\nHyper-Complex MLP\nThe longer the range of temporal dependencies in the data, the more shared information there\nis between gathered windows. In such cases, the WM-MLP, which incorporates short-term\ninformation in the frequency domain, might fail to capture long-term dependencies. To that\nend, our goal is to increase the extent to which information is shared across the STFT windows.\nTo derive a parameter-efficient solution, we incorporate HC algebra into the frequency domain\nlearning procedure.\nAssume that we are given p = 4 complex-valued STFT windows (Cin\ni ∈CB×M×E)4\ni=1, where\nthe second axis is the transformed frequency domain after top-M frequency component selection.\nWe treat the set of windows as a single Octonion tensor (Cin\n1 , Cin\n2 , Cin\n3 , Cin\n4 ) ∈OB×M×E\nand feed it through an HC-valued MLP, whose output is Cout = σ(Cin · W + B).\nFor\n7\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nLegend\n \n \n \n \n \nFigure 4: HC-MLP operating on Cin = (Cin\n1 , Cin\n2 , Cin\n3 , Cin\n4 ), implementing the HC\nmultiplication ((6)). Each output unit is the sum of the corresponding inner blocks of the\nsame color, where a L symbol denotes complex addition and a N denotes complex\nmultiplication. A red outline denotes minus multiplication, and a blue input arrow denotes\ncomplex conjugation.\nCout = (Cout\n1 , Cout\n2 , Cout\n3 , Cout\n4 ), it is given by:\nCout\n1\n= σ(Cin\n1 W1 −Cin\n2 W 2 −Cin\n3 W 3 −Cin\n4 W 4 + B1),\nCout\n2\n= σ(Cin\n2 W 1 + Cin\n1 W2 −Cin\n4 W 3 + Cin\n3 W 4 + B2),\nCout\n3\n= σ(Cin\n3 W1 + Cin\n1 W 3 −Cin\n2 W 4 + Cin\n4 W 2 + B3),\nCout\n4\n= σ(Cin\n4 W 1 + Cin\n1 W4 −Cin\n3 W 2 + Cin\n2 W 3 + B4).\n(6)\nwhere W = (W1, . . . , W4) ∈OE×E, B = (B1, . . . , B4) ∈OE×1 are the HC-MLP weights and\nbias, respectively, and σ is a standard activation function, e.g., ReLU. We stress that, as\nconsidered in the complex MLP from [16], the HC-MLP is implemented with real-valued\noperations, which allows it to plug into every existing automatic differentiation scheme over\nstandard GPUs. The HC-MLP unit is depicted in Figure 4.\nThe WM-MLP demonstrates distinct advantages depending on the prediction horizon. For\nshorter prediction lengths, it achieves better performance by effectively leveraging all available\ninformation from adjacent and nearby windows. In contrast, for longer horizons, where only\ncloser temporal information remains relevant, the WM-MLP’s ability to aggregate adjusted\nwindows proves to be more effective. This behavior is clearly demonstrated in Section 5.2.\nMoreover, the HC perspective offers a significant advantage in terms of parameter efficiency.\nIt allows for an implementation with only p weight matrices, whereas the corresponding WM-\nMLP would require 3p −2 weight matrices (and even p2 weight matrices for a generalization\nof the WM-MLP), all while preserving performance. This reduction in parameters becomes\nincreasingly dramatic as p > 4, as further detailed in Appendix C.\n8\n\n\n5\nResults and Discussion\n5.1\nExperimental Setting\nDatasets Following [8, 16], we consider the following representative real-world datasets: 1)\nWTH (Weather), 2) Exchange (Finance), 3) Traffic, 4) ECL (Electricity), 5) ETTh1\n(Electricity transformer temperature hourly), and 6) ETTm1 (Electricity transformer tem-\nperature minutely). The train/validation/test split is 70%, 15%, and 15%, respectively.\nBaselines In this research, we followed the TSF SoTA baselines: 1) FedFormer [8], 2)\nReformer [23], 3) FreTS [16], 4) PatchTST [24], 5) Informer [22], 6) Autoformer [9]\nand 7) LSTF-Linear [39].\nExperiments setup All experiments were conducted using PyTorch [40] on a single RTX\n3090, utilizing mean squared error (MSE) loss and the Adam optimizer [41]. We established\nan initial learning rate of 10−3 with an exponential decay scheduler. Hyperparameters were\noptimized individually for each dataset (see Appendix B.3 for specific details). We report\nperformance metrics under both root mean squared error (RMSE) and mean absolute error\n(MAE). Additional information on the Normalization B.5, datasets B.1, and baseline models\nB.2 can be found in the appendix.\n5.2\nMain results\nTable 1 compares the FIA-Net performance under both the WM-MLP and the HC-MLP\nbackbones with the SoTA baselines. It is evident that the FIA-Net consistently outperforms\nthe baselines on most considered values of prediction horizon T, with an average improvement\nof 5.4% in MAE and 3.8% in RMSE over SoTA models. We note that the performance of the\nHC-MLP-based network, which is implemented with significantly fewer parameters, achieves\ncomparable results with the corresponding WM-MLP and attains the best results over several\nsettings. We can deduce that the HC-MLP is more suitable for shorter-term prediction, while\nthe WM-MLP backbone is more suitable for longer ranges.\nTable 1: Forecasting performance comparison across datasets and prediction horizons using\nRMSE and MAE. Lower values indicate better performance. Bold denotes the best results,\nand underlined indicates the second-best.\nWeather\nExchange\nTraffic\nElectricity\nETTh1\nETTm1\nMetric\n96\n192\n336\n720\n96\n192\n336\n720\n96\n192\n336\n720\n96\n192\n336\n720\n96\n192\n336\n720\n96\n192\n336\n720\nHC-MLP (Ours)\nRMSE 0.069 0.079 0.090\n0.098\n0.050\n0.062\n0.078\n0.112 0.032 0.034\n0.035 0.036 0.070\n0.068\n0.071\n0.077 0.083 0.085 0.094 0.101 0.074 0.082 0.089 0.096\nMAE\n0.030 0.039 0.043 0.054\n0.035\n0.049\n0.061\n0.089 0.016 0.017\n0.017 0.018 0.040\n0.041\n0.044 0.049 0.057 0.064 0.068 0.075 0.049 0.056\n0.060\n0.067\nWM-MLP (Ours) RMSE 0.071\n0.081 0.089 0.097 0.048 0.060 0.076 0.107 0.033 0.033 0.034 0.036 0.067\n0.068\n0.070 0.076 0.084\n0.088\n0.097\n0.102\n0.076 0.082 0.089 0.094\nMAE\n0.031\n0.041\n0.045 0.053 0.034 0.047 0.058 0.086 0.016 0.016 0.016 0.018 0.039 0.041\n0.044 0.049 0.057 0.066\n0.071 0.075 0.052 0.055 0.058 0.064\nFreTS\nRMSE 0.071\n0.081\n0.090\n0.099\n0.051\n0.067\n0.082\n0.110\n0.036\n0.038\n0.038\n0.039 0.065 0.064 0.072\n0.079\n0.087\n0.091\n0.096\n0.108\n0.077\n0.083 0.089 0.096\nMAE\n0.032\n0.040\n0.046\n0.055\n0.037\n0.050\n0.062\n0.088\n0.018\n0.020\n0.019\n0.020 0.039 0.040 0.046\n0.052\n0.061\n0.065\n0.07\n0.082\n0.052\n0.057\n0.062\n0.069\nPatchTST\nRMSE 0.074\n0.084\n0.094\n0.102\n0.052\n0.074\n0.093\n0.166 0.032 0.035\n0.039\n0.040\n0.067\n0.066 0.067 0.081\n0.091\n0.094\n0.099\n0.113\n0.082\n0.085\n0.091\n0.097\nMAE\n0.034\n0.042\n0.049\n0.056\n0.039\n0.055\n0.071\n0.132 0.016 0.018\n0.020\n0.021\n0.041\n0.042 0.043 0.055\n0.065\n0.069\n0.073\n0.087\n0.055\n0.059\n0.064\n0.070\nLTSF-Linear\nRMSE 0.081\n0.089\n0.098\n0.106\n0.052\n0.069\n0.085\n0.116\n0.039\n0.042\n0.040\n0.041\n0.075\n0.070\n0.071\n0.080\n0.089\n0.094\n0.097\n0.108\n0.080\n0.087\n0.093\n0.099\nMAE\n0.040\n0.048\n0.056\n0.065\n0.038\n0.053\n0.064\n0.092\n0.020\n0.022\n0.020\n0.021\n0.045\n0.043\n0.044\n0.054\n0.063\n0.067\n0.070\n0.082\n0.055\n0.060\n0.065\n0.072\nFEDformer\nRMSE 0.088\n0.092\n0.101\n0.109\n0.067\n0.082\n0.105\n0.183\n0.036\n0.042\n0.042\n0.042\n0.072\n0.072\n0.075\n0.077\n0.096\n0.100\n0.105\n0.116\n0.087\n0.093\n0.102\n0.108\nMAE\n0.050\n0.051\n0.057\n0.064\n0.050\n0.064\n0.080\n0.151\n0.022\n0.023\n0.022\n0.022\n0.049\n0.049\n0.051\n0.055\n0.072\n0.076\n0.080\n0.090\n0.063\n0.068\n0.075\n0.081\nAutoformer\nRMSE 0.104\n0.103\n0.101\n0.110\n0.066\n0.083\n0.101\n0.181\n0.042\n0.050\n0.053\n0.050\n0.075\n0.099\n0.115\n0.119\n0.105\n0.114\n0.119\n0.136\n0.109\n0.112\n0.125\n0.126\nMAE\n0.064\n0.061\n0.059\n0.065\n0.050\n0.063\n0.075\n0.150\n0.026\n0.033\n0.034\n0.035\n0.051\n0.051\n0.088\n0.116\n0.079\n0.086\n0.088\n0.102\n0.081\n0.083\n0.091\n0.093\nInformer\nRMSE 0.139\n0.134\n0.115\n0.132\n0.084\n0.088\n0.127\n0.170\n0.039\n0.047\n0.053\n0.054\n0.124\n0.138\n0.144\n0.148\n0.121\n0.137\n0.145\n0.157\n0.096\n0.107\n0.119\n0.149\nMAE\n0.101\n0.097\n0.101\n0.132\n0.066\n0.068\n0.093\n0.117\n0.023\n0.030\n0.034\n0.035\n0.094\n0.105\n0.112\n0.116\n0.093\n0.103\n0.112\n0.125\n0.070\n0.082\n0.090\n0.115\nReformer\nRMSE 0.152\n0.201\n0.203\n0.228\n0.146\n0.169\n0.189\n0.201\n0.053\n0.054\n0.053\n0.054\n0.125\n0.138\n0.144\n0.148\n0.143\n0.148\n0.155\n0.155\n0.089\n0.108\n0.128\n0.163\nMAE\n0.108\n0.147\n0.154\n0.173\n0.126\n0.147\n0.157\n0.166\n0.035\n0.035\n0.035\n0.035\n0.095\n0.121\n0.122\n0.120\n0.113\n0.120\n0.124\n0.126\n0.065\n0.081\n0.100\n0.132\nThe WM-MLP backbone results reported in Table 1 consider an optimization with respect\n9\n\n\nto p, the number of windows, while the HC-MLP considers a fixed size of p = 4 windows. Thus,\nfor a more suitable comparison, Table 2 shows a comparison of the FIA-Net performance under\nboth backbones with p = 4. We note that when p is similar for both models, the FIA-Net\nattains similar results under both backbones, while the HC-MLP requires significantly fewer\nparameters. Consequently, when the number of windows allows for an HC-MLP version (e.g.,\np = 2ℓas we further explain in Appendix C), an HC-MLP backbone is preferable.\nTable 2: Performance comparison between WM-MLP and HC-MLP with a fixed number of\nSTFT windows (p = 4). Results demonstrate that HC-MLP achieves comparable accuracy\nwhile significantly reducing model parameters, making it preferable for efficient\nimplementations.\nTraffic\nETTh1\nETTm1\nMetric\n96\n192\n336\n720\n96\n192\n336\n720\n96\n192\n336\n720\nWM-MLP (p = 4)\nRMSE\n0.033\n0.034\n0.035\n0.036\n0.088\n0.094\n0.100\n0.103\n0.074\n0.082\n0.089\n0.096\nMAE\n0.016\n0.016\n0.017\n0.018\n0.058\n0.064\n0.068\n0.075\n0.049\n0.056\n0.060\n0.067\nHC-MLP\nRMSE\n0.032\n0.034\n0.035\n0.036\n0.083\n0.085\n0.094\n0.101\n0.072\n0.082\n0.089\n0.096\nMAE\n0.016\n0.017\n0.017\n0.018\n0.049\n0.057\n0.064\n0.068\n0.049\n0.056\n0.060\n0.067\n5.3\nAblation Studies\nWe consider three ablation studies that best demonstrate the key aspects of the proposed work.\nWe focus on the effect of frequency selection, the size of the lookback window, and the omission\nof real/imaginary components in the training procedure. We show that, in various cases, the\ntotal amount of parameters can be decreased by up to 60%. Due to space limitations, the\nresults are demonstrated on a single dataset, while a full discussion and additional results are\ngiven in Appendix D.4.\n5.3.1\nFrequency Dimension Compression\n1\n2\n4\n8\n12\n16\n20 Mmax\nM - Selected quantity of frequencies\n0.0870\n0.0872\n0.0874\n0.0876\n0.0878\n0.0880\nRMSE\n0.0578\n0.0580\n0.0582\n0.0584\n0.0586\nMAE\nRMSE\nMAE\nFigure 5: Accuracy vs. M\nWe study the effect of the parameter M in the top-M frequency\ncomponent selection process on the ETTh dataset. As seen\nin figure 5, even though the model performance varies over\ndifferent datasets and forecasting horizon sizes, in most cases,\nM = 4 attains the best accuracy.\nFurthermore, note that\ntaking M < Mmax = NF F T\n2\n+ 1 improves the model’s results.\nWe conjecture that considering fewer frequency components\ndecreases the NN class complexity, which potentially simplifies\nthe optimization procedure landscape while preserving most of\nthe information contained within the signal. We expand upon\nthis discussion and provide additional results in the Appendix D.1.\n5.3.2\nEffect of Lookback Window Size\n10\n\n\n672\n576\n480\n288\n192\n96\n24\nSequence Length (L)\n0.08\n0.09\n0.10\n0.11\n0.12\nMAE\nT = 96\nT = 192\nT = 336\nT = 720\n0.05\n0.06\n0.07\n0.08\nRMSE\nFigure 6: Accuracy vs. L.\nIn this section, we evaluate the impact of varying lookback\nwindow sizes L ∈{24, 48, 96, 192, 288, 480, 576, 720} for different\nprediction lengths T ∈{96, 192, 336, 720}. As shown in Figure\n6, the dotted line represents the RMSE, while the solid line\nrepresents the MAE. The model’s performance initially improves\nas L increases, as expected, since a longer lookback provides more\ncontextual information. However, many models exhibit parabolic\nbehavior, where performance deteriorates after a certain point\ndue to overfitting to noise or unrealistic patterns in the data. In\ncontrast, our model maintains stable performance and effectively\navoids overfitting, demonstrating its robustness to changes in\nlookback window size. Additional experiments can be found in Appendix D.2.\n5.3.3\nRedundancy of Complex Representation\nWe study the effect of the real and imaginary components on prediction quality. We fix the\nhyperparameters E = 128, p = 13, NFFT = 16, M = Mmax, and compare several scenarios,\nsuch that each scenario considers the masking of a different component, either in the data,\nthe parameters, or both. The masking occurs in both training and inference. As seen in\nTable 3, the elimination of either the real or imaginary components in the data does not\nsignificantly affect the downstream accuracy, which may hint at redundancy in the learning\nprocedure. Furthermore, this redundancy is maintained when we consider the intersection\nomission of the real/imaginary parts of both the data and the MLP weights. This phenomenon\ncan be explained through the Kramers-Kronig relation (KKR) [42, 43], which provides a\nrepresentation of the real component of an analytic complex-valued function in terms of\nits complex components and vice versa. Roughly speaking, for a complex-valued function\nc(ω) = Re{c}(ω) + iIm{c}(ω), the KKR are given by\nRe{c}(ω) = 1\nπ\nZ ∞\n−∞\nIm{c}(σ)\nω −σ\ndσ,\nIm{c}(ω) = −1\nπ\nZ ∞\n−∞\nRe{c}(σ)\nω −σ\ndσ.\nThus, we conjecture that masking one component forces the other to recover both in the learning\nprocedure by implicitly approximating the KRR. We therefore believe that a sophisticated\nsystem design that considers a KRR-based architecture may lead to the sufficiency of a single\ncomponent in the forecasting task but leaves a complete study of that subject to future work.\nThis phenomenon is further explored in Appendix 8.\n6\nConclusion\nThis paper presents FIA-Net, a new model for long-term time series forecasting using STFT\nwindow aggregation in the frequency domain and HC MLPs. The proposed methodology\nshows superior performance over existing SoTA on standard benchmark datasets. We show\nthat treating the set of STFT windows as a single HC tensor, which is processed by a novel\nHC-MLP, significantly reduces the total amount of parameters, with no degradation in the\nTSF accuracy. We study various schemes to increase model efficiency by, for example, choosing\nthe top-M magnitude frequency components. Experimental results show that the omission\n11\n\n\nDataset\nI/O\n96/96\n96/192\n96/336\n96/720\nHidden Part\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nETTm1\nXReal\n0.0522\n0.0797\n0.0560\n0.0850\n0.0597\n0.0888\n0.0658\n0.0958\nXImag\n0.0521\n0.0792\n0.0562\n0.0844\n0.0592\n0.0879\n0.0684\n0.0976\nW Real\n0.0522\n0.0791\n0.0557\n0.0843\n0.0588\n0.0875\n0.0669\n0.0964\nW Imag\n0.0526\n0.0801\n0.0560\n0.0849\n0.0596\n0.0888\n0.0651\n0.0953\nW Imag, XImag\n0.0523\n0.0798\n0.0560\n0.0849\n0.0592\n0.0884\n0.0644\n0.0947\nW Real, XReal\n0.0522\n0.0791\n0.0557\n0.0843\n0.0588\n0.0887\n0.0669\n0.0930\n∅\n0.0522\n0.0791\n0.0565\n0.0848\n0.0592\n0.0878\n0.0685\n0.0975\nTable 3: Performance comparison on ETTm1 for I/O = 96 × {96, 192, 336, 720} with various\nmodes. XReal/XImag hide the real/imaginary parts of the input, while W Real/W Imag zero out\nthe corresponding weights. Completely ignoring both components is denoted as\n(W Imag, XImag) or (W Real, XReal).\nof one of the complex representation components does not induce notable segregation in\nperformance, which may be explained by the KKR. For future work, we aim to leverage the\nKKR equations to propose a forecasting model that only considers the real component in\nthe complex representation while operating over the complex plane. Additionally, we plan to\nfurther investigate the relationship between the number of adjacent STFT windows in the\nWM-MLP backbone and the statistical properties of the datasets.\nReferences\n[1] R. A. Rajagukguk, R. A. Ramadhan, and H.-J. Lee. A review on deep learning models\nfor forecasting time series data of solar irradiance and photovoltaic power. Energies,\n13(24):6623, 2020.\n[2] X. Chen and R. Chen. A review on traffic prediction methods for intelligent transporta-\ntion system in smart cities. In 2019 12th International Congress on Image and Signal\nProcessing, BioMedical Engineering and Informatics (CISP-BMEI), pages 1–5. IEEE,\n2019.\n[3] O. B. Sezer, M. U. Gudelek, and A. M. Ozbayoglu. Financial time series forecasting\nwith deep learning: A systematic literature review: 2005–2019. Applied Soft Computing,\n90:106181, 2020.\n[4] J. Zhang and K.-F. Man. Time series prediction using rnn in multi-dimension embedding\nphase space. In Proceedings of the IEEE International Conference on Systems, Man, and\nCybernetics (SMC), pages 1868–1873. IEEE, 1998.\n[5] A. Graves. Supervised sequence labelling with recurrent neural networks. Studies in\nComputational Intelligence, 385:37–45, 2012.\n[6] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent\nneural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n12\n\n\n[7] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, Ł. Kaiser,\nN. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2tensor for\nneural machine translation. CoRR, abs/1803.07416, 2018.\n[8] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin. FEDformer: Frequency enhanced\ndecomposed transformer for long-term series forecasting. In Proc. 39th International\nConference on Machine Learning (ICML 2022), 2022.\n[9] H. Wu, J. Xu, J. Wang, and M. Long. Autoformer: Decomposition transformers with\nAuto-Correlation for long-term series forecasting. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2021.\n[10] Y. Zhang and J. Yan. Crossformer: Transformer utilizing cross-dimension dependency\nfor multivariate time series forecasting. In The Eleventh International Conference on\nLearning Representations (ICLR), 2023.\n[11] R. Pascanu, T. Mikolov, and Y. Bengio.\nOn the difficulty of training recur-\nrent neural networks.\narXiv preprint arXiv:1211.5063v2,\n2013.\nAvailable at:\nhttps://arxiv.org/abs/1211.5063.\n[12] D. Kim, J. Park, J. Lee, and H. Kim. Are self-attentions effective for time series forecasting?\narXiv preprint arXiv:2405.16877, 2024.\n[13] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang. Connecting the dots:\nMultivariate time series forecasting with graph neural networks. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining\n(KDD), pages 753–763, 2020.\n[14] B. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio. N-beats: Neural basis expansion\nanalysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.\n[15] K. Yi, Q. Zhang, L. Cao, S. Wang, G. Long, L. Hu, H. He, Z. Niu, W. Fan, and H. Xiong.\nA survey on deep learning based time series analysis with frequency transformation.\nJournal of the ACM, 37(4):111, 2023.\n[16] K. Yi, Q. Zhang, W. Fan, S. Wang, P. Wang, H. He, N. An, D. Lian, L. Cao, and\nZ. Niu. Frequency-domain MLPs are more effective learners in time series forecasting. In\nThirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023.\n[17] R. Shen, L. Liu, B. Wang, Y. Guan, Y. Yang, and J. Jiang. Freqtsf: Time series forecasting\nvia simulating frequency kramer-kronig relations. arXiv preprint arXiv:2407.21275, 2024.\n[18] D. Gabor. Theory of communication. part 1: The analysis of information. Journal of\nthe Institution of Electrical Engineers-Part III: Radio and Communication Engineering,\n93(26):429–441, 1946.\n[19] G. E. P. Box and G. M. Jenkins. Some recent advances in forecasting and control. Journal\nof the Royal Statistical Society: Series C (Applied Statistics), 17(2):91–109, 1968.\n13\n\n\n[20] G. E. P. Box and D. A. Pierce. Distribution of residual autocorrelations in autoregressive-\nintegrated moving average time series models.\nJournal of the American Statistical\nAssociation, 65:1509–1526, 1970.\n[21] M. W. Watson. Vector autoregressions and cointegration. Technical report, Princeton\nUniversity, 1993.\n[22] H. Zhou, J. Li, S. Zhang, S. Zhang, M. Yan, and H. Xiong. Expanding the prediction\ncapacity in long sequence time-series forecasting. Artificial Intelligence, 318:103886, 2023.\n[23] N. Kitaev, Ł. Kaiser, and A. Levskaya.\nReformer:\nThe efficient transformer.\nIn\nInternational Conference on Learning Representations (ICLR), 2020.\narXiv preprint\narXiv:2001.04451.\n[24] Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam. A time series is worth 64\nwords: Long-term forecasting with transformers. In International Conference on Learning\nRepresentations (ICLR), 2023.\n[25] L. Bai, L. Yao, C. Li, X. Wang, and C. Wang. Adaptive graph convolutional recurrent\nnetwork for traffic forecasting. In Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n[26] S. Woo, S. Lee, J. Kim, S. Kim, H. Kim, and W. Jang.\nEtsformer: Exponential\nsmoothing transformer for time-series forecasting. In Proceedings of the 38th International\nConference on Machine Learning (ICML), 2022.\n[27] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, T. Yao, W. Yin, and R. Jin. Film: Frequency\nimproved legendre memory model for long-term time series forecasting. arXiv preprint\narXiv:2205.08897, 2022.\n[28] L. Zhang, C. C. Aggarwal, and G.-J. Qi. Stock price prediction via discovering multi-\nfrequency trading patterns. In Proceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (KDD), pages 2141–2149, 2017.\n[29] L. Cao, K. Yi, L. Hu, Q. Zhang, N. Cao, and Z. Niu. Stemgnn: Graph neural networks for\nmultivariate time series forecasting. In Proceedings of the 37th International Conference\non Machine Learning (ICML), 2020.\n[30] W. R. Hamilton. On quaternions; or on a new system of imaginaries in algebra. Proceedings\nof the Royal Irish Academy, 1844.\n[31] T. Parcollet, M. Morchid, P.-M. Bousquet, R. Dufour, and G. Linarès. Quaternion\nconvolutional neural networks for image classification and compression. In Proceedings of\nthe IEEE Spoken Language Technology Workshop (SLT), pages 362–368. IEEE, 2016.\n[32] L. Luo, H. Feng, and L. Ding. Color image compression based on quaternion neural\nnetwork principal component analysis. In 2010 International Conference on Multimedia\nTechnology, pages 1–4. IEEE, 2010.\n[33] L. S. Saoud and H. Al-Marzouqi. Metacognitive sedenion-valued neural network and its\nlearning algorithm. IEEE Access, 8:144823–144836, 2020.\n14\n\n\n[34] R. Kycia and A. Niemczynowicz. Hypercomplex neural network in time series forecasting\nof stock data. arXiv preprint arXiv:2401.04632, 2024.\n[35] K. Yi, Q. Zhang, L. Hu, N. Cao, and Z. Niu.\nCost: A contrastive framework for\nself-supervised time series representation learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2022.\n[36] S. B. Davis and P. Mermelstein. Comparison of parametric representations for monosyllabic\nword recognition in continuously spoken sentences. IEEE Transactions on Acoustics,\nSpeech, and Signal Processing, 28(4):357–366, 1980.\n[37] P. Fryzlewicz and H. Cho. Multiple-change-point detection for auto-regressive conditional\nheteroscedastic processes. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 76(5):903–924, 2014.\n[38] K. V. Khmelnytskaya and M. Shapiro. Function theories in cayley-dickson algebras and\nnumber theory. Complex Analysis and Operator Theory, 15(2):1–40, 2021.\n[39] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time\nseries forecasting?\nIn Proceedings of the AAAI Conference on Artificial Intelligence,\n2023.\n[40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.\n[41] D. P. Kingma.\nAdam:\nA method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[42] R. de L. Kronig. On the theory of dispersion of x-rays. Journal of the Optical Society of\nAmerica, 12(6):547–557, 1926.\n[43] H. A. Kramers.\nLa diffusion de la lumière par les atomes.\nIn Atti del Congresso\nInternazionale dei Fisici, Como, volume 2, pages 545–557, 1927.\n[44] I. L. Kantor and A. S. Solodovnikov.\nHypercomplex Numbers:\nAn Elementary\nIntroduction to Algebras. Springer-Verlag, 1989.\n15\n\n\nAppendix\nA\nNotations & Symbols\nA.1\nNotation\nWe provide a detailed table of the involved notation in this paper:\nSymbol\nDescription\nB\nBatch size.\nL\nLookback window size.\nD\nNumber of features for each time step.\nT\nLength of the prediction horizon.\nE\nEmbedding size.\nM\nNumber of frequencies to select from all the frequencies using the top M\nmagnitudes.\nX\nMultivariate time series with a lookback window of size L at timestamps\nt.\nXt\nMultivariate values of D distinct series at timestamp t.\nXt,i\nThe value of the i-th feature of the distinct series at timestamp t.\nˆX\nGround truth target values.\nσ\nactivation function\nP\nNumber of windows in the STFT.\nNFFT\nNumber of frequency bins in each window of the STFT.\nω\nWindow function for the STFT.\nXE\nX after traversing through the embedding layer.\nXRec\nThe reconstructed X after the frequency alteration.\nct\ni\nThe i-th window of the input in the time domain.\nCi\nThe i-th window of the STFT containing NFFT frequency bins.\nCin\ni\nThe i-th window of the STFT, retaining the top M frequency components\nbased on magnitude.\nCout\ni\nThe i-th window of the STFT after the WM-MLP/WHC has been applied.\nWi→j\nThe weights that capture the frequency energy shift between window i\nand j, defined as Wi→j = W Real\ni→j + jW Img\ni→j, where Wi→j ∈CE×E.\nBi→j\nThe bais that capture the frequency energy shift between windows i and\nj, defined as Bi→j = BReal\ni→j + jBImg\ni→j, where Bi→j ∈CE.\nTable 4: Table of Symbols and Descriptions\n16\n\n\nA.2\nDimensions\nThe following table summarizes the dimensions of the data tensor in every step of the FIA-Net.\nSymbol\nDimension\nX\nRB×L×D\nXE\nRB×L×D×E\nCi\nCB×NF F T ×D×E\nCM\ni\nCB×M×D×E\nCin/out\ni\nCB×M×D×E\nXRec\nRB×L×D×E\nˆX\nRB×T×D\nTable 5: Table of Symbols and Dimension\nB\nAdditional Experimental Details\nB.1\nDataset Descriptions\nIn our experiments, we utilized thirteen real-world datasets to assess the effectiveness of\nmodels for long-term TSF. Below, we provide the details of these datasets, categorized by\ntheir forecasting horizon.\n• Exchange: This dataset includes daily exchange rates for eight countries (Australia,\nBritain, Canada, Switzerland, China, Japan, New Zealand, and Singapore) from 1990 to\n2016.\n• Weather: This dataset gathers 21 meteorological indicators, including humidity and air\ntemperature, from the Weather Station of the Max Planck Biogeochemistry Institute in\nGermany in 2020. The data is collected every 10 minutes.\n• Traffic: For long-term forecasting, this dataset includes hourly traffic data from 862\nfreeway lanes in San Francisco, with data collected since January 1, 2015.\n• Electricity: For long-term forecasting, this dataset covers electricity consumption data\nfrom 321 clients, with records starting from January 1, 2011, and a sampling interval of\n15 minutes.\n• ETT: This dataset is sourced from two electric transformers, labeled ETTh1 and ETTm1,\nwith two different resolutions: 15 minutes and 1 hour. These are used as benchmarks for\nlong-term forecasting.\n17\n\n\nDatasets\nWeather\nTraffic\nElectricity\nETTh1\nETTm1\nExchange\nRates\nFeatures\n21\n862\n321\n7\n7\n8\nTimesteps\n52696\n17544\n26304\n17420\n69680\n7588\nFrequency\n10m\n1h\n1h\n1h\n15m\n1d\nLookback Win-\ndow\n96\n48\n96\n96\n96\n96\nPrediction\nLength\n96, 192, 336,\n720\n96, 192, 336,\n720\n96, 192, 336,\n720\n96, 192, 336,\n720\n96, 192, 336,\n720\n96, 192, 336,\n720\nTable 6: Long Term Datasets Parameters\nB.2\nbaselines\nWe employ a selection of SoTA representative models for our comparative analysis, focusing\non Transformer-based architectures and other popular models. The models included are as\nfollows:\n• Informer: Informer enhances the efficiency of self-attention mechanisms to effectively\ncapture dependencies across variables. The source code was obtained from GitHub, and\nwe utilized the default configuration with a dropout rate of 0.05, two encoder layers, one\ndecoder layer, a learning rate of 0.0001, and the Adam optimizer.\n• Reformer: Reformer combines the power of Transformers with efficient memory and\ncomputation management, especially for long sequences. The source code was sourced\nfrom GitHub, and we employed the recommended configuration for our experiments.\n• Autoformer: Autoformer introduces a decomposition block embedded within the model\nto progressively aggregate long-term trends from intermediate predictions. The source\ncode was accessed from GitHub, and we followed the recommended settings for all\nexperiments.\n• FEDformer: FEDformer introduces an attention mechanism based on low-rank ap-\nproximation in the frequency domain combined with a mixture of expert decomposition\nto handle distribution shifts. The source code was retrieved from GitHub. We utilized\nthe Frequency Enhanced Block (FEB-f) and selected the random mode with 64 as the\nexperimental configuration.\n• LTSF-Linear: LTSF-Linear is a minimalist model employing simple one-layer linear\nmodels to learn temporal relationships in time series data. We used it as our baseline\nfor long-term forecasting, downloading the source code from GitHub, and adhered to\nthe default experimental settings.\n• PatchTST: PatchTST is a Transformer-based model designed for TSF, introducing\npatching and a channel-independent structure to enhance model performance. The\nsource code was obtained from GitHub, and we used the recommended settings for all\nexperiments.\n• FreTS: FRETS is a sophisticated model tailored for efficient TSF by exploiting a\nfrequency domain approach. The implementation is available on GitHub, and we utilized\n18\n\n\nthe default configuration as recommended by the authors. In our work, FRETS serves\nas the foundational model. We address its limitations, particularly its handling of\nnon-stationary data, while adapting its strengths, such as its complex frequency learner.\nTo fully grasp the contributions of this paper, we recommend reviewing FRETS in detail\nfirst.\nB.3\nImplementation Details\nTable 7 lists the hyperparameter values used in the FIA-Net implementation. Both WM-MLP\nand HC-MLP backbones are implemented with the same hyperparameter values, except for p,\nthe number of STFT windows.\nDataSets\nWeather\nTraffic\nElectricity\nETTh1\nETTm1\nExchange\nrate\nBatch Size\n16\n4\n4\n8\n8\n8\nEmbed Size\n128\n32\n64\n128\n128\n128\nHidden Size\n256\n256\n256\n256\n256\n256\nNFF\n16\n32\n32\n6\n48\n32\nSTFT Windows\n7\n13\n13\n33\n4\n13\nS-M\n10\nMmax\n4\n4\n4\nMmax\nEpoch\n10\n10\n10\n10\n10\n10\nTable 7: Hyperparameter Settings for Long-Term Datasets for the WM-MLP and HC-MLP\nB.4\nEvaluation Metrics\nIn this study, we use the Mean Squared Error (MSE) as the loss function during training.\nHowever, for evaluation, we report both the Mean Absolute Error (MAE) and the Root Mean\nSquared Error (RMSE).\nwhich are defined as follows:\nMSE = 1\nn\nn\nX\ni=1\n(Yi −ˆYi)2,\nRMSE =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(Yi −ˆYi)2,\nMAE = 1\nn\nn\nX\ni=1\n|Yi −ˆYi|\nWhere:\n• Yi represents the true target values,\n• ˆYi represents the predicted values,\n• n is the total number of samples.\nB.5\nNormalization Methods\nIn this study, similar to the FRETS model [16], we apply min-max normalization to standardize\nthe input data to the range between 0 and 1. This method helps in ensuring that all features\ncontribute equally to the model and prevents any specific feature from dominating due to\ndifferences in scale. The formula for min-max normalization is given by:\nXNorm =\nX −Xmin\nXmax −Xmin\n19\n\n\nBy normalizing the data, we ensure that all input features are within the same range, which\ncan improve model convergence and performance.\nC\nAdditional Information on HC Numbers and Models\nIn this section we extend the discussion on HC numbers, considering additional values of\np beyond p = 4. We couple the presentation with the construction of the corresponding\nHC-MLP in the considered base. Recall that the base of a HC number, i.e., the number of its\ncomponents is given by b = 2p. While hyper-complex number can be defined for any value\nof b, most research has been performed on b that is given by a power of 2, as the resulting\nstructure of the (algebraic) field. The addition of two HC numbers is simply given by the\ncomponent-wise summation. In what follows, we focus on HC multiplication and additional\nproperties. For more information on the HC number system„ we refer the reader to [44].\nC.1\nBase 2 - Complex Numbers\nWhen b = 2, the resulting field is the complex plane C. We describe C for completeness\nof presentation. Given two complex numbers C1 = α1 + jα2 and C2 = β1 + jβ2, where\nα1, α2, β1, β2 are real numbers, their complex multiplication is defined as:\nC1 · C2 = (α1β1 −α2β2) + j(α1β2 + α2β1)\nThe norm of a complex number is given by:\n|C1|C =\nq\nα2\n1 + α2\n2,\nwhich is preserved under multiplication, i.e.,\n|C1 · C2|C = |C1|C · |C2|C.\nSince the STFT with a single window (p = 1) is equivalent to the standard FFT, applying our\nmethod for hyper-complex number MLP results in the following equation:\nCin = FFT(X)\nCout = σ(Cin\nReal·W1,Real−Cin\nImag·W1,Imag+B1,Real)+σ(j(Cin\nReal·W1,Imag+Cin\nImag·W1,Real+B1,Imag)\nHere, Wi ∈CE×E denotes the layer weights, B ∈CE represents the bias term, and the\nmultiplication occurs across the embedding dimension. Note that for b = 2 the HV formulation\nboils down to the one from [16]. Thus, the HC-MLP can be considered as an HC generalization\nof the FD-MLP. which allows for efficient window aggregation.\nC.2\nBase 4 - Quaternions\nDenote the field of Quaternions with ˜Q. We represent Quatenions with a couple of Complex\nnumber, i.e., for H1, H2 ∈˜Q, H1 = (α1, α2) and H2 = (β1, β2), their multiplication is defined\nas\nH1 · H2 = (α1β1 −α2β2,\nα2β1 + α1β2)\n20\n\n\nThe norm of a quaternion is given by:\n|q|˜Q =\nq\n|α1|2\nC + |α2|2\nC\nThe norm is preserved under multiplication, meaning:\n|q1 · q2|˜Q = |q1|˜Q · |q2|˜Q\nFor our model, the corresponding HC-MLP (which we denote QuatMLP) operating on\nCin = (Cin\n1 , Cin\n2 ) ∈˜Q, is given by,\nCout = QuatMLP(Cin) = σ(Cin · W + B)\nwhere:\nCout\n1\n= σ(C1 · W1 −C2 · W2 + B1),\nCout\n2\n= σ(C2 · W1 + C1 · W2 + B2).\nHere, Wi ∈CE×E, i = 1, 2 denote the layer weights, B ∈CE represents the bias term, and\nthe multiplication involves complex MLP operations across the embedding dimension.\nC.3\nBase 16 - Sedenions\nElements on the Sedenions field, denoted S, are denoted with 8-tuples of complex numbers.\nGiven two sedenions represented by complex numbers S1, S2 ∈S, S1 = (α1, α2, . . . , α8) and\nS2 = (β1, β2, . . . , β8), their multiplication is given by\nS1 · S2 =\n\n\n\n\n\n\n\n\n\n\n\n\n\nα1β1 −α2β2 −α3β3 −α4β4 −α5β5 −α6β6 −α7β7 −α8β8\nα1β2 + α2β1 + α3β4 −α4β3 + α5β6 −α6β5 + α7β8 −α8β7\nα1β3 −α2β4 + α3β1 + α4β2 + α5β7 −α6β8 −α7β5 + α8β6\nα1β4 + α2β3 −α3β2 + α4β1 + α5β8 + α6β7 −α7β6 −α8β5\nα1β5 −α2β6 −α3β7 −α4β8 + α5β1 + α6β2 + α7β3 + α8β4\nα1β6 + α2β5 −α3β8 + α4β7 −α5β2 + α6β1 −α7β4 + α8β3\nα1β7 + α2β8 + α3β5 −α4β6 −α5β3 + α6β4 + α7β1 −α8β2\nα1β8 −α2β7 + α3β6 + α4β5 −α5β4 −α6β3 + α7β2 + α8β1\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere each component follows the rules of Complex multiplication. The norm of a sedenion is\ngiven by:\n|S|S =\nv\nu\nu\nt\n8\nX\nj=1\n|αj|2\nC\nUnlike nase 2, 4 and 8, Sedenions do not preserve the norm under addition and multiplication.\nThe base-16 HC-MLP, denoted SedMLP, operating on an input Cin from the STFT with\nmultiple windows Cin = (Cin\nj )8\nj=1, is given by\nCout = SedMLP(Cin) = σ(Cin · W + B)\n21\n\n\nwhere:\nCout\n1\n= σ\n\u0000Cin\n1 W1 −Cin\n2 W2 −Cin\n3 W3 −Cin\n4 W4 −Cin\n5 W5 −Cin\n6 W6 −Cin\n7 W7 −Cin\n8 W8 + B1\n\u0001\nCout\n2\n= σ\n\u0000Cin\n1 W2 + Cin\n2 W1 + Cin\n3 W4 −Cin\n4 W3 + Cin\n5 W6 −Cin\n6 W5 + Cin\n7 W8 −Cin\n8 W7 + B2\n\u0001\nCout\n3\n= σ\n\u0000Cin\n1 W3 −Cin\n2 W4 + Cin\n3 W1 + Cin\n4 W2 + Cin\n5 W7 −Cin\n6 W8 −Cin\n7 W5 + Cin\n8 W6 + B3\n\u0001\nCout\n4\n= σ\n\u0000Cin\n1 W4 + Cin\n2 W3 −Cin\n3 W2 + Cin\n4 W1 + Cin\n5 W8 + Cin\n6 W7 −Cin\n7 W6 −Cin\n8 W5 + B4\n\u0001\nCout\n5\n= σ\n\u0000Cin\n1 W5 −Cin\n2 W6 −Cin\n3 W7 −Cin\n4 W8 + Cin\n5 W1 + Cin\n6 W2 + Cin\n7 W3 + Cin\n8 W4 + B5\n\u0001\nCout\n6\n= σ\n\u0000Cin\n1 W6 + Cin\n2 W5 −Cin\n3 W8 + Cin\n4 W7 −Cin\n5 W2 + Cin\n6 W1 −Cin\n7 W4 + Cin\n8 W3 + B6\n\u0001\nCout\n7\n= σ\n\u0000Cin\n1 W7 + Cin\n2 W8 + Cin\n3 W5 −Cin\n4 W6 −Cin\n5 W3 + Cin\n6 W4 + Cin\n7 W1 −Cin\n8 W2 + B7\n\u0001\nCout\n8\n= σ\n\u0000Cin\n1 W8 −Cin\n2 W7 + Cin\n3 W6 + Cin\n4 W5 −Cin\n5 W4 −Cin\n6 W3 + Cin\n7 W2 + Cin\n8 W1 + B8\n\u0001\nHere, Wi ∈CE×E, i = 1, . . . , 8 denotes the layer weights, B ∈CE represents the bias term,\nand the multiplication involves complex MLP operations across the embedding dimension.\n22\n\n\nD\nAdditional Ablation Studies\nThis section presents additional ablation studies, expanding on the findings reported in Section\n5.3. We analyze the impact of FFT resolution, embedding size, and the number of STFT\nwindows on WM-MLP performance. Additionally, we include further results for the frequency\ncompression, sequence length, and real vs. imaginary component discussions. Furthermore, we\nprovide a comparative analysis of various hyper-complex fields (octonions, quaternions, and\nsedenions) for the HC-MLP and report the corresponding results.\nD.1\nParameter Sensitivity\nIn this section, we conduct a parameter sweep to examine the effects of different hyperparame-\nters on model performance. To accomplish this, we utilize two datasets: the ETTh1 dataset\nand the electricity dataset. Each section presents four graphs illustrating the results on the two\ndatasets for a configuration of I/O = 96 × 96, 336. Except for the specific experiment sweep,\nthe embedding size is set to 128 for the ETTh1 dataset and 64 for the electricity dataset, with\nM set to 0 for all datasets.\nEmbed Size\nIn this section, we evaluate the influence of embedding size on the model’s perfor-\nmance. We conducted experiments with embedding dimensions E ∈{1, 2, 4, 8, 16, 32, 64, 128, 256, 512},\nwhile keeping the following parameters fixed: NFFT = 16, B = 8, p = 13, and M = Mmax.\nWe can observe that as we increase the embedding size, the loss decreases until we reach a\ncertain point (which is dependent on the dataset). This is likely because a larger embedding\nsize enables the model to capture more features; however, an excessively high embedding size\nmay lead to overfitting.\n0 1 2 3 4 5 6 7 8 9 10\nlog2 (E) (Embed Size)\n0.090\n0.095\n0.100\n0.105\n0.110\nRMSE\n0.0600\n0.0625\n0.0650\n0.0675\n0.0700\n0.0725\n0.0750\n0.0775\nMAE\nRMSE\nMAE\n(a) T=96 on ETTh1\n0 1 2 3 4 5 6 7 8 9 10\nlog2 (E) (Embed Size)\n0.100\n0.105\n0.110\n0.115\n0.120\n0.125\n0.130\nRMSE\n0.070\n0.075\n0.080\n0.085\n0.090\n0.095\n0.100\nMAE\nRMSE\nMAE\n(b) T=336 on ETTh\n0\n1\n2\n3\n4\n5\n6\n7\n8\nlog2 (E) (Embed Size)\n0.070\n0.072\n0.074\n0.076\n0.078\n0.080\nRMSE\n0.042\n0.044\n0.046\n0.048\n0.050\n0.052\nMAE\nRMSE\nMAE\n(c) T=96 on electricity\n0\n1\n2\n3\n4\n5\n6\n7\n8\nlog2 (E) (Embed Size)\n0.074\n0.076\n0.078\n0.080\n0.082\nRMSE\n0.046\n0.048\n0.050\n0.052\n0.054\nMAE\nRMSE\nMAE\n(d) T=336 on electricity\nFigure 7: Comparison of MSE and MAE across different values of E for varying T on the\nETTh1 and Electricity datasets.\nAmount of Windows (High Dim)\nIn this section, we evaluate the influence of the\nnumber of windows (p) on the model’s performance. We conducted experiments with different\nwindow counts p ∈{3, 6, 14, 17, 25, 33}, while keeping the following parameters fixed: B = 8,\nM = Mmax, and the overlap between windows is 50%.\nFFT Resolution (NFFT)\nIn this section, we evaluate the influence of the FFT resolution\n(NFFT) on the model’s performance.\nWe conducted experiments with different NFFT ∈\n23\n\n\n4 7 9\n13 17\n25\n33\nNumber of STFT context windows (p)\n0.0874\n0.0875\n0.0876\n0.0877\n0.0878\n0.0879\nRMSE\n0.0584\n0.0586\n0.0588\n0.0590\nMAE\nRMSE\nMAE\n(a) T=96 on ETTh1\n4 7 9\n13 17\n25\n33\nNumber of STFT context windows (p)\n0.0995\n0.0996\n0.0997\n0.0998\n0.0999\nRMSE\n0.0686\n0.0687\n0.0688\n0.0689\n0.0690\nMAE\nRMSE\nMAE\n(b) T=336 on ETTh1\n4 7 9\n13 17\n25\n33\nNumber of STFT context windows (p)\n0.0703\n0.0704\n0.0705\n0.0706\n0.0707\n0.0708\n0.0709\n0.0710\nRMSE\n0.0425\n0.0426\n0.0427\n0.0428\n0.0429\n0.0430\n0.0431\nMAE\nRMSE\nMAE\n(c) T=96 on electricity\n4 7 9 13 17\n25\n33\nNumber of STFT context windows (p)\n0.0737\n0.0738\n0.0739\n0.0740\nRMSE\n0.04685\n0.04690\n0.04695\n0.04700\n0.04705\n0.04710\n0.04715\nMAE\nRMSE\nMAE\n(d) T=336 on electricity\nFigure 8: Comparison of MSE and MAE across different values of p for varying T on the\nETTh1 and Electricity datasets.\n{6, 8, 12, 16, 24, 32, 48}, while keeping the following parameters fixed: p = 25, B = 8, M =\nMmax.\n8 1216\n24\n32\n48\nNFFT - Window size / FFT resolution\n0.08775\n0.08780\n0.08785\n0.08790\n0.08795\nRMSE\n0.05895\n0.05900\n0.05905\n0.05910\nMAE\nRMSE\nMAE\n(a) T=96 on ETTh1\n8 1216\n24\n32\n48\nNFFT - Window size / FFT resolution\n0.0996\n0.0998\n0.1000\n0.1002\n0.1004\n0.1006\nRMSE\n0.06850\n0.06875\n0.06900\n0.06925\n0.06950\n0.06975\n0.07000\nMAE\nRMSE\nMAE\n(b) T=336 on ETTh1\n8 12\n1216\n24\n32\n48\nNFFT - Window size / FFT resolution\n0.0702\n0.0703\n0.0704\n0.0705\n0.0706\n0.0707\n0.0708\n0.0709\nRMSE\n0.0425\n0.0426\n0.0427\n0.0428\n0.0429\n0.0430\nMAE\nRMSE\nMAE\n(c) T=96 on electricity\n8 1216\n24\n32\n48\nNFFT - Window size / FFT resolution\n0.0733\n0.0734\n0.0735\n0.0736\n0.0737\nRMSE\n0.0466\n0.0467\n0.0468\n0.0469\n0.0470\nMAE\nRMSE\nMAE\n(d) T=336 on electricity\nFigure 9: Comparison of MSE and MAE across different values of NFFT for varying T on the\nETTh1 and Electricity datasets.\nFrequency Choose Max (M)\nIn this section, we provide additional results for various\ndatasets and prediction lengths T regarding the discussion on frequency compression 5.\n1\n2\n4\n8\n12\n16\n20 Mmax\nM - Selected quantity of frequencies\n0.0870\n0.0872\n0.0874\n0.0876\n0.0878\n0.0880\nRMSE\n0.0578\n0.0580\n0.0582\n0.0584\n0.0586\nMAE\nRMSE\nMAE\n(a) T=96 on ETTh1\n1\n2\n4\n8\n12\n16\n20 Mmax\nM - Selected quantity of frequencies\n0.0984\n0.0986\n0.0988\n0.0990\n0.0992\n0.0994\n0.0996\n0.0998\nRMSE\n0.0674\n0.0676\n0.0678\n0.0680\nMAE\nRMSE\nMAE\n(b) T=336 on ETTh1\n1\n2\n4\n8\n12\n16\n20 Mmax\nM - Selected quantity of frequencies\n0.0698\n0.0700\n0.0702\n0.0704\nRMSE\n0.0418\n0.0419\n0.0420\n0.0421\nMAE\nRMSE\nMAE\n(c) T=96 on electricity\n1\n2\n4\n8\n12\n16\n20 Mmax\nM - Selected quantity of frequencies\n0.0729\n0.0730\n0.0731\n0.0732\n0.0733\n0.0734\n0.0735\nRMSE\n0.0457\n0.0458\n0.0459\n0.0460\n0.0461\nMAE\nRMSE\nMAE\n(d) T=336 on electricity\nFigure 10: Comparison of MSE and MAE across different values of M for various T on the\nETTh1 and Electricity datasets.\n24\n\n\nD.2\nDifferent LookBack Window\nIn this section, we present additional results for various lookback windows on the ETTh1 and\nETTm1 datasets.\nT = 96 (MAE)\nT = 192 (MAE)\nT = 336 (MAE)\nT = 720 (MAE)\nT = 96 (RMSE)\nT = 192 (RMSE)\nT = 336 (RMSE)\nT = 720 (RMSE)\n672\n576\n480\n288\n192\n96\n24\nSequence Length (L)\n0.060\n0.065\n0.070\n0.075\n0.080\nMAE\n0.090\n0.095\n0.100\n0.105\n0.110\nRMSE\n(a) ETTh1 Dataset\n672\n576\n480\n288\n192\n96\n24\nSequence Length (L)\n0.05\n0.06\n0.07\n0.08\nMAE\n0.08\n0.09\n0.10\n0.11\n0.12\nRMSE\n(b) ETTm1 Dataset\nFigure 11: MAE and RMSE in relation to the Lookback Window L for varying prediction\nlengths T ∈{96, 192, 336, 720} for the ETTh1 and ETTm1 datasets.\n25\n\n\nD.3\nReal Vs imaginary Components\nThis section provides additional information regarding the real versus imaginary experiment\ndiscussed in Section 5.3.3.\nDataset\nI/O\n96/96\n96/192\n96/336\n96/720\nHidden Part\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nETTm1\nXReal\n0.0522\n0.0797\n0.0560\n0.0850\n0.0597\n0.0888\n0.0658\n0.0958\nXImag\n0.0521\n0.0792\n0.0562\n0.0844\n0.0592\n0.0879\n0.0684\n0.0976\nW Real\n0.0522\n0.0791\n0.0557\n0.0843\n0.0588\n0.0875\n0.0669\n0.0964\nW Imag\n0.0526\n0.0801\n0.0560\n0.0849\n0.0596\n0.0888\n0.0651\n0.0953\nW Imag, XImag\n0.0523\n0.0798\n0.0560\n0.0849\n0.0592\n0.0884\n0.0644\n0.0947\nW Real, XReal\n0.0522\n0.0791\n0.0557\n0.0843\n0.0588\n0.0887\n0.0669\n0.0930\nNormal\n0.0522\n0.0791\n0.0565\n0.0848\n0.0592\n0.0878\n0.0685\n0.0975\nETTh1\nXReal\n0.0584\n0.0877\n0.0638\n0.0944\n0.0684\n0.0997\n0.0767\n0.1047\nXImag\n0.0582\n0.0879\n0.0634\n0.0943\n0.0679\n0.0997\n0.0756\n0.1041\nW Real\n0.0586\n0.0880\n0.0644\n0.0948\n0.0685\n0.0998\n0.0759\n0.1039\nW Imag\n0.0584\n0.0880\n0.0646\n0.0951\n0.0694\n0.1008\n0.0781\n0.1065\nW Imag, XImag\n0.0586\n0.0880\n0.0644\n0.0947\n0.0685\n0.0998\n0.0759\n0.1040\nW Real, XReal\n0.0587\n0.0882\n0.0642\n0.0948\n0.0690\n0.1005\n0.0765\n0.1050\nNormal\n0.0586\n0.0878\n0.0639\n0.0945\n0.0684\n0.0998\n0.0765\n0.1043\nTable 8: Performance comparison on the ETTm1, ETTh1, and Electricity datasets for\nI/O = 96 × {96, 192, 336, 720} with different modes. XReal and XImag refer to hiding the real\nand imaginary parts of the input, respectively. W Real and W Imag denote zeroing the real and\nimaginary weights, respectively. The cases where both the real and imaginary components are\ncompletely ignored (i.e., both weights and inputs are zeroed) are represented by W Imag, XImag\nand W Real, XReal. MAE and RMSE are reported, where lower values indicate better\nperformance.\nD.4\nHC-MLP Experimental Results With For Various Values of p\nIn this section, we present additional results on the HC-MLP for various bases. Specifically,\nwe provide results for the Quaternion base (p = 2, QuatMLP), Octonion base (p = 4,\nOctMLP), and Sedenion base (p = 8, SedMLP). Additionally, we include results for a model\nthat aggregates all windows without using hyper-complex numbers, referred to as BasicMLP.\nFurther details about its implementation can be found in B.3.\n26\n\n\nTraffic\nETTh1\nETTm1\nMetric\n96\n192\n336\n720\n96\n192\n336\n720\n96\n192\n336\n720\nSedenionMLP (p = 8)\nRMSE\n0.0340\n0.0346\n0.0351\n0.0363\n0.0896\n0.0948\n0.0999\n0.1047\n0.0814\n0.0857\n0.0894\n0.0977\nMAE\n0.0168\n0.0169\n0.0173\n0.0186\n0.0598\n0.0640\n0.0685\n0.0767\n0.0542\n0.0573\n0.0609\n0.0682\nOctontionMLP (p = 4)\nRMSE\n0.0335\n0.0343\n0.0349\n0.0361\n0.0834\n0.0874\n0.0941\n0.1017\n0.0739\n0.0831\n0.0888\n0.0967\nMAE\n0.0166\n0.0167\n0.0172\n0.0185\n0.0579\n0.0635\n0.0676\n0.0759\n0.0496\n0.0556\n0.0603\n0.0673\nQuaternionMLP (p = 2)\nRMSE\n0.0335\n0.0343\n0.0350\n0.0362\n0.0874\n0.0938\n0.0997\n0.1059\n0.0796\n0.0847\n0.0887\n0.0974\nMAE\n0.0165\n0.0167\n0.0172\n0.0184\n0.0580\n0.0633\n0.0687\n0.0783\n0.0526\n0.0564\n0.0603\n0.0678\nBasicMLP\nRMSE\n0.0372\n0.0391\n0.0384\n0.0415\n0.0962\n0.1025\n0.1061\n0.1187\n0.0832\n0.0903\n0.0967\n0.1066\nMAE\n0.0180\n0.0195\n0.0201\n0.0217\n0.0650\n0.0714\n0.0761\n0.0886\n0.0546\n0.0595\n0.0649\n0.0753\nTable 9: Comparison of different hypercomplex structures on the ETT and Traffic datasets.\nQuadMLP (2 windows), OctMLP (4 windows), and SedMLP (8 windows) represent\nhypercomplex models of increasing dimensionality, while BasicMLP is a non-hypercomplex\nlinear model aggregating window information. Performance is reported using MSE and RMSE\nmetrics, where lower values indicate better accuracy.\nD.5\nExtended Neighborhood Aggregation in WM-MLP\nIn this section, we present additional results on the WM-MLP with extended neighborhood\naggregation. Specifically, we provide results for varying neighborhood sizes, where the model\nincorporates information not only from directly adjacent windows but also from second-order\nand third-order neighbors. The experiments were conducted on the ETTm1 and ETTh1\ndatasets with prediction lengths of 96, 192, 336, and 720.\nFor the two-neighbor case, the output Cout\ni\nis computed as:\nCout\ni\n= σ\n\u0010\nCin\ni Wi→i + Cin\ni−1W(i−1)→i + Cin\ni+1W(i+1)→i\n+ Cin\ni−2W(i−2)→i + Cin\ni+2W(i+2)→i + Bi\n\u0011\n.\n(7)\nFor the three-neighbor case, the output Cout\ni\nis computed as:\nCout\ni\n= σ\n\u0010\nCin\ni Wi→i + Cin\ni−1W(i−1)→i + Cin\ni+1W(i+1)→i\n+ Cin\ni−2W(i−2)→i + Cin\ni+2W(i+2)→i\n+ Cin\ni−3W(i−3)→i + Cin\ni+3W(i+3)→i + Bi\n\u0011\n.\n(8)\n27\n\n\nETTh1\nETTm1\nMetric\n96\n192\n336\n720\n96\n192\n336\n720\nWM-MLP (1 Neighbor)\nRMSE\n0.084\n0.088\n0.097\n0.102\n0.076\n0.082\n0.089\n0.094\nMAE\n0.057\n0.064\n0.068\n0.075\n0.052\n0.055\n0.058\n0.064\nWM-MLP (2 Neighbors)\nRMSE\n0.084\n0.087\n0.095\n0.100\n0.074\n0.080\n0.087\n0.092\nMAE\n0.056\n0.063\n0.066\n0.073\n0.051\n0.053\n0.057\n0.062\nWM-MLP (3 Neighbors)\nRMSE\n0.084\n0.087\n0.095\n0.101\n0.075\n0.080\n0.087\n0.094\nMAE\n0.056\n0.063\n0.066\n0.073\n0.051\n0.053\n0.057\n0.063\nTable 10: Performance comparison of WM-MLP with varying numbers of neighbors (1, 2, and\n3) on the ETTh1 and ETTm1 datasets for prediction lengths of 96, 192, 336, and 720.\nMetrics include RMSE and MAE. Results for WM-MLP with one neighbor are derived from\nthe baseline values reported in the original paper.\nD.6\nComplexity Analysis\nWe conducted an asymptotic analysis of modern models to compare their training time,\nmemory usage, and testing steps. The results are summarized in Table 11. The comparison\nhighlights the computational efficiency of the WM-MLP and HC-MLP models relative to other\nstate-of-the-art approaches. Specifically, both models demonstrate competitive performance\nwith logarithmic complexity in training time and memory, and a constant number of testing\nsteps.\nMethod\nTraining Time\nTraining Memory\nTesting Steps\nWM-MLP\nO(L log L\np )\nO(L)\n1\nHC-MLP\nO(L log L\nP + p2)\n1\n3O(L)\n1\nFreTS\nO(L log L)\nO(L)\n1\nPatchTST\nO(L/S)\nO(L/S)\n1\nLTSF-Linear\nO(L)\nO(L)\n1\nFEDformer\nO(L)\nO(L)\n1\nAutoformer\nO(L log L)\nO(L log L)\n1\nInformer\nO(L log L)\nO(L log L)\n1\nTransformer\nO(L2)\nO(L2)\nL\nReformer\nO(L log L)\nO(L log L)\n1\nTable 11: Comparison of models in terms of asymptotic complexity for training time, memory\nusage, and testing steps as a function of the lookback window length (L). Here, S denotes the\npatch size used in PatchTST, and p represents the number of windows in the STFT\ntransformation.\n28\n\n\nD.7\nVisualizations\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.00\n0.02\n0.04\n0.06\n0.08\n(a) Traffic I/O = 96/96\n0\n50\n100\n150\n200\n250\n300\n0.00\n0.02\n0.04\n0.06\n0.08\n(b) Traffic I/O = 96/192\n0\n100\n200\n300\n400\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n(c) Traffic I/O = 96/336\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0.00\n0.02\n0.04\n0.06\n0.08\n(d) Traffic I/O = 96/720\nFigure 12: Ground Truth vs. Predictions for Different I/O Settings (Traffic Dataset).\n29\n\n\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n(a) Electricity I/O = 96/96\n0\n50\n100\n150\n200\n250\n300\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n(b) Electricity I/O = 96/192\n0\n100\n200\n300\n400\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n(c) Electricity I/O = 96/336\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n(d) Electricity I/O = 96/720\nFigure 13: Ground Truth vs. Predictions for Different I/O Settings (Electricity Dataset).\n30\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.19983v1.pdf",
    "total_pages": 30,
    "title": "Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation",
    "authors": [
      "Eyal Yakir",
      "Dor Tsur",
      "Haim Permuter"
    ],
    "abstract": "Time series forecasting is a long-standing problem in statistics and machine\nlearning. One of the key challenges is processing sequences with long-range\ndependencies. To that end, a recent line of work applied the short-time Fourier\ntransform (STFT), which partitions the sequence into multiple subsequences and\napplies a Fourier transform to each separately. We propose the Frequency\nInformation Aggregation (FIA)-Net, which is based on a novel complex-valued MLP\narchitecture that aggregates adjacent window information in the frequency\ndomain. To further increase the receptive field of the FIA-Net, we treat the\nset of windows as hyper-complex (HC) valued vectors and employ HC algebra to\nefficiently combine information from all STFT windows altogether. Using the\nHC-MLP backbone allows for improved handling of sequences with long-term\ndependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up\nto three times fewer parameters than the equivalent standard window aggregation\nmethod. We evaluate the FIA-Net on various time-series benchmarks and show that\nthe proposed methodologies outperform existing state of the art methods in\nterms of both accuracy and efficiency. Our code is publicly available on\nhttps://anonymous.4open.science/r/research-1803/.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}