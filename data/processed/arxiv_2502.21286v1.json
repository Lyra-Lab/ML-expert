{
  "id": "arxiv_2502.21286v1",
  "text": "PUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n1\nEnabling AutoML for Zero-Touch Network\nSecurity: Use-Case Driven Analysis\nLi Yang, Member, IEEE, Mirna El Rajab, Student Member, IEEE, Abdallah Shami, Senior Member, IEEE,\nand Sami Muhaidat, Senior Member, IEEE\nAbstract—Zero-Touch Networks (ZTNs) represent a state-of-\nthe-art paradigm shift towards fully automated and intelligent\nnetwork management, enabling the automation and intelligence\nrequired to manage the complexity, scale, and dynamic nature of\nnext-generation (6G) networks. ZTNs leverage Artificial Intelli-\ngence (AI) and Machine Learning (ML) to enhance operational\nefficiency, support intelligent decision-making, and ensure effec-\ntive resource allocation. However, the implementation of ZTNs is\nsubject to security challenges that need to be resolved to achieve\ntheir full potential. In particular, two critical challenges arise: the\nneed for human expertise in developing AI/ML-based security\nmechanisms, and the threat of adversarial attacks targeting\nAI/ML models. In this survey paper, we provide a comprehensive\nreview of current security issues in ZTNs, emphasizing the need\nfor advanced AI/ML-based security mechanisms that require\nminimal human intervention and protect AI/ML models them-\nselves. Furthermore, we explore the potential of Automated ML\n(AutoML) technologies in developing robust security solutions for\nZTNs. Through case studies, we illustrate practical approaches\nto securing ZTNs against both conventional and AI/ML-specific\nthreats, including the development of autonomous intrusion\ndetection systems and strategies to combat Adversarial ML\n(AML) attacks. The paper concludes with a discussion of the\nfuture research directions for the development of ZTN security\napproaches.\nIndex Terms—Zero-Touch Networks, 6G Network, AutoML,\nAdversarial Attacks, Cybersecurity, Intrusion Detection System,\nNetwork Automation.\nI. INTRODUCTION\nFuture networks are envisioned to achieve fully autonomous\nnetwork and service management, as human intervention\nis still required in current networks [1]. Recently, several\nconcepts or architectures have been proposed for network\nautomation and optimization, such as Self-Organizing Net-\nwork Management (SON), Intent-Based Network Management\n(IBN), and Autonomic Network Management (ANM) [2].\nMore recently, Zero-Touch Networks (ZTNs) have emerged\nas a transformative approach and a next-generation system\nfor automating network operations to minimize manual net-\nwork tasks, including network management, configuration, and\noptimization [3]. ZTNs aim to enhance network efficiency,\nreliability, and security by leveraging Artificial Intelligence\nLi Yang is with the Faculty of Business and Information Technol-\nogy, Ontario Tech University, Oshawa, ON L1G 0C5, Canada (e-mail:\nli.yang@ontariotechu.ca)\nMirna El Rajab and Abdallah Shami are with the Department of Electrical\nand Computer Engineering, Western University, London, ON N6A 3K7,\nCanada (e-mails: melrajab@uwo.ca, abdallah.shami@uwo.ca)\nSami Muhaidat is with the Department of Electrical Engineering and\nComputer Science, Khalifa University, Abu Dhabi 127788, UAE (e-mail:\nsami.muhaidat@ku.ac.ae)\n(AI) and Machine Learning (ML) techniques, as well as the\nfifth generation of network (5G) technologies, like Software-\nDefined Networking (SDN) and Network Functions Virtual-\nization (NFV). While ZTNs are centered on the automation of\nnetwork operations, Zero-touch Network and Service Manage-\nment (ZSM) proposed by the European Telecommunications\nStandards Institute (ETSI) [4] complements ZTNs by targeting\nthe automation of network service lifecycle management. The\nnetworking and communication scientific community antici-\npates that AI/ML techniques will play a vital role in fully\nautomating the management and orchestration of the sixth\ngeneration of networks (6G) [1].\nAI/ML techniques have become key enablers for ZTNs and\n5G/6G networks, realizing the rapid and accurate processing\nof massive volumes of network data to achieve network\nautomation and optimization [5]. The implementation of ML\napproaches in network and service management leads to\nsignificant improvements in service efficiency, performance,\nand time management [6]. AI/ML methods can provide or\nenhance a wide range of network services and management\nfunctionalities, such as network behavior analysis, anomaly\ndetection, traffic classification and forecasting, mobility pre-\ndiction, and resource allocation.\nIn spite of the widespread adoption of AI/ML techniques in\ncurrent networks, developing and deploying ML algorithms\nstill requires extensive domain expertise and human effort\n[7]. Additionally, traditional AI/ML models still have many\nlimitations, such as human errors and insufficient adaptability.\nTo address the limitations of traditional AI/ML models and\nrealize ZTNs, Automated Machine Learning (AutoML) tech-\nniques emerge as potential solutions for network automation\nby enabling automated network data analytics for data-driven\nnetwork services [8]. AutoML enables the automation of var-\nious data analytics and ML procedures, including automated\ndata pre-processing, automated feature engineering, automated\nmodel selection, Hyper-Parameter Optimization (HPO), and\nautomated model updating [9]. Automated data preprocess-\ning and feature engineering aim to enhance network data\nquality for improved analytics results, whereas automated\nmodel selection and HPO produce optimized ML models with\noptimal performance. Moreover, real-world networks typically\noperate in ever-changing environments, leading to model drift\nor data distribution changes. Automated model updating is a\npotential solution to address model drift issues and maintain\nthe performance of AI/ML models [10]. Overall, AutoML\ntechniques are effective solutions for realizing ZTNs.\nCritical self-management functionalities in ZTNs include\narXiv:2502.21286v1  [cs.CR]  28 Feb 2025\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n2\nTABLE I\nSUMMARY OF EXISTING SURVEYS ON ZTN SECURITY\nPaper\nZTN Security\nVulnerabilities\nAML Attacks\nfor ZTNs\nAI/ML\nEnablers for\nZTNs\nAutomated\nEnabler for ZTNs\n(e.g., AutoML)\nML Case\nStudy\nAutoML\nCase Study\nAML\nDefense\nCase Study\nOpen-\nAccess\nCode\nCoronado et al. [2]\n!\n!\n!\nLiyanage et al. [6]\n!\n!\n!\nBenza¨ıd et al. [11]\n!\n!\n!\n!\nGallego-Madrid et\nal. [14]\n!\n!\nAshraf et al. [15]\n!\n!\n!\nBenza¨ıd et al. [16]\n!\n!\n!\n!\nOur Survey\n!\n!\n!\n!\n!\n!\n!\n!\nself-configuration, self-healing, self-optimization, and self-\nprotection [11]. Among these functionalities, both self-healing\nand self-protection are particularly relevant to network secu-\nrity concerns, making network security mechanisms critical\ncomponents in ZTNs. ZTN security exemplifies the concept\nthat security should be an integral component of the network\ninfrastructure [12]. By incorporating security measures into\nthe network itself, management and maintenance are signifi-\ncantly simplified, since no additional hardware or software is\nrequired.\nCybersecurity issues are primary challenges in ZTNs. Sim-\nilar to general networks, ZTNs face various cyber threats,\nsuch as Denial of Service (DoS), eavesdropping, Man-in-\nthe-Middle (MITM), web attacks, and malware. Apart from\ngeneral network security threats, the adoption of ZTN tech-\nnologies in Beyond 5G (B5G)/6G networks is envisioned to\nintroduce additional security challenges and increased attack\nsurfaces. These include threats to open Application Program-\nming Interface (API), intent, closed-loop network automation,\nprogrammable network technology, and AI/ML models [6]\n[11]. Among these security threats, AI/ML-based attacks, or\nAdversarial Machine Learning (AML) attacks, are expected\nto introduce critical challenges in ZTNs/6G networks. AML\nattacks exploit the vulnerabilities of AI/ML models, causing\nthem to make incorrect predictions/decisions and leading to\nsecurity breaches and unauthorized access to sensitive data\n[13]. Therefore, it is crucial to develop effective security\nmechanisms that can detect, mitigate, and prevent security\nthreats in ZTNs [12].\nAlthough there is an increasing popularity in ZTN research,\na comprehensive survey on ZTN security and cybersecurity\nautomation has yet to be conducted. This survey paper aims\nto bridge the gap between ZTN & network automation tech-\nniques and cybersecurity applications by reviewing existing\nchallenges and potential solutions between these two domains.\nTo the best of our knowledge, this survey constitutes the first\ncomprehensive examination of ZTN security and automated\ncybersecurity methods, specifically through the exploration of\nAutoML techniques.\nSpecifically, this survey paper provides the following con-\ntributions:\ni) A comprehensive review of potential security issues and\ncyber-attacks in ZTNs, covering general cybersecurity\nchallenges, security threats targeting ZTN services, and\nAI/ML model security issues.\nii) A detailed discussion of potential solutions to ZTN\nsecurity issues, including general cybersecurity automa-\ntion procedures, AutoML techniques, and AML defense\nmechanisms.\niii) Two case studies that apply AutoML techniques to prac-\ntical ZTN security tasks and conduct the cyber-defense\nexercise against AML attacks; the implementation code\nis publicly available on GitHub1.\niv) A discussion of the open challenges and research direc-\ntions of ZTN security methods.\nThe contributions of existing survey papers on ZTN security\nare summarized in Table I. Table I illustrates that all existing\nsurvey papers, including references [2], [6], [11], [14], [15],\nand [16], introduce ZTN security vulnerabilities and highlight\nAI/ML models as pivotal enablers for ZTNs. Among these,\nAML attacks are identified as critical threats to ZTN security\nin most surveys, with the exception of the paper [14], which\ndoes not address AML attacks. Additionally, the paper [11]\nstands out as the sole survey providing an AML defense case\nstudy, and [16] is distinguished as the only work presenting a\nML case study. Therefore, the primary unique contributions of\nthis survey are contributions iii) and iv), where our paper is the\nfirst to comprehensively discuss automated security solutions\nfor ZTNs, selecting AutoML as a potential solution, and\nproviding comprehensive case studies on ML-based security,\nAutoML-based ZTN security, and AML attack and defense\nscenarios. Furthermore, this survey sets a precedent as the\nfirst ZTN security survey to share open-access code for the\ndiscussed case studies, making significant contributions to the\nfield.\nTherefore, this paper focuses on exploring AutoML tech-\nniques for automating cybersecurity mechanisms for ZTNs\nwith network automation requirements and then discussing\nthe safeguarding of AI/ML and AutoML models in ZTNs\nagainst AML attacks. Based on the two main focuses of\nthis paper—automated cybersecurity solution development via\nAutoML techniques, and AML attack & defense mecha-\nnisms—the structure of the remainder of this manuscript is\noutlined in Fig. 1 and as follows. Section II provides an\noverview of the ZTN security architecture and its require-\nments. Section III details the vulnerabilities and cyber-attacks\ntargeting ZTNs. Section IV, the first focus of this paper, intro-\nduces AI/ML & AutoML techniques and discusses automating\n1Code for this paper is available at: https://github.com/Western-OC2-Lab/\nAutoML-and-Adversarial-Attack-Defense-for-Zero-Touch-Network-Security\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n3\nFig. 1. Overview of the survey’s organization.\ngeneral security mechanisms with AutoML. Section V, repre-\nsenting the second focus, delves into the most critical security\nthreats to ZTNs and AI/ML models—AML attacks—and ex-\nplores potential solutions. Section VI features two case studies:\nthe autonomous ZTN security framework utilizing AutoML\ntechniques and the AML cyber-defense exercise. Section VII\ndiscusses the challenges and future directions within the ZTN\nsecurity and AutoML research domains. Finally, Section VIII\nconcludes the paper.\nII. ZTN SECURITY OVERVIEW\nA. ZTN Introduction\nZTNs have emerged as a solution for fully automating net-\nwork operations, providing self-configuration, self-monitoring,\nself-healing, and self-optimization [14]. The adoption of ZTNs\nis driven by the demand for virtualized network functionality\nand the preference for software-based solutions over hardware-\nbased alternatives [6]. They are designed to operate, conduct\ncommercial activities, self-sustain, recover from disruptions,\nand perform other network-related tasks without human inter-\nvention. The deployability of ZTNs is facilitated by various\nAPIs and End-to-End (E2E) programmability [6]. ZTNs are\nanticipated to play a crucial role in future networks, enabling\nthe efficient distribution of network resources to meet the\nspecific requirements of various industries and individuals [2].\nThe automation capabilities provided by ZTNs are essential for\nanalyzing and responding to customer-specific needs, ensuring\nthe desired Quality of Experience (QoE). In addition, ZTNs\ncan evolve and upgrade through E2E automation, incorporat-\ning and updating new features that are required for practical\napplications.\nZTNs require a high level of automation, highlighting the\nimportance of AI/ML techniques in transforming network\nmanagement to achieve self-adaptation and self-reaction in\nhighly dynamic environments. To facilitate network automa-\ntion, Release 17 of the 3rd Generation Partnership Project\n(3GPP) includes a NFV/SDN approach via the Network Data\nAnalytics Function (NWDAF) [15]. This integration allows\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n4\nfor the collection and analysis of network data in real-time,\nenabling predictive maintenance, enhanced security, and opti-\nmized network performance.\nB. ZTN Security Architecture\nIn the field of network and service management, cyberse-\ncurity automation can be achieved through the use of AI/ML\ntechniques in combination with closed-loop operations [12].\nEach management domain within the ZTN architecture con-\ntains a data analytics service that enables closed-loop network\nsecurity and optimization operations. An example of a fully\nautomated network security framework for ZTN architectures\nis proposed in the “Intelligent security and pervasive trust for\n5G and beyond” (INSPIRE-5Gplus) project, funded by the\nEuropean Union (EU) [17].\nClosed-loop operations are essential enablers of network\nautomation within the ZTN architecture. Figure 2 illustrates\na domain-level security closed-loop ZSM framework, which\nconsists of five stages: observation, orientation, decision, ac-\ntion, and learning [12] [17]. In the first stage, data is observed\nand collected from various data sources by the security agents.\nThe collected data is then oriented and analyzed by the security\nanalytics service to comprehend its meaning and significance,\nwhich may involve data filtering and correlation analysis.\nIn the subsequent stage, the decision engine uses AI/ML\nalgorithms to recognize data patterns and make informed\ndecisions or predictions for network optimization. Upon iden-\ntifying the root causes and providing recommendations, the\nsecurity orchestrator implements environment- and situation-\nappropriate actions or plans. Lastly, the system learns from the\ndetected patterns and consequences to improve future reactions\nor decisions.\nE2E management domains are responsible for managing\nservice and resource requests across different domains [12].\nA management domain refers to a collection of services\nand resources organized based on various constraints, such\nas operations, functions, and deployment. Communication\nwithin domains is facilitated by the domain integration fabric,\nwhile communication between domains and E2E management\ndomains is enabled by the inter-domain integration fabric.\nWithin the ZTN architecture illustrated in Fig. 2, a manage-\nment domain consists of a security data collector for data\nobservation and collection, a security analytics engine for\norientation, a decision engine enabled by AI/ML methods for\ndecision-making, a policy and Security Service Level Agree-\nment (SSLA) manager for governance, a security orchestrator\nand trust manager for action-taking, and data services for\nknowledge learning.\nC. ZTN Security Requirements\nThe ZTN framework reference architecture plays an impor-\ntant role in the networking and cybersecurity landscape. As\nthe world transitions toward more automated and intelligent\nnetworks, the need for stringent security measures becomes\nincreasingly crucial. The ZTN security frameworks aim to\nreduce the risk of human error and create an efficient and\nsecure network environment. For ZTN security frameworks,\nFig. 2. The ZTN security framework [12].\nseveral essential security requirements must be met [6] [11]\n[16]:\ni) Data Protection and Integrity: The ZTN framework must\nsupport data protection mechanisms that cover data in\nuse, in transit, and at rest. Ensuring the security of\ndata management and data integrity is essential for ZTN\nframeworks. In addition, the ZTN framework must ensure\nthe availability of network data, infrastructure resources,\nand management functions.\nii) Privacy Preservation: The ZTN framework should in-\ncorporate personal data privacy features that conform to\nthe privacy-by-design and privacy-by-default principles.\nThese principles ensure that privacy is built into the\narchitecture from the outset, with the highest level of\nprivacy settings applied by default and without user\nintervention.\niii) Access Control and Security Policy Enforcement: The\nZTN framework should enable authenticated service users\nto authorize service access and follow security policies.\nThis capability allows for the automated implementation\nof appropriate security policies based on the status of\nindividual management services in relation to security re-\nquirements. Streamlining access control enhances overall\nsecurity posture.\niv) Intrusion Detection, Prevention, and Mitigation: The\nZTN framework must support automated detection,\nrecognition, prevention, and mitigation of attacks and\nincidents. These capabilities are crucial for swiftly iden-\ntifying and mitigating potential security threats, thereby\nminimizing the impact of cyber-attacks on the network\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n5\nTABLE II\nANALYSIS OF SECURITY THREATS IN ZTNS AND THEIR COUNTERMEASURES.\nThreat Category\nThreat Examples\nDescription\nDetailed Countermeasures\nGeneral Network\nThreats\nDoS/DDoS,\nEavesdropping, MITM,\nSQL Injection, XSS\nThese foundational threats compromise network\nintegrity, confidentiality, and availability,\nexploiting vulnerabilities across network layers\nand applications.\nDeploy advanced IDS with AI/ML-driven\nanomaly detection, enforce end-to-end\nencryption, and utilize behavior analysis for\nearly threat identification.\nOpen API-based\nThreats\nParameter Attacks,\nIdentity Attacks, MITM\nAttacks, DDoS Attacks\nAs ZTNs increasingly rely on APIs for network\noperations, these threats exploit API\nvulnerabilities to manipulate or disrupt services.\nImplement API security gateways, enforce strict\nauthentication and authorization controls with\nOAuth2 and JWT, and use rate limiting along\nwith comprehensive input validation to mitigate\ninjection and overload attacks.\nIntent-based Threats\nInformation Exposure,\nUndesirable\nConfiguration,\nAbnormal Behavior\nIBN vulnerabilities can lead to unauthorized\naccess and system misconfigurations,\nundermining network security and efficiency.\nApply rigorous authentication mechanisms,\nsecure intent transmission with TLS, and\nemploy intent schema validation to prevent\nmisconfigurations and ensure policy integrity.\nClosed-Loop\nNetwork\nAutomation-based\nThreats\nMITM Attacks,\nDeception Attacks\nAutomated network configurations using\nAI/ML are susceptible to interception and\nmanipulation, threatening the reliability of\nself-optimizing actions.\nSecure AI/ML data pipelines, encrypt\ncommunication channels, and integrate\nreal-time monitoring and anomaly detection to\nidentify and mitigate threats swiftly.\nProgrammable\nNetwork\nTechnology-based\nThreats\nSDN Threats, NFV\nThreats\nSDN and NFV introduce new security\nchallenges, including unauthorized access and\nmanipulation of network functions.\nStrengthen SDN controllers and NFV\norchestrators with multi-layered security\nmeasures, including role-based access control,\ntraffic encryption, and the implementation of\nsecure boot processes for virtual functions.\nand its services.\nv) AI/ML Decision Supervision and Auditing: To safeguard\nagainst vulnerabilities and cyber-attacks, the ZTN plat-\nform reference architecture should enable the supervision\nand auditing of privacy and security decisions made\nby AI/ML techniques. This mechanism guarantees that\nAI/ML-driven decisions establish security requirements,\nmitigate potential risks, and recover compromised devices\nwhile remaining resilient to AI/ML-based attacks.\nIII. ZTN SECURITY THREATS AND POTENTIAL\nSOLUTIONS\nTo ensure the cybersecurity of ZTNs, it is essential to\nimplement effective E2E security management. The increasing\nnumber of connected devices and the expanding networks\nhave resulted in a rapid increase in threats targeting 5G/6G\nnetworks. The threat surfaces of the ZTN architecture in-\nclude open API threats, intent-based threats, closed-loop net-\nwork automation-based threats, and programmable network\ntechnology-based attacks [6] [15]. This section discusses\nthe primary security challenges specific to ZTNs, the ZTN\narchitecture, and the corresponding potential counterattack\nmechanisms. The challenges presented in this section are sum-\nmarized in Table II. Moreover, the popularity of AI/ML models\nin enabling autonomous services in ZTNs has introduced a\nspecial challenge in the form of adversarial attacks, which\naim to compromise AI/ML models and disrupt ZTN services\n[2] [15]. This challenge will be further addressed in Section\nV.\nA. General Cybersecurity Threats and Countermeasures\nIn the progression from 1G to 5G networks, technological\nadvancements have enhanced communication capabilities but\nalso escalated the spectrum of cybersecurity threats. Many\nexisting vulnerabilities and cyber threats are still active in\nmodern networks and can breach ZTNs. Key threats that\nhave been identified across these network generations include\nDenial of Service (DoS) and Distributed DoS (DDoS) at-\ntacks [18], eavesdropping (e.g., port scan) [19], Man-in-the-\nMiddle (MITM) attacks [20], web application attacks (notably\nStructured Query Language (SQL) injection and Cross-Site\nScripting (XSS)) [19], and malware (e.g., viruses, worms,\nransomware, spyware) [20].\nThe persistence of these cyber threats necessitates the de-\nvelopment of robust and advanced cybersecurity mechanisms\nto protect general networks and ZTNs. To defend against\nthese cyber threats, comprehensive cybersecurity solutions\nincorporate four essential steps: anomaly detection, root cause\nanalysis, system remediation, and intrusion prevention. The\nautomation of these cybersecurity solutions is required in\nZTNs, which are discussed in detail in Section IV-D.\nB. Open API-based Security Threats and Potential Solutions\nOpen APIs are crucial components for integrating web-\nbased applications and facilitating communication within the\nZTN framework, which plays an integral role in service\nprovisioning, management, orchestration, and monitoring [21]\n[22]. As 6G networks follow the trend set by 5G networks,\nopen APIs are expected to play an even more significant role\nand be further enhanced [23]. However, as APIs are vital\nto network operations, they also become attractive to cyber-\nattackers, posing various security threats to ZTNs [6].\nCyber attacks exploiting open APIs can be divided into four\nmajor categories: parameter attacks, identity attacks, MITM\nattacks, and DDoS attacks [6] [11] [23]:\ni) Parameter Attacks: Parameter attacks take advantage of\ndata transmitted through the API, such as query pa-\nrameters, Hypertext Transfer Protocol (HTTP) headers,\nUniform Resource Locators (URLs), and post content.\nExamples of parameter attacks include script insertions,\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n6\nSQL injections, buffer overflow attacks, and injection\nattacks targeting the common data services component\nof the ZTN framework [6]. These attacks can cause\nunauthorized data access, data manipulation, and DoS\nattacks.\nii) Identity Attacks: Identity attacks exploit vulnerabilities\nin the authentication and authorization procedures of\nZTNs, such as using extracted API keys as creden-\ntials or delivering large quantities of random data to\nidentify vulnerabilities [23]. Insecure APIs can result in\nunauthorized access to domain orchestration services or\nE2E service orchestration services, allowing attackers to\nmodify configurations or create E2E service instances to\nexhaust network resources [6].\niii) MITM Attacks: MITM attacks occur when cyber-attackers\nintercept unencrypted API messages, capture confidential\ninformation, or manipulate messages [11]. These attacks\ncan compromise the confidentiality and security of trans-\nmitted data.\niv) DDoS Attacks: APIs are also vulnerable to DoS and\nDDoS attacks, where attackers can overwhelm an API\nwith a large number of requests, resulting in service\nunavailability [6].\nAPI security is essential to restrict interactions with ZTN\nAPIs to authorized users [24]. Several security measures,\nsuch as authorization, communication encryption, and input\nvalidation, can be implemented in ZTNs to mitigate API\nsecurity threats [11]. The enhancement of authorization can\nbe achieved through the application of Access Control Lists\n(ACLs), Role-Based Access Control (RBAC), and Attribute-\nBased Access Control (ABAC). These mechanisms are de-\nsigned to aid in fulfilling the imperative of the least priv-\nilege [11]. To prevent MITM attacks, API messages must\nbe encrypted, and Transport Layer Security (TLS) can be\nused to improve API message integrity and confidentiality.\nValidation of all input data in the URL, query parameters,\nHTTP headers, JavaScript Object Notation (JSON) schema,\nand content of ZTN APIs is essential for preventing injection\nattacks. Moreover, AI/ML techniques can be used to improve\nexisting security mechanisms in ZTNs, such as automated API\nthreat detection and mitigation.\nC. Intent-based Network Management (IBN) Threats and Po-\ntential Solutions\nIntent-Based Network Management (IBN) offers a promis-\ning approach to integrate AI/ML techniques into 6G networks,\naddressing the limitations of traditional networks in terms\nof efficiency, flexibility, and security [23]. By leveraging\nAI/ML techniques, IBN directly translates users’ business\nintentions into network configurations, operations, and mainte-\nnance strategies. As part of the ZTN architecture, intent-based\ninterfaces provide high-level abstractions and specify policies,\nenabling the decoupling of technology- and vendor-specific\ndetails [22]. However, IBN introduces security vulnerabilities,\nincluding information exposure, undesirable configuration, and\nabnormal behaviors [23]:\ni) Information Exposure: Intent-based interfaces may ex-\npose confidential information about the application’s de-\nsires, such as connecting with peers, advertising services\nor content, and controlling network traffic [11]. This\ninformation can be intercepted without authorization,\ncompromising system security objectives (e.g., privacy\nand confidentiality) and opening the backdoor to addi-\ntional attacks [6].\nii) Undesirable Configuration: Malicious modification of an\nintent-based service model may result in a configuration\nof an undesirable security level, leaving the network\nsegment vulnerable to security threats [11]. For instance,\na cyber-attacker could alter a high-security-level intent to\na low-security-level intent, resulting in deficient security\nmeasures in a network segment.\niii) Abnormal Behavior: A malformed intent sent to a domain\norchestration service may trigger abnormal behavior, in-\ncluding service disruption or a DoS attack [11]. Alter-\nnatively, the service may transfer the intent to a policy,\nresulting in a misconfigured network that causes network\noutages or security breaches.\nTo mitigate these intent-based threats, several potential\nmechanisms can be employed [11]:\ni) Authentication\nand\nAuthorization:\nOpenID\nConnect,\nsigned JSON Web Tokens (JWT), and OAuth2.0 can en-\nsure mutual authentication between intent producers and\nconsumers and restricted access to intent-based interfaces.\nii) Secure Transport Protocol: The exchange of intents\nshould occur over a secure transport protocol, such as\nTLS 1.2, to maintain intent integrity and confidentiality,\nthereby preventing intent tampering and eavesdropping.\niii) Intent Validation and Conflict Resolution: The intent\nengine should be able to validate the intent format and\ndetect/resolve potential conflict situations, mitigating the\nimpact of malformed or conflicting intents on insufficient\nnetwork operation and security.\nIntent-based interfaces can contribute to the overall security\nand efficacy of ZTNs within the context of 6G mobile net-\nworks by addressing these security concerns and implementing\nthe suggested mitigation mechanisms.\nD. Closed-Loop Network Automation-based Threats and Po-\ntential Solutions\n6G networks will leverage closed-loop network automation\nfor zero-touch management capabilities [25]. Closed-Loop Au-\ntomation (CLA) continuously monitors, identifies, adjusts, and\noptimizes network performance to enable self-optimization\n[23] [6]. As 6G technologies evolve, external CLA capabilities\nbecome necessary for addressing the expanding threat land-\nscape. Using AI/ML techniques, CLA security mechanisms\nhave the potential to autonomously detect and rapidly mitigate\nthreats.\nHowever, several security threats are driven by closed-\nloop networked automation, such as MITM, deception attacks,\nand other unknown threats [6] [11]. In MITM attacks, an\nintruder intercepts communications between two parties to\neavesdrop or manipulate traffic remotely. The cyber-attacker\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n7\nobtains user credentials and confidential information, and they\nmay interrupt message exchanges, resulting in data corruption.\nDeception attacks rely on manipulating the target to act based\non false information, convincing them to perceive an incorrect\nversion of the truth as fact, and causing the propagation of\nfalse feedback information.\nTo address security threats in CLA, implementing secure\ncommunication channels between network components can\nreduce the risk of MITM and deception attacks and prevent\nunauthorized access [11]. Additionally, AI/ML techniques can\nbe used to monitor network traffic and detect anomalies for\nreal-time CLA attack detection. Validation and integrity tests\nat various phases of a closed-loop process can also assist in\npreventing the injection and manipulation of malicious data.\nE. Programmable Network Technology-based Threats and Po-\ntential Solutions\nZTNs leverage SDN and NFV technologies to construct\nprogrammable networking solutions [6] [15]. Despite their\npotential benefits, these technologies introduce new attack\nvectors and security risks that compromise the application,\ndata, and control layers of SDN [26].\n1) SDN Threats and Potential Solutions: The Open Net-\nwork Foundation (ONF) has identified various threats to\nSDN, including spoofing, tampering, repudiation, information\nleakage, DoS, and privilege escalation [11]. These threats can\ntarget various SDN layers (i.e., the control, data, and applica-\ntion layers) and often result from design and implementation\nlimitations in existing SDN controller platforms [27]. The risks\nare increased when users have programmatic access to SDN,\nparticularly when they use third-party applications or standard-\nbased solutions to access the network. Insufficient isolation\ncan lead to cyber-attacks such as network manipulation and\nAddress Resolution Protocol (ARP) spoofing [6] [15]. Several\nsolutions have been suggested for mitigating SDN-related\nattacks [11] [27]. These include:\ni) Implementing authorization mechanisms such as Role-\nBased Access Control (RBAC) to regulate access levels\nand prevent privilege abuse.\nii) Ensuring the confidentiality and integrity of messages ex-\nchanged on communication channels, both in the North-\nSouth and East-West interfaces, through the use of en-\ncryption, digital signature, and Message Authentication\nCode (MAC) algorithms.\niii) Utilizing tamper-proof devices like Trusted Platform\nModules (TPM) to secure sensitive data, encryption keys,\npasswords, and certificates.\niv) Implementing a distributed SDN controller architecture\nto defend against DoS/DDoS attacks and ensure system\navailability.\n2) NFV Threats and Potential Solutions: NFV is suscep-\ntible to various virtualization and networking threats, as well\nas threats resulting from their combination [28]. These threats\ninclude generic virtualization threats like memory leakage and\ninterrupt isolation, generic networking threats like flooding\nattacks and routing security issues, and threats that arise\nfrom the integration of virtualization and networking, such\nas introspection attacks. Virtual Network Functions (VNFs)\nare vulnerable to design, implementation, and configuration\nweaknesses resulting in inappropriate monitoring of data that\nmisleads ZTN service intelligence and E2E analytics [6]. The\nNFV Security Group of ETSI has proposed various potential\nsecurity mechanisms to counter NFV-related threats [11] [28]:\ni) Trusted Platform Module (TPM) and Virtual TPM: These\nare recommended for ensuring the integrity of VNF\nbooting and enabling remote verification.\nii) Trusted Execution Environments: Examples include In-\ntel’s Software Guard Extensions (SGX) and Secure En-\ncrypted Virtualization (SEV) for Advanced Micro De-\nvices (AMD). These are utilized for establishing the\nconfidentiality and integrity of VNFs at runtime.\niii) Traffic Monitoring and Filtering: Tools such as Intrusion\nDetection Systems (IDSs) and firewalls are used for\ndetecting DoS/DDoS attacks.\niv) Robust Authentication and Authorization Mechanisms:\nThese are crucial for preventing NFV Management and\nOrchestration (MANO) hijacking.\nv) Secure Communication Channels: These are recom-\nmended for preventing MITM attacks.\nvi) System Hardening Techniques: These can help prevent\nthe exploitation of software vulnerabilities in NFV com-\nponents.\nIn essence, the security framework of ZTNs faces a broad\nspectrum of threats targeting open APIs, intent-based net-\nworking configurations, automated network operations, and\nadvanced network programming functionalities. These threats\ncompromise not just the core principles of security—integrity,\nconfidentiality, and availability—but also put to test the flexi-\nbility and operational efficacy of ZTN architectures. Address-\ning these challenges requires the deployment of a multifaceted\nsecurity strategy. This includes enforcing stringent access con-\ntrol, implementing encryption standards, employing advanced\ndetection systems, and ensuring rigorous validation of inputs.\nThrough such proactive measures, ZTNs can bolster their\ndefenses against complex cyber threats, thereby maintaining\na secure and dependable operational environment amidst the\ndynamic landscape of cybersecurity threats. This approach\nhighlights the critical need for ongoing evaluation and en-\nhancement of security protocols to safeguard ZTNs against\nexisting and emerging cyber risks. Moreover, given the high-\nlevel network automation requirements of ZTNs, autonomous\ncybersecurity methods are expected to enhance ZTN security.\nAutoML techniques emerge as potential solutions for au-\ntonomous cybersecurity solutions in ZTNs, and are introduced\nin the next section.\nIV. POTENTIAL NETWORK AUTOMATION SOLUTIONS:\nAUTOMATED MACHINE LEARNING (AUTOML)\nA. AI/ML Techniques\nAI/ML techniques are becoming increasingly vital for\nstrengthening 6G security. Traditional security solutions, such\nas firewalls and conventional IDSs, struggle against complex\ncyber-attacks within a 6G environment. AI/ML techniques\nhave proven to be a viable solution for enhancing security\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n8\nacross multiple network layers, spanning from the physical\nlayer to the network layer [29].\nAt the physical layer, AI/ML significantly bolsters security\ndefenses by improving the performance of detection engines,\nmitigating vulnerabilities such as eavesdropping and jamming\nattacks. On the network layer, AI/ML has been leveraged\nin several key technologies to enhance system performance\nwithin the 6G landscape. These include verifying node be-\nhavior to detect insider attacks, predicting potential network\nattacks, optimizing radio and computing control policies,\ndetermining the prioritization of equipment recovery, and\nscrutinizing traffic or network access behavior [29].\nAI/ML plays a crucial role in enabling efficient ZTN man-\nagement within 6G networks by automating and optimizing\na wide range of network operations. ML techniques can be\ncategorized into four groups, each with its specific applications\nin the context of ZTN services functionalities [14] [30].\n1) Supervised Learning: Supervised Learning (SL) tech-\nniques are frequently utilized in ZTNs for the automation of\nnetwork and service management [31]. These techniques are\nespecially effective in classification and regression problems,\nwhere the objective is to predict a specific class or output value\ngiven a particular input. SL is employed for tasks such as\ntraffic classification, service requirement prediction, and user\nbehavior forecasting. There are many different types of SL\nalgorithms, such as linear regression, logistic regression, deci-\nsion trees, random forests, XGBoost, LightGBM, K-Nearest\nNeighbors (KNN), Support Vector Machines (SVM), naive\nBayes, and Neural Networks (NN) [8] [32] [33].\n2) Unsupervised Learning: Unsupervised Learning (UL)\ntechniques are essential in understanding and organizing data\nwithout the need for labeled examples. They are used in\nZTNs to group network traffic flows with similar character-\nistics and assign them to appropriate network slices. This\nensures optimal utilization of network resources and allows for\ndifferentiated levels of Quality of Service (QoS). Commonly\nemployed UL algorithms for these purposes encompass K-\nmeans, Gaussian mixture models, and Principal Component\nAnalysis (PCA) [34] [35].\n3) Deep Learning: Deep Learning (DL) techniques, based\non artificial neural networks, address the challenges posed by\nthe necessity for manual feature extraction in traditional ML\napproaches [36] [37]. They enable the efficient processing of\nraw natural data and facilitate the implementation of large\nstate-action spaces. Common DL algorithms include Multi-\nLayer Perceptron (MLP), Convolutional Neural Networks\n(CNNs), and Recurrent Neural Networks (RNNs) [34]. In the\ncontext of ZTNs, DL algorithms such as deep Q-learning\nand Soft Actor-Critic (SAC) are used for tasks like traffic\nclassification and prediction, Radio Access Network (RAN)\nmanagement, multi-domain orchestration, and resource man-\nagement.\n4) Reinforcement Learning: RL techniques enable adaptive\ndecision-making in network management by modeling prob-\nlems as driven by rewards and punishments [38]. They find use\nin various applications, including slice admission strategies,\ndata migration in Multi-access Edge Computing (MEC), Radio\nAccess Technology (RAT) selection, and resource allocation\nfor network services [14]. RL algorithms are categorized as\nmodel-based and model-free models, and examples such as\nproactive resource allocation and Q-learning are utilized in\nZTN scenarios.\nB. Challenges of Traditional AI/ML Techniques\nDespite the widespread use of traditional ML algorithms\nfor various cybersecurity tasks, they encounter several critical\nchallenges [7] [8] [9]:\ni) Time-Consuming Model Development: Traditional ML\nand data analytics techniques involve a sequence of\nmanual tasks, such as data pre-processing, feature en-\ngineering, model selection, and hyperparameter tuning.\nThese tasks require significant time and effort from\nML developers and researchers, which can impede the\nrapid development and deployment of ML models within\n5G/6G networks.\nii) Human Bias and Errors: The inherent subjectivity in-\nvolved in manual ML model development can introduce\npotential biases and errors. These can result in issues such\nas overfitting and underfitting, leading to a degradation in\nmodel performance.\niii) Expertise Requirements: The deployment of traditional\nML algorithms in practical applications often necessitates\na high level of expertise. This can create a barrier to\ninterdisciplinary collaboration by increasing the demand\nfor skilled ML professionals.\niv) Insufficient Adaptability: Traditional ML models are often\nslow or unable to react to ever-changing data streams in\ndynamic networking environments. They generally lack\nthe ability to detect unknown or zero-day attacks.\nWhile traditional ML algorithms present certain challenges,\nthey remain vital techniques in the pursuit of enhanced security\nwithin the landscape of 6G networks. By addressing these\nchallenges, we can further leverage the potential of AI/ML\nin cybersecurity, improving the resilience and adaptability of\nour future networks.\nC. Automated Machine Learning (AutoML)\nAlthough ML algorithms can learn data without human\nintervention, there are certain procedures in ML pipelines that\nrequire experienced data scientists. These procedures include\npreparing appropriate and sanitized data, selecting the most\nsuitable ML algorithm, tuning hyperparameters, and determin-\ning whether the ML model needs updating. Data scientists usu-\nally experiment with various pre-processing methods and ML\nalgorithms to find the optimal combination. These procedures\nare human-centric, time-consuming, and require specialized\nexpertise in ML and data analysis [8]. Automating the process\nof designing and tuning ML methods is known as AutoML.\nAutoML refers to the fully automated process of applying\nML algorithms to real-world practical applications, allowing\nboth beginners and experts to develop ML models in a more\nefficient way.\nAutoML aims to enable domain specialists to create ML\nmodels automatically without requiring extensive knowledge\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n9\nFig. 3. An overview of the general AutoML framework.\nand experience in ML. AutoML addresses the shortage of data\nscientists and has the potential to significantly improve the\nperformance and effectiveness of ML models by shortening\nwork cycles and increasing model performance [8]. Given\nthe high demand for network automation, AutoML techniques\nhave become necessary components in ZTNs networks.\nZTNs represent autonomous networks with self-healing\nand self-protection capabilities based on the data collected\nand analyzed across all network activities [11]. Robust and\npowerful AI/ML models are key components in safeguarding\nZTNs by transforming conventional network security opera-\ntions into automated and intelligent operations. In the general\ncybersecurity process discussed in Section IV-D, supervised\nand unsupervised AutoML techniques can be developed to\nautomatically detect malicious cyber-attacks. RL-based Au-\ntoML techniques can then be used to automatically determine\nappropriate actions to defend against cyber-attacks and aid in\nsystem remediation.\nFigure 3 illustrates the general AutoML framework, which\nconsists of four stages: automated data pre-processing, au-\ntomated feature engineering, automated model learning, and\nautomated model updating. The specifics of each stage of the\nAutoML pipeline are described below.\n1) Automated Data Pre-Processing: Data pre-processing\nenhances data quality for ML model development by address-\ning common issues such as outliers, missing values, and class\nimbalance [39]. Automated Data Pre-processing (AutoDP) is\nan essential component of AutoML, streamlining the time-\nconsuming and tedious pre-processing stage. AutoDP tasks\ncan be categorized into four main areas: data transformation,\nimputation, balancing, and normalization [8] [40].\ni) Data Transformation: It involves encoding techniques for\nconverting categorical features into continuous ones (e.g.,\nlabel encoding and one-hot encoding) and discretization\ntechniques for transforming continuous features into cat-\negorical ones [8].\nii) Data Imputation: It addresses the issue of missing values\ncommonly found in real-world network datasets, which\ncan significantly impact the learning performance of ML\nmodels. Common data imputation approaches include\nzero/mean/median imputation for numerical and categori-\ncal features, forward/backward filling for time-series data,\nand ML model-based imputation techniques for advanced\nimputation [41].\niii) Data Balancing: It addresses class imbalance issues in\nnetwork datasets for classification problems, which can\nresult in ML model performance degradation. Resampling\ntechniques, such as over-sampling techniques (e.g., the\nSynthetic Minority Over-sampling TEchnique (SMOTE)\n[19] and ADAptive SYNthetic (ADASYN) [42]) and\nunder-sampling methods (e.g., random under-sampling),\ncan be employed to mitigate class imbalance by gener-\nating minority class samples or removing majority class\nsamples [43].\niv) Data Normalization: It ensures fair treatment of features\nwith different scales by ML models, especially when\ndistance calculations are involved in model learning.\nZ-score and min-max normalization are two common\napproaches used in ML model learning, with the choice\ndepending on the occurrence and percentage of outliers\n[44].\n2) Automated Feature Engineering: Feature Engineering\n(FE) is a critical aspect of the ML pipeline that transforms\nor creates features from existing data to improve data quality.\nFE can determine the upper limit of ML applications [9]\n[39]. However, manual FE is laborious and requires domain\nknowledge. To address this issue, Automated FE (AutoFE),\nan important AutoML procedure, automates the generation\nand selection of relevant features, making the process more\nefficient and reproducible [8]. AutoFE combines three main\ncomponents: feature generation, feature selection, and feature\nextraction [8]. The generate-and-select strategy is employed by\nstate-of-the-art AutoFE approaches to automate the traditional\nFE process, such as the AutoFE process used in the AutoFeat\n[45] library. AutoFeat [45] is a Python library that provides\na multi-step AutoFE process based on the Lasso regression\nmodel to improve the traditional FE process. The general\nAutoFE process involves creating an exhaustive feature pool\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n10\nthrough feature generation methods and selecting or extracting\nvaluable features using feature selection and extraction meth-\nods [46].\ni) Feature Generation: It aims to create new features by\ntransforming or combining existing features to improve\nthe generalizability and robustness of ML models. Com-\nmon feature generation operations include unary oper-\nations (e.g., logarithmic operations), binary operations\n(e.g., addition, subtraction, multiplication, and division),\nand high-order operations (e.g., maximum, minimum, and\naverage) [47]. For network time-series data, features such\nas date/time features, lag features, and window-based\nfeatures can be generated to improve model performance.\nii) Feature Selection: It aims to identify the most appropriate\nfeatures and remove irrelevant or redundant ones. Auto-\nmated feature selection, or AutoFS, frames the feature\nselection problem as an optimization problem to auto-\nmatically determine the optimal number or percentage of\nselected features [48]. Existing feature selection methods\ncan be categorized into filter methods (e.g., Information\nGain (IG), and Pearson correlation), wrapper methods\n(e.g., Recursive Feature Elimination (RFE)), and embed-\nded methods (e.g., Lasso regularization and decision tree-\nbased algorithms) [8] [19] [49].\niii) Feature Extraction: It aims to reduce dimensionality by\napplying mapping functions, creating a more concise\nrepresentation of the original dataset while potentially\nincreasing model learning efficiency [50]. Common fea-\nture extraction methods include Principal Component\nAnalysis (PCA), Linear Discriminant Analysis (LDA),\nand Autoencoders (AE) [9]. Feature extraction is typically\nutilized when the feature set remains highly dimensional\nor under-performing after feature generation and selec-\ntion.\nIn summary, AutoFE is a dynamic combination of feature\ngeneration, feature selection, and feature extraction to auto-\nmate the FE process for improved efficiency and reproducibil-\nity. More accurate and efficient ML models can be developed\nby selecting the appropriate AutoFE methods and optimization\ntechniques, ultimately advancing network performance.\n3) Automated Model Learning (CASH and NAS):\nThe\nprimary challenge in applying ML algorithms to real-world\ntasks resides in the selection and configuration of the most\nappropriate ML model to achieve optimal results. For a\nprediction task, the accuracy of different models with different\nconfigurations may vary significantly [39]. As a result, it is\ncrucial to identify the most suitable ML model with optimal\nhyperparameter configuration.\nHyperparameters are the parameters of ML algorithms that\ndefine the architecture of ML models and must be specified\nbefore model learning [34]. According to the domains of\nhyperparameters, they can be classified as continuous hy-\nperparameters (e.g., the learning rate of neural networks),\ndiscrete hyperparameters (e.g., number of nearest neighbors in\nKNN), and categorical hyperparameters (e.g., the kernel type\nin SVM). Additionally, certain hyper-parameter configurations\ninclude conditionality. For example, in neural networks, the\nnumber of hidden layers and the number of neurons in\neach layer have strong correlations [34]. Conditional hyper-\nparameters must be tuned together to identify the optimal\nconfiguration.\nSelecting an appropriate ML algorithm and hyperparameter\nvalues can be seen as a search problem. The set of potential\nML models and their corresponding hyperparameter combina-\ntions constitute a search space, wherein each ML model, along\nwith its specific hyperparameter configuration, represents a\npoint within this space. Detecting the optimal point in the\nsearch space is a global optimization problem [34].\nTherefore, using optimization techniques to automatically\ndetect the optimal ML algorithm and hyperparameter con-\nfiguration is defined as Combined Algorithm Selection and\nHyper-parameter (CASH) optimization problems [51]. CASH\nsystems consist of two levels: automated model selection\nand Hyper-Parameter Optimization (HPO). At the first level,\nthe suitable ML models are selected based on their default\nhyperparameters. At the second level, model-specific hyper-\nparameters are optimized to obtain the final model with the\nbest performance [39].\nGenerally, CASH is defined as identifying the most suitable\nML model A⋆with its optimal hyperparameter configuration\nλ⋆that minimizes the loss function L, as depicted by the\nfollowing equation:\nA⋆, λ⋆∈\nargmin\nA(j)∈A,λ(j)∈λ\n1\nK\nK\nX\ni=1\nL\n\u0010\nA(j), λ(j), D(i)\ntrain, D(i)\nvalid\n\u0011\n(1)\nwhere A(j) ∈A is the ML algorithm to be selected, λ(j) ∈\nλ refers to the hyperparameters to be optimized, Dtrain and\nDvalid are the training and validation datasets, and K indicates\nthe number of folds in k-fold cross-validation.\nSimilarly, Neural Architecture Search (NAS) is the process\nof automating the design of DL models [7]. Apart from the\nhyperparameters in DL models, such as the learning rate, NAS\ntechniques focus on automatically determining the optimal\nneural network architecture, such as the number of hidden\nlayers, the type of each layer, and the number of neurons in\neach layer.\nOptimization techniques are used to solve CASH and NAS\nproblems by automatically detecting the most suitable ML\nmodel or DL architecture with the best-performing hyperpa-\nrameter configuration as the optimal solution [34]. Common\noptimization techniques for CASH and NAS tasks include Grid\nSearch (GS), Random Search (RS), Bayesian Optimization\nwith Gaussian Process (BO-GP), Bayesian Optimization with\nTree Parzen Estimator (BO-TPE), Hyperband, Genetic Algo-\nrithm (GA), Particle Swarm Optimization (PSO), and Rein-\nforcement Learning (RL) [8]. Table III provides a summary\nof these optimization techniques, outlining their descriptions,\nstrengths, and limitations. Suitable optimization methods can\nbe selected for specific tasks according to their respective\nstrengths and limitations.\n4) Automated Model Updating: Drift Adaptation: After\nthe automated model learning process to train optimal ML\nmodels, these models are deployed into real-world production\nenvironments, leveraging their capabilities to solve complex\ntasks. However, when these ML models are introduced into\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n11\nTABLE III\nTHE COMPARISON OF OPTIMIZATION METHODS FOR CASH AND NAS TASKS.\nOptimization Method\nDescription\nStrengths\nLimitations\nGrid Search (GS)\nA brute-force search method that evaluates all\nthe given hyperparameter values.\n· Straightforward to implement.\n· Works well with categorical\nhyperparameters only.\n· High computational cost.\nRandom Search (RS)\nRandomly samples hyperparameter\nconfigurations from the search space until the\nspecified budget is exhausted.\n· Faster than GS.\n· Allows for parallel execution.\n· Does not take previous outcomes\ninto account.\n· Inefficient for conditional\nhyperparameters.\nBayesian Optimization\nwith Gaussian Process\n(BO-GP)\nSelects future hyperparameter configurations\nbased on previous evaluations modeled by the\nGaussian process.\n· Rapid convergence for\ncontinuous hyperparameters.\n· Inefficient for other types of\nhyperparameters.\nBayesian Optimization\nwith Tree Parzen\nEstimator (BO-TPE)\nSelects future hyperparameter configurations\nbased on previous evaluations modeled by the\ntree-Parzen estimator.\n· Effective for all types of\nhyperparameters.\n· Limited parallelization\ncapabilities.\nHyperband\nTrains on randomly generated subsets and\nreduces the computational cost by balancing\nevaluation cost and accuracy.\n· Supports parallel execution.\n· Inefficient for conditional\nhyperparameters.\n· Requires small budget subsets to\nbe representative.\nGenetic Algorithm\n(GA)\nA population-based heuristic search technique\nthat passes the well-performing hyperparameter\nvalues to future generations.\n· Works well with all types of\nhyperparameters.\n· Does not require optimal\ninitialization.\n· Limited parallelization\ncapabilities.\nParticle Swarm\nOptimization (PSO)\nEnables communication and cooperation\namong particles to gradually identify the global\noptimum.\n· Effective for all types of\nhyperparameters.\n· Enables parallel execution.\n· Requires appropriate\ninitialization.\nReinforcement Learning\n(RL)\nEfficiently explores the search space and adapts\nits strategy based on the performance feedback.\n· Performs well with NAS tasks\nfor DL models.\n· High computational cost.\ndynamic networking systems, they may encounter substantial\nperformance degradation due to model drift or concept drift\nissues [10].\nModel drift refers to the decline in the ML model’s perfor-\nmance over time, causing model reliability issues in dynamic\nenvironments. Model drift can be further classified as data\ndrift and concept drift. Data drift refers to the unpredictable\nchanges in data distributions, while concept drift refers to the\nshifts in the relationships between the input features and the\ntarget variables [52]. In cybersecurity applications, changes in\nthe benign event data distributions are examples of data drift,\nwhile zero-day attacks exemplify concept drift [8]. Both data\nand concept drift can lead to the degradation of ML model\nperformance, thereby underscoring the necessity for automated\nmodel updating. This process aims to preserve high prediction\naccuracy in non-stationary networking environments [8]. This\nprocess aligns with continual learning, a crucial ML procedure\nfor data stream analytics.\nTo handle model drift and ensure ML models’ continuous\nreliability, learning systems must promptly detect and adapt\nto these changes. Drift detection and adaptation methods are\nessential components of automated model updating designed\nfor streaming data with data or concept drift issues. Drift\ndetection methods are primarily classified into two categories\n[53]:\ni) Distribution-based Methods: They detect drift by mon-\nitoring data drift or data distribution changes in time\nwindows. Techniques such as ADaptive WINdowing\n(ADWIN) [54], Information Entropy (IE), and Kullback-\nLeibler (KL) divergence [55] are commonly used for\ndistribution-based drift detection. These methods are of-\nten used in systems with limited memory and can provide\nhigh interpretability. However, they may incur higher\ncomputational costs than performance-based methods.\nii) Performance-based Methods: They track changes in per-\nformance or prediction error rates of ML models to\nidentify model drift. Popular methods like Drift Detec-\ntion Method (DDM) and Early Drift Detection Method\n(EDDM) use pre-defined thresholds to determine the\noccurrence of drift [56]. Performance-based methods are\nparticularly effective for detecting sudden drift in data\nstreams but may have limitations in identifying gradual\ndrift.\nOnce a drift is detected, adapting to the drift becomes\ncrucial to updating the ML models and maintaining their\nperformance in the context of data streams. There are three\nprimary types of drift adaptation methods [8]:\ni) Model Retraining: Model retraining is a simple approach\nfor drift adaptation that involves updating learning mod-\nels on newly arrived data streams. Strategies include\nfull retraining, partial retraining, and instance weighting.\nThe full retraining process entails retraining the learning\nmodel on all available samples in the dataset. Partial\nretraining methods, such as Optimized Adaptive and Slid-\ning Windowing (OASW) [10] and KNN-ADWIN [57],\nuse adaptive or sliding windows to detect model drift and\ncollect new concept samples, allowing for efficient model\nupdating. Instance weighting adjusts the weights of data\nsamples based on their recency, which helps the learning\nmodel adapt to model drift by retraining on weighted\nsamples [58].\nii) Incremental Learning Methods: Incremental learning\nmethods continuously update learning models sequen-\ntially as new data samples are introduced, ensuring the\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n12\nmodels stay up-to-date and adapt to evolving data. Ho-\neffding Trees (HT), Adaptive Online Neural Networks\n(AONN), and their variants are popular incremental learn-\ning methods. HT algorithms, such as Very Fast Decision\nTree (VFDT), Concept-adapting VFDT (CVFDT), and\nExtremely Fast Decision Tree (EFDT), use Hoeffding’s\ninequality to partially update nodes as new samples arrive,\nmaking them suitable for data stream analytics due to\ntheir ability to adapt to new samples [59]. AONN is\nanother incremental learning method based on neural\nnetworks that updates the model when the error rate\nincreases [60]. However, it is worth noting that while\nincremental learning methods offer benefits for adapting\nto new data, they are not specifically designed to address\nmodel drift issues.\niii) Ensemble Learning Methods: Ensemble learning tech-\nniques combine multiple base learners to achieve im-\nproved model drift adaptation performance. Block-based\nensembles and online ensembles are two main categories\nof ensemble methods for data stream analytics. Block-\nbased ensembles, such as Streaming Ensemble Algorithm\n(SEA) [61], Accuracy Weighted Ensemble (AWE) [62],\nand Diversity and Transfer-based Ensemble Learning\n(DTEL) [63], train base learners on discrete data blocks\nand update the ensemble when new blocks are added\nto the database. Online ensembles, such as Adaptive\nRandom Forest (ARF) [64], Streaming Random Patches\n(SRP) [65], and Performance Weighted Probability Av-\neraging Ensemble (PWPAE) [56], integrate multiple in-\ncremental learning models to enhance continual learning\nperformance. Ensemble learning models are generally\nefficient for handling gradual and recurring drifts but may\nstruggle with abrupt drifts and increased computational\ncomplexity.\nIn summary, the detection and adaptation of model drift are\nessential steps for automated ML model updating to maintain\nhigh model performance in dynamic ZTNs.\nD. AutoML-Enabled Autonomous Cybersecurity Mechanisms\nTo enhance defense against cyber-attacks, comprehen-\nsive cybersecurity systems incorporate four essential steps:\nanomaly detection, root cause analysis, and system remedi-\nation. To enhance defense against cyber-attacks, comprehen-\nsive cybersecurity systems incorporate four essential steps:\nanomaly detection, root cause analysis, system remediation,\nand intrusion prevention. For their deployment in ZTNs, these\nprocedures could be automated by AutoML techniques to meet\nthe network automation requirements.\n1) Automated Anomaly/Intrusion Detection: Ensuring the\nreliability and availability of ZTNs and general networks re-\nquires the identification and prediction of abnormal behaviors\nresulting from intentional or malicious actions. Early detection\nof potential network threats allows for a rapid response, min-\nimizing the risk of intentional harm, service degradation, and\nmonetary loss. AI/ML techniques are recognized as essential\nenablers for next-generation networks to identify anomalous\ntraffic patterns that could lead to service unavailability or\nsecurity issues. AI/ML models have demonstrated their ability\nto develop Intrusion Detection Systems (IDSs) by uncovering\nhidden patterns in large collections of multi-dimensional, time-\nvarying data [13]. In ZTNs and future networks, IDSs should\nbe automatically developed, provisioned, and managed to\ndetect various cyber-attacks.\nIn the first stage, the ML-based IDSs should enable real-\ntime monitoring of all network data from diverse sources,\nsuch as connections, devices, radio networks, current and old\ncore networks, services, transport, and Information technology\n(IT) operations. Monitoring and collecting network data from\na variety of sources enables network security managers to gain\na multi-perspective view of the network, which is necessary\nfor identifying anomalies that impact multiple domains or\nenvironments [18]. Supervised ML-based IDSs are capable\nof detecting intrusions by classifying benign data and known\ntypes of attacks, while unsupervised ML-based IDSs can\nbe used to distinguish previously unknown attacks from be-\nnign data. AutoML techniques can significantly enhance the\nperformance of both supervised and unsupervised ML-based\nIDSs by automating the detection of malicious cyber-attacks.\nSpecifically, AutoML can improve network traffic data quality\nthrough AutoDP and AutoFE methods, and train optimized\nIDS models by facilitating automated model selection and\nHPO techniques [8].\nThe development of IDS systems utilizing ML has cap-\ntured the attention of the cybersecurity research community,\nestablishing itself as a prime example of the application of\nML in enhancing cybersecurity defenses [5]. In their work,\nAgrafiotis et al. [66] introduced an ML model combining\nembeddings with a Fully-Connected network (Embeddings &\nFC) specifically designed for identifying malware traffic within\n5G networks. This model, when evaluated using the 5G-NIDD\ndataset tailored for 5G network scenarios, exhibited a notable\nincrease in detection accuracy. Furthermore, Tayfour et al.\n[67] explored the use of a Deep Learning (DL) approach\nthrough a Long Short-Term Memory (DL-LSTM) architecture\naimed at recognizing cyber-attacks targeting IoT and 5G\nnetworks, achieving significant accuracy improvements on the\nCICIDS2017 dataset and showcasing the DL model’s capabil-\nity in accurately identifying network intrusions. Additionally,\nHe et al. [68] developed an IDS based on a Pyramid Depth-\nwise Separable Convolution neural network (PyDSC-IDS),\nwhich, when compared to other DL approaches, delivered\nsuperior accuracy in detecting network intrusions with minimal\nadded complexity across various datasets including NSL-\nKDD, UNSW-NB15, and CICIDS2017, further highlighting\nthe advancements and effectiveness of DL techniques in the\nrealm of intrusion detection systems.\nAlthough the application of AutoML techniques to cyberse-\ncurity applications is a relatively new research area, there have\nbeen several existing works that have explored using AutoML\nfor the development of autonomous IDSs. The AutoML for\nIntrusion Detection (AutoML-ID) model proposed by Singh et\nal. [69] and the Optimized Ensemble IDS (OE-IDS) proposed\nby Khan et al. [70] represent state-of-the-art approaches that\nleverage AutoML for intrusion detection. AutoML-ID employs\nBO for automated model selection and HPO, whereas OE-\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n13\nIDS utilizes the h2o AutoML tool for selecting optimal\nML models and developing the final ensemble model. The\nenhanced accuracy demonstrated by the ML-based IDSs in\nthese studies underscores the potential of AutoML to improve\nIDS development.\nApart from a comprehensive overview of the network,\neffective IDSs should provide completely autonomous func-\ntionalities capable of continuously and optimally adapting to\nchanges and concept drift. This adaptability can be achieved\nthrough AutoML-enabled automated model updating methods,\nwhich ensure that the IDSs remain effective over time. By\nmonitoring the complete scope of data and implementing\nAutoML techniques to develop adaptable and optimized ML-\nbased IDSs, cyber-attacks or network abnormalities are iden-\ntified more quickly and accurately [8].\n2) Automated Root Cause Analysis: Root cause analysis\naims to identify the underlying cause of network issues,\nenabling rapid recovery. By analyzing the entire context and\nrelevant information of the detected abnormal events, this pro-\ncedure aims to determine the possible causes and appropriate\ncountermeasures/solutions for network anomalies. Root cause\nanalysis often requires a comprehensive correlation analysis\nacross multiple architectural levels, events, environments, and\nvendors [18].\nOnce an anomaly alert has been triggered by IDSs, it\nis crucial to determine its root cause. This is necessary to\nenable self-organizing systems, implement effective mitigation\nmethods, perform network forensics, and assign responsibility.\nHowever, with the complexity and diversity of evolving mobile\nnetworks, along with the growing number of Key Performance\nIndicators (KPIs) and data related to end-users, services, and\nnetworks, identifying the underlying cause can be challenging.\nManual root cause analysis based on expert knowledge is\ndifficult, time-consuming, and labor-intensive. On the other\nhand, AI/ML models have emerged as an attractive alternative\nfor facilitating self-root cause analysis due to their ability to\nanalyze large amounts of data, identify complex non-linear\ncorrelations within the data, and make faster and more accurate\ndecisions compared to manual analysis [13].\nIn practice, a root cause analysis powered by AutoML\nfacilitates the identification of causative factors for network\nanomalies, facilitating a prompt recovery and the implementa-\ntion of effective countermeasures. Through the examination\nof correlations among various architectural levels, events,\nenvironments, and vendors, root cause analysis powered by\nAutoML can offer comprehensive insights into network prob-\nlems. This not only enhances network resilience and reliability\nbut also reduces the time and resources required for network\ndiagnostics and recovery. Additionally, the automated model\nupdating procedure in the AutoML process can provide real-\ntime root causes, despite event changes. By leveraging Au-\ntoML, networks can autonomously conduct root cause analysis\nwith increased efficiency and precision, thereby mitigating the\nlimitations linked to conventional analysis techniques.\n3) Automated System Remediation: Network system recov-\nery, which entails the process of restoring a network system\nto its normal functioning state after being compromised by an\nattack, is a critical component of network security, facilitating\norganizations to swiftly respond to security incidents and\nmitigate the impact of network attacks [71]. This process is\nclosely tied to fault tolerance, ensuring networks maintain\nfunctionality amid failures, thus bolstering resilience against\ndisruptions. AutoML-based network monitoring systems pave\nthe way for autonomous remediation by autonomously identi-\nfying network anomalies and analyzing their root causes. This\ninformation can then be used by the remediation engine to\nrecommend actions for autonomous remediation. Additionally,\nthe remediation operations can be continuously enhanced\nthrough fine-tuning in a closed feedback loop.\nThere are several methods that can recover or remediate a\ncompromised network, including backup and restore, system\nreimage, network isolation, and blacklisting [18] [72]. The\nbackup and restore method involves creating regular back-\nups of network systems and restoring them after an attack\nto recover lost or corrupted data. System reimage involves\ncompletely reinstalling the operating system and software\non compromised network devices, which proves effective in\ncases where the attack has caused significant damage to the\nnetwork system. Network isolation is the process of isolat-\ning the compromised network segment from the rest of the\nnetwork using network segmentation techniques to limit the\nspread of the attack. This helps prevent further damage and\nprovides a safe environment for recovery efforts. Blacklisting\nis a common defense method that blocks traffic or requests\nfrom detected malicious or suspicious network devices/nodes\nuntil further authentication to prevent ongoing attacks. Among\nAI/ML techniques, Reinforcement Learning (RL) models have\nbeen used as a potential autonomous solution to effectively\nselect and recommend actions for fast network recovery [73].\nThrough AutoML techniques, the process of determining the\nmost suitable remediation method or countermeasure can be\nautomated. The automated model updating process can change\nor revise the remediation method by adapting to different\nattacks and networking environments.\nV. ADVERSARIAL ML ATTACKS AND DEFENSE\nA. Adversarial ML Attacks\nThe automation of modern networks has experienced sig-\nnificant improvement through the application of AI/ML tech-\nniques, resulting in enhanced functionalities such as self-\nplanning, self-optimization, self-healing, and self-protection\n[16]. However, AI/ML techniques introduce new attack inter-\nfaces in ZTNs. The AI/ML models are vulnerable to attacks\nduring the training and testing phases, compromising the\nintegrity, availability, and privacy of the system [74]. These\ntargeted cyber threats against AI/ML models are known as\nAML attacks [75]. Although AML attacks are not explicitly\ndesigned for ZTNs/6G networks, the increasing demand for\nnetwork automation and reliance on AI/ML techniques have\nelevated AML attacks to a significant threat in these networks.\nDue to network automation requirements and insufficient\nhuman supervision, it is challenging for ZTNs to maintain\nAI/ML model security. Additionally, given the capabilities and\nextensive uses of AI/ML methods, cyber-criminals can devise\ncomplex and advanced cyber-attacks with potentially severe\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n14\nFig. 4. Adversarial ML attacks & defense for ZTNs.\nand harmful consequences [13]. Therefore, the development\nof countermeasures to AML attacks is essential to ZTN\nsecurity. This is particularly important in the AutoML model\ndevelopment process due to minimal human supervision.\nIn AML attacks, cyber-attackers aim to deceive an AI/ML\nmodel by feeding it carefully crafted data intended to trigger\ninaccurate predictions or classifications. AML attacks can be\nclassified based on the stage at which they occur during the\nML process, including data poisoning attacks, evasion attacks,\nand inference attacks [13] [76]. Data poisoning attacks occur\nduring the training stage, where the cyber-attacker injects ma-\nlicious data into the training set to contaminate the model. Eva-\nsion attacks occur after the ML model has been trained, where\nthe cyber-attacker manipulates the ML model by modifying its\nparameters or structure to degrade its performance. Inference\nattacks occur during the deployment/prediction phase, where\nthe cyber-attacker manipulates the prediction data given to the\nML model to cause a specific outcome and potentially gain\naccess to sensitive information. Figure 4 illustrates these three\ntypes of AML attacks and potential defense mechanisms. The\nspecifications for each type of attack are discussed below.\n1) Poisoning Attacks: In poisoning attacks, an attacker\nmanipulates data or the AI/ML model during the training\nphase to influence the prediction results [75]. Attackers can\nexploit the need for continuous retraining learning models to\nadapt to new data distributions, thereby creating an opportunity\nto maliciously manipulate or poison the trained model. In\nclassification problems, poisoning attackers alter the labels\nof data samples to cause the ML model to misclassify them,\nwhile in regression problems, they modify the values of input\nfeatures to cause the model to produce incorrect outputs [77].\nPoisoning attacks can be executed using various strategies,\nsuch as data injection, data manipulation, and logic corruption\n[13] [75].\nIn data injection attacks, the attackers do not have access\nto the training data and aim to alter the data distribution by\nintroducing crafted malicious samples into the training dataset\nwhile keeping the original samples unchanged. This method\nis effective when the original classifier is trained with limited\ndata and requires additional data for retraining [76].\nData manipulation attacks, on the other hand, assume that\nattackers have full access to the training data, allowing them\nto directly contaminate the original data used for training the\nlearning model [13]. Contamination involves changing labels\n(e.g., malicious to benign) or adding small perturbations to the\ninput features. Attackers often target samples or labels with\nhigh classification confidence to maximize their impact on the\nML model while minimizing the chances of detection [75].\nLogic corruption attacks specifically target the learning\nalgorithm or its underlying logic with the intention of dis-\nruption. Logic corruption can be utilized against models that\nemploy distributed learning, such as federated learning, which\nrelies on the training of multiple agents. In such attacks, a\nmalicious agent can manipulate the local model parameters to\ncompromise the global model [13].\nPoisoning attacks can cause the classifier to make erroneous\njudgments, resulting in reduced accuracy, changes to decision\nboundaries, and even errors in final test results [78]. Poisoning\nattacks pose serious risks to network performance, security,\nand privacy in 6G networks, which rely significantly on ML\nfor numerous applications and services.\n2) Evasion Attacks: The purpose of evasion attacks is\nto discover challenging data samples that are most likely\nto be misclassified to deceive AI/ML models into making\nincorrect decisions. These attacks, unlike poisoning attacks,\ndo not affect the training process of ML models. Instead, they\noccur during the testing phase, where the attacker introduces\nadversarial examples, or minor perturbations, to the input\ninstances [76].\nEvasion attacks exploit the sensitivity of high-precision ML\nmodels to subtle perturbations, rendering ML-enabled ma-\nchines in autonomous networks vulnerable to being deceived\nby adversarial examples [78]. These attacks can result in\nsystem misclassification or reduced accuracy, posing privacy\nrisks for ML-based applications in autonomous networks.\nCommon evasion attacks include Decision Tree Attack\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n15\n(DTA) [79], Fast Gradient Sign Method (FGSM) [80], and\nBasic Iterative Method (BIM) [81]. DTA targets decision tree-\nbased ML models (e.g., random forest, XGBoost, and Light-\nGBM) by manipulating input features to mislead the models.\nThis attack demonstrates the need for enhanced protections in\ntree-based classifiers to maintain their reliability in security-\ncritical applications [79]. FGSM generates adversarial samples\nby introducing imperceptible amounts of noise to the input\ndata using gradients derived from the model’s loss function\n[80]. Due to its efficiency, FGSM poses challenges to ML\nmodels, particularly DL models. Consequently, it calls for the\ndevelopment of countermeasures to defend against gradient-\noriented attacks. BIM is an iterative variant of the FGSM\nattack in which adversarial perturbations are implemented\nincrementally over multiple iterations [81]. By allowing fine-\ngrained control over the perturbation process, BIM can gen-\nerate more effective adversarial examples. This iterative ap-\nproach highlights the importance of robust defenses that can\nwithstand iterative gradient-based attacks.\nEvasion attacks constitute the primary category of AML\nattacks, and cyber-attackers employ various techniques to\ncompromise the performance of AI/ML models. Examples of\nsuch evasion attacks include the Jacobian-Based Saliency Map\nAttack (JSMA), Projected Gradient Descent (PGD), Zeroth-\nOrder Optimization (ZOO) attack, and High Confidence Low\nUncertainty (HCLU) Attack [82].\nDefending against evasion attacks is challenging due to var-\nious factors, including the high dimensionality and continuous\nnature of the input space, the difficulty in precisely modeling\nthe decision boundary of the ML models, and the restricted\navailability of labeled data for training. In addition, the effec-\ntiveness of evasion attacks often relies on the particular ML\nalgorithm, its settings, and the quality of the training data.\n3) Inference Attacks: Inference or exploratory attacks refer\nto AML attacks where the attacker aims to gain knowledge\nor confidential information about the target ML model, its\ninput data, or its architecture [76]. These attacks can be\nespecially harmful in security-critical applications, such as\nprivacy-preserving ML models, where the attacker’s goal is\nto infer sensitive information about individuals or network\ndevices.\nInference attacks include model inversion, model extrac-\ntion, and membership inference attacks [13]. Model inversion\nattacks attempt to recover the training data by exploiting\nthe model’s outputs. Model extraction attacks seek to reveal\nthe model’s architecture and parameters to replicate a nearly\nidentical ML model by observing the model’s predictions\nand/or execution time. Membership inference attacks exploit\nthe model’s output to determine whether a data sample was\npart of the training dataset used for the target ML model.\nInference/exploratory attacks pose a threat to the privacy\nand confidentiality of sensitive information contained within\nthe ML models and their training data [78]. The success of in-\nference attacks often depends on the structure and parameters\nof the ML model, the quality and quantity of the training data,\nand the presence of privacy-preserving mechanisms, such as\ndifferential privacy. In addition, the efficacy of the attack can\nbe affected by the attacker’s knowledge of the ML model and\ntraining data.\nB. AI/ML Model Security Solutions: AML Defense Mecha-\nnisms\nAML defense is an emerging research field focused on\nenhancing the resilience of ML approaches against adversarial\nattacks [75]. Its primary objective is to assess the vulnerability\nof ML algorithms to adversarial attacks and develop effective\nresponses that promote more robust learning. Several defense\nmechanisms have been proposed to address adversarial at-\ntacks, such as input validation, adversarial training, adversarial\nsample detection, defense Generative Adversarial Networks\n(GANs), concept drift adaptation, differential privacy, and\nhomomorphic encryption [11], as illustrated in Fig. 4.\n1) Countermeasures Against Poisoning Attacks: There are\nseveral potential countermeasures/solutions to defend against\npoisoning attacks. Input validation involves the sanitization\nof training data by eliminating malicious or unusual sam-\nples before integrating them into the ML model [13]. Data\nsanitization and anomaly detection methods are also used as\ncountermeasures to remove suspicious/malicious samples from\nthe training data [77].\n2) Countermeasures Against Evasion Attacks: Evasion at-\ntacks can be defeated by adversarial training, adversarial\nsample detection, ensemble methods, and adversarial concept\ndrift adaptation techniques [13] [83]. Adversarial training can\ntrain/retain AI/ML models on an augmented dataset with ad-\nversarial samples to improve model performance. Adversarial\nsamples can be generated by defense GANs. In adversarial\nsample detection countermeasures, the adversarial samples\ngenerated by AML attacks are detected and removed from\nthe input data. Ensemble learning methods can improve the\nrobustness of AI/ML models and reduce the impact of polluted\nmodels by integrating multiple base learners. Adversarial con-\ncept drift adaptation techniques can update or retrain AI/ML\nmodels to confront AML attacks once a data distribution\nchange or a model performance degradation is detected.\n3) Countermeasures Against Inference Attacks: To mitigate\nthe impact of inference attacks, researchers have proposed\nseveral approaches, including Differential Privacy (DP), data\nperturbation, and homomorphic encryption [84]. In DP meth-\nods, random noise is injected into the model’s outputs to\nprevent the attacker from gaining knowledge about the training\ndata or individual samples. Data perturbation aims to perturb\nor transform the training data to make it difficult for the\nattacker to infer sensitive information. Homomorphic encryp-\ntion enables model training with encrypted data, ensuring\nthe confidentiality of data. However, this approach introduces\ncomputational complexities that need to be carefully managed\n[13].\nC. Summary\nThe common types of AML attacks and their potential\ncountermeasures are summarized in Table IV. Although there\nare many countermeasures to general AML attacks, cyber-\nattackers have been continually evolving their tactics to evade\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n16\nTABLE IV\nSECURITY THREATS IN AI/ML MODELS.\nAttack\nCategory\nSpecific Attack Examples\nDescription\nCountermeasures\nPoisoning\nAttacks\nData Injection\nOccur during the training phase, manipulate data or\nlearning algorithms to influence prediction results.\nInput validation, data sanitization, anomaly\ndetection, etc.\nData Manipulation\nLogic Corruption\nEvasion\nAttacks\nDecision Tree Attack\nOccur after the training phase, discover challenging\ndata samples likely to be misclassified to deceive\nAI/ML models.\nAdversarial training, adversarial sample\ndetection, ensemble methods, adversarial\nconcept drift adaptation, etc.\nFast Gradient Sign Method\nBasic Iterative Method\nInference\nAttacks\nModel Inversion\nOccur during the deployment/prediction phase, aiming\nto gain knowledge or confidential information about the\ntarget ML model, its training data, or its architecture.\nDifferential Privacy (DP), data perturbation,\nhomomorphic encryption, etc.\nModel Extraction\nMembership Inference\nthese defenses and breach ML models. Therefore, the devel-\nopment of effective defense mechanisms for AML attacks in\nZTNs is critical to ensuring the security and robustness of ML\nmodels integrated into ZTNs/6G networks.\nVI. CASE STUDY\nWith the introduction of ZTNs, AutoML techniques, and\nAML attacks, this section presents two case studies to illustrate\nthe capabilities and benefits of applying AutoML techniques to\nZTN security and depict the typical AML attack and defense\nprocess. The first case study presents a detailed AutoML\npipeline designed for automated intrusion detection within\nZTNs. In the second case study, we simulate three common\nadversarial attacks and devise basic defense mechanisms to\nsafeguard the IDS model obtained from the first study against\nthese attacks. The experimental results analysis for each case\nstudy are presented in Sections VI-B and VI-C.\nA. Description of Two Use Cases\nWith the rise of 5G networks, the increase in connected\ndevices and data traffic has broadened the attack surface, mak-\ning networks more vulnerable to cyber threats. The evolution\ntowards ZTN and future networks necessitates communica-\ntion systems that are not only more secure but also more\nautonomous and efficient. IDSs, as discussed in Section IV-D1,\nplay a pivotal role in monitoring and detecting security threats\nand malicious activities within modern networks [13].\nHowever, ZTN security solutions require high levels of\nautomation and minimal human intervention [11]. Therefore,\nthe development of automated IDSs becomes essential to\nachieving optimal network management in ZTNs. As dis-\ncussed in Section IV-C, AutoML is an advanced ML tech-\nnology that automates the design and implementation of ML\nmodels. This improves traditional ML procedures, making\nthem more autonomous and optimized. By leveraging AutoML\ntechniques, it is possible to develop efficient, accurate, and\nadaptable IDSs that overcome the performance and human\nsupervision limitations of traditional IDSs [85].\nTherefore, the first case study aims to provide a compre-\nhensive analysis of the development and implementation of an\nAutoML-based IDS for ZTNs. By leveraging ML algorithms\nand optimization methods outlined in Sections IV-A and IV-C,\nthe case study aims to establish optimal classification mod-\nels capable of distinguishing malicious attacks from benign\nevents. Additionally, the online learning methods introduced\nin Section IV-C4 are used in the online AutoML-based IDS\ndevelopment for realizing the automated model updating func-\ntionalities of AutoML in dynamic ZTN environments.\nThe second case study focuses on the cyber-defense exercise\nof AML attacks targeting the AutoML-based IDS derived from\nthe first case study. As many ZTN services and functionalities\nrely on AI/ML models, they are becoming increasingly vulner-\nable to AML attacks. AML attacks exploit the weaknesses and\nvulnerabilities of ML models by generating adversarial inputs\nthat can deceive or manipulate the models into making incor-\nrect predictions [75]. In ZTNs, AML attacks pose a significant\nthreat to overall network security and reliability. This case\nstudy aims to demonstrate the detrimental impact that AML\nattacks can have on ML models in ZTNs, and presents basic\ndefense strategies to mitigate these attacks, thereby ensuring\nthe accuracy of the ML-based IDS. In this case study, three\ncommon types of adversarial attacks discussed in Section\nV-A2 (i.e., DTA [79], FGSM [80], and BIM [81]) are used to\ngenerate adversarial samples to probe the vulnerability of the\nIDS. Subsequently, the basic defense mechanisms, including\nadversarial sample detection and filtering, are devised to\nsafeguard the AutoML-based IDS for ZTNs against AML\nattacks.\nTwo public benchmark network security datasets are utilized\nin these case studies to evaluate the proposed AutoML-based\nIDS and the AML attack & defense models: the Canadian In-\nstitute for Cybersecurity Intrusion Detection System 2017 (CI-\nCIDS2017) dataset [86] and the 5th Generation Network In-\ntrusion Detection Dataset (5G-NIDD) [87]. The CICIDS2017\ndataset is a state-of-the-art intrusion detection dataset with the\nlatest network threats. The CICIDS2017 dataset is close to\nreal-world network data since it has a large amount of network\ntraffic data, a variety of network features (80), various types of\nattacks (14), and highly imbalanced classes. The primary types\nof attacks in the CICIDS2017 dataset include DoS/DDoS,\nbotnets, brute force, port scan, and web attacks.\nThe 5G-NIDD dataset, created in December 2022, is a\nfully labeled resource constructed on a functional 5G test\nnetwork for researchers and practitioners evaluating AI/ML\nsolutions in the context of 5G/6G security [87]. 5G-NIDD\nencompasses data extracted from a 5G testbed connected to\nthe 5G Test Network (5GTN) at the University of Oulu,\nFinland. The dataset is derived from two base stations, each\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n17\nfeaturing an attacker node and multiple benign 5G users.\nThe attacker nodes target a server deployed within the 5GTN\nMEC environment. The attack scenarios captured in the dataset\nprimarily include DoS attacks and port scans. By employing\nthese two up-to-date datasets that closely mirror real-world\nnetwork traffic, the case study results become highly relevant\nto intrusion detection tasks and challenges in ZTNs. The case\nstudies are conducted using a reduced CICIDS2017 dataset\nwith 28,307 samples and a reduced 5G-NIDD dataset with\n12,159 instances for the purpose of this work.\nThe AutoML-based IDSs consist of offline learning and\nonline adaptive learning functionalities. To evaluate the of-\nfline ML/AutoML models in the case studies, 5-fold cross-\nvalidation is used in the experiments, as it is a commonly-\nused split that can help maintain a balance between bias\nand variance and reduce over-fitting [88]. In the experiments\nevaluating the long-term online learning performance of drift-\nadaptive models on network data in dynamic networking\nenvironments, prequential evaluation, or called test-and-train\nevaluation, is utilized. In prequential validation, each new\ndata sample undergoes an initial real-time test by the learners.\nThen, the learning model incorporates the sample for potential\nupdates [56]. Prequential validation is acknowledged as the\nstandard approach for assessing online learning processes.\nTo gauge the effectiveness of the models, a combination\nof four classification performance metrics is employed: accu-\nracy, precision, recall, and F1-scores. These four classification\nmetrics are often used in ML research as they provide a\nwell-rounded view of ML model performance [89]. Due to\nthe inherent imbalance in intrusion detection datasets, relying\nsolely on individual performance metrics, such as accuracy,\nprecision, or recall, can lead to a partial understanding of\nmodel performance. Therefore, we considered accuracy, pre-\ncision, recall, and F1-scores collectively for a comprehensive\ncomparison of ML models to avoid skewed evaluation results.\nThe F1-score, which calculates the harmonic mean of recall\nand precision scores, provides a balanced view of anomaly\ndetection outcomes while reducing bias. By including both\nfalse negatives (assessed by recall) and false positives (mea-\nsured by precision) in its calculation, the F1-score offers a\ncomprehensive performance metric for evaluating the proposed\nAutoML pipeline.\nFurthermore, to cater to the processing time and efficiency\ndemands of network systems, the learning time of each model\nis analyzed to compare the learning efficiency of the ML\nmodels. The ideal ML model for network data analytics should\nstrike a balance between effectiveness and efficiency, ensuring\noptimal performance while minimizing computational over-\nhead.\nB. Use Case 1: AutoML-based Automated IDS\n1) Experimental Setup: The first case study utilizes a com-\nprehensive AutoML pipeline to achieve automated intrusion\ndetection for both static and dynamic ZTN environments.\nThe experiments on AutoML-based IDS development were\nrun on a machine with an i7-8700 processor and 16 GB of\nmemory, representing a server machine in ZTNs for deploying\nautonomous IDSs. The functions and methods used in this\ncase study are implemented by extending the following Python\nlibraries: Sklearn [90] and LightGBM [91] for static ML model\ndevelopment, Optunity [92] for ML model optimization and\nautomation, and River [93] for online ML model development\nand automated model updating.\nThe proposed AutoML-based IDSs enable the automation\nof all necessary procedures in ML/data analytics, including\nautomated data pre-processing, automated feature engineering,\nautomated model selection, hyperparameter optimization, and\nautomated model updating (drift adaptation). Overall, the\nprocedures, methods, and aims/operations of each stage of the\nproposed AutoML pipeline are summarized in Table V.\nAs illustrated in Table V, the AutoML pipeline begins with\nAutoDP, which involves these steps:\ni) Encoding: The pipeline automatically detects categorical\nfeatures and transforms them into numerical features\nusing label encoding. This enhances the data’s readability\nfor ML models.\nii) Imputation: It automatically spots missing values in the\ndatasets and fills them with the median values using\nmedian imputation. This step enhances data quality.\niii) Normalization: The pipeline chooses an appropriate nor-\nmalization method (z-score or min-max normalization)\nto bring features to a similar scale. This enhances data\nquality.\niv) Balancing: The proposed system automatically identifies\nwhether the dataset is imbalanced. If an imbalance is\nidentified, synthetic minority class samples will be gen-\nerated using the ADASYN method [42] to balance the\ndata and improve data quality.\nNext, the pipeline enters the AutoFE stage. In this stage,\nthe RFE method is used to select vital features and discard\nirrelevant or noisy features. RFE is a wrapper feature selection\nmethod that recursively generates and evaluates feature subsets\nto remove unimportant features until a defined number of\nfeatures is selected [49]. To automate this step, PSO is used to\noptimize the number of selected features as a hyperparameter.\nAfter AutoDP and AutoFE, the offline AutoML pipeline\nenters the automated model learning stage. In the first step of\nthis stage, the pipeline automatically selects learning models\nfrom a set of four representative ML models: K-Nearest Neigh-\nbors (KNN), Multilayer Perceptron (MLP), Random Forest\n(RF), and LightGBM. KNN serves a basic and representative\nlow-complexity ML algorithm, while MLP represents a basic\nDL model commonly used in various applications. RF and\nLightGBM are two representative ensemble ML models known\nfor effectively handling non-linear and complex network data.\nTheir widespread usage in data analytics applications attests\nto their high generalizability. After assessing the performance\nof each learning model in terms of accuracy and F1-scores,\nboth the top-performing model and the second best-performing\nmodel with default hyperparameters are selected for further\nevaluation through HPO. This selection strategy, incorporating\nboth the top and second best-performing models, mitigates the\nrisk of overlooking the true optimal model and increases the\nlikelihood of identifying the most suitable one.\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n18\nTABLE V\nTHE SPECIFICATIONS OF THE PROPOSED AUTOML PIPELINE.\nCategory\nProcedure\nMethod\nAim/Operation\nAutoDP\nEncoding\nLabel Encoding\nConvert categorical features into numerical features to enhance the readability of the\ndata for ML models.\nImputation\nMedian Imputation\nIdentify and impute missing values with the median values to improve data quality.\nNormalization\nZ-Score or Min-Max\nNormalization\nAutomatically select a method to normalize features to a similar scale to enhance data\nquality.\nBalancing\nADASYN\nGenerate synthetic minority class samples to balance data and enhance data quality.\nAutoFE\nFeature Selection\nRFE\nSelect important features and remove irrelevant features to improve model efficiency\nPearson Correlation\nRemove redundant features to improve model efficiency and accuracy\nAutomate\nModel\nLearning\nOffline Model\nSelection\nKNN\nSelect the best-performing model among four alternative offline ML models\nby evaluating their learning performance.\nMLP\nRF\nLightGBM\nHPO\nPSO\nTune the hyperparameters of the offline ML models to optimize them.\nAutomated\nModel\nUpdating\nOnline Model\nSelection\nHT\nSelect the best-performing model among four online drift-adaptive models to\nadapt to dynamic network data streams with concept drift issues\nEFDT\nARF\nSRP\nHPO\nPSO\nTune the hyperparameters of the online ML models to optimize them.\nOnce the top two learning models are identified, the HPO\ntechnique is applied to fine-tune their hyperparameters, yield-\ning two optimized models. From these, the final optimal model\nis chosen. PSO is selected as the HPO method due to its\nsuperior performance in handling large hyperparameter spaces,\nlow complexity, and generalization ability [34].\nOn the other hand, in dynamic networking environments,\nan additional step, automated model updating, is required\nto address model drift issues and maintain the continuous\nreliability of ML models. Correspondingly, online learning\nmethods are required to replace static ML models in this case.\nIn the automated model updating or online learning process,\nthe online AutoML pipeline automatically selects online learn-\ning models from four representative online models: HT [59],\nKNN-ADWIN [57], ARF [64], and SRP [65], as introduced in\nSection IV-C4. Similar to offline learning, the online AutoML\npipeline will select the top two best-performing models for\nfurther HPO by PSO to obtain the optimized online learning\nmodel for intrusion detection in dynamic ZTNs.\n2) Experimental Results and Analysis: The effectiveness\nof the AutoML framework was assessed by comparing the\naccuracy, precision, recall, F1-score, and model learning time\nof ML models developed with and without using AutoML\ntechniques. The comparative performance of the ML/AutoML\nmodels on the CICIDS2017 and the 5G-NIDD datasets is\ndemonstrated in Table VI.\nTable VI demonstrates the effectiveness of the proposed\nAutoML pipeline by presenting three sets of results. The\nfirst set of results, ”AutoML Procedures: No”, demonstrates\nthe performance of four original ML algorithms with default\nhyperparameter configurations (without AutoML) as a base-\nline. The second set of results, ”AutoML Procedures: AutoDP\n& AutoFE”, displays the performance of ML algorithms\nafter applying the proposed AutoDP and AutoFE procedures,\nemphasizing the impact of data quality enhancement through\nAutoML. Finally, the third set, ”AutoML Procedures: All”,\nexhibits the performance of the complete AutoML pipeline,\nwhich includes AutoDP, AutoFE, automated model selection\nof the top-2 ML algorithms, and HPO procedures.\nThe AutoML pipeline begins with the execution of AutoDP\nand AutoFE, followed by the automated selection of the two\nbest-performing learning models based on their F1-scores in\nthe second set of results. Subsequently, the hyperparameters\nof the chosen models are optimized to obtain an ultimate\noptimal model with the highest F1-score, as illustrated in the\nthird set of results. The top-performing configurations in each\nexperimental set are highlighted in bold in Table VI.\nDuring the automated model learning process outlined in\nTable V, the two best-performing models are selected as can-\ndidate models, and their hyperparameters are tuned using PSO\nto obtain the ultimate optimal learning model. As demonstrated\nin Table VI, the two most effective ML algorithms for both\ndatasets are RF and LightGBM; hence, their hyperparameters\nare optimized. The search space and optimal values for the\nhyperparameters of these two learning algorithms are provided\nin Table VII. Continuous hyperparameters are assigned a\nsearch range, while categorical hyperparameters are given all\npotential values/options.\nThe left part of Table VI summarizes the results of the\nproposed AutoML pipeline on the CICIDS2017 dataset. For\nthe original ML models without any AutoML procedures, four\nmodels (KNN, MLP, RF, and LightGBM) exhibit large perfor-\nmance differences. The original RF and LightGBM models\nsignificantly outperform the other two ML models. After\nimplementing the proposed AutoDP and AutoFE procedures,\nRF and LightGBM models remain the top-performing models,\nsurpassing the other models by over 6% in terms of the F1-\nscore. The data quality improvement caused by AutoDP and\nAutoFE results in an increase in F1-scores and a decrease\nin learning time for both RF and LightGBM models. The\ncomplete AutoML pipeline further boosts the performance of\nthese learning models, with the optimized LightGBM model\nachieving the highest F1-score of 99.823%.\nSimilarly, the right part of Table VI reveals that RF and\nLightGBM models outperform the other two compared ML\nmodels on the 5G-NIDD dataset. Implementing AutoDP and\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n19\nTABLE VI\nTHE EXPERIMENTAL RESULTS OF THE PROPOSED OFFLINE AUTOML-BASED IDS ON THE CICIDS2017 AND 5G-NIDD DATASETS.\nAutoML\nProcedures\nOffline\nLearning\nAlgorithm\nCICIDS2017 Dataset\n5G-NIDD Dataset\nAccuracy\n(%)\nPrecision\n(%)\nRecall\n(%)\nF1 (%)\nModel\nLearning\nTime (s)\nAccuracy\n(%)\nPrecision\n(%)\nRecall\n(%)\nF1 (%)\nModel\nLearning\nTime (s)\nNo\nKNN\n97.238\n92.081\n93.782\n92.923\n3.9\n99.301\n99.198\n99.664\n99.430\n0.6\nMLP\n88.536\n94.277\n43.701\n58.830\n15.1\n98.026\n99.587\n97.179\n98.368\n7.6\nRF\n99.703\n99.577\n98.830\n99.248\n3.0\n99.670\n99.933\n99.530\n99.731\n3.3\nLightGBM\n99.816\n99.543\n99.506\n99.525\n0.3\n99.630\n99.798\n99.597\n99.697\n0.8\nAutoDP &\nAutoFE\nKNN\n97.058\n92.024\n92.831\n92.423\n0.5\n99.630\n99.465\n99.933\n99.698\n0.6\nMLP\n85.968\n92.069\n26.563\n44.831\n16.1\n98.808\n99.863\n98.187\n99.018\n6.3\nRF\n99.735\n99.465\n99.200\n99.332\n2.8\n99.794\n99.732\n99.933\n99.832\n0.6\nLightGBM\n99.770\n99.466\n99.378\n99.422\n0.2\n99.794\n99.866\n99.799\n99.832\n0.2\nAll\nRF\n99.735\n99.465\n99.200\n99.332\n3.6\n99.836\n99.866\n99.866\n99.866\n1.3\nLightGBM\n99.823\n99.468\n99.644\n99.556\n0.2\n99.877\n99.799\n100.0\n99.899\n0.7\nTABLE VII\nTHE HPO CONFIGURATION OF THE BEST-PERFORMING LEARNING\nMODELS.\nModel\nHyperparameter\nName\nConfiguration\nSpace\nOptimal\nValue on\nCICIDS2017\nOptimal\nValue on\n5G-NIDD\nRF\nn estimators\n[50,500]\n460\n200\nmax depth\n[5,50]\n25\n38\nmin samples split\n[2,11]\n4\n5\nmin samples leaf\n[1,11]\n1\n2\ncriterion\n[’gini’,\n’entropy’]\n’entropy’\n’entropy’\nLightGBM\nn estimators\n[50,500]\n300\n340\nmax depth\n[5,50]\n42\n22\nlearning rate\n(0, 1)\n0.884\n0.101\nnum leaves\n[100,2000]\n1400\n1300\nmin child samples\n[10,50]\n30\n45\nARF\nn models\n[3, 10]\n6\n5\ndrift detector\n[‘ADWIN’,\n‘DDM’]\n‘DDM’\n‘DDM’\nSRP\nn models\n[3, 10]\n4\n5\ndrift detector\n[‘ADWIN’,\n‘DDM’]\n‘DDM’\n‘DDM’\nAutoFE results in slight F1-score improvements and reduced\nlearning time for each ML model. After applying HPO to\nthe two best-performing models, RF and LightGBM, the\noptimized LightGBM model attains the highest F1-score of\n99.877% and is chosen as the final optimal model for the 5G-\nNIDD dataset.\nIn conclusion, the offline AutoML pipeline generates a\nsuperior IDS model with F1-score improvements of 0.134%\nand 0.067%, and learning time reductions of 50% and 12.5%\ncompared to the best-performing learning model without Au-\ntoML on the CICIDS2017 and 5G-NIDD datasets, respec-\ntively. This highlights the effectiveness of the AutoML pipeline\nin improving the quality of the IDS by effectively automating\nthe ML process.\nSecondly, to demonstrate the performance of the online\nAutoML-based IDS that incorporates automated model updat-\ning for dynamic ZTN environments, the experimental results\nare illustrated in Table VIII and Figs. 5 and 6. The hyperpa-\nrameters of the two better-performing online learning models,\nARF and SRP, are optimized using PSO, as illustrated in Table\nVII.\nAs shown in Table VIII and Fig. 5, all four base online\nlearning models—HT [59], KNN-ADWIN [57], ARF [64], and\nSRP [65]—can adapt to model drifts and restore their accuracy\non the CICIDS2017 dataset. Among the base online learning\nFig. 5.\nPerformance comparison of online learning methods on the CI-\nCIDS2017 dataset.\nFig. 6. Performance comparison of online learning methods on the 5G-NIDD\ndataset.\nmodels after implementing AutoDP and AutoFE, the ARF and\nSRP models achieve higher accuracy of 98.890% and 99.008%\nand show notable adaptability to model drifts, as indicated by\nthe accuracy dips and recoveries at drift points in Figure 5. The\nSRP model, particularly its optimized variant, demonstrates\nremarkable resilience, maintaining an average accuracy of\n99.254%. This is closely followed by the ARF model with\nan average accuracy of 99.090%. These results illustrate the\npotential of AutoML-enhanced models in maintaining high\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n20\nTABLE VIII\nTHE EXPERIMENTAL RESULTS OF THE PROPOSED ONLINE AUTOML-BASED IDS ON THE CICIDS2017 AND 5G-NIDD DATASETS.\nAutoML\nProcedures\nOnline\nLearning\nAlgorithm\nCICIDS2017 Dataset\n5G-NIDD Dataset\nAccuracy\n(%)\nPrecision\n(%)\nRecall\n(%)\nF1 (%)\nModel\nLearning\nTime (s)\nAccuracy\n(%)\nPrecision\n(%)\nRecall\n(%)\nF1 (%)\nModel\nLearning\nTime (s)\nNo\nKNN-\nADWIN [57]\n96.913\n90.112\n94.782\n92.388\n12.8\n98.729\n98.469\n99.453\n98.958\n6.0\nHT [59]\n90.839\n73.092\n84.904\n78.557\n6.0\n96.827\n97.283\n97.496\n97.389\n4.7\nARF [64]\n98.451\n96.215\n95.937\n96.073\n22.9\n98.696\n98.772\n99.083\n99.927\n10.6\nSRP [65]\n98.547\n95.968\n96.714\n96.340\n80.0\n99.261\n99.438\n99.343\n99.391\n45.4\nAutoDP &\nAutoFE\nKNN-\nADWIN\n98.08\n93.768\n96.714\n95.218\n11.9\n98.771\n98.404\n99.589\n98.993\n4.9\nHT\n93.173\n80.083\n87.125\n83.456\n5.4\n97.068\n98.480\n96.661\n97.562\n2.5\nARF\n98.890\n97.683\n96.678\n97.178\n24.4\n98.987\n99.044\n99.2883\n99.166\n9.4\nSRP\n99.008\n98.452\n96.497\n97.465\n58.4\n99.269\n99.357\n99.439\n99.398\n24.0\nAll\nARF\n99.090\n98.512\n96.858\n97.678\n30.8\n99.227\n99.411\n99.316\n99.363\n19.1\nSRP\n99.254\n99.133\n99.075\n99.093\n68.2\n99.360\n99.548\n99.398\n99.473\n33.9\ndetection accuracy amidst the data volatility characteristic of\ncybersecurity threats.\nMoreover, when compared to online learning models with-\nout the integration of AutoDP and AutoFE (where AutoML\nProcedures are not applied), the accuracy and F1-scores for all\nfour models observed a modest improvement, attributable to\nthe data quality enhancement and feature selection refinement\nfacilitated by these procedures. Notably, the model learning\ntimes for these models saw a significant decrease, with the\nSRP model showing a remarkable reduction from 80.0s to\n58.4s. This reduction was primarily driven by the efficiency\nof the automated feature selection process. Among these,\nthe optimized SRP model stood out, achieving the highest\naccuracy of 99.254% and an F1-score of 99.093% on the\nCICIDS2017 dataset, while maintaining a reduced model\nlearning time of 68.2s. The optimized models exhibit improved\ndetection capabilities compared to the non-optimized models.\nThis improvement is achieved through the AutoML pipeline,\nwhich fine-tunes model parameters and structures to better suit\nthe data characteristics, as evidenced by the hyperparameter\nconfigurations detailed in Table VII. These configurations are\nthe result of an extensive search within predefined ranges,\nguided by the AutoML process to identify the most effective\nvalues for each parameter.\nThe online learning results on the 5G-NIDD dataset re-\nsults, as presented in Table VIII and illustrated in Fig. 6,\nfurther underscore the efficacy of incorporating AutoML tech-\nniques, specifically AutoDP and AutoFE, in enhancing the\nperformance of online learning models within dynamic ZTN\nenvironments. For the 5G-NIDD dataset, the application of\nAutoDP and AutoFE procedures yielded a notable improve-\nment in both accuracy and F1-scores across all four online\nlearning models—HT, KNN-ADWIN, ARF, and SRP. This\nenhancement can be attributed to the refined data quality\nand improved feature selection facilitated by these AutoML\nprocedures. Among the models, the SRP model, with Au-\ntoML optimizations, exhibited a significant performance leap,\nachieving an exceptional average accuracy of 99.360% and\nan F1-score of 99.473%. This not only highlights the SRP\nmodel’s superior adaptability to evolving network conditions\nbut also its capability to maintain high detection accuracy\namidst the complex data landscape of 5G networks.\nThe ARF model also showed commendable performance,\nwith an improved average accuracy of 99.227%, demonstrating\nits robustness in the face of AML attacks and its respon-\nsiveness to the dynamic nature of 5G network traffic. The\nobserved reduction in model learning time, especially for the\nSRP model—from 45.4s to 24.0s in the presence of AutoDP\nand AutoFE—emphasizes the impact of automated feature\nselection in streamlining the model optimization process.\nThe superiority of the optimized SRP model on the CI-\nCIDS2017 and 5G-NIDD datasets, alongside its counterpart\nARF model, reiterates the critical role of AutoML in tai-\nloring IDS solutions to specific network environments. The\nenhanced detection capabilities of these models, as evidenced\nby their performance metrics, affirm the potential of AutoML\nto revolutionize IDS development by automating the tuning\nof model parameters and structures to optimally match the\ncharacteristics of the data being analyzed. This adaptability is\nessential for maintaining the efficacy of autonomous IDSs in\nthe face of the constantly evolving threat landscape character-\nistic of cybersecurity challenges in ZTNs that have dynamic\nnetworking environments and high-level network automation\nrequirements.\nThe optimized models can achieve high performance with-\nout overfitting mainly due to the following reasons [19]:\n1) The proposed models were evaluated using 5-fold cross-\nvalidation to mitigate overfitting, providing a more accu-\nrate representation of their generalizability across unseen\ndata. This method helps in ensuring that our high perfor-\nmance is consistent across various subsets of the data.\n2) A comprehensive AutoFE method has been implemented\nto enhance the models’ ability to generalize. By system-\natically removing irrelevant and misleading features, the\nproposed models are trained only on data that contribute\nto predictive performance, reducing the risk of overfitting.\n3) The datasets used, namely CICIDS2017 and 5G-NIDD,\nhave distinct attack patterns that are easier to distin-\nguish compared to more complex datasets. This inherent\ncharacteristic, coupled with our robust model evalua-\ntion and feature engineering methods, contributes to the\nhigh accuracies reported. Comparable studies in the field\nhave achieved similar performance, indicating that our\nresults are in line with expectations for these datasets.\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n21\nFor instance, as described in Section IV-D, the DL-\nLSTM model proposed in [67] and the PyDSC-IDS\nmodel proposed in [68] achieved high accuracy and F1-\nscores of more than 99.3% on the CICIDS2017 dataset.\nSimilarly, the Embeddings & FC model, proposed in\n[66], showcased a remarkable accuracy of 99.123% and\nan F1-score of 98.666% on the 5G-NIDD dataset. This\nfurther highlights that the two datasets have distinct attack\ncharacteristics that can be easily identified, which directly\nimpacts the high performance of ML models.\nC. Use Case 2: AML Attacks and Defense\n1) Experimental Setup: Although the autonomous IDSs\ndeveloped in the first case study achieved high performance\nfor ZTNs, the network automation requirements of ZTNs have\nintroduced new security threats, AML attacks, due to the lack\nof human supervision. The second case study implements a\ncyber-defense exercise against three common AML attacks\ndiscussed in Section V-A2 (i.e., DTA [79], FGSM [80],\nand BIM [81]). These three AML attacks are launched in\nseparate experiments to generate adversarial samples. These\nadversarial samples are intended to disrupt the input data\nused by the AutoML-based IDS obtained in the first case\nstudy. The purpose of these experiments is to demonstrate\nthe potential damage caused by AML attacks on ML models\nwithin ZTNs/6G networks.\nSubsequently, the experiments progress to the next stage,\nwhere a fundamental AML defense mechanism, namely ad-\nversarial sample detection and filtering [13], is deployed\nagainst each type of AML attack. This illustrates the necessity\nand effectiveness of this fundamental defense mechanism in\nmitigating AML attacks and maintaining the reliability of\nthe AutoML-based IDS within ZTNs. In this case study, the\noptimized LightGBM models fulfill dual roles as both the\nmodels are susceptible to compromise under AML attacks and\nthe defense models for detecting adversarial samples.\nThe experiments in the second case study were conducted\non the same hardware as the first, utilizing a machine with\nan i7-8700 processor and 16 GB of RAM. This machine\nrepresents a ZTN server equipped with autonomous IDSs, now\ntargeted by AML attacks intended to disrupt IDS functionality.\nThe AML attacks and defense mechanisms were implemented\nby extending Python libraries: LightGBM [91] for the com-\npromised ML model and adversarial sample detection model\ndevelopment, and the Adversarial Robustness Toolbox (ART)\n[82] for simulating three adversarial attacks: DTA [79], FGSM\n[80], and BIM [81].\nThe specific procedures of the proposed cyber-defense ex-\nercise are as follows:\ni) Evaluate the optimized IDS model obtained from the\nfirst case study’s AutoML procedure, LightGBM with\nthe optimal hyperparameters outlined in Table VII, on\nthe original data set. This serves as the baseline model\nwithout any AML attacks.\nii) Generate adversarial samples using one of the AML\nattacks (i.e., DTA, FGSM, or BIM) and combine these\nsamples with the original training set. This simulates a\ndisrupted input dataset under the AML attack.\niii) Assess the IDS model on the adversarial samples gener-\nated in Procedure ii). This demonstrates the performance\nof a model under an AML attack.\niv) Develop an adversarial sample detection model using\nthe same LightGBM model. This model is trained on\nan integrated dataset of the original training set and\nadversarial samples to distinguish between them and\nidentify adversarial samples.\nv) Remove the detected adversarial samples from the train-\ning set to create a sanitized dataset.\nvi) Re-train the IDS model on the sanitized training set and\nevaluate its performance on the test set, representing the\neffectiveness of the chosen defense model: adversarial\nsample detection and filtering.\n2) Experimental Results and Analysis: The performance of\nthe cyber-defense exercise against AML attacks was evaluated\nby comparing the accuracy, precision, recall, F1-score, and\nmodel learning time of both the AutoML-based IDS model\nand the AML detection model, using the CICIDS2017 and 5G-\nNIDD datasets. The results for these two datasets are presented\nin Table IX. Figures 7 and 8 demonstrate the accuracy and F1-\nscore changes of the IDS model on the two datasets during\nthe AML attacks and after recovery.\nTable IX illustrates the results of two model groups: the\nAutoML-based IDS models and the AML detection models, on\nthe two datasets. The results of the AutoML-based IDS models\ncan be classified into three states: the original IDS models,\nthe IDS models under attack, and the recovered IDS models.\nThese states align with the proposed cyber-defense exercise\nprocedures i), iii), and vi), discussed in Section VI-C1.\nThe results of the first state present the performance of\nthe original IDS model as a baseline. The results of the\nsecond state display the performance of the IDS model under\ndifferent AML attacks, showcasing the impact or damage\nof these attacks on the IDS model’s performance. The third\nstate’s results demonstrate the performance of the IDS model\nafter implementing the adversarial sample detection and filter-\ning mechanism, illustrating the effectiveness of this defense\nmechanism in restoring the IDS model’s performance after\nan AML attack. The final part of the results illustrates the\nperformance of the AML attack detection models, aligning\nwith the adversarial sample detection model derived from the\nproposed cyber-defense exercise procedure iv) from Section\nVI-C1.\nThe cyber-defense exercise begins with the evaluation of\nthe original IDS model, followed by the IDS model per-\nformance assessment under DTA, FGSM, and BIM attacks.\nSubsequently, the adversarial sample detection and filtering\nmechanism is applied to remove detected adversarial sam-\nples, and the IDS model is re-trained and evaluated on the\nsanitized dataset. Table IX encapsulates the results of the\ncyber-defense exercise using the CICIDS2017 and 5G-NIDD\ndatasets. For the original IDS model without AML attacks, the\nmodel demonstrates a high degree of performance across all\nmetrics: accuracy, precision, recall, and F1-score. Specifically,\nthe F1-scores of the pristine IDS model stand at 99.556%\nand 99.899% on the CICIDS2017 and 5G-NIDD datasets,\nrespectively.\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n22\nTABLE IX\nTHE EXPERIMENTAL RESULTS OF THE AML ATTACK AND DEFENSE MODELS ON THE CICIDS2017 AND 5G-NIDD DATASETS.\nModel\nState\nModel Specifics\nCICIDS2017 Dataset\n5G-NIDD Dataset\nAccuracy\n(%)\nPrecision\n(%)\nRecall\n(%)\nF1\n(%)\nModel\nLearning\nTime (s)\nAccuracy\n(%)\nPrecision\n(%)\nRecall\n(%)\nF1\n(%)\nModel\nLearning\nTime (s)\nThe\nAutoML-\nbased IDS\nModel\nOriginal\nThe original IDS model\n99.823\n99.468\n99.644\n99.556\n0.2\n99.877\n99.799\n100.0\n99.899\n0.7\nUnder\nAttack\nThe IDS model under the\nDTA attack\n78.608\n15.873\n1.778\n3.197\n0.1\n62.459\n68.947\n70.383\n69.658\n0.6\nThe IDS model under the\nFGSM attack\n86.310\n96.296\n32.356\n48.436\n0.3\n40.584\n97.826\n3.022\n5.863\n0.5\nThe IDS model under the\nBIM attack\n88.324\n96.586\n42.756\n59.273\n0.5\n38.734\n33.333\n0.067\n0.134\n0.7\nRecovered\nThe IDS model recovered\nfrom the DTA attack\n99.806\n99.379\n99.644\n99.512\n0.2\n99.877\n99.799\n100.0\n99.899\n0.5\nThe IDS model recovered\nfrom the FGSM attack\n99.770\n99.291\n99.556\n99.423\n0.2\n99.877\n99.799\n100.0\n99.899\n0.8\nThe IDS model recovered\nfrom the BIM attack\n99.806\n99.379\n99.644\n99.512\n0.3\n99.836\n99.732\n100.0\n99.866\n0.6\nThe AML\nDetection\nModel\n-\nThe adversarial sample\ndetection model for DTA\nattacks\n99.937\n99.932\n99.942\n99.937\n0.3\n99.822\n99.720\n99.925\n99.822\n0.6\nThe adversarial sample\ndetection model for\nFGSM attacks\n99.407\n99.667\n99.144\n99.405\n0.3\n99.663\n99.775\n99.550\n99.662\n0.5\nThe adversarial sample\ndetection model for BIM\nattacks\n99.453\n99.673\n99.230\n99.451\n0.4\n99.634\n99.579\n99.691\n99.635\n0.5\nFig. 7. The performance changes of the IDS model on the CICIDS2017 dataset in the AML attacks and defense experiments.\nFig. 8. The performance changes of the IDS model on the 5G-NIDD dataset in the AML attacks and defense experiments.\nHowever, the performance of the IDS model significantly\ndecreases under the influence of DTA, FGSM, and BIM\nattacks on both datasets. The F1-scores of the IDS fall to\na range between 0.134% and 69.658%, reflecting a decline\nof at least 30% across both datasets. As shown in Table IX,\namong the three AML attacks, DTA causes the most damage\nto the CICIDS2017 dataset, while BIM proves to be the\nmost effective AML attack against the 5G-NIDD datasets. In\naddition, BIM attacks cause a significant increase in the model\nlearning time for both datasets.\nIn contrast, the implementation of the adversarial sample\ndetection and filtering mechanism results in a remarkable\nperformance improvement for the IDS model. The accuracy,\nprecision, recall, and F1-scores exhibit significant enhance-\nments, closely resembling those of the original IDS model.\nOn the CICIDS2017 dataset, the F1-scores of the restored IDS\nmodels range from 99.423% to 99.512%, whereas on the 5G-\nNIDD dataset, they range from 99.866% to 99.899%.\nThe performance of the AML detection model, as depicted\nin Table IX, demonstrates its effectiveness in identifying and\ndifferentiating adversarial samples from the original, legitimate\nsamples. For each form of AML attack, high accuracy, preci-\nsion, recall, and F1-scores are recorded, ranging from 99.451%\nto 99.937%.\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n23\nTABLE X\nTHE OPEN CHALLENGES AND RESEARCH DIRECTIONS OF ZTNS/6G NETWORK SECURITY.\nCategory\nChallenge\nBrief Summary\nGeneral\nCybersecurity\nChallenges\nRapid Evolvement of\nCyber-Attacks [13]\nThe constantly evolving nature of cyber threats and the use of AI/ML by cybercriminals demand\ncontinuous updating and deployment of new security measures.\nNetwork Complexity [74]\nThe complexity of 5G and 6G networks, with numerous connected devices and systems, makes threat\ndetection and response increasingly difficult.\nCybersecurity System\nCost [94]\nImplementing effective security measures can strain resources, particularly for small enterprises.\nCross-Layer Network\nSecurity [14]\nCross-layer intelligence in ML-based network management frameworks enhances overall network\nsecurity and resilience by leveraging interdependencies and cooperation across protocol stack layers.\nAML\nDefense\nChallenges\nLack of Autonomous\nSolutions [6]\nDeveloping autonomous cybersecurity solutions is crucial for 6G networks, but current automation\ntechniques require additional research to address AI/ML model security challenges.\nFeasible Cyber-Defense\nExercises [95]\nDigital Twin (DT) [96] technology can facilitate cyber-defense exercises without disrupting network\noperations, enabling the evaluation and improvement of AML protection models.\nModel Privacy Issues [78]\nSharing data and models between network devices raises privacy concerns; federated learning [97] and\nblockchain [98] may address these issues, but require further research.\nInterpretability and\nExplainability [99]\nIncorporating Explainable AI (XAI) techniques into AML protection models can improve understanding\nof decision-making processes, enabling more robust defenses against adversarial attacks.\nAutoML\nChallenges\nData Pre-Processing and\nFeature Engineering [102]\nAutomating data pre-processing and feature engineering in AutoML pipelines is complex, requiring\nadditional research.\nLarge Scale AutoML [34]\nApplying AutoML to large-scale data remains an open challenge, as most AutoML solutions are\ndesigned for small datasets.\nTrustworthy AI\n[103]-[104]\nDeveloping AI systems that are reliable, safe, secure, and ethical is crucial for AutoML models, and\nrequires addressing aspects like robustness, reproducibility, explainability, privacy, accountability, and\nfairness.\nIn summary, the destructiveness of AML attacks on ML\nmodels within ZTNs can be seen in the following:\ni) Reduced Accuracy and Effectiveness: AML attacks can\nsignificantly degrade the accuracy of ML-based IDSs,\nrendering them ineffective at detecting and mitigating\nsecurity threats. This poses a threat to network resources\nand infrastructure.\nii) Increased Complexity: AML attacks add an additional\nlayer of complexity to the already complex security\nlandscape of ZTNs, necessitating the development of\ndefense mechanisms to defend against these sophisticated\nthreats.\niii) Erosion of Trust: Successful AML attacks can erode\nconfidence in ML models and their capacity to provide re-\nliable security solutions for ZTNs, resulting in decreased\nconfidence in the adoption of these technologies.\nAdditionally, the adversarial sample detection and filtering\nmethod utilized in the case study can effectively distinguish\nadversarial samples generated by AML attacks and remove\nthem. This results in a clean training set capable of effectively\nrestoring the performance of ML models developed in the first\ncase study.\nVII. OPEN CHALLENGES AND RESEARCH DIRECTIONS\nDespite the existence of various network security solutions,\neffectively deploying them within ZTNs poses several unre-\nsolved challenges. In this Section, the open challenges and\nresearch directions concerning ZTN security mechanisms are\ndiscussed. These challenges and directions are comprehen-\nsively summarized in Table X.\nA. General Cybersecurity Mechanism Development Chal-\nlenges\n1) Rapid Evolvement of Cyber-Attacks: One of the major\nchallenges of general cybersecurity is the rapidly evolving\nnature of cyber threats. Cybercriminals are constantly de-\nveloping new methods to bypass existing security measures,\nand organizations must be able to adapt to these changes\nto maintain their security. Moreover, attackers can leverage\nAI/ML techniques to prioritize vulnerabilities for massive\nnetwork attacks. For instance, AI-based botnets can identify\nzero-day vulnerabilities in IoT devices and exploit them to\ninitiate DDoS attacks against 5G RAN resources [13].\nThis necessitates organizations to invest in the development\nand deployment of new cybersecurity technologies and to\nupdate existing security measures on a regular basis.\n2) Network Complexity: Another challenge of cybersecu-\nrity is the complexity of 5G and 6G networks [74]. As the\nnumber of connected devices increases and cloud computing\nbecomes prevalent, detecting and responding to cybersecurity\nthreats in complex networks becomes increasingly difficult.\nNumerous systems and devices require security measures, and\nidentifying the most vulnerable components can be challeng-\ning.\n3) Cybersecurity System Cost: The financial burden of\ncybersecurity poses a significant challenge, particularly for\nsmall enterprises. The cost of implementing effective security\nmeasures must be measured against the cost of a security\nviolation [94]. Additionally, quantifying the cost of network\nsecurity is challenging as it encompasses not only the expenses\nrelated to implementing security measures but also potential\ncosts arising from data breaches, delays, and reputational\ndamage.\n4) Cross-Layer Network Security: Traditional ML-based\nnetwork management services focus on single protocol stack\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n24\nlayers, limiting large-scale intelligence potential. Cross-layer\ncooperation, however, enables the development of more flex-\nible and efficient solutions in heterogeneous networks [14].\nIn the context of cross-layer network security, it is essential\nto examine the interdependencies between different layers\nof the protocol stack. For instance, the RAN can optimize\nwireless channel operation by dynamically adapting both the\nlink and physical layers accordingly. By leveraging cross-\nlayer intelligence, it becomes possible to identify and address\nsecurity challenges that span multiple layers, leading to more\ncomprehensive and robust security mechanisms.\nCross-layer network security solutions can also facilitate\nmore effective detection and response to cybersecurity threats,\nas the integration of information from different layers can\nprovide a more accurate and complete understanding of the\nnetwork’s security state. This holistic approach can lead to the\ndevelopment of more advanced ML-based security models that\ncan adapt to evolving threats, improving the overall resilience\nof the network.\nB. AML Defense Model Development Challenges\n1) Lack of Autonomous Solutions: Existing AML security\nmodels typically require human analysis or supervision to\ncomprehend attack patterns and design effective countermea-\nsures. However, autonomous cybersecurity solutions are re-\nquired to facilitate network automation for ZTNs/6G networks\n[6]. AutoML techniques discussed in Section IV-C are promis-\ning solutions for interacting with networking environments and\ndeveloping security models automatically. However, current\nAutoML techniques are proposed for general cybersecurity\nissues, and additional research is necessary to address AI/ML\nmodel security challenges.\n2) Feasible Cyber-Defense Exercises: Cyber-defense exer-\ncises that simulate cyber-attacks in networks are often required\nto develop and evaluate AML countermeasures and security\nmechanisms, but launching attacks will disrupt the normal\noperation of physical networks [95]. Digital Twin (DT) [96]\nis a promising technology for improving traditional cyber-\ndefense exercises. The primary objective of DT models is to\ngenerate virtual copies of a physical system or network to\nsimulate its behavior and real-time usage. DT models enable\ncyber-defense exercises and security model evaluations in\ncybersecurity applications without interfering with the normal\noperation of networks. In DT-assisted networks, AML attacks\nand defense operations can be simulated by launching multiple\ntypes of AML attacks and then evaluating the effectiveness of\nprotection mechanisms. Once their performance is evaluated,\nthe defensive models will be transmitted back to the actual\nnetwork from the virtual networks.\n3) Model Privacy Issues: The implementation of certain\nAML countermeasures for 5G/6G networks raises concerns\nregarding model privacy, as it necessitates the sharing of data\nand models from multiple network devices to develop com-\nprehensive security measures [78]. Developing effective coun-\ntermeasures for specific attacks usually requires an exhaustive\nand detailed comprehension of AML attacks and affected ML\nmodels. However, data or model leakage may occur during the\ndevelopment of AML security models. Federated learning [97]\nand blockchain [98] techniques are potential solutions to ad-\ndress the privacy issues of AML security model development,\nbut their integration with AML security approaches requires\nadditional research.\n4) Interpretability and Explainability: Explainable Arti-\nficial Intelligence (XAI) is a crucial aspect of developing\nand deploying AML protection models in ZTNs [99]. Even\nthough ML models have the potential to achieve high levels\nof accuracy and performance, their decision-making processes\nare often viewed as ”black boxes”, which makes it challenging\nfor users to understand the reasoning behind their conclusions.\nThe limited interpretability of AML models can impede the\ndevelopment of effective countermeasures, as security analysts\nface challenges in identifying potentially exploitable vulnera-\nbilities within the model. Incorporating XAI techniques into\nAML protection models can provide a deeper understanding\nof the decision-making process, enabling the identification of\nvulnerabilities and the development of more robust defenses\nagainst adversarial attacks. Common XAI methods include\nfeature importance calculation & analysis, visualization meth-\nods, and model agnostic techniques, such as SHapley Additive\nexPlanations (SHAP) and Local Interpretable Model-agnostic\nExplanations (LIME) [100]. However, achieving the optimal\nbalance between model performance, interpretability, and se-\ncurity remains a challenging task that necessitates further\nresearch. Future work in XAI is essential to enhance the\nsecurity of AI and ZTNs against emerging cyber threats,\nensuring they are reliable and safe for supporting advanced\nfunctionalities services.\nC. AutoML Technology Challenges\nDuring the past decade, AutoML has made substantial\nprogress in automating model construction and development,\nespecially for supervised learning tasks. However, AutoML\nfaces several challenges that need to be addressed before it can\nbe widely applied to real-world ZTN/6G network applications\n[102]:\n1) Automated Data Pre-Processing and Feature Engineer-\ning: Although there are many existing AutoML solutions,\nthe vast majority of them concentrate on automated model\nselection and HPO, with less emphasis placed on automated\ndata pre-processing and feature engineering [102]. Yet, these\ntwo aspects are vital components of the AutoML pipeline\nand directly influence the performance of systems, particularly\nin the context of B5G/6G networks that require large-scale\ndata analytics. Feature engineering is especially challenging\nto automate and generalize, as it heavily depends on the\nspecific task and dataset [47]. Implementing effective feature\nengineering often necessitates specialized domain knowledge\nor considerable effort. As such, automated feature engineering,\ndespite its complexity, is an imperative research topic that\nwarrants further investigation.\n2) Large Scale AutoML: AutoML’s application to large-\nscale data remains unresolved. Since AutoML pipelines often\nneed a large number of model trainings to determine the\noptimal end learner, most AutoML solutions are created on\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n25\nsmall datasets, with just a handful capable of large-scale data\nlearning. For example, due to the huge size of the ImageNet\ndataset, research on AutoML solutions for the ImageNet\nchallenge is still limited [34].\n3) Trustworthy AI: Trustworthiness of AI/ML techniques\nrefers to the extent to which an AI system is reliable, safe,\nsecure, and ethical [103]. The development of trustworthy AI\nsystems is crucial for ensuring that AI can be used for the\nbenefit of ZTNs rather than causing harm. Trustworthy AI is\nparticularly important for AutoML models, as they often lack\ndirect human supervision. Key aspects of AI trustworthiness\ninclude robustness, reproducibility, explainability, privacy, ac-\ncountability, and fairness [104].\ni) Robustness: It is a key aspect of AI systems, which\nrefers to their ability to handle execution errors, incorrect\ninputs, and unseen data. A robust AI system should\nexhibit resilience at different levels of data, algorithms,\nand systems.\nii) Reproducibility: It indicates the ability of AI/ML mod-\nels to be reproduced. It is crucial for validating and\nassessing AI/ML research, enabling the identification\nand mitigation of potential risks in AI systems. The AI\nresearch community increasingly views reproducibility as\na requirement for publishing research.\niii) Explainability: It is essential for building trust in AI/ML\ntechnology\nand\nentails\nunderstanding\nthe\ndecision-\nmaking process of AI/ML models [105]. Research on\nAI/ML explainability has been conducted in two aspects.\nThe first aspect involves the development of fully or\npartially explainable ML models, such as linear regres-\nsion and decision tree-based models. The second aspect\nfocuses on analyzing complex models like DL models by\nexamining their input, intermediate results, and output. As\ndescribed in Section VII-B.4, XAI is a critical research\ntopic to enhance the trustworthiness of AutoML and\nZTNs.\niv) Privacy Protection: It refers to safeguarding data that can\ndirectly or indirectly identify individuals or households.\nPrivacy protection spans the entire lifecycle of AI/ML\nsystems, covering data collection & pre-processing,\nmodel training, and deployment.\nv) Accountability: It is essential for ensuring that AI systems\nadhere to trustworthiness requirements. Accountability\nincorporates the entire lifecycle of an AI system and\nrequires stakeholders to justify their design, implemen-\ntation, and operation in accordance with human values.\nvi) AI Fairness: It involves mitigating various types of bias,\nsuch as data bias, model bias, and procedural bias [106].\nThese biases frequently lead to the unjust treatment of\nvarious groups based on their protected information, such\nas gender, race, and ethnicity. To prevent the perpetuation\nor exacerbation of social bias, it is vital to resolve\nimpartiality in AI systems.\nIn conclusion, establishing trust in AI/ML and AutoML\ntechnologies is a critical step in their successful application\nto ZTNs.\nVIII. CONCLUSION\nThe growing demand for network automation in the next\ngeneration of networks has led to the development of Zero-\nTouch Networks (ZTNs), where Artificial Intelligence (AI)\nand Machine Learning (ML) play a critical role. However,\nsuccessful deployment of ZTNs requires resolving numerous\nsecurity challenges during their implementation. In this sur-\nvey paper, we have conducted a comprehensive review of\nthe security issues and vulnerabilities associated with ZTNs\nand explored potential solutions, with a specific focus on\nAutomated ML (AutoML) technologies and Adversarial ML\n(AML) attacks & defense mechanisms. Two case studies are\npresented in this paper. The first case study focuses on the\ndevelopment of autonomous frameworks to address security\nissues in ZTNs, demonstrating the effectiveness of AutoML\ntechnologies in securing ZTNs. The second case study on the\ncyber-defense exercises of AML attacks has been discussed\nto illustrate the devastating impact of AML attacks, along\nwith potential defense mechanisms. Moreover, open challenges\nand future research directions for ZTN security research have\nbeen discussed, highlighting the need for ongoing research and\ninnovation in this field.\nREFERENCES\n[1] C. De Alwis et al., ”Survey on 6G Frontiers: Trends, Applications, Re-\nquirements, Technologies and Future Research,” IEEE Open J. Commun.\nSoc., vol. 2, pp. 836–886, 2021.\n[2] E. Coronado et al., ”Zero Touch Management: A Survey of Network\nAutomation Solutions for 5G and 6G Networks,” IEEE Commun. Surv.\nTutorials, 2022.\n[3] R. Kumar, P. Kumar, M. Aloqaily, and A. Aljuhani, “Deep-Learning-\nBased Blockchain for Secure Zero Touch Networks,” IEEE Commun.\nMag., vol. 61, no. 2, pp. 96–102, Feb. 2023.\n[4] G. ETSI, ”Zero-touch network and service management (zsm); reference\narchitecture,” Gr. Specif. ETSI GS ZSM, vol. 2, 2019.\n[5] L. Yang, ”Optimized and Automated Machine Learning Techniques To-\nwards IoT Data Analytics and Cybersecurity,” Electron. Thesis Diss. Re-\npos., no. 8734, 2022, [Online]. Available: https://ir.lib.uwo.ca/etd/8734\n[6] M. Liyanage et al., ”A survey on Zero touch network and Service\nManagement (ZSM) for 5G and beyond networks,” J. Netw. Comput.\nAppl., vol. 203, p. 103362, Jul. 2022.\n[7] Q. Yao et al., ”Taking Human out of Learning Applications: A Survey\non Automated Machine Learning,” arXiv, 2018.\n[8] L. Yang and A. Shami, ”IoT Data Analytics in Dynamic Environments:\nFrom An Automated Machine Learning Perspective,” Eng. Appl. Artif.\nIntell., vol. 116, pp. 1–33, 2022.\n[9] X. He, K. Zhao, and X. Chu, ”AutoML: A survey of the state-of-the-art,”\nKnowledge-Based Syst., vol. 212, no. January, p. 106622, 2021.\n[10] L. Yang and A. Shami, ”A Lightweight Concept Drift Detection and\nAdaptation Framework for IoT Data Streams,” IEEE Internet Things\nMag., vol. 4, no. 2, pp. 96–101, 2021.\n[11] C. Benza¨ıd and T. Taleb, ”ZSM Security: Threat Surface and Best\nPractices,” IEEE Netw., vol. 34, no. 3, pp. 124–133, May 2020.\n[12] S. Jayasinghe, Y. Siriwardhana, P. Porambage, M. Liyanage, and M.\nYlianttila, ”Federated Learning based Anomaly Detection as an Enabler\nfor Securing Network and Service Management Automation in Beyond\n5G Networks,” 2022 Jt. Eur. Conf. Networks Commun. 6G Summit,\nEuCNC/6G Summit 2022, pp. 345–350, 2022.\n[13] C. Benza¨ıd and T. Taleb, ”AI for beyond 5G Networks: A Cyber-Security\nDefense or Offense Enabler?,” IEEE Netw., vol. 34, no. 6, pp. 140–147,\nNov. 2020.\n[14] J. Gallego-Madrid, R. Sanchez-Iborra, P. M. Ruiz, and A. F. Skarmeta,\n”Machine learning-based zero-touch network and service management:\na survey,” Digit. Commun. Networks, vol. 8, no. 2, pp. 105–123, Apr.\n2022.\n[15] I. Ashraf, Y. Bin Zikria, S. Garg, Y. Park, G. Kaddoum, and S.\nSingh, ”Zero Touch Networks to Realize Virtualization: Opportunities,\nChallenges, and Future Prospects,” IEEE Netw., vol. 36, no. 6, pp.\n251–259, Dec. 2022.\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n26\n[16] C. Benza¨ıd and T. Taleb, ”AI-Driven Zero Touch Network and Service\nManagement in 5G and Beyond: Challenges and Research Directions,”\nIEEE Netw., vol. 34, no. 2, pp. 186–194, 2020.\n[17] C. Benza¨ıd et al., ”White Paper: Intelligent Security Architecture for 5G\nand Beyond,” INSPIRE-5Gplus, pp. 1–24, 2020.\n[18] L. Yang et al., ”Multi-Perspective Content Delivery Networks Security\nFramework Using Optimized Unsupervised Anomaly Detection,” IEEE\nTrans. Netw. Serv. Manag., vol. 19, no. 1, pp. 686–705, 2022.\n[19] L. Yang, A. Moubayed, and A. Shami, ”MTH-IDS: A Multitiered Hybrid\nIntrusion Detection System for Internet of Vehicles,” IEEE Internet\nThings J., vol. 9, no. 1, pp. 616–632, 2022.\n[20] H. P. D. Nguyen and R. Zolt´an, ”The Current Security Challenges of\nVehicle Communication in the Future Transportation System,” SISY 2018\n- IEEE 16th Int. Symp. Intell. Syst. Informatics, Proc., pp. 161–165,\n2018.\n[21] R. Khan, P. Kumar, D. N. K. Jayakody, and M. Liyanage, ”A Survey on\nSecurity and Privacy of 5G Technologies: Potential Solutions, Recent\nAdvancements, and Future Directions,” IEEE Commun. Surv. Tutorials,\nvol. 22, no. 1, pp. 196–248, Jan. 2020.\n[22] ETSI,\n”Zero-touch\nnetwork\nand\nService\nManagement\n(ZSM);\nReference\nArchitecture,”\nGS\nZSM\n002\n-\nV1.1.1,\n2019,\nAccessed:\nDec.\n16,\n2022.\n[Online].\nAvailable:\nhttps://portal.etsi.org/TB/ETSIDeliverableStatus.aspx\n[23] P. Porambage, G. G¨ur, D. P. M. Osorio, M. Liyanage, A. Gurtov, and\nM. Ylianttila, ”The Roadmap to 6G Security and Privacy,” IEEE Open\nJ. Commun. Soc., vol. 2, pp. 1094–1122, 2021.\n[24] H. A. Kholidy et al., ”Toward Zero Trust Security IN 5G Open Archi-\ntecture Network Slices,” MILCOM 2022 - 2022 IEEE Mil. Commun.\nConf., pp. 577–582, Nov. 2022.\n[25] A. Moubayed, A. Shami, and A. Al-Dulaimi, ”On End-to-End Intelligent\nAutomation of 6G Networks,” Futur. Internet, vol. 14, no. 6, p. 165, May\n2022.\n[26] O. Yurekten and M. Demirci, ”SDN-based cyber defense: A survey,”\nFutur. Gener. Comput. Syst., vol. 115, pp. 126–149, Feb. 2021.\n[27] C. Yoon et al., ”Flow Wars: Systemizing the Attack Surface and\nDefenses in Software-Defined Networks,” IEEE/ACM Trans. Netw., vol.\n25, no. 6, pp. 3514–3530, Dec. 2017.\n[28] ETSI, ”Network Functions Virtualisation (NFV); NFV Security; Prob-\nlem Statement,” GS NFV-SEC 001 - V1.1.1 - 2014, Accessed: Apr.\n24, 2023. [Online]. Available: https://www.etsi.org/deliver/etsi gs/nfv-\nsec/001 099/001/01.01.01 60/gs nfv-sec001v010101p.pdf\n[29] V. L. Nguyen, P. C. Lin, B. C. Cheng, R. H. Hwang, and Y. D. Lin,\n”Security and Privacy for 6G: A Survey on Prospective Technologies\nand Challenges,” IEEE Commun. Surv. Tutorials, vol. 23, no. 4, pp.\n2384–2428, 2021.\n[30] M. Friesen, L. Wisniewski, and J. Jasperneite, ”Machine Learning\nfor Zero-Touch Management in Heterogeneous Industrial Networks-A\nReview,” IEEE Int. Work. Fact. Commun. Syst. - Proceedings, WFCS,\nvol. 2022-April, 2022.\n[31] N. Afshan and R. K. Rout, ”Machine Learning Techniques for IoT Data\nAnalytics,” Big Data Anal. Internet Things, pp. 89–113, 2021.\n[32] L. Yang and A. Shami, ”LCCDE: A Decision-Based Ensemble Frame-\nwork for Intrusion Detection in The Internet of Vehicles,” in 2022 IEEE\nGlob. Commun. Conf. GLOBECOM 2022 - Proc., 2022, pp. 1–6.\n[33] L. Yang, ”Comprehensive Visibility Indicator Algorithm for Adaptable\nSpeed Limit Control in Intelligent Transportation Systems,” Thesis,\nUniversity of Guelph, 2018.\n[34] L. Yang and A. Shami, ”On hyperparameter optimization of machine\nlearning algorithms: Theory and practice,” Neurocomputing, vol. 415,\npp. 295–316, 2020.\n[35] S. M. Tahsien, H. Karimipour, and P. Spachos, ”Machine learning based\nsolutions for security of Internet of Things (IoT): A survey,” J. Netw.\nComput. Appl., vol. 161, p. 102630, 2020.\n[36] L. Yang and A. Shami, ”A Transfer Learning and Optimized CNN\nBased Intrusion Detection System for Internet of Vehicles,” in 2022\nIEEE International Conference on Communications (ICC), 2022, pp.\n1–6.\n[37] M. Mohammadi, A. Al-Fuqaha, S. Sorour, and M. Guizani, ”Deep\nLearning for IoT Big Data and Streaming Analytics: A Survey,” IEEE\nCommun. Surv. Tutorials, vol. 20, no. 4, pp. 2923–2960, 2018.\n[38] A. Khattab and N. Youssry, ”Machine Learning for IoT Systems,” in\nInternet of Things (IoT): Concepts and Applications, Springer Interna-\ntional Publishing, 2020, pp. 105–127.\n[39] K. Chauhan et al., ”Automated Machine Learning: The New Wave of\nMachine Learning,” 2nd Int. Conf. Innov. Mech. Ind. Appl. ICIMIA 2020\n- Conf. Proc., no. Icimia, pp. 205–212, 2020.\n[40] J. Giovanelli, B. Bilalli, and A. Abell´o, ”Effective data pre-processing\nfor AutoML,” CEUR Workshop Proc., vol. 2840, pp. 1–10, 2021.\n[41] E.\nLaw,\n”Impyute\nDocumentation,”\n2017.\n[Online].\nAvailable:\nhttps://buildmedia.readthedocs.org/media/pdf/impyute/latest/impyute.pdf\n[42] H. He, Y. Bai, E. A. Garcia, and S. Li, ”ADASYN: Adaptive synthetic\nsampling approach for imbalanced learning,” Proc. Int. Jt. Conf. Neural\nNetworks, pp. 1322–1328, 2008.\n[43] Z. Chen et al., ”Machine learning based mobile malware detection using\nhighly imbalanced network traffic,” Inf. Sci. (Ny), vol. 433–434, pp.\n346–364, 2018.\n[44] A. Pandey and A. Jain, ”Comparative Analysis of KNN Algorithm using\nVarious Normalization Techniques,” Int. J. Comput. Netw. Inf. Secur.,\nvol. 9, no. 11, pp. 36–42, 2017.\n[45] F. Horn, R. Pack, and M. Rieger, “The autofeat python library for\nautomated feature engineering and selection,” Commun. Comput. Inf.\nSci., vol. 1167 CCIS, pp. 111–120, 2020.\n[46] H. Eldeeb, S. Amashukeli, and R. El Shawi, ”An Empirical Analy-\nsis of Integrating Feature Extraction to Automated Machine Learning\nPipeline,” in Pattern Recognition. ICPR International Workshops and\nChallenges, 2021, pp. 336–344.\n[47] M. A. Z¨oller and M. F. Huber, ”Benchmark and Survey of Automated\nMachine Learning Frameworks,” J. Artif. Intell. Res., vol. 70, no. 1993,\npp. 409–472, 2021.\n[48] T. Thaher, M. Mafarja, H. Turabieh, P. A. Castillo, H. Faris, and\nI. Aljarah, ”Teaching Learning-Based Optimization With Evolutionary\nBinarization Schemes for Tackling Feature Selection Problems,” IEEE\nAccess, vol. 9, pp. 41082–41103, 2021.\n[49] A. Bauer, M. Z¨ufle, N. Herbst, A. Zehe, A. Hotho, and S. Kounev, ”Time\nSeries Forecasting for Self-Aware Systems,” Proc. IEEE, vol. 108, no.\n7, pp. 1068–1093, 2020.\n[50] A. L’Heureux, K. Grolinger, H. F. Elyamany, and M. A. M. Capretz,\n”Machine Learning with Big Data: Challenges and Approaches,” IEEE\nAccess, vol. 5, pp. 7776–7797, 2017.\n[51] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown, ”Auto-\nWEKA: Combined selection and hyperparameter optimization of clas-\nsification algorithms,” Proc. ACM SIGKDD Int. Conf. Knowl. Discov.\nData Min., vol. Part F1288, pp. 847–855, 2013.\n[52] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang, ”Learning under\nConcept Drift: A Review,” IEEE Trans. Knowl. Data Eng., vol. 31, no.\n12, pp. 2346–2363, 2019.\n[53] L. Yang and A. Shami, ”A Multi-Stage Automated Online Network\nData Stream Analytics Framework for IIoT Systems,” IEEE Trans. Ind.\nInformatics, vol. 19, no. 2, pp. 2107–2116, 2023.\n[54] A. Bifet and R. Gavald`a, ”Learning from time-changing data with\nadaptive windowing,” Proc. 7th SIAM Int. Conf. Data Min., pp. 443–448,\n2007.\n[55] P. Vorburger and A. Bernstein, ”Entropy-based concept shift detection,”\nProc. - IEEE Int. Conf. Data Mining, ICDM, pp. 1113–1118, 2006.\n[56] L. Yang, D. M. Manias, and A. Shami, ”PWPAE: An Ensemble\nFramework for Concept Drift Adaptation in IoT Data Streams,” in IEEE\nGlobal Communications Conference (GlobeCom), 2021, pp. 1–6.\n[57] V. Losing, B. Hammer, and H. Wersing, “KNN classifier with self\nadjusting memory for heterogeneous concept drift,” Proc. - IEEE Int.\nConf. Data Mining, ICDM, vol. 1, pp. 291–300.\n[58] A. S. Iwashita and J. P. Papa, ”An Overview on Concept Drift Learning,”\nIEEE Access, vol. 7, no. Section III, pp. 1532–1547, 2019.\n[59] J. Gama, I.\nˇZliobaitundefined, A. Bifet, M. Pechenizkiy, and A.\nBouchachia, ”A Survey on Concept Drift Adaptation,” ACM Comput.\nSurv., vol. 46, no. 4, Mar. 2014.\n[60] M. ˙Zarkowski, ”Adaptive online neural network for face identification\nwith concept drift,” Adv. Intell. Syst. Comput., vol. 323, pp. 703–712,\n2015.\n[61] W. Nick Street and Y. S. Kim, ”A streaming ensemble algorithm (SEA)\nfor large-scale classification,” Proc. Seventh ACM SIGKDD Int. Conf.\nKnowl. Discov. Data Min., pp. 377–382, 2001.\n[62] H. Wang, W. Fan, P. S. Yu, and J. Han, ”Mining concept-drifting data\nstreams using ensemble classifiers,” Proc. ACM SIGKDD Int. Conf.\nKnowl. Discov. Data Min., pp. 226–235, 2003.\n[63] Y. Sun, K. Tang, Z. Zhu, and X. Yao, ”Concept drift adaptation by\nexploiting historical knowledge,” arXiv, 2017.\n[64] H. M. Gomes et al., ”Adaptive random forests for evolving data stream\nclassification,” Mach. Learn., vol. 106, no. 9–10, pp. 1469–1495, 2017.\n[65] H. M. Gomes, J. Read, and A. Bifet, ”Streaming random patches\nfor evolving data stream classification,” Proc. - IEEE Int. Conf. Data\nMining, ICDM, vol. 2019-Novem, no. Icdm, pp. 240–249, 2019.\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n27\n[66] G. Agrafiotis, E. Makri, A. Lalas, K. Votis, D. Tzovaras, and N.\nTsampieris, “A Deep Learning-based Malware Traffic Classifier for\n5G Networks Employing Protocol-Agnostic and PCAP-to-Embeddings\nTechniques,” ACM Int. Conf. Proceeding Ser., pp. 193–194, Jun. 2023.\n[67] O. E. Tayfour, A. Mubarakali, A. E. Tayfour, M. N. Marsono, E. Hassan,\nand A. M. Abdelrahman, “Adapting deep learning-LSTM method using\noptimized dataset in SDN controller for secure IoT,” Soft Comput., pp.\n1–9, May 2023.\n[68] J. He, X. Wang, Y. Song, and Q. Xiang, “A multiscale intrusion\ndetection system based on pyramid depthwise separable convolution\nneural network,” Neurocomputing, vol. 530, pp. 48–59, Apr. 2023.\n[69] A. Singh, J. Amutha, J. Nagar, S. Sharma, and C. C. Lee, “AutoML-ID:\nautomated machine learning model for intrusion detection using wireless\nsensor network,” Sci. Reports 2022 121, vol. 12, no. 1, pp. 1–14, May\n2022.\n[70] M. A. Khan, N. Iqbal, Imran, H. Jamil, and D. H. Kim, “An optimized\nensemble prediction model using AutoML based on soft voting classifier\nfor network intrusion detection,” J. Netw. Comput. Appl., vol. 212, p.\n103560, Mar. 2023.\n[71] L. M. Shekhtman, M. M. Danziger, and S. Havlin, ”Recent advances\non failure and recovery in networks of networks,” Chaos, Solitons &\nFractals, vol. 90, pp. 28–36, Sep. 2016.\n[72] R. Meredith, N. Landsberg, A. Lopez, and R. Dutta, ”Recovering an\nOSPF Network from Malicious Attacks: An Experimental Evaluation of\nRecovery Techniques,” 2018 IEEE Glob. Commun. Conf. GLOBECOM\n2018 - Proc., 2018.\n[73] S. Altamimi, B. Altamimi, D. Cote, and S. Shirmohammadi, ”To-\nward a Superintelligent Action Recommender for Network Operation\nCenters Using Reinforcement Learning,” IEEE Access, vol. 11, pp.\n20216–20229, 2023.\n[74] Y. Siriwardhana, P. Porambage, M. Liyanage, and M. Ylianttila, ”AI\nand 6G security: Opportunities and challenges,” in 2021 Jt. Eur. Conf.\nNetworks Commun. 6G Summit, EuCNC/6G Summit 2021, pp. 616–621,\nJun. 2021.\n[75] Y. Shi, H. Zeng, and T. T. Nguyen, ”Adversarial Machine Learning for\nNetwork Security,” 2019 IEEE Int. Symp. Technol. Homel. Secur. HST\n2019, Nov. 2019.\n[76] Y. E. Sagduyu, Y. Shi, and T. Erpek, ”IoT Network Security from the\nPerspective of Adversarial Deep Learning,” in 2019 16th Annual IEEE\nInternational Conference on Sensing, Communication, and Networking\n(SECON), 2019, pp. 1–9.\n[77] C. Liu, B. Li, Y. Vorobeychik, and A. Oprea, ”Robust linear regression\nagainst training data poisoning,” AISec 2017 - Proc. 10th ACM Work.\nArtif. Intell. Secur. co-located with CCS 2017, pp. 91–102, Nov. 2017.\n[78] Y. Sun, J. Liu, J. Wang, Y. Cao, and N. Kato, ”When Machine Learning\nMeets Privacy in 6G: A Survey,” IEEE Commun. Surv. Tutorials, vol.\n22, no. 4, pp. 2694–2724, 2020.\n[79] N. Papernot, P. Mcdaniel, I. G. Openai, and S. Francisco, ”Transferabil-\nity in Machine Learning: from Phenomena to Black-Box Attacks using\nAdversarial Samples,” arXiv, 2016.\n[80] I. J. Goodfellow, J. Shlens, and C. Szegedy, ”Explaining and Harnessing\nAdversarial Examples,” arXiv, 2015.\n[81] A. Kurakin, G. Brain, I. J. G. Openai, and S. Bengio, ”Adversarial\nexamples in the physical world,” arxiv, 2017.\n[82] M.-I. Nicolae et al., ”Adversarial Robustness Toolbox v1.0.0,” arXiv,\n2018.\n[83] B. Biggio et al., ”Evasion attacks against machine learning at test time,”\nLect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect.\nNotes Bioinformatics), vol. 8190 LNAI, no. PART 3, pp. 387–402, 2013.\n[84] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, and D. Mukhopad-\nhyay, ”Adversarial Attacks and Defences: A Survey,” arXiv, 2018.\n[85] M. A. Khan, N. Iqbal, Imran, H. Jamil, and D. H. Kim, ”An optimized\nensemble prediction model using AutoML based on soft voting classifier\nfor network intrusion detection,” J. Netw. Comput. Appl., vol. 212, p.\n103560, Mar. 2023.\n[86] I. Sharafaldin, A. H. Lashkari, and A. A. Ghorbani, ”Toward Generating\na New Intrusion Detection Dataset and Intrusion Traffic Characteriza-\ntion,” in Proc. Int. Conf. Inf. Syst. Secur. Privacy, 2018, pp. 108–116.\n[87] S. Samarakoon et al., ”5G-NIDD: A Comprehensive Network Intrusion\nDetection Dataset Generated over 5G Wireless Network,””, IEEE Dat-\naport, 2022.\n[88] J. M. Kernbach and V. E. Staartjes, ”Foundations of Machine Learning-\nBased Clinical Prediction Modeling: Part II—Generalization and Over-\nfitting,” Acta Neurochir. Suppl., vol. 134, pp. 15–21, 2022.\n[89] F. Salo, M. Injadat, A. B. Nassif, A. Shami, and A. Essex, ”Data Mining\nTechniques in Intrusion Detection\n[90] F. Pedregosa et al., ”Scikit-learn: Machine Learning in Python,” J.\nMach. Learn. Res., vol. 12, pp. 2825–2830, 2011, [Online]. Available:\nhttp://scikit-learn.sourceforge.net.\n[91] G. Ke et al., ”LightGBM: A highly efficient gradient boosting decision\ntree,” Adv. Neural Inf. Process. Syst., vol. 2017-Decem, no. Nips, pp.\n3147–3155, 2017.\n[92] M. Claesen, J. Simm, and D. Popovic, ”Optunity Documentation,” 2015.\n[Online]. Available: https://optunity.readthedocs.io/en/latest/\n[93] J. Montiel et al., “River: Machine learning for streaming data in python,”\nJ. Mach. Learn. Res., vol. 22, pp. 1–8, 2021.\nSystems: A Systematic Literature Review,” IEEE Access, vol. 6, pp.\n56046–56058, 2018.\n[94] Z. Zhang, W. He, W. Li, and H. Abdous, ”Cybersecurity awareness\ntraining programs: a cost-benefit analysis framework,” Ind. Manag. Data\nSyst., vol. 121, no. 3, pp. 613–636, Jan. 2021\n[95] L. U. Khan, W. Saad, D. Niyato, Z. Han, and C. S. Hong, ”Digital-\nTwin-Enabled 6G: Vision, Architectural Trends, and Future Directions,”\nIEEE Commun. Mag., vol. 60, no. 1, pp. 74–80, Jan. 2022.\n[96] A. Masaracchia, V. Sharma, B. Canberk, O. A. Dobre, and T. Q. Duong,\n”Digital Twin for 6G: Taxonomy, Research Challenges, and the Road\nAhead,” IEEE Open J. Commun. Soc., pp. 1–1, Nov. 2022.\n[97] D. M. Manias, I. Shaer, L. Yang, and A. Shami, “Concept Drift Detection\nin Federated Networked Systems,” in IEEE Global Communications\nConference (GlobeCom), 2021, pp. 1–6.\n[98] I. T. Javed, F. Alharbi, T. Margaria, N. Crespi, and K. N. Qureshi,\n”PETchain: A Blockchain-Based Privacy Enhancing Technology,” IEEE\nAccess, vol. 9, pp. 41129–41143, 2021.\n[99] Z. Zhang, H. Al Hamadi, E. Damiani, C. Y. Yeun, and F. Taher,\n”Explainable Artificial Intelligence Applications in Cyber Security:\nState-of-the-Art in Research,” IEEE Access, vol. 10, pp. 93104–93139,\n2022.\n[100] G. Alicioglu and B. Sun, “A survey of visual analytics for Explainable\nArtificial Intelligence methods,” Comput. Graph., vol. 102, pp. 502–520,\nFeb. 2022.\n[101] D. L. Dutta and S. Bharali, ”TinyML Meets IoT: A Comprehensive\nSurvey,” Internet of Things, vol. 16, p. 100461, Dec. 2021.\n[102] H. J. Escalante, ”Automated Machine Learning – a brief review at the\nend of the early years,” arXiv, pp. 1–17, 2020.\n[103] A. Rawal, J. McCoy, D. B. Rawat, B. M. Sadler, and R. S. Amant,\n”Recent Advances in Trustworthy Explainable Artificial Intelligence:\nStatus, Challenges, and Perspectives,” IEEE Trans. Artif. Intell., vol.\n3, no. 6, pp. 852–866, Dec. 2022.\n[104] B. Li et al., ”Trustworthy AI: From Principles to Practices,” ACM\nComput. Surv., vol. 55, no. 9, p. 177, Jan. 2023.\n[105] A. Barredo Arrieta et al., ”Explainable Artificial Intelligence (XAI):\nConcepts, taxonomies, opportunities and challenges toward responsible\nAI,” Inf. Fusion, vol. 58, pp. 82–115, Jun. 2020.\n[106] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\n”A Survey on Bias and Fairness in Machine Learning,” ACM Comput.\nSurv., vol. 54, no. 6, Jul. 2021.\nLi Yang is currently an Assistant Professor in the\nFaculty of Business and Information Technology\nat Ontario Tech University, and an Adjunct Re-\nsearch Professor in the Department of Electrical\nand Computer Engineering at Western University.\nHe received his Ph.D. in Electrical and Computer\nEngineering from Western University in 2022. He\nwas the vice chair of the IEEE Computer Society,\nLondon Section, Canada, from 2022 to 2023. He\nwas also on the technical program committee for\nIEEE GlobeCom 2023 and 2024, the workshop chair\nfor SMC-IoT 2023, and the technical session chair for IEEE CCECE 2020.\nHis paper and code publications have received thousands of Google Scholar\ncitations and GitHub stars. His research interests include cybersecurity,\nmachine learning, deep learning, AutoML, model optimization, network data\nanalytics, Internet of Things (IoT), intrusion detection, anomaly detection,\nconcept drift, continual learning, and adversarial machine learning. Li Yang\nis also included in Stanford University/Elsevier’s List of the World’s Top 2%\nScientists. He was ranked among the world’s Top 0.5% of researchers in\n’Networking & Telecommunications’ in 2024, and 52nd in Canada.\n\n\nPUBLISHED IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT\n28\nMirna El Rajab received her MESc. in Electrical\nand Computer Engineering from Western University,\nLondon, ON, Canada in 2023. She completed her\nB.E. in Computer Engineering from Lebanese Amer-\nican University (LAU), Byblos, Lebanon, in 2021.\nShe was a research assistant at the Department of\nElectrical and Computer Engineering, LAU, from\nMay 2018 to April 2020. During that time, her work\nrevolved around buffer-aided communication sys-\ntems. Currently, her research interests lie in machine\nlearning, zero-touch networks, and next-generation\nnetworks.\nAbdallah Shami is currently a Professor and Chair\nof the Department of Electrical and Computer Engi-\nneering, Western University, London, ON, Canada,\nwhere he is also the Director of the Optimized Com-\nputing and Communications Laboratory. Dr. Shami\nhas chaired key symposia for the IEEE GLOBE-\nCOM, IEEE International Conference on Commu-\nnications, and IEEE International Conference on\nComputing, Networking and Communications. He\nwas the elected Chair for the IEEE Communications\nSociety Technical Committee on Communications\nSoftware and the IEEE London Ontario Section Chair. He is currently an\nAssociate Editor of the IEEE Transactions on Information Forensics and\nSecurity, IEEE Transactions on Network and Service Management, and IEEE\nCommunications Surveys and Tutorials journals. Dr. Shami is a Fellow of\nIEEE, a Fellow of the Canadian Academy of Engineering (CAE), and a Fellow\nof the Engineering Institute of Canada (EIC).\nSami Muhaidat received his Ph.D. in Electrical\nand Computer Engineering from the University of\nWaterloo, Ontario, in 2006. From 2007 to 2008,\nhe was an NSERC Postdoctoral Fellow in the De-\npartment of Electrical and Computer Engineering\nat the University of Toronto, Canada. From 2008\nto 2012, he served as an Assistant Professor in\nthe School of Engineering Science at Simon Fraser\nUniversity, British Columbia, Canada. Currently, he\nis a Professor and the Associate Dean for Research\nin the College of Computing and Mathematical\nSciences at Khalifa University. He is also an Adjunct Professor at Carleton\nUniversity, Ontario, Canada. Sami’s research interests include advanced digital\nsignal processing techniques for wireless communications, intelligent surfaces,\nmachine learning for communications, optical communications, and multiple-\naccess techniques. He has served in various editorial roles, including as Area\nEditor for the IEEE Transactions on Communications, Guest Editor for the\nIEEE Network special issue on ”Native Artificial Intelligence in Integrated\nTerrestrial and Non-Terrestrial Networks in 6G,” and Guest Editor for the\nIEEE Open Journal of Vehicular Technology (OJVT) special issue on ”Recent\nAdvances in Security and Privacy for 6G Networks.” Additionally, he has\nheld positions as Senior Editor and Editor for IEEE Communications Letters,\nEditor for the IEEE Transactions on Communications, and Associate Editor\nfor the IEEE Transactions on Vehicular Technology.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21286v1.pdf",
    "total_pages": 28,
    "title": "Enabling AutoML for Zero-Touch Network Security: Use-Case Driven Analysis",
    "authors": [
      "Li Yang",
      "Mirna El Rajab",
      "Abdallah Shami",
      "Sami Muhaidat"
    ],
    "abstract": "Zero-Touch Networks (ZTNs) represent a state-of-the-art paradigm shift\ntowards fully automated and intelligent network management, enabling the\nautomation and intelligence required to manage the complexity, scale, and\ndynamic nature of next-generation (6G) networks. ZTNs leverage Artificial\nIntelligence (AI) and Machine Learning (ML) to enhance operational efficiency,\nsupport intelligent decision-making, and ensure effective resource allocation.\nHowever, the implementation of ZTNs is subject to security challenges that need\nto be resolved to achieve their full potential. In particular, two critical\nchallenges arise: the need for human expertise in developing AI/ML-based\nsecurity mechanisms, and the threat of adversarial attacks targeting AI/ML\nmodels. In this survey paper, we provide a comprehensive review of current\nsecurity issues in ZTNs, emphasizing the need for advanced AI/ML-based security\nmechanisms that require minimal human intervention and protect AI/ML models\nthemselves. Furthermore, we explore the potential of Automated ML (AutoML)\ntechnologies in developing robust security solutions for ZTNs. Through case\nstudies, we illustrate practical approaches to securing ZTNs against both\nconventional and AI/ML-specific threats, including the development of\nautonomous intrusion detection systems and strategies to combat Adversarial ML\n(AML) attacks. The paper concludes with a discussion of the future research\ndirections for the development of ZTN security approaches.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}