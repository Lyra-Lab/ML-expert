{
  "id": "arxiv_2502.20681v1",
  "text": "Disentangling Feature Structure: A Mathematically Provable\nTwo-Stage Training Dynamics in Transformers\nZixuan Gong\nGaoling School of Artificial Intelligence\nRenmin University of China\nzxgong@ruc.edu.cn\nJiaye Teng∗\nSchool of Statistics and Management\nShanghai University of Finance and Economics\ntengjiaye@sufe.edu.cn\nYong Liu*\nGaoling School of Artificial Intelligence\nRenmin University of China\nliuyonggsai@ruc.edu.cn\nAbstract\nTransformers may exhibit two-stage training dynamics during the real-world training pro-\ncess. For instance, when training GPT-2 on the Counterfact dataset, the answers progress\nfrom syntactically incorrect to syntactically correct to semantically correct. However, existing\ntheoretical analyses hardly account for this two-stage phenomenon. In this paper, we theoreti-\ncally demonstrate how such two-stage training dynamics occur in transformers. Specifically,\nwe analyze the dynamics of transformers using feature learning techniques under in-context\nlearning regimes, based on a disentangled two-type feature structure. Such disentanglement of\nfeature structure is general in practice, e.g., natural languages contain syntax and semantics,\nand proteins contain primary and secondary structures. To our best known, this is the first\nrigorous result regarding a two-stage optimization process in transformers. Additionally, a\ncorollary indicates that such a two-stage process is closely related to the spectral properties of\nthe attention weights, which accords well with empirical findings 1.\n1\nIntroduction\nTransformers (Vaswani et al., 2017) have emerged as foundational architectures with broad applica-\ntions across multiple research domains, such as natural language processing (Kenton and Toutanova,\n2019; Radford et al., 2019), computer vision (Liu et al., 2021; He et al., 2022), etc. Recently,\nlarge language models (LLM) based on decoder-only transformer architectures further demonstrate\nimpressive capabilities, excelling in various downstream tasks (Brown et al., 2020; Chowdhery\net al., 2023; OpenAI, 2023). However, it remains an essential issue to delve into why LLMs exhibit\n*Corresponding author.\n1Our code is available at https://github.com/zx-gong/Two-Stage-Dynamics.\n1\narXiv:2502.20681v1  [cs.CL]  28 Feb 2025\n\n\nThe mother tongue of Thomas Joannes Stieltjes is\nQuestion\nAnswer\nPrediction\nDutch\nThe\nThe larvae feed on turf grasses and corn stalks.\nThe language of Likkutei Sichos was\nHebrew\na\nT=1\nHe became the third abbot on January 6, 1972.\nKiev-Sviatoshyn Raion, which has the capital city\nKiev\nof\nThe mother tongue of Thomas Joannes Stieltjes is\nDutch\nFrench\nThe larvae feed on turf grasses and corn stalks.\nThe language of Likkutei Sichos was\nHebrew\nFinnish\nIt is found in the Indomalayan realm. Percy Snow\nplays in the position of\nlinebacker\nquarterback\nThe mother tongue of Danielle Darrieux is\nFrench\nFrench\nThe larvae feed on turf grasses and corn stalks.\nThe language of Likkutei Sichos was\nHebrew\nHebrew\nBattle of Chacabuco is located in\nSantiago\nSantiago\nSyntax\nSemantics\nT=5\nSyntax\nSemantics\nT=100\nSyntax\nSemantics\nElementary Stage\nSpecialized Stage\nT=1\nT=5\nT=100\nFigure 1: Two-stage Learning of Syntactic and Semantic Information on Counterfact Dataset.\nsuch remarkable performance. Fortunately, exploring the optimization dynamics in transformers\npresents a promising approach for investigating the possible factors that contribute to this behavior.\nMany scholars have theoretically delved into the optimization dynamics in supervised learning or\nlanguage tasks by studying gradient flow or attention maps (Deora et al., 2023; Li et al., 2023a;\nTian et al., 2023a,b; Zhang et al., 2023; Huang et al., 2023; Cheng et al., 2023; Chen et al., 2024).\nHowever, there is less consideration of feature structure, which might be crucial to inducing a\nrealistic optimization process. Surprisingly, we observe that transformers may exhibit two-stage\ntraining dynamics during practical learning, induced by a two-type feature structure.\nFor instance, when fine-tuning GPT-2 on the Counterfact dataset in Figure 1 (more details in\nAppendix B.2), we observe the following phenomenon: at the initial time (epoch 1), most predictions\nare both syntactically and semantically incorrect. By epoch 5, we observe a significant decrease in\ntraining loss; all predictions meet syntactic requirements, but most remain semantically incorrect\nand inconsistent with the true answers. By epoch 100, all predictions are syntactically correct, with\nmost being semantically correct and achieving a small training loss. Overall, the model’s answers\nprogress from syntactically incorrect to syntactically correct to semantically correct, exhibiting\ntwo-stage training dynamics for syntactic and semantic information.\nMotivated by this phenomenon, for various tasks like language tasks, protein structure prediction\ntasks, or classic supervised learning tasks, we can disentangle feature structure into two types:\nelementary knowledge (like syntactic information), and specialized knowledge (like semantic infor-\nmation). Such disentanglement and the corresponding two-stage learning process are empirically\ngeneral in both NLP (Bao et al., 2019; Chen et al., 2019a; Huang et al., 2021) and biological\nresearch, such as AlphaFold (AlQuraishi, 2019; Jumper et al., 2021).\nBased on the above discussion, it is natural to infer that knowledge may be acquired following\nelementary-then-specialized principles. However, this leaves a critical theoretical question:\n2\n\n\nHow does the disentangled two-type feature structure induce two-stage\ntraining dynamics in transformers?\nTo demystify the training dynamics of transformers, we adopt in-context learning (ICL) regimes,\nconstructing training prompts with independently and identically distributed (i.i.d.) in-context\nsamples to study supervised classification tasks. As is well-known, ICL (Brown et al., 2020) has\nemerged as a remarkable ability in LLMs, where the model solves new tasks based on prompts\nwithout further parameter fine-tuning (Black et al., 2022; Rae et al., 2021). This ability has served\nas the foundation for developing more advanced prompting techniques to tackle complex problems\n(Huang and Chang, 2022). Recent theoretical studies mainly focus on the setting where the training\nand test prompts are embedded as sequences of labeled training samples and an unlabeled query,\nwhere transformers can mimic the behavior of supervised learning algorithms (Aky¨urek et al.,\n2022; Zhang et al., 2023; Huang et al., 2023; Cheng et al., 2023; Chen et al., 2024). This prompt-\nembedding method, the so-called ICL regime, enables theoretical analysis of attention mechanisms\nin supervised learning tasks.\nIn this paper, we derive a rigorous two-stage optimization process where transformers first master\nelementary knowledge and then unlock specialized knowledge. Simultaneously, we investigate how\ntransformer weights evolve over time, explore the convergence theory, and examine the spectral\ncharacteristics of attention weights. Our main contributions are summarized as follows:\n(a) Feature Disentangling with Feature Learning.\nBased on the above discussion, we disen-\ntangle the feature structure into two key types: elementary knowledge, and specialized knowledge.\nFurthermore, we proceed with theoretical abstraction in Section 3, presenting a general framework\nthat might potentially contribute to further explorations in transformer-based learning paradigms.\n(b) Mathematical Proof for Two-Stage Learning. Based on the underlying feature structure, to\nour best knowledge, this is the first paper presenting rigorous proofs for the two-stage learning\nprocess in transformers, distinguishing between the initial stage of mastering elementary knowledge\nand the subsequent stage of acquiring specialized knowledge (Detailed proof in Section D.3 ∼D.6).\n(c) Optimization Trajectory and Convergence Analysis. We present optimization trajectory and\nfinite-time convergence analysis in Section 4, providing deeper insights into the two-stage learning\nprocess. Specifically, by adopting feature learning and signal-noise decomposition techniques, we\ngive key propositions and lemmas in Appendix D.2, discussing the impact of signal or noise weights\non network output computations.\n(d) Extensions in Spectral Characteristics of Attention Weights. We further discuss spectral\ncharacteristics of attention weights in Section 4.3, highlighting the close relationship with the\ntwo-stage process. This theoretical finding aligns with experimental observations, demonstrating\nthat smaller eigenvalues preserve elementary knowledge, while larger eigenvalues allow the model\nto progressively acquire specialized knowledge.\n3\n\n\n2\nRelated work\nOptimization Analysis under ICL Regimes.\nThe optimization analysis under ICL regimes can\nbe roughly split into two branches. The first branch examines whether the global minimum of ICL\nloss can be reached through gradient flow across different models and tasks (Zhang et al., 2023;\nCheng et al., 2023; Zheng et al., 2024; Shen et al., 2024). However, this branch focuses less on how\nthe model weights are optimized and updated throughout training. Additionally, this line hardly\naddresses finite-time convergence or the distinct stages of learning various types of information.\nThe second branch further analyzes the optimization properties during training (Huang et al., 2023;\nChen et al., 2024; Kim and Suzuki, 2024). Of the most relevance here is Huang et al. (2023)\nwhich derives stage-wise learning of attention maps under linear regression tasks with unbalanced\nfeatures. Our work differs from Huang et al. (2023) in two aspects: (a) the stage-wise phenomenon\ncomes from the disentangled feature structure; (b) we focus on nonlinear classification tasks. In\nsummary, finite-time training dynamics of transformers remain relatively unexplored, especially\nwhen attempting to illustrate the optimization process induced by the disentangled two-type feature\nstructure (elementary knowledge and specialized knowledge).\nOptimization Analysis of Transformers without ICL Regimes.\nThere is a line of work analyzing\nthe training dynamics of transformers without ICL regimes, e.g., multi-head attention layer under\nbinary classification regimes (Deora et al., 2023), ViT under classification regimes (Li et al., 2023b),\none-layer transformers (Tian et al., 2023a,b).\nFeature Learning.\nFeature learning is among the most popular approaches in optimization\ntheory (Allen-Zhu and Li, 2020, 2022; Wen and Li, 2021; Li et al., 2023a), which aims to analyze\nthe training behavior under specific data generation models. Compared to other techniques like\nNeural Tangent Kernels (Jacot et al., 2018; Li and Liang, 2018; Allen-Zhu et al., 2019; Chen et al.,\n2019b; Du et al., 2019), feature learning approaches go beyond the lazy training regimes and allow\ncapturing the intrinsic interaction between different features and neural network dynamics.\n3\nProblem Setup\nThis section presents the details of the data, model, and training procedure. Concretely, Section 3.1\ndesigns the individual token feature structure and constructs training prompts following ICL regimes.\nSection 3.2 introduces a one-layer attention-based model and two virtual networks. Finally, Sec-\ntion 3.3 describes the corresponding loss function and optimization algorithm used for classification\ntasks.\nNotations.\nLet ∥A∥F be the Frobenius norm for matrix A and ∥x∥2 be the 2-norm for vector\nx. For vector x, ReLU(x) = max{x, 0} denotes the standard ReLU activation function, and 1(x)\ndenotes a binary vector that takes entries 1 when xi ≥0. The indicator function I(·) ∈{−1, 1} is\ndefined such that it takes value 1 if the condition is satisfied, and −1 otherwise. For order analysis,\nPoly(·) represents polynomial order, f(n) = O(g(n)) indicates that f(n) is asymptotically bounded\nabove by g(n), and f(n) = Θ(g(n)) means that f(n) and g(n) are of the same asymptotic order.\nAdditionally, throughout the paper, let U ∈R2d×2d denote a weight matrix, and W ∈Rd×d, V ∈\nRd×d denote the principal submatrices of U which will be defined later.\n4\n\n\nFundamentary\nKnowledge\nSpecialized\nKnowledge\nFeature Space\nVirtual Network \nNormalized ReLU \nSelf-Attention Layer\nQuery\nKey\nValue\nSource Sequence \nEmbedding\nPredicted label\nVirtual Network \nNormalized ReLU \nSelf-Attention Layer\nSource Sequence \nEmbedding\nPredicted label\nNonlinear Classification\nLinear Classification\nPositive \nNegative\nPositive \nNegative\nMargin \ncomponent \ncomponent \nFigure 2: Overview of Disentangled Feature Structure.\n3.1\nDisentangled Feature Structure\nIn this section, we mainly discuss the disentangled token feature structure, starting from the well-\nestablished ICL regimes (Garg et al., 2022). Under ICL regimes, a collection of samples and their\ncorresponding labels are organized in a sequence, commonly referred to as a prompt.\nTraining Prompt Structure.\nFollowing the regimes in Garg et al. (2022), ICL is trained on\nN random training prompts, denoted by {P n}n∈[N]. The n-th training prompt is constructed\nas P n =\n\u0000xn\n1, yn\n1 , · · · , xn\nL−1, yn\nL−1, xn\nL\n\u0001\nwith prompt length L, where xn\ni , i ∈[L −1] denotes\nthe input samples, yn\ni , i ∈[L −1] denotes the corresponding labels, and xn\nL denotes the query.\nAssume that xn\ni , i ∈[L −1] are i.i.d. drawn, and consider a binary classification setting with\nyn\ni = y(xn\ni ) ∈{−1, 1}. The goal of the ICL learner is to train a model f(·), such that the output\napproximates the label of the query xn\nL, namely,\nf(P n) ≈yn\nL = y(xn\nL).\nIndividual Token Feature Structure.\nAs illustrated in Figure 2, each individual token xn\ni in\nthe prompt P n is disentangled into two types: P component represents elementary knowledge\n(e.g., syntactic information in natural languages, primary structure in protein), and Q component\nrepresents specialized knowledge (e.g., semantic information in natural languages, secondary\nstructure in protein).\nSpecifically, consider a disentangled feature structure xn\ni = [xn\ni,1, xn\ni,2]⊤∈R2d, where xn\ni,1 ∈Rd\ndenotes the elementary knowledge drawn from distribution P and xn\ni,2 ∈Rd denotes the specialized\nknowledge drawn from distribution Q. We construct the distributions P and Q as follows, drawing\ninspirations from Li et al. (2019):\n5\n\n\n• For distribution P, given a fixed vector w⋆and a random vector ei ∼N\n\u0010\n0, Id×d\nd\n\u0011\n, the data\n(xn\ni,1, yn\ni,1) is constructed by\nyn\ni,1 = I(⟨w⋆, ei⟩≥0) ∈{−1, 1};\nxn\ni,1 = yn\ni,1γ0w⋆+ ei.\nSuch construction guarantees its linear separability with the classifier w⋆with a margin of\n2γ0∥w⋆∥2. Without loss of generality, assume that ∥w⋆∥2 = 1 and γ0 =\n1\n√\nd.\n• For distribution Q, given the label yn\ni,1 ∈{−1, 1} a scalar α ∈R, and two vectors ζ, z ∈Rd,\nthe data (xn\ni,2, yn\ni,2) is constructed by\nyn\ni,2 = yn\ni,1;\nxn\ni,2 = αz if yn\ni,2 = 1;\nxn\ni,2 ∼Unif ({α(z −ζ), α(z + ζ)}) if yn\ni,2 = −1.\nDifferent from distribution P, this distribution is not linear separable due to the construction\nof xn\ni,2. Without loss of generality, assume that α = 1, ∥z∥2 = u, ∥ζ∥2 = r ≪u and\n⟨z, ζ⟩= 0.\nOverall, distributions P and Q represent two types of components. P represents the elementary\nknowledge, e.g., the knowledge required to master syntax; and Q represents the specialized knowl-\nedge, e.g., the knowledge required to unlock semantics. Notably, mastering syntax is typically\nmuch easier than unlocking semantics. Fortunately, the above construction implies this in the sense\nthat fitting the elementary distribution P (linear separable) is easier than fitting the specialized\ndistribution Q (not linear separable). We finally remark that this data construction remains a highly\ncomplex non-linear task, despite the simple concatenation. Figure 8 in Appendix C.1 intuitively\nillustrates the complexity of the task utilizing two-dimensional data.\nEmbeddings.\nTo simplify the presentation, we denote the embedding matrix by stacking xn\ni or\nyn\ni . Specifically, for the feature embedding, denote\nXn\n1 =\n\u0002xn\n1,1\nxn\n2,1\n· · ·\nxn\nL,1\n\u0003\n∈Rd×L,\nXn\n2 =\n\u0002xn\n1,2\nxn\n2,2\n· · ·\nxn\nL,2\n\u0003\n∈Rd×L.\nBesides, to ensure the model output is linearly decomposable, we combine X1 and X2 to form the\ncomplete feature embedding matrix as Xn =\n\u0014Xn\n1\n0\n0\nXn\n2\n\u0015\n∈R2d×2L. Similarly, define the label\nembedding as\nY n\n1 = Y n\n2 ≜Y n =\n\u0002\nyn\n1\nyn\n2\n· · ·\n0\n\u0003\n∈R1×L,\nand the complete label embedding as eY n =\n\u0002\nY n\nY n\u0003\n∈R1×2L.\n6\n\n\n3.2\nOne-Layer Transformer Architecture\nThis section introduces the notations of the one-layer transformer, including the normalized ReLU\nself-attention layer and transformer weight structure.\nNormalized ReLU Self-Attention Layer.\nA self-attention layer (Vaswani et al., 2017) in the\nsingle-head case includes parameters θ: key, query, and value matrices WK, WQ ∈R2d×2d, WV ∈\nR2L×2L. Given the feature embedding matrix X ∈R2d×2L, we use a normalized ReLU activation in\nplace of standard softmax activation as Bai et al. (2024). Then the prediction for query xL using a\none-layer transformer is given by\nf(U; X, eY ) = eY WV · 1\n2LReLU\n\u0000X⊤W ⊤\nKWQxL\n\u0001\n= eY /2L · ReLU\n\u0000X⊤UxL\n\u0001\n,\n(1)\nwhere\n1\n2L is the normalization factor. To simplify, we reparameterize W ⊤\nKWQ ≜U ∈R2d×2d and\nassume the value matrix is the identity transformation, i.e., WV = I.\nNotably, transformers with sequence-length normalized ReLU activations have been experimentally\nstudied in Wortsman et al. (2023); Shen et al. (2023), achieving faster speed while demonstrating\ncomparable performances to standard softmax activation in many vision and NLP tasks.\nTransformer Weight Structure.\nGiven that individual samples xn\ni can be characterized by two\nspecific types of features, we abstract the real training network into two virtual networks, with the\nweight matrix composed of two distinct parts. To simplify our analysis, we consider the simplest\nstructure of weight U as a block diagonal matrix:\nU =\n\u0014W\n0\n0\nV\n\u0015\n∈R2d×2d,\nwhere weight W operates only on X1 and V operates only on X2. This structure exhibits a strong\nproperty of linear decomposability over the model output, i.e., by disentangling, the two new\npredictions with features X1 and X2 maintain a similar formulation to the original ones:\nf(U; X, eY )\n|\n{z\n}\nNU(U;X,eY )\n=\n1/2 · Y/L · ReLU\n\u0000X⊤\n1 WxL,1\n\u0001\n|\n{z\n}\nNW (W;X1,Y ) or h(X1)\n+1/2 · Y/L · ReLU\n\u0000X⊤\n2 V xL,2\n\u0001\n|\n{z\n}\nNV (V ;X2,Y ) or g(X2)\n.\n(2)\nIn summary, we naturally abstract two virtual networks: network h(X1) with parameter W operates\non X1 part to learn component P, and network g(X2) with parameter V operates on X2 part to\nlearn component Q. The overview is shown in Figure 2.\n3.3\nTraining Procedure\nLoss Function.\nTo train the transformer model on binary classification tasks, we consider the\nregularized empirical loss over N training prompts. Denote the logistic loss for each prompt as\nl(f(U; Xn, eY n)) = log(1 + e−yn\nLf(U;Xn,eY n)), then\nbL(U) = 1\nN\nN\nX\nn=1\nl\n\u0010\nf(U; Xn, eY n)\n\u0011\n,\n(3)\n7\n\n\nand the regularized loss is denoted as bLλ(U) = bL(U) + λ\n2∥U∥2\nF, where λ denotes the L2 regulariza-\ntion coefficient.\nOptimization Algorithm.\nConsider stochastic gradient descent with spherical Gaussian noise,\nwhich is a simplification of minibatch SGD. Taking initial weight [U0]ij ∼N (0, τ 2\n0 ) and noise\n[ξt]ij ∼N\n\u00000, τ 2\nξ\n\u0001\n, then the update of U with time is represented as\nUt+1 = Ut −γt∇U(bLλ(Ut) + ξt) = (1 −γtλ)Ut −γtξt −γt∇U bL(Ut).\n(4)\nSignal-noise Decomposition.\nWith noise in SGD optimization, we take signal-noise decompo-\nsition for weight U, i.e., U = U + eU (Allen-Zhu et al., 2019; Li et al., 2019). The signal weight\nis defined as the weights related to the gradient part, i.e., U t+1 ≜(1 −γtλ)U t −γt∇U bL(Ut). And\nthe noise weight is defined as the weights related to the noise part, i.e., eUt+1 ≜(1 −γtλ)eUt −γtξt.\nNote that due to Equation 4, such decomposition is always valid.\nNotably, the noise component eU follows a Gaussian distribution since it is a linear combination of\nGaussian random variables. By setting a relatively small variance τ 2\nξ , the signal component always\ndominates the noise component (Li et al., 2019). Therefore, one can always rewrite the weight\nU = U + eU as a signal part U with a small Gaussian random noise eU. Based on this observation,\nwe define the training loss K(U) which depends solely on the signal weight:\nK(U) = 1\nN\nN\nX\nn=1\nl\n\u0010\nNU(U + eU; Xn, eY n)\n\u0011\n.\n(5)\nBased on the above discussions, minimizing Equation 5 is almost equivalent to minimizing Equa-\ntion 3. Similarly, we take signal-noise decomposition for W = W + f\nW and V = V + eV , then\ndefine the training loss of linear separable component P over signal weight as K1(W), and the\ntraining loss of nonlinear separable component Q over signal weight as K2(V ):\nK1(W) = 1\nN\nN\nX\nn=1\nl\n\u0010\nNW(W + f\nW; Xn\n1 , Y n)\n\u0011\n, K2(V ) = 1\nN\nN\nX\nn=1\nl\n\u0010\nNV (V + eV ; Xn\n2 , Y n)\n\u0011\n.\n(6)\n4\nTwo-stage Optimization of Transformers\nBased on the data characteristics and the different learning complexity of component P and Q, we\nsplit the entire training process into two stages: the Elementary Stage (in Section 4.1, Theorem 4.2\nand Theorem 4.3), and the Specialized Stage (in Section 4.2, Theorem 4.4 and Theorem 4.5). We\nestablish the weight trajectory and analyze the finite-time convergence in the two stages. The main\ntheorems are summarized in Figure 3. Before diving into the details, we introduce the fundamental\nsettings of two stages, including the learning rate and training iterations. Specially,\n• Elementary Stage. Constant learning rate η1 = Θ(1); Containing 0 ≤t ≤t1 ≜\n1\n4η1λ where\nλ denotes the L2 regularization coefficient.\n• Specialized Stage. Annealing learning rate η2 = η1λ2ϵ2\nV,1r where ϵV,1 = Θ(1/Poly(d)) will\nbe introduced later, and r ≜∥ζ∥2 represents the hardness of specialized knowledge (See\nSection 3.1); Containing t1 ≤t ≤t1 + t2 where t2 ≜log2(1/ϵV,1)\n4η2λϵ2\nV,1 .\n8\n\n\nTheorem 2\nTheorem 3\nTheorem 1\n(c.1) & (c.2)\n1. The model weight  reaches\n.\n2. The loss of \n remains small.\nConclusion: \nThe model \n preserves elementary knowledge \n.\nTheorem 4\n(d.1) & (d.2)\n1. The model weight \n changes small around \n.\n2. The loss of  remains small.\nOverall Training\nInitial time\nIteration  \nIteration  \n(a.1) & (a.2)\n1. The model weight  changes small.\n2. The loss of \n remains large.\nConclusion: \nThe model \n cannot learn specialized knowledge \n.\n(b.1) & (b.2)\n1. The model weight \n reaches \n.\n2. The loss of  remains small.\nElementary Stage\nSpecialized Stage\nConclusion: \nThe model \n learns specialized knowledge \n.\nConclusion: \nThe model \n learns elementary knowledge \n.\nFigure 3: Summary of Two-stage Learning.\nThe annealing learning rate is widely adopted in practical training procedures. Besides, we present\nthe same choices of hyperparameters for two stages in Assumption 4.1.\nAssumption 4.1. Throughout the Theorems, set the variance of initialization parameter τ0 =\nΘ\n\u0010\n1\n√log d\n\u0011\n, regularization coefficient 1\nλ = Θ\n\u0000√log d\n\u0001\nand prompt length L = Θ (Poly(d)) where d\ndenotes the input dimension. We defer more discussions to Appendix C.2.\n4.1\nElementary Stage\nThis section aims to analyze the regime with η1 = Θ(1) and t ≤t1 ≜\n1\n4η1λ. Our goal is to prove that\nthe weights are optimized from U 0 =\n\u0014W 0\n0\n0\nV 0\n\u0015\nto U t1 =\n\u0014W t1 −→W ⋆\n0\n0\nV t1 ≈V 0\n\u0015\n. This means\nthat W t1 approaches the optimal weights W ⋆, while V t1 remains close to V 0. We split the derivation\ninto two theorems: Theorem 4.2 demonstrates that the component Q (specialized knowledge) is not\neffectively learned by network g, and Theorem 4.3 demonstrates that the network h successfully\nlearns the component P (elementary knowledge). We start from Theorem 4.2.\nTheorem 4.2. In the elementary stage with η1 = Θ(1) and t1 =\n1\n4η1λ where λ denotes regularization\ncoefficients. With Assumption 4.1, initial weights V0 −→0d×d and N training prompts, it holds that\n(a.1) For the model parameter V of network g, through gradient descent, ∥V t1∥F satisfies\n∥V t1∥F = Θ\n\u0012\n1\nPoly(d)\n\u0013\n.\n(a.2) With random and small noise weight, the training loss of nonlinear separable component Q\nover signal weight (Definition in Equation 6) at iteration t1 satisfies\n9\n\n\nK2\nt1\n\u0000V t1\n\u0001\n≳log 2 −\n1\n√log d −\nr\nlog d\nN .\nNamely, the nonlinear separable component Q is not efficiently learned by the network g within t1\niterations.\nMessages Behind Theorem 4.2.\nTheorem 4.2 demonstrates that the component Q cannot\nbe effectively learned by the corresponding network g defined in Equation 2. In (a.1), within\nt1 iterations, the weight ∥V t1∥F is approximately in order\n1\nPoly(d), which implies that the model\nweight V is almost not optimized since ∥V t1∥F ≈∥V 0∥F. In (a.2), we provide the lower bound\nfor the training loss of component Q. The value is close to log 2 with a large dimension d and\ntraining prompts N. Overall, the above discussions exhibit that specialized knowledge like Q is not\neffectively learned by the network g. We defer the proof to Appendix D.3 and the proof sketch in\nRemark D.22.\nTheorem 4.3. In the elementary stage with η1 = Θ(1) and t1 =\n1\n4η1λ where λ denotes regularization\ncoefficients. With Assumption 4.1 and initial weights W0 −→0d×d, it holds that there exist ϵW,1 =\nΘ (1/Poly(d)) (See Definition in Equation 16) such that\n(b.1) The model parameter W of network h is optimized by gradient descent within t1 iterations,\n∥W t1∥F = Θ (d log(1/ϵW,1)) ≫∥W 0∥F.\n(b.2) With random and small noise weight, the training loss of linear separable component P over\nsignal weight (Definition in Equation 6) at iteration t1 satisfies\nK1\nt1(W t1) ≲exp(−d log d) +\n1\n√log d.\nNamely, the network h learns the linear separable component P within t1 iterations.\nMessages Behind Theorem 4.3.\nTheorem 4.3 describes how the linear separable component\nP is learned by the corresponding network h defined in Equation 2. In (b.1), within t1 iterations,\n∥W∥F significantly grows from the order ∥W 0∥F ≈\n√\nd to the order ∥W t1∥F ≈d log(1/ϵW,1),\nindicating that the knowledge might be learned. In comparison, V t1 for the component Q changes\nsmall since ∥V t1∥F ≈∥V 0∥F ≈\n1\nPoly(d) (See Theorem 4.2 (a.1)). In (b.2), it shows that the loss\nof linear separable component P is upper bounded by an o(1) term which converges to zero as\nthe dimension d goes to infinity. In comparison, the loss of component Q is lower bounded by a\nconstant close to log 2 (See Theorem 4.2 (a.2)). In summary, the above discussions imply that the\nnetwork h learns elementary knowledge like P, marking the so-called elementary stage. We\ndefer the proof to Appendix D.4 and proof sketch to Remark D.26.\n4.2\nSpecialized Stage\nThis section aims to analyze the regime with η2 = η1λ2ϵ2\nV,1r and t1 ≤t ≤t1 + t2, where ϵV,1 =\nΘ(1/Poly(d)) is defined in Equation 17, t1 ≜\n1\n4η1λ and t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1 . Our goal is to prove that the\n10\n\n\nweights are optimized from U t1 =\n\u0014W t1\n0\n0\nV t1\n\u0015\nto U t1+t2 =\n\u0014W t1+t2 ≈W t1\n0\n0\nV t1+t2 −→V t1 + V ⋆\n\u0015\n.\nIn total, we split the derivation into two theorems: Theorem 4.4 demonstrates that the network g\nlearns specialized knowledge like component Q, and Theorem 4.5 demonstrates that the network h\ncontinues to preserve elementary knowledge like component P. We start from Theorem 4.4.\nTheorem 4.4. In the specialized stage with annealing learning rate η2 = η1λ2ϵ2\nV,1r and t1 ≤t ≤\nt1 + t2, where ϵV,1 = Θ(1/Poly(d)) (See Definition in Equation 17), t1 ≜\n1\n4η1λ, t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1 ,\nλ denotes the L2 regularization coefficient and data noise ∥ζ∥2 = r (See Section 3.1). With\nAssumption 4.1, it holds that\n(c.1) The model parameter V of network g is optimized by gradient descent within t2 iterations,\n∥V t1+t2∥F = Θ\n\u0012log(1/ϵV,1)\nϵV,1\n+\n1\nPoly(d)\n\u0013\n≫∥V t1∥F.\n(c.2) With random and small noise weight, the training loss of nonlinear separable component Q\nover signal weight (Definition in Equation 6) satisfies\nK2\nt1+t2(V t1+t2) ≲exp\n\u0012\n−log(1/ϵV,1)\nϵV,1\n\u0013\n+\n1\n√log d.\nNamely, the network g learns nonlinear separable component Q within t2 iterations.\nMessages Behind Theorem 4.4.\nTheorem 4.4 illustrates the optimization in the specialized stage.\nStatement (c.1) implies that within t2 iterations, ∥V ∥F grows from the order ∥V t1∥F ≈\n1\nPoly(d)\nto the order ∥V t1+t2∥F ≈log(1/ϵV,1)\nϵV,1\n+\n1\nPoly(d) ≈Poly(d) log Poly(d) +\n1\nPoly(d) (derivation based on\nAssumption 4.1). Statement (c.2) implies that the loss is upper bounded by o(1) which converges to\nzero as d goes to infinity. Notably, the upper bound given by the order exp (−Poly(d) log(Poly(d)))+\n1\n√log d. Compared to Theorem 4.2 with constant lower bound, we conclude that with a small learning\nrate, the network g learns specialized knowledge, marking the so-called specialized stage. We\ndefer the proof to Appendix D.5 and the proof sketch in Remark D.28.\nDiscussion on Parameter Orders.\nWe first focus on the learning rate η2 = η1λ2ϵ2\nV,1r. Given the\nchoices in Assumption 4.1, η2 ≈O\n\u0010\nlog d\n(Poly(d))2η1\n\u0011\n. It usually follows that η2 < η1, which accords\nwith practical training. Additionally, the current learning keeps t2 = O\n\u0000Poly(d)(log d)7/2/η1\n\u0001\n,\nwhich is significantly longer than t1 = O\n\u0000√log d/η1\n\u0001\n, coming from the fact that learning special-\nized components is harder than learning elementary components.\nTheorem 4.5. In the specialized stage with annealing learning rate η2 = η1λ2ϵ2\nV,1r and t1 ≤t ≤\nt1 + t2, where ϵV,1 = Θ(1/Poly(d)) (See Definition in Equation 17), t1 ≜\n1\n4η1λ, t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1 ,\nλ denotes the L2 regularization coefficient and data noise ∥ζ∥2 = r (See Section 3.1). With\nAssumption 4.1 and number of training prompts N = Θ (Poly(d)), it holds that\n(d.1) For the model parameter W of network h, through gradient descent optimization from iteration\nt1 to t1 + t2, ∥W t1+t2 −W t1∥F satisfies\n11\n\n\n\r\rW t1+t2 −W t1\n\r\r\nF ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d.\n(d.2) With random and small noise weight, the training loss of linear separable component P over\nsignal weight (Definition in Equation 2) satisfies\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1))\n\f\f ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d.\nNamely, the network h continues to preserve the elementary knowledge like P within t2 iterations.\nMessages Behind Theorem 4.5.\nTheorem 4.5 demonstrates the optimization process on\nthe linear separable part P in the specialized stage, annealing the learning rate from η1 to η2.\nStatement (d.1) demonstrates that the signal weight W does not change significantly in the\nspecialized stage, given the upper bound o(1). Concretely, the upper bound of the weight differ-\nence between two moments is\nϵ2\nV,1\nlog2(1/ϵV,1)\n√log d, with the order of\n1\n(Poly(d))2(log d)5/2. Statement\n(d.2) demonstrates that the loss also does not change much from iteration t1 to t1 + t2, en-\nsuring that the model remains low training loss on linear separable component P. In detail,\nthe small changes in loss have an order of\n1\n(Poly(d))2(log d)5/2. In summary, in the specialized\nstage, the network h continues to preserve the knowledge P acquired during the elemen-\ntary stage. Given that both the changes in signal weight W and the loss are minimal, we also\nconclude that the specialized stage is dedicated exclusively to the learning of nonlinear separa-\nble component Q. We defer the proof to Appendix D.6 and the proof sketch to Remark D.31.\n0\n50\n100\n150\n200\n250\n300\n350\n400\nEpoch\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nComponent P\nComponent Q\nFigure 4: Two-stage Learning of Compo-\nnent P and Q on Theoretical Synthetic\nData. Note: The light lines represent the\noriginal accuracy curve, while the dark\nlines show the smoothed accuracy.\nExperiments Verifying Two-Stage Learning on The-\noretical Synthetic Dataset.\nAs shown in Figure 4,\nexperimental results on the synthetic dataset verify our\ntheoretical findings, where the training dynamics ex-\nhibit a clear two-stage phenomenon. We defer more\ndetails to Appendix B.\n4.3\nExtensions in Spectral Characteristics\nof Attention Weights\nIn this section, we further explore the two-stage phe-\nnomenon on the spectral characteristics of attention\nweights Tr(W) and Tr(V ) in Corollary 4.6, based on\nTheorem 4.2 ∼4.5. Experimental results in Figure 5\naccord with the theoretical findings.\nCorollary 4.6. Under the assumptions in Theorem 4.2 ∼Theorem 4.5, it holds that\n(a) In the elementary stage within t1 ≜\n1\n4η1λ iterations, the spectral dynamics satisfy\nTr(Wt1) > Tr(Vt1).\n12\n\n\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMore Semantically Correct\nT=100\nQuestion: Battle of Chacabuco is located in\nGold Answer:  Santiago\nPrediction: ?\nBelgium\nGuam\n0.1\n               More Syntactically Correct\nof\nSony\n...\nBP\nof\nQuestion: Yacin Chikh (ALG) def. Anatoly Filipov (EUN), 5:3. Xamarin owner (?)\nGold Answer:  Microsoft\nPrediction: ?\nT=5\nBelgian\nSantiago\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n1.0\nSony\nSega\n...\nFigure 5: Spectral Characteristics in Attention Weights on Counterfact Dataset.\n(b) In the specialized stage within t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1\niterations, the spectral dynamics satisfy\nTr(Wt1+t2) < Tr(Vt1+t2).\nMessages Behind Corollary 4.6.\nCorollary 4.6 implies that when the model is sufficiently trained\n(at time t1 + t2), relatively small eigenvalues of attention weights store elementary knowledge\nand large ones store specialized knowledge. This will be further verified through experiments on\nreal-world language datasets. We defer the proof and further discussions to Appendix D.7.\nExperiments Verifing Spectral Characteristics on Language Datasets.\nWe verify the above\ninsight empirically on Counterfact dataset in Figure 5 by preserving different eigenvalues and\nobserving the model performances. Concretely, at time T = 5 (fully syntactically correct) and\nT = 100 (fully syntactically correct, nearly fully semantically correct), we set the rank preservation\nρ ranging from 0.1 to 1.0, to obtain edited matrices with different eigenvalues using SVD for compar-\ning predictions. For the left figure, we find that the model’s predictions become more semantically\nsimilar and accurate, as rank preservation ρ increases (maintaining more large eigenvalues). For the\nright figure, we find that the model gradually grasps correct syntax as ρ increases (maintaining more\nsmall eigenvalues). In addition, in the middle figure, the number of correct predictions increases\nwith larger rank preservation, which accords with intuition. In total, we find that the two-stage\nlearning process is closely related to the spectral characteristics in attention weights. We defer more\nexperimental details on Counterfact and HotpotQA Datasets to Appendix B.\n5\nConclusion\nThis paper provides rigorous proof for the two-stage learning of transformers in ICL tasks. We disen-\ntangle token feature structure into two types: elementary knowledge, and specialized knowledge. By\nemploying feature learning and signal-noise decomposition techniques, we analyze the optimization\ntrajectory, finite-time convergence, and spectral characteristics under ICL regimes, offering deeper\ninsights into the optimization process. Our work potentially provides a new perspective and a\ntheoretical framework for understanding the optimization dynamics of transformers.\n13\n\n\nReferences\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need.(nips), 2017. arXiv preprint\narXiv:1706.03762, 10:S0140525X16001837, 2017.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1,\npage 2, 2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the\nIEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 16000–16009, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. Journal of Machine Learning Research, 24(240):\n1–113, 2023.\nR OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023.\nPuneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the optimization\nand generalization of multi-head attention. arXiv preprint arXiv:2310.12680, 2023.\nYuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards\na mechanistic understanding. In International Conference on Machine Learning, pages 19689–\n19729. PMLR, 2023a.\nYuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understanding\ntraining dynamics and token composition in 1-layer transformer. Advances in Neural Information\nProcessing Systems, 36:71911–71947, 2023a.\nYuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying\nmultilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535,\n2023b.\nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.\narXiv preprint arXiv:2306.09927, 2023.\n14\n\n\nYu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint\narXiv:2310.05249, 2023.\nXiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to\nlearn non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023.\nSiyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head\nsoftmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint\narXiv:2402.19442, 2024.\nYu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, and Jiajun\nChen. Generating sentences from disentangled syntactic and semantic spaces. arXiv preprint\narXiv:1907.05789, 2019.\nMingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. A multi-task approach for\ndisentangling syntax and semantics in sentence representations. arXiv preprint arXiv:1904.01173,\n2019a.\nJames Y Huang, Kuan-Hao Huang, and Kai-Wei Chang. Disentangling semantics and syntax in\nsentence embeddings with pre-trained language models. arXiv preprint arXiv:2104.05115, 2021.\nMohammed AlQuraishi. Alphafold at casp13. Bioinformatics, 35(22):4862–4865, 2019.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate\nprotein structure prediction with alphafold. nature, 596(7873):583–589, 2021.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source\nautoregressive language model. arXiv preprint arXiv:2204.06745, 2022.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.\narXiv preprint arXiv:2212.10403, 2022.\nEkin Aky¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algo-\nrithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661,\n2022.\nChenyu Zheng, Wei Huang, Rongzhen Wang, Guoqiang Wu, Jun Zhu, and Chongxuan Li. On\nmesa-optimization in autoregressively trained transformers: Emergence and capability. arXiv\npreprint arXiv:2405.16845, 2024.\nWei Shen, Ruida Zhou, Jing Yang, and Cong Shen. On the training convergence of transformers for\nin-context classification. arXiv preprint arXiv:2410.11778, 2024.\n15\n\n\nJuno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field\ndynamics on the attention landscape. arXiv preprint arXiv:2402.01258, 2024.\nHongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shal-\nlow vision transformers: Learning, generalization, and sample complexity. arXiv preprint\narXiv:2302.06015, 2023b.\nZeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and\nself-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.\nZeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust\ndeep learning. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science\n(FOCS), pages 977–988. IEEE, 2022.\nZixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised\ncontrastive learning. In International Conference on Machine Learning, pages 11112–11122.\nPMLR, 2021.\nArthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. Advances in neural information processing systems, 31, 2018.\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\ndescent on structured data. Advances in neural information processing systems, 31, 2018.\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via\nover-parameterization. In International conference on machine learning, pages 242–252. PMLR,\n2019.\nZixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is\nsufficient to learn deep relu networks? arXiv preprint arXiv:1911.12360, 2019b.\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global\nminima of deep neural networks. In International conference on machine learning, pages\n1675–1685. PMLR, 2019.\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn\nin-context? a case study of simple function classes. Advances in Neural Information Processing\nSystems, 35:30583–30598, 2022.\nYuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large\nlearning rate in training neural networks. Advances in neural information processing systems, 32,\n2019.\nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:\nProvable in-context learning with in-context algorithm selection. Advances in neural information\nprocessing systems, 36, 2024.\nMitchell Wortsman, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Replacing softmax with relu\nin vision transformers. arXiv preprint arXiv:2309.08586, 2023.\n16\n\n\nKai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. A study on relu and\nsoftmax in transformer. arXiv preprint arXiv:2302.06461, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\nassociations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.\nPratyusha Sharma, Jordan T Ash, and Dipendra Misra. The truth is in there: Improving reasoning\nin language models with layer-selective rank reduction. arXiv preprint arXiv:2312.13558, 2023.\n17\n\n\nAppendix\nA Table of Notations\n19\nB Additional Experimental Details\n20\nB.1\nExperiments Verifying Two-Stage Learning on Theoretical Synthetic Dataset. . . .\n20\nB.2\nExperiments on Counterfact Dataset. . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.3\nExperiments on HotpotQA Dataset.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC Additional Discussions\n23\nC.1\nAdditional Discussions on Feature Structure . . . . . . . . . . . . . . . . . . . . .\n23\nC.2\nAdditional Discussions on Assumption 4.1 . . . . . . . . . . . . . . . . . . . . . .\n23\nD Proofs for Theorems and Corollary\n25\nD.1\nUseful Probability Concentration Inequalities . . . . . . . . . . . . . . . . . . . .\n25\nD.2\nPropositions, Lemmas and Corollaries . . . . . . . . . . . . . . . . . . . . . . . .\n26\nD.3\nProof for the Elementary Stage: Proof of Theorem 4.2 . . . . . . . . . . . . . . . .\n38\nD.4\nProof for the Elementary Stage: Proof of Theorem 4.3 . . . . . . . . . . . . . . . .\n54\nD.5\nProof for the Specialized Stage: Proof of Theorem 4.4 . . . . . . . . . . . . . . . .\n58\nD.6\nProof for the Specialized Stage: Proof of Theorem 4.5 . . . . . . . . . . . . . . . .\n65\nD.7\nProof for Spectral Characteristics: Proof of Corollary 4.6 . . . . . . . . . . . . . .\n69\n18\n\n\nA\nTable of Notations\nTable 1: Table of Notations.\nNotation\nDescription\nt1\nTotal iterations of the elementary stage\nt2\nTotal iterations of the specialized stage\nN\nNumber of training prompts\nL\nTraining prompt length (the last token is a query)\nxn\ni = [xn\ni,1, xn\ni,2]⊤∈R2d\nDivide the i-th token of n-th training prompts into two parts\nxn\ni,1 ∼P ∈Rd\nThe elementary knowledge in a token\nxn\ni,2 ∼Q ∈Rd\nThe specialized knowledge in a token\nXn\n1 =\nh\nxn\n1,1\nxn\n2,1\n· · ·\nxn\nL,1\ni\n∈Rd×L\nStack of xn\ni,1\nXn\n2 =\nh\nxn\n1,2\nxn\n2,2\n· · ·\nxn\nL,2\ni\n∈Rd×L\nStack of xn\ni,2\nXn =\n\u0014Xn\n1\n0\n0\nXn\n2\n\u0015\n∈R2d×2L\nStack of Xn\n1 and Xn\n2\nyn\ni ∈{−1, 1}\nBinary classification label\nY n =\n\u0002\nyn\n1\nyn\n2\n· · ·\n0\n\u0003\n∈R1×L\nStack of yn\ni\neY n =\n\u0002\nY n\nY n\u0003\n∈R1×2L\nStack of Y n\n1 and Y n\n2\nf(U; X, eY )\nNormalized ReLU self-attention output, see in Equation 1\nh(X1)\nVirtual network operates on X1, see in Equation 2\ng(X2)\nVirtual network operates on X2, see in Equation 2\nU =\n\u0014W\n0\n0\nV\n\u0015\n∈R2d×2d\nModel parameter of normalized ReLU self-attention net-\nwork\nU = U + eU ∈R2d×2d\nSignal-noise decomposition of weight U\nW = W + f\nW ∈Rd×d\nModel parameter of virtual network h, signal-noise decom-\nposition of weight W\nV = V + eV ∈Rd×d\nModel parameter of virtual network g, signal-noise decom-\nposition of weight V\nbL(U)\nThe empirical loss over weight U, see in Equation 3\nK(U)\nThe training loss over signal weight U, see in Equation 5\nK1(W)\nThe training loss over signal weight W, see in Equation 6\nK2(V )\nThe training loss over signal weight V , see in Equation 6\n19\n\n\nB\nAdditional Experimental Details\nIn this section, we provide additional experimental details, including experiments that verify two-\nstage learning on synthetic dataset (in Section B.1), as well as experiments conducted on language\ndatasets such as Counterfact (in Section B.2) and HotpotQA (in Section B.3).\nB.1\nExperiments Verifying Two-Stage Learning on Theoretical Synthetic\nDataset.\nBased on our theoretical construction of component P and Q and model setting in Section 3, we\nconduct experiments on the synthetic dataset with the following hyperparameters: data dimension\nd = 10, r ≜∥ζ∥2 = 10−7, u ≜∥z∥2 = 7, prompt length L = 128 and N = 128 training prompts.\nWe train the one-layer normalized ReLU self-attention model for 400 epochs, using SGD optimizer\nwith the learning rate annealed from 1.5 to 0.015 at epoch 20. As shown in Figure 4, the training\ndynamics of linear separable component P and nonlinear separable component Q exhibit a clear\ntwo-stage phenomenon, closely aligning with our theoretical results.\nB.2\nExperiments on Counterfact Dataset.\nDatasets, Model and Hyperparameter Settings.\nCounterfact (Meng et al., 2022) is a question-\nanswering dataset consisting of knowledge tuples in the form of (subject, relation, answer). These\ntuples are constructed using entities from Wikidata. Additionally, there are three paraphrased\nprompts for each question, resulting in a total of 65,757 examples for the entire dataset. We fine-\ntune the GPT-2 model with a batch size of 32 for 200 epochs, using the AdamW optimizer with\na learning rate of 5e-6. Notably, we adopt some experiment code from Sharma et al. (2023). All\nexperiments are conducted using a single 24GB NVIDIA GeForce RTX 3090.\nObservation (1): Verify Two-stage Learning of Disentangled Two-type Feature Structure.\nIn Figure 1, we present the training loss over 200 epochs, highlighting three key moments with\nrepresentative samples, including questions, gold answers and the model’s predictions. At the initial\ntime T = 1, many predictions are both syntactically and semantically incorrect. By T = 5, we\nobserve a significant decrease in training loss; all predictions meet syntactic requirements, but most\nare remain semantically incorrect and inconsistent with the true answers. Thus, the period from\nT = 1 to T = 5 corresponds to our theoretical Elementary Stage. By T = 100, all predictions are\nsyntactically correct, with most being semantically correct and achieving a very low loss value.\nTherefore, the period from T = 6 to T = 100 represents our theoretical Specialized Stage. Overall,\nthis experiment on language dataset supports our theory of two-stage learning for the disentangled\ntwo-type feature structure, i.e., syntax and semantics in languages.\nObservation (2): Verify Spectral Characteristics.\nFrom Theorems 4.3 ∼4.5, based on the\nrelationship of F-norm and trace, it’s straightforward to get Tr(Wt1+t2) < Tr(Vt1+t2) at convergence\ntime t1 + t2 (Detailed Corollay 4.6 is shown in Section 4.3). We know that weight W of network\nh operates on the elementary knowledge and weight V of network g operates on the specialized\n20\n\n\nknowledge. Then the corollary of Tr(Wt1+t2) < Tr(Vt1+t2) hints that, relatively small eigenvalues\nof attention weights store elementary knowledge and large ones store specialized knowledge.\nThus in Figure 5, we perform model editing on the attention layer weights of the model to analyze\nthe impact of large or small eigenvalues. Concretely, we edit attention weights at time T = 5 (fully\nsyntactically correct) and T = 100 (fully syntactically correct, nearly fully semantically correct).\nUsing SVD, we sort the eigenvalues of attention weights and set rank preservation coefficient\nρ, ranging from 0.1 to 1.0. As shown in Figure 5, the numbers in matrices represent the rank\npreservation coefficient ρ of the current matrix.\n• For the left figure, we first edit attention weights at T = 100. Eigenvalues are sorted from\nlargest to smallest and matrices preserve the top ρ proportion of the largest eigenvalues. When\nρ = 0.1, it means maintaining 10% of the largest eigenvalues and corresponding eigenvectors.\nThe figure displays 10 weight matrices, with ρ ranging from 0.1 to 1.0 from left to right. As ρ\nincreases, more large eigenvalues are preserved, and the model’s predictions become more\nsemantically similar and accurate.\n• For the right figure, we further edit attention weights at T = 5. Eigenvalues are sorted from\nsmallest to largest and matrices preserve the top ρ proportion of the smallest eigenvalues.\nFrom right to left, more small eigenvalues are included. As more eigenvalues of the full\nmatrix are used, the model gradually grasps correct syntactic information.\n• For the middle figure, it shows that the number of correct predictions increases with larger\nrank preservation, which is intuitive. In summary, the spectral characteristics insights drawn\nfrom our theory are also empirically reasonable.\nB.3\nExperiments on HotpotQA Dataset.\nDatasets, Model and Hyperparameter Settings.\nHotpotQA is a question-answering dataset that\ninvolves natural, multi-hop questions and provides supervision for the supporting facts, aiming\nto enhance the explainability of question-answering systems. We choose the HotPotQA dataset\navailable on HuggingFace, with a small size 13,530 (Meng et al., 2022). We fine-tune the GPT-2\nmodel with a batch size of 32 for 200 epochs, using the AdamW optimizer with a learning rate of\n5e-6. Notably, we adopt some experiment code from Sharma et al. (2023). All experiments are\nconducted using a single 24GB NVIDIA GeForce RTX 3090.\nObservation (1): Verify Two-stage Learning of Disentangled Two-type Feature Structure.\nIn Figure 6, we present the training loss over 60 epochs, highlighting three key moments with\nrepresentative samples, including questions, gold answers and the model’s predictions. At the initial\ntime T = 1, many predictions are both syntactically and semantically incorrect. By T = 8, we\nobserve a significant decrease in training loss; all predictions meet syntactic requirements, but most\nare remain semantically incorrect and inconsistent with the true answers. Thus, the period from\nT = 1 to T = 8 corresponds to our theoretical Elementary Stage. By T = 40, all predictions are\nsyntactically correct, with most being semantically correct and achieving a very low loss value.\nTherefore, the period from T = 9 to T = 40 represents our theoretical Specialized Stage. Overall,\n21\n\n\nCadmium Chloride is slightly soluble in this \nchemical, it is also called what?\nQuestion\nAnswer Prediction\nalcohol\n\\n\nThe 34th Foot Regiment was sent to fight who?\nAmericans\ns\nT=1\nThe short story from The Wind’s Twelve Quarters that \nwon the Hugo Award in 1974 was published in what year\n1973\nThe\nSyntax\nSemantics\nElementary Stage\nSpecialized Stage\nT=1\nT=8\nT=40\nWhat country are Mudvayne and Hellyeah both \nfrom?\nAmerican\nAustralia\nIn what year was the \"puppet state\" in which Liu \nMenggeng a politician/physician abolished?\n1945\n1978\nT=8\nWhat does Jack Horner and Jack and the \nBeanstalk have in common?\nJack         American\nSyntax\nSemantics\nWhat type of media does Caracal and The Weeknd\nhave in common?\nmusic\nIn what year was the \"puppet state\" in which Liu \nMenggeng a politician/physician abolished?\n1945\nT=40\nAre Lawson and Minutemen from the same \ncountry?\nno\nSyntax\nSemantics\nmusic\n1945\nno\nFigure 6: Two-stage Learning of Syntactic and Semantic Information on Hotpot Dataset.\nthis experiment on language dataset supports our theory of two-stage learning for the disentangled\ntwo-type feature structure, i.e., syntax and semantics in languages.\nObservation (2): Verify Spectral Characteristics.\nIn Figure 7, we perform model editing on the\nattention layer weights of the model to analyze the impact of large or small eigenvalues. Concretely,\nwe edit attention weights at time T = 8 (fully syntactically correct) and T = 40 (fully syntactically\ncorrect, nearly fully semantically correct). Using SVD, we sort the eigenvalues of attention weights\nand set rank preservation coefficient ρ, ranging from 0.1 to 1.0. As shown in Figure 5, the numbers\nin matrices represent the rank preservation coefficient ρ of the current matrix.\n• For the left figure, we first edit attention weights at T = 40. Eigenvalues are sorted from\nlargest to smallest and matrices preserve the top ρ proportion of the largest eigenvalues. When\nρ = 0.1, it means maintaining 10% of the largest eigenvalues and corresponding eigenvectors.\nThe figure displays 10 weight matrices, with ρ ranging from 0.1 to 1.0 from left to right. As ρ\nincreases, more large eigenvalues are preserved, and the model’s predictions become more\nsemantically similar and accurate.\n• For the right figure, we further edit attention weights at T = 8. Eigenvalues are sorted from\nsmallest to largest and matrices preserve the top ρ proportion of the smallest eigenvalues.\nFrom right to left, more small eigenvalues are included. As more eigenvalues of the full\nmatrix are used, the model gradually grasps correct syntactic information.\n• For the middle figure, it shows that the number of correct predictions increases with larger\nrank preservation, which is intuitive. In summary, the spectral characteristics insights drawn\nfrom our theory are also empirically reasonable.\n22\n\n\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMore Semantically Correct\nT=40   \nQuestion: Pearl Lowe and Alison Goldfrapp, is of which nationality?\nGold Answer:  English\nPrediction: ?\nAmerican\nAustralian\n0.1\n               More Syntactically Correct\nno\nno\norange\nred\nno\n...       \nQuestion: Cadmium Chloride is slightly soluble in this chemical, it is also called what? \nGold Answer:  alcohol\nPrediction: ?\nT=8\nAmerican\nItalian\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n1.0\nEnglish\n...\nwater\nwater\nFigure 7: Spectral Characteristics of Attention Weights on Hotpot Dataset.\nC\nAdditional Discussions\nIn this section, we provide additional discussions on disentangled feature structure (in Section C.1)\nand Assumption 4.1 (in Section C.2).\nC.1\nAdditional Discussions on Feature Structure\nIn Figure 8, we utilize two-dimensional data to intuitively illustrate the roles of two components P\nand Q based on the distribution, in learning both linear and nonlinear classifiers. By concatenating\nthese two components, individual sample xn\ni in a prompt is employed to tackle a more complex\ncomposite nonlinear classification task. Despite the data composition, the task’s difficulty is\nsignificantly increased rather than being a simple combination.\nC.2\nAdditional Discussions on Assumption 4.1\nAssumption 4.1. Throughout the Theorems, set the variance of initialization parameter τ0 =\nΘ\n\u0010\n1\n√log d\n\u0011\n, regularization coefficient 1\nλ = Θ\n\u0000√log d\n\u0001\nand prompt length L = Θ (Poly(d)) where d\ndenotes the input dimension. We defer more discussions to Appendix C.2.\nWe next validate the hyperparameter orders in Assumption 4.1.\n(1) τ0 denotes the variance of the initialization parameter. The requirement τ0 = Θ\n\u0010\n1\n√log d\n\u0011\nsuggests\nthat, as dimension d increases and the data complexity grows, the variance should be adaptively\ndecreased. This aligns with practical training methodologies, as a higher variance might result in a\nsignificant shift of the initial weights in high-dimensional spaces, leading to unstable training and\npotentially impeding convergence.\n(2) λ denotes the L2 regularization coefficient in the loss function. The requirement 1\nλ = Θ\n\u0000√log d\n\u0001\nsuggests that, as dimension d increases, λ should be adjusted to be correspondingly smaller. This is\n23\n\n\nLinear Classification\nPositive \nNegative\nMargin \nComposite Nonlinear Classification\nPositive \nNegative\nNonlinear Classification\nPositive \nNegative\nFigure 8: Composite nonlinear classification.\na practical consideration because, in high-dimensional scenarios, a large λ may overly constrain\nthe model, potentially causing underfitting. Furthermore, t1 ≤\n1\nη1λ implies that there might be a\nlonger period during which the model may struggle to effectively learn from the higher-dimensional\nfeature Q, which accords with the empirical intuition.\n(3) L denotes the prompt length. The requirement L = Θ(Poly(d)) suggests that the model\nanticipates longer input sequences for learning high-dimensional data, which accords with reality.\n24\n\n\nD\nProofs for Theorems and Corollary\nIn this section, we present detailed proofs for the Theorems and Corollary discussed in Section 4.\nPrior to the proofs, we first introduce useful probability concentration inequalities (in Section D.1),\nfollowed by some propositions, lemmas, and corollaries (in Section D.2). The proofs of Theorem\n4.2 and 4.3 are provided in Section D.3 and D.4, respectively, while the proofs of Theorem 4.4 and\n4.5 are provided in Section D.5 and D.6. Finally, we discuss Corollary 4.6 with its proof directly\nderived from the main theorems.\nD.1\nUseful Probability Concentration Inequalities\nLemma D.1 (Hoeffding’s Inequality for General Bounded Random Variables, cite HDP p16). Let\nX1, · · · , XN be independent random variables. Assume that Xi ∈[mi, Mi] for every i. Then, for\nany t > 0, we have\nPr\n N\nX\ni=1\n(Xi −E[Xi]) ≥t\n!\n≤exp\n \n−\n2t2\nPN\ni=1(Mi −mi)2\n!\nLemma D.2 (Bernstein’s Inequality for Bounded Random Variables, cite ¡concentration.pdf¿, lemma\n7.37). Let X1, · · · , XN be i.i.d. and suppose that |Xi| ≤c, E(Xi) = µ, σ2 =\n1\nN\nPN\ni=1 Var(Xi).\nWith probability at least 1 −δ,\n\f\f\f\f\f\nN\nX\ni=1\nXi −µ\n\f\f\f\f\f ≤\nr\n2σ2 log(1/δ)\nn\n+ 2c log(1/δ)\n3n\nLemma D.3 (Norm of Matrix with Gaussian Entries, cite HDP p85). Let A be an n × n random\nmatrix whose entries Aij are independent gaussian random variables with N(0, σ2). Then for any\nt > 0, we have\n∥A∥≲σ√n\nLemma D.4 (Standard Gaussian Concentration Inequality). Suppose that X = X1, · · · , XN are\ni.i.d. standard complex Gaussian variables, and suppose F : Cn →R is a 1-Lipschitz function with\nrespect to the Euclidean metric. Then E[X] < ∞and for all t ≥0,\nPr (X −E[X] > t) ≤e−t2\nLemma D.5 (Chernoff Bound for Guassian Variables). Let X ∼N(µ, σ2), then E[eλX =\nexp (µλ + σ2λ2/2) and for all t ≥0,\nPr (|X −µ| > t) ≤2 exp\n\u0012\n−t2\n2σ2\n\u0013\nPr\n\u0012\f\f\f\f\nX −µ\nσ\n\f\f\f\f > t\n\u0013\n≤2 exp\n\u0012\n−t2\n2\n\u0013\n25\n\n\nD.2\nPropositions, Lemmas and Corollaries\nAssumption D.6. For X1, X2 ∈Rd×L that satisfies the data structure, let i be i-th row, we have\n∥[X⊤\n1 ]i∥2 ≤u + γ0, ∥X⊤\n1 ∥F ≤\n√\nL(u + γ0)\n∥[X⊤\n2 ]i∥2 ≤u + r, ∥X⊤\n2 ∥F ≤\n√\nL(u + r)\n∥[X⊤]i∥2 ≤max{u + γ0, u + r}, ∥X⊤∥F ≤\np\nL(u + γ0)2 + L(u + r)2\nProof. For X1, we have\n∥w⋆∥2 = 1, ∥[X⊤]i∥2 ≤u + γ0, ∥X⊤∥F ≤\n√\nL(u + γ0)\nFor X2, we have\n⟨z, ζ⟩= 0, ∥z∥2 = u, ∥ζ∥2 = r\n∥[X⊤]i∥2 ≤u + r, ∥X⊤∥F ≤\n√\nL(u + r)\nProposition D.7. By signal-noise decomposition, we have the updating rules for signal weight and\nnoise weight:\nU t = −\nt\nX\ns=1\nη (1 −ηλ)t−s ∇Us−1 bL(Us−1),\neUt = (1 −ηλ)t U0 −\nt\nX\ns=1\nη (1 −ηλ)t−s ξs−1.\nProof. Decoupling the signal and noise, signal weight U is affected by the gradient updates, and\nnoise weight eU is affected by noise ξ. With Ut+1 = (1 −γtλ)Ut −γt(∇U bL(Ut) + ξt),\nU t = −\nt\nX\ns=1\nγs−1\n t−1\nY\ni=s\n(1 −γiλ)\n!\n∇Us−1 bL(Us−1)\neUt =\n t−1\nY\ni=0\n(1 −γiλ)\n!\nU0 −\nt\nX\ns=1\nγs−1\n t−1\nY\ni=s\n(1 −γiλ)\n!\nξs−1\nWhen constant learning rate γt = η,\nU t = −\nt\nX\ns=1\nη (1 −ηλ)t−s ∇Us−1 bL(Us−1)\neUt = (1 −ηλ)t U0 −\nt\nX\ns=1\nη (1 −ηλ)t−s ξs−1.\n(7)\n26\n\n\nSince U =\n\u0014W\n0\n0\nV\n\u0015\n, then\n\u0014Wt+1\n0\n0\nVt+1\n\u0015\n= (1 −γtλ)\n\u0014Wt\n0\n0\nVt\n\u0015\n−γt(∇U bL(Ut) + ξt)\nWt+1 = (1 −γtλ)Wt −γt(∇Wt bL(Ut) + ξt)\nVt+1 = (1 −γtλ)Vt −γt(∇Vt bL(Ut) + ξt)\nSimilar to the signal-noise decomposition of U with learning rate γt = η, we naturally have\nW t = −\nt\nX\ns=1\nη (1 −ηλ)t−s ∇Ws−1 bL(Us−1)\nf\nWt = (1 −ηλ)t W0 −\nt\nX\ns=1\nη (1 −ηλ)t−s ξs−1\n(8)\nV t = −\nt\nX\ns=1\nη (1 −ηλ)t−s ∇Vs−1 bL(Us−1)\neVt = (1 −ηλ)t V0 −\nt\nX\ns=1\nη (1 −ηλ)t−s ξs−1\n(9)\nProposition D.8. For any U ∈R2d×2d, W, V\n∈Rd×d, X ∈R2d×2L, X1, X2 ∈Rd×L, eY\n∈\nR1×2L, Y ∈R1×L, then we have the derivative over weight U of empirical loss, i.e. ∇bL(U)\nand its component [∇bL(U)]i is the i-th row of ∇bL(U),\n∇bL(U) = bE\nh\n1/2L · l′(f(U; X, eY ))X · diag\n\u00001(X⊤UxL)\n\u0001\nx⊤\nL\ni\n[∇bL(U)]i = bE\nh\n1/2L · l′(f(U; X, eY ))1([X⊤]iUxL)[X⊤]ix⊤\nL\ni\nAdditionally, for the derivative over weight W,\n∇W bL(U) = bE\nh\n1/2L · l′(f(U; X, eY ))X1 · diag\n\u00001(X⊤\n1 WxL,1)\n\u0001\nx⊤\nL,1\ni\n[∇W bL(U)]i = bE\nh\n1/2L · l′(f(U; X, eY ))1([X⊤\n1 ]iWxL,1)[X1]ix⊤\nL,1\ni\nfor the derivative over weight V ,\n∇V bL(U) = bE\nh\n1/2L · l′(f(U; X, eY ))X2 · diag\n\u00001(X⊤\n2 V xL,2)\n\u0001\nx⊤\nL,2\ni\n[∇V bL(U)]i = bE\nh\n1/2L · l′(f(U; X, eY ))1([X⊤\n2 ]iV xL,2)[X2]ix⊤\nL,2\ni\n27\n\n\nProof. According to the definition of training objective, define\nl(f(U; X, eY )) = −log σ\n\u0010\nyLf\n\u0010\nU; X, eY\n\u0011\u0011\nthen we have the derivative of empirical loss with weight U,\n∇bL(U) = bE\nh\nl′(f(U; X, eY ))∇(yLf(U; X, eY ))\ni\n= bE\nh\nl′(f(U; X, eY ))yL∇\n\u0010\neY /2L · ReLU\n\u0000X⊤UxL\n\u0001\u0011i\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\n2L\nX\ni=1\nyi∇ReLU\n\u0000[X⊤]iUxL\n\u0001\n#\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\n2L\nX\ni=1\nyi1([X⊤]iUxL)[X⊤]ix⊤\nL\n#\n= bE\nh\n1/2L · l′(f(U; X, eY ))X · diag\n\u00001(X⊤UxL)\n\u0001\nx⊤\nL\ni\nand [∇bL(U)]i = bE\nh\n1/2L · l′(f(U; X, eY ))1([X⊤]iUxL)[X⊤]ix⊤\nL\ni\n.\nFurthermore, when taking derivative over W,\n∇W bL(U) = bE\nh\nl′(f(U; X, eY ))∇W\n\u0010\nyLf(U; (X, eY ))\n\u0011i\n= bE\nh\nl′(f(U; X, eY ))yL∇W\n\u0010\neY /2L · ReLU\n\u0000X⊤UxL\n\u0001\u0011i\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\nL\nX\ni=1\n\u0002\nyi\nyi\n\u0003\n∇WReLU\n\u0012\u0014[X⊤\n1 ]iWxL,1\n[X⊤\n2 ]iV xL,2\n\u0015\u0013#\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\nL\nX\ni=1\nyi1([X⊤\n1 ]iWxL,1)[X1]ix⊤\nL,1\n#\n= bE\nh\n1/2L · l′(f(U; X, eY ))X1 · diag\n\u00001(X⊤\n1 WxL,1)\n\u0001\nx⊤\nL,1\ni\nand [∇W bL(U)]i = bE\nh\n1/2L · l′(f(U; X, eY ))1([X⊤\n1 ]iWxL,1)[X1]ix⊤\nL,1\ni\n. Similarly, when taking\nderivative over V , we have\n∇V bL(U) = bE\nh\n1/2L · l′(f(U; X, eY ))X2 · diag\n\u00001(X⊤\n2 V xL,2)\n\u0001\nx⊤\nL,2\ni\n[∇V bL(U)]i = bE\nh\n1/2L · l′(f(U; X, eY ))1([X⊤\n2 ]iV xL,2)[X2]ix⊤\nL,2\ni\n28\n\n\nProposition D.9. Assume that bL is K-Lipschitz continuous, then we have\n\r\r\r∇bL(U)\n\r\r\r\nF ≲K,\n\r\r\r[∇bL(U)]i\n\r\r\r\n2 ≲\nK\n√\n2d\n\r\r\r∇W bL(U)\n\r\r\r\nF ≲K,\n\r\r\r[∇W bL(U)]i\n\r\r\r\n2 ≲K\n√\nd\n\r\r\r∇V bL(U)\n\r\r\r\nF ≲K,\n\r\r\r[∇V bL(U)]i\n\r\r\r\n2 ≲K\n√\nd\nProposition D.10. With Assumption D.6 and Proposition D.9 , we have that signal weight norm\nsatisfies, for X1\n\r\rU t\n\r\r\nF ≲K\nλ ,\n\r\r[U t]i\n\r\r\n2 ≲\nK\nλ\n√\n2d\n\r\rW t\n\r\r\nF ≲K\nλ ,\n\r\r[W t]i\n\r\r\n2 ≲\nK\nλ\n√\nd\n\r\rV t\n\r\r\nF ≲K\nλ ,\n\r\r[V t]i\n\r\r\n2 ≲\nK\nλ\n√\nd\nProof. By Equation 7, 8 and 9, when 0 < 1 −ηλ < 1, i.e., 0 < ηλ < 1,\n\r\rU t\n\r\r\nF =\nt\nX\nτ=1\nη (1 −ηλ)t−τ \r\r\r∇bL(Uτ−1)\n\r\r\r\nF ≲K\nλ\n\r\r[U t]i\n\r\r\n2 =\nt\nX\nτ=1\nη (1 −ηλ)t−τ \r\r\r[∇bL(Uτ−1)]i\n\r\r\r\n2 ≲\nK\nλ\n√\n2d\n∥W t∥F =\nt\nX\nτ=1\nη (1 −ηλ)t−τ \r\r\r∇W bL(Uτ−1)\n\r\r\r\nF ≲K\nλ\n∥[W t]i∥2 =\nt\nX\nτ=1\nη (1 −ηλ)t−τ \r\r\r[∇W bL(Uτ−1)]i\n\r\r\r\n2 ≲\nK\nλ\n√\nd\n∥V t∥F =\nt\nX\nτ=1\nη (1 −ηλ)t−τ \r\r\r∇V bL(Uτ−1)\n\r\r\r\nF ≲K\nλ\n∥[V t]i∥2 =\nt\nX\nτ=1\nη (1 −ηλ)t−τ \r\r\r[∇V bL(Uτ−1)]i\n\r\r\r\n2 ≲\nK\nλ\n√\nd\nFurthermore,\n\r\r[X⊤U]ixL\n\r\r\n2 ≤∥[X]i∥2∥U∥F∥xL∥2 ≲K(u + m)2\nλ\n\r\r[X⊤\n1 W]ixL,1\n\r\r\n2 ≤∥[X1]i∥2∥W∥F∥xL,1∥2 ≲K(u + γ0)2\nλ\n\r\r[X⊤\n2 V ]ixL,2\n\r\r\n2 ≤∥[X2]i∥2∥V ∥F∥xL,2∥2 ≲K(u + r)2\nλ\n29\n\n\nProposition D.11. For time τ ≤t, we have\nProof. For τ ≤t,\neUt = (1 −ηλ)t−τ eUτ −\nt−τ\nX\nt′=1\nη(1 −ηλ)t−τ−t′ζτ+t′−1\n= (1 −ηλ)t−τ eUτ + Ξt,τ\nwhere Ξt,τ = −Pt−τ\nt′=1 η(1 −ηλ)t−τ−t′ζτ+t′−1.\nLemma D.12 (Refer to Lemma A.8 in Li et al. (2019), Lemma 8.2 of Allen-Zhu et al. (2019)). Let\nX ∈R2d×2L, xL ∈R2d be a fixed example, with ∥xL∥2 ≤B and ∥X∥F ≤\n√\n2LB. With Assumption\nD.6 and Proposition D.10, for every τ > 0, let U = U + eU where eU ∈R2d×2d is a random variable\nwhose columns have i.i.d distribution N(0, τ 2\n0 I2d×2d) and eY ∈R2L such that each entry of eY is\ni.i.d. uniform in {−1, 1}. We have that, w.h.p over the randomness of eU and eY , ∀U ∈R2d×2d, we\nhave that\n∥1(X⊤UxL) −1(X⊤eUxL)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵU\nFurthermore,\n\f\f\fNU(U; X, eY ) −NeU(U; X, eY )\n\f\f\f ≲(u + m)2K7/3λ−7/3τ −4/3\n0\nL−1/3\nProof. With Lemma A.8 of Li, we can compute the difference of activation patterns.\n∥1(X⊤UxL) −1(X⊤eUxL)∥1 ≲∥X⊤U∥4/3\nF τ −4/3\n0\nL2/3\n≲((2L)1/2B)4/3∥U∥4/3\nF (τ0(2L)1/2B)−4/3L2/3\n≲∥U∥4/3\nF τ −4/3\n0\nL2/3\nWith Assumption D.6, B = u + m, and Proposition D.10, then\n∥1(X⊤UxL) −1(X⊤eUxL)∥1 ≲∥U∥4/3\nF τ −4/3\n0\nL2/3\n≲∥U∥4/3\nF τ −4/3\n0\nL2/3\n≲K4/3λ−4/3τ −4/3\n0\nL2/3\n=\n\u0012LK2\nλ2τ 2\n0\n\u00132/3\n30\n\n\nFurthermore,\n\f\f\fNU(U; X, eY ) −NeU(U; X, eY )\n\f\f\f =\n\r\r\reY /2L ·\n\u0010\n1\n\u0000X⊤UxL\n\u0001\n−1\n\u0010\nX⊤eUxL\n\u0011\u0011\n⊙\n\u0000X⊤UxL\n\u0001\r\r\r\n≤1\n2L\nX\ni∈[2L]\n\f\f\f[eY ]i\n\f\f\f\n\f\f\f1\n\u0000[X⊤]iUxL\n\u0001\n−1\n\u0010\n[X⊤]i eUxL\n\u0011\f\f\f\n\f\f[X⊤]iUxL\n\f\f\n≤1\n2L\n\r\r\r1\n\u0000X⊤UxL\n\u0001\n−1\n\u0010\nX⊤eUxL\n\u0011\r\r\r\n1 max\ni\n\f\f[X⊤U]ixL\n\f\f\n≲K4/3λ−4/3τ −4/3\n0\nL−1/3K(u + m)2\nλ\n≲(u + m)2K7/3λ−7/3τ −4/3\n0\nL−1/3\nCorollary D.13. Let X1 ∈Rd×L, xL,1 ∈Rd be a fixed example, with Assumption D.6 and Proposi-\ntion D.10, ∥xL,1∥2 ≤u + γ0 and ∥X1∥F ≤\n√\nL(u + γ0). Then, w.h.p over the randomness of f\nW\nand Y , ∀W ∈Rd×d, we have that\n∥1(X⊤\n1 WxL,1) −1(X⊤\n1 f\nWxL,1)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵW\n(10)\nFurthermore,\n\f\fNW(W; X1, Y ) −Nf\nW(W; X1, Y )\n\f\f ≲(u + γ0)2K7/3λ−7/3τ −4/3\n0\nL−1/3\nNote. In ϵW, K is the Lipschitz constant, λ denotes the L2 regularization coefficient, τ0 denotes the\nvariance of initialization parameter and L is prompt length. When with choices in Assumption 4.1,\nwe have ϵW = (Poly(d))2/3.\nCorollary D.14. Let X2 ∈Rd×L, xL,2 ∈Rd be a fixed example, with Assumption D.6 and Proposi-\ntion D.10, ∥xL,2∥2 ≤u + r and ∥X2∥F ≤\n√\nL(u + r). Then, w.h.p over the randomness of eV and\nY , ∀V ∈Rd×d, we have that\n∥1(X⊤\n2 V xL,2) −1(X⊤\n2 eV xL,2)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵV\nFurthermore,\n\f\fNV (V ; X2, Y ) −NeV (V ; X2, Y )\n\f\f ≲(u + r)2K7/3λ−7/3τ −4/3\n0\nL−1/3\nLemma D.15. Under the same setting as Lemma D.12, we have\n\r\r1\n\u0000X⊤Ut1+t2xL\n\u0001\n−1\n\u0000X⊤Ut1xL\n\u0001\r\r\n1 ≲ϵU + L\nrη2\nη1\n+\np\nL log d\n31\n\n\nwhere ϵU = K4/3λ−4/3τ −4/3\n0\nL2/3. Furthermore,\n\f\f\fNUt1+t2(U t1+t2; X, eY ) −NUt1(U t1+t2; X, eY )\n\f\f\f ≲\n\u0012\nϵU + L\nrη2\nη1\n+\np\nL log d\n\u0013 K(u + m)2\nLλ\nand\n\f\fNUt1+t2(Ut1+t2; X, Y ) −NUt1(U t1+t2; X, Y )\n\f\f\n≲ϵ(u + r)4√\nd\nλL\n+ (u + r)4p\nLdη2/η1\nλ\n+ (u + r)4p\nd log d\n≲\n\u0012\nϵ +\nrη2\nη1\nL +\np\nL log d\n\u0013 (u + r)4√\nd\nλL\nProof. To analysis that how the sign of Ut1+t2 correlates to Ut1,\n\r\r1\n\u0000X⊤Ut1+t2xL\n\u0001\n−1\n\u0000X⊤Ut1xL\n\u0001\r\r\n1\n=\n\r\r\r1\n\u0000X⊤Ut1+t2xL\n\u0001\n−1\n\u0010\nX⊤eUt1+t2xL\n\u0011\n+ 1\n\u0010\nX⊤eUt1+t2xL\n\u0011\n−1\n\u0010\nX⊤eUt1xL\n\u0011\n+ 1\n\u0010\nX⊤eUt1xL\n\u0011\n−1\n\u0000X⊤Ut1xL\n\u0001\r\r\r\n1\n≤\n\r\r\r1\n\u0000X⊤Ut1+t2xL\n\u0001\n−1\n\u0010\nX⊤eUt1+t2xL\n\u0011\r\r\r\n1\n|\n{z\n}\nA\n+\n\r\r\r1\n\u0010\nX⊤eUt1+t2xL\n\u0011\n−1\n\u0010\nX⊤eUt1xL\n\u0011\r\r\r\n1\n|\n{z\n}\nB\n+\n\r\r\r1\n\u0010\nX⊤eUt1xL\n\u0011\n−1\n\u0000X⊤Ut1xL\n\u0001\r\r\r\n1\n|\n{z\n}\nC\nFor term A and term C, With Lemma D.12, we have\n∥1(X⊤Ut1+t2xL) −1(X⊤eUt1+t2xL)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵU\n(11)\n∥1(X⊤Ut1xL) −1(X⊤eUt1xL)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵU\n(12)\nFor term B, we first analysis the relationship between eUt1+t2 and eUt1. With Proposition D.11, for\nτ ≤t, we have\neVt = (1 −ηλ)t−τ eVτ −\nt−τ\nX\nt′=1\nη(1 −ηλ)t−τ−t′ζτ+t′−1\n= (1 −ηλ)t−τ eVτ + Ξt,τ\nwhere Ξt,τ = −Pt−τ\nt′=1 η(1 −ηλ)t−τ−t′ζτ+t′−1. Assume that there are t1 iterations in the first stage,\nlet τ = t1, t = t1 + t2, and t −τ = t2, then\neUt1+t2 = (1 −η2λ)t2 eUt1 −\nt2\nX\nt′=1\nη2(1 −η2λ)t2−t′ζt1+t′−1\n= (1 −η2λ)t2 eUt1 + Ξt1+t2,t1\n(13)\n32\n\n\nwhere Ξt1+t2,t1 = −Pt2\nt′=1 η2(1 −η2λ)t2−t′ζt1+t′−1.\nConsider [Ξt1+t2,t1]ij ∼N(0, σ2\nt1+t2,t1), for 0 < 1 −η2λ < 1, with a technical assumption that\nτ 2\nζ = τ 2\n0 −(1−η1λ)2τ 2\n0\nη2\n1\n,\nσ2\nt1+t2,t1 =\nt2\nX\nt′=1\nη2\n2(1 −η2λ)2(t2−t′)τ 2\nζ = η2\n2τ 2\nζ\n1 −(1 −η2λ)2t2−1\nη2λ\n≤η2\n2τ 2\nζ\n1\nη2λ = η2\n2\nτ 2\n0 −(1 −η1λ)2τ 2\n0\nη2\n1\n1\nη2λ ≤η2\n2\n2η1λτ 2\n0\nη2\n1\n1\nη2λ\n= 2η2τ 2\n0\nη1\nSince η2 ≪η1, then σt1+t2,t1 ≪τ0. This implies that additional noise in the second stage is small.\nWith Equation 13, we have\nX⊤eUt1+t2xL = (1 −η2λ)t2X⊤eUt1xL + X⊤Ξt1+t2,t1xL\nsince [eUt1]ij ∼N(0, τ 2\n0 ) and [Ξt1+t2,t1]i ∼N(0, σ2\nt1+t2,t1),\nVar\n\u0010\nX⊤eUt1+t2xL\n\u0011\n≳τ 2\n0 ∥X∥2\nF∥xL∥2\n2\nVar\n\u0000X⊤Ξt1+t2,t1xL\n\u0001\n≲η2τ 2\n0\nη1\n∥X∥2\nF∥xL∥2\n2\nthen naturally we have\nPr\nh\n1\n\u0010\nX⊤eUt1+t2xL\n\u0011\n̸= 1\n\u0000X⊤Ξt1,t1+t2xL\n\u0001i\n≲\ns\nη2τ 2\n0 ∥X∥2\nF∥x∥2/η1\nτ 2\n0 ∥X∥2\nF∥x∥2\n=\nrη2\nη1\n(14)\nand\nE\nh\f\f\f1\n\u0010\n[X⊤]i eUt1+t2xL\n\u0011\n−1\n\u0010\n[X⊤]i eUt1xL\n\u0011\f\f\f\ni\n= Pr\nh\n1\n\u0010\n[X⊤]i eUt1+t2xL\n\u0011\n̸= 1\n\u0010\n[X⊤]i eUt1xL\n\u0011i\n≲\nrη2\nη1\nUsing Hoeffding’s inequality in Lemma D.1, with probability at least 1 −1\nd,\n\r\r\r1\n\u0010\nX⊤eUt1+t2xL\n\u0011\n−1\n\u0010\nX⊤eUt1xL\n\u0011\r\r\r\n1 ≲2L\nrη2\nη1\n+\np\n4L log d\n≲L\nrη2\nη1\n+\np\nL log d\n(15)\n33\n\n\nCombine term A, B, C, Finally, with Equation 11, 12 and 15, we have\n\r\r1\n\u0000X⊤Ut1+t2xL\n\u0001\n−1\n\u0000X⊤Ut1xL\n\u0001\r\r\n1 ≲ϵU + L\nrη2\nη1\n+\np\nL log d\nwhere ϵU = K4/3λ−4/3τ −4/3\n0\nL2/3.\nFurthermore, with Proposition D.10,\n\f\f\fNUt1+t2(U t1+t2; X, eY ) −NUt1(U t1+t2; X, eY )\n\f\f\f\n= 1\nL\nX\ni∈[L]\n|[Y ]i|\n\f\f1\n\u0000[X⊤]iUt1+t2xL\n\u0001\n−1\n\u0000[X⊤]iUt1xL\n\u0001\f\f \f\f[X⊤]iU t1+t2xL\n\f\f\n≤1\nL\n\r\r1\n\u0000X⊤Ut1+t2xL\n\u0001\n−1\n\u0000X⊤Ut1xL\n\u0001\r\r\n1 max\ni\n\f\f[X⊤U t1+t2]ixL\n\f\f\n≲\n\u0012\nϵU + L\nrη2\nη1\n+\np\nL log d\n\u0013 K(u + m)2\nLλ\nCorollary D.16. Let X1 ∈Rd×L, xL,1 ∈Rd be a fixed example, with Assumption D.6 and Proposi-\ntion D.10, ∥xL,1∥2 ≤u + γ0 and ∥X1∥F ≤\n√\nL(u + γ0). Then, w.h.p over the randomness of f\nW\nand Y , ∀W ∈Rd×d, we have that\n\r\r1\n\u0000X⊤\n1 Wt1+t2xL,1\n\u0001\n−1\n\u0000X⊤\n1 Wt1xL,1\n\u0001\r\r\n1 ≲ϵW + L\nrη2\nη1\n+\np\nL log d\nwhere ϵW = K4/3λ−4/3τ −4/3\n0\nL2/3. Furthermore,\n\f\fNWt1+t2(W t1+t2; X1, Y ) −NWt1(W t1+t2; X1, Y )\n\f\f ≲\n\u0012\nϵW + L\nrη2\nη1\n+\np\nL log d\n\u0013 K(u + γ0)2\nLλ\nCorollary D.17. Let X2 ∈Rd×L, xL,2 ∈Rd be a fixed example, with Assumption D.6 and Proposi-\ntion D.10, ∥xL,2∥2 ≤u + r and ∥X2∥F ≤\n√\nL(u + r). Then, w.h.p over the randomness of eV and\nY , ∀V ∈Rd×d, we have that\n\r\r1\n\u0000X⊤\n2 Vt1+t2xL,2\n\u0001\n−1\n\u0000X⊤\n2 Vt1xL,2\n\u0001\r\r\n1 ≲ϵV + L\nrη2\nη1\n+\np\nL log d\nwhere ϵV = K4/3λ−4/3τ −4/3\n0\nL2/3. Furthermore,\n\f\fNVt1+t2(V t1+t2; X2, Y ) −NVt1(V t1+t2; X2, Y )\n\f\f ≲\n\u0012\nϵV + L\nrη2\nη1\n+\np\nL log d\n\u0013 K(u + r)2\nLλ\nProposition D.18. Under the same setting as Lemma D.12, we have w.h.p over the randomness of\neU,\n\f\f\fNeU(eU; X, Y )\n\f\f\f ≲τ0(u + m)2\nr\nd log d\nL\n34\n\n\nProof. We have\nNeU(eU; X, eY ) = 1\n2L\nX\ni∈[2L]\n[eY ]i\nh\n[X⊤]i eUxL\ni\n+\nWith Lemma D.3, we have ∥eU∥≲τ0\n√\nd. Then\n\r\r\r\r\nh\n[X⊤]i eUxL\ni\n+\n\r\r\r\r\n2\n≤\n\r\r\r[X⊤]i eUxL\n\r\r\r\n2 ≲τ0\n√\nd∥x∥2\n2\nUsing Hoeffding’s inequality in Lemma D.1, since [Y ]i ∈{−1, 1}, mi = −1\n2L\n\r\r\r\r\nh\n[X⊤]i eUxL\ni\n+\n\r\r\r\r\n2\n, Mi =\n1\n2L\n\r\r\r\r\nh\n[X⊤]i eUxL\ni\n+\n\r\r\r\r\n2\n, then we have\nPr\n\n\n\f\f\f\f\f\f\n1\n2L\nX\ni∈[2L]\n[Y ]i\nh\n[X⊤]i eUxL\ni\n+\n\f\f\f\f\f\f\n≥t\n\n≤2 exp\n\n\n\n\n−\n2t2\nP\ni∈[2L]\n\u0012\n2 ·\n1\n2L\n\r\r\r\r\nh\n[X⊤]i eUxL\ni\n+\n\r\r\r\r\n2\n\u00132\n\n\n\n\n\n≤2 exp\n\n\n\n\n−\n2t2\n1\nL2\nP\ni∈[2L]\n\r\r\r\r\nh\n[X⊤]i eUxL\ni\n+\n\r\r\r\r\n2\n2\n\n\n\n\n\n≲2 exp\n \n−\nt2\n1\nL(τ0\n√\nd∥x∥2)2\n!\nLet δ = 2 exp\n\u0010\n−\nt2\n1\nL (τ0\n√\nd∥x∥2)2\n\u0011\n, then with δ = 1\nd\nt =\nr\n1\nL(τ0\n√\nd∥x∥2)2 log 2\nδ\n≲τ0\n√\nd∥x∥2\nr\n1\nL log 2\nδ\n= τ0∥x∥2\nr\nd log d\nL\nThus, with 1 −δ prob, we get\n\f\f\fNeU(eU; X, Y )\n\f\f\f =\n\f\f\f\f\f\f\n1\n2L\nX\ni∈[2L]\n[Y ]i\nh\n[X⊤]i eUxL\ni\n+\n\f\f\f\f\f\f\n≲τ0∥x∥2\nr\nd log d\nL\nSince ∥x∥2 ≤u + m, then\n\f\f\fNeU(eU; X, Y )\n\f\f\f ≲τ0(u + m)2\nr\nd log d\nL\n35\n\n\nProposition D.19. Under the same setting as Lemma D.12, with Proposition D.18, we have w.h.p\nover the randomness of eU, ∀U ∈R2d×2d,\n\f\f\fNU(eU; X, eY ) −NeU(eU; X, eY )\n\f\f\f ≲(u + m)2K7/3λ−7/3τ −4/3\n0\nL−1/3\nand\n\f\f\fNU(eU; X, eY )\n\f\f\f ≲(u + m)2K7/3λ−7/3τ −4/3\n0\nL−1/3 + τ0(u + m)2\nr\nd log d\nL\nProof. For every i, 1([X⊤U]ixL) ̸= 1([X⊤eU]ixL), it holds that |[X⊤eU]ixL| ≤|[X⊤U]ixL|. Then\n\f\f\fNU(eU; X, eY ) −NeU(eU; X, eY )\n\f\f\f =\n\r\r\reY /2L ·\n\u0010\n1\n\u0000X⊤UxL\n\u0001\n−1\n\u0010\nX⊤eUxL\n\u0011\u0011\n⊙\n\u0010\nX⊤eUxL\n\u0011\r\r\r\n≤1\n2L\nX\ni∈[2L]\n\f\f\f[eY ]i\n\f\f\f\n\f\f\f1\n\u0000[X⊤]iUxL\n\u0001\n−1\n\u0010\n[X⊤]i eUxL\n\u0011\f\f\f\n\f\f\f[X⊤]i eUxL\n\f\f\f\n≤1\n2L\n\r\r\r1\n\u0000X⊤UxL\n\u0001\n−1\n\u0010\nX⊤eUxL\n\u0011\r\r\r\n1 max\ni\n\f\f[X⊤U]ixL\n\f\f\n≲K4/3λ−4/3τ −4/3\n0\nL−1/3K(u + m)2\nλ\n≲(u + m)2K7/3λ−7/3τ −4/3\n0\nL−1/3\nWith Proposition D.18, using triangle inequality, we have\n\f\f\fNU(eU; X, eY )\n\f\f\f ≲(u + m)2K7/3λ−7/3τ −4/3\n0\nL−1/3 + τ0(u + m)2\nr\nd log d\nL\n= K(u + m)2λ−1ϵU + τ0(u + m)2\nr\nd log d\nL\nCorollary D.20. Let X1 ∈Rd×L, xL,1 ∈Rd be a fixed example, with Assumption D.6 and Proposi-\ntion D.19, ∥xL,1∥2 ≤u + γ0 and ∥X1∥F ≤\n√\nL(u + γ0). Then, w.h.p over the randomness of f\nW\nand Y , ∀W ∈Rd×d,\n\f\f\fNW(f\nW; X1, Y )\n\f\f\f ≲(u + γ0)2K7/3λ−7/3τ −4/3\n0\nL−1/3 + τ0(u + γ0)2\nr\nd log d\nL\nWith choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand L = Θ (Poly(d)), then\n\f\f\fNW(f\nW; X1, Y )\n\f\f\f ≲τ0(u + γ0)2\nr\nd log d\nL\n≜ϵW,1\n(16)\nNote. In ϵW,1, τ0 denotes the variance of initialization parameter, L is prompt length and d represents\nthe input dimension. When with choices in Assumption 4.1, we have ϵW,1 = Θ\n\u0010\n1\nPoly(d)\n\u0011\n.\n36\n\n\nCorollary D.21. Let X2 ∈Rd×L, xL,2 ∈Rd be a fixed example, with Assumption D.6 and Proposi-\ntion D.19, ∥xL,2∥2 ≤u + r and ∥X2∥F ≤\n√\nL(u + r). Then, w.h.p over the randomness of eV and\nY , ∀V ∈Rd×d,\n\f\f\fNV (eV ; X2, Y )\n\f\f\f ≲(u + r)2K7/3λ−7/3τ −4/3\n0\nL−1/3 + τ0(u + r)2\nr\nd log d\nL\nWith choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand L = Θ (Poly(d)), then\n\f\f\fNV (eV ; X1, Y )\n\f\f\f ≲τ0(u + r)2\nr\nd log d\nL\n≜ϵV,1\n(17)\nNote. In ϵV,1, τ0 denotes the variance of initialization parameter, L is prompt length and d represents\nthe input dimension. When with choices in Assumption 4.1, we have ϵV,1 = Θ\n\u0010\n1\nPoly(d)\n\u0011\n.\n37\n\n\nD.3\nProof for the Elementary Stage: Proof of Theorem 4.2\nTheorem 4.2. In the elementary stage with η1 = Θ(1) and t1 =\n1\n4η1λ where λ denotes regularization\ncoefficients. With Assumption 4.1, initial weights V0 −→0d×d and N training prompts, it holds that\n(a.1) For the model parameter V of network g, through gradient descent, ∥V t1∥F satisfies\n∥V t1∥F = Θ\n\u0012\n1\nPoly(d)\n\u0013\n.\n(a.2) With random and small noise weight, the training loss of nonlinear separable component Q\nover signal weight (Definition in Equation 6) at iteration t1 satisfies\nK2\nt1\n\u0000V t1\n\u0001\n≳log 2 −\n1\n√log d −\nr\nlog d\nN .\nNamely, the nonlinear separable component Q is not efficiently learned by the network g within t1\niterations.\nRemark D.22 (Proof Sketch). We summarize the proof sketch and main techniques in Proof of\nTheorem 4.2. At the starting point, using signal-noise decomposition technique, we assume that\nthe approximate output eg uses noise part to compute activation and signal part as the weight to\ncompute attention score. We show that eg is very close to g primarily through Corollary D.14 and\nD.21. Relevant corollaries are crucial for describing the differences in activation and network\noutput under various activation and weight schemes. In the following analysis, we turn to focus\non the approximation eg. As a key step, we focus on the network g’s ability to distinguish between\npositive and negative class samples by examining the differences in their respective outputs, i.e.\n|egt(X2, z −ζ) + egt(X2, z + ζ) −2egt(X2, z)|. Decompose it into two parts Φ and Ψ, where each\npart separately contains z and ζ. Then, give the upper bound of Φ and Ψ by applying concentration\ninequalities like Chernoff, Bernstein and complex probability analysis like Gaussian integrals.\nCombining the above, we show that the prediction difference of the network for positive and negative\nsamples is upper bounded by a small value, 1/√log d. Consequently, we derive a straightforward\nlower bound 2 −1/√log d, demonstrating that the network g cannot simultaneously make accurate\npredictions for both positive and negative samples.\nFrom the network output, we further derive the changes in weight and loss. For (a.1) and (a.2): At\nan initial step, to compute the high-probability proportions for query xL,2 = z′ = {z−ζ, z+ζ} and\nxL,2 = z, we express the training loss in terms of the network outputs for positive and negative class\nsamples based on the proportion, dividing it into two parts with terms gt1(X2, z′) and gt1(X2, z)\nrespectively. As an essential step, by leveraging the convexity and Lipschitz properties of the\nlogistic loss, we derive a lower bound for the training loss in (a.2). Using Taylor expansion\ntechniques in combination with this lower bound, we further deduce a corollary of Theorem 4.2,\nwhich states: |gt1(X2, z)|, |gt1(X2, z −ζ)|, |gt1(X2, z + ζ)| ≲\n1\n(log d)1/4. By utilizing the expression\nof normalized ReLU self-attention, this corollary can be further extended to give (a.1).\n38\n\n\nProof. Using noise part to compute activation and signal part as weight.\negt(X2) = NeVt(V t; X2, Y )\n= Y\n\u0010\n1\n\u0010\nX⊤\n2 eVtxL,2\n\u0011\n⊙\n\u0000X⊤\n2 V txL,2\n\u0001\u0011\nUsing triangle inequality, with Corollary D.14 and D.21,\n|gt(X2) −egt(X2)|\n=\n\f\fNVt(Vt; X2, Y ) −NeVt(V t; X2, Y )\n\f\f\n=\n\f\f\fNVt(V t; X2, Y ) + NVt(eVt; X2, Y ) −NeVt(V t; X2, Y )\n\f\f\f\n≤\n\f\fNVt(V t; X2, Y ) −NeVt(V t; X2, Y )\n\f\f +\n\f\f\fNVt(eVt; X2, Y )\n\f\f\f\n≲(u + r)2K7/3λ−7/3τ −4/3\n0\nL−1/3 + (u + r)2K7/3λ−7/3τ −4/3\n0\nL−1/3 + τ0(u + r)2\nr\nd log d\nL\nWith choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand L = Θ (Poly(d)),\n|gt(X2) −egt(X2)| ≲(√log d)11/3\n(Poly(d))1/3 +\n1\n√log d\ns\nd log d\nPoly(d)\n≲\n1\nPoly(d)\nIn the following, we focus on egt(X2).\nDefinition D.23. For any time t, input X ∈Rd×L with query xL ∈Rd, define ϵX,xL\nt\n≜{i ∈[L] :\n[X⊤]i eVtxL ≥0} and ϵX,xL\nt\n≜{i ∈[L] : [X⊤]i eVtxL < 0}. Note that X aligns with X2 and xL\naligns with xL,2. Then 1(ϵ) ⊂{0, 1}L. Naturally, we have\n1(ϵX,xL\nt\n) = 1(X⊤eVtxL).\nLet Qt = diag(Y ⊤)X⊤\n2 V t, then\negt(X2) = NeVt(V t; X2, Y )\n= Y/L\n\u0010\n1\n\u0010\nX⊤\n2 eVtxL,2\n\u0011\n⊙\n\u0000X⊤\n2 V txL,2\n\u0001\u0011\n= 1/L · 1\n\u0010\nX⊤\n2 eVtxL,2\n\u0011⊤\u0000diag(Y ⊤)X⊤\n2 V t\n\u0001\nxL,2\n= 1/L · 1\n\u0010\nX⊤\n2 eVtxL,2\n\u0011⊤\nQtxL,2\nTo simplify, we use X that represents X2 and xL represents xL,2, in this Lemma, if there are\nno confusion.\n39\n\n\nDefine egt(X, z −ζ) as sequence X with xL = z −ζ, similarly for egt(X, z + ζ) and egt(X, z). Then\nwith Definition D.23,\n|egt(X, z −ζ) + egt(X, z + ζ) −2egt(X, z)|\n=1/L ·\n\f\f\f\f1\n\u0010\nϵX,z−ζ\nt\n\u0011⊤\nQt(z −ζ) + 1\n\u0010\nϵX,z+ζ\nt\n\u0011⊤\nQt(z + ζ) −21\n\u0010\nϵX,z\nt\n\u0011⊤\nQtz\n\f\f\f\f\n≤1/L ·\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\nQtz\n\f\f\f\f\n|\n{z\n}\nΦ\n+1/L ·\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z−ζ\nt\n\u0011\u0011⊤\nQtζ\n\f\f\f\f\n|\n{z\n}\nΨ\nDeal with term Ψ.\nFirst, consider the second term\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z−ζ\nt\n\u0011\u0011⊤\nQtζ\n\f\f\f\f. With\nAssumption D.6 that ∥ζ∥2 = r,\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z−ζ\nt\n\u0011\u0011⊤\nQtζ\n\f\f\f\f ≤\n\r\r\r\r\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z−ζ\nt\n\u0011\u0011⊤\nQt\n\r\r\r\r\n2\n∥ζ∥2\n≤r\n\f\f\fϵX,z+ζ\nt\n⊕ϵX,z−ζ\nt\n\f\f\f · max ∥[Qt]i∥2\nFor ϵX,z+ζ\nt\n⊕ϵX,z−ζ\nt\nin term Ψ.\nFor i ∈ϵX,z+ζ\nt\n⊕ϵX,z−ζ\nt\n, with [X⊤]i eVt(z+ζ) ≥0 and [X⊤]i eVt(z−\nζ) ≤0, then\n−[X⊤]i eVtζ ≤[X⊤]i eVtz ≤[X⊤]i eVtζ\n\f\f\f[X⊤]i eVtz\n\f\f\f ≤\n\f\f\f[X⊤]i eVtζ\n\f\f\f\nUsing chernoff bound for Gaussian variable in Lemma D.5, let δ = 2 exp\n\u0010\n−t2\n2σ2\n\u0011\n=\n1\nd, then\nt = σ\nq\n2 log 2\nδ = σ√2 log 2d. Substitute eVt, given that it is a Gaussian vector with each component\n[eVt]ij ∼N(0, τ 2\n0 ), we have w.h.p 1 −δ\n\f\f\f[X⊤]i eVtζ\n\f\f\f ≤r(u + r)|eVt| ≤τ0r(u + r)\np\nlog d\n\f\f\f[X⊤]i eVtz\n\f\f\f ≤\n\f\f\f[X⊤]i eVtζ\n\f\f\f ≤τ0r(u + r)\np\nlog d\ni.e., Pr\n\u0010\f\f\f[X⊤]i eVtz\n\f\f\f ≤τ0r(u + r)√log d\n\u0011\n≳1 −1\nd.\nIn the following, we try to give the upper bound of Pr\n\u0010\f\f\f[X⊤]i eVtz\n\f\f\f ≤τ0r(u + r)√log d\n\u0011\n. Define\nthe standardized variable [X⊤eVt]iz\nτ0u(u+r) ∼N(0, 1). We have Pr(|X| ≤a) = 2Φ(a) −1 where Φ is CDF.\nof standard Gaussian random variable. Substituting [X⊤eVt]iz\nτ0u(u+r) and a = r√log d\nu\n, then with large d (i.e.\n40\n\n\nlarge a),\nPr\n\u0010\n|[X⊤eVt]iz| ≤τ0r(u + r)\np\nlog d\n\u0011\n= Pr\n \f\f\f\f\f\n[X⊤eVt]iz\nτ0u(u + r)\n\f\f\f\f\f ≤τ0r(u + r)√log d\nτ0u(u + r)\n!\n= 2Φ\n\u0012r√log d\nu\n\u0013\n−1\n≈2 · r√log d\nu\n√\n2π\n≲r√log d\nu\ni.e., Pr\n\u0010\f\f\f[X⊤]i eVtz\n\f\f\f ≤τ0r(u + r)√log d\n\u0011\n≲r√log d\nu\n.\nWith Bernstein inequality in Lemma D.2, define new random variable Ri = I(|[X⊤eVt]iz| ≤τ0r(u +\nr)√log d) where I(·) is the indicator function, E [Ri] = Pr(|[X⊤eVt]iz| ≤τ0r(u + r)√log d) ≲\nr√log d\nu\n. Then w.h.p. 1 −δ = 1 −1\nd we have\n1\nL\nL\nX\ni=1\nRi −E [Ri] ≤\nr\n2σ2 log(1/δ)\nL\n+ 2c log(1/δ)\n3L\nL\nX\ni=1\nRi ≤L\nr\n2σ2 log(1/δ)\nL\n+ L2c log(1/δ)\n3L\n+ rL√log d\nu\n≲\np\nL log d + log d + rL√log d\nu\ni.e. |ϵX,z−ζ\nt\n⊕ϵX,z+ζ\nt\n| ≲√L log d + log d + rL√log d\nu\n. For sufficiently large L,\n|ϵX,z−ζ\nt\n⊕ϵX,z+ζ\nt\n| ≲rL√log d\nu\n(18)\nFor [Qt]i in term Ψ.\nFor Qt = diag(Y ⊤)X⊤V t, using Cauchy-Schwarz inequality, Assumption\nD.6 and Proposition D.10,\n∥[Qt]i∥2 =\n\r\r[Y ⊤]i[X⊤V t]i\n\r\r\n2 =\n\r\r\r\r\ryi\nd\nX\nj=1\n[X⊤]ij[V t]j\n\r\r\r\r\r\n2\n≤∥[X]i∥2∥V t∥F\n≲K(u + r)\nλ\n(19)\n41\n\n\nCombine Equation 18 and 19.\nFor term B, we have\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z−ζ\nt\n\u0011\u0011⊤\nQtζ\n\f\f\f\f ≤\n\r\r\r\r\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z−ζ\nt\n\u0011\u0011⊤\nQt\n\r\r\r\r\n2\n∥ζ∥2\n≤r\n\f\f\fϵX,z+ζ\nt\n⊕ϵX,z−ζ\nt\n\f\f\f · max ∥[Qt]i∥2\n≲rL√log d\nu\n· K(u + r)\nλ\n≲r(u + r)KL√log d\nuλ\nSince then, we have completed term Ψ in Equation.\nDeal with term Φ.\nConsider term Φ =\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\nQtz\n\f\f\f\f in this\npart. Let a =\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\n, then\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\nQtz = a⊤Qtz\nAccording to the definition of Qt and V t, we have\na⊤Qt = a⊤diag(Y ⊤)X⊤V t\n= a⊤diag(Y ⊤)X⊤\nt\nX\nτ=1\nη1 (1 −η1λ)t−τ ∇Vτ−1 bL(Uτ−1)\n= a⊤\nt\nX\nτ=1\nη1 (1 −η1λ)t−τ ∆Qτ−1\nwhere ∆Qτ = diag(Y ⊤)X⊤∇Vτ bL(Uτ). Then\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\nQtz\n\f\f\f\f\n≤η1u\nt\nX\nτ=1\n\r\r\r\r\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\n∆Qτ−1\n\r\r\r\r\n2\nFor ∆Qτ in term Φ.\nDefinition D.24. For any time t, input X ∈Rd×L with query xL ∈Rd, define GX,xL\nτ\n≜{i ∈[L] :\n[X⊤]iVτxL ≥0} and G\nX,xL\nτ\n≜{i ∈[L] : [X⊤]iVτxL < 0}. Similar to Definition D.23, note that X\naligns with X2 and xL aligns with xL,2.\nSuppose i, j satisfy that, for input xL = z −ζ and xL = z + ζ have the same activation pattern,\nthen with Definition D.24 we have\ni, j ∈GX,z−ζ\nτ\n∩GX,z+ζ\nτ\nor i, j ∈G\nX,z−ζ\nτ\n∩G\nX,z+ζ\nτ\n42\n\n\nConsider the relationship between [∆Qτ]i and [∆Qτ]j for the above i, j.\nWe have ∆Qτ =\ndiag(Y ⊤)X⊤∇Vτ bL(Uτ), then\n[∆Qτ]i =\n\u0002\ndiag(Y ⊤)\n\u0003\ni\nh\nX⊤∇Vτ bL(Uτ)\ni\ni = yi\nh\nX⊤∇Vτ bL(Uτ)\ni\ni\n[∆Qτ]j =\n\u0002\ndiag(Y ⊤)\n\u0003\nj\nh\nX⊤∇Vτ bL(Uτ)\ni\nj = yj\nh\nX⊤∇Vτ bL(Uτ)\ni\nj\nWith Proposition D.8, then\n[∆Qτ]i = yi[X⊤∇Vτ bL(Uτ)]i = yi[X⊤]ibE\n\u0002\n1/2L · l′(f(Uτ; X, Y ))1([X⊤]iUτxL)[X]ix⊤\nL\n\u0003\n[∆Qτ]j = yj[X⊤∇Vτ bL(Uτ)]j = yj[X⊤]jbE\n\u0002\n1/2L · l′(f(Uτ; X, Y ))1([X⊤]jUτxL)[X]jx⊤\nL\n\u0003\nThus for xL ∈{0, z, z −ζ, z + ζ}. If xL = 0, [∆Qτ]i = [∆Qτ]j. For all xL ∈{z, z −ζ, z + ζ},\ni, j ∈GX,z−ζ\nτ\n∩GX,z+ζ\nτ\n, and then i, j ∈GX,z\nτ\n. Thus,\n1([X⊤]iVτxL) = 1([X⊤]jVτxL) = 1\nFor fixed X, [∇Vτ bL(Uτ)]i = [∇Vτ bL(Uτ)]j. If [X]i = [X]j, then yi = yj,\n[∆Qτ]i = [∆Qτ]j\nIf [X]i, [X]j = z −ζ, z + ζ, then yi = yj,\n[∆Qτ]i = (z −ζ)C, [∆Qτ]j = (z + ζ)C\n[∆Qτ]i = (z + ζ)C, [∆Qτ]j = (z −ζ)C\n[∆Qτ]i −[∆Qτ]j = ±2ζC\nwhere C = bE\n\u0002\nl′(f(Uτ; X, Y ))1([X⊤]iUτxL)(z ± ζ)x⊤\nL\n\u0003\n. If [X2]i, [X2]j = z ±ζ, z, then yi = −yj,\n[∆Qτ]i = (z ± ζ)C, [∆Qτ]j = zC\n[∆Qτ]i = zC, [∆Qτ]j = (z ± ζ)C\n[∆Qτ]i −[∆Qτ]j = (−2z ± ζ)C, ±ζC\nwhere C = bE\n\u0002\nl′(f(Uτ; X, Y ))1([X⊤]iUτxL)(z(±ζ))x⊤\nL\n\u0003\n.\nFor\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\n∆Qτ in term Φ.\nWith Definition D.23, we have\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\n=1\n\u0010\nϵX,z−ζ\nt\n∩ϵX,z\nt\n\u0011\n+ 1\n\u0010\nϵX,z−ζ\nt\n\\ ϵX,z\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n∩ϵX,z\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\\ ϵX,z\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n∩ϵX,z−ζ\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z−ζ\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n∩ϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z+ζ\nt\n\u0011\n=1\n\u0010\nϵX,z−ζ\nt\n\\ ϵX,z\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\\ ϵX,z\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z−ζ\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z+ζ\nt\n\u0011\n= 1\n\u0010\nϵX,z+ζ\nt\n\\ ϵX,z\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z−ζ\nt\n\u0011\n|\n{z\n}\nPart I\n+ 1\n\u0010\nϵX,z−ζ\nt\n\\ ϵX,z\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z+ζ\nt\n\u0011\n|\n{z\n}\nPart II\n43\n\n\nObserve that Part I and Part II are similar, and we deal with Part I first. Let A = ϵX,z+ζ\nt\n\\ ϵX,z\nt\nand\nB = ϵX,z\nt\n\\ ϵX,z−ζ\nt\n. Similar to Definition D.23, we give the following definition to divide sets A and\nB, based on the above high probability results that is\n\f\f\f[X⊤]i eVtz\n\f\f\f ≲τ0r(u + r)√log d.\nDefinition D.25. For any time τ, input X ∈Rd×L with query xL = z ∈Rd, define F+\nτ ≜{i ∈\n[L] : [X⊤]i eVτz ≳τ0r(u + r)√log d}, F−\nτ ≜{i ∈[L] : [X⊤]i eVτz ≲−τ0r(u + r)√log d} and\nFc\nτ ≜{i ∈[L] :\n\f\f\f[X⊤]i eVτz\n\f\f\f ≲τ0r(u + r)√log d}. Similar to Definition D.23, note that X aligns\nwith X2.\nWith Definition D.25,\n\r\r\r\r\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\\ ϵX,z\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z−ζ\nt\n\u0011\u0011⊤\n∆Qτ\n\r\r\r\r\n2\n=\n\r\r\r\r\r\nX\ni∈A\n[∆Qτ]i −\nX\ni∈B\n[∆Qτ]i\n\r\r\r\r\r\n2\n≤\n\r\r\r\r\r\r\nX\ni∈A∩F+\nτ\n[∆Qτ]i −\nX\ni∈B∩F+\nτ\n[∆Qτ]i\n\r\r\r\r\r\r\n2\n+\n\r\r\r\r\r\r\nX\ni∈A∩F−\nτ\n[∆Qτ]i −\nX\ni∈B∩F−\nτ\n[∆Qτ]i\n\r\r\r\r\r\r\n2\n+\n\r\r\r\r\r\r\nX\ni∈A∩Fcτ\n[∆Qτ]i −\nX\ni∈B∩Fcτ\n[∆Qτ]i\n\r\r\r\r\r\r\n2\nWe have introduced the relationship between [∆Qτ]i and [∆Qτ]j for i, j ∈GX,z−ζ\nτ\n∩GX,z+ζ\nτ\n. In the\nfollowing, we show that if k, l ∈F+\nτ (similar for F−\nτ and Fc\nτ) then k, l ∈GX,z−ζ\nτ\n∩GX,z+ζ\nτ\n, thus we\nhave the same conclusion for [∆Qτ]k and [∆Qτ]l.\nSuppose k, l satisfy that, when x ∈{z −ζ, z + ζ}\n[X⊤]k eVτx ≳τ0r(u + r)\np\nlog d\n[X⊤]l eVτx ≳τ0r(u + r)\np\nlog d\nNaturally, we have [X⊤]k eVτz ≳τ0r(u+r)√log d and [X⊤]l eVτz ≳τ0r(u+r)√log d, i.e., k, l ∈F+\nt .\nThen\n−\n\f\f[X⊤]kV τz\n\f\f ≤[X⊤]kV τz = [X⊤]k(Vτ −eVτ)z ≤\n\f\f[X⊤]kV τz\n\f\f\nand with Assumption D.6 and Proposition D.10,\n[X⊤]kVτz ≥[X⊤]k eVτz −\n\f\f[X⊤]kV τz\n\f\f\n≥τ0r(u + r)\np\nlog d −u(u + r)K\nλ\n≳τ0r(u + r)\np\nlog d\nwhere the last inequality comes from 1\nλ = O(√log d). Since {[X⊤]kVτz ≳τ0r(u + r)√log d} ⊂\n{[X⊤]kVτz ≥0} ⊂GX,z−ζ\nτ\n∩GX,z+ζ\nτ\n, then we have k, l ∈GX,z−ζ\nτ\n∩GX,z+ζ\nt\n. Thus, if k, l ∈\nF+\nτ , F−\nτ , Fc\nτ, [∆Qτ]k and [∆Qτ]l hold the same conclusion as [∆Qτ]i and [∆Qτ]j.\n44\n\n\nTherefore, with the definition of data structure, assume that the probability of [X]i = [X]j, i.e.\n[∆Qτ]i = [∆Qτ]j, is P, then\n\r\r\r\r\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\\ ϵX,z\nt\n\u0011\n−1\n\u0010\nϵX,z\nt\n\\ ϵX,z−ζ\nt\n\u0011\u0011⊤\n∆Qτ\n\r\r\r\r\n2\n=\n\r\r\r\r\r\nX\ni∈A\n[∆Qτ]i −\nX\ni∈B\n[∆Qτ]i\n\r\r\r\r\r\n2\n≤\n\r\r\r\r\r\r\nX\ni∈A∩F+\nτ\n[∆Qτ]i −\nX\ni∈B∩F+\nτ\n[∆Qτ]i\n\r\r\r\r\r\r\n2\n+\n\r\r\r\r\r\r\nX\ni∈A∩F−\nτ\n[∆Qτ]i −\nX\ni∈B∩F−\nτ\n[∆Qτ]i\n\r\r\r\r\r\r\n2\n+\n\r\r\r\r\r\r\nX\ni∈A∩Fcτ\n[∆Qτ]i −\nX\ni∈B∩Fcτ\n[∆Qτ]i\n\r\r\r\r\r\r\n2\n≤max ∥[∆Qτ]i∥2\n\u0000|A ∩F+\nτ | + |B ∩F+\nτ | + |A ∩F−\nτ | + |B ∩F−\nτ | + |A ∩Fc\nτ| + |B ∩Fc\nτ|\n\u0001\n≤(u + r)K\n\u0010\nP\n\f\f|A ∩F+\nτ | −|B ∩F+\nτ |\n\f\f + P\n\f\f|A ∩F−\nτ | −|B ∩F−\nτ |\n\f\f + (1 −P)\n\u0000|A ∩F+\nτ | + |B ∩F+\nτ |\n\u0001\n+ (1 −P)\n\u0000|A ∩F−\nτ | + |B ∩F−\nτ |\n\u0001\n+ |A ∩Fc\nτ| + |B ∩Fc\nτ|\n\u0011\nFor |A∩F+\nτ |, |B∩F+\nτ | and ||A ∩F+\nτ | −|B ∩F+\nτ ||.\nIt is related to [X⊤]i eVtz, [X⊤]i eVtζ, [X⊤]i eVτz.\nAt time τ ≤t, we can establish the relationship of [X⊤]i eVτz, [X⊤]i eVtz. With Proposition D.11 and\nη = η1, we have\n[X⊤]i eVtz = (1 −η1λ)t−τ[X⊤]i eVτz −\nt−τ\nX\nt′=1\n(1 −η1λ)t−τ−t′[X⊤]iζτ+t′−1z\n= (1 −η1λ)t−τ[X⊤]i eVτz + [X⊤]iΞt,τz\nwhere Ξt,τ = −Pt−τ\nt′=1 η1(1 −η1λ)t−τ−t′ζτ+t′−1. Let Y1 = [X⊤]i eVtz, Y2 = [X⊤]i eVτz, Y3 =\n[X⊤]i eVtζ, Y4 = [X⊤]iΞt,τz, β = (1 −η1λ)t−τ ≲1, we have Y1 = Y4 + βY2.\nConsider Y1, given that [eVτ]ij ∼N(0, τ 2\n0 ), then\nVar([X⊤]i eVτz) = τ 2\n0 ∥z∥2\n2\nX\nj\nX2\nji = τ 2\n0 ∥z∥2\n2∥[X]i∥2\n2\nWith Assumption D.6, we have Y2 ∼N(0, τ 2\n0 u2(u + r)2). Similarly, Y1 ∼N(0, τ 2\n0 u2(u + r)2),\nY3 ∼N(0, τ 2\n0 r2(u + r)2)\nConsider Y4, denote its variance as σt,τ.\nVar([X⊤]i eVτz) = (1 −η1λ)2(t−τ)Var([X⊤]i eVτz) + Var([X⊤]iΞt,τz)\nτ 2\n0 u2(u + r)2 = (1 −η1λ)2(t−τ)τ 2\n0 u2(u + r)2 + σ2\nt,τ\nσt,τ =\nq\nτ 2\n0 u2(u + r)2 (1 −(1 −η1λ)2(t−τ)) ≳τ0u(u + r)\np\nη1λ(t −τ)\n45\n\n\nLet κ = τ0r(u + r)√log d, with Chernoff bound for Gaussian Variable in Lemma D.5, and we have\nGaussian Integral that\nR ∞\n−∞e−ax2 = p π\na, then\nPr(A ∩F+\nτ ) = Pr[i ∈ϵX,z+ζ\nt\n, i /∈ϵx,z\nt , i ∈F+\nτ ]\n= Pr [Y2 + Y3 ≥0, Y2 ≤0, Y1 ≥κ]\n= Pr [Y2 + Y3 ≥0, Y2 ≤0, Y4 ≥κ −βY2]\n= EY2 [Pr [Y3 ≥−Y2 | Y2, Y2 ≤0, Y4 ≥κ −βY2 | Y2]]\n= EY2 [Pr [Y3 ≥−Y2 | Y2] 1(Y2 ≤0) Pr [Y4 ≥κ −βY2 | Y2]]\n≲\nZ 0\n−∞\ne\n−\nz2\n2τ2\n0 r2(u+r)2 e\n−(κ−βz)2\n2σ2\nt,τ dz ≲\nZ 0\n−∞\ne\n\u0012\n−\n1\n2τ2\n0 r2(u+r)2 −\nβ2\n2σ2\nt,τ\n\u0013\nz2\ndz\n≲\n√π\n2\nq\n1\n2τ 2\n0 r2(u+r)2 +\nβ2\n2σ2\nt,τ\n≲τ0r(u + r)\nPr(B ∩F+\nτ ) = Pr[i ∈ϵz\nt, i /∈ϵz−ζ\nt\n, i ∈F+\nτ ]\n= Pr [Y2 ≥0, Y2 −Y3 ≤0, Y1 ≥κ]\n= Pr [−Y2 ≥0, −Y2 −Y3 ≤0, −Y1 ≥κ]\n= EY2 [1(Y2 ≤0) Pr [Y3 ≥−Y2 | Y2] Pr [Y4 ≤−κ −βY2 | Y2]]\n= EY2 [1(Y2 ≤0) Pr [Y3 ≥−Y2 | Y2] Pr [Y4 ≥κ + βY2 | Y2]]\n≲\nZ 0\n−∞\ne\n−\nz2\n2τ2\n0 r2(u+r)2 e\n−(κ+βz)2\n2σ2\nt,τ dz ≲\nZ 0\n−∞\ne\n\u0012\n−\n1\n2τ2\n0 r2(u+r)2 −\nβ2\n2σ2\nt,τ\n\u0013\nz2\ndz\n≲\n√π\n2\nq\n1\n2τ 2\n0 r2(u+r)2 +\nβ2\n2σ2\nt,τ\n≲τ0r(u + r)\nUsing Bernstein inequality in Lemma D.2, to bound |A ∩F+\nτ | and |B ∩F+\nτ |. Suppose Mi = 1(i ∈\nϵz+ζ\nt\n, i /∈ϵz\nt, i ∈F+\nτ ) and Ni = 1(i ∈ϵz\nt, i /∈ϵz−ζ\nt\n, i ∈F+\nτ ).\n|A ∩F+\nτ | =\nL\nX\ni=1\nMi, |B ∩F+\nτ | =\nL\nX\ni=1\nNi\nE[|A ∩F+\nτ |] = E[Mi] = Pr(Mi) ≲τ0r(u + r),\nE[|B ∩F+\nτ |] = E[Ni] = Pr(Ni) ≲τ0r(u + r)\nThen with high probability at least 1 −δ, and let δ = 1\nd,\nL\nX\ni=1\nMi ≲\np\nL log d + log d + τ0r(u + r)L\nL\nX\ni=1\nNi ≲\np\nL log d + log d + τ0r(u + r)L\n46\n\n\nFinally, for L = Θ (Poly(d)), we conclude that\n|A ∩F+\nτ | ≲τ0r(u + r)L\n|B ∩F+\nτ | ≲τ0r(u + r)L\nFurthermore, we derive that\n\f\fPr(A ∩F+\nτ ) −Pr(B ∩F+\nτ )\n\f\f\n=\n\f\f\fPr[i ∈ϵz+ζ\nt\n, i /∈ϵz\nt, i ∈F+\nτ ] −Pr[i ∈ϵz\nt, i /∈ϵz−ζ\nt\n, i ∈F+\nτ ]\n\f\f\f\n=EY2 [1(Y2 ≤0) Pr [Y3 ≥−Y2 | Y2] Pr [κ −βY2 ≤Y4 ≤κ + βY2 | Y2]]\n≲EY2\n\"\n1(Y2 ≤0)e\n−\n|Y2|2\n2τ2\n0 r2(u+r)2 |Y2|\nσt,τ\n#\n≲\nZ 0\n−∞\ne\n−\nz2\n2τ2\n0 r2(u+r)2 |z|\nσt,τ\ndz ≲\n1\nσt,τ\nZ ∞\n0\nze\n−\nz2\n2τ2\n0 r2(u+r)2 dz\n≲τ 2\n0 r2(u + r)2\nσt,τ\nZ ∞\n0\ne−vdv\n≲τ 2\n0 r2(u + r)2\nσt,τ\n≲\nτ 2\n0 r2(u + r)2\nτ0u(u + r)\np\nη1λ(t −τ)\n≲\nτ0r2(u + r)\nu\np\nη1λ(t −τ)\nUsing Bernstein inequality in Lemma D.2, to bound ||A ∩F+\nτ | −|B ∩F+\nτ ||. Suppose Mi = 1(i ∈\nϵz+ζ\nt\n, i /∈ϵz\nt, i ∈F+\nτ ) and Ni = 1(i ∈ϵz\nt, i /∈ϵz−ζ\nt\n, i ∈F+\nτ ).\n\f\f|A ∩F+\nτ | −|B ∩F+\nτ |\n\f\f =\n\f\f\f\f\f\nL\nX\ni=1\n(Mi −Ni)\n\f\f\f\f\f\n\f\fE[|A ∩F+\nτ | −|B ∩F+\nτ |]\n\f\f = E [Mi −Ni]\n= |Pr(Mi) −Pr(Ni)|\n=\n\f\f\fPr[i ∈ϵz+ζ\nt\n, i /∈ϵz\nt, i ∈F+\nτ ] −Pr[i ∈ϵz\nt, i /∈ϵz−ζ\nt\n, i ∈F+\nτ ]\n\f\f\f\n≲\nτ0r2(u + r)\nu\np\nη1λ(t −τ)\nThen with high probability at least 1 −δ, and let δ = 1\nd,\n1\nL\nL\nX\ni=1\n(Mi −Ni) −E [Mi −Ni] ≤\nr\n2σ2 log(1/δ)\nL\n+ 2c log(1/δ)\n3L\nL\nX\ni=1\n(Mi −Ni) ≤L\nr\n2σ2 log(1/δ)\nL\n+ L2c log(1/δ)\n3L\n+ τ0r2(u + r)L\nu\np\nη1λ(t −τ)\nL\nX\ni=1\n(Mi −Ni) ≲\np\nL log d + log d + τ0r2(u + r)L\nu\np\nη1λ(t −τ)\n47\n\n\nFinally, for L = Θ (Poly(d)), we get that\n\f\f|A ∩F+\nτ | −|B ∩F+\nτ |\n\f\f ≲τ0r2(u + r)L\nu\np\nη1λ(t −τ)\nFor |A ∩F−\nτ |, |B ∩F−\nτ | and ||A ∩F−\nτ | −|B ∩F−\nτ ||.\nSimilar to the above part, we have\n|A ∩F−\nτ | ≲τ0r(u + r)L\n|B ∩F−\nτ | ≲τ0r(u + r)L\n\f\f|A ∩F−\nτ | −|B ∩F−\nτ |\n\f\f ≲τ0r2(u + r)L\nu\np\nη1λ(t −τ)\nFor |A ∩Fc\ns| and |B ∩Fc\ns|.\nPr[i ∈ϵz+ζ\nt\n, i /∈ϵz\nt, i ∈Fc\ns] = Pr [Y2 + Y3 ≥0, Y2 ≤0, |Y1| ≤κ]\n= E [Pr [Y2 + Y3 ≥0, Y2 ≤0, |Y4 −βY2| ≤κ]]\n= EY2\n\u0014\n1(Y2 ≤0) Pr [Y3 ≥−Y2 | Y2] · κ\nσs,t\n\u0015\n≲EY2\n\"\n1(Y2 ≤0)e\n−\n|Y2|2\n2τ2\n0 r2(u+r)2 κ\nσt,τ\n#\n≲τ0r(u + r)κ\nσt,τ\n√\n2π\n2\n≲τ0r(u + r)τ0r(u + r)√log d\nτ0u(u + r)\np\nη1λ(t −τ)\n≲τ0r2(u + r)√log d\nu\np\nη1λ(t −τ)\nSimilarly, using Bernstein inequality in Lemma D.2, |A ∩Fc\ns| ≲τ0r2(u+r)L√log d\nu√\nη1λ(t−τ)\n, and |B ∩Fc\ns| ≲\nτ0r2(u+r)L√log d\nu√\nη1λ(t−τ)\n.\nFinally,\n\r\r\r\r\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\n∆Qτ\n\r\r\r\r\n2\n≤(u + r)K\n\u0010\nP\n\f\f|A ∩F+\nτ | −|B ∩F+\nτ |\n\f\f + P\n\f\f|A ∩F−\nτ | −|B ∩F−\nτ |\n\f\f + (1 −P)\n\u0000|A ∩F+\nτ | + |B ∩F+\nτ |\n\u0001\n+ (1 −P)\n\u0000|A ∩F−\nτ | + |B ∩F−\nτ |\n\u0001\n+ |A ∩Fc\nτ| + |B ∩Fc\nτ|\n\u0011\n≲(u + r)K\n \n2P τ0r2(u + r)L\nu\np\nη1λ(t −τ)\n+ (1 −2P)τ0r(u + r)L + τ0r2(u + r)L√log d\nu\np\nη1λ(t −τ)\n!\n≲(u + r)K τ0r2(u + r)L√log d\nu\np\nη1λ(t −τ)\n≲τ0r2(u + r)2KL√log d\nu\np\nη1λ(t −τ)\n48\n\n\nWhen t ≤\n1\nη1λ, we conclude that term Φ is\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\nQtz\n\f\f\f\f\n≤η1u\nt\nX\nτ=1\n\r\r\r\r\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\n∆Qτ−1\n\r\r\r\r\n2\n≲η1u\nt\nX\nτ=1\nτ0r2(u + r)2KL√log d\nu\np\nη1λ(t −τ)\n≲τ0r2(u + r)2KL\np\nlog d\nr\ntη1\nλ\n≲τ0λ−1r2(u + r)2KL\np\nlog d\nCombine term Ψ and term Φ.\n|egt(X, z −ζ) + egt(X, z + ζ) −2egt(X, z)|\n=1/L ·\n\f\f\f\f1\n\u0010\nϵX,z−ζ\nt\n\u0011⊤\nQt(z −ζ) + 1\n\u0010\nϵX,z+ζ\nt\n\u0011⊤\nQt(z + ζ) −21\n\u0010\nϵX,z\nt\n\u0011⊤\nQtz\n\f\f\f\f\n≤1/L ·\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z−ζ\nt\n\u0011\n+ 1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−21\n\u0010\nϵX,z\nt\n\u0011\u0011⊤\nQtz\n\f\f\f\f\n|\n{z\n}\nΦ\n+1/L ·\n\f\f\f\f\n\u0010\n1\n\u0010\nϵX,z+ζ\nt\n\u0011\n−1\n\u0010\nϵX,z−ζ\nt\n\u0011\u0011⊤\nQtζ\n\f\f\f\f\n|\n{z\n}\nΨ\n≲τ0λ−1r2(u + r)2K\np\nlog d + λ−1ru−1(u + r)K\np\nlog d\nwith choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand L = Θ (Poly(d)), therefore, we\nconclude that\n|egt(X, z −ζ) + egt(X, z + ζ) −2egt(X, z)| ≲\n1\n√log d\n1\n√log d\np\nlog d ≲\n1\n√log d\nDeal with |gt1(X2)|.\nAssume that |gt1(X2, z −ζ) + gt1(X2, z + ζ) −2gt1(X2, z)| ≲ξ and from\nTheorem 4.2 we have ξ =\n1\n√log d.\nWe would first like to analysis |gt1(X2, z)|, |gt1(X2, z −\nζ)|, |gt1(X2, z + ζ)|. Naturally, we have\ngt1(X2, z) = 1\n2 (gt1(X2, z + ζ) + gt1(X2, z −ζ)) + γ\nwhere |γ| ≤ξ.\nThen consider the proportion of xL,2 = {z −ζ, z + ζ, z} in N training sequences with high\nprobability. For xL,2 = {z −ζ, z + ζ}, its expected proportion is 1\n4 and for xL,2 = z, its expected\nproportion 1\n2. Using Hoeffding’s inequality in Lemma D.1, for example xL,2 = z −ζ, define random\nvariables,\nXn =\n(\n1\nif Xn\nL,2 = z −ζ,\n0\nelse.\n49\n\n\nSince Xn are i.i.d. and E[Xn] = 1\n4,\nPr\n \f\f\f\f\f\n1\nN\nN\nX\nn=1\nXn −1\n4\n\f\f\f\f\f ≥t\n!\n≤2 exp\n\u0000−2Nt2\u0001\nLet δ = 2 exp (−2Nt2), then t =\nq\nlog 2\nδ\n2N . If 1 −δ = 1 −1\nd, t =\nq\nlog d\nN , then with probability at\nleast 1 −δ, the proportion of xL,2 = z −ζ is 1\n4 +\nq\nlog d\nN , Naturally, the proportion of xL,2 = z + ζ\nis 1\n4 +\nq\nlog d\nN , and the proportion of xL,2 = z is 1\n2 +\nq\nlog d\nN .\nWith the definition of empirical loss, l is the logistic loss, and l(f(V ; ·); X2, Y ) = log\n\u00001 + e−yLf(V ;X2,Y )\u0001\n.\nThen w.h.p. at least 1 −δ,\nbL(Vt1) = 1\nN\nX\nn∈[N]\nl(f(Vt1; ·); X2, Y )\n=\n \n1\n4 ± O\n r\nlog d\nN\n!!\nl(gt1(X2, z + ζ)) +\n \n1\n4 ± O\n r\nlog d\nN\n!!\nl(gt1(X2, z −ζ))\n+\n \n1\n2 ± O\n r\nlog d\nN\n!!\nl(gt1(X2, z))\n=\n \n1\n4 ± O\n r\nlog d\nN\n!!\n(l(gt1(X2, z + ζ)) + l(gt1(X2, z −ζ)) + 2l(gt1(X2, z)))\n=\n \n1\n4 ± O\n r\nlog d\nN\n!! \u0012\nl(gt1(X2, z + ζ)) + l(gt1(X2, z −ζ)) −2l(gt1(X2, z) −γ)\n|\n{z\n}\nA\n+ 2l(gt1(X2, z) −γ) + 2l(gt1(X2, z))\n|\n{z\n}\nB\n\u0013\nFor term A, since l is convex, then\nA =l(gt1(X2, z + ζ)) + l(gt1(X2, z −ζ)) −2l(gt1(X2, z) −γ)\n=l(gt1(X, z + ζ)) + l(gt1(X2, z −ζ)) −2l\n\u0012gt1(X2, z + ζ) + gt1(X2, z −ζ)\n2\n\u0013\n≥0\nFurther since l is a 2-Lipschitz function, we have\n|l(gt(X, z)) −l(gt(X, z) −γ)| ≤2γ\nB =2l(gt1(X2, z) −γ) + 2l(gt1(X2, z))\n≥2l(gt1(X2, z) −γ) + 2l(gt1(X2, z) −γ) −4γ\n50\n\n\nFinally, from Theorem 4.2 we have ξ =\n1\n√log d, we have the lower bound of bL(Vt1),\nbL(Vt1) =\n \n1\n4 ± O\n r\nlog d\nN\n!!\n(A + B)\n≥\n \n1\n4 −O\n r\nlog d\nN\n!!\n(4 log 2 −4γ)\n≥log 2 −O(ξ) −O\n r\nlog d\nN\n!\n≥log 2 −O\n\u0012\n1\n√log d\n\u0013\n−O\n r\nlog d\nN\n!\nAccording to the definition of training loss of component Q on signal weight, i.e. K1(V ), we have\nK1\nt1(V t1) ≳log 2 −O\n\u0012\n1\n√log d\n\u0013\n−O\n r\nlog d\nN\n!\nNaturally, assume that bL(Vt1) ≤log 2 + O(ξ′),\nbL(Vt1) ≥\n \n1\n4 −O\n r\nlog d\nN\n!!\n(A + 4 log 2 −4γ)\n=\n \n1\n4 −O\n r\nlog d\nN\n!!\n(A + 4 log 2 −O(ξ))\nbL(Vt1) ≤log 2 + O(ξ′)\nThen,\n \n1\n4 −O\n r\nlog d\nN\n!!\nA ≤log 2 + O(ξ′) −\n \n1\n4 −O\n r\nlog d\nN\n!!\n(4 log 2 −O(ξ))\n \n1\n4 −O\n r\nlog d\nN\n!!\nA ≤O(ξ) + O(ξ′)\nA ≤\nO(ξ′) + O(ξ)\n1 −O\n\u0012q\nlog d\nN\n\u0013\nConsider the Taylor expression of A, including the 2nd order, and u = gt1(X2, z + ζ), v =\n51\n\n\ngt1(X2, z −ζ)\nlog 2 + u\n2 + u2\n8 + log 2 + v\n2 + v2\n8 −2\n\u0012\nlog 2 + u + v\n4\n+ (u + v)2\n32\n\u0013\n=u2\n8 + v2\n8 −(u + v)2\n16\n= (u + v)2\n16\n≤A ≤\nO(ξ′) + O(ξ)\n1 −O\n\u0012q\nlog d\nN\n\u0013\nFinally, we have\n|gt1(X, z)|, |gt1(X, z −ζ)|, |gt1(X, z + ζ)| ≤O\n\n\n\nv\nu\nu\nt\nξ′ + ξ\n1 −\nq\nlog d\nN\n\n\n\nthen we derive\n|gt1(X2, z −ζ) + gt1(X2, z + ζ) −2gt1(X2, z)| ≤|gt1(X2, z −ζ)| + |gt1(X2, z + ζ)| + 2|gt1(X2, z)|\n≲\nv\nu\nu\nt\nξ′ + ξ\n1 −\nq\nlog d\nN\n≲ξ\nFrom Theorem 4.2 we have ξ =\n1\n√log d, thus ξ′ =\n1\nlog d.\nFinally, we conclude that\n|gt1(X2, z)|, |gt1(X2, z −ζ)|, |gt1(X2, z + ζ)| ≲\nv\nu\nu\nt\nξ′ + ξ\n1 −\nq\nlog d\nN\n≲\nv\nu\nu\nt(ξ′ + ξ)\n \n1 +\nr\nlog d\nN\n!\n≲\ns\n1\nlog d +\n1\n√N log d +\n1\n√log d +\n1\n√\nN\n≲\n1\n(log d)1/4\nDeal with ∥Vt1∥F.\nThrough |gt1(X2)|, we then analysis ∥Vt1∥F. With Corollary D.21,\n|gt1(X2)| =NVt1(Vt1; X2, Y )\n=NVt1(V t1; X2, Y ) + NVt1(eVt1; X2, Y )\n≲1\nL\nL\nX\ni=1\nyi1([X⊤\n2 ]iVt1xL,2) ·\n\u0000[X⊤\n2 ]iV t1xL,2\n\u0001\n+ ϵV,1\n≲1\nL\n\r\r1(X⊤\n2 Vt1xL,2)\n\r\r\n1 max\n\u0000[X⊤\n2 ]iV t1xL,2\n\u0001\n+ ϵV,1\n(20)\n52\n\n\nFor\n\r\r1(X⊤\n2 Vt1xL,2)\n\r\r\n1, using Corollary D.14,\n∥1(X⊤\n2 Vt1xL,2) −1(X⊤\n2 eVt1xL,2)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵV\nthus further consider\n\r\r\r1(X⊤\n2 eVt1xL,2)\n\r\r\r\n1,\n\r\r\r1(X⊤\n2 eVt1xL,2)\n\r\r\r\n1 =\nX\ni∈[L]\n1([X⊤\n2 ]i eVt1xL,2)\nwhere 1([X⊤\n2 ]i eVt1xL,2) is Bernoulli r.v., then using Hoeffding’s inequality in Lemma D.1,\nPr\n\nX\ni∈[L]\n1([X⊤\n1 ]if\nWtxL,1) ≥t\n\n≤e−t2\n2\nLet δ = e−t2\n2 , with δ = 1\nd, t =\nq\n2 log 1\nδ = √2 log d, then with probability at least 1−δ (i.e., 1−1\nd),\n\r\r\r1(X⊤\n2 eVt1xL,2)\n\r\r\r\n1 ≲\np\nlog d\nUsing triangle inequality, we know that\n\r\r1(X⊤\n2 Vt1xL,2)\n\r\r\n1 ≲\n\r\r\r1(X⊤\n2 eVt1xL,2)\n\r\r\r\n1 + ϵV ≲\np\nlog d + ϵV\nSubstitute into Equation 20, we have\n|gt1(X2)| ≲1\nL\n\r\r1(X⊤\n2 Vt1xL,2)\n\r\r\n1 max\n\u0000[X⊤\n2 ]iV t1xL,2\n\u0001\n+ ϵV,1\n≲1\nL\n\u0010p\nlog d + ϵV\n\u0011\n(u + r)2∥Vt1∥F + ϵV,1\n≲∥Vt1∥F\n\u0010p\nlog d + ϵV\n\u0011 (u + r)2\nL\n+ ϵV,1\n≲∥Vt1∥F\n1\nPoly(d) +\n1\nPoly(d)\nwith |gt1(X2)| ≲\n1\n(log d)1/4, we have\n∥V t1∥F ≤∥Vt1∥F ≲\n1\nPoly(d)\n53\n\n\nD.4\nProof for the Elementary Stage: Proof of Theorem 4.3\nTheorem 4.3. In the elementary stage with η1 = Θ(1) and t1 =\n1\n4η1λ where λ denotes regularization\ncoefficients. With Assumption 4.1 and initial weights W0 −→0d×d, it holds that there exist ϵW,1 =\nΘ (1/Poly(d)) (See Definition in Equation 16) such that\n(b.1) The model parameter W of network h is optimized by gradient descent within t1 iterations,\n∥W t1∥F = Θ (d log(1/ϵW,1)) ≫∥W 0∥F.\n(b.2) With random and small noise weight, the training loss of linear separable component P over\nsignal weight (Definition in Equation 6) at iteration t1 satisfies\nK1\nt1(W t1) ≲exp(−d log d) +\n1\n√log d.\nNamely, the network h learns the linear separable component P within t1 iterations.\nRemark D.26 (Proof Sketch). We summarize the proof sketch and main techniques in Proof of\nTheorem 4.3. For (b.1) and (b.2): In the beginning, we first analyze the network h’s output\nunder the optimal weight, with signal-noise decomposition, separating it into the outputs under the\noptimal signal weight and small random noise weights, respectively. The upper bound of the latter\nrelies on the key Proposition D.18, D.19 and Corollary D.20, where the calculation of activations\nand attention scores is explicitly written out, leveraging the differences in activation patterns. The\nupper bound analysis of the former utilizes the properties of W ⋆and the data construction attributes\nof component P. Moving forward, we use this network output to represent the upper bound of\nthe optimal loss. Furthermore, through gradient descent analysis, we measure ∥W t1 −W ⋆∥and\n∥K1\nt1(W t1) −K1\nt1(W ⋆)∥. We use proof by contradiction to give (b.1) and (b.2), showing that there\nexists a fixed target signal matrix which will classify P correctly no matter the small noise weight.\nProof. According to Theorem 4.2, we conclude that the large learning rate creates too much noise to\nlearn Q. Also, from above we conclude that in the first stage, the network weight Vt1 on Q changes\nsmall.\nDefinition D.27. In the elementary stage, denote the optimal weight as U ⋆\n1 =\n\u0014W ⋆\n0\n0\nV t1 = ∆V\n\u0015\nwith initial W0 = V0 −→0d×d, where W ⋆≜d log(1/ϵW,1)w⋆(w⋆)⊤∈Rd×d, and ∥V t1∥F ≲\n1\nPoly(d).\nIn this section, we primarily focus on the process of optimizing from W0 to W ⋆.\nWith the\ndecomposition of signal and noise weight, consider random and small noise, we will prove that W 0\ncan be optimized to W t1, which is close to W ⋆, at the end of this section through gradient descent\nanalysis.\nSince ft is the function of signal weight with random noise weight, then we first consider the\ndecomposition of ft(W ⋆; X1, Y )\nft(W ⋆; X1, Y ) = NWt(W ⋆+ f\nWt; X1, Y )\n= NWt(W ⋆; X1, Y ) + NWt(f\nWt; X1, Y )\n54\n\n\nDeal with term NWt(f\nWt; X1, Y ).\nWith Corollary D.20, and choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ =\nO\n\u0000√log d\n\u0001\nand L = Poly(d), then we have\nNWt(f\nWt; X1, Y ) ≲τ0(u + γ0)2\nr\nd log d\nL\n≲\n1\nPoly(d) ≜ϵW,1\n(21)\nDeal with term NWt(W ⋆; X1, Y ).\nFor the term NWt(W ⋆; X1, Y ), we know that\nNWt(W ⋆; X1, Y ) = Y/L ·\n\u00001(X⊤\n1 WtxL,1) ⊙(X⊤\n1 W ⋆xL,1)\n\u0001\n= 1\nL\nL\nX\ni=1\nyi1([X⊤\n1 ]iWtxL,1) ·\n\u0000[X⊤\n1 ]iW ⋆xL,1\n\u0001\nAccording to the data structure of X1, assume that γ0 = 1/\n√\nd, with Definition D.27 and Assumption\nD.6 that (w⋆)2 = 1. We find that\n∥W ⋆∥2\nF = (d log(1/ϵW,1))2 ∥w⋆(w⋆)⊤∥2\nF\n= d2 log2(1/ϵW,1)\n(22)\nWe consider that\nyNWt(W ⋆; X1, Y ) = y · Y/L ·\n\u00001(X⊤\n1 WtxL,1) ⊙(X⊤\n1 W ⋆xL,1)\n\u0001\n= 1\nL\nL\nX\ni=1\n1([X⊤\n1 ]iWtxL,1) ·\n\u0000[X⊤\n1 ]iW ⋆xL,1\n\u0001\n= d log(1/ϵW,1)[X⊤\n1 ]iw⋆(w⋆)⊤xL,1\n\r\r1(X⊤\n1 WtxL,1)\n\r\r\n1 /L\nFor d log(1/ϵW,1)[X⊤\n1 ]iw⋆(w⋆)⊤xL,1, with ϵW,1 =\n1\nPoly(d),\nd log(1/ϵW,1)[X⊤\n1 ]iw⋆(w⋆)⊤xL,1 = d log(1/ϵW,1)\n\u0012\nsign(⟨w⋆, e⟩) 1\n√\nd\n+ ⟨w⋆, e⟩\n\u00132\n≲d log(Poly(d))\nFor\n\r\r1(X⊤\n1 WtxL,1)\n\r\r\n1, using Corollary D.13,\n∥1(X⊤\n1 WxL,1) −1(X⊤\n1 f\nWxL,1)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵW\nUsing triangle inequality, we have 1(X⊤\n1 WxL,1) ≳1(X⊤\n1 f\nWxL,1) −ϵW, thus further consider\n\r\r\r1(X⊤\n1 f\nWtxL,1)\n\r\r\r\n1,\n\r\r\r1(X⊤\n1 f\nWtxL,1)\n\r\r\r\n1 =\nX\ni∈[L]\n1([X⊤\n1 ]if\nWtxL,1)\n55\n\n\nwhere 1([X⊤\n1 ]if\nWtxL,1) is Bernoulli r.v., then using Hoeffding’s inequality,\nPr\n\nX\ni∈[L]\n1([X⊤\n1 ]if\nWtxL,1) ≤(L −t)p\n\n≤e−\nt2\n2Lp(1−p)\nFor Bernoulli r.v., we have p = 1/2, then let δ = e−\nt2\n2Lp(1−p) = e−2t2/L, with δ =\n1\nd, t =\nq\n1\n2L log 1\nδ =\nq\n1\n2L log d, then with probability at least 1 −δ (i.e., 1 −1\nd),\n\r\r\r1(X⊤\n1 f\nWtxL,1)\n\r\r\r\n1 ≳L −\np\nL log d\nUsing triangle inequality, we know that\n\r\r1(X⊤\n1 WtxL,1)\n\r\r\n1 ≥∥1(X⊤f\nWtxL)∥1 −ϵW ≳L −\np\nL log d −ϵW\nFinally,\nyNWt(W ⋆; X1, Y ) = d log(1/ϵW,1)[X⊤\n1 ]iw⋆(w⋆)⊤xL,1\n\r\r1(X⊤\n1 WtxL,1)\n\r\r\n1 /L\n≳d log d\n\u0010\nL −\np\nL log d −ϵW\n\u0011\n· 1/L\n≳d log d\n \n1 −\nr\nlog d\nL\n−ϵW\nL\n!\n(23)\nCombine Equation 21 and Equation 23.\nCombine Equation 21 and Equation 23, we have\nyf 1\nt (W ⋆; X1, Y ) = yNWt(W ⋆+ f\nWt; X1, Y )\n≥yNWt(W ⋆; X1, Y ) −\n\f\f\fyNWt(f\nWt; X1, Y )\n\f\f\f\n≳d log d\n \n1 −\nr\nlog d\nL\n−ϵW\nL\n!\n−τ0(u + γ0)2\nr\nd log d\nL\nwith choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand L = Poly(d), consider the loss\nwith signal weight W t = W ⋆and random noise weight f\nWt at time t,\nK1\nt (W ⋆) = 1\nN\nN\nX\nn=1\nl\n\u0000yf 1\nt (W ⋆; Xn\n1 , Y n)\n\u0001\n≲log\n \n1 + exp\n \n−d log d\n \n1 −\nr\nlog d\nL\n−ϵW\nL\n!\n+ τ0(u + γ0)2\nr\nd log d\nL\n!!\n≲log (1 + exp(−d log d)) ≲exp(−d log d)\nwhich comes from d log d\nq\nlog d\nL\n= d(log d)3/2\nPoly(d) , d log dϵW\nL =\nd log d\n(Poly(d))1/3, τ0(u + γ0)2\nq\nd log d\nL\n≜ϵW,1 =\n1\nPoly(d).\n56\n\n\nDeal with gradient descent to find W ⋆.\nConsider the graident descent of signal W,\nW t+1 = W t −η1∇Kt(W t) −η1λWt\n= (1 −η1λ)W t −η1∇Kt(W t)\nWith ∥W ∗∥F = d log(1/ϵW,1) ≜B from Equation 22, loss Kt is K-Lipschitz, i.e. ∥∇Kt(W t)∥F ≤\nK, assume that ∥W t −W ⋆∥F ≤R = Θ(1) ≪B, then we can measure the distance of Wt and W ⋆.\n\r\rW t+1 −W ⋆\r\r2\n2 =\n\r\r(1 −η1λ)W t −η1∇Kt −W ⋆\r\r2\n2\n=\n\r\r(1 −η1λ)(W t −W ⋆) −η1(λW ⋆+ ∇Kt)\n\r\r2\n2\n=\n\r\r(1 −η1λ)(W t −W ⋆)\n\r\r2\n2 + η2\n1 ∥λW ⋆+ ∇Kt∥2\n2 −2η1(1 −η1λ)⟨W t −W ⋆, λW ⋆⟩\n−2η1(1 −η1λ)⟨W t −W ⋆, ∇Kt⟩\n=\n\r\r(1 −η1λ)(W t −W ⋆)\n\r\r2\n2 + η2\n1 ∥(λW ⋆+ ∇Kt)∥2\n2 −2η1λ(1 −η1λ)⟨W t, W ⋆⟩\n+ 2η1λ(1 −η1λ)⟨W ⋆, W ⋆⟩−2η1(1 −η1λ)(Kt(W t) −Kt(W ⋆))\n≤\n\r\r(1 −η1λ)(W t −W ⋆)\n\r\r2\n2 + 2η2\n1(λ2B2 + K2) −2η1λ(1 −η1λ)(R + B)B\n+ 2η1λ(1 −η1λ)B2 −2η1(1 −η1λ)(Kt(W t) −Kt(W ⋆))\n≤\n\r\r(1 −η1λ)(W t −W ⋆)\n\r\r2\n2 + 2η2\n1(λ2B2 + K2) −2η1λ(1 −η1λ)RB\n−2η1(1 −η1λ)(Kt(W t) −Kt(W ⋆))\nFor the sake of contradiction, assume that K1\nt (W t) −K1\nt (W ⋆) ≥C, let 0 < 1 −η1λ < 1, and\nη1 ≪\nλBR+C\nλ2B2+λ2BR+K2+λC, and λR2 ∼C,\n\r\rW t+1 −W ⋆\r\r2\n2 ≤\n\r\r(W t −W ⋆)\n\r\r2\n2 + 2η2\n1(λ2B2 + λ2BR + K2 + λC) −2η1(λBR + C)\n≤\n\r\r(W t −W ⋆)\n\r\r2\n2 −2η1(λBR + C)\n≤\n\r\r(W t −W ⋆)\n\r\r2\n2 −4η1λR2\nThus, in the elementary stage with t1 iterations, t ≤t1 ≜\n1\n4η1λ,\n\r\rW t1 −W ⋆\r\r2\n2 ≤\n\r\r(W 0 −W ⋆)\n\r\r2\n2 −4t1η1λR2 < 0\nwhich is a contradiction, i.e., K1\nt1(W t1) −K1\nt1(W ⋆) ≤C.\nTherefore, in the elementary stage within t1 iterations, t1 ≤\n1\n4η1λ, through gradient descent opti-\nmization, ∥W t1∥F satisfies ∥W t1∥F ≤B + R, then\n∥W t1∥F = Θ (d log(1/ϵW,1))\nand the training loss satisfies\nK1\nt1(W t1) ≤K1\nt1(W ⋆) + C ≲exp(−d log d) +\n1\n√log d\n57\n\n\nD.5\nProof for the Specialized Stage: Proof of Theorem 4.4\nTheorem 4.4. In the specialized stage with annealing learning rate η2 = η1λ2ϵ2\nV,1r and t1 ≤t ≤\nt1 + t2, where ϵV,1 = Θ(1/Poly(d)) (See Definition in Equation 17), t1 ≜\n1\n4η1λ, t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1 ,\nλ denotes the L2 regularization coefficient and data noise ∥ζ∥2 = r (See Section 3.1). With\nAssumption 4.1, it holds that\n(c.1) The model parameter V of network g is optimized by gradient descent within t2 iterations,\n∥V t1+t2∥F = Θ\n\u0012log(1/ϵV,1)\nϵV,1\n+\n1\nPoly(d)\n\u0013\n≫∥V t1∥F.\n(c.2) With random and small noise weight, the training loss of nonlinear separable component Q\nover signal weight (Definition in Equation 6) satisfies\nK2\nt1+t2(V t1+t2) ≲exp\n\u0012\n−log(1/ϵV,1)\nϵV,1\n\u0013\n+\n1\n√log d.\nNamely, the network g learns nonlinear separable component Q within t2 iterations.\nRemark D.28 (Proof Sketch). We summarize the proof sketch and main techniques in Proof of\nTheorem 4.4. To begin with, we explore the properties of optimal weight V t1 + V ⋆and analyze the\nnetwork g’s output under the optimal weight at timepoint t1 + t2. Using triangle inequality, we need\nto handle three parts A, B, C separately. Part A exploits the characteristics of V ⋆in detail. Part\nB uses the key Lemma D.15 and Corollary D.17 to analyze the relationship between the network\noutput at time t1 + t2 and at time t1, taking into account the signal weight update formula. Part C\nutilizes the properties of the network output at time t1 to facilitate the analysis. Thereafter, we use\nthis network output to represent the upper bound of the optimal loss. Furthermore, through gradient\ndescent analysis, we measure ∥V t1+t2 −(V t1 + V ⋆)∥and ∥K2\nt1+t2(V t1+t2) −K2\nt1+t2(V t1 + V ⋆)∥.\nWe use proof by contradiction to give (a) and (b), showing that there exists a fixed target signal\nmatrix which will classify Q correctly no matter the small noise weight.\nProof.\nDefinition D.29. For time t1, input X ∈Rd×L with query xL = z −ζ, z, z + ζ ∈Rd, define\nH1 ≜{i ∈[L] | [X⊤]iVt1(z −ζ) ≥0, [X⊤]iVt1z ≥0, [X⊤]iVt1(z + ζ) < 0}\nH2 ≜{i ∈[L] | [X⊤]iVt1(z −ζ) ≥0, [X⊤]iVt1z < 0, [X⊤]iVt1(z + ζ) < 0}\nH3 ≜{i ∈[L] | [X⊤]iVt1(z −ζ) < 0, [X⊤]iVt1z < 0, [X⊤]iVt1(z + ζ) ≥0}\nH4 ≜{i ∈[L] | [X⊤]iVt1(z −ζ) < 0, [X⊤]iVt1z ≥0, [X⊤]iVt1(z + ζ) ≥0}\nSimilar to Definition D.23, note that X aligns with X2 and xL aligns with xL,2.\nWe first try to analyze the probability of i ∈Hi. With Assumption D.6, we can compute the cosine\n58\n\n\nof z −ζ and z,\ncos θ =\n⟨z −ζ, z⟩\n∥z∥2∥z −ζ∥2\n=\nu2 −⟨ζ, z⟩\nu\np\nu2 −2⟨ζ, z⟩+ r2 =\nu2 −ur cos θ0\nu√u2 + r2 −2ur cos θ0\nsin θ =\n√\n1 −cos2 θ =\nr sin θ0\n√u2 + r2 −2ur cos θ0\nFor small r, with Taylor expansion of arcsin θ, we have that the angle of z−ζ and z is θ = r\nu +O(r2).\nFor H1, when [X⊤]i eVt1 fall into the middle of z −ζ and z, as well as not in the positive half space\nof z + ζ, its probability is approximately the proportion of the spherical surface area corresponding\nto the angle r\nu + O(r2). Using Hoeffding’s inequality in Lemma D.1 and further consider Corollary\nD.14, let Xi = 1{i ∈H1}, then |H1| = PL\ni=1 Xi\nE [|H1|] = L · Pr(i ∈H1) ≈L ·\nr\n2πu + ϵV\nThen, let δ = 2 exp\n\u0010\n−2t2\nL\n\u0011\n, t =\nq\n1\n2L log 2\nδ, and 1 −δ = 1 −1\nd, then with probability at least 1 −δ,\n||H1| −E[|H1|]| ≤\nr\n1\n2L log 2\nδ ≲\np\nL log d\n|H1| ≲rL\n2πu + ϵV +\np\nL log d\nSimilarly, we have\n|H1|, |H2|, |H3|, |H4| ≲rL\n2πu + ϵV +\np\nL log d\nDefinition D.30. In the second stage, denote the optimal weight as U ⋆\n2 =\n\u0014W t1 + ∆W\n0\n0\nV t1 + V ⋆\n\u0015\n=\n\u0014W t1+t\n0\n0\nV t1 + V ⋆\n\u0015\n, ∥W t1+t∥F ≲d log(1/ϵW,1), and V ⋆∈Rd×d satisfies\n[X⊤\n2 V ∗]i =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(1/ϵV,1)\nrϵV,1\nz⊤\nif i ∈H1;\n−2 log(1/ϵV,1)\nrϵV,1\nz⊤\nif i ∈H2;\nlog(1/ϵV,1)\nrϵV,1\nz⊤\nif i ∈H3;\n−2 log(1/ϵV,1)\nrϵV,1\nz⊤\nif i ∈H4;\n0\notherwise.\n(24)\nWe have that\n\r\rW t1+t −W t1\n\r\r\nF ≪\n\r\rV t1 + V ⋆−Vt1\n\r\r = ∥V ⋆∥F, and we still have ∥W t1+t∥F ≲\nd log(1/ϵW,1) from Theorem 4.3. In this section, we primarily focus on the process of optimizing\n59\n\n\nfrom V t1 to V t1 + V ⋆. To calculate the Frobenius norm ∥V ∗∥F,\n∥X⊤\n2 V ∗∥2\n2\n=\nX\ni∈H1\n\u0012log(1/ϵV,1)\nrϵV,1\n\u00132\n∥z⊤∥2\n2 +\nX\ni∈H2\n\u0012\n−2 log(1/ϵV,1)\nrϵV,1\n\u00132\n∥z⊤∥2\n2 +\nX\ni∈H3\n\u0012log(1/ϵV,1)\nrϵV,1\n\u00132\n∥z⊤∥2\n2\n+\nX\ni∈H4\n\u0012\n−2 log(1/ϵV,1)\nrϵV,1\n\u00132\n∥z⊤∥2\n2\n≲u2 |H|\n\u0012log(1/ϵV,1)\nrϵV,1\n\u00132\n≲u2\n\u0012 rL\n2πu + ϵV +\np\nL log d\n\u0013 log2(1/ϵV,1)\nr2ϵ2\nV,1\n≲uL log2(1/ϵV,1)\nrϵ2\nV,1\nand then ∥V ⋆∥F = O\n\u0010\nlog(1/ϵV,1)\nϵV,1\n\u0011\n, where c is a constant.\nIn the following, we focus on the empirical loss with optimal weight V t1 + V ⋆.\nK2\nt1+t(V t1 + V ⋆) = bL(NVt1+t(V t1 + V ⋆; X2, Y ))\n= 1\nN\nX\nn∈[N]\nlog\n\u00001 + exp\n\u0000−yn\nLNVt1+t(V t1 + V ⋆; Xn\n2 , Y n)\n\u0001\u0001\nand then consider yNVt1+t(V t1 + V ∗; X2, Y ),\nyNVt1+t(V t1 + V ∗; X2, Y )\n≥yNVt1+t(V ∗; X2, Y ) −yNVt1+t(V t1; X2, Y )\n≥yNVt1(V ∗; X2, Y )\n|\n{z\n}\nA\n−\n\f\fyNVt1+t(V ⋆; X2, Y ) −yNVt1(V ⋆; X2, Y )\n\f\f\n|\n{z\n}\nB\n−yNVt1+t(V t1; X2, Y )\n|\n{z\n}\nC\nDeal with term A.\nWe have\nyNVt1(V ∗; X2, Y ) = y · Y/L ·\n\u00001\n\u0000X⊤\n2 Vt1xL,2\n\u0001\n⊙\n\u0000X⊤\n2 V ∗xL,2\n\u0001\u0001\n= 1\nL\nL\nX\ni=1\n\u00001\n\u0000[X⊤\n2 ]iVt1xL,2\n\u0001\n⊙\n\u0000[X⊤\n2 ]iV ∗xL,2\n\u0001\u0001\nFor xL,2 = z −ζ, we have that\nNVt1(V ∗; X2, Y, xL,2 = z −ζ) ≤|H1|\nL\nlog(1/ϵV,1)\nrϵV,1\nz⊤(z −ζ) −|H2|\nL\n2 log(1/ϵV,1)\nrϵV,1\nz⊤(z −ζ)\n≲−log(1/ϵV,1)u(u + r)\nrϵV,1\n \nr\n2πu + ϵV\nL +\nr\nlog d\nL\n!\n≲−log(1/ϵV,1)(u + r)\nϵV,1\n60\n\n\nand for xL,2 = z + ζ, we have that\nNVt1(V ∗; X2, Y, xL,2 = z + ζ) ≤|H3|\nL\nlog(1/ϵV,1)\nrϵV,1\nz⊤(z + ζ) −|H4|\nL\n2 log(1/ϵV,1)\nrϵV,1\nz⊤(z + ζ)\n≲−log(1/ϵV,1)u(u + r)\nrϵV,1\n \nr\n2πu + ϵV\nL +\nr\nlog d\nL\n!\n≲−log(1/ϵV,1)(u + r)\nϵV,1\nand for xL,2 = z, we have that\nNVt1(V ∗; X2, Y, xL,2 = z + ζ) ≤|H1|\nL\nlog(1/ϵV,1)\nrϵV,1\nz⊤z −|H4|\nL\n2 log(1/ϵV,1)\nrϵV,1\nz⊤z\n≲−log(1/ϵV,1)u2\nrϵV,1\n \nr\n2πu + ϵV\nL +\nr\nlog d\nL\n!\n≲−log(1/ϵV,1)u\nϵV,1\nFinally, with small r ≪u, for xL,2 ∈{z −ζ, z, z + ζ}, we have\nyNVt1(V ∗; X2, Y ) ≳u log(1/ϵV,1)\nϵV,1\nDeal with term B.\nWith the definition of ∥X⊤\n2 V ∗∥2\nF, and ∥X⊤\n2 V ∗∥2\n2 ≲uL log2(1/ϵV,1)\nrϵ2\nV,1\n, we derive\nthat\n\f\f[X⊤\n2 V ∗]i\n\f\f ≲log(1/ϵV,1)\nϵV,1\nru\nr\n\f\f[X⊤\n2 V ∗]ixL,2\n\f\f ≲log(1/ϵV,1)\nϵV,1\nr\nu(u + r)2\nr\nWith Corollary D.17,\n\f\fyNVt1+t(V ⋆; X2, Y ) −yNVt1(V ⋆; X2, Y )\n\f\f ≲\n\u0012\nϵV + L\nrη2\nη1\n+\np\nL log d\n\u0013 log(1/ϵV,1)\nϵV,1L\nr\nu(u + r)2\nr\n≲log(1/ϵV,1)\nϵV,1L√r L ·\n\u0000λϵV,1\n√r\n\u0001\nwhere the last step satisfies when with choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand\nL = Θ (Poly(d)), and η2 = η1λ2ϵ2\nV,1r. Finally,\n\f\fyNVt1+t(V ⋆; X2, Y ) −yNVt1(V ⋆; X2, Y )\n\f\f ≲λ log(1/ϵV,1)\n61\n\n\nDeal with term C.\nBefore, we have\n|gt1(X, z)|, |gt1(X, z −ζ)|, |gt1(X, z + ζ)| ≲O\n\n\n\nv\nu\nu\nt\nζ′ + ζ\n1 −\nq\nlog d\nN\n\n\n≲\n1\n(log d)1/4\nThen, combine with Corollary D.21,\n|NVt1(V t1; X2, Y )| ≤|gt1(X2)| + |NVt1(V t1; X2, Y ) −NVt1(Vt1; X2, Y )|\n≤|gt1(X2)| + |NVt1(eVt1; X2, Y )|\n≲\n1\n(log d)1/4 + ϵV,1\nWith Corollary D.17 and ∥V t1∥≲\n1\nPoly(d)\n\f\fyNVt1+t(V t1; X2, Y ) −yNVt1(V t1; X2, Y )\n\f\f ≲\n\u0012\nϵV + L\nrη2\nη1\n+\np\nL log d\n\u0013\n1\nL · Poly(d)\n≲\n1\nPoly(d)\nrη2\nη1\nFinally, we get\n\f\fyNVt1+t(V t1; X2, Y )\n\f\f ≲\n1\n(log d)1/4 + ϵV,1 +\n1\nPoly(d)\nrη2\nη1\n≲\n1\n(log d)1/4 + ϵV,1 + λϵV,1\n√log d\n≲\n1\n(log d)1/4 + ϵV,1\nwhen with choice of η2 = η1λ2ϵ2\nV,1r, 1\nλ = O\n\u0000√log d\n\u0001\n.\nCombine term A, B and C.\nyNVt1+t(V t1 + V ∗; X2, Y )\n≥yNVt1(V ∗; X2, Y )\n|\n{z\n}\nA\n−\n\f\fyNVt1+t(V ⋆; X2, Y ) −yNVt1(V ⋆; X2, Y )\n\f\f\n|\n{z\n}\nB\n−\n\f\fyNVt1+t(V t1; X2, Y )\n\f\f\n|\n{z\n}\nC\n≳u log(1/ϵV,1)\nϵV,1\n−λ log(1/ϵV,1) −\n1\n(log d)1/4 −ϵV,1\nFinally, with choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand L = Poly(d), then for the\n62\n\n\ntraining loss of component Q,\nK2\nt1+t(V t1 + V ⋆) = bL(NVt1+t(V t1 + V ⋆; X2, Y ))\n= 1\nN\nX\nn∈[N]\nlog\n\u00001 + exp\n\u0000−yn\nLNVt1+t(V t1 + V ⋆; Xn\n2 , Y n)\n\u0001\u0001\n≲log\n\u0012\n1 + exp\n\u0012\n−u log(1/ϵV,1)\nϵV,1\n+ λ log(1/ϵV,1) +\n1\n(log d)1/4 + ϵV,1\n\u0013\u0013\n≲log\n\u0012\n1 + exp\n\u0012\n−log(1/ϵV,1)\nϵV,1\n\u0013\u0013\n≲exp\n\u0012\n−log(1/ϵV,1)\nϵV,1\n\u0013\nwhich comes from log(1/ϵV,1)\nϵV,1\n= Poly(d) log d, λ log(1/ϵV,1) = √log d, ϵV,1 =\n1\nPoly(d).\nDeal with gradient descent to find V t1 + V ⋆.\nConsider the graident descent of signal V ,\nV t+1 = V t −η1∇Kt(V t) −η1λVt\n= (1 −η1λ)V t −η1∇Kt(V t)\nSimilar to gradient descent of W, let V t1+V ⋆be W ⋆, then\n\r\rV t1 + V ⋆\r\r\nF = Θ\n\u0010\nlog(1/ϵV,1)\nϵV,1\n+\n1\nPoly(d)\n\u0011\n≜\nB. Let\n\r\rV t −\n\u0000V t1 + V ⋆\u0001\r\r\nF ≤R = Θ(1) ≪B.\n\r\rV t+1 −(V t1 + V ⋆)\n\r\r2\n2\n=\n\r\r(1 −η2λ)V t −η2∇Kt −(V t1 + V ⋆)\n\r\r2\n2\n=\n\r\r(1 −η2λ)(V t −(V t1 + V ⋆)) −η2(λ(V t1 + V ⋆) + ∇Kt)\n\r\r2\n2\n=\n\r\r(1 −η2λ)(V t −(V t1 + V ⋆))\n\r\r2\n2 + η2\n2\n\r\rλ(V t1 + V ⋆) + ∇Kt\n\r\r2\n2\n−2η2(1 −η2λ)⟨V t −(V t1 + V ⋆), λ(V t1 + V ⋆)⟩\n−2η2(1 −η2λ)⟨V t −(V t1 + V ⋆), ∇Kt⟩\n≤\n\r\r(1 −η2λ)(V t −(V t1 + V ⋆))\n\r\r2\n2 + 2η2\n2(λ2B2 + K2) −2η2λ(1 −η2λ)(R + B)B\n+ 2η2λ(1 −η2λ)B2 −2η2(1 −η2λ)(Kt(V t) −Kt(V t1 + V ⋆))\n≤\n\r\r(1 −η2λ)(V t −(V t1 + V ⋆))\n\r\r2\n2 + 2η2\n2(λ2B2 + K2) −2η2λ(1 −η2λ)RB\n−2η2(1 −η2λ)(Kt(V t) −Kt(V t1 + V ⋆))\nFor the sake of contradiction, assume that (K2\nt (V t) −K2\nt (V t1 + V ⋆)) ≥C, let 0 < 1 −η2λ < 1,\nand η2 ≪\nλBR+C\nλ2B2+λ2BR+K2+λC, and λR2 ∼C,\n\r\rV t+1 −(V t1 + V ⋆)\n\r\r2\n2 ≤\n\r\r(V t −(V t1 + V ⋆))\n\r\r2\n2 + 2η2\n2(λ2B2 + λ2BR + K2 + λC) −2η2(λBR + C)\n≤\n\r\r(V t −(V t1 + V ⋆))\n\r\r2\n2 −2η2(λBR + C)\n≤\n\r\r(V t −(V t1 + V ⋆))\n\r\r2\n2 −4η2λR2\n63\n\n\nThus, in the specialized stage within t1 ≤t ≤t1 + t2 iterations, t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1 , t1 ≜\n1\n4η1λ,\n\r\rV t1+t2 −(V t1 + V ⋆)\n\r\r2\n2 ≤\n\r\r(V t1 −(V t1 + V ⋆))\n\r\r2\n2 −4t2η2λR2 ≤log2 (1/ϵV,1)\nϵ2\nV,1\n−4t2η2λR2 < 0\nwhich is a contradiction.\nFinally, we conclude that, in the specialized stage within t2 iterations, t2 ≤\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1 , t1 ≤\n1\n4η1λ,\nthrough gradient descent optimization, ∥V t1+t2∥F satisfies ∥V t1+t2∥F ≤B + R, then\n∥V t1+t2∥F = Θ\n\u0012log(1/ϵV,1)\nϵV,1\n+\n1\nPoly(d)\n\u0013\nand the training loss satisfies\nK2\nt1+t2(V t1+t2) ≤K2\nt1+t2(V t1 + V ⋆) + C ≲exp\n\u0012\n−log(1/ϵV,1)\nϵV,1\n\u0013\n+\n1\n√log d\n64\n\n\nD.6\nProof for the Specialized Stage: Proof of Theorem 4.5\nTheorem 4.5. In the specialized stage with annealing learning rate η2 = η1λ2ϵ2\nV,1r and t1 ≤t ≤\nt1 + t2, where ϵV,1 = Θ(1/Poly(d)) (See Definition in Equation 17), t1 ≜\n1\n4η1λ, t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1 ,\nλ denotes the L2 regularization coefficient and data noise ∥ζ∥2 = r (See Section 3.1). With\nAssumption 4.1 and number of training prompts N = Θ (Poly(d)), it holds that\n(d.1) For the model parameter W of network h, through gradient descent optimization from iteration\nt1 to t1 + t2, ∥W t1+t2 −W t1∥F satisfies\n\r\rW t1+t2 −W t1\n\r\r\nF ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d.\n(d.2) With random and small noise weight, the training loss of linear separable component P over\nsignal weight (Definition in Equation 2) satisfies\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1))\n\f\f ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d.\nNamely, the network h continues to preserve the elementary knowledge like P within t2 iterations.\nRemark D.31 (Proof Sketch). We summarize the proof sketch and main techniques in Proof of\nTheorem 4.5. At the first step, based on the expression for the training loss of component P\nover signal weight, we use the triangle and Cauchy-Schwarz inequality to transform the difference\nin training loss at times t1 + t2 and t1, i.e. ∥K1\nt1+t2(W t1+t2) −K1\nt1(W t1)∥, into the difference in\nmodel weights at the two times, i.e.\n\r\rW t1+t2 −W t1\n\r\r. Following that, through gradient descent\nanalysis, similar to the analysis of ∥W t1 −W ⋆∥in Theorem 4.3, we derive ∥W t1+t2 −W ⋆∥and\ncombine these to conclude ∥W t1+t2 −W t1∥in (a). Naturally utilizing the relationship between\n∥K1\nt1+t2(W t1+t2) −K1\nt1(W t1)∥and ∥W t1+t2 −W t1∥from the first step to derive (b). In total, we\ndemonstrate that the model weight W and training loss of P are almost stable.\nProof. Deal with gradient descent from W t1 to W t1+t2.\nSimilar to the optimization from W 0 to\nW ⋆in Appendix D.4, we consider the graident descent of signal W t1,\nW t+1 = W t −η2∇Kt(W t) −η2λWt\n= (1 −η2λ)W t −η2∇Kt(W t)\nWith ∥W ∗∥F = d log(1/ϵW,1) ≜B from Equation 22, loss Kt is K-Lipschitz, i.e. ∥∇Kt(W t)∥F ≤\nK. For t1 < t ≤t1 + t2, assume that ∥W t −W ⋆∥F ≤R2 ≪B. For the sake of contradiction,\nassume that K1\nt (W t) −K1\nt (W ⋆) ≥C2, let 0 < 1 −η2λ < 1, and η2 ≪\nλBR2+C2\nλ2B2+λ2BR2+K2+λC2, and\nλR2\n2 ≪C2,\n\r\rW t+1 −W ⋆\r\r2\n2 ≤\n\r\r(W t −W ⋆)\n\r\r2\n2 + 2η2\n2(λ2B2 + λ2BR2 + K2 + λC2) −2η2(λBR2 + C2)\n≤\n\r\r(W t −W ⋆)\n\r\r2\n2 −2η2(λBR2 + C2)\n≤\n\r\r(W t −W ⋆)\n\r\r2\n2 −2η2C2\n65\n\n\nFrom Theorem 4.4, in the specialized stage within t2 iterations, t2 ≜\nlog2(1/ϵV,1)\nη2λϵ2\nV,1\n, t1 ≜\n1\nη1λ. From\nthe gradient descent in Appendix D.4, we have\n\r\rW t1 −W ⋆\r\r\nF ≤R ≪B = d log(1/ϵW,1), then\n\r\rW t1+t2 −W ⋆\r\r2\n2 ≤\n\r\r(W t1 −W ⋆)\n\r\r2\n2 −2t2η2C ≤R2 −2t2η2C2 < 0\nwhich is a contradiction. We naturally have R <\nR2ϵV,1\nlog(1/ϵV,1), then we can derive that λR2 <\nλR2\n2ϵ2\nV,1\nlog2(1/ϵV,1) ≪C2. Thus, at iteration t1 + t2, the training loss of component P over signal weight\nsatisfies\nK1\nt1+t2\n\u0000W t1+t2\n\u0001\n≤K1\nt1+t2(W ⋆) + C2 ≲ϵW,1 +\n√\nd log d\nL\nϵW +\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d\nCombining the conclusion in Theorem 4.3, we have that the difference of loss between iteration t1\nand t1 + t2 is\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1))\n\f\f ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d\n(25)\nIn the following, we would like to show that the changes in W is also small. With 1-Lipschitzness\nof logistic loss, we know that\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1)\n\f\f =\n\f\f\f\f\f\f\n1\nN\nX\nn∈[N]\n\u0000l(NWt1+t2(W t1+t2; Xn\n1 , Y n)) −l(NWt1(W t1; Xn\n1 , Y n))\n\u0001\n\f\f\f\f\f\f\n≤1\nN\nX\nn∈[N]\n\f\fNWt1+t2(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\f\f\n|\n{z\n}\nA\n(26)\nDeal with Term A.\nWith Corollary D.16 and Corollary D.20, we derive that\n\f\fNWt1+t2(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\f\f\n≤\n\f\fNWt1+t2(W t1+t2; Xn\n1 , Y n) −NWt1(W t1+t2, Xn\n1 , Y n)\n\f\f\n(27)\n+\n\f\fNWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\f\f\n≲\n\u0012\nϵW + L\nrη2\nη1\n+\np\nL log d\n\u0013 K(u + γ0)2\nLλ\n+\n\f\fNWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\f\f\n(28)\nSubstitute Equation 28 into Equation 26, and use Cauchy-Shwartz inequality,\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1))\n\f\f\n≲1\nN\nX\nn∈[N]\n\f\fNWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\f\f +\n\u0012\nϵW + L\nrη2\nη1\n+\np\nL log d\n\u0013 K(u + γ0)2\nLλ\n≲1\nN\ns X\nn∈[N]\n\u0000NWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\u00012 +\n\u0012\nϵW + L\nrη2\nη1\n+\np\nL log d\n\u0013 K(u + γ0)2\nLλ\n≲1\nN\ns X\nn∈[N]\n\u0000NWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\u00012\n|\n{z\n}\nB\n+\n1\nPoly(d)\n66\n\n\nwhere the last step comes with choice of small u, r, τ0 = O\n\u0010\n1\n√log d\n\u0011\n, 1\nλ = O\n\u0000√log d\n\u0001\nand\nL = Θ (Poly(d)), and η2 = η1λ2ϵ2\nV,1r. Deal with term B.\nWith Assumption D.6, We have\n\u0000NWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\u00012\n=\n\u0010\nY n/L\n\u0010\n1\n\u0010\n[Xn\n1 ]⊤Wt1xL,1\n\u0011\n⊙\n\u0010\n[Xn\n1 ]⊤W t1+t2xL,1\n\u0011\u0011\n−Y n/L\n\u0010\n1\n\u0010\n[Xn\n1 ]⊤Wt1xL,1\n\u0011\n⊙\n\u0010\n[Xn\n1 ]⊤W t1xL,1\n\u0011\u0011\u00112\n≤1\nL2 max |Y n\ni |2\n2\n\r\r\r1\n\u0010\n[Xn\n1 ]⊤Wt1xL,1\n\u0011\r\r\r\n2\n1\n\r\r\r[Xn\n1 ]⊤W t1+t2xL,1 −[Xn\n1 ]⊤W t1xL,1\n\r\r\r\n2\n2\n≤1\nL2\n\r\r\r1\n\u0010\n[Xn\n1 ]⊤Wt1xL,1\n\u0011\r\r\r\n2\n1 ∥[Xn\n1 ]⊤∥2\nF\n\r\rW t1+t2 −W t1\n\r\r2\nF ∥xL,1∥2\n2\n≤1\nL2\n\r\r\r1\n\u0010\n[Xn\n1 ]⊤Wt1xL,1\n\u0011\r\r\r\n2\n1 L(u + γ0)4 \r\rW t1+t2 −W t1\n\r\r2\nF\nFor term\n\r\r\r1\n\u0010\n[Xn\n1 ]⊤Wt1xL,1\n\u0011\r\r\r\n2\n1.\nUsing Corollary D.13,\n∥1(X⊤\n1 WxL,1) −1(X⊤\n1 f\nWxL,1)∥1 ≲K4/3λ−4/3τ −4/3\n0\nL2/3 ≜ϵW\nthus further consider\n\r\r\r1(X⊤\n1 f\nWt1xL,1)\n\r\r\r\n1,\n\r\r\r1(X⊤\n1 f\nWt1xL,1)\n\r\r\r\n1 =\nX\ni∈[L]\n1([X⊤\n1 ]if\nWt1xL,1)\nwhere 1([X⊤\n1 ]if\nWt1xL,1) is Bernoulli r.v., then using Hoeffding’s inequality in Lemma D.1,\nPr\n\nX\ni∈[L]\n1([X⊤\n1 ]if\nWt1xL,1) ≥t\n\n≤e−t2\n2\nLet δ = e−t2\n2 , with δ = 1\nd, t =\nq\n2 log 1\nδ = √2 log d, then with probability at least 1−δ (i.e., 1−1\nd),\n\r\r\r1(X⊤\n1 f\nWt1xL,1)\n\r\r\r\n1 ≲\np\nlog d\nUsing triangle inequality, we know that\n\r\r1(X⊤\n1 Wt1xL,1)\n\r\r2\n1 ≲\n\u0010\n∥1(X⊤f\nWt1xL)∥1 + ϵW\n\u00112\n≲\n\u0010p\nlog d + ϵW\n\u00112\nThus, for term B, we have\n\u0000NWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\u00012\n≤1\nL2\n\r\r\r1\n\u0010\n[Xn\n1 ]⊤Wt1xL,1\n\u0011\r\r\r\n2\n1 L(u + γ0)4 \r\rW t1+t2 −W t1\n\r\r2\nF\n≲(u + γ0)2\nL\n\u0010p\nlog d + ϵW\n\u00112 \r\rW t1+t2 −W t1\n\r\r2\nF\n67\n\n\nand then for\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1))\n\f\f,\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1))\n\f\f\n≲1\nN\ns X\nn∈[N]\n\u0000NWt1(W t1+t2; Xn\n1 , Y n) −NWt1(W t1; Xn\n1 , Y n)\n\u00012\n|\n{z\n}\nB\n+\n1\nPoly(d)\n≲u + γ0\n√\nLN\n\u0010p\nlog d + ϵW\n\u0011 \r\rW t1+t2 −W t1\n\r\r\nF +\n1\nPoly(d)\nCombining with Equation 25, we can derive that\n\r\rW t1+t2 −W t1\n\r\r\nF ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d −\n1\nPoly(d)\nwhen\n√\nLN = Θ(√log d + ϵW), i.e. N = Θ (Poly(d)).\nTherefore, we conclude that in the specialized stage, the changes in W and the loss in the h network\nare both small, and the loss remains very low.\n\r\rW t1+t2 −W t1\n\r\r\nF ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d −\n1\nPoly(d)\nand\n\f\fK1\nt1+t2(W t1+t2) −K1\nt1(W t1))\n\f\f ≲\nϵ2\nV,1\nlog2 (1/ϵV,1) √log d\n68\n\n\nD.7\nProof for Spectral Characteristics: Proof of Corollary 4.6\nCorollary 4.6. Under the assumptions in Theorem 4.2 ∼Theorem 4.5, it holds that\n(a) In the elementary stage within t1 ≜\n1\n4η1λ iterations, the spectral dynamics satisfy\nTr(Wt1) > Tr(Vt1).\n(b) In the specialized stage within t2 ≜\nlog2(1/ϵV,1)\n4η2λϵ2\nV,1\niterations, the spectral dynamics satisfy\nTr(Wt1+t2) < Tr(Vt1+t2).\nProof. Compute the gradient of weight WK and WQ.\nWith one normalized Relu self-attention\nlayer, we have\nf(U; X, eY ) = eY · 1\n2LReLU\n\u0000X⊤W ⊤\nKWQxL\n\u0001\n= eY /2L · ReLU\n\u0000X⊤UxL\n\u0001\nwhere X ∈R2d×2L, U = W ⊤\nKWQ ∈R2d×2d. Consider the gradient of weight WK and WQ,\n∇WK bL(U) = bE\nh\nl′(f(U; X, eY ))∇(yLf(U; X, eY ))\ni\n= bE\nh\nl′(f(U; X, eY ))yL∇\n\u0010\neY /2L · ReLU\n\u0000X⊤W ⊤\nKWQxL\n\u0001\u0011i\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\n2L\nX\ni=1\nyi∇ReLU\n\u0000[X⊤]iW ⊤\nKWQxL\n\u0001\n#\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\n2L\nX\ni=1\nyi1([X⊤]iW ⊤\nKWQxL)WQxL[X⊤]i\n#\n= bE\nh\n1/2L · l′(f(U; X, eY ))WQ\n\u0000X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001⊤i\n[∇WK bL(Ut)]i = bE\nh\n1/2L · l′(f(Ut; X, eY ))yLyi1([X⊤]iW ⊤\nKWQxL)[WQ]ixL[X⊤]i\ni\n[∇WK bL(Ut)]j = bE\nh\n1/2L · l′(f(Ut; X, eY ))yLyj1([X⊤]jW ⊤\nKWQxL)[WQ]jxL[X⊤]j\ni\n69\n\n\nSimilarly, we have\n∇WQ bL(U) = bE\nh\nl′(f(U; X, eY ))∇(yLf(U; X, eY ))\ni\n= bE\nh\nl′(f(U; X, eY ))yL∇\n\u0010\neY /2L · ReLU\n\u0000X⊤W ⊤\nKWQxL\n\u0001\u0011i\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\n2L\nX\ni=1\nyi∇ReLU\n\u0000[X⊤]iW ⊤\nKWQxL\n\u0001\n#\n= bE\n\"\n1/2L · l′(f(U; X, eY ))yL\n2L\nX\ni=1\nyi1([X⊤]iW ⊤\nKWQxL)WKXix⊤\nL\n#\n= bE\nh\n1/2L · l′(f(U; X, eY ))WKX · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\ni\n[∇WQ bL(Ut)]i = bE\nh\n1/2L · l′(f(Ut; X, eY ))yLyi1([X⊤]iW ⊤\nKWQxL)[WK]iXix⊤\nL\ni\n[∇WQ bL(Ut)]j = bE\nh\n1/2L · l′(f(Ut; X, eY ))yLyj1([X⊤]jW ⊤\nKWQxL)[WK]jXjx⊤\nL\ni\nWith l = −log σ\n\u0010\nyLf(U; X, eY )\n\u0011\n, we have l′ ≜l′(f(U; X, eY )) = −yL exp(−yLf(U;X,eY ))\n1+exp(−yLf(U;X,eY )) . According\nto ∇WK bL(U) and ∇WQ bL(U), let A = bE\n\u0002\nl′X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0003\n∈Rd×d, then we have\nWK,t+1 = WK,t −η∇WK,t bL(Ut) −ηλWK,t\n= (1 −ηλ)WK,t −η∇WK,t bL(UT)\n= (1 −ηλ)WK,t −η/2L · WQ,tA⊤\nt\nSimilarly,\nWQ,t+1 = WQ,t −η∇WQ,t bL(Ut) −ηλWQ,t\n= (1 −ηλ)WQ,t −η∇WQ,t bL(Ut)\n= (1 −ηλ)WQ,t −η/2L · WK,tAt\nEigen decomposition and the gradient descent of eigenvalues.\nAssume that WK ≃WQ and\nsimultaneous diagonolizability,\nWK = M · diag(σ(WK))Φ⊤\nWQ = M · diag(σ(WQ))Φ⊤\n70\n\n\nThen,\nWK,t+1 = (1 −ηλ)WK,t −η/2L · WQ,t[At]⊤\n= (1 −ηλ)WK,t −η/2L · Mt · diag(σ(WQ,t))Φ⊤\nt [At]⊤\n= (1 −ηλ)WK,t −η/2L · Mt · diag(σ(WQ,t))Φ⊤\nt [At]⊤ΦtΦ⊤\nt\n= (1 −ηλ)WK,t −η/2L · Mt · diag(σ(WQ,t))\n\u0000Φ⊤\nt AtΦt\n\u0001⊤Φ⊤\nt\nWQ,t+1 = (1 −ηλ)WQ,t −η/2L · WK,tAt\n= (1 −ηλ)WQ,t −η/2L · Mt · diag(σ(WK,t))Φ⊤\nt At\n= (1 −ηλ)WQ,t −η/2L · Mt · diag(σ(WK,t))Φ⊤\nt AtΦtΦ⊤\nt\n= (1 −ηλ)WQ,t −η/2L · Mt · diag(σ(WK,t))\n\u0000Φ⊤\nt AtΦt\n\u0001\nΦ⊤\nt\nIf we have A is symmetric and Φ⊤AΦ is diagonal, then for the eigenvalues of WK and WQ, i.e.\nσ(WK) and σ(WQ),\nσ (WK,t+1) = (1 −ηλ)σ (WK,t) −η/2L · σ(WQ,t) ⊙σ([At]⊤)\nσ (WQ,t+1) = (1 −ηλ)σ (WQ,t) −η/2L · σ(WK,t) ⊙σ(At)\nLet √w = σ(WK) = σ(WQ) ∈Rd and w = σ(U) = σ(WK) ⊙σ(WQ) ∈Rd, α = σ(A),\nσ (WK,t+1) ⊙σ (WQ,t+1)\n=(1 −ηλ) (σ (WK,t) ⊙σ (WQ,t)) −η/2L ·\n\u0000σ(WK,t)⊙2\u0001\n⊙σ(At)\n−η/2\n\u0000σ(WQ,t)⊙2\u0001\n⊙σ([At]⊤)\n=(1 −ηλ) (σ (WK,t) ⊙σ (WQ,t)) −η/2L\n\u0000σ(WK,t)⊙2 + σ(WQ,t)⊙2\u0001\n⊙σ(At)\nFinally, we have\nwt+1 = (1 −ηλ)wt −η/2L · 2wt ⊙αt\nAnalysis the relationship of α = Tr(A) and w = Tr(U).\nIn the following, we analysis the\nrelationship of α and w. To Compute trace of matrix A,\nTr(A) = Tr\n\u0010\nbE\n\u0002\nl′X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0003\u0011\n= bE\n\u0002\nTr\n\u0000l′X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001\u0003\n= bE\n\nl′ Tr\n\u0000X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001\n|\n{z\n}\nM\n\n\n71\n\n\nFor term M,\nM = Tr\n\u0000X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001\n=\nd\nX\ni=1\n L\nX\nj=1\nXij\n\u0002\n1(X⊤W ⊤\nKWQxL)\n\u0003\nj\n!\nxLi\n≤max(∥x∥2\n2)\nL\nX\nj=1\n\u0002\n1(X⊤W ⊤\nKWQxL)\n\u0003\nj\n|\n{z\n}\nZ\nFor term Z,\nWK = M · diag(σ(WK))Φ⊤\nWQ = M · diag(σ(WQ))Φ⊤\nX⊤W ⊤\nKWQxL = X⊤Φ · diag(√w)M ⊤M · diag(√w)Φ⊤xL\n= X⊤Φ · diag(w)Φ⊤xL\nand then\n\u0002\n1(X⊤W ⊤\nKWQxL)\n\u0003\nj =\n\u0002\n1(X⊤Φ · diag(w)Φ⊤xL)\n\u0003\nj\n=\n\u0002\u00001(X⊤Φ)1(diag(w))1(Φ⊤xL)\n\u0001\u0003\nj\n= 1(\n\u0002\nX⊤]jΦ\n\u0001\n1(diag(w))1(Φ⊤xL)\n=\nd\nX\nk=1\n1([X⊤]jΦ)1(Φ⊤xL)1(wk)\n=\nd\nX\nk=1\n1([X⊤]jxL)1(wk)\nCombine term M and term Z, and assume that almost ∀wi > 0, then we have\nα = Tr(A)\n=bE\n\u0002\nl′Tr\n\u0000X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001\u0003\n=p−E\n\u0002\nl′\n−Tr\n\u0000X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001\u0003\n+ p+E\n\u0002\nl′\n+Tr\n\u0000X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001\u0003\n≥pbE\n\"\nl′\n−max(∥x∥2\n2)\nL\nX\nj=1\n\u0002\n1(X⊤W ⊤\nKWQxL)\n\u0003\nj\n#\n+ (1 −p)E\n\u0002\nl′\n+Tr\n\u0000X · diag\n\u00001(X⊤W ⊤\nKWQxL)\n\u0001\nx⊤\nL\n\u0001\u0003\n=pbE\n\"\nl′\n−max(∥x∥2\n2)\nL\nX\nj=1\nd\nX\nk=1\n1([X⊤]jxL)1(wk)\n#\n=p max(∥x∥2\n2)bE\n\"\nl′\n−\nL\nX\nj=1\nd\nX\nk=1\n1([X⊤]jxL)1(wk)\n#\n=p max(∥x∥2\n2)bE\n\u0002\nl′\n−1⊤1(X⊤xL)\n\u0003\n≜−pk\n72\n\n\nwhere p is the proportion of negative logistic loss, k = max(∥x∥2\n2)bE\n\u0002\n|l′\n−|1⊤1(X⊤xL)\n\u0003\n> 0. We\nconclude that the lower bound of α is independent with w, naturally,\nwt+1 ≤(1 −ηλ)wt + η/2L · 2pkwt\nAnalysis Wt and Vt.\nBy similar proof, for W = [W 1\nK]⊤W 1\nQ:\nLet A1 = bE\n\u0002\nl′X1 · diag\n\u00001(X⊤\n1 [W 1\nK]⊤W 1\nQxL,1)\n\u0001\nx⊤\nL,1\n\u0003\n∈Rd×d, w1 = σ(W 1\nK) ⊙σ(W 1\nQ) ∈Rd,\nα1 = σ(A1), we also have\n∇W 1\nK bL(W) = bE\nh\n1/L · l′(f(W; X1, Y ))W 1\nQ\n\u0000X1 · diag\n\u00001(X⊤\n1 [W 1\nK]⊤W 1\nQxL,1)\n\u0001\nx⊤\nL,1\n\u0001⊤i\n∇W 1\nQ bL(W) = bE\n\u0002\n1/L · l′(f(W; X1, Y ))W 1\nKX1 · diag\n\u00001(X⊤\n1 [W 1\nK]⊤W 1\nQxL,1)\n\u0001\nx⊤\nL,1\n\u0003\nand p1 is the proportion of the negative derivative of logistic loss l′(f(W; X1, Y )) < 0\nw1\nt+1 = (1 −ηλ)w1\nt + 2p1k1η/L · w1\nt,\nk1 ≜max(∥x∥2\n2)bE\n\u0002\n|l′\n−|1⊤1(X⊤\n1 xL,1)\n\u0003\nFor V = [W 2\nK]⊤W 2\nQ, let A2 = bE\n\u0002\nl′X2 · diag\n\u00001(X⊤\n2 [W 2\nK]⊤W 2\nQxL,2)\n\u0001\nx⊤\nL,2\n\u0003\n∈Rd×d, w2 = σ(W 2\nK)⊙\nσ(W 2\nQ) ∈Rd, α2 = σ(A2), we have\n∇W 2\nK bL(V ) = bE\nh\n1/L · l′(f(V ; X2, Y ))W 2\nQ\n\u0000X2 · diag\n\u00001(X⊤\n2 [W 2\nK]⊤W 2\nQxL,2)\n\u0001\nx⊤\nL,2\n\u0001⊤i\n∇W 2\nQ bL(V ) = bE\n\u0002\n1/L · l′(f(V ; X2, Y ))W 2\nKX2 · diag\n\u00001(X⊤\n2 [W 2\nK]⊤W 2\nQxL,2)\n\u0001\nx⊤\nL,2\n\u0003\nand p2 is the proportion of the negative derivative of logistic loss l′(f(V ; X2, Y )) < 0\nw2\nt+1 = (1 −ηλ)w2\nt + 2p2k2η/L · w2\nt,\nk2 ≜max(∥x∥2\n2)bE\n\u0002\n|l′\n−|1⊤1(X⊤\n2 xL,2)\n\u0003\nIn the elementary stage.\nWith learning rate η1, Tr(Wt) ≜w1\nt, and Tr(Vt) ≜w2\nt, we have\nw1\nt+1 = (1 −η1λ)w1\nt + 2p1k1η1/L · w1\nt,\nk1 ≜max(∥x∥2\n2)bE\n\u0002\n|l′\n−|1⊤1(X⊤\n1 xL,1)\n\u0003\nw2\nt+1 = (1 −η1λ)w2\nt + 2p2k2η1/L · w2\nt,\nk2 ≜max(∥x∥2\n2)bE\n\u0002\n|l′\n−|1⊤1(X⊤\n2 xL,2)\n\u0003\nthen through t1 ≤\n1\nη1λ iterations, according to the dynamic of the trace of W and V ,\nw1\nt1 = (1 −η1λ + 2p1k1η1/L)t1 w1\n0\nw2\nt1 = (1 −η1λ + 2p2k2η1/L)t1 w2\n0\nWe conclude that Tr(Wt) and Tr(Vt) have similar update rules where the rate of exponential growth\nover time mainly depends on three factors: (1) The learning rate η1. (2) The proportion of the\nnegative derivative of logistic loss p. (3) The negative derivative of the logistic loss is selected based\non the similarity between query xL and sequence X, i.e. 1(X⊤\n1 xL,1). Further compute k with the\nmean absolute value of the selected negative derivative.\n73\n\n\nCombine Theorem 4.3 with small and random noise, ∥Wt1∥F ≈∥W t1∥F and ∥Vt1∥F ≈∥V t1∥F,\nwe conclude the following corollary that at time t1,\nw1\nt1 = Tr(Wt1) ≤\nq\nTr(W ⊤\nt1 Wt1) = ∥Wt1∥F ≲d log(1/ϵW,1)\nw2\nt1 = Tr(Vt1) ≤\nq\nTr(V ⊤\nt1 Vt1) = ∥Vt1∥F ≲\n1\nPoly(d)\nFinally, we have\nTr(Wt1) > Tr(Vt1)\nIn the specialized stage.\nWith learning rate η2, Tr(Wt) ≜w1\nt, and Tr(Vt) ≜w2\nt, we have\nw1\nt+1 = (1 −η2λ)w1\nt + 2p1k1η2/L · w1\nt,\nk1 ≜max(∥x∥2\n2)bE\n\u0002\n|l′\n−|1⊤1(X⊤\n1 xL,1)\n\u0003\nw2\nt+1 = (1 −η2λ)w2\nt + 2p2k2η2/L · w2\nt,\nk2 ≜max(∥x∥2\n2)bE\n\u0002\n|l′\n−|1⊤1(X⊤\n2 xL,2)\n\u0003\nThrough t2 ≤log2(1/ϵV,1)\nη2λϵ2\nV,1\niterations, according to the dynamic of the trace of W and V ,\nw1\nt1+t2 = (1 −η2λ + 2p1k1η2/L)t2 w1\nt1\nw2\nt1+t2 = (1 −η2λ + 2p2k2η2/L)t2 w2\nt1\nSimilar to the elementary stage, we conclude that Tr(Wt) and Tr(Vt) still have similar update rules\nwhere the rate of exponential growth over time mainly depends on three factors.\nCombine with Theorem 4.4 and 4.5, we have\nw1\nt1+t2 = Tr(Wt1+t2) ≤\nq\nTr(W ⊤\nt1+t2Wt1+t2) = ∥Wt1+t2∥F ≲d log(1/ϵW,1) + log (1/ϵV,1)\nλ3/2ϵ2\nV,1\nw2\nt1+t2 = Tr(Vt1+t2) ≤\nq\nTr(V ⊤\nt1+t2Vt1+t2) = ∥Vt1+t2∥F ≲\n1\nPoly(d) + log(1/ϵV,1)\nϵV,1\nFinally, we have\nTr(Wt1+t2) < Tr(Vt1+t2)\nIn summary, by applying spectral analysis techniques, such as SVD and gradient descent on\neigenvalues, we conclude that whether in the elementary stage or specialize stage, Tr(Wt) and\nTr(Vt) follow similar update rules. The rate of exponential growth over time primarily depends on\nthree factors: (1) the learning rate η1 or η2; (2) the proportion p1 or p2 of the negative derivative of\nlogistic loss; and (3) k1 or k2 represents the mean absolute value of the selected negative derivative.\nBy the way, the negative derivative of the logistic loss is selected based on the similarity between\nquery xL and sequence X, i.e. 1(X⊤\n1 xL,1). When comparing the updating rules for the traces of\nweights in the two stages, we find that the three factors differ and vary with training. However, the\noverall exponential growth trend remains consistent.\nAdditionally, from Theorems 4.3 ∼4.5, it’s straightforward to compare the relationship of Tr(W)\nand Tr(V ) at iteration t1 and t1 + t2, which demonstrates that relatively small eigenvalues of\nattention weights store elementary knowledge and large ones store specialized knowledge.\n74\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20681v1.pdf",
    "total_pages": 74,
    "title": "Disentangling Feature Structure: A Mathematically Provable Two-Stage Training Dynamics in Transformers",
    "authors": [
      "Zixuan Gong",
      "Jiaye Teng",
      "Yong Liu"
    ],
    "abstract": "Transformers may exhibit two-stage training dynamics during the real-world\ntraining process. For instance, when training GPT-2 on the Counterfact dataset,\nthe answers progress from syntactically incorrect to syntactically correct to\nsemantically correct. However, existing theoretical analyses hardly account for\nthis two-stage phenomenon. In this paper, we theoretically demonstrate how such\ntwo-stage training dynamics occur in transformers. Specifically, we analyze the\ndynamics of transformers using feature learning techniques under in-context\nlearning regimes, based on a disentangled two-type feature structure. Such\ndisentanglement of feature structure is general in practice, e.g., natural\nlanguages contain syntax and semantics, and proteins contain primary and\nsecondary structures. To our best known, this is the first rigorous result\nregarding a two-stage optimization process in transformers. Additionally, a\ncorollary indicates that such a two-stage process is closely related to the\nspectral properties of the attention weights, which accords well with empirical\nfindings.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}