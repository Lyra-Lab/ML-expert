{
  "id": "arxiv_2502.21025v1",
  "text": "AutoQML: A Framework for Automated Quantum Machine Learning\nMarco Roth,1, ∗David A. Kreplin,1, † Daniel Basilewitsch,2 João F. Bravo,3 Dennis Klau,3\nMilan Marinov,4 Daniel Pranji´c,3 Horst Stuehler,5 Moritz Willmann,1 and Marc-André Zöller4\n1Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Nobelstraße 12, 70569 Stuttgart, Germany\n2TRUMPF SE + Co. KG, Quantum Applications Group, Johann-Maus-Straße 2, 71254 Ditzingen, Germany\n3Fraunhofer Institute for Industrial Engineering IAO, Nobelstraße 12, 70569 Stuttgart, Germany\n4USU GmbH, Rüppurrer Str. 1, 76137 Karlsruhe, Germany\n5Zeppelin GmbH, Graf-Zeppelin-Platz 1, 85748 Garching, Germany\n(Dated: March 3, 2025)\nAutomated Machine Learning (AutoML) has significantly advanced the efficiency of ML-focused software\ndevelopment by automating hyperparameter optimization and pipeline construction, reducing the need for manual\nintervention. Quantum Machine Learning (QML) offers the potential to surpass classical machine learning (ML)\ncapabilities by utilizing quantum computing. However, the complexity of QML presents substantial entry barriers.\nWe introduce AutoQML, a novel framework that adapts the AutoML approach to QML, providing a modular and\nunified programming interface to facilitate the development of QML pipelines. AutoQML leverages the QML\nlibrary sQUlearn to support a variety of QML algorithms. The framework is capable of constructing end-to-end\npipelines for supervised learning tasks, ensuring accessibility and efficacy. We evaluate AutoQML across four\nindustrial use cases, demonstrating its ability to generate high-performing QML pipelines that are competitive\nwith both classical ML models and manually crafted quantum solutions.\nI.\nINTRODUCTION\nA key factor in the success and democratization of machine\nlearning (ML) has been the development of increased abstrac-\ntion levels, which facilitate rapid prototyping and lower entry\nbarriers. Automated machine learning (AutoML) aims to auto-\nmate the predominantly manual process of ML pipeline con-\nstruction, representing a significant progression in this devel-\nopment [1]. This approach has proven successful by enhancing\nthe efficiency of specialists, allowing them to focus more on\nmodeling business problems rather than on implementation\ndetails [2]. Furthermore, the reduced expertise required to use\nthese tools democratizes ML methods, allowing companies\nwith less experience to integrate ML-based solutions into their\nworkflows [3]. This is a particularly relevant consideration in\nmarket environments that are increasingly impacted by labor\nshortages, notably in specialized domains such as ML [4].\nQuantum machine learning (QML) employs quantum com-\nputers to develop ML algorithms that harness quantum me-\nchanical principles, with the goal of expanding the capabilities\nof ML beyond the classical limits [5–7]. As an interdisci-\nplinary field requiring expertise in quantum computing, ML,\nand computer science, the entry barriers to this technology are\nparticularly high. In this work, we introduce the framework Au-\ntoQML1, which aims to transfer the success of AutoML from\nthe classical to the quantum realm, making QML accessible to\na broad audience in science, technology and industry.\nUsing quantum computers for ML instead of classic hard-\nware presents a unique set of challenges. Providing a diverse\nsuite of QML algorithms that can be orchestrated using Au-\ntoML techniques, such as combined algorithm selection and\n∗marco.roth@ipa.fraunhofer.de\n† david.kreplin@ipa.fraunhofer.de\n1 Code available at https://github.com/AutoQML/autoqml\nhyperparameter optimization (CASH) [8] requires a modular\nimplementation with a unified programming interface. Ad-\nditionally, a seamless transition from classical preprocessing\nand simulation to real quantum computers needs to be ensured.\nAutoQML builds on the QML library sQUlearn [9], which\noffers a variety of QML algorithms with a scikit-learn [10]\nprogramming interface to create a set of modular and diverse\nQML methods. The library leverages PennyLane [11] and\nQiskit [12], enabling the execution of algorithms on multiple\nsimulators and quantum computers, such as IBM Quantum [13]\nand various backends available through Amazon Braket [14].\nAutoQML creates end-to-end QML pipelines for various\nsupervised learning scenarios, such as time series classification,\ntabular regression, and image classification. Using the open-\nsource libraries Optuna [15] and Ray Tune [16], the framework\noffers fully optimizable pipelines, including quantum-specific\npreprocessing, to make QML accessible to non-experts. In\ndesigning AutoQML, we have anticipated future developments\nin quantum computing and focused particularly on modularity,\nallowing for easy extension of the algorithm pool.\nIn this work, we outline the architecture of the AutoQML\nframework. We benchmark the framework on four distinct\nindustrial use cases involving time series and image classifica-\ntion, as well as tabular and time-series regression. The results\nare compared to classical solutions and manual quantum com-\nputing pipelines. This study aims to demonstrate the capability\nof AutoQML in facilitating the development of effective QML\nsolutions in various domains.\nThe remainder of this work is structured as follows. Fol-\nlowing the related work in Sec. I A, Sec. II outlines the archi-\ntecture of the AutoQML framework. Section III describes the\nindustrial use cases to evaluate the performance of the frame-\nwork. Section IV presents the experimental results, compar-\ning AutoQML-generated pipelines with both manually crafted\nQML pipelines and classical ML approaches. Finally, Secs. VI\nand VI conclude with a summary and discussion of our findings\nand potential future research directions.\narXiv:2502.21025v1  [quant-ph]  28 Feb 2025\n\n\n2\nEvaluation\nAutoML Optimizer\n(Ray + Optuna)\nBackend Selection\nQC/ Simulator\nData\nSearch Space\nPipeline Construction\nData\nCleaning\nPreprocessing\nPrediction\nsQUlearn\nQML Pipeline\nFIG. 1. Architecture overview of the AutoQML framework. Data is supplied by the user. Using Ray and Optuna, AutoQML constructs a\npipeline that is optimized over a preconfigured search space. A loss value li is obtained for each configuration⃗λi, which consists of data\ncleaning, preprocessing, and a model with hyperparameters evaluated using a simulator or a real quantum computer (QC). After a given budget\nis exhausted, the best-performing pipeline is returned to the user. Optional pipeline steps are indicated as dashed boxes.\nA.\nRelated Work\nThe term AutoQML has been used previously, primarily em-\nphasizing the optimization of specific QML models rather than\naddressing the optimization of the complete pipeline, which\nincludes preprocessing and the selection from different QML\nmodels. This term has been applied to hyperparameter opti-\nmization for a fixed model [17], as well as in the context of\narchitecture search for optimizing the encoding of data into\nquantum states [18, 19]. In these architecture search efforts,\nthe QML model remains fixed while the encoding circuit is\noptimized for the dataset, aiming to improve the performance\nof the given model [20–23]. Our work distinguishes itself\nfrom these prior approaches by presenting, to the best of our\nknowledge, the first comprehensive framework that constructs\nand optimizes end-to-end QML pipelines. Unlike previous\nefforts that focused on optimizing specific QML models or\nencoding circuits, our approach emphasizes the integration of\ndiverse QML methods, enabling the development of adaptable\nand effective QML solutions tailored to various application\nscenarios.\nII.\nFRAMEWORK\nThe foundation of AutoQML are the open-source optimiza-\ntion, parallelization, and scheduling frameworks Ray Tune and\nOptuna. The algorithm pool is constituted by scikit-learn and\nsQUlearn, with extended functionalities designed to optimize\nand streamline the development of QML pipelines. Neural\nnetwork-based approaches such as autoencoders for prepro-\ncessing are implemented using PyTorch [24].\nA.\nPipeline Creation\nThe framework follows a standard AutoML approach known\nas pipeline synthesis and optimization [1, 25], designed to cre-\nate complete (quantum) ML pipelines. This involves searching\nfor the optimal pipeline structure and performing CASH opti-\nmization. For a given problem type such as tabular regression,\nAutoQML constructs pipelines based on best-practice tem-\nplates, where the sequence of steps is predefined but the spe-\ncific methods an. Each pipeline explores various data cleaning\nand preprocessing steps, along with testing available models\nand their hyperparameters. The available templates can be\neasily extended to accommodate additional scenarios.\nFigure 1 provides a high-level overview of the AutoQML\narchitecture. It is primarily inspired by state-of-the-art Au-\ntoML approaches, such as those introduced in Hutter et al.\n[26] and Feurer et al. [27], and adheres to the design princi-\nples of scikit-learn [28] to provide a well-known, standardized\nprogramming interface. Users are required to provide an input\ndataset and choose a suitable problem and data type, such as\ntabular regression.2 Similar to AutoML for non-quantum ML,\nthe AutoQML framework samples (quantum) ML pipelines\nfrom a pre-defined search space for automated optimization.\nThis search space includes the necessary steps to create an\nend-to-end pipeline for (quantum) ML predictions, which can\nbe categorized into three steps:\n1. Data Cleaning is responsible for eliminating potential\ndefects from the input data. It includes imputation of\n2 While an extension to other learning domains, like unsupervised learning, is\npossible, the focus of this work is on supervised learning.\n\n\n3\nmissing values, outlier removal, and encoding of cate-\ngorical features.\n2. Preprocessing transforms the data into a format suitable\nfor quantum computing. This involves dimensionality\nreduction, down-sampling, feature-centric and rescaling.\n3. Prediction uses classification or regression algorithms\nto generate the actual predictions. The available QML\nalgorithms are implemented via sQUlearn. In addition, a\nsubset of classical ML methods are optionally available.\nMost steps in the pipeline are facultative and can be combined\nin various ways to create diverse of ML pipeline configurations.\nThe set-up for the optimization is as follows. Given a com-\nplete search space description, the optimizer iteratively draws\nnew test configurations i, denoted as ⃗λi ∈Λ, where Λ is a\nconfiguration space. A configuration encapsulates an abstract\nspecification of a particular (quantum) ML pipeline. Follow-\ning the CASH optimization procedure, each configuration⃗λi\njointly describes algorithms for (quantum) ML models, data\ncleaning, preprocessing, and prediction steps, as well as their\nassociated hyperparameters. More specifically, for a given\ndata set D = (X,y), with features X = (⃗x1,...,⃗xN) from some\nfeature space X ∋⃗x j and labels y ⊂R, we solve the following\noptimization problem [1]\n⃗λ ∗= arg min⃗λ∈Λl(D,⃗λ).\n(1)\nHere, l is a suitable loss function. AutoQML thus not only\noptimizes over quantum models and their hyperparameters\nbut also over the type of preprocessing algorithms (e.g. PCA)\nand their configuration (e.g. the number of retained principal\ncomponents). Pipeline examples are shown in Tab. I.\nTo solve Eq. (1) AutoQML utilizes probabilistic models to\nguide the search for high-performing configurations. It lever-\nages Bayesian optimization using tree-structured Parzen Esti-\nmator (TPE) [29]. The search algorithm constructs a Gaussian\nmixture model to approximate the loss (e.g., validation loss or\naccuracy) based on the results of previous evaluations. For a\ngiven ML problem, the optimizer performs the following steps:\n(i) For a new pipeline i, a configuration⃗λi is drawn from Λ by\nsampling close to the optimal point of a probabilistic model,\nwhich favors promising regions of the search space. (ii) The\nproposed configuration⃗λi is passed to the evaluation function,\nyielding a performance score li. This score reflects how well\nthe pipeline performs on a validation set. (iii) The tuple (⃗λi,li)\nis used to update the probabilistic model of the loss function.\nThe model is refined to more accurately represent the relation-\nship between configurations and their performance. (iv) The\nprocess repeats until a user-provided budget, such as a time\nlimit, is exhausted. When the optimization process is finished,\nthe best-performing configuration⃗λ ⋆and the corresponding\nfitted pipeline are returned to the user. This optimization pro-\ncess enables AutoQML to efficiently discover high-performing\n(quantum) ML pipelines tailored to the specific problem at\nhand. A code example is shown in Fig. 2.\nThe fixed pipeline templates in AutoQML account for quan-\ntum computing specific preprocessing. Particularly, the limited\n# Set up pipeline with fixed order\nautoqml = TabularRegression()\n# provide data and options\ncmd = AutoQMLFitCommand(\nX=X_train ,\ny=y_train ,\ntime_budget=timedelta(seconds=100),\nbackend='pennylane',\nconfiguration=\"quantum_regression\")\n# fit the pipeline\nautoqml = autoqml.fit(cmd)\n# predict using the best performing pipeline\ny_pred = autoqml.predict(X_test)\nFIG. 2. Example code for fitting a tabular regression pipeline. Here,\nit is assumed that the training data is supplied as X_train with corre-\nsponding targets y_train. Within AutoQML, the data is split into a\ntest and validation set. Options such as the time budget timedelta\nfor the optimization or the backend for execution of the QML al-\ngorithms can be specified. In the example, the preset configuration\n\"quantum_regression\" is used to restrict the search space to quan-\ntum computing based regression algorithms only.\nsize of current quantum computer requires a significant dimen-\nsionalty reduction, e.g., through principal component analysis\n(PCA) or autoencoders. Additionally, the encoding of classical\ndata in AutoQML is done using pre-defined encoding circuits\nbased on angle-encoding. This usually requires scaling the\nfeatures to avoid non-injective maps.\nB.\nQML Integration\nThe QML algorithms are provided by sQUlearn. The library\noffers several high-level methods such as quantum neural net-\nworks (QNN) [30], quantum reservoir computing (QRC) [31],\nand various kernel methods such as quantum kernel ridge\nregression (QKRR) [32], quantum support vector machines\n(QSVM) [33], and quantum Gaussian processes (QGPR) [34].\nThese methods are accessible through a scikit-learn program-\nming interface, allowing for user-friendly implementation.\nHigh-level methods can be modularly configured using a va-\nriety of pre-implemented quantum encoding circuits. Each\ncircuit’s configuration can be tailored by adjusting hyperpa-\nrameters such as the number of qubits and the number of layer\nrepetitions. In QNNs and QRC, the form and quantity of ob-\nservables used to compute the output can also be customized by\nhyperparameters. Additionally, fidelity kernels and projected\nquantum kernels are available for quantum kernel methods. To-\ngether with classical hyperparameters, such as regularization\nstrength for the QSVM or optimization parameters for QNNs,\nthese degrees of freedom form the configuration space for the\nQML predictors.\nThe QML methods can be executed using PennyLane and\nQiskit simulators, as well as IBM Quantum computers and sev-\neral quantum computing backends provided by Amazon Braket.\nIn the case where real quantum computers are used for exe-\ncution, communication with the quantum hardware providers\nis managed by sQUlearn. For IBM quantum backends, an\n\n\n4\nautomated hardware selection routine that can prioritize either\nspeed or accuracy is available.\nIII.\nUSE CASES\nWe benchmark AutoQML on four scenarios based on real-\nworld use cases from the domains of manufacturing and au-\ntomotive. For each, we derive a supervised learning problem.\nThe learning problems have been chosen such that their (ef-\nfective) dimensionality and data set size are small enough so\nthat they can be processed by current quantum computers (or\nsimulators) while still retaining an adequate level of difficulty.\nFor some use cases this required creating synthetic data.\nIn the following, we briefly describe each use case and\nthe data sets used to benchmark the framework. For some\nproblems, the data sets are preprocessed before being inputted\ninto AutoQML. In these cases, the preprocessing steps are\ndescribed in the corresponding section for each use case.\nA.\nTime Series Classification\nWe consider a time series classification task in which sen-\nsor data is collected from an autonomous vehicle to identify\nseveral states of the vehicle, for example, the execution of\nindividual tasks or abnormal activity. To this end, we synthet-\nically generate vibration sensor data, i.e., a univariate time\nseries containing 7 individual states which are each associated\nwith a unique label for the classification task. From the time\nseries, we generate a spectrogram that is then divided into\n2-dimensional tiles of size [7×30]. Each tile is flattend into\na one-dimensional vector of dimension d = 210 and a label\ncorresponding to the states present in the tile is assigned. In\ntotal, the data set contains N = 3291 samples that are divided\ninto Ntraining = 758 training points Ntest = 2533 testing points.\nSince most tiles are associated with no sensor activity, the\ntraining set is stratified so that all classes occur roughly with\nthe same frequency.\nB.\nImage Classification\nIn sheet metal processing using laser cutting machines, metal\nplates rest on a bed of supporting slats during processing.\nThese slats are manually positioned by the machine opera-\ntor in fixed socket positions ahead of time, with the specific\nconfiguration depending on the cutting task. For optimal op-\neration, it is beneficial to know the positions of the slats in\nadvance [35]. The associated task is a binary image classifica-\ntion problem, where the goal is to determine whether a given\nimage contains a supporting slat. To ensure enough image data\nfor training, we use a proprietary synthetic pipeline to automati-\ncally generate artificial yet authentic slat images using accurate\nCAD models of all employed parts a 3D rendering framework.\nThese rendered images feature entirely randomized configu-\nrations of slat positions. Since each rendered image depicts\nmultiple slats, it is subsequently divided into smaller image\nsnippets of size 80×200 pixels, each focusing on exactly one\nslat position, which is either occupied or not. Although this\nmethodology allows for the generation of an arbitrary num-\nber of images, to keep computational requirements moderate,\nwe restrict the data set under study to 500 image snippets, di-\nvided into Ntraining = 400 training and testing points Ntest = 100.\nMoreover, we perform feature reduction via PCA in advance,\nreducing each image snippet to d = 8 features. This reduction\nis necessary to maintain the confidentiality requirements of\nthe use case. We treat the resulting latent space vectors as\ninput images. The data set has been presented in more detail\nby Basilewitsch et al. [36].\nC.\nTabular Regression\nAccurate price forecasting is essential for companies man-\naging pre-owned assets, whose values fluctuate with spatial\nand temporal variations in supply and demand. This is partic-\nularly relevant for heavy construction equipment dealers and\nrental companies, who depend on precise price predictions to\noptimize asset management. Assessing the current and future\nresidual value of their fleets enables these companies to deter-\nmine the ideal time to resell individual pieces of machinery.\nBy collecting data from seven major online construction equip-\nment portals, we create a data set with N = 165 data points\n(Ntraining = 132 and Ntest = 33). Each data point represents one\nCaterpillar type 308 construction machine. The features are\nthe construction year, the working hours, the current location\nand the model extension. The target values are the prices. The\nlocation and model extension are categorical variables with 9\nand 3 unique values, respectively. The resulting data set has six\ndimensions (16 if one-hot-encoded). The data set is a subset of\nthe data used by Stühler et al. [37].\nD.\nTime Series Forecasting\nEngineering control technology systems for the automotive\nsector heavily depend on the ability to model or simulate sensor\ntime series data. This is particularly relevant for dynamic situa-\ntions, such as when accelerating a vehicle. Producing accurate\nand precise forecasts of physical quantities can significantly\ninfluence the quality of system control. To manipulate, com-\npare, and analyze the computed models effectively, we select\nthe relative cylinder filling of an internal combustion engine\nas an application where we can easily control the dimension-\nality. The time series encodes complex non-linear dynamics.\nUsing a sliding-window approach, the problem is formulated\nas a regression task, and the number of time steps utilized for\nthe forecast can be freely chosen. Our real industrial data set\nconsists of 10000 time steps, each covering a period of 10ms.\nThe final data set is a resampled version with Ntrain = 556 and\nNtest = 140, where the features are lagged versions of the time\nseries from the four previous time steps. Training and test data\nare derived from different parts of the time series.\n\n\n5\nT1\nT2\n0.50\n0.75\n1.00\nBalanced accuracy\n(a) Time Series Clf. (higher is better)\nAutoQML\nManual QML\nClassical\nT1\nT2\n0.8\n0.9\n1.0\nAccuracy\n(b) Image Clf. (higher is better)\nT1\nT2\nTime budget\n0.0\n0.1\n0.2\nMAPE\n(c) Tabular Reg. (lower is better)\nT1\nT2\nTime budget\n0.50\n0.55\n0.60\nMASE\n(d) Time Series Fc. (lower is better)\nFIG. 3. Performance of AutoQML (boxes) for two different time budgets T1 and T2. Additionally, manual QML pipelines (red, dashed) and\nclassical models (black, dotted) are depicted. (a) shows the balanced accuracy (higher is better) for the time series classification. (b) shows the\naccuracy (higher is better) for the image classification. (c) shows the mean absolute percentage error (MAPE, lower is better) for the tabular\nregression, and (d) shows the mean absolute scaled error (MASE, lower is better) for the time series forecasting. For the box plots, points that\nare outside 1.5× the inter-quartile range are shown as circles, and the lines inside the boxes denote the sample median.\nIV.\nRESULTS\nThe performance of the AutoQML framework is evaluated\nusing the four use cases described in Sec. III. The results are\nshown in Fig. 3. For each use case, we fit AutoQML for\nten different seeds with two time budgets, T1 = 10000s and\nT2 = 50000s. Note that although AutoQML can optimize over\na joint algorithm pool consisting of classical and quantum\nML algorithms, we are interested in the performance of the\nQML algorithms in particular and thus only include quantum\nmethods in the search space for this benchmark.\nWe compare our results with manually created QML\npipelines (red, dashed lines). For two use cases, we sourced\nmanual solutions from previous studies [36, 37], while for the\nother two, we constructed custom pipelines tailored to the spe-\ncific use cases. Details of the manual models and the process\nused to obtain them are provided in Appendix A. Since these\nmodels have been crafted by quantum computing specialists,\nthey require significantly more expertise and time compared\nto the corresponding AutoQML solutions. All quantum mod-\nels are evaluated using the PennyLane statevector simulator.\nAdditionally, to better gauge the quality of the results, we com-\npare them with the performance of classical models. These\nare shown as dotted lines in Fig. 3, indicating the performance\nof the best model among random forests, XGBoost [38], and\nsupport vector machines. For regression, Gaussian process\nregression is also included. For the kernel methods, RBF ker-\nnels were used, and the hyperparameters of all models were\noptimized using Optuna. The preprocessing pipeline is the\nsame as for the manually created QML models.\nWhen evaluating the manually obtained QML pipelines\nagainst the AutoQML pipelines, we observe that the perfor-\nmance is comparable. For three out of the four use cases, the\nAutoQML pipeline outperforms the manual QML pipeline on\naverage, with a slight improvement in the tabular regression\nand time series forecasting use cases (b, c) and a clear advan-\ntage in the image classification use case (d). However, for\nthe time series classification use case (a), the manual pipeline\nprovides a superior solution.\nIn all use cases, we observe a median improvement when\ngranting a larger time budget. Specifically, for the time series\nclassification use case (a), there is a significant performance\nincrease with the budget T2 compared to T1. However, for\nthe other use cases, the performance gains are only marginal.\nAdditionally, the drastically reduced variance with time bud-\ngets T2 suggests that budget the T1 is insufficient for the time\nseries classification use case, while for other use cases, the\npipeline search appears to be nearing convergence even with\nT1. This difference in convergence is likely due to the much\nlarger dimensionality of the time series classification use case\ncompared to the other use cases.\nWhen comparing the QML models with the classical models,\nwe observe that the best AutoQML pipeline outperforms the\nbest classical solution in three out of the four use cases (b–d),\n\n\n6\nFIG. 4. Application of the best AutoQML pipelines (blue) from Fig 3 on the respective use cases. The application of the manually created\nmodels is shown in red. (a) shows signal in the upper part of the figure. The bars bellow show the presence or absence of events which are\nclassified by the models. (b) shows the two principal components with the larges singular values of the test set of the image classification use\ncase. The points that have been missclassified by the AutoQML (cross, blue) or the manual pipeline (plus, red) are shown in addition. The\nclasses 0 (no slat) and 1 (slat) are shown in different colors. Figure (c) and (d) depicts the prediction vs. the true target values of the tabular\nregression and time series forecasting, respectively.\nwhile for the time series classification (a), the classical solution\nachieves the highest score. Overall, the classical solutions are\nsimilar to the AutoQML solutions, and neither demonstrates\na clear advantage over the other. The comparable quality of\nresults across all models supports the validity of the manual\nQML models and provides evidence for the effectiveness of\nAutoQML.\nThe pipelines with the highest scores in Fig. 3 are shown\nin Tab. I. The rows show the choices for the corresponding\npipeline step. Some preprocessing steps such as one hot encod-\ning are only relevant for specific use cases are omitted in the\ntable. The final pipelines are diverse in terms of models and\npreprocessing. Notably, all models are quantum kernel mod-\nels. This is, at least partially, expected since training QNNs\nis significantly more time consuming than training quantum\nkernel methods. Therefore, QNNs are underrepresented in the\noptimization process. Estimators based on QRC were the best\noptimizers in several runs. However, in none of the use cases\nwas QRC the best among the ten runs (i.e., the ten optimiza-\ntion with different seeds per use case). Three out of the four\nkernel methods use projected quantum kernels (PQK). Interest-\ningly, all PQKs in the best performing models do not employ\nthe commonly used RBF-type outer kernel function. This is\nin agreement with other studies which found that the outer\nkernel function in PQKs should be treated as an additional\nhyper-parameter [32].\nFigure 4 shows the application of the best AutoQML\npipelines (blue) from Fig. 3 together with the models from\nthe manually crafted pipelines. Overall, the pipelines solve\nthe use cases well. In the time series classification (a), both\npipelines are able to determine the classes, although classes 4\nand 5 seem to be more difficult as both pipelines do not classify\nthem optimally. In the image classification (b), the pipelines\nare applied to the full data set but only the first to principal\ncomponents are shown for visualization. It can be seen that\nthe first two principal components are not sufficient to linearly\nseparate the model. This aligns with the result of the best Auto-\nQML pipeline, which uses all 8 principal components present\n\n\n7\nTABLE I. Summary of different model pipelines. Fidelity quantum kernels are denoted by FQK. The row Observables depicts the measurement\nobservables for projected quantum kernels (PQK) or QNNs. Here, Xi,Yi,Zi denotes the respective Pauli operator on qubit i where i = 1,...,n\nruns across all qubits. The description of the outer kernels in cases where PQKs have been used (in the present cases pairwise, Matter, dot\nproduct), and the encoding circuits can be found in the sQUlearn documentation [39].\nTime Series Classification\nImage Classification\nTabular Regression\nTime Series Regression\nDim. Red.\nPCA\nUMAP\n–\n–\nScaling\nNormalization\nStandardization\nNormalization\nNormalization\nObservable\n–\n{Xi,Zi}\n{Xi,Yi,Zi}\n{Xi,Yi}\nModel\nQSVM; FQK\nQSVM; PQK(Mattern)\nQKRR; PQK(DotPorduct)\nQGPR; PQK(pairwise)\nEncoding Circuit\n[40]\nMulti-Control\nMulti-Control\nYZ-CX [41]\nNum. Qubits\n8\n8\n8\n8\nNum. Layers\n1\n2\n3\n3\nin the data set. Most of the misclassifications happen in the\nregion where the classes overlap in the first two components,\nindicating that even with the full dimensionality, there might\nstill be some overlap of the data classes. The application of the\npipelines to the regression problems (c) and (d) is in line with\nthe expectations from the performance metrics in Fig. 3. Over-\nall, the tabular regression use case (c) is more difficult than the\ntime series forecasting problem because the tabular regression\nproblem has higher dimensionality and more noise compared\nto the relatively simple one-step ahead problem in (d). This\ndifference in difficulty is reflected in the larger deviations of\nthe predicted values from the true values in (c) compared to (d).\nComparing the training performance to the test performance,\nno significant shortcomings, i.e., overfitting or underfitting, can\nbe observed.\nV.\nDISCUSSION\nThe results in this paper have been performed with stat-\nevector simulations. As quantum computing matures, a sign-\nficant portion of the model evaluation will have to be done\non quantum computers. Although we have tested this pro-\ntotypically, a full pipeline optimization on current quantum\nhardware is currently infeasible due to both financial and time\nconstraints. Since simulation techniques co-evolve with the\nhardware [42, 43], we foresee that in the upcoming years the\npipeline search will most likely involve a combination of ad-\nvanced simulation techniques and evaluation on real devices\nwith a requirement to be efficient in QPU time as much as\npossible. Furthermore, the current evaluation time for QML\nmodels is notably longer than for classical ML models, neces-\nsitating a greater time budget for pipeline optimization. This\nunderscores a trade-off between human developer time and\ncomputational time, which is more significant than in classi-\ncal AutoML frameworks. Addressing these challenges and\nintegrating novel QML developments into AutoQML will be\ncrucial for advancing the framework’s efficacy and versatility.\nAlthough we have tested AutoQML on a diverse set of prob-\nlems, the framework in its current form is only designed for\nsupervised ML problems. Extensions to unsupervised prob-\nlems like clustering are conceivable. Through its modular\ndesign and the encapsulation of the quantum computing-facing\nmodules in sQUlearn, such extensions can be implemented\neasily. This is also true for incorporating novel developments\nin QML, such as new models.\nCurrently, AutoQML only supports a fixed, predefined li-\nbrary of circuits. These circuits can be further customized by\nthe automation process. Nevertheless, recent work indicates\nthat tailoring QML models to the dataset, rather than relying on\ngeneric hardware-efficient circuits, might be required to retain\ntrainability as the models grow in depth and width [44, 45].\nIncorporating automated approaches to quantum circuit de-\nsign [23] into the framework is thus left for future work.\nVI.\nCONCLUSION\nWe have introduced AutoQML, an innovative framework\nfor automated QML, and evaluated its performance through\ncomprehensive benchmarking on a diverse set of problems\nderived from four distinct industrial applications, specifically\ntwo classification and two regression tasks. Our results demon-\nstrate that AutoQML is capable of effectively generating QML\npipelines that incorporate QML-specific preprocessing, model\nselection, and hyperparameter optimization. Notably, the per-\nformance of the generated pipelines was competitive with that\nof manually constructed ones, which were, wherever feasible,\nderived from existing literature to reduce subjective biases.\nThese findings indicate that AutoQML is a valuable tool for\naddressing machine learning challenges in QML, requiring\nminimal expertise in quantum computing. Additionally, the\ncapabilities of AutoQML underscore its potential as a powerful\nprototyping and benchmarking resource for QML researchers\nand practitioners.\nACKNOWLEDGMENTS\nThis work was supported by the German Federal Ministry\nof Economic Affairs and Climate Action through the project\nAutoQML (grant no. 01MQ22002A). We thank Frederic Rapp,\nTimoteo Lee and Khaled Al-Gumaei for their contributions to\nthe time series classification use case. We thank Peter Schich-\ntel and Jaroslav Vondrejc for their contribution to the time\nseries forecasting use case. We thank Giorgio Silvi for the im-\n\n\n8\nplementation of the automated backend selection and Dennis\nKleinhans for the implementation of tests for the framework.\nThe authors disclose the use of LLM-based tools for grammar\nand spelling.\nAppendix A: Manual QML Models\nIn this section, we briefly describe the pipelines for the\nmanual solutions and the process by which they were obtained.\n1.\nTime Series Classification\nTo reduce the dimensionality of the data set, we perform a\nPCA with 5 components and scale the output to the interval\n[−1,1]. The manual model is a QSVM using a projected\nquantum kernel with the feature map from [40] with 5 qubits\nand 6 layers. The model is obtained using hyperparameter\noptimization over the regularization parameters of the QSVM,\nas well as the number of layers and qubits. The model used in\nthe benchmark is the best performing from optimizing over a\nset of two quantum feature maps [40, 46].\n2.\nImage Classification\nThe manually created pipeline contains no preprocesing\nsteps in addition to those described in Sec. III B. The model\nis a QNN classifier with 8 qubits and an Ising-type cost op-\nerator. The model is the best performing model presented\nby Basilewitsch et al. [36]. Details on how the circuit has been\ndetermined can be found there.\n3.\nTabular Regression\nThe reference quantum model is obtained from Ref. [37], in\nwhich it demonstrates optimal performance on a similar data\nset for a different type of construction machinery. In line with\nthe original study, categorical features are one-hot encoded,\nresulting in a dataset with a dimensionality of d = 15. These\nfeatures are then scaled to the interval [−1,1]. The reference\nmodel in Fig. 3 is a QSVM that utilizes a Fidelity Quantum\nKernel. The encoding circuit has been obtained from Fig. 5 of\nRef. [37], and employs 15 qubits (one qubit per feature). The\nhyperparameters of the underlying SVM have been optimized\nthrough a dedicated hyperparameter optimization process.\n4.\nTime Series Forecasting\nThe manually created pipeline contains no additional pre-\nprocesing steps. The model is a 4-qubit quantum reservoir\nregressor with 54 random measurement operators which are\nfed into a linear regression model. The used reservoir is the\nresult of a search over the number of qubits, the number of\nlayers used in the encoding, the number of observable,and the\narchitecture of the encoding circuit. The search is performed\nusing Optuna.\n[1] M.-A. Zöller and M. F. Huber, Benchmark and survey of au-\ntomated machine learning frameworks, J. Artif. Int. Res. 70,\n409–472 (2021).\n[2] D. Wang, Q. V. Liao, Y. Zhang, U. Khurana, H. Samulowitz,\nS. Park, M. Muller, and L. Amini, How much automation does a\ndata scientist want? (2021), arXiv:2101.03970 [cs.LG].\n[3] F. Hutter, L. Kotthoff, and J. Vanschoren, eds., Automated Ma-\nchine Learning - Methods, Systems, Challenges (Springer, 2019).\n[4] S. Miller and D. Hughes, The quant crunch: how the demand for\ndata science skills is disrupting the job market (2017), viewed\n28 Feb 2025.\n[5] Y. Liu, S. Arunachalam, and K. Temme, A rigorous and ro-\nbust quantum speed-up in supervised machine learning, Nature\nPhysics 17, 1013 (2021).\n[6] H.-Y. Huang, M. Broughton, J. Cotler, S. Chen, J. Li,\nM. Mohseni, H. Neven, R. Babbush, R. Kueng, J. Preskill, and\nJ. R. McClean, Quantum advantage in learning from experi-\nments, Science 376, 1182 (2022).\n[7] J. Liu, M. Liu, J.-P. Liu, Z. Ye, Y. Wang, Y. Alexeev, J. Eisert,\nand L. Jiang, Towards provably efficient quantum algorithms for\nlarge-scale machine-learning models, Nature Communications\n15, 434 (2024).\n[8] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown, Auto-\nweka: combined selection and hyperparameter optimization\nof classification algorithms, in Proceedings of the 19th ACM\nSIGKDD International Conference on Knowledge Discovery and\nData Mining, KDD ’13 (Association for Computing Machinery,\nNew York, NY, USA, 2013) p. 847–855.\n[9] D. A. Kreplin, M. Willmann, J. Schnabel, F. Rapp, and M. Roth,\nsqulearn: A python library for quantum machine learning, IEEE\nSoftware , 1 (PrePrints 5555).\n[10] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,\nJ. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Per-\nrot, and Édouard Duchesnay, Scikit-learn: Machine learning in\npython, Journal of Machine Learning Research 12, 2825 (2011).\n[11] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, S. Ahmed,\nV. Ajith, M. S. Alam, G. Alonso-Linaje, B. AkashNarayanan,\nA. Asadi, and others, Pennylane: Automatic differentiation of hy-\nbrid quantum-classical computations (2022), arXiv:1811.04968\n[quant-ph].\n[12] Qiskit Community, Qiskit: An open-source framework for quan-\ntum computing (2017).\n[13] IBM\nQuantum,\nhttps://quantum-computing.ibm.com\n(2023).\n[14] Amazon Web Services, Amazon Braket (2020).\n[15] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, Optuna:\nA next-generation hyperparameter optimization framework (As-\nsociation for Computing Machinery, 2019) pp. 2623–2631.\n\n\n9\n[16] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez,\nand I. Stoica, Tune: A research platform for distributed model\nselection and training (2018), arXiv:1807.05118 [cs.LG].\n[17] R. Berganza Gómez, C. O’Meara, G. Cortiana, C. B. Mendl,\nand J. Bernabé-Moreno, Towards autoqml: A cloud-based auto-\nmated circuit architecture search framework, in 2022 IEEE 19th\nInternational Conference on Software Architecture Companion\n(ICSA-C) (2022) pp. 129–136.\n[18] S. Altares-López, J. J. García-Ripoll, and A. Ribeiro, Autoqml:\nAutomatic generation and training of robust quantum-inspired\nclassifiers by using evolutionary algorithms on grayscale images,\nExpert Systems with Applications 244, 122984 (2024).\n[19] T. Koike-Akino, P. Wang, and Y. Wang, Autoqml: Automated\nquantum machine learning for wi-fi integrated sensing and com-\nmunications, in 2022 IEEE 12th Sensor Array and Multichannel\nSignal Processing Workshop (SAM) (2022) pp. 360–364.\n[20] S. Altares-López, A. Ribeiro, and J. J. García-Ripoll, Automatic\ndesign of quantum feature maps, Quantum Science and Technol-\nogy 6, 045015 (2021).\n[21] M. Incudini, D. L. Bosco, F. Martini, M. Grossi, G. Serra, and\nA. D. Pierro, Automatic and effective discovery of quantum ker-\nnels, IEEE Transactions on Emerging Topics in Computational\nIntelligence , 1 (2024).\n[22] X. Dai, T.-C. Wei, S. Yoo, and S. Y.-C. Chen, Quantum machine\nlearning architecture search via deep reinforcement learning, in\n2024 IEEE International Conference on Quantum Computing\nand Engineering (QCE), Vol. 01 (2024) pp. 1525–1534.\n[23] F. Rapp, D. A. Kreplin, M. F. Huber, and M. Roth, Reinforce-\nment learning-based architecture search for quantum machine\nlearning, Machine Learning: Science and Technology 6, 015041\n(2025).\n[24] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,\nA. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chil-\namkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch:\nan imperative style, high-performance deep learning library, in\nProceedings of the 33rd International Conference on Neural\nInformation Processing Systems (Curran Associates Inc., Red\nHook, NY, USA, 2019).\n[25] M.-A. Zöller, T.-D. Nguyen, and M. F. Huber, Incremental\nsearch space construction for machine learning pipeline syn-\nthesis (2021), arXiv:2101.10951 [cs.LG].\n[26] F. Hutter, L. Kotthoff, and J. Vanschoren, Automated Machine\nLearning - Methods, Systems, Challenges (Springer, 2019).\n[27] M. Feurer, A. Klein, K. Eggensperger, J. T. Springenber,\nM. Blum, and F. Hutter, Efficient and robust automated machine\nlearning, in International Conference on Neural Information Pro-\ncessing Systems, edited by C. Cortes, D. D. Lee, M. Sugiyama,\nand R. Garnett (MIT Press, 2015) pp. 2755–2763.\n[28] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller,\nO. Grisel, V. Niculae, P. Prettenhofer, A. Gramfort, J. Grobler,\nR. Layton, J. VanderPlas, A. Joly, B. Holt, and G. Varoquaux,\nAPI design for machine learning software: experiences from the\nscikit-learn project, in ECML PKDD Workshop: Languages for\nData Mining and Machine Learning (2013) pp. 108–122.\n[29] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, Algorithms\nfor hyper-parameter optimization, in Advances in Neural Infor-\nmation Processing Systems, Vol. 24, edited by J. Shawe-Taylor,\nR. Zemel, P. Bartlett, F. Pereira, and K. Weinberger (Curran\nAssociates, Inc., 2011).\n[30] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, Quantum\ncircuit learning, Phys. Rev. A 98, 032309 (2018).\n[31] L. Innocenti, S. Lorenzo, I. Palmisano, A. Ferraro, M. Pater-\nnostro, and G. M. Palma, Potential and limitations of quantum\nextreme learning machines, Communications Physics 6, 118\n(2023).\n[32] J. Schnabel and M. Roth, Quantum kernel methods under\nscrutiny: A benchmarking study (2024), arXiv:2409.04406\n[quant-ph].\n[33] V. Havlíˇcek, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kan-\ndala, J. M. Chow, and J. M. Gambetta, Supervised learning with\nquantum-enhanced feature spaces, Nature 567, 209 (2019).\n[34] F. Rapp and M. Roth, Quantum gaussian process regression\nfor bayesian optimization, Quantum Machine Intelligence 6, 5\n(2024).\n[35] F. Struckmeier and F. P. León, Nesting in the sheet metal indus-\ntry: dealing with constraints of flatbed laser-cutting machines,\nProcedia Manufacturing 29, 575 (2019), “18th International\nConference on Sheet Metal, SHEMET 2019”“New Trends and\nDevelopments in Sheet Metal Processing”.\n[36] D. Basilewitsch, J. F. Bravo, C. Tutschku, and F. Struckmeier,\nQuantum neural networks in practice: A comparative study with\nclassical models from standard data sets to industrial images\n(2024), arXiv:2411.19276 [quant-ph].\n[37] H. Stühler, D. Pranjic, and C. Tutschku, Evaluating quantum\nsupport vector regression methods for price forecasting applica-\ntions., in ICAART (3) (2024) pp. 376–384.\n[38] T. Chen and C. Guestrin, Xgboost: A scalable tree boosting\nsystem, in Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, KDD\n’16 (Association for Computing Machinery, New York, NY,\nUSA, 2016) p. 785–794.\n[39] sQUlearn Team, sQUlearn Documentation (2025).\n[40] T. Hubregtsen, D. Wierichs, E. Gil-Fuster, P.-J. H. S. Derks,\nP. K. Faehrmann, and J. J. Meyer, Training quantum embedding\nkernels on near-term quantum computers (2021).\n[41] T. Haug, C. N. Self, and M. S. Kim, Quantum machine learn-\ning of large datasets using randomized measurements, Machine\nLearning: Science and Technology 4, 015005 (2023).\n[42] D. Aharonov, X. Gao, Z. Landau, Y. Liu, and U. Vazirani, A\npolynomial-time classical algorithm for noisy random circuit\nsampling, in Proceedings of the 55th Annual ACM Symposium on\nTheory of Computing, STOC 2023 (Association for Computing\nMachinery, New York, NY, USA, 2023) p. 945–957.\n[43] A. Angrisani, A. Schmidhuber, M. S. Rudolph, M. Cerezo,\nZ. Holmes, and H.-Y. Huang, Classically estimating observ-\nables of noiseless quantum circuits (2024), arXiv:2409.01706\n[quant-ph].\n[44] J. Kübler, S. Buchholz, and B. Schölkopf, The inductive bias of\nquantum kernels, in Advances in Neural Information Process-\ning Systems, Vol. 34, edited by M. Ranzato, A. Beygelzimer,\nY. Dauphin, P. Liang, and J. W. Vaughan (Curran Associates,\nInc., 2021) pp. 12661–12673.\n[45] M. Cerezo, G. Verdon, H.-Y. Huang, L. Cincio, and P. J. Coles,\nChallenges and opportunities in quantum machine learning, Na-\nture Computational Science 2, 567 (2022).\n[46] D. A. Kreplin and M. Roth, Reduction of finite sampling noise\nin quantum neural networks, Quantum 8, 1385 (2024).\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21025v1.pdf",
    "total_pages": 9,
    "title": "AutoQML: A Framework for Automated Quantum Machine Learning",
    "authors": [
      "Marco Roth",
      "David A. Kreplin",
      "Daniel Basilewitsch",
      "João F. Bravo",
      "Dennis Klau",
      "Milan Marinov",
      "Daniel Pranjic",
      "Horst Stuehler",
      "Moritz Willmann",
      "Marc-André Zöller"
    ],
    "abstract": "Automated Machine Learning (AutoML) has significantly advanced the efficiency\nof ML-focused software development by automating hyperparameter optimization\nand pipeline construction, reducing the need for manual intervention. Quantum\nMachine Learning (QML) offers the potential to surpass classical machine\nlearning (ML) capabilities by utilizing quantum computing. However, the\ncomplexity of QML presents substantial entry barriers. We introduce\n\\emph{AutoQML}, a novel framework that adapts the AutoML approach to QML,\nproviding a modular and unified programming interface to facilitate the\ndevelopment of QML pipelines. AutoQML leverages the QML library sQUlearn to\nsupport a variety of QML algorithms. The framework is capable of constructing\nend-to-end pipelines for supervised learning tasks, ensuring accessibility and\nefficacy. We evaluate AutoQML across four industrial use cases, demonstrating\nits ability to generate high-performing QML pipelines that are competitive with\nboth classical ML models and manually crafted quantum solutions.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}