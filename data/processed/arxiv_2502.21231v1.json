{
  "id": "arxiv_2502.21231v1",
  "text": "ByteScale: Efficient Scaling of LLM Training with a\n2048K Context Length on More Than 12,000 GPUs\nHao Ge‚àó\ngehao@stu.pku.edu.cn\nPeking University\nJunda Feng‚àó\nfengjunda.aml@bytedance.com\nByteDance Seed\nQi Huang‚àó\nhuangqi.lucky@bytedance.com\nByteDance Seed\nFangcheng Fu‚Ä†\nccchengff@pku.edu.cn\nPeking University\nXiaonan Nie\nniexiaonan@bytedance.com\nByteDance Seed\nLei Zuo\nzuo.lei@bytedance.com\nByteDance Seed\nHaibin Lin‚Ä†\nhaibin.lin@bytedance.com\nByteDance Seed\nBin Cui‚Ä†\nbin.cui@pku.edu.cn\nPeking University\nXin Liu‚Ä†\nliuxin.ai@bytedance.com\nByteDance Seed\nAbstract\nScaling long-context ability is essential for Large Language\nModels (LLMs). To amortize the memory consumption across\nmultiple devices in long-context training, inter-data parti-\ntioning (a.k.a. Data Parallelism) and intra-data partitioning\n(a.k.a. Context Parallelism) are commonly used. Current\ntraining frameworks predominantly treat the two techniques\nas orthogonal, and establish static communication groups\nto organize the devices as a static mesh (e.g., a 2D mesh).\nHowever, the sequences for LLM training typically vary in\nlengths, no matter for texts, multi-modalities or reinforce-\nment learning. The mismatch between data heterogeneity\nand static mesh causes redundant communication and im-\nbalanced computation, degrading the training efficiency.\nIn this work, we introduce ByteScale, an efficient, flexible,\nand scalable LLM training framework for large-scale mixed\ntraining of long and short sequences. The core of ByteScale\nis a novel parallelism strategy, namely Hybrid Data Paral-\nlelism (HDP), which unifies the inter- and intra-data parti-\ntioning with a dynamic mesh design. In particular, we build\na communication optimizer, which eliminates the redundant\ncommunication for short sequences by data-aware shard-\ning and dynamic communication, and further compresses\nthe communication cost for long sequences by selective of-\nfloading. Besides, we also develop a balance scheduler to\nmitigate the imbalanced computation by parallelism-aware\ndata assignment. We evaluate ByteScale with the model\nsizes ranging from 7B to 141B, context lengths from 256K to\n2048K, on a production cluster with more than 12,000 GPUs.\nExperiment results show that ByteScale outperforms the\nstate-of-the-art training system by up to 7.89√ó.\n1\nIntroduction\nIn recent years, large language models (LLMs) have achieved\nremarkable success across various domains. The impressive\n‚àóEqual Contribution.\n‚Ä†Corresponding Authors.\nperformance of LLMs is attributed to increased model sizes,\nlarger volumes of training data, and longer context windows,\nall in accordance with the scaling law [20]. The demand\nfor long-context capabilities of LLMs has increased rapidly,\nas modern LLM applications like documents summariza-\ntion [19], video understanding [41, 42], agent interaction [1]\nand code completion [27], require the model to understand\nlong-range dependencies. It has driven many organizations\nto extend their models‚Äô context lengths. For instance, Meta‚Äôs\nLLaMA3 [11] and OpenAI‚Äôs GPT-4o [33] support 128K con-\ntexts, Anthropic‚Äôs Claude3 [3] supports 200K, and Google‚Äôs\nGemini-1.5 Pro [13] supports up to 2M contexts.\nA fundamental challenge in scaling to a long context is\nthe quadratic scaling of memory and computation for self-\nattention. Flash Attention [7, 8] has been proposed to reduce\nthe memory complexity from ùëÇ(ùëÜ2) to ùëÇ(ùëÜ), where ùëÜis the\nsequence length. To further scale the context length, it‚Äôs\nnecessary to partition the sequences across multiple devices.\nThere are broadly two categories: inter-data partitioning\n(a.k.a. Data Parallelism, DP [9, 24, 37]) distributes different\nsequences across the devices, while intra-data partitioning\n(a.k.a. Context Parallelism, CP [4, 23, 25, 31]) scatter a sin-\ngle sequence. Both categories evenly reduce the memory\nconsumption on each device, while inevitably incurring ex-\ntra communication overhead. Existing LLM training frame-\nworks, such as Megatron-LM [21, 30, 38], DeepSpeed [17, 36]\nand MegaScale [18], treat the two categories as individual\nparallelism strategies, and establish DP√óCP communication\ngroups to organize the devices as a static mesh (e.g., a 2D\nmesh), where the size of each CP group is dependent on the\nmaximum sequence length (i.e., context length). Undoubt-\nedly, it requires the sequences to be of the same length so\nthat the training workloads across devices are uniform.\nNevertheless, the sequences for LLM training usually vary\nin lengths. For one thing, sequence lengths typically exhibit\nskewed distribution in real-world datasets, no matter the text\narXiv:2502.21231v1  [cs.DC]  28 Feb 2025\n\n\nor multi-modal data. For another thing, inference-time scal-\ning (e.g. OpenAI‚Äôs o1 [34], DeepSeek-R1 [10]) increases the\nlength of the Chain-of-Thought reasoning process, further\nexacerbates length heterogeneity for reinforcement learning.\nWhen facing the sequences with variable lengths, existing\nframeworks can only configure the size of CP groups to be\nlarge enough to handle the longest sequences (yielding a\nsmall DP size), and each sample needs to be evenly parti-\ntioned across the entire CP group, regardless of sequence\nlength, degrading the overall training efficiency.\nIn particular, the mismatch between data heterogeneity\nand static system design causes two main challenges (de-\ntailed in ¬ß3). 1‚óãRedundant Communication: It is common\npractice to pack [22] shorter sequences into a single one up\nto the context length and configure a sufficient CP size to\nprevent out-of-memory (OOM) errors. However, all short se-\nquences have to undergo the same partitioning and commu-\nnication process as long sequences, even if it is unnecessary.\nWorse yet, CP requires ùëÇ(ùëÜ2) computation to overlap ùëÇ(ùëÜ)\ncommunication, which is challenging for short sequences.\n2‚óãImbalanced Computation: Although tokens are evenly\npartitioned across devices by CP and memory is balanced,\nexecution times still vary. This is because the computational\ncomplexity of each token is related to the original sequence\nlength, which is ùëÇ(ùëÜ2). The imbalanced computation causes\nsome devices to fall into idle time for synchronization.\nSummary of Contributions. To address the aforemen-\ntioned challenges, we propose ByteScale, an efficient, flexi-\nble, and scalable training framework designed for large-scale\nmixed training of long and short sequences. The main con-\ntributions are as follows:\nC1: Proposal of Hybrid Data Parallelism. We propose\na novel parallelism strategy, namely Hybrid Data Parallelism\n(HDP), which unifies both inter-data (DP) and intra-data\npartitioning (CP), and is defined to evenly distributing tokens\nacross devices. It utilizes devices in the range of [1, DP√óCP]\nto flexibly process variable-length sequences.\nC2: Communication Optimizations. To eliminate re-\ndundant communication for short sequences, HDP provides\nthe ability of data-aware sharding, where dynamic communi-\ncation groups are automatically built and each sequence will\nbe processed with a minimal number of devices individually.\nBesides, HDP also provides selective offloading to further\ncompress the communication cost for long sequences.\nC3: Balance Strategy. To mitigate the imbalanced compu-\ntation, we design a heuristic algorithm that reorganizes data\nassignment based on the characteristics of data and pipeline\nparallelism. Furthermore, for those devices with shorter exe-\ncution times, we assign more micro batches, rather than the\nsame number under the static system design.\nC4: Evaluation. We conduct experiments on a production\ncluster with more than 12,000 GPUs, scaling the model size\nfrom 7B to 141B, and context length from 256K to 2048K. The\nLinaer\nQ\ntokens\nK\nV\nMHA/GQA\nNorm\nLinaer\ntoken-wise\ncross-tokens\nAttention Module\nLinaer\nNorm\nLinaer\nFFN Module\nGeLU\nFigure 1. the Architecture of Transformer layer\n(c) packing\n(a) origin batch\n(b) padding\nx 4\nx 4\n(d) attn mask & time\nmem\ntime\nFigure 2. Sequence Padding and Packing\nresults demonstrate that ByteScale achieves up to 7.89√ó of\nspeedup compared to existing training approaches.\n2\nBackground\n2.1\nTransformers and Large Language Models\nThe transformer architecture [40] has become the most pop-\nular and widely used foundational architecture for large\nlanguage models (LLMs) [5, 14, 32, 39] nowadays. It typically\nconsists of a series of transformer layers, each comprising an\nattention module and a feed-forward network (FFN) module.\nAs shown in Figure 1, self-attention captures contextual in-\nformation throughout the entire text, necessitating all tokens\nin the full sequence to participate in computation. In con-\ntrast, other operations like normalization, linear projection,\nand activation functions perform token-wise computations,\nallowing each token to be processed independently.\n2.2\nDistributed LLM Training\nAs model sizes and training data continue to scale, distributed\ntraining techniques are indispensable in LLM training.\nData Parallelism. Data parallelism (DP) [9, 24, 37] dis-\ntributes the training data evenly across devices, while each\ndevice holds a replica of the model. During each training step,\ndevices process their local data individually, and synchro-\nnize gradients globally to update the model. ZeRO series [35]\nmethods further enhance the scalability of DP.\nModel Parallelism. Model parallelism distributes the\nmodel across devices, including tensor parallelism (TP) [38]\nand pipeline parallelism (PP) [16, 28, 29]. TP performs intra-\noperation partitioning, dividing operations and parameters\nwithin a layer across devices (e.g. Row- and Col-Parallel\nLinear in Megatron-LM [38]). It requires communication\nof intermediate results (activations), and is typically used\nwithin a single node. PP employs inter-operation partition-\ning, segmenting the model layers into different stages. It\nrequires only the exchange of activations between consecu-\ntive stages via peer-to-peer (P2P) communication, enabling\nmodel partitioning across multiple nodes.\n2\n\n\n(a) data & context parallelism\nmb#0 mb#1\ncp = 2\ncp = 2\nmb#0 mb#1\n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \nDP0\nDP1\nrank0\nrank1\nrank2\nrank3\nmb#0\nmb#1\nbubble\ntime\nmb#0 mb#1\ngrad sync\nmb#0\nmb#1\nmb#0 mb#1\ngrad sync\nbubble\nDP1\ncp = 2\nmb#0 mb#1\n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \n¬Ω \np2p \ncomm\nmb#0 mb#1\ndp = 2\nno \ncomm\nrank2\nrank3\nrank0\nrank2\nDP0\n(b) redundant communication\n(c) imbalance computation\nFigure 3. Context Parallelism with Packing\n2k\n4k\n8k\n16k\n32k\n64k 128k 256k 512k\n1M\n2M\n0.0\n0.2\n0.4\n0.6\n0.8\nSample Ratio\n60.00%\n24.00%\n8.00%\n4.00%\n2.00%\n1.00%\n0.50%\n0.25%\n0.12%\n0.08%\n0.05%\n77.72%\n10.84%\n5.94%\n3.07%\n1.46%\n0.63%\n0.23%\n0.08%\n0.02%\n0.008%\n0.003%\nByted\nGitHub\n2k\n14.2%\n4k\n11.4%\n8k\n7.6%\n16k\n7.6%\n32k\n7.6%\n64k\n7.6%\n128k\n7.6%\n256k\n7.6%\n512k\n7.3%\n1M\n9.7%\n2M\n12.1%\n2k\n34.0%\n4k\n9.5%\n8k\n10.4%\n16k\n10.7%\n32k\n10.2%\n64k\n8.8%\n128k\n6.4%\n256k\n4.5%\n512k\n2.2%\n1M\n1.8%\n2M\n1.3%\nToken Ratio\nFigure 4. Sample and Token Distribution in two Datasets\nHybrid Parallelism. Hybrid parallelism combines var-\nious parallel strategies to enhance training efficiency. Par-\nticularly, Megatron-LM employs the 3D parallelism strat-\negy [21, 30, 38] by integrating DP, TP, and PP, making it a\nmainstream approach for large-scale model training today.\nGradient Accumulation. To improve the efficiency and\nconvergence, LLMs typically require large batch size [6, 15,\n39] (e.g. it is common practice to apply nearly 30~80M tokens\nper batch for LLM training in the cluster with 10K GPUs).\nConstrained by hardware memory, processing the entire\nlarge batch at once is infeasible. Gradient accumulation di-\nvides each global batch (i.e., the sampled data in each training\nstep) into multiple micro-batches. The gradients from these\nmicro-batches are accumulated to equal the gradient as if\nthe entire global batch were processed in a single pass.\n2.3\nPadding and Packing\nTo support variable-length sequences in current static par-\nallelism strategies, techniques such as padding and packing\nare necessary. As illustrated in Figure 2, padding pads the\nsequences in the same batch to be of the same length, but\ncauses wasted computation. Packing [22] concatenates mul-\ntiple sequences into a single one without padded tokens. It\nemploys a special segmented attention mask to ensure that\neach sequence is processed independently by self-attention.\n2.4\nLong Context Training\nAs self-attention exhibits both time and memory complex-\nity of ùëÇ(ùëÜ2), when the context length scales, this quadratic\ncomplexity becomes a bottleneck. Flash Attention [7, 8]\noptimizes memory I/O and employs the tiling technique\nto reduce memory complexity from ùëÇ(ùëÜ2) to ùëÇ(ùëÜ), while\nstill maintaining ùëÇ(ùëÜ2) time complexity. Context Parallelism\n(CP) [4, 23, 25, 31] further partitions the sequence across ùëÅ\ndevices, reducing the memory fromùëÇ(ùëÜ) toùëÇ( ùëÜ\nùëÅ). Following\nFigure 1, CP shards QKV along the sequence dimension, and\ncross-tokens operations require KV slices to be exchanged\nacross devices using a ring-style P2P communication, which\noverlaps with computation. This technique is also applicable\nto packed sequences, and we will detail its implementation\nin ¬ß7. Notably, each subsequence must also be sharded across\nall CP ranks, as illustrated in Figure 2(c) and 3(a).\n3\nObervation & Motivation\n3.1\nData Heterogeneity\nLLMs are trained on sequences data. As mentioned in ¬ß1, the\ntraining data typically consists of variable-length sequences.\nThere exist two observations and one significant challenge:\nObservation 1: sequence lengths exhibit skewed distri-\nbution in real-world datasets. As shown in Figure 4, we\nprofiled the sample and token distribution of two datasets: an\nopen-source dataset GitHub and a productive dataset Byted\nfor long-context training. We observed that both of them ex-\nhibit a skewed distribution in sequence lengths. For instance,\nin the Byted dataset, if we randomly sample a global batch,\nnearly 80% of the samples are 4K tokens or shorter, while\nonly 0.05% of the samples can reach 2M tokens. However,\nfrom the perspective of token distribution, those 0.05% of the\nsamples (>=2M) contribute 12.1% of the tokens in the global\nbatch, and 1% of the samples (>=128K) contribute 44.3%. Al-\nthough the GitHub dataset has a lower proportion of long\nsequences, 16.2% of its tokens come from sequences exceed-\ning 128K, demonstrating significant data heterogeneity.\nObservation 2: mixing long and short sequences en-\nhances model performance. The existing work [12] has\ndemonstrated that training exclusively on long-context data\ncan lead to a decline in short-context performance. LLaMA3\nreport [11] indicates that when training a model with 128K\ncontext, mixing 0.1% of long data with the original short data\noptimizes the performance across both short-context and\nlong-context benchmarks. DeepSeek-R1 [10] presents the\naverage response length on the training set during the RL\nprocess, demonstrating that gradually increasing and diverse\nresponse lengths help improve model performance.\n3\n\n\n1 2\n0\n0\n3\n0\n1 2\n0\n1\n0\n0\n0\n1\n2\n3\n4\n1\n1\n1\n1\n2\n3\n4\n5\n2\n2\n2\n2\n4\n5\n6\n3\n3\n3\n3\n4\n5\n6\n7\n4\n4\n4\n4\n5\n6\n7\n5\n5\n5\n5\n6\n7\n6\n6\n6\n6\n7\n7\n7\n7\n1\n0\n0\n0\n0\n0\n2 3\n1\n2\n1\n1\n2\n3\n4\n1\n3\n4\n5\n2\n33\npp bubble\n0\n0\n0\n1\n1\n1\n2\n2\n2\n2\n4\n5\n6\n3\n3\n3\n4\n5\n6\n4\n4\n4\n4\n5\n6\n5\n5\n5\n5\n6 6\n6\n6\n6\n7\n7\n7\n7\n7\n7\n7\n7\npp bubble\n3\n7\npp bubble\ngrad sync & \nmodel update\nsync\nwait\npp bubble\ndp bubble\npipeline (b)\npipeline (a)\n0\nFigure 5. Imbalanced Data and Pipeline Parallelism\n1e12\n(a) execution time of attention and mlp\n(b) imbalanced FLOPs\n0%\n25%\n50%\n75%\n100%\n128k\n64k\n32k\n16k\n8k\n4k\n4.8 ms\n3.6 ms\n7.5 ms\n15.1 \n28.2\n14.1 ms\n48.2 ms\n175.5 ms\nattn\nmlp\n0.8 ms\n0.9 ms\n1.8 ms\n1.8 ms\nFigure 6. Imbalanced Computation\nChallenge: data heterogeneity leads to efficiency degra-\ndation. Although mixed training of long and short sequences\nis common and beneficial for model performance, it intro-\nduces new challenges. The static parallelism strategies used\nin existing systems are not well-suited to handle dynamic\nworkloads. This causes issues of redundant communication\n(¬ß3.2) and imbalanced computation (¬ß3.3), which we will\ndiscuss in more detail below.\n3.2\nRedundant Communication\nExisting systems apply static parallelism strategies through-\nout the training process. Typically, they assume that all\n(packed) sequences are of the same length and set a fixed\nCP degree to amortize them across enough devices, thereby\navoiding OOM errors. As mentioned in ¬ß2.3, to handle variable-\nlength sequences, it is common to pack sequences up to the\ncontext length. However, as depicted in Figure 3(a)-(b), all\nsequences have to be partitioned across the entire CP group,\neven if it is unnecessary for shorter ones.\nFor instance, assuming that each device has a capacity\nof 8K tokens, to train an LLM with a context length of 1M\ntokens, a CP degree of 128 is required. This configuration ne-\ncessitates 128 individual devices to process a sequence of 1M\ntokens. Concurrently, a large number of shorter sequences,\nsuch as those with lengths of 4K, 8K, and 16K tokens, are\npacked up to 1M tokens and processed in a CP group with 128\ndevices. As depicted in Figure 14, each subsequence within\nthe packed sequence needs to be partitioned into 128 chunks\nacross CP ranks, performing ring-P2P communication. In\nfact, it is unnecessary to perform cross-device partitioning\nand communication for sequences with lengths under 8K.\nFor those sequences with 16K tokens, only two CP ranks are\n¬ß5. Communication Optimizer\ncp x 4\ndp x 1\ncp x 2\norigin batch\nmodel\ntime line\nsync & \nupdate \nmodel\nHDP size=4\n¬ß6. Balance Scheduler\nHDP Profiler\nBalance \nScheduler\nDP \nBalance\nPP \nBalance\nCommunication \nOptimizer\nFigure 7. ByteScale Overview\nrequired. Using the same CP degree as for the maximum se-\nquence length leads to excessive redundant communication\nfor these shorter sequences. This issue is exacerbated when\nsequence lengths are highly skewed.\n3.3\nImbalanced Computation\nImbalanced FLOPs. Although Flash Attention enables lin-\near packing with ùëÇ(ùëÜ) memory complexity, the computa-\ntional complexity for each subsequence remains ùëÇ(ùëÜ2). As\ndepicted in Figures 2(d) and 3(c), even if two packed se-\nquences contain the same number of tokens, their actual\ncomputational workloads differ, which are proportional to\nthe areas of attention mask. As shown in Figure 6(a), when\nthe context length is shorter than 8K tokens, the ùëÇ(ùëÜ2) term\nis relatively insignificant, allowing packing to effectively bal-\nance workloads for both memory and computation. However,\nfor long-context training tasks, the ùëÇ(ùëÜ2) term becomes the\npredominant component of the computation, leading to sig-\nnificant time imbalances across different packed sequences.\nTo provide an intuitive explanation, we sampled a global\nbatch of 1.2M tokens from the GitHub dataset and randomly\npacked them into micro-batches of up to 32K tokens, aligning\nwith the model‚Äôs context length. As shown in Figure 6(b),\nwe recorded the FLOPs (Floating Point Operations) for each\nmicro-batch and observed significant variability, indicating\nthat the execution time for each micro-batch also differs.\nImbalanced Data and Pipeline Parallelism. The imbal-\nanced execution times across micro-batches further degrade\n4\n\n\nQKV\nK0 V0\nO00\nK1 V1\nO01\nK2 V2\nO02\nK3 V3\nO03\nO0\n‚Ä¶\n‚Ä¶\nQKV\nO1\nQKV\nO2\nQKV\nO3\nQ0\nK0 V0\nO10\nK1 V1\nO11\nK2 V2\nO12\nK3 V3\nO13\nQ1\nK0 V0\nO20\nK1 V1\nO21\nK2 V2\nO22\nK3 V3\nO23\nQ2\nK0 V0\nO30\nK1 V1\nO31\nK2 V2\nO32\nK3 V3\nO33\nQ3\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nQKV\nK0 V0\nO00\nK1 V1\nO01\nO0\n‚Ä¶\n‚Ä¶\nQ0\nQKV\nK0 V0\nO10\nK1 V1\nO11\nO1\n‚Ä¶\n‚Ä¶\nQ1\nQKV\nK0 V0\nO00\nK1 V1\nO01\nO0\n‚Ä¶\n‚Ä¶\nQ0\nQKV\nK0 V0\nO10\nK1 V1\nO11\nO1\n‚Ä¶\n‚Ä¶\nQ1\nQKV\nK0 V0\nO00\nK1 V1\nO01\nO0\n‚Ä¶\n‚Ä¶\nQ0\nQKV\nK0 V0\nO10\nK1 V1\nO11\nO1\n‚Ä¶\n‚Ä¶\nQ1\nQKV\nK V\nO\n‚Ä¶\n‚Ä¶\nQ\nQKV\nK V\nO\n‚Ä¶\n‚Ä¶\nQ\ngrad \nacc\n(a) optimizer \nstates\ngrad \nacc\ngrad acc \n& update\n(b) case1: comm_groups = 1, \nsize = (4)\n(c) case2: comm_groups = 2, \nsize = (2, 2)\n(d) case3: comm_groups = 3, \nsize = (1, 2, 1)\nMB#0\nMB#1\nMB#2\nS0\nS1\nS2\nS3\nS4\nS5\nrank0\nrank1\nrank2\nrank3\nFigure 8. Illustration of HDP\nthe efficiency of data and pipeline parallelism. In data par-\nallelism, all DP ranks must execute the same number of\nmicro-batches, and then synchronize gradients before the\nmodel update. As illustrated in Figure 3(c), rank-2 processes\ntokens with fewer FLOPs than rank-0, leading to idle time\n(i.e. DP Bubble) as it waits for synchronization. In pipeline\nparallelism, there are two types of ‚Äúbubbles‚Äù: the PP bubble\noccurs within a single pipeline, and the DP bubble occurs\nacross different pipelines (different DP groups). Aside from\nPP bubbles during the warmup and cooldown phases, imbal-\nanced FLOPs between micro-batches prevent the execution\ntime on different devices from overlapping as they would in\nan ideal pipeline. This leads to extra PP bubbles caused by\ninter-stage waiting, as shown in Figure 5. Additionally, since\neach micro-batch is executed sequentially across ùëëpp differ-\nent stages in the pipeline, any DP bubble will be magnified\nby a factor of ùëëpp. For example, consider two pipelines illus-\ntrated in Figure 5, the micro-batches 0 and 7 in the pipeline\n(a) have a longer forward and backward execution time com-\npared to those in the pipeline (b). Under ùëëpp = 4, this time\ngap is magnified fourfold. Consequently, after executing 8\nmicro-batches, the pipeline (b) falls into a prolonged idle\nperiod, waiting for gradient synchronization. This causes\nthe DP bubble to account for over 30% of the total execution\ntime, far exceeding the normal pipeline bubble time.\n4\nByteScale Overview\nWe present ByteScale to address these challenges. As shown\nin Figure 7, it consists of three main components. Profiler\nis to profile the environment, model configuration, data\ndistribution, and build cost models for other components.\nCommunication Optimizer is to improve the communication\nefficiency for both short and long sequences by data-aware\nsharding, dynamic communication, and selective offloading.\nBalance Scheduler is to solve the imbalanced computation\nby parallelism-aware data assignment.\n5\nCommunication Optimizer\nThis section describes how ByteScale optimizes communi-\ncation overhead. First, it reduces redundant communication\nfor short sequences by dynamic sequence sharding and com-\nmunication. Second, it further compresses the communica-\ntion cost for long sequences by selective offloading.\n5.1\nData-Aware Sharding and Communication\nHybrid Data Parallelism. To begin with, we introduce a\nnovel parallelism strategy, namely Hybrid Data Parallelism\n(HDP), to enable efficient training for different levels of se-\nquence lengths. Both DP and CP partition training data\nacross devices. DP performs inter-data partitioning by dis-\ntributing different samples evenly across devices, while CP\nperforms intra-data partitioning by sharding a single sample\nacross devices. HDP unifies both inter-data and intra-data\npartitioning and is defined to evenly distribute tokens across\ndevices. It can replace traditional DP and CP, with the paral-\nlel degree of HDP equivalent to the product of the degrees\nof DP and CP (i.e. ùëëhdp = ùëëdp √ó ùëëcp).\nUnlike DP and CP, which require all DP/CP ranks to per-\nform consistent behavior in computation or communication\n(e.g. CP requires all CP ranks to participate in homogeneous\nring-P2P communication), HDP allows for heterogeneous\nbehavior among HDP ranks. It has two key characteristics:\n1‚óãMore Flexible Communication: HDP only requires that\ndifferent HDP ranks handle an equal number of tokens.\nThis means that some HDP ranks may be assigned com-\nplete sequences (short sequences), as illustrated by ùëÜ3 and\nùëÜ5 in Figure 8(d), while some other ranks may only handle\nthe partial slice of a sequence (long sequences), as shown\nwith ùëÜ4 in Figure 8(d). This necessitates establishing more\nflexible communication groups. For instance, in Figure 8(d),\na communication group of size 2 is created only between\nrank-[1~2] to compute the distributed attention for ùëÜ4,\nwhile rank-0 and 3 can perform local computation without\ncross-device communication. In Figure 8(b), sequence ùëÜ0\nis sharded into four slices, and a communication group of\nsize 4 is created among rank-[0~3].\n2‚óãMore Finer-Grained Communication: Static parallel\nstrategies require that the product of the parallel degrees\nequals the number of devices in the cluster, i.e. ùëëdp √ó ùëëcp √ó\nùëëtp √ó ùëëpp = ùëÅcluster, where ùëëtp and ùëëpp are actually fixed\nbased on model size. To utilize all the devices and maintain\n5\n\n\ntokens\nparam\nout\ngradout\ntokensT\ngradparam\nsum\ngradparam\nforward\nToken-level Loss\nbackward\nFigure 9. Token-Level Gradient\nlayer1\nactivation \n0,1,...,29\noverlap\nactivation1\nactivation0\nCPU\nGPU\nlayer1\nlayer31\nlayer31\nH2D\ncompute\ncompute\nH2D\nH2D\nlayer0\nlayer30\nlayer0\nlayer30\nactivation0\nactivation30\nactivation \n0,1,...,30\nactivation31\ncompute\nO(N2)\nO(N)\nTime\nseqlen\nCompute: O(N2)\nD2H & H2D: O(N)\nVS\nactivation1\nactivation0\nD2H\nactivation \n0,1,...,29\nactivation30\ncompute\ncompute\nD2H\noverlap\nactivation \n0,1,...,30\nactivation31\nD2H\ncompute\ncompute\nactivation0\nFigure 10. Per-Layer Activation Offloading\nthis divisibility, ùëëdp and ùëëcp can only be scaled by a limited\nfactor, resulting in coarse granularity (e.g. assume each\nrank can handle 8K tokens, 512K can use <ùëëdp = 2, ùëëcp =\n64>, while 768K needs ùëëcp = 96 but must use <ùëëdp = 1,\nùëëcp = 128>). Meanwhile, HDP can use any amount of ranks\nin [1,ùëëhdp] to handle a sequence without considering the\ndivisibility constraints (e.g. with ùëëhdp = ùëëdp √ó ùëëcp = 128,\nHDP can use 96 ranks to handle a 768K sequence while use\nrest 32 ranks to handle 32 √ó 8K sequences individually).\nNCCL Buffer Optimization. Creating NCCL communi-\ncation groups incurs extra overhead. Firstly, the process of\nestablishing a communication group is inherently slow, and\ndynamically creating new groups for each sequence can sig-\nnificantly reduce training efficiency. Secondly, creating an\nexcessive number of communication groups can consume an\nadditional 5~10GB of memory per GPU for NCCL buffers, fur-\nther reducing the available memory. Fortunately, distributed\nattention utilizes P2P communication. With a global com-\nmunication group across all HDP ranks, P2P communication\nbetween any two devices can directly reuse the existing\ngroup, thereby alleviating the time and memory pressure\nassociated with creating temporary communication groups.\nOptimizer States Sharding. HDP evenly partitions to-\nkens across devices, and will shard neither model param-\neters nor gradients. This means that HDP ranks replicate\nthe model states like DP. Consequently, the ZeRO series\ntechnique is also suitable to HDP, as shown in Figure 8(a),\nHDP utilizes ZeRO-1 across all the HDP ranks to maximally\nshards the optimizer states, minimizing the memory usage.\nLoss and Model Update. Even though HDP ranks may\nperform different heterogeneous communications across dif-\nferent micro-batches, the final gradient for a parameter is\nequivalent to that obtained in standard DP. As shown in\nFigure 9, each token contributes a gradient to the parameter\nùúÉùëõ, and the final gradient, denoted as ùê∫ùúÉùëõ, is the sum over\npack hook\nforward graph\nbackward graph\nunpack hook\nparameter\nactivation\ngraph ctx\ntensor tag = {layer id, act id}\ncur layer \nactivations\npush\ngpu tensor\npop\nCPU\nMemory\nD2H\noffload\npush\nH2D\nreload\nprev layer \nactivations\npop\nparameter\nactivation\ntensor tag = {layer id, act id}\ngraph ctx\ngpu tensor\nDevices Mapping\noffload\nDevices Mapping\n(a) selective offloading\n(b) activation offloading\nFigure 11. Data-Aware Selective Offloading\ngradients from all tokens in global batch (denoted as B). Let\ngrad(ùëó,ùúÉùëõ) represent the gradient from the token ùëóto the\nparameter ùúÉùëõ. Then ùê∫ùúÉùëõcan be presented as:\nùê∫ùúÉùëõ=\n‚àëÔ∏Å\nùëÜùëñ‚ààB\n\u0010‚àëÔ∏Å\nùëó‚ààùëÜùëñgrad(ùëó,ùúÉùëõ)\n\u0011\n(1)\nSince parameters are replicated and tokens are evenly\ndistributed across HDP ranks (denoted as R), the local accu-\nmulated gradient corresponds to the partial sum of gradients\nfrom tokens assigned to each rank (denoted as Bùëü, i.e. micro-\nbatches in rank ùëü). Consequently, similar to DP, a global\ncollective communication like All-Reduce or Reduce-Scatter\nwill be performed across all HDP ranks to aggregate partial\ngradients. This also yields the gradient ùê∫ùúÉùëõfrom all tokens:\nùê∫ùúÉùëõ=\n‚àëÔ∏Å\nùëü‚ààR, Bùëü‚ààB\n\u0010‚àëÔ∏Å\nùëö‚ààBùëü\n\u0010‚àëÔ∏Å\nùëó‚ààùëögrad(ùëó,ùúÉùëõ)\n\u0011\u0011\n(2)\nThe Eq.(2) is equivalent to Eq.(1), and ensures that the\nresult of gradient accumulation in HDP is equivalent to that\nin standard DP. Moreover, since we calculate the gradient\nùê∫ùúÉùëõover all tokens in the global batch, it also needs to be\nscaled by the total amount of tokens, as we implement this\nby the token-level loss, which scales the loss by the token\namount rather than sample amount.\n5.2\nData-Aware Selective Offloading\nActivation Offloading. The activation size is proportional\nto the sequence length. Constrained by GPU memory, longer\nsequences require more HDP ranks to distribute the activa-\ntion. For example, processing a sequence with 1M tokens re-\nquires 128 ranks if each rank can handle 8K tokens, which is\nusually unaffordable with today‚Äôs expensive GPU resources.\nIn practice, modern GPU servers are typically equipped with\nCPU memory that far exceeds GPU memory. Therefore, an\nalternative approach is to offload activations to the CPU,\nthereby reducing the required amount of ranks. There are\ntwo characteristics to support the feasibility of this approach:\n1‚óãActivation is first-in-last-out: As shown in Figure 10,\ngiven any sequence, during the forward propagation, it will\n6\n\n\nbe processed sequentially by transformer layers, and acti-\nvations will be gradually accumulated until reaching a peak\nafter the final layer. Subsequently, during the backward\npropagation, these activations will be consumed from the\nlast layer to the first one. Since the activations produced by\nearlier layers are used more later (i.e. FILO), it is promising\nto offload these activations to the CPU during the forward\npropagation and reload them back into GPU when needed\nin the backward propagation.\n2‚óãùëÇ(ùëÅ2) computation can overlap ùëÇ(ùëÅ) offloading: It is\nwell-known that transferring data between GPU and CPU\nis typically inefficient due to the limited PCIe bandwidth.\nThe offloading time usually far exceeds the computation\ntime, making it impractical. Fortunately, as mentioned in\n¬ß2.4, the computational complexity of attention is ùëÇ(ùëÜ2),\nwhile the memory complexity is ùëÇ(ùëÜ). Therefore, for suffi-\nciently long sequences, the ùëÇ(ùëÜ2) computation time will\ninevitably surpass the ùëÇ(ùëÜ) data transfer time, allowing\nthe offloading to be perfectly masked under computation.\nAs illustrated in Figure 11(b), we designed a general com-\nponent named act_ctx (Listing 1) to support activation of-\nfloading. This component maintains two cuda streams for\nD2H (Device-to-Host) and H2D (Host-to-Device) separately.\nIt automatically captures activation tensors from the com-\nputation graph and offloads them to the CPU (use async-\nCudaMemcpy API) at appropriate times during the forward\npropagation, and establishes asynchronous dependencies\nbetween the D2H stream and the computation stream. The\noriginal tensor in the computation graph is replaced with the\nmetadata {layer id, act id}. Similarly, during the backward\npropagation, the metadata stored in the computation graph\nis used to index and reload corresponding activations in the\nH2D stream. Figure 10 illustrates the whole process. The\nact_ctx also supports a parameter named offload_ratio, pro-\nviding token-level fine-grained control over the proportion\nof activations offloaded to the CPU. This capability balances\nGPU memory savings with optimal overlap of computation.\n1\n# Separate offload_ratio to each micro -batch\n2\nact_ctx = get_act_ctx(num_micro_batch , offload_ratios)\n3\n# forward of micro -batch -i\n4\nact_ctx.update_micro_batch_id(i)\n5\nwith act_ctx:\n6\nforward_func (...)\n7\n# backward of micro -batch -j\n8\nact_ctx.update_micro_batch_id(j)\n9\nwith act_ctx:\n10\nbackward_func (...)\nListing 1. usage of act_ctx\nSelective Offloading. Activation offloading leverages CPU\nmemory to alleviate the burden on GPU memory. However,\nonly for long sequences the computation can perfectly over-\nlap with offloading. This means we cannot offload all tokens\nassigned to each rank indiscriminately. Instead, we must\nselectively offload each token based on the FLOPs.\nAlgorithm 1: Naive HDP Solution\n1 Input: Global Batch B={ùë†1,ùë†2, . . . ,ùë†ùëõ}, Rank Capacity ùê∂\nfor each sequence ùë†ùëñ‚ààB do\n2\nDetermine offload ratio ùëüand minimum required\nnumber of HDP ranks ùê∑(ùë†ùëñ) using Eq.(3);\n3\nif ùëëùëñ== 0 then\n4\nAdd ùë†ùëñto ùëùùëéùëêùëò_ùëôùëñùë†ùë°;\n5\nelse\n6\nUpdate ùëöùëéùëùùëü[ùë†ùëñ] ‚Üêùëüand ùëöùëéùëùùëë[ùë†ùëñ] ‚Üêùê∑(ùë†ùëñ);\n7 while ùëùùëéùëêùëò_ùëôùëñùë†ùë°is not empty do\n8\nPack ùë†ùë¢ùëèùë†ùëíùë°by best-fit strategy to fill capacity ùê∂;\n9\nUpdate ùëöùëéùëùùëü[ùë†ùë¢ùëèùë†ùëíùë°] ‚Üê0, ùëöùëéùëùùëë[ùë†ùë¢ùëèùë†ùëíùë°] ‚Üê1;\n10 Assign sequences to ùëë‚ÑéùëëùëùHDP ranks based on ùëöùëéùëùùëë;\n11 Initialize ùëéùëêùë°_ùëêùë°ùë•for each micro-batch using ùëöùëéùëùùëü;\n12 Return micro-batches, ùëéùëêùë°_ùëêùë°ùë•for each HDP rank\nAssume the number of layers per rank as ùëô, the token\ncapacity per rank as ùê∂. Given a sequence with length ùë†ùëñ‚â•ùê∂,\nwe define the computation time and activation size for each\nlayer as ùëá(ùë†ùëñ) and Act(ùë†ùëñ), respectively. The bandwidths of\nD2H and H2D are profiled as ùêµd2h and ùêµh2d. We aim to find\nthe offload ratio ùëüthat minimizes the required number of\nHDP ranks ùê∑(ùë†ùëñ) for ùë†ùëñby Eq. (3), where ùõº1, ùõΩ1, ùõº2, ùõΩ2 and ùõæ\nare coefficients we profiled for the cost model.\narg min\nùëü\nùê∑(ùë†ùëñ),\ns.t.\nùëá(ùë†ùëñ) = ùõº1ùë†2\nùëñ+ ùõΩ1ùë†ùëñ+ ùõæ, Act(ùë†ùëñ) = ùõº2ùë†ùëñ+ ùõΩ2,\nùê∑(ùë†ùëñ) = ‚åà2 √ó Act(ùë†ùëñ) + (1 ‚àíùëü) √ó (ùëô‚àí2) √ó Act(ùë†ùëñ)\nùëô√ó Act(ùê∂)\n‚åâ,\nùëá(ùë†ùëñ) ‚â•\nAct(ùë†ùëñ) √ó ùëü\nmin(ùêµd2h, ùêµh2d) ,\n1 ‚â•ùëü‚â•min(1,\nùëô√ó Act(ùê∂)\n(ùëô‚àí2) √ó Act(ùë†ùëñ) ).\n(3)\nSince different micro-batches have mutual independent\nforward and backward propagation, in Listing 1 we assign\na separate offload_ratio derived from Eq. (3) to each micro-\nbatch. This method effectively compresses the number of\nranks required for long sequences from ùë†ùëñ\nùê∂to ùê∑(ùë†ùëñ), as shown\nin Figure 11(a). It not only significantly reduces communi-\ncation overhead but also enables the more available HDP\nranks to process data, thereby improving efficiency.\nOverlap Efficiency Discussion. As we know, the NCCL\ncommunication needs to occupy a portion of streaming mul-\ntiprocessors (SMs), to reach the peak bandwidth over Infini-\nBand and NVLink. Consequently, even with communication-\ncomputation overlap, the computation kernels cannot fully\nutilize all the tensor cores, resulting in inefficiencies. Fortu-\nnately, the D2H and H2D kernel use the DMA engine rather\nthan SMs, making it overlap perfectly with both computation\nand communication. Moreover, we use cached pinned host\n7\n\n\n6\n5\n3\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n3\n3\n4\n1\n4\n1\n2\n3\n1\n5\n2\n2\n2\n2\n4\n5\n6\n3\n3\n3\n3\n4\n5\n6\n7\n4\n4\n4\n4\n5\n6\n7\n5\n5\n5\n6\n6\n7\n6\n6\n7\n7\n7\n7\n7\n6\n3\n5\n6\n5\n3\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n3\n3\n4\n1\n4\n1\n2\n3\n1\n5\n2\n2\n2\n2\n4\n5\n6\n3\n3\n3\n3\n4\n5\n6\n7\n4\n4\n4\n4\n5\n6\n7\n5\n5\n5\n6\n6\n7\n6\n6\n7\n7\n7\n7\n7\n6\n3\n5\n6\n5\n3\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n3\n3\n4\n1\n4\n1\n2\n3\n1\n5\n2\n2\n2\n2\n4\n5\n6\n3\n3\n3\n3\n4\n5\n6\n7\n4\n4\n4\n4\n5\n6\n7\n5\n5\n5\n6\n6\n7\n6\n6\n7\n7\n7\n7\n7\n6\n3\n5\npp bubble\npp bubble\ngrad sync & model update\npp bubble\n(a) Pipeline0: CP=1,2,3,4, micro batches=8\n6\n5\n3\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n3\n3\n4\n1\n4\n1\n2\n3\n1\n5\n2\n2\n2\n2\n4\n5\n6\n3\n3\n3\n3\n4\n5\n6\n7\n4\n4\n4\n4\n5\n6\n7\n5\n5\n5\n6\n6\n7\n6\n6\n7\n7\n7\n7\n7\n6\n3\n5\n1 2\n1\n3\n0\n0\n0\n1\n0\n2\n0\n0\n0\n0\n1\n2\n3\n4\n1\n1\n1\n1\n2\n3\n4\n5\n2\n2\n2\n2\n3\n4\n5\n6\n3\n3\n3\n3\n4\n5\n6\n7\n4\n4\n4\n4\n5\n6\n7\n8\n5\n5\n5\n5\n6\n7\n8\n9\n6\n6\n6\n6\n7\n8\n9\n10\n7\n7\n7\n7\n8\n9\n10\n11\n8\n8\n8\n8\n9\n10\n11\n12\n9\n9\n9\n9\n10\n11\n12\n13\n10\n10\n10\n10\n11\n12\n13\n14\n11\n11\n11\n11\n12\n13\n14\n15\n12\n12\n12\n12\n13\n14\n15\n16\n13\n13\n13\n13\n14\n15\n16\n14\n14\n14\n14\n15\n16\n15\n15\n15\n15\n16 16\n16\n16\n16\n17\n17\n17\n17 17\n17\n17\n17\npp bubble\nsync\nCP=3\n(b) Pipeline1: CP=1, micro batches=18\nCP=4\nCP=2\nFigure 12. Balanced Data and Pipeline Parallelism\nmemory to further reduce the overhead of CPU memory al-\nlocation and speed up the data exchange between the device\nand host. Since pipeline parallelism interleaves the forward\nand backward propagation of different micro-batches, the\nD2H and H2D kernels could execute simultaneously, thereby\nmaximizing the bidirectional bandwidth of PCIe.\n5.3\nOverall Routine\nThe overall routine of ByteScale is outlined in Alg. 1. Briefly\nspeaking, the algorithm traverses each sequence ùë†ùëñin the\nglobal batch. For long sequences, it derives the offload ratio\nùëüand determines the required number of ranks ùê∑(ùë†ùëñ) (lines\n1-6). For short sequences, it packs them to fill each rank‚Äôs\ncapacity ùê∂(lines 7-9). The processed sequences are then\nassigned to ùëëhdp ranks, and the algorithm returns the micro-\nbatches and ùëéùëêùë°_ùëêùë°ùë•, for execution (lines 10-12).\n6\nBalance Scheduler\nIn this section, we introduce the balance scheduler to ad-\ndress both the DP and PP imbalance issues. By carefully\norchestrating data assignment (instead of line 10 in Alg. 1),\nit mitigates these imbalances while keeping the minimum\ncommunication as ¬ß5 performs. We will first outline several\nkey insights and then propose our heuristic solution.\n6.1\nRedefine micro-batch\nGradient accumulation requires that different DP ranks exe-\ncute the same number of micro-batches, based on the assump-\ntion that all micro-batches have the same computational load.\nHowever, as mentioned in ¬ß3.3, execution times for different\nmicro-batches can significantly vary. In ByteScale, we re-\ndefine a more flexible strategy, which enables different HDP\nranks to process different numbers of micro-batches (same\nsize but differ in workloads), to mitigate the imbalance issue.\nAs shown in Figure 13, it makes all the ranks finish compu-\ntation at the same time. More importantly, this strategy does\nnot affect model convergence. Regardless of how sequences\nare assigned to HDP ranks, we finally calculate the sum of\ngradients from all tokens in the global batch, as discussed in\n¬ß5.1, which ensures the mathematical equivalence.\ntimeline\n(a) DP Balance\nseq0\nseq1\nseq2\nseq3\nseq4\nseq5\npipeline stages\n(b) PP Balance\nseqlen\ntimeline\nseqlen\nFigure 13. Balance Strategy\n6.2\nSolve PP Imbalance\nInsight 1: PP bubbles are less when sequences of different length\nlevels are assigned to separate pipelines.\nIt is crucial to ensure that the pipeline processes micro-\nbatches with similar execution times. As illustrated in Fig-\nure 13(b), when ùëëùëùùëù= 4, any 4 consecutive micro-batches on\nthe timeline will be executed by 4 PP stages at the same time.\nIf their execution times differ significantly, extra PP bubbles\noccur. Due to the limited number of long sequences in the\nglobal batch, some pipelines have to be assigned sequences\nof multiple length levels. Fortunately, only during transition\nphases (e.g., when 4 consecutive micro-batches belong to\ndifferent length levels) will cause extra PP bubbles.\nWe assign more micro-batches to those pipelines with less\naverage execution times. As illustrated in Figure 12(a)-(b),\npipeline-0 handles micro-batches with larger average exe-\ncution times and is therefore assigned only 8 micro-batches.\nIn contrast, pipeline-1 is assigned 18 micro-batches to syn-\nchronize with pipeline-0. Additionally, due to more micro-\nbatches, the bubble rate is further reduced.\n6.3\nSolve DP Imbalance\nInsight 2: It is only necessary to maintain load balance at each\ntime step when pipeline parallelism is not applied.\nIf only apply DP without PP, achieving load balance only\nrequires that, at any given time, the micro-batches executed\n8\n\n\nAlgorithm 2: Balance Strategy for HDP\nInput: Global Batch B = {ùë†0,ùë†1, . . . ,ùë†ùëõ}, Rank\nCapacity ùê∂, HDP Degree ùëë‚Ñéùëëùëù, Delta ùõø\nOutput: micro-batches for HDP Ranks\n1 Initialize micro_batches = [ ] √ó ùëë‚Ñéùëëùëù;\n2 Initialize exec_times = [0] √ó ùëë‚Ñéùëëùëù;\n3 # Step 1: Sort and Bucketize\n4 Sort B by sequence length in descending order;\n5 Divide B into buckets such that each bucket has an\napproximately equal sum of FLOPs;\n6 while buckets is not empty do\n7\n# Step 2: Identify Target Ranks\n8\nCalculate max_time = max(exec_times);\n9\nDetermine target_ranks = {ùëñ|\nmax_time ‚àíexec_times[ùëñ] > ùõø};\n10\n# Step 3: Assign Sequences\n11\nwhile Exist (max_time ‚àíexec_times[ùëñ] > ùõø) do\n12\nif using DP-Balance strategy then\n13\nSelect ùë†ùëíùëûùë†from the first bucket;\n14\nelse if using PP-Balance strategy then\n15\nSelect ùë†ùëíùëûùë†sequentially from all buckets;\n16\nAssign ùë†ùëíùëûùë†to target_ranks;\n17\nUpdate micro_batches and exec_times;\n18\nUpdate target_ranks based on exec_times;\n19\nif exist bucket is empty then\n20\nRemove bucket from buckets;\n21 Return micro_batches\nby different HDP ranks have similar execution times. There\nis no need to consider the workload imbalance between\nmicro-batches across different time steps on the timeline.\nA straightforward method is to assign sequences of the\nsame length level across different HDP ranks at the same\ntime, as shown in Figure 13(a). Moreover, we still assign more\nmicro-batches to those ranks that process shorter sequences\nthan others at the same time. Finally, it ensures that all HDP\nranks synchronize gradients nearly simultaneously.\n6.4\nBalance Strategy\nAlg. 2 describes the balance strategy. Firstly, we sort the\nsequences in the global batch B by length in descending order.\nThese ordered sequences are then divided into buckets with\napproximately equal sum of FLOPs, and thus the buckets\nwith longer average lengths contain fewer sequences (lines\n3-5). Secondly, we determine those ranks that have shorter\nexecution times for later assignments (lines 7-9). Thirdly, if\nusing the DP-Balance strategy, we select sequences from the\nsame bucket. Otherwise, if using the PP-Balance strategy,\nwe select the sequences sequentially from all buckets. In\npractice, ranks with shorter execution times are assigned\nGPU0\nGPU1\nGPU2\nGPU3\nseq0\nQ0\nQ1\nQ2\nQ3\nKV0\nKV1\nKV2\nKV3\n(a) standard causal mask\n(b) segmented causal mask\nseq1\nseq2\n(d) balanced segmented causal mask\n(e) balanced dist-attn with packing\n(c) heterogeneous \ncommunication\n(f) homogeneous \ncommunication\nFigure 14. Dist-attn Optimized for Packed Sequences\nwith more sequences (lines 12-15). Finally, we repeat the\nsecond and third steps until all the buckets are empty.\n7\nImplementation\nByteScale is implemented in approximately 16K lines of\ncode based on Python, C++, and CUDA. It has been inte-\ngrated with MegaScale [18], a high-performance framework\nfor LLM training. To support large-scale training and com-\nmunication, we also apply the following optimizations.\nGQA. Group Query Attention (GQA) has become an indis-\npensable feature in modern LLMs (e.g. LlaMA3 and Mistral),\nit helps reduce the number of KV heads, thereby decreas-\ning the communication volume for dist-attn. All systems\nmentioned in this paper apply the GQA technique.\nDist-attn with Packing. As workload is proportional to\nthe area of the attention mask, sequentially partitioning the\nsequence across devices causes workload imbalance. Several\ntechniques [4, 23, 31] have been proposed to solve this issue.\nHowever, they are not suitable for the special segmented\ncausal attention mask for packed sequences. As illustrated\nin Figure 14, to avoid heterogeneous computation and com-\nmunication within the CP group, we optimize the current\ndist-attn. Each subsequence of the packed sequences is uni-\nformly divided into 2ùëÅparts and symmetrically assigned\nto the ùëÅdevices. It ensures that each device holds 1\nùëÅof all\nthe subsequences and covers 1\nùëÅof the attention mask area.\nAll devices participate in the same ring-P2P communication,\nwith the same data exchange volume.\nRemote Dataloader. ByteScale requires global batch\ninformation at each training step to schedule data assign-\nment. However, existing dataloader solutions typically follow\nSPMD (Single Program, Multiple Data) mode, where each\nrank reads only partial data of the batch. To maintain the\nglobal information, all HDP ranks have to read the entire\n9\n\n\nscheduler (worker0)\nserver (worker0)\nserver (worker1)\nmeta\nmeta\nclient\n(worker0)\nclient\n(worker0)\nclient\n(worker1)\nclient\n(worker1)\nloading plan\ndata\ndata\ndata slice#1\ndata slice#0\nsingle \ncontroller\nray \nhead\nFigure 15. Remote Dataloader\nmax\nto_fp32\nsub\nexp\nsum\ndiv\nmul\nto_bf16\nfused_fw\n5.03 ms\n2.12 ms\n0.78 ms\nfused_bw\n0.91 ms\nmemory-bound\nx\nlogits\nw\nparallel \nsoftmax\nloss\nFigure 16. Fused SoftmaxCrossEntropy\nglobal batch simultaneously, which imposes significant pres-\nsure on both network communication and CPU memory. To\naddress this issue, we implement a remote dataloader using\nRay [26], which provides real-time scheduling and planning\ncapabilities in a global view. As shown in Figure 15, consider\na setup with two GPU nodes, worker-0 and worker-1, and\none CPU node as the Ray head, there exist three types of\nroles encapsulated by ray actors. The Server Roles are CPU\nprocesses in worker nodes, which fetch and preprocess raw\ndata from HDFS and generate metadata. The Scheduler Role,\nas the single controller, is a CPU process in worker-0, which\ncollects the global metadata from all servers, deduces the\nloading plan, and broadcasts it to clients. The Client Roles\nare GPU processes in worker nodes, which read the partial\ndata from servers based on the loading plan.\nFused SoftmaxCrossEntropy. Modern LLMs typically\nuse tokenizers with a large vocabulary size (e.g. 128K in\nLLaMA3 [11], 130K in Mistral [2] and over 150K in Qwen2.5 [43]).\nTo stabilize precision, current methods (e.g. VocabParallel in\nMegatron-LM) convert the logits variable from BF16 to FP32\nbefore calculating the SoftmaxCrossEntropyLoss. However,\nFP32 logits consume significant memory. For instance, with\na context length of 256K and a vocabulary size of 128K, it\nrequires 16GB under TP=8. Besides, the kernels are memory-\nbound and inefficient. As illustrated in Figure 16, we develop\nFusedSoftmaxCrossEntropy, which fuses numerous opera-\ntions into a single kernel, takes BF16 inputs, and still per-\nforms online computations in FP32 precision. It saves both\ntime and memory compared to existing methods.\n8\nExperiments\n8.1\nExperimental Setup\nEnvironments. Our experiments are conducted on a large-\nscale productive GPU cluster with more than 12,000 GPUs.\n(The specific information regarding the productive cluster, such\nas the number and type of GPUs, is hidden due to business and\nconfidential concerns.)\nBaselines. Our system is built on MegaScale, a productive\nLLM training framework for large-scale GPU clusters, which\nhas demonstrated superior performance to DeepSpeed and\nMegatron-LM. Thus, we present the advantages of ByteScale\nTable 1. Models for evaluation.\nModel\n#Layers\n#Heads\n#Groups\nHidden Dim\nLLaMA-7B\n32\n32\n8\n4096\nLLaMA-13B\n40\n40\n8\n5120\nLLaMA-30B\n60\n56\n8\n6656\nLLaMA-70B\n80\n64\n8\n8192\nMistral-8√ó7B\n32\n32\n8\n4096 (topk=2)\nMistral-8√ó22B\n56\n48\n8\n6144 (topk=2)\nby comparison in three cases: 1‚óãMegaScale with static paral-\nlelism strategies (DP, TP, PP and CP), along with the dist-attn\noptimization shown in Figure 14 for packed sequences; 2‚óã\nMegaScale with naive HDP, as described in Alg. 1, which\nonly applies communication optimizations; 3‚óãMegaScale\nwith balanced HDP, as described in Alg. 2, which applies op-\ntimizations for both communication and balance. To achieve\na fair comparison, we set the same ùëëùë°ùëùand ùëëùëùùëùfor all three\ncases and set the ùëë‚Ñéùëëùëùin 2‚óã3‚óãequal to ùëëùëëùëù√óùëëùëêùëùin 1‚óã, where\nthe ùëëùëêùëùcorresponds the minimum required number of ranks\nto support the context length of model.\nModels and Datasets. We evaluate our work with both\ndense and sparse LLMs, as detailed in Table 1. For the dense\nmodel, we choose the LLaMA-series LLMs with four different\nsizes, LLaMA-7B, LLaMA-13B, LLaMA-30B, and LLaMA-70B.\nFor the sparse model, we choose the Mistral-series LLMs\n(MoE) with two different sizes, Mistral-8x7B (active param-\neters = 13B/47B) and Mistral-8√ó22B (active parameters =\n39B/141B). Two datasets are used in our experiments, i.e.,\nGitHub and Byted, as we have introduced in ¬ß3.1. Figure 4\nillustrates the data distribution for these two datasets.\nWorkloads and Metrics. For different types and sizes of\nmodels, we scale the context length from 256K to 2M, and\nthe cluster size from 1024 GPUs to more than 12,000 GPUs to\nassess the performance of ByteScale more comprehensively.\nThe global batch for each training step is fixed to 32M tokens,\nas it‚Äôs a common practice in large-scale clusters. We use the\nthroughput (tokens per second) as the primary metric to\nevaluate the performance. All results are averaged over 200\niterations after a 20-iteration warmup.\n8.2\nEnd-to-End Evaluation\nWe first assess the end-to-end performance of three methods\nby measuring the average throughput at each training step,\nthe overall results are shown in Figure 17. It turns out that\nboth the HDP naive and balance solutions outperform the\nbaseline, achieving a maximum speedup of 7.89√ó.\nDifference in Scalability. As context length increases,\nthe baseline with static strategies must increase the CP de-\ngree to avoid OOM errors. For shorter sequences within the\nglobal batch, we have to pack them and apply the dist-attn\nshown in Figure 14, which suffers from inefficient and redun-\ndant communication. For instance, only 9.8% of the tokens\nin a global batch are longer than 256K for GitHub dataset,\n10\n\n\n256k\n512k\n1M\n2M\n0.0\n0.5\n1.0\ntokens/sec\n√ó106\n1.28x\n1.73x\n1.51x\n2.49x\n2.25x\n4.56x\n3.96x\n7.89x\n1K GPUs, LLaMA 7B, GitHub\n256k\n512k\n1M\n2M\n0.0\n2.5\n5.0\n√ó105\n1.23x\n1.68x\n1.42x\n1.98x\n1.61x\n2.34x\n2.43x\n4.26x\n1K GPUs, LLaMA 7B, Byted\n256k\n512k\n1M\n2M\n0.0\n0.5\n1.0\n√ó106\n1.26x\n1.65x\n1.50x\n2.22x\n2.59x\n4.15x\n3.67x\n6.88x\n2K GPUs, LLaMA 13B, GitHub\n256k\n512k\n1M\n2M\n0\n5\n√ó105\n1.21x\n1.53x\n1.26x\n1.69x\n1.51x\n2.28x\n2.27x\n4.13x\n2K GPUs, LLaMA 13B, Byted\n256k\n512k\n1M\n2M\n0\n5\ntokens/sec\n√ó105\n1.18x\n1.54x\n1.28x\n2.07x\n2.15x\n3.48x\n3.39x\n6.21x\n4K GPUs, LLaMA 30B, GitHub\n256k\n512k\n1M\n2M\n0.0\n2.5\n5.0\n√ó105\n1.17x\n1.49x\n1.23x\n1.58x\n1.62x\n2.06x\n2.30x\n3.80x\n4K GPUs, LLaMA 30B, Byted\n256k\n512k\n1M\n2M\n0\n5\n√ó105\n1.19x\n1.41x\n1.41x\n1.89x\n1.98x\n3.05x\n2.45x\n4.28x\n8K GPUs, LLaMA 70B, GitHub\n256k\n512k\n1M\n2M\n0.0\n2.5\n5.0\n√ó105\n1.22x\n1.52x\n1.24x\n1.70x\n1.26x\n1.93x\n1.79x\n2.84x\n8K GPUs, LLaMA 70B, Byted\n256k\n512k\n1M\n2M\n0\n1\n2\ntokens/sec\n√ó106\n1.15x\n1.46x\n1.18x\n1.52x\n1.27x\n1.84x\n1.98x\n3.42x\n8K GPUs, Mistral 8x7B, GitHub\n256k\n512k\n1M\n2M\n0\n1\n2\n√ó106\n1.17x\n1.45x\n1.20x\n1.63x\n1.27x\n1.75x\n1.35x\n2.18x\n8K GPUs, Mistral 8x7B, Byted\n256k\n512k\n1M\n2M\n0\n1\n√ó106\n1.12x\n1.28x\n1.27x\n1.59x\n1.86x\n2.72x\n2.75x\n4.13x\n>12K GPUs, Mistral 8x22B, GitHub\n256k\n512k\n1M\n2M\n0.0\n0.5\n1.0\n√ó106\n1.15x\n1.34x\n1.20x\n1.47x\n1.35x\n1.73x\n1.67x\n2.41x\n>12K GPUs, Mistral 8x22B, Byted\nBaseline\nHDP naive\nHDP balance\nFigure 17. End-to-end evaluation (measured in tokens per second).\n3m48s\n2m9s\n1m41s\n4m32s\n8m40s\n19s\n41s\n15s\n36s\n25s\n63s\n20s\n43s\n20s\n48s\n18s\n41s\n20s\n44s\n19s\n46s\n19s\n41s\n15s\n36s\n25s\n63s\n20s\n43s\n20s\n48s\n18s\n41s\n20s\n44s\n19s\n46s\n19s\n41s\n15s\n36s\n25s\n63s\n20s\n43s\n20s\n48s\n18s\n41s\n20s\n44s\n19s\n46s\n19s\n41s\n15s\n36s\n25s\n63s\n20s\n43s\n20s\n48s\n18s\n41s\n20s\n44s\n19s\n46s\n20s\n52s\n14s\n29s\n22s\n56s\n21s\n54s\n17s\n44s\n18s\n47s\n5s 13s\n20s\n54s\n20s\n52s\n14s\n34s\n8s\n21s\n12s\n30s\n22s\n56s\n11s\n28s\n7s\n18s\n21s\n54s\n10s\n26s\n5s\n14s\n17s\n42s\n14s\n34s\n8s\n21s\n18s\n47s\n12s\n30s\n8s\n21s\n2m36s\n2m37s\n2m36s\n2m36s\nBaseline\nHDP Naive\nHDP Balance\ntimeline\n4 random ranks\n302 \nus\n393 \nus\n1.5 ms\n305 \nus\n423 \nus\n1.4 ms\n307 \nus\n411 \nus\n1.5 ms\n304 \nus\n406 \nus\n1.6 ms\n4.4 ms\nattn (256 ranks): 405ms\n4.4 ms\nmlp: 10ms\n2.8 ms\n1.9 ms\nattn (39 ranks): 110ms\n4.3 ms\n4.4 ms\nmlp: 9ms\n2.8 ms\n1.9 ms\n2.8 ms\n1.9 ms\n2.8 ms\n1.9 ms\np2p comm\n(a) CP vs HDP: overlap for dist-attn\n(b) Time profile for 4 ranks in a single step\n(c) Valid computation time for all hdp ranks in a single step\ncompute\nbackward\nforward\nFigure 18. Case Study\nand scaling the context length from 256K to 2M, we can ob-\nserve that the throughput of the baseline decreases nearly 2√ó\nwhenever context length increases 2√ó. In contrast, under the\nsame conditions, the throughput of the HDP naive solution\ndecreases by 1.23√ó on average and the throughput of the\nHDP balance solution decreases by only 1.08√ó on average.\nThe HDP naive solution reduces communication overhead\nbut leaves some ranks idle due to imbalance. Meanwhile, the\nHDP balance solution eliminates these bubble times and fully\nreleases the performance enabled by flexible and dynamic\ncommunication. Consequently, ByteScale outperforms the\nbaseline by up to 7.89√ó on the GitHub dataset.\nDifference in Datasets. The Byted dataset contains more\nlong sequences than the GitHub dataset, and there exist 37%\nof the tokens in a global batch are longer than 256K. As a\nresult, the average throughput and speedup are lower than\nthat on the GitHub dataset. However, because ByteScale\nprovides communication optimizations for both long and\nshort sequences, the speedup can still achieve up to 4.26√ó.\nDifference in Parallelism Strategies. Models like LLaMA-\n7B, 13B and 30B use parallelism strategies including HDP\nand TP, thus applying the DP-Balance strategy. In contrast,\nmodels like LLaMA-70B, Mistral-8√ó7B and Mistral-8√ó22B\nemploy HDP, TP and PP, and we apply the PP-Balance strat-\negy. It can be observed that HDP with DP-Balance achieves\na higher speedup, compared to the PP-Balance. For instance,\nwith the GitHub dataset and a context length of 2M, the\nspeedup of HDP with DP-Balance is between 6.21√ó-7.89√ó,\nwhile the speedup of HDP with PP-Balance is only between\n3.42√ó-4.28√ó. As shown in Figure 13, the DP-Balance strategy\nonly needs to balance computation at each time step, which\nis easier to achieve than balance computation across all time\nsteps, as required by the PP-Balance strategy.\n11\n\n\n0\n30\n60\n90\n120\n0\n25\n50\n75\n100\nThroughput (Gb/s)\nRDMA Out Traffic for Baseline\n0\n30\n60\n90\n120\n0\n25\n50\n75\n100\nThroughput (Gb/s)\nRDMA Out Traffic for HDP Naive\n0\n30\n60\n90\n120\n0\n25\n50\n75\n100\nThroughput (Gb/s)\nRDMA Out Traffic for HDP Balance\n0\n30\n60\n90\n120\nTime (min)\n0\n10\n20\n30\n40\nUtilization (%)\nGPU Tensor Core Activity for Baseline\n0\n30\n60\n90\n120\nTime (min)\n0\n10\n20\n30\n40\nUtilization (%)\nGPU Tensor Core Activity for HDP Naive\n0\n30\n60\n90\n120\nTime (min)\n0\n10\n20\n30\n40\nUtilization (%)\nGPU Tensor Core Activity for HDP Balance\nFigure 19. Network Traffic and Tensor Core Utilization\n(a)\n(b)\n(c)\n(d)\n(e)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nThroughput (Tokens/sec)\n√ó105\n1.00x\n1.59x\n2.01x\n3.69x\n3.89x\nMegaScale\n+ Dynamic Communication\n++ Selective Offloading\n+++ Balance Strategy\n++++ Remote Dataloader\nFigure 20. Ablation Study\n8.3\nCase Studies\nTo anatomize the super performance of ByteScale more in-\ndeep, we choose the Byted dataset and conduct experiments\nby training LLaMA-7B with 2M context length on a cluster\nwith 1024 GPUs. Figure 18 presents the detailed runtime\nstatus of different ranks during a single training step.\nCommunication-Bound Case. Firstly, we randomly se-\nlect 4 ranks from the cluster, and record their forward and\nbackward times within the training step for each method.\nAs illustrated in Figure 18(b), for the baseline, the number\nof micro-batches is set as 8, and we have to set ùëëùëêùëù= 256 to\nsupport the sequence length of 2M. It can be observed that\nthese 4 ranks exhibit similar execution times. This is because\nmost micro-batches (except the third one) do not have the\ncomputational complexity of ùëÇ((2ùëÄ)2), but have to handle\nthe communication volume for 2M. As shown in Figure 18(a),\nthe P2P communication time far exceeds the computation\ntime, causing the execution time of a micro-batch almost\ndetermined by communication (97.6% of the total time).\nComputation-Imbalance Case. Under the HDP naive\nsolution, sequences within a global batch are sharded by\nthe minimal required number of ranks. As illustrated in Fig-\nure 18(a), a 312K sequence is sharded by only 39 HDP ranks\nto serve as micro-batches, and thus the computation time can\noverlap the communication overhead. However, the training\ninefficiency still exists due to the imbalance across ranks. As\nshown in Figure 18(b), although the third rank completes\nits 8 micro-batches in 1m41s, it has to wait for the first rank\nto finish at 4m32s, leading to 171s of idle time. Even so, the\nHDP naive solution saves 4m8s compared to the baseline.\nrecompute\noffload=1.0\noffload=0.8\noffload=0.6\noffload=0.5\nnormal\n0.0\n2.5\n5.0\n7.5\nE2E Time (s)\n6.70 s\n(\n -31.7%)\n6.49 s\n(\n -27.7%)\n5.93 s\n(\n -16.5%)\n5.38 s\n(\n -5.8%)\n5.08 s\n(\n 0.0%)\n5.08 s\nrecompute\noffload=1.0\noffload=0.8\noffload=0.6\noffload=0.5\nnormal\n0\n20\n40\n60\nMemory Usage (GB)\n27 GB\n(\n 54.2%)\n27 GB\n(\n 54.2%)\n33 GB\n(\n 44.1%)\n37 GB\n(\n 37.3%)\n40 GB\n(\n 32.2%)\n59 GB\nFigure 21. Effectiveness of Activation Offloading\nBalance Case. Under the HDP balance solution, all ranks\nfinish execution nearly at the same time. As shown in Fig-\nure 18(b), at any time step, each rank is assigned micro-\nbatches with similar FLOPs, and ranks with shorter execution\ntimes (e.g. the third and fourth ranks) will be assigned more\nbatches. Consequently, the total time of this step is further\nreduced to 2m37s, saving 6m3s compared to the baseline.\nOverall Comparison. As shown in Figure 18(c), we record\nthe valid computation time in a single step for all the 1024\nGPUs. It can be found that the HDP naive solution reduces\nthe peak execution time by 1.7√ó compared to the baseline,\nbut suffers from significant time variance across ranks, with\na 4.7√ó difference between the maximum and the minimum\nvalue (min=60s, max=279s, std=68s). The HDP balance solu-\ntion eliminates the time variance, thereby further reducing\nexecution time by 2.3√ó compared to the naive solution.\n8.4\nAblation Studies\nTo dive into the effectiveness of each component within\nByteScale, we further conduct ablation experiments using\nthe same configuration as ¬ß8.3, as shown in Figure 20.\nEffectiveness of Dynamic Communication. While Fig-\nure 18 provides a snapshot of runtime during a single training\nstep, we further profile the network traffic and tensor core\nutilization over two hours, as shown in Figure 19. It can\nbe observed that the baseline exhibits very heavy RDMA\ntraffic, yet the corresponding tensor core utilization is low.\nThis is because most ranks are communication-bound, and\n12\n\n\nthe computational units remain idle most of the time due to\nwaiting for redundant communication. This observation is\nconsistent with the situation depicted in Figure 18(a). When\nwe apply the HDP naive solution, the peak RDMA traffic\nis nearly halved, indicating that a significant amount of un-\nnecessary communication has been eliminated. Besides, the\ntensor core utilization also increases from 10% to 40%. Thus\nit achieves a speedup of 1.59√ó compared to baseline. How-\never, due to the imbalance issue, these improvements are not\nstable, and both communication and computation hardware\nunits occasionally experience stalls or idle periods.\nEffectiveness of Selective Offloading. Selective offload-\ning serves as a complement to the HDP naive solution. As\nshown in Figure 21, activation offloading with ratio ùëü= 1.0\nsaves the same memory as recomputation. As the context\nlength is set to 64K, the computation cannot fully overlap\nwith offloading. However, as we reduce the ratio to ùëü= 0.5,\nit can save 32.3% of memory without compromising through-\nput. Furthermore, the offload ratio is automatically derived\nfrom Eq 3, as the context length increases, a higher ratio can\nbe set to save more memory (e.g. set ùëü= 1.0 for 256K will\nnot decrease throughput). This method reduces the number\nof ranks for longer sequences, enabling more sequences to\nbe processed simultaneously with the same number of ranks,\nthereby improving speedup from 1.59√ó to 2.01√ó.\nEffectiveness of Balance Strategy. As illustrated in Fig-\nure 19, the balance strategy stables the RDMA traffic and\nmakes the tensor core utilization consistently around 40%.\nThis indicates that the hardware units of both computation\nand communication continuously work at full load for over\ntwo hours without idling. Consequently, the HDP Balance\nsolution achieves a speedup from 2.01√ó to 3.69√ó, surpassing\nthe improvements by any other strategy.\nEffectiveness of Remote Dataloader. We employ the\nremote loader depicted in Figure 15 and use CPU prefetching\nto overlap data reading with computation. This approach\nfurther improves the speedup from 3.69√ó to 3.89√ó.\n9\nConclusion\nWe proposed ByteScale, an efficient, flexible and scalable\ndistributed LLM training framework for large-scale mixed\ntraining of long and short sequences. We develop the commu-\nnication optimizer to eliminate redundant communication\nand build the balance scheduler to mitigate the imbalanced\ncomputation. We evaluate ByteScale on a production clus-\nter with more than 12,000 GPUs, and scale the model size\nfrom 7B to 141B and the context length from 256K to 2M,\nexperiment results show that it outperforms MegaScale by\nup to 7.89√ó.\nReferences\n[1] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch√∂nherr, and\nMario Fritz. 2023. LLM-Deliberation: Evaluating LLMs with Interactive\nMulti-Agent Negotiation Games. CoRR (2023).\n[2] Mistral AI. 2024. Mistral: Tokenization. https://docs.mistral.ai/guides/\ntokenization/.\n[3] Anthropic. 2024. Introducing the next generation of Claude. https:\n//www.anthropic.com/news/claude-3-family.\n[4] William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner,\nTian Jin, Zhiye Song, and Jonathan Ragan-Kelley. 2023.\nStriped\nAttention: Faster Ring Attention for Causal Transformers.\nCoRR\nabs/2311.09431 (2023).\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot\nLearners. In Annual Conference on Neural Information Processing Sys-\ntems 2020 (NeurIPS 2020).\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi\nTay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du,\nBen Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya,\nSanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne\nIppolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiri-\ndonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scal-\ning Language Modeling with Pathways. Journal of Machine Learning\nResearch (JMLR) 24 (2023), 240:1‚Äì240:113.\n[7] Tri Dao. 2023. FlashAttention-2: Faster Attention with Better Paral-\nlelism and Work Partitioning. CoRR abs/2307.08691 (2023).\n[8] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©.\n2022. FlashAttention: Fast and Memory-Efficient Exact Attention with\nIO-Awareness. In Annual Conference on Neural Information Processing\nSystems 2022 (NeurIPS 2022).\n[9] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,\nQuoc V. Le, Mark Z. Mao, Marc‚ÄôAurelio Ranzato, Andrew W. Senior,\nPaul A. Tucker, Ke Yang, and Andrew Y. Ng. 2012. Large Scale Dis-\ntributed Deep Networks. In 26th Annual Conference on Neural Infor-\nmation Processing Systems 2012 (NeurIPS 2022). 1232‚Äì1240.\n[10] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability\nin LLMs via Reinforcement Learning. CoRR abs/2501.12948 (2025).\n[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Ka-\ndian, et al. 2024. The Llama 3 Herd of Models. CoRR (2024).\n[12] Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024.\nHow to Train Long-Context Language Models (Effectively). CoRR\n(2024).\n[13] Google. 2024.\nGemini 1.5 Pro 2M context window, code\nexecution\ncapabilities,\nand\nGemma\n2\nare\navailable\ntoday.\nhttps://developers.googleblog.com/en/new-features-for-the-gemini-\napi-and-google-ai-studio/.\n[14] Google. 2024. Introducing Gemini: our largest and most capable AI\nmodel. https://blog.google/technology/ai/google-gemini-ai/.\n[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan,\nEric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\n13\n\n\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W.\nRae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-Optimal\nLarge Language Models. CoRR abs/2203.15556 (2022).\n[16] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao\nChen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,\nYonghui Wu, and Zhifeng Chen. 2019. GPipe: Efficient Training of\nGiant Neural Networks using Pipeline Parallelism. In Annual Con-\nference on Neural Information Processing Systems 2019 (NeurIPS 2019).\n103‚Äì112.\n[17] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang,\nShuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. 2023.\nDeepSpeed Ulysses: System Optimizations for Enabling Training of\nExtreme Long Sequence Transformer Models. CoRR abs/2309.14509\n(2023).\n[18] Ziheng Jiang, Haibin Lin, et al. 2024. MegaScale: scaling large language\nmodel training to more than 10,000 GPUs. In Proceedings of the 21st\nUSENIX Symposium on Networked Systems Design and Implementation\n(NSDI‚Äô24). USENIX Association, USA, Article 41, 16 pages.\n[19] Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, and Jinghua Tan. 2024.\nA Comprehensive Survey on Process-Oriented Automatic Text Summa-\nrization with Exploration of LLM-Based Methods. CoRR abs/2403.02901\n(2024).\n[20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Ben-\njamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and\nDario Amodei. 2020. Scaling Laws for Neural Language Models. CoRR\nabs/2001.08361 (2020).\n[21] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee,\nMichael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2022.\nReducing Activation Recomputation in Large Transformer Models.\nCoRR abs/2205.05198 (2022).\n[22] Mario Michael Krell, Matej Kosec, Sergio P Perez, and Andrew Fitzgib-\nbon. 2021. Efficient sequence packing without cross-contamination:\nAccelerating large language models without impacting performance.\nCoRR abs/2107.02027 (2021).\n[23] Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Joseph E. Gonzalez, Ion\nStoica, Xuezhe Ma, and Hao Zhang. 2023. LightSeq: Sequence Level\nParallelism for Distributed Training of Long Context Transformers.\nCoRR abs/2310.03294 (2023).\n[24] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis,\nTeng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania,\nand Soumith Chintala. 2020. PyTorch Distributed: Experiences on\nAccelerating Data Parallel Training. Proc. VLDB Endow. 13, 12 (2020),\n3005‚Äì3018.\n[25] Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023.\nRing Atten-\ntion with Blockwise Transformers for Near-Infinite Context. CoRR\nabs/2310.01889 (2023).\n[26] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov,\nRichard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul,\nMichael I. Jordan, and Ion Stoica. 2018. Ray: a distributed framework for\nemerging AI applications. In Proceedings of the 13th USENIX Conference\non Operating Systems Design and Implementation (OSDI‚Äô18). USENIX\nAssociation, USA, 561‚Äì577.\n[27] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu,\nand Brad Myers. 2024. Using an llm to help with code understanding. In\nProceedings of the IEEE/ACM 46th International Conference on Software\nEngineering. 1‚Äì13.\n[28] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri,\nNikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei\nZaharia. 2019. PipeDream: generalized pipeline parallelism for DNN\ntraining. In Proceedings of the 27th ACM Symposium on Operating\nSystems Principles (SOSP 2019). 1‚Äì15.\n[29] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and\nMatei Zaharia. 2021. Memory-Efficient Pipeline-Parallel DNN Training.\nIn International Conference on Machine Learning 2021 (ICML 2021),\nVol. 139. 7937‚Äì7947.\n[30] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGres-\nley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi\nKashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and\nMatei Zaharia. 2021. Efficient large-scale language model training on\nGPU clusters using megatron-LM. In International Conference for High\nPerformance Computing, Networking 2021 (SC 2021). 58.\n[31] NVIDIA. 2024.\nNVIDIA: Context Parallelism.\nhttps:\n//docs.nvidia.com/megatron-core/developer-guide/latest/api-\nguide/context_parallel.html.\n[32] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023).\n[33] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.\n[34] OpenAI. 2024. Introducing OpenAI o1. https://openai.com/o1/.\n[35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.\n2020. ZeRO: memory optimizations toward training trillion param-\neter models. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis (SC 2020).\n20.\n[36] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.\n2020. DeepSpeed: System Optimizations Enable Training Deep Learn-\ning Models with Over 100 Billion Parameters. In The 26th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining (KDD 2020). 3505‚Äì\n3506.\n[37] Alexander Sergeev and Mike Del Balso. 2018. Horovod: fast and easy\ndistributed deep learning in TensorFlow. CoRR abs/1802.05799 (2018).\n[38] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training\nMulti-Billion Parameter Language Models Using Model Parallelism.\nCoRR abs/1909.08053 (2019).\n[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-\nhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,\nAndrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xi-\nang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela\nFan, Melanie Kambadur, Sharan Narang, Aur√©lien Rodriguez, Robert\nStojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open\nFoundation and Fine-Tuned Chat Models. CoRR abs/2307.09288 (2023).\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is All you Need. In Annual Conference on Neural Information\nProcessing Systems 2017 (NeurIPS 2017). 5998‚Äì6008.\n[41] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy.\n2025. Videoagent: Long-form video understanding with large language\nmodel as agent. In European Conference on Computer Vision. Springer,\n58‚Äì76.\n[42] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan\nZhuang. 2024. LongVLM: Efficient Long Video Understanding via\nLarge Language Models. In Computer Vision ‚Äì ECCV 2024: 18th Euro-\npean Conference, Milan, Italy, September 29‚ÄìOctober 4, 2024, Proceedings,\nPart XXXIII.\n[43] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei,\nHuan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jingren Zhou, Junyang Lin, et al. 2025. Qwen2.5 Technical Report.\nCoRR abs/2412.15115 (2025).\n14\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21231v1.pdf",
    "total_pages": 14,
    "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs",
    "authors": [
      "Hao Ge",
      "Junda Feng",
      "Qi Huang",
      "Fangcheng Fu",
      "Xiaonan Nie",
      "Lei Zuo",
      "Haibin Lin",
      "Bin Cui",
      "Xin Liu"
    ],
    "abstract": "Scaling long-context ability is essential for Large Language Models (LLMs).\nTo amortize the memory consumption across multiple devices in long-context\ntraining, inter-data partitioning (a.k.a. Data Parallelism) and intra-data\npartitioning (a.k.a. Context Parallelism) are commonly used. Current training\nframeworks predominantly treat the two techniques as orthogonal, and establish\nstatic communication groups to organize the devices as a static mesh (e.g., a\n2D mesh). However, the sequences for LLM training typically vary in lengths, no\nmatter for texts, multi-modalities or reinforcement learning. The mismatch\nbetween data heterogeneity and static mesh causes redundant communication and\nimbalanced computation, degrading the training efficiency.\n  In this work, we introduce ByteScale, an efficient, flexible, and scalable\nLLM training framework for large-scale mixed training of long and short\nsequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid\nData Parallelism (HDP), which unifies the inter- and intra-data partitioning\nwith a dynamic mesh design. In particular, we build a communication optimizer,\nwhich eliminates the redundant communication for short sequences by data-aware\nsharding and dynamic communication, and further compresses the communication\ncost for long sequences by selective offloading. Besides, we also develop a\nbalance scheduler to mitigate the imbalanced computation by parallelism-aware\ndata assignment. We evaluate ByteScale with the model sizes ranging from 7B to\n141B, context lengths from 256K to 2048K, on a production cluster with more\nthan 12,000 GPUs. Experiment results show that ByteScale outperforms the\nstate-of-the-art training system by up to 7.89x.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}