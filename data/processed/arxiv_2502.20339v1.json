{
  "id": "arxiv_2502.20339v1",
  "text": "Thinking Slow, Fast:\nScaling Inference Compute with Distilled Reasoners\nDaniele Paliotta * 1 2 Junxiong Wang * 2 3 Matteo Pagliardini * 4 Kevin Y. Li * 5 Aviv Bick 5 J. Zico Kolter 5\nAlbert Gu 5 6 Franc¸ois Fleuret † 1 7 Tri Dao † 2 8\nAbstract\nRecent advancements have demonstrated that the\nperformance of large language models (LLMs)\ncan be significantly enhanced by scaling com-\nputational resources at test time.\nA common\nstrategy involves generating multiple Chain-of-\nThought (CoT) trajectories and aggregating their\noutputs through various selection mechanisms.\nThis raises a fundamental question: can models\nwith lower complexity leverage their superior gen-\neration throughput to outperform similarly sized\nTransformers for a fixed computational budget?\nTo address this question and overcome the lack\nof strong subquadratic reasoners, we distill pure\nand hybrid Mamba models from pretrained Trans-\nformers. Trained on only 8 billion tokens, our\ndistilled models show strong performance and\nscaling on mathematical reasoning datasets while\nbeing much faster at inference for large batches\nand long sequences. Despite the zero-shot perfor-\nmance hit due to distillation, both pure and hybrid\nMamba models can scale their coverage and accu-\nracy performance past their Transformer teacher\nmodels under fixed time budgets, opening a new\ndirection for scaling inference compute.\n1. Introduction\nReasoning in large language models (LLMs) has seen a\nsignificant boost in performance recently, largely driven by\nscaling inference compute. A key technique to enhance “rea-\nsoning” performance is the use of intermediate reasoning\nsteps before producing a final answer, known as Chain-of-\nThought (CoT) (Wei et al., 2023). Building on this, many\n*Equal contribution,\n†Equal advising\n1Machine Learning\nGroup, University of Geneva 2Together AI 3Cornell Univer-\nsity 4EPFL 5Carnegie Mellon University 6Cartesia.ai 7META\n8Princeton University.\nCorrespondence to:\nDaniele Paliotta\n<daniele.paliotta@unige.ch>.\ntest-time compute techniques often involve generating mul-\ntiple CoTs (Wu et al., 2024; Snell et al., 2024) and selecting\nthe best one. Even simple strategies, such as majority voting,\ncan be surprisingly effective (Brown et al., 2024; Beech-\ning et al., 2024). Furthermore, trained reward models can\nprovide scores for the final model answers and even for the\nindividual steps of the CoTs (Luo et al., 2024).\nHowever, these test-time compute techniques introduce sig-\nnificant challenges for LLM systems. Generating long CoT\nsequences or large batches of completions places substantial\ndemands on memory and compute resources. Transform-\ners, in particular, struggle with such workloads due to their\nlinear memory scaling and memory-bound nature during\ngeneration. This raises an important question: how should\nwe optimize model architectures to best scale test-time com-\npute? In particular, can alternative architectures with faster\nand more efficient generation outperform current LLMs\nunder fixed compute budgets? Addressing this problem\ncould unlock new avenues for deploying reasoning models\nwith different architectures, enabling them to run and scale\nmore efficiently on hardware and environments with limited\nmemory and compute.\nRecent subquadratic architectures have training time or pre-\nfill time linear in sequence length, and constant memory re-\nquirement (instead of linear) during inference. This enables\nup to 5× higher inference throughput (Gu & Dao, 2024;\nPeng et al., 2023) as inference time for large batch size or\nlong sequences is dominated by the time to load the model\nstates (KV cache or RNN states). Despite their efficiency,\nsubquadratic models have not been extensively explored in\nreasoning tasks, primarily due to the lack of large-scale pre-\ntrained models compared to Transformer-based counterparts.\nAs a result, it remains unclear whether: (1) scaling inference\ncompute for subquadratic models improves reasoning per-\nformance, and (2) subquadratic models can match or exceed\nTransformers models under fixed compute budgets.\nIn this work, we explore the reasoning capabilities of sub-\nquadratic architectures by distilling knowledge from pre-\ntrained Transformers into hybrid and pure Mamba models.\nTo address the scarcity of pretrained subquadratic models\nwith robust reasoning abilities, we develop recipes to distill\n1\narXiv:2502.20339v1  [cs.CL]  27 Feb 2025\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\n100\n101\nTime (seconds)\n0.2\n0.4\n0.6\n0.8\nCoverage\nLlama-1B\nLlama-3B\nLlamba-1B (ours)\nLlamba-4B (ours)\nMambaInLlama-1B (ours)\nMambaInLlama-3B (ours)\nPareto Front\n(a) Scaling with time.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.2\n0.4\n0.6\n0.8\nCoverage\n(b) Scaling with number of completions.\nFigure 1. Distilled models have better coverage on MATH for most time budgets. In (b), we show the coverage as we increase the\nnumber of sampled answers k. Compared to their associated Llama baselines, our distilled models provide a lower coverage for a given k.\nIn (a), we now show the shortest time required to reach a given coverage. For each curve in (b), we map the k-values on the x-axis to the\ntime required to generate that many samples for each model. Ideally, we would want to reach the highest coverage for short time budget.\nFor a given time budget, our distilled models can generate many more completions than their respective baselines. As such, the higher\nthroughput of our models, shown in Figure 2, allows them to overcome their lower per-sample coverage. As a result, our models push the\nPareto front forward for most time budgets.\nspecific reasoning skills into these architectures. We then\nbenchmark the models for multiple Chain-of-Thought (CoT)\ncompletions, providing a comprehensive analysis of perfor-\nmance under fixed compute and memory constraints. Our\napproach advances the Pareto front established by existing\nmodels, achieving a better trade-off between efficiency and\nreasoning capability.\nOur distilled pure and hybrid subquadratic reasoners are\nable to outperform their Transformer teachers on both cov-\nerage and accuracy on MATH (Lightman et al., 2023) and\nGSM8K (Cobbe et al., 2021) mathematical reasoning tasks\non most time budgets, reaching the same quality with 2.5×\nless inference time. Our results highlight the potential for\ndistilling reasoning and math capabilities across architec-\ntures in a cost-effective manner while maintaining the infer-\nence compute scaling properties of Transformers.\n2. Related Work\n2.1. Scaling Inference Time Compute for Reasoning\nScaling inference time compute has emerged as a promis-\ning strategy to improve the performance of LLMs. Tech-\nniques such as Chain of Thought (CoT) and its variants have\ndemonstrated significant performance improvements across\nvarious reasoning benchmarks by decomposing complex\ntasks into intermediate steps (Wei et al., 2023; Yao et al.,\n2023)\nWhile these approaches improve reasoning through task de-\ncomposition, they also increase computational demands due\nto longer generation sequences. Recent work suggests that\nthis additional compute may itself contribute to improved\nmodel abilities (Pfau et al., 2024). Dynamic compute allo-\ncation during inference has further advanced this paradigm.\nFor instance, Goyal et al. (2024) introduced pause tokens\ninto the LLM vocabulary, enabling models to allocate com-\npute more effectively and achieve better reasoning and task\nperformance.\nAnother prominent approach involves generating and search-\ning through multiple model outputs to select the best an-\nswer. Various sampling algorithms have been proposed to\nincrease the diversity and quality of generated outputs to\nincrease the likelihood of the correct or best answer being\nselected (Wang et al., 2023; Renze & Guven, 2024; Zhang\net al., 2023). In parallel, outcome and process reward mod-\nels (ORMs and PRMs) have been introduced to help evaluate\nthe best response and guide intermediate generation steps\nwithin the LLM model (Lightman et al., 2023; Zhang et al.,\n2024a; Luo et al., 2024; Uesato et al., 2022).\nRecent work has shown that smaller LLMs, when scaled\nthrough inference-time compute (e.g., via majority voting or\nPRM-guided search), can outperform larger models under\nfixed compute budgets (Snell et al., 2024; Wu et al., 2024;\nBeeching et al., 2024). However, these findings are primarily\nlimited to Transformer-based architectures. The extent to\nwhich these scaling laws apply to subquadratic architectures,\nwhich offer faster inference but may trade off expressiveness,\nremains underexplored.\n2.2. Subquadratic Architecture Alternatives\nWhile Transformers dominate the landscape of reasoning\nmodels (Grattafiori et al., 2024; Qwen et al., 2025), alterna-\ntive architectures have been proposed to mitigate their high\ncomputational cost. These models, based on RNNs (Beck\net al., 2024; Peng et al., 2023), SSMs (Gu et al., 2022; Gu &\n2\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nDao, 2024), and linear attention mechanisms (Katharopou-\nlos et al., 2020; Yang et al., 2024), offer improved infer-\nence and memory efficiency especially for long-context\ntasks and large-batch generation, making them attractive\nfor large-scale language modeling. Notably, the Mamba\nfamily of models (Mamba-1 and Mamba-2) has introduced\nselective state spaces, enabling linear-time sequence mod-\neling without sacrificing performance (Gu & Dao, 2024;\nDao & Gu, 2024). Hybrid architectures that combine sub-\nquadratic layers (e.g., Mamba) with a limited number of\nself-attention layers have also emerged, achieving superior\nperformance compared to pure Transformer or subquadratic\nmodels (Lieber et al., 2024; Ren et al., 2024; Dong et al.,\n2024). These architectures are particularly well-suited for\nthe increased compute demands of inference-time scaling.\nOur work evaluates the inference-time scaling properties of\nboth pure and hybrid subquadratic models.\n2.3. Knowledge Distillation\nKnowledge distillation has proven effective in transferring\ncapabilities from large teacher models to smaller, more effi-\ncient student models (Hinton et al., 2015). In the context of\nLLMs, distillation is commonly used to compress a larger\npre-trained LLM into a smaller version while maintaining\ncore knowledge and functionality (Gu et al., 2024; Xu et al.,\n2024). Although larger models exhibit better reasoning\nand overall abilities due to the properties of scale (Xu et al.,\n2025; Wei et al., 2022), distillation has enabled smaller mod-\nels to achieve strong reasoning performance (DeepSeek-AI\net al., 2025; Labs, 2025). While most distillation efforts\nfocus on within-architecture transfer (e.g., Transformer to\nTransformer), recent work has explored cross-architecture\ndistillation. Pretrained Transformers have been successfully\ndistilled into recurrent architectures such as RNNs (Kasai\net al., 2021; Mercat et al., 2024), linear attention (Zhang\net al., 2024b), convolutions (Ralambomihanta et al., 2024),\nand SSMs (Bick et al., 2024; Wang et al., 2025). Whether\nstrong reasoning can be distilled across architectures re-\nmains an open question.\n3. Distilling Student Reasoners\nIn this section, we describe how we distilled Llama models\ninto pure Mamba and hybrid architectures. We refer to our\npure Mamba models as Llamba, and our hybrid models\nas MambaInLlama. We distill both our hybrid and pure\nMamba models using Llama 3.2-1B-Instruct and Llama 3.2-\n3B-Instruct from the Llama family of models (Grattafiori\net al., 2024).\n3.1. Distilling into Llamba\nDistillation method. In order to distill pure Mamba models,\nwe modify the MOHAWK distillation procedure introduced\nAlgorithm 1 Initializing MambaInLlama from Llama\n1: Shapes: B - Batch, L - Length, D - embed size,\nN = D/Heads, N ′ - expand\n2: Input: ot: (B, D)\n3: Output: output: (B, D)\n4: New Params: MLP, A\n5: for each head Wk, Wq, Wv, Wo : (N, D)\nexpanding grouped KVs do\n6:\nHead Parameter: A : (N, N ′)\n7:\nfor all positions t:\n8:\nxt : (B, N) ←WV ot\n9:\nBt : (B, N) ←WKot\n10:\nCt : (B, N) ←WQot\n11:\n∆t : (B, N ′) ←MLP(xt)\n12:\nA1:T , B1:T , C1:T : (B, N, N ′) ←DISC(A, B, C, ∆)\n13:\ny ←LINEARRNN(A, B, C, x)\n14:\noutput ←output + WO⊤y\n15: end for\n16: return output\nby Bick et al. (2024). MOHAWK is composed of three\nstages: 1) matrix orientation, 2) hidden state alignment, and\n3) weight transfer and knowledge distillation. Stage 1 (ma-\ntrix orientation) aligns the Mamba-2 model’s SSM matrix\nmixer (Dao & Gu, 2024) with the teacher’s self-attention\nmatrix by minimizing the distance between the two matrices.\nStage 2 (hidden state alignment) matches the student and\nteacher’s layers’ hidden state outputs. Both of these stages\nare run independently across layers to prevent previous opti-\nmization gaps from propagation through the model. This is\ndone by setting the input of the student layer to be that of\nthe previous teacher layer’s output. Stage 3 (weight transfer\nand knowledge distillation) transfers the remaining, unop-\ntimized parameters, e.g., MLPs, embeddings, and norms,\nand finetunes the complete end-to-end student model using\na distillation loss on the student and teacher logits (Hinton\net al., 2015). We deviate from the original MOHAWK paper\nby transferring the MLP weights and norms of each teacher\ndecoder layer to the student and training those parameters as\nwell during Stage 2. This stems from the architectural differ-\nences between Phi (Li et al., 2023) (MLP and self-attention\nin parallel) and Llama (Grattafiori et al., 2024) (sequential\nself-attention and MLP). Stage 3 remains the same with\nfewer weights transferred.\nExperimental details. Our pure Mamba-distilled models,\nLlamba-1B and Llamba-4B, dubbed after Bick et al. (2025),\nwhich uses a similar methodology and the same teacher mod-\nels, are distilled from their respective teacher models using\nour adjusted MOHAWK distillation approach with only 8\nbillion tokens total each. Following Bick et al. (2024), we\nuse a variant of Mamba-2 that converts the SSM head struc-\nture to multi-head (compared to the original multi-value and\nLlama’s grouped-query structure) and converts the sequence\nmixer to entirely discrete-time. The post-convolution ac-\ntivation and pre-output projection normalization are also\n3\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\n1\n16\n32\n64\n128\n256\n300\nBatch size\n0\n2\n4\n6\n8\n10\nInference time (seconds)\n×3.6\n×3.3\n×3.1\n×2.8\n×3.2\n×3.7\n×3.7\nLlama-1B\nLlamba-1B\nMambaInLlama-1B\n(a) 1B Models.\n1\n16\n32\n64\n128\n256\n512\nBatch size\n0\n10\n20\n30\nInference time (seconds)\n×2.6\n×2.6\n×2.3\n×2.9\n×3.7\n×4.2\nOOM\nLlama-3B\nLlamba-4B\nMambaInLlama-3B\n(b) 3B Models.\nFigure 2. Faster generation of distilled models. In (a) and (b), we show the inference time measured for the baseline Llama models as\nwell as our distilled Llamba (pure Mamba) and MambaInLlama (hybrid) models at the 1B and 3B scale. We denote the speedup for our\nMambaInLlama model at each batch size. We use prompts of 512 tokens and measure the time required to generate 512 tokens. The\ntimes measured do not include the prefilling of the prompt. Overall, distilled models can generate tokens much faster with the speedup\nbeing greater for larger batch sizes. Moreover, our distilled models are more memory efficient, as shown in (b), using a batch size of 512\nyields an Out of Memory (OOM) error for Llama 3B, but not for our models. To obtain 512 completions with Llama-3B, the two batches\nof 256, result in an inference time of 58.8s. In comparison, our MambaInLlama model would take 11.6s, a speedup of ×5.1.\n20 21 22 23 24 25 26 27 28\nNumber of completions (k)\n0.4\n0.6\n0.8\nCoverage\nLlama-3B\nLlama-3B + FT\nLlama-1B\nLlama-1B + FT\n(a) Coverage.\n20 21 22 23 24 25 26 27 28\nNumber of completions (k)\n0.3\n0.4\n0.5\n0.6\nMajority voting accuracy\n(b) Majority voting.\nFigure 3. Negligible effect of finetuning Llama baselines on dis-\ntillation dataset. As our distillation dataset includes math con-\ntent, we also finetune Llama models on the distillation dataset\nOpenMathInstruct-2. Those models are marked with the ”+FT”\nprefix. We plot the coverage (a) and majority voting accuracies\n(b) as a function of the number of completions. We observe that\nfinetuning Llama models on the distillation dataset has a negligible\neffect on those metrics.\nremoved. The 8B token distillation dataset is composed of\n4B tokens from FineMath-4+ (Lozhkov et al., 2024), allo-\ncated as 1B and 3B to Stages 1 and 2 respectively, and 4B\ntokens from OpenMathInstruct-2 (Toshniwal et al., 2024)\nused in Stage 3, which is the only stage in which we apply\nthe chat template to the inputs. Unlike for our hybrid mod-\nels, we find that computing the loss on both the assistant\noutput and user prompt improves model performance over\njust the assistant output. All three distillation stages use the\nAdamW optimizer with β = (0.9, 0.95) and weight decay\nof 0.1 and a Warmup-Stable-Decay (WSD) scheduler with\n10% warmup and 10% decay (Hu et al., 2024) at a 2048\ncontext length. In Stages 1 and 2, we set the learning rate to\n1 × 10−4, while in Stage 3, it is set to 1 × 10−5. The hyper-\nparameters are the same for the 1B and 4B distillation runs.\nOur final Llamba-1B model has 16 layers of our Mamba-2\nvariant with a state size of 64 and multi-head pattern of\n32 heads and state size of 64. Likewise, our Llamba-4B\nutilizes the same pattern with 24 heads and state size of 128\nfor 28 layers. We note that our pure Mamba-2 models are\nslightly larger than their Transformer counterparts due to\nthe pattern conversion from grouped-query to multi-head\nand additional parameters found within the Mamba-2 layer,\ne.g., gating.\n3.2. Distilling into MambaInLlama\nDistillation method. We follow two separate directions for\ndistillation. For the hybrid models, we modify the protocol\nproposed by Wang et al. (2025) in order to distill some\nspecific capabilities. These techniques have been shown to\nbe effective for hybrid architectures. The Mamba-in-Llama\nframework (Wang et al., 2025) introduces a method for\ndistilling hybrid Transformer-Mamba models by reusing\nweights from the attention layers. In the distillation process\nshown in Alg 1, the linear projections for Q, K, V and O\nare initialized using the corresponding linear projections for\nC, B, X and O respectively. The only additional learned\nparameters in the new layers are the sampling rate ∆and\nthe dynamic A. These new parameters will control the con-\nstructed Mamba through the discretization function. Specif-\nically, we take ∆∈RN ′ to discretize Bt, Ct ∈RN×1 and\nobtain Bt, Ct ∈RN′×N×1 as shown in Alg 1. We directly\nreuse the MLP layers. Differently from Wang et al. (2025),\nwe replace the attention layers with Mamba layers in a sin-\ngle round and finetune the whole model. For distillation,\nwe employ token-level KL divergence. The full probabil-\n4\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nity distribution of the student model, p(·; θ), is trained to\nalign with the full distribution of the teacher model, p(·; θT ),\nby minimizing the KL divergence across all possible next\ntokens at position t. Different from (Wang et al., 2025),\nwe use the reverse KL divergence, DKL(p(·; θ) ∥p(·; θT ))\ninstead of the forward KL divergence as the loss function,\nsince reverse KL behaves more like mode-seeking and better\nmimics the peak values. And we find that it yields better\nresults empirically. We adopt the Mamba-1 architecture\nfor our hybrid models, as Lieber et al. (2024); Wang et al.\n(2024); Dong et al. (2024) demonstrates that using Mamba-1\nin a hybrid architecture yields better results, especially for\nchallenging reasoning tasks.\nExperimental details.\nOur hybrid Mamba models, named\nMambaInLlama-1B (with 4 attention layers in 16 total\nlayers) and MambaInLlama-3B (with 6 attention layers\nin 26 total layers), are distilled with 8B tokens from\nOpenMathInstruct-2 (Toshniwal et al., 2024). We apply\nthe Llama chat template, mask the user prompt, and com-\npute the loss only over the tokens generated in the assistant’s\noutput. Thus, the total number of supervised tokens is re-\nduced to roughly 7B. To speed up training, we use data\npacking to merge different sequences into a single one until\nwe reach the maximum sequence length which is set to 8192.\nWe use the AdamW optimizer with learning rate 2 × 10−5,\nβ = (0.9, 0.95) and a weight decay of 0.1. The hyperpa-\nrameters are the same for the 1B and 3B models. In Mamba\nlayers, we set the SSM state size to 16. Consequently, the\nnumber of SSM groups after expansion is 2048/16 = 128\nfor the 1B model and 3072/16 = 192 for the 3B model.\nRemarks on the distillation dataset. We finetune the\nLlama teacher models on the same data used during distilla-\ntion to avoid our models from potentially gaining an unfair\nadvantage. The results, reported in Figure 3, show that the\ncontinuous training of the base model on the distillation data\nmix has a negligible effect on performance. Moreover, we\nfind that the selection of data used during distillation has a\nsignificant impact on the final capabilities of the distilled\nmodels. Switching the Stage 3 dataset in Llamba-1B from\nOpenMathInstruct-2 to OpenHermes-2.5 (Teknium, 2023)\ndecreased greedy decoding accuracy on MATH (acc@1) by\nmore than 10 percentage points.\nLack of correlation between reasoning and general\nbenchmarks. We also highlight the lack of correlation\nbetween common multiple choice-based benchmarks and\nmathematical reasoning, as the OpenHermes variant of\nLlamba-1B outperforms the final Llamba-1B by more than 5\npoints on MMLU (Hendrycks et al., 2021a) and 0.5 point on\nARC-Challenge (Clark et al., 2018). Moreover, analyzing\nthe acc@1 performance of Llamba-1B and Llamba-4B on\nMATH after each of the three stages, we see that the sharp\nincrease in reasoning ability between Stage 2 and Stage 3 is\nnot reflected in general knowledge benchmarks (Fig. 12).\n3.3. Improving performance after distillation\nWe show that it is possible to improve the accuracy and cov-\nerage of our distilled models by performing some supervised\nfine-tuning (SFT) after distillation. This is inspired by Wang\net al. (2025), where SFT is an integral part of the distillation\nprotocol. Starting from the distilled MambainLlama-1B\nand 3B, we fine-tune the models for two epochs using 8\nbillion tokens from OpenMathInstruct-2. The distilled mod-\nels achieve impressive performance both in coverage and\naccuracy, even surpassing the original Llama models. This\nis illustrated in Figure 8.\n4. Scaling Inference Time Compute\nWe scale test-time compute using our distilled models by\ngenerating multiple CoTs to solve a set of math problems.\nThe system prompt (Figure 7) contains instructions on how\nto properly format the response. The model outputs are\nparsed to extract the final solution, which is then compared\nto the ground truth. This approach enables us to evaluate\nthe model’s performance in generating correct solutions\nacross multiple attempts. Moreover, the results demonstrate\nthat the models are able to retain their instruction following\nability after distillation.\nEvaluation metrics.\nWe evaluate our model using two\nprimary metrics: coverage and accuracy. In domains like\ncoding and formal proofs, where answers can be automati-\ncally verified, coverage directly translates to improved per-\nformance and has been widely adopted (Chen et al., 2021;\nBrown et al., 2024). Coverage is commonly referred to as\nthe pass@k metric, where k denotes the number of sam-\nples per problem (Chen et al., 2021; Brown et al., 2024).\nThis metric estimates the probability that at least one cor-\nrect solution exists among the k samples. To reduce the\nvariance when calculating coverage, we adopt the unbiased\nestimation formula from Chen et al. (2021). Specifically,\nwe generate N ≥k total samples per task. The probability\nthat a correct solution exists among a pool of k generated\nsamples can then be determined given the total number of\ncorrect solutions Ci for each task.\npass@k =\n1\n# of problems\n# of problems\nX\ni=1\n \n1 −\n\u0000N−Ci\nk\n\u0001\n\u0000N\nk\n\u0001\n!\nWe implement this formula using a numerically stable ap-\nproach as suggested by Chen et al. (2021)(see Appendix C).\nFor accuracy, we use multiple aggregation strategies. Major-\nity voting, or self-consistency decoding (Wang et al., 2023)\n5\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\n100\n101\nTime (seconds)\n0.2\n0.3\n0.4\n0.5\n0.6\nMajority voting accuracy\nLlama-1B\nLlama-3B\nLlamba-1B (ours)\nLlamba-4B (ours)\nMambaInLlama-1B (ours)\nMambaInLlama-3B (ours)\nPareto Front\n(a) Majority voting.\n100\n101\nTime (seconds)\n0.2\n0.3\n0.4\n0.5\n0.6\nWeighted RM accuracy\n(b) Weighted Best-of-N.\nFigure 4. Distilled models provide better accuracies on MATH for most time budgets. Figures are similar to Figure 1b. In (a) and (b),\nwe show the majority-voting accuracy and the weight Best-of-N accuracy (the selected answer is the one with the highest sum of reward\nmodel scores, as introduced in Beeching et al. (2024)), for different time budgets. Similarly to Figure 1b, we observe how the higher\nthroughput of our distilled models allows them to push the Pareto front for most time budgets. Interestingly, when comparing models of a\ngiven size, Llama models are better for larger time budgets. However, looking at both model sizes together reveals that larger distilled\nmodels can compensate for the lower accuracies of smaller models. As a result, the Pareto front is defined by our hybrid models. While\nLlama-3B is still more efficient for a large time budget, one could imagine distilling a larger subquadratic model that generates quicker\nnonetheless.\n100\n101\nTime (seconds)\n0.4\n0.6\n0.8\n1.0\nCoverage\nLlama-1B\nLlama-3B\nLlamba-1B (ours)\nLlamba-4B (ours)\nMambaInLlama-1B (ours)\nMambaInLlama-3B (ours)\nPareto Front\n(a) Scaling with time.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.4\n0.6\n0.8\n1.0\nCoverage\n(b) Scaling with number of completions.\nFigure 5. Distilled models have better coverage on GSM8K for most time budgets. Observations are similar to Figure 1, despite a\nsmall degradation in coverage per number of completions, the better throughput of distilled models pushes the Pareto front forward.\nHybrid models are more efficient for most time budgets.\nis the most straightforward method to aggregate responses\nand compute an accuracy score. A more refined strategy\ninvolves using a trained verifier to select the best response\n(we call this approach Best-of-N).\nAs our verifier, we utilize a reward model trained using\nprocess supervision, where the model receives feedback on\neach step of the reasoning process. Inspired by Snell et al.\n(2024), we utilize a Llama-3.1 8B-based reward model to\nscore the solutions for Best-of-N. As PRMs produce a cumu-\nlative sequence of step-level scores per solution, we perform\na reduction over the steps to obtain a single solution-level\nscore which we use for answer selection. Following Snell\net al. (2024), we use the final score in all the steps as the\nscore for Best-of-N.\nWhile Best-of-N simply selects the generation with the high-\nest reward, weighted Best-of-N aggregates the rewards of\nsamples that share the same response and returns the so-\nlution with the highest combined score. In this work, we\nchose to report weighted Best-of-N, since we often found\nit to be superior to Best-of-N, most likely due to it identify-\ning highly-rated but also common solutions. We focus on\nthe performance of the models for a fixed compute budget.\nInspired by the scaling laws for inference-time compute\n(Brown et al., 2024), we speculate that having a model that\ncan scale to very large number of completions can be very\neffective, even more so than increasing model size by a large\namount (Snell et al., 2024).\n5. Results\nWe (i) measure the inference speedup of our distilled models\n(§ 5.1), and (ii) show that this speedup can result in better\nscaling for a given inference time budget (§ 5.2).\n6\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\n100\n101\nTime (seconds)\n0.4\n0.6\n0.8\nMajority voting accuracy\nLlama-1B\nLlama-3B\nLlamba-1B (ours)\nLlamba-4B (ours)\nMambaInLlama-1B (ours)\nMambaInLlama-3B (ours)\nPareto Front\n(a) Majority voting.\n100\n101\nTime (seconds)\n0.4\n0.6\n0.8\nWeighted RM accuracy\n(b) Weighted Best-of-N.\nFigure 6. Distilled models provide better accuracies on GSM8K for most time budgets. Observations are similar to Figure 4. The\nhigher throughput of our distilled models allows them to push the Pareto front for most time budgets.\n5.1. Inference time results\nExperimental protocol. In order to measure the inference\ntime and throughput of our distilled models, we focus on\ncreating a realistic setup that matches the prompt and CoTs\nlengths that we find for MATH and GSM8K. We compare\nthe runtime of a Llama 3.2 architecture against our distilled\nmodels. For Llama, we use FlashAttention2 (Dao, 2023)\nand torch compile. The implementations of our models rely\non the standard Mamba implementation. Given a varying\nbatch size, we consider the tasks of generating 512 tokens\nfrom a 512 token prompt, which reflect the length found in\nthe evaluation datasets. The prefilling time to process the\nprompt is not included in the benchmark, as it may depend\non the redundancy of a given prompt within the batch. In\nfact, in our setting, we are only interested in the time to\ngenerate the multiple completions given one prompt. Our\nbenchmark is done on a single NVIDIA H100 GPU, and\naveraged results over multiple runs are shown in Figure 2.\nDistilled models are significantly faster. Results in Fig-\nure 2 show our distilled models are up to ×3.7 and ×4.2\nfaster than their respective Llama 1B and 3B baselines.\nMoreover, MambaInLlama and Llamba are more memory\nefficient and thus can run larger batches. In Figure 2b, our\nmodels can accommodate batches of 512 while Llama-3B\nreturns an out-of-memory error. We also notice that Mam-\nbaInLlama models are slightly faster than Llamba. We spec-\nulate that this is because MambaInLlama has a smaller SSM\nstate size of 16, while Llamba uses a larger SSM state size\nof 64. Additionally, Llamba has larger projections because\nof its multi-head structure.\nRemark. In our speed experiments, both the prompt and\nthe generation lengths are relatively short compared to many\nother domains. When evaluating multi-turn conversations,\nfor example, where distilled Mamba architectures are al-\nready shown to excel (Wang et al., 2025), it is clear that\nthe length of the context and of the CoTs can be signifi-\ncantly larger than what has been considered in our experi-\nments. Such longer sequences would significantly increase\nthe throughput advantage of our models. Unfortunately, it is\nnot yet clear how to evaluate and exploit test-time compute\nfor more subjective, conversational tasks.\nLimitations. We try to ensure fair speed comparisons be-\ntween the model types by utilizing roughly equivalent im-\nplementations: standard implementation of Mamba-1/2 and\nFlashAttention-based self-attention with Pytorch compila-\ntion. However, it is worth noting that there exist optimiza-\ntions for current Transformer architectures, such as memory\nmanagement improvements (Kwon et al., 2023), that enable\nfaster generation; similar boosts are currently not readily\navailable for alternative architectures. Deeper optimizations\nfor each architecture are beyond the scope of this work.\n5.2. Results on reasoning tasks\nExperimental protocol. We benchmark the teacher models\n(Llama-3.2 1B-Instruct and 3B-Instruct) and the distilled\nMamba students (MambaInLlama-1B, MambaInLlama-3B,\nLlamba-1B, Llamba-4B) on the MATH-500 subset of the\nMATH dataset (Lightman et al., 2023; Hendrycks et al.,\n2021b) and a randomly selected 500-sample subset of\nGSM8K (Cobbe et al., 2021). We evaluate performance\nbased on coverage and accuracy with majority voting and\nweighted Best-of-N (§ 4).\nWe sample responses from\nthe distilled models with temperatures T = 0.6, 0.8 and\ntop k = −1 to consider all tokens. For each response, we\nsample up to 2048 tokens. For the distilled models, we find\nthat T = 0.6 and T = 0.8 are ideal for the 1B and 3B scale,\nrespectively, and subsequent results are obtained using these\ntemperatures. The official Llama evaluation system prompt\nis used for the MATH dataset, while the original prompt is\nkept for GSM8K as displayed in Figure 7. For our process\nreward model, we utilize a base Llama3.1-8B-Instruct model\ntrained on Mistral-generated data (Xiong et al., 2024).\n7\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nMATH System Prompt: Solve the following math problem efficiently and clearly:\\n\\n- For simple problems (2 steps\nor fewer):\\nProvide a concise solution with minimal explanation.\\n\\n- For complex problems (3 steps or more):\\nUse this\nstep-by-step format:\\n\\n## Step 1: [Concise description]\\n[Brief explanation and calculations]\\n\\n## Step 2: [Concise\ndescription]\\n[Brief explanation and calculations]\\n\\n...\\n\\nRegardless of the approach, always conclude with:\\n\\nTherefore,\nthe final answer is: $\\boxed{answer}$. I hope it is correct.\\n\\nWhere [answer] is just the final number or expression that\nsolves the problem.\nGSM8K System Prompt: \\n\\nGiven the following problem, reason and give a final answer to the problem.\\nYour response\nshould end with ”The final answer is [answer]” where [answer] is the response to the problem.\\nProblem:\nFigure 7. System prompts for MATH and GSM8K. System prompts passed to all models (pure and hybrid distilled models and Llama\nbaseline) to be used within the chat template when generating solutions to questions in the MATH and GSM8K datasets.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.2\n0.4\n0.6\n0.8\nCoverage\n(a) Coverage.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.2\n0.3\n0.4\n0.5\n0.6\nMajority voting accuracy\nLlama-1B\nLlama-3B\nMambaInLlama-1B (ours)\nMambaInLlama-3B (ours)\nMambaInLlama-1B + SFT (ours)\nMambaInLlama-3B + SFT (ours)\n(b) Majority voting.\nFigure 8. SFT can further improve distilled models. In (a), and (b), we show the coverage and majority voting accuracies for\nMambaInLlama distilled models. We show how coverage and majority voting accuracies can be further improved through Supervised\nFine-Tuning (SFT). The improvement is especially striking for our 1B model.\nDistilled Models can Cover like Teachers. When observ-\ning the scaling of coverage as the number of generation k\nincreases in Figure 1a (resp. Fig. 5a for GSM8K), we see\ndistilled models closely matching the coverage of their teach-\ners. Only a small degradation is observed. When we plot\nthe coverage as a function of the time budget, by associating\neach coverage measurement to the time required to generate\nthe associated number of completions (see Fig. 1b), we find\nthat our distilled models are exceptional in their ability to\ngenerate correct answers fast. By generating many more\ncompletions for the same time budget, the overall Pareto\nfront for coverage in both MATH and GSM8K (Fig. 1b, 5b)\nis heavily dominated by our distilled models where both\npure and hybrid Mamba reasoners are able to achieve the\nsame degree of coverage in nearly half the time of their re-\nspective teachers. Given a sufficiently strong reward model\nor verifiable solutions, those coverage scores would in large\nparts translate to accuracy.\nDistilled Models Achieve Competitive Accuracy Under\nFixed Time. Training with distillation and utilizing sub-\nquadratic architectures—which are less expressive than\nTransformers—can degrade model quality. However, we\nfind that this is a worthy trade-off when comparing perfor-\nmance under fixed time budgets in Figure 6 and Figure 4.\nSimilarly to coverage, the lighter and faster batch infer-\nence of the distilled models, allowing for more generations,\nresults in a better accuracy/time Pareto front at several com-\npletion scales. Interestingly, while comparing models of\nsimilar sizes indicates that larger time budgets are dom-\ninated by the teacher models, we observe that the larger\ndistilled model can provide better accuracy than the smaller\nbaseline while still being faster. For example, while Llama-\n1B provides better accuracy than MambaInLlama-1B for\nlarger time budgets, MambaInLlama-3B takes over where\nMambaInLlama-1B left off, providing a better accuracy than\nLlama-1B for inference time. Similarly, we conjecture that\nit would be possible to distill a larger baseline model that\nwould outperform Llama-3B, providing better accuracy for\na given time budget.\nLarger Students Faster and Better than Smaller Teach-\ners. The core driving force behind the growing interest\nin subquadratic models is their computational efficiency.\nSuch properties enable our distilled models of larger sizes\n(3B scale) to generate samples faster than even a smaller\nTransformer (1B). Our MambaInLlama-3B and Llamba-4B\n8\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nmodels outperform the slower Llama-1B baseline on cov-\nerage and accuracy while being faster. These inference\nspeedups, rooted in the underlying architecture, allow larger\nand more capable models to be used in time-constrained\nenvironments, despite their increased number of parameters.\nSmaller models have great coverage. When focusing on\ncoverage, we observe in Figure 1b and Figure 5b that most\nof the Pareto front is occupied by 1B models. Contrasting\nthose results with the majority voting accuracy results in\nFigure 4a and Figure 6a where the gap between 1B and\n3B models is much more significant. An interpretation is\nthat, while smaller models have the ability to generate the\ncorrect answer, the probability of generating it within a con-\nstrained number of samples increases with the model size.\nThis finding has implications in choosing model size for\ntasks involving formal language where the answer is easily\nverifiable, such as coding and mathematical proofs. In those\napplications, coverage matters most, and smaller models\nmay be preferred given their better time/coverage efficiency\ncompared to larger models.\nSFT improves the models significantly Following distilla-\ntion, which establishes a strong foundation for our models,\nwe observe that additional supervised fine-tuning (SFT)\nsignificantly enhances their performance, as illustrated in\nFigure 8. This suggests that while distillation effectively\ntransfers knowledge from the teacher model, SFT further\nrefines and aligns the model’s capabilities, enabling it to\nbecome highly competitive. Our results indicate that by\nleveraging both distillation and SFT, subquadratic architec-\ntures can match or even surpass their Transformer teachers\nin absolute performance on reasoning tasks.\n6. Conclusion\nIn our work, we investigate whether lower-complexity mod-\nels can leverage their superior generation throughput to out-\nperform similarly sized Transformers under a fixed compu-\ntational budget. We focus on reasoning tasks where we can\nscale test-time compute to improve performance. Through\nextensive experimentation, we distill both pure and hybrid\nMamba models at the 1B and 3B scales and evaluate their\nreasoning capabilities on mathematical reasoning bench-\nmarks, where the ability of subquadratic models to quickly\ngenerate many completions enables them to take advantage\nof their scaling properties when increasing inference com-\npute. When fixing memory and/or compute, our models\nachieve better coverage and accuracy for most time budgets\ncompared to their Transformer, teacher counterparts. These\nfindings highlight the potential of Mamba and other atten-\ntion alternatives as strong substitutes to Transformers for\ntasks that benefit from scalable inference compute.\nWe hope this work inspires future work in pretraining sub-\nquadratic reasoners and further exploring their inference\nscaling properties. More research is required to determine\nthe best way to distill reasoning capabilities across architec-\ntures, as performance remains highly sensitive to both data\nand distillation techniques. Moreover, since our distilled\nmodels demonstrate exceptional coverage, developing better\nreward models to better identify correct answers can close\nthe accuracy gap. Finally, further investigation into scaling\ninference compute for conversational and subjective tasks\nwould open to a scenario where lighter subquadratic models\ncan achieve larger gains in performance and speed\n7. Acknowledgements\nWe thank Cartesia and TogetherAI for their support in pro-\nviding compute for this project. KL is supported by funding\nfrom the Bosch Center for Artificial Intelligence.\nReferences\nBeck, M., P¨oppel, K., Spanring, M., Auer, A., Prudnikova,\nO., Kopp, M., Klambauer, G., Brandstetter, J., and\nHochreiter, S. xlstm: Extended long short-term mem-\nory, 2024. URL https://arxiv.org/abs/2405.\n04517.\nBeeching, E., Tunstall, L., and Rush, S. Scaling test-time\ncompute with open models, 2024.\nURL https://\nhuggingface.co/spaces/HuggingFaceH4/\nblogpost-scaling-test-time-compute.\nBick, A., Li, K. Y., Xing, E. P., Kolter, J. Z., and Gu, A.\nTransformers to ssms: Distilling quadratic knowledge\nto subquadratic models, 2024. URL https://arxiv.\norg/abs/2408.10189.\nBick, A., Katsch, T., Sohoni, N., Desai, A., and Gu, A.\nLlamba: Scaling distilled recurrent models for efficient\nlanguage processing, 2025. URL https://arxiv.\norg/abs/2502.14458.\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R´e,\nC., and Mirhoseini, A. Large language monkeys: Scaling\ninference compute with repeated sampling, 2024. URL\nhttps://arxiv.org/abs/2407.21787.\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\nH. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\nBrockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\nS., and et. al. Evaluating large language models trained\non code, 2021. URL https://arxiv.org/abs/\n2107.03374.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,\nA., Schoenick, C., and Tafjord, O.\nThink you have\nsolved question answering? try arc, the ai2 reasoning\n9\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nchallenge, 2018. URL https://arxiv.org/abs/\n1803.05457.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., Hesse, C., and Schulman, J. Training verifiers to solve\nmath word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nDao, T. Flashattention-2: Faster attention with better par-\nallelism and work partitioning, 2023.\nURL https:\n//arxiv.org/abs/2307.08691.\nDao, T. and Gu, A. Transformers are ssms: Generalized\nmodels and efficient algorithms through structured state\nspace duality, 2024.\nURL https://arxiv.org/\nabs/2405.21060.\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J.,\nZhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X.,\nZhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z.,\nLi, Z., Gao, Z., and et. al. Deepseek-r1: Incentivizing rea-\nsoning capability in llms via reinforcement learning, 2025.\nURL https://arxiv.org/abs/2501.12948.\nDong, X., Fu, Y., Diao, S., Byeon, W., Chen, Z., Maha-\nbaleshwarkar, A. S., Liu, S.-Y., Keirsbilck, M. V., Chen,\nM.-H., Suhara, Y., Lin, Y., Kautz, J., and Molchanov,\nP. Hymba: A hybrid-head architecture for small lan-\nguage models, 2024. URL https://arxiv.org/\nabs/2411.13676.\nGoyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S.,\nand Nagarajan, V. Think before you speak: Training\nlanguage models with pause tokens, 2024. URL https:\n//arxiv.org/abs/2310.02226.\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian,\nA., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A.,\nVaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn,\nA., Yang, A., Mitra, A., Sravankumar, A., Korenev, A.,\nHinsvark, A., and et. al. The llama 3 herd of models, 2024.\nURL https://arxiv.org/abs/2407.21783.\nGu, A. and Dao, T. Mamba: Linear-time sequence mod-\neling with selective state spaces, 2024. URL https:\n//arxiv.org/abs/2312.00752.\nGu, A., Goel, K., and R´e, C. Efficiently modeling long\nsequences with structured state spaces, 2022.\nURL\nhttps://arxiv.org/abs/2111.00396.\nGu, Y., Dong, L., Wei, F., and Huang, M. Minillm: Knowl-\nedge distillation of large language models, 2024. URL\nhttps://arxiv.org/abs/2306.08543.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding. Proceedings of the International\nConference on Learning Representations (ICLR), 2021a.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset, 2021b.\nURL https://arxiv.org/abs/2103.03874.\nHinton, G., Vinyals, O., and Dean, J.\nDistilling the\nknowledge in a neural network, 2015. URL https:\n//arxiv.org/abs/1503.02531.\nHu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng,\nZ., Fang, Y., Huang, Y., Zhao, W., Zhang, X., Thai, Z. L.,\nZhang, K., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai,\nJ., Zhai, Z., and et. al. Minicpm: Unveiling the potential\nof small language models with scalable training strate-\ngies, 2024. URL https://arxiv.org/abs/2404.\n06395.\nKasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G.,\nPappas, N., Mao, Y., Chen, W., and Smith, N. A. Fine-\ntuning pretrained transformers into rnns, 2021. URL\nhttps://arxiv.org/abs/2103.13076.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention, 2020. URL https://arxiv.\norg/abs/2006.16236.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS\n29th Symposium on Operating Systems Principles, 2023.\nLabs, B. Bespoke-stratos: The unreasonable effectiveness of\nreasoning distillation. www.bespokelabs.ai/blog/bespoke-\nstratos-the-unreasonable-effectiveness-of-reasoning-\ndistillation, 2025. Accessed: 2025-01-22.\nLi, Y., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar,\nS., and Lee, Y. T. Textbooks are all you need ii: phi-1.5\ntechnical report, 2023. URL https://arxiv.org/\nabs/2309.05463.\nLieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedi-\ngos, I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-\nShwartz, S., Abend, O., Alon, R., Asida, T., Bergman,\nA., Glozman, R., Gokhman, M., Manevich, A., Rat-\nner, N., Rozen, N., Shwartz, E., Zusman, M., and\nShoham, Y. Jamba: A hybrid transformer-mamba lan-\nguage model, 2024.\nURL https://arxiv.org/\nabs/2403.19887.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker,\nB., Lee, T., Leike, J., Schulman, J., Sutskever, I., and\nCobbe, K. Let’s verify step by step, 2023. URL https:\n//arxiv.org/abs/2305.20050.\n10\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nLozhkov, A., Ben Allal, L., Bakouch, E., von Werra, L., and\nWolf, T. Finemath: the finest collection of mathematical\ncontent, 2024. URL https://huggingface.co/\ndatasets/HuggingFaceTB/finemath.\nLuo, L., Liu, Y., Liu, R., Phatale, S., Guo, M., Lara, H.,\nLi, Y., Shu, L., Zhu, Y., Meng, L., Sun, J., and Rastogi,\nA. Improve mathematical reasoning in language models\nby automated process supervision, 2024. URL https:\n//arxiv.org/abs/2406.06592.\nMercat, J., Vasiljevic, I., Keh, S., Arora, K., Dave, A.,\nGaidon, A., and Kollar, T. Linearizing large language\nmodels, 2024.\nURL https://arxiv.org/abs/\n2405.06640.\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\nS., Biderman, S., Cao, H., Cheng, X., Chung, M., Grella,\nM., GV, K. K., He, X., Hou, H., Lin, J., Kazienko, P.,\nKocon, J., Kong, J., Koptyra, B., Lau, H., and et. al.\nRwkv: Reinventing rnns for the transformer era, 2023.\nURL https://arxiv.org/abs/2305.13048.\nPfau, J., Merrill, W., and Bowman, S. R. Let’s think dot by\ndot: Hidden computation in transformer language mod-\nels, 2024. URL https://arxiv.org/abs/2404.\n15758.\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng,\nB., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H.,\nYang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin,\nJ., Dang, K., and et. al. Qwen2.5 technical report, 2025.\nURL https://arxiv.org/abs/2412.15115.\nRalambomihanta, T. R., Mohammadzadeh, S., Islam, M.\nS. N., Jabbour, W., and Liang, L. Scavenging hyena: Dis-\ntilling transformers into long convolution models, 2024.\nURL https://arxiv.org/abs/2401.17574.\nRen, L., Liu, Y., Lu, Y., Shen, Y., Liang, C., and Chen,\nW. Samba: Simple hybrid state space models for effi-\ncient unlimited context language modeling, 2024. URL\nhttps://arxiv.org/abs/2406.07522.\nRenze, M. and Guven, E. The effect of sampling tempera-\nture on problem solving in large language models, 2024.\nURL https://arxiv.org/abs/2402.05201.\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-\ntime compute optimally can be more effective than scal-\ning model parameters, 2024. URL https://arxiv.\norg/abs/2408.03314.\nTeknium.\nOpenhermes 2.5:\nAn open dataset of\nsynthetic data for generalist llm assistants,\n2023.\nURL\nhttps://huggingface.co/datasets/\nteknium/OpenHermes-2.5.\nToshniwal, S., Du, W., Moshkov, I., Kisacanin, B.,\nAyrapetyan, A., and Gitman, I. Openmathinstruct-2: Ac-\ncelerating ai for math with massive open-source instruc-\ntion data, 2024. URL https://arxiv.org/abs/\n2410.01560.\nUesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N.,\nWang, L., Creswell, A., Irving, G., and Higgins, I. Solv-\ning math word problems with process- and outcome-\nbased feedback, 2022. URL https://arxiv.org/\nabs/2211.14275.\nWang, J., Paliotta, D., May, A., Rush, A. M., and Dao,\nT. The mamba in the llama: Distilling and accelerating\nhybrid models. arXiv preprint arXiv:2408.15237, 2024.\nWang, J., Paliotta, D., May, A., Rush, A., and Dao, T. The\nmamba in the llama: Distilling and accelerating hybrid\nmodels.\nAdvances in Neural Information Processing\nSystems, 37:62432–62457, 2025.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,\nS., Chowdhery, A., and Zhou, D. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels, 2023. URL https://arxiv.org/abs/2203.\n11171.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D.,\nMetzler, D., Chi, E. H., Hashimoto, T., Vinyals, O.,\nLiang, P., Dean, J., and Fedus, W.\nEmergent abili-\nties of large language models, 2022.\nURL https:\n//arxiv.org/abs/2206.07682.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter,\nB., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-\nthought prompting elicits reasoning in large language\nmodels, 2023.\nURL https://arxiv.org/abs/\n2201.11903.\nWu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Infer-\nence scaling laws: An empirical analysis of compute-\noptimal inference for problem-solving with language\nmodels, 2024.\nURL https://arxiv.org/abs/\n2408.00724.\nXiong, W., Zhang, H., Jiang, N., and Zhang, T. An im-\nplementation of generative prm. https://github.\ncom/RLHFlow/RLHF-Reward-Modeling, 2024.\nXu, F., Hao, Q., Zong, Z., Wang, J., Zhang, Y., Wang, J.,\nLan, X., Gong, J., Ouyang, T., Meng, F., Shao, C., Yan, Y.,\nYang, Q., Song, Y., Ren, S., Hu, X., Li, Y., Feng, J., Gao,\nC., and Li, Y. Towards large reasoning models: A survey\nof reinforced reasoning with large language models, 2025.\nURL https://arxiv.org/abs/2501.09686.\n11\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nXu, X., Li, M., Tao, C., Shen, T., Cheng, R., Li, J.,\nXu, C., Tao, D., and Zhou, T.\nA survey on knowl-\nedge distillation of large language models, 2024. URL\nhttps://arxiv.org/abs/2402.13116.\nYang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated\nlinear attention transformers with hardware-efficient train-\ning, 2024. URL https://arxiv.org/abs/2312.\n06635.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,\nY., and Narasimhan, K. Tree of thoughts: Deliberate\nproblem solving with large language models, 2023. URL\nhttps://arxiv.org/abs/2305.10601.\nZhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y.,\nand Tang, J.\nRest-mcts*: Llm self-training via pro-\ncess reward guided tree search, 2024a. URL https:\n//arxiv.org/abs/2406.03816.\nZhang, M., Bhatia, K., Kumbong, H., and R´e, C. The\nhedgehog & the porcupine: Expressive linear attentions\nwith softmax mimicry, 2024b. URL https://arxiv.\norg/abs/2402.04347.\nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B.,\nand Gan, C. Planning with large language models for\ncode generation, 2023. URL https://arxiv.org/\nabs/2303.05510.\n12\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nA. Additional results\nFigures 9 and 10 show accuracy as a function of the number of completions.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.2\n0.3\n0.4\n0.5\n0.6\nWeighted RM accuracy\n(a) Weighted Best-of-N.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.2\n0.3\n0.4\n0.5\n0.6\nMajority voting accuracy\nLlama-1B\nLlama-3B\nLlamba-1B (ours)\nLlamba-4B (ours)\nMambaInLlama-1B (ours)\nMambaInLlama-3B (ours)\n(b) Majority voting.\nFigure 9. Accuracy scores on MATH per completion without considering inference time.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWeighted RM accuracy\n(a) Weighted Best-of-N.\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of completions (k)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMajority voting accuracy\nLlama-1B\nLlama-3B\nLlamba-1B (ours)\nLlamba-4B (ours)\nMambaInLlama-1B (ours)\nMambaInLlama-3B (ours)\n(b) Majority voting.\nFigure 10. Accuracy scores on GSM8K per completion without considering inference time.\n13\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nB. Distillation loss curves\nFigure 11. The distillation loss decreases steadily over time, showing a clear downward trend and converge at around 0.03.\nFigure 11 shows the distillation loss of MambaInLlama 3B when training on the Toshniwal et al. (2024) dataset. We\nobserved a stable decrease in loss until it converged to 0.03.\nC. Pass@k implementation\nA numerically stable script for calculating an unbiased estimate of pass@k, from Chen et al. (2021).\n1 def pass_at_k(n, c, k):\n2\n\"\"\"\n3\n:param n: total number of samples\n4\n:param c: number of correct samples\n5\n:param k: k in pass@$k$\n6\n\"\"\"\n7\nif n - c < k: return 1.0\n8\nreturn 1.0 - np.prod(1.0 - k /\n9\nnp.arange(n - c + 1, n + 1))\n14\n\n\nThinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners\nD. MOHAWK (Pure Mamba) Distillation on Reasoning and General Performance\nStage 1\nStage 2\nStage 3\n0.0\n0.2\n0.4\n0.6\nPerformance\nLlamba-1B\nStage 1\nStage 2\nStage 3\nLlamba-4B\nTraining Stage\nARC-C\nARC-E\nMMLU\nMATH acc@1\nFigure 12. Reasoning and general performance of models after various stages of MOHAWK distillation. Distilling reasoning\ncapabilities, measured by accuracy using greedy decoding (acc@1) on the MATH benchmark, does not correlate with general multiple-\nchoice reasoning benchmarks. The performance of Llamba-1B and 4B increase significantly only after the last stage of the MOHAWK\ndistillation, but Stage 2 is able to significantly improve general benchmark performance.\n15\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20339v1.pdf",
    "total_pages": 15,
    "title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners",
    "authors": [
      "Daniele Paliotta",
      "Junxiong Wang",
      "Matteo Pagliardini",
      "Kevin Y. Li",
      "Aviv Bick",
      "J. Zico Kolter",
      "Albert Gu",
      "François Fleuret",
      "Tri Dao"
    ],
    "abstract": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}