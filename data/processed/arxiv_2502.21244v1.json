{
  "id": "arxiv_2502.21244v1",
  "text": "Anatomically-guided masked autoencoder\npre-training for aneurysm detection\nAlberto M. Ceballos-Arroyo1, Jisoo Kim2,4, Chu-Hsuan Lin2, Lei Qin3,4,\nGeoffrey S. Young2,4, and Huaizu Jiang1\n1 Northeastern University\n{ceballosarroyo.a,h.jiang}@northeastern.edu\n2 Brigham and Women’s Hospital {jkim,clin71,gsyoung}@bwh.harvard.edu\n3 Dana-Farber Cancer Institute {lei_qin@dfci.harvard.edu}\n4 Harvard Medical School\nAbstract. Intracranial aneurysms are a major cause of morbidity and\nmortality worldwide, and detecting them manually is a complex, time-\nconsuming task. Albeit automated solutions are desirable, the limited\navailability of training data makes it difficult to develop such solutions\nusing typical supervised learning frameworks. In this work, we propose\na novel pre-training strategy using more widely available unannotated\nhead CT scan data to pre-train a 3D Vision Transformer model prior\nto fine-tuning for the aneurysm detection task. Specifically, we modify\nmasked auto-encoder (MAE) pre-training in the following ways: we use\na factorized self-attention mechanism to make 3D attention computa-\ntionally viable, we restrict the masked patches to areas near arteries to\nfocus on areas where aneurysms are likely to occur, and we reconstruct\nnot only CT scan intensity values but also artery distance maps, which\ndescribe the distance between each voxel and the closest artery, thereby\nenhancing the backbone’s learned representations. Compared with SOTA\naneurysm detection models, our approach gains +4-8% absolute Sensi-\ntivity at a false positive rate of 0.5. Code and weights will be released.\nKeywords: Aneurysm detection · Deep learning · Self-supervised learn-\ning\n1\nIntroduction\nIntracranial aneurysms (IAs) are outpouchings that occur in brain arteries and\noccur due to the weakening of the vessel’s wall. Aneurysms can rupture under cer-\ntain conditions, leading to potentially deadly sub-arachnoid hemorrhage (SAH),\na condition with a 30-day mortality rate of 40-50% [21]. Expert radiologists\nare capable of detecting most aneurysms through visual inspection of patient\nscans, but such is a time-consuming process involving the manual inspection of\nhundreds of images. As a result, the research community has striven to develop\nautomated solutions that can assist clinicians in detecting aneurysms. Most such\nsolutions are deep-learning-based, with some achieving over 90% Sensitivity with\nFalse Positive (FP) rates below 2 per scan [8,27].\narXiv:2502.21244v1  [cs.CV]  28 Feb 2025\n\n\n2\nCeballos-Arroyo et al.\n0\n2\n4\n6\n8\nFPr\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSensitivity\nInternal Test\n0\n2\n4\n6\n8\nFPr\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSensitivity\nExternal Test\n0\n2\n4\n6\n8\nFPr\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSensitivity\nCMHA\n0\n2\n4\n6\n8\nFPr\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSensitivity\nPrivate\nCPM-Net\nnnDet\nDeformable\nOurs\nFig. 1: Lesion-level Sensitivity vs FP rate curve for our best model compared\nwith three baselines, measured across four datasets. Our model (red) consis-\ntently achieves better Sensitivity for tolerances of 0 to 2 FPs per scan, which are\nessential to minimize the amount of time radiologists spend reviewing FPs.\nDespite ongoing efforts, many of these aneurysm detection models perform\nsignificantly worse on out-of-distribution testing data (i.e., scans taken at dif-\nferent medical centers and/or with different equipment than the training data)\n[5,8,23], restricting their clinical applicability. The limited generalization capa-\nbility of some aneurysm detection approaches can be linked to the strictly su-\npervised nature of their training pipelines [5,23,13]. While supervised learning\ncan be enough for some tasks, it requires annotated data, which is not readily\navailable for aneurysm detection. For instance, the largest annotated public IC\ndataset [5] has a training partition of 1,186 CT scans, and most recent work\n[2,13] relies on even smaller datasets (100-500 scans) for training. Thus, such\nmodels are prone to over-fitting to the data distributions present therein.\nPre-training is a popular choice when data for a task is limited. This ap-\nproach can help models generalize in the natural images domain [12,16], it is\nnot frequently used for aneurysm detection, however, as most pipelines rely on\nCNN-based backbones [19,4]. While there are various pre-training methods for\nCNNs, some require access to annotated datasets like ImageNet [22], with no\ndirect equivalent in the medical domain; others, like augmentation-based con-\ntrastive learning [7], rely on applying color or spatial transforms that may not\nresult in realistic data for modalities like CT. An approach which better fits the\ncharacteristics of CT data while still enabling better generalization is masked-\nautoencoder pre-training (MAE) [16]. In specific, a Vision Transformer (ViT)\n[14] model is used to mask out portions of unlabeled images which are then\nreconstructed by a decoder. This method has been successfully leveraged to pre-\ntrain ViTs across various domains [11,30] without relying on augmentations or\nannotations. Moreover, ViTs have shown to perform even better than CNNs in\nsome instances [14]. However, there are several particularities to consider: first,\nthe amount of data required for pre-training is often larger than for supervised\ntraining; second, MAE was designed for pre-training on 2D datasets using large\nbatch sizes, but the volumetric nature of aneurysms and CT scans necessitates\n3D attention, which might limit throughput if used naively; finally, CT data\n\n\nTitle Suppressed Due to Excessive Length\n3\nhas a degree of sparsity, with parts of the scans being irrelevant to aneurysm\ndetection, limiting the usability of existing 3D MAE approaches [26,15].\nTo address the issues above, we propose a self-supervised pre-training pipeline\nconsisting of several key components. First, we collect a dataset comprising 6,796\nhead CT scans from various source, from each of which we are able to crop dozens\nof samples for pre-training. Second, we enable reasonably fast pre-training of\na 3D ViT by using a factorized 3D self-attention mechanism inspired by the\nvideo understanding literature [1]. Third, we modify MAE pre-training to better\nfit aneurysm detection by using a sampling mechanism for crops and masked\npatches that emphasizes anatomically relevant regions, i.e., those that are close\nto brain arteries. Moreover, instead of solely reconstructing CT scan intensity\nvalues, we use continuous artery distance maps, which describe the distance of\neach voxel to the nearest artery, as a second input channel for our pipeline,\nforcing the model to associate certain visual patterns with proximity to blood\narteries. For both kinds of inputs, we minimize the Mean Squared Error (MSE).\nThe pre-trained model is then used as the feature extractor for our detection\npipeline, where the decoder is query-based Transformer inspired by DETR [31].\nWe highlight the following contributions of our work:\n– We propose an entirely Transformer-based pipeline for 3D aneurysm detec-\ntion relying on self-supervised pre-training on a collection of 6,796 CT scans,\nencompassing low- and high-resolution scans from various institutions.\n– We improve on basic masked auto-encoder training by introducing anatom-\nical information into the pre-training step both in the features to be recon-\nstructed and the way sampling is carried out.\n– Our pipeline matches state-of-the-art Sensitivity when evaluated on in- dis-\ntribution data and surpasses it on out-of-distribution data by a margin of\n4-8% absolute Sensitivity given a tolerance of at most 0.5 FPs per scan.\n2\nMaterials and methods\n2.1\nData\nFor pre-training, we use head CT scans from various public datasets [10,18,20,29],\nincluding low- and high-resolution samples. Table 1 describes our data sources in\ndetail. In total, we use 6,796 scans for pre-training, of which 1,391 are medium to\nhigh resolution (slice thickness between 0.3 and 1 mm) and the rest are low res-\nolution (slice thickness greater than 1 mm). As none of the pre-training datasets\ninclude data annotated for aneurysm detection, we use both training and testing\ndata where available. We finetune our model on Bo et al.’s [5] dataset, whose\ntraining partition contains over 1,000 scans from various clinical centers in China.\nBo’s dataset also contains two evaluation partitions, one internal from the same\nclinical centers as the training data, and another external from a different set of\nhospitals. In addition, we use the CMHA dataset [24] and our own privately an-\nnotated partition using data sourced from the United States, following hospital\nguidelines for acquisition and use. Dataset statistics are provided in Table 1.\n\n\n4\nCeballos-Arroyo et al.\nTable 1: Summary of the head CT datasets used in this paper.\nDataset\nOrigin\nPurpose\nResolution\n# scans # aneurysms\nSino-CT [18]\nChina\nPre-training Low\n5,406\n–\nCQ500 [10]\nIndia\nPre-training Medium-High\n396\n–\nMosMED [20]\nRussia\nPre-training High\n870\n–\nTopCoW [29]\nSwitzerland\nPre-training High\n125\n–\nBo, Int. Train [5] China\nFine-tuning High\n1,186\n1,373\nBo, Int. Test [5]\nChina\nEvaluation\nHigh\n152\n126\nBo, Ext. Test [5] China\nEvaluation\nHigh\n138\n101\nCMHA [24]\nChina\nEvaluation\nHigh\n141\n95\nPrivate\nUnited States Evaluation\nHigh\n143\n219\nFor both pre-training and fine-tuning, we resample all scans to have a spacing\nof 0.4 mm and we crop areas below the skull base. We follow [8,28] to train\na nn-UNet [19] model specifically for artery segmentation rather than vessel\nsegmentation (we exclude veins due to venous aneurysms being rare and less\nclinically relevant [17]). Our artery segmentation model was trained on a set of\n25 dynamic CT scans annotated with iCafe [9], and achieved a modified Dice\nScore ([Y ∩ˆY ]/ ˆY ) of 0.89 on a held-out validation set. Using the nn-UNet, we\ngenerate artery distance maps using the signed distance transform as described\nin [8]. Since the labels for all datasets are dense 3D segmentations, we convert\nthem into tight bounding cubes to accommodate our detection pipeline.\n2.2\nModeling\nTransformer Encoder. Our encoder follows the 3D ViT architecture [14], with\na sinusoidal 3D positional embedding. The input patch size is 2 × 64 × 64 × 64.\nThe first channel consists of intensity values from the CT scan and the second\nchannel is an artery distance map that encodes how far each voxel is from the\nnearest artery. To ensure 3D attention fits within our computational budget,\nwe factorize self attention by computing it in two steps. For every Transformer\nlayer, we first compute self-attention among tokens within the same slice, and\nthen we compute the attention across slices by having tokens attend only to\ntheir temporal neighbors. In addition, a learnable CLS token is able to attend\nto all tokens during both steps to better encode global information. Thus, the\ntotal memory complexity of computing attention is bound by O(N 2), where N\nis the number of tokens in each slice.\nDetection module. Given a set of tokens (3D features) from the encoder,\nwe use a 3D sinusoidal positional encoding to introduce positional information.\nNext, we define nq learnable queries with the same feature dimension as the\nencoder’s output as in [6]. Then, we use cross-attention with the 3D features\nas key and values followed by self-attention among the queries. After this step,\nwe feed the attended queries through three MLP heads, where each query is\nresponsible for predicting the class (whether it is an aneurysm or not), location,\nand size for an aneurysm (can be empty meaning no aneurysm detected for a\n\n\nTitle Suppressed Due to Excessive Length\n5\n(a)\n(b)\nFig. 2: (a) Visual depiction of our masking scheme on a single CT scan slice:\nlighter patches, which mostly overlap with vessels (cyan) areas, are masked; the\nmodel can only see the darker areas during pre-training. (b) Illustration of our\nMAE pipeline, with both the CT scan and the distance map being reconstructed.\nquery). Since the number of queries is relatively small, we are able to use full\nattention in the detection module.\nPre-training strategy. To enhance representation learning and thus improve\nthe generalization ability of our model, we do pre-training for the encoder. To\nthis end, we add an auxiliary decoder, as shown in Fig. 2 (b). Our pre-training\nmethod builds on the MAE strategy, where patches in a given sample are ran-\ndomly masked out and a decoder is forced to reconstruct them based on the\nintermediate representation produced by the encoder and the architecture is op-\ntimized by minimizing the MSE for the masked tokens. After pre-training, the\ndecoder is discarded and the encoder can be fine-tuned on related downstream\ntasks [16].\nIn order to match the input size defined earlier, we randomly crop 64×64×64\nsub-scans from the patient’s full scan that overlap at least in 10% of their voxels\nwith the corresponding artery segmentation mask. Such a restriction ensures that\npre-training will happen on areas with higher likelihood of containing aneurysms.\nIn addition to this choice, there are two key differences with respect to the orig-\ninal MAE setup. First, instead of masking patches with an uniform random\ndistribution, we bias the selection toward patches close to blood arteries (see\nFig. 2 (a)); motivation for this decision is that aneurysms are typically located\nin arteries, and by masking arteries out we force the model to learn better rep-\nresentations for such areas. Second, instead of only reconstructing scan intensity\nvalues, we concatenate artery distance maps to the scans as a second channel\nand train the encoder to reconstruct the distance values as well. This forces the\nmodel to reason about the proximity of each reconstructed patch to the nearest\narteries regardless of whether they are masked. Another benefit of reconstructing\n\n\n6\nCeballos-Arroyo et al.\nTable 2: Detection performance with tIoU = 0.3 and assuming a fixed FPr=0.5.\nWe report Se@FPr curves to illustrate the threshold-agnostic performance of\neach model in Fig 1. Out-of-distribution (O.O.D.) datasets are highlighted. The\nprivate partition contains no healthy patients.\nSe ↑(%)\nDataset\nO.O.D. Model\nAll Small\nMed. Large P-Se ↑P-Sp ↑\nBo Int. Test\n✗\nCPM-Net 87.3\n72.4\n93.3\n71.4\n86/102\n34/50\nnnDet\n77.0\n65.5\n80.0\n85.7\n83/102\n33/50\nDeform\n90.5\n79.3\n93.3\n100\n91/102\n29/50\nOurs\n92.9\n75.9\n97.7\n100\n93/102\n35/50\nBo Ext. Test\n✓\nCPM-Net 90.1\n57.1\n95.9\n92.8\n82/92\n37/46\nnnDet\n88.1\n64.3\n90.4\n100\n84/92\n30/46\nDeform\n89.1\n64.3\n93.1\n92.8\n82/92\n35/46\nOurs\n93.1\n71.4\n95.9\n100\n85/92\n36/46\nCMHA\n✓\nCPM-Net 81.1\n63.9\n98.1\n20.0\n77/95\n37/48\nnnDet\n84.2\n66.6\n94.4\n100\n80/95\n40/48\nDeform\n82.1\n58.3\n98.1\n80.0\n78/95\n31/48\nOurs\n90.5\n80.5\n98.1\n80.0\n86/95\n35/48\nPrivate\n✓\nCPM-Net 57.5\n39.28\n67.5\n80.0\n75/143\n–\nnnDet\n59.4\n44.0\n68.3\n73.3\n73/143\n–\nDeform\n66.7\n52.4\n75.0\n80.0\n86/143\n–\nOurs\n72.6\n61.9\n79.2\n80.0\n94/143\n–\ndistance maps is that they can be used as an input channel during fine-tuning,\nmaking them useful across the pipeline.\nFinetuning strategy. For finetuning, we train both the encoder and detection\nmodule (without the decoder for MAE pre-training) on the annotated aneurysm\ndetection data [5], albeit using a lower learning rate compared with the pre-\ntraining stage. Our loss function is the average of 4 losses, similar to [8]: binary\ncross entropy (BCE) for aneurysm classification and three independent MSE\nlosses for localization, size, and intersection over union. For each ground truth\naneurysm, we use Hungarian matching to pair it to the closest detection/query.\nThe spatial losses are only computed when there is a positive match; otherwise,\nonly the classification loss is used. To avoid situations where most of the queries\nare assigned to the background class (and thus only classification is optimized),\nwe allow several detections to be matched to a given ground truth if they are\nclose enough to it (under 1 mm).\n3\nResults\nSetup. We use compute nodes with 4 NVIDIA A100 80GB GPUs and 96 CPU\ncores each for training and a workstation with 32 CPU cores and a NVIDIA 4090\n24GB GPU for processing private data. In our experiments, we use a Transformer\nencoder configuration with 6 layers, two sets of 8 attention heads each (one spa-\ntial, the other temporal), and a model dimension of 384, with a matching decoder\nfor pre-training. Our tokenizer groups sets of 4×4×4 voxels together, for a total\n\n\nTitle Suppressed Due to Excessive Length\n7\nFig. 3: 3D view and corresponding CTA images. Red: ground-truth aneurysm;\nYellow: algorithm output; Blue: artery segmentation. Top row (all TP): Right\nMCA aneurysm (A, B), anterior communicating artery aneurysm (smaller) and\nleft posterior communicating artery aneurysm (larger) (C,D), left ICA aneurysm\n(E, F). Bottom row: FP-basilar tip confluence (G, H), FP-posterior communi-\ncating artery infundibulum (I, J), FN-small ICA aneurysm (K, L)\nof 16×16×16 patches. We mask 75% of the patches following [16]. The detector\nuses 8 queries and a single transformer layer with 8 attention heads. During pre-\ntraining, we train for 100 epochs with the AdamW optimizer, using a learning\nrate of 1.5 × 10−3 scheduled to decrease to 1.5 × 10−4. For fine-tuning we do\n50 epochs with AdamW and a learning rate of 1 × 10−4. In both stages we use\na weight decay of 0.05. We consider three strong baselines, two of which have\nachieved state-of-the-art results in aneurysm detection: nnDetection [4], CPM-\nNet [25], and the multi-scale deformable detector described in [8]. nnDetection is\na self-configuring pipeline which fits a Retina-UNet-based detector using 5-fold\ncross-validation for inference; CPM-Net is an anchor-free, CNN-based detector\nleveraging a 3D squeeze-and-excitation attention mechanism; and the deformable\nmodel consists of a CNN encoder which produces features for a multi-scale de-\nformable attention mechanism leveraging vessel distance maps as a positional\nencoding. All three models were trained in a fully supervised manner on Bo et\nal.’s dataset [5].\nEvaluation metrics. Our main evaluation metric is Lesion-level Sensitivity\n(Se), i.e., the percentage of aneurysms correctly detected by a model. Based on\nconversations with radiologists, we determined an average of 0.5 FPs per scan to\nbe the ideal tolerance at which to measure model reliability (as clinicians can-\nnot be expected to filter through large amount of FPs), so Table 2 only reports\nSensitivity at such value. Nevertheless, Sensitivity vs FPr curves for all models\non all datasets are shown in Fig. 1. Other reported metrics include Patient Level\nSensitivity (P-Se) and Patient Level Specificity (P-Sp). To determine what con-\nstitutes a true positive (TP), we use an intersection over union (IoU) threshold\nof 0.3, which is at least as restrictive as those defined in the literature [8,3]. In\n\n\n8\nCeballos-Arroyo et al.\naddition, we report Sensitivity results for small, medium, and large (diameters\nof 0-3 mm, 3-7 mm, and 7+ mm, respectively) aneurysms across datasets.\nTable 3: Evaluation of the impact of our design choices on Specificity at a FP\nrate of 0.5 across 4 datasets. A is our best model; statistically significant results\n(via permutation testing) with respect to A are marked with an asterisk.\nPre-training strategy\nSe ↑(%)\nMod. Mask. Sampl. Reco. # scans Intern. Extern. CMHA Private\nA\n✓\n✓\n✓\n6,796\n92.9\n93.1\n90.5\n72.6\nB\n✓\n✓\n✓\n3,500\n88.9\n90.1\n81.1 *\n68.9\nC\n✓\n✓\n✓\n1,391\n87.3 *\n84.2 *\n75.8 *\n63.5 *\nD\n✓\n✓\n✗\n6,796\n91.3\n93.1\n82.1 *\n70.8\nE\n✓\n✗\n✗\n6,796\n82.5 *\n91.1\n78.9 *\n58.0 *\nF\n✗\n✗\n✗\n6,796\n78.6 *\n79.2 *\n72.6 *\n53.4 *\nG\n–\n–\n–\n–\n80.2 *\n82.2 *\n76.8 *\n58.9 *\nComparisons with other methods. Table 2 illustrates our results in con-\ntrast with the three baselines across four datasets and three aneurysm sizes.\nWe observe that our approach consistently outperforms the others, with a wider\ngap when it comes to out of distribution (O.O.D.) data. The qualitative results\ndepicted in Figure 3 show that our model is able to accurately detect aneurysms\nof various sizes and located at various key locations in the brain vasculature.\nOur approach also produced several FPs, but we noted that these proved diffi-\ncult for our radiologist colleagues to identify as such upon review. Some missed\ncases include very tiny aneurysms (Fig. 3 K, L) which may be a result of the\nmodel using a single, somewhat coarse input resolution. This is mirrored in our\nmodel’s results on the smallest aneurysms of Bo’s internal test partition, where\nthe deformable model outperforms it. Nevertheless, our implementation is effi-\ncient enough to explore more fine-grained resolutions in the future. Interestingly,\nall models perform significantly worse on our private dataset, which contains no\nhealthy patients. This belies the need for training strategies that can adapt to\npopulations whose distributions are drastically different from the training data.\nAblation studies. As for the impact of each design choice in our pipeline,\nTable 3 depicts the effect of introducing more artery-based steps to MAE pre-\ntraining, as well as the impact of having more pre-training data. Across the\nboard, the biggest impact results from sampling sub-scans from areas intersecting\nwith vessels, although going from 1,391 to 6,796 pre-training scans results in\nsignificantly better results across the three O.O.D. datasets, which highlights\nthe scalability of our approach. Notably, MAE pre-training without any of our\nchanges results in worse performance when compared to fully supervised training.\n\n\nTitle Suppressed Due to Excessive Length\n9\n4\nConclusion\nIn this paper, we described a novel approach for masked autoencoder pre-training\ntoward aneurysm detection using artery information as a guidance mechanism\nfor sampling, masking, and reconstruction. After fine-tuning, our model sur-\npasses the previous SOTA models on the CT-based intracranial aneurysm de-\ntection task. Compared with such models, ours matches their performance on\nin-distribution data while achieving significantly better results on three out of\ndistribution datasets, showcasing the impact of using unannotated data with\nanatomical guidance. Crucially, our results suggest that downstream perfor-\nmance scales with pre-training dataset size, highlighting there is room for further\nimprovements. Moreover, unlike other approaches, we use a fully Transformer\narchitecture with a factorized self-attention mechanism. In addition to being\nable to carry out MAE pre-training, using such an architecture provides am-\nple opportunities for future work combining image and text modalities for more\npersonalized clinical applications.\nAcknowledgments. The authors acknowledge the financial support provided by NIH\ngrant 1R01LM013891-01A1. Ceballos-Arroyo, A. and Jiang H. acknowledge the Na-\ntional Artificial Intelligence Research Resource (NAIRR) Pilot (NAIRR240236) and\nMicrosoft Azure for contributing to this research result. Ceballos-Arroyo, A. is grateful\nfor the funding provided by Colombia’s Minciencias and Fulbright under the Fulbright\nMinciencias 2021 program.\nDisclosure of Interests. The authors declare no competing interests.\nReferences\n1. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., Schmid, C.: ViViT: A\nVideo Vision Transformer (Nov 2021). https://doi.org/10.48550/arXiv.2103.\n15691\n2. Assis, Y., Liao, L., Pierre, F., et al.: Aneurysm Pose Estimation with Deep Learn-\ning. In: Greenspan, H., et al. (eds.) Medical Image Computing and Computer\nAssisted Intervention – MICCAI 2023, vol. 14221, pp. 543–553. Springer Nature\nSwitzerland, Cham (2023). https://doi.org/10.1007/978-3-031-43895-0_51\n3. Assis, Y., Liao, L., Pierre, F., et al.: Intracranial aneurysm detection: An ob-\nject detection perspective. International Journal of Computer Assisted Radi-\nology and Surgery 19(9), 1667–1675 (Sep 2024). https://doi.org/10.1007/\ns11548-024-03132-z\n4. Baumgartner, M., Jäger, P.F., Isensee, F., Maier-Hein, K.H.: nnDetection: A Self-\nconfiguring Method for Medical Object Detection. In: Medical Image Computing\nand Computer Assisted Intervention – MICCAI 2021. pp. 530–539. Lecture Notes\nin Computer Science, Springer International Publishing, Cham (2021)\n5. Bo, Z.H., Qiao, H., Tian, C., Guo, Y., et al.: Toward human intervention-free\nclinical diagnosis of intracranial aneurysm via deep neural network. Patterns 2(2),\n100197 (Feb 2021). https://doi.org/10.1016/j.patter.2020.100197\n\n\n10\nCeballos-Arroyo et al.\n6. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European Conference on Computer\nVision (ECCV) (2020)\n7. Caron, M., Touvron, H., Misra, I., et al.: Emerging Properties in Self-Supervised\nVision Transformers (May 2021). https://doi.org/10.48550/arXiv.2104.14294\n8. Ceballos-Arroyo, A.M., Nguyen, H.T., Zhu, F., et al.: Vessel-Aware Aneurysm De-\ntection Using Multi-scale Deformable 3D Attention. In: Linguraru, M.G., et al.\n(eds.) Medical Image Computing and Computer Assisted Intervention – MICCAI\n2024. pp. 754–765. Springer Nature Switzerland, Cham (2024). https://doi.org/\n10.1007/978-3-031-72086-4_71\n9. Chen, L., Mossa-Basha, M., Balu, N., et al.: Development of a Quantitative In-\ntracranial Vascular Features Extraction Tool on 3D MRA Using Semi-automated\nOpen-Curve Active Contour Vessel Tracing. Magnetic resonance in medicine 79(6),\n3229–3238 (Jun 2018). https://doi.org/10.1002/mrm.26961\n10. Chilamkurthy, S., Ghosh, R., Tanamala, S., et al.: Deep learning algorithms for\ndetection of critical findings in head CT scans: A retrospective study. The Lancet\n392(10162), 2388–2396 (Dec 2018). https://doi.org/10.1016/S0140-6736(18)\n31645-3\n11. Cong, Y., Khanna, S., Meng, C., Liu, P., Rozi, E., He, Y., Burke, M., Lobell, D.B.,\nErmon, S.: SatMAE: Pre-training Transformers for Temporal and Multi-Spectral\nSatellite Imagery (Jan 2023). https://doi.org/10.48550/arXiv.2207.08051\n12. Deng, Y., Hong, J., Zhou, J., Mahdavi, M.: On the Generalization Ability of Un-\nsupervised Pretraining. Proceedings of machine learning research 238, 4519–4527\n(May 2024)\n13. Di Noto, T., Marie, G., Tourbier, S., et al.: Towards Automated Brain Aneurysm\nDetection in TOF-MRA: Open Data, Weak Labels, and Anatomical Knowl-\nedge. Neuroinformatics 21(1), 21–34 (Jan 2023). https://doi.org/10.1007/\ns12021-022-09597-0\n14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al.: An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale (Jun 2021). https://doi.org/10.\n48550/arXiv.2010.11929\n15. Feichtenhofer, C., Fan, H., Li, Y., He, K.: Masked Autoencoders As Spatiotemporal\nLearners (Oct 2022). https://doi.org/10.48550/arXiv.2205.09113\n16. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R.: Masked Autoencoders\nAre Scalable Vision Learners. In: 2022 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR). pp. 15979–15988. IEEE, New Orleans, LA, USA\n(Jun 2022). https://doi.org/10.1109/CVPR52688.2022.01553\n17. Hoell, T., Hohaus, C., Beier, A., Holzhausen, H.J., Meisel, H.J.: Cortical Venous\nAneurysm Isolated Cerebral Varix. Interventional Neuroradiology 10(2), 161 (Oct\n2004). https://doi.org/10.1177/159101990401000210\n18. Hooper, S.M., Dunnmon, J.A., Lungren, M.P., Mastrodicasa, D., et al.: Impact of\nUpstream Medical Image Processing on Downstream Performance of a Head CT\nTriage Neural Network. Radiology: Artificial Intelligence 3(4), e200229 (Jul 2021)\n19. Isensee, F., Jaeger, P.F., Kohl, S.A.A., et al.: nnU-Net: A self-configuring method\nfor deep learning-based biomedical image segmentation. Nature Methods 18(2),\n203–211 (Feb 2021). https://doi.org/10.1038/s41592-020-01008-z\n20. Khoruzhaya, A.N., Bobrovskaya, T.M., Kozlov, D.V., et al.: Expanded Brain CT\nDataset for the Development of AI Systems for Intracranial Hemorrhage Detection\nand Classification. Data 9(2), 30 (Feb 2024)\n\n\nTitle Suppressed Due to Excessive Length\n11\n21. Park, S.W., Lee, J.Y., Heo, N.H., et al.: Short- and long-term mortality of sub-\narachnoid hemorrhage according to hospital volume and severity using a nationwide\nmulticenter registry study. Frontiers in Neurology 13, 952794 (Aug 2022)\n22. Ridnik, T., Ben-Baruch, E., Noy, A., Zelnik-Manor, L.: ImageNet-21K Pretraining\nfor the Masses (Aug 2021). https://doi.org/10.48550/arXiv.2104.10972\n23. Shi, Z., Miao, C., Schoepf, U.J., et al.: A clinically applicable deep-learning model\nfor detecting intracranial aneurysm in computed tomography angiography images.\nNature Communications 11(1), 6090 (Nov 2020)\n24. Song, M., Wang, S., Qian, Q., Zhou, Y., Luo, Y., Gong, X.: Intracranial aneurysm\nCTA images and 3D models dataset with clinical morphological and hemody-\nnamic data. Scientific Data 11(1), 1213 (Nov 2024). https://doi.org/10.1038/\ns41597-024-04056-8\n25. Song, T., Chen, J., Luo, X., et al.: CPM-Net: A 3D Center-Points Matching Net-\nwork for Pulmonary Nodule Detection in CT Scans. In: Martel, A.L., et al. (eds.)\nMedical Image Computing and Computer Assisted Intervention – MICCAI 2020,\nvol. 12266, pp. 550–559. Springer International Publishing, Cham (2020)\n26. Tong, Z., Song, Y., Wang, J., Wang, L.: VideoMAE: Masked Autoencoders are\nData-Efficient Learners for Self-Supervised Video Pre-Training (Oct 2022). https:\n//doi.org/10.48550/arXiv.2203.12602\n27. Wang, J., Sun, J., Xu, J., et al.: Detection of Intracranial Aneurysms Using Multi-\nphase CT Angiography with a Deep Learning Model. Academic Radiology 30(11),\n2477–2486 (Nov 2023). https://doi.org/10.1016/j.acra.2022.12.043\n28. Yadav, S., Kim, J., Young, G., Qin, L.: Dynamic-Computed Tomography An-\ngiography for Cerebral Vessel Templates and Segmentation (Feb 2025). https:\n//doi.org/10.48550/arXiv.2502.09893\n29. Yang, K., Musio, F., Ma, Y., et al.: Benchmarking the CoW with the TopCoW\nChallenge: Topology-Aware Anatomical Segmentation of the Circle of Willis for\nCTA and MRA (Apr 2024). https://doi.org/10.48550/arXiv.2312.17670\n30. Zhou, L., Liu, H., Bae, J., He, J., Samaras, D., Prasanna, P.: Self Pre-training\nwith Masked Autoencoders for Medical Image Classification and Segmentation\n(Apr 2023). https://doi.org/10.48550/arXiv.2203.05573\n31. Zhu, X., Su, W., Lu, L., et al.: Deformable Transformers for End-to-End Object\nDetection (2021). https://doi.org/10.48550/arXiv.2010.04159\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.21244v1.pdf",
    "total_pages": 11,
    "title": "Anatomically-guided masked autoencoder pre-training for aneurysm detection",
    "authors": [
      "Alberto Mario Ceballos-Arroyo",
      "Jisoo Kim",
      "Chu-Hsuan Lin",
      "Lei Qin",
      "Geoffrey S. Young",
      "Huaizu Jiang"
    ],
    "abstract": "Intracranial aneurysms are a major cause of morbidity and mortality\nworldwide, and detecting them manually is a complex, time-consuming task.\nAlbeit automated solutions are desirable, the limited availability of training\ndata makes it difficult to develop such solutions using typical supervised\nlearning frameworks. In this work, we propose a novel pre-training strategy\nusing more widely available unannotated head CT scan data to pre-train a 3D\nVision Transformer model prior to fine-tuning for the aneurysm detection task.\nSpecifically, we modify masked auto-encoder (MAE) pre-training in the following\nways: we use a factorized self-attention mechanism to make 3D attention\ncomputationally viable, we restrict the masked patches to areas near arteries\nto focus on areas where aneurysms are likely to occur, and we reconstruct not\nonly CT scan intensity values but also artery distance maps, which describe the\ndistance between each voxel and the closest artery, thereby enhancing the\nbackbone's learned representations. Compared with SOTA aneurysm detection\nmodels, our approach gains +4-8% absolute Sensitivity at a false positive rate\nof 0.5. Code and weights will be released.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}