{
  "id": "arxiv_2502.20604v1",
  "text": "Exploring the Impact of Temperature Scaling in\nSoftmax for Classification and Adversarial\nRobustness\n1st Hao Xuan\nElectrical and Computer Engineering\nUniversity of Alberta\nEdmonton, Canada\nhxuan@ualberta.ca\n2nd Bokai Yang\nElectrical and Computer Engineering\nUniversity of Alberta\nEdmonton, Canada\nbokai5@ualberta.ca\n3rd Xingyu Li\nElectrical and Computer Engineering\nUniversity of Alberta\nEdmonton, Canada\nxingyu@ualberta.ca\nAbstract—The softmax function is a fundamental component\nin deep learning. This study delves into the often-overlooked\nparameter within the softmax function, known as \"temperature,\"\nproviding novel insights into the practical and theoretical aspects\nof temperature scaling for image classification. Our empirical\nstudies, adopting convolutional neural networks and transformers\non multiple benchmark datasets, reveal that moderate tempera-\ntures generally introduce better overall performance. Through\nextensive experiments and rigorous theoretical analysis, we explore\nthe role of temperature scaling in model training and unveil\nthat temperature not only influences learning step size but also\nshapes the model’s optimization direction. Moreover, for the first\ntime, we discover a surprising benefit of elevated temperatures:\nenhanced model robustness against common corruption, natural\nperturbation, and non-targeted adversarial attacks like Projected\nGradient Descent. We extend our discoveries to adversarial\ntraining, demonstrating that, compared to the standard softmax\nfunction with the default temperature value, higher temperatures\nhave the potential to enhance adversarial training. The insights\nof this work open new avenues for improving model performance\nand security in deep learning applications.\nI. INTRODUCTION\nDeep learning has achieved dramatic breakthroughs in recent\nyears, excelling in tasks such as image classification [12], nature\nlanguage processing (NLP) [5], and semantic segmentation [27].\nA critical component of most deep learning methods is the\nsoftmax function, which normalizes a set of real values into\nprobabilities. The generalized softmax function incorporates a\nparameter known as \"temperature,\" which controls the softness\nof the output distribution. Despite its importance in theory,\nthe impact of temperature scaling on classification tasks has\nbeen relatively underexplored, particularly in contrast to its use\nin other areas such as knowledge distillation [8], contrastive\nlearning [24], confidence calibration [21], and natural language\nprocessing. Specifically, though the temperature scaling has\noccasionally been applied in prior experimentation [6], [10],\n[23], these studies often integrate additional complex techniques\nsuch as Gaussian noise injection in [23], adversarial training\nin [6], [22], and innovative quadratic activation functions in [10],\nmaking it challenging to isolate and understand the specific\ncontribution of temperature scaling to the overall system\nperformance. Consequently, the specific role of temperature in\nclassification tasks remains ambiguous. Previous study by [1]\nhas hinted at the potential benefits of temperature scaling, but\na comprehensive investigation is still lacking.\nThis study aims to fill this gap by conducting extensive\nexperiments to explore the practical and theoretical aspects of\ntemperature scaling in the softmax function for image clas-\nsification. We employ convolutional neural networks (CNNs)\nand transformers on multiple benchmark datasets, including\nCIFAR-10 [11], CIFAR-100 [11], and Tiny-ImageNet [16],\nto systematically analyze the effects of different temperature\nvalues. Our empirical results consistently show that moderate\ntemperatures generally improve overall performance, chal-\nlenging the conventional knowledge derived from contrastive\nlearning that low temperature facilitates representation learning.\nWe also delve into the theoretical underpinnings of tem-\nperature scaling in model training. Our analysis reveals that\ntemperature not only influences the learning step size but also\nshapes the model’s optimization direction. Specifically, lower\ntemperatures focus the model’s learning on error-prone classes,\nwhile higher temperatures promote a more balanced learning\nacross all classes. This insight is crucial for understanding the\nnuanced effects of temperature scaling on model optimization.\nFurthermore, we uncover a surprising benefit of elevated\ntemperatures: enhanced model robustness against common\ncorruptions, natural perturbations, and non-targeted adversarial\nattacks, such as Projected Gradient Descent (PGD). We extend\nour investigation to adversarial training introduced by [18],\ndemonstrating that higher temperatures can potentially enhance\nthe robustness of models trained with adversarial methods\ncompared to those using the standard softmax function with\nthe default temperature.\nIn summary, this work provides new perspectives on the\npractical applications and theoretical implications of tempera-\nture scaling in the softmax function. Our contributions can be\nsummarized as follows:\n• We conduct extensive experiments demonstrating that\napplying a reasonably large temperature during model\ntraining improves overall performance.\narXiv:2502.20604v1  [cs.LG]  28 Feb 2025\n\n\n• We discover that models trained with elevated tempera-\ntures exhibit enhanced robustness against gradient-based\nuntargeted adversarial attacks.\n• Additionally, we show the potential of integrating tem-\nperature control into adversarial training to boost model\nperformance and security in deep learning applications.\nII. RELATED WORKS\nThe softmax function has been a longstanding component\nof neural networks, usually used to normalize a vector of\nreal values into probabilities. Modulating the temperature\nscaling factor within the softmax function allows for reshaping\nthe probability distribution. This section provides a concise\noverview of the application of temperature scaling in various\ncomputational tasks.\nKnowledge Distillation proposed by [8] is one innovative\nway to transfer knowledge from a teacher model to a student\nmodel. Temperature is utilized during training to control both\nthe student and teacher model’s output. The author argues that\nlower temperatures make the distillation assign less weight\nto logits that are much smaller than the average. Conversely,\nemploying larger temperatures softens the probability distribu-\ntion and pays more attention to the unimportant part of the\nlogit. Larger temperatures are proven to be beneficial in the\ndistillation process since the hard-target term already ensures\nthe dominant part of the logit (target class) is correct. By\nfocusing on the remaining logit, the student model can capture\nmore fine-grained information from the teacher model. Note\nthat despite various temperatures used during training, it is set\nto 1 when the model is deployed.\nModel Confidence Calibration usually utilizes temperature\nscaling to address the over-confident issue in deep learning [7],\n[15], [19]. It centers on estimating predictive uncertainty to\nmatch its expected accuracy [13], [14]. Despite multiple generic\ncalibration methods being proposed, temperature scaling pro-\nposed by [7] remains a baseline method for being simple,\neffective and able to apply to various cases without major\nexpense. The motivation behind temperature scaling is simple,\nsince the goal is to control the network’s confidence to match\nits accuracy, applying temperature to the softmax function that\ncan directly modify the probability distribution seems a perfect\nfit for the problem. During training, a validation set is needed\nto find the ideal temperature parameter for the network, and\nthe same temperature is used when deployed.\nContrastive Learning is one paradigm for unsupervised\nlearning [20], [26]. To achieve a powerful feature encoder,\nit utilizes contrastive loss to pull similar samples close and\npush negative pairs away in the latent space. Although the\ntemperature has long existed as a hyper-parameter in contrastive\nloss, its actual mechanism is just understudied recently. [24]\nanalyze the contrastive loss closely and find that as the temper-\nature decreases, the distribution of the contrastive loss becomes\nsharper, which applies larger penalties to samples similar to the\nanchor data. Also, uniformity of feature distribution increases,\nindicating the embedding feature distribution aligns with a\nuniform distribution better [25].\nTemperature Scaling in Image Classification has occa-\nsionally been utilized in the experimental sections of prior\nstudies, yet focused investigations on this subject remain limited.\nFor example, previous studies aiming to improve adversarial\nrobustness have utilized temperature scaling to adjust logits\nwithin their experimentation [6], [10], [23]. However, these\nstudies often integrate additional complex techniques such as\nGaussian noise injection [23], adversarial training [6], [22],\nand innovative quadratic activation functions [10], making it\nchallenging to isolate and understand the specific contribution\nof temperature scaling to the overall system performance. In\ncontrast, our study narrows its focus to investigating the direct\nimpact of temperature scaling applied through the softmax\nfunction on model optimization processes. Among the few\nrelated works, \"The Temperature Check\" by [1] is notably\nrelevant to our discussion. It mainly explores the dynamics of\nmodel training by considering factors such as temperature,\nlearning rate, and time, and presents an empirical finding\nthat a model’s generalization performance is significantly\ninfluenced by temperature settings. While our observations align\nwith these findings, our research approaches the issue from\na different perspective of gradient analysis. Specifically, we\ndelve into how temperature scaling impacts model optimization\nprocess. Furthermore, our study broadens the scope of inquiry\nby assessing the effect of temperature scaling on a model’s\nresilience to common corruptions and adversarial attacks,\nthereby adding a new dimension to the existing research.\nIII. PRELIMINARY\nA. Softmax Function\nGiven a set of real numbers, 𝑋= {𝑥1, .., 𝑥𝑁}, the generalized\nsoftmax function can be used to normalize 𝑋into a probability\ndistribution.\nS(𝑋) =\nexp(𝑋/𝜏)\nÍ\n𝑖exp(𝑥𝑖/𝜏) ,\n(1)\nwhere S represents the softmax function and 𝜏is the tempera-\nture scaling factor. The temperature 𝜏controls the smoothness\n(softness) of the probability it produces. Specifically, when\n𝜏→∞, the output tends toward a uniform distribution; while\nwhen 𝜏= 0, the softmax function assigns a probability of 1 to\nthe element with the highest value and a probability of 0 to\nthe rest. The standard (unit) softmax function, with 𝜏= 1, is\nwidely used in conventional classification tasks.\nB. Problem Definition and Notation\nWe consider multi-category classification in this study, where\npaired training data {𝒳,𝒴} = {(𝑥, 𝑦)|𝑥∈R𝐻×𝐿×𝑁, 𝑦∈\nR1×𝑀} are drawn from a data distribution D. Here, 𝐻, 𝐿, 𝑁\nare the dimension of a sample 𝑥, 𝑀is the number of categories,\nand 𝑦is a one-hot vector indicating the class of the input 𝑥. A\nclassifier, C : 𝒳−→𝒴, is a function predicting the label 𝑦for\na given data 𝑥. That is 𝐶(𝑥) = 𝑦. In the canonical classification\nsetting, a neural network classifier, C = ( 𝑓, 𝑊), is usually\ncomposed of a feature extractor 𝑓parameterized by 𝜃and a\nweight matrix 𝑊. 𝑓is a function mapping the input 𝑥to a\nreal-valued vector 𝑓(𝑥) in the model’s penultimate layer and\n\n\n𝑊= (𝑤1, ..., 𝑤𝑀) represents the coefficients of the last linear\nlayer before the softmax layer. So the likelihood probability of\ndata 𝑥corresponding to the 𝑀categories can be formulated as\nˆ𝑦= C(𝑥) = S(𝑊𝑇𝑓(𝑥)).\n(2)\nNote that each vector 𝑤𝑖in matrix 𝑊can be considered as\nthe prototype of class 𝑖and the production 𝑊𝑇𝑓(𝑥) in Eqn. 2\nquantifies the similarity between the feature 𝑓(𝑥) and different\nclass-prototypes.\nDuring training, the model C = ( 𝑓, 𝑊) is optimized to\nminimize a specific loss, usually a Cross-Entropy (CE) loss.\n𝐿𝑐𝑒(𝑥) = −𝑦log ˆ𝑦= −log\n\"\nexp(𝑤𝑇\n𝑖· 𝑓(𝑥)/𝜏)\nÍ𝑁\n𝑗=1 exp(𝑤𝑇\n𝑗· 𝑓(𝑥)/𝜏)\n#\n(3)\nThough 𝜏= 1 is the default setting in classification tasks, we\npreserve 𝜏in the Eqn.s to facilitate theoretical analysis.\nIV. GRADIENT ANALYSIS\nTo investigate the impact of temperature scaling factors for\nmodel optimization in classification tasks, we calculate the loss\ngradients with respect to the training parameters in the model.\nSpecifically, given a data sample 𝑥from the 𝑖𝑡ℎcategory, we\nrefer to 𝑤𝑖as the positive class prototype and the rest, 𝑤𝑗for\n𝑗≠𝑖, as the negative class prototypes. Then the gradients with\nrespect to the positive class prototype, negative class prototypes,\nand the encoder are:\n𝜕𝐿𝑐𝑒(𝑥)\n𝜕𝑤𝑖\n= 1\n𝜏[S(𝑤𝑇\n𝑖· 𝑓(𝑥)/𝜏) −1] 𝑓(𝑥) = 1\n𝜏[𝑃𝜏\n𝑖(𝑥) −1] 𝑓(𝑥),\n(4)\n𝜕𝐿𝑐𝑒(𝑥)\n𝜕𝑤𝑗\n= 1\n𝜏S(𝑤𝑇\n𝑗· 𝑓(𝑥)/𝜏) 𝑓(𝑥) = 1\n𝜏𝑃𝜏\n𝑗(𝑥) 𝑓(𝑥),\n(5)\n𝜕𝐿𝑐𝑒(𝑥)\n𝜕𝑓\n= 1\n𝜏\n\"∑︁\n𝑗≠𝑖\n𝑤𝑘𝑃𝜏\n𝑗(𝑥) −𝑤𝑖[1 −𝑃𝜏\n𝑖(𝑥)]\n#\n.\n(6)\nLearning rate: In Eqn. 4, 5, 6, since 0 < 𝑃𝜏\n𝑗(𝑥) < 1, the\nactual learning rate is inversely proportional to the temperature\n𝜏. That is, larger temperatures lead to a reduced gradient step in\nmodel update, while smaller temperatures not only increase the\ngradient step. Furthermore, when the sample 𝑥is misclassified,\nsmaller temperatures give a further boost on updating 𝑤𝑖and\n𝑤𝑗for 𝑗= 𝑎𝑟𝑔𝑚𝑎𝑥(𝑃𝜏\n𝑗(𝑥) 𝑓(𝑥)), because smaller temperatures\nin softmax function lead to shaper distributions.\nOptimization direction: From Eqn. 4, the positive class\nprototype 𝑤𝑖is updated toward 𝑓(𝑥) in the latent space. In\ncontrast, the negative prototypes 𝑤𝑗move away from the\ndirection of 𝑓(𝑥) according to Eqn. 5. The optimization\ndirection of 𝑓(𝑥) is a weighted sum of all class prototypes,\nas shown in Eqn. 6. The fundamental optimization policy is\nto update the trainable parameters of the encoder in such a\nway that 𝑓(𝑥) moves closer to the positive class prototype and\nfarther away from the negative class prototypes in the latent\nspace. However, when we take the temperature parameter into\naccount, we find that temperature has an impact on the update\ndirection of 𝑓(𝑥). Specifically, when the temperature is low,\nFig. 1: Demonstration of the model optimization direction\nwith different temperatures. 𝑓(𝑥) is the latent code of a data\nsample from category 3. Since 𝑓(𝑥) is close to the negative\nclass prototype 𝑤1, the CE loss with respect to the encoder 𝑓\nyields a large gradient toward the groundtruth 𝑤3. However,\nwith different temperature factors, the gradients associated with\nthe negative classes are different: low temperature makes the\nupdate more biased by the hard class (a), while an elevated\ntemperature leads to more equalized gradients (b).\nthe probability distribution produced by the softmax function is\nsharper, leading to significant differences in probability values\namong different prototypes. Consequently, the update direction\nof the encoder 𝑓is predominantly influenced by the class\nprototype with the highest probability and the positive class\nprototype (if they are different). Fig. 1(a) visualizes the bias\ntoward the hard class in model optimization, where 𝑓(𝑥) is\nthe latent code of a data sample from category 3. In contrast,\nwhen the temperature is high, the differences in probability\nvalues among different prototypes are relatively smaller, and\nthe encoder 𝑓updates with a mixture of all class prototype\ndirections, as demonstrated in Fig. 1(b). In other words, a\nlow temperature makes the model focus on learning hard-class\npairs, while a high temperature de-biases the influence among\ndifferent classes for a balanced learning.\nMoreover, when considering all the samples in one batch,\nthe compound gradient of all 𝑁samples are\n𝑁\n∑︁\n𝑛=1\n𝜕𝐿𝑐𝑒(𝑥𝑛)\n𝜕𝑤𝑖\n= −1\n𝜏\n𝑁\n∑︁\n𝑛=1\n𝑓(𝑥𝑛)[1 −𝑃𝜏\n𝑖(𝑥𝑛)],\n(7)\n𝑁\n∑︁\n𝑛=1\n𝜕𝐿𝑐𝑒(𝑥𝑛)\n𝜕𝑤𝑘\n= 1\n𝜏\n𝑁\n∑︁\n𝑛=1\n𝑓(𝑥𝑛)𝑃𝜏\n𝑘(𝑥𝑛),\n(8)\n𝑁\n∑︁\n𝑛=1\n𝜕𝐿𝑐𝑒(𝑥𝑛)\n𝜕𝑓\n= 1\n𝜏\n𝑁\n∑︁\n𝑛=1\n\"∑︁\n𝑘≠𝑖\n𝑤𝑘𝑃𝜏\n𝑘(𝑥𝑛) −𝑤𝑖[1 −𝑃𝜏\n𝑖(𝑥𝑛)]\n#\n.\n(9)\nSimilar to the single sample case, when optimizing in a\nwhole batch, with small temperatures, the model focuses on\nlearning misclassified samples (i.e. hard samples), whereas\nhigher temperatures help de-bias the update direction and\ndistribute similar weight to all samples.\n\n\nFig. 2: T-SNE [17] visualization of the CIFAR10 sample distribution after the ResNet50 encoder with different temperatures.\nFig. 3: T-SNE [17] visualization of the CIFAR10 sample distribution after the VIT encoder with different temperatures.\nV. EMPIRICAL ANALYSIS AND DISCUSSION\nAs discussed in Section 4, applying a small temperature\nencourages a model to learn more about hard (misclassified)\nsamples and hard (error-prone class) classes. A low temperature,\nhowever, leads to more equitable learning across different\nclasses and data points. Theoretically, both approaches to\noptimize feature distribution sound reasonable, with low\ntemperatures focusing on weaker classes and high temperatures\ndecreasing inequality across all negative classes. We argue that\nwhich optimization strategy is better for classification tasks\nremains an empirical problem.\nA. Experiment Setting\nWe conduct image classification on multiple benchmarks (i.e.\nCIFAR10, CIFAR100, and Tiny-ImageNet) and their extended\nCommon Corruptions and Perturbations sets (i.e. CIFAR10-C,\nCIFAR100-C, and Tiny-ImageNet-C with corruption strentgh\nbeing 3) to investigate the impact of temperature scaling.\nIn addition, we also evaluate the model’s robustness against\nadversarial attacks such as PDG20 [18] and C&W [2]. Both\nattacks are bounded by the 𝑙∞box with the same maximum\nperturbation 𝜖= 8/255.\nTo\nget\na\ncomprehensive\nevaluation,\nwe\nset\n𝜏\n∈\n{0.1, 0.5, 1, 10, 30, 50, 70, 100}. Unless stated otherwise, we\ntakes ResNet50 and VIT-small-patch16-224 as the CNN and\ntransformer backbones, respectively. The ResNet50 is trained\nfrom scratch, with SGD optimizer and learning rate setting to\n0.1. We also utilize the Cosine Annealing scheduler to better\ntrain the model. The transformer is pretrained on ImageNet-\n21K and finetuned on the target dataset using Adam optimizer.\nAll experiments run on one RTX3090.\nTo clarify, the temperature scaling only involves in model\ntraining in this study, but not model evaluation and attacks.\nAll empirical evaluation and adversarial sample generation by\nPGD and C&W are based on the standard cross entropy, i.e.\n𝜏= 1. Thus, attack gradients are not attenuated, reflecting\nmodel’s true sensitivity to data perturbation.\nB. Experiment Results\nThe quantitative results on CNN and Transformer are\nsummarized in Table I and Table II, respectively. For the\nCNN model, ResNet50, training from scratch, the standard\naccuracy increases with the temperature increase. Furthermore,\nCNN models trained at elevated temperatures show more\nrobustness against naturally corrected images. We believe that\nsuch improvements are majorly attributed to better model\noptimization with leveraged temperature. For the transformer\nfinetuned on the target set, the standard accuracy and robustness\nagainst natural corruptions and perturbations is quite stable. We\nhypothesize that such stable performance is due to the fact that\nViT has already been pre-trained on ImageNet and has reached\na relatively high-quality state. Additionally, we observed that\nthe model’s adversarial robustness gradually improves with\nincreasing temperature.\n\n\nTABLE I: Model performance and Robustness against Common Corruptions and Adversarial attacks (%) under different\ntemperatures with ResNet50 trained from scratch. -C in the table represents the corresponding Common Corruptions and\nPerturbations set.\nTemp.\nCIFAR10\nCIFAR100\nTiny-Imagenet\nClean\n-C\nPGD20\nC&W\nClean\n-C\nPGD20\nC&W\nClean\n-C\nPGD20\nC&W\n𝜏= 0.1\n90.05\n73.31\n0\n27.79\n70.39\n44.52\n0\n14.32\n54.53\n12.63\n0\n23.17\n𝜏= 0.5\n94.17\n72.51\n0\n16.03\n74.79\n45.41\n0\n8.44\n61.07\n18.55\n0\n19.44\n𝜏= 1\n94.26\n72.53\n0\n19.19\n74.58\n46.47\n0\n11.26\n62.93\n18.66\n0\n19.09\n𝜏= 10\n95.41\n73.94\n0.56\n39.79\n78.21\n50.67\n0.29\n15.33\n64.70\n21.66\n2.59\n23.88\n𝜏= 30\n95.26\n74.93\n91.09\n43.35\n78.27\n50.17\n68.47\n18.81\n63.60\n21.30\n49.45\n26.50\n𝜏= 50\n94.92\n74.44\n93.04\n36.13\n77.97\n49.87\n72.92\n20.50\n62.85\n20.40\n54.95\n28.68\n𝜏= 70\n95.05\n74.26\n93.85\n35.43\n77.20\n49.61\n73.49\n21.66\n62.14\n20.57\n55.54\n30.14\n𝜏= 100\n95.05\n73.08\n94.29\n37.32\n77.14\n49.31\n73.65\n22.83\n61.46\n18.82\n54.60\n32.71\nTABLE II: Model performance and Robustness against Com-\nmon Corruptions and Adversarial attacks (%) under different\ntemperatures with Transformer Vit-small-patch16-224. -C in\nthe table represents the corresponding Common Corruptions\nand Perturbations set.\nTemp.\nCIFAR10\nCIFAR100\nClean\n-C\nPGD20 C&W Clean\n-C\nPGD20 C&W\n𝜏= 0.1\n98.45 92.83\n0\n26.13\n89.79\n74.7\n0\n23.71\n𝜏= 0.5\n98.33 91.60\n0\n26.26\n90.53\n74.9\n0\n29.25\n𝜏= 1\n98.29 92.21\n0\n31.69\n90.78\n75.5\n0\n31.97\n𝜏= 10\n98.06 92.19\n89.07\n31.89\n89.94\n75.5\n58.71\n34.96\n𝜏= 30\n98.23 91.72\n97.10\n38.21\n89.52\n74.6\n86.25\n36.07\n𝜏= 50\n98.22 91.43\n97.75\n39.52\n89.28\n73.8\n87.29\n33.64\n𝜏= 70\n98.03 91.20\n97.72\n39.02\n89.48\n74.2\n87.96\n33.81\n𝜏= 100 98.07 91.56\n97.87\n38.26\n89.13 73.47\n86.99\n31.84\nClustering is a crucial metric when measuring how an\nencoder performs. In classification, a good encoder should\nbe able to gather samples from the same class while separating\nclusters of different classes. Fig. 2 and Fig. 3 present 2D\nTSNE visualization of the CIFAR10 sample distribution by\nResNet50 and transformer. We observe a similar trend: low\ntemperatures lead to more mixed clusters, while models trained\nwith elevated temperatures have better cluster effects. These\nempirical observations also explain the improved classification\nperformance on clean and non-adversarial perturbations, as\nwell as stronger adversarial robustness, with high temperature\nin Table I and Table II.\nC. Training Convergence\nWe then conduct experiments observing the training process\nwhen applying different temperatures to the model. We validate\nthe model on the test set every epoch and record the error\nFig. 4: Test error number during training. The red line\nrepresents 𝜏= 0.5, the green line represents 𝜏= 1, and the\norange line represents 𝜏= 50. The model used is Resnet50 and\nis tested on CIFAR10. SGD optimizer is used during training\nwith the learning rate set to 0.1 (a) and 0.01 (b). The shade\nareas consist of 6 total runs with different random seeds. The\nsolid lines indicate the mean value across all runs.\nprobability. As our results shown in Fig. 4(a), we can clearly\nobserve that not only does the training convergence speed\nincrease as the temperature goes up, but models trained with\nhigher temperatures also tend to converge to lower points,\nleading to better final performance. In fact, when we further\ndecrease the temperature to around 0.1, the model would have\na substantial risk of not converging at all. While this might\nappear contrary to the common understanding that focusing on\nhard classes will generally benefit the model, a more nuanced\nexplanation is provided by delving further into the gradient\nanalysis provided in Section 4.\nFrom Eqn. 4, 5, 6, we observe that if the logit of the target\nclass is not the largest, its gradient will increase dramatically\nwith low temperatures. This is potentially bad for models being\nknown to converge inefficiently under large learning rates. One\nstraightforward solution would be lowering the learning rate\nas shown in Fig. 4. While the training converging speeds are\ncloser, the run with a higher temperature can still reach a\nbetter performance. Furthermore, regardless of the increase in\noverall training converging speed for 𝜏= 0.5 and 𝜏= 1 runs\n\n\nFig. 5: The logit changes before and after PGD20 attack. The blue lines stand for the logits of the samples before PGD attack,\nand the orange lines stand for the logits of the samples after PGD attack.\nwhen lowering the learning rate, the final performances for all\nthree runs actually get worse than runs with 0.1 learning rate.\nTherefore, this phenomenon cannot be attributed solely to a\nhigh learning rate. However, if we shift our perspective to the\noverall direction for optimization as done in Eqn. 9, it becomes\nclear that during the early stage of training, the encoder 𝑓\nhas not converged to an ideal point, leading to sub-optimal\nvalues produced for certain update directions. If this happens\nto be the direction of the target class and the error-prone class\nwhich models with small temperatures tend to focus on, the\nmodel training can be impacted harmfully. In the meantime,\nhigh temperatures equalize the weight given to all the classes\nand ensure the update is not terribly wrong even if a few\n𝜕𝐿𝑐𝑒(𝑥)/𝜕𝑤𝑗are in the wrong direction. Upon reaching this\nconclusion, we are surprised to find that this reasoning and\nour empirical observations align perfectly with the curriculum\nlearning philosophy, that starting from hard samples may harm\nmodel optimization and learning outcomes.\nD. Adversarial Robustness\nTable I and Table II show that models trained with elevated\ntemperatures have strong adversarial robustness. TSNE plots\nin Fig. 2 and Fig. 3 also support this observation. This\nprompts questions regarding the mechanism behind the gained\nrobustness. In this section, our focus is on investigating the\nmodel’s behavior under adversarial attacks and understanding\nwhy the model demonstrates such robustness.\nGradient analysis for adversarial generation. In order to\ndiscern the source of model robustness, we follow the work\nin [9] and study the gradient of the classification loss with\nrespect to the input to analyze the direction of the PGD attack,\nwhich can be written as\n𝜕𝐿𝑐𝑒\n𝜕𝑥\n= [(S(𝑤𝑇\n𝑖· 𝑓(𝑥)) −1) · 𝑤𝑇\n𝑖+\n∑︁\n𝑗≠𝑖\n𝑤𝑇\n𝑗· S(𝑤𝑇\n𝑗· 𝑓(𝑥))] · 𝜕𝑓(𝑥)\n𝜕𝑥\n(10)\nAs illustrated above, given a well-trained model, for most\ninputs where S(𝑤𝑇\n𝑖· 𝑓(𝑥)) ≈1, the gradient does not have\na noticeable portion in target class 𝑤𝑖on the early stage of\nthe attack. This implies that rather than directly ’stepping\naway’ from the target class, the attack will initially focus on\napproaching other class prototypes. Moreover, the second term,\nÍ\n𝑗≠𝑖𝑤𝑇\n𝑗·S(𝑤𝑇\n𝑗· 𝑓(𝑥)), indicates that all the other directions are\nweighted by their according probabilities. Therefore, untargeted\nattacks are actually targeted toward the error-prone class, which\nmost commonly is the largest probability class other than the\ntarget class. However, if a model lacks an error-prone class\ngiven an input, all 𝑤𝑘will be weighted equally. Consequently,\nthe gradient would point toward all negative class prototypes,\nmaking it exceptionally challenging to determine the optimal\ndirection. We noticed that such a scenario occurs when a\nmodel is trained with a small 𝜏. Then let’s focus on the\ngradient update strength. For a data sample 𝑥is classified\ncorrectly, S(𝑤𝑇\n𝑗· 𝑓(𝑥)) would be small when the model training\ntemperature 𝜏increases. That is, when a model is trained with\nhigh temperatures, not only the gradient direction to generate\nadversarial samples is not clear, but the gradient strength is\nalso small. Both factors contribute to the robustness of the\nmodel when optimized with elevated temperatures.\nRaw Logit Analysis. With the insight from the gradient\nanalysis on adversarial attack, we then turn to observe the logit\noutput around the adversarial attack, as shown in Fig. 5. Each\nbar represents the logit value for each class, blue bars stand\nfor the logit outputs of clean samples and orange bars are the\nlogit outputs from adversarial samples. Models share similar\ncharacteristics in low temperatures, Fig. 5(a,b), with the logit\nof the target class going down while the logit of the error-\nprone class going up. However, for models trained with large\ntemperatures, Fig. 5(c,d), two logits are nearly identical with\na minimal amount of changes. This contrasts the robustness\ngains during adversarial training, where the model learns the\npattern of the adversarial noise.\nClass Prototypes Analysis. To further analyze the model\nbehavior, we investigate the relation between the encoded\nfeature, 𝑓(𝑥), and each class prototype, 𝑤𝑗. Here, we observe\nthe Euclidean distance and cosine similarity. Fig. 6 shows\nEuclidean distance and cosine similarity between one sample\nand all class prototypes. It is evident that as the training\ntemperature goes up, the feature 𝑓(𝑥) tends to have an identical\ndistance to all negative class prototypes. This indicates the\nmodel trained with high temperature is less likely to have an\nerror-prone class, which is essential for untargeted attacks as\nwe discuss above.\nFurthermore, to illustrate that the phenomenon shown in\nFig. 6 is not limited to one or a few samples, we calculate\n\n\nFig. 6: A demonstration of the Euclidean distance and cosine similarity between the encoded sample 𝑓(𝑥) and all class\nprototypes for one sample, with different temperature configurations. The red lines indicate the Euclidean distance while the\nblue lines stand for cosine similarity.\nFig. 7: Box plot of the variance of the Euclidean distance and\ncosine similarity calculated from each sample. The variances\nare calculated across all negative class prototypes, therefore,\nlower variance indicates a more uniform distribution of all\nnegative class distances. Each box is a model trained with a\ndifferent temperature, the green line shows the median value\nacross all variances and the orange line is the mean value of\nall variances.\nthe variance of Euclidean distance and cosine similarity of all\nnegative class prototypes across all samples in CIFAR10 test\nset. Note that as illustrated in Fig. 6, different models have\nvery different ranges for Euclidean distance between encoded\nfeature and class prototypes. Therefore, we map the value of\ndifferent models into the same range to make a more direct\ncomparison. Box plots are drawn in Fig. 7 showing the overall\nvariance results with each box being a model trained with\na different temperature. We can observe a clear trend that\nwhen the temperature rises, the variance for both Euclidean\ndistance and cosine similarity drops indicating the encoded\nsample, 𝑓(𝑥), has a more similar distance to all negative class\nprototypes. One might notice an increase in variance when the\ntemperature reaches some threshold. We label them as extreme\ntemperatures, which are so large that they can adversely affect\nthe model’s convergence.\nE. Further Discussion on Adversarial Robustness\nDespite the model trained with high temperatures showing\nsuperb robustness against untargeted PGD attack due to its\nnature attribute that discovers the weakness of PGD attack, it\ndoes not hold robustness against targeted attacks. The reason\nbehind this is straightforward. In targeted attacks, Eqn. 10\nno longer holds, and the gradient is not obligated to move\nTABLE III: Preliminary experiments of adversarial training on\nCIFAR-10 with temperature control. The training scheme uses\n[18] and the model is ResNet50.\nTemp.\n𝜏= 0.5\n𝜏= 1\n𝜏= 10\n𝜏= 30\n𝜏= 50\n𝜏= 70\n𝜏= 100\nClean\n88.98\n85.67\n81.71\n82.62\n83.75\n84.28\n84.27\nPGD20\n35.93\n42.63\n40.95\n44.96\n48.61\n49.16\n48.53\ntowards all negative class prototypes with a weighted step size.\nTherefore, with the only source of the model robustness gained\neliminated, it is naturally vulnerable to targeted attacks.\nRemark: Even though many attacks claim themselves to\nbe untargeted attacks, they actually optimize toward one self-\nselected target, which we do not consider untargeted attacks\nunder this setting. One popular example is the Difference of\nLogits Ratio(DLR) attack proposed by [4]. Regardless of its\nability to rescale the logit,\nDLR(𝑥, 𝑦) = −\n𝑧𝑦−max\n𝑖≠𝑦𝑧𝑖\n𝑧𝜋1 −𝑧𝜋3\n(11)\nshows that the DLR loss automatically selects the class holding\nthe largest logit other than the target class as the attack target.\nTherefore, during optimization, it does not need to optimize\ntoward all negative class prototypes. A similar example also\nincludes FAB attack [3].\nF. Extended Experiment on Adversarial Training\nGiven that our temperature control method is used inside\nthe Cross-Entropy Loss, it is possible to apply this method\nin adversarial training. Here, we do preliminary experiments\non the adversarial training baseline proposed by [18] for the\nsimplicity of its loss function. We add temperature control\ninside vanilla loss term forming\n𝐿𝐴𝑇(𝑥, 𝑥𝑎𝑑𝑣, 𝑦, 𝐹) = 𝐿𝑐𝑒(𝐹(𝑥)/𝜏, 𝑦) + 𝐿𝑐𝑒(𝐹(𝑥𝑎𝑑𝑣), 𝑦), (12)\nwhere 𝐹is a combination of encoder and class prototypes.\nOur preliminary results are listed in Table III. We can clearly\nobserve that model robustness increases as the temperature\nincreases with a slight trade-off with clean accuracy, which\n\n\nconfirms the possibility of combining the temperature control\nmethod with adversarial training. While further extension\nto other adversarial training methods is possible, it remains\na complex problem for most adversarial training involves\ncomplex loss functions that may introduce terms other than\nthe Cross-Entropy function. Also, balancing the vanilla loss\nterm and adversarial loss term largely relies on empirical\nexperiments. Therefore, further exploration of fitting this into\nother adversarial training methods falls beyond the scope of\nthis paper.\nVI. CONCLUSION & LIMITATION\nIn this paper, we investigate the under-explored property\nof temperature scaling with the softmax function on image\nclassification tasks. By performing gradient analysis with\nthe Cross-Entropy classification loss and executing different\nempirical experiments, we show that temperature scaling can be\na significant factor in model performance. Further experiments\nreveal applying high temperatures during training introduces\nenormous robustness against gradient-based untargeted adver-\nsarial attacks. We hope our work raises the interest of other\nresearchers to utilize the simple temperature scaling in the\ncommon Cross-Entropy loss.\nOne limitation of this study was that we didn’t report an\nexplicit algorithm to set the best temperature values. We\nwill work on this in our future work. One takehome note,\nas a hyperparameter, the tuning cost of the tempeerature is\nlow as a wide range of temperatures (30 to 70) can provide\nimprovements to the model.\nREFERENCES\n[1] Agarwala, A., Pennington, J., Dauphin, Y.N., Schoenholz, S.S.: Temper-\nature check: theory and practice for training models with softmax-cross-\nentropy losses. CoRR abs/2010.07344 (2020)\n[2] Carlini, N., Wagner, D.: Towards evaluating the robustness of neural\nnetworks. In: 2017 ieee symposium on security and privacy (sp). pp.\n39–57. IEEE (2017)\n[3] Croce, F., Hein, M.: Minimally distorted adversarial examples with a\nfast adaptive boundary attack. In: III, H.D., Singh, A. (eds.) Proceedings\nof the 37th International Conference on Machine Learning. Proceedings\nof Machine Learning Research, vol. 119, pp. 2196–2205. PMLR (13–18\nJul 2020)\n[4] Croce, F., Hein, M.: Reliable evaluation of adversarial robustness with an\nensemble of diverse parameter-free attacks. In: III, H.D., Singh, A. (eds.)\nProceedings of the 37th International Conference on Machine Learning.\nProceedings of Machine Learning Research, vol. 119, pp. 2206–2216.\nPMLR (13–18 Jul 2020)\n[5] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training\nof deep bidirectional transformers for language understanding. CoRR\nabs/1810.04805 (2018)\n[6] Engstrom, L., Ilyas, A., Athalye, A.: Evaluating and understanding the\nrobustness of adversarial logit pairing. arXiv preprint arXiv:1807.10272\n(2018)\n[7] Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern\nneural networks. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th\nInternational Conference on Machine Learning. Proceedings of Machine\nLearning Research, vol. 70, pp. 1321–1330. PMLR (06–11 Aug 2017)\n[8] Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural\nnetwork (2015)\n[9] Hou, P., Han, J., , Li, X.: Improving adversarial robustness with self-\npaced hard-class pair reweighting. In: Proceedings of the Thirty-Seventh\nAAAI Conference on Artificial Intelligence (2023)\n[10] Kanai, S., Yamada, M., Yamaguchi, S., Takahashi, H., Ida, Y.: Con-\nstraining logits by bounded function for adversarial robustness. In: 2021\nInternational Joint Conference on Neural Networks (IJCNN). pp. 1–8.\nIEEE (2021)\n[11] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features\nfrom tiny images (2009)\n[12] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with\ndeep convolutional neural networks. In: Pereira, F., Burges, C., Bottou,\nL., Weinberger, K. (eds.) Advances in Neural Information Processing\nSystems. vol. 25. Curran Associates, Inc. (2012)\n[13] Kull, M., Perello Nieto, M., Kängsepp, M., Silva Filho, T., Song, H.,\nFlach, P.: Beyond temperature scaling: Obtaining well-calibrated multi-\nclass probabilities with dirichlet calibration. In: Wallach, H., Larochelle,\nH., Beygelzimer, A., d'Alché-Buc, F., Fox, E., Garnett, R. (eds.) Advances\nin Neural Information Processing Systems. vol. 32. Curran Associates,\nInc. (2019)\n[14] Kumar, A., Sarawagi, S., Jain, U.: Trainable calibration measures for\nneural networks from kernel mean embeddings. In: Dy, J., Krause, A.\n(eds.) Proceedings of the 35th International Conference on Machine\nLearning. Proceedings of Machine Learning Research, vol. 80, pp. 2805–\n2814. PMLR (10–15 Jul 2018)\n[15] Lakshminarayanan, B., Pritzel, A., Blundell, C.: Simple and scalable\npredictive uncertainty estimation using deep ensembles. In: Guyon, I.,\nLuxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,\nGarnett, R. (eds.) Advances in Neural Information Processing Systems.\nvol. 30. Curran Associates, Inc. (2017)\n[16] Le, Y., Yang, X.: Tiny imagenet visual recognition challenge. CS 231N\n7(7), 3 (2015)\n[17] Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of\nmachine learning research 9(11) (2008)\n[18] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards\ndeep learning models resistant to adversarial attacks. In: International\nConference on Learning Representations (2018)\n[19] Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai, X., Houlsby,\nN., Tran, D., Lucic, M.: Revisiting the calibration of modern neural\nnetworks. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P.,\nVaughan, J.W. (eds.) Advances in Neural Information Processing Systems.\nvol. 34, pp. 15682–15694. Curran Associates, Inc. (2021)\n[20] van den Oord, A., Li, Y., Vinyals, O.: Representation learning with\ncontrastive predictive coding. CoRR abs/1807.03748 (2018)\n[21] Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., Hinton, G.: Regulariz-\ning neural networks by penalizing confident output distributions (2017),\nhttps://openreview.net/forum?id=HkCjNI5ex\n[22] Prach, B., Lampert, C.H.: Almost-orthogonal layers for efficient general-\npurpose lipschitz networks. In: European Conference on Computer Vision.\npp. 350–365. Springer (2022)\n[23] Shafahi, A., Ghiasi, A., Huang, F., Goldstein, T.: Label smoothing and\nlogit squeezing: A replacement for adversarial training? (2019)\n[24] Wang, F., Liu, H.: Understanding the behaviour of contrastive loss. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR). pp. 2495–2504 (June 2021)\n[25] Wang, T., Isola, P.: Understanding contrastive representation learning\nthrough alignment and uniformity on the hypersphere. In: III, H.D.,\nSingh, A. (eds.) Proceedings of the 37th International Conference on\nMachine Learning. Proceedings of Machine Learning Research, vol. 119,\npp. 9929–9939. PMLR (13–18 Jul 2020)\n\n\n[26] Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning\nvia non-parametric instance discrimination. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (June\n2018)\n[27] Zhao, H., Qi, X., Shen, X., Shi, J., Jia, J.: Icnet for real-time semantic\nsegmentation on high-resolution images. In: Proceedings of the European\nConference on Computer Vision (ECCV) (September 2018)\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20604v1.pdf",
    "total_pages": 9,
    "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
    "authors": [
      "Hao Xuan",
      "Bokai Yang",
      "Xingyu Li"
    ],
    "abstract": "The softmax function is a fundamental component in deep learning. This study\ndelves into the often-overlooked parameter within the softmax function, known\nas \"temperature,\" providing novel insights into the practical and theoretical\naspects of temperature scaling for image classification. Our empirical studies,\nadopting convolutional neural networks and transformers on multiple benchmark\ndatasets, reveal that moderate temperatures generally introduce better overall\nperformance. Through extensive experiments and rigorous theoretical analysis,\nwe explore the role of temperature scaling in model training and unveil that\ntemperature not only influences learning step size but also shapes the model's\noptimization direction. Moreover, for the first time, we discover a surprising\nbenefit of elevated temperatures: enhanced model robustness against common\ncorruption, natural perturbation, and non-targeted adversarial attacks like\nProjected Gradient Descent. We extend our discoveries to adversarial training,\ndemonstrating that, compared to the standard softmax function with the default\ntemperature value, higher temperatures have the potential to enhance\nadversarial training. The insights of this work open new avenues for improving\nmodel performance and security in deep learning applications.",
    "published_date": "2025-02-28",
    "source": "arxiv"
  }
}