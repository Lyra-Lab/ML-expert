{
  "id": "arxiv_2502.20134v1",
  "text": "Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware\nConcept Bottleneck Models\nItay Benou\nBen-Gurion University of the Negev\nbenoui@post.bgu.ac.il\nTammy Riklin-Raviv\nBen-Gurion University of the Negev\nrrtammy@bgu.ac.il\nFigure 1. Concept maps generated by our SALF-CBM. Inspired by human visual interpretation, our method first decomposes input\nimages into spatially-localized structures, associated with familiar concepts, independent of a specific task. Explainability of task-specific\noutputs is obtained by training a final task layer on-top of these maps.\nAbstract\nModern deep neural networks have now reached human-\nlevel performance across a variety of tasks. However, un-\nlike humans they lack the ability to explain their decisions\nby showing where and telling what concepts guided them.\nIn this work, we present a unified framework for transform-\ning any vision neural network into a spatially and con-\nceptually interpretable model.\nWe introduce a spatially-\naware concept bottleneck layer that projects “black-box”\nfeatures of pre-trained backbone models into interpretable\nconcept maps, without requiring human labels. By train-\ning a classification layer over this bottleneck, we obtain\na self-explaining model that articulates which concepts\nmost influenced its prediction, along with heatmaps that\nground them in the input image.\nAccordingly, we name\nthis method “Spatially-Aware and Label-Free Concept Bot-\ntleneck Model” (SALF-CBM). Our results show that the\nproposed SALF-CBM: (1) Outperforms non-spatial CBM\nmethods, as well as the original backbone, on a variety\nof classification tasks; (2) Produces high-quality spatial\nexplanations, outperforming widely used heatmap-based\nmethods on a zero-shot segmentation task; (3) Facilitates\nmodel exploration and debugging, enabling users to query\nspecific image regions and refine the model’s decisions by\nlocally editing its concept maps.\n1. Introduction\nHumans often rationalize visually-based assessments or\nconclusions by describing what they have seen and where\nthey have seen it, using both semantic concepts and their\narXiv:2502.20134v1  [cs.CV]  27 Feb 2025\n\n\nspatial locations. For example, an image of a dog wearing\nglasses and a hat, as shown at the top of Figure 1, is likely\nto be interpreted as playful or funny due to the unexpected\nspatial composition of concepts. Notably, this mechanism\noperates independently of a specific task; even when look-\ning for a dog in the bottom image of Figure 1, one may\nnotice the tennis ball in its mouth and the pot next to it.\nSimilarly,\nthe ability to explain AI models using\nspatially-aware concepts is crucial for elucidating their\ndecision-making processes. Such an approach enables the\nintroduction of quality control mechanisms, i.e., under-\nstanding the underlying causes of a model’s behavior and\nadjusting it when necessary. These capabilities are essen-\ntial for ensuring the safe and transparent integration of deep\nneural networks into critical fields like medical imaging and\nautonomous driving, as required by the AI Act recently\npassed by the European Parliament [1].\nMost current explainable AI (XAI) methods, however,\nprovide either spatial or concept-based explanations. Spa-\ntial approaches, generally referred to as attribution meth-\nods, produce heatmaps that highlight the image regions\nmost contributing to the model’s output. These heatmaps\nare generated either by propagating gradients through the\nmodel with respect to its input [21, 22, 24–27], or by us-\ning attribution-propagation methods [2, 4, 5, 7, 18, 28, 32]\nthat distribute “relevance” (i.e., the contribution of a neu-\nron to the output) backwards through the network, layer by\nlayer. While these methods can visualize the model’s spa-\ntial attention, in the absence of semantic descriptions, their\noutput can be ambiguous [8, 14].\nConcept Bottleneck Models (CBMs) [15, 16, 19, 31, 35],\non the other hand, are an increasingly popular method for\nobtaining concept-based explanations.\nUnlike attribution\nmethods, CBMs provide ante-hoc explanations—i.e., their\nexplainability mechanism is embedded into the model itself.\nCurrent CBMs work by introducing a non-spatial bottleneck\nlayer that maps model features to an interpretable concept\nspace, followed by training a final output layer over these\nconcepts. This design ensures that CBMs are highly inter-\npretable, as their predictions are directly based on the con-\ncepts used to explain them. However, existing CBMs pro-\nvide global concept-based explanations without localizing\nthem in the image. Moreover, the interpretable bottleneck\nlayer often comes at the expense of the final task accuracy,\nwhich limits their applicability.\nIn this work, we present a spatially-aware CBM that\ncombines concept-based explanations with the ability to\nvisually ground them in the input image.\nIn contrast to\ntraditional CBMs, we preserve the spatial information of\nfeatures and project them into a spatial concept space.\nThis is achieved in a label-free manner by leveraging the\ncapability of CLIP [20] to produce local image embed-\ndings using visual prompts [23]. Accordingly, we name\nour method “spatially-aware and label-free CBM” (SALF-\nCBM). The main contributions of our work are as follows:\n(1) Novel unified framework: we present the first label-\nfree CBM that provides both concept-based (global) and\nheatmap-based (local) explanations. (2) Classification re-\nsults: SALF-CBM outperforms non-spatial CBMs on sev-\neral classification tasks, and can even achieve better clas-\nsification results than the original (non-CBM) backbone\nmodel. (3) Heatmap quality: our method produces high-\nquality heatmaps that can be used for zero-shot segmen-\ntation. We demonstrate their advantage over widely-used\nheatmap-based methods in both qualitative and quantita-\ntive evaluations. (4) Explain anything: SALF-CBM facili-\ntates interactive model exploration and debugging, enabling\nusers to inquire about concepts identified in specific image\nregions, and to adjust the model’s final prediction by locally\nrefining its concept maps. (5) Applicability: Our method is\nmodel-agnostic and can be applied to both CNNs and trans-\nformer architectures, while not introducing any additional\nlearnable parameters compared to non-spatial CBMs.\n2. Related Work\nXAI methods for computer vision can be categorized by two\naxes: local (heatmap-based) vs. global (concept-based) ap-\nproaches, and post-hoc vs. ante-hoc explanations. In this\nsection, we overview existing methods along these lines.\nHeatmap-based explainability. This refers to a family of\npost-hoc explainability techniques, often called attribution\nmethods, that visualize the parts of the input image that con-\ntribute most to the model’s output. Gradient-based methods\ngenerate explainable heatmaps by backpropagating gradi-\nents with respect to the input of each layer. Some of these\nmethods, such as FullGrad [26], are class-agnostic as they\nproduce roughly identical results regardless of the output\nclass [25, 27], while others, such as GradCAM [21], gen-\nerate class-dependent heatmaps [6, 24]. This property is\nessential when the true class is ambiguous. While widely\nused, their main drawback is high sensitivity to gradient\nnoise, which may render their outcomes impractical [3].\nTo address this issue, some Class Activation Maps (CAM)\nmethods [34], such as ScoreCAM [29], produce gradient-\nfree explanation maps.\nAttribution propagation methods decompose the output\nof a model into the contributions of its layers by propagating\n“relevance” in a recursive manner, without exclusively re-\nlying on gradients. Common attribution propagation meth-\nods, such as Layer-wise Relevance Propagation (LRP) [5],\nare primarily applicable to Convolutional Neural Networks\n(CNNs) [18, 22, 32]. Later approaches have been adapted\nto accommodate vision transformers (ViTs) [9], exploiting\ntheir built-in self-attention mechanism [2, 7, 20, 28]. We\nnote that, unlike our SALF-CBM, both gradient-based and\nattribution propagation methods do not provide concept-\n\n\nbased explanations. Additionally, since these are post-hoc\ntechniques, they do not enable test-time user intervention.\nConcept-based explainability. An alternative way of ex-\nplaining vision models is by using human-interpretable con-\ncepts. Various methods provide such explanations in a post-\nhoc manner. For example, Testing Concept Activation Vec-\ntors (TCAV) [13] measures the importance of user-defined\nconcepts to the model’s prediction by training a linear clas-\nsifier to distinguish between concepts in their activation\nspace. However, this requires labeling images with their\ncorresponding concepts in advance. ACE [11] extends this\nidea by applying multi-resolution segmentation to images\nfrom the same class, followed by clustering similar seg-\nments into concepts to compute their TCAV scores. Sim-\nilarly, Invertible Concept Embeddings (ICE) [33] and Con-\ncept Recursive Activation Factorization (CRAFT) [10] pro-\nvide concept-based explanations using matrix factorization\nof feature maps. CRAFT also generates attribution maps\nthat localize concepts in the input image. These methods,\nhowever, are mostly applicable to CNN architectures [11],\nwhich use non-negative activations [10, 33], and therefore\ncannot be directly applied to other types of models. Addi-\ntionally, since they provide post-hoc explanations, they do\nnot enable test-time user intervention.\nIn contrast, Concept-Bottleneck Models (CBMs) is a\nfamily of ante-hoc interpretable models whose explainabil-\nity mechanism is an integral part of the model itself. CBMs\noperate by introducing a concept-bottleneck layer into pre-\ntrained models, before the final prediction layer. The goal\nof this bottleneck is to project features into an interpretable\nconcept space, where each neuron corresponds to a single\nconcept.\nUnlike post-hoc methods, the output of CBMs\nis directly based on interpretable concepts, making them\neasily explainable and allowing user intervention by mod-\nifying concept neurons activations. In the original CBM\nwork [15], the concept bottleneck layer was trained using\nmanual concept annotations, limiting its ability to scale to\nlarge datasets. Recently, Post-Hoc CBM (P-CBM) [31] and\nLabel-Free CBM (LF-CBM) [19] addressed this issue by\nleveraging CLIP to assign concept scores for training im-\nages, thus not requiring concept annotations. LF-CBM also\npresented an automatic process for creating a list of task-\nrelevant concepts using GPT-3. While showing good in-\nterpretability results, both P-CBM and LF-CBM present\na performance drop on the final classification task com-\npared to the original (non-CBM) model. Additionally, un-\nlike our SALF-CBM, these methods are limited to global\nconcept explanations, and are unable to localize these con-\ncepts within the image.\n3. Method\nGiven a pre-trained backbone model, we transform it into\nan explainable SALF-CBM as illustrated in Figure 2a:\nStep 1: Automatically generate a list of task-relevant vi-\nsual concepts; Step 2:\nUsing CLIP, compute a spatial\nconcept similarity matrix that quantifies what concepts ap-\npear at different locations in the training images; Step 3:\nTrain a spatially-aware Concept Bottleneck Layer (CBL)\nthat projects the backbone’s “black-box” features into inter-\npretable concept maps. Step 4: Train a sparse linear layer\nover the pooled concept maps to obtain the model’s final\nprediction. We describe each step in the following sections.\n3.1. Concept list generation\nLet X denote an image classification dataset with N train-\ning images {x1, . . . , xN} and L possible classes. We aim\nto generate a list of visual concepts T that is most relevant\nto the target classes, without relying on human experts. For\nthis purpose, we follow the automatic procedure described\nin [19]. First, an initial concept list is obtained by prompt-\ning GPT as follows: “List the most important features for\nrecognizing something as a {class}”; “List the things most\ncommonly seen around a {class}”; and “Give superclasses\nfor the word {class}”, for each class in the dataset. Con-\ncepts that are either too long, too similar to one of the\nclasses or to another concept, or do not appear in the train-\ning data - are then discarded. The resulting filtered list of\nM concepts is denoted by T = {t1, . . . , tM}. See [19] for\nfull details.\n3.2. Local image-concept similarities\nVision-language models such as CLIP have been widely\nused for obtaining global image descriptions, as in non-\nspatial CBMs [19, 31]. Here, we aim to expand the CBM\napproach and use visual concepts to locally describe differ-\nent image regions. Inspired by [23], we leverage CLIP’s\nvisual prompting property, which enables it to focus on a\nspecific image region while preserving global context, by\ndrawing a red circle around that region. We apply this prop-\nerty to our training set as illustrated in Figure 2b. Formally,\nlet xn ∈X denote an image in the training set with spatial\ndimensions H × W. We create a uniform grid of ˜H × ˜W\nlocations in the image with integer strides dH and dW , i.e.,\ndH = ⌊\nH\n˜\nH+1⌋and dW = ⌊\nW\n˜\nW +1⌋. A set of ˜H· ˜W augmented\nimages is then obtained by drawing a red circle with radius\nr around each location in the grid. We denote by x(h,w)\nn\nthe image xn with a red circle located at (h, w).\nNext,\nwe compute a local similarity score between a visual con-\ncept tm ∈T and the image at location (h, w) as follows:\nP[n, m, h, w] =\nIn·Tm\n∥In∥∥¯Tm∥, where In = EI(x(h,w)\nn\n) and\nTm = ET (tm) denote the CLIP embeddings of the aug-\nmented image and the concept, respectively. As demon-\nstrated in [23], this score indicates the degree of compati-\nbility between the concept tm and the image region asso-\nciated with the red circle. By iterating through all spatial\n\n\n(a) Method overview.\n(b) Local image-concept similarities.\nFigure 2. Left: Given a pre-trained backbone model, we: (i) Generate task-relevant concepts; (ii) Describe training images using local\nimage-concept similarities; (iii) Train a spatially-aware concept bottleneck to project features into interpretable concept maps; (iv) Train a\nsparse classification layer over these maps. Right: Computing local image-concept similarities using visual prompting with CLIP.\nlocations (h, w) in the grid, we obtain a concept similar-\nity map for the entire image xn, as shown in Figure 2b. A\nspatial concept similarity matrix P is constructed by calcu-\nlating the local similarities for all images xn ∈X and all\nconcepts tm ∈T . This matrix is later used for learning a\nspatially-aware concept space. We note that the grid reso-\nlution ˜H × ˜W and the circle radius r are hyper-parameters,\nwhere ˜H and ˜W control the coarseness of the concept sim-\nilarity map, and r defines the receptive field around each\nlocation. We optimize these hyper-parameters per-dataset\n(see Appendix A.2).\n3.3. Spatially-aware concept-bottleneck layer\nWe aim to learn a bottleneck layer g that linearly projects\n“black-box” feature maps f(x) of a pre-trained backbone\nmodel into interpretable concept maps. Rather than spa-\ntially pooling the backbone’s features as in conventional\nCBMs, we retain their spatial information, and resize them\nto fit the grid’s dimensions ( ˜H × ˜W) using a bilinear inter-\npolation. We then use a single 1 × 1 convolution layer with\nM output channels to produce the desired concept maps,\ni.e., c(x) = g(f(x)) ∈RM× ˜\nH× ˜\nW . We denote the full\nlist of concept maps for all training images xn ∈X by\nC[n, m, h, w] = [c(x1), . . . , c(xN)]. In order to obtain con-\ncept maps that match the image-concept similarities in P,\nwe train our bottleneck layer using an extended version of\nthe cubic cosine similarity loss from [19] as follows:\nLCBL = −\nM\nX\nm=1\nX\nh,w\nsim (q[m, h, w], p[m, h, w])\n(1)\nwhere q[m, h, w] denotes C[:, m, h, w], p[m, h, w] denotes\nP[:, m, h, w] and sim(·, ·) denotes the cubic cosine similar-\nity function sim (q, p) =\n¯q·¯p\n∥¯q∥∥¯p∥. Here, ¯q and ¯p are nor-\nmalized to have zero-mean and raised elementwise to the\npower of three to emphasize strong concept-image matches.\nWe note that while this bottleneck layer is spatially-aware,\nit does not introduce additional learnable parameters com-\npared to non-spatial CBMs [15, 19, 31], which require a\nfully-connected layer.\nFurthermore, our bottleneck layer\naccommodates both CNN and vision transformer architec-\ntures: For CNN backbones, the feature maps are taken as-is,\nwhile for ViTs, the patch tokens are reshaped into their orig-\ninal spatial formation. We also experiment with concatenat-\ning the CLS token to each patch token along the channels\ndimension (see Appendix B.1).\n3.4. Final classification layer\nOnce the concept bottleneck layer is trained, we spatially\npool its output concept maps c(x) to obtain global concept\nactivations c∗(x), each corresponds to a single visual con-\ncept tm. We aim to explain each output class of our model\nusing a small set of interpretable concepts. We therefore\ntrain a sparse linear layer on top of c∗(x) to obtain the final\nclassification scores z = Wc∗+ b and the predicted class\nˆy = arg max(z). Here, W and b denote the classification\nweights and bias term, respectively. This layer is trained in\na fully-supervised manner with the following loss function,\nusing the GLM-SAGA optimizer [30]:\nN\nX\nn=1\nLce (Wc∗(xn) + b, yn) + λR(W)\n(2)\nwhere Lce is the cross-entropy loss, yn is the class label\nof training image xn, λ is the regularization strength and\nR(W) = (1−α) 1\n2∥W∥F +α∥W∥1,1 is the elastic net reg-\nularization term, where ∥W∥F is the Forbenius norm and\n∥W∥1,1 is the elementwise matrix norm.\n\n\n3.5. Test-time explainability\nOne of the main contributions of SALF-CBMs is their abil-\nity to provide model explanations at different levels of gran-\nularity, as described below:\nGlobal decision rules. As described in Section 3.4, SALF-\nCBMs produce their final prediction as a linear combination\nof a sparse set of concept activations. Therefore, one can\ngain an intuitive understanding of the model’s decision rules\nsimply by examining which concepts tm ∈T are connected\nto a specific class l ∈{1, . . . , L} by non-zero weights. For\ninstance, in Figure 3, we show Sankey diagrams visualizing\nthe class weights of a SALF-CBM trained on ImageNet, for\ntwo different classes that may fit the image: “crate” and “toy\nstore”.\nConcept-based explanations (“tell what”). We aim to ex-\nplain individual model decisions by evaluating the contribu-\ntion of a visual concept tm ∈T to the the model’s output\nˆy on a given test image x. This is achieved by computing\na contribution score S(x, m, ˆy = l) = W[m, l]c∗(x)[m],\nwhere m is the concept index, l ∈{1, . . . , L} is the index\nof the predicted class and c∗(x)[m] is the global concept\nactivation, normalized by its mean and standard deviation\non the training data. Since W[:, l] is sparse, the majority of\ncontribution scores are zero, so the model’s prediction can\nbe explained by a small set of k concepts whose absolute\ncontribution scores are the highest. In Figure 3, we illus-\ntrate the top-3 concepts with the highest contribution scores\nfor the “toy store” and “crate” classes.\nSpatial explanations (“show where”).\nIn addition to\nglobal concept-based explanations, our method produces\nheatmaps which highlight the location of each concept in\nthe input image.\nSpecifically, given the top-k contribut-\ning concepts, we upsample their associated concept maps\nc(x)[m] to the input image dimensions, using a bilinear in-\nterpolation. Examples of heatmaps associated with the most\ncontributing concepts for two different output classes are\npresented in Figure 3. We note that the resolution of the\nheatmaps can be controlled by adjusting the density of the\nvisual prompting grid, as discussed in Section 3.2.\n3.6. Model exploration and debugging\nWe introduce two interactive features that help users explore\ntheir model’s decision-making process and enable the de-\nbugging of failure cases in an intuitive manner.\nExplain anything.\nInspired by the Segment-Anything\nModel (SAM) [17], this feature allows users to actively\n“prompt” SALF-CBM with inputs such as points, bound-\ning boxes, or free-form masks, to explore what visual con-\ncepts were recognized in the specified region-of-interest\n(ROI). Specifically, given an image x, the computed con-\ncept maps c(x) (upsampled to the image dimensions) and a\nuser-provided ROI in the form of a binary mask I, we com-\npute the aggregated activation of each concept within the\nROI: a(x, m | I) = P I ⊙c(x)[m], where ⊙represents\nelementwise multiplication. By presenting the top-k con-\ncepts with the strongest aggregated activation, we provide\na concise overview of the model’s perception of the ROI.\nWe note that in addition to user-provided ROIs, our method\nsupports segmentation masks from tools such as SAM to\nautomatically produce objects descriptions.\nLocal user intervention. We enable users to intervene in\nthe model’s final prediction by suggesting counterfactual\nexplanations in specific image regions, i.e., “how would the\nmodel’s prediction change if concept A were more/less ac-\ntivated at location B?”. Given an image x, the concept\nmap c(x)[m] of a specific concept tm and the predicted\nclass ˆy, one can locally edit the concept map according to\ntheir judgment and understanding of the task, as follows:\nc(x)[m] ←c(x)[m] + βI, where I is a binary mask of the\nedited region and β is a correction factor that can be ei-\nther positive or negative. By tuning concept activations up\nor down in specific regions and re-running the final clas-\nsification layer, one can observe how the model adjusts its\nprediction ˆy based on the revised concept maps.\n4. Experiments\nWe thoroughly evaluate the different components of our\nmethod.\nIn section 4.1, we test its classification accu-\nracy compared to several baselines, on different large-scale\ndatasets. In section 4.2, we present qualitative and quantita-\ntive evaluations of our SALF-CBM’s heatmaps in compari-\nson to several other heatmap-based methods. In section 4.3,\nwe demonstrate how the proposed Explain-Anything and\nuser intervention features are used to debug model errors.\nAdditional results are provided in the supplementary ma-\nterials, including experiments with a ViT backbone B.1,\nvalidation of concept alignment in the concept-bottleneck\nlayer C, qualitative evaluation of explanations across dif-\nferent datasets D, and additional visualizations of concept\nmaps for challenging images and video sequences E.\n4.1. Classification accuracy\nExperimental setup.\nWe test our method on a diverse\nset of classification datasets: CUB-200 (fine-grained bird-\nspecies classification), Places365 (scene recognition) and\nImageNet.\nWe train a SALF-CBM on each of the three datasets,\nwhere the backbone model is selected according to the\ndataset to allow fair comparisons with competing CBM\nmethods [19, 31]: For CUB-200 we use a ResNet-18 pre-\ntrained on CUB-200, and for both ImageNet and Places365\nwe use a ResNet-50 pre-trained on ImageNet.\nFor each\ndataset, we use the same initial concept list and regulariza-\ntion parameters α and λ as in [19], resulting in 370 concepts\nfor CUB-200, 2544 concepts for Places365 and 4741 con-\ncepts for ImageNet. For computing the spatial concept sim-\n\n\nFigure 3. Test-time explainability. Global decision rules can be inferred by visualizing the sparse class weights. Individual model\ndecisions are explained by concept contribution scores and their associated spatial heatmaps.\nilarity matrix, we use CLIP ViT-B/16 and a visual prompt-\ning grid of 7 × 7 with r = 32 for all experiments.\nIn\nAppendix A.2, we experiment with different settings of the\ngrid parameters.\nResults. Table 1 presents the classification accuracy of our\nSLAF-CBM compared to several other methods: (1) the\nstandard pre-trained backbone model with its original clas-\nsification layer; (2) the standard backbone model with a\nsparse classification layer (reported from [19]); (3) post-\nhoc CBM (P-CBM) [31]; and (4) Label-Free CBM (LF-\nCBM) [19]. We note that in P-CBM [31], they do not re-\nport their results on ImageNet and Places365, and it is un-\nclear how to scale it to those datasets. For fair comparisons,\nresults with sparse and non-sparse classification layers are\nshown separately. We see that when using a sparse final\nlayer, our SALF-CBM outperforms both P-CBM and LF-\nCBM on all three datasets. Notably, our method is the\nbest performing sparse method on the the two larger-\nscale datasets (Places365 and ImageNet), outperforming\neven the original backbone with a sparse final layer. To\ndemonstrate the high-limit potential of our method, we as-\nsess its performance with a non-sparse final layer. Remark-\nably, the non-sparse SALF-CBM achieves better classifica-\ntion results than original (non-sparse) model on both Ima-\ngeNet and Places365, even though its predictions are based\non interpretable concepts.\nThese results indicate that SALF-CBM facilitates model\ninterpretability without compromising performance; in fact,\nit can outperform the original backbone model when using\na comparable final layer (i.e., sparse or non-sparse). We\nalso note that the performance gap between the sparse and\nnon-sparse SALF-CBMs is relatively small (less that 1% on\nImageNet), indicating that our model effectively captures\nthe full span of possible explanations using a sparse set of\nconcepts.\n4.2. Beyond classification: zero-shot segmentation\nExperimental setup.\nWe conduct a quantitative analy-\nsis of the heatmaps generated by our method in a zero-\nshot segmentation task.\nWe follow a standard protocol\nDataset\nModel\nSparse\nCUB-200\nPlaces365\nImageNet\nStandard\nYes\n75.96%\n38.46%\n74.35%\nP-CBM [31]\nYes\n59.60%\nN/A\nN/A\nLF-CBM [19]\nYes\n74.31%\n43.68%\n71.95%\nSALF-CBM\nYes\n74.35%\n46.73%\n75.32%\nStandard\nNo\n76.70%\n48.56%\n76.13%\nSALF-CBM\nNo\n76.21%\n49.38%\n76.26%\nTable 1. Classification accuracy. Our method outperforms P-\nCBM and LF-CBM on all three datasets, and is the highest per-\nforming model on ImageNet and Places365. Results are shown\nseparately for sparse and non-sparse final layers. Best results are\nin bold and 2nd-best are underlined. In Appendix B.1 we present\nSALF-CBM’s classification results with a ViT backbone model.\nfor evaluating heatmap-based explainability methods [7] on\nImageNet-segmentation dataset [12], a subset of the Ima-\ngeNet validation set containing 4,276 images with ground-\ntruth segmentation masks of the class object. In order for\nour concept maps to correspond to ImageNet classes, we\ntrain a SLAF-CBM with a ResNet-50 backbone on Ima-\ngeNet, using a concept list of the form “An image of a\n{class}”, where {class} refers to each of the ImageNet\nclasses. According to [7], the resulting heatmaps are bina-\nrized to obtain a foreground/background segmentation, and\nevaluated with respect to the ground-truth masks based on\nthree metrics: mean average precision (mAP) score, mean\nintersection-over-union (mIOU) and pixel accuracy.\nResults. Table 2 presents the zero-shot segmentation results\nof our method, compared to several widely-used explain-\nability methods: LRP [5], integrated gradients (IG) [27],\nGradCAM [21], GradCAM++ [6], ScoreCAM [29], and\nFullGrad [26].\nNotably, our SALF-CBM achieves the\nbest pixel accuracy and mIOU segmentation scores, and\nthe second best mAP. Specifically, our method demon-\nstrates significant improvements in pixel accuracy and\nmIOU (+3.9% and +2.52% over the 2nd-best method, re-\n\n\nFigure 4. Qualitative heatmaps comparison. Explanation map of each method with respect to the ground-truth class (from top to bottom):\n“Cheeseburger”, “Bottle cap”, “Bell-cote”, “Monarch butterfly” and “Goose”. Results with a ViT backbone are shown in Appendix B.1.\nspectively), indicating that our heatmaps are consistently\nbetter aligned with the ground-truth masks. In Figure 4,\nwe present a qualitative comparison to the baseline meth-\nods, for different images from the ImageNet validation set.\nWe observe that LRP [5] and IG [27] typically produce\nnoisy results, and struggle to accurately localize the class\nobject. GradCAM [21], GradCAM++ [6], ScoreCAM [29]\nand FullGrad [26] manage to highlight the target region,\nbut also include unrelated background areas. Conversely,\nour method generates heatmaps that accurately captures the\nclass object, thus providing more precise explanations.\nMethod\nPixel Acc. ↑\nmIoU ↑\nmAP ↑\nLRP [5]\n69.52%\n36.85%\n69.95%\nIG [27]\n68.49%\n46.59%\n73.46%\nGradCAM [21]\n71.34%\n53.34%\n83.88%\nGradCAM++ [6]\n71.31%\n53.56%\n83.93%\nScoreCAM [29]\n69.56%\n51.44%\n81.78%\nFullGrad [26]\n73.04%\n55.78%\n88.35%\nSALF-CBM\n76.94%\n58.30%\n85.31%\nTable 2.\nZero-shot segmentation results.\nOur SALF-CBM\nachieves the highest mIoU and pixel accuracy, and the second\nhighest mAP. Best results are in bold, 2nd-best are underlined.\n4.3. Model exploration and debugging\nWe first qualitatively validate the proposed Explain-\nAnything feature, described in Section 3.6, on several dif-\nferent images from the SAM dataset [17], including a draw-\ning, indoors scenes and outdoors scenes (see Figure 5).\nFor each image, we prompt SALF-CBM (with a ResNet-\n50 backbone pre-trained on ImageNet) with two different\nROI masks obtained by SAM [17], highlighted in red and\nblue. We show the region-specific concepts identified by the\nmodel next to each image. We see that our SALF-CBM gen-\nerates informative descriptions, that accurately correspond\nto the selected ROIs. For example, in the challenging im-\nage of the child’s drawing (top left), the dress highlighted\nin blue is correctly identified as a fabric-like material, while\nthe grass area (red mask) is described as a field or lawn.\nNext, we demonstrate Explain-Anything’s usefulness in\ndiagnosing classification errors, and facilitating targeted\ncorrections using local user intervention. We present a case\nstudy from the ImageNet validation set, where our model\nmiscalssified a “traffic light” image as a “parking meter”,\nas shown in Figure 6. By applying Explain-Anything to\nthe traffic lights region in the image, we learn that the\nmodel predominantly recognized sign-related concepts in\nthat area. However, its classification layer does not asso-\nciate these concepts with the true “traffic light” class, as ev-\n\n\nFigure 5. Explain Anything. For each image, we prompted SALF-CBM with two different ROI masks produced by SAM [17] (red and\nblue regions). Our method provides accurate concept descriptions for each ROI.\nFigure 6. Model debugging. Using Explain-Anything, we reveal that the model misclassified the image since it mistakenly identified\ntraffic lights as street signs. We correct its prediction by locally editing two concepts maps (“a flashing light” and “the ability to change\ncolor”) in the examined ROI.\nidenced by the class weights visualization. This misalign-\nment between detected concepts and the true class, com-\nbined with the presence of street-related features in the im-\nage, led the model to mistakenly classify the image as a\n“parking meter”. To rectify that, we locally edit the concept\nmaps of two concepts associated with the true class - “a\nflashing light” and “the ability to change color” - within the\nselected ROI. Specifically, we increase their activation there\nby a correction factor of β = 1. As illustrated in the figure,\nthis mild adjustment promoted these concepts to the top-5\nmost activated concepts in the ROI, subsequently adjusting\nthe model’s output to the correct class.\n5. Conclusions\nIn this work, we presented SALF-CBM, a novel framework\nfor transforming any vision neural network into an explain-\nable model that provides both concept-based and spatial ex-\nplanations for its predictions. We showed that SALF-CBM\nenhances model interpretability without compromising per-\nformance, outperforming both existing CBMs and the orig-\ninal model across several classification tasks. We demon-\nstrated that it produces high-quality spatial explanations,\nachieving better zero-shot segmentation results compared\nto widely used heatmap-based explainability methods.\nAdditionally, we introduced interactive capabilities for\nmodel exploration and debugging, demonstrating their\neffectiveness in diagnosing and correcting model errors.\nWe believe that such features are particularly valuable\nfor high-stakes applications like medical imaging and\nautonomous driving. By providing expert practitioners with\nintuitive tools to understand and adjust model decisions,\nour approach can boost confidence and support safer\ndeployment in these critical fields.\nLooking ahead, as\nnew VLMs are developed across various domains, our\nfindings could help inform the design of more powerful\ninterpretability tools for a broad spectrum of AI applica-\ntions. We plan to explore these directions in future work.\n\n\nReferences\n[1] Madiaga: Artificial intelligence act (2023). http://www.\neuroparl.europa.eu/RegData/etudes/BRIE/\n2021/698792/EPRS_BRI(2021)698792_EN.pdf.\n2\n[2] Samira Abnar and Willem Zuidema.\nQuantifying atten-\ntion flow in transformers. arXiv preprint arXiv:2005.00928,\n2020. 2\n[3] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Good-\nfellow, Moritz Hardt, and Been Kim.\nSanity checks for\nsaliency maps. Advances in neural information processing\nsystems, 31, 2018. 2\n[4] Sebastian Bach, Alexander Binder, Grégoire Montavon,\nFrederick Klauschen, Klaus-Robert Müller, and Wojciech\nSamek. On pixel-wise explanations for non-linear classifier\ndecisions by layer-wise relevance propagation. PloS one, 10\n(7):e0130140, 2015. 2\n[5] Alexander Binder,\nGrégoire Montavon,\nSebastian La-\npuschkin,\nKlaus-Robert Müller,\nand Wojciech Samek.\nLayer-wise relevance propagation for neural networks with\nlocal renormalization layers. In Artificial Neural Networks\nand Machine Learning–ICANN 2016: 25th International\nConference on Artificial Neural Networks, Barcelona, Spain,\nSeptember 6-9, 2016, Proceedings, Part II 25, pages 63–71.\nSpringer, 2016. 2, 6, 7\n[6] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader,\nand Vineeth N Balasubramanian.\nGrad-cam++: General-\nized gradient-based visual explanations for deep convolu-\ntional networks. In 2018 IEEE winter conference on appli-\ncations of computer vision (WACV), pages 839–847. IEEE,\n2018. 2, 6, 7\n[7] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-\npretability beyond attention visualization. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 782–791, 2021. 2, 6\n[8] Julien Colin, Thomas Fel, Rémi Cadène, and Thomas Serre.\nWhat i cannot predict, i do not understand:\nA human-\ncentered evaluation framework for explainability methods.\nAdvances in neural information processing systems, 35:\n2832–2845, 2022. 2\n[9] Alexey Dosovitskiy.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2\n[10] Thomas Fel, Agustin Picard, Louis Bethune, Thibaut\nBoissin, David Vigouroux, Julien Colin, Rémi Cadène, and\nThomas Serre. Craft: Concept recursive activation factoriza-\ntion for explainability. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n2711–2721, 2023. 3\n[11] Amirata Ghorbani, James Wexler, James Y Zou, and Been\nKim. Towards automatic concept-based explanations. Ad-\nvances in neural information processing systems, 32, 2019.\n3\n[12] Matthieu Guillaumin, Daniel Küttel, and Vittorio Ferrari.\nImagenet auto-annotation with segmentation propagation.\nInternational Journal of Computer Vision, 110:328–348,\n2014. 6\n[13] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai,\nJames Wexler, Fernanda Viegas, et al. Interpretability be-\nyond feature attribution: Quantitative testing with concept\nactivation vectors (tcav). In International conference on ma-\nchine learning, pages 2668–2677. PMLR, 2018. 3\n[14] Sunnie SY Kim, Nicole Meister, Vikram V Ramaswamy,\nRuth Fong, and Olga Russakovsky.\nHive: Evaluating the\nhuman interpretability of visual explanations. In European\nConference on Computer Vision, pages 280–298. Springer,\n2022. 2\n[15] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen\nMussmann, Emma Pierson, Been Kim, and Percy Liang.\nConcept bottleneck models. In International Conference on\nMachine Learning (ICML), pages 5338–5348. PMLR, 2020.\n2, 3, 4\n[16] Max Losch, Mario Fritz, and Bernt Schiele. Interpretability\nbeyond classification output: Semantic bottleneck networks.\narXiv preprint arXiv:1907.10882, 2019. 2\n[17] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and\nBo Wang.\nSegment anything in medical images.\nNature\nCommunications, 15(1):654, 2024. 5, 7, 8\n[18] Grégoire Montavon,\nSebastian Lapuschkin,\nAlexander\nBinder, Wojciech Samek, and Klaus-Robert Müller.\nEx-\nplaining nonlinear classification decisions with deep taylor\ndecomposition. Pattern recognition, 65:211–222, 2017. 2\n[19] Tuomas Oikarinen, Subhro Das, Lam M Nguyen, and Tsui-\nWei Weng. Label-free concept bottleneck models. In In-\nternational Conference on Learning Representations (ICLR),\n2023. 2, 3, 4, 5, 6\n[20] A. Radford, J.W. Kim, C. Hallacy, A. Ramesh, et al. Learn-\ning transferable visual models from natural language super-\nvision.\nIn International conference on machine learning\n(ICML), pages 8748–8763, 2021. 2\n[21] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam:\nVisual explanations from deep networks via\ngradient-based localization. In Proceedings of the IEEE in-\nternational conference on computer vision, pages 618–626,\n2017. 2, 6, 7\n[22] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.\nLearning important features through propagating activation\ndifferences. In International conference on machine learn-\ning, pages 3145–3153. PMlR, 2017. 2\n[23] A. Shtedritski, C. Rupprecht, and A. Vedaldi. What does\nclip know about a red circle? visual prompt engineering for\nVLMs, 2023. 2, 3\n[24] Karen Simonyan. Deep inside convolutional networks: Visu-\nalising image classification models and saliency maps. arXiv\npreprint arXiv:1312.6034, 2013. 2\n[25] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas,\nand Martin Wattenberg. SmoothGrad: removing noise by\nadding noise, 2017. 2\n[26] Suraj Srinivas and François Fleuret. Full-gradient represen-\ntation for neural network visualization. Advances in neural\ninformation processing systems, 32, 2019. 2, 6, 7\n[27] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. In International conference on\nmachine learning, pages 3319–3328. PMLR, 2017. 2, 6, 7\n\n\n[28] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,\nand Ivan Titov. Analyzing multi-head self-attention: Spe-\ncialized heads do the heavy lifting, the rest can be pruned.\narXiv preprint arXiv:1905.09418, 2019. 2\n[29] Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian\nZhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam:\nScore-weighted visual explanations for convolutional neural\nnetworks. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition workshops, pages\n24–25, 2020. 2, 6, 7\n[30] Eric Wong, Shibani Santurkar, and Aleksander M ˛adry.\nLeveraging sparse linear layers for debuggable deep net-\nworks. arXiv preprint arXiv:2105.04857, 2021. 4\n[31] Mert\nYuksekgonul,\nMaggie\nWang,\nand\nJames\nZou.\nPost-hoc concept bottleneck models.\narXiv preprint\narXiv:2205.15480, 2022. 2, 3, 4, 5, 6\n[32] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan\nBrandt, Xiaohui Shen, and Stan Sclaroff. Top-down neu-\nral attention by excitation backprop. International Journal\nof Computer Vision, 126(10):1084–1102, 2018. 2\n[33] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A\nEhinger, and Benjamin IP Rubinstein. Invertible concept-\nbased explanations for cnn models with non-negative con-\ncept activation vectors. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, pages 11682–11690, 2021. 3\n[34] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimina-\ntive localization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2921–2929,\n2016. 2\n[35] Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba.\nInterpretable basis decomposition for visual explanation. In\nProceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 119–134, 2018. 2\n\n\nAppendix\nA. Visual prompting details\nA.1. Algorithm\nThe pseudo-algorithm for computing local image-concept similarities using visual prompts is provided below. The operation\nof drawing a red circle within training image xn at location (h, w) with radius r is denoted by Circle(xn; (h, w, r)). We use\ncircles with a line width of 2 pixels.\nAlgorithm 1 Local image-concept similarities\nInput: (i) training images {xn}N\nn=1 ∈R3×H×W ; (ii) concept list {tm}M\nm=1; (iii) CLIP’s image encoder EI and text\nencoder ET ; (iv) circle radius r and grid dimensions ( ˜H, ˜W).\nOutput: Spatial concept similarity matrix P.\nInitialize: P ←0.\ndH ←⌊H/( ˜H + 1)⌋, dW ←⌊W/( ˜W + 1)⌋\nfor n ←0 to N −1 do\n▷iterate over images\nfor h ←r to ˜H −r by dH do\nfor w ←r to ˜W −r by dW do\n▷iterate over grid locations\nx(h,w)\nn\n←Circle(xn; (h, w, r))\nIn ←EI(x(h,w)\nn\n)\nfor m ←0 to M −1 do\n▷iterate over concepts\nTm ←ET (tm)\nP[n, m, h, w] ←\nIn·Tm\n∥In∥∥Tm∥\nend for\nend for\nend for\nend for\nreturn P\nA.2. Choosing the grid parameters\nWe experiment with different settings of the visual prompting grid. In Table 3, we present the classification accuracy obtained\nusing different values for the circle radius r and the grid size ˜H × ˜W, on the ImageNet (left) and CUB-200 (right) datasets.\nIn both cases, the best performance is achieved with r = 32 and a grid size of 7 × 7. We use the same values for Places365.\nGrid size\nr = 27\nr = 32\nr = 37\n5 × 5\n74.17%\n74.37%\n75.01%\n7 × 7\n74.67%\n75.32%\n75.31%\n9 × 9\n75.06%\n75.22%\n75.22%\n(a) Results on ImageNet.\nGrid size\nr = 27\nr = 32\nr = 37\n5 × 5\n73.36%\n73.59%\n73.80%\n7 × 7\n73.42%\n74.35%\n74.01%\n9 × 9\n73.83%\n74.12%\n73.93%\n(b) Results on CUB-200.\nTable 3. Classification accuracy for different settings of the visual prompting grid, on the ImageNet (left) and CUB-200 (right) datasets.\n\n\nB. Results with ViT backbone\nB.1. Classification accuracy\nWe report the classification results of our SALF-CBM with a ViT-B/16 backbone pre-trained on ImageNet. We experiment\nwith two variations: (1) Only patch tokens are used, reshaped into their original spatial formation; (2) Both patch tokens\nand the CLS token are used, by reshaping the patch tokens into their original spatial formation and concatenating them\nwith the CLS token along the channels dimension. For each variation, the model is trained with both sparse and non-sparse\nclassification layers. We compare its results to the corresponding standard model—i.e., using the same backbone model\nwithout a bottleneck layer and with a comparable classification layer (sparse or non-sparse). Results are shown in Figure 7.\nWhen using a sparse final layer, our model significantly outperforms the corresponding standard model for both backbone\nversions. With a non-sparse final layer, our model’s performance is comparable to the standard model when using the CLS\ntoken, and is slightly lower when the CLS token is excluded.\nFigure 7. ImageNet classification results with ViT-B/16 backbone, when using a sparse final layer (left) and a dense final layer (right).\n\n\nB.2. Spatial heatmaps\nWe present qualitative results of the heatmaps generated by our method when using a ViT-B/16 backbone pre-trained on\nImageNet. Similar to section 4.2, we train our model on ImageNet using a concept list of the form “An image of a {class}”,\nwhere {class} refers to each of the ImageNet classes. In Figure 8, we show the heatmaps produced by our method compared\nto the raw attention maps of the ViT model, for different images from the ImageNet validation set. We observe that our\nSALF-CBM’s heatmaps tend to be more exclusive, while the raw attention maps often include background areas outside the\ntarget class object.\nFigure 8. Heatmaps generated by our SALF-CBM with a ViT-B/16 backbone (middle row) for random images from the ImageNet validation\nset, compared to the raw attention maps of the standard ViT model (bottom row). The ground-truth class of the images (from left to right):\n“Dalmatian”, “Balloon”, “Castle”, “Zebra”, “Consomme” and “Hamper”.\n\n\nC. CBL neurons validation\nWe qualitatively validate that neurons in our concept bottleneck layer indeed correspond to their designated target concepts.\nWe train a SALF-CBM on each dataset (ImageNet, Places365 and CUB-200) and randomly select 5 neurons from its concept\nbottleneck layer. For each neuron, we retrieve the top-3 images with the highest global concept activation c∗from the\ncorresponding validation set. As shown in Figure 9, the target concept of each neuron highly corresponds to the retrieved\nimages.\n(a) Results on ImageNet\n(b) Results on Places365\n(c) Results on CUB-200\nFigure 9. Qualitative validation of concepts learned by CBL neurons. Top 3 images with the highest concept activation c∗, for 5\nrandomly selected neurons in the CBL. The retrieved images are highly correlated with the neuron’s target concept. Results are shown for\nImageNet (left), Places365 (middle) and CUB-200 (right) datasets.\n\n\nD. Additional explanations results\nD.1. Explanations across different datasets\nWe present qualitative results of concept-based and spatial explanations across images from different datasets: ImageNet\n(Figure 10), Places365 (Figure 11) and CUB-200 (Figure 12). For each image, we present the most important concepts used\nby our SALF-CBM to classify the image, along with a heatmap of one of these concepts. By offering both concept-based\nexplanations and their visualizations on the input image, our model enables a comprehensive understanding of its decision-\nmaking process. For example, in the second row of Figure 11, we see that our model correctly classified the image as “athletic\nfield, outdoor” by identifying and accurately localizing the track behind the athlete.\nD.2. Explaning multi-class images\nWe demonstrate our method’s ability to produce class-specific explanations in Figure 13. Given an image x with two possible\nclasses, ˆy = l1 and ˆy = l2, we compute the concept contribution scores for predicting each class, i.e., S(x, m, ˆy = l1) and\nS(x, m, ˆy = l2), as described in Section 3.5. For each image, we present the concepts with the highest contribution scores\nalong with the heatmap of the most contributing concept.\nFigure 10. Concept-based and visual explanations on ImageNet.\n\n\nFigure 11. Concept-based and visual explanations on Places365.\n\n\nFigure 12. Concept-based and visual explanations on CUB-200.\n\n\nFigure 13. Explaining predictions on multi-class images. For each image, we present the most contributing concepts identified by\nSALF-CBM for explaining two different output classes that fit the image. We show the heatmap of the top concept for each class.\n\n\nE. Additional heatmaps results\nE.1. Visualizing multiple concepts\nWe demonstrate our method’s ability to localize multiple concepts within a single image. In Figure 14, we present qualitative\nresults on several images from the ImageNet validation set. For each image, we show three heatmaps generated by our\nSALF-CBM, each corresponding to a different visual concept.\nE.2. Visualizing concepts in videos\nBy applying SALF-CBM to video sequences in a frame-by-frame manner, we achieve visual tracking of specific concepts.\nIn Figure 15, we demonstrate this capability on several videos from the DAVIS 2017 dataset using a SALF-CBM trained\non ImageNet. Despite being trained on a completely different dataset, our model successfully localizes various concepts\nthroughout these videos. For example, in the “soccer ball” video at the top of the figure, the soccer ball is accurately\nhighlighted, even when it is partially occluded in the last frame.\nFigure 14. Localizing multiple concepts in images. For each image, we present three heatmaps, each corresponding to a different visual\nconcepts.\n\n\nFigure 15. Visualizing concepts in videos. By applying SALF-CBM in a frame-by-frame manner, one can visually track concepts over\ntime. Videos are from the DAVIS 2017 dataset (from top to bottom): “soccer ball”, “horsejump-high” and “rollerblade”.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20134v1.pdf",
    "total_pages": 20,
    "title": "Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models",
    "authors": [
      "Itay Benou",
      "Tammy Riklin-Raviv"
    ],
    "abstract": "Modern deep neural networks have now reached human-level performance across a\nvariety of tasks. However, unlike humans they lack the ability to explain their\ndecisions by showing where and telling what concepts guided them. In this work,\nwe present a unified framework for transforming any vision neural network into\na spatially and conceptually interpretable model. We introduce a\nspatially-aware concept bottleneck layer that projects \"black-box\" features of\npre-trained backbone models into interpretable concept maps, without requiring\nhuman labels. By training a classification layer over this bottleneck, we\nobtain a self-explaining model that articulates which concepts most influenced\nits prediction, along with heatmaps that ground them in the input image.\nAccordingly, we name this method \"Spatially-Aware and Label-Free Concept\nBottleneck Model\" (SALF-CBM). Our results show that the proposed SALF-CBM: (1)\nOutperforms non-spatial CBM methods, as well as the original backbone, on a\nvariety of classification tasks; (2) Produces high-quality spatial\nexplanations, outperforming widely used heatmap-based methods on a zero-shot\nsegmentation task; (3) Facilitates model exploration and debugging, enabling\nusers to query specific image regions and refine the model's decisions by\nlocally editing its concept maps.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}