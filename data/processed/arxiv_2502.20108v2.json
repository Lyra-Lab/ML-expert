{
  "id": "arxiv_2502.20108v2",
  "text": "VDT-Auto: End-to-end Autonomous Driving with VLM-Guided\nDiffusion Transformers\nZiang Guo\nKonstantin Gubernatorov\nSelamawit Asfaw\nZakhar Yagudin\nDzmitry Tsetserukou\nAbstract— In autonomous driving, dynamic environment and\ncorner cases pose significant challenges to the robustness of\nego vehicle’s decision-making. To address these challenges,\ncommencing with the representation of state-action mapping\nin the end-to-end autonomous driving paradigm, we introduce\na novel pipeline, VDT-Auto. Leveraging the advancement of\nthe state understanding of Visual Language Model (VLM) with\ndiffusion Transformer-based action generation, our VDT-Auto\nparses the environment geometrically and contextually for the\nconditioning of the diffusion process. Geometrically, we use a\nbird’s-eye view (BEV) encoder to extract feature grids from\nthe surrounding images. Contextually, the structured output\nof our fine-tuned VLM is processed into textual embeddings\nand noisy paths. During our diffusion process, the added\nnoise for the forward process is sampled from the noisy path\noutput of the fine-tuned VLM, while the extracted BEV feature\ngrids and embedded texts condition the reverse process of our\ndiffusion Transformers. Our VDT-Auto achieved 0.52 m on\naverage L2 errors and 21% on average collision rate in the\nnuScenes open-loop planning evaluation, presenting state-of-\nthe-art performance. Moreover, the real-world demonstration\nexhibited prominent generalizability of our VDT-Auto. The code\nand dataset will be released at https://github.com/ZionGo6/VDT-\nAuto.\nI. INTRODUCTION\nA. Motivation\nOver time, diffusion model-based approaches have proven\ntheir value in robotic policy learning tasks [1]–[3]. Dating\nback to the advancement of diffusion models, they have\ngained recognition as a cornerstone in the field of generative\nmodeling [4]. Conditioned diffusion models extend vanilla\ndiffusion models by incorporating additional information\nduring the generation process, while latent diffusion mod-\nels improve computational efficiency and sample quality\nby operating in a compressed latent space [5]. As shown\nabove, diffusion models have exhibited promising potential\nin generating high-quality data across various modalities and\nimproving the representation of complex data structures [6].\nIn robotic applications, multisensory data often includes\nrich and heterogeneous sources, such as camera images,\nLiDAR point clouds, etc. Diffusion models, through their\nability to condition on various modalities, can generate\ncoherent and contextually relevant outputs. This capability\nis particularly advantageous for robotic state-action mapping,\nwhere accurate interpretation and synthesis of multisensory\nThe\nauthors\nare\nwith\nthe\nIntelligent\nSpace\nRobotics\nLaboratory,\nCenter\nfor\nDigital\nEngineering,\nSkolkovo\nInstitute\nof\nScience\nand\nTechnology,\nMoscow,\nRussia\n{ziang.guo,\nKonstantin.Gubernatorov, Selamawit.Asfaw,\nZakhar.Yagudin, d.tsetserukou}@skoltech.ru\n(a) VLM’s path proposals from\n(b) Conditional sampled paths\nthe continuous frames based on\nby our diffusion Transformers.\na consistent scenario.\nFig. 1: We conduct the experiments with our VDT-Auto on\nunseen real-world driving dataset in a zero-shot way. The\nVLM is fine-tuned on our processed nuScenes dataset while\nthe path proposals from the fine-tuned VLM are able to\nprovide contextual approximation across the unseen continu-\nous frames. Subsequently, our diffusion Transformers sample\nthe path proposals based on the geometric and contextual\nconditions.\ninputs are crucial for effective decision-making and action\nexecution [7].\nRegarding autonomous driving, where the end-to-end\nparadigm has evolved vigorously [8], state-action mapping\nis a core principle that enables vehicles to learn effective\ndecision-making policies directly from raw sensor inputs [9].\nThis process involves mapping the current state of the vehicle\nand its environment to appropriate control and planning\nactions.\nTo enrich the state-understanding capacity of end-to-\nend autonomous driving systems, Visual Language Models\n(VLMs) have exhibited an outstanding impact, significantly\nimproving the systems’ interpreting capability of complex\ndriving scenarios [10], [11]. Accordingly, it is essential to\nenhance the decision-making capabilities as the improvement\nof state understanding by proposing adaptive and context-\naware actions tailored to various driving scenarios [12].\nWith these insights, we propose VDT-Auto, an end-to-\nend paradigm that bridges states and actions via VLM and\ndiffusion Transformers. For state understanding, images from\nthe surrounding cameras are encoded into bird’s-eye view\n(BEV) features. In addition, a front image among the sur-\narXiv:2502.20108v2  [cs.CV]  1 Mar 2025\n\n\nrounding images is passed to a supervised fine-tuned VLM\nfor contextual interpretation by the description of the detec-\ntion, the advice of ego vehicle’s behavior and the proposal\nof a path. Meanwhile, the designed diffusion Transformers\nencode both the BEV features from the BEV backbone and\nthe contextual embeddings from the VLM as the states to\npredict the optimized path, where the added noise in the\ndiffusion process is sampled from the VLM’s proposed path.\nOur contributions in this paper are summarized as follows:\n• We introduce a novel pipeline, VDT-Auto, which em-\nploys a BEV encoder and a VLM to geometrically\nand contextually parse the environment. The parsed\ninformation is then used to condition the diffusion\nprocess of our diffusion Transformers to generate the\noptimized actions of the ego vehicle.\n• VDT-Auto is differentiable, where we use a processed\nnuScenes dataset to train our BEV encoder for per-\nception and fine-tune our VLM for conditioning the\ndiffusion Transformers. The constructed and processed\ndataset will be publicly available.\n• In the nuScenes open-loop planning evaluation, our\nVDT-Auto achieved 0.52 m on average L2 errors and\n21% on average collision rate. In our real-world driving\ndataset, VDT-Auto showed promising performance on\nthe unseen data in a zero-shot way.\nB. Related Work\n1) Conditioned Diffusion Models: By operating the data\nin latent space instead of pixel space, conditioned diffu-\nsion models have gained promising development [5]. MM-\nDiffusion [13] designed for joint audio and video generation\ntook advantage of coupled denoising autoencoders to gener-\nate aligned audio-video pairs from Gaussian noise. Extending\nthe scalability of diffusion models, diffusion Transformers\ntreat all inputs, including time, conditions, and noisy image\npatches, as tokens, leveraging the Transformer architecture\nto process these inputs [14]. In DiT [4], William et al.\nemphasized the potential for diffusion models to benefit from\nTransformer architectures, where conditions were tokenized\nalong with image tokens to achieve in-context conditioning.\n2) Diffusion Models in Robotics: Recently, a probabilistic\nmultimodal action representation was proposed by Cheng\nChi et al. [1], where the robot action generation is considered\nas a conditional diffusion denoising process. Leveraging the\ndiffusion policy, Ze et al. [15] conditioned the diffusion\npolicy on compact 3D representations and robot poses to\ngenerate coherent action sequences. Furthermore, GR-MG\ncombined a progress-guided goal image generation model\nwith a multimodal goal-conditioned policy, enabling the\nrobot to predict actions based on both text instructions\nand generated goal images [7]. BESO used score-based\ndiffusion models to learn goal-conditioned policies from\nlarge, uncurated datasets without rewards. Score-based dif-\nfusion models progressively add noise to the data and then\nreverse this process to generate new samples, making them\nsuitable for capturing the multimodal nature of play data\n[16]. RDT-1B employed a scalable Transformer backbone\ncombined with diffusion models to capture the complexity\nand multimodality of bimanual actions, leveraging diffusion\nmodels as a foundation model to effectively represent the\nmultimodality inherent in bimanual manipulation tasks [17].\nNoMaD exploited the diffusion model to handle both goal-\ndirected navigation and task-agnostic exploration in unfamil-\niar environments, using goal masking to condition the policy\non an optional goal image, allowing the model to dynami-\ncally switch between exploratory and goal-oriented behaviors\n[18]. The aforementioned insights grounded the significant\nadvancements of diffusion models in robotic tasks.\n3) VLM-based\nAutonomous\nDriving:\nEnd-to-end\nau-\ntonomous driving introduces policy learning from sensor data\ninput, resulting in a data-driven motion planning paradigm\n[19]. As part of the development of VLMs, they have\nshown significant promise in unifying multimodal data for\nspecific downstream tasks, notably improving end-to-end\nautonomous driving systems [20]. DriveMM can process sin-\ngle images, multiview images, single videos, and multiview\nvideos, and perform tasks such as object detection, motion\nprediction, and decision making, handling multiple tasks and\ndata types in autonomous driving [21]. HE-Drive aims to\ncreate a human-like driving experience by generating trajec-\ntories that are both temporally consistent and comfortable.\nIt integrates a sparse perception module, a diffusion-based\nmotion planner, and a trajectory scorer guided by a Vision\nLanguage Model to achieve this goal [22]. Based on current\nperspectives, a differentiable end-to-end autonomous driving\nparadigm that directly leverages the capabilities of VLM and\na multimodal action representation should be developed.\nII. FRAMEWORK OVERVIEW\nA. BEV Encoder\nIn Fig. 2, our BEV encoder is based on LSS [23], [24],\nwhere the surrounding camera images from the T time steps\nare lifted into the BEV feature grids. F k\nt ∈R(Cf +Dd)×H×W\nrepresents the extracted features of the k-th camera at time\nt from the image backbone, where F k\nt,Cf ∈RCf ×H×W is\nthe contextual features and F k\nt,Dd ∈RDd×H×W represents\nthe estimated depth distribution. Then the contextual feature\nmap in height dimension F ′k\nt\nis computed as F k\nt,Cf ⊗F k\nt,Dd.\nAccording to the nuScenes camera setup [25], with the\nintrinsics and extrinsics of the cameras, F ′k\nt\nis then aggre-\ngated and weighted along the height dimension into the ego-\ncentered coordinate system to obtain the BEV feature grids\nGt ∈RCstate×H×W at time t, where Cstate is the number of\nstate channels.\nB. VLM Module\nFor our work, Qwen2-VL-7B is used to bridge the input\nof sensory data and the output of contextual conditions [26].\nIn Qwen2-VL, Multimodal Rotary Position Embedding (M-\nRoPE) is applied to process multimodal input by decompos-\ning rotary embedding into temporal, height, and width com-\nponents, which equips Qwen2-VL with powerful multimodal\ndata handling capabilities. In our VDT-Auto, the supervised\nfine-tuning of Qwen2-VL-7B is carried out by feeding a\n\n\nFig. 2: Framework overview of VDT-Auto. At each time step, the surrounding images are encoded by the BEV encoder to\nprovide the geometric feature grids of the scenario. A front image from the surrounding images is analyzed by our fine-tuned\nVLM to provide the contextual information of the conditions. Based on the BEV feature grids and VLM output, we construct\nthe conditional latents for our diffusion Transformers, where the BEV feature grids and VLM’s detection and advice are\nembedded and VLM’s path proposal is sampled for conditioning. In Section III, we introduce our noise sampling approach\nin details. Finally, our diffusion Transformers denoise the VLM’s path proposal, conditioning on the geometric feature grids\nof the scenario and the contextual information from our fine-tuned VLM.\nfront image of surrounding cameras and system prompts,\nexpecting the output of the description of the detection, the\nstructured advice of the behavior of the ego vehicle, and the\nproposal of a path. To achieve supervised fine-tuning, we\nconstructed our fine-tuning dataset by extracting ground truth\ninformation from the nuScenes dataset [25]. In Section III,\nwe will introduce more details about our dataset construction\nand supervised fine-tuning.\nC. Diffusion Prerequisites\nIn Fig. 2, we show our entire VDT-Auto pipeline, where\nthe feature grids Gt ∈RCstate×H×W of the BEV encoder\nand the contextual output St of Qwen2-VL-7B including\nthe description of the detection and structured advice on\nthe behavior of the ego vehicle are encoded as state con-\nditions for the diffusion process. Thus, in our designed\ndiffusion Transformers, the conditioned policy πθ(At|Gt, St)\npredicts the denoised path At = (a0\nt, a1\nt, . . . , an\nt ) of length\nn, conditioned on both the current BEV features Gt and\ncontextual embeddings St [27]–[29]. During training, the\nproposal of a path based on supervised fine-tuning of Qwen2-\nVL-7B at time t pairing with current BEV features Gt and\ncontextual embeddings St to form the training set, where\nour diffusion Transformers aim to maximize log-likelihood\nℓtraining throughout the training set,\nℓtraining = arg max\nθ\n(ai\nt,gi\nt,si\nt)∈(A′\nt,Gt,St) log πθ(ai\nt|gi\nt, si\nt), (1)\nwhere ai\nt, gi\nt, si\nt are sampled from our constructed training\nset. We extract the noise distribution σVLM from the path\noutput of the supervised fine-tuned Qwen2-VL-7B to con-\nstruct the noisy path dataset A′\nt by adding the sampled noise\nfrom the extracted noise distribution σVLM to the ground truth\npath Agt of nuScenes.\nIn Section III, we demonstrate that the noise distribution\nσVLM of the path proposal from our fine-tuned Qwen2-VL-\n7B is treated as a normal distribution, where we examine the\nextracted noise using One-Sample Kolmogorov-Smirnov test\nfor both the x and y coordinates of the paths [30].\nD. Loss Functions\nOur diffusion Transformers predict the denoised path At\nconditioned on current BEV features Gt and contextual\nembeddings St. Therefore, the loss function is defined as\nfollows.\nLtrain = LMSE(πθ(At|gt, st, ϵ), Agt)+\nLMSE(\nn\nX\nj=1\naj,\nn\nX\nj=1\naj\ngt),\n(2)\nwhere πθ is our trained diffusion Transformers. Under the\nconditions of encoded BEV features gt ∈Gt, contextual\nembeddings st ∈St, and added noise ϵ ∈σVLM, the first part\nof our loss function is the mean squared error between the\npath prediction At and the ground truth path from nuScenes\nAgt. Besides, the second part of our loss function is the mean\nsquared error between the cumulative sum of the waypoints\naj ∈At and aj\ngt ∈Agt.\nIII. METHODOLOGY\nA. Supervised Fine-tuning of Qwen2-VL-7B\nWe extract the information including the detection results\nand ego future trajectory from nuScenes. Then we generate\nthe advice for the ego vehicle’s behavior according to the\nchanges in the ego vehicle’s speed and temporal trajectory.\nBy feeding the system prompts and a front image from the\nsurrounding cameras into the VLM, an example of our fine-\ntuning dataset is shown in Fig. 3.\n\n\nFig. 3: In our constructed dataset for VLM’s supervised\nfine-tuning, we input a system prompt and a front image\ninto VLM and expect a structured output including detected\nobjects, advice of the ego vehicle’s driving behavior, and the\nproposal of a path.\nTABLE I: The results of the verification of normally dis-\ntributed noise from our fine-tuned VLM’s responses.\nNumber of VLM\npath outputs\nNumber of the paths\nwith normally\ndistributed noise\nPercentage of the paths\nwith normally\ndistributed noise\n20235\n19830\n98.00%\n37812\n36890\n97.60%\n67113\n65453\n97.52%\n111384\n108497\n97.40%\nB. VLM-guided Diffusion Transformers\nNormal distribution verification. To obtain the responses\nfrom our VLM module, we iteratively feed the system\nprompts and the front image of the surrounding cameras per\ntime t to our fine-tuned VLM throughout the training set\nto perform inference. Based on all the responses obtained,\nwe extract the noise distribution σVLM from the coordinates\nx and y of the proposal of the paths by subtracting the\ncorresponding ground truth paths Agt. In Table I, we show\nthat the percentage of paths with normally distributed noise\naccording to the amounts of the proposal of the paths\nobtained, where the One-Sample Kolmogorov-Smirnov test\nis used on both x and y coordinates to compare the em-\npirical cumulative distribution function (EDF) of our noise\ndata against the theoretical cumulative distribution function\n(CDF) of a normal distribution with the same mean and\nstandard deviation as our noise data due to the nonparametric\nattributes of the Kolmogorov-Smirnov test [30]. Given a path\nsample a0, a1, . . . , an, the empirical distribution function\n(EDF) Fn(x) is defined as\nFn(x) = 1\nn\nn\nX\ni=1\nI(ai ≤x),\n(3)\nwhere I(·) is the indicator function that equals 1 if the\ncondition inside is true and 0 otherwise.\nThe Kolmogorov-Smirnov distribution Dn is defined as\nthe maximum absolute difference between the EDF Fn(x)\nand the CDF F(x),\nDn = sup\nx |Fn(x) −F(x)|,\n(4)\nwhere sup is the supremum of the set of distances.\nFor our path sample a0, a1, . . . , an, p is defined as the\nprobability that the Kolmogorov distribution K exceeds the\ncalculated Dn. If p is below the significance level αp, the\npath sample does not follow a normal distribution. Setting\nαp = 0.05, the Kolmogorov distribution K is defined by its\ncumulative distribution function,\nP(K ≤t) = 1 −2\n∞\nX\nk=1\n(−1)k−1e−2k2t2,\n(5)\nfor t > 0.\nThus, when p = Pr(K > Dn) < αp, the noise is\nconsidered as being drawn from a normal distribution for\nboth the coordinates x and y of the proposal of the paths.\nDuring the verification, we iteratively input the front images\nfrom the nuScenes dataset into our fine-tuned VLM to obtain\nthe responses. We then identify the paths with normally\ndistributed noise from these responses, where the normal\ndistribution is verified via One-Sample Kolmogorov-Smirnov\ntest for both the x and y coordinates of the paths. In Table I,\nwe show the number and percentage of the paths qualified by\nthe Kolmogorov-Smirnov test on both x and y coordinates\nacross different samples from the responses obtained.\nDiffusion process. Based on the DDIM scheduler [31], we\ngradually add the sampled noise from the VLM to the ground\ntruth nuScenes path Agt in a forward diffusion process as\nfollows,\nai′ =\n√\n¯αia0\ngt +\np\n1 −¯αiϵ,\nϵ ∼σVLM,\n(6)\nwhere ai′ is the noised sample at scheduler timestep i. ϵ is\nthe sampled noise from σVLM. βt is the sequence of variances\nthat control the amount of noise added at each diffusion\ntimestep and ¯αi = Qi\nt=1 αt = Qi\nt=1(1 −βt). To ensure\nthe stability of our training and unbiased noising, we then\nstandardize the noised path ai′ ∈A′\nt.\nIn the reverse diffusion process, our diffusion Transform-\ners πθ predict the denoised path At for each denoising\ntimestep tR, where the noisy path from the output of our\nVLM is denoised following the DDIM scheduler. We define\nthe noise scheduler as σ(tR) = e−tR. Given the input of the\nnoisy path A′\nt and the prediction At, the denoised path is\nupdated as follows,\nA′\nt =\n\u0012σ(tR+1)\nσ(tR)\n\u0013\n· A′\nt −(e−h −1) · At,\n(7)\nwhere h = tR+1 −tR is the denoising time interval.\n\n\nTABLE II: Ablation study of our timestep embedding (TSE),\ncross-attention-based fusion of geometric and contextual\nembedding (CAF), contextual average pooling (CAP), and\nBEV feature compression (BFC) on nuScenes validation set.\nTSE\nCAF\nCAP\nBFC\nAvg. L2\nAvg. Collision Rate\n✓\n✓\n✓\n✓\n0.52\n0.21\n✓\n✓\n✓\n1.08\n0.60\n✓\n✓\n1.21\n0.88\nFig. 4: Our experimental car for the recording of our real-\nworld driving dataset.\nIV. EXPERIMENTS\nA. Experimental Setup\nWe conducted our training and open-loop experiments on\nthe nuScenes dataset that consists of 1, 000 street scenes\ncollected from Boston and Singapore, known for their dense\ntraffic and challenging driving conditions [25]. In addition,\nwe evaluated our pipeline on a real-world driving dataset\nin a zero-shot manner. The real-world driving dataset was\nrecorded by our experimental car shown in Fig. 4 [39].\nIn our experiments, we first trained our BEV encoder\nand fine-tuned Qwen2-VL-7B on nuScenes to obtain the\nBEV features and the VLM responses from their inference.\nThen we cached the BEV features and VLM responses and\nconstructed the training set for the training of our diffusion\nTransformers.\nB. Comparison with Other State-of-the-Art Methods on\nnuScenes\nDuring the evaluation, on the nuScenes validation set [25],\nwe compared our VDT-Auto with other methods by the L2\nerror in meters and the collision rate in percentage in Table\nIII. The average L2 error is determined by calculating the\ndistance between each waypoint in the planned trajectory and\nthe corresponding waypoint in the ground truth trajectory.\nThis metric indicates how closely the planned trajectory\naligns with a human-driven trajectory. The collision rate\nis assessed by positioning an ego-vehicle bounding box at\neach waypoint along the planned trajectory and subsequently\nchecking for any intersections with the ground truth bound-\ning boxes of other objects. Our VDT-Auto achieved state-of-\nthe-art performance in open-loop planning tasks.\nC. Experiments on Real-world Driving Dataset\nIn our real-world driving dataset, we demonstrate the\npotentials of our VDT-Auto on unseen data in a zero-\nshot way, where our BEV encoder is adjusted to obtain\nthe extracted features from a single front image, and the\nfine-tuned VLM analyzes the front image to provide the\ncontextual information. In Fig. 1, we show the VLM’s path\nproposals from the continuous frames based on a consistent\nscenario in the left column (a), while the conditional sampled\npaths are shown in the right column (b).\nD. Ablation Study\nTo verify the effectiveness of our design, in Table II,\nwe show the results of the ablation experiments of our\nVDT-Auto with timestep embedding (TSE), cross-attention-\nbased fusion of geometric and contextual embedding (CAF),\ncontextual average pooling (CAP), and BEV feature com-\npression (BFC) in nuScenes validation set. In TSE, we\nembed the noise scheduler timesteps for the preparation of\na cross-attention-based fusion of geometric and contextual\nembedding, while CAP and BFC are the dimensionality\nreduction of the BEV features Gt and contextual embeddings\nSt for the stability of training and inference.\nV. CONCLUSION\nConsidering the advancements of state understanding and\nthe corresponding decision-making capabilities, we propose\na novel pipeline, VDT-Auto, where the state information\nis encoded geometrically and contextually, conditioning a\ndiffusion Transformer-based action generation. In this paper,\nwe demonstrate the methodology of using powerful VLM\nsuch as Qwen2-VL to bridge states and conditions, as well\nas the connections between conditions and actions via a\ndiffusion policy. The verification of our VDT-Auto was\nperformed using nuScenes open-loop planning evaluation,\nwhere our VDT-Auto achieved 0.52 m on average L2 errors\nand 21% on average collision rate. In addition, on a real-\nworld driving dataset, our VDT-Auto shows its promising\ngeneralizability.\nDuring our development, we discovered that the distribu-\ntion of training data had varying influences on the different\nparts of our VDT-Auto due to the model scales of the VLM\nand the diffusion model-based network. Therefore, with\nsufficient computational resources, an end-to-end training\napproach should be developed to mitigate the influence of\ndata distribution. In our future work, with an end-to-end\ntraining pipeline, VDT-Auto will be targeting towards more\ncomplex traffic scenarios and a close-loop evaluation. Owing\nto the rapid evolution of VLMs and robotic policy, VDT-\nAuto is able to contribute as a cornerstone case in data-driven\npolicy learning tasks.\n\n\nTABLE III: The open-loop planning results of our VDT-Auto on nuScenes validation set.\nNo.\nMethods\nL2 (m) ↓\nCollision Rate (%) ↓\n1s\n2s\n3s\nAvg.\n1s\n2s\n3s\nAvg.\n1\nFF [32]\n0.55\n1.20\n2.54\n1.43\n0.06\n0.17\n1.07\n0.43\n2\nEO [33]\n0.67\n1.36\n2.78\n1.60\n0.04\n0.09\n0.88\n0.33\n3\nST-P3 [34]\n1.33\n2.11\n2.90\n2.11\n0.23\n0.62\n1.27\n0.71\n4\nUniAD [35]\n0.48\n0.96\n1.65\n1.03\n0.05\n0.17\n0.71\n0.31\n5\nGPT-Driver [36]\n0.27\n0.74\n1.52\n0.84\n0.07\n0.15\n1.10\n0.44\n6\nVLP-UniAD [37]\n0.36\n0.68\n1.19\n0.74\n0.03\n0.12\n0.32\n0.16\n7\nRDA-Driver [38]\n0.23\n0.73\n1.54\n0.80\n0.00\n0.13\n0.83\n0.32\n8\nDriveVLM [10]\n0.18\n0.34\n0.68\n0.40\n0.10\n0.22\n0.45\n0.27\n9\nHE-Drive-B [22]\n0.30\n0.56\n0.89\n0.58\n0.00\n0.03\n0.14\n0.06\n10\nOurs\n0.20\n0.47\n0.88\n0.52\n0.05\n0.18\n0.40\n0.21\nREFERENCES\n[1] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song,\n“Diffusion policy: Visuomotor policy learning via action diffusion,” in\nProceedings of Robotics: Science and Systems (RSS), 2023.\n[2] B. Yang, H. Su, N. Gkanatsios, T.-W. Ke, A. Jain, J. Schneider, and\nK. Fragkiadaki, “Diffusion-es: Gradient-free planning with diffusion\nfor autonomous driving and zero-shot instruction following,” arXiv\npreprint arXiv:2402.06559, 2024.\n[3] W. Yu, J. Peng, H. Yang, J. Zhang, Y. Duan, J. Ji, and Y. Zhang, “Ldp:\nA local diffusion planner for efficient robot navigation and collision\navoidance,” in 2024 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS).\nIEEE, 2024, pp. 5466–5472.\n[4] W. Peebles and S. Xie, “Scalable diffusion models with transform-\ners,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 2023, pp. 4195–4205.\n[5] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n“High-resolution image synthesis with latent diffusion models,” in\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022, pp. 10 684–10 695.\n[6] X. Yang and X. Wang, “Diffusion model as representation learner,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 18 938–18 949.\n[7] P. Li, H. Wu, Y. Huang, C. Cheang, L. Wang, and T. Kong, “Gr-mg:\nLeveraging partially-annotated data via multi-modal goal-conditioned\npolicy,” IEEE Robotics and Automation Letters, 2025.\n[8] W. Sun, X. Lin, Y. Shi, C. Zhang, H. Wu, and S. Zheng, “Sparsedrive:\nEnd-to-end autonomous driving via sparse scene representation,” arXiv\npreprint arXiv:2405.19620, 2024.\n[9] B. Liao, S. Chen, H. Yin, B. Jiang, C. Wang, S. Yan, X. Zhang, X. Li,\nY. Zhang, Q. Zhang et al., “Diffusiondrive: Truncated diffusion model\nfor end-to-end autonomous driving,” arXiv preprint arXiv:2411.15139,\n2024.\n[10] X. Tian, J. Gu, B. Li, Y. Liu, Y. Wang, Z. Zhao, K. Zhan,\nP. Jia, X. Lang, and H. Zhao, “Drivevlm: The convergence of au-\ntonomous driving and large vision-language models,” arXiv preprint\narXiv:2402.12289, 2024.\n[11] Z. Guo, A. Lykov, Z. Yagudin, M. Konenkov, and D. Tsetserukou,\n“Co-driver: Vlm-based autonomous driving assistant with human-like\nbehavior and understanding for complex road scenes,” arXiv preprint\narXiv:2405.05885, 2024.\n[12] Z. Li, K. Li, S. Wang, S. Lan, Z. Yu, Y. Ji, Z. Li, Z. Zhu,\nJ. Kautz, Z. Wu et al., “Hydra-mdp: End-to-end multimodal planning\nwith multi-target hydra-distillation,” arXiv preprint arXiv:2406.06978,\n2024.\n[13] L. Ruan, Y. Ma, H. Yang, H. He, B. Liu, J. Fu, N. J. Yuan, Q. Jin,\nand B. Guo, “Mm-diffusion: Learning multi-modal diffusion models\nfor joint audio and video generation,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n10 219–10 228.\n[14] F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu, “All are\nworth words: A vit backbone for diffusion models,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2023, pp. 22 669–22 679.\n[15] Y. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu, “3d diffusion\npolicy,” arXiv preprint arXiv:2403.03954, 2024.\n[16] M. Reuss, M. Li, X. Jia, and R. Lioutikov, “Goal-conditioned imi-\ntation learning using score-based diffusion policies,” arXiv preprint\narXiv:2304.02532, 2023.\n[17] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su,\nand J. Zhu, “Rdt-1b: a diffusion foundation model for bimanual\nmanipulation,” arXiv preprint arXiv:2410.07864, 2024.\n[18] A. Sridhar, D. Shah, C. Glossop, and S. Levine, “Nomad: Goal\nmasked diffusion policies for navigation and exploration,” arXiv\npre-print, 2023. [Online]. Available: https://arxiv.org/abs/2310.07896\n[19] S. Chen, B. Jiang, H. Gao, B. Liao, Q. Xu, Q. Zhang, C. Huang,\nW. Liu, and X. Wang, “Vadv2: End-to-end vectorized autonomous\ndriving via probabilistic planning,” arXiv preprint arXiv:2402.13243,\n2024.\n[20] Y. Ma, Y. Cao, J. Sun, M. Pavone, and C. Xiao, “Dolphins: Multimodal\nlanguage model for driving,” in European Conference on Computer\nVision.\nSpringer, 2024, pp. 403–420.\n[21] Z. Huang, C. Feng, F. Yan, B. Xiao, Z. Jie, Y. Zhong, X. Liang, and\nL. Ma, “Drivemm: All-in-one large multimodal model for autonomous\ndriving,” arXiv preprint arXiv:2412.07689, 2024.\n[22] J. Wang, X. Zhang, Z. Xing, S. Gu, X. Guo, Y. Hu, Z. Song, Q. Zhang,\nX. Long, and W. Yin, “He-drive: Human-like end-to-end driving with\nvision language models,” arXiv preprint arXiv:2410.05051, 2024.\n[23] J. Philion and S. Fidler, “Lift, splat, shoot: Encoding images from\narbitrary camera rigs by implicitly unprojecting to 3d,” in Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part XIV 16. Springer, 2020, pp. 194–210.\n[24] A. Hu, Z. Murez, N. Mohan, S. Dudas, J. Hawke, V. Badrinarayanan,\nR. Cipolla, and A. Kendall, “Fiery: Future instance prediction in bird’s-\neye view from surround monocular cameras,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2021, pp.\n15 273–15 282.\n[25] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,\nA. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A\nmultimodal dataset for autonomous driving,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 11 621–11 631.\n[26] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen,\nX. Liu, J. Wang, W. Ge et al., “Qwen2-vl: Enhancing vision-language\nmodel’s perception of the world at any resolution,” arXiv preprint\narXiv:2409.12191, 2024.\n[27] F. Bao, S. Nie, K. Xue, C. Li, S. Pu, Y. Wang, G. Yue, Y. Cao,\nH. Su, and J. Zhu, “One transformer fits all distributions in multi-\nmodal diffusion at scale,” in International Conference on Machine\nLearning.\nPMLR, 2023, pp. 1692–1717.\n[28] Y. Han, R. Wang, C. Zhang, J. Hu, P. Cheng, B. Fu, and H. Zhang,\n“Emma: Your text-to-image diffusion model can secretly accept multi-\nmodal prompts,” arXiv preprint arXiv:2406.09162, 2024.\n[29] M. Reuss, ¨O. E. Ya˘gmurlu, F. Wenzel, and R. Lioutikov, “Multimodal\ndiffusion transformer: Learning versatile behavior from multimodal\ngoals,” in First Workshop on Vision-Language Models for Navigation\nand Manipulation at ICRA 2024, 2024.\n[30] M. Karson, “Handbook of methods of applied statistics. volume\ni: Techniques of computation descriptive methods, and statistical\ninference. i. m. chakravarti, r. g. laha, and j. roy, new york, john\nwiley; 1967,” Journal of the American Statistical Association, vol. 63,\nno. 323, 1968. [Online]. Available: https://doi.org/10.1080/01621459.\n1968.11009335\n\n\n[31] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit\nmodels,” arXiv preprint arXiv:2010.02502, 2020.\n[32] P. Hu, A. Huang, J. Dolan, D. Held, and D. Ramanan, “Safe local\nmotion planning with self-supervised freespace forecasting,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021, pp. 12 732–12 741.\n[33] T. Khurana, P. Hu, A. Dave, J. Ziglar, D. Held, and D. Ramanan,\n“Differentiable raycasting for self-supervised occupancy forecasting,”\nin European Conference on Computer Vision.\nSpringer, 2022, pp.\n353–369.\n[34] S. Hu, L. Chen, P. Wu, H. Li, J. Yan, and D. Tao, “St-p3: End-\nto-end vision-based autonomous driving via spatial-temporal feature\nlearning,” in European Conference on Computer Vision.\nSpringer,\n2022, pp. 533–549.\n[35] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du,\nT. Lin, W. Wang et al., “Planning-oriented autonomous driving,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 17 853–17 862.\n[36] J. Mao, Y. Qian, J. Ye, H. Zhao, and Y. Wang, “Gpt-driver: Learning\nto drive with gpt,” arXiv preprint arXiv:2310.01415, 2023.\n[37] C. Pan, B. Yaman, T. Nesti, A. Mallik, A. G. Allievi, S. Velipasalar,\nand L. Ren, “Vlp: Vision language planning for autonomous driving,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024, pp. 14 760–14 769.\n[38] Z. Huang, T. Tang, S. Chen, S. Lin, Z. Jie, L. Ma, G. Wang,\nand X. Liang, “Making large language models better planners with\nreasoning-decision alignment,” in European Conference on Computer\nVision.\nSpringer, 2024, pp. 73–90.\n[39] Z. Guo, S. Perminov, M. Konenkov, and D. Tsetserukou, “Hawkdrive:\nA transformer-driven visual perception system for autonomous driving\nin night scene,” arXiv preprint arXiv:2404.04653, 2024.\n\n\n",
  "metadata": {
    "source_file": "dataset/pdfs/arxiv_2502.20108v2.pdf",
    "total_pages": 7,
    "title": "VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers",
    "authors": [
      "Ziang Guo",
      "Konstantin Gubernatorov",
      "Selamawit Asfaw",
      "Zakhar Yagudin",
      "Dzmitry Tsetserukou"
    ],
    "abstract": "In autonomous driving, dynamic environment and corner cases pose significant\nchallenges to the robustness of ego vehicle's decision-making. To address these\nchallenges, commencing with the representation of state-action mapping in the\nend-to-end autonomous driving paradigm, we introduce a novel pipeline,\nVDT-Auto. Leveraging the advancement of the state understanding of Visual\nLanguage Model (VLM), incorporating with diffusion Transformer-based action\ngeneration, our VDT-Auto parses the environment geometrically and contextually\nfor the conditioning of the diffusion process. Geometrically, we use a\nbird's-eye view (BEV) encoder to extract feature grids from the surrounding\nimages. Contextually, the structured output of our fine-tuned VLM is processed\ninto textual embeddings and noisy paths. During our diffusion process, the\nadded noise for the forward process is sampled from the noisy path output of\nthe fine-tuned VLM, while the extracted BEV feature grids and embedded texts\ncondition the reverse process of our diffusion Transformers. Our VDT-Auto\nachieved 0.52m on average L2 errors and 21% on average collision rate in the\nnuScenes open-loop planning evaluation. Moreover, the real-world demonstration\nexhibited prominent generalizability of our VDT-Auto. The code and dataset will\nbe released after acceptance.",
    "published_date": "2025-02-27",
    "source": "arxiv"
  }
}